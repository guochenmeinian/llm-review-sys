# Robust Mixture Learning when

Outliers Overwhelm Small Groups

Daniil Dmitriev\({}^{1*}\)  Raes-Darius Buhai\({}^{1*}\)  Stefan Tiegel\({}^{1}\)  Alexander Wolters\({}^{2}\)

Gleb Novikov\({}^{3}\)  Amartya Sanyal\({}^{4}\)  David Steurer\({}^{1}\)  Fanny Yang\({}^{1}\)

\({}^{1}\)ETH Zurich \({}^{2}\)TU Munich

\({}^{3}\)Lucerne School of Computer Science and Information Technology

\({}^{4}\)University of Copenhagen

\({}^{*}\)_Equal contribution_

###### Abstract

We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers. While strong guarantees are available when the outlier fraction is significantly smaller than the minimum mixing weight, much less is known when outliers may crowd out low-weight clusters - a setting we refer to as list-decodable mixture learning (LD-ML). In this case, adversarial outliers can simulate additional spurious mixture components. Hence, if all means of the mixture must be recovered up to a small error in the output list, the list size needs to be larger than the number of (true) components. We propose an algorithm that obtains order-optimal error guarantees for each mixture mean with a minimal list-size overhead, significantly improving upon list-decodable mean estimation, the only existing method that is applicable for LD-ML. Although improvements are observed even when the mixture is non-separated, our algorithm achieves particularly strong guarantees when the mixture is separated: it can leverage the mixture structure to partially cluster the samples before carefully iterating a base learner for list-decodable mean estimation at different scales.

## 1 Introduction

Estimating the mean of a distribution from empirical data is one of the most fundamental problems in statistics. The mean often serves as the primary summary statistic of the dataset or is the ultimate quantity of interest that is often not precisely measurable. In practical applications, data frequently originates from a mixture of multiple groups (also called subpopulations) and a natural goal is to estimate the distinct means of each group separately. For example, we might like to use representative individuals to study how a complex decision or procedure would impact different subpopulations. In other applications, such as genetics [1] or astronomy [2] research, finding the means themselves can be a crucial first step towards scientific discovery. In both scenarios, the algorithm should output a list of estimates that are close to the unobservable true means.

However, in practice, the data may also contain outliers, for example due to measurement errors or abnormal events. We would like to find good mean estimates for all inlier groups even when the proportion of such _additive adversarial contaminations_ is larger than some smaller groups that we want to properly represent. The central open question that motivates our work is thus:

_What is the cost of efficiently recovering small groups that may be outnumbered by outliers?_

More specifically, consider a scenario where the practitioner would like to recover the means of small but significant enough inlier groups which constitute at least \(w_{\mathrm{low}}\in(0,1)\) proportion of the(corrupted) data. If \(k\) is the number of such inlier groups, for all \(i\in[k]\), we then denote by \(w_{i}\geqslant w_{\mathrm{low}}\) the unknown weight of the \(i-\)th group with mean \(\mu_{i}\). Further, we use \(\varepsilon\) to refer to the proportion of additive contamination - the data that comes from an unknown adversarial distribution. The goal is to estimate the unknown means \(\mu_{i}\) for all \(i\in[k]\).

Existing works on robust mixture learning such as [3, 4] consider the problem when the fraction of additive adversarial outliers is smaller than the weight of the smallest subgroup, i.e. \(\varepsilon<w_{\mathrm{low}}\). However, for large outlier proportions where \(\varepsilon\geqslant w_{\mathrm{low}}\), these algorithms are not guaranteed to recover small clusters with \(w_{i}\leqslant\varepsilon\). In this case, outliers can form additional spurious clusters that are indistinguishable from small inlier groups. As a consequence, generating a list of size equal to the number of components would possibly lead to neglecting the means of small groups. In order to ensure that the output contains a precise estimate for each of the small group means, it is thus necessary the estimation algorithm to provide a list whose size is strictly larger than the number of components. We call this paradigm _list-decodable mixture learning_ (LD-ML), following the footsteps of a long line of work on list-decodable learning (see Sections 2 and 5).

Specifically, the main challenge in LD-ML is to provide a _short_ list that contains good mean estimates for all inlier groups. We first note that there is a minimum list size the algorithm necessarily has to output to guarantee that all groups are recovered. For example, consider an outlier distribution that includes several copies of the smallest inlier group distribution with means spread out throughout the domain. Since inlier groups are indisntinguishable from spurious outlier ones, the shortest list that includes means of all inlier groups must be of size at least \(|L|\geqslant k+\frac{\varepsilon}{\min_{i}w_{i}}\). Here, \(\frac{\varepsilon}{\min_{i}w_{i}}\) can be interpreted as the minimal list-size overhead that is necessary due to "caring" about groups with weight smaller than \(\varepsilon\). The key question is hence how good the error guarantees of an LD-ML algorithm can be when the list size overhead stays close to \(\frac{\varepsilon}{\min_{i}w_{i}}\), while being agnostic to \(w_{i}\) aside from the knowledge of \(w_{\mathrm{low}}\). Furthermore, we are interested in _computationally efficient_ algorithms for LD-ML, especially when dealing with high-dimensional data.

To the best of our knowledge, the only existing efficient algorithms that are guaranteed to recover inlier groups with weights \(w_{i}\leqslant\varepsilon\) are _list-decodable mean estimation_ (LD-ME) algorithms. LD-ME algorithms model the data as a mixture of one inlier and outlier distribution with weights \(\alpha\leqslant 1/2\) and \(1-\alpha\) respectively. Provided with the weight parameter \(\alpha\), they output a list that contains an estimate close to the inlier mean with high probability. However, for the LD-ML setting, the inlier weights \(w_{i}\) are not known and we would have to use LD-ME algorithms with \(w_{\mathrm{low}}\) as weight estimates for each group. This leads to suboptimal error in particular for large groups, that hence (somewhat counter intuitively) would have to "pay" for the explicit constraint to recover small groups. Furthermore, even if LD-ME were provided with \(w_{i}\), by design it would treat inlier points from other components also as outliers, unnecessarily inflating the fraction of outliers to \(1-w_{i}\) instead of \(\varepsilon\).

ContributionsIn this paper, we propose an algorithm that (i) correctly estimates the weight of each component only given a lower bound and (ii) does not overestimate proportion of outliers when components are well-separated. In particular, we construct a meta-algorithm that uses mean estimation algorithms as base learners that are designed to deal with adversarial corruptions. This meta-algorithm inherits guarantees from the base learner and any improvement of the latter translates to better results for LD-ML. For example, if the base learner runs in polynomial time, so does our meta-algorithm. Our approach of using the output of weak base learners to achieve better performance is reminiscent of the _boosting_ paradigm that is common in machine learning practice.

Our algorithm achieves significant improvements in error and list-size guarantees for multiple settings. For ease of comparison, we summarize error improvements for inlier Gaussian mixtures in Table 1 (see Theorem 3.3 for the general result regarding distributions with bounded moments). The main focus of our contributions is represented in the second row; that is the setting where outliers outnumber some inlier groups with weight \(w_{j}\leqslant\varepsilon\) and the inlier components are _well-separated_, i.e., \(\|\mu_{i}-\mu_{j}\|\gtrsim^{1}\sqrt{\log\frac{1}{w_{\mathrm{low}}}}\), where \(\mu_{i}\)'s are the inlier component means. As we mentioned before, robust mixture learning algorithms, such as [4, 7], are not applicable here and the best error guarantees in prior work is achieved by an LD-ME algorithm, e.g. from [3]. While its error bounds are of order \(O(\sqrt{\log\frac{1}{w_{\mathrm{low}}}})\) for a list size of \(O(\frac{1}{w_{\mathrm{low}}})\), our approach guarantees error \(O(\sqrt{\log\frac{\varepsilon}{w_{i}}})\) for a list size of \(k+O(\frac{\varepsilon}{w_{\mathrm{low}}})\). Remarkably, we obtain the same error guarantees as if an oracle would run LD-ME on each inlier group _with the correct weight_\(w_{i}\) separately (with outliers). Hence, the only cost for recovering small groups is the increased list-size overhead of order \(O(\frac{\varepsilon}{w_{\mathrm{low}}})\). Further, a sub-routine in our meta-algorithm also obtains novel guarantees under _no_ separation assumption, as shown in the third row of Table 1. This algorithm achieves the same error guarantees for similar list size as a base learner that knows the correct weights of the inlier components.

Based on a reduction argument from LD-ME to LD-ML, we also provide information-theoretic (IT) lower bounds for LD-ML. If the LD-ME base learners achieve the IT lower bound (possible for inlier Gaussian mixtures), so does our LD-ML algorithm. In synthetic experiments, we implement our meta-algorithm with the LD-ME base learner from [8] and show clear improvements compared to the only prior method with guarantees, while being comparable or better than popular clustering methods such as k-means and DBSCAN for various attack models.

## 2 Settings

We now introduce the learning settings that appear in the paper. Let \(d\in\mathbb{N}_{+}\) be the ambient dimension of the data and \(k\in\mathbb{N}_{+}\) be the number of mixture components (inlier groups/clusters).

### List-decodable mixture learning under adversarial corruptions

We focus on mixtures that consist of distributions that are sufficiently bounded in the following sense.

**Definition 2.1**.: Let \(t\in\mathbb{N}_{+}\) be even and let \(D(\mu)\) be a distribution on \(\mathbb{R}^{d}\) with mean \(\mu\). We say that \(D(\mu)\) has _sub-Gaussian \(t\)-th central moments_ if for all even \(s\leqslant t\) and for every \(v\in\mathbb{R}^{d}\) with \(\|v\|=1\), \(\mathbb{E}_{x\sim D}\left\langle x-\mu,v\right\rangle^{s}\leqslant(s-1)!!\).

This class of distributions is closely related to commonly studied distributions in the literature (see, e.g., [5]) with bounded \(t\)-th moment. Our requirement for the boundedness of all moments \(s\leqslant t\) stems from the fact that our algorithm should adapt to unknown and possibly non-uniform mixture weights.

We assume that we are given samples from a corrupted \(d\)-dimensional mixture of \(k\) inlier distributions \(D_{i}(\mu_{i})\) satisfying Definition 2.1, where the mixture is defined as

\[\mathcal{X}=\sum_{i=1}^{k}w_{i}D_{i}(\mu_{i})+\varepsilon Q,\] (2.1)

and \(\sum_{i=1}^{k}w_{i}+\varepsilon=1\), where for all \(i=1,\ldots,k\), it holds that \(w_{i}\geqslant w_{\mathrm{low}}\). Further, an \(\varepsilon>0\) proportion of the data comes from an _outlier_ distribution \(Q\) chosen by the adversary with full knowledge of our algorithm and inlier mixture. Samples drawn from \(D_{i}(\mu_{i})\) constitute the \(i^{\text{th}}\)_inlier cluster_. The goal in mixture learning under corruptions as in Eq. (2.1), is to design an algorithm that takes in i.i.d. samples from \(\mathcal{X}\) and outputs a list \(L\), such that for each \(i\in[k]\), there exists \(\hat{\mu}\in L\) with small estimation error \(\|\mu_{i}-\hat{\mu}\|\).

To the best of our knowledge, we are the first to study the _list-decodable mixture learning_ problem (LD-ML) that considers the case of large fractions of outliers \(\varepsilon\geqslant\min_{i}w_{i}\) and the goal is to achieve

\begin{table}
\begin{tabular}{l|l|l|l} \hline
**Type of inlier mixture** & **Best prior work** & **Ours** & **Inf.-theor. lower bound** \\ \hline Large (\(\forall j:\,\varepsilon\leqslant w_{j}\)), sep. groups & \(\widetilde{O}(\varepsilon/w_{i})\) & \(\widetilde{O}(\varepsilon/w_{i})\) & \(\Omega(\varepsilon/w_{i})\), see [5] \\ Small (\(\exists j:\,\varepsilon\geqslant w_{j}\)), sep. groups & \(O\left(\sqrt{\log\frac{1}{w_{\mathrm{low}}}}\right)\) & \(O\left(\sqrt{\log\frac{\varepsilon+w_{i}}{w_{i}}}\right)\) & \(\Omega\left(\sqrt{\log\frac{\varepsilon+w_{i}}{w_{i}}}\right)\), Prop. 3.5 \\ Non-separated groups & \(O\left(\sqrt{\log\frac{1}{w_{\mathrm{low}}}}\right)\) & \(O\left(\sqrt{\log\frac{1}{w_{i}}}\right)\) & \(\Omega\left(\sqrt{\log\frac{1}{w_{i}}}\right)\), see [6] \\ \hline \end{tabular}
\end{table}
Table 1: For a mixture of Gaussian components \(\mathcal{N}(\mu_{i},I_{d})\), we show upper and lower bounds for the **error of the \(i\)-component** given a output list \(L\) (of the respective algorithm) \(\min_{i\in L}\|\hat{\mu}-\mu_{i}\|\). When the error doesn’t depend on \(i\), all means have the same error guarantee irrespective of their weight. Note that depending on the type of inlier mixture, different methods in [3] are used as the ’best prior work’: robust mixture learning for the first row and list-decodable mean estimation for the rest.

small estimation errors while the list size \(|L|\) remains small. While in robust estimation problems, the fractions of inliers and outliers are usually provided to the algorithm, in mixture learning, the mixture proportions are explicit quantities of interest. Throughout the paper, we hence assume that _both_ the true weights \(w_{i}\) of the mixture and the fraction of outliers \(\varepsilon\) are _unknown_. Instead, by definition in Eq. (2.1), we assume knowledge of a valid lower bound \(w_{\text{low}}\leqslant\min_{i}w_{i}\).

Note that when \(\varepsilon\lesssim\min_{i}w_{i}\), the problem is known as robust mixture learning and can be solved with list size \(|L|=k\) as discussed in [3; 4; 7]. However, algorithms for robust mixture learning fail when the fraction of outliers becomes comparable to the inlier group size. In the presence of "spurious" adversarial clusters, it is information-theoretically impossible to output a list \(L\), such that (i) \(|L|=k\) and (ii) \(L\) contains precise estimate for each true mean.

### Mean estimation under adversarial corruptions

In order to solve LD-ML, we use mean estimation procedures that have provable guarantees under adversarial contamination. Mean estimation can be viewed as a particular case of the mixture learning problem in Eq. (2.1) with \(k=1\), the fraction of inliers \(\alpha=w_{1}\) and the fraction of outliers \(\varepsilon=1-\alpha\). The mean estimation algorithms we use to solve LD-ML with \(w_{\text{low}}\) need to exhibit guarantees under a stronger adversarial model, where the adversary can also replace a small fraction (depending on \(w_{\text{low}}\)) of the inlier points; see details in Definition B.1. This is a special case of the general contamination model as opposed to the slightly more benign additive contamination model in Eq. (2.1). For different regimes of \(\alpha\) we use black-box learners that solve corresponding regime when _provided with \(\alpha\)_.

Robust mean estimationWhen the majority of points are inliers, we are in the \(\operatorname{RME}\) setting. Robust statistics has studied this setting with different corruption models and efficient algorithms are known to achieve information-theoretically optimal error guarantees (see Section 5).

List-decodable mean estimationWhen inliers form a minority, we are in the list-decodable setting and are required to return a list instead of a single estimate. We refer to this setting as \(\operatorname{cor-kLD}\) (_corrupted known list-decoding_). For mixture learning, \(\alpha\) is usually unknown and we need to solve the \(\operatorname{cor-aLD}\) (_corrupted agnostic list-decoding_) problem (i.e., \(\alpha\) is _not provided_, but instead a lower bound \(\alpha_{\text{low}}\in[w_{\text{low}},\alpha]\) is given to the algorithm). Finally, when only additive adversarial contamination is present, as in Eq. (2.1), we recover the standard list-decoding setting studied in prior works (see Section 5) that we call \(\operatorname{sLD}\) (_simple list-decoding_). In Appendix G we show that two algorithms designed for \(\operatorname{sLD}\) also exhibit guarantees for \(\operatorname{cor-kLD}\) for any \(w_{\text{low}}\).

## 3 Main results

We now present our main results for list-decodable and robust mixture learning defined in Section 2. In Section 3.1, we provide algorithmic upper bounds and information-theoretic lower bounds. For the special case of spherical Gaussian mixtures, we show in Section 3.1 that we achieve optimality. Our results are constructive as we provide a meta-algorithm for which these bounds hold.

As depicted in Figure 1, our meta-algorithm (Algorithm 2) is a two-stage process. The outer stage (Algorithm 6) reduces the problem to mean estimation by leveraging the mixture structure and splitting the data into a small collection \(\mathcal{T}\) of sets \(T\). Each set \(T\in\mathcal{T}\) should (i) contain at most one inlier cluster (and few samples from other clusters) and (ii) the total number of outliers across all sets should be at most \(O(\varepsilon n)\). We then run the inner stage (Algorithm 3) on sets \(T\), which outputs a mean estimate for the inlier cluster in \(T\). First, a \(\operatorname{cor-aLD}\) algorithm identifies the weight of the inlier cluster and returns the result of a \(\operatorname{cor-kLD}\) base learner with this weight. Then, if the weight is large, we improve the error via an \(\operatorname{RME}\) base learner. A careful filtering procedure in both stages achieves the significantly reduced list size and better error guarantees. We require the base learners to satisfy the following set of assumptions.

**Assumption 3.1** (Mean-estimation base learners for mixture learning).: Let \(t\) be an even integer and consider the corruption setting defined in Definition B.1. Further, let the inlier distribution \(D(\mu^{*})\in\mathcal{D}\) where \(\mathcal{D}\) is the the family of distributions satisfying Definition 2.1 for \(t\). We assume that 1. for \(\alpha\in[w_{\mathrm{low}},1/3]\) in the \(\mathrm{cor}\)-\(\mathrm{kLD}\) regime, there exists an algorithm \(\mathcal{A}_{\mathrm{kLD}}\) that uses \(N_{LD}(\alpha)\) samples and \(T_{LD}(\alpha)\) time to output a list of size bounded by \(1/\alpha^{O(1)}\) that with probability at least \(1/2\) contains some \(\hat{\mu}\) with \(\|\hat{\mu}-\mu^{*}\|\leqslant f(\alpha)\), where \(f\) is non-increasing.
2. for \(\alpha\in[1-\varepsilon_{\mathrm{RME}},1]\), with \(0\leqslant\varepsilon_{\mathrm{RME}}\leqslant 1/2-2w_{\mathrm{low}}^{2}\) in the \(\mathrm{RME}\) regime, there exists an \(\mathrm{RME}\) algorithm \(\mathcal{A}_{R}\) that uses \(N_{R}(\alpha)\) samples and \(T_{R}(\alpha)\) time to output with probability at least \(1/2\) some \(\hat{\mu}\) with \(\|\hat{\mu}-\mu^{*}\|\leqslant g(\alpha)\), where \(g\) is non-increasing.

Note that the sample and time-complexity functions such as \(N_{LD}\) and \(T_{LD}\), might depend on \(t\), for example growing as \(d^{t}\). We emphasize that (i) the guarantees of our meta-algorithm depend on the guarantees of the base learners and (ii) we only require the base learners to work in the well-studied setting with _known_ fraction of inliers. Corollary 3.4 uses known base learners for Gaussian distributions achieving information-theoretically optimal error bounds. There also exists base learners for distributions beyond Gaussians, such as bounded covariance or log-concave distributions, see, e.g. [9].

### Upper bounds for list-decodable mixture learning

Key quantities that appear in our error bounds are the relative proportion of inliers \(\tilde{w}_{i}\) and outliers \(\tilde{\varepsilon}_{i}\):

\[\tilde{w}_{i}=\frac{w_{i}}{w_{i}+\varepsilon+w_{\mathrm{low}}^{2}}\quad\text{ and}\quad\tilde{\varepsilon}_{i}=1-\tilde{w}_{i}.\] (3.1)

These quantities reflect that each set \(T\) in the inner stage contains at most one inlier cluster and a small (\(\lesssim w_{\mathrm{low}}^{2}\)) fraction of points from other inlier clusters. We now present a simplified version of our main result in Theorem 3.3 (see Theorem C.1 for the detailed result) that allows for a more streamlined presentation of the results using the following 'well-behavedness' of \(f\) and \(g\).

**Assumption 3.2**.: Let \(f\), \(g\) be as defined in Assumption 3.1. For some \(C>0\), we assume (i) \(\varepsilon_{\mathrm{RME}}\geqslant 0.01\), (ii) \(\forall x\in(0,1/3]\), \(f(x/2)\leqslant Cf(x)\), and (iii) \(\forall x\in[0.99,1]\), \(g(x-(1-x)^{2})\leqslant Cg(x)\).

We are now ready to state the main result of the paper.

**Theorem 3.3**.: _Let \(d,k\in\mathbb{N}_{+}\), \(w_{\mathrm{low}}\in(0,1/2]\), and \(t\) be an even integer. Let \(\mathcal{X}\) be a \(d\)-dimensional mixture distribution following Eq. (2.1). Let \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) satisfy Assumptions 3.1 and 3.2 for some even \(t\). Further, suppose that \(\|\mu_{i}-\mu_{j}\|\gtrsim\sqrt{t}(1/w_{\mathrm{low}})^{4/t}+f(w_{\mathrm{low}})\) for all \(i\neq j\in[k]\)._

_Then there exists an algorithm that, given \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(N_{LD}(w_{\mathrm{low}})+N_{R}(w_{ \mathrm{low}}))\) i.i.d. samples from \(\mathcal{X}\) as well as \(d\), \(k\), \(w_{\mathrm{low}}\), and \(t\), runs in time \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(T_{LD}(w_{\mathrm{low}})+T_{R}(w_{ \mathrm{low}}))\) and with probability at least \(1-w_{\mathrm{low}}^{O(1)}\) outputs a list \(L\) of size \(|L|\leqslant k+O(\varepsilon/w_{\mathrm{low}})\) where, for each \(i\in[k]\), there exists \(\hat{\mu}\in L\) such that_

\[\|\hat{\mu}-\mu_{i}\|=O\left(\min_{1\leqslant\ell^{\prime}\leqslant t}\sqrt{t ^{\prime}}(1/\tilde{w}_{i})^{1/t^{\prime}}+f(\min(\tilde{w}_{i},1/3))\right).\]

_If the relative weight of the \(i\)-th cluster is large, i.e., \(\tilde{\varepsilon}_{i}\leqslant 0.001\), then the error is further bounded by_

\[\|\hat{\mu}-\mu_{i}\|=O\left(g(\tilde{w}_{i})\right).\]

The proof together with a more general statement, Theorem C.1, can be found in Appendix C.

Note that for a mixture setting with \(k\geqslant 2\), the assumption \(w_{\mathrm{low}}\leqslant 1/k\leqslant 1/2\) is automatically fulfilled. Also, for large weights \(\tilde{w}_{i}\) such that \(\log(1/\tilde{w}_{i})\ll t\), the \(t^{\prime}\) that minimizes \(\sqrt{t^{\prime}}(1/\tilde{w}_{i})^{1/t^{\prime}}\) is smaller than \(t\), and for small weights the minimizer is \(t^{\prime}=t\).

Figure 1: Schematic of the meta-algorithm (Algorithm 2) underlying Theorem 3.3

Gaussian caseFor Gaussian inlier distributions, LD-ME and RME base learners with guarantees for Assumption 3.1 have already been developed in prior work. We can thus readily use them in the meta-algorithm to arrive at the following statement with the relative proportions defined in Eq. (3.1).

**Corollary 3.4** (Gaussian case).: _Let \(d,k,w_{\mathrm{low}}\) and \(t\) be as in Theorem 3.3. Let \(\mathcal{X}\) be as in Eq. (2.1) with \(D_{i}(\mu_{i})=\mathcal{N}(\mu_{i},I_{d})\) with \(\mu_{i}\)'s satisfying \(\|\mu_{i}-\mu_{j}\|\gtrsim\sqrt{\log 1/w_{\mathrm{low}}}\) for all \(i\neq j\in[k]\). There exists an algorithm that for \(t=O(\log 1/w_{\mathrm{low}})\), given \(N=\mathrm{poly}(d^{t},(1/w_{\mathrm{low}})^{t})\) i.i.d. samples from \(\mathcal{X}\) and \(w_{\mathrm{low}}\), runs in \(\mathrm{poly}(N)\) time and outputs a list \(L\) such that with high probability \(|L|=k+O(\varepsilon/w_{\mathrm{low}})\) and, for all \(i\in[k]\), there exists \(\hat{\mu}\in L\) such that_

\[\|\hat{\mu}-\mu_{i}\|=O\left(\sqrt{\log 1/\hat{w}_{i}}\right)\,.\]

_If the relative weight of the \(i\)-th cluster is large, i.e. \(\tilde{\varepsilon}_{i}\leqslant 0.001\), then the error is further bounded by_

\[\|\hat{\mu}-\mu_{i}\|=O\left(\tilde{\varepsilon}_{i}\sqrt{\log 1/\tilde{ \varepsilon}_{i}}\right).\]

Proof.: Theorem 6.12 from [5] provides an LD-ME algorithm \(\mathcal{A}_{\mathrm{kLD}}\) achieving error \(f(\alpha)\leqslant O(\sqrt{\tilde{t}^{\prime}}(1/\alpha)^{1/t^{\prime}})\) for all \(t^{\prime}\leqslant t\). The sample and time complexity scale as \(\mathrm{poly}(d^{t},(1/\alpha)^{t})\). Also, Theorem 5.1 from [10] provides a robust mean estimation algorithm \(\mathcal{A}_{R}\) such that for a small enough constant fraction of outliers \(\varepsilon=1-\alpha\) it achieves error \(g(\alpha)=O((1-\alpha)\sqrt{\log 1/(1-\alpha)})\) with sample complexity \(\tilde{\Omega}(d/\varepsilon^{2})\). Using these \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\), we recover the desired bounds. 

Comparison with prior workWe now compare our result with the only previous method that can achieve guarantees in the LD-ML setting with unknown \(w_{i}\). As discussed in [3], algorithms for the simple list-decoding model with \(\alpha=w_{\mathrm{low}}\) can be used for LD-ML by viewing a single mixture component as the "ground truth" distribution and effectively treating all other inlier components and original outliers as outliers. Besides requiring a much larger list size of \(O(1/w_{\mathrm{low}})\gg k+O(\varepsilon/w_{\mathrm{low}})\) and error \(O(\sqrt{\log 1/w_{\mathrm{low}}})\), this approach has two drawbacks that manifest in the suboptimal guarantees: 1) For larger clusters \(i\) with \(w_{i}\gg w_{\mathrm{low}}\), LD-ME only achieves an error \(O\left(\sqrt{\log 1/w_{\mathrm{low}}}\right)\).

Our result, even without separation assumption, achieves a sharper error bound \(O\left(\sqrt{\log 1/w_{i}}\right)\). 2) When the mixture is separated, LD-ME cannot exploit the structure since it still models the data as \(w_{\mathrm{low}}\mathcal{N}(\mu_{i},I_{d})+(1-w_{\mathrm{low}})Q\) for each \(i\), so that the algorithm inevitably treats all other true components as outliers. This results in the error \(O\left(\sqrt{\log 1/w_{\mathrm{low}}}\right)\gg O\left(\sqrt{\log 1/\tilde{w}_{i}} \right)=O(1)\) (when \(\varepsilon\sim w_{i}\ll 1\)). We refer to Appendix A for further illustrative examples. As a simple example, consider the uniform inlier mixture with \(\varepsilon=w_{i}=1/(k+1)\), where \(k\) is large. In this case, previous results have error guarantees \(O(\sqrt{\log k})\), while we obtain error \(O(1)\).

Separation assumptionFor the problem of learning mixture models, a separation assumption is common in the literature [3, 9, 11, 12]. We require separation \(\|\mu_{i}-\mu_{j}\|\gtrsim\sqrt{t}(1/w_{\mathrm{low}})^{4/t}\), which we believe to be sub-optimal for the case of finite \(t\). In cases when \(\varepsilon\) is small (namely \(\varepsilon\lesssim w_{\mathrm{low}}\)), there exist prior works on clustering allowing smaller separation. Specifically, when \(t=2\), a recent work [13] only requires \(\|\mu_{i}-\mu_{j}\|\gtrsim 1/\sqrt{w_{\mathrm{low}}}\). For a general \(t\geqslant 4\), [9] succeeds under separation \((1/w_{\mathrm{low}})^{2/t}\). We leave the possible relaxation of the separation requirement in the case of general \(t\) and large \(\varepsilon\) for the future work.

In the Gaussian case we require separation \(\|\mu_{i}-\mu_{j}\|\gtrsim\sqrt{\log 1/w_{\mathrm{low}}}\), which is optimal in the uniform (\(w_{i}=1/k\)) case. Indeed, without the separation assumption, even in the _noiseless_ uniform Gaussian case, [6] shows that no efficient algorithm can obtain error asymptotically better than \(\Omega(\sqrt{\log 1/w_{i}})\). In Corollary B.5, we prove that the inner stage (Algorithm 3) of our algorithm, without knowledge of \(w_{i}\) and separation assumption, achieves with high probability matching error guarantees \(O(\sqrt{\log 1/w_{i}})\) with list size bounded by \(O(1/w_{\mathrm{low}})\).

### Information-theoretical lower bounds and optimality

Next, we present information-theoretical lower bounds for list-decodable mixture learning on well-separated distributions \(\mathcal{X}\) as defined in Eq. (2.1). We show that our error is optimal as long as the list size is required to be small. Our proof uses a simple reduction technique and leverages established lower bounds in [3] for the list-decodable mean estimation model (\(\mathrm{sLD}\) in Section 2).

**Proposition 3.5** (Information-theoretic lower bounds).: _Let \(\mathcal{A}\) be an algorithm that, given access to \(\mathcal{X}\), outputs a list \(L\) that, with probability \(\geqslant 1/2\), for each \(i\in[k]\) contains \(\hat{\mu}\in L\) with \(\|\hat{\mu}-\mu_{i}\|\leqslant\beta_{i}\)._

1. _Consider the case with_ \(\|\mu_{i}-\mu_{j}\|\gtrsim(1/w_{\mathrm{low}})^{4/t}\) _for_ \(i\neq j\in[k]\)_,_ \(D_{i}(\mu_{i})\) _having_ \(t\)_-th bounded sub-Gaussian central moments and_ \(\beta_{i}\leqslant C(1/w_{\mathrm{low}})^{1/t}\) _for each_ \(i\in[k]\)_. If for some_ \(s\in[k]\) _it holds that_ \(w_{s}\leqslant\varepsilon\)_, then algorithm_ \(\mathcal{A}\) _must either have error bound_ \(\beta_{s}=\Omega((1/\tilde{w}_{i})^{1/t})\) _or_ \(|L|\geqslant k+d-1\)_._
2. _Consider the case with_ \(\|\mu_{i}-\mu_{j}\|\gtrsim\sqrt{\log 1/w_{\mathrm{low}}}\) _for_ \(i\neq j\in[k]\)_,_ \(D_{i}(\mu_{i})=\mathcal{N}(\mu_{i},I_{d})\) _and_ \(\beta_{i}\leqslant C\sqrt{\log 1/w_{\mathrm{low}}}\) _for each_ \(i\in[k]\)_. If for some_ \(s\in[k]\) _it holds that_ \(w_{s}\leqslant\varepsilon\)_, then algorithm_ \(\mathcal{A}\) _must either have error bound_ \(\beta_{s}=\Omega(\sqrt{\log 1/\tilde{w}_{i}})\) _or_ \(|L|\geqslant k+\min\{2^{\Omega(d)},(1/\tilde{w}_{i})^{\omega(1)}\}\)_._

In the Gaussian inlier case, Corollary 3.4 together with Proposition 3.5 imply optimality of our meta-algorithm. Indeed, if one plugs in optimal base learners (as in the proof of Corollary 3.4), we obtain error guarantee that matches lower bound. In particular, "exponentially" larger list size is necessary for asymptotically smaller error. For inlier components with bounded sub-Gaussian moments, [3] obtains information-theoretically (nearly-)optimal LD-ME base learners.

Furthermore, in [3], formal evidence of computational hardness was obtained (see their Theorem 5.7, which gives a lower bound in the statistical query model introduced by [14]) that suggests obtaining error \(\Omega_{t}((1/\tilde{w}_{s})^{1/t})\) requires running time at least \(d^{\Omega(t)}\). This was proved for Gaussian inliers and the running time matches ours up to a constant in the exponent.

## 4 Algorithm sketch

We now sketch our meta-algorithm specialized to the case of separated Gaussian components \(\mathcal{N}(\mu_{i},I_{d})\) and provide intuition for how it achieves the guarantees in Corollary 3.4. In this section, we only discuss how to obtain an error of \(O(\sqrt{\log 1/\tilde{w}_{i}})\) for each mean when \(\varepsilon\gtrsim\min_{i}w_{i}\). We refer to Appendix D for how to achieve the refined error guarantee of \(O(\tilde{\varepsilon}_{i}\sqrt{\log 1/\tilde{\varepsilon}_{i}})\) when \(\tilde{\varepsilon}_{i}\) is small.

As discussed in Section 3.1, running an out-of-the-box LD-ME algorithm for the sLD problem on our input with parameter \(\alpha=w_{\mathrm{low}}\) would give sub-optimal guarantees. In contrast, our two-stage Algorithm 2, equipped with the appropriate \(\mathrm{cor}\)-\(\mathrm{kLD}\) and \(\mathrm{RME}\) base learners as depicted in Figure 1, obtains for each component an error guarantee that is as good as if we had access to the samples _only_ from this component and from the outliers. We now give more details about the outer stage, Algorithm 1, and inner stage, Algorithm 3, and describe on a high-level how they contribute to a short output list with optimal error bound in Corollary 3.4 for large outlier fractions.

### Inner stage: list-decodable mean estimation with unknown inlier fraction

We now describe how to use a black-box \(\mathrm{cor}\)-\(\mathrm{kLD}\) algorithm to obtain a list-decoding algorithm \(\mathcal{A}_{\mathrm{aLD}}\) for the \(\mathrm{cor}\)-\(\mathrm{aLD}\) mean-estimation setting with access only to \(\alpha_{\mathrm{low}}\leqslant\alpha\). \(\mathcal{A}_{\mathrm{aLD}}\) is usedin the proof of Corollary B.5 and plays a crucial role (see Figure 1) in our meta-algorithm. In particular, it deals with the unknown weight of the inlier distribution in each set returned by the outer stage. Note that estimating \(\alpha\) from the input samples is impossible by nature. Indeed, we cannot distinguish between potential outlier clusters of arbitrary proportion \(\leqslant 1-\alpha\) and the inlier component. Undererestimating the size of a large component would inevitably lead to a suboptimal error guarantee. We now show how to overcome this challenge and achieve an error guarantee \(O(\sqrt{\log 1/\alpha})\) for a list size \(1+O((1-\alpha)/\alpha_{\mathrm{low}})\) for the \(\mathrm{cor}\)-aLD setting. Here we only outline our algorithm and refer to Appendix D for the details.

Algorithm 3 first produces a large list of estimates corresponding to many potential values of \(\alpha\) and then prunes it while maintaining a good estimate in the list. In particular, for each \(\hat{\alpha}\in A\coloneqq\{\alpha_{\mathrm{low}},2\alpha_{\mathrm{low}}, \ldots,\lfloor 1/(3\alpha_{\mathrm{low}})\rfloor\alpha_{\mathrm{low}}\}\), we run \(\mathcal{A}_{\mathrm{kLD}}\) with parameter \(\hat{\alpha}\) to obtain a list of means. We append \(\hat{\alpha}\) to each mean in the list and obtain a list of pairs \((\hat{\mu},\hat{\alpha})\). We concatenate these lists of pairs for all \(\hat{\alpha}\) and obtain a list \(L\) of size \(O(1/\alpha_{\mathrm{low}}^{2})\). By design, one element of \(A\) is close to the true \(\alpha\), so the list \(L\) contains at least one \(\hat{\mu}\) that is \(O(\sqrt{\log 1/\alpha})\)-close -- the error guarantee that we aim for -- and there is indeed at least an \(\alpha\)-fraction of samples near \(\hat{\mu}\). We call such a hypothesis "nearby".

Finally, we prune this concatenated list by verifying for each \(\hat{\mu}\) whether there is indeed an \(\hat{\alpha}\)-fraction of samples "not too far" from it. This is similar to pruning procedures with known \(\alpha\) proposed in prior work (see Proposition B.1 in [3]). Our procedure (i) never discards a "nearby" hypothesis, and outputs a list where (ii) every hypothesis contains a sufficient number of points close to it and (iii) all hypotheses are separated. Property (i) implies that the final error is \(O(\sqrt{\log 1/\alpha})\) and properties (ii) and (iii) imply list size bound \(1+O((1-\alpha)/\alpha_{\mathrm{low}})\). Note that when \(\alpha<\alpha_{\mathrm{low}}\), the list size can be simply upper bounded by \(O(1/\alpha_{\mathrm{low}})\), see Remark B.4.

### Two-stage meta-algorithm

Even though we could run \(\mathcal{A}_{\mathrm{aLD}}\) on the entire dataset with \(\alpha_{\mathrm{low}}=w_{\mathrm{low}}\), we would only achieve an error for the \(i^{\mathrm{th}}\) inlier cluster mean of \(O(\sqrt{\log 1/w_{i}})\) - which can be much larger than \(O(\sqrt{\log 1/\tilde{w}_{i}})\) - for a list of size \(O(1/w_{\mathrm{low}})\). While \(\mathcal{A}_{\mathrm{aLD}}\) takes into account the unknown weight of the clusters, it still treats other inlier clusters as outliers. We now show that if the outer stage Algorithm 1 of our meta-algorithm Algorithm 2 separates the samples into a not-too-large collection \(\mathcal{T}\) of sets with certain properties, running \(\mathcal{A}_{\mathrm{aLD}}\) separately on each of the sets can lead to the desired guarantees. In particular, let us assume that \(\mathcal{T}\) consists of potentially overlapping sets such that:

1. For each inlier cluster \(C^{*}\), there exists one set \(T\in\mathcal{T}\) such that \(T\) contains (almost) all points from \(C^{*}\) and at most \(O(\varepsilon n)\) other points,
2. It holds that \(\sum_{T\in\mathcal{T}}|T|\leqslant n+O(\varepsilon n)\).

By (1), for every inlier cluster \(C^{*}\) with a corresponding true weight \(w^{*}\), there exists a set \(T\) such that the points from \(C^{*}\) constitute at least an \(\tilde{w}\)-fraction of \(T\) with \(\tilde{w}:=\Omega(w^{*}/(w^{*}+\varepsilon))\). By Section 4.1, applying \(\mathcal{A}_{\mathrm{aLD}}\) with \(\alpha_{\mathrm{low}}=w_{\mathrm{low}}\cdot n/\left|T\right|\) on such a \(T\) then yields a list of size \(1+O((1-\tilde{w})/w_{\mathrm{low}})\) with an estimation error at most \(O(\sqrt{\log 1/\tilde{w}})\). If \(T\) contains (almost) no inliers, that is, there is no inlier component that should recovered, then \(\mathcal{A}_{\mathrm{kLD}}\) returns a list of size \(O(|T|/(w_{\mathrm{low}}n))\).

Now, by the two properties, (almost) all inlier points lie in at most \(k\) sets of \(\mathcal{T}\), and all other sets of \(\mathcal{T}\) contain in total at most \(O(\varepsilon n)\) points. Hence, concatenating all lists outputted by \(\mathcal{A}_{\mathrm{aLD}}\) applied to all \(T\in\mathcal{T}\) leads to a final list size bounded by \(k+O(\varepsilon/w_{\mathrm{low}})\).

### Outer stage: separating inlier clusters

We now informally describe the outer stage that produces the collection of sets \(\mathcal{T}\) with the desiderata described in Section 4.2, leaving the details to Appendix E. The main steps are outlined in pseudocode in Algorithm 1.

Given a set \(X\) of \(N=\mathrm{poly}(d^{t},1/w_{\mathrm{low}})\) i.i.d. input samples from the distribution Eq. (2.1) with Gaussian inlier components, the first step of the meta-algorithm is to run Algorithm 1 on \(X\) and \(w_{\mathrm{low}}\) with \(\Delta=O(\sqrt{\log 1/w_{\mathrm{low}}})\). Algorithm 1 runs an sLD algorithm on the samples and produces a (large) list of estimates \(L\) such that, for each mean, at least one estimate is \(O(\sqrt{\log 1/w_{\mathrm{low}}})\)-close to it. It then add sets to \(\mathcal{T}\) that correspond to these estimates via a dynamic "two-scale" process.

Specifically, for each \(\hat{\mu}\in L\), we construct _two sets_\(S^{(1)}_{\hat{\mu}}\subseteq S^{(2)}_{\hat{\mu}}\) consisting of samples close to \(\hat{\mu}\). By construction, we guarantee that if \(S^{(1)}_{\hat{\mu}}\) contains a non-negligible fraction of samples from any inlier cluster \(C^{*}\), then \(S^{(2)}_{\hat{\mu}}\) contains (almost) all samples from \(C^{*}\) (see Theorem B.7 (ii)).

Now we very briefly illustrate how this process could be helpful in proving properties (1) and (2). Observe that, as long as there exists some \(\hat{\mu}\) with \(|S^{(2)}_{\hat{\mu}}|\leqslant 2|S^{(1)}_{\hat{\mu}}|\), we add \(S^{(2)}_{\hat{\mu}}\) to \(\mathcal{T}\) and remove the samples from \(S^{(1)}_{\hat{\mu}}\). Consider one such \(\hat{\mu}\). For property (1), we merely note that if \(S^{(1)}_{\hat{\mu}}\) contains a part of an inlier cluster \(C^{*}\), then \(S^{(2)}_{\hat{\mu}}\) contains (almost) all of \(C^{*}\), so we add to \(\mathcal{T}\) a set that contains (almost) all of \(C^{*}\); otherwise, when we remove \(S^{(1)}_{\hat{\mu}}\) we remove (almost) no points from \(C^{*}\), so (almost) all the points from \(C^{*}\) remain in play. For property (2), we merely note that whenever we add \(S^{(2)}_{\hat{\mu}}\) to \(\mathcal{T}\), increasing the number of points in it by \(|S^{(2)}_{\hat{\mu}}|\), we also remove the samples from \(S^{(1)}_{\hat{\mu}}\), reducing the number of samples by \(|S^{(1)}_{\hat{\mu}}|\geqslant|S^{(2)}_{\hat{\mu}}|/2\). The proof of the properties uses some additional arguments of a similar flavor, and we defer it to Appendix E.

## 5 Related work

List-decodable mean estimationInspired by the list-decoding paradigm that was first introduced for error-correcting codes for large error rates [15], list-decodable mean estimation has become a popular approach for robustly learning the mean of a distribution when the majority of the samples are outliers. A long line of work has proposed efficient algorithms with theoretical guarantees. These algorithms are either based on convex optimization [9; 16], a filtering approach [3; 17], or low-dimensional projections [18]. Near-linear time algorithms were obtained in [19] and [8]. The list-decoding paradigm is not only used for mean estimation but also other statistical inference problems. Examples include sparse mean estimation [20; 21], linear regression [22; 23; 24], subspace recovery [25; 26], clustering [27], stochastic block models and crowd sourcing [16; 28].

Robust mean estimation and mixture learningWhen the outliers constitute a minority, algorithms typically achieve significantly better error guarantees than in the list-decodable setting. Robust mean estimation algorithms output a single vector close to the mean of the inliers. In a variety of corruption models, efficient algorithms are known to achieve (nearly) optimal error

Robust mixture learning tackles the model in Eq. (2.1) with \(\varepsilon\ll\min_{i}w_{i}\) and aims to output exactly \(k\) vectors with an accurate estimate for the population mean of each component [3; 4; 7; 9; 11; 13; 29; 30]. These algorithms do not enjoy error guarantees for clusters with weights \(w_{i}<\varepsilon\). To the best of our knowledge, our algorithm is the first to achieve non-trivial guarantees in this larger noise regime.

Robust clusteringRobust clustering [31] also addresses the presence of small fractions of outliers in a similar spirit to robust mixture learning, conceptually implemented in the celebrated DBScan algorithm [32]. Assuming the output list size is large enough to capture possible outlier clusters, these methods may also be used to tackle list-decodable mixture learning - however, they do not come with an inherent procedure to determine the right choice of hyperparameters that ultimately output a list size that adapts to the problem.

## 6 Discussion and future work

In this work, we prove that even when small groups are outnumbered by adversarial data points, efficient list-decodable algorithms can provide an accurate estimation of all means with minimal list size. The proof for the upper bound is constructive and analyzes a plug-and-play meta-algorithm (cf. Figure 1) that inherits guarantees of the black-box \(\mathrm{cor}\)-\(\mathrm{kLD}\) algorithm \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathrm{RME}\) algorithm \(\mathcal{A}_{R}\), which it uses as base learners. Notably, when the inlier mixture is a mixture of Gaussians with identity covariance, we achieve optimality. Furthermore, any new development for the base learners automatically translates to improvements in our bounds.

We would like to end by discussing the possible practical impact of this result. Since an extensive empirical study is out of the scope of this paper, besides the fact that ground-truth means for unsupervised real-world data are hard to come by, we provide preliminary experiments on synthetic data. Specifically, we generate data from a separated \(k-\)Gaussian mixture with additive contaminations as in Eq. (2.1) and different types of adversarial distributions (see detailed description in Appendix I). We focus on the regime \(\varepsilon\sim w_{i}\) where our algorithms shows the largest theoretical improvements.

We then compare the output of our algorithm with the vanilla LD-ME algorithm from [8] with \(w_{\mathrm{low}}=0.02\) and (suboptimal) LD-ML guarantees as well as well-known (robust) clustering heuristics without LD-ML guarantees, such as the \(k\)-means [33], Robust \(k\)-means [34], and DBSCAN [32]. Even though none of these heuristics have LD-ML guarantees, they are commonly used and known to also perform well in practice in noisy settings. In Figure 2 (left), we fix the list size to \(10\) and plot the errors for the worst inlier cluster, typically the smallest. We compare the performance of the algorithms by plotting the worst-case estimation errors for a given list size and list sizes that algorithms require to achieve a given worst-case estimation error. In Figure 2 (right), we fix the error and plot the minimal list size at which competing algorithms reach the same or smaller worst estimation error. Further details on the experiments are provided in Appendix I. In a different experiment (see Figure 3 and Appendix I.1 for details), we observe that our approach outperforms LD-ME when \(w_{\mathrm{low}}\) varies, both in achieving smaller list size and smaller estimation error.

Overall, in line with our theory, our method significantly outperforms the LD-ME algorithm, and performs better or on par with the heuristic approaches. Additional experimental comparison and implementation details can be found in Appendix I. Even though these experiments do not allow conclusive statements about the improvement of our algorithm for mixture learning for real-world data, they do provide encouraging evidence that effort could be well-spent on follow-up empirical and theoretical work building on our results. For example, it would be interesting to conduct a more extensive empirical study comparing our algorithm with a variety of robust clustering algorithms. Additionally, practical data often contains components with varying scales. An interesting direction for future work could be to extend our algorithm to handle differently scaled covariances in an agnostic manner.

Figure 3: Comparison of list size and estimation error for large inlier cluster for varying \(w_{\mathrm{low}}\) inputs. The experimental setup is illustrated in Appendix I. We plot the median values with error bars showing \(25\)th and \(75\)th quantiles. As \(w_{\mathrm{low}}\) decreases, we observe a roughly constant estimation error for our algorithm while the error for LD-ME increases. Further, the decrease in list size is much more severe for LD-ME than for our algorithm.

Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing \(25\)th and \(75\)th percentile.

## Acknowledgements

DD is supported by ETH AI Center doctoral fellowship and ETH Foundations of Data Science initiative. RB, ST, GN, and DS have received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 815464).

## References

* [1] David R Bickel. Robust cluster analysis of microarray gene expression data with the number of clusters determined biologically. _Bioinformatics_, 19(7):818-824, 2003.
* [2] Eric D Feigelson and G Jogesh Babu. Statistical methods for astronomy. _arXiv preprint arXiv:1205.2064_, 2012.
* [3] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical Gaussians. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1047-1060, 2018.
* [4] Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1234-1247, 2022.
* [5] Ilias Diakonikolas and Daniel M Kane. _Algorithmic high-dimensional robust statistics_. Cambridge University Press, 2023.
* [6] Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 85-96. IEEE, 2017.
* [7] Misha Ivkov and Pravesh K Kothari. List-decodable covariance estimation. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1276-1283, 2022.
* [8] Ilias Diakonikolas, Daniel M Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. Clustering mixture models in almost-linear time via list-decodable mean estimation. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1262-1275, 2022.
* [9] Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1035-1046, 2018.
* [10] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high-dimensions without the computational intractability. _SIAM Journal on Computing_, 48(2):742-864, 2019.
* [11] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1021-1034, 2018.
* [12] Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1248-1261, 2022.
* [13] Ilias Diakonikolas, Daniel M Kane, Jasper CH Lee, and Thanasis Pittas. Clustering mixtures of bounded covariance distributions under optimal separation. _arXiv preprint arXiv:2312.11769_, 2023.
* [14] Michael Kearns. Efficient noise-tolerant learning from statistical queries. _Journal of the ACM (JACM)_, 45(6):983-1006, 1998.
* [15] Peter Elias. List decoding for noisy channels. Technical report, Research Laboratory of Electronics, Massachusetts Institute of Technology, 1957.

* [16] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, pages 47-60, 2017.
* [17] Ilias Diakonikolas, Daniel Kane, and Daniel Kongsgaard. List-decodable mean estimation via iterative multi-filtering. _Advances in Neural Information Processing Systems_, 33:9312-9323, 2020.
* [18] Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. List-decodable mean estimation in nearly-PCA time. _Advances in Neural Information Processing Systems_, 34:10195-10208, 2021.
* [19] Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau. List decodable mean estimation in nearly linear time. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 141-148. IEEE, 2020.
* [20] Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Ankit Pensia, and Thanasis Pittas. List-decodable sparse mean estimation via difference-of-pairs filtering. _Advances in Neural Information Processing Systems_, 35:13947-13960, 2022.
* [21] Shiwei Zeng and Jie Shen. List-decodable sparse mean estimation. _Advances in Neural Information Processing Systems_, 35:24031-24045, 2022.
* [22] Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regression. _Advances in Neural Information Processing Systems_, 32, 2019.
* [23] Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 161-180. SIAM, 2020.
* [24] Ilias Diakonikolas, Daniel Kane, Ankit Pensia, Thanasis Pittas, and Alistair Stewart. Statistical query lower bounds for list-decodable linear regression. _Advances in Neural Information Processing Systems_, 34:3191-3204, 2021.
* [25] Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 1279-1297. SIAM, 2021.
* [26] Prasad Raghavendra and Morris Yau. List decodable subspace recovery. In _Conference on Learning Theory_, pages 3206-3226. PMLR, 2020.
* [27] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for clustering via similarity functions. In _Proceedings of the fortieth annual ACM symposium on Theory of computing_, pages 671-680, 2008.
* [28] Michela Meister and Gregory Valiant. A data prism: Semi-verified learning in the small-alpha regime. In _Conference On Learning Theory_, pages 1530-1546. PMLR, 2018.
* [29] Ainesh Bakshi, Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, Sushrut Karmalkar, and Pravesh K Kothari. Outlier-robust clustering of Gaussians and other non-spherical mixtures. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 149-159. IEEE, 2020.
* [30] Allen Liu and Ankur Moitra. Settling the robust learnability of mixtures of Gaussians. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 518-531, 2021.
* [31] Luis Angel Garcia-Escudero, Alfonso Gordaliza, Carlos Matran, and Agustin Mayo-Iscar. A review of robust clustering methods. _Advances in Data Analysis and Classification_, 4:89-109, 2010.
* [32] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In _kdd_, pages 226-231, 1996.

* [33] Stuart Lloyd. Least squares quantization in pcm. _IEEE transactions on information theory_, pages 129-137, 1982.
* [34] Christian Brownlees, Emilien Joly, and Gabor Lugosi. Empirical risk minimization for heavy-tailed losses. _The Annals of Statistics_, 43(6):2507-2536, 2015.
* [35] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a Gaussian: Getting optimal error, efficiently. In Artur Czumaj, editor, _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018_, pages 2683-2702. SIAM, 2018.

Examples

\(k\) **inlier cluster, \(c\) outlier clusters.**One tricky adversarial distribution is the Gaussian mixture model itself. In particular, we consider

\[\mathcal{X}_{c}=\frac{k}{k+c}\sum_{i=1}^{k}\frac{1}{k}\mathcal{N}(\mu_{i},I)+ \frac{c}{k+c}\sum_{i=1}^{c}\frac{1}{c}\mathcal{N}(\tilde{\mu}_{i},I),\] (A.1)

where the first \(k\) Gaussian components are inliers and \(Q\) is a GMM with \(c\) components, which we call _fake_ clusters. Since all inlier cluster weights are identical, we denote \(w\coloneqq w_{i}=1/(k+c)\). Assume that \(1\ll c\ll k\), which corresponds to \(\varepsilon\gg w\). Then, relative weights are \(\bar{w}=1/(c+1)\approx 1/c\). Due to large adversary, previous results on learning GMMs cannot be applied, leaving vanilla list-decodable learning. However, the latter also cannot guarantee anything better that \(\Omega(\sqrt{t}k^{1/t})\) even with the knowledge of \(k\), as long as list size is \(O(k+c)\), which can be much worse than our guarantees of \(O(\sqrt{t}c^{1/t})\) for the same list size.

Their drawback is that they do not utilize separation between true clusters, i.e., for each \(i\), they model the data as

\[\mathcal{X}=\frac{1}{k+c}\mathcal{N}(\mu_{i},I)+\left(1-\frac{1}{k+c}\right)Q.\]

where \(Q\) can be "arbitratily adversarial" for recovering \(\mu_{i}\).

Big + small inlier clustersConsider the mixture

\[\mathcal{X}_{b}=(1-w-\varepsilon)\,\mathcal{N}(\mu_{1},I_{d})+w\mathcal{N}( \mu_{2},I_{d})+\varepsilon Q,\] (A.2)

where \(\|\mu_{1}-\mu_{2}\|=\Omega(\sqrt{\log 1/w})\), \(w\ll\varepsilon\ll 1\), and \(Q\) is chosen adversarially. In this example we have two inlier clusters, one with large weight \(\approx 1\) and another with small weight \(w\). Adversarial distribution \(Q\) has large weight relative to the small cluster, but still negligible weight compared to the large one.

Previous methods would either (i) recover large cluster with optimal error \(O(\varepsilon)\) (see, e.g., [35]) but miss out small cluster or (ii) recover both clusters using list-decodable mean estimation with known \(\alpha=w\), but with suboptimal errors \(O(\sqrt{\log 1/w})\) and list size \(O(1/w)\). In contrast, Corollary 3.4 guarantees list size at most \(1+O(\varepsilon/w)\), error \(O(\sqrt{\log\varepsilon/w})\) for the small cluster, and error \(O(\varepsilon\sqrt{\log 1/\varepsilon})\) for the larger. In general, we achieve (i) optimal errors for both clusters and (ii) optimal (up to constants) list size.

## Appendix B Inner and outer stage algorithms and guarantees

Our meta-algorithm Algorithm 2 assumes black-box access to a list-decodable mean estimation algorithm and a robust mean estimation algorithm for sub-Gaussian (up to the \(t^{\mathrm{th}}\) moment) distributions. From these we obtain stronger mean estimation algorithms when the fraction of outliers is unknown, and finally stronger algorithms for learning separated mixtures when the fraction of outliers can be arbitrarily large. Our algorithm achieves guarantees with polynomial runtime and sample complexity if the black-box learners achieve the guarantees for their corresponding mean estimation setting. In this section we discuss the corruption model and inner and out stage of the meta-algorithm in detail and prove properties needed for the proof of the main Theorem 3.3.

### Detailed setting

In order to achieve these guarantees, our black-box algorithms need to work under a model in which an adversary is allowed to remove a small fraction of the inliers and to add arbitrarily many outliers. In our proofs, for simplicity of exposition, we require the algorithms to have mean estimation guarantees for a small adversarially removed fraction of \(w_{\mathrm{low}}^{2}\). Formally, the corruption model as defined as follows.

**Definition B.1** (Corruption model).: Let \(d\in\mathbb{N}_{+}\), and \(\alpha\in[w_{\mathrm{low}},1]\). Let \(D\) be a \(d\)-dimensional distribution. An input of size \(n\) according to our corruption model is generated as follows:* Draw a set \(C^{*}\) of \(n_{1}=\lceil\alpha n\rceil\) i.i.d. samples from the distribution \(D\).
* An adversary is allowed to arbitrarily remove \(\lfloor w_{\mathrm{low}}^{2}n_{1}\rfloor\) samples from \(C^{*}\). We refer to the resulting set as \(S^{*}\) with size \(n_{2}=|S^{*}|\).
* An adversary is allowed to add \(n-n_{2}\) arbitrary points to \(S^{*}\). We refer to the resulting set as \(S_{\text{adv}}\) with size \(n_{3}=|S_{\text{adv}}|\).
* If \(n_{3}<n\), pad \(S_{\text{adv}}\) with \(n-n_{3}\) arbitrary points and call the resulting set \(S\).
* Return \(S\).

We call \(\mathrm{cor}\text{-}\mathrm{kLD}\) the model when \(w_{\mathrm{low}}\) and \(\alpha\) are given to the algorithm and \(\mathrm{cor}\text{-}\mathrm{aLD}\) the model when \(w_{\mathrm{low}}\) and lower bound \(\alpha_{\mathrm{low}}\geqslant w_{\mathrm{low}}\) are given to the algorithm, such that \(\alpha\geqslant\alpha_{\mathrm{low}}\). Note that \(\alpha\) is **not** provided in \(\mathrm{cor}\text{-}\mathrm{aLD}\) model.

Note that in Definition B.1\(|S|=n\) and \(S^{*}\) constitutes at least an \(\alpha(1-w_{\mathrm{low}}^{2})\)-fraction of \(S\).

### Inner stage algorithm and guarantees

The algorithm consists of three steps: (1) Constructing a list of hypotheses, (2) Filtering the hypotheses, and (3) Improving the hypotheses if \(\alpha\geqslant 1-\varepsilon_{\mathrm{RME}}\). For convenience, we restate the \(\mathrm{InnerStage}\) algorithm introduced in the main text.

**Theorem B.2** (Inner stage guarantees).: _Let \(d\in\mathbb{N}_{+}\), \(w_{\mathrm{low}}\in(0,10^{-4}]\), \(w_{\mathrm{low}}\leqslant\alpha_{\mathrm{low}}\leqslant\alpha\leqslant 1\), and \(t\) be an even integer. Let \(D(\mu^{*})\) be a \(d\)-dimensional distribution with mean \(\mu^{*}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments._

_Consider the \(\mathrm{cor}\text{-}\mathrm{aLD}\) corruption model in Definition B.1 with parameters \(d\), \(w_{\mathrm{low}}\), \(\alpha\) and distribution \(D=D(\mu^{*})\). Let \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) satisfy Assumption 3.1 with high success probability (see Remark B.3)._

_Then \(\mathrm{InnerStage}\) (Algorithm 3), given an input of \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(N_{LD}(w_{\mathrm{low}})+N_{R}(w_{ \mathrm{low}}))\) samples from the \(\mathrm{cor}\text{-}\mathrm{aLD}\) corruption model, and access to the parameters \(d\), \(w_{\mathrm{low}}\), \(\alpha_{\mathrm{low}}\)and \(t\), runs in time \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(T_{LD}(w_{\mathrm{low}})+T_{R}(w_{ \mathrm{low}}))\) and outputs a list \(L\) of size \(|L|\leqslant 1+O((1-\alpha)/\alpha_{\mathrm{low}})\) such that, with probability \(1-w_{\mathrm{low}}^{O(1)}\),_

1. _There exists_ \(\hat{\mu}\in L\) _such that_ \[\|\hat{\mu}-\mu^{*}\|\leqslant O(\psi_{t}(\alpha/4)+f(\alpha/4)).\]
2. _If_ \(\alpha\geqslant 1-\varepsilon_{\mathrm{RME}}\)_, then there exists_ \(\hat{\mu}\in L\) _such that_ \[\|\hat{\mu}-\mu^{*}\|\leqslant O(g(\alpha-w_{\mathrm{low}}^{2})).\]

Proof of Theorem B.2 can be found in Appendix D.

**Remark B.3**.: _For any \(r\in\mathbb{N}\), we can increase probabilities of success of \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) from \(1/2\) to \(1-2^{-r}\) in the following way: we increase number of samples by a factor of \(r\), randomly split \(S\) into \(r\) subsets of equal size, apply \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) to these subsets and concatenate their outputs. In the proofs we assume that the success probabilities are \(1-w_{\mathrm{low}}^{C}\) for large enough constant \(C\). This increases the size of the list returned by \(\mathcal{A}_{\mathrm{kLD}}\), the number of samples, and the running time by a factor \(O(\log(1/w_{\mathrm{low}}))\). In particular, we assume that the size of the list returned by \(\mathcal{A}_{\mathrm{kLD}}\) is much smaller than the inverse failure probability._

**Remark B.4**.: _In the execution of the meta-algorithm, it may happen that Algorithm 3 is run on set \(T\) with almost no inliers, i.e., \(\alpha<\alpha_{\mathrm{low}}\). We note that from the analysis (see Appendix D, or [3], Proposition B.1), we always have upper bound \(|L|=O(1/\alpha_{\mathrm{low}})\)._An immediate consequence of Theorem B.2 are the following guarantees of directly applying Algorithm 3 to the mixture learning case with no separation. Here we present upper bounds for Algorithm 3, when no separation assumptions are imposed.

**Corollary B.5**.: _Let \(d,k\in\mathbb{N}_{+}\), \(w_{\mathrm{low}}\in(0,10^{-4}]\), and \(t\) be an even integer. For all \(i=1,\ldots,k\), let \(D_{i}(\mu_{i})\) be a \(d\)-dimensional distribution with mean \(\mu_{i}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments. Let \(\varepsilon>0\) and, for all \(i=1,\ldots,k\), let \(w_{i}\in[w_{\mathrm{low}},1]\), such that \(\sum_{i=1}^{k}w_{i}+\varepsilon=1\). Let \(\mathcal{X}\) be the \(d\)-dimensional mixture distribution_

\[\mathcal{X}=\sum_{i=1}^{k}w_{i}D_{i}(\mu_{i})+\varepsilon Q,\]

_where \(Q\) is an unknown adversarial distribution that can depend on all the other parameters. Let \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) satisfy Assumption 3.1._

_Then there exists an algorithm that, given \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(N_{LD}(w_{\mathrm{low}})+N_{R}(w_{ \mathrm{low}}))\) i.i.d. samples from \(\mathcal{X}\), and given also \(d,\ k\), \(w_{\mathrm{low}}\), and \(t\), runs in time \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(T_{LD}(w_{\mathrm{low}})+T_{R}(w_{ \mathrm{low}}))\) and outputs a list \(L\) of size \(|L|=O(1/w_{\mathrm{low}})\), such that, with probability at least \(1-w_{\mathrm{low}}^{O(1)}\):_

1. _For each_ \(i\in[k]\)_, there exists_ \(\hat{\mu}\in L\) _such that_ \[\|\hat{\mu}-\mu_{i}\|\leqslant O(\psi_{t}(w_{i}/4)+f(w_{i}/4)).\]
2. _For each_ \(i\in[k]\)_, if_ \(w_{i}\geqslant 1-\varepsilon_{\mathrm{RME}}\)_, then there exists_ \(\hat{\mu}\in L\) _such that_ \[\|\hat{\mu}-\mu_{i}\|\leqslant O(g(w_{i}-w_{\mathrm{low}}^{2})).\]

Proof.: Proof follows by applying Theorem B.2 to \(\mathcal{X}\) with \(\alpha_{\mathrm{low}}=w_{\mathrm{low}}\) and treating each component as a corresponding inlier distribution with \(\alpha=w_{i}\). This gives error upper bound for all inlier components, furthermore, since \(\alpha_{\mathrm{low}}=w_{\mathrm{low}}\), list size can be bounded as \(|L|\leqslant 1+O((1-\alpha)/\alpha_{\mathrm{low}})=O(1/w_{\mathrm{low}})\). 

### Outer stage algorithm and guarantees

In the outer stage, presented in Algorithm 6, we make use of the list-decodable mean estimation algorithm in Theorem B.2 in order to solve list-decodable mixture estimation with separated means. We now present results on the outer stage algorithm. For ease of notation, when it's clear from the context, we drop the indices and refer to elements \(\mu_{j}\in M\) for some \(j\in[|M|]\) as \(\mu\) and their corresponding sets \(S_{j}^{(1)},S_{j}^{(2)}\), as defined in lines 6-7 in Algorithm 6, as \(S^{(1)},S^{(2)}\). Further, for \(i\in[k]\), let \(C_{i}^{*}\) denote the set of points corresponding to the \(i\)-th inlier component, also called the \(i\)-th inlier cluster.

**Theorem B.6** (Outer stage guarantees, beginning of execution).: _Let \(S\) consist of \(n\) i.i.d. samples from \(\mathcal{X}\) as in the statement of Theorem C.1. Run \(\mathrm{OuterStage}\) (Algorithm 6) on \(S\) and consider the first iteration of the while-loop and for each \(\mu\in M\), denote the corresponding sets as \(S^{(1)},S^{(2)}\). Then, with probability at least \(1-w_{\mathrm{low}}^{O(1)}\), we have that_

1. _the list_ \(M\) _that_ \(\mathcal{A}_{\mathrm{sLD}}\) _outputs has size_ \(|M|\leqslant 2/w_{\mathrm{low}}\)_,_
2. _for each_ \(i\in[k]\)_, there exists_ \(m_{i}\in[|M|]\) _such that_ \(\left|S_{m_{i}}^{(1)}\cap C_{i}^{*}\right|\geqslant(1-\frac{w_{\mathrm{low}}^ {2}}{2})\left|C_{i}^{*}\right|\)_,_
3. _for each_ \(i\in[k]\) _and_ \(\mu\in M\)_, we have_ \(\left|S^{(1)}\cap C_{i}^{*}\right|<w_{\mathrm{low}}^{4}\left|C_{i}^{*}\right| \text{or}\left|S^{(2)}\cap C_{i}^{*}\right|\geqslant(1-\frac{w_{\mathrm{low}}^ {2}}{2})\left|C_{i}^{*}\right|\)_,_
4. _for each_ \(i\in[k]\) _and_ \(\mu\in M\) _such that_ \(\left|S^{(2)}\cap C_{i}^{*}\right|\geqslant w_{\mathrm{low}}^{4}\left|C_{i}^{*}\right|\)_, we have_ \(\sum_{i^{\prime}\in[k]\setminus\{i\}}\left|S^{(2)}\cap C_{i^{\prime}}^{*} \right|\leqslant w_{\mathrm{low}}^{4}n\)_,_
5. _for_ \(i\neq i^{\prime}\in[k]\) _and for_ \(j,j^{\prime}\in[|M|]\)_, if_ \(\left|S_{j}^{(2)}\cap C_{i}^{*}\right|\geqslant w_{\mathrm{low}}^{4}|C_{i}^{*}|\) _and_ \(\left|S_{j^{\prime}}^{(2)}\cap C_{i^{\prime}}^{*}\right|\geqslant w_{\mathrm{ low}}^{4}|C_{i^{\prime}}^{*}|\)_, then_ \(S_{j}^{(2)}\cap S_{j^{\prime}}^{(2)}=\emptyset\)_._In words, Theorem B.6 (ii) states that _at initialization_, \(\mathrm{OuterStage}\) represents each inlier cluster well, i.e., for each \(i\), the \(i\)-th cluster is almost entirely contained in some set \(S_{j}^{(1)}\) for some \(j\in[|M|]\). Next, (iii) states that either \(S_{j}^{(1)}\) intersects negligibly some true component, or \(S_{j}^{(2)}\) contains almost entirely the same component. Further, (iv) and (v) state that sets that sufficiently intersect with some true component must be separated from other components and each other.

We now introduce some notation to present the next theorem that establishes further guarantees for the algorithm output during execution. For \(\mathcal{T}\), a collection of sets that is the output of Algorithm 6, we define

\[G\coloneqq\left\{i\in[k]\text{, such that there exists }T=S_{j}^{(2)}\in \mathcal{T}\text{ with }\left|S_{j}^{(1)}\cap C_{i}^{*}\right|\geqslant w_{\mathrm{low}}^{4}\left|C_ {i}^{*}\right|,\text{ for some }j\right\}.\] (B.1)

In words, it is the set of inlier components for which a corresponding set with "sufficiently many" points from the \(i\)-th component was added to \(\mathcal{T}\). It may happen that for a given index \(i\in G\), several \(j\in[|M|]\) satisfy \(S_{j}^{(2)}\in\mathcal{T}\) and \(\left|S_{j}^{(1)}\cap C_{i}^{*}\right|\geqslant w_{\mathrm{low}}^{4}\left|C_ {i}^{*}\right|\). We define \(g_{i}\in[|M|]\) to denote the index of the _first_ such set \(S_{g_{i}}^{(2)}\) added to \(\mathcal{T}\).

Further, we define \(U_{i}\coloneqq(C_{i}^{*}\cap S_{g_{i}}^{(2)})\setminus S_{g_{i}}^{(1)}\) to be the set of inlier points from the \(i\)-th component, which were _not_ removed from \(S\) at the iteration corresponding to \(g_{i}\). Let \(U\coloneqq\cup_{i\in G}U_{i}\) denote the union of such 'left-over' inlier points.

**Theorem B.7** (Outer stage guarantees, during execution).: _Let \(S\) consist of \(n\) i.i.d. samples from \(\mathcal{X}\) as in the statement of Theorem C.1. Run \(\mathrm{OuterStage}\) (Algorithm 6) on \(S\) and consider the moment when the sets \(S_{i}^{(2)}\) are added to \(\mathcal{T}\). We have that, with probability at least \(1-w_{\mathrm{low}}^{O(1)}\), all of the following are true:_

1. \(|U|\leqslant(2\varepsilon+O(w_{\mathrm{low}}^{2}))n\)_,_
2. _for_ \(i\in G\)_, we have that_ \(\left|S_{g_{i}}^{(2)}\cap C_{i}^{*}\right|\geqslant(1-\frac{w_{\mathrm{low}}^ {2}}{2}-O(w_{\mathrm{low}}^{3}))\left|C_{i}^{*}\right|\geqslant(1-w_{\mathrm{ low}}^{2})w_{i}n\)_,_
3. _for_ \(j\in[|M|]\setminus\left\{g_{i}\left|\,i\in G\right.\right\}\)_, either_ \(\left|S_{j}^{(2)}\right|\leqslant O(w_{\mathrm{low}}^{2})n\)_, or at least half of the samples in_ \(S_{j}^{(1)}\) _are either adversarial samples or lie in_ \(U\)_,_
4. _if when the else statement is triggered,_ \(|S|\geqslant 0.1w_{\mathrm{low}}n\)_, then at least a_ \(0.4\)_-fraction of the samples in_ \(S\) _are adversarial, or equivalently,_ \(|S|\leqslant 2.5\varepsilon n\)_._

Note that the else statement of \(\mathrm{OuterStage}\) can only be triggered once, at the end of the execution. In words, Theorem B.7 (i) states that, for \(i\in G\), samples from \(i\)-th cluster that remained in \(S\) after \(S_{g_{i}}^{(1)}\) was removed, constitute a small (comparable with \(\varepsilon\)) fraction. Further, (ii) states that the sets _added to_\(\mathcal{T}\), corresponding to \(i\in G\), almost entirely contain \(C_{i}^{*}\). Finally, (iii) describes the sets that do not correspond to any \(g_{i},i\in G\). These sets must either be small, or contain a significant amount of outlier points in the neighborhood. The proofs of Theorems B.6 and B.7 can be found in Appendix E.

## Appendix C Proof of Theorem 3.3

In this section, we state and prove a refined version of our main result, Theorem C.1, from which the statement of Theorem 3.3 directly follows.

### General theorem statement

We define

\[\psi_{t}(\alpha)=\begin{cases}\sqrt{t}(1/\alpha)^{1/t}&\text{if }t\leqslant 2 \log 1/\alpha,\\ \sqrt{2e\log 1/\alpha}&\text{else},\end{cases}\] (C.1)

which captures a tail decay of a distribution with sub-Gaussian \(t\)-th central moments: \(\mathbb{P}_{x\sim\mathcal{D}}\left(\left\langle x-\mu,v\right\rangle^{t} \geqslant\psi_{t}(\alpha)\right)\lesssim\alpha\).

We now state our main result for list-decodable mixture learning. Recall that \(\varepsilon_{\mathrm{RME}}\) is defined in Assumption 3.1.

**Theorem C.1** (Main mixture model result).: _Let \(d,k\in\mathbb{N}_{+}\), \(w_{\mathrm{low}}\in(0,10^{-4}]\), and \(t\) be an even integer. For all \(i=1,\ldots,k\), let \(D_{i}(\mu_{i})\) be a \(d\)-dimensional distribution with mean \(\mu_{i}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments. Let \(\varepsilon>0\) and, for all \(i=1,\ldots,k\), let \(w_{i}\in[w_{\mathrm{low}},1]\), such that \(\sum_{i=1}^{k}w_{i}+\varepsilon=1\). Let \(\mathcal{X}\) be the \(d\)-dimensional mixture distribution_

\[\mathcal{X}=\sum_{i=1}^{k}w_{i}D_{i}(\mu_{i})+\varepsilon Q,\]

_where \(Q\) is an unknown adversarial distribution that can depend on all the other parameters. Let \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) satisfy Assumption 3.1. Further, suppose that \(\|\mu_{i}-\mu_{j}\|\geqslant 200\psi_{t}(w_{\mathrm{low}}^{4})+200f(w_{ \mathrm{low}})\) for all \(i\neq j\in[k]\)._

_Then there exists an algorithm (Algorithm 2) that, given \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(N_{LD}(w_{\mathrm{low}})+N_{R}(w_{ \mathrm{low}}))\) i.i.d. samples from \(\mathcal{X}\), and given also \(d\), \(k\), \(w_{\mathrm{low}}\), and \(t\), runs in time \(\mathrm{poly}(d,1/w_{\mathrm{low}})\cdot(T_{LD}(w_{\mathrm{low}})+T_{R}(w_{ \mathrm{low}}))\) and with probability at least \(1-w_{\mathrm{low}}^{O(1)}\) outputs a list \(L\) of size \(|L|\leqslant k+O(\varepsilon/w_{\mathrm{low}})\) such that, for each \(i\in[k]\), there exists \(\hat{\mu}\in L\) such that:_

\[\|\hat{\mu}-\mu_{i}\|\leqslant O(\psi_{t}(\tilde{w}_{i}/10)+f(\tilde{w}_{i}/1 0)),\qquad\text{where}\quad\tilde{w}_{i}=w_{i}/(w_{i}+\varepsilon+w_{\mathrm{ low}}^{2}).\]

_If the relative weight of the \(i\)-th inlier cluster is large, i.e., \(\tilde{w}_{i}\geqslant 1-\varepsilon_{\mathrm{RME}}+2w_{\mathrm{low}}^{2}\), then there exists \(\hat{\mu}\in L\) such that_

\[\|\hat{\mu}-\mu_{i}\|\leqslant O(g(\tilde{w}_{i}-3w_{\mathrm{low}}^{2})).\]

Further, we assume \(w_{\mathrm{low}}\in(0,1/10000]\), since this simplifies some of the proofs. We note that in a mixture with \(k\) components we necessarily have \(w_{\mathrm{low}}\leqslant 1/k\). Furthermore, when \(w_{\mathrm{low}}\in(1/10000,1/2]\), then we obtain the same result by replacing \(w_{\mathrm{low}}\) with \(w_{\mathrm{low}}/5000\) throughout the statements and the proof. This would only affect both list size and error guarantees by at most a multiplicative constant, which is absorbed in the Big-O notation.

Proof of Theorem 3.3.: Proof follows directly from Theorem C.1, by noticing that Assumption 3.2 allows to replace \(f(\tilde{w}_{i}/10)\) by \(Cf(\tilde{w}_{i})\) and \(g(\tilde{w}_{i}-3w_{\mathrm{low}}^{2})\) by \(Cg(\tilde{w}_{i})\) for some constant \(C>0\) large enough.

### Proof of Theorem c.1

We now show how to use the results on the inner and outer stage, Theorem B.2 and Theorem B.7 respectively, to arrive at the guarantees for the full algorithm Algorithm 2 in Theorem C.1. For simplicity of the exposition, we split the proof of Theorem C.1 into two separate parts, proving that (i) the output list contains an estimate with small error and that (ii) the size of the output list is small. In what follows we condition on the event \(E^{\prime}\) from the proof of Theorem B.7.

(i) Proof of error statementWe now prove that, conditioned on the event \(E\), the list \(L\) output by Algorithm 2 for each \(i\in[k]\) contains an estimate \(\hat{\mu}\in L\), such that,

1. \(\|\hat{\mu}-\mu_{i}\|\leqslant O(\psi_{t}(\tilde{w}_{i}/10)+f(\tilde{w}_{i}/10))\),
2. if \(\tilde{w}_{i}\geqslant 1-\varepsilon_{\mathrm{RME}}+2w_{\mathrm{low}}^{2}\), then \(\|\hat{\mu}-\mu_{i}\|\leqslant O(g(\tilde{w}_{i}-3w_{\mathrm{low}}^{2}))\).

We start by showing that list-decoding error guarantees as in (1) are achievable for all inlier clusters and proceed by improving the error to (2) with RME base learner. Recall that \(G\) is as defined in Eq. (B.1).

Proof of (1): We now show how the output of the base learner and filtering procedure lead to the error in (1). Fix \(i\in[k]\). Recall that \(C_{i}\) denotes the set of \(w_{i}n\) points from \(i\)-th inlier component with mean \(\mu_{i}\).

If \(i\in G\), then on event \(E\), we have \(\left|S_{g_{i}}^{(2)}\cap C_{i}^{*}\right|\geqslant(1-w_{\mathrm{low}}^{2})w_ {i}n\) by Theorem B.7 (ii), \(\sum_{j\neq i}\left|S_{g_{i}}^{(2)}\cap C_{j}^{*}\right|\leqslant w_{\mathrm{ low}}^{4}n\) by Theorem B.6 (iv), and that the total number of adversarial points is at most \((\varepsilon+w_{\mathrm{low}}^{4})n\).

Therefore, the fraction of points from \(C_{i}^{*}\) in \(S_{g_{i}}^{(2)}\) is at least \(\frac{(1-w_{\mathrm{low}}^{2})w_{i}}{w_{i}+\varepsilon+w_{\mathrm{low}}^{2}}\), which implies \(\alpha\geqslant\tilde{w}_{i}\) as in Definition B.1. Then, by Theorem B.2, the \(\mathrm{InnerStage}\) algorithm applied to \(T\) leads to error \(\|\hat{\mu}-\mu_{i}\|\leqslant O(\psi_{t}(\tilde{w}_{i}/4)+f(\tilde{w}_{i}/4))\). Otherwise, if \(i\not\in G\), when the \(\mathrm{OutStage}\) algorithm reaches the else statement, \(S\) contains at least \((1-O(w_{\mathrm{low}}^{1}))|C_{i}^{*}|\) samples from \(C_{i}^{*}\). Indeed, since \(i\notin G\), each time we remove points from \(S\), we remove at most \(w_{\mathrm{low}}^{1}n\) points from \(C_{i}^{*}\). By Theorem B.6 (i), we do at most \(O(1/w_{\mathrm{low}})\) removals, so when the \(\mathrm{Outeratedge}\) algorithm reaches the else statement, \(S\) contains at least \((1-O(w_{\mathrm{low}}^{3}))\left|C_{i}^{*}\right|\) samples from \(C_{i}^{*}\).

We showed that samples from \(C_{i}^{*}\) make up at least a \((1-w_{\mathrm{low}}^{2})w_{i}n/|S|\) fraction of \(S\). Based on this fact we can then use Theorem B.7 (iv) and the assumption on the range of \(w_{\mathrm{low}}\) to conclude that \(|S|\leqslant 2.5\varepsilon n\) and that the fraction of inliers is at least \((1-w_{\mathrm{low}}^{2})w_{i}/(2.5\varepsilon)\). Therefore, \(S\) can be seen as containing samples from the corruption model \(\mathrm{cor}\)-\(\mathrm{aLD}\) with \(\alpha\) at least \(w_{i}/(2.5\varepsilon)\geqslant w_{i}/(2.5(w_{i}+\varepsilon))\). Since \(S\) is added to \(\mathcal{T}\) in the else statement, applying \(\mathrm{InnerStage}\) yields the error bound as in (1).

Proof of (2): Next, we prove that for all inlier components \(i\) with large weight, i.e., such that \(w_{i}/(w_{i}+\varepsilon)\geqslant 1-\varepsilon_{\mathrm{RME}}\), there exists a set \(T\in\mathcal{T}\) that consists of samples from the corruption model \(\mathrm{cor}\)-\(\mathrm{aLD}\) with \(\alpha\geqslant w_{i}/(w_{i}+\varepsilon)-2w_{\mathrm{low}}^{2}\). Then, running \(\mathrm{InnerStage}\), in particular the RME base learner, results in the error bound as in (2) by Theorem B.2 (ii). If \(i\in G\), in the previous paragraph we showed that there exists \(T\in\mathcal{T}\), such that the corresponding \(\alpha\geqslant\frac{w_{i}}{w_{i}+\varepsilon+w_{\mathrm{low}}^{2}}\geqslant \frac{w_{i}}{w_{i}+\varepsilon}-2w_{\mathrm{low}}^{2}\). We now prove by contradiction that the case \(i\notin G\) does not occur. Now assume \(i\notin G\) so that as we argued before when the else statement is triggered, \(S\) contains at least \((1-O(w_{\mathrm{low}}^{3}))|C_{i}^{*}|\) samples from \(C_{i}^{*}\).

By Theorem B.6 (ii), for some \(m_{i}\in[|M|]\), we have that \(|S_{m_{i}}^{(1)}\cap C_{i}^{*}|\geqslant(1-w_{\mathrm{low}}^{2}/2-O(w_{\mathrm{ low}}^{3}))|C_{i}^{*}|\) and by Theorem B.6 (iv), \(S_{m_{i}}^{(2)}\) contains at most \(w_{\mathrm{low}}^{4}n\) samples from other true clusters. Then, since \(|S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|\), we have that \(|S_{m_{i}}^{(2)}|\) contains at least

\[(1-w_{\mathrm{low}}^{2}-O(w_{\mathrm{low}}^{3}))|C_{i}^{*}|-w_{\mathrm{low}}^{4} n\geqslant(1-1.5w_{\mathrm{low}}^{2})|C_{i}^{*}|\]

adversarial samples. Therefore, \(\varepsilon\geqslant(1-1.5w_{\mathrm{low}}^{2})|C_{i}^{*}|/n\), and using that \(|C_{i}^{*}|\geqslant w_{i}n-w_{\mathrm{low}}^{10}n\), we have \(\varepsilon\geqslant(1-1.5w_{\mathrm{low}}^{2})w_{i}-w_{\mathrm{low}}^{10}\). However, this contradicts \(w_{i}/(w_{i}+\varepsilon)\geqslant 1-\varepsilon_{\mathrm{RME}}\) unless \(\varepsilon_{\mathrm{RME}}\geqslant 1/2-2w_{\mathrm{low}}^{2}\), which is prohibited by the assumptions in Theorem C.1. Therefore whenever \(w_{i}/(w_{i}+\varepsilon)\geqslant 1-\varepsilon_{\mathrm{RME}}\) we are in the case \(i\in G\).

(ii) Proof of small list sizeWe now prove that on the set \(E\), we have that \(|L|\leqslant k+O(\varepsilon/w_{\mathrm{low}})\). Here, we need to carefully analyze iterations in the while loop where an inlier component is "selected" for the first time in order to obtain a tight list size bound. Recall that \(g_{i}\) corresponds to the index in \(R\) that is first selected for the \(i\)-th inlier cluster.

First selection of a componentFor any \(i\in[k]\), if \(i\in G\), then Theorem B.7 (ii) implies that running \(\mathrm{InnerStage}\) on \(S^{(2)}_{g_{i}}\) produces a list of size at most \(1+O((|S^{(2)}_{g_{i}}\setminus C^{*}_{i}|)/(w_{\mathrm{low}}n))\). Then, over all \(i\in G\), these sets \(S^{(2)}_{g_{i}}\) contribute to the list size \(|L|\) at most \(k+O\left(\sum_{i=1}^{k}|S^{(2)}_{g_{i}}\setminus C^{*}_{i}|/(w_{\mathrm{low}}n )\right)\). Furthermore, by Theorem B.6 (v), all these sets \(S^{(2)}_{g_{i}}\) are disjoint and each of them contains at most \(w^{4}_{\mathrm{low}}n\) samples from other true clusters. Therefore \(\sum_{i=1}^{k}|S^{(2)}_{g_{i}}\setminus C^{*}_{i}|\leqslant\varepsilon n+O(w ^{3}_{\mathrm{low}})n\). Then the contribution to \(|L|\) of all these \(S_{i}\)'s corresponding to true clusters is at most \(k+O\left((\varepsilon+w^{3}_{\mathrm{low}})/w_{\mathrm{low}}\right)\). Note that if \(\varepsilon\leqslant w^{3}_{\mathrm{low}}\) and \(w_{\mathrm{low}}\) is small enough, Algorithm 3 actually produces a list of size \(1\) in each run considered above, so the contribution is exactly \(k\); otherwise we can bound the contribution by \(k+O(\varepsilon/w_{\mathrm{low}})\).

Samples left over from a componentNext, all inlier samples that were not removed, i.e., constituting \(U\), can be considered outlier points for the future iterations, which, by Theorem B.7 (i), only increases the outlier fraction to \(\tilde{\varepsilon}=3\varepsilon+O(w^{2}_{\mathrm{low}})\). For the same reason as above, without loss of generality, we can consider \(\varepsilon>w^{2}_{\mathrm{low}}\) since otherwise, the corresponding list size overhead (for small enough \(w^{2}_{\mathrm{low}}\)) would again amount to zero.

Clusters of adversarial samplesFor iterations where a set \(S^{(2)}_{j}\) was added to the final list, which does not correspond to some \(g_{i},i\in G\), Theorem B.7 (iii), states that either (i) at least half of the samples in \(S^{(2)}_{j}\) were adversarial, or (ii) the cardinality of the set on which Algorithm 3 was executed is small. In both cases the set \(S^{(2)}_{j}\) contributes at most \(O(\varepsilon/w_{\mathrm{low}})\) to the final list size \(|L|\).

List size in the else statementFinally, when the algorithm reaches the else statement, as argued in the first part, by Theorem B.7 (iv), at that iteration \(|S|\leqslant O(\varepsilon)n\). Since Algorithm 3 always produces a list of size bounded by \(O(|S|\left/(w_{\mathrm{low}}n)\right)\) (see Remark B.4), the contribution to \(|L|\) at this iteration is bounded by \(O(\varepsilon/w_{\mathrm{low}})\).

Overall, we obtain the desired bound on \(|L|\) of \(k+O(\varepsilon/w_{\mathrm{low}})\).

## Appendix D Proof of Theorem b.2

(i) Proof of error statementWe now prove that, with probability \(1-w^{O(1)}_{\mathrm{low}}\), for the output list \(L\) of Algorithm 6,

1. there exists \(\hat{\mu}\in L\) such that \[\|\hat{\mu}-\mu^{*}\|\leqslant O(\psi_{t}(\alpha/4)+f(\alpha/4)),\]
2. if \(\alpha\geqslant 1-\varepsilon_{\mathrm{RME}}\), then there exists \(\hat{\mu}\in L\) such that \[\|\hat{\mu}-\mu^{*}\|\leqslant O(g(\alpha-w^{2}_{\mathrm{low}})).\]

By Lemma D.1 we have \(|M|\leqslant 1/w^{O(1)}_{\mathrm{low}}\) and, with probability at least \(1-w^{O(1)}_{\mathrm{low}}\), there exists \((\hat{\mu},\hat{\alpha})\in M\) such that \(\hat{\alpha}\geqslant\alpha/4\) and \(\|\hat{\mu}-\mu^{*}\|\leqslant f(\hat{\alpha})\). Then Lemma D.2 implies that, with probability at least \(1-|M|^{2}w^{O(1)}_{\mathrm{low}}\), \((\hat{\mu},\hat{\alpha})\) will not be removed from \(M\). Therefore, either \((\hat{\mu},\hat{\alpha})\in L\), or there exists \((\tilde{\mu},\tilde{\alpha})\in L\) such that (i) \(\hat{\alpha}\geqslant\hat{\alpha}\) and (ii) \(\|\tilde{\mu}-\hat{\mu}\|\leqslant 4\beta(\hat{\alpha})\). The latter case implies that \(\|\tilde{\mu}-\mu^{*}\|\leqslant 40\psi_{t}(\alpha/4)+4f(\alpha/4)\).

For the second part, set first \(\tilde{\mu}=\hat{\mu}\). Then, in the \(i^{\mathrm{th}}\) iteration, \(\tilde{\mu}\) moves away by at most \((3\tau/2)/2^{i-1}\). Since \(\sum_{i=1}^{\infty}1/2^{i}\leqslant 1\), the distance between \(\tilde{\mu}\) and \(\hat{\mu}\) is always bounded by \(3\tau\). Now, assume that indeed \(\alpha\geqslant 1-\varepsilon_{\mathrm{RME}}\) and \(\|\hat{\mu}-\mu^{*}\|\leqslant\tau\). Whenever \(\tilde{\alpha}\leqslant\alpha\), with high probability \(\mathcal{A}_{R}\) produces some \(\mu_{\mathrm{RME}}\) such that \(\|\mu_{\mathrm{RME}}-\mu^{*}\|\leqslant g(\tilde{\alpha})\leqslant\tilde{ \beta}/2\). Furthermore, as long as \(\tilde{\alpha}\leqslant\alpha\), at the moment of the while statement check we have \(\|\tilde{\mu}-\mu^{*}\|\leqslant\tilde{\beta}\): in the first iteration this is by assumption, and in later iterations it follows because \(\tilde{\mu}\) is the former \(\mu_{\mathrm{RME}}\). Therefore the while statement check passes as long as \(\tilde{\alpha}\leqslant\alpha\).

There exists the possibility that the algorithm returns or breaks even though \(\tilde{\alpha}\leqslant\alpha\). If the algorithm returns early, then the error \(\tau\) achieved by \(\hat{\mu}\) is already within a factor of two of the optimal. If the algorithm breaks, either \(\tilde{\alpha}+w_{\mathrm{low}}^{2}>1\), case in which \(\tilde{\mu}\) already satisfies \(\|\tilde{\mu}-\mu^{*}\|\leqslant g(1-w_{\mathrm{low}}^{2})\), or else \(\|\tilde{\mu}-\mu^{*}\|\) is already within a factor of two of the optimal. Therefore these cases do not affect the error negatively.

Finally, let us consider what happens when \(\tilde{\alpha}>\alpha\) and the while statement check continues to pass. The first time we reach some \(\tilde{\alpha}>\alpha\), we must have \(\|\tilde{\mu}-\mu^{*}\|\leqslant\tilde{\beta}\leqslant 2g(\alpha-w_{ \mathrm{low}}^{2})\). Then, in later iterations, \(\tilde{\mu}\) can move from this estimate by a distance of most \(3\tilde{\beta}\leqslant 6g(\alpha-w_{\mathrm{low}}^{2})\), by the same argument as the argument that \(\|\tilde{\mu}-\hat{\mu}\|\leqslant 3\tau\). Overall, at the end we have

\[\|\tilde{\mu}-\hat{\mu}\|\leqslant\max(2g(1),g(1-w_{\mathrm{low}}^{2}),8g( \alpha))\leqslant 8g(\alpha-w_{\mathrm{low}}^{2}).\]

The number of runs is at most \(1/w_{\mathrm{low}}^{2}\), so with probability \(1-w_{\mathrm{low}}^{O(1)}\) all runs of \(\mathcal{A}_{R}\) succeed.

We showed that there exists \((\hat{\mu},\hat{\alpha})\in L\) such that \(\hat{\alpha}\geqslant\alpha/4\) and \(\|\hat{\mu}-\mu^{*}\|\leqslant 40\psi_{t}(\hat{\alpha})+4f(\hat{\alpha})\). This error can increase by running \(\mathrm{ImproveWithRME}\) with \(\tau=40\psi_{t}(\hat{\alpha})+4f(\hat{\alpha})\) to at most

\[\|\hat{\mu}-\mu^{*}\|\leqslant 160\psi_{t}(\hat{\alpha})+16f(\hat{\alpha})=O( \psi_{t}(\alpha)+f(\alpha)).\]

Furthermore, if \(\alpha\geqslant 1-\tau_{\mathrm{min}}\), this \((\hat{\mu},\hat{\alpha})\in L\) satisfies the conditions of \(\mathrm{ImproveWithRME}\), so with high probability the error is reduced by running \(\mathrm{ImproveWithRME}\) with \(\tau=40\psi_{t}(\hat{\alpha})+4f(\hat{\alpha})\) to \(\|\hat{\mu}-\mu^{*}\|\leqslant 8g(\alpha-w_{\mathrm{low}}^{2})\).

(ii) Proof of list sizeWe now prove that \(|L|\leqslant 1+O((1-\alpha)/\alpha_{\mathrm{low}})\).

First, assume that \(\alpha\leqslant 9/10\). Since all \(\hat{\alpha}_{s}\geqslant\alpha_{\mathrm{low}}\), we have that \(|L|\leqslant 10/(9\alpha_{\mathrm{low}})\leqslant 12(1-\alpha)/\alpha_{\mathrm{low}}\).

For the rest of the proof we assume that \(\alpha>9/10\). We analyze sets \(J\) and \(T_{i}\) for \(i\in J\) at the end of execution of \(\mathrm{ListFilter}\). In particular, recall that \(L=\{(\hat{\mu}_{i},\hat{\alpha}_{i}),\;i\in J\}\). Also, at the end of Algorithm 4 we have the following expression for \(T_{i}\):

\[T_{i}=\bigcap_{j\in J\setminus\{i\}}\{x\in S,\;\text{s.t.}\;|v_{ij}^{\top}(x- \hat{\mu}_{i})|\leqslant\max(\beta(\hat{\alpha}_{i}),\beta(\hat{\alpha}_{j}))\},\]

where \(v_{ij}\) are unit vectors in direction \(\hat{\mu}_{i}-\hat{\mu}_{j}\). Select the \(s\in J\) for which \(\hat{\alpha}_{s}\) is maximized. By (i), with probability at least \(1-w_{\mathrm{low}}^{60}\), there exists a hypothesis in \(J\) with \(\hat{\alpha}\geqslant\alpha/4\geqslant 0.2\). Then we have that \(\hat{\alpha}_{s}\geqslant 0.2\). In addition, for all hypotheses, \(\hat{\alpha}_{s}\leqslant 1/3\). Let \(j\in J\) be such that \(j\neq s\). We will show that at least half of the points in \(T_{j}\) are adversarial, i.e., \(|T_{j}|\geqslant 2\,|T_{j}\cap C^{*}|\). If this is indeed the case, we can treat all inlier points in all \(T_{j}\) as outliers, as it would at most double total number of outlier points in \(S\).

Now, assume that for some \(j\neq s\), \(|T_{j}|<2\,|T_{j}\cap C^{*}|\). Note that, because \(|T_{s}|\geqslant 0.9\cdot 0.2n=0.18n\) and \(|C^{*}|\geqslant 0.9n\), \(|T_{s}\cap C^{*}|\geqslant 0.18n-0.1n\geqslant 0.08n\). Therefore

\[\big{|}\big{\{}x\in C^{*},\,\text{s.t.}\;|v_{ssj}^{\top}(x-\hat{\mu}_{s})| \leqslant\beta(\hat{\alpha}_{s})\big{\}}\big{|}\geqslant 0.08\,|C^{*}|\,.\]

Also note that Lemma H.2 for radius \(10\psi_{t}(1/2)\leqslant 10\psi_{t}(\hat{\alpha}_{s})\) implies that, with exponentially small failure probability,

\[\big{|}\big{\{}x\in C^{*},\,\text{s.t.}\;|v_{ssj}^{\top}(x-\mu^{*})|\leqslant \beta(\hat{\alpha}_{s})\big{\}}\big{|}\geqslant 0.99\,|C^{*}|\,.\]

Since these two sets necessarily intersect, we can bound \(|v_{ssj}^{\top}(\hat{\mu}_{s}-\mu^{*})|\leqslant 2\beta(\hat{\alpha}_{s})\), implying that \(|v_{sj}^{\top}(\hat{\mu}_{j}-\mu^{*})|\geqslant 2\beta(\hat{\alpha}_{j})\), since \(\|\hat{\mu}_{s}-\hat{\mu}_{j}\|\geqslant 4\beta(\hat{\alpha}_{j})\). Thus, if \(|v_{sj}^{\top}(x-\hat{\mu}_{j})|\leqslant\beta(\hat{\alpha}_{j})\), then \(|v_{sj}^{\top}(x-\mu^{*})|>\beta(\hat{\alpha}_{j})\), implying that

\[(T_{j}\cap C^{*})\subseteq\big{\{}x\in C^{*},\,\text{s.t.}\;|v_{sj}^{\top}(x-\mu^ {*})|>\beta(\hat{\alpha}_{j})\big{\}}\,.\] (D.1)

However, Lemma H.2 tells us that with high probability only a small fraction of points in \(C^{*}\) satisfies \(|v_{sj}^{\top}(x-\mu^{*})|>\beta(\hat{\alpha}_{j})\). In particular, applying the lemma with radius \(10\psi_{t}(\hat{\alpha}_{j})\), we get that with exponentially small failure probability,

\[\big{|}\big{\{}x\in C^{*},\,\text{s.t.}\;|v_{sj}^{\top}(x-\mu^{*})|\leqslant \beta(\hat{\alpha}_{j})\big{\}}\big{|}\geqslant\left(1-\frac{\hat{\alpha}_{j}}{50} \right)|C^{*}|\,.\] (D.2)From eqs. (D.1) and (D.2) it follows that \(\left|T_{j}\cap C^{*}\right|\leqslant\hat{\alpha}_{j}\left|C^{*}\right|/50\). Using that \(\left|T_{j}\right|\geqslant 9\hat{\alpha}_{j}n/10\geqslant 9\hat{\alpha}_{j} \left|C^{*}\right|/10\) by the properties of \(\mathrm{ListFilter}\), we obtain

\[9\hat{\alpha}_{j}\left|C^{*}\right|/10\leqslant\left|T_{j}\right|\leqslant 2 \left|T_{j}\cap C^{*}\right|\leqslant\hat{\alpha}_{j}\left|C^{*}\right|/25,\]

which is a contradiction.

Therefore, for all \(j\in J\) such that \(j\neq s\), we have that \(\left|T_{j}\right|\geqslant 2\left|T_{j}\cap C^{*}\right|\). As we said in the beginning, by treating all inlier points in those \(T_{j}\) as outliers we at most double total number of outlier points. Since there are at most \((1-\alpha+\alpha w_{\mathrm{low}}^{2})n\) outlier points and the sets \(T_{j}\) are non-intersecting, we get \(\sum_{j\in J\setminus\{s\}}\left|T_{j}\right|\leqslant 2(1-\alpha+\alpha w_{ \mathrm{low}}^{2})n\). The lower bound on the size \(\left|T_{j}\right|\geqslant 9\alpha_{\mathrm{low}}n/10\) implies \(\left|J/\left\{s\right\}\right|\leqslant\frac{2(1-\alpha+\alpha w_{\mathrm{ low}}^{2})n\cdot 10}{9\alpha_{\mathrm{low}}n}\) and thus \(\left|L\right|=\left|J\right|\leqslant 1+3(1-\alpha)/\alpha_{\mathrm{low}}\).

Note that in \(\mathrm{InnerStage}\) we set \(\alpha_{\mathrm{low}}=\min(\alpha_{\mathrm{low}},1/100)\). Therefore, for the original \(\alpha_{\mathrm{low}}\), the list size is bounded by \(1+240(1-\alpha)/\alpha_{\mathrm{low}}\).

ConclusionCombining the probabilities of success of all steps, we get that the algorithm succeeds with probability at least \(1-w_{\mathrm{low}}^{O(1)}\) for some large constant in the exponent. Our algorithm, ignoring the calls to \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\), has sample complexity and time complexity bounded by \(\mathrm{poly}(d,1/w_{\mathrm{low}})\), which gives the desired sample and time complexity when taking \(\mathcal{A}_{\mathrm{kLD}}\) and \(\mathcal{A}_{R}\) into consideration. This completes the proof of the theorem.

### Auxiliary lemmas and proofs

**Lemma D.1** (List initialization).: _Let \(S\), \(\alpha_{\mathrm{low}}\) and \(\alpha\) be as in \(\mathrm{cor}\mathrm{-aLD}\) model. If \(\mathrm{InnerStage}\) (Algorithm 3) is run with \(S\) and \(\alpha_{\mathrm{low}}\), the size of \(M\) is at most \(1/w_{\mathrm{low}}^{O(1)}\), all \((\hat{\mu},\hat{\alpha})\in M\) satisfy \(\hat{\alpha}\leqslant 1/3\), and with probability at least \(1-w_{\mathrm{low}}^{O(1)}\) there exists \((\hat{\mu},\hat{\alpha})\in M\) such that \(\alpha/4\leqslant\hat{\alpha}\leqslant\min(\alpha,1/3)\) and \(\|\hat{\mu}-\mu^{*}\|\leqslant f(\hat{\alpha})\)._

Proof.: There are at most \(1/\alpha_{\mathrm{low}}\) choices for \(\hat{\alpha}\), and for each of them the output of \(\mathcal{A}_{\mathrm{kLD}}\) has size at most \(1/w_{\mathrm{low}}^{O(1)}\), so \(\left|M\right|\leqslant 1/w_{\mathrm{low}}^{O(1)}\). With probability \(1-w_{\mathrm{low}}^{O(1)}\), \(\mathcal{A}_{\mathrm{kLD}}\) succeeds in all up to \(1/\alpha_{\mathrm{low}}\) runs. Then we are guaranteed to produce one \(\hat{\alpha}\) with \(\alpha/4\leqslant\hat{\alpha}\leqslant\min(\alpha,1/3)\), and then \(\mathcal{A}_{\mathrm{kLD}}\) is guaranteed to produce one corresponding \(\hat{\mu}\) with \(\|\hat{\mu}-\mu^{*}\|\leqslant f(\hat{\alpha})\). 

**Lemma D.2** (Good hypotheses are not removed).: _Let \(S\), \(\alpha_{\mathrm{low}}\) and \(\alpha\) be as in \(\mathrm{cor}\mathrm{-aLD}\) model. Run \(\mathrm{ListFilter}\) (Algorithm 4) on \(S\) and \(\alpha_{\mathrm{low}}\) with \(M\) obtained from \(\mathrm{InnerStage}\) (Algorithm 3) and call a hypothesis \((\hat{\mu},\hat{\alpha})\in M\) good if \(\hat{\alpha}\geqslant\alpha/4\) and \(\|\hat{\mu}-\mu^{*}\|\leqslant f(\hat{\alpha})\). Then, with probability at least \(\left|M\right|^{2}w_{\mathrm{low}}^{O(1)}\), no good hypothesis is removed from \(M\) (including in any of the reruns triggered by the algorithm)._

Proof.: Let \(\ell\) be an arbitrary iteration of the outer for loop. Then, at the beginning of the \(\ell^{\mathrm{th}}\) iteration,

1. \(T_{j}\cap T_{s}=\emptyset\) for any \(j<s\in J\),
2. \(\left|T_{j}\right|\geqslant 0.9\hat{\alpha}_{j}n\) for any \(j\in J\),
3. \(\left|J\right|\leqslant 10/(9\hat{\alpha}_{\ell})\).

Indeed, the second property follows directly from the algorithm.

For the first property, assume that for \(j<s\in J\), there exists \(x\in S\), such that \(x\in T_{j}\cap T_{s}\). This would imply that \(\left|v_{js}^{\top}(\hat{\mu}_{j}-\hat{\mu}_{s})\right|\leqslant 2\beta(\hat{ \alpha}_{s})\), so \(\|\hat{\mu}_{j}-\hat{\mu}_{s}\|\leqslant 2\beta(\hat{\alpha}_{s})\). However, in this case the first 'if' condition would pass and we would not add \(s\) to \(J\). Thus, \(T_{j}\cap T_{s}=\emptyset\).

For the third property, note that

\[n\geqslant\sum_{j\in J}\left|T_{j}\right|\geqslant\sum_{j\in J}0.9\hat{\alpha} _{j}n\geqslant 0.9\left|J\right|\hat{\alpha}_{\ell}n,\]

which implies \(\left|J\right|\leqslant 10/(9\hat{\alpha}_{\ell})\).

Now, let \(s\) be the index of a hypothesis with \(\hat{\alpha}_{s}\geqslant\alpha/4\) and \(\left\|\hat{\mu}_{s}-\mu^{*}\right\|\leqslant f(\hat{\alpha}_{s})\). If \(s\) was skipped in the \(s^{\mathrm{th}}\) iteration (i.e., there exists \(j\in J\) with \(\hat{\mu}_{j}\) close to \(\hat{\mu}_{s}\)), then \((\hat{\mu}_{s},\hat{\alpha}_{s})\) is trivially not removed from \(M\). For the rest of the proof we assume that \(s\) is not skipped.

For the sake of the analysis, we introduce the analogue of the sets \(T_{s}\), which we call \(\tilde{T}_{s}\), defined for points in the set \(C^{*}\) (i.e., all inlier points before the adversarial removal), and show that (i) \(\left|\tilde{T}_{s}\right|\) is large and (ii) \(\left|\tilde{T}_{s}\setminus T_{s}\right|\) is small. In particular, let

\[\tilde{T}_{s}=\bigcap_{j\in J}\left\{x\in C^{*},\text{ s.t. }|v_{js}^{\top}(x-\hat{\mu}_{s})| \leqslant\beta(\hat{\alpha}_{s})\right\},\]

where we recall \(\beta(\hat{\alpha}_{s})=10\psi_{t}(\hat{\alpha}_{s})+f(\hat{\alpha}_{s})\). Note that \(|T_{s}|\geqslant\left|\tilde{T}_{s}\right|-|C^{*}\setminus S^{*}|\geqslant \left|\tilde{T}_{s}\right|-w_{\mathrm{low}}^{2}\left|C^{*}\right|\). Also, for any \(\alpha^{\prime}\leqslant\hat{\alpha}_{s}\), applying Lemma H.2 with radius \(10\psi_{t}(\alpha^{\prime})\), using that \(\left\|\hat{\mu}_{s}-\mu^{*}\right\|\leqslant f(\hat{\alpha}_{s})\leqslant f (\alpha^{\prime})\) and \(t\geqslant 2\), we get that with exponentially small failure probability,

\[\left|\left\{x\in C^{*},\text{ s.t. }|v^{\top}(x-\hat{\mu}_{s})|>\beta(\alpha^{ \prime})\right\}\right|\leqslant\frac{\alpha^{\prime}}{50}\left|C^{*}\right|.\] (D.3)

Consider the \(s^{\mathrm{th}}\) iteration. Using a union bound over \(|J|\leqslant 2/\alpha_{\mathrm{low}}\) directions, and since all \(\hat{\alpha}_{s}\geqslant\alpha_{\mathrm{low}}\), we get that with exponentially small failure probability

\[\left|\tilde{T}_{s}\right|\geqslant\left|C^{*}\right|-\sum_{i\in J}\left| \left\{x\in C^{*},\text{ s.t. }|v_{is}^{\top}(x-\mu_{s})|>\beta(\hat{\alpha}_{s})\right\}\right|\geqslant \left(1-\frac{\hat{\alpha}_{s}}{50}\left|J\right|\right)\left|C^{*}\right| \geqslant 0.95\left|C^{*}\right|,\]

where we used that and \(|J|\leqslant 10/(9\hat{\alpha}_{s})\). This implies that

\[|T_{s}|\geqslant\left|\tilde{T}_{s}\right|-w_{\mathrm{low}}^{2}\left|C^{*} \right|\geqslant(0.95-w_{\mathrm{low}}^{2})\left|C^{*}\right|\geqslant 0.92 \left|C^{*}\right|\geqslant 0.9\alpha n\geqslant 0.9\hat{\alpha}_{s}n,\]

i.e., \((\hat{\mu}_{s},\hat{\alpha}_{s})\) is not removed from \(M\) during \(s^{\mathrm{th}}\) iteration.

The pair \((\hat{\mu}_{s},\hat{\alpha}_{s})\) could also be removed during later iterations, when we recalculate \(T_{s}\) by removing points along new directions. However, following a similar argument, we show that still, with high probability, \(|T_{s}|\geqslant 0.9\hat{\alpha}_{s}n\). Assume that we are now in the \(k^{\mathrm{th}}\) iteration of the outer cycle, where \(k>s\). We define again \(\tilde{T}_{s}\) and sets \(A,B\):

\[\tilde{T}_{s} \coloneqq\bigcap_{i\in J\setminus\{s\}}\left\{x\in C^{*},\text{ s.t. }|v_{is}^{\top}(x-\hat{\mu}_{s})|\leqslant\max(\beta(\hat{\alpha}_{s}),\beta( \hat{\alpha}_{i}))\right\},\] \[A \coloneqq\bigcap_{i<s,i\in J}A_{i},\quad\text{ for }\quad A_{i} \coloneqq\left\{x\in C^{*},\text{ s.t. }|v_{is}^{\top}(x-\hat{\mu}_{s})|\leqslant\beta(\hat{\alpha}_{s})\right\},\] \[B \coloneqq\bigcap_{i>s,i\in J}B_{i},\quad\text{ for }\quad B_{i} \coloneqq\left\{x\in C^{*},\text{ s.t. }|v_{is}^{\top}(x-\hat{\mu}_{s})|\leqslant\beta(\hat{\alpha}_{i})\right\},\]

so that \(\tilde{T}_{s}=A\cap B\) and again \(|T_{s}|\geqslant\left|\tilde{T}_{s}\right|-w_{\mathrm{low}}^{2}\left|C^{*}\right|\). It is crucial that we have different right hand sides in the definitions of \(A_{i}\) and \(B_{i}\) (we wrote them in boldface to emphasize this).

Using a union bound again, we write

\[\left|\tilde{T}_{s}\right|\geqslant\left|C^{*}\right|-\sum_{i<s,i\in J}\left| C^{*}\setminus A_{i}\right|-\sum_{i>s,i\in J}\left|C^{*}\setminus B_{i}\right|.\]

Using eq. (D.3), with exponentially small failure probability, for all \(i\in J\),

\[\left|C^{*}\setminus A_{i}\right|\leqslant(\hat{\alpha}_{s}/50)\left|C^{*} \right|\quad\text{(for $i<s$)}\quad\text{and}\quad\left|C^{*}\setminus B_{i}\right| \leqslant(\hat{\alpha}_{i}/50)\left|C^{*}\right|\quad\text{(for $i>s$)}.\]

Next, note that before the last element was added, we had that (i) \(T_{i}\bigcap T_{j}=\emptyset\) for any \(i\neq j\in J\) and (ii) \(|T_{i}|\geqslant 0.9\hat{\alpha}_{i}n\) for any \(i\in J\). This implies that \(\sum_{i\in J}\hat{\alpha}_{i}<10/9+\hat{\alpha}_{\mathrm{last}}<19/9\), where \(\hat{\alpha}_{\mathrm{last}}\) corresponds to the element which was added last (it might happen that after addition of the last element, we have \(|T_{i}|<0.9\hat{\alpha}_{i}n\) for several \(i\in J\)). Therefore, as before, we obtain that

\[\left|\tilde{T}_{s}\right|\geqslant\left(1-\sum_{i<s,i\in J}(\hat{\alpha}_{s}/5 0)-\sum_{i>s,i\in J}(\hat{\alpha}_{i}/50)\right)\left|C^{*}\right|\geqslant(1 -10/(9\cdot 50)-19/(9\cdot 50))\left|C^{*}\right|\geqslant 0.93\left|C^{*}\right|,\]therefore \(\left|T_{s}\right|\geqslant\left|\tilde{T}_{s}\right|-w_{\mathrm{low}}^{2}\left|C^{ *}\right|\geqslant 0.92\left|C^{*}\right|\geqslant 0.9\hat{\alpha}_{s}n\) and \((\hat{\mu}_{s},\hat{\alpha}_{s})\) will not be removed from \(M\).

We established that in a single run of the algorithm a good hypothesis is removed with exponentially small probability. The number of good hypotheses is bounded by \(\left|M\right|\). Furthermore, the number of runs of the algorithm is also bounded by \(\left|M\right|\), since whenever the algorithm is rerun a hypothesis is removed from \(M\). Then, by a union bound, we can bound the probability that any good hypothesis is removed in any run of the algorithm by \(\left|M\right|^{2}w_{\mathrm{low}}^{O(1)}\). 

## Appendix E Proof of outer stage algorithm guarantees in Appendix B.3

Recall that \(\gamma=4\psi_{t}(w_{\mathrm{low}}^{4})\) and \(\gamma^{\prime}=160\psi_{t}(w_{\mathrm{low}}/4)+16f(w_{\mathrm{low}}/4)\).

### Proof of Theorem b.6

In what follows we condition on the event \(E\) that the events under which the conclusions in Lemmas H.2 and H.3 hold and that \(\mathcal{A}_{\mathrm{sLD}}\) succeeds. This event holds with probability \(1-w_{\mathrm{low}}^{O(1)}\) by Assumption 3.1, Remark B.3 and union bound (also see Appendix G).

Proof of Theorem b.6 (i)The list size bound follows from the standard results on \(\mathcal{A}_{\mathrm{sLD}}\) (see [3], Proposition B.1).

Proof of Theorem b.6 (ii)Guarantees of \(\mathcal{A}_{\mathrm{sLD}}\) imply that there exists \(\mu_{i}\in M\) such that \(\left\|\mu_{i}-\mu^{*}\right\|\leqslant\gamma^{\prime}\). By Lemma H.3, a \((1-w_{\mathrm{low}}^{2}/2)\)-fraction of the samples in \(C^{*}\) are \(\gamma\)-close to \(\mu^{*}\) along each direction \(v_{ij}\) with \(i\neq j\in[\left|M\right|]\). Then, the same \((1-w_{\mathrm{low}}^{2}/2)\)-fraction of samples are \((\gamma+\gamma^{\prime})\)-close to \(\mu_{i}\) along each direction \(v_{ij}\), so they are included in \(\mathcal{S}_{i}^{(1)}\).

Proof of Theorem b.6 (iii)Suppose \(|S_{i}^{(1)}\cap C^{*}|\geqslant w_{\mathrm{low}}^{4}|C^{*}|\). Previous point implies that there exists \(\mu_{j}\in M\) be such that \(\left\|\mu_{j}-\mu^{*}\right\|\leqslant\gamma^{\prime}\). Then at least an \(w_{\mathrm{low}}^{4}\)-fraction of the samples in \(C^{*}\) are \((\gamma+\gamma^{\prime})\)-close to \(\mu_{i}\) in direction \(\mu_{i}-\mu_{j}\). By Lemma H.2, \(\mu^{*}\) is also \(\gamma\)-close in direction \(\mu_{i}-\mu_{j}\) to more than a \((1-w_{\mathrm{low}}^{4})\)-fraction of the samples in \(C^{*}\), so it is \(\gamma\)-close to at least one sample in any \(w_{\mathrm{low}}^{4}\)-fraction of samples in \(C^{*}\). Therefore \(\mu^{*}\) is also \((2\gamma+\gamma^{\prime})\)-close to \(\mu_{i}\) in direction \(\mu_{i}-\mu_{j}\). Then \(\left\|\mu_{i}-\mu_{j}\right\|\leqslant 2\gamma+2\gamma^{\prime}\) and \(\left\|\mu_{i}-\mu^{*}\right\|\leqslant 2\gamma+3\gamma^{\prime}\). Again, using Lemma H.3 we obtain that there exists a \((1-w_{\mathrm{low}}^{2}/2)\)-fraction of the samples in \(C^{*}\), which is included in \(S_{i}^{(2)}\).

Proof of Theorem b.6 (iv)Similarly, if \(|S_{i}^{(2)}\cap C^{*}|\geqslant w_{\mathrm{low}}^{4}|C^{*}|\), then there exists \(\mu_{j}\in M\), such that at least an \(w_{\mathrm{low}}^{4}\)-fraction of the samples in \(C^{*}\) are \((3\gamma+3\gamma^{\prime})\)-close to \(\mu_{i}\) in direction \(\mu_{i}-\mu_{j}\). By the same arguments as in previous paragraph, we obtain that \(\left\|\mu_{i}-\mu_{j}\right\|\leqslant 4\gamma+4\gamma^{\prime}\) and \(\left\|\mu_{i}-\mu^{*}\right\|\leqslant 4\gamma+5\gamma^{\prime}\).

Then any other true cluster with mean \((\mu^{*})^{\prime}\) and set of samples \((C^{*})^{\prime}\) satisfies \(\left\|\mu^{*}-(\mu^{*})^{\prime}\right\|\geqslant 16\gamma+16\gamma^{\prime}\), so \(\left\|\mu_{i}-(\mu^{*})^{\prime}\right\|\geqslant 12\gamma+11\gamma^{\prime}\). From guarantees of \(\mathcal{A}_{\mathrm{sLD}}\), there exists \(\mu_{j}^{\prime}\in M\) such that \(\left\|\mu_{j}^{\prime}-(\mu^{*})^{\prime}\right\|\leqslant\gamma^{\prime}\). Then \(\left\|\mu_{i}-\mu_{j}^{\prime}\right\|\geqslant 12\gamma+10\gamma^{\prime}\). By Lemma H.2, more than an \(w_{\mathrm{low}}^{4}\)-fraction of the samples from \((C^{*})^{\prime}\) are \(\gamma\)-close to \((\mu^{*})^{\prime}\) in direction \(\mu_{i}-\mu_{j}^{\prime}\), so also \((\gamma+\gamma^{\prime})\)-close to \(\mu_{j}^{\prime}\) in direction \(\mu_{i}-\mu_{j}^{\prime}\), so also \((11\gamma+9\gamma^{\prime})\)-far from \(\mu_{i}\) in direction \(\mu_{i}-\mu_{j}^{\prime}\). Then \(S_{i}^{(2)}\) selects at most a \(w_{\mathrm{low}}^{4}\)-fraction of the samples from \((C^{*})^{\prime}\). Overall, \(S_{i}^{(2)}\) selects from all other true clusters at most \(w_{\mathrm{low}}^{4}n\) samples.

Proof of Theorem b.6 (v)Note that by the same argument, \(\left\|\mu_{i}-\mu^{*}\right\|\leqslant 4\gamma+5\gamma^{\prime}\) and \(\left\|\mu_{i^{\prime}}-(\mu^{*})^{\prime}\right\|\leqslant 4\gamma+5\gamma^{\prime}\). However, \(\left\|\mu^{*}-(\mu^{*})^{\prime}\right\|\geqslant 16\gamma+16\gamma^{\prime}\), so also \(\left\|\mu_{i}-\mu_{i^{\prime}}\right\|\geqslant 8\gamma+6\gamma^{\prime}\), so \(S_{i}^{(2)}\) and \(S_{i^{\prime}}^{(2)}\) are disjoint by the condition that each selects only samples that are \((3\gamma+3\gamma^{\prime})\)-close along direction \(\mu_{i}-\mu_{i^{\prime}}\) to the respective means \(\mu_{i}\) and \(\mu_{i^{\prime}}\).

### Proof of Theorem b.7

In the sequel, for any \(i\in G\), let \(m_{i}\) be the index in \(R\) after initialization that satisfies Theorem B.6 (ii). We condition on the event \(E^{\prime}\) that event \(E\) from the proof of Theorem B.6 holds and that both \(||C_{i}^{*}|-w_{i}n|\leqslant w_{\mathrm{low}}^{10}n\) for all \(i\in[k]\) and the number of adversarial points lies in the range \(\varepsilon n\pm w_{\mathrm{low}}^{10}n\). By Hoeffding's inequality and the union bound, the probability of \(E^{\prime}\) is at least \(1-w_{\mathrm{low}}^{O(1)}\).

Proof of Theorem b.7 (i)Let \(i\in G\), and consider the beginning of the iteration when \(\mu_{g_{i}}\) is selected. Then, using that all previous iterations could have removed at most \(O(w_{\mathrm{low}}^{3})|C_{i}^{*}|\) samples from \(C_{i}^{*}\), we have that

\[|S_{m_{i}}^{(1)}\cap C_{i}^{*}|\geqslant(1-w_{\mathrm{low}}^{2}/2-O(w_{ \mathrm{low}}^{3}))|C_{i}^{*}|.\]

Therefore at the iteration in which \(\mu_{g_{i}}\) is selected, we still have \(m_{i}\in R\). We now discuss two cases: First, consider the case that \(|S_{m_{i}}^{(2)}|\leqslant 2|S_{m_{i}}^{(1)}|\). Then, because we selected \(\mu_{g_{i}}\in M\) and not \(\mu_{m_{i}}\in M\) it means that \(|S_{g_{i}}^{(1)}|\geqslant|S_{m_{i}}^{(1)}|\geqslant(1-w_{\mathrm{low}}^{2}/2 -O(w_{\mathrm{low}}^{3}))|C_{i}^{*}|\). Note also by Theorem B.6 (iv), the number of samples from other true clusters in \(S_{g_{i}}^{(2)}\) is at most \(w_{\mathrm{low}}^{4}n\). Then the number of adversarial samples in \(S_{g_{i}}^{(2)}\) is at least

\[|S_{g_{i}}^{(2)}|-|C_{i}^{*}|-w_{\mathrm{low}}^{4}n\geqslant|S_{g_{i}}^{(2)} \setminus S_{g_{i}}^{(1)}|-O(w_{\mathrm{low}}^{2})|C_{i}^{*}|-w_{\mathrm{low} }^{4}n\geqslant|S_{g_{i}}^{(2)}\setminus S_{g_{i}}^{(1)}|-O(w_{\mathrm{low}}^ {2})|C_{i}^{*}|.\]

Then, either \(\left|S_{g_{i}}^{(2)}\setminus S_{g_{i}}^{(1)}\right|=O(w_{\mathrm{low}}^{2} )|C_{i}^{*}|\) and \(|U_{i}|\leqslant\left|S_{g_{i}}^{(2)}\setminus S_{g_{i}}^{(1)}\right|=O(w_{ \mathrm{low}}^{2})|C_{i}^{*}|\), or \(\left|S_{g_{i}}^{(2)}\setminus S_{g_{i}}^{(1)}\right|\gg w_{\mathrm{low}}^{2} |C_{i}^{*}|\). In the latter case, even if \(S_{g_{i}}^{(2)}\setminus S_{g_{i}}^{(1)}\) consists of adversarial examples only, then, since \(|S_{g_{i}}^{(2)}|\leqslant 2|S_{g_{i}}^{(1)}|\), \(U_{i}\) contains at most double the number of adversarial examples in \(S_{g_{i}}^{(1)}\), i.e. \(|U_{i}|\leqslant 2V_{i}\) where \(V_{i}\) denotes the number of adversarial examples in \(S_{g_{i}}^{(1)}\).

Now consider the case that \(|S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|\). By Theorem B.6 (iv), the number of samples from true clusters in \(S_{m_{i}}^{(2)}\) is at most \(|C_{i}^{*}|+w_{\mathrm{low}}^{4}n\leqslant 1.02|S_{m_{i}}^{(1)}|\), so the number \(W_{i}\) of adversarial samples in \(S_{m_{i}}^{(2)}\) is at least \(W_{i}\geqslant|S_{m_{i}}^{(2)}|-1.02|S_{m_{i}}^{(1)}|\geqslant 0.98|S_{m_{i}}^{(1)}| \geqslant 0.96|C_{i}^{*}|\). Then, \(|U_{i}|=\left|(C_{i}^{*}\cap S_{g_{i}}^{(2)})\setminus S_{g_{i}}^{(1)} \right|\leqslant|C_{i}^{*}|\leqslant 2W_{i}\).

Finally note that by Theorem B.6 (v), the sets \(S_{g_{i}}^{(2)}\) and \(S_{m_{i}}^{(2)}\) are disjoint from any other sets \(S_{g_{j}}^{(2)}\) and \(S_{m_{j}}^{(2)}\) that correspond to another component \(C_{j}^{*}\). Therefore, the number of adversarial examples in the \(S_{m_{i}}^{(2)}\) in the second case and \(S_{g_{i}}^{(2)}\) in the first case is smaller than the total number of adversarial examples, i.e.

\[\sum_{\begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|\leqslant 2|S_{m_{i}}^{(1)}|\end{subarray}}V_{i}+\sum_{ \begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|\end{subarray}}W_{i}\leqslant(\varepsilon+ w_{\mathrm{low}}^{10})n.\]

Therefore, we directly obtain

\[|U|\leqslant\sum_{i\in G}\left|(C_{i}^{*}\cap S_{g_{i}}^{(2)} \setminus S_{g_{i}}^{(1)}\right| =\sum_{\begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|\leqslant 2|S_{m_{i}}^{(1)}|\end{subarray}}\left|(C_{i}^{*}\cap S_{g_{i}}^{ (2)})\setminus S_{g_{i}}^{(1)}\right|+\sum_{\begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|\end{subarray}}\left|(C_{i}^{*}\cap S_{g_{i} }^{(2)})\setminus S_{g_{i}}^{(1)}\right|\] \[\leqslant\sum_{\begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|\leqslant 2|S_{m_{i}}^{(1)}|\end{subarray}}2V_{i}+\sum_{ \begin{subarray}{c}i\in G\\ |S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|\end{subarray}}2W_{i}+O(w_{\mathrm{low}}^ {2})n\leqslant(2\varepsilon+O(w_{\mathrm{low}}^{2}))n.\]

Proof of Theorem b.7 (ii)Each iteration before \(g_{i}\) was selected, removed at most \(w_{\mathrm{low}}^{4}|C_{i}^{*}|\) samples from \(C_{i}^{*}\), so all previous iterations removed at most \(O(w_{\mathrm{low}}^{3})|C_{i}^{*}|\) samples from \(C_{i}^{*}\). Then, by Lemma B.6 (iii), \(S_{g_{i}}^{(2)}\) contains at least \((1-w_{\mathrm{low}}^{2}/2-O(w_{\mathrm{low}}^{3}))|C_{i}^{*}|\) samples from \(C_{i}^{*}\). The statement follows then since on the event \(E^{\prime}\), we have \(w^{*}n-w_{\mathrm{low}}^{10}n\leqslant|C^{*}|\leqslant w^{*}n+w_{\mathrm{low}}^{1 0}n\).

Proof of Theorem b.7 (iii)Here, either for all \(i\in[k]\), \(|S^{(1)}_{j}\cap C^{*}_{i}|<u^{4}_{\text{low}}|C^{*}_{i}|\) or \(i\in G\) and the algorithm had already selected in a previous iteration \(\mu_{g_{i}}\in M\) with \(|S^{(1)}_{g_{i}}\cap C^{*}_{i}|\geqslant w^{4}_{\text{low}}|C^{*}_{i}|\). Consider a first case, in which \(|S^{(1)}_{j}\cap C^{*}_{i}|<w^{4}_{\text{low}}|C^{*}_{i}|\) for all \(i\in[k]\). Then the total number of samples from true clusters in \(S^{(1)}_{j}\) is at most \(w^{4}_{\text{low}}n\). Using that \(|S^{(1)}_{j}|>100w^{4}_{\text{low}}n\), it follows that more than half of the samples in \(S^{(1)}_{j}\) are adversarial.

The second case is that \(|S^{(1)}_{j}\cap C^{*}_{i}|\geqslant w^{4}_{\text{low}}|C^{*}_{i}|\) for some \(i\in G\) for which in a previous iteration \(g_{i}\) we had that \(|S^{(1)}_{g_{i}}\cap C^{*}_{i}|\geqslant w^{4}_{\text{low}}|C^{*}_{i}|\). Note that at most \(w^{2}_{\text{low}}|C^{*}_{i}|/2\) of the samples in \(S\cap C^{*}_{i}\) are not considered adversarial at this point (the ones that were outside \(S^{(2)}_{g_{i}}\)). Also, by Theorem b.6 (iv), \(S^{(1)}_{j}\) contains at most \(w^{4}_{\text{low}}n\) samples from other true clusters. Therefore either more than half of the samples in \(S^{(1)}_{j}\) are considered adversarial or

\[|S^{(1)}_{j}|\leqslant w^{2}_{\text{low}}|C^{*}_{i}|+2w^{4}_{\text{low}}n \leqslant O(w^{2}_{\text{low}})n.\]

Proof of Theorem b.7 (iv)Suppose that when the algorithm reaches the else statement we have for some \(i\in[k]\) that \(i\in R\) and \(|S^{(1)}_{m_{i}}\cap C^{*}_{i}|\geqslant 20w^{2}_{\text{low}}|C^{*}_{i}|\). We have that \(|S^{(2)}_{m_{i}}\cap C^{*}_{i}|\) is at most \(|S^{(1)}_{m_{i}}\cap C^{*}_{i}|+w^{2}_{\text{low}}|C^{*}_{i}|/2\), where we use that by Theorem b.6 (ii), at most \(w^{2}_{\text{low}}|C^{*}_{i}|/2\) samples can fail to be captured by \(S^{(1)}_{m_{i}}\). By Theorem b.6 (iv), furthermore, the number of samples from other true clusters in \(S^{(2)}_{m_{i}}\) is at most \(w^{4}_{\text{low}}n\). Therefore, using that \(|S^{(2)}_{m_{i}}|>2|S^{(1)}_{m_{i}}|\), the number of adversarial samples in \(S^{(2)}_{m_{i}}\) is at least

\[|S^{(2)}_{m_{i}}|-|S^{(1)}_{m_{i}}\cap C^{*}_{i}|-w^{2}_{\text{low}}|C^{*}_{i} |/2-w^{4}_{\text{low}}n\geqslant 0.45|S^{(2)}_{m_{i}}|-w^{4}_{\text{low}}n \geqslant 0.44|S^{(2)}_{m_{i}}|\,,\]

where in the last inequality we used that \(|S^{(2)}_{m_{i}}|>100w^{4}_{\text{low}}n\). Let \(V\) be the union, over all \(i\in[k]\), of all sets \(S^{(2)}_{m_{i}}\) such that \(i\in R\) and \(|S^{(1)}_{m_{i}}\cap C^{*}_{i}|\geqslant 20w^{2}_{\text{low}}|C^{*}_{i}|\). Theorem b.6 (v) gives that all such sets \(S^{(2)}_{m_{i}}\) are disjoint. Therefore at least a \(0.44\)-fraction of the samples in \(V\) are adversarial.

Consider now for some \(i\in[k]\) how many samples from \(S\cap C^{*}_{i}\) can be outside \(V\) when the algorithm reaches the else statement. By Theorem b.6 (ii), \(S^{(1)}_{m_{i}}\) can fail to capture at most \(w^{2}_{\text{low}}|C^{*}_{i}|/2\) samples from \(C^{*}_{i}\), and we have no guarantee that these samples are in \(V\). Consider now the samples in \(S^{(1)}_{m_{i}}\cap C^{*}_{i}\). If \(i\in R\), we may miss up to \(20w^{2}_{\text{low}}|C^{*}_{i}|\) of these samples if \(|S^{(1)}_{m_{i}}\cap C^{*}_{i}|<20w^{2}_{\text{low}}|C^{*}_{i}|\), because in this case we do not include \(S^{(2)}_{m_{i}}\) in \(V\). On the other hand, if \(i\not\in R\), there are at most \(100w^{4}_{\text{low}}n\) samples in \(S^{(1)}_{m_{i}}\cap C^{*}_{i}\). Then the total number of samples from \(S\cap C^{*}_{i}\) outside \(V\) is at most \(w^{2}_{\text{low}}|C^{*}_{i}|/2+20w^{2}_{\text{low}}|C^{*}_{i}|+100w^{4}_{\text {low}}n\). Summed across all \(i\in[k]\), this makes up at most \(21w^{2}_{\text{low}}n\) samples.

Overall, the number of adversarial samples in \(S\) when the algorithm reaches the else statement is at least

\[0.44|V|+(|S|-|V|-21w^{2}_{\text{low}}n)=|S|-0.56|V|-21w^{2}_{\text{low}}n \geqslant 0.44|S|-21w^{2}_{\text{low}}n\geqslant 0.4|S|\]

where in the last inequality we also used that \(|S|\geqslant 0.1w_{\text{low}}n\).

## Appendix F Proof of Proposition 3.5

We now prove lower bounds for the case of Gaussian distributions and distributions with \(t\)-th sub-Gaussian moments.

### Case b): For the Gaussian inliers

We first focus on the case when \(D_{i}(\mu_{i})=\mathcal{N}(\mu_{i},I)\). The proof goes through an efficient reduction from the problem considered by Proposition F.1 to the problem solved by algorithm \(\mathcal{A}\).

**Proposition F.1** ([5], Proposition 5.11).: _Let \(\mathcal{D}\) be the class of identity covariance Gaussians on \(\mathbb{R}^{d}\) and let \(0<\alpha\leqslant 1/2\). Then any list-decoding algorithm that learns the mean of an element of \(\mathcal{D}\) with failure probability at most \(1/2\), given access to \((1-\alpha)\)-additively corrupted samples, must either have error bound \(\beta=\Omega(\sqrt{\log 1/\alpha})\) or return \(\min(2^{\Omega(d)},(1/\alpha)^{w(1)})\) many hypotheses._First, we describe the means of the components in the input distribution to algorithm \(\mathcal{A}\). Let \(\bar{\mu}_{1},\ldots,\bar{\mu}_{k-1}\in\mathbb{R}^{d}\) be any set of \(k-1\) points with pairwise separation larger than \(2C\sqrt{\log 1/w_{\mathrm{low}}}\). Then let \(\mu_{k}=(\bar{\mu},0)\in\mathbb{R}^{d+1}\) and \(\mu_{i}=(\bar{\mu}_{i},2C\sqrt{\log 1/w_{\mathrm{low}}}+1)\in\mathbb{R}^{d+1}\) for all \(i\in[k-1]\). Then \(\mu_{1},\ldots,\mu_{k}\) also have pairwise separation larger than \(2C\sqrt{\log 1/w_{\mathrm{low}}}\).

Then, given \(n\) points \(y_{1},\ldots,y_{n}\in\mathbb{R}^{d}\) as in the input to the problem in Proposition F.1 (i.e. \((1-\alpha)\)-additively corrupted samples), we generate \(n\) points that we give as input to algorithm \(\mathcal{A}\) as follows: let \(S=\{1,\ldots,n\}\), and then for each of the \(n\) points, draw \(i\sim\mathrm{Unif}\{1,\ldots,k\}\) and generate the point as follows:

1. if \(i\in[k-1]\), sample the point from \(N(\mu_{i},I_{d+1})\),
2. if \(i=k\), sample \(j\sim S\) uniformly at random, remove \(j\) from \(S\), sample \(g\sim N(0,1)\), and let the point be \((y_{j},g)\in\mathbb{R}^{d+1}\).

We note that this construction simulates an input sampled i.i.d. according to the mixture \(\frac{1}{k}N(\mu_{1},I_{d+1})+\ldots+\frac{1}{k}N(\mu_{k-1},I_{d+1})+\frac{ \alpha}{k}N(\mu_{k},I_{d+1})+\frac{1-\alpha}{k}Q^{\prime}\) for some \(Q^{\prime}\). Then with success probability at least \(1/2\) running \(\mathcal{A}\) on this input with \(w_{\mathrm{low}}=\frac{\alpha}{k}\) returns a list \(L\) such that there exists \(\hat{\mu}\in L\) with \(\|\hat{\mu}-\mu_{k}\|\leqslant\beta_{k}\). Note that this implies that \(\|(\hat{\mu})_{1:d}-\bar{\mu}\|\leqslant\beta_{k}\). Finally, we create a pruned list \(L^{\prime}\) as follows: initialize \(L^{\prime}=L\) and then for each \(i\in[k-1]\) remove all \(\hat{\mu}\in L^{\prime}\) such that \(\|\hat{\mu}-\mu_{i}\|\leqslant C\sqrt{\log 1/w_{\mathrm{low}}}\). Then we return \(L^{\prime}\) as the output for the original problem in Proposition F.1.

Let us analyze now this output. The separation between the means ensures that any hypothesis \(\hat{\mu}\in L\) that is \(C\sqrt{\log 1/w_{\mathrm{low}}}\)-close to \(\mu_{k}\) is not removed in the pruning. Therefore \(L^{\prime}\) continues to contain a hypothesis \(\hat{\mu}\) such that \(\|(\hat{\mu})_{1:d}-\bar{\mu}\|\leqslant\beta_{k}\). Then, if \(\beta_{k}\neq\Omega(\sqrt{\log 1/\alpha})\) and \(|L^{\prime}|<\min\{2^{\Omega(d)},((w_{k}+\varepsilon)/w_{k})^{\omega(1)}\}\), this reduction violates the lower bound of Proposition F.1. Therefore we must have either \(\beta_{k}=\Omega(\sqrt{\log 1/\alpha})\) or \(|L^{\prime}|\geqslant\min\{2^{\Omega(d)},(1/\tilde{w}_{k})^{\omega(1)}\}\).

Finally, we show that these lower bounds on \(\beta_{k}\) and \(|L^{\prime}|\) imply the desired lower bound for \(\mathcal{A}\). Consider first the case: \(\beta_{k}=\Omega(\sqrt{\log 1/\alpha})\). Note that in the input to algorithm \(\mathcal{A}\) we have \(\tilde{w}_{k}=\alpha\). Therefore \(\beta_{k}=\Omega(\sqrt{\log 1/\alpha})\) corresponds to the desired lower bound in the lemma statement. Consider second the case: \(|L^{\prime}|\geqslant\min\{2^{\Omega(d)},(1/\tilde{w}_{k})^{\omega(1)}\}\). We note that, for each \(i\in[k-1]\), the original list \(L\) must contain some \(\hat{\mu}\in L\) such that \(\|\hat{\mu}-\mu_{i}\|\leqslant C\sqrt{\log 1/w_{\mathrm{low}}}\). Furthermore, because the means \(\mu_{i}\) have pairwise separation larger than \(2C\sqrt{\log 1/w_{\mathrm{low}}}\), the original list \(L\) must contain at least \(k-1\) means of this kind. However, all of these means are removed in the pruning procedure, so \(|L|\geqslant k-1+|L^{\prime}|\), so \(|L|\geqslant k-1+\min\{2^{\Omega(d)},(1/\tilde{w}_{k})^{\omega(1)}\}\). This matches the desired lower bound in the lemma statement. (The choice to make the hidden mean the \(k\)-th mean was without loss of generality, as the distribution is invariant to permutations of the components.)

### Case a): For distributions with \(t\)-th sub-Gaussian moments

The proof for the case when \(D_{i}(\mu_{i})\) has sub-Gaussian \(t\)-th central moments employs the same reduction scheme, but reduces from Proposition F.2.

**Proposition F.2** ([5], Proposition 5.12).: _Let \(\mathcal{D}\) be the class of distributions on \(\mathbb{R}^{d}\) with bounded \(t\)-th central moments for some positive even integer \(t\), and let \(0<\alpha<2^{-t-1}\). Then any list-decoding algorithm that learns the mean of an element of \(\mathcal{D}\) with failure probability at most \(1/2\), given access to \((1-\alpha)\)-additively corrupted samples, must either have error bound \(\beta=\Omega(\alpha^{-1/t})\) or return a list of at least \(d\) hypotheses._

Furthermore, in [3], formal evidence of computational hardness was obtained (see their Theorem 5.7, which gives a lower bound in the statistical query model introduced by [14]) that suggests obtaining error \(\Omega_{t}((1/\tilde{w}_{s})^{1/t})\) requires running time at least \(d^{\Omega(t)}\). This was proved for Gaussian inliers and the running time matches ours up to a constant in the exponent.

Stability of list-decoding algorithms

In this section we discuss two of the existing list-decodable mean estimation algorithms for identity-covariance Gaussian distributions and show that they also work when a \(w_{\mathrm{low}}^{2}\)-fraction of the inliers is adversarially removed.

First, we consider the algorithm in Theorem 3.1 in [3]. A central object in their analysis is an "\(\alpha\)-good multiset", which is a multiset of samples such that all are within distance \(O(\sqrt{d})\) of each other and at least an \(\alpha\)-fraction of them come from a \((1-\Omega(\alpha))\)-fraction of an i.i.d. set of samples from a Gaussian distribution \(N(\mu,I_{d})\). Then their algorithm essentially works as long as the input contains an \(\alpha\)-good multiset. For our case, after the removal of a \(w_{\mathrm{low}}^{2}\)-fraction of inliers, the input essentially continues to contain a \((1-w_{\mathrm{low}}^{2})\alpha\)-good multiset, so the algorithm continues to work in our corruption model.

Second, we consider the algorithm in Theorem 6.12 in [5]. The main distributional requirement of their algorithm is that \(\mathbb{E}_{x,y\sim S^{*}}[p^{2}(x-y)]\leqslant 2\mathbb{E}_{g,h\sim N(0,I_{d})}[p ^{2}(g-h)]\) for all degree-\((t/2)\) polynomials \(p\), where \(S^{*}\) is the set of inliers. Concentration arguments give with high probability that \(\mathbb{E}_{x,y\sim C^{*}}[p^{2}(x-y)]\leqslant 1.5\mathbb{E}_{g,h\sim N(0,I_{d})}[ p^{2}(g-h)]\). Furthermore, the distribution over \(x,y\sim S^{*}\) can be seen as a \((1-w_{\mathrm{low}}^{2})^{2}\)-fraction of the distribution over \(x,y\sim C^{*}\). Then Fact G.1, which follows by standard probability calculations, also gives that any event under the former distribution can be bounded in terms of the second distribution:

**Fact G.1**.: For any event \(A\),

\[\mathop{\mathbb{P}}_{x,y\sim S^{*}}(A)\leqslant\mathop{\mathbb{P}}_{x,y\sim C ^{*}}(A)/(1-w_{\mathrm{low}}^{2})^{2},\] (G.1)

where probabilities are taken over a uniform sample from \(S^{*}\) and \(C^{*}\) respectively.

Overall we obtain

\[\mathbb{E}_{x,y\sim S^{*}}[p^{2}(x-y)]\leqslant 1.5/(1-w_{\mathrm{low}}^{2})^{ 2}\mathbb{E}_{g,h\sim N(0,I_{d})}[p^{2}(g-h)],\]

so for \(w_{\mathrm{low}}\) small enough we have \(\mathbb{E}_{x,y\sim S^{*}}[p^{2}(x-y)]\leqslant 2\mathbb{E}_{g,h\sim N(0,I_{d})}[ p^{2}(g-h)]\) and their algorithm continues to work in our corruption model.

## Appendix H Concentration bounds

In this section we prove some concentration bounds essential to our analysis.

**Lemma H.1**.: _Let \(D\) be a \(d\)-dimensional distribution with mean \(\mu^{*}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments with parameter \(1\). Fix a unit vector \(v\in\mathbb{R}^{d}\). Then_

\[\mathop{\mathbb{P}}_{x\sim D}[|\langle x-\mu^{*},v\rangle|\leqslant R] \geqslant 1-\left(\frac{\sqrt{t}}{R}\right)^{t}\,.\]

Proof.: We have that

\[\mathop{\mathbb{P}}_{x\sim D}[|\langle x-\mu^{*},v\rangle|>R]\leqslant\frac{ \mathbb{E}_{x\sim D}\langle x-\mu^{*},v\rangle^{t}}{R^{t}}\leqslant\frac{(t-1 )!!}{R^{t}}\leqslant\left(\frac{\sqrt{t}}{R}\right)^{t}\,,\]

where we used that \((t-1)!!\leqslant t^{t/2}=\sqrt{t}^{t}\). 

**Lemma H.2**.: _Let \(D\) be a \(d\)-dimensional distribution with mean \(\mu^{*}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments with parameter \(1\). Let \(C^{*}\) be a set of i.i.d. samples drawn from \(D\). Fix a unit vector \(v\in\mathbb{R}^{d}\). Then with probability at least \(1-\exp\left(-2|C^{*}|\left(\frac{\sqrt{t}}{R}\right)^{2t}\right)\),_

\[|\{x\in C^{*}\text{, s.t. }|\langle x-\mu^{*},v\rangle|\leqslant R\}|\geqslant \left(1-2\left(\frac{\sqrt{t}}{R}\right)^{t}\right)|C^{*}|\,.\]

Proof.: The result follows by Lemma H.1 and a Binomial tail bound.

**Lemma H.3**.: _Let \(D\) be a \(d\)-dimensional distribution with mean \(\mu^{*}\in\mathbb{R}^{d}\) and sub-Gaussian \(t\)-th central moments with parameter \(1\). Let \(C^{*}\) be a set of i.i.d. samples drawn from \(D\). Fix \(m\) unit vectors \(v_{1},\ldots,v_{m}\in\mathbb{R}^{d}\). Then with probability at least \(1-\exp\left(-2|S^{*}|m^{2}\left(\frac{\sqrt{t}}{R}\right)^{2t}\right)\),_

\[\Big{|}\bigcap_{i\in[m]}\left\{x\in S^{*}\text{, s.t. }|\langle x-\mu^{*},v_{i} \rangle|\leqslant R\right\}\Big{|}\geqslant\left(1-2m\left(\frac{\sqrt{t}}{R} \right)^{t}\right)|S^{*}|\,.\]

Proof.: By Lemma H.1 and a union bound over the \(m\) directions, we get

\[\operatorname*{\mathbb{P}}_{x\sim D}[|\langle x-\mu^{*},v_{i}\rangle| \leqslant R,\forall i\in[m]]\geqslant 1-m\left(\frac{\sqrt{t}}{R}\right)^{t}.\]

Then the result follows by a Binomial tail bound. 

## Appendix I Experimental details

Adversarial line and adversarial clustersThe following figure illustrates the adversarial distributions used in Figure 2 and further in this section.

Data DistributionWe consider a mixture of \(k=7\) well-separated (\(\|\mu_{i}-\mu_{j}\|\geqslant 40\)) \(d=100\) dimensional inlier clusters whose subgroup sizes range from \(0.3\) to \(0.02\). The experiments are conducted once using a Gaussian distribution and once using a heavy-tailed t-distribution with five degrees of freedom for both inlier and adversarial clusters. In Figure 6 the latter suggests that our algorithm works comparatively well even for mixture distributions which do not fulfill our assumptions. We set \(w_{\mathrm{low}}=0.02\) and \(\varepsilon=0.12\) so that it is larger than the smallest clusters but smaller than the largest ones and set the total number of data points to \(10000\). The Gaussian noise model simply computes the empirical mean and covariance matrix of the clean data and samples \(1200\) noisy samples from a Gaussian distribution with this mean and covariance. The adversarial cluster model and the adversarial model are as depicted in Figure 4.

Attack distributionsWe consider three distinct adversarial models (see Figure 4 for reference).

1. _Adversarial clusters_: After sampling the inlier cluster means, we choose the cluster with the smallest weight. Let \(\mu_{s}\) denote its mean. Then, we sample a random direction \(v_{c}\) with \(\|v_{c}\|=10\). After that, we sample three directions \(v_{1},v_{2}\) and \(v_{3}\) with \(\|v_{i}\|=10\). Then we put three additional (outlier) clusters with means at \(\mu_{s}+v_{c}+v_{i}\). This roughly corresponds to the right picture in Figure 4. The samples for each adversarial cluster are drawn from a distribution that matches the covariance of the inlier clusters, with the sample size being twice as large as of the affected inlier cluster.
2. _Adversarial line_: After sampling the inlier cluster means, we again choose the cluster with the smallest weight. Let \(\mu_{s}\) denote its mean. Then, we sample a random direction \(v_{c}\) with \(\|v_{c}\|=10\). We put three additional (outlier) clusters with means at \(\mu_{s}+v_{c},\mu_{s}+2v_{c}\) and \(\mu_{s}+3v_{c}\), which form a line as shown in Figure 4. The samples are drawn similarly to the adversarial clusters, with the difference that the covariance is scaled by a factor of \(5\) in the direction of the line.

Figure 4: Two variants of adversarial distribution: adversarial line (left) and adversarial clusters (right).

3. _Gaussian adversary_: Here we simply introduce noise matching the empirical mean and covariance of all inlier data (i.e., as if all inlier clusters are generated from the same Gaussian distribution).

Note that in the first and second attack, the adversary creates clusters that do not respect the separation assumption of the true inlier clusters: either adversarial clusters are placed around the smallest inlier cluster (Adversarial Cluster), or the adversarial clusters form a line, pointing out in some fixed direction (Adversarial Line).

Implementation detailsWe implement the list-decodable mean estimation base learner in our InnerStage algorithm (Algorithm 3) based on [8]. It leverages an iterative multi-filtering strategy and one-dimensional projections. In particular, we use the simplified gaussian version of the algorithm. It is designed for distributions sampled from a Gaussian but also shows promising results for the experiments involving a heavy-tailed t-distribution as depicted in Figure 6. The robust mean estimator used to improve the mean hypotheses for large clusters is omitted in our implementation.

Hyper-parameter search and experiment executionThe hyper-parameters of our algorithm are tuned beforehand based on the experimental setup. For the comparative algorithms, hyper-parameter searches are conducted within each experiment after initial tuning. For our algorithm, key parameters include the pruning radius \(\gamma\) used in the OuterStage routine (Algorithm 6) and \(\beta\) used in the InnerStage (Algorithm 4). In addition, parameters for the LD-ME base learner, such as the cluster concentration threshold, also require careful selection, resulting in a total of 7 parameters. The tuning for these was performed using a grid search comparing about \(250\) different configurations. Similarly, we independently tune the vanilla LD-ME algorithm, which we run with \(w_{\mathrm{low}}\) as weight parameter. For

Figure 5: Comparison of five algorithms with three adversarial noise models. On the left we show worst estimation error of algorithms with constrained list size and on the right the smallest list size with constrained error guarantee. We plot the median of the metrics with the error bars showing \(25\)th and \(75\)th percentile. We observe that our method consistently outperforms prior works in terms of list size and worst estimation error, with the exception of DBSCAN, which performs at a similiar level.

Figure 6: Worst estimation error and list size comparison for the case where inlier distributions are heavy-tailed. We can observe numerical stability of our approach.

DBSCAN, we optimize the list size and error metrics by searching over a range of \(100\) values for \(\varepsilon\), which controls the maximum distance between samples considered in the same neighbourhood. The minimum samples threshold, which validates the density based clusters, is pretuned beforehand and adjusted based on \(w_{\text{low}}\). For \(k\)-means and its robust version, utilizing a median-of-means weighting scheme, we explore \(21\) values for \(k\), including the true number of clusters. Each parameter setting is executed \(100\) times to account for stochastic variations in the algorithmic procedures, such as \(k\)-means initialization. The list size and worst estimation error for each list of clusters obtained is visualized exemplarily for one iteration of the experiment in Figure 7. The plot provides insight into how the different algorithms perform and vary with different list sizes.

Evaluation detailsNote that we have two sources of randomness: the data is random and also the algorithms themselves are random (except DBSCAN). For a clear comparison, we sample and fix one dataset for each attack model. we plot the performance of \(100\) runs of each algorithm for each parameter setting, each time recording the returned list size together with the worst estimation error \(\max_{i\in[k]}\min_{\hat{\mu}\in L}\lVert\mu_{i}-\hat{\mu}\rVert\). Then we either (i) report the worst estimation error for all runs with constrained list size (we pick the list size most frequently returned by our algorithm, specifically \(7\) or \(10\) in our experiments) (see Figure 5, left), or (ii) report the smallest list size required to achieve the same or smaller worst estimation error (we pick the \(75\)th quantile of errors of our algorithm for a threshold) (see Figure 5, right). Under size constraint (i), the bar plots correspond to the median over the runs, with error bars indicating the \(25\)th and \(75\)th quantiles. Under error constraint (ii), the bar plots represent the minimum list size for which the median over the runs falls below the threshold, while the error bars show the minimum list size for which the \(25\)th and \(75\)th quantiles meet the constraint. Note that 'n/a' indicates that, within the scope of our parameter search, no list size achieves an error below the specified constraint.

In Figure 6 we study the numerical stability of our approach. In particular, whether the performance degrades when inlier distribution does not satisfy required assumptions. We observe that if one uses our meta-algorithm with base learner designed for Gaussian inliers, we still obtain stable results even in the case of heavy-tailed inlier distribution.

### Variation of \(w_{\text{low}}\)

To study the effect of varying \(w_{\text{low}}\) input on the performance of our approach and LD-ME, we introduce a new noise model. As illustrated in Figure 8, we consider a mixture of \(k=3\) well-separated clusters: one small cluster with a weight of \(0.045\) and two large clusters, each with a weight of \(0.2\). We place two adversarial clusters (see paragraph on attack distributions for details): one near the small cluster and another near one of the large clusters. Furthermore, uniform noise is introduced,

Figure 7: Scatter plot of all results for one iteration of the experiment using three adversarial noise models.

spanning the range of the data generated by the inlier and its nearby outlier cluster and accounting for \(10\%\) of the data in this region. Overall, \(\varepsilon=0.56\) and we draw \(22650\) samples from this mixture distribution.

For both algorithms we run 100 seeds for each \(w_{\text{low}}\) ranging from 0.02 to 0.2, which corresponds to the weight of the largest inlier cluster. In Figure 9, we plot the median estimation error with error bars showing the \(25\)th and \(75\)th quantiles for the small cluster (top left) and the large cluster near the outlier cluster (top right). As expected from our theoretical results, we observe that our algorithm performs roughly constant in estimating the mean of the large cluster, regardless of the initial \(w_{\text{low}}\). Meanwhile, the estimation error of LD-ME increases as \(w_{\text{low}}\) decreases further below the true cluster weight. Furthermore, the plots show that our approach does consistently outperform LD-ME in terms of both worst estimation error and list size. Figure 10 also compares the performance of the clustering algorithms in this experimental setup with results similar to the ones obtained in the previous experimental settings.

### Computational resources

Our implementation of the algorithm and experiments leverages multi-threading. It utilizes CPU resources of an internal cluster with \(128\) cores, which results in a execution time of about \(5\) minutes for a single run of the experiment for one noise model with \(10000\) samples. We remark that classic

Figure 8: Setup for \(w_{\text{low}}\) variation experiment with clusters contaminated by an adversarial cluster and uniform noise. Lower color intensities indicate smaller cluster weights.

Figure 9: Comparison of list size and estimation error for small and large inlier clusters for varying \(w_{\text{low}}\) inputs. The experimental setup is illustrated in Figure 8. The plot on the top left shows the estimation error for the small cluster and the plot on the top right shows the error for the large cluster. We plot the median values with error bars indicating 25th and 75th quantiles. As \(w_{\text{low}}\) decreases, our algorithm maintains a roughly constant estimation error for the large cluster, while the error for LD-ME increases.

approaches like \(k\)-means and DBSCAN perform fast and the most time-consuming part is the execution of the LD-ME base learner. Given our experimental setup with three noise models, it takes about \(15\) minutes to reproduce all our results for one data distribution.

Figure 10: Worst estimation error and list size comparison for the setup used in the \(w_{\text{low}}\) variation experiment.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We summarized our contributions in Table 1 in the introduction, and all the claims made there (including more general results) appear in Section 3 and Appendix C. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our theoretical results in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Our assumptions are stated in Sections 2 and 3, in particular Assumption 3.1. The complete proof is given in Appendix C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide experimental details, including explanation of the adversarial noise distribution in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We uploaded zip archive containing the code together with instructions on how to reproduce the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide short version of experimental details in Section 6 and more detailed one in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: As described in Appendix I, we plot the median of the metric (error or list size) accompanied by the 25% and 75% percentiles, acting as error bars. The statistics was collected over 100 reruns of all algorithms under comparison. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide experimental details, including computational resources, in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewed the NeurIPS Code of Ethics and confirm that our paper respects it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Main focus of our result is to deepen our theoretical understanding of fundamental problems in statistics, such as mixture learning. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgement, our paper does not pose misuse risks, as it has theoretical focus. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our work does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not use release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.