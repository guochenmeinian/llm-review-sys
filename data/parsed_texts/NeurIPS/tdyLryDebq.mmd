# FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy

Zuhao Yang\({}^{1}\), Yingfang Yuan\({}^{2}\), Yang Xu\({}^{3}\), Shuo Zhan\({}^{1}\), Huajun Bai\({}^{4}\), Kefan Chen\({}^{2}\)

\({}^{1}\) School of Computer Science and Engineering, Nanyang Technological University

\({}^{2}\) School of Mathematical and Computer Sciences, Heriot-Watt University

\({}^{3}\) Department of Computer Science, Southern University of Science and Technology

\({}^{4}\) Genify

Equal contribution, in random orderCorresponding author, email: xuyang@sustech.edu.cn

###### Abstract

Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on _F_ourier _A_nalysis of the estimated _Cross-Entropy_ of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.

## 1 Introduction

The concept of _entropy_ from Information Theory is broadly applied in Natural Language Processing (NLP) technology and computational linguistic studies. The most notable example is the use of _cross-entropy_ in training and evaluating language models, where the exponentiation of cross-entropy, perplexity, is adopted to measure models' performance in next-word (or masked-word) prediction task. However, low perplexity alone does not guarantee good performance in language generation tasks, which not only depend on model sizes but are also closely related to the sampling techniques used in _decoding_ stage. The complexity of the generation task makes it especially important to have different metrics that can reflect the generation quality from multiple angles. One particular perspective is that the language generated from a good model should have a similar distribution of words/tokens as in the "natural" human language.

Recent advances in psycholinguistics put forward new directions for developing more sophisticated metrics other than Zipf's coefficient. In particular, studies on temporal and spectral patterns in dialogue [7, 47] reveal that cross-entropy (or referred to as surprisal, information density in the psycholinguistics literature) changes _periodically_ in natural language, which points out the potentials of using fine-grained transformation of cross-entropy to quantify the differences in language data (see Section 3 for a detailed review). It motivates the basic idea of this study: Can we effectively quantify the _periodical_ pattern of the cross-entropy, and use it as an indicator to distinguish human and model-generated languages?

We summarize our contributions as follows: 1. We propose a set of metrics based on the frequency spectra obtained from the Fast Fourier Transform (FFT) of the cross-entropy sequences of language data, named FACE (_F_ourier _A_nalysis of _Cross-Entropy_). 2. We empirically show FACE's performance on identifying human-model gap and how it scales with model sizes in Section 4.1. 3. We exploreFACE's correlations with sampling methods and human evaluation in Section 4.2 and Section 4.3, respectively. 4. We validate the statistical soundness of FACE in Section 4.4. 5. We discuss an intuitive interpretation of the metrics and how it reflects the characteristics of language use in Section 4.5. 6. Implementation and experiments code are available in this public repository: https://github.com/CLCS-SUSTech/FACE.

## 2 Face

The basic idea of FACE is to obtain the spectra of cross-entropy from different data sources (human or models) and compute their similarities. The overall workflow is shown in Figure 1, which we describe in five steps:

1. Collect the datasets for human-written and model-generated texts, \(\mathcal{D}_{h}\) and \(\mathcal{D}_{m}\).
2. Use a third pre-trained language model \(m_{\text{est}}\) to estimate the cross-entropy of text in \(\mathcal{D}_{h}\) and \(\mathcal{D}_{m}\), resulting in two sequences of cross-entropy output, \(\mathcal{E}_{h}\) and \(\mathcal{E}_{m}\).
3. Obtain the frequency spectra for each cross-entropy sequences, \(\mathcal{E}_{h}\Rightarrow\mathcal{F}_{h}\) and \(\mathcal{E}_{m}\Rightarrow\mathcal{F}_{m}\).
4. Develop FACE metrics that quantify the spectral similarity between \(\mathcal{F}_{h}\) and \(\mathcal{F}_{m}\).
5. Evaluate FACE on different model types/sizes, sampling methods, and the correlations with other metrics for Natural Language Generation (NLG).

We describe the steps in detail from Section 2.1 to Section 2.3.

### Estimate cross-entropy

We use a pre-trained language model \(m_{\text{est}}\) as the estimator for cross-entropy, which runs in the evaluation model (no gradients produced). It takes as input a sequence of \(T\) tokens, \([t_{1},t_{2},\dots,t_{T}]\); for each position \(i=1,\dots,T\), it predicts the probability of the next token \(P(t_{i+1}|t_{1},\dots,t_{i})\); the cross-entropy between this probability and the ground truth token \(t_{i+1}\) is then computed, resulting in the cross-entropy sequence that consists of \(T-1\) real values \(\mathcal{E}=[c_{1},c_{2},\dots,c_{T-1}]\), as the first token is not predicted:

\[\mathcal{E}=[c_{1},c_{2},\dots,c_{T-1}]\triangleq[-\log P(t_{2}|t_{1}),-\log P (t_{3}|t_{1},t_{2}),\dots,-\log P(t_{T}|t_{1},t_{2},\dots,t_{T-1})]\] (1)

Note that \(\sum c_{i}=-\sum_{i=2}^{T}\log P(t_{i}|t_{1}\dots t_{i-1})\) is exactly the definition of negative log-likelihood loss, i.e., cross-entropy loss, for training a language model, where \(c_{i}\) is the negative logarithm of the predicted probability for each token \(t_{i+1}\). In psycholinguistic studies, this \(c_{i}\) quantity is usually referred to several different terms, including _surprisal_[16; 17], _information density_[25; 20; 48], and _entropy_[13; 14; 46; 47], each of which has a specific theoretical flavor. There have been debates over the justifiability of using "entropy" to denote the negative log-likelihood, because it is not a weighted summation as originally defined in [37]. Albeit, we decide to use _cross-entropy_ as it is the most broadly communicated term and we believe it will not cause confusion as its mathematical form is clearly defined. Apparently, the choice for \(m_{est}\) will influence the next steps, because better

Figure 1: Overall workflow of this study.

language models produce lower perplexity scores, that is, lower cross entropy. Therefore, we discuss how different choices for \(m_{est}\) affect our metrics in Section 4.4.

### Fast Fourier transform

We treat the estimated cross-entropy sequence \([c_{1},\dots,c_{T-1}]\) as a finite discrete signal in the time domain, where the sampling interval is approximated with the average duration of one token. With this simplified assumption, we find that the discrete Fourier transform (DFT) is the most suitable spectral analysis tool [39]3. The formula for DFT is as follows:

Footnote 3: https://ccrma.stanford.edu/~jos/sasp/Fourier_Transform_Continuous_Discrete_Time_Frequency.html

\[X(\omega_{k})\triangleq\sum_{n=0}^{N-1}x(t_{n})e^{-j\omega_{k}t_{n}},\ k=0,1, \dots,N-1\] (2)

in which \(x(t_{n})\) is the signal at time \(t_{n}\), corresponding to the \(n\)-th cross-entropy value \(c_{n}\) (\(n=1\dots,T-1\) and \(N\triangleq T-1\)). \(X(\omega_{k})\) is a complex number that reflects the magnitude (strength) of the \(k\)-th frequency component \(\omega_{k}=2\pi k/N\). In practice, DFT is implemented with an efficient algorithm known as Fast Fourier Transform [5] that runs in \(O(n\log n)\) time.

We compared two methods, periodogram and vanilla FFT. The periodogram approach computes the Fourier transform after applying auto-correlation and time-averaging windows to the signal for de-noising purposes [43]. However, we think de-noising is inappropriate because our "signal" is a time series of cross-entropy, whose value reflects the sampling result at each time step from a large. Auto-correlation or time averaging will remove the distinctiveness of rare tokens. Therefore, we use vanilla FFT and take the _real_ part of \(X(\omega_{k})\) to represent the magnitude spectrum for the frequency component \(\omega_{k}\), which is written as \(X(\omega_{k})\) for brevity.

For an input cross-entropy sequence \(\mathcal{E}=[c_{1},\dots,c_{T-1}]\) obtained from Section 2.1, the resulting frequency spectrum can be represented as a list of tuples of the same length, \(\mathcal{F}=[\langle\omega_{1},X(\omega_{1})\rangle,\dots,\langle\omega_{T-1},X(\omega_{T-1})\rangle]\), where \([\omega_{1},\dots,\omega_{T-1}]\) are the \(T-1\) sample frequencies, and \([X(\omega_{1}),\dots,X(\omega_{T-1})]\) are the corresponding magnitudes.

### Spectral similarity metrics

We develop four metrics to measure the similarity between spectra \(\mathcal{F}_{h}\) and \(\mathcal{F}_{m}\): Spectral Overlap (_SO_), Spectrum Angle Mapper (_SAM_) [6], Pearson's correlation (_CORR_), and Spearman's correlation (_SPEAR_), as summarized in Figure 2. Before computing the metrics, two spectra \(\mathcal{F}_{h}\) and \(\mathcal{F}_{m}\) which are of different lengths \(N_{1}\) and \(N_{2}\), are first interpolated to the same length: \(\mathcal{F}_{h}\in\mathbb{R}^{N_{1}}\Rightarrow\mathcal{F}_{h}^{\prime}\in \mathbb{R}^{N_{C}}\), \(\mathcal{F}_{m}\in\mathbb{R}^{N_{2}}\Rightarrow\mathcal{F}_{m}^{\prime}\in \mathbb{R}^{N_{C}}\). Here, \(N_{C}\) is the maximum length of the spectrum in our data. Thereafter, the computation of the subsequent metrics can commence.

**Spectral Overlap (_SO_)** is inspired by the power spectrum overlap proposed in [28], which is used in [47] for measuring the spectral similarity between dialogue participants. The frequency magnitudes in \(\mathcal{F}_{h}^{\prime}\) and \(\mathcal{F}_{m}^{\prime}\) are converted to absolute values, i.e., \(X(\omega_{k})\Rightarrow|X(\omega_{k})|\), and then compute the Area-Under-Curve (AUC) for the interaction \(\mathcal{F}_{h}^{\prime}\cap\mathcal{F}_{m}^{\prime}\) and the union \(\mathcal{F}_{h}^{\prime}\cup\mathcal{F}_{m}^{\prime}\), respectively. _SO_ is defined as the ratio of the two: \(SO=\text{AUC}(\mathcal{F}_{h}^{\prime}\cap\mathcal{F}_{m}^{\prime})/\text{AUC }(\mathcal{F}_{h}^{\prime}\cup\mathcal{F}_{m}^{\prime})\). The procedure of converting

Figure 2: Definitions of four FACE metrics.

to absolute values is indispensable, since negative values in \(X(\omega_{k})\) will result in negative AUCs. \(SO\) has the range \([0,1]\), and a higher value indicates a stronger resemblance between the two spectra.

**Spectrum Angle Mapper (_SAM_)** calculates the angles between \(\mathcal{F}^{\prime}_{h}\) and \(\mathcal{F}^{\prime}_{m}\), treating them as two vectors in a space [22]. The angle is measured in radians, which is calculated by the inverse function \(\arccos(\mathcal{F}^{\prime}_{h}\cdot\mathcal{F}^{\prime}_{m}/||\mathcal{F}^{ \prime}_{h}||\cdot||\mathcal{F}^{\prime}_{m}||)\), producing a value within \([0,\pi]\). We understand _SAM_ is equivalent to the cosine similarity score, which is more commonly-used in NLP, but here we just follow the conventions in [22; 2]. A smaller _SAM_ value indicates a greater similarity between \(\mathcal{F}^{\prime}_{h}\) and \(\mathcal{F}^{\prime}_{m}\).

**Pearson's correlation (_CORR_)** can also be leveraged to measure spectral similarities as discussed in [22]. \(\textit{CORR}=cov(\mathcal{F}^{\prime}_{h},\mathcal{F}^{\prime}_{m})/\sigma( \mathcal{F}^{\prime}_{h})\sigma(\mathcal{F}^{\prime}_{m})\), with a \([-1,1]\) range. A positive _CORR_ value indicates high similarity (negative for dissimilarity), and 0 indicates weak correlation between \(\mathcal{F}^{\prime}_{h}\) and \(\mathcal{F}^{\prime}_{m}\).

**Spearman's correlation (_SPEAR_)**[41] is commonly used to assess the monotonic relationship between the comparison and reference groups and to capture the presence of non-linear associations between the two. It has not been used for spectral similarity to the best of our knowledge, but we test it in our experiments. _SPEAR_ also has the range \([-1,1]\) with meanings similar to _CORR_.

## 3 Related Work

**Entropy as a metric in psycholinguistics.** The entropy of human language has long been a research interest in computational linguistics and psycholinguistics. The entropy of written text is estimated with the average per-word negative log-probability in sentences, and then used to validate the principle of _entropy rate constancy_ (ERC) in human language [13; 14]. Similar studies were conducted in dialogue [46; 32]. Entropy is also defined in probabilistic grammars to describe the capacity of a language [40], and is used to develop complexity metrics to measure the cognitive load of processing syntactic expressions [16; 24; 17]. In the line of work on language production, a different term _information density_ with the same mathematical formulation is used instead of entropy. It is found that speakers reduce syntactic complexity when the information density (or entropy) is high [25; 20]. In parallel with the concept of ERC, this line of work summarizes the tendency of distributing information evenly in human language with the term _uniform information density_ (UID), which is commonly used as a equivalent term as ERC, for example, in [48; 15]. In conclusion, entropy is commonly used as a metric for essential properties of human language.

**Periodical change of cross-entropy in language.** We draw inspiration from the following studies about the distribution of information in dialogue. Humans are sensitive to the _peaks_ and _troughs_ of entropy in speech, with evidence from human-system dialogues and crowd-sourced ratings from human judges [7]. The entropy of utterances from two speakers converge towards each other within the scope of topical segments in spontaneous dialogues [48]. They measure the entropy of utterances from two participants of a task-oriented dialogue, and have found that the frequency domain features - power spectrum overlap and phase delay - are useful predictors of task outcomes. Both works reviewed above suggest that the periodical up-and-downs of entropy are commonly observable in the human language. It naturally leads to the question of whether and to what extent model-generated language aligns with this empirical finding.

**Automatic measures for text generation.** Entropy and its related variants have already used as a metric for evaluating generated text, for instance, entropy provides good visualization for the difference between GPT2-generated text and human written ones [12]. Other than entropy, there is a rich body of existing metrics targeted on discriminating human-written text and model-generated text, which we summarize in three branches: (1) statistics-based; (2) language modeling; (3) reference-based. Table 1 gives a brief summary of these three categories, as well as our proposed frequency-based FACE.

_Statistics-based measures_ compare the model-generated distribution \(M\) with respect to the human-written distribution \(H\) in terms of some statistic. The Zipf coefficient [30] is used in [19] to describe the distribution of word frequencies in text. Self-BLEU [52] is derived by calculating the BLEU [29] score for each generated text utilizing all other generations as references. Repetition measures the sequence-level degree of repetition on the basis of the percentage of duplicated n-grams in the generated continuations \(\bm{x}_{\text{cont}}\sim M\)[44]. Meanwhile, we aggregate the 2-gram, 3-gram, and 4-gram repetition rates to evaluate the lexical diversity in an inverse manner.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

speaking, _SO_ and _SAM_ show a higher positive correlation to MAUVE than _CORR_ and _SPEAR_, given that seven out of nine voting results (marked with yellow in Table 3) are identical.

To further evaluate model sizes, we apply FACE to the original GPT2 output data (webtext) 4 generated from GPT2-sm and GPT2-xl. GPT2-xl has a higher _SO_ score than GPT2-sm, which is confirmed with the \(t\)-test, but non-significant effects are found on the other three metrics. Combining our generation task with the original GPT2 data, we illustrate the results for _SO_ in Figure 3.

Footnote 4: https://github.com/openai/gpt-2-output-dataset

To conclude, we discover three keypoints: (1) FACE is consistent with MAUVE in evaluating three different model types (two sizes for each); (2) the metrics estimating similarity between human-written and model-generated text generations (e.g., FACE, MAUVE) may produce opposite results to the text-centered metrics (e.g., Diversity, Coherence); (3) the four metrics of FACE show relatively homogeneous results, and using these metrics together helps to identify model-generated texts with a more comprehensive evaluation.

### Sampling methods

Recent work [19; 26] has indicated three clear trends in open-ended text generation using auto-regressive LMs: (1) maximization-based decoding algorithms (e.g., beam search, greedy decoding, etc.) lead to copious repetition, while sampling with temperature may result in incoherence; (2) truncation-based sampling methods like nucleus sampling produce text with higher quality; (3) contrastive decoding outperform nucleus sampling in terms of both fluency and coherence. Accordingly, to demonstrate the effectiveness of our approach, FACE should follow the inequality: maximization-based/temperature-based \(\prec\) nucleus \(\prec\) contrastive in terms of the quality relationship.

Figure 4 visualizes the correlation between FACE scores and various decoding algorithms. The contrastive decoding approach yields the best performance among the four FACE metrics. It can be clearly observed that the maximization-based sampling methods behave worse than other algorithms. Moreover, adding the temperature parameter to top-\(k\) sampling results in incoherent text generations,

Figure 4: FACE scores (conditional generation) on original experimental data of [19] and [26]. Nine sampling methods are compared: greedy, beam search, stochastic beam search, pure sampling, temperature, top-\(k\), top-\(k\) with temperature, nucleus, and contrastive. Note that logarithmic normalization on parameter values as well as enlarged markers for greedy decoding, pure sampling, and contrastive decoding are adopted for better visualization effect. Best viewed when zoomed in.

Figure 3: FACE-_SO_ scores on OPT, BLOOM and GPT2 original output data. Model sizes compared: small vs. large for OPT and BLOOM; -sm vs. -xl for GPT2. Error bars represent 95% confidence intervals from bootstrap. The significant levels are based on \(t\)-test between the two model-size groups.

which explains the gap between the red curve (top-\(k\) w/o temperature) and the gray curve (top-\(k\) w/ temperature). We also plot the correlation graphs of unconditional generation (in the Appendix) with fewer sampling methods involved. The trends and patterns in the visualization of unconditional generation are basically consistent with its conditional counterpart.

In Table 4, FACE scores on different decoding algorithms are summarized. FACE metrics correctly match the expected quality relationship of the sampling methods examined by assigning the best _SO_ (\(.44\)), _CORR_ (\(.75\)), _SAM_ (\(.23\)), and _SPEAR_ (\(.17\)) scores to contrastive decoding. Other evaluation metrics fail to capture the correct relationship, for example, the perplexity rates nucleus-sampled text as better than contrastive-decoded text, which is irrational suggested by Li et al. [26].

### Human judgments

We also explore the correlation between FACE and human judgement scores, using the crowd-source dataset collected in [31] when human evaluation is available. The dataset contains model-generated continuations (by GPT2-sm, -md, -lg, and -xl with ancestral and nucleus sampling), human-written continuations using the same prefix, and the crowd-source workers' answers on which completion is more human-like, interesting, and sensible. We follow the same experimental settings and protocol to verify whether the FACE scores of the text completions correlate well with the human quality judgements by computing the Spearman's rank correlation coefficient. The results are presented in Table 5.

We observe a high and positive correlation between FACE-_SO_ and human judgments scores, which outperforms five out of the six evaluation metrics reported in [31] and achieves a comparative performance against MAUVE. The remaining three FACE metrics have insignificant correlations. However, we consider human judgments to be subjective and sometimes biased. Including more fine-grained questions to perform human judgments may lead to more accurate correlation statistics. Additionally, we recomputed the correlations with human judgement scores to keep those pairs in which there are exactly one item from human and the other item from model (i.e., a subset of data used for the analysis in Table 5). As shown in _SO_-S and MAUVE-S columns, FACE-_SO_ has a stronger correlation than MAUVE among two of these three dimensions.

### Sanity tests

**Sanity test on validity.** We evaluate the validity of FACE by examining whether its scores on human-human split is lower than human-model groups - an expected result based on our assumption that the spectral difference between human's "natural" language and models' "artificial" ones should be _amplified_ by FACE. Therefore, the sanity tests are conducted as follows: first, we evenly and randomly split the human data into two folds (across three domains) to serve as control groups. The FACE scores between these control folds are then computed. As for the human-to-model experimental group, we create other two folds using human data and the best model-generated data (from contrastive decoding [26]) in terms of text quality. If FACE can effectively capture the fundamental difference

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Sampling Method** & **Perplexity** & **Self-BLEU** & **Zipf Coefficient** & **Repetition** & _SO_ (\(\uparrow\)) & _CORR_ (\(\uparrow\)) & _SAM_ (\(\downarrow\)) & _SPEAR_ (\(\uparrow\)) \\ \hline \hline Human & 12.38 & 0.31 & 0.93 & 0.28 & - & - & - & - \\ \hline Greedy & 1.50 & 0.50 & 1.00 & 73.66 & 0.20 & 0.56 & 0.31 & 0.04 \\ Beam(=16) & 1.48 & 0.44 & 0.94 & 28.94 & 0.21 & 0.31 & 0.40 & 0.04 \\ Stochastic Beam (=16) & 19.20 & 0.28 & 0.91 & 0.32 & 0.37 & 0.49 & 0.33 & 0.04 \\ \hline Pure Sampling & 22.73 & 0.28 & **0.93** & 0.22 & 0.41 & 0.63 & 0.28 & 0.03 \\ Sampling (\(t\)=0.9) & 10.25 & 0.35 & 0.96 & 0.66 & 0.42 & 0.61 & 0.29 & 0.03 \\ Top- (\(k\)=40) & 6.88 & 0.39 & 0.96 & 0.78 & 0.40 & 0.64 & 0.28 & 0.03 \\ Top- \(k\) (=640) & 13.82 & **0.32** & 0.96 & **0.28** & 0.42 & 0.63 & 0.28 & 0.03 \\ Top- \(k\) (=40, \(t\)=0.7) & 3.48 & 0.44 & 1.00 & 8.86 & 0.34 & 0.61 & 0.29 & 0.03 \\ Nucleus (\(p\)=0.95) & **13.13** & **0.32** & 0.95 & 0.36 & 0.42 & 0.63 & 0.28 & 0.03 \\ Contrastive Decoding & 14.39 & 0.54 & 1.04 & 0.24 & **0.44** & **0.75** & **0.23** & **0.17** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for comparing all sampling methods with selected parameters regarding the conditional generation. The values _closest to human scores_ are **bolded**, except for our proposed FACE scores, where the _highest (for SO, CORR, and SPEAR) or the lowest (for SAM)_ values are in **bold**.

between human and model languages, then we expect to observe higher scores in control groups than in the experimental group. The results are shown in Table 6.

It can be seen from Table 6 that the control groups show significantly better FACE scores than the experimental group: FACE-_SO_ and FACE-_CORR_ are higher in human-to-human folds, while FACE-_SAM_ scores are lower. The only exception is FACE-_SPEAR_, while we will show it is a good metric in later sections. Nonetheless, these tabulated results have proved the validity of FACE in effectively capturing human-to-model spectral differences.

**Choice of estimator model.** We examine how different choices of the estimator model \(m_{\text{est}}\) affect the resulting spectra, using GPT2-sm, -md, -lg and -xl as \(m_{\text{est}}\), respectively. The spectra of webtext and the original GPT2 output data are computed. It is found that the spectra obtained from \(m_{\text{est}}\) have different magnitudes, but their aggregated curves have the same shape (see Appendix). Therefore, the choice of \(m_{\text{est}}\) will not affect FACE scores as long as the same \(m_{\text{est}}\) is used for all data.

**Stationarity tests.** One of the assumptions of the Fourier transform is that the signal is _stationary_[21], i.e., the mean and variance do not change over time. We applied the Augmented Dickey-Fuller (ADF) test [8] to examine the stationarity of the cross-entropy sequences for all the human and model-generated data used in this study. The null hypothesis \(H_{0}\) of the ADF test is non-stationarity, and thus a \(p<.05\) testing result rejects \(H_{0}\) and accepts the alternative hypothesis of stationarity in the series. We calculate the proportions of cross-entropy sequences that pass the ADF test with \(p<.05\) for all model-generated and human data: 97.4% for GPT2, 92.1% for OPT, 74.5% for BLOOM, and 97.9% for human. Thus, the majority of data meets the stationarity requirement.

### Interpretation of spectra

As the frequency spectrum reflects the key characteristics of a signal, we attempt to interpret the spectra to see if they tell how the "signals" - entropy of human and machine languages - differ. Without aggregation, the raw spectra of single cross-entropy sequence look indistinguishable between GPT2-sm, GPT2-xl, and human (see the left plot in Figure 5). By aggregating 5,000 spectra from each group and smoothing the curves, it can be seen that GPT2-xl's curve is closer to human than the GPT2-sm curve (readers can find this by zooming in the middle plot in Figure 5). Here, the smoothing is done with generalized additive models (GAMs) [45]. Results from other models are included in the Appendix.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & _SO_ (\(\uparrow\)) & _CORR_ (\(\uparrow\)) & _SAM_ (\(\downarrow\)) & _SPEAR_ (\(\uparrow\)) \\ \hline \hline h-h (wiki) & **0.45** & **0.76** & **0.22** & 0.05 \\ h-h (news) & **0.45** & **0.76** & **0.22** & 0.07 \\ h-h (stories) & **0.47** & **0.79** & **0.21** & 0.05 \\ h-m & 0.44 & 0.75 & 0.23 & **0.17** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of sanity test on FACE’s validity. The top three rows are the control groups, and “h-h” stands for human-to-human folds. The last row is the experimental group, where “h-m” is for human-to-model fold. Better FACE scores are in bold. The scores in the bottom row are retrieved from Table 4.

\begin{table}
\begin{tabular}{l c c c c c c c|c c} \hline \hline
**Metric** & **Generation Perplexity** & **Zipf Coefficient** & **Repetition** & **Distinct-4** & **Self-BLEU** & _SO_ & **MAUVE** & _SO-S_ & **MAUVE-S** \\ \hline \hline Human-like/BT & 0.810 & 0.833 & \(-0.167\) & 0.738 & 0.595 & 0.881 & **0.952** & **0.357** & \(0.214\) \\ Interesting/BT & 0.643 & 0.524 & \(-0.143\) & 0.524 & 0.405 & 0.762 & **0.810** & \(0.524\) & **0.667** \\ Sensible/BT & 0.738 & 0.690 & \(-0.071\) & 0.595 & 0.524 & 0.788 & **0.857** & **0.995** & \(0.706\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Spearman’s rank correlation coefficients of _SO_ and five other metrics with human judgments. Higher scores mean better correlation. All the numbers except the _SO_, _SO_-S, and MAUVE-S columns are sourced from [31]. “BT” denotes the Bradley-Terry score of the pairwise human evaluation, which is employed to compute the Spearman’s rank correlation with the scores of other metrics. Additionally, it is important to note that the original human judgments encompass certain pairs in which both texts are generated by models, albeit different models. Therefore, we refine the original human judgment dataset to only include judgments involving both human and model-generated languages, and the results are shown in _SO_-S and MAUVE-S.

When plotted separately, the aggregated spectra from human and different models have similar shapes: First, the majority of components exist in the low-frequency range (\(\omega<0.05\)). In addition, the locations of peaks and troughs are almost the same between groups. For instance, \(\omega_{1}=0.06\) is the first trough, and \(\omega_{2}=0.12\) is the first peak (see the right plots in Figure 5). Thus, roughly speaking, the main difference between human and model spectra is not in the locations of peak and trough frequencies but in the relative magnitudes of those frequencies.

We propose a simple way to interpret the peaks in spectra: the reciprocal of a frequency component \(T_{k}=1/\omega_{k}\) denotes the corresponding cycle in the time domain. Because the time interval (i.e., sampling interval) of an entropy sequence is not measured in _seconds_ but fixed as one _token_, the measurement unit of \(T_{k}\) is also in number of tokens. For example, the first frequency peak in Figure 5 (right plot) implies \(\omega_{2}=0.12\Rightarrow T_{2}=1/0.12\approx 8.3\) (tokens), which approximately means that tokens of the same cross-entropy levels tend to _recur_ every 8.3 tokens. This pattern is consistent in both human and model data. However, the degree of this _recurrence_ can mark the difference between the human and model languages. We leave more detailed interpretations of spectra to future work.

## 5 Conclusion and Limitations

We propose FACE, a set of metrics based on the Fourier analysis of cross-entropy, which is able to distinguish human and model-generated language with satisfactory performance in the open-ended generation task. The metrics scale with model sizes; reflect the effect of various sampling methods; correlate well with other existing metrics and outperform most of them in alignment with human judgement scores. Among the four implementation methods of FACE experimented, Spectral Overlap (_SO_) has the best overall performance.

FACE is computationally efficient with easy-to-interpret output. As a method inspired by psycholinguistic studies on the predictability (entropy/surprisal/information density) of human language, we believe FACE is a good example of incorporating knowledge from different fields for better human-centered AIs. We can generally conclude that better language models can produce spectral representations of information that are more similar to human.

Our current work has several limitations: Firstly, for open-ended generation experiments (Section 4.1), a broader set of sampling methods other than top-\(k\) can be used. Secondly, larger models (with more than 100 billion parameters) need to be included for more comprehensive comparisons. We will improve from these aspects in future work.

## Acknowledgement

This material is based upon work supported by the National Science Foundation under Grant No. (2105192).

Figure 5: Intuitive observations on the spectra from GPT2 and human data (webtext). **Left**: Spectra of three randomly sampled entropy sequences from GPT2-sm, GPT2-xl, and webtext. **Middle**: Smoothed plot of 5,000 aggregated spectra with absolute values, \(|X_{\omega_{k}}|\sim\omega_{k}\). **Right**: Typical smoothed plot of raw spectra \(X_{\omega_{k}}\sim\omega_{k}\), with peaks and troughs annotated.

## References

* Ahmed et al. [2017] H. Ahmed, I. Traore, and S. Saad. Detection of online fake news using n-gram analysis and machine learning techniques. In _Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments: First International Conference, ISDDC 2017, Vancouver, BC, Canada, October 26-28, 2017, Proceedings 1_, pages 127-138. Springer, 2017.
* Boardman [1992] J. Boardman. Sips user's guide spectral image processing system, version 1.2. _Center for the Study of Earth from Space: Boulder, CO, USA_, 1992.
* Bradley and Terry [1952] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952. ISSN 00063444. URL http://www.jstor.org/stable/2334029.
* Clark et al. [2019] E. Clark, A. Celikyilmaz, and N. A. Smith. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2748-2760, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1264. URL https://aclanthology.org/P19-1264.
* Cooley and Tukey [1965] J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation of complex fourier series. _Mathematics of computation_, 19(90):297-301, 1965.
* De Carvalho and Meneses [2000] O. A. De Carvalho and P. R. Meneses. Spectral correlation mapper (scm): an improvement on the spectral angle mopper (sam). In _Summaries of the 9th JPL Airborne Earth Science Workshop, JPL Publication 00-18_, volume 9, page 2. JPL publication Pasadena, CA, USA, 2000.
* Dethlefs et al. [2016] N. Dethlefs, H. Hastie, H. Cuayahuitl, Y. Yu, V. Rieser, and O. Lemon. Information density and overlap in spoken dialogue. _Computer speech & language_, 37:82-97, 2016.
* Dickey and Fuller [1979] D. A. Dickey and W. A. Fuller. Distribution of the estimators for autoregressive time series with a unit root. _Journal of the American statistical association_, 74(366a):427-431, 1979.
* Djolonga et al. [2019] J. Djolonga, M. Lucic, M. Cuturi, O. Bachem, O. Bousquet, and S. Gelly. Precision-recall curves using information divergence frontiers. In _International Conference on Artificial Intelligence and Statistics_, 2019.
* Fan et al. [2018] A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 889-898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.
* Gao et al. [2021] T. Gao, X. Yao, and D. Chen. SimCSE: Simple contrastive learning of sentence embeddings. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6894-6910, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.552.
* Gehrmann et al. [2019] S. Gehrmann, H. Strobelt, and A. Rush. GLTR: Statistical detection and visualization of generated text. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 111-116, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-3019. URL https://aclanthology.org/P19-3019.
* Genzel and Charniak [2002] D. Genzel and E. Charniak. Entropy rate constancy in text. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 199-206, 2002.
* Genzel and Charniak [2003] D. Genzel and E. Charniak. Variation of entropy and parse trees of sentences as a function of the sentence number. In _Proceedings of the 2003 conference on empirical methods in natural language processing_, pages 65-72, 2003.
* Giulianelli et al. [2021] M. Giulianelli, A. Sinclair, and R. Fernandez. Is information density uniform in task-oriented dialogues? In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 8271-8283, 2021.

* Hale [2001] J. Hale. A probabilistic carley parser as a psycholinguistic model. In _Second meeting of the north american chapter of the association for computational linguistics_, 2001.
* Hale [2016] J. Hale. Information-theoretical complexity metrics. _Language and Linguistics Compass_, 10(9):397-412, 2016.
* Hashimoto et al. [2019] T. B. Hashimoto, H. Zhang, and P. Liang. Unifying human and statistical evaluation for natural language generation. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1689-1701, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1169. URL https://aclanthology.org/N19-1169.
* Holtzman et al. [2020] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.
* Jaeger [2010] T. F. Jaeger. Redundancy and reduction: Speakers manage syntactic information density. _Cognitive psychology_, 61(1):23-62, 2010.
* Kaplan [2001] I. Kaplan. Dft of a non-stationary time series, Sep 2001. URL http://bearcave.com/misl/misl_tech/signal/nonstat/index.html.
* Kruse et al. [1993] F. A. Kruse, A. Lefkoff, J. Boardman, K. Heidebrecht, A. Shapiro, P. Barloon, and A. Goetz. The spectral image processing system (sips)--interactive visualization and analysis of imaging spectrometer data. _Remote sensing of environment_, 44(2-3):145-163, 1993.
* Kynkaanniemi et al. [2019] T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf.
* Levy [2008] R. Levy. Expectation-based syntactic comprehension. _Cognition_, 106(3):1126-1177, 2008.
* Levy and Jaeger [2007] R. Levy and T. Jaeger. Speakers optimize information density through syntactic reduction. In B. Scholkopf, J. Platt, and T. Hofmann, editors, _Advances in Neural Information Processing Systems_, volume 19, pages 849-856, 2007.
* Li et al. [2022] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis. Contrastive decoding: Open-ended text generation as optimization. _arXiv preprint arXiv:2210.15097_, 2022.
* Merity et al. [2017] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In _International Conference on Learning Representations_, 04 2017.
* Oullier et al. [2008] O. Oullier, G. C. De Guzman, K. J. Jantzen, J. Lagarde, and J. Scott Kelso. Social coordination dynamics: Measuring human bonding. _Social neuroscience_, 3(2):178-192, 2008.
* Papineni et al. [2002] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.
* Piantadosi [2014] S. Piantadosi. Zipf's word frequency law in natural language: A critical review and future directions. _Psychonomic bulletin & review_, 21, 03 2014. doi: 10.3758/s13423-014-0585-6.
* Pillutla et al. [2021] K. Pillutla, S. Swayamdipta, R. Zellers, J. Thickstun, S. Welleck, Y. Choi, and Z. Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 4816-4828. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/260c2432a0eec28ce03c10dadc078a4-Paper.pdf.

* Qian and Jaeger [2011] T. Qian and T. F. Jaeger. Topic shift in efficient discourse production. In _Proceedings of the Annual Meeting of the Cognitive Science Society_, volume 33, 2011.
* Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019.
* Sajjadi et al. [2018] M. S. M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. In _Advances in Neural Information Processing Systems_, page 5234-5243, Red Hook, NY, USA, 2018. Curran Associates Inc.
* Scao et al. [2022] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* Sellam et al. [2020] T. Sellam, D. Das, and A. Parikh. BLEURT: Learning robust metrics for text generation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main.704.
* Shannon [1949] C. E. Shannon. Communication theory of secrecy systems. _The Bell System Technical Journal_, 28(4):656-715, 1949.
* Shimanaka et al. [2018] H. Shimanaka, T. Kajiwara, and M. Komachi. RUSE: Regressor using sentence embeddings for automatic machine translation evaluation. In _Proceedings of the Third Conference on Machine Translation: Shared Task Papers_, pages 751-758, Belgium, Brussels, Oct. 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6456. URL https://aclanthology.org/W18-6456.
* Smith [2011] J. O. Smith. _Spectral Audio Signal Processing_. http://ccrma.stanford.edu/~jos/sasp/, accessed 3/14/2023. Online book, 2011 edition.
* Soule [1974] S. Soule. Entropies of probabilistic grammars. _Information and Control_, 25(1):57-74, 1974.
* Spearman [1961] C. Spearman. The proof and measurement of association between two things. _The American Journal of Psychology_, 1961.
* Su et al. [2022] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier. A contrastive framework for neural text generation. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=V88BafmH9Pj.
* Welch [1967] P. Welch. The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modified periodograms. _IEEE Transactions on audio and electroacoustics_, 15(2):70-73, 1967.
* Welleck et al. [2020] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with unlikelihood training. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJeYe0NtvH.
* Wood [2017] S. N. Wood. _Generalized additive models: an introduction with R_. CRC press, 2017.
* Xu and Reitter [2016] Y. Xu and D. Reitter. Entropy converges between dialogue participants: Explanations from an information-theoretic perspective. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 537-546, 2016.
* Xu and Reitter [2017] Y. Xu and D. Reitter. Spectral analysis of information density in dialogue predicts collaborative task performance. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 623-633, 2017.
* Xu and Reitter [2018] Y. Xu and D. Reitter. Information density converges in dialogue: Towards an information-theoretic model. _Cognition_, 170:147-163, 2018.
* Zhang et al. [2022] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.

* Zhang* et al. [2020] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.
* Zhao et al. [2019] W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 563-578, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1053. URL https://aclanthology.org/D19-1053.
* Zhu et al. [2018] Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu. Texygen: A benchmarking platform for text generation models. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 1097-1100, 06 2018. doi: 10.1145/3209978.3210080.

## Appendix

### 1 Broader Impacts

FACE measures the distance between human and model-generated languages, therefore it is technically possible to be used for designing or augmenting systems that mimic humans. We acknowledge the risks of FACE (and other metrics) being utilized in applications that deliberately confuse human-authored and model-produced text. We call for the collective efforts from the community to come up with a systematic framework that unifies different metrics, for developing more reliable and natural language generation systems.

### 2 Implementation Details

**Preprocessing.** We utilize three raw datasets: WritingPrompts, WikiText-103, and RealNews. For WritingPrompts, the prompt set has already been well-curated, so we just extracted the first 5,000 prompts (the length may vary) for our generation task. WikiText-103 and RealNews contain many complete texts. For each complete text, we further truncate it corresponding to the first 35 tokens as a prompt. To fairly evaluate the performance of metrics, we also divide text generations according to five predefined length (from 0 up to 1024) intervals for each dataset. Thereby, the human-written texts and model-produced texts used to evaluate the performance of metrics may be generated by different prompts (i.e., unpaired comparison).

**Hyper-parameters.** We have several hyper-parameters during the text generation and evaluation phases. For both conditional and unconditional generation, we preset a random seed integer (32 by default). Furthermore, the maximum length of each text (1024 by default) as well as the batch size (which varies according to GPUs capacity) for perplexity computation have to be determined before automatic evaluation.

### 3 Miscellaneous Details

**Software.** Our experiments were performed on Ubuntu 20.04.1 system with Python 3.9.16. The versions of key Python libraries include: Transformers 4.27.4, PyTorch-CUDA 11.6, PyTorch 1.13.1, Scipy 1.5.4.

**Hardware.** For the text generation task, we use the remote workstation that has two NVIDIA RTX A6000 graphics cards. It should be noted that all models were run in parallel when available.

**Computation time for text generation.** We spent 10 and 25 hours or so obtaining 5,000 text continuations by GPT2-sm, -xl, respectively. OPT-125m, -6.7b cost our GPU resources roughly 11 and 44 hours to output the same number of text continuations, respectively. When it comes to BLOOM-560m, -7b, they took approximately 18 and 48 hours, respectively, to generate 5,000 continuations per task domain.

**Evaluation time for FACE.** Computation time of four FACE metrics for a single pair of references are: \(5.96\times 10^{-8}\) seconds for _SO_, \(5.01\times 10^{-8}\) seconds for _CORR_, \(4.53\times 10^{-8}\) seconds for _SAM_, and \(4.29\times 10^{-8}\) seconds for _SPEAR_, respectively. The cross-entropy, which should be calculated beforehand, takes \(5.65\times 10^{-2}\) seconds. All of the above measurements take place on an AMD Ryzen Threadripper PRO 3995WX 64-Cores CPU (frequency range \(\in\) [2200.00MHz, 4308.40MHz]). Users can leverage more advanced GPU resources to perform the whole computation process with a faster speed.

### 4 Additional Experimental Results

#### 4.1 Model sizes (generation length)

It should be emphasized that LMs have diverse designs and were pre-trained using different strategies on different datasets, giving them distinct preferences on the generation length. The numbers of text generations in each length interval are summarized in Table 7.

To ensure the consistency of our experiments, we run six LMs separately (using their own tokenizers) with the same prompt sets and settings as described in Table 2 to generate 5,000 pieces of continuations in each domain. Besides, we utilize the GPT2Tokenizer to calculate the numbers of continuations for each interval, which allows us to compare FACE scores with other metrics more objectively, as we believe it is unfair to explicitly compare texts of varying lengths. Then, we compute weighted arithmetic mean to evaluate a model in each domain, by \(s^{\prime}=\sum_{i=1}^{n}\frac{m_{i}}{M}s_{i}\), where \(s^{\prime}\) denotes the weighted mean; \(n\) denotes the number of length intervals; \(m_{i}\) is the number of generated continuations in the length interval \(i\); \(M=\sum_{i=1}^{n}m_{i}\), and \(s_{i}\) means a certain metric value in the interval \(i\).

Figure 6 conveys a more intuitive representation (via bar plots) of Table 3.

### Sampling methods (unconditional generation)

We also carried out experiments on unconditional text generation. Here, the prompt is not required as we generate continuations from a random seed (set to 32 empirically). Four sampling methods, which are greedy decoding, beam search, stochastic beam search, and contrastive decoding, are not involved in this set of experiments.

The results are displayed in Figure 7. The overall trends are same as its conditional counterpart, where the previous quality relationship (maximization-based/temperature-based \(\prec\) nucleus \(\prec\) contrastive) is satisfied. Yet, it is crucial to note that the advantages of top-\(k\) sampling w/o temperature become more obvious compared to the conditional case.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline
**Domain** & **Length Interval** & **GPT2-sm** & **GPT2-xl** & **OPT-125m** & **OPT-6.7b** & **BLOOM-560m** & **BLOOM-7b** \\ \hline \hline \multirow{4}{*}{Wiki text} & 0-200 & 403 & 485 & 964 & 1522 & 4928 & 803 \\  & 201-400 & 571 & 672 & 888 & 929 & 61 & 599 \\  & 401-600 & 251 & 316 & 441 & 417 & 8 & 388 \\  & 601-800 & 260 & 310 & 268 & 285 & 1 & 316 \\  & 801-1024 & 3515 & 3217 & 2439 & 1847 & 2 & 2894 \\ \hline \multirow{4}{*}{News} & 0-200 & 750 & 836 & 844 & 1119 & 4978 & 1371 \\  & 201-400 & 1222 & 1336 & 1220 & 1325 & 20 & 917 \\ \cline{1-1}  & 401-600 & 824 & 759 & 1194 & 939 & 1 & 628 \\ \cline{1-1}  & 601-800 & 584 & 678 & 764 & 593 & 0 & 427 \\ \cline{1-1}  & 801-1024 & 1620 & 1391 & 978 & 1024 & 1 & 1657 \\ \hline \multirow{4}{*}{Stories} & 0-200 & 549 & 745 & 2731 & 3588 & 4924 & 1608 \\ \cline{1-1}  & 201-400 & 625 & 757 & 715 & 501 & 63 & 688 \\ \cline{1-1}  & 401-600 & 296 & 404 & 241 & 176 & 9 & 410 \\ \cline{1-1}  & 601-800 & 241 & 324 & 160 & 95 & 4 & 271 \\ \cline{1-1}  & 801-1024 & 3289 & 2770 & 1153 & 640 & 0 & 2023 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Domain-specific generation length with respect to different **models** (GPT2/OPT/BLOOM) and **model sizes** (one large model and one small model) using top-\(k\) (\(k=50\)) sampling corresponding to five continuous length intervals.

Figure 6: FACE scores of GPT2 (our generated data), OPT, and BLOOM with different model sizes.

### Human judgments

Table 8 shows the FACE scores based on the output texts from MAUVE. Each column of FACE scores is used to compute the Spearman's rank correlation coefficient between a specific FACE metric and Bradley-Terry scores (\(4\) model sizes \(\times 2\) sampling methods \(=8\) scores in total) from one criterion (three criteria correspond to three questions in total).

### Choice of estimator model

We examine how different choices of estimator model \(m_{\text{est}}\) affect the resulting spectra of cross-entropy. Five input data sources are examined (webtext plus four GPT2 original output datasets), on which four different estimator models are applied: \(m_{\text{est}}\in\{\text{GPT2-sm},\text{GPT2-md},\text{GPT2-lg},\text{GPT2-xl}\}\), resulting in \(5\times 4=20\) aggregated spectra curves in Figure 8. It can be found that on the same input data, the spectra from four estimators largely overlap. It indirectly suggests that FACE should be stable across different \(m_{\text{est}}\)s. We leave the full inspection for future work.

### Intuitive interpretation of spectra

As pointed out in Section 4.5, the aggregated spectral shapes from human and different models are nearly identical. A set of higher resolution plots from GPT-xl, OPT, BLOOM and human (webtext) are shown in Figure 9. It can be seen that although the \(X(\omega_{k})\) has different ranges on \(y\)-axis, the \(x\) coordinates of the peaks and troughs are the same.

### Corner Cases

Two examples highlight the difference between our proposed FACE-_SO_ and MAUVE in their ability to recognize human-generated and model-generated texts. In the filtered datasets for human judgments, the average values for FACE-_SO_ and MAUVE are 0.4738 and 0.9549, respectively. In Case 1, human evaluators noted a high level of similarity between the model-generated text and human text, resulting

\begin{table}
\begin{tabular}{l l c c c} \hline \hline
**Model** & **Sampling Method (parameter)** & _SO_ & _CORR_ & _SAM_ & _SPEAR_ \\ \hline \hline GPT2-xl & Nucleus Sampling (\(p\)=0.95) & 0.481 & 0.821 & 0.191 & 0.359 \\  & Ancestral Sampling & 0.472 & 0.807 & 0.199 & 0.331 \\ \hline GPT2-lg & Nucleus Sampling (\(p\)=0.95) & 0.480 & 0.819 & 0.193 & 0.356 \\  & Ancestral Sampling & 0.472 & 0.814 & 0.196 & 0.338 \\ \hline GPT2-md & Nucleus Sampling (\(p\)=0.9) & 0.478 & 0.815 & 0.194 & 0.358 \\  & Ancestral Sampling & 0.462 & 0.813 & 0.197 & 0.310 \\ \hline GPT2-sm & Nucleus Sampling (\(p\)=0.9) & 0.476 & 0.817 & 0.194 & 0.359 \\  & Ancestral Sampling & 0.468 & 0.816 & 0.195 & 0.319 \\ \hline \hline \end{tabular}
\end{table}
Table 8: FACE results based on MAUVE’s original experimental data6.

Figure 7: FACE scores (unconditional generation) on original experimental data5of_nucleus sampling_. Five sampling (decoding) methods are compared: pure sampling, temperature, top-\(k\), top-\(k\) with temperature, and nucleus. Note that logarithmic normalization on parameter values as well as an enlarged marker for pure sampling are adopted for better visualization.

in ties for human-like, interesting, and sensible aspects. However, the MAUVE score in Case 1 is lower than the average value, while the FACE-_SO_ score surpasses its mean. This discrepancy suggests that SO aligns more consistently with human opinions. Conversely, in Case 2, human judgement indicates a significant dissimilarity between the model-generated text and human text, making them easily distinguishable. However, the MAUVE score exceeds its mean, suggesting the two texts are similar to each other. Our FACE-_SO_ score is lower than its mean, indicating better alignment with human opinion.

Figure 8: Aggregated spectra (using GAM smoothing) from four estimator models \(m_{\text{est}}\in\{\text{GPT2-sm},\text{GPT2-md},\text{GPT2-lg},\text{GPT2-xl}\}\). Inputs are from GPT2 original output and webtext.

Figure 9: Aggregated spectra for GPT-xl, OPT, BLOOM, and human (webtext).

Figure 10: Example of two corner cases. For each case, the prompt text, model-generated text, human text, MAUVE and FACE-_SO_ scores, as well as the results from human judgments are tabulated.