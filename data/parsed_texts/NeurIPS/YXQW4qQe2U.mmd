# Optimal Top-Two Method for Best Arm Identification and Fluid Analysis

 Agniv Bandyopadhyay

TIFR Mumbai, India

agniv.bandyopadhyay@tifr.res.in

&Sandeep Juneja

Ashoka University, India

sandeep.juneja2010@gmail.com

&Shubhada Agrawal

Georgia Institute of Technology, USA

sagrawal362@gatech.edu

###### Abstract

Top-\(2\) methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability \(\beta\), and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified \(\delta>0\). Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as \(\delta\to 0\) by computationally demanding plug-in methods. The above top 2 algorithm for any \(\beta\in(0,1)\) has sample complexity within a constant of the lower bound. However, determining the optimal \(\beta\) that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchored at a threshold. If it exceeds the threshold then the algorithm samples the empirical best arm. Otherwise, it samples the challenger arm. We show that the proposed algorithm is optimal as \(\delta\to 0\). Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm. We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution.

## 1 Introduction

Stochastic best arm identification (BAI) problem has attracted a great deal of attention in the multi armed bandit community (see [21], [10], [3] for some early references in BAI). The basic problem involves a finite number of unknown probability distributions or arms that can be sampled from independently and the aim is to identify the arm with the largest mean. We consider a popular fixed confidence version of the problem where the sampling is sequential and the aim is to minimise sample complexity while guaranteeing that the probability of selecting the wrong arm is restricted to a pre-specified \(\delta>0\). Applications are many including in healthcare and recommendation systems. [11] developed asymptotically (as \(\delta\to 0\)) tight lower bound on sample complexity of \(\delta-\)correct algorithms for these BAI problems under the assumption that arms belong to a single parameter exponential family (SPEF). This assumption reduces a probability distribution to a single parameter and allows the analysis to better focus on certain aspects of the problem structure. We retain it for similar reasons. The sample complexity lower bounds involve solving an optimization problem that also identifies optimal proportion of allocations across arms. They also propose a track-and-stopalgorithm that plugs in the empirical estimates of the distribution parameters in the lower bound and tracks the resulting approximations to optimal proportions of arms to sample. Although, this plug-in algorithm was shown to asymptotically match the lower bound, it involves repeatedly solving an optimization problem and is computationally demanding. [14] consider linear Gaussian bandits, [1] consider bandits with general distributions. Both the references propose track and stop algorithms where computation is sped up through batch processing.

Substantial literature has come up on 'top-2' based, alternative faster and intuitively appealing algorithms to identify the best arm (see [25], [26] for Bayesian approaches; [24], [22], [16] for frequentist approaches). The algorithms essentially proceed by identifying at each stage an empirical winner arm, that is, an arm with the largest mean, and its closest challenger. The empirical arm is pulled with probability \(\beta\), and the challenger arm with the complimentary probability. In the frequentist setting, in [16], the challenger arm is the one with the smallest 'index function'. Heuristically, this index function measures the likelihood of the challenger arm actually being the best one. The smaller the index function, more the likelihood. Further, with high probability, the index function increases with increased allocations to the corresponding arm. As is standard (see, e.g., [11], [19]), the algorithm is terminated when the generalized log-likelihood ratio (GLLR, given in Section 3) statistic exceeds a specified threshold. These algorithms are shown to be \(\beta\) optimal in the sense that they match the lower bound on sample complexity satisfied by algorithms that pull the best arm \(\beta\) fraction of times (see [15] for non-asymptotic analysis when \(\beta=1/2\)). However, determining optimal \(\beta\) has been an open problem that has generated considerable activity and that we address in this paper.

**Contributions - Algorithm:** The key insight from index based top-2 algorithm is that once a sample is given to a challenger arm with the smallest index, its index function increases. The net effect is that as the algorithm progresses, the challenger arm indexes tend to come close to each other and move together. We build upon the above insight. Through the first order conditions associated with the lower bound problem, we identify a function \(g\) that equals zero under optimal allocation when the underlying arm distributions are known. We propose an _anchored_ top-2 type algorithm where when \(g>0\), the empirical winner arm is pulled and that tends to decrease \(g\). When \(g<0\), our algorithm pulls a challenger arm (arm with the smallest index function), and that typically increases \(g\). We observe that the indexes of challenger arms that have been pulled, tend to rise up together until they catch up with arms with higher indexes. Once challenger arms associated with all the indexes have been pulled, call this the time to stability, then, since \(g\) is close to zero and indexes of all challenger arms are close together, it can be seen that the proportionate samples to the empirical winner and the remaining arms are close to the optimal proportions as per the lower bound. This continues until the GLLR statistic exceeds a threshold, roughly of order \(\log(1/\delta)\). The time to stability can be bounded from above by a random time with finite expectation independent of \(\delta\), while the time from stability till the GLLR statistic hits a threshold scales with \(\log(1/\delta)\) with a constant that matches the lower bound.

**Fluid model:** Our other key contribution is to capture the above intuitive description through constructing an idealized _fluid_ dynamics where \(g\) stays equal to zero once it touches zero and where the indexes that have been pulled, remain equal and rise together as the algorithm progresses. We further show that the resulting equations have an invertible Jacobian. Implicit function theorem (IFT) (see [20, Appendix A.6] for an introduction to IFT) then becomes an important tool in analyzing this idealized fluid system as it allows the arm allocations \((N_{a}:a\in K)\) to be unique functions of the overall allocation \(N\). IFT further allows us to identify the ordinary differential equations satisfied by the derivatives \(N^{\prime}_{a}=\frac{dN_{a}}{dN_{a}}\) as the allocations \(N\) increase. The overall path till stability is constructed by pasting together the ode paths followed by arm allocations as the set of indexes that have already been pulled and are increasing together with \(N\), meet another higher index. Once all the indexes have been pulled, our ode stabilizes so that the proportions \(N_{a}/N\) thereafter remain constant and equal the optimal proportions as \(N\) increases. IFT further helps show that the proposed algorithm remains close to the fluid dynamics, and matches the lower bound for small \(\delta\). For completeness, in Appendix E.2, we also identify the ode paths under fluid dynamics for \(\beta\) top-2 algorithms. A great deal of technical analysis goes into showing that the algorithm, observed after sufficiently large amount of samples so that the sample means are close to the true means, is close to the fluid process and they both converge to the same limit.

**Other related literature:**[22] also develop a top-2 type algorithm for a single parameter family of distributions. There algorithm decides between the empirical best and the challenger arm based on directional change in a certain index (related to the LB) when the underlying allocation proportionsare perturbed. It is less directly connected to the first order conditions in the LB problem compared to our algorithm. Empirically, we observe that the our proposed algorithm has lower sample complexity, and is computationally substantially faster (Their algorithm can be sub-optimal. We discuss this in Appendix D.2). [7] consider an algorithm structurally similar to ours. They focus on the BAI fixed budget (FB) setting where the total number of samples are fixed and the aim is to allocate samples to minimise the probability of incorrect selection. Unlike the fixed confidence (FC) setting (the one that we consider), the FB setting requires optimizing the first argument of relative entropy functions that appear in the lower bound. In FC setting, the second argument is optimized ([7] vary the first argument). Fundamentally, this is because FB is concerned with sample allocations that control the probability of the data conducting a large deviations to arrive at an incorrect conclusion, while FC is concerned with controlling sample allocations on high probability paths and gathering enough evidence to rule out the likelihood that the observed data is a result of large deviations. Furthermore, [7] prove weaker a.s. convergence results for associated indexes although not for allocations, and since they focus on FB settings, they do not provide sample complexity bounds or probabilistic false selection guarantees. Our analysis is more nuanced and structurally detailed, and we prove that the sample complexity of the proposed algorithm is asymptotically optimal. [28] study the best-\(k\)-arm identification problem in the BAI setting with fixed confidence and bring out the structural complexities that arise in lower bound analysis when \(k>1\). For \(k=1\), they develop an asymptotically optimal top-2 algorithm when arm distributions are restricted to be Gaussian. [27] consider related pure exploration problems using Frank-Wolfe algorithm. Their implementation involves solving a linear program at each iteration. [17], [13], [6] provide algorithms that provide finite \(\delta\) sample complexity guarantees, however they are order optimal and do not match the constant in the lower bound.

Finally, while fluid analysis is common in many settings including mean field analysis and games (e.g., [4]), stochastic approximation (e.g., [5]) and queuing theory (e.g., [8]), to the best of our knowledge little or no work exists that arrives at it through IFT.

**Roadmap:** In Section 2, we describe the problem and develop lower bound related analysis. The proposed algorithm and our main result, Theorem 3.1, demonstrating algorithm's efficacy are stated in Section 3 where we also develop the relevant IFT framework. Section 4 spells out the fluid dynamics associated with the algorithm. Key steps involved in proving Theorem 3.1 are outlined in Section 5. We describe the numerical experiments in Section 6. Detailed proof of all results are in the appendix.

**Key limitations:** The proposed algorithm extends from SPEF to bounded random variables in a straightforward manner. While we do not provide supporting analysis (this limitation is due to space constraints), our numerical results in Appendix J suggest that our algorithm improves upon existing ones even in this setting. As is standard in the bandit literature, we also assume that samples from arm distributions are independent. Further, another limitation is the assumption of stationarity of the underlying distributions. This may be true when relatively short sampling horizons are involved.

## 2 Problem description and lower bound

**Distributional assumption:** As mentioned earlier, we focus on arm distributions that belong to a known SPEF. Let \(\mathcal{S}\subset\mathbb{R}\) denote the open set of possible means of the SPEF under consideration. The details related to SPEF are reviewed in Appendix B.

**Fixed confidence BAI set-up:** Consider an instance with \(K\) unknown probability distributions or _arms_, denoted by the mean vector \(\boldsymbol{\mu}=(\mu_{1},\ldots,\mu_{K})\), where each \(\mu_{i}\in\mathcal{S}\) (we refer to each \(\mu_{i}\) interchangeably as a distribution as well as its mean in the SPEF context). As is standard in the BAI framework, we assume that there is a unique arm with the largest mean. Thus, without loss of generality \(\mu_{1}>\max_{i\geq 2}\mu_{i}\). One way to handle the case where \(2\) or more arms are tied for the largest mean is to look for an \(\epsilon\)-best arm (an arm whose mean is within \(\epsilon\) of the best arm). However, that is technically a significantly more demanding problem (see [12]). Assuming uniqueness of the best arm and focusing on the best arm identification allows us to highlight the simple fluid dynamics underlying the proposed algorithm.

**Algorithm:** Given an unknown bandit instance \(\mu\), we consider algorithms that sequentially generate samples - if \(A_{N}\) denotes the arm pulled at sample \(N\), and \(X_{N}\) denotes the associated reward generated independently from distribution \(\mu_{A_{N}}\), then \(A_{N}\) is chosen sequentially and adaptively as a function of generated \((A_{n},X_{n}:n=1,2,\ldots,N-1)\). Further, an algorithm stops at some finite random stopping time \(\tau\) and announces the best arm. \(\delta-\)**correct algorithm** is an algorithm that, given a \(\delta>0\)stops at time \(\tau_{\delta}>0\) and outputs a best arm estimate \(k_{\tau_{\delta}}\) such that \(\mathbb{P}(\tau_{\delta}<\infty,\ k_{\tau_{\delta}}\neq 1)\leq\delta\). That is, it identifies the arm with highest mean with probability at least \(1-\delta\). Our interest is in identifying a \(\delta\)-correct algorithm that minimizes \(\mathbb{E}[\tau_{\delta}]\). To this end lower bounds on sample complexity of \(\delta\)-correct algorithms are established using, e.g., the data processing inequality (see, e.g., [18]). We see that

\[\inf_{x}\ \ \{\mathbb{E}[N_{1}]d(\mu_{1},x)+\mathbb{E}[N_{a}]d(\mu_{a},x)\} \geq\log(1/(2.4\delta))\]

with \(d(\nu,x)\) denoting the Kullback-Leibler divergence between two distributions in \(\mathcal{S}\) with means \(\nu\) and \(x\), and the expectation is under measure \(\mathbb{P}_{\boldsymbol{\mu}}\) associated with \(\boldsymbol{\mu}\). The infimum above is solved at \(x^{\star}=\frac{\mu_{1}\mathbb{E}[N_{1}]+\mu_{a}\mathbb{E}[N_{a}]}{\mathbb{E }[N_{1}]+\mathbb{E}[N_{a}]}\). With this, we obtain the lower bound \(\mathbb{E}[\tau_{\delta}]\geq T^{\star}(\boldsymbol{\mu})\log\frac{1}{2.4\delta}\), where \(T^{\star}(\boldsymbol{\mu})\) is the reciprocal of the optimal value of a max-min problem,

\[(T^{\star}(\boldsymbol{\mu}))^{-1}\ =\ \max_{\boldsymbol{\omega}=(\omega_{a} :a\in[K])\in\Sigma_{K}}\ \min_{a\neq 1}\ \left(\omega_{1}d(\mu_{1},x_{1,a})+\omega_{a}d(\mu_{a},x_{1,a})\right),\] (1)

where \(x_{1,a}=(\omega_{1}\mu_{1}+\omega_{a}\mu_{a})/(\omega_{1}+\omega_{a})\) and \(\Sigma_{K}\) denotes a simplex in \(K\) dimension.

The popular **plug-in track and stop** algorithm involves solving the max-min problem (1) repeatedly for optimal weights with empirical distribution plugged in for \(\boldsymbol{\mu}\) above. The algorithm at each stage \(t\), generates the next sample from an arm so that the proportion of arms sampled closely match the resulting optimal weights while ensuring an adequate, sub-linear exploration (e.g., each arm gets at least \(\sqrt{t}\) samples at each stage \(t\)).

Propositions 2.1 and 2.2 below are crucial for our analysis. Proposition 2.1 helps in constructing the fluid dynamics in Section 4. Proposition 2.2 provides a characterization of the unique optimal allocation \(\boldsymbol{\omega}^{\star}=(\omega_{a}^{\star}:a\in[K])\) which motivates our algorithm's sampling strategy. Before stating the two propositions, we need some notation. Let \(B\subseteq[K]/\{1\}\), and \(\overline{B}=B\cup\{1\}\). Whenever \(\overline{B}^{c}\neq\emptyset\), let \(\boldsymbol{N}_{\overline{B}^{c}}=(N_{a}\geq 0:a\in\overline{B}^{c})\) denote an allocation of samples to the arms in \(\overline{B}^{c}\) and we treat this as a constant in the following discussion and also in the statement of the two propositions. We define the quantity \(N_{1,1}\) depending on \(\boldsymbol{N}_{\overline{B}^{c}}\) in the following way: **1)** If \(\overline{B}^{c}=\emptyset\) or \(\sum_{a\in\overline{B}^{c}}N_{a}=0\), \(N_{1,1}\) is zero. **2)** Otherwise, \(N_{1,1}\) is the value of \(N_{1}\) at which \(\sum_{a\in\overline{B}^{c}}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}=1\) for the given allocation \(\boldsymbol{N}_{\overline{B}^{c}}\). To see existence of such \(N_{1,1}\), observe that whenever \(\overline{B}^{c}\neq\emptyset\) and \(\sum_{a\in\overline{B}^{c}}N_{a}>0\), the function \(N_{1}\to\sum_{a\in\overline{B}^{c}}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}\) is continuous and it monotonically decreases from \(\infty\) to \(0\) as \(N_{1}\) increases from \(0\) to \(\infty\). Hence, a unique \(N_{1}\) exists where this function equals \(1\). We define the quantity \(N_{\min}=N_{1,1}+\sum_{a\in\overline{B}^{c}}N_{a}\).

**Proposition 2.1**.: _For every positive \(N\) satisfying \(N\geq N_{\min}\), there is a unique set of variables \(\boldsymbol{N}_{\overline{B}}(N)=(N_{a}(N):a\in\overline{B})\) and \(I_{B}(N)\) satisfying the following conditions_

\[\left.\begin{array}{c}\sum_{a\neq 1}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a}) }\ =\ 1,\quad\text{where}\quad x_{1,a}=\frac{N_{1}(N)\cdot\mu_{1}+N_{a}(N)\cdot\mu_{a }}{N_{1}(N)+N_{a}(N)},\quad\sum_{a\in[K]}N_{a}(N)=N,\\ \text{and,}\quad\text{for every}\ a\in B,\quad N_{1}(N)\cdot d(\mu_{1},x_{1,a})+N _{a}(N)\cdot d(\mu_{a},x_{1,a})\ =\ I_{B}(N).\end{array}\right\}\] (2)

_Furthermore, \(\boldsymbol{N}_{\overline{B}}(\cdot)\) and \(I_{B}(\cdot)\) are continuously differentiable w.r.t. \(N\) for \(N>N_{\min}\)._

**Proposition 2.2**.: _Upon taking \(B=[K]/\{1\}\) and \(N=1\), \(\boldsymbol{N}_{\overline{B}}(1)\), as defined in Proposition 2.1 is same as the unique allocation \(\boldsymbol{\omega}^{\star}\) solving the max-min problem in (1). Further, \(I_{B}(1)=T^{\star}(\boldsymbol{\mu})^{-1}\). Moreover, for every \(N>0\), if \(B=[K]/\{1\}\), the unique solution \(\boldsymbol{N}_{\overline{B}}(N)=(N_{a}(N):a\in[K])\) satisfies \(N_{a}(N)=N\omega_{a}^{\star}\)._

Proposition 2.1 is proved by applying the Implicit function theorem (IFT). Proposition 2.2 is subsumed by [11, Theorem 5], but we prove it using a different set of tools by applying the IFT. See Appendix D for the detailed arguments.

For two vectors \(\boldsymbol{\nu}=(\nu_{a}\in\mathcal{S}:a\in[K])\) and \(\boldsymbol{N}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in[K])\) define the _anchor function_, \(g(\boldsymbol{\nu},\boldsymbol{N})\ =\ \sum_{a\in[K]/\{\hat{j}\}}\frac{d(\nu_{\max,\pi_{a}})}{d(\nu_{a},z_{a})}-1,\) where \(\hat{j}=\text{arg}\max_{a}\ \nu_{a}\), \(\nu_{\max}=\max_{a}\nu_{a}\), and \(z_{a}=(N_{j}\nu_{\max}+N_{a}\nu_{a})/(N_{j}+N_{a})\) for all \(a\neq\hat{j}\).

**Remark 2.1**.: It follows from Proposition 2.2 that the anchor function \(g(\boldsymbol{\mu},\boldsymbol{\omega})=0\) and all the indexes \(\omega_{1}d(\mu_{1},x_{1,a})+\omega_{a}d(\mu_{a},x_{1,a})\) equal to each other, uniquely identify the optimal proportion \(\boldsymbol{\omega}^{\star}\) solving the max-min problem (1) (see Appendix D.1 for an easier and more insightful derivation of these conditions). The algorithm proposed in Section 3 ensures that the empirical version of the anchor function \(g(\cdot)\) quickly becomes close to zero and thereafter remains close to zero. Further, the indexes sequentially come close to each other and once they are close, they stay close through the remaining steps of the algorithm.

## 3 Anchored Top-2 (AT2) Algorithm

**Notation:** Some notation is needed to help state the proposed algorithm. For every arm \(a\in[K]\) and iteration \(N\), \(\widetilde{N}_{a}(N)\) denotes the number of times arm \(a\) has been drawn till iteration \(N\), and \(\widetilde{\boldsymbol{N}}(N)=(\widetilde{N}_{a}(N):a\in[K])\). Thus, \(N=\sum_{a}\widetilde{N}_{a}(N)\). Let \(\widetilde{\boldsymbol{\mu}}(N)=(\widetilde{\mu}_{a}(N):a\in[K])\) where \(\widetilde{\mu}_{a}(N)\) denotes the sample mean of arm \(a\) at time \(N\), i.e., \(\widetilde{\mu}_{a}(N)=\sum_{t=1}^{N}\mathbb{I}(A_{t}=a)\cdot X_{t}/ \widetilde{N}_{a}(N)\), and \(\hat{i}_{N}=\text{arg}\max_{a\in[K]}\ \widetilde{\mu}_{a}(N)\), with an arbitrary tie breaking rule.

For every pair of arms \(a,b\), define

\[x_{a,b}(N)\!=\!\frac{\widetilde{N}_{a}(N)\cdot\mu_{a}+\widetilde{N}_{b}(N) \cdot\mu_{b}}{\widetilde{N}_{a}(N)+\widetilde{N}_{b}(N)},\quad\text{ and }\quad \widetilde{x}_{a,b}(N)\!=\!\frac{\widetilde{N}_{a}(N)\cdot\widetilde{\mu}_{a}(N )+\widetilde{N}_{b}(N)\cdot\widetilde{\mu}_{b}(N)}{\widetilde{N}_{a}(N)+ \widetilde{N}_{b}(N)}.\]

Let, \(I_{a,b}(N)=\widetilde{N}_{a}(N)\cdot d(\mu_{a},x_{a,b}(N))+\widetilde{N}_{b}(N )\cdot d(\mu_{b},x_{a,b}(N))\), and \(\mathcal{I}_{a,b}(N)=\widetilde{N}_{a}(N)\cdot d\left(\widetilde{\mu}_{a}(N),\widetilde{x}_{a,b}(N)\right)+\widetilde{N}_{b}(N)\cdot d\left(\widetilde{\mu }_{b}(N),\widetilde{x}_{a,b}(N)\right)\). For \(a\neq\hat{i}_{N}\), we call \(I_{\hat{i}_{N},a}(N)\), and \(\mathcal{I}_{\hat{i}_{N},a}(N)\), respectively, _actual index_ (or, simply _index_) and _empirical index_ of arm \(a\) at iteration \(N\), and denote them using \(I_{a}(N)\), and \(\mathcal{I}_{a}(N)\). For notational simplicity, we hide the dependency on \(N\) whenever it doesn't cause confusion. Note that \(\mathcal{I}_{a}(N)\) is a function of \(\widetilde{N}_{\hat{i}_{N}}(N),\ \widetilde{N}_{a}(N),\ \widetilde{\mu}_{\hat{i}_{N}}(N)\) and \(\widetilde{\mu}_{a}(N)\).

**Stopping Rule:** As is typical in this literature, in our algorithm below, we follow a generalized log likelihood ratio (GLLR) to decide when to stop the algorithm. It is easy to check that \(\min_{a\in[K]/\{\hat{i}_{\mathcal{I}}\}}\mathcal{I}_{a}(N)\) denotes the GLLR, that is log of likelihood function (LF) evaluated at maximum likelihood estimator (MLE) divided by the LF evaluated at MLE of parameters restricted to alternate set with a different best arm compared to MLE (see [11, Section 3.2] for a detailed derivation). Define stopping time \(\tau_{\delta}=\inf\{N|\text{ for all }a\in[K]/\{\hat{i}_{\mathcal{I}}\},\ \mathcal{I}_{a}(N)>\beta(N,\delta)\}\), for an appropriate choice of threshold \(\beta(N,\delta)\). After stopping at \(\tau_{\delta}\), the algorithm outputs \(\hat{i}_{\tau_{\delta}}\) as the best arm. [19, Eq. 25, Section 5.1] argued that for instances in SPEF, upon choosing \(\beta(N,\delta)\approx\log((K-1)/\delta)+6\log\left(\log(N/2)+1\right)+8\log(1 +2\log((K-1)/\delta))\), the GLLR based stopping rule is \(\delta\)-correct for any sampling strategy including the one we propose. In our numerical experiments, we follow [11] and choose a smaller threshold, \(\beta(N,\delta)=\log((1+\log N)/\delta)\).

**Description of the AT2 and Improved AT2 (IAT2) Algorithm:** The AT2 algorithm takes in confidence parameter \(\delta>0\) and exploration parameter \(\alpha\in(0,1)\) as inputs, and executes the following steps at iteration \(N\):

1. Let \(\mathcal{V}_{N}\stackrel{{\text{def}}}{{=}}\{a\in[K]\ |\ \widetilde{N}_{a}(N-1)<N^{\alpha}\}\) be the set of under-explored arms.
2. If \(\mathcal{V}_{N}\neq\emptyset\), choose \(A_{N}=\text{arg}\min_{a\in[K]}\ \widetilde{N}_{a}(N-1)\), and go to step \(5\).
3. Else, if \(g(\widetilde{\boldsymbol{\mu}}(N-1),\widetilde{\boldsymbol{N}}(N-1))>0\), choose the empirically best arm _i.e._\(A_{N}=\hat{i}_{N-1}\), and go to step \(5\).
4. Else, if \(g(\widetilde{\boldsymbol{\mu}}(N-1),\widetilde{\boldsymbol{N}}(N-1))\leq 0\), choose the challenger arm _i.e._\(A_{N}=\text{arg}\min_{a\in[K]/\{\hat{i}_{N-1}\}}\ \mathcal{I}_{a}(N-1)\) using some arbitrary tie breaking rule, and go to step \(5\).
5. Sample \(X_{N}\) from \(A_{N}\) and update \(\widetilde{\boldsymbol{\mu}}(N)\) and \(\widetilde{\boldsymbol{N}}(N)\) using \(X_{N}\), \(A_{N}\).
6. If \(\min_{a\in[K]/\{\hat{i}_{N}\}}\mathcal{I}_{a}(N)>\beta(N,\delta)\), terminate and return \(\hat{i}_{N}\).

Inspired from the Improved Transportation Cost Balancing (ITCB) policy for selecting the challenger arm in [16], Improved AT2 (IAT2) algorithm has the same input and follows the same strategy for exploration (step 1 and 2) and choosing the best arm (step 3) as AT2. IAT2 differs from AT2 only by its choice of the challenger arm in step 4, where IAT2 samples from the arm \(A_{N}=\text{arg}\min_{a\in[K]/\{\hat{i}_{N-1}\}}\ (\mathcal{I}_{a}(N-1)+\log \widetilde{N}_{a}(N-1))\).

Empirically, we see that typically IAT2 performs better than AT2 with respect to sample complexity. In the appendix, we provide pseudo-codes of AT2 and IAT2 in Algorithms 1 and 2, respectively.

### Theoretical guarantees

Proposition 3.1 below shows that the allocations made by AT2 and IAT2 algorithms converge to the optimal allocations \(\bm{\omega}^{\star}\) w.p. 1 in \(\mathbb{P}_{\bm{\mu}}\). For every \(a\in[K]\) we define \(\widetilde{\omega}_{a}(N)=\widetilde{N}_{a}(N)/N\), and use \(\widetilde{\bm{\omega}}(N)=(\widetilde{\omega}_{a}(N):a\in[K])\) to denote the algorithm's proportion at iteration \(N\).

**Proposition 3.1** (**Convergence to optimal proportions**).: _There exists a random time \(T_{stable}\) and a constant \(C_{1}>0\) depending on \(\bm{\mu},\alpha\), and \(K\), and independent of \(\delta\), such that, \(\mathbb{E}_{\bm{\mu}}[T_{stable}]<\infty\), and for every \(N\geq T_{stable}\) and arm \(a\in[K]\),_

\[|\widetilde{\omega}_{a}(N)-\omega_{a}^{\star}|\leq C_{1}N^{\frac{-3\alpha}{8} },\quad\text{and}\quad|\widetilde{\mu}_{a}(N)-\mu_{a}|\leq\epsilon(\bm{\mu})N ^{\frac{-3\alpha}{8}},\]

_where \(\epsilon(\bm{\mu})\) is a positive constant depending only on \(\bm{\mu}\) and defined in Appendix B._

**Theorem 3.1** (_Asymptotic optimality of AT2 and IAT2_).: _Both AT2 and IAT2 are \(\delta\)-correct over instances in \(\mathcal{S}\), and are asymptotically optimal, i.e., for both the algorithms, the corresponding stopping times satisfy, \(\limsup_{\delta\to 0}\frac{E_{\bm{\mu}}[\tau_{s}]}{\log(1/\delta)}\leq T^{\star}( \bm{\mu})\), and \(\limsup_{\delta\to 0}\frac{\tau_{\delta}}{\log(1/\delta)}\leq T^{\star}(\bm{\mu})\) a.s. in \(\mathbb{P}_{\bm{\mu}}\). Moreover, we can find a constant \(C>0\) depending on the instance \(\bm{\mu}\) and \(\alpha\), such that,_

\[\tau_{\delta}\ \leq\ \max\{\ T_{stable},\ T^{\star}(\bm{\mu})\cdot\log(1/ \delta)+C\left(\log(1/\delta)\right)^{1-3\alpha/8}\ \}\quad\text{a.s. in}\quad\mathbb{P}_{\bm{\mu}}.\]

**Proof idea of Theorem 3.1:** We assume Proposition 3.1 and sketch the argument by which asymptotic optimality follows from it. For \(N\geq T_{stable}\), and \(a\in[K]\), from Proposition 3.1, \(\widetilde{N}_{a}(N)\approx\omega_{a}^{\star}N\) and \(\widetilde{\mu}_{a}(N)\approx\mu_{a}\). As a result, from Proposition 2.2, after \(T_{stable}\), \(\mathcal{I}_{a}(N)\approx NI_{[K]/1}(1)=NT^{\star}(\bm{\mu})^{-1}\) for every \(a\neq 1\). Therefore, if \(\min_{a\neq 1}\mathcal{I}_{a}(N)\) crosses \(\beta(N,\delta)\) at \(N=\tau_{\delta}\), since \(\beta(N,\delta)=\log(1/\delta)+O(\log\log(1/\delta)+\log\log(N))\), we have \(\tau_{\delta}T^{\star}(\bm{\mu})^{-1}\approx_{\delta\to 0}\log(1/\delta)+O(\log\log(1/ \delta)+\log\log(\tau_{\delta}))\), which gives \(\limsup_{\delta\to 0}\frac{\tau_{\delta}}{\log(1/\delta)}\leq T^{\star}(\bm{\mu}) \ \mathrm{a.s.}\) in \(\mathbb{P}_{\bm{\mu}}\). Detailed proof is in Appendix H. \(\square\)

We outline the key steps of the proof of Proposition 3.1 for AT2 in Section 5, and the detailed proof is in Appendix G.2. Similar arguments hold for IAT2. Considerable technical effort goes in proving this proposition due to the noise in the empirical estimate \(\widetilde{\bm{\mu}}(N)\), resulting in noise in the anchor function and the empirical indexes. However, before presenting the proof sketch, in the next section, we first observe the algorithm's dynamics in the limiting fluid regime where this noise is zero. Several of the important proof steps for the algorithmic allocations rely on insights from the simpler fluid model.

## 4 Fluid dynamics

**Motivation:** The fluid dynamics idealizes our algorithm's evolution through making assumptions at each iteration \(N\) that hold for the algorithm in the limit as the number of samples increase to infinity. Unlike the real setting with discrete samples, here we treat samples as a continuous object getting distributed between different arms as the sampling budget (also referred to as 'time') evolves. We denote the no. of samples allocated to an arm \(a\in[K]\) at some time \(N>0\) using \(N_{a}(N)\), and define the tuple \(\bm{N}(N)=(N_{a}(N):a\in[K])\). Note that \(\sum_{a\in[K]}N_{a}(N)=N\). The rate \(N_{a}^{\prime}(N)\) at which samples get allocated to arm \(a\) at time \(N\) depends on a continuous version of the AT2 algorithm, which we refer to as the algorithm's fluid dynamics. We define the index of arm \(a\neq 1\) at time \(N\) as, \(I_{a}(N)=N_{1}(N)\cdot d(\mu_{1},x_{1,a}(N))+N_{a}(N)\cdot d(\mu_{a},x_{1,a}(N ))\), where \(x_{1,a}(N)=\frac{N_{1}(N)\mu_{1}+N_{a}(N)\mu_{a}}{N_{1}(N)+N_{a}(N)}\). Notice that \(I_{a}(N)\) defined in Section 3 is the index of arm \(a\) with respect to the algorithm's allocation \(\widetilde{\bm{N}}(N)\), whereas in our current context, \(I_{a}(N)\) represents the index with respect to the fluid allocations \(\bm{N}(N)\).

**Description of the fluid dynamics:** First we explain the fluid dynamics in words. We formally characterize the fluid allocation \(\bm{N}(N)\) via a system of ODEs in Theorem 4.1. Later in this section, we exploit the obtained ODEs to argue that, after starting the fluid dynamics from some time \(N^{0}>0\), the allocations \(\bm{N}(N)\) reach the optimal proportions \(\bm{\omega}^{\star}=(\omega_{a}^{\star}:a\in[K])\) by a time \(\left(\min_{a\in[K]}\omega_{a}^{\star}\right)^{-1}\cdot N^{0}\). In other words, for \(N\geq\left(\min_{a\in[K]}\omega_{a}^{\star}\right)^{-1}\cdot N^{0}\), we have \(N_{a}(N)=\omega_{a}^{\star}\cdot N\) for every arm \(a\in[K]\) irrespective of the initial allocation we had at time \(N^{0}\).

For notational simplicity, we hide the dependency on \(N\), whenever it doesn't cause any confusion. Recall the anchor function \(g(\cdot)\) introduced in Section 2. We use \(g\) to denote \(g(\boldsymbol{\mu},\boldsymbol{N}(N))\). For every subset \(A\subseteq[K]/\{1\}\), we use \(\overline{A}\) to denote \(A\cup\{1\}\).

We start the fluid dynamics at time \(N^{0}>0\) with some initial allocation \(\boldsymbol{N}^{0}=(N_{a}^{0}\geq 0:a\in[K])\). We assume that the vector of true means \(\boldsymbol{\mu}\) is known. The fluid dynamics evolves according to the following steps at a given total allocation \(N\geq N^{0}\): **1)** If \(g>0\), then \(N_{1}\) increases with \(N\) while other \(N_{a}\)'s for \(a\neq 1\) are held constant till \(g=0\) (\(g\) can be seen to be a monotonically decreasing function of \(N_{1}\)). **2)** If \(g=0\), let \(B\) denote the set of minimum indexes. Thus, \(I_{a}(N)\) are equal for all \(a\in B\) (the equal value is denoted by \(I_{B}(N)\)) and \(I_{a}(N)>I_{B}(N)\) for all \(a\in\overline{B}^{c}\). Then, as \(N\) increases, allocations \(N_{1}\) and \((N_{a}:a\in B)\) increase such that \(g\) remains equal to zero, while the indexes in \(B\) remain equal. In Proposition 2.1, we characterize and prove existence of such allocations, which the fluid dynamics will track. Later we observe that, \(I_{B}\) increases atleast at a linear rate and indexes of arms in \(\overline{B}^{c}\) stay bounded from above by a constant. **3)** If \(g<0\), let \(B\) be the set of minimum index arms and \(I_{B}\) be the index of arms in \(B\). In this situation, \((N_{a}:a\in B)\) increase with \(N\) keeping index of the arms in \(B\) equal, while \(N_{1}\) and \((N_{a}:a\in\overline{B}^{c})\) are unchanged. With this \(g\) also increases, since \(g\) is a strictly increasing function of \(N_{a}\) for every \(a\in B\). The dynamics in this case are simple and described in Proposition E. **4)** Once, \(g=0\), and \(B=\{2,\ldots,K\}\), we show that each allocation increases linearly with \(N\) such that \(N_{a}^{\prime}=\omega_{a}^{\star}\).

**The fluid ODEs:** In Appendix E, we argue that if the fluid dynamics has \(g\neq 0\) at time \(N^{0}\), then \(g\) becomes zero within a finite time by following step 1 or step 3 of the description. This is easy to observe when \(g>0\) at \(N^{0}\), because \(g\) is strictly decreasing in \(N_{1}\), and \(g\to-1\) as \(N_{1}\to\infty\). Therefore, following step 1, \(g\) becomes \(0\) at some finite \(N\). We now consider the situation where \(g=0\) at some \(N>N^{0}\). Setting \(B\) to the set of minimum index arms, the algorithm evolves by tracking the allocation \(\boldsymbol{N}_{\overline{B}}(N)=(N_{a}(N):a\in\overline{B})\) defined through the system (2) in Proposition 2.1. By Proposition 2.1, \(\boldsymbol{N}_{\overline{B}}(N)\) is continuously differentiable w.r.t. \(N\). Applying IFT to (2), we obtain the ODEs via which the allocations and the indexes evolve and present them in Theorem 4.1.

**Some definitions:** Let \(f(\boldsymbol{\mu},a,N)=-\frac{\partial}{\partial x}\left(\frac{d(\mu_{1},x)}{ d(\mu_{a},x)}\right)\Big{|}_{x=x_{1,a}}\cdot f(\boldsymbol{\mu},a,\boldsymbol{N})\) is strictly positive because \(\frac{d(\mu_{1},x)}{d(\mu_{a},x)}\) is strictly decreasing with \(x\) for \(x\in(\mu_{a},\mu_{1})\).

Let \(\Delta_{a}=\mu_{1}-\mu_{a}\), and \(h_{a}(\boldsymbol{\mu},N_{1},N_{a})=f(\boldsymbol{\mu},a,\boldsymbol{N}) \frac{N_{1}^{2}\Delta_{a}}{(N_{1}+N_{a})^{2}}\). For notational simplicity, we denote \(h(\boldsymbol{\mu},N_{1},N_{a})\) by \(h_{a}\). Further, for each \(a\), we denote \(d(\mu_{1},x_{1,a})\) by \(d_{1,a}\) and \(d(\mu_{a},x_{1,a})\) by \(d_{a,a}\). Recall that for given allocations \((N_{a}:a\in[K])\), \(B\) denotes a set such that \(N_{1}d_{1,a}+N_{a}d_{a,a}=I_{B}(N)\) for all \(a\in B\), and \(N_{1}d_{1,a}+N_{a}d_{a,a}>I_{B}(N)\) for all \(a\in\overline{B}^{c}\). Let \(h(B)=\sum_{a\in B}h_{a}d_{a,a}^{-1}\), \(h(N)=\sum_{a\in\overline{B}^{c}}h_{a}N_{a}\), and \(d_{B}=\left(\sum_{a\in B}d_{a,a}^{-1}\right)^{-1}\).

**Theorem 4.1** (Fluid ODEs).: _If at total allocation \(N\geq N^{0}\), we have \(g=0\), and \(B\) is the set of minimum index arms, i.e., \(B=\text{arg}\min_{a\in[K]/\{1\}}\ I_{a}(N)\), then the following holds true: 1. As \(N\) increases, and till \(I_{B}(N)\) increases to hit an index in \(\overline{B}^{c}\),_

\[N_{1}^{\prime}\!=\!\frac{N_{1}h(B)}{(N_{1}+\sum_{a\in B}N_{a})h(B)+d_{B}^{-1}h( N)},\ \ \ \text{and}\ \ \ N_{b}^{\prime}\!=\!\frac{N_{b}h(B)+d_{b,b}^{-1}h(N)}{(N_{1}+\sum_{a\in B}N_{a})h( B)+d_{B}^{-1}h(N)},\] (3)

_for all \(b\in B\). It follows that, \(I_{B}^{\prime}(N)=\frac{I_{B}(N)h(B)+h(N)}{(N_{1}+\sum_{a\in B}N_{a})h(B)+d_{B}^{- 1}h(N)}\)._
2. _Furthermore, for_ \(a\in\overline{B}^{c}\)_,_ \(I_{a}^{\prime}(N)=N_{1}^{\prime}d_{1a}=\frac{N_{1}h(B)d_{1a}}{(N_{1}+\sum_{a\in B }N_{a})h(B)+d_{B}^{-1}h(N)}\)_._
3. _There exists a_ \(\beta>0\)_, independent of_ \(N\) _such that_ \(I_{B}^{\prime}(N)>\beta\)_. In addition, for_ \(a\in\overline{B}^{c}\)_,_ \(N_{a}^{\prime}=0\)_,_ \(I_{a}(N)\leq N_{a}^{0}d(\mu_{a},\mu_{1})\)_, thus the index is bounded from above. Thus, if_ \(\overline{B}^{c}\neq\emptyset\)_,_ \(I_{B}(N)\) _eventually catches up with another index in_ \(\overline{B}^{c}\)_. In this way, the set_ \(B\) _grows into_ \(\{2,\ldots,K\}\)_._

**Indexes once they meet must stay together:** In Appendix F.1 we argue via contradiction that in our fluid dynamics, once a set of smallest indexes that are equal, increase and catch up with another index, their union then remains equal and increases together with \(N\). This argument is important as it motivates the proof in our algorithm that after sufficient amount of samples, once a sub-optimal arm is pulled, its index stays close to indexes of the other arms that have been pulled.

**Bounding the time to reach optimal proportion:** We define \(N^{\star}\) to be smallest time after \(N^{0}\) at which the fluid dynamics has both \(B=\{2,\ldots,K\}\) and \(g=0\). Let \((N^{\star}_{a}:a\in[K])\) be the allocation at \(N^{\star}\). We first argue that: _there exists \(i\in[K]\) such that \(N^{\star}_{i}=N^{0}_{i}\)_. We have argued before that if \(g\neq 0\) at \(N^{0}\), then \(g\) becomes zero by some finite time, which we call \(M\). By definition \(M\leq N^{\star}\). Now if \(B\neq\{2,\ldots,K\}\) at \(M\) or \(M=N^{0}\), then after time \(M\) the fluid dynamics evolve by the ODEs in (3) and \(N^{\star}\) is the time at which \(B\) becomes \(\{2,\ldots,K\}\), which is finite by statement 3 of Theorem 4.1. In this case \(i\) is the last element to be added to \(B\). Otherwise if \(B=\{2,\ldots,K\}\) at \(M\) and \(M>N^{0}\), the only way this can happen is \(g<0\) in \([N^{0},M)\). In this case, \(i=1\) and \(M=N^{\star}\). Since \(g=0\) and \(B=\{2,\ldots,K\}\) at time \(N^{\star}\), Proposition 2.2 implies \(N^{\star}_{a}=\omega^{\star}_{a}N^{\star}\) for all \(a\). Combining our observations, we have \(\omega^{\star}_{i}N^{\star}=N^{\star}_{i}=N^{0}_{i}\leq N^{0}\). Hence \(N^{\star}\leq\frac{N^{0}}{\omega^{\star}_{i}}\leq(\min_{a\in[K]}\omega^{\star} _{a})^{-1}N^{0}\). Thus \(N^{\star}\) is within a constant times of \(N^{0}\). We bound \(T_{stable}\) of Proposition 5.1 using a similar argument.

**Remark 4.1** (Incorporating the stopping rule into the fluid dynamics).: At stopping time the idealized GLLR (which is the GLLR defined in Section 3 by replacing the estimated means with the true means) just exceeds \(\log(1/\delta)\). Since the idealized GLLR grows linearly with the allocated samples, the stopping time increases linearly with \(\log(1/\delta)\). Since the time for fluid dynamics to reach stability is independent of \(\delta\), for small \(\delta\), stability will be reached before the algorithm stops.

**Remark 4.2** (\(\beta\)-fluid dynamics).: In Appendix E.2, we construct the fluid dynamics for the \(\beta\)-EB-TCB algorithm [16] using IFT. We prove that, for every \(\beta\in(0,1)\), the \(\beta\)-fluid dynamics started at some time \(N^{0}>0\) reach the \(\beta\)-optimal proportion (which is the solution to the max-min problem 1 with the added constraint \(\omega_{1}=\beta\)) by a time which is a constant times \(N^{0}\).

## 5 Convergence of algorithmic allocations to the optimal proportions

We now outline the proof steps for Proposition 3.1. To simplify our analysis, we analyze the AT2 algorithm after the random time \(T_{0}\) defined as,

\[T_{0}=\inf\Big{\{}N^{\prime}\geq 1\Bigm{|}\forall a\in[K]\text{ and }N\geq N^{\prime},\ |\widetilde{\mu}_{a}(N)-\mu_{a}|\leq\epsilon(\boldsymbol{\mu})\cdot N^{-3 \alpha/8}\Big{\}},\]

after which the estimates \(\widetilde{\boldsymbol{\mu}}(N)\) are converging to \(\boldsymbol{\mu}\). Recall that \(\alpha\in(0,1)\) is the exploration parameter, and \(\epsilon(\boldsymbol{\mu})>0\) is a constant depending only on \(\boldsymbol{\mu}\). By the definition of \(\epsilon(\boldsymbol{\mu})\) in Appendix B, we have \(\widetilde{\mu}_{a}(N)<\widetilde{\mu}_{1}(N)\) for all \(a\neq 1\) and \(N\geq T_{0}\). As a result, arm \(1\) becomes the empirically best arm after \(T_{0}\). In Appendix G.3, we use Chernoff's bound to prove that \(\mathbb{P}_{\boldsymbol{\mu}}(T_{0}=n+1)=\exp(-\Omega(n^{\alpha/4}))\), which implies \(\mathbb{E}_{\boldsymbol{\mu}}[T_{0}]<\infty\). In the following discussion, all the results mentioned are true for both AT2 and IAT2 algorithms.

Proposition 5.1 shows that the allocations made by the proposed algorithm converges to the first order condition satisfied by the optimal proportion \(\boldsymbol{\omega}^{\star}=(\omega^{\star}_{a}:a\in[K])\) at a rate \(O(N^{-3\alpha/8})\), where \(\alpha\) is the exploration parameter.

**Proposition 5.1**.: _There exists a random time \(T_{stable}\geq T_{0}\) satisfying \(\mathbb{E}_{\boldsymbol{\mu}}[T_{stable}]<\infty\) and a constant \(C_{2}>0\) depending on \(\boldsymbol{\mu},\alpha\) and \(K\), and independent of the sample paths, such that, for \(N\geq T_{stable}\)_

\[|g(\boldsymbol{\mu},\widetilde{\boldsymbol{\omega}}(N))|\ =\ \Bigm{|}\sum_{a\neq 1} \frac{d(\mu_{1},x_{1,a}(N))}{d(\mu_{a},x_{1,a}(N))}-1\ \Bigm{|}\ \leq\ C_{2}N^{-3\alpha/8},\] (4)

\[\text{and}\qquad\max_{a,b\in[K]/\{1\}}|I_{a}(N)-I_{b}(N)|\ \leq\ C_{2}N^{1-3\alpha/8}.\] (5)

Before outlining the proof of Proposition 5.1, we explain how Proposition 3.1 follows from Proposition 5.1 just using the IFT.

**Proof idea of Proposition 3.1:** If our algorithm follows optimal proportions at time \(N\), _i.e._, \(\widetilde{N}_{a}(N)=\omega_{a}^{\star}N\) for all \(a\in[K]\), RHS of (4) and (5) becomes zero by Proposition 2.2. Moreover, by Proposition 2.2\(\boldsymbol{\omega}^{\star}\) uniquely satisfies the conditions: anchor function is zero and all alternative arms have equal index. (4) and (5) imply that, \(\widetilde{\boldsymbol{\omega}}(N)\) satisfies these conditions upto a perturbation of \(C_{2}N^{-3\alpha/8}\) after \(T_{stable}\). Using the IFT, we prove that the algorithm's allocation is a Lipschitz continuous function of the perturbation when it is sufficiently small. Hence, by choosing \(T_{stable}\) large enough and using Lipschitzness, we get \(\max_{a\in[K]}|\widetilde{\omega}_{a}(N)-\omega_{a}^{\star}|=O(N^{-3\alpha/8})\). Closeness of \(\widetilde{\boldsymbol{\mu}}(\cdot)\) to \(\boldsymbol{\mu}\) follows from the fact that \(T_{stable}\geq T_{0}\). \(\square\)

**Proof idea of Proposition 5.1:** We separately outline the proofs of (4) and (5) in Proposition 5.1. In the following discussion, constants hidden in \(O(\cdot),\Omega(\cdot)\) and \(\Theta(\cdot)\) notations are independent of the sample path after time \(T_{stable}\). To simplify our analysis, we choose \(T_{stable}\) such that exploration stops after \(T_{stable}\), _i.e._, \(\mathcal{V}_{N}=\emptyset\) for \(N\geq T_{stable}\) (see the discussion before Definition G.1 in Appendix G.1.1 for justification).

_Key ideas in the proof of (4)_: We prove (4) via induction. We prove the existence of a constant \(D>0\) such that at every \(N\geq T_{stable}\), whenever the actual anchor value \(g(\boldsymbol{\mu},\widetilde{N}(N))\) (we denote using \(g(N)\)) satisfies \(|g(N)|>DN^{-3\alpha/8}\), our algorithm pushes \(g(\cdot)\) towards zero by \(\Theta(1/N)\) in the next iteration through steps 3 and 4. Whereas the interval \([-C_{2}N^{-3\alpha/8},C_{2}N^{-3\alpha/8}]\) shrinks by \(O(N^{-(1+3\alpha/8)})\) from both ends. Since \(N^{-(1+3\alpha/8)}<<N^{-1}\), we choose the constant \(C_{2}\) large enough such that \(g(\cdot)\) stays in the said interval in iteration \(N+1\).

_Key ideas in the proof of (5)_: The following lemma forms a crucial part of the argument for proving closeness of the indexes in the non-fluid setting.

**Lemma 5.1**.: _There exists a random time \(T_{good}\in[T_{0},T_{stable}]\) such that the algorithm picks all the alternative arms in \([K]/\{1\}\) atleast once between the iterations \(T_{good}\) and \(T_{stable}\). Moreover, for \(N\geq T_{good}\), if the algorithm picks some arm \(a\in[K]/\{1\}\) at iteration \(N\), then it picks arm a again within a next \(O(N^{1-3\alpha/8})\) iterations._

Proof of Lemma 5.1 (in Appendix G.1.2) is technically involved and requires proving several supplementary lemmas. Several of the key steps of this proof borrow insights from the fluid dynamics, and we outline them in Appendix F. Here we assume Lemma 5.1 and sketch the argument by which (5) follows from it for the AT2 algorithm. For any \(a,b\neq 1\) and after any \(N\geq T_{stable}\), \(\mathcal{I}_{a}(\cdot)\) and \(\mathcal{I}_{b}(\cdot)\) crosses each other before the algorithm picks both \(a,b\) atleast once. We can show that, for \(j=a,b\) and \(N\geq T_{stable}\), \(\mathcal{I}_{j}(N)\) and \(I_{j}(N)\) differ by \(O(N^{1-3\alpha/8})\). As a result, when \(\mathcal{I}_{a}(\cdot)\) crosses \(\mathcal{I}_{b}(\cdot)\) at \(N+R\), we have \(|I_{a}(N+R)-I_{b}(N+R)|=O((N+R)^{1-3\alpha/8})=O(N^{1-3\alpha/8})\) since \(R=O(N^{1-3\alpha/8})\). For \(j=a,b\), the partial derivatives of \(I_{j}(\cdot)\) w.r.t. \(\widetilde{N}_{1}\) and \(\widetilde{N}_{j}\) are non-negative and bounded from above by \(\max\{d(\mu_{1},\mu_{j}),d(\mu_{j},\mu_{1})\}=O(1)\). As a result, \(|I_{j}(N+R)-I_{j}(N)|=O(R)=O(N^{1-3\alpha/8})\) for \(j=a,b\). Hence, \(|I_{a}(N)-I_{b}(N)|\leq|I_{a}(N+R)-I_{b}(N+R)|+\sum_{j=a,b}|I_{j}(N+R)-I_{j}(N)|= O(N^{1-3\alpha/8})\). \(\square\)

**Bounding \(T_{stable}\):** In Appendix G.2, we choose \(T_{good}\) and \(T_{stable}\) such that \(T_{stable}\) is the time after \(T_{good}\) by which the algorithm picks all the sub-optimal arms atleast once. By Proposition 3.1, the algorithm approximately matches \(\boldsymbol{\omega}^{\star}\) after \(T_{stable}\). Using an argument similar to the one for bounding time to reach the optimal proportion in the fluid dynamics of Section 4, we can prove that \(T_{stable}\lessapprox(\omega_{\min}^{\star})^{-1}T_{good}\) a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\), where \(\omega_{\min}^{\star}=\min_{a\in[K]}\omega_{a}^{\star}\) (Lemma G.4, Appendix G.2.1).

**Role of forced exploration in analysis:** As we observe in the numerical results in Appendix J.4, forced exploration (step 1 of our algorithm) does not increase the observed sample complexity. We emphasize that without the forced exploration, Propositions 5.1 and 3.1 continue to hold if we can show that the proposed algorithm perform sufficient exploration over the instance. That is, after a random time \(T\) depending on the instance and satisfying \(\mathbb{E}[T]<\infty\), every arm has \(\widetilde{N}_{a}(N)=\Omega(\sqrt{N})\). As a result, upon proving sufficient exploration, asymptotic optimality will follow without the forced exploration step.

In Appendix J.4, we see in the numerical experiments, when there is no forced exploration, AT2's sample complexity blows up over instances where multiple sub-optimal arms have equal mean. Onthe other hand, IAT2 performs optimally over the same instances and its sample complexity remains unaffected even when there is no forced exploration. To understand AT2's sample complexity blow up when multiple sub-optimal arms have the same mean, consider the sample paths where the best arm observes unusually small values in the first few samples. As a result, with positive probability, AT2 confuses one of the multiple sub-optimal arms with equal mean as the best arm and stay stuck sampling between those sub-optimal arms forever. However, IAT2 avoids such situation because of the exploration of every sub-optimal arm induced by the extra logarithmic term in the index. Based on these observations, we make the following conjectures: **1)** AT2 performs sufficient exploration over instances where the means of all the sub-optimal arms are distinct, and **2)** IAT2 performs sufficient exploration over all instances including when some of the sub-optimal arms may have equal means.

## 6 Numerical results

In this section, we numerically demonstrate the dynamics followed by the algorithm AT2, and also compare its performance against the \(\beta\)-EB-TCB algorithm of [16] for different values of \(\beta\), and TCB algorithm of [22]. We consider \(4\) armed Gaussian bandit with unit variance and mean vector \(\mu=[10,8,7,6.5]\). We simulate one sample path of the AT2 without stopping rule, and plot the value of normalized indexes of the sub-optimal arms. Figure 2 demonstrates that the normalised indexes once close remain close, and hence, AT2 closely mimics the fluid path. In Figure 2, we plot the sample complexities of the (I)AT2, (I)TCB, and \(\beta\)-EB-(I)TCB, for different choices of \(\beta\), and observe that (I)AT2 outperforms all the other algorithms. Note that we use the same forced exploration rule and stopping rule for all algorithms.

In Appendix J, we demonstrate by several examples that both the AT2 and IAT2 algorithms significantly outperform the \(\beta\)-EB-TCB and \(\beta\)-EB-ITCB of [16] when \(\beta\) is chosen different from the optimal \(\beta\). We also illustrate that the AT2 and IAT2 algorithms have average sample complexity significantly lesser than the TCB and ITCB policies of [22]. In fact, we observe numerically, that (I)TCB doesn't quite satisfy the asymptotic optimality conditions (Figure 4, Appendix J). Next, in Appendix J, we study the effect of choice of the forced exploration parameter \(\alpha\) on the sample complexities of AT2 and IAT2. Additionally, we conduct simulations for natural extensions of these algorithms to bandits with distributions supported in \([0,1]\) (Appendix J.5).

## 7 Conclusion

We considered the best-arm identification problem under the popular top-2 framework. In the literature, top-2 framework involves sequentially identifying the empirical best arm and the most-likely challenger arm, and sampling the empirical best with probability \(\beta\) and the other with the complimentary probability. However, optimal \(\beta\) was not known. [22] recently proposed a deterministic rule for deciding between the empirical best and the challenger arm. In this paper, we have provided a most natural first order optimality condition based rule to help decide between the two. We showed that our associated algorithm is asymptotically optimal, and empirically performs better than [22] both in sample and computational complexity. Our another key contribution was to identify the underlying limiting ordinary differential equation based fluid dynamics that our algorithm tracks. This structure also provides important insights which help prove convergence of the proposed algorithm.

Figure 1: Normalised index on \(1\) sample path. Figure 2: Sample complexity comparison.

**Acknowledgments:**_We thank Arun Suggala and Karthikeyan Shanmugam from Google Research Bangalore for initial discussions on this project. The second and the third author initiated this work while visiting Google Research in Bangalore._

## References

* [1] Shubhada Agrawal, Sandeep Juneja, and Peter Glynn. Optimal \(\delta\)-correct best-arm selection for heavy-tailed distributions. In _Algorithmic Learning Theory_, pages 61-110. PMLR, 2020.
* [2] Shubhada Agrawal, Sandeep Juneja, Karthikeyan Shanmugam, and Arun Sai Suggala. Optimal best-arm identification in bandits with access to offline data. _arXiv preprint arXiv:2306.09048_, 2023.
* [3] Jean-Yves Audibert, Sebastien Bubeck, and Remi Munos. Best arm identification in multi-armed bandits. In _Conference on Learning Theory_, pages 41-53, 2010.
* [4] Alain Bensoussan, Jens Frehse, Phillip Yam, et al. _Mean field games and mean field type control theory_, volume 101. Springer, 2013.
* [5] Vivek S Borkar. _Stochastic approximation: a dynamical systems viewpoint_, volume 48. Springer, 2009.
* [6] Lijie Chen, Jian Li, and Mingda Qiao. Towards instance optimal bounds for best arm identification. In _Conference on Learning Theory_, pages 535-592. PMLR, 2017.
* [7] Ye Chen and Ilya O Ryzhov. Balancing optimal large deviations in sequential selection. _Management Science_, 69(6):3457-3473, 2023.
* [8] Jim G Dai. On positive harris recurrence of multiclass queueing networks: a unified approach via fluid limit models. _The Annals of Applied Probability_, 5(1):49-77, 1995.
* [9] Amir Dembo and Ofer Zeitouni. _Large deviations techniques and applications_, volume 38. Springer Science & Business Media, 2009.
* [10] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6), 2006.
* [11] Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In _Conference on Learning Theory_, pages 998-1027. PMLR, 2016.
* [12] Aurelien Garivier and Emilie Kaufmann. Nonasymptotic sequential tests for overlapping hypotheses applied to near-optimal arm identification in bandit models. _Sequential Analysis_, 40(1):61-96, 2021.
* [13] Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. lil'ucb: An optimal exploration algorithm for multi-armed bandits. In _Conference on Learning Theory_, pages 423-439. PMLR, 2014.
* [14] Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. In _Advances in Neural Information Processing Systems_, volume 33, pages 10007-10017, 2020.
* [15] Marc Jourdan and Remy Degenne. Non-asymptotic analysis of a ucb-based top two algorithm. In _Advances in Neural Information Processing Systems_, volume 36, pages 68980-69020, 2023.
* [16] Marc Jourdan, Remy Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms revisited. _Advances in Neural Information Processing Systems_, 35:26791-26803, 2022.
* [17] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In _ICML_, volume 12, pages 655-662, 2012.
* [18] Emilie Kaufmann. _Contributions to the Optimal Solution of Several Bandit Problems_. PhD thesis, Universite de Lille, 2020.

* Kaufmann and Koolen [2021] Emilie Kaufmann and Wouter M. Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. _Journal of Machine Learning Research_, 22(246):1-44, 2021.
* Luenberger and Ye [2008] David G. Luenberger and Yinyu Ye. _Linear and Nonlinear Programming_. Springer US, 2008.
* Mannor and Tsitsiklis [2004] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. _Journal of Machine Learning Research_, 5(Jun):623-648, 2004.
* Mukherjee and Tajer [2023] Arpan Mukherjee and Ali Tajer. Best arm identification in stochastic bandits: Beyond \(\beta-\) optimality. _arXiv preprint arXiv:2301.03785_, 2023.
* Nesterov et al. [2018] Yurii Nesterov et al. _Lectures on convex optimization_, volume 137. Springer, 2018.
* Qin et al. [2017] Chao Qin, Diego Klabjan, and Daniel Russo. Improving the expected improvement algorithm. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* Russo [2016] Daniel Russo. Simple bayesian algorithms for best arm identification. In _Conference on Learning Theory_, pages 1417-1418. PMLR, 2016.
* Shang et al. [2020] Xuedong Shang, Rianne Heide, Pierre Menard, Emilie Kaufmann, and Michal Valko. Fixed-confidence guarantees for bayesian best-arm identification. In _International Conference on Artificial Intelligence and Statistics_, pages 1823-1832. PMLR, 2020.
* Wang et al. [2021] Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via frank-wolfe. _Advances in Neural Information Processing Systems_, 34:5810-5821, 2021.
* You et al. [2023] Wei You, Chao Qin, Zihao Wang, and Shuoguang Yang. Information-directed selection for top-two algorithms. In _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 2850-2851. PMLR, 12-15 Jul 2023.

## Appendix A Outline

Below we provide a brief outline of the topics presented in the appendices.

1. Algorithm (1) and (2) are, respectively, the pseudocodes of the AT2 and IAT2 algorithms introduced in Section 3.
2. Appendix B: We define the single parameter exponential family (SPEF) of distributions, and prove several inequalities bounding the index function, anchor function, and the derivatives of the anchor function, which are crucial for our analysis.
3. Appendix C: We introduce a framework using which we apply the implicit function theorem for proving several properties related to the fluid dynamics and the algorithm's allocations.
4. Appendix D: We prove Propositions 2.1 and 2.2 from Section 2.
5. Appendix E: We provide the proofs of the results mentioned in Section 4, and also construct the fluid dynamics for the \(\beta\)-EB-TCB algorithm ([16]) in Appendix E.2.
6. Appendix F: We provide a heuristic argument to show that if the minimum index meets with the index of some sub-optimal arm \(a\neq 1\) following the ODEs in Theorem 4.1, then arm \(a\) must be incorporated into the set of minimum index arms. We argue via contradiction to show that, if this is not the case then index of arm \(a\) becomes strictly less than the minimum index of the arms, which implies a contradiction. Several of the key steps in the proof of Lemma 5.1 are extensions of the said argument to the non-fluid setting of the algorithm with additional terms because of the noise in the estimates.
7. Appendix G: We prove Proposition 3.1, 5.1, and provide detailed proofs of all the results mentioned in Section 5.
8. Appendix H: We provide the detailed proof of Theorem 3.1.
9. Appendix I: We describe a natural extension of the proposed AT2 and IAT2 algorithms to the class of distributions with support contained in \([0,1]\). We do not theoretically analyze this algorithm owing to space constraints. However we compare the proposed algorithm with existing algorithms experimentally in Appendix J.5.
10. Appendix J: We compare the performance of the proposed algorithms against existing algorithms through numerical experiments. We also illustrate how our algorithm follows the fluid dynamics as the no. of samples increase.

``` Input :Confidence parameter \(\delta>0\), exploration parameter \(\alpha\in(0,1)\) for\(N\geq 1\)do
1\(\mathcal{V}_{N}\leftarrow\left\{a\in[K]\mid\widetilde{N}_{a}(N-1)<N^{\alpha}\right\}\) if\(\mathcal{V}_{N}\neq\emptyset\)then
2\(A_{N}\leftarrow\arg\min_{a\in[K]}\;\;\widetilde{N}_{a}(N-1)\)// Forced exploration
3elseif\(g(\widetilde{\boldsymbol{\mu}}(N-1),\widetilde{\boldsymbol{N}}(N-1))>0\)then
4\(A_{N}\leftarrow\hat{i}_{N-1}\)// Choosing leader
5else
6\(A_{N}\leftarrow\arg\min_{a\in[K]/\{\hat{i}_{N-1}\}}\;\mathcal{I}_{a}(N-1)\)// Choosing challenger
7 Sample \(X_{N}\) from \(A_{N}\) and compute \(\widetilde{\boldsymbol{\mu}}(N)\) and \(\widetilde{\boldsymbol{N}}(N)\) /* Generalized Log-Likelihood Ratio (GLLR) Test */
8if\(\min_{a\in[K]/\{\hat{i}_{N}\}}\mathcal{I}_{a}(N)>\beta(N,\delta)\)then return\(i_{N}\) ```

**Algorithm 1**Anchored Top-Two (AT2) Algorithm

## Appendix B Single parameter exponential family of distributions

We consider single parameter exponential family (SPEF) of distributions of the form

\[d\nu_{\theta}(x)=\exp(\theta x-b(\theta))d\rho(x)\]

where \(\rho\) is a dominating measure which we assume to be degenerate, \(\theta\) lies in the interior of set \(\Theta\) defined below (denoted by \(\Theta^{o}\)):

\[\Theta=\left\{\theta\;\Big{|}\;\int_{\mathbb{R}}\exp(\theta x)d\rho(x)<\infty \right\},\]

and

\[b(\theta)=\log\left(\int_{\mathbb{R}}\exp(\theta x)d\rho(x)\right)\]

is the log-moment generating function of the measure \(\rho(\cdot)\).

For \(\theta,\widetilde{\theta}\in\Theta^{o}\), the KL-divergence between the measures \(\nu_{\theta}\) and \(\nu_{\widetilde{\theta}}\) is,

\[KL(\nu_{\theta},\nu_{\widetilde{\theta}})=(\theta-\widetilde{\theta})b^{ \prime}(\theta)-b(\theta)+b(\widetilde{\theta}).\]

The mean under \(\nu_{\theta}\) is given by \(b^{\prime}(\theta)\). Let \(\mathcal{S}\) be the image of the set \(\Theta^{o}\) under the mapping \(b^{\prime}(\cdot)\). Note that \(\mathcal{S}\) is an open interval. Also, since \(b^{\prime\prime}(\cdot)>0\) in \(\Theta^{o}\), \(b^{\prime}(\cdot)\) is strictly increasing in \(\Theta^{o}\), and is a bijection between \(\Theta^{o}\) and \(\mathcal{S}\). This implies we can parameterize the distributions in the SPEF using their means as well.

Let \(\theta_{\mu}\) be the unique \(\theta\) satisfying \(b^{\prime}(\theta)=\mu\) for some \(\mu\in\mathcal{S}\). Clearly, \(\theta_{\mu}\) is a strictly increasing function of \(\mu\). This follows since \(b^{\prime}(\cdot)\) is strictly increasing in \(\Theta^{o}\). Additionally, all the higher derivatives of \(b(\cdot)\) exist in the set \(\Theta^{o}\) (see Exercise 2.2.24 in [9]).

For \(\mu,\widetilde{\mu}\in\mathcal{S}\) we define \(d(\mu,\widetilde{\mu})\) as,

\[d(\mu,\widetilde{\mu})=KL(\nu_{\theta_{\mu}},\nu_{\theta_{\widetilde{\mu}}}) =(\theta_{\mu}-\theta_{\widetilde{\mu}})\mu-b(\theta_{\mu})+b(\theta_{ \widetilde{\mu}}).\]

We define \(\mu_{\inf}\in\mathbb{R}\cup\{-\infty\}\) and \(\mu_{\sup}\in\mathbb{R}\cup\{+\infty\}\), respectively, to be the infimum and supremum of the interval \(\mathcal{S}\). Then, \(\mathcal{S}=(\mu_{\inf},\mu_{\sup})\).

**Definition B.1**.: For \(\boldsymbol{\mu}=(\mu_{1},\mu_{2},\ldots,\mu_{K})\in\mathcal{S}^{K}\), define \(r_{\min}(\boldsymbol{\mu})=\min_{i\in[K]}\{\min\{\mu_{i}-\mu_{\inf},\mu_{\sup}- \mu_{i}\}\}\).

Since \(\mathcal{S}\) is an open interval, \(r_{\min}(\bm{\mu})\) is positive for every \(\bm{\mu}\in\mathcal{S}^{K}\), and can be \(\infty\) if both \(\mu_{\inf}\) and \(\mu_{\sup}\) are \(\infty\).

**Partial derivatives of \(d(\mu,\widetilde{\mu})\):** For every pair \(\mu,\widetilde{\mu}\in\mathcal{S}\), the partial derivatives of \(d(\mu,\widetilde{\mu})\) with respect to the first argument is

\[d_{1}(\mu,\widetilde{\mu})\stackrel{{\text{def.}}}{{=}}\frac{ \partial}{\partial\mu}d(\mu,\widetilde{\mu})=\theta_{\mu}-\theta_{\widetilde{ \mu}},\]

and that with respect to the second argument is,

\[d_{2}(\mu,\widetilde{\mu})\stackrel{{\text{def.}}}{{=}}\frac{ \partial}{\partial\widetilde{\mu}}d(\mu,\widetilde{\mu})=\frac{\widetilde{ \mu}-\mu}{b^{\prime\prime}(\theta_{\widetilde{\mu}})}.\]

**Enveloping the KL-divergence:** In the following discussion, we try to bound the KL-divergence \(d(\mu,\widetilde{\mu})\) from both sides using the squared distance \(|\mu-\widetilde{\mu}|^{2}\) after imposing some restrictions on the choice of \(\mu,\widetilde{\mu}\in\mathcal{S}\). For an instance \(\bm{\mu}\in\mathcal{S}^{K}\), we define the constants \(\Delta_{\min}(\bm{\mu})\) and \(\epsilon(\bm{\mu})\) as

\[\Delta_{\min}(\bm{\mu})=\min_{i\in[K]/\{1\}}(\mu_{1}-\mu_{i})\quad\text{and} \quad\epsilon(\bm{\mu})=\min\left\{\frac{\Delta_{\min}(\bm{\mu})}{4},\frac{r_{ \min}(\bm{\mu})}{2}\right\}.\]

We further define

\[\mathcal{H}(\bm{\mu}) = \bigcup_{i\in[K]/\{1\}}[\mu_{i}-\epsilon(\bm{\mu}),\mu_{1}+ \epsilon(\bm{\mu})],\] \[\sigma_{\max}(\bm{\mu}) = \max_{\mu\in\mathcal{H}(\bm{\mu})}b^{\prime\prime}(\theta_{\mu}) \quad\text{and}\quad\sigma_{\min}(\bm{\mu})\ =\ \min_{\mu\in\mathcal{H}(\bm{\mu})}b^{\prime\prime}(\theta_{\mu}).\]

Since \(\mathcal{H}(\bm{\mu})\subset\mathcal{S}\), all distributions with mean in \(\mathcal{H}(\bm{\mu})\) have positive variance. Note that \(b^{\prime\prime}(\theta_{\mu})\) represents the variance of the distribution with mean \(\mu\). As a result, since \(\mathcal{H}(\bm{\mu})\) is a compact set, both \(\sigma_{\max}(\bm{\mu})\) and \(\sigma_{\min}(\bm{\mu})\) are positive constants.

Hence, \(b(\cdot)\) is \(\sigma_{\min}(\bm{\mu})\)-strongly convex and \(b^{\prime}(\cdot)\) is \(\sigma_{\max}(\bm{\mu})\)-Lipschitz on the set \((b^{\prime})^{-1}(\mathcal{H}(\bm{\mu}))\). Thus, using [23, Theorems 2.1.5 and 2.1.10], for every \(\theta_{1},\theta_{2}\in(b^{\prime})^{-1}(\mathcal{H}(\bm{\mu}))\), we have

\[\frac{|b^{\prime}(\theta_{1})-b^{\prime}(\theta_{2})|^{2}}{2\sigma_{\max}(\bm {\mu})}\leq b(\theta_{2})-b(\theta_{1})-b^{\prime}(\theta_{1})\cdot(\theta_{2 }-\theta_{1})=d(\nu_{\theta_{1}},\nu_{\theta_{2}})\leq\frac{|b^{\prime}(\theta _{1})-b^{\prime}(\theta_{2})|^{2}}{2\sigma_{\min}(\bm{\mu})},\]

and hence, for \(\mu,\widetilde{\mu}\in\mathcal{H}(\bm{\mu})\),

\[\frac{|\mu-\widetilde{\mu}|^{2}}{2\sigma_{\max}(\bm{\mu})}\leq d(\mu, \widetilde{\mu})\leq\frac{|\mu-\widetilde{\mu}|^{2}}{2\sigma_{\min}(\bm{\mu})}.\] (6)

**Bounding the partial derivatives:** We now introduce bounds on the partial derivatives \(d_{1}\) and \(d_{2}\) introduced earlier. Consider \(\mu,x,\widetilde{\mu}\in\mathcal{H}(\bm{\mu})\) such that \(\mu>\widetilde{\mu}\), and \(x\in[\widetilde{\mu},\mu]\). Recall

\[d_{1}(\mu,x)=\theta_{\mu}-\theta_{x}\quad\text{and}\quad d_{1}(\widetilde{ \mu},x)=-(\theta_{x}-\theta_{\widetilde{\mu}}).\]

Since \(\theta_{(\cdot)}=(b^{\prime})^{-1}(\cdot)\), we have,

\[\frac{\mu-x}{\sigma_{\max}(\bm{\mu})}\leq d_{1}(\mu,x)\leq\frac{\mu-x}{\sigma_ {\min}(\bm{\mu})},\quad\text{and}\quad-\frac{x-\widetilde{\mu}}{\sigma_{\max}( \bm{\mu})}\geq d_{1}(\widetilde{\mu},x)\geq-\frac{x-\widetilde{\mu}}{\sigma_{ \min}(\bm{\mu})}.\] (7)

Similarly, since,

\[d_{2}(\mu,x)=-\frac{\mu-x}{b^{\prime\prime}(\theta_{x})}\quad\text{and}\quad d _{2}(\widetilde{\mu},x)=\frac{x-\widetilde{\mu}}{b^{\prime\prime}(\theta_{x})},\]

we have,

\[-\frac{\mu-x}{\sigma_{\max}(\bm{\mu})}\geq d_{2}(\mu,x)\geq-\frac{\mu-x}{ \sigma_{\min}(\bm{\mu})},\quad\text{and}\quad\frac{x-\widetilde{\mu}}{\sigma_{ \min}(\bm{\mu})}\leq d_{2}(\widetilde{\mu},x)\leq\frac{x-\widetilde{\mu}}{\sigma _{\min}(\bm{\mu})}.\] (8)

[MISSING_PAGE_FAIL:16]

Using the same notation, as we used in (10), we have,

\[\mathcal{W}_{a}(\widetilde{\bm{\mu}},\bm{N})\;=\;\Theta\left(\frac{N_{1}N_{a}}{N_ {1}+N_{a}}\right)\] (12)

for every arm \(a\in[K]/\{1\}\).

**Enveloping the partial derivatives of anchor function with respect to \(\bm{N}\):** While analyzing the AT2 and IAT2 algorithms, we need to show that the anchor function converges to zero at a uniform rate as no. of iteration goes to infinity. During this step, we need to bound the partial derivatives of the anchor function \(g(\bm{\mu},\bm{N})\) with respect to \(\bm{N}\). Below we evaluate the partial derivatives of \(g(\bm{\mu},\bm{N})\) with respect to \(N_{a}\) for different arms \(a\in[K]\).

\[\frac{\partial g}{\partial N_{1}}(\bm{\mu},\bm{N}) =-\sum_{a\neq 1}f(\bm{\mu},a,\bm{N})\frac{N_{a}\Delta_{a}}{(N_{1}+N _{a})^{2}},\text{ and }\] \[\forall a\neq 1,\;\;\frac{\partial g}{\partial N_{a}}(\bm{\mu},\bm{N }) =f(\bm{\mu},a,\bm{N})\frac{N_{1}\Delta_{a}}{(N_{1}+N_{a})^{2}},\] (13)

where \(\Delta_{a}=\mu_{1}-\mu_{a}\) and

\[f(\bm{\mu},a,\bm{N})=-\frac{\partial}{\partial x}\left(\frac{d(\mu_{1},x)}{d( \mu_{a},x)}\right)\Big{|}_{x=x_{1,a}}=-\frac{d_{2}(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}+\frac{d(\mu_{1},x_{1,a})d_{2}(\mu_{a},x_{1,a})}{(d(\mu_{a},x_{1,a}) )^{2}}.\]

Using (8), and (6), we have,

\[-d_{2}(\mu_{1},x_{1,a}) =\;\Theta(\mu_{1}-x_{1,a}),\quad d_{2}(\mu_{a},x_{1,a})\;=\; \Theta(x_{1,a}-\mu_{a}),\] \[d(\mu_{1},x_{1,a}) =\;\Theta((\mu_{1}-x_{1,a})^{2}),\quad\text{and}\quad d(\mu_{a}, x_{1,a})\;=\;\Theta((x_{1,a}-\mu_{a})^{2}),\]

where the constants hidden in \(\Theta(\cdot)\) depend only on \(\bm{\mu}\) and are independent of the sample path. Therefore,

\[f(\bm{\mu},a,\bm{N}) =\Theta\left(\frac{\mu_{1}-x_{1,a}}{(x_{1,a}-\mu_{a})^{2}}+\frac {(\mu_{1}-x_{1,a})^{2}}{(x_{1,a}-\mu_{a})^{3}}\right)\] \[=\Theta\left(\frac{N_{a}}{N_{1}}\left(1+\frac{N_{a}}{N_{1}} \right)^{2}\right).\] (14)

As a consequence, we have,

\[\frac{\partial g}{\partial N_{1}}(\bm{\mu},\bm{N}) =-\Theta\left(\sum_{a\in[K]/\{1\}}\frac{N_{a}^{2}}{N_{1}^{3}} \right),\;\;\text{and}\] \[\forall a\neq 1,\;\;\frac{\partial g}{\partial N_{a}}(\bm{\mu},\bm{N }) =\Theta\left(\frac{N_{a}}{N_{1}^{2}}\right).\] (15)

## Appendix C Framework for applying the Implicit function theorem (IFT)

In this appendix we explain a general framework using which we later apply the Implicit function theorem for the following purposes:

1. Constructing the fluid dynamics for the AT2 and \(\beta\)-EB-TCB algorithms in Appendix E.
2. Proving convergence of the algorithm's allocations to the optimal proportions in Appendix G.2.2.

We introduce the variables: \(\bm{N}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in[K]),\;\;I\in\mathbb{R},\;\bm{\eta}=( \eta_{a}\in\mathbb{R}:a\in[K]),\;\;\text{and}\;\;N\geq 0\). After fixing some instance \(\bm{\mu}\in\mathcal{S}^{K}\) (\(\mathcal{S}\) is defined in Appendix B), we define the following functions:

\[\Phi_{1}(\bm{N},\bm{\eta}) = \sum_{a\in[K]/\{1\}}\frac{d(\mu_{1},x_{1,a}(N_{1},N_{a}))}{d(\mu_{a},x_{1,a}(N_{1},N_{a}))}-1-\eta_{1},\] \[\text{for }a\in[K]/\{1\},\;\;\Phi_{a}(\bm{N},I,\bm{\eta}) = N_{1}d(\mu_{1},x_{1,a}(N_{1},N_{a}))+N_{a}d(\mu_{a},x_{1,a}(N_{1 },N_{a}))-I-\eta_{a},\] \[\text{and}\;\;\;\Phi_{K+1}(\bm{N},N) = \sum_{a\in[K]}N_{a}-N,\]

where

\[x_{1,a}(N_{1},N_{a})=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{a}}.\]

For every non-empty subset \(B\subseteq[K]/\{1\}\), we define the vector valued functions,

\[\bm{\Phi}_{B}(\bm{N},I,\bm{\eta},N) = [\,\Phi_{1}(\bm{N},\bm{\eta}),\quad(\Phi_{a}(\bm{N},I,\bm{\eta}) )_{a\in B},\quad\Phi_{K+1}(\bm{N},N)\,]\,,\quad\text{and}\] \[\widetilde{\bm{\Phi}}_{B}(\bm{N},I,\bm{\eta}) = [\,\Phi_{1}(\bm{N},\bm{\eta}),\quad(\Phi_{a}(\bm{N},I,\bm{\eta}) )_{a\in B}\,]\,.\]

We denote \(\bm{\Phi}_{[K]/\{1\}}(\cdot)\) just using \(\bm{\Phi}(\cdot)\).

In the definitions of \(\bm{\Phi}_{B}(\cdot)\) and \(\widetilde{\bm{\Phi}}_{B}(\cdot)\), without loss of generality, we assume that, the functions \(\Phi_{a}(\cdot)\) in the tuple \((\Phi_{a}(\cdot))_{a\in B}\) are enumerated in the increasing order of \(a\in B\), _i.e._, if we have \(B=\{a_{1},a_{2},\ldots,a_{|B|}\}\) with \(1<a_{1}<a_{2}<\ldots<a_{|B|}\), then,

\[\bm{\Phi}_{B} = \left[\,\Phi_{1},\quad\Phi_{a_{1}},\quad,\Phi_{a_{2}},\quad\Phi_ {a_{3}},\quad\ldots\quad\Phi_{a_{|B|}},\quad\Phi_{K+1}\,\right],\quad\text{and}\] \[\widetilde{\bm{\Phi}}_{B} = \left[\,\Phi_{1},\quad\Phi_{a_{1}},\quad,\Phi_{a_{2}},\quad\Phi_ {a_{3}},\quad\ldots\quad\Phi_{a_{|B|}}\,\right].\]

Before stating the main result of this section in Lemma C.1, we define some notation that are essential for the lemma statement. For any \(A\subseteq[K]\), we use the notation \(\bm{N}_{A}\) to denote the tuple of variables \((N_{a}:a\in A)\). For some vector valued function \(\bm{G}\) depending on \(\bm{N}\), denote the Jacobian of \(G(\cdot)\) with respect to the tuple of variables \(\bm{N}_{A}\) using \(\frac{\partial\bm{G}}{\partial\bm{N}_{A}}\).

For any non-empty \(B\subseteq[K]/\{1\}\), we define \(\overline{B}=B\cup\{1\}\). For every \(k\geq 1\), \(\bm{0}_{k}\) and \(\bm{1}_{k}\), respectively, refers to a \(k\)-dimensional vector with entries \(0\) and \(1\). We define \(\mathcal{Z}_{B}\) to be the set of tuples \((\bm{N},I,\bm{0}_{K},N)\) with \(\bm{N}\in\mathbb{R}_{\geq 0}^{K}\), \(I\in\mathbb{R}\), and \(N\in\mathbb{R}_{>0}\), which satisfy

\[\bm{\Phi}_{B}(\bm{N},I,\bm{0}_{K},N)\;=\;\bm{0}_{|B|+2}.\]

**Lemma C.1** (Invertibility of the Jacobian).: _For all non-empty subset \(B\subseteq[K]/\{1\}\), the following statements hold true at every tuple \((\bm{N},I,\bm{0}_{K},N)\) in the set \(\mathcal{Z}_{B}\),_

1. _The Jacobian_ \(\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial\bm{N}_{\overline{B}}}\) _is invertible at_ \((\bm{N},I,\bm{0}_{K})\)_._
2. _We have_ \[\bm{1}_{|B|+1}^{T}\left(\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial\bm{ N}_{\overline{B}}}\right)^{-1}\frac{\partial\widetilde{\bm{\Phi}}_{B}}{ \partial I}\leq-\sum_{a\in B}\frac{1}{d(\mu_{a},\mu_{1})},\] _at_ \((\bm{N},I,\bm{0}_{K})\)_._
3. _The Jacobian_ \(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\) _defined as,_ \[\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\;=\;\;\left[ \begin{array}{c|c}\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial\bm{N}_{ \overline{B}}}&\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial I}\\ \hline\\ \frac{\partial\Phi_{K+1}}{\partial\bm{N}_{\overline{B}}}&\frac{\partial\Phi_{K+ 1}}{\partial I}\end{array}\right]\] _is invertible at_ \((\bm{N},I,\bm{0}_{K},N)\)_._Proof.: **Statement 1:** The Jacobian \(\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial N_{\overline{B}}}\) is equivalent to,

\[\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial N_{\overline{B}}}= \left[\begin{array}{cccccc}\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{ \partial N_{1}}&\frac{\partial\Phi_{1}}{\partial N_{a_{1}}}&\frac{\partial \Phi_{1}}{\partial N_{a_{2}}}&\dots&\frac{\partial\Phi_{1}}{\partial N_{a_{|B|} }}\\ \frac{\partial\Phi_{a_{1}}}{\partial N_{1}}&\frac{\partial\Phi_{a_{1}}}{ \partial N_{a_{1}}}&\frac{\partial\Phi_{a_{1}}}{\partial N_{a_{2}}}&\dots&\frac {\partial\Phi_{a_{1}}}{\partial N_{a_{|B|}}}\\ \vdots&\vdots&\vdots&\dots&\vdots\\ \frac{\partial\Phi_{a_{|B|}}}{\partial N_{1}}&\frac{\partial\Phi_{a_{|B|}}}{ \partial N_{a_{1}}}&\frac{\partial\Phi_{a_{|B|}}}{\partial N_{a_{2}}}&\dots& \frac{\partial\Phi_{a_{|B|}}}{\partial N_{a_{|B|}}}\end{array}\right].\] (16)

Now we observe the following properties about the sign of the entries of the above Jacobian matrix,

* We have \(N>0\) and \(\Phi_{1}(\bm{N},\bm{0}_{K})=0\). This implies \(N_{1}>0\) and \(\max_{a\in[K]/\{1\}}N_{a}>0\). As a result, using (15), we have \(\frac{\partial\Phi_{1}}{\partial N_{1}}<0\) and \(\frac{\partial\Phi_{1}}{\partial N_{a_{i}}}\geq 0\) for every \(i\in\{1,2,\dots,|B|\}\).
* For \(i\in\{2,\dots,|B|+1\}\), in the \(i\)-th row, the only non-zero entries can be the first and the \(i\)-th entry. The first entry is \(\frac{\partial\Phi_{a_{i}}}{\partial N_{1}}=d(\mu_{1},x_{1,a_{i}}(N_{1},N_{a_{ i}}))\geq 0\). The \(i\)-th entry is \(\frac{\partial\Phi_{a_{i}}}{\partial N_{a_{i}}}=d(\mu_{a_{i}},x_{1,a_{i}}(N_{1},N_{a_{i}}))\). Since we have \(N_{1}>0\), using (6), we have \(d(\mu_{a_{i}},x_{1,a_{i}}(N_{1},N_{a_{i}}))>0\), making the \(i\)-th entry positive.

Therefore, considering only the sign of the elements, the matrix in (16) is of the form,

\[\begin{bmatrix}--&+&+&+&\dots&+\\ +&++&0&0&\dots&0\\ +&0&++&0&\dots&0\\ +&0&0&++&\dots&0\\ \vdots&\vdots&\vdots&\vdots&\dots&\vdots\\ +&0&0&0&\dots&++\end{bmatrix},\] (17)

where the symbols \(++,\ --\) and \(+\) implies that the corresponding entries are positive, negative and non-negative.

We now argue that a matrix of the above structure has a rank \(|B|+1\). To see that, by subtracting some appropriate constant times the \(i\)-th column from the first column, we can make the entries in position \(i\in\{2,3,\dots,|B|+1\}\) in the first column zero. As a result of these transformations, since we are subtracting non-negative quantities from the first entry of the first column, that entry remains negative. The matrix we obtain after this sequence of transformations has a structure,

\[\begin{bmatrix}--&+&+&+&\dots&+\\ 0&++&0&0&\dots&0\\ 0&0&++&0&\dots&0\\ 0&0&0&++&\dots&0\\ \vdots&\vdots&\vdots&\vdots&\dots&\vdots\\ 0&0&0&0&\dots&++\end{bmatrix}.\] (18)

Clearly a matrix of the above structure has a rank \(|B|+1\) and therefore invertible.

**Statement 2:** Using statement 1 of Lemma C.1, if we take \(\bm{v}=\left(\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial N_{ \overline{B}}}\right)^{-1}\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{ \partial I}\), then we have,

\[\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial N_{\overline{B}}}\bm {v}\ =\ \frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial I}.\]

Note that, the RHS of the above linear system _i.e._\(\frac{\partial\widetilde{\bm{\mathcal{R}}}_{B}}{\partial I}\) is a \(|B|+1\) dimensional vector with zero in its first entry and \(-1\) in every other entry. Using (17), \(\bm{v}=\left[v_{1},v_{2},v_{3},\dots,v_{|B|+1}\right]^{T}\) satisfies a linear system with coefficients having the following signs,

\[(--)v_{1}+(+)v_{2}+(+)v_{3}+(+)v_{4}+\dots+(+)v_{|B|+1}\ =\ 0,\quad\text{and}\]\[\frac{\partial\boldsymbol{\Phi}_{B}}{\partial(\boldsymbol{N}_{\overline{B}},I)}= \left[\begin{array}{c|c}\frac{\partial\boldsymbol{\tilde{\Phi}}_{B}}{\partial \boldsymbol{N}_{\overline{B}}}&\frac{\partial\boldsymbol{\tilde{\Phi}}_{B}}{ \partial I}\\ \frac{\partial\Phi_{K+1}}{\partial\boldsymbol{N}_{\overline{B}}}&\frac{ \partial\Phi_{K+1}}{\partial I}\end{array}\right]=\left[\begin{array}{c|c} \frac{\partial\boldsymbol{\tilde{\Phi}}_{B}}{\partial\boldsymbol{N}_{ \overline{B}}}&\frac{\partial\boldsymbol{\tilde{\Phi}}_{B}}{\partial I}\\ \hline\boldsymbol{1}_{|B|+1}^{T}&0\end{array}\right]\]We do the following determinant preserving column operation on \(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\),

\[\left[\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\right]_{:,|B|+2}\Longleftarrow\left[\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{ \overline{B}},I)}\right]_{:,|B|+2}-\left[\frac{\partial\bm{\Phi}_{B}}{ \partial(\bm{N}_{\overline{B}},I)}\right]_{:,1:|B|+1}\left(\frac{\partial\bm {\widetilde{\Phi}}_{B}}{\partial\bm{N}_{\overline{B}}}\right)^{-1}\frac{ \partial\bm{\widetilde{\Phi}}_{B}}{\partial I},\]

where \(\left[\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\right] _{:,|B|+2}\) and \(\left[\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\right] _{:,1:|B|+1}\), respectively, denotes the \(|B|+2\)-th column and left \((|B|+2)\times(|B|+1)\) submatrix of \(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\).

The above column operation gives us the matrix,

\[\left[\begin{array}{c|c}\frac{\partial\bm{\widetilde{\Phi}}_{B}}{\partial N _{\overline{B}}}&\bm{0}_{|B|+1}\\ \hline\\ \bm{1}_{|B|+1}^{T}&-\bm{1}_{|B|+1}^{T}\left(\frac{\partial\bm{\widetilde{\Phi} }_{B}}{\partial N_{\overline{B}}}\right)^{-1}\frac{\partial\bm{\widetilde{\Phi }}_{B}}{\partial I}\end{array}\right],\]

which has the same determinant as \(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\). Therefore,

\[\det\left(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)} \right)=\left(-\bm{1}_{|B|+1}^{T}\left(\frac{\partial\bm{\widetilde{\Phi}}_{B }}{\partial\bm{N}_{\overline{B}}}\right)^{-1}\frac{\partial\bm{\widetilde{\Phi }}_{B}}{\partial I}\right)\times\det\left(\frac{\partial\bm{\widetilde{\Phi}}_ {B}}{\partial\bm{N}_{\overline{B}}}\right).\]

Using statement 1 and 2 of Lemma C.1, both the quantities in the above product are non-zero, making the Jacobian \(\frac{\partial\bm{\Phi}_{B}}{\partial(\bm{N}_{\overline{B}},I)}\) invertible for every tuple in \(\mathcal{Z}_{B}\). 

## Appendix D Proofs from Section 2

Theorem D.1 is essential for proving Propositions 2.1 and 2.2 in Section 2. Before stating Theorem D.1 we state an alternative formulation of the max-min problem 1 which we call \(\mathbf{O}\).

\[\mathbf{O}\ : \min\ \sum\nolimits_{a=1}^{K}N_{a}\] s.t. \[\forall a\neq 1,\ \ W_{a}(N_{1},N_{a}):=N_{1}d(\mu_{1},x_{1,a})+N_{a}d (\mu_{a},x_{1,a})\geq\log\frac{1}{2.4\delta},\] (20)

where \(N_{a}\geq 0\) for all \(a\), and each \(x_{1,a}=\frac{\mu_{1}N_{1}+\mu_{a}N_{a}}{N_{1}+N_{a}}\).

The optimal value of the problem \(\mathbf{O}\) is of the form \(T^{\star}(\bm{\mu})\log(1/(2.4\delta))\), where \(T^{\star}(\bm{\mu})\) is the reciprocal of the optimal value of (1). If \(\bm{N}^{\star}=(N_{a}^{\star}:a\in[K])\) is an optimal allocation solving \(\mathbf{O}\), then \(\bm{\omega}_{a}^{\star}=\frac{N_{a}^{\star}}{\sum_{b\in[K]}N_{b}^{\star}}\) is an optimal proportion solving (1). Similarly if \(\bm{\omega}^{\star}\) is an optimal proportion solving (1) then \(\bm{N}^{\star}=(N_{a}^{\star}:a\in[K])\) with \(N_{a}^{\star}=\omega_{a}^{\star}T^{\star}(\bm{\mu})\) is an optimal allocation solving \(\mathbf{O}\). Theorem D.1 implies uniqueness to the solution of \(\mathbf{O}\) which also implies uniqueness of the solution of (1).

Some notations are needed before stating Theorem D.1. Let \(B\subset[K]/1\) and \(\overline{B}=B\cup\{1\}\). Let \(\bm{\nu}=(\nu_{a}:a\in[K])\in\mathcal{S}^{K}\) be some instance with \(\nu_{1}>\max_{a\neq 1}\nu_{a}\). Let \((I_{a}:a\in B)\) each be strictly positive. If \(\overline{B}^{c}\neq\emptyset\) then let \((N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B}^{c})\) be the no. of samples allocated to arms in \(\overline{B}^{c}\). We define \(N_{1,1}\) as zero when \(\sum_{a\in\overline{B}^{c}}N_{a}=0\) or \(\overline{B}^{c}=\emptyset\). Otherwise, we take \(N_{1,1}\) to be the unique \(N_{1}>0\) that solves,

\[\sum_{a\in\overline{B}^{c}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}=1,\] (21)

where \(x_{1,a}=\frac{\nu_{1}N_{1}+\nu_{a}N_{a}}{N_{1}+N_{a}}\). Note that if \(\overline{B}^{c}\neq\emptyset\) and \(\sum_{a\in\overline{B}^{c}}N_{a}>0\), there exists an \(a\in\overline{B}^{c}\) with \(N_{a}>0\). As a result, the LHS of (21) decreases from \(\infty\) to \(0\) as \(N_{1}\) increases from \(0\) to \(\infty\). This implies the existence of a unique \(N_{1}>0\) solving (21).

Set \(N_{1,2}:=\max_{a\in B}I_{a}d(\nu_{1},\nu_{a})^{-1}\).

**Theorem D.1**.: _There exists a unique solution \(N_{1}^{\star}\geq 0\) and \((N_{a}^{\star}\geq 0:a\in B)\) satisfying_

\[\sum_{a\neq 1}\frac{d(\nu_{1},x_{1,a}^{\star})}{d(\nu_{a},x_{1,a}^{\star})}-1=0 \quad\text{ where }\quad x_{1,a}^{\star}=\frac{\nu_{1}N_{1}^{\star}+\nu_{a}N_{a}^{\star}}{N_{1}^ {\star}+N_{a}^{\star}},\] (22)

\[\text{ and }\quad N_{1}^{\star}d(\nu_{1},x_{1,a}^{\star})+N_{a}^{\star}d(\nu_{a},x_{1,a}^{\star})=I_{a}\quad\forall a\in B.\] (23)

_Further, \(N_{1}^{\star}\geq\max(N_{1,1},N_{1,2})\) and each \(N_{a}^{\star}\geq\frac{I_{a}}{d(\nu_{a},\nu_{1})}\)._

_Furthermore, The optimal solution to \(\mathbf{O}\) is uniquely characterized by the solution above with \(B=\{2,\ldots,K\}\) and each \(I_{a}=\log(1/(2.4\delta))\) and constraints (20) tight, that is, indexes of all the sub-optimal arms being equal to \(\log(1/(2.4\delta))\). Further, \(N_{1}^{\star}\geq\max_{a\in[K]\setminus 1}\frac{\log(1/(2.4\delta)}{d(\nu_{1}, \nu_{a})}\) and each \(N_{a}^{\star}\geq\frac{\log(1/(2.4\delta)}{d(\nu_{a},\nu_{1})}\)._

**Proof:** First observe that every solution \(N_{1}^{\star}\) and \((N_{a}^{\star}:a\in B)\) to the system (22) and (23) must satisfy \(N_{1}\geq\max\{N_{1,1},N_{1,2}\}\) and \(N_{a}^{\star}\geq\frac{I_{a}}{d(\nu_{a},\nu_{1})}\), for every \(a\in B\).

If \(\overline{B}^{c}\neq\emptyset\), (22) implies,

\[\sum_{a\in\overline{B}^{c}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}\ \leq\ \sum_{a\in[K]/\{1\}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}\ =\ 1.\]

If \(\sum_{a\in\overline{B}^{c}}N_{a}=0\), then \(N_{1}\geq 0=N_{1,1}\). Otherwise, we can find an \(a\in\overline{B}^{c}\) with \(N_{a}>0\), making \(\sum_{a\in\overline{B}^{c}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}\) strictly decreasing in \(N_{1}\). As a result, by the definition of \(N_{1,1}\) we have \(N_{1}^{\star}\geq N_{1,1}\).

Now, for every \(a\in B\), we have

\[I_{a}\ =\ N_{1}^{\star}d(\nu_{1},x_{1,a})+N_{a}^{\star}d(\nu_{a},x_{1,a})\ =\ \min_{x\in[\nu_{a},\nu_{1}]}\{N_{1}^{\star}d(\nu_{1},x)+N_{a}^{\star}d(\nu_{a}, x)\}.\]

Note that RHS of the above inequality is upper bounded by \(\min\{N_{1}^{\star}d(\nu_{1},\nu_{a}),N_{a}^{\star}d(\nu_{a},\nu_{1})\}\). As a result, for every \(a\in B\), \(N_{a}^{\star}\geq\frac{I_{a}}{d(\nu_{a},\nu_{1})}\), and \(N_{1}^{\star}\geq\frac{I_{a}}{d(\mu_{1},\mu_{a})}\). This further implies, \(N_{1}^{\star}\geq\max_{a\in B}\frac{I_{a}}{d(\nu_{1},\nu_{a})}=N_{1,2}\).

Now we prove existence of a unique \(N_{1}^{\star}\) and \((N_{a}^{\star}:a\in B)\). For every \(N_{1}\geq N_{1,2}\) and \(a\in B\), as \(N_{a}\) increases from \(0\) to \(\infty\), \(N_{1}d(\nu_{1},x_{1,a})+N_{a}d(\nu_{a},x_{1,a})\) increases monotonically from \(0\) to \(N_{1}d(\nu_{1},\nu_{a})\). Note that \(N_{1}d(\nu_{1},\nu_{a})\geq N_{1,2}d(\nu_{1},\nu_{a})\geq I_{a}\) (by the definition of \(N_{1,2}\)). As a result, we can find a unique \(N_{a}\) for which \(N_{1}d(\nu_{1},x_{1,a})+N_{a}d(\nu_{a},x_{1,a})=I_{a}\). For every \(a\in B\), we call that unique \(N_{a}\) as \(N_{a}(N_{1})\). Observe that, since \(I_{a}>0\), \(N_{a}(N_{1})\) is strictly decreasing in \(N_{1}\), and if \(I_{a}=0\), then \(N_{a}(N_{1})=0\). Also, if \(a=\text{arg}\max_{b\in B}\ \frac{I_{b}}{d(\nu_{1},\nu_{b})}\), then \(N_{a}(N_{1,2})=\infty\) and \(\lim_{N_{1}\to\infty}N_{a}(N_{1})=\frac{I_{a}}{d(\nu_{a},\nu_{1})}\).

We now consider the allocation where every arm \(a\in B\) has \(N_{a}(N_{1})\) samples, and consider the function,

\[h(N_{1})\ =\ \sum_{a\in[K]/\{1\}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}.\]

Observe that, for all \(a\in B\), \(\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}\) is strictly decreasing for \(N_{1}\geq N_{1,2}\). Also if \(\overline{B}^{c}\) is non-empty, then every term \(\frac{d(\nu_{1},x_{1,a})}{d(\nu_{a},x_{1,a})}\) with \(N_{a}>0\) is strictly decreasing in \(N_{1}\). As a result, the overall function \(N_{1}\to h(N_{1})\) is strictly decreasing.

Moreover, as \(N_{1}\) increases to \(\infty\), \(N_{a}(N_{1})\) converges to \(\frac{I_{a}}{d(\nu_{a},\nu_{1})}\) for every \(a\in B\). Hence, \(h(N_{1})\) decreases to \(0\) as \(N_{1}\to\infty\). Therefore, if we can show that \(h(\max\{N_{1,1},N_{2,1}\})\geq 1\), then If \(N_{1,2}\geq N_{1,1}\), then some \(a\in B\) has \(N_{a}(N_{1,2})=\infty\). As a result, we have \(h(N_{1,2})=\infty\geq 1\). Otherwise, if \(N_{1,1}\geq N_{1,2}>0\), then by definition of \(N_{1,1}\), we have

\[h(N_{1,1})\ \geq\ \sum_{a\in\overline{B}^{c}}\frac{d(\nu_{1},x_{1,a})}{d(\nu_{ a},x_{1,a})}=1.\]

Hence we finish proving the first part of Theorem D.1.

To see the necessity of the stated optimality conditions for \(\mathbf{O}\) observe that we cannot have \(N_{1}^{\star}=0\) or \(N_{a}^{\star}=0\) as that implies that the index \(W_{a}(N_{1}^{\star},N_{a}^{\star})\) is zero. Further, if \(W_{a}(N_{1}^{\star},N_{a}^{\star})>\log(1/(2.4\delta))\), the objective improves by reducing \(N_{a}^{\star}\). Thus the constraints (20) must be tight. To see the tightness of (20) again note that the derivative of \(W_{a}(N_{1},N_{a})\) with respect to \(N_{1}\) and \(N_{a}\), respectively, equals \(d(\mu_{1},x_{1,a})\) and \(d(\mu_{a},x_{1,a})\).

Now, perturbing \(N_{1}\) by a tiny \(\epsilon\) and adjusting each \(N_{a}\) by \(\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}\epsilon\) maintains the value of \(W_{a}(N_{1}^{\star},N_{a}^{\star})\). Thus, at optimal \(\boldsymbol{N}^{\star}\), necessity of tightness of inequalities in (20) follows. This condition can also be seen through the Lagrangian (see [2]).

The fact that these three criteria uniquely specify the optimal solution follows from our analysis above. Since the convex problem \(\mathbf{O}\) has a solution, the uniqueness of the solution above satisfying the necessary conditions implies that this uniquely solves \(\mathbf{O}\). 

To prove Propositions 2.1 and 2.2, we need to use the Implicit function theorem. For that, we define the following functions,

\[J_{1}(\boldsymbol{N})=g(\boldsymbol{\mu},\boldsymbol{N})=\sum_{a \neq 1}\frac{d(\mu_{1},x_{1,a}(N_{1},N_{a}))}{d(\mu_{a},x_{1,a}(N_{1},N_{a}))}-1,\] \[\forall\ a\in[K]/\{1\} J_{a}(\boldsymbol{N},I)=N_{1}d(\mu_{1},x_{1,a}(N_{1},N_{a}))+N_{a}d( \mu_{a},x_{1,a}(N_{1},N_{a}))-I,\] \[J_{K+1}(\boldsymbol{N},N)=\sum_{a\in[K]}N_{a}-N,\]

where \(\boldsymbol{N}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in[K])\), \(I\in\mathbb{R}\), \(N\in\mathbb{R}_{+}\), and, for every \(a\in[K]/\{1\}\), \(x_{1,a}(N_{1},N_{a})=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{a}}\).

Using these functions, for every non-empty \(B\subseteq[K]/\{1\}\), we define the vector valued function

\[\boldsymbol{J}_{B}(\boldsymbol{N},I,N)=\left[J_{1}(\boldsymbol{N}),\ (J_{a}( \boldsymbol{N},I))_{a\in B},\ J_{K+1}(\boldsymbol{N},N)\right].\]

We call \(\boldsymbol{J}_{[K]/\{1\}}(\cdot)\) as \(\boldsymbol{J}(\cdot)\). Recall that \(\overline{B}\) denotes \(B\cup\{1\}\).

For every \(m\geq 1\), we use the notation \(\mathbf{0}_{m}\) to denote a \(m\)-dimensional vector with all entries set to zero. Observe that, for every \(B\subseteq[K]/\{1\}\), \(\boldsymbol{J}_{B}(\boldsymbol{N},I,N)=\boldsymbol{\Phi}_{B}(\boldsymbol{N},I, \mathbf{0}_{K},N)\), where the function \(\boldsymbol{\Phi}_{B}(\cdot)\) is defined in Appendix C.

Lemma D.1 is essential for proving Proposition 2.1.

**Lemma D.1**.: _For every \(N>0\) and non-empty \(B\subseteq[K]/\{1\}\), if \(\hat{\boldsymbol{N}}=(\hat{N}_{a}\in\mathbb{R}_{\geq 0}:a\in[K])\) satisfies \(\sum_{a\in\overline{B}^{c}}\hat{N}_{a}<N\), and \(\boldsymbol{J}_{B}(\hat{\boldsymbol{N}},\hat{I},N)=\mathbf{0}_{|\overline{B}| +1}\), then, the Jacobian of \(\boldsymbol{J}_{B}(\cdot)\) with respect to the arguments \((\boldsymbol{N}_{\overline{B}},I)\) is invertible at \((\hat{\boldsymbol{N}},\hat{I},N)\)._

Proof.: We have \(\boldsymbol{J}_{B}(\boldsymbol{N},I,N)=\boldsymbol{\Phi}_{B}(\boldsymbol{N},I, \mathbf{0}_{K},N)\), for every tuple \(\boldsymbol{N},I,N\), and non-empty \(B\subseteq[K]/\{1\}\). As a result, Lemma D.1 follows from statement 3 of Lemma C.1. 

For every non-empty subset \(B\subseteq[K]/\{1\}\), we define the function \(\widetilde{\boldsymbol{J}}_{B}(\cdot)\) to be the first \(|B|+1\) components of the vector valued function \(\boldsymbol{J}_{B}(\cdot)\), or in other words,

\[\widetilde{\boldsymbol{J}}_{B}(\cdot)\ =\ [\ J_{1}(\cdot)\,\ (J_{a}(\cdot))_{a\in B}\ ]\,.\]

Observe that \(\widetilde{\boldsymbol{J}}_{B}(\cdot)\) depends only on the tuple \(\boldsymbol{N}\) and \(I\), and doesn't depend on \(N\). Also for every tuple \((\boldsymbol{N},I)\in\mathbb{R}_{\geq 0}^{K}\), \(\widetilde{\boldsymbol{J}}_{B}(\boldsymbol{N},I)=\widetilde{\boldsymbol{\Phi} }_{B}(\boldsymbol{N},I,\mathbf{0}_{K})\), where \(\widetilde{\boldsymbol{\Phi}}_{B}\) is defined in Appendix C.

We now proceed on proving Propositions 2.1 and 2.2.

**Proof of Proposition 2.1:** Observe that, for every non-empty \(B\subseteq[K]/\{1\}\), solving the system (2) is equivalent to solving for the pair \(\bm{N}_{\overline{B}},I_{B}\) in \(\bm{J}_{B}((\bm{N}_{\overline{B}},\bm{N}_{\overline{B}^{c}}),I_{B},N)=0\).

For every \(I\geq 0\), by Theorem D.1, there is a unique \(\bm{N}_{\overline{B}}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B})\) for which, \(\widetilde{\bm{J}}_{B}((\bm{N}_{\overline{B}},\bm{N}_{\overline{B}^{c}}),I)= \bm{0}_{|B|+1}\). We denote that solution using \(\bm{N}_{\overline{B}}(I)\) (we supress the dependence on \(\bm{N}_{\overline{B}^{c}}\) for cleaner presentation, and also because we will be treating \(\bm{N}_{\overline{B}^{c}}\) like a constant in the rest of the proof). Since, \(\widetilde{\bm{J}}_{B}(\bm{N},I)=\widetilde{\bm{\Phi}}_{B}(\bm{N},I,\bm{0}_{K})\), by the Implicit function theorem and using statement 1 of Lemma C.1, the function \(I\to\bm{N}_{\overline{B}}(I)\) is continuously differentiable. Also, we have

\[\bm{N}_{\overline{B}}^{\prime}(I)\ =\ \ -\left(\frac{\partial\widetilde{\bm{J}}_{B }}{\partial\bm{N}_{\overline{B}}}\right)^{-1}\frac{\partial\widetilde{\bm{J}} _{B}}{\partial I}\ =\ \ -\left(\frac{\partial\widetilde{\bm{\Phi}}_{B}}{ \partial\bm{N}_{\overline{B}}}\right)^{-1}\frac{\partial\widetilde{\bm{\Phi}} _{B}}{\partial I},\]

where the right most quantity is evaluated at the tuple \(((\bm{N}_{\overline{B}}(I),\bm{N}_{\overline{B}^{c}}),I,\bm{0}_{K})\). Moreover, using statement 2 of Lemma C.1, we have,

\[\sum_{a\in\overline{B}}N_{a}^{\prime}(I)\ =\ \bm{1}_{|B|+1}^{T}\bm{N}_{ \overline{B}}^{\prime}(I)\ =\ -\bm{1}_{|B|+1}^{T}\left(\frac{\partial\widetilde{\bm{\Phi}}_{B}}{ \partial\bm{N}_{\overline{B}}}\right)^{-1}\frac{\partial\widetilde{\bm{\Phi}} _{B}}{\partial I}\ \geq\ \sum_{a\in B}\frac{1}{d(\mu_{a},\mu_{1})}>0\]

As a result, the function \(\sum_{a\in\overline{B}}N_{a}(I)\) is strictly increasing in \(I\) with a derivative atleast \(\sum_{a\in B}\frac{1}{d(\mu_{a},\mu_{1})}\). Also, for \(I=0\), the unique solution is \(N_{1}(0)=N_{1,1}\) and \(N_{a}(0)=0\) for every \(a\in B\). As a result, as \(I\) increases from \(0\) to \(\infty\), \(\sum_{a\in\overline{B}}N_{a}(I)\) increases from \(N_{11}\) to \(\infty\) monotonically. Hence, for every \(N\geq N_{11}+\sum_{a\in B^{c}}N_{a}\), we can find a unique \(I_{B}\) for which \(\sum_{a\in\overline{B}}N_{a}(I_{B})+\sum_{a\in\overline{B}^{c}}N_{a}=N\). Therefore \(\bm{N}_{\overline{B}}(I_{B}),I_{B}\) becomes the unique tuple to satisfy, \(\bm{J}_{B}((\bm{N}_{\overline{B}},\bm{N}_{\overline{B}^{c}}),I_{B},N)=\bm{0} _{|B|+2}\). \(\Box\)

**Proof of Proposition 2.2:** Recall that solving \(\mathbf{O}\) in (20) is equivalent to solving (1). With this observation Proposition 2.2 follows directly from the definition of \(\bm{N}_{[K]/\{1\}}(1),I_{[K]/\{1\}}(1)\) and the second statement of Theorem D.1. \(\Box\)

### Single variable formulation of the lower bound problem and intuition behind the anchor function

Now we show that the \(K\)-variable convex optimization problem \(\mathbf{O}\) defined in (20) can be reduced to a single variable convex optimization problem involving only \(N_{1}\). To see this, observe that \(\mathbf{O}\) is equivalent to the problem

\[\mathbf{O}_{1} \min\sum_{a\in[K]}N_{a}\] s.t. \[\forall a\neq 1,\ W_{a}(N_{1},N_{a})\ =\ N_{1}d(\mu_{1},x_{1,a})+N_{a}d(\mu_{a},x_{1,a})\ =\ \log(1/(2.4\delta))\] \[\text{where}\ x_{1,a}=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{ a}},\ \text{and}\ \forall a\in[K],\ N_{a}\geq 0.\]

This follows from the proof of Theorem D.1.

We first make some observations about \(\mathbf{O}_{1}\).

1. Upon fixing some \(N_{1}>0\), \(N_{a}\to W_{a}(N_{1},N_{a})\) is strictly increasing for every \(a\neq 1\). Moreover, we have \[W_{a}(N_{1},0)=0\quad\text{and}\quad\lim_{N_{a}\to\infty}W_{a}(N_{1},N_{a})=N_ {1}d(\mu_{1},\mu_{a}).\] As a result, every feasible solution of \(\mathbf{O}_{1}\) must satisfy \(N_{1}d(\mu_{1},\mu_{a})>\log(1/(2.4\delta))\) for every \(a\neq 1\), which implies \(N_{1}>\frac{\log(1/(2.4\delta))}{\min_{a\neq 1}d(\mu_{1},\mu_{a})}\).

2. Using the preceding observation, for every \(a\neq 1\) and \(N_{1}>\frac{\log(1/(2.4\delta))}{\min_{a\neq 1}d(\mu_{1},\mu_{a})}\), we can find a unique \(N_{a}\) such that \(W_{a}(N_{1},N_{a})=\log(1/(2.4\delta))\). We use \(\bar{N}_{a}(N_{1})\) to denote that unique \(N_{a}\) as a function of \(N_{1}\). Using the Implicit function theorem, it is easy to prove that the function \(N_{1}\to\bar{N}_{a}(N_{1})\) is differentiable for every \(N_{1}>\frac{\log(1/(2.4\delta))}{\min_{a\neq 1}d(\mu_{1},\mu_{a})}\) with the derivative being \[\bar{N}_{a}^{\prime}(N_{1})\ =\ -\frac{d(\mu_{1},z_{a}(N_{1}))}{d(\mu_{a},z_{a}( N_{1}))}\quad\text{where}\quad z_{a}(N_{1})\ =\ \frac{N_{1}\mu_{1}+\bar{N}_{a}(N_{1})\mu_{a}}{N_{1}+\bar{N}_{a}(N_{1})}.\]

It follows that \(\mathbf{O}_{1}\) is equivalent to the single variable optimization problem:

\[\mathbf{O}_{2} \min f(N_{1})\ \stackrel{{\text{def.}}}{{=}}\ N_{1}+\sum_{a \neq 1}\bar{N}_{a}(N_{1})\] s.t. \[N_{1}>\frac{\log(1/(2.4\delta))}{\min_{a\neq 1}d(\mu_{1},\mu_{a})}.\]

Further,

\[f^{\prime}(N_{1}) =\ 1+\sum_{a\neq 1}\bar{N}_{a}^{\prime}(N_{1})\] \[=\ 1-\sum_{a\neq 1}\frac{d(\mu_{1},z_{a}(N_{1}))}{d(\mu_{a},z_{a }(N_{1}))},\]

which is exactly negative of the anchor function defined in Section 2 w.r.t. the allocation where arm 1 gets \(N_{1}\)samples and every sub-optimal arm \(a\neq 1\) gets \(\bar{N}_{a}(N_{1})\) samples. Furthermore, \(f^{\prime}(N_{1})\) is strictly increasing in \(N_{1}\) and increases from \(-\infty\) to \(1\) as \(N_{1}\) increases from \(\frac{\log(1/(2.4\delta))}{\min_{a\neq 1}d(\mu_{1},\mu_{a})}\) to \(\infty\). As a result, \(\mathbf{O}_{2}\) is a convex problem w.r.t. \(N_{1}\) and the optimal \(N_{1}\) solving \(\mathbf{O}_{2}\) is uniquely identified by the condition:

\[\sum_{a\neq 1}\frac{d(\mu_{1},z_{a}(N_{1}))}{d(\mu_{a},z_{a}(N_{1}))}-1=0,\]

which is equivalent to saying that the anchor function \(g\) must be zero when evaluated at the allocation where the first arm gets \(N_{1}\) samples and every arm \(a\neq 1\) gets \(\bar{N}_{a}(N_{1})\) samples. Figure 3 shows an illustrative plot of \(\mathbf{O}_{2}\)'s objective \(f(N_{1})\).

This observation also implies that the unique \(\beta\) which makes the \(\beta\)-EB-TCB(I) policy in [16] asymptotically optimal is uniquely identified by the first order conditions: 1) the anchor function \(g\) must be zero and 2) indexes of the sub-optimal arms must be equal. Hence allocations made by every

Figure 3: Illustrative plot of \(\mathbf{O}_{2}\)s objective \(f(N_{1})\)

asymptotically optimal sampling policy must converge to these first order conditions. Both the fluid dynamics in Section 4 and Proposition 5.1 in Section 5, respectively, show that the sampling policies of (I)AT2 algorithm converges to these first order conditions in a fluid model and in the proposed algorithms.

### Sub-optimality of TCB(I)

[22] implicitly assumes that the optimal proportion is uniquely identified by the condition that indexes of every sub-optimal arm under the proportion are equal. Note that the optimal proportion is a \(K\) dimensional vector and the conditions mentioned in [22, Lemma 2, statement 2] has \(K-1\) equations (\(K-2\) equations to maintain equality of \(K-1\) indexes and one more equation to make sure that all the entries in the \(K\)-dimensional vector add up to \(1\)). Moreover, in Appendix D.1, changing the variables of the problem \(\mathbf{O}_{2}\) to capture proportion of samples allocated to every arm, it is not hard to prove that for every value of \(w_{1}\in(0,1)\), we can get a unique proportion \((w_{1},w_{2},\ldots,w_{K})\) such that \(\sum_{a\in[K]}w_{a}=1\) and index of all sub-optimal arms under the proportion are equal. By the argument in Appendix D.1, the unique optimal proportion out of these infinite no. of proportions satisfying equality of the indexes, is uniquely identified by the necessary and sufficient condition that the anchor function \(g(\cdot)\) evaluated at that proportion must be zero. Without this condition, the allocation cannot be optimal. In the numerical experiments of Appendix J.2, we see that the anchor function doesn't always converge to zero for the TCB(I) algorithm. As a result, allocations made by TCB(I) can be sub-optimal.

## Appendix E Proofs from Section 4

**Proof of Theorem 4.1:** We first prove all the steps of Theorem 4.1 except for showing the existence of \(\beta>0\) and independent of \(N\) such that \(I^{\prime}_{b}(N)>\beta\). That requires intermediate lemmas and is done separately.

First suppose that \(B\) contains a singleton index \(b\). Define \(N_{1}(N)\) and \(N_{b}(N)\) using IFT through the equations

\[\sum_{a\neq 1}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}-1=0\] (24)

and \(\sum_{a}N_{a}=N\). For each \(a\), letting \(x^{\prime}_{1,a}\) denote the derivative of \(x_{1,a}\) with respect to \(N_{1}\), \(\widetilde{x}^{\prime}_{1,b}\) denote the derivative of \(x_{1,b}\) with respect to \(N_{b}\). It is easy to check that

\[\widetilde{x}^{\prime}_{1,b}=-\frac{N_{1}}{N_{a}}x^{\prime}_{1,b},\] (25)

and each \(x^{\prime}_{1,a}=\frac{N_{a}\Delta_{a}}{(N_{1}+N_{a})^{2}}\). Differentiating (24) with respect to \(N\), observing that \(N^{\prime}_{b}=1-N^{\prime}_{1}\), we get

\[N^{\prime}_{1}\sum_{a\neq 1}h_{a}N_{a}=N_{1}h_{b}(1-N^{\prime}_{1}).\]

It follows that

\[N^{\prime}_{1}=\frac{N_{1}h_{b}}{\sum_{a\neq 1}h_{a}N_{a}+N_{1}h_{b}}\]

as stipulated. Also \(N^{\prime}_{b}=\frac{\sum_{a\neq 1}h_{a}N_{a}}{\sum_{a\neq 1}h_{a}N_{a}+N_{1}h_{b}}\).

Now consider the case where \(g=0\), and we have set \(B\subset[K]/1\) of indices where the indexes are equal, they are higher for the remaining set. Cardinality of \(B\) is at least 2. We want to argue that as \(N\) increases, and the equality of indexes in \(B\) is maintained along with \(g=0\), then the tied indexes will increase with \(N\).

We have for \(b,a\in B\)

\[N_{1}d(\mu_{1},x_{1,b})+N_{b}d(\mu_{b},x_{1,b})=N_{1}d(\mu_{1},x_{1,a})+N_{a}d (\mu_{a},x_{1,a}).\] (26)Furthermore, \(g=0\), i.e.,

\[\sum_{a\neq 1}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}=1.\] (27)

Keeping a particular \(b\in B\) fixed, differentiating with respect to \(N\), (since for each \(a\), by definition of \(x_{1,a}\), \(N_{1}d^{\prime}(\mu_{1},x_{1,a})+N_{a}d^{\prime}(\mu_{a},x_{1,a})=0\)) we see from (26) that

\[N_{1}^{\prime}d(\mu_{1},x_{1,b})+{N_{b}}^{\prime}d(\mu_{b},x_{1,b})=N_{1}^{ \prime}d(\mu_{1},x_{1,a})+{N_{a}}^{\prime}d(\mu_{a},x_{1,a}).\]

Using (26) again in the above equality,

\[N_{a}^{\prime}=\frac{1}{N_{1}d(\mu_{a},x_{1,a})}\left(N_{a}d(\mu_{a},x_{1,a})- N_{b}d(\mu_{b},x_{1,b})\right)N_{1}^{\prime}+\frac{d(\mu_{b},x_{1,b})}{d(\mu_{a},x _{1,a})}N_{b}^{\prime}.\] (28)

Then from (27), we have that

\[N_{1}^{\prime}\sum_{a\neq 1}f(\boldsymbol{\mu},a,\boldsymbol{N})x_{1,a}^{ \prime}+\sum_{a\in B}f(\boldsymbol{\mu},a,\boldsymbol{N})\widetilde{x}_{1,a}^ {\prime}N_{a}^{\prime}=0.\] (29)

(Recall that for each \(a\), \(x_{1,a}^{\prime}\) denotes the derivative of \(x_{1,a}\) with respect to \(N_{1}\) and \(\widetilde{x}_{1,a}^{\prime}\) denotes the derivative of \(x_{1,a}\) with respect to \(N_{a}\).)

Plugging (28) and (25) in (29), multiplying each term by \(N_{1}^{2}\), we see that \(N_{1}^{\prime}\) is a ratio of

\[N_{1}N_{b}^{\prime}d_{b,b}h_{B}\]

with

\[h(N)+N_{b}d_{b,b}h_{B}.\]

Then,

\[N_{b}^{\prime}=N_{1}^{\prime}\frac{h(N)d_{b,b}^{-1}+N_{b}h_{B}}{N_{1}h_{B}}.\]

In particular, since, \(\sum_{a}N_{a}^{\prime}=1\), (3) follow. Statement 3 of Theorem 4.1 follows from (3) and expression for \(I_{a}(N)\). Since \(I_{a}^{\prime}(N)>0\) index is non-decreasing in \(N\). Furthermore, \(\lim\limits_{N\to\infty}I_{a}(N)=N_{a}d(\mu_{a},\mu_{1})\). 

To prove the existence of \(\beta>0\) and independent of \(N\) such that \(I_{b}^{\prime}(N)>\beta\), we need Lemmas (E.1), (E.2) and (E.3). In Lemma E.3, we argue that the indexes in set \(B\) grow linearly with the number of samples. Since index for arm \(a\in\overline{B}^{c}\) are bounded, eventually indexes in set \(B\) catch-up with other indexes.

Some notation first. Observe that \(d(\mu_{1},x)-d(\mu_{a},x)\) is a continuous and strictly decreasing function of \(x\in[\mu_{a},\mu_{1}]\). It equals \(d(\mu_{1},\mu_{a})\) for \(x=\mu_{a}\) and \(-d(\mu_{a},\mu_{1})\) for \(x=\mu_{1}\). Let \(x_{a}\in(\mu_{a},\mu_{1})\) be such that

\[d(\mu_{1},x_{a})=d(\mu_{a},x_{a}).\]

Furthermore, let

\[\widetilde{a}=\arg\max_{a}\frac{d(\mu_{1},x_{1,a})}{d(\mu_{a},x_{1,a})}.\]

Let \(x(\widetilde{a})\) be such that

\[d(\mu_{1},x(\widetilde{a}))=(K-1)^{-1}d(\mu_{a},x(\widetilde{a})).\]

It is guaranteed to exist since the ratio \(d(\mu_{a},x)/d(\mu_{1},x)\in(0,\infty)\) is monotonic in \(x\).

Next, let

\[d_{a}=\frac{\mu_{1}-x_{a}}{x_{a}-\mu_{a}}\quad\text{ and }\quad d(\widetilde{a} )=\frac{\mu_{1}-x(\widetilde{a})}{x(\widetilde{a})-\mu_{a}}.\]

Since \(K\geq 2\), we have \(x(\widetilde{a})\geq x_{\widetilde{a}}\), and \(d(\widetilde{a})\leq d_{\widetilde{a}}\).

**Lemma E.1**.: _Suppose that \(g=0\). Then,_

\[\left(1+\sum_{a\neq 1}d_{a}\right)^{-1}N\leq N_{1}\leq\left(1+d(\widetilde{a}) \right)^{-1}N.\] (30)

**Lemma E.2**.: _Suppose that \(g=0\). Then,_

1. _there exist constants_ \(H\) _and_ \(D\) _such that_ \(h_{a}\leq H\) _for all_ \(a\)_, and_ \(d_{a,a}^{-1}\leq D\) _for all_ \(a\)_._
2. _Further, there exists an_ \(\widetilde{a}\) _such that_ \(N_{\widetilde{a}}>\alpha N\) _for some_ \(\alpha>0\) _and the corresponding_ \(h_{\widetilde{a}}\) _is bounded from below by a positive constant._

**Lemma E.3**.: _Suppose that \(g=0\), and for \(B\subset[K]/1\) the indexes are all equal and are strictly higher for the remaining set. Then there exists a constant \(\beta>0\) such that_

\[N_{1}^{\prime}d(\mu_{1},x_{1,a})+N_{a}^{\prime}d(\mu_{a},x_{1,a})=I_{B}^{ \prime}(N)>\beta.\]

**Proof of Lemma (E.1):** Since \(g=0\), it follows that for each \(a\in[K]\setminus 1\),

\[\frac{d_{1,a}}{d_{a,a}}\leq 1.\]

Thus, \(x_{1,a}\geq x_{a}\). This in turn implies that for each \(a\),

\[N_{1}\geq N_{a}d_{a}^{-1}.\]

The above follows from substituting for \(x_{1,a}\) in the inequality \(x_{1,a}\geq x_{a}\), and from the definition of \(d_{a}\). Moreover, since \(g=0\), it also follows that for each \(a\neq 1\), \(x((\widetilde{a})\geq x_{1,a}\), implying

\[N_{1}d((\widetilde{a}))\leq N_{a}.\]

Then,

\[N_{1}\left(1+\sum_{a\neq 1}d_{a}\right)\geq N\]

and

\[N_{1}(1+d(\widetilde{a}))\leq N,\]

and the result follows. \(\Box\)

**Proof of Lemma (E.2):** Recall the definitions of \(h_{a}\) and \(f(\boldsymbol{\mu},a,\boldsymbol{N})\) from Section 4.

Since, \(g=0\) implies that \(x_{1,a}\geq x_{a}\) for all \(a\), it follows that \(d_{a,a}=d(\mu_{a},x_{1,a})\geq d(\mu_{a},x_{a})\). In particular, for all \(a\)

\[d_{a,a}^{-1}\leq D\]

for \(D=\max_{a}d(\mu_{a},x_{a})^{-1}\).

Further, \(d^{\prime}(\mu_{a},x_{1,a})\) is continuous in \(x_{1,a}\) and is bounded from above by \(\sup_{x_{a}\leq x_{1,a}\leq\mu_{1}}d^{\prime}(\mu_{a},x_{1,a})\). Similarly, \(-d^{\prime}(\mu_{1},x_{1,a})\) is bounded from above by \(\sup_{x_{a}\leq x_{1,a}\leq\mu_{1}}-d^{\prime}(\mu_{1},x_{1,a})\). This implies that \(f(\boldsymbol{\mu},a,\boldsymbol{N})\) is bounded from above by a positive constant and hence so is \(h_{a}\).

To see part 2, observe from definition of \(x(\tilde{a})\) that \(x_{1,\tilde{a}}\leq x(\tilde{a})\). It follows that

\(N_{\tilde{a}}\geq N_{1}d_{\tilde{a}}^{-1}\). Therefore,

\[N_{\tilde{a}}\geq Nd_{\tilde{a}}^{-1}(1+\sum_{a\neq 1}d_{a})^{-1}.\] (31)

Again, \(x_{1,\tilde{a}}\leq x(\widetilde{a})\). Therefore, \(d_{\widetilde{a},\widetilde{a}}=d(\mu_{\widetilde{a}},x_{1,\widetilde{a}}) \leq d(\mu_{\widetilde{a}},x(\widetilde{a}))\) and \(d_{1,\widetilde{a}}=d(\mu_{1},x_{1,\widetilde{a}})\geq d(\mu_{1},x(\widetilde {a}))\).

Further, \(d^{\prime}(\mu_{\widetilde{a}},x_{1,\widetilde{a}})\) is continuous in \(x_{1,\widetilde{a}}\) and is bounded from below by

\[\inf_{x_{\widetilde{a}}\leq x_{1,\widetilde{a}}\leq x(\widetilde{a})}d^{\prime}( \mu_{\widetilde{a}},x_{1,\widetilde{a}}).\]

Similarly, \(-d^{\prime}(\mu_{1},x_{1,\widetilde{a}})\) is bounded from below by \(\inf_{x_{\widetilde{a}}\leq x_{1,\widetilde{a}}\leq x(\widetilde{a})}-d^{ \prime}(\mu_{1},x_{1,\widetilde{a}})\).

Thus, \(f(\bm{\mu},\widetilde{a})\) is bounded from below. Further, since each \(N_{a}\leq d_{a}N_{1}\), \(N_{1}^{2}/(N_{1}+N_{\widetilde{a}})^{2}\geq(1+d_{\widetilde{a}})^{-2}\), hence, \(h_{\widetilde{a}}\) is bounded from below by a positive constant. 

**Proof of Lemma (E.3):** Recall from (31) that

\[N_{\widetilde{a}}\geq Nd_{\widetilde{a}}^{-1}(1+\sum_{a\neq 1}d_{a})^{-1}.\] (32)

Also, recall the definition of \(f(\bm{\mu},a,\bm{N})\) and \(h_{a}\) from Section 4.

Because of (30) and (32), and since \(N_{\widetilde{a}}\leq N\), we see that \(f(\bm{\mu},\widetilde{a})\) is bounded from below by a positive constant, and the same is true for \(h_{\widetilde{a}}\).

If \(\widetilde{a}\in B\), recall that \(h_{B}=\sum_{a\in B}h_{a}d_{a,a}^{-1}\). Thus, \(h_{B}\) is greater than a constant times \(N\). This ensures that \(N_{\widetilde{a}}^{\prime}\) is bounded from below by a positive constant. Since \(d_{\widetilde{a},\widetilde{a}}\) is also bounded from below by a positive constant, we conclude that there exists a \(\beta>0\) such that \(I_{B}^{\prime}(N)>\beta\).

If \(\widetilde{a}\notin B\), then recalling that \(h(N)=\sum_{a\in B^{c}/1}h_{a}N_{a}\), we conclude that \(N_{\widetilde{a}}^{\prime}\) is bounded from below by a positive constant. This implies that as \(N\) increases by a positive fraction, so does each \(N_{a}\) for \(a\in B\). This in turn ensures that then \(h_{B}\) is thereafter bounded from below by a positive constant. In particular, after some delay we have \(I_{B}^{\prime}(N)>\beta\) for some \(\beta>0\). 

### Fluid dynamics starting at \(g<0\)

Proposition E.1 provides us the ODEs by which the fluid allocations evolve in step 3 of the description of fluid dynamics when \(g<0\).

**Proposition E.1**.: _Now consider the case where \(g<0\) at total allocations \(N\), and \(B\) again denotes the set of arms whose indexes have the minimum value. Then, till \(I_{B}(N)\) increases with \(N\) to either hit an index in \(\overline{B}^{c}\), or for \(g\) to equal zero, whichever is earlier, \(I_{B}^{\prime}(N)=\left(\sum_{a\in B}d_{a,a}^{-1}\right)^{-1}\), and for \(a\in B\), \(N_{a}^{\prime}=d_{a,a}^{-1}\left(\sum_{a\in B}d_{a,a}^{-1}\right)^{-1}\). In particular, \(I_{B}(N)\) and each \((N_{a}\), \(a\in B)\), are increasing functions of \(N\)._

Proof.: Let \(i_{1}\) denote the arm corresponding to a minimum index. Recall that \(\bm{\omega}^{\star}=(\omega^{\star}_{a}:a\in[K])\) denote the optimal proportions to the lower bound problem. Consider \(\hat{N}_{a}=\frac{\omega^{\star}_{a}}{\omega^{\star}_{1}}N_{1}\). Recall that at these samples, \(g=0\) and all the indexes are equal. Let \(\hat{I}\) denote the corresponding value of the indexes at this allocation.

First we argue that \(N_{i_{1}}<\hat{N}_{i_{1}}\).

Suppose this is not true, then \(g<0\) implies that for \(N_{1}\) fixed, there exists an arm \(a\) so that \(N_{a}<\hat{N}_{a}\), else if each \(N_{a}\geq\hat{N}_{a}\) then since \(g\) increases with \(N_{a}\), we would have \(g\geq 0\). This contradiction implies that index for arm \(a\) is \(<\hat{I}_{B}(N)\). It follows that the index corresponding to \(i_{1}\) is \(<\hat{I}_{B}(N)\). Since the index increases with \(N_{i_{1}}\), it follows that \(N_{i_{1}}<\hat{N}_{i_{1}}\).

Thus, initially \(N\) increases due to increase in \(N_{i_{1}}\). Let \(B=\{i_{1}\}\). Suppose, iteratively that \(B=\{i_{1},\ldots,i_{j-1}\}\), denotes the smallest indexes that are equal and increase with \(N\) and \(g<0\). Proof follows by observing that the derivative of each index \(a\in B\) satisfies the relation \(N_{a}^{\prime}d_{a,a}=I_{B}^{\prime}(N)\). Further, \(\sum_{a\in B}N_{a}^{\prime}=1\).

Thus, as \(N\) increases, each \(N_{a}(N),a\in B\) increases, so that \(g\) increases. Since all indexes corresponding to \(\overline{B}^{c}\) are constant, as \(N\) increases, either \(g=0\) first or another index \(I_{j}\) becomes equal to \(I_{B}(N)\)

### Fluid dynamics of the \(\beta\)-EB-TCB algorithm ([16])

For every \(\beta\in(0,1)\) and allocation \(\bm{N}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in[K])\), we define the \(\beta\)-anchor function as,

\[g(\bm{N};\beta)\;=\;\beta-\frac{N_{1}}{\sum_{a\in[K]}N_{a}}.\]

Note that, if \(g(\bm{N};\beta)=0\), then \(\beta\)-fraction of the total no. of samples in the allocation \(\bm{N}\) is allocated to the first arm. The fluid dynamics for the \(\beta\)-EB-TCB algorithm (see [16]) can be constructed similarly to that of the Anchored Top Two algorithm, by replacing the anchor function \(g(\bm{\mu},\bm{N})\) with the \(\beta\)-anchor function \(g(\bm{N};\beta)\) in Section 4.

**Existence of fluid dynamics:** Recall that, for every \(B\subseteq[K]/\{1\}\), \(\overline{B}\) denotes the set \(B\cup\{1\}\). Lemma E.4 and Proposition E.2 are essential for constructing the fluid behavior for \(\beta\)-EB-TCB algorithm.

**Lemma E.4**.: _Given a non-empty \(B\subseteq[K]/\{1\}\), some tuple \(\bm{N}_{\overline{B}^{c}}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B}^{c})\) and \(I\geq 0\), there is a unique tuple \(\bm{N}_{\overline{B}}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B})\) which satisfies,_

\[N_{1}\;=\;\beta\sum_{a\in[K]}N_{a},\quad\text{and}\]

\[\text{for every }a\in B,\quad N_{1}d(\mu_{1},x_{1,a})+N_{a}d(\mu_{a},x_{1,a})\;=\;I,\]

_where \(x_{1,a}=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{a}}\)._

_Moreover, if we define \(N_{1,1}=\beta\sum_{a\in\overline{B}^{c}}N_{a}\quad\text{and}\quad N_{1,2}= \frac{I}{\min_{a\in B}d(\mu_{1},\mu_{a})}\), then \(N_{1}\geq\max\{N_{1,1},N_{1,2}\}\)._

Proof.: Proof of this lemma follows an argument similar to the proof of Theorem D.1.

First we fix some \(I\geq 0\) and \(N_{1}\geq N_{1,2}\). Note that for every \(a\in B\), as \(N_{a}\) increases from \(0\) to \(\infty\), \(N_{1}d(\mu_{1},x_{1,a})+N_{a}d(\mu_{a},x_{1,a})\) increases monotonically from \(0\) to \(N_{1}d(\mu_{1},\mu_{a})\). Since \(N_{1}\geq N_{1,2}\geq\frac{I}{d(\mu_{1},\mu_{a})}\), we have \(N_{1}d(\mu_{1},\mu_{a})\geq I\). This implies, for a given \(N_{1}\), there is a unique value of \(N_{a}\) for which \(N_{1}d(\mu_{1},x_{1,a})+N_{a}d(\mu_{a},x_{1,a})=I\). We call this value \(N_{a}(N_{1})\) for every \(a\in B\).

Observe that \(N_{1}\to N_{a}(N_{1})\) is a strictly decreasing function of \(N_{1}\), and if \(N_{1}=N_{1,2}\), then there exists an \(a\in B\) for which \(N_{a}(N_{1,2})=\infty\). On the other hand, if \(N_{1}\to\infty\), \(N_{a}(N_{1})\to\frac{I}{d(\mu_{a},\mu_{1})}\) for every \(a\in B\).

For every \(N_{1}\), we consider the function

\[h(N_{1};\beta)=\beta-\frac{N_{1}}{N_{1}+\sum_{a\in B}N_{a}(N_{1})+\sum_{a\in \overline{B}^{c}}N_{a}}.\]

Note that \(h(N_{1};\beta)\) is the value of \(g(\bm{N};\beta)\), when the tuple \(\bm{N}\) has \(N_{a}=N_{a}(N_{1})\) for every \(a\in B\). Note that \(N_{1}\to h(N_{1};\beta)\) is strictly decreasing for \(N_{1}\geq N_{1,2}\). Moreover, as \(N_{1}\to\infty\), \(h(N_{1};\beta)\to\beta-1<0\). In the rest of the argument, we show that \(h(\max\{N_{1,1},N_{1,2}\};\beta)\geq 0\). After we prove this, we can find a unique \(N_{1}\) for which \(h(N_{1};\beta)=0\). Using this, we take \(N_{a}=N_{a}(N_{1})\) for every \(a\in B\) to obtain our unique tuple \(\bm{N}_{\overline{B}}\).

Now we consider two cases.

**Case 1:**: If \(N_{1,1}\geq N_{1,2}\), then at \(N_{1}=N_{1,1}\),

\[N_{1,1}\;=\;\beta\sum_{a\in\overline{B}^{c}}N_{a}\;\leq\;\beta(N_{1,1}+\sum_{a \in[K]/\{1\}}N_{a}).\]

As a result,

\[\frac{N_{1,1}}{N_{1,1}+\sum_{a\in\overline{B}^{c}}N_{a}+\sum_{a\in B}N_{a}(N_{1,1})}\leq\beta,\]which implies \(h(N_{1,1};\beta)\geq 0\).

**Case 2:** If \(N_{1,2}\geq N_{1,1}\), then, as we argued before, there exists an \(a\in B\) for which \(N_{a}(N_{1,2})=\infty\). As a result,

\[\frac{N_{1,2}}{N_{1,2}+\sum_{a\in\overline{B}^{c}}N_{a}+\sum_{a\in B}N_{a}(N_{ 1,2})}=0\]

implying \(h(N_{1,2};\beta)=\beta>0\). 

Proposition E.2 stated below is crucial for constructing the fluid dynamics of the \(\beta\)-EB-TCB policy and is analogous to Proposition 2.1 used for constructing the fluid dynamics of the anchored top-two algorithm.

**Proposition E.2**.: _For every non-empty \(B\subseteq[K]/\{1\}\), tuple \(\bm{N}_{\overline{B}^{c}}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B}^{c})\), and \(N\geq\frac{1}{1-\beta}\sum_{a\in\overline{B}^{c}}N_{a}\), there exists a unique tuple \(\bm{N}_{\overline{B}}=(N_{a}\in\mathbb{R}_{\geq 0}:a\in\overline{B})\) and \(I_{B}\geq 0\) for which,_

\[N_{1}\;=\;\beta N,\quad\sum_{a\in[K]}N_{a}\;=\;N,\] \[\text{for every }a\in B,\quad N_{1}d(\mu_{a},x_{1,a})+N_{a}d(\mu_{ a},x_{1,a})\;=\;I_{B},\quad\text{and}\] \[\text{where}\quad x_{1,a}\;=\;\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{ 1}+N_{a}}\quad\text{for every }a\in[K]/\{1\}.\]

_If we denote that tuple by \(\bm{N}_{\overline{B}}(N)\) and \(I_{B}(N)\), then the functions \(N\to\bm{N}_{\overline{B}}(N),I_{B}(N)\) are continuously differentiable with respect to \(N\)._

Proof.: Proof of Proposition E.2 follows by an argument similar to the one used in the proof of Proposition 2.1, by using Lemma E.4 instead of Theorem D.1. Observe that the \(\beta\)-anchor function \(g(\bm{N};\beta)\) is strictly decreasing in \(N_{1}\) and strictly increasing in \(N_{a}\), when \(N_{1}>0\). As a result, statement 1 of Lemma C.1 in Appendix C holds true upon having,

\[\Phi_{1}(\bm{N},\bm{\eta})\;=\;g(\bm{N};\beta)-\eta_{0},\]

and defining the set \(\mathcal{Z}_{B}\) using the modified function \(\bm{\Phi}_{B}\).

With the above modification, if we can find a constant \(\gamma>0\) such that, \(\bm{1}_{|B|+1}^{T}\left(\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial N_ {B}}\right)^{-1}\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial I}\leq- \gamma<0\) for every tuple in \(\mathcal{Z}_{B}\), then statements 2 and 3 of Lemma C.1 also hold true for this modified \(\bm{\Phi}_{B}\). As a result, Proposition E.2 follows using Lemma E.4 by the same argument used for proving Proposition 2.1 using Theorem D.1.

In the rest of the proof we argue the existence of such a constant \(\gamma>0\).

Let \(\bm{v}=(v_{a}:a\in\overline{B})\in\mathbb{R}^{|B|+1}\) be the solution to the system

\[\frac{\partial\widetilde{\bm{\Phi}}_{B}}{\partial\bm{N}_{B}}\bm{v}\;=\;\frac{ \partial\widetilde{\bm{\Phi}}_{B}}{\partial I}.\]

We have \(\frac{\partial\Phi_{1}}{\partial\bm{N}_{\overline{B}}}\bm{v}=\frac{\partial \widetilde{\bm{\Phi}}_{1}}{\partial I}\), and \(N_{1}=\beta\sum_{a\in[K]}N_{a}\), which after some algebraic manipulation implies,

\[-\left(\frac{1}{\beta}-1\right)v_{1}+\sum_{a\in B}v_{a}\;=\;0.\] (33)

(33) further implies,

\[\bm{1}_{|B|+1}^{T}\bm{v}\;=\;\sum_{i=1}^{|B|+1}v_{i}\;=\;\frac{v_{1}}{\beta}.\]

Therefore proving that \(v_{1}\) is upper bounded by a negative constant is sufficient for proving the desired result.

For every \(a\in B\), we have

\[\frac{\partial\Phi_{a}}{\partial\bm{N}_{\overline{B}}}\bm{v}=\frac{\partial\Phi_ {a}}{\partial I}=-1,\]

which implies

\[v_{1}d(\mu_{1},x_{1,a})+v_{a}d(\mu_{a},x_{1,a})\ =\ -1,\] (34)

where \(x_{1,a}=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{a}}\). We use \(d_{1,a}\) and \(d_{a,a}\), respectively, to denote \(d(\mu_{1},x_{1,a})\) and \(d(\mu_{a},x_{1,a})\). For every \(a\in B\), we can eliminate \(v_{a}\) for (33) using (34). After this procedure, we get,

\[-v_{1} = \frac{\sum_{a\in B}\frac{1}{d_{a,a}}}{\frac{1}{\beta}-1+\sum_{a \in B}\frac{d_{1,a}}{d_{a,a}}}\] (35) \[\geq \frac{\sum_{a\in B}\frac{1}{d_{a,a}}}{\frac{1}{\beta}-1+\sum_{a \neq 1}\frac{d_{1,a}}{d_{a,a}}}\] \[\geq \frac{\sum_{a\in B}\frac{1}{d_{a,a}}}{\frac{1}{\beta}-1+\sum_{a \neq 1}\frac{d(\mu_{1},\mu_{a})}{d_{a,a}}}.\quad\text{(since $d_{1,a}\leq d(\mu_{1},\mu_{a})$)}\]

Since \(N_{1}=\beta\sum_{a\in[K]}N_{a}\), using (6) we have \(d_{a,a}=\varTheta(1)\) for all \(a\in[K]/\{1\}\), where the constant hidden in \(\varTheta(\cdot)\) is independent of \(\bm{N}\). As a result, by (35), \(-v_{1}=\varOmega(1)\). Hence we conclude the proof. 

**Constructing the fluid ODEs:** Without loss of generality, we assume that the fluid dynamics starts from a state \(\bm{N}\) where \(g(\bm{N};\beta)=0\). Otherwise,

1. If \(g(\bm{N};\beta)>0\), the algorithm gives samples to arm \(1\) till \(g(\bm{N};\beta)=0\).
2. If \(g(\bm{N};\beta)<0\), the \(\beta\)-EB-TCB algorithm follows the dynamics in Proposition E.1, and reaches \(g(\bm{N};\beta)=0\) in a finite amount of time.

Following Proposition E.2, the algorithm tracks the allocation \(\bm{N}_{\overline{B}}(N)\) at a given time \(N\), where \(B\) denotes the set of minimum index arms. We now determine the ODEs by which the state of the algorithm evolves.

To simplify the notations, we use \(g_{g}\) as a shorthand for \(g(\bm{N}(N);\beta)\) at a given time \(N\). For every \(a\in[K]\), we use \(N_{a},N_{a}^{\prime},I_{B},\) and \(I_{B}^{\prime}\), respectively, to denote \(N_{a}(N),N_{a}^{\prime}(N),I_{B}(N)\) and \(I_{B}^{\prime}(N)\). For every \(a\in\overline{B}^{c}\), we use \(I_{a}\) to denote \(I_{a}(N)\). Also for every \(a\in[K]/\{1\}\), we adopt the notation \(d_{1,a}\) and \(d_{a,a}\), respectively, for the quantities \(d(\mu_{1},x_{1,a})\) and \(d(\mu_{a},x_{1,a})\), where \(x_{1,a}=\frac{N_{1}\mu_{1}+N_{a}\mu_{a}}{N_{1}+N_{a}}\).

For every non-empty \(B\subseteq[K]/\{1\}\) we define the quantity,

\[d_{B}=\left(\sum_{a\in B}\frac{1}{d_{a,a}}\right)^{-1}.\]

We now show the fluid ODEs in the following proposition.

**Proposition E.3** (Fluid ODEs for \(\beta\)-EB-TCB).: _Let us assume the algorithm starts from a state \(\bm{N}(N^{0})=(N_{a}^{0}:a\in[K])\) with \(\sum_{a\in[K]}N_{a}^{0}=N^{0}\), \(N_{1}^{0}=\beta N^{0}\) and \(N^{0}>0\). Let \(B\subseteq[K]/\{1\}\) be the set of arms having minimum index at a given time \(N\geq N^{0}\). The following statements hold true about the allocation \(\bm{N}_{\overline{B}}(N)=(N_{a}(N):a\in\overline{B})\) made by \(\beta\)-EB-TCB algorithm,_

1. _The allocation_ \(\bm{N}(N)=(N_{a}(N):a\in[K])\) _evolves by the following system of ODEs,_ \[N_{1}^{\prime} = \beta,\quad\text{and}\] \[\text{for every}\quad b\in B,\quad N_{b}^{\prime} = \frac{((1-\beta)N-\sum_{a\in B}N_{a})d_{B}+N_{b}d_{b,b}}{Nd_{b,b}}.\] (36)2. _The index_ \(I_{B}(N)\) _of the arms in_ \(B\) _evolves by the following ODE,_ \[I^{\prime}_{B}\ =\ \left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N}\right)d_{B}+ \frac{I_{B}}{N}.\] (37)
3. _There exists a constant_ \(c>0\) _such that,_ \(I^{\prime}_{B}\geq c\)_. On the other hand, indexes of the arms in_ \(\overline{B}^{c}\) _remains upper bounded by_ \(N_{a}^{0}d(\mu_{a},\mu_{1})\)_._

Proof.: **Statement 1:**\(N^{\prime}_{1}=\beta\) follows directly from the fact that \(g_{\beta}=0\).

By definition of \(B\), we have

\[N_{1}d_{1,a}+N_{a}d_{a,a}\ =\ N_{1}d_{1,b}+N_{b}d_{b,b}\] (38)

for every \(a,b\in[K]\). Taking derivative on both sides, we get,

\[N^{\prime}_{1}d_{1,a}+N^{\prime}_{a}d_{a,a}\ =\ N^{\prime}_{1}d_{1,b}+N^{\prime}_ {b}d_{b,b},\]

which implies,

\[N^{\prime}_{a}\ =\ \frac{d_{1,b}-d_{1,a}}{d_{a,a}}N^{\prime}_{1}+\frac{d_{b,b} }{d_{a,a}}N^{\prime}_{b}.\] (39)

Using (38), we have

\[d_{1,b}-d_{1,a}\ =\ \frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N_{1}}.\]

Using the above expression in (39), we get,

\[N^{\prime}_{a}\ =\ \frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{N_{1}d_{a,a}}N^{\prime}_{1 }+\frac{d_{b,b}}{d_{a,a}}N^{\prime}_{b}.\]

Since \(N^{\prime}_{1}=\beta\) and \(N_{1}=\beta N\) (which follows from \(g_{\beta}=0\)), the above equation implies,

\[N^{\prime}_{a}\ =\ \frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{Nd_{a,a}}+\frac{d_{b,b} }{d_{a,a}}N^{\prime}_{b}.\]

Adding both sides for \(a\in B\), we get,

\[1-\beta\ =\ \sum_{a\in B}N^{\prime}_{a} =\ \sum_{a\in B}\frac{N_{a}d_{a,a}-N_{b}d_{b,b}}{Nd_{a,a}}+d_{b,b}N^{ \prime}_{b}\sum_{a\in B}d^{-1}_{a,a}\] \[=\ \frac{\sum_{a\in B}N_{a}}{N}-\frac{N_{b}}{N}d_{b,b}d^{-1}_{B}+N^{ \prime}_{b}d_{b,b}d^{-1}_{B},\]

which implies

\[N^{\prime}_{b}\ =\ \frac{((1-\beta)N-\sum_{a\in B}N_{a})d_{B}+N_{b}d_{b,b}}{Nd_{b,b}}.\]

**Statement 2:** We know

\[I^{\prime}_{B}\ =\ N^{\prime}_{1}d_{1,b}+N^{\prime}_{b}d_{b,b}\quad\text{for every $b\in B$.}\]

Putting in the derivatives from (36), we obtain,

\[I^{\prime}_{B} =\ \beta d_{1,b}+\left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N} \right)d_{B}+\frac{N_{b}d_{b,b}}{N}\] \[=\ \left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N}\right)d_{B}+\frac{ \beta Nd_{1,b}+N_{b}d_{b,b}}{N}\] \[=\ \left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N}\right)d_{B}+\frac{I _{B}}{N}.\quad\text{(since $N_{1}=\beta N$ and $I_{b}=I_{B}$ for every $b\in B$)}\]

**Statement 3:** For the following argument, the constants hidden in \(O(\cdot),\Omega(\cdot)\) and \(\Omega(\cdot)\) are independent of \(N\).

Note that \(N_{1}=\beta N\). As a result, using (6), \(d_{a,a}=\Theta(1)\) for every \(a\in[K]/\{1\}\). This implies, we can find a constant \(c_{1}>0\) such that \(d_{B}\geq c_{1}\). On the other hand, using (12), we have,

\[I_{B}\;=\;\Theta\left(\frac{N_{1}N_{a}}{N_{1}+N_{a}}\right)\]

for every \(a\in B\). Since \(N_{1}=\beta N\) and \(N_{a}\leq N\), we have

\[I_{B}\;=\;\Theta(N_{a}).\]

Adding for all \(a\in B\), we get \(I_{B}=\Theta(\sum_{a\in B}N_{a})\). Therefore, \(I_{B}\geq c_{2}\sum_{a\in B}N_{a}\), for some constant \(c_{2}>0\).

Now using (37),

\[I^{\prime}_{B} =\;\left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N}\right)d_{B}+\frac{ I_{B}}{N}\] \[\geq\;c_{1}\times\left(1-\beta-\frac{\sum_{a\in B}N_{a}}{N}\right) +c_{2}\times\frac{\sum_{a\in B}N_{a}}{N}\] \[\geq\;\min\{c_{1},c_{2}\}\times(1-\beta).\]

Taking \(c=\min\{c_{1},c_{2}\}>0\), we have the desired result.

Now for arms \(a\in\overline{B}^{c}\), note that \(x_{1,a}=\text{arg}\min_{x\in[\mu_{a},\mu_{1}]}\;\left(N_{1}d(\mu_{1},x)+N_{a}^ {0}d(\mu_{a},x)\right)\) and \(I_{a}=N_{1}d(\mu_{1},x_{1,a})+N_{a}^{0}d(\mu_{a},x_{1,a})\).

As a result, putting \(x=\mu_{1}\), we have \(I_{a}\leq N_{a}^{0}d(\mu_{a},\mu_{1})\). \(\square\)

**Reaching \(\beta\)-optimal proportions:** By statement 3 of Proposition E.3, the indexes of the arms in \(B\) increase at a linear rate, whereas the indexes of the arms in \(\overline{B}^{c}\) stay bounded above by a constant. As a result, by some finite time, \(I_{B}\) crosses the index of some arm in \(a\in\overline{B}^{c}\), after which \(B\) gets updated to \(B\cup\{a\}\). The same process then continues with the updated \(B\). In this way, eventually \(B=[K]/\{1\}\) and the fluid dynamics reaches the \(\beta\)-optimal proportion \(\bm{\omega}^{\star}(\beta)=(\omega_{a}^{\star}(\beta):a\in[K])\) (\(\beta\)-optimal proportion is the solution to the max-min problem (1) with the added constraint \(\omega_{1}=\beta\)) where,

\[\frac{N_{1}}{N}\;=\;\omega_{1}^{\star}(\beta)\;=\;\beta\quad\text{and}\quad \frac{N_{a}}{N}\;=\;\omega_{a}^{\star}(\beta)\quad\text{for every }a\in[K]/\{1\}.\]

Applying the same argument as used to bound the time to reach optimal proportions in Section 4, if the fluid dynamics start at some time \(N^{0}\) with state \(\bm{N}(N^{0})=(N_{a}(N^{0}):a\in[K])\), then it reaches stability by a time atmost \(\frac{N^{0}}{\min_{a\in[K]}\omega_{a}^{\star}(\beta)}\).

## Appendix F Intuitions based on fluid dynamics applied to algorithmic behavior

### Indexes once they meet do not separate

In the fluid dynamics described in Theorem 4.1, once the indexes meet thereafter they move up together by construction. It turns out that \(I^{\prime}_{B}(N)\) is positive. Below we give a heuristic argument that in our fluid dynamics, once a set of smallest indexes that are equal, increase and catch up with another index, their union then remains equal and increases together with \(N\). This argument provides important insights which help us later to prove that after after a random time of finite expectation, if our algorithm picks a suboptimal arm, then it picks that arm again in a periodic manner, which helps us prove closeness of indexes w.r.t. the algorithmic allocations (see Lemma 5.1). Differentiating \(g=0\) with respect to \(N\), we see that,

\[N^{\prime}_{1}\sum_{a\neq 1}f(\bm{\mu},a,\bm{N})\frac{N_{a}\Delta_{a}}{(N_{1}+N _{a})^{2}}=\sum_{a\neq 1}N^{\prime}_{a}f(\bm{\mu},a,\bm{N})\frac{N_{1}\Delta_{a}}{(N _{1}+N_{a})^{2}}.\] (40)Inductively, suppose that a set \(B\) of indexes are moving up together and they run into another index \(b\) at time \(N\). Upon assuming contradiction, we can have a neighbourhood \([N,N+\Delta N]\) where the algorithm only allocates to a subset \(C\subset B\cup\{b\}\) and doesn't allocate to arms in \(D=B\cup\{b\}-C\neq\emptyset\). Then the allocations follow the ODEs in (3) of Theorem 4.1 with \(B=C\), in the interval \([N,N+\Delta N]\).

Consider the probability vector \((p_{a}:a\in[K]/\{1\})\) where,

\[p_{a}=\frac{f(\boldsymbol{\mu},a,\boldsymbol{N})\frac{N_{a} \Delta_{n}}{(N_{1}+N_{a})^{2}}}{\sum_{b\neq 1}f(\boldsymbol{\mu},b, \boldsymbol{N})\frac{N_{b}\Delta_{n}}{(N_{1}+N_{b})^{2}}}.\]

Note that \(p_{a}>0\) for every \(a\in[K]/\{1\}\). We have from (40) that

\[\frac{N_{1}^{\prime}}{N_{1}}\ =\ \sum_{a\in C}\frac{N_{a}^{\prime}}{N_{a}}p_{a}\] (41)

Letting \(b=\operatorname*{arg\,max}_{a\in C}\frac{N_{a}^{\prime}(N)}{N_{a}(N)}\) (where \(N_{a}^{\prime}(N)\) is the derivative in (3) of Theorem 4.1, upon putting \(B=C\)), we have

\[\frac{N_{1}^{\prime}}{N_{1}}\leq\left(\sum_{a\in C}p_{a}\right) \frac{N_{b}^{\prime}}{N_{b}}\stackrel{{(1)}}{{<}}\frac{N_{b}^{ \prime}}{N_{b}},\] (42)

where the strict inequality in (1) follows from the fact that \(D=B\cup\{b\}-C\neq\emptyset\), causing \(\sum_{a\in C}p_{a}<1\).

We now argue that \(D\) must be empty. Suppose instead that \(D\neq\emptyset\) and \(a\in D\). Because all indexes in \(B\) are equal at time \(N\), we have, \(N_{1}d(\mu_{1},x_{1,a})+N_{a}d(\mu_{a},x_{1,a})=N_{1}d(\mu_{1},x_{1,b})+N_{b}d (\mu_{b},x_{1,b})\) at N. Observe that for any arm \(d\in[K]/\{1\}\), derivative of its index with respect to \(N\) equals \(N_{1}^{\prime}d(\mu_{1},x_{1,d})+N_{d}^{\prime}d(\mu_{d},x_{1,d})\) (since, by definition of \(x_{1,d}\), \(N_{1}d_{2}(\mu_{1},x_{1,d})+N_{d}d_{2}(\mu_{d},x_{1,d})=0\)). Since arm \(a\) gets no sample in \([N,N+\Delta N]\), we have \(N_{a}^{\prime}=0\), which implies

\[I_{a}^{\prime}=N_{1}^{\prime}d(\mu_{1},x_{1,a})\text{ in }[N,N+\Delta N].\]

By our previous discussion

\[I_{b}^{\prime}\ =\ N_{1}^{\prime}d(\mu_{1},x_{1,b})+N_{b}^{\prime}d(\mu_{b},x_ {1,b}).\] (43)

We now argue that \(N_{1}^{\prime}d(\mu_{1},x_{1,a})\) is strictly less than (43) at \(N\). As a result, if \(\Delta N>0\) is picked sufficiently small, index of \(b\), which is the minimum index, outruns index of \(a\) in \([N,N+\Delta N]\).

Consider the difference

\[N_{1}^{\prime}d(\mu_{1},x_{1,a})-N_{1}^{\prime}d(\mu_{1},x_{1,b})-N_{b}^{ \prime}d(\mu_{b},x_{1,b})\ =\ N_{1}^{\prime}((\mu_{1},x_{1,a})-d(\mu_{1},x_{1,b}))-N_{b}^{\prime}d(\mu_{ b},x_{1,b}).\] (44)

We want show that this is strictly negative. We consider two cases,

**Case I:** If \(d(\mu_{1},x_{1,a})-d(\mu_{1},x_{1,b})\leq 0\), it follows trivially.

**Case II:** If \(d(\mu_{1},x_{1,a})-d(\mu_{1},x_{1,b})>0\), since \(N_{b}^{\prime}>0\), using (42) we can upper bound (44) by

\[N_{b}^{\prime}\cdot\left(\frac{N_{1}}{N_{b}}\left(d(\mu_{1},x_{1,a})-d(\mu_{1 },x_{1,b})\right)-d(\mu_{b},x_{1,b})\right).\] (45)

Since the two indexes are equal at this point, we have

\[N_{1}(d(\mu_{1},x_{1,a})-d(\mu_{1},x_{1,b}))=N_{b}d(\mu_{b},x_{1,b})-N_{a}d( \mu_{a},x_{1,a}).\]

Substituting this in (45), the latter equals

\[N_{b}^{\prime}\cdot\left(\frac{1}{N_{b}}(N_{b}d(\mu_{b},x_{1,b})-N_{a}d(\mu_{ a},x_{1,a}))-d(\mu_{b},x_{1,b})\right)\leq-N_{b}^{\prime}\cdot\frac{N_{a}}{N_{b}}d( \mu_{a},x_{1,a})<0.\]

We thus have our contradiction. Therefore, indexes of the arms in \(B\cup\{b\}\) move together.

[MISSING_PAGE_EMPTY:36]

Using the above inequality in (49), we have,

\[\mathcal{I}_{a}(N+t)-\mathcal{I}_{b_{t}}(N+t) \leq-\Delta\widetilde{N}_{b_{t}}(N,t)\cdot\frac{\widetilde{N}_{a}(N )}{\widetilde{N}_{b_{t}}(N)}d(\widetilde{\mu}_{a},\widetilde{x}_{1,a})+O(N^{1- \frac{3\alpha}{8}})\] \[\quad+O(\Delta\widetilde{N}_{b_{t}}(N,t)\cdot N^{-\frac{3\alpha}{ 8}}),\] \[\leq-\Delta\widetilde{N}_{b_{t}}(N,t)\cdot\frac{\widetilde{N}_{a}( N)}{\widetilde{N}_{b_{t}}(N)}d(\widetilde{\mu}_{a},\widetilde{x}_{1,a})\] \[\quad+O(N^{1-\frac{3\alpha}{8}})\quad\text{(since $\Delta \widetilde{N}_{b_{t}}(N,t)\leq R=O(N)$)},\] (50)

whenever \(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{ x}_{1,b_{t}})\geq 0\).

Since \(\widetilde{N}_{j}(N)=\Theta(N)\) and \(\widetilde{\mu}_{j}(N)\approx\mu_{j}\) for all \(j\in[K]\) and \(N\geq T_{good}\), the coefficient of \(\Delta\widetilde{N}_{b_{t}}(N,t)\) in (49) and (50) are \(-\Theta(1)\). As a result, we can find a constant \(C_{3}>0\), such that, for \(t\leq R\) and \(N\geq T_{good}\),

\[\mathcal{I}_{a}(N+t)-\mathcal{I}_{b_{t}}(N+t)\;\leq\;-C_{3}\Delta\widetilde{N }_{b}(N,t)+O(N^{1-3\alpha/8}).\] (51)

Applying the mean value theorem and using the fact that \(t\) can be atmost \(O(N)\), we can prove that, (51) implies,

\[\mathcal{I}_{a}(N+t-1)-\mathcal{I}_{b_{t}}(N+t-1)\;\leq\;-C_{3}\Delta \widetilde{N}_{b}(N,t)+O(N^{1-3\alpha/8}).\]

As a result, we have a constant \(C_{4}>0\) such that,

\[\mathcal{I}_{a}(N+t-1)-\mathcal{I}_{b_{t}}(N+t-1)\;\leq\;-C_{3}\Delta \widetilde{N}_{b}(N,t)+C_{4}N^{1-3\alpha/8}.\] (52)

Using (47), we can choose \(T_{good}\) suitably, and find constants \(D_{1},\;D_{2}>0\), such that, whenever \(N\geq T_{good}\),

\[R\geq D_{1}N^{1-3\alpha/8}\quad\implies\quad\Delta\widetilde{N}_{b_{R}}(N,R) \geq D_{2}R\quad\text{(see Lemma G.15 of Appendix G.6).}\]

We consider the case where \(R\geq\max\left\{D_{1},\frac{2C_{4}}{C_{3}D_{2}}\right\}\times N^{1-3\alpha/8}\).

Since \(R\geq D_{1}N^{1-3\alpha/8}\), we have

\[\Delta\widetilde{N}_{b_{R}}(N,R)\geq D_{2}R\geq D_{2}\times\frac{2C_{4}}{C_{3 }D_{2}}N^{1-3\alpha/8}=\frac{2C_{4}}{C_{3}}N^{1-3\alpha/8}.\]

For notational simplicity, we use \(b\) to denote \(b_{R}\). We consider the iteration \(N+S\) where arm \(b\) was selected for the last time before iteration \(N+R\). Then by definition of \(b_{R}\) and \(S\), and using the above inequality, we have

\[\Delta\widetilde{N}_{b}(N,S)\;=\;\Delta\widetilde{N}_{b}(N,R)\;\geq\;\frac{2C _{4}}{C_{3}}N^{1-3\alpha/8}.\] (53)

Also, since \(\Delta\widetilde{N}_{b}(N,S)=\Delta\widetilde{N}_{b}(N,R)\) and for every \(j\neq 1\)\(\Delta\widetilde{N}_{j}(N,R)\geq\Delta\widetilde{N}_{j}(N,S)\), we conclude \(b=b_{S}\). Therefore,

\[\mathcal{I}_{a}(N+S-1)-\mathcal{I}_{b}(N+S-1) =\;\mathcal{I}_{a}(N+S-1)-\mathcal{I}_{b_{S}}(N+S-1)\] \[\text{(using \eqref{eq:2})} \leq\;-C_{3}\Delta\widetilde{N}_{b_{S}}(N,S)+C_{4}N^{1-3\alpha/8}\] \[\text{(since $b=b_{S}$)} =\;-C_{3}\Delta\widetilde{N}_{b}(N,S)+C_{4}N^{1-3\alpha/8}\] \[\text{(using \eqref{eq:2})} \leq\;-C_{3}\times\frac{2C_{4}}{C_{3}}N^{1-3\alpha/8}+C_{4}N^{1-3 \alpha/8}\] \[=-C_{4}N^{1-3\alpha/8}.\]

The above inequality implies, the AT2 algorithm pulls arm \(b\) at iteration \(N+S\), even though

\[\mathcal{I}_{a}(N+S-1)\leq\mathcal{I}_{b}(N+S-1)-C_{4}N^{1-3\alpha/8},\]

which is contradicting the algorithm's description. Hence we must have

\[R\leq\max\left\{D_{1},\frac{2C_{4}}{C_{3}D_{2}}\right\}\times N^{1-3\alpha/8}=O (N^{1-3\alpha/8}).\]Algorithmic allocations: non fluid behaviour

In the following sections, unless otherwise stated, the proof of the mentioned results for AT2 (1) and IAT2 (2) algorithms follow a similar argument. Also, the constants introduced while stating the results in the following sections might be different for the two algorithms.

While using the \(O(\cdot),\Theta(\cdot)\) and \(\Omega(\cdot)\) notations, we imply that the hidden constants can depend on the choice of algorithm among AT2 and IAT2, instance \(\boldsymbol{\mu}\), exploration factor \(\alpha\in(0,1)\) and no. of arms \(K\), and are independent of the sample path.

### Convergence of algorithmic allocations to the optimality conditions

In this section, our agenda is to prove the convergence of the allocations of AT2 and IAT2 algorithms to the optimality conditions mentioned in Proposition 2.2. For ease of presentation we state the conditions uniquely characterizing the optimal proportion \(\boldsymbol{\omega}^{\star}\) below according to Proposition 2.2:

\[\sum_{a\neq 1}\frac{d(\mu_{1},x_{1,a}^{\star})}{d(\mu_{a},x_{1,a}^{ \star})}\ =\ 1\quad\text{and}\quad\forall a\neq 1,\quad\omega_{1}^{\star}d(\mu_{1},x_{1,a}^{\star})+\omega_{a}d(\mu_{a},x_{1,a}^{\star})\ =\ I^{\star}\ =\ T^{\star}(\boldsymbol{\mu})^{-1}\] (54)

\[\text{where}\quad x_{1,a}^{\star}\ =\ \frac{\omega_{1}^{\star}\mu_{1}+\omega_{a}^{ \star}\mu_{a}}{\omega_{1}^{\star}+\omega_{a}^{\star}},\quad\text{and}\quad \sum_{a\in[K]}\omega_{a}^{\star}=1.\]

Recall that for every \(a\in[K]\) and iteration \(N\), \(\widetilde{\omega}_{a}(N)=\frac{\widetilde{N}_{a}(N)}{N}\) denotes the proportion of samples allocated by the algorithm to arm \(a\). Let \(\widetilde{\boldsymbol{\omega}}(N)=(\widetilde{\omega}_{a}(N):a\in[K])\).

Recall the anchor function \(g(\boldsymbol{\mu},\widetilde{\boldsymbol{N}}(\cdot))\) and index \(I_{a}(\cdot)\) for every alternative arm \(a\in[K]/\{1\}\). In Section 5, we defined the normalized index \(H_{a}(\cdot)\) of every arm \(a\in[K]/\{1\}\) at iteration \(N\) as \(H_{a}(N)=\frac{1}{N}I_{a}(N)\). In the next two sections, we prove,

\[|g(\boldsymbol{\mu},\widetilde{\boldsymbol{\omega}}(N))|\ =\ \left|\ \sum_{a\in[K]}\frac{d(\mu_{1},x_{1,a}(N))}{d(\mu_{a},x_{1,a}(N)}-1\ \right|\ \longrightarrow\ 0,\] (55)

and

\[\max_{a,b\in[K]/\{1\}}|H_{a}(N)-H_{b}(N)|\ \longrightarrow\ 0\] (56)

a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\) as \(N\to\infty\). Moreover, we show that, after a random time of finite expectation, both the convergences in (55) and (56) happen at a uniform rate over all sample paths. We prove these convergence results in Proposition G.1 and G.2 stated below.

**Proposition G.1** (**Convergence of \(g\) to zero)**.: _There exists constants \(M_{4}\geq 1\) and \(C>0\) independent of the sample paths, such that, if \(T_{6}\) is defined as the iteration at which \(g(\widetilde{\boldsymbol{\mu}}(\cdot),\widetilde{\boldsymbol{N}}(\cdot))\) crosses the value zero after iteration \(\max\{M_{4},T_{5}\}\) (\(T_{5}\) is a random time satisfying \(\mathbb{E}_{\boldsymbol{\mu}}[T_{5}]<\infty\) and defined in Definition G.1 of Appendix G.1.1), then for \(N\geq T_{6}\) we have,_

\[\left|g(\boldsymbol{\mu},\widetilde{\boldsymbol{N}}(N))\right|\ \leq\ CN^{-3\alpha/8}.\]

_Moreover, the random time \(T_{6}\) satisfies \(\mathbb{E}_{\boldsymbol{\mu}}[T_{6}]<\infty\)._

**Proposition G.2** (**Closeness of the indexes**).: _There exists a random time \(T_{8}\) (defined in Definition G.4 of Appendix G.1.2) satisfying \(\mathbb{E}_{\boldsymbol{\mu}}[T_{8}]<\infty\), such that, for \(N\geq T_{8}\), every pair of alternative arms \(a,b\in[K]/\{1\}\) has,_

\[|I_{a}(N)-I_{b}(N)|\ =\ O(N^{1-3\alpha/8}).\]

**Proof of Proposition 5.1:** By the definition of \(T_{stable}\) in Definition G.5 of Appendix G.2, we have \(T_{stable}\geq T_{6},\ T_{8}\), where \(T_{6}\) and \(T_{8}\) are the random times mentioned, respectively, in Proposition G.1and G.2. As a result, Proposition 5.1 follows trivially from Proposition G.1 and G.2. 

Proof of Proposition G.1 is in Appendix G.1.1. We prove a detailed version of Proposition G.2 as Proposition G.3 in Appendix G.1.2. Both these results are crucial later for proving the convergence of the algorithmic proportions \(\widetilde{\bm{\omega}}(N)=(\widetilde{\omega}_{a}(N):a\in[K])\) to the optimal proportions \(\bm{\omega}^{*}=(\omega_{a}^{*}:a\in[K])\) in Proposition 3.1 from Section 3. We prove a detailed version of Proposition 3.1 as Proposition G.4 in Appendix G.2.

To prove Proposition G.1, G.2, and later Proposition G.4, we need to prove several technical properties related to exploration and the allocations made by the algorithms. The detailed technical results related to exploration are in Appendix G.4 and those related to the algorithmic allocations are in Appendix G.5. The arguments in Appendix G.1.1, G.1.2, and G.2 are self-contained, and we refer the reader to the related technical results whenever necessary. For ease of exposition, we provide below a brief summary of the statements proven in Appendix G.4 and G.5.

#### Summary of technical results in Appendix G.4 and G.5

We summarize below the results proven in Appendix G.4 and G.5 as events happening between the non-decreasing sequence of random times \(T_{0},\ T_{1},\ T_{2},\ T_{3}\), and \(T_{4}\), which are defined in Appendix G.5.

1. \(T_{0}\stackrel{{\text{def.}}}{{=}}\min\{N^{\prime}\geq 1\mid \forall N\geq N^{\prime},\ \max_{a\in[K]}|\widetilde{\mu}_{a}(N)-\mu_{a}|\leq\epsilon(\bm{\mu})N^{-3 \alpha/8}\}\), where \(\epsilon(\bm{\mu})>0\) (defined in Appendix B), is a constant depending on the instance \(\bm{\mu}\). By definition, we have \(\epsilon(\bm{\mu})\leq\frac{1}{4}\min_{a\neq 1}(\mu_{1}-\mu_{a})\). As a result, the first arm becomes the empirically best arm and stays that way forever after iteration \(T_{0}\). In Lemma G.7 of Appendix G.3, we prove that \(\mathbb{E}_{\bm{\mu}}[T_{0}]<\infty\), which implies \(T_{0}<\infty\) a.s. in \(\mathbb{P}_{\bm{\mu}}\).
2. \(T_{1}\stackrel{{\text{def.}}}{{=}}\max\{T_{\text{explo}},T_{0}\}\), where \(T_{\text{explo}}<\infty\) is a constant defined in Definition G.7 of Appendix G.4. After iteration \(T_{\text{explo}}\), the algorithm consecutively does exploration over a strecth of atmost \(K\) iterations. Moreover, over a single such "_epoch_" of consecutive explorations, the algorithm explores every arm atmost once (follows from statement 1 and 3 of Proposition G.5). Note that \(\mathbb{E}_{\bm{\mu}}[T_{1}]\leq T_{\text{explo}}+\mathbb{E}_{\bm{\mu}}[T_{0}]<\infty\).
3. \(T_{2}\) is defined in Lemma G.11 as the iteration at which the anchor function \(g(\widetilde{\bm{\mu}}(\cdot),\widetilde{\bm{N}}(\cdot))\) crosses the value zero after the iteration \(\max\{M_{1},T_{1}\}\) (\(M_{1}\geq 1\) is a constant independent of the sample paths and defined in the proof of Lemma G.11). By Lemma G.9, there exists a constant \(C_{1}\geq 1\) independent of the sample paths, such that \(T_{2}\leq C_{1}\max\{M_{1},T_{1}\}\). As a result, \(\mathbb{E}_{\bm{\mu}}[T_{2}]\leq C_{1}(M_{1}+\mathbb{E}_{\bm{\mu}}[T_{1}])<\infty\). After iteration \(T_{2}\), the empirical anchor function \(g(\widetilde{\bm{\mu}}(\cdot),\widetilde{\bm{N}}(\cdot))\) remains bounded inside an interval of the form \([-(1-d_{\min}),d_{\max}-1]\), where \(d_{\min}\in(0,1)\) and \(d_{\max}\in(1,\infty)\) are constants independent of the sample paths (see Lemma G.11). Exploiting this, we argue that both \(\widetilde{N}_{1}(N)\) and \(\max_{a\in[K]/\{1\}}\widetilde{N}_{a}(N)\) become \(\Omega(N)\) after iteration \(T_{2}\) (see Corollary G.1).
4. \(T_{3}\stackrel{{\text{def.}}}{{=}}\max\{M_{2},T_{2}\}+2\), where \(M_{2}\geq 1\) is a constant chosen in the proof of Lemma G.12 and is independent of the sample paths. After iteration \(T_{3}\), whenever the algorithm picks an alternative arm \(a\in[K]/\{1\}\), then for every other alternative arm \(b\in[K]/\{1,a\}\), we have \(\widetilde{N}_{b}(N)\geq\gamma\widetilde{N}_{a}(N)\), for some constant \(\gamma\in(0,1)\) independent of the sample paths (see Lemma G.12). Note that \(\mathbb{E}_{\bm{\mu}}[T_{3}]\leq M_{2}+2+\mathbb{E}_{\bm{\mu}}[T_{2}]<\infty\).
5. \(T_{4}=C_{2}(T_{3}+1)\) for some constant \(C_{2}\geq 1\) independent of the sample paths, defined in Lemma G.13. After iteration \(T_{4}\), all the arms \(a\in[K]\) have \(\widetilde{N}_{a}(N)=\varTheta(N)\) (see Lemma G.13). Note that \(\mathbb{E}_{\bm{\mu}}[T_{4}]\leq C_{2}(\mathbb{E}_{\bm{\mu}}[T_{3}]+1)<\infty\).

#### g.1.1 Convergence of the anchor function to zero

The following lemma bounds the fluctuation of \(g(\widetilde{\bm{\mu}},\widetilde{\bm{N}})\) around \(g(\bm{\mu},\widetilde{\bm{N}})\) due to the noise in the estimate \(\widetilde{\bm{\mu}}\) of \(\bm{\mu}\). We need this lemma later for proving convergence of the anchor function \(g\) to zero in Proposition G.1.

**Lemma G.1** (Bounding the noise in \(g\)).: _For every \(N\geq T_{2}\) (where \(T_{2}\) is the random time defined in Lemma G.11 and satisfies \(\mathbb{E}_{\bm{\mu}}[T_{2}]<\infty\)), we have,_

\[|g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))-g(\bm{\mu},\widetilde{\bm{N} }(N))|\ =\ O(N^{-3\alpha/8}).\]

Proof.: Using mean value theorem for function of several variables, we have,

\[|g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))-g(\bm{\mu},\widetilde{\bm{N} }(N))|\ \leq\ \sum_{a=1}^{K}\left|\frac{\partial g}{\partial\mu_{a}}(\hat{\bm{\mu}}, \widetilde{\bm{N}}(N))\right|\cdot|\widetilde{\mu}_{a}(N)-\mu_{a}|,\]

where \(\hat{\mu}_{a}\) lies between \(\mu_{a}\) and \(\widetilde{\mu}_{a}(N)\) for every \(a\in[K]\).

We define

\[\hat{x}_{1,a}=\frac{\widetilde{N}_{1}(N)\hat{\mu}_{1}+\widetilde{N}_{a}(N) \hat{\mu}_{a}}{\widetilde{N}_{1}(N)+\widetilde{N}_{a}(N)},\quad\text{for every }a\in[K]/\{1\}.\]

Note that,

\[\frac{\partial g}{\partial\mu_{1}}(\hat{\bm{\mu}},\widetilde{\bm {N}}(N)) =\ \sum_{a\neq 1}\left(\frac{d_{1}(\hat{\mu}_{1},\hat{x}_{1,a})}{d( \hat{\mu}_{a},\hat{x}_{1,a})}-f(\hat{\bm{\mu}},a,\hat{\bm{N}})\cdot\frac{ \widetilde{N}_{1}}{\widetilde{N}_{1}+\widetilde{N}_{a}}\right),\quad\text{and},\] \[\forall a\neq 1,\quad\frac{\partial g}{\partial\mu_{a}}(\hat{\bm{\mu}}, \widetilde{\bm{N}}(N)) =\ -\ \frac{d(\hat{\mu}_{1},\hat{x}_{1,a})d_{1}(\hat{\mu}_{a},\hat{x}_{1,a })}{(d(\hat{\mu}_{a},\hat{x}_{1,a}))^{2}}-f(\hat{\bm{\mu}},a,\hat{\bm{N}}) \cdot\frac{\widetilde{N}_{a}}{\widetilde{N}_{1}+\widetilde{N}_{a}},\] (57) \[\text{where}\quad f(\hat{\bm{\mu}},a,\hat{\bm{N}}) =\ -\ \frac{d_{2}(\hat{\mu}_{1},\hat{x}_{1,a})}{d(\hat{\mu}_{a},\hat{x}_{1,a })}+\frac{d(\hat{\mu}_{1},\hat{x}_{1,a})d_{2}(\hat{\mu}_{a},\hat{x}_{1,a})}{ (d(\hat{\mu}_{a},\hat{x}_{1,a}))^{2}},\]

and recall that \(d_{1}(\cdot,\cdot)\) and \(d_{2}(\cdot,\cdot)\), respectively, denote the partial derivatives of \(d(\cdot,\cdot)\) with respect to its first and second argument.

By (6), for \(N>T_{2}\), we have,

\[d(\hat{\mu}_{a},\hat{x}_{1,a})=\Theta\left((\hat{x}_{1,a}-\hat{\mu}_{a})^{2} \right)\ =\ \Theta\left(\frac{\widetilde{N}_{1}(N)^{2}}{(\widetilde{N}_{1}(N)+\widetilde{N }_{a}(N))^{2}}\right).\]

By Corollary G.1 from Appendix G.5, we have \(\widetilde{N}_{1}(N)=\Omega(N)\) for \(N>T_{2}\). As a result, \(d(\hat{\mu}_{a},\hat{x}_{1,a})=\Theta(1)\) for \(N>T_{2}\).

Moreover, for \(N>T_{2}\), we have: \(|d_{1}(\hat{\mu}_{1},\hat{x}_{1,a})|=O(1),\ \ |d_{1}(\hat{\mu}_{a},\hat{x}_{1,a})|=O(1)\) (using (7)) ; \(|d_{2}(\hat{\mu}_{1},\hat{x}_{1,a})|=O(1),\ |d_{2}(\hat{\mu}_{a},\hat{x}_{1,a})|=O(1)\) (using (8)) ; and \(d(\hat{\mu}_{1},\hat{x}_{1,a})=O(1)\) (using (6)). As a result, for \(N>T_{2}\), all the partial derivatives in (57) are \(O(1)\). Therefore, for \(N>T_{2}\),

\[|g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))-g(\bm{\mu},\widetilde{\bm{N} }(N))|\ =\ O\left(\sum_{a\in[K]}|\widetilde{\mu}_{a}(N)-\widetilde{\mu}_{a}|\right)\ =\ O(N^{-3\alpha/8}),\] (58)

and hence completing the proof. 

**Halting of exploration:** By Lemma G.13, for \(N\geq T_{4}\), every arm \(a\in[K]\) has \(\widetilde{N}_{a}(N)=\Theta(N)\). As a result, we can find a constant \(\lambda\in(0,1)\) such that \(\widetilde{N}_{a}(N)\geq\lambda N\) for every \(a\in[K]\) and \(N\geq T_{4}\). We choose \(M_{3}\) large enough such that, for every \(N\geq M_{3}\), \(\lambda(N-1)>N^{\alpha}\). Then we have \(\min_{a\in[K]/\{1\}}\widetilde{N}_{a}(N-1)>N^{\alpha}\) for every \(N\geq\max\{M_{3},T_{4}+1\}\). As a result, the algorithm doesn't do any exploration after iteration \(\max\{M_{3},T_{4}+1\}\). With this, we define the following random time,

**Definition G.1**.: _We define \(T_{5}=\max\{M_{3},T_{4}+1\}\)._

Note that \(\mathbb{E}_{\bm{\mu}}[T_{5}]<\infty\), since, \(\mathbb{E}_{\bm{\mu}}[T_{4}]<\infty\).

We restate Proposition G.1 below,

[MISSING_PAGE_FAIL:41]

Let \(a\in[K]\) be the arm pulled at iteration \(T_{6}\). Applying the mean value theorem we can find \(\hat{N}_{a}\) between \(\widetilde{N}_{a}(T_{6}-1)\) and \(\widetilde{N}_{a}(T_{6})\), can take \(\hat{N}_{b}=\widetilde{N}_{b}(T_{6})\) for all \(b\neq a\), and define the tuple \(\hat{\bm{N}}=(\hat{N}_{b})_{b\in[K]}\), such that, (62) is bounded by,

\[\bigg{|}\;\frac{\partial g}{\partial N_{a}}(\bm{\mu},\hat{\bm{N}})\;\bigg{|}+3 C_{1}T_{6}^{-3\alpha/8}.\]

Using (60) and the above upper bound, we have,

\[\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(T_{6}))\Big{|} \leq\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(T_{6}))-g(\widetilde{\bm {\mu}}(T_{6}),\widetilde{\bm{N}}(T_{6}))\Big{|}+\Big{|}g(\widetilde{\bm{\mu}}( T_{6}),\widetilde{\bm{N}}(T_{6}))\Big{|}\] \[\leq\;C_{1}T_{6}^{-3\alpha/8}+\bigg{|}\;\frac{\partial g}{ \partial N_{a}}(\bm{\mu},\hat{\bm{N}})\;\bigg{|}+3C_{1}T_{6}^{-3\alpha/8}\] \[\leq\;4C_{1}T_{6}^{-3\alpha/8}+C_{2}^{\prime}T_{6}^{-1}\;\;\;( \text{using (\ref{eq:1})})\] \[\leq\;(4C_{1}+C_{2}^{\prime})T_{6}^{-3\alpha/8}\;=\;CT_{6}^{-3 \alpha/8}.\]

**Induction:** Note that at a given iteration \(N\) the algorithm can only see \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\). By (60), for \(N\geq T_{6}\), \(g(\widetilde{\bm{\mu}},\widetilde{\bm{N}})\) and \(g(\bm{\mu},\widetilde{\bm{N}})\) may have different signs only when \(\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(N))\Big{|}\leq C_{1}N^{-3\alpha/8}\). Based on this, we consider two cases.

**Case I:**\(|g(\bm{\mu},\widetilde{\bm{N}}(N))|\leq C_{1}N^{-3\alpha/8}\). We assume \(a\in[K]\) to be the arm pulled in iteration \(N+1\). Using the mean value theorem, we can find \(\hat{N}_{a}\in\Big{[}\widetilde{N}_{a}(N),\widetilde{N}_{a}(N+1)\Big{]}\), can take \(\hat{N}_{b}=\widetilde{N}_{b}(N)\) for all \(b\neq a\), and define the tuple \(\hat{\bm{N}}=(\hat{N}_{b})_{b\in[K]}\), such that,

\[\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(N+1))\Big{|} \leq\;\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(N))\Big{|}+\bigg{|} \frac{\partial g}{\partial N_{a}}(\bm{\mu},\hat{\bm{N}})\bigg{|}\] \[\overset{(1)}{\leq} C_{1}N^{-3\alpha/8}+C_{2}^{\prime}N^{-1}\;\leq\;(C_{1}+C_{2}^{ \prime})N^{-3\alpha/8},\]

where (1) follows from (61).

Note that \(N\geq T_{6}\geq M_{4}\geq M_{42}\). By the definition of \(M_{42}\), we have

\[\Big{|}g(\bm{\mu},\widetilde{\bm{N}}(N+1))\Big{|}\;\leq\;(C_{1}+C_{2}^{\prime })N^{-3\alpha/8}\leq(4C_{1}+C_{2}^{\prime})(N+1)^{-3\alpha/8}=C(N+1)^{-3\alpha /8},\]

for every \(N\geq T_{6}\).

**Case II:**\(|g(\bm{\mu},\widetilde{\bm{N}}(N))|>C_{1}N^{-3\alpha/8}\): In this case \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\) and \(g(\bm{\mu},\widetilde{\bm{N}}(N))\) have the same sign. Let arm \(a\) has been sampled from in iteration \(N+1\). Using the mean value theorem, we have \(\hat{N}_{a}\in\Big{[}\widetilde{N}_{a}(N),\widetilde{N}_{a}(N+1)\Big{]}\), can take \(\hat{N}_{b}=\widetilde{N}_{b}(N)\) for all \(b\neq a\), and define the tuple \(\hat{\bm{N}}=(\hat{N}_{b})_{b\in[K]}\), such that,

\[g(\bm{\mu},\widetilde{\bm{N}}(N+1))=g(\bm{\mu},\widetilde{\bm{N}}(N))+\frac{ \partial g}{\partial N_{a}}(\bm{\mu},\hat{\bm{N}}).\] (63)

We first consider the case when \(g(\bm{\mu},\widetilde{\bm{N}}(N))>0\). After the algorithm sees \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))>0\), it pulls the first arm. As a result, by (61) and (63), \(g(\bm{\mu},\widetilde{\bm{N}}(\cdot))\) decreases in iteration \(N+1\) atmost by \(C_{2}^{\prime}N^{-1}\) and atleast by \(C_{2}N^{-1}\). Now there can be two possibilities:

1. If \(g(\bm{\mu},\widetilde{\bm{N}}(N+1))<0\), we must have \(g(\bm{\mu},\widetilde{\bm{N}}(N+1))\geq-C_{2}^{\prime}N^{-1}\). Since \(N\geq T_{6}\geq M_{4}\geq M_{43}\), we have \(C(N+1)^{-3\alpha/8}\geq C_{2}^{\prime}N^{-1}\) by the definition of \(M_{43}\). As a result, \[g(\bm{\mu},\widetilde{\bm{N}}(N+1))\;\geq\;-\;C_{2}^{\prime}N^{-1}\;\geq\;- \;C(N+1)^{-3\alpha/8}.\]2. If \(g(\bm{\mu},\widetilde{\bm{N}}(N+1))\geq 0\), then \(g(\bm{\mu},\widetilde{\bm{N}}(\cdot))\) has moved towards zero by atleast \(C_{2}N^{-1}\). Whereas, by iteration \(N+1\), the interval \(\big{[}\;-CN^{-3\alpha/8},\;CN^{3\alpha/8}\;\big{]}\) has reduced from both ends by \[CN^{-3\alpha/8}-C(N+1)^{-3\alpha/8}\;\leq\;\frac{3C\alpha}{8}N^{-(1+\frac{3 \alpha}{8})}.\] Since \(N\geq T_{6}\geq M_{4}\geq M_{44}\), by the definition of \(M_{44}\), we have \(\frac{3C\alpha}{8}(N+1)^{-(1+\frac{3\alpha}{8})}<C_{2}N^{-1}\) for every \(N\geq T_{6}\). As a result, we can ensure \(g(\bm{\mu},\widetilde{\bm{N}}(N+1))\leq C(N+1)^{-3\alpha/8}\) at iteration \(N+1\).

In the other case, when \(g(\bm{\mu},\widetilde{\bm{N}}(N))<0\), the algorithm sees \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))<0\), and hence pulls some arm \(a\in[K]/\{1\}\). As a result, by (61) and (63), \(g(\bm{\mu},\widetilde{\bm{N}}(\cdot))\) increases in iteration \(N+1\) atmost by \(C_{2}^{\prime}N^{-1}\) and atleast by \(C_{2}N^{-1}\). Then we apply the same argument as for the case \(g(\bm{\mu},\widetilde{\bm{N}}(N))>0\), but by reversing the signs. Therefore, the inductive statement holds true for this case as well. Hence (59) stands proved.

\(T_{6}\) **has finite expectation:** By Lemma G.9, we can have a constant \(C_{3}>0\), such that \(T_{6}\leq C_{3}\max\{M_{4},T_{5}\}\). As a result, since \(\mathbb{E}_{\bm{\mu}}[T_{5}]<\infty\), we have \(\mathbb{E}_{\bm{\mu}}[T_{6}]\leq C_{1}(M_{4}+\mathbb{E}_{\bm{\mu}}[T_{5}])<\infty\). 

#### g.1.2 Closeness of the indexes

Lemma G.2 is a detailed version of Lemma 5.1 mentioned in Section 5, and is essential for proving closeness of the indexes under the allocations made by AT2 and IAT2 algorithms. Recall that \(T_{6}\) is the random time defined in Proposition G.1 and satisfies \(\mathbb{E}_{\bm{\mu}}[T_{6}]<\infty\).

**Lemma G.2**.: _For both AT2 and IAT2 algorithms, there exists constants \(M_{5}\geq 1\) and \(C_{1}>0\) independent of the sample paths, such that, for every \(N\geq\max\{M_{5},T_{6}\}\), if the algorithm picks an arm \(a\in[K]/\{1\}\) at iteration \(N\), then it again picks arm \(a\) within the next \(\lceil C_{1}N^{1-3\alpha/8}\rceil\) iterations._

Proof of Lemma G.2 is in Appendix G.6, and requires proving several technical lemmas. Some of those supporting lemmas involve arguments similar to the ones used for proving closeness of the indexes while the algorithm operates under an idealized fluid model (discussed in Section 4). In the rest of this section, we use Lemma G.2 to prove closeness of indexes for alternative arms in Proposition G.2.

**Definition G.2**.: _We define the random time \(T_{7}=\max\{M_{5},T_{6}\}\)._

Note that \(\mathbb{E}_{\bm{\mu}}[T_{7}]<\infty\), since \(\mathbb{E}_{\bm{\mu}}[T_{6}]<\infty\).

**Definition G.3**.: _For every \(M\geq 1\), define \(T_{7,M}=\max\{M,T_{7}\}\), and \(T_{8,M}\) as the smallest iteration after \(T_{7,M}\) by which all the alternative arms in \([K]/\{1\}\) have been picked atleast once by the algorithm._

Below we state a detailed version of Proposition G.2.

**Proposition G.3**.: _For every \(M\geq 1\), we have \(\mathbb{E}_{\bm{\mu}}[T_{7,M}]<\infty\) and \(\mathbb{E}_{\bm{\mu}}[T_{8,M}]<\infty\). Moreover, for every \(M\geq 1\) and \(N\geq T_{8,M}\), every pair of arms \(a,b\in[K]/\{1\}\) satisfy,_

\[|I_{a}(N)-I_{b}(N)|\;=\;O(N^{1-3\alpha/8}),\]

_where the constant hidden in \(O(\cdot)\) is independent of \(M\) and the sample path after \(T_{7}\)._

**Definition G.4**.: _We define \(T_{8}=T_{8,1}\), where \(T_{8,1}\) is defined according to Proposition G.3._

By the defintion of \(T_{8}\) above, Proposition G.2 follows trivially from Proposition G.3.

The following lemma helps us to bound the deviation of the empirical index \(\mathcal{I}_{a}(N)\) from the index \(I_{a}(N)\) due to the noise in the estimates \(\widetilde{\bm{\mu}}\), for every alternative arm \(a\in[K]/\{1\}\).

**Lemma G.3**.: _For \(a\in[K]/\{1\}\) and \(N\geq T_{0}\), we have,_

\[|\mathcal{I}_{a}(N)-I_{a}(N)|\;=\;O(N^{1-3\alpha/8}).\]

Proof.: Proof of this lemma uses mean value theorem. For any arm \(a\in[K]/\{1\}\), upon expanding the indexes,

\[|\mathcal{I}_{a}(N)-I_{a}(N)|\;\leq\;\widetilde{N}_{1}\cdot|d(\widetilde{\mu} _{1},\widetilde{x}_{1,a})-d(\mu_{1},x_{1,a})|+\widetilde{N}_{a}\cdot|d( \widetilde{\mu}_{a},\widetilde{x}_{1,a})-d(\mu_{a},x_{1,a})|,\] (64)where \(\widetilde{N}_{1},\widetilde{N}_{a},\widetilde{\mu}_{1},\widetilde{\mu}_{a}\), and \(\widetilde{x}_{1,a}\) are evaluated at \(N\). Since \(\widetilde{N}_{1},\widetilde{N}_{a}\leq N\), the difference (64) is bounded above by,

\[|\mathcal{I}_{a}(N)-I_{a}(N)|\ \leq\ N\cdot\left(|d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\mu_{1},x_{1,a})|+|d(\widetilde{\mu}_{a},\widetilde{x} _{1,a})-d(\mu_{a},x_{1,a})|\right).\]

Now considering the first term in the RHS, and applying mean value theorem, we get

\[|d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\mu_{1},x_{1,a})| =\ \left|\ d_{1}(\hat{\mu}_{1},\hat{x}_{1,a})+d_{2}(\hat{\mu}_{1}, \hat{x}_{1,a})\cdot\frac{\widetilde{N}_{1}}{\widetilde{N}_{1}+\widetilde{N}_{a }}\ \right|\cdot|\widetilde{\mu}_{1}-\mu_{1}|\] \[+\left|\ d_{2}(\hat{\mu}_{1},\hat{x}_{1,a})\cdot\frac{ \widetilde{N}_{a}}{\widetilde{N}_{1}+\widetilde{N}_{a}}\ \right|\cdot|\widetilde{\mu}_{a}-\mu_{a}|,\]

where \(\hat{\mu}_{1},\hat{\mu}_{a}\), respectively, lie between \(\widetilde{\mu}_{1},\mu_{1}\), and \(\widetilde{\mu}_{a},\mu_{a}\), and \(\hat{x}_{1,a}=\frac{\widetilde{N}_{1}\hat{\mu}_{1}+\widetilde{N}_{a}\hat{\mu }_{a}}{\widetilde{N}_{1}+\widetilde{N}_{a}}\). Using (7) and (8), all the partial derivatives in the above upper bound are \(O(1)\) for \(N\geq T_{0}\). Therefore,

\[|d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\mu_{1},x_{1,a})| =\ O\left(|\widetilde{\mu}_{1}-\mu_{1}|+|\widetilde{\mu}_{a}-\mu_ {a}|\right)\] (65) \[=\ O(N^{-3\alpha/8}).\]

Following a similar procedure, we can argue using (7) and (8), that the partial derivatives of \(d(\widetilde{\mu}_{j},\widetilde{x}_{1,j})\) with respect to \(\widetilde{\mu}_{1}\) and \(\widetilde{\mu}_{j}\) are \(O(1)\) in magnitude. As a result, using the mean value theorem,

\[|d(\widetilde{\mu}_{a},\widetilde{x}_{1,a})-d(\mu_{a},x_{1,a})|\ =\ O(N^{-3 \alpha/8}).\] (66)

Therefore, we have,

\[|\mathcal{I}_{a}(N)-I_{a}(N)|\ =\ O(N^{1-3\alpha/8}),\]

for \(N\geq T_{0}\) and completing the proof. \(\Box\)

**Proof of Proposition G.2:** We have \(\mathbb{E}_{\boldsymbol{\mu}}[T_{T,M}]\leq M+\mathbb{E}_{\boldsymbol{\mu}}[T_ {7}]<\infty\). By Lemma G.13, \(\widetilde{N}_{a}(N)=\varTheta(N)\) for \(N\geq T_{7,M}\). Hence, by the definition of \(T_{8,M}\), there exists a constant \(C^{\prime}>0\) independent of \(M\), such that, for every \(M\geq 1\), \(T_{8,M}\leq C^{\prime}T_{7,M}\). As a result, \(\mathbb{E}_{\boldsymbol{\mu}}[T_{8,M}]\leq C^{\prime}\mathbb{E}_{\boldsymbol{ \mu}}[T_{7,M}]<\infty\).

Note that \(T_{7,1}=T_{7}\). Also, for every \(M\geq 1\), \(T_{8,M}\geq T_{8,1}=T_{8}\) (\(T_{8}\) is defined in Definition G.4). It is sufficient to prove the proposition for every \(N\geq T_{8}\).

We now argue for the algorithms AT2 and IAT2 separately.

**AT2:** We consider any two alternative arms \(a,b\in[K]/\{1\}\), and define the time \(\tau_{a,b}(N)\) as,

\[\tau_{a,b}(N)=\min\Big{\{}\ t\geq 1\ \Big{|}\ \mathcal{I}_{b}(N+t)-\mathcal{I}_{ a}(N+t)\ \ \mbox{and}\ \ \mathcal{I}_{b}(N)-\mathcal{I}_{a}(N)\ \ \mbox{have opposite signs}\ \Big{\}}.\]

Note that \(N+\tau_{a,b}(N)\) must be before the iteration after \(N\) by which the algorithm has picked both \(a\) and \(b\) atleast once. By the definition of \(T_{7}\) and \(T_{8}\), for every \(N\geq T_{8}\), all alternative arms in \([K]/\{1\}\) has been sampled from atleast once between iterations \(T_{7}\) and \(N\). Therefore, by Lemma G.2, we have \(\tau_{a,b}(N)=O(N^{1-3\alpha/8})\).

Since \(\mathcal{I}_{a}(N)-\mathcal{I}_{b}(N)\) and \(\mathcal{I}_{a}(N+\tau_{a,b}(N))-\mathcal{I}_{b}(N+\tau_{a,b}(N))\) have opposite signs, we have,

\[|\mathcal{I}_{a}(N)-\mathcal{I}_{b}(N)| \leq\ |(\mathcal{I}_{a}(N)-\mathcal{I}_{b}(N))\ -\ (\mathcal{I}_{a}(N+\tau_{a,b}(N))-\mathcal{I}_{b}(N+\tau_{a,b}(N)))|\] \[\leq\ |\mathcal{I}_{a}(N+\tau_{a,b}(N))-\mathcal{I}_{a}(N)|\ +\ | \mathcal{I}_{b}(N+\tau_{a,b}(N))-\mathcal{I}_{b}(N)|\] \[\leq\ |I_{a}(N+\tau_{a,b}(N))-I_{a}(N)|\ +\ |I_{b}(N+\tau_{a,b}(N))-I_{b}(N)|\] \[\ \Therefore,

\[|{\cal I}_{a}(N)-{\cal I}_{b}(N)|\ \leq\ |I_{a}(N+\tau_{a,b}(N))-I_{a}(N)|\ +\ |I_{b}(N+\tau_{a,b}(N))-I_{b}(N)|\ +O(N^{1-3 \alpha/8}).\]

By Lemma G.3, we know \(|I_{a}(N)-I_{b}(N)|\ \leq\ |{\cal I}_{a}(N)-{\cal I}_{b}(N)|+O(N^{1-3\alpha/8})\). Therefore, the above inequality implies,

\[|I_{a}(N)-I_{b}(N)| \leq\ |{\cal I}_{a}(N)-{\cal I}_{b}(N)|\ +\ O(N^{1-3\alpha/8})\] (67) \[\leq\ |I_{a}(N+\tau_{a,b}(N))-I_{a}(N)|\ +\ |I_{b}(N+\tau_{a,b}(N))-I_{b}(N)|\] \[\ +\ O(N^{1-3\alpha/8}).\]

Using mean value theorem, for \(j\in\{a,b\}\), we have,

\[|I_{j}(N+\tau_{a,b}(N))-I_{j}(N)|\ \leq\ \left(\sum_{i\in\{1,j\}}\frac{ \partial I_{j}}{\partial N_{i}}(\hat{N}_{1},\hat{N}_{j})\right)\cdot\tau_{a, b}(N),\] (68)

where \(\hat{N}_{i}\in\left[\widetilde{N}_{i}(N),\ \widetilde{N}_{i}(N+\tau_{a,b}(N))\right]\) for \(i=1,a,b\).

We know,

\[\frac{\partial I_{j}}{\partial N_{1}}(\hat{N}_{1},\hat{N}_{j})\ =\ d(\mu_{1},\hat{x}_{1,j})\quad\mbox{and}\quad\frac{ \partial I_{j}}{\partial N_{j}}(\hat{N}_{1},\hat{N}_{j})\ =\ d(\mu_{j},\hat{x}_{1,j}),\]

where \(\hat{x}_{1,j}=\frac{\hat{N}_{1}\mu_{1}+\hat{N}_{j}\mu_{j}}{\hat{N}_{1}+\hat{N }_{j}}\). Note that both the partial derivatives above are bounded from above by \(\max\{d(\mu_{1},\mu_{a}),d(\mu_{a},\mu_{1})\}\), and therefore \(O(1)\). As a result, since \(\tau_{a,b}(N)=O(N^{1-3\alpha/8})\), we have,

\[|I_{j}(N+\tau_{a,b}(N))-I_{j}(N)|\ \leq\ O(N^{1-3\alpha/8})\quad\mbox{for}\ j=a,b.\] (69)

Using (69) in (67), we get

\[|I_{a}(N)-I_{b}(N)|\ =\ O(N^{1-3\alpha/8}),\quad\mbox{for}\ N\geq T_{8}.\]

**IAT2:** First we define the modified empirical index of every alternative arm \(a\in[K]/\{1\}\) using the notation \({\cal I}_{a}^{(m)}(N)\) as,

\[{\cal I}_{a}^{(m)}(N)\ =\ {\cal I}_{a}(N)+\log(\widetilde{N}_{a}(N)).\]

We define the time \(\tau_{a,b}^{(m)}(N)\) as,

\[\tau_{a,b}^{(m)}(N)\ =\ \min\Big{\{}\ t\geq 1\ \Big{|} {\cal I}_{b}^{(m)}(N+t)-{\cal I}_{a}^{(m)}(N+t)\quad\mbox{and}\] \[{\cal I}_{b}^{(m)}(N)-{\cal I}_{a}^{(m)}(N)\quad\mbox{have opposite signs }\Big{\}}.\]

Note that, for every \(a\in[K]/\{1\}\), \({\cal I}_{a}^{(m)}(N)\) differs from \({\cal I}_{a}(N)\) by atmost \(\log(N)\) and \({\cal I}_{a}(N)\) differs from \(I_{a}(N)\) by atmost \(O(N^{1-3\alpha/8})\) for \(N\geq T_{0}\). Therefore,

\[|{\cal I}_{a}^{(m)}(N)-I_{a}(N)|\ =\ O(N^{1-3\alpha/8})\quad\mbox{for}\ N\geq T_{0} \ \mbox{and every}\ a\in[K]/\{1\}.\]

Now \(N+\tau_{a,b}^{(m)}(N)\) must be earlier than the iteration after \(N\) by which the algorithm has picked both \(a\) and \(b\) atleast once. Using the same argument as AT2, by Lemma G.2, we have \(\tau_{a,b}^{(m)}(N)=O(N^{1-3\alpha/8})\). Also, following the same steps as AT2, by replacing the empirical index \({\cal I}\) with the modified empirical index \({\cal I}^{(m)}\) for every alternative arm, we obtain,

\[|I_{a}(N)-I_{b}(N)|\ \leq\ |I_{a}(N+\tau_{a,b}^{(m)}(N))-I_{a}(N)|\ +\ |I_{b}(N+\tau_{a,b}^{(m)}(N))-I_{b}(N)|\ +\ O(N^{1-3 \alpha/8}).\]Using the mean value theorem, since the partial derivatives of \(I_{a}\) and \(I_{b}\) with respect to \(\widetilde{N}_{1},\widetilde{N}_{a}\) and \(\widetilde{N}_{b}\) are \(O(1)\), we have

\[|I_{j}(N+\tau_{a,b}^{(m)}(N))-I_{j}(N)|\ \leq\ O\left(\tau_{a,b}^{(m)}(N)\right)=O(N ^{1-3\alpha/8})\quad\text{for $j=a,b$.}\]

From the last two observations, we conclude

\[|I_{a}(N)-I_{b}(N)|\ \leq\ O(N^{1-3\alpha/8})\quad\text{for $N\geq T_{8}$.}\]

### Convergence of algorithm to optimal proportions

In this appendix we prove a slightly detailed version of Proposition 3.1 from Section 3. In Proposition 3.1, we argue that the proportion of samples allocated by the algorithm converges to the optimal proportions for the instance, a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\), as the no. of samples grows to \(\infty\).

For every \(M\geq 1\), we use \(T_{7,M}\) and \(T_{8,M}\) as defined in Definition G.3 in Appendix G.1.2. Recall that \(T_{7}=T_{7,1}\) and \(T_{8}=T_{8,1}\). By Proposition G.3, we have \(\mathbb{E}_{\boldsymbol{\mu}}[T_{8,M}]<\infty\), and \(T_{8,M}\geq T_{8}\) for every \(M\geq 1\).

Recall that \(\boldsymbol{\omega}^{\star}\) is the unique optimal allocation according to Proposition 2.2, and \(\widetilde{\boldsymbol{\omega}}(N)=(\widetilde{\omega}_{a}(N):a\in[K])\) with \(\widetilde{\omega}_{a}(N)=\frac{\widetilde{N}_{a}(N)}{N}\) is the algorithms allocation at iteration \(N\). We now state a slightly detailed version of Proposition 3.1 from Section 3,

**Proposition G.4**.: _There exists constants \(C_{1}>0\) and \(M_{6}\geq 1\) depending on \(\boldsymbol{\mu},\alpha\), and \(K\) such that, for every \(N\geq T_{8,M_{6}}\) and \(a\in[K]\), we have_

\[|\widetilde{\omega}_{a}(N)-\omega_{a}^{\star}|\ \leq\ C_{1}N^{-3\alpha/8}\quad \text{and}\quad|\widetilde{\mu}_{a}(N)-\mu_{a}|\ \leq\ \epsilon(\boldsymbol{\mu})N^{-3\alpha/8},\]

_where \(\epsilon(\boldsymbol{\mu})\) is a constant depending only on \(\boldsymbol{\mu}\) and defined in Appendix B._

Detailed proof of Proposition G.4 is in Appendix G.2.2 and relies on using IFT.

Below we define the random times \(T_{good}\) and \(T_{stable}\), which are mentioned in the statements of Proposition 3.1, 5.1, and Lemma 5.1 from the main body of the paper.

**Definition G.5** (\(T_{stable}\) **and \(T_{good}\))**.: _We define \(T_{good}=T_{7,M_{6}}\) and \(T_{stable}=T_{8,M_{6}}\), where \(M_{6}\geq 1\) is introduced in Proposition G.4._

**Remark G.1**.: _Note that, by definition, \(T_{good}\geq T_{4},T_{6}\). As a result, by Proposition G.1 and Lemma G.13, \(\Big{|}g(\boldsymbol{\mu},\widetilde{N}(N))\Big{|}=O(N^{-3\alpha/8})\) and \(\widetilde{N}_{j}(N)=\Theta(N)\) for every \(j\in[K]\) and \(N\geq T_{good}\)._

Proof of Lemma 5.1:By the definition of \(T_{7,M},\ T_{8,M}\) in Appendix G.1.2, and since \(T_{good}=T_{7,M_{6}},\ T_{stable}=T_{8,M_{6}}\), every alternative arm in \([K]/\{1\}\) gets picked atleast once between the iterations \(T_{good}\) and \(T_{stable}\). The other part of the statement of Lemma 5.1 follows from Lemma G.2 because \(T_{good}\geq T_{7}\). 

Before proving Proposition G.4 in Appendix G.2.2, we find a tighter upper bound on the time to reach optimal proportion \(T_{stable}\) in the following Appendix G.2.1. While doing this, we identify a similarity between the time to reach stabilty in fluid dynamics and that for the algorithm.

#### g.2.1 Bounding time to reach stability

Lemma G.4 gives an upper bound on the time to reach stability for the algorithmic allocations. We define \(\omega_{\min}^{\star}=\min_{a\in[K]/\{1\}}\omega_{a}^{\star}\).

According to the discussion in Section 4, if the fluid dynamics has state \(\widetilde{\boldsymbol{N}}(T_{good})\) at time \(T_{good}\), then it hits all the indexes and reaches stability by a time atmost \(\frac{T_{good}}{\omega_{\min}^{\star}}\). In Lemma G.4, we argue that, the algorithm also approximately reaches the optimal proportion \(\boldsymbol{\omega}^{\star}\) by atmost \(\approx\frac{T_{good}}{\omega_{\min}^{\star}}\) iterations.

**Lemma G.4**.: _For every \(M\geq M_{6}\) (\(M_{6}\) is a constant defined in the statement of Proposition G.4),_

\[T_{8,M}\;\leq\;\frac{T_{7,M}+1}{\omega_{\min}^{\star}-C_{1}M^{-3\alpha/8}},\]

_which implies_

\[T_{stable}\;\leq\;\frac{T_{good}+1}{\omega_{\min}^{\star}-C_{1}M_{6}^{-3\alpha/ 8}}.\]

_Moreover, we have_

\[\limsup_{M\to\infty}\frac{T_{8,M}}{T_{7,M}}\;\leq\;\frac{1}{\omega_{\min}^{ \star}}\mbox{ a.s. in }\mathbb{P}_{\boldsymbol{\mu}}.\]

Proof.: From Proposition G.4, it follows that, for every \(M\geq M_{6}\) and \(N\geq T_{8,M}\geq T_{8,M_{6}}\), we have,

\[\max_{a\in[K]}|\widetilde{\omega}_{a}(N)-\omega_{a}^{\star}|\;\leq\;C_{1}N^{- 3\alpha/8}.\] (70)

Since \(T_{8,M}\) is the first iteration after \(T_{7,M}\) by which every alternative arm has been picked atleast once, we have some arm \(a\in[K]/\{1\}\) such that,

\[\widetilde{N}_{a}(T_{8,M})\;=\;\widetilde{N}_{a}(T_{7,M})+1\;\leq\;T_{7,M}+1.\]

Now by (70), we have

\[\widetilde{N}_{a}(T_{8,M})\;\geq\;(\omega_{a}^{\star}-C_{1}T_{8,M}^{-3\alpha/ 8})T_{8,M}\;\geq\;(\omega_{\min}^{\star}-C_{1}M^{-3\alpha/8})T_{8,M}.\]

Combining the last two observation, we have,

\[T_{8,M}\;\leq\;\frac{T_{7,M}+1}{\omega_{\min}^{\star}-C_{1}M^{-3\alpha/8}}\quad \mbox{a.s. in }\mathbb{P}_{\boldsymbol{\mu}}\]

for every \(M\geq M_{6}\).

Since \(T_{8,M},T_{7,M}\to\infty\) as \(M\to\infty\) a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\), we have,

\[\limsup_{M\to\infty}\frac{T_{8,M}}{T_{7,M}}\;\leq\;\frac{1}{\omega_{\min}^{ \star}}\quad\mbox{a.s. in }\mathbb{P}_{\boldsymbol{\mu}}.\]

#### g.2.2 Proving Proposition G.4

By Proposition G.1 and G.3, there exists a constant \(C>0\) independent of the sample paths, such that \(\widetilde{\boldsymbol{\omega}}(N)=(\widetilde{\omega}_{a}(N))_{a\in[K]}\) satisfies,

\[|g(\boldsymbol{\mu},\widetilde{\boldsymbol{\omega}}(N))|\;\;=\; \;\Bigg{|}\;\sum_{a\in[K]/\{1\}}\frac{d(\mu_{1},x_{1,a}(N))}{d(\mu_{a},x_{1,a} (N))}-1\;\Bigg{|}\;\;\leq\;CN^{-3\alpha/8},\quad\mbox{and}\] \[\max_{a,b\in[K]/\{1\}}|I_{a}(N)-I_{b}(N)|\;\leq\;CN^{-3\alpha/8},\] (71)

for all \(N\geq T_{8}\) a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\).

Proof of Proposition G.4 relies on using the implicit function theorem. Before proving the proposition, we describe below the framework over which we apply the implicit function theorem. We define the following functions,

\[\Psi_{1}(\boldsymbol{\omega},\boldsymbol{\eta}) =\;g(\boldsymbol{\mu},\boldsymbol{\omega})-\eta_{1}\;=\;\sum_{a \neq 1}\frac{d(\mu_{1},x_{1,a}(\omega_{1},\omega_{a}))}{d(\mu_{a},x_{1,a}( \omega_{1},\omega_{a}))}-1-\eta_{1},\] \[\mbox{for }a\in[K]/\{1\},\quad\Psi_{a}(\boldsymbol{\omega},I, \boldsymbol{\eta}) =\;W_{a}(\omega_{1},\omega_{a})-I-\eta_{a},\quad\mbox{and}\] \[\Psi_{K+1}(\boldsymbol{\omega}) =\;\sum_{a\in[K]}\omega_{a}-1,\]where \(\quad\boldsymbol{\omega}=(\omega_{a})_{a\in[K]}\in\mathbb{R}_{\geq 0}^{K},\quad \boldsymbol{\eta}=(\eta_{a})_{a\in[K]}\in\mathbb{R}^{K},\quad I\in\mathbb{R},\quad\) and for every \(a\in[K]/\{1\}\), \(x_{1,a}(\omega_{1},\omega_{a})=\frac{\omega_{1}\mu_{1}+\omega_{a}\mu_{a}}{ \omega_{1}+\omega_{a}}\quad\text{and}\quad W_{a}(\omega_{1},\omega_{a})= \omega_{1}d(\mu_{1},x_{1,a}(\omega_{1},\omega_{a}))+\omega_{a}d(\mu_{a},x_{1,a} (\omega_{1},\omega_{a})).\)

Using the functions defined above, we define the vector valued function \(\boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})\) as follows,

\[\boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})=(\ \Psi_{1}( \boldsymbol{\omega},\boldsymbol{\eta}),\ \Psi_{2}(\boldsymbol{\omega},I,\boldsymbol{\eta}),\ \Psi_{3}(\boldsymbol{\omega},I,\boldsymbol{\eta}),\ \ldots,\ \Psi_{K}( \boldsymbol{\omega},I,\boldsymbol{\eta}),\ \Psi_{K+1}(\boldsymbol{\omega})\ )\.\]

\(\boldsymbol{\Psi}\) maps tuples of the form \(\quad(\boldsymbol{\omega},I,\boldsymbol{\eta})\in\mathbb{R}_{\geq 0}^{K} \times\mathbb{R}\times\mathbb{R}^{K}\) to \(\mathbb{R}^{K+1}\).

Its easy to observe that for every \(\boldsymbol{\omega}=(\omega_{a}:a\in[K])\in\mathbb{R}_{\geq 0}^{K}\) satisfying \(\sum_{a\in[K]}\omega_{a}=1\), and \(I\in\mathbb{R}\), there is a unique \(\boldsymbol{\eta}\in\mathbb{R}^{K}\) for which \(\boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})=\mathbf{0}_{K+1}\). We refer to the quantity \(\max_{a\in[K]}|\eta_{a}|\) as the _violation_ caused by the pair \((\boldsymbol{\omega},I)\) to the optimality conditions in (54).

By (54), all the alternative arms in \([K]/\{1\}\) have equal normalized index under the optimal allocation \(\boldsymbol{\omega}^{\star}\). Let \(I^{\star}=W_{a}(\omega_{1}^{\star},\omega_{a}^{\star})\) for every \(a\in[K]/\{1\}\). Then Proposition 2.2 implies \((\boldsymbol{\omega}^{\star},I^{\star})\) is the unique tuple satisfying

\[\boldsymbol{\Psi}(\boldsymbol{\omega}^{\star},I^{\star},\mathbf{0}_{K})= \mathbf{0}_{K+1}.\]

To prove Proposition G.4 we need the two technical lemmas: Lemma G.5 and G.6. Let us define \(\|\boldsymbol{x}\|_{\infty}=\max_{a\in[K]}|x_{a}|\) for every \(\boldsymbol{x}\in\mathbb{R}^{K}\).

Lemma G.5 shows that, the set of allocations satisfying the optimality conditions in (54) upto a maximum violation of \(r>0\) shrinks to \(\boldsymbol{\omega}^{\star}\) as \(r\) decreases to zero. In Lemma G.6, we use Lemma G.5 and IFT to argue that if the perturbation vector \(\boldsymbol{\eta}\) satisfies \(\|\boldsymbol{\eta}\|_{\infty}\leq\eta_{\max}\), where \(\eta_{\max}>0\) is a constant depending only on \(\boldsymbol{\mu}\), then there is a unique pair \((\boldsymbol{\omega},I)\) satisfying \(\boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})=\mathbf{0}_{K+1}\). Moreover, the function mapping a perturbation vector \(\boldsymbol{\eta}\in[-\eta_{\max},\eta_{\max}]^{K}\) to the unique pair \((\boldsymbol{\omega},I)\) solving \(\boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})=\mathbf{0}_{K+1}\) is Lipschitz continuous.

It is now easy to see Proposition G.4 follows from Lemma G.5 and G.6. By (71), the violation caused by the algorithmic allocation \(\widetilde{\boldsymbol{\omega}}(N)\) to the optimality conditions in (54) converges to zero uniformly at a rate \(O(N^{-3\alpha/8})\). We wait for sufficiently many iterations such that, the violation becomes smaller than \(\eta_{\max}\). Then using Lipschitzness of the allocation as a function of perturbation (proven in Lemma G.6), we have \(\|\widetilde{\boldsymbol{\omega}}(N)-\boldsymbol{\omega}^{\star}\|_{\infty}=O (N^{-3\alpha/8})\).

**Lemma G.5**.: _For every \(r\geq 0\), we define the quantity,_

\[dist(\boldsymbol{\omega}^{\star},r)\ =\ \max\Big{\{} \max\{\|\boldsymbol{\omega}-\boldsymbol{\omega}^{\star}\|_{ \infty},\ |I-I^{\star}|\}\ \Big{|}\ \boldsymbol{\omega}\in\mathbb{R}_{\geq 0}^{K},\ \ I\in\mathbb{R},\ \text{ and}\] \[\exists\ \boldsymbol{\eta}\in[-r,r]^{K}\ \text{ such that }\ \boldsymbol{\Psi}(\boldsymbol{\omega},I,\boldsymbol{\eta})=\mathbf{0}_{K+1}\ \Big{\}}.\]

_The following statements are true about the mapping \(r\mapsto dist(\boldsymbol{\omega}^{\star},r)\),_

1. \(dist(\boldsymbol{\omega}^{\star},0)=0\)_,_
2. \(dist(\boldsymbol{\omega}^{\star},r)\) _is non-decreasing in_ \(r\)_, and_
3. \(\lim_{r\to 0}dist(\boldsymbol{\omega}^{\star},r)=0\)_._

Proof.: **Statement 1:** Statement 1 follows directly from the fact that \((\boldsymbol{\omega}^{\star},I^{\star})\) is the unique tuple satisfying \(\boldsymbol{\Psi}(\boldsymbol{\omega}^{\star},I^{\star},\mathbf{0}_{K})= \mathbf{0}_{K+1}\), as proven in Proposition 2.2.

**Statement 2:** Follows directly from the definition of \(dist(\boldsymbol{\omega}^{\star},r)\).

**Statement 3:** By statement 2, \(\lim_{r\to 0}dist(\boldsymbol{\omega}^{\star},r)\) exists and is non-negative. We consider a contradiction to statement 3 and assume that \(\lim_{r\to 0}dist(\boldsymbol{\omega}^{\star},r)=d>0\).

Since \(\ r\to dist(\boldsymbol{\omega}^{\star},r)\\) is non-decreasing, we can construct a decreasing sequence \(\{r_{n}\}_{n\geq 1}\) such that, for every \(n\geq 1,\ \ r_{n}>0,\ \ dist(\boldsymbol{\omega}^{\star},r_{n})\geq d,\ \ \text{and}\ \ \lim_{n\to\infty}r_{n}=0\). As a result, using the definition of \(dist(\boldsymbol{\omega}^{\star},r)\), we have a sequence of tuples \(\{(\boldsymbol{\omega}_{n},I_{n},\boldsymbol{\eta}_{n})\}_{n\geq 1}\), such that,

\[\text{for every }n\geq 1,\ \ \ \|\boldsymbol{\eta}_{n}\|_{\infty}\leq r_{n},\quad \boldsymbol{\Psi}(\boldsymbol{\omega}_{n},I_{n},\boldsymbol{\eta}_{n})=\mathbf{0}_{K +1},\quad\text{and}\]\[\liminf_{n\to\infty}\max\left\{\|\bm{\omega}_{n}-\bm{\omega}^{\star}\|_{\infty},|I_{ n}-I^{\star}|\right\}\geq d.\]

Since \(\varPsi_{K+1}(\bm{\omega}_{n})=0\) for every \(n\geq 1\), the whole sequence \((\bm{\omega}_{n})_{n\geq 1}\) lies in the set

\[\left\{\begin{array}{l}(\omega_{1},\omega_{2},\ldots,\omega_{K})\in\mathbb{R} _{\geq 0}^{K}\ \left|\ \ \sum_{i\in[K]}\omega_{i}=1\ \right.\end{array}\right\},\]

which is compact with respect to the norm \(\|\cdot\|_{\infty}\).

Let for every \(n\geq 1\) and \(a\in[K]\), \(\omega_{a,n}\) and \(\eta_{a,n}\) be, respectively, the \(a\)-th component of the vectors \(\bm{\omega}_{n}\) and \(\bm{\eta}_{n}\). For every \(n\geq 1\) and \(a\in[K]/\{1\}\), we have \(I_{n}=W_{a}(\omega_{1,n},\omega_{a,n})-\eta_{a,n}\). Since \(W_{a}(\cdot,\cdot)\) always lies in the interval \([0,d(\mu_{1},\mu_{a})+d(\mu_{a},\mu_{1})]\) and \(|\eta_{a,n}|\leq r_{n}\), we have

\[-r_{n}\;\leq\;I_{n}\;\leq\;d(\mu_{1},\mu_{a})+d(\mu_{a},\mu_{1})+r_{n}\quad \text{for every $n\geq 1$}.\]

By our assumption, we already have \(r_{n}\to 0\), which also implies, the sequence \(r_{n}\) is bounded from above. As a result, \(I_{n}\) is also bounded. Therefore, we can have a convergent subsequence \(\{(\bm{\omega}_{n_{k}},I_{n_{k}})\}_{k\geq 1}\), with limits \(\bm{\omega}_{n_{k}}\to\bm{\omega}^{\prime}\) and \(I_{n_{k}}\to I^{\prime}\) in the \(\|\cdot\|_{\infty}\)-norm.

For every \(k\geq 1\), we have \(\varPsi(\bm{\omega}_{n_{k}},I_{n_{k}},\bm{\eta}_{n_{k}})=\bm{0}_{K+1}\). As a result, using the continuity of \(\varPsi(\cdot)\) with respect to its arguments, we have \(\varPsi(\bm{\omega}^{\prime},I^{\prime},\bm{0}_{K})=\bm{0}_{K+1}\), implying \(\bm{\omega}^{\prime}\) is an optimal allocation for the instance \(\bm{\mu}\).

Hence, our assumption \(\liminf_{n\to\infty}\max\{\|\bm{\omega}_{n}-\bm{\omega}^{\star}\|_{\infty},|I _{n}-I^{\star}|\}\geq d\) implies \(\max\{\|\bm{\omega}^{\star}-\bm{\omega}^{\star}\|_{\infty},\ |I^{\prime}-I^{\star}|\}\geq d>0\), which further implies \(\bm{\omega}^{\prime}\neq\bm{\omega}^{\star}\). As a result, the instance \(\bm{\mu}\) has two distinct optimal allocations \(\bm{\omega}^{\prime}\) and \(\bm{\omega}^{\star}\), which contradicts Proposition 2.2. 

**Lemma G.6**.: _There exists \(\eta_{\max}>0\) depending only on the instance \(\bm{\mu}\), such that the following statements are true,_

1. _For every_ \(\bm{\eta}\in[-\eta_{\max},\eta_{\max}]^{K}\)_, there exists a unique tuple_ \((\bm{\omega},I)\in\mathbb{R}_{\geq 0}^{K}\times\mathbb{R}\) _which satisfies,_ \(\varPsi(\bm{\omega},I,\bm{\eta})=\bm{0}_{K+1}\)_._
2. _For every_ \(\bm{\eta}\in[-\eta_{\max},\eta_{\max}]^{K}\)_, we call the unique tuple mentioned in statement_ 1 _as_ \((\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}))\)_. Then the function_ \[(\overline{\bm{\omega}},\overline{I}):[-\eta_{\max},\eta_{\max}]^{K}\mapsto \mathbb{R}_{\geq 0}^{K}\times\mathbb{R}\] _is_ \(L\)_-Lipschitz, for some_ \(L>0\) _depending on the instance_ \(\bm{\mu}\)_._

Proof.: By Proposition 2.2, we know that the optimal allocation \(\bm{\omega}^{\star}\) is the unique allocation satisfying,

\[\varPsi(\bm{\omega}^{\star},I^{\star},\bm{0}_{K})\;=\;\bm{0}_{K+1}\]

for some \(I^{\star}>0\). Note that \(\varPsi(\bm{\omega},I,\bm{\eta})=\varPhi(\bm{\omega},I,\bm{\eta},1)\) for every tuple \((\bm{\omega},I,\bm{\eta})\), where \(\varPhi\) is the function defined in Appendix C. By statement 3 of Lemma C.1, the Jacobian \(\frac{\partial\varPsi}{\partial(\bm{\omega},I)}\) of the function \(\varPsi(\bm{\omega},I,\bm{\eta})\) is invertible at the tuple \((\bm{\omega}^{\star},I^{\star},\bm{0}_{K})\).

Therefore, applying the Implicit function theorem, we can find \(\delta_{0},\delta_{1}>0\), and continuously differentiable functions

\[(\overline{\bm{\omega}}(\cdot),\overline{I}(\cdot))\;:\;(-\delta_{0},\delta_{0 })^{K}\mapsto\mathbb{R}_{\geq 0}^{K}\times\mathbb{R},\]

such that,

1. \(\overline{\bm{\omega}}(\bm{0}_{K})=\bm{\omega}^{\star}\), \(\overline{I}(\bm{0}_{K})=I^{\star}\), and
2. for every \(\bm{\eta}\in(-\delta_{0},\delta_{0})^{K}\), \((\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}))\) is the unique tuple in \(\mathbb{R}_{\geq 0}^{K}\times\mathbb{R}\) to satisfy, \[\max\left\{\|\overline{\bm{\omega}}(\bm{\eta})-\bm{\omega}^{\star}\|_{\infty},| \overline{I}(\bm{\eta})-I^{\star}|\right\}\leq\delta_{1}\quad\text{and}\quad \varPsi(\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}),\bm{\eta})= \bm{0}_{K+1}.\]By statement 3 of Lemma G.5, we can find a \(\delta_{2}>0\) such that, \(dist(\bm{\omega}^{\star},r)<\delta_{1}\) for \(r\in[0,\delta_{2}]\). We define \(\eta_{\max}=\min\left\{\frac{\delta_{0}}{2},\delta_{2}\right\}\).

By the definition of \(dist(\bm{\omega}^{\star},\cdot)\), for every \(\bm{\eta}\in[-\eta_{\max},\eta_{\max}]^{K}\), if a tuple \((\bm{\omega},I)\) satisfies \(\bm{\Psi}(\bm{\omega},I,\bm{\eta})=\bm{0}_{K+1}\), then it also satisfies \(\max\{\|\bm{\omega}-\bm{\omega}^{\star}\|_{\infty},|I-I^{\star}|\}<\delta_{1}\).

On the other hand, by IFT, since \(\eta_{\max}<\delta_{0}\), \((\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}))\) is the only such tuple possible. Therefore, for every \(\bm{\eta}\in[-\eta_{\max},\eta_{\max}]^{K}\), \((\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}))\) is the unique element in \(\mathbb{R}^{K}_{\geq 0}\times\mathbb{R}\) such that \(\bm{\Psi}(\overline{\bm{\omega}}(\bm{\eta}),\overline{I}(\bm{\eta}),\bm{\eta} )=\bm{0}_{K+1}\). This proves the first statement of Lemma G.6.

Since \(\overline{\bm{\omega}}(\cdot),\overline{I}(\cdot)\) is continuously differentiable in \((-\delta_{0},\delta_{0})^{K}\), every component of this mapping must be \(L\)-Lipschitz for some \(L>0\) in \(\left[-\frac{\delta_{0}}{2},\frac{\delta_{0}}{2}\right]^{K}\) equipped with \(\|\cdot\|_{\infty}\)-norm. We can take \(L\) to be the maximum of the \(\|\cdot\|_{1}\)-norm of the gradients of different components of \((\overline{\bm{\omega}}(\cdot),\overline{I}(\cdot))\) over the set \(\left[-\frac{\delta_{0}}{2},\frac{\delta_{0}}{2}\right]^{K}\). Since the gradients are all continuous, their \(\|\cdot\|_{1}\)-norm must be bounded in a compact set like \(\left[-\frac{\delta_{0}}{2},\frac{\delta_{0}}{2}\right]^{K}\), and hence \(L<\infty\). Therefore, the second part of Lemma G.6 follows from our assumption \(\eta_{\max}\leq\frac{\delta_{0}}{2}\). 

We now proceed on proving Proposition G.4.

**Proof of Proposition G.4:** Recall that in Section 5, for every \(a\in[K]/\{1\}\), we defined the normalized index as \(H_{a}(N)=\frac{I_{a}(N)}{N}\).

Taking \(H(N)=H_{2}(N)=\frac{I_{2}(N)}{N}\), let \(\widetilde{\bm{\eta}}(N)\) be the unique \(\bm{\eta}\in\mathbb{R}^{K}\) to satisfy, \(\bm{\Psi}(\widetilde{\bm{\omega}}(N),H(N),\bm{\eta})=\bm{0}_{K+1}\).

Note that for every \(a\in[K]/\{1\}\), we have \(W_{a}(\widetilde{\omega}_{1}(N),\widetilde{\omega}_{a}(N))=H_{a}(N)\). As a result, by (71), we have \(\|\widetilde{\bm{\eta}}(N)\|_{\infty}\leq CN^{-3\alpha/8}\) for all \(N\geq T_{8}\).

Now we pick \(M_{6}\geq 1\) large enough, such that,

\[CM_{6}^{-3\alpha/8}<\eta_{\max},\]

where \(\eta_{\max}\) is introduced in Lemma G.6. We define \(T_{stable}=T_{8,M_{6}}\). Note that \(T_{stable}\geq T_{6}\geq T_{0}\). As a result, by the definition of \(T_{0}\) in Appendix G.3, we have \(\max_{a\in[K]}|\widetilde{\mu}_{a}(N)-\mu_{a}|\leq\epsilon(\bm{\mu})N^{-3 \alpha/8}\) for every \(N\geq T_{stable}\).

Now, by (71), for \(N\geq T_{stable}\), the allocations \(\widetilde{\bm{\omega}}(N)\) satisfies, \(\bm{\Psi}(\widetilde{\bm{\omega}}(N),H(N),\widetilde{\bm{\eta}}(N))=\bm{0}_{K+1}\) with

\[\|\widetilde{\bm{\eta}}(N)\|_{\infty}\;\leq\;CN^{-3\alpha/8}\;\leq\;CM_{6}^{-3 \alpha/8}\;<\;\eta_{\max}.\]

As a result, by Lemma G.6, we have

\[\widetilde{\omega}_{a}(N)\;=\;\overline{\omega}_{a}(\widetilde{\bm{\eta}}(N)) \quad\text{for every $a\in[K]$},\;\text{and}\;N\geq T_{stable},\]

where \(\overline{\omega}_{a}(\cdot)\) is the \(a\)-th component of the vector valued function \(\overline{\bm{\omega}}(\cdot)\) introduced in Lemma G.6.

By Lemma G.6, for every \(a\in[K]\), \(\overline{\omega}_{a}(\cdot)\) is \(L\)-Lipschitz in \([-\eta_{\max},\eta_{\max}]^{K}\) equipped with \(\|\cdot\|_{\infty}\)-norm. As a result, for \(N\geq T_{stable}\), we have,

\[\max_{a\in[K]}|\widetilde{\omega}_{a}(N)-\omega_{a}^{\star}|\;\;= \;\max_{a\in[K]}|\widetilde{\omega}_{a}(\widetilde{\bm{\eta}}(N))-\overline{ \omega}_{a}(\bm{0}_{K})| \leq\;L\|\widetilde{\bm{\eta}}(N)-\bm{0}_{K}\|_{\infty}\] \[=\;L\|\widetilde{\bm{\eta}}(N)\|_{\infty}\;\leq\;LCN^{-3\alpha/8}.\]

Taking \(C_{1}=LC\), we have the desired result. 

### \(T_{0}\) has finite expectation

In Section 5, we introduced the random time \(T_{0}\) as,

\[T_{0}\;=\;\min\left\{\;\;N^{\prime}\geq 1\;\;\Big{|}\;\;\forall N\geq N^{\prime}, \;\;\max_{a\in[K]}\;|\;\widetilde{\mu}_{a}(N)-\mu_{a}|\leq\epsilon(\bm{\mu})N ^{-3\alpha/8}\;\;\right\},\]

where \(\epsilon(\bm{\mu})\) is a positive constant defined in Appendix B and depends only on the instance \(\bm{\mu}\).

**Lemma G.7**.: _The random time \(T_{0}\) satisfies \(\mathbb{E}_{\bm{\mu}}[T_{0}]<\infty\) and hence \(T_{0}<\infty\) a.s. in \(\mathbb{P}_{\bm{\mu}}\)._

Proof.: To avoid notational clutter, let \(\mathbb{P}=\mathbb{P}_{\bm{\mu}}\) and \(\epsilon=\epsilon(\bm{\mu})\). Then for any \(N\),

\[\mathbb{P}(T_{0}=N+1) \leq\ \mathbb{P}\left(\exists a\in[K],\ \ |\widetilde{\mu}_{a}(N)-\mu_{a}|> \epsilon N^{-3\alpha/8}\right)\] \[\leq\sum_{a\in[K]}\mathbb{P}\left(|\widetilde{\mu}_{a}(N)-\mu_{ a}|>\epsilon N^{-3\alpha/8}\right)\] \[\leq\sum_{a\in[K]}\sum_{t=(N^{\alpha}-C_{1})_{+}}^{N}\mathbb{P} \left(|\hat{\mu}_{a,t}-\mu_{a}|>\epsilon N^{-3\alpha/8}\right),\]

where \(\hat{\mu}_{a,t}\) denotes the empirical mean of \(t\) i.i.d. samples drawn from the \(a\)-th arm, and the last step follows from statement 2 of Proposition G.5, which says \(\widetilde{N}_{a}(N)\geq N^{\alpha}-C_{1}\) for some constant \(C_{1}>0\).

Using Chernoff's bound (like in the proof of Lemma 19 of [11]), we have,

\[\mathbb{P}\left(|\hat{\mu}_{a,t}-\mu_{a}|>\epsilon N^{-3\alpha/8 }\right)\ \leq\ \mathbb{P}\left(\hat{\mu}_{a,t}>\mu_{a}+\epsilon N^{-3\alpha/8}\right)+ \mathbb{P}\left(\hat{\mu}_{a,t}<\mu_{a}-\epsilon N^{-3\alpha/8}\right)\] \[\qquad\leq\ \exp\left(-t\cdot d(\mu_{a}+\epsilon N^{-3\alpha/8}, \mu_{a})\right)+\exp\left(-t\cdot d(\mu_{a}-\epsilon N^{-3\alpha/8},\mu_{a})\right)\] \[\qquad\leq\ 2\exp\left(-t\cdot\min\Big{\{}\ d(\mu_{a}+\epsilon N ^{-3\alpha/8},\mu_{a}),\ d(\mu_{a}-\epsilon N^{-3\alpha/8},\mu_{a})\Big{\}}\right)\]

Using (6), we have a constant \(C_{2}>0\) depending on the instance \(\bm{\mu}\) and such that,

\[\min\Big{\{}d(\mu_{a}+\epsilon N^{-3\alpha/8},\mu_{a}),\ d(\mu_{a}-\epsilon N ^{-3\alpha/8},\mu_{a})\Big{\}}\ \geq\ \epsilon^{2}C_{2}N^{-\frac{3\alpha}{4}}.\]

Therefore, we have,

\[\mathbb{P}\left(|\hat{\mu}_{a,t}-\mu_{a}|>\epsilon N^{-3\alpha/8}\right)\ \leq\ 2\exp\left(-t\epsilon^{2}C_{2}N^{-\frac{3\alpha}{4}}\right).\]

Therefore,

\[\mathbb{P}(T_{0}=N+1)\ \leq\ \sum_{i\in[K]}\ \sum_{t=(N^{\alpha}-C_{1})_{+} }^{N}2\exp\left(-t\epsilon^{2}C_{2}N^{-\frac{3\alpha}{4}}\right)\] \[\leq\ \sum_{i\in[K]}\ \sum_{t=N^{\alpha}-C_{1}}^{N}2\exp\left(-t \epsilon^{2}C_{2}N^{-\frac{3\alpha}{4}}\right)\] \[\leq\ 2\sum_{i\in[K]}\exp\left(-\epsilon^{2}C_{2}(N^{\alpha}-C_{1})N ^{-\frac{3\alpha}{4}}\right)\cdot\Big{(}\sum_{t=N^{\alpha}-C_{1}}^{N}\exp \left(-\epsilon^{2}C_{2}N^{-\frac{3\alpha}{4}}(t-N^{\alpha}+C_{1})\right) \Big{)}\] \[\leq\ 2KN\exp\left(-\epsilon^{2}C_{2}(N^{\alpha}-C_{1})N^{-\frac{3 \alpha}{4}}\right)\ =\ 2KN\exp(-\varOmega(N^{\frac{\alpha}{4}})),\]

where the constant hidden in \(\varOmega(\cdot)\) depends only on \(\bm{\mu}\). Using the obtained upper bound,

\[\mathbb{E}[T_{0}] =\ \mathbb{P}(T_{0}=1)+\sum_{N\geq 1}(N+1)\mathbb{P}(T_{0}=N+1)\] \[\leq\ 1+\sum_{N\geq 1}2KN(N+1)\exp(-\varOmega(N^{\frac{\alpha}{4} })).\]

Note that the series on the RHS is convergent for any \(\alpha\in(0,1)\). Therefore \(\mathbb{E}_{\bm{\mu}}[T_{0}]<\infty\)

### Properties of exploration

In the following discussion, the set of iterations in which the algorithm does exploration is defined as all iterations \(N\) where \(\min_{a\in[K]}\widetilde{N}_{a}(N-1)<N^{\alpha}\) (which is equivalent to having \(\mathcal{V}_{N}\neq\emptyset\), where \(\mathcal{V}_{N}\) denotes the set of starved arms at iteration \(N\), and is defined in Section 3). We define the epoch of exploration at some arbitrary iteration as follows,

**Definition G.6**.: _If the algorithm does exploration at iteration \(N\), the epoch of exploration at \(N\) is the maximum no. of consecutive iterations including \(N\) in which the algorithm has done exploration. More precisely, if \(N_{1}\) and \(N_{2}\) are, respectively, defined as,_

\[N_{1} =\ \max\left\{t\leq N\mid t-1\text{ is not an exploration}\right\}\quad \text{and}\] \[N_{2} =\ \min\left\{t\geq N\mid t+1\text{ is not an exploration}\right\}\]

_then the epoch of exploration at iteration \(N\) is \(N_{2}-N_{1}+1\)._

**Proposition G.5**.: _The following statements are true:_

1. _For every iteration which is an exploration, the epoch of exploration at that iteration is upper bounded by a constant depending on_ \(K\) _and_ \(\alpha\)_. We denote this constant using_ \(T_{\text{epoch}}\)_._
2. _There exists a constant_ \(C\) _depending on_ \(K\) _and_ \(\alpha\) _such that, over every sample path, we have_ \[\min_{a\in[K]}\widetilde{N}_{a}(N)\ \geq\ N^{\alpha}-C.\] _As a result,_ \(\widetilde{N}_{a}(N)=\varOmega(N^{\alpha})\) _for every arm_ \(a\in[K]\)_._
3. _There exists a_ \(M\) _depending on_ \(K\) _and_ \(\alpha\) _such that, every epoch of exploration starting after iteration_ \(M\) _has length atmost_ \(K\)_, and every arm can get pulled atmost once in that epoch. We call the constant_ \(M\) _as_ \(M_{\text{explo}}\)_._
4. _If an epoch of exploration starts from some_ \(N\geq M_{\text{explo}}\)_, then the next epoch of exploration doesn't start before another_ \(\varTheta(N^{1-\alpha})\) _iterations._
5. _Let_ \(\hat{N}\geq M_{\text{explo}}\) _be such that,_ \(\hat{N}\) _is an exploration. Define the following sequence,_ \(N_{0}=\hat{N}\)_, and for_ \(k\geq 1\)_,_ \[N_{k}\ =\ \min\left\{\begin{array}{l}N>N_{k-1}\end{array}\right|\ N_{k} \text{ is the begining of an epoch of exploration}\end{array}\right\}.\] _Then_ \(N_{k}=\hat{N}+\varOmega(k^{1/\alpha})\)_. In other words, for any_ \(N\geq M_{\text{explo}}\)_, the_ \(k\)_-th epoch of exploration after iteration_ \(N\) _starts after_ \(N+\varOmega(k^{1/\alpha})\) _iterations._
6. _For any_ \(N\geq M_{\text{explo}}+T_{\text{epoch}}+1\) _and_ \(T\geq 1\)_, the no. of epochs of exploration intersecting with the set of iterations_ \(\{N,N+1,N+2,\ldots,N+T\}\) _is_ \(O(T^{\alpha})\)_._

Proof.: **Statement 1:** Let the algorithm does exploration at iteration \(N\). We can always choose \(N\) in such a way that, iteration \(N-1\) was not an exploration, by choosing \(N\) to be the iteration at which an epoch begins. If the epoch of exploration starting at iteration \(N\) continues till iteration \(N+t\), _i.e._, the iterations \(N,N+1,\ldots,N+t\) are exploration, then,

\[(N+t)^{\alpha} \geq\ \min_{a\in[K]}\widetilde{N}_{a}(N+t-1)\ \overset{(1)}{\geq}\ \min_{a\in[K]} \widetilde{N}_{a}(N-1)+\frac{t}{K}\] \[\geq\ \min_{a\in[K]}\widetilde{N}_{a}(N-2)+\frac{t}{K}\ \overset{(2)}{\geq}\ (N-1)^{ \alpha}+\frac{t}{K},\]

where (1) follows from the fact that, \(\min_{a\in[N]}\widetilde{N}_{a}(\cdot)\) increments by atleast \(1\) over every \(K\) consecutive iterations in an epoch of exploration, and (2) from the fact that iteration \(N-1\) is not an exploration. From the above inequality, we have,

\[\frac{t}{K}\ \leq\ (N+t)^{\alpha}-(N-1)^{\alpha}.\]Note that, \(N\to(N+t)^{\alpha}-(N-1)^{\alpha}\) is decreasing in \(N\). Hence \(RHS\leq(1+t)^{\alpha}\leq 1+t^{\alpha}\leq 2t^{\alpha}\) (since \(t\geq 1\)). Therefore, we have,

\[\frac{t}{K}\ \leq\ 2t^{\alpha}\quad\mbox{implying}\quad t\ \leq\ (2K)^{1/(1- \alpha)}.\]

Therefore every epoch of exploration is atmost \((2K)^{1/(1-\alpha)}\) iterations long.

**Statement 2:** In the following discussion we use \([i:j]\) for a pair of integers \(i<j\) to denote the set \(\{i,i+1,i+2,\ldots,j\}\). We consider only those iterations where \(\min_{a\in[K]}\widetilde{N}_{a}(N-1)<N^{\alpha}\). By statement 1, if we consider \(N_{1}\leq N\leq N_{2}\) such that, \(N_{1},\ N_{1}+1,\ \ldots,\ N_{2}-1,\ N_{2}\) are all explorations and \(N_{1}-1,N_{2}+1\) are not explorations, then we must have, \(N_{2}-N_{1}\leq T_{\mbox{\scriptsize epoch}}\). As a result, we have,

\[N^{\alpha}-\min_{a\in[K]}\widetilde{N}_{a}(N)\ \leq\ \max_{N\in[N_{1}:N_{2}]}(N^{ \alpha}-\min_{a\in[K]}\widetilde{N}_{a}(N)).\]

Now, the slowest rate at which \(\min_{a\in[K]}\widetilde{N}_{a}(N)\) can grow while iterations \(N\in\{N_{1},\ N_{1}+1,\ \ldots,\ N_{2}-1,\ N_{2}\}\) is if the algorithm pulls the \(K\) arms consecutively in those iterations. Therefore, we have,

\[\min_{a\in[K]}\widetilde{N}_{a}(N)\ \geq\ \min_{a\in[K]}\widetilde{N}_{a}(N_{1}- 1)+\frac{N-N_{1}+1}{K}\quad\mbox{for every $N\in[N_{1}:N_{2}]$}.\]

Using this, we have,

\[N^{\alpha}-\min_{a\in[K]}\widetilde{N}_{a}(N)\ \leq\ \max_{N\in[N_{1}:N_{2}]} \left(N^{\alpha}-\min_{a\in[K]}\widetilde{N}_{a}(N_{1}-1)-\frac{N-N_{1}+1}{K} \right).\]

Since iteration \(N_{1}-1\) is not an exploration, we have

\[\min_{a\in[K]}\widetilde{N_{a}}(N_{1}-1)\ \geq\ \min_{a\in[K]}\widetilde{N}_{a}(N_{1}- 2)\ \geq\ (N_{1}-1)^{\alpha}.\]

Using this, the upper bound becomes,

\[\max_{N\in[N_{1}:N_{2}]}\left(N^{\alpha}-(N_{1}-1)^{\alpha}-\frac {N-N_{1}+1}{K}\right) \leq\ \max_{N\in[N_{1}:N_{2}]}\left((N-N_{1}+1)^{\alpha}-\frac{N-N_{1}+1}{K}\right)\] \[\leq\ \max_{z\in[0,T_{\mbox{\scriptsize epoch}}]}\left((z+1)^{ \alpha}-\frac{1+z}{K}\right)=C,\]

where \(C\) depends only on \(K\) and \(\alpha\).

Hence we get,

\[\min_{a\in[K]}\widetilde{N}_{a}(N)\ \geq\ N^{\alpha}-C\ =\ \Omega(N^{\alpha}).\]

**Statement 3:** If the epoch of exploration starts from \(T\) and continues for more than \(K\) iterations, note that \(\min_{a\in[K]}\widetilde{N}_{a}(\cdot)\) gets incremented by atleast \(1\) during the iterations \(T,\ T+1,\ \ldots,\ T+K\). As a result, we have,

\[\min_{a\in[K]}\widetilde{N}_{a}(T+K-1)-\min_{a\in[K]}\widetilde{N}_{a}(T-1)\ \geq\ 1.\]

Since iteration \(T-1\) is not an exploration, we have \(\widetilde{N}_{a}(T-1)\geq\widetilde{N}_{a}(T-2)\geq(T-1)^{\alpha}\). Similarly, since iteration \(T+K\) is an exploration, \(\min_{a\in[K]}\widetilde{N}_{a}(T+K-1)<(T+K)^{\alpha}\). Using these two observations, we have,

\[1\ \leq\ (T+K)^{\alpha}-(T-1)^{\alpha}\ \leq\ (T-1)^{\alpha}\times\left( \left(1+\frac{K+1}{T-1}\right)^{\alpha}-1\right)\ \leq\ \alpha(K+1)(T-1)^{-(1-\alpha)}.\]Therefore,

\[T\;\leq\;(\alpha(K+1))^{1/(1-\alpha)}+1.\]

Let \(M_{\text{explo}}=(\alpha(K+1))^{1/(1-\alpha)}+2\). Then from the above argument, if \(T\geq M_{\text{explo}}\), the epoch of exploration starting at \(T\) will last for at most \(K\) iterations.

Let us assume that, the epoch begining from some \(T_{i}\geq M_{\text{explo}}\) lasts till iteration \(T_{f}\). Therefore, we have,

\[\min_{a\in[K]}\widetilde{N}_{a}(T_{f}-1)\;<\;T_{f}^{\alpha}\quad\text{and} \quad\min_{a\in[K]}\widetilde{N}_{a}(T_{i}-2)\;\geq\;(T_{i}-1)^{\alpha}.\]

Using this,

\[\min_{a\in[K]}\widetilde{N}_{a}(T_{f}-1)-\min_{a\in[K]}\widetilde{N}_{a}(T_{i }-2)\;\leq\;T_{f}^{\alpha}-(T_{i}-1)^{\alpha}\;\leq\;\alpha(T_{i}-1)^{-(1- \alpha)}(T_{f}-T_{i}+1).\]

We argued earlier that \(T_{f}-T_{i}\leq K\). We also have, \(T_{i}-1\geq M_{\text{explo}}-1\geq(\alpha(K+1))^{1/(1-\alpha)}+1\). Using this observations, we get

\[\min_{a\in[K]}\widetilde{N}_{a}(T_{f}-1)-\min_{a\in[K]}\widetilde{N}_{a}(T_{i }-2)\;\leq\;\frac{\alpha(K+1)}{((\alpha(K+1))^{1/(1-\alpha)}+1)^{1-\alpha}}\; <\;1.\]

Since \(\min_{a\in[K]}\widetilde{N}_{a}(T_{f})\leq\min_{a\in[K]}\widetilde{N}_{a}(T_{f }-1)+1\) and \(\min_{a\in[K]}\widetilde{N}_{a}(T_{i}-2)\leq\min_{a\in[K]}\widetilde{N}_{a}( T_{i}-1)\), we obtain,

\[\min_{a\in[K]}\widetilde{N}_{a}(T_{f})-\min_{a\in[K]}\widetilde{N}_{a}(T_{i}-1 )\;<\;2,\]

implying \(\min_{a\in[K]}\widetilde{N}_{a}(T_{f})-\min_{a\in[K]}\widetilde{N}_{a}(T_{i}- 1)\leq 1\), which is possible only if every arm is pulled at most once in iterations \(T_{i},\;T_{i}+1,\;\ldots,\;T_{f}-1,\;T_{f}\).

**Statement 4:** Let an epoch of exploration starts from \(N\geq M_{\text{explo}}\) and the next epoch starts from \(N+T\) for some \(T\geq 1\). The epoch starting from \(N\) continues till atmost \(\min\{N+K,\;N+T-2\}\) by statement 3. Moreover in that epoch, every arm gets pulled atmost once and \(\min_{a\in[K]}\widetilde{N}_{a}(\cdot)\) increments by \(1\). Therefore,

\[\min_{a\in[K]}\widetilde{N}_{a}(N+T-1)-\min_{a\in[K]}\widetilde{N}_{a}(N-1)\; \geq\;1.\]

Since iteration \(N-1\) is not an exploration, we have, \(\min_{a\in[K]}\widetilde{N}_{a}(N-1)\geq\min_{a\in[K]}\widetilde{N}_{a}(N-2) \geq(N-1)^{\alpha}\). Since iteration \(N+T\) is an exploration, we have \(\min_{a\in[K]}\widetilde{N}_{a}(N+T-1)<(N+T)^{\alpha}\). Using these in the above inequality, we obtain,

\[1\;\leq\;(N+T)^{\alpha}-(N-1)^{\alpha}\;\leq\;\alpha(N-1)^{-(1-\alpha)}(T+1),\]

which implies \(T\geq\frac{1}{\alpha}(N-1)^{1-\alpha}-1=\Theta(N^{1-\alpha})\).

**Statement 5:** Using statement 4, we can have a constant \(C_{1}\) such that,

\[N_{k}\;\geq\;N_{k-1}+C_{1}N_{k-1}^{1-\alpha},\]

for \(k\geq 1\), where \(N_{0}=1\). We now inductively argue that, there exists some constant \(C_{2}\) independent of \(k\) and \(N\) such that, \(N_{k}\geq N+C_{2}k^{1/\alpha}\) for \(k\geq 1\).

* For \(k=1\), we choose \(C_{2}\leq 1\).
* Now for some \(k\geq 2\), if \(N_{k-1}\geq N+C_{2}(k-1)^{1/\alpha}\), we have, \[N_{k} \geq\;N_{k-1}+C_{1}N_{k-1}^{1-\alpha}\;\geq\;N+C_{2}(k-1)^{1/ \alpha}+C_{1}(N+C_{2}(k-1)^{1/\alpha})^{1-\alpha}\] \[\geq\;N+C_{2}k^{1/\alpha}+\left(C_{1}(N+C_{2}(k-1)^{1/\alpha})^{1 -\alpha}-C_{2}(k^{1/\alpha}-(k-1)^{1/\alpha})\right)\] \[\geq\;N+C_{2}k^{1/\alpha}+\left(C_{1}(N+C_{2}(k-1)^{1/\alpha})^{1 -\alpha}-\frac{C_{2}}{\alpha}k^{\frac{1}{\alpha}-1}\right).\]Upon choosing \(C_{2}\leq\left(\frac{\alpha C_{1}}{2^{\frac{1}{\alpha}-1}}\right)^{1/\alpha}\), since we already have \(k\geq 2\), we obtain

\[C_{1}(N+C_{2}(k-1)^{1/\alpha})^{1-\alpha}\ \geq\ C_{1}\left(C_{2}\left(\frac{k}{2} \right)^{1/\alpha}\right)^{1-\alpha} =\ \left(C_{2}^{-\alpha}\frac{\alpha C_{1}}{2^{\frac{1}{\alpha}-1}} \right)\frac{C_{2}}{\alpha}k^{1/\alpha}\] \[\geq\ \frac{C_{2}}{\alpha}k^{1/\alpha}.\]

Therefore, \(N_{k}\geq N+C_{2}k^{\frac{1}{\alpha}}\).

Therefore, choosing \(C_{2}=\min\left\{1,\left(\frac{\alpha C_{1}}{2^{\frac{1}{\alpha}-1}}\right)^{ \frac{1}{\alpha}}\right\}\), we have \(N_{k}\geq N+C_{2}k^{\frac{1}{\alpha}}\) for all \(k\geq 1\).

**Statement 6:** If \(N\geq M_{\text{explo}}+T_{\text{epoch}}+1\) and \(T\geq 1\), every epoch of explorations intersecting with the iterations \(\{N,\ N+1,\ \ldots,\ N+T-1,\ N+T\}\) has length atmost \(K\) (by statement 3). Hence, every such epoch must have started on or after iteration \(N-K\). Let \(N_{0}\) be the time when the first such epoch has started and the sequence \((N_{k})_{k\geq 1}\) be defined similar to statement 5. Then \(N_{0}\geq N-K\) and \(N_{k}\geq N_{0}+C_{2}k^{\frac{1}{\alpha}}\geq N-K+C_{2}k^{\frac{1}{\alpha}}\). Now, if the \(k\)-th epoch starting after \(N_{0}\) intersects with the iterations \(\{N,\ N+1,\ \ldots,\ N+T-1,\ N+T\}\), then,

\[N+T\ \geq\ N-K+C_{2}k^{\frac{1}{\alpha}},\quad\text{which implies},\quad k\ \leq\ \frac{(T+K)^{\alpha}}{C_{2}^{\alpha}}\ =\ O(T^{\alpha}).\]

**Definition G.7**.: _We define the constant \(T_{\text{explo}}=M_{\text{explo}}+T_{\text{epoch}}+1\), where the constants \(M_{\text{explo}}\), and \(T_{\text{epoch}}\) are defined in Proposition G.5._

**Lemma G.8**.: _For \(N\geq T_{\text{explo}}\) and \(T\geq 1\), the no of times an arm \(a\in[K]\) is pulled for exploration by the algorithm during iterations \(N,\ N+1,\ \ldots,\ N+T-1,\ N+T\) is \(O(T^{\alpha})\)._

Proof.: By statement 3 of Proposition G.5, for \(N\geq T_{\text{explo}}\) and \(T\geq 1\), every epoch of exploration intersecting with the set of iterations \(N,\ N+1,\ N+2,\ \ldots,\ N+T\) is of length atmost \(K\). In every such epoch, every arm is pulled almost once. As a result, no. of times an arm is pulled during the iterations \(N,\ N+1,\ \ldots,\ N+T\) is upper bounded by the no. of epoch of iterations intersecting with the set \(\{N,\ N+1,\ \ldots,\ N+T-1,\ N+T\}\). The later quantity is \(O(T^{\alpha})\) by statement 6 of Proposition G.5. Hence the lemma stands proved. 

### Technical lemmas related to algorithmic allocations

In this appendix we prove several properties about the anchor function (\(g\)) and the algorithmic allocations \(\widetilde{\bm{N}}(N)=(\widetilde{N}_{a}(N):a\in[K])\). We exploit the results proven in Appendix B and G.4 to prove that the following properties hold after a random time of finite expectation,

* if the algorithm has \(g\neq 0\) at some iteration \(N\), then \(g\) crosses the value zero withing an \(O(N)\) iterations, where the constant hidden in \(O(\cdot)\) is independent of the sample paths (in Lemma G.9), and
* every arm \(a\in[K]\) has \(\widetilde{N}_{a}(N)=\varTheta(N)\) samples (in Lemma G.13).

Each of the properties stated above are used extensively in the proofs of Proposition G.1 and G.2.

**Definition G.8**.: We define the random time \(T_{1}=\max\{T_{0},T_{\text{explo}}\}\) (where \(T_{\text{explo}}\) and \(T_{0}\) are defined, respectively, in Appendix G.4 and G.3).

Note that \(\mathbb{E}_{\bm{\mu}}[T_{1}]\leq\mathbb{E}_{\bm{\mu}}[T_{0}]+T_{\text{explo}}<\infty\).

We can have \(g\) far from the value zero at iteration \(T_{1}\). \(g\) will still be finite at \(T_{1}\) because of exploration. Lemma G.9 bounds the no. of iterations the algorithm takes to reach the value zero.

**Lemma G.9** (**Upper bound to the time to reach \(g=0\)**).: _If \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\neq 0\) at iteration \(N\geq T_{1}\), and_

\[U=\min\Big{\{}\ t>0\ \left|\ g\left(\widetilde{\bm{\mu}}(N+t),\widetilde{\bm{N}} (N+t)\right)\quad\text{and}\quad g\left(\widetilde{\bm{\mu}}(N),\widetilde{\bm{ N}}(N)\right)\quad\text{have opposite signs}\ \Big{\}}\right.,\]

_then there exists a constant \(C_{1}>0\) independent of the sample paths such that \(U\leq C_{1}N\)._

Proof.: Using (10), for \(N\geq T_{1}\), we have,

\[g\left(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N)\right)\ =\ \Theta\left(\sum_{a\neq 1} \frac{\widetilde{N}_{a}(N)^{2}}{\widetilde{N}_{1}(N)^{2}}\right)-1 =\ \Theta\left(\left(\sum_{a\neq 1}\frac{\widetilde{N}_{a}(N)}{ \widetilde{N}_{1}(N)}\right)^{2}\right)-1\] \[=\ \Theta\left(\left(\frac{N}{\widetilde{N}_{1}(N)}-1\right)^{2} \right)-1.\] (72)

We now consider two situations separately,

**Case I: \(g\left(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N)\right)>0\)**: We know that, there is a constant \(D_{1}>0\), such that,

\[g\left(\widetilde{\bm{\mu}}(N+t),\widetilde{\bm{N}}(N+t)\right)\ \leq\ D_{1}\left(\frac{N+t}{\widetilde{N}_{1}(N+t)}-1\right)^{2}-1,\]

for every \(t\geq 1\). Now for \(t<U\), the algorithm selects an alternative arm from \([K]/\{1\}\) only while exploring. Therefore, using statement 6 of Proposition G.5, we have a constant \(c_{1}>0\) such that, \(\widetilde{N}_{1}(N+t)\geq t-c_{1}t^{\alpha}\). Hence,

\[g\left(\widetilde{\bm{\mu}}(N+t),\widetilde{\bm{N}}(N+t)\right)\ \leq\ D_{1}\left(\frac{N+t}{t-c_{1}t^{\alpha}}-1\right)^{2}-1\ =\ D_{1}\left(\frac{N+c_{1}t^{\alpha}}{t-c_{1}t^{\alpha}}\right)^{2}-1\quad \text{for $t<U$}.\]

Since \(g\left(\widetilde{\bm{\mu}}(N+U-1),\widetilde{\bm{N}}(N+U-1)\right)\geq 0\), RHS of the above inequality is non-negative at \(t=U-1\). After some algebraic manipulation, this implies,

\[U-1-c_{1}(1+D_{1}^{1/2})(U-1)^{\alpha}\ \leq\ D_{1}^{1/2}N,\]

Since LHS of the above inequality is linear in \(U\), we can find a constant \(C_{11}\) such that \(U\leq C_{11}N\).

**Case II: \(g\left(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N)\right)<0\)**: Using (72), we have a constant \(D_{2}\) such that,

\[g\left(\widetilde{\bm{\mu}}(N+t),\widetilde{\bm{N}}(N+t)\right)\ \geq\ D_{2}\left(\frac{N+t}{ \widetilde{N}_{1}(N+t)}-1\right)^{2}-1\]

for all \(t\geq 1\). Now for \(t<U\), the algorithm pulls arm \(1\) only for exploration. As a result, using statement 6 of Proposition G.5, we have, a constant \(c_{1}>0\) such that \(\widetilde{N}_{1}(N+t)\leq N+c_{1}t^{\alpha}\). Using this,

\[g\left(\widetilde{\bm{\mu}}(N+t),\widetilde{\bm{N}}(N+t)\right)\ \geq\ D_{2}\left(\frac{N+t}{N+c_{1}t^{\alpha}}-1\right)^{2}-1\ \geq\ D_{2}\left(\frac{t-c_{1}t^{\alpha}}{N+c_{1}t^{\alpha}}\right)^{2}-1,\]

for \(t<U\). Since \(g(\widetilde{\bm{\mu}}(N+U-1),\widetilde{\bm{N}}(N+U-1))\leq 0\), the RHS of the above inequality is not positive at \(t=U-1\). After some algebraic manipulation, this implies,

\[U-1-c_{1}(1+D_{2}^{-1/2})(U-1)^{\alpha}\leq D_{2}^{-1/2}N.\]

Again the LHS of the above inequality is linear in \(U\). As a result, we can find a constant \(C_{12}\) such that, \(U\leq C_{12}N\).

We take \(C_{1}=\max\{C_{11},C_{12}\}\) and have \(U\leq C_{1}N\)Lemma G.10, G.11, and G.12 are necessary for proving Lemma G.13, which says \(\widetilde{N}_{a}(N)=\varTheta(N)\) for every arm \(a\in[K]\), after a random time of finite expectation. This property is essential for proving Proposition G.1 and G.2 stated earlier.

**Lemma G.10**.: _There exists constants \(\gamma_{1},\gamma_{2}\in(0,1)\) independent of the sample path, such that, for \(N\geq T_{1}\), whenever \(g\left(\widetilde{\boldsymbol{\mu}}(N),\widetilde{\boldsymbol{N}}(N)\right)\) and \(g\left(\widetilde{\boldsymbol{\mu}}(N+1),\widetilde{\boldsymbol{N}}(N+1)\right)\) have opposite signs, we have,_

1. \(\widetilde{N}_{1}(N)\geq\gamma_{1}N\)_, and_
2. \(\max_{a\in[K]/\{1\}}\widetilde{N}_{a}(N)\geq\gamma_{2}N\)_._

Proof.: For \(N\geq T_{1}\), we have constants \(D_{1},D_{2}>0\) such that,

\[D_{1}\left(\frac{N}{\widetilde{N}_{1}(N)}-1\right)^{2}-1\;\leq\;g\left( \widetilde{\boldsymbol{\mu}}(N),\widetilde{\boldsymbol{N}}(N)\right)\;\leq \;D_{2}\left(\frac{N}{\widetilde{N}_{1}(N)}-1\right)^{2}-1\]

We consider only the situation where \(g\left(\widetilde{\boldsymbol{\mu}}(N),\widetilde{\boldsymbol{N}}(N)\right)\geq 0\) and \(g\left(\widetilde{\boldsymbol{\mu}}(N+1),\widetilde{\boldsymbol{N}}(N+1) \right)\leq 0\). Extending this to the other case follows similar argument.

From \(g(\widetilde{\boldsymbol{\mu}}(N+1),\widetilde{\boldsymbol{N}}(N+1))\leq 0\), we have,

\[D_{1}\left(\frac{N+1}{\widetilde{N}_{1}(N+1)}-1\right)^{2}-1 \leq 0,\text{ which implies}\] \[\widetilde{N}_{1}(N+1) \geq(1+D_{1}^{-1/2})^{-1}N.\] (73)

Since \(\widetilde{N}_{1}(N)\geq 1\) (for exploration), we have \(\widetilde{N}_{1}(N+1)\leq\widetilde{N}_{1}(N)+1\leq 2\widetilde{N}_{1}(N)\). Therefore, using (73) we get \(\widetilde{N}_{1}(N)\geq\frac{1}{2}(1+D_{1}^{-1/2})^{-1}N\) and we can take \(\gamma_{1}=\frac{1}{2}(1+D_{1}^{-1/2})^{-1}\).

Similarly from, \(g(\widetilde{\boldsymbol{\mu}}(N),\widetilde{\boldsymbol{N}}(N))\geq 0\), we have,

\[D_{2}\left(\frac{N}{\widetilde{N}_{1}(N)}-1\right)^{2}-1\geq 0,\text{ which implies}\]

\[\widetilde{N}_{1}(N)\leq(1+D_{2}^{-1/2})^{-1}N.\] (74)

Using (74), we have, \(\sum_{a\in[K]/\{1\}}\widetilde{N}_{a}(N)\geq(1+D_{2}^{1/2})^{-1}N\). This further implies

\[\max_{a\in[K]/\{1\}}\widetilde{N}_{a}(N)\geq\frac{1}{K-1}(1+D_{2}^{1/2})^{-1}N.\]

Hence we can take \(\gamma_{2}=\frac{1}{K-1}(1+D_{2}^{1/2})^{-1}\). 

**Lemma G.11**.: _There exists constants \(M_{1}\geq 1\), \(d_{max}\in[1,\infty)\) and \(d_{min}\in(0,1]\) independent of the sample path, such that, if \(T_{2}\) is the time at which \(g\) crosses zero after the iteration \(\max\{M_{1},T_{1}\}\), then, for \(N>T_{2}\), we have:_

\[d_{min}\leq\sum_{a\neq 1}\frac{d(\widetilde{\mu}_{1}(N),\widetilde{x}_{1,a}(N) )}{d(\widetilde{\mu}_{a}(N),\widetilde{x}_{1,a}(N))}\leq d_{max}.\]

_Moreover, we have \(\mathbb{E}_{\boldsymbol{\mu}}[T_{2}]<\infty\)._

Proof.: Let,

\[D(\widetilde{\boldsymbol{\mu}}(N),\widetilde{\boldsymbol{N}}(N))=\sum_{a\neq 1 }\frac{d(\widetilde{\mu}_{1}(N),\widetilde{x}_{1,a}(N))}{d(\widetilde{\mu}_{a}( N),\widetilde{x}_{1,a}(N))}.\]Also for every \(M\geq 1\), we define \(T_{1,M}=\max\{M,T_{1}\}\) and \(T_{2,M}\) as the iteration at which \(g\) crosses zero for the first time after the iteration \(T_{1,M}\), _i.e._,

\[T_{2,M}\stackrel{{\text{def.}}}{{=}}\min\Big{\{}N \geq T_{1,M}\ \Big{|}\ g(\widetilde{\bm{\mu}}(N+1),\widetilde{\bm{N}}(N+1))\text{ and}\] \[\ g(\widetilde{\bm{\mu}}(T_{1,M}),\widetilde{\bm{N}}(T_{1,M})) \text{ are of opposite signs }\Big{\}}.\]

**Existence of \(d_{\max}\):** If \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\leq 0\), then \(D(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\) is bounded above trivially by \(1\). Otherwise if \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))>0\) for some \(N>T_{2,M}\), let \(T=\max\{\,t\leq N\ \mid\ g(\widetilde{\bm{\mu}}(t),\widetilde{\bm{N}}(t))\leq 0\ \}\)_i.e._ the time before iteration \(N\) when \(g\) has crossed the level zero. As a result, \(T\geq T_{2,M}\) (by definition of \(T_{2,M}\)), and by Lemma G.10, \(\widetilde{N}_{1}(T)\geq\gamma_{1}T\) for some \(\gamma_{1}\in(0,1)\). Let \(S=N-T\). During iterations \(T+1,\ldots,T+S\), since \(g\) is positive, the algorithm pulls alternative arms in \([K]/\{1\}\) only for exploration. As a result, using statement 6 of Proposition G.5, we have

\[\widetilde{N}_{1}(T+S)\geq\widetilde{N}_{1}(T)+S-c_{1}S^{\alpha}\geq\gamma_{1} T+S-c_{1}S^{\alpha}\]

for some \(c_{1}>0\). Using (72), we have a constant \(D_{2}>0\) such that,

\[D(\widetilde{\bm{\mu}}(T+S),\widetilde{\bm{N}}(T+S))\leq D_{2}\left(\frac{T+S} {\gamma_{1}T+S-c_{1}S^{\alpha}}-1\right)^{2}=D_{2}\left(\frac{(1-\gamma_{1})T+ c_{1}S^{\alpha}}{\gamma_{1}T+S-c_{1}S^{\alpha}}\right)^{2}.\]

We take \(M_{11}=-\frac{2}{\gamma_{1}}\min_{S\geq 0}(S-c_{1}S^{\alpha})\). It is easy to observe that for \(T\geq M_{11}\) and \(S\geq 0\), the function of \(T,S\) in the RHS is bounded above. Therefore, we take,

\[d_{\max}=\max\left\{1,\max_{T\geq M_{11},S\geq 0}D_{2}\left(\frac{(1-\gamma_{1}) T+c_{1}S^{\alpha}}{\gamma_{1}T+S-c_{1}S^{\alpha}}\right)^{2}\right\}<\infty.\]

Hence, if \(M\geq M_{11}\), we have \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\leq d_{\max}\) for every \(N>T_{2,M}\).

**Existence of \(d_{\min}\):** If \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\geq 0\), we have \(1\) has the trivial lower bound. Otherwise, if \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))<0\), we define \(T=\max\{\,t\leq N\ \mid\ g(\widetilde{\bm{\mu}}(t),\widetilde{\bm{N}}(t))\geq 0\ \}\) and \(S=N-T\). As a result, \(T\geq T_{2,M}\) (by definition of \(T_{2,M}\)). By Lemma G.10, we have \(\widetilde{N}_{1}(T)\leq(1-\gamma_{2})N\) for some \(\gamma_{2}\in(0,1)\). Also, the algorithm pulls the first arm only for exploration during the iterations \(T+1,T+2,\ldots,T+S\), since \(g\) is negative. Therefore, using statement 6 of Proposition G.5, we have, \(\widetilde{N}_{1}(T+S)\leq\widetilde{N}_{1}(T)+c_{1}S^{\alpha}\leq(1-\gamma_{ 2})T+c_{1}S^{\alpha}\). By (72), we have \(D_{1}>0\) such that,

\[D(\widetilde{\bm{\mu}}(T+S),\widetilde{\bm{N}}(T+S))\geq D_{1}\left(\frac{T+S} {(1-\gamma_{2})T+c_{1}S^{\alpha}}-1\right)^{2}=D_{1}\left(\frac{\gamma_{2}T+S-c _{1}S^{\alpha}}{(1-\gamma_{2})T+c_{1}S^{\alpha}}\right)^{2}.\]

Let \(M_{12}=-\frac{2}{\gamma_{2}^{2}}\min_{S\geq 0}(S-c_{1}S^{\alpha})\). The function of \(T,S\) in the RHS is bounded below by a positive constant for \(T\geq M_{12}\) and \(S\geq 0\). Let,

\[d_{\min}=\min\left\{1,\min_{T\geq M_{12},\ S\geq 0}D_{1}\left(\frac{\gamma_{2}T+S-c _{1}S^{\alpha}}{(1-\gamma_{2})T+c_{1}S^{\alpha}}\right)^{2}\right\}>0.\]

Then for \(M\geq M_{12}\), we have \(g(\widetilde{\bm{\mu}}(N),\widetilde{\bm{N}}(N))\geq d_{\min}>0\) for every \(N>T_{2,M}\).

Now, upon taking \(M_{1}=\max\{M_{11},M_{12}\}\), and \(T_{2}=T_{2,M_{1}}\), the lemma follows. By Lemma G.9, we have a constant \(C_{1}>0\) such that, \(T_{2}\leq C_{1}\max\{M_{1},T_{1}\}\). As a result, \(\mathbb{E}_{\bm{\mu}}[T_{2}]\leq C_{1}(M_{1}+\mathbb{E}_{\bm{\mu}}[T_{1}])<\infty\). 

Using a technique similar to the one used in the proof of Lemma G.10 using (72), Lemma G.11 implies the following corollary. We define the random time \(T_{2}\) as the one introduced in Lemma G.11.

**Corollary G.1**.: _There exists constants \(\beta_{1},\beta_{2}\in(0,1)\) independent of the sample paths, such that, for every \(N>T_{2}\)._1. \(\widetilde{N}_{1}(N)\geq\beta_{1}N\)_, and_
2. \(\max_{a\in[K]/\{1\}}\widetilde{N}_{a}(N)\geq\beta_{2}N\)_._

**Lemma G.12**.: _There exists constants \(\gamma>0\) and \(M_{2}\geq 1\) independent of the sample path, such that, for \(N>\max\{M_{2},T_{2}\}+1\), if the algorithm pulls some arm \(a\in[K]/\{1\}\), then for every alternate arm \(b\in[K]/\{1,a\}\), \(\widetilde{N}_{b}(N)\geq\gamma\widetilde{N}_{a}(N)\)._

Proof.: We consider two separate cases.

**Case I: Iteration \(N\) is an exploration:** Then we have a constant \(C_{1}\) (introduced in statement 2 of Proposition G.5) such that,

\[\widetilde{N}_{b}(N)\geq N^{\alpha}-C_{1}\geq\widetilde{N}_{a}(N-1)-C_{1}= \widetilde{N}_{a}(N)-1-C_{1}.\] (75)

We also have \(\widetilde{N}_{a}(N)\geq N^{\alpha}-C_{1}\). Now let \(M_{2}\geq 2\) be chosen such that, \(M_{2}^{\alpha}>3C_{1}+2\). As a result, for \(N\geq\max\{M_{2},T_{2}\}\) we have, \(\widetilde{N}_{a}(N)\geq N^{\alpha}-C_{1}\geq 2(1+C_{1})\). This together with (75) implies, \(\widetilde{N}_{b}(N)\geq\frac{1}{2}\widetilde{N}_{a}(N)\).

**Case II: Iteration \(N\) is not an exploration:** We consider the AT2 (1) and IAT2 (2) algorithms separately.

**Case II: AT2 Algorithm:** If iteration \(N\) is not an exploration, we have, \(\mathcal{I}_{a}(N-1)\leq\mathcal{I}_{b}(N-1)\). Using (11), for \(N\geq\max\{M_{2},T_{2}\}\), we have constants \(C_{2}\) and \(C_{3}\) such that,

\[C_{2}\frac{\widetilde{N}_{1}(N-1)\cdot\widetilde{N}_{a}(N-1)}{ \widetilde{N}_{1}(N-1)+\widetilde{N}_{a}(N-1)} \leq\mathcal{I}_{a}(N-1)\] \[\leq\mathcal{I}_{b}(N-1) \leq C_{3}\frac{\widetilde{N}_{1}(N-1)\cdot\widetilde{N}_{b}(N-1) }{\widetilde{N}_{1}(N-1)+\widetilde{N}_{b}(N-1)}.\]

As a result, we have,

\[\widetilde{N}_{a}(N-1)\leq\frac{C_{3}}{C_{2}}\frac{\widetilde{N}_{1}(N-1)+ \widetilde{N}_{a}(N-1)}{\widetilde{N}_{1}(N-1)+\widetilde{N}_{b}(N-1)} \widetilde{N}_{b}(N-1)\overset{(1)}{\leq}\frac{2C_{3}}{\beta_{1}C_{2}} \widetilde{N}_{b}(N-1)\leq\frac{2C_{3}}{\beta_{1}C_{2}}\widetilde{N}_{b}(N),\]

where, for (1), we first note that \(N>T_{2}+1\). As a result, using Corollary (G.1), \(\widetilde{N}_{1}(N-1)\geq\beta_{1}(N-1)\quad(\beta_{1}\in(0,1)\) is the constant introduced in Corollary G.1), and on the other hand \(\widetilde{N}_{1}(N-1)+\widetilde{N}_{b}(N-1)\leq 2(N-1)\). Hence, we get

\[\widetilde{N}_{b}(N)\geq\frac{\beta_{1}C_{2}}{2C_{3}}\widetilde{N}_{a}(N-1)= \frac{\beta_{1}C_{2}}{2C_{3}}(\widetilde{N}_{a}(N)-1).\]

Again using statement 2 of Proposition G.5, \(\widetilde{N}_{a}(N)\geq N^{\alpha}-C_{1}\geq M_{2}^{\alpha}-C_{1}\geq 2\) (since \(M_{2}^{\alpha}\geq 3C_{1}+2\)). As a result,

\[\widetilde{N}_{b}(N)\geq\frac{\beta_{1}C_{2}}{4C_{3}}\widetilde{N}_{a}(N).\]

Taking \(\gamma=\min\left\{\frac{1}{2},\frac{\beta_{1}C_{2}}{4C_{3}}\right\}\), we have the desired result for AT2 algorithm.

**Case II.II IAT2 Algorithm:** In this case, we have,

\[\log(\widetilde{N}_{a}(N-1))+\mathcal{I}_{a}(N-1)\ \leq\ \log(\widetilde{N}_{b}(N-1))+ \mathcal{I}_{b}(N-1).\]

Using (11), we have constants \(C_{2}\) and \(C_{3}\) such that,

\[C_{2}\frac{\widetilde{N}_{1}(N-1)\cdot\widetilde{N}_{a}(N-1)}{ \widetilde{N}_{1}(N-1)+\widetilde{N}_{a}(N-1)} \leq\log(\widetilde{N}_{a}(N-1))+\mathcal{I}_{a}(N-1)\] \[\leq\mathcal{I}_{b}(N-1)+\log(\widetilde{N}_{b}(N-1))\leq C_{3} \frac{\widetilde{N}_{1}(N-1)\cdot\widetilde{N}_{b}(N-1)}{\widetilde{N}_{1}(N-1) +\widetilde{N}_{b}(N-1)}+\frac{1}{e}\widetilde{N}_{b}(N-1).\]As a result, using the same arguments used for the AT2 algorithm in Case II.I, we can conclude,

\[\widetilde{N}_{b}(N)\geq\frac{\beta_{1}}{4}\left(\frac{C_{3}}{C_{2}}+\frac{1}{e} \right)^{-1}\widetilde{N}_{a}(N),\]

for \(N>\max\{M_{2},T_{2}\}+1\).

Now taking \(\gamma=\min\left\{\frac{1}{2},\frac{\beta_{1}}{4}\left(\frac{C_{2}}{C_{3}}+ \frac{1}{e}\right)^{-1}\right\}\), we have the desired result for IAT2 algorithm. 

**Definition G.9**.: We define the random time \(T_{3}=\max\{M_{2},T_{2}\}+2\), where \(M_{2}\) is the constant introduced in Lemma G.12, and \(T_{2}\) is the random time defined in Lemma G.11.

**Definition G.10**.: We define the random time \(T_{4}\) as \(T_{4}=\frac{1}{\beta_{2}}(T_{3}+1)\), where \(\beta_{2}\in(0,1)\) is the constant introduced in Corollary G.1.

Note that \(\mathbb{E}_{\boldsymbol{\mu}}[T_{3}]<\infty\), since \(\mathbb{E}_{\boldsymbol{\mu}}[T_{2}]<\infty\). For the same reason, by Definition G.10, \(T_{4}\) satisfies \(\mathbb{E}_{\boldsymbol{\mu}}[T_{4}]<\infty\) since \(\mathbb{E}_{\boldsymbol{\mu}}[T_{3}]<\infty\).

**Lemma G.13** (**Sufficient sampling**).: _For every \(N\geq T_{4}\), we have \(\widetilde{N}_{a}(N)=\varTheta(N)\) for every \(a\in[K]\)._

Proof.: For \(a=1\) and every \(N\geq T_{4}\), we have \(\widetilde{N}_{1}(N)=\varTheta(N)\) by Corollary G.1.

Otherwise for \(a\neq 1\) and every \(N\geq T_{4}\), we define \(A^{\prime}_{N}=\text{arg}\max_{b\in[K]/\{1\}}~{}\widetilde{N}_{b}(N)\). By Corollary G.1,

\[\widetilde{N}_{A^{\prime}_{N}}(N)\;\geq\;\beta_{2}N\;\geq\;\beta_{2}T_{4}\;= \;T_{3}+1.\]

Therefore, arm \(A^{\prime}_{N}\) must have been pulled by the algorithm somewhere between iterations \(T_{3}\) and \(N\). Let us define the time \(N^{\prime}\) to be the last time before \(N\) when \(A^{\prime}_{N}\) was pulled. Then \(N^{\prime}\geq T_{3}>\max\{M_{2},T_{2}\}+1\). As a result, using Lemma G.12, we have,

\[\widetilde{N}_{a}(N^{\prime})\;\geq\;\gamma\widetilde{N}_{A^{\prime}_{N}}(N^{ \prime})\;\stackrel{{\text{(i)}}}{{=}}\;\gamma\widetilde{N}_{A^{ \prime}_{N}}(N),\]

where (i) follows by definition of \(N^{\prime}\). Since \(N\geq N^{\prime}\), we have \(\widetilde{N}_{a}(N)\geq\widetilde{N}_{a}(N^{\prime})\). As a result, we obtain, for every \(a\in[K]/\{1\}\) and \(N\geq T_{4}\),

\[\widetilde{N}_{a}(N)\;\geq\;\gamma\widetilde{N}_{A^{\prime}_{N}}(N)\;=\;\gamma \max_{b\in[K]/\{1\}}\widetilde{N}_{b}(N)\;\geq\;\gamma\beta_{2}N,\]

where \(\beta_{2}\in(0,1)\) is the constant mentioned in Corollary G.1. Hence we have \(\widetilde{N}_{a}(N)=\varOmega(N)\) for \(N\geq T_{4}\) and the lemma follows. 

### Proving Lemma G.2

In this appendix, we prove Lemma G.2 from Appendix G.1.2. For improved clarity, we reiterate Lemma G.2 from Appendix G.1.2.

**Statement of Lemma G.2**.: _For both AT2 and IAT2 algorithms, there exists constants \(M_{5}\geq 1\) and \(C_{1}>0\) independent of the sample paths, such that for every \(N\geq\max\{M_{5},T_{6}\}\), if the algorithm picks an arm \(a\in[K]/\{1\}\) at iteration \(N\), then it again picks arm \(a\) within the next \(\lceil C_{1}N^{1-3\alpha/8}\rceil\) iterations._

For every arm \(a\in[K]\), iteration \(N\geq 1\) and \(R\geq 1\), we define the following quantities,

\[\varDelta\widetilde{N}_{a}(N,R) =~{}\widetilde{N}_{a}(N+R)-\widetilde{N}_{a}(N),\] \[b(N,R) =~{}\text{arg}\max_{j\in[K]/\{1\}}~{}\frac{\varDelta\widetilde{N} _{j}(N,R)}{\widetilde{N}_{j}(N)},\quad\text{and}\] \[\tau(N,R) =~{}\max\{\,t\leq N+R\;\mid A_{t}=b(N,R)\;\}.\]

For proving Lemma G.2, we need the three technical lemmas: Lemma G.14, G.15 and G.16.

**Lemma G.14**.: _For \(N\geq T_{6}\) (\(T_{6}\) is the random time defined in the statement of Proposition G.1 and it satisfies \(\mathbb{E}_{\bm{\mu}}[T_{6}]<\infty\)), we have,_

\[\frac{\Delta\widetilde{N}_{1}(N,R)}{\Delta\widetilde{N}_{b(N,R)}(N,R)}\leq\frac {\widetilde{N}_{1}(N)}{\widetilde{N}_{b(N,R)}(N)}+O\left(RN^{-1}+(\Delta \widetilde{N}_{b(N,R)}(N,R))^{-1}N^{1-3\alpha/8}(1+RN^{-1})^{3}\right).\] (76)

Proof.: In the proof, for readability, we use \(b\) to denote \(b(N,R)\). Also, for every arm \(a\in[K]\), we use \(\Delta\widetilde{N}_{a}\) to denote \(\Delta\widetilde{N}_{a}(N,R)\).

By Proposition G.1, we have a constant \(C>0\) independent of the sample paths, such that for \(N\geq T_{6}\),

\[\Big{|}\ g(\bm{\mu},\widetilde{\bm{N}}(N+R))-g(\bm{\mu},\widetilde{\bm{N}}(N) )\ \Big{|}\ \leq\ 2CN^{-3\alpha/8}.\]

Therefore, applying the mean value theorem, we have,

\[\left|\ \frac{\partial g}{\partial N_{1}}(\bm{\mu},\hat{\bm{N}})\Delta \widetilde{N}_{1}+\sum_{a\in[K]/\{1\}}\frac{\partial g}{\partial N_{a}}(\bm{ \mu},\hat{\bm{N}})\Delta\widetilde{N}_{a}\ \right|\ \leq\ 2CN^{-3\alpha/8},\]

where \(\hat{\bm{N}}=(\hat{N}_{a})_{a\in[K]}\), with \(\hat{N}_{a}\in\left[\widetilde{N}_{a}(N),\widetilde{N}_{a}(N+R)\right]\) for every \(a\in[K]\).

Now expanding the partial derivatives of \(g\) with respect to \(N_{a}\)'s for \(a\in[K]\), we get

\[\left|\Delta\widetilde{N}_{1}\sum_{a\in[K]/\{1\}}f(\bm{\mu},a,\hat{\bm{N}}) \frac{\hat{N}_{a}\Delta_{a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}}-\sum_{a\in[K]/\{1 \}}f(\bm{\mu},a,\hat{\bm{N}})\Delta\widetilde{N}_{a}\frac{\hat{N}_{1}\Delta_{ a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}}\right|\leq 2CN^{-3\alpha/8},\] (77)

where, \(f(\bm{\mu},a,\hat{\bm{N}})\) were defined in (13) of Appendix B.

Letting

\[\hat{p}_{j}=\frac{f(\bm{\mu},j,\hat{\bm{N}})\frac{\hat{N}_{j}\Delta_{j}}{( \hat{N}_{1}+\hat{N}_{j})^{2}}}{\sum_{a\neq 1}f(\bm{\mu},a,\hat{\bm{N}})\frac{ \hat{N}_{a}\Delta_{a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}}}\]

for every arm \(j\in[K]/\{1\}\), we have,

\[\left|\ \frac{\Delta\widetilde{N}_{1}}{\hat{N}_{1}}-\sum_{a\neq 1}\hat{p}_{ a}\frac{\Delta\widetilde{N}_{a}}{\hat{N}_{a}}\ \right|\ \leq\ 2CN^{-3\alpha/8}\times\hat{N}_{1}^{-1}\left(\sum_{a\in[K]/\{1\}}f(\bm{\mu},a,\hat{\bm{N}})\frac{\hat{N}_{a}\Delta_{a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}} \right)^{-1}.\] (78)

We know from (14) of Appendix B that \(f(\bm{\mu},a,\bm{N})=\Theta\left(\frac{\hat{N}_{a}}{\hat{N}_{1}}\left(1+\frac {\hat{N}_{a}}{\hat{N}_{1}}\right)^{2}\right)\). As a result,

\[\hat{N}_{1}^{-1}\left(\sum_{a\neq 1}f(\bm{\mu},a,\hat{\bm{N}})\frac{\hat{N}_{a} \Delta_{a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}}\right)^{-1}=\Theta\left(\frac{\hat{N }_{1}^{2}}{\sum_{a\in[K]/\{1\}}\hat{N}_{a}^{2}}\right)\]

By Lemma G.13, we have \(\hat{N}_{a}\geq\widetilde{N}_{a}=\Theta(N)\) and \(\hat{N}_{1}\leq N+R\) for \(N\geq T_{6}\). As a result, we get,

\[\widetilde{N}_{1}^{-1}\left(\sum_{a\neq 1}f(\bm{\mu},a,\hat{\bm{N}})\frac{\hat{N}_{ a}\Delta_{a}}{(\hat{N}_{1}+\hat{N}_{a})^{2}}\right)^{-1}=\ O\left((1+RN^{-1})^{2}\right).\]

Putting this in (78), we obtain

\[\left|\ \frac{\Delta\widetilde{N}_{1}}{\hat{N}_{1}}-\sum_{a\neq 1}\hat{p}_{ a}\frac{\Delta\widetilde{N}_{a}}{\hat{N}_{a}}\ \right|\ =\ O\left(N^{-3\alpha/8}(1+RN^{-1})^{2}\right),\]which further implies,

\[\frac{\Delta\widetilde{N}_{1}}{\hat{N}_{1}} \leq\ \sum_{a\neq 1}\hat{p}_{a}\frac{\Delta\widetilde{N}_{a}}{\hat{N}_{a}}+O \left(N^{-3\alpha/8}(1+RN^{-1})^{2}\right)\] \[\leq\ \sum_{a\neq 1}\hat{p}_{a}\frac{\Delta\widetilde{N}_{a}}{ \widetilde{N}_{a}}+O\left(N^{-3\alpha/8}(1+RN^{-1})^{2}\right)\quad\text{( since $\widetilde{N}_{a}\geq\hat{N}_{a}$).}\]

Let \(b=\text{arg}\max_{a\neq 1}\ \frac{\Delta\widetilde{N}_{a}}{\hat{N}_{a}}\). The above inequality implies,

\[\frac{\Delta\widetilde{N}_{1}}{\hat{N}_{1}}\ \leq\ \frac{\Delta\widetilde{N}_{b}}{ \widetilde{N}_{b}}+O\left(N^{-3\alpha/8}(1+RN^{-1})^{2}\right).\]

Now multiplying both sides by \(\hat{N}_{1}/\Delta\widetilde{N}_{b}\), we get,

\[\frac{\Delta\widetilde{N}_{1}}{\Delta\widetilde{N}_{b}} \leq\ \frac{\hat{N}_{1}}{\widetilde{N}_{b}}+O\left(\Delta \widetilde{N}_{b}^{-1}\hat{N}_{1}N^{-3\alpha/8}(1+RN^{-1})^{2}\right)\] \[\leq\ \frac{\hat{N}_{1}}{\widetilde{N}_{b}}+O\left(\Delta \widetilde{N}_{b}^{-1}N^{1-3\alpha/8}(1+RN^{-1})^{3}\right)\quad\text{(since $\hat{N}_{1}\leq N+R$).}\]

(76) follows from the above inequality upon observing that, \(\hat{N}_{1}\leq\widetilde{N}_{1}+R\) and \(\widetilde{N}_{b}=\Omega(N)\) (by Lemma G.13). 

**Lemma G.15**.: _There exists constants \(M_{51}\geq 1\), \(D_{1},D_{2}>0\) independent of the sample paths such that, for \(N\geq\max\{M_{51},T_{6}\}\), we have \(\lceil D_{1}N^{1-3\alpha/8}\rceil<N\), and for all \(R\in\{\lceil D_{1}N^{1-3\alpha/8}\rceil,\ \lceil D_{1}N^{1-3\alpha/8} \rceil+1,\ldots,\ N\}\), we have \(\Delta\widetilde{N}_{b(N,R)}(N,R)\geq D_{2}R\)._

Proof.: To simplify notation, we adopt \(b\) to denote \(b(N,R)\) and, for every arm \(j\in[K]\), we use \(\Delta\widetilde{N}_{j}\) to denote \(\Delta\widetilde{N}_{j}(N,R)\).

By (76) of Lemma G.14, there exists a constant \(D_{3}>0\) independent of the sample paths, such that, for all \(N\geq T_{6}\) and \(R\in\{1,2,\ldots,N\}\),

\[\Delta\widetilde{N}_{1} \leq\ \Delta\widetilde{N}_{b}\times\left(\frac{\widetilde{N}_{1}}{ \widetilde{N}_{b}}+D_{3}\left(1+\Delta\widetilde{N}_{b}^{-1}N^{1-3\alpha/8} \right)\right)\quad\text{(we have $RN^{-1}\leq 1$)}\] \[\leq\ \left(\frac{\widetilde{N}_{1}}{\widetilde{N}_{b}}+D_{3} \right)\Delta\widetilde{N}_{b}+D_{3}N^{1-3\alpha/8}\]

Since \(\widetilde{N}_{1}(N)\) and \(\widetilde{N}_{b}(N)\) are both \(\varTheta(N)\) (by Lemma G.13), we have a constant \(D_{4}>0\) such that, \(\frac{\widetilde{N}_{1}(N)}{\widetilde{N}_{b}(N)}+D_{3}\leq D_{4}\) for every \(N\geq T_{6}\). Using this in the above inequality, we get,

\[\Delta\widetilde{N}_{1}\leq D_{4}\Delta\widetilde{N}_{b}+D_{3}N^{1-3\alpha/8}.\] (79)

We take \(D_{1}=2D_{3}\) and \(M_{51}\geq 1\) to be the smallest number such that, every \(N\geq M_{51}\) satisfies \(\lceil D_{1}N^{1-3\alpha/8}\rceil<N\).

Now if \(N\geq\max\{M_{51},T_{6}\}\) and \(R\in\{\lceil D_{1}N^{1-3\alpha/8}\rceil,\ \lceil D_{1}N^{1-3\alpha/8} \rceil+1,\ \ldots,\ N\}\), from (79) we get,

\[\Delta\widetilde{N}_{1} \leq D_{4}\Delta\widetilde{N}_{b}+D_{3}N^{1-3\alpha/8}\] \[\leq D_{4}\Delta\widetilde{N}_{b}+\frac{R}{2}.\] (80)

By definition of \(b\), we have,

\[\frac{\widetilde{N}_{a}}{\widetilde{N}_{b}}\Delta\widetilde{N}_{b}\ \geq\ \Delta \widetilde{N}_{a}\quad\text{for every $a\in[K]/\{1\}$.}\]Again, since \(\widetilde{N}_{a}(N)=\varTheta(N)\) for every \(a\in[K]\) (by Lemma G.13), there exists a constant \(D_{5}>0\) independent of the sample paths such that,

\[D_{5}\Delta\widetilde{N}_{b}\;\geq\;\Delta\widetilde{N}_{a}\quad\text{for every $a\in[K]/\{1,b\}$}.\]

As a result,

\[R = \sum_{a\in[K]/\{1,b\}}\Delta\widetilde{N}_{a}+\Delta\widetilde{N} _{1}+\Delta\widetilde{N}_{b}\] \[\leq (K-2)D_{5}\Delta\widetilde{N}_{b}+D_{4}\Delta\widetilde{N}_{b}+ \frac{R}{2}+\Delta\widetilde{N}_{b}\quad\text{(using (\ref{eq:201}))}\] \[\leq (1+(K-2)D_{5}+D_{4})\Delta\widetilde{N}_{b}+\frac{R}{2},\] \[\implies\;\Delta\widetilde{N}_{b} \;\geq\;\frac{1}{2(1+(K-2)D_{5}+D_{4})}R.\]

Now taking \(D_{2}=\frac{1}{2(1+(K-2)D_{5}+D_{4})}>0\), we have the desired conclusion. 

**Lemma G.16**.: _For AT2 (1) and IAT2 (2) algorithms, there exists constants \(C_{3},C_{4}>0\) independent of the sample paths, such that, for \(N\geq T_{6}\) and \(R\in\{1,2,\ldots,N\}\), if the algorithm pulls some arm \(a\in[K]/\{1\}\) at iteration \(N\) and doesn't pull \(a\) for the next \(R\) iterations, then,_

\[\text{{AT2:}}\quad\mathcal{I}_{a}(N+R)-\mathcal{I}_{b(N,R)}(N+R) \;\leq -C_{3}\Delta\widetilde{N}_{b(N,R)}(N,R)+C_{4}N^{1-3\alpha/8}\] \[+O\left(\left(N^{-3\alpha/8}+RN^{-1}\right)R\right),\] (81) \[\text{{IAT2:}}\quad\mathcal{I}_{a}^{(m)}(N+R)-\mathcal{I}_{b(N,R )}^{(m)}(N+R) \;\leq -C_{3}\Delta\widetilde{N}_{b(N,R)}(N,R)+C_{4}N^{1-3\alpha/8}\] \[+O\left(\left(N^{-3\alpha/8}+RN^{-1}\right)R\right),\] (82)

_where for every alternative arm \(j\in[K]/\{1\}\),_

\[\mathcal{I}_{j}^{(m)}(N)\;=\;\mathcal{I}_{j}(N)+\log(\widetilde{N}_{j}(N))\]

_is the modified empirical index of that arm at iteration \(N\)_

Proof.: For cleaner presentation, we use \(b\) to denote \(b(N,R)\). We also use \(\widetilde{\mu}_{j},\;\widetilde{x}_{1,j},\;\Delta\widetilde{N}_{j}\) and \(\widetilde{N}_{j}\), respectively, to denote \(\widetilde{\mu}_{j}(N),\;\widetilde{x}_{1,j}(N),\;\Delta\widetilde{N}_{j}(N,R)\) and \(\widetilde{N}_{j}(N)\) for every arm \(j\in[K]\), whenever it doesn't cause confusion.

**AT2 Algorithm:** Let

\[S_{a,b}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\;=\;\mathcal{I}_{a}(N )-\mathcal{I}_{b}(N)\]

denote the the difference between the empirical indexes of the two arms \(a\) and \(b\) at iteration \(N\). Note that \(S_{a,b}(\widetilde{\bm{N}},\widetilde{\bm{\mu}})\) depends only on the tuple of variables \(\;(\widetilde{N}_{1},\widetilde{N}_{a},\widetilde{N}_{b},\widetilde{\mu}_{1},\widetilde{\mu}_{a},\widetilde{\mu}_{b})\). In the following argument, we apply mean value theorem over \(S_{a,b}\) and the empirical indexes \(\mathcal{I}_{a}\) and \(\mathcal{I}_{b}\), treating them as functions of \(\widetilde{N}_{1}\), \(\widetilde{N}_{a}\), \(\widetilde{N}_{b}\), \(\widetilde{\mu}_{1}\), \(\widetilde{\mu}_{a}\), and \(\widetilde{\mu}_{b}\).

Using the multivariate mean value theorem, we have,

\[S_{a,b}(\widetilde{\bm{N}}(N+R),\widetilde{\bm{\mu}}(N+R))-S_{a,b} (\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\;=\;\sum_{j=1,a,b} \frac{\partial S_{a,b}}{\partial\mu_{j}}(\hat{\bm{N}},\hat{\bm{\mu}})\Delta \widetilde{\mu}_{j}\] \[+\frac{\partial S_{a,b}}{\partial N_{1}}(\hat{\bm{N}},\hat{\bm{ \mu}})\cdot\Delta\widetilde{N}_{1}+\frac{\partial S_{a,b}}{\partial N_{b}}( \hat{\bm{N}},\hat{\bm{\mu}})\cdot\Delta\widetilde{N}_{b},\] (83)

where \(\;\Delta\widetilde{\mu}_{j}=\widetilde{\mu}_{j}(N+R)-\widetilde{\mu}_{j}(N)\;\) for \(j=1,a,b\), and \(\;(\hat{\bm{N}},\hat{\bm{\mu}})=(\hat{N}_{1},\hat{N}_{a},\hat{N}_{b},\hat{\mu} _{1},\hat{\mu}_{a},\hat{\mu}_{b})\;\) such that, for \(j=1,a,b\), \(\hat{\mu}_{j}\) lies between \(\widetilde{\mu}_{j}(N)\) and \(\widetilde{\mu}_{j}(N+R)\), and \(\hat{N}_{j}\) lies in \(\left[\widetilde{N}_{j}(N),\widetilde{N}_{j}(N+R)\right]\). Note that there is no contribution on the RHS in (83) due to \(\widetilde{N}_{a}\), because \(\widetilde{N}_{a}(\cdot)\) doesn't change during iterations \(N+1,N+2,\ldots,N+R\), owing to our assumption that the algorithm doesn't pull arm \(a\) during the mentioned iterations.

First we consider the partial derivatives of \(S_{j,b}\) with respect to \(\mu_{1},\mu_{j}\) and \(\mu_{b}\) in (83). We have

\[\frac{\partial S_{a,b}}{\partial\mu_{1}}(\hat{\bm{N}},\hat{\bm{\mu}})\ =\ \hat{N}_{1}\left(d_{1}(\hat{\mu}_{1},\hat{x}_{1,a})-d_{1}(\hat{\mu}_{1},\hat{x} _{1,b})\right),\]

where \(\hat{x}_{1,j}=\frac{\hat{N}_{1}\hat{\mu}_{1}+\hat{N}_{j}\hat{\mu}_{j}}{\hat{N}_ {1}+\hat{N}_{j}}\) for \(j=a,b\).

Since \(R\leq N\), we have for \(j=1,a,b\), \(\hat{N}_{j}\leq\widetilde{N}_{j}+N=\varTheta(N)\) (by Lemma G.13). As a result, using (7) of Appendix B, both \(d_{1}(\hat{\mu}_{1},\hat{x}_{1,a})\) and \(d_{1}(\hat{\mu}_{1},\hat{x}_{1,b})\) are \(\varTheta(1)\). Using this, \(\left|\frac{\partial S_{a,b}}{\partial\mu_{1}}(\hat{\bm{N}},\hat{\bm{\mu}})\right|\) is \(O(N)\).

Similarly,

\[\frac{\partial S_{a,b}}{\partial\mu_{a}}(\hat{\bm{N}},\hat{\bm{\mu}})=\hat{N} _{a}d_{1}(\hat{\mu}_{a},\hat{x}_{1,a})\ \text{and}\ \frac{\partial S_{a,b}}{\partial\mu_{b}}(\hat{\bm{N}},\hat{\bm{\mu}})=- \hat{N}_{b}d_{1}(\hat{\mu}_{b},\hat{x}_{1,b}).\]

Using the same argument as for the partial derivative of \(S_{a,b}\) with respect to \(\mu_{1}\), we have \(\left|\frac{\partial S_{a,b}}{\partial\mu_{j}}(\hat{\bm{N}},\hat{\bm{\mu}}) \right|=O(N)\) for both \(j=a,b\).

Therefore, the contribution in the RHS of (83) due to the noisiness in the empirical means \(\widetilde{\mu}_{1},\widetilde{\mu}_{a},\widetilde{\mu}_{b}\) is bounded above by,

\[\sum_{j=1,a,b}\frac{\partial S_{a,b}}{\partial\mu_{j}}(\hat{\bm{ N}},\hat{\bm{\mu}})\Delta\widetilde{\mu}_{j} \leq \sum_{j=1,a,b}\left|\frac{\partial S_{a,b}}{\partial\mu_{j}}( \hat{\bm{N}},\hat{\bm{\mu}})\right|\cdot|\Delta\widetilde{\mu}_{j}|\] (84) \[\stackrel{{(\ref{eq:1})}}{{=}} O(N)\times O(N^{-3\alpha/8})\] \[= O(N^{1-3\alpha/8}),\]

where (1) follows since for \(N\geq T_{0}\) and \(j=1,a,b\), \(|\Delta\widetilde{\mu}_{j}|=O(N^{-3\alpha/8})\).

Now considering the partial derivative of \(S_{a,b}(\cdot)\) with respect to \(N_{1}\), we get

\[\frac{\partial S_{a,b}}{\partial N_{1}}(\hat{\bm{N}},\hat{\bm{\mu}})\ =\ d(\hat{\mu}_{1},\hat{x}_{1,a})-d(\hat{\mu}_{1},\hat{x}_{1,b}).\]

Using Lemma G.13, \(\widetilde{N}_{j}(N)\) and \(\widetilde{N}_{j}(N+R)\) are both \(\varTheta(N)\) for all \(j=1,a,b\) and \(N\geq T_{6}\), since \(R\leq N\). As a result, using (7), (8), we have,

\[\left|\frac{\partial^{2}S_{a,b}}{\partial\mu_{j}\partial N_{k}}(\bm{N}^{ \prime},\bm{\mu}^{\prime})\right|=O(1),\quad\text{and}\quad\left|\frac{ \partial^{2}S_{a,b}}{\partial N_{j}\partial N_{k}}(\bm{N}^{\prime},\bm{\mu}^{ \prime})\right|=O(N^{-1})\]

for every \(j,k\in\{1,a,b\}\), and tuple \((\bm{N}^{\prime},\bm{\mu}^{\prime})=(N^{\prime}_{1},N^{\prime}_{a},N^{\prime} _{b},\mu^{\prime}_{1},\mu^{\prime}_{a},\mu^{\prime}_{b})\) having \(N^{\prime}_{i}\in\left[\widetilde{N}_{i}(N),\widetilde{N}_{i}(N+R)\right]\) and \(\mu^{\prime}_{i}\) lying between \(\hat{\mu}_{i}\) and \(\widetilde{\mu}_{i}\), for every \(i=1,a,b\).

Therefore, applying the mean value theorem, we get,

\[\frac{\partial S_{a,b}}{\partial N_{1}}(\hat{\bm{N}},\hat{\bm{\mu}}) =\ d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{ 1},\widetilde{x}_{1,b})+O\left(\sum_{j=1,a,b}|\hat{\mu}_{j}-\widetilde{\mu}_{j} |\right)+O\left(N^{-1}\sum_{j=1,b}\Delta\widetilde{N}_{j}\right)\] \[=\ d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{ 1},\widetilde{x}_{1,b})+O\left(N^{-3\alpha/8}+RN^{-1}\right).\]

Similarly, considering the partial derivative with respect to \(N_{b}\),

\[\frac{\partial S_{a,b}}{\partial N_{b}}(\hat{\bm{N}},\hat{\bm{\mu}})\ =\ -d( \widetilde{\mu}_{b},\widetilde{x}_{1,b})+O\left(N^{-3\alpha/8}+RN^{-1}\right).\]Now, using all these upper bounds in the RHS of (83), we obtain

\[S_{a,b}(\widetilde{\boldsymbol{N}}(N+R),\widetilde{\boldsymbol{\mu} }(N+R))-S_{a,b}(\widetilde{\boldsymbol{N}}(N),\widetilde{\boldsymbol{\mu}}(N))\] \[\leq\ \Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta \widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\] \[\ \ \ \ +O\Big{(}N^{1-3\alpha/8}+R(N^{-3\alpha/8}+RN^{-1})\Big{)}.\] (85)

Since arm \(a\) was pulled in iteration \(N\), we have \(\mathcal{I}_{a}(N-1)\leq\mathcal{I}_{b}(N-1)\). Also since arm \(b\) was not pulled in iteration \(N\), its empirical index remains unchanged, _i.e._\(\mathcal{I}_{b}(N-1)=\mathcal{I}_{b}(N)\). Combining these two observations, we have \(\mathcal{I}_{a}(N-1)\leq\mathcal{I}_{b}(N)\).

As a result,

\[S_{a,b}(\widetilde{\boldsymbol{N}}(N),\widetilde{\boldsymbol{\mu }}(N)) =\ \mathcal{I}_{a}(N)-\mathcal{I}_{b}(N)\] \[=\ (\mathcal{I}_{a}(N)-\mathcal{I}_{a}(N-1))+(\mathcal{I}_{a}(N- 1)-\mathcal{I}_{b}(N))\] \[\leq\ \mathcal{I}_{a}(N)-\mathcal{I}_{a}(N-1)\] \[=\ I_{a}(N)-I_{a}(N-1)+O(N^{1-3\alpha/8})\quad\text{(using Lemma G.3)}\] \[\leq\ d(\mu_{a},\mu_{1})+O(N^{1-3\alpha/8})=O(N^{1-3\alpha/8}),\] (86)

where the last step follows from the fact that, the partial derivatives of \(I_{a}(\cdot)\) with respect to \(N_{a}\) is \(d(\mu_{a},x_{1,a}(N))\leq d(\mu_{a},\mu_{1})\). As a result, since arm \(a\) was pulled in iteration \(N\), the increment \(I_{a}(N)-I_{a}(N-1)\) in \(I_{a}(\cdot)\) is upper bounded by \(d(\mu_{a},\mu_{1})\).

Therefore (85) implies,

\[S_{a,b}(\widetilde{\boldsymbol{N}}(N+R),\widetilde{\boldsymbol{ \mu}}(N+R)) \leq\ \Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta \widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\] \[+S_{a,b}(\widetilde{\boldsymbol{N}}(N),\widetilde{\boldsymbol{ \mu}}(N))+O\left(\left(N^{-3\alpha/8}+RN^{-1}\right)R\right)\] \[\leq\ \Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta \widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\] \[+O\left(N^{1-3\alpha/8}+\left(N^{-3\alpha/8}+RN^{-1}\right)R \right).\] (87)

Now we consider two possibilities:

Case I\(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b})<0\):In this case, by (85), \(S_{a,b}(\widetilde{\boldsymbol{N}}(N+R),\widetilde{\boldsymbol{\mu}}(N+R))\) is upper bounded by,

\[-\Delta\widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})+O \left(N^{1-3\alpha/8}+\left(N^{-3\alpha/8}+RN^{-1}\right)R\right).\]

By Lemma G.13 and (6), since both \(\widetilde{N}_{1}(N)\) and \(\widetilde{N}_{b}(N)\) are \(\varTheta(N)\), we have \(d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})=\varTheta(1)\). As a result, (81) follows.

Case II\(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b})\geq 0\):In this case, the RHS of (85) can be rewritten as,

\[\Delta\widetilde{N}_{b}\left(\frac{\Delta\widetilde{N}_{1}}{\Delta\widetilde{ N}_{b}}(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b}))-d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\right)+O \left(N^{1-3\alpha/8}+\left(N^{-3\alpha/8}+RN^{-1}\right)R\right).\] (88)

Using (76) in Lemma G.14, the upper bound becomes

\[\Delta\widetilde{N}_{b}\Big{(}\frac{\widetilde{N}_{1}}{\widetilde{ N}_{b}}(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b}))-d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\Big{)}\] \[+O\Big{(}N^{1-3\alpha/8}(1+RN^{-1})^{3}+\left(N^{-3\alpha/8}+RN^ {-1}\right)R\Big{)}\] \[=\ \Delta\widetilde{N}_{b}\left(\frac{\widetilde{N}_{1}}{\widetilde{ N}_{b}}(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b}))-d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\right)\] \[+O\left(N^{1-3\alpha/8}+\left(N^{-3\alpha/8}+RN^{-1}\right)R \right)\quad\text{(since $R\leq N$)}.\] (89)By (86), we have,

\[{\cal I}_{a}(N)\;\leq\;{\cal I}_{b}(N)+O(N^{1-3\alpha/8}).\] (90)

Expanding the empirical index terms, we get,

\[\widetilde{N}_{1}d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})+ \widetilde{N}_{a}d(\widetilde{\mu}_{a},\widetilde{x}_{1,a})\;\leq\;\widetilde{ N}_{1}d(\widetilde{\mu}_{1},\widetilde{x}_{1,b})+\widetilde{N}_{b}d( \widetilde{\mu}_{b},\widetilde{x}_{1,b})+O(N^{1-3\alpha/8}).\]

By Lemma G.13 we have \(\widetilde{N}_{b}=\varTheta(N)\). As a result, upon dividing both sides of the above inequality by \(\widetilde{N}_{b}\) and after some re-arrangement of terms, we obtain,

\[\frac{\widetilde{N}_{1}}{\widetilde{N}_{b}}(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,j})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-d( \widetilde{\mu}_{b},\widetilde{x}_{1,b})\;\leq\;-\;\frac{\widetilde{N}_{j}}{ \widetilde{N}_{b}}d(\widetilde{\mu}_{j},\widetilde{x}_{1,j})+O(N^{-3\alpha/8}).\]

Using the above inequality in (89), we get the upper bound,

\[-\Delta\widetilde{N}_{b}\left(\frac{\widetilde{N}_{j}}{\widetilde{N}_{b}}d( \widetilde{\mu}_{a},\widetilde{x}_{1,a})\right)+O\left(N^{1-3\alpha/8}+\left( N^{-3\alpha/8}+RN^{-1}\right)R\right)\] (91)

By Lemma G.13 and (6), we have, \(\frac{\widetilde{N}_{j}}{\widetilde{N}_{b}}d(\widetilde{\mu}_{j},\widetilde{ x}_{1,j})=\varTheta(1)\). Therefore (81) follows.

**IAT2 Algorithm:** The proof of (82) for IAT2 is very similar to the proof of (81) for AT2. We first consider the difference of the modified empirical indexes between the two arms \(a\) and \(b\),

\[S_{a,b}^{(m)}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\;= \;{\cal I}_{a}^{(m)}(N)-{\cal I}_{b}^{(m)}(N).\]

Following a similar procedure as the AT2 algorithm, we apply the mean value theorem to obtain,

\[S_{a,b}^{(m)}(\widetilde{\bm{N}}(N+R),\widetilde{\bm{\mu}}(N+R)) -S_{a,b}^{(m)}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\;= \;\sum_{j=1,a,b}\frac{\partial S_{a,b}^{(m)}}{\partial\mu_{j}}(\hat{\bm{N}}, \hat{\bm{\mu}})\cdot\Delta\widetilde{\mu}_{j}\] \[+\frac{\partial S_{a,b}^{(m)}}{\partial N_{1}}(\hat{\bm{N}}, \hat{\bm{\mu}})\cdot\Delta\widetilde{N}_{1}+\frac{\partial S_{a,b}^{(m)}}{ \partial N_{b}}(\hat{\bm{N}},\hat{\bm{\mu}})\cdot\Delta\widetilde{N}_{b},\] (92)

where \(\Delta\widetilde{\mu}_{j}=\widetilde{\mu}_{j}(N+R)-\widetilde{\mu}_{j}(N)\), and \((\hat{\bm{N}},\hat{\bm{\mu}})=(\hat{N}_{1},\hat{N}_{a},\hat{N}_{b},\hat{\mu} _{1},\hat{\mu}_{a},\hat{\mu}_{b})\), such that, \(\hat{\mu}_{j}\) lies between \(\widetilde{\mu}_{j}(N)\) and \(\widetilde{\mu}_{j}(N+R)\), and \(\hat{N}_{j}\in\left[\widetilde{N}_{j}(N),\widetilde{N}_{j}(N+R)\right]\), for every \(j=1,a,b\).

The contribution to (92) due to noise in \(\widetilde{\bm{\mu}}\) is \(O(N^{1-3\alpha/8})\) by the same argument we proved (84) for AT2.

Now we consider the partial derivatives of \(S_{a,b}^{(m)}\) with respect to \(N_{1},N_{a},N_{b}\). Following the same steps as used for AT2 algorithm, we have,

\[\frac{\partial S_{a,b}^{(m)}}{\partial N_{1}}(\hat{\bm{N}},\hat{ \bm{\mu}})\;=\;\frac{\partial S_{a,b}}{\partial N_{1}}(\hat{\bm{N}},\hat{\bm{ \mu}})\;=\;d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1}, \widetilde{x}_{1,b})+O(N^{-3\alpha/8}+RN^{-1}),\;\text{and}\] \[\frac{\partial S_{a,b}^{(m)}}{\partial N_{b}}(\hat{\bm{N}},\hat{ \bm{\mu}})\;=\;\frac{\partial S_{a,b}}{\partial N_{b}}(\hat{\bm{N}},\hat{\bm{ \mu}})-\frac{1}{\widetilde{N}_{b}}\;\leq\;-\;d(\widetilde{\mu}_{b},\widetilde{x }_{1,b})+O(N^{-3\alpha/8}+RN^{-1}).\]

As a result, the contribution to (92) due to \(\widetilde{N}_{1}\) and \(\widetilde{N}_{b}\) is,

\[\frac{\partial S_{a,b}^{(m)}}{\partial N_{1}}(\hat{\bm{N}},\hat{ \bm{\mu}})\cdot\Delta\widetilde{N}_{1}+\frac{\partial S_{a,b}^{(m)}}{\partial N _{b}}(\hat{\bm{N}},\hat{\bm{\mu}})\cdot\Delta\widetilde{N}_{b}\] \[\;\leq\;\Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta \widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})+O\left( \left(N^{-3\alpha/8}+RN^{-1}\right)R\right).\]Therefore, adding the contributions of the noise in \((\widetilde{\mu}_{j})_{j=1,a,b}\), (92) can be further upper bounded by,

\[S^{(m)}_{a,b}(\widetilde{\bm{N}}(N+R),\widetilde{\bm{\mu}}(N+R))-S^ {(m)}_{a,b}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\] \[\leq\ \Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1}, \widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta \widetilde{N}_{b}\cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\] \[\qquad+O\left(N^{1-3\alpha/8}+\left(N^{-3\alpha/8}+RN^{-1} \right)R\right).\] (93)

Now, we find an upper bound to \(S^{(m)}_{a,b}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N))\). Since the algorithm pulls arm \(a\) and doesn't pull arm \(b\) at iteration \(N\), we must have

\[\mathcal{I}^{(m)}_{a}(N-1)\ \leq\ \mathcal{I}^{(m)}_{b}(N-1)\ =\ \mathcal{I}^{(m)}_{b}(N).\]

Therefore,

\[S^{(m)}_{a,b}(\widetilde{\bm{N}}(N),\widetilde{\bm{\mu}}(N)) =\ \mathcal{I}^{(m)}_{a}(N)-\mathcal{I}^{(m)}_{b}(N)\] \[=\ (\mathcal{I}^{(m)}_{a}(N)-\mathcal{I}^{(m)}_{a}(N-1))+( \mathcal{I}^{(m)}_{a}(N-1)-\mathcal{I}^{(m)}_{b}(N))\] \[\leq\ \mathcal{I}^{(m)}_{a}(N)-\mathcal{I}^{(m)}_{a}(N-1)\] \[=\ \mathcal{I}_{a}(N)-\mathcal{I}_{a}(N-1)+\log\left(\frac{ \widetilde{N}_{a}(N)}{\widetilde{N}_{a}(N-1)}\right)\] \[\stackrel{{(1)}}{{=}}\ \mathcal{I}_{a}(N)-\mathcal{I}_{a}(N-1)+O(1)\] \[\stackrel{{(2)}}{{=}}\ O(N^{1-3\alpha/8}),\] (94)

where (1) follows from the fact that \(\widetilde{N}_{a}(N)=\varTheta(N)\) for \(N\geq T_{6}\), and (2) follows using the arguments used while bounding \(\mathcal{I}_{a}(N)-\mathcal{I}_{a}(N-1)\) in (86).

Putting this in (93), we get,

\[S^{(m)}_{a,b}(\widetilde{\bm{N}}(N+R),\widetilde{\bm{\mu}}(N+R) \leq\Delta\widetilde{N}_{1}\cdot(d(\widetilde{\mu}_{1},\widetilde{ x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_{1,b}))-\Delta\widetilde{N}_{b} \cdot d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\] \[+O\left(N^{1-3\alpha/8}+R(N^{-3\alpha/8}+RN^{-1})\right).\] (95)

Now there can be two cases.

**Case I \(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{ x}_{1,b})<0\) :** Then, using the same argument as was used for Case I of AT2 algorithm, we get (82).

**Case II \(d(\widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{ x}_{1,b})\geq 0\) :** Using (76) of Lemma G.12, the RHS in (95) is upper bounded by,

\[\Delta\widetilde{N}_{b}\left(\frac{\widetilde{N}_{1}}{\widetilde{N}_{b}}(d( \widetilde{\mu}_{1},\widetilde{x}_{1,a})-d(\widetilde{\mu}_{1},\widetilde{x}_ {1,b}))-d(\widetilde{\mu}_{b},\widetilde{x}_{1,b})\right)+O\left(N^{1-3\alpha/8 }+\left(N^{-3\alpha/8}+RN^{-1}\right)R\right).\]

Using (94), we have

\[\mathcal{I}^{(m)}_{a}(N)\ \leq\ \mathcal{I}^{(m)}_{b}(N)+O\left(N^{1-3\alpha/8} \right),\]

which implies,

\[\mathcal{I}_{a}(N)\ \leq\ \mathcal{I}_{b}(N)+\log\left(\frac{\widetilde{N}_{b}}{ \widetilde{N}_{a}}\right)+O(N^{1-3\alpha/8})\ \stackrel{{(1)}}{{=}}\ \mathcal{I}_{b}(N)+O(N^{1-3\alpha/8}),\]

where (1) follows from the fact that \(\widetilde{N}_{a}(N)\) and \(\widetilde{N}_{b}(N)\) are both \(\varTheta(N)\), causing \(\log\left(\frac{\widetilde{N}_{a}(N)}{\widetilde{N}_{b}(N)}\right)=O(1)\) for \(N\geq T_{6}\). Note that the above inequality is same as (90) obtained in Case II of AT2. Now (82) follows using exactly the same argument as in the Case II of AT2 after (90).

For every \(a\in[K]/\{1\}\), \(N\geq T_{6}\) and \(R\in\{1,2,\ldots,N\}\), we have,

\[|\mathcal{I}_{a}(N+R)-\mathcal{I}_{a}(N+R-1)| \stackrel{{(1)}}{{=}} |I_{a}(N+R)-I_{a}(N+R-1)|+O((N+R)^{1-3\alpha/8})\] \[\stackrel{{(2)}}{{=}} |I_{a}(N+R)-I_{a}(N+R-1)|+O(N^{1-3\alpha/8})\] \[\stackrel{{(3)}}{{\leq}} \max\{d(\mu_{1},\mu_{a}),d(\mu_{a},\mu_{1})\}+O(N^{1-3\alpha/8})=O (N^{1-3\alpha/8}),\] (96)

where: (1) follows by Lemma G.3; (2) follows since \(R\leq N\); and (3) follows from the fact that \(I_{a}(\cdot)\) can increment by atmost \(\max\{d(\mu_{1},\mu_{a}),d(\mu_{a},\mu_{1})\}\) in one iteration.

Since, at every iteration \(N\geq 1\) and for every arm, the modified empirical index differ from the empirical index by atmost \(\log(N)\), we have,

\[\Big{|}\mathcal{I}_{a}^{(m)}(N+R)-\mathcal{I}_{a}^{(m)}(N+R-1) \Big{|} \leq |\mathcal{I}_{a}(N+R)-\mathcal{I}_{a}(N+R-1)|+2\log(N+R)\] (97) \[\stackrel{{(i)}}{{\leq}} |\mathcal{I}_{a}(N+R)-\mathcal{I}_{a}(N+R-1)|+O(\log(N))\] \[= O(N^{1-3\alpha/8})+O(\log(N))\;=\;O(N^{1-3\alpha/8}),\]

where (i) follows because \(R\leq N\).

Using (96) and (97), the following corollary follows from Lemma G.16,

**Corollary G.2**.: _For AT2 (1) and IAT2 (2) algorithms, for \(N\geq T_{6}\) and \(R\in\{1,2,\ldots,N\}\), if the algorithm pulls some arm \(a\in[K]/\{1\}\) at iteration \(N\) and doesn't pull a for the next \(R\) iterations, then,_

\[\textbf{AT2:}\quad\mathcal{I}_{a}(N+R-1)-\mathcal{I}_{b(N,R)}(N+ R-1) \leq -C_{3}\Delta\widetilde{N}_{b(N,R)}(N,R)+C_{5}N^{1-3\alpha/8}\] (98) \[+O\left(\left(N^{-3\alpha/8}+RN^{-1}\right)R\right),\]

\[\textbf{IAT2:}\quad\mathcal{I}_{a}^{(m)}(N+R-1)-\mathcal{I}_{b(N,R)}^{(m)}(N+R-1) \leq -C_{3}\Delta\widetilde{N}_{b(N,R)}(N,R)+C_{5}N^{1-3\alpha/8}\] (99) \[+O\left(\left(N^{-3\alpha/8}+RN^{-1}\right)R\right),\]

_where \(C_{3},C_{5}>0\) are constants independent of the sample paths._

**Proof of Lemma G.2:** We prove the proposition for the AT2 algorithm. The proof for IAT2 algorithm follows the exact same argument by replacing \(\mathcal{I}\) with \(\mathcal{I}^{(m)}\).

In the proof, we argue via contradiction. We show that, there exists constants \(M_{5}\geq 1\) and \(C_{1}>0\) independent of the sample paths, such that for \(N\geq\max\{M_{5},T_{6}\}\), if the algorithm pulls some arm \(a\in[K]/\{1\}\) at iteration \(N\) and doesn't pull it for the next \(R(N)\stackrel{{\rm def}}{{=}}[C_{1}N^{1-3\alpha/8}]\) iterations, then at iteration \(\tau(N,R(N))\), the algorithm pulls arm \(b(N,R(N))\), even though arm \(a\) has empirical index strictly less than that of arm \(b(N,R(N))\).

Using Corollary G.2, there exists constants \(C_{3},C_{5},C_{6}>0\) independent of the sample paths, such that, for \(N\geq T_{6}\) and \(R\in\{1,2,\ldots,N\}\),

\[\mathcal{I}_{a}(N+R-1)-\mathcal{I}_{b(N,R)}(N+R-1) \leq -C_{3}\Delta\widetilde{N}_{b(N,R)}(N,R)+C_{4}N^{1-3\alpha/8}\] (100) \[+C_{5}R(N^{-3\alpha/8}+RN^{-1}).\]

We define

\[C_{1}=\max\left\{D_{1},\frac{2C_{4}}{D_{2}C_{3}}\right\}+1,\]where \(D_{1}\) and \(D_{2}\) are the constants introduced in Lemma G.15. Let \(M_{52}\geq 1\) to be the smallest number such that, every \(N\geq M_{52}\) satisfies \(\lceil C_{1}N^{1-3\alpha/8}\rceil<N\). Since \(C_{1}>D_{1}\), by the definition of \(M_{51}\) in the proof of Lemma G.15, we have \(M_{52}>M_{51}\).

Now let \(R(N)=\lceil C_{1}N^{1-3\alpha/8}\rceil\). Then by Lemma G.15, for \(N\geq\max\{M_{52},T_{6}\}\), since \(R(N)>\lceil D_{1}N^{1-3\alpha/8}\rceil\), we have,

\[\Delta\widetilde{N}_{b(N,R(N))}(N,R(N))\;\geq\;D_{2}R(N)\;\geq\; \frac{2C_{4}}{C_{3}}N^{1-3\alpha/8}.\] (101)

Since arm \(b(N,R)\) is not pulled between the iterations \(\tau(N,R)\) and \(N+R\), we have

\[\Delta\widetilde{N}_{b(N,R)}(N,t)\;=\;\Delta\widetilde{N}_{b(N,R)}(N,R)\]

for all \(t\in\{\;\tau(N,R)-N,\;\;\tau(N,R)-N+1,\;\;\ldots,\;R\;\}\).

As a result, by definition of \(b(N,R)\) we have,

\[b(N,R(N)) = b(N,\tau(N,R(N))-N)\quad\text{and}\] \[\Delta\widetilde{N}_{b(N,R(N))}(N,R(N)) = \Delta\widetilde{N}_{b(N,R)}(N,\tau(N,R(N))-N).\]

In the rest of the proof, we denote \(b(N,R(N))\) and \(\tau(N,R(N))-N\), respectively, using \(b(N)\) and \(\tau_{b}(N)\).

Therefore, using Corollary G.2, we have, for \(N\geq\max\{M_{52},T_{6}\}\),

\[\mathcal{I}_{a}(N+\tau_{b}(N)-1)-\mathcal{I}_{b(N)}(N+\tau_{b}(N)-1)\] \[\leq -C_{3}\Delta\widetilde{N}_{b(N)}(N,\tau_{b}(N))+C_{4}N^{1-3\alpha /8}+C_{5}\tau_{b}(N)\times(N^{-3\alpha/8}+\tau_{b}(N)N^{-1})\] \[\stackrel{{(1)}}{{\leq}} -C_{3}\times\frac{2C_{4}}{C_{3}}N^{1-3\alpha/8}+C_{4}N^{1-3\alpha /8}+C_{5}R(N)\times(N^{-3\alpha/8}+R(N)N^{-1})\] \[\stackrel{{(2)}}{{\leq}} -C_{4}N^{1-\alpha/8}+C_{5}(C_{1}+1)(C_{1}+2)N^{1-3\alpha/8}\times N ^{-3\alpha/8}\] \[\stackrel{{(3)}}{{=}} -(C_{4}-C_{6}N^{-3\alpha/8})N^{1-3\alpha/8},\] (102)

where: (1) follows using (101) and \(\tau_{b}(N)\leq R(N)\), (2) follows since \(R(N)\leq(C_{1}+1)N^{-3\alpha/8}\), and (3) follows by letting \(C_{6}=C_{5}(C_{1}+1)(C_{1}+2)\).

We now take \(M_{53}\geq 1\) to be large enough, such that, \(C_{6}M_{53}^{-3\alpha/8}<C_{4}\). Let \(M_{5}=\max\{M_{52},M_{53}\}\). Then (102) implies, for \(N\geq\max\{M_{5},T_{6}\}\) and \(R(N)=\lceil C_{1}N^{1-3\alpha/8}\rceil\), if the algorithm picks some arm \(a\in[K]/\{1\}\) at iteration \(N\) and doesn't pick \(a\) for the next \(R(N)\) iterations, then, at iteration \(N+\tau_{b}(N)\), the algorithm picks arm \(b(N)\), even though, \(\mathcal{I}_{a}(N+\tau_{b}(N)-1)-\mathcal{I}_{b(N)}(N+\tau_{b}(N)-1)<0\). Thus we arrive at a contradiction. 

## Appendix H Proof of Theorem 3.1

By Proposition 3.1, we know that, for every \(a\in[K]\) and \(N\geq T_{stable}\),

\[|\widetilde{\omega_{a}}(N)-\omega_{a}^{\star}|\;\leq\;C_{1}N^{-3 \alpha/8}.\]

Recall the functions \(W_{a}(\cdot,\cdot)\) defined in Appendix G.2.2. Note that for every allocation \(\boldsymbol{\omega}=(\omega_{a})_{a\in[K]}\), and \(a\in[K]/\{1\}\), we have,

\[\frac{\partial W_{a}}{\partial\omega_{1}}(\omega_{1},\omega_{a})\;= \;d(\mu_{1},x_{1,a}(\omega_{1},\omega_{a}))\quad\text{and}\quad\frac{ \partial W_{a}}{\partial\omega_{a}}(\omega_{1},\omega_{a})\;=\;d(\mu_{a},x_{1,a}(\omega_{1},\omega_{a})),\]

where \(x_{1,a}(\omega_{1},\omega_{a})=\frac{\omega_{1}\mu_{1}+\omega_{a}\mu_{a}}{ \omega_{1}+\omega_{a}\mu_{a}}\).

As a result, for every \(a\in[K]/\{1\}\), the partial derivatives of \(W_{a}(\omega_{1},\omega_{a})\) with respect to \(\omega_{1}\) and \(\omega_{a}\) are both \(O(1)\). Therefore, using the mean value theorem, for every \(a\in[K]/\{1\}\) and \(N\geq T_{stable}\), we have,

\[|W_{a}(\widetilde{\omega}_{1}(N),\widetilde{\omega}_{a}(N))-W_{a}(\omega_{1}^ {\star},\omega_{a}^{\star})|\;=\;O(N^{-3\alpha/8}).\] (103)Define the normalized index of every arm as

\[H_{a}(N)\;=\;\frac{I_{a}(N)}{N}\;=\;W_{a}(\widetilde{\omega}_{1}(N),\widetilde{ \omega}_{a}(N)).\]

Also by (54), \(W_{a}(\omega_{1}^{\star},\omega_{a}^{\star})=I^{\star}=T^{\star}(\boldsymbol{ \mu})^{-1}\) for every alternative arm \(a\in[K]/\{1\}\).

Therefore (103) gives us,

\[|H_{a}(N)-T^{\star}(\boldsymbol{\mu})^{-1}|\;=\;O(N^{-3\alpha/8}),\quad\text{ for $a\in[K]/\{1\}$ and $N\geq T_{stable}$}.\] (104)

By Lemma G.3, we also know,

\[|\mathcal{I}_{a}(N)-NH_{a}(N)|\;=\;|\mathcal{I}_{a}(N)-I_{a}(N)|\;=\;O(N^{1-3 \alpha/8}).\] (105)

Combining (104) and (105), we get

\[|\mathcal{I}_{a}(N)-NT^{\star}(\boldsymbol{\mu})^{-1}| \;\leq\;|\mathcal{I}_{a}(N)-NH_{a}(N)|+N|H_{a}(N)-T^{\star}( \boldsymbol{\mu})^{-1}|\] \[\;=\;O(N^{1-3\alpha/8}),\]

for every \(a\in[K]/\{1\}\). Hence, we can find a constant \(C_{2}>0\), independent of the sample paths, such that,

\[\mathcal{I}_{a}(N)\;\geq\;\frac{N}{T^{\star}(\boldsymbol{\mu})}-C_{2}N^{1-3 \alpha/8},\]

for every \(N\geq T_{stable}\) and \(a\in[K]/\{1\}\). As a result, for \(N\geq T_{stable}\), we have,

\[\min_{a\in[K]/\{1\}}\mathcal{I}_{a}(N)\;\geq\;\frac{N}{T^{\star}(\boldsymbol{ \mu})}-C_{2}N^{1-3\alpha/8}.\] (106)

The threshold function \(\beta(N,\delta)\) deciding our stopping condition satisfies,

\[\log(1/\delta)\;\leq\;\beta(N,\delta)\;\leq\;\log(1/\delta)+C_{3}\log\log(1/ \delta)+C_{4}\log\log(N)+C_{5}\]

for constants \(C_{3},C_{4},C_{5}>0\). For every \(\delta>0\), we define the deterministic quantity,

\[t_{\max,\delta}\;=\;\min\left\{\;N\geq T_{stable}\;\;\Big{|}\;\;\frac{N}{T^{ \star}(\boldsymbol{\mu})}-C_{2}N^{1-3\alpha/8}>\beta(N,\delta)\;\right\}.\] (107)

Now we make the following observations about \(t_{\max,\delta}\),

1. Note that \(\frac{N}{T^{\star}(\boldsymbol{\mu})}-C_{2}N^{1-3\alpha/8}\) increases linearly in \(N\) and \(\beta(N,\delta)\) is \(O(\log\log(N)+\log(1/\delta))\), for a fixed \(\delta>0\). Hence, \(t_{\max,\delta}\) is finite for every \(\delta>0\).
2. We have \(\beta(N,\delta)\geq\log(1/\delta)\) and \(\frac{N}{T^{\star}(\boldsymbol{\mu})}-C_{2}N^{1-3\alpha/8}<\frac{N}{T^{\star}( \boldsymbol{\mu})}\). As a result, \(t_{\max,\delta}\) is atleast the iteration at which \(\frac{N}{T^{\star}(\boldsymbol{\mu})}\) exceeds \(\log(1/\delta)\), which is atleast \(T^{\star}(\boldsymbol{\mu})\log(1/\delta)\). This implies \(t_{\max,\delta}\geq T^{\star}(\boldsymbol{\mu})\log(1/\delta)\). As a result, \(t_{\max,\delta}\to\infty\) as \(\delta\to 0\).
3. If \(\tau_{\delta}>T_{stable}\), then \(\min_{a\in[K]/\{1\}}\mathcal{I}_{a}(N)\) exceed \(\beta(N,\delta)\) before the lower bound of \(\min_{a\in[K]/\{1\}}\mathcal{I}_{a}(N)\) in (106) exceeds \(\beta(N,\delta)\). As a result, \(\tau_{\delta}\leq t_{\max,\delta}\), whenever \(\tau_{\delta}\geq T_{stable}\). This gives us the upper bound, \[\tau_{\delta}\;\leq\;\max\{T_{stable},t_{\max,\delta}\}\quad\text{a.s. in $\mathbb{P}_{\boldsymbol{\mu}}$}.\] (108)

We have \(\mathbb{E}_{\boldsymbol{\mu}}[T_{stable}]<\infty\), which also implies \(T_{stable}<\infty\) a.s. in \(\mathbb{P}_{\boldsymbol{\mu}}\). Now using (108), we get,

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\mu}}[\tau_{\delta}]}{\log(1/ \delta)}\;\leq\;\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\mu}}[T_{stable}]+t_{ \max,\delta}}{\log(1/\delta)}\;=\;\limsup_{\delta\to 0}\frac{t_{\max,\delta}}{ \log(1/\delta)}.\]Similarly, we have,

\[\limsup_{\delta\to 0}\frac{\tau_{\delta}}{\log(1/\delta)}\ \leq\ \limsup_{\delta\to 0} \frac{T_{stable}+t_{\max,\delta}}{\log(1/\delta)}\ =\ \limsup_{\delta\to 0}\frac{t_{\max,\delta}}{\log(1/\delta)}\quad\text{a.s. in }\mathbb{P}_{\bm{\mu}}.\]

Therefore to prove asymptotic optimality, it is sufficient to prove \(\limsup_{\delta\to 0}\frac{t_{\max,\delta}}{\log(1/\delta)}\leq T^{\star}(\bm{\mu})\).

Let \(s_{\max,\delta}=t_{\max,\delta}-1\). Note that \(\limsup_{\delta\to 0}\frac{t_{\max,\delta}}{\log(1/\delta)}=\limsup_{ \delta\to 0}\frac{s_{\max,\delta}}{\log(1/\delta)}\). By definition of \(t_{\max,\delta}\), we have

\[\frac{s_{\max,\delta}}{T^{\star}(\bm{\mu})}-C_{2}s_{\max,\delta}^{1-3\alpha/8 }\ \leq\ \beta(s_{\max,\delta},\delta)=\log(1/\delta)+C_{3}\log\log(1/\delta)+C_{4} \log\log(s_{\max,\delta})+C_{5}.\]

After some rearrangement of terms, upon dividing both sides by \(\log(1/\delta)\), we get,

\[\frac{1}{T^{\star}(\bm{\mu})}\frac{s_{\max,\delta}}{\log(1/\delta )}\Big{(}1-C_{2}s_{\max,\delta}^{-3\alpha/8}-C_{3}s_{\max,\delta}^{-1}\log \log(s_{\max,\delta})\] \[-C_{5}s_{\max,\delta}^{-1}\Big{)}\ \leq\ 1+C_{3}\frac{\log\log(1/\delta)}{\log(1/\delta)}.\]

By Observation 2, \(s_{\max,\delta}=t_{\max,\delta}-1\to\infty\) as \(\delta\to 0\). As a result, the above inequality implies,

\[\frac{1}{T^{\star}(\bm{\mu})}\limsup_{\delta\to 0}\frac{s_{\max,\delta}}{\log(1/ \delta)}\ \leq\ 1.\]

We already argued that \(\limsup_{\delta\to 0}\frac{t_{\max,\delta}}{\log(1/\delta)}\leq T^{\star}(\bm{\mu})\). As a result, we have a constant \(C_{6}>0\), such that \(t_{\max,\delta}\leq C_{6}\log(1/\delta)\) for all \(\delta\). By (108) we have:

\[\frac{t_{\max,\delta}-1}{T^{\star}(\bm{\mu})}-C_{2}(t_{\max,\delta}-1)^{1-3 \alpha/8}\leq\beta(t_{\max,\delta}-1,\delta)\]

which implies,

\[t_{\max,\delta}\leq T^{\star}(\bm{\mu})\log(1/\delta)+O\left(\log\log(1/ \delta)+t_{\max,\delta}^{1-3\alpha/8}\right).\]

Putting \(t_{\max,\delta}\leq C_{6}\log(1/\delta)\) in the above inequality,

\[t_{\max,\delta}\ \leq\ T^{\star}(\bm{\mu})\log(1/\delta)+O\left(\left(\log(1/ \delta)\right)^{1-3\alpha/8}\right).\] (109)

Since \(\tau_{\delta}\leq\max\{T_{stable},1+t_{\max,\delta}\}\), we can find a constant \(C>0\) such that \(\tau_{\delta}\leq\max\big{\{}T_{stable},T^{\star}(\bm{\mu})\log(1/\delta)+C( \log(1/\delta))^{1-3\alpha/8}\big{\}}\). Hence Theorem 3.1 stands proved.

## Appendix I Extending the proposed algorithm to distributions with bounded support

We describe a natural extension of AT2 and IAT2 algorithms to bandit instances from a non-parametric family. We conduct experiments to compare the proposed algorithm with the existing ones. We consider the class of distributions having their supports contained in \([0,1]\), which we denote by \(\mathcal{F}_{[0,1]}\). This is similar to the assumptions made in [16]. Some definitions are in order. We use \(\mu(G)\) to denote the mean of distribution \(G\in\mathcal{F}_{[0,1]}\).

For every \(F\in\mathcal{F}_{[0,1]}\) and \(x\in[0,1]\), we define \(\text{KL}_{\inf}^{+}\) and \(\text{KL}_{\inf}^{-}\) as:

\[\text{KL}_{\inf}^{+}(F,x) =\ \inf\{\text{ KL}(F,G)\mid\mu(G)>x\ \}\quad\text{and}\] \[\text{KL}_{\inf}^{-}(F,x) =\ \inf\{\text{ KL}(F,G)\mid\mu(G)<x\ \}.\]

At iteration \(N\) of the algorithm, let \(\widetilde{F}_{a}(N)\) be the empirical distribution of the samples collected from some arm \(a\in[K]\), \(\widetilde{\bm{F}}(N)=(\widetilde{F}_{a}(N):a\in[K])\), \(\widetilde{N}_{a}(N)\) be the total no. of samples collected from \(a\) till \(N\), and \(\widetilde{\bm{N}}(N)=(\widetilde{N}_{a}(N):a\in[K])\). Let \(\widetilde{\mu}_{a}(N)=\mu(\widetilde{F}_{a}(N))\) and \(\hat{\bar{\imath}}_{N}=\text{arg}\max_{a\in[K]}\ \widetilde{\mu}_{a}(N)\). We now define:

\[x_{\hat{\imath}_{N},a}(N)\ =\ \text{arg}\min_{x\in[0,1]}\ \Big{\{}\ \widetilde{N}_{\hat{\imath}_{N}}(N)\cdot\text{KL}_{\inf}^{-}( \widetilde{F}_{\hat{\imath}_{N}}(N),x)+\widetilde{N}_{a}(N)\cdot\text{KL}_{ \inf}^{+}(\widetilde{F}_{a}(N),x)\ \Big{\}}.\]At every iteration, we compute the _empirical index_ of every arm \(a\neq\hat{i}_{N}\) as:

\[\mathcal{I}_{a}(N)\ =\ \min_{x\in[0,1]}\Big{\{}\,\widetilde{N}_{\hat{i}_{N}}(N) \cdot\text{KL}^{-}_{\inf}(\widetilde{F}_{\hat{i}_{N}}(N),x)+\widetilde{N}_{a}(N )\cdot\text{KL}^{+}_{\inf}(\widetilde{F}_{a}(N),x)\ \Big{\}},\]

and the _anchor function_ as,

\[g(\widetilde{\bm{F}}(N),\widetilde{\bm{N}}(N))\ =\ \sum_{a\neq\hat{i}_{N}}\frac{ \text{KL}^{-}_{\inf}(\widetilde{F}_{\hat{i}_{N}}(N),x_{\hat{i}_{N},a}(N))}{ \text{KL}^{+}_{\inf}(\widetilde{F}_{a}(N),x_{\hat{i}_{N},a}(N))}-1.\]

The AT2 and IAT2 algorithm for the class \(\mathcal{F}_{[0,1]}\), respectively, follows the same steps as in (1) and (2) with the anchor and index functions defined as above.

We experimentally demonstrate the performance of the proposed algorithms in Appendix J.5.

## Appendix J Experiments

### Dynamics of the algorithms

**Experiment 1 (Gaussian and Bernoulli bandits with well-separated arms):** In the main text (Figure 1), we presented the evolution of normalized indexes for the sub-optimal arms for AT2, when run without the stopping rule. Numerically, we observe similar plots for normalized indexes even for the other algorithms: \(0.5\)-EB-TCB (proposed in [16] with \(\beta=0.5\)), and TCB (proposed in [22]). Hence, we do not report them. However, we do observe differences in the evolution of the anchor function value across these algorithms. We present this in two different settings in the current section.

Interestingly, as per our implementations, we observe that only AT2 satisfies the asymptotic optimality conditions, maintaining the anchor function close to \(0\), in addition to maintaining the equality of the normalized indexes.

In this section, we consider the following two examples:

1. _Gaussian bandit._ In the first setup (Figure 4), we consider a \(4\) armed Gaussian bandit with unit variance and mean vector \(\mu=[10,8,7,6.5]\). This is the same setting as in Section 6 from the main text.
2. _Bernoulli bandit._ In the second setup (Figure 5), we consider a Bernoulli bandit with means \(\mu=[0.91,0.73,0.64,0.59]\).

In Figures 4 and 5, we plot the evolution of anchor function value for the three algorithms in the two settings, without implementing the stopping rule. The solid lines in the figure represent the average of anchor function over \(4,000\) independent runs. The shaded bands around the sold lines (almost invisible in these figures), represent 2 standard deviation bands around the mean.

We observe that only AT2 maintains the anchor function value close to \(0\). Our experiments suggest that that TCB algorithm, as per our implementation, doesn't satisfy the asymptotic optimality conditions.

### Sample complexity comparison

In Section 6 in the main text, we compared the sample complexities (SC) of the three algorithms on a well-separated Gaussian bandit (Figure 2). In this section, we compare the SC of all the algorithms, as a function of different parameters. We consider harder Gaussian as well as Bernoulli bandit instances (with means close to each other), presented below.

1. _Gaussian bandit._ A \(4\) armed Gaussian bandit with unit variance and mean vector \(\mu=[7.25,7.05,7,7.1]\) so that the means are closer together.
2. _Bernoulli bandit._ As a second example, we consider a \(4\)-armed Bernoulli bandit with close-by means: \(\mu=[0.99,0.96,0.95,0.97]\).

**Experiment 2 (SC as function of \(\beta\)):** In this experiment, we compare SC of (I)AT2, (I)TCB, \(\beta\)-EB-(I)TCB algorithms, for \(\beta\in[0.2,0.3,0.4,0.5,0.6,0.7,0.8]\), on the Gaussian instance (Figure 6) and the Bernoulli instance (Figure 7), described above. The error probability \(\delta\) in both these experiments is set to \(0.001\). All the algorithms use the same forced exploration rule and stopping rules.

[MISSING_PAGE_FAIL:73]

**Experiment 3 (SC as function of \(\delta\)):** In Figure 9 and Figure 9, we plot the sample complexities of the three algorithms -- AT2, \(0.5\)-EB-TCB, and TCB -- as a function of \(\delta\), for the Gaussian and Bernoulli bandits considered in Experiment 2 above. All the algorithms use the same forced exploration and stopping rules. We observe that AT2 consistently outperforms both the previously known algorithms, and the gap in performance increases as we reduce \(\delta\).

**Experiment 4 (SC as a function of number of arms).** We plot the number of samples needed by the three algorithms, as a function of number of arms in the bandit instance. \(\delta\) is set to \(0.001\) in this experiment.

For scalability, in this experiment, we consider a simple Gaussian bandit (well-separated means) with all arms having a unit variance. Arm 1 is optimal with mean \(10\). To study the effect of number of arms on sample complexity, we choose all the other arms to be same with mean \(8\). Thus, the bandit instances have Gaussian arms with unit variance, and means

\[\mu=[10,8,\dots].\]

As in the earlier experiments, for fair comparison, all the algorithms are implemented with the same forced exploration and stopping rules. Results are presented in Figure 10. We observe that the sample complexity increases linearly with number of arms for the three algorithms. In this experiment, the performance of TCB and AT2 looks comparable, with TCB requiring slightly more number of samples. However, the gap in their performance is expected to increase for smaller values of \(\delta\).

### Runtime comparison

**Experiment 5:** In this experiment, we compare the run-time of (I)AT2 and (I)TCB algorithms on a 4 armed Gaussian bandit with means \(\mu=[10,9.4,7,6.5]\) and unit variance, averaged over longer

[MISSING_PAGE_FAIL:75]

### Experiments for bandits with bounded-support distributions

In this section, we experimentally demonstrate the performance of a natural extension of AT2 to a non-parametric setting of bandits with arms having distributions supported in \([0,1]\) (see Appendix I for the modified AT2). This is the setting considered in, for example, [16].

**Experiment 7:** Consider a \(4\)-armed bandit with the following arm distributions:

\[\mathrm{Beta}(1.5,1),\ \mathrm{Beta}(2,6),\ \mathrm{Beta}(1,1.5),\ \mathrm{and}\ \ \mathrm{Beta}(1,7).\]

Here, the arms have means

\[\mu=[0.6,0.25,0.4,0.125].\]

While we do not provide the analysis of the algorithm for this setting, numerically we observe that even in this non-parametric setting, extension of AT2 to this setting outperforms \(\beta\)-EB-TCB, and a corresponding natural extension of TCB to this setting.

**Reproducibility:** Our code is implemented in Julia 1.7.1, and the plots are generated with the Plots.jl package. Other dependencies are listed in the Readme.md file, which also includes instructions to reproduce the figures and tables presented here. We build upon the publicly available code for [16]. Our experiments are conducted on an institutional cluster computing facility having an Intel Xeon Gold 6130 2.1GHz CPU with 32 cores.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The optimality of the proposed algorithm is proven in Theorem 3.1. We also demonstrate the dynamics of the algorithm, as proposed in Theorem 4.1 via numerical simulations.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of the results presented in this paper due to model assumptions such that independence, considering bandit instances from an SPEF, etc. at the end of Section 1.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We include all the assumptions made in the statements of the results. We provide all the formal proofs in Appendix, and refer to them. All the theorems/lemmas/propositions are numbered and referenced appropriately.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our key contribution is a new algorithm with its theoretical guarantees. We perform numerical simulations to support the theoretical results. We include all the implementation details with the choices for different parameters in our numerical experiments discussion in the Appendix J.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We include the code with scripts for reproducing the numerical results.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include the choices of all the parameters in each simulation.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We include mean and standard deviations in all our experiments, along with the assumptions made (independence of different runs of same simulation).

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We use an institutional computing facility for our experiments (see, end of Appendix J).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The results of this work are theoretical in nature. We appropriately reference all existing works to the best of our knowledge. We include justifications for all the results, and implementation details for reproducing the simulation results of this work.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The main contributions of this work are theoretical in nature. While we propose a new algorithm, there are no harmful societal impacts or consequences of this research.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We run numerical simulations. We do not use any publicly available datasets for our experiments.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please see paragraph on Reproducibility in Appendix J.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release any new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowd sourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects.