# SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models

Hongxin Li\({}^{*,1,2}\), Jingran Su\({}^{*,3,4}\), Yuntao Chen\({}^{\dagger}\)\({}^{3}\), Qing Li\({}^{\dagger}\)\({}^{4}\), and Zhaoxiang Zhang\({}^{\dagger}\)\({}^{1,2,3,5}\)

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)

\({}^{2}\)State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)Center for Artificial Intelligence and Robotics, HKISI, CAS

\({}^{4}\)The Hong Kong Polytechnic University

\({}^{5}\)Shanghai Artificial Intelligence Laboratory

###### Abstract

Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/.

## 1 Introduction

The ability to intuitively direct sophisticated software through natural language has long been an aspiration pursued across generations. With the emergence of Large Language Models (LLMs) that can comprehend and respond to human directives, this vision is within closer reach now than ever. LLMs augmented with tools [24, 25, 20, 26, 2, 27] and reasoning abilities [32, 18, 30, 31] have shown promising results in recent research.

While LLMs continue advancing rapidly, their ability to interoperate seamlessly with existing software tools remains under-explored and not fully understood. Enabling LLMs to harness the rich functionality of countless existing software tools could unlock nearly limitless potential [21].

Progress in endowing LLMs with the ability to direct complex software systems has been hindered by the lack of both a standardized framework for model-application interaction and a comprehensive benchmark for evaluating their performance. Several challenges exist in designing a framework to facilitate interaction between LLMs and sophisticated software applications, including 1) Translating the complex internal state and vast functionality of applications into text forms comprehensible formodels [32]. This requires determining how to systematically represent software interfaces and logic through natural language; 2) Enabling models to generate software commands and parameters accurately and safely [9; 2]. Mechanisms must exist for validating, debugging, and as needed, rejecting or revising model outputs to avoid undesirable operations or states; 3) Providing models with means of monitoring software state changes, exceptions, and errors during multi-step tasks so that these models can respond appropriately. Models are required to understand software feedback and diagnose issues, and adjust directives as needed to robustly accomplish goals. In addition, enabling LLMs to direct complex software also requires curating datasets that capture the diversity and ambiguity of real-world language use, as well as developing automated techniques to reliably evaluate model performance at scale [7].

To systematically investigate the substantial challenges in developing natural language interfaces for software control, a robust application platform is required; as the most pervasive and multi-functional productivity tool, the spreadsheet serves as an ideal substrate for this work. To this end, we propose a general framework for facilitating interaction between language models (LMs) and software applications, along with an agent called **SheetCopilot**. As shown in Fig. 1 SheetCopilot understands high-level spreadsheet manipulation requests expressed in natural language. It decomposes these complex requests into step-by-step plans, and issues commands to automatically carry out the necessary operations using the spreadsheet application. In addition to our spreadsheet-manipulating agent, SheetCopilot, we propose a dataset consisting of complex, interactive spreadsheet manipulation requests expressed through natural language and an evaluation framework with automated metrics to assess how accurately models comprehend requests, devise optimal plans, and perform operations through the spreadsheet interface. We believe robust measurement is key to accelerating progress in this area.

Our agent SheetCopilot achieved substantial capabilities for guiding spreadsheet software through natural language. It generated fully executable command sequences for 87.3% of problems in our benchmark suite and produced completely correct solutions for over 44.3% of tasks--surpassing the traditional programming approaches by a wide margin. To rigorously assess model performance, we curated a dataset of 221 representative spreadsheet tasks collected from superuser.com, including verified solutions created by the authors for each task.

We present three primary contributions to the goal of achieving sophisticated interfaces between language models and traditional software applications:

* We proposed a general framework for facilitating model-software interaction along with SheetCopilot, an agent specialized for spreadsheets that translates high-level, task-oriented requests expressed in natural language into executable command sequences.
* We developed comprehensive resources for systematically evaluating model and interface performance, including a benchmark suite of interactive spreadsheet tasks reflecting real-world requests and a fully automated pipeline for measuring how accurately models comprehend complex prompts, devise optimal plans and execute operations through the software interface.
* We conducted an in-depth assessment benchmarking the abilities of leading LLMs in this challenging domain. The experiments show that LLMs equipped with our method significantly outperform the strong code generation baseline.

## 2 Related Works

**Tool-augmented Large Language Models** Recently, the impressive performance of LLMs has sparked significant interest in exploring the task-planning capabilities of LLMs in various fields. Benefitting from the internalized knowledge about the world [1], a number of works have managed to enable LLMs to solve tasks by following instructions. One line of research [14; 3; 16; 15; 8] has utilized prompt engineering to elicit a sequence of mid-level steps from LLMs for household robotic tasks. To ground LLMs in the real world, these works have used auxiliary models [3; 16; 15] or trained LLMs via mixing visual-language data and embodied data [8]. Another promising direction is to connect LLMs with external tools [25], such as a web browser [22], HuggingFace model hub [26], chemical software [2], PowerPoint [20], and even a tool library [25; 24]. These works employ LLMs to generate action sequences which are further parsed into API calls of the tools. Compared with these works, our work is targeted at spreadsheet manipulation, which is a common demand in almost all scenarios (e.g. economics, management, and research). To the best of our knowledge, limited research has been conducted on benchmarking the capability of LLMs for spreadsheet manipulation.

**Natural Language Processing (NLP) for Spreadsheets** Several studies [12; 11; 28; 6; 17] have investigated the feasibility of using NLP methods to guide the manipulation of Excel sheets. An early work is Flash Fill [11], which automates string processing tasks using program synthesis by example. NLyze [12] utilizes a translation algorithm to convert a user's natural language instruction to a ranked set of likely programs. Inspired by the success of Codex [5] and AlphaCode [19], one recent study [17] has explored the use of LLMs for generating Excel formulas given natural language descriptions of the desired output. They compared the performance of several state-of-the-art LLMs, including GPT-3 and T5, and found that these models can generate accurate formulas with high efficiency. However, this study focused on formula generation rather than general sheet control tasks. In this paper, we aim to address this gap by benchmarking the capability of LLMs for sheet control tasks.

## 3 Dataset and Evaluation

Prior research on language interfaces for spreadsheet control [12; 6; 17] has focused primarily on limited subsets of tasks like formula generation and lacked comprehensive, standardized means of evaluation. To address this issue, we aim to construct a high-quality evaluation benchmark as a foundation for assessing the spreadsheet control capabilities of LLM-based agents.

Our dataset compilation procedure incorporates gathering tasks and worksheets from the Internet, filtering low-quality or irrelevant tasks, consolidating redundant entries, adapting seed tasks, and manually annotating a core set. The end product is a comprehensive and cleanly-labeled collection of spreadsheet-related tasks. We also report statistics and analysis to characterize the dataset properties,

Figure 1: We maneuver SheetCopilot to control software such as Microsoft Excel, generate step-by-step solutions fulfilling the user’s requirements. In each step, SheetCopilot plans an initial atomic action according to the sheet state and then revises this step using the external document which provides the action usage and examples. Finally, the action with its arguments is extracted from the revised step and submitted to the simulation environment for execution. The entire process on the right shows that SheetCopilot successfully solves the task specified in the instruction using the provided available atomic actions.

guide future work, and set initial baselines. Moreover, we develop an automated, reproducible evaluation framework closely tailored to our curated natural language spreadsheet control dataset. This enables systematically assessing model abilities, gaining insights into current limitations, and driving continued progress in this domain.

### Diverse Seed Task and Workbench Collection

We first scrape all questions with spreadsheet-related tags on www.superuser.com and obtain a raw dataset comprising \(\sim\)16k question and answer (Q&A) pairs. Sourcing questions from SuperUser ensures our task dataset is both comprehensive and representative. As not every question represents a sheet manipulation task we apply keyword-based and LLM-based filters to remove Q&A pairs unrelated to spreadsheet automation, resulting in a remain of \(\sim\)13k pairs. To analyze the distribution of the dataset, we define six task categories: Entry and Manipulation, Management, Formatting, Charts, Pivot Tables, and Formulas. We label exemplar Q&A pairs with at least one category to prompt the language model to categorize each pair, as pairs may belong to multiple categories. To identify representative Q&A pairs, we embed and cluster pairs within each unique category combination. We then choose 67 pairs representing the clustering centers and involving operations supported by our evaluation environment. The spreadsheet tasks described in these pairs are regarded as the seed tasks which capture the most important patterns of our dataset.

To evaluate LLMs, we also collect 28 real-world spreadsheets as our workbench by 1) downloading practical sheets from the Internet, and 2) Generating typical daily-use sheets by hand. These sheets represent common uses such as analyzing sales data, calculating financial metrics, and visualizing data with charts.

### Core Set Collection

The seed tasks cannot be directly used since their original sheets differ from the evaluation sheets. We propose collecting a core dataset by adapting and simplifying the seed tasks to bridge this gap.

**Adaptation**. Inspired by self-instruct [29], we prompt an LLM to adapt the seed tasks according to the detailed descriptions of the evaluation sheets. Specifically, GPT-4 is prompted to change the manipulated elements in the seed tasks to generate new tasks compatible with the evaluation sheets. For instance, GPT-4 can change the data types required to be set or ranges to be modified in the original seed task. In this step, 1669 new task instructions are generated (See Tab. D for examples).

**Simplification**. The adaptations are likely to mention specific formulas and operations. To address this issue, we prompt an LLM to simplify each task by replacing specific expressions with natural spoken language so that the task instruction reads like the fashion of a non-expert user. This step reduces the average token length from \(47.1\) to \(33.8\)1.

Footnote 1: We followed the instruction on https://github.com/openai/tiktoken to count the token number using the model “gpt-3.5-turbo”.

Figure 2: Dataset overview. We present an overview of the core set by showing the wordclouds of the instructions and involved atomic actions. The two clouds show that the core set contains diverse tasks that involve various spreadsheet operations.

To collect a core set, the authors select several tasks for each category combination from the simplified tasks. The authors also act as non-expert users to compose more tasks to enrich the core set, obtaining 221 tasks in total. Finally, multiple reference solutions are collected as the ground truth for each task. See Fig. 2 for the dataset statistics and more in the appendix.

### Task Evaluation by Execution

It is hard to evaluate solutions generated by LLMs through verbatim comparison, as it is likely that multiple solutions can successfully complete a task. A viable approach is assessing whether the final sheet state after executing the solution meets the task instruction. We only assess the necessary properties required for the ground truth spreadsheet's operation. For example, in the task "Plot a line chart with the X-axis showing the week and the Y-axis showing sales", we only consider properties related to the chart, ignoring other aspects. To assess an LLM-generated solution, we evaluate the consistency of the necessary properties between the spreadsheet resulting from executing this solution and the ground truth spreadsheet in our evaluation environment. If the necessary properties of the resulting spreadsheet fully match any potential ground truth candidate, the associated solution is deemed correct.

## 4 Method

SheetCopilot enables natural language interactions with spreadsheets. It takes spreadsheets and user tasks described in natural language as input and generates plans to modify spreadsheet contents and create diagrams or tables. We adopt an in-context learning framework inspired by models such as GPT-3 [4]. We propose "atomic actions" - a set of virtual APIs representing common spreadsheet functions. These actions allow language models to accurately interact with the spreadsheet software. We also propose a state machine-based task planning framework to handle the complex, multi-turn interaction between the language models and the spreadsheets. The atomic actions and state machine-based planning framework enable language models to effectively and robustly direct spreadsheet software through natural language.

### Prompting LMs as a SheetCopilot

We design a systematic prompt template to turn LMs into copilots as shown in the left of Fig. 1. Our prompt consists of a general role description, a list of atomic actions with arguments, a set of output requirements, and a multi-round interaction example between a user and an assistant.

The general role description serves as an anchor for enabling LMs to understand the context. A list of atomic actions provides LMs with the interface information needed for task planning. The output requirement tells LMs how to generate texts that can be programmatically extracted and executed. The multi-round example hints LMs how the observe-propose-revise-act loop appears and improves the overall planning quality.

### Atomic Action as A Bridge for LMs and Software

State-of-the-art LMs have shown the superb ability to generate detailed plans for household tasks [16], software control [20], and debugging [5]. However, the generated plans are in natural language which is easy for humans to read but not directly admissible for machine execution.

To overcome the limitation mentioned above, we propose to model the functionalities of existing spreadsheet software as a set of virtual APIs called atomic actions. An atomic action is comprised of an API name, a typed argument list, a usage document string, and several usage examples. These atomic actions can be implemented on different spreadsheet platforms. The example implementations in Tab. H of the appendix show that the atomic actions involve cell value modification, formatting, sheet management, formula and functions, charts, and pivot tables.

Choosing proper atomic action granularity is crucial, as actions must be expressive yet concise to fit in the LM context windows. We determine our atomic actions as follows: 1) Extract all actions involved in the top SuperUser spreadsheet Q&As; 2) Embed and cluster the extracted actions into candidates; 3) Select the minimum set of actions covering all the tasks we collected in Sec. 3.1.

**Relation to Agents Generating VBA Codes** LMs are also capable of generating machine-readable codes [5]. This approach is especially tempting for Microsoft Excel as it comes with an embedded script language called Visual Basic for Applications(VBA). However, the code generation approach faces challenges from both the LMs side and the spreadsheet software side. On the code LMs side, the existing training corpus [10; 13; 5] for code LMs hardly contains VBA source files as it is only a niche programming language compared with C++ or Python. On the spreadsheet software side, software such as Google Sheets, Libre Office, and WPS either do not support VBA at all (Google Sheets) or only support a limited subset of VBA functions (Libre Office and WPS). Therefore, we advocate a more software-agnostic approach that does not rely on embedded programming language support.

### State Machine-based Task Planning

A normal spreadsheet task usually involves several steps, while a sophisticated one often requires over ten steps. Open-loop planning - directly generating a complete task plan from the instruction - becomes exponentially harder as steps increase. Each step changes the sheet state so the correct step \(T+1\) relies on perfectly understanding how the sheet state changes after the previous \(T\) steps. As tasks become more complex, open-loop planning struggles.

We propose a state machine-based planner which revises the plan according to feedback from either LMs or software. Our planner is divided into observing, proposing, revising, and acting stages. The state transition between these stages will be described below. Due to the page limit, please refer to the appendix for examples of complete planning logs.

**Observing Stage** In this stage, we add a brief description of the sheet state \(S_{t}\) to the query, providing information such as the name of each column and the total number of rows for LMs to determine atomic action arguments. This allows LMs to generate solutions in a closed-loop manner by observing the previous actions' consequences without implicitly modeling sheet states.

**Proposing Stage** In this stage, we concatenate the system prompt \(P\), the initial task instruction \(I\), the sheet state \(S_{t}\) and the planning history \(H_{t}\) and ask the LMs to plan the next atomic action \(A_{t+1}\).

\[A_{t+1}=\text{Validate}(R_{t+1})=\text{Validate}(\text{LanguageModel}(P,I,S _{t},H_{t})).\] (1)

The direct response \(R_{t+1}\) from the language model is not always convertible to an admissible atomic action \(A_{t+1}\). Common errors found in the validating step include missing the format requirement, hallucinating undefined actions, and incorrectly determining action parameters.

**Revising Stage** Two ways are adopted to revise a proposed atomic action: a feedback-based one and a retrieval-based one. Feedback-based revision utilizes the error feedback from both the atomic action validation and the spreadsheet software execution. For example, if the atomic action validating step detects a hallucinated atomic action, a new prompt will be created to inform the LM of this error and to ask it to reiterate the available atomic actions. Additionally, we use retrieval-based revision to supply the LM with detailed external knowledge that does not fit in the system prompt due to the context window limit. For example, if the LM uses an atomic action with wrong arguments, a detailed document containing the argument descriptions and usage examples of this action is provided in the new prompt to enhance the probability of the LM correctly determining the atomic action arguments. This process resembles how a human programmer behaves when encountering less familiar APIs.

A special case in the revision stage is that after being supplied with more information about the initially proposed atomic action, the LM suddenly finds that it has chosen a wrong action and decides to return to the revising stage.

**Acting Stage** After the proposing and revising stages, the atomic action \(A_{t+1}\) is submitted to the spreadsheet software for execution.

\[S_{t+1}=\text{SpreadSheetEnv}(A_{t+1},S_{t}).\] (2)

The planning history \(H_{t}\) is updated if the execution succeeds,

\[H_{t+1}=H_{t}\cup\{A_{t+1},S_{t+1}\}.\] (3)

If the software reports a run-time error, the state machine will return to the proposing stage to prompt the LM to re-plan according to the error feedback.

### Hallucination Mitigation

To enable the state machine to less frequently return to the proposing stage due to hallucination-induced errors, we adopt the following means.

**Output Formatting** The underlying functions of atomic actions require precisely formatted planning results. However, we found that LMs probably generate semantically correct yet inadmissible action plans as shown in Fig. 1. Therefore, we require LMs to wrap actions with special tokens (e.g. @) and detect the tokens in the output to check whether the output is correctly formatted.

**Atomic Action Disambiguation** The internalized knowledge in LMs is likely to be confused with the atomic action definitions in the document. Due to this conflict, LMs are prone to self-delusion, which means that it hallucinates undefined actions or adds illegal action arguments [23, 14]. To tackle this problem, the atomic action names are substituted with a set of synonyms that are far away from the official names in an embedding space. For instance, Write and SetConditionalFormat are substituted with RangeInputValue and FormatWithRules, respectively (See the details in the appendix).

## 5 Experiments

The goals of our experiments are threefold: (i) compare representative LLMs on the proposed dataset; (ii) demonstrate that the proposed method improves the success rate and efficiency over a simple baseline; (iii) show the flexibility and stability of our method.

### Benchmark Protocol

**Dataset** The 221 tasks introduced in Sec. 3.2 are used to conduct the following experiments.

**Metrics** Exec@1 measures the proportion of solutions executed without throwing exceptions. Pass@1 is used to evaluate functional correctness [5]. A generated plan is considered correct if the final sheet state completely fulfills the task requirements. Beyond correctness, we propose A50 and A90 scores to measure solution efficiency. These divide the number of atomic actions in a generated plan by the number in the ground truth and then calculate the 50th and 90th percentiles over all tasks. Lower A50 and A90 scores indicate that the LLM tends to use fewer actions to complete a task.

**Models** We adopt leading large language models with public API access, including GPT-3.5-Turbo/GPT-4 from OpenAI and Claude v1 from Anthropic. Details of the models and hyper-arguments used for generation could be found in the appendix.

### Comparing Task Planning Ability of Different LLMs

We compare the three LLMs on the proposed dataset with the same token limit of 4096. For less accessible LLM APIs like GPT-4 and Claude, only 10% of the dataset is used for evaluation. We have maintained the diversity of this subset to avoid data distribution shift (see the appendix for details). The results in Tab. 1 show that GPT-4 demonstrates its strong planning capability by significantly outperforming both GPT-3.5-Turbo and Claude in the Pass@1 and A50/A90. To explain why GPT-4 is inferior in Exec@1, we check the results and find that it is mainly because GPT-4 exceeds the

\begin{table}
\begin{tabular}{l c|c c c c} \hline \hline Data & Models & Exec@1\(\uparrow\) & Pass@1\(\uparrow\) & A50\(\downarrow\) & A90\(\downarrow\) \\ \hline
10\% & GPT-3.5-Turbo & **85.0\%** & 45.0\% & 2.00 & 4.50 \\
10\% & GPT-4 & 65.0\% & **55.0\%** & **1.33** & **2.00** \\
10\% & Claude & 80.0\% & 40.0\% & 1.50 & 4.40 \\ \hline
100\% & GPT-3.5-Turbo & 87.3\% & 44.3\% & 1.50 & 3.00 \\ \hline
100\% & VBA & 77.8\% & 16.3\% & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performances of the compared LLMs and a VBA-based method. The three LLMs exhibit impressive Exec@1 and Pass@1, with GPT-3.5-Turbo achieving the highest Exec@1 and GPT-4 obtaining the best Pass@1 and efficiency. With our method, GPT-3.5-Turbo outperforms the method that generates and runs VBA code by a large margin.

token limit when solving difficult tasks although it has generated correct mid-steps. In contrast, GPT-3.5-Turbo and Claude generate short but incorrect plans for most of these difficult tasks. Claude is slightly worse than GPT-3.5-Turbo in Exec@1 and Pass@1 but exhibits better A50/A90, which shows that Claude is a strong competitor of GPT-3.5-Turbo. See more detailed failure analysis in Sec. F.

To evaluate the category-specific performances, We further break down the subset into the six categories (defined in Sec. 3.1). The four metrics in each category are illustrated in Fig. 3. The radar charts demonstrate that the two GPT models both achieve 100% Exec@1 and Pass@1 in the Management and Entry & manipulation categories. Interestingly, The three models exhibit different patterns of A50/A90: GPT-3.5-Turbo, GPT-4, and Claude reach their best efficiency in the Formula, Management, and Pivot Table category, respectively. This suggests that it is difficult for these models to excel in all task categories.

### Ablation Studies of State Machine

We conduct ablation studies for GPT-3.5-Turbo on the full dataset to analyze the impact of the two types of feedback and external document insertion. The results are shown in Tab. 2.

**A) Closed-loop control generally boosts functional correctness** Individually adding the sheet state feedback at the proposing stage increases the Exec@1 and Pass@1 (rows 3 vs. row 2) since the model no longer needs to implicitly infer the sheet state. Individually adding error feedback obtains the highest Exec@1 (row 4 vs. row 2) as a longer context window can be used to re-plan without the sheet state feedback, increasing the probability of completely finishing a plan. The combination of the two feedback types further improves Pass@1 but at the cost of slightly increasing A50 and A90 (row 7 vs. row 2), probably because the model solves more difficult tasks but with plenty of steps. It is noticeable that without the external document and usage examples, the improvement in Exec@1 becomes narrow and Pass@1 even drops slightly (row 5 vs. row 1). This is because the

\begin{table}
\begin{tabular}{c|c c|c c c|c c c} \hline \hline No. & State feedback & Error feedback & External doc. & Usage examples & Exec@1\(\uparrow\) & Pass@1\(\uparrow\) & A50\(\downarrow\) & A90\(\downarrow\) \\ \hline
1 & & & & & & 56.6\% & 18.6\% & 1.50 & 4.00 \\
2 & & & & ✓ & ✓ & 67.9\% & 18.6\% & 1.00 & 2.50 \\
3 & ✓ & & & ✓ & ✓ & 75.6\% & 24.4\% & 1.50 & 3.00 \\
4 & & ✓ & ✓ & ✓ & ✓ & 92.8\% & 23.5\% & 1.00 & 2.00 \\ \hline
5 & ✓ & ✓ & & & & 68.3\% & 18.1\% & 1.50 & 3.78 \\
6 & ✓ & ✓ & ✓ & ✓ & & 85.5\% & 28.1\% & 1.50 & 3.00 \\
7 & ✓ & ✓ & ✓ & ✓ & ✓ & 87.3\% & 44.3\% & 1.50 & 3.00 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation studies of the observe-propose-revise-act framework proposed in Sec. 4.3. The sheet state and error feedback increase the Exec@1 and Pass@1 when individually applied (rows 3, 4 vs. 2) and bring a significant improvement when both are applied (row 7 vs. 2). Inserting the external atomic action document with usage examples boosts the Exec@1 and Pass@1 and increases efficiency (row 2 vs. 1 and row 7 vs. 5). The synergy of the four components witnesses a large increase (30.7%) in Exec@1 over the baseline (row 7 vs. 1).

Figure 3: The four metrics decomposed in the six categories. The two GPT models both achieve 100% Exec@1 and Pass@1 in the Management and Entry & manipulation categories. The three models obtain their own best efficiency in different categories, suggesting that it is difficult for these models to excel in all task categories.

simple action list in the initial prompt (shown in Tab. 1) fails to provide sufficient information about how to determine correct action arguments even if a detailed sheet description is provided.

**B) Inserting the external document improves both functional correctness and efficiency** Merely adding the external atomic action document enjoys clear improvements of **17.2%** in Exec@1 and **10.0%** in Pass@1 (row 6 vs. row 5). This result demonstrates that presenting this document to the model enables it to less frequently hallucinate illegal actions (which improves Exec@1) and to more accurately determine the action arguments according to the sheet state (which improves Pass@1). Further adding usage examples in the document reaches the highest performance (row 7). Additionally, without any feedback, the improvements over the baseline become narrower (row 2 vs. row 1) since it is hard to determine correct action arguments without knowing the exact properties of the spreadsheet elements even if more details of atomic actions are provided. Adding the external document and usage examples reduces A50/A90 (row 2 vs. row 1 and rows 6, 7 vs. row 5), showing that more information about atomic action usage helps to avoid redundant steps.

**C) SheetCopilot surpasses the VBA-based method** To further demonstrate the advantages of our method, we compare it with a method that generates and runs VBA code. Tab. 1 shows that SheetCopilot with GPT-3.5-Turbo as its backend outperforms the VBA-based method by a large margin, increasing the Exec@1 by **9.5%** and Pass@1 by **28.0%**. This result shows that prompting powerful LLMs with our method to control spreadsheets is a better alternative to directly translating natural language requests to VBA code.

### Ablation Study of Atomic Action Names

To inspect the potential confusion problem stated in Sec. 4.4, we conduct another ablation study for GPT-3.5-Turbo on the full dataset by comparing the impact of adopting the official names and the synonyms. The results in Tab. 3 surprisingly show that using the synonyms increases the Pass@1 and obtains lower A50/A90, which means that the model learns to use the synonyms to generate more correct solutions with fewer steps. This result demonstrates the flexibility of our method: it is possible for users to define their own atomic actions and prompt LLMs to use them.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Models & Exec@1\(\uparrow\) & Pass@1\(\uparrow\) & A50\(\downarrow\) & A90\(\downarrow\) \\ \hline Official names & 87.3\% & 44.3\% & 1.50 & 3.00 \\ Synonyms & 86.9\% & 45.2\% & 1.33 & 2.79 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study of the atomic action names. Utilizing the synonyms far away from the official names brings an increase in Pass@1 and slightly better efficiency (lower A50).

Figure 4: Stability experiments results obtained by conducting evaluation 3 times for each temperature except 0.0. The line charts show that SheetCopilot achieves stable performances even if the GPT-3.5 API temperature changes from 0.0 to 1.0.

### The Influence of LLM Temperature on Task Plan Generation

We evaluate the stability of our method by running the full method three times with temperatures from 0.0 to 1.0. The results in Fig. 4 show that the metrics are stable with slight deviations from the values for temperature=0.0.

### Atomic Action at Different Granularity

A natural question is how to determine the granularity of atomic actions, i.e. the number of workbook elements an action manipulates.

To investigate this question, two experiments are conducted: 1) The actions that set chart properties are incorporated into the CreateChart action, and the original separate actions are removed, with the expectation that SheetCopilot will set chart properties when creating charts. 2) In another experiment, SetFormat, which is originally used to set multiple format properties, is split into finer-grained format-setting actions. Please refer to Sec. D.2 for details.

We conduct these two experiments with GPT-3.5-Turbo backend on the chart and format-related tasks. The results in Tab. 4 show that using an integrated CreateChart action to handle chart creation and property setting simultaneously obtains slightly higher efficiency (lower A50 and A90). However, this variant encounters significantly inferior Exec@1 and Pass@1. In contrast, splitting the original SetFormat action witnesses considerable gains in Exec@1 and Pass@1.

After analyzing the results, we found that an integrated CreateChart encounters lower functional correctness as its complex document makes it difficult for SheetCopilot to understand the action usage, thus being less able to correctly determine action arguments. In addition, the lengthy documentation of this integrated action frequently exceeds GPT-3.5's token limit. In contrast, we observed that after splitting SetFormat, the LLM can easily understand the simple finer-grained actions, thereby encountering fewer hallucination cases. See Sec. D.2 for detailed analysis.

These results suggest that it is more desirable to use finer-grained atomic actions instead of integrated high-level actions in terms of functional correctness.

## 6 Conclusion

We propose SheetCopilot, a spreadsheet agent based on the observe-propose-revise-act framework that decomposes a high-level task into step-by-step solutions for software control. To evaluate our agent, we curate a realistic and diverse dataset representing the typical demand of non-expert spreadsheet users. The experimental results show that SheetCopilot can perform spreadsheet manipulation tasks with a high pass rate and acceptable efficiency, outperforming VBA-based methods. The ablation studies show that the closed-loop control and external document insertion used by SheetCopilot bring clear improvements over the baseline and that adopting a set of atomic action names dissimilar to the official names achieves a surprising performance gain. We also find that utilizing finer-grained atomic actions instead of integrated high-level actions can notably improve functional correctness. We hope our work provides a useful roadmap for researchers interested in the field of LLM-based autonomous agents and sheds light on future research.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Experiment & Method & Exec@1\(\uparrow\) & Pass@1\(\uparrow\) & A50\(\downarrow\) & A90\(\downarrow\) \\ \hline \multirow{2}{*}{Integrating CreateChart} & Ours full & **91.7\%** & **43.3\%** & 1.25 & 1.67 \\  & Ours + Integrated CreateChart & 79.1\% & 37.2\% & **1.00** & **1.50** \\ \hline \multirow{2}{*}{Splitting SetFormat} & Ours full & 70.7\% & 9.8\% & 2.75 & 6.65 \\  & Ours + Split SetFormat & **80.5\%** & **12.2\%** & **2.00** & **5.60** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study of atomic action granularity on the chart-related and format-related tasks. The results show that using an integrated CreateChart action achieves slightly lower A50 and A90 but encounters significantly inferior Exec@1 and Pass@1. Additionally, splitting the SetFormat into finer-grained format-setting actions leads to higher Exec@1 and Pass@1.

## Acknowledgments and Disclosure of Funding

This work was supported in part by the National Key R&D Program of China (NO. 2022ZD0160102), the National Natural Science Foundation of China (No. 61836014, No. U21B2042, No. 62072457, No. 62006231), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, and the InnoHK Fund.

## References

* [1] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumboya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajhi, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Freeshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2022.
* [2] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. _arXiv preprint arXiv:2304.05376_, 2023.
* [3] brian ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as i can, not as i say: Grounding language in robotic affordances. In _6th Annual Conference on Robot Learning_, 2022.
* [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _ArXiv_, abs/2005.14165, 2020.
* [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.
* Chen et al. [2021] Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. Spreadsheetcoder: Formula prediction from semi-structured context. In _International Conference on Machine Learning_, pages 1661-1672. PMLR, 2021.
* Chiang and Lee [2023] Cheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evaluations?, 2023.
* Driess et al. [2023] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In _arXiv preprint arXiv:2303.03378_, 2023.
* Dziri et al. [2022] Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. On the origin of hallucinations in conversational models: Is it the datasets or the models? In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5271-5285, Seattle, United States, July 2022. Association for Computational Linguistics.
* Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* Gulwani [2011] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. _SIGPLAN Not._, 46(1):317-330, jan 2011.
* Gulwani and Marron [2014] Sumit Gulwani and Mark Marron. Nlyze: Interactive programming by natural language for spreadsheet data analysis and manipulation. In _Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data_, SIGMOD '14, page 803-814, New York, NY, USA, 2014. Association for Computing Machinery.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. _ArXiv_, abs/2105.09938, 2021.
* Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 9118-9147. PMLR, 17-23 Jul 2022.
* Huang et al. [2023] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding: Guiding text generation with grounded models for robot control. _arXiv preprint arXiv:2303.00855_, 2023.
* Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. Inner monologue: Embodied reasoning through planning with language models. In _6th Annual Conference on Robot Learning_, 2022.
* Joshi et al. [2023] Harshit Joshi, Abishai Ebenezer, Jose Cambronero, Sumit Gulwani, Aditya Kanade, Vu Le, Ivan Radicek, and Gust Verbruggen. Flame: A small language model for spreadsheet formulas, 2023.
* Kojima et al. [2022] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 22199-22213. Curran Associates, Inc., 2022.
* Li et al. [2021] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022.
* [20] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. _arXiv preprint arXiv:2303.16434_, 2023.
* [21] Gregoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. _ArXiv_, abs/2302.07842, 2023.
* [22] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.
* [23] Pedro A. Ortega, Markus Kunesch, Gr'egoire Del'etang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, Tom Everitt, Corentin Tallec, Emilio Parisotto, Tom Erez, Yutian Chen, Scott E. Reed, Marcus Hutter, Nando de Freitas, and Shane Legg. Shaking the foundations: delusions in sequence models for interaction and control. _ArXiv_, abs/2110.10819, 2021.
* [24] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. _arXiv preprint arXiv:2303.09014_, 2023.
* [25] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
* [26] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggt: Solving ai tasks with chatgpt and its friends in huggingface. _arXiv preprint arXiv:2303.17580_, 2023.
* [27] D'idac Sur'is, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. _ArXiv_, abs/2303.08128, 2023.
* [28] Xinyu Wang, Sumit Gulwani, and Rishabh Singh. Fidex: Filtering spreadsheet data using examples. In _Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications_, OOPSLA 2016, page 195-213, New York, NY, USA, 2016. Association for Computing Machinery.
* [29] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _ArXiv_, abs/2212.10560, 2022.
* [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [31] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. _ArXiv_, abs/2303.11381, 2023.
* [32] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.

Supplementary Material

### Details of Dataset Collection

#### Details of Seed Task Collection

The details of the keyword-based and LLM-based filters described in Section 3.1 are shown below: **Keyword-based filtering**: Questions containing keywords about irrelevant spreadsheet control tasks (e.g., Visual Basic for Application (VBA), loading and saving files, keyboard and mouse shortcuts, using images and shapes) are removed. **LLM-based filtering**: To easily identify irrelevant question and answer (Q&A) pairs, we introduce the seventh task category, i.e. Invalid, apart from the six categories. With these categories, a set of Q&A pairs are labeled as exemplars to prompt ChatGPT to assign at least one category label to each pair (See Tab. B for the used prompt). After classification, we remove 2432 pairs labeled as Invalid, thereby obtaining a clean dataset containing 13574 pairs annotated with task categories.

We notice that multiple Q&A pairs are likely to refer to similar tasks with different expressions. To identify representative questions, clustering is performed within each combination of task categories (note that a pair may be labeled with multiple task categories). Specifically, the title and text of each question are concatenated as a single paragraph which is then embedded into a 768d vector2. For speeding up computation, all embeddings within a category combination are further projected to 64d vectors using Principle Component Analysis. Finally, these embeddings are grouped into 10 clusters using K-means++, which means that 10 representative questions are selected for each category combination. Clustering all category combinations, 220 representatives are selected from the 13574 pairs.

Footnote 2: The model used for embedding is OpenAI text-embedding-ada-002.

As our evaluation environment does not support excessively complicated tasks (e.g., consolidating data, setting page layout, connecting pivot tables), out of these representatives, we further select 67 that involve operations supported by the evaluation environment. The tasks described in these questions are regarded as the seed tasks that capture the most important patterns of the whole clean dataset.

#### Details of Workbench Collection

To lessen the difficulties of collecting a large-scale task dataset, the evaluation workbooks in the workbench strictly follow three rules: (1) Data in every sheet starts from cell A1; (2) Blank columns within tables are not allowed; (3) Each column is provided with a header on the first row, which means that the records of each column are listed from the second row. To better utilize the internal knowledge of LLMs, we also provide a short paragraph describing the context of each evaluation workbook. This context briefly introduces the usage and domain of the workbook and includes useful formulas that assist in manipulating the workbook. See Tab. A for the contexts of all evaluation workbooks.

#### Details of Core Set Collection

**Adaptation**. During adaptation, GPT-4 is prompted to change the manipulated sheet elements (e.g. ranges, charts, data types) mentioned in the seed tasks. To enable GPT-4 to generate clear task instructions relevant to the evaluation sheets, three types of descriptions are included in the prompt, i.e., a brief sheet summary, column headers, and row numbers. In addition, adaptation exemplars written by the authors are included in the input to teach GPT-4 in a few-shot manner. In the end, a batch of 10 seed tasks to be adapted is listed so that the token limit is fully utilized, which speeds up the adaptation process and saves the cost. See the used prompt in Tab. C and adaptation examples in Tab. D.

**Verification**. Between adaptation and simplification, an extra verification step is required since a number of the adapted tasks are unrealistic and irrelevant to the target sheet. To retain valid proportions, we prompt GPT-3.5 (GPT-4 is not used as it is overly expensive) to classify each task according to four criteria, i.e., realism, clarity, relevance, and completeness. A task is valid only if it simultaneously fulfills the four criteria. GPT-3.5 is asked to describe how each task fulfills the criteria before determining its validity as we empirically find that this method more accurately 

\begin{table}
\begin{tabular}{l l} \hline \hline \multicolumn{2}{c}{Workbook Name} \\ \hline Invoices & My workbook records many inv inv invices made on different dates. \\ SalesRep & My workbook records the monthly sales of all employees. \\ SummerSales & My workbook records the sales of my company in the summer. \\ EntireSummerSales & My workbook records the sales of my company in the summer. \\ ShippingCosts & My company needs to deliver the goods to customers by truck. My workbook records the distances between my customers and four destinations. The per-mile shipping charge is 3.11 with a minimum charge of 75. \\ EntireShippingCosts & My company needs to deliver the goods to customers by truck. My workbook records the distances between my customers and four destinations. The per-mile shipping charge is 3.5 with a minimum charge of 80. \\ PricingTable & My workbook contains two tables: Sheet ”Sheet1” records my transactional data which are the number of rolls of fence sold on certain dates. Sheet ”Pricing Table” is a pricing table used to determine the price per roll according to the range the roll number falls in (The range is bounded by Units From and Unit To). \\ BoomerangSales & My workbook has two tables. Sheet ”Sheet1” records the sales of a boomerang company. Sheet ”Realial Price” lists the retail prices for all products. \\ WeeklySales & My workbook records weekly sales and COGS but the profit has not been calculated. The necessary formula is Profit + Sales - COGS. \\ NetIncome & My workbook records revenue and expense. Net Income = Revenue - Total Expenses. \\ PeriodRate & My workbook records the annual rates of my investments. A year can consist of several periods. \\ Tax & My workbook records the weekly sales of my company and is used to compute taxes. The necessary formulas are as follows: Profit Before Tax = Sales - Total Expenses Before Tax; Tax Expense = Profit Before Tax * Tax Rate. \\ MaturityDate & My workbook records my loans with their lengths in days. \\ StockChange & My workbook records the values of my stocks on two dates. \\ IncomeStatement & My workbook records the yearly accounting data of my company. The necessary accounting formulas are as follows: Gross Profit + Net Sales – Cost of Goods Sold (COGS); Operating Profit = Gross Profit - Operating Expenses; Net Profit = Operating Profit - Tax Expense. \\ IncomeStatement2 & My workbook records the yearly accounting data of my company. The necessary accounting formulas are as follows: Gross Profit + Net Sales – Cost of Goods Sold (COGS); Net sales = Sales - Sales return - Discounts and allowances; Cost of goods sold = Materials charges + Labor charges + Overhead; Gross profit = Net sales - Cost of goods sold. \\ SmallBalanceSheet & My workbook records the total assets, liabilities, and owner’s equity. Here are the necessary financial formulas: Assets = Current Assets + Fixed Assets + Other Assets; Liabilities + Owner’s Equity = Current Liabilities + Long-term Liabilities + Owner’s Equity. \\ SimpleCompoundInterest & My workbook is blank and used to record the interests of my investment. The necessary formulas are as follows: Simple Interest = Principle amount * Year * Interest rate; Compound Interest = Principle amount * (1 + Interest rate) ”Year. \\ FutureValue & My workbook records several investments whose future values need to be calculated according to the formula Future value = Present value * (1 + Annual Interest Rate / # Compound periods) *(Years * \# Compound periods). \\ PresentValue & My workbook records several investments whose present values need to be calculated according to the formula Present value = Future value / (1 + Annual Interest Rate / # Compound periods) *(Years * \# Compound periods). \\ ExpenseReport & My workbook records all aspects of expenses but has not yet been completed. The necessary formulas are as follows: Tax = Subloidal * Tax rate; Total = Subloidal + Tax. \\ DemographicProfile & My workbook records information of respondents. \\ GDPBreakdown & I have two sheets: Sheet ”Sheet1” records economic indicators of countries across the years. Sheet ”Sheet2” records a list of chosen country names. \\ EasyGDPBreakdown & My workbook records the economic indicators of countries across many years. \\ XYScatterPlot & My sheet shows how two variables (Range and Height) change along with the projection angle. \\ VelocityDisplacement & My sheet records velocity against displacement. \\ Dragging & My sheet records data from an experiment where one hanging block (m2) drags a block (m1=0.75 \\  & kg) on a frictionless table via a rope around a frictionless and massless pulley. \\ RampUpAndDown & My sheet records the accelerations of a block in two physical scenarios but has not been completed. \\  & One scenario is in columns A to B while the other is in C to D. \\ \hline \hline \end{tabular}
\end{table}
Table A: The names and contexts of the collected workbooks. A context describes the usage and domain of the corresponding workbook as well as useful formulas used to manipulate the workbook.

identifies invalid tasks. Finally, 1515 valid tasks (\(90.7\%\)) remain. See Tab. F for the used prompt and an example of using GPT-3.5 to verify the tasks.

**Simplification**. As the adapted task instructions are likely to refer to specific functions and operations built in Excel, these instructions need to be simplified to read like the tone and fashion of a non-expert user. We establish the following rules for prompting GPT-3.5 to simplify the tasks: 1) Convert specific mentions to natural language descriptions and remove redundant words while retaining the original intention and order of actions. 2) Avoid referring to existing columns by the column indices since it is more natural to refer to a column by the column header. 3) Mention the insertion place and add a column header when inserting a new column to avoid ambiguity. 4) Finally, use domain-specific knowledge to diversify the expression of the generated instructions. See Tab. G for the prompt and an example.

After these steps, a raw dataset containing diverse, natural, and realistic tasks is produced. To extract a core set from the simplified tasks, six random tasks are selected for each category combination, resulting in 402 selected tasks. The authors polish these selected tasks by further revising them and discarding a fraction not supported by the simulator, resulting in 184 remaining tasks. To enrich the core set, the authors act as non-expert users to compose 37 more tasks. Lastly, 221 tasks exist in the core set. See Fig. A for the instruction length and atomic action distributions, Fig. B for the proportions of the six task categories and the task diversity of the core set, and Fig. C for the numbers of each category combination.

The final step is to prepare reference solutions for these tasks. To objectively judge the performance of LLMs, we use GPT-3.5 to generate multiple solutions for each task and then retain the successful ones after verifying the final s by hand. As a number of solutions are likely to exist for a task, multiple reference solutions are collected as the ground truth for one task.

### Selecting the 10% Dataset Used for Comparing the LLMs

20 tasks are selected from the 221-task core set to approximately maintain the percentages of the six categories and the distribution of the numbers of atomic actions. This collection basically represents the pattern of the core set, avoiding a data distribution shift to a large extent. See Fig. D for the statistics of this subset.

## Appendix B Atomic Action Names

### Collecting Atomic Actions

Selecting a set of indivisible and easy-to-understand atomic actions is necessary since LLMs need to generate an interpretable solution to the user-specified task. As no existing atomic actions used in spreadsheet software have been curated and publicized, we obtain the names of these actions during classifying the SuperUser Q&A pairs (Described in Sec. A.1). Specifically, ChatGPT is prompted to determine which atomic actions each Q&A pair requires to complete the task, which resembles a text summarization problem. As multiple similar names are likely to be generated for an atomic action, we

Figure A: The distributions of the instruction lengths and the numbers of atomic actions involved in each instruction. The two histograms demonstrate the diverse task complexity of the core set.

[MISSING_PAGE_FAIL:17]

\begin{table}
\begin{tabular}{|l|} \hline (Requirements for adjtation) \\ As an Exed expert, you have been assigned the responsibility of adapting a set of task instructions for specific Exed workbooks. These instructions will be utilized to evaluate the Excel manipulation capabilities of large language models. \\ Requirements: \\
1. First, identify individual atomic actions used in the original instructions, then develop new instructions incorporating those actions. \\
2. Use the detailed descriptions of the provided workbooks to modify the original instructions so that they become compatible with the workbooks. Specifically, you must change the manipulated objects (targets, horses, trovs, and columns) in the original instructions. To run this note change the way you use atomic actions. For instance, if the original instruction sets the data type as accounting, you can change to other types in the adaptation. \\
3. Use standard range references, such as “Sheezial"C1-57,"A-216,"A-Filium,"C-or two for \\
4. Use different phasing (e.g., various sentence structures and workflows) to create diverse instructions. \\
5. Apply domain-specific knowledge to diversity the generated instructions. For instance, use financial knowledge to calculate various metrics, demonstrate trends in product sales fusion different perspectives, visualize data using various types of data, and so on. \\
6. Imputation (T) generated instructions that describe realistic tasks and inverse the normal use of atomic actions according to the workbook descriptions. \\
7. In every work instructions, we hules and prove that we created a new workbook and start from A1. Only one new sheet is allowed to be created. Besides, the headers of new columns/rows and the names of new sheets must be. \\
8. The instructions after adaptation should look like what a non-expert Excel user would say and should not mention any specific functions or operations built in Excel. \\ (Available atomic actions) used to label the adaptation results. \\ Here are the atomic actions you can identify within the six categories: \\ A. Entry and manipulation. \\ \end{tabular}
\end{table}
Table C: An example of using the adaptation prompt to adapt the seed tasks. For clarity, the prompt components and an example GPT-4 response are marked with brief descriptions in blue.

Figure 10: The distribution of the category combinations.

Figure 11: The proportions of the six categories. Right: Diversity of the verb-noun phrases in the core set. We demonstrate the diversity of the core set by showing the top 10 most frequent root verbs (the inner circle) and their direct noun objects (the outer circle) in the instructions.

\begin{table}
\begin{tabular}{l l} \hline \hline \multicolumn{1}{c}{Seed tasks} & \multicolumn{1}{c}{Adaptions} \\ \hline \multicolumn{1}{c}{\({}^{\star}\) Count the number of values that appear more than once in column A.} & \multicolumn{1}{c}{\({}^{\star}\) In a new cell, count the number of “Product” [Column C) values in \({}^{\star}\) \\ \multicolumn{1}{c}{\({}^{\star}\) Shéert” that appear more than once.} \\ \multicolumn{1}{c}{\({}^{\star}\) Add loading zeros to every number in column A then append them to the texts in column B.} & \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; format the years in column A from “Sheetz” as and append the corresponding Gross Proft value [Column B] from “Sheetz”.} \\ \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating in a column based on two other columns.} & \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating to cells in column A based on the cell value in column B. If the value in A is greater than the value in B, display the entry, the value in column A in a net.} \\ \multicolumn{1}{c}{\({}^{\star}\) If use for groups of data each occupying two columns. If it like to have & \multicolumn{1}{c}{\({}^{\star}\) Create a Proft Table in a new sheet named “Sheetz” to show the sum of distinct for each customer to each destination, and check one across selected column charts stuck together that share the same y-axis.} \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Simplification examples.** The simplification results demonstrate three features: (1) using brief and clear references to operated ranges; (2) expressing the intention with spoken language instead of directly mentioning Excel built-in functions; (3) considerably reducing the sentence length.

\begin{table}
\begin{tabular}{l l} \hline \hline \multicolumn{1}{c}{Adaptations} & \multicolumn{1}{c}{Simplification Results} \\ \hline \({}^{\star}\) Calculate the sum of “Quantity” [Column E) in “Sheetz” [ONLY if the “Product” [Column C) in “Sheetz” includes “Product” [Column A) in “Retail Price’ sheet. & \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; use INDEX and MATCHI to retrieve “slice information from “Sheetz” based on the row IDs provided in a list on another sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days difference between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; use INDEX and MATCHI to retrieve “slice information from “Sheetz” based on the row IDs provided in a list on another sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days difference between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days difference between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; use INDEX and MATCHI to retrieve “slice information from “Sheetz” based on the row IDs provided in a list on another sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days difference between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; format the years in column A from “Sheetz” as and append the corresponding Gross Proft value [Column B] from “Sheetz”.} \\ \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating to cells in column A based on the cell value in the cell value in column B. If the value in A is greater than the value in B, display the value in column A in a net.} \\ \multicolumn{1}{c}{\({}^{\star}\) If have for groups of data each occupying two columns. If it like to have & \multicolumn{1}{c}{\({}^{\star}\) Create a Proft Table in a new sheet named “Sheetz” to show the sum of distinct for each customer to each destination, and check one across selected column charts stuck together that share the same y-axis.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a new sheet named “Sheetz”; format the years in column A from “Sheetz” as and append the corresponding Gross Proft value [Column B] from “Sheetz”.} \\ \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating to cells in column A based on the cell value in column B. If the value in A is greater than the value in B, display the value in column A in a net.} \\ \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating to cells in column A based on the cell value in column B. If the value in A is greater than the value in B, display the value in column A in a net.} \\ \multicolumn{1}{c}{\({}^{\star}\) Apply conditional formating to cells in column A based on the cell value in column B. If the value in A is greater than the value in B, display the value in column A in a net.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) In a column (Column H) with header “Days from Today”, calculate the number of days difference between the “Date” [Column A) and today’s date.} \\ \multicolumn{1}{c}{\({}^{\star}\) Find the total quantity sold for each product listed in the “Retail Price’ sheet.} \\ \multicolumn{1}{c}{\({}^{\star}\) Retail Price’ sheet.

\begin{table}
\begin{tabular}{|p{284.5pt}|} \hline (Requirements for verification) \\ I will give you a batch of Excel task instructions that are utilized to evaluate the spreadsheet manipulation capabilities of large language models. Please check all instructions according to the criteria and the descriptions of the given workloads. \\ Requirements: \\ \hline
1. If an instruction fulfills all of the following four criteria strictly at the same time, it is valid. \\ A. Realism: Verify that the instructions are representative of real-world Excel problems encountered by expert users. Avoid including contrived or overly complex tasks that would rarely be encountered in practice. \\ B. Relevance: Verify that the instructions are relevant to the context of the given workloads. Reject instructions that do not relate to the content of the workloads or are not applicable in the context of Excel. \\ C. Clarity: Verify that the instructions are clear and easy to understand. They should be free from grammatical errors and ambiguities. \\ D. Completeness: Verify that the instructions can be completed using the provided workload data and Excel features. If additional data or functionality is needed to accomplish a task, reject the instruction. \\ \hline
2. Use your domain-specific knowledge to reason and make decisions. For example, it is unlikely to calculate monthly or yearly averages of some physical values because this task is impractical. \\ \hline (Isomething for verification. The sheet description is provided to more accurately verify the fulfillment of the four criteria) \\ I will give you an example first: \\ Given an Excel webbook: \\ My workload records many invocies made on different dates. Sheet "Sheet" has "columns (Headers are A. "No.", B." Date", C. "Salesman", D. "Product", E. "Price", F. "Unary", G.", "Sales") and "fy" (nothing the header rows). The cells in the "No." column range from 10500.00 to 10505.00. The cells in the "Date" column can be 2011-05-20 (800,000). "2011-05-20 (900,000)." 2011-05-20 (900,000)." 2011-05-20 (900,000)." The cells in the "Salesman" column on the "Joo", "Chin", "Moe". The cells in the "Product" column are the "Quad", "Magnetic", "Belher", "Carlup", "Alique". The cells in the "Price" column range from 22.00 to 32.00. The cells in the "Luna" column range from 2.00 to 750.00. \\ \hline \end{tabular}
\end{table}
Table 10: An example of using the verification prompt to classify the adapted tasks. For clarity, the prompt components and the GPT-3.5 response are marked with brief descriptions in blue and the redundant contents are substituted with ellipsis.

[MISSING_PAGE_FAIL:22]

\begin{table}
\begin{tabular}{c c c} \hline \hline Category & Official Name & Synonym \\ \hline  & Write & RangeInputValue \\  & Delete & DiscartRange \\  & InsertRow & NewRowAtIndex \\  & InsertColumn & ColumnCreation \\  & AutoFill & RangeValueTransfer \\ Entry and manipulation & CopyPaste & ReplicateRange \\  & FindReplace & AlterRange \\  & SetHyperlink & LinkRangeAssociator \\  & RemoveDuplicate & DistinctData \\  & CreateSheet & WorksheetCreation \\  & Clear & EraseRangeContents \\  & Sort & AdvancedRangeSort \\ Management & Filter & SmartRangeSelector \\  & CreateNamedRange & SetRangeName \\  & FreezePanes & LockRowsColumns \\  & SetFormat & CustomizeFont \\  & SetDataType & RangeTypeFormater \\ Formatting & Merge & ConcatenateCells \\  & ResizeRowColumn & RangeDimensionAdjuster \\  & SetConditionalFormat & FormatWithRules \\  & SetCellLock & ProtectActiveSheet \\  & CreateChart & GraphConstructor \\  & CreateCharlFromPivoTable & CreatePivoGraph \\  & SetCharTitle & ChartTitleSettings \\  & SetCharAxis & CustomizeAxisAttributes \\  & SetChartHasAxis & AxisDisplayManager \\ Charts & SetCharlLegend & LegendConfiguration \\  & SetCharType & ChartTypeSwitch \\  & SetCharMarker & DefineSeriesMarker \\  & SetCharlTrendline & TrendlineAdjustments \\  & AddDataLabels & DisplayChartDataLabels \\  & AddErrorBars & ErrorBarsIntegration \\  & CreatePivoTable & PivorTableConstructor \\  & SetPivoTableSummaryFunction & PivorFunctionChange \\  & Date and time functions & \\  & Logical functions & \\  & Lookup and reference functions & \\ Formula & Math functions & N/A \\  & Statistical functions & \\  & Text functions & \\  & Financial functions & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The official names and synonyms of the atomic actions used in our method. The Formula category contains the types of formulas used for calculation in the core set tasks and does not represent specific operations.

project all generated action names into 768d embeddings using the OpenAI embedding model3 and then reduce these embeddings to 64d vectors before performing Kmeans++ clustering. This process produces 80 clustering centers each of which represents a group of semantically similar action names. After checking the names corresponding to these centers by hand, we find that these names are the official names used for the built-in features of Microsoft Excel. According to the potential use of atomic actions in the core set tasks and the implementation restrictions of the evaluation environment, a proportion of these action names (44 out of 80) are used by our SheetCopilot (Listed in the initial prompt and used in the external document). These 44 atomic action names are used for conducting the comparison experiment (Sec. 5.2), state machine ablation study (Sec. 5.3), and the stability test experiment (Sec. 5.5). Please see Tab. H for these atomic action names.

Footnote 3: The model used for embedding is OpenAI text-embedding-ada-002.

### Collecting Synonyms for the Official Atomic Action Names

To investigate the confusion issue mentioned in Sec. 4.4, we attempt to adopt a different set of atomic action names. Specifically, we use GPT-4 to generate 10 candidate synonyms for each atomic action name according to the low-level implementation of the action. Then, we adopt as the new name the candidate farthest from the official name in terms of the cosine similarity in the embedding space4. These synonyms (shown in Tab. H) are used in the ablation study in Sec. 5.4 to investigate the impact of adopting different sets of names.

Footnote 4: The model used for embedding is the same as above.

## Appendix C Details of SheetCopilot Implementation

We introduce the implementation of the state machine our SheetCopilot is based on and the prompt used to query LLMs to generate task solutions.

### State Machine Implementation

The Observe-Propose-Revise-Act framework is implemented using a state machine shown in Fig. E. At the beginning of each step, the Observing stage enables the LLM to receive the sheet state before planning an action in the Proposing stage. Since the LLM is likely to output an incorrect action, an external document of the initially planned action is inserted into the query to prompt the LLM to revise the action. If the initially planned action is invalid, a validation error will occur and then the state machine will return to the Proposing stage; If the revised action is invalid, a validation error will also occur but the state machine will return to the Revising stage. If the revised action has been validated, the action will be submitted to the Acting stage for execution. The revised action is still probably erroneous, causing run-time errors in the software. In this case, the state machine will return to the Revising stage to prompt the LLM to re-plan according to the error information until the execution is finished. The entire process is repeated until the LLM outputs "Done!".

### Prompting LLMs to Generate Task Solutions

The initial prompt used to query the LLM to generate step-by-step solutions is shown in Tab. I. The prompt is formatted as a multi-turn dialog according to the usage of OpenAI ChatGPT API, with each turn comprising a content field and a role field. The prompt consists of an overall task goal, a list of atomic actions, requirements, an exemplar, and finally a task to be solved.

## Appendix D Additional Ablation Studies

### Qualitative Experiment of Missing Atomic Actions

We deliberately remove several atomic actions to test the robustness of our method. Specifically, we remove SetConditionalFormat and test SheetCopilot on a task involving conditional formatting. We also remove another important atomic action - CreatePivotTable - to see whether SheetCopilot is able to adopt other feasible solutions to analyze data. We use GPT-3.5-Turbo as the backend of our SheetCopilot. Fig. F present surprising results: Example 1 shows that SheetCopilot uses a filter to retain the rows fulfilling the task requirement, set the formats of these rows, and finally cancel the 

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_EMPTY:26]

filter to restore the data. Example 2 shows that SheetCopilot cleverly utilizes the SUMIF function to substitute the pivot table. Despite redundant calculation, it obtains functionally correct results.

### Detailed Analysis of Atomic Action at Different Granularity

We provide further details about the ablation studies on the action granularity (Sec. 5.6 in the main paper). In our design, the low-level implementations of the atomic action used by SheetCopilot are consistent with the atomic actions used by Excel in terms of behavior, which means that our atomic actions possess a normal granularity and are familiar to average users. However, one may argue that a high-level atomic action that combines several actions used by our SheetCopilot may bring better performances as using high-level actions possibly improves efficiency.

To investigate this interesting problem, we combine several chart-related atomic actions into one high-level chart manipulation action and conduct an ablation study on the tasks involving charts. Specifically, we incorporate the actions that set chart properties into the CreateChart action and remove the original separate actions, expecting the SheetCopilot to set all required properties on the creation of the chart. Additionally, the usages of these actions are merged and a new example of showing how to use this integrated CreateChart action is also added (See the documents of the separate actions and integrated action in Tab. 1). Additionally, in another experiment, we split SetFormat which originally contains multiple arguments related to setting various format properties to see the impact of using finer-grained format-setting actions (See the documents of the SetFormat action and finer-grained format-setting actions in Tab. 1). The SetConditionalFormat action possesses a function overlapped with that of SetFormat, thus interfering with the experiment. Therefore, we remove the SetConditionalFormat action when conducting this experiment.

We conduct this ablation study with GPT-3.5-Turbo as the SheetCopilot backend on the chart-related tasks (43 out of 221) and format-related tasks (41 out of 221). The results in Tab. 4 show that using a high-level CreateChart action to handle chart creation and chart property setting simultaneously obtains slightly higher efficiency (lower A50 and A90). However, it is surprising that this method encounters significantly inferior functional correctness, with Exec@1 and Pass@1 decreasing by 12.6% and 6.1 %, respectively. Likewise, splitting the original SetFormat action witnesses considerable performance gains in Exec@1 and Pass@1.

After analyzing the chart-related results, we found that using an integrated CreateChart encounters lower functional correctness since its document is so complex that SheetCopilot struggles to understand the action usage, thus being less able to correctly determine all action arguments as required. In addition, the document of this integrated CreateChart action is extremely lengthy, causing the query to frequently exceed the GPT-3.5-Turbo token limit. These results suggest that it is more desirable to use finer-grained atomic actions instead of integrated high-level actions in terms of functional correctness. After analyzing the format-related results, we observed that using the original SetFormat action tends to result in argument hallucination. For example, the LLM frequently adds a non-existing argument such as "criteria=...", trying to use SetFormat in a way similar to the removed SetConditonalFormat. The LLM outputs such erroneous action repeatedly even if the error feedback is provided, finally exceeding the token limit and causing an execution error. In contrast, after splitting the SetFormat action, the LLM is able to easily understand the simple finer-grained actions, thereby encountering fewer hallucination cases.

## Appendix E SheetCopilot Manipulation Example

To better illustrate the strengths of our SheetCopilot, an example of using SheetCopilot (GPT-3.5) to produce solutions to users' requests is shown in Fig. 10. SheetCopilot is asked to complete the sales data, analyze the results, and visualize the trend. SheetCopilot accurately understands the intention of the user's instruction and correctly revises its solution utilizing the external document of the chosen atomic action and the error feedback provided by the evaluation environment.

## Appendix F Analysis of failure cases

To obtain a clearer view of the distinction between the LLMs compared in Sec. 5.2, we perform an in-depth analysis of the failure cases for each LLM. The predefined failure types are as follows:

\begin{table}
\begin{tabular}{l c} \hline \hline \multicolumn{2}{c}{The documents of the finer-grained format-setting actions} & The document of the original SetFormat action \\ \hline SetFormat: & \\ args: “source: stf, font: stf/” & \\ args explanation: & \\ source (string:) The range to stf font font. & \\ font (string:) The foot to stf. & \\ usage: Set font for the source range. & \\ example: & \\ \# Example 1: Set font for the range (A1:B6) to ‘Ariar’. & \\ SetFormat(source”Shee111A1B6”, ‘tense-Ariar’) & \\ \# After implementing this action, the range (A1:B6) will be set to ‘Ariar’ font. & \\ \# After implementing this action, the range (A1:B6) will be set to ‘Ariar’ font. & \\ SetFormatSize: & \\ args: “(source: stf, fontSize: float)” & \\ args explanation: & \\ usage: Set font size for the source range. & \\ example: & \\ \# Example 1: Set font size for the range (A1:B6) to 20. & \\ SetFormatSize(source”Shee111A1B6”, ‘tense-Ariar’) & \\ \# After implementing this action, the range (A1:B6) will be set to ‘Ariar’ font. & \\ SetFormatSize: & \\ args: “(source: stf, fontSize: float)” & \\ args explanation: & \\ usage: Set font size for the source range. & \\ example: & \\ \# Example 1: Set font size for the range (A1:B6) to 1red. & \\ SetFormatSize(source”Shee111A1B6”, ‘colore-red’) & \\ \# After implementing this action, the range (A1:B6) will be set to ‘red’ font color. & \\ SetFormatSize: & \\ args: “(source: stf, fileColor: stf)” & \\ args explanation: & \\ usage: Set fill color for the source range. & \\ example: & \\ \# Example 1: Set fill color for the range (A1:B6) to ‘red. & \\ SetFileColor(source”Shee111A1B6”, ‘illColore-red’) & \\ \# After implementing this action, the range (A1:B6) will be set to ‘red’ fill color. & \\ SetBlod: & \\ args: “(source: stf, hold: bool)” & \\ args explanation: & \\ \hline \multicolumn{2}{c}{} \\ \(\ldots\) & \\ example: & \\ \# Example 1: Set bold for the range (A1:B6). & \\ SetBlod(source”Shee111A1B6”, ‘hold-True) & \\ \# Example 1: Set allot for the range (A1:B6). & \\ \# Example 1: Set bold for the range (A1:B6). & \\ SetBlod(source”Shee111A1B6”, ‘hold-True) & \\ \# Example 1: Set allot for the range (A1:B6) will be set to italic. & \\ \# After implementing this action, the range (A1:B6) will be set to italic. & \\ SetIdentifier: & \\ args: “(source: stf, underline: bool)” & \\ args explanation: & \\ usage: Set underline for the source range. & \\ example: & \\ \# Example 1: Set underline for the range (A1:B6). & \\ SetIdentifier(source”Shee111A1B6”, ‘undermine-True) & \\ \# After implementing this action, the range (A1:B6) will be set to underline. & \\ SetFormatSize: & \\ args: “(source: stf, horizontalAlignment: stf)” & \\ args explanation: & \\ source (string:) The range to stf horizontal alignment. & \\ horizontalAlignment (string:) The horizontal alignment to stf. It can be ‘left’, ‘center’, ‘right’. & \\ usage: Set horizontal alignment for the source range. & \\ example: & \\ \# Example 1: Set horizontal alignment for the range (A1:B6) to ‘left’. & \\ SetFormatSize(source”Shee11A1B6”, ‘tense-Ariar’) & \\ args explanation: & \\ source (string:) The range to stf horizontal alignment. & \\ horizontalAlignment (string:) The horizontal alignment to stf. It can be ‘left’, ‘center’, ‘right’. & \\ usage: Set horizontal alignment for the source range. & \\ example: & \\ \# Example 1: Set horizontal alignment for the range (A1:B6) to 1red. & \\ SetFormatSize(source”Shee11A1B6”, ‘tense-Ariar’) & \\ \# After implementing this action, the range (A1:B6) will be set to ‘left’ horizontal alignment. & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The documents of the finer-grained format-setting actions (left column) and the original SetFormat action (right column). The argument and usage of the original SetFormat are both divided as the arguments and usages of the finer-grained actions. For clarity, the redundant contents are substituted with ellipsis.

Figure 6: SheetCopilot example: Handling sales data. The left column shows that SheetCopilot generates a step-by-step solution according to the sheet state feedback and correctly revises its mistakes using the external atomic action document as well as the error feedback. The incorrect arguments are marked with red rectangles. The right column demonstrates the state changes of the evaluation sheet corresponding to each step on the left. For illustration clarity, only brief documents are displayed.

* **Wrong action** Use wrong atomic actions to attain a specific goal. For instance, hide columns using a filter (a filter can only be used to hide rows) or perform meaningless actions to destroy the data.
* **Wrong range** Select the wrong ranges to manipulate. For instance, fail to use absolute references when required or select incomplete ranges as chart data sources.
* **Certain arguments unset** Perform the atomic actions required by the tasks but fail to set all required arguments. For instance, set the cell background color but forget to set the cell font color.
* **Repeated output** Output the same atomic action repeatedly until exceeding the token limit.
* **Hallucinate data** Hallucinate data to manipulate while ignoring the real data.
* **Incomplete solution** Terminate a solution without fulfilling all task requirements.
* **Disobey requirements** Perform an atomic action in a way that deviates from the task requirements. For example, insert a hyperlink different from the one specified in the task instruction.
* **API inability** Failure caused by the limitations of the low-level implementations of the atomic actions. For instance, the low-level implementation of conditional formatting does not support all types of formulas (e.g. array formulas or formulas available only in a specific software version).

The categorized failure cases of the three LLMs are shown in Fig. H. From an overall viewpoint, we can obtain two **interesting findings**: 1) GPT-3.5-Turbo and GPT-4 share similar patterns of the failure case proportions while Claude demonstrates a largely different failure pattern. This finding suggests that the two GPT models possess almost identical alignment in terms of spreadsheet manipulation and that a gap exists between the alignment of Claude and those of the GPT models. 2) Jointly analyzing the failure cases and the performances in Tab. 1, we can see that although the prompt used by SheetCopilot (See the prompt in Tab. I) is tuned for GPT-3.5-Turbo, Claude exhibits competitive performances compared with GPT-3.5-Turbo.

Inspecting individual cases, we found that GPT-3.5-Turbo tends to use wrong formulas, showing that its ability to predict formulas is weak. GPT-4 is prone to incomplete solutions due to the limited context window. In contrast, Claude encounters no incomplete solutions and less frequent repeated output but tends to use wrong formulas and wrong ranges. One common failure case among the three LLMs is that they often use a combination of UNIQUE and SUMIF/AVERAGEIF/COUNTIF to replace pivot tables. This combination is technically correct but these three models forget to apply absolute reference when selecting manipulated ranges used by auto-filling, leading to incorrect calculation results.

We also check the failure cases made by GPT-3.5-Turbo on the full core set (See the experimental results in Tab. 1) and illustrate the percentages of these cases in Fig. I. The pie chart shows that GPT-3.5-Turbo performs poorly at predicting formulas and determining appropriately selected ranges. These two types of failure occupy over 50% of all cases. Inspecting the wrong formula cases, we

Figure I: The proportions of different failure cases when evaluating GPT-3.5-Turbo on the full coreset.

found that GPT-3.5-Turbo often fails to use absolute references when necessary, uses self-reference, and uses wrong criteria for a filter or conditional formatting. Inspecting the wrong range cases, we found that GPT-3.5-Turbo tends to use wrong auto-filling ranges, use wrong chart data sources, and leave out the headers when copy-pasting columns. Another two prominent types of failure are Wrong action and Incomplete solution, which occupy 33.7% in total. After inspecting the wrong action cases, we found that GPT-3.5-Turbo misunderstands the usage of the atomic actions it incorrectly uses. For example, GPT-3.5-Turbo inserts an unnecessary column to fill in data, ignoring the existing column where the data should be written; GPT-3.5-Turbo also uses a filter to hide columns, which violates the actual usage of a filter. After inspecting the incomplete solutions, we found that GPT-3.5-Turbo fails to fulfill all task requirements in these cases although other steps are correct (e.g. it forgets to set the chart title or to autofill after filling in the first row). In the cases where repeated output occurs, GPT-3.5-Turbo generates the same action repeatedly as it fails to revise its plan according to the given error feedback. GPT-3.5-Turbo occasionally misses setting required arguments or disobeys the task requirements to set incorrect arguments which both make up a small fraction of the failure cases. A small proportion of the failure cases result from the implementation limitations of several atomic actions, which can be fixed by upgrading these implementations.

## Appendix G Demo

Video demos for our SheetCopilot agent can be found on our project website (https://sheetcopilot.github.io).

## Appendix H Limitations

Our SheetCopilot possesses the following limitations:

* We have not optimized the number of tokens used to solve spreadsheet tasks. This means that SheetCopilot consumes a large number of tokens and is only capable of solving short-horizon tasks. Future work is needed to focus on designing efficient prompting methods that save tokens or using LLMs with a larger context window.
* State feedback only consists of the basic information of cells, ignoring the state of charts, pivot tables, filters, and frozen panes. SheetCopilot probably creates charts and pivot tables repeatedly as it is unable to observe the outcome of creating these elements.
* Our dataset is manually curated and is not fully fledged yet. More labor work is required to generate more ground truth as numerous correct solutions to each task probably exist. Moreover, the more solutions are collected to conduct evaluation, the more accurate and stable performances we can obtain.
* Our evaluation environment does not support all Excel built-in features, which restricts the task categories our SheetCopilot is able to handle. More effort is needed to continuously upgrade the evaluation system to accommodate more diverse tasks.

## Appendix I Broader Impact

Our proposed SheetCopilot possesses the potential to significantly simplify the process of spreadsheet control, which positively impacts productivity and efficiency. By enabling users to interact with spreadsheets using natural language, SheetCopilot reduces the time and effort required to perform a wide range of tasks, such as simple data entry and complex data analysis.

However, it is imperative to consider potential negative impacts. One concern is that the increased ease of spreadsheet control possibly leads to an over-reliance on automation, potentially reducing the need for human expertise in certain areas, which may cause unemployment in society. Additionally, there is a risk that the use of SheetCopilot exacerbates existing inequalities in the workforce as those who lack access to SheetCopilot or who are not proficient in using it may be at a disadvantage.

Another potential risk of using SheetCopilot is that a wrong operation is likely to face undesirable consequences when controlling sensitive data forms, such as tax forms or financial statements. This risk highlights the importance of ensuring the robustness and accuracy of the agent's actions, as well as the need for appropriate safeguards and oversight to minimize the risk of errors.

Additionally, SheetCopilot relies on the capabilities of language models, which are known to potentially inherit biases present in the training data. As a result, SheetCopilot may inadvertently exacerbate existing biases when interacting with spreadsheets and processing data. It is crucial to acknowledge the potential for inherited bias from LLMs in the development and deployment of SheetCopilot and to take proactive steps to mitigate any negative impacts on fairness and equity.