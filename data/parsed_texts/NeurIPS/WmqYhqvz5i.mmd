# Contextual Bandits and Imitation Learning with Preference-Based Active Queries

 Ayush Sekhari

MIT

sekhari@mit.edu

&Karthik Sridharan

Cornell University

ks999@cornell.edu

&Wen Sun

Cornell University

ws455@cornell.edu

&Runzhe Wu

Cornell University

rw646@cornell.edu

Authors are listed in alphabetical order of their last names.

###### Abstract

We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as \(O(\min\{\sqrt{T},d/\Delta\})\), where \(T\) represents the number of interactions, \(d\) represents the eluder dimension of the function class, and \(\Delta\) represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of \(\Delta\), and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only \(O(\min\{T,d^{2}/\Delta^{2}\})\) queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length \(H\), and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms (Ross and Bagnell, 2014) that require access to the expert's actions and also reward signals.

## 1 Introduction

Human feedback for training machine learning models has been widely used in scenarios including robotics (Ross et al., 2011, 2013; Jain et al., 2015; Laskey et al., 2016; Christiano et al., 2017) and natural language processing (Stiennon et al., 2020; Ouyang et al., 2022). By integrating human feedback into the training process, these prior works provide techniques to align machine-learning models with human intention and enable high-quality human-machine interaction (e.g., ChatGPT).

Existing methods generally leverage two types of human feedback. The first is the action from human experts, which is the dominant feedback mode in the literature of imitation learning and learning from demonstrations (Abbeel and Ng, 2004; Ziebart et al., 2008; Daume et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017; Osa et al., 2018; Li et al., 2023). The second type of feedback is preference-based feedback, which involves comparing pairs of actions. In this approach, the expert provides feedback by indicating their preference between two options selected by the learner. While both types of feedback have their applications, our focus in this work is on preference-based feedback, which is particularly suitable for scenarios where it is challenging for human experts to recommend the exact optimal action while making pairwise comparisons is much easier.

Learning via preference-based feedback has been extensively studied, particularly in the field of _dueling bandits_(Yue and Joachims, 2011; Yue et al., 2012; Zoghi et al., 2014; Ailon et al., 2014; Komiyama et al., 2015; Wu and Liu, 2016; Saha and Gaillard, 2021; Benggs et al., 2021; Saha and Gaillard, 2022) and _contextual dueling bandits_(Dudik et al., 2015; Saha, 2021; Saha and Krishnamurthy, 2022; Benggs et al., 2022; Wu et al., 2023). Different from the standard bandit setting, the learner proposes two actions in dueling bandits and only gets noisy preference feedback from the human expert. Follow-up works extend the preference-based learning model from the one-step bandit setting to the multi-step decision-making (e.g., imitation learning and reinforcement learning) setting (Chu and Ghahramani, 2005; Sadigh et al., 2017; Christiano et al., 2017; Lee et al., 2021; Chen et al., 2022; Saha et al., 2023). These studies mainly focus on learning a high-quality policy from human feedback, without concerning the question of active query in order to minimize the query complexity.

However, query complexity is an important metric to optimize when learning from human feedback, as human feedback is expensive to collect (Lightman et al., 2023). For instance, InstructGPT (Ouyang et al., 2022) is trained only on around 30K pieces of human feedback, which is significantly fewer than the internet-scale dataset used for pre-training the base model GPT3, indicating the challenge of scaling up the size of human feedback datasets. In other areas, such as robotics, learning from human feedback is also not easy, and prior studies (e.g., Cohn et al. (2011); Zhang et al. (2022); Myers et al. (2023)) have explored this issue from various perspectives. Ross et al. (2013); Laskey et al. (2016) pointed out that querying human feedback in the learning loop is challenging, and extensively querying for feedback puts too much burden on the human experts.

In this work, we design _principled algorithms that learn from preference-based feedback while at the same time minimizing query complexity_ under the settings of contextual bandits (Auer et al., 2002) and imitation learning (Ross et al., 2011). Our main contributions can be summarized as follows.

* in short of _Active preference qUeRy fOR contextual bAndits_) that achieves a best-of-both-worlds regret bound (i.e., achieving the minimum of the worst-case regret and an instance dependent regret), while at the same providing an instance-dependent query complexity bound. For benign instances with small eluder dimension and large gap, our regret and query complexity bounds both scale with \(\ln(T)\) where \(T\) is the total number of interactions in contextual bandits.
* a much stronger feedback mode than ours.

To the best of our knowledge, for both contextual bandit and imitation learning with preference-based feedback, our algorithms are the first to achieve best-of-both-worlds regret bounds via active querying.

### Related works

**Selective Sampling.** Numerous studies have been conducted on selective sampling across various settings (Cesa-Bianchi et al., 2005; Dekel et al., 2012; Agarwal, 2013; Hanneke and Yang, 2015,2021; Zhu and Nowak, 2022), with the work of Sekhari et al. (2023) being closest to ours. Sekhari et al. (2023) presented a suite of provably efficient algorithms that are applicable to settings including contextual bandits and imitation learning. The primary distinction between our setting and the prior works lies in the feedback modality-we assume preference-based feedback, whereas they assume direct label feedback or reward signals.

**Contextual bandits with preference feedback.**Dudik et al. (2015) is the first to consider contextual dueling bandits, and one of their algorithms achieves the optimal regret rate. Saha and Krishnamurthy (2022) studied contextual dueling bandits using a value function class and proposed an algorithm based on a reduction to online regression, which also achieves an optimal worst-case regret bound. In this paper, we mainly follow the setting of the latter and make notable improvements in two aspects: (1) in addition to the \(O(\sqrt{AT})\) optimal regret rate where \(A\) is the number of actions and \(T\) is the number of interaction rounds, we established an instance-dependent regret upper bound that can be significantly smaller when the bandit exhibits a favorable structure; (2) our algorithm has an instance-dependent upper bound on the number of queries, and thus when the underlying instance is well behaved (has small eluder dimension and large gap), we will make significantly fewer queries.

Another related work is Saha and Gaillard (2022) which achieves the best-of-both-worlds regret for non-contextual dueling bandits. Our setting is more general due to the context and general function approximation, which enables us to leverage function classes beyond linear and tabular cases.

**RL with preference feedback.** RL with preference feedback has been widely employed in recent advancements in AI (Ouyang et al., 2022; OpenAI, 2023). According to Wirth et al. (2017), there are generally three types of preference feedback: action preferences (Furnkranz et al., 2012), state preferences (Wirth and Furnkranz, 2014), and trajectory preferences (Busa-Fekete et al., 2014; Novoseller et al., 2020; Xu et al., 2020; Lee et al., 2021; Chen et al., 2022; Saha et al., 2023; Pacchiano et al., 2021; Biyik and Sadigh, 2018; Taranovic et al., 2022; Sadigh et al., 2017). We focus on the action preference with the goal of achieving tight regret bounds and query complexities.

The concurrent work from Zhan et al. (2023) investigates the experimental design in both the trajectories-based and action-based preference settings, for which they decouple the process of collecting trajectories from querying for human feedback. Their action-based setting is the same as ours, but they mainly focus on linear parameterization, while our approach is a reduction to online regression and can leverage general function approximation beyond linear function classes.

**Imitation learning.** In imitation learning, two common feedback modalities are typically considered: expert demonstration and preference. The former involves directly acquiring expert actions (Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017; Chang et al., 2015; Sekhari et al., 2023), while the latter focuses on obtaining preferences between selected options (Chu and Ghahramani, 2005; Lee et al., 2021; Zhu et al., 2023). Brown et al. (2019, 2020) leveraged both demonstrations and preference-based information and empirically showed that their algorithm can learn to outperform experts. Our imitation learning setting belongs to the second category, and we established bounds on the regret and the query complexity for our algorithm. We show that our algorithm can learn a policy that can provably outperform the expert (when it is suboptimal for the underlying environment).

## 2 Preliminaries

### Contextual Bandits with Preference-Based Feedback

In this section, we introduce the contextual dueling bandits setting. For notation, we denote \([N]\) as the integer set \(\{1,\dots,N\}\) and denote \(\Delta(\mathcal{S})\) as the set of distributions over a set \(\mathcal{S}\).

We assume a context set \(\mathcal{X}\) and an action space \(\mathcal{A}=[A]\). At each round \(t\in[T]\), a context \(x_{t}\) is drawn _adversarially_, and the learner's task is to pick a pair of actions \((a_{t},b_{t})\in\mathcal{A}\times\mathcal{A}\) and then decide whether to make a query to the expert. If making a query, a noisy feedback \(y_{t}\in\{-1,1\}\) is revealed to the learner regarding whether \(a_{t}\) or \(b_{t}\) is better. We assume that the expert relies on a preference function \(f^{\star}:\mathcal{X}\times\mathcal{A}\times\mathcal{A}\to[-1,1]\) to samples its preference feedback \(y_{t}\):

\[\Pr(a_{t}\text{ is preferred to }b_{t}\,|\,x_{t})\coloneqq\Pr(y_{t}=1\,|\,x_{t},a_ {t},b_{t})=\phi\big{(}f^{\star}(x_{t},a_{t},b_{t})\big{)}\]

where \(\phi(d):[-1,1]\to[0,1]\) is the link function, which satisfies \(\phi(d)+\phi(-d)=1\) for any \(d\). If the learner does not make a query, it will not receive any feedback for the selected actions \(a_{t}\) and \(b_{t}\). Let \(Z_{t}\in\{0,1\}\) indicate whether the learner makes a query at round \(t\).

We assume that the learner has access to a function class \(\mathcal{F}\subseteq\mathcal{X}\times\mathcal{A}\times\mathcal{A}\to[-1,1]\) that realizes \(f^{\star}\). Furthermore, we assume that \(f^{\star}\), as well as the functions in \(\mathcal{F}\), is transitive and anti-symmetric.

**Assumption 1**.: _We assume \(f^{\star}\in\mathcal{F}\) and any functions \(f\in\mathcal{F}\) satisfies the following two properties: (1) transitivity: for any \(x\in\mathcal{X}\) and \(a,b,c\in\mathcal{A}\), if \(f(x,a,b)>0\) and \(f(x,b,c)>0\), then we must have \(f(x,a,c)>0\); (2) anti-symmetry: \(f(x,a,b)=-f(x,b,a)\) for any \(x\in\mathcal{X}\) and any \(a,b\in\mathcal{A}\)._

We provide an example below for which Assumption 1 is satisfied.

**Example 1**.: _Assume there exists a function \(r^{\star}:\mathcal{X}\times\mathcal{A}\to[0,1]\) such that \(f^{\star}(x,a,b)=r^{\star}(x,a)-r^{\star}(x,b)\) for any \(x\in\mathcal{X}\) and \(a,b\in\mathcal{A}\). Typically, such a function \(r^{\star}\) represents the "reward function" of the contextual bandit. In such a scenario, we can first parameterize a reward class \(\mathcal{R}\subseteq\mathcal{X}\times\mathcal{A}\to[0,1]\) and define \(\mathcal{F}=\{f:f(x,a,b)=r(x,a)-r(x,b),r\in\mathcal{R}\}\). Moreover, it is common to have \(\phi(d)\coloneqq 1/(1+\exp(-d))\) in this setting, which recovers the Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952) -- a commonly used model in practice for learning reward models (Christiano et al., 2017)._

Assumption 1 ensures the existence of an optimal arm, as stated below.

**Lemma 1**.: _Under Assumption 1, for any function \(f\in\mathcal{F}\) and any context \(x\in\mathcal{X}\), there exists an arm \(a\in\mathcal{A}\) such that \(f(x,a,b)\geq 0\) for any arm \(b\in\mathcal{A}\). We denote this best arm by \(\pi_{f}(x):=a\).2_

Footnote 2: When the best arms is not unique, the ties are broken arbitrarily but consistently.

The learner's goal is to minimize the regret while also minimizing the number of queries, defined as:

\[\mathrm{Regret}_{T}^{\mathrm{CB}}\coloneqq\sum_{t=1}^{T}\big{(}f^{\star}(x_{ t},\pi_{f^{\star}}(x_{t}),a_{t})+f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),b_{t}) \big{)},\quad\mathrm{Queries}_{T}^{\mathrm{CB}}\coloneqq\sum_{t=1}^{T}Z_{t}.\]

It is worth noting that when \(f^{\star}\) is the difference in rewards (as in Example 1), the regret defined above reduces to the standard regret of a contextual bandit. We also remark that our feedback model generalizes that of Saha and Krishnamurthy (2022) in that we assume an additional link function \(\phi\), while they assume the feedback is sampled from \(\Pr(y=1\,|\,x,a,b)=(P_{t}[a_{t},b_{t}]+1)/2\) where \(P_{t}\) is a preference matrix. Their loss function is captured in our setting (Example 2). However, Saha and Krishnamurthy (2022) do not assume transitivity.

### Imitation Learning with Preference-Based Feedback

In our imitation learning setup, we consider that the learner operates in a finite-horizon Markov decision process (MDP), which is a tuple \(M(\mathcal{X},\mathcal{A},r,P,H)\) where \(\mathcal{X}\) is the state space, \(\mathcal{A}\) is the action space, \(P\) is the transition kernel, \(r:\mathcal{X}\times\mathcal{A}\to[0,1]\) is the reward function, and \(H\) is the length of each episode. The interaction between the learner and the environment proceeds as follows: at each episode \(t\in[T]\), the learner receives an initial state \(x_{t,0}\) which could be chosen adversarially. Then, the learner interacts with the environment for \(H\) steps. At each step \(h\), the learner first decides whether to make a query. If making a query, the learner needs to select a pair of actions \((a_{t,h},b_{t,h})\in\mathcal{A}\times\mathcal{A}\), upon which a feedback \(y_{t,h}\in\{-1,1\}\) is revealed to the learner regarding which action is preferred from the expert's perspective. Here the feedback is sampled according to

\[\Pr(a_{t,h}\text{ is preferred to }b_{t,h}\,|\,x_{t,h},h)\coloneqq\Pr(y_{t,h}=1\,| \,x_{t,h},a_{t,h},b_{t,h},h)=\phi\big{(}f^{\star}_{h}(x,a_{t,h},b_{t,h})\big{)}.\]

Irrespective of whether the learner made a query, it then picks a single action from \(a_{t,h},b_{t,h}\) and transit to the next step (our algorithm will just pick an action uniformly at random from \(a_{t,h},b_{t,h}\)). After \(H\) steps, the next episode starts. Let \(Z_{t,h}\in\{0,1\}\) indicate whether the learner decided to query at step \(h\) in episode \(t\). We assume that the function class \(\mathcal{F}\) is a product of \(H\) classes, i.e., \(\mathcal{F}=\mathcal{F}_{0}\times\cdots\mathcal{F}_{H-1}\) where, for each \(h\), we use \(\mathcal{F}_{h}=\{f:\mathcal{X}\times\mathcal{A}\times\mathcal{A}\to[-1,1]\}\) to model \(f^{\star}_{h}\) and assume that \(\mathcal{F}_{h}\) satisfies Assumption 1.

A policy is a mapping \(\pi:\mathcal{X}\to\Delta(\mathcal{A})\). For a policy \(\pi\), the state value function for a state \(x\) at step \(h\) is defined as \(V^{\pi}_{h}(x)\coloneqq\mathbb{E}[\sum_{i=h}^{H-1}r_{i}\,|\,x_{h}=x]\) and the state-action value function for a state-action pair \((x,a)\) is \(Q^{\pi}_{h}(x,a)\coloneqq\mathbb{E}[\sum_{i=h}^{H-1}r_{i}\,|\,x_{h}=x,a_{h}=a]\), where the expectations are taken w.r.t. the trajectories sampled by \(\pi\) in the underlying MDP.

In the imitation learning setting, we assume that the expert (who gives the preference-based feedback) is equipped with a markovian policy \(\pi_{e}\), and that the preference of the expert is dependent on the reward-to-go under \(\pi_{\epsilon}\) (i.e. on a state \(x\), actions with higher values of \(Q^{\pi_{\epsilon}}(s,a)\) will be preferred by the expert). Formalizing this intuition, we assume that \(f_{h}^{*}\) is defined such that as \(f_{h}^{*}(x,a,b)\coloneqq Q_{h}^{\pi_{\epsilon}}(x,a)-Q_{h}^{\pi_{\epsilon}}(x,b)\). The goal of the learner is still to minimize the regret and number of queries:

\[\mathrm{Regret}_{T}^{\mathrm{IL}}\coloneqq\sum_{t=1}^{T}\big{(}V_{0}^{\pi_{ \epsilon}}(x_{t,0})-V_{0}^{\pi_{\epsilon}}(x_{t,0})\big{)},\quad\mathrm{Queries }_{T}^{\mathrm{IL}}\coloneqq\sum_{t=1}^{T}\sum_{h=0}^{H-1}Z_{t,h}.\]

Here \(\pi_{\epsilon}\) is the strategy the learner uses to select actions at episode \(t\).

### Link Function and Online Regression Oracle

Following the standard practice in the literature [Agarwal, 2013], we assume \(\phi\) is the derivative of some \(\alpha\)-strongly convex function (Definition 3) \(\Phi:[-1,1]\to\mathbb{R}\) and define the associated loss function as \(\ell_{\phi}(d,y)=\Phi(d)-d(y+1)/2\). Additionally, in line with prior works [Foster et al., 2021, Simechi-Levi and Xu, 2022, Foster et al., 2018a, Sekhari et al., 2023], our algorithm utilizes an online regression oracle, which is assumed to have a sublinear regret guarantee w.r.t. \(\mathcal{F}\) on arbitrary data sequences.

**Assumption 2**.: _We assume the learner has access to an online regression oracle pertaining to the loss \(\ell_{\phi}\) such that for any sequence \(\{(x_{1},a_{1},b_{1},y_{1}),\ldots,(x_{T},a_{T},b_{T},y_{T})\}\) where the label \(y_{t}\) is generated by \(y_{t}\sim\phi(f^{*}(x_{i},a_{t},b_{t}))\), we have_

\[\sum_{t=1}^{T}\ell_{\phi}\big{(}f_{t}(x_{t},a_{t},b_{t}),y_{t}\big{)}-\inf_{f \in\mathcal{F}}\ell_{\phi}\big{(}f(x_{t},a_{t},b_{t}),y_{t}\big{)}\leq\Upsilon (\mathcal{F},T)\]

_for some \(\Upsilon(\mathcal{F},T)\) that grows sublinearly with respect to \(T\).3 For notational simplicity, whenever clear from the context, we define \(\Upsilon:=\Upsilon(\mathcal{F},T)\)._

Footnote 3: The online regression oracle updates as follows: in each iteration, after seeing \(x_{t},a_{t},b_{t}\), it proposes a decision \(f_{t}\), then \(y_{t}\) is revealed and the online regression oracle incurs loss \(\ell_{\phi}(f_{t}(x_{t},a_{t},b_{t}),y_{t})\).

Here \(\Upsilon\) represents the regret upper bound and is typically of logarithmic order in \(T\) or the cardinality of the function class \(\mathcal{F}\) in many cases (here we drop the dependence on \(T\) in notation for simplicity). We provide a few examples below.

**Example 2** (Squared loss).: _If we consider \(\Phi(d)=d^{2}/4+d/2+1/4\), which is \(1/4\)-strongly convex, then we obtain \(\phi(d)=(d+1)/2\) and \(\ell_{\phi}(d,y)=(d-y)^{2}/4\), thereby recovering the squared loss, which has been widely studied in prior works. For example, Rakhlin and Sridharan (2014) characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity, resulting in favorable bounds for the regret. Specifically, we have \(\Upsilon=O(\log|\mathcal{F}|)\) assuming the function class \(\mathcal{F}\) is finite, and \(\Upsilon=O(d\log(T))\) assuming \(\mathcal{F}\) is a \(d\)-dimensional linear class. We also kindly refer the readers to Krishnamurthy et al. (2017), Foster et al. (2018a) for efficient implementations._

**Example 3** (Logistic loss).: _When \(\Phi(d)=\log(1+\exp(d))\) which is strongly convex at \([-1,1]\), we have \(\phi(d)=1/(1+\exp(-d))\) and \(\ell_{\phi}(d,y)=\log(1+\exp(-yd))\). Thus, we recover the logistic regression loss, which allows us to use online logistic regression and achieve \(\Upsilon=O(\log|\mathcal{F}|)\) assuming finite \(\mathcal{F}\). There have been numerous endeavors in minimizing the log loss, such as Foster et al. (2018b) and Cesa-Bianchi and Lugosi (2006, Chapter 9)._

## 3 Contextual Bandits with Preference-Based Active Queries

We first present the algorithm, named AURORA, for contextual dueling bandits, as shown in Algorithm 1. At each round \(t\in[T]\), the learner first constructs a version space \(\mathcal{F}_{t}\) containing all functions close to past predictors on the observed data. Here, the threshold \(\beta\) set to \(4\Upsilon/\alpha+(16+24\alpha)\log\big{(}4\delta^{-1}\log(T)\big{)}/\alpha^{2}\) ensures that \(f^{*}\in\mathcal{F}_{t}\) for any \(t\in[T]\) with probability at least \(1-\delta\) (Lemma 9). Thus, \(\mathcal{A}_{t}\) is non-empty for all \(t\in[T]\) and correspondingly Line 17 is well defined. The learner then forms a candidate arm set \(\mathcal{A}_{t}\) consisting of greedy arms induced by all functions in the version space. When \(|\mathcal{A}_{t}|=1\), the only arm in the set is the optimal arm since \(f^{*}\in\mathcal{F}_{t}\), and thus no query is needed (\(Z_{t}=0\)). However, when \(|\mathcal{A}_{t}|>1\), any arm in \(\mathcal{A}_{t}\) could potentially be the optimal arm, and thus the learner needs to make a comparison query to obtain more information.

Next, we explain the learner's strategy for making queries. Firstly, the learner computes \(w_{t}\), which is the "width" of the version space. Specifically, \(w_{t}\) overestimates the instantaneous regret for playing any arm in \(\mathcal{A}_{t}\) (Lemma 8). Then, the learner defines \(\lambda_{t}\) that indicates if the estimated cumulative regret \(\sum_{s=1}^{t-1}Z_{s}w_{s}\) has exceeded \(\sqrt{AT/\beta}\). Note that \(Z_{t}\) is multiplied to \(w_{t}\) since no regret is incurred when \(Z_{t}=0\). The learner then chooses the actions (to be queried) depending on the values of \(\lambda_{t}\):

* If \(\lambda_{t}=0\), the cumulative reward has not yet exceeded \(\sqrt{AT/\beta}=O(\sqrt{T})\), so the learner will explore as much as possible by uniform sampling from \(\mathcal{A}_{t}\).
* If \(\lambda_{t}=1\), the regret may have reached \(O(\sqrt{T})\), and therefore the learner uses a technique similar to inverse gap weighting (IGW), as inspired by Saha and Krishnamurthy (2022), to achieve a better balance between exploration and exploitation. Specifically, the learner solves the convex program4 in Line 12, which is feasible and whose solution \(p_{t}\) satisfies (Lemma 11) Footnote 4: It is convex as it can be written as \(|\mathcal{A}_{t}|\) convex constraints: \(\sum_{b}f_{t}(x_{t},a,b)p_{t}(b)+\frac{2}{\gamma_{t}p_{t}(a)}\leq\frac{5A}{ \gamma_{t}},\forall a\in\mathcal{A}_{t}\).

\[\operatorname*{\mathbb{E}}_{a\sim p_{t}}\left[f^{\star}(x_{t},\pi_{f^{\star}}( x),a)\right]=O\left(\gamma_{t}\operatorname*{\mathbb{E}}_{a,b\sim p_{t}}\left[ \left(f_{t}(x_{t},a,b)-f^{\star}(x_{t},a,b)\right)^{2}\right]+\frac{A}{\gamma _{t}}\right).\] (1) As a result of the above relation, we can convert the instantaneous regret to the point-wise error between the predictor \(f_{t}\) and the truth \(f^{\star}\) plus an additive \(A/\gamma_{t}\). This allows us to bound the cumulative point-wise error by the regret of the online regression oracle. In the special case where there exists a "reward function" \(r:\mathcal{X}\times\mathcal{A}\to[0,1]\) for each \(f\in\mathcal{F}\) such that \(f(x,a,b)=r(x,a)-r(x,b)\) (Example 1), the solution \(p_{t}\) can be directly written as

\[p_{t}(a)=\begin{cases}\frac{1}{A+\gamma_{t}\left(r_{t}(x_{t},\pi_{f_{t}}(x_{t} ))-r_{t}(x_{t},a)\right)}&a\neq\pi_{f_{t}}(x_{t})\\ 1-\sum_{a^{\prime}\neq\pi_{f_{t}}(x_{t})}p_{t}(a^{\prime})&a=\pi_{f_{t}}(x_{t })\end{cases},\]

where \(r_{t}\) is the reward function associated with \(f_{t}\), i.e., \(f_{t}(x,a,b)=r_{t}(x,a)-r_{t}(x,b)\). This is the standard IGW exploration strategy (Foster and Rakhlin, 2020) and leads to the same guarantee as (1) (see Lemma 12).

We discuss the computational tractability of Algorithm 1 in Appendix A. In short, it is computationally tractable for some structured function classes (e.g. linear and tabular function classes). For general function classes, it is also efficient given a regression oracle.

### Theoretical Analysis

Towards the statistical guarantees of Algorithm 1, we employ two quantities to characterize a contextual bandit instance: the uniform gap and the eluder dimension, which are introduced below.

**Assumption 3** (Uniform gap).: _We assume the optimal arm \(\pi_{f^{\star}}(x)\) induced by \(f^{\star}\) under any context \(x\in\mathcal{X}\) is unique. Further, we assume a uniform gap \(\Delta:=\inf_{x}\inf_{a\neq\pi_{f^{\star}}(x)}f^{\star}(x,\pi_{f^{\star}}(x),a)>0\)._

We note that the existence of a uniform gap is a standard assumption in the literature of contextual bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Audibert et al., 2010; Garivier et al., 2019; Foster and Rakhlin, 2020; Foster et al., 2021). Next, we introduce the eluder dimension (Russo and Van Roy, 2013) and begin by defining "\(\epsilon\)-dependence".

**Definition 1** (\(\epsilon\)-dependence).: _Let \(\mathcal{G}\subseteq\mathcal{X}\to\mathbb{R}\) be any function class. We say an element \(x\in\mathcal{X}\) is \(\epsilon\)-dependent on \(\{x_{1},x_{2},\ldots,x_{n}\}\subseteq\mathcal{X}\) with respect to \(\mathcal{G}\) if any pair of functions \(g,g^{\prime}\in\mathcal{G}\) satisfying \(\sum_{i=1}^{n}(g(x_{i})-g^{\prime}(x_{i}))^{2}\leq\epsilon^{2}\) also satisfies \(g(x)-g^{\prime}(x)\leq\epsilon\). Otherwise, we say \(x\) is \(\epsilon\)-independent of \(\{x_{1},x_{2},\ldots,x_{n}\}\)._

**Definition 2** (Eluder dimension).: _The \(\epsilon\)-eluder dimension of a function class \(\mathcal{G}\subseteq\mathcal{X}\to\mathbb{R}\), denoted by \(\dim_{\mathcal{E}}(\mathcal{G},\epsilon)\), is the length \(d\) of the longest sequence of elements in \(\mathcal{X}\) satisfying that there exists some \(\epsilon^{\prime}\geq\epsilon\) such that every element in the sequence is \(\epsilon^{\prime}\)-independent of its predecessors._

Eluder dimension is a complexity measure for function classes and has been used in the literature of bandits and RL extensively (Chen et al., 2022; Osband and Van Roy, 2014; Wang et al., 2020; Foster et al., 2021; Wen and Van Roy, 2013; Jain et al., 2015; Ayoub et al., 2020; Ishfaq et al., 2021; Huang et al., 2022). Examples where the eluder dimension is small include linear functions, generalized linear models, and functions in Reproducing Kernel Hilbert Space (RKHS).

Given these quantities, we are ready to state our main results. The proofs are provided in Appendix C.

**Theorem 1**.: _Under Assumptions 1 to 3, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:_

\[\mathrm{Regret}_{T}^{\mathrm{CB}}=\widetilde{O}\left(\min\left\{ \sqrt{AT\beta},\ \frac{A^{2}\beta^{2}\mathrm{dim}_{E}\left(\mathcal{F},\Delta\right)}{\Delta} \right\}\right),\] \[\mathrm{Queries}_{T}^{\mathrm{CB}}=\widetilde{O}\left(\min\left\{ T,\ \frac{A^{3}\beta^{3}\mathrm{dim}_{E}^{2}\left(\mathcal{F},\Delta\right)}{\Delta^{2}} \right\}\right)\]

_with probability at least \(1-\delta\). We recall that \(\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(\delta^{-1}\log(T)))\), and \(\alpha\) represents the coefficient of strong convexity of \(\Phi\). Logarithmic terms are hidden in the upper bounds for brevity._

For commonly used loss function (Examples 2 and 3), \(\beta\) only scales logarithmically with the size (or effective size) of \(\mathcal{F}\) and is thus mild. For instance, for a finite class \(\mathcal{F}\), \(\beta\) depends on \(\log|\mathcal{F}|\). Furthermore, when \(\mathcal{F}\) is infinite, we can replace \(|\mathcal{F}|\) by the covering number of \(\mathcal{F}\) following the standard techniques: for \(d\)-dimensional linear function class, \(\beta\) will have a dependence of \(O(d)\) (effective complexity of \(\mathcal{F}\)), and for the tabular class, \(\beta\) will have a dependence of \(O(SA^{2})\). In other words, the rate of \(\beta\) is acceptable as long as the complexity of the function class \(\mathcal{F}\) is acceptable.

**Best-of-both-worlds guarantee.** We observe that both the regret bound and the query complexity bound consist of two components: the worst-case bound and the instance-dependent bound. The worst-case bound provides a guarantee under all circumstances, while the instance-dependent one may significantly improve the upper bound when the underlying problem is well-behaved (i.e., has a small eluder dimension and a large gap).

**Lower bounds.** To see whether these upper bounds are tight, we provide a lower bound which follows from a reduction from regular multi-armed bandits to contextual dueling bandits.

**Theorem 2** (Lower bounds).: _The following two claims hold: (1) For any algorithm, there exists an instance that leads to \(\mathrm{Regret}_{T}^{\mathrm{CB}}=\Omega(\sqrt{AT})\); (2) For any algorithm achieving a worse-case expected regret upper bound in the form of \(\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]=O(\sqrt{AT})\), there exists an instance with gap \(\Delta=\sqrt{A/T}\) that results in \(\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]=\Omega(A/\Delta)\) and \(\mathbb{E}[\mathrm{Queries}_{T}^{\mathrm{CB}}]=\Omega(A/\Delta^{2})=\Omega(T)\)._By relating these lower bounds to Theorem 1, we conclude that our algorithm achieves a tight dependence on the gap \(\Delta\) and the number of rounds \(T\) up to logarithmic factors. Furthermore, as an additional contribution, we establish an alternative lower bound in Section C.4.1 by conditioning on the limit of regret, rather than the worst-case regret as assumed in Theorem 2.

**Intuition of proofs.** We next provide intuition for why our algorithm has the aforementioned theoretical guarantees. First, we observe that from the definition of \(\lambda_{t}\), the left term inside the indicator is non-decreasing, which allows us to divide rounds into two phases. In the first phase, \(\lambda_{t}\) is always 0, and then at some point, it changes to 1 and remains 1 for the rest rounds. After realizing this, we first explain the intuition of the worst-case regret. In the first phase, as \(w_{t}\) is an overestimate of the instantaneous regret (see Lemma 8), the accumulated regret in this phase cannot exceed \(O(\sqrt{T})\). In the second phase, we adapt the analysis of IGW to this scenario to obtain an \(O(\sqrt{T})\) upper bound. A similar technique has been used in Saha and Krishnamurthy (2022), Foster et al. (2021). As the regret in both phases is at most \(O(\sqrt{T})\), the total regret cannot exceed \(O(\sqrt{T})\). Next, we explain the intuition of instance-dependent regret. Due to the existence of a uniform gap \(\Delta\), we can first prove that as long as \(|\mathcal{A}_{t}|>1\), we must have \(w_{t}\geq\Delta\) (see Lemma 7). This means that for all rounds that may incur regret, the corresponding width is at least \(\Delta\). However, this cannot happen too many times as this frequency is bounded by the eluder dimension, which leads to an instance-dependent regret upper bound. Leveraging a similar technique, we can also obtain an upper bound on the number of queries.

**Comparison to regret bounds of dueling bandits.** As established by prior works (Yue et al., 2012; Saha and Gaillard, 2022), for dueling bandits, the minimax regret rate is \(\tilde{\Theta}(\sqrt{AT})\) and the instance-dependent rate is \(\tilde{\Theta}\left(A/\Delta\right)\). If we reduce our result (Theorem 1) into the dueling bandits setting, we will get

\[\mathrm{Regret}_{T}=\tilde{O}\left(\min\left\{\sqrt{AT},\frac{A^{2}\mathrm{ dim}_{E}(\mathcal{F},\frac{\Delta}{2A^{2}})}{\Delta}\right\}\right)=\tilde{O} \left(\min\left\{\sqrt{AT},\frac{A^{3}}{\Delta}\right\}\right)\]

where the second equality holds since the eluder dimension is upper bounded by \(A\) for dueling bandits. We observe that the worst-case regret rate is the same, but there is a gap of \(A^{2}\) in the instance-dependent bound. The improvement of this gap is an interesting future direction.

**Comparison to MinMaxDB (Saha and Krishnamurthy, 2022).** In this prior work, the authors assume that \(\Pr(y=1\,|\,x,a,b)=(f^{\star}(x,a,b)+1)/2\), which is a specification of our feedback model (Example 2). While our worst-case regret bound matches their regret bound, our paper improves upon their results by having an additional instance-dependent regret bound that depends on the eluder dimension and gap. Furthermore, we also provide bounds on the query complexity which could be small for benign instances while MinMaxDB simply queries on every round.

**Comparison to AdaCB (Foster et al., 2021).** Our method shares some similarities with Foster et al. (2021), especially in terms of theoretical results, but differs in two aspects: (1) they assume regular contextual bandits where the learner observes the reward directly, while we assume preference feedback, and (2) they assume a stochastic setting where contexts are drawn i.i.d., but we assume that the context is adversarially chosen. While these two settings may not be directly comparable, it should be noted that (Foster et al., 2021) do not aim to minimize query complexity.

**Results without the uniform gap assumption.** We highlight that Theorem 1 can naturally extend to scenarios where a uniform gap does not exist (i.e., when Assumption 3 is not satisfied) without any modifications to the algorithm. The result is stated below, which is analogous to Theorem 1.

**Theorem 3**.: _Under Assumptions 1 and 2, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:_

\[\mathrm{Regret}_{T}^{\mathrm{CB}}=\widetilde{O}\left(\min\left\{\sqrt{AT \beta},\,\min_{\epsilon>0}\left\{T_{\epsilon}\beta+\frac{A^{2}\beta^{2} \mathrm{dim}_{E}\left(\mathcal{F},\epsilon\right)}{\epsilon}\right\}\right\} \right),\]

\[\mathrm{Queries}_{T}^{\mathrm{CB}}=\widetilde{O}\left(\min\left\{T,\,\min_{ \epsilon>0}\left\{T_{\epsilon}^{2}\beta/A+\frac{A^{3}\beta^{3}\mathrm{dim}_{E} ^{2}\left(\mathcal{F},\epsilon\right)}{\epsilon^{2}}\right\}\right\}\right)\]

_with probability at least \(1-\delta\). Here we define the gap of context \(x\) as \(\mathrm{Gap}(x)\coloneqq\min_{a\neq\pi_{f^{\star}}(x)}f^{\star}(x,\pi_{f^{\star} }(x),a)\) and the number of rounds where contexts have small gap as \(T_{\epsilon}\coloneqq\sum_{t=1}^{T}\mathds{1}\{\mathrm{Gap}(x_{t})\leq\epsilon\}\). We also recall that \(\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(\delta^{-1}\log(T)))\), and \(\alpha\) denotes the coefficient of strong convexity of \(\Phi\)._Compared to Theorem 1, the above result has an extra gap-dependent term, \(T_{\epsilon}\), measuring how many times the context falls into a small-gap region. We highlight that \(T_{\epsilon}\) is small under certain conditions such as the Tsybakov noise condition (Tsybakov, 2004). It is also worth mentioning that our algorithm is agnostic to \(\epsilon\), thus allowing us to take the minimum over all \(\epsilon>0\).

**Comparion to SAGE-Bandit (Sekhari et al., 2023).** Theorem 3 is similar to Theorem 4 in Sekhari et al. (2023), which studies active queries in contextual bandits with standard reward signal. Although our result looks slightly worse in terms of the factor \(A\), we believe that this inferiority is reasonable since our approach requires two actions to form a query, thus analytically expanding the action space to \(\mathcal{A}^{2}\). Whether this dependency can be improved remains a question for future investigation.

## 4 Imitation Learning with Preference-Based Active Queries

In this section, we introduce our second algorithm, which is presented in Algorithm 2 for imitation learning. In essence, the learner treats the MDP as a concatenation of \(H\) contextual bandits and runs an instance of AURORA (Algorithm 1) for each time step. Specifically, the learner first creates \(H\) instances of AURORA, denoted by \(\text{AURORA}_{h}\) (for \(h=0,\dots,H-1\)). Here, \(\text{AURORA}_{h}\) should be thought of as an interactive program that takes the context \(x\) as input and outputs \(a\), \(b\), and \(Z\). At each episode \(t\), and each step \(h\) therein, the learner first feeds the current state \(x_{t,h}\) to \(\text{AURORA}_{h}\) as the context; then, \(\text{AURORA}_{h}\) decides whether to query (i.e. \(Z_{t,h}\)) and returns the actions \(a_{t,h}\) and \(b_{t,h}\). If it decides to make a query, the learner will ask for the feedback \(y_{t,h}\) on the proposed actions \(a_{t,h},b_{t,h}\), and provide the information \(((x_{t,h},a_{t,h},b_{t,h}),y_{t,h})\) back to \(\text{AURORA}_{h}\) to update its online regression oracle (and other local variables). We recall that the noisy binary feedback \(y_{t,h}\) is sampled as \(y_{t,h}\sim\phi(Q_{h}^{\pi_{\epsilon}}(x_{t,h},a_{t,h})-Q_{h}^{\pi_{\epsilon} }(x_{t,h},b_{t,h}))\), and also emphasize that the learner neither has access to \(a\sim\pi_{\epsilon}(x_{t,h})\) like in DAgger(Ross et al., 2011) nor reward-to-go like in AggreVaTe(D) (Ross and Bagnell, 2014; Sun et al., 2017). Finally, the learner chooses one of the two actions uniformly at random, executes it in the underlying MDP, and transits to the next state \(x_{t,h+1}\) in the episode. The above process is then repeated with \(\text{AURORA}_{h+1}\) till the episode ends. We name this algorithm AURORAE, the plural form of \(\text{AURORA}\), which signifies that the algorithm is essentially a stack of multiple AURORA instances.

```
1:Function class \(\mathcal{F}_{0},\mathcal{F}_{1},\dots,\mathcal{F}_{H-1}\), confidence parameter \(\beta\).
2:Learner creates \(H\) instances of Algorithm 1: \(\text{AURORA}_{h}(\mathcal{F}_{h},\beta)\) for \(h=0,1,\dots,H-1\).
3:for\(t=1,2,\dots,T\)do
4: Learner receive initial state \(x_{t,0}\).
5:for\(h=0,1,\dots,H-1\)do
6: Learner feeds \(x_{t,h}\) to \(\text{AURORA}_{h}(\mathcal{F}_{h},\beta)\), and receives back \(a_{t,h}\), \(b_{t,h}\), \(Z_{t,h}\).
7:if\(Z_{t,h}=1\)then
8: Learner receives feedback \(y_{t,h}\).
9: Learner feeds \(((x_{t,h},a_{t,h},b_{t,h}),y_{t,h})\) to \(\text{AURORA}_{h}(\mathcal{F}_{h},\beta)\) to update its online regression oracle and local variables.
10:endif
11: Learner executes \(a\sim\text{Uniform}(\{a_{t,h},b_{t,h}\})\) and transits to \(x_{t,h+1}\).
12:endfor
13:endfor ```

**Algorithm 2** Active preference qUeBy fOR imitAtion I learning (AURORAE)

### Theoretical Analysis

As Algorithm 2 is essentially a stack of Algorithm 1, we can inherit many of the theoretical guarantees from the previous section. To state the results, we first extend Assumption 3 into imitation learning.

**Assumption 4** (Uniform Gap).: _Let \(f_{h}^{*}\) be defined such that for any \(x\in\mathcal{X}\), \(a,b\in\mathcal{A}^{2}\), \(f_{h}^{*}(x,a,b)=Q_{h}^{\pi_{\epsilon}}(x,a)-Q_{h}^{\pi_{\epsilon}}(x,b)\). For all \(h\), we assume the optimal action for \(f_{h}^{*}\) under any state \(x\in\mathcal{X}\) is unique. Further, we assume a uniform gap \(\Delta:=\inf_{h}\inf_{x}\inf_{a\neq\pi_{f_{h}^{*}}(x)}f_{h}^{*}(x,\pi_{f_{h}^{* }}(x),a)>0\)._

This assumption essentially says that \(Q_{h}^{\pi_{\epsilon}}\) has a gap in actions. We remark that, just as Assumption 3 is a common condition in the bandit literature, Assumption 4 is also common in MDPs (Du et al.,2019, Foster et al., 2021, Simchowitz and Jamieson, 2019, Jin and Luo, 2020, Lykouris et al., 2021, He et al., 2021]. The theoretical guarantee for Algorithm 2 is presented in Theorem 4. We note a technical difference between this result and Theorem 1: although we treat the MDP as a concatenation of \(H\) contextual bandits, the instantaneous regret of imitation learning is defined as the performance gap between the combined policy \(\pi_{t}\) derived from the \(H\) instances as a cohesive unit and the expert policy. This necessitates the use of performance difference lemma (Lemma 5) to get a unified result.

**Theorem 4**.: _Under Assumptions 1, 2 and 4, Algorithm 2 guarantees the following upper bounds of the regret and the number of queries:_

\[\mathrm{Regret}_{T}^{\mathrm{IL}}\leq\widetilde{O}\left(H\cdot \min\left\{\sqrt{AT\beta},\ \frac{A^{2}\beta^{2}\mathrm{dim}_{E}\left(\mathcal{F},\Delta\right)}{\Delta} \right\}\right)-\mathrm{Adv}_{T},\] \[\mathrm{Queries}_{T}^{\mathrm{IL}}\leq\widetilde{O}\left(H\cdot \min\left\{T,\ \frac{A^{3}\beta^{3}\mathrm{dim}_{E}^{2}\left(\mathcal{F},\Delta\right)}{ \Delta^{2}}\right\}\right)\]

_with probability at least \(1-\delta\). Here \(\mathrm{Adv}_{T}\coloneqq\sum_{t=1}^{T}\sum_{h=0}^{H-1}\mathbb{E}_{x_{t,h} \sim d_{t_{t},0,h}^{\pi_{t}}}[\max_{a}A_{h}^{\pi_{e}}(x_{t,h},a)]\) is non-negative, and \(d_{x_{t,0,h}}^{\pi_{t}}(x)\) denotes the probability of \(\pi_{t}\)5 reaching the state \(x\) at time step \(h\) starting from initial state \(x_{t,0}\). In the above, \(\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(H\delta^{-1}\log(T)))\) and \(\alpha\) denotes the coefficient of strong convexity of \(\Phi\)._

Footnote 5: Policy \(\pi_{t}\) consists of \(H\) time-dependent policies \(\pi_{t,1},\ldots,\pi_{t,H}\), where each \(\pi_{t,h}\) is defined implicitly via \(\mathrm{AURORA}_{h}\), i.e., \(\pi_{t,h}\) generates action as follows: given \(x_{t,h}\), \(\mathrm{AURORA}_{h}\) recommends \(a_{t,h},b_{t,h}\), followed by uniformly sampling an action from \(\{a_{t,h},b_{t,h}\}\).

Compared to Theorem 1, the main terms of the upper bounds for imitation learning are precisely the bounds in Theorem 1 multiplied by \(H\). In the proof presented in Appendix C.6, we use the performance difference lemma to reduce the regret of imitation learning to the sum of the regret of \(H\) contextual dueling bandits, which explains this additional factor of \(H\).

Another interesting point is that the main term of the regret upper bound is subtracted by a non-negative term \(\mathrm{Adv}_{T}\), which measures the degree to which we can _outperform_ the expert policy. In other words, our algorithm not only competes with the expert policy but can also surpass it to some extent. To see this, let us consider the _average regret_, which is defined as \(\mathrm{AveRegret}_{T}^{\mathrm{IL}}:=\mathrm{Regret}_{T}^{\mathrm{IL}}/T= \sum_{t=1}^{T}(V_{0}^{\pi_{e}}(x_{t,0})-V_{0}^{\pi_{t}}(x_{t,0}))/T.\) Then, Theorem 4 implies that \(\mathrm{AveRegret}_{T}^{\mathrm{IL}}=O(H\sqrt{A\beta/T})-\mathrm{Adv}_{T}/T\) where we have simplified it by ignoring the instance-dependent upper bound and logarithmic factors for clarity. Now, consider a case where \(\max_{a}A_{h}^{\pi_{e}}(x,a)>\alpha_{0}\) for some constant \(\alpha_{0}>0\) for all \(x\) and \(h\). This can happen when the expert policy is suboptimal for every state. Consequently, we have \(\mathrm{Adv}_{T}>\alpha_{0}HT\). In this case, the average regret is further bounded by \(\mathrm{AveRegret}_{T}^{\mathrm{IL}}=O(H\sqrt{A\beta/T})-\alpha_{0}H.\) When \(T\rightarrow\infty\), we have \(\mathrm{AveRegret}_{T}^{\mathrm{IL}}\rightarrow-\alpha_{0}H<0\). This means that the best (or average) learned policy will eventually outperform the expert policy. This guarantee is stronger than that of DAgger(Ross et al., 2011) in that DAgger cannot ensure the learned policy is better than the expert policy regardless of how suboptimal the expert may be. While this may look surprising at first glance since we are operating under a somewhat weaker query mode than that of DAgger, we note that by querying experts for comparisons on pairs of actions with feedback sampling as \(y\sim\phi(Q^{\pi_{e}}(x,a)-Q^{\pi_{e}}(x,b))\), it is possible to identify the action that maximizes \(Q^{\pi_{e}}(x,a)\) (even if we cannot identify the value \(Q^{\pi_{e}}(x,a)\)). Finally, we remark that our worst-case regret bound is similar to that of Ross and Bagnell (2014); Sun et al. (2017), which can also outperform a suboptimal expert but require access to both expert's actions and reward signals -- a much stronger query model than ours.

## 5 Conclusion

We presented interactive decision-making algorithms that learn from preference-based feedback while minimizing query complexity. Our algorithms for contextual bandits and imitation learning share worst-case regret bounds similar to the bounds of the state-of-the-art algorithms in standard settings while maintaining instance-dependent regret bounds and query complexity bounds. Notably, our imitation learning algorithm can outperform suboptimal experts, matching the result of Ross and Bagnell (2014); Sun et al. (2017), which operates under much stronger feedback.

## Acknowledgements

AS acknowledges support from the Simons Foundation and NSF through award DMS-2031883, as well as from the DOE through award DE-SC0022199. KS acknowledges support from NSF CAREER Award 1750575, and LinkedIn-Cornell grant.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abbeel and Ng (2004) Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the twenty-first international conference on Machine learning_, page 1, 2004.
* Agarwal (2013) Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In _International Conference on Machine Learning_, pages 1220-1228. PMLR, 2013.
* Agarwal et al. (2019) Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. _CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep_, pages 10-4, 2019.
* Ailon et al. (2014) Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In _International Conference on Machine Learning_, pages 856-864. PMLR, 2014.
* Audibert et al. (2010) Jean-Yves Audibert, Sebastien Bubeck, and Remi Munos. Best arm identification in multi-armed bandits. In _COLT_, pages 41-53, 2010.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Ayoub et al. (2020) Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* Benggs et al. (2021) Viktor Benggs, Robert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hullermeier. Preference-based online learning with dueling bandits: A survey. _The Journal of Machine Learning Research_, 22(1):278-385, 2021.
* Benggs et al. (2022) Viktor Benggs, Aadirupa Saha, and Eyke Hullermeier. Stochastic contextual dueling bandits under linear stochastic transitivity models. In _International Conference on Machine Learning_, pages 1764-1786. PMLR, 2022.
* Biyik and Sadigh (2018) Erdem Biyik and Dorsa Sadigh. Batch active preference-based learning of reward functions. In _Conference on robot learning_, pages 519-528. PMLR, 2018.
* Bradley and Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* Brown et al. (2019) Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In _International conference on machine learning_, pages 783-792. PMLR, 2019.
* Brown et al. (2020) Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In _Conference on robot learning_, pages 330-359. PMLR, 2020.
* Busa-Fekete et al. (2014) Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, Weiwei Cheng, and Eyke Hullermeier. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. _Machine learning_, 97:327-351, 2014.
* Cesa-Bianchi and Lugosi (2006) Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Cesa-Bianchi et al. (2005) Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. _IEEE Transactions on Information Theory_, 51(6):2152-2162, 2005.
* Cesa-Bianchi et al. (2019)Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume III, and John Langford. Learning to search better than your teacher. In _International Conference on Machine Learning_, pages 2058-2066. PMLR, 2015.
* Chen et al. (2022) Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In _International Conference on Machine Learning_, pages 3773-3793. PMLR, 2022.
* Cheng and Boots (2018) Ching-An Cheng and Byron Boots. Convergence of value aggregation for imitation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1801-1809. PMLR, 2018.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Chu and Ghahramani (2005) Wei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In _Proceedings of the 22nd international conference on Machine learning_, pages 137-144, 2005.
* Cohn et al. (2011) Robert Cohn, Edmund Durfee, and Satinder Singh. Comparing action-query strategies in semi-autonomous agents. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 25, pages 1102-1107, 2011.
* Dani et al. (2008) Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In _21st Annual Conference on Learning Theory_, pages 355-366, 2008.
* Daume et al. (2009) Hal Daume, John Langford, and Daniel Marcu. Search-based structured prediction. _Machine learning_, 75:297-325, 2009.
* Dekel et al. (2012) Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single and multiple experts. _The Journal of Machine Learning Research_, 13(1):2655-2697, 2012.
* Du et al. (2019) Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient q-learning with function approximation via distribution shift error checking oracle. _Advances in Neural Information Processing Systems_, 32, 2019.
* Dudik et al. (2015) Miroslav Dudik, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In _Conference on Learning Theory_, pages 563-587. PMLR, 2015.
* Foster and Rakhlin (2020) Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 3199-3210. PMLR, 2020.
* Foster et al. (2018a) Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 1539-1548. PMLR, 2018a.
* Foster et al. (2021) Dylan Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. In _Conference on Learning Theory_, pages 2059-2059. PMLR, 2021.
* Foster et al. (2018b) Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. In _Conference On Learning Theory_, pages 167-208. PMLR, 2018b.
* Furnkranz et al. (2012) Johannes Furnkranz, Eyke Hullermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based reinforcement learning: a formal framework and a policy iteration algorithm. _Machine learning_, 89:123-156, 2012.
* Garivier et al. (2019) Aurelien Garivier, Pierre Menard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in bandit problems. _Mathematics of Operations Research_, 44(2):377-399, 2019.
* Hanneke and Yang (2015) Steve Hanneke and Liu Yang. Minimax analysis of active learning. _J. Mach. Learn. Res._, 16(1):3487-3602, 2015.
* Hanneke et al. (2018)Steve Hanneke and Liu Yang. Toward a general theory of online selective sampling: Trading off mistakes and queries. In _International Conference on Artificial Intelligence and Statistics_, pages 3997-4005. PMLR, 2021.
* He et al. (2021) Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In _International Conference on Machine Learning_, pages 4171-4180. PMLR, 2021.
* Huang et al. (2022) Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in zero-sum markov games. In _International Conference on Learning Representations_, 2022.
* Ishfaq et al. (2021) Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In _International Conference on Machine Learning_, pages 4607-4616. PMLR, 2021.
* Jain et al. (2015) Ashesh Jain, Shikhar Sharma, Thorsten Joachims, and Ashutosh Saxena. Learning preferences for manipulation tasks from online coactive feedback. _The International Journal of Robotics Research_, 34(10):1296-1313, 2015.
* Jin and Luo (2020) Tiancheng Jin and Haipeng Luo. Simultaneously learning stochastic and adversarial episodic mdps with known transition. _Advances in neural information processing systems_, 33:16557-16566, 2020.
* Kakade and Tewari (2008) Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. _Advances in Neural Information Processing Systems_, 21, 2008.
* Komiyama et al. (2015) Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem. In _Conference on learning theory_, pages 1141-1154. PMLR, 2015.
* Krishnamurthy et al. (2017) Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, and John Langford. Active learning for cost-sensitive classification. In _International Conference on Machine Learning_, pages 1915-1924. PMLR, 2017.
* Laskey et al. (2016) Michael Laskey, Sam Staszak, Wesley Yu-Shu Hsieh, Jeffrey Mahler, Florian T Pokorny, Anca D Dragan, and Ken Goldberg. Shiv: Reducing supervisor burden in dagger using support vectors for efficient learning from demonstrations in high dimensional state spaces. In _2016 IEEE International Conference on Robotics and Automation (ICRA)_, pages 462-469. IEEE, 2016.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lee et al. (2021a) Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-based reinforcement learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021a.
* Lee et al. (2021b) Kimin Lee, Laura M Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In _International Conference on Machine Learning_, pages 6152-6163. PMLR, 2021b.
* Li et al. (2023) Zihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism. _arXiv preprint arXiv:2305.18438_, 2023.
* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. _arXiv preprint arXiv:2305.20050_, 2023.
* Lykouris et al. (2021) Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration in episodic reinforcement learning. In _Conference on Learning Theory_, pages 3242-3245. PMLR, 2021.
* Myers et al. (2023) Vivek Myers, Erdem Biyik, and Dorsa Sadigh. Active reward learning from online preferences. _arXiv preprint arXiv:2302.13507_, 2023.
* Mairair et al. (2017)* Novoseller et al. [2020] Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In _Conference on Uncertainty in Artificial Intelligence_, pages 1029-1038. PMLR, 2020.
* OpenAI [2023] OpenAI. Gpt-4 technical report. _arXiv_, 2023.
* Osa et al. [2018] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. _Foundations and Trends(r) in Robotics_, 7(1-2):1-179, 2018.
* Osband and Van Roy [2014] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27, 2014.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Pacchiano et al. [2021] Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling rl: reinforcement learning with trajectory preferences. _arXiv preprint arXiv:2111.04850_, 2021.
* Rakhlin and Sridharan [2014] Alexander Rakhlin and Karthik Sridharan. Online non-parametric regression. In _Conference on Learning Theory_, pages 1232-1264. PMLR, 2014.
* Ross and Bagnell [2014] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. _arXiv preprint arXiv:1406.5979_, 2014.
* Ross et al. [2011] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
* Ross et al. [2013] Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J Andrew Bagnell, and Martial Hebert. Learning monocular reactive uav control in cluttered natural environments. In _2013 IEEE international conference on robotics and automation_, pages 1765-1772. IEEE, 2013.
* Russo and Roy [2013] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Sadigh et al. [2017] Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. _Active preference-based learning of reward functions_. 2017.
* Saha [2021] Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. _Advances in Neural Information Processing Systems_, 34:30050-30062, 2021.
* Saha and Gaillard [2021] Aadirupa Saha and Pierre Gaillard. Dueling bandits with adversarial sleeping. _Advances in Neural Information Processing Systems_, 34:27761-27771, 2021.
* Saha and Gaillard [2022] Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences. In _International Conference on Machine Learning_, pages 19011-19026. PMLR, 2022.
* Saha and Krishnamurthy [2022] Aadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In _International Conference on Algorithmic Learning Theory_, pages 968-994. PMLR, 2022.
* Saha et al. [2023] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences. In _International Conference on Artificial Intelligence and Statistics_, pages 6263-6289. PMLR, 2023.
* Sekhari et al. [2023] Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. Selective sampling and imitation learning via online regression. _arXiv preprint arXiv:2307.04998_, 2023.
* Seshia et al. [2018]David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 47(3):1904-1931, 2022.
* Simchowitz and Jamieson (2019) Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. _Advances in Neural Information Processing Systems_, 32, 2019.
* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* Sun et al. (2017) Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In _International conference on machine learning_, pages 3309-3318. PMLR, 2017.
* Taranovic et al. (2022) Aleksandar Taranovic, Andras Gabor Kupcsik, Niklas Freymuth, and Gerhard Neumann. Adversarial imitation learning with preferences. In _The Eleventh International Conference on Learning Representations_, 2022.
* Tsybakov (2004) Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. _The Annals of Statistics_, 32(1):135-166, 2004.
* Wang et al. (2020) Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135, 2020.
* Wen and Van Roy (2013) Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in deterministic systems. _Advances in Neural Information Processing Systems_, 26, 2013.
* Wirth and Furnkranz (2014) Christian Wirth and Johannes Furnkranz. On learning from game annotations. _IEEE Transactions on Computational Intelligence and AI in Games_, 7(3):304-316, 2014.
* Wirth et al. (2017) Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes Furnkranz, et al. A survey of preference-based reinforcement learning methods. _Journal of Machine Learning Research_, 18(136):1-46, 2017.
* Wu and Liu (2016) Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. _Advances in neural information processing systems_, 29, 2016.
* Wu et al. (2023) Yue Wu, Tao Jin, Hao Lou, Farzad Farnoud, and Quanquan Gu. Borda regret minimization for generalized linear dueling bandits. _arXiv preprint arXiv:2303.08816_, 2023.
* Xu et al. (2020) Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. _Advances in Neural Information Processing Systems_, 33:18784-18794, 2020.
* Yue and Joachims (2011) Yisong Yue and Thorsten Joachims. Beat the mean bandit. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 241-248. Citeseer, 2011.
* Yue et al. (2012) Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. _Journal of Computer and System Sciences_, 78(5):1538-1556, 2012.
* Zhan et al. (2023) Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback efficiently in rl? _arXiv preprint arXiv:2305.18505_, 2023.
* Zhang et al. (2022) David Zhang, Micah Carroll, Andreea Bobu, and Anca Dragan. Time-efficient reward learning via visually assisted cluster ranking. _arXiv preprint arXiv:2212.00169_, 2022.
* Zhu et al. (2023) Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human feedback from pairwise or \(k\)-wise comparisons. In _International Conference on Machine Learning_. PMLR, 2023.
* Zhu and Nowak (2022) Yinglun Zhu and Robert Nowak. Efficient active learning with abstention. _Advances in Neural Information Processing Systems_, 35:35379-35391, 2022.
* Zhu et al. (2020)Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In _Proceedings of the 23rd national conference on Artificial intelligence-Volume 3_, pages 1433-1438, 2008.
* Zoghi et al. (2014) Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In _International conference on machine learning_, pages 10-18. PMLR, 2014.

Computational tractability of Algorithm 1

We observe that the computational complexity of the proposed algorithm mainly depends on the computation of the candidate arm set (Line 3) and the width (Line 6). When \(\mathcal{F}\) is a \(d\)-dimensional linear function class, the computational complexity can be \(\tilde{O}(dTA)\) since the version space exhibits an ellipsoid structure and thus both the candidate arm and the width can be computed in \(\tilde{O}(dA)\) time. When \(\mathcal{F}\) is tabular, it can be considered as a special case of linear class with one-hot encoding. In this case, we have \(d=S\times A\), resulting in a computational complexity of \(\tilde{O}(TSA^{2})\).

For a more general convex function class \(\mathcal{F}\), we can design an efficient algorithm based on a weighted regression oracle for \(\mathcal{F}\). To this end, we first note that an approach to efficiently compute the width has been proposed by Foster et al. (2018), and now we propose the following method to compute the candidate arm set. An arm \(a\) belongs to the candidate arm set at round \(t\) if and only if the following optimization problem (with an constant objective) is feasible

\[\min_{f\in\mathcal{F},\,\xi\in\mathbb{R}^{A}}\,1\quad\text{s.t.} \quad f(x,a,a^{\prime})=\xi_{a^{\prime}}\quad,\quad\xi_{a^{\prime}}>0\quad( \forall a^{\prime}\neq a),\] \[\text{and}\quad\sum_{s=1}^{t-1}Z_{s}\left(f(x_{s},a_{s},b_{s})-f_ {t}(x_{s},a_{s},b_{s})\right)^{2}\leq\beta.\]

Here we introduce the slack variable \(\xi\) so that the optimization part for \(f\) can be simply reduced to a weighted regression oracle. Next, we convert the above into Lagrangian formulation and obtain

\[\min_{f\in\mathcal{F},\,\xi\in\mathbb{R}^{A}}\max_{a\in\mathbb{R}^{ A}_{+},\gamma\in\mathbb{R}^{A}_{+},\lambda\in\mathbb{R}_{+}} 1 +\sum_{a^{\prime}\neq a}\alpha_{a^{\prime}}\big{(}f(x,a,a^{\prime })-\xi_{a^{\prime}}\big{)}^{2}-\sum_{a^{\prime}\neq a}\gamma_{a^{\prime}}\xi_{a^ {\prime}}\] \[+\lambda\left(\sum_{s=1}^{t-1}Z_{s}\left(f(x_{s},a_{s},b_{s})-f_{t }(x_{s},a_{s},b_{s})\right)^{2}-\beta\right)\] \[=\max_{\alpha\in\mathbb{R}^{A}_{+},\gamma\in\mathbb{R}^{A}_{+}, \lambda\in\mathbb{R}_{+}}\min_{f\in\mathcal{F},\,\xi\in\mathbb{R}^{A}} 1 +\sum_{a^{\prime}\neq a}\alpha_{a^{\prime}}\big{(}f(x,a,a^{\prime })-\xi_{a^{\prime}}\big{)}^{2}-\sum_{a^{\prime}\neq a}\gamma_{a^{\prime}}\xi_{a ^{\prime}}\] \[+\lambda\left(\sum_{s=1}^{t-1}Z_{s}\left(f(x_{s},a_{s},b_{s})-f_ {t}(x_{s},a_{s},b_{s})\right)^{2}-\beta\right).\]

Here we can swap the min and max since the objective is convex in the joint space of \(f\) and \(\xi\). Then, the inner minimization problem can be solved by updating \(f\) via the regression oracle and updating \(\xi\) via gradient descent; for the outer maximization problem, we can do projected gradient ascent.

## Appendix B Preliminaries

**Lemma 2** (Kakade and Tewari (2008, Lemma 3)).: _Suppose \(X_{1},\ldots,X_{T}\) is a martingale difference sequence with \(|X_{t}|\leq b\). Let_

\[\operatorname{Var}_{t}X_{t}=\operatorname{Var}\left(X_{t}\mid X_{1},\ldots,X_{ t-1}\right)\]

_Let \(V=\sum_{t=1}^{T}\operatorname{Var}_{t}X_{t}\) be the sum of conditional variances of \(X_{t}\)'s. Further, let \(\sigma=\sqrt{V}\). Then we have, for any \(\delta<1/e\) and \(T\geq 3\),_

\[\Pr\left(\sum_{t=1}^{T}X_{t}>\max\{2\sigma,3b\sqrt{\ln(1/\delta)}\}\sqrt{\ln(1 /\delta)}\right)\leq 4\ln(T)\delta.\]

**Lemma 3** (Foster and Rakhlin (2020, Lemma 3)).: _For any vector \(\hat{y}\in[0,1]^{A}\), if we define \(p\) to be_

\[p(a)=\begin{cases}\frac{1}{A+\gamma\left(\hat{y}(\hat{a})-\hat{y}(a)\right)}& \text{if }a\neq\hat{a},\\ 1-\sum_{a\neq\hat{a}}p(a)&\text{if }a=\hat{a}\end{cases}\]

_where \(\hat{a}=\operatorname*{arg\,max}_{a}\hat{y}(a)\), then for any \(y^{\star}\in[0,1]^{A}\) and \(\gamma>0\), we have_

\[\operatorname*{\mathbb{E}}_{a\sim p}\left[\left(y^{\star}(a^{\star})-y^{\star }(a)\right)-\gamma\Big{(}\hat{y}(a)-y^{\star}(a)\Big{)}^{2}\right]\leq\frac{A }{\gamma}.\]

**Lemma 4** (Zhu and Nowak (2022, Lemma 2)).: _Let \((Z_{t})_{t\leq T}\) to be real-valued sequence of positive random variables adapted to a filtration \(\mathfrak{F}_{t}\). If \(|Z_{t}|\leq B\) almost surely, then with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}Z_{t}\leq\frac{3}{2}\sum_{t=1}^{T}\mathbb{E}_{t}\left[Z_{t} \right]+4B\log\left(2\delta^{-1}\right),\]

_and_

\[\sum_{t=1}^{T}\mathbb{E}_{t}\left[Z_{t}\right]\leq 2\sum_{t=1}^{T}Z_{t}+8B\log \left(2\delta^{-1}\right).\]

**Lemma 5** (Performance difference lemma (Agarwal et al., 2019)).: _For any two policies \(\pi\) and \(\pi^{\prime}\) and any state \(x_{0}\in\mathcal{X}\), we have_

\[V_{0}^{\pi}(x_{0})-V_{0}^{\pi^{\prime}}(x_{0})=\sum_{h=0}^{H-1}\underset{x_{h },a_{h}\sim d_{x_{0},h}^{\pi}}{\mathbb{E}}\left[A_{h}^{\pi^{\prime}}(x_{h},a_ {h})\right]\]

_where \(A_{h}^{\pi}(x,a)=Q_{h}^{\pi}(x,a)-V_{h}^{\pi}(x,a)\) and \(d_{x_{0},h}^{\pi}(x,a)\) is the probability of \(\pi\) reaching the state-action pair \((x,a)\) at time step \(h\) starting from initial state \(x_{0}\)._

**Lemma 6**.: _For any two Bernoulli distributions \(\mathrm{Bern}(x)\) and \(\mathrm{Bern}(y)\) with \(x,y\in[b,1-b]\) for some \(0<b\leq 1/2\), the KL divergence is bounded as_

\[\mathrm{KL}\Big{(}\mathrm{Bern}(x),\mathrm{Bern}(y)\Big{)}\leq\frac{2(x-y)^{2 }}{b}.\]

Proof of Lemma 6.: Denote \(\Delta=x-y\). Then, by definition, we have

\[\mathrm{KL}\Big{(}\mathrm{Bern}(x),\mathrm{Bern}(y)\Big{)}= x\ln\frac{x}{y}+(1-x)\ln\frac{1-x}{1-y}\] \[= x\ln\frac{x}{x-\Delta}+(1-x)\ln\frac{1-x}{1-x+\Delta}\] \[= x\ln\left(1+\frac{\Delta}{x-\Delta}\right)+(1-x)\ln\left(1- \frac{\Delta}{1-x+\Delta}\right)\]

Since \(\ln(1+x)\leq x\) for all \(x>-1\), we have

\[\mathrm{KL}\Big{(}\mathrm{Bern}(x),\mathrm{Bern}(y)\Big{)}\leq x\cdot\frac{\Delta}{x-\Delta}-(1-x)\cdot\frac{\Delta}{1-x+\Delta}\] \[= \Delta\cdot\left(\frac{x}{x-\Delta}-\frac{1-x}{1-x+\Delta}\right)\] \[= \Delta\cdot\left(\frac{\Delta}{x-\Delta}+\frac{\Delta}{1-x+ \Delta}\right)\] \[\leq \Delta^{2}\cdot\left(\frac{1}{y}+\frac{1}{1-y}\right)\leq\frac{2 \Delta^{2}}{b}.\]

## Appendix C Missing Proofs

### Supporting Lemmas

**Definition 3** (Strong convexity).: _A function \(\Phi:[-1,1]\to\mathbb{R}\) is \(\alpha\)-strongly-convex if for all \(u,u^{\prime}\in\mathbb{R}\), we have_

\[\frac{\alpha}{2}(u^{\prime}-u)^{2}\leq\Phi(u^{\prime})-\Phi(u)-\nabla\Phi(u)( u^{\prime}-u).\]

_where \(\nabla\Phi\) means the derivative of \(\Phi\)._

**Lemma 7**.: _For any \(t\in[T]\), if \(f^{\star}\in\mathcal{F}_{t}\), then we have \(w_{t}\geq\Delta\) whenever \(|\mathcal{A}_{t}|>1\)._Proof of lemma 7.: When \(|\mathcal{A}_{t}|>1\), we know there exists a function \(f^{\prime}\in\mathcal{F}_{t}\) satisfying

\[a^{\prime}:=\pi_{f^{\prime}}(x_{t})\neq\pi_{f^{\star}}(x_{t})=:a_{t}^{\star}.\]

Then we have \(\Delta\leq f^{\star}(x_{t},a_{t}^{\star},a^{\prime})\leq f^{\star}(x_{t},a_{t} ^{\star},a^{\prime})-f^{\prime}(x_{t},a_{t}^{\star},a^{\prime})\leq w_{t}\) where the second inequality holds since \(f^{\prime}(x_{t},a_{t}^{\star},a^{\prime})\leq 0\). 

**Lemma 8**.: _For any \(t\in[T]\) and any arm \(a\in\mathcal{A}_{t}\), we have \(f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a)\leq w_{t}\)._

Proof of Lemma 8.: For any \(a\in\mathcal{A}_{t}\), by the definition of \(\mathcal{A}_{t}\), there must exists a function \(f\) for which \(a=\pi_{f}(x_{t})\). Hence,

\[f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a)\leq f^{\star}(x_{t},\pi_{f^{\star}}( x_{t}),a)-f(x_{t},\pi_{f^{\star}}(x_{t}),a)\leq w_{t},\]

where the first inequality holds since \(f(x_{t},\pi_{f^{\star}}(x_{t}),a)\leq 0\). 

The following lemma is adapted from Agarwal (2013, Lemma 2).

**Lemma 9**.: _The following holds with probability at least \(1-\delta\) for any \(T>3\),_

\[\sum_{t=1}^{T}Z_{t}\big{(}f^{\star}(x_{t},a_{t},b_{t})-f_{t}(x_{t},a_{t},b_{t} )\big{)}^{2}\leq\frac{4\Upsilon}{\alpha}+\frac{16+24\alpha}{\alpha^{2}}\log \big{(}4\delta^{-1}\log(T)\big{)}.\]

Proof of Lemma 9.: Throughout the proof, we denote \(z_{t}:=(x_{t},a_{t},b_{t})\) for notational simplicity. We define \(D_{\Phi}\) as the Bregman divergence of the function \(\Phi\):

\[D_{\Phi}(u,v)=\Phi(u)-\Phi(v)-\phi(v)(u-v)\]

where we recall that \(\phi=\Phi^{\prime}\) is the derivative of \(\Phi\). Since \(\Phi\) is \(\alpha\)-strong convex, we have \(\alpha(u-v)^{2}/2\leq D_{\Phi}(u,v)\), and hence,

\[\sum_{t=1}^{T}Z_{t}\big{(}f^{\star}(z_{t})-f_{t}(z_{t})\big{)}^{2}\leq\frac{ 2}{\alpha}\sum_{t=1}^{T}Z_{t}D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t})).\] (2)

Hence, it suffice to derive an upper bound for the Bregman divergence in the right hand side above. Define \(\nu_{t}\) as below:

\[\nu_{t}:= Z_{t}\Big{[}D_{\Phi}\left(f_{t}(z_{t}),f^{\star}(z_{t})\right)- \left(\ell_{\phi}\left(f_{t}(z_{t}),y_{t}\right)-\ell_{\phi}\left(f^{\star}(z _{t}),y_{t}\right)\right)\Big{]}\] \[= Z_{t}\Big{[}D_{\Phi}\left(f_{t}(z_{t}),f^{\star}(z_{t})\right)- \left(\Phi\left(f_{t}(z_{t})\right)-(y_{t}+1)f_{t}(z_{t})/2-\Phi\left(f^{ \star}(z_{t})\right)+(y_{t}+1)f^{\star}(z_{t})/2\right)\Big{]}\] \[= Z_{t}\Big{[}\Phi\left(f_{t}(z_{t})\right)-\Phi\left(f^{\star}(z _{t})\right)-\phi\left(f^{\star}(z_{t})\right)\left(f_{t}(z_{t})-f^{\star}(z _{t})\right)\] \[\quad-\left(\Phi\left(f_{t}(z_{t})\right)-(y_{t}+1)f_{t}(z_{t})/2 -\Phi\left(f^{\star}(z_{t})\right)+(y_{t}+1)f^{\star}(z_{t})/2\right)\Big{]}\] \[= Z_{t}\big{(}f_{t}(z_{t})-f^{\star}(z_{t})\big{)}\big{(}(y_{t}+1) /2-\phi(f^{\star}(z_{t}))\big{)}\]

We note that \(\mathbb{E}_{t}[(y_{t}+1)/2]=\phi(f^{\star}(z_{t}))\), and thus \(\mathbb{E}_{t}[\nu_{t}]=0\), which means \(\nu_{t}\) is a martingale difference sequence. Now we bound the value and the conditional variance of \(\nu_{t}\) in order to derive concentration results.

1. Bound the value of \(\nu_{t}\): \[|\nu_{t}|\leq \left|(y_{t}+1)/2-\phi\left(f^{\star}(z_{t})\right)|\cdot|f_{t}(z_{t})-f ^{\star}(z_{t})|\leq 1\cdot 2=2.\]
2. Bound the conditional variance of \(\nu_{t}\): \[\mathbb{E}_{t}[\nu_{t}^{2}]= Z_{t}\,\mathbb{E}_{t}\,\mathbb{E}_{t}\,\Big{[}((y_{t}+1)/2- \phi\left(f^{\star}(z_{t})\right))^{2}\left(f_{t}(z_{t})-f^{\star}(z_{t}) \right)^{2}\Big{]}\] \[\leq Z_{t}\,\mathbb{E}_{t}\,\Big{[}(f_{t}(z_{t})-f^{\star}(z_{t}))^{2} \Big{]}\] \[\leq Z_{t}\,\mathbb{E}_{t}\,\mathbb{E}_{t}\,\Big{[}\frac{2}{\alpha} \cdot D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t}))\Big{]}\] \[\leq \frac{2Z_{t}}{\alpha}D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t}))\] where for the last line we note that \(x_{t},g_{t}\) are measurable at \(t\).

Now we apply Lemma 2, which yields for any \(\delta<1/e\) and \(T>3\), with probability at least \(1-4\delta\log(T)\),

\[\sum_{t=1}^{T}\nu_{t}\leq \max\left\{2\sqrt{\sum_{t=1}^{T}\frac{2Z_{t}}{\alpha}D_{\Phi}(f_{t} (z_{t}),f^{\star}(z_{t}))},6\sqrt{\log(1/\delta)}\right\}\sqrt{\log(1/\delta)}\] \[\leq 2\sqrt{\sum_{t=1}^{T}\frac{2Z_{t}}{\alpha}D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t}))\text{log}(1/\delta)}+6\text{log}(1/\delta)\qquad\text{( since }\max(a,b)\leq a+b\text{)}\] \[\leq \sum_{t=1}^{T}\frac{1}{2}Z_{t}D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t }))+\frac{4\log(1/\delta)}{\alpha}+6\text{log}(1/\delta)\] (AM-GM)

Recall the definition of \(\nu_{t}\), and we conclude that

\[\sum_{t=1}^{T}Z_{t}D_{\Phi}\left(f_{t}(z_{t}),f^{\star}(z_{t}) \right)-\sum_{t=1}^{T}Z_{t}\Big{(}\ell_{\phi}\big{(}f_{t}(z_{t}),y_{t}\big{)}- \ell_{\phi}\big{(}f^{\star}(z_{t}),y_{t}\big{)}\Big{)}\leq\] \[\sum_{t=1}^{T}\frac{1}{2}Z_{t}D_{\Phi}(f_{t}(z_{t}),f^{\star}(z_{t }))+\frac{4\log(1/\delta)}{\alpha}+6\text{log}(1/\delta),\]

which implies

\[\frac{1}{2}\sum_{t=1}^{T}Z_{t}D_{\Phi}\left(f_{t}(z_{t}),f^{\star}(z_{t}) \right)\leq\sum_{t=1}^{T}Z_{t}\Big{(}\ell_{\phi}\big{(}f_{t}(z_{t}),y_{t}\big{)} -\ell_{\phi}\big{(}f^{\star}(z_{t}),y_{t}\big{)}\Big{)}+\frac{4\log(1/\delta)} {\alpha}+6\text{log}(1/\delta).\]

Plugging this upper bound of Bregman divergence into (2), we obtain that, with probability at least \(1-4\delta\log(T)\), for any \(\delta<1/e\) and \(T>3\), we have

\[\sum_{t=1}^{T}Z_{t}\big{(}f^{\star}(z_{t})-f_{t}(z_{t})\big{)}^{2}\leq\frac{4 }{\alpha}\Upsilon+\left(\frac{16}{\alpha^{2}}+\frac{24}{\alpha}\right)\log( \delta^{-1})=:\beta\]

Finally, we finish the proof by adjusting the coefficient \(\delta\) and taking a union bound to obtain the desired result. 

The following lemma is a variant of Russo and Van Roy (2013, Proposition 3), with the main difference being that (1) the version space is established using the function produced by the oracle instead of the least squares estimator, and (2) the extra multiplicative factor \(Z_{t}\).

**Lemma 10**.: _For Algorithm 1, it holds that_

\[\sum_{t=1}^{T}Z_{t}\mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{F}_{t}}f(x_ {t},a_{t},b_{t})-f^{\prime}(x_{t},a_{t},b_{t})>\epsilon\right\}\leq\left(\frac {4\beta}{\epsilon^{2}}+1\right)\dim_{E}(\mathcal{F},\epsilon)\] (3)

_for any constant \(\epsilon>0\),_

Proof of Lemma 10.: We first define a subsequence consisting only of the elements for which we made a query in that round. Specifically, we define \(((x_{i_{1}},a_{i_{1}},b_{i_{1}}),(x_{i_{2}},a_{i_{2}},b_{i_{2}}),\ldots,(x_{i_{ k}},a_{i_{k}},b_{i_{k}}))\) where \(1\leq i_{1}<i_{2}<\cdots<i_{k}\leq T\) and \((x_{t},a_{t},b_{t})\) belongs to the subsequence if and only if \(Z_{t}=1\). We further simplify the notation by defining \(z_{j}:=(x_{i_{j}},a_{i_{j}},b_{i_{j}})\) and \(f(z_{j}):=f(x_{i_{j}},a_{i_{j}},b_{i_{j}})\). Then we note that the left-hand side of (3) is equivalent to

\[\sum_{j=1}^{k}\mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{F}_{j}}f(z_{j})- f^{\prime}(z_{j})>\epsilon\right\},\] (4)

and the version space in Algorithm 1 is equal to

\[\mathcal{F}_{j}=\left\{f\in\mathcal{F}:\sum_{s=1}^{j-1}\Big{(}f(z_{s})-f_{s}(z_ {s})\Big{)}^{2}\leq\beta\right\}.\] (5)Hence, it suffice to establish the lower bound for (4) under the version space of (5). To that end, we make one more simplification in notation: we denote

\[w^{\prime}_{j}:=\sup_{f,f^{\prime}\in\mathcal{F}_{j}}f(z_{j})-f^{\prime}(z_{j})\]

We begin by showing that if \(w^{\prime}_{j}>\epsilon\) for some \(j\in[k]\), then \(z_{j}\) is \(\epsilon\)-dependent on at most \(4\beta/\epsilon^{2}\) disjoint subsequence of its predecessors. To see this, we note that when \(w^{\prime}_{j}>\epsilon\), there must exist two function \(f,f^{\prime}\in\mathcal{F}_{j}\) such that \(f(z_{j})-f^{\prime}(z_{j})>\epsilon\). If \(z_{j}\) is \(\epsilon\)-dependent on a subsequence \((z_{i_{1}},z_{i_{2}},\ldots,z_{i_{n}})\) of its predecessors, we must have

\[\sum_{s=1}^{n}\big{(}f(z_{i_{s}})-f^{\prime}(z_{i_{s}})\big{)}^{2}>\epsilon^{2}.\]

Hence, if \(z_{j}\) is \(\epsilon\)-dependent on \(l\) disjoint subsequences, we have

\[\sum_{s=1}^{j-1}\big{(}f(z_{s})-f^{\prime}(z_{s})\big{)}^{2}>l\epsilon^{2}.\] (6)

For the left-hand side above, we also have

\[\sum_{s=1}^{j-1}\big{(}f(z_{s})-f^{\prime}(z_{s})\big{)}^{2}\leq 2\sum_{s=1}^{ j-1}\big{(}f(z_{s})-f_{s}(z_{s})\big{)}^{2}+2\sum_{s=1}^{j-1}\big{(}f_{s}(z_{s}) -f^{\prime}(z_{s})\big{)}^{2}\leq 4\beta\] (7)

where the first inequality holds since \((a+b)^{2}\leq 2(a^{2}+b^{2})\) for any \(a,b\), and the second inequality holds by (5). Combining (6) and (7), we get that \(l\leq 4\beta/\epsilon^{2}\).

Next, we show that for any sequence \((z^{\prime}_{1},\ldots,z^{\prime}_{r})\), there is at least one element that is \(\epsilon\)-dependent on at least \(\tau/d-1\) disjoint subsequence of its predecessors, where \(d:=\dim_{E}(\mathcal{F},\epsilon)\). To show this, let \(m\) be the integer satisfying \(md+1\leq\tau\leq md+d\). We will construct \(m\) disjoint subsequences, \(B_{1},\ldots,B_{m}\). At the beginning, let \(B_{i}=(z^{\prime}_{i})\) for \(i\in[m]\). If \(z^{\prime}_{m+1}\) is \(\epsilon\)-dependent on each subsequence \(B_{1},\ldots,B_{m}\), then we are done. Otherwise, we select a subsequence \(B_{i}\) which \(z^{\prime}_{m+1}\) is \(\epsilon\)-independent of and append \(z^{\prime}_{m+1}\) to \(B_{i}\). We repeat this process for all elements with indices \(j>m+1\) until either \(z^{\prime}_{j}\) is \(\epsilon\)-dependent on each subsequence or \(j=\tau\). For the latter, we have \(\sum_{i=1}^{m}|B_{i}|\geq md\), and since each element of a subsequence \(B_{i}\) is \(\epsilon\)-independent of its predecessors, we must have \(|B_{i}|=d\) for all \(i\). Then, \(z_{\tau}\) must be \(\epsilon\)-dependent on each subsequence by the definition of eluder dimension.

Finally, let's take the sequence \((z^{\prime}_{1},\ldots z^{\prime}_{\tau})\) to be the subsequence of \((z_{1},\ldots,z_{k})\) consisting of elements \(z_{j}\) for which \(w^{\prime}_{j}>\epsilon\). As we have established, we have (1) each \(z^{\prime}_{j}\) is \(\epsilon\)-dependent on at most \(4\beta/\epsilon^{2}\) disjoint subsequences, and (2) some \(z^{\prime}_{j}\) is \(\epsilon\)-dependent on at least \(\tau/d-1\) disjoint subsequences. Therefore, we must have \(\tau/d-1\leq 4\beta/\epsilon^{2}\), implying that \(\tau\leq(4\beta/\epsilon^{2}+1)d\). 

The following lemma is adopted from Saha and Krishnamurthy (2022, Lemma 3).

**Lemma 11**.: _For any function \(f\in\mathcal{F}\) and any context \(x\in\mathcal{X}\), the following convex program of \(p\in\Delta(\mathcal{A})\) is always feasible:_

\[\forall a\in\mathcal{A}:\sum_{b}f(x,a,b)p(b)+\frac{2}{\gamma p(a)}\leq\frac{5A }{\gamma}.\]

_Furthermore, any solution \(p\) satisfies:_

\[\operatorname*{\mathbb{E}}_{a\sim p}\Big{[}f^{\star}(x,\pi_{f^{\star}}(x),a) \Big{]}\leq\frac{\gamma}{4}\operatorname*{\mathbb{E}}_{a,b\sim p}\Big{[}\big{(} f(x,a,b)-f^{\star}(x,a,b)\big{)}^{2}\Big{]}+\frac{5A}{\gamma}\]

_whenever \(\gamma\geq 2A\)._

**Lemma 12**.: _Assume that for each \(f\in\mathcal{F}\), there exists an associated function \(r:\mathcal{X}\times\mathcal{A}\to[0,1]\) such that \(f(x,a,b)=r(x,a)-r(x,b)\) for any \(x\in\mathcal{X}\) and \(a,b\in\mathcal{A}\). In this case, for any context \(x\in\mathcal{X}\), if we define \(p\) as_

\[p(a)=\begin{cases}\frac{1}{A+\gamma\big{(}r(x,\pi_{f}(x))-r(x,a)\big{)}}&a\neq \pi_{f}(x)\\ 1-\sum_{a\neq\pi_{f}(x)}p(a)&a=\pi_{f}(x)\end{cases},\]_then we have_

\[\mathop{\mathbb{E}}_{a\sim p}\left[f^{\star}(x,\pi_{f^{\star}}(x),a)\right]\leq \gamma\mathop{\mathbb{E}}_{a,b\sim p}\left[\left(f(x,a,b)-f^{\star}(x,a,b) \right)^{2}\right]+\frac{A}{\gamma}\]

Proof of lemma 12.: Fix any \(b\in\mathcal{A}\). Then, the distribution \(p\) can be rewritten as

\[p(a)=\begin{cases}\left(A+2\gamma\left(\frac{r(x,\pi_{f^{\star}}(x))-r(x,b)+1} {2}-\frac{r(x,a)-r(x,b)+1}{2}\right)\right)^{-1}&a\neq\pi_{f}(x)\\ 1-\sum_{a\neq\pi_{f}(x)}p(a)&a=\pi_{f}(x)\end{cases}.\]

Therefore, denoting \(f^{\star}(x,a,b)=r^{\star}(x,a)-r^{\star}(x,b)\) for some function \(r^{\star}\), we have

\[\mathop{\mathbb{E}}_{a\sim p}\left[f^{\star}(x,\pi_{f^{\star}}(x),a)\right]= \mathop{\mathbb{E}}_{a\sim p}\left[r^{\star}(x,\pi_{f^{\star}}(x) )-r^{\star}(x,a)\right]\] \[= 2\mathop{\mathbb{E}}_{a\sim p}\left[\frac{r^{\star}(x,\pi_{f^{ \star}}(x))-r^{\star}(x,b)+1}{2}-\frac{r^{\star}(x,a)-r^{\star}(x,b)+1}{2}\right]\] \[\leq 2\cdot 2\gamma\mathop{\mathbb{E}}_{a\sim p}\left[\left(\frac{r(x,a )-r(x,b)+1}{2}-\frac{r^{\star}(x,a)-r^{\star}(x,b)+1}{2}\right)^{2}\right]+ \frac{A}{\gamma}\] \[= \gamma\mathop{\mathbb{E}}_{a\sim p}\left[\left(f(x,a,b)-f^{\star }(x,a,b)\right)^{2}\right]+\frac{A}{\gamma}\]

where for the inequality above we invoked Lemma 3 with \(\hat{y}(a)=(r(x,a)-r(x,b)+1)/2\) and \(y^{\star}(a)=(r^{\star}(x,a)-r^{\star}(x,b)+1)/2\). We note that the above holds for any \(b\in\mathcal{A}\). Hence, we complete the proof by sampling \(b\sim p\). 

**Lemma 13**.: _Assume \(f^{\star}\in\mathcal{F}_{t}\) for all \(t\in[T]\). Suppose there exists some \(t^{\prime}\in[T]\) such that \(\lambda_{t}=0\) for all \(t\leq t^{\prime}\). Then we have_

\[\sum_{t=1}^{t^{\prime}}Z_{t}w_{t}\leq 56A^{2}\beta\cdot\frac{\dim_{E} \left(\mathcal{F},\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\]

with probability at least \(1-\delta\).

Proof.: Since \(f^{\star}\in\mathcal{F}_{t}\), we always have \(\pi_{f^{\star}}(x_{t})\in\mathcal{A}_{t}\) for all \(t\in[T]\). Hence, whenever \(Z_{t}\) is zero, we have \(\mathcal{A}_{t}=\{\pi_{f^{\star}}(x_{t})\}\) and thus we do not incur any regret. Hence, we know \(Z_{t}w_{t}\) is either 0 or at least \(\Delta\) by Lemma 7. Let us fix an integer \(m>1/\Delta\), whose value will be specified later. We divide the interval \([\Delta,1]\) into bins of width \(1/m\) and conduct a refined study of the sum of \(Z_{t}w_{t}\):

\[\sum_{t=1}^{t^{\prime}}Z_{t}w_{t}\leq \sum_{t=1}^{t^{\prime}}\sum_{j=0}^{(1-\Delta)m-1}Z_{t}w_{t}\cdot \mathds{1}\left\{Z_{t}w_{t}\in\left[\Delta+\frac{j}{m},\,\Delta+\frac{j+1}{m} \right]\right\}\] \[\leq \sum_{j=0}^{(1-\Delta)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{ t=1}^{t^{\prime}}Z_{t}\mathds{1}\left\{w_{t}\geq\Delta+\frac{j}{m}\right\}\] \[= \sum_{j=0}^{(1-\Delta)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{ t=1}^{t^{\prime}}Z_{t}\mathds{1}\left\{\sup_{a,b\in\mathcal{A}_{t}}\sup_{f,f^{ \prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq\Delta+\frac{j} {m}\right\}\] \[= \sum_{j=0}^{(1-\Delta)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{ t=1}^{t^{\prime}}Z_{t}\sup_{a,b\in\mathcal{A}_{t}}\mathds{1}\left\{\sup_{f,f^{ \prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq\Delta+\frac{j} {m}\right\}\] \[\leq \sum_{j=0}^{(1-\Delta)m-1}\left(\Delta+\frac{j+1}{m}\right)A^{2} \underbrace{\sum_{t=1}^{t^{\prime}}Z_{t}\mathop{\mathbb{E}}_{a,b\sim p_{t}} \mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x _{t},a,b)\geq\left(\Delta+\frac{j}{m}\right)\right\}}_{(*)}\]where in the third inequality we replace the supremum over \(a,b\) by the summation over \(a,b\), and in the last inequality we further replace it by the expectation. Here recall that \(p_{t}(a)\) is uniform when \(\lambda_{t}=0\), leading to the extra \(A^{2}\) factor. To deal with \((*)\), we first apply Lemma 4 to recover the empirical \(a_{t}\) and \(b_{t}\), and then apply Lemma 10 to get an upper bound via the eluder dimension:

\[(*)\leq 2\sum_{t=1}^{t^{\prime}}Z_{t}\mathds{1}\left\{\sup_{f,f^{\prime} \in\mathcal{F}_{t}}f(x_{t},a_{t},b_{t})-f^{\prime}(x_{t},a_{t},b_{t})\geq\left( \Delta+\frac{j}{m}\right)\right\}+8\log(\delta^{-1})\] \[\leq 2\left(\frac{4\beta}{\left(\Delta+\frac{j}{m}\right)^{2}}+1 \right)\dim_{E}\left(\mathcal{F};\Delta\right)+8\log(\delta^{-1})\] \[\leq \frac{10\beta}{\left(\Delta+\frac{j}{m}\right)^{2}}\cdot\dim_{E} \left(\mathcal{F};\Delta\right)+8\log(\delta^{-1})\]

with probability at least \(1-\delta\). Plugging \((*)\) back, we obtain

\[\sum_{t=1}^{t^{\prime}}Z_{t}w_{t}\leq \sum_{j=0}^{(1-\Delta)m-1}\left(\Delta+\frac{j+1}{m}\right)\cdot \frac{10A^{2}\beta}{\left(\Delta+\frac{j}{m}\right)^{2}}\cdot\dim_{E}\left( \mathcal{F};\Delta\right)+8mA^{2}\log(\delta^{-1})\] \[= 10A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\sum_{j =0}^{(1-\Delta)m-1}\frac{\Delta+\frac{j+1}{m}}{\left(\Delta+\frac{j}{m}\right) ^{2}}+8mA^{2}\log(\delta^{-1})\] \[\leq 10A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\left( \frac{\Delta+1/m}{\Delta^{2}}+\sum_{j=1}^{(1-\Delta)m-1}\frac{2}{\Delta+\frac {j}{m}}\right)+8mA^{2}\log(\delta^{-1})\] \[\leq 10A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\sum_{j =0}^{(1-\Delta)m-1}\frac{2}{\Delta+\frac{j}{m}}+8mA^{2}\log(\delta^{-1})\] \[\leq 20A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\sum_{j =0}^{(1-\Delta)m-1}\int_{j-1}^{j}\frac{1}{\Delta+\frac{x}{m}}\,\mathrm{d}x+8mA ^{2}\log(\delta^{-1})\] \[= 20A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\int_{- 1}^{(1-\Delta)m-1}\frac{1}{\Delta+\frac{x}{m}}\,\mathrm{d}x+8mA^{2}\log( \delta^{-1})\] \[= 20A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\cdot m \log\left(\frac{1}{\Delta-m^{-1}}\right)+8mA^{2}\log(\delta^{-1})\]

where for the second inequality, we use the fact that \((j+1)/m\leq 2j/m\) for any \(j\geq 1\); for the third inequality, we assume \(m>1/\Delta\). Setting \(m=2/\Delta\), we arrive at

\[\sum_{t=1}^{t^{\prime}}Z_{t}w_{t}\leq 40A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{ \Delta}\cdot\log(2/\Delta)+16A^{2}\log(\delta^{-1})/\Delta\] \[\leq 56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{ \Delta}\cdot\log(2/(\delta\Delta)),\]

which completes the proof. 

**Lemma 14**.: _Whenever_

\[56A^{2}\beta\cdot\dim_{E}\left(\mathcal{F},\Delta\right)\cdot\log(2/(\delta \Delta))/\Delta<\sqrt{AT/\beta},\]

_we have \(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{T}=0\) with probability at least \(1-\delta\)._

Proof of Lemma 14.: We prove it via contradiction. Assume the inequality holds but there exists \(t^{\prime}\) for which \(\lambda_{t^{\prime}}=1\). Without loss of generality, we assume that \(\lambda_{t}=0\) for all \(t<t^{\prime}\), namely that \(t^{\prime}\) is the first time that \(\lambda_{t}\) is 1. Then by definition of \(\lambda_{t^{\prime}}\), we have

\[\sum_{s=1}^{t^{\prime}-1}Z_{s}w_{s}\geq\sqrt{AT/\beta}.\]On the other hand, by Lemma 13, we have

\[\sum_{s=1}^{t^{\prime}-1}Z_{s}w_{s}\leq 56A^{2}\beta\cdot\frac{\dim_{E} \left(\mathcal{F},\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta)).\]

The combination of the above two inequalities contradicts with the conditions. 

### Proof of Lemma 1

Proof of Lemma 1.: We prove it via contradiction. If no such arm exists, meaning that for any arm \(a\), there exists an arm \(b\) such that \(f^{\star}(x,a,b)<0\). Then we can find a sequence of arms \((a_{1},a_{2},\ldots,a_{k})\) such that \(f^{\star}(x,a_{i},a_{i+1})<0\) for any \(i=1,\ldots,k-1\) and \(f^{\star}(x,a_{k},a_{1})<0\), which contradicts with the transitivity (Assumption 1). 

### Proof of Theorem 1

We begin by showing the worst-case regret upper bound.

**Lemma 15** (Worst-case regret upper bound).: _For Algorithm 1, assume \(f^{\star}\in\mathcal{F}_{t}\) for all \(t\in[T]\). Then, we have_

\[\mathrm{Regret}_{T}^{\mathrm{CB}}\leq 68\sqrt{AT\beta}\cdot\log(4 \delta^{-1})\]

_with probability at least \(1-\delta\)._

Proof of Lemma 15.: We recall that the regret is defined as

\[\mathrm{Regret}_{T}^{\mathrm{CB}}=\sum_{t=1}^{T}\big{(}f^{\star}( x_{t},\pi_{f^{\star}}(x_{t}),a_{t})+f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),b_{t}) \big{)}.\]

Since \(a_{t}\) and \(b_{t}\) are always drawn independently from the same distribution in Algorithm 1, we only need to consider the regret of the \(a_{t}\) part in the following proof for brevity -- multiplying the result by two would yield the overall regret.

We first observe the definition of \(\lambda_{t}\) in Algorithm 1: the left term \(\sum_{s=1}^{t-1}Z_{s}w_{s}\) in the indicator is non-decreasing in \(t\) while the right term remains constant. This means that there exists a particular time step \(t^{\prime}\in[T]\) dividing the time horizon into two phases: \(\lambda_{t}=0\) for all \(t\leq t^{\prime}\) and \(\lambda_{t}=1\) for all \(t>t^{\prime}\). Now, we proceed to examine these two phases individually.

For all rounds before or on \(t^{\prime}\), we can compute the expected partial regret as

\[\sum_{t=1}^{t^{\prime}}\mathop{\mathbb{E}}_{a\sim p_{t}}\big{[} f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a)\big{]}=\sum_{t=1}^{t^{\prime}}Z_{t} \mathop{\mathbb{E}}_{a\sim p_{t}}\big{[}f^{\star}(x_{t},\pi_{f^{\star}}(x_{t} ),a)\big{]}\leq \sum_{t=1}^{t^{\prime}}Z_{t}w_{t}\leq\sqrt{AT\beta},\] (8)

where the equality holds since we have \(\mathcal{A}_{t}=\{\pi_{f^{\star}}(x_{t})\}\) whenever \(Z_{t}=0\) under the condition that \(f^{\star}\in\mathcal{F}_{t}\), and thus we don't incur regret in this case. The first inequality is Lemma 8, and the second inequality holds by the definition of \(\lambda_{t}\) and the condition that \(\lambda_{t}=0\).

On the other hand, for all rounds after \(t^{\prime}\), we have

\[\sum_{t=t^{\prime}+1}^{T}\mathop{\mathbb{E}}_{a\sim p_{t}}\left[f^{ \star}(x_{t},\pi_{f^{\star}}(x_{t}),a)\right]\] \[=\sum_{t=t^{\prime}+1}^{T}Z_{t}\mathop{\mathbb{E}}_{a\sim p_{t}} \left[f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a)\right]\] \[\leq\sum_{t=t^{\prime}+1}^{T}Z_{t}\left(\frac{5A}{\gamma_{t}}+ \frac{\gamma_{t}}{4}\mathop{\mathbb{E}}_{a,b\sim p_{t}}\left[\left(f^{\star}(x _{t},a,b)-f_{t}(x_{t},a,b)\right)^{2}\right]\right)\] \[=\sum_{t=t^{\prime}+1}^{T}Z_{t}\left(\frac{5A}{\sqrt{AT/\beta}}+ \frac{\sqrt{AT/\beta}}{4}\mathop{\mathbb{E}}_{a,b\sim p_{t}}\left[\left(f^{ \star}(x_{t},a,b)-f_{t}(x_{t},a,b)\right)^{2}\right]\right)\] \[\leq 5\sqrt{AT\beta}+\frac{\sqrt{AT/\beta}}{4}\sum_{t=t^{\prime}+1}^ {T}Z_{t}\mathop{\mathbb{E}}_{a,b\sim p_{t}}\left[\left(f^{\star}(x_{t},a,b)-f _{t}(x_{t},a,b)\right)^{2}\right]\] \[\leq 5\sqrt{AT\beta}+\frac{\sqrt{AT/\beta}}{2}\sum_{t=t^{\prime}+1}^ {T}Z_{t}\big{(}f^{\star}(x_{t},a_{t},b_{t})-f_{t}(x_{t},a_{t},b_{t})\big{)}^{ 2}+8\sqrt{AT/\beta}\cdot\log(4\delta^{-1})\] \[\leq 5\sqrt{AT\beta}+\frac{\sqrt{AT\beta}}{2}+8\sqrt{AT/\beta} \cdot\log(4\delta^{-1}).\] (9)

where the first inequality holds by Lemma 11 (or Lemma 12 for specific function classes), the second equality is by the definition of \(\gamma_{t}\), the third inequality is by Lemma 4, and the fourth inequality holds by Lemma 9.

Putting the two parts, (8) and (9), together, we arrive at

\[\sum_{t=1}^{T}\mathop{\mathbb{E}}_{a\sim p_{t}}\left[f^{\star}(x_{t},\pi_{f^{ \star}}(x_{t}),a)\right]\leq 7\sqrt{AT\beta}+8\sqrt{AT/\beta}\cdot\log(4 \delta^{-1})\leq 15\sqrt{AT\beta}\cdot\log(4\delta^{-1}).\]

Now we apply Lemma 4 again. The following holds with probability at least \(1-\delta/2\),

\[\sum_{t=1}^{T}f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a_{t})\leq 2\sum_{t=1}^ {T}\mathop{\mathbb{E}}_{a\sim p_{t}}\left[f^{\star}(x_{t},\pi_{f^{\star}}(x_{ t}),a)\right]+4\log(4\delta^{-1})\leq 34\sqrt{AT\beta}\cdot\log(4\delta^{-1}).\]

The above concludes the regret of the \(a_{t}\) part. The regret of the \(b_{t}\) can be shown in the same way. Adding them together, we conclude that

\[\mathrm{Regret}_{T}^{\mathrm{CB}}=\sum_{t=1}^{T}\left(f^{\star}(x_{t},\pi_{f^{ \star}}(x_{t}),a_{t})+f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),b_{t})\right)\leq 6 8\sqrt{AT\beta}\cdot\log(4\delta^{-1}).\]

**Lemma 16** (Instance-dependent regret upper bound).: _For Algorithm 1, assume \(f^{\star}\in\mathcal{F}_{t}\) for all \(t\in[T]\). Then, we have_

\[\mathrm{Regret}_{T}^{\mathrm{CB}}\leq 3808A^{2}\beta^{2}\cdot\frac{\dim_{E} \left(\mathcal{F},\Delta\right)}{\Delta}\cdot\log^{2}(4/(\delta\Delta))\]

_with probability at least \(1-\delta\)._

Proof of Lemma 16.: We consider two cases. First, when

\[56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{\Delta}\cdot \log(2/(\delta\Delta))<\sqrt{AT/\beta},\] (10)we invoke Lemma 14 and get that \(\lambda_{t}=0\) for all \(t\in[T]\). Hence, we have

\[\mathrm{Regret}_{T}^{\mathrm{CB}}= \sum_{t=1}^{T}\left(f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a_{t})+ f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),b_{t})\right)\] \[\leq 2\sum_{t=1}^{T}Z_{t}w_{t}\] \[\leq 112A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)} {\Delta}\cdot\log(2/(\delta\Delta))\] \[\leq 3808A^{2}\beta^{2}\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta \right)}{\Delta}\cdot\log^{2}(4/(\delta\Delta))\]

where the first inequality is by Lemma 8 and the fact that we incur no regret when \(Z_{t}=0\) since \(f^{\star}\in\mathcal{F}_{t}\). The second inequality is by Lemma 13.

On the other hand, when the contrary of (10) holds, i.e.,

\[56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{\Delta}\cdot \log(2/(\delta\Delta))\geq\sqrt{AT/\beta},\] (11)

applying Lemma 15, we have

\[\mathrm{Regret}_{T}^{\mathrm{CB}}\leq 68\sqrt{AT\beta}\cdot\log(4\delta^{-1})\] \[= 68\beta\cdot\log(4\delta^{-1})\cdot\sqrt{AT/\beta}\] \[\leq 68\beta\cdot\log(4\delta^{-1})\cdot 56A^{2}\beta\cdot\frac{\dim_{E} \left(\mathcal{F},\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\] \[\leq 3808A^{2}\beta^{2}\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta \right)}{\Delta}\cdot\log^{2}(4/(\delta\Delta))\]

where we apply the condition (11) in the second inequality. 

**Lemma 17** (Query complexity).: _For Algorithm 1, assume \(f^{\star}\in\mathcal{F}_{t}\) for all \(t\in[T]\). Then, we have_

\[\mathrm{Queries}_{T}^{\mathrm{CB}}\leq\min\left\{T,\,3136A^{3}\beta^{3}\frac {\dim_{E}^{2}\left(\mathcal{F},\Delta\right)}{\Delta^{2}}\cdot\log^{2}(2/( \delta\Delta))\right\}\]

_with probability at least \(1-\delta\)._

Proof of Lemma 17.: We consider two cases. First, when

\[56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{\Delta}\cdot \log(2/(\delta\Delta))<\sqrt{AT/\beta}\] (12)

we can invoke Lemma 14 and get that \(\lambda_{t}=0\) for all \(t\in[T]\). Hence,

\[\mathrm{Queries}_{T}^{\mathrm{CB}}= \sum_{t=1}^{T}Z_{t}\] \[= \sum_{t=1}^{T}Z_{t}\mathds{1}\{w_{t}\geq\Delta\}\] \[= \sum_{t=1}^{T}Z_{t}\sup_{a,b\in\mathcal{A}_{t}}\mathds{1}\left\{ \sup_{f,f^{\prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq \Delta\right\}\] \[\leq \sum_{t=1}^{T}Z_{t}\sum_{a,b}\mathds{1}\left\{\sup_{f,f^{\prime} \in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq\Delta\right\}\] \[\leq A^{2}\underbrace{\sum_{t=1}^{T}Z_{t}}_{a,b\sim p_{t}}\mathds{1} \left\{\sup_{f,f^{\prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b )\geq\Delta\right\}_{(*)}\]where the second equality is by Lemma 7, the second inequality holds as \(p_{t}(a)\) is uniform for any \(a,b\) when \(\lambda_{t}=0\). We apply Lemma 4 and Lemma 10 to \((*)\) and obtain

\[(*)\leq 2\sum_{t=1}^{T}Z_{t}\mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{ F}_{t}}f(x_{t},a_{t},b_{t})-f^{\prime}(x_{t},a_{t},b_{t})\geq\Delta\right\}+8 \log(\delta^{-1})\] \[\leq 2\left(\frac{4\beta}{\Delta^{2}}+1\right)\dim_{E}(\mathcal{F}; \Delta)+8\log(\delta^{-1})\] \[\leq \frac{10\beta}{\Delta^{2}}\cdot\dim_{E}(\mathcal{F};\Delta)+8 \log(\delta^{-1}).\]

Plugging this back, we obtain

\[\mathrm{Queries}_{T}^{\mathrm{CB}}\leq \frac{10A^{2}\beta}{\Delta^{2}}\cdot\dim_{E}(\mathcal{F};\Delta) +8A^{2}\log(\delta^{-1})\] \[\leq 3136A^{3}\beta^{3}\frac{\dim_{E}^{2}\left(\mathcal{F},\Delta \right)}{\Delta^{2}}\cdot\log^{2}(2/(\delta\Delta)).\]

On the other hand, when the contrary of (12) holds, i.e.,

\[56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\Delta\right)}{\Delta}\cdot \log(2/(\delta\Delta))\geq\sqrt{AT/\beta}.\]

Squaring both sides, we obtain

\[3136A^{4}\beta^{2}\frac{\dim_{E}^{2}\left(\mathcal{F},\Delta\right)}{\Delta^{ 2}}\cdot\log^{2}(2/(\delta\Delta))\geq AT/\beta\]

which leads to

\[T\leq 3136A^{3}\beta^{3}\frac{\dim_{E}^{2}\left(\mathcal{F},\Delta\right)}{ \Delta^{2}}\cdot\log^{2}(2/(\delta\Delta)).\]

We note that we always have \(\mathrm{Queries}_{T}^{\mathrm{CB}}\leq T\), and thus,

\[\mathrm{Queries}_{T}^{\mathrm{CB}}\leq T\leq 3136A^{3}\beta^{3}\frac{\dim_{E}^{ 2}\left(\mathcal{F},\Delta\right)}{\Delta^{2}}\cdot\log^{2}(2/(\delta\Delta)).\]

Hence, we complete the proof. 

Having established the aforementioned lemmas, we are now able to advance towards the proof of Theorem 1.

Proof of Theorem 1.: By Lemma 9 and the construction of version spaces \(\mathcal{F}_{t}\) in Algorithm 1, we have \(f^{*}\in\mathcal{F}_{t}\) for all \(t\in[T]\) with probability at least \(1-\delta\). Then, the rest of the proof follows from Lemmas 15 to 17. 

### Proof of Theorem 2

In this section, we will prove the following theorem, which is stronger than Theorem 2.

**Theorem 5** (Lower bounds).: _The following two claims hold:_

1. _for any algorithm, there exists an instance that leads to_ \(\mathrm{Regret}_{T}^{\mathrm{CB}}=\Omega(\sqrt{AT})\)_;_
2. _for any algorithm achieving a worse-case expected regret upper bound in the form of_ \(\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]=O(\sqrt{A}\cdot T^{1-\beta})\) _for some_ \(\beta>0\)_, there exists an instance with gap_ \(\Delta=\sqrt{A}\cdot T^{-\beta}\) _that results in_ \(\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]=\Omega(A/\Delta)=\Omega(\sqrt{A} \cdot T^{\beta})\) _and_ \(\mathbb{E}[\mathrm{Queries}_{T}^{\mathrm{CB}}]=\Omega(A/\Delta^{2})=\Omega(T^{ 2\beta})\)_._

We observe that Theorem 2 can be considered as a corollary of the above theorem when setting \(\beta=1/2\).

In what follows, we will first demonstrate lower bounds in the setting of _multi-armed bandits (MAB) with active queries_ and subsequently establish a reduction from it to contextual dueling bandits in order to achieve these lower bounds. We start by formally defining the setting of MAB with active queries below.

**Multi-armed bandits with active queries.** We consider a scenario where there exist \(A\) arms. Each arm \(a\) is assumed to yield a binary reward (0 or 1), which is sampled from a Bernoulli distribution \(\text{Bern}(\bar{r}_{a})\), where \(\bar{r}_{a}\) denotes the mean reward associated with arm \(a\).The arm with the highest mean reward is denoted by \(a^{*}\coloneqq\arg\max_{a}\bar{r}_{a}\). Let \(\Delta_{a}\coloneqq\bar{r}_{a^{*}}-\bar{r}_{a}\) denote the gap of arm \(a\in[A]\). The interaction proceeds as follows: at each round \(t\in[T]\), we need to pull an arm but can choose whether to receive the reward signal (denote this choice by \(Z_{t}\)). The objective is to minimize two quantities: the regret and the number of queries,

\[\mathrm{Regret}_{T}=\sum_{t=1}^{T}\Delta_{a_{t}},\quad\mathrm{ Queries}_{T}=\sum_{t=1}^{T}Z_{t}.\] (13)

Towards the lower bounds, we will start with a bound on the KL divergence over distributions of runs under two different bandits. This result is a variant of standard results which can be found in many bandit literature (e.g., Lattimore and Szepesvari (2020)).

**Lemma 18**.: _Let \(I_{1}\) and \(I_{2}\) be two instances of MAB. We define \(p_{1}\) and \(p_{2}\) as their respective distributions over the outcomes of all pulled arms and reward signals when a query is made. Concretely, \(p_{1}\) and \(p_{2}\) are measuring the probability of outcomes (denoted by \(O\)) in the following form:_

\[O=\big{(}Z_{1},a_{1},(r_{1}),\ldots,Z_{T},a_{T},(r_{T})\big{)}\]

_where the reward \(r_{t}\) is included only when \(Z_{t}=1\), and we added parentheses above to indicate this point. We denote \(\Pr_{1}\) (resp. \(\Pr_{2}\)) as the reward distribution of \(I_{1}\) (resp. \(I_{2}\)). We define \(\bar{n}_{a}=\sum_{t=1}^{T}Z_{t}\mathds{1}\{a_{t}=a\}\) as the number of times arm \(a\) is pulled when making a query. Then, given any algorithm \(\mathfrak{A}\), the Kullback-Leibler divergence between \(p_{1}\) and \(p_{2}\) can be decomposed in the following way_

\[\mathrm{KL}(p_{1},p_{2})=\sum_{a=1}^{A}\operatorname*{\mathbb{E} }_{p_{1}}[\bar{n}_{a}]\cdot\mathrm{KL}\big{(}\Pr_{1}(r\,|\,a),\Pr_{2}(r\,|\,a) \big{)}.\]

Proof of Lemma 18.: We define the conditional distribution

\[\overline{\Pr}_{1}(r_{t}\,|\,Z_{t},a_{t})\begin{cases}\Pr_{1}(r_{t}\,|\,a_{t} )&\text{if }Z_{t}=1\\ 1&\text{if }Z_{t}=0\end{cases},\]

and similarly for \(\overline{\Pr}_{2}\). Additionally, we denote \(\Pr_{\mathfrak{A}}\) as the probability associated with algorithm \(\mathfrak{A}\). Then, for any outcome \(O\), we have

\[p_{1}(O)=\prod_{t=1}^{T}\Pr_{\mathfrak{A}}\big{(}Z_{t},a_{t}\,|\,Z_{1},a_{1},( r_{1}),\ldots,Z_{t-1},a_{t-1},(r_{t-1})\big{)}\overline{\Pr}_{1}(r_{t}\,|\,Z_{t},a_{t}),\]

and we can write \(p_{2}(O)\) in a similar manner. Hence,

\[\mathrm{KL}(p_{1},p_{2}) =\operatorname*{\mathbb{E}}_{O\sim p_{1}}\left[\log\left(\frac{ \prod_{t=1}^{T}\Pr_{\mathfrak{A}}\big{(}Z_{t},a_{t}\,|\,Z_{1},a_{1},(r_{1}), \ldots,Z_{t-1},a_{t-1},(r_{t-1})\big{)}\overline{\Pr}_{1}(r_{t}\,|\,Z_{t},a_{t })}{\prod_{t=1}^{T}\Pr_{\mathfrak{A}}\big{(}Z_{t},a_{t}\,|\,Z_{1},a_{1},(r_{1}),\ldots,Z_{t-1},a_{t-1},(r_{t-1})\big{)}\overline{\Pr}_{2}(r_{t}\,|\,Z_{t},a_{t })}\right)\right]\] \[=\operatorname*{\mathbb{E}}_{O\sim p_{1}}\left[\sum_{t=1}^{T}Z_{t} \operatorname*{\mathbb{E}}_{r_{t}\sim\Pr_{1}(\,|\,a_{t})}\left[\log\left( \frac{\Pr_{1}(r_{t}\,|\,a_{t})}{\Pr_{2}(r_{t}\,|\,a_{t})}\right)\right]\right]\] \[=\operatorname*{\mathbb{E}}_{O\sim p_{1}}\left[\sum_{t=1}^{T}Z_{t} \cdot\mathrm{KL}\big{(}\Pr_{1}(\cdot\,|\,a_{t}),\Pr_{2}(\cdot\,|\,a_{t})\big{)}\right]\] \[=\sum_{a=1}^{A}\operatorname*{\mathbb{E}}_{O\sim p_{1}}[\bar{n}_{a }]\cdot\mathrm{KL}\big{(}\Pr_{1}(\cdot\,|\,a_{t}),\Pr_{2}(\cdot\,|\,a_{t}) \big{)}\]where the third equality holds by the definition of \(\operatorname{\overline{Pr}}_{1}\) and \(\operatorname{\overline{Pr}}_{2}\). 

The following lemma establishes lower bounds for MAB with active queries. It presents a trade-off between the regret and the number of queries.

**Lemma 19**.: _Let \(\mathcal{I}\) denote the set of all MAB instances. Assume \(\mathsf{ALG}\) is an algorithm that achieves the following worst-case regret upper bound for some \(C\) and \(\beta\):_

\[\operatorname{\mathbb{E}}\left[\operatorname{Regret}_{T}\right]\leq CT^{1- \beta},\]

_for all \(I\in\mathcal{I}\). Then, for any MAB instance \(I\in\mathcal{I}\), the regret and the number of queries made by algorithm \(\mathsf{ALG}\) are lower bounded:_

\[\operatorname{\mathbb{E}}\left[\operatorname{Regret}_{T}\right]\geq\sum_{a \neq a^{\ast}}\frac{\zeta}{\Delta_{a}}\log\left(\frac{\Delta_{a}}{4CT^{-\beta} }\right),\quad\operatorname{\mathbb{E}}\left[\operatorname{Queries}_{T} \right]\geq\sum_{a\neq a^{\ast}}\frac{\zeta}{\Delta_{a}^{2}}\log\left(\frac{ \Delta_{a}}{4CT^{-\beta}}\right)\]

_where the coefficient \(\zeta=\min_{a}\min\{\bar{r}_{a},1-\bar{r}_{a}\}\) depends on the instance \(I\)._

Proof of Lemma 19.: For any MAB instance \(I\) and any arm \(a^{\dagger}\), we define a corresponding MAB instance \(I^{\prime}\) as follows. Denote \(\bar{r}\) and \(\bar{r}^{\prime}\) as the mean reward of \(I\) and \(I^{\prime}\), respectively. For \(I^{\prime}\), we set the mean reward \(\bar{r}_{a}^{\prime}=\bar{r}_{a}\) for any \(a\neq a^{\dagger}\) and \(\bar{r}_{a^{\dagger}}^{\prime}=\bar{r}_{a^{\dagger}}+2\Delta_{a^{\dagger}}\). Consequently, the optimal arm of \(I^{\prime}\) is \(a^{\dagger}\) with margin \(\Delta_{a^{\dagger}}\). Let \(n_{a}\) denote the number of times that arm \(a\) is pulled. We define the event

\[E=\{n_{a^{\dagger}}>T/2\}.\]

Then, we have

\[\operatorname{\mathbb{E}}_{p}\left[\operatorname{Regret}_{T}\right]\geq\frac{ T\Delta_{a^{\dagger}}}{2}\cdot p(E),\quad\operatorname{\mathbb{E}}_{p^{\prime}} \left[\operatorname{Regret}_{T}\right]\geq\frac{T\Delta_{a^{\dagger}}}{2}\cdot p ^{\prime}(E^{\complement}).\]

Hence,

\[2CT^{1-\beta} \geq\operatorname{\mathbb{E}}_{p}\left[\operatorname{Regret}_{T} \right]+\operatorname{\mathbb{E}}_{p^{\prime}}\left[\operatorname{Regret}_{T}\right]\] \[\geq\] \[\geq \frac{T\Delta_{a^{\dagger}}}{2}\exp\left(-\frac{1}{2}\cdot \operatorname{KL}(p,p^{\prime})\right).\]

By Lemma 18, we have

\[\operatorname{KL}(p,p^{\prime})= \sum_{a=1}^{A}\operatorname{\mathbb{E}}_{p}[\bar{n}_{a}]\cdot \operatorname{KL}\big{(}\Pr(r\,|\,a),\Pr^{\prime}(r\,|\,a)\big{)}\] \[= \operatorname{\mathbb{E}}_{p}[\bar{n}_{a^{\dagger}}]\cdot \operatorname{KL}\big{(}\Pr(r\,|\,a^{\dagger}),\Pr^{\prime}(r\,|\,a^{\dagger} )\big{)}\] \[\leq \operatorname{\mathbb{E}}_{p}[\bar{n}_{a^{\dagger}}]\cdot\Delta_ {a^{\dagger}}^{2}\cdot 2/\zeta\]

where the last inequality is by Lemma 6. Putting the above two inequality together, we arrive at

\[\operatorname{\mathbb{E}}_{p}[\bar{n}_{a^{\dagger}}]\geq\frac{\zeta}{\Delta_{a ^{\dagger}}^{2}}\log\left(\frac{\Delta_{a^{\dagger}}}{4CT^{-\beta}}\right).\]

This establishes a query lower bound for arm \(a^{\dagger}\). Consequently, we have

\[\operatorname{\mathbb{E}}[\operatorname{Regret}_{T}]\geq\sum_{a\neq a^{\ast} }\operatorname{\mathbb{E}}_{p}[\bar{n}_{a}]\cdot\Delta_{a}\geq\sum_{a\neq a^{ \ast}}\frac{\zeta}{\Delta_{a}}\log\left(\frac{\Delta_{a}}{4CT^{-\beta}}\right),\]and similarly,

\[\mathbb{E}[\mathrm{Queries}_{T}]\geq\sum_{a\neq a^{*}}\mathbb{E}_{p}[\bar{n}_{a}] \geq\sum_{a\neq a^{*}}\frac{\zeta}{\Delta_{a}^{2}}\log\left(\frac{\Delta_{a}}{4 CT^{-\beta}}\right).\]

Now we can proceed with the proof of Theorem 5.

Proof of Theorem 5.: We provide a reduction from the multi-armed bandits with active queries to the contextual dueling bandits. Our desired lower bound for the contextual dueling bandit setting thus follows from the above lower bound for Multi-Armed Bandits (MABs). Let ALG denote any algorithm for contextual dueling bandits.

**Reduction.** Since we focus on the multi-armed bandit where no context is involved, we just ignore the notation of context everywhere for brevity. We will start from an MAB instance, and then simulate a binary feedback and feed it to a dueling bandit algorithm ALG which is used to solve the original MAB instance. Particularly, consider the MAB instance with A-many actions each with an expected reward denoted as \(\bar{r}_{a}\).

At the beginning of iteration \(t\) in the MAB instance, the learner calls the dueling algorithm ALG to generate two actions \(a_{t}\) and \(b_{t}\). The learner plays \(a_{t}\) at iteration \(t\) to receive a reward \(y_{a_{t}}\); the learner then moves to iteration \(t+1\) to play \(b_{t}\), and receives reward \(y_{b_{t}}\). At the end of iteration \(t+1\), the learner simulates a binary feedback by setting \(o=1\) if \(y_{a_{t}}>y_{b_{t}}\); \(o=-1\) if \(y_{a_{t}}<y_{b_{t}}\); \(o\) being \(1\) or \(-1\) uniform randomly if \(y_{a_{t}}=y_{b_{t}}\). Then, the learner sends \((a_{t},b_{t},o)\) to the dueling algorithm ALG to query for two actions which will be played at iterations \(t+2\) and \(t+3\), respectively.

From the dueling algorithm ALG's perspective, given two actions \(a\) and \(b\), we can verify that the probability of seeing label 1 is \((\bar{r}_{a}-\bar{r}_{b}+1)/2\). So we can just specify the link function to be \(\phi(d)=(d+1)/2\). As we verified earlier, the corresponding \(\Phi\) is strongly convex (Example 2). Moreover, since \(f^{\star}(a,b)=\bar{r}_{a}-\bar{r}_{b}\), if we define the gap of the MAB instance as \(\bar{\Delta}\coloneqq\min_{a\neq a^{\star}}(\bar{r}_{a^{\star}}-\bar{r}_{a})\) where \(a^{\star}\coloneqq\arg\max_{i}\bar{r}_{i}\), then we have \(\bar{\Delta}=\Delta\) in this reduction where \(\Delta\) is the definition of the gap in the dueling setting. We further note that the regret of the MAB instance is

\[\sum_{t=1}^{T}(\bar{r}_{a^{\star}}-\bar{r}_{a_{t}})+\sum_{t=1}^{T}(\bar{r}_{a ^{\star}}-\bar{r}_{b_{t}}),\]

which, by our definition of \(f^{\star}\), is equivalent to the preference-based regret that occurred to the dueling algorithm ALG. The number of queries is clearly equivalent as well. Thus, the regret and the query complexity of the dueling algorithm ALG can be directly translated to the regret and the query complexity of the MAB instance.

Now, we are ready to prove the two claims in our statement.

**Proof of the first claim.** We refer the reader to Lattimore and Szepesvari (2020, Theorem 15.2) for a proof of the minimax regret lower bound of \(\Omega(\sqrt{AT})\) for the MAB. Through the reduction outlined above, that lower bound naturally extends to the dueling bandits setting, yielding \(\mathrm{Regret}_{T}^{\mathrm{CB}}\geq\Omega(\sqrt{AT})\) (otherwise, via the above reduction, we would have achieved an approach that breaks the lower bound of MAB).

**Proof of the second claim.** We choose an arbitrary MAB for which \(\zeta=\min_{a}\min\{\bar{r}_{a},1-\bar{r}_{a}\}>0.2\) and the gaps of all arms are equal to \(\Delta\). Invoking Lemma 19, we have

\[\mathbb{E}\left[\mathrm{Regret}_{T}\right] \geq\frac{0.2(A-1)}{\Delta}\log\left(\frac{\Delta}{4CT^{-\beta}} \right)\geq\Omega\left(\frac{A}{\Delta}\right),\] \[\mathbb{E}\left[\mathrm{Queries}_{T}\right] \geq\frac{0.2(A-1)}{\Delta^{2}}\log\left(\frac{\Delta}{4CT^{- \beta}}\right)\geq\Omega\left(\frac{A}{\Delta^{2}}\right).\]

We further choose \(\Delta=40CT^{-\beta}\) and \(C=\sqrt{A}\), leading to

\[\mathbb{E}\left[\mathrm{Regret}_{T}\right] \geq\frac{0.2(A-1)}{40\sqrt{A}}\cdot T^{\beta}=\Omega\left( \sqrt{A}\cdot T^{\beta}\right),\] \[\mathbb{E}\left[\mathrm{Queries}_{T}\right] \geq\frac{0.2(A-1)}{1600A}\cdot T^{2\beta}=\Omega\left(T^{2\beta} \right).\]Via the reduction we have shown above, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof. 

#### c.4.1 Alternative Lower Bounds Conditioning on the Limit of Regret

In this section, we establish an analogue of Theorem 5 but under a different condition. We first introduce the concept of _diminishing regret_.

**Definition 4**.: _We say that an algorithm guarantees a diminishing regret if for all contextual dueling bandit instances and \(p>0\), it holds that_

\[\lim_{T\to\infty}\frac{\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]}{T^{p}}=0.\]

The lower bounds under the assumption of diminishing regret guarantees are stated as follows.

**Theorem 6** (Lower bounds).: _The following two claims hold:_

1. _for any algorithm, there exists an instance that leads to_ \(\mathrm{Regret}_{T}^{\mathrm{CB}}\geq\Omega(\sqrt{AT})\)_;_
2. _for any gap_ \(\Delta\) _and any algorithm achieving diminishing regret, there exists an instance with gap_ \(\Delta\) _that results in_ \(\mathbb{E}[\mathrm{Regret}_{T}^{\mathrm{CB}}]\geq\Omega(A/\Delta)\) _and_ \(\mathbb{E}[\mathrm{Queries}_{T}^{\mathrm{CB}}]\geq\Omega(A/\Delta^{2})\) _for sufficiently large_ \(T\)_._

We should highlight that the condition of diminishing regret (Theorem 6) and the worst-case regret upper bounds (Theorems 2 and 5) are not comparable in general. However, Theorem 6 is also applicable to our algorithm (Algorithm 1) since our algorithm possesses an instance-dependent regret upper bound that is clearly diminishing.

To prove Theorem 6, we first show the following lemma, which is a variant of Lemma 19.

**Lemma 20**.: _Let \(\mathcal{I}\) denote the set of all MAB instances. Assume \(\mathsf{ALG}\) is an algorithm that achieves diminishing regret for all MAB instances in \(\mathcal{I}\), i.e., for any \(I\in\mathcal{I}\) and \(p>0\), it holds that_

\[\lim_{T\to\infty}\frac{\mathbb{E}[\mathrm{Regret}_{T}]}{T^{p}}=0.\]

_Then, for any MAB instance \(I\in\mathcal{I}\), the regret and the number of queries made by algorithm \(\mathsf{ALG}\) are lower bounded in the following manner:_

\[\liminf_{T\to\infty}\frac{\mathbb{E}\left[\mathrm{Regret}_{T}\right]}{\log T} \geq\sum_{a\neq a^{*}}\frac{\zeta}{\Delta_{a}},\quad\liminf_{T\to\infty}\frac {\mathbb{E}\left[\mathrm{Queries}_{T}\right]}{\log T}\geq\sum_{a\neq a^{*}} \frac{\zeta}{\Delta_{a}^{2}}\]

_where the coefficient \(\zeta\coloneqq\min_{a}\min\{\bar{r}_{a},1-\bar{r}_{a}\}\) depends on the instance \(I\). Recall that \(\mathrm{Regret}_{T}\) and \(\mathrm{Queries}_{T}\) are defined in (13)._

Proof of Lemma 20.: The proof is similar to Lemma 19. For any MAB instance \(I\in\mathcal{I}\) and any arm \(a^{\dagger}\), we define a corresponding MAB instance \(I^{\prime}\) as follows. Denote \(\bar{r}\) and \(\bar{r}^{\prime}\) as the mean reward of \(I\) and \(I^{\prime}\), respectively. For \(I^{\prime}\), we set the mean reward \(\bar{r}^{\prime}_{a}=\bar{r}_{a}\) for any \(a\neq a^{\dagger}\) and \(\bar{r}^{\prime}_{a^{\dagger}}=\bar{r}_{a^{\dagger}}+2\Delta_{a^{\dagger}}\). Consequently, the optimal arm of \(I^{\prime}\) is \(a^{\dagger}\) with margin \(\Delta_{a^{\dagger}}\). Let \(n_{a}\) denote the number of times that arm \(a\) is pulled. We define the event

\[E=\{n_{a^{\dagger}}>T/2\}.\]

Let \(p\) and \(p^{\prime}\) denote the probability of \(I\) and \(I^{\prime}\), respectively. Then, we have

\[\mathbb{E}_{p}\left[\mathrm{Regret}_{T}\right]\geq\frac{T\Delta_{a^{\dagger}} }{2}\cdot p(E),\quad\mathbb{E}_{p^{\prime}}\left[\mathrm{Regret}_{T}\right] \geq\frac{T\Delta_{a^{\dagger}}}{2}\cdot p^{\prime}(E^{\complement})\]where \(E^{\complement}\) means the complement of event \(E\). Hence,

\[\mathbb{E}_{p}\left[\mathrm{Regret}_{T}\right]+\mathbb{E}_{p^{ \prime}}\left[\mathrm{Regret}_{T}\right]\geq \frac{T\Delta_{a^{\dagger}}}{2}\big{(}p(E)+p^{\prime}(E^{\complement })\big{)}\] \[= \frac{T\Delta_{a^{\dagger}}}{2}\left(1-\big{(}p^{\prime}(E)-p(E) \big{)}\right)\] \[\geq \frac{T\Delta_{a^{\dagger}}}{2}\left(1-\mathrm{TV}\big{(}p,p^{ \prime}\big{)}\right)\] \[\geq \frac{T\Delta_{a^{\dagger}}}{2}\left(1-\sqrt{1-\exp\big{(}- \mathrm{KL}(p,p^{\prime})\big{)}}\right)\] \[\geq \frac{T\Delta_{a^{\dagger}}}{2}\exp\left(-\frac{1}{2}\cdot \mathrm{KL}(p,p^{\prime})\right).\]

Here \(\mathrm{TV}\) denotes the total variation distance. By Lemma 18, we have

\[\mathrm{KL}(p,p^{\prime})= \sum_{a=1}^{A}\mathbb{E}_{p}[\bar{n}_{a}]\cdot\mathrm{KL}\big{(} \Pr(r\,|\,a),\Pr^{\prime}(r\,|\,a)\big{)}\] \[= \mathbb{E}_{p}[\bar{n}_{a^{\dagger}}]\cdot\mathrm{KL}\big{(}\Pr (r\,|\,a^{\dagger}),\Pr^{\prime}(r\,|\,a^{\dagger})\big{)}\] \[\leq \mathbb{E}_{p}[\bar{n}_{a^{\dagger}}]\cdot\Delta_{a^{\dagger}}^{2 }\cdot 2/\zeta\]

where the last inequality is by Lemma 6. Putting it all together, we arrive at

\[\mathbb{E}_{p}[\bar{n}_{a^{\dagger}}]\geq\frac{\zeta}{\Delta_{a^{ \dagger}}^{2}}\log\left(\frac{T\Delta_{a^{\dagger}}}{2\Big{(}\mathbb{E}_{p} \left[\mathrm{Regret}_{T}\right]+\mathbb{E}_{p^{\prime}}\left[\mathrm{Regret}_ {T}\right]\Big{)}}\right).\]

Taking the limit on both sides yields

\[\liminf_{T\to\infty}\frac{\mathbb{E}_{p}[\bar{n}_{a^{\dagger}}]}{ \log T}\geq \liminf_{T\to\infty}\frac{\zeta}{\Delta_{a^{\dagger}}^{2}}\cdot \frac{\log\left(\frac{T\Delta_{a^{\dagger}}}{2\Big{(}\mathbb{E}_{p}\left[ \mathrm{Regret}_{T}\right]+\mathbb{E}_{p^{\prime}}\left[\mathrm{Regret}_{T} \right]\Big{)}}\right)}{\log T}\] \[= \liminf_{T\to\infty}\frac{\zeta}{\Delta_{a^{\dagger}}^{2}}\cdot \left(1+\underbrace{\frac{\log(\Delta_{a^{\dagger}}/2)}{\log T}}_{(\mathrm{i} )}-\underbrace{\frac{\log\Big{(}\mathbb{E}_{p}\left[\mathrm{Regret}_{T} \right]+\mathbb{E}_{p^{\prime}}\left[\mathrm{Regret}_{T}\right]\Big{)}}{\log T }}_{(\mathrm{ii})}\right).\]

Here the limit of \((\mathrm{i})\) is clearly 0. For the limit of \((\mathrm{ii})\), we note that by the definition of diminishing regret, for any \(C>0\), there exists a \(T^{\prime}\) such that \(\mathbb{E}[\mathrm{Regret}_{T}]/T^{p}\leq C\) for any \(T>T^{\prime}\). This implies

\[\frac{\log\Big{(}\mathbb{E}_{p}\left[\mathrm{Regret}_{T}\right]+\mathbb{E}_{p^ {\prime}}\left[\mathrm{Regret}_{T}\right]\Big{)}}{\log T}\leq\frac{\log\Big{(} 2CT^{p}\Big{)}}{\log T}=\frac{\log(2C)}{\log T}+p\]

for any \(p>0\). Therefore, the limit of \((\mathrm{ii})\) is also 0. Plugging these back, we obtain

\[\liminf_{T\to\infty}\frac{\mathbb{E}_{p}[\bar{n}_{a^{\dagger}}]}{\log T}\geq \frac{\zeta}{\Delta_{a^{\dagger}}^{2}}.\]

This establishes a query lower bound for arm \(a^{\dagger}\). Consequently, we have

\[\liminf_{T\to\infty}\frac{\mathbb{E}[\mathrm{Regret}_{T}]}{\log T}\geq\liminf_ {T\to\infty}\sum_{a\neq a^{*}}\frac{\mathbb{E}_{p}[\bar{n}_{a}]\cdot\Delta_{a} }{\log T}\geq\sum_{a\neq a^{*}}\frac{\zeta}{\Delta_{a}},\]

and similarly,

\[\liminf_{T\to\infty}\frac{\mathbb{E}[\mathrm{Queries}_{T}]}{\log T}\geq \liminf_{T\to\infty}\sum_{a\neq a^{*}}\frac{\mathbb{E}_{p}[\bar{n}_{a}]}{\log T }\geq\sum_{a\neq a^{*}}\frac{\zeta}{\Delta_{a}^{2}}.\]Now, we proceed with the proof of Theorem 6.

Proof of Theorem 6.: The proof of the first claim is the same as Theorem 5, so we will omit it here. Let us now focus on the proof of the second claim. By Lemma 20, for any algorithm achieving diminishing regret, the following is true for any MAB instance:

\[\liminf_{T\to\infty}\frac{\mathbb{E}\left[\mathrm{Regret}_{T}\right]}{\log T} \geq\sum_{a\neq a^{*}}\frac{\zeta}{\Delta_{a}},\quad\liminf_{T\to\infty}\frac{ \mathbb{E}\left[\mathrm{Queries}_{T}\right]}{\log T}\geq\sum_{a\neq a^{*}} \frac{\zeta}{\Delta_{a}^{2}}.\]

We choose an arbitrary MAB for which \(\zeta\geq 0.2\) and the gaps of all suboptimal arms are equal to \(\Delta\). Then, for this instance, we have

\[\liminf_{T\to\infty}\frac{\mathbb{E}\left[\mathrm{Regret}_{T}\right]}{\log T} \geq\frac{0.2(A-1)}{\Delta},\quad\liminf_{T\to\infty}\frac{\mathbb{E}\left[ \mathrm{Queries}_{T}\right]}{\log T}\geq\frac{0.2(A-1)}{\Delta^{2}}.\]

By the definition of limit, when \(T\) is large enough (exceeding a certain threshold), we have

\[\frac{\mathbb{E}\left[\mathrm{Regret}_{T}\right]}{\log T}\geq\frac{0.1(A-1)}{ \Delta},\quad\frac{\mathbb{E}\left[\mathrm{Queries}_{T}\right]}{\log T} \geq\frac{0.1(A-1)}{\Delta^{2}}.\]

Via the reduction we have shown in the proof of Theorem 5, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof. 

### Proof of Theorem 3

Proof of Theorem 3.: We establish the bounds for regret and the number of queries, consecutively. First, we set an arbitrary gap threshold \(\epsilon>0\). Since our algorithm is independent of \(\epsilon\), we can later choose any \(\epsilon\) that minimizes the upper bounds.

**Proof of regret.** We start with the regret upper bound. By definition, we have

\[\mathrm{Regret}_{T}^{\mathrm{CB}}=\sum_{t=1}^{T}\big{(}f^{*}(x_{t},\pi_{f^{*} }(x_{t}),a_{t})+f^{*}(x_{t},\pi_{f^{*}}(x_{t}),b_{t})\big{)}.\]

Since \(a_{t}\) and \(b_{t}\) are always drawn independently from the same distribution in Algorithm 1, we only need to consider the regret of the \(a_{t}\) part in the following proof for brevity -- multiplying the result by two would yield the overall regret.

The worst-case regret upper bound presented in Lemma 15 doesn't reply on the gap assumption and thus remains applicable in this setting. Hence, we only need to prove the instance-dependent regret upper bound. To that end, we first need an analogue of Lemma 14.

**Lemma 21**.: _Fix any \(\epsilon>0\). Whenever_

\[2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon \right)}{\epsilon}\cdot\log(2/(\delta\epsilon))<\sqrt{AT/\beta},\]

_we have \(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{T}=0\) with probability at least \(1-\delta\)._

Proof of Lemma 21.: The proof is similar to Lemma 14 and is via contradiction. Assume the inequality holds but there exists \(t^{\prime}\) for which \(\lambda_{t^{\prime}}=1\). Without loss of generality, we assume that \(\lambda_{t}=0\) for all \(t<t^{\prime}\), namely that \(t^{\prime}\) is the first time that \(\lambda_{t}\) is 1. Then by definition of \(\lambda_{t^{\prime}}\), we have

\[\sum_{s=1}^{t^{\prime}-1}Z_{s}w_{s}\geq\sqrt{AT/\beta}.\]

On the other hand, we have

\[\sum_{s=1}^{t^{\prime}-1}Z_{s}w_{s}= \sum_{s=1}^{t^{\prime}-1}\mathds{1}\{\mathrm{Gap}(x_{t})\leq \epsilon\}Z_{s}w_{s}+\sum_{s=1}^{t^{\prime}-1}\mathds{1}\{\mathrm{Gap}(x_{t})> \epsilon\}Z_{s}w_{s}\] \[\leq 2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F}, \epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))\]

where the inequality is by Lemma 13. The above two inequalities contradicts with the conditions.

Towards an instance-dependent regret upper bound, we adapt the proof of Lemma 16 to this setting. We consider two cases. First, when

\[2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right)}{ \epsilon}\cdot\log(2/(\delta\epsilon))<\sqrt{AT/\beta},\] (14)

we invoke Lemma 21 and get that \(\lambda_{t}=0\) for all \(t\in[T]\). Hence, we have

\[\mathrm{Regret}_{T}^{\mathrm{CB}}= \sum_{t=1}^{T}\left(f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),a_{t} )+f^{\star}(x_{t},\pi_{f^{\star}}(x_{t}),b_{t})\right)\] \[\leq 2\sum_{t=1}^{T}\mathds{1}\{\mathrm{Gap}(x_{t})\leq\epsilon\}Z_ {t}w_{t}+2\sum_{t=1}^{T}\mathds{1}\{\mathrm{Gap}(x_{t})>\epsilon\}Z_{t}w_{t}\] \[\leq 4T_{\epsilon}+112A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))\] \[\leq 136\beta\cdot\log(4\delta^{-1})\cdot T_{\epsilon}+3808A^{2} \beta^{2}\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right)}{\epsilon}\cdot \log^{2}(4/(\delta\epsilon))\]

where the first inequality is by Lemma 8 and the fact that we incur no regret when \(Z_{t}=0\) since \(f^{\star}\in\mathcal{F}_{t}\). The second inequality is by Lemma 13.

On the other hand, when the contrary of (14) holds, i.e.,

\[2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right) }{\epsilon}\cdot\log(2/(\delta\epsilon))\geq\sqrt{AT/\beta},\] (15)

applying Lemma 15, we have

\[\mathrm{Regret}_{T}^{\mathrm{CB}}\leq 68\sqrt{AT\beta}\cdot\log(4\delta^{-1})\] \[= 68\beta\cdot\log(4\delta^{-1})\cdot\sqrt{AT/\beta}\] \[\leq 68\beta\cdot\log(4\delta^{-1})\cdot\left(2T_{\epsilon}+56A^{2} \beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right)}{\epsilon}\cdot \log(2/(\delta\epsilon))\right)\] \[\leq 136\beta\cdot\log(4\delta^{-1})\cdot T_{\epsilon}+3808A^{2} \beta^{2}\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon\right)}{\epsilon}\cdot \log^{2}(4/(\delta\epsilon))\]

where we apply the condition (15) in the second inequality.

**Proof of the number of queries.** To show an upper bound for the number of queries, we also consider two cases. First, when

\[2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon \right)}{\epsilon}\cdot\log(2/(\delta\epsilon))<\sqrt{AT/\beta},\] (16)

we can invoke Lemma 21 and get that \(\lambda_{t}=0\) for all \(t\in[T]\). Hence, similar to the proof of Lemma 17, we have

\[\mathrm{Queries}_{T}^{\mathrm{CB}}= \sum_{t=1}^{T}Z_{t}\] \[= \sum_{t=1}^{T}Z_{t}\mathds{1}\{\mathrm{Gap}(x_{t})<\epsilon\}+ \sum_{t=1}^{T}Z_{t}\mathds{1}\{\mathrm{Gap}(x_{t})\geq\epsilon\}\] \[= T_{\epsilon}+\sum_{t=1}^{T}Z_{t}\sup_{a,b\in\mathcal{A}_{t}} \mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime }(x_{t},a,b)\geq\epsilon\right\}\] \[\leq T_{\epsilon}+\sum_{t=1}^{T}Z_{t}\sum_{a,b}\mathds{1}\left\{\sup_{ f,f^{\prime}\in\mathcal{F}_{t}}f(x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq\epsilon\right\}\] \[\leq T_{\epsilon}+A^{2}\underbrace{\sum_{t=1}^{T}Z_{t}\underset{a,b \sim p_{t}}{\mathbb{E}}\mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{F}_{t}}f (x_{t},a,b)-f^{\prime}(x_{t},a,b)\geq\epsilon\right\}}_{(*)}\]where the second inequality holds as \(p_{t}(a)\) is uniform for any \(a,b\) when \(\lambda_{t}=0\). We apply Lemma 4 and Lemma 10 to \((*)\) and obtain

\[(*)\leq 2\sum_{t=1}^{T}Z_{t}\mathds{1}\left\{\sup_{f,f^{\prime}\in\mathcal{ F}_{t}}f(x_{t},a_{t},b_{t})-f^{\prime}(x_{t},a_{t},b_{t})\geq\epsilon\right\}+8 \log(\delta^{-1})\] \[\leq 2\left(\frac{4\beta}{\epsilon^{2}}+1\right)\dim_{E}(\mathcal{F} ;\epsilon)+8\log(\delta^{-1})\] \[\leq \frac{10\beta}{\epsilon^{2}}\cdot\dim_{E}(\mathcal{F};\epsilon)+ 8\log(\delta^{-1}).\]

Plugging this back, we obtain

\[\mathrm{Queries}_{T}^{\mathrm{CB}}\leq T_{\epsilon}+\frac{10A^{2}\beta}{\epsilon^{2}}\cdot\dim_{E}( \mathcal{F};\epsilon)+8A^{2}\log(\delta^{-1})\] \[\leq 8T_{\epsilon}^{2}\beta/A+6272A^{3}\beta^{3}\frac{\dim_{E}^{2} \left(\mathcal{F},\epsilon\right)}{\epsilon^{2}}\cdot\log^{2}(2/(\delta \epsilon))\]

where the second line corresponds to the upper bound derived from the alternative case, which is shown below.

When the contrary of (16) holds, i.e.,

\[2T_{\epsilon}+56A^{2}\beta\cdot\frac{\dim_{E}\left(\mathcal{F},\epsilon \right)}{\epsilon}\cdot\log(2/(\delta\epsilon))\geq\sqrt{AT/\beta}.\]

Squaring both sides and leveraging the inequality \((a+b)^{2}\leq 2a^{2}+2b^{2}\), we obtain

\[8T_{\epsilon}^{2}+6272A^{4}\beta^{2}\frac{\dim_{E}^{2}\left(\mathcal{F}, \epsilon\right)}{\epsilon^{2}}\cdot\log^{2}(2/(\delta\epsilon))\geq AT/\beta\]

which leads to

\[T\leq 8T_{\epsilon}^{2}\beta/A+6272A^{3}\beta^{3}\frac{\dim_{E}^{2}\left( \mathcal{F},\epsilon\right)}{\epsilon^{2}}\cdot\log^{2}(2/(\delta\epsilon)).\]

We note that we always have \(\mathrm{Queries}_{T}^{\mathrm{CB}}\leq T\) and thus

\[\mathrm{Queries}_{T}^{\mathrm{CB}}\leq T\leq 8T_{\epsilon}^{2}\beta/A+6272A^{3 }\beta^{3}\frac{\dim_{E}^{2}\left(\mathcal{F},\epsilon\right)}{\epsilon^{2} }\cdot\log^{2}(2/(\delta\epsilon)).\]

**Minimizing on \(\epsilon\).** Given that the aforementioned proofs hold for any threshold \(\epsilon\), we can select the specific value of \(\epsilon\) that minimizes the upper bounds. Hence, we deduce the desired result. 

### Proof of Theorem 4

Proof of Theorem 4.: The upper bound of the number of queries is straightforward: Algorithm 2 is simply running \(H\) instances of Algorithm 1, so the total number of queries is simply the sum of these \(H\) instances. For bounding the regret, we have

\[\mathrm{Regret}_{T}^{\mathrm{IL}}= \sum_{t=1}^{T}V_{0}^{\pi_{e}}(x_{t,0})-V_{0}^{\pi_{t}}(x_{t,0})\] \[\leq \sum_{h=0}^{H-1}\sum_{t=1}^{T}\operatorname*{\mathbb{E}}_{x_{t,h},a_{t,h}\sim x_{t,h}^{\pi_{e}}}\left[Q_{h}^{\pi_{e}}(x_{t,h},\pi_{h}^{\pi_{e} }(x_{t,h}))-Q_{h}^{\pi_{e}}(x_{t,h},a_{t,h})\right]\] \[\leq \sum_{h=0}^{H-1}\sum_{t=1}^{T}\operatorname*{\mathbb{E}}_{x_{t,h},a_{t,h}\sim x_{t,h}^{\pi_{e}}}\left[Q_{h}^{\pi_{e}}(x_{t,h},\pi_{h}^{+}(x_{t,h}))-Q_{h}^{\pi_{e}}(x_{t,h},a_{t,h})\right]\] \[\qquad-\sum_{h=0}^{H-1}\sum_{t=1}^{T}\operatorname*{\mathbb{E}}_{x _{t,h}\sim x_{t,h}^{\pi_{e}}}\left[A_{h}^{\pi_{e}}(x_{t,h},\pi_{h}^{+}(x_{t,h} ))\right]\] \[\leq H\cdot\operatorname*{\mathbb{E}}\left[\mathrm{Regret}_{T}^{ \mathrm{CB}}\right]-\mathrm{Adv}_{T}.\]

where the first inequality holds by Lemma 5, and we denote \(\pi_{h}^{+}(x_{t,h})=\arg\max_{a}Q_{h}^{\pi_{e}}(x_{t,h},a)\) in the second inequality. Then, we can plug the upper bound of \(\mathrm{Regret}_{T}^{\mathrm{CB}}\) (Theorem 1). Moreover, we need to take a union bound over all \(h\in[H]\).