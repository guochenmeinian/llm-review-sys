# Faster Repeated Evasion Attacks in Tree Ensembles

 Lorenzo Cascioli

Department of Computer Science

KU Leuven

Leuven, Belgium

lorenzo.cascioli@kuleuven.be

&Laurens Devos

Department of Computer Science

KU Leuven

Leuven, Belgium

&Ondrej Kuzelka

Faculty of Electrical Engineering

Czech Technical University in Prague

Prague, Czech Republic

&Jesse Davis

Department of Computer Science

KU Leuven

Leuven, Belgium

###### Abstract

Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.

## 1 Introduction

One of the most popular and widely used classes of models is tree ensembles which encompasses techniques such as gradient boosting [16] and random forests [3]. However, like other flexible model classes such as (deep) neural networks [28; 17], they are susceptible to evasion attacks [23]. That is, an adversary can craft an imperceptible perturbation that, when applied to an otherwise valid input example, elicits a misprediction by the ensemble. As an example, consider a bank that uses a learned model to assess whether to approve or deny loan applications. In this setting, an evasion attack could entail slightly altering a potential customer's data (e.g., adding one month to their work seniority) that results in the model making a different decision on the customer's application. The slightly modified customer record is an adversarial example. There is significant interest in reasoning about tree ensembles to both generate such adversarial examples [15; 36] and perform empirical robustness checking [23; 8; 10] where the goal is to determine how close the nearest adversarial example is.

Generating adversarial examples is an NP-hard problem [23], which has spurred the development of approximate techniques [8; 36; 10]. These methods exploit the structure of the trees to find adversarial examples faster, e.g., by using graph transformations [8] or discrete (heuristic) search [36; 10; 12]. Still, these techniques can be slow, particularly if there is a large number of attributes in the domain. This is compounded by the fact that one often wants to generate large sets of adversarial examples.

A weakness to existing approaches is that they ignore the fact that adversarial example generation is often a sequential task where multiple similar problems are being solved in a row. That is, one has access to a large number of "normal" examples each of which should be perturbed to elicita misprediction. Alas, existing approaches treat each considered example in isolation and solve the problem from scratch. However, there are likely regularities among the problems, meaning that the algorithms perform redundant work. If these regularities can be identified efficiently and this information can be exploited to guide the search for an adversarial example, then the run time performance of repeated adversarial example generation can be improved.

Studying these regularities in order to make adversarial example generation faster is an important problem. First, it advances our understanding of the nature of adversarial examples in tree ensembles and their generation methods. This might inspire improvements to generation methods, and in turn lead to better defense or detection methods. Second, model evaluation by verification [26; 29; 10] is quickly becoming important as machine learning is applied in sensitive application areas. Being able to efficiently generate adversarial examples is crucial for computing empirical robustness (e.g., [10]), adversarial accuracy (e.g., [32]), and for model hardening (e.g., [23]). Third, some scenarios exist where an attacker would want to perform a large scale evasion attack. For example, some DNS registries use models to flag new domain registrations as potentially malicious (e.g., for phishing, fake webshops) [27] and scammers likely need to register many such domains. Finally, techniques in the planning community for analyzing policy safety through predicate abstraction involve performing repeated verification queries on the same model [30; 31; 22].

We propose a novel approach that analyzes previously solved adversarial example generation tasks to inform the search for subsequent tasks. Our approach is based on the observation that for a fixed learned tree ensemble, adversarial examples tend to be generated by perturbing the same, relatively small set of features. We propose a theoretically grounded manner to quickly find this set of features. We then propose two novel strategies to use the identified features to guide the search for adversarial examples, one of which is guaranteed to produce an adversarial example if it exists. We apply our proposed approach to two different algorithms for generating adversarial examples [23; 10]. Empirically, our approaches result in speedups of up to 36x/21x and on average of 9x/4x (\(\pm\) 8x/3x). The source code for the presented algorithms and all the experiments is publicly available at https://github.com/lorenzocascioli/faster-repeated-evasion-tree-ensembles.

## 2 Preliminaries

We briefly explain tree ensembles, evasion attacks, and the two adversarial generation methods used in the experiments. We assume a \(d\)-dimensional input space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and binary output space \(\mathcal{Y}=\{-1,1\}\). We focus on binary classification because most existing methods for generating adversarial examples for tree ensembles are designed for this setting [1; 23; 10].

Tree EnsemblesTree ensembles include algorithms such as (gradient) boosted decision trees (GBDTs) [16; 9] and random forests [3; 25]. A tree ensemble contains a number of trees and most implementations only learn binary trees. A binary tree \(T\) contains two types of nodes. _Internal nodes_ store references to a left and a right sub-tree, and a split condition on some attribute \(f\) in the form of a less-than comparison \(X_{f}<\tau\), where \(\tau\) is the split value. _Leaf nodes_ have no children and only contain an output value. Each tree starts with a _root node_, the only one without a parent.

Given an example \(x\), an individual tree is evaluated recursively starting from the _root node_. In each internal node, the split condition is applied and if it is satisfied, then the example is sorted to the left subtree and if not it is sorted to the right one. This procedure terminates when a _leaf node_ is reached. The final prediction of the ensemble \(\bm{T}(x)\) is obtained by combining the predicted leaf values for each tree in the ensemble. In gradient boosting, the class probability is computed by applying a sigmoid transformation to the sum of the leaf values.

Evasion AttacksAn _evasion attack_ involves manipulating valid inputs \(x\) into adversarial examples \(\tilde{x}\) in order to elicit a misprediction [23]. Following existing work on tree ensembles [23; 7; 10], we say that \(\tilde{x}\) is an **adversarial example** for normal example \(x\) when (1) \(\left\|\tilde{x}-x\right\|_{\infty}<\delta\) where \(\delta\) is a user-selected maximum distance (i.e., the two are sufficiently close), (2) the ensemble predicts the correct label for \(x\), and (3) the model's predicted labels for \(\tilde{x}\) and \(x\) differ.

We briefly describe the two existing adversarial example generation methods \(\mathcal{A}:(\bm{T},x,\delta,t_{\max})\rightarrow\{\textit{SAT}(\tilde{x}), \textit{UNSAT},\textit{TIMEOUT}\}\) used in this paper: _kantchelian_[23] and _veritas_[10]. These methods take as input an ensemble \(\bm{T}\), a normal example \(x\), a maximum perturbation size \(\delta\), and a timeout \(t_{\max}\). They output \(\mathit{SAT}(\tilde{x})\), where \(\tilde{x}\) is an adversarial example for \(x\), \(\mathit{UNSAT}\), indicating that no adversarial example exists, or \(\mathit{TIMEOUT}\), indicating that no result could be found within \(t_{\max}\). Timeouts are explicitly handled because adversarial example generation is NP-hard [23].

_kantchelian_ formulates the adversarial example generation task as a mixed-integer linear program (MILP) and uses a generic MILP solver (e.g., Gurobi [20]). Specifically, _kantchelian_ directly minimizes the \(\delta=\left\|x-\tilde{x}\right\|_{\infty}\) value. Given an example \(x\), it computes:

\[\min_{\tilde{x}}\left\|x-\tilde{x}\right\|_{\infty}\quad\text{ subject to}\quad\bm{T}(x)\neq\bm{T}(\tilde{x}).\] (1)

This approach exploits the fact that a tree ensemble can be viewed as a set of linear (in)equalities. Three sets of MILP variables are used. _Predicate variables_\(p_{i}\) represent the split conditions, i.e., each \(p_{i}\) logically corresponds to a split on an attribute \(f\): \(p_{i}\equiv f<\tau\). _Leaf variables_\(l_{i}\) indicate whether a leaf node is active. The _bound variable_\(b\) represents the \(l_{\infty}\) distance between the original example \(x\) and the adversarial example \(\tilde{x}\). Constraints between the variables encode the structure of the tree. A set of predicate consistency constraints encode the ordering between splits. For example, if two split values \(\tau_{1}<\tau_{2}\) appear in the tree for attribute \(f\), and \(p_{1}\equiv f<\tau_{1}\) and \(p_{2}\equiv f<\tau_{2}\), then \(p_{1}\implies p_{2}\). Leaf consistency constraints enforce that a leaf is only active when the splits on the root-to-leaf path to that leaf are satisfied. Lastly, the mislabel constraint requires the output to be a certain class: for leaf values \(v_{i}\), \(\sum_{i}v_{i}l_{i}\lessgtr 0\). The objective directly minimizes the _bound variable_.

_veritas_ improves upon _kantchelian_ in terms of run time by formulating the adversarial example generation problem as a heuristic search problem in a graph representation of the ensemble (originally proposed by [8]). The nodes in this graph correspond to the leaves in the trees of the ensemble. Guided by a heuristic, the search then repeatedly selects compatible leaves. Leaves of two different trees are compatible when the conjunction of the split conditions along the root-to-leaf paths of the leaves are logically consistent. For a given \(\delta\), _veritas_ solves the following optimization problem:1

Footnote 1: We are abusing terminology: here, \(\bm{T}(x)\) is the predicted probability. Previously, it was the predicted label.

\[\operatorname*{\mathrm{optimize}}_{\tilde{x}}\ \bm{T}(\tilde{x})\quad \text{subject to}\quad\left\|x-\tilde{x}\right\|_{\infty}<\delta\] (2)

The output of the model \(\bm{T}(\tilde{x})\) is maximized when the target class for \(\tilde{x}\) is positive, and minimized otherwise. While _veritas_ can also directly optimize \(\delta\), in this paper we will use a predefined \(\delta\) for _veritas_. To the best of our knowledge, _veritas_ is the fastest approximate evasion attack for tree ensembles (see Appendix B.1).

## 3 Method

Adversarial example generation methods are often applied in the following setting:

\begin{tabular}{l l}
**Given** & a tree ensemble \(\bm{T}\), a set of test examples \(\mathcal{D}\), and a maximum perturbation size \(\delta\) \\
**Generate** & adversarial examples for each \(x\in\mathcal{D}\). \\ \end{tabular}

The goal of this paper is to exploit the fact that adversarial examples are sequentially generated for each example in \(\mathcal{D}\). By analyzing previously found adversarial examples, we aim to improve the efficiency of adversarial example generation algorithms by biasing the search towards the perturbations that are most likely to lead to an adversarial example.

Our hypothesis is that some parts of the ensemble are disproportionately sensitive to small perturbations, i.e., crossing the thresholds of split conditions in these parts of the ensemble results in large changes in the predicted value. Prior work has hypothesized that robustness is related to fragile features and that such features are included in models because learners search for any signal that improves predictive performance [21]. One would expect that the attributes used in the split conditions in these disproportionately sensitive parts are exploited by adversarial examples more frequently than other attributes. Figure 1 illustrates this point by showing how often each attribute is perturbed in a set of a 10 000 adversarial examples generated by _kantchelian_ for two datasets. The bar plots distinguish among attributes are (1) never modified by any adversarial example (left), (2) modified by at least one but at most 5% of all adversarial examples (middle), and (3) modified by more than 5% of the adversarial examples (right). Less than 10% of the attributes are used by more than 5% of the adversarial examples. The figure shows that regularities exist in constructed adversarial examples: examples generated for different normal examples tend to exhibit perturbations to the same small set of attributes. Thus the two questions are how can one identify these frequently-modified attributes and how can algorithms exploit this knowledge to more quickly generate adversarial examples.

Our proposed approach has two parts. The first part simplifies the search for adversarial examples by only allowing perturbations to a limited subset of features. Namely, we exploit the knowledge that certain feature values are fixed to simplify the ensemble, by pruning away branches that can never be reached. The second part identifies a subset of commonly perturbed features by counting how often each feature is perturbed by adversarial examples. The size of this subset is determined by applying a theoretically grounded statistical test.

### Modifying the Search Procedure

Our proposed approach speeds up the adversarial example generation procedure by limiting the scope of the adversarial perturbations to a subset of features \(F_{S}\). This section assumes that we are given such a subset of features. The next section covers how to identify these features.

We consider three settings: _full_, _pruned_, and _mixed_. The _full_ setting is the original configuration of _kantchelian_ and _veritas_: the methods may perturb any attribute within a certain maximum distance \(\delta\). That is, for each attribute \(f\in F\) with value \(x_{f}\), the attribute values are limited to \([x_{f}-\delta,x_{f}+\delta]\). Algorithm 1 summarizes the _pruned_ and _mixed_ approaches. We now describe both in greater detail.

```
1:parameters: maximum perturbation size \(\delta\), timeouts \(t_{\max}^{full}\) and \(t_{\max}^{prun}\) for _full_ and _pruned_, generation method \(\mathcal{A}:(\bm{T},x,\delta,t)\rightarrow\{SAT(\tilde{x}),\,\textit{UNSAT}, \,\textit{TIMEOUT}\}\)
2:functionGenerate(\(\bm{T}_{full},\mathcal{D},F_{S},\textit{mixed flag}\))
3:\(\tilde{\mathcal{D}}\leftarrow\emptyset\)
4:for\(x\in\mathcal{D}\)do
5:\(\bm{T}_{prun}\leftarrow\textsc{Prune}\left(\bm{T}_{full},F_{S},x\right)\) (Sec. 3.1)
6:\(\alpha\leftarrow\mathcal{A}\left(\bm{T}_{prun},x,\delta,t_{\max}^{prun}\right)\)
7:if\(\alpha\neq SAT(\tilde{x})\wedge\textit{mixed flag set then
8:\(\alpha\leftarrow\mathcal{A}\left(\bm{T}_{full},x,\delta,t_{\max}^{full}\right)\)
9:endif
10:\(\tilde{\mathcal{D}}\leftarrow\tilde{\mathcal{D}}\cup\{\alpha\}\)
11:endfor
12:return:\(\tilde{\mathcal{D}}\)
13:endfunction ```

**Algorithm 1** Fast repeated adversarial example generation

Pruned ApproachThe _pruned_ setting disallows modifications to the attributes in the _non-selected_ set of attributes \(F_{NS}=F\setminus F_{S}\). We accomplish this by pruning the trees in the ensemble. Any node splitting on attributes in \(F_{NS}\) is removed. Its parent node is directly connected to the only child node that can be reached by examples with the fixed value for the attribute. Figure 2 shows an example of this procedure. We refer to this procedure as \(\textsc{Prune}(\bm{T},F_{S},x)\). The adversarial example generation methods can be applied as normal to the pruned ensemble, but they will only generate adversarial examples with perturbations to the attributes in \(F_{S}\). Pruning simplifies the MILP problem of _kantchelian_ because all predicate variables \(p_{i}\) that correspond to splits in internal nodes of pruned

Figure 1: Bar plots showing that most attributes are not modified by the majority of adversarial examples on the _mnist_ and _webspam_ datasets. The leftmost bar shows the number of attributes that are never changed by any of the 10 000 adversarial examples generated by _kantchelian_’s approach. The middle bar shows the number of attributes that are modified at least once but at most by 5% of the adversarial examples. The rightmost bar shows the number of frequently modified features.

subtrees, and leaf variables \(l_{i}\) that correspond to leaves of pruned subtrees can be removed from the mathematical formulation. For _veritas_, the search space is reduced in size because the pruned leaves are removed from the graph representation of the ensemble. Hence, for both systems, on average, the problem difficulty is reduced by pruning the ensembles.

Pruning the trees does not affect the validity of generated adversarial examples: if \(\tilde{x}\) is an adversarial example for a normal example \(x\) generated on a pruned ensemble, then \(\tilde{x}\) is also an adversarial example for the full ensemble.

**Proposition 3.1**.: _Given normal example \(x\) that is correctly classified by the full ensemble \(\bm{T}_{\mathit{full}}\). Let \(\bm{T}_{prun}=\textsc{Prun}(\bm{T}_{\mathit{full}},F_{S},x)\) and \(\tilde{x}=\mathcal{A}(\bm{T}_{prun},x,\delta,t_{\max})\) (i.e., \(\bm{T}_{prun}(x)\neq\bm{T}_{prun}(\tilde{x})\) and \(\left\|x-\tilde{x}\right\|_{\infty}<\delta\)). Then it holds that \(\bm{T}_{\mathit{full}}(x)\neq\bm{T}_{\mathit{full}}(\tilde{x})\)._

Proof.: Because only branches not visited by \(x\) are removed, \(\bm{T}_{prun}(x)=\bm{T}_{\mathit{full}}(x)\). The values for features in \(F_{\mathit{NS}}\) are fixed, so these values are equal between \(x\) and \(\tilde{x}\). Hence, \(\tilde{x}\) only visits branches in \(\bm{T}_{\mathit{full}}\) that are also in \(\bm{T}_{prun}\). Therefore, \(\bm{T}_{prun}(\tilde{x})=\bm{T}_{\mathit{full}}(\tilde{x})\). 

However, an _UNSAT_ generated on a pruned ensemble is inconclusive: it might still be the case that an adversarial example exists for the full ensemble, albeit one with perturbations to features in \(F_{\mathit{NS}}\). The _pruned_ setting generates a **false negative** if it reports _UNSAT_, yet the _full_ setting reports _SAT_.

Mixed ApproachThe _mixed_ setting takes advantage of the fast adversarial generation capabilities of the _pruned_ setting, but falls back to the _full_ setting when the _pruned_ setting returns an _UNSAT_ or times out. A much stricter timeout \(t_{\max}^{prun}\) is used for the _pruned_ setting to fully take advantage of the fast _SAT_s, while avoiding spending time on an uninformative _UNSAT_. The _mixed_ setting is guaranteed to find an adversarial example if the _full_ setting can find one.

**Theorem 3.2**.: _Assume a normal example \(x\) and maximum distance \(\delta\). If an adversarial example can be found for the full ensemble \(\bm{T}_{\mathit{full}}\), then the mixed setting is guaranteed to find an \(\tilde{x}\) such that \(\left\|x-\tilde{x}\right\|_{\infty}<\delta\) and \(\bm{T}_{\mathit{full}}(x)\neq\bm{T}_{\mathit{full}}(\tilde{x})\)._

Proof.: The _mixed_ setting first operates on the pruned ensemble \(\bm{T}_{prun}\) using a tight timeout and optimizes Equation 1 or 2 using _kantchelian_ or _veritas_ respectively. This returns (1) an adversarial example \(\tilde{x}\), (2) an _UNSAT_ or (3) times out. In case (1), the generated adversarial example \(\tilde{x}\) is also an adversarial example for the full ensemble (Prop 3.1). In cases (2) and (3), the _mixed_ setting falls back to the _full_ setting operating on the full ensemble \(\bm{T}_{\mathit{full}}\) with the same timeout. Hence, it inherits the full method's guarantees. 

### Identifying Relevant Features

A good subset of relevant attributes \(F_{S}\) should satisfy two properties. First, it should minimize the number of false negatives, which occur when the _pruned_ approach reports _UNSAT_, but the _full_ approach reports _SAT_. Second, the feature subset should be small. The smaller \(F_{S}\) is, the more the ensemble can be pruned, and the larger the speedup is. These two objectives are somewhat in tension. Including more features will reduce the number of false negatives but limit the speedups, whereas using a very small subset will restrict the search too much resulting in many false negatives (or slow calls to the full search in the _mixed_ setting). The procedure is given in Algorithm 2.

Figure 2: An example tree using two attributes Height and Age (left). Suppose \(F_{\mathit{NS}}=\{\textsc{Age}\}\). Given an example where \(\textsc{Age}=55\), we can prune away the internal node splitting on Age. In the resulting tree (right), subtree (b) is pruned because it is unreachable given that \(\textsc{Age}=55\) and only subtrees (a) and (c) remain.

We address the first requirement by adding features to the subset that are frequently perturbed by adversarial examples. We rank features by counting how often each one differs between the perturbed adversarial examples in \(\tilde{\mathcal{D}}\) so far and their corresponding normal examples in \(\mathcal{D}\).

The second requirement is met by statistically testing whether the identified subset guarantees that the false negative rate is smaller than a given threshold \(\tau\) with probability at least \(1-\eta\), for a specified confidence parameter \(\eta\). If it is not guaranteed, then the subset is expanded. This is done at most 4 times for subsets of 5%, 10%, 20%, 30% of the features (ExpandFeatureSet\((F_{S},\mathcal{D},\tilde{\mathcal{D}})\) in Algorithm 2). If all tests fail, then a final feature subset of 40% of the most commonly modified features is used. We do not go beyond 40% because using the full feature set is then more efficient. Each test is executed on a small set of \(n\) generated adversarial examples. A first zeroth set is used merely for obtaining the first feature counts.

The statistical tests are performed as follows. The null hypothesis is that _FNR_ is greater than the threshold \(\tau\). Take \(\mathcal{D}_{F}=(x_{1},x_{2},\ldots,x_{N})\) the dataset we use to find the feature subset \(F_{S}\). We define \(\mathbf{v}=(v_{1},v_{2},\ldots,v_{N})\) to be the binary vector such that \(v_{i}=1\) if the _pruned_ search with the feature subset \(F_{S}\) returns _UNSAT_ for the example \(x_{i}\) but the _full_ search returns \(SAT\), and \(v_{i}=0\) otherwise. Then the true false negative rate corresponding to \(F_{S}\) can be written as \(\textit{FNR}=\frac{1}{N}\sum_{i=1}^{N}v_{i}\). The small set of \(n\) examples from which we are estimating the false negative rate is a random vector \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})\) sampled without replacement from \(\mathcal{D}_{F}\). We also define \(\mathbf{V}=(V_{1},V_{2},\ldots,V_{n})\) where \(V_{i}\) is the binary random variable defined anisogically to how we defined \(v_{i}\). It follows that \(\sum_{i=1}^{n}V_{i}\) is distributed as a hypergeometric random variable. We use the method of _inversion of acceptance intervals_ to find a one-sided confidence interval \([0;\Delta]\) for the false negative rate with confidence level equal to a given \(1-\eta\) (see, e.g., Section 5.2 in [2]), exploiting the fact that the cumulative distribution function of a hypergeometric distribution can be computed efficiently (ConfidenceInterval\((\bar{v},n,\eta)\) in Algorithm 2). We reject the null hypothesis if the confidence interval does not contain the threshold \(\tau\). It follows from the basic properties of confidence intervals that this yields the desired test with confidence \(1-\eta\). Since we execute the test 4 times in the algorithm, we apply a union-bound correction of factor 4 (we use confidence level 0.9). Note that there is a trade-off. The higher \(n\), the better the statistical estimates and the counts are, but also the more examples we process with a potentially suboptimal feature subset.

```
1:parameters: set of normal examples \(\mathcal{D}\), sample size \(n\), acceptable false negative rate \(\tau\), confidence parameter \(\eta\)
2:\(F_{S}\leftarrow\emptyset\)
3:for\(k\in 0..4\)do
4:\(\tilde{\mathcal{D}}\leftarrow\textsc{Generate}(\boldsymbol{T},\mathcal{D}[kn,k(n+ 1)],F_{S},\textit{true})\)
5:\(\bar{v}\leftarrow\frac{1}{n}\times\) number of false negatives in \(\tilde{\mathcal{D}}\)
6:\([0;\Delta]\leftarrow\textsc{ConfidenceInterval}(\bar{v},n,\eta)\)
7:if the threshold \(\tau\) is in \([0;\Delta]\), then ExpandFeatureSet\((F_{S},\mathcal{D},\tilde{\mathcal{D}})\)
8:else break the loop
9:endfor
10:return:\(F_{S}\) ```

**Algorithm 2** Find feature subset

## 4 Experiments

Empirically, we address three questions: (Q1) Is our approach able to improve the run time of generating adversarial examples? (Q2) How does ensemble complexity affect our approach's performance? (Q3) What is our empirical false negative rate?

Because the described procedure is based on identifying a subset of relevant features, it makes sense to exploit it only when the dataset has a large number of dimensions. Therefore, we present numerical experiments for ten binary classification tasks on high-dimensional datasets, using both tabular data and image data, as shown in Table 1.

Experimental SetupWe apply 5-fold cross validation for each dataset. We use four of the folds to train an XGBoost [9], random forest [3; 25] or GROOT forest (a robustified ensemble type [32])ensemble \(\bm{T}\). From the test set, we randomly sample 10 000 normal examples and attempt to generate adversarial examples by perturbing each one using the _kantchelian_ or _veritas_ attack. Table 1 also reports the adopted values of maximum perturbation \(\delta\) and the hyperparameters of the learned ensembles, which were selected via tuning using the grid search described in Appendix B. The experiments were run on an Intel(R) E3-1225 CPU with 32GiB of memory.

The _pruned_ and _mixed_ settings work as follows. We use the procedure from Section 3.2 to select a subset of relevant features. We use \(\tau=0.25\), \(n=100\) and \(\eta=0.1\). We then apply Algorithm 2: we generate 5 sets of \(n\) adversarial examples to (1) find which features are perturbed most often and (2) determine the size of the feature subset \(F_{S}\). Therefore the extracted feature set gives us a \(1-\eta=90\%\) confidence that our true false negative rate is below \(25\%\). After Algorithm 2 terminates, \(F_{S}\) is fixed, and we run the _pruned_ and _mixed_ settings on all the remaining test examples (Algorithm 1). We set a timeout of one minute for the _full_ setting, and a much stricter timeout of 1 (_kantchelian_) or 0.1 (_veritas_) seconds in the _pruned_ setting. We can be stricter with _veritas_ as it is an approximate method that is faster than the exact _kantchelian_.2

Footnote 2: Appendix D provides a sensitivity analysis for the hyperparameter settings of the statistical test and timeouts.

Q1: Run TimeTable 2 reports the average run time for the _full_ setting and the average speedup given by the _pruned_ and _mixed_ settings. We present here results for XGBoost and random forest, and report results for GROOT together with more extended results in Appendix C. Considering all three model types and both attacks, speedups for the _pruned_ setting are in the range 1.4x-36.2x with an average of 9x (\(\pm\) 8x), and for the _mixed_ in the range 1.1x-20.5x with an average of 4x (\(\pm\) 3x).

We notice that generating adversarial examples is more difficult for random forests (RF) than XGB. This leads to our strategies offering larger wins for RF than for XGB, with average speedups of 9.4x/3.5x (\(\pm\) 7.2x/1.6x) for RF and 4.7x/2.7x (\(\pm\) 3.4x/1.2x) for XGB. The robustified GROOT forests are even harder to attack, meaning our methods offer even larger improvements with average speedups of 11.4x/4.9x (\(\pm\) 9.9x/5.0x).3

Footnote 3: See Tables 5 and 6 in the supplement.

Tables 5 and 6 in the supplement also report additional statistics on the presented experiments. On average, the _mixed_ setting falls back to the _full_ search 10.5% of the time. The model and attack type do not seem to have a strong influence on the proportion of calls to the _full_ search. This helps it achieve a speedup by taking advantage of the fast _SAT_ results of the _pruned_ setting while still offering the theoretical guarantee from Theorem 3.2.

We also report the attack success rate, which is the fraction of times where our methods generate an adversarial example given that a valid adversarial example exists for the full model. The _pruned_ search has an average success rate of 90% (\(\pm\) 6%). The _mixed_ search has success rate 100% by definition (Theorem 3.2).

\begin{table}
\begin{tabular}{l r r r r r r r r r r r r} \hline \hline  & & & \multicolumn{3}{c}{**XGBoost**} & \multicolumn{3}{c}{**RF**} & \multicolumn{3}{c}{**GROOT**} \\ \cline{3-13}
**Dataset** & N & \#F & \(\delta\) & M & d & \(\eta\) & \(\delta\) & M & d & \(\delta\) & M & d & \(\epsilon\) \\ \hline covtype & 581k & 54 & 0.1 & 50 & 6 & 0.9 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.01 \\ fmunist & 70k & 784 & 0.3 & 50 & 6 & 0.1 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.3 \\ higgs & 250k & 33 & 0.08 & 50 & 6 & 0.1 & 0.08 & 50 & 10 & 0.4 & 50 & 10 & 0.01 \\ miniboone & 130k & 51 & 0.08 & 50 & 6 & 0.1 & 0.08 & 50 & 10 & 0.5 & 50 & 10 & 0.01 \\ mnist & 70k & 784 & 0.3 & 50 & 6 & 0.5 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.3 \\ prostate & 100k & 103 & 0.1 & 50 & 4 & 0.5 & 0.2 & 50 & 10 & 0.2 & 50 & 10 & 0.01 \\ roadsafety & 111k & 33 & 0.06 & 50 & 6 & 0.5 & 0.12 & 50 & 10 & 0.2 & 50 & 10 & 0.05 \\ sensciects & 58.5k & 48 & 0.06 & 50 & 6 & 0.5 & 0.12 & 50 & 10 & 0.2 & 50 & 10 & 0.01 \\ vehicle & 98k & 101 & 0.15 & 50 & 6 & 0.1 & 0.15 & 50 & 10 & 0.4 & 50 & 10 & 0.1 \\ webspam & 350k & 254 & 0.04 & 50 & 5 & 0.06 & 50 & 10 & 0.1 & 50 & 10 & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Datasets’ characteristics: \(N\) and \(\#F\) are the number of examples and the number of features. _higgs_ and _prostate_ are random subsets of the original, bigger datasets. Multi-class classification datasets were converted to binary classification: for _covtype_ we predict majority-vs-rest, for _mnist_ and _fmnist_ we predict classes 0-4 vs. classes 5-9, and for _sensorless_ classes 0-5 vs. classes 6-10. We also report adopted values of max allowed perturbation \(\delta\) and learners’ tuned hyperparameters after the grid search described in Appendix B. Each ensemble \(\bm{T}\) has maximum tree depth d and contains M trees. The learning rate for XGBoost is \(\eta\). GROOT robustness is defined by \(\epsilon\).

Figure 3 shows the number of executed searches as a function of time for four combinations of attack algorithm and model type.4 For XGB, both attacks benefit. Moreover, the _mixed_ setting is typically very close in run time to the _pruned_. On RF, the _pruned_ setting offers larger speedups. However, we see a more noticeable difference between the _pruned_ and the _mixed_ search on several datasets. This indicates that the _mixed_ strategy falls back more often to an expensive _full_ search.

Footnote 4: The supplement shows these plots for all datasets plus for GROOT forests (see Appendix C).

Finally, it is natural to wonder how the quality of the generated adversarial examples is affected by the modified search procedure. While this is difficult to quantify, Figure 4 provides some examples of constructed adversarial examples for the _mnist_ dataset and an XGBoost ensemble. Visually, the examples constructed by _full_ and _pruned_ settings for both attacks are very similar. The examples constructed using _kantchelian_ look more similar to the base example than those for _veritas_ because _kantchelian_ finds the closest possible adversarial example whereas _veritas_ has a different objective: it constructs an adversarial example that will elicit a highly confident misprediction. See Appendix E for more generated examples.

Q2: Scaling BehaviorTwo key hyperparameters of tree ensembles are the maximum depth of each learned tree and the number of trees in the ensemble. We explore how varying these affects our approach, employing the same setup as described in Q1. We use the _mnist_ dataset and omit

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Kantchelian XGB**} & \multicolumn{3}{c}{**Kantchelian RF**} & \multicolumn{3}{c}{**Veritas XGB**} & \multicolumn{3}{c}{**Veritas RF**} \\ \cline{2-13}  & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(9.9\)m & \(3.3\times\) & \(2.1\times\) & \(25.7\)m & \(11.0\times\) & \(3.7\times\) & \(5.3\)s & \(1.7\times\) & \(1.4\times\) & \(45.0\)s & \(6.0\times\) & \(2.9\times\) \\ fmnist & \(1.5\)h & \(4.9\times\) & \(3.9\times\) & \(56.2\)m & \(7.8\times\) & \(4.3\times\) & \(43.6\)s & \(1.4\times\) & \(1.3\times\) & \(6.6\)m & \(3.6\times\) & \(3.0\times\) \\ higgs & \(3.3\)h & \(4.1\times\) & \(1.8\times\) & \(5.1\)h & \(3.0\times\) & \(1.4\times\) & \(20.4\)s & \(2.9\times\) & \(2.4\times\) & \(41.8\)m & \(21.4\times\) & \(2.5\times\) \\ miniboone & \(6.0\)h* & \(10.9\times\) & \(3.4\times\) & \(6.0\)h* & \(9.4\times\) & \(5.2\times\) & \(1.2\)m & \(8.4\times\) & \(5.9\times\) & \(12.9\)m & \(11.8\times\) & \(6.4\times\) \\ mnist & \(23.9\)m & \(6.9\times\) & \(5.1\times\) & \(36.6\)m & \(5.7\times\) & \(4.8\times\) & \(47.9\)s & \(2.5\times\) & \(2.1\times\) & \(3.3\)m & \(3.2\times\) & \(2.9\times\) \\ prostate & \(12.8\)m & \(3.4\times\) & \(2.8\times\) & \(6.0\)h* & \(11.8\times\) & \(5.2\times\) & \(9.9\)s & \(2.5\times\) & \(2.2\times\) & \(23.7\)m & \(16.9\times\) & \(2.6\times\) \\ roadsfery & \(10.7\)m & \(3.0\times\) & \(2.0\times\) & \(45.2\)m & \(5.4\times\) & \(2.3\times\) & \(11.4\)s & \(2.7\times\) & \(2.1\times\) & \(40.6\)m & \(33.5\times\) & \(3.1\times\) \\ sensorless & \(29.8\)m & \(2.3\times\) & \(2.1\times\) & \(52.5\)m & \(5.7\times\) & \(3.6\times\) & \(12.1\)s & \(2.9\times\) & \(1.8\times\) & \(4.1\)m & \(4.7\times\) & \(1.5\times\) \\ vehicle & \(2.5\)h & \(5.9\times\) & \(3.4\times\) & \(3.8\)h & \(7.1\times\) & \(5.8\times\) & \(19.4\)m & \(15.6\times\) & \(1.9\times\) & \(42.8\)m & \(9.1\times\) & \(1.1\times\) \\ webspam & \(24.2\)m & \(5.7\times\) & \(3.7\times\) & \(1.5\)h & \(7.3\times\) & \(5.7\times\) & \(18.8\)s & \(2.6\times\) & \(2.1\times\) & \(12.9\)m & \(3.9\times\) & \(1.2\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average run times and speedups when attempting to generate \(10\,000\) adversarial examples using _kantchelian/veritas_ on an XGBoost/random forest ensemble for _full_, _pruned_ and _mixed_. A * means that the dataset exceeded the global timeout of six hours.

Figure 3: Average run times for \(10\,000\) calls to _full_, _pruned_ and _mixed_ for _kantchelian_ (top) and _veritas_ (bottom). Results are given for both XGBoost and random forest for four selected datasets.

[MISSING_PAGE_FAIL:9]

perform robustness checking [8], and verify that the ensembles satisfy certain criteria [11; 10; 26; 29]. Kantchelian et al. [23] were the first to show that tree ensembles are susceptible to evasion attacks. Their MILP formulation is still the most frequently used method to check robustness and generate adversarial examples. Beyond this exact approach, several approximate approaches exist [8; 10; 35; 36] though not all of them are able to generate concrete adversarial examples (e.g., [8; 35]).

Other work focuses on making tree ensembles more robust. Approaches for this include adding generated adversarial examples to the training data (model hardening) [23], or modifying the splitting procedure [7; 4; 32]. Gaining further insights into how evasion attacks target tree ensembles, like those contained in this paper, may inspire novel ways to improve the robustness of learners.

Another line of work aims at directly training tree ensembles that admit verification in polynomial time [5; 13]. However, a drawback to current approaches is that they result in (large) decreases in predictive performance.

Finally, performing evasion attacks has been studied for other model classes with deep neural networks receiving particular attention [28; 17; 24; 6]. However, state-of-the-art algorithms are tailored to one specific model type as they typically exploit specific properties of the model, e.g., the work on tree ensembles often exploits the logical structure of a decision tree.

## 6 Conclusions

This paper explored two methods to efficiently generate adversarial examples for tree ensembles. We showed that considering only the _same subset of features_ is typically sufficient to generate adversarial examples for tree ensemble models. We proposed a simple procedure to quickly identify such a subset of features, and two generic approaches that exploit it to speed up adversarial examples generation. We showed how to apply them to an exact (_kantchelian_) and approximate (_veritas_) evasion attack on three types of tree ensembles, and discussed their properties and run time performances.

**Limitations.** Our approach speeds up evasion attacks in the specific scenario when the same model is repeatedly attacked. Plus, it excels on high-dimensional datasets. Our evaluation only considered \(l_{\infty}\) attacks, whereas other norms such \(l_{1}\) and \(l_{2}\) are also relevant.

**Impact Statement.** While this work does make attacking tree ensembles faster, it is also important to understand what attackers may do. This work also targets increasing the applicability of robustness checking and hardening techniques, which can lead to approaches for training more robust models.

**Acknowledgements.** This research is supported by the Research Foundation Flanders (FWO, LC: 1118125N), The European Union's Horizon Europe Research and Innovation program under the grant agreement TUPLES No. 101070149 (LC, LD, OK, JD), and the Flemish Government under the "Onderzoeksprogramma Artificelle Intelligentie (AI) Vlaanderen" program (JD).

## References

* [1] M. Andriushchenko and M. Hein. Provably robust boosted decision stumps and trees against adversarial attacks. In _Advances in Neural Information Processing Systems_, 2019.

Figure 6: Speedups achieved by the _pruned_ setting when attempting to generate \(10\,000\) adversarial examples using _kantchelian_ (left) and _veritas_ (right) on an XGB ensemble, varying the empirical false negative rate. The dotted horizontal line corresponds to a speedup of \(1\)x, i.e., same run time of the _full_ setting.

* [2] J. Bartroff, G. Lorden, and L. Wang. Optimal and fast confidence intervals for hypergeometric successes. _The American Statistician_, 2023.
* [3] L. Breiman. Random forests. _Machine learning_, 2001.
* [4] S. Calzavara, C. Lucchese, G. Tolomei, S. A. Abebe, and S. Orlando. Treatant: training evasion-aware decision trees. _Data Mining and Knowledge Discovery_, 2020.
* [5] S. Calzavara, L. Cazzaro, G. E. Pibiri, and N. Prezza. Verifiable learning for robust tree ensembles. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, 2023.
* [6] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In _IEEE Symposium on Security and Privacy_, 2017.
* [7] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh. Robust decision trees against adversarial examples. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [8] H. Chen, H. Zhang, S. Si, Y. Li, D. Boning, and C.-J. Hsieh. Robustness verification of tree-based models. In _Advances in Neural Information Processing Systems_, 2019.
* [9] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2016.
* [10] L. Devos, W. Meert, and J. Davis. Versatile verification of tree ensembles. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* [11] L. Devos, W. Meert, and J. Davis. Verifying tree ensembles by reasoning about potential instances. In _Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)_, 2021.
* [12] L. Devos, L. Cascioli, and J. Davis. Robustness verification of multiclass tree ensembles. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2024.
* [13] L. Devos, D. C. Oruc, W. Meert, H. Blockeel, and J. Davis. Tree ensemble compression for interpretability. In _Advances in Interpretable Machine Learning and Artificial Intelligence (AIMLAI)_, 2024.
* [14] D. I. Diochnos, S. Mahloujifar, and M. Mahmoody. Adversarial risk and robustness: General definitions and implications for the uniform distribution. In _Advances in Neural Information Processing Systems_, 2018.
* [15] G. Einziger, M. Goldstein, Y. Sa'ar, and I. Segall. Verifying robustness of gradient boosted models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019.
* [16] J. H. Friedman. Greedy function approximation: a gradient boosting machine. _Annals of statistics_, 2001.
* [17] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations_, 2015.
* [18] P. Gourdeau, V. Kanade, M. Kwiatkowska, and J. Worrell. On the hardness of robust classification. _Journal of Machine Learning Research_, 2021.
* [19] J.-Q. Guo, M.-Z. Teng, W. Gao, and Z.-H. Zhou. Fast provably robust decision trees and boosting. In _Proceedings of the 39th International Conference on Machine Learning_, 2022.
* [20] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.gurobi.com.
* [21] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not bugs, they are features. In _Advances in Neural Information Processing Systems_, 2019.
* [22] C. Jain, L. Cascioli, L. Devos, M. Vinzent, M. Steinmetz, J. Davis, and J. Hoffmann. Safety verification of tree-ensemble policies via predicate abstraction. In _Proceedings of the 27th European Conference on Artificial Intelligence_, 2024.

* [23] A. Kantchelian, J. D. Tygar, and A. Joseph. Evasion and hardening of tree ensemble classifiers. In _Proceedings of the 33th International Conference on Machine Learning_, 2016.
* [24] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In _Computer Aided Verification_, 2017.
* [25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 2011.
* [26] F. Ranzato and M. Zanella. Abstract interpretation of decision tree ensemble classifiers. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020.
* [27] J. Spooren, T. Vissers, P. Janssen, W. Joosen, and L. Desmet. Premadoma: An operational solution for DNS registries to prevent malicious domain registrations. In _Proceedings of the 35th Annual Computer Security Applications Conference_, 2019.
* [28] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, 2014.
* [29] J. Tornblom and S. Nadjm-Tehrani. Formal verification of input-output mappings of tree ensembles. _Science of Computer Programming_, 2020.
* [30] M. Vinzent, M. Steinmetz, and J. Hoffmann. Neural network action policy verification via predicate abstraction. In _Proceedings of the International Conference on Automated Planning and Scheduling_, 2022.
* [31] M. Vinzent, S. Sharma, and J. Hoffmann. Neural policy safety verification via predicate abstraction: CEGAR. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023.
* [32] D. Vos and S. Verwer. Efficient training of robust decision trees against adversarial examples. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* [33] D. Vos and S. Verwer. Robust optimal classification trees against adversarial examples. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.
* [34] D. Vos and S. Verwer. Adversarially robust decision tree relabeling. In _Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2022._, 2022.
* [35] Y. Wang, H. Zhang, H. Chen, D. Boning, and C.-J. Hsieh. On lp-norm robustness of ensemble decision stumps and trees. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [36] C. Zhang, H. Zhang, and C.-J. Hsieh. An efficient adversarial attack for tree ensembles. In _Advances in Neural Information Processing Systems_, 2020.

Analysis of the Problem Setting

Adversarial examples are often generated for tasks like computing adversarial accuracy [32], computing empirical robustness [10], and performing model hardening [23]. The effect of using the approximation proposed in this paper differs for each task.

Computing the adversarial accuracy of a classifier only requires determining whether an adversarial example \(\tilde{x}\) exists within the given \(\delta\) for each provided normal example \(x\). Because the _mixed_ strategy reverts to the original complete search when the _pruned_ approach returns an _UNSAT_, as stated in Theorem 3.2 it is guaranteed to find an adversarial example if it exists. Hence, the _mixed_ strategy can speed up computing the adversarial accuracy without affecting its value.

Computing the empirical robustness of a classifier requires finding the nearest adversarial example \(\tilde{x}\) for each normal example \(x\). Because the _pruned_ approach does not consider all features and the _mixed_ approach may not, they may return an adversarial example that is further away than if the full search space was considered. Hence, when using an exact attack like _kantchelian_, the empirical robustness computed using the _mixed_ strategy is an overestimate of the true empirical robustness. We show this and we study what happens with an approximate method in Appendix E.

In model hardening, a large number of adversarial examples are generated and added to the training data [23]. The _pruned_ approach can be used to generate a lot more adversarial examples in a fixed amount of time.

## Appendix B Employed Datasets, Models and Attacks

We expand the discussion on Table 1, which reports the characteristics of the employed datasets and models. In our experiments, we use ten high-dimensional datasets where the number of dimensions is greater than 25. We verified that our approach does not bring consistent run time improvements for datasets with less than 25 features, or with a fully categorical domain (where the \(l_{\infty}\) norm loses meaning).

Table 1 also reports the hyperparameters of the learned ensembles, which are tuned through grid search. For all model types, we choose the number of trees in \(\{10,20,50\}\). Max depth is chosen in the range \([3,6]\) for XGBoost, and in \(\{5,7,10\}\) for random forest and GROOT forest (which typically need deeper trees to work better). XGBoost learning rate is chosen among \(\{0.1,0.5,0.9\}\). GROOT forest \(\epsilon\) is chosen among \(\{0.01,0.05,0.1,0.3,0.5\}\), and we select the model with the largest \(\epsilon\) such that GROOT forest accuracy does not drop below 90% of the corresponding RF accuracy. Note that in GROOT a bigger \(\epsilon\) corresponds to a more robust model, hence accuracy drops up to a point where the model can become useless in practice.

When running _kantchelian_ on random forests and GROOT forests, we had to limit the number of estimators to 25 due to the extremely long run times.

While the model sizes are smaller, these ensembles are already challenging for the full settings of _kantchelian_ and _veritas_. This is also highlighted in Q2 from Section 4 where we empirically study the effect of increasing the ensemble size on performance. Those results show that the _full_ procedures becomes increasingly slower as the ensemble complexity grows, and our method offers larger wins.

Table 3 gives specific reference to each of the considered datasets.

### Comparison with LT Attack

We evaluate our two methods on a representative set of scenarios, varying the model type (XGBoost, random forest, GROOT forest), and the evasion attack variant (one exact (_kantchelian_) and one approximate (_veritas_)). Given the fact that (1) it is reasonable to expect that most evasion attacks will benefit from smaller pruned models, and (2) we see improvement across all these settings, we are confident that run time improvements also translate to other evasion attack methods.

In practice, other approximate attacks alternative to _veritas_ exist. To the best of our knowledge, no alternative outperforms _veritas_ run times. Computationally, we have compared _veritas_ to another popular state-of-the-art method: LT-attack [36]. On the full setting, they have the same success rate and _veritas_ is 25 to 60 times faster, as shown in Table 4 for XGBoost ensembles.

[MISSING_PAGE_FAIL:14]

\begin{table}
\begin{tabular}{l r r r r r r r r r r r} \hline \hline  & \multicolumn{4}{c}{_full_} & \multicolumn{4}{c}{_pruned_} & \multicolumn{2}{c}{_mixed_} & \multicolumn{2}{c}{\% rel. fears} & \multicolumn{2}{c}{\% full calls} & \multicolumn{2}{c}{\% false neg.} & \multicolumn{2}{c}{\% _pruned_ attack} & \multicolumn{2}{c}{\% _full skip_} \\  & \multicolumn{1}{c}{_full_} & \multicolumn{1}{c}{_pruned_} & \multicolumn{1}{c}{_mixed_} & \multicolumn{1}{c}{\% rel. fears} & \multicolumn{1}{c}{\% full calls} & \multicolumn{1}{c}{\% false neg.} & \multicolumn{1}{c}{\% _pruned_ attack} & \multicolumn{1}{c}{\% _full skip_} \\  & \multicolumn{1}{c}{_full_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} & \multicolumn{1}{c}{_s_} \\ \hline covtype & \(9.9\)m & \(3.0\)m & \(3.3\times\) & \(4.7\)m & \(2.1\times\) & \(5.6\%\) & \(11.7\%\) & \(11.3\%\) & \(88.7\%\) & \(0\) & \% \\ fmnist & \(1.5\)h & \(18.5\)m & \(4.9\times\) & \(22.9\)m & \(3.9\times\) & \(14.6\%\) & \(3.9\%\) & \(3.9\%\) & \(96.1\%\) & \(0\) & \% \\ higgs & \(3.3\)h & \(47.7\)m & \(4.1\times\) & \(1.8\)h & \(1.8\times\) & \(18.0\%\) & \(20.0\%\) & \(9.8\%\) & \(80.4\%\) & \(0\) & \% \\ miniboone & \(6.0\)h* & \(55.5\)m & \(9.0\times\) & \(2.8\)h & \(3.4\times\) & \(33.2\%\) & \(9.2\%\) & \(4.3\%\) & \(91.7\%\) & \(43.1\%\) \\ mnist & \(23.9\)m & \(3.4\)m & \(6.9\times\) & \(4.7\)m & \(5.1\times\) & \(8.8\%\) & \(4.0\%\) & \(4.0\%\) & \(96.0\%\) & \(0\) & \% \\ prostate & \(12.8\)m & \(3.8\)m & \(3.4\times\) & \(4.6\)m & \(2.8\times\) & \(11.8\%\) & \(7.6\%\) & \(6.6\%\) & \(93.3\%\) & \(0\) & \% \\ roadsafety & \(10.7\)m & \(3.6\)m & \(3.0\times\) & \(5.4\)m & \(2.0\times\) & \(23.1\%\) & \(12.9\%\) & \(12.7\%\) & \(87.2\%\) & \(0\) & \% \\ sensorless & \(29.8\)m & \(12.8\)m & \(2.3\times\) & \(14.4\)m & \(2.1\times\) & \(33.8\%\) & \(7.4\%\) & \(7.0\%\) & \(92.9\%\) & \(0\) & \% \\ vehicle & \(2.5\)h & \(25.7\)m & \(5.9\times\) & \(44.2\)m & \(3.4\times\) & \(27.4\%\) & \(12.2\%\) & \(12.2\%\) & \(87.8\%\) & \(0\) & \% \\ webspam & \(24.2\)m & \(4.3\)m & \(5.7\times\) & \(6.6\)m & \(3.7\times\) & \(7.2\%\) & \(8.7\%\) & \(8.7\%\) & \(91.3\%\) & \(0\) & \% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average run times (and speedups) when attempting to generate \(10\,000\) adversarial examples using _kantchelian_ on an XGBoost/random forest/GROOT forest ensemble for all three approaches: _full_, _pruned_ and _mixed_. We also report the average size of the relevant feature subset, the number of calls to the _full_ setting during _mixed_ (= number of _UNSAT_ + number of _TIMEOUT_ for _pruned_), the number of false negatives (_pruned_ returns _UNSAT_, but _full_ returns _SAT_), the _pruned_ attack success rate (i.e., _pruned_ succeeds in generating an adversarial example if an example can be generated for the full ensemble), and the percent of examples that were skipped due to the _full_ search reaching the global timeout of six hours. Experiments that exceeded the timeout are starred. The _pruned_ and _mixed_ settings never reach the global timeout.

\begin{table}
\begin{tabular}{l r r r r r r r r r r} \hline \hline  & _full_ & _pruned_ & \multicolumn{2}{c}{_mixed_} & \multicolumn{2}{c}{\% rel. feats} & \multicolumn{2}{c}{\% full calls} & \multicolumn{2}{c}{\% false neg.} & \multicolumn{2}{c}{\% _pruned_ attack} & \multicolumn{2}{c}{\% _full skip_} \\  & & & & & & & & & success rate & \\ \hline covtype & \(5.3\)s & \(3.1\)s & \(1.7\times\) & \(3.8\)s & \(1.4\times\) & \(5.9\)s & \(12.1\)s & \(11.7\)s & \(88.3\)s & \(0\)s \\ fmnsist & \(43.6\)s & \(31.9\)s & \(1.4\times\) & \(33.6\)s & \(1.3\times\) & \(9.3\)s & \(3.8\)s & \(3.7\)s & \(96.2\)s & \(0\)s \\ higgs & \(20.4\)s & \(7.1\)s & \(2.9\times\) & \(8.6\)s & \(2.4\times\) & \(20.0\)s & \(3.4\)s & \(2.9\)s & \(97.1\)s & \(0\)s \\ miniboone & \(1.2\)m & \(8.5\)s & \(8.4\times\) & \(12.0\)s & \(5.9\times\) & \(14.8\)s & \(4.0\)s & \(3.8\)s & \(96.2\)s & \(0\)s \\ mnist & \(47.9\)s & \(19.0\)s & \(2.5\times\) & \(22.4\)s & \(2.1\times\) & \(5.2\)s & \(7.5\)s & \(7.5\)s & \(92.5\)s & \(0\)s \\ prostate & \(9.9\)s & \(3.9\)s & \(2.5\times\) & \(4.4\)s & \(2.2\times\) & \(11.2\)s & \(7.1\)s & \(6.1\)s & \(93.8\)s & \(0\)s \\ roadsafety & \(11.4\)s & \(4.2\)s & \(2.7\times\) & \(5.4\)s & \(2.1\times\) & \(31.9\)s & \(9.8\)s & \(9.7\)s & \(90.3\)s & \(0\)s \\ sensorless & \(12.1\)s & \(4.2\)s & \(2.9\times\) & \(6.6\)s & \(1.8\times\) & \(17.5\)s & \(15.1\)s & \(14.7\)s & \(85.2\)s & \(0\)s \\ vehicle & \(19.4\)m & \(1.2\)m & \(15.6\times\) & \(10.0\)m & \(1.9\times\) & \(18.6\)s & \(13.9\)s & \(13.6\)s & \(86.2\)s & \(0\)s \\ webspam & \(18.8\)s & \(7.3\)s & \(2.6\times\) & \(9.0\)s & \(2.1\times\) & \(5.3\)s & \(10.2\)s & \(10.2\)s & \(89.8\)s & \(0\)s \\ \hline \hline \multicolumn{11}{c}{**Veritas, RF**} \\ \hline  & _full_ & _pruned_ & \multicolumn{2}{c}{_mixed_} & \multicolumn{2}{c}{\% rel. feats} & \multicolumn{2}{c}{\% full calls} & \multicolumn{2}{c}{\% false neg.} & \multicolumn{2}{c}{\% _pruned_ attack} & \multicolumn{2}{c}{\% _full skip_} \\  & & & & & & & & success rate & \\ \hline covtype & \(45.0\)s & \(7.5\)s & \(6.0\times\) & \(15.4\)s & \(2.9\times\) & \(5.9\)s & \(5.7\)s & \(4.4\)s & \(95.5\)s & \(0\)s \\ fmnsist & \(6.6\)m & \(1.8\)m & \(3.6\times\) & \(2.2\)m & \(3.0\times\) & \(10.7\)s & \(10.9\)s & \(7.4\)s & \(89.1\)s & \(0\)s \\ higgs & \(41.8\)m & \(2.0\)m & \(21.4\times\) & \(16.8\)m & \(2.5\times\) & \(31.3\)s & \(8.8\)s & \(6.8\)s & \(92.0\)s & \(0\)s \\ miniboone & \(12.9\)m & \(1.1\)m & \(11.8\times\) & \(2.0\)m & \(6.4\times\) & \(19.2\)s & \(6.0\)s & \(4.8\)s & \(94.2\)s & \(0\)s \\ mnist & \(3.3\)m & \(1.1\)m & \(3.2\times\) & \(1.2\)m & \(2.9\times\) & \(10.5\)s & \(4.8\)s & \(3.5\)s & \(95.2\)s & \(0\)s \\ prostate & \(23.7\)m & \(1.4\)m & \(16.9\times\) & \(9.3\)m & \(2.6\times\) & \(13.1\)s & \(11.5\)s & \(10.0\)s & \(89.1\)s & \(0\)s \\ roadsafety & \(40.6\)m & \(1.2\)m & \(33.5\times\) & \(13.0\)m & \(3.1\times\) & \(18.8\)s & \(11.6\)s & \(11.1\)s & \(88.9\)s & \(0\)s \\ sensorless & \(4.1\)m & \(52.3\)s & \(4.7\times\) & \(2.8\)m & \(1.5\times\) & \(21.2\)s & \(12.1\)s & \(11.2\)s & \(87.9\)s & \(0\)s \\ vehicle & \(42.8\)m & \(4.7\)m & \(9.1\times\) & \(38.9\)m & \(1.1\times\) & \(23.0\)s & \(32.8\)s & \(17.1\)s & \(67.3\)s & \(0\)s \\ webspam & \(12.9\)m & \(3.3\)m & \(3.9\times\) & \(10.6\)m & \(1.2\times\) & \(10.6\)s & \(13.9\)s & \(2.8\)s & \(86.2\)s & \(0\)s \\ \hline \hline \multicolumn{11}{c}{**Veritas, GROOT**} \\ \hline  & _full_ & _pruned_ & \multicolumn{2}{c}{_mixed_} & \multicolumn{2}{c}{\% rel. feats} & \multicolumn{2}{c}{\% full calls} & \multicolumn{2}{c}{\% false neg.} & \multicolumn{2}{c}{\% _pruned_ attack} & \multicolumn{2}{c}{\% _full skip_} \\  & & & & & & & & success rate & \\ \hline covtype & \(15.9\)s & \(5.8\)s & \(2.8\times\) & \(7.7\)s & \(2.1\times\) & \(5.6\)s & \(4.7\)s & \(4.4\)s & \(95.6\)s & \(0\)s \\ fmnsist & \(32.2\)m & \(53.4\)s & \(36.2\times\) & \(1.6\)m & \(20.5\times\) & \(5.5\)s & \(2.1\)s & \(1.6\)s & \(97.9\)s & \(0\)s \\ higgs & \(4.1\)h* & \(11.6\)m & \(21.8\times\) & \(31.5\)m & \(8.1\times\) & \(24.7\)s & \(7.9\)s & \(3.8\)s & \(90.8\)s & \(16.4\)s \\ miniboone & \(2.6\)m & \(6.1\)s & \(25.8\times\) & \(9.7\)s & \(16.2\times\) & \(7.2\)s & \(2.6\)s & \(2.6\)s & \(97.4\)s & \(0\)s \\ mnist & \(3.1\)m & \(1.8\)m & \(1.7\times\) & \(2.5\)m & \(1.2\times\) & \(6.2\)s & \(11.8\)s & \(7.7\)s & \(88.2\)s & \(0\)s \\ prostate & \(33.9\)m &In Figure 9, we show the empirical FNR on the x-axis versus the speedup of the _pruned_ approach on the y-axis, using an XGB ensemble on four selected datasets. The empirical FNR is the fraction of times the _pruned_ approach returns _UNSAT_ but the full approach returns _SAT_. Going from left to right, we observe that the FNR is more efficient than the FNR.

Figure 7: Run times to attempt to generate adversarial examples for 10 000 test examples with the three presented settings (_full_, _pruned_ and _mixed_), using _kantchelian_ on an XGBoost/random forest/GROOT forest ensemble, averaged over 5 folds.

right along the x-axis, higher values of the FNR correspond to smaller subsets of selected features, hence more aggressive pruning. Using less features, the _pruned_ search becomes faster, at the cost of more false negatives.

Figure 8: Run times to attempt to generate adversarial examples for 10 000 test examples with the three presented settings (_full_, _pruned_ and _mixed_), using _veritas_ on an XGBoost/random forest/GROOT forest ensemble, averaged over 5 folds.

There are cases where false negatives are less problematic, such as when one simply needs to generate a lot of adversarial examples for model hardening [23]. In these cases, the _pruned_ approach really excels at offering run time improvements.

Note that for the considered datasets, FNR values higher than 25% are rare and only occur for very small subsets of features (e.g., 5 out of the 784 features in _mnist_).

## Appendix D Sensitivity Analysis

We briefly discuss how sensitive our algorithm is to the choice of its hyperparameters, namely the threshold and confidence for the statistical test in the feature selection process (see 3.2) and the timeout for the _pruned_ setting.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Kantchelian, XGBoost**} \\ \hline \hline  & \multicolumn{2}{c}{run times} & \multicolumn{4}{c}{\% timeouts} \\ \cline{2-7}  & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(9.9\mathrm{m}\pm 23.0\mathrm{s}\) & \(3.0\mathrm{m}\pm 12.3\mathrm{s}\) & \(4.7\mathrm{m}\pm 8.4\mathrm{s}\) & 0\% & 0\% & 0\% \\ fmnsist & \(1.5\mathrm{h}\pm 4.1\mathrm{m}\) & \(18.5\mathrm{m}\pm 11.4\mathrm{m}\) & \(22.9\mathrm{m}\pm 9.5\mathrm{m}\) & 0\% & \(<\)1\% & 0\% \\ higgs & \(3.3\mathrm{h}\pm 3.3\mathrm{m}\) & \(47.7\mathrm{m}\pm 32.6\mathrm{m}\) & \(1.8\mathrm{h}\pm 42.7\mathrm{m}\) & 0\% & 9.6\% & 0\% \\ minibbone & \(6.0\mathrm{h}\pm 1.8\mathrm{s}\) & \(55.5\mathrm{m}\pm 24.2\mathrm{m}\) & \(2.8\mathrm{h}\pm 52.3\mathrm{m}\) & 0\% & \(<\)1\% & 0\% \\ mnist & \(23.9\mathrm{m}\pm 1.3\mathrm{m}\) & \(3.4\mathrm{m}\pm 52.1\mathrm{s}\) & \(4.7\mathrm{m}\pm 24.1\mathrm{s}\) & 0\% & 0\% & 0\% \\ prostate & \(12.8\mathrm{m}\pm 14.3\mathrm{s}\) & \(3.8\mathrm{m}\pm 20.5\mathrm{s}\) & \(4.6\mathrm{m}\pm 11.0\mathrm{s}\) & 0\% & 0\% & 0\% \\ roadsfafety & \(10.7\mathrm{m}\pm 28.2\mathrm{s}\) & \(3.6\mathrm{m}\pm 40.9\mathrm{s}\) & \(5.4\mathrm{m}\pm 35.3\mathrm{s}\) & 0\% & 0\% & 0\% \\ sensorless & \(29.8\mathrm{m}\pm 2.8\mathrm{m}\) & \(12.8\mathrm{m}\pm 58.0\mathrm{s}\) & \(14.4\mathrm{m}\pm 1.2\mathrm{m}\) & \(<\)1\% & 0\% & 0\% \\ vehicle & \(2.5\mathrm{h}\pm 5.0\mathrm{m}\) & \(25.7\mathrm{m}\pm 9.2\mathrm{m}\) & \(44.2\mathrm{m}\pm 4.3\mathrm{m}\) & 0\% & \(<\)1\% & 0\% \\ webspam & \(24.2\mathrm{m}\pm 1.7\mathrm{m}\) & \(4.3\mathrm{m}\pm 2.1\mathrm{m}\) & \(6.6\mathrm{m}\pm 35.4\mathrm{s}\) & 0\% & 0\% & 0\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: Average run times (with standard deviations) when attempting to generate \(10\,000\) adversarial examples using _kantchelian_ on an XGBoost/random forest/GROOT forest ensemble for all three approaches: _full_, _pruned_ and _mixed_. The average fraction of timeouts incurred during the search is also reported.

### Sensitivity to Statistical Test Parameters

The statistical test described in 3.2 takes as hyperparameters the acceptable false negative rate \(\tau\) and the confidence \(1-\eta\). We perform a sensitivity analysis where we vary \(\tau\in[0.05,0.1,0.25,0.5]\) and \(1-\eta\in[0.8,0.9,0.95]\) on the _miniboone_ dataset.

Table 9 shows the _mixed_ speedup for (_veritas_, XGBoost) for all combinations of the considered values for \(\tau\) (FNR) and \(1-\eta\) (confidence). For all settings, our approach improves upon the run time of always running a _full_ search (i.e., speedup is always \(>1\)). When \(\tau=0.05\), many features are selected and hence there is less pruning. \(\tau=0.1\) and \(\tau=0.25\) perform identically. When \(\tau=0.5\), the value of \(1-\eta\) impacts the selected feature set. A confidence of 0.95 keeps the same feature set of \(\tau=0.1\) and \(\tau=0.25\). However, a lower confidence results in an even smaller feature set, which degrades the performance of the _mixed_ setting because there are more calls to the full search.

Hence more in general, the threshold on the false negative rate \(\tau\) is inversely proportional to the number of chosen features: the larger the selected feature subset, the lower the FNR will be. This is in tension with the goal of using as small of a feature subset as possible, to speed up the _pruned_

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Veritas, XGBoost**} \\ \hline  & \multicolumn{2}{c}{run times} & \multicolumn{3}{c}{\% timeouts} \\ \cline{2-7}  & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(5.3\pm 0.1\)s & \(3.18\pm 0.2\)s & \(3.88\pm 0.2\)s & 0\% & 0\% & 0\% \\ fmnist & \(43.6\pm 1.1\)s & \(31.98\pm 5.4\)s & \(33.68\pm 3.0\)s & 0\% & \textless{}1\% & 0\% \\ higgs & \(20.48\pm 0.3\)s & \(7.18\pm 0.1\)s & \(8.68\pm 0.4\)s & 0\% & 0\% & 0\% \\ miniboone & \(1.2\pm 8.0\)s & \(8.58\pm 2.4\)s & \(12.08\pm 2.5\)s & 0\% & 0\% & 0\% \\ mnist & \(47.98\pm 1.7\)s & \(19.0\pm 0.6\)s & \(22.48\pm 1.1\)s & 0\% & 0\% & 0\% \\ prostate & \(9.98\pm 0.2\)s & \(3.98\pm 0.1\)s & \(4.48\pm 0.0\)s & 0\% & 0\% & 0\% \\ roadsafety & \(11.48\pm 2.7\)s & \(4.28\pm 0.1\)s & \(5.48\pm 0.5\)s & 0\% & 0\% & 0\% \\ sensorless & \(12.18\pm 1.1\)s & \(4.2\pm 0.8\)s & \(6.68\pm 0.5\)s & 0\% & 0\% & 0\% \\ vehicle & \(19.4m\pm 7.3\)m & \(1.21\pm 40.5\)s & \(10.0m\pm 4.2\)m & \textless{}1\% & \textless{}1\% & 4\% \\ webspam & \(18.88\pm 0.8\)s & \(7.38\pm 0.6\)s & \(9.08\pm 0.3\)s & 0\% & 0\% & 0\% \\ \hline \hline \multicolumn{7}{c}{**Veritas, RF**} \\ \hline  & \multicolumn{2}{c}{run times} & \multicolumn{3}{c}{\% timeouts} \\ \cline{2-7}  & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(45.08\pm 5.5\)s & \(7.5\pm 0.3\)s & \(15.48\pm 2.8\)s & 0\% & 0\% & 0\% \\ fmnist & \(6.6m\pm 1.2\)m & \(1.8\pm 1.9\)s & \(2.2\pm 19.0\)s & \textless{}1\% & 3.4\% & \textless{}1\% \\ higgs & \(41.8m\pm 32.7\)m & \(2.0\pm 1.1\)m & \(16.8m\pm 12.2\)m & \textless{}1\% & 1.1\% & \textless{}1\% \\ miniboone & \(12.9m\pm 1.6\)m & \(1.1\pm 38.2\)s & \(2.0\pm 1.0\)m & \textless{}1\% & \textless{}1\% & 0\% \\ mnist & \(3.3\pm 9.5\)s & \(1.1\pm 5.7\)s & \(1.2\pm 5.5\)s & 0\% & 1.3\% & 0\% \\ prostate & \(23.7m\pm 1.7\)m & \(1.4\pm 28.2\)s & \(9.3\pm 2.4\)m & \textless{}1\% & \textless{}1\% & \textless{}1\% \\ roadsafety & \(40.6m\pm 14.7\)m & \(1.2\pm 28.1\)s & \(13.0m\pm 5.6m\) & \textless{}1\% & 0\% & 0\% \\ sensorless & \(4.1m\pm 1.9\)m & \(52.3\pm 19.1\)s & \(2.8m\pm 1.7\)m & \textless{}1\% & 0\% & 0\% \\ vehicle & \(42.8m\pm 18.9\)m & \(4.7m\pm 1.6m\) & \(38.9m\pm 19.7m\) & \textless{}1\% & 15.6\% & \textless{}1\% \\ webspam & \(12.9m\pm 2.5\)m & \(3.3m\pm 27.4\)s & \(10.6m\pm 2.1\)m & \textless{}1\% & 11.0\% & \textless{}1\% \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{4}{c}{run times} & \multicolumn{3}{c}{\% timeouts} \\ \cline{2-7}  & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(15.98\pm 1.4\)s & \(5.88\pm 0.2\)s & \(7.78\pm 1.9\)s & 0\% & 0\% & 0\% \\ fmnist & \(32.2m\pm 8.4\)m & \(53.48\pm 4.4\)s & \(1.6m\pm 23.2\)s & \textless{}1\% & \textless{}1\% & \textless{}1\% \\ higgs & \(4.11\pm 1.7\)h & \(11.6m\pm 8.7\)m & \(31.5m\pm 13.0\)m & \(1.1\)\% & 4.0\% & \textless{}1\% \\ miniboone & \(2.6m\pm 3.7\)m & \(6.18\pm 2.9\)s & \(9.78\pm 6.5\)s & 0\% & 0\% & 0\% \\ mnist & \(3.1m\pm 59.0\)s & \(1.8m\pm 25.4\)s & \(2.5m\pm 54.8\)s & \textless{}1\% & 4.1\% & \textless{}1\% \\ prostate & \(33.9m\pm 2.1\)m & \(4.2m\pm 36.4\)s & \(15.7m\pm 1.2\)m & \textless{}1\% & 12.3\% & \textless{}1\% \\ roadsafety & \(1.0m\pm 16.8\)s & \(1.14\pm 2.1\)s & \(4.68\pm 18.5\)s & 0\% & 0\% & 0\% \\ sensorless & \(3.4m\pm 4.2\)m & \(8.68\pm 3.6\)s & \(1.9m\pm 2.7\)m & \textless{}1\% & 0\% & 0\% \\ vehicle & \(4.0h\pm 1.9\)h & \(8.2m\pm 3.7\)m & \(2.1\pm 50.4\)m & \(1.2\%\) & 10.7\% & \textless{}1\% \\ webspam & \(9.5m\pm 51.9\)s & \(2.2m\pm 1.1\)m & \(4.6m\pm 1.3\)m & \textless{}1\% & 6.6\% & \textless{}1\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: Average run times (with standard deviations) when attempting to generate 10 000 adversarial examples using _veritas_ on an XGBoost/random forest/GROOT forest ensemble for all three approaches: _full_, _pruned_ and _mixed_. The average fraction of timeouts incurred during the search is also reported.

setting. Moreover, a larger confidence \(1-\eta\) can increase the number of selected features as it shrinks the confidence interval for the empirical FNR.

### Sensitivity to Timeouts

Timeouts always need to be explicitly handled, due to the hardness of the evasion problem [23]. While Tables 7 and 8 show that in most of our experiments timeouts are rare or totally absent, to complete the discussion we perform a sensitivity analysis on the value used for the _pruned_ setting timeout. We vary \(t_{\max}^{prun}\in[0.001,0.01,0.1,1]\) seconds when attempting to generate \(10\,000\) adversarial examples on the _fmnist_ dataset. We only consider _veritas_ for RF in this experiment.

Table 10 shows the fraction of timeouts for the _pruned_ setting as well as the _pruned_ speedup and the _mixed_ speedup for each value of \(t_{\max}^{prun}\). The smaller \(t_{\max}^{prun}\), the larger the number _pruned_ timeouts,

\begin{table}
\begin{tabular}{c c c} \hline \hline \(\tau\) & \(1-\eta\) & _mixed_ speedup \\ \hline  & 0.8 & 3.7x \\
0.05 & 0.9 & 3.7x \\  & 0.95 & 3.7x \\ \hline  & 0.8 & 6.3x \\
0.10 & 0.9 & 6.3x \\  & 0.95 & 6.3x \\ \hline  & 0.8 & 6.3x \\
0.25 & 0.9 & 6.3x \\  & 0.95 & 6.3x \\ \hline  & 0.8 & 1.9x \\
0.50 & 0.9 & 1.9x \\  & 0.95 & 6.3x \\ \hline \hline \end{tabular}
\end{table}
Table 9: Speedup of the _mixed_ setting when attempting to generate \(10\,000\) adversarial examples for _miniboone_ using (_veritas_, XGBoost), for different values of \(\tau\) (threshold on the allowed false negative rate) and \(1-\eta\) (confidence of the statistical test).

Figure 9: Speedups introduced by the _pruned_ setting when attempting to generate \(10\,000\) test examples using _kantchelian_ (top) and _veritas_ (bottom) on an XGB ensemble, varying the empirical false negative rate. Higher FNRs correspond to smaller feature subsets. The dotted horizontal line corresponds to a speedup of 1x, i.e., same run time of the _full_ setting.

which corresponds to a faster (but less accurate) _pruned_ search. If the timeout value is too low, this adversely affects _mixed_ search because it leads to more calls to the _full_ procedure. Conversely, if \(t_{\max}^{prun}\) is too large, the search becomes slower as the _pruned_ setting starts losing time on a few slow instances.

Thus the ideal \(t_{\max}^{prun}\) lays in between the two extremes. In the presented case, \(t_{\max}^{prun}=0.01\) works best. The best choice likely depends on the specific dataset, model, and attack type. However, tuning its value is time consuming (i.e., negates the benefits of the proposed approach).

## Appendix E Quality of Generated Adversarial Examples

We extend Figure 4 by further discussing the quality of generated adversarial examples, providing more examples, and looking in detail at their distance with respect to the base examples.

Figure 10 shows a large set of adversarial examples generated for _mnist_ digits using _kantchelian_ and _veritas_ on an XGBoost ensemble. For each attack, we plot the base example \(x\) and the two adversarial examples generated with the _full_ and the _pruned_ setting.

### Empirical Robustness

Tables 11 (_kantchelian_) and 12 (_veritas_) show the average empirical robustness in all the performed experiments for the _full_, _pruned_ and _mixed_ settings. An ensemble's _empirical robustness_ is defined as the average distance to the nearest adversarial example for each \(x\) in a test set. We use adversarial examples generated with the experiments presented in Q1 in Section 4 (and Appendix C).

The objective of the _kantchelian_ attack is to find the closest adversarial example. Given that the method is exact, the _full_ setting returns the optimal solution. The _pruned_ search works with a restricted feature set, thus it might not be able to find the closest adversarial example, if that requires altering features not included in the selected feature subset. As a consequence, the _empirical robustness_ values for the _pruned_ and _mixed_ search are overestimates of the true value given by the _full_ setting.

Unlike _kantchelian_, _veritas_ does not try to find the closest adversarial example. Instead, it maximizes the confidence that the ensemble assigns to the incorrect label. In this case, there is little difference in the empirical robustness values among all considered settings, with the _pruned_ and _mixed_ settings typically managing to even lower the distance to the base example.

### Change in Predicted Probability for Adversarial Examples

_veritas_ tries to generate an adversarial example such that the ensemble assigns as high a probability as possible to the incorrect label. Hence, a natural empirical measure for the quality of the generated examples is to compare the difference in the ensembles probabilistic predictions for the adversarial examples generated by each approach. Namely, we compute \(\bm{T}(\tilde{x})\) - \(\bm{T}(\tilde{x}^{\prime})\) where \(\tilde{x}\) is generated by the full search, \(\tilde{x}^{\prime}\) is generated by the _pruned_ (_mixed_) search, and (in an abuse of notation) \(\bm{T}(x)\) returns the probability an example belongs to most likely class.

Table 13 shows the average differences in predicted probability between _full_ and _pruned/mixed_ adversarial examples.

Using _kantchelian_, adversarial examples generated with our approaches are assigned very similar probabilities to those generated with the _full_ search. In _veritas_, differences are typically higher, as the model output is directly optimized.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(t_{\max}^{prun}\) (s) & \% _pruned_ timeouts & _pruned_ speedup & _mixed_ speedup \\ \hline \(0.001\) & \(24\)\% & \(7.9\times\) & \(2.8\times\) \\ \(0.01\) & \(12\)\% & \(5.8\times\) & \(3.2\times\) \\ \(0.1\) & \(4\)\% & \(2.8\times\) & \(2.3\times\) \\ \(1\) & \(1\)\% & \(1.1\times\) & \(1.1\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Fraction of _pruned_ timeouts and speedup of the _pruned_ and _mixed_ settings when attempting to generate \(10\,000\) adversarial examples for _fmnist_ using (_veritas_, random forest), for different values of the _pruned_ setting timeout \(t_{\max}^{prun}\) (in seconds).

## Appendix F Expanded Related Work

Adversarial examples have been theoretically studied and defined in multiple different ways [14; 18]. More specifically, Ilyas et al. showed how certain features in a dataset might be fragile and thus naturally lead to adversarial examples [21]. Approaches to reason about learned tree ensembles have received substantial interest in recent years. These include algorithms for performing evasion attacks [23; 15] (i.e., generate adversarial examples), perform robustness checking [8], and verify that the ensembles satisfy certain criteria [11; 10; 26; 29]. Kantchelian et al. [23] were the first to show that, just like neural networks, tree ensembles are susceptible to evasion attacks. Their MILP formulation is still the most frequently used method to check robustness and generate adversarial examples. Other notable methods for adversarial example generation are SMT-based systems [15; 11]. These approaches propose varying ways to encode a tree ensemble in a set of logical formulas using the primitives from Satisfiability Modulo Theories (SMT). While the formulation of an ensemble in SMT is very elegant, it tends to perform worse than MILP in practice.

Because MILP and SMT are exact approaches,6 they search for the optimal answer which in certain cases can be difficult (i.e., time consuming) to find. Often an approximate answer will be sufficient and several approximate methods have been proposed that are specifically tailored to tree ensembles. Chen et al. proposed a \(K\)-partite graph representation in which a max-clique corresponds to a specific output of the ensemble [8; 35]. They introduced a fast method to approximately evaluate robustness,

\begin{table}
\begin{tabular}{l c c c} \multicolumn{4}{c}{**Kantchelian, XGBoost**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.018\pm 0.0\) & \(0.038\pm 0.0\) & \(0.038\pm 0.0\) \\ fmnist & \(0.034\pm 0.002\) & \(0.056\pm 0.012\) & \(0.056\pm 0.012\) \\ higgs & \(0.011\pm 0.0\) & \(0.015\pm 0.002\) & \(0.016\pm 0.002\) \\ miniboone & \(0.001\pm 0.0\) & \(0.001\pm 0.0\) & \(0.001\pm 0.0\) \\ mnist & \(0.006\pm 0.001\) & \(0.021\pm 0.007\) & \(0.02\pm 0.007\) \\ prostate & \(0.02\pm 0.0\) & \(0.037\pm 0.001\) & \(0.038\pm 0.001\) \\ roadsferd & \(0.005\pm 0.0\) & \(0.014\pm 0.003\) & \(0.015\pm 0.003\) \\ sensoreless & \(0.006\pm 0.001\) & \(0.008\pm 0.001\) & \(0.009\pm 0.0\) \\ vehicle & \(0.017\pm 0.001\) & \(0.045\pm 0.013\) & \(0.042\pm 0.011\) \\ webspam & \(0.002\pm 0.0\) & \(0.006\pm 0.002\) & \(0.006\pm 0.002\) \\ \hline \hline \end{tabular} \begin{tabular}{l c c} \multicolumn{4}{c}{**Kantchelian, RF**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.102\pm 0.003\) & \(0.113\pm 0.002\) & \(0.117\pm 0.001\) \\ fmnist & \(0.02\pm 0.002\) & \(0.046\pm 0.013\) & \(0.045\pm 0.012\) \\ higgs & \(0.016\pm 0.0\) & \(0.016\pm 0.001\) & \(0.019\pm 0.0\) \\ miniboone & \(0.001\pm 0.0\) & \(0.001\pm 0.0\) & \(0.001\pm 0.0\) \\ mnist & \(0.006\pm 0.001\) & \(0.023\pm 0.002\) & \(0.023\pm 0.002\) \\ prostate & \(0.058\pm 0.0\) & \(0.089\pm 0.0\) & \(0.094\pm 0.0\) \\ roadsferd & \(0.022\pm 0.001\) & \(0.025\pm 0.004\) & \(0.028\pm 0.003\) \\ sensoreless & \(0.017\pm 0.002\) & \(0.032\pm 0.001\) & \(0.032\pm 0.001\) \\ vehicle & \(0.015\pm 0.001\) & \(0.033\pm 0.006\) & \(0.033\pm 0.006\) \\ webspam & \(0.003\pm 0.0\) & \(0.006\pm 0.001\) & \(0.006\pm 0.001\) \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c} \multicolumn{4}{c}{**Kantchelian, GROOT**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.124\pm 0.001\) & \(0.136\pm 0.002\) & \(0.139\pm 0.002\) \\ fmnist & \(0.295\pm 0.001\) & \(0.309\pm 0.002\) & \(0.309\pm 0.002\) \\ higgs & \(0.109\pm 0.008\) & \(0.122\pm 0.015\) & \(0.127\pm 0.01\) \\ miniboone & \(0.025\pm 0.0\) & \(0.048\pm 0.014\) & \(0.048\pm 0.014\) \\ mnist & \(0.277\pm 0.001\) & \(0.296\pm 0.001\) & \(0.298\pm 0.0\) \\ prostate & \(0.063\pm 0.0\) & \(0.091\pm 0.003\) & \(0.096\pm 0.003\) \\ roadsferd & \(0.092\pm 0.006\) & \(0.075\pm 0.011\) & \(0.092\pm 0.007\) \\ sensoreless & \(0.051\pm 0.003\) & \(0.077\pm 0.013\) & \(0.079\pm 0.013\) \\ vehicle & \(0.149\pm 0.001\) & \(0.165\pm 0.003\) & \(0.168\pm 0.002\) \\ webspam & \(0.034\pm 0.001\) & \(0.052\pm 0.002\) & \(0.052\pm 0.002\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Average empirical robustness (i.e., distance to the closest adversarial example) for the _full_, _mixed_ and _pruned_ methods using _kantchelian_ attack on XGBoost/random forest/GROOT forest ensembles.

but it cannot generate concrete adversarial examples. Devos et al. further improved upon this work by proposing a heuristic search procedure in this graph which is capable of finding concrete adversarial examples very effectively [10]. Zhang et al. propose a method based on a greedy discrete search through the space of leaves specifically optimized for fast adversarial example generation [36].

Other work focuses on making tree ensembles more robust. There are multiple approaches: adding generated adversarial examples to the training data (model hardening) [23], modifying the splitting procedure [7, 4, 32], using the framework of optimal decision trees to encode robustness constraints [33], relabeling and pruning the leaves of the trees [34], simplifying the base learner [1] and using a robust 0/1 loss [19]. Gaining further insights into how evasion attacks target tree ensembles, like those contained in this paper, may inspire novel ways to improve the robustness of learners.

Another line of work aims at directly training tree ensembles that admit verification in polynomial time [5, 13]. However, a drawback to current approaches is that they result in (large) decreases in predictive performance.

Finally, performing evasion attacks has been studied for other model classes with deep neural networks receiving particular attention [28, 17, 24, 6]. However, state-of-the-art algorithms are tailored to one specific model type as they typically exploit specific properties of the model, e.g., the work on tree ensembles often exploits the logical structure of a decision tree.

\begin{table}
\begin{tabular}{l c c c} \multicolumn{4}{c}{**Veritas, McGloost**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.094\pm 0.0\) & \(0.088\pm 0.001\) & \(0.088\pm 0.001\) \\ frmist & \(0.287\pm 0.002\) & \(0.275\pm 0.004\) & \(0.276\pm 0.004\) \\ higgs & \(0.071\pm 0.0\) & \(0.061\pm 0.003\) & \(0.061\pm 0.003\) \\ miniboone & \(0.06\pm 0.003\) & \(0.026\pm 0.013\) & \(0.029\pm 0.012\) \\ mnist & \(0.289\pm 0.003\) & \(0.253\pm 0.016\) & \(0.257\pm 0.013\) \\ prostate & \(0.097\pm 0.0\) & \(0.095\pm 0.0\) & \(0.095\pm 0.0\) \\ roadsfed & \(0.057\pm 0.0\) & \(0.055\pm 0.0\) & \(0.066\pm 0.0\) \\ sensorless & \(0.055\pm 0.0\) & \(0.047\pm 0.006\) & \(0.049\pm 0.004\) \\ vehicle & \(0.14\pm 0.0\) & \(0.133\pm 0.003\) & \(0.134\pm 0.002\) \\ webspam & \(0.038\pm 0.0\) & \(0.035\pm 0.001\) & \(0.035\pm 0.001\) \\ \hline \hline \multicolumn{4}{c}{**Veritas, RF**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.272\pm 0.001\) & \(0.249\pm 0.006\) & \(0.251\pm 0.006\) \\ frmist & \(0.293\pm 0.0\) & \(0.277\pm 0.004\) & \(0.278\pm 0.004\) \\ higgs & \(0.07\pm 0.001\) & \(0.065\pm 0.002\) & \(0.066\pm 0.002\) \\ miniboone & \(0.061\pm 0.002\) & \(0.038\pm 0.01\) & \(0.04\pm 0.01\) \\ mnist & \(0.289\pm 0.002\) & \(0.275\pm 0.004\) & \(0.276\pm 0.004\) \\ prostate & \(0.194\pm 0.0\) & \(0.186\pm 0.002\) & \(0.188\pm 0.002\) \\ roadsfed & \(0.11\pm 0.001\) & \(0.094\pm 0.006\) & \(0.097\pm 0.005\) \\ sensorless & \(0.113\pm 0.0\) & \(0.101\pm 0.004\) & \(0.103\pm 0.003\) \\ vehicle & \(0.138\pm 0.0\) & \(0.129\pm 0.004\) & \(0.134\pm 0.003\) \\ webspam & \(0.058\pm 0.0\) & \(0.054\pm 0.001\) & \(0.055\pm 0.001\) \\ \hline \hline \multicolumn{4}{c}{**Veritas, GROOT**} \\ \hline \hline  & _full_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.359\pm 0.002\) & \(0.329\pm 0.006\) & \(0.331\pm 0.006\) \\ frmist & \(0.394\pm 0.0\) & \(0.382\pm 0.003\) & \(0.383\pm 0.003\) \\ higgs & \(0.37\pm 0.0\) & \(0.034\pm 0.008\) & \(0.357\pm 0.007\) \\ miniboone & \(0.459\pm 0.0\) & \(0.042\pm 0.049\) & \(0.233\pm 0.051\) \\ mnist & \(0.397\pm 0.0\) & \(0.389\pm 0.003\) & \(0.39\pm 0.003\) \\ prostate & \(0.196\pm 0.0\) & \(0.19\pm 0.001\) & \(0.192\pm 0.0\) \\ roadsfed & \(0.191\pm 0.0\) & \(0.187\pm 0.001\) & \(0.188\pm 0.001\) \\ sensorless & \(0.189\pm 0.0\) & \(0.166\pm 0.009\) & \(0.17\pm 0.007\) \\ vehicle & \(0.392\pm 0.0\) & \(0.378\pm 0.01\) & \(0.382\pm 0.007\) \\ webspam & \(0.098\pm 0.0\) & \(0.096\pm 0.001\) & \(0.097\pm 0.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Average empirical robustness (i.e., distance to the closest adversarial example) for the _full_, _mixed_ and _pruned_ methods using _veritas_ attack on XGBoost/random forest/GROOT forest ensembles.

\begin{table}
\begin{tabular}{l c c c c c c} \multicolumn{6}{c}{**Kantchelian**} \\ \hline \hline  & \multicolumn{2}{c}{**XGBoost**} & \multicolumn{2}{c}{**RF**} & \multicolumn{2}{c}{**GROOT**} \\ \cline{2-7}  & _pruned_ & _mixed_ & _pruned_ & _mixed_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.093\pm 0.001\) & \(0.082\pm 0.002\) & \(0.01\pm 0.002\) & \(0.009\pm 0.002\) & \(0.011\pm 0.001\) & \(0.011\pm 0.001\) \\ fmnist & \(0.023\pm 0.004\) & \(0.021\pm 0.003\) & \(0.012\pm 0.001\) & \(0.011\pm 0.0\) & \(0.019\pm 0.002\) & \(0.018\pm 0.002\) \\ higgs & \(0.012\pm 0.001\) & \(0.01\pm 0.001\) & \(0.005\pm 0.0\) & \(0.003\pm 0.0\) & \(0.007\pm 0.002\) & \(0.005\pm 0.003\) \\ miniboone & \(0.013\pm 0.001\) & \(0.011\pm 0.001\) & \(0.006\pm 0.0\) & \(0.006\pm 0.0\) & \(0.005\pm 0.0\) & \(0.005\pm 0.0\) \\ mnist & \(0.109\pm 0.011\) & \(0.103\pm 0.011\) & \(0.02\pm 0.002\) & \(0.019\pm 0.002\) & \(0.018\pm 0.004\) & \(0.016\pm 0.004\) \\ prostate & \(0.026\pm 0.0\) & \(0.023\pm 0.0\) & \(0.004\pm 0.0\) & \(0.003\pm 0.0\) & \(0.003\pm 0.0\) & \(0.003\pm 0.0\) \\ roadsafety & \(0.135\pm 0.025\) & \(0.115\pm 0.019\) & \(0.013\pm 0.002\) & \(0.011\pm 0.001\) & \(0.033\pm 0.016\) & \(0.027\pm 0.014\) \\ sensorless & \(0.045\pm 0.001\) & \(0.041\pm 0.002\) & \(0.008\pm 0.0\) & \(0.007\pm 0.0\) & \(0.009\pm 0.001\) & \(0.008\pm 0.001\) \\ vehicle & \(0.015\pm 0.004\) & \(0.013\pm 0.003\) & \(0.005\pm 0.0\) & \(0.005\pm 0.0\) & \(0.005\pm 0.001\) & \(0.004\pm 0.001\) \\ webspam & \(0.049\pm 0.006\) & \(0.044\pm 0.004\) & \(0.007\pm 0.0\) & \(0.006\pm 0.0\) & \(0.006\pm 0.0\) & \(0.005\pm 0.0\) \\ \hline \hline \multicolumn{6}{c}{**Veritas**} \\ \hline \hline  & \multicolumn{2}{c}{**XGBoost**} & \multicolumn{2}{c}{**RF**} & \multicolumn{2}{c}{**GROOT**} \\ \cline{2-7}  & _pruned_ & _mixed_ & _pruned_ & _mixed_ & _pruned_ & _mixed_ \\ \hline covtype & \(0.129\pm 0.02\) & \(0.112\pm 0.012\) & \(0.069\pm 0.007\) & \(0.065\pm 0.006\) & \(0.054\pm 0.005\) & \(0.051\pm 0.005\) \\ fmnist & \(0.228\pm 0.065\) & \(0.213\pm 0.056\) & \(0.373\pm 0.011\) & \(0.327\pm 0.009\) & \(0.379\pm 0.009\) & \(0.367\pm 0.01\) \\ higgs & \(0.071\pm 0.006\) & \(0.067\pm 0.005\) & \(0.062\pm 0.009\) & \(0.054\pm 0.007\) & \(0.057\pm 0.01\) & \(0.051\pm 0.009\) \\ miniboone & \(0.246\pm 0.042\) & \(0.231\pm 0.036\) & \(0.178\pm 0.027\) & \(0.164\pm 0.028\) & \(0.104\pm 0.022\) & \(0.1\pm 0.019\) \\ mnist & \(0.196\pm 0.026\) & \(0.179\pm 0.022\) & \(0.294\pm 0.012\) & \(0.275\pm 0.009\) & \(0.241\pm 0.022\) & \(0.21\pm 0.021\) \\ prostate & \(0.237\pm 0.01\) & \(0.218\pm 0.009\) & \(0.225\pm 0.024\) & \(0.195\pm 0.016\) & \(0.214\pm 0.006\) & \(0.169\pm 0.006\) \\ roadsafety & \(0.17\pm 0.04\) & \(0.146\pm 0.031\) & \(0.083\pm 0.009\) & \(0.071\pm 0.009\) & \(0.059\pm 0.006\) & \(0.048\pm 0.003\) \\ sensorless & \(0.142\pm 0.053\) & \(0.116\pm 0.039\) & \(0.143\pm 0.021\) & \(0.122\pm 0.013\) & \(0.13\pm 0.012\) & \(0.113\pm 0.01\) \\ vehicle & \(0.21\pm 0.026\) & \(0.175\pm 0.017\) & \(0.207\pm 0.048\) & \(0.135\pm 0.036\) & \(0.087\pm 0.024\) & \(0.065\pm 0.018\) \\ webspam & \(0.274\pm 0.008\) & \(0.244\pm 0.009\) & \(0.267\pm 0.013\) & \(0.226\pm 0.01\) & \(0.276\pm 0.023\) & \(0.231\pm 0.016\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Average difference in predicted probability between an adversarial example generated using _kantchelian/veritas_ with the _full_ setting and an adversarial example generated with the _pruned/mixed_ setting, for the same base example. All adversarial examples are those generated during the experiments from Section 4 and Appendix C.

Figure 10: Adversarial examples generated for _mnist_ with both attacks (_kantchelian_ and _veritas_) on an XGBoost ensemble, to show the quality of generated examples.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction we claim that we can identify a subset of relevant features to perform faster repeated evasion attacks by only perturbing those features. Q1 in Section 4 supports our claim. We provide further evidence in Appendix C. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the proposed methods are discussed in the paper. In Sections 1 and 3 we clearly state that our approach speeds up adversarial example generation in scenarios where a lot of repeated evasion attacks need to be performed. We discuss the specific scenarios where this is the case in Sections 1, 3 and in Appendix A. At the beginning of Section 4, we also point out that our approach works best on high-dimensional datasets. We summarize these limitations in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Theoretical results are only present in Section 3, which also reports the needed references and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In the paper we clearly describe all the adopted tree ensemble types (XGB, RF, GROOT, Section 2), evasion attacks (_kantchelian_ and _veritas_, Section 2), and datasets (Section 4 and Appendix B). The method is described in Section 3 with the aid of Algorithms 1 and 2. We then describe our experimental methodology in Section 4. Table 1 reports all datasets characteristics, each tuned model's hyperparameters, and the max perturbation size \(\delta\) for each attack. Finally, Table 3 points toward the publicly available sources we gathered the datasets from. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide full access to the code that implements our method and reproduces all the experiments from the paper at https://github.com/lorezocascioli/faster-repeated-evasion-tree-ensembles. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We perform 5-fold cross validation as described in Section 4. Appendix B reports the full grid employed to tune all our ensembles through grid search. Plus, it discusses the choice of the employed evasion attacks. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Given that our key performance metric is run time, we always report standard deviations on run times and related metrics (e.g., when we discuss speed ups in Sections 1 and 4). Tables 7 and 8 in the supplement report run time standard deviations for all the performed experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The compute worker characteristics and memory are specified in Section 4. Run times are the key performance metric, hence they are thoroughly discussed in Section 4 and Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We are aware of the code of ethics and do not violate any clause. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We included an impact statement in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Only the authors contributed to this paper. The adopted datasets (Table 3), models and attack methods (Section 2) are all publicly available and properly referenced. The license has been selected for submission in OpenReview (CC BY 4.0). Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code implementing our new method (described in Section 3) as well as all the presented experiments are publicly distributed at https://github.com/lorezocascioli/faster-repeated-evasion-tree-ensembles. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.