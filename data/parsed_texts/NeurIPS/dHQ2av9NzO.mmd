# On the Convergence of

Black-Box Variational Inference

Kyurae Kim

University of Pennsylvania

kyrkim@seas.upenn.edu

&Jisu Oh

North Carolina State University

joh26@ncsu.edu

&Kaiwen Wu

University of Pennsylvania

kaiwenwu@seas.upenn.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

&Jacob R. Gardner

University of Pennsylvania

jacobrg@seas.upenn.edu

###### Abstract

We provide the first convergence guarantee for black-box variational inference (BBVI) with the reparameterization gradient. While preliminary investigations worked on simplified versions of BBVI (_e.g._, bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Notably, our analysis reveals that certain algorithm design choices commonly employed in practice, such as nonlinear parameterizations of the scale matrix, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations and thus achieves the strongest known convergence guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale Bayesian inference problems.

## 1 Introduction

Despite the practical success of black-box variational inference (BBVI; Kucukelbir _et al._, 2017; Ranganath _et al._, 2014; Titsias & Lazaro-Gredilla, 2014), also known as stochastic gradient variational Bayes and Monte Carlo variational inference, whether it converges under appropriate assumptions on the target problem have been an open problem for a decade. While our understanding of BBVI has been advancing (Bhatia _et al._, 2022; Challis & Barber, 2013; Domke, 2019, 2020; Hoffman & Ma, 2020), a full convergence guarantee that extends to the practical implementations as used in probabilistic programming languages (PPL) such as Stan (Carpenter _et al._, 2017), Turing (Ge _et al._, 2018), Tensorflow Probability (Dillon _et al._, 2017), Pyro (Bingham _et al._, 2019), and PyMC (Patil _et al._, 2010) has yet to be demonstrated.

Due to our lack of understanding, a consensus on how we should implement our BBVI algorithms has yet to be achieved. For example, when the variational family is chosen to be the location-scale family, the "scale" matrix can be parameterized linearly or nonlinearly, and both parameterizations are used by default in popular software packages. (See Table 1 in Kim _et al._2023.) Surprisingly, as we will show, seemingly innocuous design choices like these can substantially impact the convergence of BBVI. This is critical as BBVI has been shown to be less robust (_e.g._, sensitive to initial points, stepsizes, and such) than competing inference methods such as Markov chain Monte Carlo (MCMC). (See Dhaka _et al._, 2020; Domke, 2020; Welandawe _et al._, 2022; Yao _et al._, 2018.) Instead, the evaluation of BBVI algorithms has been relying on expensive empirical evaluations (Agrawal _et al._, 2020; Dhaka _et al._, 2021; Giordano _et al._, 2018; Yao _et al._, 2018).

To rigorously analyze the design of BBVI algorithms, we establish the first convergence guarantee for the implementations _precisely_ as used in practice. We provide results for BBVI with the reparameterization gradient (RP; Kingma and Welling, 2014; Titsias and Lazaro-Gredilla, 2014) and the location-scale variational family, arguably the most widely used combination in practice. Our results apply to log-smooth posteriors, which is a routine assumption for analyzing the convergence of stochastic optimization (Garrigos and Gower, 2023) and sampling algorithms (Dwivedi et al., 2019, SS2.3). The key is to show that evidence lower bound (ELBO; Jordan et al., 1999) satisfies regularity conditions required by convergence proofs of stochastic gradient descent (SGD; Bottou, 1999; Nemirovski et al., 2009; Robbins and Monro, 1951), the workhorse underlying BBVI.

Our analysis reveals that nonlinear scale matrix parameterizations used in practice are suboptimal: they provably break strong convexity and sometimes even convexity. Even if the posterior is strongly log-concave, the ELBO is not strongly convex anymore. This contrasts with linear parameterizations, which guarantee the ELBO to be strongly convex if the posterior is strongly log-concave (Domke, 2020). Under linear parameterizations, however, the ELBO is no longer smooth, making optimization challenging. Because of this, Domke (2020) proposed to use proximal SGD, which Agrawal and Domke (2021, Appendix A) report to have better performance than vanilla SGD with nonlinear parameterizations. Indeed, we show that BBVI with proximal SGD achieves the _fastest_ known converges rates of SGD, unlike vanilla BBVI. Thus, we provide a concrete reason for employing proximal SGD. We evaluate this insight on large-scale Bayesian inference problems by implementing an Adam-like (Kingma and Ba, 2015) variant of proximal SGD proposed by Yun et al. (2021).

Concurrently to this work, convergence guarantees on BBVI with the RP and the sticking-the-landing estimator (STL; Roeder et al., 2017) under the linear parameterization were published by Domke et al. (2023). To achieve this, they show that a quadratic bound on the gradient variance is sufficient to guarantee the convergence of projected and proximal SGD. In contrast, we focus on analyzing the ELBO under nonlinear parameterizations and connect it to existing analysis strategies. A more in-depth comparison of the two works is provided in Appendix E.

* **Convergence Guarantee for BBVI:** Theorem 3 establishes a convergence guarantee for BBVI with assumptions matching the implementations used in practice. That is, without algorithmic simplifications and unrealistic assumptions such as bounded domain or bounded support.
* **Optimality of Linear Parameterizations:** Theorem 2 shows that, for location-scale variational families, nonlinear scale parameterizations prevent the ELBO from being strongly-convex even when the target posterior is strongly log-concave.
* **Convergence Guarantee for Proximal BBVI:** Theorem 4 guarantees that, if proximal SGD is used, BBVI on \(\mu\)-strongly log-concave posteriors can obtain a solution \(\epsilon\)-close to the global optimum with \(\mathcal{O}\left(1/\epsilon\right)\) iterations.
* **Evaluation of Proximal BBVI in Practice:** In Section 5, we evaluate the utility of proximal SGD on large-scale Bayesian inference problems.

## 2 Background

NotationRandom variables are denoted in serif (_e.g._, \(x\), \(x\)), vectors are in bold (_e.g._, \(x\), \(x\)), and matrices are in bold capitals (_e.g._\(A\)). For a vector \(x\in\mathbb{R}^{d}\), we denote the inner product as \(x^{\mathsf{T}}x\) and \(\left\langle x,x\right\rangle\), the \(\ell_{2}\)-norm as \(\left\|x\right\|_{2}=\sqrt{x^{\mathsf{T}}x}\). For a matrix \(A\), \(\left\|A\right\|_{\mathrm{F}}=\sqrt{\operatorname{tr}\left(A^{\mathsf{T}}A \right)}\) denotes the Frobenius norm. \(\mathbb{S}^{d}_{++}\) is the set of positive definite matrices. For some function \(f\), \(\mathrm{D}_{\mathrm{i}}f\) denotes the \(i\)th coordinate of \(\nabla f\), and \(\mathrm{C}^{k}\left(\mathcal{X},\mathcal{Y}\right)\) is the set of \(k\)-time differentiable continuous functions mapping from \(\mathcal{X}\) to \(\mathcal{Y}\).

### Black-Box Variational Inference

Variational inference (VI, Blei et al., 2017; Jordan et al., 1999; Zhang et al., 2019) aims to minimize the exclusive (or backward/reverse) Kullback-Leibler (KL) divergence as:

\[\begin{array}{l}\operatorname*{minimize}_{\lambda\in\Lambda}\;\;\mathrm{D}_{ \mathrm{KL}}\left(q_{\lambda},\pi\right)\triangleq\mathbb{E}_{x\sim q_{\lambda }}-\log\pi\left(z\right)-\mathbb{H}\left(q_{\lambda}\right),\\ \text{where}\;\;\;\mathrm{D}_{\mathrm{KL}}\left(q_{\lambda},\pi\right)\;\; \text{is the KL divergence,}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\Equivalently, one minimizes the negative _evidence lower bound_(ELBO, Jordan _et al._, 1999):

\[\underset{\mathbf{\lambda}\in\mathbf{\Lambda}}{\text{minimize}}\;\;F(\mathbf{\lambda})\triangleq \mathbb{E}_{\mathbf{z}\sim q_{\mathbf{\lambda}}}-\log p\left(\mathbf{z},\mathbf{x}\right)-\mathbb{ H}\left(q_{\mathbf{\lambda}}\right),\]

where \(\log p\left(\mathbf{z},\mathbf{x}\right)\) is the _joint likelihood_, which is proportional to the posterior as \(\pi\left(\mathbf{z}\right)\propto p\left(\mathbf{z},\mathbf{x}\right)=p\left(\mathbf{x}\mid\bm {z}\right)p\left(\mathbf{z}\right)\), where \(p\left(\mathbf{x}\mid\mathbf{z}\right)\) is the likelihood and \(p\left(\mathbf{z}\right)\) is the prior.

### Variational Family

In this work, we focus on the following variational family. (\(\overset{\text{d}}{=}\) is equivalence in distribution.)

**Definition 1** (**Reparameterized Family**).: Let \(\varphi\) be some \(d\)-variate distribution. Then, \(q_{\mathbf{\lambda}}\) that can be equivalently represented as

\[\mathbf{z}\sim q_{\mathbf{\lambda}}\quad\Leftrightarrow\quad\mathbf{z}\overset{\text{d}}{ =}\mathcal{J}_{\mathbf{\lambda}}\left(\mathbf{u}\right);\quad\mathbf{u}\sim\varphi,\]

is said to be part of a reparameterized family generated by the base distribution \(\varphi\) and the reparameterization function \(\mathcal{J}_{\mathbf{\lambda}}^{\text{d}}\).

The location-scale family enables detailed theoretical analysis, as demonstrated by (Domke, 2019, Fujisawa & Sato, 2021, Kim _et al._, 2023), and includes the most widely used variational families such as the Student-t, elliptical, and Gaussian families (Titsias & Lazaro-Gredilla, 2014).

Handling Constrained SupportFor common choices of the base distribution \(\varphi\), the support of \(q_{\mathbf{\lambda}}\) is the whole \(\mathbb{R}^{d}\). Therefore, special treatment is needed when the support of \(\pi\) is constrained. Kucukelbir _et al._ (2017) proposed to handle this by applying diffeomorphic transformation denoted with \(\mathbf{\psi}\), often called _bjectors_(Dillon _et al._, 2017; Fjelde _et al._, 2020; Leger, 2023), to \(q_{\mathbf{\lambda}}\) such that

\[\mathbf{\zeta}\sim q_{\mathbf{\psi},\mathbf{\lambda}}\qquad\Leftrightarrow\quad\mathbf{\zeta} \triangleq\mathbf{\psi}^{-1}(\mathbf{z});\quad\mathbf{z}\sim q_{\mathbf{\lambda}},\]

such that the support of \(q_{\mathbf{\psi},\mathbf{\lambda}}\) matches that of \(\pi\). For example, when the support of \(\pi\) is \(\mathbb{R}_{+}\), one can choose \(\mathbf{\psi}^{-1}=\exp\). This approach, known as automatic differentiation VI (ADVI), is now standard in most modern PPLs.

Why focus on posteriors with unconstrained supports?When bijectors are used, the entropy of \(q_{\mathbf{\lambda}}\), \(\mathbb{H}\left(q_{\mathbf{\lambda}}\right)\), needs to be adjusted by the Jacobian of \(\mathbf{\psi}\)(Kucukelbir _et al._, 2017), \(\mathbf{J}_{\mathbf{\psi}^{-1}}\). However, applying the transformation to \(\pi\) instead of \(q_{\mathbf{\lambda}}\) is mathematically equivalent and more convenient. In fact, bijectors can be automatically incorporated into our notation by implicitly setting

\[p\left(\mathbf{x}\mid\mathbf{z}\right)=\widetilde{p}\left(\mathbf{x}\mid\mathbf{\psi}^{-1} \left(\mathbf{z}\right)\right)\quad\text{and}\quad p\left(\mathbf{z}\right)=\widetilde {p}\left(\mathbf{\psi}^{-1}\left(\mathbf{z}\right)\right)\left|\mathbf{\mathbf{\psi}}^{-1} \left(\mathbf{z}\right)\right|,\]

such that \(\widetilde{\pi}\left(\mathbf{\zeta}\right)\propto\widetilde{p}\left(\mathbf{x}\mid\bm {\zeta}\right)\widetilde{p}\left(\mathbf{\zeta}\right)\), where \(\widetilde{\pi}\) is the constrained posterior that we are actually interested in. Therefore, our setup in Section 2.1, where the domain of \(\mathbf{z}\) is taken to be the unconstrained \(\mathbb{R}^{d}\), already encompasses constrained posteriors through ADVI.

Lastly, we impose light assumptions on the base distribution \(\varphi\), which are already satisfied by most variational families used in practice. (_i.i.d._: independently and identically distributed.)

**Assumption 1** (**Base Distribution**).: \(\varphi\) is a \(d\)-variate distribution such that \(\mathbf{u}\sim\varphi\) and \(\mathbf{u}=\left(u_{1},...,u_{d}\right)\) with _i.i.d._ components. Furthermore, \(\varphi\) is **(i)** symmetric and standardized such that \(\mathbb{E}u_{l}=0\), \(\mathbb{E}u_{1}^{2}=1\), \(\mathbb{E}u_{1}^{3}=0\), and **(ii)** has finite kurtosis \(\mathbb{E}u_{1}^{4}=k_{\varphi}<\infty\).

The assumptions on the variational family we will use throughout this work are collectively summarized in the following assumption:

**Assumption 2**.: The variational family is the location-scale family formed by Definitions 1 and 2 with the base distribution \(\varphi\) satisfying Assumption 1.

### Scale Parameterizations

For the "scale" matrix \(\mathbf{C}\left(\mathbf{\lambda}\right)\) in the location-scale family, any parameterization that results in a positive-definite covariance \(\mathbf{CC}^{\mathsf{T}}\in\mathbb{S}_{++}^{d}\) is valid. However, for the ELBO to ever be convex, the entropy \(\mathbb{H}\left(q_{\mathbf{\lambda}}\right)\) must be convex, which requires the mapping \(\mathbf{\lambda}\mapsto\mathbf{CC}^{\mathsf{T}}\) to be convex. To ensure this, we restrict \(\mathbf{C}\) to (lower) triangular matrices with strictly positive eigenvalues, essentially, Cholesky factors. This leaves two of the most common parameterizations:

**Definition 3** (**Mean-Field Family.**).: \(\mathbf{C=D_{\phi}(s)}\) where the \(d\) elements of \(\mathbf{s}\) forms the diagonal and \(\mathbf{\lambda}\in\Lambda\) such that

\[\Lambda=\{(\mathbf{m},\mathbf{s})\,|\,\mathbf{m}\in\mathbb{R}^{d},\mathbf{s}\in\mathcal{S}\}.\]

Here, \(S\) is discussed in the next paragraph, \(\mathbf{D_{\phi}(s)}\in\mathbb{R}^{d\times d}\) is a diagonal matrix such that \(\mathbf{D_{\phi}(s)}\triangleq\operatorname{diag}\left(\mathbf{\phi(s)}\right)= \operatorname{diag}\left(\mathbf{\phi(s_{1})},\,...\,,\mathbf{\phi(s_{d})}\right)\), and \(\mathbf{\phi}\) is a function we call a _diagonal conditioner_.

Linear v.s. Nonlinear ParameterizationsWhen the diagonal conditioner is a linear function \(\mathbf{\phi}(x)=x\), we say that the covariance parameterization is _linear_. In this case, to ensure that \(\mathbf{C}\) is a Cholesky factor, the domain of \(\mathbf{s}\) is set as \(\mathcal{S}=\mathbb{R}_{+}^{d}\). On the other hand, by choosing a nonlinear conditioner \(\mathbf{\phi}:\,\mathbb{R}\to\mathbb{R}_{+}\), we can make the domain of \(\mathbf{s}\) to be the unconstrained \(\mathcal{S}=\mathbb{R}^{d}\). Because of this, nonlinear conditioners such as the softplus \((x)\triangleq\log\left(1+\exp\left(x\right)\right)\)(Dugas _et al._, 2000) are frequently used in practice, especially for mean-field. (See Table 1 by Kim _et al._, 2023).

### Problem Structure of Black-Box Variational Inference

Exclusive KL minimization VI is fundamentally a composite (regularized) optimization problem

\[F\left(\mathbf{\lambda}\right)=f\left(\mathbf{\lambda}\right)+h\left(\mathbf{\lambda} \right),\] (ELBO)

where \(f(\mathbf{\lambda})\triangleq\mathbb{E}_{\mathbf{z}\sim q_{\mathbf{\lambda}}}\ell(\mathbf{z})\) is the _energy term_, \(\ell(\mathbf{z})\triangleq-\log\,p\left(\mathbf{z},\mathbf{x}\right)\) is the negative joint log-likelihood, and \(h\left(\mathbf{\lambda}\right)\triangleq-\mathbb{H}\left(\mathbf{\lambda}_{\mathbf{\phi}, \mathbf{\lambda}}\right)\) is the _entropic regularizer_. From here, BBVI introduces more structure.

An illustration of the taxonomy is shown in Figure 1. In particular, BBVI has an _infinite sum_ structure (IS). That is, it cannot be represented as a sum of finite subcomponents as in ERM. Furthermore,

\[F\left(\mathbf{\lambda}\right) =\mathbb{E}_{\mathbf{u}\sim\mathbf{\phi}}\,f\left(\mathbf{\lambda};\mathbf{u} \right)+h\left(\mathbf{\lambda}\right)\] (CP \[\cap\] IS) \[=\mathbb{E}_{\mathbf{u}\sim\mathbf{\phi}}\,\ell\left(\mathcal{I}_{\mathbf{ \lambda}}\left(\mathbf{u}\right)\right)+h\left(\mathbf{\lambda}\right),\] (CP \[\cap\] IS \[\cap\] RP)

where \(f\left(\mathbf{\lambda};\mathbf{u}\right)\triangleq\ell\left(\mathcal{I}_{\mathbf{ \lambda}}\left(\mathbf{u}\right)\right)\).

Theoretical ChallengesThe structure of BBVI has multiple challenges that have hindered its theoretical analysis: **(i)** the stochasticity of the Jacobian of \(\mathcal{T}\) and **(ii)** The infinite sum structure.

For Item (i), we can see that in

\[\nabla_{\mathbf{\lambda}}\ell\left(\mathcal{I}_{\mathbf{\lambda}}\left(\mathbf{u}\right) \right)=\frac{\partial\mathcal{I}_{\mathbf{\lambda}}\left(\mathbf{u}\right)}{\partial \mathbf{\lambda}}\nabla\ell\left(\mathcal{I}_{\mathbf{\lambda}}\left(\mathbf{u}\right) \right)=\frac{\partial\mathcal{I}_{\mathbf{\lambda}}\left(\mathbf{u}\right)}{\partial \mathbf{\lambda}}g\left(\mathbf{\lambda};\mathbf{u}\right),\]

where \(g\left(\mathbf{\lambda};\mathbf{u}\right)\triangleq\left(\nabla\ell\circ\mathcal{I}_ {\mathbf{\lambda}}\right)\left(\mathbf{u}\right)\), both the Jacobian of \(\mathcal{I}_{\mathbf{\lambda}}\) and the gradient of the log-likelihood, \(g\), depend on the randomness \(\mathbf{u}\). Effectively decoupling the two is a major challenge to analyzing the properties of the ELBO and its gradient estimators (Domke, 2019, 2020).

For Item (ii), the problem is that recent analyses of SGD (Garrigos & Gower, 2023; Gower _et al._, 2019; Nguyen _et al._, 2018; Vaswani _et al._, 2019) have increasingly been relying on the assumption that \(f\left(\mathbf{\lambda};\mathbf{u}\right)\) is smooth for all \(\mathbf{u}\) such that

\[\|\nabla_{\mathbf{\lambda}}f\left(\mathbf{\lambda};\mathbf{u}\right)-\nabla_{\mathbf{\lambda}}f \left(\mathbf{\lambda}^{\prime};\mathbf{u}\right)\|\leq L\|\mathbf{\lambda}-\mathbf{\lambda}^ {\prime}\|\]

for some \(L<\infty\). This is sensible if the support of \(\mathbf{u}\) is bounded, which is true for the ERM setting but not for the class of infinite sum (IS) problems. Previous works circumvented this issue by assuming **(i)** that the support of \(\mathbf{u}\) is bounded (Fujisawa & Sato, 2021) which implicitly changes the variational family, or **(ii)** that the gradient \(\nabla f\) is bounded by a constant (Buchholz _et al._, 2018; Liu & Owen, 2021) which contradicts strong convexity (Nguyen _et al._, 2018).

Figure 1: **Taxonomy of variational inference**. Within BBVI, this work only considers the reparameterization gradient (BBVI \(\cap\) RP, shown in **dark red**). This leaves out BBVI with the score gradient (BBVI \(\setminus\) RP, shown in **light red**). The set VI \(\cap\) FS includes sparse variational Gaussian processes (Titsias, 2009), while the remaining set VI \(\setminus\) (FS \(\cup\) IS \(\cup\) RP) includes coordinate ascent VI (Blei _et al._, 2017).

The Evidence Lower Bound Under Nonlinear Scale Parameterizations

Under the linear parameterization (\(\phi(x)=x\)), the properties of the ELBO, such as smoothness and convexity, have been previously analyzed by Challis and Barber (2013); Domke (2020); Titsias and Lazaro-Gredilla (2014). We generalize these results to nonlinear conditioners.

### Technical Assumptions

Let \(g_{i}(\lambda;u)\) be the \(i\)th coordinate of \(g(\lambda;u)\) and recall that \(u_{i}\) denote the \(i\)th element of \(u\). Establishing convexity and smoothness of the ELBO under nonlinear parameterizations depends on a pair of necessary and sufficient assumptions. To establish smoothness:

**Assumption 3.** The gradient of \(\mathcal{E}\) under reparameterization, \(g\), satisfies

\[\left|\mathbb{E}g_{i}(\lambda;u)\,u_{i}\phi^{*}(s_{i})\right|\leq L_{s}\]

for every coordinate \(i=1,...\,d\), any \(\lambda\in\Lambda\), and some \(0<L_{s}<\infty\).

Here, \(\phi^{*}\) is the second derivative of \(\phi\). The next one is required to establish convexity:

**Assumption 4.** The gradient of \(\mathcal{E}\) under reparameterization, \(g\), satisfies

\[\mathbb{E}g_{i}(\lambda;u)\,u_{i}\geq 0\]

for every coordinate \(i=1,...\,d\).

Intuitively, these assumption control how much \(\nabla\mathcal{E}\) and \(\mathcal{F}_{\lambda}\)_rotate_ the randomness \(u\). (Notice that the assumptions are closely related to the matrix \(\operatorname{Cov}\left(g(\lambda;u),u\right)\), the covariance between \(g\) and \(u\).) However, the peculiar aspect of these assumptions is that they are not implied by the convexity and smoothness of \(\mathcal{E}\). Especially, Assumption 3 strongly depends on the internals of \(\nabla\mathcal{E}\).

### Smoothness of the Entropy

Under the linear parameterization, Domke (2020) has previously shown that the entropic regularizer term \(h\) is not smooth. This fact immediately implies the ELBO is not smooth. However, certain nonlinear conditioners do result in a smooth regularizer.

**Lemma 1.**_If the diagonal conditioner \(\phi\) is \(L_{h}\)-log-smooth, then the entropic regularizer \(h(\lambda)\) is \(L_{h}\)-smooth._

_Proof._ See the _full proof_ in page 24.

**Example 1.** The following diagonal conditioners result in a smooth entropic regularizer:

1. Let \(\phi(x)=\operatorname{softplus}\left(x\right)\). Then, \(h\) is \(L_{h}\)-smooth with \(L_{h}\approx 0.167096\).
2. Let \(\phi(x)=\exp\left(x\right)\). Then, \(h\) is \(L_{h}\)-smooth for arbitrarily small \(L_{h}\).

This might initially suggest that diagonal conditioners are a promising way of making the ELBO globally smooth. Unfortunately, the properties of the _energy_, \(f\), change unfavorably.

### Smoothness of the Energy

Inapplicability of Existing Proof StrategyPreviously, Domke (2020, Theorem 1) have proven that the energy is smooth when \(\phi\) is linear. The key step was to use Bessel's inequality based on the observation that the partial derivatives of the reparameterization function \(\mathcal{F}\) form unit bases in expectation. That is,

\[\mathbb{E}\left(\frac{\partial\mathcal{F}_{\lambda}(u)}{\partial\lambda_{i}}, \frac{\partial\mathcal{F}_{\lambda}(u)}{\partial\lambda_{j}}\right)=\mathbb{1} _{i=j},\]

where \(\mathbb{1}_{i=j}\) is an indicator function that is \(1\) only when \(i=j\) and \(0\) otherwise.

Unfortunately, when \(\phi\) is nonlinear, the partial derivatives \(\partial\mathcal{F}_{\lambda}(u)\partial\lambda_{i}\) for \(i=1,...\,p\) no longer form unit bases: while they are still orthogonal in expectation, the _lengths_ change nonlinearly depending on \(\lambda\). This leaves Bessel's inequality inapplicable. To circumvent this challenge, we establish a replacement for Bessel's inequality:

**Lemma 2.**_Let \(H\) be a \(n\times n\) symmetric random matrix, where it is bounded as \(\left\|H\right\|_{2}\leq L<\infty\) almost surely. Also, let \(J\) be an \(m\times n\) random matrix such that \(\left\|\mathbb{E}J^{\mathsf{T}}J\right\|_{2}<\infty\). Then,_

\[\left\|\mathbb{E}J^{\mathsf{T}}HJ\right\|_{2}\leq L\left\|\mathbb{E}J^{ \mathsf{T}}J\right\|_{2}.\]

_Proof._ See the _full proof_ in page 24.

**Remark 1.** By assuming that the joint log-likelihood \(\mathcal{E}\) is smooth and twice-differentiable, we retrieve Theorem 1 of Domke (2020) by setting \(J\) to be the Jacobian of \(\mathcal{F}\), and \(H\) to be the Hessian of \(\mathcal{E}\) under reparameterization.

**Remark 2**.: While our reparameterization function's partial derivatives still form orthogonal bases, they need not be; unlike Bessel's inequality, Lemma 2 does not require this. This implies that Lemma 2 is a strategy more general than Bessel's inequality.

Equipped with Lemma 2, we present our main result on smoothness:

**Theorem 1**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and twice differentiable. Then, the following results hold:_

1. _If_ \(\phi\) _is linear, the energy_ \(f\) _is_ \(L_{\ell}\)_-smooth._
2. _If_ \(\phi\) _is 1-Lipschitz, the energy_ \(\ell\) _is_ \((L_{\ell}+L_{\mathrm{s}})\)_-smooth if and only if Assumption_ 3 _holds._

Proof.: See the _full proof_ in page 27.

Combined with Lemma 1, this directly implies that the overall ELBO is smooth.

**Corollary 1** (Smoothness of the ELBO).: _Let \(\ell\) be \(L_{\ell}\)-smooth and Assumption 3 hold. Furthermore, let the diagonal conditioner be 1-Lipschitz continuous, and \(L_{\phi}\)-log-smooth. Then, the ELBO is \((L_{\ell}+L_{\mathrm{s}}+L_{\phi})\)-smooth._

The increase of the smoothness constant implies that we need to use a smaller stepsize to guarantee convergence when using a nonlinear \(\phi\). Furthermore, even on simple \(L\)-smooth examples Assumption 3 may not hold:

**Example 2**.: Let \(\ell\left(\mathbf{z}\right)=(\nicefrac{{1}}{{2}})\,\mathbf{z}^{\mathsf{T}} \mathbf{A}\mathbf{z}\) and the diagonal conditioner be \(\phi\left(x\right)=\operatorname{softplus}\left(x\right)\). Then,

1. if \(\mathbf{A}\) is dense and the variational family is the mean-field family or
2. if \(\mathbf{A}\) is diagonal and the variational family is the Cholesky family,

Assumption 3 holds with \(L_{\mathrm{s}}\approx 0.26034\left(\max_{i=1,\ldots,d}A_{ii}\right)\).
3. if \(\mathbf{A}\) is dense but the Cholesky family is used, Assumption 3 does not hold.

Proof.: See the _full proof_ in page 29.

Example 2 illustrates that establishing the smoothness of the energy becomes non-trivial under nonlinear parameterizations. Even when smoothness does hold, the increased smoothness constant implies that BBVI will be less robust to initialization and stepizes. Furthermore, in the next section, we will show a much more grave problem: nonlinear parameterizations may affect the convergence _rate_.

### Convexity of the Energy

The convexity of the ELBO under linear parameterizations has first been established by Titsias and Lazaro-Gredilla (2014, Proposition 1) and Domke (2020, Theorem 9). In particular, Domke (2020) show that, when \(\phi\) is linear, if \(\ell\) is \(\mu\)-strongly convex, the energy is also \(\mu\)-strongly convex. However, when using a nonlinear \(\phi\) with a co-domain of \(\mathbb{R}_{+}\), which is the whole point of using a nonlinear conditioner, strong convexity of \(\ell\)_never_ transfers to \(f\).

**Theorem 2**.: _Let \(\ell\) be \(\mu\)-strongly convex. Then, we have the following:_

1. _If_ \(\phi\) _is linear, the energy_ \(f\) _is_ \(\mu\)_-strongly convex._
2. _If_ \(\phi\) _is convex, the energy_ \(f\) _is convex if and only if Assumption_ 4 _holds._
3. _If_ \(\phi\) _is such that_ \(\phi\in\mathrm{C}^{1}\left(\mathbb{R},\mathbb{R}_{+}\right)\)_, the energy_ \(f\) _is not strongly convex._

Proof.: See the _full proof_ in page 33.

The following proposition provides some conditions for Assumption 4 to hold or not hold.

**Proposition 1**.: _We have the following:_

1. _If_ \(\ell\) _is convex, then for the mean-field family, Assumption_ 4 _holds._
2. _For the Cholesky family, there exists a convex_ \(\ell\) _where Assumption_ 4 _does not hold._

Proof.: See the _full proof_ in page 31.

Figure 2: **Optimization landscape resulting from different \(\phi\) on a strongly-convex \(\ell\). \(\ell\) is the counter-example of Proposition 1 Item (ii). \(\phi(x)=x\) preserves strong convexity as shown by the lower-bounding quadratic (red dotted line \(\cdots\)). \(\phi=\operatorname{softplus}\) violates the first-order condition of convexity (black dotted line \(\cdots\)).**For any continuous, differentiable nonlinear conditioner that maps only to non-negative reals, the strong convexity of \(\mathcal{E}\) does lead to a strongly-convex ELBO. This phenomenon is visualized in Figure 2. The loss surface becomes flat near the optimal scale parameter. This problem becomes more noticeable as the optimal scale becomes smaller.

Nonlinear conditioners are suboptimal.As the dataset grows, Bayesian posteriors are known to "contract" as characterized by the Bernstein-von Mises theorem (van der Vaart, 1998). That is, the posterior variance becomes close to \(0\). This behavior also applies to misspecified variational posteriors as shown by Wang and Blei (2019). Thus, for large datasets, nonlinear conditioners mostly operate in the regime where they are suboptimal (locally less strongly convex). But linear conditioners result in a non-smooth entropy (Domke, 2020). This dilemma originally motivated Domke to consider proximal SGD, which we analyze in Section 4.2.

## 4 Convergence Analysis of Black-Box Variational Inference

### Black-Box Variational Inference

BBVI with SGD repeats the steps:

\[\lambda_{t+1}=\lambda_{t}-\gamma_{t}\left(\widehat{\nabla f}\left(\lambda_{t} \right)+\nabla h\left(\lambda_{t}\right)\right),\quad\text{where}\quad\widehat {\nabla f}\left(\lambda_{t}\right)=\frac{1}{M}\sum_{m=1}^{M}\nabla_{\lambda} \mathcal{E}\left(\mathcal{I}_{\lambda}\left(\mathbf{u}_{m}\right)\right) \tag{1}\]

with \(\mathbf{u}_{m}\sim\varphi\) is the \(M\)-sample reparameterization gradient estimator and \(\gamma_{t}\) is the stepsize. (See Kucukelbir et al., 2017 for algorithmic details.)

With our results in Section 3 and the results of Khaled and Richtarik (2023); Kim et al. (2023), we obtain a convergence guarantee. To apply the result of Kim et al. (2023), which bounds the gradient variance, we require an additional assumption.

**Assumption 5**.: The negative log-likelihood \(\mathcal{E}_{\text{like}}(\mathbf{z})\triangleq-\log p\left(\mathbf{x}\mid \mathbf{z}\right)\) is \(\mu\)-quadratically growing for all \(\mathbf{z}\in\mathbb{R}^{d}\) such that

\[\frac{\mu}{2}\|\mathbf{z}-\mathbf{\hat{z}}_{\text{like}}\|_{2}^{2}\leq \mathcal{E}_{\text{like}}\left(\mathbf{z}\right)-\mathcal{E}_{\text{like}}^{ \ast},\]

where \(\mathbf{\hat{z}}_{\text{like}}\) is the projection of \(\mathbf{z}\) to the set of minimizers of \(\mathcal{E}_{\text{like}}\), and \(\mathcal{E}_{\text{like}}^{\ast}=\inf_{\mathbf{z}\in\mathbb{R}^{d}}\mathcal{E} _{\text{like}}\left(\mathbf{z}\right)\).

This assumption is weaker than assuming that the likelihood satisfies the Polyak-Lojasiewicz inequality (Karimi et al., 2016).

**Theorem 3**.: _Let Assumption 2 hold, the likelihood satisfy Assumption 5, and the assumptions of Corollary 1 hold such that the ELBO \(F\) is \(L_{F}\)-smooth with \(L_{F}=L_{\mathcal{E}}+L_{\phi}+L_{\text{s}}\). Then, the iterates generated by BBVI through Equation (1) and the \(M\)-sample reparameterization gradient include an \(\epsilon\)-stationary point such that \(\min_{0\leq t\leq T-1}\mathbb{E}\|\nabla F\left(\lambda_{t}\right)\|_{2}\leq\epsilon\) for any \(\epsilon>0\) if_

\[T\geq\mathcal{O}\left(\frac{\left(F\left(\lambda_{0}\right)-F^{\ast}\right)^{2 }L_{F}L_{\mathcal{E}}^{2}\mathcal{C}\left(d,k_{\varphi}\right)}{\mu M\epsilon ^{4}}\right)\]

_for some fixed stepsize \(\gamma\), where \(\mathcal{C}\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family and \(\mathcal{C}\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 35.

**Remark 3**.: Finding an \(\epsilon\)-stationary point of the ELBO has an iteration complexity of \(\mathcal{O}\left(dL_{\ell}^{2}xM^{-1}\epsilon^{-4}\right)\) for the Cholesky family and \(\mathcal{O}\left(\sqrt{d}L_{\ell}^{2}\kappa M^{-1}\epsilon^{-4}\right)\) for the mean-field family.

### Black-Box Variational Inference with Proximal SGD

**Proximal SGD** For a composite objective \(F=f+h\), proximal SGD repeats the steps:

\[\lambda_{t+1}=\operatorname{prox}_{\gamma_{t},h}\left(\lambda_{t}-\gamma_{t} \widehat{\nabla f}\left(\lambda_{t}\right)\right)=\operatorname*{arg\,min}_{ \lambda\in\Lambda}\ \left[\left\langle\widehat{\nabla f}\left(\lambda_{t}\right), \lambda\right\rangle+h\left(\lambda\right)+\frac{1}{2\gamma_{t}}\|\lambda- \lambda_{t}\|_{2}^{2}\ \right], \tag{2}\]

where prox is known as the _proximal_ operator and \(\gamma_{1}\),..., \(\gamma_{T}\) is a stepsize schedule.

In the context of VI, proximal SGD has previously been considered by Altosaar et al. (2018); Diao et al. (2023); Khan et al. (2016, 2015). Their overall focus has been on developing alternative algorithms by generalizing \(\|\lambda-\lambda^{\ast}\|\) to other metrics. In contrast, Domke (2020) considered proximal SGD with the regular Euclidean metric \(\|\lambda-\lambda^{\ast}\|_{2}\) for overcoming the non-smoothness of \(h\) under linear parameterizations. Here, we prove the convergence of this scheme and show that it retrieves the fastest known convergence rates in stochastic first-order optimization.

Proximal Operator for BBVIIn our context, \(h\) is the entropy of \(q_{A}\) in the location-scale family. For this, Domke (2020) show that the the proximal update for \(s_{1},\ldots,s_{d}\), is

\[\operatorname{prox}_{\gamma_{t},h}\left(s_{i}\right)=s_{i}+\frac{1}{2}\left( \sqrt{s_{i}^{2}+4\gamma_{t}}-s_{i}\right).\]

For other parameters, the proximal operator is the regular gradient descent update in Equation (1).

Gradient Variance BoundWe first establish a bound on the gradient variance. In ERM, contemporary strategies do this by exploiting the finite sum structure of the objective (Section 2.4). Here, we establish a variance bound for RP estimator that does not rely on the finite sum assumption.

**Lemma 3** (**Convex Expected Smoothness)**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex with the variational family satisfying Assumption 2 with the linear parameterization. Then,_

\[\mathbb{E}\|\nabla_{\lambda}f\left(\lambda;u\right)-\nabla_{\lambda^{\prime}} f\left(\lambda^{\prime};u\right)\|_{2}^{2}\leq 2L_{\ell}\kappa\,C\left(d,\varphi \right)\,\operatorname{B}_{f}\left(\lambda,\lambda^{\prime}\right)\]

_holds, where \(\operatorname{B}_{f}\left(\lambda,\lambda^{\prime}\right)\triangleq f\left( \lambda\right)-f\left(\lambda^{\prime}\right)-\left\langle\nabla f\left( \lambda^{\prime}\right),\lambda-\lambda^{\prime}\right\rangle\) is the Bregman divergence, \(\kappa=L_{\ell}/\mu\) is the condition number, \(C\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family, and \(C\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 36.

Furthermore, the gradient variance at the optimum must be bounded:

**Lemma 4** (Domke, 2019; Kim _et al._, 2023).: _Let \(\ell\) be \(L_{\ell}\)-smooth with the variational family satisfying Assumption 2 and a 1-Lipschitz diagonal conditioner \(\phi\). Then, the gradient variance at the optimum \(\lambda^{\star}\in\operatorname*{arg\,min}_{\lambda\in\Lambda}F\left(\lambda\right)\) is bounded as_

\[\sigma^{2}\leq\frac{1}{M}C\left(d,\varphi\right)\,L_{\ell}^{2}\,\left(\left\| \tilde{\boldsymbol{z}}-\boldsymbol{m}^{\star}\right\|_{2}^{2}+\left\|C^{\star }\right\|_{F}^{2}\right),\]

_where \(\tilde{\boldsymbol{z}}\) is a stationary point of \(\ell\), \(\boldsymbol{m}^{\star}\) and \(\boldsymbol{C}^{\star}\) are the location and scale formed by \(\lambda^{\star}\), the constants are \(C\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family and \(C\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family, \(k_{\varphi}\) is the kurtosis of \(\varphi\) as defined in Assumption 1._

Proof.: The full-rank case is proven by Domke (2019, Theorem 3), while the mean-field case is a basic corollary of the result by Kim _et al._ (2023, Lemma 2). 

**Remark 4**.: The dimensional dependence in the complexity of BBVI is transferred from the variance bound in Lemma 4. Unfortunately, for the Cholesky family, this dimensional dependence in the variance bound is tight (Domke, 2019).

Main ResultWith the gradient variance bounds, we now present our complexity result. The proof is identical to Theorem 3.2 by Gower _et al._ (2019), where they use a 2-stage decreasing stepsize schedule: the stepsize is initially held constant and then reduced in a \(1/t\) rate.

**Theorem 4**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex. Then, for any \(\epsilon>0\), BBVI with proximal SGD in Equation (2), the \(M\)-sample reparameterization gradient estimator, a variational family satisfying Assumption 2 with the linear parameterization guarantees \(\mathbb{E}\|\lambda_{T}-\lambda^{\star}\|_{2}^{2}\leq\epsilon\) if_

\[\gamma_{t}=\begin{cases}\frac{M}{2L_{\ell}\kappa C\left(d,\varphi\right)}& \text{for}\quad t\leq 4T_{\kappa}\\ \frac{2t+1}{\left(t+1\right)^{2}\mu}&\text{for}\quad t>4T_{\kappa},\end{cases} T\geq\max\left(\frac{8\sigma^{2}}{\mu^{2}\,\epsilon}+\frac{4T_{\kappa}\| \lambda_{0}-\lambda^{\star}\|_{2}}{\text{e}\sqrt{\epsilon}},\ \ 4T_{\kappa}\right)\]

_where \(\sigma^{2}\) is defined in Lemma 4, \(T_{\kappa}=\left[\kappa^{2}C\left(d,\varphi\right)M^{-1}\right]\), \(\kappa=L_{\ell}/\mu\) is the condition number, \(\text{e}\) is Euler's constant, \(\lambda^{\star}=\operatorname*{arg\,min}_{\lambda\in\Lambda}F\left(\lambda\right)\), \(C\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family, and \(C\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 38.

**Remark 5**.: BBVI with proximal SGD on \(\mu\)-strongly convex and \(L_{\ell}\)-smooth \(\ell\) has a complexity \(\mathcal{O}\left(\kappa^{2}dM^{-1}\,\epsilon^{-1}\right)\) for the Cholesky family and \(\mathcal{O}\left(\kappa^{2}\sqrt{d}M^{-1}\,\epsilon^{-1}\right)\) for the mean-field family.

**Remark 6**.: We also provide a similar result with a fixed stepsize in Theorem 7 of Appendix F.3.2. In this case, the complexity is \(\mathcal{O}\left(\kappa^{2}dM^{-1}\epsilon^{-1}\log\epsilon^{-1}\right)\) for the Cholesky family and \(\mathcal{O}\left(\kappa^{2}\sqrt{d}M^{-1}\epsilon^{-1}\log\epsilon^{-1}\right)\) for the mean-field family.

## 5 Experiments

### Synthetic Problem

SetupWe first compare proximal SGD against vanilla SGD with linear and nonlinear parameterizations on a synthetic problem, which is log-smooth, strongly log-concave, and the exact solution is known. While a similar experiment was already conducted by Domke (2020), here we include nonlinear parameterizations, which were not originally considered. We run all algorithms with a fixed stepsize to infer a multivariate Gaussian with a full-rank covariance matrix. The variational approximation is a full-rank Gaussian formed by \(\varphi=\mathcal{N}(0,1)\) and the Cholesky parameterization.

ResultsThe results are shown in Figure 3. Proximal SGD is clearly the most robust against initialization. Also, SGD with the nonlinear parameterization \(\phi(x)=\text{softplus}(x)\) is much slower to converge under all initializations. This confirms that linear parameterizations are indeed superior for both robustness against initializations and convergence speed.

### Realistic Problems

SetupWe now evaluate proximal SGD on realistic problems. In practice, Adam (Kingma and Ba, 2015) is observed to be robust against stepsize choices (Zhang et al., 2019). The reason why Adam performs well on non-smooth, non-convex problems is still under investigation (Kunstner et al., 2023; Reddi et al., 2023; Zhang et al., 2022). Nonetheless, to compare fairly against Adam, we implement a recently proposed variant of proximal SGD called Pro

Figure 4: **Comparison of BBVI convergence speed (ELBO v.s. Iteration) of different optimization algorithms. The error bands are the 80% quantiles estimated from 20 (10 for AR-eeg) independent replications. The results shown used a base stepsize of \(\gamma=10^{-3}\), while the initial point was \(\mathbf{m}_{0}=\mathbf{0},\mathbf{C}_{0}=\mathbf{I}\). Details on the setup can be found in the text of Section 5.2 and Appendix G.**

Figure 3: **Stepsize versus the number of iterations for vanilla SGD and proximal SGD to achieve \(\text{D}_{\text{KL}}(q_{\lambda},\pi)\leq\epsilon=1\) under different initializations for Gaussian posteriors. The initializations \(C(\lambda_{0})\) are \(\mathbf{I}\), \(10^{-3}\mathbf{I}\), \(10^{-5}\mathbf{I}\) from left to right, respectively. The average suboptimality at iteration \(t\) was estimated from 10 independent runs. For each run, the target posterior was a 10-dimensional Gaussian with a covariance with a condition number \(\kappa=10\) and a smoothness of \(L=100\).**includes an Adam-like update rule. The probabilistic models and datasets are fully described in Appendix G. We implement these models and BBVI on top of the Turing (Ge _et al._, 2018) probabilistic programming framework. Due to the size of these datasets, we implement _doubly stochastic_ subsampling (Titsias & Lazaro-Gredilla, 2014) with a batch size of \(\mathcal{B}=100\) (\(B=500\) for \(\mathcal{B}\)T-tennis) with \(M=10\) Monte Carlo samples. For batch subsampling, we implement random-reshuffling, which is faster than independent subsampling both empirically (Bottou, 2009) and theoretically (Ahn _et al._, 2020; Haochen & Sra, 2019; Mishchenko _et al._, 2020; Nagaraj _et al._, 2019). We also observe that doubly stochastic BBVI benefits from reshuffling, but leave a detailed investigation to future works.

ResultsRepresentative results are shown in Figure 4, with additional results in Appendix H. Both ProxGen-Adam and Adam with linear parameterizations converge faster than Adam with nonlinear parameterization. Furthermore, for the case of election and buzz, Adam with the nonlinear parameterization converges much slower than the alternatives. When using linear parameterizations, ProxGen-Adam appears to be generally faster than Adam. We note, however, that due to the difference in the update rule between ProxGen-Adam and Adam, proximal operators alone might not fully explain the performance difference. Nevertheless, the results of our experiment do conclusively suggest that linear parameterizations are superior.

## 6 Discussions

ConclusionsIn this work, we have proven the convergence of BBVI. Our assumptions encompass implementations that are actually used in practice, and our theoretical analysis revealed limitations in some of the popular design choices (mainly the use of nonlinear conditioners). To resolve this issue, we re-evaluated the utility of proximal SGD both theoretically and practically, where it achieved the strongest theoretical guarantees in stochastic first-order optimization.

Related WorksTo prove the convergence of BBVI, early works have _a-priori_ "assumed" the regularity of the ELBO and the gradient estimator (Alquier & Ridgway, 2020; Buchholz _et al._, 2018; Khan _et al._, 2016, 2015; Liu & Owen, 2021; Regier _et al._, 2017). Towards a more rigorous understanding, Domke (2019); Fan _et al._ (2015); Kim _et al._ (2023); Xu _et al._ (2019) studied the reparameterization gradient, Xu & Campbell (2022) studied the asymptotics of the ELBO, Challis & Barber (2013); Domke (2020); Titsias & Lazaro-Gredilla (2014) established convexity, and Domke (2020) established smoothness. On the other hand, Bhatia _et al._ (2022); Hoffman & Ma (2020) established rigorous convergence guarantees by considering simplified variant of BBVI where only the scale is optimized, and Fujisawa & Sato (2021) assumed that the support of \(\varphi\) is bounded almost surely. Meanwhile, under similar assumptions to ours, Diao _et al._ (2023); Lambert _et al._ (2022) recently established convergence guarantees for proximal SGD BBVI with a Bures-Wasserstein metric. Their computational properties differ from BBVI as they require Hessian evaluations. Also, understanding BBVI, which is VI with a Euclidean metric, is an important problem due to its practical relevance.

LimitationsOur work has multiple limitations: **(i)** Our results are restricted to the location-scale family, **(ii)** the reparameterization gradient, and **(iii)** smooth joint log-likelihoods. However, the location-scale family with the reparameterization gradient is the most widely used combination in practice, and replacing the smoothness assumption is an active area of research in stochastic optimization. For our results on proximal SGD, we further assume that the joint log-likelihood is \(\mu\)-strongly convex (equivalently strongly log-concave posteriors). It is unclear how to extend the guarantees to only smooth but non-log-concave joint log-likelihoods.

Open ProblemsAlthough we have proven that the mean-field dimensional family has a dimension dependence of \(\mathcal{O}\left(\sqrt{d}\right)\), empirical results suggest room for improvement (Kim _et al._, 2023). Therefore, we pose the following conjecture:

**Conjecture 1**.: _Under mild assumptions, BBVI for the mean-field variational family converges with only logarithmic dimensional dependence or no explicit dimensional dependence at all._

This would put mean-field BBVI in a regime clearly faster than approximate MCMC (Freund _et al._, 2022). Also, it is unknown whether the \(\mathcal{O}\left(\kappa^{2}\right)\) condition number dependence dependence is tight. In fact, for proximal SGD BBVI in Bures-Wasserstein space, Diao _et al._ (2023) report a dependence of \(\mathcal{O}\left(\kappa\right)\). Lastly, it would be interesting to see whether natural gradient VI (NGVI; Amari, 1998; Khan & Lin, 2017) can achieve similar convergence guarantees. While it is empirically known that NGVI often converges faster (Lin _et al._, 2019), theoretical evidence has yet to follow.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Justin Domke for discussions on the concurrent results, Javier Burroni for pointing out a mistake in the earlier version of this work, and the anonymous reviewers for their constructive comments.

K. Kim and J. R. Gardner were funded by the National Science Foundation Award [IIS-2145644], while Y.-A. Ma was funded by the National Science Foundation Grants [NSF-SCALE MoDL-2134209] and [NSF-CCF-2112665 (TILOS)], the U.S. Department Of Energy, Office of Science, and the Facebook Research award.

## References

* Agrawal et al. (2021) Agrawal, Abhinav, & Domke, Justin. 2021. Amortized Variational Inference for Simple Hierarchical Models. _Pages 21388-21399 of:_ _Advances in Neural Information Processing Systems_, vol. 34. Curran Associates, Inc. (page 2)
* Agrawal et al. (2020) Agrawal, Abhinav, Sheldon, Daniel R, & Domke, Justin. 2020. Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization. _Pages 17358-17369 of:_ _Advances in Neural Information Processing Systems_, vol. 33. Curran Associates, Inc. (page 1)
* Ahn et al. (2020) Ahn, Kwangjun, Yun, Chulhee, & Sra, Suvrit. 2020. SGD with Shuffling: Optimal Rates without Component Convexity and Large Epoch Requirements. _Pages 17526-17535 of:_ _Advances in Neural Information Processing Systems_, vol. 33. Curran Associates, Inc. (page 10)
* Alquier & Ridgway (2020) Alquier, Pierre, & Ridgway, James. 2020. Concentration of Tempered Posteriors and of Their Variational Approximations. _The Annals of Statistics_, **48**(3), 1475-1497. (page 10)
* Altosaar et al. (2018) Altosaar, Jaan, Ranganath, Rajesh, & Blei, David. 2018. Proximity Variational Inference. _Pages 1961-1969 of:_ _Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 84. JMLR. (page 7)
* Amari (1998) Amari, Shun-ichi. 1998. Natural Gradient Works Efficiently in Learning. _Neural Computation_, **10**(2), 251-276. (page 10)
* Bertin-Mahieux et al. (2011) Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whitman, Brian, & Lamere, Paul. 2011. The Million Song Dataset. _In: Proceedings of the International Conference on Music Information_. (page 40)
* Bhatia et al. (2022) Bhatia, Kush, Kuang, Nikki Lijing, Ma, Yi-An, & Wang, Yixin. 2022 (July). _Statistical and Computational Trade-Offs in Variational Inference: A Case Study in Inferential Model Selection_. arXiv Preprint arXiv:2207.11208. arXiv. (pages 1, 10)
* Bingham et al. (2019) Bingham, Eli, Chen, Jonathan P., Jankowiak, Martin, Obermeyer, Fritz, Pradhan, Neeraj, Karaletsos, Theofanis, Singh, Rohit, Szerlip, Paul, Horsfall, Paul, & Goodman, Noah D. 2019. Pyro: Deep Universal Probabilistic Programming. _Journal of Machine Learning Research_, **20**(28), 1-6. (page 1)
* Blei et al. (2017) Blei, David M., Kucukelbir, Alp, & McAuliffe, Jon D. 2017. Variational Inference: A Review for Statisticians. _Journal of the American Statistical Association_, **112**(518), 859-877. (pages 2, 4)
* Bottou (1999) Bottou, Leon. 1999. On-Line Learning and Stochastic Approximations. _Pages 9-42 of:_ _On-Line Learning in Neural Networks_, first edn. Cambridge University Press. (page 2)
* Bottou (2009) Bottou, Leon. 2009. _Curiously Fast Convergence of Some Stochastic Gradient Descent Algorithms_. (page 10)
* Buchholz et al. (2018) Buchholz, Alexander, Wenzel, Florian, & Mandt, Stephan. 2018. Quasi-Monte Carlo Variational Inference. _Pages 668-677 of:_ _Proceedings of the International Conference on Machine Learning_. PMLR, vol. 80. JMLR. (pages 4, 10)
* Carpenter et al. (2017) Carpenter, Bob, Gelman, Andrew, Hoffman, Matthew D., Lee, Daniel, Goodrich, Ben, Betancourt, Michael, Brubaker, Marcus, Guo, Jiqiang, Li, Peter, & Riddell, Allen. 2017. Stan: A Probabilistic Programming Language. _Journal of Statistical Software_, **76**(1). (page 1)
* Carvalho et al. (2009) Carvalho, Carlos M., Polson, Nicholas G., & Scott, James G. 2009. Handling Sparsity via the Horseshoe. _Pages 73-80 of:_ _Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 5. JMLR. (page 41)
* Carvalho et al. (2010) Carvalho, Carlos M., Polson, Nicholas G., & Scott, James G. 2010. The Horseshoe Estimator for Sparse Signals. _Biometrika_, **97**(2), 465-480. (page 41)Challis, Edward, & Barber, David. 2013. Gaussian Kullback-Leibler Approximate Inference. _Journal of Machine Learning Research_, **14**(68), 2239-2286. (pages 1, 5, 10)
* Christmas & Everson (2011) Christmas, Jacqueline, & Everson, Richard. 2011. Robust Autoregression: Student-T Innovations Using Variational Bayes. _IEEE Transactions on Signal Processing_, **59**(1), 48-57. (page 41)
* Dhaka et al. (2020) Dhaka, Akash Kumar, Catalina, Alejandro, Andersen, Michael R, ns Magnusson, Ma, Huggins, Jonathan, & Vehtari, Aki. 2020. Robust, Accurate Stochastic Optimization for Variational Inference. _Pages 10961-10973 of: Advances in Neural Information Processing Systems_, vol. 33. Curran Associates, Inc. (page 1)
* Dhaka et al. (2021) Dhaka, Akash Kumar, Catalina, Alejandro, Welandawe, Manushi, Andersen, Michael R., Huggins, Jonathan, & Vehtari, Aki. 2021. Challenges and Opportunities in High Dimensional Variational Inference. _Pages 7787-7798 of: Advances in Neural Information Processing Systems_, vol. 34. Curran Associates, Inc. (page 1)
* Diao et al. (2023) Diao, Michael Ziyang, Balasubramanian, Krishna, Chewi, Sinho, & Salim, Adil. 2023. Forward-Backward Gaussian Variational Inference via JKO in the Bures-Wasserstein Space. _Pages 7960-7991 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 202. JMLR. (pages 7, 10)
* Dieng et al. (2017) Dieng, Adji Bousso, Tran, Dustin, Ranganath, Rajesh, Paisley, John, & Blei, David. 2017. Variational Inference via \(\chi\) Upper Bound Minimization. _Pages 2729-2738 of: Advances in Neural Information Processing Systems_, vol. 30. Curran Associates, Inc. (page 2)
* Dillon et al. (2017) Dillon, Joshua V., Langmore, Ian, Tran, Dustin, Brevdo, Eugene, Vasudevan, Srinivas, Moore, Dave, Patton, Brian, Alemi, Alex, Hoffman, Matt, & Saurous, Rif A. 2017 (Nov.). _TensorFlow Distributions_. arXiv Preprint arXiv:1711.10604. arXiv. (pages 1, 3)
* Domke (2019) Domke, Justin. 2019. Provable Gradient Variance Guarantees for Black-Box Variational Inference. _Pages 329-338 of: Advances in Neural Information Processing Systems_, vol. 32. Curran Associates, Inc. (pages 1, 3, 4, 8, 10, 21, 22, 36)
* Domke (2020) Domke, Justin. 2020. Provable Smoothness Guarantees for Black-Box Variational Inference. _Pages 2587-2596 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 119. JMLR. (pages 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 25, 27, 33)
* Domke et al. (2023) Domke, Justin, Garrigos, Guillaume, & Gower, Robert. 2023. Provable Convergence Guarantees for Black-Box Variational Inference. _In: Advances in Neural Information Processing Systems (to Appear)_. New Orleans, LA, USA: arXiv. (pages 2, 17, 21)
* Dua & Graff (2017) Dua, Dheeru, & Graff, Casey. 2017. UCI Machine Learning Repository. (page 40)
* Duchi et al. (2011) Duchi, John, Hazan, Elad, & Singer, Yoram. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. _Journal of Machine Learning Research_, **12**(Jul), 2121-2159. (page 20)
* Dugas et al. (2000) Dugas, Charles, Bengio, Yoshua, Belisle, Francois, Nadeau, Claude, & Garcia, Rene. 2000. Incorporating Second-Order Functional Knowledge for Better Option Pricing. _In: Advances in Neural Information Processing Systems_, vol. 13. MIT Press. (page 4)
* Dwivedi et al. (2019) Dwivedi, Raaz, Chen, Yuansi, Wainwright, Martin J., & Yu, Bin. 2019. Log-Concave Sampling: Metropolis-Hastings Algorithms Are Fast. _Journal of Machine Learning Research_, **20**(183), 1-42. (page 2)
* Fan et al. (2015) Fan, Kai, Wang, Ziteng, Beck, Jeff, Kwok, James, & Heller, Katherine A. 2015. Fast Second Order Stochastic Backpropagation for Variational Inference. _Pages 1387-1395 of: Advances in Neural Information Processing Systems_, vol. 28. Curran Associates, Inc. (page 10)
* Fjelde et al. (2020) Fjelde, Tor Erlend, Xu, Kai, Tarek, Mohamed, Yalburgi, Sharan, & Ge, Hong. 2020. Bijectors.jl: Flexible Transformations for Probability Distributions. _Pages 1-17 of: Proceedings of The Symposium on Advances in Approximate Bayesian Inference_. PMLR, vol. 118. JMLR. (page 3)
* Freund et al. (2022) Freund, Yoav, Ma, Yi-An, & Zhang, Tong. 2022. When Is the Convergence Time of Langevin Algorithms Dimension Independent? A Composite Optimization Viewpoint. _Journal of Machine Learning Research_, **23**(214), 1-32. (page 10)
* Fujisawa & Sato (2021) Fujisawa, Masahiro, & Sato, Issei. 2021. Multilevel Monte Carlo Variational Inference. _Journal of Machine Learning Research_, **22**(278), 1-44. (pages 3, 4, 10)Garrigos, Guillaume, & Gower, Robert M. 2023 (Feb.). _Handbook of Convergence Theorems for (Stochastic) Gradient Methods_. arXiv Preprint arXiv:2301.11235. arXiv. (pages 2, 4, 21, 37, 38)
* Ge et al. (2018) Ge, Hong, Xu, Kai, & Ghahramani, Zoubin. 2018. Turing: A Language for Flexible Probabilistic Inference. _Pages 1682-1690 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 84. JMLR. (pages 1, 10)
* Gelman & Hill (2007) Gelman, Andrew, & Hill, Jennifer. 2007. _Data Analysis Using Regression and Multilevel/Hierarchical Models_. Analytical Methods for Social Research. Cambridge; New York: Cambridge University Press. (page 40)
* Giordano et al. (2018) Giordano, Ryan, Broderick, Tamara, & Jordan, Michael I. 2018. Covariances, Robustness, and Variational Bayes. _Journal of Machine Learning Research_, **19**(51), 1-49. (page 1)
* Giordano et al. (2023) Giordano, Ryan, Ingram, Martin, & Broderick, Tamara. 2023 (Apr.). _Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box_. arXiv Preprint arXiv:2304.05527. arXiv. (page 41)
* Goldberger et al. (2000) Goldberger, Ary L., Amaral, Luis A. N., Glass, Leon, Hausdorff, Jeffrey M., Ivanov, Plamen Ch., Mark, Roger G., Mietus, Joseph E., Moody, George B., Peng, Chung-Kang, & Stanley, H. Eugene. 2000. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. _Circulation_, **101**(23). (page 41)
* Gorbunov et al. (2020) Gorbunov, Eduard, Hanzely, Filip, & Richtarik, Peter. 2020. A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent. _Pages 680-690 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 108. JMLR. (pages 21, 37)
* Gower et al. (2019) Gower, Robert Mansel, Loizou, Nicolas, Qian, Xun, Sailanbayev, Alibek, Shulgin, Egor, & Richtarik, Peter. 2019. SGD: General Analysis and Improved Rates. _Pages 5200-5209 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 97. JMLR. (pages 4, 8, 21, 38)
* Haochen & Sra (2019) Haochen, Jeff, & Sra, Suvrit. 2019. Random Shuffling Beats SGD after Finite Epochs. _Pages 2624-2633 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 97. JMLR. (page 10)
* Hernandez-Lobato et al. (2016) Hernandez-Lobato, Jose, Li, Yingzhen, Rowland, Mark, Bui, Thang, Hernandez-Lobato, Daniel, & Turner, Richard. 2016. Black-Box Alpha Divergence Minimization. _Pages 1511-1520 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 48. JMLR. (page 2)
* Hoffman & Ma (2020) Hoffman, Matthew, & Ma, Yian. 2020. Black-Box Variational Inference as a Parametric Approximation to Langevin Dynamics. _Pages 4324-4341 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 119. JMLR. (pages 1, 10)
* Jager et al. (2003) Jager, F., Taddei, A., Moody, G. B., Emdin, M., Antolic, G., Dorn, R., Smrdel, A., Marchesi, C., & Mark, R. G. 2003. Long-Term ST Database: A Reference for the Development and Evaluation of Automated Ischaemia Detectors and for the Study of the Dynamics of Myocardial Ischaemia. _Medical and Biological Engineering and Computing_, **41**(2), 172-182. (pages 40, 41)
* Jordan et al. (1999) Jordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., & Saul, Lawrence K. 1999. An Introduction to Variational Methods for Graphical Models. _Machine Learning_, **37**(2), 183-233. (pages 2, 3)
* Karimi et al. (2016) Karimi, Hamed, Nutini, Julie, & Schmidt, Mark. 2016. Linear Convergence of Gradient and Proximal-Gradient Methods under the Polyak-Lojasiewicz Condition. _Pages 795-811 of: Machine Learning and Knowledge Discovery in Databases_. Lecture Notes in Computer Science. Cham: Springer International Publishing. (page 7)
* Kawala et al. (2013) Kawala, Francois, Douzal-Chouakria, Ahlame, Gaussier, Eric, & Diemert, Eustache. 2013. Predictions d'activite Dans Les Reseaux Sociaux En Ligne. _Page 16 of: Actes de La Conference Sur Les Modeles et L'Analyse Des Reseaux : Approches Mathematiques et Informatique_. (page 40)
* Khaled & Richtarik (2023) Khaled, Ahmed, & Richtarik, Peter. 2023. Better Theory for SGD in the Nonconvex World. _Transactions of Machine Learning Research_. (pages 7, 21, 34, 35)
* Khan & Lin (2017) Khan, Mohammad, & Lin, Wu. 2017. Conjugate-Computation Variational Inference: Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models. _Pages 878-887 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 54. JMLR. (page 10)Khan, Mohammad Emtiyaz, Babanezhad, Reza, Lin, Wu, Schmidt, Mark, & Sugiyama, Masashi. 2016. Faster Stochastic Variational Inference Using Proximal-Gradient Methods with General Divergence Functions. _Pages 319-328 of: Proceedings of the Conference on Uncertainty in Artificial Intelligence_. UAI'16. Arlington, Virginia, USA: AUAI Press. (pages 7, 10)
* Khan et al. (2015) Khan, Mohammad Emtiyaz E, Baque, Pierre, Fleuret, Francois, & Fua, Pascal. 2015. Kullback-Leibler Proximal Variational Inference. _In: Advances in Neural Information Processing Systems_, vol. 28. Curran Associates, Inc. (pages 7, 10)
* Kim et al. (2022) Kim, Kyurae, Oh, Jisu, Gardner, Jacob, Dieng, Adji Bousso, & Kim, Hongseok. 2022. Markov Chain Score Ascent: A Unifying Framework of Variational Inference with Markovian Gradients. _Pages 34802-34816 of: Advances in Neural Information Processing Systems_, vol. 35. Curran Associates, Inc. (page 2)
* Kim et al. (2023) Kim, Kyurae, Wu, Kaiwen, Oh, Jisu, & Gardner, Jacob R. 2023. Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. _Pages 16853-16876 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 202. Honolulu, HI, USA: JMLR. (pages 1, 3, 4, 7, 8, 10, 21, 22, 34, 35, 36)
* Kingma & Ba (2015) Kingma, Diederik P., & Ba, Jimmy. 2015. Adam: A Method for Stochastic Optimization. _In: Proceedings of the International Conference on Learning Representations_. (pages 2, 9, 20)
* Kingma & Welling (2014) Kingma, Diederik P., & Welling, Max. 2014 (Apr.). Auto-Encoding Variational Bayes. _In: Proceedings of the International Conference on Learning Representations_. (page 2)
* Kucukelbir et al. (2017) Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, & Blei, David M. 2017. Automatic Differentiation Variational Inference. _Journal of Machine Learning Research_, **18**(14), 1-45. (pages 1, 3, 7, 21)
* Kunstner et al. (2023) Kunstner, Frederik, Chen, Jacques, Lavington, Jonathan Wilder, & Schmidt, Mark. 2023 (Feb.). Noise Is Not the Main Factor behind the Gap between Sgd and Adam on Transformers, but Sign Descent Might Be. _In: Proceedings of the International Conference on Learning Representations_. (page 9)
* Lambert et al. (2022) Lambert, Marc, Chewi, Sinho, Bach, Francis, Bonnabel, Silvere, & Rigollet, Philippe. 2022. Variational Inference via Wasserstein Gradient Flows. _Pages 14434-14447 of: Advances in Neural Information Processing Systems_, vol. 35. Curran Associates, Inc. (page 10)
* Leger (2023) Leger, Jean-Benoist. 2023 (Jan.). _Parametrization Cookbook: A Set of Bijective Parametrizations for Using Machine Learning Methods in Statistical Inference_. arXiv Preprint arXiv:2301.08297. arXiv. (page 3)
* Lin et al. (2019) Lin, Wu, Khan, Mohammad Emtiyaz, & Schmidt, Mark. 2019. Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-Family Approximations. _Pages 3992-4002 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 97. JMLR. (page 10)
* Liu & Owen (2021) Liu, Sifan, & Owen, Art B. 2021. Quasi-Monte Carlo Quasi-Newton in Variational Bayes. _Journal of Machine Learning Research_, **22**(243), 1-23. (pages 4, 10)
* Magnusson et al. (2022) Magnusson, Mans, Burkner, Paul, & Vehtari, Aki. 2022 (Nov.). _Posteriordb: A Set of Posteriors for Bayesian Inference and Probabilistic Programming_. (page 40)
* Mishchenko et al. (2020) Mishchenko, Konstantin, Khaled, Ahmed, & Richtarik, Peter. 2020. Random Reshuffling: Simple Analysis with Vast Improvements. _Pages 17309-17320 of: Advances in Neural Information Processing Systems_, vol. 33. Curran Associates, Inc. (page 10)
* Naesseth et al. (2020) Naesseth, Christian, Lindsten, Frederik, & Blei, David. 2020. Markovian Score Climbing: Variational Inference with KL(p\(\|\)q). _Pages 15499-15510 of: Advances in Neural Information Processing Systems_, vol. 33. Curran Associates, Inc. (page 2)
* Nagaraj et al. (2019) Nagaraj, Dheeraj, Jain, Prateek, & Netrapalli, Praneeth. 2019. SGD without Replacement: Sharper Rates for General Smooth Convex Functions. _Pages 4703-4711 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 97. JMLR. (page 10)
* Nemirovski et al. (2009) Nemirovski, A., Juditsky, A., Lan, G., & Shapiro, A. 2009. Robust Stochastic Approximation Approach to Stochastic Programming. _SIAM Journal on Optimization_, **19**(4), 1574-1609. (page 2)
* Nguyen et al. (2018) Nguyen, Lam, Nguyen, Phuong Ha, van Dijk, Marten, Richtarik, Peter, Scheinberg, Katya, & Takac, Martin. 2018. SGD and Hogwild! Convergence without the Bounded Gradients Assumption.

Pages 3750-3758 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 80. JMLR. (page 4)
* Patil et al. (2010) Patil, Anand, Huard, David, & Fonnesbeck, Christopher. 2010. PyMC: Bayesian Stochastic Modelling in Python. _Journal of Statistical Software_, **35**(4). (page 1)
* Ranganath et al. (2014) Ranganath, Rajesh, Gerrish, Sean, & Blei, David. 2014. Black Box Variational Inference. _Pages 814-822 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 33. JMLR. (page 1)
* Reddi et al. (2023) Reddi, Sashank J., Kale, Satyen, & Kumar, Sanjiv. 2023 (May). On the Convergence of Adam and Beyond. _In: Proceedings of the International Conference on Learning Representations_. (page 9)
* Regier et al. (2017) Regier, Jeffrey, Jordan, Michael I, & McAuliffe, Jon. 2017. Fast Black-Box Variational Inference through Stochastic Trust-Region Optimization. _Pages 2399-2408 of: Advances in Neural Information Processing Systems_, vol. 30. Curran Associates, Inc. (page 10)
* Robbins & Monro (1951) Robbins, Herbert, & Monro, Sutton. 1951. A Stochastic Approximation Method. _The Annals of Mathematical Statistics_, **22**(3), 400-407. (page 2)
* Roeder et al. (2017) Roeder, Geoffrey, Wu, Yuhuai, & Duvenaud, David K. 2017. Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference. _Pages 6928-6937 of: Advances in Neural Information Processing Systems_, vol. 30. Curran Associates, Inc. (pages 2, 21)
* Shannon et al. (2003) Shannon, Paul, Markiel, Andrew, Ozier, Owen, Baliga, Nitin S., Wang, Jonathan T., Ramage, Daniel, Amin, Nada, Schwikowski, Benno, & Ideker, Trey. 2003. Cytoscape: A Software Environment for Integrated Models of Biomolecular Interaction Networks. _Genome Research_, **13**(11), 2498-2504. (page 40)
* Titsias (2009) Titsias, Michalis. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. _Pages 567-574 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 5. JMLR. (page 4)
* Titsias & Lazaro-Gredilla (2014) Titsias, Michalis, & Lazaro-Gredilla, Miguel. 2014. Doubly Stochastic Variational Bayes for Non-Conjugate Inference. _Pages 1971-1979 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 32. JMLR. (pages 1, 2, 3, 5, 6, 10, 21)
* van der Vaart (1998) van der Vaart, A. W. 1998. _Asymptotic Statistics_. First edn. Cambridge University Press. (page 7)
* Vaswani et al. (2019) Vaswani, Sharan, Bach, Francis, & Schmidt, Mark. 2019. Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron. _Pages 1195-1204 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 89. JMLR. (page 4)
* Wang & Blei (2019) Wang, Yixin, & Blei, David. 2019. Variational Bayes under Model Misspecification. _In: Advances in Neural Information Processing Systems_, vol. 32. Curran Associates, Inc. (page 7)
* Welandawe et al. (2022) Welandawe, Manushi, Andersen, Michael Riis, Vehtari, Aki, & Huggins, Jonathan H. 2022 (Mar.). _Robust, Automated, and Accurate Black-Box Variational Inference_. arXiv Preprint arXiv:2203.15945. arXiv. (page 1)
* Wright & Recht (2021) Wright, Stephen J., & Recht, Benjamin. 2021. _Optimization for Data Analysis_. New York: Cambridge University Press. (page 21)
* Xu et al. (2019) Xu, Ming, Quiroz, Matias, Kohn, Robert, & Sisson, Scott A. 2019. Variance Reduction Properties of the Reparameterization Trick. _Pages 2711-2720 of: Proceedings of the International Conference on Artificial Intelligence and Statistics_. PMLR, vol. 89. JMLR. (page 10)
* Xu & Campbell (2022) Xu, Zuheng, & Campbell, Trevor. 2022. The Computational Asymptotics of Gaussian Variational Inference and the Laplace Approximation. _Statistics and Computing_, **32**(4), 63. (page 10)
* Yao et al. (2018) Yao, Yuling, Vehtari, Aki, Simpson, Daniel, & Gelman, Andrew. 2018. Yes, but Did It Work?: Evaluating Variational Inference. _Pages 5581-5590 of: Proceedings of the International Conference on Machine Learning_. PMLR, vol. 80. JMLR. (page 1)
* Yun et al. (2021) Yun, Jihun, Lozano, Aurelie C, & Yang, Eunho. 2021. Adaptive Proximal Gradient Methods for Structured Neural Networks. _Pages 24365-24378 of: Advances in Neural Information Processing Systems_, vol. 34. Curran Associates, Inc. (pages 2, 9, 20)
* Zhang et al. (2019) Zhang, Cheng, Butepage, Judith, Kjellstrom, Hedvig, & Mandt, Stephan. 2019. Advances in Variational Inference. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, **41**(8), 2008-2026. (pages 2, 9)Zhang, Yushun, Chen, Congliang, Shi, Naichen, Sun, Ruoyu, & Luo, Zhi-Quan. 2022. Adam Can Converge without Any Modification on Update Rules. _Pages 28386-28399 of: Advances in Neural Information Processing Systems_, vol. 35. Curran Associates, Inc. (page 9)

**On the Convergence of**

**Black-Box Variational Inference**

_Appendix_

**Table of Contents**

* 1 Introduction
* 2 Background
	* 2.1 Black-Box Variational Inference
	* 2.2 Variational Family
	* 2.3 Scale Parameterizations
	* 2.4 Problem Structure of Black-Box Variational Inference
* 3 The Evidence Lower Bound Under Nonlinear Scale Parameterizations
	* 3.1 Technical Assumptions
	* 3.2 Smoothness of the Entropy
	* 3.3 Smoothness of the Energy
	* 3.4 Convexity of the Energy
* 4 Convergence Analysis of Black-Box Variational Inference
	* 4.1 Black-Box Variational Inference
	* 4.2 Black-Box Variational Inference with Proximal SGD
* 5 Experiments
	* 5.1 Synthetic Problem
	* 5.2 Realistic Problems
* 6 Discussions
* A Computational Resources
* B Nomenclature
* C Definitions
* D ProxGen Adam for Black-Box Variational Inference
* E Detailed Comparison Against Domke _et al._ (2023)
* F Proofs
* F.1 Auxiliary Lemmas
* F.2 Properties of the Evidence Lower Bound
* F.2.1 Smoothness
* F.2.2 Convexity
* F.3 Convergence of Black-Box Variational Inference
* F.3.1 Vanilla Black-Box Variational Inference
* F.3.2 Proximal Black-Box Variational Inference
* G Details of Experimental Setup
* H Additional Experimental Results

## Appendix A Computational Resources

Running the experiments took approximately a week.

## Appendix B Nomenclature

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Type** & **Model and Specifications** \\ \hline System Topology & 2 nodes with 2 sockets each with 24 logical threads (total 48 threads) \\ Processor & 1 Intel Xeon Silver 4310, 2.1 GHz (maximum 3.3 GHz) per socket \\ Cache & 1.1 MiB L1, 30 MiB L2, and 36 MiB L3 \\ Memory & 250 GiB RAM \\ Accelerator & 1 NVIDIA RTX A5000 per node, 2 GHZ, 24GB RAM \\ \hline \hline \end{tabular}
\end{table}
Table 1: Computational ResourcesDefinitions

For completeness, we provide formal definitions for some of the terms we used throughout the paper.

**Definition 5** (Smoothness).: A function \(f:\mathcal{Z}\rightarrow\mathbb{R}\) is said to be \(L\)-smooth if the inequality

\[\|\nabla f\left(\mathbf{z}\right)-\nabla f\left(\mathbf{z}^{\prime}\right)\| \leq\|\mathbf{z}-\mathbf{z}^{\prime}\|\]

holds for all \(\mathbf{z},\mathbf{z}^{\prime}\in\mathcal{Z}\).

This assumption, also occasionally called Lipschitz smoothness, restricts the amount the gradient can change for a given distance. When \(f\) is twice differentiable, an equivalent condition is the Hessian to be bounded:

**Definition 6** (Smoothness).: A twice differentiable function \(f:\mathcal{Z}\rightarrow\mathbb{R}\) is said to be \(L\)-smooth if the inequality

\[\|\nabla^{2}f\left(\mathbf{z}\right)\|\leq L\]

holds for all \(\mathbf{z}\in\mathcal{Z}\).

**Remark 7**.: Assuming a function \(f\) is smooth is equivalent to assuming that \(f\) can be upper bounded by a quadratic function everywhere.

**Remark 8**.: When the log-density \(\log\pi\) of a probability measure \(\Pi\) is \(L\)-smooth, \(\log\pi\) can be upper bounded everywhere by the log-density of a Gaussian.

**Definition 7** (Strong Convexity).: A twice differentiable function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is said to be \(\mu\)-strongly convex if the inequality

\[\frac{\mu}{2}\|\mathbf{z}-\mathbf{z}^{\prime}\|^{2}+\left\langle\nabla f\left( \mathbf{z}\right),\mathbf{z}-\mathbf{z}^{\prime}\right\rangle+f\left(\mathbf{ z}\right)\leq f\left(\mathbf{z}^{\prime}\right)\]

holds for all \(\mathbf{z},\mathbf{z}^{\prime}\in\mathbb{R}^{d}\) and some \(\mu>0\).

**Remark 9**.: If Definition 7 holds only for \(\mu=0\), \(f\) is said to be (non-strongly) convex.

**Remark 10**.: Assuming a function \(f\) is strongly convex is equivalent to assuming that \(f\) can be lower bounded by a quadratic.

**Definition 8** (Strongly Log-Concave Measures).: For a probability measure \(\Pi\) in a Euclidean measurable space \(\left(\mathbb{R}^{d},\mathcal{B}\left(\mathbb{R}^{d}\right),\mathbb{P}\right)\), where \(\mathcal{B}\left(\mathbb{R}^{d}\right)\) is the \(\sigma\)-algebra of Borel-measurable subsets of \(\mathbb{R}^{d}\), \(\mathbb{P}\) is the Lebesgue measure, we say \(\Pi\) is \(\mu\)-strongly log-concave if its log-density \(\log\pi\left(\mathbf{z}\right):\mathbb{R}^{d}\rightarrow\mathbb{R}\) is \(\mu\)-strongly convex for some \(\mu>0\).

**Remark 11**.: If Definition 8 holds only for \(\mu=0\), \(\Pi\) is said to be (non-strongly) log-concave.

**Remark 12**.: When \(\Pi\) is \(\mu\)-strongly log-concave, \(\log\pi\) can be lower bounded everywhere by the log-density of a Gaussian.

## Appendix D ProxGen Adam for Black-Box Variational Inference

```
0: Initial variational parameters \(\lambda_{0}\), base stepsize \(\alpha\), second moment stepsize \(\beta_{2}\), momentum stepsize \(\{\beta_{1},t\}_{t=1}^{T}\), small positve constant \(\epsilon\) for\(t=1,...\,,T\)do  estimate gradient of energy \(\widehat{\nabla f}\) \(\mathbf{g}_{t}=\widehat{\nabla f}(\mathbf{\lambda})+\nabla h(\mathbf{\lambda})\) \(\overline{\lambda}_{t+1}=\beta_{1,t}\overline{\lambda}_{t}+\big{(}1-\beta_{1, t}\big{)}\overline{\lambda}_{t}\) \(\mathbf{v}_{t+1}=\beta_{2}\mathbf{v}_{t}+\big{(}1-\beta_{2}\big{)}\mathbf{g}_{t}^{2}\) \(\mathbf{\Gamma}_{t+1}=\operatorname{diag}\big{(}\alpha/\big{(}\sqrt{\mathbf{v}_{t+1} +\epsilon}\big{)}\big{)}\) \(\mathbf{\lambda}_{t+1}=\mathbf{\lambda}_{t}-\mathbf{\Gamma}_{t+1}\overline{\lambda}_{t+1}\) \(\mathbf{s}_{t+1}\leftarrow\text{getscale}(\overline{\lambda}_{t+1})\) \(\mathbf{s}_{t+1}\leftarrow\mathbf{s}_{t+1}+\frac{1}{2}\bigg{(}\sqrt{\overline{s}_{t+1 }^{2}+4\gamma_{\mathbf{s},t+1}}-\mathbf{s}_{t+1}\bigg{)}\) \(\mathbf{\lambda}_{t+1}\leftarrow\text{setscale}(\mathbf{\lambda}_{t+1},\mathbf{s}_{t+1})\)  end for
```

**Algorithm 1**ProxGen-Adam for Black-Box Variational Inference

Adaptive and matrix-valued stepsize-variants of SGD such as Adam (Kingma and Ba, 2015), AdaGrad (Duchi et al., 2011) are widely used. The matrix stepsize of Adam at iteration \(t\) is given as

\[\mathbf{\Gamma}_{t+1}=\operatorname{diag}\big{(}\alpha/\big{(}\sqrt{\mathbf{v}_{t+1}}+ \epsilon\big{)}\big{)},\]

where \(\mathbf{v}_{t}\) is the exponential moving average of the second moment, \(\alpha\) is the "base stepsize." Furthermore, the matrix stepsize is applied to the moving average of the gradients, a scheme often called the (heavy-ball) momentum, denoted here as \(\overline{\lambda}_{t}\).

Recently, Yun et al. (2021) have proven the convergence for these adaptive, momentum, and matrix-valued stepsize-based SGD methods with proximal steps. Then, the proximal operator is applied as

\[\operatorname{prox}_{\mathbf{\Gamma}_{t},\mathbf{h}}\big{(}\lambda_{t}-\mathbf{\Gamma}_{t }\overline{\lambda}_{t}\big{)}=\operatorname*{arg\,min}_{\mathbf{\lambda}}\ \Big{\{}\big{(}\overline{\lambda}_{t},\mathbf{\lambda}\big{)}+h(\mathbf{\lambda})+ \frac{1}{2}(\mathbf{\lambda}-\mathbf{\lambda}_{t})^{\top}\mathbf{\Gamma}_{t}^{-1}(\mathbf{ \lambda}-\mathbf{\lambda}_{t})\Big{\}}.\]

For Adam, the matrix-valued stepsize is a diagonal matrix. Thus, the proximal operator of Domke (2020) for each \(s_{i}\) forms independent 1-dimensional quadratic problems. Thus, the proximal step is given in the closed-form

\[\operatorname{prox}_{\mathbf{\Gamma}_{t},\mathbf{h}}\left(s_{i}\right)=s_{i}+\frac{1} {2}\left(\sqrt{\overline{s_{i}^{2}+4\gamma_{s_{i}}}}-s_{i}\right),\]

where, dropping the index \(t\) for clarity, \(\overline{s}_{i}\) is the element of \(\overline{\lambda}_{t}\) corresponding to \(s_{i}\), \(\gamma_{s_{i}}\) denotes the stepsize of \(s_{i}\) (a diagonal element of \(\mathbf{\Gamma}_{t}\)). Combined with the Adam-like stepsize rule, the algorithm is shown in Algorithm 1.

```
0: Initial variational parameters \(\lambda_{0}\), base stepsize \(\alpha\), second moment stepsize \(\beta_{2}\), momentum stepsize \(\{\beta_{1},t\}_{t=1}^{T}\), small positve constant \(\epsilon\) for\(t=1,...\,,T\)do  estimate gradient of energy \(\widehat{\nabla f}\) \(\mathbf{g}_{t}=\widehat{\nabla f}(\mathbf{\lambda})+\nabla h(\mathbf{\lambda})\) \(\overline{\lambda}_{t+1}=\beta_{1,t}\overline{\lambda}_{t}+\big{(}1-\beta_{1, t}\big{)}\overline{\lambda}_{t}\) \(\mathbf{v}_{t+1}=\beta_{2}\mathbf{v}_{t}+\big{(}1-\beta_{2}\Detailed Comparison Against Domke _et al._ (2023)

In this section, we contrast our results against those of Domke _et al._ (2023). First, the main challenge to establishing a convergence guarantee for BBVI has been on bounding the gradient variance. In particular, Domke (2019) proved that the variance of the reparameterization gradient for the energy, \(\widehat{\nabla f}\), is bounded as

\[\mathbb{E}\|\widehat{\nabla f}\left(\lambda\right)\|_{2}^{2}\leq\alpha\| \lambda-\tilde{\lambda}\|_{2}^{2}+\beta \tag{3}\]

for some finite positive constants \(\alpha,\beta\) depending on the problem constants \(d,L,k_{\varphi}\). Domke _et al._ (2023) call a gradient estimator satisfying this bound to be a "quadratic variance" estimator. Furthermore, they prove that the closed-form entropy (CFE; Kucukelbir _et al._, 2017; Titsias & Lazaro-Gredilla, 2014) estimator:

\[\widehat{\nabla F}_{\text{CFE}}\left(\lambda\right)\triangleq\widehat{\nabla f }\left(\lambda\right)+\nabla h\left(\lambda\right)\]

and the STL estimator by Roeder _et al._ (2017):

\[\widehat{\nabla F}_{\text{STL}}\left(\lambda\right)\triangleq\frac{1}{M}\sum _{m=1}^{M}-\nabla_{\lambda}\ell\left(\mathcal{F}_{\lambda}\left(\mathbf{u}_{m} \right)\right)+\nabla_{\lambda}\log q_{\lambda}\left(\mathcal{F}_{\lambda} \left(\mathbf{u}_{m}\right)\right)\big{|}_{\mathbf{v}=\lambda},\]

where \(\mathbf{u}\sim\varphi\), also qualify as quadratic variance estimators.

Unfortunately, it has been unknown whether SGD is guaranteed to converge with a quadratic variance estimator except for strongly convex objectives (Wright & Recht, 2021, p. 85). Domke _et al._ (2023) expand the boundaries of SGD and prove that projected and proximal SGD with a quadratic variance estimator converges for both convex and strongly convex objectives. In particular, for the location-scale variational family, the linear parameterization, and log-concave objectives, they prove a complexity of \(\mathcal{O}\left(1/\epsilon^{2}\right)\), and for strongly log-concave objectives, they prove a complexity of \(\mathcal{O}\left(1/\epsilon\right)\).

On the other hand, Kim _et al._ (2023) and Lemma12 developed the bound in Equation3 to be of the form of

\[\mathbb{E}\|\widehat{\nabla F}\left(\lambda\right)\|_{2}^{2}\leq A\left(F\left( \lambda\right)-F^{\ast}\right)+\|\nabla F\left(\lambda\right)\|_{2}^{2}+C \tag{4}\]

for some positive finite constants \(A,B,C\), for which the convergence of SGD for convex, strongly convex (Garrigos & Gower, 2023; Gorbunov _et al._, 2020), and non-convex objectives (Khaled & Richtarik, 2023) have already been proven. Applying these results to log-smooth and log-quadratically growing objectives, we prove a complexity of \(\mathcal{O}\left(1/\epsilon^{4}\right)\), while for strong log-concave objectives, we also prove a complexity of \(\mathcal{O}\left(1/\epsilon\right)\).

Overall, both approaches can be summarized as follows: we focused on establishing gradient variance bounds of known convergence proofs, while Domke _et al._ (2023) aimed to prove that the bound by Domke (2019) is sufficient to guarantee convergence. Note that, for strongly log-concave objectives, Equation3 immediately implies Equation4. Therefore, both approaches intersect in the case of strongly log-concave objectives. Indeed, Theorem8 and the analogous result of Domke _et al._ (2023) are both based on the same proof strategy by Gower _et al._ (2019).

Proofs

### Auxiliary Lemmas

**Lemma 5**.: _Let \(\phi(x)=x\). Then, the parameterization is linear in the sense that \(\mathcal{T}_{\lambda}\) is a bilinear function such that_

\[\mathcal{T}_{\lambda-\lambda^{\prime}}\left(\mathbf{u}\right)=\mathcal{T}_{\lambda} \left(\mathbf{u}\right)-\mathcal{T}_{\lambda^{\prime}}\left(\mathbf{u}\right).\]

_for any \(\lambda,\lambda^{\prime}\in\Lambda\)._

Proof.: \[\mathcal{T}_{\lambda-\lambda^{\prime}}\left(\mathbf{u}\right) =\left(\mathbf{C}\left(\lambda-\lambda^{\prime}\right)\right)\mathbf{u}+ \left(\mathbf{m}-\mathbf{m}^{\prime}\right)\] \[=\left(\mathbf{D}_{\phi}\left(\mathbf{s}-\mathbf{s}^{\prime}\right)+\left(\mathbf{ L}-\mathbf{L}^{\prime}\right)\right)\mathbf{u}+\left(\mathbf{m}-\mathbf{m}^{\prime}\right),\]

using the fact that \(\phi\) is the identity function,

\[=\left(\mathbf{D}_{\phi}\left(\mathbf{s}\right)-\mathbf{D}_{\phi}\left(\mathbf{s} ^{\prime}\right)+\left(\mathbf{L}-\mathbf{L}^{\prime}\right)\right)\mathbf{u}+\left(\mathbf{m }-\mathbf{m}^{\prime}\right)\] \[=\left(\mathbf{C}\left(\lambda\right)-\mathbf{C}\left(\lambda^{\prime} \right)\right)\mathbf{u}+\left(\mathbf{m}+\mathbf{m}^{\prime}\right)\] \[=\left(\mathbf{C}\left(\lambda\right)\mathbf{u}+\mathbf{m}\right)-\left(\mathbf{ C}\left(\lambda^{\prime}\right)\mathbf{u}+\mathbf{m}^{\prime}\right)\] \[=\mathcal{T}_{\lambda}\left(\mathbf{u}\right)-\mathcal{T}_{\lambda^{ \prime}}\left(\mathbf{u}\right).\]

The linearity with respect to \(\mathbf{u}\) is obvious. 

**Lemma 6**.: _Let the linear parameterization be used. Then, for any \(\lambda,\lambda^{\prime}\in\Lambda\), the inner product of the Jacobian of the reparameterization function satisfies the following equalities for any \(\mathbf{u}\in\mathbb{R}^{d}\)._

* _For the Cholesky family (Domke, 2019, Lemma 8),_ \[\left(\frac{\partial\mathcal{T}_{\lambda}\left(\mathbf{u}\right)}{\partial\lambda} \right)^{\top}\frac{\partial\mathcal{T}_{\lambda}\left(\mathbf{u}\right)}{\partial \lambda}=\left(1+\left\|\mathbf{u}\right\|_{2}^{2}\right)\mathbf{I}\]
* _For the mean-field family (Kim et al., 2023, Lemma 1),_ \[\left(\frac{\partial\mathcal{T}_{\lambda}\left(\mathbf{u}\right)}{\partial\lambda} \right)^{\top}\frac{\partial\mathcal{T}_{\lambda}\left(\mathbf{u}\right)}{\partial \lambda}=\left(1+\left\|\mathbf{U}^{2}\right\|_{\mathrm{F}}\right)\mathbf{I},\] _where_ \(\mathbf{U}=\mathrm{diag}\left(u_{1},...\,,u_{d}\right)\)_._

**Lemma 7**.: _Let the linear parameterization be used. Then, for any \(\lambda\in\Lambda\) and any \(\mathbf{z}\in\mathbb{R}^{d}\), the following relationships hold._

* _For the Cholesky family (Domke, 2019, Lemma 2),_ \[\mathbb{E}\left(1+\left\|\mathbf{u}\right\|_{2}^{2}\right)\left\|\mathcal{T}_{ \lambda}\left(\mathbf{u}\right)-\mathbf{z}\right\|_{2}^{2}=\left(d+1\right)\left\|\bm {m}-\mathbf{z}\right\|_{2}^{2}+\left(d+k_{\phi}\right)\left\|\mathbf{C}\right\|_{ \mathrm{F}}^{2}\]
* _For the mean-field family (Kim et al., 2023, Lemma 2),_ \[\mathbb{E}\left(1+\left\|\mathbf{U}^{2}\right\|_{\mathrm{F}}\right)\left\|\mathcal{ T}_{\lambda^{\prime}}\left(\mathbf{u}\right)-\mathcal{T}_{\lambda}\left(\mathbf{u} \right)\right\|_{2}^{2}\leq\left(\sqrt{dk_{\phi}}+k_{\phi}\sqrt{d}+1\right) \left\|\mathbf{m}-\mathbf{z}\right\|_{2}^{2}+\left(2k_{\phi}\sqrt{d}+1\right)\left\| \mathbf{C}\right\|_{\mathrm{F}}^{2}.\]

**Corollary 2**.: _Let the linear parameterization be used and \(\lambda,\lambda^{\prime}\in\Lambda\) be any pair of variational parameters._

* _For the Cholesky family,_ \[\mathbb{E}\left(1+\left\|\mathbf{u}\right\|_{2}^{2}\right)\left\|\mathcal{T}_{ \lambda^{\prime}}\left(\mathbf{u}\right)-\mathcal{T}_{\lambda}\left(\mathbf{u}\right) \right\|_{2}^{2}\leq\left(k_{\phi}+d\right)\left\|\lambda-\lambda^{\prime} \right\|_{2}^{2}\]
* _For the mean-field family,_ \[\mathbb{E}\left(1+\left\|\mathbf{U}^{2}\right\|_{\mathrm{F}}\right)\left\|\mathcal{ T}_{\lambda^{\prime}}\left(\mathbf{u}\right)-\mathcal{T}_{\lambda}\left(\mathbf{u} \right)\right\|_{2}^{2}\leq\left(2k_{\phi}\sqrt{d}+1\right)\left\|\lambda- \lambda^{\prime}\right\|_{2}^{2}\]

Proof.: The results are a direct consequence of Lemma 7 and Lemma 5.

Proof of (i)We start from Lemma 7 as

\[\mathbb{E}\left(1+\left\|\mathbf{u}\right\|_{2}^{2}\right)\left\|\mathcal{J}_{\lambda -\lambda^{\prime}}\left(\mathbf{u}\right)-\mathbf{z}\right\|_{2}^{2} =\left(d+1\right)\left\|\left(\mathbf{m}-\mathbf{m}^{\prime}\right)-\mathbf{z} \right\|_{2}^{2}+\left(d+k_{\varphi}\right)\left\|\mathbf{C}\left(\lambda\right)- \mathbf{C}\left(\lambda^{\prime}\right)\right\|_{\text{F}}^{2},\]

setting \(\mathbf{z}=\mathbf{0}\),

\[=\left(d+1\right)\left\|\mathbf{m}-\mathbf{m}^{\prime}\right\|_{2}^{2}+\left(d+k_{ \varphi}\right)\left\|\mathbf{C}\left(\lambda\right)-\mathbf{C}\left(\lambda^{\prime} \right)\right\|_{\text{F}}^{2},\]

and since \(k_{\varphi}\geq 3\) by the property of the kurtosis,

\[\leq\left(d+k_{\varphi}\right)\left(\left\|\mathbf{m}-\mathbf{m}^{\prime}\right\|_{2} ^{2}+\left\|\mathbf{C}\left(\lambda\right)-\mathbf{C}\left(\lambda^{\prime}\right) \right\|_{\text{F}}^{2}\right)\]

\[=\left(d+k_{\varphi}\right)\left\|\mathbf{\lambda}-\lambda^{\prime}\right\|_{2}^{2}.\]

Proof of (ii)Similarly, for the mean-field family, we can apply Lemma 7 as

\[\mathbb{E}\left(1+\left\|\mathbf{\lambda}^{2}\right\|_{\text{F}}\right)\left\| \mathcal{J}_{\lambda-\lambda^{\prime}}\left(\mathbf{u}\right)-\mathbf{z}\right\|_{2}^ {2} \leq\left(\sqrt{dk_{\varphi}}+k_{\varphi}\sqrt{d}+1\right)\left\| \left(\mathbf{m}-\mathbf{m}^{\prime}\right)-\mathbf{z}\right\|_{2}^{2}+\left(2k_{\varphi} \sqrt{d}+1\right)\left\|\mathbf{C}-\mathbf{C}^{\prime}\right\|_{\text{F}}^{2},\]

setting \(\mathbf{z}=\mathbf{0}\),

\[=\left(\sqrt{dk_{\varphi}}+k_{\varphi}\sqrt{d}+1\right)\left\|\mathbf{m}-\mathbf{m}^{ \prime}\right\|_{2}^{2}+\left(2k_{\varphi}\sqrt{d}+1\right)\left\|\mathbf{C}-\mathbf{ C}^{\prime}\right\|_{\text{F}}^{2},\]

and since \(k_{\varphi}\geq 3\) by the property of the kurtosis,

\[\leq\left(2k_{\varphi}\sqrt{d}+1\right)\left(\left\|\mathbf{m}-\mathbf{m}^{\prime} \right\|_{2}^{2}+\left\|\mathbf{C}\left(\lambda\right)-\mathbf{C}\left(\lambda^{\prime }\right)\right\|_{\text{F}}^{2}\right)\]

\[=\left(2k_{\varphi}\sqrt{d}+1\right)\left\|\mathbf{\lambda}-\lambda^{\prime} \right\|_{2}^{2}.\]

**Lemma 8**.: _For the linear parameterization,_

\[\mathbb{E}\left\|\mathcal{J}_{\lambda}\left(\mathbf{u}\right)-\mathcal{J}_{\lambda^ {\prime}}\left(\mathbf{u}\right)\right\|_{2}^{2}=\left\|\mathbf{\lambda}-\lambda^{ \prime}\right\|_{2}^{2}\]

_for any \(\mathbf{\lambda},\mathbf{\lambda}^{\prime}\in\Lambda\)._

Proof.: First notice that, for linear parameterizations, we have

\[\mathbb{E}\left\|\mathcal{J}_{\lambda}\left(\mathbf{u}\right)\right\|_{2}^{2} =\mathbb{E}\left\|\mathbf{C}\mathbf{u}+\mathbf{m}\right\|_{2}^{2}\]

\[=\mathbb{E}\mathbf{u}^{\top}\mathbf{C}^{\top}\mathbf{C}\mathbf{u}+\left\|\mathbf{m}\right\|_{2}^{ 2}+2\mathbf{m}^{\top}\mathbf{C}\mathbb{E}\mathbf{u}\]

\[=\mathbb{E}\mathbf{\text{tr}}\left(\mathbf{u}^{\top}\mathbf{C}^{\top}\mathbf{C}\mathbf{u}\right)+ \left\|\mathbf{m}\right\|_{2}^{2}+2\mathbf{m}^{\top}\mathbf{C}\mathbb{E}\mathbf{u},\]

rotating the elements of the trace,

\[=\text{tr}\left(\mathbf{C}^{\top}\mathbf{C}\mathbf{E}\mathbf{u}^{\top}\right)+\left\|\mathbf{m} \right\|_{2}^{2}+2\mathbf{m}^{\top}\mathbf{C}\mathbb{E}\mathbf{u},\]

applying Assumption 1

\[=\text{tr}\left(\mathbf{C}^{\top}\mathbf{C}\right)+\left\|\mathbf{m}\right\|_{2 }^{2}\] \[=\left\|\mathbf{C}\right\|_{\text{F}}^{2}+\left\|\mathbf{m}\right\|_{2}^{2}\] \[=\left\|\mathbf{\lambda}\right\|_{2}^{2}.\]

Combined with Lemma 5, we have

\[\mathbb{E}\left\|\mathcal{J}_{\lambda}\left(\mathbf{u}\right)-\mathcal{J}_{\lambda^ {\prime}}\left(\mathbf{u}\right)\right\|_{2}^{2}=\mathbb{E}\left\|\mathcal{J}_{ \lambda-\lambda^{\prime}}\left(\mathbf{u}\right)\right\|_{2}^{2}=\left\|\mathbf{ \lambda}-\lambda^{\prime}\right\|_{2}^{2}.\]

### Properties of the Evidence Lower Bound

#### f.2.1 Smoothness

**Lemma 1**.: _If the diagonal conditioner \(\phi\) is \(L_{h}\)-log-smooth, then the entropic regularizer \(h\left(\lambda\right)\) is \(L_{h}\)-smooth._

Proof.: The entropic regularizer is

\[h\left(\lambda\right)=-\mathrm{H}\left(\varphi\right)-\sum_{i=1}^{d}\log\phi \left(s_{i}\right),\]

and depends only on the diagonal elements \(s_{1},...\,,s_{d}\) of \(\mathcal{C}\). The Hessian of \(h\) is then a diagonal matrix, where only the entries that correspond to \(s_{1},...\,,s_{d}\) are non-zero. The Lipschitz smoothness constant is then the constant \(L_{h}<\infty\) that satisfies

\[\frac{\delta^{2}h\left(\lambda\right)}{\delta s_{i}^{2}}=-\frac{\mathrm{d}^{2} \log\phi}{\mathrm{d}s_{i}^{2}}<L_{h}\]

for all \(i=1,...\,,d\), which is the smoothness constant of \(s_{i}\mapsto\log\phi\left(s_{i}\right)\). 

**Lemma 2**.: _Let \(\mathbf{H}\) be a \(n\times n\) symmetric random matrix, where it is bounded as \(\left\|\mathbf{H}\right\|_{2}\leq L<\infty\) almost surely. Also, let \(\mathbf{J}\) be an \(m\times n\) random matrix such that \(\left\|\mathbb{E}J^{\mathsf{T}}\mathbf{J}\right\|_{2}<\infty\). Then,_

\[\left\|\mathbb{E}J^{\mathsf{T}}\mathbf{H}\mathbf{J}\right\|_{2}\leq L\left\|\mathbb{E} J^{\mathsf{T}}\mathbf{J}\right\|_{2}.\]

Proof.: By the property of the Rayleigh quotients, for a symmetric matrix \(\mathbf{A}\), its maximum eigenvalue is given in the variational form

\[\sup_{\left\|\mathbf{x}\right\|\leq 1}\mathbf{x}^{\mathsf{T}}\mathbf{H}\mathbf{x}=\sigma_{\max }\left(\mathbf{H}\right)\leq\sqrt{\sigma_{\max}\left(\mathbf{H}\right)^{2}}=\left\|\mathbf{ H}\right\|_{2},\]

where \(\sigma_{\max}\left(\mathbf{A}\right)\) is the maximal eigenvalue of \(\mathbf{A}\). Notice the relationship with the \(\mathcal{E}_{2}\)-operator norm. The inequality is strict only if all eigenvalues are negative.

From the property above,

\[\left\|\mathbb{E}J^{\mathsf{T}}\mathbf{H}\mathbf{J}\right\|_{2}=\sup_{\left\|\mathbf{x} \right\|_{2}\leq 1}\mathbf{x}^{\mathsf{T}}\left(\mathbb{E}J^{\mathsf{T}}\mathbf{H}\mathbf{J} \right)\mathbf{x}.\]

By reparameterizing as \(\mathbf{y}=\mathbf{J}\mathbf{x}\),

\[=\sup_{\left\|\mathbf{x}\right\|_{2}\leq 1}\mathbb{E}y^{\mathsf{T}}\mathbf{H}\mathbf{y},\]

and the property of the \(\mathcal{E}_{2}\)-operator norm,

\[\leq\sup_{\left\|\mathbf{x}\right\|_{2}\leq 1}\mathbb{E}\left\|\mathbf{H}\right\|_{2} \left\|\mathbf{y}\right\|_{2}^{2}=\sup_{\left\|\mathbf{x}\right\|_{2}\leq 1}\mathbb{E} \left\|\mathbf{H}\right\|_{2}\left\|\mathbf{J}\mathbf{x}\right\|_{2}^{2}.\]

From our assumption about the maximal eigenvalue of \(\mathbf{H}\),

\[\leq L\sup_{\left\|\mathbf{x}\right\|_{2}\leq 1}\mathbb{E}\left\|\mathbf{J}\mathbf{x} \right\|_{2}^{2},\]

denoting the \(\mathcal{E}_{2}\) vector norm as a quadratic form as,

\[=L\sup_{\left\|\mathbf{x}\right\|_{2}\leq 1}\mathbf{x}^{\mathsf{T}}\left(\mathbb{E}J^{ \mathsf{T}}\mathbf{J}\right)\mathbf{x},\]

again, by the property of the \(\mathcal{E}_{2}\)-operator norm,

\[\leq L\left\|\mathbb{E}J^{\mathsf{T}}\mathbf{J}\right\|_{2}\sup_{ \left\|\mathbf{x}\right\|_{2}\leq 1}\left\|\mathbf{x}\right\|_{2}^{2}\] \[=L\left\|\mathbb{E}J^{\mathsf{T}}\mathbf{J}\right\|_{2}.\]

[MISSING_PAGE_FAIL:25]

Finally, the elements of \(\mathbf{J}_{\mathbf{L}}^{\mathsf{T}}\mathbf{J}_{\mathbf{L}}\) are

\[\mathbb{E}\sum_{l=0}^{d}\frac{\delta\mathcal{T}_{l}(\mathbf{\lambda};\mathbf{u})}{\delta L _{jk}}\frac{\delta\mathcal{T}_{l}(\mathbf{\lambda};\mathbf{u})}{\delta L_{lm}}=\mathbb{ E}\sum_{l=0}^{d}u_{k}u_{m}\mathbb{I}_{l=j}\mathbb{I}_{l=l}=\mathbb{I}_{j=l}( \mathbb{E}u_{k}u_{m})=\mathbb{I}_{j=l}\mathbb{I}_{k=m},\]

where the last equality follows from Assumption 1, which forms an identity matrix as

\[\mathbb{E}\mathbf{J}_{\mathbf{L}}^{\mathsf{T}}\mathbf{J}_{\mathbf{L}}=\mathbf{\mathrm{I}}.\]

Therefore, the expected-squared Jacobian is now

\[\mathbb{E}\left(\frac{\delta\mathcal{T}_{l}(\mathbf{u})}{\delta\mathbf{ \lambda}}\right)^{\mathsf{T}}\frac{\delta\mathcal{T}_{l}(\mathbf{u})}{\delta\mathbf{ \lambda}} =\begin{bmatrix}\mathbf{\mathrm{I}}&\mathbb{E}\mathbf{J}_{\mathbf{s}}&\mathbb{ E}\mathbf{J}_{\mathbf{L}}\\ \mathbb{E}\mathbf{J}_{\mathbf{s}}^{\mathsf{T}}&\mathbb{E}\mathbf{J}_{\mathbf{s}}^{\mathsf{T}} \mathbf{J}_{\mathbf{s}}&\mathbb{E}\mathbf{J}_{\mathbf{s}}^{\mathsf{T}}\mathbf{J}_{\mathbf{L}}\\ \mathbb{E}\mathbf{J}_{\mathbf{L}}^{\mathsf{T}}&\mathbb{E}\mathbf{J}_{\mathbf{L}}^{\mathsf{T}} \mathbf{J}_{\mathbf{s}}&\mathbb{E}\mathbf{J}_{\mathbf{L}}^{\mathsf{T}}\mathbf{J}_{\mathbf{L}}\end{bmatrix}\] \[=\begin{bmatrix}\mathbf{\mathrm{I}}&\\ &\mathrm{diag}\left(\mathbf{\phi}(\mathbf{s})\right)^{2}&\\ &\mathbf{\mathrm{I}}\end{bmatrix},\]

which, conveniently, is a diagonal matrix. The maximal singular value of a block-diagonal matrix is the maximal singular value of each block. And since each block is diagonal with only positive entries, the largest element forms the maximal singular value. As we assume that \(\mathbf{\phi}\) is 1-Lipchitz, the element of all blocks is lower-bounded by 0 and upper-bounded by 1. Therefore, the maximal singular value of the expected-squared Jacobian is bounded by 1.

**Theorem 1**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and twice differentiable. Then, the following results hold:_

1. _If_ \(\phi\) _is linear, the energy_ \(f\) _is_ \(L_{\ell}\)_-smooth._
2. _If_ \(\phi\) _is 1-Lipschitz, the energy_ \(\ell\) _is_ \((L_{\ell}+L_{\mathrm{s}})\)_-smooth if and only if Assumption_ 3 _holds._

Proof.: For notational clarity, we will occasionally represent \(\mathcal{F}_{\lambda}\) as

\[\mathcal{F}_{\lambda}\left(\mathbf{u}\right)=\mathcal{F}\left(\mathbf{ \lambda};\mathbf{u}\right),\]

such that \(\mathcal{F}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)\) denotes the \(i\)th component of \(\mathcal{F}_{\lambda}\).

By the Leibniz and chain rule, the Hessian of the energy \(f\) follows as

\[\nabla^{2}f\left(\mathbf{\lambda}\right) =\mathbb{E}\nabla_{\lambda}^{2}\ell\left(\mathcal{F}_{\lambda} \left(\mathbf{u}\right)\right)\] \[=\underbrace{\mathbb{E}\left(\frac{\partial\mathcal{F}_{\lambda} \left(\mathbf{u}\right)}{\partial\mathbf{\lambda}}\right)^{\top}\nabla^{2}\ell\left( \mathcal{F}_{\lambda}\left(\mathbf{u}\right)\right)\frac{\partial\mathcal{F}_{ \lambda}\left(\mathbf{u}\right)}{\partial\mathbf{\lambda}}}_{\triangleq T_{\mathrm{in}}} +\underbrace{\mathbb{E}\sum_{l=1}^{d}D_{l}\ell\left(\mathcal{F}_{\lambda} \left(\mathbf{u}\right)\right)\frac{\partial^{2}\mathcal{F}_{l}\left(\mathbf{\lambda}; \mathbf{u}\right)}{\partial\mathbf{\lambda}^{2}}}_{\triangleq T_{\mathrm{non}}}.\]

When \(\mathcal{F}\) is linear with respect to \(\mathbf{\lambda}\), it is clear that we have

\[\frac{\partial^{2}\mathcal{F}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial \mathbf{\lambda}^{2}}=\mathbf{0}. \tag{5}\]

Then, \(T_{\mathrm{non}}\) is zero. In contrast, \(T_{\mathrm{in}}\) appears for both the linear and nonlinear cases. Therefore, \(T_{\mathrm{non}}\) fully characterizes the effect of nonlinearity in the reparameterization function.

Now, the triangle inequality yields

\[\left\|\nabla^{2}f\left(\mathbf{\lambda}\right)\right\|_{2}=\left\|T _{\mathrm{in}}+T_{\mathrm{non}}\right\|_{2}\leq\left\|T_{\mathrm{in}}\right\| _{2}+\left\|T_{\mathrm{non}}\right\|_{2},\]

where equality is achieved when either term is \(0\). On the contrary, the reverse triangle inequality states that

\[\left\|\left|T_{\mathrm{in}}\right|\right\|_{2}-\left\|T_{\mathrm{ non}}\right\|_{2}\right|\leq\left\|\nabla^{2}f\left(\mathbf{\lambda}\right)\right\|_ {2}.\]

This implies that, if either \(T_{\mathrm{in}}\) or \(T_{\mathrm{non}}\) is unbounded, the Hessian is not bounded. Thus, ensuring that \(T_{\mathrm{in}}\) and \(T_{\mathrm{non}}\) are bounded is sufficient and necessary to establish that \(f\) is smooth.

Proof of (i)The bound on the linear part, \(T_{\mathrm{in}}\), follows from Lemma 2 as

\[\left\|T_{\mathrm{in}}\right\|_{2} =\left\|\mathbb{E}\left(\frac{\partial\mathcal{F}_{\lambda}\left( \mathbf{u}\right)}{\partial\mathbf{\lambda}}\right)^{\top}\nabla^{2}\ell\left( \mathcal{F}_{\lambda}\left(\mathbf{u}\right)\right)\frac{\partial\mathcal{F}_{ \lambda}\left(\mathbf{u}\right)}{\partial\mathbf{\lambda}}\right\|_{2}\] \[\leq L_{\ell}\left\|\mathbb{E}\left(\frac{\partial\mathcal{F}_{ \lambda}\left(\mathbf{u}\right)}{\partial\mathbf{\lambda}}\right)^{\top}\frac{ \partial\mathcal{F}_{\lambda}\left(\mathbf{u}\right)}{\partial\mathbf{\lambda}}\right\| _{2},\]

and from the 1-Lipschitzness of \(\phi\), Lemma 9 yields

\[\leq L_{\ell}.\]

When \(\phi\) is linear, it immediately follows from Equation (5) that

\[\left\|\nabla^{2}f\left(\mathbf{\lambda}\right)\right\|_{2}=\left\|T _{\mathrm{in}}\right\|_{2}\leq L_{\ell},\]

which is tight as shown by Domke (2020, Theorem 6).

Proof of (ii)For the nonlinear part \(T_{\mathrm{non}}\), we use the fact that \(\mathcal{F}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)\) is given as

\[\mathcal{F}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)=m_{l}+\phi\left(s _{l}\right)u_{l}+\sum_{j\neq i}L_{ij}u_{j}.\]The second derivative of \(\mathcal{T}_{l}\) is clearly non-zero only for the nonlinear part involving \(s_{1},...,s_{d}\). Thus, \(T_{\text{non}}\) follows as

\[T_{\text{non}} =\mathbb{E}\sum_{i=1}^{d}\operatorname{D}_{l}\mathscr{E}\left( \mathcal{T}_{\mathbf{i}}\left(\mathbf{u}\right)\right)\frac{\partial^{2}\mathcal{T }_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial\lambda^{2}}\] \[=\mathbb{E}\sum_{i=1}^{d}g_{i}\left(\mathbf{\lambda};\mathbf{u}\right) \frac{\partial^{2}\mathcal{T}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial \lambda^{2}}\] \[=\begin{bmatrix}\cdot&\cdot&\cdot\\ \cdot&\mathbb{E}\sum_{i=1}^{d}g_{i}\left(\mathbf{\lambda};\mathbf{u}\right)\frac{ \partial^{2}\mathcal{T}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial\lambda^ {2}}&\cdot\\ \cdot&\cdot&\cdot\end{bmatrix}.\]

Furthermore, the second-order derivatives with respect to \(s_{1},...,s_{d}\) are given as

\[\frac{\partial^{2}\mathcal{T}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial s _{j}^{2}}=\operatorname{I}_{i=j}\phi^{\ast}\left(s_{j}\right).\]

Considering this, the only non-zero block of \(T_{\text{non}}\) forms a diagonal matrix as

\[\mathbb{E}\sum_{i=1}^{d}g_{i}\left(\mathbf{\lambda};\mathbf{u}\right) \frac{\partial^{2}\mathcal{T}_{l}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial s} =\begin{bmatrix}\mathbb{E}g_{1}\left(\mathbf{\lambda};\mathbf{u}\right) \frac{\partial^{2}\mathcal{T}_{1}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial s _{1}^{2}}&\\ &\ddots&\\ &&\mathbb{E}g_{d}\left(\mathbf{\lambda};\mathbf{u}\right)\frac{\partial^{2}\mathcal{T} _{d}\left(\mathbf{\lambda};\mathbf{u}\right)}{\partial s_{d}^{2}}\end{bmatrix}\] \[=\begin{bmatrix}\mathbb{E}g_{1}\left(\mathbf{\lambda};\mathbf{u}\right) \phi^{\ast\prime}\left(s_{1}\right)u_{1}&\\ &\ddots&\\ &&\mathbb{E}g_{d}\left(\mathbf{\lambda};\mathbf{u}\right)\phi^{\ast\prime}\left(s_{d} \right)u_{d}\end{bmatrix}\]

This implies that the only non-zero entries of \(T_{\text{non}}\) lie on its diagonal. Since the \(\ell_{2}\) norm of a diagonal matrix is the value of the maximal diagonal element,

\[\left\lVert T_{\text{non}}\right\rVert_{2}\leq\max_{i=1,...,d}\mathbb{E}g_{i }\left(\mathbf{\lambda};\mathbf{u}\right)\phi^{\ast\prime}\left(s_{1}\right)u_{1}\leq L _{s},\]

where \(L_{s}\) is finite constant if Assumption 3 holds. On the contrary, if a finite \(L_{s}\) does not exist, \(\left\lVert T_{\text{non}}\right\rVert_{2}\) cannot be bounded. Therefore, the energy is smooth if and only if Assumption 3 holds. When it does, the energy \(f\) is \(L_{f}+L_{s}\) smooth.

**Example 2**.: Let \(\ell\left(\mathbf{z}\right)=\left(\nicefrac{{1}}{{2}}\right)\mathbf{z}^{\top} \mathbf{A}\mathbf{z}\) and the diagonal conditioner be \(\phi\left(x\right)=\text{softplus}\left(x\right)\). Then,

1. if \(\mathbf{A}\) is dense and the variational family is the mean-field family or
2. if \(\mathbf{A}\) is diagonal and the variational family is the Cholesky family,

Assumption 3 holds with \(L_{\text{s}}\approx 0.26034\left(\max_{i=1,\ldots,d}A_{ii}\right)\).
3. If \(\mathbf{A}\) is dense but the Cholesky family is used, Assumption 3 does not hold.

Proof.: Since the gradient is

\[\nabla\mathcal{E}\left(\mathbf{z}\right)=\mathbf{A}\mathbf{z},\]

combined with reparameterization, we have

\[g\left(\lambda;\mathbf{u}\right)=\mathbf{A}\left(\mathbf{C}\mathbf{u}+ \mathbf{m}\right)\]

Then, for each coordinate \(i=1,\ldots,d\), we have

\[\mathbb{E}g_{i}\left(\lambda;\mathbf{u}\right)u_{i}\phi^{\prime \prime}(s_{i}) =\mathbb{E}\left(\sum_{j}\sum_{k\leq j}A_{ij}C_{jk}u_{k}+\sum_{j} A_{ij}m_{j}\right)u_{i}\phi^{\prime\prime}(s_{i})\] \[=\sum_{j}\sum_{k\leq j}A_{ij}C_{jk}\mathbb{E}u_{k}u_{i}\phi^{ \prime\prime}(s_{i})+\sum_{j}A_{ij}m_{j}\mathbb{E}u_{i}\phi^{\prime\prime}(s_ {i}),\]

and from Assumption 1,

\[=\phi^{\prime\prime}(s_{i})\sum_{j}\sum_{k\leq j}A_{ij}C_{jk} \mathbb{I}_{k=i}\] \[=\phi^{\prime\prime}(s_{i})\sum_{j}A_{ij}C_{ji}.\]

Furthermore, the diagonal of \(\mathbf{C}\) involves \(\phi\) such that

\[\mathbb{E}g_{i}\left(\lambda;\mathbf{u}\right)u_{i}\phi^{\prime\prime}(s_{i}) =\underbrace{A_{ii}\phi\left(s_{i}\right)\phi^{\prime\prime}\left(s_{i} \right)}_{T_{\text{diag}}}+\underbrace{\sum_{j<i}A_{ij}C_{ji}\phi^{\prime \prime}(s_{i})}_{T_{\text{off}}}.\]

For the softplus function, we have

\[0<\phi^{\prime\prime}\left(s\right)<1\]

for any finite \(s\), and we have

\[\sup_{s}\phi\left(s\right)\phi^{\prime\prime}\left(s\right)\approx 0.26034,\]

where the supremum was numerically approximated. Then, it is clear that \(T_{\text{diag}}\) is finite as long as the diagonals of \(\mathbf{A}\) are finite. Furthermore, we have the following:

1. If \(\mathbf{A}\) is diagonal, then \(T_{\text{off}}\) is \(0\).
2. If \(\mathbf{A}\) is dense but \(\mathbf{C}\) is diagonal due to the use of the mean-field family, \(T_{\text{off}}\) is again \(0\).
3. However, when both \(\mathbf{A}\) and \(\mathbf{C}\) are not diagonal, \(T_{\text{off}}\) can be made arbitrarily large.

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_FAIL:31]

**Lemma 11**.: _For any function \(f\in\mathrm{C}^{1}(\mathbb{R},\mathbb{R}_{+})\), there is no constant \(0<L<\infty\) such that_

\[|f(x)-f(y)|\geq L|x-y|.\]

Proof.: Suppose for the sake of contradiction that such \(L>0\) exists. Letting \(y\to x\) gives \(|f^{\prime}(x)|\geq L\) for all \(x\in\mathbb{R}\). For each \(x\), either \(f^{\prime}(x)\leq-L\) or \(f^{\prime}(x)\geq L\) holds. We discuss two cases based on the value of \(f^{\prime}(0)\).

If \(f^{\prime}(0)\geq L\), we claim that \(f^{\prime}(x)\geq L\) for all \(x\in\mathbb{R}\). Otherwise, \(f^{\prime}(x)<L\) for some \(x\) implies \(f^{\prime}(x)\leq-L\). By the intermediate value theorem (\(f^{\prime}\) is continuous), there exists a point \(y\) between \(0\) and \(x\) that attains the value \(f^{\prime}(y)=0\), which is a contradiction.

Now that \(f^{\prime}(x)\geq L>0\) for all \(x\), \(f\) is an increasing function. For any \(x<0\), we have

\[f(x) =f(x)-f(0)+f(0)\] \[=-|f(x)-f(0)|+f(0)\] \[\leq-L|x|+f(0).\]

Here, we can plug \(x^{\prime}=-\frac{f(0)}{L}\) as

\[f(x^{\prime})=-L\Big{|}-\frac{f(0)}{L}\Big{|}+f(0)=-|f(0)|+f(0)\leq 0,\]

which implies that \(f(x^{\prime})\not\in\mathbb{R}_{+}\), which is a contradiction.

Now we discuss the second case \(f^{\prime}(0)\leq-L\). By a similar argument, \(f^{\prime}(x)\leq-L\) for all \(x\in\mathbb{R}\). Thus, \(f\) is a decreasing function. For any \(x>0\), we have

\[f(x) =f(x)-f(0)+f(0)\] \[=-|f(x)-f(0)|+f(0)\] \[\leq-Lx+f(0).\]

Picking \(x^{\prime}=\frac{f(0)}{L}\) results in \(f(x^{\prime})\not\in\mathbb{R}_{+}\), which is a contradiction.

**Theorem 2**.: _Let \(\ell\) be \(\mu\)-strongly convex. Then, we have the following:_

1. _If_ \(\phi\) _is linear, the energy_ \(f\) _is_ \(\mu\)_-strongly convex._
2. _If_ \(\phi\) _is convex, the energy_ \(f\) _is convex if and only if Assumption_ 4 _holds._
3. _If_ \(\phi\) _is such that_ \(\phi\in C^{1}\left(\mathbb{R},\mathbb{R}_{+}\right)\)_, the energy_ \(f\) _is not strongly convex._

Proof.: The special case **(i)** is proven by Domke (2020, Theorem 9). We focus on the general statement **(ii)**.

If \(\ell\) is \(\mu\)-strongly convex, the inequality

\[\ell\left(\mathbf{z}\right)-\ell\left(\mathbf{z}^{\prime}\right)\geq\left\langle \nabla\ell\left(\mathbf{z}^{\prime}\right),\mathbf{z}-\mathbf{z}^{\prime}\right\rangle+ \frac{\mu}{2}\|\mathbf{z}-\mathbf{z}^{\prime}\|_{2}^{2} \tag{8}\]

holds, where the general convex case is obtained as a special case with \(\mu=0\). The goal is to relate this to the (\(\mu\)-strong-)convexity of the energy with respect to the variational parameters given by

\[f\left(\mathbf{\lambda}\right)-f\left(\mathbf{\lambda}^{\prime}\right)\geq\left\langle \nabla_{\mathbf{\lambda}}f\left(\mathbf{\lambda}^{\prime}\right),\mathbf{\lambda}-\mathbf{ \lambda}^{\prime}\right\rangle+\frac{\mu}{2}\|\mathbf{\lambda}-\mathbf{\lambda}^{ \prime}\|_{2}^{2}.\]

Proof of **(ii)**Plugging the reparameterized latent variables to Equation (8) and taking the expectation, we have

\[\mathbb{E}\ell\left(\mathcal{F}_{\lambda}\left(\mathbf{u}\right) \right)-\mathbb{E}\ell\left(\mathcal{F}_{\lambda^{\prime}}\left(\mathbf{u}\right) \right) \geq\mathbb{E}\left\langle\nabla\ell\left(\mathcal{F}_{\lambda^{ \prime}}\left(\mathbf{u}\right)\right),\mathcal{F}_{\lambda}\left(\mathbf{u}\right)- \mathcal{F}_{\lambda^{\prime}}\left(\mathbf{u}\right)\right\rangle+\frac{\mu}{2} \mathbb{E}\|\mathcal{F}_{\lambda}\left(\mathbf{u}\right)-\mathcal{F}_{\lambda^{ \prime}}\left(\mathbf{u}\right)\|_{2}^{2}\] \[\Leftrightarrow f\left(\mathbf{\lambda}\right)-f\left(\mathbf{\lambda}^{\prime}\right) \geq\mathbb{E}\left\langle\nabla g\left(\lambda^{\prime};\mathbf{u} \right),\mathcal{F}_{\lambda}\left(\mathbf{u}\right)-\mathcal{F}_{\lambda^{ \prime}}\left(\mathbf{u}\right)\right\rangle+\frac{\mu}{2}\mathbb{E}\|\mathcal{F} _{\lambda}\left(\mathbf{u}\right)-\mathcal{F}_{\lambda^{\prime}}\left(\mathbf{u} \right)\|_{2}^{2}\]

Thus, the energy is convex if and only if

\[\mathbb{E}\left(g\left(\mathbf{\lambda};\mathbf{u}\right),\mathcal{F}_{\lambda}^{ \prime}\left(\mathbf{u}\right)-\mathcal{F}_{\lambda^{\prime}}\left(\mathbf{u}\right) \right)\geq\left\langle\nabla f\left(\mathbf{\lambda}\right),\mathbf{\lambda}-\mathbf{ \lambda}^{\prime}\right\rangle\]

holds. This is established by Lemma 10.

Proof of **(iii)**We now prove that, under the nonlinear parameterization, the energy cannot be strongly convex. When the energy is convex, it is also strongly convex if and only if

\[\frac{\mu}{2}\mathbb{E}\|\mathcal{F}_{\lambda}\left(\mathbf{u}\right)-\mathcal{F} _{\lambda^{\prime}}\left(\mathbf{u}\right)\|_{2}^{2}\geq\frac{\mu}{2}\|\mathbf{\lambda }-\mathbf{\lambda}^{\prime}\|_{2}^{2}.\]

From the proof of Domke (2020, Lemma 5), it follows that

\[\mathbb{E}\|\mathcal{F}_{\lambda}\left(\mathbf{u}\right)-\mathcal{F}_{\lambda^{ \prime}}\left(\mathbf{u}\right)\|_{2}^{2}=\|\mathbf{C}-\mathbf{C}^{\prime}\|_{\text{F}}^{ 2}+\|\mathbf{m}-\mathbf{m}^{\prime}\|_{2}^{2}.\]

Furthermore, under nonlinear parameterizations,

\[\|\mathbf{C}-\mathbf{C}^{\prime}\|_{\text{F}}^{2}+\|\mathbf{m}-\mathbf{m}^{\prime }\|_{2}^{2}\] \[=\left\|\left(\mathbf{D}_{\phi}\left(\mathbf{s}\right)-\mathbf{D}_{\phi}

[MISSING_PAGE_FAIL:34]

**Theorem 3**.: _Let Assumption 2 hold, the likelihood satisfy Assumption 5, and the assumptions of Corollary 1 hold such that the ELBO F is \(L_{F}\)-smooth with \(L_{F}=L_{\ell}+L_{\phi}+L_{S}\). Then, the iterates generated by BBVI through Equation (1) and the M-sample reparameterization gradient include an \(\epsilon\)-stationary point such that \(\min_{0\leq t\leq T-1}\|\nabla F\left(A_{t}\right)\|_{2}\leq\epsilon\) for any \(\epsilon>0\) if_

\[T\geq\mathcal{O}\left(\frac{\left(F\left(A_{0}\right)-F^{*}\right)^{2}L_{F}L_ {2}^{2}\mathcal{C}\left(d,k_{\varphi}\right)}{\mu M\epsilon^{4}}\right)\]

_for some fixed stepsize \(\gamma\), where \(C\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family and \(C\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: As a corollary to Theorem 5, Khaled & Richtarik (2023, Corollary 1) show that, for an \(L_{F}\)-smooth objective function \(F\), a gradient estimator satisfying the \(ABC\) condition, an \(\epsilon\)-stationary point can be encountered if

\[\gamma=\min\left(\frac{1}{\sqrt{L_{F}AT}},\frac{1}{L_{F}B},\frac{\epsilon}{2L _{F}C}\right),\qquad T\geq\frac{12\left(F\left(\lambda_{0}\right)-F^{*}\right) L_{F}}{\epsilon^{2}}\max\left(B,\frac{12\left(F\left(\lambda_{0}\right)-F^{*} \right)A}{\epsilon^{2}},\frac{2C}{\epsilon^{2}}\right).\]

Under Assumption 5, Kim _et al._ (2023) show that the Monte Carlo gradient estimates satisfy

\[\mathbb{E}\|\widehat{\nabla F}\left(\lambda\right)\|_{2}^{2} \leq\frac{4L_{\phi}^{2}\mathcal{C}\left(d,\varphi\right)}{\mu M} \left(F\left(\lambda\right)-F^{*}\right)+B\|\nabla F\|_{2}^{2}\] \[\quad+\frac{2L_{\phi}^{2}\mathcal{C}\left(d,\varphi\right)}{\mu M }\|\boldsymbol{\bar{z}}_{\text{joint}}-\boldsymbol{\bar{z}}_{\text{ikc}}\|_{2 }^{2}+\frac{4L_{\phi}^{2}\mathcal{C}\left(d,\varphi\right)}{\mu M}\left(F^{*}- \mathcal{E}_{\text{like}}^{*}\right),\]

This means that the \(ABC\) condition is satisfied with constants

\[A=\frac{4L_{f}^{2}}{\mu M}C\left(d,\varphi\right),\qquad B=1,\qquad C=\frac{2L _{f}^{2}}{\mu M}C\left(d,\varphi\right)\left(\|\boldsymbol{\bar{z}}_{\text{ joint}}-\boldsymbol{\bar{z}}_{\text{ikc}}\|_{2}^{2}+2\left(F^{*}-f_{\perp}^{*} \right)\right).\]

where

\[\boldsymbol{\bar{z}}_{\text{joint}} =\text{proj}_{\ell}\left(\boldsymbol{z}\right) \text{is the projection of }\boldsymbol{z}\text{ onto set of minimizers of }\mathcal{E}\] \[\boldsymbol{\bar{z}}_{\text{like}} =\text{proj}_{\ell_{\text{like}}}\left(\boldsymbol{z}\right) \text{is the projection of }\boldsymbol{z}\text{ onto set of minimizers of }\mathcal{E}_{\text{like}},\] \[F^{*} =\inf_{\lambda\in\mathbb{A}}F\left(\lambda\right),\] \[e_{\text{like}}^{*} =\inf_{\lambda\in\mathbb{A}}\mathcal{E}_{\text{like}}\left( \boldsymbol{z}\right),\] \[C(d,\varphi) =d+k_{\varphi} \text{for the Cholesky family},\] \[C(d,\varphi) =2k_{\varphi}\sqrt{d}+1 \text{for the mean-field family},\] \[M \text{is the number of Monte Carlo samples}.\]

Plugging these constants in, we obtain

\[T \geq\frac{12\left(F\left(\lambda_{0}\right)-F^{*}\right)L_{F}}{ \epsilon^{2}}\max\left(1,\frac{48\left(F\left(\lambda_{0}\right)-F^{*}\right) L_{\phi}^{2}\mathcal{C}\left(d,\varphi\right)}{\mu M\epsilon^{2}},\frac{8L_{\phi}^{2} \mathcal{C}\left(d,\varphi\right)\left(\|\boldsymbol{\bar{z}}_{\text{joint}}- \boldsymbol{\bar{z}}_{\text{like}}\|_{2}^{2}+\left(F^{*}-\mathcal{E}_{\text{ like}}^{*}\right)\right)}{\mu M\epsilon^{2}}\right)\] \[=\mathcal{O}\left(\frac{\left(F\left(\lambda_{0}\right)-F^{*} \right)^{2}L_{F}L_{\phi}^{2}\mathcal{C}\left(d\right)}{\mu M\epsilon^{4}} \right),\]

where we omitted the dependence on \(k_{\varphi}\) and the minimizers of \(\mathcal{E}\) and \(\mathcal{E}_{\text{like}}\).

[MISSING_PAGE_FAIL:36]

**Lemma 12** (**Variance Transfer**).: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex with the variational family satisfying Assumption 2 with the linear parameterization. Also, let \(\widehat{\nabla f}\) be an M-sample gradient estimator of the energy. Then,_

\[\operatorname{tr}\mathbb{V}\widehat{\nabla f}\left(\lambda\right)\leq\frac{4L_ {\ell}\kappa\,C\left(d,\varphi\right)}{M}\operatorname{B}_{f}\left(\lambda, \lambda^{\prime}\right)+2\operatorname{tr}\mathbb{V}\widehat{\nabla f}\left( \lambda^{\prime}\right),\]

\(\kappa=L_{\ell}/\mu\) _is the condition number, \(\operatorname{B}_{f}\) is the Bregman divergence defined in Lemma 3, \(C\left(d,\varphi\right)=d+k_{\varphi}\) for the Cholesky family, and \(C\left(d,\varphi\right)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: First, the \(M\)-sample gradient estimator is defined as

\[\widehat{\nabla f}\left(\lambda\right)=\frac{1}{M}\sum_{m=1}^{M}\nabla_{ \lambda}f\left(\lambda;\mathbf{u}_{m}\right),\]

where \(\mathbf{u}_{m}\sim\varphi\). Since \(\mathbf{u}_{1},\ldots,\mathbf{u}_{m}\) are independent and identically distributed, we have

\[\operatorname{tr}\mathbb{V}\widehat{\nabla f}\left(\lambda\right)=\frac{1}{M} \operatorname{tr}\mathbb{V}\nabla_{\lambda}f\left(\lambda;\mathbf{u}\right).\]

From here, given Lemma 3, the proof is identical with that of Garrigos & Gower (2023, Lemma 8.20), except for the constants. 

**Theorem 6**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex. Then, BBVI with proximal SGD in Equation (2), \(M\)-Monte Carlo samples, a variational family satisfying Assumption 2, the linear parameterization, and a fixed stepsize \(0<\gamma\leq\frac{M}{2L_{\ell}\,\kappa\,C(d,\varphi)}\), the iterates satisfy_

\[\mathbb{E}\|\lambda_{T}-\lambda^{*}\|_{2}^{2}\leq(1-\gamma\mu)^{T}\|\lambda_{0 }-\lambda^{2}\|_{2}^{2}+\frac{2\gamma\sigma^{2}}{\mu},\]

_where \(\kappa=L_{\ell}/\mu\) is the condition number, \(\sigma^{2}\) is defined in Lemma 4, \(\lambda^{*}=\operatorname*{arg}\min_{\lambda\in\Lambda}F(\lambda)\), \(C(d,\varphi)=d+k_{\varphi}\) for the Cholesky family, and \(C(d,\varphi)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: Provided that

1. the energy \(f\) is \(\mu\)-strongly convex,
2. the energy \(f\) is \(L_{\ell}\)-smooth,
3. the regularizer \(h\) is convex,
4. the regularizer \(h\) is lower semi-continuous,
5. the convex expected smoothness condition holds,
6. the variance transfer condition holds, and
7. the gradient variance \(\sigma^{2}\) at the optimum is finite such that \(\sigma^{2}<\infty\),

the proof is identical to that of Garrigos & Gower (2023, Theorem 11.9), which is based on the results of Gorbunov _et al._ (2020, Corollary A.2).

In our setting,

1. is established by Theorem 2,
2. is established by Theorem 1,
3. is trivially satisfied since \(h\) is the negative entropy,
4. is trivially satisfied since \(h\) is continuous,
5. is established in Lemma 3,
6. is established in Lemma 12,
7. is established in Lemma 4.

The only difference is that, we replace the constant \(L_{\max}\) in the proof of Garrigos & Gower to \(L_{\ell}\kappa\,C\left(d,\varphi\right)/M\). This stems from the different constants in the variance transfer condition.

**Theorem 7**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex. Then, for any \(\epsilon>0\), BBVI with proximal SGD in Equation2, M-Monte Carlo samples, a variational family satisfying Assumption2, and the linear parameterization guarantees \(\mathbb{E}\|\mathbf{\lambda}_{T}-\mathbf{\lambda}^{*}\|_{2}^{2}\leq\epsilon\) if_

\[\gamma=\min\left(\frac{\epsilon}{2}\frac{\mu}{2\sigma^{2}},\frac{M}{2L_{\ell} \kappa\,C\left(d,\varphi\right)}\right),\qquad T\geq\max\left(\frac{1}{\epsilon }\frac{4\sigma^{2}}{\mu^{2}},\frac{2\kappa^{2}\,C\left(d,\varphi\right)}{M} \right)\log\left(\frac{2\|\mathbf{\lambda}_{0}-\mathbf{\lambda}^{*}\|}{\epsilon}\right),\]

_where \(\kappa=L_{\ell}/\mu\), \(\sigma^{2}\) is defined in Lemma4, \(\mathbf{\lambda}^{*}=\arg\min_{\mathbf{\lambda}\in\mathbf{\Lambda}}F(\mathbf{\lambda})\), \(C(d,\varphi)=d+k_{\varphi}\) for the Cholesky family, and \(C(d,\varphi)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: This is a corollary of the fixed stepsize convergence guarantee in Theorem6 as shown by Garrigos & Gower (2023, Corollary 11.10). They guarantee an \(\epsilon\)-accurate solution as long as

\[\gamma=\min\left(\frac{\epsilon}{2}\frac{2}{2\sigma_{\mathrm{F}}^{*}},\frac{1 }{2L_{\max}}\right),\quad T\geq\max\left(\frac{1}{\epsilon}\frac{4\sigma_{ \mathrm{F}}^{*}}{\mu^{2}},\frac{2L_{\max}}{\mu}\right)\log\left(\frac{2\|\mathbf{ \lambda}_{0}-\mathbf{\lambda}^{*}\|}{\epsilon}\right).\]

In our notation, \(\sigma_{\mathrm{F}}^{*}=\sigma^{2}\) and \(L_{\max}=L_{\ell}\kappa\,C\left(d,\varphi\right)/M\). 

**Theorem 8**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex. Then, BBVI with proximal SGD in Equation2, the M-sample reparameterization gradient estimator, a variational family satisfying Assumption2, the linear parameterization, \(T\geq 4T_{\kappa}\), and a stepsize schedule of_

\[\gamma_{t}=\begin{cases}\frac{M}{2L_{\ell}\kappa\,C\left(d,\varphi\right)}& \text{for}\quad t\leq 4T_{\kappa}\\ \frac{2t+1}{\left(t+1\right)^{2}\mu}&\text{for}\quad t>4T_{\kappa},\end{cases}\]

_where \(T_{\kappa}=\lceil\kappa^{2}C\left(d,\varphi\right)M^{-1}\rceil\), \(\kappa=L_{\ell}/\mu\) is the condition number, \(C(d,\varphi)=d+k_{\varphi}\) for the Cholesky family, and \(C(d,\varphi)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family, then the iterates satisfy_

\[\mathbb{E}\|\mathbf{\lambda}_{T}-\mathbf{\lambda}^{*}\|_{2}^{2}\leq\frac{16\,T_{\kappa }^{2}\,\|\mathbf{\lambda}_{0}-\mathbf{\lambda}^{*}\|_{2}^{2}}{\mathrm{e}^{2}T^{2}}+ \frac{8\sigma^{2}}{\mu^{2}T}\]

_where \(\sigma^{2}\) is defined in Lemma4, \(\mathrm{e}\) is Euler \(\hat{\gamma}\) constant, and \(\mathbf{\lambda}^{*}=\arg\min_{\mathbf{\lambda}\in\mathbf{\Lambda}}F(\mathbf{\lambda})\)._

Proof.: Under our assumptions, Theorem6 holds, of which the proof is essentially obtaining the recursion

\[\mathbb{E}\|\mathbf{\lambda}_{t+1}-\mathbf{\lambda}^{*}\|_{2}^{2}=\left(1-\gamma_{t} \mu\right)\mathbb{E}\|\mathbf{\lambda}_{t}-\mathbf{\lambda}^{*}\|_{2}^{2}+2\gamma_{t} ^{2}\sigma^{2}.\]

Instead of a fixed stepsize, we can apply the decreasing stepsize rule in the proof statement, then which the proof becomes identical to that of Gower _et al._ (2019, Theorem 3.2). We only need to replace \(\mathcal{L}\) with \(L_{\max}\) in the proof of Garrigos & Gower (2023, Theorem 11.9). This, in our notation, is \(L_{\max}=L_{\ell}\kappa\,C\left(d,\varphi\right)/M\). 

**Theorem 4**.: _Let \(\ell\) be \(L_{\ell}\)-smooth and \(\mu\)-strongly convex. Then, for any \(\epsilon>0\), BBVI with proximal SGD in Equation2, the M-sample reparameterization gradient estimator, a variational family satisfying Assumption2 with the linear parameterization guarantees \(\mathbb{E}\|\mathbf{\lambda}_{T}-\mathbf{\lambda}^{*}\|_{2}^{2}\leq\epsilon\) if_

\[\gamma_{t}=\begin{cases}\frac{M}{2L_{\ell}\kappa\,C\left(d,\varphi\right)}& \text{for}\quad t\leq 4T_{\kappa}\\ \frac{2\,\mathbb{I}+1}{\left(t+1\right)^{2}\mu}&\text{for}\quad t>4T_{\kappa},\end{cases}\qquad T\geq\max\left(\frac{8\sigma^{2}}{\mu^{2}\,\epsilon}+ \frac{4T_{\kappa}\|\mathbf{\lambda}_{0}-\mathbf{\lambda}^{*}\|_{2}}{\mathrm{e}\sqrt{ \epsilon}},\ \ 4T_{\kappa}\right)\]

_where \(\sigma^{2}\) is defined in Lemma4, \(T_{\kappa}=\lceil\kappa^{2}C\left(d,\varphi\right)M^{-1}\rceil\), \(\kappa=L_{\ell}/\mu\) is the condition number, \(\mathrm{e}\) is Euler \(\hat{\gamma}\) constant, \(\mathbf{\lambda}^{*}=\arg\min_{\mathbf{\lambda}\in\mathbf{\Lambda}}F(\mathbf{\lambda})\), \(C(d,\varphi)=d+k_{\varphi}\) for the Cholesky family, and \(C(d,\varphi)=2k_{\varphi}\sqrt{d}+1\) for the mean-field family._

Proof.: The computational complexity follows from the smallest number of iterations \(T\) such that

\[\mathbb{E}\|\mathbf{\lambda}_{T}-\mathbf{\lambda}^{*}\|_{2}^{2}\leq\frac{16T_{\kappa}^{ 2}\|\mathbf{\lambda}_{0}-\mathbf{\lambda}^{*}\|_{2}^{2}}{\mathrm{e}^{2}T^{2}}+\frac{8 \sigma^{2}}{\mu^{2}T}\leq\epsilon\]By multiplying both sides with \(T^{2}\) as

\[T^{2}\varepsilon-\frac{8\sigma^{2}}{\mu^{2}}T-\frac{16T_{k}^{2}\|\lambda_{0}- \lambda^{*}\|_{2}^{2}}{\mathrm{e}^{2}}\geq 0, \tag{11}\]

we can see that we are looking for the smallest positive integer that is larger than the solution of a quadratic equation with respect to \(T\). This is given as

\[T\geq\frac{\frac{8\sigma^{2}}{\mu^{2}}+\sqrt{\left(\frac{8\sigma^{2}}{\mu^{2}} \right)^{2}+64\varepsilon\frac{T_{k}^{2}\|\lambda_{0}-\lambda^{*}\|_{2}^{2}}{ \varepsilon^{2}}}{2\varepsilon}.\]

Applying the inequality \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\),

\[\frac{\frac{8\sigma^{2}}{\mu^{2}}+\sqrt{\left(\frac{8\sigma^{2}}{ \mu^{2}}\right)^{2}+64\varepsilon\frac{T_{k}^{2}\|\lambda_{0}-\lambda^{*}\|_{ 2}^{2}}{\varepsilon^{2}}}{2\varepsilon} \leq\frac{8\sigma^{2}}{\mu^{2}}+\left(\frac{8\sigma^{2}}{\mu^{2} }\right)+\sqrt{64\varepsilon\frac{T_{k}^{2}\|\lambda_{0}-\lambda^{*}\|_{2}^{2 }}{\varepsilon^{2}}}{2\varepsilon}\] \[=\frac{\frac{16\sigma^{2}}{\mu^{2}}+\sqrt{\varepsilon\frac{8T_{k} \|\lambda_{0}-\lambda^{*}\|_{2}}{\varepsilon}}{2\varepsilon}}{2\varepsilon}\] \[=\frac{8\sigma^{2}}{\mu^{2}\,\varepsilon}+\frac{4T_{k}\|\lambda_{ 0}-\lambda^{*}\|_{2}}{\varepsilon\sqrt{\varepsilon}}.\]

Thus, \(\mathbb{E}\|\lambda_{T}-\lambda^{*}\|_{2}^{2}\leq\varepsilon\) can be satisfied with a number of iterations at least

\[T\geq\max\left(\frac{8\sigma^{2}}{\mu^{2}\,\varepsilon}+\frac{4T_{k}\|\lambda _{0}-\lambda^{*}\|_{2}}{\varepsilon\sqrt{\varepsilon}},\ \ 4T_{k}\right).\]

[MISSING_PAGE_FAIL:40]

Bradley-Terry (BT-Tennis)BT-Tennis is a Bradley-Terry model for estimating the skill of professional tennis players used by Giordano _et al._ (2023). The model is described as

\[\sigma \sim\mathcal{N}_{+}\left(0,1\right)\] \[\theta \sim\mathcal{N}\left(\mathbf{0},\sigma^{2}\mathbf{I}\right)\] \[p_{i} \sim\mathcal{G}[\text{win}_{i}]-\mathcal{G}[\text{los}_{i}]\] \[y_{i} \sim\text{bernoulli}\left(p\right),\]

where \(\text{win}_{i}\), \(\text{los}_{i}\) are the indices of the winning and losing players for the \(i\)th game, respectively. While we subsample over the games \(i=1,...\,,N\), each player's involvement is sparse in that each player plays only a handful of games. Consequently, the subsampling noise is substantial. Therefore, we use a larger batch size of 500. Similarly to Giordano _et al._ (2023), we use the ATP World Tour data publically available online 1.

Footnote 1: [https://datahub.io/sports-data/atp-world-tour-tennis-data](https://datahub.io/sports-data/atp-world-tour-tennis-data)

Autoregression (AR-ecg)AR-ecg is a linear autoregressive model. Here, we use a Student-t likelihood as originally proposed by Christmas & Everson (2011). While they originally imposed an automatic relevance detection prior on the autoregressive coefficients, we instead set a horseshoe shrinkage prior (Carvalho _et al._, 2009, 2010). Since the horseshoe is known to result in complex posterior geometry, this should make the problem more challenging. The model is described as

\[\alpha_{d} =10^{-2},\quad\beta_{d}=10^{-2},\quad\alpha_{d}=10^{-2},\quad\beta _{d}=10^{-2},\] \[d \sim\text{gamma}\,(\alpha_{d},\beta_{d}),\] \[\sigma^{-1} \sim\text{inverse-gamma}\,(\alpha_{\sigma},\beta_{\sigma}),\] \[\tau \sim\text{cauchy}_{+}\left(0,1\right),\] \[\lambda \sim\text{cauchy}_{+}\left(\mathbf{0},\mathbf{1}\right),\] \[\theta \sim\mathcal{N}\left(0,\tau\,\text{diag}\left(\lambda\right)\right)\] \[y[n] \sim\text{stduent-t}\left(d,\,\theta_{1}y[n-1]+\theta_{2}y[n-2]+ \cdots+\theta_{P}y[n-P],\sigma\right),\]

where \(d\) is the degrees-of-freedom for the Student-t likelihood, \(\text{cauchy}_{+}\) is a half-Cauchy prior.

For the dataset, we use the long-term electrocardiogram measurements of Jager _et al._ (2003) obtained from Physionet (Goldberger _et al._, 2000). The data instance we used has a duration of 23 hours sampled at 250 Hz with 12-bit resolution over a range of \(\pm 10\) millivolts. During the experiments, we observed that the hyperparameters suggested by Christmas & Everson are sensitive to the signal amplitude. Therefore, we scaled the signal amplitude to be \(\pm 10\).

[MISSING_PAGE_EMPTY:42]

Figure 6: **BBVI convergence speed (ELBO v.s. Iteration) and robustness against stepsize (ELBO at \(T=50,000\) v.s. Base stepsize). The error bands are the 80% quantiles estimated from 20 (10 for AR-eeg) independent replications. The initial point was \(\mathbf{m}_{0}=\mathbf{0},\mathbf{C}_{0}=\mathbf{I}\).**