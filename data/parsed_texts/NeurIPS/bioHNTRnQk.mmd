# Model Collapse Demystified: The Case of Regression

 Elvis Dohmatob\({}^{\lx@sectionsign}\)  Yunzhen Feng\({}^{\dagger}\) Julia Kempe\({}^{\lx@sectionsign}\)

\({}^{\lx@sectionsign}\)FAIR, Meta

\({}^{\dagger}\) Center for Data Science, New York University

\({}^{\ddagger}\)Courant Institute of Mathematical Sciences, New York University

\({}^{*}\)Correspondence to dohmatob@meta.com

###### Abstract

The era of proliferation of large language and image generation models begs the question of what happens if models are trained on the synthesized outputs of other models. The phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e. the model collapses. In this work, we investigate this phenomenon within the context of high-dimensional regression with Gaussian data, considering both low- and high-dimensional asymptotics. We derive analytical formulas that quantitatively describe this phenomenon in both under-parameterized and over-parameterized regimes. We show how test error increases linearly in the number of model iterations in terms of all problem hyperparameters (covariance spectrum, regularization, label noise level, dataset size) and further isolate how model collapse affects both bias and variance terms in our setup. We show that even in the noise-free case, catastrophic (exponentially fast) model-collapse can happen in the over-parametrized regime. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.

## 1 Introduction

_Model collapse_ describes the situation where the performance of large language models (LLMs) or large image generators degrade as more and more AI-generated data becomes present in their training dataset [44]. Indeed, in the early stages of the generative AI evolution (e.g the ChatGPT-xyz series of models), there is emerging evidence suggesting that retraining a generative AI model on its own outputs can lead to various anomalies in the model's later outputs. This phenomenon has been particularly observed in LLMs, where retraining on their generated content introduces irreparable defects, resulting in what is known as "model collapse", the production of nonsensical or gibberish output [44, 8]. Though several recent works demonstrate facets of this phenomenon _empirically_ in various settings [23, 32, 33, 8, 9, 21], a theoretical understanding is still missing.

In this work, we initiate a theoretical study of model collapse in the setting of high-dimensional supervised-learning with linear regression. This is equivalent to kernel regression1, which serves as an effective proxy for neural networks in various regimes, for instance in the infinite-width limit [37, 49, 25, 28] or in the lazy regime of training [12]. [11] characterize the power-law generalization error of regularized least-squares kernel algorithms, assuming a power-decay spectrum of the kernel (capacity) and of the coefficients of the target function (source). Source and capacity power decaycapture properties of the data and the model that give rise to power law scaling of test error in terms of data set size and model capacity, as empirically observed e.g. in [26, 24]. More recently, scaling laws have been shown for kernel models under the Gaussian design, e.g. in [46, 13, 14] for regression and [15] for classification. [39, 43, 31] study scaling laws for regression in the random feature model.

**Summary of Main Contributions.** Following the rich tradition in prior works outlined above, we study the Gaussian design where the input \(x\) is sampled from a multivariate zero-mean Gaussian \(\mathcal{N}(0,\Sigma)\) and labels \(y\) are determined by a linear ground truth function with independent label noise \(\epsilon\) as \(y=x^{\top}w_{0}+\epsilon\) (we present the linear regression setting for ease, the generalization to the kernel setting is straightforward). At each generation step, an approximation to \(w_{0}\) is learned from the data, and used to generate new, fake /synthetic labels for the next generation. Note that the machine learner has no control over the fake data generation process. It only sees data from a stage \(n\) of this process, which is then used to fit a downstream predictor. Our main findings can be summarized as follows:

_(1) Exact Characterization of Test Error under Iterative Retraining on Synthesized Data._ In Section 4 (Theorem 4.3), we obtain analytic formulae for test error under the influence of training data with fake / synthesized labels. For \(n\)-fold iteration of data-generation, this formula writes

\[E_{test}=E_{test}^{clean}+\Delta Bias+n\cdot\sigma_{0}\rho(\lambda,T,T_{0}, \sigma,\Sigma),\] (1)

where \(E_{test}^{clean}\) is the usual test error of the model trained on clean data (not AI-generated) and \(\sigma_{0}^{2}\) the label noise level in the clean data distribution. The non-negative term \(\rho\) precisely highlights the effects of all the relevant problems parameters: the feature covariance matrix \(\Sigma\), sample size \(T\), original data size \(T_{0}\), label noise level in the fake data distribution \(\sigma^{2}\), and regularization \(\lambda\). The non-negative term \(\Delta Bias\) is an increase in bias brought about by the iterative synthetic data generation process. This term disappears in the _under-parametrized_ regime (Corollary 4.4), if each stage in the process was fitted on sufficiently many samples \(T_{0}\) compared to the input dimension \(d\) (i.e if \(T_{0}\geq d\)). In the _over-parametrized_ case where \(T_{0}<d\), this term is either a constant (Theorem 4.5) or an increasing function of \(n\), depending on whether the design matrix stays the same or is resampled across different generations (Theorem 4.6). Notably, even in the case of noiseless labels (when \(\sigma_{0}=0\)), the downstream model converges to a Gaussian process around zero exponentially fast with the number of iterations \(n\), leading to "catastrophic" model collapse.

A direct consequence of (1) is that, as the number of generations \(n\) becomes large, the effect of re-synthesizing will make learning impossible. We note that the multiplicative degradation in scaling with the number of generations \(n\) is completely analogous to what has been shown in [18] for infinite memory models and their variants and empirically observed there. Illustration in Figures 0(a) and 2.

_(2) Modified Scaling Laws._ Turning to the special case of power-law spectra of the covariance matrix \(\Sigma\), which allows to derive test-error scaling laws [11, 46, 14, 29], we obtain in Section 5 (see Theorem 5.1) precise new scaling laws of the test error that quantitatively highlight the negative effect of training

Figure 1: **Demystifying model collapse.** Refer to Appendix D for details on the experimental setup.

on synthetically generated data. Further exploiting our analytic estimates, we obtain (Corollary 5.2) the optimal ridge regularization parameter \(\lambda\) as a function of all the problem parameters (sample size, spectral exponents, strength of fake data-generator, etc.). This new regularization parameter corresponds to a correction of the the value proposed in the classical theory on clean data [14], and highlights a novel crossover phenomenon where for an appropriate tuning of the regularization parameter, the effect of training on fake data is a degradation of the fast error rate in the noiseless regime [14; 11] to a much slower error rate which depends on the amount of true data on which the fake data-generator was trained in the first place. On the other hand, a choice of regularization which is optimal for the classical setting (training on real data), might lead to catastrophic failure: the test error diverges. See Figure 0(b) for an illustration.

## 2 Related Work

Current LLMs [17; 30; 10; 47], including GPT-4 [1], were trained on predominantly human-generated text; similarly, diffusion models like DALL-E [40], Stable Diffusion [42], Midjourney [35] are trained on web-scale image datasets. Their training corpora already potentially exhaust all the available clean data on the internet. A growing number of synthetic data generated with these increasingly popular models starts to populate the web, often indistinguishable from "real" data. Recent works call attention to the potential dramatic deterioration in the resulting models, an effect referred to as _"model collapse"_[44].

Empirical evidence of model collapse has been reported across various domains [23; 32; 33; 8; 9; 21]. Some theoretical studies [44; 7; 2; 18] have begun exploring this phenomenon. [44] attribute collapse to finite sampling bias and function approximation errors in the (single) Gaussian case but only provide lower bounds without detailed analytic expressions. [7] analyze the training process at the distribution level using both clean and synthetic data and provide stability results. However, these results do not account for finite samples and are only valid locally in parameter space, making them more relevant to fine-tuning rather than training from scratch. [2] examine "self-consuming loops" in the Gaussian case by assuming a sampling bias that reduces data variance with each generation--a (martingale) assumption that we do not require. These studies lack a comprehensive theoretical framework to quantify model collapse and its impact on scaling laws. Our work addresses these gaps by providing an analytic theory that captures how model collapse emerges from training on synthetic data, providing a deeper understanding that goes beyond merely identifying the collapse. A concurrent study by [18] demonstrate that model collapse in foundation models can be attributed to a breakdown in scaling laws [26; 24], where increasing the sample size eventually fails to improve model performance. This finding, theoretically shown for discrete data in variants of the infinite memory model, complements our analytical results on how synthetic data alters the rate of scaling laws, as discussed in Section 5.

Figure 2: **Model collapse in the case of noiseless over-parametrized synthetic data generator. Here \(d=300\), the sample sizes for the different versions of the fake data generator are equal, i.e \(T_{n}=T_{0}=d/2\) for all \(n\), and noise levels are \(\sigma_{0}=0\) and \(\sigma=0.1\). Everything else is as in the setting of Figure 0(a). Broken lines correspond to the theoretical estimates given in Theorem 4.3. As predicted by our theory, the test error of the model fitted on synthetic data (\(n\geq 1\)) increases (relative to the baseline \(n=0\), corresponding to training on clean data). The model collapse here, even in the absence of noise (\(\sigma_{0}=0\)), is due to the fact that the synthetic data-generator does not have access to enough data to capture the true labelling function. (a) Importantly, and in accordance to our theory, the amount of model collapse in the case \(X_{n}\equiv X_{0}\) is due to an increase in bias term of the test error of the model and does not depend on the number of generations \(n\) as long as \(n\geq 1\). (b) In contrast, for the case where the \(X_{n}\)’s are independent, the increase in bias term grows with \(n\), leading to “catastrophic” model collapse (Theorem 4.6). Refer to Appendix D for the experimental setup.**

Theoretical Setup

We now present a setup which is simple enough to be analytically tractable, but rich enough to exhibit a wide range of regimes to illustrate a range of new phenomena that emerge with model collapse.

**Data Distribution and Synthetized Data.** Consider the distribution \(P_{\Sigma,w_{0},\sigma^{2}}\) on \(\mathbb{R}^{d}\times\mathbb{R}\) given by

**(Input)**\(x\sim N(0,\Sigma),\quad\textbf{(Noise)}\;\epsilon\sim N(0,\sigma^{2}),\; \text{indep. of}\;x\quad\textbf{(Output/Label)}\;y=x^{\top}w_{0}+\epsilon.\) (2)

The positive integer \(d\) is the input-dimension, the vector \(w_{0}\in\mathbb{R}^{d}\) defines the ground-truth labelling function \(x\mapsto x^{\top}w_{0}\), the matrix \(\Sigma\in\mathbb{R}^{d\times d}\) is the covariance structure of the inputs. The scalar \(\sigma^{2}\) is the level of label noise. Here, we consider the linear case for clarity. We describe the extension to the kernel setting in Appendix C. Thus, in classical linear regression, given a sample \((X,Y)\equiv\{(x_{1},y_{1}),\ldots,(x_{T},y_{T})\}\) of size \(T\) from \(P_{\Sigma,w_{0},\sigma^{2}}\), one seeks a linear model \(\widehat{w}\in\mathbb{R}^{d}\) with small test error

\[E_{test}(\widehat{w}):=\mathbb{E}_{x,y}[(x^{\top}\widehat{w}-y)^{2}]-\sigma^{ 2}=\|\widehat{w}-w_{0}\|_{\Sigma}^{2},\] (3)

where \((x,y)\sim P_{\Sigma,w_{0},\sigma^{2}}\) is a random clean test point. In our setup for studying model collapse, the training data \((X,Y)\) is sampled from an iterative loop where each generation of the model serves as the labeller for the data for the next generation. This process is described below.

**Structure of the Synthesized / Fake Data Generator.** Consider a sequence of data distributions

\[P_{\Sigma,w_{0},\sigma_{0}^{2}}\to P_{\Sigma,\widehat{w}_{1},\sigma_{1}^{2}} \rightarrow\ldots\to P_{\Sigma,\widehat{w}_{n},\sigma_{n}^{2}\rightarrow \ldots},\] (4)

where \(\widehat{w}_{n}\)'s is defined recursively by \(\widehat{w}_{n}=w_{0}\), and

\[\widehat{w}_{n}=\operatorname{Fit}(X_{n-1},\overline{Y}_{n-1}),\;\text{for} \;n\geq 1,\] (5)

where \(\overline{Y}_{n}:=X_{n}\widehat{w}_{n}+E_{n}\) and \(\operatorname{Fit}(A,B)=\operatorname{OLS}(A,B):=A^{\dagger}B\) is ordinary-least squares (OLS). The design matrices \((X_{n})_{n\geq 0}\) are of shapes \(T_{n}\times d\), each with iid rows from \(N(0,\Sigma)\). The sequence of noise vectors \((E_{n})_{n\geq 0}\) forms an independent collection, which is independent of the \((X_{n})_{n\geq 0}\) ; each \(E_{n}\in\mathbb{R}^{T_{n}}\) has iid components \(\epsilon_{n,i}\) from \(N(0,\sigma_{n}^{2})\). Refer to Figure 3.

**The Downstream Model: Ridge Regression.** For a number of iterations \(n\geq 0\), noise levels \(\sigma_{0}\) and \(\sigma\), dataset sizes \(T_{0}\) and \(T\), and regularization parameter \(\lambda\geq 0\), let \(\widehat{w}_{n}^{pred}=\widehat{w}_{n,T_{0},\sigma_{n}^{2},T,\sigma,\lambda} ^{pred}\in\mathbb{R}^{d}\) be the ridge predictor constructed from an iid sample \(\{(x_{1},y_{1}),\ldots,(x_{T},y_{T})\}\) of size \(T\) from the \(n\)-fold fake data distribution \(P_{\Sigma,\widehat{w}_{n},\sigma_{n}^{2}}\), where for ease of presentation of our results we will assume that

\[T_{n-1}=\ldots=T_{1}=T_{0},\quad T_{n}=T\quad\text{and}\quad\sigma_{n-1} \quad=\ldots=\sigma_{1}=\sigma_{0},\quad\sigma_{n}=\sigma.\] (6)

For an \(n\)-fold fake data generator \(P_{\Sigma,\widehat{w}_{n},\sigma_{n}^{2}},\) we denote with \(X:=X_{n}\in\mathbb{R}^{T\times d}\) the design matrix with iid rows from \(N(0,\Sigma)\), with \(E:=E_{n}\in\mathbb{R}^{T}\) the stage-\(n\) label-noise vector with components in \(N(0,\sigma_{n}^{2})\), and \(Y:=\overline{Y}_{n}=X\widehat{w}_{n}+E\in\mathbb{R}^{T}\) the labels generated by \(P_{\Sigma,\widehat{w}_{n},\sigma_{n}^{2}}\). Let \(\widehat{\Sigma}:=X^{\top}X/T\in\mathbb{R}^{d\times d}\) is the sample covariance matrix, and \(R=R(\lambda):=(\widehat{\Sigma}+\lambda I_{d})^{-1}\) denote its _resolvent_, so that

\[\widehat{w}_{n}^{pred}=RX^{\top}Y/T\;\text{for}\;\lambda>0,\;\text{and}\; \widehat{w}_{n}^{pred}=X^{\dagger}Y\;\text{for}\;\lambda=0.\] (7)

Figure 3: **Illustration of the theoretical framework.** The process begins with the original model \(\widehat{w}_{0}(w_{0})\) and the original dataset \((X_{0},\overline{Y}_{0})\). \(n\) synthetic data generators \(\widehat{w}_{1}\) to \(\widehat{w}_{n}\) are iteratively fit on data labelled by the previous model with label noise \(\sigma_{0}\), using \(T_{0}\) samples each. We evaluate the test error (with respect to the ground truth labels from \(w_{0}\)) of \(\widehat{w}_{n}^{pred}\), trained on \((X,Y):=(X_{n},\overline{Y}_{n})\) using \(T\) samples with label noise \(\sigma\) and a regularization coefficient \(\lambda\).

Thus, in summary, each \(\widehat{w}_{n}\) results from fitting a model on a dataset of size \(T_{n-1}\) from \(P_{\Sigma,\widehat{w}_{n-1},\sigma_{n-1}^{2}}\), for every generation index \(n\geq 1\).

We are interested in the dynamics of the test error \(E_{test}(\widehat{w}_{n}^{pred})\) of this linear model. Importantly, the evaluation of the model is performed on the true data distribution \(P_{\Sigma,w_{0},\sigma_{0}^{2}}\), even though the model is trained on the fake data distribution \(P_{\Sigma,\widehat{w}_{n},\sigma^{2}}\). Note that for \(n=0,E_{test}^{clean}:=E_{test}(\widehat{w}_{n}^{pred})\) corresponds to the usual test error when the downstream model is trained on clean data. Importantly, the downstream model has no control over this process. It will only see training data from a given version \(P_{\Sigma,\widehat{w}_{n},\sigma_{n}^{2}}\), but evaluation will be on the true distribution \(P_{\Sigma,w_{0},\sigma_{0}^{2}}\).

The mental picture is as follows: each generation \(\widehat{w}_{n}\) can be seen as a proxy for a specific version of ChatGPT, for example. The sample size \(T_{0}\) used to create the fake labelling functions \(\widehat{w}_{n}\) is a proxy for the strength of the fake data-generator thus constructed. Other works which have considered model collapse under such a self-looping training process include [44; 2; 7; 18].

## 4 Exact Test Error Characterization

In this section we establish generic analytic formulae for the test error of the downstream model \(\widehat{w}_{n}^{pred}\) (7) trained on \(n\)-fold fake data-generation as outlined in Section 3. The fully general technical key Theorem F.1 detailing formula (1), with a trace expression for \(\rho\), (as well as proofs) are given in Appendix F; consult part F.1 for an exposition. Notations are standard (summarized in Appendix E).

### Warm-up: Ordinary Least Squares on Isotropic Data

For a start, let us first consider the case of unregularized regression, where \(\lambda=0\) in Equation (7).

**Theorem 4.1**.: _For an \(n\)-fold fake data generation process with \(T_{0}\geq d+2\) samples, the test error for the linear predictor \(\widehat{w}_{n}^{pred}\) in Equation (7) learned on \(T\geq d+2\) samples, with \(\lambda=0\), is given by_

\[E_{test}(\widehat{w}_{n}^{pred})\simeq\frac{\sigma^{2}\phi}{1-\phi}+\frac{n \sigma_{0}^{2}\phi_{0}}{1-\phi_{0}},\text{ with }\phi=\frac{d}{T},\ \phi_{0}=\frac{d}{T_{0}},\] (8)

_where the notation \(f(T)\simeq g(T)\) means \(f(T)/g(T)\to 1\), for large \(T\)._

The first term \(E_{test}(\widehat{w}_{0}^{pred})\simeq\sigma^{2}\phi/(1-\phi)\) in the above decomposition corresponds to the usual error when the downstream model is fitted on clean data (see [22], for example). The additional term \(n\sigma_{0}^{2}\phi_{0}/(1-\phi_{0})\), proportional to the number of generations \(n\), is responsible for model collapse.

**Model collapse versus more training data.** Note that the linear degeneration in test error highlighted by Equation (8) is a direct consequence of using the same dataset size \(T_{0}\) across the fake data generator. Of course, if the underlying synthetic generating process has access to a larger data budget across generations, this decay can be significantly alleviated. For instance, if fake data increases gradually with the number of generations \(m\geq 2\) as \(T_{m}=(m\log^{2}m)T_{0}\) (and, to simplify, \(\sigma=\sigma_{0}\)) a trivial extension of Theorem 4.1 yields

\[E_{test}(\widehat{w}_{n}^{pred})\simeq(1+\frac{1}{2\log^{2}2}+\frac{1}{3\log^ {2}3}+\dots)E_{test}(\widehat{w}_{0}^{pred})\simeq E_{test}(\widehat{w}_{0}^{ pred}),\]

which will keep collapse at bay at the expense of largely increased training data ([44] also has a similar formula). This does not avoid model collapse; rather, it trades additional data generation and training effort against deterioration from generations of fake data. Thus, while for clean data increasing the dataset size n-fold leads to better scaling, with synthetic data, we forfeit this improvement. Also, note that we do not assume access to samples from any of the intermediate generation steps \(\widehat{w}_{0},\dots,\widehat{w}_{n-1}\); we only train the downstream model \(\widehat{w}_{n}^{pred}\) on data from the last step \(\widehat{w}_{n}\).

**Model Collapse as Change of Scaling Laws.** In the low-dimensional regime (fixed \(d\)), Theorem 4.1 already predicts a change of scaling law from \(\sigma^{2}T^{-1}\) to \(\sigma^{2}T^{-1}+n\sigma_{0}^{2}T_{0}^{-1}\). Thus, as the sample size \(T\) is scaled up, the test error eventually plateaus at the value \(n\sigma_{0}^{2}T_{0}^{-1}\) and does not vanish. This phenomenon, also established in [18] in the context of large language models, is clearly visible in Figure 0(a). In the rest of this section and also in Section 5, we shall establish an analogous picture for high-dimensional regimes (\(d\rightarrow\infty\)).

**Mitigation via Regularization.** Note that the test error of the null predictor \(w_{null}=0\) is \(E_{test}(w_{null})=\|w_{0}\|_{\Sigma}^{2}\), and so

\[\frac{E_{test}(\widehat{w}_{n}^{pred})}{E_{test}(w_{null})}=\frac{1}{\text{ SNR}}\frac{\phi}{1-\phi}+\frac{n}{\text{SNR}_{0}}\frac{\phi_{0}}{1-\phi_{0}},\]where \(\text{SNR}:=\|w_{0}\|_{\Sigma}^{2}/\sigma^{2}\) and \(\text{SNR}_{0}:=\|w_{0}\|_{\Sigma}^{2}/\sigma_{0}^{2}\). We deduce that if \(n\gg\text{SNR}_{0}/(1/\phi_{0}-1)\), then the learned model is already much worse than the null predictor! This suggests that a possible strategy for mitigating the negative effects on learning on AI-generated data is regularization, as empirically illustrated in Figures 0(a), 0(b), 0(c), and also in 0(c) of Appendix D.

Furthermore, in Section 5 we shall establish that the optimal regularization parameter established in [14], in the case of polynomially decreasing spectra (a regime which is relevant to wide neural networks), must be modified in the presence of synthetic training data in order to prevent the generalization error to diverge to infinity (i.e catastrophic failure).

### High-Dimensional Regimes

In order to analyze the trace term \(\rho\) appearing in Equation (1) (and spelled out in (32) in Appendix F.1), we need some tools from RMT, and ultimately obtain analytic formulae for \(E_{test}(\tilde{w}_{n}^{pred})\) in Theorem 4.3. Such tools have been used extensively to analyze anisotropic ridge regression [41; 22; 4].

**Random Matrix Equivalents.** For any sample size \(T\geq 1\) and \(\lambda\geq 0\), define \(\kappa(\lambda,T)\) implicitly by

\[\kappa(\lambda,T)-\lambda=\kappa(\lambda,T)\cdot\mathrm{df}_{1}(\kappa( \lambda,T))/T,\] (9)

where, for any \(\lambda\geq 0\) and \(m\in\mathbb{N}_{*}\), \(\mathrm{df}_{m}(\lambda)\) is the \(m\)th order "degree of freedom" of the covariance matrix \(\Sigma\) is given by \(\mathrm{df}_{m}(\lambda)=\mathrm{df}_{m}(\lambda;\Sigma):=\mathrm{tr}\,\Sigma ^{m}(\Sigma+\lambda I_{d})^{-m}\).

The effect of ridge regularization at level \(\lambda\geq 0\) is to improve the condition of the empirical covariance matrix \(\widehat{\Sigma}\); what the \(\kappa\)-function does is translate this into regularization on \(\Sigma\) at level \(\kappa(\lambda,T)\), so as control the capacity of the former, i.e. the "effective dimension" of the underlying problem. Quantitatively, there is an equivalence of the form \(\mathrm{df}_{1}(\lambda;\widehat{\Sigma})\approx\mathrm{df}_{1}(\kappa( \lambda,T);\Sigma)\). Roughly speaking, RMT is the business of formalizing such a relationship and derivatives (w.r.t. \(\lambda\)) thereof. A standard reference on the subject is [5].

**Example: Isotropic Data.** As an illustration, note that \(\mathrm{df}_{m}(\lambda)\equiv d/(1+\lambda)^{m}\) (polynomial decay) in the isotropic case where \(\Sigma=I_{d}\). Consequently, we have

\[\kappa(\lambda,T)-\lambda=\phi\cdot\kappa(\lambda,T)/(1+\kappa(\lambda,T)), \text{ with }\phi:=d/T.\]

In this case, it is easy to obtain the following well-known formula for \(\kappa=\kappa(\lambda,T)\):

\[\kappa=\frac{\lambda+\overline{\phi}+\sqrt{(\lambda+\overline{\phi})^{2}+4 \lambda}}{2},\text{ with }\overline{\phi}:=\phi-1,\] (10)

which is reminiscent of the celebrated Marchenko-Pastur law [34].

**Asymptotic Regime.** We shall work in the following so-called proportionate asymptotic scaling regime which is a standard analysis based on random matrix theory (RMT):

\[T,d\to\infty,\quad d/T\to\phi,\quad\|\Sigma\|_{op},\|\Sigma^{-1}\|_{op}=O(1).\] (11)

Later in Section 5 when we consider power-law spectra, this scaling will be extended to account for the more realistic case where \(d\) and \(T\) are of the same order on log scale, i.e

\[T,d\to\infty,\quad d^{1/C}\lesssim T\lesssim d^{C},\quad\|\Sigma\|_{op},\| \Sigma^{-1}\|_{op}=O(1),\] (12)

for some absolute constant \(C\geq 1\). Such non-proportionate settings are covered by the theory developed in [27; 48]. For clarity of presentation, even in this more general regime of Equations (12), we will still continue to write \(\phi_{0}:=d/T_{0}\) and \(\phi:=d/T\).

**Bias-Variance Decomposition.** With everything now in place, let us recall for later use, the classical bias-variance decomposition for ridge regression (for example, see [41; 22; 4]):

**Proposition 4.2**.: _In the RMT limit (12), the test error of a ridge predictor \(w(\lambda)\) based on \(T\) iid samples from the true data distribution \(P_{\Sigma,w_{0},\sigma^{2}}\) is given by_

\[E_{test}(w(\lambda)) =\mathbb{E}\,\|w(\lambda)-w_{0}\|_{\Sigma}^{2}\simeq Bias+Var,\] (13) \[\text{ with }Bias \simeq\frac{\kappa^{2}w_{0}^{\top}\Sigma(\Sigma+\kappa I)^{-2}w_{ 0}}{1-\mathrm{df}_{2}(\kappa)/T},\quad Var\simeq\frac{\sigma^{2}\mathrm{df}_{ 2}(\kappa)}{T}\cdot\frac{1}{1-\mathrm{df}_{2}(\kappa)/T},\] (14)

_where \(\kappa=\kappa(\lambda,T)\) is as given in Equation (9)._

### Analytic Formula for Test Error

The following result gives the test error for the downstream ridge predictor \(\widehat{w}_{n}^{pred}\) defined in Equation (7), in the context of fake training data, and will be heavily exploited later to obtain precise estimates in different regimes. Define generic \(Var\) and \(Bias\) by:

\[Var=\mathbb{E}\left\|RX^{\top}E/T\right\|_{\Sigma}^{2}=\sigma^{2}\frac{1}{T} \operatorname{tr}\Sigma R^{2}\widehat{\Sigma}\qquad Bias=\mathbb{E}\left\| \widehat{\Sigma}Rw_{0}-w_{0}\right\|_{\Sigma}^{2},\]

and note that \(E_{test}^{clean}:=Bias+Var\), for standard ridge regression fitted on clean data from the true data distribution \(P_{\Sigma,w_{0},\sigma^{2}}\) (e.g., see Hastie et al. [22]). Let \(Q_{n-1}=P_{n-1}P_{n-2}\ldots P_{0}\) where \(P_{m}\) is the orthogonal projection unto the subspace of \(\mathbb{R}^{d}\) spanned by the rows of \(X_{m}\) and define

\[\Delta Bias :=\mathbb{E}\left\|\widehat{\Sigma}R(Q_{n-1}w_{0}-w_{0})\right\|_{ \Sigma}^{2}\geq 0.\] (15)

**Theorem 4.3**.: _For an \(n\)-fold fake data-generation process, the test error of a ridge predictor \(\widehat{w}_{n}^{pred}\) based on a sample of size \(T\) with regularization parameter \(\lambda\), is given in the RMT limit (12) by_

\[E_{test}(\widehat{w}_{n}^{pred})\simeq\widetilde{Bias}+Var+n\sigma_{0}^{2}\rho,\] (16)

_where \(\rho\) is as given in Theorem F.1, and \(\widetilde{Bias}\) satisfies_

\[\widetilde{Bias}\geq Bias+\Delta Bias\geq Bias\text{ (with equality if }T_{0}\geq d),\]

_and \(\Delta Bias\) as given in (15). Furthermore, if one of the following conditions holds_

\[T_{0}\geq d\text{ \ OR \ }X_{n}=X_{0}\text{ \ for all }n\geq 1,\] (17)

_then, we have the following explicit formula for \(\rho\)_

\[\rho=\frac{\operatorname{tr}\Sigma^{4}(\Sigma+\kappa_{0}I)^{-2}(\Sigma+\kappa I )^{-2}}{T_{0}-\operatorname{df}_{2}(\kappa_{0})}+\frac{\kappa^{2}\operatorname {tr}\Sigma^{2}(\Sigma+\kappa_{0}I)^{-2}(\Sigma+\kappa I)^{-2}}{T_{0}- \operatorname{df}_{2}(\kappa_{0})}\cdot\frac{\operatorname{df}_{2}(\kappa)}{ T-\operatorname{df}_{2}(\kappa)},\] (18)

_with \(\kappa=\kappa(\lambda,T)\) and \(\kappa_{0}:=\kappa(0,T_{0})\) are as given in Equation (9)._

Instructively, the term \(\Delta Bias\) measures how biased the synthetic data-generation process away from ground-truth model \(w_{0}\). This term disappears if the generator was fitted on sufficiently many samples (i.e. if \(T_{0}\geq d\)). More quantitatively, when \(T_{0}<d\) and \(X_{n}=X_{0}\), it is easy to see that \(\Delta Bias\geq\mathbb{E}\left[\|\Sigma^{1/2}\widehat{\Sigma}R\|_{op}^{2} \right]\cdot Bias_{0}\), where \(Bias_{0}:=\mathbb{E}\left\|P_{0}w_{0}-w_{0}\right\|_{2}^{2}\) measures the inability due to lack of enough data, of the first generation (\(n=1\)) to reliably estimate \(w_{0}\) even in the absence of noise (\(\sigma_{0}=0\)) in the data-generating process. This gap propagates over to higher generations of the process. The situation is illustrated in Figure 2. In the case where \(T_{0}<d\) and the \(X_{n}\)'s are independent, we shall see in Section 4.5 that this increase in bias actually grows with \(n\), even in the case of fake data generation without label noise (i.e. \(\sigma_{0}=0\)).

### Model Collapse in the Case of Under-Parametrized Fake Data-Generator

We now consider the scenario of under-parameterization, where \(T_{0}\geq d\), indicating that the number of data points exceeds the number of dimensions. This condition typically results in a unique solution for the regression. In this case, \(P_{0}=I_{d}\) a.s., leading to \(\widetilde{Bias}=Bias\) (given as in formula (14)), and \(\kappa_{0}=0\) in (18), and so Theorem 4.3 gives

\[\rho=\frac{\operatorname{df}_{2}(\kappa)}{T_{0}-d}+\frac{\kappa^{2} \operatorname{tr}\left(\Sigma+\kappa I\right)^{-2}}{T_{0}-d}\frac{\operatorname {df}_{2}(\kappa)}{T-\operatorname{df}_{2}(\kappa)}.\] (19)

We have the following corollary to Theorem 4.3.

**Corollary 4.4**.: _Consider the setting of Theorems 4.3 and F.1. If \(T_{0}\geq d\) additionally, then it holds in the RMT limit (12) that \(E_{test}(\widehat{w}_{n}^{pred})\simeq Bias+Var+n\sigma_{0}^{2}\rho\), where \(Bias\) and \(Var\) are as given in formula (14), and \(\rho\) is as given in Equation (19)._

_Moreover, in the special case of isotropic features, it holds that_

\[Bias+Var\simeq\frac{\kappa^{2}\|w_{0}\|_{2}^{2}+\sigma^{2}\phi}{(1+\kappa)^{2} -\phi},\quad\rho\simeq\frac{\phi_{0}}{1-\phi_{0}}\left(\frac{1}{(1+\kappa)^{2} }+\frac{1}{(1+\kappa)^{2}}\frac{\phi\kappa^{2}}{(1+\kappa)^{2}-\phi}\right),\]

_with \(\phi:=d/T\), \(\phi_{0}:=d/T_{0}\), and \(\kappa=\kappa(\lambda,T)\) as in Equation (10)._

Note that Theorem 4.1 is special case of the above result corresponding to \(\lambda=0\) and \(\phi\geq 1\). A result like Corollary 4.4 gives us the needed analytical handle for understanding \(n\)-fold model collapse in terms of all problem hyper-parameters (covariance spectrum, regularization, label-noise level, etc.).

### Model Collapse in the Absence of Label Noise

We now consider the over-parametrized regime, where the different iterations of the synthetic data-generator (refer to the illustration in Figure 3) are fitted on insufficient data. For simplicity of exposition, we restrict our presentation to isotropic covariance \(\Sigma=I_{d}\). Since we will be focusing on the possible increase \(\Delta Bias\) above the bias (defined in Equation (14)) due to \(n\geq 1\) generations as predicted by Theorem 4.3, we further restrict ourselves to the noiseless regime where the fake data-generating process has no label noise, i.e. \(\sigma_{0}=0\). Thanks to Lemma F.4, we know that the generation-\(n\) fake labelling vector \(\widehat{w}_{n}\) (defined in Eqn. (5)) is given explicitly as a series of projections

\[\widehat{w}_{n}=Q_{n-1}w_{0}=P_{n-1}P_{n-2}\ldots P_{0}w_{0}.\] (20)

Further, for simplicity we will assume \(T=T_{n}>d\), i.e the downstream model has access to enough data. We shall focus on two important special cases.

**The Dependent Case.** We first consider the case where \(T_{m}=T_{0}<d\) and \(X_{m}=X_{0}\) for all \(m\leq n-1\). It is clear that Equation (20) reduces to \(\widehat{w}_{n}=P_{0}w_{0}\), with \(\operatorname{rank}P_{0}=T_{0}<d\).

**Theorem 4.5**.: _In the limit \(\lambda\to 0^{+}\) and \(d,T_{0}\to\infty\) with \(d/T_{0}\to\phi_{0}>1\), it holds that_

\[\|\widehat{w}_{n}\|^{2}\simeq\|w_{0}\|^{2}/\phi_{0},\quad\Delta Bias\simeq\| w_{0}\|^{2}(1-1/\phi_{0}).\] (21)

We see that in this setting, the increase in bias \(\Delta Bias\simeq(1-1/\phi_{0})\|w_{0}\|^{2}\) brought about by synthetic data is a positive constant which does not grow with the number of generations \(n\geq 1\). This increase in bias (i.e compared to training on clean data) is due to the fact that, with probability \(1\), the random subspace of \(\mathbb{R}^{d}\) spanned by \(X_{0}\) does not contain the ground truth model \(w_{0}\). The expression is nothing but a RMT estimate of \(\|P_{0}w_{0}-w_{0}\|^{2}\), i.e. the squared norm of the projection of \(w_{0}\) onto the orthogonal complement of this subspace. The result is illustrated in Figure 2(a).

**The Independent Case.** For our second example, we remove the assumption that \(T_{m}=T_{0}\) and \(X_{m}=X_{0}\) for all \(m\leq n-1\) considered in the previous case (Theorem 4.5). We instead assume that (A) The \(X_{m}\)'s are assumed to be independent, and (B) we are in the following high-dimensional limit

\[\lambda\to 0^{+},\quad d,T_{1},\ldots,T_{n-1}\to\infty,\quad d/T_{m}\to\phi_{m },\quad\text{ for some }\phi_{1},\ldots,\phi_{n-1}>0.\] (22)

Define \(\eta:=\prod_{n=0}^{n-1}\min(1/\phi_{m},1)\in(0,1]\). We have the following theorem.

**Theorem 4.6**.: _In the limit (22), it holds that \(\|\widehat{w}_{n}\|^{2}\simeq\|w_{0}\|^{2}\eta\) and \(\Delta Bias\simeq\left\|w_{0}\right\|^{2}(1-\eta)\). In particular, if \(n\to\infty\) with infinitely many \(\phi_{m}>1\), then \(\widehat{w}_{n}\to 0\) and \(\Delta Bias\to\left\|w_{0}\right\|^{2}\)._

The theorem predicts that a sequence of over-parametrized fake data-generators \((\widehat{w}_{n})_{n}\) collapses to zero (and thus, effectively escapes from the ground truth model \(w_{0}\)). Consequently, the downstream model \(\widehat{w}_{n}^{pred}\) convergences to a Gaussian process around zero, instead of the true model \(w_{0}\), leading to an increase in the bias term of the test error!

For example if \(\phi_{n}=\phi_{0}>1\), then Theorem 4.6 predicts that \(\Delta Bias\simeq(1-\phi_{0}^{-n})\|w_{0}\|^{2}\), which grows exponentially fast towards \(\|w_{0}\|^{2}\), the test error of the null predictor. This compounding effect is due to the fact that in (20), each projection \(P_{m}\) spins the fake data labelling vector \(\widehat{w}_{n}\) further away from the ground-truth \(w_{0}\). The result is illustrated in Figure 2(b).

Comparing the dependent case and the independent case, 4.6 shows that the increase in bias is proportional to \(1-\eta_{0}\eta_{1}\ldots\eta_{n-1}\), which is typically much larger than \(1-\eta_{0}\), which is the increase in the dependent case. Sampling different design matrices results in a more pronounced model collapse.

## 5 The Case of Heavy Tails (Power Law)

Neural scaling laws [26, 24], relate a model's test error to the sample size, model size, and computational resources, and are critical tools for practitioners in strategically allocating resources during the design and implementation of large language models. Previous theoretical works [11, 41, 14] have examined scaling laws in our tractable setting of linear regression with Gaussian design in the context of a power-law covariance spectrum. Now we explore how synthetic data alters these scaling laws in this setting.

Let the spectral decomposition of the covariance matrix \(\Sigma\) be \(\Sigma=\lambda_{1}v_{1}v_{1}^{\top}+\ldots+\lambda_{d}v_{d}v_{d}^{\top}\), where \(\lambda_{1}\geq\ldots\geq\lambda_{d}\geq 0\) are the eigenvalues and \(v_{1},\ldots,v_{d}\in\mathbb{R}^{d}\) are the eigenvectors. For any feature index \(j\in[d]\), define a coefficient \(c_{j}:=w_{0}^{\top}v_{j}\), i.e the projection of \(w_{0}\) along the \(j\)th eigenvector of \(\Sigma\). We shall work under the following well studied spectral conditions

\[\begin{split}&\textbf{(Capacity Condition)}\ \lambda_{j}\asymp j^{-\beta}\ \text{for all}\ j\in[d],\\ &\textbf{(Source Condition)}\,\|\Sigma^{1/2-r}w_{0}\|=O(1), \end{split}\] (23)

where \(\beta>1\) and \(r>0\). The parameter \(r\) measures the amount of dispersion of \(w_{0}\) relative to the spectrum of \(\Sigma\); a large value of \(r\) means \(w_{0}\) is concentrated only along a few important eigen-directions (i.e. the learning problem is easy). For later convenience, define \(\delta\), \(\underline{r}\), and \(c\) by

\[\delta:=1+\beta(2r-1)\in\mathbb{R},\quad\underline{r}:=\min(r,1)\in(0,1), \quad c:=2\beta\underline{r}/(2\beta\underline{r}+1)\in(0,1).\] (24)

As noted in [14], the source condition in (23) is satisfied if \(c_{j}\asymp j^{-\delta/2}\) for all \(j\in[d]\).

Consider adaptive ridge regularization strength of the form

\[\lambda=\lambda(T)\asymp T^{-\ell},\] (25)

for fixed \(\ell\geq 0\). The case where \(\ell=0\) corresponds to non-adaptive regularization; otherwise, the level of regularization decays polynomially with the sample size \(T\). Define

\[\ell_{crit}:=\beta/(2\beta\underline{r}+1).\] (26)

In [14], KRR under normal circumstances (corresponding to \(n=0\), i.e. no fake data) was considered and it was shown that this value for the regularization exponent in (25) is minimax-optimal for normal test error in the noisy regime (\(\sigma>0\)), namely \(E_{test}(\tilde{w}_{0}^{pred})\asymp T^{-c}\). This represents a crossover from the noiseless regime where it was shown that the test error scales like \(E_{test}(\tilde{w}_{0}^{pred})\asymp T^{-2\beta\underline{r}}\), a much faster rate. In the context of training on fake data, which is the object of this manuscript, we shall establish new scaling laws which paint a drastically different picture.

**A "Collapsed" Scaling Law.** The following result shows that model collapse is a modification of usual scaling laws induced by fake data. All proofs of this section can be found in Appendix H. Here, for simplicity of presentation, we restrict to the case \(T_{0}\geq d+2\) to make the results easier to present. This condition can be removed as in Theorem 4.3.

**Theorem 5.1**.: _Consider \(n\)-fold fake-data generation with sample size \(T_{0}\geq d+2\). For a ridge predictor \(\tilde{w}_{n}^{pred}\) given in Equation (7) based on a fake data sample of size \(T\), with regularization parameter \(\lambda=\lambda(T)\) tuned adaptively as in Equation (25) with exponent \(\ell\in[0,\beta)\), the test error satisfies the following scaling law in the RMT limit (12):_

\[E_{test}(\tilde{w}_{n}^{pred})\asymp\max(\sigma^{2},T^{1-2\underline{r}\ell- \frac{\delta}{\delta}})\cdot T^{-(1-\frac{\delta}{\delta})}+\frac{n\sigma_{0}^ {2}}{1-\phi_{0}}\max\,(T/\mathcal{T}_{0},\phi_{0})\cdot T^{-(1-\frac{\delta}{ \delta})}.\] (27)

We now provide an instructive interpretation of Theorem 5.1 and outline the effect of regularization.

**The Noiseless Regime.** First consider the case \(\sigma=0\) (or equivalently, exponentially small in \(T\)) and \(\phi_{0}\in(0,1)\) is fixed, and consider a number of generations \(n\) such that \(n\sigma_{0}^{2}\asymp T^{a}\), where \(0\leq a\leq 1-\ell/\beta\leq 1\). Note that \(a=0\) corresponds to a constant number of generations. Also take \(T_{0}=T^{b}\), for some constant \(b\in(0,\infty)\). According to Theorem 5.1, if we want to balance out the model-collapsing negative effect of training on fake data, we should chose \(\ell\) so as to balance the second term \(n(T/T_{0})T^{-(1-t/\beta)}=T^{-(b-\ell/\beta-a)}\) and the first term \(T^{-2\ell\underline{r}}\). We have the following:

**Corollary 5.2**.: _In the setting of Theorem 5.1 with \(T_{0}\asymp T^{b}\) and \(n\asymp T^{b}\), the optimal exponent of the ridge regularization parameter in Equation (25) is \(\ell=\ell_{\star}\), where_

\[\ell_{\star}=\min((b-a)\ell_{crit},\beta),\] (28)

_and \(\ell_{crit}\) is as in Eqn. (26), with corresponding optimal test error \(\inf_{\ell\geq 0}E_{test}(\widehat{w}_{n}^{pred})\asymp T^{-(b-a)c}\)._

Observe that when \((b-a)c<2\beta\underline{r}\), which is the case when \(n=O(1)\), \(r\geq 1\) and \(b\leq a+1\), this corresponds to the condition \(T\gtrsim T_{0}\). The above result represents a crossover from the fast rate \(E_{test}(\widehat{w}_{0}^{pred})\asymp T^{-2\beta\underline{r}}\) in the case of training on clean data [14], to a much slower rate \(E_{test}(\tilde{w}_{n}^{pred})\asymp T^{-(b-a)c}\), attained by the adaptive regularization \(\lambda\asymp T^{-\ell_{*}}\), which is optimal in this setting. Furthermore, in this setting if we still use \(\lambda\asymp T^{-\ell_{crit}}\) as proposed in [14] in the clean data setting, Corollary 5.2 predicts that

\[E_{test}(\tilde{w}_{n}^{pred})\gtrsim T^{-(b-\ell_{crit}/\beta-a)}=T^{-(c+b-a-1 )},\]

which diverges to infinity if \(b\geq a+1-c\). This is a catastrophic form of model collapse, and is empirically illustrated in Figures 0(b) and 4.

**The Noisy Regime.** This discussion can be found in Appendix G.

**Remark.** In all the analyses above, we quantitatively demonstrate how model collapse manifests as a _change in scaling laws_ within a setting commonly used to understand scaling behavior in current foundation models [46; 13; 14]. Our results indicate that, in the presence of synthetic data, scaling laws with respect to dataset size slow down (i.e., exhibit smaller exponents), meaning a much larger sample size is needed to achieve the same reduction in test error as with real data. Furthermore, the optimal scaling law with synthetic data requires different regularization; the optimal settings for real data could lead to catastrophic model collapse. Related findings are reported in Dohmatob et al. [18] in the setting of discrete data for infinite memory models and their variants.

## 6 Experiments

To further support our theoretical findings, we conduct experiments using kernel ridge regression on the MNIST dataset [16], as detailed in Appendix D.2. Our experiments validate the theoretical predictions for both RBF and Polynomial kernels, demonstrating the parallels between linear regression and kernel regression, and highlighting the relevance of our theory to more complex settings.

We also explore the behavior of real neural networks by training two-layer networks in two different settings: fixing the first layer or training both layers (see Appendix D.3). Consistent with our theoretical insights, we observe a linear pattern of model collapse when the first layer is fixed. However, a more severe, nearly quadratic model collapse is observed when both layers are trained, with our theory providing a lower bound for this behavior. These results reinforce the ability of our theory to capture the dynamics of model collapse across varying complexities. Full experimental details and results are provided in Appendix D.

## 7 Concluding Remarks

As we navigate the "synthetic data age", our findings signal a departure from traditional test error rates (e.g. neural scaling laws), introducing novel challenges and phenomena with the integration of synthetic data from preceding AI models into training sets. Our work provides a solid analytical handle for demystifying the model collapse phenomenon as a modification of usual scaling laws caused by fake / synthesized training data.

On the practical side, our analysis reveals that AI-generated data alters the optimal regularization for downstream models and changes the scaling laws. Drawing from the insight that regularization mirrors early stopping [3], our study suggests that models trained on mixed real and AI-generated data may initially improve but later decline in performance (model collapse), necessitating early detection of this inflection point. To preserve model quality when scaling laws are altered, it is essential to employ data filtering and watermarking techniques to distinguish real data from synthetic content. Recent studies have also explored methods for data selection [19] and correction [20]. These observations prompt a re-evaluation of current training approaches and underscores the complexity of model optimization in the era of synthetic data.

## Acknowledgments

YF and JK are supported by the National Science Foundation under NSF Award 1922658. Part of this work was done while JK and YF were hosted by the Centre Sciences de Donnees at the Ecole Normale Superieure (ENS) in 2023/24, whose hospitality they gratefully acknowledge. This work was partially supported through the NYU IT High Performance Computing (HPC) resources, services, and staff expertise.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative models go mad. _arXiv preprint arxiv:2307.01850_, 2023.
* [3] Alnur Ali, J. Zico Kolter, and Ryan J. Tibshirani. A continuous-time view of early stopping for least squares regression. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 1370-1378. PMLR, 16-18 Apr 2019.
* [4] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. 2023.
* [5] Zhidong. Bai and Jack W. (Jack William) Silverstein. _Spectral analysis of large dimensional random matrices_. Springer series in statistics. Springer, New York ;, 2nd ed. edition, 2010. ISBN 9781441906601.
* [6] Raphael Berthier, Francis R. Bach, and Pierre Gaillard. Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model. _CoRR_, abs/2006.08212, 2020. URL https://arxiv.org/abs/2006.08212.
* [7] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining of generative models on their own data. _arXiv preprint arxiv:2310.00429_, 2023.
* [8] Matyas Bohacek and Hany Farid. Nepotistically trained generative-ai models collapse, 2023.
* [9] Martin Briesch, Dominik Sobania, and Franz Rothlauf. Large language models suffer from their own output: An analysis of the self-consuming training loop, 2023.
* [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
* [11] Andrea Caponnetto and Ernesto de Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7:331-368, 2007.
* [12] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* [13] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [14] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114004, nov 2022.
* [15] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Error scaling laws for kernel classification under source and capacity conditions. _Machine Learning: Science and Technology_, 4(3):035033, August 2023. ISSN 2632-2153.

* [16] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [18] Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. A tale of tails: Model collapse as a change of scaling laws. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=KVvku47shW.
* [19] Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and Julia Kempe. Beyond model collapse: Scaling up with synthesized data requires reinforcement. _arXiv preprint arXiv:2406.07515_, 2024.
* [20] Nate Gillman, Michael Freeman, Daksh Aggarwal, HSU Chia-Hong, Calvin Luo, Yonglong Tian, and Chen Sun. Self-correcting self-consuming loops for generative model training. In _Forty-first International Conference on Machine Learning_, 2024.
* [21] Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chloe Clavel. The curious decline of linguistic diversity: Training language models on synthetic text, 2023.
* [22] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. _The Annals of Statistics_, 50(2), 2022.
* [23] Ryuichiro Hataya, Han Bao, and Hiromi Arai. Will large-scale generative models corrupt future datasets? In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 20555-20565, October 2023.
* [24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [25] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [27] Antti Knowles and Jun Yin. Anisotropic local laws for random matrices. _Probability Theory and Related Fields_, 169(1):257-352, 2017.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018.
* [29] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel "Ridgeless" regression can generalize. _The Annals of Statistics_, 48(3), 2020.
* [30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [31] Alexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws, 2022.

* Martinez et al. [2023] Gonzalo Martinez, Lauren Watson, Pedro Reviriego, Jose Alberto Hernandez, Marc Juarez, and Rik Sarkar. Combining generative artificial intelligence (ai) and the internet: Heading towards evolution or degradation? _arXiv preprint arxiv: 2303.01255_, 2023.
* Martinez et al. [2023] Gonzalo Martinez, Lauren Watson, Pedro Reviriego, Jose Alberto Hernandez, Marc Juarez, and Rik Sarkar. Towards understanding the interplay of generative artificial intelligence and the internet. _arXiv preprint arxiv: 2306.06130_, 2023.
* Marcenko and Pastur [1967] V.A. Marcenko and Leonid Pastur. Distribution of eigenvalues for some sets of random matrices. _Math USSR Sb_, 1:457-483, 01 1967.
* Midjourney [2023] Midjourney. Midjourney ai, 2023. URL https://www.midjourney.com/.
* Mobahi et al. [2020] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in Hilbert space. In _Advances in Neural Information Processing Systems_, volume 33, pages 3351-3361. Curran Associates, Inc., 2020.
* Neal [1996] Radford M. Neal. Priors for infinite networks. In _Bayesian Learning for Neural Networks_, pages 29-53. Springer, New York, 1996.
* Pillaud-Vivien et al. [2018] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis R. Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018_, pages 8125-8135, 2018.
* Rahimi and Recht [2008] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2008.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8821-8831. PMLR, 18-24 Jul 2021.
* Richards et al. [2021] Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression under general source condition. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_. PMLR, 2021.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* Rudi and Rosasco [2017] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In _Advances in Neural Information Processing Systems_. Curran Associates Inc., 2017. ISBN 9781510860964.
* Shumailov et al. [2023] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. _arXiv preprint arxiv:2305.17493_, 2023.
* Simon et al. [2021] James B. Simon, Madeline Dickens, and Michael Robert DeWeese. Neural tangent kernel eigenvalues accurately predict generalization. 2021.
* Spigler et al. [2020] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher-student paradigm. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12), 2020.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.

* [48] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_. PMLR, 2022.
* [49] Christopher Williams. Computing with infinite networks. In M.C. Mozer, M. Jordan, and T. Petsche, editors, _Advances in Neural Information Processing Systems_, volume 9. MIT Press, 1996.

## Appendix / Supplementary Material for

Model Collapse Demystified: The Case of Regression

###### Contents

* A Differences to Self-Distillation
* B Related work on Kernel Ridge Regression with Gaussian Design
* C Extension to Kernel Methods
* D Details of Experiments
* D.1 Simulated Data
* D.2 Real Data: Kernel Ridge Regression on MNIST
* D.3 Neural Networks on MNIST
* E Notations
* F Exact Characterization of Test Error Under Model Collapse
* F.1 A General Formula for Test Error
* F.2 Proof of Theorem 4.1 (Rigeless Regression)
* F.3 Proof of Theorem F.1 (Ridge Regression + General Covariance)
* F.4 Proof of Theorem 4.3
* F.5 Proof of Corollary 4.4
* F.6 A Note on Proposition 4.2
* F.7 Proof of Theorem 4.5 and Theorem 4.6 (Model Collapse in the Absence of Label Noise)
* G The Noisy Regime for Power Law Spectra
* H Proof of Results for Power-Law Covariance Spectrum
* H.1 Proof of Theorem 5.1
* H.2 Representation of Clean Test Error
* I Auxiliary Lemmas

## Appendix A Differences to Self-Distillation

An important point we wish to make is that the fake data generation process that we analyse should not be confused with self-distillation as formulated in Mobahi et al. [36] for example. Our settingis inspired by the model collapse phenomenon, where increasingly vast amounts of synthetic data generated by users is posted online, and will necessarily enter the training set of the next foundation model. In this case, we do not have ground truth labels, nor is the generation of synthetic data not controlled by us, but by other users. Therefore, we adopt the setting of solely synthetic labels with added noise.

Specifically, in our setup, at generation \(n>0\), we do not have access to the true labels \(Y_{0}=f_{0}(X)+noise\) for the training samples \(X\), but rather to some \(\hat{Y}_{n}=\hat{f}_{n}(X)+noise\), where \(\hat{f}_{n}\) is an unknown function, which synthesizes fake labels iteratively; the integer \(n\) is the number of iterations. In our work, we make the structural assumption that \(\hat{f}_{n}\) is obtained by iterative / successive regressions on a true dataset \(D_{0}=(X_{0},Y_{0})\). We do not have any control over the creation of these labels, which is reflected by the noise injected at each stage.

In the _self-distillation_ setting, the data generation process actually helps performance of the downstream model. The model has access to training labels from the true data distribution \(Y\), but decides to fit a model on this data, and then use its outputs as the new labels \(Y_{n}:=F_{n}(X,Y)\), iterating this process possibly over severable steps. Thus, self-distillation has control over the data generating process, which is carefully optimized for the next stage training. Specifically, [36] study self-distillation in the same Gaussian regression model underlying our analysis, but in each distillation generation are able to tune the regularization parameter for downstream performance as a function of the original data labels (with the data being the same at each generation). In the setting of model collapse, there is no control over the data generation process, since it constitutes synthesized data which typically comes from the wide web.

Self-distillation for linear regression would amount to a very special instance of our analysis where (1) \(X_{0}=X_{1}=\ldots=X_{n-1}=X_{n}=X\) and (2) \(\sigma_{0}=\ldots=\sigma_{n-1}=0\). That is, there is exactly one design matrix which is used in the data generation process and in the downstream estimator, and also no additional source of label noise is present at the end of each generation.

In the general setup considered in our work, (1) is not imposed. We typically assume that \(X_{0},X_{1},\ldots,X_{n-1},X_{n}\) with \(X_{n}=X\), are all independent random matrices. An exception is line 247 ("The Dependent Case") of Section 3.5, where we assume \(X_{m}=X_{0}\) for all \(m\leq n-1\), and independent of \(X_{n}=X\). That setup (considered for the purposes of showing that model collapse can still occur in the absence of label noise) also assumes \(\sigma_{m}=0\) for all \(m\); the analytic picture which emerges (Theorem 3.5) is already drastically different from what one would get from self-distillation (corresponding to additional assumption that \(X=X_{0}\)).

## Appendix B Related work on Kernel Ridge Regression with Gaussian Design

This model has been studied by a vast body of works. For example, Richards et al. [41], Hastie et al. [22], Bach [4] analyze the classical bias-variance decomposition of the test error for ridge regression in the high dimensional setting where dataset size and dimension diverge proportionately, using tools from Random Matrix Theory (RMT). In Section 4 we significantly extend this type of analysis to training on iteratively generated synthetic data. This model is also particularly attractive because it allows to analyze an important trade-off: the relative decay of the eigenvalues of the kernel (_capacity_) and the coefficients of the target function in feature space (_source_). Sizeable effort has been dedicated to characterize the influence on the decay rate of the test error as a function of these two relative decays (aka _power laws_) [11; 38; 6; 41; 46; 14; 15]. In Section 5 we extend these efforts, in particular based on works of Cui et al. [13; 14] which has given a full characterization of all regimes and test error decay that can be observed at the interplay of noise and regularization, characterizing a crossover transition of rates in the noisy setting. Our work uncovers fascinating new effects as a result of iterative training on synthetic data.

## Appendix C Extension to Kernel Methods

Though we present our results in the case of linear regression in \(\mathbb{R}^{d}\) for clarity, they can be rewritten in equivalent form in the kernel setting. Indeed, as in [11; 45; 14; 29], it suffices to replace \(x\) with a feature map induced by a kernel \(K\), namely \(\psi(x):=K_{x}\in\mathcal{H}_{K}\). Here, \(\mathcal{H}_{K}\) is the reproducing kernel Hilbert space (RKHS) induced by \(K\). In the data distribution (2), we must now replace the Gaussian marginal distribution condition \(x\sim N(0,\Sigma)\) with \(\psi(x)\sim N(0,\Sigma)\). The ground-truthlabeling linear function in (2) is now just a general function \(f_{0}\in L^{2}\). The predictor (7) is then given by (_Representer Theorem_) \(\widehat{f}_{n}^{pred}(x):=K(X,x)^{\top}\widehat{c}_{n}\), with \(\widehat{c}_{n}=(G+\lambda TI_{d})^{-1}Y\in\mathbb{R}^{n}\), where \(K(X,x):=(K(x_{1},x),\ldots,K(x_{T},x))\), and \(G=K(X,X)\in\mathbb{R}^{n\times n}\) is the Gram matrix.

## Appendix D Details of Experiments

We perform the following experiments on both simulated and real data to empirically validate our theoretical results.

### Simulated Data

We consider ordinary / linear ridge regression in \(\mathbb{R}^{d}\), for \(d=300\) and different structures for the covariance matrix \(\Sigma\) of the inputs: isotropic (i.e \(\Sigma=I_{d}\)) and power-law (23), with \((\beta,r)=(2,0.375)\). For each value of \(n\) (the generation index), the fake data-generator is constructed according to the process described in (4). Then, for different values of \(T\) (between 1 and \(1000,000\)), a sample of size \(T\) is drawn from this fake data-generator and then a downstream ridge model (7) is fitted. The test set consists of \(100,000\) clean pairs \((x,y)\) form the true data distribution \(P_{\Sigma,w_{0},\sigma^{2}}\). This experiment is repeated \(10\) times to generate error bars. The results for the isotropic setting are shown in Figure 0(a) and the results for the power-law setting are shown in Figure 0(b). Figure 2 shows the over-parametrized setting.

### Real Data: Kernel Ridge Regression on MNIST

As in Cui et al. [14], Wei et al. [48] we consider a distribution on MNIST [16], a popular dataset in the ML community. The classification dataset contains \(60,000\) training and \(10,000\) test data points (handwritten), with labels from 0 to 9 inclusive. Like in Cui et al. [14], we convert the labels into real numbers (i.e a regression problem) as follows: \(y=\text{label mod }2+\text{ noise }\), where the variance of the noise is \(\sigma^{2}=1\) (for simplicity, we also set \(\sigma^{2}_{0}=1\)). The test set consists of \(10,000\) pairs \((x,y)\), with the labels \(y\) constructed as described in the previous sentence. The fake data used for training is generated as in the previous experiment, but via kernel ridge regression (instead of least squares) with the RBF kernel (bandwidth = \(10^{-4}\)) and the polynomial kernel (degree = \(5\), bandwidth = \(10^{-3}\)). Note that it was empirically shown in Cui et al. [14] that these datasets verify (23) with \((\beta,r)\approx(1.65,0.097)\) in the case of the aforementioned RBF kernel, and \((\beta,r)\approx(1.2,0.15)\) in the case of the polynomial kernel. Then, for different values of \(T\) (between \(1\) and \(1000\)), a sample of size \(T\) is drawn from this fake data-generator and then a downstream kernel ridge model is fitted. Each of these experiments are repeated \(10\) times to generate error bars (due to different realizations of label noise). The results are shown in Figure 4.

### Neural Networks on MNIST

We now further examine model collapse in two-layer neural networks on the MNIST dataset, beyond the linear setting and Gaussian data. We consider two scenarios:

* learning with a random features (RF) model, where the first layer is fixed randomly, and only the second layer is trained, and
* learning with a fully trainable neural network.

For the two-layer network with the first layer fixed, our theory predicts a linear increase in test error as a function of the number of iterations \(n\). This is because such models belong to the linearized regimes as finite-width random feature models and can be approximated by kernel regression [25; 45]. For fully-trained neural networks, our theory does not directly apply. However, we anticipate that the general trends uncovered by our asymptotic theory will hold true -- for example, more parameters are expected to lead to greater model collapse, as shown in Theorem 4.1.

Specifically, the models were trained using stochastic gradient descent (SGD) with a batch size of 128 and a learning rate of 0.1. We employed a regression setting where labels were converted to one-hot vectors, and the model was trained using mean squared error for 200 epochs to convergence. When generating the synthetic data, Gaussian label noise with a standard deviation of 0.1 is added. The test error is consistently evaluated on the test set using clean labels.

The results for RF models of width (i.e number of hidden dimensions) \(k\) of 20,000 are presented in Figure 5. We observe that, with the exception of the first two generations, the decay in MSE loss generally follows a linear trend, which is consistent with the predictions of our theory.

Next, we consider the scenario of training the entire neural network. By varying the width \(k\), we adjust the number of parameters to further explore the theoretical predictions on how the number of parameters influences model collapse.

**Observations.** From Figure 6, we can observe that

* More parameters (wider neural networks, i.e large \(k\)) lead to increased model collapse. This observation is consistent with our results proved in the linear regime (Theorem 4.1). For linear models, the number of parameters is proportional to \(d\) (the input dimension), whereas in two-layer neural networks, the number of parameters is of order \(kd\) (i.e proportional to the width \(k\)).
* The dependence of model collapse on the number of iterations \(n\) is linear for small values of \(n\) (with \(n\leq 4\) in our experiments), and becomes superlinear (possibly quadratic) for larger values of \(n\) (with \(n\geq 4\)). Recall that \(n=0\) corresponds to training on clean data from the data distribution. Thus, possibly, model collapse neural networks appears to be even more severe than in linear regression.

Figure 4: **Model collapse in kernel ridge regression (power-law covariance spectrum) on MNIST. Here, we use adaptive regularization \(T^{-\ell}\) for different values of the exponent \(\ell\geq 0\) (see Section D for full experimental setup). Top row: RBF kernel. Bottom row: polynomial kernel. In each plot, we show test error curves as a function of sample size \(T\), from different generations (\(n\)) of fake data. The broken vertical line corresponds to \(T=T_{0}\), where \(T_{0}\) is the number of samples (from the true data distribution) which was used to train the label faker. The value of the exponent regularization \(\ell=\ell_{\star}\) (broken curves) is the optimal value in the presence of iterative data relabeling, while \(\ell=\ell_{crit}\) (solid curves) corresponds to the optimal value without iterative re-labelling (i.e \(n=0\)) proposed in Cui et al. [14] (see (26)). Specifically, we take \(\ell_{\star}=(b-a)\ell_{crit}=b\ell_{crit}\), where \(b=\log T_{0}/\log T\) (so that \(T_{0}=T^{b}\)), as proposed in Theorem 5.1, formula (28). Notice how the effect of fake data makes the test error become non decreasing in sample size \(T\). This is effectively a collapse of the learned model.**

## Appendix E Notations

The set of integers from \(1\) through \(d\) is denoted \([d]\). Given a variable \(z\) (which can be the input dimension \(d\) or the sample size \(\tilde{T}\), etc.) the notation \(f(z)\lesssim g(z)\) means that \(f(z)\leq Cg(z)\) for sufficiently large \(z\) and an absolute constant \(C\), while \(f(z)\asymp g(z)\) means \(f(z)\lesssim g(z)\lesssim f(z)\). Further, \(f(z)\simeq g(z)\) means \(f(z)=(1+o(1))g(z)\), where \(o(1)\) stands for a quantity which tends to zero in the limit \(z\to\infty\). We denote with \(A^{\dagger}\) the Moore-Penrose pseudo-inverse any matrix \(A\), and by \(\|A\|_{op}\) is operator norm, while the trace of a square matrix \(A\) is denoted \(\operatorname{tr}A\). Finally, \(\|u\|_{\Sigma}:=\sqrt{u^{\top}\Sigma u}\) is the Mahalanobis norm induced by a positive-definite matrix \(\Sigma\).

## Appendix F Exact Characterization of Test Error Under Model Collapse

### A General Formula for Test Error

We now consider the case of general ridge penalty \(\lambda>0\), and drop the requirements \(T\geq d+2\) and \(T_{0}\geq d+2\). Recall the definitions of \(X,Y,E\) and the random matrices \(R\) and \(\widehat{\Sigma}\) appearing in (7). For later reference, define

\[Bias:=\mathbb{E}\,\|\widehat{\Sigma}Rw_{0}-w_{0}\|_{\Sigma}^{2},\] (29) \[Var=\mathbb{E}\,\|RX^{\top}E/T\|_{\Sigma}^{2}=\sigma^{2}\frac{1 }{T}\operatorname{tr}\Sigma R^{2}\widehat{\Sigma}.\] (30)

These are respectively the bias and variance terms in the classical bias-variance decomposition

\[E_{test}^{clean}:=Bias+Var,\] (31)

for standard ridge regression fitted on clean data from the true data distribution \(P_{\Sigma,w_{0},\sigma^{2}}\) (e.g., see Hastie et al. [22]).

**Theorem F.1**.: _For an \(n\)-fold fake data generation process, the test error of a ridge predictor \(\widehat{w}_{n}^{pred}\) based on a sample of size \(T\geq 1\) with regularization parameter \(\lambda\) is given by_

\[\left.\begin{aligned} E_{test}(\widehat{w}_{n}^{pred})& =\widetilde{Bias}+Var+n\sigma_{0}^{2}\rho,\\ \widetilde{Bias}&=\mathbb{E}\,\|\widehat{\Sigma}RQ_{ -1}w_{0}-w_{0}\|_{\Sigma}^{2},\\ \rho&:=\frac{1}{n}\sum_{m=0}^{n-1}\mathbb{E}\, \operatorname{tr}C_{n-1,m}\widehat{\Sigma}R\Sigma R\widehat{\Sigma},\end{aligned}\right\}\] (32)

_where \(Var\) is as given in (30) and \(C_{k,m}:=\overline{Q}_{k,m}\overline{Q}_{k,m}^{\top}\) for \(\overline{Q}_{k,m}=Q_{k,m}X_{m}^{\dagger}\), \(Q_{k,m}:=P_{k}P_{k-1}\dots P_{m}\), \(Q_{k}:=Q_{k,0}=P_{k}P_{k-1}\dots P_{0}\), with \(P_{m}=X_{m}^{\dagger}X_{m}\) being the orthogonal projection matrix onto the subspace of \(\mathbb{R}^{d}\) spanned by the rows of \(X_{m}\).__In particular, if \(T_{0}\geq d+2\) (under-parametrized data-generator), then \(\widetilde{Bias}=Bias\) as in (29), and_

\[\left.\begin{aligned} E_{test}(\widetilde{w}_{n}^{pred})& \simeq E_{test}^{clean}+n\sigma_{0}^{2}\rho,\\ \rho&=\frac{1}{T_{0}-d-1}\mathbb{E}\operatorname{tr }\Sigma^{-1}\widehat{\Sigma}R\Sigma\widehat{\Sigma}R.\end{aligned}\right\}\] (33)

In the second part of the theorem, the term \(E_{test}^{clean}\) (introduced earlier in (31)) corresponds to the usual test error when the downstream model is trained on real (not fake) data, for which well-known formulae exist in a variety of scenarios (see Proposition 4.2).

**Remark F.2**.: _We show in Theorem 4.3 that \(\widetilde{Bias}\geq Bias+\Delta Bias\), where \(\Delta Bias\geq 0\) in the appropriate asymptotic limit, with equality if \(T_{0}\geq d+2\) (the under-parametrized regime). Thus, apart from the variance term, an over-parametrized (\(T_{0}<d+2\)) synthetic data-generator harms the bias term of the test error of downstream models. In contrast, an under-parametrized synthetic data-generator (\(T_{0}\geq d+2\)) only harms the variance. The increase in bias suffered in the over-parametrized regime is precisely quantified in Section 4.5, and shown to be an increasing function of the number of generations \(n\)._

The test error decomposition in Theorem F.1 is thus of the promised form (1). This additional term means that there is competition between the usual test error \(E_{test}^{clean}\) and the additional term induced by the fake labeling process. Understanding the interaction of these two terms is key to demystifying the origins of model collapse.

Low-Dimensional Limit.Observe that if \(d\) is fixed and \(T\to\infty\), then the empirical covariance matrix \(\widehat{\Sigma}\) converges to2 its population version \(\Sigma\), and so for \(T_{0}\geq d+2\), we have

Footnote 2: e.g. weakly, w.r.t. operator-norm.

\[\rho\simeq\frac{\operatorname{tr}\Sigma^{2}(\Sigma+\lambda I_{d})^{-2}}{T_{0} -d}=\frac{\operatorname{d}\!\operatorname{f}_{2}(\lambda)}{T_{0}-d},\]

where for any \(\lambda\geq 0\) and \(m\in\mathbb{N}_{\star}\), \(\operatorname{d}\!\operatorname{f}_{m}(\lambda)\) is the \(m\)th order "degree of freedom" of the covariance matrix \(\Sigma\) is given by

\[\operatorname{d}\!\operatorname{f}_{m}(\lambda)=\operatorname{d}\! \operatorname{f}_{m}(\lambda;\Sigma):=\operatorname{tr}\Sigma^{m}(\Sigma+ \lambda I_{d})^{-m}.\]

Note that \(\operatorname{d}\!\operatorname{f}_{m}(\lambda)\leq d\) always. In the high-dimensional setting (where \(d\) can grow beyond \(T_{0}\)), the precise analysis of \(\rho\) will be carried out via random matrix theory (RMT).

### Proof of Theorem 4.1 (Rigeless Regression)

The proof is by induction on the number of generations \(n\) of fake data. For \(n=0\), we have

\[\begin{split} E_{test}(\widetilde{w}_{0}^{pred})&= \mathbb{E}\left\|\widehat{w}_{0}^{pred}-w_{0}\right\|_{\widehat{\Sigma}}^{2} =\mathbb{E}\left\|\widehat{w}_{0}^{pred}-\widehat{w}_{0}\right\|_{2}^{2}= \mathbb{E}\|(X_{0}^{\top}X_{0})^{-1}X_{0}^{\top}E_{0}\|_{2}^{2}\\ &=\sigma^{2}\mathbb{E}\operatorname{tr}(X_{0}^{\top}X_{0})^{-1} =\sigma^{2}\frac{d}{T-d-1}\simeq\frac{\sigma^{2}\phi}{1-\phi},\end{split}\] (34)

where \(\phi:=d/T\in(0,1)\) and the last step has made use of Lemma F.3 below. This is a well-known result for the test error of linear regression in the under-parametrized regime, without any AI pollution (fake / synthesized training data).

Analogously, for \(n=1\) one computes the test error after the first generation of fake data as follows

\[E_{test}(\widehat{w}_{1}^{pred}) =\mathbb{E}\|\widehat{w}_{1}^{pred}-w_{0}\|_{\Sigma}^{2}=\mathbb{E} \|\widehat{w}_{1}^{pred}-\widehat{w}_{0}\|_{2}^{2}\] \[=\mathbb{E}\|\widehat{w}_{1}^{pred}-\widehat{w}_{1}+\widehat{w}_{ 1}-\widehat{w}_{0}\|_{2}^{2}\] \[=\mathbb{E}\|(X_{1}^{\top}X_{1})^{-1}X_{1}^{\top}E_{1}+\widehat{w} _{0}^{pred}-\widehat{w}_{0}\|_{2}^{2}\] \[=\mathbb{E}\left\|w_{0}-\widehat{w}_{0}^{pred}\right\|_{2}^{2}+ \mathbb{E}\left\|(X_{1}^{\top}X_{1})^{-1}X_{1}^{\top}E_{1}\right\|_{2}^{2}\] \[=E_{test}(\widehat{w}_{0}^{pred})+\frac{\sigma_{1}^{2}d}{T_{1}-d-1}\] \[=E_{test}(\widehat{w}_{0}^{pred})+\frac{\sigma_{0}^{2}d}{T_{0}-d-1}\] \[\simeq\frac{\sigma^{2}\phi}{1-\phi}+\frac{\sigma_{0}^{2}\phi_{0} }{1-\phi_{0}},\]

where \(\phi_{0}=d/T_{0}\in(0,1)\). Continuing the induction on \(n\), we obtain the result. 

**Lemma F.3**.: _Let \(X_{0}\) be an \(T_{0}\times d\) random matrix with iid rows from \(N(0,\Sigma)\). If \(T_{0}\geq d+2\), then the empirical covariance matrix \(\widehat{\Sigma}_{0}:=X_{0}^{\top}X_{0}/T_{0}\) is invertible a.s and_

\[\mathbb{E}\left[\widehat{\Sigma}_{0}^{-1}\right]=\frac{T_{0}}{T_{0}-d-1}\Sigma ^{-1}.\]

### Proof of Theorem F.1 (Ridge Regression + General Covariance)

#### f.3.1 Representation of \(\widehat{w}_{n}\) and \(\widehat{w}_{n}^{pred}\)

We first obtain explicit formulae for the labelling vectors \(\widehat{w}_{n}\) used in the fake-data generation process (5). For any integer \(m\geq 0\), define \(P_{m}=X_{m}^{\dagger}X_{m}\), the orthogonal projection matrix onto the subspace of \(\mathbb{R}^{d}\) spanned by the rows of \(X_{m}\). Observe from (5) that

\[\begin{split}\widehat{w}_{n}&=X_{n-1}^{\dagger} \overline{Y}_{n-1}=X_{n-1}^{\dagger}(X_{n-1}\widehat{w}_{n-1}+E_{n-1})=P_{n-1 }\widehat{w}_{n-1}+X_{n-1}^{\dagger}E_{n-1}\\ &=P_{n-1}X_{n-2}^{\dagger}(X_{n-2}\widehat{w}_{n-2}+E_{n-2})+X_{n -1}^{\dagger}E_{n-1}\\ &=P_{n-1}P_{n-2}\widehat{w}_{n-2}+P_{n-1}X_{n-2}^{\dagger}E_{n-2 }+X_{n-1}^{\dagger}E_{n-1}\\ &\quad\vdots\\ &=P_{n-1}P_{n-2}\ldots P_{0}w_{0}+P_{n-1}P_{n-2}\ldots P_{1}X_{1}^ {\dagger}E_{1}+P_{n-1}P_{n-2}\ldots P_{2}X_{2}^{\dagger}E_{2}+\ldots\\ &\quad\vdots\\ &=P_{n-1}P_{n-2}\ldots P_{0}w_{0}+\sum_{m=0}^{n-1}P_{n-1}P_{n-2} \ldots P_{m}X_{m}^{\dagger}E_{m}.\end{split}\] (35)

We get the following result.

**Lemma F.4**.: _For any \(n\geq 0\), the following formula holds_

\[\widehat{w}_{n}=\begin{cases}w_{0},&\text{if }n=0,\\ Q_{n-1}w_{0}+\sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E_{m},&\text{if }n\geq 1, \end{cases}\] (36)

_where \(\overline{Q}_{k,m}:=Q_{k,m}X_{m}^{\dagger}\), \(Q_{k,m}:=P_{k}P_{k-1}\ldots P_{m}\) and \(Q_{k}:=Q_{k,0}=P_{k}P_{k-1}\ldots P_{0}.\) Moreover, \(\widehat{w}_{n}\in\operatorname{Im}P_{n-1}\) as soon as \(n\geq 1\)._

_In particular, under the simplifying condition (17), it holds that_

\[\widehat{w}_{n}=\begin{cases}w_{0},&\text{if }n=0,\\ P_{0}w_{0}+X_{0}^{\dagger}\overline{E}_{n-1}\in\operatorname{Im}P_{0},&\text{if }n \geq 1.\end{cases}\] (37)

_where \(\overline{E}_{n-1}:=\sum_{m=0}^{n-1}E_{m}\), a random vector of length \(T_{0}\), with iid entries from \(N(0,n\sigma_{0}^{2})\), and independent of \(X_{0}\). Moreover, \(\widehat{w}_{n}\in\operatorname{Im}P_{0}\) as soon as \(n\geq 1\)._Note that the second part of the result uses the elementary linear-algebraic fact that \(P_{m}X_{m}^{\dagger}=X_{m}^{\dagger}\). In the special case where \(T_{0}\geq d\), we have \(P_{0}=I\) a.s., and so \(\widehat{w}_{n}=w_{0}+X_{0}^{\dagger}\overline{E}_{n-1}\). Otherwise, even in the absence of generator noise (\(\sigma_{0}=0\)), the fake data labeller \(\widehat{w}_{n}=P_{0}w_{0}\) drifts away from the truth \(w_{0}\), into a subspace of \(\mathbb{R}^{d}\) spanned by the rows of \(X_{0}\).

Next, let us obtain a decomposition for the downstream predictor \(\widehat{w}_{n}^{pred}\) defined in (7). As usual, let \(\widehat{\Sigma}:=X^{\top}X/T\) be the empirical covariance matrix with resolvent \(R=(\widehat{\Sigma}+\lambda I)^{-1}\), and observe that the downstream model writes

\[\begin{split}\widehat{w}_{n}^{pred}&=RX^{\top} \overline{Y}_{n}(X)/T=RX^{\top}(X\widehat{w}_{n}+E)/T\\ &=RX^{\top}(XQ_{n-1}w_{0}+X\sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E _{m}+E)/T\\ &=R\widehat{\Sigma}Q_{n-1}w_{0}+RX^{\top}E/T+R\widehat{\Sigma} \sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E_{m}.\end{split}\] (38)

#### f.3.2 Proof of Theorem F.1

Using the decomposition (38) for the downstream model \(\widehat{w}_{n}^{pred}\), we deduce that

\[\begin{split} E_{test}(\widehat{w}_{n}^{pred})&= \mathbb{E}\left\|\widehat{w}_{n}^{pred}-w_{0}\right\|_{\Sigma}^{2}\\ &=\mathbb{E}\left\|R\widehat{\Sigma}P_{0}w_{0}+RX^{\top}E/T+R \widehat{\Sigma}\sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E_{m}-w_{0}\right\|_{ \Sigma}^{2}\\ &=\mathbb{E}\left\|R\widehat{\Sigma}P_{0}w_{0}-w_{0}+RX^{\top}E/ T+R\widehat{\Sigma}\sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E_{m}\right\|_{ \Sigma}^{2}\\ &=\mathbb{E}\|R\widehat{\Sigma}P_{0}w_{0}-w_{0}\|_{\Sigma}^{2}+ \mathbb{E}\left\|RX^{\top}E/T\right\|_{\Sigma}^{2}+\mathbb{E}\left\|R\widehat{ \Sigma}\sum_{m=0}^{n-1}\overline{Q}_{n-1,m}E_{m}\right\|_{\Sigma}^{2}\\ &=\widetilde{Bias}+Var+n\sigma_{0}^{2}\rho,\end{split}\] (39)

where \(\widehat{\Sigma}:=X^{\top}X/T\), \(\widetilde{Bias}\), \(Var\), and \(\rho\) are as given in the theorem. On the second line, we have used independence (of \(X\), \(X_{0}\), \(E\), and \(\overline{E}_{n-1}\)) and the fact that \(E\) and \(\overline{E}_{n-1}\) are centered Gaussian random vectors, with iid components of variances \(\sigma^{2}\) and \(n\sigma_{0}^{2}\) respectively. 

### Proof of Theorem 4.3

Analysis of Bias-like Term.An exact analysis of the \(\widetilde{Bias}\) term appearing in Theorems F.1 and 4.3 is presumably a treacherous enterprise given dependency on \(X\) (via \(R\) and \(\widehat{\Sigma}\)) and \(X_{0}\) (via \(P_{0}\)). In place of such an analysis, we shall settle for the following result which gives an instructive lower-bound.

**Proposition F.5**.: _In the RMT limit (12), it holds that_

\[\lim\widetilde{Bias}-\lim Bias\geq\lim\mathbb{E}\left\|R\widehat{\Sigma}P_{0 }w_{0}-R\widehat{\Sigma}w_{0}\right\|_{\Sigma}^{2}\geq 0.\]

_Thus, training on fake / synthesized data increases the bias term of the downstream model's test error!_

Proof.: Letting \(A:=R\widehat{\Sigma}\), one computes

\[\begin{split}\widetilde{Bias}-Bias&=\|AP_{0}w_{0}- w_{0}\|_{\Sigma}^{2}-\|Aw_{0}-w_{0}\|_{\Sigma}^{2}\\ &=\|AP_{0}w_{0}-Aw_{0}+Aw-w\|_{\Sigma}^{2}-\|Aw_{0}-w_{0}\|_{ \Sigma}^{2}\\ &=\|AP_{0}w_{0}-Aw_{0}\|_{\Sigma}^{2}+2w_{0}^{\top}(A-P_{0}A) \Sigma(I-A)w_{0}\\ &=\|AP_{0}w_{0}-Aw_{0}\|_{\Sigma}^{2}+2w_{0}^{\top}(I-P_{0})A \Sigma(I-A)w_{0}.\end{split}\]

It then suffices to observe that, in the RMT limit (12), it holds that

\[\lim\mathbb{E}\,w_{0}^{\top}(I-P_{0})A\Sigma(I-A)w_{0}\geq 0,\]

as can be seen from repeated application of Propositions 1 and 2 of Bach [4].

Analysis of \(\rho\) Term.Define a \(d\times d\) random psd matrix \(H:=\hat{\Sigma}R\hat{\Sigma}\hat{R}\). Under the simplifying assumption (17), the matrices \(\overline{Q}_{k,m}\) defined in the theorem all equal \(\overline{Q}_{0,0}=X_{0}^{\dagger}\). It follows that the \(\rho\)-term in (32) then writes

\[\rho=\frac{1}{n}\sum_{m=0}^{n-1}\mathbb{E}\left[\overline{Q}_{n-1,m}\overline{Q }_{n-1,m}^{\top}H\right]=\mathbb{E}\left[\operatorname{tr}X_{0}^{\dagger}(X_{0} ^{\dagger})^{\top}H\right]=\mathbb{E}_{H}\mathbb{E}\left[\operatorname{tr}X_{ 0}^{\dagger}(X_{0}^{\dagger})^{\top}H\mid H\right].\] (40)

Now, one computes the conditional expectation as follows

\[\mathbb{E}\left[\operatorname{tr}X_{0}^{\dagger}(X_{0}^{\dagger })^{\top}H\mid H\right] =\mathbb{E}\left[\operatorname{tr}X_{0}^{\top}(X_{0}X_{0}^{\top })^{-2}X_{0}H\mid H\right]\] \[=\lim_{\lambda_{0}\to 0^{+}}\frac{1}{T_{0}}\frac{\partial}{ \partial\lambda_{0}}\mathbb{E}\left[\operatorname{tr}X_{0}^{\top}(X_{0}X_{0} ^{\top}+\lambda_{0}T_{0}I)^{-1}X_{0}H\mid H\right].\]

Furthermore, defining \(A:=\Sigma^{1/2}H\Sigma^{1/2}\) and \(Z_{0}=X_{0}\Sigma^{-1/2}\), we have

\[\operatorname{tr}X_{0}^{\top}(X_{0}X_{0}^{\top}+\lambda_{0}T_{0} I)^{-1}X_{0}H =\operatorname{tr}\Sigma^{1/2}Z_{0}^{\top}(Z_{0}\Sigma Z_{0}^{ \top}+\lambda_{0}T_{0}I)^{-1}Z_{0}\Sigma^{1/2}H\] \[=\operatorname{tr}AZ_{0}^{\top}(Z_{0}\Sigma Z_{0}^{\top}+\lambda_ {0}T_{0}I)^{-1}Z_{0},\]

We deduce from Proposition 2 of Bach [4] that

\[\mathbb{E}\left[\operatorname{tr}X_{0}^{\top}(X_{0}X_{0}^{\top}+ \lambda_{0}T_{0}I)^{-1}X_{0}H\mid H\right] \simeq\operatorname{tr}A(\Sigma+\kappa(\lambda_{0},T_{0})I)^{-1}\] \[=\operatorname{tr}H(\Sigma+\kappa(\lambda_{0},T_{0})I)^{-1}\Sigma.\]

Differentiating w.r.t. \(\lambda_{0}\) and letting this parameter tend to zero from above gives

\[\mathbb{E}\left[\operatorname{tr}X_{0}^{\dagger}(X_{0}^{\dagger })^{\top}H\mid H\right] =-\frac{1}{T_{0}}\lim_{\lambda_{0}\to 0^{+}}\frac{\partial}{ \partial\lambda_{0}}\mathbb{E}\left[\operatorname{tr}X_{0}^{\top}(X_{0}X_{0} ^{\top}+\lambda_{0}T_{0}I)^{-1}X_{0}H\mid H\right]\] \[\simeq-\frac{1}{T_{0}}\lim_{\lambda_{0}\to 0^{+}}\frac{\partial \kappa(\lambda_{0},T_{0})}{\partial\lambda_{0}}\cdot\frac{\partial}{ \partial t}\operatorname{tr}H(\Sigma+tI)^{-1}\Sigma\bigg{|}_{t=\kappa(\lambda_ {0},T_{0})}\] \[\simeq\frac{\operatorname{tr}H(\Sigma+\kappa_{0}I)^{-2}\Sigma}{T _{0}-\operatorname{df}_{2}(\kappa_{0})},\]

where \(\kappa_{0}=\kappa(0,T_{0})\), and we have made use of Lemma I.2. Combining with (40) and then applying Proposition 1 of Bach [4] to compute \(\mathbb{E}_{H}\operatorname{tr}H(\Sigma+\kappa_{0}I)^{-2}\Sigma=\mathbb{E}_{X }\operatorname{tr}\hat{\Sigma}R\hat{\Sigma}\hat{R}(\Sigma+\kappa_{0}I)^{-2}\Sigma\) gives the following result.

**Proposition F.6**.: _In the RMT limit (12), it holds for any \(\lambda>0\) that_

\[\rho=\frac{\operatorname{tr}\Sigma^{4}(\Sigma+\kappa_{0}I)^{-2}(\Sigma+\kappa I )^{-2}}{T_{0}-\operatorname{df}_{2}(\kappa_{0})}+\frac{\kappa^{2}\operatorname {tr}\Sigma^{2}(\Sigma+\kappa_{0}I)^{-2}(\Sigma+\kappa I)^{-2}}{T_{0}- \operatorname{df}_{2}(\kappa_{0})}\cdot\frac{\operatorname{df}_{2}(\kappa)}{T -\operatorname{df}_{2}(\kappa)},\] (41)

_where \(\kappa_{0}:=\kappa(\lambda_{0},T_{0})\) and \(\kappa=\kappa(\lambda,T)\)._

_In particular, if \(T_{0}\geq d\), then_

\[\rho\simeq\frac{\operatorname{df}_{2}(\kappa)}{T-\operatorname{df}_{2}(\kappa )}\left(1+\frac{\kappa^{2}\operatorname{tr}(\Sigma+\kappa I)^{-2}}{T_{0}- \operatorname{df}_{2}(\kappa_{0})}\right).\] (42)

This result completes the proof of Theorem 4.3. 

### Proof of Corollary 4.4

For the first part, we know from Theorem F.1 that

\[E_{test}(\widehat{w}_{n}^{pred})=E_{test}(\widehat{w}_{0}^{pred})+n \sigma_{0}^{2}\rho,\text{ with }\] (43) \[\rho:=\frac{\mathbb{E}\operatorname{tr}\Sigma^{-1}\hat{\Sigma}( \hat{\Sigma}+\lambda I)^{-1}\Sigma(\hat{\Sigma}+\lambda I)^{-1}\hat{\Sigma}}{T _{0}-d}.\] (44)The \(E_{test}(\tilde{w}_{0}^{pred})\) term is taken care of by Proposition 4.2, since this corresponds to generalization error on clean training data. For the \(\rho\) term, we use Proposition 1 of Bach [4] with \(A=\Sigma^{-1}\) and \(B=\Sigma\) to get

\[\rho \simeq\frac{\operatorname{tr}(\Sigma+\kappa I)^{-2}\Sigma^{2}}{T_ {0}-d}+\frac{\kappa^{2}\operatorname{tr}(\Sigma+\kappa I)^{-2}}{T_{0}-d}\frac{ \operatorname{tr}(\Sigma+\kappa I)^{-2}\Sigma^{2}}{T-\operatorname{df}_{2}( \kappa)}\] \[=\frac{\operatorname{df}_{2}(\kappa)}{T_{0}-d}+\frac{\kappa^{2} \operatorname{tr}(\Sigma+\kappa I)^{-2}}{T_{0}-d}\frac{\operatorname{df}_{2}( \kappa)}{T-\operatorname{df}_{2}(\kappa)},\]

which proves the first part of the result.

For the second part, note that \(\operatorname{df}_{2}(\kappa)=d/(1+\kappa)^{2}\) when \(\Sigma=I\), (10) holds, and so

\[(1-1/\phi_{0})\rho \simeq\frac{1}{(1+\kappa)^{2}}+\frac{\kappa^{2}}{(1+\kappa)^{4} }\frac{d}{T-d/(1+\kappa)^{2}}\] \[\simeq\frac{1}{(1+\kappa)^{2}}+\frac{\kappa^{2}}{(1+\kappa)^{4} }\frac{\phi}{1-\phi/(1+\kappa)^{2}}\] \[=\frac{1}{(1+\kappa)^{2}}+\frac{1}{(1+\kappa)^{2}}\frac{\phi \kappa^{2}}{(1+\kappa)^{2}-\phi},\]

and the result follows. 

### A Note on Proposition 4.2

As mentioned in the main text, the result is classical Richards et al. [41], Hastie et al. [22], Bach [4]). Only the second part needs a comment which we now provide. Indeed, the second part of the result follows from the first as we now see. Indeed, \(w_{0}^{\top}\Sigma(\Sigma+\kappa I)^{-2}w_{0}=r^{2}/(1+\kappa)^{2}\), \(\operatorname{df}_{2}(\kappa)=d/(1+\kappa)^{2}\) and so we deduce from the first part that

\[Var \simeq\sigma^{2}\phi\frac{1}{(1+\kappa)^{2}}\frac{1}{1-\phi/(1+ \kappa)^{2}}=\frac{\sigma^{2}\phi}{(1+\kappa)^{2}-\phi},\] \[Bias \simeq\kappa^{2}\|w_{0}\|_{2}^{2}\frac{1}{(1+\kappa)^{2}}\frac{1 }{1-\phi/(1+\kappa)^{2}}=\frac{\kappa^{2}\|w_{0}\|_{2}^{2}}{(1+\kappa)^{2}- \phi},\]

from which the result follows. 

We now need to estimate \(\delta^{\top}H\delta\) for a deterministic psd matrix \(H\). Observe that

\[\delta^{\top}H\delta =(Q_{n-1}w_{0}-w_{0})^{\top}H(Q_{n-1}w_{0}-w_{0})\] (45) \[=w_{0}^{\top}Q_{n-1}^{\top}HQ_{n-1}w_{0}-2w_{0}^{\top}Q_{n-1}^{ \top}Hw_{0}+w_{0}^{\top}Hw_{0}.\]

### Proof of Theorem 4.5 and Theorem 4.6 (Model Collapse in the Absence of Label Noise)

We first prove Theorem 4.6. Note that since we are in the isotropic case, \(\Delta Bias\) defined in (15) is now given by \(\Delta Bias:=\mathbb{E}\,\|\widehat{\Sigma}R(Q_{n-1}w_{0}-w_{0})\|^{2}\), where \(Q_{n-1}:=P_{n-1}P_{n-1}\dots P_{0}\). Moreover, since \(T>d\) and \(\lambda=0\) by assumption, we have \(\Sigma R=I_{d}\), and so we further have \(\Delta Bias:=\mathbb{E}\,\|Q_{n-1}w_{0}-w_{0}\|^{2}\). Now, one computes

\[\|Q_{n-1}w_{0}-w_{0}\|^{2} =\|w_{0}\|^{2}-2w_{0}^{\top}Q_{n-1}w_{0}+w_{0}^{\top}Q_{n-1}^{ \top}Q_{n-1}w_{0}\] \[=\|w_{0}\|^{2}-w_{0}^{\top}Q_{n-1}w_{0}\] \[\simeq\|w_{0}\|^{2}-w_{0}^{\top}\left(\prod_{m=0}^{n-1}(I+\kappa_{ m}I)^{-1}\right)w_{0}\] (46) \[=\|w_{0}\|^{2}-\|w_{0}\|^{2}\prod_{m=0}^{n-1}\min(1/\phi_{m},1),\]

where on the 2nd line we have used the fact that \(Q_{n-1}^{\top}Q_{n-1}=Q_{n-1}\) because the \(P_{m}\)'s are projections; on the 3rd line we have used Lemma F.7 with \(\Sigma=I\) and \(u=v=w_{0}\); on the 4th line we have used the fact that \(\kappa_{m}:=\kappa(0,T_{m})=\max(\phi_{m}-1,0)=\max(\phi_{m},1)-1\). This completes the proof of Theorem 4.6.

The proof of Theorem 4.5 is completely analogous, with \(Q_{n-1}\) replaced with \(Q_{0}\). 

**Lemma F.7**.: _Let \(X_{0},\ldots,X_{n-1}\) be independent random matrices of shapes \(T_{m}\times d\) for \(m=0,\ldots,n-1\), with rows iid from \(N(0,\Sigma)\), and let \(Q_{n-1}:=P_{n-1}P_{n-2}\ldots P_{0}\), where \(P_{m}=X_{m}^{\dagger}X_{m}\) is the orthogonal projection onto the subspace of \(\mathbb{R}^{d}\) spanned by the rows of \(X_{m}\). Then, in the limit \(d,T_{0},\ldots,T_{n-1}\to\infty\) such that \(d/T_{0}\to\phi_{0}\in(0,\infty),\ldots,d/T_{n-1}\to\phi_{n-1}\in(0,\infty)\) with \(\|\Sigma\|_{op},\|\Sigma^{-1}\|_{op}=O(1)\), it holds that for deterministic \(L_{2}\)-bounded sequences of vectors \(u\) and \(v\)_

\[u^{\top}Q_{n-1}v\simeq u^{\top}\left(\prod_{m=0}^{n-1}(\Sigma+\kappa_{m}I)^{- 1}\right)v,\] (47)

_where \(\kappa_{m}=\kappa(0,T_{m})\) is as defined in (9)._

Proof.: The prof is by induction on \(n\geq 1\). For \(n=1\), we have \(Q_{n-1}=Q_{0}=P_{0}\). Thus,

\[\begin{split} u^{\top}Q_{0}v&=u^{\top}P_{0}v=\lim_{ t\to 0^{+}}u^{\top}X_{0}^{\top}(X_{0}X_{0}^{\top}+tI)^{-1}X_{0}v\\ &\simeq\lim_{t\to 0^{+}}u^{\top}(\Sigma+\kappa(t,T)I)^{-1}v=u^{ \top}(\Sigma+\kappa_{0}I)^{-1}v,\end{split}\] (48)

where \(\kappa_{0}:=\kappa(0,T)\) and we used Proposition 2 of Bach [4] at the beginning of the 2nd line. Now, suppose the claim holds for \(n\), and let's prove that it holds for \(n+1\). Indeed,

\[u^{\top}Q_{n}v=u^{\top}P_{n}Q_{n-1}v\simeq u^{\top}P_{n-1}\prod_{m=0}^{n-1}( \Sigma+\kappa_{m})^{-1}v\simeq u^{\top}\prod_{m=0}^{n}(\Sigma+\kappa_{m})^{-1 }v,\]

where the second step is an application of the induction hypothesis with \(P_{n}u\) in place of \(u\). 

The following lemma can be used to compute \(\|Q_{n-1}w_{0}-w_{0}\|_{\Sigma}^{2}\) in the case of anisotropic \(\Sigma\).

**Lemma F.8**.: _Under the hypothesis of Lemma F.7, it holds that_

\[u^{\top}Q_{n-1}v \simeq u^{\top}\Sigma^{n}\left(\prod_{m=0}^{n-1}(\Sigma+\kappa_{m }I)^{-1}\right)v,\] (49) \[u^{\top}Q_{n-1}^{\top}\Sigma Q_{n-1}v \simeq u^{\top}\Sigma^{n}\left(\prod_{m=0}^{n-1}A_{m}\right)v,\] (50) \[\text{with }A_{m}:=(\Sigma+\kappa_{m}I)^{-2}\left(\Sigma^{2}+ \frac{\kappa_{m}^{2}\mathrm{df}_{2}(\kappa_{m})}{T-\mathrm{df}_{2}(\kappa_{m} )}I\right),\] (51)

_where \(\kappa_{m}:=\kappa(0,T_{m})\) as defined in (9)._

Proof.: The first formula follows directly from Lemma F.7 with \(u\) replaced with \(\Sigma u\). For the second formula, we can write

\[\begin{split} u^{\top}Q_{n-1}^{\top}MQ_{n-1}v&=u^{ \top}P_{0}P_{1}\ldots P_{n-2}P_{n-1}MP_{n-1}P_{n-2}\ldots P_{0}P_{1}v\\ &=\widetilde{u}_{n-1}^{\top}P_{n-1}MP_{n-1}\widetilde{v}_{n-1}, \end{split}\]

where \(\widetilde{u}_{n-1}:=P_{n-2}\ldots P_{0}u\). So we really only need to prove the result for \(n=1\); the general case will follow by induction and due to multiplicativity. Indeed, defining \(A=\Sigma^{1/2}uv^{\top}\Sigma^{1/2}\), \(B=\Sigma^{1/2}M\Sigma^{1/2}\), and \(Z_{0}=X_{0}\Sigma^{-1/2}\), we have

\[\begin{split} u^{\top}P_{0}MP_{0}v&=\lim_{t\to 0^{+}}u^{ \top}X_{0}^{\top}(X_{0}X_{0}^{\top}+tI)^{-1}X_{0}MX_{0}^{\top}(X_{0}X_{0}^{ \top}+tI)^{-1}X_{0}v\\ &=\lim_{t\to 0^{+}}\operatorname{tr}AZ_{0}(Z_{0}\Sigma Z_{0}^{\top}+tI)^{- 1}Z_{0}BZ_{0}^{\top}(Z_{0}Z_{0}^{\top}+tI)^{-1}Z_{0}\\ &\simeq\operatorname{tr}A(\Sigma+\kappa_{0}I)^{-1}B(\Sigma+ \kappa_{0}I)^{-1}+\kappa_{0}^{2}\operatorname{tr}A(\Sigma+\kappa_{0}I)^{-2} \cdot\frac{\operatorname{tr}B(\Sigma+\kappa_{0}I)^{-2}}{T-\mathrm{df}_{2}( \kappa_{0})}\\ &=u^{\top}(\Sigma+\kappa_{0}I)^{-1}\Sigma M\Sigma(\Sigma+\kappa_{0 }I)^{-1}v+\kappa_{0}^{2}u^{\top}\Sigma(\Sigma+\kappa_{0}I)^{-2}v\cdot\frac{ \operatorname{tr}M\Sigma(\Sigma+\kappa_{0})^{-2}}{T-\mathrm{df}_{2}(\kappa_{0}) }\\ &=u^{\top}\Sigma A_{0}v,\text{ for }M=\Sigma,\end{split}\]

where the 3rd line is an application of Proposition 2 of Bach [4].

The Noisy Regime for Power Law Spectra

Here we discuss the consequences of Theorem 5.1 for the noisy regime.

Now fix \(\sigma^{2}\neq 0\) and \(\phi_{0}\in(0,1)\). In this regime, Theorem 5.1 predicts that consistency (i.e. \(E_{test}(\widehat{w}_{n}^{pred})\stackrel{{ T\to\infty}}{{\longrightarrow}}0\)) is only possible if \(\ell\leq\ell_{\star}\). First consider values of \(\ell\) for which the clean variance \(\sigma^{2}T^{-(1-\ell/\beta)}\) is less than the clean bias \(T^{-2\ell}\) in (27) i.e. \(0\leq\ell\leq\ell_{crit}\). We get

\[E_{test}(\widehat{w}_{n}^{pred})\asymp T^{-2\ell_{\mathbb{T}}}+T^{-(b-a-\ell/ \beta)},\]

which is minimized by taking \(\ell=\min(\ell_{\star},\ell_{crit})\). For other values of \(\ell\), variance dominates, giving

\[E_{test}(\widehat{w}_{n}^{pred})\asymp T^{-(1-\ell/\beta)}+T^{-(b-\ell/\beta-a )}\asymp T^{-(\gamma-\ell/\beta)},\]

where \(\gamma:=\min(1,b-a)\). This is minimized by taking \(\ell=\ell_{crit}\), leading to

\[E_{test}(\widehat{w}_{n}^{pred})\asymp T^{-(\gamma-1/(2\beta_{\mathbb{T}}+1))}.\]

This tends to zero with \(T\to\infty\) only if \(b>a+1/(2\beta_{\mathbb{T}}+1)\).

## Appendix H Proof of Results for Power-Law Covariance Spectrum

### Proof of Theorem 5.1

From Theorem F.1, we need to analyze the quantity

\[\rho\simeq\frac{\mathrm{df}_{2}(\kappa(\lambda))}{T_{0}-d}+\frac{\kappa( \lambda)^{2}\operatorname{tr}\left(\Sigma+\kappa(\lambda)I_{d}\right)^{-2}}{T _{0}-d}\cdot\frac{\mathrm{df}_{2}(\kappa(\lambda))}{T-\mathrm{df}_{2}(\kappa( \lambda))}.\] (52)

Now, for small \(\lambda\), \(\kappa:=\kappa(\lambda)\) is small and one can compute

\[\mathrm{df}_{m}(\kappa)\asymp\sum_{i}\frac{\lambda_{i}^{m}}{(\lambda_{i}+ \kappa)^{m}}=\kappa^{-m}\sum_{i}\frac{\lambda_{i}^{m}}{(1+\kappa^{-1}\lambda_ {i})^{m}}\asymp\kappa^{-m}\kappa^{(m-1/\beta)}=\kappa^{-1/\beta},\]

where we have used Lemma I.1 with \(D=\kappa^{-1}\) and \(n=m\) in the last step. On the other hand, we can use some of the results of Appendix A (Section 3) of [14] to do the following. It can be shown (see aforementioned paper) that

* If \(\ell>\beta\), then \(\kappa\asymp T^{-\beta}\), and so \(\mathrm{df}_{m}(\kappa)\asymp T\) for all \(m\geq 1\).
* If \(\ell<\beta\), then \(\kappa\asymp\lambda\asymp T^{-\ell}\), and so \(\mathrm{df}_{m}(\kappa)\asymp T^{\ell/\beta}=o(T)\) for all \(m\geq 1\).

For \(\ell<\beta\), plugging this into (52) gives

\[\rho \asymp\frac{T^{\ell/\beta}}{T_{0}-d}+\frac{d}{T_{0}-d}\frac{T^{ \ell/\beta}}{T-T^{\ell/\beta}}\asymp T_{0}^{-1}T^{\ell/\beta}+\frac{\phi_{0}}{1 -\phi_{0}}T^{-(1-\ell/\beta)}\] \[\asymp\frac{1}{1-\phi_{0}}\max\left(T/T_{0},\phi_{0}\right)T^{-(1 -\ell/\beta)},\]

where \(\phi_{0}:=d/T_{0}\). Combining our Theorem F.1 with (57), we get the claimed result. 

### Representation of Clean Test Error

We make a small digression to present the following curiosity: with a slight leap of faith, the main results of [14] can be obtained in a few lines from the tools developed in [4], namely Proposition 4.2. This is significant, because the computations in [14] were done via methods of statistical physics (replica trick), while [4] is based on RMT.

Indeed, for regularization parameter \(\lambda\asymp T^{-\ell}\) given in (25), we have \(\kappa=\kappa(\lambda)\simeq\lambda\). Thus

\[\kappa\asymp T^{-\ell},\,\mathrm{df}_{2}(\kappa)\asymp\kappa^{-1/\beta}\asymp T ^{\ell/\beta}.\] (53)Now, since \(\lambda_{i}\asymp i^{-\beta}\) (capacity condition) and \((w_{0}^{\top}v_{i})^{2}=c_{i}^{2}\asymp i^{-\delta}\) (source condition), we deduce

\[\begin{split}\kappa^{2}w_{0}^{\top}\Sigma(\Sigma+\kappa I)^{-2}w_{0 }&\asymp w_{0}^{\top}\left(\sum_{i}\frac{\lambda_{i}}{(\lambda_{i }+\kappa^{-1}\lambda_{i})^{2}}v_{i}v_{i}^{\top}\right)w_{0}=\sum_{i}\frac{c_{ i}^{2}\lambda_{i}}{(\lambda_{i}+\kappa^{-1}\lambda_{i})^{2}}\\ &=\sum_{i}\frac{c_{i}^{2}\lambda_{i}}{(\lambda_{i}+\kappa^{-1} \lambda_{i})^{2}}\asymp\sum_{i}\frac{\lambda_{i}^{1+\delta/\beta}}{(\lambda_{ i}+\kappa^{-1}\lambda_{i})^{2}}\asymp\kappa^{-\gamma}\asymp T^{-\ell\gamma},\end{split}\] (54)

where \(\gamma=\min(2,1+\delta/\beta-1/\beta)=\min(2,2r)=2r\), with \(\underline{r}:=\min(r,1)\). The exponent is so because \(\delta=1+\beta(2r-1)\), and so \(\delta/\beta=1/\beta+2r-1\) by construction. The estimation of the last sum in (54) is thanks to Lemma I.1 applied with \(D=\kappa^{-1}\), \(n=1+\delta/\beta\), and \(m=2\). Therefore, invoking Proposition 4.2 gives

\[Bias \simeq\frac{\kappa^{2}w_{0}^{\top}\Sigma(\Sigma+\kappa)^{-2}w_{0 }}{1-\mathrm{df}_{2}(\kappa)/T}\asymp\frac{T^{-\ell\gamma}}{1-T^{-(1-\ell/ \beta)}}\asymp T^{-\ell\gamma}=T^{-2\ell\underline{r}}\] (55) \[Var \simeq\sigma^{2}\frac{\mathrm{df}_{2}(\kappa)}{T}\cdot\frac{1}{1 -\mathrm{df}_{2}(\kappa)/T}\asymp\sigma^{2}\frac{T^{\ell/\beta}}{T}\frac{1}{1 -o(1)}\asymp\sigma^{2}T^{-(1-\ell/\beta)}.\] (56)

We deduce the scaling law

\[E_{test}\simeq Bias+Var\asymp T^{-2\ell\underline{r}}+\sigma^{2}T^{-(1-\ell/ \beta)}\asymp\max(\sigma^{2},T^{1-2\ell\underline{r}-\ell/\beta)})T^{-(1-\ell /\beta)},\] (57)

which is precisely the main result of [14].

Low-Noise Regime.In the low noise regime where \(\sigma^{2}=O(T^{-2\beta\underline{r}})\), one may take \(\ell=\beta\); the variance is then much smaller than the bias, and one has the fast rate

\[E_{test}\asymp T^{-2\beta\underline{r}}.\] (58)

High-Noise Regime.Now, consider the case where \(\sigma^{2}=\Theta(1)\). Setting \(2\ell\underline{r}=1-\ell/\beta\) to balance out the bias and variance gives \(\ell=\ell_{crit}\), where

\[\ell_{crit}:=\frac{\beta}{2\beta\underline{r}+1}\in(0,\beta).\] (59)

With this value of the exponent \(\ell\), we get the error rate

\[E_{test}\asymp T^{-2\ell_{crit}\underline{r}}=T^{-c},\text{ with }c:=\frac{2\beta \underline{r}}{2\beta\underline{r}+1},\] (60)

which is precisely the main result of [14], known to be minimax optimal (de Vito [11], etc.)!

## Appendix I Auxiliary Lemmas

**Lemma I.1**.: _Let the sequence \((\lambda_{k})_{k\geq 1}\) of positive numbers be such that \(\lambda_{k}\asymp k^{-\beta}\) for some constant \(\beta>0\), and let \(m,n\geq 0\) with \(n\beta>1\). Then, for \(D\gg 1\), it holds that_

\[\sum_{k=1}^{\infty}\frac{\lambda_{k}^{n}}{(1+D\lambda_{k})^{m}}\asymp D^{-c} \begin{cases}\log D,&\text{if }m=n-1/\beta,\\ 1,&\text{else,}\end{cases}\] (61)

_where \(c:=\min(m,n-1/\beta)\geq 0\)._

Proof.: First observe that

\[\begin{split}\lambda_{k}^{n}/(1+D\lambda_{k})^{m}&\asymp \lambda_{k}^{n}\min(1,(D\lambda_{k})^{-m})\\ &=\begin{cases}\lambda_{k}^{n}=k^{-n\beta},&\text{if }D\lambda_{k}<1, \text{ i.e if }k>D^{1/\beta},\\ D^{-m}\lambda_{k}^{-(m-n)}=D^{-m}k^{(m-n)\beta},&\text{else.}\end{cases}\end{split}\]

We deduce that

\[\sum_{k=1}^{\infty}\frac{\lambda_{k}^{n}}{(1+D\lambda_{k})^{m}}\asymp D^{-m} \sum_{1\leq k\leq D^{1/\beta}}k^{(m-n)\beta}+\sum_{k>D^{1/\beta}}k^{-n\beta}.\] (62)By comparing with the corresponding integral, one can write the first sum in (62) as

\[D^{-m}\sum_{1\leq k\leq D^{1/\beta}}k^{(m-n)\beta} \asymp D^{-m}\int_{1}^{D^{1/\beta}}u^{(m-n)\beta}\mathrm{d}u\] \[\asymp D^{-m}\begin{cases}(D^{1/\beta})^{1+(m-n)\beta}=D^{-(n-1/ \beta)},&\text{if }n-1/\beta<m,\\ \log D,&\text{if }m=n-1/\beta,\\ 1,&\text{else}.\end{cases}\] \[=\begin{cases}D^{-(n-1/\beta)},&\text{if }n-1/\beta<m,\\ D^{-m}\log D,&\text{if }m=n-1/\beta,\\ D^{-m},&\text{else}.\end{cases}\] \[=D^{-c}\begin{cases}\log D,&\text{if }m=n-1/\beta,\\ 1,&\text{else},\end{cases}\]

where \(c\geq 0\) is as given in the lemma.

Analogously, one can write the second sum in (62) as

\[\sum_{k>D^{1/\beta}}k^{-n\beta}\asymp\int_{D^{1/\beta}}^{\infty}u^{-n\beta} \mathrm{d}u\asymp(D^{1/\beta})^{1-n\beta}=D^{-(n-1/\beta)},\]

and the result follows upon putting things together. 

**Lemma I.2**.: _For \(\kappa=\kappa(\lambda,T)\) defined as in (9), it holds that_

\[\frac{\partial\kappa}{\partial\lambda}=\frac{1}{1-\mathrm{d}I_{2}(\kappa)/T} \geq 1.\] (63)

The formula given in the above lemma is useful because it can be combined with the identities

\[Bias =w_{0}^{\top}\Sigma(\Sigma+\kappa I)^{-2}w_{0}\frac{\partial \kappa}{\partial\lambda},\] (64) \[Var =\sigma^{2}\frac{\mathrm{d}I_{2}(\kappa)}{T}\frac{\partial\kappa }{\partial\lambda}.\] (65)

The RHS of (64) is usually referred to as the omniscient risk Hastie et al. [22], Cui et al. [13], Wei et al. [48].

Proof of Lemma I.2.: By definition of \(\kappa\), we know that

\[\kappa-\lambda=\kappa\mathrm{d}I_{1}(\kappa)/T=\kappa\operatorname{tr}\Sigma( \Sigma+\kappa I)^{-1}/T.\]

Let \(\kappa:=\frac{\partial\kappa}{\partial\lambda}\). Differentiating the above identity w.r.t. \(\lambda\) gives

\[\kappa^{\prime}-1=\kappa^{\prime}(\operatorname{tr}\Sigma(\Sigma+\kappa I)^{ -1}-\kappa\operatorname{tr}\Sigma(\Sigma+\kappa)^{-2})/T=\kappa^{\prime} \operatorname{tr}\Sigma^{2}(\Sigma+\kappa I)^{-2}/T=\kappa^{\prime} \mathrm{d}I_{2}(\kappa)/T,\]

and the result follows upon rearranging. Note that we have used the identity

\[I-\kappa(\Sigma+\kappa I)^{-1}=\Sigma(\Sigma+\kappa I)^{-1},\]

to rewrite \(\Sigma(\Sigma+\kappa I)^{-1}-\kappa\Sigma(\Sigma+\kappa I)^{-2}=\Sigma^{2}( \Sigma+\kappa I)^{-2}\). 

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We explicitly describe the setting, as well as the conclusions of our theory in various regimes. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clearly state that our theory applies to the case of kernel ridge regression. All conclusions to different models are by analogy only, as is always done when this type of model is studied. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All assumptions are clearly stated in the main text, and all proofs can be found in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We clearly give all experimental details, including parameters. These are sufficient to reproduce all our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We only use one publicly available dataset, MNIST, and no idiosyncratic model. Thus, we provide neither dataset nor code, as the dataset is publicly available, and the experiments are easy to reproduce from their description. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We precisely describe all our (simple) linear or kernel methods in sufficient detail. They can easily be reproduced from these. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are given and we describe the number of trials over which they are obtained. We believe these are chosen suitably. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compared to most contributions, our experiments, which accompany our theory, do not require any significant compute whatsoever. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: our research does not involve human participants, and never uses private data or models. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our paper provides toy theory that underlines the point that retraining on data generated from currently available models might lead to a vicious loop of model collapse, which might have societal impact. We call for attention to this matter.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release any models or new data. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The only public dataset we use is MNIST [16], which we properly credit. The MNIST dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release any new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not involve crowdsourcing nor research on human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not involve crowdsourcing nor research on human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.