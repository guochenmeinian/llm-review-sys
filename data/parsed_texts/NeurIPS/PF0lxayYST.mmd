# On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets

Jiashuo Liu\({}^{1,}\)1, Tianyu Wang\({}^{2,}\)2, Peng Cui\({}^{1}\), Hongseok Namkoong\({}^{3}\)

\({}^{1}\)Department of Computer Science and Technology, Tsinghua University

\({}^{2}\)Department of Industrial Engineering and Operations Research, Columbia University

\({}^{3}\)Decision, Risk, and Operations Division, Columbia Business School

liujiashuo77@gmail.com, tw2837@columbia.edu

cuip@tsinghua.edu.cn, namkoong@gsb.columbia.edu

Equal contribution

Footnote 1: footnotemark:

###### Abstract

Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they _implicitly_ focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the \(Y|X\) distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that \(Y|X\)-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since \(Y|X\)-shifts are prevalent in tabular settings, we _identify covariate regions_ that suffer the biggest \(Y|X\)-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of why distributions differ.2

Footnote 2: footnotemark:

## 1 Introduction

The performance of predictive models has been observed to degrade under distribution shifts in a wide range of applications, such as healthcare [9; 95; 76; 93], economics [36; 25], education [6], vision [74; 63; 86; 98], and language [62; 7]. Distribution shifts vary in type, typically defined as either a change in the marginal distribution of the covariates (\(X\)-shifts) or the conditional relationship between the outcome and covariate (\(Y|X\)-shifts). Real-world scenarios comprise of both types of shifts. In computer vision [62; 47; 81; 38; 101], \(Y|X\)-shifts are less likely to occur as \(Y\) is constructed from human knowledge given an input \(X\), unless the labeling noise is severe. For tabular datasets, \(Y|X\)-shifts can be more common because of missing variables and hidden confounders. For example, the prevalence of a disease may be affected by unrecorded covariates whose distribution changes across domains, such as lifestyle factors and socioeconomic status [39; 103; 93].

Different types of distribution shifts require different solutions. When facing \(X\)-shifts, the implicit goal of many researchers is to develop a single robust model that can be generalized effectively across multiple domains. Various algorithms have been developed to align the marginal distributions (\(P_{X}\)), including domain adaptation and importance sampling methods. However, under \(Y|X\)-shifts, there may be a fundamental trade-off between learning algorithms: to perform well on a target distribution, a model may have to _necessarily_ perform worse on others. Algorithmically, typical methods foraddressing \(Y|X\)-shifts include distributionally robust optimization (DRO) [13, 85, 28, 80, 27] and causal learning methods [72, 8, 83, 46]. Operationally, the modeler can identify and collect an unobserved confounder \(C\) such that \(Y|X,C\) remains invariant across domains, or resort to overhauling the entire model development pipeline to collect more samples from the target.

However, existing distribution shift benchmarks only focus on \(X\)-shifts [74, 47, 107, 81, 101]. To illustrate this concretely, consider popular tabular datasets used to benchmark model performance over demographic subgroups: Adult, BRFSS, COMPAS, ACS Public Coverage, and ACS Income[3, 102, 31]. We take the largest demographic group as training \(P\) and the smallest as target \(Q\) to simulate subgroup shifts, e.g., in Adult, \(P=\)white men and \(Q=\)non-white women. We measure the optimality gap of the model \(f_{P}\) trained on \(P\) as measured on the target \(Q\) using the _relative regret_

\[\frac{\mathbb{E}_{Q}[\ell(Y,f_{P}(X))]}{\min_{f\in F}\mathbb{E}_{Q}[\ell(Y,f( X))]}-1,\ \ \ \text{where}\ \ \ f_{P}\in\operatorname*{argmin}_{f\in\mathcal{F}}\mathbb{E}_{P}[\ell(Y,f(X))]\] (1.1)

and \(\ell(\cdot,\cdot)\) is the 0-1 loss. For these widely-used benchmarks, the relative regret is small (left 5 bars in Figure 1), suggesting the \(Y|X\) distribution is _largely transferable_ across those demographic groups.

To study diverse distribution shift patterns, we consider 5 real-world tabular datasets constructed from the US Census (as proposed by Ding _et al._[25]) and traffic measurements [64, 65, 2, 1]. We focus on spatiotemporal shifts to model most common natural shifts. Our full benchmark covers 22 settings (see Table 3 in Appendix D), where each setting includes one source (e.g., California) and a number of possible targets (e.g., other states). For illustration purposes, we focus on 7 settings covering 169 possible source-target pairs (see Table 2) and carefully select one target per setting to represent a wide range of \(Y|X\)-shifts (right bars in Figure 1).

We find \(Y|X\)-shifts constitute a substantial proportion of real-world distribution shifts, yet previous (unqualified) empirical findings in the literature only hold over mild \(X\)-shifts and fail to hold over \(Y|X\)-shifts (Section 2). Out of 169 source-target pairs with significant performance degradation (\(>8\) percentage points of accuracy drop), 80% of them are primarily attributed to \(Y|X\)-shifts. \(Y|X\)-shifts introduce considerable performance variations on the target distribution, leading to different relationships between in- and out-of-distribution performances across settings and datasets. This is in stark contrast to the recently observed accuracy-on-the-line phenomenon [63], where the in- and out-of-distribution performance have been posited to exhibit a strong linear relationship. In Figure 2, we showcase how the accuracy-on-the-line trend fails to hold when \(Y|X\)-shifts are strong. Our results imply that the standard practice of blindly evaluating performance over various shifts is only justified over \(X\)-shifts, where we expect there to be a single model that is robust across domains.

For severe \(Y|X\)-shifts, the training data may not even be informative for modeling the \(Y|X\) relationship in the target. To inform algorithmic and data-based interventions, we must understand _why_ the distribution changed. In Section 3, we illustrate the need for more methodological research that builds a deep understanding of distributional differences. As a concrete example, we show that a simple approach for identifying covariate regions with strong \(Y|X\)-shifts can suggest data-based interventions. Our case study shows it can be useful to collect target data over a particular covariate region, or features \(C\) such that the \(Y|X,C\) distribution is more stable across source and target.

In Section 4, we provide the details of our benchmark, WhyShift. We use five datasets to construct 7 spatiotemporal distribution shifts, and evaluate 22 methods over 86,000+ model configurations. We compare a broad range of algorithms including tree ensembles, DRO, imbalance, and fairness methods and summarize our key findings below.

* Rankings of model performance change over different shift patterns. As the validity of empirical findings implicitly depends on the type of shift, any methodological development must be grounded by the specific shifts it addresses.
* Tree ensemble methods are competitive, but still suffer from significant performance degradation.

Figure 1: _Relative regret (1.1) in typical benchmarks [26, 31] (left 5 bars) and seven settings designed in our benchmark (right 7 bars)._ We use XGBoost \(\mathcal{F}\) here for illustration.

* DRO methods are sensitive to configurations and exhibit significant performance variations.
* Imbalance and fairness methods show similar performance with the base learner (XGBoost).
* A small validation data from the target distribution goes a long way, and more generally, non-algorithmic interventions warrant greater consideration.

## 2 Distribution Shifts in Tabular Settings

To illustrate how complex distribution shift patterns arise in tabular data. we compare 22 algorithms including tree ensemble methods, robust learning, imbalance, and fairness methods. On 5 real-world tabular datasets (ACS Income, ACS Public Coverage (ACS Pub.Cov), ACS Mobility[25], US Accident[64, 65] and Taxi[2, 1]), we consider the natural spatial shifts between states/cities, e.g., California to Puerto Rico. For the ACS Pub.Cov dataset, we also consider temporal shifts, e.g., from 2010 to 2017. Since all natural distribution shifts we consider are largely induced by \(Y|X\)-shifts, we construct a synthetic subgroup shift from younger people to older people in order to simulate \(X\)-shifts. Deferring a detailed summary to Section 4.1, we focus on introducing representative phenomena in this section.

In Figure 2, we present the source (in-distribution) and target (out-of-distribution) performances of 22 algorithms, each with 200 hyperparameter configurations. To understand shift patterns, we utilize the recently proposed DIstribution Shift DEcomposition (DISDE) framework [16] which decomposes the performance degradation into components attributed to \(Y|X\)- and \(X\)-shifts. Using the best XGBoost configuration as the baseline model for each source-target pair, we present the total performance degradation and the proportion attributed to \(Y|X\)-shifts.

Distribution shifts are predominantly \(Y|X\)-shifts in our empirical studyWe find performance degradation under natural shifts is overwhelmingly attributed to \(Y|X\)-shifts, as illustrated in the curated list in Figure 2. More generally, out of the 169 source-target pairs whose performance degradation is larger than \(8\) percentage points, 87.2% of them have over 50% of the performance degradation attributed to \(Y|X\)-shifts (70.2% of them have over 60% of the gap attributed to \(Y|X\)-shifts). We conjecture that \(Y|X\)-shifts are prevalent in tabular data due to missing features. For example, in the context of income prediction, individual outcomes may change due to unobserved economic and political factors whose distribution changes over geographical locations [25]. In contrast, in vision and language tasks, the input (e.g., pixels and words) often encapsulates most of the necessary information for predicting the outcome, making strong \(Y|X\)-shifts less likely unless the labeling noise is severe. Consequently, compared to vision and language data in domain generalization

Figure 2: Target vs. source accuracies for 22 algorithms and datasets in our benchmark. A linear fit (green line) and its corresponding \(R^{2}\) value is reported on the top left of each figure. Each blue point represents one hyperparameter configuration. **(a)-(f)**: six examples of ACS Income, ACS Mobility, Taxi, ACS Pub.Cov, US Accident datasets. **(g)**: simulated covariate shifts on on sub-sampled ACS Income dataset.

tasks, tabular data exhibits more pronounced real \(Y|X\)-shifts. Our findings highlight the importance of understanding the _cause_ of the distribution shift.

Accuracy-on-the-line fails to hold over \(Y|X\)-shiftsWe find significant variation in the relationship between source and target performance throughout all natural distribution shifts presented in Figure 2 (a)-(g). The correlation between source and target performance is relatively weak, and we tend to see poor linear fits (low \(R^{2}\)) when the bulk of the performance degradation is attributed to \(Y|X\)-shifts. That is, we see in Figures 2 (a)-(f) that the relationship between the two performances exhibits significant fluctuations across different source-target pairs. In Figure 3, we observe that performance rankings of algorithms substantially vary across different \(Y|X\)-shifts. Our finding highlights the inherent complexity associated with real distribution shifts in tabular datasets, which stands in sharp contrast to the "accuracy-on-the-line" phenomena [63]. The varied shift patterns in tabular data highlight how empirical observations must be qualified over the range of shifts they remain valid over. This is particularly important for \(Y|X\)-shifts which introduce larger variations in the relationship between source and target performance.

Source and target performances are correlated when \(X\)-shifts dominateAcross all natural shifts we study, we find \(X\)-shifts are only prominent in temporal shifts (ACS Time dataset; Figure 2f) To better investigate the role of \(X\)-shifts, we subsample the data to artificially induce strong covariate shifts over an individual's age. Specifically, we focus on individuals from California and form two groups according to whether their age is \(\geq 25\). The source data oversamples low age groups where 80% is drawn from the age \(\leq 25\) group; proportions are reversed in the target data.

On this synthetic shift we construct, the DISDE [16] method attributes the bulk of the performance degradation to \(X\)-shifts in Figure 2g. Our finding confirms the intuition that unobserved economic factors remain relatively consistent for individuals from the same state (CA). In this synthetic example with \(X\)-shifts, we observe a relatively strong correlation between source and target performance. Moreover, the large performance degradation on these datasets suggests that existing robust learning methods are still severely affected by covariate shifts, indicating the need for future research that addresses covariate shifts in tabular data.

## 3 Case Study: Understanding Distribution Shifts Facilitates Interventions

Typical algorithmic approaches to handling practical distribution shifts aim to optimize performance over a postulated set of distribution shifts. Causal learning assumes the underlying causal structure can be learned to withstand distribution shifts [72, 83, 82, 77], while DRO methods explicitly optimize worst-case performance over a set of distributions [13, 53, 28, 27]. Despite progress in algorithm design, there are few efforts that examine the patterns of real-world distribution shifts. It remains unclear whether the data assumptions made by algorithms hold in practice, and this _mismatch_ often leads to poor empirical performance [40, 95, 76, 19, 46].

Complementing the active literature on algorithmic development, we present an empirical study that underscores the practical significance of tools that provide a qualitative understanding of the shift at hand. In light of the prevalence of \(Y|X\)-shifts in tabular data, we introduce a simple yet effective approach for identifying covariate regions that suffer strong \(Y|X\)-shifts. We demonstrate our approach on the income prediction task (ACS Income), and show that it can guide operational interventions for addressing distribution shifts. Our case study is not meant to be a rigorous scientific analysis, but rather a (heuristic) vignette illustrating the need for future research on methodologies that can generate qualitative insights on distributional differences.

Figure 3: Performances of typical algorithms of 7 settings in our benchmark.

### Identifying Regions with Strong \(Y|x\)-shifts

Here we propose a simple yet effective method for identifying covariate regions with strong \(Y|X\)-shifts. Despite its simplicity, we demonstrate in the following subsections that our method can inspire operational and modeling interventions. Letting \((X,Y)\) be random variable supported on the space \(\mathcal{X}\times\mathcal{Y}\), consider a model \(f:\mathcal{X}\rightarrow\mathcal{Y}\) that predicts outcome \(Y\in\mathcal{Y}\) from covariates \(X\in\mathcal{X}\) with the associated loss function \(\ell(f(X),Y)\). Given samples \((X,Y)\) drawn from the source and target distributions \(P\) and \(Q\), our goal is to _identify a region \(\mathcal{R}\subseteq\mathcal{X}\) where \(P_{Y|X}\) differs a lot from \(Q_{Y|X}\)_.

Since \(P_{Y|X}\) and \(Q_{Y|X}\) are undefined outside of the support of \(P_{X}\) and \(Q_{X}\) respectively, the comparison can only be made on a subset of the _common support_. To aid comparisons on the common support, Cai _et al._[16] introduced a shared distribution approach. The shared distribution has high density when both \(p_{X}\) and \(q_{X}\) are high, and low density whenever either is small. Following Cai _et al._[16], we choose a specific _shared distribution_\(S_{X}\) over \(X\) from the likelihood ratio:

\[s_{X}(x)\propto p_{X}(x)q_{X}(x)/(p_{X}(x)+q_{X}(x)).\] (3.1)

We provide more discussion on the choices of \(S_{X}\) and other technical details justifying the correctness of \(s_{X}\) in Appendix C.3. Since we do not have access to samples from the shared distribution \(S_{X}\), we reweight samples from \(P_{X}\) and \(Q_{X}\) using the likelihood ratios \(s_{X}(x)/p_{X}(x)\propto q_{X}(x)/(p_{X}(x)+q_{X}(x))\) and \(s_{X}(x)/q_{X}(x)\propto p_{X}(x)/(p_{X}(x)+q_{X}(x))\). The ratio can be modeled as the probability that an input \(x\) is from \(P_{X}\) vs \(Q_{X}\). Denote \(\alpha^{*}\) as the proportion of the pooled data that comes from \(Q_{X}\) and \(\pi^{*}(x):=\mathbb{P}(\tilde{X}\) from \(Q_{X}|\tilde{X}=x)\), we can express the likelihood ratios as:

\[\frac{s_{X}}{p_{X}}(x) \propto\frac{\pi^{*}(x)}{(1-\alpha^{*})\pi^{*}(x)+\alpha^{*}(1- \pi^{*}(x))}=:w_{P}(\pi^{*}(x),\alpha^{*}),\] (3.2) \[\frac{s_{X}}{q_{X}}(x) \propto\frac{1-\pi^{*}(x)}{(1-\alpha^{*})\pi^{*}(x)+\alpha^{*}(1- \pi^{*}(x))}=:w_{Q}(\pi^{*}(x),\alpha^{*}).\] (3.3)

With the likelihood ratios, we estimate the best prediction model under \(P\) and \(Q\) over the shared distribution \(S_{X}\) (using XGBoost as the model class \(\mathcal{F}\)):

\[f_{\mu}:= \arg\min_{f\in\mathcal{F}}\left\{\mathbb{E}_{S_{X}}\left[\mathbb{E }_{\mu}[\ell(f(X),Y)|X]\right]\left(=\mathbb{E}_{\mu}\left[\ell(f(X),Y)w_{\mu }(\pi^{*}(x),\alpha^{*})\right]\right)\right\},\text{ for }\mu=P,Q.\] (3.4)

Then, for any threshold \(b\in[0,1]\), \(\{x\in\mathcal{X}:|f_{P}(x)-f_{Q}(x)|\geq b\}\) suggests a region that may suffer model performance degradation with at least \(b\) due to \(Y|X\)-shifts. Without evaluating the performance on the shared distribution \(S_{X}\), it is hard to distinguish the source of the model performance degradation, i.e. from \(X\)-shifts or \(Y|X\)-shifts.

Empirically, given samples \(\{(x_{i}^{P},y_{i}^{P})\}_{i\in[n_{P}]}\) from \(P\) and \(\{(x_{j}^{Q},y_{j}^{Q})\}_{j\in[n_{Q}]}\) from \(Q\), we estimate \(\hat{\alpha}=\frac{n_{Q}}{n_{P}+n_{Q}}\) and then train a binary "domain" classifier \(\hat{\pi}(x)\) to approximate the ratio \(\pi^{*}(x)\). Note that the "domain" classifier can be any black-box method, and we use XGBoost throughout. Then we plug these empirical estimands in to obtain the estimated likelihood ratios \(w_{\mu}(\hat{\pi}(x),\hat{\alpha})\) and learn prediction models \(f_{P}\) and \(f_{Q}\) in Equation (3.4). To investigate the model difference under \(S_{X}\), we pool samples from \(P\) and \(Q\) together and set sample weights as:

\[\lambda_{i}^{P}=\frac{w_{P}(\hat{\pi}(x_{i}^{P}),\hat{\alpha})}{\sum_{k\in[n_{P }]}w_{P}(\hat{\pi}(x_{k}^{P}),\hat{\alpha})}\quad\forall i\in[n_{P}],\quad \lambda_{j}^{Q}=\frac{w_{Q}(\hat{\pi}(x_{j}^{Q}),\hat{\alpha})}{\sum_{k\in[n_{ Q}]}w_{Q}(\hat{\pi}(x_{k}^{Q}),\hat{\alpha})}\quad\forall j\in[n_{Q}],\] (3.5)

which are used to learn a prediction model \(h(x)\) to approximate \(|f_{P}(x)-f_{Q}(x)|\) on the _shared distribution_\(S_{X}\). The pseudo-code is summarized in the Algorithm 1; To allow simple interpretation and efficient region identification, we use a shallow _decision tree_\(h(x)\) and consider the region \(\mathcal{R}\) corresponding to the feature range of a leaf node within the tree. More details could be found in Appendix C.5 and Appendix C.6. We show that the node splitting criterion in a standard decision trees training procedure is equivalent with our goal of finding regions with the largest discrepancy in Appendix C.4.

### Data-based Interventions

Using Algorithm 1, we now demonstrate how a better understanding of distribution shifts can facilitate the design of interventions. We focus on the ACS Income dataset where the goal is to predict whetheran individual's income exceeds 50k (\(Y\)) based on their tabular census data (\(X\)). We train an income classifier on 20,000 samples from California (CA, source), and deploy the classifier in Puerto Rico and South Dakota (PR & SD, target), where we get 4,000 samples from PR and SD after deployment. Given the considerable disparities in the economy, job markets, and cost of living between CA and PR/SD, we observe substantial performance degradation due to distribution shifts.

In Figure 3(a), we first decompose the performance degradation from CA to PR to understand the shift and find \(Y|X\)-shifts are the predominant factor. The calculation of \(X\)-shifts and \(Y|X\)-shifts is deferred to Appendix C.1. We dive deeper into the significant \(Y|X\)-shifts and identify from CA to PR for the XGBoost and MLP classifier. From the region shown in Figure 3(c) and Figure 3(d), we find college-educated individuals in business and educational roles (such as management, business, and educational work) exhibit large \(Y|X\) differences.

To illustrate how our analysis can inspire subsequent operational interventions to enhance performance on the target distribution, we study two operational interventions.

**Collect specific data from the target**  To improve target performance, the most natural operational intervention is to collect additional data from the target distribution. While a rich body of work on domain adaptation [69, 23, 30, 90, 89] study how to effectively utilize data from the target distribution to improve performance, there is little work that discusses how to efficiently collect supervised data from the target distribution to maximize out-of-distribution generalization. To highlight the need for future research in this space, we use the interpretable region identified by Algorithm 1 as shown in Figure 3(c) to simulate a concerted data collection effort.

Since indiscriminately collecting data from the target distribution can be resource-intensive, we concentrate sampling efforts on the subpopulation that may suffer from \(Y|X\)-shifts and selectively gather data on them. For five base methods (logistic regression, MLP, random forest, lightGBM, and XGBoost), we randomly sample 250 points from the whole target distribution and the identified region suffering prominent \(Y|X\)-shifts, respectively. We report the test accuracies in Figure 3(e), and observe that incorporating data from this region is more effective in enhancing OOD generalization. While preliminary, our results demonstrate the potential robustness benefits of efficiently allocating resources toward concerted data collection. Future methodological research in this direction may be fruitful; potential connections may exist with active learning algorithms [84, 96, 59].

**Add more relevant features** We now illustrate the potential benefits of generating qualitative insights on the distribution shift at hand. Our analysis in Figure 3(c) suggests educated individuals in financial, educational, and legal professions tend to experience large \(Y|X\)-shifts from CA to PR. These roles typically need communication skills, and language barriers could potentially affect their incomes. In California (CA), English is the primary language, while in Puerto Rico (PR), despite both English and Spanish being recognized as official languages, Spanish is predominantly spoken. Consequently, for a model trained on CA data and tested on PR data, incorporating a new feature that denotes English language proficiency (hereafter denoted "ENG") might prove beneficial in improving generalization performances. However, this feature is not included in the ACS Income dataset.

To address this, we went back to the Census Bureau's American Community Survey database to include the ENG feature in the set of covariates. In Figure 3(b), we observe that the inclusion of this feature substantially reduces the degradation due to \(Y|X\)-shifts, verifying that the originally missing ENG feature may be one cause of \(Y|X\)-shifts. Figure 4f contrasts the performances of 22 algorithms (each with 200 hyperparameter configurations) with original features with those that additionally use the ENG feature. The new feature significantly improves target performances across all algorithms; roughly speaking, we posit that we have identified a variable \(C\) such that \(Y|X,C\) remains similar across CA and PR. However, when we extend this comparison to the source-target pair (CA \(\rightarrow\) SD), we observe no significant improvement (Figure 4g). This highlights that the selection of new features should be undertaken judiciously depending on the target distributions of interest. A feature that proves effective in one target distribution might not yield similar results in another.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \#ID & Dataset & Type & \#Samples & \#Features & Outcome & \#Domains & Selected Settings & Shift Patterns \\ \hline
1 & _kCS Income_ & Natural & 1,599,229 & 9 & Income\(>\)50k & 51 & California \(\rightarrow\) Puerto Rico & \(Y|X>X\) \\
2 & _kCS Mobility_ & Natural & 620,937 & 21 & Residential Address & 51 & Mississippi \(\rightarrow\) Hawaii & \(Y|X>X\) \\
3 & Taxi & Natural & 1,567,769 & 7 & Duration time\(>\) 30 min & 4 & New York City\(\rightarrow\) Boston & \(Y|X>X\) \\
4 & _kCS Pub.Cov_ & Natural & 1,127,446 & 18 & Public Ins. Coverage & 51 & Nebraska \(\rightarrow\) Louisiana & \(Y|X>X\) \\
5 & US Accident & Natural & 297,132 & 47 & Severity of Accident & 14 & California\(\rightarrow\) Oregon & \(Y|X>X\) \\
6 & _kCS Pub.Cov_ & Natural & 895,632 & 18 & Public Ins. Coverage & 4 & 2010 (NY)\(\rightarrow\) 2017 (NY) & \(Y|X<X\) \\
7 & _kCS Income_ & Synthetic & 195,665 & 9 & Income\(>\)50k & 2 & Younger\(\rightarrow\) Older & \(Y|X<X\) \\ \hline \end{tabular}
\end{table}
Table 1: Overview of datasets and 7 selected settings.

Figure 4: Case study illustrations. **(a)-(b)** Decomposition of performance degradation for the XGBoost classifier from CA to PR. Figure (a) is for the original setting and (b) corresponds to the results post-integration of the “ENG” feature. **(c)-(d)** Demonstration of Algorithm 1: an interpretable version of the region with strong \(Y|X\)-shifts for the XGBoost and MLP models, respectively. **(e)** Test accuracies of five typical base methods trained on the source, post addition of 250 randomly selected _target observations_, and 250 observations from the identified _risk region_. **(f)-(g)** Performances of all algorithms prior to and following the addition of the “ENG” feature. Figure (f) corresponds to the CA to PR, and Figure (g) is CA to SD.

## 4 WhyShift: Benchmarking Distribution Shifts on Tabular Data

In this section, we detail our benchmark and summarize the main observations. Our finding highlights the importance of future research that builds an understanding of _why_ the distribution has shifted.

### Setup

DatasetsWe explore distribution shifts on 5 real-world tabular datasets from the economic and traffic sectors with _natural_ spatiotemporal distribution shifts. For economic data, we use ACS Income, ACS Mobility, and ACS Public Coverage datasets from the US-wide ACS PUMS data [25], where the outcome is whether an individual's income exceeds 50k, whether an individual changed the residential address one year ago, and whether an individual is covered by public health insurance, respectively. We primarily focus on spatial shifts across different states in the US. To complement spatial shifts, we derive an ACS Time task based on the ACS Public Coverage dataset, where there are temporal shifts between different years (2010 to 2021). For traffic data, we use US Accident[64, 65] and Taxi[2, 1], where the outcome is whether an accident is severe and whether the total ride duration time exceeds 30 minutes, respectively. We focus on spatial shifts between different states/cities. We summarize the datasets in Table 1 and defer a full description to the Appendix D.1.

AlgorithmsWe evaluate **22** algorithms that span a wide range of learning strategies on tabular data, and compare their performances under different patterns of distribution shifts we construct. Concretely, these algorithms include: (1) _base learners_: Logistic Regression, SVM, fully-connected neural networks (MLP) with standard ERM optimization; (2) _tree ensemble models_: Random Forest, XGBoost, LightGBM; (3) _robust learning_: CVaR-DRO and \(\chi^{2}\)-DRO with fast implementation [53], CVaR-DRO and \(\chi^{2}\)-DRO of outlier-robust enhancement [105], Group DRO [79]; (4) _imbalanced learning_: JTT [57], SUBY, RWY, SUBG, RWG [42], DWR [51] and (5) _fairness-enhancing methods_: inprocessing method [4] with demographic parity, equal opportunity, error parity as constraints, postprocessing method [37] with exponential and threshold controls. For DRO methods (i.e. (3)), we use MLP as the backbone model. For other algorithms compatible with tree ensemble models (i.e. (4-5)), we use the XGBoost model due to its superior performance on tabular data [34]. For algorithms requiring group labels, we use 'hour' for US Accident and Taxi, and'sex' for the others. Detailed descriptions for each algorithm can be found in Appendix D.5.

BenchmarksWe conduct experiments with more than 86,000 model configurations on various source-target distribution shift pairs, and carefully select _7 selected pairs with different distribution shift patterns_. In Table 1, we characterize the shift patterns of these 7 source-target pairs, which contain different proportions of \(Y|X\)-shifts and \(X\)-shifts corresponding with plots in Figure 2. The first six settings are natural shifts. In the last setting, we sub-sample the dataset according to age to introduce covariate shift, where we focus on individuals from CA and form two groups according to whether their age is \(\geq 25\). The source data over-samples the low age group where 80% is drawn from the group where the individual's age \(\geq 25\), and the proportions are reversed in the target data.

In Figure 5 and Figure 6, we plot the performance of algorithms using their best hyperparameter configuration on the validation dataset (\(i.i.d.\) with the source distribution). Additional results with various source distributions are in the Appendix. Our benchmark is designed to support empirical research, including new learning algorithms and diagnostics that provide qualitative insights on distribution shifts.

Figure 5: Overall performances of all algorithms on the target data in our selected 7 settings.

**Hyper-parameter Tuning** For each model, we conduct a grid search over a large set of hyper-parameters. See Appendix D.3 for the complete search space for each method. When one method includes another as a "base" learner (e.g., DRO with MLP, RWY with XGBoost), we explore the full tuning space for the base model (e.g., the cross-product of all MLP hyper-parameters with all DRO hyper-parameters). To control for computational effort, each method is run with 200 configurations for each source-target pair and we select the best configuration according to the \(i.i.d.\) validation performance. In Figure 6(b), we further compare different choices of validation protocols.

**Evaluation Metrics** In our benchmark, we include different metrics for a thorough evaluation. Specifically, we use Average Accuracy (micro-average), Worst-group Accuracy, and Macro-F1 score in our main results where we only have one target distribution. For the results with multiple target distributions (i.e. 3 in Taxi, 13 in US Accident and 50 in the others), we present _all_ target accuracies and Macro-F1 scores, as well as the worst-distribution accuracy and Macro-F1 score among all target distributions in Appendix D.6, D.7, D.8, D.9.

### Analysis

**Different algorithms do not exhibit consistent rankings over different shift patterns.** In Figure 5, we observe the rankings across different shifts are quite different, especially for ACS Income (CA\(\rightarrow\)PR) and ACS Mobility (MS\(\rightarrow\)HI) where \(Y\)\(\mid\)\(X\)-shifts dominate. This observation reaffirms the phenomena in Figure 2 that as \(Y\)\(\mid\)\(X\)-shifts become stronger, the relationship between source vs target performances becomes less consistent. In Appendix D.3, we also show that even for a fixed source distribution in one fixed prediction task, algorithmic rankings of performances on different target distributions vary a lot.

**Tree ensemble methods show competitive performance, but do not significantly improve the generalization drop between source and target data.** From Figure 5, tree-based ensembles (yellow bars) show robust and competitive performance on the target distribution in 6 out of 7 settings. However, in Figure 6 which plots the performance degradation between source and target, tree ensembles do not show improved robustness. This suggests that they do _not_ actually achieve better robustness against real-world distribution shifts, and their better performances on target data may simply be due to better fitting the source distribution.

**DRO methods are sensitive to configurations, with rankings varying significantly across 7 different settings.** From Figure 5, DRO methods exhibit competitive performances on ACS Mobility (MS\(\rightarrow\)HI), Taxi (NYC\(\rightarrow\)BOG), and ACS Income (Young\(\rightarrow\)Old), yet underperform in others. This sensitivity to configurations, as shown in Figure 6(a) (red points), could be attributed to

Figure 6: Performance drop between source and target data of all algorithms in our selected 7 settings.

Figure 7: **(a)**: Sensitivity of DRO methods and Imbalance Methods w.r.t. configurations. **(b)** Target performances of 22 algorithms under different validation protocols on ACS Income (CA\(\rightarrow\)PR) setting.

the worst-case optimization that perturbs the training distribution within a pre-defined uncertainty set, without any information regarding the target distribution. However, when target information is incorporated for hyper-parameter tuning, as shown in Figure 6(b), there is a notable improvement in the performance of DRO methods. Our observations suggest potential avenues for building more refined uncertainty sets in DRO methods.

**Imbalance methods and fairness methods show similar performance with the base learner (XGBoost).** In our experiments, we choose the XGBoost model as the base learner for imbalance and fairness methods due to its superior performance on tabular data [34]. However, from Figure 5 and Figure 6, imbalance methods and fairness methods do not show a clear improvement upon their base learner (XGBoost, last yellow bar). Further, as shown in Figure 6(a), imbalance methods (green) are also quite sensitive to configurations, and their performances do not improve much when their hyperparameters are tuned over the target data (Oracle).

**Target information matters in validation.** Based on the ACS Income (CA\(\rightarrow\)PR) dataset, we compare different validation protocols, including the best average accuracy, minimum subgroup discrepancy, and best worst-subgroup accuracy on validation data generated from the _source_ distribution. We also use the Oracle validation that chooses the configuration with the best average accuracy on validation data generated from the _target_ distribution. In Figure 6(b), we find the first three protocols do not show a significant difference. However, oracle validation with target information substantially improves the effectiveness of both DRO and tree ensemble methods. We conclude using target information for model selection can provide robustness gains even with a small target dataset.

**Non-algorithmic interventions warrant greater consideration.** Reflecting on Section 3, it is clear that operational interventions yield significant enhancements for various methods, as demonstrated in Figure 3(e) and Figure 3(f). In comparison to algorithmic interventions, such as designing different algorithms (e.g., DRO, Imbalance methods), a data-centric approach can be more effective in addressing distribution shifts. For instance, research on feature collection and feature engineering methods may prove impactful. Another avenue for future work is developing methods that can optimally incorporate expensive samples from the target distribution.

## 5 Discussion

We explore the complexity of distribution shifts in real-world tabular datasets in depth. Using natural shifts from 5 real-world tabular datasets across different domains, we specify each shift pattern and evaluate 22 methods via experiments with over 86k trained models. Our benchmark WhyShift encompasses various distribution shift patterns to evaluate the robustness of the methods. We propose a simple but effective algorithm to identify regions with large \(Y|X\)-shifts, and through a comprehensive case study, we demonstrate how a better understanding of distribution shifts facilitates algorithmic and data-based interventions. Our findings highlight the importance of future research to understand how and why distributions differ in real-world applications.

Our study leaves many open directions for improvements in future work. Our benchmark only includes tabular datasets from the economic and transportation domain. Considering datasets from other domains such as the medicine or those involving feature embeddings may highlight different types of distribution shift. On the algorithmic side, our region-identification algorithm requires some target data to identify risky regions and cannot be used in cases where the target distribution is completely unknown. Furthermore, targeted data collection on regions of \(Y|X\)-shifts may be pose ethical and privacy concerns for marginalized groups. We provide more discussion in Appendix B.

## Acknowledgement

We thank Tiffany Cai for her help with implementing the DISDE method on our benchmarks. Peng Cui was supported in part by National Key R&D Program of China (No. 2018AAA0102004), National Natural Science Foundation of China (No. U1936219, 62141607). Hongseok Namkoong was partially supported by the Amazon Research Award.

## References

* [1]O. taxi dataset. https://www.kaggle.com/datasets/mnavas/taxi-routes-for-mexico-city-and-quito.
* [2] Us taxi dataset. https://www.kaggle.com/competitions/nyc-taxi-trip-duration/data.
* [3] Robert Adragna, E. Creager, D. Madras, and R. S. Zemel. Fairness and robustness in invariant learning: A case study in toxicity classification. _CoRR_, abs/2011.06485, 2020.
* [4] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In _International Conference on Machine Learning_, pages 60-69. PMLR, 2018.
* [5] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 145-155. PMLR, 13-18 Jul 2020.
* [6] E. Amorim, M. Cancado, and Adriano Veloso. Automated essay scoring in the presence of biased ratings. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 229-237, 2018.
* [7] Shushan Arakelyan, Rocktim Jyoti Das, Yi Mao, and Xiang Ren. Exploring distributional shifts in large language models for code analysis. _CoRR_, abs/2303.09128, 2023.
* [8] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [9] Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. _IEEE transactions on medical imaging_, 38(2):550-560, 2018.
* [10] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar. The iwildcam 2021 competition dataset. _arXiv preprint arXiv:2105.03494_, 2021.
* [11] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. _IBM J. Res. Dev._, 63(4/5):4:1-4:15, 2019.
* [12] Sarah Bird, Miro Dudik, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in ai. _Microsoft, Tech. Rep. MSR-TR-2020-32_, 2020.
* [13] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.

* [14] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In _WWW_, pages 491-500, 2019.
* [15] Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. Why did the distribution change? In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, pages 1666-1674. PMLR, 2021.
* [16] Tiffany Cai, Hongseok Namkoong, Steve Yadlowsky, et al. Diagnosing model performance under distribution shift. _arXiv preprint arXiv:2303.02011_, 2023.
* [17] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1448-1458. PMLR, 13-18 Jul 2020.
* [18] Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Joint transfer and batch-mode active learning. In _International conference on machine learning_, pages 253-261. PMLR, 2013.
* [19] Lu Cheng, Ruocheng Guo, Raha Moraffah, Paras Sheth, K Selcuk Candan, and Huan Liu. Evaluation methods and measures for causal learning algorithms. _IEEE Transactions on Artificial Intelligence_, 3(6):924-943, 2022.
* [20] David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. _Journal of artificial intelligence research_, 4:129-145, 1996.
* [21] Ian C Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. _The Journal of Machine Learning Research_, 22(1):9477-9566, 2021.
* [22] Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment Inference for Invariant Learning. In _Proceedings of the 38th International Conference on Machine Learning_, pages 2189-2200. PMLR, 2021.
* [23] Gabriela Csurka. A comprehensive survey on domain adaptation for visual applications. In Gabriela Csurka, editor, _Domain Adaptation in Computer Vision Applications_, Advances in Computer Vision and Pattern Recognition, pages 1-35. Springer, 2017.
* [24] Peng Cui and Susan Athey. Stable learning establishes some common ground between causal inference and machine learning. _Nat. Mach. Intell._, 4(2):110-115, 2022.
* [25] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. _Advances in neural information processing systems_, 34:6478-6490, 2021.
* [26] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* [27] John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for latent covariate mixtures. _Operations Research_, 71(2):649-664, 2023.
* [28] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. _The Annals of Statistics_, 49(3):1378-1406, 2021.
* [29] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In _ICCV_, pages 1657-1664, 2013.
* [30] Abolfazal Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R. Arabnia. A brief review of domain adaptation. _CoRR_, abs/2010.03978, 2020.
* [31] Josh Gardner, Zoran Popovic, and Ludwig Schmidt. Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation, 2022.
* [32] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In _International Conference on Machine Learning_, pages 2242-2251. PMLR, 2019.

* Ghorbani et al. [2022] Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch active learning. In _2022 56th Asilomar Conference on Signals, Systems, and Computers_, pages 1456-1462. IEEE, 2022.
* Grinsztajn et al. [2022] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on tabular data?, 2022.
* Gulrajani and Lopez-Paz [2021] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* Hand [2006] David J Hand. Classifier technology and the illusion of progress. _Statistical Science_, 2006.
* Hardt et al. [2016] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. _Advances in neural information processing systems_, 29, 2016.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization, 2021.
* Hernan et al. [2004] Miguel A Hernan, Sonia Hernandez-Diaz, and James M Robins. A structural approach to selection bias. _Epidemiology_, pages 615-625, 2004.
* Hu et al. [2018] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does Distributionally Robust Supervised Learning Give Robust Classifiers? In _Proceedings of the 35th International Conference on Machine Learning_, pages 2029-2037. PMLR, 2018.
* Hussein et al. [2017] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. _ACM Computing Surveys (CSUR)_, 50(2):1-35, 2017.
* Idrissi et al. [2022] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In _Proceedings of the First Conference on Causal Learning and Reasoning_, pages 336-351. PMLR, 2022.
* Iooss and Lemaitre [2015] Bertrand Iooss and Paul Lemaitre. A review on global sensitivity analysis methods. _Uncertainty management in simulation-optimization of complex systems: algorithms and applications_, pages 101-122, 2015.
* Jia et al. [2019] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1167-1176. PMLR, 2019.
* Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* Kaddour et al. [2022] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal machine learning: A survey and open problems. _arXiv preprint arXiv:2206.15475_, 2022.
* Koh et al. [2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A Benchmark of in-the-Wild Distribution Shifts. In _Proceedings of the 38th International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* Konyushkova et al. [2017] Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. _Advances in neural information processing systems_, 30, 2017.
* Koyama and Yamaguchi [2021] Masanori Koyama and Shoichiro Yamaguchi. When is invariance useful in an out-of-distribution generalization problem?, 2021.

* [50] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5815-5826. PMLR, 18-24 Jul 2021.
* [51] Kun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, and Bo Li. Stable prediction with model misspecification and agnostic distribution shift. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4485-4492, 2020.
* [52] Sean Kulinski and David I. Inouye. Towards explaining distribution shifts. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 17931-17952. PMLR, 23-29 Jul 2023.
* [53] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. _Advances in Neural Information Processing Systems_, 33:8847-8860, 2020.
* [54] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _ICCV_, pages 5542-5550, 2017.
* [55] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. Tune: A research platform for distributed model selection and training. _arXiv preprint arXiv:1807.05118_, 2018.
* [56] Justin Lim, Christina X. Ji, Michael Oberst, Saul Blecker, Leora Horwitz, and David Sontag. Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance. https://arxiv.org/abs/2110.14508v1, 2021.
* [57] Evan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In _Proceedings of the 38th International Conference on Machine Learning_, pages 6781-6792. PMLR, 2021.
* [58] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 6804-6814. PMLR, 2021.
* [59] Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He. Influence selection for active learning. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pages 9254-9263. IEEE, 2021.
* [60] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* [61] Rafid Mahmood, Sanja Fidler, and Marc T. Law. Low-budget active learning via wasserstein distance: An integer programming approach. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [62] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In _International Conference on Machine Learning_, pages 6905-6916. PMLR, 2020.
* [63] John P. Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. In _Proceedings of the 38th International Conference on Machine Learning_, pages 7721-7735. PMLR, 2021.
* [64] Sobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. A countrywide traffic accident dataset. _CoRR_, abs/1906.05409, 2019.

* [65] Sobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. Accident risk prediction based on heterogeneous sparse data: New dataset and insights. In Farnoush Banaei Kashani, Goce Trajcevski, Ralf Hartmut Guting, Lars Kulik, and Shawn D. Newsam, editors, _Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2019, Chicago, IL, USA, November 5-8, 2019_, pages 33-42. ACM, 2019.
* [66] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In _EMNLP-IJCNLP_, pages 188-197, 2019.
* [67] Michael Oberst, Fredrik D. Johansson, Dennis Wei, Tian Gao, Gabriel Brat, David Sontag, and Kush R. Varshney. Characterization of Overlap in Observational Studies, 2020.
* [68] Art B Owen. Sobol'indices and shapley value. _SIAM/ASA Journal on Uncertainty Quantification_, 2(1):245-251, 2014.
* [69] Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. In Craig Boutilier, editor, _IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17, 2009_, pages 1187-1192, 2009.
* [70] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 8024-8035, 2019.
* [71] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python. _J. Mach. Learn. Res._, 12:2825-2830, 2011.
* [72] Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, pages 947-1012, 2016.
* [73] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees. _ACM SIGPLAN Notices_, 51(10):731-747, 2016.
* [74] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International conference on machine learning_, pages 5389-5400. PMLR, 2019.
* [75] Marinka Zitnik Yuxiao Dong Hongyu Ren, Bowen Liu Michele Catasta Jure Leskovec, Weihua Hu, and Matthias Fey. Open graph benchmark: Datasets for machine learning on graphs. _arXiv preprint arXiv:2005.00687_, 2020.
* [76] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, et al. Common pitfalls and recommendations for using machine learning to detect and prognosticate for covid-19 using chest radiographs and ct scans. _Nature Machine Intelligence_, 3(3):199-217, 2021.
* [77] Mateo Rojas-Carulla, Bernhard Scholkopf, Richard E. Turner, and Jonas Peters. Invariant models for causal transfer learning. _J. Mach. Learn. Res._, 19:36:1-36:34, 2018.
* [78] Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.

* [79] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. _arXiv:1911.08731_, 2020.
* [80] Roshni Sahoo, Lihua Lei, and Stefan Wager. Learning from a biased sample. _arXiv preprint arXiv:2209.01754_, 2022.
* [81] Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. In _International Conference on Learning Representations_, 2021.
* July 1, 2012_. icml.cc / Omnipress, 2012.
* [83] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proc. IEEE_, 109(5):612-634, 2021.
* [84] Burr Settles. Active learning literature survey. 2009.
* [85] Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. _Journal of Machine Learning Research_, 20(103):1-68, 2019.
* [86] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9661-9669, 2021.
* [87] Jie-Jing Shao, Yunlu Xu, Zhanzhan Cheng, and Yu-Feng Li. Active model adaptation under unknown shift. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1558-1566, 2022.
* [88] Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 739-748, 2020.
* [89] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30, 2016.
* [90] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7167-7176, 2017.
* [91] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _CVPR_, pages 5018-5027, 2017.
* [92] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. _Advances in Neural Information Processing Systems_, 31, 2018.
* [93] Andrew Wong, Erkin Oules, John P Donnelly, Andrew Krumm, Jeffrey McCullough, Olivia DeTroyer-Cooley, Justin Pestrue, Marie Phillips, Judy Konye, Carleen Penoza, et al. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. _JAMA Internal Medicine_, 181(8):1065-1070, 2021.
* [94] Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In _International Conference on Machine Learning_, pages 6818-6827. PMLR, 2019.
* [95] Laure Wynants, Ben Van Calster, Gary S Collins, Richard D Riley, Georg Heinze, Ewoud Schuit, Elena Albu, Banafsheh Arshi, Vanesa Bellou, Marc MJ Bonten, et al. Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal. _bmj_, 369, 2020.

* [96] Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, and Xinjing Cheng. Towards fewer annotations: Active learning via region impurity and prediction uncertainty for domain adaptive semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 8058-8068. IEEE, 2022.
* [97] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective, 2023.
* [98] Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer look at subpopulation shift. _CoRR_, abs/2302.12254, 2023.
* [99] Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is Hard: A Closer Look at Subpopulation Shift, 2023.
* [100] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In _NeurIPS_, 2022.
* [101] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. _Advances in Neural Information Processing Systems_, 35:10309-10324, 2022.
* [102] Han Yu, Peng Cui, Yue He, Zheyan Shen, Yong Lin, Renzhe Xu, and Xingxuan Zhang. Stable learning via sparse variable independence. _AAAI_, abs/2212.00992, 2022.
* [103] John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. _PLoS medicine_, 15(11):e1002683, 2018.
* [104] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In _International conference on machine learning_, pages 325-333. PMLR, 2013.
* [105] Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. DORO: Distributional and Outlier Robust Optimization. In _Proceedings of the 38th International Conference on Machine Learning_, pages 12345-12355. PMLR, 2021.
* [106] Haoran Zhang, Harvineet Singh, Marzyeh Ghassemi, and Shalmali Joshi. "Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts, 2023.
* [107] Xingxuan Zhang, Yue He, Tan Wang, Jiaxin Qi, Han Yu, Zimu Wang, Jie Peng, Renzhe Xu, Zheyan Shen, Yulei Niu, et al. Nico challenge: Out-of-distribution generalization for image recognition challenges. In _Computer Vision-ECCV 2022 Workshops: Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VI_, pages 433-450. Springer, 2023.
* [108] Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer. Multi-source domain adaptation for semantic segmentation. _arXiv preprint arXiv:1910.12181_, 2019.

## Appendix A Relevant Work

* 1 On Distribution Shifts
* 2 Connection between Non-algorithmic Interventions and Algorithmic Interventions
* B Discussion on Limitations
* C Case Study Details
* 3 DISDE to \(X\)-shifts and \(Y|X\)-shifts
	* 4.2 Prevalence of \(Y|X\)-Shifts
	* 5.3 Details of Algorithm 1
	* 5.4 Analysis of Decision Tree
	* 5.5 Details of Non-algorithmic Interventions
	* 6.6 Potential Alternative Approach of Identifying Risk Region
* D Benchmark Details
	* 4.1 Datasets & Settings
	* 4.2 Algorithms & Implementations
	* 4.3 Parameter Search Space
	* 4.4 Training Details
	* 4.5 Detailed Results of 7 Selected Settings in Main Body
	* 4.6 Full Results of 22 Settings
	* 4.7 Worst-Domain Accuracy
	* 4.8 Average Macro-F1 Score
	* 4.9 Worst-Domain Macro-F1 Score
* 4.10 Agreement and Maintenance Plan

## Appendix A Relevant Work

### On Distribution Shifts

Distribution Shift Benchmarks.Existing distribution shift benchmarks primarily concentrate on image and language datasets [74, 47, 107, 81, 101] to assess the robustness and efficiency of algorithms in real applications.

We briefly review benchmarks that address distribution shifts across various data types, including image and language data. For _image data_, several datasets capture natural distribution shifts, such as spatial and temporal variations. PACS [54] and Office-Home [91] categorize environments to be image styles. VLCS [29] and iWildCam [10] set their primary environments as data sources. DomainNet [108] is built on PACS and provides a more extensive selection with additional domains and categories. In recent developments, Koh et al. [47] collect several datasets to establish WILDS, setting a new structure for OOD generalization. Similarly, Yao et al. [100] introduced Wild-Time, highlighting temporal distribution shifts across diverse real-world scenarios. For _language_ data, CivilComments [14] and Amazon [66] consist of individual comments collected from different users and distinctive groups (e.g. male and female). GLUE-X [97] provides a unified benchmark for evaluating OOD robustness in NLP models. For _other types_ of data, OGB-MolPCBA [75] collectsmolecular graphs in over 100,000 scaffolds and formulates a molecular property prediction task across different scaffolds. Towards auto-engineering, Py150 [73] contains codes from 8,421 git repositories for code completion generalization.

However, these datasets/benchmarks do not specify or investigate in-depth the distribution shift patterns, and there is a noticeable lack of benchmark papers that specifically address real-world tabular datasets. These tabular datasets often present distinct patterns compared to image/language datasets, including prevalent methods [34] and shift patterns.

Therefore, it is important to understand detailed distribution patterns among these datasets to develop corresponding methodologies to address specific shifts, which is also observed in a recent benchmark [101]. Yang et al. [99] recently introduce one benchmark characterizing different patterns of subpopulation shift. This benchmark focuses on the relationship between a specific attribute and covariates, such as spurious correlation, which can be challenging to identify in tabular datasets. Besides, they focus on changes in the subpopulation, which is a subset of distribution shift patterns. For example, individuals in different states may suffer from little subpopulation shifts but still incur a large distribution shift. Individuals' demographic features are similar but the income level differs greatly between CA and PR in ACS Income, as demonstrated in Table 2. Besides, Kulinski and Inouye [52] use optimal transport to explain the shift between two distributions recently. But their method could not explicitly decompose the performance degradation as in our work. Motivated by the challenges above, our work hopes to fill in the gap by demonstrating one benchmark on tabular datasets with detailed analysis.

Choice of Domains in Distribution ShiftsAlthough it is popular in the fairness literature to set each demographic subgroup in the adult and ACS dataset as one domain [104, 22, 25], the _relative regret_ of models trained on these domains is quite small shown in Figure 1 empirically. This implicitly shows that \(Y|X\)-shifts are not strong across different demographic subgroups. In contrast, the model usually experiences relatively large \(Y|X\)-shifts under different spatial domains, which corresponds to the right-hand side of Figure 1. Besides, from the practical perspective, machine learning models often need to be deployed in spatialtemporal domains (by city, state, and country-wise; by year) only with few observed data while the model is trained with abundant data from the source domain. This is why we mainly choose spatiotemporal domains to benchmark distribution shifts in our case.

### Connection between Non-algorithmic Interventions and Algorithmic Interventions

Here we discuss the connections between our non-algorithmic interventions and several branches of methods addressing the distribution shifts, including active learning, imitation learning, causal learning, feature sensitivity analysis, and feature importance analysis.

Active Learning.Active learning aims to improve model performance by acquiring a limited number of labels from the target distribution. See the survey [84] for a detailed reference. The challenge here is to quantify the value of unlabeled data so that we can select samples better. The selection criterion of existing approaches includes estimated variance [20], influence on the model performance [59], and Shapley value [32, 33, 44], or source-target distance metrics through the importance weighting [18, 61]. Some work also proposed running a regression problem for the query procedure based on learning strategies from the existing dataset [48]. Specifically, the target distribution where we query data and aim to evaluate the model subsequently differs from the source

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline State & CA & MA & NY & NE & MT & SD & PR \\ \hline Total Size & 195665 & 40114 & 103021 & 10785 & 5463 & 4899 & 9071 \\ \hline White Man & 0.3311 & 0.4161 & 0.3687 & 0.4822 & 0.4959 & 0.4691 & 0.3526 \\ Nonwhite Man & 0.1969 & 0.0878 & 0.1379 & 0.0454 & 0.0428 & 0.0492 & 0.1818 \\ White Woman & 0.2873 & 0.4066 & 0.3464 & 0.4318 & 0.4221 & 0.4297 & 0.3124 \\ NonWhite Woman & 0.1847 & 0.0895 & 0.1471 & 0.0404 & 0.0392 & 0.0521 & 0.1532 \\ \hline \(P(Y=0)\) & 0.589 & 0.532 & 0.585 & 0.688 & 0.707 & 0.730 & 0.894 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Descriptive Characteristics under different states in ACS income datasetdistribution. Some work also develops active learning methods on domain adaptation to select additional samples from the target distribution [88, 87]. However, these works usually assume some restricted distribution structures between the source and target distribution such as only \(X\)-shifts occur, which may not hold in the tabular data. In contrast, we assume we have a few labeled target data but impose no restrictions on the two distributions. And we sample data in specific regions after identifying regions where we experience the largest \(Y|X\)-shifts.

Imitation LearningImitation learning aims to mimic the behavior of the expert with some off-policy data. This usually occurs in the reinforcement learning setup where a learner aims to learn the best action given each state [41]. Here, distribution shifts occur due to the mismatch of the state coverage between the observed data (source domain) and the environment to deploy (target domain). This raises the need for the importance sampling method to match the two distributions in imitation learning [92, 94]. In fact, the underlying conditional distribution does not change only with covariate (state) shifts in the imitation learning setup. Therefore, this does not fit into our case where the conditional distribution between the source and target domains differ.

Causal LearningCausal learning methods receive much attention in the field of machine learning. The core idea of causal learning [72, 8] is to learn causally invariant relationships across multiple pre-defined training environments. Arjovsky et al. [8] propose Invariant Risk Minimization (IRM) to learn invariant representations across environments, and follow-up works [17, 49, 5, 50] propose variations with similar invariant regularizations. However, the effectiveness of these methods are challenged both theoretically and empirically. Theoretically, Rosenfeld et al. [78] illustrate that IRM could fail in a nonlinear context, and Liu et al. [58] demonstrate that the learned invariance property largely depends on the quality of pre-defined environments. Empirically, Gulrajani and Lopez-Paz [35] show that when carefully implemented and tuned, ERM still outperforms the state-of-the-art methods in terms of average performance.

Compared with the simple non-algorithmic interventions in this work, causal learning methods rely heavily on the invariance assumptions and have strict requirements on the quality of multi-environment data. This restricts their applicability in practice, since modern datasets are often collected without explicit environment labels, and in many scenarios, it is quite hard to pre-define meaningful environment labels. Our proposed non-algorithmic interventions (collecting features and data) do not rely on the invariance assumption, and it could serve as a "solution" when observing performance degradation, which helps to analyze the model failure and to direct further improvements. And we hope that these simple data-centric interventions could inspire future research in this direction to mitigate the effects of distribution shifts.

Since we are also considering other missing features to improve the performance under distribution shifts, there are some works providing insights on the feature / region importance to the final output.

Feature Analysis: Sensitivity and Importance.There are two streams of literature measuring the relationship between input features and the response variable. These are feature sensitivity analysis and feature importance. Feature sensitivity analysis aims to quantify the sensitivity of the performance metrics to each input. This helps understand the impact of variations in input features on the output of a simulation system. Classical metrics include ANOVA, Sobol indices [68] and Morris methods [43]. Meanwhile, feature importance aims to understand the performance decomposition of different algorithms. Shapley value-based approaches gain the most popularity in understanding the attribution of predictions to each input feature [60, 21]. Some work leverages these ideas to understand the difference in distribution shifts to each existing input feature in the data [15, 106]. They decompose the shift on the joint distribution to particular \(X_{i}\)-shifts or the condition \(X_{j}|X_{i}\)-shifts (\(X_{i}\) and \(X_{j}\) denote different features) under a known causal graph. In contrast, we investigate the additional missing feature beyond existing datasets and aim to reduce the performance drop under distribution shifts in our paper. Specifically, we focus on the local regions where the distribution incurs the largest shifts and add the feature ENG in our main body due to our prior knowledge that "ENG" feature would yield the largest difference in that subpopulation between two distributions. Our region-basedapproach, in fact, can be further rigorously extended to investigate the marginal distribution of what features would yield the largest difference in that region between two distributions. We hope this can inspire researchers to apply refined tools in this line such as Shapley value to help understand and mitigate distribution shifts.

Region Analysis: Attribution of Distribution Shifts.Similar to our algorithmic goal in Section 3.1 of identifying regions where model learners are different, Oberst _et al._[67] and Lim _et al._[56] developed specific methods to identify specific covariate regions where model learners are different. The difference between their setup and ours lies in the sampling and assumption of observed data. In their approach, the observed data is sampled i.i.d. across various prediction models, without any distribution shift. Besides, they can observe only one selected prediction result per sample from all the model learners. However, in our case study, the models are built on datasets from two domains that experience distribution shifts. As a result, we can further isolate the model difference based on the shared input space \(X\) and differentiate it from the total difference, specifically focusing on the \(Y|X\)-shifts.

## Appendix B Discussion on Limitations

We discuss some limitations of our work.

For BenchmarkFirst, we only consider the source-target transfer pairs from datasets including the economic and transportation domain, and we leave the detailed pattern evaluation of these datasets in other domains such as the medical area (e.g. MIMIC-III[45]) as an interesting direction of the future work. Besides, we only consider the source and target from one fixed domain (i.e. one state or city). In practice, it is reasonable to extend this benchmark to consider the source and target distribution with multiple domains of varying proportions. Our results of characterizing the distribution shift patterns highlight the importance of utilizing other refined tools. These tools can help us understand the difference between real-world distribution shifts and enable further investigation and analysis.

For Algorithm 1Our Algorithm 1 is proposed to identify risky regions with large \(Y|X\)-shifts when observing severe model degradation between the source target distributions. Therefore, one limitation is that it requires target data to identify the risky regions with large \(Y|X\)-shifts. This algorithm cannot evaluate the generalization performance when the target distribution is completely unknown.

Furthermore, when conducting non-algorithmic interventions based on the risky regions learned by Algorithm 1, researchers should be careful and _incorporate more background knowledge_ to find the proper way. For example, in Section 3.2, we analyze the risky regions and find that the "ENG" feature may be important to mitigate the \(Y|X\)-shifts. This is from our prior knowledge that the official language between the two states is different. Also, when Algorithm 1 is misused (e.g., the learned risky regions are used without destination or not being checked carefully), it might harm some vulnerable groups.

For Data-CollectionIn Section 3.2, we propose two simple non-algorithmic interventions to mitigate the \(Y|X\)-shifts, one of which is to efficiently collect target data from the risky region. It achieves much better results combined with several typical methods on our benchmark. However, in practice, we acknowledge that this technique could only be used when we can obtain data from the target distribution and the data collection procedure does not raise any privacy concerns and predatory inclusion.

Applicability Across Different Data ModalitiesIn this work, we focus on real-world distribution shifts in tabular data settings. We propose Algorithm 1 to find the risk regions and come up with some simple data-based interventions. Therefore, we do not investigate in-depth the applicability of the proposed methods across other data types. However, we argue that our proposed Algorithm 1 could generalize to complicated data types (e.g., image data) with corresponding deep models.

Specifically, both the domain classifier (to estimate \(\mathbb{P}(X\) from \(Q_{X}|X)\)) and region learner \(h(X)\) should be replaced by deep neural networks. And our data-based interventions also have the potential to incorporate with more complicated data types. We leave this investigation to future work.

## Appendix C Case Study Details

In this section, we provide more details about our case study in the main body.

### DISDE to \(X\)-shifts and \(Y|x\)-shifts

When facing performance degradation under distribution shifts, one direct idea is to figure out the reasons why the performance drop. To this end, Cai et al. [16] propose DIstribution Shift DEcomposition (DISDE) to attribute the total performance degradation to \(Y|X\)-shifts and \(X\)-shifts. Specifically, given samples \((X,Y)\) from distributions \(P\) and \(Q\), to quantify the discrepancy between \(P_{Y|X}\) and \(Q_{Y|X}\), they first control the marginal distribution on \(\mathcal{X}\) by introducing the shared distribution \(S_{X}\). From that, we can estimate the performance degradation caused by \(Y|X\)-shifts and that caused by \(X\)-shifts could also be estimated by comparing \(S_{X}\) with \(P_{X}\) and \(Q_{X}\), respectively. Note that DISDE could be used in image datasets (see Section 4.2 in [16]). The official code for DISDE could be found at https://github.com/namkoong-lab/disde. We specify the formula of DISDE as follows:

\[\mathbb{E}_{Q}[\ell(f_{P}(X),Y)]-\mathbb{E}_{P}[\ell(f_{P}(X),Y)] =\mathbb{E}_{S_{X}}[R_{P}(X)]-\mathbb{E}_{P}[R_{P}(X)]\] (I) \[+\mathbb{E}_{S_{X}}[R_{Q}(X)-R_{P}(X)]\] (II) \[+\mathbb{E}_{Q}[R_{Q}(X)]-\mathbb{E}_{S_{X}}[R_{Q}(X)],\] (III)

where \(R_{\mu}(x):=\mathbb{E}_{\mu}[\ell(f_{P}(X),Y)|X=x]\) for \(\mu=P,Q\) is denoted as the conditional risks on \(P\) and \(Q\). \(S_{X}\) is the share distribution with support contained in both \(P_{X}\) and \(Q_{X}\). _Then we see the sum of the two terms (I) and (III) as the performance drop attributed to \(X\)-shifts and the term (II) as the performance drop attributed to \(Y|X\)-shifts._ Besides, we also implement DISDE in our released package named WhyShift, which could be found at https://github.com/namkoong-lab/whyshift.

### Prevalence of \(Y|x\)-Shifts

For each of the 7 settings in Table 1, we only select one target distribution in the main body, while our benchmark supports multiple target distributions. Specifically, for ACS Income, ACS Mobility, ACS Pub.Cov, we have 50 target distributions (the other 50 American states) in total for each setting; for Taxi, we have 3 target distributions (other cities Mexico City, Bogota, Quitio); for the temporal shift version of ACS Pub.Cov (i.e. setting 6), we have 3 target distributions (i.e. the year 2014, 2017, and 2021); and for US Accident, we have 13 target distributions (13 American states). Therefore, we have 169 source-target transfer pairs in total for these 7 settings.

We use XGBoost classifier and calculate the decomposition of performance degradation via DISDE [16]. We calculate the \(Y|X\)**-shift ratio** from the source distribution \(P\) to the target distribution \(Q\):

\[Y|X\text{-shift ratio}=\frac{\mathbb{E}_{S_{X}}[R_{Q}(X)-R_{P}(X)]}{\mathbb{ E}_{Q}[\ell(f_{P}(X),Y)]-\mathbb{E}_{P}[\ell(f_{P}(X),Y)]},\]

where \(R_{\mu}(X)\) is defined in Appendix C.1. We first focus on pairs with relatively strong distribution shifts (i.e. performance degradation larger than 8 percentage points). Across these pairs, we find that **70.2%** pairs have over **60%**\(Y|X\)-shifts, and **87.2%** pairs have over **50%**\(Y|X\)-shifts, indicating the prevalence of \(Y|X\) shifts. To visualize the result better, in all 22 settings in our benchmark (as shown in Table 3), we focus on transfer pairs with performance degradation larger than 5 percentage points and plot the histogram of the ratio of \(Y|X\)-shifts in Figure 8. From Figure 8, we could see that \(Y|X\)-shifts are prevalent in real-world distribution shifts.

### Details of Algorithm 1

In this section, we provide a more detailed introduction of our proposed Algorithm 1. We propose a simple yet efficient method to identify data regions with strong \(Y|X\) shifts, and it could inspire operational and modeling interventions as shown in Section 3.2.

Consider a model \(f:\mathcal{X}\to\mathcal{Y}\) that predicts outcome \(Y\) from covariates \(X\). Let \(\ell(f(x),y)\) be a loss function denoting a notion of predictive error (e.g., cross-entropy loss, mean-square error). Our goal is to identify a region \(\mathcal{S}\subseteq\mathcal{X}\) where \(P(Y|X)\) differs a lot from \(Q(Y|X)\). In order to directly compare \(P(Y|X)\) and \(Q(Y|X)\), we must do an apples-to-apples comparison: we cannot know \(Q(Y|X)\) for \(X\)'s that are primarily observed in \(P\) (and vice versa).

To address this, we first construct a shared distribution \(S_{X}\) over \(X\) whose support is contained in that of both \(P_{X}\) and \(Q_{X}\). We choose a specific _shared distribution_\(S_{X}\) over \(X\) whose support is contained in that of \(P_{X}\) and \(Q_{X}\) (following [16]). Ideally, the chosen shared distribution would exhibit a higher density when both \(P_{X}\) and \(Q_{X}\) densities are high, and a lower density when either of the two possesses a low density. This strategy effectively allows regions of shared density to be more pronounced. Recall that \(p_{X},q_{X},s_{X}\) are the densities of \(X\) under \(P,Q\) and \(S\), we formulate \(s_{X}\) as

\[s_{X}(x)\propto p_{X}(x)q_{X}(x)/(p_{X}(x)+q_{X}(x)).\] (C.1)

Choices of \(S_{X}\)Here we would like to discuss different choices of \(S_{X}\) to provide more intuitions. As demonstrated in [16], we can use many forms of the shared distribution. For example, we could choose the following form:

\[s_{X}(x)\propto\text{min}\{p_{X}(x),q_{X}(x)\},\] (C.2)

which guarantees that the support of \(S_{X}\) is contained in that of both \(P_{X}\) and \(Q_{X}\). Another choice is:

\[s_{X}(x)\propto\begin{cases}p_{X}(x)+q_{X}(x)&\text{if }\min\{\frac{p_{X}(x)}{q_{ X}(x)},\frac{q_{X}(x)}{p_{X}(x)}\}\geq\epsilon,\\ 0&\text{otherwise},\end{cases}\] (C.3)

for some \(\epsilon\geq 0\). This form of \(S_{X}\) defines shared samples as those with high likelihood ratios. Notably, for all the three forms of \(S_{X}\), if \(P_{X}=Q_{X}\), then \(S_{X}=P_{X}=Q_{X}\). And when \(p_{X}(x)\gg q_{X}(x)\) or \(p_{X}(x)\ll q_{X}(x)\), Equation (C.1) and Equation (C.2) become similar. In our Algorithm 1, we use the form of Equation (C.1), and Cai et al. [16] observe that in practice the qualitative conclusions are not very sensitive to the specific choice of shared distribution.

Intuitions behind Algorithm 1Since we do not have access to samples from the shared distribution \(S_{X}\), we reweight samples from \(P_{X}\) and \(Q_{X}\) using the likelihood ratios:

\[\frac{s_{X}}{p_{X}}(x)\propto\frac{q_{X}(x)}{p_{X}(x)+q_{X}(x)}\text{ and }\frac{s_{X}}{q_{X}}(x)\propto\frac{p_{X}(x)}{p_{X}(x)+q_{X}(x)}.\] (C.4)

Then we define \(\hat{\alpha}\) as the proportion of the pooled data that comes from distribution \(Q\):

\[\hat{\alpha}=\frac{n_{Q}}{n_{P}+n_{Q}}\quad\text{and}\quad\hat{\pi}(x)=\mathbb{ P}(\tilde{X}\text{ from }Q_{X}|\tilde{X}=x),\] (C.5)

Figure 8: Histogram of \(Y|X\)-shift ratio in our benchmark.

where \(\hat{\pi}(x)\) denotes the probability of a sample to come from \(Q_{X}\). Using Bayes' rule, we have:

\[\hat{\pi}(x) =\frac{\mathbb{P}(\tilde{X}=x|\tilde{X}\text{ from }Q_{X})\mathbb{P}( \tilde{X}\text{ from }Q_{X})}{\mathbb{P}(x)}=\frac{\hat{\alpha}q(x)}{\hat{\alpha}q(x)+(1- \hat{\alpha})p(x)},\] (C.6) \[=\frac{\hat{\alpha}}{\hat{\alpha}+(1-\hat{\alpha})\frac{p(x)}{q( x)}}.\]

Noting that the ratio \(\hat{\pi}(x)\) can be modeled as the probability that an input \(x\) came from \(P_{X}\) vs \(Q_{X}\), we train a binary "domain" classifier to estimate the ratios. (The "domain" classifier can be any black-box method, and we use XGBoost throughout.)

Then the likelihood ratios that we care about could be reformulated as:

\[\frac{s_{X}}{p_{X}}(x)\propto\frac{1}{\frac{p_{X}(x)}{q_{X}(x)}+1}\text{ and }\frac{s_{X}}{q_{X}}(x)\propto\frac{\frac{p_{X}(x)}{q_{X}(x)}}{\frac{p_{X}(x)}{q _{X}(x)}+1},\] (C.7)

which gives that:

\[\frac{s_{X}}{p_{X}}(x)\propto\frac{\hat{\pi}(x)}{(1-\hat{\alpha})\hat{\pi}(x) +\hat{\alpha}(1-\hat{\pi}(x))}\quad\text{and}\quad\frac{s_{X}}{q_{X}}(x)\propto \frac{1-\hat{\pi}(x)}{(1-\hat{\alpha})\hat{\pi}(x)+\hat{\alpha}(1-\hat{\pi}(x) )}.\] (C.8)

After obtaining the likelihood ratios \(\frac{s_{X}}{p_{X}}(x)\) and \(\frac{s_{X}}{q_{X}}(x)\), we could do an apples-to-apples comparison: we estimate \(P_{Y|X}\) and \(Q_{Y|X}\) over the shared distribution \(S_{X}\) (using XGBoost \(\mathcal{F}\))

\[f_{P} :=\arg\min_{f\in\mathcal{F}}\left\{\mathbb{E}_{S_{X}}\Big{[} \mathbb{E}_{P}[\ell(f(X),Y)|X]\Big{]}=\mathbb{E}_{P}\Big{[}\ell(f(X),Y)\frac{ dS_{X}}{dP_{X}}(X)\Big{]}\right\},\] (C.9) \[f_{Q} :=\arg\min_{f\in\mathcal{F}}\left\{\mathbb{E}_{S_{X}}\Big{[} \mathbb{E}_{Q}[\ell(f(X),Y)|X]\Big{]}=\mathbb{E}_{Q}\Big{[}\ell(f(X),Y)\frac{ dS_{X}}{dQ_{X}}(X)\Big{]}\right\}.\] (C.10)

Then, for any threshold \(b\in[0,1]\), \(\{x\in\mathcal{X}:|f_{P}(x)-f_{Q}(x)|\geq b\}\) suggests a region that may suffer model performance degradation due to \(Y|X\)-shifts.

### Analysis of Decision Tree

In Algorithm 1, we use a shallow _decision tree_\(h(x)\) to approximate \(y=|f_{P}(x)-f_{Q}(x)|\) on the shared distribution \(S_{X}\) to find the covariate region with highest discrepancy. In our decision tree, we use the _squared error_ as the splitting criterion. And below we demonstrate that this criterion is equivalent to maximizing the discrepancy between two children nodes.

Suppose there are \(N\) samples with outcomes \(\{y_{i}\}_{i\in[N]}\) belonging to tree node \(fa\), and these samples are split into two children nodes \(s_{1},s_{2}\), where the node \(s_{1},s_{2}\) denote the set of sample indices in the two children nodes respectively. The squared error criterion to split \(fa\) into \(s_{1}\) and \(s_{2}\) is:

\[\min_{s_{1},s_{2}}\bigg{\{}\mathcal{L}(s_{1},s_{2}):=\frac{1}{N} \bigg{(}\sum_{i\in s_{1}}(y_{i}-\mu_{Y,1})^{2}+\sum_{i\in s_{2}}(y_{i}-\mu_{Y, 2})^{2}\bigg{)}\bigg{\}},\] (C.11)

where

\[\mu_{Y,1}:=\frac{\sum_{i=1}^{N}y_{i}\mathbf{1}_{\{i\in s_{1}\}}}{ \sum_{i=1}^{N}\mathbf{1}_{\{i\in s_{1}\}}},\quad\mu_{Y,2}:=\frac{\sum_{i=1}^{N} y_{i}\mathbf{1}_{\{i\in s_{2}\}}}{\sum_{i=1}^{N}\mathbf{1}_{\{i\in s_{2}\}}}\] (C.12)

denote the mean values of the outcome \(Y\) with samples in children nodes \(s_{1}\) and \(s_{2}\). Denote the distribution of the outcome \(Y\) follows the empirical distribution over the \(N\) samples \(\{y_{i}\}_{i\in[N]}\). Simplifying (C.11), we have:

\[\mathcal{L}(s_{1},s_{2})=P(Y\in s_{1})\text{Var}_{s_{1}}(Y)+P(Y\in s _{2})\text{Var}_{s_{2}}(Y)=\mathbb{E}_{S}[\text{Var}(Y|S)],\] (C.13)

where \(\text{Var}_{s}(Y)\) denotes the variance of the outcome variable \(Y\) in node \(s\), \(S=\{s_{1},s_{2}\}\) is the variable representing the children nodes. Therefore, given that \(\text{Var}_{fa}(Y)(:=\text{Var}_{S}(\mathbb{E}[Y|S])+\mathbb{E}_{S}[\text{Var}(Y| S)])\) is constant, the minimal \(\mathbb{E}_{S}[\text{Var}(Y|S)]\) corresponds with the largest \(\text{Var}_{S}(\mathbb{E}[Y|S])\), which maximizes the discrepancy of the outcome between two children nodes.

### Details of Non-algorithmic Interventions

In Section 3.2, we propose two potential non-algorithmic interventions to mitigate the performance degradation. In this section, we introduce in detail the intervention of collecting specific data from the target.

Experiment SetupWe focus on the income prediction task using the ACS Income dataset. Consider a practical scenario where the training set consists of 20,000 samples from California (CA) and the trained model was deployed in Puerto Rico (PR) in trial. After the trial deployment in PR, we got a _small amount_ of samples from PR with labels and observed performance degradation. Under this setting, we investigate the effect of non-algorithmic interventions.

Collect specific data from the target We first identify the regions with high discrepancy between source and target. Note that the sample size of the target state is small compared to the training samples. Then for typical algorithms like logistic regression (LR), MLP, random forest (RF), LightGBM and XGBoost, we compare the performances of:

* original setting (only 20,000 samples from the source);
* original setting with \(N\) additional random samples drawn from the whole target state;
* original setting with \(N\) additional random samples drawn from the _risk region_ of the target state.

In this experiment, we first select the best configuration of each method according to the \(i.i.d\) validation set in the original setting (only samples from CA), and fix it for the other two interventions. We vary \(N\) as 100, 200, 300 and the results are shown in Figure 9. From Figure 9, incorporating data from the risk region leads to a _stable improvement_ on typical algorithms even for small target sample sizes. However, we observe that LightGBM and XGBoost would easily overfit \(f_{Q}\) on the target data, and we use random forest under this setup as an alternative. It is worth investigating approaches to find risk regions effectively under small/imbalanced sample sizes in the future. The approach mentioned here is a simple way of non-algorithmic and explainable interventions and we hope it could inspire further research in this direction.

### Potential Alternative Approach of Identifying Risk Region

In Algorithm 1, we propose a simple way to identify the risk region to explain the cause of \(Y|X\)-shifts. And the identified region could be used to guide the collecting process of target data, which could help to further reduce the effects of performance degradation. However, in practice, when the amount of target samples is quite small, it may be hard to train the \(f_{Q}\) only with target samples accurately.

Figure 9: Test accuracies of different ways to incorporate data.

Therefore, it may be better to propose a method that does not need \(f_{Q}\) to fit \(Q(Y|X)\) on the target distribution.

Following this idea, we propose an alternative way of Algorithm 1. Since the training data is enough, it is feasible to fit \(f_{P}\) on the shared distribution \(S_{X}\) as:

\[f_{P}:=\operatorname*{argmin}_{f\in\mathcal{F}}\mathbb{E}_{P}\left[\frac{dS_{X} }{dP_{X}}\ell(f(X),Y)\right].\] (C.14)

Then for \(n_{Q}\) samples from target distribution \(Q\), we could use a prediction model \(h(x)\) to approximate \(|Y-f_{P}(X)|\) on the shared distribution (by reweighting density ratio \(\frac{dS_{X}}{dQ_{X}}\)). Note that this method does not need to train \(f_{Q}\) on target samples, but the quality of the density ratio \(dS_{X}/dP_{X}\) and the prediction model \(h(x)\) still depend on the target samples. We hope the estimation of the ratio and the residuals \(|Y-f_{P}(X)|\) can be less affected by the low sample size \(n_{Q}\). This non-algorithmic intervention is not the main focus of this work, but we hope this idea could help to promote this line of research.

## Appendix D Benchmark Details

In this section, we provide the details of our benchmark. In our benchmark, we explore distribution shifts on 5 real-world tabular datasets from the economic and traffic sectors with natural spatiotemporal distribution shifts. We carefully design 22 settings utilizing these 5 datasets, and the overview of 22 settings is shown in Table 3. Note that in our main body, due to space limitations, we only pick 7 typical settings and select only one representative target domain for each setting, as shown in Table 1. Detailed introductions of all datasets, algorithms, and experiment settings will be given in the following sections. A Python package for our benchmark can be found at https://github.com/namkoong-lab/whyshift.

### Datasets & Settings

Here we introduce 5 real-world tabular datasets in detail. ACS Income, ACS Mobility, ACS Public Coverage are based on the American Community Survey (ACS) Public Use Microdata Sample (PUMS) [25]. And here we use the same data filtering as [25].

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \#ID & Dataset & Type & \#Features & Outcome & Source & \#Train Samples & \#Test Domains & Dom. Ratio\({}^{\circ}\) \\ \hline
1 & ACS Income & Spatial & 9 & Income\(\geq\)50k & California & 195,665 & 50 & \(Y|X\): 13/14 \\
2 & ACS Income & Spatial & 9 & Income\(\geq\)50k & Connecticut & 19,785 & 50 & \(Y|X\): 24/24 \\
3 & ACS Income & Spatial & 9 & Income\(\geq\)50k & Massachusetts & 40,114 & 50 & \(Y|X\): 12/22 \\
4 & ACS Income & Spatial & 9 & Income\(\geq\)50k & South Dakota & 4,899 & 50 & \(Y|Y\): 99 \\
5 & ACS Mobility & Spatial & 21 & Residential Address & Mississippi & 5,318 & 50 & \(Y|X\): 28/34 \\
6 & ACS Mobility & Spatial & 21 & Residential Address & New York & 40,463 & 50 & \(Y|X\): 30/31 \\
7 & ACS Mobility & Spatial & 21 & Residential Address & California & 80,329 & 50 & \(Y|X\): 9/17 \\
8 & ACS Mobility & Spatial & 21 & Residential Address & Pennsylvania & 23,918 & 50 & \(Y|X\): 17/17 \\
9 & Taxi & Spatial & 7 & Duration time\(>\)30 min & Bogota & 3,063 & 3 & \(Y|X\): 10/2 \\
10 & Taxi & Spatial & 7 & Duration time\(>\)30 min & New York City & 1,458,466 & 3 & \(Y|X\): 3/3 \\
11 & ACS Pub.Cov & Spatial & 18 & Public Ins. Coverage & Nebraska\& 6,332 & 50 & \(Y|X\): 32/99 \\
12 & ACS Pub.Cov & Spatial & 18 & Public Ins. Coverage & Florida & 71,297 & 50 & \(Y|X\): 28/29 \\
13 & ACS Pub.Cov & Spatial & 18 & Public Ins. Coverage & Texas & 98,928 & 50 & \(Y|X\): 33/34 \\
14 & ACS Pub.Cov & Spatial & 18 & Public Ins. Coverage & Indiana & 24,330 & 50 & \(Y|X\): 11/13 \\
15 & US Accident & Spatial & 47 & Severity of Accident & Texas & 26,664 & 13 & \(Y|X\): 7/17 \\ \hline
16 & US Accident & Spatial & 47 & Severity of Accident & California & 64,909 & 13 & \(X\): 22/31 \\
17 & US Accident & Spatial & 47 & Severity of Accident & Florida & 32,278 & 13 & \(X\): 5/7 \\
18 & US Accident & Spatial & 47 & Severity of Accident & Minnesota & 8,927 & 13 & \(X\): 8/11 \\
19 & ACS Pub.Cov & Temporal & 18 & Public Ins. Coverage & Year 201 (NY) & 73,208 & 3 & \(X\): 2/22 \\
20 & ACS Pub.Cov & Temporal & 18 & Public Ins. Coverage & Year 2010 (CA) & 149,441 & 3 & \(X\): 2/22 \\
21 & ACS Income & Synthetic & 9 & Income\(\geq\)50k & Younger People (80\%) & 20,000 & 1 & \(X\): 1/1 \\
22 & ACS Income & Synthetic & 9 & Income\(\geq\)30k & Younger People (90\%) & 20,000 & 1 & \(X\): 1/1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Overview of datasets and the whole 22 selected settings. In our benchmark, each setting has multiple target domains (except the last setting). In our main body, we select only one target domain for each setting. \(\circ\): we report the “Dom. Ratio” to represent the dominant ratio of \(Y|X\)-shifts or \(X\)-shifts in source-target pairs with performance degradation larger than **5** percentage points in each setting. For example, “\(Y|X\): 13/14” means that there are 14 source-target pairs in Setting 1 with degradation larger than 5 percentage points and 13 out of them with over 50% degradation attributed to \(Y|X\) shifts. We use XGBoost to measure this.

ACS IncomeThe task is to predict whether an individual's income is above $50,000. We filter the dataset to only include individuals above the age of 16, usual working hours of at least 1 hour per week in the past year, and an income of at least $100.

* In our Setting 1\(\sim\)4, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from California, Connecticut, Massachusetts, and South Dakota respectively, and test the generalization performance on the other 50 American states.
* In our Setting 21\(\sim\)22, we sub-sample the dataset according to age to introduce covariate shift, where we focus on individuals from California and form two groups according to whether their age is \(\geq 25\). In Setting 21, the source data over-samples the low age group where 80% is drawn from the age \(\geq 25\) group, and the proportions are reversed in the target data (20% from age \(\geq 25\) group). In Setting 22, the source data over-samples the low age group where 90% is drawn from the age \(\geq 25\) group, and the proportions are reversed in the target data (10% from age \(\geq 25\) group).

ACS MobilityThe task is to predict whether an individual had the same residential address one year ago. We filter the dataset to only include individuals between the ages of 18 and 35, which increases the difficulty of the prediction task.

* In our Setting 5\(\sim\)8, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from Mississippi, New York, California, and Pennsylvania respectively, and test the generalization performance on the other 50 American states.

ACS Public CoverageThe task is to predict whether an individual has public health insurance. We focus on low-income individuals who are not eligible for Medicare by filtering the dataset to only include individuals under the age of 65 and with an income of less than $30,000.

* In our Setting 11\(\sim\)14, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from Nebraska, Florida, Texas, and Indiana respectively, and test the generalization performance on the other 50 American states.
* In our Setting 19\(\sim\)20, we consider the temporal shifts. We use data from 2010 in training and data from 2014, 2017, and 2021 in testing (3 test domains). In Setting 19, the training data come from New York, and in Setting 20, the training data come from California.

US AccidentThe task is to predict whether an accident is severe (long delay) or not (short delay) based on weather features and Road conditions features.

* In our Setting 15\(\sim\)18, the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from California, Florida, Texas, and Minnesota respectively, and test the generalization performance on other 13 American states. Here we only involve 13 test domains because the sample sizes in the other states are quite small.

TaxiThe task is to predict whether the total ride duration time exceeds 30 minutes, based on location and temporal features. We filter the data in 2017 and remove some extremely large or small features (e.g. samples with too long distances which can be easily classified).

* In our Setting 9\(\sim\)10, we use data from 2016 and the source/target domain refers to different cities. We train methods on data from Bogota and New York City and test the generalization performance in the other cities.

### Algorithms & Implementations

In our benchmark, we evaluate 22 algorithms that span a wide range of learning strategies on tabular data and compare their performances under different patterns of distribution shifts we construct. Concretely, these algorithms include:Base MethodsWe include some typical supervised learning methods for tabular data: Logistic Regression (LR), SVM, and fully-connected neural networks (MLP) with standard ERM optimization. For LR and SVM, we use the standard implementation in scikit-learn[71] and train them on CPUs. For MLP, we implement it via PyTorch[70] and train it on GPUs.

Tree Ensemble ModelsAs shown by Gardner _et al._[31], several tree-based methods achieve good performances on tabular datasets. And gradient-boosted trees (e.g., XGBoost, LightGBM) are widely considered as the state-of-the-art methods on tabular data. Therefore, we evaluate XGBoost and LightGBM in our benchmark. Also, we evaluate Random Forest (RF) to incorporate the performance of tree bagging methods.

DRO MethodsDistributionally Robust Optimization (DRO) methods are proposed to address the distribution shifts, which is the form of:

\[\min_{\theta\in\Theta}\sup_{Q\in\mathcal{P}(P_{tr})}\mathbb{E}_{Q}[\ell(f_{ \theta}(X),Y)],\] (D.1)

where \(\mathcal{P}(P_{tr})\) denotes the uncertainty set around the training distribution \(P_{tr}\). Following Gardner _et al._[31], we implement two typical variants of DRO, namely CVaR-DRO and \(\chi^{2}\)-DRO. CVaR-DRO is equivalent to Conditional Value at Risk (CVaR), and \(\chi^{2}\)-DRO uses \(\chi^{2}\)-divergence to regulate the uncertainty set. We use the fast implementation [53] in PyTorch[70]. We also consider the DORO[105], which discards a proportion \(\epsilon\) of the largest error points in each iteration to mitigate the outliers in DRO with two variants, CVaR-DORO and \(\chi^{2}\)-DORO. We also evaluate the Group DRO[79], which is proposed to minimize the worst-group loss and shows good generalization performances on many vision tasks. This method needs the group label and we define groups according to the "SEX" feature on ACS datasets. For US Accident and Taxi, we do not run Group DRO, since the current Group DRO model in the codebase only accepts the input with few groups while it is hard to define such group here. For all DRO methods, we use the MLP as the backbone model. We do not choose tree ensemble methods since tree ensemble methods are difficult to adapt to the distributionally robust case. Therefore, we leave their method developments and implementations as our future work.

Imbalanced Learning MethodsRecently, some simple data balancing methods [42] have shown good worst-group performances under distribution shifts. In our benchmark, we implement 4 typical balancing methods, namely Sub-Sampling \(Y\) (SUBY), Reweighting \(Y\) (RWY), Sub-Sampling Group (SUBG), and Reweighting Group (RWG). Besides, "Just Train Twice" (JTT [57]) exhibits good performances in many vision tasks, and therefore we also evaluate it in our tabular settings. Furthermore, stable learning methods [51, 24] propose to de-correlate sample covariates for an accurate estimation of causal relationships via global balancing, which could mitigate distribution shifts. In our benchmark, we implement one typical method named DWR [51]. For these imbalanced learning methods, we use XGBoost as the backbone model due to its superiority on tabular data and adjust sample weight or training procedure accordingly for each of the methods.

Fairness-enhancing MethodsFollowing Ding _et al._[25] and Gardner _et al._[31], fairness-enhancing methods have the potential to mitigate the performance degradation under distribution shifts. In our benchmark, we evaluate the in-processing and post-processing intervention methods. The in-processing method [4] minimizes the prediction error subject to some fairness constraints, and in our benchmark, we choose three typical fairness constraints, including demographic parity (DP), equal opportunity (EO), error parity (EP). And the post-processing method [37] randomizes the predictions of a fixed classifier to satisfy equalized odds criterion, and we use exponential and threshold controls in our benchmark. We use the implementations of aif360[11] and fairlearn[12].

### Parameter Search Space

We provide the hyperparameter grids in Table 4. We mainly use the hyperparameter grids proposed in [31], and we restrict the grid size of each method in each setting to 200 in consideration of computational costs. For each setting, we randomly pick 200 configurations for each algorithm for a fair comparison. For methods incorporating backbone models (e.g., MLP/XGBoost), we choose the top 10 best configurations for that backbone model to reduce the search space, making the searched best configuration represent its best performance more accurately. Moreover, to accelerate the grid search process, we utilize Ray [55] to run experiments in parallel.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Model** & **Total Grid Size** & **Hyperparameter** & **Value Range** \\ \hline \hline \multicolumn{4}{c}{**Base Methods**} \\ \hline \multirow{4}{*}{MLP} & \multirow{4}{*}{270\({}^{\circ}\)} & Learning Rate & \(\{0.001,0.003,0.0001,0.005,0.01\}\) \\  & & Batch Size & \(\{64,128,256\}\) \\  & & Hidden Units & \(\{16,32,64\}\) \\  & & Dropout Ratio & \(\{0,0.1\}\) \\  & & Train Epoch & \(\{50,100,200\}\) \\ \hline \multirow{4}{*}{SVM} & \multirow{4}{*}{96} & C & \(\{0.01,0.1,1.10,100,1000\}\) \\  & & Kernel & [linear, RBF] \\  & & Loss & Squared Hinge \\  & & \(\gamma\) & \(\{0.1,0.3,0.5,1.0,1.5,2.0,\text{scale, auto}\}\) \\ \hline Logistic Regression & 23 & \(L_{2}\) penalty & \(\{0.001,0.03,0.005,0.007,0.01,0.03,0.05,\dots\}\) \\  & & \(1.3,1.7,5,10,50,100,5e2,1e3,5e3,1e4\}\) \\ \hline \multicolumn{4}{c}{**Tree Ensemble Methods**} \\ \hline \multirow{4}{*}{Random Forest} & \multirow{4}{*}{640\({}^{\circ}\)} & Num. Estimators & \(\{32,64,128,256,512\}\) \\  & & Max Features & \(\{\text{sqrt},\text{log}2\}\) \\  & & Min. Samples Split & \(\{2,4,8,16\}\) \\  & & Min. Samples Leaf & \(\{1,2,4,8\}\) \\  & & Cost-Complexity \(\alpha\) & \(\{0.,0.001,0.01,0.1\}\) \\ \hline \multirow{4}{*}{XGBoost} & \multirow{4}{*}{1944\({}^{\circ}\)} & Learning Rate & \(\{0.1,0.3,1.0,2.0\}\) \\  & & Min. Split Loss & \(\{0.0,1.0,5\}\) \\  & & Max. Depth & \(\{4,6,8\}\) \\  & & Column Subsample Ratio (tree) & \(\{0.7,0.9,1\}\) \\  & & Column Subsample Ratio (level) & \(\{0.7,0.9,1\}\) \\  & & Max. Bins & \(\{128,256,512\}\) \\  & & Growth Policy & \{Depthwise, Loss Guide\} \\ \hline \multirow{4}{*}{LightGBM} & \multirow{4}{*}{1680\({}^{\circ}\)} & Learning Rate & \(\{0.01,0.1,0.5,1.1\}\) \\  & & Num. Estimators & \(\{64,128,256,512\}\) \\  & & \(L_{2}\)-reg. & \(\{0.,0.001,0.01,0.1,1.1\}\) \\  & & Min. Child Samples & \(\{1,2,4,8,16,32,64\}\) \\  & & Column Subsample Ratio (tree) & \(\{0.5,0.8,1.\}\) \\ \hline \multicolumn{4}{c}{**DRO Methods**} \\ \hline DRO \(\chi^{2}\) & 1890\({}^{\circ}\) & Uncertainty set size \(\alpha\) & \(\{0.01,0.1,0.2,0.3,0.4,0.5,0.6\}\) \\  & & Backbone Model & MLP \\ \hline DRO CVaR & 1890\({}^{\circ}\) & Uncertainty set size \(\alpha\) & \(\{0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6\}\) \\  & & Backbone Model & MLP \\ \hline \multirow{2}{*}{DRO \(\chi^{2}\)} & \multirow{2}{*}{8100\({}^{\circ}\)} & Uncertainty set size \(\alpha\) & \(\{0.001,0.01,0.1,0.2,0.3\}\) \\  & & Outlier proportion \(\epsilon\) & \(\{0.001,0.01,0.1,0.2,0.3\}\) \\  & & Backbone Model & MLP \\ \hline \multirow{2}{*}{DRO CvaR} & \multirow{2}{*}{8100\({}^{\circ}\)} & Uncertainty set size \(\alpha\) & \(\{0.1,0.2,0.3,0.4,0.5,0.6\}\) \\  & & Outlier proportion \(\epsilon\) & \(\{0.001,0.01,0.1,0.2,0.3\}\) \\ \hline \multirow{2}{*}{Group DRO} & \multirow{2}{*}{1080\({}^{\circ}\)} & Group weights step size & \(\{0.001,0.01,0.1,0.2\}\) \\  & & Backbone Model & MLP \\ \hline \multicolumn{4}{c}{**Imbalanced Learning Methods**} \\ \hline SUBY, RWY, SUBG, RWG & 1944\({}^{\circ}\) & Backbone Model & XGBoost \\ \hline JTT & \(11664^{\circ}\) & Up-weight \(\lambda\) & \(\{4,5,6,20,50,100\}\) \\  & & Backbone Model & XGBoost \\ \hline \multirow{2}{*}{DWR} & \multirow{2}{*}{48600\({}^{\circ}\)} & \(L_{1}\) penalty \(\lambda\) & \(\{1e-3,1e-2,1e-1,0.2,0.3\}\) \\  & & \(L_{2}\) penalty \(\lambda\) & \(\{1e-3,1e-2,1e-1,0.2,0.3\}\) \\  & & Backbone Model & XGBoost \\ \hline \multicolumn{4}{c}{**Fairness Methods**} \\ \hline In-processing & 1944\({}^{\circ}\) & Constraint Type & \{DP, EO, Error Parity\} \\  & & Backbone Model & XGBoost \\ \hline Post-processing & 1944\({}^{\circ}\) & Constraint Type & \{Exp, Threshold\} \\  & & Backbone Model & XGBoost \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameter grids used in all experiments. \(\diamond\): for methods with the total grid size above 200, we randomly sample _200_ configurations for fair comparisons. For methods incorporating backbone models (e.g., MLP/XGBoost), we choose top-10 best configurations for that backbone model to reduce the search space, making the searched best configuration represent its best performance more accurately.

### Training Details

In each setting, we randomly sample 20,000 samples from the source domain for training, 20,000 samples from the source domain for validation, and 20,000 samples from the target domain for testing. For US Accident and Taxi datasets, we only randomly sample 8,000 samples for the source/target domain due to fewer samples involved in each setting. For setting where the source domain has fewer samples, we use 80% samples for training and 20% for validation. For results not specified by the validation protocol, we selected the best configuration according to the performance on such validation set (\(i.i.d\) with training).

For settings with ACS Income, ACS Public Coverage, ACS Mobility and US Accident datasets, all experiments were run on a server using 48 cores from two AMD EPYC 7402 24-Core Processors. For settings with Taxi dataset, all experiments were run on a cluster using 24 cores from an Intel Xeon Gold 6126 Processor. Neural-network-based models (MLP, DRO methods) were trained on GPU, NVIDIA GeForce RTX 3090, the other methods were trained only on CPUs. Note that all experiments are small-scale and could be run efficiently. Besides, during training, we found that the bi-search in \(\chi^{2}\)-DRO sometimes failed to converge, and therefore we set the maximal iteration num to 2500.

### Detailed Results of 7 Selected Settings in Main Body

In the main body, due to space limitations, we only visualize the target performances of different methods. Here we provide the detailed results of all algorithms on the 7 selected settings in Table 1. We select the top-10 configurations according to the validation set \(i.i.d\) with training data and report the mean accuracy as well as the standard deviation in Table 5. Also, in Figure 11, we visualize the results of 22 algorithms of all configurations in our 7 selected settings.

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l l l l l l} \hline \hline
**Dataset** & \multicolumn{2}{l}{ACS Income} & \multicolumn{2}{l}{ACS Mobility} & \multicolumn{2}{l}{US Taxi} & \multicolumn{2}{l}{ACS Pub.Cov} & \multicolumn{2}{l}{US Accident} & \multicolumn{2}{l}{ACS Time} & \multicolumn{2}{l}{Sub-Sampling} \\
**Shift Pattern** & \multicolumn{2}{l}{\(Y|X\) dominates} & \multicolumn{2}{l}{\(Y|X\) dominates} & \multicolumn{2}{l}{\(Y|X\) dominates} & \multicolumn{2}{l}{\(Y|X\) more} & \multicolumn{2}{l}{\(Y|X\) more} & \multicolumn{2}{l}{\(Y\) dominates} \\ Source \(\rightarrow\) Target & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} & \multicolumn{2}{l}{External} \\  & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} & \multicolumn{2}{l}{_i.d._} \\ \hline \multirow{3}{*}{Base} & LR & 80.5\(\pm\)4.2 & 72.3\(\pm\)2.2 & 76.9\(\pm\)4.2 & **75.9\(\pm\)**2.6 & 83.6\(\pm\)0.3 & 73.7\(\pm\)0.2 & 83.2\(\pm\)4.6 & 66.3\(\pm\)1.4 & 82.3\(\pm\)1.7 & 73.3\(\pm\)1.6 & 66.2\(\pm\)1.4 & 92.4\(\pm\)1.8 & 81.5\(\pm\)0.0 \\ Methods & SVM & 90.1\(\pm\)0.9 & 67.0\(\pm\)0.9 & 78.6\(\pm\)**7.6 & **76.5\(\pm\)**0.0 & 83.1\(\pm\)0.4 & **74.5\(\pm\)**0.0 & 83.9\(\pm\)1.4 & 82.1\(\pm\)0.4 & 65.0\(\pm\)0.1 & 73.4\(\pm\)1.6 & 64.7\(\pm\)1.9 & 91.3\(\pm\)1.7 & 75.9\(\pm\)0.7 \\ MLP & 90.1\(\pm\)0.9 & 89.1\(\pm\)0.9 & 89.1\(\pm\)0.9 & 88.5\(\pm\)0.6 & 78.4\(\pm\)0.2 & 84.7\(\pm\)0.5 & 73.6\(\pm\)0.5 & 83.6\(\pm\)1.1 & 80.4\(\pm\)0.2 & 86.5\(\pm\)0.0 & 77.1\(\pm\)0.8 & 79.8\(\pm\)0.5 & 92.2\(\pm\)1.0 & **81.0\(\pm\)0.1** \\ \hline \multirow{3}{*}{Tree} & Random forest & 81.1\(\pm\)0.2 & 72.9\(\pm\)0.4 & 80.2\(\pm\)0.3 & 72.0\(\pm\)0.3 & 80.5\(\pm\)0.0 & 72.0\(\pm\)0.3 & 80.5\(\pm\)0.0 & 85.5\(\pm\)0.0 & 80.3\(\pm\)0.1 & 66.2\(\pm\)1.6 & **66.5\(\pm\)0.1** & 78.5\(\pm\)0.2 & 78.6\(\pm\)1.9 & 92.2\(\pm\)1.1 & 81.2\(\pm\)0.1 \\ Ensemble & LightGBM & 82.4\(\pm\)1.4 & 72.4\(\pm\)0.4 & 72.6\(\pm\)0.5 & 79.5\(\pm\)0.3 & 86.6\(\pm\)0.0 & 72.0\(\pm\)0.3 & 80.5\(\pm\)0.0 & 83.7\(\pm\)0.0 & 80.6\(\pm\)0.0 & 86.6\(\pm\)0.0 & 70.1\(\pm\)0.1 & **73.1\(\pm\)0.1** & **73.1\(\pm\)0.1** & **81.0\(\pm\)0.1** & **82.4\(\pm\)0.0** \\ Methods & XGBoost & 82.4\(\pm\)1.7 & 72.6\(\pm\)0.6 & 79.5\(\pm\)0.3 & 72.0\(\pm\)0.3 & 80.0\(\pm\)0.0 & 72.0\(\pm\)0.0 & 83.7\(\pm\)0.0 & 83.7\(\pm\)0.1 & 80.1\(\pm\)0.1 & 86.6\(\pm\)0.0 & 70.1\(\pm\)0.1 & 71.6\(\pm\)0.1 & 71.6\(\pm\)0.1 & 71.6\(\pm\)0.1 & 72.6\(\pm\)0.1 & 81.6\(\pm\)0.0 \\ \hline \multirow{3}{*}{DR0} & \(\chi^{2}\)-DRO & 80.7\(\pm\)0.2 & 72.2\(\pm\)0.2 & 76.3\(\pm\)0.3 & 76.3\(\pm\)0.3 & 78.4\(\pm\)0.2 & 80.4\(\pm\)0.4 & 84.0\(\pm\)0.4 & 73.0\(\pm\)0.1 & 80.6\(\pm\)0.6 & 69.1\(\pm\)0.7 & 83.3\(\pm\)0.4 & 66.0\(\pm\)0.0 & 70.5\(\pm\)0.1 & 64.6\(\pm\)0.0 & 70.9\(\pm\)0.1 & 71.6\(\pm\)0.1 & 71.6\(\pm\)0.1 & 72.9\(\pm\)0.1 & 81.6\(\pm\)0.1 \\ Methods & \(\chi^{2}\)-DRO0 & 80.9\(\pm\)0.2 & 71.7\(\pm\)0.1 & 76.4\(\pm\)0.3 & 76.4\(\pm\)0.3 & 83.6\(\pm\)0.4 & 85.4\(\pm\)0.4 & 73.0\(\pm\)0.0 & 80.6\(\pm\)0.0 & 83.7\(\pm\)0.0 & 80.4\(\pm\)0.0 & 83.5\(\pm\)0.0 & 72.1\(\pm\)0.1 & 64.5\(\pm\)0.1 & 72.9\(\pm\)0.1 & 81.6\(\pm\)0.1 \\ \cline{1-1} \cline{3-1} \cline{3-1} \cline{3-1} \cline{3-1} \cline{3-1} \cline{3-1} \cline{3-1}

AnalysisFrom the numbers of numerical results in that Table, we give a more detailed analysis corresponding with our main body.

* Different algorithms do not exhibit consistent rankings over different distribution shift patterns. In Table 5, we boldface the best target performance within each class of methods. And we show the top-3 classes at the bottom. The results show that the algorithmic rankings across different settings and tasks are quite different. And even the rankings of algorithms within the same class vary a lot. This further demonstrates the complexity of \(Y|X\) shifts. Also, Figure 10 shows that even for the fixed source state, algorithmic rankings vary a lot across different target states.
* Tree ensemble methods show competitive performances but do not significantly eliminate the generalization error between source and target data, characterized by the difference

Figure 11: Target vs. source accuracies for 22 algorithms and datasets in our benchmark. Each point represents one hyperparameter configuration. **(a)-(b)**: two examples of ACS Income dataset with California (CA) as the source state, and Puerto Rico (PR) and South Dakota (SD) as targets. **(c)-(g)**: five examples of ACS Mobility, Taxi, ACS Pub.Cov, US Accident datasets. **(h)**: simulated covariate shifts on on sub-sampled ACS Income dataset.

between _o.o.d._ and _i.i.d._ model performance. From the bottom, we could see that tree ensemble methods achieve top-3 performances in most of the selected settings (6 out of 7), exhibiting their superiority on tabular data. However, the performance degradation between source and target is still large.
* Imbalance methods and fairness methods show similar performance with the base learner (XGBoost). We find these methods do not have a significant improvement over the base learner (XGBoost). This may be due to that they are not designed to incorporate tree ensemble methods.

### Full Results of 22 Settings

In this section, we provide the full results of our 22 settings in Table 3.

#### d.6.1 Average Accuracy

In Table 6, Table 7 and Table 8, we report the source accuracy and the target accuracy of all algorithms. Since in these settings, we have multiple target domains, as for the target accuracy of each algorithm, we report the average accuracy as well as the standard deviation calculated on all target domains. Here the standard deviation reflects the stability of out-of-distribution generalization performances across different target domains different instead of random seeds.

From the average result over, Table 6, Table 7 and Table 8, we can still obtain similar findings compared to that from the 7 selected settings in the main body. Especially, while tree ensemble methods serve as a strong in-sample benchmark illustrated by their competitive _i.i.d._ performance, these methods do not yield better performance than other methods in the target distribution uniformly. And when we average across different target domains, DRO methods do not perform better than their empirical counterpart, indicating that the worst-case optimization from the uncertainty sets constructed by existing DRO methods can rarely occur in practical setups and does not help much compared with standard ERM cases. This tricky part of worst-case intervention also holds when we compare imbalanced learning and fairness methods with the XGB base learner. These methods can lead to better _o.o.d._ results than the base learner sometimes indeed, while this may not hold true when we average over different target domains. In general, existing methods cannot generalize well averaging over different target domains, and it would be interesting to develop methods to close that gap.

### Worst-Domain Accuracy

Another widely-used metric in evaluating generalization performance is worst-case accuracy. In settings built on ACS Income, ACS Pub.Cov, ACS Mobility, and US Accident, for the target accuracy of each algorithm, we report the worst target domain accuracy in Table 9 and Table 10. For other settings, we do not report the worst-case accuracy because there are not many target domains, and the average& standard deviation results could reflect the population generalization performance well. In terms of the worst target accuracy for each setting, we usually observe a large performance degradation (usually over a 10 percent performance drop) in all of the existing methods. And algorithms do not show a consistent and stable ranking performance in the _o.o.d._ performance. DRO / Imbalanced learning methods can perform quite well in one _o.o.d._ setup but are dominated by other methods in some other cases. This demonstrates a need to carefully understand the difference between different domains first and develop corresponding methods then.

### Average Macro-F1 Score

In consideration of the label imbalance in real-world tabular datasets, we also calculate the Macro-F1 Score for all algorithms under our settings. In this section, we report the average results in Table 11, Table 12, and Table 13. Here the standard deviation reflects the stability of out-of-distribution generalization performances (not the randomness).

### Worst-Domain Macro-F1 Score

In settings built on ACS Income, ACS Pub.Cov, ACS Mobility, and US Accident, for the target accuracy of each algorithm, we report the worst target domain Macro-F1 score in Table 14 and 

[MISSING_PAGE_FAIL:33]

\begin{table}
\begin{tabular}{l l l c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Dataset} & \multicolumn{6}{c}{Taxi} & \multicolumn{6}{c}{ACS} & \multicolumn{6}{c}{Pub.Cov (Temporal)} & \multicolumn{6}{c}{ACS} & \multicolumn{6}{c}{Income (Synthetic)} \\ \cline{3-14} \multicolumn{1}{c}{Source} & & \multicolumn{3}{c}{Bog} & \multicolumn{3}{c}{WFC} & \multicolumn{3}{c}{NY} & \multicolumn{3}{c}{CA} & \multicolumn{3}{c}{20\%} & \multicolumn{3}{c}{10\%} \\ \cline{3-14} \multicolumn{1}{c}{} & \multicolumn{3}{c}{_i.d._} & \multicolumn{3}{c}{_o.d._} & \multicolumn{3}{c}{_i.d._} & \multicolumn{3}{c}{_o.d._} & \multicolumn{3}{c}{_i.d._} & \multicolumn{3}{c}{_o.d._} & \multicolumn{3}{c}{_i.d._} & \multicolumn{3}{c}{_o.d._} & \multicolumn{3}{c}{_i.d._} & \multicolumn{3}{c}{_o.d._} \\ \hline \multirow{3}{*}{Base} & LR & 75.6 & 71.2 \(\pm\) & 23.0 & 87.0 & 70.5 & 80.9 & 71.1 & 80.1 & 69.4 & 81.3 & 49.6 & 84.1 & 49.0 & 84.3 & 0.52 & 78.6 & 54.7 \\  & SVM & 79.4 & 67.1 & 78.6 & 61.6 & 80.1 & 62.2 & 79.3 & 64.6 & 80.5 & 53.0 & 84.2 & 50.3 & 84.8 & 50.4 & 71.1 & 57.4 \\  & MLP & 81.8 & 68.3 & 81.4 & 70.8 & 81.7 & 71.3 & 80.5 & 70.5 & 83.3 & 54.3 & 58.4 & 53.9 & 54.5 & 52.9 & 78.8 & 56.7 \\ \hline \multirow{3}{*}{Tree} & Random Forest & 81.7 & 72.6 & 81.2 & 71.6 & 81.5 & 71.2 & 80.2 & 68.3 & 84.6 & 54.2 & 86.5 & 52.8 & 86.7 & 52.9 & 81.2 & 59.0 \\  & LightGBM & 81.8 & 71.1 & 82.1 & 69.5 & 82.3 & 70.1 & 80.2 & 69.6 & 84.6 & 56.9 & 86.9 & 53.9 & 86.9 & 52.3 & 81.2 & 60.4 \\  & XGBoat & 82.0 & 72.2 & 81.9 & 70.1 & 82.2 & 70.9 & 80.5 & 70.5 & 84.6 & 54.8 & 86.7 & 53.7 & 87.3 & 55.4 & 81.2 & 59.6 \\ \hline \multirow{3}{*}{DRO} & \(\chi^{2}\)-DRO & 81.8 & 69.7 & 81.2 & 69.7 & 81.6 & 67.9 & 80.2 & 70.3 & 82.9 & 52.5 & 85.4 & 50.5 & 83.4 & 52.9 & 78.4 & 62.4 \\  & CvB-DRO & 81.4 & 72.1 & 81.2 & 70.7 & 82.0 & 70.0 & 79.7 & 88.4 & 82.6 & 52.2 & 85.2 & 51.7 & 83.3 & 52.3 & 77.8 & 53.9 \\  & \(\chi^{2}\)-DRO & 81.1 & 70.3 & 80.4 & 62.6 & 81.2 & 63.8 & 78.8 & 69.5 & 78.8 & 43.6 & 82.1 & 43.6 & 82.2 & 43.6 & 72.4 & 49.9 \\  & CvB-DRO & 80.9 & 68.7 & 80.5 & 64.9 & 81.1 & 64.3 & 79.6 & 69.1 & 80.7 & 49.5 & 84.4 & 52.4 & 81.9 & 43.6 & 76.4 & 56.2 \\  & Group DRO & 81.9 & 72.6 & 81.3 & 71.8 & 81.7 & 69.2 & 79.8 & 0.8 & 68.3 & 53.2 & 85.4 & 53.2 & 83.5 & 51.3 & 72.2 & 43.6 \\ \hline \multirow{3}{*}{Imbalanced} & SUBY & 81.0 & 65.2 & 81.5 & 68.0 & 82.0 & 66.3 & 74.8 & 72.5 & 79.8 & 51.3 & 82.3 & 45.4 & 77.8 & 64.5 & 77.8 & 68.3 \\  & RWY & 81.5 & 65.2 & 81.8 & 68.3 & 82.1 & 67.6 & 77.7 & 73.2 & 80.7 & 53.3 & 82.5 & 45.6 & 83.7 & 57.1 & 78.7 & 65.8 \\ \cline{1-1}  & SUBG & 81.9 & 71.4 & 81.6 & 70.1 & 82.0 & 68.4 & 80.2 & 70.3 & 84.3 & 56.1 & 86.7 & 53.0 & 86.3 & 53.6 & 80.8 & 60.2 \\ \cline{1-1}  & RWG & 82.0 & 71.3 & 81.9 & 70.6 & 82.2 & 70.3 & 80.2 & 70.6 & 84.5 & 56.8 & 86.5 & 52.8 & 86.5 & 52.8 & 80.9 & 61.5 \\ \cline{1-1}  & TIT & 83.8 & 68.4 & 78.1 & 71.2 & 79.1 & 71.6 & 77.9 & 68.6 & 80.6 & 59.4 & 83.1 & 56.2 & 84.7 & 55.4 & 77.6 & 56.7 \\ \cline{1-1}  & DWR & 81.9 & 71.2 & 81.6 & 71.0 & 82.1 & 69.0 & 80.1 & 72.0 & 84.3 & 58.1 & 86.8 & 54.2 & 85.9 & 52.8 & 80.6 & 59.4 \\ \hline \multirow{3}{*}{Fairness} & DP & 81.3 & 70.3 & 81.0 & 68.8 & 81.8 & 70.7 & 79.3 & 68.4 & 84.3 & 57.4 & 86.6 & 53.7 & 86.4 & 52.9 & 80.6 & 57.8 \\ \cline{1-1}  & EO & 81.8 & 70.6 & 81.8 & 70.5 & 82.1 & 70.8 & 80.0 & 70.6 & 84.4 & 55.4 & 86.8 & 53.6 & 86.4 & 53.6 & 80.8 & 61.6 \\ \cline{1-1}  & Ep & 81.8 & 70.7 & 81.9 & 69.8 & 82.1 & 70.9 & 80.3 & 69.2 & 84.4 & 55.5 & 86.4 & 54.6 & 86.4 & 53.6 & 80.9 & 60.1 \\ \cline{1-1}  & Exp & 81.4 & 70.6 & 81.0 & 69.4 & 81.8 & 70.4 & 79.1 & 68.8 & 84.5 & 55.4 & 86.8 & 52.8 & 86.4 & 52.5 & 80.9 & 61.5 \\ \cline{1-1}  & Threshold & 77.9 & 71.8 & 77.0 & 69.8 & 77.5 & 70.9 & 78.0 & 62.8 & 84.3 & 57.3 & 86.2 & 52.4 & 85.8 & 52.9 & 80.0 & 59.9 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Average target domain accuracy** of all algorithms on the Taxi, ACS Pub.Cov (Temporal) and ACS Income (Synthetic) datasets. In Taxi and ACS Pub.Cov (Temporal), we test all algorithms on the 3 target domains and report the mean and standard deviation across all 3 target domains. The standard deviation here reflects the stability of performances across different target states. In ACS Income (Synthetic), we simulate strong covariate shifts according to the “Age” feature, where 10% and 20% means the minor group ratio in training, respectively.

\begin{table}
\begin{tabular}{l l l c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Dataset} & \multicolumn{6}{c}{ACS} & \multicolumn{6}{c}{INCOME} & \multicolumn{6}{c}{ACS

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{13}{l}{Dataset} & \multicolumn{13}{c}{ACS MOBILITY} & \multicolumn{13}{c}{US ACCIDENT} \\ \multicolumn{13}{l}{Source State} & \multicolumn{1}{c}{NY} & \multicolumn{1}{c}{CA} & \multicolumn{1}{c}{NS} & \multicolumn{1}{c}{PA} & \multicolumn{1}{c}{CA} & \multicolumn{1}{c}{FL} & \multicolumn{1}{c}{TX} & \multicolumn{1}{c}{IM} \\ \cline{3-13} \multicolumn{13}{c}{} & \multicolumn{1}{c}{_i.d._} & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0.d_ \\ \hline \multirow{2}{*}{Base Methods} & LR & 78.6 & 64.7 & 77.1 & 64.6 & 77.3 & 64.8 & 75.4 & 66.3 & 83.2 & 44.9 & 90.7 & 48.3 & 95.1 & 57.4 & 86.4 & 52.8 \\  & SVM & 79.8 & 64.6 & 77.7 & 66.8 & 79.4 & 65.3 & 77.1 & 64.7 & 83.2 & 49.2 & 88.7 & 58.3 & 93.7 & 55.7 & 88.6 & 56.1 \\  & MLP & 78.7 & 64.7 & 77.1 & 64.8 & 78.1 & 66.3 & 75.9 & 67.0 & 84.7 & 58.4 & 92.7 & 61.1 & 94.9 & 57.2 & 88.9 & 57.1 \\ \hline Tree & Random Forest & 80.4 & 68.0 & 79.0 & 68.2 & 80.6 & 67.5 & 78.6 & 68.7 & 86.6 & 66.6 & 93.5 & 58.2 & 95.2 & 58.5 & 91.6 & 68.0 \\ Ensemble & LightGBM & 80.6 & 68.7 & 78.7 & 69.0 & 80.0 & 67.9 & 78.6 & 69.8 & 87.1 & 65.3 & 93.3 & 59.0 & 95.4 & 58.6 & 92.2 & 68.1 \\ Methods & XGBost & 80.4 & 68.6 & 79.0 & 69.0 & 80.9 & 68.3 & 79.0 & 69.4 & 87.1 & 65.6 & 94.6 & 59.0 & 95.3 & 58.8 & 92.5 & 68.5 \\ \hline \multirow{2}{*}{DRO} & \(\chi^{2}\)-DRO & 78.5 & 65.1 & 77.5 & 66.6 & 76.8 & 65.4 & 76.0 & 67.1 & 84.4 & 62.9 & 92.6 & 53.3 & 95.0 & 57.9 & 89.0 & 53.5 \\  & CVAR-DRO & 78.6 & 65.0 & 77.2 & 65.5 & 77.7 & 64.4 & 75.7 & 67.6 & 84.7 & 57.2 & 92.7 & 57.5 & 95.0 & 58.4 & 89.5 & 56.3 \\  & \(\chi^{2}\)-DRO & 78.6 & 64.3 & 77.8 & 66.9 & 77.1 & 64.7 & 75.3 & 64.7 & 84.7 & 53.9 & 92.3 & 61.1 & 94.3 & 57.0 & 87.6 & 59.8 \\ Methods & CVAR-DRO & 78.7 & 64.7 & 78.0 & 67.4 & 77.4 & 74.7 & 75.3 & 64.9 & 84.9 & 57.8 & 92.8 & 58.1 & 94.9 & 57.5 & 89.1 & 57.6 \\  & Group DRD & 78.6 & 65.5 & 77.1 & 65.1 & 76.0 & 64.5 & 76.0 & 66.8 & N/A & N/A & N/A & N/A & N/A & N/A \\ \hline \multirow{4}{*}{Imbalanced Learning Methods} & SUBY & 78.2 & 65.0 & 76.3 & 64.6 & 76.6 & 64.7 & 74.7 & 65.0 & 86.5 & 64.2 & 93.7 & 58.8 & 95.2 & 58.5 & 91.8 & 65.6 \\  & RWY & 78.5 & 64.7 & 76.7 & 65.0 & 76.5 & 65.6 & 74.5 & 64.6 & 86.9 & 64.8 & 93.4 & 58.3 & 95.3 & 58.5 & 92.5 & 67.5 \\  & SUBG & 80.2 & 68.8 & 79.2 & 68.9 & 79.5 & 67.0 & 78.6 & 69.2 & 86.3 & 66.4 & 93.0 & 58.1 & 95.1 & 58.4 & 91.1 & 67.4 \\  & RWG & 80.3 & 68.4 & 79.1 & 69.4 & 80.1 & 68.3 & 78.7 & 69.4 & 87.5 & 65.4 & 93.4 & 59.4 & 95.2 & 58.4 & 92.4 & 68.6 \\  & JTT & 78.3 & 64.7 & 76.5 & 64.7 & 77.6 & 66.6 & 75.1 & 65.6 & 86.5 & 65.9 & 92.7 & 58.6 & 94.9 & 58.6 & 91.8 & 68.2 \\  & DWR & 79.7 & 67.4 & 78.3 & 68.5 & 79.1 & 67.4 & 77.5 & 69.0 & 87.2 & 65.8 & 93.4 & 58.1 & 95.3 & 58.5 & 92.3 & 67.6 \\ \hline \multirow{4}{*}{Fairness Methods} & DP & 80.5 & 69.2 & 78.8 & 68.4 & 80.0 & 68.2 & 78.3 & 68.7 & 86.6 & 65.2 & 92.8 & 59.0 & 94.8 & 58.4 & 91.9 & 66.6 \\  & EO & 80.3 & 69.3 & 78.9 & 68.9 & 79.8 & 68.8 & 78.5 & 68.5 & 86.6 & 65.6 & 93.2 & 58.8 & 95.2 & 58.5 & 92.2 & 68.1 \\  & EP & 80.3 & 68.7 & 78.9 & 68.5 & 80.1 & 68.1 & 78.4 & 68.5 & 87.0 & 65.6 & 93.3 & 59.0 & 95.1 & 58.6 & 91.9 & 67.9 \\  & Exp & 80.5 & 69.0 & 78.9 & 68.6 & 81.0 & 68.8 & 78.4 & 69.0 & 85.9 & 65.4 & 90.1 & 58.7 & 93.1 & 56.5 & 86.3 & 64.7 \\  & Threshold & 80.5 & 69.0 & 78.6 & 69.2 & 78.2 & 66.7 & 78.5 & 68.7 & 84.3 & 64.5 & 88.1 & 59.5 & 91.2 & 55.7 & 82.0 & 64.0 \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Worst target domain accuracy** of all algorithms on the ACS Mobility and US Accident datasets. In ACS Mobility, we test all algorithms on the 50 American states and report the worst accuracy among all 50 target states. And in US Accident, we choose 13 American states with relatively high sample sizes in testing.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c c c c} \hline \hline \multicolumn{13}{l}{Dataset} & \multicolumn{13}{c}{ACS MOBILITY} & \multicolumn{13}{c}{IK3 Prob.Cor} \\ \multicolumn{13}{l}{Source State} & \multicolumn{1}{c}{CA} & \multicolumn{1}{c}{TN} & \multicolumn{1}{c}{IN} & \multicolumn{1}{c}{FL} & \multicolumn{1}{c}{TX} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{IM} \\ \cline{3-13} \multicolumn{13}{c}{} & \multicolumn{1}{c}{_i.d._} & _0.0.d_ & _i.d._ & _0.0.d_ & _i.d._ & _0.0d_ & _i.d._ & _0.0d_ & _i.d._ & _0.0d_ & _i.d.

[MISSING_PAGE_FAIL:36]

Table 15. For other settings, we do not report the worst-case results because there are not many target domains there and the mean/standard deviation results could reflect the generalization performance well.

For the results of Macro-f1 Score reported in Tables 11, 12, 13, 14 and 15, we find similar conclusion patterns to the accuracy score, while we usually witness a wider gap between _o.o.d._ and _i.i.d._ model performance.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{l}{Dataset} & \multicolumn{1}{c}{ACS} & INCOME & \multicolumn{1}{c}{ACS} & \multicolumn{1}{c}{FUB.} & \multicolumn{1}{c}{COV} & \multicolumn{1}{c}{\multirow{2}{*}{TS}} \\ \multicolumn{1}{c}{Source State} & \multicolumn{1}{c}{CA} & \multicolumn{1}{c}{CT} & \multicolumn{1}{c}{MN} & \multicolumn{1}{c}{SD} & \multicolumn{1}{c}{FL} & \multicolumn{1}{c}{TT} & \multicolumn{1}{c}{NE} & \multicolumn{1}{c}{TS} \\ \cline{3-14} \multicolumn{1}{c}{Source State} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} & \multicolumn{1}{c}{_i.d._} & \multicolumn{1}{c}{_o.d._} \\ \hline \multirow{3}{*}{Base Methods} & LR & 0.451 & 0.397 & 0.462 & 0.399 & 0.531 & 0.450 & 0.527 & 0.449 & 0.800 & 0.437 & 0.898 & 0.489 & 0.936 & 0.573 & 0.855 & 0.528 \\  & SVM & 0.604 & 0.544 & 0.592 & 0.527 & 0.635 & 0.489 & 0.623 & 0.559 & 0.793 & 0.505 & 0.891

### Agreement and Maintenance Plan

Hosting Platform.We will use Github as the hosting platform of our code. We provide detailed preprocessing Python scripts to guide users to replicate and process data from scratch. We also illustrate methodologies to run full experiment results in the code. We also list the license for each dataset and user guidance in the Readme file in that Github.

Dependencies.The benchmark is built upon Python 3.8+ and depends on PyTorch, aif360, fairlearn, xgboost, lightgbm. Besides, it uses numpy, scipy, and pandas for basic data manipulation.

Maintenance Plan.The datasets provided here will be maintained by the authors of the paper, which can be contacted by raising an issue on GitHub or by contacting the first authors directly. Our benchmark and a simple open-sourced Python package based on that may be updated at the discretion of authors in the future, which includes more refined algorithm implementations, datasets, and framework with improved efficiency.

Author Statements.To the best of our knowledge, the proposed benchmark is based on existing datasets and does not violate any existing licenses non contain personally identifiable or privacy-related information. And we claim all the responsibility in case of a violation of rights if such a violation were to exist.