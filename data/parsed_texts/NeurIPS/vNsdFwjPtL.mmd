# Suggesting Variable Order for Cylindrical Algebraic Decomposition via Reinforcement Learning

Fuqi Jia

Yuhang Dong

Minghao Liu

Pei Huang

Stanford University, Stanford, USA {jiafq,liumh,maff,zj}@ios.ac.cn, dongyuhang22@mails.ucas.ac.cn, huangpei@stanford.edu

Feifei Ma

Jian Zhang

These authors contributed equally.State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China Laboratory of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences, Beijing, China University of Chinese Academy of Sciences, Beijing, China Stanford University, Stanford, USA {jiafq,liumh,maff,zj}@ios.ac.cn, dongyuhang22@mails.ucas.ac.cn, huangpei@stanford.edu

###### Abstract

Cylindrical Algebraic Decomposition (CAD) is one of the pillar algorithms of symbolic computation, and its worst-case complexity is double exponential to the number of variables. Researchers found that variable order dramatically affects efficiency and proposed various heuristics. The existing learning-based methods are all supervised learning methods that cannot cope with diverse polynomial sets. This paper proposes two Reinforcement Learning (RL) approaches combined with Graph Neural Networks (GNN) for Suggesting Variable Order (SVO). One is GRL-SVO(UP), a branching heuristic integrated with CAD. The other is GRL-SVO(NUP), a fast heuristic providing a total order directly. We generate a random dataset and collect a real-world dataset from SMT-LIB. The experiments show that our approaches outperform state-of-the-art learning-based heuristics and are competitive with the best expert-based heuristics. Interestingly, our models show a strong generalization ability, working well on various datasets even if they are only trained on a 3-var random dataset. The source code and data are available at https://github.com/dongyuhang22/GRL-SVO.

## 1 Introduction

As learned in school, we know how to answer the question of whether a quadratic equation for \(x\) has a real root. For example,

\[x^{2}+bx+c=0,\]

where \(b,c\) are unknowns. We can answer it by checking whether the discriminant is non-negative, i.e., \(b^{2}-4c\geq 0\). What if the number and degree of variables increase, and the formula involves the combination of the universal quantifier (\(\forall\)), existential quantifier (\(\exists\)), and logical operators (and(\(\wedge\)), or(\(\vee\)), not(\(\neg\)))? Checking whether polynomials satisfy some mathematical constraints is a difficult problem and has puzzled mathematicians since ancient times. Until the 1930s, Alfred Tarski [1] answered the question by proving that the theory of real closed fields admits the elimination of quantifiers and gives a quantifier elimination procedure. Unfortunately, the procedure was impractical due to its non-elementary complexity. In 1975, George Collins discovered the first relatively efficient algorithm, Cylindrical Algebraic Decomposition (CAD) [2]. Currently, CAD (and variants) hasbecome one of the most fundamental algorithms in symbolic computation and is widely used in computational geometry [3], robot motion planning [4], constraint programming [5; 6; 7].

More precisely, CAD is an algorithm that eliminates (technically called _project_) variables one by one and finally results in a list of regions that are sign-invariant to the polynomials (technically called _cells_). CAD provides an efficient quantifier elimination in real space, thereby enabling the solution of various problems related to polynomials. For example, the space of the discriminant of the quadratic equation (i.e., \(b^{2}-4c\geq 0\)) can be the combination of satisfied cells. We provide a detailed description in Appendix A. Due to its powerful analytical ability and great versatility, it is also accompanied by huge limitations. The theoretical worst complexity is double exponential to the number of variables [2].

Researchers have conducted in-depth studies on improving its efficiency. According to theoretical and practical research, there lives a very important conclusion that "variable order in CAD can be crucially important" [8; 9; 10; 11; 12; 13; 14; 15; 16]. The selection of variable orders has a great effect on the time, memory usage as well as the number of cells in CAD. As an example, [8] introduces a category of problems where one variable order leads to a result with double exponential complexity to the number of variables, while another order yields a constant-sized result.

In this paper, we present a Graph-based Reinforcement Learning for Suggesting Variable Order (GRL-SVO) approach for CAD. It has two variants: GRL-SVO(UP) (i.e., utilizing _project_) and GRL-SVO(NUP) (i.e., not utilizing _project_). GRL-SVO(UP) is integrated into the CAD and can select the "best" next projecting variable. Considering the high cost of interacting with the symbolic computation tool, we also propose a fast approach GRL-SVO(NUP), which will simulate the state transition (i.e., _project_) via two rules (update rule and delete rule). It can report a total variable order before the CAD process. To evaluate the effectiveness of the models, we conduct a dataset of random polynomial sets with 3 to 9 variables and collected instances from SMTLIB [13; 17] to form a real-world dataset. Experimental results show that our approaches outperform state-of-the-art learning-based heuristics and are competitive with the best expert-based heuristics. GRL-SVO also exhibits a strong generalization capability. The models are only trained on a 3-var random dataset, but they still work well on other datasets.

## 2 Background and Related Work

In this section, we briefly introduce some basic definitions of CAD [2; 18]. We classify the previous works of SVO and give an overview of the techniques used.

### Cylindrical Algebraic Decomposition (CAD)

A Cylindrical Algebraic Decomposition (CAD) is a decomposition algorithm of a set of polynomials in ordered \(\mathbb{R}^{n}\) space resulting in finite sign-invariant regions, named _cells_. As shown in Figure 0(a), there are 3 cells with different colors (two infinite regions and the curve), and any points in the cell lead to the same sign of \(y-x^{2}\). Let \(\mathbb{R}[\bm{x}]\) be the ring of polynomials in the variable vector \(\bm{x}=[x_{1},\cdots,x_{n}]\) with coefficients in \(\mathbb{R}\)[9].

Figure 1: Examples of CAD. Figure 0(a) shows cells of \(\{y-x^{2}\}\). Figure 0(b) and 0(c) are CAD with different variable orders on \(\{x^{3}y+4x^{2}+xy,-x^{2}+2xy-1\}\).

**Definition 1** (Cell).: _For any finite set \(Q\subseteq\mathbb{R}[\bm{x}]\), a cell of \(Q\) is defined as a maximally connected set in \(\mathbb{R}^{n}\) where the sign of every polynomial in \(Q\) is constant._

CAD accepts a set of polynomials and a fixed variable order and mainly consists of three running phases: _project, root isolate_, and _lift_. The _project_ phase eliminates a variable of a polynomial set once at a time. It will result in a new _projected_ polynomial set that carries enough information to ensure possible decomposition. After repeating calls to _project_, CAD constitutes a step-like (from n-1 variables to 1 variable) set of _projected_ polynomials. The _root isolate_ procedure isolates all roots of the univariate polynomial set, and the roots split \(\mathbb{R}\) into some segmentations. The _lift_ phase samples in the segmentations and assigns the sampled value to the former _projected_ polynomial set so that the former polynomial set will become a univariate polynomial set. After repeating _root isolate_ and _lift_\(n-1\) times, CAD reconstructs the entire space via a set of cells characterized by the sample points. Since the CAD process and the _project_ operators are not prerequisites to understand our approach, we arrange more details in Appendix A. Here, we exemplify the process and the effect of different variable orders.

**Example 2.1**.: _Consider a polynomial set \(\{x^{3}y+4x^{2}+xy,-x^{2}+2xy-1\}\) as in Figure 0(b) and 0(c)._

_CAD process._ _Assume that the variable order is_ \(x\prec y\)_. CAD eliminates_ \(y\) _first and results in a polynomial set_ \(\{x,x^{2}+1,x^{4}+10x^{2}+1\}\) _(i.e., project phase). The polynomial set has only one root_ \(x=0\) _(i.e., root isolation phase). Then_ \(\mathbb{R}\) _will be split into three segmentations:_ \(\{x<0,x=0,x>0\}\)_. We sample_ \(\{x=-5,x=0,x=5\}\) _and result in three different polynomial sets:_ \(\{-130y+100,-10y-26\}\)_,_ \(\{-1\}\)_, and_ \(\{130y+100,10y-26\}\) _(i.e., lift phase). Let's take the first polynomial set as an example, and it has two roots, i.e.,_ \(\{\frac{10}{13},-\frac{13}{5}\}\) _(i.e., root isolation phase). Then_ \(\mathbb{R}\) _will be split into five segmentations:_ \(\{y<-\frac{13}{5},y=-\frac{13}{5},-\frac{13}{5}<y<\frac{10}{13},y=\frac{10}{13},y>\frac{10}{13}\}\)_. As shown in Figure 0(b), the sample red point_ \((-5,4)\) _can represent a sign-invariant region, the whole shaded area (i.e.,_ \(x^{3}y+4x^{2}+xy<0\land-x^{2}+2xy-1<0\land x<0\)_)._

_Effect of different variable orders._ _Figure 0(b) first eliminates_ \(y\) _then_ \(x\) _and results in 13 cells, and Figure 0(c) first eliminates_ \(x\) _then_ \(y\) _and results in 89 cells, almost seven times that of the former._

### Suggesting Variable Order for Cylindrical Algebraic Decomposition

An **Expert-Based (EB)** heuristic is a sequence of meticulous mechanized rules. It is mainly derived from theoretical analysis or a large number of observations on practical instances and summarized by experts. The heuristics can capture the human-readable characteristics of the problem. A **Learning-Based (LB)** heuristic will suggest an order through the scoring function or a variable selection distribution given by the learning model. It can exploit features deep inside the problem statement via high-dimensional abstraction.

Another important indicator is whether invoking _project_, as the _project_ phases are time-consuming for SVO heuristics in practice. In the following, **UP** denotes heuristics utilizing _project_, and **NUP** denotes heuristics not utilizing _project_.

**EB & UP.** The heuristics _sotd_ (sum of total degree) [10], and _ndrr_ (number of distinct real roots) [12] will project utilizing all different variable orders until the polynomial sets with only one variable. Then _sotd_ will select the order with the smallest sum of total degrees for each monomial, while _ndrr_ will select the order with the smallest number of distinct real roots. Because of the combinatorial explosion of orders, the heuristics projecting all orders only work on the case with a small number of variables. Based on CAD complexity analysis, _gmods_[13] selects the variable with the lowest degree sum in the polynomial set after each _project_ phase.

**EB & NUP.** The heuristics _brown_[9], and _triangular_[11] introduced a series of rules about statistical features like degree, total degree, and occurrence to distinguish the importance of variables. The heuristic _chord_[14] also provides an efficient algorithm based on the associated graph. It makes a variable order via _perfect elimination ordering_ on the graph. Note that chord heuristic only works on the _chordal_ graph. It is a special case that, after each _project_ phase, the graph only removes the linked edges of the projected variable without changing the other components.

**LB & UP.** To the best of our knowledge, no heuristic should be classified into this category.

**LB & NUP.** The approach _EMLP_[15] utilizes an MLP neural network. The network takes the selected statistics of the polynomial set as input and outputs a label for variable order directly. If thepolynomial set has 3 variables, then \(3!=6\) output labels are necessary for the neural network. The approach _PVO_[16] combines neural network and EB & NUP heuristics like _brown_ and _triangular_. The neural network is trained to predict the best first variable while the EB & NUP heuristics decide other parts of orders. These kinds of heuristics work on 6 variables at most in their experiments.

According to the classification, our proposed approaches, GRL-SVO(UP) and GRL-SVO(NUP), can be categorized as LB & UP and LB & NUP, respectively.

### Graph Neural Network and Reinforcement Learning

Graph Neural Networks (GNNs) are a class of deep learning models for graph-structured data. GNNs include many variants according to the characteristics of the problems, such as GCN [19], GAT [20], superGAT [21], and so on. Reinforcement Learning (RL) is an advanced machine learning paradigm where an agent learns to make decisions by interacting with its environment. It includes various frameworks, such as REINFORCE [22], DQN [23], and so on. By leveraging the expressive power of GNNs to learn complex graph structures and the adaptability of Reinforcement Learning (RL) to find optimal decision-making policies, researchers have remarkably succeeded in combinatorial algorithm design [24; 25; 26]. GRL-SVO is based on an Advantage Actor-Critic (A2C) framework [24; 27] with a Graph Network [28].

## 3 Method

This section starts with the problem formulation for the framework, followed by an overview and description of the graph representation and architecture. Finally, we introduce the state transition without _project_, which is the key technique for GRL-SVO(NUP).

### Problem Formulation

We now give the formulation of SVO for CAD. Our goal is to improve CAD efficiency by suggesting a better variable order. Computation time and memory usage are important indicators, but they will be affected by random factors, such as CPU clock, usage rate, etc. As the main output of CAD, cells can be the best candidate. In order to measure the quality of the result, the number of cells is an appropriate indicator that intuitively shows the effect of CAD [29]. In theory, a large number of cells means that the partitions are fragmented compared to a small number of cells. Usually, the polynomial set generated from _project_ phase is complex and difficult for the next phases of CAD. In practice, the number of cells also strongly correlates to the computation time and memory usage. Figures of the relation are listed in Appendix B, and we found that the computation time and memory usage increase when the number of cells increases. The objective is to minimize the number of cells \(N(Q,\sigma)\), where \(Q\subseteq\mathbb{R}[\bm{x}]\) is a polynomial set with coefficients in \(\mathbb{R}\) and \(\sigma\) is the given variable order, i.e., \(\min N(Q,\sigma)\).

By analyzing the input polynomial set \(Q\), we can derive the variable order so that we ought to minimize the objective:

\[\min N(Q,\sigma(Q)).\]

The difficulties of this framework mainly come from two aspects:

* Huge input space. The expression of a polynomial is compressed, and any slight change in form (such as power) will change the geometric space drastically. The EB heuristics may become inefficient when encountering characteristics beyond the summarized patterns.
* Huge output space. The number of variable orders and the number of variables have a factorial relationship, i.e., \(n\) variables resulting in \(n!\) different variable orders. For example, 10 variables lead to \(3628800\) candidate variable orders. _sotd_-like, _ndrr_-like, and _EMLP_-like heuristics become impractical due to the vast number of candidate variable orders.

### GRL-SVO Overview

In this paper, considering the challenges mentioned above, we propose the GRL-SVO approach, and Figure 2 shows the overall architectures. For huge input space, we compress the polynomial information into an associated graph [30] with embeddings, which is simple and can depict the relationship between variables. For huge output space, we utilize the neural network to predict the next best variable, and by repeating until no variables are left, the trajectory corresponds to a variable order. In detail, the actor neural network provides a distribution of actions, i.e., the choice of variables. The critic neural network scores the total order and stabilizes the training process as a state-dependent baseline. GNN encodes each variable of the current state as a high-dimensional embedding, and additional neural network components transform them into our policy. As for state transformation, GRL-SVO(UP) and GRL-SVO(NUP) are different in utilizing _project_. The environment of GRL-SVO(UP) projects the selected variable and reorganizes the total graph, while that of GRL-SVO(NUP) updates the graph via the update rule and delete rule.

### Graph Representation

The graph of a polynomial set can be different. We introduce a graph structure that can reflect the coupling relationship between variables.

**Definition 2** (Associated Graph [30]).: _Given a polynomial set \(F\), and the variable set of \(F\), \(V=var(F)\), an associated graph \(G_{F}(V,E)\) of \(F\) is an undirected graph, where \(E=\{(x_{i},x_{j})|\exists f\in F,x_{i},x_{j}\in var(f)\}\)._

In other words, if two variables appear in the same polynomial, they will have an edge.

GNNs have invariance to permutations and awareness of input sparsity [31; 32; 33]. The strength similarly applies to our work. The associated graph is pure and simple, which only retains information related to variables and "neighbors" in the same polynomial. Note that such a graph can easily become a complete graph. For example, \(x_{1}+x_{2}+x_{3}+x_{4}\) corresponds to a complete associated graph. So, we need to distinguish nodes via rich embeddings, detailed in Appendix B. The embeddings are proposed based on former research [16; 34] and our observations. The embedding vectors of variables will be first normalized via the z-score method, i.e.,

\[E^{\prime}_{j}[x]=\frac{E_{j}[x]-mean(\{E_{j}[v]\})}{std(\{E_{j}[v]\})},v\in var (F^{0}),\]

where \(F^{0}\) is the corresponding polynomial set and \(E_{j}[x]\) denotes the \(j\)-th scalar of the original embedding of variable \(x\).

The graph representation is a tuple \((F^{0},A^{0},X^{0})\) where \(A^{0}\in\{0,1\}^{n\times n}\) is the adjacency matrix of the associated graph and \(X^{0}\in\mathbb{R}^{n\times d}\) is a normalized node embedding matrix for variables.

To encode the representation, we utilize a stack of \(k\) GNN layers, Formula (5.7) in [28]. The process encodes the representations \(X^{i+1}=[X^{i+1}[0];\cdots;X^{i+1}[n-1]]\) via

\[X^{i+1}[u]=\sigma(X^{i}[u]\cdot\theta^{i}_{g,self}+\sum_{v\in\mathcal{N}(u)}X ^{i}[v]\cdot\theta^{i}_{g,neigh}+\theta^{i}_{g,bias}),\]

where \(\theta^{i}_{g,self}\) and \(\theta^{i}_{g,neigh}\) are trainable parameter matrices in \(\mathbb{R}^{H^{i}\times H^{i+1}}\), and \(\sigma\) denotes the activation function. \(\theta^{i}_{g,bias}\) is the trainable bias term in \(\mathbb{R}^{H^{i+1}}\), and \(\mathcal{N}(u)\) is the set of neighbours of variable

Figure 2: The architecture of GRL-SVO(UP) and GRL-SVO(NUP) where \(\phi(\cdot,x)=MLP(CONCAT(\cdot,x))\) for updating the embedding for the neighbours of \(x\). The dashed lines represent that it will be only utilized in training mode.

\(u\). \(H^{i}\) is the dimension of hidden channels, and \(H^{0}=d\). The layer will aggregate the local information of variables and update the embedding sequentially. Finally, we obtain the intermediate tuple \((F^{k},A^{k},X^{k})\).

### Architecture

#### 3.4.1 Markov Decision Process (MDP)

**State Space and Action Space.** GRL-SVO(UP/NUP) will suggest a variable order for any polynomial set. The state space includes the graph representation of any polynomial set, i.e., \(\mathcal{S}=\{G=(F^{0},A^{0},X^{0})\}\). Although it is a very large space, the state \(s\) provides sufficient statistics to evaluate actions. Action corresponds to a candidate variable to _project_. For a given state \(s\), the action space is \(\mathcal{A}=var(F_{s})\), where \(F_{s}\) denotes the polynomial set of current state \(s\).

**Environment.** At the time \(t\), the environment of GRL-SVO(UP) takes current state \(s^{t}\) and selected variable (action \(a^{t}\)) as input and outputs a new state \(s^{t+1}\) via processing the projected polynomial set of CAD. That of GRL-SVO(NUP) only removes the selected variable and linked edges from the current state \(s^{t}\) and updates embeddings via neural networks, which is detailed in Section 3.5.

**Reward.** The number of cells is sufficient to reflect the impact of variable order on efficiency. \(R(\sigma|s^{0})=-N(F_{s^{0}},\sigma)/M\) denotes the reward for a given variable order \(\sigma\) under the initial state \(s^{0}\), i.e., the negative number of cells divided by a static normalization factor \(M\). If the order leads to running timeout and cannot obtain the number of cells, \(R(\sigma|s^{0})=-1\) directly. The reward of agent policy will increase as the training progresses.

#### 3.4.2 Neural Network Architecture

**Neural Network.** The actor network \(\theta_{a}:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{n}\) combines an MLP and softmax layer that transforms the \(X_{t}^{k}\) of \(s^{t}\) to the action distribution at time \(t\). The action obeys the distribution, i.e.,

\[a^{t}\sim\theta_{a}(X_{t}^{k})=softmax(MLP(X_{t}^{k})).\]

The critic network \(\theta_{c}:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}\) is a combination of MeanPool and MLP layers, where \(MeanPool:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{d}\). The critic value is defined as

\[\theta_{c}(X_{0}^{k})=MLP(MeanPool(X_{0}^{k})).\]

**Training.** The parameters of GRL-SVO \(\theta=\{\theta_{g},\theta_{a},\theta_{c}\}\) will go through an end-to-end training process via stochastic gradient descent method. Given initial state \(s\), we aim to learn the parameters of a stochastic policy \(p_{\theta}(\sigma|s)\), which assigns high probabilities to order with a small number of cells and low probabilities to order with a large number of cells. Our neural network architecture uses the chain rule to factorize the probability of a variable order \(\sigma\) as \(p_{\theta}(\sigma|s)=\prod_{t=1}^{n}p_{\theta}(\sigma^{t}|s,\sigma^{<t})\), where \(p_{\theta}(\sigma^{t}|s,\sigma^{<t})=\theta_{a}(X_{t}^{k})\) is current action distribution. \(\sigma^{t}\) is the \(t\)-th element in the variable order \(\sigma\) and \(\sigma^{<t}\) is the partial order from \(\sigma^{1}\) to \(\sigma^{t-1}\). The training objective is the expected reward, which is defined as \(J(\theta|s)=\mathbb{E}_{\sigma\sim p_{\theta}(\cdot|s)}R(\sigma|s)\).

Through the well-known REINFORCE algorithm [35], the gradient of the training objective is

\[\nabla_{\theta}J(\theta|s)=\mathbb{E}_{\sigma\sim p_{\theta}(\cdot|s)}[(R( \sigma|s)-c(s))\nabla_{\theta}logp_{\theta}(\sigma|s)],\]

where \(c(s)=\theta_{c}(X_{0}^{k})\) is the predicting critic value.

Through Monte Carlo sampling, we obtain \(N\)\(i.i.d\) polynomial sets corresponding to states \(s_{1},s_{2},\cdots,s_{N}\sim\mathcal{S}\), and \(N\) variable orders \(\sigma_{i}\sim p_{\theta}(\cdot|s_{i})\). So we update the parameters of neural networks via

\[\theta_{a} \leftarrow\theta_{a}+\frac{1}{N}\sum_{i=1}^{N}{(R(\sigma_{i}|s_{i} )-c(s_{i}))\nabla_{\theta}logp_{\theta}(\sigma_{i}|s_{i})},\] \[\theta_{c} \leftarrow\theta_{c}+\frac{1}{N}\sum_{i=1}^{N}{(R(\sigma_{i}|s_{i} )-c(s_{i}))\nabla_{\theta}c(s_{i})}.\]

**Inference.** At inference time, we generate an order via a greedy selection. For the \(i\)-th element of the order \(\sigma\), we select the variable \(x\) with the maximal probability, i.e., \(x=\arg\max_{x\in V_{t-1}}(p(x|s,\sigma^{<t}))\)where \(V_{t-1}=var(F_{s^{t-1}})\) is the set of variables that have not been projected at time \(t\). After selecting variables \(n-1\) times, we obtain the total order \(\sigma\).

### State Transition without _Project_

We provide an LB & NUP heuristic to free from interaction with the symbolic computation tools. It simulates the _project_ via a neural network for embedding transformation and a delete rule.

As an example, Figure 3 shows a specific case of state transition. The polynomial set changes after _project_ phase the variable \(x_{4}\), where \(3x_{1}^{2}x_{4}-4x_{4}^{3}-1\) is reduced to a set \(\{x_{1}-1,x_{1}+1,x_{1}^{2}-x_{1}+1,x_{1}^{2}+x_{1}+1\}\) without \(x_{4}\) while the polynomial \(x_{1}^{3}-4x_{2}^{2}x_{3}+12x_{2}+3\) remains unchanged. So, the embedding of neighbors of \(x_{4}\), i.e., \(x_{1}\), will change greatly while that of other variables will change slightly. Besides, the projected variable should also be removed from the associated graph. Based on the aforementioned inspiring situations, we propose two rules for approximately simulating _project_.

At time \(t-1\), assume \(x_{i}\) is the next projecting variable. We mainly consider the projected variable's influence on its 1-hop neighbor variables.

**Update Rule.** We update the embedding \(X\) without _project_ for other variables \(x_{j}\) via

\[X[x_{j}]^{t}=\left\{\begin{array}{ll}\phi(X[x_{j}]^{t-1},X[x_{i}]^{t-1}),&A [x_{i}][x_{j}]=1,\\ X[x_{j}]^{t-1},&otherwise,\end{array}\right.\]

where \(\phi(a,b)=MLP(CONCAT(a,b))\) is the neural network that simulates the _project_ for embedding transformation.

**Delete Rule.** It will trivially remove \(x_{i}\) and edges linked to \(x_{i}\) and update \(A,X\) in the state correspondingly.

\[A\gets RemoveRowColumn(A,Map(x_{i})),\] \[X\gets RemoveRow(X,Map(x_{i})),\] \[Map(x_{j})\gets Map(x_{j})-1,i<j<n,\]

where the function \(Map(x):V\rightarrow\mathbb{N}\), maps the variable \(x\) to the index in matrix \(A\) and \(X\), the operation \(RemoveRowColumn(A,Map(x_{i}))\) removes the row and column of variable \(x_{i}\) from \(A\), and the operation \(RemoveRow(X,Map(x_{i}))\) removes the row of variable \(x_{i}\) from \(X\).

After such transitions, the state will feed the model defined by Section 3.4 and obtain the next projecting variable. After calling the model \(n-1\) times, the trajectory corresponds to a variable order suggested by GRL-SVO(NUP).

## 4 Experiments

### Setup

**Implementation and Environments.** We utilized PyTorch Geometric [36] for implementations of our approach. The hyper-parameters are listed in Appendix B. We utilized the NVIDIA Tesla V100 GPU for training. After the heuristics output the variable order, all instances with the given variable order are run with MAPLE 2018 on an Intel Xeon Platinum 8153 CPU (2.00GHz). The run-time limit is 900 seconds, and the time to predict a variable order is also counted.

**Dataset.** We utilize two CAD datasets: _random_ and _SMT-LIB_. The detailed parameters of _random_ generation and collecting methods for _SMT-LIB_ are presented in Appendix B.

The _random_ dataset contains 7 categories from 3-var to 9-var. We generate 20000 3-var instances and split them into training, testing, and validation sets in a ratio of 8:1:1. We pre-run all \(3!=6\) variable orders and remove non-conforming instances where some variables are eliminated due to random generation, for example, \(x-x=0\); we remove the instances that are all timeout under 6 orders. The rest sets have 14990, 1871, and 1887 instances, respectively. The other categories contain 1000 instances. They are generated by _randpoly_ of MAPLE. We also collect a _SMT-LIB_ dataset where there are from 3-var to 9-var instances with numbers \(\{5908,1371,131,123,318,41,24\}\).

**Baselines.** We compare our approach with two kinds of state-of-the-art heuristics, **UP** or **NUP**. For UP, we compare our approaches with _sotd_ and _ndrr_ implemented in the ProjectionCAD package [37]. Besides, we also implement _gmods_[13] in MAPLE. For NUP, we compare our approaches with _brown_, and _triangular_ implemented in the RegularChains package [38]. _EMLP_ approach is proposed by England [15] and its implementation only supports 3-var instances. _PVO(brown)_ and _PVO(triangular)_ are _PVO_ approaches combined with EB & NUP heuristics, _brown_ and _triangular_, respectively. The provided implementations [16] only support 4,5,6-var instances.

**Criterion.** Assume \(\bm{T}\) and \(\bm{N}\) denote the running time and number of cells. \(\bm{AVG}\bm{.T}\) and \(\bm{AVG}\bm{.N}\) denote the average of \(T\) and \(N\). If an instance runs timeout, we count the maximum time (900 seconds) and the maximum number of cells of this instance solved by other heuristics into the calculation. We remove the instances that all heuristics lead to CAD timeout, as such instances can not distinguish the ability of heuristics. \(\bm{PT}\) and \(\bm{\#SI}\) denote the variable order prediction time used by heuristics and the number of solved instances within the time limit. For UP heuristics, \(\bm{PT}\) also takes _project_ time into consideration. Except for \(\bm{\#SI}\), the criteria are the smaller, the better.

**Experiments.** We conduct three experiments to evaluate our approaches. First of all, we only train the models on the 3-var _random_ dataset. Then, we generalize the trained models on _random_ datasets with up to 9 variables and the _SMT-LIB_ dataset. Finally, we compare GRL-SVO(UP) and GRL-SVO(NUP) and discuss different application scenarios. We list ablation experiments in Appendix C, investigating the effect of features (of the initial embedding), network size, network structure, reward normalization factor \(M\), GNN architecture, and coefficient. We also list additional results in Appendix D, including results under other criteria and performance of fine-tuning.

### Results

**Training GRL-SVO.** Figure 3(a) and Figure 3(b) show the performance during training with the instances in the training set. GRL-SVO outperforms the other heuristics through training and shows a rapid performance improvement. The UP heuristics are better than the NUP heuristics. The inference can be more accurate because the UP heuristics obtain more information than the NUP heuristics. In the beginning, GRL-SVO(NUP) with the random initial parameters is better than GRL-SVO(UP). After training, GRL-SVO(UP) shows better performance.

**Generalization.** We only train on the 3-var dataset and generalize the models to the other datasets. After removing all timeout instances, there are 1876, 651, 416, 354, 349, 409, and 383 instances for the _random_ dataset from 3-var to 9-var; for the _SMT-LIB_ dataset, 1777, 387, 17 instances for 3-var, 4-var to 6-var and 7-var to 9-var, respectively. Table 1 shows the performance of all heuristics, and the best scores are bolded. GRL-SVO(UP) is the only LB approach with UP heuristics, achieving the best performance among most UP and NUP heuristics except for the 4-var category. GRL-SVO(NUP) also achieves competitive performance compared to other NUP heuristics. GRL-SVO also shows

Figure 4: The performance of all heuristics. Figure 3(a) and 3(b) correspond to the training phase, and the horizontal lines represent the timeout instances of corresponding heuristics on the training set. Figure 3(c) is the \(PT\) graph over number of variables.

competitive performance in the real-world dataset, _SMT-LIB_ dataset. Note that although the heuristics _sotd_ perform better than GRL-SVO(UP) on the 4-var instances, they run timeout in most instances from 5-var due to the combinatorial explosion of the number of variable orders as shown in Figure 3(c). The NUP heuristics take a short prediction time, and the polylines in Figure 3(c) overlap. For example, an instance with 7 variables leads to \(7!=5040\) variable orders to project for _sotd_ and _ndrr_.

### Discussion on GRL-SVO(UP/NUP)

As in Table 1, GRL-SVO(UP) performs better in \(\bm{\#SI}\), \(\bm{AVG.T}\), \(\bm{AVG.N}\) compared to GRL-SVO(NUP). It is understandable because GRL-SVO(UP) receives more real-world information from the projected polynomial set at each step. As in Figure 3(c), GRL-SVO(NUP) is faster than GRL-SVO(UP). The inference time of GRL-SVO(NUP) is almost unchanged with the increase of variables, but there is an obvious increase of GRL-SVO(UP). As the number of variables grows, the time of _project_ and interactions between GRL-SVO(UP) and symbolic computation tools will be critical. It is also the bottleneck of UP heuristics like GRL-SVO(UP), _gmods_, _sotd_, and _ndrr_. Therefore, the application scenarios corresponding to the two models will be different.

Internalizing GRL-SVO(UP) into the CAD process is a promising option. As _project_ is an algorithm component of CAD, internalization will help reuse the results of projection and reduce the interaction time. GRL-SVO(NUP) is cheap and can extract information directly from polynomial representations. It might be applied to other tools that do not use the entire CAD process. As a canonical example, the automated reasoning tools like Z3 [5], YICES2 [6], CVC5[39], utilize _project_ partially only for generating lemmas when solving non-linear arithmetic problems. At the beginning of solving, they also require a fixed variable order. For tasks that are time-critical and do not utilize full CAD in the solving process, GRL-SVO(NUP) seems to be a better option.

## 5 Limitations

Our graph representation cannot embed complete information on polynomials. The associated graph is simple and only shows the relationship between variables. Through selection, we conduct the embedding of graph nodes, but they still ignore plenty of minutiae information, for example, the distribution of coefficients. Besides, the lack of sufficiently large datasets is also a matter of urgency. We can generate large amounts of random data but may lack practical instances for training. Another slight limitation is the prediction time. Python and PyTorch are both heavy techniques. The prediction time of GRL-SVO shown in Figure 3(c) is actually not much different from other heuristics.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \multicolumn{2}{c|}{**Categories**} & \multicolumn{3}{c|}{**NUP**} & \multicolumn{3}{c}{**UF**} \\ \cline{3-10} \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{**E8**} & \multicolumn{2}{c|}{**FQ3D**} & \multicolumn{2}{c|}{**HQ**} & \multicolumn{2}{c|}{**HQ**} & \multicolumn{2}{c|}{**HQ**} \\ \hline \multirow{3}{*}{3-var(sotd)} & _SST_ & \multicolumn{2}{c|}{_sotd_} & \multicolumn{2}{c|}{_FQ3D_} & \multicolumn{2}{c|}{_FQ3D_} & \multicolumn{2}{c|}{_FQ3D_} & \multicolumn{2}{c|}{_sotd_} & \multicolumn{2}{c|}{_sotd_} & \multicolumn{2}{c|}{_sotd_} \\ \cline{3-10}  & _sotd_ & _ST_ & 171.41 & 215.82 & 140.87 & - & - & 94.77 & 97.44 & 146.32 & 124.06 & **76.08** \\  & _AVG.N_ & 2171.74 & 215.82 & 140.88 & - & - & 246.67 & 243.80 & 247.89 & 259.18 & 219.68 \\ \hline \multirow{3}{*}{4-var} & _AVG.T_ & 245.73 & 207.66 & 408 & 392 & 24.03 & **42.58** & **45.83** & 257.53 & 53.58 \\  & _AVG.T_ & 3128.74 & 394.72 & - & 360.33 & 372.71 & 345.47 & **32.12** & 212.66 & 215.48 & 191.45 \\  & _AVG.N_ & 524.95 & 558.50 & 523.83 & 558.46 & 531.10 & **3925.23** & **24.68** & 648.48 & 1746.90 \\ \hline \multirow{3}{*}{5-var} & _AVG.T_ & 245.32 & 204.72 & - & 224.72 & 278.20 & 287 & 27.75 & 155.98 & **34.60** \\  & _AVG.N_ & 2413.70 & 2120.70 & 2148.24 & - & 1185.34 & 246.43 & 245.81 & 287.47 & 835.28 & 236.01 & **207.79** \\  & _AVG.N_ & 2123.70 & 2132.84 & - & 1179.05 & 1255.82 & 1200.49 & 489.48 & 1468.45 & 1866.79 & **974.83** \\ \hline \multirow{3}{*}{6-var} & _AVG.T_ & 175.75 & 149 & - & 180 & 160 & 200 & 5 & 5 & 273 & **306** \\  & _AVG.N_ & 3017.55 & 5251.44 & - & 690.72 & 578.49 & 469.72 & 289.86 & 156.39 & 899.74 & 214.58 \\  & _AVG.N_ & 2860.90 & 2040.23 & - & 2081.98 & 19290.77 & 1930.92 & 2199.23 & 2323.09 & 17561.67 & **18715.29** \\ \hline \multirow{3}{*}{7-var} & _AVG.T_ & 163.18 & 118 & - & - & 15 & 1 & 1 & 270 & **297** \\  & _AVG.N_ & 548.13 & 613.85 & - & - & - & 552.47 & 879.75 & 879.72 & 313.73 & **245.77** \\  & _AVG.N_ & 2779.31 & 2779.75 & - & - & 2700.28 & 3042.546 & 3046.31 & 2445.39 & **2422.39** \\ \hline \multirow{3}{*}{8-var} & _AVG.T_ & 173.33 & 138 & - & - & 172 & 0 & 0 & 310 & **346** \\  & _AVG.T_ & 619.00 & 564.20 & - & - & 597.90 & 900.00 & 900.32 & 324.34 & **232.80** \\  & _AVG.N_ & 30323.26 & 4607.93 & - & - & 38813.98 & 411.21 & 431.21 & 43046.19 & **3886.21** \\ \hline \multirow{3}{*}{9-var} & _AVG.T_ & 151.52 & - & - & - & 150 & 0 & 0 & 296 & **238** \\  & _AVG.T_ & 649.41 & 609.29 & - & - & 625.90 & 900.00 & 900.00 & 9343.11 & **374.78** \\  & _AVG.N_ & 4237.67 & 4083.28 & - & - & 40640.69 & 5271.03 & 3171.03 & 4594.25 & **422.791** \\ \hline \multirow{3}{*}{SMT-LIB (3-var)} & _AVG.T_ & 170.73 & 163.75 & - & - & 176 & 176 & 176 & **1772** & **1772** \\  & _AVG.T_ & 233.38 & 23.68 & 63.09 & - & - & 22.38 & 34.85 & 61.00 & **18.32** & 18.53 \\  & _AVG.N_ & 4409.79 & 5070.66 & 167.07 & - & 4104.04 & **3272.12** & 4307.27 & 3872.72 & 3968.64 \\ \hline \multirow{3}{*}{SMT-LIB (4-var to 6-var)} & _AVG.T_ & 374.32 & 372 & 372 & 372 & 364 & 356 & 39 & **399** & **379** \\  & _AVG.N_ & 8603.09 & 89.55 & - & 88.32 & 88.91 & 91.17 & 105.18 & 143.22 & **599** & 67.51 \\ \hline \multirow{3}{*}{SMT-LIB (7-var to 9-var)} & _AVG.T_ & 3696.29 & 2456.08 & - & 2409.99 & 22770.

Conclusion and Future Work

In the paper, we have proposed the first RL-based approach to suggest variable order for cylindrical algebraic decomposition. It has two variants: GRL-SVO(UP) for LB & UP and GRL-SVO(NUP) for LB & NUP. GRL-SVO(UP) can suggest branching variables in the CAD process; GRL-SVO(NUP) can suggest total variable order before the CAD process. Our approaches outperform state-of-the-art learning-based heuristics and are competitive with the best expert-based heuristics. Our RL-based approaches also show a strong learning and generalization ability. Future work is to deploy our approach to practical applications, such as constructing an RL-based package for MAPLE and an algorithm component for automated reasoning tools for non-linear arithmetic solving.

## Acknowledgement

This work has been supported by the National Natural Science Foundation of China (NSFC) under grants No.61972384 and No.62132020. Feifei Ma is also supported by the Youth Innovation Promotion Association CAS under grant No.Y202034. The authors would like to thank Bican Xia, Jiawei Liu, and the anonymous reviewers for their comments and suggestions.

## References

* [1] Alfred Tarski. A Decision Method for Elementary Algebra and Geometry. In _Quantifier elimination and cylindrical algebraic decomposition_, pages 24-84. Springer, 1998.
* [2] George E Collins. Quantifier elimination for real closed fields by cylindrical algebraic decomposition. In _Automata theory and formal languages_, pages 134-183. Springer, 1975.
* [3] Mark de Berg, Otfried Cheong, Marc J. van Kreveld, and Mark H. Overmars. _Computational Geometry: Algorithms and Applications, 3rd Edition_. Springer, 2008.
* [4] Steven M. LaValle. _Planning Algorithms_. Cambridge University Press, 2006.
* [5] Dejan Jovanovic and Leonardo Mendonca de Moura. Solving Non-linear Arithmetic. In _IJCAR 2012_, volume 7364 of _Lecture Notes in Computer Science_, pages 339-354. Springer, 2012.
* [6] Dejan Jovanovic. Solving Nonlinear Integer Arithmetic with MCSAT. In _VMCAI 2017_, volume 10145 of _Lecture Notes in Computer Science_, pages 330-346. Springer, 2017.
* [7] Fuqi Jia, Rui Han, Pei Huang, Minghao Liu, Feifei Ma, and Jian Zhang. Improving Bit-Blasting for Nonlinear Integer Constraints. In _ISSTA 2023_, pages 14-25. ACM, 2023.
* [8] Christopher W. Brown and James H. Davenport. The Complexity of Quantifier Elimination and Cylindrical Algebraic Decomposition. In _ISSAC 2007_, pages 54-60. ACM, 2007.
* [9] Christopher W Brown. Companion to the Tutorial: Cylindrical Algebraic Decomposition. _United States Naval Academy_, 2004.
* [10] Andreas Dolzmann, Andreas Seidl, and Thomas Sturm. Efficient Projection Orders for CAD. In _ISSAC 2004_, pages 111-118. ACM, 2004.
* [11] Changbo Chen, James H. Davenport, Francois Lemaire, Marc Moreno Maza, Bican Xia, Rong Xiao, and Yuzhen Xie. Computing the Real Solutions of Polynomial Systems with the RegularChains Library in Maple. _ACM Commun. Comput. Algebra_, 45(3/4):166-168, 2011.
* [12] Russell J. Bradford, James H. Davenport, Matthew England, and David J. Wilson. Optimising Problem Formulation for Cylindrical Algebraic Decomposition. In _CICM 2013_, volume 7961 of _Lecture Notes in Computer Science_, pages 19-34. Springer, 2013.
* [13] Tereso del Rio and Matthew England. New Heuristic to Choose a Cylindrical Algebraic Decomposition Variable Ordering Motivated by Complexity Analysis. In _CASC 2022_, volume 13366 of _Lecture Notes in Computer Science_, pages 300-317. Springer, 2022.

* [14] Haokun Li, Bican Xia, Huiying Zhang, and Tao Zheng. Choosing Better Variable Orderings for Cylindrical Algebraic Decomposition via Exploiting Chordal Structure. _J. Symb. Comput._, 116:324-344, 2023.
* [15] Matthew England and Dorian Florescu. Comparing Machine Learning Models to Choose the Variable Ordering for Cylindrical Algebraic Decomposition. In _CICM 2019_, volume 11617 of _Lecture Notes in Computer Science_, pages 93-108. Springer, 2019.
* [16] Changbo Chen, Zhangpeng Zhu, and Haoyu Chi. Variable Ordering Selection for Cylindrical Algebraic Decomposition with Artificial Neural Networks. In _ICMS 2020_, volume 12097 of _Lecture Notes in Computer Science_, pages 281-291. Springer, 2020.
* [17] Clark Barrett, Pascal Fontaine, and Cesare Tinelli. The Satisfiability Modulo Theories Library (SMT-LIB). www.SMT-LIB.org, 2016.
* [18] Dennis S. Arnon, George E. Collins, and Scott McCallum. Cylindrical Algebraic Decomposition I: The Basic Algorithm. _SIAM J. Comput._, 13(4):865-877, 1984.
* [19] Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In _ICLR 2017_. OpenReview.net, 2017.
* [20] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph Attention Networks. In _ICLR 2018_. OpenReview.net, 2018.
* [21] Dongkwan Kim and Alice Oh. How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision. In _ICLR 2021_. OpenReview.net, 2021.
* [22] Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In _NIPS 1999_, pages 1057-1063. The MIT Press, 1999.
* [23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement Learning. _CoRR_, abs/1312.5602, 2013.
* [24] Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural Combinatorial Optimization with Reinforcement Learning. In _ICLR 2017_. OpenReview.net, 2017.
* [25] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Can Q-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver? In _NIPS 2020_. OpenReview.net, 2020.
* [26] Yao Qin, Hua Wang, Shanwen Yi, Xiaole Li, and Linbo Zhai. A Multi-Objective Reinforcement Learning Algorithm for Deadline Constrained Scientific Workflow Scheduling in Clouds. _Frontiers of Computer Science_, 15:1-12, 2021.
* [27] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In _ICML 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 1928-1937. JMLR.org, 2016.
* [28] William L Hamilton. _Graph representation learning_. Morgan & Claypool Publishers, 2020.
* [29] Russell J. Bradford, James H. Davenport, Matthew England, Scott McCallum, and David J. Wilson. Truth Table Invariant Cylindrical Algebraic Decomposition. _J. Symb. Comput._, 76:1-35, 2016.
* [30] Chenqi Mou, Yang Bai, and Jiahua Lai. Chordal Graphs in Triangular Decomposition in Top-Down Style. _J. Symb. Comput._, 102:108-131, 2021.
* [31] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT Solver from Single-Bit Supervision. In _ICLR 2019_. OpenReview.net, 2019.

* [32] Minghao Liu, Fan Zhang, Pei Huang, Shuzi Niu, Feifei Ma, and Jian Zhang. Learning the Satisfiability of Pseudo-Boolean Problem with Graph Neural Networks. In _CP 2020_, volume 12333 of _Lecture Notes in Computer Science_, pages 885-898. Springer, 2020.
* [33] Quentin Cappart, Didier Chetelat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial Optimization and Reasoning with Graph Neural Networks. In _IJCAI 2021_, pages 4348-4355. ijcai.org, 2021.
* [34] Zongyan Huang, Matthew England, David J. Wilson, James H. Davenport, Lawrence C. Paulson, and James P. Bridge. Applying Machine Learning to the Problem of Choosing a Heuristic to Select the Variable Ordering for Cylindrical Algebraic Decomposition. In _CIKM 2014_, volume 8543 of _Lecture Notes in Computer Science_, pages 92-107. Springer, 2014.
* [35] Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. _Mach. Learn._, 8:229-256, 1992.
* [36] Matthias Fey and Jan E. Lenssen. Fast Graph Representation Learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* [37] Matthew England, David J. Wilson, Russell J. Bradford, and James H. Davenport. Using the Regular Chains Library to Build Cylindrical Algebraic Decompositions by Projecting and Lifting. In _ICMS 2014_, volume 8592 of _Lecture Notes in Computer Science_, pages 458-465. Springer, 2014.
* [38] Francois Lemaire, Marc Moreno Maza, and Yuzhen Xie. The RegularChains library in MAPLE. _SIGSAM Bull._, 39(3):96-97, 2005.
* [39] Haniel Barbosa, Clark W. Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres Notzli, Alex Ozdemir, Mathias Preiner, Andrew Reynolds, Ying Sheng, Cesare Tinelli, and Yoni Zohar. CVC5: A Versatile and Industrial-Strength SMT Solver. In _TACAS 2022_, volume 13243 of _Lecture Notes in Computer Science_, pages 415-442. Springer, 2022.
* [40] Scott McCallum. An Improved Projection Operation for Cylindrical Algebraic Decomposition. In _EUROCAL 1985_, volume 204 of _Lecture Notes in Computer Science_, pages 277-278. Springer, 1985.
* [41] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019_, pages 257-266. ACM, 2019.
* [42] Shyam A. Tailor, Felix L. Opolka, Pietro Lio, and Nicholas D. Lane. Adaptive filters and aggregator fusion for efficient graph convolutions. _CoRR_, abs/2104.01481, 2021.
* [43] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [44] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [45] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. _CoRR_, abs/1711.07553, 2017.
* [46] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. In _NIPS 2017_, pages 1024-1034, 2017.
* [47] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 1548-1554. ijcai.org, 2021.

Cylindrical Algebraic Decomposition

### Detailed Description

#### a.1.1 Polynomial

\(\mathbb{N}\) and \(\mathbb{R}\) denote the set of natural numbers and real numbers, respectively. Let \(\bm{x}=\{x_{1},\cdots,x_{n}\}\) be the variable set, where \(x_{1}<x_{2}<\cdots<x_{n}\) is the order of the variables.

A term \(t_{i}\) is a finite production of powers of variables, i.e. \(t_{i}=\prod_{j=1}^{n}x_{j}^{d_{i,j}}\), where \(d_{i,j}\in\mathbb{N}\) as the degree of the variable \(x_{j}\). We denote \(\sum_{j=1}^{n}d_{i,j}\) as the _degree_ of the term.

A polynomial \(P\in\mathbb{R}[\bm{x}]\) of general form is a finite sum of terms, i.e., \(P=\sum_{i=1}^{m}c_{i}t_{i}\), where \(c_{i}\in\mathbb{R}\) is coefficient of term \(t_{i}\). In addition, the equivalent nested form of polynomial \(Q\in\mathbb{R}[x_{1},\cdots,x_{i-1},x_{i}]\),

\[Q=a_{m}x_{i}^{d_{m}}+a_{m-1}x_{i}^{d_{m-1}}+\cdots+a_{0},\]

where \(0<d_{1}<\cdots<d_{m}\), and the coefficients \(a_{i}\) are polynomials in \(\mathbb{R}[x_{1},\cdots,x_{i-1}]\) with \(a_{m}\neq 0\). We denote \(x_{i}\) as _main variable_, \(d_{m}\) as _degree_, \(a_{m}\) as _leading coefficient_ of the polynomial \(Q\). For example, given a variable order \(x_{1}<x_{2}<x_{3}\),

\[p(x_{1},x_{2},x_{3}) =x_{1}x_{2}x_{3}^{2}+x_{2}x_{3}^{2}+x_{3}^{2}+x_{1}x_{3}+x_{2}x_{3}\] \[=((x_{1}+1)x_{2})x_{3}^{2}+(x_{1}+x_{2})x_{3}.\]

The degree of \(x_{1}x_{2}x_{3}^{2}\), \(x_{2}x_{3}^{2}\), \(x_{3}^{2}\), \(x_{1}x_{3}\), \(x_{2}x_{3}\) are 4,3,2,2,2, while that of the polynomial \(p(x_{1},x_{2},x_{3})\) is 2, and leading coefficient is \(((x_{1}+1)x_{2})\).

#### a.1.2 Cylindrical Algebraic Decomposition

A Cylindrical Algebraic Decomposition (CAD) is a decomposition algorithm of a set of polynomials in ordered \(\mathbb{R}^{n}\) space resulting in finite sign-invariant regions, named _cells_. After CAD, it can query a limited set of sample points in corresponding cells. Due to the sign of each polynomial is either always positive, always negative, or always zero on any given cell, one can determine the sign of the polynomials at any point in \(\mathbb{R}^{n}\) using this set of sample points.

Note that computing the CAD of a set of univariate polynomials (one-dimensional CAD) is quite simple. We only need to calculate all the real roots of the polynomials, and the cells are precisely these real roots themselves along with the intervals they divide. This leads to a motivation for computing the CAD of a set of polynomials with \(n\) variables, which involves recursively reducing the construction of a \(k\)-dimensional CAD to the construction of a \((k-1)\)-dimensional CAD, until reaching the recursive boundary of computing a one-dimensional CAD, and then constructing higher-dimensional CAD from lower-dimensional CAD continuously.

Formally, the algorithm consists of three components: projection, root isolation, and lift. A diagram is shown in Figure 5. The _project_ phase eliminates the variables of a polynomial set \(P_{n}\) with \(n\) variables by a strictly defined projection operator _proj_ in a given order \(x_{1}<x_{2}<\cdots<x_{n}\) until only one variable \(x_{1}\) left, resulting in polynomial sets \(P_{1},\cdots,P_{n-1}\), where \(P_{i}=\textit{proj}(P_{i+1})\) contains only the variables \(x_{1},\cdots,x_{i}\). The projection operator is carefully designed to ensure that the CAD of \(P_{i}\) can be constructed from the CAD of \(P_{i-1}\). Then the _root isolate_ and _lift_ phases are alternated successively. To make the following statement compatible, we replace the notation of \(P_{1}\) with \(P_{1,1}\). Let \(N(i)\) denote the number of cells generated by \(P_{i}\) (also equals to the number of sample points). We set \(N(0)\) to be 1. The _root isolate_ phase will output all roots of a univariate polynomial set. When \(i=1\), the roots of \(P_{1,1}\) split \(\mathbb{R}\) into \(N(1)\) cells. Let \(SP_{i,k},1\leq k\leq N(i-1)\) denote the set of sample points of the cells generated by \(P_{i,k}\) and \(SP_{i}\) denote the union of \(SP_{i,k}\). Actually, \(SP_{i}\) is precisely the sample points of the cells generated by \(P_{i}\) and \(|SP_{i}|=N(i)\). The _lift_ phase assigns each sample point \(s_{i,k}\) of \(SP_{i}\) to variables of \(P_{i+1}\), i.e., \((x_{1},x_{2},\cdots,x_{i})\gets s_{i,k},1\leq k\leq N(i)\), to polynomials in \(P_{i+1}\), resulting in univariate polynomial sets \(P_{i+1,k},1\leq k\leq N(i)\). Invoking the _root isolate_ phase will obtain each \(SP_{i+1,k}\) and finally their union \(SP_{i+1}\). After repeating _root isolate_ and _lift_ for \(n-1\) times, we achieve the cells of \(P_{n}\) characterized by the sample points \(SP_{n}\). The paper provides a concrete example of the CAD process in Example 2.1.

The projection operator _proj_ plays a key role in the CAD process, which carries enough information to ensure that the CAD of any set of polynomials \(P\) can be constructed from the CAD of \(\textit{proj}(P)\)The first projection operator that satisfies the above property is designed by George Collins[2], which is too complicated, however. Here, we introduce another classic simplified projection operator, the McCallum Projection Operator[40]. It is the default projection operator used in the ProjectionCAD package[37] and our architecture of GRL-SVO(UP).

In mathematics, the resultant is used to determine whether two polynomials have common zeros, while the discriminant is used to determine if one polynomial has repeated roots. Both these two tools are crucial in the construction of the projection operator. We first introduce the definitions of the resultant and the discriminant, and then the McCallum Projection Operator is detailed in Definition 5.

**Definition 3** (Resultant).: _Let \(f_{1}\), \(f_{2}\) be two polynomials in \(\mathbb{R}[x_{1},\ldots,x_{n}]\). Assume that_

\[f_{1}=a_{m}x_{n}^{d_{m}}+a_{m-1}x_{n}^{d_{m-1}}+\cdots+a_{0},\]

\[f_{2}=b_{n}x_{n}^{d_{n}}+b_{n-1}x_{n}^{d_{n-1}}+\cdots+b_{0}.\]

_The resultant of \(f_{1}\) and \(f_{2}\) with respect to \(x_{n}\), \(\text{Res}(f_{1},f_{2},x_{n})\), is:_

\[\text{Res}(f_{1},f_{2},x_{n})=\left|\begin{array}{cccccc}a_{m}&a_{m-1}& \cdots&a_{0}&&\\ &a_{m}&a_{m-1}&\cdots&a_{0}&&\\ &&\ddots&\ddots&\ddots&\ddots&\\ &&&a_{m}&a_{m-1}&\cdots&a_{0}\\ b_{n}&b_{n-1}&\cdots&b_{0}&&\\ &b_{n}&b_{n-1}&\cdots&b_{0}&&\\ &&\ddots&\ddots&\ddots&\ddots&\\ &&&b_{n}&b_{n-1}&\cdots&b_{0}\end{array}\right|\]

**Definition 4** (Discriminant).: _Let \(f\) be a polynomial in \(\mathbb{R}[x_{1},\ldots,x_{n}]\). Assume that_

\[f=a_{m}x_{n}^{d_{m}}+a_{m-1}x_{n}^{d_{m-1}}+\cdots+a_{0}.\]

_The discriminant of \(f\) with respect to \(x_{n}\), \(\text{Dis}(f,x_{n})\), is:_

\[\text{Dis}(f,x_{n})=\frac{(-1)^{\frac{m(m-1)}{2}}}{a_{m}}\text{Res}(f,\frac{ \partial f}{\partial x_{n}},x_{n}).\]

**Definition 5** (McCallum Projection Operator).: _Let \(F=\{f_{1},\ldots,f_{k}\}\) be a set of polynomials in \(\mathbb{R}[x_{1},\ldots,x_{n}]\). The McCallum projection operator, \(\text{proj}_{m}\), is a mapping that maps \(F\) to \(\text{proj}_{m}(F)\), where \(\text{proj}_{m}(F)\) is the set of polynomials in \(\mathbb{R}[x_{1},\ldots,x_{n-1}]\) defined by:_

* _The coefficients of each polynomial in_ \(F\)_,_
* _The discriminant of each polynomial in_ \(F\) _with respect to_ \(x_{n}\)_,_
* _The resultant of any two different polynomials_ \(f_{i}\)_,_ \(f_{j}\) _in_ \(F\) _with respect to_ \(x_{n}\)_._

Another important component in the CAD algorithm is the real root isolation algorithm, which accepts a set of univariate polynomials and results in all the roots (actually, the arbitrarily small intervals that contain each root) of the univariate polynomials. This can be accomplished by invoking

Figure 5: The process of CAD.

the subalgorithm _RootIsolation_ multiple times and adjusting the upper and lower bounds of the initial interval for each univariate polynomial.

We provide the specifics of the algorithm _RootIsolation_ in Algorithm 2. Before that, some necessary concepts like the sign variation, the 1-norm, and the Sturm sequence are introduced, which play a role in the algorithm _RootIsolation_.

**Definition 6** (Sign Variation).: _For a sequence of non-zero real numbers \(\overline{c}\): \(c_{1}\), \(c_{2}\), \(\cdots\), \(c_{k}\), the sign variation of \(\overline{c}\), \(V(\overline{c})\), is:_

\[V(\overline{c})=|\{i|1\leq i<k\&c_{i}c_{i+1}<0\}|.\]

_For a sequence of univariate polynomials \(\overline{S}\): \(f_{1}\), \(f_{2}\), \(\cdots\), \(f_{k}\), the sign variation of \(\overline{S}\) at real number \(a\), \(V_{a}(\overline{S})\) is:_

\[V_{a}(\overline{S})=V(\overline{s}),\]

_where \(\overline{s}\) is the real number sequence \(f_{1}(a)\), \(f_{2}(a)\), \(\cdots\), \(f_{k}(a)\)._

**Definition 7** (1-norm).: _Given a univariate polynomial \(f=a_{m}x^{d_{m}}+a_{m-1}x^{d_{m-1}}+\cdots+a_{0}\). The 1-norm of \(f\), \(||f||_{1}\), is:_

\[||f||_{1}=\sum_{i=0}^{m}|a_{i}|.\]

**Input** : \(f\): a univariate polynomial

**Output**: \(sturm\): the Sturm Sequence of \(f\)

```
1:\(sturm\leftarrow[]\)
2:\(h\gets f\)
3:\(g\gets f^{\prime}\)
4:\(r\leftarrow-rem(g,h,x)\)
5:while\(r\neq 0\)do
6: Append \(r\) to \(sturm\)
7:\(h\gets g\)
8:\(g\gets r\)
9:\(r\leftarrow-rem(g,h,x)\)
10:endwhile
11:return\(sturm\) ```

**Algorithm 1** SturmSequence

```
1:\(\overline{S}\gets SturmSequence(f)\)
2:\(a\leftarrow-||f||_{1}\)
3:\(b\leftarrow||f||_{1}\)
4:if\(V_{a}(\overline{S})=V_{b}(\overline{S})\)then
5:return failure
6:endif
7:while\(V_{a}(\overline{S})-V_{b}(\overline{S})>1\)do
8:\(c\leftarrow\frac{a+b}{2}\)
9:if\(V_{a}(\overline{S})>V_{c}(\overline{S})\)then
10:\(b\gets c\)
11:else
12:\(a\gets c\)
13:endif
14:endwhile
15:return\((a,b)\) ```

**Algorithm 2** RootIsolation

```
1:\(\overline{S}\gets SturmSequence(f)\)
2:\(a\leftarrow-||f||_{1}\)
3:\(b\leftarrow||f||_{1}\)
4:if\(V_{a}(\overline{S})=V_{b}(\overline{S})\)then
5:return failure
6:endif
7:while\(V_{a}(\overline{S})-V_{b}(\overline{S})>1\)do
8:\(c\leftarrow\frac{a+b}{2}\)
9:if\(V_{a}(\overline{S})>V_{c}(\overline{S})\)then
10:\(b\gets c\)
11:else
12:\(a\gets c\)
13:endif
14:endwhile
15:return\((a,b)\) ```

**Algorithm 3** RootIsolation

### Case Study: Discriminant

Actually, whether \(x^{2}+bx+c=0\) has a real root is equivalent to checking the satisfiability of \(\exists x.x^{2}+bx+c=0\). Let's elaborate on why the quantified formula \(\exists x.x^{2}+bx+c=0\) can be transformed into an equivalent quantifier-free formula \(b^{2}-4c\geq 0\) via CAD techniques. By feeding the CAD algorithm with the set of polynomial \(\{x^{2}+bx+c\}\) and an order \(b\prec c\prec x\), CAD decomposes \(\mathbb{R}^{3}\) into 9 cells:

\[\begin{cases}b=b\\ c<\frac{b^{2}}{4}\\ x<-\frac{b}{2}-\frac{\sqrt{b^{2}-4ac}}{2},\\ b=b\\ c<\frac{b^{2}}{4}\\ x>-\frac{b}{2}+\frac{\sqrt{b^{2}-4ac}}{2},\\ b=b\\ c=\frac{b^{2}}{4}\\ x<-\frac{b}{2},\\ \end{cases}\begin{cases}b=b\\ c<\frac{b^{2}}{4}\\ x=-\frac{b}{2}-\frac{\sqrt{b^{2}-4ac}}{2},\\ b=b\\ c<\frac{b^{2}}{4}\\ x=-\frac{b}{2}+\frac{\sqrt{b^{2}-4ac}}{2},\\ b=b\\ c<\frac{b^{2}}{4}\\ -\frac{b}{2}-\frac{\sqrt{b^{2}-4ac}}{2}<x<-\frac{b}{2}+\frac{\sqrt{b^{2}-4ac} }{2},\\ b=b\\ c=\frac{b^{2}}{4}\\ x>-\frac{b}{2},\\ \end{cases}\]

Note that the sign of the polynomial \(x^{2}+bx+c\) is zero if and only if \((b,c,x)\) belongs to cells:

\[\begin{cases}b=b\\ c=\frac{b^{2}}{4}\\ x=-\frac{b}{2},\\ \end{cases}\quad\begin{cases}b=b\\ c<\frac{b^{2}}{4}\\ x=-\frac{b}{2}-\frac{\sqrt{b^{2}-4ac}}{2},\\ \end{cases}\quad\begin{cases}b=b\\ c<\frac{b^{2}}{4}\\ x=-\frac{b}{2}+\frac{\sqrt{b^{2}-4ac}}{2}.\\ \end{cases}\]

So we know that there must exist \(x\) such that \(x^{2}+bx+c=0\) when \(b^{2}-4c>0\) or \(b^{2}-4c=0\), and it is impossible to find a \(x\) such that \(x^{2}+bx+c=0\) when \(b^{2}-4c<0\). So, we can conclude the quantified formula \(\exists x.x^{2}+bx+c=0\) is equivalent to the quantifier-free formula \(b^{2}-4c\geq 0\). In fact, similar processes can be abstracted into a universal algorithm to solve the quantifier elimination problems in the real closed field. See [2] for more details.

### Relation of #Cells And Efficiency

As in Figure 6, there is a strong correlation between the number of cells produced and the computation time, as well as the memory usage. When the number of cells increases, the computation time and the memory usage also increase.

Figure 6: The relationships between the number of cells and other important indicators. Figure 5(a) and Figure 5(b) respectively correspond to the relationships between the number of cells and computation time and the relationships between the number of cells and memory usage. Five hundred instances of 3-var are randomly selected from the _random_ dataset to construct the two scatterplots.

## Appendix B Experiment Setup

### Datasets

#### b.1.1 _Random_ Dataset

We generated the random polynomial set via the _randpoly(vars, opts)_ function in MAPLE, where _opts_ are specifying properties like _coeffs, expons, terms, degree, dense, homogeneous_. Table 2 lists the descriptions of parameters. We also show an example of random polynomial generation.

For example, _randpoly(\([x_{1},x_{2},x_{3}]\), terms = 4, expons = rand(0..2), coeffs = rand(-100..100))_ will generate a polynomial,

\[56x_{1}x_{2}^{2}x_{3}^{2}-4x_{1}^{2}x_{2}x_{3}+37x_{1}x_{2}^{2}x_{3}-32x_{1}x_{ 2}x_{3}^{2},\]

where _rand(a..b)_ is a random number generator in the range of [_a, b_]. Note that the random polynomial is _sparse_ and _non-homogeneous_ by default. We also ignore the parameter _degree_, because it is only valid in the case of _dense_ random polynomial generation.

Table 3 lists the parameters for _randpoly_ for generating the _random_ dataset, where _#vars_ = \(n\) corresponds to a list of variables \([x_{1},x_{2},\cdots,x_{n}]\). The random number for _terms_ is generated with Python's _random_ library outside the MAPLE script. If the polynomial produced by MAPLE lacks a constant term, one is added, ranging from -100 to 100, using the same Python library.

#### b.1.2 _Smt-Lib_ Dataset

The _SMT-LIB_ dataset is built by the instances that only involve real constraints in the QF_NRA category of the _SMT-LIB_[17]. Using the Python interface of Z3[5], we parse the instances to extract the polynomial sets, discarding any instances that cannot be correctly parsed. Then we categorize the processed polynomial sets based on the number of variables and finally build the _SMT-LIB_ dataset, containing instances ranging from 3 to 9 variables with counts \(\{5908,\)\(1371,131,123,318,41,24\}\). Furthermore, we observed that there are numerous polynomial sets in the dataset that yield the same result after factoring out the irreducible factors, which are indistinguishable from CAD algorithms. So we cluster them and only one instance of each clustering is included in the dataset, resulting in \(\{1777,387,17\}\) instances for 3-var, 4-var to 6-var and 7-var to 9-var, respectively.

### Neural Networks

Table 4 lists the hyper-parameters of the GNN encoder, the networks used in GRL-SVO(NUP)'s architecture to transform the original embeddings linearly and simulate the _project_ process, actor network, critic network, and RL architecture.

## Appendix C Ablation Experiments

### The Effect of Features

There are 14 different features to characterize a variable listed in Table 5. We conduct an experiment on the effect of features. We make masks for these features, where the mask will set the features that we do not care about as zero.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Parameter** & **Description** \\ \hline vars & List or set of variables \\ coeffs & Generator of coefficients \\ expons & Generator of exponents \\ terms & Number of terms \\ degree & Total degree for a dense polynomial \\ dense & The polynomial is to be dense \\ homogeneous & The polynomial is to be homogeneous \\ \hline \hline \end{tabular}
\end{table}
Table 2: Parameters of _randpoly_ function.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**\#vars** & **coeffs** & **expons** & **terms** \\ \hline
3 & rand(-100..100) & rand(0..2) & rand(3..6) \\
4 & rand(-100..100) & rand(0..2) & 3 \\
5 & rand(-100..100) & rand(0..2) & 3 \\
6 & rand(-100..100) & rand(0..2) & 3 \\
7 & rand(-100..100) & rand(0..2) & 3 \\
8 & rand(-100..100) & rand(0..2) & 3 \\
9 & rand(-100..100) & rand(0..2) & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Parameters of _random_ dataset generation.

1. One-hot masks (test the effect of a single feature), for example, to test the effect of \(E_{1}\), the corresponding one-hot mask is (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0). Multiplying with input feature will result in a feature vector with only \(E_{1}\) while others are 0.
2. Operation masks (test the effect of different operations in features) will group features according to their operation type (maximum, sum, and proportion). Note that we treat \(E_{1},E_{2}\) as sub-features utilizing sum operation. * Max: \(E_{3},E_{5},E_{6},E_{7},E_{13},E_{14}\) * Sum: \(E_{1},E_{2},E_{4},E_{8},E_{9},E_{10}\) * Prop: \(E_{11},E_{12}\)
3. Object masks (test the effect of different objects in features) will group features according to their target objects (variable, term, and polynomials). * Var: \(E_{1},E_{3},E_{4},E_{13},E_{14}\) * Term: \(E_{5},E_{6},E_{7},E_{8},E_{9},E_{10},E_{12}\) * Poly: \(E_{2},E_{11}\)
4. Because degree is a common feature that most heuristics are concerned with, testing the effect of degree is necessary. Degree masks will group features according to whether they utilize degree. * Degree: \(E_{3},E_{4},E_{5},E_{7},E_{8},E_{9}\) * NoDegree: \(E_{1},E_{2},E_{6},E_{10},E_{11},E_{12},E_{13},E_{14}\)

Table 6 and Table 7 show the results of different single, operation, object, and degree features. We can conclude that only one feature is not enough. The sum, term, and degree may be the most important factors, as using Sum/Term/Degree features will result in the largest difference in performance.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Category** & **Parameter** & **Value** \\ \hline GNN & The number of GNN layers & 4 \\ GNN & The number of Intermediate layer features & 256 \\ GNN & Aggregation method & mean \\ GNN & Use bias & True \\ NUP\_transform & The size of MLP layer features & [14, 256, 128, 64] \\ NUP\_simulate & The size of MLP layer features & [128, 512, 256, 64] \\ Actor & The size of MLP layer features & [256, 512, 128, 1] \\ Critic & The size of MLP layer features & [256, 512, 128, 1] \\ RL & Batch size & 32 \\ RL & Learning rate & 2e-5 \\ RL & Training maximum epoch & 100 \\ RL & Reward normalization factor (\(M\)) & 50000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The hyper-parameters.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Symbol** & **Description** \\ \hline \(E_{1}(x)\) & Number of other variables occurring in the same polynomials \\ \(E_{2}(x)\) & Number of polynomials containing \(x\) \\ \(E_{3}(x)\) & Maximum degree of \(x\) among all polynomials \\ \(E_{4}(x)\) & Sum of degree of \(x\) among all polynomials \\ \(E_{5}(x)\) & Maximum degree of leading coefficient of \(x\) among all polynomials \\ \(E_{6}(x)\) & Maximum number of terms containing \(x\) among all polynomials \\ \(E_{7}(x)\) & Maximum degree of all terms containing \(x\) \\ \(E_{8}(x)\) & Sum of degree of all terms containing \(x\) \\ \(E_{9}(x)\) & Sum of degree of leading coefficient of \(x\) \\ \(E_{10}(x)\) & Sum of number of terms containing \(x\) \\ \(E_{11}(x)\) & Proportion of \(x\) occurring in polynomials \\ \(E_{12}(x)\) & Proportion of \(x\) occurring in terms \\ \(E_{13}(x)\) & Maximum number of other variables occurring in the same term \\ \(E_{14}(x)\) & Maximum number of other variables occurring in the same polynomial \\ \hline \hline \end{tabular}
\end{table}
Table 5: The original embedding of a variable in an associated graph.

### The Effect of Network Size

We conduct an experiment on the effect of network size, and the results are listed in Table 8. The number of GNN layers (for short, #G) \(\in\) {1, 2, 3, 4, 5} and the number of Intermediate layer features (for short, #I) \(\in\) {32, 64, 128, 256, 512} where Actor and Critic will keep the same proportion 2:4:1. Note that the input dimension of Actor and Critic is the same as #I. For example, if #I = 32, then the dimension of the Actor and Critic are both [32, 64, 16, 1]. Elements in the following tables are #SI in the validation set and #SI in the testing set, respectively.

### The Effect of Network Structure

We conduct an experiment on the effect of network structure. We build two models: one without embedding (NO_EMB), and the other without edge (NO_EDGE). Note that the GNN updating operator we used is \(\mathbf{x}_{i}^{\prime}=\mathbf{W}_{1}\mathbf{x}_{i}+\mathbf{W}_{2}\sum_{j\in \mathcal{N}(i)}e_{j,i}\cdot\mathbf{x}_{j}+\mathbf{b}\). As edges are not considered in NO_EDGE, actually, the operator will be \(\mathbf{x}_{i}^{\prime}=\mathbf{W}_{1}\mathbf{x}_{i}+\mathbf{b}\). So NO_EDGE is equivalent to MLP. For NO_EMB, \(\#SI=1459,\mathit{AVG}.T=252.03,\mathit{AVG}.N=2874.91\). Table 9 shows the result of NO_EDGE on testing set. The performance of NO_EMB drops dramatically while that of NO_EDGE is good, and GRL-SVO can still outperform such models. We explore the performance of NO_EDGE(NUP) under different sizes of parameters with GRL-SVO(NUP). MLP_4_512 has twice as many parameters as ours. GRL-SVO(NUP) can outperform all MLPs as shown in Figure 7. It is the advantage of the graph structure where a variable can grasp neighbor information.

### The Effect of Reward Normalization Factor (\(M\))

We make an ablation experiment on \(M\) that we train the models with \(M\)=10000, 20000, 50000(ours), 100000, and without \(M\). Note that if there is no \(M\), the reward (the number of cells) will be a relatively large integer. As shown in Table 10, \(M\) is necessary. The first number is the result of the validation set, while the second is the result of the testing set.

### The Effect of GNN Architecture

We conduct experiments using different GNN architectures available in PyTorch Geometric that have similar formal parameters as the GNN architecture we used: ClusterGCNConv[41], EGConv[41],

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c} \hline \hline  & & \multicolumn{3}{c|}{GRL-SVO(NUP)} & \multicolumn{3}{c}{GRL-SVO(UP)} \\ \hline  & & 32 & 64 & 128 & 236 & 512 & 32 & 64 & 128 & 236 & 512 \\ \hline
1 & 1754,1737 & 1756,1741 & **1771.1765** & 1770,1765 & 1764,1765 & 1767,1759 & 1768,1758 & 1777,1774 & 1789,1789 & 1790,1793 \\
2 & **1771**,1764 & 1762,1747 & 1765,1763 & 1767,1766 & 1764,1768 & 1773,1763 & 1780,1777 & 1788,1773 & 1786,1793 & 1788,**1794** \\
3 & 1765,1758 & 1756,1764 & 1766,1758 & 1761,1766 & 1770,1769 & 1781,1767 & 1783,1778 & 1782,1785 & 1788,**1794** & 1786,1793 \\
4 & 1765,1761 & 1768,1761 & 1764,1762 & 1766,1765 & 1769,1765 & 1775,1766 & 1784,1775 & 1780,1779 & **1792,1794** & 1788,1793 \\
5 & 1768,1762 & 1765,1763 & **1771.175** & 1766,1761 & 1763,1762 & 1779,1779 & 1784,1781 & 1788,1797 & **1792,1790** & 1791,1790 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The performance (#SI) of GRL-SVO(NUP) and GRL-SVO(UP).

\begin{table}
\begin{tabular}{c c|c c c|c c c|c c c} \hline \hline  & & & \multicolumn{3}{c|}{**Operation**} & \multicolumn{3}{c|}{**Object**} & \multicolumn{3}{c}{**Degree**} \\ \hline  & & Max & Sum & Prop & Var & Term & Poly & Degree & NoDegree \\ \hline \multirow{3}{*}{NUP} & \(\#SI\) & 1640 & **1754** & 1670 & 1635 & 1726 & 1462 & 1723 & 1685 \\  & \(\mathit{AVG}.T\) & 159.42 & **101.21** & 147.48 & 152.11 & 116.39 & 249.17 & 114.95 & 141.65 \\  & \(\mathit{AVG}.N\) & 2444.09 & **2145.71** & 2496.74 & 2250.21 & 2300.32 & 2857.07 & 2177.08 & 2468.04 \\ \hline \multirow{3}{*}{UP} & \(\#SI\) & 1716 & **1788** & 1707 & 1698 & 1751 & 1540 & 1770 & 1715 \\  & \(\mathit{AVG}.T\) & 119.80 & **81.93** & 129.53 & 119.32 & 102.07 & 211.55 & 91.31 & 125.53 \\ \cline{1-1}  & \(\mathit{AVG}.N\) & 2346.79 & **2082.22** & 2369.74 & 2184.08 & 2232.74 & 2743.79 & 2103.21 & 2359.62 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The performance of different feature classifications.

[MISSING_PAGE_FAIL:20]

The number of cells of (a) and (b) are the same, while (c), (d), and (e) are different. But the best variable order is the same (\(x\prec y\)) in these cases. To a certain extent, in these cases, the coefficient mostly affects the number of cells. We conduct an experiment on coefficient that we have randomly modified the coefficients (in [-100, 100]) of 1000 instances randomly selected from a 3-var testing set. Since the coefficients were the only modification made, we used the original variable order generated from the unaltered instances. Our models continue to outperform other heuristics, as demonstrated by Table 12. We observe a slight decline in the performance of all heuristics, indicating that the coefficient also plays a significant role as a parameter (although it may not be the most crucial one).

## Appendix D Additional Results

### Results Under Other Criteria

Assume \(\bm{T}\) and \(\bm{N}\) denote the running time and number of cells. \(\bm{COMAVG}\). \(\bm{T}\) and \(\bm{COMAVG}\).\(\bm{N}\) denote the average of \(T\) and \(N\) of the instances that all heuristics solved within the time limit. Since the _sotd_ and _ndrr_ heuristics are not applicable to most instances with the number of variables more than 5, we remove the comparison with these two heuristics on the results of the new criteria.

There are 1325, 292, 142, 106, 84, 103, and 87 common instances that all heuristics except _sotd_ and _ndrr_ solved within the time limit for the _random_ dataset from 3-var to 9-var; and for the _SMT-LIB_ dataset, 1670, 354, 9 instances for 3-var, 4-var to 6-var and 7-var to 9-var, respectively. Table 13 shows the results and the best scores are bolded. We can observe that GRL-SVO still achieves a relatively good performance under the new criteria.

### Performance of Fine-Tuning

As GRL-SVO(UP) has demonstrated strong generalization abilities and achieved the best performance on almost all datasets, there is still room for further improvement in the generalization capabilities of GRL-SVO(NUP). So, we further investigate the performance of GRL-SVO(NUP) after fine

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & brown & triangular & EMLP & sotd & ndrr & gmods & GRL-SVO(NUP) & GRL-SVO(UP) \\ \hline
#SI & 831 & 762 & 855 & 898 & 842 & 867 & 892 & **912** \\ AVG.T & 155.42 & 212.50 & 125.54 & 79.73 & 141.29 & 100.30 & 79.96 & **64.89** \\ AVG.N & 2380.67 & 2606.40 & 2353.11 & 2065.13 & 2288.08 & 2162.19 & 2100.57 & **2023.58** \\ \hline \hline \end{tabular}
\end{table}
Table 11: The performance of GRL-SVO(NUP) with different GNN architectures.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & ClusterGCNConv & EGCConv & FiLmConv & LEConv & GATConv \\ \hline
#SI & 1767 & 1765 & 1760 & 1761 & 1726 \\ AVG.T & 94.26 & 98.53 & 96.49 & 96.76 & 119.72 \\ AVG.N & 2144.66 & 2177.32 & 2136.67 & 2151.75 & 2166.78 \\ \hline  & GATV2Conv & GeneralConv & ResGatedGraphConv & SageConv & TransformerConv \\ \hline
#SI & 1762 & 1768 & 1759 & 1765 & 1769 \\ AVG.T & 104.14 & 95.17 & 96.25 & 96.74 & 94.45 \\ AVG.N & 2184.86 & 2132.81 & 2146.59 & 2131.22 & 2134.51 \\ \hline \hline \end{tabular}
\end{table}
Table 12: The performance of different heuristics after the coefficients are randomly modified.

Figure 8: The results of slight changes of coefficients.

tuning. We conduct a case study on fine-tuning, utilizing 100 instances with 4 variables. All the hyperparameters employed during the training process were identical to those listed in Table 4.

As shown in Table 14, after fine-tuning, GRL-SVO(NUP) exhibits the best performance among all NUP methods across all categories of the _random_ dataset. The \(\bm{\#SI}\) indicator for GRL-SVO(NUP) showed enhancements ranging from 1.00% to 14.38% across the 4-var to 9-var categories of the _random_ dataset, with slight improvements also observed in the _SMT-LIB_ dataset. Although fine-tuning can help improve the efficiency of GRL-SVO(NUP), the approach is not fit for the instances with a relatively larger number of variables, as we need to run the most variable orders of each instance to build a training dataset. So, for high-dimensional instances, how to fine-tune is a promising direction.

\begin{table}
\begin{tabular}{c c|c c c c c|c c} \hline \multicolumn{2}{c|}{**Categories**} & \multicolumn{5}{c|}{**NUP**} & \multicolumn{2}{c}{**UP**} \\ \hline  & & \multicolumn{2}{c|}{**EB**} & \multicolumn{2}{c|}{**NUP**} & \multicolumn{2}{c|}{**LK**} & \multicolumn{2}{c}{**EB**} & \multicolumn{2}{c}{**LR**} \\ \hline  & & \multicolumn{2}{c|}{_brown_} & \multicolumn{2}{c|}{_transolar_} & \multicolumn{2}{c|}{_EMLP_} & \multicolumn{2}{c}{_PVO(brown)_} & \multicolumn{2}{c|}{_PVO(triangular)_} & \multicolumn{2}{c}{_GRL-SVO(NUP)_} \\ \hline \multirow{3}{*}{3-var(test)} & \(\#SI\) & 1620 & 1504 & 1686 & - & - & **1772** \\  & _AVG.T_ & 170.63 & 227.60 & 140.06 & - & - & **94.07** \\  & _AVG.N_ & 2421.18 & 2663.09 & 2384.29 & - & - & **2157.92** \\ \hline \multirow{3}{*}{4-var} & \(\#SI\) & 415 & 376 & - & 408 & 392 & **456** \\  & _AVG.T_ & 352.87 & 394.71 & - & 360.33 & 376.71 & **298.74** \\  & _AVG.N_ & 5258.57 & 5539.73 & - & 5338.45 & 5536.28 & **5060.19** \\ \hline \multirow{3}{*}{5-var} & \(\#SI\) & 236 & 202 & - & 242 & 218 & **253** \\  & _AVG.T_ & 435.64 & 495.34 & - & 419.50 & 466.47 & **394.12** \\  & _AVG.N_ & 12465.41 & 14493.00 & - & 11932.31 & 12826.53 & **11586.65** \\ \hline \multirow{3}{*}{6-var} & \(\#SI\) & 175 & 149 & - & 180 & 160 & **204** \\  & _AVG.T_ & 500.62 & 551.15 & - & 489.57 & 526.80 & **432.64** \\  & _AVG.N_ & 20487.45 & 20068.2 & - & 20029.06 & **18993.53** & 19407.97 \\ \hline \multirow{3}{*}{7-var} & \(\#SI\) & 163 & 118 & - & - & - & **175** \\  & _AVG.T_ & 549.15 & 632.62 & - & - & - & **222.35** \\  & _AVG.N_ & 28552.12 & 29182.78 & - & - & - & **27779.48** \\ \hline \multirow{3}{*}{8-var} & \(\#SI\) & 173 & 138 & - & - & - & **177** \\  & _AVG.T_ & 601.17 & 653.60 & - & - & - & **594.77** \\  & _AVG.N_ & 39540.72 & 40902.42 & - & - & - & **38957.71** \\ \hline \multirow{3}{*}{9-var} & \(\#SI\) & 151 & 125 & - & - & - & **171** \\  & _AVG.T_ & 651.36 & 691.92 & - & - & - & **619.39** \\  & _AVG.N_ & 48860.36 & 50315.52 & - & - & - & **47406.84** \\ \hline \multirow{3}{*}{SMT-LIB (3-var} & \#SI\) & **1770** & 1763 & 1675 & - & - & 1768 \\  & _AVG.T_ & **203.23** & 23.68 & 83.09 & - & - & 20.86 \\ \cline{1-1}  & _AVG.N_ & 4449.79 & 5070.46 & 7654.04 & - & - & **4014.59** \\ \hline \multirow{3}{*}{SMT-LIB (4-var to 6-var)} & \(\#SI\) & **374** & 372 & - & 372 & 372 & 365 \\  & _AVG.T_ & **383.92** & 87.85 & - & 86.22 & 86.88 & **88.42** \\ \cline{1-1}  & _AVG.N_ & 24514.73 & 24178.54 & - & 22953.99 & 22644.04 & **20910.95** \\ \hline \multirow{3}{*}{SMT-LIB (7-var to 9-var)} & \(\#SI\) & **13** & 12 & - & - & - & **12** \\  & _AVG.T_ & **308.14** & 377.32 & - & - & - & **341.21** \\ \cline{1-1}  & _AVG.N_ & 53971.24 & 58675.94 & - & - & - & **52381.82** \\ \hline \end{tabular}
\end{table}
Table 14: The performance of **NUP** heuristics, with the performance of GRL-SVO(NUP) after fine-tuning. The dash “-” indicates that the method does not support the category.

\begin{table}
\begin{tabular}{c c|c c c c c c} \hline \multicolumn{2}{c|}{**Categories**} & \multicolumn{5}{c}{**NUP**} & \multicolumn{2}{c}{**LK**} \\ \hline  & & \multicolumn{2}{c|}{_brown_} & \multicolumn{2}{c|}{_transolar_} & \multicolumn{2}{c}{_EMLP_} & \multicolumn{2}{c}{_PVO(brown)_} & \multicolumn{2}{c}{_PVO(triangular)_} & \multicolumn{2}{c}{_GRL-SVO(NUP)_} \\ \hline \multirow{3}{*}{3-var(test)} & \(\#SI\) & 1620 & 1504 & 1686 & - & - & **1772** \\  & _AVG.T_ & 170.63 & 227.60 & 140.06 & - & - & **94.07** \\  & _AVG.N_ & 2421.18 & 2663.09 & 2384.29 & - & - & **2157.92** \\ \hline \multirow{3}{*}{4-var} & \(\#SI\) & 415 & 376 & - & 408 & 392 & **456** \\  & _AVG.T_ & 352.87 & 394.71 & - & 360.33 & 376.71 & **298.74** \\  & _AVG.N_ & 5258.7 & 5539.73 & - & 5338.45 & 536.28 & **5060.19** \\ \hline \multirow{3}{*}{5-var} & \(\#SI\) & 236 & 202 & - & 242 & 218 & **253** \\  & _AVG.T_ & 435.64 & 495.34 & - & 419.50 & 466.47 & **394.12** \\  & _AVG.N_ & 12465.41 & 14493.00 & - & 11932.31 & 12826.53 & **11586.65** \\ \hline \multirow{3}{*}{6-var} & \(\#SI\) & 175 & 149 & - & 180 & 160 & **204** \\  & _AVG.T_ & 500.62 & 551.15 & - & 489.57 & 526.80 & **432.64** \\  & _AVG.N_ & 20487.45 & 20068.2 & - & 20029.06 & **18993.53** & 19407.97 \\ \hline \multirow{3}{*}{7-var} & \(