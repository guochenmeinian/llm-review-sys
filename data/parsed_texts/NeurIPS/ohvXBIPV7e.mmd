# _Cspg_: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search

 Ming Yang, Yuzheng Cai, Weiguo Zheng

School of Data Science, Fudan University, China

{ yangm24, yuzhengcai21 } @m.fudan.edu.cn zhengweiguo@fudan.edu.cn

###### Abstract

The state-of-the-art approximate nearest neighbor search (ANNS) algorithm builds a large proximity graph on the dataset and performs a greedy beam search, which may bring many unnecessary explorations. We develop a novel framework, namely _crossing sparse proximity graph (CSPG)_, based on random partitioning of the dataset. It produces a smaller sparse proximity graph for each partition and routing vectors that bind all the partitions. An efficient two-staged approach is designed for exploring _CSPG_, with fast approaching and cross-partition expansion. We theoretically prove that _CSPG_ can accelerate the existing graph-based ANNS algorithms by reducing unnecessary explorations. In addition, we conduct extensive experiments on benchmark datasets. The experimental results confirm that the existing graph-based methods can be significantly outperformed by incorporating _CSPG_, achieving 1.5x to 2x speedups of _QPS_ in almost all recalls.

## 1 Introduction

_Nearest Neighbor Search_ (NNS) aims to find some vectors in a set of high-dimensional vectors with the smallest distance to a query vector. It is becoming increasingly popular in various application domains [1, 2, 3, 4, 5, 6, 7], such as information retrieval [8, 9], pattern recognition [10, 11], recommendation systems [12, 13], and retrieval augmented generation (RAG) [14, 15]. However, it is costly to find the exact nearest neighbors in practice, thus recent studies have focused on _Approximate Nearest Neighbor Search_ (ANNS), which targets efficiency while mildly relaxing accuracy constraints [16, 17].

Existing ANNS algorithms can be divided into four categories [16], including tree-based approaches [18, 19, 20, 21, 1, 22], hashing-based approaches [23, 24, 25, 26, 27], quantization-based approaches [28, 29, 30, 31], and graph-based approaches [6, 32, 33, 34, 35]. Among these approaches, graph-based ANNS algorithms stand out with the high answer quality and low latency [16], by constructing a _Proximity Graph (shorted as PG)_ on the given vector dataset. As shown in Figure 1, each vector is represented by a node in the graph, and each node is connected to its nearby neighbors.

For the greedy beam search over the proximity graph, it is observed that the distance computation dominates the overall time cost [36, 37]. Since at each step, all neighbors of the current node are pushed into the candidate set according to the computed distance. The number of distance computations can be calculated as \(\sigma\) multiplied by the number of explored nodes [6], where \(\sigma\) is the average degree of the graph. Intuitively, searching within a smaller graph requires less exploration and thus reduces the overall cost, which has been proved for a particular type of proximity graph, i.e., Monotonic Search Network (MSNET) [6, 38]. However, for most proximity graphs, it is very likely to degrade the answer quality when the graph is smaller.

In this paper, we present a novel and effective framework, namely _Crossing Sparse Proximity Graph (CSPG)_, enabling efficient search while not sacrificing answer quality. The basic idea is to reduce the number of explored vectors by searching in the much smaller graphs. Specifically, we randomly divide the whole dataset into several partitions, and for each partition, we construct a proximity graph that is smaller than the proximity graph built on the whole dataset. These partitions share a set of _routingvectors_ (Section 3.1) that allow the greedy search to travel across different partitions dynamically. The query process involves two stages, i.e., fast approaching and cross-partition expansion. The first stage conducts the greedy search within one partition, using a small candidate set to quickly approach the nearby regions of the query vector. Then, the second stage continues the greedy search with a larger candidate set, allowing it to travel across different partitions for more precise results.

We theoretically prove that the expected number of explored vectors when searching across these small graphs is the same as searching on the small proximity graph for one of the partitions. Hence, by random partitioning with randomly sampled routing vectors, we can reduce the number of explored vectors compared with the traditional proximity graph built on the whole dataset, thus reducing the number of distance computations. By integrating _CSPG_ with various graph-based ANNS algorithms, extensive experiments show that it significantly speeds up the query performance on benchmark datasets, and the detailed empirical results also align with our theoretical analysis.

Contributions.In summary, we make the following contributions in this paper.

* To improve the query performance by reducing the number of explored vectors, we propose a general framework, namely _Crossing Sparse Proximity Graph (CSPG)_, through random partitioning and random routing vectors. This framework can integrate with and enhance the existing graph-based ANNS indexes.
* We develop an efficient two-staged search paradigm over the _CSPG_, including fast approaching and cross-partition expansion.
* We theoretically prove that _CSPG_ can benefit the existing graph-based ANNS algorithms by introducing _Approximate Monotonic Search Network_ (AMSNET) that considers the distance backtracking in the search path.
* Extensive experiments confirm that by integrating the _CSPG_, the existing graph-based algorithms can be speeded up significantly under the same answer quality.

## 2 Background

### Problem definition

Let \(\mathcal{D}=\{v_{1},v_{2},...,v_{n}\}\) denote the dataset of \(n\) vectors, where \(v_{i}\) represents a vector in the \(d\)-dimensional Euclidean space \(\mathbb{R}^{d}\). The L2 distance between any two vectors \(p\in\mathbb{R}^{d}\) and \(q\in\mathbb{R}^{d}\) is denoted as \(\delta(p,q)\). The task of \(k\)-nearest neighbor search (\(k\)-NNS) can be defined as follows.

**Definition 1** (\(k\)-Nearest Neighbor Search, shorted as \(k\)-Nns).: _Given a dataset \(\mathcal{D}\) and a query vector \(q\), \(k\)-NNS returns a subset of \(k\) vectors, denoted by \(T\subseteq\mathcal{D}\), such that for any \(t\in T\) and \(v\in\mathcal{D}\setminus T\), we have \(\delta(v,q)\geq\delta(t,q)\)._

**Definition 2** (\(k\)-Approximate Nearest Neighbor Search, shorted as \(k\)-Anns ).: _Given a dataset \(\mathcal{D}\) and a query vector \(q\), \(k\)-ANNS returns a subset of \(k\) vectors, denoted by \(S\subseteq\mathcal{D}\), such that \(|S\cap T|\) is as large as possible, where \(T\) is the answer set to \(k\)-NNS w.r.t. the query \(q\). For simplicity, \(k\) is omitted when \(k=1\)._

In other words, the task of \(k\)-ANNS returns \(k\) approximate closest vectors of the query vector, not guaranteeing all the exact top-\(k\) nearest vectors, to improve query efficiency.

### Graph-based ANNS algorithms

As discussed above, graph-based ANNS algorithms conduct a best-first greedy beam search on the proximity graphs to approach the closest nodes for a query vector. Their built proximity graphs can be classified into four categories [16; 40] as follows. Please refer to Appendix A for more details.

Figure 1: An example dataset of vectors and its proximity graph.

**Delaunay Graph (DG) [41; 42]**. It ensures that for any edge, no other vectors will be situated within the hypersphere defined by an edge connecting two vectors, where the hypersphere is centered at the midpoint of the edge and the length of the edge is the diameter.

**Relative Neighborhood Graph (RNG)**[39]. It guarantees that for any edge between \(p\) and \(q\), no other vectors will reside within the \(lune(p,q)=\{u\in\mathbb{R}^{d}\,|\,\delta(u,p)\leq\delta(p,q)\wedge\delta(u,q) \leq\delta(p,q)\}\). RNG imposes stricter restrictions on its edges, thus decreasing the average degree [43].

**K-Nearest Neighbor Graph (KNNG)**[44]. In KNNG, neighbors of each vector \(v\in\mathcal{D}\) are its top-\(k\) nearest neighbors in \(\mathcal{D}\). NN-Descent [44] proposes a method for constructing KNNG.

**Minimum Spanning Tree (MST) Graph**[45]. The MST utilizes distances between vectors as edge weights. Then, it performs hierarchical clustering on the dataset multiple times randomly, adding some edges to the edge set. MST can establish global connectivity with a minimal number of edges.

### Monotonic Search Network

Monotonic Search Network provides theoretical results to understand the costs of greedy search.

**Definition 3** (Monotonic Path, shorted as MP [38]).: _Given a proximity graph built on dataset \(\mathcal{D}\), for two nodes \(p\) and \(u\) in the graph, a path from \(p\) to \(u\) is denoted as \(p\leadsto u=\{v_{1},v_{2},...,v_{t}\}\), where \(p=v_{1}\) and \(u=v_{t}\). It is a monotonic path iff it satisfies that \(\delta(v_{1},u)>\delta(v_{2},u)>...>\delta(v_{t-1},u)\)._

**Definition 4** (Monotonic Search Network, shorted as MSNET [38]).: _Given a dataset \(\mathcal{D}\) consisting of \(n\) vectors in the space \(\mathbb{R}^{d}\), a proximity graph built on \(\mathcal{D}\) is called a monotonic search network iff there exists at least one monotonic path from \(p\) to \(u\) for any two nodes \(p\) and \(u\) in \(\mathcal{D}\)._

When running a greedy beam search in an MSNET, we will continuously approach the query vector since the distance strictly decreases at each step, i.e., distance backtracking can be avoided [38]. Let \(C\) denote the smallest convex hull that can cover a set of \(n\)\(d\)-dimensional vectors \(\mathcal{D}\), and let \(R\) represent the maximum distance between two vectors in \(\mathcal{D}\). Denote the volume of \(C\) as \(V_{C}\) and let \(V_{B}(\cdot,R)\) represent the volume of a sphere with radius \(R\). For an MSNET built on \(\mathcal{D}\), when there exists a constant \(\kappa\) s.t. \(\kappa V_{C}\geq V_{B}(\cdot,R)\), which implies that the distribution of vectors should be relatively uniform (never in some extremely special shape), the search length expectation of an MSNET (denoted as \(\mathbb{E}^{M}\)) is \(\mathcal{O}\left(n^{\frac{1}{d}}\log n^{\frac{1}{d}}/\Delta r\right)\) as proved in [6], where

\(\Delta r=\min\limits_{v_{1},v_{2},v_{3}\in\mathcal{D}}\left\{|\delta\left(v_ {1},v_{2}\right)-\delta\left(v_{1},v_{3}\right)|,\;|\delta\left(v_{1},v_{2} \right)-\delta\left(v_{2},v_{3}\right)|,\;|\delta\left(v_{1},v_{3}\right)- \delta\left(v_{2},v_{3}\right)|\right\}.\)

In other words, \(\Delta r\) is the minimum distance difference for any non-isosceles triangle on \(\mathcal{D}\). As \(n\) increases, \(\Delta r\) decreases and approaches a constant value when \(n\) is large [6].

## 3 Crossing Sparse Proximity Graphs

### _Csp_G: Crossing Sparse Proximity Graphs

An effective approach to the \(k\)-ANNS problem is expected to identify more vectors that are closest to the query with a minimal cost. A straightforward approach is to relax edge selection by allowing a vector to connect with both nearby and relatively distant neighbors. To guarantee efficiency, the node degree cannot be increased too much, making it challenging to maintain both nearby and distant neighbors. To address the problem, we propose a method to maximize the number of vectors searched near the query without increasing average node degrees. The basic idea is randomly partitioning the dataset \(\mathcal{D}\) into multiple groups that share some common vectors, called _routing vectors_. Then a sparse proximity graph (shorted SPG) is built for each group of vectors. Since these SPGs are sparser than that built for the whole dataset \(\mathcal{D}\), allowing larger steps to approach the query quickly. Moreover, the _routing vectors_ across multiple SPGs enable efficient fine-grained search. For ease of presentation, let \(PG(\mathcal{D})\) denote the proximity graph built on the dataset \(\mathcal{D}\).

**Definition 5** (Random Partition).: _Given a vector dataset \(\mathcal{D}\), the group of subsets \(\mathcal{P}_{1},\mathcal{P}_{2},\cdots,\mathcal{P}_{m}\) constitute a random partition of \(\mathcal{D}\) such that (1) \(\mathcal{P}_{1}\cup\mathcal{P}_{2}\cup\cdots\cup\mathcal{P}_{m}=\mathcal{D}\), (2) \(\mathcal{P}_{1}\cap\mathcal{P}_{2}\cap\cdots\cap\mathcal{P}_{m}=\mathcal{C}\), and (3) \((\mathcal{P}_{i}\backslash\mathcal{C})\cap(\mathcal{P}_{i}\backslash\mathcal{C})=\emptyset\) for \(i\neq j\), where \(\mathcal{P}_{i}\) is randomly sampled from \(\mathcal{D}\) and \(\mathcal{C}\) is the common vectors shared by all the subsets (also called routing vectors)._

**Definition 6** (Crossing Sparse Proximity Graphs, shorted as _CSPG_).: _Given a vector dataset \(\mathcal{D}\), its CSPG(\(\mathcal{D}\)) consists of multiple proximity graphs \(\mathcal{G}_{1},\mathcal{G}_{2},\cdots,\mathcal{G}_{m}\) for \(\mathcal{D}\)'s random partition \(\mathcal{P}_{1},\mathcal{P}_{2},\cdots,\mathcal{P}_{m}\), respectively, i.e., \(\mathcal{G}_{i}=PG(\mathcal{P}_{i})\)._

Note that \(\mathcal{P}_{i}\) is sparser than \(\mathcal{D}\) as it is randomly sampled from \(\mathcal{D}\). Thus, the average edge length (i.e., the distance between two vectors) of the resultant proximity graph \(\mathcal{G}_{i}\) is larger than that of theproximity graph for \(\mathcal{D}\). Generally, any existing graph-based index can be used to build the proximity graphs in _CSPG_. Since the partitions share routing vectors, the corresponding proximity graphs are interrelated through these routing vectors. Hence, the routing vectors serve to navigate the greedy search across different proximity graphs. Let \(v_{j}^{i}\) denote that vector \(v_{j}\) belongs to graph \(\mathcal{G}_{i}\).

**Example 1**.: _For the dataset \(\mathcal{D}=\{v_{1},v_{2},...,v_{9}\}\) in Figure 1, we build the CSPG in Figure 2, by randomly sampling 2 routing vectors \(\mathcal{C}=\{v_{4},v_{7}\}\). And there are two partitions \(\mathcal{P}_{1}=\{v_{1},v_{3},v_{4},v_{5},v_{7},v_{9}\}\) and \(\mathcal{P}_{2}=\{v_{2},v_{4},v_{6},v_{7},v_{8}\}\), where routing vectors are highlighted in red, with the green graph representing \(\mathcal{G}_{1}\) and the blue graph representing \(\mathcal{G}_{2}\)._

### Novelty of _Cspg_

**Comparison with _Inverted File Index (IVF)_**. Generally, _IVF_ uses clustering algorithms (e.g., k-means) to divide the dataset into buckets. During the search, it selects some buckets with the closest centroids w.r.t. the query, after which vectors in such buckets will be scanned for final results. The buckets of _IVF_ index disrupt the distribution of vectors in the original dataset. In contrast, _CSPG_ preserves the original distribution by random partition, which diffuses all vectors, and the routing vectors are used for connecting all proximity graphs from different partitions.

**Comparison with _PG_(\(\mathcal{D}\)). The _CSPG_ index is built based on random partitions, with the help of routing vectors for connectivity. When the number of partitions \(m=1\), _CSPG_ falls into the special case that builds a _PG_ index over all vectors, which is consistent with most state-of-the-art graph-based ANNS algorithms. Some existing _PG_ index [46, 47] also utilized similar ideas of using data partition and redundancy. Existing studies [46, 47] uses k-means or other methods to divide the dataset, and obtain some redundant vectors. Then, such algorithms also construct proximity graphs in each partition separately, which are eventually _merged into a large PG_ as the final proximity graph covering all vectors. Such existing techniques are developed to deal with a huge amount of vectors, making it feasible to handle large datasets. In contrast, the _CSPG_ methods aim at building several proximity graphs to speed up query answering.

**Comparison with _Hnsw._** (1) From the perspective of redundancy, the lower level of _HNSW_ contains all vectors from the upper level. But in _CSPG_, there is a common overlap between partitions, and the remaining points of different partitions are distinct. (2) From the perspective of structure, _HNSW_ is a hierarchical structure. In contrast, _CSPG_ serves as a framework rather than a specific structure (hierarchical or flat), allowing to enhance query performance across a broad range of mainstream graph indices. _HNSW_ transitions from top level to bottom level unidirectionally, while _CSPG_ builds horizontally with smaller, sparser proximity graphs. (3) From the perspective of searching, _HNSW_ can only unidirectionally search each level from top to bottom, and the final results are obtained from the bottom-level graph. But _CSPG_ can traverse back and forth between different sparse proximity graphs, collecting final results from various partitions. The performance of _HNSW_ is closely tied to the quality of the bottom-level graph, while _CSPG_ generates more diverse and robust answers by leveraging cross-partition traversal and result collection.

### _Cspg_ index construction and updates

Algorithm 2 presents the process for _CSPG_ index construction. It first samples the routing vectors \(RV\) from dataset \(\mathcal{D}\), then the other vectors \(\mathcal{D}\backslash RV\) are randomly assigned to \(m\) partitions. Finally, for each partition \(\mathcal{P}_{i}\), we construct the proximity graph by applying the graph-based ANNS algorithm.

Since _CSPG_ is a framework based on mainstream proximity graphs, current updating methods of the underlying graph index are applicable. Moreover, the random partitioning makes it highly flexible for vector insertion and deletion. Details are presented in Algorithms 3 and 4 of Appendix B.

Figure 2: An example of _CSPG_ index, where the proximity graphs are built using relative neighborhood graph \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) (with very similar degree to Figure 0(b).

**Time and space complexity.** For dataset \(\mathcal{D}\) with \(n\) vectors, the time and space cost of building index are \(\mathcal{O}(T(n))\) and \(\mathcal{O}(S(n))\), respectively. As each partition has \(\frac{n(1-\lambda)}{m}+\lambda n\) vectors, _CSPG_ consumes \(\mathcal{O}\left(m\cdot T\left(\frac{n(1-\lambda)}{m}+\lambda n\right)\right)\) time and \(O\left(m\cdot S\left(\frac{n(1-\lambda)}{m}+\lambda n\right)\right)\) space, which is at the same order of magnitude comparing with the original costs for most graph-based ANNS algorithms.

## 4 Two-stage search on _Cspg_

The search process of _CSPG_ is divided into two stages, i.e., _fast approaching_ and _precise search_. Specifically, the first stage aims to quickly approach the query vector by using only one proximity graph, while the second stage will carefully search around by considering all partitions for the final closest vectors. Traditional beam search on a single proximity graph maintains a fixed-size candidate set. In contrast, _CSPG_ uses different sizes \(ef_{1}\) and \(ef_{2}\) for the two stages respectively, where \(ef_{1}<ef_{2}\). Algorithm 1 outlines the procedure of searching on the _CSPG_ index.

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), query vector \(q\), parameters \(ef_{1}\) and \(ef_{2}\)
0:\(k\) nearest neighbors of \(q\)
1:\(\mathcal{L}\leftarrow\{\) selected entry vector \(p\in\mathcal{P}_{1}\}\), \(visited\leftarrow\{p\}\)\(\triangleright\) First stage starts
2:while\(|\mathcal{L}|\neq 0\)do
3:\((r,h)\leftarrow\) the closest vector w.r.t. \(q\) in \(\mathcal{L}\)
4:for all unvisited neighbor \(u\) of \(r\) in \(\mathcal{G}_{1}\)do
5:\(\mathcal{L}\leftarrow\mathcal{L}\cup\{(u,1)\}\), \(visited\gets visited\cup\{u\}\)
6:if\(|\mathcal{L}|>ef_{1}\)then remove the farthest vectors w.r.t. \(q\) to keep \(|\mathcal{L}|=ef_{1}\)
7:\(p\leftarrow\) the closest vector w.r.t. \(q\) in \(visited\)\(\triangleright\) Second stage starts
8:\(\mathcal{L}\leftarrow\{(p,1)\}\),\(visited\leftarrow\{p\}\)
9:while\(|\mathcal{L}|\neq 0\)do
10:\((r,h)\leftarrow\) the closest vector w.r.t. \(q\) in \(\mathcal{L}\)
11:for all unvisited neighbor \(u\) of \(r\) in \(\mathcal{G}_{h}\)do
12:\(\mathcal{L}\leftarrow\mathcal{L}\cup\{(u,h)\}\), \(visited\gets visited\cup\{u\}\)
13:if\(u\) is a routing vector then\(\mathcal{L}\leftarrow\mathcal{L}\cup\{(u,\,i)\ |\ i\in\{1,2,...,m\}\wedge i\neq h\}\)
14:if\(|\mathcal{L}|>ef_{2}\)then remove the farthest vectors w.r.t. \(q\) to keep \(|\mathcal{L}|=ef_{2}\)
15:return top-\(k\) closest vectors w.r.t. \(q\) in \(visited\) ```

**Algorithm 1** Search on _CSPG_ index

In the first stage, the algorithm quickly approaches the query nearby with a shorter search length and fewer neighbor expansions. As shown in Algorithm 1, it conducts a greedy beam search discussed in Section 1 on the graph \(\mathcal{G}_{1}\).

**Example 2**.: _Given a dataset \(\mathcal{D}\) and a query vector \(\otimes\), We build CSPG and conduct the first stage search (\(ef_{1}=1\)) to approach the nearby region of the query within just 1 step (Figure 2)._

Each proximity graph in _CSPG(\(\mathcal{D}\))_ is smaller and sparser than the proximity graph for the whole dataset \(\mathcal{D}\) (denoted as _PG(\(\mathcal{D}\))_). This sparsity allows the first stage search to use larger steps and fewer moves to approximate the query. On the other hand, _CSPG_ uses a smaller candidate size \(ef_{1}\), eliminating some expansions that do not contribute to the final results.

### The Second Stage: cross-partition expansion for precise search

After the greedy search in the first stage, the candidate set contains the closest vector delivered in the first stage from the first partition \(\mathcal{P}_{1}\) w.r.t. the query vector. As shown in Algorithm 1, after resetting the visited set, it continues the greedy beam search with a size \(ef_{2}\) for the candidate set \(\mathcal{L}\). In the second stage, the significant difference from the first stage lies in line 13. Specifically, if the expanded neighbor \(u\) is a routing vector, all its instances in all partitions will be pushed into \(\mathcal{L}\). This approach allows the search process to dynamically traverse different proximity graphs, maximizing the search space to include as many potential closest vectors as possible.

**Example 3**.: _This example continues the search from Example 2. For comparison, let us consider the traditional greedy beam search within \(PG(\mathcal{D})\). As shown in Figure 0(b), it takes 6 steps to approach the nearest neighbor \(v_{9}\) (with a fixed candidate set size \(ef=3\)). For CSPG, it first conducts the first stage, then switches the candidate set size from \(ef_{1}=1\) to \(ef_{2}=3\) and enters the second stage for amore precise search in the query nearby as shown in Figure 2 (the two stages have 4 steps in total). In the second stage, CSPG expands the neighbors of \(v_{4}^{1}\), \(\mathcal{N}(v_{3}^{1})=\{v_{4}^{1}\}\), in \(\mathcal{G}_{1}\). Since \(v_{4}\) is a routing vector, both \(v_{4}^{2}\) and \(v_{4}^{1}\) are added to \(\mathcal{L}\). Next, the algorithm expands \(v_{4}^{2}\), updating \(\mathcal{L}\) to \(\{v_{1}^{1},v_{7}^{2},v_{4}^{1}\}\) as \(v_{7}\) is also a routing vector. Then, by expanding \(v_{7}^{1}\), we reach the closest node \(v_{9}^{1}\) in \(\mathcal{G}_{1}\)._

Due to the sparsity of each proximity graph in _CSPG_, the search on _CSPG_(\(\mathcal{D}\)) approaches the query results faster than _PG_(\(\mathcal{D}\)). Moreover, some expansions may be removed. For example, with a candidate size of \(ef_{2}=2\), the candidate set in Step 2 would be \(\{v_{7}^{1},v_{7}^{2}\}\), removing \(v_{4}^{1}\) due to the limited size, ignoring the unnecessary expansion to \(v_{5}^{1}\).

In _PG_(\(\mathcal{D}\)), redundant vectors are mainly used to merge graphs. In contrast, _CSPG_ leverages the distribution of routing vectors to ensure that the expansion of one graph aids in reducing expansions in other graphs. This means a position reached by one graph can be accessed by other graphs without additional expansion. For example, moving from \(v_{4}^{2}\) to \(v_{7}^{2}\) in \(\mathcal{G}_{2}\) allows continuing the search from \(v_{7}^{1}\) to \(v_{9}^{1}\) in the search across multiple graphs in the second stage appears as though it is conducted within a single proximity graph. The total number of steps to traverse the query nearby is 3, fewer than the 4 steps in _PG_(\(\mathcal{D}\)) while maintaining the same precision. In practice, with appropriate partitioning, _CSPG_ outperforms traditional _PG_ algorithms, as discussed in Section 6.

## 5 Analysis of search efficiency

For most graph-based ANNS algorithms, calculating the distance between two vectors usually dominates the overall search time. In this section, we focus on the number of distance computations during query processing, denoted by \(C\). We will show that the expected cost \(\mathbb{E}[C]\) for the proposed _CSPG_ method is lower than the traditional PG under certain constraints.

### The expected number of distance computations

Following the setting of previous work [6], assume that the start vector \(p\) and query vector \(q\) are randomly selected from the \(d\)-dimensional vector dataset. On the proximity graph \(\mathcal{G}\) built on the dataset, the greedy search sequence is denoted by \(p\rightsquigarrow q=\{v_{1},v_{2},...,v_{t}\}\), where \(v_{1}=p\), \(v_{t}=q\). Denote \(|p\rightsquigarrow q|\) as its length. The search sequence length of an MSNET equals its search path length, as the search consistently approaches the query without backtracking. Recall that when exploring each node, all its unvisited neighbors are pushed into the candidate set, and their distance to \(q\) is computed. By assuming that the average degree of \(\mathcal{G}\) is \(\sigma\), the expected number of distance computation is \(\mathbb{E}[C]=\sigma\mathbb{E}\left[|p\rightsquigarrow|\right]\).

### Expected search sequence length on Monotonic Search Network

As introduced in Section 2.3, for a Monotonic Search Network (MSNET), the expected length of the search sequence \(\mathbb{E}^{M}[|p\rightsquigarrow u|]=\mathcal{O}\left(n^{\frac{1}{d}}\log n^{ \frac{1}{d}}/\Delta r\right)\)[6]. In _CSPG_ schema, assume that there are \(m\) partitions (\(m\ll n\)), and the proximity graph \(\mathcal{G}_{i}\) on each partition \(\mathcal{P}_{i}\) is a MSNET. Since we randomly select routing vectors \(RV\) and then randomly divide the other vectors into \(m\) partitions, the distribution of vectors is the same in each partition. Then, we have the following theorem.

**Theorem 1**.: _Given a start vector \(s\in RV\) and a query vector \(q\in RV\), by performing greedy beam search from \(s\) on each MSNET \(\mathcal{G}_{i}\) independently, we can obtain \(m\) monotonic paths \(s^{i}\rightsquigarrow q^{i}\), where \(s^{i},q^{i}\in\mathcal{G}_{i}\). It holds that \(\mathbb{E}^{\mathcal{G}_{i}}[|s^{i}\rightsquigarrow q^{i}|]=\mathbb{E}^{ \mathcal{G}_{j}}[|s^{j}\rightsquigarrow q^{j}|]\) for \(1\leq i,j\leq m\)._

Proof.: Since the distribution of vectors in each graph \(\mathcal{G}_{i}\) are the same, the assumptions for deriving the expected path length in [6] remain unchanged, thus they have the same expected path length. 

By starting the search on a random entry vector \(p\) in proximity graph \(\mathcal{G}_{1}\), the routing vectors help us to travel across different partitions \(\mathcal{P}_{i}\) in the second stage. Thus, the search sequence of _CSPG_ is composed of several sub-sequences from different graphs \(\mathcal{G}_{i}\). The following theorem reveals that the expected search sequence length of _CSPG_ is the same as the case of searching on the _PG_(\(\mathcal{G}_{1}\)).

**Theorem 2**.: _Denote \(\mathbb{E}^{CSPG}[|p\rightsquigarrow q|]\) as the expected length of search sequence in CSPG. Denote \(\mathbb{E}^{\mathcal{G}_{i}}[|p\rightsquigarrow q|]\) as the expected sequence length when searching only on the graph \(\mathcal{G}_{i}\). It holds that_

\[\mathbb{E}^{CSPG}[|p\rightsquigarrow q|]=\mathbb{E}^{\mathcal{G}_{i}}[|p \rightsquigarrow q|].\]

Please refer to Appendix C for the detailed proofs. Based on Theorem 2, when the proximity graphs in _CSPG_ are MSNET, the expected search path length is

\[\mathbb{E}^{CSPG}\left[|p\rightsquigarrow q|\right]=\mathbb{E}^{\mathcal{G}_{i }}\left[|p\rightsquigarrow q|\right]=\mathcal{O}\left(\left(\lambda n+\frac{n(1- \lambda)}{m}\right)^{\frac{1}{d}}\log\left(\lambda n+\frac{n(1-\lambda)}{m} \right)^{\frac{1}{d}}/\Delta r\right).\]

### Approximate Monotonic Search Network

Most proximity graphs in practice are the approximation of MSNET, where the search path may have detours due to the lack of some necessary monotonic paths, resulting in distance backtracking. Given a query vector \(q\), we say that vector \(u\) conquers \(q\) iff \(\exists v\in\mathcal{N}(u),\delta(v,q)<\delta(u,q)\), denoted by \(u\succ q\) (\(u\neq q\)). For a certain vector \(u\) in the search path \(p\rightsquigarrow q\), distance backtracking is avoided iff \(u\succ q\), since when exploring \(u\), we can visit \(v\) to strictly decrease the distance w.r.t. \(q\).

Intuitively, for the proximity graph \(\mathcal{G}\), when the degree of every vector \(u\) is large enough, \(u\succ q\) is likely to be met for any query vector \(q\in\mathcal{G}\), which help to avoid distance backtracking. However, since the average degree is usually limited in practical proximity graphs, there is a probability that vector \(u\) conquers a random query vector \(q\in\mathcal{G}\), formally defined as \(\rho(u)=\frac{\sum_{q\in\mathcal{G}}\mathbb{1}(u\succ q)}{n}\)., where \(\mathbb{1}(u\succ q)=1\) iff \(u\succ q\). Next, we introduce the _Approximate Monotonic Search Network (AMSNET)_, which reduces distance backtracking by maximizing \(\rho(u)\) of each vector \(u\).

**Definition 7** (Approximate Monotonic Search Network, shorted as AMSNET).: _Given a dataset \(\mathcal{D}\) of \(n\) vectors, a proximity graph \(\mathcal{G}\) built on \(\mathcal{D}\) is called an approximate monotonic search network iff for every vector \(u\in\mathcal{D}\), its neighbor set \(\mathcal{N}(u)\) satisfies that \(|\mathcal{N}(u)|\leq\sigma\) while maximizing \(\rho(u)\)._

**Theorem 3**.: _For datasets with the same distribution, as the number of vectors \(n\) decreases, \(\rho(u)\) is monotonically non-decreasing._

Please refer to Appendix C for detailed proofs. Since AMSNET allows distance backtracking, there exists a detour factor \(w>1\) for the expected search sequence length. Specifically, when the underlying proximity graphs of _CSPG_ are AMSNETs, the expected search sequence length is \(\hat{\mathbb{E}}^{CSPG}\left[p\rightsquigarrow q\right]=\mathcal{O}\left(w \left(\lambda n+\frac{n(1-\lambda)}{m}\right)^{\frac{1}{d}}\log\left(\lambda n +\frac{n(1-\lambda)}{m}\right)^{\frac{1}{d}}/\Delta r\right)\). Moreover, according to Theorem 3, for every vector \(u\), as dataset size \(n\) decreases, \(\rho(u)\) is non-decreasing. In other words, the probability of distance backtracking at every vector is non-increasing as \(n\) decreases, thus \(w\) is non-increasing as \(n\) decreases. We confirm the monotonicity of \(w\) in Section 6.4.

### Speedup analysis for _Cspg_

For the dataset \(\mathcal{D}\) of \(n\) vectors, we have the following assumptions as discussed above: (1) \(\exists\kappa,\ \kappa V_{C}\geq V_{B}\left(\cdot,R\right)\). (2) \(m>1\), \(\lambda<1\), and the vector distribution of each partition \(\mathcal{P}_{i}\) is the same as \(\mathcal{D}\). (3) The proximity graphs built in _PG_ and _CSPG_ are AMSNETs with a degree upper bound of \(\sigma\).

When the proximity graph is AMSNET, the expected path length for _PG_ method is \(\hat{\mathbb{E}}^{PG}[|p\rightsquigarrow q\|]=\mathcal{O}(n^{\frac{1}{d}}\log n ^{\frac{1}{d}}/\Delta r^{PG})\), where \(\Delta r^{PG}\) is the minimum distance difference for any non-isosceles triangle on \(\mathcal{D}\). Similarly, \(\Delta r^{CSPG}\) is defined for each partition \(\mathcal{P}_{i}\) in _CSPG_. And the detour factor \(w\) for _PG_ and _CSPG_ are denoted as \(w^{PG}\) and \(w^{CSPG}\), respectively. Thus, considering the expected number of distance computations, the speedup ratio of _CSPG_ over _PG_ is

\[Speedup=\frac{\sigma\times\hat{\mathbb{E}}^{PG}[|p\rightsquigarrow q\|]}{ \sigma\times\hat{\mathbb{E}}^{CSPG}[|p\rightsquigarrow q\|]}=\left(\frac{w^{ PG}}{w^{CSPG}}\times\frac{\Delta r^{CSPG}}{\Delta r^{PG}}\right)\times\frac{n^{ \frac{1}{d}}\log n^{\frac{1}{d}}}{\left(\lambda n+\frac{n(1-\lambda)}{m}\right) ^{\frac{1}{d}}\log\left(\lambda n+\frac{n(1-\lambda)}{m}\right)^{\frac{1}{d}}}\]

Define \(\alpha=\frac{w^{PG}}{w^{CSPG}}\times\frac{\Delta r^{CSPG}}{\Delta r^{PG}}\). Since each proximity graph in _CSPG_ are smaller than that in _PG_, and Section 5.3 shows that \(w\) is non-increasing as \(n\) decreases, \(w^{PG}\geq w^{CSPG}\). Also, since \(\Delta r\) decreases as \(n\) increases [6], we have \(\Delta r^{CSPG}\geq\Delta r^{PG}\). Thus, it holds that \(\alpha\geq 1\). Next, we consider \(\beta=\frac{n^{\frac{1}{d}}\log n^{\frac{1}{d}}}{\left(\lambda n+\frac{n(1- \lambda)}{m}\right)^{\frac{1}{d}}\log\left(\lambda n+\frac{n(1-\lambda)}{m} \right)^{\frac{1}{d}}}\). In _CSPG_, each partition has less than \(n\) vectors when \(m>1\) and \(\lambda<1\), i.e., \(\lambda n+\frac{n(1-\lambda)}{m}<n\) and \(\beta>1\). Thus, \(Speedup=\alpha\beta>1\), which ensures that _CSPG_ always outperforms _PG_ when using _AMSNET_. As the dataset size \(n\) increases, \(\beta\) is decreasing and we have \(\lim\limits_{n\rightarrow\infty}Speedup=\alpha\left(\frac{m}{(m-1)\lambda+1} \right)^{\frac{1}{d}}\). Moreover, when \(n\rightarrow\infty\) and \(d\) is increasing, \(\beta\) is also decreasing and \(\lim_{d\rightarrow\infty}Speedup=\alpha\).

## 6 Evaluation

### Experimental setup

As summarized in Table 3, four benchmark datasets are used in our experiments, which are the most commonly used public datasets come from _Ann-benchmarks_[48].

[MISSING_PAGE_FAIL:8]

**Varying the number of partitions.** Since the number of partitions \(m\) affects the _CSPG_ index quality, we conduct experiments for evaluating the query performance with \(m=1,2,4,8,16\) partitions, in which \(m=1\) indicates the original graph-based ANNS algorithm without partition. Note that the other parameters are the same as the default settings. Intuitively, larger \(m\) results in fewer vectors in each partition, and _CSPG_ seems to achieve better performance. However, large \(m\) may decrease the similarity of vector distribution among different partitions, which contradicts the assumptions of the same vector distribution discussed in Section 5.4. Therefore, for different datasets with various distributions, choosing an appropriate parameter \(m\) is crucial. As shown in Figure 5, for SIFT1M dataset, dividing all vectors into \(2\) or \(4\) partition results in better QPS-recall curves. Figure 11 shows that the optimal value of \(m\) for the other datasets ranges from \(2\) to \(8\).

**Varying the sampling ratio.** When constructing the _CSPG_ schema, the sampling ratio \(\lambda\) is used to randomly sample \(\lambda n\) routing vectors before dataset partition. By using the default values of all the other parameters, Figure 6 reveals that for SIFT1M dataset, larger \(\lambda\) tends to improve the query performance of _CSPG_. And it also holds for other datasets, as shown in Figure 12. This is because more routing vectors help to navigate the search traveling across different partitions efficiently. Also, more routing vectors result in more shared vectors in each partition, increasing the similarity of vector distribution among different partitions, which is more aligned with the assumptions of the same vector distribution discussed in Section 5.4.

**Varying the candidate set sizes.**_CSPG_ has two parameters \(ef_{1}\) and \(ef_{2}\) for searching, limiting the size of the candidate set in the two stages, respectively. As shown in Figure 7 and Figure 13, we try different \(ef_{1}=1,2,4,8,16\) and obtain 5 QPS v.s. recall curves. The marks in each curve are obtained by varying the value of \(ef_{2}\) uniformly picked from \([10,300]\). In most cases, \(ef_{1}=1\) provides the best query performance, which aligns with our goal of the first stage fast approaching.

**Statistics for detour factor.** We randomly sample \(0.1\), \(0.2\), \(0.5\), \(2\), and \(5\) million vectors from SIFT10M dataset. By using _CSPG_ with the default parameter settings, at different \(Recall@k\), we obtain the empirical detour factor \(w=\frac{\text{length of search sequence}}{\text{length of search sequence}}\)

Figure 4: Query performance when varying the dataset size \(n\)

Figure 5: Query performance when varying the number of partitions \(m\)

Figure 6: Query performance when varying the sampling ratio \(\lambda\)search paths. As shown in Figure 8, larger \(n\) results in larger \(w\) at a fixed \(Recall@k\), which aligns with our discussion in Section 5.3 that \(w\) is non-increasing as \(n\) decreases.

### Hyperparameter grid search and evaluation with ANN-Benchmarks

To show the Pareto frontiers with the optimal hyperparameters, we conduct a grid search experiment to identify the most effective hyperparameter combinations for each baseline on SIFT1M dataset. As presented in Appendix D, we summarise the parameter selection sets for different indices in Table 4, and the results of the grid search experiment are illustrated in Figure 14.

Next, we conduct a more comprehensive evaluation using the ANN-Benchmarks [48], which is a widely used benchmarking environment. For each algorithm, we choose the optimal parameters from the grid search experiment with the highest QPS when Recall@10 \(=0.9\pm 5e^{-3}\) on SIFT1M dataset. As shown in Figure 9, enhanced with the proposed _CSPG_ framework, representative algorithms such as _HCNNG_, _HNSW_, _Vamana_ and _NSG_ can achieve better performance on SIFT1M dataset.

## 7 Conclusion

We proposed a novel graph-based indexing schema named _CSPG_ for Approximate Nearest Neighbor Search (ANNS), which is compatible with the current leading graph-based approaches in high-recall scenarios. Furthermore, we propose a novel search algorithm for the _CSPG_ schema, which uses a two-stage strategy and a cross-partition expansion to reduce meaningless expansion during the graph search and make the process more focused on the parts related to the answer. Next, we analyze the expectation of _CSPG_'s search amount, establish a speedup model, and prove that _CSPG_ can always have an advantage. Finally, we investigate the advantages of _CSPG_ through experiments and carry out a more detailed evaluation of the key factors affecting the performance of _CSPG_.

Figure 8: Detour factor when varying the dataset size \(n\)

Figure 7: Query performance when varying the candidate set size \(ef_{1}\) in the first stage

Figure 9: QPS v.s. Recall@10 curve on SIFT1M with the optimal parameters in ANN-Benchmarks

## Acknowledgments and Disclosure of Funding

This work was substantially supported Key Projects of the National Natural Science Foundation of China (Grant No. U23A20496) and Shanghai Science and Technology Innovation Action Plan (Grant No. 21511100401). Weiguo Zheng is the corresponding author.

## References

* [1] Akhil Arora, Sakshi Sinha, Piyush Kumar, and Arnab Bhattacharya. Hd-index: Pushing the scalability-accuracy boundary for approximate knn search in high-dimensional spaces. _arXiv preprint arXiv:1804.06829_, 2018.
* [2] Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. _IEEE transactions on pattern analysis and machine intelligence_, 42(4):824-836, 2018.
* [3] Kazuo Aoyama, Kazumi Saito, Hiroshi Sawada, and Naonori Ueda. Fast approximate similarity search based on degree-reduced neighborhood graphs. In _Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 1055-1063, 2011.
* [4] Minjia Zhang and Yuxiong He. Zoom: Ssd-based vector search for optimizing accuracy, latency and memory. _arXiv preprint arXiv:1809.04067_, 2018.
* [5] Rentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo, Qianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, et al. Manu: a cloud native vector database management system. _Proceedings of the VLDB Endowment_, 15(12):3548-3561, 2022.
* [6] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approximate nearest neighbor search with the navigating spreading-out graph. _arXiv preprint arXiv:1707.00143_, 2017.
* [7] Wenhui Zhou, Chunfeng Yuan, Rong Gu, and Yihua Huang. Large scale nearest neighbors search based on neighborhood graph. In _2013 International Conference on Advanced Cloud and Big Data_, pages 181-186. IEEE, 2013.
* [8] Chun Jiang Zhu, Tan Zhu, Haining Li, Jinbo Bi, and Minghu Song. Accelerating large-scale molecular similarity search through exploiting high performance computing. In _2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)_, pages 330-333. IEEE, 2019.
* [9] Myron Flickner, Harpreet Sawhney, Wayne Niblack, Jonathan Ashley, Qian Huang, Byron Dom, Monika Gorkani, Jim Hafner, Denis Lee, Dragutin Petkovic, et al. Query by image and video content: The qbic system. _computer_, 28(9):23-32, 1995.
* [10] Atsutake Kosuge and Takashi Oshima. An object-pose estimation acceleration technique for picking robot applications by using graph-reusing k-nn search. In _2019 First International Conference on Graph Computing (GC)_, pages 68-74. IEEE, 2019.
* [11] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. _IEEE transactions on information theory_, 13(1):21-27, 1967.
* [12] Yitong Meng, Xinyan Dai, Xiao Yan, James Cheng, Weiwen Liu, Jun Guo, Benben Liao, and Guangyong Chen. Pmd: An optimal transportation-based user distance for recommender systems. In _Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part II 42_, pages 272-280. Springer, 2020.
* [13] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering recommendation algorithms. In _Proceedings of the 10th international conference on World Wide Web_, pages 285-295, 2001.
* [14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.

* [15] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_, 2023.
* [16] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. _arXiv preprint arXiv:2101.12631_, 2021.
* [17] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and Xuemin Lin. Approximate nearest neighbor search on high dimensional data--experiments, analyses, and improvement. _IEEE Transactions on Knowledge and Data Engineering_, 32(8):1475-1488, 2019.
* [18] Keinosuke Fukunaga and Patrenahalli M. Narendra. A branch and bound algorithm for computing k-nearest neighbors. _IEEE transactions on computers_, 100(7):750-753, 1975.
* [19] Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. The r*-tree: An efficient and robust access method for points and rectangles. In _Proceedings of the 1990 ACM SIGMOD international conference on Management of data_, pages 322-331, 1990.
* [20] Chanop Silpa-Anan and Richard Hartley. Optimised kd-trees for fast image descriptor matching. In _2008 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1-8. IEEE, 2008.
* [21] Jon Louis Bentley. Multidimensional binary search trees used for associative searching. _Communications of the ACM_, 18(9):509-517, 1975.
* [22] Hosagrahar V Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang. distance: An adaptive b+-tree based indexing method for nearest neighbor search. _ACM Transactions on Database Systems (TODS)_, 30(2):364-397, 2005.
* [23] Qiang Huang, Jianlin Feng, Yikai Zhang, Qiong Fang, and Wilfred Ng. Query-aware locality-sensitive hashing for approximate nearest neighbor search. _Proceedings of the VLDB Endowment_, 9(1):1-12, 2015.
* [24] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing. In _Vldb_, pages 518-529, 1999.
* [25] Jinyang Gao, Hosagrahar Visvesvaraya Jagadish, Wei Lu, and Beng Chin Ooi. Dsh: data sensitive hashing for high-dimensional k-nnsearch. In _Proceedings of the 2014 ACM SIGMOD international conference on Management of data_, pages 1127-1138, 2014.
* [26] Yingfan Liu, Jiangtao Cui, Zi Huang, Hui Li, and Heng Tao Shen. Sk-lsh: an efficient index structure for approximate nearest neighbor search. _Proceedings of the VLDB Endowment_, 7(9):745-756, 2014.
* [27] Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. _Advances in neural information processing systems_, 21, 2008.
* [28] Fabien Andre, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. Cache locality is not enough: High-performance nearest neighbor search with product quantization fast scan. In _42nd International Conference on Very Large Data Bases_, page 12, 2016.
* [29] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate nearest neighbor search. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2946-2953, 2013.
* [30] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. _IEEE transactions on pattern analysis and machine intelligence_, 33(1):117-128, 2010.
* [31] Jianyang Gao and Cheng Long. Rabitq: Quantizing high-dimensional vectors with a theoretical error bound for approximate nearest neighbor search. _Proceedings of the ACM on Management of Data_, 2(3):1-27, 2024.

* [32] Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, and Vladimir Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. _Information Systems_, 45:61-68, 2014.
* [33] Sunil Arya and David M Mount. Approximate nearest neighbor queries in fixed dimensions. In _SODA_, volume 93, pages 271-280. Citeseer, 1993.
* [34] Yubao Wu, Ruoming Jin, and Xiang Zhang. Fast and unified local search for random walk based k-nearest-neighbor query in large graphs. In _Proceedings of the 2014 ACM SIGMOD international conference on Management of Data_, pages 1139-1150, 2014.
* [35] Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and Hong Zhang. Fast approximate nearest-neighbor search with k-nearest neighbor graph. In _Twenty-Second International Joint Conference on Artificial Intelligence_, 2011.
* [36] Zeyu Wang, Haoran Xiong, Zhenying He, Peng Wang, et al. Distance comparison operators for approximate nearest neighbor search: Exploration and benchmark. _arXiv preprint arXiv:2403.13491_, 2024.
* [37] Jianyang Gao and Cheng Long. High-dimensional approximate nearest neighbor search: with reliable and efficient distance comparison operations. _Proceedings of the ACM on Management of Data_, 1(2):1-27, 2023.
* [38] DW Dearholt, N Gonzales, and G Kurup. Monotonic search networks for computer vision databases. In _Twenty-Second Asilomar Conference on Signals, Systems and Computers_, volume 2, pages 548-553. IEEE, 1988.
* [39] Godfried T Toussaint. The relative neighbourhood graph of a finite planar set. _Pattern recognition_, 12(4):261-268, 1980.
* [40] Yun Peng, Byron Choi, Tsz Nam Chan, Jianye Yang, and Jianliang Xu. Efficient approximate nearest neighbor search in multi-dimensional databases. _Proceedings of the ACM on Management of Data_, 1(1):1-27, 2023.
* [41] Franz Aurenhammer. Voronoi diagrams--a survey of a fundamental geometric data structure. _ACM Computing Surveys (CSUR)_, 23(3):345-405, 1991.
* [42] Ben Harwood and Tom Drummond. Fanng: Fast approximate nearest neighbour graphs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5713-5722, 2016.
* [43] Jerzy W Jaromczyk and Godfried T Toussaint. Relative neighborhood graphs and their relatives. _Proceedings of the IEEE_, 80(9):1502-1517, 1992.
* [44] Wei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for generic similarity measures. In _Proceedings of the 20th international conference on World wide web_, pages 577-586, 2011.
* [45] Javier Vargas Munoz, Marcos A Goncalves, Zanoni Dias, and Ricardo da S Torres. Hierarchical clustering-based graphs for large scale approximate nearest neighbor search. _Pattern Recognition_, 96:106970, 2019.
* [46] Suhas Jayaram Subramanya, Fru Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. _Advances in Neural Information Processing Systems_, 32, 2019.
* [47] Jing Wang, Jingdong Wang, Gang Zeng, Zhuowen Tu, Rui Gan, and Shipeng Li. Scalable k-nn graph construction for visual descriptors. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1106-1113. IEEE, 2012.
* [48] Martin Aumuller, Erik Bernhardsson, and Alexander Faithfull. ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. _Information Systems_, 87:101374, 2020.
* [49] Magdalen Dobson, Zheqi Shen, Guy E Blelloch, Laxman Dhulipala, Yan Gu, Harsha Vardhan Simhadri, and Yihan Sun. Scaling graph-based anns algorithms to billion-size datasets: A comparative analysis. _arXiv preprint arXiv:2305.04359_, 2023.

Related graph-based ANNS algorithms

As discussed above, graph-based ANNS algorithms conduct a best-first greedy beam search on the proximity graphs to approach the closest nodes for a query vector. Their built proximity graphs can be classified into four categories [16, 40] as follows.

**Delaunay Graph (DG) [41, 42]**. It ensures that for any edge, no other vectors will be situated within the hypersphere defined by an edge connecting two vectors, where the hypersphere is centered at the midpoint of the edge and the length of the edge is the diameter. But when the dimension \(d\) is large, DG tends to become a complete graph [42], rapidly increasing the costs of greedy search.

**Relative Neighborhood Graph (RNG)**[39]. It guarantees that for any edge between \(p\) and \(q\), no other vectors will reside within the \(lune(p,q)=\{u\in\mathbb{R}^{d}\,|\,\delta(u,p)\leq\delta(p,q)\wedge\delta(u,q) \leq\delta(p,q)\}\). Compared to DG, RNG imposes stricter restrictions on its edges, thus decreasing the average degree [43]. Various works are based on RNG, including the Monotonic Relative Neighbor Graph (MRNG) [6] and FANNG [42].

**K-Nearest Neighbor Graph (KNNG)**[44]. In KNNG, neighbors of each vector \(v\in\mathcal{D}\) are its top-\(k\) nearest neighbors in \(\mathcal{D}\), avoiding too many explored neighbors during searching. Since each node in KNNG has only \(k\) neighbors, it cannot achieve the same level of connectivity as DG. NN-Descent [44] proposes a method for constructing KNNG. For constructing KNNG, NN-Descent [44] was proposed, which starts from a random graph and iteratively refines the graph towards the KNNG.

**Minimum Spanning Tree (MST) Graph**[45]. The MST utilizes distances between vectors as edge weights. Then, it performs hierarchical clustering on the dataset multiple times randomly, each time adding some edges to the edge set. MST can establish global connectivity with a minimal number of edges but may result in detours during searching.

Most well-known graph-based ANNS algorithms, including _HNSW_[2], _Vamana_[46], _HCNNG_[45], and _NSG_[6], do not exactly fit into one of the four categories. Instead, they usually evolve from one or more of these proximity graphs. For example, _Vamana_ evolved from the combination of two prior algorithms. _HNSW_ originated from both the DG and RNG algorithms, while _NSG_ originated solely from the RNG algorithm.

## Appendix B Algorithms and illustrations

```
0: dataset \(\mathcal{D}=\{v_{1},v_{2},...,v_{n}\}\), number of partitions \(m\), and sampling ratio \(\lambda\in[0,1]\)
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},..,\mathcal{G}_{m}\}\), routing vectors \(RV\)
1:\(\mathcal{P}_{i}\leftarrow\emptyset\) for \(1\leq i\leq m\), \(RV\leftarrow\emptyset\)
2:\(RV\leftarrow\) random \(\lfloor n\times\lambda\rfloor\) samples from \(\mathcal{D}\)
3:for each vector \(v\) in \(\mathcal{D}\setminus RV\)do
4: add \(v\) to a random sampled partition \(\mathcal{P}_{i}\)
5:\(\mathcal{G}\leftarrow\) construct a proximity graph \(\mathcal{G}_{i}\) for each partition \(\mathcal{P}_{i}\)
6:return\(\mathcal{G}\) and \(RV\) ```

**Algorithm 2**_CSPG_ index construction

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:assign a random \(i\), insert \(x\) to \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
2:if\(x\) is selected as route vector then
3: insert \(x\) into \(RV\)
4:for each \(j\in\{1,2,...,m\}\) s.t. \(i\neq j\)do
5: insert \(x\) to \(\mathcal{G}_{j}\) with the insertion method of the underlying graph index ```

**Algorithm 3**_CSPG_ vector insertion

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:for each \(i\in\{1,2,...,m\}\) s.t. \(x\in\mathcal{G}_{i}\)do
2: remove \(x\) from \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
3:if\(x\) is a route vector then
4: remove \(x\) from \(RV\) ```

**Algorithm 4**_CSPG_ vector deletion

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:for each \(i\in\{1,2,...,m\}\) s.t. \(x\in\mathcal{G}_{i}\)do
2: remove \(x\) from \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
3:if\(x\) is a route vector then
4: remove \(x\) from \(RV\) ```

**Algorithm 5**_CSPG_ index construction

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:assign a random \(i\), insert \(x\) to \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
2:if\(x\) is selected as route vector then
3: insert \(x\) into \(RV\)
4:for each \(j\in\{1,2,...,m\}\) s.t. \(i\neq j\)do
5: insert \(x\) to \(\mathcal{G}_{j}\) with the insertion method of the underlying graph index
6: remove \(x\) from \(RV\) ```

**Algorithm 6**_CSPG_ vector insertion

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:assign a random \(i\), insert \(x\) to \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
2:if\(x\) is selected as route vector then
4: insert \(x\) into \(RV\)
5: insert \(x\) into \(RV\)
6:if\(x\) is selected as route vector then
7: remove \(x\) from \(RV\) ```

**Algorithm 7**_CSPG_ vector insertion

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:assign a random \(i\), insert \(x\) to \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
2:if\(x\) is selected as route vector then
8: insert \(x\) into \(RV\)
9: insert \(x\) into \(RV\)
10: insert \(x\) into \(RV\)
11:if\(x\) is selected as route vector then
12: remove \(x\) from \(RV\) ```

**Algorithm 8**_CSPG_ vector insertion

```
0:CSPG index \(\mathcal{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},...,\mathcal{G}_{m}\}\), routing vectors \(RV\), and vector \(x\)
1:assign a random \(i\), insert \(x\) to \(\mathcal{G}_{i}\) with the insertion method of the underlying graph index
2:if\(x\) is selected as route vector then
3: insert \(x\) into \(RV\)
4: insert \(x\) into \(RV\)
5: insert \(x\) into \(RV\)
6: insert \(x\) into \(RV\)

## Appendix C Proofs for Section 5

**Theorem 2**.: Denote \(\mathbb{E}^{CSPG}[|p\rightsquigarrow q|]\) as the expected length of search sequence in _CSPG_. Denote \(\mathbb{E}^{\mathcal{G}_{i}}[|p\rightsquigarrow q|]\) as the expected sequence length when searching only on the graph \(\mathcal{G}_{i}\). It holds that

\[\mathbb{E}^{CSPG}[|p\rightsquigarrow q|]=\mathbb{E}^{\mathcal{G}_{i}}[|p \rightsquigarrow q|].\]

Proof.: For the greedy search in _CSPG_ starting from \(\mathcal{G}_{i}\), denote \(\vartheta\in RV\) as the first routing vector that the search sequence visits. Thus, \(\Pr(\vartheta\text{ not exists})+\sum_{s\in RV}\Pr(\vartheta=s)=1\), where \(\vartheta\) not exists indicates that the sequence never visits a routing vector before reaching the query vector \(q\). Then, we have

\[\mathbb{E}^{CSPG}\left[|p\rightsquigarrow q|\right]= \Pr(\vartheta\text{ not exists})\mathbb{E}^{\mathcal{G}_{i}} \left[|p\rightsquigarrow q|\right]+\] \[\sum_{s\in RV}\Pr(\vartheta=s)\mathbb{E}^{CSPG}\left[|p\rightsquigarrow q |\right]\] \[= \Pr(\vartheta\text{ not exists})\mathbb{E}^{\mathcal{G}_{i}} \left[|p\rightsquigarrow q|\right]\vartheta\text{ not exists}\right]+\] \[\sum_{s\in RV}\Pr(\vartheta=s)\left(\mathbb{E}^{\mathcal{G}_{i}} \left[|p\rightsquigarrow s|\right]+\mathbb{E}^{CSPG}\left[|s\rightsquigarrow q |\right]\right).\]

Note that the term of \(\mathbb{E}^{CSPG}\left[|s\rightsquigarrow q|\right]\) can be further expanded similarly. Since any search sequence is bounded by length \(n\), such expansion can be done iteratively in a finite number. Consider the last expansion of

\[\mathbb{E}^{CSPG}\left[|s^{\prime}\rightsquigarrow q|\right]= \Pr(\vartheta\text{ not exists})\mathbb{E}^{\mathcal{G}_{i}} \left[|s^{\prime}\rightsquigarrow q|\right]\vartheta\text{ not exists}\right]+\] \[\sum_{s^{\prime\prime}\in RV}\Pr(\vartheta=s^{\prime\prime}) \left(\mathbb{E}^{\mathcal{G}_{i}}\left[|s^{\prime}\rightsquigarrow s^{\prime \prime}|\right]+\mathbb{E}^{CSPG}\left[|s^{\prime\prime}\rightsquigarrow q| \right]\right).\]

The search path \(s^{\prime\prime}\rightsquigarrow q\) lies in one of the graphs. According to Theorem 1, since the path \(s\rightsquigarrow q\) has the same expected length in any graph, it holds that \(\mathbb{E}^{CSPG}\left[|s^{\prime\prime}\rightsquigarrow q|\right]=\mathbb{E }^{\mathcal{G}_{i}}\left[|s^{\prime\prime}\rightsquigarrow q|\right]\). Thus,

\[\mathbb{E}^{CSPG}\left[|s^{\prime}\rightsquigarrow q|\right]= \Pr(\vartheta\text{ not exists})\mathbb{E}^{\mathcal{G}_{i}} \left[|s^{\prime}\rightsquigarrow q|\right]\vartheta\text{ not exists}\right]+\] \[\sum_{s^{\prime\prime}\in RV}\Pr(\vartheta=s^{\prime\prime}) \left(\mathbb{E}^{\mathcal{G}_{i}}\left[|s^{\prime}\rightsquigarrow s^{\prime \prime}|\right]+\mathbb{E}^{\mathcal{G}_{i}}\left[|s^{\prime\prime}\rightsquigarrow q |\right]\right)\] \[= \Pr(\vartheta\text{ not exists})\mathbb{E}^{\mathcal{G}_{i}} \left[|s^{\prime}\rightsquigarrow q|\right]\vartheta\text{ not exists}\right]+\] \[\sum_{s^{\prime\prime}\in RV}\Pr(\vartheta=s^{\prime\prime}) \mathbb{E}^{\mathcal{G}_{i}}\left[|s^{\prime}\rightsquigarrow q|\right]\vartheta=s ^{\prime\prime}\] \[= \mathbb{E}^{\mathcal{G}_{i}}\left[|s^{\prime}\rightsquigarrow q| \right].\]

By recursively apply such induction, we can prove that \(\mathbb{E}^{CSPG}[|p\rightsquigarrow q|]=\mathbb{E}^{\mathcal{G}_{i}}[|p \rightsquigarrow q|]\). 

**Theorem 3**.: For datasets with the same distribution, as the number of vectors \(n\) decreases, \(\rho(u)\) is monotonically non-decreasing.

\begin{table}
\begin{tabular}{l l l} \hline Step & _PG_s Candidate Set & _CSPG_s Candidate Set \\ \hline
0 & \(\{v_{3}\}\) & \(\{v_{3}^{1}\}\) \\
1 & \(\{v_{4}\}\) & \(\{v_{4}^{2},v_{4}^{1}\}\) \\
2 & \(\{v_{6},v_{5}\}\) & \(\{v_{7}^{1},v_{7}^{2},v_{4}^{1}\}\) \\
3 & \(\{v_{7},v_{5}\}\) & \(\{v_{8}^{1},v_{7}^{2},v_{4}^{1}\}\) \\
4 & \(\{v_{8},v_{8},v_{5}\}\) & \\ \hline \end{tabular}
\end{table}
Table 2: Candidate Set for the search in the query nearby (dashed parallelogram).

Proof.: Given an AMSNET \(\mathcal{G}\) build on a dataset \(\mathcal{D}\) with \(n\) vectors, for each vector \(u\in\mathcal{G}\), \(\rho(u)\) is the probability that \(u\) conquers a random vector in \(\mathcal{G}\). Consider the case that we randomly remove a vector \(q^{*}\) from \(\mathcal{D}\). For the AMSNET \(\mathcal{G}^{\prime}\) built on dataset \(\mathcal{D}\backslash\{q^{*}\}\), denote \(\rho^{\prime}(u)\) as the probability that \(u\) conquers a random vector in \(\mathcal{G}^{\prime}\). Next, we show that \(\rho^{\prime}(u)\geq\rho(u)\).

In AMSNET \(\mathcal{G}\), since \(q^{*}\) is randomly sampled from \(\mathcal{D}\), the probability that \(u\succ q^{*}\) is \(\rho(u)\). Consider the following two cases.

* If \(u\succ q^{*}\) holds in \(\mathcal{G}\), \(\sum_{q\in\mathcal{G}^{\prime}}\mathbb{I}(u\succ q)\geq\sum_{q\in\mathcal{G}} \mathbb{I}(u\succ q)-1\). It is because \(\mathcal{G}^{\prime}\) does not have \(q^{*}\), and for building the AMSNET \(\mathcal{G}^{\prime}\) we can adjust the neighbors of \(u\) and \(u\) may conquer more vectors in \(\mathcal{G}^{\prime}\).
* If \(u\succ q^{*}\) does not hold in \(\mathcal{G}\), \(\sum_{q\in\mathcal{G}^{\prime}}\mathbb{I}(u\succ q)=\sum_{q\in\mathcal{G}} \mathbb{I}(u\succ q)\), since we can use the same neighbors of \(u\) for both \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\).

Therefore, we have

\[\rho^{\prime}(u)= \frac{\rho(u)\left[\sum_{q\in\mathcal{G}^{\prime}}\mathbb{I}(u \succ q)\right]+(1-\rho(u))\left[\sum_{q\in\mathcal{G}^{\prime}}\mathbb{I}(u \succ q)\right]}{n-1}\] \[\geq \frac{\rho(u)\left[\sum_{q\in\mathcal{G}}\mathbb{I}(u\succ q)-1 \right]+(1-\rho(u))\left[\sum_{q\in\mathcal{G}}\mathbb{I}(u\succ q)\right]}{ n-1}\] \[= \frac{\sum_{q\in\mathcal{G}}\mathbb{I}(u\succ q)-\rho(u)}{n-1}= \frac{n\rho(u)-\rho(u)}{n-1}=\rho(u).\]

## Appendix D Experiments settings and results

The detailed index construction parameters are listed as follows, which are from previous studies [49] and _Ann-benchmarks_[48].

* _HNSW_. The degree upper bound \(M=32\), and the \(efConstruction=128\).
* _Vamana_. The degree upper bound \(R=32\), beam size \(L=128\), and pruning parameter \(\alpha=1.2\).
* _HCNNG_. The number of cluster trees \(T=10\), the leaf size of _MST_\(Ls=1000\), and the _MST_ degree \(s=5\).
* _CSPG_. For fairness, for each graph-based algorithm used in _CSPG_, we slightly adjust its parameters to ensure the average degree of the built graphs is the same as the original algorithm.

The parameters \(ef\) for each baseline algorithm and the \(ef_{2}\) for _CSPG_ method are uniformly picked from \([10,300]\) to obtain _QPS_ at different _Recall_. For _CSPG_ method, we set parameter \(ef_{1}=1\) and sampling ratio \(\lambda=0.5\) by default, while their impacts on performance are discussed in Section 6.4.

\begin{table}
\begin{tabular}{c r r r r} \hline
**Dataset** & **Dimension** & **Data type** & **\# Base** & **\# Query** \\ \hline SIFT1M & 128 & float & 1,000,000 & 10,000 \\ GIST1M & 960 & float & 1,000,000 & 1,000 \\ DEEP1M & 96 & float & 1,000,000 & 10,000 \\ SIFT10M & 128 & uint8 & 10,000,000 & 10,000 \\ \hline \end{tabular}
\end{table}
Table 3: Statistics of DatasetsFigure 11: Query performance when varying the number of partitions \(m\)

Figure 13: Query performance when varying the candidate set size \(ef_{1}\) in the first stage

Figure 10: Number of distance computation v.s. recall curves for comparing query performance

Figure 14: QPS v.s. Recall@10 over different parameters combination

\begin{table}
\begin{tabular}{l|l} \hline
**Baseline** & **Parameters Selection Sets** \\ \hline _Vamana_ & \(R\in\{16,32,64\},\alpha\in\{1.0,1.2,1.4\},efConstruction\in\{64,128,256\}\) \\ \hline _HNSW_ & \(M\in\{16,32,64\},efConstruction\in\{64,128,256\}\) \\ \hline _NSG_ & \(R\in\{16,32,64\},efConstruction\in\{64,128,256\}\) \\ \hline _HCNNG_ & \(s\in\{3,5,7\},T\in\{5,10,15\},Ls\in\{750,1000,1250\}\) \\ \hline \end{tabular}
\end{table}
Table 4: Hyperparameters selection sets

Figure 12: Query performance when varying the sampling ratio \(\lambda\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As claimed in the abstract and introduction, this article does present a graph index schema _CSPG_ that can be utilized to speed up graph-based ANNS algorithms. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In our theoretical analysis (Section 5), we assume that the vector distribution of each partition in _CSPG_ remains consistent. However, practically _CSPG_ can still improve query performance by random partitioning, which ensures the similarity of vector distributions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All the assumptions of our theoretical analysis are presented in Section 5, and we provide all the proofs in Appendix C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Appendix D provides all the parameters, environment settings, and the implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our datasets are all commonly used datasets in previous ANNS studies. And all our experimental codes are available with detailed guidelines for reproducing. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details including hyperparameters in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Error bars are not commonly used in our research field, and we ensure that all experimental results are measured with multiple times and we remove outliers. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All our experimental environments and hardware-related technical descriptions are detailed in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the relevant guidelines of NeurIPS Code of Ethics carefully and strictly follow them. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our research on ANNS can significantly enhance the efficiency and scalability of various real-world applications, leading to improved user experiences and advancements in technology.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

1. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our research is not related to any unsafe datasets or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

1. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The license and terms of use are explicitly mentioned and respected. Creators and original owners of all assets used in the paper are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our research is not related to this issue. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our study for Approximate Nearest Neighbor Search and the datasets involved are not related to this issue. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our research is not related to this issue. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.