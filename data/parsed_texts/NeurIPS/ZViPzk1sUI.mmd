# Structured Semidefinite Programming

for Recovering Structured Preconditioners

 Arun Jambulapati

Simons Institute

jmblpati@berkeley.edu

&Christopher Musco

New York University

cmusco@nyu.edu

&Jerry Li

Microsoft Research

jerrl@microsoft.com

&Kirankumar Shiragur

Broad Institute of MIT and Harvard

shiragur@stanford.edu

&Aaron Sidford

Stanford University

sidford@stanford.edu

&Kevin Tian

University of Texas at Austin

kjtian@cs.utexas.edu

Work completed at Stanford and the University of Washington.Work completed at Stanford.Work completed at Stanford and Microsoft Research.

###### Abstract

We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including:

* **Diagonal preconditioning.** We give an algorithm which, given positive definite \(\mathbf{K}\in\mathbb{R}^{d\times d}\) with \(\operatorname{nnz}(\mathbf{K})\) nonzero entries, computes an \(\epsilon\)-optimal diagonal preconditioner in time \(\widetilde{O}(\operatorname{nnz}(\mathbf{K})\cdot\operatorname{poly}(\kappa^{ \star},\epsilon^{-1}))\), where \(\kappa^{\star}\) is the optimal condition number of the rescaled matrix.
* **Structured linear systems.** We give an algorithm which, given \(\mathbf{M}\in\mathbb{R}^{d\times d}\) that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in \(\mathbf{M}\) in \(\widetilde{O}(d^{2})\) time.

Our diagonal preconditioning results improve state-of-the-art runtimes of \(\Omega(d^{3.5})\) attained by general-purpose semidefinite programming, and our solvers improve state-of-the-art runtimes of \(\Omega(d^{\omega})\) where \(\omega>2.3\) is the current matrix multiplication constant. We attain our results via new algorithms for a class of semidefinite programs (SDPs) we call _matrix-dictionary approximation SDPs_, which we leverage to solve an associated problem we call _matrix-dictionary recovery_.

## 1 Introduction

Preconditioning is a fundamental primitive in the theory and practice of numerical linear algebra, optimization, and data science. Broadly, its goal is to improve conditioning properties (e.g., the range of eigenvalues) of a matrix \(\mathbf{M}\) by finding another matrix \(\mathbf{N}\) which approximates the inverse of \(\mathbf{M}\) and is more efficient to construct and apply than computing \(\mathbf{M}^{-1}\). This strategy underpins a variety of popular recently-developed tools, such as adaptive gradient methods for machine learning (e.g., Adagrad and Adam [1, 1]), and near-linear time solvers for combinatorially-structuredmatrices (e.g., graph Laplacians [14]). Despite widespread practical adoption of such techniques, there is a surprising lack of provably efficient algorithms for preconditioning.

Our work introduces a new tool, _matrix-dictionary recovery_, and leverages it to obtain the first near-linear time algorithms for several structured preconditioning problems in well-studied applications. Informally, the problem we study is as follows (see Section 4 for the formal definition).

\[\begin{split}&\text{\emph{Given a matrix }}\mathbf{M}\text{ \emph{and a "matrix-dictionary"}}\{\mathbf{M}_{i}\}\text{, \emph{find the best preconditioner}}\\ &\mathbf{N}=\sum_{i}w_{i}\mathbf{M}_{i}\text{\emph{ of }}\mathbf{M}\text{ \emph{ expressible as a nonnegative linear combination of }}\{\mathbf{M}_{i}\}.\end{split}\] (1)

We develop general-purpose solvers for the problem (1). We further apply these solvers to obtain state-of-the-art algorithms for fundamental tasks such as preconditioning linear systems and regression, and approximately recovering structured matrices, including the following results.

* **Diagonal preconditioning.** We consider the classical numerical linear algebra problem of _diagonal preconditioning_[13]. Given \(\mathbf{K}\in\mathbb{S}_{\succ 0}^{d}\), the goal is to find a diagonal \(\mathbf{W}\in\mathbb{S}_{\succ 0}^{d}\) minimizing the condition number of \(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}}\). Theorem 1 obtains the first near-linear time algorithms for this problem when the optimal condition number of the rescaled matrix is small.
* **Semi-random regression.** We consider a related problem, motivated by semi-random noise models, which takes full-rank \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(n\geq d\) and seeks \(\mathbf{W}\in\mathbb{S}_{\succ 0}^{n}\) minimizing the condition number of \(\mathbf{A}^{\top}\mathbf{W}\mathbf{A}\). Theorem 2 gives the first near-linear time algorithm for this problem, and applications of it reduce risk bounds for statistical linear regression.
* **Structured linear systems.** We robustify Laplacian system solvers, e.g., [14], to obtain near-linear time solvers for systems in dense matrices well-approximated spectrally by Laplacians in Theorem 3. We also give new near-linear time solvers for several families of structured matrices, e.g., dense inverse Laplacians and M-matrices,4 in Theorems 4 and 5. Footnote 4: Inverse M-matrices are necessarily dense, see Appendix B of the supplementary material.

For the preconditioning problems considered in Theorems 1, 2, and 3, we give the first runtimes faster than a generic SDP solver, for which, state-of-the-art runtimes [14, 15] are highly superlinear (\(\Omega(d^{3.5})\) for diagonal preconditioning and \(\Omega(d^{2\omega})\) for approximating Laplacians, where \(d\) is the matrix dimension and \(\omega>2.3\) is the current matrix multiplication constant [1]). For the corresponding linear system solving problems in each case, as well as in Theorems 4 and 5, the prior state-of-the-art was to treat the linear system as generic and ran in \(\Omega(d^{\omega})\) time.

**Organization.** We begin by overviewing the main applications of our matrix-dictionary recovery framework in Sections 2 and 3, which respectively cover our results on diagonal preconditioning and structured linear algebra. These sections are self-contained (with some definitions of the matrix families we study in Section 3 deferred to the supplementary material), and can be read independently. In Section 4, we contextualize and formalize the general matrix-dictionary recovery problem (1) we introduce and study. We also provide our main meta-algorithm and its guarantees, and an overview of how the results of Sections 2 and 3 follow from applications of it. We finally compare our framework to related algorithms and give a more thorough runtime comparison in Section 5.

**Notation.** In Section 2 (focusing on diagonal preconditioning and semi-random regression) only, we refer to the matrices to be preconditioned as \(\mathbf{K}\) or \(\mathbf{A}\). This is for consistency with the numerical linear algebra literature, where \(\mathbf{K}\) represents a positive definite kernel matrix, and \(\mathbf{A}\) denotes the data in a regression problem \(\min_{x}\|\mathbf{A}x-b\|_{2}\). In the rest of the paper, our notation will be consistent with (1). The \(d\times d\) symmetric matrices are \(\mathbb{S}^{d}\), the positive semidefinite (PSD) and definite (PD) cones are \(\mathbb{S}_{\succeq 0}^{d}\) and \(\mathbb{S}_{\succeq 0}^{d}\); the remainder of our notation is standard and deferred to Section 2 of the supplement.

## 2 Diagonal preconditioning

When solving linear systems via iterative methods, one of the most popular preconditioning strategies is to use a diagonal matrix. This is appealing because diagonal matrices can be applied and inverted quickly. Determining the best diagonal preconditioner is a classical numerical linear algebra problem studied since the 1950s [13, 14, 15], and has gained recent popularity due to its use in adaptive gradient methods [1, 12]. In Section 4, we discuss how diagonal preconditioningis an instance of (1) in the matrix-dictionary \(\mathbf{M}_{i}=e_{i}e_{i}^{\top}\), where \(e_{i}\) is the \(i^{\text{th}}\) basis vector. Leveraging this viewpoint, we design algorithms for two natural instantiations of diagonal preconditioning.

**Outer scaling.** One formulation of the optimal diagonal preconditioning problem, which we refer to as _outer scaling_, asks to optimally reduce the condition number of positive definite \(\mathbf{K}\in\mathbb{R}^{d\times d}\) with a diagonal matrix \(\mathbf{W}\), i.e., return diagonal \(\mathbf{W}=\mathbf{diag}\left(w\right)\) for \(w\in\mathbb{R}_{>0}^{d}\) such that5

Footnote 5: \(\kappa(\mathbf{M})\) is the condition number of positive definite \(\mathbf{M}\), i.e. the eigenvalue ratio \(\lambda_{\max}(\mathbf{M})/\lambda_{\min}(\mathbf{M})\).

\[\kappa(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}})\approx \kappa_{o}^{\star}(\mathbf{K}):=\min_{\text{diagonal}\,\mathbf{W}\succ\mathbf{ 0}}\kappa(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}}).\]

Given \(\mathbf{W}\), a solution to \(\mathbf{K}x=b\) can be obtained by solving the better-conditioned \(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}}y=\mathbf{W}^{\frac {1}{2}}b\) and returning \(x=\mathbf{W}^{\frac{1}{2}}y\). The optimal \(\mathbf{W}\) can be obtained via a semidefinite program (SDP) [13], but the computational cost of general-purpose SDP solvers outweighs benefits for solving linear systems. Outer scaling is poorly understood algorithmically; prior to our work, even attaining a constant-factor approximation to \(\kappa_{o}^{\star}(\mathbf{K})\) without a generic SDP solver was unknown.

This state of affairs has resulted in the widespread use of heuristics for constructing \(\mathbf{W}\), such as _Jacobi preconditioning_[12, 13] and _matrix scaling_[1, 10, 11]. The former strategy, where the preconditioner is taken as the inverse diagonal to \(\mathbf{K}\), was notably highlighted by Adagrad [1], which used Jacobi preconditioning to improve computational costs. However, both heuristics have clear drawbacks from theoretical or practical perspectives.

Prior to our work the best approximation guarantee known for Jacobi preconditioning was a result of van der Sluis [12, 13], which shows the Jacobi preconditioner is an \(m\)-factor approximation to the optimal preconditioning problem where \(m\leq d\) is the maximum number of non-zeros in any row of \(\mathbf{K}\): in dense matrices this is linear in the problem dimension and can be much larger than \(\kappa_{o}^{\star}(\mathbf{K})\). We review and slightly strengthen this result in Appendix C of the supplement. We also prove a new _dimension-independent_ baseline result of independent interest: the Jacobi preconditioner obtains condition number no worse than \((\kappa_{o}^{\star}(\mathbf{K}))^{2}\). Unfortunately, we exhibit a family of matrices showing this bound is tight, dashing hopes they solve outer scaling near-optimally. On the other hand, while sometimes effective as a heuristic [12], matrix scaling algorithms target a different objective (normalizing row and column sums) and do not yield provable guarantees on \(\kappa(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}})\).

**Inner scaling.** Another formulation of diagonal preconditioning, which we refer to as _inner scaling_, takes as input a full-rank \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and asks to find an \(n\times n\) positive diagonal \(\mathbf{W}\) with

\[\kappa(\mathbf{A}^{\top}\mathbf{W}\mathbf{A})\approx\kappa_{i}^{\star}( \mathbf{A}):=\min_{\text{diagonal}\,\mathbf{W}\succ\mathbf{0}}\kappa(\mathbf{A} ^{\top}\mathbf{W}\mathbf{A}).\]

As a comparison, when outer scaling is applied to the kernel matrix \(\mathbf{K}=\mathbf{A}^{\top}\mathbf{A}\), \(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}}\) can be seen as rescaling the columns of \(\mathbf{A}\). On the other hand, in inner scaling we instead rescale rows of \(\mathbf{A}\). Inner scaling has natural applications to improving risk bounds in a robust statistical variant of linear regression, which we comment upon shortly. Nonetheless, as in the outer scaling case, no algorithms faster than general SDP solvers are known to obtain even a constant-factor approximation to \(\kappa_{i}^{\star}(\mathbf{A})\). Further, despite clear problem similarities, it is unclear how to best extend heuristics (e.g., Jacobi preconditioning and matrix scaling) for outer scaling to inner scaling.

**Our results.** We give the first nontrivial approximation algorithms (beyond generic SDP solvers) for both the outer and inner scaling problems, yielding diagonal preconditioners attaining constant-factor approximations to \(\kappa_{o}^{\star}\) and \(\kappa_{i}^{\star}\) in near-linear time.6\(\mathcal{T}_{\text{mv}}(\mathbf{M})\) denotes the time required to multiply a vector by \(\mathbf{M}\); this is at most the sparsity of \(\mathbf{M}\), but can be substantially faster for structured \(\mathbf{M}\).

Footnote 6: We are not currently aware of a variant of our matrix dictionary recovery framework which extends to simultaneous inner and outer scaling, though it is worth noting that prior work [13] does obtain such a result via semidefinite programming. Obtaining such a variant is an interesting open problem for future work.

**Theorem 1** (Outer scaling).: _Let \(\epsilon>0\) be a fixed constant.7 There is an algorithm, which given full-rank \(\mathbf{K}\in\mathbb{S}_{\succ 0}^{d}\) computes \(w\in\mathbb{R}_{\geq 0}^{d}\) such that \(\kappa(\mathbf{W}^{\frac{1}{2}}\mathbf{K}\mathbf{W}^{\frac{1}{2}})\leq(1+ \epsilon)\kappa_{o}^{\star}(\mathbf{K})\) with probability \(\geq 1-\delta\) in time \(O(\mathcal{T}_{\text{mv}}(\mathbf{K})\cdot(\kappa_{o}^{\star}(\mathbf{K}))^{1.5}\cdot\text{polylog}(\frac{d\kappa_{o}^{\star}(\mathbf{K})}{\delta}))\)._

Footnote 7: We do not focus on the \(\epsilon\) dependence and instead take it to be constant since, in applications involving solving linear systems, there is little advantage to obtaining better than a \(2\)-approximation (i.e., \(\epsilon=1\)).

**Theorem 2** (Inner scaling).: _Let \(\epsilon>0\) be a fixed constant. There is an algorithm, which given full-rank \(\mathbf{A}\in\mathbb{R}^{n\times d}\) for \(n\geq d\) computes \(w\in\mathbb{R}_{\geq 0}^{n}\) such that \(\kappa(\mathbf{A}^{\top}\mathbf{W}\mathbf{A})\leq(1+\epsilon)\kappa_{i}^{ \star}(\mathbf{A})\) with probability \(\geq 1-\delta\) in time \(O(\mathcal{T}_{\text{mv}}(\mathbf{A})\cdot(\kappa_{i}^{\star}(\mathbf{A}))^{1.5}\cdot\text{polylog}(\frac{n\kappa_{i}^{\star}(\mathbf{A})}{\delta}))\)._Our methods pay a small polynomial overhead in the quantities \(\kappa^{*}_{o}\) and \(\kappa^{*}_{i}\), but notably suffer no dependence on the _original conditioning_ of the matrices. Typically, the interesting use case for diagonal preconditioning is when \(\kappa^{*}_{o}(\mathbf{K})\) or \(\kappa^{*}_{i}(\mathbf{A})\) is small but \(\kappa(\mathbf{K})\) or \(\kappa(\mathbf{A}^{\top}\mathbf{A})\) is large, a regime where our runtimes are near-linear and substantially faster than directly applying iterative methods.

It is worth noting that in light of our new results on Jacobi preconditioning, the end-to-end runtime of Theorem 1 for solving linear systems (rather than optimal preconditioning) can be improved: accelerated gradient methods on a preconditioned system with condition number \((\kappa^{*}_{o})^{2}\) have runtimes scaling as \(\kappa^{*}_{o}\). That said, when repeatedly solving multiple systems in the same matrix, Theorem 1 may offer an advantage over Jacobi preconditioning. Our framework also gives a potential route to achieve the optimal end-to-end runtime scaling as \(\sqrt{\kappa^{*}_{o}}\), detailed in Appendix D of the supplement.

Beyond that which is obtainable by black-box using general SDP solvers, we are not aware of any other claimed runtime in the literature for solving the inner and outer scaling problems considered in Theorems 1 and 2. Directly using state-of-the-art SDP solvers [JKL\({}^{+}\)20, HJS\({}^{+}\)22] incurs substantial overhead \(\Omega(n^{\omega}\sqrt{d}+nd^{2.5})\) or \(\Omega(n^{\omega}+d^{4.5}+n^{2}\sqrt{d})\), where \(\omega<2.372\) is the current matrix multiplication constant [11, 6, 10, 12]. For outer scaling, where \(n=d\), this implies an \(\Omega(d^{3.5})\) runtime; for other applications, e.g., preconditioning \(d\times d\) perturbed Laplacians where \(n=d^{2}\), the runtime is \(\Omega(d^{2\omega})\). Applying state-of-the-art approximate SDP solvers (rather than our custom ones, i.e., Theorems 6 and 7) appears to yield runtimes \(\Omega(\operatorname{nnz}(\mathbf{A})\cdot d^{2.5})\), as described in Appendix E.2 of [10]. This is in contrast with our Theorems 1, 2 which achieve \(\widetilde{O}\left(\operatorname{nnz}(\mathbf{A})\cdot(\kappa^{\star})^{1.5}\right)\). Hence, we improve existing tools by \(\operatorname{poly}(d)\) factors in the main regime of interest where the optimal rescaled condition number \(\kappa^{\star}\) is small. Concurrent to our work, [QGH\({}^{+}\)22] gave algorithms for constructing optimal diagonal preconditioners using interior point methods for SDPs, which run in at least the superlinear times discussed previously.

**Statistical aspects of preconditioning.** Unlike an outer scaling, a good inner scaling does not speed up a least squares regression problem \(\min_{x}\|\mathbf{A}x-b\|_{2}\). Instead, it allows for a faster solution to the reweighted problem \(\min_{x}\|\mathbf{W}^{\frac{1}{2}}(\mathbf{A}x-b)\|_{2}\). This has a number of implications from a statistical perspective. We explore an interesting connection between inner scaling preconditioning and _semi-random_ noise models for least-squares regression, situated in the literature in Section 5.

As a motivating example of our noise model, consider the case when there is a hidden parameter vector \(x_{\text{true}}\in\mathbb{R}^{d}\) that we want to recover, and we have a "good" set of consistent observations \(\mathbf{A}_{g}x_{\text{true}}=b_{g}\), in the sense that \(\kappa(\mathbf{A}_{g}^{\top}\mathbf{A}_{g})\) is small. Here, we can think of \(\mathbf{A}_{g}\) as being drawn from a well-conditioned distribution. Now, suppose an adversary gives us a superset of these observations \((\mathbf{A},b)\) such that \(\mathbf{A}x_{\text{true}}=b\), and \(\mathbf{A}_{g}\) are an (unknown) subset of rows of \(\mathbf{A}\), but \(\kappa(\mathbf{A}^{\top}\mathbf{A})\gg\kappa(\mathbf{A}_{g}^{\top}\mathbf{A} _{g})\). This can occur when rows are sampled from heterogeneous sources. Perhaps counterintuitively, by giving additional consistent data, the adversary can arbitrarily hinder the cost of iterative methods. This failure can be interpreted as being due to overfitting to generative assumptions (e.g., sampling rows from a well-conditioned covariance, instead of a mixture): standard iterative methods assume too much structure, where ideally they would use as little as information-theoretically possible.

Our inner scaling methods can be viewed as "robustifying" linear system solving to such semi-random noise models (by finding \(\mathbf{W}\) yielding a rescaled condition number comparable or better than the indicator of the rows of \(\mathbf{A}_{g}\), which are not known a priori). In Section 6 of the supplement, we demonstrate applications of inner scaling in reducing the mean-squared error risk in statistical regression settings encompassing our semi-random noise model, where the observations \(b\) are corrupted by (homoskedastic or heteroskedastic) noise. In all settings, our preconditioning algorithms yield computational gains, improved risk bounds, or both, by factors of roughly \(\kappa(\mathbf{A}^{\top}\mathbf{A})/\kappa^{\star}_{i}(\mathbf{A})\).

## 3 Robust linear algebra for structured matrices

Over the past decade, the theoretical computer science and numerical linear algebra communities have dedicated substantial effort to developing faster solvers for regression problems in various families of combinatorially-structured matrices. Perhaps the most prominent example is [13], who gave a near-linear time solver for linear systems in graph Laplacian matrices.8 A long line of exciting work has obtained improved solvers for these systems [10, 11, 12, 13, 14, 15, 16, 17, 18], which have been used to improve the runtimes for a wide variety of graph-structured problems, including maximum flow [14, 15, 16], sampling random spanning trees [10, 17, 15, 18], graph clustering [16, 17, 18], and more [19, 18, 17, 16, 15, 14]. Additionally, efficient linear system solvers have been developed for solving systems in other types of structured matrices, e.g., block diagonally dominant systems [10, 11], M-matrices [1], and directed Laplacians [14, 15, 16, 17, 18].

**Perturbations of structured matrices.** Despite the importance of these matrices with combinatorial structure, previously-developed solvers are in some ways quite brittle. For example, there are simple matrix families closely related to Laplacians for which the best-known runtimes for solving linear systems are achieved by ignoring problem structure, and using generic matrix multiplication techniques as a black box. Perhaps the simplest example is solving systems in _perturbed Laplacians_, i.e., matrices which admit constant-factor approximations by a Laplacian matrix, but which are not Laplacians themselves. This situation can arise when a Laplacian is used to approximate a physical phenomenon [1]. We show that the framework we develop for (1) yields, as a consequence, robustifications and recovery routines building upon previously-developed solvers for structured linear systems. As a first example, we give the following perturbed Laplacian solver.

**Theorem 3** (Perturbed Laplacian solver).: _Let \(\mathbf{M}\succeq\mathbf{0}\in\mathbb{R}^{n\times n}\) be such that there exists an (unknown) Laplacian \(\mathbf{L}\) with \(\mathbf{M}\preceq\mathbf{L}\preceq\kappa^{*}\mathbf{M}\), and that \(\mathbf{L}\) corresponds to a graph with edge weights between \(w_{\min}\) and \(w_{\max}\), with \(\frac{w_{\max}}{w_{\min}}\leq U\). For any \(\delta\in(0,1)\) and \(\epsilon>0\), there is an algorithm recovering a Laplacian \(\mathbf{L}^{\prime}\) with \(\mathbf{M}\preceq\mathbf{L}^{\prime}\preceq(1+\epsilon)\kappa^{*}\mathbf{M}\) with probability \(\geq 1-\delta\) in time \(O(n^{2}\cdot(\kappa^{*})^{2}\cdot\text{poly}(\frac{\log\frac{n\kappa^{*}U}{ \epsilon}}{\epsilon}))\). Consequently, there is an algorithm for solving linear systems in \(\mathbf{M}\) to \(\epsilon\)-relative accuracy with probability \(\geq 1-\delta\), in time \(O(n^{2}\cdot(\kappa^{*})^{2}\cdot\text{poly}(\frac{\log\frac{n\kappa^{*}U}{ \delta\epsilon}}{\epsilon}))\)._9__

Footnote 9: See (6) and the following discussion for the definition of solving to relative accuracy.

Theorem 3 can be viewed as solving a preconditioner construction problem, where we know there exists a Laplacian matrix \(\mathbf{L}\) which spectrally resembles \(\mathbf{M}\), and wish to efficiently recover a Laplacian with similar guarantees. Our matrix-dictionary recovery framework (1) captures the setting of Theorem 3 by leveraging an appropriate matrix-dictionary of edge Laplacians, discussed in Section 4. The conceptual message of Theorem 3 is that near-linear time solvers for Laplacians robustly extend through our preconditioning framework to efficiently solve matrices approximated by Laplacians. Beyond this specific application, our framework could be used to solve perturbed generalizations of future families of structured matrices.

**Recovery of structured matrices.** In addition to directly spectrally approximating and solving in matrices which are well-approximated by preconditioners with diagonal or combinatorial structure, our framework also yields solvers for new families of matrices. We show that our preconditioning techniques can be used in conjunction with properties of graph-structured matrices to provide solvers and spectral approximations for _inverse M-matrices_ and _Laplacian pseudoinverses_. Recovering Laplacians from their pseudoinverses and solving linear systems in the Laplacian pseudoinverse arise when trying to fit a graph to data or recover a graph from effective resistances, a natural distance measure (see [13] for motivation and discussion of related problems). More broadly, the problem of solving linear systems in inverse symmetric M-matrices is prevalent and corresponds to statistical inference problems involving distributions that are multivariate totally positive of order 2 (\(\operatorname{MTP}_{2}\)) [16, 17, 18, 19]. Our main results are the following.

**Theorem 4** (M-matrix recovery and inverse M-matrix solver).: _Let \(\mathbf{M}\) be the inverse of an unknown invertible symmetric M-matrix, let \(\kappa\) upper bound its condition number, and let \(U\) be the multiplicative range of \(\mathbf{M}\mathbf{1}\).10 For any \(\delta\in(0,1)\) and \(\epsilon>0\), there is an algorithm recovering a \((1+\epsilon)\)-spectral approximation to \(\mathbf{M}^{-1}\) in time \(O(n^{2}\cdot\text{poly}(\frac{\log\frac{n\kappa U}{\epsilon}}{\epsilon}))\). Consequently, there is an algorithm for solving linear systems in \(\mathbf{M}\) to \(\epsilon\)-relative accuracy with probability \(\geq 1-\delta\), in time \(O(n^{2}\cdot\text{poly}(\frac{n\kappa U}{\delta\epsilon}))\)._

Footnote 10: \(\mathbf{M}\mathbf{1}\) is entrywise positive, as shown in Appendix B of the supplement.

**Theorem 5** (Laplacian recovery and Laplacian pseudoinverse solver).: _Let \(\mathbf{M}\) be the pseudoinverse of unknown Laplacian \(\mathbf{L}\), and that \(\mathbf{L}\) corresponds to a graph with edge weights between \(w_{\min}\) and \(w_{\max}\), with \(\frac{w_{\max}}{w_{\min}}\leq U\). For any \(\delta\in(0,1)\) and \(\epsilon>0\), there is an algorithm recovering a Laplacian \(\mathbf{L}^{\prime}\) with \(\mathbf{M}^{\dagger}\preceq\mathbf{L}^{\prime}\preceq(1+\epsilon)\mathbf{M}^{\dagger}\) in time \(O(n^{2}\cdot\text{poly}(\frac{\log\frac{nU}{\epsilon}}{\epsilon}))\). Consequently, there is an algorithm for solving linear systems in \(\mathbf{M}\) to \(\epsilon\)-relative accuracy with probability \(\geq 1-\delta\), in time \(O(n^{2}\cdot\text{poly}(\frac{nU}{\delta\epsilon}))\)._Theorems 4 and 5 are perhaps a surprising demonstration of the utility of our techniques: just because a matrix family is well-approximated by structured preconditioners, it is not a priori clear that their inverses also are. However, we show that by applying recursive preconditioning tools in conjunction with our recovery methods, we can obtain analogous results for these inverse families. These results add to the extensive list of combinatorially-structured matrix families admitting efficient linear algebra primitives. We view our approach as a proof-of-concept of further implications in designing near-linear time system solvers for structured families via algorithms for (1).

Similarly to our results in Section 2, our results on solving matrix-dictionary recovery for graph-structured matrices (Theorems 3, 4, and 5) are the first we are aware of with runtimes improving upon black-box generic algorithms. In particular, for key matrices in each of these cases (e.g., constant-factor spectral approximations of Laplacians, inverse M-matrices, and Laplacian pseudoinverses) we obtain \(\widetilde{O}(n^{2})\) time algorithms for solving linear systems in these matrices to inverse polynomial accuracy. This runtime is near-linear when the input is dense and in each case when the input is dense the state-of-the-art prior methods were to run general linear system solvers using \(O(n^{\omega})\) time.

## 4 Matrix-dictionary recovery: a general preconditioning framework

Our general strategy for matrix-dictionary recovery, i.e., recovering preconditioners in the sense of (1), is via applications of a new custom approximate solver we develop for a family of structured SDPs. SDPs are fundamental optimization problems that have been the source of extensive study for decades [23], with numerous applications across operations research and theoretical computer science [24], statistical modeling [25, 26], and machine learning [13]. Though there have been recent advances in solving general SDPs (e.g., [12, 13, 14] and references therein), the current state-of-the-art solvers have superlinear runtimes, prohibitive in large-scale applications. Consequently, there has been extensive research on designing faster approximate SDP solvers under different assumptions [25, 26, 15, 16, 17].

We now provide context for our solver for structured "matrix-dictionary approximation" SDPs, state our algorithm and its guarantees, and summarize how it is used to obtain Theorems 1, 2, 3, 4, and 5.

**Positive SDPs.** One prominent class of structured SDPs are what we refer to as _positive SDPs_, namely SDPs in which the cost and constraint matrices are all positive semidefinite (PSD), a type of structure present in many important applications [24, 25, 26, 27, 28, 19, 20], including those in this paper. Positive SDPs generalize positive linear programming, itself a well-studied problem over the past several decades [12, 25, 26, 27]. It was recently shown that a prominent special case of positive SDPs known as _packing SDPs_ can be solved in nearly-linear time [1, 13, 18], a fact that has had numerous applications in robust learning and estimation [1, 1, 28, 29, 20] as well as in combinatorial optimization [13]. However, extending known packing SDP solvers to broader classes of positive SDPs, e.g. mixed packing-covering SDPs has been elusive [11, 12], and is a key open problem in the algorithmic theory of structured optimization.11 The mixed packing-covering SDP problem is parameterized by "packing" and "covering" matrices \(\{\mathbf{P}_{i}\}_{i\in[n]},\mathbf{P},\{\mathbf{C}_{i}\}_{i\in[n]},\mathbf{ C}\in\mathbb{S}_{-\mathbf{0}}^{d}\), and asks to find the smallest \(\mu>0\) such that there exists \(w\in\mathbb{R}_{\geq 0}^{n}\) with \(\sum_{i\in[n]}w_{i}\mathbf{P}_{i}\preceq\mu\mathbf{P}\) (packing into \(\mu\mathbf{P}\)) and \(\sum_{i\in[n]}w_{i}\mathbf{C}_{i}\succeq\mathbf{C}\) (covering \(\mathbf{C}\)). Redefining \(\mathbf{P}_{i}\leftarrow\frac{1}{\mu}\mathbf{P}^{-\frac{1}{2}}\mathbf{P}_{i} \mathbf{P}^{-\frac{1}{2}}\), \(\mathbf{C}_{i}\leftarrow\mathbf{C}^{-\frac{1}{2}}\mathbf{C}_{i}\mathbf{C}^{- \frac{1}{2}}\) for all \(i\in[n]\), (a slight strengthening of) this problem is equivalent to finding \(w\in\mathbb{R}_{\geq 0}^{n}\) such that

Footnote 11: A faster solver for general positive (mixed packing-covering) SDPs was claimed in [12], but an error was later discovered in that work, as is recorded in the most recent arXiv version [12].

\[\sum_{i\in[n]}w_{i}\mathbf{P}_{i}\preceq\mathbf{I}\preceq\sum_{i\in[n]}w_{i} \mathbf{C}_{i},\] (2)

or refuting its existence. This was studied by [11, 12], and an important open problem in structured convex programming is designing a "width-independent" solver for testing feasibility of (2) up to a \(1+\epsilon\) factor (i.e. testing whether (2) is approximately feasible with an iteration count polynomial in \(\epsilon^{-1}\) and polylogarithmic in other parameters). Such solvers have remained elusive beyond pure packing SDPs [13, 18, 19], even for basic extensions such as pure covering.

**Matrix-dictionary approximation SDPs.** In Theorem 6, we develop our main meta-algorithm, an efficient solver for specializations of (2) where the packing and covering matrices \(\{\mathbf{P}_{i}\}_{i\in[n]},\{\mathbf{C}_{i}\}_{i\in[n]}\)are multiples of each other. As we will see, this family of structured SDPs, which we call _matrix-dictionary approximation SDPs_, is highly effective for capturing the forms of approximation required by (1). All our aforementioned preconditioning results follow via careful applications of our matrix-dictionary approximation SDP solver in Theorem 6 (and a generalization of it in Theorem 7).

Specifically, we develop efficient algorithms for the following main meta-problem we study. Given a set of matrices (a "matrix-dictionary") \(\{\mathbf{M}_{i}\}_{i\in[n]}\in\mathbb{S}_{\geq 0}^{d}\), a constraint matrix \(\mathbf{B}\in\mathbb{S}_{>0}^{d}\), a tolerance parameter \(\epsilon\in(0,1)\), and \(\kappa^{\star}\geq 1\) such that there exists \(w^{\star}\in\mathbb{R}_{\geq 0}^{n}\) with

\[\mathbf{B}\preceq\sum_{i\in[n]}w_{i}^{\star}\mathbf{M}_{i}\preceq\kappa^{ \star}\mathbf{B},\] (3)

the goal of matrix-dictionary approximation is to return weights \(w\in\mathbb{R}_{\geq 0}^{n}\) such that

\[\mathbf{B}\preceq\sum_{i\in[n]}w_{i}\mathbf{M}_{i}\preceq(1+\epsilon)\kappa^ {\star}\mathbf{B}.\] (4)

When \(\mathbf{B}=\mathbf{I}\), the problem in (3), (4) is a special case of (2) where each \(\mathbf{M}_{i}=\mathbf{C}_{i}=\kappa^{\star}\mathbf{P}_{i}\); we call this the isotropic case. We further handle general \(\mathbf{B}\), and demonstrate that our formulation captures many interesting applications. We refer to the problem in (3), (4) as _matrix-dictionary recovery_.

Our results: matrix-dictionary recovery.Our results concerning (3) and (4) assume that the matrix-dictionary \(\{\mathbf{M}_{i}\}_{i\in[n]}\) is "simple" in two respects. First, we assume that we have explicit factorizations

\[\mathbf{M}_{i}=\mathbf{V}_{i}\mathbf{V}_{i}^{\top},\ \mathbf{V}_{i}\in \mathbb{R}^{d\times m}.\] (5)

Our applications in Sections 2 and 3 satisfy this assumption with \(m=1\). Second, denoting \(\mathcal{M}(w):=\sum_{i\in[n]}w_{i}\mathbf{M}_{i}\), we assume we can approximately solve systems in \(\mathcal{M}(w)+\lambda\mathbf{I}\) for any \(w\in\mathbb{R}_{\geq 0}^{n}\) and \(\lambda\geq 0\). Concretely, for any \(\epsilon>0\), we assume there is a linear operator \(\widetilde{\mathcal{M}}_{w,\lambda,\epsilon}\) which we can compute and apply in \(\mathcal{T}_{\mathcal{M}}^{\text{sol}}\cdot\log\frac{1}{\epsilon}\) time,12 and that \(\widetilde{\mathcal{M}}_{w,\lambda,\epsilon}\approx(\mathcal{M}(w)+\lambda \mathbf{I})^{-1}\) in that:

Footnote 12: We use this notation because, if \(\mathcal{T}_{\mathcal{M}}^{\text{sol}}\) is the complexity of solving the system to constant error \(c<1\), then we can use an iterative refinement procedure to solve the system to accuracy \(\epsilon\) in time \(\mathcal{T}_{\mathcal{M}}^{\text{sol}}\cdot\log\frac{1}{\epsilon}\) for any \(\epsilon>0\).

\[\left\|\widetilde{\mathcal{M}}_{w,\lambda,\epsilon}v-\left(\mathcal{M}(w)+ \lambda\mathbf{I}\right)^{-1}v\right\|_{2}\ \text{for all}\ v\in\mathbb{R}^{d}.\] (6)

In this case, we say "we can solve in \(\mathcal{M}\) to \(\epsilon\)-relative accuracy in \(\mathcal{T}_{\mathcal{M}}^{\text{sol}}\cdot\log\frac{1}{\epsilon}\) time." If \(\mathcal{M}\) is a single matrix \(\mathbf{M}\), we say "we can solve in \(\mathbf{M}\) to \(\epsilon\)-relative accuracy in \(\mathcal{T}_{\mathcal{M}}^{\text{sol}}\cdot\log\frac{1}{\epsilon}\) time." Notably, for the matrix-dictionary in our applications, e.g., diagonal \(1\)-sparse matrices or edge Laplacians, such access to \(\{\mathbf{M}_{i}\}_{i\in[n]}\) exists so we obtain end-to-end efficient algorithms. Ideally (for near-linear time algorithms), \(\mathcal{T}_{\mathbf{M}}^{\text{sol}}\) is roughly the total sparsity of \(\{\mathbf{M}_{i}\}_{i\in[n]}\), which holds in all our applications.

Under these assumptions, we give the following novel meta-solvers for matrix-dictionary recovery.13

Footnote 13: We did not heavily optimize logarithmic factors and \(\epsilon^{-1}\) dependences; for many applications (notably Theorems 1, 2), \(\epsilon\) is a constant, so our runtimes are near-linear for a natural representation of the problem under the assumption (5). In several applications (e.g., Theorems 1 and 2) the most important parameter is the “relative condition number” \(\kappa^{\star}\) in the promise (3), so we primarily optimized for the dependence on \(\kappa^{\star}\).

**Theorem 6** (Matrix dictionary recovery, isotropic case).: _Given matrices \(\{\mathbf{M}_{i}\}_{i\in[n]}\) with explicit factorizations (5), such that (3) is feasible for \(\mathbf{B}=\mathbf{I}\) and some \(\kappa^{\star}\geq 1\), we can return weights \(w\in\mathbb{R}_{\geq 0}^{n}\) satisfying (4) with probability \(\geq 1-\delta\) in time_

\[O\left(\mathcal{T}_{\text{mv}}(\{\mathbf{V}_{i}\}_{i\in[n]})\cdot(\kappa^{ \star})^{1.5}\cdot\text{poly}\left(\frac{\log\frac{mnds^{\star}}{\delta}}{ \epsilon}\right)\right).\]

Here \(\mathcal{T}_{\text{mv}}(\{\mathbf{V}_{i}\}_{i\in[n]})\) denotes the computational complexity of multiplying an arbitrary vector by _all_ matrices in \(\{\mathbf{V}_{i}\}_{i\in[n]}\). Notably, in the isotropic case \(\mathbf{B}=\mathbf{I}\), Theorem 6 does not require solvers in the sense of (6). We next state our solver which handles the case of general \(\mathbf{B}\), under access to (6).

**Theorem 7** (Matrix dictionary recovery, general case).: _Given matrices \(\{\mathbf{M}_{i}\}_{i\in[n]}\) with explicit factorizations (5), such that (3) is feasible for some \(\kappa^{\star}\geq 1\) and we can solve in \(\mathcal{M}\) to \(\epsilon\) relative accuracy in \(\mathcal{T}^{\text{sol}}_{\mathcal{M}}\cdot\log\frac{1}{\epsilon}\) time, and \(\mathbf{B}\) satisfying_

\[\mathbf{B}\preceq\mathcal{M}(\mathbb{1})\preceq\alpha\mathbf{B}\text{ and }\mathbf{I}\preceq\mathbf{B}\preceq\beta\mathbf{I},\] (7)

_we can return weights \(w\in\mathbb{R}^{n}_{\geq 0}\) satisfying (4) with probability \(\geq 1-\delta\) in time_

\[O\left(\mathcal{T}_{\text{tot}}\cdot(\kappa^{\star})^{2}\cdot\text{poly} \left(\frac{\log\frac{mnds^{\star}\alpha\beta}{\delta}}{\epsilon}\right) \right),\text{ where }\mathcal{T}_{\text{tot}}:=\mathcal{T}_{\text{mw}}\left(\{ \mathbf{V}_{i}\}_{i\in[n]}\cup\{\mathbf{B}\}\right)+\mathcal{T}^{\text{sol}}_ {\mathcal{M}}.\]

The first condition in (7) is no more general than assuming we have a "warm start" reweighting \(w_{0}\in\mathbb{R}^{n}_{\geq 0}\) (not necessarily \(\mathbb{1}\)) satisfying \(\mathbf{B}\preceq\sum_{i\in[n]}[w_{0}]_{i}\mathbf{M}_{i}\preceq\alpha\mathbf{B}\), by exploiting scale invariance of the problem and setting \(\mathbf{M}_{i}\leftarrow[w_{0}]_{i}\mathbf{M}_{i}\). The second bound in (7) is equivalent to \(\kappa(\mathbf{B})\leq\beta\) up to constant factors, since given a bound \(\beta\), we can use the power method to shift the scale of \(\mathbf{B}\) so it is spectrally larger than \(\mathbf{I}\). The operation requires just a logarithmic number of matrix vector multiplications with \(\mathbf{B}\), which does not impact the runtime in Theorem 7.

**Proof sketches of Theorems 6 and 7.** We defer full proofs of Theorems 6 and 7 to Section 3 of the supplement, but overview our techniques here. Our main workhorse is the following Algorithm 1, which solves a decision variant of the isotropic matrix-dictionary recovery problem (i.e., \(\mathbf{B}=\mathbf{I}\)), leveraging any subroutine \(\mathcal{A}_{\text{pack}}\) for solving pure packing instances of (2) from the literature.14

Footnote 14: For simplicity, we assume each matrix dictionary element’s top eigenvalue is in a bounded range. We explicitly bound the cost of achieving this in applications (via rescaling by a constant-factor approximation to the top eigenvalue of each matrix using the power method, see Fact 3 of the supplement), and this does not dominate the runtime. The runtime bottleneck in all our applications is the cost of approximate packing SDP oracles in Line 7; this is an active research area and improvements therein would also reflect in our algorithm’s runtime.

We define our approximation notions (used in Lines 6 and 10 of Algorithm 1) in Section 2 of the supplement. In Section 3.1 of the supplement, we analyze correctness of Algorithm 1 using regret bounds for the matrix multiplicative weights framework [2], which Lines 4-12 are an instance of, and demonstrate tolerance to the stated approximations. Our proof shows that Algorithm 1 meets its output guarantees, which directly implies a solver for the matrix-dictionary recovery problem (3), (4) in the isotropic case \(\mathbf{B}=\mathbf{I}\), provided we can efficiently implement the algorithm's steps.15

Footnote 15: We show how to search for \(\kappa^{\star}\) in (3) using an incremental search over a small number of calls to Algorithm 1.

By carefully combining polynomial approximations to the exponential, Johnson-Lindenstrauss sketches, and the power method, we obtain the needed approximations in Lines 6 and 10 of Algorithm 1, which combined with our correctness proof yields Theorem 6. Our proof of Theorem 7 in Section 3.2 of the supplement builds upon Theorem 6 and recursive preconditioning, based on the observation that if we could efficiently apply \(\mathbf{B}^{-\frac{1}{2}}\), setting \(\mathbf{M}_{i}\leftarrow\mathbf{B}^{-\frac{1}{2}}\mathbf{M}_{i}\mathbf{B}^{- \frac{1}{2}}\) reduces to the isotropic case. We show that at a \(\sqrt{\kappa^{\star}}\) overhead, we can use (6) to efficiently simulate \((\mathbf{B}+\lambda\mathbf{I})^{-\frac{1}{2}}\) for \(\lambda\) values which are recursively halved, allowing use of Theorem 6 for each recursive call. This general type of adaptive regularization strategy, which we term a homotopy method, is reminiscent of techniques used by other recent works in the literature on numerical linear algebra and structured continuous optimization, such as [1, 1, 2, 1].

**Preconditioning applications.** Formal proofs of our applications in Section 2 are given in Section 5 of the supplement. Theorem 2 follows immediately from Theorem 6 with the dictionary \(\mathbf{M}_{i}=a_{i}a_{i}^{\top}\), where \(\{a_{i}\}_{i\in[n]}\) are rows of \(\mathbf{A}\), which satisfies (5) with \(m=1\). Specifically, note for \(w\in\mathbb{R}^{n}_{\geq 0}\),

\[\mathcal{M}(w)=\sum_{i\in[n]}w_{i}\mathbf{M}_{i}=\sum_{i\in[n]}w_{i}a_{i}a_{i} ^{\top}=\mathbf{A}^{\top}\mathbf{W}\mathbf{A}.\]

A result analogous to Theorem 1, but depending quadratically on \(\kappa^{\star}_{0}(\mathbf{K})\), follows from applying Theorem 7 with \(n=d\), \(\mathbf{M}_{i}=e_{i}e_{i}^{\top}\), \(\kappa=\kappa^{\star}_{0}(\mathbf{K})\), and \(\mathbf{B}=\frac{1}{\kappa}\mathbf{K}\) (i.e., using the dictionary of \(1\)-sparse diagonal matrices to approximate \(\mathbf{K}\), which satisfies (5) with \(m=1\) and (6) with \(\mathcal{T}^{\text{sol}}_{\mathcal{M}}=d\)). Compared to inner scaling, this application exchanges the role of the dictionary and the constraint matrix. Theorem 1 goes beyond this black-box use via a homotopy method for simulating access to matrix square roots (to reduce to the isotropic case), yielding an improved \((\kappa^{\star}_{0}(\mathbf{K}))^{1.5}\) dependence.

Our applications in Section 3 are deferred to Section 4 of the supplement. Robust linear system solvers for perturbed variants of structured matrix families follow directly from Theorem 7, taking the matrix dictionary to be a suitable basis for the relevant non-perturbed structured family, which naturally satisfy (6). As an example, to prove Theorem 3, we use Theorem 7 with the dictionary of all \(b_{e}b_{e}^{\top}\) for edges \(e\) of a complete graph, where \(b_{e}\) is the associated incidence vector; the access (6) is then afforded by known Laplacian solvers. Finally, Theorems 4 and 5 follow by combining Theorems 6, 7 with homotopy techniques, alongside structural facts about M-matrices and Laplacians.

**Further work.** A natural open question is if, e.g., for outer scaling, the \(\kappa_{o}^{\star}(\mathbf{K})\) dependence in Theorem 1 can be reduced further, ideally to \(\sqrt{\kappa_{o}^{\star}(\mathbf{K})}\). This would match the most efficient solvers in \(\mathbf{K}\) under diagonal rescaling, _if the best known outer scaling was known in advance_. Towards this goal, we prove in Appendix D of the supplement that if a width-independent variant of Theorem 6 is developed, it can achieve such improved runtimes for Theorem 1 (with an analogous improvement for Theorem 2). We also give generalizations of this improvement to finding rescalings which minimize natural average notions of conditioning, under existence of such a conjectured solver.

## 5 Additional related work

**Matrix-dictionary recovery.** Our algorithm for Theorem 6 is based on matrix multiplicative weights [13, 14, 1], a popular meta-algorithm for approximately solving SDPs, with carefully chosen gain matrices formed by using packing SDP solvers as a black box. In this sense, it is an efficient reduction from structured SDP instances of the form (3), (4) to pure packing instances.

Similar ideas were previously used in [10] (repurposed in [1]) for solving graph-structured matrix-dictionary recovery problems. Our Theorems 6 and 7 improve upon these results both in generality (prior works only handled \(\mathbf{B}=\mathbf{I}\), and \(\kappa^{\star}=1+\epsilon\) for sufficiently small \(\epsilon\)) and efficiency (our reduction calls a packing solver \(\approx\log d\) times for constant \(\epsilon,\kappa^{\star}\), while [10] used \(\approx\log^{2}d\)calls). Perhaps the most direct analog of Theorem 6 is Theorem 3.1 of [10], which builds upon the proof of Lemma 3.5 of [11] (but lifts the sparsity constraint). The primary qualitative difference with Theorem 6 is that Theorem 3.1 of [10] only handles the case where the optimal rescaling is in \([1,1.1]\), whereas we handle general \(\kappa^{\star}\). This restriction is important in the proof technique of [10], as their approach relies on bounding the change in potential functions based on the matrix exponential of dictionary linear combinations (e.g., the Taylor expansions in their Lemma B.1), which scales poorly with large \(\kappa^{\star}\). Moreover, our method is a natural application of the MMW framework, and is arguably simpler. This simplicity is useful in diagonal scaling applications, as it allows us to obtain a tighter characterization of our \(\kappa^{\star}\) dependence, the primary quantity of interest. Finally, to our knowledge Theorem 7 (which handles general constraint matrices \(\mathbf{B}\), crucial for our applications in Theorems 3, 4, and 5) has no analog in prior work, which focused on the isotropic case.

**Semi-random models.** The semi-random noise model we introduce in Section 2 for linear system solving, presented in more detail and formality in Section 6 of the supplement, follows a line of noise models originating in [1]. A semi-random model consists of an (unknown) planted instance which a classical algorithm performs well against, augmented by additional information given by a "monotone" or "helpful" adversary masking the planted instance. Conceptually, when an algorithm fails given this "helpful" information, it may have overfit to its generative assumptions. This model has been studied in various statistical settings [14, 15, 16, 17]. Of particular relevance to our work, which studies robustness to semi-random noise in the context of fast algorithms (as opposed to the distinction between polynomial-time algorithms and computational intractability) is [10], which developed an algorithm for semi-random matrix completion.

#### Acknowledgments

AS was supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship. KS was supported by a Stanford Data Science Scholarship and a Dantzig-Lieberman Operations Research Fellowship. KT was supported by a Google Ph.D. Fellowship, a Simons-Berkeley VMware Research Fellowship, a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and a PayPal research award.

We would like to thank Huishuai Zhang for his contributions to an earlier version of this project, Moses Charikar and Yin Tat Lee for helpful conversations, and anonymous reviewers for feedback on earlier variations of this paper.

## References

* [AHK12] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. _Theory Comput._, 8(1):121-164, 2012.
* [AJSS19] AmirMahdi Ahmadinejad, Arun Jambulapati, Amin Saberi, and Aaron Sidford. Perron-frobenius theory in nearly linear time: Positive eigenvectors, m-matrices, graph kernels, and other applications. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 1387-1404, 2019.
* [AK07] Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In _Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, California, USA, June 11-13, 2007_, pages 227-236, 2007.
* [AKPS19] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement for p-norm regression. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 1405-1424, 2019.
* [AL17] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigenvectors and faster MMWU. In _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, pages 116-125, 2017.
* [ALO16] Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width-independent, parallel, simpler, and faster positive SDP solver. In _Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016_, pages 1824-1831, 2016.
* achieving width-independence and -convergence. _Math. Program._, 175(1-2):307-353, 2019.
* [ARV09] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander flows, geometric embeddings and graph partitioning. _J. ACM_, 56(2):5:1-5:37, 2009.
* 13, 2021_, pages 522-539. SIAM, 2021.
* [AZLOW17] Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms for matrix scaling. In _Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS)_, pages 890-901, 2017.
* [BBN13] Michel Baes, Michael Burgisser, and Arkadi Nemirovski. A randomized mirror-prox method for solving structured large-scale matrix saddle-point problems. _SIAM J. Optimization_, 23(2):934-962, 2013.
* [BCLL18] Sebastien Bubeck, Michael B. Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method for l\({}_{\text{P}}\) regression provably beyond self-concordance and in input-sparsity time. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 1130-1137, 2018.
* [BHV08] Erik G. Boman, Bruce Hendrickson, and Stephen A. Vavasis. Solving elliptic finite element systems in near-linear time with support preconditioners. _SIAM J. Numerical Analysis_, 46(6):3264-3284, 2008.
* [BS95] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. _J. Algorithms_, 19(2):204-234, 1995.

* [CDG19] Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estimation in nearly-linear time. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 2755-2771, 2019.
* [CDST19] Yair Carmon, John C. Duchi, Aaron Sidford, and Kevin Tian. A rank-1 sketch for matrix multiplicative weights. In _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, pages 589-623, 2019.
* [CFB19] Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L. Bartlett. Fast mean estimation with sub-gaussian rates. In _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, pages 786-806, 2019.
* [CG18] Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adversary. In _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, pages 1362-1394, 2018.
* [CKK\({}^{+}\)18] Michael B. Cohen, Jonathan A. Kelner, Rasmus Kyng, John Peebles, Richard Peng, Anup B. Rao, and Aaron Sidford. Solving directed laplacian systems in nearly-linear time through sparse LU factorizations. In _59th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018_, pages 898-909, 2018.
* [CKM\({}^{+}\)11] Paul F. Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs. In _Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011_, pages 273-282, 2011.
* June 03, 2014_, pages 343-352, 2014.
* [CKP\({}^{+}\)16] Michael B. Cohen, Jonathan A. Kelner, John Peebles, Richard Peng, Aaron Sidford, and Adrian Vladu. Faster algorithms for computing the stationary distribution, simulating random walks, and more. In _IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA_, pages 583-592, 2016.
* [CKP\({}^{+}\)17] Michael B. Cohen, Jonathan A. Kelner, John Peebles, Richard Peng, Anup B. Rao, Aaron Sidford, and Adrian Vladu. Almost-linear-time algorithms for markov chains and new spectral primitives for directed graphs. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017_, pages 410-419, 2017.
* [CMSV17] Michael B. Cohen, Aleksander Madry, Piotr Sankowski, and Adrian Vladu. Negative-weight shortest paths and unit capacity minimum cost flow in \(\bar{\text{o}}\) (\(m^{10/7}\) log \(W\)) time (extended abstract). In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19_, pages 752-771, 2017.
* [CMTV17a] Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and balancing via box constrained newton's method and interior point methods. In _Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS)_, pages 902-913, 2017.
* [CMTV17b] Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and balancing via box constrained newton's method and interior point methods. In _58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017_, pages 902-913, 2017.

* [CMY20] Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau. List decodable mean estimation in nearly linear time. In _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020_, pages 141-148, 2020.
* [DHS11] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 12:2121-2159, 2011.
* [DKP\({}^{+}\)17] David Durfee, Rasmus Kyng, John Peebles, Anup B. Rao, and Sushant Sachdeva. Sampling random spanning trees faster than matrix multiplication. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017_, pages 730-742, 2017.
* [DS08] Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized flow via interior point algorithms. In _Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008_, pages 451-460, 2008.
* [DWZ23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing. In _64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023._ IEEE, 2023.
* [FK00] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph. _Random Struct. Algorithms_, 16(2):195-208, 2000.
* [FK01] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. _J. Comput. Syst. Sci._, 63(4):639-671, 2001.
* [FLS\({}^{+}\)17] Shaun Fallat, Steffen Lauritzen, Kayvan Sadeghi, Caroline Uhler, Nanny Wermuth, and Piotr Zwiernik. Total positivity in markov structures. _Ann. Statist._, 45(3):1152-1184, 06 2017.
* [FS55] G. E. Forsythe and E. G. Straus. On best conditioned matrices. _Proceedings of the American Mathematical Society_, 6(3):340-345, 1955.
* [Gal14] Francois Le Gall. Powers of tensors and fast matrix multiplication. In _International Symposium on Symbolic and Algebraic Computation, ISSAC '14, Kobe, Japan, July 23-25, 2014_, pages 296-303. ACM, 2014.
* [GHM15] Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, pages 560-568, 2015.
* [GM12] Bernd Gartner and Jiri Matousek. _Approximation Algorithms and Semidefinite Programming_. Springer, 2012.
* [GO18] Ankit Garg and Rafael Oliveira. Recent progress on scaling algorithms and applications. _Bulletin of EATCS_, 2(125), 2018.
* [GR89] A. Greenbaum and G. H. Rodrigue. Optimal preconditioners of a given sparsity pattern. _BIT Numerical Mathematics_, 29(4):610-634, 1989.
* [GW95] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. _J. ACM_, 42(6):1115-1145, 1995.
* November 3, 2022_, pages 233-244. IEEE, 2022.
* [HMMT18] Jeremy G. Hoskins, Cameron Musco, Christopher Musco, and Charalampos E. Tsourakakis. Learning networks from random walk-based node similarities. _CoRR_, abs/1801.07386, 2018.

[Jer92] Mark Jerrum. Large cliques elude the metropolis process. _Random Struct. Algorithms_, 3(4):347-360, 1992.
* [JJUW11] Rahul Jain, Zhengfeng Ji, Sarvagya Upadhyay, and John Watrous. QIP = PSPACE. _J. ACM_, 58(6):30:1-30:27, 2011.
* [JKL\({}^{+}\)20] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020_, pages 910-918, 2020.
* [JLL\({}^{+}\)20] Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, and Kevin Tian. Positive semidefinite programming: Mixed, parallel, and width-independent. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020_, 2020.
* [JLL\({}^{+}\)21] Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, and Kevin Tian. Positive semidefinite programming: Mixed, parallel, and width-independent. _CoRR_, abs/2002.04830v3, 2021.
* [JLT20] Arun Jambulapati, Jerry Li, and Kevin Tian. Robust sub-gaussian principal component analysis and width-independent schatten packing. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* 13, 2021_, pages 540-559, 2021.
* [JY12] Rahul Jain and Penghui Yao. A parallel approximation algorithm for mixed packing and covering semidefinite programs. _CoRR_, abs/1201.6090, 2012.
* [KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _Proceedings of the 3rd International Conference on Learning Representations (ICLR)_, 2015.
* [KLM\({}^{+}\)14] Michael Kapralov, Yin Tat Lee, Cameron Musco, Christopher Musco, and Aaron Sidford. Single pass spectral sparsification in dynamic streams. In _55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014_, pages 561-570, 2014.
* [KLP\({}^{+}\)16] Rasmus Kyng, Yin Tat Lee, Richard Peng, Sushant Sachdeva, and Daniel A. Spielman. Sparsified cholesky and multigrid solvers for connection laplacians. In _Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016_, pages 842-850, 2016.
* [KM09] Jonathan A. Kelner and Aleksander Madry. Faster generation of random spanning trees. In _50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA_, pages 13-21, 2009.
* [KMP10] Ioannis Koutis, Gary L. Miller, and Richard Peng. Approaching optimality for solving SDD linear systems. In _51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October 23-26, 2010, Las Vegas, Nevada, USA_, pages 235-244, 2010.
* [KMP11] Ioannis Koutis, Gary L. Miller, and Richard Peng. A nearly-m log n time solver for SDD linear systems. In _IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS 2011, Palm Springs, CA, USA, October 22-25, 2011_, pages 590-598, 2011.

* [KOSZ13] Jonathan A. Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combinatorial algorithm for solving SDD systems in nearly-linear time. In _Symposium on Theory of Computing Conference, STOC'13, Palo Alto, CA, USA, June 1-4, 2013_, pages 911-920, 2013.
* 438, 1983.
* [KRSS15] Rasmus Kyng, Anup Rao, Sushant Sachdeva, and Daniel A. Spielman. Algorithms for lipschitz learning on graphs. In _Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015_, pages 1190-1223, 2015.
* [KRU14] Philip A. Knight, Daniel Ruiz, and Bora Ucar. A symmetry preserving algorithm for matrix scaling. _SIAM Journal on Matrix Analysis and Applications_, 35(3):931-955, 2014.
* fast, sparse, and simple. In _IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA_, pages 573-582, 2016.
* [KV05] Adam Tauman Kalai and Santosh S. Vempala. Efficient algorithms for online decision problems. _J. Comput. Syst. Sci._, 71(3):291-307, 2005.
* [LMP13] Mu Li, Gary L. Miller, and Richard Peng. Iterative row sampling. In _54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA_, pages 127-136, 2013.
* [LN93] Michael Luby and Noam Nisan. A parallel approximation algorithm for positive linear programming. In _Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, May 16-18, 1993, San Diego, CA, USA_, pages 448-457, 1993.
* [LS13] Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems. In _54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA_, pages 147-156, 2013.
* [LS14] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in 0(vrank) iterations and faster algorithms for maximum flow. In _55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014_, pages 424-433, 2014.
* [LS17] Yin Tat Lee and He Sun. An sdp-based algorithm for linear-sized spectral sparsification. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017_, pages 678-687, 2017.
* [LSTZ20] Jerry Li, Aaron Sidford, Kevin Tian, and Huishuai Zhang. Well-conditioned methods for ill-conditioned systems: Linear regression with semi-random noise. _CoRR_, abs/2008.01722, 2020.
* [Mad13] Aleksander Madry. Navigating central path with electrical flows: From flows to matchings, and back. In _54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA_, pages 253-262, 2013.
* 22, 2012_, pages 367-384, 2012.
* [MPW16] Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for community detection? In _Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016_, pages 828-841, 2016.

* [MRWZ16] Michael W. Mahoney, Satish Rao, Di Wang, and Peng Zhang. Approximating the solution to mixed packing and covering lps in parallel \(\tilde{O}(\epsilon^{-3})\) time. In _43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy_, pages 52:1-52:14, 2016.
* [MST15] Aleksander Madry, Damian Straszak, and Jakub Tarnawski. Fast generation of random spanning trees and the effective resistance metric. In _Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015_, pages 2019-2036, 2015.
* 22, 2012_, pages 1141-1160, 2012.
* [OV11] Lorenzo Orecchia and Nisheeth K. Vishnoi. Towards an sdp-based approach to spectral methods: A nearly-linear-time algorithm for graph partitioning and decomposition. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California, USA, January 23-25, 2011_, pages 532-545, 2011.
* [PG90] Giorgio Pini and Giuseppe Gambolati. Is a simple diagonal scaling the best preconditioner for conjugate gradients on supercomputers? _Advances in Water Resources_, 13(3):147-153, 1990.
* June 03, 2014_, pages 333-342, 2014.
* [PST95] Serge A. Plotkin, David B. Shmoys, and Eva Tardos. Fast approximation algorithms for fractional packing and covering problems. _Math. Oper. Res._, 20(2):257-301, 1995.
* [PTZ16] Richard Peng, Kanat Tangwongsan, and Peng Zhang. Faster and simpler width-independent parallel algorithms for positive semidefinite programming. _CoRR_, abs/1201.5135v3, 2016.
* [QGH\({}^{+}\)22] Zhaonan Qu, Wenzhi Gao, Oliver Hinder, Yinyu Ye, and Zhengyuan Zhou. Optimal diagonal preconditioning: Theory and practice. _CoRR_, abs/2209.00809, 2022.
* [QYZ20] Zhaonan Qu, Yinyu Ye, and Zhengyuan Zhou. Diagonal preconditioning: Theory and algorithms. _arXiv:2003.07545_, 2020.
* [RSL18] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 10900-10910, 2018.
* [Sch18] Aaron Schild. An almost-linear time algorithm for uniform random spanning tree generation. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 214-227, 2018.
* [SH14] Martin Slawski and Matthias Hein. Estimation of positive definite m-matrices and structure learning for attractive gaussian markov random fields. 473, 04 2014.
* [ST04] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In _Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, June 13-16, 2004_, pages 81-90, 2004.
* [VB96] Lieven Vandenberghe and Stephen P. Boyd. Semidefinite programming. _SIAM Review_, 38(1):49-95, 1996.

* [vdS69] A. van der Sluis. Condition numbers and equilibration of matrices. _Numerische Mathematik_, 14(1):14-23, 1969.
* 22, 2012_, pages 887-898. ACM, 2012.
* [WK06] Manfred K. Warmuth and Dima Kuzmin. Randomized PCA algorithms with regret bounds that are logarithmic in the dimension. In _Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006_, pages 1481-1488, 2006.
* [WSV00] Henry Wolkowicz, Romesh Saigal, and Lieven Vandenberghe. _Handbook of Semidefinite Programming: Theory, Algorithms, and Applications_. Springer Nature, 2000.
* [WXXZ23] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. _CoRR_, abs/2307.07970, 2023.
* [You01] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering. In _42nd Annual Symposium on Foundations of Computer Science, FOCS 2001, 14-17 October 2001, Las Vegas, Nevada, USA_, pages 538-546, 2001.
* [ZLO15] Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret minimization beyond matrix multiplicative updates. In _Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015_, pages 237-245, 2015.