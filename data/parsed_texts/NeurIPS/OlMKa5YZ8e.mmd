# KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs

 Lujun Li\({}^{*}\) Peijie Dong\(\dagger\) Anggeng Li\(\dagger\) Zimian Wei Ya Yang

HKUST HKUST(GZ) Huawei NUST CityU

{ililujunai, dongpeijie98}@gmail.com,{anggeng.li,yya9}@outlook.com, weizimian16@nudt.edu.cn

Corresponding author, \(\dagger\) Co-first authors, equal contribution.

###### Abstract

Knowledge distillation (KD) has emerged as an effective technique for compressing models that can enhance the lightweight model. Conventional KD methods propose various designs to allow student model to imitate the teacher better. However, these handcrafted KD designs heavily rely on expert knowledge and may be sub-optimal for various teacher-student pairs. In this paper, we present a novel framework, KD-Zero, which utilizes evolutionary search to automatically discover promising distiller from scratch for any teacher-student architectures. Specifically, we first decompose the generalized distiller into knowledge transformations, distance functions, and loss weights. Then, we construct our distiller search space by selecting advanced operations for these three components. With sharpness and represent gap as fitting objectives, we evolve candidate populations and generate better distillers by crossover and mutation. To ensure efficient searching, we employ the loss-rejection protocol, search space shrinkage, and proxy settings during the search process. In this manner, the discovered distiller can address the capacity gap and cross-architecture challenges for any teacher-student pairs in the final distillation stage. Comprehensive experiments reveal that KD-Zero consistently outperforms other state-of-the-art methods across diverse architectures on classification, detection, and segmentation tasks. Noticeably, we provide some practical insights in designing the distiller by analyzing the distiller discovered. Codes are available in supplementary materials.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved great success in tackling a variety of visual recognition tasks [31, 17, 66]. Despite the appealing performance, the prevailing DNN models usually have large numbers of parameters, leading to heavy costs of memory and computation. Conventional techniques such as Neural Architecture Search [16, 6, 26, 15] and quantizing networks to use low-bit parameters [11, 53, 49] have proven to be effective for mitigating this computational burden. Recently, Knowledge Distillation (KD) has been widely used for training compact and efficient neural networks by transferring the knowledge lied in the logits [23] or features [57] from a large, pre-trained teacher to a smaller student.

Recently, although KD has made significant progress in the handcrafted designs, there are still some limitations to its practical

Figure 1: Illustration of search space in KD-Zero.

application for different scenarios from three aspects: **(1) Capacity gap problem:** Traditional KD methods do not always distill better from stronger teachers because of the large teacher-student gap [27, 46]. Larger and more accurate teacher models tend to be overconfident and fail to improve students [79]. To alleviate this, architecture-level methods use the assistant model [46] or architecture search [45], introducing additional training budgets. Other KD techniques use optimized designs or manually modified distillers to reduce the disparity. However, these techniques rely on expert knowledge and require careful tuning, which suffer from poor generality and efficiency. **(2) Cross-architecture issue.** Teacher-student networks with different architectural styles suffer from mismatches in position dependence, receptive fields, and feature dimensions. As shown in Figure 2, while handcrafted methods (_e.g._, KD [23], DIST [27, 46] and WSLD [79]) provide slight gains in some cases, they still are weak in most cross-architecture pairs 2. **(3) Manual tuning difficulties.** Different choices of knowledge sources, distillation functions, and hyperparameters can largely affect the performance of KD. However, designing and training a proper distillation setting requires trial and error, substantial effort, and experiments. Thus, two questions are raised: **(1) How to efficiently discover the optimal distillation strategies without expert knowledge? (2) How to reduce the teacher-student gap with different capabilities and architectures? _"We may hope that machines will eventually compete with men in all purely intellectual fields."_

Footnote 2: In our paper, W40-2, R32\(\times\)4, R8\(\times\)4, MV2, SV1, and SV2 stand for WRN-40-2, ResNet32\(\times\)4, ResNet8\(\times\)4, MobileNetV2, ShuffleNetV1, and ShuffleNetV2. DeiT-Ti [61] T2T-ViT-7 [72], PVT-Ti [64].

As this well-said quote goes, recently, machine learning approaches have successfully replaced human experts in algorithm design, architecture search, and drug discovery. For the first problem, inspired by Auto-Zero [55], we decide to automate the process of designing the distillation functions for the first time in the field of knowledge distillation. For the second question, we systematically review previous studies of KD design for distillation gaps. Our observations reveal that: (1) Most of the distillers can be divided into three components with various key elements: knowledge transformation, distance function, and loss weights. (2) Some essential operations, like normalize ops [3] and mask ops [71], play important roles in reducing the teacher-student gap. (3) Optimal distillation functions can be effectively integrated with additional strategies (_e.g._, feature aggregation [50], projector ensemble [9], N-to-One match [67], and cross-layer mapping [28, 13] in Figure 3).

Based on the above analysis, we present KD-zero, an automated search framework that utilizes evolutionary algorithms from scratch to efficiently discover the best distiller without manual design. Specifically, our framework is organized into three parts: search space, search algorithm, and acceleration strategy. Firstly, we establish the search space with basic transform operations, distance functions, and loss weights (see Figure 1). For example, we select operators normalized in different dimensions (_e.g._, \(batchnorm,norm_{HW,C,N}\)), various types of activation functions (_e.g._, exp, relu, tanh, sigmoid, pow,), multi-scale process and spatial-wise/channel-wise mask transforms and other advanced operations in the knowledge transformation. Our distance function options include smooth \(\ell_{1}\), \(\ell_{1}\), \(\ell_{2}\), \(\ell_{KL}\), \(\ell_{hard}\), \(\ell_{Cosine}\), \(\ell_{Pearson}\) and \(\ell_{Correlation}\) distance. Options in the loss weight part include various values for loss factors, temperature factors, and weight calibration strategies. In this way, our search space includes over 10,000 candidates covering the existing SOTA KD methods and designs. Then, we construct a calculation graph for these candidates and use selected features, representations, and logits from the teacher-student network as input. Based graph structure, we initialize the candidate total groups from the search space, crossover, and mutate them according to the evaluation results. We employ loss-rejection protocol and search space shrinkage for search efficiency to filter out weak candidates. With early-stop proxy settings, we achieve at least 40\(\times\) acceleration during the distiller search. For distillation gap reduction, we take the representation gap and sharpness gap between teacher-student as the fitting objectives besides the accuracy metric of the validation set. Finally, we distill student architectures with discovered distiller, and our KD-Zero surpasses existing KD approaches by a large margin without prior knowledge (see Figure 3).

In principle, our KD-Zero differs from previous hand-designed KD methods, opening new doors to automated distillation designs. Its merits can be highlighted in three aspects: (1) **Effective.** KD-Zero effectively reduces the teacher-student gap by presenting a general distiller search space and adaptive evolutionary search for different teacher-student pairs. KD-Zero extends the KD formulation and allows for additional gains with extra design besides distillers. In addition, it reduces human bias and ensures that the resulting distillers are optimized for the target problem or dataset. (2) **Efficient.** KD-Zero increases efficiency in practice by a series of flexible, systematic, and efficient search procedures without additional laborious tuning. By contrast, other manual methods with fixed KD forms involve complex parameter tuning with additional training time and resources. (3) **Insightful.** KD-Zero undertakes an in-depth analysis of the existing advanced distillation designs, with the aim of exploring their potential combination to produce numerous novel distillers. KD-Zero provides guidelines for practical applications and develops a new research direction. We hope our efforts on the automated design of distillers could facilitate future research for automated KD works to some extent. In summary, our contributions are:

* To alleviate architecture & capability gaps of teacher-student, we present KD-Zero, the first auto-search framework for evolving best distillers from scratch to our best knowledge.
* We present a comprehensive distiller search space, including advanced operations on transformations, distance functions, and loss weights. Then, we evolve the distiller search with performance and sharpness & represent the gap as fitting objectives. In addition, We achieve significant search acceleration via loss-rejection protocol & space shrinkage, and proxy settings.
* We conduct extensive experiments on classification, detection, and segmentation. KD-Zero performs state-of-the-art in multiple datasets and architectures (_e.g._, CNN and vision transformer). Specifically, ResNet-18 and MobileNet with KD-Zero achieve 72.17% and 73.02% Top-1 accuracy on ImageNet, outperforming KD by 1.51%, 2.34%, respectively.

## 2 Related Work

**Knowledge Distillation.** The idea of teacher-student learning is first proposed in pioneering explorations [1; 2], and the formal definition is defined by the original KD [23]. Subsequent efforts explore on different knowledge (_e.g._, intermediate feature representations [38; 43; 37; 33; 36], sample relationships [48; 60]) and applications [18; 14; 35]. **Compared to KD for distillation gap.** Previous methods propose assistant teachers, architecture search, KD designs on transformations [27], distance functions [59], and weight-tuning [42; 34] for this problem. However, such KD designs rely on expert knowledge and tuning, and their performance can fluctuate significantly across different situations. In contrast to these methods, KD-Zero develops automated searches for distillers to address these difficulties that do not require additional architecture modification and manual KD design. **Compared to Meta-KDs.** These works [13; 42] only focus on hyperparameter tuning and involve complex optimization challenges. In contrast, our approach searches for complementary distiller design besides hyperparameters establishing a new paradigm for KD research and application.

**Automated Machine Learning.** AutoML [81; 80] aims to automate Network Architecture Search (NAS) and HyperParameter Optimization (HPO), making them more accessible to non-experts. NAS chooses architecture rather than KD designs. **Compared to HPOs [54; 69],** they generally focus on the hyperparameters on training configurations. Recent methods search for loss formulation [39; 32]. In contrast to these methods, we present a new complex search space for distiller design in transforms, distances, loss weights, and new search objectives and accelerations according to the KD task.

## 3 Methodology

In this section, we first illustrate the design of our distiller search space, the search process, the acceleration, and the fitting objectives. Then we analyze the search results and give some guidelines. Finally, we analyze the student distilled via KD-Zero and expansion for different distillation scenarios. The pipeline of our approach is shown in Figure 4.

### Search Space for Distillers Discovery

**Search space structure.** In KD, the student student \(S\) is distilled with the fixed teacher \(T\) by minimizing:

\[\mathcal{L}_{KD}=\tau^{2}\times\mathcal{W}_{f}\times\mathcal{W}_{Cal}\times \mathcal{D}\big{(}\mathcal{T}(f_{S}/\tau),\mathcal{T}(f_{T}/\tau)\big{)},\] (1)

where \(\mathcal{W}_{f}\) and \(\mathcal{W}_{Cal}\) is the loss weights factor and calibration [79], \(\tau\) is the temperature factor, \(\mathcal{T}\) is transformations, \(\mathcal{D}(\cdot,\cdot)\) is distance function measuring the knowledge difference. \(f_{T}\) and \(f_{S}\) are outputs (_e.g._, features, embeddings, and logits) of the teacher-student. Following this general KD formulation, our search space consists of different types of operations (see Table 1) in transformations, distance functions, and loss weights parts. Then, we use a computation graph to represent each candidate, in which the input nodes are different types of knowledge and the intermediate nodes are primitive operations. In addition, we assign three transform options as _transform-1\(\rightarrow\)transform-2\(\rightarrow\)transform-3_ for the transformation part to ensure effective processing of the input knowledge.

**Insight of space design.** Our space design enjoys multiple merits. **(1) Comprehensive & flexible:** KD-Zero contains key elements of most existing KD methods, including the normalize, mask,

\begin{table}
\begin{tabular}{l|l} \hline \hline \multirow{3}{*}{Transform} & norm-based: \(batchnorm,min-max,norm_{HW,C,N},softmax_{HW,C,N},logsoftmax_{HW,C,N}\) \\  & activation-based: \(exp,mish,leaky,relu,tanh, sigmoid,pow2,pow4,log,sqrt\) \\  & scale-based: \(scale,multi-scale,scale_{rel,1,\tau_{1},2},local_{rel,1,\tau_{2},4,batch},channel\) \\  & attention&mask-based: \(drop,sat,nat,matt,mat,mask\), other: \(no,bmm,mm\) \\ \hline Distance & no-norm loss: smooth \(\ell_{1}\), \(\ell_{1}\), \(\ell_{2}\), \(\ell_{KL}\), \(t_{rad}\); norm loss: \(\ell_{Cosine}\), \(\ell_{Pearson}\), \(\ell_{Correlation}\) \\ \hline Weight & calibration: \(entropy, focal,sim,conf,no\); weight values: 0.01,..., 100, \(\tau\) values:1.4, 8 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Specific operations in KD-Zero. More details of their formulas are available in the Appendix.

Figure 4: Overview of KD-Zero. In the search phase, we first randomly sample candidate distillers to initialize the population and evaluate their validation performance and sharpness & represent the gap between teacher-student. Then, we perform loss-rejection protocol & space shrinkage to remove weak individuals and crossover & mutation to generate new populations within promising ones. Finally, we pick up the best-performing distiller for distillation.

attention, and focal-calibration KDs [3; 79; 27] for distillation gaps. In addition, we introduce basic operations in loss design to improve the flexibility and diversity of the search. **(2) Extended & Innovative:** We extend advanced KD design for omni-dimensional and various knowledge. For example, we expand the normalized operation for batch, channel, and spatial dimensions, which can be used for logits, embeddings, and feature knowledge. Thus, such a unified and flexible search space would provide some inspiration for KD design.

### Evolution Procedure, Acceleration and Objective

**Evolution process.** Our EA starts with an initial population of candidate distillers evaluated for their fitness based on our distillation gap-related multi-objectives. The algorithm then iteratively evolves the population over generations using genetic operators, such as selection, crossover, and mutation, to generate better solutions. Specifically, distillers are first randomly generated to form the initial population. Each candidate solution in the population is evaluated using multiple fitness functions that measure teacher-student gaps. Based on these evaluations, we select the best-performing individuals from the population to create a new population for the next generation. Then, we apply crossover to the selected individuals to create new offspring and use mutation to the new offspring to introduce diversity into the population, which helps to explore new distillers.

**Search acceleration** As the search space is sparse with many unpromising distillers, we employ several strategies to accelerate: **(1) Loss-rejection protocol.** We filter out candidates with excessive loss values or collapsed optimization during the search. **(2) Search space shrinkage.** We reduce sampling probabilities for the operations frequently in loss rejection and tail candidates with search iterations. **(3) Proxy settings.** With diverse and informative knowledge learned from a teacher, student models offer advantages in terms of faster training speeds. Based on these properties, we employ early stop the training process once the student model performs well enough to determine the quality of the candidate distillation. Nevertheless, proxy settings can also introduce evaluation uncertainties, and we alleviate this issue by introducing multiple distillation gap metrics in the following sections.

**Fitting objectives.** To accurately evaluate each distiller and reduce the distillation gap, we include cross-entropy loss, sharpness-gap [20] on prediction, and CKA-gap [52] on representation between teacher-student as the multi-objectives. Specifically, we conduct a training-free evolutionary search algorithm to efficiently discover the optimal distiller \(\alpha^{*}\) from search space \(\mathcal{A}\), as:

\[\alpha^{*}=\arg\min_{\alpha\in\mathcal{A}}(\mathcal{L}_{CE}(f_{S},Y)+\overbrace {(log(\exp(f_{S}))-log(\exp(f_{T})))}^{Sharpness-gap}-\overbrace{\frac{ \text{HSIC}(f_{S},f_{T})}{\sqrt{\text{HSIC}(f_{S},f_{S})\text{HSIC}(f_{T},f_{ T})}}}^{CKA-gap}),\] (2)

where \(\mathcal{L}_{CE}\) is the regular cross-entropy objective with labels \(Y\), the sharpness metric is the logarithm of the exponential sum of logits, and Centered Kernel Alignment (CKA) metric is normalized from Hilbert-Schmidt Independence Criterion (HSIC) [19] on high-level feature.

### Results Analysis and Practical Guidance

Figure 5 present searched distillers for different models. Based on these results, some practical guidance for KD designs can be summarized as follows:

* For knowledge input, feature knowledge enjoys superiority while logits and embedded knowledge share identical status. Searched distillers with feature knowledge take more proportion, and ablation studies on vanilla feature knowledge also surpass logits and embedding knowledge.

Figure 5: Probability of operations within each search part, which counted from the Top-3 searched distillers for all teacher-student pairs in the CIFAR-100 experiment.

This observation also aligns with the existing conclusion that feature-based KD outperforms other KD on detection tasks [27; 63].
* For transformations, the normalized-based operations play a key role in most optimal distillers, and its vanilla performance outperforms other types of transformations in ablation studies. These observations illustrate that normalization benefits distillation, consistent with the current KD methods. Scale-based and activation-based operations are also crucial for the KD because they are often present in optimal distillers and enjoy good vanilla performance in ablation studies.
* For distance functions, normalized-based distances such as \(\ell_{Pearson}\) and \(\ell_{correlation}\) enjoy better results in ablation studies and are often adopted by teacher-student pairs across architectures. This suggests that these normalized-based distances can reduce distillation gaps in complex scenarios. In addition, some simple distance functions (_e.g._, \(\ell_{1}\), \(\ell_{2}\) and \(\ell_{KL}\)) also appear in the optimal distiller. These may be attributed to these distillers employing advanced transformation operations for input knowledge.
* For loss weights, adjusting the temperature values is helpful for different scenarios, and the optimal weight values are generally between 1 and 10 based on search results and ablation studies. In addition, the focal weight calibration outperforms the no-weight calibration in ablation studies, and it is often used for some optimal distillers. In summary, we should employ smaller weight values and actively explore different weight calibration and temperature values to reduce distillation gaps under different teacher-student pairs.

### Distilling Student via Discovered KD-Zero functions

After search, the discovered distiller \(\mathcal{L}_{KD}\) is used for distilling student \(f_{S}\) combined with cross-entropy \(\mathcal{L}_{CE}\) via label \(Y\) in single teacher \(f_{T}\) or multiple teacher \(f_{T_{i}}\) KD as:

\[\mathcal{L}_{single}=\mathcal{L}_{CE}(f_{S},Y)+\mathcal{L}_{KD}(f_{S},f_{T}), \quad\mathcal{L}_{multiple}=\mathcal{L}_{CE}(f_{S},Y)+\sum_{i=1}^{N}\mathcal{ L}_{KD}(f_{S},f_{T_{i}^{\prime}}).\] (3)

**Extended to various distillation designs & scenarios** KD-Zero focuses on the distiller's search and employs simple 1\(\times\)1-Conv for channel alignment. Recently, some KD methods have proposed other designs in feature aggregation [50], projector ensemble [9], N-to-One match [67], and cross-layer mapping [28; 13]. As shown in Table 2, KD-Zero can combine well with them by replacing their default \(\ell_{2}\)/\(\ell_{KL}\) losses. In addition, the distiller search of KD-Zero also benefits different distillation scenarios. Specifically, Self-KDs, online KDs, and multi-teacher KDs with KD-Zero achieve extra gains than the default settings. Also, KD-Zero can combine architecture-level methods (_e.g._, Assistant [46]) to reduce distillation gaps further.

**Why can KD-Zero bridge the teacher-student gap?** The answer is intuitive: In KD-Zero, the search space contains many operators for distillation gap reduction, and search objectives are directly designed to reduce the prediction and representation gap between teacher-student. For example, We approximate the sharpness gap using a Taylor second expansion [20]:

\[G_{gap}=log(\exp(f_{T}))-log(\exp(f_{S}))\approx\log\left(1+f_{T}+\frac{1}{2 }f_{T}^{2}\right)-\log\left(1+f_{S}+\frac{1}{2}f_{S}^{2}\right),\] (4)

Following Hindon's assumption [23] that the logits of each training sample are approximately zero-meaned, i.e., \(\bar{f}_{T},\bar{f}_{S}=0\). So the gap can be rewritten as \(\log\left(1+\frac{1}{2}Var(f_{T})\right)-\log\left(1+\frac{1}{2}Var(f_{S})\right)\)[20]. Our options like normalized transform [3; 27], and weight calibrations [79] can effectively reduce the variance of teacher-studens' outputs and minimize the distillation gap. In addition, the student models distilled with KD-zero enjoy the merits: (1) Fast convergence and superior performance. As shown in Figure 6, KD-Zero has surpassed the best accuracies of KD

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c} \hline \multirow{2}{*}{Net} & \multicolumn{4}{c}{_Dependent Adam_} & \multicolumn{4}{c}{_Differential KD scenarios_} \\ \cline{2-11}  & Review [50] & [Pr.] & NORM [67] & L2T-ww [28] & ONE [76] & BYOT [75] & DML [77] & AVE-MRD [58] & AE-MRD [58] & Assistant [46] \\ \hline \multirow{2}{*}{R20} & \(\ell_{S}\) & \(\ell_{T}\) & 1.89 & 71.76 & 71.55 & 70.89 & 70.77 & 70.37 & 70.92 & 71.24 & 71.36 & 71.06 \\ \cline{2-11}  & KD-Zero & 22.27 & 72.18 & 72.00 & 72.03 & 71.38 & 70.98 & 72.02 & 72.22 & 72.35 & 71.89 \\ \hline \multirow{2}{*}{R20} & \(\ell_{S}\) & \(\ell_{T}\) & 76.20 & 76.02 & 75.65 & 75.30 & 74.25 & 74.12 & 75.33 & 75.22 & 75.68 & 75.35 \\ \cline{2-11}  & KD-Zero & 76.62 & 76.47 & 76.26 & 76.36 & 75.65 & 75.21 & 76.45 & 76.72 & 76.78 & 76.05 \\ \hline \end{tabular}
\end{table}
Table 2: Top-1 (%) accuracy of KD-Zero combined with different KD designs and scenarios for ResNet-20 and WRN-16-2 on CIFAR-100.

and baseline at the beginning of the 3rd learning-rate decay stage. (2) Smaller teacher-student gap. As shown in Figure 8, features and logits of students via KD-Zero have stronger similarities with teachers than the original KD method.

## 4 Experiments

In this section, we assess the efficacy of our proposed KD-Zero approach on classification, detection, and segmentation tasks. Additionally, we compare its performance with other KD methods, ensuring fair comparisons by utilizing the same training settings. We report mean results based on more than 3 repeated trials. More detailed experiment settings and results are available in the Appendix.

### Experiments on CIFAR-100

**Implementation**. We utilize the CIFAR-100 dataset [30] in knowledge distillation. During the distiller search phase, we apply 5% early-stopping training epochs with full training data for acceleration settings. Our evolutionary algorithm with 20 population sizes performs 100 iterations for each teacher-student pair. During the distillation phase, all teacher-student networks are trained using typical training settings, with a training epoch of 240. The multi-step learning rate commences at 0.1, which decays by 0.1 at 100 and 150 epochs. Recently, knowledge distillation enabled training Vision Transformers (ViT) from scratch with CNNs as teachers. To evaluate the effectiveness of KD-Zero, we conduct the evolutionary search for ViT-based distillation strategies with the same settings as the CNN experiment. Subsequently, we train the ViT with the optimal distiller obtained and ResNet-56 as CNN teacher. The training is conducted on \(224\times 224\) resolution images for 300 epochs, with an initial learning rate of 5e-4 and a weight decay 0.05 using the AdamW optimizer.

**Comparison results on CNN models.** Table 3 presents a comparative analysis of our KD-Zero with other state-of-the-art (SOTA) KD methods. We conduct multiple trials with randomly selected distillers in the same search space, called Rand-KD, to evaluate the efficacy of our EA search. For teacher-student pairs with the same architectural style, KD-Zero outperforms the baselines by margins ranging from \(3.16\%\sim 4.90\%\). Compared with Rand-KD and other KDs, KD-Zero obtains consistent performance gains (\(1.0\%\sim 3.2\%\)). Besides strengths in the same architecture pairs, KD-Zero exhibits even stronger performance when dealing with different architectural styles, while other KD methods suffer from noticeable accuracy reductions. Specifically, KD-Zero outperforms the baseline by margins of \(5.6\%\sim 7.3\%\) and the random search results by margins of \(1.9\%\sim 2.3\%\), demonstrating the effectiveness of our design for different structures. Compared with other SOTA KD methods, our KD-Zero achieves \(1.2\%\sim 1.5\%\) gains. These results show that KD-Zero can improve each student model with simple settings under different teacher-student pairs.

**Comparison results on vision transformer.** Table 3 presents the results of the vanilla and distillation models employing different distillation methods. The results indicate that KD-Zero can significantly improve the performance of vision transformers with \(8.0\%\sim 13.1\%\) margins and consistently yields superior performance than other methods. In addition, our proposed method applies to various ViT architectures, thereby validating its effectiveness. Note that most ViT students possess larger model sizes (_e.g._, DeiT-Ti with 5 million parameters) and greater capabilities than the CNN teacher (_e.g._, ResNet-56 with only 0.86 million parameters). Some ViT students outperform the CNN model in the strong regularization setting on large-scale datasets. However, when it comes to ViT distillation on small datasets, employing CNN teachers helps address the issue of ViT models struggling to train effectively from scratch. In this context, using CNN teachers in distillation is akin to auxiliary training or providing additional regularization supervision. As a result, these ViTs demonstrate their original strong representation capability after distillation and consequently outperform the CNN teacher.

### Experiments on ImageNet

**Implementation**. We additionally conduct experiments on the ImageNet[12]. Following CIFAR-100 trials, we employ similar EA settings on a subset of ImageNet for search acceleration. Then, we utilize the discovered distiller for the training of student models (_e.g._, ResNet-18 [21] and MobileNet [25]). The training settings are the same as the other KD methods and involve training for 100 epochs using a multi-step learning rate, which commences at 0.1 and decays by 0.1 at 30, 60, and 90 epochs.

**Comparison results.** As shown in Table 4, our proposed KD-Zero significantly improves the accuracy of baseline models, yielding gains of \(2.5\%\sim 2.9\%\) in Top-1 accuracy for ResNet-18 and MobileNet, respectively. In addition, KD-Zero surpasses other SOTA methods with clear gains, demonstrating its superiority in large-scale datasets. These findings substantiate the effectiveness of KD-Zero in distillation optimization with considerable benefits, establishing the versatility and potency of our framework. In summary, KD-Zero facilitates substantially improved predictive accuracy of student models on ImageNet, more complex domains while preserving superior performance.

**Visualizations.** The comparison between the Grad-CAM++ maps generated by the student model trained with KD-Zero and other methods is presented in Figure 8. The results indicate that the Grad-CAM++ map generated by the student model trained with KD-Zero is more similar to that of the teacher model compared to the student model trained independently. In contrast, independent training of the student model leads to incorrect focus areas. These findings suggest that the KD-Zero

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l} \hline \hline Teacher & Student & Acc. & Teacher & Student & KD [23] & AI [73] & OPD [22] & SRRL [29] & CRD [60] & Review [50] & MOD [71] & **KD-Zero** \\ \hline ResNet-34 & ResNet-18 & Top-1 & 73.40 & 69.75 & 70.66 & 70.69 & 70.81 & 71.73 & 71.17 & 71.61 & 71.58 & **72.71\({}_{+1.9}\)** \\ \hline ResNet-34 & ResNet-18 & Top-5 & 91.42 & 89.07 & 98.83 & 90.01 & 98.98 & 90.60 & 90.13 & 99.51 & 90.35 & **90.46\({}_{+1.2}\)** \\ \hline \multirow{2}{*}{ResNet-50} & \multirow{2}{*}{MobileNet} & Top-1 & 76.16 & 70.13 & 70.68 & 70.72 & 71.25 & 72.49 & 71.37 & 72.36 & 72.35 & **73.02\({}_{+1.2}\)** \\  & & Top-5 & 92.86 & 89.49 & 90.30 & 90.03 & 90.34 & 90.92 & 90.41 & 91.00 & 90.71 & **91.05\({}_{+1.9}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of results on ImageNet. The results of other methods quote the original papers report [8, 60, 71]. We report Top-1 “mean (std)” accuracies (%) for KD-Zero.

approach is more effective than traditional knowledge distillation methods in guiding the student model to learn from the teacher model, resulting in improved model interpretability and performance.

### Experiments on Object Detection & Semantic Segmentation

**Object detection.** We conduct experiments on the MS-COCO dataset[41]. We use the optimal distilller on ImageNet to distill knowledge from teacher detectors to students. Based on the strong baseline [5], we apply KD-Zero to two-stage detector (_e.g._, Faster R-CNN [56]) and the one-stage detector (_e.g._, RetinaNet [40]), which are widely used object detection frameworks. Following common practice [40], all models are trained with a 2\(\times\) learning schedule (24 epochs). We train all the models with SGD optimizer, where the momentum is 0.9, and the weight decay is 0.0001. As shown in Table 5, our KD-Zero improves the AP by 3.5 on RetinaNet and 3.4 on Faster R-CNN, respectively, outperforms previous state-of-the-art techniques, including [59; 70; 74], for both object detectors. The results substantiate the potential of KD-Zero for scaling knowledge transfer to broader datasets and more complex computer vision problems while preserving improved accuracy.

**Semantic segmentation.** We evaluate KD-Zero on Cityscapes dataset[10]. Following the previous work, we adopt PSPNet-ResNet101 [78; 51] as the teacher and PSPNet and DeepLabV3[7] models with the ResNet18 backbone as the student. During distillation, the batch size is 8, and the models are trained for 40K iterations with the SGD optimizer, where the momentum is 0.9 and the weight decay is 0.0005. The results are reported with mean Intersection-over-Union (mIoU) under the single-scale evaluation setting. As shown in Table 6, the student PSPNet and DeepLabV3 get 3.17 and 3.7 mIoU improvement by adding our KD-Zero loss. These results indicate that our method surpasses the state-of-the-art distillation method for semantic segmentation, demonstrating that searched distillers facilitate student learning.

\begin{table}
\begin{tabular}{l|c} Method & mIoU (\%) \\ \hline T: DeepLabV3-R101 & 78.07 \\ \hline S: DeepLabV3-R18 & 74.21 \\ SKD [44] & 75.42 \\ IFVD [65] & 75.59 \\ CWD [59] & 75.55 \\ CIRKD [68] & 76.38 \\ DIST [27] & 77.10 \\ KD-Zero & 77.38 \\ \hline S: PSPNet-R18 & 72.55 \\ SKD [44] & 73.29 \\ IFVD [65] & 73.71 \\ CWD [59] & 74.36 \\ CIRKD [68] & 74.73 \\ KD-Zero & 76.25 \\ \end{tabular}
\end{table}
Table 6: **Results on Cityscapes val dataset with ImageNet Pretrain.**

Figure 10: Comparison of search algorithms (_left_ and organization (_right_) of ResNet-20 on C-100. and correlation visualization (_right_) of ResNet-20.

[MISSING_PAGE_FAIL:10]

## References

* [1] Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In _NIPS_, 2014.
* [2] Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In _KDD_, 2006.
* [3] Weihan Cao, Yifan Zhang, Jianfei Gao, Anda Cheng, Ke Cheng, and Jian Cheng. Pkd: General distillation framework for object detectors via pearson correlation coefficient. In _NeurIPS_, 2022.
* [4] A. Chattopadhyay, Anirban Sarkar, Prantik Howlader, and V. Balasubramanian. Grad-cam++: Improved visual explanations for deep convolutional networks. In _WACV_, 2018.
* [5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019.
* [6] Kunlong Chen, Liu Yang, Yitian Chen, Kunjin Chen, Yidan Xu, and Lujun Li. Gp-nas-ensemble: a model for the nas performance prediction.
* [7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [8] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In _CVPR_, 2021.
* [9] Yudong Chen, Sen Wang, Jiajun Liu, Xuwei Xu, Frank de Hoog, and Zi Huang. Improved feature distillation via projector ensemble. In _NeurIPS_, 2022.
* [10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3213-3223, 2016.
* [11] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In _NIPS_, 2015.
* [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. pages 248-255, 2009.
* [13] Xueqing Deng, Dawei Sun, Shawn Newsam, and Peng Wang. Distpro: Searching a fast knowledge distillation process via meta optimization. 2022.
* [14] Peijie Dong, Lujun Li, and Zimian Wei. Diswot: Student architecture search for distillation without training. In _CVPR_, 2023.
* [15] Peijie Dong, Xin Niu, Lujun Li, Zhiliang Tian, Xiaodong Wang, Zimian Wei, Hengyue Pan, and Dongsheng Li. Rd-nas: Enhancing one-shot supernet ranking ability via ranking distillation from zero-cost proxies. _arXiv preprint arXiv:2301.09850_, 2023.
* [16] Peijie Dong, Xin Niu, Lujun Li, Linzhen Xie, Wenbin Zou, Tian Ye, Zimian Wei, and Hengyue Pan. Prior-guided one-shot neural architecture search. _arXiv preprint arXiv:2206.13329_, 2022.
* [17] Peijie Dong, Xin Niu, Zhiliang Tian, Lujun Li, Xiaodong Wang, Zimian Wei, Hengyue Pan, and Dongsheng Li. Progressive meta-pooling learning for lightweight image classification model. In _ICASSP_, 2023.
* [18] Nuno C Garcia, Pietro Morerio, and Vittorio Murino. Modality distillation with multiple stream networks for action recognition. In _ECCV_, 2018.

* [19] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with hilbert-schmidt norms. In _International conference on algorithmic learning theory_, pages 63-77, 2005.
* [20] Jia Guo. Reducing the teacher-student gap via adaptive temperatures. 2021.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. pages 770-778, 2016.
* [22] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In _ICCV_, 2019.
* [23] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [25] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint, arXiv:1704.04861_, 2017.
* [26] Yiming Hu, Xingang Wang, Lujun Li, and Qingyi Gu. Improving one-shot nas with shrinking-and-expanding supernet. _Pattern Recognition_, 2021.
* [27] Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge distillation from a stronger teacher. _arXiv preprint arXiv:2205.10536_, 2022.
* [28] Yunhun Jang, Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Learning what and where to transfer. In _ICML_, 2019.
* [29] Adrian Bulat Georgios Tzimiropoulos Jing Yang, Brais Martinez. Knowledge distillation via softmax regression representation learning. In _ICLR2021_, 2021.
* [30] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. _Tech Report_, 2009.
* [31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _NIPS_, 2012.
* [32] Hao Li, Tianwen Fu, Jifeng Dai, Hongsheng Li, Gao Huang, and Xizhou Zhu. Autoloss-zero: Searching loss functions from scratch for generic tasks. In _CVPR_, 2022.
* [33] Lujun Li. Self-regulated feature learning via teacher-free feature distillation. In _European Conference on Computer Vision (ECCV)_, 2022.
* [34] Lujun Li, Peijie Dong, Zimian Wei, and Ya Yang. Automated knowledge distillation via monte carlo tree search. In _ICCV_, 2023.
* [35] Lujun Li and Zhe Jin. Shadow knowledge distillation: Bridging offline and online knowledge transfer. In _NeuIPS_, 2022.
* [36] Lujun Li, Liang Shiauan-Ni, Ya Yang, and Zhe Jin. Boosting online feature transfer via separable feature fusion. In _IJCNN_, 2022.
* [37] Lujun Li, Liang Shiauan-Ni, Ya Yang, and Zhe Jin. Teacher-free distillation via regularizing intermediate representation. In _IJCNN_, 2022.
* [38] Lujun Li, Yikai Wang, Anbang Yao, Yi Qian, Xiao Zhou, and Ke He. Explicit connection distillation. 2020.
* [39] Zelong Li, Jianchao Ji, Yingqiang Ge, and Yongfeng Zhang. Autolossgen: Automatic loss function generation for recommender systems. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1304-1315, 2022.

* [40] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _ICCV_, 2017.
* [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.
* [42] Jihao Liu, Boxiao Liu, Hongsheng Li, and Yu Liu. Meta knowledge distillation. _arXiv preprint arXiv:2202.07940_, 2022.
* [43] Xiaolong Liu, Lujun Li, Chao Li, and Anbang Yao. Norm: Knowledge distillation via n-to-one representation matching. In _ICLR_, 2023.
* [44] Yifan Liu, Changyong Shu, Jingdong Wang, and Chunhua Shen. Structured knowledge distillation for dense prediction. _IEEE transactions on pattern analysis and machine intelligence_, 2020.
* [45] Yu Liu, Xuhui Jia, Mingxing Tan, Raviteja Vemulapalli, Yukun Zhu, Bradley Green, and Xiaogang Wang. Search to distill: Pearls are everywhere but not the eyes. In _CVPR_, 2020.
* [46] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In _AAAI_, 2020.
* [47] Yulei Niu, Long Chen, Chang Zhou, and Hanwang Zhang. Respecting transfer gap in knowledge distillation. 2022.
* [48] Wonpyo Park, Yan Lu, Minsu Cho, and Dongju Kim. Relational knowledge distillation. In _CVPR_, 2019.
* [49] Zimian Wei Xin Niu ZHILIANG TIAN Hengyue Pan Peijie Dong, Lujun Li. Emq: Evolving training-free proxies for automated mixed precision quantization. In _ICCV_, 2023.
* [50] Chen Pengguang, Liu Shu, Zhao Hengshuang, and Jiaya Jia. Distilling knowledge via knowledge review. In _CVPR_, 2021.
* [51] Jie Qin, Jie Wu, Xuefeng Xiao, Lujun Li, and Xingang Wang. Activation modulation and recalibration scheme for weakly supervised semantic segmentation. In _AAAI_, 2022.
* [52] Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, and Wanli Ouyang. Better teacher better student: Dynamic prior knowledge for knowledge distillation. _arXiv preprint arXiv:2206.06067_, 2022.
* [53] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Joseph. Xnor-net: Imagenet classification using binary convolutional neural networks. In _ECCV_, 2016.
* [54] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: Evolving machine learning algorithms from scratch. In _ICML_, 2020.
* [55] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. Automl-zero: Evolving machine learning algorithms from scratch, 2020.
* [56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _arXiv preprint, arXiv:1506.01497_, 2015.
* [57] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In _ICLR_, 2015.
* [58] Xiaojie Li Jianlong Wu Fei Wang Chen Qian Shangchen Du, Shan You and Changshui Zhang. Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. In _NeurIPS_, 2020.
* [59] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and Chunhua Shen. Channel-wise knowledge distillation for dense prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5311-5320, 2021.

* [60] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In _ICLR_, 2020.
* [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. pages 10347-10357, 2021.
* [62] Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In _ICCV_, 2019.
* [63] Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling object detectors with fine-grained feature imitation. In _CVPR_, 2019.
* [64] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_.
* [65] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class feature variation distillation for semantic segmentation. In _European Conference on Computer Vision_, pages 346-362. Springer, 2020.
* [66] Zimian Wei, Hengyue Pan, Lujun Li Li, Menglong Lu, Xin Niu, Peijie Dong, and Dongsheng Li. Convformer: Closing the gap between cnn and vision transformers. _arXiv preprint arXiv:2209.07738_, 2022.
* [67] Liu Xiaolong, Li Lujun, Li Chao, and Anbang Yao. Norm: Knowledge distillation via n-to-one representation matching. 2022.
* [68] Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang. Cross-image relational knowledge distillation for semantic segmentation. _arXiv preprint arXiv:2204.06986_, 2022.
* [69] Jie Yang et al. Automatically labeling video data using multi-class active learning. In _Proceedings Ninth IEEE international conference on computer vision_, pages 516-523. IEEE, 2003.
* [70] Zhendong Yang, Zhe Li, Xiaohu Jiang, Yuan Gong, Zehuan Yuan, Danpei Zhao, and Chun Yuan. Focal and global knowledge distillation for detectors. _arXiv preprint arXiv:2111.11837_, 2021.
* [71] Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan. Masked generative distillation. _arXiv preprint arXiv:2205.01529_, 2022.
* [72] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _ICCV_, 2021.
* [73] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In _ICLR_, 2017.
* [74] Linfeng Zhang and Kaisheng Ma. Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors. In _International Conference on Learning Representations_, 2020.
* [75] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In _ICCV_, 2019.
* [76] Matthew Shunshi Zhang and Bradly Stadie. One-shot pruning of recurrent neural networks by jacobian spectrum evaluation. _arXiv preprint arXiv:1912.00120_, 2019.
* [77] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In _CVPR_, 2018.

* [78] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2881-2890, 2017.
* [79] Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. 2021.
* [80] Chendi Zhu, Lujun Li, Yuli Wu, and Zhengxing Sun. Saswot: Real-time semantic segmentation architecture search without training. In _AAAI_, 2024.
* [81] Zimian Zimian Wei, Lujun Li Li, Peijie Dong, Zheng Hui, Anggeng Li, Menglong Lu, Hengyue Pan, and Dongsheng Li. Auto-prox: Training-free vision transformer architecture search via automatic proxy discovery. In _AAAI_, 2024.