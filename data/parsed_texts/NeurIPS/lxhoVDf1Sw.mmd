# Predictive Attractor Models

 Ramy Mounir  Sudeep Sarkar

Department of Computer Science and Engineering, University of South Florida, Tampa

{ramy, sarkar}@usf.edu

###### Abstract

Sequential memory, the ability to form and accurately recall a sequence of events or stimuli in the correct order, is a fundamental prerequisite for biological and artificial intelligence as it underpins numerous cognitive functions (e.g., language comprehension, planning, episodic memory formation, etc.) However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context. Inspired by biologically plausible neuroscience theories of cognition, we propose _Predictive Attractor Models (PAM)_, a novel sequence memory architecture with desirable generative properties. PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input _only once_. Additionally, we find that PAM avoids catastrophic forgetting by uniquely representing past context through lateral inhibition in cortical minicolumns, which prevents new memories from overwriting previously learned knowledge. PAM generates future predictions by sampling from a union set of predicted possibilities; this generative ability is realized through an attractor model trained alongside the predictor. We show that PAM is trained with local computations through Hebbian plasticity rules in a biologically plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based learning, capacity scaling) are discussed throughout the paper. Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research. Illustration videos and code are available on our project page: https://ramymounir.com/publications/pam.

## 1 Introduction

Modeling the temporal associations between consecutive inputs in a sequence (i.e., sequential memory) enables biological agents to perform various cognitive functions, such as episodic memory formation [53, 64, 15], complex action planning [22] and translating between languages [3]. For example, playing a musical instrument requires remembering the sequence of notes in a piece of music; similarly, playing a game of chess requires simulating, planning, and executing a sequence of moves in a specific order. While the ability to form such memories of static, unrelated events has been extensively studied [56, 55, 63, 69, 66], the ability of biologically-plausible artificial networks to learn and recall temporally-dependent patterns has not been sufficiently explored in literature [62]. The task of sequential memory is considered challenging for models operating under biological constraints (i.e., local synaptic computations) for many reasons, including catastrophic forgetting, ambiguous context representations, multiple future possibilities, etc.

In addition to the biological constraint, we impose the following set of desirable characteristics as learning constraints on sequence memory models.

* The learning of one sequence does not overwrite the previously learned sequences. This property is defined under the continual learning framework as avoiding catastrophic forgetting and evaluated with the Backward Transfer (BWT) metric [38].
* The model should uniquely encode inputs based on their context in a sequence. Consider the sequence of letters "EVER"; the representation of "E" at position 1 should be different from "E" at position 3, thus resulting in different predictions: "V" and "R". Moreover, the representation of "E" at position 3 in "EVER" should be different from "E" at position 3 in "CLEVER". Therefore, positional encoding is not a valid solution.
* When presented with multiple valid, future possibilities, the model should learn to represent each possibility separately yet stochastically sample a single valid possibility. Consider the two sequences "THAT" and "THEY"; after seeing "TH", the model should learn to generate either "A" or "E", but not an average [37] or a union of both [25].
* The model should be capable of incrementally learning each transition without seeing the whole sequence or revisiting older sequence transitions that are previously learned. This property falls under online learning constraints, also called stream learning [45; 25].
* The learning algorithm should be tolerant and robust to significant input noise. A model should continuously clean the noisy inputs using learned priors and beliefs, thus performing future predictions based on the noise-free observations [25].

We propose _Predictive Attractor Models (PAM)_, which consists of a state prediction model and a generative attractor model. The predictor in PAM is inspired by the Hierarchical Temporal Memory (HTM) [25] learning algorithm, where a group of neurons in the same cortical minicolum share the same receptive feedforward connection from the sensory input on their proximal dendrites. The depolarization of the voltage of any neuron in a single minicolum (i.e., on distal dendrites) primes this depolarized neuron to fire first while inhibiting all the other neurons in the same minicolum from firing (i.e., competitive learning). The choice of which neurons fire within the same minicolum is based on the previously active neurons and their trainable synaptic weights to the depolarized neurons, which gives rise to a unique context representation for every input [20]. The sparsity of representations (discussed later in Section 3.2) allows for multiple possible predictions to be represented as a union of individual cell assemblies. The Attractor Model learns to disentangle possibilities by strengthening the synaptic weights between active neurons of input representations and inhibiting the other predicted possibilities from firing, effectively forming fixed point attractors during online learning. During recall, the model uses these learned conditional attractors to sample one of the valid predicted possibilities or uses the attractors as prior beliefs for removing noise from sensory observations.

PAM satisfies the above-listed constraints for a sequential memory model, whereas the current state-of-the-art models fail in all or many of the constraints, as shown in the experiments. Our contributions can be summarized as follows: (1) Present the novel _PAM_ learning algorithm that can explicitly represent context in memory without backpropagation, avoid catastrophic forgetting, and perform stochastic generation of multiple future possibilities. (2) Perform extensive evaluation of PAM on multiple tasks (e.g., sequence capacity, sequence generation, catastrophic forgetting, noise robustness, etc.) and different data types (e.g., protein sequences, text, vision). (3) Formulate PAM and its learning rules as a State Space Model grounded in variational inference and the Free Energy Principle [17].

## 2 Background and Related Works

### Predictive Coding

Predictive coding proposes a framework for the hierarchical processing of information. It was initially formulated as a time series compression algorithm to create a more efficient coding system [16; 46]. A few decades later, PC was used to model visual processing in the Retina [59; 27] as an inference model. In the seminal work of Rao and Ballard [52], PC was reformulated as a general computational model of the cortex. The main intuition is that the brain continuously predicts all perceptual inputs, resulting in a quantity of prediction error, which can be minimized by adjusting its neural activities and synaptic strengths. In-depth variational free energy derivation is provided in Appendix A.1.

PC defines two subgroups of neurons: value \(\bm{z}\) and error. Each neuron contains a value node sending its prediction to the lower level \(\hat{\bm{z}}_{l}=f_{l+1}(\bm{z}_{l+1})\) through learnable function \(f\), and error node propagating its computed error to the higher level. The total prediction error is computed as \(\epsilon=\sum_{l}\|(\bm{z}_{l}-\hat{\bm{z}}_{l})\|_{2}^{2}\), which is minimized by first running the network value nodes to equilibrium through optimizing the value nodes \(\{\bm{z}_{l}\}_{l=0}^{L}\). At equilibrium, the value nodes are fixed, and inferential optimization is performed by optimizing the functions \(\{f_{l}\}_{l=1}^{L}\). Both optimizations aim to minimize the same prediction error over all neurons. This propagation of error to equilibrium is shown to be equivalent to backpropagation but using only local computations [66]. The PC formulation has shown success in training on static and i.i.d data [66; 69; 57; 23]. More recently [62], Temporal Predictive Coding (tPC) has also shown some success in sequential memory tasks by modifying error formulation to account for a one-step synaptic delay through interneurons, thus modeling temporal associations between sequence inputs. In the experiments, we compare our model to tPC and its 2-layer variant [62]. A few PC-based models [47; 48] have shown promise for class-incremental continual learning tasks, other PC-inspired models, such as [39; 23; 24], have diverged from the biologically plausible constraints by training through backpropagation through time.

### Fixed-Point Attractor Dynamics

Attractor dynamics refer to mathematical models that describe the behavior of dynamical systems. In our review, we focus on fixed point attractors, specifically Hopfield Networks [26] (HN), which is an instance of associative memory models [32; 29; 30]. Consider, an ordered sequence of \(T+1\) consecutive patterns \(\bm{x}=[\bm{x}_{t}]_{t=1}^{T+1}\), where \(\bm{x}_{t}\in\{-1,1\}^{N}\). We refer to the Universal Hopfield Networks (UHN) framework [41] to describe all variants of HN architecture using a similarity (sim) function and a separation (sep) function, as shown in equation 1. This family of models can be viewed as a memory recall function, where a query \(\xi\) (i.e., noisy or incomplete pattern) is compared to the existing patterns to compute similarity scores using the "sim" function. These scores are then used to weight the projection patterns after applying a "sep" function to increase the separation between similarity scores. The classical HN uses symmetric weights to store the patterns; therefore, it cannot be used to model temporal associations in a sequence. The asymmetric Hopfield Network (AHN) [58] uses asymmetric weights to recall the next pattern in the sequence for a given query \(\xi\).

\[\hat{\xi}=\underset{\text{Projection}}{\underbrace{\text{ \Large{P}}}}\cdot\underset{\text{Separation}}{\underbrace{\text{ \Large{s}}}}\left(\underbrace{\text{\Large{s}}(M,\xi)}_{\text{Separation}} \right)=\begin{cases}\sum_{t=1}^{T}\mathbf{x}_{t+1}\text{sep}\left(\text{ \Large{s}}(\mathbf{x}_{t},\xi)\right)&\text{Asymmetric Weights}\\ \sum_{t=1}^{T+1}\mathbf{x}_{t}\text{sep}\left(\text{\Large{s}}(\mathbf{x}_{t},\xi)\right)&\text{Symmetric Weights}\end{cases}\] (1)

When a dot product "sim" function and identity "sep" function are used, we get the classical HN [26] and AHN [58]. A few variants have been proposed to increase the capacity of the model. Recently, [12] has extended AHN by using a polynomial (with degree \(d\)) or a softmax function (with temperature \(\beta\)) as the "sep" function. HN can also be applied to continuous dense patterns [34; 51; 12], and extended to sparse modern Hopfield models [28; 67].

### Predictive Learning

Predictive learning takes a more general form of minimizing the prediction error between two views of the same input to improve representations. Many backpropagation-based approaches to predictive learning have been proposed; most recently, JEPA [37] and its variants [8; 10; 9; 70], learn useful dense representations from images and videos using the predictive objective. Other models, such as [11; 21; 13] - to list a few, use a similar methodology of predicting distorted versions of the same input to learn good feature representations. Prediction-based approaches have also been used to segment videos into events temporally [2; 42] and spatially [44; 43; 1]. More recently, STREAMER [45] used a predictive learning approach to achieve hierarchical segmentation and representation learning from streaming egocentric videos, where a high prediction error is used as an event boundary. While these biologically implausible approaches show impressive results on their respective tasks, they still suffer from deep learning known limitations, such as catastrophic forgetting and the inability to generate multiple possibilities in regression-based predictive tasks. _Hierarchical Temporal Memory_ (HTM) [25] is a predictive approach that is heavily inspired by neurobiology. HTM relies on lateral inhibition between neurons of the same minicolumn and sparsity of input representations (i.e., SDR) to learn temporal context and associations using only local Hebbian rules.

HTM can be applied to online tasks, such as anomaly detection [6] and object recognition [36], but it is currently incapable of generating future predictions in auto-regressive prediction tasks.

## 3 Predictive Attractor Models

### State Space Model (SSM) formulation

PAM can be represented as a dynamical system with its structure depicted by a Bayesian probabilistic graphical [35, 31] model, more specifically, a State Space Model, where we can perform Bayesian inference on the latent variables and derive learning rules using Variational Inference. Formally, we define a state space model as a tuple \((\mathcal{Z},\mathcal{X},f,g)\), where \(\mathcal{Z}\) is the latent state space, \(\mathcal{X}\) is the observation space, and \(f\) and \(g\) are the transition and emission functions respectively (similar to HMM [50]). We consider a Gaussian form with white Gaussian noise covariance \(\bm{\Sigma}_{z}\) for the latent states. However, we assume a latent state \(\bm{z}\) can generate multiple valid possibilities. Therefore, we model the conditional probability \(p(\bm{x}_{t}|\bm{z}_{t})\) as a Multivariate Gaussian Mixture Model (GMM), where each mode is considered a possibility or a fixed-point attractor in an associative memory model. The GMM has \(C\) components with means \(g_{c}(\bm{z}_{t})\), covariances \(\bm{\Sigma}_{c}\) and component weights \(w_{c}\). The SSM dynamics can be formally represented with the following equations:

\[\bm{z}_{t}|\bm{z}_{t-1}\sim\mathcal{N}(f(\bm{z}_{t-1}),\bm{\Sigma}_{z}),\quad \text{and}\quad\bm{x}_{t}|\bm{z}_{t}\sim\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(g _{c}(\bm{z}_{t}),\bm{\Sigma}_{c})\,,\] (2)

where \(\bm{z}_{t}\in\mathcal{Z}\) and \(\bm{x}_{t}\in\mathcal{X}\). From the Bayesian inference viewpoint, we are interested in the posterior \(p(\bm{z}_{t}|\bm{x}_{t},\bm{z}_{t-1})\). Since the functions \(f\) and \(g\) are non-linear, the computation of this posterior is intractable (unlike an LG-SSM, such as Kalman Filter [65]). Therefore, we utilize variational inference to approximate the posterior by assuming the surrogate posterior \(q(\bm{z})\) has a Gaussian form, and minimize the Variational Free Energy (VFE) [18]. The minimization of VFE (in equation 3) minimizes the KL-divergence between the approximate posterior \(q(\bm{z})\) and the true posterior \(p(\bm{z}_{t}|\bm{x}_{t},\bm{z}_{t-1})\). Derivation 2 of Variational Free Energy is provided in appendix A.1

\[\underbrace{\sum_{t=1}^{T}\mathbb{E}_{q}\left[\log\left(\frac{q( \bm{z}_{t})}{p(\bm{x}_{t},\bm{z}_{t}|\bm{z}_{t-1})}\right)\right]}_{\text{ Variational Free Energy}}=\underbrace{\sum_{t=1}^{T}\mathbb{E}_{q}\left[\frac{1}{\log(p(\bm{z}_{t}|\bm{z}_{t-1}))} \right]}_{\text{Latent State Error}}+\underbrace{\sum_{t=1}^{T}\mathbb{E}_{q} \left[\frac{1}{\log(p(\bm{x}_{t}|\bm{z}_{t}))}\right]}_{\text{Observation Error}}-\mathcal{H}_{q}\,,\] (3)

where \(\mathbb{E}_{q}\equiv\mathbb{E}_{\bm{z}_{t}\sim q(\bm{z}_{t})}\) and \(\mathcal{H}_{q}\) is the Entropy of the approximate posterior \(q(\bm{z})\). The assumption of Gaussian forms for the latent and observable states can further simplify the negative log-likelihood

Figure 1: State Space Model. (Left): Dynamical system represented by first-order Markov chain of latent states \(\bm{z}\) with transition function \(f\) and an emission function \(g\) which projects to the observation states \(\bm{x}\). (Right): Gaussian form assumptions for the prior \(\hat{\bm{z}}\) and posterior \(\bm{z}\) latent states, and the Mixture of Gaussian model representing the conditional probability of multiple possibilities \(p(\bm{x}|\bm{z})\)

terms (i.e., Latent State Accuracy and Observation Accuracy) to prediction errors. This learning objective encourages the approximate posterior \(q(z)\) to assign a high probability to states that explain the observations well and follow the latent dynamics of the system. We minimize the prediction errors (i.e., learn the transition and emission functions) through Hebbian rules as shown in equations 7 and 8.

**Theorem 1**: _Assume the likelihood \(p(\bm{x}_{t}|\bm{z}_{t})\) in eqn 3 represents multiple possibilities using a Gaussian Mixture Model (GMM) conditioned on the latent state \(\bm{z}_{t}\), as shown in eqn 2. The maximization of such log-likelihood function (i.e., \(\frac{\partial}{\partial\bm{x}_{t}}|\bm{z}_{t})\)) w.r.t a query observation state \(\bm{x}\) is equivalent to the Hopfield recall function (i.e., eqn 1) with the means of the GMM representing the attractors of a Hopfield model. Formally, the weighted average of the GMM means (i.e., \(\{\bm{\mu}_{c}\}_{c=1}^{C}\)), with the weights being a similarity measure, maximizes the log-likelihood of \(\bm{x}\) under the mixture model._

\[\bm{x}=\sum_{c=1}^{C}\underbrace{\frac{w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_ {c},\bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}}{\sum_{c=1}^{C}w_{c}\cdot \mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}}}_{ \text{similarity score}}\cdot\underbrace{\bm{\mu}_{c}}_{\text{projection}}\] (4)

* **Proof:** See derivation 3 in appendix A.2 for the full proof.

### Preliminaries and Notations

Sparse Distributed Representation (SDR)Inspired by the sparse coding principles observed in the brain, SDRs encode information using a small set of active neurons in high dimensional binary representation. We adopt SDRs as a more biologically plausible cell assembly representation [30]. SDRs have desirable characteristics, such as a low chance of false positives and collisions between multiple SDRs and high representational capacity [5] (More on SDRs in Appendix G). An SDR is parameterized by the total number of neurons \(N\) and the number of active neurons \(W\). The ratio \(S=W/N\) denotes the SDR sparsity. A 1-dimensional SDR \(\bm{x}\) can be indexed as \(\bm{x}^{i}\in\{0,1\}\), whereas a 2-dimensional SDR \(\bm{z}\) can be indexed as \(\bm{z}^{ij}\in\{0,1\}\). To identify the active neurons, we define the function \(I:\{0,1\}^{N}\mapsto\mathbb{N}^{W}\) to represent the indices of the active neurons in an SDR \(\bm{x}\) as \(I(\bm{x})=\{i|\bm{x}^{i}=1\}\).

Context as Orthogonal DimensionWe transform the high-order Markov dependencies between observation states into a first-order Markov chain of latent states by storing context information in those latent states. The latent states SDRs, \(\bm{z}\in\{0,1\}^{N_{c}\times N_{k}}\), are represented with two orthogonal dimensions, where content information about the input is stored in one dimension with size \(N_{c}\), while context related information is stored in an orthogonal dimension with size \(N_{k}\). Therefore, the projection of the latent state \(\bm{z}\) on the first dimension (i.e., \(\downarrow\bm{z}\)) removes all context information from the state. In contrast, adding context information to an observation state \(\bm{x}\) expands the dimensionality of the state (i.e., \(\uparrow\bm{x}\)) such that context can be encoded without affecting its content. Competitive learning is enforced on the context dimension through lateral inhibition, effectively minimizing the collisions between contexts of multiple SDRs. We define a projection operator \(\downarrow\{0,1\}^{N_{c}\times N_{k}}\mapsto\{0,1\}^{N_{c}}\). Additionally, we define a projection operator \(\uparrow\colon\{0,1\}^{N_{c}}\mapsto\{0,1\}^{N_{c}\times N_{k}}\) for 1-dimensional SDRs (i.e., \(\bm{x}\)) as shown in equation 5. A detailed list of notations is provided in the supplementary material (Table 1).

\[(\downarrow\bm{z})^{i}=\begin{cases}1&\text{if }\exists j\text{ s.t. }\bm{z}^{ij}=1,\\ 0&\text{otherwise},\end{cases},\quad(\uparrow\bm{x})^{ij}=\begin{cases}1&\text{if }\bm{x}^{i}=1,\\ 0&\text{otherwise},\end{cases}\] (5)

### Sequence Learning

Given a sequence of \(T+1\) SDR patterns \([\bm{x}_{t}]_{t=1}^{T+1}\), where \(\bm{x}_{t}\in\{0,1\}^{N_{c}}\), the sequence can be learned by modeling the context-dependent transitions between consecutive inputs within the sequence. We define learnable weight parameters for transition and emission functions, \(\bm{A}\in\mathbb{R}^{N_{c}N_{k}\times N_{c}N_{k}},\bm{B}\in\mathbb{R}^{N_{c} \times N_{c}}\) respectively. A single latent state transition is defined as \(\hat{\bm{z}}_{t}=\delta(\bm{A}\cdot\bm{z}_{t-1})=\delta(\bm{a}_{t})\), where \(\delta\) is a threshold function transforming the logits \(\bm{a}_{t}\) to the predicted SDR state \(\hat{\bm{z}}_{t}\). The full sequence learning algorithm is provided in algorithm 1.

**Context Encoding through Competitive Learning** Every observation \(\bm{x}_{t}\) contains only content information about the input; we embed the observation with context by expanding the state with an orthogonal dimension (i.e., \(\uparrow\bm{x}_{t}\)), which activates all neurons in the minicolumns at the indices \(I(\bm{x}_{t})\). Then, for each active minicolum, the neuron in a predictive state (i.e., higher than the prediction threshold) fires and inhibits all the other neurons in the same minicolum from firing (i.e., lateral inhibition), as shown in Equation 6. If none - or more than one - of the neurons are in a predictive state, random Gaussian noise (\(\epsilon\)) acts as a tiebreaker to select a context neuron. We _do not_ allow multiple neurons to fire in the same minicolumn, which is different from HTM [25], where multiple cells can fire in any minicolumn (e.g., bursting).

\[m(\bm{a}_{t})^{ij}=\begin{cases}1&\text{if }\bm{a}_{t}^{ij}=\max(\{\delta(\bm{a }_{t}^{ij})+\epsilon\}_{j=0}^{N_{k}}),\\ 0&\text{otherwise},\end{cases},\quad\bm{z}_{t}=(\uparrow\bm{x}_{t})\cap m(\bm{a}_ {t})\] (6)

State Transition LearningThe transition between latent states is learned through local computations with Hebbian-based rules. We modify the synaptic weights \(\bm{A}\) to model the transition between pre-synaptic neurons \(\bm{z}_{t-1}\) and post-synaptic neurons \(\bm{z}_{t}\). Only the synapses with active pre-synaptic neurons are modified. The weights operate on context-dependant latent states (i.e., \(\bm{z}_{t-1}\to\bm{z}_{t}\)); thus, the learning of one transition does not overwrite previously learned transition of different contexts, regardless of the input contents (i.e., \(\bm{x}_{t-1}\)). We use the learning constant coefficients \(\eta_{A}^{+}\) and \(\eta_{A}^{-}\) to independently control learning and forgetting behavior, as shown in equation 7. A lower \(\eta_{A}^{-}\) encourages learning multiple possibilities by slowing down the forgetting behavior.

\[\Delta\bm{A}=\underbrace{\eta_{A}^{+}\cdot\bm{z}_{t-1}\cdot\bm{z}_{t}}_{ \Delta\bm{A}_{\text{success}}}^{T}+\underbrace{\eta_{A}^{-}\cdot\bm{z}_{t-1} \cdot\left(1-\bm{z}_{t}\right)^{T}}_{\Delta\bm{A}_{\text{success}}}\] (7)

Contrastive Attractors FormationThe attractors are formed in an online manner by contrasting the input observation \(\bm{x}_{t}\) to the predicted union set of possibilities \(\downarrow\hat{\bm{z}}_{t}\). The goal is to learn excitatory synapses between active neurons of \(\bm{x}_{t}\), and bidirectional inhibitory synapses between \(\bm{x}_{t}\) and the union set of predicted possibilities _excluding_ the \(\bm{x}_{t}\) possibility (i.e., \((\downarrow\hat{\bm{z}}_{t})\setminus\bm{x}_{t}\)), as shown in equation 8.

\[\Delta\bm{B}=\underbrace{\eta_{B}^{+}\cdot\bm{x}_{t}\cdot\bm{x}_{t}}_{\Delta \bm{B}_{\text{success}}}+\underbrace{\eta_{B}^{-}\cdot\left[\bm{x}_{t}\cdot \left((\downarrow\hat{\bm{z}}_{t})\setminus\bm{x}_{t}\right)^{T}+\left(( \downarrow\hat{\bm{z}}_{t})\setminus\bm{x}_{t}\right)\cdot\bm{x}_{t}^{T} \right]}_{\Delta\bm{B}_{\text{success}}}\] (8)

```
1:procedureFrainst(\(\bm{x}\))
2:\(\bm{z}_{0}=(\uparrow\bm{x}_{0})\cap m(\bm{a}_{0})\)\(\triangleright\)Eqn. 6
3:for\(t=1\) to \(T+1\)do
4:\(\bm{a}_{t}=\bm{A}\cdot\bm{z}_{t-1}\)
5:\(\hat{\bm{z}}_{t}=(\uparrow\bm{x}_{t})\cap m(\bm{a}_{t})\)
6:\(\bm{A}=\bm{A}+\Delta\bm{A}\)\(\triangleright\) Update \(A\) via Eqn. 7
7:\(\hat{\bm{z}}_{t}=\delta(\bm{A}\cdot\bm{z}_{t-1})\)
8:\(\bm{B}=\bm{B}+\Delta\bm{B}\)\(\triangleright\) Update \(B\) via Eqn. 8
9:endfor
10:endprocedure ```

**Algorithm 2** : **Sequence Generation**. Given a noisy sequence (i.e., online), or the first input in a sequence (i.e., offline). The model uses the learned functions \(\bm{A}\) and \(\bm{B}\) to generate the full sequence. \(\sim\) denotes sampled from (eqn 9).

### Sequence Generation

After learning one or multiple sequences using algorithm 1, we use algorithm 2 to generate sequences. First, we define two generative tasks: online and offline. In online sequence generation, a noisyversion of the sequence is provided as input, and the model is expected to generate the original learned sequence. In offline sequence generation, the model is only provided with the first input, and it is expected to generate the entire sequence auto-regressively. For cases with equally valid future predictions (e.g., "a" and "e" after "TH" in "THAT" and "THEY"), the model is expected to stochastically generate either one of the possibilities (i.e., "THAT" or "THEY"). The online generation task is a more challenging extension of the online recall task in [62], where the noise-free inputs are provided, and the model only makes a 1-step prediction into the future. During offline sequence generation, the model randomly samples from the union set of predictions \(\downarrow\hat{\bm{z}}\) a single SDR with \(W\) active neurons (equation 9) to initialize the iterative attractor procedure. \(\pi\) denotes a random permutation function. This random permutation function allows the model to randomly generate a different sequence with every generation.

\[\tilde{\bm{x}}^{i}=\begin{cases}1&\text{if }i\in\{\pi(I(\downarrow\hat{\bm{z} }_{t}))^{w}\}_{w=0}^{W},\\ 0&\text{otherwise}\end{cases}\] (9)

## 4 Experiments

### Evaluation and Metrics

MetricsTo evaluate the similarity of two SDRs, we use the Jaccard Index (i.e., IoU)1, which focuses on the active bits in sparse binary representations. Since the sparsity \(S\) of the representations can change across experiments and methods, we normalize the IoU by the expected IoU (Derived in Theorem 2 in Appendix A.3) of two random SDRs at their specified sparsities. The normalized IoU is computed as \(\frac{\text{IoU}-\text{E}[\text{IoU}]}{1-\text{E}[\text{IoU}]}\). We use the Backward Transfer [38] metric in evaluating catastrophic forgetting. Mean Squared Error (MSE) is used to compare images for vision datasets.

Footnote 1: In the context of SDRs, IoU refers to the number of active bits in the intersection SDR divided by the number of active bits in the union SDR

DatasetsWe perform evaluations on synthetic and real datasets. The synthetic datasets allow us to manually control variables (e.g., sequence length, correlation, noise, input size) to better understand the models' behavior across various settings. Additionally, we evaluate on real datasets of various types (e.g., protein sequences, text, vision) to benchmark PAM's performance relative to other models on more challenging and real sequences. For all vision experiments, we use an SDR autoencoder to learn a mapping between images and SDRs (Details on the autoencoder are provided in Appendix C). We run all experiments for 10 different seeds and report the mean and standard deviation in all the figures. More experimental details and results are provided in Appendices C and E.

Figure 2: Sequence Generation. (Left): Offline generation by sampling a single possibility (i.e., attractor point) from a union of predicted possibilities. (Right): Online generation by removing noise from an observation using the prior beliefs about the observed state. Markov Blanket separates the agent’s latent variables from the world’s observable states.

### Results

We align our evaluation tasks with the desired characteristics of a biologically plausible sequence memory model, as listed in the introduction. We show that PAM outperforms current predictive coding and associative memory SoTA approaches on all tasks. Most importantly, PAM is capable of long context encoding, multiple possibilities generation, and learning continually and efficiently while avoiding catastrophic forgetting. These tasks pose numerous significant challenges to other methods.

Offline Sequence CapacityWe evaluate the models' capacity to learn long sequences by varying the input size \(N_{c}\), model parameters (e.g., \(N_{k}\)), and sequence correlation. The correlation is increased by reducing the number of unique patterns (i.e., vocab) used to create a sequence of length \(T\). Correlation is computed as \(1.0-\frac{\text{vocab}}{T}\). In Figure 3**A**, we vary the input size \(N_{c}\) and ablate the models to find the maximum sequence length to be encoded and retrieved, in an _offline_ manner, with a Normalized IoU higher than 0.9. We set the number of active bits \(W\) to be 5 unless otherwise specified. Results show that Hopfield Networks (HN) fail to learn with sparse representations; therefore, we use \(W\) of \(0.5N_{c}\) only for HN and normalize the IoU metric accordingly. PAM's capacity significantly increases with context neurons \(N_{k}\), as expected. HN's capacity also increases with the polynomial degree \(d\) of its separation function; however, as shown in Figure 3**B**, the capacity sharply drops as correlation increases. PAM retains its capacity with increasing correlation, reflecting its ability to encode context in long sequences (i.e., high-order Markov memory). This context encoding property is also demonstrated in the qualitative CIFAR [33] results in Figure 3**E** and **F**, where a short sequence of images with high correlation is used. The model must uniquely encode the context to correctly predict at every step in the sequence. While PAM correctly predicts the full context, single-layer tPC learns to indefinitely alternate between the patterns, while two-layered tPC attempts to average its predictions. AHN shows similar low performance and failure mode as in [62].

Catastrophic ForgettingTo assess the model's performance in a continual learning setup, we sequentially train each model on multiple sequences and compute the Backward Transfer (BWT) [38]

Figure 3: Quantitative results on (**A-B**) Offline sequence capacity, (**C**) Noise robustness, and (**D**) Time of sequence learning and recall. Qualitative results on highly correlated CIFAR sequence in (**E**) offline and (**F**) online settings. The mean and standard deviation of 10 trials are reported for all plots.

metric by reporting the average normalized IoU on previously learned sequences after learning a new one. In Figure 4**A**, we report BWT for 50 synthetically generated sequences with varying correlations. AHN can avoid catastrophic forgetting when the patterns are not correlated, whereas tPC fails to retain previous knowledge regardless of the correlation value. PAM, with high enough context \(N_{k}\), does not overwrite or forget previously learned sequences after learning new ones but performs poorly when \(N_{k}=1\), as expected. We repeat the experiment on more challenging sequences from ProteinNet [7], which contains highly correlated (\(>0.9\)) and long sequences (details in appendix). The results in Figure 4**B** show a similar trend with PAM requiring more context neurons \(N_{k}\) to handle the more challenging data. Qualitative results on moving-MNIST [60] in Figure 4**F** further demonstrate the catastrophic forgetting challenge where the learning of the second sequence overwrites the learned sequence. PAM successfully retrieves the previously learned sequence while the other models fail.

Multiple Possibilities GenerationIn addition to accurately encoding input contexts, PAM is designed to represent multiple valid possibilities and sample a single possibility. We perform evaluation on a dataset of four-letter English words (details in appendix), which includes many possible future completions (e.g., "th[is]", "th[at]", "th[em]", etc.) We train PAM on the list of letters sequentially (i.e., one word at a time); the other methods are trained in a simpler batched setup as in [62] because they suffer from catastrophic forgetting. This puts PAM at a disadvantage, but as shown in Figure 4**C**, PAM still significantly outperforms the other methods in accurately generating valid words (high average IoU) in an offline manner. Both tPC and AHN fail to generate meaningful words when trained on sequences with multiple future possibilities. Figure 4**D** further demonstrates the stochastic generative property of PAM. We show PAM's ability to recall more of the dataset as it repeats the generation process, whereas PC and AHN fail in the dataset recall task.

Online Noise RobustnessThe online generation ability of PAM shown in Figure 2 allows the model to use the learned attractors to clean the noisy observations before using them as inputs to the predictor. This step allows the model to use its prior belief about future observations to modify the noisy inputs. In Figure 3**C**, we evaluate the models' performances by changing a percentage of the active bits during online generation. PAM is able to completely replace the noisy input with its prior belief if it does not exist in the predicted set of possibilities \(\hat{\bm{z}}\). In contrast, the other methods use the noisy inputs, thus hindering their performances. We provide qualitative results on CLEVRER [68]

Figure 4: Qualitative results on (**A**) synthetic and (**B**) protein sequences backward transfer, and (**C-D**) multiple possibilities generation on text datasets. Qualitative results on (**E**) noise robustness on CLEVRER sequence, and (**F**) catastrophic forgetting on Moving MNIST dataset. **a** highlights the first frame with significant error. The mean and standard deviation of 10 trials are reported for all plots.

in Figure 4**E**; PAM retrieves the original sequence despite getting noisy inputs (40% noise), and outperforms the other models. Interestingly, tPC performs reasonably well on this task despite the added noise.

EfficiencyWe report, in Figure 3**D**, the time each model requires to learn and recall a sequence. For this study, we use input size \(N_{c}=100\) and vary the sequence length. PAM operates entirely on _CPU_. The results show that a single-layer tPC model requires more time than all PAM variants (\(N_{k}\leq 24\)). Additionally, a two-layered tPC requires two to three orders of magnitude more time than PAM or single-layered tPC, significantly limiting its scalability and practicality when applied to real data with long sequences.

## 5 Limitations

PAM requires that its representations be sparse and binary (i.e., SDRs) in order to represent multiple possibilities as a union of SDRs with minimal overlap. Therefore, PAM cannot be directly applied to images in the input space like Dense Hopfield Networks. We argue that the neocortex encodes sensory input into SDRs for processing and does not operate directly on the input in its dense representation. Methods that operate directly on dense representations (e.g., images) are arguably less biologically plausible as the neocortex uses sparse binary representations (i.e., cell assemblies with a small fraction of firing neurons) to store and manipulate information. This paper focuses on learning multiple sequences without catastrophic forgetting and stochastically generating multiple possibilities efficiently, and we assume the sequence is provided in SDR format. Additionally, methods that operate on the input directly face challenges when the input is naturally sparse (see Figure 3**A**). Therefore, it is useful to encode the input into a representation with fixed sparsity before applying sequential memory learning algorithms. In future work, we plan to investigate how to encode high dimensional complex inputs (e.g., images) in a compositional part-whole structure of SDRs where we can apply PAM at different levels of abstraction.

## 6 Conclusion

We proposed _PAM_, a biologically plausible generative model inspired by neuroscience findings and theories of cognition. We demonstrate that PAM is capable of encoding unique contexts with tremendous scalable capacity that is not affected by sequence correlation or noise. PAM is a generative model; it can represent multiple possibilities as a union of SDRs (a property of sparsity) and sample single possibilities, thus predicting multiple steps in the future despite multiple possible continuations. We also show that PAM does not suffer from catastrophic forgetting as it learns multiple sequences. PAM is trained using local computations through Hebbian rules and runs efficiently on CPUs. Future directions include hierarchical sensory processing and higher-order sparse predictive models.

## Acknowledgements

This research was supported by the US National Science Foundation Grants CNS 1513126 and IIS 1956050.

## References

* [1]S. Aakur and S. Sarkar (2020) Action localization through continual predictive learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV 16, pp. 300-317. Cited by: SS1.
* [2]S. Ahmad and J. Hawkins (2015) Properties of sparse distributed representations and their application to hierarchical temporal memory. arXiv preprint arXiv:1503.07469. Cited by: SS1.
* [3]S. Ahmad, A. Lavin, S. Purdy, and Z. Agha (2017) Unsupervised real-time anomaly detection for streaming data. Neurocomputing262, pp. 134-147. Cited by: SS1.
* [4]M. A. Aksran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas (2023) Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15619-15629. Cited by: SS1.
* [5]A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas (2024) Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471. Cited by: SS1.
* [6]A. Bardes, J. Ponce, and Y. LeCun (2023) Mc-jepa: a joint-embedding predictive architecture for self-supervised learning of motion and content features. arXiv preprint arXiv:2307.12698. Cited by: SS1.
* [7]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650-9660. Cited by: SS1.
* [8]H. Chaudhry, J. Zavatone-Veth, D. Krotov, and C. Pehlevan (2024) Long sequence hopfield memory. Advances in Neural Information Processing Systems36. Cited by: SS1.
* [9]X. Chen and K. He (2021) Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15750-15758. Cited by: SS1.
* [10]N. Chrysanthidis, F. Fiebig, and A. Lansner (2019) Introducing double bouquet cells into a modular cortical associative memory model. Journal of computational neuroscience47 (2), pp. 223-230. Cited by: SS1.
* [11]M. A. Conway (2009) Episodic memories. Neuropsychologia47 (11), pp. 2305-2313. Cited by: SS1.
* [12]P. Elias (1955) Predictive coding-i. IRE transactions on information theory1 (1), pp. 16-24. Cited by: SS1.
* [13]K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, and W. Penny (2007) Variational free energy and the laplace approximation. Neuroimage34 (1), pp. 220-234. Cited by: SS1.

[MISSING_PAGE_POST]

* [20] Dileep George, Rajeev V Rikhye, Nishad Gothoskar, J Swaroop Guntupalli, Antoine Dedieu, and Miguel Lazaro-Gredilla. Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps. _Nature communications_, 12(1):2392, 2021.
* [21] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [22] Patrick Haggard. Planning of action sequences. _Acta Psychologica_, 99(2):201-215, 1998.
* [23] Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, and Zhongming Liu. Deep predictive coding network with local recurrent processing for object recognition. _Advances in neural information processing systems_, 31, 2018.
* [24] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_, pages 0-0, 2019.
* [25] Jeff Hawkins and Subutai Ahmad. Why neurons have thousands of synapses, a theory of sequence memory in neocortex. _Frontiers in neural circuits_, 10:174222, 2016.
* [26] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. _Proceedings of the national academy of sciences_, 79(8):2554-2558, 1982.
* [27] Toshihiko Hosoya, Stephen A Baccus, and Markus Meister. Dynamic predictive coding by the retina. _Nature_, 436(7047):71-77, 2005.
* [28] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [29] Hiroshi Kage. Implementing associative memories by echo state network for the applications of natural language processing. _Machine Learning with Applications_, 11:100449, 2023.
* [30] Pentti Kanerva. _Sparse distributed memory_. MIT press, 1988.
* [31] Daphne Koller and Nir Friedman. _Probabilistic graphical models: principles and techniques_. MIT press, 2009.
* [32] Bart Kosko. Bidirectional associative memories. _IEEE Transactions on Systems, man, and Cybernetics_, 18(1):49-60, 1988.
* [33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [34] Dmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition. _Advances in neural information processing systems_, 29, 2016.
* [35] Steffen L Lauritzen. _Graphical models_, volume 17. Clarendon Press, 1996.
* [36] Niels Leadholm, Marcus Lewis, and Subutai Ahmad. Grid cell path integration for movement-based visual object recognition. _arXiv preprint arXiv:2102.09076_, 2021.
* [37] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022.
* [38] David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. _Advances in neural information processing systems_, 30, 2017.
* [39] William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. _arXiv preprint arXiv:1605.08104_, 2016.
* [40] Henry Markram, Maria Toledo-Rodriguez, Yun Wang, Anirudh Gupta, Gilad Silberberg, and Caizhi Wu. Interneurons of the neocortical inhibitory system. _Nature reviews neuroscience_, 5(10):793-807, 2004.
* [41] Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Universal hopfield networks: A general framework for single-shot associative memory models. In _International Conference on Machine Learning_, pages 15561-15583. PMLR, 2022.
* [42] Ramy Mounir, Sathyanarayanan Aakur, and Sudeep Sarkar. Self-supervised temporal event segmentation inspired by cognitive theories. In _Advanced Methods and Deep Learning in Computer Vision_, pages 405-448. Elsevier, 2022.

* [43] Ramy Mounir, Roman Gula, Jorn Theuerkauf, and Sudeep Sarkar. Spatio-temporal event segmentation for wildlife extended videos. In _International Conference on Computer Vision and Image Processing_, pages 48-59. Springer, 2021.
* [44] Ramy Mounir, Ahmed Shahabaz, Roman Gula, Jorn Theuerkauf, and Sudeep Sarkar. Towards automated ethogramming: Cognitively-inspired event segmentation for streaming wildlife video monitoring. _International journal of computer vision_, 131(9):2267-2297, 2023.
* [45] Ramy Mounir, Sujal Vijayaraghavan, and Sudeep Sarkar. Streamer: Streaming representation learning and event segmentation in a hierarchical manner. _Advances in Neural Information Processing Systems_, 36, 2024.
* [46] J O'Neal. Entropy coding in speech and television differential pcm systems (corresp.). _IEEE Transactions on Information Theory_, 17(6):758-761, 1971.
* [47] Alex Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Lifelong neural predictive coding: Learning cumulatively online without forgetting. _Advances in Neural Information Processing Systems_, 35:5867-5881, 2022.
* [48] Alexander Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Continual learning of recurrent neural networks by locally aligning distributed representations. _IEEE transactions on neural networks and learning systems_, 31(10):4267-4278, 2020.
* [49] A Peters and C Sethares. The organization of double bouquet cells in monkey striate cortex. _Journal of neurocytology_, 26(12):779-797, 1997.
* [50] Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. _ieee assp magazine_, 3(1):4-16, 1986.
* [51] Hubert Ramsauer, Bernhard Schauf, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael K Kopp, et al. Hopfield networks is all you need. In _International Conference on Learning Representations_, 2020.
* [52] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nature neuroscience_, 2(1):79-87, 1999.
* [53] Edmund T Rolls. A computational theory of episodic memory formation in the hippocampus. _Behavioural brain research_, 215(2):180-196, 2010.
* [54] Tommaso Salvatori, Ankur Mali, Christopher L Buckley, Thomas Lukasiewicz, Rajesh PN Rao, Karl Friston, and Alexander Ororbia. Brain-inspired computational intelligence via predictive coding. _arXiv preprint arXiv:2308.07870_, 2023.
* [55] Tommaso Salvatori, Luca Pinchetti, Beren Millidge, Yuhang Song, Tianyi Bao, Rafal Bogacz, and Thomas Lukasiewicz. Learning on arbitrary graph topologies via predictive coding. _Advances in neural information processing systems_, 35:38232-38244, 2022.
* [56] Tommaso Salvatori, Yuhang Song, Yujian Hong, Lei Sha, Simon Frieder, Zhenghua Xu, Rafal Bogacz, and Thomas Lukasiewicz. Associative memories via predictive coding. _Advances in Neural Information Processing Systems_, 34:3874-3886, 2021.
* [57] Tommaso Salvatori, Yuhang Song, Beren Millidge, Zhenghua Xu, Lei Sha, Cornelius Emde, Rafal Bogacz, and Thomas Lukasiewicz. Incremental predictive coding: A parallel and fully automatic learning algorithm. _arXiv preprint arXiv:2212.00720_, 2022.
* [58] Haim Sompolinsky and Ido Kanter. Temporal association in asymmetric neural networks. _Physical review letters_, 57(22):2861, 1986.
* [59] Mandyam Veerambudi Srinivasan, Simon Barry Laughlin, and Andreas Dubs. Predictive coding: a fresh view of inhibition in the retina. _Proceedings of the Royal Society of London. Series B. Biological Sciences_, 216(1205):427-459, 1982.
* [60] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In _International conference on machine learning_, pages 843-852. PMLR, 2015.
* [61] Dan D Stettler, Aniruddha Das, Jean Bennett, and Charles D Gilbert. Lateral connectivity and contextual interactions in macaque primary visual cortex. _Neuron_, 36(4):739-750, 2002.

* [62] Mufeng Tang, Helen Barron, and Rafal Bogacz. Sequential memory with temporal predictive coding. _Advances in Neural Information Processing Systems_, 36, 2024.
* [63] Mufeng Tang, Tommaso Salvatori, Beren Millidge, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Recurrent predictive coding models for associative memory employing covariance learning. _PLoS computational biology_, 19(4):e1010719, 2023.
* [64] Endel Tulving et al. Episodic and semantic memory. _Organization of memory_, 1(381-403):1, 1972.
* [65] Greg Welch, Gary Bishop, et al. An introduction to the kalman filter. 1995.
* [66] James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. _Neural computation_, 29(5):1229-1262, 2017.
* [67] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. Stanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. _arXiv preprint arXiv:2312.17346_, 2023.
* [68] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clever: Collision events for video representation and reasoning. In _International Conference on Learning Representations_, 2019.
* [69] Jinsoo Yoo and Frank Wood. Bayespcn: A continually learnable predictive coding associative memory. _Advances in Neural Information Processing Systems_, 35:29903-29914, 2022.
* [70] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International conference on machine learning_, pages 12310-12320. PMLR, 2021.

**Supplementary Material**

## Appendix A Theorems and Derivations

### Variational Free Energy

Predictive CodingConsider a hierarchical generative model with hidden states \(\{\bm{z}\}_{l=0}^{L}\), where \(l\leq L\) denotes the level in the hierarchy. The conditional probability \(p(\bm{z}_{l}|\bm{z}_{l+1})\) is assumed to be a multivariate Gaussian distribution with its mean calculated as a function \(f_{l+1}\) of the higher-level hidden representation \(\bm{z}_{l+1}\) and covariance \(\bm{\Sigma}_{l}\) as shown in equation 10.

\[p(\bm{z}_{l}|\bm{z}_{l+1})=\mathcal{N}(f_{l+1}(\bm{z}_{l+1}),\bm{\Sigma}_{l})\] (10)

The goal is to calculate the posterior of hidden states given an observation \(\bm{x}\), (i.e., \(P(\{\bm{z}\}_{l=0}^{L}|\bm{x})\)). Since the prediction function \(f\) contains a non-linear activation, we cannot analytically compute the posterior and we have to approximate it with a surrogate posterior (i.e., \(q(\{\bm{z}\}_{l=0}^{L})\) by maximizing the Evidence Lower Bound (ELBO). We apply the mean field approximation to factorize this joint posterior probability into conditionally independent posteriors \(\{q(\bm{z}_{l})\}_{l=0}^{L}\)), and apply the Laplace approximation to use Gaussian forms for the approximate distribution [19; 18; 54]. Through these approximations, we can maximize the ELBO, or equivalently minimize the Variational Free Energy, in equation 11.

\[\underbrace{\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log(\frac{q(\bm{z})}{p(\bm{x}, \bm{z})})]}_{\text{Variational Free Energy}}=\underbrace{\mathbb{E}_{\bm{z}\sim q(\bm{z})}[ \log(\frac{q(\bm{z})}{p(\bm{z})})]}_{D_{KL}(q||\bm{p})}+\underbrace{\mathbb{E }_{\bm{z}\sim q(\bm{z})}[\log(\frac{1}{p(\bm{x}|\bm{z})})]}_{\text{Accuracy}}\] (11)

The variational free energy can be reduced to minimizing the negative log-likelihood (Accuracy term), which is simply the prediction error when the likelihood is assumed to take a Gaussian Form. Therefore, minimizing the prediction error reduces to the sum of the squared prediction error of every neuron.

**Derivation 1**: _Variational Free Energy derivation for the predictive coding objective function in equation 11. We approximate the true posterior \(p(\bm{z}|\bm{x})\) with a surrogate posterior \(q(\bm{z})\). The objective is to minimize the reverse KL divergence \(D_{KL}(q(\bm{z})||p(\bm{z}|\bm{x}))\)._

\[D_{KL}(q(\bm{z})||p(\bm{z}|\bm{x})) =\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log\frac{q(\bm{z})}{p(\bm{z}| \bm{x})}]\] _(KL Divergence definition)_ \[=\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log\frac{q(\bm{z})p(\bm{x})} {p(\bm{z}|\bm{z})p(\bm{z})}]\] _(Bayes Theorem)_ \[=\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log\frac{q(\bm{z})}{p(\bm{x}| \bm{z})p(\bm{z})}]+\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log p(\bm{x})]\] _(Linearity of expectations)_ \[D_{KL}(q(\bm{z})||p(\bm{z}|\bm{x})) =\underbrace{\mathbb{E}_{\bm{z}\sim q(\bm{z})}[\log\frac{q(\bm{z} )}{p(\bm{x}|\bm{z})p(\bm{z})}]}_{\text{Variational Free Energy}}]+\log p(\bm{x})\] _(Evidence does not depend on \[q(\bm{x})\] )_

_To minimize the KL divergence, we can minimize the Variational Free energy instead because the Evidence term (\(\log p(\bm{x})\)) is a constant negative term. The Variational Free Energy can be further simplified as follows:_

[MISSING_PAGE_FAIL:16]

\[\frac{\partial}{\partial\bm{x}}\log p(\bm{x}|\bm{z}) =\frac{\partial}{\partial\bm{x}}[\log\sum_{c=1}^{C}w_{c}\cdot\mathcal{ N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})]\] \[=\frac{1}{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c}, \bm{\Sigma}_{c})}\cdot\frac{\partial}{\partial\bm{x}}\sum_{c=1}^{C}w_{c}\cdot \mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})\] \[=\frac{1}{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})}\cdot\sum_{c=1}^{C}w_{c}\cdot[-\bm{\Sigma}_{c}^{-1}(\bm{x}- \bm{\mu}_{c})]\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})\] \[=\frac{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c}, \bm{\Sigma}_{c})}{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm {\Sigma}_{c})}\]

_By setting the partial derivative of the log-likelihood to \(0\), we can estimate the value of \(\bm{x}\), which maximizes the function \(\log p(\bm{x}|\bm{z})\)._

\[\frac{\partial}{\partial\bm{x}}\log p(\bm{x}|\bm{z}) =0\] \[\frac{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm {\Sigma}_{c})\cdot-\bm{\Sigma}_{c}^{-1}(\bm{x}-\bm{\mu}_{c})}{\sum_{c=1}^{C}w_{ c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})} =0\] \[\frac{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm {\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}\bm{x}}{\sum_{c=1}^{C}w_{c}\cdot \mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})} =\frac{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm {\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}\bm{\mu}_{c}}{\sum_{c=1}^{C}w_{c}\cdot \mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})}\]

_Finally, we can rearrange the equation in terms of \(\bm{x}\) and show that it is equivalent to the Hopfield recall function where the recall value \(\bm{x}\) equals a weighted average of the attractors (i.e., means of GMM \(\{\bm{\mu}_{c}\}_{c=1}^{C}\)), with the weights being a similarity score function._

\[\bm{x} =\frac{\sum_{c=1}^{C}w_{c}\cdot\mathcal{N}(\bm{x};\bm{\mu}_{c}, \bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}\bm{\mu}_{c}}{\sum_{c=1}^{C}w_{c} \cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}}\] \[\bm{x} =\sum_{c=1}^{C}\underbrace{\frac{w_{c}\cdot\mathcal{N}(\bm{x}; \bm{\mu}_{c},\bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1}}{\sum_{c=1}^{C}w_{c} \cdot\mathcal{N}(\bm{x};\bm{\mu}_{c},\bm{\Sigma}_{c})\cdot\bm{\Sigma}_{c}^{-1 }}}_{\text{similarity score}}\cdot\underbrace{\bm{\mu}_{c}}_{\text{ projection}}\]

### Expected IoU of Random SDRs

**Theorem 2**: _Consider two SDRs with sparsity defined as random variables \(p\sim\mathcal{U}(0,1)\) and \(q\sim\mathcal{U}(0,1)\), the expected Jaccard Index (i.e., IoU) of the two random SDRs follows:_

\[\frac{pq}{p+q-pq}\]

Proof:Given the sparsity random variables of both SDRs (i.e., \(p\) and \(q\)) and the SDR size \(n\), the number of active bits at the same location in both SDRs is equal to the joint probability of both SDRs being active multiplied by the SDR size (i.e., \(npq\)). The union of both SDRs is the total number of active bits minus the active bits in both SDRs, which is equal to \(np+nq-npq\). Therefore, the expected intersection over union is \(\frac{npq}{np+nq-npq}=\frac{pq}{p+q-pq}\)Empirical Validation:We perform empirical validation of the above theorem as shown in figure 5. The sparsity of the first SDR is fixed at 0.1 and 0.5. We vary the sparsity of the second SDR between 0.0 and 1.0 in steps of 0.1 and calculate the average IoU over a population of 1000 pairs of SDR for every setting. Empirical results agree with the derived formulation in theorem 2.

## Appendix B Notations

The notations used in our paper are summarized in Table 1.

## Appendix C Datasets

SyntheticFor synthetic experiments, we generate SDRs with the specified size \(N_{c}\) and uniformly initialized active bits \(W\) to match the required sparsity \(S\). In many of the experiments, \(N_{c}\) is set to 100 with 5 active bits unless otherwise specified. For Hopfield experiments, we set the sparsity to 50% to improve its performance.

Protein SequencesWe use the dataset ProteinNet 7 [7] to extract protein sequences. Each sequence consists of a chain of Amino Acids. In the dataset, there are only 20 different types of Amino Acids (i.e., vocabulary), creating long protein sequences with hundreds of Amino Acids. The dataset is reported in the fasta format, where each Amino Acid is represented with a single-letter code. We create a dictionary mapping from the Amino Acid types to random SDRs with \(N_{c}=100\) and \(W=5\) to train the models. When choosing the sequences, we ensure that the starting Amino Acid is unique for all the dataset sequences to avoid ambiguous predictions in the continual learning evaluation. A sample of the protein sequence is provided below:

\begin{tabular}{l} MGAAAASIQITVNTLSERISSKLEQEANASAQITKCDIEGNYIRONHIGCN \\ LTVKNMCSADADAQLDAVLASATEYGSLTPEQKAYVPAMIFTAALNIQIS \\ VNTVVRDFENYVKQTNSSAAVVDNKLIQNVIDECYGAPGSPTNLEFIN \\ TGSSKGNAKALMQLTTKATTQIPAKQVAGTGVQFYMIVGVILAALF \\ MYYAKRMLFISTNDKIKILANKENVHWITYMDIFFRTISPMVIATIDMQN \\ \end{tabular}

TextTo evaluate the generative ability of PAM, we use a dataset of the most frequently used English words. For preprocessing, we extract one hundred 4-letter words from the dataset and create a mapping dictionary from all the unique letters in the dataset to random SDRs with \(N_{c}=100\) and

Figure 5: Empirical Validation of Theorem 2

\(W=5\) (except for AHN; \(W=0.5N_{c}\)). The dataset contains many words with ambiguous future possibilities. The selected words are provided below:

``` thatwiththeyhavethisfromwordwhatsomewere whenyoursaideachtimewillmanythenthemlike longmakelookmorecomemostoverknowthancall downsidebeenfindworkparttakemadeliveback onlyyearcameshowgoodgivenameveryjustform helplineturnmuchmeanmovesametelldoeswant wellalsoplayhomereadhandportevenlandhere musthighsuchwentkindneednearselfheadpage growfoodfourkeeplastcitytreefarmharddraw leftlatereallifeopenseemnextwalkeaseboth ```

VisionIn our experiment, we evaluate on sequences extracted from Moving MNIST [60], CLEVRER [68] as well as synthetically generated sequences of CIFAR [33] images. In order to convert images to SDRs and SDRs back to images while encoding semantics into the SDRs, we design an SDR AutoEncoder. The goal is to force the bottleneck representation of the autoencoder to become a sparse binary representation, where two visually similar images would result in two SDRs with a high overlap of active neurons. We simply design a CNN autoencoder with a 3-layer CNN encoder and 3-layer CNN decoder and apply top K binarization operation on the bottleneck embedding during training. The full architecture of the SDR autoencoder is shown in Figure 6.

In practice, we use a weighted average of the SDR and Dense representation to allow gradients of the reconstruction loss to freely propagate into the encoder. The weight of the SDR (i.e., \(\alpha\)) is gradually and linearly increased (from 0.0 to 1.0) with the number of training epochs. This gradual increase is fundamental to the training of the SDR Autoencoder as it smooths the loss landscape and allows the model to distribute the active bits on the full SDR. The total mse loss becomes \(\mathcal{L}_{enc}+\mathcal{L}_{recon}\). We use Adam optimizer with a learning rate of \(1\times 10^{-4}\). For Moving MNIST, we use a bottleneck embedding

\begin{table}
\begin{tabular}{c|l}
**Symbol** & **Description** \\ \hline \hline \(\bm{A}\) & Learnable transition weight matrix \\ \(\bm{B}\) & Learnable emission weight matrix \\ \(\bm{x}_{t}\) & Observation at time \(t\) \\ \(\tilde{\bm{x}}_{t}\) & Noisy observation at time \(t\) during recall \\ \(\bm{z}_{t}\) & Posterior Latent state after observing \(\bm{x}_{t}\) (i.e., single possibility) \\ \(\hat{\bm{z}}_{t}\) & Prior Latent state before observing \(\bm{x}_{t}\) (i.e., multiple possibilities) \\ \(\bm{a}_{t}\) & Predicted latent logits at time \(t\) (i.e., \(\bm{A}\cdot\bm{z}_{t-1}\)) before applying threshold \\ \(\delta\) & Threshold function: \(\mathbb{R}^{N_{c}\times N_{k}}\mapsto\{0,1\}^{N_{c}\times N_{k}}\) or \(\mathbb{R}^{N_{c}}\mapsto\{0,1\}^{N_{c}}\) \\ \(\eta\) & Hebbian learning strength for adjusting the synaptic weights \\ \(\uparrow\) & Projection operator adds context to a 1d observation state \\ \(\downarrow\) & Projection operator removes context from a 2d latent state \\ \(I\) & Function for computing the indices of active bits in an SDR \\ \(\pi\) & Random permutation function \\ \hline \(N_{c}\) & Input size of a pattern \\ \(N_{k}\) & Number of neurons per minicolumn for context encoding \\ \(W\) & Number of active bits in a Sparse Distributed Representation (SDR) \\ \(S\) & Sparsity of SDR, calculated as \(W/N\) \\ \(T\) & Number of patterns in one sequence (i.e., sequence length) \\ \(d\) & Degree of polynomial in Hopfield separation function \\ \(L\) & Number of Layers used in temporal Predictive Coding (tPC) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Table of Notations(i.e., \(N_{c}\)) of size 100 with 5 active bits, whereas, for more complex datasets (i.e., CLEVERR, CIFAR), we use an SDR of size 200 with 10 active bits. We show examples of the autoencoder reconstruction with full binary SDR (i.e., \(\alpha=1\)) for all three datasets in Figure 7.

Figure 6: Architecture of the SDR Autoencoder

Figure 7: Examples of Autoencoder reconstructions from SDRs for all three datasets

Implementation Details

In this section, we describe the implementation details and hyperparameters of each method. For each model, we optimize a single set of hyperparameters for all the experiments.

PamThe neurons in both, transition and emission, functions are fully connected. We _do not_ assume any of the weight matrices are symmetric. All synaptic weights are initialized by sampling from a normal distribution with a mean of 0.0 and a standard deviation of 0.1. All \(\eta^{+}\) values in Equations 7 & \(8\) are set to \(0.1\). \(\eta_{B}^{-}\) is set to \(-0.1\), while \(\eta_{A}^{-}\) is set to \(0.0\) to avoid forgetting previous possibilities when learning new transitions; PAM learns a union of possibilities. The threshold for the \(\delta\) function is set as a function of the SDR sparsity. For the transition function, we use a threshold of \(0.8W\), where \(W\) is the active number of bits in the latent state (\(\bm{z}\)) SDR. For the emission function, we use a threshold of \(0.1W\), where \(W\) is the active number of bits in the observation state (\(\bm{x}\)) SDR. During offline generation we sample an initial \(\tilde{\bm{x}}\) from \(\downarrow\hat{\bm{z}}\) with \(W=1\) active neurons. During generation, we set the maximum number of attractor iterations to 100 but stop iterating when the energy of the state converges to a local minimum. During sequence learning, we update \(\bm{A}\) and \(\bm{B}\) iteratively until the transition is learned before learning the next transition. This iterative weight update makes the model insensitive to the hyperparameter values \(\eta\). Both \(\bm{A}\) and \(\bm{B}\) are always clamped in the range \([-1,1]\). The states \(\bm{z}\) are flattened into a single dimension before applying the learning rule in Equation 7. Binary representations (i.e., {0, 1}) are used as inputs.

Temporal Predictive CodingFor the tPC architecture, we use learning rate of 1e-4 for 800 learning iterations. When a 2-layer tPC model is used, the inference learning rate is set to 1e-2 for 400 inference iterations. Also, the hidden size is set to twice the input size. We found that these parameters work best for all of the experiments and allow the model to fully converge. Bipolar representations (i.e., {-1, 1}) are used as inputs.

Asymmetric Hopfield NetworkThe Hopfield model does not require hyperparameters other than the ablated separation function. In many experiments, we use a polynomial separation function with degree \(d\) set to 1 or 2. Bipolar representations (i.e., {-1, 1}) are used as inputs.

## Appendix E Experiments

In this section, we describe the setup of each figure in the main paper and provide additional quantitative and qualitative results for each task. _All experiments are run for 10 different seeds/trials. We report the mean and standard deviation in all the figures and tables._

### Sequence Capacity

Figure 3 AThis experiment plots the maximum offline sequence length (i.e., sequence capacity, \(T_{max}\)) at different input sizes. The input size \(N_{c}\) is varied from \(10\) to \(100\) while the number of active bits \(W\) is fixed to \(5\). We compare variants of our model with \(N_{k}\) set to \(4\) and \(8\) to temporal predictive coding (tPC) and Asymmetric Hopfield Network (AHN). We observe that AHN completely fails as the sparsity \(S\) of the pattern decreases. Therefore, we also compare to AHN with the sparsity set to 50% (i.e., \(W=0.5N_{c}\)). For AHN models, we experiment with a polynomial exponential function with degrees \(1\) and \(2\), as recently proposed [12] and used for evaluation in recent papers [62]. All models in this experiment are set to recall/generate in an offline manner, where only the first input is provided. PAM outperforms all other methods and has the potential to improve further by expanding the context neurons \(N_{k}\). The patterns in this experiment are uncorrelated, such that each pattern has active bits that are uniformly initialized.

Figure 3 BThis experiment plots the effect of sequence correlation on the maximum offline capacity. The higher the correlation value, the more exact repetitions of patterns are available in the sequence. We enforce correlation by limiting the number of unique patterns (i.e., vocab) used to create the sequence. All patterns in this experiment are set to a size of \(N_{c}=100\) and \(W=5\) (except for AHN, which is set at \(W=0.5N_{c}\)). Results show that the capacity of all other methods sharply drops when correlation is introduced. PAM retains most of its original capacity.

Figure 3 E & FIn this experiment, we provide a qualitative example of a short sequence (\(T=10\)) with high correlation (\(0.8\)). The sequence is learned by all the methods; then we perform offline (**E**) and online (**F**) recall on the sequence. We use the SDR autoencoder to create SDRs from these CIFAR images for training and recall. The SDRs have a size \(N_{c}\) of 200 and \(W=10\). In the offline recall, only the first input is provided and the model auto-regressively generates the full sequence using its own predictions at every time step. In online recall, the models perform a single-step prediction and always use the ground truth input at every time step to perform predictions. Results show that only PAM can retain a context of correlated sequence and accurately predict into the future based on this context.

Figure 8 AIn this experiment, we show the effect of scaling the model context memory beyond a simple \(N_{k}=4\) and \(N_{k}=8\). We show that when using \(N_{k}=16\) and \(N_{k}=24\), PAM can model much longer sequences. We vary the input size \(N_{c}\) from 10 to 50 and report the offline sequence capacity of the model as ablations.

Figure 8 BSimilar to the experiment plotted in Figure 3 A, we report the sequence capacity with input size \(N_{c}\). However, this experiment evaluates the online generation capacity, where the model uses the correct pattern at every prediction time step instead of using its own prediction from the previous time step. Results show that PAM significantly increased in capacity (three times in some cases), whereas the other methods have not increased as much in modeling longer sequences.

Figure 9We provide additional qualitative example on a different highly correlated sequence. The result shows a different failure mode for AHN, whereas PAM still performs well.

### Catastrophic Forgetting

Figure 4 AWe benchmark the performance of difference models in the challenging continual learning setup. The models are expected to avoid catastrophic forgetting by not overwriting previously learned sequences. In this experiment, we use 50 sequences, each with size \(N_{c}=100\) and length \(T=10\). We vary the correlation of the sequences from \(0.0\) to \(0.5\) and compute the backward transfer metric with the normalized IoU as the measure of similarity. Results show that AHN can avoid catastrophic forgetting when the sequence is uncorrelated but quickly drops in performance with correlation. tPC fails to retain learned sequences regardless of correlation. PAM performs well with more context neurons \(N_{k}\). When setting \(N_{k}\) to \(1\), the model fails to retain its knowledge due to the decreased context modeling capability with a single context neuron.

Figure 8: Additional sequence capacity experiments. **A**: scaling of the offline sequence capacity with context memory size \(N_{k}\) and input size \(N_{c}\). **B**: Online sequence capacity of various methods.

Figure 3 E & FIn this experiment, we provide a qualitative example of a short sequence (\(T=10\)) with high correlation (\(0.8\)). The sequence is learned by all the methods; then we perform offline (**E**) and online (**F**) recall on the sequence. We use the SDR autoencoder to create SDRs from these CIFAR images for training and recall. The SDRs have a size \(N_{c}\) of 200 and \(W=10\). In the offline recall, only the first input is provided and the model auto-regressively generates the full sequence using its own predictions at every time step. In online recall, the models perform a single-step prediction and always use the ground truth input at every time step to perform predictions. Results show that only PAM can retain a context of correlated sequence and accurately predict into the future based on this context.

In this experiment, we report the performance of the models on protein sequences. This is a more challenging setup due to the long sequence (a few hundred on average) with high correlation (only 20 unique Amino Acids). We show a similar trend where the other methods fail due to high correlation or sequence lengths. PAM outperforms the other methods when using context memory \(N_{k}\) of \(16\) or \(24\). All Amino Acid types are converted to fixed and randomly initialized SDRs with \(N_{c}=100\). The sparsity is set similar to sequence capacity experiments (i.e., \(W=5\) and \(W=0.5N_{c}\)).

We provide qualitative results on a simple experiment with 2 sequences from moving MNIST. The models learn the first sequence and then learn the second sequence. The models are not allowed to train on the first sequence after they have trained on the second sequence. We then perform online generation on the first sequence with all models. We use the SDR autoencoder to generate SDRs for all images in the sequences; the SDRs have \(N_{c}=100\) with \(W=5\) (for all methods except AHN). The results show that PAM can recall the full sequence even after being trained on another sequence. Other methods fail in this simple task, even in online recall setup.

We provide continual learning results similar to Figures 4**A** & **B**; however, instead of offline generation, we perform the evaluation in online manner.

We provide additional qualitative results on Moving MNIST, as well as quantitative results averaged over 10 trials of MNIST sequence pairs. These quantitative results are reported as the Mean Squared Error of the reconstructed image. Results show that PAM reports the lowest error with a much smaller variance.

The Backward Transfer metric (BWT) to evaluate catastrophic forgetting is computed by taking the average of the performance on previously learned sequences after training on a new sequence. In addition to plotting this average in previous experiments, we provide the full tables for one of the experiments as an example. All tables show results on 10 sequences and \(N_{c}=100\). The BWT metric is calculated as the average of the similarity metric reported in these tables.

Figure 4: **B**

Figure 10: **We provide continual learning results similar to Figures 4**A** & **B**; however, instead of offline generation, we perform the evaluation in online manner.**

Figure 9: Additional qualitative example of correlated sequential memory with CIFAR images.

Figure 11: Additional qualitative catastrophic forgetting visualization on Moving MNIST and quantitative results on the reconstruction error of 10 Moving MNIST random examples

\begin{table}
\begin{tabular}{|c|c c c c c c c c c c c|} \hline \hline \multirow{2}{*}{Train Test} & \multicolumn{8}{c|}{Sequence ID} & \multicolumn{8}{c|}{} \\  & \multicolumn{1}{c|}{1} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \multirow{9}{*}{10} & 1 & - & - & - & - & - & - & - & - & - & - & - \\  & 2 & 0.692 & 0.220 & - & - & - & - & - & - & - & - & - \\  & 3 & 0.654 & 0.252 & 0.586 & 0.314 & & - & - & - & - & - & - & - \\  & 4 & 0.581 & 0.201 & 0.619 & 0.411 & 0.759 & 0.229 & - & - & - & - & - & - \\  & 5 & 0.644 & 0.236 & 0.553 & 0.348 & 0.817 & 0.200 & 0.823 & 0.425 & - & - & - & - \\  & 6 & 0.617 & 0.220 & 0.501 & 0.322 & 0.605 & 0.259 & 0.546 & 0.329 & 0.673 & 0.299 & - & - & - \\  & 7 & 0.660 & 0.246 & 0.545 & 0.343 & 0.706 & 0.265 & 0.490 & 0.239 & 0.664 & 0.325 & 0.716 & 0.293 & - & - \\  & 8 & 0.631 & 0.557 & 0.437 & 0.693 & 0.292 & 0.575 & 0.236 & 0.631 & 0.320 & 0.571 & 0.297 & 0.738 \(\pm\) 0.268 & - & - \\  & 9 & 0.509 & 0.231 & 0.531 & 0.357 & 0.234 & 0.645 & 0.286 & 0.584 & 0.227 & 0.665 & 0.284 & 0.613 \(\pm\) 0.265 & 0.539 \(\pm\) 0.267 & - & - \\  & 10 & 0.659 & 0.235 & 0.488 & 0.317 & 0.602 & 0.326 & 0.498 & 0.260 & 0.532 & 0.246 & 0.630 \(\pm\) 0.257 & 0.664 \(\pm\) 0.327 & 0.660 \(\pm\) 0.330 & 0.575 \(\pm\) 0.313 & - \\ \hline \end{tabular}
\end{table}
Table 2: Catastrophic forgetting experiment results on 10 sequences for PAM with \(N_{k}=1\). The table shows the mean normalized IoU and standard deviation of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the shown numbers. Results are averaged over 10 trials.

Figure 10: Catastrophic forgetting experiments in an online manner. Results are shown on a synthetic dataset of SDR sequences with different correlations and on protein sequences.

[MISSING_PAGE_FAIL:25]

increases, the performance of other models decreases as they struggle to model ambiguous future predictions; however, PAM outperforms the other approaches by sampling from these possibilities.

Figure 4 DThis experiment evaluates the ability of the models to recall the dataset words. We compute the recall as the number of valid unique words generated divided by the total number of words in the dataset. Since PAM is a generative stochastic model, the recall increases with every generation. The other methods are deterministic and, therefore, do not report an increase in dataset recall with more generations. The other methods completely fail to generate any meaningful words. We use an average IoU threshold of 0.9 to classify a generated word as correct, similar to sequence capacity experiments.

### Noise Robustness

Figure 3 CTo evaluate for noise robustness, we plot each model's performance (Normalized IoU) with varying levels of noise added in an online generation setting. The noisy inputs are created by changing a percentage of the active neurons to different uniformly chosen neurons in the SDR. The noise is computed as a percentage for a fair comparison across different SDR sparsities (e.g., tPC vs. AHN). We show the results for sequences of lengths \(T=200\) and no correlation. PAM has the ability to compare the noisy input representation to the learned attractors to recover the correct clean input. Therefore, even when the SDR is completely changed, PAM relies on its predictions and completely ignores the noisy input. During generation, PAM always generates (or corrects a noisy input) from within the predicted set of possibilities. The other approaches use the noisy inputs during recall, which affects their performance.

Figure 4 EWe provide qualitative results on the CLEVRER dataset. The memories sequence is learned by all the models, then a noisy sequence is used during generation. We only add noise starting from the second pattern in the input sequence. The results show that tPC models are performing relatively well, yet they are still outperformed by PAM \(N_{k}=8\). We set \(N_{c}=200\) in the SDR autoencoder to learn the SDRs used in this experiment. We use 40% noise in this experiment.

Figure 13 We perform additional experiments on varying the sequence lengths and the correlation in the sequence, all the other settings remain the same as in the experiment of Figure 3**C**. The results

Figure 12: Qualitative results showing the generated words from PAM, tPC [62] and AHN [12] Words highlighted in green are available in the dataset (i.e., True positives). Words highlighted in red are not available in the dataset (i.e., False positives).

Figure 13: We perform additional experiments on varying the sequence lengths and the correlation in the sequence, all the other settings remain the same as in the experiment of Figure 3**C**. The results

show that with shorter sequences (\(\leq 200\)), no noise, and no correlation, all the models recall the learned sequence well. When higher correlation is used, 2-layered tPC performs relatively well with short sequences (i.e., \(T=10\)) but fails with longer sequences (i.e., \(T=100\)). The Hopfield model fails more with correlation than sequence length. The added noise affects all reported methods except for PAM due to its ability to rely on its predictions and attractors to clean the noisy signal.

Figure 14: Additional qualitative example of online generation with noise on CLEVRER dataset, and quantitative results of reconstruction error over 10 CLEVRE examples at different noise levels.

Figure 13: The effect of noise on online generation with varying sequence lengths and sequence correlations.

### Efficiency

Figure 3 DWe compare the efficiency of the models and show that PAM is at least two orders of magnitude more efficient than tPC with 2 layers. A single layer tPC is almost equivalent to PAM with high context memory of \(N_{k}=24\). AHN is highly efficient as the model is not usually trained, but the recall equation is used instead. Therefore, we exclude AHN from the comparison.

### Additional Results

Figure 15 AWe show the performance of PAM in an online and offline generation task as the sequence length increases. In offline generation, incorrect predictions accumulate quickly, leading to a sharp drop in performance. However, online generation attempts to correct inaccurate predictions; therefore, performance smoothly degrades with sequence length. Note that both experiments (i.e., online and offline) use identical seeds/sequences for a fair comparison.

Figure 15 BWe provide additional results on testing the effect of connection density. While neurons in PAM are currently densely connected (similar to Hopfield Network), we show that PAM can still perform relatively well in a sparser setup.

Figure 15 CWe provide additional results comparing PAM to a 2-layer transformer on continually learning protein sequences. Backward transfer results show low performance of transformers in remembering older sequences after training on new sequences. Table 6 supplements these results by showing detailed sequence-by-sequence performance values. Transformers excel at learning the current task (diagonal values) but suffer in remembering previously learned tasks (lower triangle values).

\begin{table}
\begin{tabular}{|c|c|c c c c c c c c c|} \hline \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{8}{c|}{Sequence ID} \\  & & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \multirow{9}{*}{
\begin{tabular}{c} \\ \end{tabular} } & 1 & 0.749 & - & - & - & - & - & - & - & - & - \\  & 2 & 0.000 & 0.903 & - & - & - & - & - & - & - \\  & 3 & 0.054 & 0.051 & 0.954 & - & - & - & - & - & - \\  & 4 & 0.066 & 0.0485 & 0.002 & 0.931 & - & - & - & - & - \\  & 5 & 0.066 & 0.079 & 0.0482 & 0.09 & 0.863 & - & - & - & - & - \\  & 6 & 0.056 & 0.054 & 0.035 & 0.076 & 0.945 & - & - & - & - \\  & 7 & 0.061 & 0.042 & 0.104 & 0.099 & 0.041 & 0.071 & 0.727 & - & - & - \\  & 8 & 0.042 & 0.068 & 0.129 & 0.049 & 0.078 & 0.079 & 0.033 & 0.719 & - \\  & 9 & 0.039 & 0.039 & 0.078 & 0.019 & 0.052 & 0.026 & 0.077 & 0.078 & 0.723 & - \\  & 10 & 0.062 & 0.077 & 0.135 & 0.042 & 0.029 & 0.008 & 0.032 & 0.065 & 0.073 & 0.955 \\ \hline \end{tabular}
\end{table}
Table 6: Catastrophic forgetting experiment results on 10 protein sequences for a 2-layer Transformer. The table shows the mean normalized IoU of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the numbers in the lower triangle.

Figure 15: (**A**) Performance of online and offline PAM (\(N_{c}=50,N_{k}=4\)) generation with sequence length. (**B**) Performance of PAM (\(N_{c}=50,N_{k}=4\)) generation for different connection densities. (**C**) Backward transfer performance for PAM and 2-layer transformer on the protein sequence task.

Discussions

### Biological Plausibility

PAM inherits assumptions from Hierarchical Temporal Memory (HTM) [25] that the sequence modeling takes place in Layer IV of the cortical column, where it receives both thalamocortical sensory driving input and contextual information. Sensory input arrives on the proximal dendrites of pyramidal cells, while contextual information reaches the basal dendrites, priming specific contexts by depolarizing neurons. Depolarized neurons then fire first and inhibit other neurons within the same minicolumn via lateral inhibition, a process assumed to be mediated by interneurons (possibly Double Bouquet Cells (DBCs)) [49]. HTM assumes interneurons [40] perform lateral inhibition during prediction but does not learn the synapses of the interneurons. Interneurons are also assumed to perform the inhibitory tasks of the attractor model [14].

Biological plausibility in PAM mainly refers to excluding the obvious biologically implausible learning rules, such as backpropagation or copying of weights. This allows models like PC and HN to be referred to as biologically plausible despite clear contradictions with anatomy [54, 14]. Moreover, PAM also constrains its learning and representations to sparse and binary cell assemblies, which improves the plausibility of the model. Therefore, a biologically plausible model here refers to the ability to continually learn using only local learning rules (e.g., Hebbian, anti-Hebbian, STDP) and sparse binary representations. However, we do not intend to accurately map every detail in PAM to the anatomy of the neocortex.

### Future Directions

In future work, we plan to implement a hierarchy of PAM blocks that supports compositional, part-whole predictions. Unlike simply stacking layers as in Transformer or CNN architectures, our aim is to establish a hierarchy where higher-level blocks make predictions based on broader, contextual patterns. For instance, in vision, higher levels could predict entire objects based on multiple visual cues, while in NLP, they could predict phrases or sentences rather than individual tokens. To achieve this, each PAM level would send a summary representation of the current sequence (its latent state) to the next level for recursive, higher-order processing.

Additionally, we plan to explore top-down feedback mechanisms between PAM blocks - inspired by the hierarchical feedback observed in the neocortex. In the visual system, for example, top-down signals travel from higher regions (e.g., V2) to lower ones (e.g., V1), reaching the apical dendrites of pyramidal neurons. These connections originate in layer VI(a) of the higher region and form en-passant synapses in layer VI(a) of the lower region in the hierarchy (e.g., V1). These connections reach Layer I and provide feedback on the apical dendrites of other layers [61]. In PAM, a similar feedback pathway could enable higher-level blocks to modulate lower-level processing based on partial observations, refining predictions by ruling out certain lower-level possibilities that conflict with higher-level context. For example, the attractor in the higher-level block can reach a decision on the object being seen (e.g., human) based on partial observation (V1: layer III \(\rightarrow\) V2: layer IV), and therefore, the feedback inhibitory connections on the apical dendrites can rule out some lower-level possibilities (e.g., paws).

## Appendix G Sparse Distributed Representations

The neocortex stores and represents information using sparse activity patterns, as demonstrated by empirical evidence [4]. Inspired by HTM [25] and neuroscience-based theories of cortical function, we use Sparse Distributed Representations (SDRs) as the main representation format of PAM. An SDR is a sparse binary representation of a cell assembly where only a small fraction of the neurons in the SDR are active at any time. The location of these active neurons encodes the information that is represented by this SDR. In this section, we describe some useful properties of SDRs and discuss their robustness to noise as opposed to dense representations.

### SDR Properties

SDRs are used to represent rich sensory information in the neocortex as a sparse activity pattern. Therefore, from a mathematical viewpoint, an SDR must have the ability to represent many patterns and easily distinguish between them. The capacity of an SDR can be calculated as the possible combinations of locations where neurons can be active. Consider an SDR with size \(N\) and number of active neurons \(W\). The total capacity of this SDR is computed as shown in Equation 12.

\[\begin{pmatrix}N\\ W\end{pmatrix}=\frac{N!}{W!(N-W)!}\] (12)

Based on the above Binomial coefficient equation, it may seem that sparsity is not optimal for capacity as the capacity will be the highest when \(W\) is exactly half of \(N\). While capacity is important, we aim to represent multiple possibilities as a union of SDRs and, therefore, minimize the overlap between them. From an information-theoretic viewpoint, the goal is to minimize _mutual information_ between SDRs to ensure that each SDR carries unique information and the union represents a more comprehensive and diverse set of features. We can minimize the expected IoU by using lower sparsities as shown in Theorem 2. In our experiments, we use \(N=100\) and \(W=5\), which results in a capacity of \(\approx 75\times 10^{6}\) and an expected IoU of \(\approx 0.02\). However, when scaled up to more typical values of SDR sizes and sparsities in the neocortex [25, 4] (i.e., \(N=2048\), \(W=40\)), we get capacity of \(\approx 2.37\times 10^{84}\) (more than the estimated number of atoms in the observable universe \(\approx 10^{80}\)) and expected IoU of \(\approx 0.01\). A sparsity of \(0.5\) maximizes the mutual information and results in an expected IoU of \(0.33\), which cannot be used to represent multiple possibilities as a union of SDRs in spite of the optimal capacity. In practice, the size \(N\) of the SDR is increased to increase the capacity, and the sparsity \(W/N\) is decreased to minimize the expected overlap.

### The robustness of SDRs

Sparse representations naturally minimize the overlap between random SDRs; therefore, they are very tolerant to noise. To visualize this robustness property of SDRs, we design an experiment (Figure 16) where we train an SDR autoencoder with different sparsities and then decode SDRs at various levels of noise added. When the sparsity is increased to \(50\%\), there is a high chance of overlap between SDRs. Therefore, a small amount of noise can cause collisions between SDRs. However, a \(5\%\) sparsity can tolerate much more noise without overlapping with other SDRs.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims are supported by experimental results and theoretical proofs where applicable. Derivations are provided in the supplemental material. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, the limitations section has been moved from the supplemental material to the main paper for the camera ready version. Guidelines: *

Figure 16: Three examples of decoding an SDR with different noise levels. The results are shown for SDRs with different sparsities trained in an SDR autoencoder on the CLEVRER dataset.

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide proofs and derivations in the appendix, where applicable. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide general implementation details for all the methods used. Additionally, we describe the experimental setup for every plot/table in the main paper and in the appendix. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our codebase _(now released)_ includes a script for every single experiment to accurately reproduce the plots in the paper. We fix the seeds for exact reproducibility. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide implementation details for all methods and the datasets' details in the appendix. The code also contains a separate configuration file with all the needed parameters. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments are repeated 10 times at different seeds. The means and standard deviations are provided in all plots and tables. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We specify the compute resources required for PAM (i.e., CPU) and plot a comparison of the time required by each method at different parameters in Figure 3**D**. Guidelines:* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms with NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: PAM is a powerful sequence modeling algorithm. We do not see direct positive or negative societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: No such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Authors of comparison methods and datasets are properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are released. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.