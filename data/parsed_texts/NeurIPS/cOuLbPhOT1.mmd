# PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization

 Yao Ni\({}^{\dagger}\)   Shan Zhang\({}^{\ddagger,\dagger}\)   Piotr Koniusz\({}^{*,\lx@sectionsign,\dagger}\)

\({}^{\dagger}\)The Australian National University  \({}^{\lx@sectionsign}\)Data6l\({}^{\lx@sectionsign}\)CSIRO

\({}^{\ddagger}\)Australian Institute for Machine Learning, The University of Adelaide

\({}^{\dagger}\)yao.ni@anu.edu.au  shan.zhang@adelaide.edu.au  \({}^{\lx@sectionsign}\)piotr.koniusz@data61.csiro.au

The corresponding author.

###### Abstract

Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained transformers to downstream tasks. However, the optimization of tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improvements in model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such an issue, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC, few-shot learning, domain adaptation) showcasing its potential for resource-efficient fine-tuning. It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K). The code is available at github.com/MaxwellYaoNi/PACE.

## 1 Introduction

Transformers [68], with the self-attention mechanism [3] capturing long-range dependencies in data, succeed in various deep learning tasks, including image classification (ViT [16]), multimodal learning (CLIP [55]), image synthesis (StableDiffusion [57]), semantic segmentation (SAM [33]) and text generation (LLaMA [65]). The success of transformers can be largely attributed to the availability of abundant data, such as ImageNet [11] and Laion5B [60], which empower researchers to scale up these models by training them under an enormous number of parameters.

Such huge models, with knowledge from large-scale pre-training [63], constitute on foundation models that can be easily adapted to various downstream tasks through full fine-tuning or linear probing [20], eliminating the need for task-specific model design [8]. However, full fine-tuning is storage-intensive and infeasible for maintaining separate model weights as the number of tasks grows, while linear probing, which only trains the last head layer, yields inferior adaptation performance.

To overcome these limitations, Parameter-Efficient Fine-Tuning (PEFT) [24] fine-tunes only a small subset of parameters, thereby reducing storage requirements while surpassing the performance offull fine-tuning and linear probing. These advantages have popularized PEFT and inspired the development of various PEFT methods for deep learning tasks, which can be categorized into two groups: those increasing inference cost and cost-efficient ones. The first group introduces additional learning branches, such as non-linear adapters [25; 8], or concatenates learnable parameters with input tokens, _e.g._, visual prompts [28; 82; 52], increasing inference cost. The second group, focuses on cost-efficiency by lower-rank adaptation in linear layers [7; 26], or affine transformations such as SSF [41] and RepAdapters [45], which can be reparameterized during inference for efficiency.

Despite the superiority and efficiency of PEFT, prioritizing optimization for downstream tasks compromises the generalizability of fine-tuned models, yielding suboptimal performance. Although some analyses have been conducted on PEFT [63; 27; 18; 72; 39], they fail to fully explain the generalization of PEFT, leading to ineffective strategies for improving generalization.

To address this gap in understanding generalization in PEFT, we establish a theoretical connection from generalization theory: smaller weight gradient norms and larger data volumes contribute to better generalization. Motivated by this, we propose reducing weight gradient norms and aligning output space of the fine-tuned model with the pre-trained one to retain knowledge captured from large pre-training data. Yet, theoretical analyses reveal this naive alignment does not guarantee gradient regularization and can even cause gradient explosion, complicating efforts for gradient management. To address this issue, we propose perturbing features learned from the adapter with multiplicative noise and constraining the network output to be consistent across different perturbations.

Our method, called PACE, marries generalization of PArameter-efficient fine-tuning with Consistency rEgalization. Its name, PACE, reflects our goal of keeping the output behavior of the fine-tuned model _in pace with_ the pre-trained one. Despite its simplicity, theoretical analysis confirms that PACE not only implicitly regularizes weight gradients for better generalization but also implicitly aligns the fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Experimental evidence supports our theories. PACE improves existing PEFT methods, achieving superior results across six adaptation benchmarks. Our key contributions are:

1. We establish a theory connecting smaller weight gradient norms and larger datasets with enhanced generalization, motivating gradient reduction and model alignment for fine-tuning.
2. We propose PACE, a simple yet effective method perturbing features from adapters with multiplicative noise and constraining output of fine-tuned model to be consistent across perturbations.
3. Our theoretical and empirical evidence confirms that PACE implicitly regularizes gradients and aligns the fine-tuned model with the pre-trained one. PACE excels on 4 visual adaptation tasks.
4. We provide novel theoretical explanations of how gradient penalization and consistency regularization benefit generalization, offering fundamental insights applicable across deep learning.

## 2 Related work

**Parameter-Efficient Fine-Tuning (PEFT).** LoRA [26] uses low-rank decomposition to reduce parameters and treats adapters as side paths. SSF [41] proposes affine transformations on latent features. FacT [30] decomposes and reassembles parameter matrices in ViT. Surgical fine-tuning [36] of different network parts improves adaptation to distribution shifts. FLoRA [74] performs a batched low-rank adaptation. GLoRA [7] unifies cost-efficient PEFT methods. NOAH [82] uses parameter search on neural prompts. ARC [14] leverages cross-layer ViT similarity, parameter-sharing adapter and scaling factors for lower fine-tuning cost. RLRR [15] incorporates a residual term for flexibility while preserving pre-trained representation. RepAdapter [45] reparameterizes adapters for efficient inference. Res-tuning [29] unbinds tuners from the backbone for memory efficiency. Zhao _et al._[84] show impressive fine-tuning results by tuning layernorm in attention. OFT [54] and BOFT [42] propose orthogonal fine-tuning to preserve hypersphere energy between neurons.

**Consistency Regularization.** Fixmatch [61] applies consistency regularization over augmented images for semi-supervised learning. Openmatch [59] utilizes it on outlier predictions for open-set semi-supervised learning. R-Drop [76] applies it to transformers [68] with dropout for NLP tasks. CR [79] applies it over augmented real and fake images for GAN training. CAGAN [50] enforces consistency on discriminators with dropout for GAN training. Despite the empirical success of consistency regularization demonstrated by previous works, theoretical analysis is lacking. While NICE [47] demonstrates that consistency regularization lowers latent feature gradients for stable GAN training, it fails to reveal reduced weight gradient for enhanced generalization. Our study goes beyond prior works by providing a theoretical link between smaller weight gradients and improved generalization, effectively marrying generalization of PEFT with consistency regularization.

**Generalization of Fine-Tuning.** Li _et al_. [38] constrain the fine-tuned model's closeness to the pre-trained model in weight space. Fu _et al_. [18] induce sparsity on PEFT for better generalization. Wang _et al_. [72] studies generalization of PEFT fine-tuning graph neural network. Zhang _et al_. [83] employ rank-1 gradient boosting (GB) updates supported by the GB theoretical framework. VioLET [73], PromptSRC [31] and CoPrompt [58] naively align the fine-tuned model with the pre-trained one for enhanced generalization or avoiding forgetting. Additionally, L2SP [77], DELTA [40], and FTP [64] aim to retain pre-trained knowledge by aligning fine-tuned models with pre-trained ones, reducing distance in weight space, feature space and using projected gradient descent, respectively. However, they fail to provide a theoretical analysis for this alignment. Our study goes beyond understanding generalization of PEFT by discovering the benefits of gradient regularization and model alignment. We propose PACE to match both requirements, paving a comprehensive understanding for PEFT.

**Gradient regularization.** Previous studies have empirically shown that gradient regularization improves performance [67; 85; 48; 49] and adversarially robust accuracy [13]. However, they lack theoretical connection between smaller gradient norms and better generalization [17; 81; 6]. We bridge this gap by establishing a fundamental theory between reduced gradient norms and improved generalization, providing a solid foundation for future research on enhancing generalization.

## 3 Approach

We begin with a unified perspective on cost-efficient PEFT based on GLoRA [7], linking generalization with gradients and large-scale data, and motivating the alignment of the fine-tuned model with the pre-trained model to leverage its knowledge. We identify limitations of naive alignment in gradient regularization and introduce PACE, which implicitly enhances gradient regularization and model alignment. We conclude with theoretical justification and efficient implementations.

### A unified perspective on cost-efficient PEFT methods

The transformer architectures [68; 16] have excelled in natural language processing and computer vision tasks through their powerful sequential modeling capabilities. This success stems from their ability to process text/image tokens through \(L\) transformer blocks, where each block contains self-attention and MLP modules primarily composed of linear layers. These linear layers enable the self-attention mechanism to capture long-range dependencies, allowing transformers to achieve superior performance when scaled to a huge number of parameters and trained on extensive datasets.

With massive parameters, pre-trained on large-scale data, transformers serve as foundation models that can be fine-tuned for downstream tasks using limited data. However, fully fine-tuning all parameters for various downstream tasks requires substantial memory and can lead the forgetting of pre-trained knowledge. To alleviate this without increasing inference cost, adapters with lightweight parameters are often preferred for fine-tuning. Let \(\bar{h}_{0}(\cdot)\) be a transformation within the pre-trained transformer. Current adapters can be unified as introducing a residual branch \(\Delta\bar{h}\) to form a new transformation \(\bar{h}\):

\[\bar{h}(\bm{a})=\bar{h}_{0}(\bm{a})+\Delta\bar{h}(\bm{a}).\] (1)

Here, \(\bm{a}\) is the input and \(\bar{h}_{0}(\cdot)\) can represent MLP modules, as in Adapter [25] and AdaptFormer [8], or linear layers in self-attention and MLP modules, as in [26; 7; 12; 34]. In SSF [41], \(\bar{h}_{0}(\cdot)\) is the identity mapping and \(\Delta\bar{h}(a)=\bm{a}\odot(\bm{\gamma}-\bm{1})+\bm{\beta}\) with \(\bm{\gamma}\) and \(\bm{\beta}\) as affine transformation parameters.

Given that linear layers are key components in transformer, tuning them offers a flexible and effective way to adapt models to downstream tasks. This work focuses on methods that tune the linear layer without increasing inference cost. Let \((\bm{W}_{0},\bm{b}_{0})\), \((\Delta\bm{W},\Delta\bm{b})\), and \((\bm{W},\bm{b})\) be the parameters of pre-trained model, adapter and fine-tuned model, respectively, where \(\bm{W}_{0},\Delta\bm{W},\bm{W}\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}\) and \(\bm{b}_{0},\Delta\bm{b},\bm{b}\in\mathbb{R}^{d_{\text{out}}}\). Fine-tuning a linear layer in self-attention or MLP module can be formed as:

\[h(\bm{a}) =\bm{W}\bm{a}+\bm{b}=(\bm{W}_{0}+\Delta\bm{W})\bm{a}+(\bm{b}_{0} +\Delta\bm{b})\] \[=h_{0}(\bm{a})+\Delta h(\bm{a})=(\bm{W}_{0}\bm{a}+\bm{b}_{0})+( \Delta\bm{W}\bm{a}+\Delta\bm{b}).\] (2)

Based on GLoRA [7], cost-efficient PEFT methods for linear layers vary in the form of \(\Delta\bm{W},\Delta\bm{b}\):

**LoRA\({}_{\text{add}}\)**: \(\Delta\bm{W}=\bm{W}_{\text{d}}\bm{W}_{\text{u}},\Delta\bm{b}=\bm{b}_{\text{lora}}\) where \(\bm{W}_{\text{d}}\in\mathbb{R}^{d_{\text{on}}\times r},\bm{W}_{\text{u}}\in \mathbb{R}^{r\times d_{\text{a}}}\), and \(r\) is the rank.

**LoRA\({}_{\text{mul}}\)**: \(\Delta\bm{W}\!=\!\bm{W}_{0}\odot(\bm{W}_{\text{d}}\bm{W}_{\text{u}})\), \(\Delta\bm{b}\!=\!\bm{b}_{0}\odot\bm{b}_{\text{lora}}\), including RepAdapter [45] via reparameterization.

**VPT\({}_{\text{add}}\)**: \(\Delta\bm{W}\) is zero, \(\Delta\bm{b}=\bm{W}_{0}\bm{P}\), with learnable \(\bm{P}\in\mathbb{R}^{d_{\text{on}}\times 1}\) as layer-wise visual prompt. We use VPT\({}_{\text{add}}\) to differentiate from VPT [28], which concatenates \(\bm{P}\) with tokens, increasing inference cost.

### Generalization of deep neural networks

Having established a unified perspective on cost-efficient PEFT, we now motivate our method from a perspective on improving generalization of neural networks to enhance performance on unseen data. Consider a network \(f:=\phi(g(x))\) with \(l\) layers, where \(g\) is feature extractor and \(\phi\) is the classification head. Let \(\bm{\theta}:=\{(\bm{W}^{(i)},\bm{b}^{(i)})\}_{i=1}^{l}\) be the parameter set with dimension \(d\) and \(\mathcal{D}^{n}:=\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{n}\) be the training set of size \(n\) drawn _i.i.d._ from distribution \(\mathscr{D}\), which contains infinite data. The following lemma from [17] explains the relationship between the empirical and population loss.

**Lemma 1**: _(Theorem 1 from [17]) Let \(\mathcal{L}_{\mathcal{D}^{n}}(\bm{\theta})\) be the empirical loss function over \(f\) on training set \(\mathcal{D}^{n}\) and \(\mathcal{L}_{\mathscr{D}}(\bm{\theta})\) be the population loss. For any \(\rho>0\), with high probability over \(\mathcal{D}^{n}\sim\mathscr{D}\), we have_

\[\mathcal{L}_{\mathscr{D}}(\bm{\theta})\leq\max_{\|\bm{\epsilon}\|_{2}\leq\rho }\mathcal{L}_{\mathcal{D}^{n}}(\bm{\theta}+\bm{\epsilon})+R\Big{(}\frac{\|\bm {\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)},\] (3)

_where \(R:(\mathbb{R}_{+},\mathbb{R}_{+})\rightarrow\mathbb{R}_{+}\) is an increasing function (under conditions on \(\mathcal{L}_{\mathscr{D}}(\bm{\theta})\) and \(n\) as in SSB.5)._

Lemma 1 bounds the population loss by the empirical loss with perturbed weights, indicating that a minimal empirical loss increase from small weight perturbations implies low population loss.

By observing that the maximum of \(\mathcal{L}_{\mathcal{D}^{n}}\) is achieved at \(\bm{\epsilon}=\frac{\rho\bm{\nabla}_{\bm{\theta}}}{\|\bm{\nabla}_{\bm{\theta} }\|_{2}}\), where \(\bm{\nabla}_{\bm{\theta}}\) is the gradient of \(\mathcal{L}_{\mathcal{D}^{n}}\) at \(\bm{\theta}\), and performing a Taylor expansion of \(\mathcal{L}_{\mathcal{D}^{n}}\) around \(\bm{\theta}\), we formulate the following theorem.

**Theorem 1**: _Denote \(\bm{\nabla}_{\bm{\theta}}\) as the gradient and \(\lambda_{\text{max}}^{\bm{H}}\) as the largest eigenvalue of the Hessian matrix \(\bm{H}_{\bm{\theta}}\) of \(\mathcal{L}_{\mathcal{D}^{n}}\) at \(\bm{\theta}\). For any \(\rho>0\), with high probability over training set \(\mathcal{D}^{n}\sim\mathscr{D}\), we have_

\[\mathcal{L}_{\mathscr{D}}(\bm{\theta})\leq\mathcal{L}_{\mathcal{D}^{n}}(\bm{ \theta})+\rho\|\bm{\nabla}_{\bm{\theta}}\|_{2}+\frac{\rho^{2}}{2}\lambda_{ \text{max}}^{\bm{H}}+R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{ n}\Big{)}.\] (4)

_Here, higher-order terms from the Taylor expansion are incorporated into \(R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)}\), which is related to weights norm and inversely related to the training data size \(n\)._

Theorem 1 (proof in SSB.1) outlines strategies for enhancing generalization. They involve regularizing weight norms and the largest Hessian eigenvalues, and crucially, increasing data size \(n\) and reducing the weight gradient norms (illustrated in Figure 1). However, excessive reduction should be avoided as it could impair network's representation capacity, yielding higher empirical and population loss.

### Motivation and limitation of aligning the fine-tuned model with the pre-trained model

Theorem 1 emphasizes that large-scale data and smaller gradient magnitudes are essential for better generalization in neural network training. Therefore, aligning the fine-tuned model with the pre-trained one is crucial, as it ensures retention of knowledge obtained from large-scale data, preserving generalization. PEFT methods, often outperforming full fine-tuning, achieve this alignment by limiting the number of trainable parameters, restricting the model's capacity to deviate from the pre-trained one. However, the training objective prioritizes downstream task performance, compromising alignment with pre-trained knowledge. While sparsity regularization [18] and weight decay on adapter weights help, they do not ensure alignment, as even small weight changes can lead to significant divergence in output space [75; 21; 17]. Therefore, we propose to achieve the alignment by reducing the FP-distance (output distance between fine-tuned and pre-trained models on training samples):

\[D^{\text{fp}}(\bm{\theta})=\frac{1}{n}\sum_{i=1}^{n}\lVert f(\bm{x}_{i};\bm{ \theta})-f(\bm{x}_{i};\bm{\theta}_{0})\rVert_{2}^{2},\quad\bm{\theta}=\bm{ \theta}_{0}+\Delta\bm{\theta},\] (5)

where \(\bm{\theta},\bm{\theta}_{0},\Delta\bm{\theta}\in\mathbb{R}^{d}\) are parameters for the fine-tuned model, pre-trained model and the adapter.

While reducing FP-distance keeps the fine-tuned model close to the pre-trained model, thus preserving its knowledge, it does not ensure reduced gradient magnitudes, leading to suboptimal generalization.

To understand the gradient-related limitations in this alignment, we assume \(\Delta\bm{\theta}\) is small enough for a Taylor expansion approximation. Following standard practices [17; 80; 2], we perform the expansion up to the second-order terms. Given the independence between elements in squared \(L_{2}\) distances (SSB.4) and to simplify our theories, we analyze a one-dimensional output for a single _i.i.d._ sample, which leads us to the following proposition.

**Proposition 1**: _Assuming \(\Delta\bm{\theta}\) is small, denote \(f(\bm{\theta})\in\mathbb{R}\) as the one-dimensional output for \(\bm{x}\), with \(\bm{\nabla}\) and \(\bm{H}\) as its gradient and Hessian at \(\bm{\theta}\). FP-distance over \(\bm{x}\) can be decomposed as follows:_

\[[f(\bm{\theta})-f(\bm{\theta}_{0})]^{2} =[f(\bm{\theta})-f(\bm{\theta}-\Delta\bm{\theta})]^{2}\approx \big{[}f(\bm{\theta})-[f(\bm{\theta})-\Delta\bm{\theta}^{T}\bm{\nabla}+\frac{ 1}{2}\Delta\bm{\theta}^{T}\bm{H}\Delta\bm{\theta}]\big{]}^{2}\] \[\approx[\Delta\bm{\theta}^{T}\bm{\nabla}-\frac{1}{2}\Delta\bm{ \theta}^{T}\bm{H}\Delta\bm{\theta}]^{2}.\] (6)

Prop. 1 establishes the relationship between weight gradients, adapter weights, and FP-distance. However, it remains unclear if it regulates gradients. Our experiments show that minimizing FP-distance can sometimes increase gradient magnitude, complicating efforts for managing gradient.

### Consistency regularization

To achieve better generalization by both regularizing gradients and aligning the fine-tuned model with the pre-trined model, we propose a consistency regularization loss for \(f\), encouraging invariance of \(f\) to the same input under varying multiplicative noise perturbations on the adapter weights, as follows:

\[D^{\text{pace}}(\bm{\theta})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\bm{z}_{1}, \bm{z}_{2}}\|f(\bm{x}_{i};\bm{\theta}_{0}+\bm{z}_{1}\odot\Delta\bm{\theta})-f (\bm{x}_{i};\bm{\theta}_{0}+\bm{z}_{2}\odot\Delta\bm{\theta})\|_{2}^{2},\] (7)

where \(\bm{z}_{1},\bm{z}_{2}\sim\mathcal{N}(\bm{1},\sigma^{2}\bm{I})\) is the multiplicative noise applied on adapter weight. To understand the generalization benefits in this consistency regularization, we simplify the analysis by focusing on one-dimensional output for a single sample, resulting in the following theorem.

**Theorem 2**: _Using notations from Prop. 1, let \(f(\bm{\theta}_{0}+\bm{z}\odot\Delta\bm{\theta})\in\mathbb{R}\) be the one-dimensional output for \(\bm{x}\). Define \(\Delta\theta_{j}\) as \(j\)-th element in \(\bm{\Delta\theta}\), \(\nabla_{j}\) as the \(j\)-th element in \(\bm{\nabla}\) and \(H_{jk}\) as the \((j,k)\)-entry in \(\bm{H}\). With \(\bm{z}_{1},\bm{z}_{2}\sim\mathcal{N}(\bm{1},\sigma^{2}\bm{I})\), the consistency loss over \(\bm{x}\) can be approximated as:_

\[\mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}[f(\bm{\theta}_{0}+\bm{z}_{1} \odot\Delta\bm{\theta})-f(\bm{\theta}_{0}+\bm{z}_{2}\odot\Delta\bm{\theta})]^ {2}\] \[\approx 2\sigma^{2}\!\sum_{j}\Delta\theta_{j}^{2}\nabla_{j}^{2}+ \sigma^{4}\!\sum_{j,k}\Delta\theta_{k}^{2}\Delta\theta_{j}^{2}H_{jk}^{2} =2\sigma^{2}\|\Delta\bm{\theta}\odot\bm{\nabla}\|_{2}^{2}\!+\!\sigma^{4}\|( \Delta\bm{\theta}\Delta\bm{\theta}^{T})\odot\bm{H}\|_{F}^{2}.\] (8)

Theorem 2 (proof in SSB.2) shows that the consistency regularization essentially penalizes the first- and second-order gradients of \(f\) at \(\bm{\theta}\) (illustrated in Figure 1), with the regularization strength controlled by the noise variance \(\sigma^{2}\) and adaptively influenced by the magnitude of elements in adapter weight \(\Delta\bm{\theta}\). Thus, minimizing the consistency loss implicitly regularizes the gradients, improving generalization.

With the FP-distance in Prop. 1 and consistency loss in Theorem 2, we establish their relationship as:

**Theorem 3**: _With \(d\) as the dimension of \(\bm{\theta}\), Eq. 6 can be upper-bounded as:_

\[[\Delta\bm{\theta}^{T}\bm{\nabla}-\frac{1}{2}\Delta\bm{\theta}^{T}\bm{H} \Delta\bm{\theta}]^{2}\leq 2d\|\Delta\bm{\theta}\odot\bm{\nabla}\|_{2}^{2}+d^{2}\|( \Delta\bm{\theta}\Delta\bm{\theta}^{T})\odot\bm{H}\|_{F}^{2}.\] (9)

Figure 1: Thm. 1: A flatter minimum has smaller gradient and Hessian norms, yielding better generalization. Thm. 2: Large gradient norms indicate large differences among perturbations. PACE minimizes these differences, reducing gradient norms. Thm. 3: Minimizing all pairs of distances between \(f(\bm{\theta}_{0}+\bm{z}_{1}\odot\Delta\bm{\theta})\) and \(f(\bm{\theta}_{0}+\bm{z}_{2}\odot\Delta\bm{\theta})\) where \(\bm{z}_{1},\bm{z}_{2}\!\sim\!\mathcal{N}(\bm{1},\sigma^{2}\bm{I})\) also reduces FP-distance (between fine-tuned \(f(\bm{\theta}_{0}+\Delta\bm{\theta})\) and pre-trained \(f(\bm{\theta}_{0})\)), especially when \(\bm{z}_{1}\!\!=\!\!\bm{1}\), \(\bm{z}_{2}\!=\!\bm{0}\) or vice versa.

Theorem 3 (proof in B.3) establishes the relationship between Eq. 6 and Eq. 8, showing Eq. 6 is upper-bounded by terms involving \(\|\Delta\bm{\theta}\odot\bm{\nabla}\|_{2}^{2}\) and \(\|(\Delta\bm{\theta}\Delta\bm{\theta}^{T})\odot\bm{H}\|_{F}^{2}\) which appear in Eq. 8. Reducing these terms results in a decrease in Eq. 6. Thus minimizing the consistency loss implicitly aligns the fine-tuned and pre-trained models (illustrated in Figure 1), preserving pre-trained knowledge.

### Efficient implementation of PACE

Providing different weight perturbations for each input in a mini-batch increases memory and computational demands. To avoid this, we perturb feature outputs from the adapter \(\Delta h(\cdot)\), effectively simulating perturbation that shares noise across each row in the weight matrix \(\Delta\bm{W}\). Our simple pipeline is shown in Figure 2. Consider \(\bm{X}\in\mathbb{R}^{B\times T\times d_{\text{in}}}\) as a batch of data where \(B\) and \(T\) are the batch and token sizes. The calculation for the linear layer of the fine-tuned model, which utilizes pre-trained weights \(\bm{W}_{0},\bm{b}_{0}\) and adapter weights \(\Delta\bm{W},\Delta\bm{b}\), processes an output size of \(d_{\text{out}}\) as:

\[h_{0}(\bm{X}) =\bm{W}_{0}\bm{X}+\bm{b}_{0};\quad\Delta h(\bm{X})=\Delta\bm{W} \bm{X}+\Delta\bm{b},\] (10) \[h(\bm{X}) =h_{0}(\bm{X})+\bm{Z}\odot\Delta h(\bm{X}).\] (11)

Operator \(\odot\) is the element-wise multiplication after expanding the left matrix \(\bm{Z}\in\mathbb{R}^{B\times d_{\text{out}}}\sim\mathcal{N}(\bm{1},\sigma^{2 }\bm{I})\) into \(B\times T\times d_{\text{out}}\) where tokens within the same example share the same noise. Motivated by [37], the \(\sigma\) decreases linearly as block depth increases. Let \(f_{1}(\cdot)\) and \(f_{2}(\cdot)\) be two networks share same weights but do not share the noise patterns. The loss function for PACE is:

\[\mathcal{L}^{\text{PACE}}=\frac{1}{n}\sum_{i=1}^{n}\ell(f_{1}(\bm{x}_{i}),\bm{ y}_{i})+\lambda\|f_{1}(\bm{x}_{i})-f_{2}(\bm{x}_{i})\|_{2}^{2},\] (12)

where \(\ell\) is the classification loss and \(\lambda\) is a hyperparameter controlling regularization strength. During inference, noise and regularization are ommitted, \(\Delta\bm{W},\Delta\bm{b}\) are integrated with \(\bm{W}_{0},\bm{b}_{0}\) for efficiency:

\[\bm{W}=\bm{W}_{0}+\Delta\bm{W};\quad\bm{b}=\bm{b}_{0}+\Delta\bm{b};\quad h( \bm{X})=\bm{W}\bm{X}+\bm{b}.\] (13)

**Efficient PACE variants.** In SSC, we present two variants that match the computational/memory costs of the baseline while achieving superior performance with substantially reduced resources.

## 4 Experiments

We combine LoRAmil and VPTadd to form a strong baseline LoRAmil+VPTadd, outperforming other combinations in most cases. We evaluate our method across four visual classification adaptation tasks: VTAB-1K [78], few-shot learning [30], FGVC [28] and domain adaptation [82]. We demonstrate PACE improves LoRA on GLUE [70] for text classification and GSM-8K [9] for text generation.

**Datasets and evaluations. VTAB-1K** comprises 19 datasets organized into (i) Natural images, (ii) Specialized datasets (remote sensing, medical) and (iii) Structured datasets (scene structure) domains. Each dataset has 1K training examples. Following [78; 28], we use the provided 800-200 train split for hyperparameter selection, evaluate using the full training set and report average accuracy across three trails. **Few-shot learning** involves 5 fine-grained datasets: FGVC-Aircraft [46], Food101 [4], OxfordFlowers102 [51], OxfordPets [53] and StanfordCars [35]. Following [30], we evaluate 1,

Figure 2: Our pipeline. Adapter \(\Delta h(\cdot)\) and \(h_{0}(\cdot)\) from pre-trained model form the linear layer \(h\) of Multi-Head Attention and MLP in fine-tuned model. We perturb \(\Delta h(\cdot)\) with multiplicative noise and ensure the network remains consistent to same inputs under varying perturbations.

2, 4, 8 and 16 shots, train on the provided training set, tune hyperparameters using validation and report average test accuracy over three random seeds. **FGVC** includes 5 fine-grained datasets: CUB-200-2011 [69], NABirds [66], OxfordFlowers [51], StanfordDogs [10] and StanfordCars [35]. We follow [28] to use validation set for hyperparameter and report test results. For **domain adaptation**, following [82, 7], we train on ImageNet [11] with a 16-shot setting, use the validation split by [82] for hyperparameter selection and report the results on the official validation set and 4 out-of-domain datasets: ImageNet-Sketch [71], ImageNet-V2 [56], ImageNet-A [23] and ImageNet-R [22]. We evaluate on GLUE [70] for **text classification** and GSM-8K [9] for **mathematical reasoning**.

**Pre-trained backbones**. We experiment with two vision transformers, Vision Transforms (ViT-B/16) [16] and Swin Transformer (Swin-B) [44]. These two are pre-trained on ImageNet-21K [11]. We test a ViT-B-Laion-IN12K model, pre-trained on Laino-2B [60] and fine-tuned on ImageNet-12K [11]. We use RoBERT\({}_{\text{base}}\)[43] and Phi-3-mini-4k-instruct [1] for text classification and generation.

**Implementation details**. We follow [28] for image processing: \(224\times 224\) resizing for VTAB-1K; random flips and crops to \(224\times 224\) for FGVC and few-shot learning; stronger augmentation for domain adaptation task, following [16, 82, 41]. We use the Adam optimizer [32] with cosine learning rate decay and linear warm-up (first 10 epochs). Models are fine-tuned for 300 epochs on VTAB-1K and 100 epochs on other vision adaptation tasks, with batch size 64. For text classification we follow [26]. See SSG for mathematical reasoning details. All experiments used an NVIDIA H100 GPU.

**Baseline.** For each dataset, we identified the better method (LoRA\({}_{\text{mult}}\)+VPT\({}_{\text{add}}\) or LoRA\({}_{\text{add}}\)) and tuned the rank, learning rate, and weight decay to form a strong baseline. The detailed baseline settings for each task and the number of trainable parameters are provided in SSF, where LoRA\({}_{\text{mult}}\)+VPT\({}_{\text{add}}\) generally outperformed other variants. Building on the strong LoRA\({}_{\text{mult}}\)+VPT\({}_{\text{add}}\), we use the grid search for our \(\lambda\) and \(\sigma\), following strategies from previous studies [28, 41, 26]. Beyond LoRA\({}_{\text{mult}}\)+VPT\({}_{\text{add}}\), PACE also enhances PEFT methods such as AdaptFormer, GLoRA, COFT, and BOFT (SSD.4).

\begin{table}
\begin{tabular}{|l|c c c c c|c c c c|c c c c c|} \hline \multirow{2}{*}{Method} & \multicolumn{6}{c|}{Natural} & \multicolumn{6}{c|}{Specialized} & \multicolumn{6}{c|}{Structured} & \multirow{2}{*}{} \\ \cline{2-13} \cline{5-13} \cline{13-13} \multicolumn{13}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \cline{2-13} \cline{13-13} \cline{13-13} \multicolumn{13-13}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline Full & 68.9 & 87.7 & 64.3 & 97.3 & 86.9 & 87.4 & 38.8 & 79.7 & 95.7 & 84.2 & 73.9 & 56.3 & 58.6 & 41.7 & 65.5 & 57.5 & 46.7 & 25.7 & 29.1 & 68.9 \\ Linear & 64.4 & 85.0 & 63.2 & 97.0 & 86.3 & 36.6 & 51.0 & 78.5 & 87.5 & 68.5 & 74.0 & 34.3 & 30.6 & 33.2 & 55.4 & 12.5 & 62.0 & 9.6 & 19.2 & 57.6 \\ \hline VPT-Deep & 78.8 & 80.8 & 65.8 & 98.0 & 88.3 & 78.1 & 49.6 & 81.8 & 96.1 & 83.4 & 68.4 & 68.5 & 60.0 & 46.5 & 72.8 & 73.6 & 47.9 & 32.9 & 37.8 & 72.0 \\ AdaptFormer & 69.2 & 90.1 & 68.0 & 98.8 & 89.9 & 82.8 & 54.3 & 84.0 & 94.9 & 81.7 & 95.8 & 80.9 & 65.3 & 43.6 & 78.3 & 74.8 & 48.5 & 29.9 & 41.6 & 73.9 \\ AdaptFormer & 70.8 & 91.2 & 70.5 & 99.1 & 90.9 & 86.6 & 54.8 & 83.0 & 95.8 & 84.4 & 76.3 & 81.9 & 64.3 & 49.3 & 80.3 & 76.3 & 45.7 & 31.7 & 41.1 & 74.7 \\ LoRA & 67.1 & 91.4 & 69.4 & 98.8 & 90.4 & 85.3 & 54.0 & 84.9 & 95.3 & 84.4 & 73.6 & 82.9 & 69.2 & 49.8 & 78.5 & 75.7 & 47.1 & 31.0 & 44.0 & 74.5 \\ NOAH & 69.6 & 92.7 & 70.2 & 99.1 & 90.4 & 86.1 & 53.7 & 84.4 & 85.3 & 79.8 & 82.8 & 68.9 & 49.9 & 81.7 & 81.8 & 84.3 & 32.8 & 44.2 & 74.2 \\ RepAdapter & 69.0 & 92.6 & **75.1** & 99.4 & 91.8 & 90.2 & 52.9 & 87.4 & 95.9 & 87.5 & 75.9 & 62.3 & 53.3 & 80.6 & 77.3 & 54.9 & 29.5 & 37.9 & 76.1 \\ RLRR & 75.6 & 92.4 & 72.9 & 99.3 & 91.5 & 89.7 & 87.0 & 86.8 & 92.5 & 85.3 & 75.9 & 79.7 & 64.2 & 53.9 & 82.1 & 83.9 & 53.7 & 33.4 & 43.6 & 76.7 \\ GLoRA & 76.4 & 92.9 & 74.6 & **99.6** & **92.5** & 91.5 & 57.8 & 87.3 & **96.8** & 88.0 & 76.0 & 83.1 & 67.3 & 54.5 & **86.2** & 83.8 & 52.9 & 37.0 & 41.4 & 78.0 \\ \hline Baseline & 74.9 & 93.3 & 72.0 & 99.4 & 91.0 & 91.5 & 54.8 & 85.2 & 95.7 & 86.9 & 74.2 & 83.0 & 70.5 & 51.9 & 81.4 & 77.9 & 51.7 & 33.6 & 44.4 & 76.4 \\ \hline
**PACC** & **79.0** & **94.2** & 73.6 & **99.4** & **92.4** & **93.7** & **58.0** & **87.4** & **96.4** & **89.3** & **77.1** & **84.9** & **79.0** & **54.9** & **84.3** & **84.7** & **57.3** & **39.3** & **44.8** & **79.0** \\ \hline \end{tabular}
\end{table}
Table 1: Results on VTAB-1K with ViT-B/16. Mean Acc. is the average of group mean values.

\begin{table}
\begin{tabular}{|l|c c c c c|c c c c c|c c c c c c|} \hline \multirow{2}{*}{Method} & \multicolumn{6}{c|}{FGVC Aircraft} & \multicolumn{6}{c|}{Food101} & \multicolumn{6}{c|}{Flowers102} \\ \cline{2-13} \cline{5-13} \cline{13-13} \cline{

### Comparison with the State of the Arts

**Results on VTAB-1K.** Table 1 presents the results comparing PACE with recent state-of-the-art PEFT methods. PACE improves the strong baseline by 2.6% accuracy, surpassing the previous SOTA GLoRA [7] by 1%, which uses two stages for parameter search. In SSD.1, we show that reducing training epochs to 50 or 100 has minimal impact on PACE performance.

**Results on Few-shot Learning**. Table 2 compares performance w/ and w/o our PACE. PACE improves LoRA\({}_{\text{add}}\), VPT\({}_{\text{add}}\), LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\), with LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) +PACE performing best in most cases. PACE yields notable improvement, especially when the number of shot is small.

**Results on FGVC.** Table 3 shows that PACE improves the strong LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) by 0.7%, outperforming SSF [41], ARC [14] and RLRR [15] that use strongly pre-trained ViT with augmentations. In SSD.2, PACE achieves larger improvements on smaller datasets.

**Results on domain adaptation.** Table 4 compares PACE with others. LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) outperforms GLoRA [7] which relies on parameter search. Meanwhile, PACE improves LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) by 1.5%, outperforming other PEFT methods, demonstrating superior performance on domain adaptation.

**Results on text classification and mathematical reasoning.** Table 5 shows that PACE outperforms LoRA by 1% on GLUE text classification and by 3.11% on GSM-8K mathematical reasoning.

**Generalization on other backbones.** We evaluate PACE on CIFAR-100 (VTAB-1K) and domain adaptation using Swin-B [44] pre-trained on ImageNet-21K and ViT-B (pre-trained on Laion 2B, then fine-tuned on ImageNet-12K). Table 7 shows PACE outperforms LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) and other PEFT methods across all backbones, demonstrating its strong generalizability. Further experiments in SSD.3 show PACE works effectively with self-supervised models such as MAE [19] and DINO [5].

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline Method & Accuracy \\ \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{ImageNet-1K} & \multicolumn{2}{c|}{CIFAR} & \multicolumn{2}{c|}{ImageNet-12K} & \multicolumn{2}{c|}{CIFAR} & \multicolumn{2}{c|}{ImageNet-1K} \\  & -100 & Src. -S -V -A -R & -100 & Src. -S -V -A -R & -100 & Src. -S -V -A -R \\ \hline Full & 51.6 & 63.9 & 18.5 & 52.5 & 3.2 & 21.2 & 51.2 \\ Linear & 63.4 & 67.9 & 14.4 & 60.8 & 9.4 & 25.6 & 61.9 \\ LoRA\({}_{\text{add}}\) & 71.2 & 73.8 & 27.1 & 64.8 & 13.6 & 25.0 & 71.3 \\ VPT\({}_{\text{add}}\) & 73.6 & 74.3 & 27.1 & 65.9 & 11.5 & 26.7 & 71.8 \\ LoRA\({}_{\text{mul}}\) & 74.3 & 78.1 & 31.2 & 68.3 & 13.4 & 23.7 & 73.2 & 76.6 \\ LoRA\({}_{\text{mul}}\) & 74.3 & 76.1 & 37.2 & 68.7 & 40.4 & 68.7 & 22.4 & 38.4 \\ LoRA\({}_{\text{mul}}\) & 74.3 & 78.1 & 31.2 & 68.3 & 13.4 & 23.7 & 73.2 & 78.6 \\ LoRA\({}_{\text{add}}\)+VPT\({}_{\text{add}}\) & 70.3 & 76.8 & 28.7 & 66.6 & 13.7 & 29.9 & 71.8 \\ \hline LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) & 74.9 & 77.8 & 30.6 & 68.1 & 14.1 & 32.5 & 73.8 & 73.4 \\ \hline \({}_{\text{add}}\)+VPT\({}_{\text{add}}\) & **79.0** & **79.3** & **81.8** & **69.4** & **16.3** & **35.2** & **78.0** \\ \hline \end{tabular}
\end{table}
Table 4: Results on domain adaptation with ViT-B/16 pre-trained on ImageNet-21K.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Method & CUB NA- & Oxford Stan. & Stan. & Mean \\ -2011 & Birds Flowers & Dogs & Cars & Acc. \\ \hline Full & 87.3 & 82.7 & 98.8 & 89.4 & 84.5 & 85.9 \\ Linear & 85.3 & 75.9 & 97.9 & 86.2 & 51.3 & 79.3 \\ VPT & 88.5 & 84.2 & 99.0 & 90.2 & 83.6 & 89.1 \\ LoRA & 88.3 & 85.6 & 99.2 & 91.0 & 83.2 & 89.5 \\ SSF* & 89.5 & 85.7 & 99.6 & 89.6 & 89.2 & 90.7 \\ ARC* & 89.3 & 85.7 & **99.7** & 89.1 & **89.5** & 90.7 \\ RLRR* & 89.8 & 85.3 & 99.6 & 90.0 & 90.4 & 91.0 \\ \hline LoRA\({}_{\text{mul}}\)+VPT\({}_{\text{add}}\) & 88.9 & 87.1 & 99.4 & 91.2 & 87.5 & 90.8 \\
**APACE** & **89.8** & **87.3** & **99.5** & **92.2** & **88.8** & **91.5** \\ \hline \end{tabular} 
\begin{tabular}{|l|c|c|c|} \hline Method & Accuracy \\ \hline \multirow{2}{*}{Pre-trained} & 62.01 \\  & 62.6 & 90.3 & 88.4 \\  & 65.6 & 90.7 & 89.5 & 78.7 & 91.8 & 94.6 & 85.2 \\ \hline LoRA & 63.4 & 91.5 & 89.7 & 86.6 & 93.3 & 95.1 & 86.6 \\ \hline \({}_{\text{\ddagger}}\)** & **66.2** & **92.0** & 91.4 & **86.9** & **93.6** & **95.6** & **87.6** \\ \hline \end{tabular}
\end{table}
Table 5: Results for GLUE w/ RoBERT\({}_{\text{base}}\). Matthew’s correlation for COLA, Pearson correlation for STSB, and accuracy for others.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Method & COLA & STSB & MRPC & RTE & QNLI & SST2 & Avg. \\ \hline Full & 63.6 & 91.2 & 90.2 & 78.7 & 92.8 & 94.8 & 85.2 \\ BitFit & 62.0 & 90.8 & **92.7** & 81.5 & 91.8 & 93.7 & 85.4 \\ Adapt & 62.6 & 90.3 & 88.4 & 75.9 & 93.0 & 94.7 & 84.2 \\ VeRA & 65.6 & 90.7 & 89.5 & 78.7 & 91.8 & 94.6 & 85.2 \\ \hline LoRA & 63.4 & 91.5 & 89.7 & 86.6 & 93.3 & 95.1 & 86.6 \\ \hline \({}_{\text{\ddagger}}\)** & **66.2** & **92.0** & 91.4 & **86.9** & **93.6** & **95.6** & **87.6** \\ \hline \end{tabular}
\end{table}
Table 7: Classification results on domain adaptation and CIFAR-100 in VTAB-1K based different pre-trained models. Src. is short for ‘source’ in Table 4.

### Analyses

To verify our theories, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16 and Camelyon (VTAB-1K) on Swin-B. Figures 3 & 4 show the gradient norm (summed across all layers) and FP-distance (Eq. 5) and the train & validation accuracy during training for baseline LoRAmul+VPTadd and PACE on validation set. Figures 2(a) & 3(a) show that PACE has a smaller gradient norm than baseline, verifying Theorem 2 that PACE can implicitly lower the weight gradient norm for better generalization. Figures 2(b) & 3(b) demonstrate that PACE maintains a lower FP-distance than the baseline, verifying Theorem 3 that PACE can implicitly align the fine-tuned model with pre-trained model, retaining knowledge from large-scale pre-training. Owing to the advantages of the gradient regularization and model alignment, PACE shortens the performance gap between seen and unseen data, yielding higher accuracy on the unseen validation set, as shown in Figures 2(c) & 3(c).

To clarify why naive alignment is problematic, we vary the regularization strength \(\lambda\) over a wide range (1e-3 to 5e4) for both Fine-tuned Pre-trained model Alignment (FPA) by minimizing \(D^{\text{fp}}\) in Eq. 5 and PACE. Figure 5 shows the averaged gradient norm over training (see also Figures 8 & 9 for more visualizations). PACE robustly lowers gradient norms with larger \(\lambda\), while FPA exhibits unpredictable behavior, even causing gradient explosion. This verifies Prop. 1 that minimizing \(D^{\text{fp}}\) is problematic for gradient regularization, complicating gradient management.

### Ablation studies

We ablate PACE based on the baseline LoRAmul+VPTadd on CIFAR-100 (VTAB-1K) and ImageNet-1K in domain adaption as shown in Table 8. The ablations include Noise (baseline w/ noise perturbing

Figure 4: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val. accuracy are evaluated on the validation set of Camelyon (VTAB-1K) with baseline LoRAmul+VPTadd on Swin-B.

Figure 5: Gradient norms of models across wide range of regularization strengths \(\lambda\) on CIFAR-100 (VTAB-1K) w/ ViT-B/16. Line and shadow represent mean and std across training epochs.

Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val. accuracy are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline LoRAmul+VPTadd on ViT-B/16.

adapter), \(\text{PACE}_{\text{add}}\) (replacing the multiplicative noise with the additive noise), \(\text{PACE}_{h}\) (perturbing \(h(\cdot)\) instead of \(\Delta h(\cdot)\) in Eq. 11), \(\text{PACE}_{\text{drop}}\) (replacing the Gaussian noise with the dropout noise), \(\text{PACE}_{\sigma=}\) (all transformer blocks share the same \(\sigma\)), \(\text{PACE}_{\sigma\uparrow}\) (\(\sigma\) increases linearly with depth), FPA (fine-tuned and pre-trained alignment by minimizing Eq. 5), SAM (sharpness-aware minimization [17]), GP (gradient penalization), \(\ell_{1}\) (sparsity regularization), and transfer learning methods L2SP [77], DELTA [40] and FTP [64]. We grid-search hyperparameters and report the best results.

Table 8 presents the results for all variants. PACE improves over Noise, which itself is better than baseline, justifying our adapter perturbation and consistency regularization. \(\text{PACE}_{\text{add}}\) performs worse than PACE, showing the superiority of the multiplicative noise. Although \(\text{PACE}_{h}\) can implicitly regularize gradients, it performs worse than PACE, verifying the advantages of perturbing adapter to implicitly align models. \(\text{PACE}_{\text{drop}}\) is worse than PACE, indicating the dropout noise is suboptimal. \(\text{PACE}_{\sigma=}\) and \(\text{PACE}_{\sigma\uparrow}\) perform worse, justifying our design of linearly decreasing \(\sigma\). FPA, SAM and GP, which either only align models or only regularize gradients, are outperformed by PACE. Despite combining FPA+GP, it still performs worse than ours, suggesting ineffective combination. \(\ell_{1}\), L2SP, DELTA, and FTP obtain worse results than PACE, showing their limitations in improving generalization. PACE regularizes gradients for better generalization and aligns models to retain knowledge, surpassing all other variants.

We further evaluate applying PACE across multiple \(M\) networks during training or applying it lazily with half-batch size at every \(N\) steps (\(\text{PACE}_{\text{lay}}^{\text{half}}\) in SSC). Figure 6 presents the results, showing that applying PACE among two networks at every training step performs best. However, lazy regularization applied every few steps can still provide reasonable results while saving computational/memory costs.

We test the sensitivity of hyperparameters \(\lambda\) and \(\sigma\) introduced in our PACE on OxfordPets for few-shot learning across 1, 2, 4, 8 shots. The results presented in Figure 7 demonstrate that with less data, larger \(\lambda\) and \(\sigma\) are favored, verifying the effectiveness of PACE in improving generalization.

## 5 Conclusions

We have introduced PACE, a novel and effective method that combines generalization of PArameter-efficient fine-tuning with Consistency rEolarization. Through rigorous theoretical analyses, we have shown PACE reduces weight gradient for improved generalization and it aligns the fine-tuned model with the pre-trained model for retaining pre-training knowledge. Our experimental results support the theoretical analyses, justifying the generalization advantages of PACE over other PEFT methods. With its dual advantages, PACE consistently outperforms other variants across different backbones, firmly establishing PACE as a powerful solution for enhancing generalization for PEFT methods. Limitations and border impacts are discussed in SSA.

**Acknowledgments.** We thank Moyang Liu, Melody Ip, Chenyi Du, and Yinuo Xu for their valuable discussions and support. PK is funded by CSIRO's Science Digital.

## References

* [1]M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al. (2024) Phi-3 technical report: a highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Cited by: SS1.
* [2]G. Alain and Y. Bengio (2014) What regularized auto-encoders learn from the data-generating distribution. JMLR15 (110), pp. 3743-3773. Cited by: SS1.
* [3]D. Bahdanau, K. Cho, and Y. Bengio (2014) Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Cited by: SS1.
* [4]L. Bossard, M. Guillaumin, and L. Van Gool (2014) Food-101-mining discriminative components with random forests. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pp. 446-461. Cited by: SS1.
* [5]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650-9660. Cited by: SS1.
* [6]J. Cha, S. Chu, K. Lee, H. Cho, S. Park, Y. Lee, and S. Park (2021) Swad: domain generalization by seeking flat minima. Advances in Neural Information Processing Systems34, pp. 22405-22418. Cited by: SS1.
* [7]A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen (2023) One-for-all: generalized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967. Cited by: SS1.
* [8]S. Chen, C. Ge, Z. Tong, J. Wang, Y. Song, J. Wang, and P. Luo (2022) Adaptformer: adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems35, pp. 16664-16678. Cited by: SS1.
* [9]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.
* [10]E. Dataset (2011) Novel datasets for fine-grained image categorization. In First Workshop on Fine Grained Visual Categorization, CVPR, Citeseer, Cited by: SS1.
* [11]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Cited by: SS1.
* [12]T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer (2024) Qlora: efficient finetuning of quantized lllms. Advances in Neural Information Processing Systems36. Cited by: SS1.
* [13]J. Dong, P. Koniusz, X. Qu, and Y. Ong (2025) Stabilizing modality gap & lowering gradient norms improves zero-shot adversarial robustness of vlms. 31st SIGKDD Conference on Knowledge Discovery and Data Mining. Cited by: SS1.
* [14]W. Dong, D. Yan, Z. Lin, and P. Wang (2024) Efficient adaptation of large vision transformer via adapter re-composing. Advances in Neural Information Processing Systems36. Cited by: SS1.
* [15]W. Dong, X. Zhang, B. Chen, D. Yan, Z. Lin, Q. Yan, P. Wang, and Y. Y. (2024) Low-rank rescaled vision transformer fine-tuning: a residual design approach. arXiv preprint arXiv:2403.19067. Cited by: SS1.
* [16]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. In International Conference on Learning Representations, Cited by: SS1.
* [17]P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur (2021) Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, Cited by: SS1.
* [18]Z. Fu, H. Yang, A. Man-C. So, W. Lam, L. Bing, and N. Collier (2023) On the effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, pp. 12799-12807. Cited by: SS1.
* [19]K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick (2022) Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009. Cited by: SS1.
* [20]K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick (2020) Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729-9738. Cited by: SS1.
* [21]Z. He, A. Siraj Rakin, J. Li, C. Chakrabarti, and D. Fan (2020) Defending and harnessing the bit-flip based adversarial weight attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14095-14103. Cited by: SS1.
* [22]D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. (2021) The many faces of robustness: a critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8340-8349. Cited by: SS1.
** [23] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15262-15271, 2021.
* [24] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* [25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* [26] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* [27] Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and Maosong Sun. Sparse structure search for delta tuning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [28] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In _European Conference on Computer Vision_, pages 709-727. Springer, 2022.
* [29] Zeynizi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, and Jingren Zhou. Res-tuning: A flexible and efficient tuning paradigm via unbinding tuner from backbone. _Advances in Neural Information Processing Systems_, 36, 2024.
* [30] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision transformer. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 1060-1068, 2023.
* [31] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15190-15200, 2023.
* [32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [34] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In _The Twelfth International Conference on Learning Representations_, 2024.
* [35] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* [36] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In _The Eleventh International Conference on Learning Representations_, 2023.
* [37] Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu. Dropkey for vision transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22700-22709, 2023.
* [38] Dongyue Li and Hongyang Zhang. Improved regularization and robustness for fine-tuning in neural networks. _Advances in Neural Information Processing Systems_, 34:27249-27262, 2021.
* [39] Shenguri Li, Xueting Han, and Jing Bai. Adaptergnn: Parameter-efficient fine-tuning improves generalization in gnns. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 13600-13608, 2024.
* [40] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu Chen, and Jun Huan. Delta: Deep learning transfer using feature map with attention for convolutional networks. _arXiv preprint arXiv:1901.09229_, 2019.
* [41] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [42] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Scholkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In _ICLR_, 2024.
* [43] Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.

* [45] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and Rongong Ji. Towards efficient visual adaption via structural re-parameterization. _arXiv preprint arXiv:2302.08106_, 2023.
* [46] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [47] Yao Ni and Piotr Koniusz. Nice: Noise-modulated consistency regularization for data-efficient gans. _Advances in Neural Information Processing Systems_, 36:13773-13801, 2023.
* [48] Yao Ni and Piotr Koniusz. Chain: Enhancing generalization in data-efficient gans via lipschitz continuity constrained normalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6763-6774, June 2024.
* [49] Yao Ni, Piotr Koniusz, Richard Hartley, and Richard Nock. Manifold learning benefits gans. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11265-11274, 2022.
* [50] Yao Ni, Dandan Song, Xi Zhang, Hao Wu, and Lejian Liao. Cagan: Consistent adversarial training enhanced gans. In _IJCAI_, pages 2588-2594, 2018.
* [51] Maria-Elena Nilsback and Andrew Zisserman. A visual vocabulary for flower classification. In _IEEE Conference on Computer Vision and Pattern Recognition_, volume 2, pages 1447-1454, 2006.
* [52] Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. Blackvip: Black-box visual prompting for robust transfer learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24224-24235, 2023.
* [53] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2012.
* [54] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. In _NeurIPS_, 2023.
* [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [56] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International conference on machine learning_, pages 5389-5400. PMLR, 2019.
* [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [58] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [59] Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set semi-supervised learning with open-set consistency regularization. _Advances in Neural Information Processing Systems_, 34:25956-25967, 2021.
* [60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [61] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in neural information processing systems_, 33:596-608, 2020.
* [62] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. _Transactions on Machine Learning Research_, 2022.
* [63] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability of prompt tuning for natural language processing. _arXiv preprint arXiv:2111.06719_, 2021.
* [64] Junjiao Tian, Yen-Cheng Liu, James S Smith, and Zsolt Kira. Fast trainable projection for robust fine-tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [65] Hugo Touvron, Thibaut Lavril, Gautier Lacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [66] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 595-604, 2015.

* [67] Daniel Varga, Adrian Csiszarik, and Zsolt Zombori. Gradient regularization improves accuracy of discriminative models. _arXiv preprint arXiv:1712.09936_, 2017.
* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [69] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Thecaltech-ucsdbirds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [70] Alex Wang. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [71] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. _Advances in Neural Information Processing Systems_, 32, 2019.
* [72] Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [73] Yaoming Wang, Yuchen Liu, Xiaopeng Zhang, Jin Li, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, and Qi Tian. Violet: Vision-language efficient tuning with collaborative multi-modal gradients. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 4595-4605, 2023.
* [74] Yeming Wen and Swart Chaudhuri. Batched low-rank adaptation of foundation models. _arXiv preprint arXiv:2312.05677_, 2023.
* [75] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. _Advances in neural information processing systems_, 33:2958-2969, 2020.
* [76] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. R-drop: Regularized dropout for neural networks. _Advances in Neural Information Processing Systems_, 34:10890-10905, 2021.
* [77] LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _International Conference on Machine Learning_, pages 2825-2834. PMLR, 2018.
* [78] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. _arXiv preprint arXiv:1910.04867_, 2019.
* [79] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. _arXiv preprint arXiv:1910.12027_, 2019.
* [80] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In _ICLR_, 2021.
* [81] Shan Zhang, Yao Ni, Jinhao Du, Yanxia Liu, and Piotr Koniusz. Semantic transfer from head to tail: Enlarging tail margin for long-tailed visual recognition. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1350-1360, 2024.
* [82] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. _arXiv preprint arXiv:2206.04673_, 2022.
* [83] Yifei Zhang, Hao Zhu, Aiwei Liu, Han Yu, Piotr Koniusz, and Irwin King. Less is more: Extreme gradient boost rank-1 adaption for efficient finetuning of llms. _arXiv preprint arXiv:2410.19694_, 2024.
* [84] Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention: Towards efficient multi-modal LLM finetuning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [85] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In _International Conference on Machine Learning_, pages 26982-26992. PMLR, 2022.

**PACE: Marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization**

**(Supplementary Material)**

**Yao Ni\({}^{\dagger}\)****Shan Zhang\({}^{\ddagger,\dagger}\)****Piotr Koniusz** *,\({}^{\lx@sectionsign,\ddagger}\)****

\({}^{\dagger}\)The Australian National University****Data6\({}^{\dagger}\)**PCSIRO**

\({}^{\ddagger}\)Australian Institute for Machine Learning, The University of Adelaide

\({}^{\dagger}\)yao.ni@anu.edu.au****shan.zhang@adelaide.edu.au****piotr.koniusz@data61.csiro.au

Footnote *: The corresponding author.

## Appendix A Broader impacts and limitations

### Broader impacts

Our work provides a powerful solution for improving generalization in Parameter Efficient Fine-Tuning (PEFT), allowing for effective fine-tuning of pre-trained models while reducing the heavily reliance on pre-training from scratch using large scale data. Our advancements in PEFT, supported by Theorems 1, 2 and 3, offer novel insights into gradient regularization and model alignment. These insights extend beyond PEFT and can be applied to other areas such as continual learning and transfer learning, potentially enhancing the performance and efficiency of models in various domains. By leveraging our findings, practitioners can develop more robust and adaptable models that generalize well to new tasks and environments, leading to more intelligent and versatile AI systems. In terms of negative impacts, the robustness of our fine-tuning method could potentially be misused to create more convincing deepfakes, raising concerns about the spread of misinformation, manipulation of public opinion, and malicious activities such as fraud, blackmail, or harassment. However, potential misuse is a downside with any improvements that have universal nature.

### Limitations

While our work effectively improves generalization ability, it introduces additional computational costs by requiring input samples to be passed through the network twice for regularization. However, this can be mitigated by using two efficient variants, \(\text{PACE}_{\text{fast}}\) and \(\text{PACE}_{\text{large}}^{\text{half}}\), proposed in SSC, where we demonstrate the potential for resource-efficient fine-tuning. Additionally, our method introduces extra hyperparameters \(\lambda\) and \(\sigma\), which require caution during hyperparameter search. Nonetheless, Figure 7 suggests that fewer training data requires larger \(\lambda\) and \(\sigma\) values, providing insight for hyperparameter tuning.

Proofs

### Proof of Theorem 1

Settling \(\bm{\epsilon}=\frac{\rho\bm{\nabla}_{\bm{\theta}}}{\|\bm{\nabla}_{\bm{\theta}}\|_{2}}\), we perform a second-order Taylor expansion of \(\mathcal{L}_{\mathcal{D}^{n}}\) around \(\bm{\theta}\). By incorporating the higher-order terms from the Taylor expansion into \(R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)}\), we derive:

\[\mathcal{L}_{\mathscr{D}}(\bm{\theta}) \leq\mathcal{L}_{\mathcal{D}^{n}}\Big{(}\bm{\theta}+\frac{\rho \bm{\nabla}_{\bm{\theta}}}{\|\bm{\nabla}_{\bm{\theta}}\|_{2}}\Big{)}+R\Big{(} \frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)}\] \[\approx\mathcal{L}_{\mathcal{D}^{n}}(\bm{\theta})+\rho\|\bm{ \nabla}_{\bm{\theta}}\|_{2}+\frac{\rho^{2}}{2\|\bm{\nabla}_{\bm{\theta}}\|_{2 }^{2}}\bm{\nabla}_{\bm{\theta}}^{T}\bm{H}_{\bm{\theta}}\bm{\nabla}_{\bm{\theta }}+R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)}.\] (14)

Assuming that the approximation does not alter the inequality relationship, _i.e._, it preserves the \(\leq\) relation on both sides and considering the largest eigenvalue of \(\bm{H}_{\bm{\theta}}\) as \(\lambda_{\text{max}}^{H}\), implying \(\bm{v}^{T}\bm{H}_{\bm{\theta}}\bm{v}\leq\lambda_{\text{max}}^{\bm{H}}\|\bm{v} \|_{2}^{2}\) for any \(\bm{v}\), we further bound Eq. 14 as follows and arrive at:

\[\mathcal{L}_{\mathscr{D}}(\bm{\theta})\leq\mathcal{L}_{\mathcal{D}^{n}}(\bm{ \theta})+\rho\|\bm{\nabla}_{\bm{\theta}}\|_{2}+\frac{\rho^{2}}{2}\lambda_{ \text{max}}^{\bm{H}}+R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1 }{n}\Big{)}.\]

### Proof of Theorem 2

The proof is motivated by Ni and Koniusz [47]. We include the proof process for completeness. Denote \(\bm{m}_{1}=\bm{z}_{1}-\bm{1},\bm{m}_{2}=\bm{z}_{2}-\bm{1}\) thus \(\bm{m}_{1},\bm{m}_{2}\sim\mathcal{N}(\bm{0},\sigma^{2})\)

\[d^{\text{pace}}= \mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}[f(\bm{\theta}_{0}+\bm{z}_{1} \odot\Delta\bm{\theta})-f(\bm{\theta}_{0}+\bm{z}_{2}\odot\Delta\bm{\theta})]^ {2}\] \[= \mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}[f(\bm{\theta}_{0}+\Delta\bm{ \theta}+(\bm{z}_{1}-\bm{1})\odot\Delta\bm{\theta})-f(\bm{\theta}_{0}+\Delta \bm{\theta}+(\bm{z}_{2}-\bm{1})\odot\Delta\bm{\theta})]^{2}\] \[= \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}}[f(\bm{\theta}+\bm{m}_{1} \odot\Delta\bm{\theta})-f(\bm{\theta}+\bm{m}_{2}\odot\Delta\bm{\theta})]^{2}.\] (15)

Defining \(\bm{v}:=\bm{m}_{1}\odot\Delta\bm{\theta}\) and \(\bm{u}:=\bm{m}_{2}\odot\Delta\bm{\theta}\), where \(\bm{v},\bm{u}\sim\mathcal{N}(\bm{0},\sigma^{2}\text{diag}(\Delta\bm{\theta} \odot\Delta\bm{\theta}))\), we can rewrite Eq. 15 as follows:

\[\mathbb{E}_{\bm{v},\bm{u}}[f(\bm{\theta}+\bm{v})-f(\bm{\theta}+ \bm{u})]^{2}\] \[\approx \mathbb{E}_{\bm{v},\bm{u}}\big{[}f(\bm{\theta})+\bm{v}^{T}\bm{ \nabla}+\frac{1}{2}\bm{v}^{T}\bm{H}\bm{v}-f(\bm{\theta})-\bm{u}^{T}\bm{\nabla }-\frac{1}{2}\bm{u}^{T}\bm{H}\bm{u}\big{]}^{2}\] \[= \mathbb{E}_{\bm{v},\bm{u}}\big{[}\bm{v}^{T}\bm{\nabla}+\frac{1}{ 2}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{\nabla}-\frac{1}{2}\bm{u}^{T}\bm{H}\bm{ u}\big{]}^{2}\] \[= \mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}-\bm{u})^{T}\bm{\nabla} \big{]}^{2}\] (16) \[+\mathbb{E}_{\bm{v},\bm{u}}\big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \nabla}\big{)}\big{(}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{H}\bm{u}\big{)}\big{]}\] (17) \[+\frac{1}{4}\mathbb{E}_{\bm{v}}[\bm{v}^{T}\bm{H}\bm{v}]^{2}+\frac {1}{4}\mathbb{E}_{\bm{u}}[\bm{u}^{T}\bm{H}\bm{u}]^{2}\] (18) \[-\frac{1}{2}\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}^{T}\bm{H}\bm{ v})(\bm{u}^{T}\bm{H}\bm{u})].\] (19)

Next, we derive the four terms, Eq. 16, 17, 18, and 19, respectively as follows:

**Eq. 16.** Using \(\mathbb{E}_{z_{1},z_{2}}[(z_{1}-z_{2})^{2}]=2\sigma^{2}\) for \(z_{1},z_{2}\sim\mathcal{N}(0,\sigma^{2})\), we can simplify (Eq. 16) as follows, noting that terms related to different dimensions are canceled due to zero-mean independent Gaussian noise:

\[\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}-\bm{u})^{T}\bm{\nabla} \big{]}^{2}=\mathbb{E}_{\bm{v},\bm{u}}\big{[}\sum_{j}(v_{j}-u_{j})^{2}\nabla_{j }^{2}]=2\sigma^{2}\sum_{j}\Delta\theta_{j}^{2}\nabla_{k}^{2}.\] (20)

**Eq. 17.** Utilizing \(E[z^{3}]=\mu^{3}+3\mu\sigma^{2}\) for \(z\sim\mathcal{N}(\mu,\sigma^{2})\), and noting that \(E[z^{3}]=0\) for \(\mu=0\), Eq. 17 is derived as:

\[\mathbb{E}_{\bm{v},\bm{u}}\big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \nabla}\big{)}\big{(}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{H}\bm{u}\big{)}\big{]}\] \[= \mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{\nabla})(\bm{v}^{T}\bm{H }\bm{v})\big{]}+\mathbb{E}_{\bm{u}}\big{[}(\bm{u}^{T}\bm{\nabla})(\bm{u}^{T}\bm{H }\bm{u})\big{]}-\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}^{T}\bm{\nabla})(\bm{u}^{ T}\bm{H}\bm{u})\big{]}-\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{u}^{T}\bm{\nabla})(\bm{v}^{T} \bm{H}\bm{v})\big{]}\] \[= 2\mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{\nabla})(\bm{v}^{T}\bm{ H}\bm{v})\big{]}=0.\] (21)

[MISSING_PAGE_FAIL:17]

### Proof of Theorem 3

The Cauchy-Schwarz inequality states that for \(\bm{u},\bm{v}\in\mathbb{R}^{d}\), we have \((\sum_{j}u_{j}v_{j})^{2}\leq(\sum_{j}u_{j}^{2})(\sum_{j}v_{j}^{2})\). Let \(\bm{u}=\bm{1}\), it follows that \((\sum_{j}v_{j})^{2}\leq d\|\bm{v}\|_{2}^{2}\). Using this inequality, we then prove the following:

\[[\Delta\bm{\theta}^{T}\bm{\nabla}-\frac{1}{2}\Delta\bm{\theta}^{T}\bm{H}\Delta \bm{\theta}]^{2}\leq 2[\Delta\bm{\theta}^{T}\bm{\nabla}]^{2}+[\Delta\bm{\theta}^{T} \bm{H}\Delta\bm{\theta}]^{2}\]

\[[\Delta\bm{\theta}^{T}\bm{\nabla}]^{2}=\Big{(}\sum_{j}\Delta\theta_{j}\nabla_ {j}\Big{)}^{2}\leq d\|\Delta\bm{\theta}\odot\bm{\nabla}\|_{2}^{2}.\] (30)

\[[\Delta\bm{\theta}^{T}\bm{H}\Delta\bm{\theta}]^{2}=\Big{(}\sum_{j,k}\Delta \theta_{j}\Delta\theta_{k}H_{jk}\Big{)}^{2}\leq d^{2}\big{\|}(\Delta\bm{ \theta}\Delta\bm{\theta}^{T})\odot\bm{H}\big{\|}_{F}^{2}\] (31)

Here, the inequality is obtained by treating \(\Delta\theta_{j}\Delta\theta_{k}H_{jk}\) as an element of a vector with size of \(d^{2}\). This leads to the final results.

### Rationale for one-dimensional output analysis

We use the squared \(L_{2}\) distance for multi-dimensional outputs for \(D^{\text{fp}}\) and \(D^{\text{pace}}\), which allows our one-dimensional analysis to naturally generalize to multiple dimensions. For example, for a vector-valued function in the naive alignment, \(f(\bm{\theta})=[f_{1}(\bm{\theta}),...,f_{m}(\bm{\theta})]\), where \(m\) is the output dimension, we have:

\[\|f(\bm{\theta}_{0})-f(\bm{\theta}_{0}+\Delta\bm{\theta})\|_{2}^{2}=\sum_{i=1 }^{m}[f_{i}(\bm{\theta}_{0})-f_{i}(\bm{\theta}_{0}+\Delta\bm{\theta})]^{2}.\]

This equality shows that the squared \(L_{2}\) distance in multiple dimensions is simply the sum of non-negative squared differences in each dimension. Consequently, this additive nature enables our one-dimensional analysis to extend seamlessly to multiple dimensions in practice, aligning with our empirical observations.

### \(R\) increases with \(\frac{1}{n}\)

According to [17], the function \(R\big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\big{)}\) in Eq. 3 is defined as:

\[R\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n}\Big{)}=\sqrt{ \frac{k\log\Big{(}1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+\sqrt{ \frac{\log n}{k}}\big{)}^{2}\Big{)}+4\log\frac{n}{\delta}+8\log(6n+3k)}{n-1}}.\]

Here \(k\) is the number of parameters, \(n\) is the number of training samples, \(\delta\in(0,1]\) is the confidence level and \(\rho\) is the max norm of the Gaussian perturbation noise.

To ensure \(R\) is valid, we require \(n>1\). To analyze how \(R\) changes with \(n\), we fix \(\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\) and break the expression under the square root of \(R\) into three terms:

\[R_{1}=\frac{k\log\Big{(}1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+ \sqrt{\frac{\log n}{k}}\big{)}^{2}\Big{)}}{n-1},\quad R_{2}=\frac{4\log n-4 \log\delta}{n-1},\quad R_{3}=\frac{8\log(6n+3k)}{n-1}\]

We analyze each term separately to determine whether it decreases with increasing \(n\).

**Analysis for \(\bm{R_{1}}\):** The derivative for \(R_{1}\) w.r.t. \(n\) is:\[R_{1}^{\prime} =\frac{\frac{k}{1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+ \sqrt{\frac{\log n}{k}}\big{)}}\cdot\frac{1}{2\sqrt{\frac{\log n}{k}}}\cdot \frac{1}{kn}\cdot(n-1)-k\log\Big{(}1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}} \big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2}\Big{)}}{(n-1)^{2}}.\] \[=\frac{\frac{\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+ \sqrt{\frac{\log n}{k}}\big{)}}{\frac{1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2 }}\big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2}}\cdot\frac{1}{\sqrt{\frac{\log n }{k}}}\cdot\frac{n-1}{n}-k\log\Big{(}1+\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2 }}\big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2}\Big{)}}{(n-1)^{2}}}\] \[<\frac{\frac{\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+ \sqrt{\frac{\log n}{k}}\big{)}}{\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(} 1+\sqrt{\frac{\log n}{k}}\big{)}^{2}}\cdot\frac{1}{\sqrt{\frac{\log n}{k}}}-k \log\Big{(}\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\big{(}1+\sqrt{\frac{\log n }{k}}\big{)}^{2}\Big{)}}{(n-1)^{2}}\] \[<\frac{\frac{1}{1+\sqrt{\frac{\log n}{k}}}\cdot\frac{1}{\sqrt{ \frac{\log n}{k}}}-k\Big{(}\log\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}+\log \big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2}\Big{)}}{(n-1)^{2}}\] \[<\frac{\frac{1}{\sqrt{\frac{\log n}{k}}}\cdot\frac{1}{\sqrt{\frac {\log n}{k}}}-k\log\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}-k\log\big{(}1+ \sqrt{\frac{\log n}{k}}\big{)}^{2}}{(n-1)^{2}}\] \[=\frac{k}{(n-1)^{2}}\cdot\Big{(}\frac{1}{\log n}-\log\frac{\|\bm {\theta}\|_{2}^{2}}{\rho^{2}}-\log\big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2} \Big{)}.\]

Since \(\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}}\) is generally large, the smallest \(n\) is 2 and \(\log\big{(}1+\sqrt{\frac{\log n}{k}}\big{)}^{2}>0\). Therefore, for \(n>1\), \(R_{1}^{\prime}<0\), meaning \(R_{1}\) decreases as \(n\) increase.

**Analysis of \(\bm{R_{2}}\):** The derivative for \(R_{2}\) w.r.t. \(n\) is

\[R_{2}^{\prime}=\frac{4}{(n-1)^{2}}(1-\frac{1}{n}-\log n+\log\delta).\]

Since \(\delta\leq 1\), for \(n>1\), \(R_{2}^{\prime}<0\), indicating that \(R_{2}\) decreases with increasing \(n\).

**Analysis of \(\bm{R_{3}}\):** The derivative for \(R_{3}\) w.r.t. \(n\) is

\[R_{3}^{\prime}=\frac{8\big{(}6n-1\big{)}-\log(6n+3k)\big{)}}{(n-1)^{2}}<\frac{ 8\big{(}1-\log(6n+3k)\big{)}}{(n-1)^{2}}.\]

For \(n>1\), \(\log(6n+3k)>1\), implying that \(R_{3}^{\prime}<0\) and \(R_{3}\) decrease as \(n\) increases.

**Conclusion.** For \(n>1\), all terms \(R_{1}\), \(R_{2}\) and \(R_{3}\) decreases as \(n\) increases. Thus \(R(\frac{\|\bm{\theta}\|_{2}^{2}}{\rho^{2}},\frac{1}{n})\) is a decreasing function of \(n\).

## Appendix C Efficient PACE variants

Building upon strong theoretical foundation of PACE for generalization, we demonstrate that simple modifications can reduce memory and training time requirements of PACE. In this section, we explore two efficient variants, \(\text{PACE}_{\text{fast}}\) and \(\text{PACE}_{\text{lary}}^{\text{half}}\), both maintaining similar computational and memory requirements as the baseline while improving performance. We then provide empirical results which show that \(\text{PACE}_{\text{fast}}\) slightly outperforms \(\text{PACE}_{\text{lazy}}^{\text{half}}\) while requiring no additional hyperparameters and using fewer computational resources. Given its superior efficiency, we further explore the potential of \(\text{PACE}_{\text{fast}}\) for resource-efficient fine-tuning. By simply reducing the batch size and epochs, \(\text{PACE}_{\text{fast}}\) outperforms the baseline while using significantly less GPU memory and training time.

\(\text{PACE}_{\text{fast}}\): Building on the observation that only small datasets are typically available for fine-tuning, we assume that the model behavior changes gradually across epochs. Under this assumption, we store the model outputs from the previous epoch (\(f_{e-1}(\bm{x})\)), which contain inherent noise due to the adapter perturbation, and compute the consistency regularization loss between these stored outputs and the current epoch's noiseed outputs:

\[d_{\text{fast}}^{\text{pace}}(\bm{x})=\|f(\bm{x})-\bm{o}_{e-1}\|_{2}^{2};\quad \text{where}\quad\bm{o}_{e-1}=f_{e-1}(\bm{x}).\] (32)Here the output vector \(\bm{o}\in\mathbb{R}^{C}\), where \(C\) is the number of classes. Since \(f(\cdot)\) applies noise perturbation to the adapter and changes gradually between epochs, \(f_{e-1}(\bm{x})\) and \(f(\bm{x})\) can be seen as applying different _i.i.d._ noises to similar model states. This approach preserves the theoretical foundation of PACE while incurring minimal storage and computation costs. With typically few classes \(C\) and a limited number of samples in fine-tuning, storing \(\bm{o}_{e-1}\) within GPU or CPU memory is manageable.

\(\mathbf{PACE_{\text{lazy}}^{\text{half}}}\).During training, the network always applies noise perturbations. Every \(N\)-th iteration uses a half batch size and consistency regularization, while all other iterations use the full batch size.

**Memory and computational efficiency of two variants.** Both variants maintain similar computational and memory requirements as the baseline. To demonstrate this, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16, Camelyon (VTAB-1K) with Swin-B, and ImageNet (domain adaptation) with ViT-B/16. Table 9 compares maximum GPU memory usage, total training time, and accuracy for each task, showing that \(\text{PACE_{\text{fast}}}\) and \(\text{PACE_{\text{lazy}}^{\text{half}}}\) significantly improve upon the baseline while maintaining similar computational demands.

We find that \(\text{PACE_{\text{fast}}}\) slightly outperforms \(\text{PACE_{\text{lazy}}^{\text{half}}}\) without requiring additional hyperparameters, yet it needs to store outputs from the previous epoch. We therefore analyze its memory requirements.

**Memory efficiency of \(\mathbf{PACE_{\text{fast}}}\).** We compare the additional memory requirement of \(\text{PACE_{\text{fast}}}\) with the baseline GPU memory consumption. Table 10 shows that the memory overhead of \(\text{PACE_{\text{fast}}}\) is negligible compared to the baseline GPU memory requirements and can be easily stored in GPU. Moreover, even in the rare scenario of fine-tuning on the full ImageNet 1K dataset (1.2 million samples), \(\text{PACE_{\text{fast}}}\) requires only 4.8GB of additional memory for storing the output of the model's classification head. This is significantly smaller than the dataset itself (>100GB) and can be easily accommodated in the CPU/GPU memory.

**Resource-Efficient training with \(\mathbf{PACE_{\text{fast}}}\).** Given the superior performance, minimal memory overhead, and no need for additional hyperparameters of \(\text{PACE_{\text{fast}}}\), we explore its potential for resource-efficient training by maintaining the same number of updates with reduced batch size and proportionally reduced epochs. Table 11 shows that even with 1/8 batch size and epochs, \(\text{PACE_{\text{fast}}}\) still outperforms the baseline by 1.7% while only using \(\sim\)1/3 GPU memory and \(\sim\)1/4 training time. This demonstrates the robustness and generalization benefits that \(\text{PACE_{\text{fast}}}\) brings to models, enabling them to excel under constrained training configurations. Such an efficiency is particularly valuable for fine-tuning large foundation models, where resource constraints necessitate small batch sizes and typically lead to sharp loss landscapes, yet the theoretical guarantee of PACE for smooth loss landscapes provides a promising solution for these challenges.

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline Dataset & Memory of \(\text{PACE_{\text{fast}}}\) & Baseline GPU Memory & Ratio \\ \hline CIFAR-100 (VTAB-1K w/ ViT/16-B) & 390KB & 8.9GB & 0.0042\% \\ Camelyon (VTAB-1K w/ Swin-B) & 7.81KB & 15.7GB & 0.000047\% \\ ImageNet (Domain adaptation w/ ViT/16-B) & 61MB & 8.9GB & 0.67\% \\ \hline \end{tabular}
\end{table}
Table 10: Comparison of \(\text{PACE_{\text{fast}}}\) memory overhead and the baseline GPU memory requirements.

\begin{table}
\begin{tabular}{|l|c c c|c c c|c c c|} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{CIFAR-100 (ViT/16-B)} & \multicolumn{3}{c|}{Camelyon (Swin-B)} & \multicolumn{3}{c|}{ImageNet (ViT/16-B)} \\ \cline{2-10}  & GPU Memory & Time & Accuracy & GPU Memory & Time & Accuracy & GPU Memory & Time & Mean Acc. \\ \hline LoRA\({}_{\text{multi}}\)+VPT\({}_{\text{sd}}\) & 8.9GB & 29m & 74.6 & 15.7GB & 33m & 86.7 & 8.9GB & 161m & 44.8 \\ \hline +PACE & 17.7GB & 53m & 79.0 & 29.4GB & 60m & 89.3 & 17.7GB & 278m & 46.3 \\ \(\text{+PACE_{\text{fast}}}\) & **9.0GB** & **29m** & **78.3** & 15.7GB & 34m & 88.8 & **9.0GB** & **162m** & **46.1** \\ \(\text{+PACE_{\text{lazy}}^{\text{half}}}\) (\(N=2\)) & 9.3GB & 29m & 78.7 & **15.7GB** & **36m** & **89.2** & 9.0GB & 165m & 46.0 \\ \(\text{+PACE_{\text{lazy}}^{\text{half}}}\) (\(N=4\)) & 9.3GB & 29m & 78.4 & 15.7GB & 35m & 88.9 & 9.0GB & 163m & 45.6 \\ \(\text{+PACE_{\text{lazy}}^{\text{half}}}\) (\(N=6\)) & 9.3GB & 29m & 78.4 & 15.7GB & 35m & 89.0 & 9.0GB & 163m & 45.7 \\ \(\text{+PACE_{\text{lazy}}^{\text{half}}}\) (\(N=10\)) & 9.3GB & 29m & 78.2 & 15.7GB & 35m & 88.9 & 9.0GB & 162m & 45.6 \\ \hline \end{tabular}
\end{table}
Table 9: GPU memory usage, training time, and accuracy for \(\text{PACE_{\text{fast}}}\) and \(\text{PACE_{\text{lazy}}^{\text{half}}}\). here, ‘m’ denotes minutes, Both variants outperform the baseline while maintaining similar computational demands.

## Appendix D Additional Experiments

In this section, we provide additional experiments of PACE on VTAB-1K with different epochs, varying training data sizes on FGVC benchmarks, self-supervised pre-trained backbones and combinations with other PEFT methods.

### Experiments of VTAB-1K with different epochs

In Table 1, We use 300 epochs for VTAB-1K tasks as we observed slight improvements over 100 epochs. However, this does not mean PACE requires longer training to converge. Since the optimizer uses the cosine learning rate decay, reducing the number of training epochs to 100 has a minimal impact on performance, as shown in Table 12.

To ensure fair memory and computational budgets, we also tested PACE with half the batch size and 50 epochs. Table 12 shows that under these conditions, PACE still improves baseline accuracy by 2.10%, and outperforms the previous SOTA GLoRA, which uses 500 epochs for training and 30 for parameter search. These results demonstrate PACE's efficiency and effectiveness across various training configurations.

### Experiments on FGVC with limited training data

To validate generalization benefits of PACE on limited data settings, we conduct experiments on FGVC using 50%, 20%, and 10% of the original training samples. Table 13 shows that PACE achieves larger improvements with smaller data sizes, aligning with our theoretical analyses.

### Experiments on self-supervised pre-trained backbones

To further verify the effectiveness of PACE on a self-supervised pre-trained backbone, we conduct VTAB-1K experiments on SVHN, Camelyon, and Clevr-Count using MAE [19] and DINO [19], with ViT-B/16 pre-trained on ImageNet-1K [11]. Table 14 shows that PACE improves the baseline on these self-supervised backbones, confirming its applicability to fine-tuning self-supervised models.

\begin{table}
\begin{tabular}{|l|c|c c c c|c c c|c c c|c c c|} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{CUB} & \multicolumn{3}{c|}{NAB} & \multicolumn{3}{c|}{Flowers} & \multicolumn{3}{c|}{Stanford Dogs} & \multicolumn{3}{c|}{Stanford Cars} \\ \cline{2-13}  & 50\% & 20\% & 10\% & 50\% & 20\% & 10\% & 50\% & 20\% & 10\% & 50\% & 20\% & 10\% & 50\% & 20\% & 10\% \\ \hline baseline & 87.1 & 83.9 & 79.1 & 80.7 & 75.0 & 70.2 & 98.5 & 96.5 & 93.1 & 90.6 & 88.7 & 86.9 & 78.7 & 54.9 & 30.1 \\ \cline{2-13}
**+PACE** & **88.4** & **85.5** & 81.4 & 82.9 & **77.5** & **73.8** & **99.2** & **97.9** & **96.1** & 91.8 & **90.9** & **89.8** & **80.5** & **57.3** & **33.2** \\ \hline \end{tabular}
\end{table}
Table 13: Classification results on FGVC using varying percentages of data based on ViT-B/16.

\begin{table}
\begin{tabular}{|l|c c c|c c c|c c c|c c|} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{Camelyon} & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c|}{Average} \\ \cline{2-13}  & Mem. & Time & Acc. & Mem. & Time & Acc. & Mem. & Time & MeanAcc. & Mem. & Time & Acc. \\ \hline LoRA\({}_{\text{mid}}\)+VPT\({}_{\text{add}}\) & 8.9GB & 29m & 74.6 & 15.7GB & 33m & 86.7 & 8.9GB & 161m & 44.8 & 11.1GB & 74m & 68.7 \\ \hline +PACE\({}_{\text{fast}}\) (\(\frac{1}{2}\) batch size, \(\frac{1}{2}\) epochs) & 5.4GB & 17m & 78.1 & 8.6GB & 21m & 88.9 & 5.4GB & 85m & 45.8 & 6.5GB & 41m & 70.9 \\ +PACE\({}_{\text{fast}}\) (\(\frac{1}{2}\) batch size, \(\frac{1}{4}\) epochs) & 3.5GB & 10m & 77.8 & 6.0GB & 14m & 88.7 & 3.5GB & 50m & 45.6 & 4.3GB & 25m & 70.7 \\ +PACE\({}_{\text{fast}}\) (\(\frac{1}{8}\) batch size, \(\frac{1}{8}\) epochs) & 2.9GB & 6m & 77.2 & 5.2GB & 10m & 88.6 & 2.9GB & 32m & 45.5 & 3.7GB & 16m & 70.4 \\ \hline \end{tabular}
\end{table}
Table 11: Results of PACE\({}_{\text{fast}}\) with a reduced batch size and epochs on CIFAR-100 (VTAB-1K w/ ViT-B/16), Camelyon (VTAB-1K w/ Swin-B), ImageNet (Domain adaptaion w/ ViT-B/16). PACE\({}_{\text{fast}}\) outperforms baseline while using less GPU memory and training time.

\begin{table}
\begin{tabular}{|l|c|c c c c|c c|} \hline \#Epoch & Method & Natural & Specialized & Structured & Avg. \\ \hline
530 & GLoRA & 83.61 & 87.02 & 63.27 & 77.97 \\ \hline
100 & Baseline & 81.94 & 85.40 & 61.40 & 76.24 \\
100 & +PACE & 83.94 & 87.44 & 64.62 & 78.67 \\
50 & +PACE (half batch size) & 83.77 & 87.32 & 63.92 & 78.34 \\ \hline
200 & Baseline & 82.28 & 85.30 & 61.64 & 76.40 \\
200 & +PACE & 84.13 & 87.57 & 64.85 & 78.85 \\ \hline
300 & Baseline & 82.41 & 85.00 & 61.80 & 76.40 \\
300 & +PACE & 84.32 & 87.55 & 65.13 & 79.00 \\ \hline \end{tabular}
\end{table}
Table 12: Classification results for different methods on VTAB-1K with different training epochs.

[MISSING_PAGE_FAIL:22]

## Appendix G Experiment details for GSM-8K

We conduct experiments on text generation tasks by fine-tuning Phi-3-mini-4k-instruct [1] on the GSM-8K [9] dataset using causal language modeling. We use learning rate of 2e-6, batch size of 4, LoRA rank of 16, prompt "Answer below question. First think step-by-step and then answer the final number:n\n<Question>" as instruction and fine-tune models on the training set and evaluated the performance on the test set.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We theoretically and empirically verify the claims and contributions made in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of our work are discussed in SSA Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Complete proofs for each theorem are provided in SSB. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Training details and hyperparameter selection are presented in Sec. 4 and S7, respectively. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We will release our code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental settings and details are presented in Sec. 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All reported results are averaged over three random seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments were conducted on a single NVIDIA H100 GPU with 96 GB memory, with each experiment completing within 8 hours. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes], Justification: We have carefully reviewed and adhered to the code of ethics throughout our research and writing process. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Potential impacts are discussed in SSA. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: Our work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All publicly available assets (models, code, and data) used in this work have been properly credited, and their respective licenses and terms of use have been explicitly mentioned and adhered to. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets in the submission. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.