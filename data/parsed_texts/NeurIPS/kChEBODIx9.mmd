# Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?

Jialu Gao\({}^{1}\)1, Kaizhe Hu\({}^{1,2,3}\)1, Guowei Xu\({}^{1}\), Huazhe Xu\({}^{1,2,3}\)

\({}^{1}\) Tsinghua University \({}^{2}\) Shanghai Qi Zhi Institute \({}^{3}\) Shanghai AI Lab

gaojialululu@gmail.com, huazhe_xu@mail.tsinghua.edu.cn

equal contribution

Footnote 1: footnotemark:

###### Abstract

Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as "wiping" a stain off a table. Subsequently, LfVoid trains an ensembled goal discriminator on the generated image to provide reward signals for a reinforcement learning agent, guiding it to achieve the goal. The ability of LfVoid to learn with zero in-domain training on expert demonstrations or true goal observations (the void) is attributed to the utilization of knowledge from web-scale generative models. We evaluate LfVoid across three simulated tasks and validate its feasibility in the corresponding real-world scenarios. In addition, we offer insights into the key considerations for the effective integration of visual generative models into robot learning workflows. We posit that our work represents an initial step towards the broader application of pre-trained visual generative models in the robotics field. Our project page: LfVoid.github.io.

## 1 Introduction

What is the simplest way to provide guidance or goals to a robot? This question is answered multiple times: a set of expert demonstrations [1; 2; 3; 4], goal images [5; 6; 7], or natural language instructions [8; 9]. However, these answers either require a laborious and sometimes prohibitive effort to collect data, or contain ambiguity across modalities. The desire to alleviate the challenges begs the question: can we generate goal images from natural language instructions directly, without physically achieving the goals for robots?

Large text-to-image generative models [10; 11; 12; 13] have achieved exciting breakthroughs, demonstrating an unprecedented ability to generate plausible images that are semantically aligned with given text prompts. Trained on extensive datasets, these models are thought to possess a basic understanding of the world [14]. Therefore, we aim to harness the power of these large generative models to provide unambiguous visual goals for robotic tasks without any in-domain training.

In the past, a common approach to leverage off-the-shelf large generative models in robot learning involves using these models for data augmentation on expert datasets to improve policy generalization [15; 16; 17]. Despite the success, these methods still rely on human demonstrations, not generative models, as the main source of guidance.

DALL-E-Bot [18], an early attempt to utilize web-scale generative models in a zero-shot manner for guiding manipulation tasks, utilizes DALL-E 2 [10] to generate goal images for object rearrangement tasks. However, the generated images are often too diverse and largely disagree with real-world scenarios, requiring segmentation masks for object matching and a rule-based transformation planner to close the visual discrepancy.

The recent uprise of language-based image editing methods [19; 20; 21] provides a new paradigm: editing the generated images to align with language descriptions, such as "wiping a stain off a table", while keeping the visual appearance of the other objects and the background roughly unchanged. However, the edited images are usually examined by visual appearance rather than embodied tasks.

In this work, we introduce Learning from the Void (LfVoid), a method that uses image editing techniques on pre-trained diffusion models to generate visual goals for Reinforcement Learning (RL). LfVoid contains a unique image editing pipeline capable of performing appearance-based and structure-based editing to generate goal images according to language instructions. With these goal images, LfVoid improves upon example-based RL methods [5; 6] to solve several robot control tasks without the need for any reward function or demonstrations.

We evaluate LfVoid on three simulated environment tasks and the corresponding real-world scenarios. Empirical results show that LfVoid can generate goal images with higher fidelity to both the editing instructions and the source images when compared to existing image editing techniques, leading to better downstream control performance in comparison to language-guided and other image-editing-based methods. We also validate the feasibility of LfVoid in real-world settings: LfVoid can retrieve meaningful reward signals from generated images comparable to those from true goal images. Based on these observations, we discuss key considerations for the current generative model research when aiming for real-world robotic applications.

Our contributions are threefold: First, we propose an effective approach to leverage the knowledge encapsulated in large pre-trained generative models and apply it in a zero-shot manner to guide robot learning tasks. Second, we provide empirical evidence that suggests LfVoid-edited images provide guidance more effectively than other methods including the direct usage of text prompts. Lastly, we identify the existing gap between the capabilities of current text-conditioned image generation models and the demands in robotic applications, thus shedding light on a potential direction for future research in this domain.

## 2 Related work

Image editing with diffusion models.Recent work in text-to-image diffusion models has demonstrated a strong ability to generate images that are semantically aligned with given text prompts [22; 23; 10; 11; 12; 24; 25]. Based on these generative models, Prompt-to-Prompt proposes to perform text-conditioned editing through the injection of the cross-attention maps. Imagic [20] proposes to perform editing by using interpolation between source and target embeddings to generate images with edited effects. InstructPix2Pix [26] demonstrates another approach for training a diffusion model on the paired source and target images and the corresponding editing instructions. Apart from editing techniques, several methods focusing on controlled editing have been proposed. Textual-Inversion [27] and DreamBooth [28] both aim to learn a special token corresponding to a user-specified object and at inference time can generate images containing that object with a diverse background. Directed Diffusion [21] focuses on object placement generation, which can control the location of a specified object to reside in a given area.

Visual RL with implicit rewards.Standard reinforcement learning methods require an explicit reward function to guide the agent towards desired goal states, yet these reward functions may not always be available or may not capture the essence of the task at hand. One possible way of learning without access to explicit reward functions is using self-supervised learning to learn a representation where the distances can be used as reward signals [29; 30]. Another line of work uses example-based RL methods that aim to utilize the goal states to guide the learning process. VICE, proposed by Fu et al. [5], establishes a general framework for learning from goal observations: it trains a discriminator with the observations in the replay buffer as negative samples and goal imagesas positive samples. The positive logits for new observations are used as a reward to guide the agent toward the goal observation. Building upon this, Singh et al. [6] introduce VICE-RAQ, integrating the VICE algorithm with active querying methods through periodically asking a human to provide labels for ambiguous observations. In addition, it proposes a label-smoothing technique for a better reward shaping. Other works [31; 32] also explore ways to improve the reward shaping of VICE as well. In a parallel direction, Eysenbach et al. [7] develop Recursive Classification of Examples (RCE), a method that learns the Q function of actor-critic methods directly from the goal observations.

Large generative models for robot control.Current application of large generative models in robotic tasks mainly focuses on using Large Language Models for planning [8; 9; 33; 34; 35] or learning a language conditioned policy [36; 37], while the application of pre-trained image generative models for robotics tasks are limited. A common way of leveraging large-scale diffusion models for robot learning is by using diffusion models to perform data augmentation on training data, as suggested in CACTI [15], GenAug [16] and ROSIE [17]. Another line of work suggests using diffusion models to generate plans to solve robotics tasks [38; 39; 40]. More recently, video generation and editing techniques have also been used with imitation learning to guide robot policies [41; 42]. More specifically, UniPi[42] trains a text-to-video generation model on web-scale datasets and expert demonstrations to generate image sequences for planning and inverse modeling. The most related work is DALL-E-Bot [18], which uses the DALL-E [21] model to generate goal plans and performs object rearrangement according to these goals. However, unlike our work, DALL-E-Bot does not apply image editing techniques for goal generation and requires rule-based matching and predefined pick-and-place actions to bridge the visual gap between generated images and true observations.

## 3 Method

In this study, we aim to provide zero-shot visual goals to reinforcement learning agents for manipulation tasks using only text prompts. Our approach utilizes the information grounded in large-scale pre-trained visual generative models to create semantically meaningful visual goals. We first generate a synthetic goal image dataset from raw observations using image editing techniques, then employ example-based visual reinforcement learning that is optimized for our task with the generated dataset.

The structure of this section unfolds as follows: Section 3.1 elucidates the image editing techniques and adjustments applied to create goal images from text prompts and initial image observations, as depicted in Figure 1(a). Section 3.2 discusses the execution of example-based visual reinforcement learning using the generated goal images, as outlined in Figure 1(b).

### Visual goal generation

In the first phase of our methodology, we edit and generate goal images from raw observations based on natural language guidance provided by humans. Given a source prompt \(\mathcal{P}\), a source image \(x_{src}\), and an editing instruction, LfVoid synthesizes a target image \(x_{tgt}\) using a pre-trained Latent Diffusion Model (LDM) [12]. We consider two types of editing instructions: either a target prompt \(\mathcal{P}^{*}\) describing the appearance changes in the target image, or a bounding-box region \(\mathcal{B}\) and a set of tokens \(\mathcal{I}\) corresponding to the object to be relocated when structural changes are needed.

To provide sufficient visual guidance for downstream reinforcement learning tasks, the generated goal images should highlight the visual changes while preserving the irrelevant scene as much as possible, which is a challenging requirement even for state-of-the-art image editing techniques. To this end, we integrate a number of different techniques in the visual goal generation pipeline of LfVoid, consisting of a feature extracting module, an inversion module, and an editing module, as outlined in Figure 1(a). Following conventional notations, we denote \(x_{t}\) as the synthesized LDM image at time step \(t\in\{T,T-1,...,0\}\), where \(T\) is the total diffusion time steps. Specifically, \(x_{T}\) denotes the Gaussian noise, and \(x_{0}\) denotes the generated image. The techniques used in each module and their purpose are described as follows.

#### 3.1.1 Feature extracting module

To ensure high fidelity to the source image \(x_{src}\), we learn a unique token \(sks\) that encapsulates the visual features of objects within \(x_{src}\) as in DreamBooth [28]. This process involves optimizing the diffusion model parameters along with the special token \(sks\), using a set of images that contain the target object. As a result, we are able to derive a specialized model that can accurately retain key details of \(x_{src}\), such as the color and texture of a cube to be positioned, or the shape of a Franka robot arm for manipulation. As later demonstrated in our experiments, this module substantially boosts the resemblance in details of our edited images to the corresponding source images.

#### 3.1.2 Inversion module

After employing the special token \(sks\) to capture the essential features of the source scene, we invert the provided source image \(x_{src}\) to a diffusion process. A simple inversion technique using the Denoising Diffusion Implicit Model (DDIM) [23] sampling scheme calculates the samples \(x_{T},x_{T-1},...,x_{0}\) by reversing the ODE process. However, the inverted image \(x_{0}\) obtained this way often deviates from \(x_{src}\) due to cumulative errors. Null-text inversion [43] aims to mitigate this discrepancy by fine-tuning the "null-text" embedding used in the classifier-free guidance [44]\(\Phi_{t}\) for each \(t\in\{T,T-1,...,1\}\) to control the generation process. Thus, with the initial noise sample \(x_{T}\) obtained through DDIM inversion and the optimized "null-text" embedding \(\Phi_{T},\Phi_{T-1},...,\Phi_{1}\), the diffusion process is able to generate the image \(x^{\prime}_{src}\), an approximation of the source image \(x_{src}\). Based on the diffusion process obtained with the inversion module, LfVoid can perform precise and detailed editing control on the source image, which will be discussed in Section 3.1.3.

#### 3.1.3 Editing module

Depending on the specific requirements, we divide the editing tasks into two distinct categories: appearance-based and structure-based image editing. Appearance-based editing involves maintaining the structural layout of the image while altering certain visual aspects, such as cleaning the surface of a table or lighting up an LED bulb. In contrast, structure-based editing involves changes in the layout of the image, such as relocating objects within the image from one area to another.

Appearance-based editing.In tasks involving appearance-based editing, we employ the Prompt-to-Prompt editing control technique. When the Latent Diffusion Model (LDM) generates an image \(x_{0}\) conditioned on a text prompt \(\mathcal{P}\), the information encapsulated in \(\mathcal{P}\) influences the diffusion process via the cross-attention layers [12; 11; 19]. Prompt-to-Prompt reveals that the spatial configuration

Figure 1: **An overview of LfVoid.** The goal of LfVoid is to learn robot policies requiring only language descriptions from humans. LfVoid consists of two parts: (a) Goal image generation, where we apply image editing on the initial observations according to different editing instructions to obtain a visual goal dataset; (b) Example-Based Visual RL, where we perform reinforcement learning on the generated dataset to achieve the desired goal image in various environments.

of the image \(x_{0}\) largely depends on the cross-attention maps \(M_{t}\) within these cross-attention layers, especially during the initial diffusion steps. Consequently, it suggests replacing the attention maps \(M_{t}^{*}\) of the target image diffusion process with the attention maps \(M_{t}\) from the source image diffusion process for the first \(N\) time steps, beginning at time step \(t=T\):

\[\mathrm{P2PEdit}(M,M^{*},t)=\left\{\begin{array}{ll}M_{t}&\mathrm{if}\ t>T-N\\ M_{t}^{*}&\mathrm{otherwise}\end{array}\right.\] (1)

This method ensures that the structure of the source image encapsulated in the attention maps \(M_{t}\) is conserved in the target image in a more controlled manner, which is crucial in downstream RL tasks.

Structure-based editing.A notable limitation of Prompt-to-Prompt editing is the incapacity to spatially move existing objects across the image, i.e., implement structural changes. Therefore, we introduce a novel approach termed P2P-DD for structure-based editing tasks. P2P-DD combines Prompt-to-Prompt control with the idea of Directed Diffusion [21], extending its capability to perform object replacement.

Directed Diffusion [21] illustrates the possibility of achieving object replacement through direct editing of attention maps during the first \(N\) time steps of the generation process. The method performs attention strengthening on the cross-attention maps corresponding to the tokens in \(\mathcal{I}\) (recall that \(\mathcal{I}\) is the token sets representing the object of interest), through calculating a Gaussian strengthening mask (SM) of the bounding-box region \(\mathcal{B}\). It also performs attention annealing to the remaining area \(\mathcal{\bar{B}}\) by applying a constant weakening mask (WM), and the two masks are weighted by a scalar \(c\):

\[\mathrm{DDEdit}(M_{t},\mathcal{B},\mathcal{I})=\left\{\begin{array}{ll}M_{t} \odot\text{WM}(\mathcal{\bar{B}},\mathcal{I})+c\cdot\text{SM}(\mathcal{B}, \mathcal{I})&\mathrm{if}\ t>T-N\\ M_{t}&\mathrm{otherwise}\end{array}\right.\] (2)

Our proposed P2P-DD method aims to encourage the background and other details of the image generated by Directed Diffusion with a higher resemblance to the source image. P2P-DD first injects cross-attention maps \(M_{t}\) into \(M_{t}^{*}\) as suggested by Prompt-to-Prompt. This will preserve the structure and background information from the source image, providing a decent starting point for attention map editing. Next, P2P-DD performs pixel value strengthening and weakening on the attention maps \(M_{t}\) according to the bounding-box \(\mathcal{B}\) and the tokens in \(\mathcal{I}\), as suggested in the Directed Diffusion, to achieve the desired object placement:

\[\mathrm{P2P\text{-}DDEdit}(M_{t},M_{t}^{*},\mathcal{B},\mathcal{I})=\mathrm{ DDEdit}(\mathrm{P2PEdit}(M,M^{*},t),\mathcal{B},\mathcal{I})\] (3)

This combined editing control is only applied in the first \(N\) diffusion time steps, i.e. \(\{T,...,T-N+1\}\). After the first \(N\) time steps, we cease any control on attention maps and allow the target diffusion

Figure 2: **Image editing with structural changes.** The details of P2P-DD technique used in the editing module of Lfvoid to perform structural changes. Lfvoid performs a combination of P2P attention map injection with Directed Diffusion attention editing on the diffusion process generating the target image \(x_{tgt}\), based on the Gaussian noise \(x_{T}\) and the optimized null-text embeddings \(\Phi_{T},\Phi_{T-1},...,\Phi_{1}\) obtained through the Inversion Module of Lfvoid.

process to complete in a conventional denoising manner. The details of P2P-DD are shown in Figure 2. Attention visualization of the generation process can be found in Appendix A.1, and we provide a detailed description of the full algorithm in Appendix A.2.

### Example-based visual reinforcement learning

We modify and extend the approach of VICE [5] to devise our method for example-based visual reinforcement learning. At the core of our process, we employ the image editing methods described in Section 3.1 on observations gathered from randomly initialized environments, producing a dataset of 1024 target images for each task. During training, these target images serve as positive samples, while the images sampled from the agent's replay buffer after a number of random exploration steps are treated as negative samples. We train a discriminator using these instances with the binary cross-entropy loss and use the output of the discriminator for new observations for the positive class as the agent reward.

The discriminator shares the CNN encoder with the reinforcement learning agent and performs classification over the output latent representations. To improve the reward shaping, we utilize the label mixup method [6], which performs a random linear interpolation of the 0-1 labels and their corresponding hidden vectors to obtain continuous labels between 0 and 1. We discover that restricting the negative instances from the recent portion of the replay buffer (the last 5%) promotes the discriminator's ability to discern subtle differences between the target and current observations. Additionally, we find that our method enjoys an ensemble of discriminators for classification results (i.e., RL rewards), which gives the agent more representative rewards.

For the reinforcement learning backbone algorithm, we use DrQ-v2 [45] for visual RL training. Based on Twin Delayed DDPG (TD3) [46], DrQ-v2 enhances image representation by applying random crop augmentation to the input images. Collectively, these refinements constitute our approach to example-based visual reinforcement learning, making the "learning" from an initial observation and language prompts possible.

## 4 Experiments

In this section, we present a comprehensive evaluation of LfVoid across both simulated environments and real-world robotic tasks. An illustration of each task can be found in Figure 3.

The environments we use are: 1) LED, where the robot reaches for a switch or touches the LED light directly to turn the light from red to green; 2) Wipe, where the robot needs to wipe out stains from the table; 3) Push, where the robot needs to push a red cube to a goal position indicated by a green dot. The simulated tasks are developed based on the Robosuite benchmark, while we provide corresponding real-world tasks for each environment. A full description of the environments is provided in Appendix B.1.

### Goal generation

In this section, we evaluate the ability of LfVoid and other baseline methods to generate goal images according to two types of editing instructions: appearance-based editing and structure-based editing.

Figure 3: **Visualization of robot manipulation tasks.** We evaluate LfVoid on three simulation tasks: Wipe, Push, and LED, as well as three real-robot environments correspondingly.

Baselines and ablations.We select three recent image editing methods as baselines: Imagic [20], InstructPix2Pix [26], and mask-based DALL E-2 editing used in DALL-E-Bot [18](with masks provided by humans). For ablations, DDIM P2P removes both the feature extracting module and the inversion module, and DDIM DD additionally removes P2P in the editing module for structure-based editing. On top of DDIM DD, Null-text DD adds the inversion module back. For both appearance-based and structure-based editing tasks, DreamBooth P2P/P2P-DD ablates the inversion module, and Null-text P2P/P2P-DD ablates the feature extracting module.

Appearance-based goal image editing results.In Figure 4, we present the visual goal generation results of the Wipe and LED tasks, focusing on editing object appearances in both simulated and real-world environments. Imagic struggles to preserve the structure and details of the source image, and InstructPix2Pix frequently fails to perform local color editing, unable to locate the specific area indicated by the editing instruction. DALL E-2 is able to preserve the scenes outside the masks but is not capable of performing the desired edits within the masked area. For ablations, DDIM P2P and DreamBooth P2P-DD can mostly carry out the desired edits but often diverge largely from the source image. This demonstrates the importance of the inversion module in preserving the details and structures of the source image. Null-text P2P performs worse in the Wipe tasks compared to LfVoid, demonstrating the gains provided by the DreamBooth token. Overall, LfVoid demonstrates superior performance in terms of maintaining the appearance of unrelated parts while effectively editing specified regions: through directly applying control and editing on the attention maps, LfVoid is capable of locating the target area and performing the desired edit based on language instructions.

Structure-based goal image editing results.In Figure 5, we report the performance of LfVoid in editing tasks with structural changes (the Push task) in both simulated and real-world settings. For baselines, Imagic distorts the robot arm and table significantly and often introduces multiple objects into the image, rather than moving the existing one. InstructPix2Pix only changes the overall color of the image or replaces the gripper with a geometric body, failing to achieve the required object displacement. DALL E-2 also fails to relocate the object to the desired place. For ablative results, while DDIM DD and Null-text DD can successfully move objects to the lower-left corner, the background and the robot arm nearly disappear. Null-text P2P-DD improves the preservation of the source image's background, but the shape of the moved object is inconsistent with its original form. DreamBooth P2P-DD can preserve the shape of the moved object and relocation is successful, but

Figure 4: **Appearance-based goal image editing results.** We compare LfVoid against three recent image editing methods: Imagic, InstructPix2Pix, and DALL E-2, as well as three ablations of LfVoid. Note that InstructPix2Pix has different prompts because it requires an editing instruction rather than a description of the image. In both simulation and real-world settings, LfVoid can generate images that are better aligned with given text prompts while preserving the remaining source scene. See Appendix C for more examples.

the background is not consistent with the source image. Our method, when compared to all baselines, can perform the object displacement task successfully while better preserving the visual features of the object and the remaining scene. These experiments reveal the importance of each component in our method. The creative integration of different image editing techniques provides a powerful tool for image editing with structural changes, demonstrating a significant improvement over existing methods.

Quantitative results.In Table 1, we present the quantitative results of goal image generation. We manually sample 500 real goal images per task(only for evaluation purposes) and calculate the pair-wise LIPIS distance between the generated images and the real goal images. The lower distance represents higher image editing quality, and results show that LfVoid outperforms all the baselines and ablations. Additionally, we present an ablation study of the feature extracting module on the Wipe and Push task in Table 2, showing the performance gain brought by the DreamBooth token.

### Example-based visual reinforcement learning

We now dive into the results obtained from applying example-based visual reinforcement learning using the goal images generated by LfVoid. We select three baselines for comparison: CLIP, IP2P,

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Wipe(Sim) & LED(Sim) & Push(Sim) & Wipe(Real) & LED(Real) & Push(Real) \\ \hline \begin{tabular}{l} InstructPix2Pix \\ Imagic \\ \end{tabular} & 0.23\(\pm\)0.11 & 0.41\(\pm\)0.06 & 0.34\(\pm\)0.17 & 0.18\(\pm\)0.04 & 0.45\(\pm\)0.06 & 0.53\(\pm\)0.08 \\ \begin{tabular}{l} DAML-E 2 \\ \end{tabular} & 0.23\(\pm\)0.05 & 0.42\(\pm\)0.08 & 0.43\(\pm\)0.11 & 0.34\(\pm\)0.13 & 0.35\(\pm\)0.16 & 0.49\(\pm\)0.11 \\ \begin{tabular}{l} DreamBooth+Editing \\ \end{tabular} & 0.10\(\pm\)0.04 & 0.25\(\pm\)0.07 & 0.20\(\pm\)0.06 & 0.39\(\pm\)0.08 & 0.20\(\pm\)0.07 & 0.38\(\pm\)0.03 \\ \begin{tabular}{l} DDIM+DD/P2P \\ Null-text+DD \\ \end{tabular} & 0.53\(\pm\)0.05 & 0.62\(\pm\)0.05 & 0.42\(\pm\)0.15 & 0.58\(\pm\)0.06 & 0.43\(\pm\)0.08 & 0.36\(\pm\)0.06 \\ \begin{tabular}{l} DDIM+DD/P2P \\ Null-text+DD \\ \end{tabular} & 0.26\(\pm\)0.07 & 0.37\(\pm\)0.06 & 0.43\(\pm\)0.04 & 0.44\(\pm\)0.04 & 0.40\(\pm\)0.03 & 0.50\(\pm\)0.03 \\ \begin{tabular}{l} Null-text+DD \\ \end{tabular} & N/A & N/A & 0.36\(\pm\)0.05 & N/A & N/A & 0.51\(\pm\)0.06 \\ 
\begin{tabular}{l} LfVoid \\ \end{tabular} & **0.08\(\pm\)0.04** & **0.17\(\pm\)0.06** & **0.14\(\pm\)0.07** & **0.12\(\pm\)0.05** & **0.19\(\pm\)0.05** & **0.32\(\pm\)0.03** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Quantitative results on goal image generation.** We calculate the pair-wise LIPIS distance between generated images and the manually created real goal images for evaluation purposes.

Figure 5: **Structure-based goal image editing results.** We compare LfVoid against Imagic, InstructPix2Pix, and DALL E-2, as well as four ablations of LfVoid. Note that InstructPix2Pix has different prompts because it requires an editing instruction rather than a description of the image. LfVoid can successfully perform object displacement and preserve the background and details of the source image, while other methods fail to do so. See Appendix C for more examples.

and Real Goal. CLIP relies only on language guidance and does not require any synthesized goal images. It leverages the Contrastive Language-Image Pre-training score [47] of the task prompt and observations as a reward, and it aims to verify the effectiveness of translating prompts into images against raw language hints. IP2P uses InstructPix2Pix for generating the goal images and then runs the same example-based RL as in LfVoid. It allows us to compare the performance of LfVoid with a straightforward image editing technique. Real Goal is an upper-bound performance of our pipeline, where we manually create ideal goal images, such as erasing the stains directly in the simulator for the Wipe task. This strategy illustrates the potential of our algorithm, indicating the gap between the generated goals and the real ones, and demonstrates how the quality of generated images will influence the performance of downstream RL.

Simulation results.We report both episode reward (Figure 6) and numerical metrics (Table 3) on three simulation tasks. The training details can be found in Appendix B.3. We observe that the CLIP baseline achieves low rewards in all the tasks, showing that directly using language guidance through CLIP embeddings is not sufficient. Moreover, LfVoid consistently outperforms the IP2P baseline and gains comparable performance with the Real Goal upper bound, providing evidence that generated goal images with high fidelity to both the original scene and the editing instructions is crucial for successful deployment of downstream example-based RL.

Real-world results.In the more complex real-world settings, we aim to validate the feasibility of LfVoid through visualization of reward functions. As in our previous experiment, we generate goal images using InstructPix2Pix (IP2P) and LfVoid, and also manually collect a real goal dataset as an upper-bound (Real Goal). We then train discriminators on these datasets separately and use the outputs of the discriminators as rewards. In addition, we compare with language-based guidance (CLIP), which uses the distance of the CLIP embeddings between the task prompt and the observation images as rewards. We manually record five successful trajectories for each task and use these reward functions to obtain the reward curves. As illustrated in Figure 7, LfVoid can provide near monotonic dense reward signals comparable to those from Real Goal. On the contrary, the curves obtained by IP2P and CLIP fail to accurately track the progress of each task. This demonstrates that our method provides more instructive learning signals for real-world robotic tasks than either IP2P or CLIP.

Comparison to RL with imagined goals.In a parallel direction, RL with imagined goals can also solve robot manipulation tasks without the need for human-specified reward functions and expert demonstrations [29, 30]. These methods first leverage self-supervised learning to learn a representation

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & CLIP & InstructPix2Pix & Real Goal (Oracle) & Ours \\ \hline Wipe (Cleaned Stains / Patch ) & 0.0\(\pm\)0.0 & 1.7\(\pm\)1.9 & 22.0\(\pm\)5.8 & **21.3\(\pm\)5.8** \\ LED (Success Rate / \%) & 10.0\(\pm\)20.0 & 0.0\(\pm\)0.0 & 93.3\(\pm\)11.5 & **75.0\(\pm\)50.0** \\ Push (Success Rate / \%) & 12.5\(\pm\)19.1 & 3.3\(\pm\)3.5 & 30.0\(\pm\)13.7 & **27.9\(\pm\)12.7** \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Numerical metrics of simulation tasks.** We report the success rate for LED and Push, and the number of stain patches cleaned for Wipe. Please refer to Appendix B.1 for details.

Figure 6: **Episode reward of simulation tasks.** The results of the CLIP baseline (CLIP), InstructPix2Pix baseline (IP2P), using real goal image as upper bound (Real Goal), and LfVoid (Ours).

space of observations, and then sample imagined goals to train a goal-conditioned policy. However, unlike LVoid, a real goal image is still required at test time to run the goal-conditioned policy. In Table 4, we compare the performance of LfVoid with two algorithms: Visual RL with Imagined Goals (VIG) [29] and Goal-Aware Prediction (GAP) [30], on the three simulation tasks. We observe that only LfVoid can solve the tasks with positive rewards, the other two methods fail in all tasks. Because both these methods use random policy for exploration, they may never reach the user-specified goal states during training. Therefore, they lack the knowledge required to fulfill the goals. This further demonstrates the significance of leveraging web-scale text-to-image models to provide informative visual guidance during training. Please refer to Appendix C.2 for more analysis.

## 5 Discussion

In this work, we present LfVoid, an effective approach for leveraging the knowledge of large-scale text-to-image models and enabling zero-shot reinforcement learning from text prompts and raw observation images. We have identified and tackled numerous challenges that arise when applying state-of-the-art image editing technologies to example-based RL methods. Our work highlights the potential for the adaptation and application of image-generation techniques in the realm of robotics. Our findings not only enhance the understanding of image editing for robotic applications but also provide a clear direction for the image generation community to address real-world challenges.

Although our proposed method has shown promising results, we acknowledge that it is not without limitations. While LfVoid has succeeded in generating goal images to train example-based RL policies, a certain level of prompt tuning is needed in order to achieve optimal editing performance (see Appendix B.2), and we refer the readers to Appendix D for an analysis of failure cases.

We would also like to highlight that there exists a gap between the ability of text-to-image generation models and the need for robot learning. For example, large diffusion models exhibit poor understandings of the spatial relationships between objects [48]; therefore, both the generative models and the editing methods struggle to handle object displacement solely through language prompts. The Directed Diffusion technique, while being able to perform movements of objects, requires a user-defined bounding box to achieve precise control. Furthermore, large generative models sometimes struggle to generate images that are considered valid under physics laws. When asked to pick up a bottle with a robot arm, the model simply stretches the shape of the arm to reach the bottle rather than changing the joint position. Lastly, AI-generated images inevitably introduce alterations to the details of objects. The robustness of current visual reinforcement learning algorithms to such changes remains an open question.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Push(Sim) & Wipe(Sim) & LED(Sim) \\ \hline Visual rl with Imagined Goals (VIG) & -0.39\(\pm\)1.79 & -6.03\(\pm\)2.02 & -1.77\(\pm\)0.55 \\ Goal-Aware Prediction (GAP) & -0.07\(\pm\)0.91 & -5.84\(\pm\)1.00 & -0.39\(\pm\)0.11 \\ Learning from the Void (LfVoid) & **0.35\(\pm\)0.51** & **12.86\(\pm\)4.31** & **4.47\(\pm\)2.10** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Comparison with RL using imagined goals.** We report the episode reward during test time and compare LfVoid with existing methods of visual RL using imagined goals.

Figure 7: **Reward curve on successful trajectories for real-robot tasks.** Visualization of the reward function of LfVoid (Ours) compared with baselines. See Appendix B.4 for implementation details.

## Acknowledgment

HX is supported by National Key R&D Program of China (2022ZD0161700).

## References

* [1] Georgios Papagiannis and Yunpeng Li. Imitation learning with sinkhorn distances. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19-23, 2022, Proceedings, Part IV_, pages 116-131. Springer, 2023.
* [2] Robert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation learning. _arXiv preprint arXiv:2006.04678_, 2020.
* [3] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning. _arXiv preprint arXiv:1809.02925_, 2018.
* [4] Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Supercharging imitation with regularized optimal transport. In _Conference on Robot Learning_, pages 32-43. PMLR, 2023.
* [5] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. _Advances in neural information processing systems_, 31, 2018.
* [6] Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. End-to-end robotic reinforcement learning without reward engineering. _arXiv preprint arXiv:1904.07854_, 2019.
* [7] Ben Eysenbach, Sergey Levine, and Russ R Salakhutdinov. Replacing rewards with examples: Example-based policy search via recursive classification. _Advances in Neural Information Processing Systems_, 34:11541-11552, 2021.
* [8] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.
* [10] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [13] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _ArXiv_, abs/2206.10789, 2022.
* [14] Stephen Adams, Tyler Cody, and Peter A Beling. A survey of inverse reinforcement learning. _Artificial Intelligence Review_, 55(6):4307-4346, 2022.
* [15] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, Shuran Song, Aravind Rajeswaran, and Vikash Kumar. Cacti: A framework for scalable multi-task multi-scene visual imitation learning. _arXiv preprint arXiv:2212.05711_, 2022.

* [16] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. Genaug: Retargeting behaviors to unseen situations via generative augmentation. _arXiv preprint arXiv:2302.06671_, 2023.
* [17] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. _arXiv preprint arXiv:2302.11550_, 2023.
* [18] Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale diffusion models to robotics. _arXiv preprint arXiv:2210.02438_, 2022.
* [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. _arXiv preprint arXiv:2210.09276_, 2022.
* [21] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and Thomas Leung. Directed diffusion: Direct control of object placement through attention guidance. _arXiv preprint arXiv:2302.13153_, 2023.
* [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [23] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [24] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* [25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [26] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. _arXiv preprint arXiv:2211.09800_, 2022.
* [27] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell. Zero-shot reward specification via grounded natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sirvan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14743-14752. PMLR, 17-23 Jul 2022.
* [28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.
* [29] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. _CoRR_, abs/1807.04742, 2018.
* [30] Suraj Nair, Silvio Savarese, and Chelsea Finn. Goal-aware prediction: Learning to model what matters, 2020.
* [31] Kevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H Pong, Aurick Zhou, Justin Yu, and Sergey Levine. Mural: Meta-learning uncertainty-aware rewards for outcome-driven reinforcement learning. In _International conference on machine learning_, pages 6346-6356. PMLR, 2021.
* [32] Daesol Cho, Seungjae Lee, and H Jin Kim. Outcome-directed reinforcement learning by uncertainty & temporal distance-aware curriculum goal generation. _arXiv preprint arXiv:2301.11741_, 2023.
* [33] Maria Attarian, Advaya Gupta, Ziyi Zhou, Wei Yu, Igor Gilitschenski, and Animesh Garg. See, plan, predict: Language-guided cognitive planning with video prediction. _arXiv preprint arXiv:2210.03825_, 2022.

* Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.
* Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object rearrangement. _arXiv preprint arXiv:2303.06247_, 2023.
* Shridhar et al. [2022] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pages 894-906. PMLR, 2022.
* Jiang et al. [2022] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. _arXiv preprint arXiv:2210.03094_, 2022.
* Liu et al. [2023] Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, and Chris Paxton. Structodiffusion: Language-guided creation of physically-valid structures using unseen objects, 2023.
* Gkanatsios et al. [2023] Nikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu Zhang, Christopher Atkeson, and Katerina Fragkiadaki. Energy-based models as zero-shot planners for compositional scene rearrangement. _arXiv preprint arXiv:2304.14391_, 2023.
* Mishra and Chen [2023] Utkarsh A Mishra and Yongxin Chen. Reorientdiff: Diffusion model based reorientation for object manipulation. _arXiv preprint arXiv:2303.12700_, 2023.
* Bahl et al. [2022] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild, 2022.
* Dai et al. [2023] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. _arXiv preprint arXiv:2302.00111_, 2023.
* Mokady et al. [2022] Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. _arXiv preprint arXiv:2211.09794_, 2022.
* Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Yarats et al. [2021] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International Conference on Learning Representations_, 2021.
* Fujimoto et al. [2018] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. _ArXiv_, abs/1802.09477, 2018.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Conwell and Ullman [2022] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. _arXiv preprint arXiv:2208.00005_, 2022.
* Zhang et al. [2017] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* Yarats et al. [2021] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. _arXiv preprint arXiv:2107.09645_, 2021.
* Zhu et al. [2020] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martin-Martin, Abhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: A modular simulation framework and benchmark for robot learning. In _arXiv preprint arXiv:2009.12293_, 2020.

* [52] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018.
* [53] Mark E. Glickman. The glicko-2 system. http://www.glicko.net/glicko/glicko2.pdf, 2013.