# Lexically-constrained automated prompt augmentation: A case study using adversarial T2I data

Anonymous Authors

###### Abstract

Ensuring the safety of images generated by text-to-image (T2I) models is crucial, yet there are limited datasets of adversarial prompts and images available for evaluating model resilience against novel attacks. Existing literature focuses on using either purely human-driven or purely automated techniques to generate adversarial prompts for T2I models. Human-generated data often results in datasets that are small and at times unbalanced. On the other hand, while automated generation can easily scale, the prompts generated often lack diversity and fall short of incorporating human or realistic elements encountered in practice.

To address this gap, we combine the strength of both approaches by creating an augmented dataset that leverages two attack strategies identified from the human-written Adversarial Nibbler Dataset. This new dataset consists of realistic and semantically similar prompts, generated in a constrained yet scalable manner. It maintains about 72% of the failure rate of the human-generated data for inappropriate content, while preserving the realistic nature of the prompts and replicating their ability to cause real-world harms. Our work highlights the importance of human-machine collaboration to leverage human creativity in scalable red-teaming techniques to continuously enhance T2I model safety.

**Content warning:** This paper includes examples that contain offensive content (e.g., violence, sexually explicit content, and negative stereotypes).

## 1 Introduction

Text-to-image (T2I) models such as DALL-E [47; 48], Stable Diffusion [53], and Midjourney [36] have broad impact due to their adoption not only among technology-enthusiasts and creative professionals, but also among casual users with varied literacy about the limits of such tools. When models generate harmful images from innocuous user prompts (i.e., prompts that do not explicitly instruct a model to produce a harmful image), there is the risk of inflicting psychological distress on users, perpetuating and solidifying negative stereotypes, and decreasing trust in generative AI more broadly. To minimize risks of harm, generative model released from industry groups often include details on their red-teaming processes [39; 40]. In these reports, however, we often find limited discussion of efforts to understand the safety implications of T2I models _responding to safe prompts with unsafe generations._ In this paper, we refer to innocuous-looking prompts that result in unsafe generations as _implicitly adversarial prompts_. An example of such a prompt is illustrated in Figure 1.

What counts as a "safe" image generation is inherently subjective. A person's view of safety can be influenced by their values and experiences [24; 2], so we expect many cases in which there is no single perspective on what counts as safe or unsafe. Thus, it is crucial to gather safety data from people with diverse values and experiences in order to comprehensively report diverging views of safety. This calls for a descriptive approach to understanding safety rather than a prescriptive one determined by binary labels calculated from the aggregation of ratings [28; 54; 17].

Such a subjective and context-dependent problem requires a participatory and dynamic approach to robustly identify blind spots in harmful image generation. The utility of such approaches has been demonstrated for image classification models [4] and the discovery of unknown unknowns [1; 41] (i.e., examples where the model is confident about its answer, but is actually wrong). Quaye et al. recently released a public dataset for T2I model safety developed through an ongoing, dynamic effort to capture multiple snapshots of model blind spots. While this dataset is a valuable tool for exposing model vulnerabilities, it is not possible to determine from just a few examples how pervasive or severe the issues are. Thus, in this paper, we use Quaye et al.'s insights on creative human attack strategies to create a high-quality augmented red-teaming dataset with a 5x increase in size from the original. This new dataset allows us to establish the benefit of a lexically-constrained data augmentation technique drawn from the original human input. Our method addresses two primary limitations of red-teaming efforts - (i) the lack of representation and diversity of human thought in scalable red-teaming efforts [43; 55] and (ii) the unrealistic structure of synthetically-generated prompts that are not representative of prompts used in practice.

Our main contribution in this work is the creation of a data-centric automated method of scaling a seed dataset with a traceable and understandable number of steps. We choose a dataset that was richly annotated by humans in order to extend augmentation beyond synthetic generation without guidance on the strategy. Specifically, we leverage our context about the attack strategies to constrain the prompt generation process in a way that reflects an understanding of what causes these failures. Since we are able to track what exactly is being extended for the different prompt groups, we have a greater degree of control without requiring excessive manual filtering. After proving the validity of this technique, our next step is to compare our output with purely LLM-based methods of augmentation.

## 2 Related Work

**Safety and fairness evaluation of T2I models.** Proliferation of T2I models has led to extensive work on establishing benchmarks for their evaluation [33; 25]. These approaches often rely on existing datasets (and their modified versions) [33] as well as on automated metrics [21; 22]. Specifically, in the space of safety and fairness testing of T2I models, many existing benchmark datasets have focused largely on social biases and representational harms [6; 7; 37; 20; 34; 66], but have limited topic coverage and engagement with diverse communities, often staying scoped only to Western contexts [55]. Research has also looked at model capabilities in generating overly sexual or violent images [64; 56] with proposals for mitigation.

We chose to use Quaye et al.'s Nibbler dataset as our seed because it covers a _broad range of harms_ (including both representational issues and inappropriate content) observed from five different T2I models. Additionally, they focus on safety that is not already ensured by text-based safety filters by _only considering safe prompts that trigger unsafe image generations_ (implicitly adversarial prompts).

**The power of humans in the loop.** Model testing for safety and fairness is typically conducted by experts such as industry practitioners or academic researchers. However, such expert-led evaluations often lack coverage and diversity in failures identified, compared to the issues surfaced by the

Figure 1: An example of an implicitly adversarial prompt and the corresponding image that was generated by a T2I model, as seen in [44]

crowd [12, 23, 59]. More generally, small-group approaches may fail to find problematic model behaviors because the participants lack the cultural background to identify certain issues, or these issues may only appear when the model is used in a specific context that was not considered [12, 13].

The many benefits of crowd-based model evaluation have inspired several research thrusts. Notably, the Dynabench project [26] has spurred multiple crowd-oriented challenges, inviting everyone to attack models and help generate datasets that identify previously unknown vulnerabilities [60, 27, 35]. More recently, large-scale text-to-text (T2T) datasets have been developed by crowdsourcing prompts from more diverse participants [29] or by gamifying the collection process for adversarial prompts [57].

To _preserve the benefits of crowd-based data collection_, we use Quaye et al.'s Adversarial Nibbler dataset in our experiments.

**Automated data augmentation.** While red-teaming methodology where users report failures is valuable and necessary for surfacing harms [18], these efforts depend on extensive human labor [61, 38, 14]. Consequently, automated approaches have been proposed [15, 30, 42, 46, 50] to scale red-teaming. Automated redeaming often takes two main directions: (1) The use of jailbreaking techniques and (2) The training of large language models (LLMs) to design system-breaking prompts.

Jailbreaking strategies (usually via token- and sentence-level prompt modification) such as suffix optimization [67], persona modulation [58], and persuasive tactics [65] are often used to increase attack success rates. However, they are often limited by a small pre-defined set of harmful behaviors. Due to the long-tail nature of safety problems, it is challenging to cover the wide range of edge cases encountered in practical applications. The goal of this paper is to _amplify as many long-tail use cases that are more likely to be encountered in everyday interactions with T2I models as possible_.

[62, 10, 16, 63] have used white-box access to tweak model parameters in their red-teaming efforts. However, most of the publicly-available generative models do not offer white-box access. Thus, newer techniques are being used to automate red-teaming methods. For example, reinforcement learning has been used to provide dynamic adaptation to black-box settings where LLMs serve as the red team [3, 5]. Yet, even with explicit regularization to favor diversity and novelty, approaches that use reinforcement learning to finetune attacker language models suffer from mode collapse (model over exploits a successful attack, restricting diversity) or end up generating ineffective attacks [32]. Additionally, purely synthetic data generation strategies that leverage a wide span of taxonomies [31] often produce inorganic results that are not reflective of human interactions with these generative models.

When a set of seed prompts is expanded largely by an AI-based method, the augmentation procedure is often random [50] and not sensitive to human diversity. While this leads to a larger dataset, it does not provide high-quality outputs that are representative of those seen in practice. Additionally, much criticism of bias evaluation centers around the randomness of perturbations introduced by automated methods, wherein the external validity of the resulting prompts is not guaranteed [8]. These automated approaches often compromise diversity by focusing on maximizing attack success rate. Ideally, the red team should focus on capturing as many use cases as possible instead of being skewed towards the most egregious responses.

To overcome the limitations of automated dataset augmentation, we take a data-driven approach (using findings about human strategies) to augment the dataset in a targeted way. We believe that this informed method of expanding red-teaming datasets helps us to mitigate the resource-intensive nature of crowdsourced red-teaming. A purely computational approach often demands a trade off between attack quality and attack diversity. Our goal is to _find the sweet spot between these two by increasing the degree of human influence on the prompt design, while maintaining scalability and producing relevant results_.

Explicit attempts to trigger model failure such as poisoning memory or knowledge databases [9], encoded interactions such as prompt obfuscation, context overload with repeated tokens, and social hacking [51] are out of scope for this work.

Method

We select these two attack strategies from [44]'s list because they enable us to model implicit adversariality:

* **Typographical errors in the prompt -** The use of wrong spellings of words such as "_fatile_ accident" or "_inoscent_ blood" to circumvent safety filters. These errors are common in everyday usage and can realistically occur when users interact with generative models.
* **Semantic ambiguity of certain phrases -** The use of synonyms or visual descriptives (e.g., "a man standing next to a kneeling woman in a bedroom"), double entendres (words or phrases with multiple interpretations such as "cleavage"), or vague terms (e.g., "Friday prayers"). Models often find it difficult to clearly identify the intended use of words and phrases that have multiple definitions.

Due to our focus on automatic augmentation and repeatability of evaluations, all safety evaluations in this work use automated safety classifiers, though future work to examine success of these augmentation methods against human judgments would be useful.

### The Source Dataset

We use Quaye et al.'s publicly available dataset that was obtained from their Adversarial Nibbler Challenge. This dataset contains 1,241 unique prompts that successfully circumvented text safety filters to trigger generation of inappropriate images. Each prompt is labelled with the attack strategy that was used by the human participant, which enables us to easily categorize the prompts for our use case. We also leverage insights from their qualitative analysis of the attack strategies used by participants to inform our augmentation process. To ensure consistency in comparison, we utilize the same T2I model suite as Quaye et al.: Dall-E-2 and four variations of Stable Diffusion (SD-XL 1.0, SD-1.5, SD-2.1 Base, SD+MSE).

### Technique 1: Validating Efficacy of Typographical Errors in Prompts

Several examples from [44] establish that typographical errors were a successful attack strategy used by participants. To test whether typographical errors truly cause model failures, we use the SymSpell package [19] to detect misspelled words in submitted prompts. Then, we replace the misspelled word with the most likely replacement word generated by SymSpell. After all the mispelled words are detected, we manually remove any wrongly-detected misspellings (e.g., abbreviations such as "HD", names such as "tanaka"). Upon conclusion of our process, we identified 37 legitimate spelling errors in the 1,241 prompts.

In each instance, we replace the wrongly spelled word with its correct spelling but leave the prompt otherwise unchanged. These "new" prompts are then used to generate a new set of images using the same model endpoints that were used in [44]. The generated images are then analyzed using the Stable Diffusion image safety classifier.

Technique 2: Exploring the degree to which Semantic Ambiguity of Words and Phrases cause model failures

There are three possible interpretations of the "trigger words" derived from [44]'s NPMI analysis where they explore the correlation between words and certain failure modes: (1) These words directly cause the model failures; (2) The meaning of these words is the trigger, and thus any synonym will also trigger a model failure; and (3) The co-occurrence of some words with a given failure category is accidental or the result of a bias particular to either the dataset or the red-teaming process.

To determine which of these three possibilities is correct and how best a small dataset can be automatically augmented in a data-driven and meaningful way, we conducted a small experiment which is illustrated in Figure 2.

We assess whether the semantic ambiguity of a trigger word is responsible for the safety failures observed in the dataset. To accomplish this, we select the top 20 trigger words within each of the four failure categories defined by Quaye et al. : bias, hate, sexually explicit, violent. Then, we generate 5 synonyms for each trigger word using _gpt-3.5-turbo_ and replace the trigger word in the original prompt with the generated synonym. When there are more than 10 prompts containing a trigger word, we select a subset of 10 original prompts and minimize the cosine similarity between the 10 selected prompts. The prompts updated with the synonyms are then submitted to the model endpoints used in Quaye et al. We analyze the images using the Stable Diffusion image safety classifier to detect failures that are (i) _aligned_ with the original failure mode (e.g., if the original failure was "violence" and the newly generated image also failed on "violence"), and those (ii) classified as unsafe on _any_ one of the classifiers (e.g., if the original failure was "bias" but the newly generated image failed on a "sexual" classifier).

### Automated Safety Classifiers

Due to the multimodal nature of our work, we use two types of automatic safety classifiers: **text safety classifiers** to validate and verify the implicit adversariality of our newly generated _prompts_ and an **image safety classifier** to assess the safety of the _images_ generated by the T2I models.

Text Safety Classifiers for Prompts:We use three state-of-the-art open source safety classifiers to validate the safety of our newly generated prompts. This enables us to make the claim that our newly generated prompts can bypass existing text safety filters. We opt for a suite of three classifiers because text classifiers differ substantially in behavior on implicitly adversarial prompts.1

Footnote 1: Though we observe a high degree of agreement between Llama Guard and Perspective API, Perspective API appears to have less strict safety guards with respect to our prompts (only 0.7% of the prompts were annotated as unsafe by Perspective API, compared to 12.1% by Llama Guard). The DistilBERT-based Inappropriate Text Classifier is much stricter with safety relative to the other two models.

* **Llama Guard Input-Output Model 2:** An input-output safeguard model based on Llama 2 which can be used to classify the content of both LLM inputs (i.e., prompt classification) and LLM outputs (i.e., response classification). It is trained on around 13k examples from the Anthropic dataset [11] and other in-house collected red-teaming examples. We analyze all prompts using the default safety policy of the Llama Guard model combined with the safety violations from [44] (i.e., violent or graphic imagery, sexually explicit imagery, stereotypes & bias, and hate symbols, hate groups & harassment). We consider a prompt as "unsafe" when it is flagged as "unsafe" in at least one of the categories, and "safe" otherwise.

Figure 2: An overview of our approach to exploring semantic ambiguity of words. Each of the top 20 trigger words in the 4 safety categories undergo this process. After 5 synonyms of the word are generated, these synonyms are used to replace the words in the original prompts to create 5 “new” prompts. The “new” prompts are then sent to the t2i model endpoints to generate images, which are assessed for safety.

**Perspective API 3:** Based on multilingual BERT-based models trained on millions of comments from a variety of online forums, such as Wikipedia and The New York Times. It provides a probability score between 0 and 1 for the safety of a text for the following attributes: "toxicity", "severe toxicity", "identity attack", "insult", "profanity", and "threat". We consider a prompt "unsafe" when the model predicted a score >= 0.7 for at least one of the six attributes; this is the recommended threshold for research purposes in the model documentation. Footnote 3: https://perspectiveapi.com
* **DistilBERT-based Inappropriate Text Classifier 4:** A transformer model, based on DistilBERT and fine-tuned with 19,604 Reddit posts to classify a text as either "not safe for work" (NSFW) or "safe for work" (SFW). In addition, it also predicts the safety likelihood of the label, with scores between 0.5 and 1. For our analysis, we consider a prompt to be safe when the model predicts the "SFW" label and unsafe when the model predicts the "NSFW" label. Footnote 4: https://huggingface.co/michellejieli/inappropriate_text_classifier

Image Safety Classifier:We use the post-hoc safety filter in the Stable Diffusion model [52] that blocks explicit images. The filter blocks out any generated image that is close (in the embedding space of OpenAI's CLIP model [45]) to at least one of 17 pre-defined sensitive concepts.5 This classifier provides a binary output for safe/unsafe content and is primarily intended for detecting sexually explicit content in images, thus it will likely miss other unsafe categories such as violence, stereotyping, etc.

Footnote 5: The full list of concepts is: _sexual, nude, sex, 18+, naked, nsfw, porn, dick, vagina, explicit content, uncensored, fuck, nipples, naked breast, areola._ More details about the thresholds applied to each concept are provided in [49].

## 4 Preliminary Results and Discussion

### Evaluation of Technique 1: Typographical Errors

We track the number of instances where the corrected prompts (prompts where the misspelled words were corrected) generated at least one image with a safety failure in Table 1. We find that no failures persist for bias (0/4 prompts), hate (0/4 prompts), or other categories (0/5 prompts). Only one failure persists for violence after correcting the spelling (1/5 prompts), but in sexually explicit failures, 62% of failures persist (16/26 prompts). At first glance, the lack of failures after correcting spelling errors may indicate that misspelling is unlikely to be the _cause_ of sexually explicit failures. However, further probing reveals otherwise.

Because the image safety classifiers we used were designed to capture "not safe for work" content, they are less successful at identifying bias and hate failures (see Section 3.4). We, therefore, manually inspected the images in these categories and found clear cases of bias in the newly generated images. For example, images generated for the corrected prompt "a kid before the police" were predominantly images of Black children.

To ensure that images in violence and sexual categories were not systematically misclassified as safe, we also manually inspected these images. Within violence, manual inspection revealed images that were visually jarring and could be considered unsafe by some users (e.g., the image generated by the corrected prompt "a recrophiliac getting caught in the act" was quite graphic).

In the sexually explicit category, the corrected prompts for which the model did not generate an unsafe image were cases where the spelling correction led to a prompt so highly inappropriate that the model's safety filters blocked image generation.

Our findings indicate that spelling variations are a successful attack strategy if one can find an automated classifier that is good at identifying the failure of interest. The value of having access to human data helps ground the model with more realistic typographical errors that one might find in every day life. We believe that this strategy is better than a data-agnostic automated technique that might generate typographical errors which will not be encountered in practice. Sanity checking outcomes also helps to catch places that fully automated processes do not make provision for.

Evaluation of Technique 2: Exploring the degree to which Semantic Ambiguity of Words and Phrases cause model failures

If a specific word is responsible for the safety failures of the T2I model, we expect that replacing this word with a synonym will now cause the model to generate safe images. However, if we observe that the "replaced" prompt (prompt where synonym replaces original trigger word) still generates unsafe images, then this points to one of two possibilities: (i) the meaning of the trigger word is responsible for the model's failure, or (ii) the prompt as a whole, regardless of the trigger word, is causing the model failure.

Table 2 shows that sexually explicit and violent failures persist after replacing potential trigger words with a synonym; we observe similar rates of generation of unsafe images in both the original prompts as well as the replaced prompts. Results for bias and hate categories are difficult to interpret due to the unavailability of image safety classifiers that are capable of identifying representational harms (whereas humans can identify such examples as unsafe). Guided by a data-driven approach to selectively perturb prompt sections, this targeted augmentation effectively expands our source dataset and increases lexical diversity, while maintaining high attack success rates and preserving the original prompt's implicit adversariality.

### What Causes Some Data Augmentations to be Successful?

To determine whether the persistent failures discovered in Section 4.2 occur due to the semantics of the trigger word, or due to the broader characteristics of the prompts containing the trigger word, we conduct one extra experiment: **Technique 3: Generate 5 synonymous sentences for each prompt by rephrasing the entire prompt** using \(gpt\)_-3.5-\(turbo\). Then, we make a comparison between the two aggregation strategies: (i) computing the percent of errors for _each rephrased prompt_, and (ii) computing the percent of errors for each prompt where _only the trigger word was replaced with a synonym_.

If the semantic meaning of the trigger word is the primary cause of the observed failures, then we expect that the synonyms of that trigger word will be very effective at causing model failures. This is because the synonyms also capture the semantic meaning of the trigger word. However, if the primary cause of the failure is the broader context of the carrier phrase, then we expect to see that the rephrased prompts will be more effective at causing model failures. This is because the rephrased prompts capture the semantic meaning of the entire prompt. The results of this analysis are shown in Table 3 and Table 4.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Failure Type & \# Prompts & \# Failures & \% Persistent failures \\ \hline Bias & 4 & 0 & 0\% \\ Hate & 4 & 0 & 0\% \\ Sexual & 26 & 16 & 62\% \\ Violent & 5 & 1 & 20\% \\ Other & 5 & 0 & 0\% \\ \hline Total & 44 & 17 & 39\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results from fixing typographical errors across failure categories. The number of failures is the number of corrected prompts (prompts where the misspelled words were corrected) that generated at least one image with a safety failure.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Failure & N original & \multicolumn{2}{c}{\% of Original Prompts with:} & \multicolumn{2}{c}{\% of Replaced Prompts with:} \\ Type & Prompts & **Any** failure & **Aligned** failure & **Any** failure & **Aligned** failure \\ \hline bias & 17 & 17.65\% & 0.00\% & 25.48\% & 0.00\% \\ hate & 15 & 26.67\% & 6.67\% & 8.49\% & 0.00\% \\ sexual & 273 & 99.63\% & 96.70\% & 89.71\% & 65.69\% \\ violent & 142 & 66.20\% & 14.08\% & 56.28\% & 11.78\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results from experiments replacing the trigger word in prompts with its synonyms.

Due to challenges finding an automatic safety classifier that could confidently classify "Bias" and "Hate" violations, we abstrain from commenting on the results for those two categories.

Sexually Explicit CategoryWe observe in Table 3 that 79.6% of the replaced trigger words in the sexually explicit category were successful at causing the model to fail at least 50% of the time. This is slightly higher than when the rephrased prompts were used; 71% of the rephrased prompts caused the models to fail at least 50% of the time (see Table 4). This indicates that the trigger words that were identified from Quaye et al.'s NPMI analysis as responsible for causing sexually explicit failures are likely to be correct; replacement of these words with synonyms that preserve their semantic meaning resulted in persistence of the model failures.

Violent CategoryWe observe in Table 4 that 17.1% of the rephrased prompts caused the models to fail at least 50% of the time. This is slightly higher than when only the trigger words were replaced with synonyms; 15.1% of those prompts caused the model to fail at least 50% of the time (see Table 3). Though the difference is not significant, it signals that prompts that cause violent failures are more likely influenced by the broader meaning of the carrier phrase than by specific trigger words. A manual inspection of Quaye et al.'s top-20 violent trigger words (e.g. "child", "ground", "large") reveals that these words are not unique to violence.

## 5 Conclusion and Future Work

We present a scalable, data-driven approach to augmenting a dataset of human-written prompts that may seem harmless but can bypass text safety filters and deceive T2I models into producing unsafe images. Our study underscores the necessity of a balanced approach in developing adversarial datasets for evaluating (T2I) model safety. We confirm the potential of human-machine collaboration to significantly improve red-teaming, by retaining human-generated failure rates while ensuring that the generated prompts remain realistic and applicable to real-world scenarios. After validating our idea with these preliminary results, our future work involves scaling up our automation techniques to cover all the attack strategies in the Quaye et al. dataset and other well-annotated datasets. We will also utilize a wider range of state-of-the-art large language models with complementary strengths to refine our prompting techniques and extend our augmentation beyond synonym generation. We believe that this project will pave the way for the Machine Learning community to scale red-teaming efforts without compromising on the reality and relevance of the generated content.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{Percent of **rephrased prompts** leading to n\% image failures} \\ Failure category & \(\geq\)75\% & 50-75\% & 25-50\% & \(<\)25\% \\ \hline Bias & 0.0\% & 0.0\% & 11.1\% & 88.9\% \\ Hate & 0.0\% & 6.7\% & 0.0\% & 93.3\% \\ Sexual & 19.4\% & 51.6\% & 23.4\% & 5.5\% \\ Violence & 1.4\% & 15.7\% & 31.4\% & 51.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of Technique 3 (rephrase the entire prompt) with safety failures **aggregated by specific prompt** within each failure category.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{Percent of **trigger words** leading to n\% image failures} \\ Failure category & \(\geq\)75\% & 50-75\% & 25-50\% & \(<\)25\% \\ \hline Bias & 0.0\% & 0.0\% & 20.0\% & 80.0\% \\ Hate & 0.0\% & 1.9\% & 1.9\% & 96.2\% \\ Sexual & 4.1\% & 75.5\% & 20.5\% & 0.0\% \\ Violence & 3.8\% & 11.3\% & 67.9\% & 17.0\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of Technique 2 (replace only trigger word with 5 synonyms) with safety failures **aggregated by specific trigger words** within each failure category.

## References

* Aroyo and Paritosh [2021] Lora Aroyo and Praveen Paritosh. Uncovering unknown unknowns in machine learning. _Google Research Blog_, 2021. URL https://research.google/blog/uncovering-unknown-unknowns-in-machine-learning/.
* Aroyo et al. [2023] Lora Aroyo, Alex Taylor, Mark Diaz, Christopher Homan, Alicia Parrish, Gregory Serapio-Garcia, Vinodkumar Prabhakaran, and Ding Wang. Dices dataset: Diversity in conversational ai evaluation for safety. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 53330-53342. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/a74b697bce4cac6c91896372abaa8863-Paper-datasets_and_Benchmarks.pdf.
* Askell et al. [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kermion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. _CoRR_, abs/2112.00861, 2021. URL https://arxiv.org/abs/2112.00861.
* Attenberg et al. [2015] Joshua Attenberg, Panos Ipeirotis, and Foster Provost. Beat the machine: Challenging humans to find a predictive model's "unknown unknowns". _J. Data and Information Quality_, 6(1), mar 2015. ISSN 1936-1955. doi: 10.1145/2700832. URL https://doi.org/10.1145/2700832.
* Bai et al. [2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamille Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. URL https://arxiv.org/abs/2212.08073.
* Basu et al. [2023] Abhipsa Basu, R Venkatesh Babu, and Danish Pruthi. Inspecting the geographical representativeness of images from text-to-image models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* Bianchi et al. [2023] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, 2023.
* Blodgett et al. [2021] Su Lin Blodgett, Gilisinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1004-1015. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.81. URL https://aclanthology.org/2021.acl-long.81.
* Chen et al. [2024] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases, 2024. URL https://arxiv.org/abs/2407.12784.
* Cheng et al. [2020] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):3601-3608, April 2020. ISSN 2159-5399. doi: 10.1609/aaai.v34i04.5767. URL http://dx.doi.org/10.1609/aaai.v34i04.5767.

* B. D. G. and D. K. K. (2017)Audio-level approach to the evaluation of deep learning. In Proceedings of the 2017 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1-10. ACM. External Links: ISBN 978145035811055, Link, Document Cited by: SS1.
* D. G. and D. K. (2018)Audio-level approach to the evaluation of deep learning. In Proceedings of the 2018 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1-16. Cited by: SS1.
* D. G. and D. K. (2019)Audio-level approach to the evaluation of deep learning. In Proceedings of the 2019 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1-16. Cited by: SS1.
* D. G. and D. K. (2019)Audio-level approach to the evaluation of deep learning. In Proceedings of the 2019 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1-16. Cited by: SS1.
* D. G. and D. K. (2019)Audio-level approach to the evaluation of deep learning. In Proceedings of the 2019 ACM SIGKDD International Conference on Discovery and Data Mining, pp.

* [24] Christopher Homan, Gregory Serapio-Garcia, Lora Aroyo, Mark Diaz, Alicia Parrish, Vinodkumar Prabhakaran, Alex Taylor, and Ding Wang. Intersectionality in AI safety: Using multilevel models to understand diverse perceptions of safety in conversational AI. In Gavin Abercrombie, Valerio Basile, Davide Bernadi, Shiran Dudy, Simona Frenda, Lucy Havens, and Sara Tonelli, editors, _Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLP perspectives) @ LREC-COLING 2024_, pages 131-141, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.nlperspectives-1.15.
* [25] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. _Advances in Neural Information Processing Systems_, 36:78723-78747, 2023.
* [26] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in NLP. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4110-4124, 2021.
* [27] Hannah Kirk, Bertie Vidgen, Paul Rottger, Tristan Thrush, and Scott Hale. Hatemoji: A test suite and adversarially-generated dataset for benchmarking and detecting emoji-based hate. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1352-1368, 2022.
* [28] Hannah Rose Kirk, Bertie Vidgen, Paul Rottger, and Scott A. Hale. The empty signifier problem: Towards clearer paradigms for operationalising "alignment" in large language models. _arXiv preprint arXiv:2310.02457_, 2023.
* [29] Hannah Rose Kirk, Alexander Whitefield, Paul Rottger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. _arXiv preprint arXiv:2404.16019_, 2024.
* [30] Rafal Kocielnik, Shrimai Prabhumoye, Vivian Zhang, R Michael Alvarez, and Anima Anandkumar. Autobiastest: Controllable sentence generation for automated and open-ended social bias testing in language models. _arXiv preprint arXiv:2302.07371_, 2023.
* [31] Anurakt Kumar, Divyanshu Kumar, Jatan Loya, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, and Prashanth Harshangi. Sage-rt: Synthetic alignment data generation for safety evaluation and red teaming, 2024. URL https://arxiv.org/abs/2408.11851.
* [32] Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Moksh Jain. Learning diverse attacks on large language models for robust red-teaming and safety tuning, 2024. URL https://arxiv.org/abs/2405.18540.
* [33] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [34] Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Analyzing societal representations in diffusion models. _arXiv preprint arXiv:2303.11408_, 2023.
* [35] Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, and Douwe Kiela. Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking. _Advances in Neural Information Processing Systems_, 34:10351-10367, 2021.
* [36] Midjourney. Midjourney documentation and user guide. https://docs.midjourney.com/, 2023. (Accessed on 04/19/2023).

* Naik and Nushi [2023] Ranjita Naik and Besmira Nushi. Social biases through the text-to-image generation lens. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (AIES)_, 2023.
* Nie et al. [2020] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4885-4901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441.
* OpenAI [2020] OpenAI. DALL-E 2 system card,. URL https://github.com/openai/dalle-2-preview/blob/main/system-card.md#early-work. https://github.com/openai/dalle-2-preview/blob/main/system-card.md#early-work.
* DALL-E 3 system card,. URL https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf. https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf.
* Parrish et al. [2024] Alicia Parrish, Susan Hao, Sarah Laszlo, and Lora Aroyo. Is a picture of a bird a bird: A mixed-methods approach to understanding diverse human perspectives and ambiguity in machine vision models. In _Proceedings of the 2024 NLPerspectives Workshop_, 2024.
* Perez et al. [2022] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2022.
* Prabhakaran et al. [2022] Vinodkumar Prabhakaran, Rida Qadri, and Ben Hutchinson. Cultural incongrurencies in artificial intelligence. _arXiv preprint arXiv:2211.13069_, 2022.
* Quaye et al. [2024] Jessica Quaye, Alicia Parrish, Oana Inel, Charvi Rastogi, Hannah Rose Kirk, Minsuk Kahng, Erin Van Liemt, Max Bartolo, Jess Tsang, Justin White, Nathan Clement, Rafael Mosquera, Juan Ciro, Vijay Janapa Reddi, and Lora Aroyo. Adversarial Nibbler: An open red-teaming method for identifying diverse harms in text-to-image generation. In _The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, page 388-406. Association for Computing Machinery, 2024. doi: 10.1145/3630106.3658913. URL https://doi.org/10.1145/3630106.3658913.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Radharapu et al. [2023] Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, and Preethi Lahoti. Aart: Ai-assisted red-teaming with diverse data generation for new llvm-powered applications. _arXiv preprint arXiv:2311.08592_, 2023.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, pages 8821-8831. PMLR, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents, 2022.
* Rando et al. [2022] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. Red-teaming the stable diffusion safety filter. _arXiv preprint arXiv:2210.04610_, 2022.
* Rastogi et al. [2023] Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, and Saleema Amershi. Supporting Human-AI collaboration in auditing LLMs with LLMs. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (AIES)_, pages 913-926, 2023.
* Rawat et al. [2024] Ambrish Rawat, Stefan Schoepf, Giulio Zizzo, Giandomenico Cornacchia, Muhammad Zaid Hameed, Kieran Fraser, Erik Miehling, Beat Buesser, Elizabeth M. Daly, Mark Purcell, Prasanna Sattigeri, Pin-Yu Chen, and Kush R. Varshney. Attack atlas: A practitioner's perspective on challenges and pitfalls in red teaming genai, 2024. URL https://arxiv.org/abs/2409.15398.

* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, 2022.
* Rottger et al. [2022] Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pierrehumbert. Two contrasting data annotation paradigms for subjective NLP tasks. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 175-190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL https://aclanthology.org/2022.naacl-main.13.
* Sambasivan et al. [2021] Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi, and Vinodkumar Prabhakaran. Re-imagiting algorithmic fairness in india and beyond. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 315-328, 2021.
* Schramowski et al. [2023] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22522-22531, 2023.
* Schulhoff et al. [2023] Sander V Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Franos Bouchard, Chenglei Si, Jordan Lee Boyd-Graber, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, and Christopher R Carnahan. Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition. In _Empirical Methods in Natural Language Processing_, 2023.
* Shah et al. [2023] Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. Scalable and transferable black-box jailbreaks for language models via persona modulation, 2023. URL https://arxiv.org/abs/2311.03348.
* Shen et al. [2021] Hong Shen, Alicia DeVos, Motahhare Eslami, and Kenneth Holstein. Everyday algorithm auditing: Understanding the power of everyday users in surfacing harmful algorithmic behaviors. _Proceedings of the ACM on Human-Computer Interaction_, 5(CSCW2), oct 2021. doi: 10.1145/3479577. URL https://doi.org/10.1145/3479577.
* Vidgen et al. [2021] Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically generated datasets to improve online hate detection. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1667-1682, 2021.
* Wallace et al. [2018] Eric Wallace, Pedro Rodriguez, Shi Feng, and Jordan L. Boyd-Graber. Trick me if you can: Adversarial writing of trivia challenge questions. _CoRR_, abs/1809.02701, 2018. URL http://arxiv.org/abs/1809.02701.
* Wallace et al. [2019] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2153-2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL https://aclanthology.org/D19-1221.
* Wichers et al. [2024] Nevan Wichers, Carson Denison, and Ahmad Beirami. Gradient-based language model red teaming. In Yvette Graham and Matthew Purver, editors, _Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2862-2881, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.175.

* Yang et al. [2023] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Nan Xu, and Qiang Xu. Mma-diffusion: Multimodal attack on diffusion models. _arXiv preprint arXiv:2311.17516_, 2023.
* Zeng et al. [2024] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing lms, 2024. URL https://arxiv.org/abs/2401.06373.
* Zhang et al. [2023] Yanzhe Zhang, Lu Jiang, Greg Turk, and Diyi Yang. Auditing gender presentation differences in text-to-image models. _arXiv preprint arXiv:2302.03675_, 2023.
* Zou et al. [2023] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. URL https://arxiv.org/abs/2307.15043.