# MoVie: Visual Model-Based Policy Adaptation

for View Generalization

Sizhe Yang\({}^{12*}\), Yanjie Ze\({}^{13*}\), Huazhe Xu\({}^{415}\)

\({}^{1}\)Shanghai Qi Zhi Institute, \({}^{2}\)University of Electronic Science and Technology of China,

\({}^{3}\)Shanghai Jiao Tong University, \({}^{4}\)Tsinghua University, \({}^{5}\)Shanghai AI Lab

\({}^{*}\) Equal contribution

yangsizhe.github.io/MoVie

###### Abstract

Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of _view generalization_. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual **M**odel-based policies for **V**ew generalization (**MoVie**) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of **18** tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of **33\(\%\)**, **86\(\%\)**, and **152\(\%\)** respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Code and videos are available at yangsizhe.github.io/MoVie.

## 1 Introduction

Visual Reinforcement Learning (RL) has achieved great success in various applications such as video games [17, 18], robotic manipulation [22], and robotic locomotion [34]. However, one significant challenge for real-world deployment of visual RL agents remains: a policy trained with very limited views (commonly one single fixed view) might not generalize to unseen views. This challenge is especially pronounced in robotics, where a few fixed views may not adequately capture the variability of the environment. For instance, the RoboNet dataset [3] provides diverse views across a range of manipulation tasks, but training on such large-scale data only yields a moderate success rate (\(10\%\sim 20\%\)) for unseen views [3].

Recent efforts have focused on improving the visual generalization of RL agents [1, 7, 9, 36]. However, these efforts have mainly concentrated on generalizing to different appearances and backgrounds. In contrast, view generalization presents a unique challenge as the deployment view is unknown and may move freely in the 3D space. This might weaken the weapons such as data augmentations [12, 14, 33, 15] that are widely used in appearance generalization methods. Meanwhile, scaling up domain

Figure 1: **View generalization results across 3 domains. Our method MoVie largely improves the test-time view generalization ability, especially in robotic manipulation domains.**randomization [19; 21; 31] to all possible views is usually unrealistic because of the large cost and the offline nature of existing robot data. With these perspectives combined, it is difficult to apply those common approaches to address view generalization.

In this work, we commence by explicitly formulating the test-time view generalization problem into four challenging settings: _a)_**novel view**, where the camera changes to a fixed novel position and orientation; _b)_**moving view**, where the camera moves continuously around the scene, _c)_**shaking view**, where the camera experiences constant shaking, and _d)_**novel FOV**, where the field of view (FOV) of the camera is altered initially. These settings cover a wide spectrum of scenarios when visual RL agents are deployed to the real world. By introducing these formulations, we aim to advance research in addressing view generalization challenges and facilitate the utilization of robot datasets [3] and deployment on physical robotic platforms.

To address the view generalization problem, we argue that _adaptation_ to novel views during test time is crucial, rather than aiming for view-invariant policies. We propose MoVie, a simple yet effective method for adapting visual **M**odel-based policies to generalize to unseen **V**iews. MoVie leverages collected transitions from interactions and incorporates spatial transformer networks (STN [13]) in shallow layers, using the learning objective of the dynamics model (DM). Notably, MoVie requires no modifications during training and is compatible with various visual model-based RL algorithms. It only necessitates small-scale interactions for adaptation to the deployment view.

We perform extensive experiments on 7 robotic manipulation tasks (Advit hand [20] and xArm [7]) and 11 locomotion tasks (DMControl suite [30])), across the proposed 4 view generalization settings, totaling \(\mathbf{18}\)\(\times\)\(\mathbf{4}\) configurations. MoVie improves the view generalization ability substantially, compared to strong baselines including the inverse dynamics model (IDM [7]) and the dynamics model (DM). Remarkably, MoVie attains a relative improvement of \(86\%\) in xArm and \(152\%\) in Adroit, underscoring the potential of our method in robotics. **We are committed to releasing our code and testing platforms.** To conclude, our contributions are three-fold:

* We formulate the problem of view generalization in visual reinforcement learning with a wide range of tasks and settings that mimic real-world scenarios.
* We propose a simple model-based policy adaptation method for view generalization (**MoVie**), which incorporates STN into the shallow layers of the visual representation with a self-supervised dynamics prediction objective.
* We successfully showcase the effectiveness of our method through extensive experiments. The results serve as a testament to its capability and underscore its potential for practical deployment in robotic systems, particularly with complex camera views.

## 2 Related Work

**Visual generalization in reinforcement learning.** Agents trained by reinforcement learning (RL) from visual observations are prone to overfitting the training scenes, making it hard to generalize to

Figure 2: **Overview of MoVie**. During training (left), the agent is trained with the latent dynamics loss. At test time (right), we freeze the dynamics model and modify the encoder as a spatial adaptive encoder to adapt the agents to novel views.

unseen environments with appearance differences. A large corpus of recent works has focused on addressing this issue [7; 14; 15; 19; 21; 33; 35; 36; 9]. Notably, SODA [10] provides a visual generalization benchmark to better evaluate the generalizability of policies, while they only consider appearance changes of agents and backgrounds. Distracting control suite [26] adds both appearance changes and camera view changes into DMControl [30], where the task diversity is limited.

**View generalization in robotics.** The field of robot learning has long grappled with the challenge of training models on limited views and achieving generalization to unseen views. Previous studies, such as RoboNet [3], have collected extensive video data encompassing various manipulation tasks. However, even with pre-training on such large-scale datasets, success rates on unseen views have only reached approximately \(10\%\sim 20\%\)[3]. In recent efforts to tackle this challenge, researchers have primarily focused on third-person imitation learning [23; 24; 25] and view-invariant visual representations [32; 37; 16; 2; 4], but these approaches are constrained by the number of available camera views. In contrast, our work addresses a more demanding scenario where agents trained on a single fixed view are expected to generalize to diverse unseen views and dynamic camera settings.

**Test-time training.** There is a line of works that train neural networks at test-time with self-supervised learning in computer vision [28; 5; 29], robotics [27], and visual RL [7]. Specifically, PAD is the closest to our work [7], which adds an inverse dynamics model (IDM) objective into model-free policies for both training time and test time and gains better appearance generalization. In contrast, we differ in a lot of aspects: _(i)_ we focus on visual model-based policies, _(ii)_ we require no modification in training time, and _(iii)_ our method is designed for view generalization specifically.

## 3 Preliminaries

**Formulation.** We model the problem as a Partially Observable Markov Decision Process (POMDP) \(\mathcal{M}=\langle\mathcal{O},\mathcal{A},\mathcal{T},\mathcal{R},\gamma\rangle\), where \(\mathbf{o}\in\mathcal{O}\) are observations, \(\mathbf{a}\in\mathcal{A}\) are actions, \(\mathcal{F}:\mathcal{O}\times\mathcal{A}\mapsto\mathcal{O}\) is a transition function (called dynamics as well), \(r\in\mathcal{R}\) are rewards, and \(\gamma\in[0,1)\) is a discount factor. During training time, the agent's goal is to learn a policy \(\pi\) that maximizes discounted cumulative rewards on \(\mathcal{M}\), _i.e._, \(\max\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}\right]\). During test time, the reward signal from the environment is not accessible to agents and only observations are available, which are possible to experience subtle changes such as appearance changes and camera view changes.

**Model-based reinforcement learning. TD-MPC**[11] is a model-based RL algorithm that combines model predictive control and temporal difference learning. TD-MPC learns a visual representation \(\mathbf{z}=h(\mathbf{o})\) that maps the high-dimensional observation \(\mathbf{o}\in\mathcal{O}\) into a latent state \(\mathbf{z}\in\mathcal{Z}\) and a latent dynamics model \(d:\mathcal{Z}\times\mathcal{A}\mapsto\mathcal{Z}\) that predicts the future latent state \(\mathbf{z}^{\prime}=d(\mathbf{z},\mathbf{a})\) based on the current latent state \(\mathbf{z}\) and the action \(\mathbf{a}\). **MoDem**[8] accelerates TD-MPC with efficient utilization of expert demonstrations \(\mathcal{D}=\{D_{1},D_{2},\cdots,D_{N}\}\) to solve challenging tasks such as dexterous manipulation [20]. In this work, we select TD-MPC and MoDem as the backbone algorithm to train model-based agents, while our algorithm could be easily extended to most model-based RL algorithms such as Dreamer [6].

## 4 Method

We propose a simple yet effective method, visual **M**odel-based policy adaptation for **V**ew generalization (**MoVie**), which can accommodate visual RL agents to novel camera views at test time.

**Learning objective for the test time.** Given a tuple \((\mathbf{o}_{t},\mathbf{a}_{t},\mathbf{o}_{t+1})\), the original latent state dynamics prediction objective can be written as

\[\mathcal{L}_{\text{dynamics}}=\|d(h(\mathbf{o}_{t}),\mathbf{a}_{t})-h(\mathbf{ o}_{t+1})\|_{2},\] (1)

where \(h\) is an image encoder that projects a high-dimensional observation from space \(\mathcal{O}\) into a latent space \(\mathcal{Z}\) and \(d\) is a latent dynamics model \(d:\mathcal{Z}\times\mathcal{A}\mapsto\mathcal{Z}\).

In test time, the observations under unseen views lie in a different space \(\mathcal{O}^{\prime}\), so that their corresponding latent space also changes to \(\mathcal{Z}^{\prime}\). However, the projection \(h\) learned in training time can only map \(\mathcal{O}\mapsto\mathcal{Z}\) while the policy \(\pi\) only learns the mapping \(\mathcal{Z}\mapsto\mathcal{A}\), thus making the policy hard to generalize to the correct mapping function \(\mathcal{Z}^{\prime}\mapsto\mathcal{A}\). Our proposal is thus to adapt the projection \(h\) from a mapping function \(h:\mathcal{O}\mapsto\mathcal{Z}\) to a more useful mapping function \(h^{\prime}:\mathcal{O}^{\prime}\mapsto\mathcal{Z}\) so that the policy would execute the correct mapping \(\mathcal{Z}\mapsto\mathcal{A}\) without training. A vivid illustration is provided in Figure 3.

We freeze the latent dynamics model \(d\), denoted as \(d^{\star}\), so that the latent dynamics model is not a training target but a supervision. We also insert STN blocks [13] into the shallow layers of \(h\) to better adapt the projection \(h\), so that we write \(h\) as \(h^{\text{SAE}}\) (SAE denotes spatial adaptive encoder). Though the objective is still the latent state dynamics prediction loss, the supervision here is **superficially identical but fundamentally different** from training time. The formal objective is written as

\[\mathcal{L}_{\text{view}}=\|d^{\star}(h^{\text{SAE}}(\mathbf{o}),\mathbf{a})- h^{\text{SAE}}(\mathbf{o_{t+1}})\|_{2}.\] (2)

**Spatial adaptive encoder.** We now describe more details about our modified encoder architecture during test time, referred to as spatial adaptive encoder (SAE). To keep our method simple and fast to adapt, we only insert two different STNs into the original encoder, as shown in Figure 4. We observe in our experiments that transforming the low-level features (_i.e._, RGB features and shallow layer features) is most critical for adaptation, while the benefit of adding more STNs is limited (see Table 6). An STN block consists of two parts: _(i)_ a localisation net that predicts an affine transformation with \(6\) parameters and _(ii)_ a grid sampler that generates an affined grid and samples features from the original feature map. The point-wise affine transformation is written as

\[\left(\begin{array}{c}x^{s}\\ y^{s}\end{array}\right)=\mathcal{T}_{\phi}\left(G\right)=\mathrm{A}_{\phi} \left(\begin{array}{c}x^{t}\\ y^{t}\\ 1\end{array}\right)=\left[\begin{array}{ccc}\phi_{11}&\phi_{12}&\phi_{13}\\ \phi_{21}&\phi_{22}&\phi_{23}\end{array}\right]\left(\begin{array}{c}x^{t} \\ y^{t}\\ 1\end{array}\right)\] (3)

where \(G\) is the sampling grid, \((x^{t}_{i},y^{t}_{i})\) are the target coordinates of the regular grid in the output feature map, \((x^{s}_{i},y^{s}_{i})\) are the source coordinates in the input feature map that define the sample points, and \(\mathrm{A}_{\phi}\) is the affine transformation matrix.

**Training strategy for SAE.** We use a learning rate \(1\times 10^{-5}\) for STN layers and \(1\times 10^{-7}\) for the encoder. We utilize a replay buffer with size \(256\) to store history observations and update \(32\) times for each time step to heavily utilize the online data. Implementation details remain in Appendix A.

Figure 4: **The architecture of our spatial adaptive encoder (SAE).** We incorporate STNs before the first two convolutional layers of the encoder for better adaptation of the representation \(h\).

Figure 3: **An illustration of the reason why MoVie is effective.** We treat the frozen dynamics model as the source of supervision to adapt the latent space of \(h^{\text{SAE}}\) to that of the training views.

## 5 Experiments

In this section, we investigate how well an agent trained on a single fixed view generalizes to unseen views during test time. During the evaluation, agents have no access to reward signals, presenting a significant challenge for agents to self-supervise using online data.

### Experiment Setup

**Formulation of camera view variations**: _a)_ **novel view**, where we maintain a fixed camera target while adjusting the camera position in both horizontal and vertical directions by a certain margin, _b)_ **moving view**, where we establish a predefined trajectory encompassing the scene and the camera follows this trajectory, moving back and forth for each time step while focusing on the center of the scene, _c)_ **shaking view**, where we add Gaussian noise onto the original camera position at each time step, and _d)_ **novel FOV**, where the FOV of the camera is altered once, different from the training phase. A visualization of four settings is provided in Figure 6 and videos are available in yangsizhe.github.io/MoVie for a better understanding of our configurations. Details remain in Appendix B.

**Tasks.** Our test platform consists of **18** tasks from **3** domains: 11 tasks from DMControl [30], 3 tasks from Adroit [20], and 4 tasks from xArm [7]. A visualization of the three domains is in Figure 5. Although the primary motivation for this study is for addressing view generalization in real-world robot learning, which has not yet been conducted, we contend that the extensive range of tasks tackled in our research effectively illustrates the potential of MoVie for real-world application. We run 3 seeds per experiment with seed numbers \(0,1,2\) and run 20 episodes per seed. During these 20 episodes, the models could store the history transitions. We report cumulative rewards for DMControl tasks and success rates for robotic manipulation tasks, averaging over episodes.

**Baselines.** We first train visual model-based policies with TD-MPC [11] on DMControl and xArm environments and MoDem [8] on Adroit environments under the default configurations and then test these policies in the view generalization setting. Since MoVie consists of two components mainly (_i.e._, DM and STN), we build two test-time adaptation algorithms by replacing each module: _a)_ **DM**, which removes STN from MoVie and _b)_ **IDM+STN**, which replaces DM in MoVie with IDM [7]. IDM+STN is very close to PAD [7], and we add STN for fair comparison. We keep all training settings the same.

Figure 5: **Visualization of three task domains**, including DMControl [30], xArm [7], Adroit [20].

Figure 6: **Visualization of the training view and four view generalization settings.** Trajectory visualization for all tasks is available on our website yangsizhe.github.io/MoVie.

[MISSING_PAGE_EMPTY:6]

disparity to two factors: _a)_ the inherent complexity of robotic manipulation tasks and _b)_ the pressing need for effective view generalization in this domain.

**Challenges in handling shaking view.** Despite improvements of MoVie in various settings, we have identified a relative weakness in addressing the _shaking view_ scenario. For instance, in xArm tasks, the success rate of MoVie is only \(45\%\), which is close to the \(42\%\) success rate of TD-MPC without adaptation. Other baselines such as DM and IDM+STN also experience performance drops. We acknowledge the inherent difficulty of the shaking view scenario, while it is worth noting that in real-world robotic manipulation applications, cameras often exhibit smoother movements or are positioned in fixed views, partially mitigating the impact of shaking view.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Shaking view & TD-MPC & DM & IDM+STN & MoVie \\ \hline Cheetah, run & \(381.04\pm 39.31\) & \(317.66\pm 9.57\) & \(212.31\pm 19.74\) & \(\mathbf{493.54\pm 56.80}\) \\ Walker, walk & \(662.13\pm 57.71\) & \(627.77\pm 21.47\) & \(340.48\pm 36.92\) & \(\mathbf{835.99\pm 14.98}\) \\ Walker, stand & \(\mathbf{834.70\pm 1.71}\) & \(604.55\pm 16.20\) & \(471.30\pm 69.19\) & \(687.96\pm 9.11\) \\ Walker, run & \(251.58\pm 13.69\) & \(186.83\pm 3.75\) & \(128.31\pm 9.19\) & \(\mathbf{291.39\pm 7.94}\) \\ Cup, catch & \(752.86\pm 41.81\) & \(835.16\pm 32.91\) & \(648.75\pm 12.77\) & \(\mathbf{951.20\pm 21.01}\) \\ Finger, spin & \(89.55\pm 47.72\) & \(88.56\pm 5.86\) & \(145.68\pm 29.33\) & \(\mathbf{284.05\pm 473.73}\) \\ Finger, turn-easy & \(\mathbf{717.60\pm 40.93}\) & \(449.10\pm 20.48\) & \(656.23\pm 50.35\) & \(694.20\pm 91.02\) \\ Finger, turn-hard & \(411.41\pm 40.28\) & \(383.96\pm 32.94\) & \(132.40\pm 34.25\) & \(\mathbf{629.20\pm 110.57}\) \\ Pendulum, swingup & \(23.73\pm 9.24\) & \(\mathbf{57.41\pm 4.50}\) & \(27.30\pm 2.38\) & \(48.23\pm 30.25\) \\ Reacher, easy & \(529.58\pm 65.05\) & \(225.08\pm 31.02\) & \(299.11\pm 59.00\) & \(\mathbf{771.38\pm 150.60}\) \\ Reacher, hard & \(205.53\pm 25.16\) & \(54.80\pm 32.45\) & \(144.11\pm 5.60\) & \(\mathbf{453.48\pm 381.60}\) \\ D\(\mathsf{MControl}\) & \(441.79\) & \(348.26\) & \(291.45\) & \(\mathbf{558.23(\pm 26\%)}\) \\ \hline xArm, reach & \(76\pm 2\) & \(76\pm 7\) & \(83\pm 2\) & \(\mathbf{88\pm 20}\) \\ xArm, push & \(\mathbf{64\pm 3}\) & \(55\pm 10\) & \(61\pm 5\) & \(57\pm 13\) \\ xArm, peg in box & \(3\pm 2\) & \(\mathbf{15\pm 13}\) & \(3\pm 2\) & \(5\pm 5\) \\ xArm, hammer & \(25\pm 5\) & \(28\pm 7\) & \(28\pm 12\) & \(\mathbf{30\pm 13}\) \\ xArm & \(42\) & \(44\) & \(44\) & \(\mathbf{45(\pm 17\%)}\) \\ \hline Adroit, door & \(1\pm 2\) & \(10\pm 0\) & \(5\pm 5\) & \(\mathbf{83\pm 2}\) \\ Adroit, hammer & \(58\pm 12\) & \(65\pm 18\) & \(55\pm 20\) & \(\mathbf{70\pm 27}\) \\ Adroit, pen & \(30\pm 5\) & \(30\pm 8\) & \(18\pm 5\) & \(\mathbf{35\pm 0}\) \\ Adroit & \(30\) & \(35\) & \(26\) & \(\mathbf{63(\pm 110\%)}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Results in shaking view.** The best method on each task is in **bold**.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Moving view & TD-MPC & DM & IDM+STN & MoVie \\ \hline Cheetah, run & \(235.37\pm 63.39\) & \(344.70\pm 7.54\) & \(199.48\pm 25.78\) & \(\mathbf{365.22\pm 42.60}\) \\ Walker, walk & \(632.77\pm 15.62\) & \(707.60\pm 26.72\) & \(373.44\pm 30.85\) & \(\mathbf{810.19\pm 7.77}\) \\ Walker, stand & \(\mathbf{803.97\pm 15.75}\) & \(706.05\pm 5.75\) & \(576.87\pm 44.69\) & \(712.48\pm 11.67\) \\ Walker, run & \(\mathbf{295.54\pm 4.39}\) & \(250.84\pm 7.02\) & \(190.02\pm 2.61\) & \(281.43\pm 33.34\) \\ Cup, catch & \(887.20\pm 3.67\) & \(910.51\pm 4.12\) & \(915.16\pm 7.31\) & \(\mathbf{951.26\pm 10.68}\) \\ Finger, spin & \(636.91\pm 3.91\) & \(697.11\pm 6.67\) & \(532.90\pm 3.25\) & \(\mathbf{896.00\pm 21.65}\) \\ Finger, turn-easy & \(715.45\pm 19.23\) & \(728.93\pm 180.26\) & \(683.85\pm 112.64\) & \(\mathbf{744.45\pm 16.54}\) \\ Finger, turn-hard & \(\mathbf{593.46\pm 48.22}\) & \(454.58\pm 88.50\) & \(559.20\pm 121.35\) & \(\mathbf{55.26\pm 25.64}\) \\ Pendulum, swingup & \(26.23\pm 1.85\) & \(83.88\pm 10.55\) & \(47.20\pm 4.39\) & \(236.81\pm 50.23\) \\ Reacher, easy & \(\mathbf{984.10\pm 43.38}\) & \(981.78\pm 6.06\) & \(695.03\pm 326.91\) & \(982.58\pm 13.39\) \\ Reacher, hard & \(854.76\pm 31.54\) & \(864.55\pm 56.37\) & \(752.55\pm 83.55\) & \(\mathbf{872.46\pm 51.30}\) \\ D\(\mathsf{MControl}\) & \(605.98\) & \(611.87\) & \(502.34\) & \(\mathbf{673.74(\pm 11\%)}\) \\ \hline xArm, reach & \(15\pm 5\) & \(71\pm 2\) & \(21\pm 2\) & \(\mathbf{80\pm 21}\) \\ xArm, push & \(58\pm 5\) & \(52\pm 10\) & \(55\pm 5\) & \(\mathbf{73\pm 5}\) \\ xArm, peg in box & \(0\pm 0\) & \(0\pm 0\) & \(1\pm 2\) & \(\mathbf{5\pm 8}\) \\ xArm, hammer & \(8\pm 7\) & \(0\pm 0\) & \(\mathbf{10\pm 8}\) & \(8\pm 7\) \\ xArm & \(20\) & \(31\) & \(24\) & \(\mathbf{42(\pm 110\%)}\) \\ \hline Adroit, door & \(0\pm 0\) & \(10\pm 5\) & \(0\pm 0\) & \(\mathbf{66\pm 7}\) \\ Adroit, hammer & \(25\pm 8\) & \(31\pm 12\) & \(28\pm 02\) & \(\mathbf{43\pm 18}\) \\ Adroit, pen & \(20\pm 0\) & \(20\pm 10\) & \(18\pm 14\) & \(\mathbf{26\pm 2}\) \\ Adroit

**Effective adaptation in novel view, moving view, and novel FOV scenarios.** In addition to the shaking view setting, MoVie consistently outperforms TD-MPC without adaptation by \(2\times\sim 4\times\) in robotic manipulation tasks across the other three settings. It is worth noting that these three settings are among the most common scenarios encountered in real-world applications.

**Real-world implications.** Our findings have important implications for real-world deployment of robots. Previous methods, relying on domain randomization and extensive data augmentation during training, often hinder the learning process. Our proposed method enables direct deployment of offline or simulation-trained agents, improving success rates with minimal interactions.

### Ablations

To validate the rationale behind several choices of MoVie and our test platform, we perform a comprehensive set of ablation experiments.

**Integration of STN with low-level features.** To enhance view generalization, we incorporate two STN blocks [13], following the image observation and the initial convolutional layer of the feature encoder. This integration is intended to align the low-level features with the training view, thereby preserving the similarity of deep semantic features to the training view. As shown in Table 6, by progressively adding more layers to the feature encoder, we observe that deeper layers do not provide significant additional benefits, supporting our intuition for view generalization.

**Different novel views.** We classify novel views into three levels of difficulty, with our main experiments employing the _medium_ difficulty level by default. Table 7 presents additional results for

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Cheetah-run & 0 & 1 & 2 (MoVie) & 3 & 4 \\ \hline Novel View & \(254.42_{\pm 13.89}\) & \(259.64_{\pm 32.01}\) & \(\mathbf{342.39_{\pm 54.05}}\) & \(309.25_{\pm 33.19}\) & \(327.39_{\pm 10.44}\) \\ Moving View & \(344.70_{\pm 7.54}\) & \(360.29_{\pm 13.64}\) & \(\mathbf{365.22_{\pm 42.66}}\) & \(361.28_{\pm 6.78}\) & \(357.29_{\pm 31.24}\) \\ Shaking View & \(317.66_{\pm 9.57}\) & \(491.96_{\pm 20.07}\) & \(\mathbf{493.54_{\pm 56.80}}\) & \(454.82_{\pm 19.57}\) & \(472.57_{\pm 8.49}\) \\ Novel FOV & \(379.01_{\pm 10.90}\) & \(512.69_{\pm 17.67}\) & \(\mathbf{532.94_{\pm 19.74}}\) & \(505.65_{\pm 12.02}\) & \(492.79_{\pm 21.66}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Ablation on applying different numbers of STNs from shallow to deep.** The best method on each setting is in **bold**.

\begin{table}
\begin{tabular}{c c c c|c} \hline \hline Novel FOV & TD-MPC & DM & IDM+STN & MoVie \\ \hline Cheetah, run & \(128.55_{\pm 6.57}\) & \(379.01_{\pm 10.90}\) & \(299.02_{\pm 88.47}\) & \(\mathbf{532.94_{\pm 19.74}}\) \\ Walker, walk & \(239.54_{\pm 129.47}\) & \(882.21_{\pm 12.75}\) & \(579.58_{\pm 47.87}\) & \(\mathbf{920.18_{\pm 8.76}}\) \\ Walker, stand & \(679.70_{\pm 21.21}\) & \(732.07_{\pm 14.77}\) & \(678.46_{\pm 55.32}\) & \(\mathbf{785.52_{\pm 24.45}}\) \\ Walker, run & \(184.28_{\pm 1.39}\) & \(337.28_{\pm 11.40}\) & \(295.99_{\pm 10.00}\) & \(\mathbf{482.03_{\pm 13.84}}\) \\ Cup, catch & \(940.20_{\pm 28.21}\) & \(912.70_{\pm 34.54}\) & \(964.83_{\pm 10.45}\) & \(\mathbf{973.60_{\pm 1.94}}\) \\ Finger, spin & \(675.81_{\pm 4.99}\) & \(886.63_{\pm 4.54}\) & \(414.83_{\pm 79.20}\) & \(\mathbf{917.21_{\pm 1.71}}\) \\ Finger, turn-easy & \(792.26_{\pm 154.24}\) & \(690.61_{\pm 140.88}\) & \(805.63_{\pm 54.95}\) & \(\mathbf{845.35_{\pm 35.52}}\) \\ Finger, turn-hard & \(591.43_{\pm 109.84}\) & \(393.18_{\pm 118.38}\) & \(449.88_{\pm 121.99}\) & \(\mathbf{640.21_{\pm 18.23}}\) \\ Pendulum, swingup & \(126.90_{\pm 75.86}\) & \(184.33_{\pm 83.99}\) & \(253.36_{\pm 75.26}\) & \(\mathbf{699.90_{\pm 50.64}}\) \\ Reacher, easy & \(929.40_{\pm 12.35}\) & \(931.56_{\pm 12.69}\) & \(937.20_{\pm 55.97}\) & \(\mathbf{971.48_{\pm 11.07}}\) \\ Reacher, hard & \(424.11_{\pm 83.74}\) & \(421.56_{\pm 94.93}\) & \(287.96_{\pm 6.33}\) & \(\mathbf{707.75_{\pm 86.05}}\) \\ DMControl & \(527.47\) & \(613.74\) & \(542.43\) & \(\mathbf{770.56}_{\pm 69\%}\) \\ \hline xArm, reach & \(53_{\pm 12}\) & \(98_{\pm 2}\) & \(8_{\pm 5}\) & \(\mathbf{100_{\pm 0}}\) \\ xArm, push & \(60_{\pm 13}\) & \(71_{\pm 5}\) & \(60_{\pm 5}\) & \(\mathbf{81_{\pm 2}}\) \\ xArm, peg in box & \(20_{\pm 5}\) & \(13_{\pm 10}\) & \(6_{\pm 11}\) & \(\mathbf{63_{\pm 11}}\) \\ xArm, hammer & \(3_{\pm 2}\) & \(6_{\pm 5}\) & \(8_{\pm 2}\) & \(\mathbf{55_{\pm 5}}\) \\ xArm & \(34\) & \(47\) & \(40\) & \(\mathbf{75_{\pm 121\%}}\) \\ \hline Adroit, door & \(1_{\pm 2}\) & \(58_{\pm 10}\) & \(3_{\pm 5}\) & \(\mathbf{81_{\pm 12}}\) \\ Adroit, hammer & \(66_{\pm 7}\) & \(75_{\pm 18}\) & \(76_{\pm 10}\) & \(\mathbf{81_{\pm 10}}\) \\ Adroit, pen & \(26_{\pm 10}\) & \(20_{\pm 8}\) & \(15_{\pm 5}\) & \(\mathbf{41_{\pm 7}}\) \\ Adroit & \(31\) & \(51\) & \(31\) & \(\mathbf{68_{\pm 110\%}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Novel FOV.** The best method on each task is in **bold**.

the _easy_ and _hard_ difficulty levels. As the difficulty level increases, we observe a consistent decrease in the performance of all the methods.

**The efficacy of STN in conjunction with IDM.** Curious readers might be concerned about our direct utilization of IDM+STN in the main experiments, suggesting that STN could potentially be detrimental to IDM. However, Table 8 shows that STN not only benefits our method but also improves the performance of IDM, thereby demonstrating effectiveness of SAE for adaptation of the representation and validating our baseline selection.

**Finetune or freeze the DM.** In our approach, we employ the frozen DM as a form of supervision to guide the adaptation process of the encoder. However, it remains unverified for the readers whether end-to-end finetuning of both the DM and the encoder would yield similar benefits. The results presented in Table 9 demonstrate that simplistic end-to-end finetuning does not outperform MoVie, thereby reinforcing the positive results obtained by MoVie.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Cheetah-run & TD-MPC & DM & IDM+STN & MoVie \\ \hline Easy & \(265.90\pm 7.94\) & \(441.61\pm 14.00\) & \(349.86\pm 55.82\) & \(\mathbf{466.67\pm 58.94}\) \\ Medium & \(90.13\pm 20.38\) & \(254.43\pm 13.89\) & \(127.76\pm 16.16\) & \(\mathbf{342.39\pm 54.95}\) \\ Hard & \(48.64\pm 13.20\) & \(112.39\pm 8.36\) & \(59.64\pm 5.35\) & \(\mathbf{215.36\pm 62.03}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Ablation on different novel views.** The best method on each setting is in **bold**.

\begin{table}
\begin{tabular}{c c c} \hline \hline Task & Setting & IDM & IDM+STN \\ \hline \multirow{4}{*}{xArm, push} & Novel view & \(40\pm 5\) & \(\mathbf{60\pm 5}\) \\  & Moving view & \(46\pm 2\) & \(\mathbf{55\pm 5}\) \\  & Shaking view & \(60\pm 10\) & \(\mathbf{61\pm 5}\) \\  & Novel FOV & \(58\pm 2\) & \(\mathbf{60\pm 5}\) \\  & All settings & \(51\) & \(\mathbf{59}\) \\ \hline \multirow{4}{*}{xArm, hammer} & Novel view & \(6\pm 5\) & \(\mathbf{8\pm 5}\) \\  & Moving view & \(5\pm 0\) & \(\mathbf{10\pm 5}\) \\  & Shaking view & \(\mathbf{35\pm 8}\) & \(28\pm 12\) \\  & Novel FOV & \(6\pm 7\) & \(\mathbf{8\pm 2}\) \\  & All settings & \(13\) & \(\mathbf{14}\) \\ \hline \multirow{4}{*}{Cup, catch} & Novel view & \(867.87\pm 3.14\) & \(\mathbf{932.75\pm 23.34}\) \\  & Moving view & \(912.85\pm 10.22\) & \(\mathbf{915.16\pm 7.31}\) \\  & Shaking view & \(435.40\pm 87.32\) & \(\mathbf{648.75\pm 12.77}\) \\  & Novel FOV & \(815.27\pm 34.61\) & \(\mathbf{964.83\pm 10.45}\) \\  & All settings & \(757.84\) & \(\mathbf{865.37}\) \\ \hline \multirow{4}{*}{Finger, spin} & Novel view & \(\mathbf{177.26\pm 2.49}\) & \(141.03\pm 37.52\) \\  & Moving view & \(332.90\pm 0.28\) & \(\mathbf{532.90\pm 3.25}\) \\  & Shaking view & \(106.72\pm 7.11\) & \(\mathbf{145.68\pm 2.93}\) \\  & Novel FOV & \(311.66\pm 8.55\) & \(\mathbf{414.83\pm 79.20}\) \\  & All settings & \(232.13\) & \(\mathbf{301.86}\) \\ \hline \multirow{4}{*}{Cheetah, run} & Novel view & \(111.73\pm 18.98\) & \(\mathbf{127.76\pm 16.16}\) \\  & Moving view & \(\mathbf{205.19\pm 25.45}\) & \(199.48\pm 25.78\) \\ \cline{1-1}  & Shaking view & \(210.06\pm 27.99\) & \(\mathbf{212.32\pm 19.74}\) \\ \cline{1-1}  & Novel FOV & \(262.76\pm 62.17\) & \(\mathbf{299.03\pm 88.47}\) \\ \cline{1-1}  & All settings & \(197.43\) & \(\mathbf{209.43}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Ablation on IDM with and without STN.** The best method is in **bold**.

## 6 Conclusion

In this study, we present MoVie, a method for adapting visual model-based policies to achieve view generalization. MoVie mainly finetunes a spatial adaptive image encoder using the objective of the latent state dynamics model during test time. Notably, we maintain the dynamics model in a frozen state, allowing it to function as a form of self-supervision. Furthermore, we categorize the view generalization problem into four distinct settings: novel view, moving view, shaking view, and novel FOV. Through a systematic evaluation of MoVie on 18 tasks across these four settings, totaling 64 different configurations, we demonstrate its general and remarkable effectiveness.

One limitation of our work is the lack of real robot experiments, while our focus is on addressing view generalization in robot datasets and deploying visual reinforcement learning agents in real-world scenarios. In our future work, we would evaluate MoVie on real-world robotic tasks.

## Acknowledgment

This work is supported by National Key R&D Program of China (2022ZD0161700).

\begin{table}
\begin{tabular}{c c c c} \hline \hline Task & Setting & Finetune DM & MoVie \\ \hline \multirow{4}{*}{xArm, push} & Novel view & **51\(\pm\)2** & 48\(\pm\)7 \\  & Moving view & 48\(\pm\)7 & **73\(\pm\)5** \\  & Shaking view & 45\(\pm\)5 & **57\(\pm\)13** \\  & Novel FOV & 68\(\pm\)5 & **81\(\pm\)2** \\  & All settings & 53 & **64** \\ \hline \multirow{4}{*}{xArm, hammer} & Novel view & 10\(\pm\)5 & **36\(\pm\)12** \\  & Moving view & 0\(\pm\)0 & **8\(\pm\)7** \\  & Shaking view & 5\(\pm\)5 & **30\(\pm\)13** \\  & Novel FOV & 11\(\pm\)2 & **55\(\pm\)5** \\  & All settings & 6 & **32** \\ \hline \multirow{4}{*}{Cup, catch} & Novel view & 648.90\(\pm\)25.668 & **961.98\(\pm\)2.68** \\  & Moving view & 676.05\(\pm\)79.69 & **951.26\(\pm\)10.68** \\  & Shaking view & 228.20\(\pm\)7.84 & **951.20\(\pm\)21.01** \\  & Novel FOV & 658.50\(\pm\)8.48 & **973.60\(\pm\)1.94** \\  & All settings & 552.91 & **959.51** \\ \hline \multirow{4}{*}{Finger, spin} & Novel view & 278.57\(\pm\)26.34 & **892.01\(\pm\)1.20** \\  & Moving view & 192.95\(\pm\)72.76 & **896.00\(\pm\)21.65** \\  & Shaking view & 1.50\(\pm\)0.70 & **284.05\(\pm\)473.73** \\  & Novel FOV & 372.82\(\pm\)51.63 & **917.21\(\pm\)1.71** \\  & All settings & 211.46 & **747.31** \\ \hline \multirow{4}{*}{Cheetah, run} & Novel view & 273.01\(\pm\)11.29 & **342.39\(\pm\)54.95** \\  & Moving view & 331.55\(\pm\)22.22 & **365.22\(\pm\)42.66** \\ \cline{1-1}  & Shaking view & 476.25\(\pm\)30.25 & **493.54\(\pm\)56.80** \\ \cline{1-1}  & Novel FOV & **561.94\(\pm\)37.73** & 532.94\(\pm\)19.74 \\ \cline{1-1}  & All settings & 410.68 & **433.52** \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Ablation on whether to finetune the DM. The best method is in bold.**

## References

* [1] David Bertoin, Adil Zouitine, Mehdi Zouitine, and Emmanuel Rachelson. Look where you look! saliency-guided q-networks for generalization in visual reinforcement learning. _NeurIPS_, 2022.
* [2] Boyuan Chen, Pieter Abbeel, and Deepak Pathak. Unsupervised learning of visual 3d keypoints for control. In _ICML_, 2021.
* [3] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. _CoRL_, 2019.
* [4] Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, and Marc Toussaint. Reinforcement learning with neural radiance fields. _arXiv_, 2022.
* [5] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. _NeurIPS_, 2022.
* [6] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. _arXiv_, 2020.
* [7] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. _ICLR_, 2021.
* [8] Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, and Aravind Rajeswaran. Modem: Accelerating visual model-based reinforcement learning with demonstrations. _ICLR_, 2023.
* [9] Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. _NeurIPS_, 2021.
* [10] Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. In _International Conference on Robotics and Automation_, 2021.
* [11] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control. _ICML_, 2022.
* [12] Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, and Xiaolong Wang. On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline. _ICML_, 2023.
* [13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. _NeurIPS_, 2015.
* [14] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. _NeurIPS_, 2020.
* [15] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique for generalization in deep reinforcement learning. _arXiv preprint arXiv:1910.05396_, 2019.
* [16] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene representations for visuomotor control. In _CoRL_, 2022.
* [17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv_, 2013.
* [18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 2015.
* [19] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In _ICRA_, 2018.
* [20] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. _RSS_, 2018.
* [21] Fabio Ramos, Rafael Carvalhoess Possas, and Dieter Fox. Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. _arXiv_, 2019.

* [22] Rutav Shah and Vikash Kumar. Rrl: Resnet as representation for reinforcement learning. _ICML_, 2021.
* [23] Jinghuan Shang and Michael S Ryoo. Self-supervised disentangled representation learning for third-person imitation learning. In _IROS_, 2021.
* [24] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta. Third-person visual imitation learning via decoupled hierarchical controller. _NeurIPS_, 2019.
* [25] Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. _ICLR_, 2017.
* [26] Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite-a challenging benchmark for reinforcement learning from pixels. _arXiv_, 2021.
* [27] Yu Sun, Wyatt L Ubellacker, Wen-Loong Ma, Xiang Zhang, Changhao Wang, Noel V Csomay-Shanklin, Masayoshi Tomizuka, Koushil Sreenath, and Aaron D Ames. Online learning of unknown dynamics for model-based controllers in legged locomotion. _RA-L_, 2021.
* [28] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _ICML_, 2020.
* [29] Yu Sun, Xiaolong Wang, Liu Zhuang, John Miller, Moritz Hardt, and Alexei A. Efros. Test-time training with self-supervision for generalization under distribution shifts. In _ICML_, 2020.
* [30] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv_, 2018.
* [31] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _IROS_, 2017.
* [32] Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, and Katerina Fragkiadaki. 3d-oes: Viewpoint-invariant object-factorized environment simulators. _arXiv_, 2020.
* [33] Kaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforcement learning with mixture regularization. _NeurIPS_, 2020.
* [34] Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, and Xiaolong Wang. Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. _ICLR_, 2021.
* [35] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. _arXiv_, 2021.
* [36] Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, and Huazhe Xu. Pre-trained image encoder for generalizable visual reinforcement learning. _NeurIPS_, 2022.
* [37] Yanjie Ze, Nicklas Hansen, Yinbo Chen, Mohit Jain, and Xiaolong Wang. Visual reinforcement learning with self-supervised 3d representations. _RA-L_, 2023.

## Appendix A Implementation Details

In this section, we describe the implementation details of our algorithm for training on the training view and test time training in view generalization settings on the DMControl [30], xArm [7], and Adroit [20] environments. We utilize the official implementation of TD-MPC [11] and MoDem [8] which are available at github.com/nicklashansen/dmpc and github.com/facebookresearch/modem as the model-based reinforcement learning codebase. During training time, we use the default hyperparameters in official implementation of TD-MPC and MoDem. We present relevant hyperparameters during both training and test time in Table 10 and Table 11. One seed of our experiments could be run on a single 3090 GPU with fewer than \(2\)GB and it takes \(\sim 1\) hours for test-time training.

**Training time setup.** We train visual model-based policies with TD-MPC on DMControl and xArm environments, and MoDem on Adroit environments, We employ identical network architecture and hyperparameters as original TD-MPC and MoDem during training time.

The network architecture of the encoder in original TD-MPC is composed of a stack of 4 convolutional layers, each with 32 filters, no padding, stride of 2, 7 \(\times\) 7 kernels for the first one, 5 \(\times\) 5 kernels for the second one and 3 \(\times\) 3 kernels for all others, yielding a final feature map of dimension 3 \(\times\) 3 \(\times\) 32 (inputs whose framestack is 3 have dimension 84 \(\times\) 84 \(\times\) 9). After the convolutional layers, a fully connected layer with an input size of 288 performs a linear transformation on the input and generates a 50-dimensional vector as the final output.

The network architecture of the encoder in original Modem is composed of a stack of 6 convolutional layers, each with 32 filters, no padding, stride of 2, 7 \(\times\) 7 kernels for the first one, 5 \(\times\) 5 kernels for the second one and 3 \(\times\) 3 kernels for all others, yielding a final feature map of dimension 2 \(\times\) 2 \(\times\) 32 (inputs whose framestack is 2 have dimension 224 \(\times\) 224 \(\times\) 6). After the convolutional layers, a fully connected layer with an input size of 128 performs a linear transformation on the input and generates a 50-dimensional vector as the final output.

**Test time training setup.** During test time, we train spatial adaptive encoder (SAE) to adapt to view changes. We insert STN blocks before and after the first convolutional layer of the original encoders in TD-MPC and MoDem. The original encoders are augmented by inserting STN blocks, resulting in the formation of SAE. Particularly, for the STN block inserted before the first convolutional layer, the input is a single frame. This means that when the frame stack size is N, N individual frames are fed into this STN block. This is done to apply different transformations to different frames in cases of moving and shaking view.

To update the SAE, we collect online data using a buffer with a size of 256. For each update, we randomly sample 32 (observation, action, next_observation) tuples from the buffer as a batch. The optimization objective is to minimize the loss in predicting the dynamics of the latent states, as defined in Equation 2.

During testing on each task, we run 20 consecutive episodes, although typically only a few or even less than one episode is needed for the test-time training to converge. To make efficient use of the data collected with minimal interactions, we employ a multi-update strategy. After each interaction with the environment, the SAE is updated 32 times.

The following is the network architecture of the first STN block inserted into the encoder of TD-MPC.

STN_Block_0_TDMPC(  (localization): Sequential(  # By default, each image consists of three channels. Each frame in the  \(\rightarrow\) observation is treated as an independent input to the STN.  (0): Conv2d(in_channels=3, out_channels=8, kernel_size=7, stride=1)  (1): MaxPool2d(kernel_size=4, stride=4, padding=0)  (2): ReLU()  (3): Conv2d(in_channels=8, out_channels=10, kernel_size=5, stride=1)  (4): MaxPool2d(kernel_size=4, stride=4, padding=0)  (5): ReLU()  )  (fc_loc): Sequential(  (0): Linear(in_dim=90, out_dim=32)(1): ReLU()  (2): Linear(in_dim=32, out_dim=6)  ) ) The following is the network architecture of the second STN block inserted into the encoder of TD-MPC.

 STN_Block_1_TDMPC(  (localization): Sequential(  (0): Conv2d(in_channels=32, out_channels=8, kernel_size=7, stride=1)  (1): MaxPool2d(kernel_size=3, stride=3, padding=0)  (2): ReLU()  (3): Conv2d(in_channels=8, out_channels=10, kernel_size=5, stride=1)  (4): MaxPool2d(kernel_size=2, stride=2, padding=0)  (5): ReLU()  )  ) (fc_loc): Sequential(  (0): Linear(in_dim=90, out_dim=32)  (1): ReLU()  (2): Linear(in_dim=32, out_dim=6)  ) ) The following is the network architecture of the first STN block inserted into the encoder of MoDem.

 STN_Block_0_MoDem(  (localization): Sequential(  # By default, each image consists of three channels. Each frame in the  -> observation is treated as an independent input to the STN.  (0): Conv2d(in_channels=3, out_channels=5, kernel_size=7, stride=2)  (1): MaxPool2d(kernel_size=4, stride=4, padding=0)  (2): ReLU()  (3): Conv2d(in_channels=5, out_channels=10, kernel_size=5, stride=2)  (4): MaxPool2d(kernel_size=4, stride=4, padding=0)  (5): ReLU()  )  )  (fc_loc): Sequential(  (0): Linear(in_dim=90, out_dim=32)  (1): ReLU()  (2): Linear(in_dim=32, out_dim=6)  ) ) The following is the network architecture of the second STN block inserted into the encoder of MoDem.

 STN_Block_1_MoDem(  (localization): Sequential(  (0): Conv2d(in_channels=32, out_channels=8, kernel_size=7, stride=2)  (1): MaxPool2d(kernel_size=3, stride=3, padding=0)  (2): ReLU()  (3): Conv2d(in_channels=8, out_channels=10, kernel_size=5, stride=2)  (4): MaxPool2d(kernel_size=2, stride=2, padding=0)  (5): ReLU()  )  ) (fc_loc): Sequential(  (0): Linear(in_dim=90, out_dim=32)  (1): ReLU()  (2): Linear(in_dim=32, out_dim=6)  ) )

## Appendix B Environment Details

We categorize the view generalization problem into four distinct settings: novel view, moving view, shaking view, and novel FOV. In this section, we provide descriptions of the implementation details for each setting. The detailed camera settings can be referred to in the code of the environments that we are committed to releasing or in the visualization available on our website yangsizhe.github.io/MoVie.

**Novel view.** In this setting, for locomotion tasks (cheetah-run, walker-stand, walker-walk, and walker-run), the camera always faces the moving agent, while for other tasks, the camera always faces a fixed point in the environment. Therefore, as we change the camera position, the camera orientation also changes accordingly.

**Moving view.** Similar to the previous setting, the camera also always faces the moving agent or a fixed point in the environment. The camera position varies continuously.

**Shaking view.** To simulate camera shake, we applied Gaussian noise to the camera position (XYZ coordinates in meters) at each time step. For DMControl and Adroit, the mean of the distribution is 0, the standard deviation is 0.04, and we constrain the noise within the range of -0.07 to +0.07. For xArm, the mean of the distribution is 0, the standard deviation is 0.4, and we constrain the noise within the range of -0.07 to +0.07.

**Novel FOV.** We experiment with a larger FOV. For DMControl, we modify the FOV from 45 to 53. For xArm, we modify the FOV from 50 to 60. For Adroit, we modify the FOV from 45 to 50. We also experiment with a smaller FOV and results are presented in Appendix F.

## Appendix C Visualization of Feature Maps from Shallow to Deep Layers

We incorporate STN in the first two layers of the visual encoder for all tasks. After visualizing the features of different layers (as shown in Figure 7), we found that the features of shallow layers contain

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Value \\ \hline Discount factor & 0.99 \\ Image size & 84 \(\times\) 84 (TD-MPC) \\  & 224 \(\times\) 224 (MoDem) \\ Frame stack & 3 (TD-MPC) \\  & 2 (MoDem) \\ Action repeat & 1 (xArm) \\  & 2 (Adroit, Finger, and Walker in DMControl) \\  & 4 (otherwise) \\ Data augmentation & \(\pm 4\) pixel image shifts (TD-MPC) \\  & \(\pm 10\) pixel image shifts (MoDem) \\ Seed steps & 5000 \\ Replay buffer size & Unlimited \\ Sampling technique & PER (\(\alpha\) = 0.6, \(\beta\) = 0.4) \\ Planning horizon & 5 \\ Latent dimension & 50 \\ Learning rate & 1e-3 (TD-MPC) \\  & 3e-4 (MoDem) \\ Optimizer (\(\theta\)) & Adam (\(\beta_{1}\) = 0.9, \(\beta_{2}\) = 0.999) \\ Batch size & 256 \\ Number of demos & 5 (MoDem only) \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Hyperparameters for training time.**

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Value \\ \hline Buffer size & 256 \\ Batch size & 32 \\ Multi-update times & 32 \\ Learning rate for encoder & 1e-6 (xArm) \\  & 1e-7 (otherwise) \\ Learning rate for STN blocks & 1e-5 \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Hyperparameters for test time training.**more information about the spatial relationship. Therefore transforming the features of shallow layers for view generalization is reasonable.

## Appendix D Visualization of Feature Map Transformation

We visualize the first layer feature map of the image encoder from TD-MPC and MoVie in Figure 8. It is observed that the feature map from MoVie on the novel view exhibits a closer resemblance to that on the training view.

## Appendix E Extended Description of Baselines

**TD-MPC.** We test the agent trained on training view without any adaptation in the view generalization settings.

**DM.** This is derived from MoVie by removing STN blocks, which just adapts encoder during test time.

**IDM+STN.** This is derived from MoVie by replacing the dynamics model with the inverse dynamics model which predicts the action in between based on the latent states before and after transition. The inverse dynamics model is finetuned together with the encoder and STN blocks during testing.

## Appendix F Ablation on Different FOVs

In our main experiments, we consider the novel FOV as a FOV larger than the original. In Table 12, we present results for both smaller and larger FOV scenarios. Our method demonstrates the successful handling of both cases.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Cheetah-run & TD-MPC & DM & IDM+STN & MoVie \\ \hline Small FOV & \(104.85_{\pm 4.59}\) & \(398.75_{\pm 17.52}\) & \(75.92_{\pm 8.76}\) & \(\mathbf{530.37\pm_{12.84}}\) \\ Large FOV & \(128.55_{\pm 6.57}\) & \(379.01_{\pm 10.90}\) & \(299.02_{\pm 88.47}\) & \(\mathbf{532.94\pm_{19.74}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Ablation on different FOVs.** The best method on each setting is in **bold**.

Figure 8: **Visualization of the first layer feature maps of the original encoder on the training view and on the novel view, and the learned SAE on the novel view.**

Figure 7: **Visualization of features from shallow to deep layers.** The features of shallow layers contain more information about the spatial relationship.

[MISSING_PAGE_FAIL:17]