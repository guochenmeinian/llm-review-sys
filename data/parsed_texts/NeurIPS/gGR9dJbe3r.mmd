# Exponential Quantum Communication Advantage in Distributed Inference and Learning

 Dar Gilboa

Google Quantum AI

Venice, CA, United States &Hagay Michaeli

Technion

Haifa, Israel &Daniel Soudry

Technion

Haifa, Israel &Jarrod R. McClean

Google Quantum AI

Venice, CA, United States

darg@google.com.

Equal contribution.

###### Abstract

Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints. We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest overhead relative to standard gradient-based methods. We show that certain graph neural networks are particularly amenable to implementation within this framework, and moreover present empirical evidence that they perform well on standard benchmarks. To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems that hold regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponentially with model depth. We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification. Communication of quantum states that potentially limit the amount of information that can be extracted from them about the data and model parameters may also lead to improved privacy guarantees for distributed computation. Taken as a whole, these findings form a promising foundation for distributed machine learning over quantum networks.

## 1 Introduction

As the scale of the datasets and parameterized models used to perform computation over data continues to grow [62, 51], distributing workloads across multiple devices becomes essential for enabling progress. The choice of architecture for large-scale training and inference must not only make the best use of computational and memory resources, but also contend with the fact that communication may become a bottleneck [97]. This is particularly pertinent as models grow so large that they cannot rely on high-bandwidth interconnects within datacenters [17], but are instead trained across multiple datacenters [108]. When using modern optical interconnects, classical computers exchange bits represented by light. This however does not fully utilize the potential of the physical substrate; given suitable computational capabilities and algorithms, the _quantum_ nature of light can be harnessed as a powerful communication resource. Here we show that for a broad class of parameterized models, if quantum bits (_qubits_) are communicated instead of classical bits, an exponential reduction in the communication required to perform inference and gradient-based training can be achieved. This protocol additionally guarantees improved privacy of both the user data andmodel parameters through natural features of quantum mechanics, without the need for additional cryptographic or privacy protocols. To our knowledge, this is the first example of generic, exponential quantum advantage on problems that occur naturally in the training and deployment of large machine learning models. These types of communication advantages help scope the future roles and interplay between quantum and classical communication for distributed machine learning.

Quantum computers promise dramatic speedups across a number of computational tasks, with perhaps the most prominent example being the ability revolutionize our understanding of nature by enabling the simulation of quantum systems, owing to the inherently quantum nature of many many physical phenomena [39, 72]. However, much of the data that one would like to compute with in practice seems to come from an emergent classical world rather than directly exhibiting quantum properties. While there are some well-known examples of exponential quantum speedups for classical problems, most famously factoring [106] and related hidden subgroup problems [31], these tend to be isolated and at times difficult to relate to practical applications that involve learning from data. In addition, even though significant speedups are known for certain ubiquitous problems in machine learning such as matrix inversion [48] and principal component analysis [73], the advantage is often lost when including the cost of loading classical data into the quantum computer or of reading out the result into classical memory. This is because the complexity of loading dense classical data into the amplitudes of a quantum state (which is typically the encoding needed to obtain an exponential runtime advantage) and of reading out the amplitudes from a quantum state into classical memory, are both polynomial in the number of amplitudes [1]. In applications where an efficient data access model avoids the above pitfalls, the complexity of quantum algorithms tends to depend on condition numbers of matrices which scale with system size in a way that reduces or even eliminates any quantum advantage [82]. It is worth noting that much of the discussion about the impact of quantum technology on machine learning has focused on computational advantage. However quantum resources are not only useful in reducing computational complexity -- they can also provide an advantage in communication complexity, enabling exponential reductions in communication for some problems [101, 15]. Inspired by these results, we study a setting where quantum advantage in communication is possible across a wide class of machine learning models. This advantage holds without requiring any sparsity assumptions or elaborate data access models such as QRAM [42].

We focus on compositional distributed learning, known as _pipelining_[56, 16]. While there are a number of strategies for distributing machine learning workloads that are influenced by the requirements of different applications and hardware constraints [115, 61], splitting up a computational graph in a compositional fashion (Figure 1) is a common approach. We describe distributed, parameterized quantum circuits that can be used to perform inference over data when distributed in this way, and can be trained using gradient methods. The ideas we present can also be used to optimize models that use certain forms of data parallelism (Appendix C). In principle, such circuits could be implemented on quantum computers that are able to communicate quantum states.

We show the following:

* Even for simple distributed quantum circuits, there is an exponential quantum advantage in communication for the problem of estimating the loss and the gradients of the loss with respect to the parameters (Section 3). This additionally implies a privacy advantage from Holevo's bound (Appendix H). We also show that this is advantage is not a trivial consequence of the data encoding used, since it does not hold for certain problems like linear classification (Appendix E).
* We study a class of models that can efficiently approximate certain graph neural networks (Section 4), and show that they both maintain the exponential communication advantage and achieve performance comparable to standard classical models on common node and graph classification benchmarks (Section 5).
* For certain distributed circuits, there is an exponential advantage in communication for the entire training process, and not just for a single round of gradient estimation. This includes circuits for fine-tuning using pre-trained features. The proof is based on convergence rates for stochastic gradient descent under convexity assumptions (Appendix D).
* The ability to interleave multiple unitaries encoding nonlinear features of data enables expressivity to grow exponentially with depth, and universal function approximation in some settings. This implies that these models are highly expressive in contrast to popular belief about linear restrictions in quantum neural networks (Appendix F).

## 2 Preliminaries

### Large-scale learning problems and distributed computation

Pipelining is a commonly used method of distributing a machine learning workload, in which different layers of a deep model are allocated distinct hardware resources [56; 87]. Training and inference then require communication of features between nodes. Pipelining enables flexible changes to the model architecture in a task-dependent manner, since subsets of a large model can be combined in an adaptive fashion to solve many downstream tasks. Additionally, pipelining allows sparse activation of a subset of a model required to solve a task, and facilitates better use of heterogeneous compute resources since it does not require storing identical copies of a large model. The potential for large models to be easily fine-tuned to solve multiple tasks is well-known [25; 20], and pipelined architectures which facilitate this are the norm in the latest generation of large language models [99; 16]. Data parallelism, in contrast, involves storing multiple copies of the model on different nodes, training each on a subsets of the data and exchanging information to synchronize parameter updates. In practice, different parallelization strategies are combined in order to exploit trade-offs between latency and throughput in a task-dependent fashion [115; 61; 97]. Distributed quantum models were considered recently in [94], but the potential for quantum advantage in communication in these settings was not discussed.

### Communication complexity

Communication complexity [117; 65; 98] is the study of distributed computational problems using a cost model that focuses on the communication required between players rather than the time or computational complexity. The key object of study in this area is the tree induced by a communication protocol whose nodes enumerate all possible communication histories and whose leaves correspond to the outputs of the protocol. The product structure induced on the leaves of this tree as a function of the inputs allows one to bound the depth of the tree from below, which gives an unconditional lower bound on the communication complexity. The power of replacing classical bits of communication with qubits has been the subject of extensive study [30; 23; 27]. For certain problems such as Hidden Matching [15] and a variant of classification with deep linear models [101] an exponential quantum communication advantage holds, while for other canonical problems such as Disjointness only a polynomial advantage is possible [102]. Exponential advantage was also recently shown for the problem of sampling from a distribution defined by the solution to a linear regression problem

Figure 1: _Left:_ Distributed, compositional computation. Dashed lines separate devices with computational and storage resources. The circular nodes represent parameterized functions that are allocated distinct hardware resources and are spatially separated, while the square nodes represent data (yellow) and outputs corresponding to different tasks (green). The vertical axis represents time. This framework of hardware allocation enables flexible modification of the model structure in a task-dependent fashion. _Right:_ Computation of gradient estimators \(g_{\ell}\) at different layers of a model distributed across multiple devices by pipelining. Computing forward features \(\mu_{\ell}\) and backwards features \(\nu_{\ell}\) (also known as computing a forward or backward pass) requires a large amount of classical communication (grey) but an exponentially smaller amount of quantum communication (yellow). \(\mathcal{L}\) is the classical loss function, and \(\mathcal{P}_{0}\) an operator whose expectation value with respect to a quantum model gives the analogous loss function in the quantum case.

[83]. While there are many models of both quantum and classical communication, our results apply to _randomized_ classical communication complexity, wherein the players are allowed to exchange random bits independent of their problem inputs, and are allowed to output an incorrect answer with some probability (bounded away from \(1/2\) for a problem with binary output). It is also worth noting that communication advantages of the type we demonstrate can be naturally related to space advantages in streaming algorithms that may be of interest even in settings that do not involve distributed training [103].

At a glance, the development of networked quantum computers may seem much more challenging than the already herculean task of building a fault tolerant quantum computer. However, for some quantum network architectures, the existence of a long-lasting fault tolerant quantum memory as a quantum repeater, may be the enabling component that lifts low rate shared entanglement to a fully functional quantum network [86], and hence the timelines for small fault tolerant quantum computers and quantum networks may be more coincident than it might seem at first. As such, it is well motivated to consider potential communication advantages alongside computational advantages when talking about the applications of fault tolerant quantum computers. In Appendix G we briefly survey approaches to implementing quantum communication in practice, and the associated challenges.

In addition, while we largely restrict ourselves here to discussions of communication advantages, and most other studies focus on purely computational advantages, there may be interesting advantages at their intersection. For example, it is known that no quantum state built from a simple (or polynomial complexity) circuit can confer an exponential communication advantage, however states made from simple circuits can be made computationally difficult to distinguish [59]. Hence the use of quantum pre-computation [57] and communication may confer advantages even when traditional computational and communication cost models do not admit such advantages due to their restriction in scope.

## 3 Distributed learning with quantum resources

In this work we focus on parameterized models that are representative of the most common models used and studied today in quantum machine learning, sometimes referred to as quantum neural networks [79; 38; 28; 104]. We will use the standard Dirac notation of quantum mechanics throughout. A summary of relevant notation and the fundamentals of quantum mechanics is provided in Appendix A. We define a class models with parameters \(\Theta\), taking an input \(x\) which is a tensor of size \(N\). The models take the following general form:

**Definition 3.1**.: \(\{A_{\ell}(\theta^{A}_{\ell},x)\},\{B_{\ell}(\theta^{B}_{\ell},x)\}\) _for \(\ell\in\{1,\ldots,L\}\) are each a set of unitary matrices of size \(N^{\prime}\times N^{\prime}\) for some \(N^{\prime}\) such that \(\log N^{\prime}=O(\log N)\)3. The \(\theta^{A}_{\ell},\theta^{B}_{\ell}\) are vectors of \(P\) parameters each. For every \(\ell,i\), we assume that \(\frac{\partial A_{\ell}}{\partial\theta^{A}_{\ell}}\) is anti-hermitian and has two eigenvalues, and similarly for \(B_{\ell}\)4._

Footnote 3: We will consider some cases where \(N^{\prime}=N\), but will find it helpful at times to encode nonlinear features of \(x\) in these unitaries, in which case we may have \(N^{\prime}>N\).

Footnote 4: The condition on the derivatives is in fact satisfied by many of the most common quantum neural network architectures [28; 34; 104]. It is satisfied for example if \(A_{\ell}=\prod_{j=1}^{P}e^{i\alpha^{A}_{\ell j}\theta^{A}_{\ell j}\mathcal{P}^ {A}_{\ell j}}\) and the \(\mathcal{P}^{A}_{\ell j}\) are Pauli matrices, while \(\alpha^{A}_{\ell j}\) are scalars. Such models are naturally amenable to implementation on quantum devices, and for \(P=\tilde{O}(N^{2})\) any unitary over \(\log N^{\prime}\) qubits can be written in this form [90].

_The model we consider is defined by_

\[\ket{\varphi(\Theta,x)}\equiv\left(\prod_{\ell=L}^{1}A_{\ell}(\theta^{A}_{ \ell},x)B_{\ell}(\theta^{B}_{\ell},x)\right)\ket{\psi(x)},\] (3.1)

_where \(\psi(x)\) is a fixed state of \(\log N^{\prime}\) qubits._

_The loss function is given by_

\[\mathcal{L}(\Theta,x)\equiv\bra{\varphi(\Theta,x)}\mathcal{P}_{0}\ket{\varphi (\Theta,x)},\] (3.2)

_where \(\mathcal{P}_{0}\) is a Pauli matrix that acts on the first qubit._

In standard linear algebra notation, the output of the model is a unit norm \(N^{\prime}\)-dimensional complex vector \(\varphi_{L}\), defined recursively by

\[\varphi_{0}=\psi(x),\quad\varphi_{\ell}=A_{\ell}(\theta^{A}_{\ell},x)B_{\ell} (\theta^{B}_{\ell},x)\varphi_{\ell-1},\] (3.3)where the entries of \(\varphi_{L}\) are represented by the amplitudes of a quantum state. The loss takes the form \(\mathcal{L}(\Theta,x)=\left(\varphi_{L}^{\ast}\right)^{T}\mathcal{P}_{0}\varphi_ {L}\) where \({}^{\ast}\) indicates the entrywise complex conjugate, and this definition includes the standard \(L^{2}\) loss as a special case.

Subsequently we omit the dependence on \(x\) and \(\Theta\) (or subsets of it) to lighten notation, and consider special cases where only subsets of the unitaries depend on \(x\), or where the unitaries take a particular form and may not be parameterized. Denote by \(\nabla_{A(B)}\mathcal{L}\) the entries of the gradient vector that correspond to the parameters of \(\{A_{\ell}\}(\{B_{\ell}\})\).

In the special case where \(x\) in a unit norm \(N\)-dimensional vector, a simple choice of \(\left|\psi(x)\right\rangle\) is the amplitude encoding of \(x\), given by

\[\left|\psi(x)\right\rangle=\left|x\right\rangle=\sum_{i=0}^{N-1}x_{i}\left|i \right\rangle.\] (3.4)

However, despite its exponential compactness in representing the data, a naive implementation of the simplest choice is restricted to representing quadratic features of the data that can offer no substantial quantum advantage in a learning task [55], so the choice of data encoding is critical to the power of a model. The interesting parameter regime for classical data and models is one where \(N,P\) are large, while \(L\) is relatively modest. For general unitaries \(P=O(N^{2})\), which matches the scaling of the number of parameters in fully-connected networks. When the input tensor \(x\) is a batch of datapoints, \(N\) is equivalent to the product of batch size and input dimension.

The model in Definition 3.1 can be used to define distributed inference and learning problems by dividing the input \(x\) and the parameterized unitaries between two players, Alice and Bob. We define their respective inputs as follows:

\[\begin{split}\mathrm{Alice}:&\quad\left|\psi(x) \right\rangle,\{A_{\ell}\},\\ \mathrm{Bob}:&\quad\{B_{\ell}\}.\end{split}\] (3.5)

The problems of interest require that Alice and Bob compute certain joint functions of their inputs. As a trivial base case, it is clear that in a communication cost model, all problems can be solved with communication cost at most the size of the inputs times the number of parties, by a protocol in which each party sends its inputs to all others. We will be interested in cases where one can do much better by taking advantage of quantum communication.

Given the inputs eq. (3.5), we will be interested chiefly in the two problems specified below.

**Problem 1** (Distributed Inference).: _Alice and Bob each compute an estimate of \(\left\langle\varphi\right|\mathcal{P}_{0}\left|\varphi\right\rangle\) up to additive error \(\varepsilon\)._

The straightforward algorithm for this problem, illustrated in fig. 2, requires \(L\) rounds of communication. The other problem we consider is the following:

**Problem 2** (Distributed Gradient Estimation).: _Alice computes an estimate of \(\nabla_{A}\left\langle\varphi\right|\mathcal{P}_{0}\left|\varphi\right\rangle\), while Bob computes an estimate of \(\nabla_{B}\left\langle\varphi\right|Z_{0}\left|\varphi\right\rangle\), up to additive error \(\varepsilon\) in \(L^{\infty}\)._

### Communication complexity of inference and gradient estimation

We show that inference and gradient estimation are achievable with a logarithmic amount of quantum communication, which will represent an exponential improvement over the classical cost for some cases:

Figure 2: Distributed quantum circuit implementing \(\mathcal{L}\) for \(L=2\). Both \(\mathcal{L}\) and its gradients with respect to the parameters of the unitaries can be estimated with total communication that is polylogarithmic in the size of the input data \(N\) and the number of trainable parameters per unitary \(P\).

**Lemma 1**.: _Problem 1 can be solved by communicating \(O(\log N)\) qubits over \(O(L/\varepsilon^{2})\) rounds._

Proof: Appendix B.

**Lemma 2**.: _Problem 2 can be solved with probability greater than \(1-\delta\) by communicating \(\tilde{O}(\log N(\log P)^{2}\log(L/\delta)/\varepsilon^{4})\) qubits over \(O(L^{2})\) rounds. The time and space complexity of the algorithm is \(\sqrt{P}\ L\ \mathrm{poly}(N,\log P,\varepsilon^{-1},\log(1/\delta))\)._

Proof: Appendix B.

This upper bound is obtained by simply noting that the problem of gradient estimation at every layer can be reduced to a shadow tomography problem [7]:

**Theorem 1** (Shadow Tomography [3] solved with Threshold Search [13]).: _For an unknown state \(\ket{\psi}\) of \(\log N\) qubits, given \(K\) known two-outcome measurements \(E_{i}\), there is an explicit algorithm that takes \(\ket{\psi}^{\otimes k}\) as input, where \(k=\tilde{O}(\log^{2}K\log N\log(1/\delta)/\varepsilon^{4})\), and produces estimates of \(\bra{\psi}E_{i}\ket{\psi}\) for all \(i\) up to additive error \(\varepsilon\) with probability greater than \(1-\delta\). \(\tilde{O}\) hides subdominant polylog factors._

Using reductions from known problems in communication complexity, we can show that the amount of classical communication required to solve this problem is polynomial in the size of the input, and additionally give a lower bound on the number of rounds of communication required by any quantum or classical algorithm:

**Lemma 3**.:
1. _The classical randomized communication complexity of Problem_ 1 _and Problem_ 2 _with_ \(\varepsilon<1/2\) _is_ \(\Omega(\max(\sqrt{N},L))\)_._ 5__ Footnote 5: The inputs to Problem 1 and Problem 2 are defined in terms of real numbers, which is seemingly incompatible with the setting of communication complexity which typically deals with finite inputs. However, similar (but slightly worse) lower bounds hold for discretized analogs of these problem that use \(O(\log N)\) bits to represent the real numbers [101].
2. _Any algorithm (quantum or classical) for Problem_ 1 _or Problem_ 2 _requires either_ \(\Omega(L)\) _rounds of communication or_ \(\Omega(N/L^{4})\) _qubits (or bits) of communication._

Proof: Appendix B

The implication of the second result in Lemma 3 is that \(\Omega(L)\) rounds of communication are necessary in order to obtain an exponential communication advantage for small \(L\), since otherwise the number of qubits of communication required can scale linearly with \(N\).

The combination of Lemma 1, Lemma 2 and Lemma 3 immediately implies exponential savings in communication for gradient estimation and inference:

**Theorem 2**.: _If \(L=O(\mathrm{polylog}(N)),P=O(\mathrm{poly}(N))\) and sufficiently large \(N\), solving Problem 1 or Problem 2 with nontrivial success probability requires \(\Omega(\sqrt{N})\) bits of classical communication, while \(O(\mathrm{polylog}(N,1/\delta)\mathrm{poly}(1/\varepsilon))\) qubits of communication suffice to solve these problems with probability at least \(1-\delta\)._

The regime where \(L=O(\mathrm{polylog}(N))\) is relevant for many classes of machine learning models. The required overhead in terms of time and space is only polynomial when compared to the straightforward classical algorithms for these problems.

The distribution of the model as in eq. (3.5) is an example of pipelining. Data parallelism is another common approach to distributed machine learning in which subsets of the data are distributed to identical copies of the model. In Appendix C we show that it can also be implemented using quantum circuits, which can then trained using gradient descent requiring quantum communication that is logarithmic in the number of parameters and input size.

Quantum advantage is possible in these problems because there is a bound on the complexity of the final output, whether it be correlated elements of the gradient up to some finite error or the low-dimensional output of a model. This might lead one to believe that whenever the output takes such a form, encoding the data in the amplitudes of a quantum state will trivially give an exponential advantage in communication complexity. We show however that the situation is slightly more nuanced, by considering the problem of inference with a linear model:

**Lemma 4**.: _For the problem of distributed linear classification, there can be no exponential advantage in using quantum communication in place of classical communication._

The precise statement and proof of this result are presented in Appendix E. This result also highlights that the worst case lower bounds such as Lemma 3 may not hold for circuits with certain low-dimensional or other simplifying structure.

## 4 Graph neural networks

The communication advantages in the previous section apply to relatively unstructured data and quantum circuits (essentially the only structure in the problem is related to the promise of the vector-in-subspace problem [101]), and it is a priori unclear how relevant they are to circuits that approximate useful neural networks, or act on structured data. Here we consider a class of shallow graph neural networks that achieve good performance on node classification tasks on large graphs [40]. We prove that an exponential quantum communication advantage still holds for this class of models.

Consider a graph with \(N\) nodes. Define a local message passing operator on the graph \(A\) which may be the normalized Laplacian or some other operator. Given some \(N\times D_{0}\) matrix of graph features \(X\), we consider models of the following form:

\[\Phi(X)=\mathcal{P}\left(\sigma(AXW_{1})\right)W_{2}\] (4.1)

where \(W_{1}\in\mathbb{R}^{D_{0}\times D_{1}},W_{2}\in\mathbb{R}^{D_{1}\times D_{2}}\) are parameter matrices, \(\sigma\) is a non-linearity and \(\mathcal{P}\) is a sum pooling operator acting on the first index of its input, which can be represented as multiplication by an \(N/t\times N\) matrix \(\tilde{P}\). Since we would like a scalar output and a nonlinearity that can be implemented on a quantum computer, we instead compute

\[\varphi(X)=\mathrm{tr}\left[\mathcal{P}\left(\sigma(AXW_{1})\right)W_{2}\right],\] (4.2)

with

\[\sigma(x)=ax^{2}+bx+c\] (4.3)

and \(W_{2}\in\mathbb{R}^{D_{1}\times N/t}\) where \(t\) is the size of the pooling window.

This allows us to define the following problem:

**Problem 3** (Quadratic graph network inference (\(\mathrm{QGNI}_{N,t}\))).: _Alice is given \(X\), Bob is given \(A,W_{1},W_{2}\). Only Alice is allowed to send messages. Their goal is to estimate \(\varphi(X/||X||_{F})\) to additive error \(\varepsilon\)._

This models a scenario where only Bob has access to the connectivity of the graph, while Alice has access to the graph features. The normalization ensures that the choice of \(X\) does not introduce a dependence of the final output on \(N\).

In the following, we denote by \(R_{\varepsilon}^{\rightarrow}\) and \(Q_{\varepsilon}^{\rightarrow}\) the classical (public key randomized) communication complexity and quantum communication complexity respectively. We show:

**Lemma 5**.: \(R_{\varepsilon}^{\rightarrow}(\mathrm{QGNI}_{N,t})=\Omega(\sqrt{N/t})\) _for any \(\varepsilon\leq\frac{1}{4\left(t+\frac{1}{2}\right)\left(t+\frac{3}{2}\right)}\)._

Proof: Appendix B

**Lemma 6**.: \(Q_{\varepsilon}^{\rightarrow}(\mathrm{QGNI}_{N,t})=O((|a|\,\alpha^{2}+|b|\, \alpha)\left\|W_{2}\tilde{P}\right\|_{\infty}\log(ND_{0})/\varepsilon)\) _where \(\alpha=\left\|W_{1}\right\|\,\left\|A\right\|\)._

Proof: Appendix B

If this upper bound was a polynomial function of \(N\), it would imply that an exponential communication advantage is impossible. For the parameter choices that realize classical communication lower bound, this is not the case, implying the following:

**Lemma 7**.: _An exponential quantum advantage in communication holds for solving the inference problem \(\mathrm{QGNI}_{N,t}\) up to error \(\varepsilon\leq\frac{1}{4\left(t+\frac{1}{2}\right)\left(t+\frac{3}{2}\right)}\), for any \(t\) such that \(t=\mathrm{polylog}(N)\)._Proof: Appendix B. Note that this exponential advantage does not hold only for a single setting of the model weights, but rather for the entire family of models that can be used to solve \(f-\mathrm{BHP}_{N,t}\) for functions \(f\) that satisfy eq. (B.24).

Note that generically, one would not expect the numerator in the upper bound of Lemma 6 to scale polynomially with \(N\). If \(A\) is for example a normalized graph Laplacian then \(||A||\leq 2\). If we use a standard initialization scheme for the weights (say Gaussians with variance \(1/(n_{in}+n_{out})\)), the upper bound scales like \(O((|a|+|b|)\log(ND_{0})\mathrm{poly}(t,D_{0},D_{1})/\varepsilon)\) in expectation. Note that if the model output decays polynomially with \(N\), the upper bound will not be useful since one would need to choose \(\varepsilon\) to be inverse polynomial in \(N\). This could happen for example in a classification task considered in Section 5.2.1 when the classes are exactly balanced, or when the network is untrained and not sensitive to the structure in the data. While it is difficult to argue analytically about the scaling out the network output or the norms of the weight matrices after training due to the nonlinearity of the dynamics, we empirically compute these and find that they remain controlled for the datasets we study (see Appendix J.3).

In Section 5, we show that models of the form studied here achieve good performance on standard benchmarks, commensurate with state of the art models. Of particular relevance are the graph classification problems considered in Section 5.2.1, where the output takes the form eq. (4.2).

## 5 Experimental results

### Model

We evaluate our model6 (as defined in Equation (4.1)) on several graph tasks using common benchmarks and the DGL library [113]. We use the SIGN model proposed by [40] as a baseline. The SIGN model can be seen as an instance of our model where the message passing operator \(A\) represents a column stack of \(R\) hops, the original features of \(X\) are duplicated \(R\) times and \(W_{1}\) is a block diagonal matrix. In Section 5.2 we simply replace the PReLU activation function with a second-degree polynomial with trainable coefficients [80] and compare the models on three node classification tasks. In Section 5.3, we implement a more general form of SIGN by relaxing \(W_{1}\) to be a dense matrix and evaluate our model over several graph-classification datasets.

Footnote 6: Our code will be available at github.com/hmichaeli/quantum_gnns/.

### Node classification

We evaluate our model on three public node classification datasets: OGBN-Products [54], Reddit [47], and Cora [78]. For both the baseline and polynomial model, we use SIGN with 5 hops of the neighbor averaging operator. We train on each dataset for 1000 epochs using Adam optimizer and report the test accuracy averaged on 10 runs (full details in Appendix J). Our results in Table 3 show that replacing the PReLU activation function with a second-degree polynomial causes a reduction of less than 1% on all of the tested datasets.

#### 5.2.1 Decision problems

We reduce the node classification task into a binary graph classification task by proposing the following decision problem: for a pair of classes \((c_{1},c_{2})\), return 1 if \(c_{1}\) has more nodes; otherwise, return 0. We solve this task for each pair of classes by summing the node classification model output across all nodes and choosing the class with the higher score. We use the _node_ classification training,

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Node Classification} & \multicolumn{3}{c}{Decision Problem} \\ \cline{2-7} Model & OGBN-Products & Reddit & Cora & OGBN-Products & Reddit & Cora \\ \hline SIGN (PReLU) & 79.48 \(\pm\) 0.07 & 96.55 \(\pm\) 0.02 & 78.84 \(\pm\) 0.37 & 84.39 \(\pm\) 1.73 & 90.33 \(\pm\) 0.33 & 88.10 \(\pm\) 5.61 \\ SIGN (Poly) & 78.51 \(\pm\) 0.05 & 96.31 \(\pm\) 0.03 & 78.69 \(\pm\) 0.26 & 83.70 \(\pm\) 1.48 & 89.37 \(\pm\) 0.60 & 87.14 \(\pm\) 3.92 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test Accuracy for Node Classification and Decision Problem. Replacing PReLU with a polynomial of degree 2 causes a slight reduction in accuracy (less than 1%) for both node classification and decision problem across all datasets.

choose the model with the highest validation accuracy on the _graph_ classification task, and report its accuracy on the test sets. The model output in this form is given by eq. (4.1).

### Graph classification

We evaluate our model on several graph classification benchmarks: bioinformatics datasets (MUTAG, PTC, NCI1, PROTEINS) [105; 50; 35; 21] and social networks (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI) [116]. For the bioinformatics datasets, we use the standard categorical node features. As proposed in [114], we use one-hot encoding of the node degree as node features for the COLLAB and IMDB datasets, and for REDDIT datasets all nodes are set with an identical scalar feature of 1. We convert the polynomial SIGN model in Section 5.2 into a graph classification model by inserting a SumPool operator as described in Equation (4.1). We use the sign diffusion operator [113] and stack \(R_{i}\) instances of each of its four message passing operators, where \(\{R_{i}\}_{i=1}^{4}\) are selected during a hyperparameter tuning, as well as the hidden dimension size and optimization setting (see Appendix J for more details). We follow the validation regime proposed by [114]; we perform 10-fold cross-validation, train each fold for 350 epochs using Adam optimizer, and report in Table 2 the maximal value and standard-deviation of the averaged validation accuracy curve. For all datasets, except for REDDIT, our model achieves comparable to or better than other commonly used models, despite most of them using multiple layers. While the results show that on most datasets our shallow architecture suffices given sufficient width in the message passing and hidden layer, we hypothesize that datasets without any node features (such as REDDIT) require at least two layers of message passing.

## 6 Discussion

This work constitutes a preliminary investigation into a generic class of quantum circuits that has the potential for enabling an exponential communication advantage in problems of classical data processing including training and inference with large parameterized models over large datasets, with inherent privacy advantages. Communication constraints may become even more relevant if such models are trained on data that is obtained by inherently distributed interaction with the physical world [37]. The ability to compute using data with privacy guarantees can be potentially applied to proprietary data. This could become highly desirable even in the near future as the rate of publicly-available data production appears to be outstripped by the growth rate of training sets of large language models [110].

A limitation of the current results is that it's unclear to what extent powerful neural networks can be approximated using quantum circuits, even though we provide positive evidence in the form of the results on graph networks in Section 4. Additionally, the advantages we study require deep (\(\mathrm{poly}(N)\)), fault-tolerant quantum circuits. While this is a common feature of problems for which quantum communication advantages hold, the overhead of quantum error-correction in such circuits may be considerable. Detailed resource estimates would be necessary to understand better the practicality of this approach for achieving useful quantum advantage. Our results naturally raise further questions regarding the expressive power and trainability of these types of circuits, which may be of independent interest. We collect some of these in Appendix I.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{6}{c}{Dataset} \\ \cline{2-9} Model & MUTAG & PTC & NCI1 & PROTEINS & COLLAB & IMDB-M & REDDIT-M \\ \hline GIN [114] & 89.40\(\pm\)5.60 & 64.60\(\pm\)7.0 & 82.17\(\pm\)1.7 & 76.2 \(\pm\)2.8 & 80.2 \(\pm\)1.90 & 52.3 \(\pm\)2.8 & 57.5\(\pm\)1.5 \\ DropGIN[92] & 90.4 \(\pm\)7.0 & 66.3 \(\pm\)8.6 & - & 76.3 \(\pm\)6.1 & - & 51.4 \(\pm\)2.8 & - \\ DGCNN[118] & 85.8 \(\pm\)1.7 & 58.6 \(\pm\)2.5 & - & 75.5 \(\pm\)0.9 & - & 47.8 \(\pm\)0.9 & - \\ U2GNN [89] & 89.97\(\pm\)3.65 & 69.63\(\pm\)3.60 & - & 78.53\(\pm\)4.07 & 77.84\(\pm\)1.48 & 53.60\(\pm\)3.53 & - \\ HGP-SL[119] & - & - & 78.45\(\pm\)0.77 & 84.91\(\pm\)1.62 & - & - & - \\ WKPI[120] & 88.30\(\pm\)2.6 & 68.10\(\pm\)2.4 & 87.5 \(\pm\)0.5 & 78.5\(\pm\)0.4 & - & 49.5 \(\pm\) 0.4 & 59.5 \(\pm\) 0.6 \\ \hline SIGN (ours) & 92.02\(\pm\)6.45 & 68.0 \(\pm\)8.17 & 77.25\(\pm\)1.42 & 76.55\(\pm\)5.10 & 81.82\(\pm\)1.42 & 53.13\(\pm\)3.01 & 54.09\(\pm\)1.76 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Graph Classification Test Accuracy. Our model achieves comparable results to GIN and other known models on most datasets (see full table in Table 5).

## Acknowledgements

The authors would like to thank Amira Abbas, Ryan Babbush, Dave Bacon and Robbie King for helpful discussions and comments on the manuscript. The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI.

## References

* [1] Scott Aaronson. Read the fine print. _Nat. Phys._, 11(4):291-293, 2015.
* [2] Scott Aaronson. Introduction to quantum information science. https://www.scottaaronson.com/qclec.pdf, 2017.
* [3] Scott Aaronson. Shadow tomography of quantum states. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 325-338, New York, NY, USA, 2018. ACM.
* [4] Scott Aaronson, Andris Ambainis, Janis Iraids, Martins Kokainis, and Juris Smotrovs. Polynomials, quantum query complexity, and grothendieck's inequality. In _Proceedings of the 31st Conference on Computational Complexity_, number Article 25 in CCC '16, pages 1-19, Dagstuhl, DEU, 2016. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
* [5] Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak. Online learning of quantum states. _J. Stat. Mech._, 2019(12):124019, 2019.
* [6] Scott Aaronson and Guy N Rothblum. Gentle measurement of quantum states and differential privacy. In _STOC'19--Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing_, pages 322-333. ACM, New York, 2019.
* [7] Amira Abbas, Robbie King, Hsin-Yuan Huang, W Huggins, R Movassagh, D Gilboa, and J McClean. On quantum backpropagation, information reuse, and cheating measurement collapse. _Neural Inf Process Syst_, abs/2305.13362:44792-44819, 2023.
* [8] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary coins. _J. Comput. System Sci._, 66(4):671-687, 2003.
* [9] Alekh Agarwal, Peter L Bartlett, Pradeep Ravikumar, and Martin J Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. _arXiv [stat.ML]_, 2010.
* [10] Srinivasan Arunachalam, Uma Girish, and Noam Lifshitz. One clean qubit suffices for quantum communication advantage. _arXiv [quant-ph]_, 2023.
* [11] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando G S L Brandao, David A Buell, Brian Burkett, Yu Chen, Zijun Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Edward Farhi, Brooks Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve Habegger, Matthew P Harrigan, Michael J Hartmann, Alan Ho, Markus Hoffmann, Trent Huang, Travis S Humble, Sergei V Isakov, Evan Jeffrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul V Klimov, Sergey Knysh, Alexander Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore Mandra, Jarrod R McClean, Matthew McKen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C Platt, Chris Quintana, Eleanor G Rieffel, Pedram Roushan, Nicholas C Rubin, Daniel Sank, Kevin J Satzinger, Vadim Smelyanskiy, Kevin J Sung, Matthew D Trevithick, Amit Vainsencher, Benjamin Villalonga, Theodore White, Z Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven, and John M Martinis. Quantum supremacy using a programmable superconducting processor. _Nature_, 574(7779):505-510, 2019.
* [12] Koji Azuma, Sophia E Economou, David Elkouss, Paul Hilaire, Liang Jiang, Hoi-Kwong Lo, and Ilan Tzitrin. Quantum repeaters: From quantum networks to the quantum internet. _arXiv [quant-ph]_, 2022.
* [13] Costin Badescu and Ryan O'Donnell. Improved quantum data analysis. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 1398-1411, 2021.
* [14] Krishna C Balram and Kartik Srinivasan. Piezoelectric optomechanical approaches for efficient quantum microwave-to-optical signal transduction: the need for co-design. _arXiv [physics.optics]_, 2021.

* [15] Ziv Bar-Yossef, T S Jayram, and Iordanis Kerenidis. Exponential separation of quantum and classical one-way communication complexity. _SIAM J. Comput._, 38(1):366-384, 2008.
* [16] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A Thekkath, and Yonghui Wu. Pathways: Asynchronous distributed dataflow for ML. _arXiv [cs.DC]_, 2022.
* [17] Luiz Andre Barroso, Jimmy Clidaras, and Urs Holzle. The datacenter as a computer: An introduction to the design of warehouse-scale machines, second edition. _Synth. Lect. Comput. Archit._, 8(3):1-154, 2013.
* [18] C H Bennett, G Brassard, C Crepeau, R Jozsa, A Peres, and W K Wootters. Teleporting an unknown quantum state via dual classical and einstein-podolsky-rosen channels. _Phys. Rev. Lett._, 70(13):1895-1899, 1993.
* [19] Charles H Bennett, Gilles Brassard, Sandu Popescu, Benjamin Schumacher, John A Smolin, and William K Wootters. Purification of noisy entanglement and faithful teleportation via noisy channels. _arXiv [quant-ph]_, 1995.
* [20] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Dounbouya, Esin Durmus, Stefano Ermon, John Etchemardy, Kavin Ethayarabi, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Freeshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W Thomas, Florian Tramer, Rose E Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. _arXiv [cs.LG]_, 2021.
* [21] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(suppl_1):i47-i56, 2005.
* [22] Fernando G S Brandao, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M Svore, and Xiaodi Wu. Quantum SDP solvers: Large speed-ups, optimality, and applications to quantum learning. _arXiv [quant-ph]_, 2017.
* [23] Gilles Brassard. Quantum communication complexity (a survey). _arXiv [quant-ph]_, 2001.
* [24] Adam R Brown and Leonard Susskind. The second law of quantum complexity. _arXiv [hep-th]_, 2017.
* [25] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33:1877-1901, 2020.

* [26] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _arXiv [math.OC]_, 2014.
* [27] Harry Buhrman, Richard Cleve, Serge Massar, and Ronald de Wolf. Non-locality and communication complexity. _arXiv [quant-ph]_, 2009.
* [28] M Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C Benjamin, Suguru Endo, Keisuke Fujii, Jarrod R McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick J Coles. Variational quantum algorithms. _arXiv [quant-ph]_, 2020.
* [29] Shantanav Chakraborty, Andras Gilyen, and Stacey Jeffery. The power of block-encoded matrix powers: improved regression techniques via faster hamiltonian simulation. _arXiv [quant-ph]_, 2018.
* [30] A Chi-Chih Yao. Quantum circuit complexity. In _Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science_, pages 352-361, 1993.
* [31] Andrew M Childs and Wim van Dam. Quantum algorithms for algebraic problems. _arXiv [quant-ph]_, 2008.
* [32] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. _arXiv [cs.NE]_, 2015.
* [33] Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling geometry. _arXiv [cs.NE]_, 2016.
* [34] Gavin E Crooks. Gradients of parameterized quantum gates using the parameter-shift rule and gate decomposition. _arXiv [quant-ph]_, 2019.
* [35] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991.
* [36] Joao F Doriguello and Ashley Montanaro. Exponential quantum communication reductions from generalizations of the boolean hidden matching problem. _arXiv [quant-ph]_, 2020.
* [37] Danny Driess, Fei Xia, Mehdi S M Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An embodied multimodal language model. _arXiv [cs.LG]_, 2023.
* [38] Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term processors. _arXiv [quant-ph]_, 2018.
* [39] Richard P Feynman. Simulating physics with computers. _Int. J. Theor. Phys._, 21(6):467-488, 1982.
* [40] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and Federico Monti. SIGN: Scalable inception graph neural networks. _arXiv [cs.LG]_, 2020.
* [41] Andras Gilyen, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. _arXiv [quant-ph]_, 2018.
* [42] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory. _Phys. Rev. Lett._, 100(16):160501, 2008.
* [43] Lukas Gonon and Antoine Jacquier. Universal approximation theorem and error bounds for quantum neural networks and quantum reservoirs. _arXiv [quant-ph]_, 2023.
* [44] Google Quantum AI. Suppressing quantum errors by scaling a surface code logical qubit. _Nature_, 614(7949):676-681, 2023.

* [45] Goren Gordon and Gustav Rigolin. Generalized teleportation protocol. _arXiv [quant-ph]_, 2005.
* [46] Robert Gower, Donald Goldfarb, and Peter Richtarik. Stochastic block BFGS: Squeezing more curvature out of data. In Maria Florina Balcan and Kilian Q Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 1869-1878, New York, New York, USA, 2016. PMLR.
* [47] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs, 2018.
* [48] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. _Phys. Rev. Lett._, 103(15):150502, 2009.
* [49] Aram W Harrow and John C Napp. Low-depth gradient measurements can improve convergence in variational hybrid quantum-classical algorithms. _Phys. Rev. Lett._, 126(14):140502, 2021.
* [50] Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. The predictive toxicology challenge 2000-2001. _Bioinformatics_, 17(1):107-108, 2001.
* [51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. _arXiv [cs.CL]_, 2022.
* [52] Alexander Semenovich Holevo. Bounds for the quantity of information transmitted by a quantum communication channel. _Problemy Peredachi Informatsii_, 1973.
* [53] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. _arXiv [cs.CL]_, 2018.
* [54] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs, 2021.
* [55] Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R McClean. Power of data in quantum machine learning. _Nat. Commun._, 12(1):2631, 2021.
* [56] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyoukjoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. _arXiv [cs.CV]_, 2018.
* [57] William J Huggins and Jarrod R McClean. Accelerating quantum algorithms with precomputation. _arXiv [quant-ph]_, 2023.
* [58] Rahul Jain, Jaikumar Radhakrishnan, and Pranab Sen. The quantum communication complexity of the pointer chasing problem: The bit version. In _FST TCS 2002: Foundations of Software Technology and Theoretical Computer Science_, Lecture notes in computer science, pages 218-229. Springer Berlin Heidelberg, Berlin, Heidelberg, 2002.
* [59] Zhengfeng Ji, Yi-Kai Liu, and Fang Song. Pseudorandom quantum states. In _Lecture Notes in Computer Science_, Lecture notes in computer science, pages 126-152. Springer International Publishing, Cham, 2018.
* [60] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In _NeurIPS_, 2013.
* [61] Norman P Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Svinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson. TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. _arXiv [cs.AR]_, 2023.

* [62] J Kaplan, S McCandlish, T Henighan, T B Brown, and others. Scaling laws for neural language models. _arXiv preprint arXiv_, 2020.
* [63] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. _arXiv [cs.LG]_, 2020.
* [64] V Krutyanskiy, M Galli, V Krcmarsky, S Baier, D A Fioretto, Y Pu, A Mazloom, P Sekatski, M Canteri, M Teller, J Schupp, J Bate, M Meraner, N Sangouard, B P Lanyon, and T E Northup. Entanglement of trapped-ion qubits separated by 230 meters. _arXiv [quant-ph]_, 2022.
* [65] Eyal Kushilevitz and Noam Nisan. _Communication Complexity_. Cambridge University Press, Cambridge, England, 2011.
* [66] Nikolai Lauk, Neil Sinclair, Shabir Barzanjeh, Jacob P Covey, Mark Saffman, Maria Spiropulu, and Christoph Simon. Perspectives on quantum transduction. _Quantum Sci. Technol._, 5(2):020501, 2020.
* [67] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. _arXiv [math.OC]_, 2012.
* [68] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with fourier transforms. _arXiv [cs.CL]_, 2021.
* [69] Yoav Levine, Or Sharir, Alon Ziv, and Amnon Shashua. On the long-term memory of deep recurrent networks. _arXiv [cs.LG]_, 2017.
* [70] Yoav Levine, Noam Wies, Or Sharir, Hoft Bata, and Amnon Shashua. The depth-to-width interplay in self-attention. _arXiv [cs.LG]_, 2020.
* [71] Bo Li, Yuan Cao, Yu-Huai Li, Wen-Qi Cai, Wei-Yue Liu, Ji-Gang Ren, Sheng-Kai Liao, Hui-Nan Wu, Shuang-Lin Li, Li Li, Nai-Le Liu, Chao-Yang Lu, Juan Yin, Yu-Ao Chen, Cheng-Zhi Peng, and Jian-Wei Pan. Quantum state transfer over 1200 km assisted by prior distributed entanglement. _Phys. Rev. Lett._, 128(17):170501, 2022.
* [72] S Lloyd. Universal quantum simulators. _Science_, 273(5278):1073-1078, 1996.
* [73] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis. _Nat. Phys._, 10(9):631-633, 2014.
* [74] Guang Hao Low and Isaac L Chuang. Optimal hamiltonian simulation by quantum signal processing. _Phys. Rev. Lett._, 118(1):010501, 2017.
* [75] P Magnard, S Storz, P Kurpiers, J Schar, F Marxer, J Lutolf, T Walter, J-C Besse, M Gabureac, K Reuer, A Akin, B Royer, A Blais, and A Wallraff. Microwave quantum link between superconducting circuits housed in spatially separated cryogenic systems. _Phys. Rev. Lett._, 125(26):260502, 2020.
* [76] Vitaly Maiorov and Allan Pinkus. Lower bounds for approximation by MLP neural networks. _Neurocomputing_, 25(1):81-91, 1999.
* [77] John M Martyn, Zane M Rossi, Andrew K Tan, and Isaac L Chuang. A grand unification of quantum algorithms. _arXiv [quant-ph]_, 2021.
* [78] Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. _Inf. Retr._, 3(2):127-163, 2000.
* [79] Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Alan Aspuru-Guzik. The theory of variational hybrid quantum-classical algorithms. _arXiv [quant-ph]_, 2015.
* [80] Hagay Michaeli, Tomer Michaeli, and Daniel Soudry. Alias-free convnets: Fractional shift invariance via polynomial activations. _arXiv [cs.CV]_, 2023.
* [81] Boris Mityagin. The zero set of a real analytic function. _arXiv [math.CA]_, 2015.

* [82] Ashley Montanaro and Sam Pallister. Quantum algorithms and the finite element method. _arXiv [quant-ph]_, 2015.
* [83] Ashley Montanaro and Changpeng Shao. Quantum communication complexity of linear regression. _arXiv preprint arXiv:2210.01601_, 2022.
* [84] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-BFGS algorithm. In Arthur Gretton and Christian C Robert, editors, _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_, volume 51 of _Proceedings of Machine Learning Research_, pages 249-258, Cadiz, Spain, 2016. PMLR.
* [85] Danial Motlagh and Nathan Wiebe. Generalized quantum signal processing. _arXiv [quant-ph]_, 2023.
* [86] William J Munro, Koji Azuma, Kiyoshi Tamaki, and Kae Nemoto. Inside quantum repeaters. _IEEE J. Sel. Top. Quantum Electron._, 21(3):78-90, 2015.
* [87] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. PipeDream: generalized pipeline parallelism for DNN training. In _Proceedings of the 27th ACM Symposium on Operating Systems Principles_, SOSP '19, pages 1-15, New York, NY, USA, 2019. Association for Computing Machinery.
* [88] Ashwin Nayak and Felix Wu. The quantum query complexity of approximating the median and related statistics. _arXiv [quant-ph]_, 1998.
* [89] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. Universal graph transformer self-attention networks, 2022.
* [90] Michael A Nielsen and Isaac L Chuang. _Quantum Computation and Quantum Information: 10th Anniversary Edition_. Cambridge University Press, 2010.
* [91] Brad G Osgood. _Lectures on the Fourier Transform and Its Applications (Pure and Applied Undergraduate Texts) (Pure and Applied Undergraduate Texts, 33)_. American Mathematical Society, 2019.
* [92] Pal Andras Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts increase the expressiveness of graph neural networks, 2021.
* [93] Adrian Perez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and Jose I Latorre. Data re-uploading for a universal quantum classifier. _arXiv [quant-ph]_, 2019.
* [94] Lirande Pira and Chris Ferrie. An invitation to distributed quantum neural networks. _Quantum Machine Intelligence_, 5(2):1-24, 2023.
* [95] M Pompili, S L N Hermans, S Baier, H K C Beukers, P C Humphreys, R N Schouten, R F L Vermeulen, M J Tiggelman, L Dos Santos Martins, B Dirkse, S Wehner, and R Hanson. Realization of a multinode quantum network of remote solid-state qubits. _Science_, 372(6539):259-264, 2021.
* [96] Stephen J Ponzio, Jaikumar Radhakrishnan, and S Venkatesh. The communication complexity of pointer chasing. _J. Comput. System Sci._, 62(2):323-355, 2001.
* [97] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. _arXiv [cs.LG]_, 2022.
* [98] Anup Rao and Amir Yehudayoff. _Communication Complexity and Applications_. Cambridge University Press, 2020.
* [99] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, pages 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery.

* [100] Arthur G Rattew and Patrick Rebentrost. Non-linear transformations of quantum amplitudes: Exponential improvement, generalization, and applications. _arXiv [quant-ph]_, 2023.
* [101] Ran Raz. Exponential separation of quantum and classical communication complexity. In _Proceedings of the thirty-first annual ACM symposium on Theory of Computing_, STOC '99, pages 358-367, New York, NY, USA, 1999. Association for Computing Machinery.
* [102] Alexander Razborov. Quantum communication complexity of symmetric predicates. _arXiv [quant-ph]_, 2002.
* [103] Tim Roughgarden. Communication complexity (for algorithm designers). _arXiv [cs.CC]_, 2015.
* [104] Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer. The effect of data encoding on the expressive power of variational quantum machine learning models. _arXiv [quant-ph]_, 2020.
* [105] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. _J. Mach. Learn. Res._, 12:2539-2561, 2011.
* [106] P W Shor. Algorithms for quantum computation: discrete logarithms and factoring. In _Proceedings 35th Annual Symposium on Foundations of Computer Science_. IEEE Comput. Soc. Press, 1994.
* [107] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv [cs.CL]_, 2023.
* [108] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, and Others. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312. 11805_, 2023.
* [109] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30, pages 5998-6008. Curran Associates, Inc., 2017.
* [110] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. _arXiv [cs.LG]_, 2022.
* [111] A J Walker. New fast method for generating discrete random numbers with arbitrary frequency distributions. _Electron. Lett._, 10(8):127-128, 1974.
* [112] Changqing Wang, Ivan Gonin, Anna Grassellino, Sergey Kazakov, Alexander Romanenko, Vyacheslav P Yakovlev, and Silvia Zorzetti. High-efficiency microwave-optical quantum transduction based on a cavity electro-optic superconducting system with long coherence time. _npj Quantum Information_, 8(1):1-10, 2022.
* [113] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mutei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph library: A graph-centric, highly-performant package for graph neural networks. _arXiv preprint arXiv:1909.01315_, 2019.
* [114] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?, 2019.
* [115] Yuanzhong Xu, Hyoukjoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: General and scalable parallelization for ML computation graphs. _arXiv [cs.DC]_, 2021.
* [116] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.

* [117] Andrew Chi-Chih Yao. Some complexity questions related to distributive computing(preliminary report). In _Proceedings of the eleventh annual ACM symposium on Theory of computing_, STOC '79, pages 209-213, New York, NY, USA, 1979. Association for Computing Machinery.
* [118] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [119] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning, 2019.
* [120] Qi Zhao and Yusu Wang. Learning metrics for persistence-based summaries and applications for graph classification, 2019.

Notation and a very brief review of quantum mechanics

We denote by \(\{a_{i}\}\) a set of elements indexed by \(i\), with \(1\)-based indexing unless otherwise specified, with the maximal value of \(i\) explicitly specified when it is not clear from context. \([N]\) denotes the set \(\{0,\ldots,N-1\}\). The complex conjugate of a number \(c\) is denoted by \(c^{*}\), and the conjugate transpose of a complex-valued matrix \(A\) by \(A^{\dagger}\).

We denote by \(\ket{\psi}\) a vector of complex numbers \(\{\psi_{i}\}\) representing the state of a quantum system when properly normalized, and by \(\bra{\psi}\) its dual (assuming it exists). The inner product between two such vectors of length \(N\) is denoted by

\[\bra{\psi}\sigma\rangle=\sum_{i=0}^{N-1}\psi_{i}^{*}\varphi_{i}.\] (A.1)

Denoting by \(\ket{i}\) for \(i\in[N]\) a basis vector in an orthonormal basis with respect to the above inner product, we can also write

\[\ket{\psi}=\sum_{i=0}^{N-1}\psi_{i}\ket{i}.\] (A.2)

Matrices will be denoted by capital letters, and when acting on quantum states will always be unitary. These can be specified in terms of their matrix elements using the Dirac notation defined above, as in

\[A=\underset{ij}{\sum}A_{ij}\ket{i}\bra{j}.\] (A.3)

Matrix-vector product are specified naturally in this notation by

Quantum mechanics is, in the simplest possible terms, a theory of probability based on conservation of the \(L^{2}\) norm rather than the standard probability theory based on the \(L^{1}\) norm [2, 90]. The state of a pure quantum system is described fully by a complex vector of \(N\) numbers known as amplitudes which we denote by \(\{\psi_{i}\}\) where \(i\in\{0,\ldots,N-1\}\), and is written using Dirac notation as \(\ket{\psi}\). The state is normalized so that

\[\bra{\psi}\psi\rangle=\sum_{i=0}^{N-1}\psi_{i}^{*}\psi_{i}=\sum_{i=0}^{N-1} \ket{\psi_{i}}^{2}=1,\] (A.4)

which is the \(L^{2}\) equivalent of the standard normalization condition of classical probability theory. It is a curious fact that the choice of \(L^{2}\) requires the use of complex rather than real amplitudes, and that no consistent theory can be written in this way for any other \(L^{p}\) norm [2]. The most general state of a quantum system is a probabilistic mixture of pure states, in the sense of the standard \(L^{1}\)-based rules of probability. We will not be concerned with these types of states, and so omit their description here, and subsequently whenever quantum states are discussed, the assumption is that they are pure.

Since any closed quantum system conserves probability, the \(L^{2}\) norm of a quantum state is conserved during the evolution of a quantum state. Consequently, when representing and manipulating quantum states on a quantum computer, the fundamental operation is the application of a unitary matrix to a quantum state.

Given a quantum system with some discrete degrees of freedom, the number of amplitudes corresponds to the number of possible states of the system, and is thus exponential in the number of degrees of freedom. The simplest such degree of freedom is a binary one, called a qubit, which is analogous to a bit. Thus a state of \(\log N\) qubits is described by \(N\) complex amplitudes.

A fundamental property of quantum mechanics is that the amplitudes of a quantum state are not directly measurable. Given a Hermitian operator

\[\mathcal{O}=\sum_{i=0}^{N-1}\lambda_{i}\ket{v_{i}}\bra{v_{i}}\] (A.5)

with real eigenvalues \(\{\lambda_{i}\}\), a measurement of \(\mathcal{O}\) with respect to a state \(\ket{\psi}\) gives the result \(\lambda_{i}\) with probability \(\left|\bra{v_{i}}\psi\rangle\right|^{2}\). The real-valued quantity

\[\bra{\psi}\mathcal{O}\ket{\psi}=\sum_{i=0}^{N-1}\lambda_{i}\left|\bra{\psi} \ket{v_{i}}\right|^{2}\] (A.6)is the expectation value of \(\mathcal{O}\) with respect to \(\ket{\psi}\), and its value can be estimated by measurements. After a measurement with outcome \(\lambda_{i}\), the original state is destroyed, collapsing to the state \(\ket{v_{i}}\). A consequence of the fundamentally destructive nature of quantum measurement is that simply encoding information in the amplitudes of a quantum state does not necessarily render it useful for downstream computation. It also implies that operations using amplitude-encoded data such as evaluating a simple loss function incur measurement error, unlike their classical counterparts that are typically limited only by machine precision. The design of quantum algorithms essentially amounts to a careful and intricate design of amplitude manipulations and measurements in order to extract useful information from the amplitudes of a quantum state. For a more complete treatment of these topics see [90].

## Appendix B Proofs

Proof of Lemma 1.: \(\bra{\varphi}\mathcal{P}_{0}\ket{\varphi}\) can be estimated by preparing \(\ket{\varphi}\) and measuring it \(O(1/\varepsilon^{2})\) times. Preparing each copy of \(\ket{\varphi}\) requires \(O(L)\) rounds of communication, with each round involving the communication of a \(\log N^{\prime}\)-qubit quantum state. Alice first prepares \(\ket{\psi(x)}\), and this state is passed back and forth with each player applying \(A_{\ell}\) or \(B_{\ell}\) respectively for \(\ell\in\{1,\ldots,L\}\).

Proof of Lemma 2.: We consider the parameters of the unitaries that Alice possesses first, and an identical argument follows for the parameters of Bob's unitaries.

We have

\[\begin{split}\frac{\partial}{\partial\theta_{\ell i}^{A}}\bra{ \varphi}\mathcal{P}_{0}\ket{\varphi}&=& 2\mathrm{Re}\bra{\varphi}\mathcal{P}_{0}\prod_{k=L}^{\ell+1}A_{k}B_{k}\frac{ \partial A_{\ell}}{\partial\theta_{\ell i}^{A}}B_{\ell}\prod_{k=\ell-1}^{1}A_ {k}B_{k}\ket{\psi(x)}\\ &\equiv& 2\mathrm{Re}\bra{\nu_{\ell i}^{A}}\mu_{\ell}^{A} \end{split}\] (B.1)

where

\[\ket{\mu_{\ell}^{A}}\] (B.2)

correspond to forward and backward features for the \(i\)-the parameter of \(A_{\ell}\) respectively. This is illustrated graphically in Figure 1. We also write

\[\ket{\nu_{\ell 0}^{A}}=\prod_{k=L}^{\ell+1}B_{k}^{\dagger}A_{k}^{\dagger} \mathcal{P}_{0}\ket{\varphi}.\] (B.3)

Attaching an ancilla qubit denoted by \(a\) to the feature states defined above, we define

\[\ket{\psi_{\ell i}^{A}}\equiv\frac{1}{\sqrt{2}}\left(\ket{0}\ket{\mu_{\ell}^{ A}}+\ket{1}\ket{\nu_{\ell i}^{A}}\right),\] (B.4)

and a Hermitian measurement operator

\[\begin{split} E_{\ell i}^{A}&\equiv\quad\left(\ket{ 0}\bra{0}\otimes I+\ket{1}\bra{1}\otimes\left(\frac{\partial A_{\ell}}{ \partial\theta_{\ell i}^{A}}\right)\right)X_{a}\left(\ket{0}\bra{0}\otimes I+ \ket{1}\bra{1}\otimes\left(\frac{\partial A_{\ell}}{\partial\theta_{\ell i}^{ A}}\right)^{\dagger}\right)\\ &=\quad\ket{1}\bra{0}\otimes\left(\frac{\partial A_{\ell}}{ \partial\theta_{\ell i}^{A}}\right)+\ket{0}\bra{1}\otimes\left(\frac{\partial A _{\ell}}{\partial\theta_{\ell i}^{A}}\right)^{\dagger},\end{split}\] (B.5)

we then have

\[\begin{split}\bra{\psi_{\ell 0}^{A}}E_{\ell i}^{A}\ket{\psi_{ \ell 0}^{A}}&=\bra{\psi_{\ell i}^{A}}X_{a}\ket{\psi_{\ell i}^{A}}\\ &=&\frac{\partial}{\partial\theta_{\ell i}^{A}} \bra{\varphi}\mathcal{P}_{0}\ket{\varphi},\end{split}\] (B.6)

where \(X_{a}\) acts on the ancilla.

Note that \(\left|\psi_{\ell 0}^{A}\right\rangle^{\otimes k}\) can be prepared by Alice first preparing \((\left|+\right\rangle\left|\psi(x)\right\rangle)^{\otimes k}\) and sending this state back and forth at most \(2L\) times, with each player applying the appropriate unitaries conditioned on the value of the ancilla. Additionally, for any choice of \(\ell\) and any \(i\), Alice has full knowledge of the \(E_{\ell i}^{A}\). They can thus be applied to quantum states and classical hypothesis states without requiring any communication.

The gradient can then be estimated using shadow tomography (Theorem 1). Specifically, for each \(\ell\), Alice prepares \(\tilde{O}(\log^{2}P\log N\log(L/\delta)/\varepsilon^{4})\) copies of \(\left|\psi_{0}^{A_{\ell}}\right\rangle\), which requires \(O(L)\) rounds of communication, each of \(\tilde{O}(\log^{2}P\log^{2}N\log(L/\delta)/\varepsilon^{4})\) qubits. She then runs shadow tomography to estimate \(\nabla_{A_{\ell}}\left\langle\varphi|Z_{0}|\varphi\right\rangle\) up to error \(\varepsilon\) with no additional communication. Bob does the same to estimate \(\nabla_{B_{\ell}}\left\langle\varphi|Z_{0}|\varphi\right\rangle\). In total \(O(L^{2})\) rounds are needed to estimate the full gradient. The success probability of all \(L\) applications of shadow tomography is at least \(1-\delta\) by a union bound.

Based on the results of [22], the space and time complexity of each application of shadow tomography is \(\sqrt{P}\mathrm{poly}(N,\log P,\varepsilon^{-1},\log(1/\delta))\). This is the query complexity of the algorithm to oracles that implement the measurement operators \(\left\{E_{\ell i}^{Q}\right\}\). Instantiating these oracles will incur a cost of at most \(O(N^{2})\). In cases where these operators have low rank the query complexity complexity will depend polynomially only on the rank instead of on \(N\).

Proof of Lemma 3.: We first prove an \(\Omega(\sqrt{N})\) lower bound on the amount of classical communication. Consider the following problem:

**Problem 4** ([101]).: _Alice is given a vector \(x\in S^{N-1}\) and two orthogonal linear subspaces of \(\mathbb{R}^{N}\) each of dimension \(N/2\), denoted \(M_{1},M_{2}\). Bob is given an orthogonal matrix \(O\). Under the promise that either \(\left\|M_{1}Ox\right\|_{2}\geq\sqrt{1-\theta^{2}}\) or \(\left\|M_{2}Ox\right\|_{2}\geq\sqrt{1-\theta^{2}}\) for \(0<\theta<1/\sqrt{2}\), Alice and Bob must determine which of the two cases holds._

Ref. [101] showed that the randomized7 classical communication complexity of the problem is \(\Omega(\sqrt{N})\).

Footnote 7: In this setting Alice and Bob can share an arbitrary number of random bits that are independent of their inputs.

The reduction from Problem 4 to Problem 1 is obtained by choosing \(\theta=1/2\) and simply setting \(L=1,B_{1}=O,\left|\psi(x)\right\rangle=\left|x\right\rangle,\mathcal{P}_{0}=Z_ {0}\), and

\[A_{1}=\sum_{j=0}^{N/2-1}\left|0\right\rangle\left|j\right\rangle\left\langle v _{j}^{1}\right|+\sum_{j=0}^{N/2-1}\left|1\right\rangle\left|j\right\rangle \left\langle v_{j}^{2}\right|,\] (B.7)

where the first register contains a single qubit and \(\left\{v_{j}^{k}\right\}\) form an orthonormal basis of \(M_{k}\), and picking any \(\varepsilon<1/2\). Note that this choice of \(\left|\psi(x)\right\rangle\) implies \(N^{\prime}=N\). Estimating \(\mathcal{L}\) to this accuracy now solves the desired problem since \(\mathcal{L}=\left\langle x\right|O^{\mathcal{I}}\left(\Pi_{1}-\Pi_{2}\right)O \left|x\right\rangle\) where \(\Pi_{k}\) is a projector onto \(M_{k}\), and hence estimating this quantity up to error \(1/2\) allows Alice and Bob to determine which subspace has large overlap with \(Ox\).

The reduction from Problem 4 to Problem 2 is obtained by setting \(L=2\), picking \(\left|\psi(x)\right\rangle,A_{1},B_{1}\) as before, and additionally \(B_{2}=I,A_{2}=e^{-i\theta_{A,1}^{A}X_{0}/2}\) initialized at \(\theta_{2,1}^{A}=-\pi/2\). By the parameter shift rule [34], we have that if \(U=e^{-i\theta\mathcal{P}/2}\) for some Pauli matrix \(\mathcal{P}\), and \(U\) is part of the parameterized circuit that defines \(\left|\varphi\right\rangle\), then

\[\frac{\partial\mathcal{L}}{\partial\theta}=\frac{1}{2}(\mathcal{L}(\theta+ \frac{\pi}{2})-\mathcal{L}(\theta-\frac{\pi}{2})).\] (B.8)It follows that

\[\begin{split}\frac{\partial\mathcal{L}}{\partial\theta_{2,1}^{A}} \Bigg{|}_{\theta_{2,1}^{A}=-\pi/2}=&\frac{1}{2}\left(\mathcal{L}(0 )-\mathcal{L}(-\pi)\right)\\ =&\frac{1}{2}\left(\mathcal{L}(0)-\bra{x}B_{1}^{ \dagger}A_{1}^{\dagger}e^{-i\frac{\pi}{2}X_{0}}Z_{0}e^{i\frac{\pi}{2}X_{0}}A _{1}B_{1}\ket{x}\right)\\ =&\frac{1}{2}\left(\mathcal{L}(0)-\bra{x}B_{1}^{ \dagger}A_{1}^{\dagger}X_{0}Z_{0}X_{0}A_{1}B_{1}\ket{x}\right)\\ =&\frac{1}{2}\left(\mathcal{L}(0)+\bra{x}B_{1}^{ \dagger}A_{1}^{\dagger}Z_{0}A_{1}B_{1}\ket{x}\right)\\ =&\mathcal{L}(0).\end{split}\] (B.9)

Estimating \(\nabla_{A}\bra{\varphi}Z_{0}\ket{\varphi}\) to accuracy \(\varepsilon<2\) allows one to determine the sign of \(\mathcal{L}(0)\), which as before gives the solution to Problem 4.

Next, we show that \(\Omega(L)\) rounds are necessary in both the quantum and classical setting by a reduction from the bit version of pointer-chasing, as studied in [58, 96].

**Problem 5** (Pointer-chasing, bit version).: _Alice receives a function \(f_{A}:[N]\rightarrow[N]\) and Bob receives a function \(f_{B}:[N]\rightarrow[N]\). Alice is also given a starting point \(x\in[N]\), and both receive an integer \(L_{0}\). Their goal is to compute the least significant bit of \(f^{(L_{0})}(x)\), where \(f^{(1)}(x)=f_{B}(x),f^{(2)}(x)=f_{A}(f_{B}(x)),\ldots\)._

Ref. [58] show that the quantum communication complexity of \(L_{0}\)-round bit pointer-chasing when Bob speaks first is \(\Omega(N/L_{0}^{4})\) (which holds for classical communication as well). This also bounds the \((L_{0}-1)\)-round complexity when Alice speaks first (since such a protocol is strictly less powerful given that there are fewer rounds of communication). On the other hand, there is a trivial \(L_{0}\)-round protocol when Alice speaks first that requires \(\log N\) bits of communication per round, in which Alice sends Bob \(x\), he sends back \(f^{(1)}(x)\), she replies with \(f^{(2)}(f^{(1)}(x))\), and so forth. This, combined with the lower bound, implies as exponential separation in communication complexity as a function of the number of rounds.

To reduce this problem to Problem 1, we assume \(f_{A},f_{B}\) are invertible. This should not make the problem any easier since it implies that \(f_{A},f_{B}\) have the largest possible image. In this setting, \(f_{A},f_{B}\) can be described by unitary permutation matrices:

\[U_{A}=\sum_{i}\ket{f_{A}(i)}\bra{i},U_{B}=\sum_{i}\ket{f_{B}(i)}\bra{i}.\] (B.10)

The corresponding circuit eq. (3.5) is then given by

\[\ket{\varphi}=\mathrm{SWAP}_{0\leftrightarrow\log N-1}U_{B}\ldots U_{A}U_{B} \ket{x}\] (B.11)

in the case where Bob applies the function last, with an analogous circuit in the converse situation (if Bob performed the swap, Alice applies an additional identity map). Estimating \(Z_{0}\) to accuracy \(\varepsilon<1\) using this state will then reveal the least significant bit of \(f^{(L_{0})}(x)\). This gives a circuit with \(L\) layers, where \(L_{0}\leq 2L-1\). Thus any protocol with less than \(L_{0}\) rounds (meaning less than \(2L-1\) rounds) would require communicating \(\Omega(N/L_{0}^{4})=\Omega(N/L^{4})\) qubits, since the converse will contradict the results of [58]. The reduction to Problem 2 is along similar lines to the one described by eq. (B.9), with the state in that circuit replaced by eq. (B.11). This requires at most two additional rounds of communication.

Since quantum communication is at least as powerful than classical communication, these bounds also hold for classical communication. Since each round involves communicating at least a single bit, this gives an \(\Omega(L)\) bound on the classical communication complexity. 

Proof of Lemma 5.: The proof is based on a reduction from the \(f\)-Boolean Hidden Partition problem (\(f-\mathrm{BHP}_{N,t}\)) studied in [36]. This is defined as follows:

**Problem 6** (Boolean Hidden Partition [36] (\(f-\mathrm{BHP}_{N,t}\))).: _Assume \(t\) divides \(N\). Alice is given \(x\in\{-1,1\}^{N}\). Bob is given a permutation \(\Pi\) over \([N]\), a boolean function \(f:\{-1,1\}^{t}\rightarrow\{-1,1\}\), and a vector \(v\in\{-1,1\}^{N/t}\). We are guaranteed that for any \(k\in\{1,\ldots,N/t\}\),_

\[f([\Pi x]_{[(k-1)t+1:kt]})*v_{k}=s\] (B.12)_for some \(s\in\{-1,1\}\). Their goal is to determine the value of \(s\)._

A polynomial \(p_{f}:\{-1,1\}^{t}\to\mathbb{R}\) is said to sign-represent a boolean function \(f\) if \(\operatorname{sign}(p_{f}(y))=f(y)\) for all \(y\in\{0,1\}^{t}\). The _sign-degree_ of \(f\) (\(\operatorname{sdeg}(f)\)) is the minimal degree of a polynomial that sign-represents it. In the special case \(\operatorname{sdeg}(f)=2\), \(f-\operatorname{BHP}_{N,t}\) can be solved with exponential quantum communication advantage [36]. For a vector \(y\in\{0,1\}^{t}\), define \(\tilde{y}=(1,y_{1},\ldots,y_{t})\). It is also known that if \(\operatorname{sdeg}(f)=2\), then there exists a sign-representing polynomial \(p_{f}\) that can be written as

\[p_{f}(y)=\tilde{y}^{T}R\tilde{y}\] (B.13)

for some matrix real \(R\)[4]. Moreover, for any \(f\) there exists such a \(p_{f}\) with \(\max_{x\in\{-1,1\}^{t}}|p_{f}(x)|\leq 3\). We denote by \(\beta=\min_{x\in\{-1,1\}^{t}}|p_{f}(x)|\) the _bias_ of \(p_{f}\).

We now describe a reduction from \(f-\operatorname{BHP}_{N,t}\) with \(\operatorname{sdeg}(f)=2\) to \(\operatorname{QGNI}_{CN,t}\) for some constant \(1\leq C\leq 3/2\). As is typical in communication complexity, the parties are allowed to exchange bits that are independent of the problem input, and these are not counted when measuring the communication complexity of a protocol that depends on the inputs. Before receiving their inputs, Alice thus sends two orthogonal vectors \(u_{0},u_{1}\) of length \(D_{0}\) to Bob, with each entry described by \(K\) bits 8.

Footnote 8: Since \(D_{0}\) is arbitrary and in particular independent of \(N\), even if we count this communication it will not affect the scaling with \(N\) which the main property we are interested in. This independence is also natural since it implies that the number of local graph features is independent of the size of the graph.

Assume Alice and Bob are given an instance of \(\operatorname{BHP}_{N,t}\). They use it to construct an instance of \(\operatorname{QGNI}_{(t+1)N/t,t}\) with \(D_{1}=1\). Alice constructs \(X\in\mathbb{R}^{(t+1)N/t}\) by picking the rows \(X_{i}\) according to

\[X_{i}=\begin{cases}\frac{1}{\sqrt{(t+1)N/t}}\left(\frac{1-x_{i}}{2}u_{0}^{T}+ \frac{1+x_{i}}{2}u_{1}^{T}\right)&i\leq N\\ \frac{1}{\sqrt{(t+1)N/t}}u_{1}&i>N\end{cases}.\] (B.14)

Note that with this definition \(||X||_{F}=1\). Bob defines a permutation \(\pi^{\prime}\) over \([(t+1)N/t]\) by

\[\pi^{\prime}(i)=\begin{cases}\left\lfloor i/t\right\rfloor(t+1)+i\%t+1&i\leq N \\ (i-N-1)(t+1)+1&i>N\end{cases},\] (B.15)

denoting the corresponding permutation matrix \(\Pi^{\prime}\). Define by \(\overline{x}\) the concatenation of Alice's input \(x\) with \(1^{(t+1)N/t}\). The purpose of this permutation is that \(\Pi^{\prime}\Pi\overline{x}\equiv\tilde{x}\) will be a concatenation of \(N/t\) vectors of length \(t+1\), with the \(i\)-th vector equal to \((1,[\Pi x]_{t(i-1)+1},\ldots,[\Pi x]_{ti})\equiv\tilde{x}_{(i)}\).

Note that we can assume wlog that \(R\) in eq. (B.13) is symmetric since \(p_{f}\) is independent of its antisymmetric part. It can thus be diagonalized by an orthogonal matrix \(U\), and denoting the diagonal matrix of its real eigenvalues by \(D\), we define a (complex-valued) matrix \(S=U\sqrt{D}\) that satisfies \(R=SS^{T}\). Bob therefore defines his model by

\[A=(I_{N/t}\otimes S^{T})\Pi^{\prime}\Pi,\quad W_{1}=u_{1}-u_{0},\quad W_{2}=v^ {T}.\] (B.16)

Additionally, he picks the pooling operator \(\mathcal{P}:\mathbb{R}^{(t+1)N/t}\to\mathbb{R}^{N/t}\) to be sum pooling with window size \(t+1\) (i.e. \(\mathcal{P}(x)_{j}=\sum_{k=(j-1)(t+1)+1}^{(j+1)}x_{k}\)). Bob also uses a simple quadratic nonlinearity by choosing \(a=1,b=c=0\) in eq. (4.3).

To see that solving \(\mathrm{QGNI}_{(t+1)N/t,t}\) to error \(\varepsilon<1/2\) indeed provides a solution to \(\mathrm{BHP}_{N,\ell}\), note that

\[\mathcal{P}\left(\sigma(AXW_{1})\right)_{i} =\mathcal{P}\left(\sigma(\frac{1}{\sqrt{(t+1)N/t}}A\overline{x}) \right)_{i}\] (B.17) \[=\mathcal{P}\left(\sigma(\frac{1}{\sqrt{(t+1)N/t}}(I_{N/t}\otimes S ^{T})\Pi^{\prime}\Pi\overline{x})\right)_{i}\] (B.18) \[=\mathcal{P}\left(\sigma(\frac{1}{\sqrt{(t+1)N/t}}(I_{N/t}\otimes S ^{T})\bar{x})\right)_{i}\] (B.19) \[=\frac{1}{(t+1)N/t}\sum_{j=1}^{t+1}([S^{T}\tilde{x}_{(i)}]_{j})^{2}\] (B.20) \[=\frac{1}{(t+1)N/t}\tilde{x}_{(i)}^{T}SS^{T}\tilde{x}_{(i)}\] (B.21) \[=\frac{1}{(t+1)N/t}p_{f}([\Pi x]_{[(i-1)t+1:it]}).\] (B.22)

Given the choice of \(W_{2}\), one obtains

\[\varphi(X/||X||_{F})=\frac{1}{(t+1)N/t}\sum_{i=1}^{N/t}p_{f}([\Pi x]_{(i-1)t+1 :it]})v_{i}.\] (B.23)

It follows that \(\mathrm{sign}(\varphi(X))=s\) and \(|\varphi(X)|\geq\beta\). It is thus possible to decide the value of \(s\) if \(\varphi(X/||X||_{F})\) is estimated to some error smaller than \(\beta\).

From Theorem 4 of [36], we have \(R^{\rightarrow}(f-\mathrm{BHP}_{N,t})=\Omega(\sqrt{N/t})\) for any \(f\) that has sign-degree \(2\) and satisfies some additional conditions. The reduction then implies

\[R^{\rightarrow}_{\beta}(\mathrm{QGNI}_{N,t})=\Omega(\sqrt{(t/(t+1))N/t}).\] (B.24)

This can be simplified by noting that since \(t\geq 2\), \(t/(t+1)\geq 2/3\). The lower bound in [36] is based on choosing \(f\) which belongs to a specific class of symmetric boolean functions (meaning \(f(y)=\tilde{f}(|y|)\) where \(|y|=|\{i:y_{i}=-1\}|\)). Specifically, \(\tilde{f}\) is defined by the choice of \(t\) and two additional integer parameters \(\theta_{1},\theta_{2}\) such that \(0\leq\theta_{1}<\theta_{2}<t\) and

\[\tilde{f}(|y|)=\begin{cases}1&0\leq|y|\leq\theta_{1}\text{ or }\theta_{2}<|y|,\\ -1&\theta_{1}<|y|\leq\theta_{2},\end{cases}\] (B.25)

(and an additional technical condition that will not be of relevance to our analysis).

We next construct a sign-representing polynomial \(p_{f}\) for any \(f\) that takes the above form, and compute its bias \(\beta\). Since \(f\) is symmetric of sign degree \(2\), it suffices to construct a polynomial \(\tilde{p}_{f}:\mathbb{R}\rightarrow\mathbb{R}\) such that \(p_{f}(y)=\tilde{p}_{f}(|y|)\) for this purpose. If we can produce some \(\beta^{\prime}\) that bounds \(\beta\) from below for any choice of \(t,\theta_{1},\theta_{2}\), then the lower bound from Theorem 4 of [36] holds for any error smaller than \(\beta^{\prime}\).

We choose \(\tilde{p}_{f}(z)=\tilde{a}z^{2}+\tilde{b}z+1\), with the constraints \(\tilde{p}_{f}(\theta_{1}+1/2)=0,\tilde{p}_{f}(\theta_{2}+1/2)=0\). These lead to the solution

\[\tilde{p}_{f}(z)=\frac{1}{\theta_{1}^{+}\theta_{2}^{+}}z^{2}-\frac{1}{\theta_ {1}^{+}\theta_{2}^{+}}\frac{\theta_{2}^{+2}-\theta_{1}^{+}}{\theta_{2}^{+}- \theta_{1}^{+}}z+1.\] (B.26)

Since this is a quadratic function with known roots that is only evaluated at integer inputs, if we want to bound the bias of \(p_{f}\) it suffices to check the values of \(\tilde{p}_{f}\) at the integers closest to the roots, namely \(\{\theta_{1},\theta_{1}+1,\theta_{2},\theta_{2}+1\}\). Plugging in these values gives

\[\tilde{p}_{f}(\theta_{1})= 1-\frac{\theta_{2}-1}{(1+\frac{1}{2\theta_{1}})(\theta_{2}+\frac{ 1}{2})}\] (B.27) \[\geq 1-\frac{1}{1+\frac{1}{2\theta_{1}}}\] \[\geq \frac{1}{4\theta_{1}}\] \[\geq \frac{1}{4t},\]

where in the third line we used \(\frac{1}{1+x}\leq 1-x/2\) which holds for \(0\leq x\leq 1\). Using \(\theta_{2}\leq\theta_{1}+1\), we also have

\[\tilde{p}_{f}(\theta_{1}+1)= 1-\frac{\theta_{1}+1}{\left(\theta_{1}+\frac{1}{2}\right)\left( 1+\frac{1}{2\theta_{2}}\right)}\] (B.28) \[\leq 1-\frac{\theta_{1}+1}{\left(\theta_{1}+\frac{1}{2}\right)\left( 1+\frac{1}{2\theta_{1}+2}\right)}\] \[= -\frac{1}{4\left(\theta_{1}+\frac{1}{2}\right)\left(\theta_{1}+ \frac{3}{2}\right)}\] \[\leq -\frac{1}{4\left(t+\frac{1}{2}\right)\left(t+\frac{3}{2}\right)}.\]

\(\tilde{p}_{f}(\theta_{2})\) takes the same value as \(\tilde{p}_{f}(\theta_{1}+1)\). Similarly,

\[\tilde{p}_{f}(\theta_{2}+1)\] (B.29) \[\geq 1-\frac{\theta_{2}+1}{(\theta_{2}+\frac{1}{2})(1+\frac{1}{2 \theta_{2}-2})}\] \[= \frac{2\theta_{2}^{2}-\frac{5}{4}}{\theta_{2}^{2}-\frac{1}{4}}.\]

For \(\theta_{2}\leq 1\) this is a monotonically increasing function of \(\theta_{2}\), and is thus lower bounded by picking \(\theta_{2}=1\), giving \(\tilde{p}_{f}(2)\geq 1\). It follows that for any choice of \(t,\theta_{1},\theta_{2}\), the bias is bounded from below by

\[\beta^{\prime}=\frac{1}{4\left(t+\frac{1}{2}\right)\left(t+\frac{3}{2}\right)}.\] (B.30)

Note that our bound on the bias allows us to use the reduction from \(f-\mathrm{BHP}_{N,t}\) to \(\mathrm{QGNI}_{(t+1)N/t,t}\) for any valid choice of \(f\) (satisfying eq. (B.24)).

Proof of Lemma 6.: Alice encodes her input in the quantum state

\[\ket{\tilde{X}}_{0}\equiv\frac{1}{\sqrt{2}\ket{X}_{F}}\ket{0}\sum_{i=0}^{N-1 }\sum_{j=0}^{D_{0}-1}X_{ij}\ket{i,j}+\frac{1}{\sqrt{2}}\ket{1}\ket{0}\ket{0^{ \otimes N},0^{\otimes D_{0}}}\] (B.31)

over \(\log(ND_{0})+1\) qubits. She sends this state to Bob. Define \(D=\max\{D_{0},D_{1}\}\). Bob augments this state by attaching zero qubits and, reordering the first two qubits, obtains the state

\[\ket{\tilde{X}}\] (B.32) \[\equiv \frac{1}{\sqrt{2}}\ket{0}\ket{0}\ket{\overline{X}}+\frac{1}{\sqrt {2}}\ket{1}\ket{0}\ket{0^{\otimes N},0^{\otimes D}}\]over \(\log(ND)+2\) qubits.

Define by \(\overline{W}_{1}\) the \(D\times D\) matrix obtained by appending zero rows or columns to the rectangular matrix \(W_{1}\) to obtain a square matrix, and denote \(\alpha=\left\|A\otimes\overline{W}_{1}\right\|\). Bob prepares an \((\alpha,1,0)\)-block-encoding of \(A\otimes\overline{W}_{1}\), denoted \(U_{A\otimes\overline{W}_{1}}\), which acts on \(\log(ND)+1\) qubits. Bob then applies this unitary conditioned on the value of the first qubit, giving

\[\begin{split}\left(\ket{0}\bra{0}U_{A\otimes\overline{W}_{1}}+ \ket{1}\bra{1}\right)\ket{\tilde{X}}&=\!\frac{1}{\sqrt{2}}\ket{0 }U_{A\otimes\overline{W}_{1}}\ket{0}\ket{\overline{X}}+\frac{1}{\sqrt{2}}\ket{ 1}\ket{0}\ket{0^{\otimes N},0^{\otimes D}}\\ &=\!\frac{1}{\sqrt{2}}\ket{0}\left(\frac{1}{\alpha}\ket{0}A \otimes\overline{W}_{1}\ket{\overline{X}}+\ket{1}\ket{g}\right)+\frac{1}{ \sqrt{2}}\ket{1}\ket{0}\ket{0^{\otimes N},0^{\otimes D}}\\ &\equiv\!\frac{1}{\sqrt{2}}\ket{0}\left(\frac{1}{\alpha}\ket{0} \overline{|\overline{AXW}_{1}\rangle}+\ket{1}\ket{g}\right)+\frac{1}{\sqrt{2}} \ket{1}\ket{0}\ket{0^{\otimes N},0^{\otimes D}}\\ &\equiv\!\ket{\psi}\end{split}\] (B.33)

where \(\ket{g}\) is an unnormalized garbage state. Above, \(\overline{AXW_{1}}\) is an \(N\times D\) matrix obtained by adding zero columns to \(W_{1}\) as needed.

The sum pooling operator \(\mathcal{P}\) can be implemented by multiplication by an \(N/t\times N\) matrix which we denote by \(\tilde{P}\). Define by \(\overline{W_{2}}\) the \(D\times N/t\) matrix obtained by appending zero rows to \(W_{2}\) if needed. Given a matrix \(M\) of size \(N_{1}\times N_{2}\), denote by \(V[M]\) the vectorization of \(M\). Bob then constructs the Hermitian matrix

\[\mathcal{O}=\left(\begin{array}{cc}2a\alpha^{2}\ket{0}\bra{0}\otimes \operatorname{diag}(V[\overline{W_{2}}\tilde{P}])&b\alpha\ket{0}\bra{0}\otimes V [\overline{W_{2}}\tilde{P}]\bra{0^{\otimes N},0^{\otimes D}}\\ b\alpha\ket{0}\bra{0}\otimes\ket{0^{\otimes N},0^{\otimes D}}V[\overline{W_{2} }\tilde{P}]^{\dagger}&0\end{array}\right).\] (B.34)

It follows that

\[\begin{split}\bra{\psi}\mathcal{O}\ket{\psi}+c\mathrm{tr}\left[W _{2}P1^{N\times D_{1}}\right]&=\!\frac{a}{\left\|X\right\|_{F}^{2}}\mathrm{ tr}\left[W_{2}\tilde{P}\left(AXW_{1}\right)^{2}\right]+\frac{b}{\left\|X \right\|_{F}}\mathrm{tr}\left[W_{2}PAXW_{1}\right]+c\mathrm{tr}\left[W_{2}P1^{ N\times D_{1}}\right]\\ &=\!\mathrm{tr}\left[W_{2}\tilde{P}\sigma(A\frac{X}{\left\|X\right\|_{F}}W _{1})\right]\\ &=\!\varphi(X/\left\|X\right\|_{F}),\end{split}\] (B.35)

where \(1^{N\times D_{1}}\) is an all ones matrix. The last term on the RHS is independent of \(X\) and can be computed by Bob without requiring Alice's message. Estimating \(\bra{\psi}O\ket{\psi}\) to accuracy \(\varepsilon\) requires \(O(\left\|\mathcal{O}\right\|/\varepsilon)\) measurements. Since

\[\begin{split}\left\|\mathcal{O}\right\|&\leq\! \left\|2a\alpha^{2}\ket{0}\bra{0}\otimes\operatorname{diag}(V[\overline{W_{2} }\tilde{P}])\right\|+2\left\|b\alpha\ket{0}\bra{0}\otimes V[\overline{W_{2} }\tilde{P}]\bra{0^{\otimes N},0^{\otimes D}}\right\|\\ &\leq\!2(\left|a\right|\alpha^{2}+\left|b\right|\alpha)\left\|W_{ 2}\tilde{P}\right\|_{\infty},\end{split}\] (B.36)

Bob requires \(O(\left(\left|a\right|\alpha^{2}+\left|b\right|\alpha\right)\left\|W_{2} \tilde{P}\right\|_{\infty}/\varepsilon)\) copies of Alice's state in order to do this. 

Proof of lemma 7.: For the parameter choices used to obtain the classical lower bound (eq. B.14) and eq. B.16), we have \(||W_{1}||=1,||W_{2}P||_{\infty}\leq t\). Additionally, for the polynomials constructed in eq. B.26), we have \(|p_{f}(y)|\leq Ct^{2}\) from which it follows that the matrix \(R\) used in the matrix representation of \(p_{f}\) has constant operator norm \(C\), and thus \(\left\|A\right\|=\left\|S\right\|=\sqrt{C}\). We also have \(a=1,b=c=0\) for the nonlinearity used (eq. 4.3), and it thus follows from Lemma 6 that \(Q_{c}^{-}(\mathrm{QGNI}_{N,t})=O(t^{3}\log(ND_{0}))\) for \(\varepsilon\leq\frac{1}{4\left(t+\frac{1}{3}\right)\left(t+\frac{3}{3}\right)}\). With this choice of \(\varepsilon\), the classical lower bound in Lemma 5 holds, and thus an exponential advantage in communication is obtained by using quantum communication. 

Proof of Lemma 11.: Consider first a single variable \(z\), with data-dependent unitaries given by eq. F.4a). If \(\{\lambda_{\ell i}\}\) are chosen i.i.d. from a uniform distribution over say \([0,1]\), then with probability \(1\) they are all unique and so are all sums of the form \(\Lambda_{\overline{j}}=\sum\limits_{\ell=1}^{L}\lambda_{\ell j_{\ell}}\) as well as differences \(\Lambda_{\overline{j}}-\Lambda_{\overline{k}}\) for 

[MISSING_PAGE_EMPTY:27]

Proof of Lemma 12.: Consider a periodic function \(f\) with period \(1\). Denote by \(S_{M}[f]\) the truncated Fourier series of \(f\) written in terms of trigonometric functions:

\[\begin{split} S_{M}[f](y)&=\!\!\!\sum_{m=0}^{M-1}\int \limits_{x=-1/2}^{1/2}f(x)\cos\left(2\pi mx\right)\mathrm{d}x\cos\left(2\pi my \right)+\sum_{m=0}^{M-1}\int\limits_{x=-1/2}^{1/2}f(x)\sin\left(2\pi mx\right) \mathrm{d}x\sin\left(2\pi my\right)\\ &\equiv\!\!\!\sum_{m=0}^{M-1}\!\hat{f}_{m}^{+}\cos\left(2\pi my \right)+\sum_{m=0}^{M-1}\hat{f}_{m}^{-}\sin\left(2\pi my\right).\end{split}\] (B.43)

If \(f\) is \(p\)-times continuously differentiable, it is known that the Fourier series converges uniformly, with rate

\[\left\|S_{M}[f]-f\right\|_{\infty}<\frac{C}{M^{p-1/2}}.\] (B.44)

for some absolute constant \(C\)[91]. For analytic functions the rate is exponential in \(M\).

We now define the following circuit:

\[A_{1}(x)=\mathrm{diag}((\underbrace{1,\ldots,1}_{N^{\prime}/2},\underbrace{1,\frac{e^{2\pi ix},e^{2\pi i2x}\ldots,e^{2\pi i(N^{\prime}/4-1)x}}{N^{\prime}/4 }}_{N^{\prime}/4},\underbrace{1,\frac{e^{2\pi ix},e^{2\pi i2x}\ldots,e^{2\pi i (N^{\prime}/4-1)x}}{N^{\prime}/4}}_{N^{\prime}/4})),\] (B.45)

\[B_{1}=\big{|}\hat{f}\big{\rangle}\left\langle 0\right|+\left|0\right\rangle \big{\langle}\hat{f}\big{|},\] (B.46)

where

\[\big{|}\hat{f}\big{\rangle}=\frac{1}{\sqrt{\sum\limits_{m}\left|\hat{f}_{m} \right|}}\sum_{m=0}^{N^{\prime}/4-1}\left(\sqrt{\hat{f}_{m}^{+}}\frac{\left|0 \right\rangle+\mathrm{sign}(\hat{f}_{m}^{+})\left|1\right\rangle}{\sqrt{2}} \left|0\right\rangle+\sqrt{\hat{f}_{m}^{-}}\frac{\left|0\right\rangle-i \mathrm{sign}(\hat{f}_{m}^{-})\left|1\right\rangle}{\sqrt{2}}\left|1\right \rangle\right)\left|m\right\rangle.\] (B.47)

Choosing \(\left|\psi(x)\right\rangle=\left|0\right\rangle\) as the initial state, this gives

\[\begin{split}\left|\varphi\right\rangle&=\!\!A_{1}B_{ 1}\left|0\right\rangle\\ &=\!\!A_{1}\big{|}\hat{f}\big{\rangle}\\ &=\!\!\frac{1}{\sqrt{\sum\limits_{m}\left|\hat{f}_{m}\right|}} \sum_{m=0}^{N^{\prime}/4-1}\left(\sqrt{\hat{f}_{m}^{+}}\frac{\left|0\right\rangle +\mathrm{sign}(\hat{f}_{m}^{+})e^{2\pi imx}\left|1\right\rangle}{\sqrt{2}} \left|0\right\rangle+\sqrt{\hat{f}_{m}^{-}}\frac{\left|0\right\rangle-i \mathrm{sign}(\hat{f}_{m}^{-})e^{2\pi imx}\left|1\right\rangle}{\sqrt{2}} \left|1\right\rangle\right)\left|m\right\rangle\end{split}\] (B.48)

It follows that

\[\begin{split}\left\langle\varphi\right|X_{0}\left|\varphi\right\rangle &=\!\!\frac{1}{\sum\limits_{m}\left|\hat{f}_{m}\right|}\sum_{m=0}^{ N^{\prime}/4-1}\ \ \left|\hat{f}_{m}^{+}\right|\frac{\left\langle 0\right|+\mathrm{ sign}(\hat{f}_{m}^{+})e^{-2\pi imx}\left\langle 1\right|}{\sqrt{2}}X_{0}\frac{\left|0 \right\rangle+\mathrm{sign}(\hat{f}_{m}^{+})e^{2\pi imx}\left|1\right\rangle}{ \sqrt{2}}\\ &=\!\!\frac{1}{\sum\limits_{m}\left|\hat{f}_{m}\right|}\sum_{m=0} ^{N^{\prime}/4-1}\hat{f}_{m}^{+}\cos(2\pi mx)+\hat{f}_{m}^{-}\sin(2\pi mx)\\ &=\!\!\frac{1}{\sum\limits_{m}\left|\hat{f}_{m}\right|}S_{N^{ \prime}/4}[f](x)\end{split}\] (B.49)

This approximation thus converges uniformly according to eq. (B.44), with error decaying exponentially with number of qubits \(\log N^{\prime}\) as long as \(f\) is continuously differentiable at least once. 

Proof of Lemma 10.: The algorithm in Theorem 5 of [100] takes as input a state-preparation unitary \(U\) acting on \(n=\log N\) qubits such that \(U\left|0\right\rangle^{\otimes n}=\left|z\right\rangle\). Using \(O(\log 1/\varepsilon)\) queries to \(U\) and \(U^{\dagger}\) and 

[MISSING_PAGE_FAIL:29]

Exponential advantages in end-to-end training

So far we have discussed the problems of inference and estimating a single gradient vector. It is natural to also consider when these or other gradient estimators can be used to efficiently solve an optimization problem (i.e. when the entire training processes is considered rather than a single iteration). Applying the gradient estimation algorithm detailed in Lemma 2 iteratively gives a distributed stochastic gradient descent algorithm which we detail in Algorithm 2, yet one may be concerned that a choice of \(\varepsilon=O(\log N)\) which is needed to obtain an advantage in communication complexity will preclude efficient convergence. Here we present a simpler algorithm that requires a single quantum measurement per iteration, and can provably solve certain convex problems efficiently, as well as an application of shadow tomography to fine-tuning where convergence can be guaranteed, again with only logarithmic communication cost. In both cases, there is an exponential advantage in communication even when considering the entire training process.

### "Smooth" circuits

Consider the case where \(A_{\ell}\) are product of rotations for all \(\ell\), namely

\[A_{\ell}=\prod_{j=1}^{P}\!\!e^{-\frac{1}{2}i\beta_{\ell j}^{A}\theta_{\ell j}^ {A}\mathcal{P}_{\ell j}^{A}},\] (D.1)

where \(\mathcal{P}_{\ell j}^{A}\) are Pauli matrices acting on all qubits, and similarly for \(B_{\ell}\). These can also be interspersed with other non-trainable unitaries. This constitutes a slight generalization of the setting considered in [49], and the algorithm we present is essentially a distributed distributed version of theirs. Denote by \(\beta\) an \(2PL\)-dimensional vector with elements \(\beta_{\ell j}^{Q}\) where \(Q\in\{A,B\}\)9. The quantity \(\left\lVert\beta\right\rVert_{1}\) is the total evolution time if we interpret the state \(\left\lvert\varphi\right\rangle\) as a sequence of Hamiltonians applied to the initial state \(\left\lvert x\right\rangle\).

Footnote 9: [49] actually consider a related quantity for which has smaller norm in cases where multiple gradient measurements commute, leading to even better rates.

In Appendix D.3 we describe an algorithm that converges to the neighborhood of a minimum, or achieves \(\mathbb{E}\mathcal{L}(\Theta)-\mathcal{L}(\Theta^{\star})\leq\varepsilon_{0}\), for a convex \(\mathcal{L}\) after

\[2\left\lVert\Theta^{(0)}-\Theta^{\star}\right\rVert_{2}^{2}\left\lVert\beta \right\rVert_{1}^{2}/\varepsilon_{0}^{2}\] (D.2)

iterations, where \(\Theta^{\star}\) are the parameter values at the minimum of \(\mathcal{L}\). The expectation is with respect to the randomness of quantum measurement and additional internal randomness of the algorithm. The algorithm is based on classically sampling a single coordinate to update at every iteration, and computing an unbiased estimator of the gradient with a single measurement. It can thus be seen as a form of probabilistic coordinate descent.

This implies an exponential advantage in communication for the entire training process as long as \(\left\lVert\Theta^{(0)}-\Theta^{\star}\right\rVert_{2}^{2}\left\lVert\beta \right\rVert_{1}^{2}=\mathrm{polylog}(N)\). Such circuits either have a small number of trainable parameters (\(P=O(\mathrm{polylog}(N))\)), depend weakly on each parameter (e.g. \(\beta_{\ell j}^{Q}=O(1/P)\) for arbitrary \(P\)), or have structure that allows initial parameter guesses whose quality diminishes quite slowly with system size. Nevertheless, over a convex region the loss can rapidly change by an \(O(1)\) amount. One may also be concerned that in the setting \(\left\lVert\Theta^{(0)}-\Theta^{\star}\right\rVert_{2}^{2}\left\lVert\beta \right\rVert_{1}^{2}=\mathrm{polylog}(N)\) only a logarithmic number of parameters is updated during the entire training process and so the total effect of the training process may be negligible. It is important to note however that each such sparse update depends on the structure of the entire gradient vector as seen in the sampling step. In this sense the algorithm is a form of probabilistic coordinate descent, since the probability of updating a coordinate \(\left\lvert\beta_{\ell j}^{Q}\right\rvert/\left\lVert\beta\right\rVert_{1}\) is proportional to the the magnitude of the corresponding element in the gradient (actually serving as an upper bound for it).

Remarkably, the time complexity of a single iteration of this algorithm is proportional to a forward pass, and so matches the scaling of classical backpropagation. This is in contrast to the polynomial overhead of shadow tomography (Theorem 1). Additionally, it requires a single measurement per iteration, without any of the additional factors in the sample complexity of shadow tomography.

### Fine-tuning the last layer of a model

Consider a model given by eq. (3.1) where only the parameters of \(A_{L}\) are trained, and the rest are frozen, and denote this model by \(\ket{\varphi_{f}}\). The circuit up to that unitary could include multiple data-dependent unitaries that represent complex features in the data. Training only the final layer in this manner is a common method of fine-tuning a pre-trained model [53]. If we now define

\[\tilde{E}_{Li}^{A}=\ket{1}\bra{0}\otimes A_{L}^{\dagger}\mathcal{P}_{0}\frac{ \partial A_{L}}{\partial\theta_{Li}^{A}}+\ket{0}\bra{1}\otimes\left(\frac{ \partial A_{L}}{\partial\theta_{Li}^{A}}\right)^{\dagger}\mathcal{P}_{0}A_{L},\] (D.3)

the expectation value of \(\tilde{E}_{Li}^{A}\) using the state \(\ket{+}\ket{\mu_{L}^{A}}\) gives \(\frac{\partial\mathcal{L}}{\partial\theta_{Li}^{A}}\). Here \(\ket{\mu_{L}^{A}}=B_{L}(x)\underset{k=L-1}{\overset{1}{\prod}}A_{k}(x)B_{k}( x)\ket{\psi(x)}\) is the forward feature computed by Alice at layer \(L\) with the parameters of all the other unitaries frozen (hence the dependence on them is dropped). Since the observables in the shadow tomography problem can be chosen in an online fashion [5; 6; 13], and adaptively based on previous measurements, we can simply define a stream of measurement operators by measuring \(P\) observables to estimate the gradients w.r.t. an initial set of parameters, updating these parameters using gradient descent with step size \(\eta\), and defining a new set of observables using the updated parameters. Repeating this for \(T\) iterations gives a total of \(PT\) observables (a complete description of the algorithm is given in Algorithm 3).

By the scaling in Lemma 2, the total communication needed is \(\tilde{O}(\log N(\log TP)^{2}\log(1/\delta)/\varepsilon^{4})\) over \(O(L)\) rounds (since only \(O(L)\) rounds are needed to create copies of \(\ket{\mu^{A_{L}}}\). This implies an exponential advantage in communication for the entire training process (under the reasonable assumption \(T=O(\mathrm{poly}(N,P))\)), despite the additional stochasticity introduced by the need to perform quantum measurements. For example, assume one has a bound \(\norm{\nabla\mathcal{L}}_{2}^{2}\leq K\). If the circuit is comprised of unitaries with Hermitian derivatives, this holds with \(K=PL\). In that case, denoting by \(g\) the gradient estimator obtained by shadow tomography, we have

\[\norm{g}_{2}^{2}\leq\norm{\nabla\mathcal{L}}_{2}^{2}+\norm{\nabla\mathcal{L}- g}_{2}^{2}\leq K+\varepsilon^{2}PL.\] (D.4)

It then follows directly from Lemma 8 that for an appropriately chosen step size, if \(\mathcal{L}\) is convex one can find parameter values \(\overline{\Theta}\) such that \(\mathcal{L}(\overline{\Theta})-\mathcal{L}(\Theta^{\star})\leq\varepsilon_{0}\) using

\[T=2\norm{\Theta^{(0)}-\Theta^{\star}}_{2}^{2}(K+\varepsilon^{2}PL)^{2}/ \varepsilon_{0}^{2}\] (D.5)

iterations of gradient descent. Similarly if \(\mathcal{L}\) is \(\lambda\)-strongly convex then \(T=2(K+\varepsilon^{2}PL)^{2}/\lambda\varepsilon_{0}+1\) iterations are sufficient. In both cases therefore an exponential advantage is achieved for the optimization process as a whole, since in both cases one can implement the circuit that is used to obtain the lower bounds in Lemma 3.

In the following, we make use of well-known convergence rates for stochastic gradient descent:

**Lemma 8** ([26]).: _Given an objective function \(\mathcal{L}(\Theta)\) with a minimum at \(\Theta^{\star}\) and a stochastic gradient oracle that returns a noisy estimate of the gradient \(g(\Theta)\) such that \(\mathbb{E}g(\Theta)=\nabla\mathcal{L}(\Theta),\mathbb{E}\norm{g}_{2}^{2}\leq G ^{2}\), and denoting by \(\Theta^{(0)}\) a point in parameter space and \(R=\norm{\Theta^{(0)}-\Theta^{\star}}_{2}\), we have:_

1. _If_ \(\mathcal{L}\) _is convex in a Euclidean ball of radius_ \(R\) _around_ \(\Theta^{\star}\)_, then gradient descent with step size_ \(\eta=\frac{R}{G}\sqrt{\frac{2}{T}}\) _achieves_ \[\mathbb{E}\mathcal{L}(\frac{1}{T}{\sum_{t=1}^{T}}\Theta^{(t)})-\mathcal{L}( \Theta^{\star})\leq RG\sqrt{\frac{2}{T}}.\] (D.6)
2. _If_ \(\mathcal{L}\) _is_ \(\lambda\)_-strongly convex in a Euclidean ball of radius_ \(R\) _around_ \(\Theta^{\star}\)_, then gradient descent with step size_ \(\eta_{t}=\frac{2}{\lambda(t+1)}\) _achieves_ \[\mathbb{E}\mathcal{L}(\frac{1}{T(T+1)}{\sum_{t=1}^{T}}2t\Theta^{(t)})-\mathcal{ L}(\Theta^{\star})\leq\frac{2G^{2}}{\lambda(T+1)}.\] (D.7)

### Distributed Probabilistic Coordinate Descent

Given distributed states of the form eq. (D.1), optimization over \(\Theta\) can be performed using Algorithm 1. We verify the correctness of this algorithm and provide convergence rates following [49]. Define the Hermitian measurement operator

\[\hat{E}_{\ell i}^{Q}=\left(\ket{0}\bra{0}\otimes I-i\ket{1}\bra{1}\otimes \mathcal{P}_{\ell i}^{Q}\right)^{\dagger}X_{a}\left(\ket{0}\bra{0}\otimes I-i \ket{1}\bra{1}\otimes\mathcal{P}_{\ell i}^{Q}\right),\] (D.8)

with eigenvalues in \(\{-1,1\}\). Note that \(\beta_{\ell i}^{Q}\bra{\psi_{\ell 0}^{Q}}\hat{E}_{\ell i}^{Q}\ket{\psi_{\ell 0}^{Q}}= \frac{\partial C}{\partial\theta_{\ell i}^{Q}}\), and this is essentially a compact way of representing a Hadamard test for the relevant expectation value. Now consider a gradient estimator that first samples \((Q,\ell,i)\) with probability \(|\beta_{\ell i}^{Q}|/||\beta||_{1}\), then returns a one-sparse vector with \(g_{\ell i}^{Q}=\operatorname{sign}(\beta_{\ell i}^{Q})\left\|\beta\right\|_{1}m\), where \(m\) is the result of a single measurement of \(\hat{E}_{\ell i}^{Q}\) using the state \(\ket{\psi_{\ell 0}^{Q}}\). For this estimator we have

\[\mathbb{E}g_{\ell i}^{Q}=\operatorname{sign}(\beta_{\ell i}^{Q})\left\|\beta \right\|_{1}\frac{\left|\beta_{\ell i}^{Q}\right|}{\left\|\beta\right\|_{1}} \left\langle\psi_{\ell 0}^{A}\right|\hat{E}_{\ell i}^{Q}\ket{\psi_{\ell 0}^{A}} =\frac{\partial\mathcal{L}}{\partial\theta_{\ell i}^{Q}},\] (D.9)

where the expectation is taken over both the index sampling process and the quantum measurement. The procedure generates a valid gradient estimator.

In order to show convergence, one simply notes that by construction, \(\left\|g\right\|_{2}=\left\|\beta\right\|_{1}\). It then follows immediately from Lemma 8 that, with an appropriately chosen step size, Algorithm 1 achieves \(\mathbb{E}\mathcal{L}(\Theta)-\mathcal{L}(\Theta^{\star})\leq\varepsilon_{0}\) for a convex \(\mathcal{L}\) using

\[\frac{2\left\|\Theta^{(0)}-\Theta^{\star}\right\|_{2}^{2}\left\|\beta\right\|_ {1}^{2}}{\varepsilon_{0}^{2}}\] (D.10)

queries. For a \(\lambda\)-strongly convex \(\mathcal{L}\), only

\[\frac{2\left\|\beta\right\|_{1}^{2}}{\lambda\varepsilon_{0}}+1\] (D.11)

queries are required. The pre-processing in step 1 of Algorithm 1 requires time \(O(P\log P)\) and subsequently enables sampling in time \(O(1)\) using e.g. [111]10.

```
0: Alice: \(x,\{A_{\ell}\},\Theta_{L}^{(1)},\eta,T\). Bob: \(\{B_{\ell}\},\Theta_{B}^{(1)},\eta,T\).
0: Alice: Updated parameters \(\Theta_{A}^{(T)}\). Bob: Updated parameters \(\Theta_{B}^{(T)}\).
1:for\(t\in\{1,\ldots,T\}\)do
2:for\(\ell\in\{1,\ldots,L\}\)do
3: Alice prepares \(\tilde{O}(\log^{2}P\log N^{\prime}\log(L/\delta)/\varepsilon^{4})\) copies of \(\left|\psi_{\ell 0}^{A}(\Theta^{(t)})\right\rangle\) {\(O(L)\) rounds of communication}
4: Alice runs Shadow Tomography to estimate \(\{\mathbb{E}E_{\ell i}^{A}(\Theta^{(t)})\}_{i=1}^{P}\) up to error \(\varepsilon\), denoting these \(\{g_{\ell i}^{A}(\Theta^{(t)})\}_{i=1}^{P}\).
5: Bob prepares \(\tilde{O}(\log^{2}P\log N^{\prime}\log(L/\delta)/\varepsilon^{4})\) copies of \(\left|\psi_{\ell 0}^{B}(\Theta^{(t)})\right\rangle\) {\(O(L)\) rounds of communication}
6: Bob runs Shadow Tomography to estimate \(\{\mathbb{E}E_{\ell i}^{B}(\Theta^{(t)})\}_{i=1}^{P}\) up to error \(\varepsilon\), denoting these \(\{g_{\ell i}^{B}(\Theta^{(t)})\}_{i=1}^{P}\).
7: Alice sets \(\theta_{\ell}^{A(t+1)}\leftarrow\theta_{\ell}^{A(t)}-\eta g_{\ell}^{A}(\Theta ^{(t)})\).
8: Bob sets \(\theta_{\ell}^{B(t+1)}\leftarrow\theta_{\ell}^{B(t)}-\eta g_{\ell}^{B}(\Theta ^{(t)})\).
9:endfor
10:endfor ```

**Algorithm 2** Shadow Tomographic Distributed Gradient Descent

```
0: Alice: \(x,\{A_{\ell}\},\theta_{L}^{A(1)},\eta,T\). Bob: \(\{B_{\ell}\}\)
0: Alice: Updated parameters \(\Theta_{A}^{(T)}\).
1: Alice prepares \(\tilde{O}(\log^{2}(PT)\log N^{\prime}\log(1/\delta)/\varepsilon^{4})\) copies of \(\left|\mu_{L}^{A}\right\rangle\) {\(O(L)\) rounds of communication}
2:for\(t\in\{1,\ldots,T\}\)do
3: Alice runs online Shadow Tomography to estimate \(\{\mathbb{E}\tilde{E}_{Li}^{A}(\theta_{L}^{A(t)})\}\) up to error \(\varepsilon\), denoting these \(\{g_{Li}^{A}(\theta_{L}^{A(t)})\}\).
4: Alice sets \(\theta_{L}^{A(t+1)}\leftarrow\theta_{L}^{A(t)}-\eta g_{L}^{A}(\theta_{L}^{A(t )})\).
5:endfor ```

**Algorithm 3** Shadow Tomographic Distributed Fine-Tuning

### Algorithms based on Shadow Tomography

```
0: Alice: \(x,\{A_{\ell}\},\theta_{L}^{A(1)},\eta,T\). Bob: \(\{B_{\ell}\}\)
0: Alice: Updated parameters \(\Theta_{A}^{(T)}\).
1: Alice prepares \(\tilde{O}(\log^{2}(PT)\log N^{\prime}\log(1/\delta)/\varepsilon^{4})\) copies of \(\left|\mu_{L}^{A}\right\rangle\) {\(O(L)\) rounds of communication}
2:for\(t\in\{1,\ldots,T\}\)do
3: Alice runs online Shadow Tomography to estimate \(\{\mathbb{E}\tilde{E}_{Li}^{A}(\theta_{L}^{A(t)})\}\) up to error \(\varepsilon\), denoting these \(\{g_{Li}^{A}(\theta_{L}^{A(t)})\}\).
4: Alice sets \(\theta_{L}^{A(t+1)}\leftarrow\theta_{L}^{A(t)}-\eta g_{L}^{A}(\theta_{L}^{A(t )})\).
5:endfor ```

**Algorithm 4** Alice and Bob-based Algorithm

## Appendix E Communication Complexity of Linear Classification

While the separation in communication complexity for expressive networks can be quite large, interestingly we will show that for some of the simplest models this advantage can vanish due to the presence of structure. In particular, when a linear classifier is well-suited to a task such that the margin is large, the communication advantage will start to wane, while a lack of structure in linear classification will make the problem difficult for quantum algorithms as well. More specifically, we consider the following classification problem:

**Problem 7** (Distributed Linear Classification).: _Alice and Bob are given \(x,y\in S^{N}\), with the promise that \(|x\cdot y|\geq\gamma\) for some \(0\leq\gamma\leq 1\). Their goal is to determine the sign of \(x\cdot y\)._

This is one of the simplest distributed inference problem in high dimensions that one can formulate. \(x\) can be thought of as the input to the model, while \(y\) defines a separating hyperplane with some margin. Since with finite margin we are only required to resolve the inner product between the vectors to some finite precision, it might seem that an exponential quantum advantage should be possible for this problem by encoding the inputs in the amplitudes of a quantum state. However, we show that classical algorithms can leverage this structure as well, and consequently that the quantum advantage in communication that can be achieved for this problem is at most polynomial in \(N\). Weprove this with respect to the the randomized classical communication model, in which Alice and Bob are allowed to share random bits that are independent of their inputs 11.

Footnote 11: This resource can have a dramatic effect on the communication complexity of a problem. The canonical example is equality of \(N\) bit strings, which can be solved with constant success probability using \(1\) bit of communication and shared randomness, while requiring \(N\) bits of communication otherwise.

**Lemma 9**.: _The quantum communication complexity of Problem 7 is \(\Omega\left(\sqrt{N/\max(1,\lceil\gamma N\rceil)}\right)\). The randomized classical communication complexity of Problem 7 is \(O(\min(N,1/\gamma^{2}))\)._

Proof.: We first describe a protocol that allows Alice and Bob to solve the linear classification problem with margin \(\gamma\) using \(O(1/\gamma^{2})\) bits of classical communication and shared randomness, assuming \(\gamma>0\). Note that this bound accords with the notion that the margin rather than the ambient dimension sets the complexity of these types of problems, which is also manifest in the sample complexity of learning with linearly separable data.

Alice and Bob share \(kN\) bits sampled i.i.d. from a uniform distribution over \(\{0,1\}\), and that these bits are arranged in a \(k\times N\) matrix \(R\). Alice and Bob then receive \(x\) and \(y\) respectively, which are valid inputs to the linear classification problem with margin \(\gamma\). For any \(N\)-dimensional vector \(z\), define the random projection

\[f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{k},\quad f(z)=\frac{1}{k}(2R-1)z,\] (E.1)

where addition is element-wise. Applying the Johnson-Lindenstrauss lemma for projections with binary variables [8], we obtain that if \(k=C/\varepsilon^{2}\), for some absolute constant \(C\), then with probability larger than \(2/3\) we have for any \(z,z^{\prime}\in\{x,y,0\}\) (all of these being vectors in \(\mathbb{R}^{N}\)), \(f\) is an approximate isometry in the sense

\[(1-\varepsilon)\left\|z-z^{\prime}\right\|_{2}^{2}\leq\left\|f(z)-f(z^{\prime })\right\|_{2}^{2}\leq(1+\varepsilon)\left\|z-z^{\prime}\right\|_{2}^{2}.\] (E.2)

The key feature of this result is that \(k\) is completely independent of \(N\). Applying it repeatedly gives

\[\begin{array}{ll}\left\|f(x)-f(y)\right\|_{2}^{2}-\left\|f(x)\right\|_{2}^{ 2}-\left\|f(y)\right\|_{2}^{2}&\leq&(1+\varepsilon)\left\|x-y\right\|_{2}^{2} -2(1-\varepsilon)\\ f(x)\cdot f(y)&\geq&(1+\varepsilon)x\cdot y-2\varepsilon.\end{array}\] (E.3)

Obtaining an upper bound in a similar fashion using the converse inequalities, we have

\[(1+\varepsilon)x\cdot y-2\varepsilon\leq f(x)\cdot f(y)\leq(1-\varepsilon)x \cdot y+2\varepsilon.\] (E.4)

Assume now that \(x,y\) are valid inputs to the linear classification problem with margin \(\gamma\), and specifically that \(x\cdot y\geq\gamma\). The lower bound above gives

\[(1+\varepsilon)\gamma-2\varepsilon\leq f(x)\cdot f(y),\] (E.5)

and if we choose \(\varepsilon=\gamma/8\) we obtain

\[\frac{\gamma}{2}\leq(1+\frac{\gamma}{8})\gamma-\frac{\gamma}{4}\leq f(x) \cdot f(y),\] (E.6)

where we used \(\gamma\leq 1\). Similarly, if instead \(x\cdot y\leq-\gamma\) we obtain

\[f(x)\cdot f(y)\leq-(1-\frac{\gamma}{8})\gamma+\frac{\gamma}{4}\leq-\frac{ \gamma}{2}.\] (E.7)

It follows that if Alice computes \(f(x)\) and sends the resulting \(O(k)=O(1/\gamma^{2})\) bits that describe this vector to Bob (assuming some finite precision that is large enough so as not to affect the margin, which will contribute ), Bob can simply compute \(f(x)\cdot\bar{f}(y)\) which will reveal the result of the classification problem, which he can then communicate to Alice using a single bit.

If \(\gamma=0\) there is a trivial \(O(N)\) classical algorithm where Alice sends Bob \(x\).

We next describe the quantum lower bound for Problem 7. Denote by \(d_{H}\) the Hamming distance between binary vectors. We will use lower bounds for the following problem:

**Problem 8** (Gap Hamming with general gap).: _Alice and Bob are given \(\hat{x},\hat{y}\in\{0,1\}^{N}\) respectively. Given a promise that either \(d_{H}(\hat{x},\hat{y})\geq N/2+g/2\) or \(d_{H}(\hat{x},\hat{y})\leq N/2-g/2\), Alice and Bob must determine which one is the case._There is a simple reduction from Problem 8 to Problem 7 for certain values of \(\gamma\), which we will then use to obtain a result for all \(\gamma\). Assuming Alice is given \(\hat{x}\) and Bob is given \(\hat{y}\), they construct unit norm real vectors by \(x=(2\hat{x}-1)/\sqrt{N},y=(2\hat{y}-1)/\sqrt{N}\) with addition performed element-wise.

If \(d_{H}(x,y)\geq N/2+g/2\) then

\[x\cdot y = \!\!\!\!\sum_{i,x_{i}=y_{i}}\!\!\!\frac{1}{N}+\sum_{i,x_{i}\neq y _{i}}(-\frac{1}{N})\] (E.8) \[\geq \!\!\!\!\frac{N+g}{2}\frac{1}{N}+\frac{N-g}{2}(-\frac{1}{N})\] \[= \!\!\!\!\frac{g}{N}.\]

Similarly, \(d_{H}(\hat{x},\hat{y})\leq N/2-g/2\Rightarrow x\cdot y\leq-g/N\). It follows that \(x,y\) are valid inputs for a linear classification problem over the unit sphere with margin \(2g/N\). From the results of [88], any quantum algorithm for the Gap Hamming problem with gap \(g\in\{1,\ldots,N\}\) requires \(\Omega(\sqrt{N/g})\) qubits of communication. It follows that the linear classification problem requires \(\Omega(\sqrt{1/\gamma})\) qubits of communication. This bound holds for integer \(\gamma N\). To get a result for general \(0<\gamma\leq 1\) we simply note that the communication complexity must be a non-decreasing function of \(1/\gamma\), since any inputs which constitute a valid instance with some \(\gamma\) are also a valid instance for any \(\gamma^{\prime}<\gamma\). Given some real \(\gamma\), the resulting communication problem is at least as hard as the one with margin \(\left\lceil\gamma N\right\rceil/N\geq\gamma\). It follows that a \(\Omega(\sqrt{N/\left\lceil\gamma N\right\rceil})\) bound holds for all \(0<\gamma\leq 1\).

If \(\gamma=0\), by a similar argument we can apply the lower bound for \(\gamma=1/N\), implying that \(\Omega(\sqrt{N})\) qubits of communication are necessary. Once again there is only a polynomial advantage at best. 

## Appendix F Expressivity of quantum circuits

### Expressivity of compositional models

It is natural to ask how expressive models of the form of eq. (3.1) can be, given the unitarity constraint of quantum mechanics on the matrices \(\{A_{\ell},B_{\ell}\}\). This is a nuanced question that can depend on the encoding of the data that is chosen and the method of readout. On the one hand, if we pick \(\left|\psi(x)\right\rangle\) as in eq. (3.4) and use \(\{A_{\ell},B_{\ell}\}\) that are independent of \(x\), the resulting state \(\left|\varphi\right\rangle\) will be a linear function of \(x\) and the observables measured will be at most quadratic functions of those entries. On the other hand, one could map bits to qubits 1-to-1 and encode any reversible classical function of data within the unitary matrices \(\{A_{\ell}(x)\}\) with the use of extra space qubits. However, this negates the possibility of any space or communication advantages (and does not provide any real computational advantage without additional processing). As above, one prefers to work on more generic functions in the amplitude and phase space, allowing for an exponential compression of the data into a quantum state, but one that must be carefully worked with.

We investigate the consequences of picking \(\{A_{\ell}(x)\}\) that are _nonlinear_ functions of \(x\), and \(\{B_{\ell}\}\) that are data-independent. This is inspired by a common use case in which Alice holds some data or features of the data, while Bob holds a model that can process these features. Given a scalar variable \(x\), define \(A_{\ell}(x)=\operatorname{diag}((e^{-2\pi i\lambda_{\ell 1}x},\ldots,e^{-2\pi i \lambda_{\ell N^{\prime}}x}))\) for \(\ell\in\{1,\ldots,L\}\). We also consider parameterized unitaries \(\{B_{\ell}\}\) that are independent of the \(\{\lambda_{\ell i}\}\) and inputs \(x,y\), and the state obtained by interleaving the two in the manner of eq. (3.1) by \(\left|\varphi(x)\right\rangle\).

We next set \(\lambda_{\ell 1}=0\) for all \(\ell\in\{1,\ldots,L\}\) and \(\lambda_{L2}=0\). If we are interested in expressing the frequency

\[\Lambda_{\overline{j}}=\sum_{\ell=1}^{L-1}\lambda_{\ell j_{\ell}},\] (F.1)

where \(j_{\ell}\in\{2,\ldots,N^{\prime}\}\), we simply initialize with \(\left|\psi(x)\right\rangle=\left|+\right\rangle_{0}\left|0\right\rangle\) and use

\[B_{\ell}=\left|j_{\ell}-1\right\rangle\left\langle j_{\ell-1}-1\right|+\left| j_{\ell-1}-1\right\rangle\left\langle j_{\ell}-1\right|,\] (F.2)

with \(j_{1}=j_{L}=2\). It is easy to check that the resulting state is \(\left|\varphi(x)\right\rangle=\left(\left|0\right\rangle+e^{-2\pi i\Lambda_{ \overline{j}}x}\left|1\right\rangle\right)/\sqrt{2}\). Since the basis state \(\left|0\right\rangle\) does not accumulate any phase, while the \(B_{\ell}\)s swap the \(\left|1\right\rangle\) state with the appropriate basis state at every layer in order to accumulate a phase corresponding to a single summand in eq. (F.1). Choosing to measure the operator \(\mathcal{P}_{0}=X_{0}\), it follows that \(\left\langle\varphi(x)\right|X_{0}\left|\varphi(x)\right\rangle=\cos(2\pi\Lambda _{\overline{j}}x)\).

It is possible to express \(O((N^{\prime})^{L-1})\) different frequencies in this way, assuming the \(\Lambda_{\overline{j}}\) are distinct, which will be the case for example with probability \(1\) if the \(\{\lambda_{\ell_{i}}\}\) are drawn i.i.d. from some distribution with continuous support. This further motivates the small \(L\) regime where exponential advantage in communication is possible. These types of circuits with interleaved data-dependent unitaries and parameterized unitaries was considered for example in [104], and is also related to the setting of quantum signal processing and related algorithms [74, 77]. We also show that such circuits can express dense function in Fourier space, and for small \(N\) we additionally find that these circuits are universal function approximators (Appendix F.2), though in this setting the possible communication advantage is less clear.

The problem of applying nonlinearities to data encoded efficiently in quantum states is non-trivial and is of interest due to the importance of nonlinearities in enabling efficient function approximation [76]. One approach to resolving the constraints of unitarity with the potential irreversibility of nonlinear functions is the introduction of slack variables via additional ancilla qubits, as typified by the techniques of block-encoding [29, 41]. Indeed, these techniques can be used to apply nonlinearities to amplitude encoded data efficiently, as was recently shown in [100]. This approach can be applied to the distributed setting as well. Consider the communication problem where Alice is given \(x\) as input and Bob is given unitaries \(\{U_{1},U_{2}\}\) over \(\log N\) qubits. Denote by \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) a nonlinear function such as the sigmoid, exponential or standard trigonometric functions, and \(n=2^{N}\). We show the following:

**Lemma 10**.: _There exists a model \(\left|\varphi_{\sigma}\right\rangle\) of the form definition 3.1 with \(L=O(\log 1/\varepsilon),N^{\prime}=2^{n^{\prime}}\) where \(n^{\prime}=2n+4\) such that \(\left|\varphi_{\sigma}\right\rangle=\alpha\left|0\right\rangle^{\otimes n+4} \left|\hat{y}\right\rangle+\left|\phi\right\rangle\) for some \(\alpha=O(1)\), where \(\left|\hat{y}\right\rangle\) is a state that obeys_

\[\left\|\left|\hat{y}\right\rangle-\left|U_{2}\frac{1}{\left\|\sigma(U_{1}x) \right\|_{2}}\sigma(U_{1}x)\right\rangle\right\|_{2}<\varepsilon.\] (F.3)

\(\left|\phi\right\rangle\) _is a state whose first \(n+4\) registers are orthogonal to \(\left|0\right\rangle^{\otimes n+4}\)._

Proof: Appendix B.

This result implies that with constant probability, after measurement of the first \(n+4\) qubits of \(\left|\varphi_{\sigma}\right\rangle\), one obtains a state whose amplitudes encode the output of a single hidden layer neural network. It may also be possible to generalize this algorithm and apply it recursively to obtain a state representing a deep feed-forward network with unitary weight matrices.

It is also worth noting that the general form of the circuits we consider resembles self-attention based models with their nonlinearities removed (motivated for example by [107]), as we explain in Appendix F.3. Finally, in Appendix F.4 we discuss other strategies for increasing the expressivity of these quantum circuits by combining them with classical networks.

### Additional results on oscillatory features

Extending the unitaries considered in Appendix F.1 to more than one variable, for two scalar variables \(x,y\) define

\[A_{\ell}(x)= \mathrm{diag}((e^{-2\pi i\lambda_{\ell_{1}}x},\ldots,e^{-2\pi i \lambda_{\ell N^{\prime}}x})),\] (F.4a) \[A_{\ell}(x,y)= \mathrm{diag}((e^{-2\pi i\lambda_{\ell_{1}}x},\ldots,e^{-2\pi i \lambda_{\ell N^{\prime}/2}x},e^{-2\pi i\lambda_{\ell,N^{\prime}/2+1}y}, \ldots,e^{-2\pi i\lambda_{\ell N^{\prime}}y}))\] (F.4b)

for \(\ell\in\{1,\ldots,L\}\). Once again \(\{B_{\ell}\}\) are data-independent unitaries, and we denote by \(\left|\varphi(x)\right\rangle,\left|\varphi(x,y)\right\rangle\) the states defined by interleaving these unitaries in the manner of eq. (3.1), and by \(\mathcal{L}_{1},\mathcal{L}_{2}\) the corresponding loss functions when measuring \(X_{0}\).

While the circuits in Appendix F.1 enable one to represent a small number of frequencies from a set that is exponential in \(L\), one can easily construct circuits that are supported on an exponentially large number of frequencies, as detailed in Lemma 11. We also use measures of expressivity of classical neural networks known as _separation rank_ to show that circuits within the class eq. (3.1) can represent complex correlations between their inputs. For a function \(f\) of two variables \(y,z\), its separation rank is defined by

\[\mathrm{sep}(f)\equiv\min\left\{R:f(x)=\sum_{i=1}^{R}g_{i}(y)h_{i}(z)\right\}.\] (F.5)

If for example \(f\) cannot represent any correlations between \(y\) and \(z\), then \(\mathrm{sep}(f)=1\). When computed for certain classes of neural networks, \(y,z\) are taken to be subsets of a high-dimensional input. The separation rank can be used for example to quantify the inductive bias of convolutional networks towards learning local correlations [33], the effect of depth in recurrent networks [69], and the ability of transformers to capture correlations across sequences as a function of their depth and width [70].

We find that the output of estimating an observable using circuits of the form eq. (F.4) can be supported on an exponential number of frequencies, and consequently has a large separation rank:

**Lemma 11**.: _For \(\{\lambda_{\ell i}\}\) drawn i.i.d. from any continuous distribution and parameterized unitaries \(\{B_{\ell}\}\) such that the real and imaginary parts of each entry in these matrices is a real analytic function of parameters \(\Theta\) drawn from a subset of \(\mathbb{R}^{PL}\), aside from a set of measure \(0\) over the choice of \(\{\lambda_{\ell i}\},\{B_{\ell}\}\),_

1. _The number of nonzero Fourier components in_ \(\mathcal{L}_{1}\) _is_ \(\left(\frac{N^{\prime}(N^{\prime}-1)}{2}\right)^{L-1}N^{\prime}\)_._
2. \[\mathrm{sep}(\mathcal{L}_{2})=2\left(\frac{N^{\prime}(N^{\prime}-1)}{2}\right) ^{L-1}N^{\prime}.\] (F.6)

Proof: Appendix B

This almost saturates the upper bound on the number of frequencies that can be expressed by a circuit of this form that is given in [104]. The separation rank implies that complex correlations between different parts of the sequence can in principle be represented by such circuits. The constraint on \(\{B_{\ell}\}\) is quite mild, and applies to standard choices of parameterize unitaries.

The main shortcoming of a result such as Lemma 11 is that it is not robust to measurement error as it is based on constructing states that are equal weight superpositions of an exponential number of terms. It is straightforward to show that circuits of this form can serve as universal function approximators, at least for a small number of variables. For high-dimensional functions it is unclear when a communication advantage is possible, as we describe below.

**Lemma 12**.: _Let \(f\) be a \(p\)-times continuously differentiable function with period \(1\), and denote by \(\hat{f}_{:M}\) the vector of the first \(M\) Fourier components of \(f\). If \(\left\|\hat{f}_{:M}\right\|_{1}=1\) then there exists a circuit of the form eq. (3.1) over \(O(\log M)\) qubits such that_

\[\left\|\mathcal{L}-f\right\|_{\infty}\leq\frac{C}{M^{p-1/2}}\] (F.7)

_for some absolute constant \(C\)._

Proof: Appendix B

This result improves upon the result in [93, 104] about universal approximation with similarly structured circuits both because it is non-asymptotic and because it shows uniform convergence rather than convergence in \(L_{2}\). Non-asymptotic results universal approximation results were also obtained recently by [43], however their approximation error scales polynomially with the number of qubits, as opposed to exponentially as in Lemma 12.

The result of Lemma 12 applies to an \(L=1\) circuit. The special hierarchical structure of the Fourier transform implies that the same result can be obtained using even simpler circuits with larger \(L\). Consider instead single-qubit data-dependent unitaries over \(L+1\) qubits that take the form

\[A_{\ell}=\left|0\right\rangle_{0}\left\langle 0\right|_{0}+\left|1\right\rangle_{ 0}\left\langle 1\right|_{0}\left(\left|0\right\rangle_{\ell+1}\left\langle 0 \right|_{\ell+1}+e^{2\pi i2^{\ell-1}x}\left|1\right\rangle_{\ell+1}\left\langle 1 \right|_{\ell+1}\right),\] (F.8)for \(\ell\in\{1,\ldots,L\}\). This is simply a single term in a hierarchical decomposition of the same feature matrix we had in the shallow case, since

\[\prod_{\ell=1}^{L}\!A_{\ell}=\ket{0}_{0}\bra{0}_{0}\otimes I_{1:L}+\ket{1}_{0} \bra{1}_{0}\otimes I_{1}\otimes\left(\sum_{m=0}^{2^{L}-1}\!e^{2\pi imx}\ket{m} \bra{m}\right),\] (F.9)

which is identical to eq. (B.45). As before, set

\[B_{1}=\big{|}\hat{f}\big{>}\bra{0}+\ket{0}\big{>}\hat{f}\big{|},\] (F.10)

with \(N^{\prime}/4=2^{L}\) and \(B_{\ell}=I\) for \(\ell>1\). This again gives an approximation of \(f\) up to normalization. The data-dependent unitaries are particularly simple when decomposed in this way. The fact that they act on a single qubit and thus have "small width" is reminiscent of classical depth-separation result such as [32], where it is shown that (roughly speaking) within certain classes of neural network, in order to represent the function implemented by a network of depth \(L\), a shallow network must have width exponential in \(L\). In this setting as well the expressive power as measured by the convergence rate of the approximation error grows exponentially with \(L\) by eq. (B.44).

The circuits above can be generalized in a straightforward way to multivariate functions of the form \(f:[-1/2,1/2]^{D}\to\mathbb{R}\) and combined with multivariate generalization of eq. (B.44). In this case the scalar \(m\) is replaced by a \(D\)-dimensional vector taking \(M^{D}\) possible values, and we can define

\[A_{1}(x)=\ket{0}_{0}\bra{0}_{0}\otimes I_{1:D\log M-1}+\ket{1}_{0}\bra{1}_{0} \otimes I_{1}\otimes\left(\sum_{m\in[M]^{D}}e^{2\pi im\cdot x}\ket{m}\bra{m} \right).\] (F.11)

Note that using this feature map, the number of neurons is linear in the spatial dimension \(D\). Because of this, such circuits are not strictly of the form eq. (3.1) for general \(N\) since it is _not_ the case that \(\log N^{\prime}=O(\log N)\) where \(N^{\prime}\) is the Hilbert space on which the unitaries in the circuit act and \(N\) is the size of \(x\). An alternative setting where the features themselves are also learned from data could enable much more efficient approximation of functions that are sparse in Fourier space.

### Unitary Transformers

Transformers based on self-attention [109] form the backbone of large language models [25; 16] and foundation models more generally [20]. A self-attention layer, which is the central component of transformers, is a map between sequences in \(\mathbb{R}^{S\times N^{\prime}}\) (where \(S\) is the sequence length) defined in terms of weight matrices \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{N^{\prime}\times N^{\prime}}\), given by

\[X^{\prime}(X)=\mathrm{softmax}\left(\frac{XW_{Q}W_{K}^{T}X^{T}}{\sqrt{N}} \right)XW_{V}\equiv A(X)XW_{V},\] (F.12)

where \(\mathrm{softmax}(x)_{i}=e^{x_{i}}/\!\sum\limits_{i}e^{x_{i}}\) for a vector \(x\), and acts row-wise on matrices. There is an extensive literature on replacing the softmax-based attention matrix \(A(X)\) with matrices that can be computed more efficiently, which can markedly improve the time complexity of inference and training without a significant effect on performance [63; 70]. In some cases \(A(X)\) is replaced by a unitary matrix [68]. Remarkably, recent work shows that models without softmax layers can in fact outperform standard transformers on benchmark tasks while enabling faster inference and a reduced memory footprint [107].

Considering a simplified model that does not contain the softmax operation as in [70] and dropping normalization factors, the linear attention map is given by

\[X^{\prime}_{\mathrm{lin}}(X)=XW_{Q}W_{K}^{T}X^{T}XW_{V}.\] (F.13)

Iterating this map twice gives

\[X^{\prime}_{\mathrm{lin}}(X^{\prime}_{\mathrm{lin}}(X))=\begin{array}{c}XW_{ Q}^{(1)}W_{K}^{(1)T}X^{T}XW_{V}^{(1)}W_{Q}^{(2)}W_{K}^{(2)T}W_{V}^{(1)T}X^{T}X*\\ W_{K}^{(1)}W_{Q}^{(1)T}X^{T}XW_{Q}^{(1)}W_{K}^{(1)T}X^{T}XW_{V}^{(1)}W_{V}^{(2)}.\end{array}\] (F.14)

Iterating this map \(K\) times (with different weight matrices at each layer) gives a function of the form:

\[X^{(K)}_{\mathrm{lin}}(X)=XR_{0}\prod_{\ell=1}^{(3^{K}-1)/2}\left(X^{T}XR_{ \ell}\right),\] (F.15)where the \(\{R_{\ell}\}\) matrices depend only on the trainable parameters. If we now constrain these to be parameterized unitary matrices, and additionally replace \(X^{T}X\) with a unitary matrix \(U_{X}\) encoding features of the input sequence itself, then the \(i\)-th row of the output of this model is encoded in the amplitudes of a state of the form eq.3.5 with \(L=(3^{K}-1)/2+1,\ket{\psi(x)}=\ket{X_{i}},A_{\ell}(x)=U_{X},B_{\ell}=R_{\ell}\).

### Ensembling and point-wise nonlinearities

An additional method for increasing expressivity while maintaining an advantage in communication is through ensembling. Given \(K\) models of the form Definition3.1 with \(P\) parameters each, one can combine their loss functions \(\mathcal{L}_{1},\ldots,\mathcal{L}_{K}\) into any differentiable nonlinear function

\[\tilde{\mathcal{L}}(\mathcal{L}_{1}(\Theta_{1},x),\ldots,\mathcal{L}_{K}( \Theta_{K},x),\tilde{\Theta},x)\] (F.16)

that depends on additional parameters \(\tilde{\Theta}\). As long as \(K\) and \(\ket{\tilde{\Theta}}\) scale subpolynomially with \(N\) and \(P\), the gradients for this more expressive model can be computed while maintaining the exponential communication advantage in terms of \(N,P\).

## Appendix G Realizing quantum communication

Given the formidable engineering challenges in building a large, fault tolerant quantum processor [11, 44], the problem of exchanging coherent quantum states between such processors might seem even more ambitious. We briefly outline the main problems that need to be solved in order to realize quantum communication and the state of progress in this area, suggesting that this may not be the case.

We first note that sending a quantum state between two processors can be achieved by the well-known protocol of quantum state teleportation [18, 45]. Given an \(n\) qubit state \(\ket{\psi}\), Alice can send \(\ket{\psi}\) to Bob by first sharing \(n\) Bell pairs of the form

\[\ket{b}=\frac{1}{\sqrt{2}}\left(\ket{0}\ket{0}+\ket{1}\ket{1}\right),\] (G.1)

(sharing such a state involves sending a one of the two qubits to Bob) and subsequently performing local processing on the Bell pairs and exchanging \(n\) bits of classical communication. Thus quantum communication can be reduced to communicating Bell pairs up to a logarithmic overhead, and does not require say transmitting an arbitrary quantum state in a fault tolerant manner, which appears to be a daunting challenge given the difficulty of realizing quantum memory on a single processor. Bell pairs can be distributed by a third party using one-way communication.

In order to perform quantum teleportation, the Bell pairs must have high fidelity. As long as the fidelity of the communicated Bell pairs is above \(.5\), purification can be used produce high fidelity Bell pairs [19], with the fidelity of the purified Bell pair increasing exponentially with the number of pairs used. Thus communicating arbitrary quantum states can be reduced to communicating noisy Bell pairs.

Bell pair distribution has been demonstrated across multiple hardware platforms including superconducting waveguides [75], optical fibers [64], free space optics at distances of over \(1,200\) kilometers [71]. At least in theory, even greater distances can be covered by using quantum repeaters, which span the distance between two network nodes. Distributing a Bell pair between the nodes can then be reduced to sharing Bell pairs only between adjacent repeaters and local processing [12].

A major challenge in implementing a quantum network is converting entangled states realized in terms of photons used for communication to states used for computation and vice versa, known as transduction [66]. Transduction is a difficult problem due to the several orders of magnitude in energy that can separate optical photons from the energy scale of the platform used for computation. Proof of principle experiments have been performed across a number of platforms including trapped ions [64], solid-state systems [95], and superconducting qubits operating at microwave frequencies [14, 112].

## Appendix H Privacy of Quantum Communication

In addition to an advantage in communication complexity, the quantum algorithms may potentially lead to advantages in terms of privacy. It is well known that the number of bits of information that can be extracted from an unknown quantum state is proportional to the number of qubits. It follows immediately that since the above algorithm requires exchanging a logarithmic number of copies of states over \(O(\log N)\) qubits, even if all the communication between the two players is intercepted, an attacker cannot extract more than a logarithmic number of bits of classical information about the input data or model parameters. Specifically, we have:

**Corollary 1**.: _If Alice and Bob are implementing the quantum algorithm for gradient estimation described in Lemma 2, and all the communication between Alice and Bob is intercepted by an attacker, the attacker cannot extract more than \(\tilde{O}(L^{2}(\log N)^{2}(\log P)^{2}\log(L/\delta)/\varepsilon^{4})\) bits of classical information about the inputs to the players._

This follows directly from Holevo's theorem [52], since the multiple copies exchanged in each round of the protocol can be thought of as a quantum state over \(\tilde{O}((\log N)^{2}(\log P)^{2}\log(L/\delta)/\varepsilon^{4})\) qubits. As noted in [3], this does not contradict the fact that the protocol allows one to estimate all \(P\) elements of the gradient, since if one were to place some distribution over the inputs, the induced distribution over the gradient elements will generally exhibit strong correlations. An analogous result holds for the inference problem described in Lemma 1.

It is also interesting to ask how much information either Bob or Alice can extract about the inputs of the other player by running the protocol. If this amount is logarithmic as well, it provides additional privacy to both the model owner and the data owner. It allows two actors who do not necessarily trust each other, or the channel through which they communicate, to cooperate in jointly training a distributed model or using one for inference while only exposing a vanishing fraction of the information they hold.

It is also worth mentioning that data privacy is also guaranteed in a scenario where the user holding the data also specifies the processing done on the data. In this setting, Alice holds both data \(x\) and a full description of the unitaries she wishes to apply to her state. She can send Bob a classical description of these unitaries, and as long as the data and features are communicated in the form of quantum states, only a logarithmic amount of information can be extracted about them. In this setting there is of course no advantage in communication complexity, since the classical description of the unitary will scale like \(\mathrm{poly}(N,P)\).

## Appendix I Some Open Questions

### Expressivity

Circuits that interleave parameterized unitaries with unitaries that encode features of input data are also used in Quantum Signal Processing [74, 77], where the data-dependent unitaries are time evolution operators with respect to some Hamiltonian of interest. The original QSP algorithm involved a single parameterized rotation at each layer, and it is also known that extending the parameter space from \(U(1)\) to \(SU(2)\) by including an additional rotation improves the complexity of the algorithm and improves its expressivity [85]. In both cases however the expressive power (in terms of the degree of the polynomial of the singular values that can be expressed) grows only linearly with the number of interleaved unitaries. Given the natural connection to the distributed learning problems considered here, it is interesting to understand the expressive power of such circuits with more powerful multi-qubit parameterized unitaries.

We present a method of applying a single nonlinearity to a distributed circuit using the results of [100]. Since this algorithm requires a state-preparation unitary as input and produces a state with a nonlinearity applied to the amplitudes, it is natural to ask whether it can be applied recursively to produce a state with the output of a deep network with nonlinearities encoded in its amplitudes. This will require extending the results of [100] to handle noisy state-preparation unitaries, yet the effect of errors on compositions of block encodings [29, 41], upon which these results are based, is relatively well understood. It is also worth noting that these approaches rely on the approximation of nonlinear functions by polynomials, and so it may also be useful to take inspiration directly from classical neural network polynomial activations, which in some settings are known to outperform other types of nonlinearities [80].

### Optimization

The results of Appendix D rely on sublinear convergence rates for general stochastic optimization of convex functions (Lemma 8). It is known however that using additional structure, stochastic gradients can be used to obtain linear convergence (meaning that the error decays exponentially with the number of iterations). This is achievable when subsampling is the source of stochasticity [67], or with occasional access to noiseless gradients in order to implement a variance reduction scheme [60, 84, 46], neither of which seem applicable to the setting at hand. It is an interesting open question to ascertain whether there is a way to exploit the structure of quantum circuits to obtain linear convergence rates using novel algorithms. Aside from advantages in time complexity, this could imply an exponential advantage in communication for a more general class of circuits.

Conversely, it is also known that given only black-box access to a noisy gradient oracle, an information-theoretic lower bound of \(\Omega(1/T)\) on the error holds given \(T\) oracle queries, precluding linear convergence without additional structure, even for strongly convex objectives [9]. [49] provide a similar lower bound for their algorithm, at least for a restricted class of circuits. Perhaps these results be used to show optimality of algorithms that rely on the standard variational circuit optimization paradigm that involves making quantum measurements at every iteration and using these to update the parameters. This might imply that linear convergence is only possible if the entire optimization process is performed coherently.

In this context, we note that the treatment of gradient estimation at every layer and every iteration as an independent shadow tomography problem is likely highly suboptimal, since no use is made of the correlations across iterations between the states and the observables of interest. While in Appendix D.2 this is not the case, that algorithm applies only to fine-tuning of a single layer. Is there a way to re-use information between iterations to reduce the resource requirements of gradient descent using shadow tomography? One approach could be warm-starting the classical resource states by reusing them between iterations. Improvements along these lines might find applications for other problems as well.

### Exponential advantage under weaker assumptions

The lower bound in Lemma 3 applies to circuits that contain general unitaries, and thus have depth \(\mathrm{poly}(N)\) when compiled using a reasonable gate set. One can ask whether the lower bound can be strengthened to apply to more restricted classes of unitaries as well, and in particular \(\log\)-depth unitaries. While it is known that exponential communication advantages require the circuits to have \(\mathrm{poly}(N)\) gate complexity overall [7], this does not rule out the possibility of computational separations resulting from the clever encoding and transmission of states nor does it rule out communication advantages resulting from very short time preparations from \(\log\)-depth protocols. The rapid growth of complexity of random circuits composed from local gates with depth suggests that this might be possible [24]. This is particularly interesting since Algorithm 1 requires only a single measurement per iteration and may thus be suitable for implementation on near-term devices whose coherence times restrict them to implementing shallow circuits. It has also been recently shown that an exponential quantum advantage in communication holds for a problem which is essentially equivalent to estimating the loss of a circuit of the form Definition 3.1 with \(L=2\), under a weaker model of quantum communication than the standard one we consider [10]. This is the one-clean-qubit model, in which the initial state \(|\psi(x)\rangle\) consists of a single qubit in a pure state, while all other qubits are in a maximally mixed state.

## Appendix J Experiments additional details

### Node classification training

We use the same training regime for all datasets using the recommended hyperparameters in DGL [113] examples, reported in Table 3.

We trained each model 10 times for all three datasets using a single NVIDIA RTX A6000, taking approximately 15 minutes per execution.

### Graph classification training

As, to the best of our knowledge, we are the first to use a SIGN variant on graph classification tasks, we conducted a comprehensive hyperparameter tuning for the model structure (including the number of message passing operators, the hidden dimension, and normalization after the hidden layer) and optimization settings. The tuning was performed using Bayesian hyperparameter optimization to identify the optimal values for each dataset. This process involved varying the hidden dimension, the number of SIGN hops per operation, the learning rate, and dropout rates. The values considered for each hyperparameter are detailed in Table 4. The full results of these experiments are in Table 5.

We scan each task for approximately 150 runs, using a single NVIDIA RTX A6000.

### Empirical bounds

We measure \(\left\|W_{1}\right\|\) and \(\left\|W_{2}\right\|_{\infty}\) of the trained graph classification models in Section 5.2.1, corresponding to Equation (4.2) and report the average results over 10 runs in Table 6 (note that we use \(P=I\) so that no pooling matrix is present, and in any case the pooling window will typically be a small constant). \(W_{1}\) is constructed as a block diagonal matrix of the weights of the SIGN hidden layer. \(\left\|W_{2}\right\|_{\infty}\) is the infinity norm of the weight matrix of the output layer of SIGN, multiplied by 2 (since we compute differnces between numbers of nodes in two classes).

We measure the score difference of the graph classification task in Section 5.2.1 and compare them to the differences of the class sizes in Figure 3. Most of the differences are significant (larger than \(1/\mathrm{poly}(N)\) where \(N\) is the number of nodes; see fig. 3(c)). Some class pairs have low differences making them indistinguishable, however, fig. 3(a),(b) indicate those are typically classes with similar

\begin{table}
\begin{tabular}{l c} \hline \hline Hyperparameter & Value \\ \hline Hidden dimension & 512 \\ SIGN hops & 5 \\ Learning rate & 0.001 \\ Input dropout & 0.3 \\ Hidden dropout & 0.4 \\ Weight decay & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyper parameters of node classification training

\begin{table}
\begin{tabular}{l c} \hline \hline Hyperparameter & Values \\ \hline Hidden dimension & 8,12,16,32,64,96,128,148,256 \\ SIGN hops (per operation) & [0-10] \\ Learning rate & 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1 \\ Input dropout & 0.0 \\ Hidden dropout & 0.0, 0.1, 0.2, 0.3, 0.4, 0.5 \\ Weight decay & 0.0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4 \\ Batch size & 32, 64, 128, 256, 512, 1024 \\ Normalization layer & BatchNorm, LayerNorm, none \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyper parameters of node classification training

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{Dataset} \\ \cline{2-10}  & MUTAG & PTC & NCI1 & PROTEINS & COLLAB & IMDB-B & IMDB-M & REDDIT-B & REDDIT-M \\ \hline GIN [141] & 89.40\(\pm\)5.60 & 64.60\(\pm\)7.0 & 82.17\(\pm\)1.7 & 76.2 \(\pm\) 2.8 & 80.2 \(\pm\)1.90 & 75.1 \(\pm\)5.1 & 52.3 \(\pm\)2.8 & 92.4 \(\pm\)2.5 & 57.5\(\pm\)1.5 \\ DropGN[10] & 90.4 \(\pm\)7.0 & 6.3 \(\pm\)8.6 & - & 76.3 \(\pm\)6.1 & - & 75.7 \(\pm\)4.2 & 51.4 \(\pm\)2.8 & - & - \\ DGCNN[10] & 85.8 \(\pm\)1.7 & 58.6 \(\pm\)2.5 & - & 75.5 \(\pm\)0.9 & - & 70.0 \(\pm\)0.9 & 47.8 \(\pm\)0.9 & - & - \\ U2GNN [11] & 89.97\(\pm\)3.65 & 69.63\(\pm\)3.60 & - & 78.53\(\pm\)0.47 & 77.84\(\pm\)1.48 & 77.04\(\pm\)3.45 & 53.60\(\pm\)3.53 & - & - \\ HCP-SL[11] & - & - & 78.45\(\pm\)0.7 & 78.41\(\pm\)6.2 & - & - & - & - \\ WKPL[120] & 88.30\(\pm\)2.6 & 68.10\(\pm\)2.4 & 87.5 \(\pm\)0.5 & 78.5\(\pm\)0.4 & - & 75.1 \(\pm\)1.1 & 49.5 \(\pm\) 0.4 & - & 59.5 \(\pm\) 0.6 \\ \hline SIGN (ours) & 92.02\(\pm\)6.45 & 68.0 \(\pm\)8.17 & 77.25\(\pm\)1.42 & 76.55\(\pm\)5.10 & 81.82\(\pm\)1.42 & 76 \(\pm\)2.49 & 53.13\(\pm\)3.01 & 78.95\(\pm\)2.72 & 54.09\(\pm\)1.76 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Graph Classification Test Accuracy. Our model achieves comparable results to GIN and other known models on most datasets.

number of nodes. This provides evidence that when there is a considerable class imbalance (i.e. one that scales with system size), the magnitude of the model output when computing this difference will not decay with the size of the graph.

While evidence of asymptotic scaling will require experiments on graphs of different sizes, our results suggest that the upper bound in Lemma 6 is not large for models trained on standard benchmarks, implying that they can be efficiently simulated on a quantum computer.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \multicolumn{3}{c}{Dataset} \\ \cline{2-4} Value & OGBN-Products & Reddit & Cora \\ \hline \(\left\|W_{1}\right\|\) & 3.46 \(\pm\) 0.27 & 1.32 \(\pm\) 0.12 & 8.5 \(\pm\) 8.0 \\ \(\left\|W_{2}\right\|_{\infty}\) & 0.13 \(\pm\) 0.02 & 0.04 \(\pm\) 0.00 & 0.1 \(\pm\) 0.07 \\ \hline \#nodes & 2,449,029 & 232,965 & 2708 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Weight norms of the graph classification models. We measure the norms of the final decision problem models, averaging the values over 8 runs.

Figure 3: (a): Difference between class sizes in ogbn-products test set. (b) Difference between the graph classification model class scores. The score differences are correlated to the class size differences. (c) Histogram of the class pairs differences. Most of the differences are significantly larger than \(1/N\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See end of introduction for list of contributions and pointers to additional details and proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See the Discussion section for the main limitations we have identified. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Full proofs are given in the appendices (mostly Appendix B). Assumptions are stated in Lemma and Theorem statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experiments are explained in Section 5 and Appendix J. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: All datasets are publicly available in DGL library. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Hyperparameters and hyper parameters tuning ranges are mentioned in Appendix J. Data splits are either fixed or identical to the DGL examples default. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experimental results include standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The used computation resources are mentioned in Appendix J. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer:[Yes] Justification: The code of ethics has been conformed to. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work applies to models of computation that have not yet been realized. It serves to motivate further study of these and will have no societal impact within the next several years. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No risk of misuse (see broader impacts answer). Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets are from DGL library. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not relevant. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not relevant. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.