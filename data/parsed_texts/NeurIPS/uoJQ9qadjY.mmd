# Learning to Reason Iteratively and Parallelly for

Complex Visual Reasoning Scenarios

 Shantanu Jaiswal\({}^{1,2}\)  Debaditya Roy\({}^{2}\)  Basura Fernando\({}^{2,3}\)  Cheston Tan\({}^{2,3}\)

\({}^{1}\) Carnegie Mellon University \({}^{2}\) IHPC, A*STAR Singapore

\({}^{3}\) Centre for Frontier AI Research, A*STAR Singapore

Correspondence to: sjaiswa3@cs.cmu.edu

###### Abstract

Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural _Iterative_ and _Parallel Reasoning Mechanism_ (IPRM) that combines two distinct forms of computation - iterative and parallel - to better address complex VQA scenarios. Specifically, IPRM's _"iterative"_ computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query _"determine the color of pen to the left of the child in red t-shirt sitting at the white table"_). Meanwhile, its _"parallel"_ computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of operations that are mutually independent (e.g. when counting individual colors for the query: _"determine the maximum occurring color amongst all t-shirts"_). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM's internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors. Source code at: https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism

## 1 Introduction

Visual reasoning and question answering (VQA) at its core requires a model to identify relevant visual operations, execute those operations, and then combine their results to make an inference. Complex visual reasoning scenarios (depicted in fig. 1) are particularly challenging in this regard. They require models to reason compositionally over a large number of reasoning steps and to engage in a variety of higher-level reasoning operations such as causal linking, logical reasoning, and spatiotemporal processing that extend beyond core perception capabilities. In this context, two powerful computational priors exist - iterative and parallel. While each has its own limitations, when combined, they can complement each other and effectively address the challenges of complex VQA tasks. Specifically, **iterative computation**, wherein individual operations are identified and composed in a step-by-step manner, is a beneficial prior for multi-step reasoning scenarios explored by past VQA works [28, 25, 18]. However, pure iterative computation can exhibit limitations inscenarios wherein the entailed visual operations are independent of one-another, or where distinct stimuli need to be processed and tracked simultaneously.

For example, consider the first scenario shown in fig. 1. When executing the language phrase _"maximum occurring shape"_ (i.e. _"what shape appears the most"_), a purely iterative method would: (i) compute the count of each shape (each of which itself could take multiple iterations), (ii) then update and maintain the counts in memory (without forgetting count of all previous shapes), and (iii) finally, recall each shape's count to compute the _"maximum"_. Besides taking more reasoning steps than required, such computation also increases the demand for information retention and recall in memory, which in this scenario could scale by the number of shapes to be counted. In complex video reasoning scenarios, a purely iterative method would similarly struggle in tracking and reasoning over multiple co-occurring events. In such scenarios, from both efficiency and efficacy perspectives, it is advantageous to process operations or stimuli in parallel, rather than solely iteratively.

More generally, **parallel computation** facilitates the simultaneous maintenance and exploration of distinct reasoning paths, and thereby enables reasoning to be more comprehensive, efficient and robust. For example, to compute _"maximum occuring shape"_, parallel compute can enable distinct shape queries to be simultaneously computed prior to computing the _"maximum"_ operation. Similarly, it is effective for other scenarios illustrated in fig. 1, such as in processing independent logical clauses (_"[are X] and [Y made of plastic]"_) or when tracking and processing co-occurring events in videos.

Such computation can be implicitly realized in conventional transformer-based parallel attention mechanisms [70]. However, transformer-based attention does not explicitly incorporate iterative compositional computation [14, 41], which as described is beneficial for multi-step reasoning scenarios wherein operations need to be composed sequentially. Accordingly, while parallel computation may effectively compute the result of _"maximum occurring shape"_ in fig. 1, it would potentially struggle to integrate the result with further operations such as _"green object with..."_, _"small object in front of green..."_, and _"color of..."_ that need to be computed step-by-step to answer the question.

Based on the above insights, we design the **Iterative and Parallel Reasoning Mechanism (IPRM),** a novel neural reasoning architecture that combines step-by-step iterative computation with the ability to process multiple independent operations and stimuli simultaneously. Inspired by how humans utilize working memory [71, 72] to facilitate complex reasoning, IPRM internally maintains a latent memory of parallel "operation states", keyed to which are "results states". Given vision and language inputs, IPRM performs the following _iterative_ computation. First, it forms a new set of _parallel_ operations by retrieving relevant language information conditioned on its prior operation states. Then, it "executes" these operations _parallelly_ by retrieving relevant visual information conditioned on its new operations as well as prior result states. Finally, it integrates its new operations (and their results) into memory by dynamically composing these operations with one-another as well as prior operation states, and subsequently, repeats the entire process in its next _iterative_ step.

This strategy effectively enables us to take advantage of both parallel and iterative computations and notably helps improve state-of-arts across various complex image and video reasoning tasks using a single reasoning mechanism. Equally importantly, IPRM's internal computations can be visualized

Figure 1: Complex VQA scenarios (CLEVR-Humans [33], GQA [29], CLEVRER-Humans [51]), AGQA[20] and STAR[79]) wherein combination of iterative (step-by-step) computation (blue phrases) and parallel computation (orange phrases) can be beneficial for reasoning.

across reasoning steps, which helps better interpret what operations it was doing and accordingly where it was looking visually when processing a complex reasoning scenario.

## 2 Iterative and Parallel Reasoning Mechanism

Our proposed iterative-and parallel-reasoning mechanism (IPRM) is a fully-differentiable neural architecture. Given visual features \(\mathbf{X}_{\mathbf{V}}\in\mathbb{R}^{N_{V}\times D_{V}}\) and language or task-description features \(\mathbf{X}_{\mathbf{L}}\in\mathbb{R}^{N_{L}\times D_{L}}\), IPRM outputs a _"reasoning result"_\(\mathbf{y}_{\mathbf{s}}\in\mathbb{R}^{D_{m}}\) and, optionally, a set of _"reasoning result tokens"_\(\mathbf{Y}_{\mathbf{R}}\in\mathbb{R}^{N_{m}\times D_{m}}\). As previously mentioned, IPRM operates iteratively for \(T\) reasoning steps and internally, maintains an explicit memory \(\mathbf{M}:\{\mathbf{M}_{\mathbf{op}},\mathbf{M}_{\mathbf{res}}\}\). The memory is modelled as a set of _"operation states"_\(\mathbf{M}_{\mathbf{op}}\in\mathbb{R}^{N_{op}\times D_{m}}\), keyed to which are _"result states"_\(\mathbf{M}_{\mathbf{res}}\in\mathbb{R}^{N_{op}\times D_{m}}\) as shown in fig. 1. Here, \(N_{op}\) denotes the number of parallel operations to be computed while \(D_{m}\) denotes the mechanism's internal feature dimension. On a high level, at each reasoning step (denoted by \(t\in\{1,\cdots,T\}\)), IPRM performs the following computations:

1. First, conditioned on the existing operation states \(\mathbf{M}_{\mathbf{op,t}}\), we retrieve relevant information from language or task-description features \(\mathbf{X}_{\mathbf{L}}\) to form a new set of latent operations \(\mathbf{Z}_{\mathbf{op,t}}\in\mathbb{R}^{N_{op}\times D_{m}}\). We term this computation as _"Operation Formation"_. \[\mathbf{Z}_{\mathbf{op,t}}=\mathbf{Op\_Form}(\mathbf{X}_{\mathbf{L}};\mathbf{ M}_{\mathbf{op,t}})\] (1)
2. Then, conditioned on the latent operations \(\mathbf{Z}_{\mathbf{op,t}}\) and the existing result state \(\mathbf{M}_{\mathbf{res,t}}\), we attend and retrieve relevant information from visual features \(\mathbf{X}_{\mathbf{V}}\) which represents a new set of latent results \(\mathbf{Z}_{\mathbf{res,t}}\in\mathbb{R}^{N_{op}\times D_{m}}\) corresponding to \(\mathbf{Z}_{\mathbf{op,t}}\). We term this computation as _"Operation Execution"_. \[\mathbf{Z}_{\mathbf{res,t}}=\mathbf{Op\_Exec}(\mathbf{X}_{\mathbf{V}};[\mathbf{ Z}_{\mathbf{op,t}},\mathbf{M}_{\mathbf{res,t}}])\] (2)
3. Finally, to facilitate interaction amongst parallel operations, we perform inter-operation attention. Here, each operation \(\mathbf{Z}_{\mathbf{op,k,t}};\mathbf{k}\in\{1,\cdots,N_{op}\}\), is composed with other operations in \(\mathbf{Z}_{\mathbf{op,t}}\) as well as prior operation states \(\mathbf{M}_{\mathbf{op[t-W:t]}}\) within a lookback-window \(W\). The corresponding result \(\mathbf{Z}_{\mathbf{res,k,t}}\) is similarly composed with other results \(\mathbf{Z}_{\mathbf{res,t}}\) and prior result states denoted as \(\mathbf{M}_{\mathbf{res[(t-W):t]}}\). We term this computation as _"Operation Composition"_ \[\mathbf{M}_{\mathbf{t+1}}=\mathbf{Op\_Comp}(\{\mathbf{Z}_{\mathbf{op,t}}, \mathbf{Z}_{\mathbf{res,t}}\},\mathbf{M}_{[(t-W):t]})\] (3) As shown in eq. 1, this output is the new memory state \(\mathbf{M}_{\mathbf{t+1}}:\{\mathbf{M}_{\mathbf{op}},\mathbf{M}_{\mathbf{res}}\}\).

The overall computation flow is illustrated in fig. 1 and we provide specific details and intuitions behind these computations in the following sub-sections.

Figure 2: IPRMâ€™s computation flow diagram. First, a new set of N-parallel latent operations \(\mathbf{Z}_{\mathbf{op}}\) are retrieved from language features \(\mathbf{X}_{\mathbf{L}}\) conditioned on prior operation states \(\mathbf{M}_{\mathbf{op}}\). Then, visual features \(\mathbf{X}_{\mathbf{V}}\) are queried conditioned on both \(\mathbf{Z}_{\mathbf{op}}\) and prior result states results \(\mathbf{M}_{\mathbf{res}}\), to form the new results \(\mathbf{Z}_{\mathbf{res}}\). Finally, both \(\mathbf{Z}_{\mathbf{res}}\) and \(\mathbf{Z}_{\mathbf{op}}\) are passed to the Operation Composition Unit (see 2.3), the output of which becomes the new memory state \(\mathbf{M}\).

### Operation Formation

The _"operation formation"_ stage conceptually models a reasoner that based on its prior set of operations, decides what language features to retrieve in order to form the next set of relevant operations. This can be effectively implemented through conventional attention mechanisms. Specifically, the cumulative set of prior operations (maintained in \(\mathbf{M_{op,t}}\)) can be projected to form the 'query' \(\mathbf{Q_{L,t}}\in\mathbb{R}^{N_{op}\times D_{m}}\) representing "what features to look for". The language features \(\mathbf{X_{L}}\) can be projected to form the 'key' \(\mathbf{K_{L}}\in\mathbb{R}^{N_{L}\times D_{m}}\) and 'value' \(\mathbf{V_{L}}\in\mathbb{R}^{N_{L}\times D_{m}}\). Finally, the new set of latent operations \(\mathbf{Z_{op,t}}\) can be retrieved by computing \(\mathtt{attn}(\mathbf{Q_{L}},\mathbf{K_{L}},\mathbf{V_{L}})\). These steps are formally represented below:

\[\mathbf{Q_{L,t}}=\mathbf{W_{L,q2}}(\mathtt{Tanh}(\mathbf{W_{L,q1}}( \mathbf{M_{op,t}}))),\mathbf{K_{L}}=\mathbf{W_{L,k}}(\mathbf{X_{L}}),\mathbf{V _{L}}=\mathbf{W_{L,v}}(\mathbf{X_{L}})\] (4) \[\mathbf{Z_{op,t}}=\mathtt{attn}(\mathbf{Q_{L,t}},\mathbf{K_{L}}, \mathbf{V_{L}})\] (5)

Here, \(\mathbf{W_{L,q2}}\in\mathbb{R}^{D_{m}\times D_{m}}\), \(\mathbf{W_{L,q1}}\in\mathbb{R}^{D_{m}\times D_{m}}\), \(\mathbf{W_{L,k}}\in\mathbb{R}^{D_{m}\times D_{l}}\) and \(\mathbf{W_{L,v}}\in\mathbb{R}^{D_{m}\times D_{l}}\). Note \(\mathbf{K_{L}}\) and \(\mathbf{V_{L}}\) are not computation-step dependent and only computed once. We use a simple linear-modulated formulation (with appropriate broadcasting and projection weight \(\mathbf{W_{a}}\in\mathbb{R}^{D_{k}\times 1}\)) to implement \(\mathtt{attn}(.)\) (further details in appendix sec. 13).

### Operation Execution

In the _"operation execution"_ stage, the reasoner determines what visual features need to be retrieved depending on both the newly formed operations and existing result states. To model the constituent visual attention mechanism, we draw insights from existing recurrent visual reasoning methods [28, 6] that incorporate feature modulation for memory-guided attention. Specifically, we retrieve a set of feature modulation weights \(\mathbf{S_{V,t}}\in\mathbb{R}^{N_{op}\times D_{m}/r}\) through a joint projection of the new operations \(\mathbf{Z_{op,t}}\) and prior results \(\mathbf{M_{res,t}}\) as shown in eq. 6.

\[\mathbf{S_{V,t}}=\mathbf{W_{V,s}}([\mathbf{W_{V,op}}(\mathbf{Z_{op,t}}),\mathbf{ W_{V,res}}(\mathbf{M_{res,t}})])\] (6)

Here, \(r\) is a feature reduction ratio [23, 21]. \(\mathbf{S_{V,t}}\) is then applied dimension wise to a projection of \(\mathbf{X_{V}}\) to retrieve an intermediate attention key \(\mathbf{K^{\prime}_{V,t}}\in\mathbb{R}^{N_{op}\times N_{k}\times D_{m}/r}\). The final attention key \(\mathbf{K_{V,t}}\) is then obtained through a joint multi-layer-projection of \(\mathbf{K^{\prime}_{V,t}}\) and the previously projected representation of \(\mathbf{X_{V}}\) as shown in eq. 7.

\[\mathbf{K^{\prime}_{V,t}}=\mathbf{S_{V,t}}\odot\mathbf{W_{V,k1}}(\mathbf{X_{V} }),\ \mathbf{K_{V,t}}=\mathbf{W_{V,k3}}(\phi(\mathbf{W_{V,k2}}([\mathbf{W_{V,k1}}( \mathbf{X_{V}}),\mathbf{K^{\prime}_{V,t}}])))\] (7)

Finally, the attention query and value are formed through separate projections of \(\mathbf{Z_{op,t}}\) and \(\mathbf{X_{V}}\) respectively. These are then fed together with \(\mathbf{K_{V,t}}\) to the attention function to retrieve the new operation results \(\mathbf{Z_{res,t}}\) as shown in eq. 8. Intuitively, the overall process allows for both prior results and the new set of operations to jointly guide visual attention.

\[\mathbf{Q_{V,t}},\mathbf{V_{V,t}}=\mathbf{W_{V,q}}(\mathbf{Z_{op,t}}),\mathbf{ W_{V,v}}(\mathbf{X_{V}}),\mathbf{Z_{res,t}}=\mathtt{attn}(\mathbf{Q_{V,t}}, \mathbf{K_{V,t}},\mathbf{V_{V,t}})\] (8)

Here, \(\mathbf{W_{V,op}}\in\mathbb{R}^{D_{m}/r\times D_{m}}\), \(\mathbf{W_{V,res}}\in\mathbb{R}^{D_{m}/r\times D_{m}}\), \(\mathbf{W_{V,s}}\in\mathbb{R}^{D_{m}/r\times 2D_{m}/r}\), \(\mathbf{W_{V,k1}}\in\mathbb{R}^{D_{m}/r\times D_{v}}\), \(\mathbf{W_{V,k2}}\in\mathbb{R}^{D_{m}/r\times 2D_{m}/r}\), \(\mathbf{W_{V,k3}}\in\mathbb{R}^{D_{m}/r\times D_{m}/r}\), \(\mathbf{W_{V,q}}\in\mathbb{R}^{D_{m}/r\times D_{m}}\) and \(\mathbf{W_{V,v}}\in\mathbb{R}^{D_{m}\times D_{m}}\).

### Operation Composition

Finally, in the _"operation composition"_ stage, the reasoner first integrates the executed operations \(\mathbf{Z_{op,t}}\) and their results \(\mathbf{Z_{res,t}}\) into the existing memory state \(\mathbf{M_{t}}\) through a simple recurrent update as shown in eqs. 7 and 10. Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states \(\mathbf{M^{\prime}_{op,t+1}}\) with other operation states in \(\mathbf{M^{\prime}_{op,t+1}}\) and also prior operation states in \(\mathbf{M_{op,t-W.t}}\). Here, \(W\) is an attention look-back window.

This composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, \(\mathbf{M^{\prime}_{op,t+1}}\) is projected to obtain a set of queries \(\mathbf{Q_{op,t}}\), while the token-wise concatenation of \(\mathbf{M^{\prime}_{op,t+1}}\) and \(\mathbf{M_{op,t-W.t}}\) are projected to obtain the operation attention keys \(\mathbf{K_{op,t}}\) and values \(\mathbf{V_{op,t}}\). A second set of values \(\mathbf{V_{res,t}}\) are also formed through projection of respective result statesas shown in eq. (14). Further, an identity attention mask \(\mathbf{I_{N_{op}}}\) is used to ensure that operations in \(\mathbf{Q_{op,t}}\), can only attend to other operations and not themselves. This is done to enable a higher degree of operation composition. As shown in eq. (15), \(\mathbf{Q_{op,t}}\), \(\mathbf{K_{op,t}}\), \(\mathbf{V_{op,t}}\) and \(\mathbf{I_{N_{op}}}\) are passed to the attention operation, which outputs an intermediate representation \(\mathbf{M^{\prime\prime}_{op,t+1}}\) and the softmaxed-attention weights \(\mathbf{A_{op,t}}\). \(\mathbf{M^{\prime\prime}_{op,t+1}}\) is then added to a projection of \(\mathbf{M^{\prime}_{op,t+1}}\) to effectively combine attended operation states with the original operation states, and thereby form the next mem. operation state \(\mathbf{M_{op,t+1}}\).

Finally, the next result states are obtained by applying \(\mathbf{A_{op,t}}\) on \(\mathbf{V_{res,t}}\) and then adding a projection of \(\mathbf{M^{\prime}_{res,t+1}}\) as shown in eq. (17). Note \(\mathbf{A_{op,t}}\) is specifically utilized to ensure that results are composed based on attentions between operation states. Here, all the mentioned weights \(\mathbf{W}_{\cdot}\in\mathbb{R}^{D_{m}\times D_{m}}\) and \([\cdot;\cdot]\) represents token-wise concatenation. \(\mathbf{I_{N_{op}}}\) in eq. (15) is an identity matrix which is concatenated with zeros (i.e. unmasked) for window tokens if window len. \(\mathbf{W}>0\).

\[\mathbf{M^{\prime}_{op,t+1}}=\mathbf{W_{opU}}(\mathbf{Z_{op,t}})+ \mathbf{W_{opH}}(\mathbf{M_{op,t}})\] (9) \[\mathbf{M^{\prime}_{res,t+1}}=\mathbf{W_{resU}}(\mathbf{Z_{res,t}} )+\mathbf{W_{resH}}(\mathbf{M_{res,t}})\] (10) \[\mathbf{Q_{op,t}}=\mathbf{W_{op,q}}(\mathbf{M^{\prime}_{op,t+1}})\] (11) \[\mathbf{K_{op,t}}=\mathbf{W_{op,k}}([\mathbf{M^{\prime}_{op,t+1}}; \mathbf{M_{op,t-W:t}}])\] (12) \[\mathbf{V_{op,t}}=\mathbf{W_{op,v}}([\mathbf{M^{\prime}_{op,t+1}}; \mathbf{M_{op,t-W:t}}])\] (13) \[\mathbf{V_{res,t}}=\mathbf{W_{res,v}}([\mathbf{M^{\prime}_{res,t+1}} ;\mathbf{M_{res,t-W:t}}])\] (14) \[\mathbf{M^{\prime\prime}_{op,t+1}},\mathbf{A_{op,t}}=\texttt{attn }(\mathbf{Q_{op,t}},\mathbf{K_{op,t}},\mathbf{V_{op,t}},\text{mask=}\mathbf{I_ {N_{op}}})\] (15) \[\mathbf{M_{op,t+1}}=\mathbf{M^{\prime\prime}_{op,t+1}}+\mathbf{W_{ op,u2}}(\mathbf{M^{\prime}_{op,t+1}})\] (16) \[\mathbf{M_{res,t+1}}=\mathbf{A_{op,t}}(\mathbf{V_{res,t}})+\mathbf{ W_{res,v2}}(\mathbf{M^{\prime}_{res,t+1}})\] (17)

**Obtaining Reasoning Summary** As mentioned before, our proposed mechanism outputs a set of _"reasoning result tokens"_\(\mathbf{Y_{R}}\) and a _"reasoning result"_\(\mathbf{y_{s}}\). \(\mathbf{Y_{R}}\) is simply equivalent to the last memory result states \(\mathbf{M_{res,T+1}}\). To obtain \(\mathbf{y_{s}}\), we perform attention on the last operation states \(\mathbf{M_{op,T+1}}\) by utilizing a summary representation \(\mathbf{I_{s}}\in\mathbb{R}^{D_{l}}\) of \(\mathbf{X_{L}}\) as the attention-query.

We set \(\mathbf{I_{s}}\) to be the first token in case of transformer-based language backbones and as last hidden state in case of LSTM-based language backbones. As shown in eq. (18), \(\mathbf{I_{s}}\) is projected to obtain a single-token attention query \(\mathbf{p_{q}}\) while \(\mathbf{M_{op,T+1}}\) is projected to obtain the attention keys \(\mathbf{P_{k}}\). The attention value is simply the result states \(\mathbf{M_{res,T+1}}\), and the output of the attention function is the _"reasoning result"_. Intuitively, this computation corresponds to the reasoner deciding which final operation states in \(\mathbf{M_{op,T+1}}\) are most relevant to the summary of the input language or task-description \(\mathbf{X_{L}}\), based on which corresponding result states \(\mathbf{M_{res,T+1}}\) are weighted and retrieved.

\[\mathbf{p_{q}},\mathbf{P_{k}} =\mathbf{W_{pq,q}}(\mathbf{I_{s}}),\mathbf{W_{pk,k}}(\mathbf{M_{op, T+1}})\] (18) \[\mathbf{y_{s}} =\texttt{attn}(\mathbf{p_{q}},\mathbf{P_{k}},\mathbf{M_{res,T+1}})\] (19)

Here, \(\mathbf{W_{pq,q}}\in\mathbb{R}^{D_{m}\times D_{l}}\) and \(\mathbf{W_{pk,k}}\in\mathbb{R}^{D_{m}\times D_{m}}\).

**Reasoning mechanism general applicability.** Our proposed iterative and parallel reasoning mechanism is an end-to-end trainable neural module. It can be conveniently applied on top of different vision and language backbones, and be trained directly as a new computational block with no specific adjustments. Further, IPRM is weight-tied which means that its number of parameters is constant regardless of number of computation steps and parallel operations. We provide parameter and computational details along with module implementation in appendix sec.

Figure 3: Operation Composition Unit

## 3 Experiments

We evaluate IPRM on STAR[79], AGQAv2[20] and CLEVRER-Humans[51] for video reasoning tasks and CLEVR-Humans[35], GQA[29] and CLEVR-CoGenT[34] for image reasoning tasks. For all tasks, we set IPRM's parallel operations (\(N_{op}\)) to 6, reasoning steps (\(T\)) to 9, reduction ratio (\(r\)) to 2 and window length (\(W\)) to 2 (informed by ablative analysis detailed in sec. 3.3). We follow task-specific practices (detailed in appendix C.1) for respective vision and language backbones in our primary experiments, besides also demonstrating integration of IPRM with large-scale VL backbones such as CLIP. Further, besides task-specific methods, we also consider two prominent transformer-based VL modules as baselines - concat-att (where lang. and vis. tokens are concatenated as in [36], [37]) and cross-att (where lang. tokens are "query" to vis. tokens as "key" and "value"; as in [43]). Further implementation and training details are provided in appendix sec.4.

### Video Reasoning and Question Answering (STAR, AGQAv2 and CLEVRER-Humans)

We first evaluate IPRM on recent video reasoning benchmarks. STAR[79] and AGQAv2 comprise real-world videos and test multiple reasoning skills in context of situational reasoning and compositional spatiotemporal reasoning respectively. STAR contains 60K questions testing four broad types of video reasoning abilities: _feasibility_, _interaction_, _prediction_ and _sequence_. Meanwhile, AGQAv2 contains 2.27M balanced questions distributed across 16 different question types. As shown in table[1], IPRM obtains 69.9% average acc. on STAR and 60.4% overall acc. on AGQAv2, outperforming prior videoQA-specific methods by 5% on both benchmarks.

Interestingly, on STAR IPRM obtains an 8% and 7% improvement over SeViLA-BLIP2 on the predictive and sequencing scenarios respectively. This is possibly due to IPRM's capability to reason over multiple events simultaneously across frames, which may enhance its capacity to retrieve and cumulatively reason an relevant information needed to predict future events and determine appropriate sequences. However, IPRM performs less effectively than SeViLA-BLIP2 in feasibility scenarios, possibly because these scenarios require not only visual reasoning but also commonsense knowledge, which can benefit from integration with larger-scale vision-language backbones and auxiliary training tasks such as introduced in LRR[5].

Similarly, On AGQAv2, IPRM improves performances across various question types, notably achieving a 8% improvement in questions that require determining sequence of events. Further, IPRM also outperforms both 4-layer concat- and cross-attention modules on STAR (scaling further attention-layers was not found to benefit performance as detailed in appendix table[7]).

Next, we evaluate IPRM on the CLEVRER-Humans benchmark[51], which comprises synthetic videos of simultaneous object motions and multiple collisions, and tests a model's ability to determine causal links between events. We perform zero-shot, finetuned and from-scratch evaluation. As shown in table[2], IPRM outperforms task-specific neurosymbolic models NS-DR and VR-DP as well as state-of-the-art ALOE across the three settings. Specifically, IPRM improves zero-shot per-question acc. by 7%, finetuned per-question acc. by 18.8% and scratch per-question acc. by 6.2%. These results further suggest that IPRM can better track and process co-occuring events, and in this case, more accurately determine causal links.

\begin{table}
\begin{tabular}{|l|c c c c c|c|} \hline Model & Int. & Seq. & Pred. & Feas. & Avg. \\ \hline LRR[51] & 73.7 & 71.0 & 71.3 & 65.1 & 70.3 \\ LRR[50]/w/o surrogate & 54.5 & 48.7 & 44.3 & 45.5 & 48.2 \\ All-in-One[52] & 47.5 & 50.8 & 47.7 & 44.0 & 47.5 \\ Temp[ATP][51] & 50.6 & 52.8 & 49.3 & 40.6 & 48.3 \\ MIST[53] & 55.5 & 54.2 & 54.2 & 44.4 & 51.1 \\ InternetVideo[8][73] & 62.7 & 65.6 & 54.9 & 51.9 & 58.7 \\ SeViLA-BLIP2[63] & 63.7 & 70.4 & 63.1 & **62.4** & 64.9 \\ Concat-Att-AL & 68.1 & 71.4 & 66.6 & 55.2 & 65.3 \\ Cross-Att-AL & 67.5 & 72.1 & 64.4 & 58.5 & 65.6 \\ \hline IPRM & **71.8** & **77.7** & **71.0** & 59.1 & **69.9** \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (0). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.

### Compositional Image Reasoning (CLEVR-Humans, CLEVR-CoGen and GQA)

Here, we evaluate IPRM on challenging compositional image reasoning benchmarks. CLEVR-Humans tests generalization of multi-hop reasoning to free-form human crowdsourced questions which entail reasoning skills/scenarios beyond a model's training on the original CLEVR dataset and provides a limited finetuning set (2.5% of CLEVR). Similarly, CLEVR-CoGenT tests generalization on novel attribute compositions not observed in training (e.g.,"_gray cubes_" and "_red cylinders_" are in training but eval. is on "_gray cylinders_" and "_red cubes_"; see suppl. for exact specification). The CLOSURE [3] benchmark further tests systematic generalization for different question type compositions.

As shown in table 3, IPRM achieves 3.9% and 3.8% improvements in zero-shot and fully-finetuned performance over prior state-of-art vision-language model MDETR. Further, IPRM neither requires bounding-box pre-training supervision (as done in MDETR) nor functional programs, and can be trained directly with only vision-language inputs and task supervision. fig. 5 illustrates IPRM's performance across different training ratios compared to MDETR and cross- and concat-att transformer modules. Notably, IPRM exceeds MDETR's fully-finetuned performance by 1.1% with only half of training data. Further, IPRM exhibits these improvements while being relatively lightweight (4.4M params) in comparison to MDETR's transformer blocks (17.4M; \(\sim\)4x more parameters) and cross- and concat-VL attention methods (16.8M and 12.6M respectively). These results suggest that IPRM exhibits strong generalization capabilities and more sample-efficient learning of novel reasoning skills and scenarios in context of multi-step imageQA.

For CLEVR-CoGenT, IPRM achieves state-of-art results in out-of-domain generalization on novel attribute compositions and outperforms MDETR (having parallel transformer compute) by 3.6% and MAC (iterative method) by 2%. This suggests the combination of iterative and parallel computation as done in IPRM can implicitly enable more disentangled feature processing and thereby improve compositional learning of primitive attributes in context of multi-step imageQA. Similarly, on CLOSURE, IPRM achieves an average zero-shot accuracy of 75.6% which is highest amongst fully neural reasoning methods (that require no extra supervision) and is close to the neurosymbolic method NS-VQA which utilizes ground truth programs and object bounding boxes supervision. Further, detailed breakdown of models on CLOSURE question types is provided in appendix table

\begin{table}
\begin{tabular}{|l|c c|c c|c c|} \hline \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Zero-shot} & \multicolumn{2}{c|}{Finetune} & \multicolumn{2}{c|}{Scratch} \\  & Opt. & Qs. & Opt. & Qs. & Opt. & Qs. \\ \hline NS-DR [3] & 51.0 & 32.0 & - & - & - & - \\ VRDP [1] & 50.9 & 31.6 & - & - & - & - \\ \hline CNN-LSTM [3] & 56.3 & 30.0 & 51.7 & 34.2 & 51.5 & 30.8 \\ CNN-BERT [3] & 52.9 & 32.0 & 52.4 & 30.2 & 50.1 & 30.4 \\ ALOC [2] & 54.0 & 26.9 & 51.8 & 31.7 & 52.7 & 32.1 \\ \hline IPRM & **61.7** & **38.9** & **74.1** & **53.0** & **62.0** & **38.3** \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of methods for CLEVR-Humans [3] (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings.

We also evaluate IPRM on the GQA benchmark which tests compositional VQA on real-world images. We perform comparisons with prior VQA methods as well as large-scale VL models such as LXMERT and 12-in-1 that do not utilize the ground truth scene graphs in GQA. As shown in table 1, IPRM achieves 60.3%, outperforming prior reasoning methods such as MCAN and LCGN as well as large-scale VL models such as LXMERT and 12-in-1. However, IPRM's performance is behind the larger VL model OSCAR which performs pretraining on 4.3M data samples collated from COCO, VG, SBU and Flickr. Note, IPRM is a standalone reasoning module only trained on GQA balanced with no pre-training. Further, IPRM when trained with perfect perception (i.e. ground truth object bounding boxes and attributes), achieves 87.2% on GQA validation set suggesting strong reasoning performances can be achieved through advancements in visual detectors and backbones. Finally, in appendix B.3, we demonstrate that IPRM more effectively enhances the performance of frozen CLIP variants on complex reasoning benchmarks compared to scaling traditional cross- and concat-attention modules.

### Model ablations and reasoning visualization

We perform ablations to study the contributions of IPRM's salient components. First, we analyze the impact of varying number of parallel operations (\(N_{op}\)) against number of iterative computation steps (\(T\)). We compare models with \(N_{op}\in\{1,3,6,9\}\) and \(T\in\{1,3,6,9\}\), resulting in 16 different models. Given the large amount of ablative models, we perform analysis on a reduced-resolution setting of CLEVR-Humans with pretraining on CLEVR for 15 epochs. As shown in fig. 1 (plot (i)), we find that \(T\) and \(N_{op}\) appear to be co-dependent and that neither by itself can lead to high performance. E.g. setting \(T\) = 1 generally results in performances around 75% regardless of \(N_{op}\), while setting \(N_{op}\)= 1 or 3 results in a sharp performance drop of 3% when changing \(T\) to 9 from 6. In contrast, for \(N_{op}>3\), we observe that performance increases steadily with higher \(T\), suggesting that a higher number of parallel operations may prevent overfitting in a model with high computation steps. Overall, we find that (\(N_{op}=6\), \(T=9\)) and (\(N_{op}=9\), \(T=9\)) are the two best performing models achieving above 82% accuracy (with the former preferred as \(N_{op}=6\) performs better for diff. \(T\) compared to \(N_{op}=9\)).

Next, we study the impact of the operation composition block (OPC) by evaluating \(N_{op}=6\) (and diff. computation steps \(T\)) with and without OPC. As shown in fig. 1 (plot (ii)), while \(T=1\) has a relatively low drop of \(\sim\)2%, the performance drops are more significant for higher \(T\). The (\(N_{op}=6\), \(T=9\)) model which reached \(\sim\)82% acc. with OPC, drops to \(\sim\)74% acc. without OPC.

Finally, we study the impacts of the dimension reduction ratio (\(r\)) and memory-lookback window length (\(W\)). As shown in fig. 1 (plot (iii)), \(r=2\) leads to negligible drop (0.3%) in performance compared to no dimension reduction (i.e. \(r=1\)) while being computationally faster and requiring less memory. However, setting \(r\) to \(8\) significantly deteriorates performance. Similarly, as shown in fig. 1 (plot (iv)), a setting of window-length \(W\) is 2 works sufficiently well, and decreasing it below that significantly impacts performance.

\begin{table}
\begin{tabular}{c c c c c c c c}  & LCGN [23] & MCAN [87] & LXMERT* [66] & 12-in-1* [49] & OSCAR* [66] & CFR** [53] & IPRM \\ \hline GQA & 55.8 & 57.4 & 60.0 & 60.0 & **61.6** & 72.1 & 60.3 \\ \hline \end{tabular}
\end{table}
Table 4: Performance comparison on GQA with imageQA methods and large-scale models that do not utilize ground-truth scene graphs. * indicates large-scale pretrained VL model. **Utilizes ground truth scene graphs, programs and bounding boxes for auxiliary training.

Figure 6: IPRM Model ablations in order: **(i)** Impact of number of parallel operations (\(N_{op}\)) vs computation steps (\(T\)). **(ii)** Impact of Operation Composition Block (OPC). **(iii)**: Impact of reduction ratio (\(r\)) and **(iv)** memory window length (\(W\)).

[MISSING_PAGE_FAIL:9]

contributes an alternative and possibly more effective reasoning mechanism that can be integrated with such models in future to enhance complex VQA capabilities.

**Memory and recurrence-augmented transformers.** Multiple works have identified limitations of purely feedforward computation as realized in transformers and worked on encoding recurrence [30][26] and memory-augmented computation [31][32]. Notably, Recurrent Memory Transformer [33] and MemFormer[34] introduce recurrent and dynamic memory to improve language modelling capabilities. More recently, EMAT [35] introduces efficient memory to augment knowledge retrieval, MemViT [36] introduces a cache memory to retain prior context for long-video tasks, and [37] introduces memory for more effective action anticipation. While these methods study recurrent and memory-augmented computation on specific natural language processing and computer vision tasks, our work focuses on the integration of iterative-parallel computation and working memory in a single neural reasoning mechanism beneficial for complex VQA scenarios.

## 5 Conclusion

We introduced a novel fully-differentiable and end-to-end trainable iterative and parallel reasoning mechanism (IPRM) to address complex VQA scenarios. We comprehensively evaluated IPRM on various complex image and video VQA benchmarks testing distinct reasoning capabilities, and found it improves state-of-arts on multiple such benchmarks. We also performed quantiative ablations to study individual impacts of parallel and iterative computation besides qualitative analysis of IPRM's reasoning computation visualization.

## 6 Limitations and Future work

Here, we note possible limitations of IPRM. Similar to existing VQA and deep-learning methods, IPRM may reflect biases that are present in the training distribution of VQA benchmarks. This may lead it to overfit to certain image inputs or question forms and possibly provide skewed answers in such scenarios. Further, the utilized vision-language backbones in our experiments may also entail visual, language and cultural biases in their original training distribution which may permeate to IPRM upon integration for VQA scenarios. In this regard, we hope the capability to visualize intermediate reasoning of IPRM and diagnose its error cases (as shown in section 3.3) can serve a useful tool to benefit interpretability in VQA and identify possible reasoning biases that may emerge in the model.

For future work, scaling the IPRM architecture to a foundational video-language model by integrating it with large-scale transformer-based vision-language models and relevant instruction-tuning approaches presents an exciting research opportunity. Moreover, while we designed and evaluated IPRM in the context of complex VQA, we believe it has the potential to operate as a general reasoning mechanism applicable to tasks beyond visual reasoning and question answering, such as language processing and embodied reasoning.

**Acknowledgment** This research/project is supported by the National Research Foundation, Singapore, under its NRF Fellowship (Award# NRF-NRFF14-2022-0001). This research is also supported by funding allocation to B.F. and C.T. by the Agency for Science, Technology and Research (A*STAR) under its SERC Central Research Fund (CRF), as well as its Centre for Frontier AI Research (CFAR).

## References

* [1]J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
* [2]J. Andreas, M. Rohrbach, T. Darrell, and D. Klein (2016) Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705. Cited by: SS1.
* [3]D. Bahdanau, H. de Vries, T. J. O'Donnell, S. Murty, P. Beaudoin, Y. Bengio, and A. Courville (2019) Closure: assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783. Cited by: SS1.
** [4] Z. Bai, R. Wang, and X. Chen. Glance and focus: Memory prompting for multi-event video question answering. _Advances in Neural Information Processing Systems_, 36, 2024.
* [5] A. Bhattacharyya, S. Panchal, R. Pourreza, M. Lee, P. Madan, and R. Memisevic. Look, remember and reason: Grounded reasoning in videos with language models. In _The Twelfth International Conference on Learning Representations_.
* [6] B. Bogin, S. Subramanian, M. Gardner, and J. Berant. Latent compositional representations improve systematic generalization in grounded question answering. _Transactions of the Association for Computational Linguistics_, 9:195-210, 2021.
* [7] S. Buch, C. Eyzaguirre, A. Gaidon, J. Wu, L. Fei-Fei, and J. C. Niebles. Revisiting the" video" in video-language understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2917-2927, 2022.
* [8] A. Bulatov, Y. Kuratov, and M. Burtsev. Recurrent memory transformer. _Advances in Neural Information Processing Systems_, 35:11079-11091, 2022.
* [9] A. Capon, S. Handley, and I. Dennis. Working memory and reasoning: An individual differences perspective. _Thinking & Reasoning_, 9(3):203-244, 2003.
* [10] Y. Cong, W. Liao, H. Ackermann, B. Rosenhahn, and M. Y. Yang. Spatial-temporal transformer for dynamic scene graph generation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 16372-16382, 2021.
* [11] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. _arXiv preprint arXiv:1807.03819_, 2018.
* [12] D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. Attention over learned object embeddings enables complex visual reasoning. _Advances in neural information processing systems_, 34:9112-9124, 2021.
* [13] M. Ding, Z. Chen, T. Du, P. Luo, J. Tenenbaum, and C. Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. _Advances in Neural Information Processing Systems_, 34:887-899, 2021.
* [14] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, et al. Faith and fate: Limits of transformers on compositionality. _Advances in Neural Information Processing Systems_, 36, 2024.
* [15] C. Fan, X. Zhang, S. Zhang, W. Wang, C. Zhang, and H. Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1999-2007, 2019.
* [16] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu. Rmpe: Regional multi-person pose estimation. In _Proceedings of the IEEE international conference on computer vision_, pages 2334-2343, 2017.
* [17] D. Fougnie. The relationship between attention and working memory. _New research on short-term memory_, 1:45, 2008.
* [18] D. Gao, L. Zhou, L. Ji, L. Zhu, Y. Yang, and M. Z. Shou. Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14773-14783, 2023.
* [19] R. Girshick. Fast r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 1440-1448, 2015.
* [20] M. Grunde-McLaughlin, R. Krishna, and M. Agrawala. Agqa: A benchmark for compositional spatio-temporal reasoning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11287-11297, 2021.
* [21] M. Grunde-McLaughlin, R. Krishna, and M. Agrawala. Agqa 2.0: An updated benchmark for compositional spatio-temporal reasoning. _arXiv preprint arXiv:2204.06105_, 2022.

* [22] J. Hsu, J. Mao, J. Tenenbaum, and J. Wu. What's left? concept grounding with logic-enhanced foundation models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [23] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7132-7141, 2018.
* [24] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko. Learning to reason: End-to-end module networks for visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pages 804-813, 2017.
* [25] R. Hu, A. Rohrbach, T. Darrell, and K. Saenko. Language-conditioned graph networks for relational reasoning. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10294-10303, 2019.
* [26] F. Huang, K. Lu, C. Yuxi, Z. Qin, Y. Fang, G. Tian, and G. Li. Encoding recurrence into transformers. In _The Eleventh International Conference on Learning Representations_, 2022.
* [27] D. Hudson and C. D. Manning. Learning by abstraction: The neural state machine. _Advances in Neural Information Processing Systems_, 32, 2019.
* [28] D. A. Hudson and C. D. Manning. Compositional attention networks for machine reasoning. _arXiv preprint arXiv:1803.03067_, 2018.
* [29] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.
* [30] D. Hutchins, I. Schlag, Y. Wu, E. Dyer, and B. Neyshabur. Block-recurrent transformers. _Advances in Neural Information Processing Systems_, 35:33248-33261, 2022.
* [31] S. Jaiswal, B. Fernando, and C. Tan. Tdam: Top-down attention module for contextually guided feature selection in cnns. In _European Conference on Computer Vision_, pages 259-276. Springer, 2022.
* [32] A. Jha, B. Patro, L. Van Gool, and T. Tuytelaars. Barlow constrained optimization for visual question answering. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1084-1093, 2023.
* [33] C. Jing, Y. Jia, Y. Wu, X. Liu, and Q. Wu. Maintaining reasoning consistency in compositional visual question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5099-5108, 2022.
* [34] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2901-2910, 2017.
* [35] J. Johnson, B. Hariharan, L. Van Der Maaten, J. Hoffman, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Inferring and executing programs for visual reasoning. In _Proceedings of the IEEE international conference on computer vision_, pages 2989-2998, 2017.
* [36] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1780-1790, 2021.
* [37] W. Kim, B. Son, and I. Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In _International Conference on Machine Learning_, pages 5583-5594. PMLR, 2021.
* [38] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [39] T. M. Le, V. Le, S. Venkatesh, and T. Tran. Hierarchical conditional relation networks for video question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9972-9981, 2020.

* [40] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7331-7341, 2021.
* [41] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does clip bind concepts? probing compositionality in large image models. _arXiv preprint arXiv:2212.10537_, 2022.
* [42] C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. _arXiv preprint arXiv:2205.12005_, 2022.
* [43] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.
* [45] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: A simple and performant baseline for vision and language. _arXiv preprint arXiv:1908.03557_, 2019.
* [46] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX 16_, pages 121-137. Springer, 2020.
* [47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.
* [48] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [49] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10437-10446, 2020.
* [50] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. _arXiv preprint arXiv:1904.12584_, 2019.
* [51] J. Mao, X. Yang, X. Zhang, N. Goodman, and J. Wu. Cleverr-humans: Describing physical and causal events the human way. _Advances in Neural Information Processing Systems_, 35:7755-7768, 2022.
* [52] D. Mascharka, P. Tran, R. Soklaski, and A. Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4942-4950, 2018.
* [53] S. S. Mondal, J. D. Cohen, and T. W. Webb. Slot abstractors: Toward scalable abstract visual reasoning. _arXiv preprint arXiv:2403.03458_, 2024.
* [54] S. S. Mondal, T. Webb, and J. D. Cohen. Learning to reason over visual objects. _arXiv preprint arXiv:2303.02260_, 2023.
* [55] B. X. Nguyen, T. Do, H. Tran, E. Tjiputra, Q. D. Tran, and A. Nguyen. Coarse-to-fine reasoning for visual question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4558-4566, 2022.
* [56] D. K. Nguyen, V. Goswami, and X. Chen. Movie: Revisiting modulated convolutions for visual counting and beyond. In _International Conference on Learning Representations_, 2020.
* [57] R. OpenAI. Gpt-4 technical report. _ArXiv_, 2303, 2023.

* [58] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [59] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 1532-1543, 2014.
* [60] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [62] R. Shrestha, K. Kafle, and C. Kanan. Answer them all! toward universal visual question answering models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10472-10481, 2019.
* [63] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A corpus of natural language for visual reasoning. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 217-223, 2017.
* [64] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi. A corpus for reasoning about natural language grounded in photographs. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 6418-6428, 2019.
* [65] H. Tan and M. Bansal. Object ordering with bidirectional matchings for visual reasoning. In _Proceedings of NAACL-HLT_, pages 444-451, 2018.
* [66] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 5100-5111, 2019.
* [67] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [68] A. Urooj, H. Kuehne, B. Wu, K. Chheu, W. Bousselham, C. Gan, N. Lobo, and M. Shah. Learning situation hyper-graphs for video question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14879-14889, 2023.
* [69] M. Vaishnav and T. Serre. Gamr: A guided attention model for (visual) reasoning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [71] J. Wang, G. Chen, Y. Huang, L. Wang, and T. Lu. Memory-and-anticipation transformer for online action understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13824-13835, 2023.
* [72] J. Wang, Y. Ge, R. Yan, Y. Ge, K. Q. Lin, S. Tsutsui, X. Lin, G. Cai, J. Wu, Y. Shan, et al. All in one: Exploring unified video-language pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6598-6608, 2023.
* [73] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.

* [74] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, et al. Image as a foreign language: Beit pretraining for vision and vision-language tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19175-19186, 2023.
* [75] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. _arXiv preprint arXiv:2212.03191_, 2022.
* [76] T. Webb, S. S. Mondal, and J. D. Cohen. Systematic visual reasoning through object-centric relational abstraction. _Advances in Neural Information Processing Systems_, 36, 2024.
* [77] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.
* [78] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45, 2020.
* [79] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan. Star: A benchmark for situated reasoning in real-world videos. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* [80] C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13587-13597, 2022.
* [81] Q. Wu, Z. Lan, K. Qian, J. Gu, A. Geramifard, and Z. Yu. Memformer: A memory-augmented transformer for sequence modeling. In Y. He, H. Ji, S. Li, Y. Liu, and C.-H. Chang, editors, _Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022_, pages 308-318, Online only, Nov. 2022. Association for Computational Linguistics.
* [82] Y. Wu, Y. Zhao, B. Hu, P. Minervini, P. Stenetorp, and S. Riedel. An efficient memory-augmented transformer for knowledge-intensive nlp tasks. _arXiv preprint arXiv:2210.16773_, 2022.
* [83] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked attention networks for image question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 21-29, 2016.
* [84] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. _arXiv preprint arXiv:1910.01442_, 2019.
* [85] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. _Advances in neural information processing systems_, 31, 2018.
* [86] S. Yu, J. Cho, P. Yadav, and M. Bansal. Self-chained image-language model for video localization and question answering. _arXiv preprint arXiv:2305.06988_, 2023.
* [87] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian. Deep modular co-attention networks for visual question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6281-6290, 2019.
* [88] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting visual representations in vision-language models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5579-5588, 2021.

Appendix / supplemental material

## Appendix B Elaborated Experiments and Results Discussion

### Further comparisons on CLEVR-Humans, CLEVR-CoGenT, CLOSURE and NLVRv1

Here, we provide further comparisons with benchmark-specific methods for CLEVR-Humans [35], CLEVR-CoGenT [34], CLOSURE [3] and NLVRv1 [63] (not reported in main paper due to space limitations). As mentioned in main paper, these benchmarks utilize synthetic images and are a test of pure visual reasoning capabilities that are minimally influenced by increased world knowledge or usage of stronger visual backbones.

CLEVR-Humans as already mentioned in main paper evaluates a model's reasoning generalization capabilities to unseen scenarios or question forms. CLEVR-CoGenT studies compositional attribute generalization. Specifically, it has two conditions - i) cond.A wherein all cubes have color \(\in\{gray,blue,brown,yellow\}\) and cylinders \(\in\{red,green,purple,cyan\}\) (spheres can be any color), and ii) cond.B wherein color-sets are switched b/w cubes and cylinders. A model is then trained on one condition and evaluated on both the original and alternate condition. A higher accuracy on the alternate condition indicates that the model learns more 'compositionally' as it generalizes better to novel shape-color combinations with less feature/attribute combination overfitting.

Finally, NLVRv1 evaluates language-grounded visual reasoning. Each sample of this benchmark comprises a set of three synthetic images and a composite natural language statement about the images which can evaluate to True or False and requires various visual-linguistic reasoning skills.

As shown in table [8] IPRM achieves state-of-art results across the three benchmarks and does not require pre-annotated bounding-boxes or functional programs as additional supervision. For **CLEVR-Humans** (table [8] left), it outperforms larger-scale models such as MDETR and RAMEN in zero-shot performance even though the latter is pre-trained on multiple VQA datasets. It also increases state-of-art in finetuned setting by 3.8%.

For **CLEVR-CogenT** (table [8] centre), IPRM achieves the highest generalization results amongst methods in both the CoGen-Train A and Finetune B. Specifically, it obtains 80.3% acc. on cond. B (when trained on cond. A), which is 1.5% higher than the previous state-of-art cond.B method FILM and 3.6% higher than MDETR. When further finetuned on cond.B, IPRM generalizes for both cond.A and cond.B achieving 98.0% and 98.2% unlike FILM which overfits to cond.B and thereby has poor performance on cond.A. Further, its performance on cond.A (99.1%) is highest amongst methods that do not utilize bounding box or localization supervision and marginally lower than MDETR and NS-VQA (which utilize bounding-box supervision).

**For NLVRv1** (table [8] right), IPRM model trained from scratch achieves 63.8% acc. and performs competitively with existing task-specific state-of-art model CNN-BiAtt. When finetuned from its CLEVR checkpoint, we find IPRM achieves 73.0% acc. which is 7% higher than existing visual inputs state-of-art for NLVRv1 and suggests strong reasoning transfer capabilities of IPRM. It further outperforms the N2NMN method which requires pre-defined neural modules to be identified for the dataset.

\begin{table}
\begin{tabular}{|l|c c c|c|} \hline Model & \multicolumn{2}{c|}{CLVR-Humans} & \multicolumn{2}{c|}{CoGenT-A} & \multicolumn{2}{c|}{CoGenT-B} \\  & 28 & FT & Vala & Valb & Vala & Valb \\ \hline PG+EF & 54.0 & 66.6 & NS-VQA [35] & - & 67.8 \\ NLVR [63] & - & 67.8 & METR [63] & **99.8** & 76.7 & - & - \\ RAMEN [62] & 57.8 & StackAtt-ML [63] & 80.3 & 68.7 & 75.7 & 75.8 \\ FILM [64] & - & 75.8 & PG + EF [63] & 96.6 & 73.7 & 76.1 & 92.7 \\ LET [64] & - & 78.8 & MedNet [62] & 98.8 & 75.4 & 96.9 & 96.3 \\ MAC [65] & 57.4 & 81.5 & MAC [63] & 99.0 & 78.3 & 97.2 & 96.1 \\ MDETR [64] & 59.9 & 81.7 & FILM [64] & 98.3 & 78.8 & 81.1 & 96.9 \\ \hline IPRM & **63.8** & **85.5** & IPRM & 99.1 & **80.3** & **98.0** & **98.2** \\ \hline \end{tabular}
\end{table}
Table 5: Elaborated results on CLEVR-Humans (left), CLEVR-CoGenT (middle) and NLVRv1 (right). IPRM achieves state-of-art across the three benchmarks and does not require additional supervision such as bounding boxes or functional programs. * requires func. programs supervision / pre-defined dataset-specific neural modules. * requires object bounding-boxes supervision.

Finally, we have also provided results of models on CLOSURE for different subsets / question types in table 1. For the CLEVR-environment, we have visualized some failure cases in figs. 10 and 11. More cases can be run through the visualization framework in provided source code.

### Elaborated STAR results and ablations for video reasoning tasks

We provide results on the STAR Test, further baselines and model ablations for video reasoning tasks in table 1.

### CLIP Integration Results

We provide results with frozen CLIP 1 visual backbones including CLIP VIT-L/14, CLIP VIT-B/16 and CLIP VIT-L/14@336px on GQA [29], NLVRv2 [24] and CLEVR-Humans in table 1. We empirically found utilizing a Distil-roberta language backbone to be more effective than the associated CLIP language backbone, and hence used the former for question processing. We compare with alternate prominent vision-language attention mechanisms including Cross-att and Concat-att blocks as well as a simple joint projection of vision and language pooled representations (referred as Wt-Proj-Att). As shown in the table, IPRM can enhance performance for the CLIP variants across GQA, NLVRv2 and CLV-Humans in comparison to concat and cross-att blocks. Further, it is more parameter efficient with only 5.5M additional parameters in comparison to 4-layer as well as 2-layer stacks of Cross-Att (9.2M 2-layer, 17.6M 4-layer) and Concat-Att (7.2M 2-layer, 13.6M 4-layer). With regards to computational FLOPs, IPRM consumes 5.9GFLOPs which is marginally higher than Cross-Att 4-layer config (3.1GFLOPs) and lower than Concat-Att 4-layer config (8.9GFLOPs). Note, that the performance benefits of adding further layers of cross- or concat-att blocks are observed to be minimal after 4 layers, and can also depend on the amount of training data available. E.g. Both cross- and concat-att blocks of 2 layers had better performances on NLVRv2 (which has a limited set of training questions relative to GQA and CLEVR) in comparison to 4 layer config.

### Further reasoning computation visualizations

We provide elaborate reasoning computation visualizations of IPRM showing the lang. and vis. attentions across parallel operations and computation steps during _operation formation_ and _operation execution_ stages. Fig. 1 shows a scenario wherein IPRM correctly utilizes parallel and iterative computations to compute intermediate operations of "find object close to front", "retrieve/compare

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline
**Model (ZerosShot)** & \multicolumn{1}{c}{\begin{tabular}{c} and \_mat.qa_ \\ \end{tabular} } & \multicolumn{1}{c}{embed\_mat.qa} & \multicolumn{1}{c}{embed\_spa.mat} & \multicolumn{1}{c}{
\begin{tabular}{c} or\_mat.qa.spa_ \\ \end{tabular} } & \multicolumn{1}{c}{compare\_mat.} & \multicolumn{1}{c}{compare\_mat.spa} \\ \hline FLM & 41.4\&18 & 62.11 & 93.42 & 34.4\&13 & 35.25\& 66.24\& 8.56 & 65.845 \\ MAC & 63.7\&25 & 76.84\&11 & 994.01\& 75 & 25.413 & 70.4415 & 65.348.6 & 66.246.4 \\ NS-YQA* & 33.48\& 97.34\&6.9 & 98.34 & 69.5428 & 50.44\&27 & 96.16\& 6.2 & 95.446.7 \\ PG-Tensor-NMN* & 37.6\&17 & 79.84\&1.4 & 95.65\& 65.3 & 34.18\& 2.55\& 81.9 & 92.42\& 39.842.7 \\ PG-Vector-NMN* & 32.5\& 417 & 97.3\& 24.2 & 97.24\& 9.64\& 92.45\& 47.42\& 89.345.7 & 94.446.4 \\ \hline IPRM & 83.1\&6.4 & 80.3\& 7.2 & 99.2\(\pm\)0.1 & 75.74\&9.4 & 67.14\& 62.47\& 61.545.8 \\ \hline \end{tabular}
\end{table}
Table 6: Zero-shot performances of models on CLOSURE subsets (2 trials run for IPRM)

\begin{table}
\begin{tabular}{|l|c|c|c c c|} \hline Model & Int. & Seq. & Pred. & Feas. & Avg. \\ \hline All-noe [29] & 47.5 & 50.8 & 47.7 & 44.0 & 47.5 \\ Term/APTM[1] & 50.6 & 52.8 & 49.3 & 40.6 & 48.3 \\ MIST [13] & 55.5 & 54.2 & 54.2 & 44.4 & 51.1 \\ Interflow (8) & 62.7 & 65.6 & 54.9 & 51.9 & 58.7 \\ Event-AL-BLP2 [26] & 63.7 & 70.4 & 63.1 & **62.4** & 64.9 \\ Concat-Att-2L & 66.0 & 68.9 & 66.4 & 55.1 & 64.1 \\ Concat-Att-4L & 68.1 & 71.4 & 66.6 & 55.2 & 65.3 \\ Cross-Att-4L & 67.5 & 72.1 & 64.4 & 58.5 & 65.6 \\ Concat-Att-6L & 66.3 & 71.4 & 66.6 & 55.7 & 65.0 \\ Cross-Att-6L & 59.8 & 63.0 & 56.4 & 49.9 & 57.3 \\ \hline IPRM(1,1) & 65.6 & 69.8 & 65.1 & 54.5 & 63.8 \\ IPRM(1,1) & 70.0 & 75.7 & 70.2 & 57.8 & 68.4 \\ IPRM(6,1) & 69.5 & 75.2 & 68.6 & 57.4 & 67.7 \\ IPRM & **71.8** & **77.7** & **71.0** & 59.1 & **69.9** \\ IPRM (GTV) & 78.7 & 85.5 & 81.7 & 71.2 & 79.3 \\ \hline \end{tabular}
\end{table}
Table 7: **Left:** Results on STAR 1 official hidden test (evaluation server) with ground-truth vision (GT V) and predicted vision (PR V); **Right:** Results on STAR val. set with num. of sampled frames =32 unless otherwise stated in ().

shape and size", "find applicable objects with both same shape and size". Fig. 1 shows another correct prediction of IPRM, and this time, its intermediate reasoning visualization is useful to determine that the entailed reasoning appears sensible. Fig. 1 shows an incorrect prediction by IPRM and its intermediate reasoning visualizations also suggest that IPRM did not understand the question and thereby did not attend to relevant objects. Finally, Fig. 1 shows a scenario wherein while IPRM produces the correct answer, it's intermediate reasoning appears imprecise which makes the prediction (and underlying reasoning) less reliable. We provide further visualizations with a CLIP VIT-L/14 backbone on GQA samples in the supplemental jupyter notebook output (html format for easier viewing).

## Appendix C Model implementation and experiment details

We implement IPRM in PyTorch [58] as a generic vision-language module receiving a set of input vision (or scene-representation) tokens and input language (or task-representation) tokens. We provide **Python-style pseudocode of IPRM in figs 12[13] and 14**. For all experiments, we set the internal dimension of IPRM to 512 and use the same configuration of num. parallel operations (\(N_{op}\))=6, num. computation steps (T)=9, reduction ratio (r)=2 and window size (W)=2. We follow benchmark-specific conventions for vision-language backbones that are detailed below in sec. C.1. For CLIP [61], we utilize the official models from Huggingface [72]. All experiments are performed on a single NVIDIA A40 GPU with 46GB memory and averaged over 3 trials with different random seeds wherever possible (done for primary experiments on STAR, AGQA, CLEVRER-Humans, CLEVR-Humans). Unless otherwise specified, the learning rate is initialized to 1e-4 with Adam [33] optimizer and gradient clipping value of 8. The learning-rate is reduced based on validation acc. plateau with reduction factor 0.5, threshold 0.001 and patience 0. Further experiment hyper-parameters and settings are provided below. Source code at: https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism

### Benchmark-specific experiment details

**CLEVR-Humans**. We use the CLEVR-Humans dataset from [33] which comprises images from original CLEVR dataset [34] and human crowdsourced questions. We use a batch size of 216 for training. We use the same language encoder (Distil-Roberta [43] from Huggingface[73]) as in existing state-of-art MDETR [35] and frozen ResNet101 backbone layer 3 spatial features (as in [23][52]). We perform all ablation experiments with 14x14x1024 visual features. Each ablation model is first pretrained for 10 epochs on the original CLEVR dataset (the initial learning rate for IPRM is 1e-4 and for language encoder is 1e-5) and then finetuned on CLEVR-Humans for 40 epochs with early stopping (learning rate of 1e-4 throughout). As observed in prior work [52], we similarly found in multiple scenarios with occluded objects that visual attention only partially identified such objects. Hence, we simply resampled (bilinear sampling) visual input to obtain 16x16x1024 features and empirically found more complete visual attentions with a corresponding 1.1% improvement in accuracy. The final two best performing model configurations (_Nop=6, T=9, W=2, R=2_ and _Nop=6, T=9, W=2, R=1_) from ablations were then pre-trained for 35 epochs on CLEVR and finetuned

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline Model (CLIP & \multirow{2}{*}{+Param} & \multirow{2}{*}{+GFLOPs} & GQA & NLVR2 & CLV-H \\ VIT-L/14 bbone) & & TestD & Test & ZS & FT \\ \hline Wi-Pro-Fusion & 0.6M & 0.1 & 53.5 & 60.8 & 58.5 & 74.4 \\ Cross-Att (XL) & 9.2M & 1.5 & 55.1 & 62.1 & - & - \\ Concat-Att (XL) & 7.2M & 4.4 & 55.3 & 60.5 & - & - \\ Cross-Att (XL) & 17.6M & 3.1 & 57.4 & 54.4 & 60.3 & 80.0 \\ Concat-Att (XL) & 13.6M & 8.9 & 58.7 & 55.9 & 61.2 & 81.1 \\ Cross-Att (6L) & 26.0M & 4.5 & 56.8 & x & 60.8 & 80.4 \\ Concat-Att (6L) & 19.7M & 13.3 & 57.4 & x & 62.0 & 81.8 \\ \hline IPRM & 5.2M & 5.9 & **59.2** & **65.1** & **64.3** & **84.6** \\ \hline \end{tabular} \begin{tabular}{|l|c|c|} \hline Model (CLIP & GQA & NLVR2 \\ VIT-B/16 bbone) & TestD & Test \\ \hline Wi-Pro-Fusion & 51.4 & 59.9 \\ Cross-Att & 54.6 & 56.6 \\ Concat-Att & 56.0 & 57.4 \\ \hline IPRM & 55.9 & 60.8 \\ \hline \end{tabular} 
\begin{tabular}{|l|c|} \hline Model (CLIP & GQA & NLVR2 \\ VIT-L/14@336) & TestD & Test \\ \hline Wi-Pro-Fusion & 54.0 & 61.1 \\ Cross-Att & 57.4 & 58.4 \\ Concat-Att & 57.3 & 59.1 \\ \hline IPRM & 59.0 & 65.3 \\ \hline \end{tabular}
\end{table}
Table 8: **Left:** Comparison of IPRM with prominent vision-language attention mechanisms with CLIP VIT-L/14 backbones on CLEVR-Humans, GQA and NLVRv2 benchmarks (â€˜4Lâ€™ indicates 4 att layers; â€˜xâ€™ indicates model did not converge). **Right:** Results with other CLIP variants VIT-B and VIT-L@ 336 on GQA and NLVRv2.

[MISSING_PAGE_FAIL:19]

performance model to evaluate generalization performance on cond.B. For finetuning on cond.B we finetuned the best cond.A model for 20 epochs and used the best cond.B validation performance model to also evaluate on cond.A. All other hyperparameters are the same as mentioned for CLEVR-Humans.

**NLVR.** We use the NLVRv1 and NLVRv2 datasets from [63, 64]. NLVRv1 comprises 3 synthetic images and a language statement while NLVRv2 comprises 2 real-world images and a lang. statement. For both datasets, the obtained visual tokens for each images was flattened to obtain the final visual input and an image-wise positional embedding was added to indicate image order. For the language encoder, we used a simple Bi-LSTM.

## Appendix D Potential Negative Impact

In relation to VQA and deep-learning methods in general, the deployment of IPRM in real-world applications without thorough consideration of dataset or training distribution biases, could inadvertently reinforce existing vision, language and cultural biases present in the data, leading to erroneous outcomes or skewed answers. Further, the deployment of VQA methods such as IPRM in sensitive domains such as healthcare or scene/footage analysis could raise ethical concerns, including privacy violations, algorithmic reliability, and the potential for unintended consequences stemming from erroneous or biased predictions.

Figure 8: **Top**: original image and question; **middle**: language attentions across parallel operations (clubbed together; op_k represents parallel operation k) and computation steps. **Bottom**: Visual attentions across parallel ops and computation steps. Here, IPRM correctly utilizes parallel and iterative compute to locate the correct candidate object for prediction (to which all operations attend in last step).

Figure 9: In this example, IPRM predicts the correct answer and its visual attention trace provides evidence of correct intermediate reasoning. In penultimate reasoning step, IPRM correctly localizes the gray object with maximum occuring shape (cylinder) and in the final step, the parallel operations attend to both the cyan cube and the brown cylinder closest to previously identified gray cylinder.

Figure 10: Example where IPRM outputs incorrect answer and the intermediate reasoning appears faulty possibly due to lack of understanding what a â€œprimary color isâ€. The pair of blue (a primary color) cubes in this case should have been identified but are not visually attended in any of the operations across reasoning steps).

Figure 11: Example wherein IPRM produces correct answer but its visual attention trace suggests intermediate reasoning may be imprecise. The maximum occuring shape is cube; however both the blue small cylinder and blue small cube appear to be attended in the penultimate step as the â€œblue small object with max occuring shapeâ€ making the reasoning and prediction less reliable.

[MISSING_PAGE_EMPTY:25]

#Below, "Lin"referstoalinearlayer
#and"MLP"referstoa2-layermulti-layer-perceptronlayer defoperation_formation(lang_tokens,#B\(\times\)Nl\(\times\)Dm prev_op_state#B\(\times\)Nop\(\times\)Dm(Nop=numparallelops) ):
#1.Formnewop"query"basedonprioropstate op_q=MLP_l(prev_op_state)#papereq.4
#2.Uselang_token_featsasatin"key"and"value"(papereq.5) lang_k=lang_tokens lang_v=lang_tokens
#3.Retrievenewlatentopsfromlang.repthroughattention latent_ops,lang_attn=mod_attn(op_q,lang_k,lang_v,lang_attn_proj)#papereq.6;L194
#returnlatent_ops,lang_attn
#defoperation_execution(vis_tokens,#B\(\times\)Nw\(\times\)Dm new_ops,#B\(\times\)Nop\(\times\)Dm prev_res_state):#B\(\times\)Nop\(\times\)Dm
#1.Formfeaturemodulationweights(papereq.7) s_v=concat([Lin_op(new_ops),Lin_res(prev_res_state)])#concatacrossfeat. \(\leftrightarrow\)axis s_v=Lin_s(s_v)
#2.Formvisualattention"key"(papereqs.8and9) vis_red_rep=Lin_v1(vis_tokens) mod_vis=s_v+vis_red_rep Nop=mod_vis.size(1) vis_k=MLP_v(concat([mod_vis,vis_red_rep]))#concatacrossfeat.axis
#3.Formvisualattention"query"and"value"(papereq.10) vis_q=Lin_op_q(new_ops) vis_v=Lin_v2(vis_tokens)
#4.Obtainingnevelatent"results"throughvisattention(papereq.11) latent_results,vis_attn=mod_attn(vis_q,vis_k,vis_v,vis_att_proj)
#returnlatent_results,vis_attn
#defmod_attn(q,k,v,att_proj_layer,attn_mask): qk_mult=q*k#element-wiseproduct attn_wt=att_proj_layer(qk_mult)#linearprojection(paperL194) attn_wt=softmax(attn_wt+(attn_mask*-1e30)) out=(attn_wt*v).sum()#sumacrossfeatureaxis returnout,attn_wt

Figure 13: IPRM pseudocode (2/3)

[MISSING_PAGE_EMPTY:27]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and introduction reflects the motivation of method and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Limitations are provided in appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Benchmark and training details are provided along with module pseudocode and example CLIP integration. Source code for experiments will made publicly available along with checkpoints. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer:[Yes] Justification: Paper provides training and implementations details and provides module pseudocode. Full source code will be made publicly available upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendix provides details on hyperparameters and dataset specific settings for all experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Appendix mentions that results were averaged over atleast 3 seeds for primary experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix mentions the type of GPU and its memory for all experiments along with batch size of experiments.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Authors reviewed code of ethics during submission. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[Yes] Justification: Paper mentions potential negative impacts of work in appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: answerNA

Justification: Paper does not explicitly pose such risks; however in limitations and potential negative impact this is mentioned.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, all coding libraries and datasets are properly cited and credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification:No new asset Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Paper does not involve crowdsourcing or human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Paper does not involve crowdsourcing or human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.