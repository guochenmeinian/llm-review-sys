# Generative Forests

 Richard Nock

Google Research

richardnock@google.com &Mathieu Guillame-Bert

Google

gbm@google.com

We use this now common parlance expression on purpose, to avoid confusion with the other "generative" problem that consists in modelling densities [7, 37].

###### Abstract

We focus on generative AI for a type of data that still represent one of the most prevalent form of data: tabular data. Our paper introduces two key contributions: a new powerful class of forest-based models fit for such tasks and a simple training algorithm with strong convergence guarantees in a boosting model that parallels that of the original weak / strong supervised learning setting. This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (_i.e. supervised learning_) with two classes. Experiments on the quality of generated data display substantial improvements compared to the state of the art. The losses our algorithm minimize and the structure of our models make them practical for related tasks that require fast estimation of a density given a generative model and an observation (even partially specified): such tasks include missing data imputation and density estimation. Additional experiments on these tasks reveal that our models can be notably good contenders to diverse state of the art methods, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models.

## 1 Introduction

There is a substantial resurgence of interest in the ML community around tabular data, not just because it is still one of the most prominent kind of data available [6]: it is recurrently a place of sometimes heated debates on what are the best model architectures to solve related problems. For example, even for well-posed problems with decades of theoretical formalization like supervised learning [39], after more than a decade of deep learning disruption [22], there is still much ink spilled in the debate decision forests vs neural nets [28]. Generative AI* makes no exception. Where the consensus has long been established on the best categories of architectures for data like image and text (neural nets), tabular data still flourishes with a variety of model architectures building up - or mixing - elements from knowledge representation, logics, kernel methods, graph theory, and of course neural nets [4, 9, 12, 31, 34, 41, 43, 44] (see Section 2). Because of the remarkable nature of tabular data where a single variable can bring considerable information about a target to model, each of these classes can be a relevant choice at least in _some_ cases (such is is the conclusion of [28] in the context of supervised learning). A key differentiator between model classes is then training and the formal guarantees it can provide [31, 44].

**In this paper**, we introduce new generative models based on sets of trees that we denote as _generative forests_ (gf), along with a training algorithm which has two remarkable features: it is extremely simple and brings strong convergence guarantees in a weak / strong learning model that parallels that of the original boosting model of Valiant's PAC learning [18]. These guarantees improve upon the best state of the art guarantees [44, 31]. Our training algorithm, gf.Boost, supports training from data with missing values and is simple enough to be implementable by a few tweaks on the popular induction scheme for _decision tree induction_ with two classes, which is supported by a huge numberof repositories / ML software implementing algorithms like CART or C4.5 [1; 35; 3; 36; 45]. From the model standpoint, generative forests bring a sizeable combinatorial advantage over its closest competitors, generative trees [31] (see Table 1 for an example) and adversarial random forests [44]. Experiments on a variety of simulated or readily available domains display that our models can substantially improve upon state of the art, with models of ours as simple as a set of stumps potentially competing with other approaches building much more complex models.

The models we build have an additional benefit: it is computationally easy to compute the full density given an observation, _even partially specified_; hence, our generative models can also be used for side tasks like missing data imputation or density estimation. Additional experiments clearly display that our approach can be a good contender to the state of the art. To save space and preserve readability, all proofs and additional experiments and results are given in an Appendix.

## 2 Related work

It would not do justice to the large amount of work in the field of "Generative AI" for tabular data to just sample a few of them, so we devote a part of the Appendix to an extensive review of the state of the art. Let us just mention that, unlike for unstructured data like images, there is a huge variety of model types, based on trees [7; 31; 44], neural networks [12; 14; 20; 47], probabilistic circuits [5; 43; 38], kernel methods [4; 34], graphical models [41] (among others: note that some are in fact hybrid models). The closest approaches to ours are [44] and [31], because the models include trees with a stochastic activation of edges to pick leaves, and a leaf-dependent data generation process. While [31] learn a single tree, [44] use a way to generate data from a set of trees - called an adversarial random forest - which is simple: sample a tree, and then sample an observation from the tree. The model is thus simple but not economical: each observation is generated by a single tree only, each of them thus having to represent a good sampler in its own. In our case, generating one observation makes use of _all_ trees (Figure 1, see also Section 4). We also note that the theory part of [44] is limited because it resorts to statistical consistency (infinite sample) and Lipschitz continuity of the target density, with second derivative continuous, square integrable and monotonic. Similarly, [31] resort to a wide set of proper losses, but with a symmetry constraint impeding in a generative context. As we shall see, our theoretical framework does not suffer from such impediments.

## 3 Basic definitions

Perhaps surprisingly at first glance, this Section introduces generative and _supervised_ loss functions. Indeed, our algorithm, which trains a data generator and whose overall convergence shall be shown on a generative loss, turns out to locally optimize a _supervised_ loss. For the interested reader, the key link, which is of independent interest since it links in our context losses for supervised and generative learning, is established in Lemma A (Appendix).

\(\forall k\in\mathbb{N}_{>0}\), let \([k]\doteq\{1,2,...,k\}\). Our notations follow [37]. Let \(\mathfrak{B}\doteq(\pi,\Lambda,\mathrm{B})\) denote a _binary task_, where \(\mathrm{A},\mathrm{B}\) (and any other measure defined hereafter) are probability measures with the same support, also called _domain_, \(\mathcal{X}\), and \(\pi\in[0,1]\) is a _prior_. \(\mathrm{M}\doteq\pi\cdot\mathrm{A}+(1-\pi)\cdot\mathrm{B}\) is the corresponding mixture measure. For the sake of simplicity, we assume \(\mathcal{X}\) bounded hereafter, and note that tricks can be used to remove this assumption [31; Remark 3.3]. In tabular data, each of the \(\dim(\mathcal{X})\) features

\begin{table}
\begin{tabular}{c c c} \hline \hline domain & \(\mathtt{circgauss}\). _Center_: density learned by a generative forest (gf) consisting of a single tree, boosted for a small number (50) of iterations. _Right_: density learned by a gf consisting of 50 boosted tree stumps (_Center_ and _Right_ learned using gf.Boost). In a domain \(\mathcal{X}\) with dimension \(d\), a single tree with \(n\) splits can only partition the domain in \(n+1\) parts. On the other hand, a set of \(n\) stumps in a gf can boost this number to \(n^{\tilde{\Omega}(d)}\) (the tilda omits \(\log\) dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump. \\ \hline \hline \end{tabular}
\end{table}
Table 1: _Left_: domain \(\mathtt{circgauss}\). _Center_: density learned by a generative forest (gf) consisting of a single tree, boosted for a small number (50) of iterations. _Right_: density learned by a gf consisting of 50 boosted tree stumps (_Center_ and _Right_ learned using gf.Boost). In a domain \(\mathcal{X}\) with dimension \(d\), a single tree with \(n\) splits can only partition the domain in \(n+1\) parts. On the other hand, a set of \(n\) stumps in a gf can boost this number to \(n^{\tilde{\Omega}(d)}\) (the tilda omits \(\log\) dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.

can be of various _types_, including categorical, numerical, etc., and associated to a natural measure (counting, Lebesgue, etc.) so we naturally associate \(\mathcal{X}\) to the product measure, which can thus be of mixed type. We also write \(\mathcal{X}\doteq\times_{i=1}^{d}\mathcal{X}_{i}\), where \(\mathcal{X}_{i}\) is the set of values that can take on variable \(i\). Several essential measures will be used in this paper, including U, the uniform measure, \(\mathbb{G}\), the measure associated to a generator that we learn, \(\mathbb{R}\), the _empirical_ measure corresponding to a training sample of observations. Like in [31], we do not investigate generalisation properties.

**Loss functions** There is a natural problem associated to binary task \(\mathfrak{B}\), that of estimating the probability that an arbitrary observation \(\boldsymbol{x}\in\mathcal{X}\) was sampled from \(\mathrm{A}-\mathrm{call}\) such _positive_ - or \(\mathrm{B}-\mathrm{call}\) these _negative_ -. To learn a _supervised_ model \(\mathcal{X}\to[0,1]\) for such a _class probability estimation_ (cpe) problem, one usually has access to a set of examples where each is a couple (observation, class), the class being in set \(\mathcal{Y}=\{-1,1\}\) (={negative, positive}). Examples are drawn i.i.d. according to \(\mathfrak{B}\). Learning a model is done by minimizing a loss function: when it comes to cpe, any such cpe loss [2] is some \(\ell:\mathcal{Y}\times[0,1]\to\mathbb{R}\) whose expression can be split according to _partial_ losses \(\ell_{1},\ell_{-1}\), \(\ell(y,u)\doteq[y=1]\cdot\ell_{1}(u)+[y=-1]\cdot\ell_{-1}(u)\). Its (pointwise) _Bayes risk_ function is the best achievable loss when labels are drawn with a particular positive base-rate,

\[\underline{L}(p)\doteq\inf_{u}\mathsf{E}_{\mathsf{Y}\sim\mathsf{B}(p)}\ell( \mathsf{Y},u),\] (1)

where \(\mathtt{b}\doteq\text{Bernoulli random variable for class 1}\). A fundamental property for a cpe loss is _properness_, encouraging to guess ground truth: \(\ell\) is proper iff \(\underline{L}(p)=\mathsf{E}_{\mathsf{Y}\sim\mathsf{B}(p)}\ell(\mathsf{Y},p), \forall p\in[0,1]\), and _strictly_ proper if \(\underline{L}(p)<\mathsf{E}_{\mathsf{Y}\sim\mathsf{B}(p)}\ell(\mathsf{Y},u), \forall u\neq p\). Strict properness implies strict concavity of Bayes risk. For example, the square loss has \(\ell_{1}^{\otimes 0}(u)=(1-u)^{2}\), \(\ell_{-1}^{\otimes 0}(u)\doteq u^{2}\), and, being strictly proper, Bayes risk \(L^{\otimes 0}(u)\doteq u(1-u)\). Popular strictly proper ML include the log and Matusita's losses. All these losses are _symmetric_ since \(\ell_{1}^{\otimes 0}(u)=\ell_{-1}^{\otimes 0}(1-u),\forall u\in(0,1)\) and _differentiable_ because both partial losses are differentiable.

In addition to cpe losses, we introduce a set of losses relevant to generative approaches, that are popular in density ratio [29; 40]. For any differentiable and convex \(F:\mathbb{R}\to\mathbb{R}\), the Bregman divergence with generator \(F\) is \(D_{F}(z|z^{\prime})\doteq F(z)-F(z^{\prime})-(z-z^{\prime})F^{\prime}(z^{ \prime})\). Given function \(g:\mathbb{R}\to\mathbb{R}\), the generalized perspective transform of \(F\) given \(g\) is \(\hat{F}(z)\doteq g(z)\cdot F\left(z/g(z)\right)\), \(g\) being implicit in notation \(\check{F}\)[26; 27; 32]. The _Likelihood ratio risk_ of \(\mathrm{B}\) with respect to \(\mathrm{A}\) for loss \(\ell\) is

\[\mathbb{D}_{\ell}\left(\mathrm{A},\mathrm{B}\right) \doteq \pi\cdot\mathbb{E}_{\textsc{U}}\left[D_{\left(\widetilde{- \underline{L}}\right)}\left(\frac{\mathrm{d}\mathrm{A}}{\mathrm{d}\mathrm{U}} \left|\frac{\mathrm{d}\mathrm{B}}{\mathrm{d}\mathrm{U}}\right.\right)\right],\] (2)

with \(g(z)\doteq z+(1-\pi)/\pi\) in the generalized perspective transform. The prior multiplication is for technical convenience. \(\mathbb{D}_{\ell}\) is non-negative; strict properness is necessary for a key property of \(\mathbb{D}_{\ell}\): \(\mathbb{D}_{\ell}=0\) iff \(\mathrm{A}=\mathrm{B}\) almost everywhere [31].

## 4 Generative forests: models and data generation

**Architecture** We first introduce the basic building block of our models, _trees_.

**Definition 4.1**.: _A **tree**\(\Upsilon\) is a binary directed tree whose internal nodes are labeled with an observation variable and arcs are consistently labeled with subsets of their tail node's variable domain._

_Consistency_ is an important generative notion; informally, it postulates that the arcs' labels define a partition of the measure's support. To make this notion formal, we proceed need a key definition.

Figure 1: Sketch of comparison of two approaches to generate one observation, using Adversarial Random Forests [44] (left) and using generative forests, gf (right, this paper). In the case of Adversarial Random Forests, a tree is sampled uniformly at random, then a leaf is sampled in the tree and finally an observation is sampled according to the distribution “attached” to the leaf. Hence, only one tree is used to generate an observation. In our case, we leverage the combinatorial power of the trees in the forest: _all trees_ are used to generate one observation, as each is contributing to one leaf. Figure 3 provides more details on generation using gf.

For any node \(\nu\in\mathcal{N}(\Upsilon)\) (the whole set of nodes of \(\Upsilon\), including leaves), we denote \(\mathcal{X}_{\nu}\subseteq\mathcal{X}\) the _support_ of the node. The root has \(\mathcal{X}_{\nu}=\mathcal{X}\). To get \(\mathcal{X}_{\nu}\) for any other node, we initialize it to \(\mathcal{X}\) and then descend the tree from the root, progressively updating \(\mathcal{X}_{\nu}\) by intersecting an arc's observation variable's domain in \(\mathcal{X}_{\nu}\) with the sub-domain labelling the arc until we reach \(\nu\). Then, a labeling of arcs in a tree is _consistent_ iff it complies with one constraint **(C)**:

**(C)**: for each internal node \(\nu\) and its left and right children \(\nu_{t},\nu_{\mathfrak{t}}\) (respectively), \(\mathcal{X}_{\nu}=\mathcal{X}_{\nu_{t}}\cup\mathcal{X}_{\nu_{\mathfrak{t}}}\) and the measure of \(\mathcal{X}_{\nu_{t}}\cap\mathcal{X}_{\nu_{\mathfrak{t}}}\) with respect to \(\mathbb{G}\) is zero.

For example, the first split at the root of a tree is such that the union of the domains at the two arcs equals the domain of the feature labeling the split (see Figure 2). We define our generative models.

**Definition 4.2**.: _A **generative forest** (gf), \(\mathbb{G}\), is a set of trees, \(\{\Upsilon_{t}\}_{t=1}^{T}\), associated to measure \(\mathbb{R}\) (implicit in notation)._

Figure 2 shows an example of gf. Following the consistency requirement, any single tree defines a recursive partition of \(\mathcal{X}\) according to the splits induced by the inner nodes. Such is _also_ the case for a set of trees, where _intersections_ of the supports of tuples of leaves (1 for each tree) define the subsets:

\[\mathcal{P}(\mathbb{G})\doteq\big{\{}\gamma_{i=1}^{T}\mathcal{X}_{\lambda_{i} }\text{ s.t. }\lambda_{i}\in\Lambda(\Upsilon_{i}),\forall i\big{\}}\] (3)

(\(\Lambda(\Upsilon)\subseteq\mathcal{N}(\Upsilon)\) is the set of leaves of \(\Upsilon\)). Notably, we can construct the elements of \(\mathcal{P}(\mathbb{G})\) using the same algorithm that would compute it for 1 tree. First, considering a first tree \(\Upsilon_{1}\), we compute the support of a leaf, say \(\mathcal{X}_{\lambda_{1}}\), using the algorithm described for the consistency property above. Then, we start again with a second tree \(\Upsilon_{2}\)_but_ replacing the initial \(\mathcal{X}\) by \(\mathcal{X}_{\lambda_{1}}\), yielding \(\mathcal{X}_{\lambda_{1}}\cap\mathcal{X}_{\lambda_{2}}\). Then we repeat with a third tree \(\Upsilon_{3}\) replacing \(\mathcal{X}\) by \(\mathcal{X}_{\lambda_{1}}\cap\mathcal{X}_{\lambda_{2}}\), and so on until the last tree is processed. This yields one element of \(\mathcal{P}(\mathbb{G})\).

**Generating one observation** Generating one observation relies on a _stochastic_ version of the procedure just described. It ends up in an element of \(\mathcal{P}(\mathbb{G})\) of positive measure, from which we sample uniformly one observation, and then repeat the process for another observation. To describe the process at length, we make use of two key routines, Init and StarUpdate, see Algorithms 1 and 2. Init initializes "special" nodes in each tree, that are called _star nodes_, to the root of each tree (notation for a variable \(v\) relative to a tree \(\Upsilon\) is \(\Upsilon.v\)). Stochastic activation, performed in StarUpdate, progressively makes star nodes descend in trees. When all star nodes have reached a leaf in their respective tree, an observation is sampled from the intersection of the leaves' domains (which is an element of \(\mathcal{P}(\mathbb{G})\)). A Boolean flag, done takes value true when the star node is in the leaf set of the tree, indicating no more StarUpdate calls for the tree ([.] is Iverson's bracket, [21]).

StarUpdate is called with a tree of the gf for which done is false, a subset \(\mathcal{C}\) of the whole domain and measure \(\mathbb{R}\). The first call of this procedure is done with \(\mathcal{C}=\mathcal{X}\). When all trees are marked done, \(\mathcal{C}\) has been "reduced" to some \(\mathcal{C}_{s}\in\mathcal{P}(\mathbb{G})\), where the index reminds that this is the last \(\mathcal{C}\) we obtain, from which we sample an observation, uniformly at random in \(\mathcal{C}_{\kappa}\). Step 1 in StarUpdate is

Figure 2: A gf (\(T=2\)) associated to UCI German Credit. Constraint **(C)** (see text) implies that the domain of ”Number existing credits” is \(\{0,1,...,8\}\), that of ”Job” is {A171, A172, A173, A174}, etc..

fundamental: it relies on tossing an unfair coin (a Bernoulli event noted \(\mathrm{B}(p)\)), where the head probability \(p\) is just the mass of \(\mathrm{R}\) in \(\mathcal{X}_{\mathrm{T},\nu^{*}_{*}}\cap\mathcal{C}\)_relative to \(\mathcal{C}\)_. Hence, if \(\mathcal{X}_{\mathrm{T},\nu^{*}_{*}}\cap\mathcal{C}=\mathcal{C}\), \(p=1\). There is a simple but important invariant (proof omitted).

**Lemma 4.3**.: _In StarUpdate, it always holds that the input \(\mathcal{C}\) satisfies \(\mathcal{C}\subseteq\mathcal{X}_{\mathrm{T},\nu^{*}}\)._

We have made no comment about the _sequence_ of tree choices over which StarUpdate is called. Let us call _admissible_ such a sequence that ends up with _some_\(\mathcal{C}_{s}\subseteq\mathcal{X}\). \(T\) being the number of trees (see Init), for any sequence \(\bm{v}\in[T]^{D(\mathcal{C}_{s})}\), where \(D(\mathcal{C}_{s})\) is the sum of the depths of all the star _leaves_ whose support intersection is \(\mathcal{C}_{s}\), we say that \(\bm{v}\) is _admissible for \(\mathcal{C}_{s}\)_ if there exits a sequence of branchings in Step 1 of StarUpdate, whose corresponding sequence of trees follows the indexes in \(\bm{v}\), such that at the end of the sequence all trees are marked done and the last \(\mathcal{C}=\mathcal{C}_{s}\). Crucially, the probability to end up in \(\mathcal{C}_{s}\) using StarUpdate, given any of its admissible sequences, is the _same_ and equals its mass with respect to \(\mathrm{R}\).

**Lemma 4.4**.: _For any \(\mathcal{C}_{s}\in\mathcal{P}(\mathrm{G})\) and admissible sequence \(\bm{v}\in[T]^{D(\mathcal{C}_{s})}\) for \(\mathcal{C}_{s}\), \(p_{\mathrm{G}}\left[\mathcal{C}_{s}|\bm{v}\right]=p_{\mathrm{R}}\left[\mathcal{ C}_{s}\right]\)._

The Lemma is simple to prove but fundamental in our context as the way one computes the sequence - and thus the way one picks the trees - does not bias generation: the sequence of tree choices could thus be iterative, randomized, concurrent (_e.g._ if trees were distributed), etc., this would not change generation's properties from the standpoint of Lemma 4.4. We defer to Appendix (Section II) three examples of sequence choice. Figure 3 illustrates a sequence and the resulting \(\mathcal{C}_{s}\).

**Missing data imputation and density estimation with \(\mathsf{GFs}\)** A generative forest is not just a generator: it models a density and the exact value of this density at any observation is easily available. Figure 4 (top row) shows how to compute it. Note that if carried out in parallel, the complexity to get this density is cheap, of order \(O(\max_{t}\mathrm{depth}(\Upsilon_{t}))\). If one wants to prevent predicting zero density, one can stop the descent if the next step creates a support with zero empirical measure. A \(\mathsf{GF}\) thus also models an easily tractable density, but this fact alone is not enough to make it a _good_ density estimator. To get there, one has to factor the loss minimized during training. In our case, as we shall see in the following Section, we train a \(\mathsf{GF}\) by minimizing an information-theoretic loss directly formulated on this density (2). So, using a \(\mathsf{GF}\) also for density estimation can be a reasonable additional benefit of training \(\mathsf{GFs}\).

The process described above that finds leaves reached by an observation can be extended to missing values in the observation using standard procedures for classification using decision trees. We obtain a simple procedure to carry out _missing data imputation_ instead of density estimation: once a tuple of leaves is reached, one uniformly samples the missing features in the corresponding support. This is fast, but at the expense of a bit more computations, we can have a much better procedure, as explained in Figure 4 (bottom row). In a first step, we compute the density of each support subset corresponding to _all_ (not just 1 as for density estimation) tuples of leaves reached in each tree. This provides us with the _full density_ over the missing values _given_ (i) a partial assignment of the tabular domain's variables and (ii) the \(\mathsf{GF}\). We then keep the elements corresponding to the maximal density value and finally simultaneously sample all missing features uniformly in the corresponding domain. Overall, the whole procedure is \(O(d\cdot(\sum_{t}\mathrm{depth}(\Upsilon_{t}))^{2})\).

Figure 3: From left to right and top to bottom: updates of the argument \(\mathcal{C}\) of StarUpdate through a sequence of run of StarUpdate in a generative forest consisting of three trees (the partition of the domain induced by each tree is also depicted, alongside the nature of splits, vertical or horizontal, at each internal node) whose star nodes are indicated with chess pieces (\(\boxplus\), \(\boxplus\), \(\boxplus\)). In each picture, \(\mathcal{C}\) is represented at the bottom of the picture (hence, \(\mathcal{C}=\mathcal{X}\) after Init). In the bottom-right picture, all star nodes are leaves and thus \(\mathcal{C}_{s}=\mathcal{C}\) displays the portion of the domain in which an observation is sampled. Remark that the last star node update produced no change in \(\mathcal{C}\).

## 5 Learning generative forests using supervised boosting

**From the GAN framework to a fully supervised training of generative models** It can appear quite unusual to train generative models using a supervised learning framework, so before introducing our algorithm, we provide details on its filiation in the generative world, starting from the popular GAN training framework [12]. In this framework, one trains a generative model against a discriminator model which has no purpose other than to parameterize the generative loss optimized. As shown in [33], there is a generally inevitable slack between the generator and the discriminator losses with neural networks, which translates into uncertainty for training. [31] show that the slack disappears for calibrated models, a property satisfied by their generative trees (and also by our generative forests). Moreover, [31] also show that the GAN training can be simplified and made much more efficient for generative trees by having the discriminator (a decision tree) copy the tree structure of the generator, a setting defined as _copycat_. Hence, one gets reduced uncertainty and more efficient training. Training still involves two models but it becomes arguably very close to the celebrated boosting framework for top-down induction of decision trees [19], up to the crucial detail that the generator turns out to implements boosting's leveraged "hard" distribution. This training gives guarantees on the likelihood ratio risk we define in (2). Our paper closes the gap with boosting: copycat training can be equivalently simplified to training a _single generative model_ in a _supervised_ (2 classes) framework. The two classes involved are the observed data and the uniform distribution. Using the uniform distribution is allowed by the assumption that the domain is closed, which is reasonable for tabular data but can also be alleviated by reparameterization [31, Remark 3.3]. The (supervised) loss involved for training reduces to the popular concave "entropic"-style losses used in CART, C4.5 and in popular packages like scikit-learn. And of course, minimizing such losses _still_ provides guarantee on the generative loss (2). While it is out of scope to show how copycat training does ultimately simplify, we provide all components of the "end product": algorithm, loss optimized and the link between the minimization of the supervised and generative losses via boosting.

Figure 4: (**Top row**) Density estimation using a gf, on an observation indicated by \(\bullet\) (_Left_). In each tree, the leaf reached by the observation is found and the intersection of all leaves’ supports is computed. The estimated density at \(\bullet\) is computed as the empirical measure in this intersection over its volume (_Right_). (**Bottom row**) Missing data imputation using the same gf, and an observation with one missing value (if \(\mathcal{X}\subset\mathbb{R}^{2}\), then \(y\) is missing). We first proceed like in density estimation, finding in each tree _all_ leaves _potentially_ reached by the observation if \(y\) were known (_Left_); then, we compute the density in _each_ non-empty intersection of all leaves’ supports; among the corresponding elements with maximal density, we get the missing value(s) by uniform sampling (_Right_).

**The algorithm** To learn a gf, we just have to learn its set of trees. Our training algorithm, gf.Boost (Algorithm 3), performs a greedy top-down induction. In Step 1, we initialize the set of \(T\) trees to \(T\) roots. Steps 2.1 and 2.2 choose a tree (\(\Upsilon_{*}\)) and a leaf to split (\(\lambda_{*}\)) in the tree. In our implementation, we pick the leaf among all trees which is the heaviest with respect to \(\mathbb{R}\). Hence, we merge Steps 2.1 and 2.2. Picking the heaviest leaf is standard to grant boosting in decision tree induction [19]; in our case, there is a second benefit: we tend to learn size-balanced models. For example, during the first \(J=T\) iterations, each of the \(T\) roots gets one split because each root has a larger mass (1) than any leaf in a tree with depth \(>0\). Step 2.4 splits \(\Upsilon_{*}\) by replacing \(\lambda_{*}\) by a stump whose corresponding splitting predicate, p, is returned in Step 2.3 using a weak splitter oracle called splitPred. "weak" refers to boosting's weak/strong learning setting [18] and means that we shall only require lightweight assumptions about this oracle; in decision tree induction, this oracle is the key to boosting from such weak assumptions [19]. This will also be the case for our generative models. We now investigate Step 2.3 and splitPred.

**The weak splitter oracle splitPred** In decision tree induction, a splitting predicate is chosen to reduce an expected Bayes risk (1) (_e.g._ that of the log loss [36], square loss [1], etc.). In our case, splitPred does about the same _with a catch in the binary task it addresses_, which is+ (our mixture measure is thus \(\textsc{M}\doteq\pi\cdot\textsc{R}+(1-\pi)\cdot\textsc{U}\)). The corresponding expected Bayes risk that splitPred seeks to minimize is just:

Footnote †: The prior is chosen by the user: without reasons to do otherwise, a balanced approach suggests \(\pi=0.5\).

\[\underline{\mathbb{L}}(\mathcal{T})\quad\doteq\quad\sum_{\mathcal{E}\in \mathcal{P}(\mathcal{T})}p_{\textsc{M}}[\mathcal{C}]\cdot\underline{L}\left( \frac{\pi p_{\textsc{R}}[\mathcal{C}]}{p_{\textsc{M}}[\mathcal{C}]}\right).\] (4)

The concavity of \(\underline{L}\) implies \(\underline{\mathbb{L}}(\mathcal{T})\leqslant\underline{L}(\pi)\). Notation \(\mathcal{P}(.)\) overloads that in (3) by depending explicitly on the set of trees of \(\textsc{G}\) instead of \(\textsc{G}\) itself. Regarding the minimization of (4), there are three main differences with classical decision tree induction. The first is computational: in the latter case, (4) is optimized over a single tree: there is thus a single element in \(\mathcal{P}(\mathcal{T})\) which is affected by the split, \(\mathcal{X}_{\lambda_{\emptyset}}\). In our case, multiple elements in \(\mathcal{P}(\mathcal{T})\) can be affected by one split, so the optimisation is more computationally demanding but a simple trick allows to keep it largely tractable: we do not need to care about keeping in \(\mathcal{P}(\mathcal{T})\) support elements with zero empirical measure since they will generate no data. Keeping only elements with strictly positive empirical measure guarantees a size \(|\mathcal{P}(\mathcal{T})|\) never bigger than the number of observations defining \(\mathbb{R}\). The second difference plays to our advantage: compared to classical decision tree induction, a single split generally buys a substantially bigger slack in \(\underline{\mathbb{L}}(\mathcal{T})\) in our case. To see this, remark that for the candidate leaf \(\lambda_{*}\),

\[\sum_{\begin{subarray}{c}\mathcal{E}\in\mathcal{P}(\mathcal{T})\\ \mathcal{E}\subseteq\mathcal{X}_{\lambda_{\emptyset}}\end{subarray}}p_{ \textsc{M}}[\mathcal{C}]\cdot\underline{L}\left(\frac{\pi p_{\textsc{R}}[ \mathcal{C}]}{p_{\textsc{M}}[\mathcal{C}]}\right) = p_{\textsc{M}}[\mathcal{X}_{\lambda_{\emptyset}}]\cdot\sum_{ \begin{subarray}{c}\mathcal{E}\in\mathcal{P}(\mathcal{T})\\ \mathcal{E}\subseteq\mathcal{X}_{\lambda_{\emptyset}}\end{subarray}}\frac{p_{ \textsc{M}}[\mathcal{C}]}{p_{\textsc{M}}[\mathcal{X}_{\lambda_{\emptyset}}] }\cdot L\left(\frac{\pi p_{\textsc{R}}[\mathcal{C}]}{p_{\textsc{M}}[ \mathcal{C}]}\right)\] \[\leqslant p_{\textsc{M}}[\mathcal{X}_{\lambda_{\emptyset}}]\cdot \underline{L}\left(\frac{\pi p_{\textsc{R}}[\mathcal{X}_{\lambda_{\emptyset}}] }{p_{\textsc{M}}[\mathcal{X}_{\lambda_{\emptyset}}]}\right)\]

(because \(\underline{L}\) is concave and \(\sum_{\mathcal{E}\in\mathcal{P}(\mathcal{T}),\mathcal{E}\subseteq\mathcal{X} _{\lambda_{\emptyset}}}p_{\textsc{R}}[\mathcal{C}]=p_{\textsc{R}}[\mathcal{X} _{\lambda_{\emptyset}}]\)). The top-left term is the contribution of \(\lambda_{*}\) to \(\underline{\mathbb{L}}(\mathcal{T})\), the bottom-right one its contribution to \(\underline{\mathbb{L}}(\{\Upsilon_{*}\})\) (the decision tree case), so the slack gives a proxy to our potential advantage after splitting. The third difference with decision tree induction is the possibility to converge much faster to a good solution in our case. Scrutinizing the two terms, we indeed get that in the decision tree case, the split only gets two new terms. In our case however, there can be up to \(2\cdot\mathrm{Card}(\{\mathcal{E}\in\mathcal{P}(\mathcal{T}):\mathcal{E} \subseteq\mathcal{X}_{\lambda_{\emptyset}}\})\) new elements in \(\mathcal{P}(\mathcal{T})\).

**Boosting** Two questions remain: can we quantify the slack in decrease and of course what quality guarantee does it bring for the _generative_ model \(\textsc{G}\) whose set of trees is learned by gf.Boost? We answer both questions in a single Theorem, which relies on a weak learning assumption that parallels the classical weak learning assumption of boosting:

**Definition 5.1**.: _(\(\textbf{WLA}(\gamma,\kappa)\)) There exists \(\gamma>0,\kappa>0\) such that at each iteration of gf.Boost, the couple \((\lambda_{*},\mathfrak{p})\) chosen in Steps 2.2, 2.3 of gf.Boost satisfies the following properties: **(a)**\(\lambda_{*}\) is not skinny: \(p_{\textsc{M}}[\mathcal{X}_{\lambda_{\emptyset}}]\geqslant 1/\mathrm{Card}( \Lambda(\Upsilon_{*}))\), **(b)** truth values of \(\mathfrak{p}\) moderately correlates with \(\mathfrak{B}_{\textsc{GEN}}\) at \(\lambda_{*}\): \(p_{\textsc{R}}[\mathfrak{p}|\mathcal{X}_{\lambda_{\emptyset}}]-p_{\textsc{U}}[ \mathfrak{p}|\mathcal{X}_{\lambda_{\emptyset}}]\geqslant\gamma\), and finally **(c)** there is a minimal proportion of real data at \(\lambda_{*}\): \(\pi p_{\textsc{R}}[\mathcal{X}_{\lambda_{\emptyset}}]/p_{\textsc{M}}[ \mathcal{X}_{\lambda_{\emptyset}}]\geqslant\kappa\)._

The convergence proof of [19] reveals that both **(a)** and **(b)** are jointly made at any split, where our **(b)** is equivalent to their _weak hypothesis assumption_ where their \(\gamma\) parameter is twice ours. **(c)**postulates that the leaf split has empirical measure at least a fraction of its uniform measure - thus, of its relative volume. **(c)** is important to avoid splitting leaves that would essentially be useless for our tasks: a leaf for which **(c)** is invalid would indeed locally model comparatively tiny density values.

**Theorem 5.2**.: _Suppose the loss \(\ell\) is **strictly proper** and **differentiable**. Let \(\mathrm{G}_{0}\) (= \(\cup\)) denote the initial \(\mathrm{\textsc{gf}}\) with \(T\) roots in its trees (Step 1, \(\mathrm{\textsc{gf}}\).Boost) and \(\mathrm{G}_{J}\) the final \(\mathrm{\textsc{gf}}\), assuming wlog that the number of boosting iterations \(J\) is a multiple of \(T\). Under \(\mathrm{\textsc{WLA}}(\gamma,\kappa)\), we get the following on likelihood ratio risks: \(\mathbb{D}_{\ell}\left(\mathrm{R},\mathrm{G}_{J}\right)\leq\mathbb{D}_{\ell} \left(\mathrm{R},\mathrm{G}_{0}\right)-\frac{\kappa\gamma^{2}\kappa^{2}}{8}\cdot T \log\left(1+\frac{J}{T}\right)\), for some constant \(\kappa>0\)._

It comes from [25, Remark 1] that we can choose \(\kappa=\inf\{\ell^{\prime}_{-1}-\ell^{\prime}_{1}\}\), which is \(>0\) if \(\ell\) is strictly proper and differentiable. Strict properness is also essential for the loss to guarantee that minimization gets to a good generator (Section 3).

## 6 Experiments

Our code is provided and commented in Appendix, Section V.2. The main setting of our experiments is realistic data generation ('Lifelike'), but we have also tested our method for missing data imputation ('impute') and density estimation ('density'): for this reason, we have selected a broad panel of state of the art approaches to test against, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models, with mice[42], adversarial random forests (ARFs [44]), CT-GANs [47], Forest Flows [17], Vine copulas auto-encoders (VCAE, [41]) and Kernel density estimation (KDE, [4; 34]). All algorithms _not_ using neural networks were ran on a low-end CPU laptop - this was purposely done for our technique. Neural network-based algorithms are run on a bigger machine (technical specs in Appendix, Section V.2). We carried out experiments on a total of 21 datasets, from UCI [10], Kaggle, OpenML, the Stanford Open Policing Project, or simulated. All are presented in Appendix, Section V.1. We summarize results that are presented _in extenso_ in Appendix. Before starting, we complete the 2D heatmap of Table 1 by another one showing our models can also learn deterministic dependences in real-world domains (Table 2). The Appendix also provides an example experiment on interpreting our models for a sensitive domain (in Section V.V.3.1).

**Generation capabilities of our models: lifelike** The objective of the experiment is to evaluate whether a generative model is able to create "realistic" data. The evaluation pipeline is simple: we create for each domain a 5-fold stratified experiment. Evaluation is done via four metrics: a regularized optimal transport ("Sinkhorn, [8]), coverage and density [30] and the F1 score [17]. All metrics are obtained after generating a sample of the same size as the test fold. Sinkhorn evaluates an optimal transport distance between generated and real and F1 estimates the error of a classifier (a 5-nearest neighbor) to distinguish generated vs real (smaller is better for both). Coverage and density are refinements of precision and recall for generative models (the higher, the better). Due to size constraint, we provide results on one set of parameters for "medium-sized" generative forests with \(T=500\) trees, \(J=\)2 000 total splits (Table 5), thus learning very small trees with an average 4 splits per tree. In the Appendix, we provide additional results on even smaller models (\(T=200\), \(J=500\)

\begin{table}
\begin{tabular}{c|c} \hline \hline \(\mathrm{\textsc{gf}}\)\(\gg\) KDE\(\mathrm{\textsc{ineither}}\) & KDE \(\gg\)\(\mathrm{\textsc{gf}}\) \\ \hline
9 & 5 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: density: comparison between us and KDE (summary), counting the number of domains for which we are statistically better (left), or worse (right). The central column counts the remaining domains, for which no statistical significance ever holds.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## Acknowledgments

The authors thank the reviewers for engaging discussions and many suggestions that helped to improve the paper's content.

## References

* [1] L. Breiman, J. H. Freidman, R. A. Olshen, and C. J. Stone. _Classification and regression trees_. Wadsworth, 1984.
* [2] A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation ans classification: structure and applications, 2005. Technical Report, University of Pennsylvania.
* [3] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In _22\({}^{nd}\) KDD_, pages 785-794, 2016.
* [4] Y.-C. Chen. A tutorial on kernel density estimation and recent advances. _Biostatistics \(\&\) Epidemiology_, 1:161-187, 2017.
* [5] Y. Choi, A. Vergari, and G. Van den Broeck. Probabilistic circuits: a unifying framework for tractable probabilistic models, 2020. http://starai.cs.ucla.edu/papers/ProbCirc20.pdf.
* [6] M. Chui, J. Manyika, M. Miremadi, N. Henke, R. Chung P. Nel, and S. Malhotra. _Notes from the AI frontier_. McKinsey Global Institute, 2018.
* [7] A.-H.-C. Correia, R. Peharz, and C.-P. de Campos. Joints in random forests. In _NeurIPS'20_, 2020.
* [8] M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In _NIPS*26_, pages 2292-2300, 2013.
* [9] A. Darwiche. A logical approach to factoring belief networks. In Dieter Fensel, Fausto Giunchiglia, Deborah L. McGuinness, and Mary-Anne Williams, editors, _KR'02_, pages 409-420. Morgan Kaufmann, 2002.
* [10] D. Dua and C. Graff. UCI machine learning repository, 2021.
* [11] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A.-C. Courville. Adversarially learned inference. In _ICLR'17_. OpenReview.net, 2017.
* [12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In _NIPS*27_, pages 2672-2680, 2014.
* [13] L. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on tabular data? In _NeurIPS'22 Datasets and Benchmarks_, 2022.
* [14] A. Grover and S. Ermon. Boosted generative models. In _AAAI'18_, pages 3077-3084. AAAI Press, 2018.
* [15] G.-E. Hinton. The forward-forward algorithm: Some preliminary investigations. _CoRR_, abs/2212.13345, 2022.
* [16] R.C. Holte. Very simple classification rules perform well on most commonly used datasets. _MLJ_, 11:63-91, 1993.
* [17] A. Jolicoeur-Martineau, K. Fatras, and T. Kachman. Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees. In _AISTATS'24_, volume 238, pages 1288-1296. PMLR, 2024.
* [18] M.J. Kearns. Thoughts on hypothesis boosting, 1988. ML class project.
* [19] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In _Proc. of the 28 \({}^{th}\) ACM STOC_, pages 459-468, 1996.
* [20] D.-P. Kingma and M. Welling. Auto-encoding variational bayes. In _ICLR'14_, 2014.
* [21] D.-E. Knuth. Two notes on notation. _The American Mathematical Monthly_, 99(5):403-422, 1992.
* [22] A. Krizhevsky, I. Sutskever, and G.-E. Hinton. ImageNet classification with deep convolutional neural networks. In _NIPS*25_, pages 1106-1114, 2012.

* [23] S. Lang, M. Mundt, F. Ventola, R. Peharz, and K. Kersting. Elevating perceptual sample quality in pcs through differentiable sampling. In _NeurIPS 2021 Workshop on Pre-Registration in Machine Learning, 13 December 2021, Virtual_, volume 181 of _Proceedings of Machine Learning Research_, pages 1-25. PMLR, 2021.
* [24] Q. Li and J.-S. Racine. Nonparametric estimation of distributions with categorical and continuous data. _Journal of Multivariate Analysis_, 86:266-292, 2003.
* [25] Y. Mansour, R. Nock, and R.-C. Williamson. Random classification noise does not defeat all convex potential boosters irrespective of model choice. In _ICML'23_, 2023.
* [26] P. Marechal. On a functional operation generating convex functions, part 1: duality. _J. of Optimization Theory and Applications_, 126:175-189, 2005.
* [27] P. Marechal. On a functional operation generating convex functions, part 2: algebraic properties. _J. of Optimization Theory and Applications_, 126:375-366, 2005.
* [28] D. McElfresh, S. Khandagale, J. Valverde, V. Prasad C, G. Ramakrishnan, M. Goldblum, and C. White. When do neural nets outperform boosted trees on tabular data? In _NeurIPS'23 Datasets and Benchmarks_, 2023.
* [29] A. Menon and C.-S. Ong. Linking losses for density ratio and class-probability estimation. In _33rd ICML_, pages 304-313, 2016.
* [30] M.-F. Naeem, S.-J. Oh, Y. Uh, Y. Choi, and J. Yoo. Reliable fidelity and diversity metrics for generative models. In _ICML'20_, volume 119 of _Proceedings of Machine Learning Research_, pages 7176-7185. PMLR, 2020.
* [31] R. Nock and M. Guillame-Bert. Generative trees: Adversarial and copycat. In _39th ICML_, pages 16906-16951, 2022.
* [32] R. Nock, A.-K. Menon, and C.-S. Ong. A scaled Bregman theorem with applications. In _NIPS'29_, pages 19-27, 2016.
* [33] S. Nowozin, B. Cseke, and R. Tomioka. \(f\)-GAN: training generative neural samplers using variational divergence minimization. In _NIPS'29_, pages 271-279, 2016.
* [34] E. Parzen. On estimation of a probability density function and mode. _The Annals of Mathematical Statistics_, 33:1065-1076, 1962.
* [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [36] J. R. Quinlan. _C4.5 : programs for machine learning_. Morgan Kaufmann, 1993.
* [37] M.-D. Reid and R.-C. Williamson. Information, divergence and risk for binary experiments. _JMLR_, 12:731-817, 2011.
* [38] R. Sanchez-Cauce, I. Paris, and F.-J. Diez. Sum-product networks: A survey. _IEEE Trans.PAMI_, 44(7):3821-3839, 2022.
* [39] L.-J. Savage. Elicitation of personal probabilities and expectations. _J. of the Am. Stat. Assoc._, pages 783-801, 1971.
* Introduction to Covariate Shift Adaptation_. Adaptive computation and machine learning. MIT Press, 2012.
* [41] N. Tagasovska, D. Ackerer, and T. Vatter. Copulas as high-dimensional generative models: Vine copula autoencoders. In _NeurIPS'32_, pages 6525-6537, 2019.
* [42] S. van Buuren and K. Groothuis-Oudshoorn. mice: Multivariate Imputation by Chained Equations in R. _Journal of Statistical Software_, 45(3):1-67, 2011.
* [43] A. Vergari, Y. Choi, and R. Peharz. Probabilistic circuits: Representations, inference, learning and applications, 2022. NeurIPS'22 tutorials.
* [44] D.-S. Watson, K. Blesch, J. Kapar, and M.-N. Wright. Adversarial random forests for density and generative modeling. In _AISTATS'23_, Proceedings of Machine Learning Research. PMLR, 2023.

* [45] I. Witten and E. Frank. _Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations_. Morgan Kaufmann, 1999.
* [46] C. Xiao, P. Zhong, and C. Zheng. BourGAN: Generative networks with metric embeddings. In _NeurIPS'18_, pages 2275-2286, 2018.
* [47] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni. Modeling tabular data using conditional GAN. In _NeurIPS'32_, pages 7333-7343, 2019.

Appendix

To differentiate with the numberings in the main file, the numbering of Theorems, etc. is letter-based (A, B,...).

## Table of contents

### Related work

#### Additional content

Pg 16

#### Supplementary material on proofs

Pg 17

Proof of Lemma 4.4

Proof of Theorem 5.2

#### Simpler models: ensembles of generative trees

Supplementary material on experiments

Pg 25

Domains

Algorithms configuration and choice of parameters

Pg 25

Interpreting our models:'scrutinize'

Pg 27

More examples of Table 1 (mf)

The generative forest of Table 1 (mf) developed further

Full comparisons with mice on missing data imputation

Experiment lifelike _in extenso_

Comparison with "the optimal generator": gen-discrim

Pg 33Related work

The typical Machine Learning (ML) problem usually contains at least three parts: (i) a training algorithm minimizes (ii) a loss function to output an object whose key part is (iii) a model. The main problem we are interested in is data generation as generally captured by "Generative AI". The type of data we are interested in still represents one of the most prevalent form of data: tabular data [6]. When it comes to tabular data, a singular phenomenon of the data world can be observed: there is a fierce competition on part (iii) above, the _models_. When data has other forms, like images, the ML community has generally converged to a broad idea of what the best models look like at a high-level for many generic tasks: neural networks++. Tabular data offers no such consensus yet, even on well-defined tasks like supervised learning [13]. In fact, even on such "simple tasks" the consensus is rather that there is no such consensus [28]. It is an understatement to state that in the broader context of all tasks of interest to us, a sense of a truly intense competition emerges, whose "gradient" clearly points towards simultaneously the most complex / expressive / tractable models, as shown in [43, Slides 27, 53]. One can thus end up finding models based on trees [7, 31, 44], neural networks [12, 14, 20, 47], probabilistic circuits [5, 43, 38], kernel methods [4, 34], graphical models [41] (among others: note that some are in fact hybrid models).

Footnote ‡: Whether such a perception is caused by the models themselves or the dazzling amount of optimisation that has progressively wrapped the models, as advocated for the loss in [23], is not the focus of our paper.

So the tabular data world is blessed with a variety of possible models to solve problems like the ones we are interested in, _but_ - and this is another singular phenomenon of the tabular data world -, getting the best solutions is not necessarily a matter of competing on size or compute. In fact, it can be the opposite: striving for model simplicity or (non exclusive) lightweight tuning can substantially pay off [28]. In relative terms, this phenomenon is not new in the tabular data world: it has been known for more than two decades [16]. That it has endured all major revolutions in ML points to the fact that lasting solutions can be conveniently addressing _all_ three parts (i-iii) above at once on models, algorithms and losses.

From this standpoint, the closest approaches to ours are [44] and [31], first and foremost because the models include trees with a stochastic activation of edges to pick leaves, and a leaf-dependent data generation process. While [31] learn a single tree, [44] use a way to generate data from a set of trees - called an adversarial random forest - which is simple: sample a tree, and then sample an observation from the tree. Hence, the distribution is a convex combination of the trees' density. This is simple but it suffers several drawbacks: (i) each tree has to be accurate enough and thus big enough to model "tricky" data for tree-based models (tricky can be low-dimensional, see the 2D data of Table 1, main file); (ii) if leaves' samplers are simple, which is the case for [31] and our approach, it makes it tricky to learn sets of simple models, such as when trees are stumps (we do not have this issue, see Table 1). In our case, while our models include sets of trees, generating one observation makes use of leaves in _all_ trees instead of just one as in [44] (Figure 1, main file). We note that the primary goal of that latter work is in fact not to generate data [44, Section 4].

Theoretical results that are relevant to data generation are in general scarce compared to the flurry of existing methods if we omit the independent convergence rates of the toolbox often used, such as for (stochastic) gradient descent. Specific convergence results are given in [44], but they are relevant to statistical consistency (infinite sample) and they also rely on assumptions that are not realistic for real world domains, such as Lipschitz continuity of the target density, with second derivative continuous, square integrable and monotonic. The assumption made on models, that splitting probabilities on trees is lowerbounded by a constant, is also impeding to model real world data.

In the generative trees of [31], leaf generation is the simplest possible: it is uniform. This requires big trees to model real-world or tricky data. On the algorithms side, [31] introduce two training algorithms in the generative adversarial networks (GAN) framework [12], thus involving the generator to train but also a discriminator, which is a decision tree in [31]. The GAN framework is very convenient to tackle (ii) above in the context of our tasks because it allows to tie using first principles the problem of learning a density or a sampler and that of training a model (a "discriminator") to distinguish fakes from real, model that parameterizes the loss optimized. An issue with neural networks is that this parameterization has an uncontrollable slack unless the discriminator is extremely complex [33]. The advantage of using trees as in [31] is that for such classifiers, the slack disappears because the models are calibrated, so there is a tight link between training the generator and the discriminator. [31] go further, showing that one can replace the adversarial training by a _copycat_ training, involving copying parts of the discriminator in the generator to speed-up training (also discussed in [15] for neural nets), with strong rates on training in the original boosting model. There is however a limitation in the convergence analysis of [31] as losses have to be symmetric, a property which is not desirable for data generation since it ties the misclassification costs of real and fakes with no argument to do so in general.

Our paper lifts the whole setting of [31] to models that can fit more complex densities using simpler models (Table 1), using sets of trees that can be much smaller than those of [44] (Figure 1); training such models is achieved by merging the two steps of copycat training into a single one where only the generator is trained, furthermore keeping strong rates on training via the original boosting model, all this while getting rid of the undesirable symmetry assumption of [31] for the loss at hand.

## II Additional content

In this additional content, we provide the three ways to pick the trees in a Generative Forest to generate one observation (sequential, concurrent, randomized), and then give a proof that the optimal splitting threshold on a continuous variable when training a generative forest using gf.Boost is always an observed value if there is one tree, but can be another value if there are more (thus highlighting some technical difficulties of one wants to stick to the optimal choice of splitting).

### Sequentially choosing trees in a gf for the generation of observations

### Concurrent generation of observations

We provide a concurrent generation using Algorithm 5, which differs from Algorithm 4 (main file). In concurrent generation, each tree runs concurrently algorithm UpdateSupport (hence the use of the Java-style this handler), with an additional global variables (in addition to \(\mathcal{C}_{s}\), initialized to \(\mathcal{X}\)): a Boolean semaphore accessible implementing a lock, whose value 1 means \(\mathcal{C}\) is available for an update (and otherwise it is locked by a tree in Steps 1.2/1.3 in the set of trees of the gf). We assume that Init has been run beforehand (eventually locally).

``` Input: Trees \(\{\Upsilon_{t}\}_{t=1}^{T}\) of a gf; Output: sampling support \(\mathcal{C}_{s}\) for one observation; Step 1 : \(\mathcal{C}_{s}\leftarrow\mathcal{X}\); Step 2 : Init(\(\{\Upsilon_{t}\}_{t=1}^{T}\)); Step 3 : for\(t\in[T]\) Step 2.1 : while!\(\Upsilon_{t}\).done Step 2.1.1 : StarUpdate(\(\Upsilon_{t},\mathcal{C}_{s},\mathbb{R}\)); return\(\mathcal{C}_{s}\); ```

**Algorithm 5** ConcurrentSupportUpdate

### Randomized generation of observations

We provide a general randomized choice of the sequence of trees for generation, in Algorithm 6.

## III Supplementary material on proofs

### Proof of Lemma 4.4

Given sequence \(\bm{v}\) of dimension \(\dim(\bm{v})\), denote \(\{\mathcal{G}_{j}\}_{j\in[1+\dim(\bm{v})]}\) the sequence of subsets of the domain appearing in the parameters of UpdateSupport through sequence \(\bm{v}\), to which we add a last element, \(\mathcal{C}_{s}\) (and its first element is \(\mathcal{X}\)). If we let \(\mathcal{X}_{j}\) denote the support of the corresponding star node whose Bernoulli event is triggered at index \(j\) in sequence \(\bm{v}\) (for example, \(\mathcal{X}_{1}=\mathcal{X}\)), then we easily get

\[\mathcal{C}_{j} \subseteq \mathcal{X}_{j},\forall j\in[\dim(\bm{v})],\] (5)

indicating the Bernoulli probability in Step 1.1 of StarUpdate is always defined. We then compute

\[p_{\mathrm{G}}\left[\mathcal{C}_{s}|\bm{v}\right] = p_{\mathrm{G}}(\cap_{j}\mathcal{C}_{j})\] (6) \[= \prod_{j=1}^{N}p_{\mathrm{G}}\left[\mathcal{C}_{j+1}|\mathcal{C} _{j}\right]\] (7) \[= \prod_{j=1}^{N}\frac{p_{\mathrm{R}}\left[\mathcal{C}_{i+1}\right] }{p_{\mathrm{R}}\left[\mathcal{C}_{i}\right]}\] (8) \[= \frac{p_{\mathrm{R}}\left[\mathcal{C}_{N+1}\right]}{p_{\mathrm{R }}\left[\mathcal{C}_{0}\right]}\] (9) \[= \frac{p_{\mathrm{R}}\left[\mathcal{C}_{s}\right]}{p_{\mathrm{R} }\left[\mathcal{X}\right]}\] \[= p_{\mathrm{R}}\left[\mathcal{C}_{s}\right].\] (10)

(7) holds because updating the support generation is Markovian in a gf, (8) holds because of Step 3.2 and (9) is a simplification through cancelling terms in products.

### Proof of Theorem 5.2

Notations: we iteratively learn generators \(\mathrm{G}_{0},\mathrm{G}_{1},...,\mathrm{G}_{J}\) where \(\mathrm{G}_{0}\) is just a root. We let \(\mathcal{P}(\mathrm{G}_{j})\) denote the partition induced by the set of trees of \(\mathrm{G}_{j}\), recalling that each element is the (non-empty) intersection of the support of a set of leaves, one for each tree (for example, \(\mathcal{P}(\mathrm{G}_{0})=\{\mathcal{X}\}\)). The criterion minimized to build \(\mathrm{G}_{j+1}\) from \(\mathrm{G}_{j}\) is

\[\mathbb{L}(\mathrm{G}_{j}) \doteq \sum_{\mathcal{C}\in\mathcal{P}(\mathrm{G}_{j})}p_{\mathrm{M}}[ \mathcal{C}]\cdot\underline{L}\left(\frac{\pi p_{\mathrm{R}}[\mathcal{C}]}{p_ {\mathrm{M}}[\mathcal{C}]}\right).\] (11)

The proof entails fundamental notions on loss functions, models and boosting. We start with loss functions. A key function we use is

\[g^{\pi}(t)\doteq(\pi t+1-\pi)\cdot\underline{L}\left(\frac{\pi t}{\pi t+1-\pi }\right),\forall t\in\mathbb{R}_{+},\]

which is concave [37, Appendix A.3].

**Lemma A**.: \[\mathbb{D}_{\ell}\left(\mathrm{R},\mathrm{G}_{J-1}\right)-\mathbb{D}_{ \ell}\left(\mathrm{R},\mathrm{G}_{J}\right) = \mathbb{L}(\mathrm{G}_{J-1})-\mathbb{L}(\mathrm{G}_{J}).\] (12)

Proof.: We make use of the following important fact about gfs:

[MISSING_PAGE_EMPTY:18]

We now come to models and boosting. Suppose we have \(t\) trees \(\{\Upsilon_{1},\Upsilon_{2},...,\Upsilon_{t}\}\) in \(\mathds{G}_{j}\). We want to split leaf \(\lambda\in\Upsilon_{t}\) into two new leaves, \(\lambda_{t},\lambda_{t}\). Let \(\mathcal{P}_{\lambda}(\mathds{G}_{j})\) denote the subset of \(\mathcal{P}(\mathds{G}_{j})\) containing only the subsets defined by intersection with the support of \(\lambda\), \(\mathcal{X}_{\lambda}\). The criterion we minimize can be reformulated as

\[\mathbb{L}(\mathds{G}_{j}) = \sum_{\lambda\in\Lambda(\Upsilon)}p_{\,\mathbb{M}}[\mathcal{X}_{ \lambda}]\cdot\sum_{\mathcal{E}\in\mathcal{P}_{\lambda}(\mathds{G}_{j})}\frac{p _{\,\mathbb{M}}[\mathcal{E}]}{p_{\,\mathbb{M}}[\mathcal{X}_{\lambda}]}\cdot \underline{L}\left(\frac{\pi p_{\mathbb{R}}[\mathcal{E}]}{p_{\,\mathbb{M}}[ \mathcal{E}]}\right),\Upsilon\in\{\Upsilon_{1},\Upsilon_{2},...,\Upsilon_{t}\}.\]

Here, \(\Upsilon\) is any tree in the set of trees, since \(\cup_{\lambda\in\Lambda(\Upsilon)}\mathcal{P}_{\lambda}(\mathds{G}_{j})\) covers the complete partition of \(\mathcal{X}\) induced by \(\{\Upsilon_{1},\Upsilon_{2},...,\Upsilon_{t}\}\). If we had a single tree, the inner sum would disappear since we would have \(\mathcal{P}_{\lambda}(\mathds{G}_{j})=\{\mathcal{X}_{\lambda}\}\) and so one iteration would split one of these subsets. In our case however, with a set of trees, we still split \(\mathcal{X}_{\lambda}\) but the impact on the reduction of \(\mathbb{L}\) can be substantially better as we may simultaneously split as many subsets as there are in \(\mathcal{P}_{\lambda}(\mathds{G}_{j})\). The reduction in \(\underline{\mathbb{L}}\) can be obtained by summing all reductions to which contribute each of the subsets.

To analyze it, we make use of a reduction to the ModaBoost algorithm of [25]. We present the algorithm in Algorithm 7. The original ModaBoost algorithm trains _partition-linear models_, _i.e._ models whose output is defined by a sum of reals over a set of partitions to which the observation to be classified belongs to. Training means then both learning the organisation of partitions and the reals - which are just the output of a weak classifier given by a weak learner, times a leveraging constant computed by ModaBoost. As in the classical boosting literature, the original ModaBoost includes the computation and updates of _weights_.

In our case, the structure of partition learned is that of a set of trees, each weak classifier \(h\) is of the form

\[h(\bm{x}) \doteq K\cdot\mathsf{p}(\bm{x}),\]

where \(K\) is a real and \(\mathsf{p}\) is a Boolean predicate, usually doing an axis-parallel split of \(\mathcal{X}\) on an observed feature, such as \(\mathsf{p}(\bm{x})\equiv 1_{x_{i}\succeq a}\).

From now on, it will be interesting to dissociate our generator \(\mathds{G}_{j}\) - which includes a model structure and an algorithm to generate data from this structure- from its set of trees - _i.e._ its model structure - which we denote \(\Gamma_{j}\doteq\{\Upsilon_{t}\}_{t=1}^{T}\). ModaBoost learns both \(\Gamma_{j}\) but also predictions for each possible outcome, predictions that we shall not use since the loss we are minimizing can be made to depend only on the model structure \(\Gamma_{j}\). Given the simplicity of the weak classifiers \(h\) and the particular nature of the partitions learned, the original ModaBoost of [25] can be simplified to our presentation in Algorithm 7. Among others, the simplification discards the weights from the presentation of the algorithm. What we show entangles two objectives on models and loss functions as we show that

ModaBoost _learns the same structure \(\Gamma\) as our_ GF.Boost_, and yields a guaranteed decrease of \(\mathbb{L}(\mathbb{G})\),_

and it is achieved via the following Theorem.

**Theorem B**.: ModaBoost _greedily minimizes the following loss function:_

\[\mathbb{L}(\Gamma_{j}) = \sum_{\lambda\in\Lambda(\Upsilon)}p_{\,\mathbb{M}}[\mathcal{X}_{ \lambda}]\cdot\sum_{\mathbb{E}\in\mathcal{P}_{\lambda}(\Gamma_{j})}\frac{p_{\, \mathbb{M}}[\mathcal{C}]}{p_{\,\mathbb{M}}[\mathcal{X}_{\lambda}]}\cdot \mathbb{L}\left(\frac{\pi p_{\mathbb{R}}[\mathcal{C}]}{p_{\,\mathbb{M}}[ \mathcal{C}]}\right),\Upsilon\in\Gamma_{j}.\]

_Furthermore, suppose there exists \(\gamma>0,\kappa>0\) such that at each iteration \(j\), the predicate \(\mathfrak{p}\) splits \(\mathcal{X}_{\lambda}\) of leaf \(\lambda\in\Lambda(\Upsilon)\) into \(\mathcal{X}_{\lambda}^{p}\) and \(\mathcal{X}_{\lambda}^{-p}\) (same nomenclature as in (17), (19)) such that:_

\[p_{\,\mathbb{M}}[\mathcal{X}_{\lambda}] \geqslant \frac{1}{\mathrm{Card}(\Lambda(\Upsilon))},\] (22) \[\left|\frac{\int_{\mathcal{X}_{\lambda}^{p}}\mathrm{d}\mathbf{R}} {\int_{\mathcal{X}_{\lambda}}\mathrm{d}\mathbf{R}}-\frac{\int_{\mathcal{X}_{ \lambda}^{p}}\mathrm{d}\mathbf{U}}{\int_{\mathcal{X}_{\lambda}}\mathrm{d} \mathbf{U}}\right| \geqslant \gamma,\] (23) \[\min\left\{\frac{\pi p_{\mathbb{R}}[\mathcal{X}_{\lambda}]}{p_{\, \mathbb{M}}[\mathcal{X}_{\lambda}]},1-\frac{\pi p_{\mathbb{R}}[\mathcal{X}_{ \lambda}]}{p_{\,\mathbb{M}}[\mathcal{X}_{\lambda}]}\right\} \geqslant \kappa.\] (24)

_Then we have the following guaranteed slack between two successive models:_

\[\mathbb{L}(\Gamma_{j+1})-\mathbb{L}(\Gamma_{j}) \leqslant -\frac{\kappa\gamma^{2}\kappa^{2}}{8\mathrm{Card}(\Lambda(\Upsilon ))},\] (25)

_where \(\kappa\) is such that \(0<\kappa\leqslant\inf\{\ell_{-1}^{\prime}-\ell_{1}^{\prime}\}\)._

Proof sketchThe loss function comes directly from [25, Lemma 7]. At each iteration, ModaBoost makes a number of updates that guarantee, once Step 2.4 is completed (because

Figure III.1: Learning a set of two trees \(\Gamma\) with ModaBoost. If we want a split at leaf \(\lambda\) indicated, then it would takes two steps (but a _single_ application of the weak learning assumption, [25, Lemma 5]) of the original ModaBoost to learn \(\Upsilon_{1}\) alone [25, Section 4]. Each step, figured with a plain arrow, ends with adding a new leaf. In our case however, we have to take into account the partition induced by the leaves of \(\Upsilon_{2}\), which results in considering not one but three subsets in \(\mathcal{P}_{\lambda}\), thus adding not one but three weak hypotheses simultaneously (one for each of the dashed arrows on \(\Upsilon_{2}\)). Thanks to Jensen’s inequality, a guaranteed decrease of the loss at hand can be obtained by a single application of the weak learning assumption at \(\lambda\), just like in the original ModaBoost.

the elements in \(\mathcal{P}_{\lambda}(\Gamma_{j})\) are disjoint)

\[\mathbb{L}(\Gamma_{j+1})-\mathbb{L}(\Gamma_{j}) \leqslant -\frac{\kappa}{2}\cdot\sum_{\mathbb{G}\in\mathcal{P}_{\lambda}( \Gamma_{j})}p_{\,\mathrm{M}}[\mathcal{C}]\cdot\mathbb{E}_{\,\mathrm{M}| \mathbb{G}}\left[(w_{j+1}-w_{j})^{2}\right]\] (26) \[= -\frac{\kappa}{2}\cdot p_{\,\mathrm{M}}[\mathcal{X}_{\lambda}] \mathbb{E}_{\,\mathrm{M}|\mathcal{X}_{\lambda}}\left[(w_{j+1}-w_{j})^{2}\right],\] (27)

where the weights are given in [25]. Each expression in the summand of (26) is exactly the guarantee of [25, ineq. before (69)]; all such expressions are not important; what is more important is (26): all steps occurring in Step 2.4 are equivalent to a single step of ModaBoost carried out over the whole \(\mathcal{X}_{\lambda}\). Overall, the number of "aggregated" steps match the counter \(j\) in ModaBoost. We then just have to reuse the proof of [25, Theorem B], which implies, in lieu of their [25, eq. (74)]

\[\mathbb{L}(\Gamma_{j+1})-\mathbb{L}(\Gamma_{j}) \leqslant\] (28)

with \(\underline{L}^{\mathrm{s}0}(u)\doteq u(1-u)\). Noting \(2\underline{L}^{\mathrm{s}0}(u)\geq\min\{u,1-u\}\), we then use (22) - (24), which yields the statement of the Theorem. 

As a consequence, if we assume that the total number of boosting iterations \(J\) is a multiple of the number of trees \(T\), it comes from [25, eq. (29)] that after \(J\) iterations, we have

\[\mathbb{L}(\Gamma_{J})-\mathbb{L}(\Gamma_{0}) \leqslant -\frac{\kappa\mathrm{\gamma}^{2}\kappa^{2}}{8}\cdot T\log\left(1+ \frac{J}{T}\right).\] (29)

Using Lemma A and the fact that the induction of the sets of trees in ModaBoost is done in the same way as the induction of the set of trees of our generator \(\mathbb{G}\) in gf.Boost, we get:

\[\pi\cdot\mathbb{D}_{\ell}\left(\mathbb{R},\mathbb{G}_{J}\right) = \pi\cdot\mathbb{D}_{\ell}\left(\mathbb{R},\mathbb{G}_{0}\right)+ \left(\mathbb{L}(\mathbb{G}_{J})-\mathbb{L}(\mathbb{G}_{0})\right)\] \[= \pi\cdot\mathbb{D}_{\ell}\left(\mathbb{R},\mathbb{G}_{0}\right)+ \left(\mathbb{L}(\Gamma_{J})-\mathbb{L}(\Gamma_{0})\right)\] \[\leqslant \pi\cdot\mathbb{D}_{\ell}\left(\mathbb{R},\mathbb{G}_{0}\right)- \frac{\kappa\mathrm{\gamma}^{2}\kappa^{2}}{8}\cdot T\log\left(1+\frac{J}{T} \right),\]

as claimed.

**Remark C**.: _One may wonder what is the relationship between the loss that we minimize and "conventional" losses used to train generative models. The advantage of our Theorem 5.2 is that by choosing different proper losses, one can get convergence guarantees for different "conventional" losses. Let us illustrate this with the kl divergence between measures \(\mathrm{A}\) and \(\mathrm{B}\), noted \(\mathtt{kl}(\mathrm{A}|\mathrm{B})\)._

**Lemma D**.: _Suppose the prior satisfies \(\pi>0\) and \(\mathbb{R}\) absolutely continuous with respect to \(\mathbb{G}\). Then for the choice \(\ell=\ell^{\mathrm{s}00}\) = log-loss, we get_

\[\mathbb{D}_{\ell^{\mathrm{s}00}}\left(\mathbb{R},\mathbb{G}\right) = \pi\cdot\mathtt{kl}\left(\mathbb{R}|\mathbb{G}\right)-\mathtt{kl} \left(\pi\mathrm{dR}+(1-\pi)\mathrm{dU}|\pi\mathrm{dG}+(1-\pi)\mathrm{dU} \right).\] (30)

Proof.: We first recall that for any convex \(F\), we have with our choice of \(g\) (assuming \(\pi>0\)),

\[\tilde{F}(z) = \frac{\pi z+1-\pi}{\pi}\cdot F\left(\frac{\pi z}{\pi z+1-\pi} \right),\] (31) \[\tilde{F}^{\prime}(z) = F\left(\frac{\pi z}{\pi z+1-\pi}\right)+\frac{1-\pi}{\pi z+1-\pi }\cdot F^{\prime}\left(\frac{\pi z}{\pi z+1-\pi}\right).\] (32)

The log-loss has \(\ell^{\mathrm{s}00}_{1}(u)=-\log u,\ell^{\mathrm{s}00}_{-1}(u)=-\log(1-u)\). It is strictly proper and so it comes

\[\left(\widetilde{-L}\right)(z) = z\cdot\log\left(\frac{\pi z}{\pi z+1-\pi}\right)+\frac{1-\pi}{ \pi}\cdot\log\left(\frac{1-\pi}{\pi z+1-\pi}\right).\]

We thus get, in our case

\[\left(\widetilde{-L}\right)\left(\frac{\mathrm{dR}}{\mathrm{dU} }\right) = \frac{\mathrm{dR}}{\mathrm{dU}}\cdot\log\left(\frac{\pi\mathrm{dR} }{\pi\mathrm{dR}+(1-\pi)\mathrm{dU}}\right)+\frac{1-\pi}{\pi}\cdot\log\left( \frac{(1-\pi)\mathrm{dU}}{\pi\mathrm{dR}+(1-\pi)\mathrm{dU}}\right),\] (33)and using (32) and the fact that for the log-loss \((-L)^{\prime}(u)=\log(u/(1-u))\),

\[\left(\widetilde{-L}\right)^{\prime}\left(\frac{\mathrm{d}G}{ \mathrm{d}U}\right) = \frac{\pi\mathrm{d}G}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U}\cdot\log \left(\frac{\pi\mathrm{d}G}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U}\right)\] (34) \[+\frac{(1-\pi)\mathrm{d}U}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U} \cdot\log\left(\frac{(1-\pi)\mathrm{d}U}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U}\right)\] \[+\frac{(1-\pi)\mathrm{d}U}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U} \cdot\log\left(\frac{\pi\mathrm{d}G}{(1-\pi)\mathrm{d}U}\right)=\log\left( \frac{\pi\mathrm{d}G}{\pi\mathrm{d}G+(1-\pi)\mathrm{d}U}\right)\]

Finally, we get, assuming \(\mathrm{R}\) absolutely continuous with respect to \(\mathrm{G}\),

\[\mathbb{D}_{\ell^{\mathrm{LOO}}}\left(\mathrm{R},\mathrm{G}\right)\] (35) \[\doteq \pi\cdot\int_{\mathcal{X}}\mathrm{d}U\cdot\left(\left(\widetilde{ -L}\right)\left(\frac{\mathrm{d}R}{\mathrm{d}U}\right)-\left(\widetilde{-L} \right)\left(\frac{\mathrm{d}G}{\mathrm{d}U}\right)-\left(\frac{\mathrm{d}R- \mathrm{d}G}{\mathrm{d}U}\right)\cdot\left(\widetilde{-L}\right)^{\prime} \left(\frac{\mathrm{d}G}{\mathrm{d}U}\right)\right)\] \[= \int_{\mathcal{X}}\left\{\begin{array}{l}\pi\mathrm{d}R\cdot \log\left(\frac{\pi\mathrm{d}R}{\pi\mathrm{d}R+(1-\pi)\mathrm{d}U}\right)+(1- \pi)\mathrm{d}U\cdot\log\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\right)\\ -\pi\mathrm{d}G\cdot\log\left(\frac{\pi\mathrm{d}G}{\pi\mathrm{d}G-(1-\pi) \mathrm{d}U}\right)-(1-\pi)\mathrm{d}U\cdot\log\left(\frac{(1-\pi)\mathrm{d}U} {\pi\mathrm{d}G+(1-\pi)\mathrm{d}U}\right)\\ -\pi\mathrm{d}R\cdot\log\left(\frac{\pi\mathrm{d}G}{\pi\mathrm{d}G+(1-\pi) \mathrm{d}U}\right)+\pi\mathrm{d}G\cdot\log\left(\frac{\pi\mathrm{d}G}{\pi \mathrm{d}G+(1-\pi)\mathrm{d}U}\right)\end{array}\right.\] \[= \int_{\mathcal{X}}\left\{\begin{array}{l}\pi\mathrm{d}R\cdot \log\left(\frac{\mathrm{d}R}{\mathrm{d}G}\right)\\ -\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\right)\cdot\log\left(\pi\mathrm{d}R+( 1-\pi)\mathrm{d}U\right)\\ +\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\right)\cdot\log\left(\pi\mathrm{d}G+( 1-\pi)\mathrm{d}U\right)\end{array}\right.\] (36) \[= \pi\cdot\textsc{kl}\left(\mathrm{R}\middle|\mathrm{G}\right)- \textsc{kl}\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\middle\|\pi\mathrm{d}G+(1- \pi)\mathrm{d}U\right),\] (37)

as claimed (end of the proof of Lemma D). 

_Because of the joint convexity of KL divergence, we always have_

\[\textsc{kl}\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\middle\|\pi \mathrm{d}G+(1-\pi)\mathrm{d}U\right) \leq \pi\textsc{kl}\left(\mathrm{R}\middle|\mathrm{G}\right)+(1-\pi) \textsc{kl}\left(\mathrm{d}U\middle|\mathrm{d}U\right)\] \[=\pi\textsc{kl}\left(\mathrm{R}\middle|\mathrm{G}\right),\]

_which yields (another proof) that our loss, \(\mathbb{D}_{\ell^{\mathrm{LOO}}}\left(\mathrm{R},\mathrm{G}\right)\), is lowerbounded by 0. In general, if we let \(\gamma>0\) such that \(\textsc{kl}\left(\pi\mathrm{d}R+(1-\pi)\mathrm{d}U\middle\|\pi\mathrm{d}G+(1-\pi) \mathrm{d}U\right)\leq(1-\gamma)\cdot\pi\textsc{kl}\left(\mathrm{R}\middle| \mathrm{G}\right)\) (The strict positivity of \(\gamma\) is also a weak assumption as long as \(\mathrm{R},\mathrm{G}\) sufficiently differ from the uniform distribution), then_

\[\mathbb{D}_{\ell^{\mathrm{LOO}}}\left(\mathrm{R},\mathrm{G}\right) \geq \gamma\pi\cdot\textsc{kl}\left(\mathrm{R}\middle|\mathrm{G}\right),\] (38)

_so any upperbound on \(\mathbb{D}_{\ell^{\mathrm{LOO}}}\left(\mathrm{R},\mathrm{G}\right)\) (such as obtained from Theorem 5.2) translates to an upperbound on the KL divergence. Note that the condition for absolute continuity in Lemma D is always met with the models we learn (both generative forests and ensembles of generative trees)._

## IV Simpler models: ensembles of generative trees

Storing a gf requires keeping information about the empirical measure \(\mathrm{R}\) to compute the branching probability in Step 1 of StarUpdate. This does not require to store the full training sample, but requires at least an index table recording the \(\left\{\mathrm{leaves}\right\}\times\left\{\mathrm{observations}\right\}\) association, for a storage cost in between \(\Omega(m)\) and \(O(mT)\) where \(m\) is the size of the training sample. There is a simple way to get rid of this constraint and approximate the gf by a set of _generative trees_ (gts) of [31].

Figure IV.1: A generative tree (gt) associated to UCI German Credit.

[MISSING_PAGE_FAIL:23]

(43) is due to the fact that, since \(\mathcal{C}\subseteq\mathcal{X}_{\Upsilon,\nu^{*}},P_{\mathrm{R}}\left[\mathcal{C}_ {\nu}\right]=p_{\mathrm{R}}\left[\mathcal{C}_{\nu}\cap\mathcal{X}_{\Upsilon, \nu^{*}_{\nu}}\right]=p_{\mathrm{R}}\left[\mathcal{C}_{\nu}|\mathcal{X}_{ \Upsilon,\nu^{*}_{\nu}}\right]p_{\mathrm{R}}\left[\mathcal{X}_{\Upsilon,\nu^{* }_{\nu^{*}}}\right]\), and then using (39). (44) is obtained by dividing numerator and denominator by \(p_{\mathrm{R}}\left[\mathcal{X}_{\Upsilon,\nu^{*}_{\mathrm{t}}}\right]+p_{ \mathrm{R}}\left[\mathcal{X}_{\Upsilon,\nu^{*}_{\mathrm{t}}}\right]=p_{ \mathrm{R}}\left[\mathcal{X}_{\Upsilon,\nu^{*}_{\mathrm{t}}}\right]\).

\(\mathrm{G}\) and \(\mathrm{\hat{G}}\) satisfy \(\mathcal{P}(\mathrm{G})=\mathcal{P}(\mathrm{\hat{G}})\) so

\[\mathrm{kl}(\mathrm{G}|\mathrm{\hat{G}}) = \int\mathrm{d}\mathrm{G}\log\frac{\mathrm{d}\mathrm{G}}{\mathrm{ d}\mathrm{\hat{G}}}\] (45) \[= \sum_{\mathcal{C}\in\mathcal{P}(\mathrm{G})}p_{\mathrm{G}}[ \mathcal{C}]\log\frac{p_{\mathrm{G}}[\mathcal{C}]}{p_{\mathrm{\hat{G}}}[ \mathcal{C}]}.\] (46)

Finally, \(p_{\mathrm{G}}[\mathcal{C}]\) and \(p_{\mathrm{\hat{G}}}[\mathcal{C}]\) are just the product of the branching probabilities in any admissible sequence. If we use the same admissible sequence in both generators, we can write \(p_{\mathrm{G}}[\mathcal{C}]=\prod_{j=1}^{n(\mathcal{C})}p_{j}\) and \(p_{\mathrm{\hat{G}}}[\mathcal{C}]=\prod_{j=1}^{n(\mathcal{C})}\hat{p}_{j}\), and (44) directly yields \(p_{j}\leq\exp(2\varkappa)\cdot\hat{p}_{j},\forall j\in[n(\mathcal{C})]\), \(n(\mathcal{C})\) being a shorthand for \(\mathrm{depth}_{\mathrm{G}}(\mathcal{C})=\mathrm{depth}_{\mathrm{\hat{G}}}( \mathcal{C})\) (see main file). So, for any \(\mathcal{C}\in\mathcal{P}(\mathrm{G})\),

\[\frac{p_{\mathrm{G}}[\mathcal{C}]}{p_{\mathrm{\hat{G}}}[ \mathcal{C}]} \leq \exp(2\varkappa\cdot\mathrm{depth}_{\mathrm{G}}(\mathcal{C})),\] (47)

and finally, replacing back \(n(\mathcal{C})\) by notation \(\mathrm{depth}(\mathcal{C})\),

\[\mathrm{kl}(\mathrm{G}|\mathrm{\hat{G}}) \leq 2\varkappa\cdot\sum_{\mathcal{C}\in\mathcal{P}(\mathrm{G})}p_{ \mathrm{G}}[\mathcal{C}]\cdot\mathrm{depth}(\mathcal{C})\] (48) \[=2\varkappa\cdot\overline{\mathrm{depth}(\mathrm{G})},\]

as claimed. 

Note the additional leverage for training and generation that stems from Assumption A, not just in terms of space: computing \(p_{\mathrm{U}}\left[\mathcal{C}_{\nu}|\mathcal{X}_{\Upsilon,\nu^{*}_{\nu^{*}}}\right]\) is \(O(d)\) and does not necessitate data so the computation of key conditional probabilities (40) drops from \(\Omega(m)\) to \(O(d)\) for training and generation in an eogt. Lemma C provides the change in StarUpdate to generate data.

TrainingTo train an eogt, we cannot rely on the idea that we can just train a gf and then replace each of its trees by generative trees. To take a concrete example of how this can be a bad idea in the context of missing data imputation, we have observed empirically that a generative forest (gf) can have many trees whose node's observation variables are the _same_ within the tree. Taken independently of the forest, such trees would only model marginals, but in a gf, sampling is dependent on the other trees and Lemma 4.4 guarantees the global accuracy of the forest. _However_, if we then replace the gf by an eogt with the same trees and use them for missing data imputation, this results in imputation at the mode of the marginal(s) (exclusively), which is clearly suboptimal. To avoid this, we have to use a specific training for an eogt, and to do this, it is enough to change a key part of training in splitPred, the computation of probabilities \(p_{\mathrm{R}}[.]\) used in (4). Suppose \(\varkappa\) very small in Assumption A. To evaluate a split at some leaf, we observe (suppose we have a single tree)

\[p_{\mathrm{R}}[\mathcal{X}_{\lambda_{\mathrm{t}}}]=p_{\mathrm{R}}[\mathcal{X}_ {\lambda}]\cdot p_{\mathrm{R}}[\mathcal{X}_{\lambda_{\mathrm{t}}}|\mathcal{X}_ {\lambda}]\approx p_{\mathrm{R}}[\mathcal{X}_{\lambda}]\cdot p_{\mathrm{U}}[ \mathcal{X}_{\lambda_{\mathrm{t}}}|\mathcal{X}_{\lambda}]\]

(and the same holds for \(\lambda_{\mathrm{t}}\)). Extending this to multiple trees and any \(\mathcal{C}\in\mathcal{P}(\mathcal{T})\), we get the computation of any \(p_{\mathrm{R}}[\mathcal{C}_{\mathrm{f}}]\) and \(p_{\mathrm{R}}[\mathcal{C}_{\mathrm{t}}]\) needed to measure the new (4) for a potential split. Crucially, it does not necessitate to split the empirical measure at \(\mathcal{C}\) but just relies on computing \(p_{\mathrm{U}}[\mathcal{C}_{\nu}|\mathcal{C}],\nu\in\{\mathtt{f},\mathtt{t}\}\): with the product measure and since we make axis-parallel splits, it can be done in \(O(1)\), thus substantially reducing training time.

More with ensembles of generative treeswe can follow the exact same algorithmic steps as for generative trees to perform missing data imputation and density estimation, but use branching probabilities in the trees to decide branching, instead of relying on the empirical measure at tuples of nodes, which we do not have access to anymore. For this, we rely on the approximation (40) in Lemma C. A key difference with a gf is that no tuple can have zero density because branching probabilities are in \((0,1)\) in each generative tree.

## V Appendix on experiments

### Domains

ringGauss is the seminal 2D ring Gaussians appearing in numerous GAN papers [46]; those are eight (8) spherical Gaussians with equal covariance, sampling size and centers located on regularly spaced (2-2 angular distance) and at equal distance from the origin. gridGauss was generated as a decently hard task from [11]: it consists of 25 2D mixture spherical Gaussians with equal variance and sampled sizes, put on a regular grid. circGauss is a Gaussian mode surrounded by a circle, from [46]. randGauss is a substantially harder version of ringGauss with 16 mixture components, in which covariance, sampling sizes and distances on sightlines from the origin are all random, which creates very substantial discrepancies between modes.

### Algorithms configuration and choice of parameters

gf.BoostWe have implemented gf.Boost in Java, following Algorithm 3's blueprint. Our implementation of tree and leaf in Steps 2.1, 2.2 is simple: we pick the heaviest leaf among all trees (with respect to \(\mathrm{R}\)). The search for the best split is exhaustive unless the variable is categorical with more than a fixed number of distinct modalities (22 in our experiments), above that threshold, we pick the best split among a random subset. We follow [31]'s experimental setting: in particular, the input of our algorithm to train a generator is a.csv file containing the training data _without any further information_. Each feature's domain is learned from the training data only; while this could surely and trivially be replaced by a user-informed domain for improved results (_e.g._ indicating a proportion's domain as \([0\%,100\%]\), informing the complete list of socio-professional categories, etc.) -- and is in fact standard in some ML packages like weka's ARFF files, we did not pick this option to alleviate all side information available to the GT learner. Our software automatically recognizes three types of variables: nominal, integer and floating point represented.

**Comments on implementation** For the interested reader, we give here some specific implementation details regarding choices made for two classical bottlenecks on tree-based methods (this also applies to the supervised case of learning decision trees): handling features with large number of modalities and handling continuous features.

We first comment the case where the number of modalities of a feature is large. The details can be found in the code in File Algorithm.java (class Algorithm), as follows:

* method public GenerativeModelBasedOnEnsembleOfTrees learn_geot() implements both GF.Boost (for generative forests) and its extension to training ensembles of generative trees (Appendix) as a single algorithm.
* method Node choose_leaf(String how_to_choose_leaf) is the method to choose a leaf (Step 2.2). Since it picks the heaviest leaf among all trees, it also implements Step 2.1 (we initialize all trees to their roots; having a tree = root for generation does not affect generation).
* method public boolean split_generative_forest(Node leaf, HashSet <MeasuredSupportAtTupleOfNodes> tol) finds the split for a Generative Forest, given a node chosen for the split. Here is how it works:

1. It first computes the complete description of possible splits (not their quality yet), using method public static Vector<FeatureTest> ALL_FEATURE_TESTS(Feature f, Dataset ds) in Feature.java. The method is documented: for continuous features, the list of splits is just a list of evenly spaced splits.
2. then it calls public SplitDetails split_top_down_boosting_generative_forest_fast(Node leaf, boolean [] splittable_feature, FeatureTest [][] all_feature_tests, HashSet <MeasuredSupportAtTupleOfNodes> tol), which returns the best split as follows: (i) it shuffles the potential list of splits and then (ii) picks the best one in the sublist containing the first Algorithm.MAXIMAL_NUMBER_OF_SPLIT_TESTS_TRIES_PER_BOOSTING_ITERATION elements (if the list is smaller, the search for the best is exhaustive); this class variable is currently = 1000.

Last, we comment how we handle continuous variables: the boosting theory tells us that finding a moderately good split (condition (b) Definition 5.1, main file) is enough to get boosting-compliant convergence (Theorem 5.2, main file), so we have settled for a trivial and efficient mechanism: cut-points are evenly spaced and the number is fixed beforehand. See method public static Vector<FeatureTest> ALL_FEATURE_TESTS(Feature f, Dataset ds) in Feature.java for the details. It turns out that this works very well and shows our theory was indeed concretely used in the design/implementation of the training algorithm.

miceWe have used the R mice package V 3.13.0 with two choices of methods for the round robin (column-wise) prediction of missing values: cart[1] and random forests (rf)[42]. In that last case, we have replaced the default number of trees (10) by a larger number (100) to get better results. We use the default number of round-robin iterations (5). We observed that random forests got the best results so, in order not to multiply the experiments reported and perhaps blur the comparisons with our method, we report only mice's results for random forests.

TensorFlowTo learn the additional Random Forests involved in experiments gen-discrim, we used Tensorflow Decision Forests library8. We use 300 trees with max depth 16. Attribute sampling: sqrt(number attributes) for classification problems, number attributes / 3 for regression problems (Breiman rule of thumb); the min \(\#\)examples per leaf is 5.

Footnote 8: https://github.com/gogble/yggdrasil-decision-forests/blob/main/documentation/learners.md

Footnote 9: https://github.com/sdv-dev/CTGAN

CT-GANWe used the Python implementation++ with default values [47].

Adversarial Random ForestsWe used the R code of the generator forge made available from the paper [44]1, learning forests containing a variable number of trees in \(\{10,50,100,200\}\). We noted that the code does not run when the dataset has missing values and we also got an error when trying to run the code on kc1.

Footnote 1: https://github.com/bips-hb/arf_paper

Vine Copulas AutoEncodersWe used the Python code available from the paper [41]2, which processes only fully numerical datasets. We got a few errors when trying to run the code on compas.

Footnote 2: https://github.com/samsungSAILMontreal/ForestDiffusion.

Forest FlowsWe used the R code available from the paper [17]3. Hyperparameters used were default parameters44.

Footnote 3: https://htmlpreview.github.io/?https://github.com/SamsungSAILMontreal/ForestDiffusion/master/R-Package/Vignette.html.

Kernel Density EstimationWe used the R code available in the package npudens with default values following the approach of [24]. We tried several kernels but ended up with sticking to the default choices, that seemed to provide overall some of the best results. Compared to us with gf, KDE's code is extremely compute-intensive as on domains below itctaco in Table A1: it took orders of magnitude more than ours to get the density values on all folds, so we did not run it on the biggest domains. Notice that in our case, computation time includes not just training the generative model, but also, _at each applicable iteration_\(J\) in gf.Boost, the computation of the same number of density values as for KDE.

Computers usedWe ran part of the experiments on a Mac Book Pro 16 Gb RAM w/ 2 GHz Quad-Core Intel Core i5 processor, and part on a desktop Intel(R) Xeon(R) 3.70GHz with 12 cores and 64 Gb RAM. CT-GANs and VCAEs were run on the desktop, the other algorithms on the laptop.

### Supplementary results

Due to the sheer number of tables to follow, we shall group them according to topics

#### v.v.3.1 Interpreting our models:'scrutinize'

Table A2 provides experimental results on sensitive real world domain compas. It shows that it is easy to flag potential bias / fairness issues in data by directly estimating conditional probabilities: considering the gf of this example and ignoring variable age for simplicity, conditionally to having 0 felony count and race being African-American, the probability of generating an observation with two_year_recid = true is \(.562\) (=1138/2024).

#### 0.v.3.2 More examples of Table 1 (mf)

Tables 16 completes Tables 2 and 17 (main file), displaying that even a stochastic dependence can be accurately modeled.

We provide in Table 18 the density learned on all our four 2D (for easy plotting) simulated domains and not just circgauss as in Table 1 (mf). We see that the observations made in Table 1 (mf) can be generalized to all domains. Even when the results are less different between 50 stumps in a generative forest and 1 generative tree with 50 splits for ringgauss and randgauss, the difference on gridgauss is stark, 1 generative tree merging many of the Gaussians.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline Ground truth & gf: 50 stumps & gf: 1 tree w/ 50 splits \\ \hline \hline \end{tabular}
\end{table}
Table A4: Comparison of the density learned by a Generative Forest (gf) consisting of a single tree learned by gf.Boost with \(n\) total splits (_right_) and a set of \(n\) gf stumps (1 split per tree) learned by gf.Boost (_center_). The left picture is the domain, represented using the same heatmap. On each domain, 5% of the data is missing (MCAR = missing completely at random).

#### v.v.3.3 The generative forest of Table 1 (mf) developed further

One might wonder how a set of stumps gets to accurately fit the domain as the number of stumps increases. Table A5 provides an answer. From this experiment, we can see that 16 stumps are enough to get the center mode. The ring shape takes obviously more iterations to represent but still, is takes a mere few dozen stumps to clearly get an accurate shape, the last iterations just finessing the fitting.

The objective of the experiment is to evaluate whether a generative model is able to create "realistic" data. Tabular data is hard to evaluate visually, unlike _e,g._ images or text, so we have considered a simple evaluation pipeline: we create for each domain a 5-fold stratified experiment. After a generative model has been trained, we generate the same number of observations as in the test fold and compute the optimal transport (OT) distance between the generated sample and the fold's test sample. To fasten the computation of OT costs, we use Sinkhorn's algorithm [8] with an \(\varepsilon\)-entropic regularizer, for some \(\varepsilon=0.5\) which we observed was the smallest in our experiments to run with all domains without leading to numerical instabilities. To balance the importance of categorical features (for which the cost is binary, depending on whether the guess is right or not) and numerical features, we normalize numerical features with the domain's mean and standard deviation prior to computing OT costs. We then compare our method to three possible contenders: Adversarial Random Forests (ARF) [44], CT-GANs [47], Forest Flows [17] and Vine copula autoencoders (VCAE) [41]. All these approaches rely on models that are very different from each other. Adversarial Random Forests (ARF) and Forest Flows (FFs) are tree-based generators. ARFs work as follows to generate one observation: one first sample uniformly at random a tree, then samples a leaf in the tree. The leaf is attached to a distribution which is then used to sample the observation. As we already pointed out in Section 2, there are two main differences with our models. From the standpoint of the model's

[MISSING_PAGE_FAIL:32]

Results vs Adversarial Random ForestsTable A7 digs into results obtained against adversarial random forests on the Sinkhorn metric. A first observation, not visible in the table, is that ARFs indeed tend to learn big models, typically with dozens of nodes in each tree. For the largest ARFs with 100 or 200 trees, this means in general a total of thousands of nodes in models. A consequence, also observed experimentally, is that there is sometimes little difference in performance in general between models with a different number of trees in ARFs as each tree is in fact an already good generator. In our case, this is obviously not the case. Noticeably, the performance of ARFs is not monotonic in the number of trees, so there could be a way to find the appropriate number of trees in ARFs to buy the slight (but sometimes significant) increase in performance in a domain-dependent way. In our case, there is obviously a dependency in the number of trees chosen as well. From the standpoint of the optimal transport metric, even when ARFs make use of distributions at their tree leaves that are much more powerful than ours and in fact fit to some of our simulated domains (Gaussians used in circgauss, randgauss and gridgauss), we still manage to compete or beat ARFs on those simulated domains.

Globally, we manage to beat ARFs on almost all runs, and very significantly on many of them. Since training generative forests does not include a mechanism to select the size of models, we have completed this experiment by another one on which we learn much smaller generative forests. The corresponding Table is in Appendix, Section V.V.3.4. Because we clearly beat ARFs on student_performance_mat and student_performance_por in Table A7 but are beaten by ARFs for much smaller generative forests, we conclude that there could exist mechanisms to compute the "right size" of our models, or to prune big models to get models with the right size. Globally however, even with such smaller models, we still manage to beat ARFs on a large majority of cases, which is a good figure given the difference in model sizes. The ratio "results quality over model size" tips in our favor and demonstrates the potential in using all trees in a forest to generate an observation (us) vs using a single of its trees (ARFs), see Figure 1.

Results vs CT-GANsTable A8 summarizes our results against CT-GAN using various numbers of training epochs (\(E\)). In our case, our setting is the same as versus ARFs: we stick to \(T=500\) trees trained for a total number of \(J=2000\) iterations in the generative forest, for all domains. We see that generative forests consistently and significantly outperform CT-GAN on nearly all cases. Furthermore, while CT-GANs performances tend to improve with the number of epochs, we observe that on a majority of datasets, performances at 1 000 epochs are still far from those of generative forests and already induce training times that are far bigger than for generative forests: for example, more than 6 hours per fold for stm while it takes just a few seconds to train a generative forest. Just like we did for ARFs, we repeated the experiment vs CT-GAN using much smaller generative forests (\(T=200,J=500\)). The results are presented in Appendix, Section V.V.3.4. There is no change in the conclusion: even with such small models, we still beat CT-GANs, regardless of the number of training epochs, on all cases (and very significantly on almost all of them).

More results with small generative forestsTables A9 and A10 present the results of generative forests vs adversarial random forests and CT-GANs, when the size of our models is substantially smaller than in the main file (\(T=200,J=500\)). We still manage to compete or beat ARFs on simulated domains for which modelling the leaf distributions in ARFs should represent an advantage (Gaussians used in circgauss, randgauss and gridgauss), even more using comparatively very small models compared to ARFs. We believe this could be due to two factors: (i) the fact that in our models all trees are used to generate each observation (which surely facilitates learning small and accurate models) and (ii) the fact that we do not generate data during training but base our splitting criterion on an _exact_ computation of the reduction of Bayes risk in growing trees. If we compute aggregated statistics comparing, for each number of trees in ARFs, the number of times we win / lose versus ARFs, either considering only significant \(p\)-values or disregarding them, our generative forests always beat ARFs on a majority of domains. Globally, this means we win on 34 out of 52 cases. The results vs CT-GANs are of the same look and feel as in the main file with larger generative forests: generative forests beat them in all cases, and very significantly in most of them.

#### V.V.3.5 Comparison with "the optimal generator": gen-discrim

Rather than compare our method with another one, the question we ask here is "can we generate data that looks like domain's data"? We replicate the experimental pipeline of [31]. In short, we shuffle a 3-partition (say \(\mathrm{R}_{1},\mathrm{R}_{2},\mathrm{R}_{3}\)) of the training data in a \(3!=6\)-fold CV, then train a generator from \(\mathrm{R}_{1}\) (we use generative forests with gf.Boost), then train a random forest (rf) to distinguish between \(\mathrm{G}_{1}\) and \(\mathrm{R}_{2}\), and finally estimate its test accuracy on \(\mathrm{G}_{1}\) vs \(\mathrm{R}_{3}\). The _lower_ this accuracy, the less distinguishable are fakes from real and thus the _better_ the generator. We consider 3 competing baselines to our models: (i) the "optimal" one, copy, which consists in replacing \(\mathrm{G}_{1}\) by a set of real data, (ii) the "worst" one, unif(orm), which uniformly samples data, and (iii) gf.Boost for \(T=1\), which learns a generative tree (gt). We note that this pipeline is radically different from the one used in [44]. In this pipeline, domains used are supervised (the data includes a variable to predict). A generator is trained from some training data, then generates an amount of data equal to the size of the training data. Then, two classifiers are trained, one from the original training data, one from the generated data, to predict for the variable to predict and their performances are compared on this basis (the higher the accuracy, the better). There is a risk in this pipeline that we have tried to mitigate with our sophisticated pipeline: if the generator consists just in copying its training data, it is guaranteed to perform well according to [44]'s metric. Obviously, this would not be a good generator however. Our pipeline would typically prevent this, \(\mathrm{R}_{2}\) being different from \(\mathrm{R}_{3}\).

**Results** Tables A11 and A12 provide the results we got, including statistical \(p\)-values for the test of comparing our method to copy. They display that it is possible to get gfs generating realistically looking data: for iris, the Student \(t\)-tests show that we can keep \(H_{0}\) = "\(\mathrm{\texttt{\small{\small{\small{\small{\small{\small{\small{\small{\small{ \small{\small{\small{\small{\small{\small{\small{\small{\small{\small{\small{\small{\small{ \small{ \small{{        }}}}}}}}}}}}}}}}}}\) perform identically to copy" for \(J=T=40\) (\(p>.2\)). For mat, the result is quite remarkable not because of the highest \(p\)-val, which is not negligible (\(>0.001\)), but because to get it, it suffices to build a gf in which the total number of feature occurrences (\(J=80\)) is barely three times as big as the domain's dimension (\(d=33\)). The full tables display a clear incentive to grow further the gfs as domains increase in size. However, our results show, pretty much like the experiment against Adversarial Random Forests, that there could be substantial value of learning the right model size: iris and student_performance_por clearly show nontrivial peaks of \(p\)-values, for which we would get the best models compared to copy.

#### v.v.3.6 Full comparisons with mice on missing data imputation

The main file provides just two examples of domains for the comparison, abalone and analcatdata_supreme. We provide here more complete results on the domain used in the main file and results on more domains in the form of one table for each additional domain:

* Table A13: experiments on analcatdata_supreme completing the results shown in Table 4 (mf);
* Table A14: experiments on abalone completing the results shown in Table 4 (mf); (tables below are ordered in increasing domain size, see Table A1)
* Table A15: experiments on iris;
* Table A16: experiments on ringgauss;
* Table A17: experiments on circgauss;
* Table A18: experiments on gridgauss;
* Table A19: experiments on randgauss;
* Table A20: experiments on student_performance_mat;
* Table A21: experiments on student_performance_por;
* Table A22: experiments on kc1;
* Table A23: experiments on sigma_cabs;
* Table A24: experiments on compas;
* Table A25: experiments on open_policing_hartford;

The large amount of pictures to process may make it difficult to understand at first glance how our approaches behave against mice, so we summarize here the key points:

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_EMPTY:37]

\begin{table}
\begin{tabular}{c|c|c} \hline \hline \(J\) & Ensembles of Generative Trees (EOGt) & Generative Forests (GF) \\  & perr & rmse & perr \\ \hline \hline \end{tabular}
\end{table}
Table 14: Missing data imputation: results on abalone with \(5\%\) missing features (MCAR), for gf.Boost learning gfs and eogts, vs mice using RF (random forests). Conventions follow Table A13.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Ens. of Generative Trees (eogt) & Generative Forests (GF) \\ \(J\) & rmse & rmse \\ \hline \hline \end{tabular}
\end{table}
Table A16: Missing data imputation: results on ringgauss with \(5\%\) missing features (MCAR), for gf.Boost learning gfs and eogts, vs mice using RF (random forests). Since there are no nominal attributes in the domain, we have removed column perr. Conventions otherwise follow Table A13.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{J} & \multicolumn{2}{c}{Ensembles of Generative Trees (eogt)} & \multicolumn{2}{c}{Generative Forests (gf)} \\  & perr & rmse & perr & rmse \\ \hline \hline \end{tabular}
\end{table}
Table A21: Missing data imputation: results on student_performance_por with \(5\%\) missing features (MCAR), for gf.Boost learning gfs and eogt, vs mice using RF (random forests). Conventions follow Table A13.

#### v.v.3.7 Full comparisons with KDE on density estimation

For each domain we either use the expected density, or the expected negative log-density, as the metric to be maximized. We consider the expected density to cope with the eventuality of zero (pointwise) density predictions. Tables A28 and A29 present the results against KDE (details in Table A28). Table A26 singles out a result on the domain gridGauss, which displays an interesting pattern. The leaf chosen for a split in GF.Boost is always the heaviest leaf; hence, as long as the iteration number \(j\leqslant T\) (the maximum number of trees, which is 500 in this experiment), GF.Boost grows stumps. In the case of gridGauss, like in a few other domains, there is a clear plateau in performances for \(j\leqslant 500\), which does not hold anymore as \(j>500\). This, we believe, characterizes the fact that on these domains, which all share the property that their dimension is small, training stumps attains a limit in terms of performances. Table A27 shows that by changing basic parameters - here, decreasing the number of trees, increasing the total number of iterations -, one can obtain substantially better results.

[MISSING_PAGE_EMPTY:51]

[MISSING_PAGE_EMPTY:52]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Claims are supported by both the theoretical results presented in the paper and extensive experimental results, summarized in the main paper but otherwise presented in extenso in the Supplement. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are discussed, in particular in the experimental section vs the SOTA (see e.g. experiment Lifelike). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Assumptions are detailed, formal results given and full proofs are provided in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: **(i)** full code provided, **(ii)** extensive README.txt, **(iii)** coding choices discussed in Appendix, **(iv)** all scripts provided for easy running of each experiments, **(v)** special visualization routines (e.g. heatmaps) also included in the code, **(vi)** example domains (public and simulated) provided for quick assessment. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: **(i)** full code provided, **(ii)** extensive README.txt, **(iii)** coding choices discussed in Appendix, **(iv)** all scripts provided for easy running of each experiments, **(v)** special visualization routines (e.g. heatmaps) also included in the code, **(vi)** example domains (public and simulated) provided for quick assessment. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are provided in main text and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: standard devs and inferential statistics test done (details in main file + appendix). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA]. Justification: Our code runs on any standard computer. Details of codes, computer and SOTA used (version, etc.) in appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research of the paper follows the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See conclusion. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No release of data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Appropriate credit / referencesse are provided. Licenses listed in Appendix. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets provided. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.