Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm

 Leo Zhou

Walter Burke Institute for Theoretical Physics and the IQIM

California Institute of Technology, Pasadena, CA 91125

Electrical and Computer Engineering Department

University of California, Los Angeles, CA 90095

leoxzhou@ucla.edu

Joao Basso

Department of Mathematics

University of California, Berkeley, CA 94720

Quantum Artificial Intelligence Lab (QuAIL)

NASA Ames Research Center, Moffett Field, CA 94035

Research Institute for Advanced Computer Science (RIACS)

USRA, Mountain View, CA 94043

joao.basso@berkeley.edu

Song Mei

Department of Statistics and Department of EECS

University of California, Berkeley, CA 94720

songmei@berkeley.edu

###### Abstract

The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization that has been a promising avenue for near-term quantum advantage. In this paper, we analyze the performance of the QAOA on the spiked tensor model, a statistical estimation problem that exhibits a large computational-statistical gap classically. We prove that the weak recovery threshold of \(1\)-step QAOA matches that of \(1\)-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of \(p\)-step QAOA matches that of \(p\)-step tensor power iteration when \(p\) is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the asymptotic classical computation threshold \(\Theta(n^{(q-2)/4})\) for spiked \(q\)-tensors. Meanwhile, we characterize the asymptotic overlap distribution for \(p\)-step QAOA, discovering an intriguing sine-Gaussian law verified through simulations. For some \(p\) and \(q\), the QAOA has an effective recovery threshold that is a constant factor better than tensor power iteration. Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure.

## 1 Introduction

We study statistical estimation in the spiked tensor model, where we observe a \(q\)-tensor \(\bm{Y}\in\mathbb{R}^{n^{q}}\) in \(n^{q}\) dimensions given by

\[\bm{Y}=(\lambda_{n}/n^{q/2})\cdot\bm{u}^{\otimes q}+(1/\sqrt{n})\cdot\bm{W} \in\mathbb{R}^{n^{q}}.\] (1.1)Here \(\bm{u}\sim\mathrm{Unif}(\{+1,-1\}^{n})\) is some hidden signal,1 and \(\bm{W}\in(\mathbb{R}^{n})^{\otimes q}\) is a noise tensor whose entries are i.i.d. standard Gaussian \(\mathcal{N}(0,1)\). The parameter \(\lambda_{n}>0\) is the _signal-to-noise ratio_ (SNR). The goal is to estimate \(\bm{u}\) given only access to \(\bm{Y}\). That is, we seek an estimator \(\hat{\bm{u}}\colon(\mathbb{R}^{n})^{\otimes q}\to\mathbb{S}^{n-1}(\sqrt{n})\) achieving nontrivial overlap with the signal:

Footnote 1: Another commonly studied prior is the uniform distribution over the \(n\)-sphere, \(\mathrm{Unif}(\mathbb{S}^{n-1}(\sqrt{n}))\). In this work, we choose the Rademacher prior for the convenience of the QAOA analysis.

\[\liminf_{n\to\infty}\mathbb{E}[\langle\hat{\bm{u}}(\bm{Y}),\bm{u} \rangle^{2}/n^{2}]>0.\] (1.2)

This task is known as _weak recovery_ in the spiked tensor model.

The spiked tensor model is a famous problem because it exhibits a huge computational-statistical gap, referring to regimes of SNR where the statistical estimation problem is information-theoretically solvable, but no efficient algorithm has been found. For example, it is known that the Bayes optimal estimator achieves non-trivial overlap with the signal \(\bm{u}\) when \(\lambda_{n}>\lambda_{\mathrm{IT}}\) for some constant threshold \(\lambda_{\mathrm{IT}}=\Theta(1)\), whereas the problem is information-theoretically impossible when \(\lambda_{n}\leq\lambda_{\mathrm{IT}}\)[1]. Furthermore, the maximum likelihood estimator also achieves non-trivial overlap with the signal when \(\lambda_{n}>\lambda_{\mathrm{MILE}}\) for some \(\lambda_{\mathrm{MILE}}=\Theta(1)\). However, the best-known polynomial-time classical algorithms for computing a non-trivial estimator require a much higher SNR of \(\lambda_{n}=\Theta(n^{(q-2)/4})\). These include tensor power iteration, gradient descent, approximate message passing, and spectral methods with tensor unfolding [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]. Indeed, assuming the secret leakage planted clique conjecture, Ref. [13] proves an \(\Omega(n^{(q-2)/4})\) lower bound on the SNR needed by any polynomial-time classical algorithm. See Fig. 1 for an illustration of the different SNR thresholds and Section 2.1 for more background.

On the other hand, quantum algorithms are widely believed to have computational advantages over classical algorithms for many problem classes. In particular, we focus on the Quantum Approximate Optimization Algorithm (QAOA) [14], a general-purpose quantum optimization algorithm that can be applied to optimize any objective function on bit-strings. The QAOA has received an enormous amount of attention in the quantum computing community for several reasons. First, the QAOA is simple and allows efficient implementation on near-term quantum hardware with many applications [15, 16, 17, 18, 19]. Additionally, the QAOA is computationally universal [20], and its generalization can realize other powerful algorithms such as the quantum singular value transform [21]. Under common complexity-theoretic assumptions, no classical device can efficiently simulate the output distribution of the QAOA even at shallow depth [22, 23]. Furthermore, the QAOA is guaranteed to find optimal solutions when its number of steps (or depth) diverges [14]. Nevertheless, analyzing the asymptotic performance of QAOA remains challenging: classical simulation of the algorithm is limited to small problem dimension \(n\), and analytical computations are often highly non-trivial [24, 25, 26, 27, 28]. Given the enormous \(\Omega(n^{(q-2)/4})\) computational-statistical gap in the spiked tensor model (compared to e.g., the constant factor gap in spin-glass optimization [29]), it is an interesting open question whether the QAOA, as a realistic quantum algorithm with asymptotic convergence guarantees, can provide any computational advantages.

In this work, we investigate the performance of QAOA for the spiked tensor model. In particular, we choose the log-likelihood objective of spiked tensor \(C(\bm{z})=\langle\bm{Y},\bm{z}^{\otimes q}\rangle/n^{(q-2)/2}\). Its maximizer, the maximum likelihood estimator, achieves non-trivial overlap with the signal whenever \(\lambda_{n}>\lambda_{\mathrm{MILE}}=\Theta(1)\). While the infinite-step QAOA could compute the maximizer, we are interested in the performance of QAOA when the depth is polynomial in the problem size, and hope that it can surpass the \(\Theta(n^{(q-2)/4})\) classical threshold. Although some limitations of the QAOA are known for certain random optimization problems in the low-depth regime [26, 30, 31, 32], these negative results do not apply to the spiked tensor model since they rely on either sparse connectivity or concentration, both of which are absent in the current setting. Here, as a first attempt to bridge the gap in understanding how well a popular quantum algorithm may perform on a classically hard statistical estimation problem, we study the asymptotic behavior of the QAOA on the spiked tensor model in the constant-depth regime, where we are able to obtain rigorous and analytical results.

Our contribution.In this paper, we analyze the signal-to-noise ratio threshold of \(p\)-step QAOA for weak recovery in the spiked tensor model, in the regime of fixed \(p\) and \(n\) approaching infinity. For \(p=1\), we prove the weak recovery threshold is \(\lambda_{n}=\Theta(n^{(q-1)/2})\), matching that of 1-step tensor power iteration. For \(p>1\), heuristic calculations suggest the threshold is \(\lambda_{n}=\Theta(n^{(q-2+\varepsilon_{p})/2})\)where \(\varepsilon_{p}=(q-2)/[(q-1)^{p}-1]\), again matching \(p\)-step tensor power iteration. Additionally, given an initialization vector with \(n^{c}/n\) correlation to the signal for \(1/2<c<1\), we prove the weak recovery threshold for 1-step QAOA is \(\lambda_{n}=\Theta(n^{(1-c)(q-1)})\), identical to 1-step tensor power iteration. These results indicate that constant-step QAOA has the same asymptotic recovery threshold as tensor power iteration in the spiked tensor model. Meanwhile, further heuristic analysis suggests that QAOA with tensor unfolding could achieve the classical computation threshold \(\Theta(n^{(q-2)/4})\).

Furthermore, we derive the asymptotic distribution of the overlap for \(p\)-step QAOA, revealing an intriguing sine-Gaussian law distinct from \(p\)-step tensor power iteration. Analyzing the second moment, we see that, for certain \((p,q)\) pairs, the QAOA effectively has a recovery threshold that is a constant factor better than tensor power iteration. Since there are classical algorithms that achieve better recovery thresholds than power iteration, it remains an interesting open question whether quantum advantage over the state-of-the-art classical algorithms may be obtained at larger QAOA depths that grow with system size. To our current knowledge, our work is the first to obtain analytical results using the QAOA for a statistical inference problem.

The proof of the sine-Gaussian distribution adopted novel techniques, including using discrete Fourier transforms and the central limit theorem to handle combinatorial summations. The Fourier transform technique also allows us to replace nonlinear polynomials in the exponents with dual variables, leaving linear exponents that become easy in combinatorial sums. These techniques are of independent interest and could be useful for analyzing the QAOA in other models.

## 2 Background and related work

### Spiked tensor model and prior algorithms

The spiked tensor model (1.1) was first introduced as a statistical model for tensor principal component analysis in [2], where it was studied with a spherical prior \(\bm{u}\in\mathbb{S}^{n-1}(\sqrt{n})\). The information-theoretic threshold for weak recovery under this model with the spherical prior [3; 7; 8] and the Rademacher prior \(\bm{u}\in\{\pm 1\}^{n}\)[1] are both \(\lambda_{n}=\Theta(1)\).

Tensor power iteration.A well-studied classical algorithm for the spiked tensor model is tensor power iteration [2; 10; 33]. Starting from a uniform random initialization \(\hat{\bm{u}}_{0}\sim\mathrm{Unif}(\mathbb{S}^{n-1})\), the \(k\)-th iteration is given by \(\hat{\bm{u}}_{k}\), where

\[\hat{\bm{u}}_{k}=\sqrt{n}\bm{Y}[\hat{\bm{u}}_{k-1}^{\otimes(q-1)}]/\big{\|} \bm{Y}[\hat{\bm{u}}_{k-1}^{\otimes(q-1)}]\big{\|}_{2},\ \ \ \ k\geq 1,\ \ \ \hat{\bm{u}}_{0}\sim\mathrm{Unif}(\mathbb{S}^{n-1}).\] (2.1)

Here, \(\bm{Y}[\hat{\bm{u}}^{\otimes(q-1)}]\in\mathbb{R}^{n}\) denotes contracting the order-\(q\) tensor \(\bm{Y}\in\mathbb{R}^{n^{q}}\) with the order-\((q-1)\) tensor \(\hat{\bm{u}}^{\otimes(q-1)}\in\mathbb{R}^{n^{q-1}}\). It is shown that with \((\log n)\) iterations, weak recovery is possible if the SNR satisfies \(\lambda_{n}=\Omega(n^{(q-2)/2}/\operatorname{polylog}(n))\)[10; 33]. However, tensor power iteration does not match the best-known classical algorithms. Furthermore, we remark that rounding the tensor power iteration to \(\mathrm{sign}(\hat{\bm{u}}_{k})\in\{\pm 1\}^{n}\) does not give a better threshold.

Other classical algorithms and related results.[2] showed that the tensor power iteration and approximate message passing algorithms with random initialization can recover the signal provided \(\lambda_{n}=\Omega(n^{(q-1)/2})\). This SNR threshold was later improved to \(\lambda_{n}=\Omega(n^{(q-2)/2})\) by [3; 10; 33] for these same methods. The same threshold \(\lambda_{n}=\Omega(n^{(q-2)/2})\) could also be achieved by gradient

Figure 1: Different thresholds for the spiked tensor model.

descent and Langevin dynamics as proved in [9]. On maximum likelihood estimation for the spiked tensor model with a spherical prior, [5; 6] studied the loss landscape, providing intuition that it contains many saddle points and local minima near the equator, but no bad critical points off the equator.

The best currently known polynomial-time algorithms can achieve a sharp threshold of \(\lambda_{n}=\Omega(n^{(q-2)/4})\). These include spectral methods with tensor unfolding [2; 11], sum-of-squares algorithms [34; 35; 36], sophisticated iteration algorithms [37; 38; 39], and gradient descent on the smoothed landscape [40; 41].

Another line of research has attempted to prove computational lower bounds in restricted computational models, including low-degree polynomials and statistical query algorithms [42; 43]. Under the secreted leakage planted clique conjecture, [13] proved that any classical polynomial-time algorithm requires \(\lambda_{n}=\Omega(n^{(q-2)/4})\) for weak recovery of the signal.

A quantum algorithm by Ref. [44].To the best of our knowledge, the only prior quantum algorithm proposed for the spiked tensor model with provable guarantees is by Hastings in Ref. [44]. Hastings' algorithm is based on a spectral method for a Hamiltonian on \(M\) bosons over \(n\) modes, living in a Hilbert space of dimension \(n^{M}\), where \(M\gg[n^{(q-2)/4}/\lambda_{n}]^{4/(q-2)}\times\mathrm{polylog}(n)\). Finding the dominant eigenvector of this Hamiltonian allows for weak recovery in the regime where \(\lambda_{n}=\Theta(n^{(q-2)/4})\). In this regime, where \(M=\Omega(\mathrm{polylog}\,n)\), the standard classical matrix power iteration algorithm can extract the dominant eigenvector and recover the signal in \(\tilde{O}(n^{M})\) time. For the proposed quantum algorithm, Ref. [44] uses a combination of quantum phase estimation, amplitude amplification, and clever state initialization to recover the signal in \(\tilde{O}(n^{M/4})\) time, achieving a quartic speedup. (A few months after our paper appeared online, a related work [45] emerged, simplifying Hastings' algorithm and generalizing it to another planted inference problem.)

We remark that Hastings' algorithm runs in superpolynomial time \(n^{\Omega(\mathrm{polylog}\,n)}\) and does not improve over the asymptotic computational threshold in SNR for recovery (although a constant factor improvement is possible). For comparison, the classical spectral method based on tensor unfolding [2; 11] achieves recovery when \(\lambda_{n}>n^{(q-2)/4}\) in polynomial time \(O(\mathrm{poly}(n^{q}))\). In this work, we study the QAOA in the constant-step regime, where the gate complexity grows only linearly in the problem size \(O(n^{q})\).

### Quantum approximate optimization algorithm

The quantum approximate optimization algorithm (QAOA) was introduced by [14] as a quantum algorithm for finding approximate solutions to combinatorial optimization problems. The QAOA can be applied to optimize any cost function on bit-strings, \(C:\{\pm 1\}^{n}\to\mathbb{R}\). In the spiked tensor model, we consider optimizing the log-likelihood function given by

\[\hat{\bm{u}}_{\mathrm{MLE}}=\arg\max_{\bm{\sigma}\in\{\pm 1\}^{n}}\Big{\{}C(\bm{ \sigma})=\langle\bm{Y},\bm{\sigma}^{\otimes q}\rangle/n^{(q-2)/2}\Big{\}}.\] (2.2)

The maximum likelihood estimator \(\hat{\bm{u}}_{\mathrm{MLE}}\) achieves non-trivial correlation with the signal when \(\lambda_{n}>\lambda_{\mathrm{MLE}}\) for some constant \(\lambda_{\mathrm{MLE}}=\Theta(1)\). However, classical algorithms cannot efficiently compute the MLE unless \(\lambda_{n}=\Omega(n^{(q-2)/4})\)[13]. This paper investigates whether QAOA could compute \(\hat{\bm{u}}_{\mathrm{MLE}}\), or an approximate estimator, for smaller values of \(\lambda_{n}\).

The inputs to the QAOA algorithm are a cost function \(C:\{\pm 1\}^{n}\to\mathbb{R}\) and parameter vectors \(\bm{\gamma},\bm{\beta}\in\mathbb{R}^{p}\). The initial QAOA state \(|s\rangle=2^{-n/2}\sum_{\bm{z}}|\bm{z}\rangle\) is the rescaled all-one vector \(2^{-n/2}\bm{1}_{2^{n}}\in\mathbb{C}^{2^{n}}\), assigning equal probability to measuring each possible bit-string upon quantum measurement. See Appendix A.1 for a review of quantum computing terminology, where we also define the Pauli operators \(\{X_{k},Y_{k},Z_{k}\}_{k=1}^{n}\) acting on the \(k\)-th qubit. The cost function \(C\) associates with a \(2^{n}\times 2^{n}\) diagonal matrix, where the \(|\bm{z}|\)'th diagonal gives \(C(\bm{z})\). For the spiked tensor model with cost function \(C(\bm{z})=\langle\bm{Y}\,,\bm{z}^{\otimes q}\rangle/n^{(q-2)/2}\), this matrix is \(C=\sum_{j_{1},\ldots,j_{q}=1}^{n}Y_{j_{1}\cdots j_{q}}Z_{1}\cdots Z_{q}/n^{(q -2)/2}\in\mathbb{C}^{2^{n}\times 2^{n}}\). Letting \(B=\sum_{j=1}^{n}X_{j}\in\mathbb{C}^{2^{n}\times 2^{n}}\), for any parameter \((\gamma,\beta)\), the unitary matrices \(e^{-i\gamma C},e^{-i\gamma B}\in\mathbb{C}^{2^{n}\times 2^{n}}\) are matrix exponents of \(-i\gamma C\) and \(-i\gamma B\). Given \(\bm{\gamma},\bm{\beta}\in\mathbb{R}^{p}\), the \(p\)-step QAOA state is

\[|\bm{\gamma},\bm{\beta}\rangle=e^{-i\beta_{p}B}e^{-i\gamma_{p}C}\cdots e^{-i \beta_{1}B}e^{-i\gamma_{1}C}|s\rangle\in\mathbb{C}^{2^{n}}.\] (2.3)One can verify \(|\bm{\gamma},\bm{\beta}\rangle\) is a unit vector since \(|s\rangle\in\mathbb{C}^{2^{n}}\) is unit and \(e^{-i\gamma_{k}B}\in\mathbb{C}^{2^{n}\times 2^{n}}\) and \(e^{-i\gamma_{k}C}\in\mathbb{C}^{2^{n}\times 2^{n}}\) are unitary matrices. After preparing the quantum state \(|\bm{\gamma},\bm{\beta}\rangle\), QAOA samples a bit string \(\bm{z}\sim|\bm{\gamma},\bm{\beta}\rangle\) in \(\{\pm 1\}^{n}\) by quantum measurement. In our main results, we will analyze the distribution of the overlap \(\mathcal{R}_{\mathrm{QAOA}}\) of this quantum measurement \(\bm{z}\) with respect to the signal \(\bm{u}\):

\[\mathcal{R}_{\mathrm{QAOA}}\equiv\bm{z}^{\top}\bm{u}/n=\frac{1}{n}\sum_{i=1}^ {n}z_{i}u_{i}\in[-1,1].\] (2.4)

For any function \(f(\bm{z})=\sum_{k=0}^{n}\sum_{\{j_{1},\cdots,j_{k}\}}\hat{f}_{j_{1}\cdots j_{k }}z_{j_{1}}\cdots z_{j_{k}}\), its expectation under the QAOA state \(|\bm{\gamma},\bm{\beta}\rangle\) is given by \(\langle\bm{\gamma},\bm{\beta}|f(\bm{Z})|\bm{\gamma},\bm{\beta}\rangle\), where \(\langle\bm{\gamma},\bm{\beta}|\in\mathbb{C}^{1\times 2^{n}}\) is the conjugate transpose of \(|\bm{\gamma},\bm{\beta}\rangle\in\mathbb{C}^{2^{n}\times 1}\), and \(f(\bm{Z})=\sum_{k=0}^{n}\sum_{\{j_{1},\cdots,j_{k}\}}\hat{f}_{j_{1}\cdots j_{k }}Z_{j_{1}}\cdots Z_{j_{k}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) for Pauli-Z matrices \(Z_{j}\). To simplify the notations, we denote \(\langle\cdot\rangle_{\bm{\gamma},\bm{\beta}}\) by the expectation with the quantum measurement from \(|\bm{\gamma},\bm{\beta}\rangle\), so that

\[\langle f(\bm{Z})\rangle_{\bm{\gamma},\bm{\beta}}=\langle\bm{\gamma},\bm{ \beta}|f(\bm{Z})|\bm{\gamma},\bm{\beta}\rangle.\] (2.5)

In the main theorems of this paper, we will focus on the second moment of the overlap of QAOA, denoted as \(\langle\mathcal{R}_{\mathrm{QAOA}}^{2}\rangle_{\bm{\gamma},\bm{\beta}}= \langle\bm{\gamma},\bm{\beta}|\widehat{\mathcal{R}}^{2}|\bm{\gamma},\bm{ \beta}\rangle\), where \(\widehat{\mathcal{R}}\equiv\frac{1}{n}\sum_{i=1}^{n}u_{i}Z_{i}\). We defer further related literature on theoretical analyses of the QAOA to Appendix A.2.

In terms of experimental realizations, the QAOA has been implemented in quantum computing platforms such as trapped ions [16; 19], superconducting qubits [17], and neutral atoms [18], for optimization problems with up to 179 bit variables. Implementing the QAOA for the spiked tensor model, however, poses additional challenges due to the all-to-all connectivity in its cost function, leading to a higher overhead in the number of quantum gates and circuit compilation costs. Currently, the largest experimental implementations for problems with dense connectivity include 17-bit Sherrington-Kirkpatrick spin-glass models on superconducting qubits [17], and an 18-bit LABS problem on trapped-ion quantum processors [19]. We expect larger problems can be implemented as quantum hardware matures, but quantum error-correction is likely necessary to observe any quantum advantage at scale [46].

## 3 Main results

### Weak recovery threshold and overlap distribution for \(1\)-step QAOA

We first consider the general \(1\)-step QAOA for weak recovery in the spiked tensor model. Consider the spiked tensor model \(\bm{Y}\) (1.1) with planted signal \(\bm{u}\sim\mathrm{Unif}(\{\pm 1\}^{n})\) and the \(1\)-step QAOA quantum state \(|\gamma_{n},\beta_{n}\rangle=e^{-i\beta_{n}B}e^{-i\gamma_{n}C}|s\rangle\) (see Section 2.2) with parameters \((\gamma_{n},\beta_{n})\in\mathbb{R}_{>0}\times[0,2\pi]\). The quantum state \(|\gamma_{n},\beta_{n}\rangle\) depends randomly on \(\bm{Y}\) through \(C(\bm{\sigma})=\langle\bm{Y},\bm{\sigma}^{\otimes q}\rangle/n^{(q-2)/2}\). Our main results characterize the distribution of the overlap \(\mathcal{R}_{\mathrm{QAOA}}=\hat{\bm{u}}^{\top}\bm{u}/n\) between a sample \(\hat{\bm{u}}\sim|\gamma_{n},\beta_{n}\rangle\) and the signal vector \(\bm{u}\).

**Theorem 1** (Weak recovery threshold and overlap distribution for \(1\)-step QAOA).: _Consider the spiked tensor model (1.1) and the \(1\)-step QAOA overlap as defined above. Then the following hold._

1. _Take any sequence of_ \(\{\gamma_{n}\}_{n\geq 1}\subseteq\mathbb{R}\)_,_ \(\{\beta_{n}\}_{n\geq 1}\subseteq[0,2\pi]\)_, and any sequence of_ \(\{\lambda_{n}\}_{n\geq 1}\subseteq[0,\infty)\) _with_ \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=0\)_. We have_ \[\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}_{\mathrm{ QAOA}}^{2}\rangle_{\gamma_{n},\beta_{n}}]=0.\] (3.1)
2. _Take any sequence of_ \(\{\gamma_{n}\}_{n\geq 1}\)_,_ \(\{\beta_{n}\}_{n\geq 1}\)_, and_ \(\{\lambda_{n}\}_{n\geq 1}\) _which satisfies_ \[\lim_{n\to\infty}(\gamma_{n},\beta_{n},\lambda_{n}/n^{(q-1)/2})=(\gamma,\beta, \Lambda).\] (3.2) _Then, over the randomness of_ \(\bm{Y}\) _and the quantum measurement, the overlap_ \(\mathcal{R}_{\mathrm{QAOA}}\) _of the_ \(1\)_-step QAOA converges in distribution to a sine-Gaussian law as_ \[\mathcal{R}_{\mathrm{QAOA}}\xrightarrow{d}e^{-2q\gamma^{2}}\sin(2\beta)\sin(2q \Lambda\gamma G^{q-1}),\quad\text{where }G\sim\mathcal{N}(0,1).\] (3.3)
3. _As a corollary of (b), under the asymptotic limit of (_3.2_) with_ \(\Lambda>0\)_,_ \(\gamma>0\)_, and_ \(\beta\not\in\{k\pi/2:k\in\mathbb{Z}\}\)_, we have_ \[\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}_{\mathrm{ QAOA}}^{2}\rangle_{\gamma_{n},\beta_{n}}]>0.\] (3.4)The full proof of Theorem 1 is contained in Appendix C.

**Remark 3.1** (Weak recovery threshold).: Theorem 1(c) implies that when \(\lambda_{n}=\Theta(n^{(q-1)/2})\), the overlap will be non-zero with non-trivial probability over both the random draw of the tensor and the quantum randomness. In contrast, Theorem 1(a) shows that when \(\lambda_{n}=o(n^{(q-1)/2})\) the overlap will be zero with high probability. This establishes that \(\lambda_{n}=\Theta(n^{(q-1)/2})\) is the weak recovery threshold of 1-step QAOA in the spiked tensor model.

**Remark 3.2** (Overlap distribution).: Theorem 1 does not show that the overlap distribution for a typical instance \(\bm{Y}\) converges to the same sine-Gaussian law. In Section 4, we perform numerical simulations that provide evidence that the overlap distribution will concentrate over the random draw of \(\bm{Y}\), which would imply that the overlap distribution is indeed sine-Gaussian for any typical \(\bm{Y}\).

Comparison with classical tensor power iteration.The \(1\)-step tensor power iteration estimator (Eq. (2.1)) is redefined here for the reader's convenience: \(\hat{\bm{u}}_{1}=\sqrt{n}\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]/\|\bm{Y}[\hat {\bm{u}}_{0}^{\otimes(q-1)}]\|_{2}\), where \(\hat{\bm{u}}_{0}\sim\mathrm{Unif}(\mathbb{S}^{n-1})\) is a random initialization vector. In the following proposition, we show that the weak recovery threshold for the \(1\)-step power iteration estimator is also \(\lambda_{n}=\Theta(n^{(q-1)/2})\), and we provide the distribution of the overlap \(\mathcal{R}_{\mathrm{PI}}\equiv\hat{\bm{u}}_{1}^{\top}\bm{u}/n\) between the power iteration estimator \(\hat{\bm{u}}_{1}\) and the signal \(\bm{u}\).

**Proposition 3.3** (Weak recovery threshold for \(1\)-step tensor power iteration).: _Assume that the rescaled signal-to-noise ratio has a limit \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=\Lambda\). Then over the randomness of \(\bm{W}\) and initialization \(\hat{\bm{u}}_{0}\), the overlap \(\mathcal{R}_{\mathrm{PI}}\) of the power iteration estimator with the signal converges in distribution to_

\[\mathcal{R}_{\mathrm{PI}}\stackrel{{ d}}{{\longrightarrow}}\sin[ \arctan(\Lambda G^{q-1})],\quad\text{ where }G\sim\mathcal{N}(0,1).\] (3.5)

_As a corollary, when \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=0\), we have \(\mathcal{R}_{\mathrm{PI}}\stackrel{{ p}}{{\longrightarrow}}0\)._

The proof of Proposition 3.3 is contained in Appendix H.1.

**Remark 3.4** (Comparing the overlaps).: Theorem 1 and Proposition 3.3 show that both 1-step QAOA and 1-step power iteration have the same weak recovery threshold \(\lambda_{n}=\Theta(n^{(q-1)/2})\). To compare the two algorithms more precisely, we take the limit \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=\Lambda\) for some small \(\Lambda>0\). Eq. (3.3) and Eq. (3.5) give the limiting squared overlap distributions for 1-step QAOA and 1-step power iteration, respectively:

\[\begin{split}\lim_{\Lambda\to 0}\Lambda^{-2}&\lim_{n\to \infty}\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\mathrm{QAOA}}\rangle_{ \gamma,\beta}]=e^{-4q\gamma^{2}}4q^{2}\gamma^{2}\sin^{2}(2\beta)\,\mathbb{E}_{G \sim\mathcal{N}(0,1)}[G^{2q-2}],\\ &\lim_{\Lambda\to 0}\Lambda^{-2}\,\lim_{n\to\infty}\mathbb{E}_{\bm{Y }}[\mathcal{R}^{2}_{\mathrm{PI}}]=\,\mathbb{E}_{G\sim\mathcal{N}(0,1)}[G^{2q -2}].\end{split}\] (3.6)

This gives

\[\max_{\gamma,\beta}\Big{\{}\lim_{\Lambda\to 0+}\lim_{n\to\infty}\mathbb{E}_{\bm{Y }}[\langle\mathcal{R}^{2}_{\mathrm{QAOA}}\rangle_{\gamma,\beta}]/\,\mathbb{E }_{\bm{Y}}[\mathcal{R}^{2}_{\mathrm{PI}}]\Big{\}}=e^{-4q\gamma_{*}^{2}}4q^{2} \gamma_{*}^{2}\sin^{2}(2\beta_{*})=q/e,\] (3.7)

where the maximizer is \((\gamma_{\star},\beta_{*})=(\frac{1}{2\sqrt{q}},\pi/4)\). Thus, for \(q>e\), 1-step QAOA gives better overlap than 1-step power iteration.

**Remark 3.5** (Rounding via \(\mathrm{sign}(\hat{\bm{u}})\) will not improve the overlap).: The readers may wonder whether the overlap of tensor power iteration will be improved by rounding the estimator via \(\bar{\bm{u}}_{1}=\mathrm{sign}(\hat{\bm{u}}_{1})\in\{\pm 1\}^{n}\), outputting an estimator in the signal space. Defining \(\overline{\mathcal{R}}_{\mathrm{PI}}=\bar{\bm{u}}_{1}^{\top}\bm{u}/n\), it is straightforward to show that as \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=\Lambda\),

\[\overline{\mathcal{R}}_{\mathrm{PI}}\stackrel{{ d}}{{ \longrightarrow}}\Phi(\Lambda G^{q-1}),\quad\text{ where }G\sim\mathcal{N}(0,1),\ \ \Phi(t)=2\times\mathbb{P}_{Z\sim \mathcal{N}(0,1)}(Z\leq t)-1.\] (3.8)

Hence, the computational threshold has the same exponent by rounding, but the overlap becomes smaller:

\[\lim_{\Lambda\to 0}\Lambda^{-2}\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[\overline{ \mathcal{R}}_{\mathrm{PI}}^{2}]=(2/\pi)\cdot\mathbb{E}_{G\sim\mathcal{N}(0,1)}[ G^{2q-2}].\] (3.9)

**Remark 3.6** (Sine-Gaussian law versus sine-arctan-Gaussian law).: The sine-Gaussian law of QAOA is particularly interesting in that the overlap will not concentrate as \(\Lambda\to\infty\). Instead, it will satisfy a sine-uniform distribution, i.e., \(\sin(2q\Lambda\gamma G^{q-1})\stackrel{{ d}}{{\longrightarrow}}\sin(U)\) for \(U\sim\mathrm{Unif}([0,2\pi])\). In contrast, the sine-arctan-Gaussian law of tensor power iteration will concentrate at \(\{\pm 1\}\) as \(\Lambda\to\infty\).

In Appendix E, we also study the scenario where prior information about the signal may be leveraged to recover the signal with a smaller SNR. There, we rigorously analyzed the 1-step QAOA applied to boost the signal in a weak estimator in Theorem 2, and compared it classical power iteration in Proposition E.2. Our result shows that the 1-step QAOA has the same asymptotic computational efficiency as 1-step power iteration, albeit with a constant-factor better overlap in the \(\Lambda\ll 1\) regime when \(q>e\).

### Weak recovery threshold and overlap distribution for \(p\)-step QAOA

We next consider the general \(p\)-step QAOA for weak recovery in the spiked tensor model. Although it is known that the QAOA is able to output the MLE that weakly recovers the signal when \(p\) grows unboundedly with \(n\), here we focus on a more analytically tractable regime where \(p\) is an arbitrary fixed constant in the \(n\to\infty\) limit. Using a physics-style derivation, we show that the \(p\)-step QAOA can achieve weak recovery when the signal-to-noise ratio satisfies

\[\lambda_{n}=\Omega\Big{(}n^{(q-2+\varepsilon_{p})/2}\Big{)},\quad\text{where} \quad\varepsilon_{p}=\begin{cases}\frac{q-2}{(q-1)^{p}-1},&q>2,\\ 1/p,&q=2.\end{cases}\] (3.10)

Observe that \(0<\varepsilon_{p}\leq 1\) and \(\lim_{p\to\infty}\varepsilon_{p}=0\). Hence, the \(p\)-step QAOA can recover the signal with a progressively weaker SNR as \(p\) increases. Moreover, we are able to characterize the overlap distribution \(\mathcal{R}_{\mathrm{QAOA}}\equiv\hat{\bm{u}}^{\top}\bm{u}/n\) of \(p\)-step QAOA between a sample \(\hat{\bm{u}}\sim|\bm{\gamma},\bm{\beta}\rangle\) (see Eq. (2.3)) and the signal \(\bm{u}\) as follows:

**Claim 3.7** (\(p\)-step QAOA for weak recovery).: _Consider the \(p\)-step QAOA with parameters \(\{(\bm{\gamma}_{n},\bm{\beta}_{n})\}_{n\geq 1}\) applied to the spiked tensor model (1.1) with signal-to-noise ratio \(\{\lambda_{n}\}_{n\geq 1}\). Suppose_

\[\lim_{n\to\infty}\Big{(}\bm{\gamma}_{n},\bm{\beta}_{n},\lambda_{n}/n^{(q-2+ \varepsilon_{p})/2}\Big{)}=(\bm{\gamma},\bm{\beta},\Lambda).\] (3.11)

_Then, there are parameter-dependent coefficients \((a_{p}(\bm{\gamma},\bm{\beta}),b_{p}(\bm{\gamma},\bm{\beta}))\) such that over the randomness of \(\bm{Y}\) and the quantum measurement, the overlap \(\mathcal{R}_{\mathrm{QAOA}}\) of the \(p\)-step QAOA converges in distribution to a sine-Gaussian law as_

\[\mathcal{R}_{\mathrm{QAOA}}\stackrel{{ d}}{{\longrightarrow}}a_{p }\sin(b_{p}\Lambda^{1/\varepsilon_{p}}G^{(q-1)^{p}}),\qquad\text{where}\quad G \sim\mathcal{N}(0,1).\] (3.12)

The derivation of Claim 3.7 is contained in Appendix D. We remark that our derivation uses non-rigorous heuristics from physics such as the Dirac delta function and its Fourier transform to linearize exponents in combinatorial sums (see Appendix D.1 for a sketch). Analytical expressions for the coefficients \(a_{p}(\bm{\gamma},\bm{\beta})\) and \(b_{p}(\bm{\gamma},\bm{\beta})\) can be found in Appendix D.5.

**Remark 3.8** (Weak recovery threshold).: As \(\Lambda\to 0\), Eq. (3.12) implies that \(\mathcal{R}_{\mathrm{QAOA}}\stackrel{{ p}}{{\longrightarrow}}0\). Thus, Claim 3.7 implies that \(\lambda_{n}=\Theta(n^{(q-2+\varepsilon_{p})/2})\) is the weak recovery threshold by the \(p\)-step QAOA in the spiked tensor model in the regime of fixed QAOA parameter. We believe this scaling is also the weak recovery threshold for the QAOA with any sequence of parameters \((\bm{\gamma}_{n},\bm{\beta}_{n})\), but proving this requires ruling out better performance of the QAOA when \((\bm{\gamma}_{n},\bm{\beta}_{n})\) is allowed to depend strongly on \(n\) as we have done in Theorem 1(a); we leave this as future work. Since \(\varepsilon_{p}\to 0\) as \(p\to\infty\), this means \(\lambda_{n}=\Theta(n^{(q-2)/2})\) is the recovery threshold given a diverging number of QAOA steps (but constant with respect to \(n\)). However, this does not achieve the \(\Theta(n^{(q-2)/4})\) computational threshold for classical algorithms.

Comparison with classical tensor power iteration.We now compare the overlap from the \(p\)-step QAOA to that from the classical \(p\)-step tensor power iteration algorithm. We show that the weak recovery threshold for the \(p\)-step power iteration estimator is also \(\lambda_{n}=\Theta(n^{(q-2+\varepsilon_{p})/2})\), and we provide the distribution of the overlap \(\mathcal{R}_{\mathrm{PI}}\equiv\hat{\bm{u}}_{p}^{\top}\bm{u}/n\) between the \(p\)-step power iteration estimator \(\hat{\bm{u}}_{p}\) (see Eq. (2.1)) and the signal \(\bm{u}\).

**Proposition 3.9** (Corollary of Lemma 3.2 of [33]).: _Consider a random instance of the spiked tensor model with \(\lim_{n\to\infty}\lambda_{n}/n^{(q-2+\varepsilon_{p})/2}=\Lambda\). The overlap \(\mathcal{R}_{\mathrm{PI}}\) of the \(p\)-step tensor power iteration algorithm converges in distribution as_

\[\mathcal{R}_{\mathrm{PI}}\stackrel{{ d}}{{\longrightarrow}}\sin \left[\arctan(\Lambda^{1/\varepsilon_{p}}G^{(q-1)^{p}})\right],\quad\text{ where}\quad G\sim\mathcal{N}(0,1).\] (3.13)The proof of Proposition 3.9 is contained in Appendix H.3.

**Remark 3.10** (Comparing the overlaps).: In the small \(\Lambda\ll 1\) regime, we have

\[\mathcal{R}_{\mathrm{QAOA}}\asymp(|a_{p}b_{p}|^{\varepsilon_{p}}\Lambda)^{1/ \varepsilon_{p}}G^{(q-1)^{p}}\qquad\text{and}\qquad\mathcal{R}_{\mathrm{PI}} \asymp\Lambda^{1/\varepsilon_{p}}G^{(q-1)^{p}}.\] (3.14)

When \(|a_{p}b_{p}|>1\), the QAOA has a constant factor advantage over the classical power iteration algorithm in the overlap achieved, assuming the conjectured Claim 3.7 based on heuristic derivations. To quantify this advantage, we consider the quantum enhancement factor, \(|a_{p}b_{p}|^{\varepsilon_{p}}\), which is the factor that the signal-to-noise ratio can shrink for the QAOA while maintaining the same overlap as the power iteration algorithm. Effectively, this factor \(|a_{p}b_{p}|^{\varepsilon_{p}}\) corresponds to a quantum improvement in the recovery threshold by the QAOA over classical power iteration. We numerically optimize \(|a_{p}b_{p}|^{\varepsilon_{p}}\) with respect to the QAOA parameters \((\bm{\gamma},\bm{\beta})\), and present the optimized values in Table 1.

**Remark 3.11** (Weak recovery threshold for QAOA with tensor unfolding).: Although neither the constant-step QAOA nor the tensor power iteration matches the \(\Theta(n^{(q-2)/4})\) recovery threshold for the best polynomial-time classical algorithms, we can achieve this threshold using the idea of tensor unfolding. When \(q\) is even, the tensor \(\bm{Y}\in\mathbb{R}^{n}\) can be unfolded into a matrix \(\bm{\overline{Y}}\):

\[\bm{\overline{Y}}=(\lambda_{n}/n^{q/2})\cdot\bm{\bar{u}}\bm{\bar{u}}^{\top}+( 1/\sqrt{n})\cdot\bm{\overline{W}}\in\mathbb{R}^{n^{q/2}\times n^{q/2}}.\] (3.15)

Here \(\overline{Y}_{(j_{1},\ldots,j_{q/2}),(j_{q/2+1},\ldots,j_{q})}=Y_{j_{1}\cdots j _{q}}\), \(\overline{W}_{(j_{1},\ldots,j_{q/2}),(j_{q/2+1},\ldots,j_{q})}=W_{j_{1}\cdots j _{q}}\), and \(\bar{\bm{u}}=\mathrm{vec}(\bm{u}^{\otimes(q/2)})\in\{\pm 1\}^{n^{q/2}}\). Existing work [2, 47] have demonstrated that the leading eigenvector \(\bar{\bm{z}}\) of \(\bm{\overline{Y}}\) has non-vanishing correlation with the signal \(\bar{\bm{u}}\) as soon as \(\lambda_{n}>n^{(q-2)/4}\). Furthermore, for the eigenvector \(\bar{\bm{z}}\) in such a regime, standard analysis as in [2] implies that the top singular vector of \(\mathrm{mat}(\bar{\bm{z}})\in\mathbb{R}^{n\times n^{q/2-1}}\) will have non-trivial overlap with the signal \(\bm{u}\), achieving the \(\Theta(n^{(q-2)/4})\) weak recovery threshold for the spectral method with tensor-unfolding.

A similar tensor-unfolding pre-processing could be applied to the QAOA to improve the computational threshold. Indeed, the QAOA method could be adopted to maximize the cost function \(\overline{C}(\bar{\bm{\sigma}})=\bar{\bm{\sigma}}^{\top}\overline{\bm{Y}}\bar {\bm{\sigma}}/n^{(q-1)/2}\) with decision variable \(\bar{\bm{\sigma}}\in\{\pm 1\}^{n^{q/2}}\). Notice that such a QAOA method needs to be applied to a \(n^{q/2}\)-qubit system. Effectively, \(\overline{C}(\bar{\bm{\sigma}})\) could be interpreted as the cost function of a spiked \(2\)-tensor model of size \(\bar{n}=n^{q/2}\) and with a rescaled signal-to-noise ratio \(\bar{\lambda}_{n}=\lambda_{n}/n^{(q-2)/4}\). According to Claim 3.7, \(p\)-step QAOA outputs a long bit-string \(\bar{\bm{z}}\in\{\pm 1\}^{n^{q/2}}\) overlapping with the signal \(\bar{\bm{u}}\) as long as \(\bar{\lambda}_{n}=\bar{n}^{\varepsilon_{p}/2}\) for \(\varepsilon_{p}=1/p\). Translating to the scaling of \(\lambda_{n}\), the computational threshold for QAOA with tensor unfolding is \(\lambda_{n}=\Omega(n^{(q-2+\varepsilon_{p}^{\prime})/4})\) where \(\varepsilon_{p}^{\prime}=q/p\). This recovers the classical \(\Theta(n^{(q-2)/4})\) threshold as \(p\to\infty\).

## 4 Numerical simulations

We now validate our theoretical results by conducting numerical simulations of the QAOA through classical computers. In this section, we focus on the case of 1-step QAOA (\(p=1\)) for the spiked

\begin{table}
\begin{tabular}{c||c|c|c|c|c|c} \(p\) & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline \hline
1 & 0.8578 & 1.0505 & 1.2131 & 1.3562 & 1.4857 & 1.6047 \\
2 & 0.9663 & 1.0505 & 1.1916 & 1.2882 & 1.4167 & 1.5162 \\
3 & 1.0204 & 1.0314 & 1.1615 & 1.2555 & 1.3844 & 1.4917 \\
4 & 1.0487 & 1.0144 & 1.1419 & 1.2447 & 1.3795 & 1.4858 \\
5 & 1.0631 & 1.0063 & 1.1327 & 1.2411 & 1.3770 & 1.4845 \\
6 & 1.0697 & 1.0013 & 1.1297 & 1.2399 & 1.3743 & 1.4842 \\
7 & 1.0719 & & & & & \\ \end{tabular}
\end{table}
Table 1: **The quantum enhancement factor \(|a_{p}b_{p}|^{\varepsilon_{p}}\) of the \(p\)-step QAOA over the \(p\)-step tensor power iteration**, for spiked \(q\)-tensors when \(\lambda_{n}=\Lambda n^{(q-2+\varepsilon_{p})/2}\) in the \(\Lambda\ll 1\) regime. Note in the first row, which corresponds to \(p=1\) with \(\varepsilon_{1}=1\), we know the optimal value \(|a_{1}b_{1}|=\sqrt{q/e}\) from Eq. (3.7). The remaining values are optimized via a quasi-Newton method starting with 1000 heuristic initial guesses of \((\bm{\gamma},\bm{\beta})\) and keeping the best value; hence, they currently should be considered as lower bounds on the best possible enhancement factors.

matrix model (\(q=2\)), where we can obtain an explicit formula the expected squared overlap at any finite problem dimension \(n\) (see Appendix F for a derivation):

\[\begin{split}\mathbb{E}_{\boldsymbol{Y}}[\langle\mathcal{R}^{2}_{ \text{QAOA}}\rangle_{{}_{\gamma},\beta}]&=\frac{n-1}{2n}e^{-8\gamma^{2}(n-2)/n }\sin^{2}(2\beta)[1-\cos^{n-2}(8\lambda\gamma/n)]\\ &+\frac{n-1}{n}e^{-4\gamma^{2}(n-1)/n}\sin(4\beta)\sin(4\lambda \gamma/n)\cos^{n-2}(4\lambda\gamma/n)+\frac{1}{n}.\end{split}\] (4.1)

In Fig. 2(a), we report the overlap distribution of 1-step QAOA (\(p=1\)) for the spiked matrix model (\(q=2\)) where the SNR is chosen as \(\lambda_{n}=n^{1/2}\). The histogram shows the Monte Carlo simulation results following the predicted sine-Gaussian law. The dashed gray lines are from the simulations of the QAOA using classical algorithms for \(n=26\), each corresponding to one of 40 instances. Note that simulating QAOA classically has complexity \(O(2^{n})\), which limits us to \(n=26\). We see

Figure 3: Example overlap distributions from \(p\)-step QAOA for the spiked tensor model for \(1\leq p\leq 5\). The top row shows data from 40 random 26-bit instances with \(q=2\) and \(\lambda_{n}=n^{1/(2p)}\). The bottom row shows data from 40 random 23-bit instances with \(q=3\) and \(\lambda_{n}=n^{[1+1/(2^{p}-1)]/2}\). Different columns correspond to different \(p\), using the QAOA parameters (\(\boldsymbol{\gamma},\boldsymbol{\beta}\)) that optimized \(|a_{p}b_{p}|^{\varepsilon_{p}}\) in Table 1. Dash gray lines connect data from the same instance. Blue histograms are the theoretical sine-Gaussian distributions in the \(n\to\infty\) limit, where \(\mathcal{R}_{\text{QAOA}}\sim a_{p}\sin[b_{p}G^{(q-1)^{p}}]\) according to Claim 3.7. (Note here \(\Lambda=1\).)

Figure 2: (a) Example overlap distribution from 1-step QAOA for the spiked matrix model (\(q=2\)), where simulation data is collected from 40 random generated instances with \(n=26\) bits. The signal-to-noise ratio is chosen to be \(\lambda_{n}=n^{1/2}\), and \((\gamma,\beta)=(\sqrt{\ln 5/32},\pi/4)\). Dash gray lines connect data from the same instance. (b) Average of squared overlap \(\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{{}_{\gamma},\beta}\) from the QAOA output distribution for 40 random instances generated at various problem dimensions.

that, despite some finite sample effects, the predicted sine-Gaussian distribution matches the QAOA simulation.

Fig. 2(b) reports the expected squared overlap from the QAOA simulations. The green dashed line is the theoretical prediction in the \(n\rightarrow\infty\) limit. The blue solid line is the finite \(n\) theoretical prediction from Eq. (4.1). The gray dots are the squared overlaps from individual QAOA instances simulated classically. The average over instances (red crosses) agrees well with the finite \(n\) theory prediction, which converges to the \(n\rightarrow\infty\) limit with order \(1/n\) deviation.

We also perform simulations for \(1\leq p\leq 5\) and \(q=2,3\). Fig. 3 plots the overlap distribution for \(p\)-step QAOA. The simulation curves follow the shape of the theoretical histograms for \(p\leq 2\). For \(p\geq 3\), the shapes of the simulated and theoretical overlap distributions do not match well, likely due to finite size effects (simulations for large \(n>26\) are computationally challenging).

In Appendix G, we present additional numerical simulation results on higher \((p,q)\) and find the second moment of the QAOA overlap converges to our theoretical predictions up to \(O(1/n)\) deviations. We also describe more details of the simulation methods.

An interesting phenomenon apparent from Fig. 2(a) and Fig. 3 is that the output distribution of the QAOA appears to concentrate over the randomness of instances \(\bm{Y}\), but not over the quantum measurements. This is in stark contrast to previous concentration results on the QAOA where concentration over measurements were shown, e.g., for spin-glass models in [24; 26; 32]. We note that such anti-concentration is also expected in the limit of zero noise (\(\lambda\rightarrow\infty\)), where it is known the constant-\(p\) QAOA can prepare the GHZ state [48]. Since existing limitations of both classical [29] and quantum algorithms [30; 26; 31; 32] on various problems over random structures rely heavily on concentration, extending these negative results to the QAOA for the spiked tensor model do not seem possible due to the absence of concentration. Nevertheless, our analysis shows that the constant-\(p\) QAOA is unable to improve the recovery threshold in the spiked tensor model achieved by classical algorithms by more than a constant factor.

## 5 Discussion

In this paper, we have investigated the power of quantum algorithms for the spiked tensor model, a canonical problem in statistical inference with a large computational-statistical gap that has so far eluded classical algorithms. We gave the first rigorous study of a polynomial-time quantum algorithm on this problem by analyzing the performance of the QAOA, a popular variational quantum algorithm that has been implemented on current quantum computing hardware. We showed that \(p\)-step QAOA achieves the same asymptotic SNR threshold for weak recovery as \(p\)-step tensor power iteration. A heuristic analysis showed that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold \(\Theta(n^{(q-2)/4})\). This implies that achieving a strong quantum advantage via the QAOA requires using a number of steps \(p\) that grows with \(n\). However, we revealed that the asymptotic overlap distribution of QAOA exhibits an intriguing sine-Gaussian law, distinct from tensor power iteration. For certain parameters \((p,q)\), the QAOA effectively has a recovery threshold that is a constant factor better, indicating a modest quantum advantage over the classical power iteration. Overall, while achieving identical scalings as power iteration, the QAOA demonstrates qualitative differences and potential for quantum speedups.

There are many interesting questions that remain open. One worthy challenge would be a rigorous proof for the \(p>1\) analysis without relying on heuristic arguments. Additionally, it would be interesting to prove that the sine-Gaussian distribution is concentrated over problem instances but not over measurements, as suggested by our simulations. This is in contrast to recent results showing that the low-depth QAOA is concentrated over measurements [26; 32], a seemingly essential ingredient for many proofs of algorithmic limitations [26; 29; 30; 31; 32]. Despite the absence of concentration in the spiked tensor setting, our results show that the constant-\(p\) QAOA has limited power, similar to the message of recent works [26; 30; 31; 32; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51] proving limitations up to \(p=O(\log n)\). This suggests that demonstrating strong quantum advantage requires analyzing super-logarithmic depth QAOA, which remains an outstanding open question. Finally, it would be interesting to study quantum algorithms in other statistical inference models that classically exhibit computational-statistical gaps, including planted clique, Bayesian linear models, and sparse PCA. Overcoming any such gap with a polynomial-time quantum algorithm would be an exciting superpolynomial quantum speedup with practical relevance.

## Acknowledgments

We thank David Gamarnik for insightful discussions and Stuart Hadfield for detailed comments on the manuscript. We thank Yuchen Wu for providing the proof of Proposition 3.9 and Ruixiang Zhang for the helpful discussion on the potential for making Claim 3.7 rigorous. LZ acknowledges funding from the Walter Burke Institute for Theoretical Physics at Caltech. JB is partially supported by a grant from the Simons Foundation under Award No. 825053 and the NASA Ames Research Center, from NASA Academic Mission Services (NAMS) under Contract No. NNA16BD14C, and from the DARPA ONISQ program under interagency agreement IAA 8839, Annex 114. SM is supported by NSF DMS-2210827, CCF-2315725, CAREER DMS-2339904, ONR N00014-24-S-B001, an Amazon Research Award, a Google Research Scholar Award, and an Okawa Foundation Research Grant.

## References

* [1] Wei-Kuo Chen. Phase transition in the spiked random tensor with Rademacher prior. _The Annals of Statistics_, 47(5):2734-2756, 2019.
* [2] Andrea Montanari and Emile Richard. A statistical model for tensor PCA. In _Proceedings of the 27th International Conference on Neural Information Processing Systems_, volume 2 of _NIPS'14_, pages 2897-2905, Cambridge, MA, USA, 2014. MIT Press.
* [3] Thibault Lesieur, Leo Miolane, Marc Lelarge, Florent Krzakala, and Lenka Zdeborova. Statistical and computational phase transitions in spiked tensor estimation. In _2017 IEEE International Symposium on Information Theory (ISIT)_, pages 511-515. IEEE, 2017.
* [4] Alexander S Wein, Ahmed El Alaoui, and Cristopher Moore. The Kikuchi hierarchy and tensor PCA. In _2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 1446-1468. IEEE, 2019.
* [5] Gerard Ben Arous, Song Mei, Andrea Montanari, and Mihai Nica. The landscape of the spiked tensor model. _Communications on Pure and Applied Mathematics_, 72(11):2282-2330, 2019.
* [6] Valentina Ros, Gerard Ben Arous, Giulio Biroli, and Chiara Cammarota. Complex energy landscapes in spiked-tensor and simple glassy models: Ruggedness, arrangements of local minima, and phase transitions. _Physical Review X_, 9(1):011003, 2019.
* [7] Aukosh Jagannath, Patrick Lopatto, and Leo Miolane. Statistical thresholds for tensor PCA. _Annals of Applied Probability_, 30(4):1910-1933, 2020.
* [8] Amelia Perry, Alexander S Wein, and Afonso S Bandeira. Statistical limits of spiked tensor models. _Annales de l'Institut Henri Poincare, Probabilites et Statistiques_, 56:230-264, 2020.
* [9] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Algorithmic thresholds for tensor PCA. _The Annals of Probability_, 48(4):2052-2087, 2020.
* [10] Jiaoyang Huang, Daniel Z Huang, Qing Yang, and Guang Cheng. Power Iteration for Tensor PCA. _The Journal of Machine Learning Research_, 23(1):5567-5613, 2022.
* [11] Gerard Ben Arous, Daniel Zhengyu Huang, and Jiaoyang Huang. Long random matrices and tensor unfolding. _The Annals of Applied Probability_, 33(6B):5753-5780, 2023.
* [12] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for sgd: Effective dynamics and critical scaling. _Advances in Neural Information Processing Systems_, 35:25349-25362, 2022.
* [13] Matthew Brennan and Guy Bresler. Reducibility and statistical-computational gaps from secret leakage. In _Conference on Learning Theory_, pages 648-847. PMLR, 2020.
* [14] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization algorithm. _arXiv preprint arXiv:1411.4028_, 2014.
* [15] Leo Zhou, Sheng-Tao Wang, Soonwon Choi, Hannes Pichler, and Mikhail D Lukin. Quantum approximate optimization algorithm: Performance, mechanism, and implementation on near-term devices. _Physical Review X_, 10(2):021067, 2020.
* [16] Guido Pagano, Aniruddha Bapat, Patrick Becker, Katherine S. Collins, Arinjoy De, Paul W. Hess, Harvey B. Kaplan, Antonis Kyprianidis, Wen Lin Tan, Christopher Baldwin, Lucas T. Brady, Abhinav Deshpande, Fangli Liu, Stephen Jordan, Alexey V. Gorshkov, and ChristopherMonroe. Quantum approximate optimization of the long-range ising model with a trapped-ion quantum simulator. _Proceedings of the National Academy of Sciences_, 117(41):25396-25401, 2020.
* Harrigan et al. [2021] Matthew P Harrigan, Kevin J Sung, Matthew Neeley, Kevin J Satzinger, Frank Arute, Kunal Arya, Juan Atalaya, Joseph C Bardin, Rami Barends, Sergio Boixo, et al. Quantum approximate optimization of non-planar graph problems on a planar superconducting processor. _Nature Physics_, 17(3):332-336, 2021.
* Ebadi et al. [2022] Sepehr Ebadi, Alexander Keesling, Madelyn Cain, Tout T Wang, Harry Levine, Dolev Bluvstein, Giulia Semeghini, Ahmed Omran, J-G Liu, Rhine Samajdar, et al. Quantum optimization of maximum independent set using Rydberg atom arrays. _Science_, 376(6598):1209-1215, 2022.
* Shaydulin et al. [2021] Ruslan Shaydulin, Changhao Li, Shouvanik Chakrabarti, Matthew DeCross, Dylan Herman, Niraj Kumar, Jeffrey Larson, Danylo Lykov, Pierre Minssen, Yue Sun, Yuri Alexeev, Joan M. Dreiling, John P. Gaebler, Thomas M. Gatterman, Justin A. Gerber, Kevin Gilmore, Dan Gresh, Nathan Hewitt, Chandler V. Horst, Shaohan Hu, Jacob Johansen, Mitchell Matheny, Tanner Mengle, Michael Mills, Steven A. Moses, Brian Neyenhuis, Peter Siegfried, Romain Yalovetzky, and Marco Pistoia. Evidence of scaling advantage for the quantum approximate optimization algorithm on a classically intractable problem. _Science Advances_, 10(22):eadm6761, 2024.
* Lloyd [2018] Seth Lloyd. Quantum approximate optimization is computationally universal. _arXiv preprint arXiv:1812.11075_, 2018.
* Lloyd et al. [2021] Seth Lloyd, Bobak T Kiani, David RM Arvidsson-Shukur, Samuel Bosch, Giacomo De Palma, William M Kaminsky, Zi-Wen Liu, and Milad Marvian. Hamiltonian singular value transformation and inverse block encoding. _arXiv preprint arXiv:2104.01410_, 2021.
* Farhi and Harrow [2016] Edward Farhi and Aram W Harrow. Quantum Supremacy through the Quantum Approximate Optimization Algorithm. _arXiv preprint arXiv:1602.07674_, 2016.
* Krovi [2022] Hari Krovi. Average-case hardness of estimating probabilities of random quantum circuits with a linear scaling in the error exponent. _arXiv preprint arXiv:2206.05642_, 2022.
* Farhi et al. [2022] Edward Farhi, Jeffrey Goldstone, Sam Gutmann, and Leo Zhou. The Quantum Approximate Optimization Algorithm and the Sherrington-Kirkpatrick Model at Infinite Size. _Quantum_, 6:759, 2022.
* Basso et al. [2022] Joao Basso, Edward Farhi, Kunal Marwaha, Benjamin Villalonga, and Leo Zhou. The Quantum Approximate Optimization Algorithm at High Depth for MaxCut on Large-Girth Regular Graphs and the Sherrington-Kirkpatrick Model. In _17th Conference on the Theory of Quantum Computation, Communication and Cryptography (TQC 2022)_, volume 232, pages 7:1-7:21, 2022.
* Basso et al. [2022] Joao Basso, David Gamarnik, Song Mei, and Leo Zhou. Performance and limitations of the QAOA at constant levels on large sparse hypergraphs and spin glass models. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 335-343. IEEE, 2022.
* Boulebnane and Montanaro [2021] Sami Boulebnane and Ashley Montanaro. Predicting parameters for the Quantum Approximate Optimization Algorithm for MAX-CUT from the infinite-size limit. _arXiv preprint arXiv:2110.10685_, 2021.
* Boulebnane and Montanaro [2024] Sami Boulebnane and Ashley Montanaro. Solving Boolean Satisfiability Problems With The Quantum Approximate Optimization Algorithm. _PRX Quantum_, 5:030348, 2024.
* Huang and Sellke [2022] Brice Huang and Mark Sellke. Tight lipschitz hardness for optimizing mean field spin glasses. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 312-322, 2022.
* Farhi et al. [2020] Edward Farhi, David Gamarnik, and Sam Gutmann. The quantum approximate optimization algorithm needs to see the whole graph: A typical case. _arXiv preprint arXiv:2004.09002_, 2020.
* Chou et al. [2022] Chi-Ning Chou, Peter J. Love, Juspreet Singh Sandhu, and Jonathan Shi. Limitations of Local Quantum Algorithms on Random MAX-k-XOR and Beyond. In _49th International Colloquium on Automata, Languages, and Programming (ICALP 2022)_, volume 229, pages 41:1-41:20, Dagstuhl, Germany, 2022.
* Anshu and Metger [2023] Anurag Anshu and Tony Metger. Concentration Bounds for Quantum States and Limitations on the QAOA from Polynomial Approximations. In _14th Innovations in Theoretical Computer Science Conference (ITCS 2023)_, volume 251, pages 5:1-5:8, 2023.

* [33] Yuchen Wu and Kangjie Zhou. Sharp analysis of power iteration for tensor PCA. _Journal of Machine Learning Research_, 25(195):1-42, 2024.
* [34] Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 178-191, 2016.
* [35] Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via sum-of-square proofs. In _Conference on Learning Theory_, pages 956-1006. PMLR, 2015.
* [36] Chiheon Kim, Afonso S Bandeira, and Michel X Goemans. Community detection in hypergraphs, spiked tensor models, and sum-of-squares. In _2017 International Conference on Sampling Theory and Applications (SampTA)_, pages 124-128. IEEE, 2017.
* [37] Rungang Han, Rebecca Willett, and Anru R Zhang. An optimal statistical and computational framework for generalized tensor estimation. _The Annals of Statistics_, 50(1):1-29, 2022.
* [38] Yuetian Luo, Garvesh Raskutti, Ming Yuan, and Anru R Zhang. A sharp blockwise tensor perturbation bound for orthogonal iteration. _The Journal of Machine Learning Research_, 22(1):8106-8153, 2021.
* [39] Anru Zhang and Dong Xia. Tensor SVD: Statistical and computational limits. _IEEE Transactions on Information Theory_, 64(11):7311-7338, 2018.
* [40] Anima Anandkumar, Yuan Deng, Rong Ge, and Hossein Mobahi. Homotopy analysis for tensor PCA. In _Conference on Learning Theory_, pages 79-104. PMLR, 2017.
* [41] Giulio Biroli, Chiara Cammarota, and Federico Ricci-Tersenghi. How to iron out rough landscapes and get optimal performances: averaged gradient descent and its application to tensor PCA. _Journal of Physics A: Mathematical and Theoretical_, 53(17):174003, 2020.
* [42] Rishabh Dudeja and Daniel Hsu. Statistical query lower bounds for tensor pca. _The Journal of Machine Learning Research_, 22(1):3729-3779, 2021.
* [43] Afonso S Bandeira, Ahmed El Alaoui, Samuel Hopkins, Tselil Schramm, Alexander S Wein, and Ilias Zadik. The franz-parisi criterion and computational trade-offs in high dimensional statistics. _Advances in Neural Information Processing Systems_, 35:33831-33844, 2022.
* [44] Matthew B Hastings. Classical and quantum algorithms for tensor principal component analysis. _Quantum_, 4:237, 2020.
* [45] Alexander Schmidhuber, Ryan O'Donnell, Robin Kothari, and Ryan Babbush. Quartic quantum speedups for planted inference. _arXiv preprint arXiv:2406.19378_, 2024.
* [46] Daniel Stilck Franca and Raul Garcia-Patron. Limitations of optimization algorithms on noisy quantum devices. _Nature Physics_, 17(11):1221-1227, 2021.
* [47] Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. 2005.
* [48] Wen Wei Ho, Cheryne Jonay, and Timothy H. Hsieh. Ultrafast variational simulation of nontrivial quantum states with long-range interactions. _Phys. Rev. A_, 99:052332, May 2019.
* [49] Sergey Bravyi, Alexander Kliesch, Robert Koenig, and Eugene Tang. Obstacles to variational quantum optimization from symmetry protection. _Physical review letters_, 125(26):260505, 2020.
* [50] Edward Farhi, David Gamarnik, and Sam Gutmann. The quantum approximate optimization algorithm needs to see the whole graph: Worst case examples. _arXiv preprint arXiv:2005.08747_, 2020.
* [51] Antares Chen, Neng Huang, and Kunal Marwaha. Local algorithms and the failure of log-depth quantum advantage on sparse random CSPs. _arXiv preprint arXiv:2310.01563_, 2023.
* [52] Jahan Claes and Wim van Dam. Instance independence of single layer quantum approximate optimization algorithm on mixed-spin models at infinite size. _Quantum_, 5:542, 2021.
* [53] Asier Ozaeta, Wim van Dam, and Peter L McMahon. Expectation values from the single-layer quantum approximate optimization algorithm on ising problems. _Quantum Science and Technology_, 7(4):045036, 2022.
* [54] Michel Talagrand. The Parisi formula. _Annals of mathematics_, pages 221-263, 2006.

###### Contents

* 1 Introduction
* 2 Background and related work
	* 2.1 Spiked tensor model and prior algorithms
	* 2.2 Quantum approximate optimization algorithm
* 3 Main results
	* 3.1 Weak recovery threshold and overlap distribution for \(1\)-step QAOA
	* 3.2 Weak recovery threshold and overlap distribution for \(p\)-step QAOA
* 4 Numerical simulations
* 5 Discussion
* A Additional background
* A.1 Review of quantum computing terminology
* A.2 Related literature on the QAOA
* B Moment generating function of the QAOA overlap at \(p=1\)
* C Proof of Theorem 1
* C.1 Proof sketch for Theorem 1(b) and emergence of sine-Gaussian law.
* C.2 Proof of Theorem 1(b)
* C.2.1 Proof of Lemma C.1
* C.2.2 Proof of Lemma C.2
* C.2.3 Proof of Lemma C.3
* C.3 Proof of Theorem 1(a)
* C.3.1 Proof of Lemma C.5
* C.3.2 Proof of Lemma C.6
* D Derivation for general \(p\)-step QAOA (Claim 3.7)
* D.1 Sketch of derivation ideas
* D.2 Organizing the finite \(n\) sum
* D.3 Rescaling the summand for the \(n\to\infty\) limit
* D.4 MGF at general \(p\) in the \(n\to\infty\) limit to show Claim 3.7
* D.5 A self contained formula for \(a_{p}(\boldsymbol{\gamma},\boldsymbol{\beta})\) and \(b_{p}(\boldsymbol{\gamma},\boldsymbol{\beta})\)
* E Signal boosting with \(1\)-step QAOA
* E.1 Proof of Theorem 2
* E.2 Proof of Lemma E.3
* E.3 Proof of Lemma E.4

* [54] Finite \(n\) **calculation for \(1\)-step QAOA on the spiked matrix** (\(q=2\))
* [55] G Additional numerical simulations
* [56] H Analysis of classical power iteration algorithm
* [57] H.1 Proof of Proposition 3.3
* [58] H.2 Proof of Proposition E.2
* [59] H.3 Proof of Proposition 3.9

## Appendix A Additional background

### Review of quantum computing terminology

For the convenience of readers who are unfamiliar with quantum computing terminology, we briefly introduce relevant linear algebra concepts. A quantum state of an \(n\)-qubit system is a \(2^{n}\)-dimensional unit complex vector \(\bm{\psi}\in\mathbb{C}^{2^{n}}\) satisfying \(\sum_{i\in[2^{n}]}|\psi_{i}|^{2}=1\). Each bit-string \(\bm{z}\in\{\pm 1\}^{n}\) associates with a quantum state \(|\bm{z}\rangle\in\mathbb{C}^{2^{n}}\), representing the \(|\bm{z}|\)'th canonical basis vector \([0,\cdots,0,1,0,\cdots,0]^{\top}\in\mathbb{C}^{2^{n}}\), where only position \(|\bm{z}|\) equals \(1\) (with \(|\bm{z}|=1+\sum_{j\in[n]}2^{j-1}(1-z_{j})/2\) denoting the rank of bit-string \(\bm{z}\)). Therefore, \(\bm{\psi}=\sum_{\bm{z}\in\{\pm 1\}}\psi_{|\bm{z}|}\bm{z}\) where \(|\psi_{|\bm{z}|}|^{2}\) gives the probability of observing \(\bm{z}\) upon measurement. This represents \(\bm{\psi}\) as a probability distribution over all \(2^{n}\) bit-strings in \(\{\pm 1\}^{n}\).

The Pauli operators \(\sigma_{x},\sigma_{y},\sigma_{z}\) on a single qubit are represented as \(2\times 2\) complex matrices:

\[I=\begin{bmatrix}1&0\\ 0&1\end{bmatrix},\ \ \sigma_{x}=\begin{bmatrix}0&1\\ 1&0\end{bmatrix},\ \ \sigma_{y}=\begin{bmatrix}0&-i\\ i&0\end{bmatrix},\ \ \sigma_{z}=\begin{bmatrix}1&0\\ 0&-1\end{bmatrix}.\] (A.1)

In an \(n\)-qubit system, the Pauli operators \(\{X_{k},Y_{k},Z_{k}\}\in\mathbb{C}^{2^{n}\times 2^{n}}\) associated to the \(k\)-th qubit are defined by \(I^{\otimes(k-1)}\otimes\{\sigma_{x},\sigma_{y},\sigma_{z}\}\otimes I^{\otimes( n-k)}\in\mathbb{C}^{2^{n}\times 2^{n}}\), where \(\otimes\) is the Kronecker product operator.

### Related literature on the QAOA

In terms of theoretical analysis of its computational complexity, the performance of the QAOA has been studied for various models, including the Sherrington-Kirkpatrick model [24], MaxCut [25, 27], the Max-\(q\)-XORSAT for regular hypergraphs [25], \(q\)-spin spin-glass models [52, 26], the ferromagnetic Ising model [53], and random constraint satisfaction problems [28]. While [24] shows promising evidence for the QAOA to achieve the ground state energy of the Sherrington-Kirkpatrick model [54, 26] proves that constant-step QAOA cannot achieve the ground state for \(q\)-spin spin-glass models in general.

There is also a line of work aiming to prove computational hardness results for the QAOA and related quantum algorithms. [50, 51, 30, 31] studied the limitation of local quantum algorithms like the QAOA for solving combinatorial optimization problems on sparse random graphs, using the bounded light-cone of the algorithms at sufficiently low depths. This limitation was translated to the dense spin-glass models in [26]. Furthermore, [49, 32] proved hardness results for the QAOA by exploiting the symmetry of the problem. Note all previously known limitations of the QAOA in the average-case setting [30, 31, 26, 32] have relied on concentration of the output distribution in the Hamming weight basis, which is not present in the spiked tensor model.

Our work studies the QAOA for a statistical inference problem, distinct from these existing results. Furthermore, we develop new techniques for analyzing the QAOA that do not exist in prior work.

Moment generating function of the QAOA overlap at \(p=1\)

We dedicate this section to derive a combinatorial expression for expected moment-generating function of the QAOA overlap, defined as

\[M_{n}(\zeta;\gamma_{n},\beta_{n},\lambda_{n}):=\left\langle e^{\zeta\widehat{ \mathcal{R}}}\right\rangle_{\gamma_{n},\beta_{n}}.\] (B.1)

We write \(M_{n}(\zeta)=M_{n}(\zeta;\gamma_{n},\beta_{n},\lambda_{n})\) for short. This quantity will be used in future derivations.

We use the techniques and conventions first introduced in [24]. First, we define bistrings \(\bm{a}\in B:=\{\pm 1\}^{3}\) indexed as \(\bm{a}=(a_{1},a_{\mathrm{m}},a_{2})\). Since \(p=1\), we write \(\beta=\beta_{1}\), \(\gamma=\gamma_{1}\). Additionally, define the quantities given by

\[Q_{\bm{a}} =\frac{1}{2}\left\langle a_{1}|e^{i\beta X}|1\right\rangle\left \langle 1|e^{-i\beta X}|a_{2}\right\rangle,\] (B.2) \[\Phi_{\bm{a}} =\gamma(a_{1}-a_{2}).\] (B.3)

We may also write \(\{n_{\bm{a}}\}_{\bm{a}\in B}\subseteq\mathbb{Z}^{|B|}\) where \(\sum_{\bm{a}\in A}n_{\bm{a}}=n\) to assign a count to each bit-string. If we underscore the bit-string, we mean \(\underline{\bm{a}}=(\bm{a}_{1},\bm{a}_{2},\ldots,\bm{a}_{q})\in B^{q}\). We also write \(\Phi_{\underline{\bm{a}}}=\Phi_{\bm{a}_{1}\bm{a}_{2}\cdots\bm{a}_{q}}\).

We now have the notation to state the following lemma.

**Lemma B.1** (QAOA overlap expected moment-generating function in the configuration basis for \(p=1\)).: _The expectation over the spiked tensor disorder in Eq. (1.1) of the moment-generating function defined in Eq. (B.1) for \(p=1\) is given by_

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{\{n_{\bm{a}}\}}\binom{n} {\{n_{\bm{a}}\}}\prod_{\bm{a}\in B}Q_{\bm{a}}^{n_{\bm{a}}}\exp\Bigg{[} -\frac{1}{2n^{q-1}}\sum_{\bm{a}\in B^{q}}\Phi_{\bm{a}}^{2}\prod_{s =1}^{q}n_{\bm{a}_{s}}+\frac{i\lambda_{n}}{n^{q-1}}\sum_{\underline{\bm{a}}\in B ^{q}}\Phi_{\underline{\bm{a}}}\prod_{s=1}^{q}(\bm{a}_{s})_{\mathrm{m}}n_{\bm {a}_{s}}\] \[+\frac{\zeta}{n}\sum_{\bm{v}\in B}v_{\mathrm{m}}n_{\bm{v}}\Bigg{]}.\] (B.4)

Proof of Lemma b.1.: Without loss of generality, we assume that \(\bm{u}=\bm{1}\) and proceed as in [24, Section 5] and [26, Appendix D.2]. By definition, we have that

\[M_{n}(\zeta) =\left\langle\bm{\gamma},\bm{\beta}\right|e^{\zeta\widehat{\mathcal{ R}}}\left|\bm{\gamma},\bm{\beta}\right\rangle\] \[=\left\langle s\right|e^{i\gamma C}e^{i\beta B}e^{\zeta\widehat{ \mathcal{R}}}e^{-i\beta B}e^{i\gamma C}\left|s\right\rangle.\] (B.5)

Inserting \(3\) resolutions of the identity \(\mathbb{I}=\sum_{\bm{z}}\left|\bm{z}\right\rangle\left\langle\bm{z}\right|\) observing that every computation basis state \(\left|\bm{z}\right\rangle\) is an eigenvector of \(C\) and \(\widehat{\mathcal{R}}\), we have that

\[M_{n}(\zeta) =\sum_{\bm{z}^{1},\bm{z}^{\mathrm{m}},\bm{z}^{2}}\left\langle s \right|e^{i\gamma C}\left|\bm{z}^{1}\right\rangle\left\langle\bm{z}^{1}\right| e^{i\beta B}e^{\zeta\widehat{\mathcal{R}}}\left|\bm{z}^{m}\right\rangle\left\langle \bm{z}^{m}\right|e^{-i\beta B}\left|\bm{z}^{2}\right\rangle\left\langle\bm{z}^ {2}\right|e^{i\gamma C}\left|s\right\rangle\] \[=\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{\mathrm{m}},\bm{z}^{2}} \left\langle\bm{z}^{1}\right|e^{i\beta B}\left|\bm{z}^{\mathrm{m}}\right\rangle e ^{i\gamma C(\bm{z}^{1})}e^{\zeta\widehat{\mathcal{R}}(\bm{z}^{m})}e^{i\gamma C (\bm{z}^{2})}\left\langle\bm{z}^{m}\right|e^{-i\beta B}\left|\bm{z}^{2}\right\rangle\] \[=\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{\mathrm{m}},\bm{z}^{2}} f_{\beta}^{*}(\bm{z}^{1}\bm{z}^{\mathrm{m}})f_{\beta}(\bm{z}^{\mathrm{m}}\bm{z}^{2}) \exp\left[i\gamma(C(\bm{z}^{1})-C(\bm{z}^{2}))+\zeta\mathcal{R}(\bm{z}^{ \mathrm{m}})\right]\] \[=\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{\mathrm{m}},\bm{z}^{2}} f_{\beta}^{*}(\bm{z}^{1}\bm{z}^{\mathrm{m}})f_{\beta}(\bm{z}^{\mathrm{m}}\bm{z}^{2})\] \[\quad\times\exp\left[i\gamma\sum_{i_{1},\ldots,i_{q}=1}^{n}\left( \frac{\lambda_{n}}{n^{q-1}}+\frac{W_{i_{1},\ldots,i_{q}}}{n^{q-1}/2}\right)(z_{ i_{1}}^{1}\cdots z_{i_{q}}^{1}-z_{i_{1}}^{2}\cdots z_{i_{q}}^{2})+\frac{\zeta}{n} \sum_{j=1}^{n}z_{j}^{\mathrm{m}}\right]\!,\] (B.6)

where we defined \(f_{\beta}(\bm{z}\bm{z}^{\prime})=\left\langle\bm{z}\right|e^{-i\beta B}\left|\bm {z}^{\prime}\right\rangle\) since this quantity only depends on the bitwise product \(\bm{z}\bm{z}^{\prime}\). We also used the definitions of \(C(\bm{z})=\left\langle\bm{Y},\bm{z}^{\otimes q}\right\rangle/n^{(q-2)/2}\) and \(\widehat{\mathcal{R}}(\bm{z})=\frac{1}{n}\sum_{j=1}^{n}z_{j}\). Next, we tranform the \(\bm{z}^{j}\) as follows:

\[\bm{z}^{1}\to\bm{z}^{1}\bm{z}^{\mathrm{m}},\qquad\bm{z}^{2}\to\bm{z}^{2}\bm{z}^{ \mathrm{m}}.\] (B.7)This gives

\[M_{n}(\zeta) =\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{m},\bm{z}^{2}}f_{\beta}^{*}( \bm{z}^{1})f_{\beta}(\bm{z}^{2})\] \[\quad\times\exp\Bigg{[}i\gamma\sum_{i_{1},\dots,i_{q}=1}^{n}\left( \frac{\lambda_{n}}{n^{q-1}}+\frac{W_{i_{1},\dots,i_{q}}}{n^{(q-1)/2}}\right)z_{ i_{1}}^{\text{m}}\cdots z_{i_{q}}^{\text{m}}(z_{i_{1}}^{1}\cdots z_{i_{q}}^{1}-z_{ i_{1}}^{2}\cdots z_{i_{q}}^{2})+\frac{\zeta}{n}\sum_{j=1}^{n}z_{j}^{\text{m}} \Bigg{]}\] \[=\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{m},\bm{z}^{2}}f_{\beta}^{ *}(\bm{z}^{1})f_{\beta}(\bm{z}^{2})\] \[\quad\times\exp\Bigg{[}i\sum_{i_{1},\dots,i_{q}=1}^{n}\left(\frac {\lambda_{n}}{n^{q-1}}+\frac{W_{i_{1},\dots,i_{q}}}{n^{(q-1)/2}}\right)z_{i_{1 }}^{\text{m}}\cdots z_{i_{q}}^{\text{m}}\Phi_{i_{1},\dots,i_{q}}(\bm{Z})+ \frac{\zeta}{n}\sum_{j=1}^{n}z_{j}^{\text{m}}\Bigg{]},\] (B.8)

where we denoted \(\bm{Z}=(\bm{z}^{1},\bm{z}^{2})\) and

\[\Phi_{i_{1},\dots,i_{q}}(\bm{Z})=\gamma(z_{i_{1}}^{1}\cdots z_{i_{q}}^{1}-z_{ i_{1}}^{2}\cdots z_{i_{q}}^{2}).\] (B.9)

Hence, the expected moment-generating function is

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\frac{1}{2^{n}}\sum_{\bm{z}^{1},\bm{z}^{m},\bm{z}^{2}}f_{\beta}^ {s}(\bm{z}^{1})f_{\beta}(\bm{z}^{2})\] \[\quad\times\exp\Bigg{[}\sum_{i_{1},\dots,i_{q}=1}^{q}\left(\frac{ i\lambda_{n}}{n^{q-1}}z_{i_{1}}^{\text{m}}\cdots z_{i_{q}}^{\text{m}}\Phi_{i_{1}, \dots,i_{q}}(\bm{Z})-\frac{1}{2n^{q-1}}\Phi_{i_{1},\dots,i_{q}}^{2}(\bm{Z}) \right)+\frac{\zeta}{n}\sum_{j=1}^{n}z_{j}^{\text{m}}\Bigg{]}.\] (B.10)

Now we change to the so-called configuration basis. For any bit-string \(1\leq j\leq n\), we look at a new bit-string:

\[(z_{j}^{1},z_{j}^{\text{m}},z_{j}^{2})\in B.\] (B.11)

For any \(\bm{a}\in B\), we represent by \(n_{\bm{a}}\) the number of times that configuration \(\bm{a}\) happens. Note that \(\sum_{\bm{a}\in B}=n\). For more details, we again refer the reader to [26, Appendix D.2]. Now, instead of counting over each bit of \(\bm{z}^{1},\bm{z}^{\text{m}},\bm{z}^{2}\), we can count over configurations in \(A\):

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\sum_{\{n_{\bm{a}}\}}\binom{n}{\{n_{\bm{a}}\}}\prod_{\bm{a}\in B }Q_{\bm{a}^{n_{\bm{a}}}}^{n_{\bm{a}}}\exp\Bigg{[}\frac{i\lambda_{n}}{n^{q-1}} \sum_{\bm{a}_{1},\dots,\bm{a}_{q}\in B}\Phi_{\bm{a}_{1}\cdots\bm{a}_{q}}(\bm{a }_{1})_{\text{m}}\cdots(\bm{a}_{q})_{\text{m}}n_{\bm{a}_{1}}\cdots n_{\bm{a}_{ q}}\] \[\quad-\frac{1}{2n^{q-1}}\sum_{\bm{a}_{1},\dots,\bm{a}_{q}\in B} \Phi_{\bm{a}_{1}\cdots\bm{a}_{q}}^{2}n_{\bm{a}_{1}}\cdots n_{\bm{a}_{q}}+\frac{ \zeta}{n}\sum_{\bm{a}\in B}a_{\text{m}}n_{\bm{a}}\Bigg{]},\] (B.12)

which finishes the proof of Lemma B.1. 

## Appendix C Proof of Theorem 1

### Proof sketch for Theorem 1(b) and emergence of sine-Gaussian law.

Here we briefly sketch the proof of Theorem 1 for \(1\)-step QAOA, explain how the sine-Gaussian law appears, and highlight the technical ideas.

To derive the distribution of the QAOA overlap, we compute its expected moment-generating function. We start by following the steps from [24, 26] to reformulate the expected moment-generating function (MGF). With some algebra, we arrive at the following equation (see Appendix B and Lemma C.1 for the derivation):

\[\mathbb{E}_{\bm{Y}}[\langle e^{\zeta\widehat{\mathcal{R}}}\rangle_{ \gamma,\beta}]=\sum_{t=0}^{n}\binom{n}{t}\Big{(}\frac{\sin(2\beta)}{4n}\Big{)}^ {t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1}}S_{n,t}\] (C.1) \[S_{n,t}=n^{t}\sum_{\sum n_{i}=t}\binom{t}{\{n_{i}\}}(-i)^{n_{1}- n_{2}+n_{3}-n_{4}}e^{(\zeta/n)(n_{1}+n_{2}-n_{3}-n_{4})}Z_{n,t}(n_{1}-n_{2}-n_{3}+n_ {4}),\] (C.2) \[Z_{n,t}(k)=\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n- t}{\tau_{+},\tau_{-}}(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)^{ \tau_{+}}(e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)^{\tau_{-}}\] \[\qquad\qquad\qquad\qquad\qquad\times e^{i\Lambda\gamma[(\tau_{+}- \tau_{-})+k)^{q}-((\tau_{+}-\tau_{-})-k)^{q}]/n^{(q-1)/2}}.\] (C.3)

Looking upon the term \(Z_{n,t}(k)\), we can interpret the summand \(\tau_{+}\) as a binomial variable \(\operatorname{Binom}(n-t,1/2)\), and by the Central Limit Theorem, we have \((\tau_{+}-\tau_{-})/\sqrt{n}\overset{d}{\longrightarrow}G\sim\mathcal{N}(0,1)\). This gives

\[\lim_{n\to\infty}Z_{n,t}(k)=\mathbb{E}_{G\sim\mathcal{N}(0,1)}[e^{i2qk\Lambda \gamma G^{q-1}}]=:Z_{t}(k).\]

This is the step where the power of Gaussian appears. Next, assuming that we can replace \(Z_{n,t}\) by \(Z_{t}\) in the expression of \(S_{n,t}\) as in (C.2), and using the multinomial theorem, we get

\[S_{n,t} \overset{\cdot}{=}\mathbb{E}_{G\sim\mathcal{N}(0,1)}\,n^{t}\sum_ {\sum n_{i}=t}\binom{t}{\{n_{i}\}}(-i)^{n_{1}-n_{2}+n_{3}-n_{4}}e^{(\zeta/n)(n _{1}+n_{2}-n_{3}-n_{4})}e^{(n_{1}-n_{2}-n_{3}+n_{4})i2q\Lambda\gamma G^{q-1}}\] \[\overset{\cdot}{\to}\mathbb{E}\bigg{\{}\sum_{t=0}^{\infty}\frac{ 1}{t!}[\zeta e^{-2q\gamma^{2}}\sin(2\beta)\sin(2q\Lambda\gamma G^{q-1})]^{t} \bigg{\}}=\mathbb{E}_{G\sim\mathcal{N}(0,1)}\left\{e^{\zeta e^{-2q\gamma^{2}} \sin(2\beta)\sin(2q\Lambda\gamma G^{q-1})}\right\}.\]

This gives the moment-generating function of the sine-Gaussian law.

We should notice that several steps in the above proof sketch are non-rigorous, in the sense that we could not sequentially take \(n\to\infty\) in \(Z_{n,t}\), \(S_{n,t}\), and the MGF. To make this step rigorous, we use the idea of discrete Fourier transform in Eq. (C.2) to decouple the two terms \(e^{(\zeta/n)(n_{1}+n_{2}-n_{3}-n_{4})}\) and \(Z_{n,t}\) (see Lemma C.1), which allows one to treat the \(n\to\infty\) limit of these two terms separately in the expression of \(S_{n,t}\). For more details, see the full proof in the following section.

### Proof of Theorem 1(b)

To prove Theorem 1(b), it suffices to show that the moment-generating function (MGF) of the QAOA overlap converges to the MGF of a sine-Gaussian law as follows:

\[\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\mathbb{E}_{G\sim\mathcal{ N}(0,1)}\left\{\exp\left[\zeta e^{-2q\gamma^{2}}\sin(2\beta)\sin(2q\Lambda \gamma G^{q-1})\right]\right\}=:M(\zeta).\] (C.4)

We start the proof of Eq. (C.4) with the following lemma, which obtains a more explicit expression for the MGF that we derived in Section B.

**Lemma C.1** (Expected moment-generating function).: _The expected moment-generating function in Eq. (B.4) can be evaluated as_

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{n}\binom{n}{t}e^{- \gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1}}[\sinh(\zeta/n)\sin(2\beta)]^{t}\cdot E _{n,t},\] (C.5)_where_

\[E_{n,t} =\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sin^{t}(2\pi\xi/(2t+1))\hat{Z}_{n,t} (\xi),\] \[\hat{Z}_{n,t}(\xi) =\sum_{k=-t}^{t}e^{-2\pi i\xi k/(2t+1)}Z_{n,t}(k),\] \[Z_{n,t}(k) =\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau_{+ },\tau_{-}}(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)^{\tau_{+}}(e^{- \zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)^{\tau_{-}}\] \[\quad\times e^{i\Lambda_{n}\gamma[((\tau_{+}-\tau_{-})+k)^{q}-(( \tau_{+}-\tau_{-})-k)^{q}]/n^{(q-1)/2}}.\] (C.6)

_Here we have \(\Lambda_{n}=\lambda_{n}/n^{(q-1)/2}\)._

The proof of Lemma C.1 is deferred to Section C.2.1. Now we define

\[\Lambda =\lim_{n\to\infty}\Lambda_{n},\] \[I_{n,t} =\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1}}[\sinh( \zeta/n)\sin(2\beta)]^{t}\cdot E_{n,t},\] (C.7) \[I_{t} =\frac{1}{t!}\,\mathbb{E}_{G\sim\mathcal{N}(0,1)}\left[[\zeta e^ {-2q\gamma^{2}}\sin(2\beta)\sin(2q\Lambda\gamma G^{q-1})]^{t}\right].\]

Then it is easy to see that

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{n}I_{n,t},\quad M(\zeta)=\sum_{t =0}^{\infty}I_{t}.\]

As a consequence, we have

\[\Big{|}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]-M(\zeta)\Big{|}\leq\sum_{t=0}^{T}| I_{n,t}-I_{t}|+\Big{|}\sum_{t\geq T+1}I_{t}\Big{|}+\sum_{t=T+1}^{n}|I_{n,t}|.\] (C.8)

The following lemma gives the limit of \(E_{n,t}\) for fixed \(t\) as \(n\to\infty\), which indicates that \(I_{t}\) is the limit of \(I_{n,t}\).

**Lemma C.2**.: _For any fixed integer \(t\), we have_

\[\lim_{n\to\infty}E_{n,t}=\mathbb{E}_{G\sim\mathcal{N}(0,1)}[\sin^{t}(2q\Lambda \gamma G^{q-1})]\equiv E_{t}.\]

_As a consequence, we have_

\[\lim_{n\to\infty}I_{n,t}=I_{t}.\]

Furthermore, we have the following upper bound of \(I_{n,t}\).

**Lemma C.3**.: _For any \(t\leq n\) and \(\zeta\leq n\), we have_

\[|I_{n,t}|\leq\frac{1}{t!}(6|\zeta|)^{t}(2t+1)e^{|\zeta|}\equiv s_{t},\]

_where_

\[\sum_{t=0}^{\infty}s_{t}<\infty.\]

The proof of Lemma C.2 and C.3 is deferred to Section C.2.2 and C.2.3, respectively. Now we assume that these two lemmas hold. By the fact that \(\sum_{t=0}^{\infty}I_{t}\) is finite and by Lemma C.3, for any \(\varepsilon>0\), there exists \(T=T_{\varepsilon}\) such that

\[\Big{|}\sum_{t\geq T_{\varepsilon+1}}I_{t}\Big{|}\leq\varepsilon/3,\quad\sum_ {t\geq T_{\varepsilon}+1}s_{t}\leq\varepsilon/3.\]Furthermore, by Lemma C.2, there exists \(N=N_{\varepsilon}\) such that as long as \(n\geq N_{\varepsilon}\), we have

\[\sum_{t=0}^{T_{\varepsilon}}|I_{n,t}-I_{t}|\leq\varepsilon/3.\]

As a consequence, by Eq. (C.8), for any \(n\geq n_{\varepsilon}\) and \(\zeta\leq n\), we have

\[\Big{|}\operatorname{\mathbb{E}}_{\boldsymbol{Y}}[M_{n}(\zeta)]-M(\zeta) \Big{|}\leq\sum_{t=0}^{T_{\varepsilon}}|I_{n,t}-I_{t}|+\Big{|}\sum_{t\geq T_{ \varepsilon}+1}I_{t}\Big{|}+\sum_{t=T_{\varepsilon}+1}^{\infty}s_{t}\leq\varepsilon.\] (C.9)

This proves Eq. (C.4) as desired, and hence finishes the proof of Theorem 1(b).

#### c.2.1 Proof of Lemma c.1

Our starting point is Eq. (B.4), which we can compute explicitly with a careful organization of the sum. To this end, let

\[t_{+} =n_{++-}+n_{-++}, t_{-} =n_{+--}+n_{--+},\] (C.10) \[d_{+} =n_{++-}-n_{-++}, d_{-} =n_{+--}-n_{--+},\] \[\tau_{+} =n_{+++}+n_{---}, \tau_{-} =n_{+-+}+n_{-+-},\] \[\Delta_{+} =n_{+++}-n_{---}, \Delta_{-} =n_{+-+}-n_{-+-}.\]

Observe that these 8 variables completely determine \(\{n_{\boldsymbol{a}}:\boldsymbol{a}\in B\}\). Furthermore, let

\[t=t_{+}+t_{-},\qquad n-t=\tau_{+}+\tau_{-}.\] (C.11)

Then explicit computation shows that

\[\sum_{\boldsymbol{a}\in B^{q}}\Phi_{\boldsymbol{a}}^{2}\prod_{s= 1}^{q}n_{\boldsymbol{a}_{s}} =4\gamma^{2}\sum_{\boldsymbol{a}}1\{a_{11}\cdots a_{q1}\neq a_{1 2}\cdots a_{q2}\}\prod_{s=1}^{q}n_{\boldsymbol{a}_{q}}\] (C.12) \[=2\gamma^{2}\Big{[}n^{q}-(n-2t)^{q}\Big{]},\] \[\sum_{\boldsymbol{a}\in B^{q}}\Phi_{\boldsymbol{a}}\prod_{s=1}^{ q}(\boldsymbol{a}_{s})_{m}n_{\boldsymbol{a}_{s}} =\gamma\Big{[}\big{(}\sum_{\boldsymbol{a}}a_{1}a_{m}n_{\boldsymbol{ a}}\big{)}^{q}-\big{(}\sum_{\boldsymbol{a}}a_{2}a_{m}n_{\boldsymbol{a}} \big{)}^{q}\Big{]}\] \[=\gamma[(\tau_{+}-\tau_{-})+(d_{+}-d_{-}))^{q}-((\tau_{+}-\tau_{- })-(d_{+}-d_{-}))^{q}],\] (C.13) \[\sum_{\boldsymbol{v}\in B}v_{m}n_{\boldsymbol{v}} =t_{+}-t_{-}+\Delta_{+}-\Delta_{-}.\] (C.14)

Plugging this into Eq. (B.4) and breaking up the sum, we get

\[\operatorname{\mathbb{E}}_{\boldsymbol{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t} \binom{n-t}{\tau_{+},\tau_{-}}\] \[\qquad\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}} Q_{---}^{n_{---}}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{+-+}}Q_{+-+}^{n_{+-}}Q_{-+-}^{n _{+-}}e^{(\zeta/n)(t_{+}-t_{-}+\Delta_{+}-\Delta_{-})}\] \[\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++}}Q_{-++}^{n_{-++ }}\sum_{d_{-}}\binom{t_{-}}{n_{+-}}Q_{+--}^{n_{--+}}Q_{--+}^{n_{--+}}\] \[\quad e^{i\Lambda_{n}\gamma[(d_{+}-d_{-}+\tau_{+}-\tau_{-})^{q}-(( \tau_{+}-\tau_{-})-(d_{+}-d_{-}))^{q}]/n^{(q-1)/2}},\] (C.15)

where \(\Lambda_{n}=\lambda_{n}/n^{(q-1)/2}\) as shorthand. Now, let us evaluate the sum over \(\Delta_{\pm}\). We can use the following identity

\[2^{\tau_{+}}\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}}Q_{--- }^{n_{---}}e^{(\zeta/n)\Delta_{+}}=(2Q_{++}+e^{\zeta/n}+2Q_{---}e^{-\zeta/n})^ {\tau_{+}}.\] (C.16)Applying this to the earlier sum, and note that \(Q_{+++}=Q_{+-+}\) and \(Q_{---}=Q_{-+-}\), we get:

\[\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1} }\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n -t}{\tau_{+},\tau_{-}}\] (C.17) \[\qquad\frac{1}{2^{n-t}}(2Q_{+++}e^{\zeta/n}+2Q_{---}e^{-\zeta/n}) ^{\tau_{+}}(2Q_{++}e^{-\zeta/n}+2Q_{---}e^{\zeta/n})^{\tau_{-}}e^{(\zeta/n)(t_{ +}-t_{-})}\] \[\qquad\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++-}}Q_{++}^ {n_{++-}}\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{--+}^{n_{--+}}\] \[\qquad e^{i\Lambda_{n}\gamma[(d_{+}-d_{-}+\tau_{+}-\tau_{-})^{q}- ((\tau_{+}-\tau_{-})-(d_{+}-d_{-}))^{q}]/n^{(q-1)/2}}.\]

To further simplify the expression, we define \(Z_{n,t}(k)\) as in Eq. (C.6), which we reproduce here:

\[Z_{n,t}(k)=\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t} \binom{n-t}{\tau_{+},\tau_{-}}(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n} \sin^{2}\beta)^{\tau_{+}}(e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)^ {\tau_{-}}\] (C.18) \[\qquad\times e^{i\Lambda_{n}\gamma[((\tau_{+}-\tau_{-})+k)^{q}-(( \tau_{+}-\tau_{-})-k)^{q}]/n^{(q-1)/2}}.\]

Then we have

\[\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{d_{+}}\binom{t_{+}}{n_{++-}} Q_{++-}^{n_{+++}}\] (C.19) \[\qquad\times\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{+--}^{n_{+--}}Q_ {--+}^{n_{--+}}e^{(\zeta/n)(t_{+}-t_{-})}Z_{n,t}(d_{+}-d_{-}).\]

Let \(\Omega_{t}=\{-t,-t+1,\ldots,t-1,t\}\), and let \(\{\hat{Z}_{n,t}(\xi)\}_{\xi\in\Omega_{t}}\) to be the discrete Fourier transform of \(Z_{n,t}(k)\) as defined in Eq. (C.6), i.e.,

\[\hat{Z}_{n,t}(\xi)=(\mathcal{F}_{t}Z_{n,t})(\xi)=\sum_{k=-t}^{t}e^{-2\pi i\xi k /(2t+1)}Z_{n,t}(k),\] (C.20)

By the property of Fourier transforms, we have

\[Z_{n,t}(k)=(\mathcal{F}_{t}^{-1}\hat{Z})(k)=\frac{1}{2t+1}\sum_{\xi=-t}^{t}e^ {2\pi i\xi k/(2t+1)}\hat{Z}_{n,t}(\xi).\] (C.21)

Plugging Eq. (C.21) into Eq. (C.19), we have

\[\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{d_{+}}\binom{t_{+}}{n_{++-} }Q_{++-}^{n_{+++}}\] (C.22) \[\qquad\times\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{+--}^{n_{+--}}Q_ {--+}^{n_{--+}}e^{(\zeta/n)(t_{+}-t_{-})}\frac{1}{2t+1}\sum_{\xi=-t}^{t}e^{2 \pi i\xi(d_{+}-d_{-})/(2t+1)}\hat{Z}_{n,t}(\xi)\]

\[\overset{(i)}{=}\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^ {q-1}}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}e^{(\zeta/n)(t_{+}-t_{-})}\] (C.23) \[\qquad\times(-1)^{t_{-}}\cdot\frac{1}{2t+1}\sum_{\xi=-t}^{t} \Big{(}2iq_{++-}\sin(2\pi\xi/(2t+1))\Big{)}^{t}\hat{Z}_{n,t}(\xi)\] \[\overset{(ii)}{=}\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}- (n-2t)^{q}]/n^{q-1}}(\sinh(\zeta/n)\sin(2\beta))^{t}\] \[\qquad\times\frac{1}{2t+1}\sum_{\xi=-t}^{t}\Big{(}\sin(2\pi\xi/(2 t+1))\Big{)}^{t}\hat{Z}_{n,t}(\xi),\] (C.24)where (i) used the equation

\[\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++-}}Q_{-++}^{n_{-++}}e^{i\xi d_{+}}= \Big{(}2iQ_{++-}\sin\xi\Big{)}^{t_{+}},\] (C.25)

and (ii) used the equation

\[\sum_{r+s=t}\binom{t}{r,s}(+1)^{r}(-1)^{s}\exp\{\zeta(r-s)\}=2^{t}\sinh(\zeta)^ {t}.\] (C.26)

Note we have also used \(4iQ_{++-}=\sin 2\beta\) to get rid of the two factors of \(2^{t}\). This completes the proof of Lemma C.1.

#### c.2.2 Proof of Lemma c.2

We first look at the limit of \(Z_{n,t}(k)\) for fixed integer \(-t\leq k\leq t\) (c.f. Eq. (C.6)). We denote \(T_{n}=(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)\), \(U_{n}=(e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)\) and \(G_{n}=(\tau_{+}-\tau_{-})/\sqrt{n}\) with \(\tau_{+}\sim\mathrm{Bin}(n-t,1/2)\)) to be a random variable. Then we have

\[Z_{n,t}(k)=\mathbb{E}_{G_{n}}\Big{[}T_{n}^{\sqrt{n}G_{n}}(T_{n}U_{n})^{(n- \sqrt{n}G_{n})/2}e^{i\Lambda_{n}\gamma\sqrt{n}[(G_{n}+k/\sqrt{n})^{q}-(G_{n}-k /\sqrt{n})^{q}])}\Big{]}.\] (C.27)

Note that we have \(\lim_{n\to\infty}T_{n}^{\sqrt{n}}=\lim_{n\to\infty}(T_{n}U_{n})^{-\sqrt{n}/2} =\lim_{n\to\infty}(T_{n}U_{n})^{n/2}=1\) and by assumption we have \(\lim_{n\to\infty}\Lambda_{n}=\Lambda\). Furthermore, by central limit theorem, we have \(G_{n}\to G\sim\mathcal{N}(0,1)\) so that for any fixed \(-t\leq k\leq t\),

\[\sqrt{n}[(G_{n}+k/\sqrt{n})^{q}-(G_{n}-k/\sqrt{n})^{q}])\overset{d}{\longrightarrow }2qkG^{q-1}.\]

This implies that

\[\lim_{n\to\infty}Z_{n,t}(k)=\mathbb{E}_{G\sim\mathcal{N}(0,1)}[e^{ik2q\Lambda \gamma G^{q-1}}]\equiv Z(k).\]

As a consequence, we have

\[\lim_{n\to\infty}E_{n,t}=\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sin(2\pi\xi/(2t+1))^ {t}\Big{(}\sum_{k=-t}^{t}e^{-2\pi i\xi k/(2t+1)}\,\mathbb{E}_{G\sim\mathcal{N }(0,1)}[e^{ik2q\Lambda\gamma G^{q-1}}]\Big{)}.\]

Finally, by Lemma C.4 below and noting that \(\sin(2\pi\xi/(2t+1))^{t}\) can be expressed as a degree \(t\) polynomial of \((e^{2\pi i\xi/(2t+1)},e^{-2\pi i\xi/(2t+1)})\), the right hand side of the equation above gives

\[\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sin(2\pi\xi/(2t+1))^{t}\Big{(}\sum_{k=-t}^{t} e^{-2\pi i\xi k/(2t+1)}\,\mathbb{E}_{G\sim\mathcal{N}(0,1)}[e^{ik2q\Lambda \gamma G^{q-1}}]\Big{)}=\mathbb{E}_{G\sim\mathcal{N}(0,1)}[\sin(2q\Lambda \gamma G^{q-1})^{t}].\]

This proves Lemma C.2.

**Lemma C.4**.: _Let \(t\in\mathbb{Z}_{\geq 0}\) be an integer and let \(\Omega_{t}=\{-t,-t+1,\ldots,t-1,t\}\). For a vector \((Z(k))_{k\in\Omega_{t}}\), we denote \(\mathcal{F}_{t}\colon\mathbb{C}^{2t+1}\to\mathbb{C}^{2t+1}\) to be the discrete Fourier transform_

\[(\mathcal{F}_{t}Z)(\xi)\equiv\sum_{k=-t}^{t}e^{-2\pi i\xi k/(2t+1)}Z(k).\]

_Let \(P:\mathbb{C}^{2}\to\mathbb{C}\) be any fixed polynomials with degree less or equal to \(t\in\mathbb{Z}_{\geq 0}\). Let \(X\) be a real-valued random variable. Then we have_

\[\frac{1}{2t+1}\sum_{\xi=-t}^{t}\Big{[}P(e^{2\pi i\xi/(2t+1)},e^{-2\pi i\xi/(2t+ 1)})\Big{(}\mathcal{F}_{t}\big{(}\,\mathbb{E}_{X}[e^{ikX}]\big{)}\Big{)}(\xi) \Big{]}=\mathbb{E}_{X}[P(e^{iX},e^{-iX})].\] (C.28)

Proof of Lemma c.4.: By linearity of the expectation operator and the discrete Fourier transform operator, we just need to prove Eq. (C.28) for \(P(e^{2\pi i\xi/(2t+1)},e^{-2\pi i\xi/(2t+1)})=e^{2\pi ip\xi/(2t+1)}\) for some integer \(-t\leq p\leq t\). Note that we have

\[\frac{1}{2t+1}\sum_{\xi=-t}^{t}\left[e^{2\pi ip\xi/(2t+1)}\mathcal{F} _{t}\Big{(}\operatorname{\mathbb{E}}_{X}[e^{ikX}]\Big{)}\right]\] \[=\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sum_{k=-t}^{t}e^{2\pi ip\xi/(2t+1 )}e^{-2\pi ik\xi/(2t+1)}\operatorname{\mathbb{E}}_{X}[e^{ikX}]\] \[=\operatorname{\mathbb{E}}_{X}\Big{[}\frac{1}{2t+1}\sum_{\xi=-t}^ {t}\sum_{k=-t}^{t}e^{2\pi i(p-k)(\xi/(2t+1)-X/(2\pi))}e^{ipX}\Big{]}=\operatorname {\mathbb{E}}_{X}[e^{ipX}],\] (C.29)

where the last equality used the fact that

\[\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sum_{k=-t}^{t}e^{2\pi i(p-k)(\xi/(2t+1)-X/(2 \pi))}=1\]

for any integer \(-t\leq p\leq t\) and any real \(X\). This completes the proof of Lemma C.4. 

#### c.2.3 Proof of Lemma C.3

By the definition of \(Z_{n,t}(k)\) as in Eq. (C.6), it is easy to see that

\[|Z_{n,t}(k)| \leq\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau _{+},\tau_{-}}\left|e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta\right| ^{\tau_{+}}\left|e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta\right|^{ \tau_{-}}\] \[\quad\times\left|e^{i\Lambda_{n}\gamma[(\tau_{+}-\tau_{-})+k)^{q} -((\tau_{+}-\tau_{-})-k)^{q}]/n^{(q-1)/2}}\right|\] \[\leq\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau _{+},\tau_{-}}e^{\tau_{+}|\zeta|/n}e^{\tau_{-}|\zeta|/n}\cdot 1\] \[=e^{(n-t)|\zeta|/n}\] \[\leq e^{|\zeta|}.\] (C.30)

As a consequence, we have \(|\hat{Z}_{n,t}(\xi)|\leq(2t+1)e^{|\zeta|}\), which gives

\[|E_{n,t}|\leq(2t+1)e^{|\zeta|}.\]

As a consequence, by the definition of \(I_{n,t}\) as in Eq. (C.7), we have

\[|I_{n,t}|\leq\frac{n^{t}}{t!}|\sinh(\zeta/n)|^{t}(2t+1)e^{|\zeta|}.\]

Note that when \(\zeta/n\leq 1\), we have \(|\sinh(\zeta/n)|\leq 6|\zeta|/n\). This gives

\[|I_{n,t}|\leq\frac{1}{t!}(6|\zeta|)^{t}(2t+1)e^{|\zeta|}.\]

This proves Lemma C.3.

### Proof of Theorem 1(a)

Theorem 1(a) is a combination of the two lemmas below.

**Lemma C.5**.: _Take any sequence of \(\{\beta_{n}\}_{n\geq 1}\subseteq[0,2\pi]\), \(\{\gamma_{n}\}_{n\geq 1}\subseteq\mathbb{R}\) with \(\lim_{n\to\infty}\gamma_{n}=\infty\), and any sequence of \(\{\lambda_{n}\}_{n\geq 1}\subseteq[0,\infty)\). We have_

\[\lim_{n\to\infty}\operatorname{\mathbb{E}}_{W}[\langle\mathcal{R}_{\rm QAOA} ^{2}\rangle_{\gamma_{n},\beta_{n}}]=0.\] (C.31)

**Lemma C.6**.: _Take any sequence of \(\{\beta_{n}\}_{n\geq 1}\subseteq[0,2\pi]\), \(\{\gamma_{n}\}_{n\geq 1}\subseteq\mathbb{R}\) with \(\sup_{n}\gamma_{n}<\infty\), and any sequence of \(\{\lambda_{n}\}_{n\geq 1}\subseteq[0,\infty)\) with \(\lim_{n\to\infty}\lambda_{n}/n^{(q-1)/2}=0\). We have_

\[\lim_{n\to\infty}\operatorname{\mathbb{E}}_{W}[\langle\mathcal{R}_{\rm QAOA} ^{2}\rangle_{\gamma_{n},\beta_{n}}]=0.\] (C.32)

#### c.3.1 Proof of Lemma c.5

Proof of Lemma c.5.: Denote \(\Lambda_{n}=\lambda_{n}/n^{(q-1)/2}\). We can write

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\mathrm{QAOA}}\rangle_{\gamma_{n}, \beta_{n}}]=\frac{\partial^{2}}{\partial\zeta^{2}}\big{|}_{\zeta=0}\,\mathbb{E }_{\bm{Y}}[M_{n}(\zeta;\gamma_{n},\beta_{n},\lambda_{n})].\] (C.33)

Using Eq. (C.5), we can see that only a few terms depend on \(\zeta\), whose derivative gives

\[\partial^{2}_{\zeta}\big{|}_{\zeta=0}\Big{[}\big{(}\sinh(\zeta/n )\sin(2\beta)\big{)}^{t}\big{(}e^{\zeta/n}\cos^{2}(\beta)+e^{-\zeta/n}\sin^{2} (\beta)\big{)}^{\tau_{+}}\big{(}e^{-\zeta/n}\cos^{2}(\beta)+e^{\zeta/n}\sin^{2 }(\beta)\big{)}^{\tau_{-}}\Big{]}\] \[= \frac{\delta_{t=0}}{2n^{2}}\left(2t\sin(2\beta)+\big{[}(\tau_{+} -\tau_{-})^{2}-(\tau_{+}+\tau_{-})\big{]}\cos(4\beta)+\big{[}(\tau_{+}-\tau_{ -})^{2}+(\tau_{+}+\tau_{-})\big{]}\right)\] \[+ \frac{\delta_{t=1}}{n^{2}}t(\tau_{+}-\tau_{-})\sin(4\beta)\] \[+ \frac{\delta_{t=2}}{n^{2}}t(t-1)\sin^{2}(2\beta).\] (C.34)

Hence, only the \(t=0,1,2\) terms survive, and we can write

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\mathrm{QAOA}}\rangle_{\gamma_{n},\beta_{n}}]=T_{0}+T_{1}+T_{2}\] (C.35)

where

\[T_{0}=\frac{1}{2^{n+1}n^{2}}\sum_{\tau_{+}+\tau_{-}=n}\binom{n}{ \tau_{+},\tau_{-}}\left(\big{[}(\tau_{+}-\tau_{-})^{2}-(\tau_{+}+\tau_{-}) \big{]}\cos(4\beta)+\big{[}(\tau_{+}-\tau_{-})^{2}+(\tau_{+}+\tau_{-})\big{]} \right),\] (C.36)

\[T_{1}=\frac{\sin(4\beta_{n})}{3n\cdot 2^{n-1}}e^{-\gamma_{n}^{2}[n^ {q}-(n-2)^{q}]/n^{q-1}}\sum_{\xi\in\{\pm 1\}}\sin(2\pi\xi/3)\sum_{k=-1}^{1}e^{-2 \pi i\xi k/3}\] \[\qquad\times\sum_{\tau_{+}+\tau_{-}=n-1}\binom{n-1}{\tau_{+},\tau _{-}}(\tau_{+}-\tau_{-})e^{i\Lambda_{n}\gamma_{n}[(\tau_{+}-\tau_{-})+k)^{q}- ((\tau_{+}-\tau_{-})-k)^{q}]/n^{(q-1)/2}},\] (C.37) \[T_{2}=\frac{(n-1)\sin^{2}(2\beta_{n})}{10n\cdot 2^{n-2}}e^{- \gamma_{n}^{2}[n^{q}-(n-4)^{q}]/n^{q-1}}\sum_{\xi\in\{\pm 1,\pm 2\}}\sin^{2}(2\pi \xi/5)\sum_{k=-2}^{2}e^{-2\pi i\xi k/5}\] \[\qquad\times\sum_{\tau_{+}+\tau_{-}=n-2}\binom{n-2}{\tau_{+},\tau _{-}}e^{i\Lambda_{n}\gamma_{n}[(\tau_{+}-\tau_{-})+k)^{q}-((\tau_{+}-\tau_{-}) -k)^{q}]/n^{(q-1)/2}}.\] (C.38)

Lemma C.5 then immediately follows from the Lemma C.7 below. 

**Lemma C.7**.: _For any \(n\geq n_{0}\) for some large \(n_{0}\), we have_

\[T_{0}=(1+\cos(4\beta_{n}))/(2n),\] \[|T_{1}|\leq 2\sin(4\beta_{n})e^{-q\gamma_{n}^{2}},\] \[|T_{2}|\leq 2\sin^{2}(2\beta_{n})e^{-q\gamma_{n}^{2}}.\]

Proof of Lemma c.7.: We can compute the first term directly as follows. Note that

\[\sum_{\tau_{+}+\tau_{-}=n}\binom{n}{\tau_{+},\tau_{-}}(\tau_{+}- \tau_{-})^{q}=\frac{\partial^{q}}{\partial x^{q}}\Big{|}_{x=0}\sum_{\tau_{+}+ \tau_{-}=n}\binom{n}{\tau_{+},\tau_{-}}e^{x(\tau_{+}-\tau_{-})}\] (C.39) \[=\frac{\partial^{q}}{\partial x^{q}}\Big{|}_{x=0}\Big{(}2\cosh(x) \Big{)}^{n}.\] (C.40)

In particular,

\[\sum_{\tau_{+}+\tau_{-}=n}\binom{n}{\tau_{+},\tau_{-}}(\tau_{+}- \tau_{-})=0,\] (C.41) \[\sum_{\tau_{+}+\tau_{-}=n}\binom{n}{\tau_{+},\tau_{-}}(\tau_{+}- \tau_{-})^{2}=2^{n}n.\] (C.42)It follows that

\[T_{0}=\frac{1}{2^{n+1}n^{2}}\left((2^{n}n-0)\cos(4\beta)+(2^{n}n+0)\right)=\frac{ \cos(4\beta)+1}{2n}.\] (C.43)

For the remaining terms, upper bounds suffice:

\[|T_{1}| \leq\frac{\sin(4\beta_{n})}{3n\cdot 2^{n-1}}e^{-\gamma_{n}^{2}[n^{q}-(n -2)^{q}]/n^{q-1}}\sum_{\xi\in\{\pm 1\}}1\cdot\sum_{k=-1}^{1}1\cdot\sum_{\tau_{+}+ \tau_{-}=n-1}\binom{n-1}{\tau_{+},\tau_{-}}\cdot(n-1)\cdot 1\] \[\leq\frac{\sin(4\beta_{n})}{3}e^{-q\gamma_{n}^{2}}\cdot 2\cdot 3\] \[=2\sin(4\beta_{n})e^{-q\gamma_{n}^{2}},\] (C.44)

and

\[|T_{2}| \leq\frac{(n-1)\sin^{2}(2\beta_{n})}{10n\cdot 2^{n-2}}e^{-q \gamma_{n}^{2}}\sum_{\xi\in\{\pm 1,\pm 2\}}1\cdot\sum_{k=-2}^{2}1\cdot\sum_{ \tau_{+}+\tau_{-}=n-2}\binom{n-2}{\tau_{+},\tau_{-}}\cdot 1\] \[\leq\frac{\sin^{2}(2\beta_{n})}{10}e^{-q\gamma_{n}^{2}}\cdot 4 \cdot 5\] \[=2\sin^{2}(2\beta_{n})e^{-q\gamma_{n}^{2}}.\] (C.45)

This finishes the proof of Lemma C.7. 

#### c.3.2 Proof of Lemma c.6

Lemma C.6 follows from Theorem 1(b).

## Appendix D Derivation for general \(p\)-step QAOA (Claim 3.7)

### Sketch of derivation ideas

We now briefly sketch some ideas behind the derivation for Claim 3.7 that characterizes the overlap distribution of the \(p\)-step QAOA when the SNR ratio scales as in Eq. (3.10). Similar to Theorem 1, our approach is to evaluate the moment-generating function of the QAOA overlap in the \(n\to\infty\) limit. As evident in the proof of Theorem 1, as well as in previous analyses of the QAOA applied to spin-glass models [24, 26, 28], the key technical difficulty is handling a "generalized multinomial sum" of the following form:

\[S=\sum_{m_{j}\geq 0,\ \sum_{j}m_{j}=n}\binom{n}{\{m_{j}\}}\Big{(}\prod_{j}Q_ {j}^{m_{j}}\Big{)}\exp[P(\bm{m})],\] (D.1)

where \(P(\bm{m})\) is a polynomial over entries of \(\bm{m}=(m_{j})_{j}\) with degree \(q\). Note the above summation has no analytical simplification when \(P\) is not a linear polynomial (\(q>1\)). Previous works have evaluated this sum in the \(n\to\infty\) limit either by proving a "generalized multinomial theorem" that exploits combinatorial structures of the polynomial \(P\)[24, 26], or by employing a Gaussian integration trick and the saddle-point method when \(q=2^{\ell}\)[28]. However, neither approach is sufficient for the spiked tensor model that we study in the present paper.

Instead, we develop an alternative approach based on the Fourier transform to linearize exponents in the summands. The idea is to replace \(\bm{m}\) with continuous variables \(\bm{\mu}\) via Dirac delta functions, which after Fourier transforms yield exponents that are linear in \(\bm{m}\), enabling us to analytically evaluate the multinomial sum over \(\bm{m}\) as follows:

\[S =\int d\bm{\mu}\int d\hat{\bm{\mu}}\sum_{m_{j}\geq 0,\ \sum_{j}m_{j}=n} \binom{n}{\{m_{j}\}}\Big{(}\prod_{j}Q_{j}^{m_{j}}\Big{)}\exp[P(\bm{\mu})]e^{i \hat{\bm{\mu}}\cdot(\bm{m}-\bm{\mu})}\] \[=\int d\bm{\mu}\int d\hat{\bm{\mu}}\left(\sum_{j}Q_{j}e^{i\hat{ \bm{\mu}}_{j}}\right)^{n}e^{P(\bm{\mu})-i\hat{\bm{\mu}}\cdot\bm{\mu}}.\] (D.2)See Appendix D.4 for more details. This is a powerful approach to replace the cumbersome multinomial sums with simpler integrals. However, it is difficult to make such manipulations involving Dirac delta functions rigorous, which we leave open as future work. Nevertheless, we proceed with the heuristic derivation in the current paper: by writing the variables \((m_{j})_{j}\) in an alternative basis and rescaling them cleverly, we are able to evaluate the integrals to obtain the moment-generating function in the \(n\to\infty\) limit.

### Organizing the finite \(n\) sum

Our goal is to evaluate the moment-generating function of the overlap with signal, \(M_{n}(\zeta)=\langle\bm{\gamma},\bm{\beta}|\exp(\zeta\widehat{\mathcal{R}})| \bm{\gamma},\bm{\beta}\rangle,\) for the general \(p\)-step QAOA. Using the same method as in the \(p=1\) case, we can show that the disorder-averaged moment-generating function can be written as the following combinatorial sum:

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{\{n_{a}\}}\binom{n}{\{n_{\bm{a}}\}}\prod _{\bm{a}\in B}Q_{\bm{a}}^{n_{a}}\exp\Big{[}\mathcal{A}+i\lambda_{n}\mathcal{ B}+\zeta\mathcal{C}\Big{]},\] (D.3)

where

\[\mathcal{A}=-\frac{1}{2n^{q-1}}\sum_{\underline{\bm{a}}\in B^{q}} \Phi_{\underline{\bm{a}}}^{2}\prod_{s=1}^{q}n_{\bm{a}_{s}},\qquad\mathcal{B} =\frac{1}{n^{q-1}}\sum_{\underline{\bm{a}}\in B^{q}}\Phi_{\bm{a}} \prod_{s=1}^{q}(\bm{a}_{s})_{m}n_{\bm{a}_{s}},\qquad\mathcal{C}=\frac{1}{n} \sum_{\bm{v}\in B}v_{\mathrm{m}}n_{\bm{v}},\] (D.4)

and

\[B =\big{\{}(a_{1},a_{2},\ldots,a_{p},a_{m},a_{-p},\ldots,a_{-1}):a_{ j}\in\{\pm 1\}\big{\}},\] (D.5) \[Q_{\bm{a}} =\tfrac{1}{2}\prod_{r=1}^{p}(\cos\beta_{r})^{1+(a_{r}+a_{-r})/2}( \sin\beta_{r})^{1-(a_{r}+a_{-r})/2}(i)^{(a_{-r}-a_{r})/2},\] \[\Phi_{\bm{a}} =\sum_{r=1}^{p}\gamma_{r}\big{(}a_{r}a_{r+1}\cdots a_{p}\ -\ a_{-p}\cdots a_{-r-1}a_{-r}\big{)},\] \[\Phi_{\bm{a}} =\Phi_{\bm{a}_{1}\bm{a}_{2}\cdots\bm{a}_{q}}.\]

Note \(Q_{\bm{a}}\) and \(\Phi_{\bm{a}}\) are independent of \(a_{\mathrm{m}}\).

This is a straightforward generalization of the proof in Appendix B, where we insert \(2p+1\) resolutions of the identity instead of \(3\). This also closely follows the derivation in Ref. [26, Appendix D.2]. \(B,Q_{\bm{a}},\Phi_{\bm{a}}\) are also generalizations of the same quantities in Appendix B for \(p>1\).

Define the rank function

\[\ell(\bm{a})=\max(\{i:a_{-i}\neq a_{i}\}\cup\{0\}).\] (D.6)

A canonical basis.We next perform further simplifications that remove the explicit dependence on \(a_{\mathrm{m}}\). First we define the set of \(2p\)-bit strings as

\[A=\big{\{}(a_{1},a_{2},\ldots,a_{p},a_{-p},\ldots,a_{-1}):a_{j}\in\{\pm 1\} \big{\}},\]

and define \(A_{0}\) and \(D\) according to a similar convention as that in [26] as follows:

\[A_{0} :=\{\bm{a}\in A:\ell(\bm{a})=0\}=\{\bm{a}\in A:a_{-k}=a_{k}\ \text{for}\ 1\leq k\leq p\},\] \[D :=\Big{\{}\bm{a}\in A:\ell(\bm{a})>0\quad\text{and}\quad\prod_{j= 1}^{p}a_{j}=+1\Big{\}}.\] (D.7)

Given the rank function in Eq. (D.6), we can define an ordering on \(D\), which we borrow from [26]. For any two distinct element \(\bm{a}_{1},\bm{a}_{2}\in D\), we define the \(\prec\) relation as following: (1) If \(\ell(\bm{a}_{1})<\ell(\bm{a}_{2})\), we let \(\bm{a}_{1}\prec\bm{a}_{2}\); (2) If \(\ell(\bm{a}_{1})>\ell(\bm{a}_{2})\), we let \(\bm{a}_{2}\prec\bm{a}_{1}\); (3) If \(\ell(\bm{a}_{1})=\ell(\bm{a}_{2})\) and if \(\bm{a}_{1}\) is lexically less than \(\bm{a}_{2}\), we let \(\bm{a}_{1}\prec\bm{a}_{2}\); (3) If \(\ell(\bm{a}_{1})=\ell(\bm{a}_{2})\) and if \(\bm{a}_{1}\) is lexically greater than \(\bm{a}_{2}\), we let \(\bm{a}_{2}\prec\bm{a}_{1}\) (here lexical order means that, for example, \((-1,-1),(-1,1),(1,-1),(1,1)\) are in lexically increasing order). It is easy to see that such \(\prec\) relation is a full order, so that we can also define \(\preceq,\succeq\), and \(\succ\) accordingly.

For any \(\bm{a}\in A\), we define

\[n_{\bm{a}\pm}=n_{\bm{b}}\quad\text{where}\quad\bm{b}=(a_{1},\ldots,a_{p},\pm 1,a_{-p},\ldots,a_{-1}).\] (D.8)Let

\[t_{\bm{a}+} =n_{\bm{a}+}+n_{\bar{\bm{a}}+}, t_{\bm{a}-} =n_{\bm{a}-}+n_{\bar{\bm{a}}-}, \quad\forall\bm{a}\in D,\] (D.9) \[d_{\bm{a}+} =n_{\bm{a}+}-n_{\bar{\bm{a}}+}, d_{\bm{a}-} =n_{\bm{a}-}-n_{\bar{\bm{a}}-}, \quad\forall\bm{a}\in D,\] \[n_{\bm{a}} =n_{\bm{a}+}+n_{\bm{a}-}, \quad\delta n_{\bm{a}} =n_{\bm{a}+}-n_{\bm{a}-}, \quad\forall\bm{a}\in A_{0}.\]

Furthermore, \(\forall\bm{a}\in D\), let

\[t_{\bm{a}} =t_{\bm{a}+}+t_{\bm{a}-}, d_{\bm{a}} =d_{\bm{a}+}+d_{\bm{a}-},\] (D.10) \[\delta t_{\bm{a}} =t_{\bm{a}+}-t_{\bm{a}-}, \delta d_{\bm{a}} =d_{\bm{a}+}-d_{\bm{a}-}.\] (D.11)

Observe that these new variables constitute a basis transformation via

\[\{n_{\bm{a}}\}_{\bm{a}\in B} \equiv\{t_{\bm{a}+},d_{\bm{a}\pm}\}_{\bm{a}\in D}\cup\{n_{\bm{a}},\delta n_{\bm{a}}\}_{\bm{a}\in A_{0}}\] (D.12) \[\equiv\{t_{\bm{a}},\delta t_{\bm{a}},d_{\bm{a}},\delta d_{\bm{a}} \}_{\bm{a}\in D}\cup\{n_{\bm{a}},\delta n_{\bm{a}}\}_{\bm{a}\in A_{0}}.\]

We will call the last line as the "canonical basis". As a side note, comparing to the \(p=1\) derivation in Eq. (C.6), we have \(k=\delta d_{+-}\) and \(\tau_{+}-\tau_{-}=\delta n_{++}-\delta n_{--}\).

In what follows, we will convert all our expressions into the canonical basis. It is also helpful to denote the shorthand

\[t=\sum_{\bm{a}\in D}t_{\bm{a}},\qquad\text{and thus}\quad n-t=\sum_{\bm{a} \in A_{0}}n_{\bm{a}}.\] (D.13)

In this basis, we can rewrite (D.3) as

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}\sum_{\{n_{\bm{a}}\}_{\bm{a}\in A_{0} }}\binom{n-t}{\{n_{\bm{a}}\}}\prod_{\bm{a}\in A_{0}}\bigg{\{}Q^{n_{\bm{a}}}_{ \bm{a}}\sum_{\delta n_{\bm{a}}}\binom{n_{\bm{a}}}{n_{\bm{a}+}}\bigg{\}}\] (D.14) \[\quad\times\sum_{\{t_{\bm{a}}\}_{\bm{a}\in D}}\binom{t}{\{t_{\bm {a}}\}}\prod_{\bm{a}\in D}\iint_{d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}} }^{t_{\bm{a}}}\exp\Big{[}\mathcal{A}+i\lambda_{n}\mathcal{B}+\zeta\mathcal{C} \Big{]}.\]

where we have used the fact that \(Q_{\bm{a}\pm}=Q_{\bm{a}}\) does not depend on \(a_{\mathrm{m}}\) (here we also slightly abused notation allowing \(Q_{\bm{a}}\) to take \(\bm{a}\in A\) as argument). Here we also define, for any \(\bm{a}\in D\) and \(t_{\bm{a}}\in\mathbb{Z}_{\geq 0}\), the **little-sum** operator on functions of \((d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}})\) as

\[\iint_{d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}}}^{t_{\bm{a} }}(\cdots):=\sum_{t_{\bm{a}+},t_{\bm{a}-}}\binom{t_{\bm{a}}}{t_{\bm{a}+},t_{ \bm{a}-}}\sum_{d_{\bm{a}+}}\binom{t_{\bm{a}+}}{n_{\bm{a}+}}Q^{n_{\bm{a}+}}_{ \bm{a}}Q^{n_{\bm{a}+}}_{\bm{a}}\sum_{d_{\bm{a}-}}\binom{t_{\bm{a}-}}{n_{\bm{a }-}}Q^{n_{\bm{a}-}}_{\bm{a}}(\cdots).\] (D.15)

Now let us rewrite \(\mathcal{B}\) in the canonical basis, and we will show that it is purely a function of \(\{\delta d_{\bm{a}},\delta t_{\bm{a}}\}_{\bm{a}\in D}\cup\{\delta n_{\bm{c}} \}_{\bm{c}\in A_{0}}\). Observe that

\[n^{q-1}\mathcal{B}=\sum_{r=1}^{p}\gamma_{r}\Big{[}\big{(}B_{r}^{ +}\big{)}^{q}-\big{(}B_{r}^{-}\big{)}^{q}\Big{]},\quad\text{where}\quad B_{r} ^{\pm}=\sum_{\bm{a}\in B}a_{\pm r}^{\ast}a_{\mathrm{m}}n_{\bm{a}},\] (D.16)

and we have denoted \(a_{r}^{\ast}=a_{r}\cdots a_{p}\) for any \(1\leq r\leq p\). Note \(a_{r}^{\ast}-a_{-r}^{\ast}\neq 0\) only if \(\ell(\bm{a})\geq r\). Hence, we have

\[B_{r}^{+} =\sum_{\bm{a}\in A_{0}}a_{r}^{\ast}\delta n_{\bm{a}}+\sum_{\bm{a} \in D,\ell(\bm{a})\leq r-1}a_{r}^{\ast}\delta t_{\bm{a}}+\sum_{\bm{a}\in D,\ell (\bm{a})\geq r}a_{r}^{\ast}\delta d_{\bm{a}},\] (D.17) \[B_{r}^{-} =\sum_{\bm{a}\in A_{0}}a_{r}^{\ast}\delta n_{\bm{a}}+\sum_{\bm{a} \in D,\ell(\bm{a})\leq r-1}a_{r}^{\ast}\delta t_{\bm{a}}+\sum_{\bm{a}\in D,\ell (\bm{a})\geq r}a_{-r}^{\ast}\delta d_{\bm{a}}.\]

To reveal additional structures of \(where we have defined

\[L_{r} =\frac{1}{2}(B_{r}^{+}-B_{r}^{-})=\sum_{\bm{a}\in D,\ell(\bm{a}) \geq r}\frac{1}{2}(a_{r}^{*}-a_{-r}^{*})\delta d_{\bm{a}},\] (D.17) \[R_{r} =\frac{1}{2}(B_{r}^{+}+B_{r}^{-})=\sum_{\bm{a}\in A_{0}}a_{r}^{*} \delta n_{\bm{a}}+\sum_{\bm{a}\in D,\ell(\bm{a})\leq r-1}a_{r}^{*}\delta t_{ \bm{a}}+\sum_{\bm{a}\in D,\ell(\bm{a})\geq r}\frac{1}{2}(a_{r}^{*}+a_{-r}^{*}) \delta d_{\bm{a}}.\] (D.18)

We note here that \(\mathcal{B}\) consists of terms that have at least one power of the \(\{\delta d_{\bm{a}}\}_{\bm{a}\in D}\) variables through the dependence on \(L_{r}\), which is a fact that will become important later.

Proceeding in the same way for \(\mathcal{A}\) and \(\mathcal{C}\), we can also write them in the canonical basis. We note \(\mathcal{A}\) is a polynomial that has appeared in [26], where it can be shown to only depend on \(\{t_{\bm{a}},d_{\bm{a}}\}_{\bm{a}\in D}\cup\{n_{\bm{c}}\}_{\bm{c}\in A_{0}}\). In summary, we note the dependence of \(\mathcal{A},\mathcal{B},\mathcal{C}\) on the canonical basis variables is as follows:

\[\mathcal{A} =\mathcal{A}\Big{(}\{t_{\bm{a}}\}_{\bm{a}\in D},\{d_{\bm{a}}\}_{ \bm{a}\in D},\{n_{\bm{c}}\}_{\bm{c}\in A_{0}}\Big{)},\] (D.19) \[i\lambda_{n}\mathcal{B} =i\lambda_{n}\mathcal{B}\big{(}\{\delta d_{\bm{a}},\delta t_{\bm{ a}}\}_{\bm{a}\in D}\cup\{\delta n_{\bm{c}}\}_{\bm{c},\in A_{0}}\big{)},\] \[\mathcal{C} =\frac{1}{n}\Big{(}\sum_{\bm{a}\in A_{0}}\delta n_{\bm{a}}+\sum_{ \bm{a}\in D}\delta t_{\bm{a}}\Big{)}.\]

Operator shorthands for different parts of the sum.To streamline notations, we now introduce three operators \(\mathbb{T}\), \(\mathbb{S}\), \(\mathbb{U}\) as shorthands for different parts of the sum that appear in Eq. (D.13).

Let us define the \(\mathbb{T}_{n}^{t}\) operator acting on a function \(f(\{t_{\bm{a}}:\bm{a}\in D\})\) as

\[\mathbb{T}_{n}^{t}f=\frac{t!}{n^{t}}\binom{n}{t}\sum_{t_{\bm{a}}\geq 0,\forall \bm{a}\in D,\sum_{\bm{a}}t_{\bm{a}}=t}f(\{t_{\bm{a}}\}).\] (D.20)

Next, let us define the operator \(\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\) acting on any function \(g(\{d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}}\}_{\bm{a}\in D})\) as follows:

\[\mathbb{S}_{n}^{\{t_{\bm{a}}\}}g =\prod_{\bm{a}\in D}\bigg{\{}\frac{n^{t_{\bm{a}}}}{t_{\bm{a}}!} \int_{d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}}}^{t_{\bm{a}}}\bigg{\}}g\] \[=\prod_{\bm{a}\in D}\bigg{\{}\frac{(nQ_{\bm{a}})^{t_{\bm{a}}}}{t _{\bm{a}}!}\sum_{t_{\bm{a}+},t_{\bm{a}-}}\binom{t_{\bm{a}}}{t_{\bm{a}+},t_{\bm {a}-}}\sum_{d_{\bm{a}+}}\binom{t_{\bm{a}+}}{n_{\bm{a}+}}(+1)^{n_{\bm{a}+}}(-1) ^{n_{\bm{a}+}}\] \[\sum_{d_{\bm{a}-}}\binom{t_{\bm{a}-}}{n_{\bm{a}-}}(+1)^{n_{\bm{a} -}}(-1)^{n_{\bm{a}-}}\bigg{\}}g.\] (D.21)

Note \(S_{n}^{\{t_{\bm{a}}\}}1=\mathds{1}\{t_{\bm{a}}=0\ \forall\bm{a}\in D\}\).

Lastly, we define the \(\mathbb{U}_{n}^{t}\) operator acting on a function \(h(\{n_{\bm{c}}/n,\delta n_{\bm{c}}/\sqrt{n}:\bm{c}\in A_{0}\})\) as

\[\mathbb{U}_{n}^{t}h=\sum_{\{n_{\bm{a}}\}_{\bm{a}\in A_{0}}}\binom{n-t}{\{n_{ \bm{a}}\}}\prod_{\bm{a}\in A_{0}}\bigg{\{}Q_{\bm{a}}^{n_{\bm{a}}}\sum_{\delta n _{\bm{a}}}\binom{n_{\bm{a}}}{n_{\bm{a}+}}\bigg{\}}h.\] (D.22)

With these summing operators defined, we can rewrite (D.13) as

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{n}e_{n}(t),\] (D.23)

where

\[e_{n}(t)=\mathbb{U}_{n}^{t}\mathbb{T}_{n}^{t}\mathbb{S}_{n}^{\{t_{\bm{a}}\}}[ \exp(\mathcal{A}+i\lambda_{n}\mathcal{B}+\zeta\mathcal{C})].\] (D.24)

### Rescaling the summand for the \(n\to\infty\) limit

In the \(n\to\infty\) limit, we want to rescale the canonical basis variables \(\{t_{\bm{a}},\delta t_{\bm{a}},d_{\bm{a}},\delta d_{\bm{a}}\}_{\bm{a}\in D}\cup\{n_ {e},\delta n_{e}\}_{\bm{c}\in A_{0}}\) so that the summing operators \((\mathbb{U}_{n}^{t},\mathbb{T}_{n}^{t},\mathbb{S}_{n}^{\{t_{\bm{a}}\}})\) and the summand \(\mathcal{A}+i\lambda_{n}\mathcal{B}+\zeta\mathcal{C}\) converge to simplified forms. To this end, for all \(\bm{a},\bm{b}\in D\), \(\bm{c}\in A_{0}\), we will rescale by defining

\[t_{\bm{a}} =\tau_{\bm{a}}, \delta t_{\bm{a}}/n^{\rho_{\bm{a}}} =\delta\tau_{\bm{a}},\] (D.25) \[d_{\bm{b}}/n =\eta_{\bm{b}}, \delta d_{\bm{b}}/n^{1-\rho_{\bm{b}}} =\delta\eta_{\bm{b}}\] \[n_{\bm{c}}/n =\omega_{\bm{c}}, \delta n_{\bm{c}}/\sqrt{n} =\delta\omega_{\bm{c}},\]

where \((\tau_{\bm{a}},\eta_{\bm{b}},\omega_{\bm{c}},\delta\tau_{\bm{a}},\delta\eta_{ \bm{b}},\delta\omega_{\bm{c}})\) are new dimensionless variables that will be integrated over, and \(\rho_{\bm{a}}\) are scaling exponents which we will define shortly.

The goal of this subsection is to derive the summand in the \(n\to\infty\) limit. Specifically, we consider the summand broken into two parts, each as a polynomial of a distinct subset of the rescaled variables as follows:

\[\Gamma_{n}(\{t_{\bm{a}},\eta_{\bm{b}},\omega_{\bm{c}}\}) :=\mathcal{A}(\{t_{\bm{a}},\eta_{\bm{b}}n,\omega_{\bm{c}}n\}),\] (D.26) \[\Xi_{n}(\{\delta\tau_{\bm{a}},\delta\eta_{\bm{b}},\delta\omega_{ \bm{c}}\}) :=i\lambda_{n}\mathcal{B}(\{\delta\tau_{\bm{a}}n^{\rho_{\bm{a}}}, \delta\eta_{\bm{b}}n^{1-\rho_{\bm{b}}},\delta\omega_{\bm{c}}\sqrt{n}\})+ \zeta\mathcal{C}(\{\delta\tau_{\bm{a}}n^{\rho_{\bm{a}}},\delta\omega_{\bm{c}} \sqrt{n}\}),\] (D.27)

where the subscripts in the arguments implicitly iterate over \(\bm{a},\bm{b}\in D\) and \(\bm{c}\in A_{0}\). We think of \(\Gamma_{n}\) and \(\Xi_{n}\) as polynomials in their arguments, whose coefficients can depend on \(n\).

First, we know from [26, Lemma D.2] that with the rescaling specified in Eq. (D.25) and \(\gamma_{j},\beta_{j}=\Theta(1)\), we have

\[\lim_{n\to\infty}\Gamma_{n}(\{t_{\bm{a}},\eta_{\bm{b}},\omega_{\bm{c}}\}_{\bm{ a},\bm{b}\in D,e\in A_{0}})=\sum_{\bm{a}\in D}t_{\bm{a}}P_{\bm{a}}(\{\eta_{ \bm{b}}\}_{\bm{b}\prec\bm{a}},\{\omega_{\bm{c}}\}_{\bm{c}\in A_{0}})=:\Gamma.\] (D.28)

For the rest of this subsection, we derive the limit of \(\Xi_{n}=i\lambda_{n}\mathcal{B}+\zeta\mathcal{C}\).

Choosing the scaling exponents \(\rho_{\bm{a}}\).We want to choose the scaling exponents for \((\delta d_{\bm{a}},\delta t_{\bm{a}})\) variables, such that all the terms of \(\mathcal{B}\) except those are linear in \(\delta d_{\bm{a}}\) vanish in the \(n\to\infty\) limit. This would then imply the polynomial \(\Xi_{n}\) in the limit would only be at most linear in \(\delta\eta_{\bm{a}}\), which is very helpful later for evaluating certain integrals as we shall see in Eq. (D.61).

In the general \(p\)-step QAOA applied to the spiked \(q\)-tensor model, suppose the SNR parameter \(\lambda\) has a scaling as follows

\[\lambda_{n}=\Lambda n^{c(p,q)},\] (D.29)

where \(c(p,q)\) is to be determined. Also suppose that the appropriate scaling for \(\delta d_{\bm{a}}\) and \(\delta t_{\bm{a}}\) are

\[\delta d_{\bm{a}}\sim n^{1-\rho_{\ell(\bm{a})}},\quad\delta t_{\bm{a}}\sim n^ {\rho_{\ell(\bm{a})}},\] (D.30)

so that they only depend on the rank \(\ell(\bm{a})\) of \(\bm{a}\). Based on the explicit derivation at \(p=1\), we believe we only care about the terms in \(\mathcal{B}\) that look like \(\delta d_{\bm{a}}\delta n_{\bm{b}}^{q-1}\) when \(\ell(\bm{a})=1\) and \(\ell(\bm{b})=0\), or \(\delta d_{\bm{a}}\delta t_{\bm{b}}^{q-1}\) when \(\ell(\bm{a})=\ell\) and \(\ell(\bm{b})=\ell-1>0\). Also recall that \(\delta n_{\bm{b}}\sim\sqrt{n}\) for \(\bm{b}\in A_{0}\) from (D.25). For these terms in \(\mathcal{B}\), we have

\[\frac{\lambda_{n}}{n^{q-1}}\delta d_{\bm{a}}\delta n_{\bm{b}}^{q- 1} \sim n^{c(p,q)+1-\rho_{1}+(q-1)/2-(q-1)},\] (D.31) \[\frac{\lambda_{n}}{n^{q-1}}\delta d_{\bm{a}}\delta t_{\bm{b}}^{q- 1} \sim n^{c(p,q)+1-\rho_{\ell}+(q-1)\rho_{\ell-1}-(q-1)}.\] (D.32)

To ensure that all such terms in \(\mathcal{B}\) are order 1, we impose the condition that

\[c(p,q)+1-\rho_{\ell}+(q-1)(\rho_{\ell-1}-1)=0,\qquad\text{and}\qquad\rho_{0}= \frac{1}{2}.\] (D.33)

Solving this recurrence equation, we get that

\[\rho_{\ell}=1-\frac{(q-1)^{\ell}}{2}+c(p,q)\frac{(q-1)^{\ell}-1}{q-2}.\] (D.34)If we impose the additional condition that \(\rho_{p}=1\) (so that \(\delta t_{\bm{a}}/n=\Theta(1)\) to yield a nonvanishing overlap in \(\mathcal{C}\)), this implies that the SNR scaling needs to be

\[c(p,q)=\frac{q-2}{2}\frac{(q-1)^{p}}{(q-1)^{p}-1}=\frac{q-2}{2}+\frac{q-2}{2[(q -1)^{p}-1]}.\] (D.35)

Plugging this into Eq. (D.34), we get

\[\rho_{\ell}=\frac{1}{2}\frac{(q-1)^{p}+(q-1)^{\ell}-2}{(q-1)^{p}-1}.\] (D.36)

For the special case of \(q=2\), we have \(c(p,2)=\frac{1}{2p}\), and \(\rho_{\ell}=\frac{1}{2}+\frac{\ell}{2p}\).

Note \(1/2\leq\rho_{\ell}\leq 1\) since \(1\leq(q-1)^{\ell}\leq(q-1)^{p}\) and \(0\leq\ell\leq p\). This means \(\delta d_{\bm{a}}=O(n^{1/2})\) and \(\delta t_{\bm{a}}=\Omega(n^{1/2})\). Another property to note is that \(\rho_{\ell}\) is monotonically increasing with \(\ell\). In particular, \(\rho_{0}=1/2\) and \(\rho_{p}=1\).

The limiting expression for \(\Xi_{n}\).To get the limiting polynomial for \(\Xi_{n}=i\lambda_{n}\mathcal{B}+\zeta\mathcal{C}\), we substitute \(\delta d_{\bm{a}}=\delta\eta_{\bm{a}}n^{1-\rho_{\bm{a}}}\), \(\delta t_{\bm{a}}=\delta\tau_{\bm{a}}n^{\rho_{\bm{a}}}\), and \(\delta n_{\bm{e}}=\delta\omega_{\bm{e}}\sqrt{n}\), and take the \(n\to\infty\) limit. We first consider \(\mathcal{B}\) as written in Eq. (D.16). In terms of the rescaled dimensionless variables, we have

\[L_{r} =\sum_{\bm{a}\in D,\ell(\bm{a})\geq r}\frac{1}{2}(a_{r}^{*}-a_{-r }^{*})\delta\eta_{\bm{a}}n^{1-\rho_{\bm{a}}},\] \[R_{r} =\frac{1}{2}(B_{r}^{+}+B_{r}^{-})=\sum_{\bm{a}\in A_{0}}a_{r}^{*} \delta\omega_{\bm{a}}\sqrt{n}+\sum_{\bm{a}\in D,\ell(\bm{a})\leq r-1}a_{r}^{*} \delta\tau_{\bm{a}}n^{\rho_{\bm{a}}}+\sum_{\bm{a}\in D,\ell(\bm{a})\geq r} \frac{1}{2}(a_{r}^{*}+a_{-r}^{*})\delta\eta_{\bm{a}}n^{1-\rho_{\bm{a}}}.\]

With the exponents defined in Eq. (D.36), we note that \(L_{r}\) is dominated by \(\{\delta\eta_{\bm{a}}:\ell(\bm{a})=r\}\), and \(R_{r}\) is dominated by \(\{\delta\omega_{\bm{a}}:\bm{a}\in A_{0}\}\) when \(r=1\) and \(\{\delta\tau_{\bm{a}}:\ell(\bm{a})=r-1\}\) when \(r>1\). Thus, the appropriately rescaled \(L_{r}\) and \(R_{r}\) in the limit are

\[\tilde{L}_{r} :=\lim_{n\to\infty}\frac{L_{r}}{n^{1-\rho_{r}}}=\sum_{\bm{a}\in D,\ell(\bm{a})=r}\frac{1}{2}(a_{r}^{*}-a_{-r}^{*})\delta\eta_{\bm{a}}=\sum_{\bm {a}\in D,\ell(\bm{a})=r}a_{r}^{*}\delta\eta_{\bm{a}},\] (D.37) \[\tilde{R}_{r} :=\lim_{n\to\infty}\frac{R_{r}}{n^{\rho_{r-1}}}\simeq\begin{cases} \sum_{\bm{a}\in A_{0}}a_{r}^{*}\delta\omega_{\bm{a}},&r=1\\ \sum_{\bm{a}\in D,\ell(\bm{a})=r-1}a_{r}^{*}\delta\tau_{\bm{a}},&r>1\end{cases}.\] (D.38)

For \(\lambda_{n}=\Lambda n^{c(p,q)}\), we have

\[i\lambda_{n}\mathcal{B} =\frac{i\lambda_{n}}{n^{q-1}}\sum_{r=1}^{p}\gamma_{r}\sum_{k\text { odd}}2\binom{q}{k}L_{r}^{k}R_{r}^{q-k},\] \[\lim_{n\to\infty}i\lambda_{n}\mathcal{B} =\lim_{n\to\infty}\frac{i\Lambda n^{c(p,q)}}{n^{q-1}}\sum_{r=1}^{p} \gamma_{r}\sum_{k\text{ odd}}2\binom{q}{k}\tilde{L}_{r}^{k}\tilde{R}_{r}^{q-k}n^{k( 1-\rho_{r})+(q-k)\rho_{r-1}}.\]

One can verify that for any \(1\leq r\leq p\),

\[\lim_{n\to\infty}\frac{n^{c(p,q)}}{n^{q-1}}n^{k(1-\rho_{r})+(q-k)\rho_{r-1}}=n ^{(k-1)(1-\rho_{r}-\rho_{r-1})}=\begin{cases}1,&k=1\\ 1/n^{\epsilon}\text{ for some }\epsilon>0,&k\geq 3\end{cases}\] (D.39)

Hence, in the \(n\to\infty\) limit, only the \(k=1\) term survives, and

\[\lim_{n\to\infty}i\lambda\mathcal{B}=i\Lambda\sum_{r=1}^{p}2q\gamma_{r}\tilde{L }_{r}\tilde{R}_{r}^{q-1}.\] (D.40)

Similarly, consider

\[\mathcal{C}=\sum_{\bm{a}\in A_{0}}\frac{\delta n_{\bm{a}}}{n}+\sum_{\bm{a}\in D }\frac{\delta t_{\bm{a}}}{n}=\sum_{\bm{a}\in A_{0}}\frac{\delta\omega_{\bm{a} }}{\sqrt{n}}+\sum_{\bm{a}\in D}\frac{\delta\tau_{\bm{a}}n^{\rho_{\bm{a}}}}{n}.\] (D.41)

In the \(n\to\infty\) limit, the only terms that survive are \(\delta\tau_{\bm{a}}\) when \(\ell(\bm{a})=p\) for which \(\rho_{\bm{a}}=1\).

Combining the two equations above, we have

\[\lim_{n\to\infty}\Xi_{n}(\{\delta\tau_{\bm{a}},\delta\eta_{\bm{b}},\delta\omega_{ \bm{c}}\}_{\bm{a},\bm{b}\in D,c\in A_{0}})=i\Lambda\sum_{r=1}^{p}2q\gamma_{r} \tilde{L}_{r}\tilde{R}_{r}^{q-1}+\zeta\sum_{\bm{a}:\ell(\bm{a})=p}\delta\tau_{ \bm{a}}=:\Xi.\] (D.42)

### MGF at general \(p\) in the \(n\to\infty\) limit to show Claim 3.7

For succinctness, we denote the following vectors of (rescaled) variables

\[\bm{t}=(t_{\bm{a}})_{\bm{a}\in D},\qquad\bm{d}=(d_{\bm{b}}/n)_{\bm{b}\in D}, \qquad\bm{n}=(n_{\bm{e}}/n)_{\bm{e}\in A_{0}},\]

\[\bm{\delta t}=(\delta t_{\bm{a}}/n^{\rho_{a}})_{\bm{a}\in D},\qquad\bm{\delta d }=(\delta d_{\bm{b}}/n^{1-\rho_{\bm{b}}})_{\bm{b}\in D},\qquad\bm{\delta n}=( \delta n_{\bm{e}}/\sqrt{n})_{\bm{e}\in A_{0}}.\] (D.43)

We can then write the MGF as

\[\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\mathbb{E}_{\bm{Y}}\left[\,\langle\bm{\gamma },\bm{\beta}|\exp(\zeta\frac{1}{n}\sum_{i=1}^{n}Z_{i})|\bm{\gamma},\bm{\beta} \rangle\,\right]=\sum_{t=0}^{n}e_{n}(t),\] (D.44)

where

\[e_{n}(t)=\mathbb{T}_{n}^{t}\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\mathbb{U}_{n}^{t} \Big{[}\exp\big{(}\Gamma_{n}(\bm{t},\bm{d},\bm{n})+\Xi_{n}(\bm{\delta t},\bm{ \delta d},\bm{\delta n})\big{)}\Big{]}.\] (D.45)

Here, \(\Gamma_{n}\) and \(\Xi_{n}\) are polynomials of their arguments whose coefficients can depend on \(n\). Furthermore, \(\mathbb{T}_{n}^{t},\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\), and \(\mathbb{U}_{n}^{t}\) are summing operators defined in Eqs. (D.20), (D.21), (D.22) earlier.

We now introduce dummy variables \((\bm{\delta\tau},\bm{\eta},\bm{\delta\eta},\bm{\omega},\bm{\delta\omega})\) which will replace \((\bm{\delta t},\bm{d},\bm{\delta d},\bm{n},\bm{\delta n})\) via Dirac delta functions:

\[e_{n}(t)=\int_{\bm{\delta\tau},\bm{\eta},\bm{\delta\eta},\bm{ \omega},\bm{\delta\omega}}\mathbb{T}_{n}^{t}\mathbb{S}_{n}^{\{t_{\bm{a}}\}} \mathbb{U}_{n}^{t}\Big{[} \exp\big{(}\Gamma_{n}(\bm{t},\bm{\eta},\bm{\omega})+\Xi_{n}(\bm{ \delta\tau},\bm{\delta\eta},\bm{\delta\omega})\big{)}\] \[\delta(\bm{\delta t}-\bm{\delta\tau})\delta(\bm{d}-\bm{\eta}) \delta(\bm{\delta d}-\bm{\delta\eta})\delta(\bm{n}-\bm{\omega})\delta(\bm{ \delta n}-\bm{\delta\omega})\Big{]}\] \[=\int_{\bm{\delta\tau},\bm{\eta},\bm{\delta\eta},\bm{\omega},\bm {\delta\omega}}\int_{\bm{\delta\hat{\tau}},\bm{\hat{\eta}},\bm{\delta\hat{ \eta}},\bm{\hat{\delta\eta}},\bm{\hat{\omega}},\bm{\hat{\delta\omega}}} \mathbb{T}_{n}^{t}\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\mathbb{U}_{n}^{t}\Big{[} \exp\big{(}\Gamma_{n}(\bm{t},\bm{\eta},\bm{\omega})+\Xi_{n}(\bm{\delta\tau}, \bm{\delta\eta},\bm{\delta\omega})\big{)}\] \[e^{i\bm{\delta\hat{\tau}}\cdot(\bm{\delta t}-\bm{\delta\tau})+i \hat{\bm{\eta}}\cdot(\bm{d}-\bm{\eta})+i\bm{\delta\hat{\eta}}\cdot(\bm{ \delta d}-\bm{\delta\eta})+i\hat{\omega}\cdot(\bm{n}-\bm{\omega})+i\bm{\delta \hat{\omega}}\cdot(\bm{\delta n}-\bm{\delta\omega})}\Big{]}.\]

where in the last line we used the Fourier representation of delta functions and introduced dual variables \((\bm{\delta\hat{\tau}},\hat{\bm{\eta}},\bm{\delta\hat{\eta}},\bm{\hat{\omega} },\bm{\delta\hat{\omega}})\).

Note that \(\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\) is a sum over \((\bm{d},\bm{\delta d},\bm{\delta t})\) and \(\mathbb{U}_{n}^{t}\) is a sum over \((\bm{n},\bm{\delta n})\). We can apply them directly to the relevant exponentials since their dependence is now linear, but involves the dual variables.

First, let us evaluate the \(\mathbb{S}_{n}^{\{t_{\bm{a}}\}}\) sum, which is defined in Eq. (D.21) as a composition of many little-sums. We start by considering a single little-sum with parameters \((\kappa_{\rm I},\kappa_{\rm II},\kappa_{\rm III})\) of the following form:

\[F_{\bm{a}}(\kappa_{\rm I},\kappa_{\rm II},\kappa_{\rm III}) :=\iint_{d_{\bm{a}},\delta d_{\bm{a}},\delta t_{\bm{a}}}^{t_{ \bm{a}}}e^{\kappa_{\rm I}d_{\bm{a}}+\kappa_{\rm II}\delta d_{\bm{a}}+\kappa_{ \rm II}\delta t_{\bm{a}}}\] (D.46) \[=\sum_{t_{\bm{a}}+,t_{\bm{a}}-}\binom{t_{\bm{a}}}{t_{\bm{a}+},t_{ \bm{a}}-}\sum_{d_{\bm{a}+}}\binom{t_{\bm{a}+}}{n_{\bm{a}+}}Q_{\bm{a}}^{n_{\bm{ a}+}}Q_{\hat{\bm{a}}}^{n_{\bm{a}+}}\sum_{d_{\bm{a}-}}\binom{t_{\bm{a}-}}{n_{\bm{a}-}}Q_{\bm{a}}^{n_{ \bm{a}-}}Q_{\hat{\bm{a}}}^{n_{\bm{a}-}}e^{\kappa_{\rm I}d_{\bm{a}}+\kappa_{ \rm II}\delta d_{\bm{a}}+\kappa_{\rm III}\delta t_{\bm{a}}}.\]

This can be evaluated using \(Q_{\hat{\bm{a}}}=-Q_{\bm{a}}\) and the basic identity \(\sum_{d_{\bm{a}+}}\binom{t_{\bm{a}+}}{n_{\bm{a}+}}(+1)^{n_{\bm{a}+}}(-1)^{n_{ \bm{a}+}}e^{\kappa d_{\bm{a}+}}=[2\sinh\kappa]^{t_{\bm{a}+}}\). Applying this to the two inner sums in \(F_{\bm{a}}\), we get that

\[F_{\bm{a}}(\kappa_{\rm I},\kappa_{\rm II},\kappa_{\rm III}) =Q_{\bm{a}}^{t_{\bm{a}}}\sum_{t_{\bm{a}+},t_{\bm{a}-}}\binom{t_{ \bm{a}}}{t_{\bm{a}+},t_{\bm{a}-}}[2\sinh(\kappa_{\rm I}+\kappa_{\rm II})]^{t_{ \bm{a}+}}[2\sinh(\kappa_{\rm I}-\kappa_{\rm II})]^{t_{\bm{a}-}}e^{\kappa_{\rm II }\delta t_{\bm{a}}}\] \[=(2Q_{\bm{a}})^{t_{\bm{a}}}[\sinh(\kappa_{\rm I}+\kappa_{\rm II})e ^{\kappa_{\rm III}}+\sinh(\kappa_{\rm I}-\kappa_{\rm II})e^{-\kappa_{\rm III}}]^{t_ {\bm{a}}}\] \[=(4Q_{\bm{a}})^{t_{\bm{a}}}(\sinh\kappa_{\rm I}\cosh\kappa_{\rm II }\cosh\kappa_{\rm III}+\cosh\kappa_{\rm I}\sinh\kappa_{\rm II}\sinh\kappa_{\rm III })^{t_{\bm{a}}}.\] (D.47)

Returning to \(\mathbb{S}_{n}^{t}\), we get

\[\mathbb{S}_{n}^{\{t_{\bm{a}}\}}[e^{i\bm{\delta\hat{\tau}}\cdot\bm{ \delta d}t+i\bm{\delta\hat{\eta}}\cdot\bm{\delta d}t+i\bm{\hat{\eta}}\cdot\bm{d}}] =\prod_{\bm{a}\in D}\bigg{\{}\frac{n^{t_{\bm{a}}}}{t_{\bm{a}}!}\iint_{d_{\bm{a}}, \delta d_{\bm{a}},\delta t_{\bm{a}}}^{t_{\bm{a}}}\bigg{\}}e^{i\bm{\delta \hat{\tau}}\cdot\bm{\delta t}+i\bm{\delta\hat{\eta}}\cdot\bm{\delta d}t+i\bm{ \hat{\eta}}\cdot\bm{d}}\] \[=\prod_{\bm{a}\in D}\frac{(4nQ_{\bm{a}})^{t_{\bm{a}}}}{t_{\bm{a} }!}\Big{(}i\sin\frac{\hat{\eta}_{\bm{a}}}{n}\cos\frac{\delta\hat{\eta}_{\bm{ a}}}{n^{1-\rho_{\bm{a}}}}\cos\frac{\delta\hat{\tau}_{\bm{a}}}{n^{\rho_{\bm{a}}}}- \cos\frac{\hat{\eta}_{\bm{a}}}{n}\sin\frac{\delta\hat{\eta}_{\bm{a}}}{Next, for \(\mathbb{U}_{n}^{t}\), we have from the multinomial theorem that

\[\mathbb{U}_{n}^{t}[e^{i\bm{\omega}\cdot\bm{n}+i\bm{\delta}\bm{\hat{ \omega}}\cdot\bm{\delta}\bm{n}}] =\sum_{\{n_{\bm{a}}\}_{\bm{a}\in A_{0}}}\binom{n-t}{\{n_{\bm{a}} \}}\prod_{\bm{a}\in A_{0}}\bigg{\{}Q_{\bm{a}}^{n_{\bm{a}}}\sum_{\delta n_{\bm{ a}}}\binom{n_{\bm{a}}}{n_{\bm{a}+}}\bigg{\}}e^{i\bm{\hat{\omega}}\cdot\bm{n}+i \bm{\delta}\bm{\hat{\omega}}\cdot\bm{\delta}\bm{n}}\] \[=\Big{(}\sum_{\bm{a}\in A_{0}}2Q_{\bm{a}}e^{i\hat{\omega}_{\bm{a} }/n}\cos\frac{\delta\hat{\omega}_{\bm{a}}}{\sqrt{n}}\Big{)}^{n-t}.\] (D.49)

Take \(n\to\infty\) limit of \(e_{n}(t)\).We now take the \(n\to\infty\) limit while keeping \(t\) fixed, assuming \(\lambda_{n}=\Lambda n^{c(p,q)}\). Recall the fact from Appendix D.3 that \(0<\rho_{\bm{a}}<1\) when \(\ell(\bm{a})<p\) and \(\rho_{\bm{a}}=1\) when \(\ell(\bm{a})=p\). Then taking the \(n\to\infty\) limit of (D.48) yields

\[\lim_{n\to\infty}\mathbb{S}_{n}^{\{t_{\bm{a}}\}}[e^{i\bm{\delta}\cdot\bm{\hat{ \tau}}\cdot\bm{\delta}t+i\bm{\delta}\bm{\hat{\eta}}\cdot\bm{\delta}\bm{d}+i \bm{\hat{\eta}}\cdot\bm{d}}]=\prod_{\bm{a}\in D}\frac{(4Q_{\bm{a}})^{t_{\bm{a }}}}{t_{\bm{a}}!}[g_{\bm{a}}(\delta\hat{\tau}_{\bm{a}},\hat{\eta}_{\bm{a}}, \delta\hat{\eta}_{\bm{a}})]^{t_{\bm{a}}}\] (D.50)

where

\[g_{\bm{a}}(\delta\hat{\tau}_{\bm{a}},\hat{\eta}_{\bm{a}},\delta\hat{\eta}_{\bm {a}})=\begin{cases}i\hat{\eta}_{\bm{a}}-\delta\hat{\eta}_{\bm{a}}\hat{\tau}_{ \bm{a}},&\ell(\bm{a})<p\\ i\hat{\eta}_{\bm{a}}\cos\delta\hat{\eta}_{\bm{a}}-\delta\hat{\tau}_{\bm{a}}\sin \delta\hat{\eta}_{\bm{a}},&\ell(\bm{a})=p\end{cases}.\] (D.51)

Similarly, taking the \(n\to\infty\) limit of (D.49) gives

\[\lim_{n\to\infty}\mathbb{U}_{n}^{t}[e^{i\bm{\hat{\omega}}\cdot\bm{n}+i\bm{ \delta}\bm{\hat{\omega}}\cdot\bm{\delta}n}]=\exp\Big{[}\sum_{\bm{a}\in A_{0}}2 Q_{\bm{a}}(i\hat{\omega}_{\bm{a}}-\frac{1}{2}\delta\hat{\omega}_{\bm{a}}^{2}) \Big{]},\] (D.52)

where we used the fact that \(\sum_{\bm{a}\in A_{0}}2Q_{\bm{a}}=1\). We also note that for any sequence of functions \(\{f_{n}(\bm{t})\}_{n}\) that pointwise converges to \(f(\bm{t})\), we have

\[\lim_{n\to\infty}\mathbb{T}_{n}^{t}f_{n}(\bm{t})=\lim_{n\to\infty} \frac{t!}{n^{t}}\binom{n}{t}\sum_{t_{\bm{a}}\geq 0,\forall\bm{a}\in D, \sum_{\bm{a}}t_{\bm{a}}=t}f_{n}(\bm{t})=\sum_{t_{\bm{a}}\geq 0,\forall\bm{a} \in D,\sum_{\bm{a}}t_{\bm{a}}=t}f(\bm{t})=:\mathbb{T}^{t}f(\bm{t}).\] (D.53)

Plugging these back into \(e_{n}(t)\), we get in the limit

\[e(t) :=\lim_{n\to\infty}e_{n}(t)\] \[=\int_{\bm{\delta}\bm{\hat{\tau}},\bm{\eta},\bm{\delta}\bm{\eta}, \bm{\omega},\bm{\delta}\bm{\omega}}\int_{\bm{\delta}\bm{\hat{\tau}},\bm{ \hat{\eta}},\bm{\delta}\bm{\hat{\eta}},\bm{\delta}\bm{\hat{\eta}},\bm{\delta} \bm{\hat{\omega}}}\mathbb{T}^{\mathbb{T}}\Big{[}e^{\Gamma(\bm{t},\bm{\eta},\bm{ \omega})+\Xi(\bm{\delta}\bm{\hat{\tau}},\bm{\delta}\bm{\eta},\bm{\delta}\bm{ \omega})}e^{-i\bm{\delta}\bm{\hat{\tau}}\cdot\bm{\delta}\bm{\hat{\tau}}-i\bm{ \hat{\eta}}\cdot\bm{\eta}-i\bm{\delta}\bm{\hat{\eta}}\cdot\bm{\delta}\bm{\eta} -i\bm{\hat{\omega}}\cdot\bm{\omega}-i\bm{\delta}\bm{\hat{\omega}}\cdot\bm{ \delta}\bm{\omega}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad e^{i \bm{\hat{\omega}}\cdot(2\bm{Q})-\frac{1}{2}\bm{\delta}\bm{\hat{\omega}}\cdot(2 \bm{Q}\ \bm{\delta}\bm{\hat{\omega}})}\prod_{\bm{a}\in D}\frac{(4Q_{\bm{a}})^{t_{\bm{a}} }}{t_{\bm{a}}!}[g_{\bm{a}}(\delta\hat{\tau}_{\bm{a}},\hat{\eta}_{\bm{a}}, \delta\hat{\eta}_{\bm{a}})]^{t_{\bm{a}}}\Big{]}.\] (D.54)

where we denoted the vector \(\bm{Q}=(Q_{\bm{a}})_{\bm{a}\in A_{0}}\), and \((2\bm{Q}\ \bm{\delta}\bm{\omega})_{j}=2Q_{j}\delta\omega_{j}\) to mean element-wise product.

Sum over \(e(t)\) to get MGF.Now we perform the sum over \(t\) to get the moment-generating function of the overlap distribution, since (heuristically) \(\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{\infty}e(t)\). Note that

\[\sum_{t=0}^{\infty}\mathbb{T}^{t}f(\bm{t})=\sum_{t_{\bm{a}}\geq 0,\bm{a}\in D}f(\bm{t}).\] (D.55)

So in the \(n\to\infty\) limit, effectively we are summing over \(\{t_{\bm{a}}\}\) independently. We can also use the fact from [26, Lemma D.2] that \(\Gamma(\bm{t},\bm{\eta},\bm{\omega})\) is linear in \(\bm{t}\),

\[\Gamma(\bm{t},\bm{\eta},\bm{\omega})=\sum_{\bm{a}\in D}t_{\bm{a}}P_{\bm{a}}(\bm{ \eta},\bm{\omega}).\] (D.56)

Hence, we have

\[\sum_{t=0}^{\infty}e(t)=\int_{\bm{\delta}\bm{\hat{\tau}},\bm{\eta}, \bm{\delta}\bm{\eta},\bm{\omega},\bm{\delta}\bm{\omega}}\int_{\bm{\delta} \bm{\hat{\tau}},\bm{\hat{\eta}},\bm{\delta}\bm{\hat{\eta}},\bm{\delta}\bm{\hat{ \eta}},\bm{\delta}\bm{\hat{\omega}}}e^{-i\bm{\delta}\bm{\hat{\tau}}\cdot\bm{ \hat{\eta}}\cdot\bm{\eta}-i\bm{\delta}\bm{\hat{\eta}}\cdot\bm{\delta}\bm{\hat{ \eta}}}e^{i\bm{\hat{\omega}}\cdot(2\bm{Q}-\bm{\omega})}e^{-i\bm{\delta}\bm{ \hat{\omega}}\cdot\bm{\delta}\bm{\hat{\omega}}-\frac{1}{2}\bm{\delta}\bm{ \hat{\omega}}\cdot(2\bm{Q}\ \bm{\delta}\bm{\hat{\omega}})}\] \[\exp\bigg{[}\sum_{\bm{a}\in D}4Q_{\bm{a}}g_{\bm{a}}(\bm{\delta} \bm{\hat{\tau}},\bm{\hat{\eta}},\bm{\delta}\bm{\hat{\eta}})e^{P_{\bm{a}}(\bm{ \eta},\bm{\omega})}\bigg{]}e^{\Xi(\bm{\delta}\bm{\hat{\tau}},\bm{\delta} \bm{\eta},\bm{\delta}\bm{\omega})}.\] (D.57)The integrals over \((\hat{\bm{\omega}},\bm{\omega})\) yield Dirac delta functions that set each \(\omega_{\bm{a}}=2Q_{\bm{a}}\). The integral over \(\bm{\delta\hat{\omega}}\) yields a Gaussian density function for \(\bm{\delta\omega}\), each with mean 0 and variance \(2Q_{\bm{a}}\). So we can set \(\delta\omega_{\bm{a}}=G_{\bm{a}}\sim\mathcal{N}(0,2Q_{\bm{a}})\), and replace the integrals over \((\bm{\delta\hat{\omega}},\bm{\delta\omega})\) with an expectation over \(\bm{G}=(G_{\bm{a}})_{\bm{a}\in A_{0}}\). Our expression then simplifies to

\[\sum_{t=0}^{\infty}e(t) =\mathbb{E}_{\bm{G}}\int_{\bm{\delta\tau},\bm{\eta},\bm{\delta \eta}}\int_{\bm{\delta\hat{\tau}},\bm{\hat{\eta}},\bm{\delta\hat{\eta}}}e^{-i \bm{\delta\hat{\tau}}\bm{\cdot}\bm{\delta\tau}-i\hat{\bm{\eta}}\bm{\cdot}\bm{ \eta}-i\bm{\delta\hat{\eta}}\bm{\cdot}\bm{\delta\eta}}\exp\Big{[}\sum_{\bm{a} \in D}4Q_{\bm{a}}g_{\bm{a}}(\bm{\delta\hat{\tau}},\bm{\hat{\eta}},\bm{\delta \hat{\eta}})e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}\Big{]}e^{\Xi(\bm{\delta\tau}, \bm{\delta\eta},\bm{G})}\] \[=:\mathbb{E}_{\bm{G}}\int_{\bm{\delta\tau},\bm{\eta},\bm{\delta \hat{\eta}}}\int_{\bm{\delta\hat{\tau}},\bm{\hat{\eta}},\bm{\delta\hat{\eta}}} e^{S}.\] (D.58)

To do the remaining integrals, it is necessary to use additional structure of the polynomials \(P_{\bm{a}}\), \(g_{\bm{a}}\) and \(\Xi\). From [26], we know there is an ordering (\(\prec\)) of the elements of \(D\) such that the \(\bm{\eta}\) dependence in \(P_{\bm{a}}\) is only on \(\{\eta_{\bm{b}}:\bm{b}\prec\bm{a}\}\). Furthermore, from Appendix D.3, we know \(\Xi(\bm{\delta\tau},\bm{\delta\eta},\bm{\delta\omega})\) has a particular form:

\[\Xi(\bm{\delta\tau},\bm{\delta\eta},\bm{\delta\omega})=i\sum_{\bm{a}\in D} \delta\eta_{\bm{a}}R_{\bm{a}}(\bm{\delta\tau},\bm{\delta\omega})+\zeta\sum_{ \bm{b}:\ell(\bm{b})=p}\delta\tau_{\bm{b}}.\] (D.59)

We also know that the \(\bm{\delta\tau}\) dependence in \(R_{\bm{a}}\) is only on \(\{\delta\tau_{\bm{b}}:\ell(\bm{b})<\ell(\bm{a})\}\). More explicitly, from Eq. (D.38),

\[R_{\bm{a}}(\bm{\delta\tau},\bm{G})=2q\Lambda\gamma_{r}a_{r}^{*}X_{r}^{q-1}, \quad\text{ where }r=\ell(\bm{a})\text{ and }X_{r}=\begin{cases}\sum_{\bm{b}\in A_{0}}b_{r}^{*}G_{\bm{b}},&r=1\\ \sum_{\bm{b}\in D,\ell(\bm{b})=r-1}b_{r}^{*}\delta\tau_{\bm{b}},&r>1\end{cases}.\] (D.60)

Let us now write out the exponent \(S\) in (D.58) using the form of \(g_{\bm{a}}\) in (D.51) and \(\Xi\) in (D.59):

\[S =-i\bm{\delta\hat{\tau}}\bm{\cdot}\bm{\delta\tau}-i\bm{\hat{\eta} }\bm{\cdot}\bm{\eta}-i\bm{\delta\hat{\eta}}\bm{\cdot}\bm{\delta\eta}\] \[+\sum_{\bm{a}\in D:\ell(\bm{a})<p}4Q_{\bm{a}}(i\hat{\eta}_{\bm{a} }-\delta\hat{\eta}_{\bm{a}}\delta\hat{\tau}_{\bm{a}})e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}+\sum_{\bm{a}\in D:\ell(\bm{a})=p}4Q_{\bm{a}}(i\hat{\eta}_{\bm{a}} \cos\delta\hat{\eta}_{\bm{a}}-\delta\hat{\tau}_{\bm{a}}\sin\delta\hat{\eta}_{ \bm{a}})e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}\] \[+i\sum_{\bm{a}\in D}\delta\eta_{\bm{a}}R_{\bm{a}}(\bm{\delta\tau},\bm{G})+\zeta\sum_{\bm{b}:\ell(\bm{b})=p}\delta\tau_{\bm{b}}.\]

Regrouping terms, we have

\[S =\sum_{\bm{a}\in D:\ell(\bm{a})<p}i\hat{\eta}_{\bm{a}}(4Q_{\bm{a} }e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}-\eta_{\bm{a}})+\sum_{\bm{a}\in D:\ell(\bm{a })=p}i\hat{\eta}_{\bm{a}}(4Q_{\bm{a}}\cos\delta\hat{\eta}_{\bm{a}}e^{P_{\bm{a}}( \bm{\eta},2\bm{Q})}-\eta_{\bm{a}})\] \[+\sum_{\bm{a}\in D:\ell(\bm{a})<p}i\delta\hat{\tau}_{\bm{a}}(i4Q_ {\bm{a}}\delta\hat{\eta}_{\bm{a}}e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}-\delta \tau_{\bm{a}})+\sum_{\bm{a}\in D:\ell(\bm{a})=p}i\delta\hat{\tau}_{\bm{a}}(i4Q_ {\bm{a}}\sin\delta\hat{\eta}_{\bm{a}}e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}-\delta \tau_{\bm{a}})\] \[+\sum_{\bm{a}\in D}i\delta\eta_{\bm{a}}[R_{\bm{a}}(\bm{\delta\tau},\bm{G})-\delta\hat{\eta}_{\bm{a}}]+\zeta\sum_{\bm{b}\in D:\ell(\bm{b})=p} \delta\tau_{\bm{b}}.\] (D.61)

Integrating over \((\bm{\hat{\eta}},\bm{\eta})\) yields delta functions that assign \(\eta_{\bm{a}}=4Q_{\bm{a}}e^{P_{\bm{a}}(\bm{\eta},2\bm{Q})}\) when \(\ell(\bm{a})<p\), or \(\eta_{\bm{a}}=4Q_{\bm{a}}\cos\delta\hat{\eta}_{\bm{a}}e^{P_{\bm{a}}(\bm{\eta},2 \bm{Q})}\) when \(\ell(\bm{a})=p\). Note \(4Q_{\bm{a}}e^{P_{\bm{a}}}=2W_{\bm{a}}\) where \(W_{\bm{a}}\) is defined the same way for \(q\)-spin models as in [26], so we will use \(W_{\bm{a}}=2Q_{\bm{a}}e^{P_{\bm{a}}}\) in what follows. Then, integrating over \((\bm{\delta\eta},\bm{\delta\hat{\eta}})\) yields delta functions that assign \(\delta\hat{\eta}_{\bm{a}}=R_{\bm{a}}(\bm{\delta\tau},\bm{G})\). Note here the linear dependence in \(\delta\eta_{\bm{a}}\) in \(\Xi(\bm{\delta\tau},\bm{\delta\eta},\bm{\delta\omega})\), as in Eq. (D.59), is important for allowing us to evaluate the integrals. Finally, integrating over \((\bm{\delta\hat{\tau}},\bm{\delta\tau})\) yields delta functions that assign \(\delta\tau_{\bm{a}}=i4Q_{\bm{a}}R_{\bm{a}}e^{P_{\bm{a}}}=i2W_{\bm{a}}R_{\bm{a}}\) when \(\ell(\bm{a})<p\), and \(\delta\tau_{\bm{a}}=i4Q_{\bm{a}}\sin R_{\bm{a}}e^{P_{\bm{a}}}=i2W_{\bm{a}} \sin R_{\bm{a}}\) when \(\ell(\bm{a})=p\). Note that these assignments by delta functions are consistent if we perform the integrals according to the ascending order of the set \(D\), since \(P_{\bm{a}}\), \(R_{\bm{a}}\) only depend on the variables \(\{(\eta_{\bm{b}},\delta\tau_{\bm{b}}):\bm{b}\prec\bm{a}\}\), which would have already been assigned values from earlier integrals.

The MGF of the overlap distribution is then

\[\lim_{n\to\infty}\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{\infty}e(t)= \mathbb{E}_{\bm{G}}\left[\exp\Big{(}\zeta\sum_{\bm{b}\in D:\ell(\bm{b})=p}i2W_{ \bm{b}}\sin R_{\bm{b}}(\bm{G})\Big{)}\right].\] (D.62)In what follows, let us denote \(D_{r}=\{\bm{b}\in D:\ell(\bm{b})=r\}\) for \(1\leq r\leq p\). Also let \(\tilde{\gamma}_{r}=2q\Lambda\gamma_{r}\), and \(G=\sum_{\bm{a}\in A_{0}}a_{1}^{*}G_{\bm{a}}\). Note \(G\sim\mathcal{N}(0,1)\) since \(G_{\bm{a}}\sim\mathcal{N}(0,2Q_{\bm{a}})\) and \(\sum_{\bm{a}\in A_{0}}2Q_{\bm{a}}=1\). To get a sense of the MGF formula, observe that

\[\ell(\bm{a})=1 \implies R_{\bm{a}}=\tilde{\gamma}_{1}a_{1}^{*}G^{q-1},\] \[\ell(\bm{a})=2 \implies R_{\bm{a}}=\tilde{\gamma}_{2}a_{2}^{*}\Big{(}\sum_{\bm{b}\in D_{1 }}i2W_{\bm{b}}R_{\bm{b}}b_{2}^{*}\Big{)}^{q-1}=\tilde{\gamma}_{2}a_{2}^{*} \Big{(}\sum_{\bm{b}\in D_{1}}i2W_{\bm{b}}b_{1}\Big{)}^{q-1}[\tilde{\gamma}_{1} G^{q-1}]^{q-1}.\]

Note in the last line we used \(b_{1}^{*}b_{2}^{*}=b_{1}\). Doing this iteratively, we see that when \(\ell(\bm{a})=r\), we have

\[R_{\bm{a}}=a_{r}^{*}K_{r}G^{(q-1)^{r}},\qquad\text{where}\quad K_{r}=\tilde{ \gamma}_{r}\Big{(}\sum_{\bm{b}\in D_{r-1}}i2W_{\bm{b}}b_{r-1}\Big{)}^{q-1}K_{r -1}^{q-1}\] (D.63)

with initial condition \(K_{1}=\tilde{\gamma}_{1}\). Note that \(K_{r}\sim\Lambda^{[(q-1)^{r}-1]/(q-2)}\) when \(q>2\) and \(K_{r}\sim\Lambda^{r}\) when \(q=2\). Furthermore, using the fact that \(\sin(aX)=a\sin X\) when \(a\in\{\pm 1\}\), we have from Eq. (D.62) that

\[\mathcal{R}_{\text{QAOA}}\stackrel{{ d}}{{\longrightarrow}} \Big{(}\sum_{\bm{a}\in D_{p}}i2W_{\bm{a}}a_{p}^{*}\Big{)}\sin\big{[}K_{p}G^{( q-1)^{p}}\big{]},\] (D.64)

which is indeed of the form of the sine-Gaussian law in Claim 3.7.

We then note that the factors

\[\sum_{\bm{b}\in D_{r-1}}2W_{\bm{b}}b_{r-1},\qquad\sum_{\bm{b}\in D_{p}}2W_{ \bm{b}}b_{p}^{*}\] (D.65)

can be evaluated efficiently using the iterative procedure in [25] due to Theorem 3 in [26]. We give this procedure in the section that immediately follows. This concludes the derivation that shows Claim 3.7.

A self contained formula for \(a_{p}(\bm{\gamma},\bm{\beta})\) and \(b_{p}(\bm{\gamma},\bm{\beta})\)

In this section, we give a self-contained description of the formula for \((a_{p},b_{p})\), following Eq. (D.64). Let \(B\) be the set of \((2p+1)\)-bit strings indexed as \(B=\big{\{}(z_{1},z_{2},\ldots,z_{p},z_{0},z_{-p},\ldots,z_{-1}):z_{j}\in\{\pm 1 \}\big{\}}\). Define

\[f(\bm{z})=\frac{1}{2}\left\langle z_{1}|e^{i\beta_{1}X}|z_{2} \right\rangle\cdots\left\langle z_{p-1}|e^{i\beta_{p-1}X}|z_{p}\right\rangle \left\langle z_{p}|e^{i\beta_{p}X}|z_{0}\right\rangle\] \[\qquad\times\left\langle z_{0}|e^{-i\beta_{p}X}|z_{-p}\right\rangle \left\langle z_{-p}|e^{-i\beta_{p-1}X}|z_{-(p-1)}\right\rangle\cdots\left\langle z _{-2}|e^{-i\beta_{1}X}|z_{-1}\right\rangle\] (D.66)

where \(z_{i}\in\{+1,-1\}\), and \(\left\langle z_{1}|e^{i\beta X}|z_{2}\right\rangle=\cos\beta\) if \(z_{1}=z_{2}\), or \(i\sin(\beta)\) otherwise. Define matrices \(\bm{H}^{[m]}\in\mathbb{C}^{(2p+1)\times(2p+1)}\) for \(0\leq m\leq p\) as follows. For \(j,k\in\{1,\ldots,p,0,-p,\ldots,-1\}\), let \(H^{[0]}_{j,k}=\sum_{\bm{z}\in B}f(\bm{z})z_{j}z_{k}\), and

\[H^{[m]}_{j,k}=\sum_{\bm{z}\in B}f(\bm{z})z_{j}z_{k}\exp\left(-\frac{q}{2} \sum_{j^{\prime},k^{\prime}=-p}^{p}\big{(}H^{[m-1]}_{j^{\prime},k^{\prime}} \big{)}^{q-1}\gamma_{j^{\prime}}\gamma_{k^{\prime}}z_{j^{\prime}}z_{k^{\prime }}\right)\quad\text{for $1\leq m\leq p$},\] (D.67)

where we use the convention that \(\gamma_{-r}=-\gamma_{r}\) for \(1\leq r\leq p\), and \(\gamma_{0}=0\). Note these matrices first appeared in [25] in the context of assessing the performance of the QAOA on locally treelike Max-\(q\)-XORSAT problems and can be evaluated in \(O(p^{2}4^{p})\) time.

Once we have the matrix \(\bm{H}^{[p]}\), we compute for \(1\leq r\leq p\),

\[a_{r}=i\sum_{\bm{z}\in B}f(\bm{z})\frac{z_{r}z_{r+1}-z_{-r}z_{-(r+1)}}{2}\prod _{s=r+1}^{p}\frac{1+z_{s}z_{-s}}{2}\exp\left(-\frac{q}{2}\sum_{j,k=-p}^{p}H^{[p ]}_{j,k}\gamma_{j}\gamma_{k}z_{j}z_{k}\right).\] (D.68)

Finally, let \(b_{1}=2q\gamma_{1}\), and for \(r=2,3,\ldots,p\), compute

\[b_{r}=2q\gamma_{r}(a_{r-1}b_{r-1})^{q-1}.\] (D.69)Example formula at \(p=2\).As an example, we now describe the explicit formula at \(p=2\), which applies in the regime where \(\lambda_{n}=\Lambda n^{(q-2+1/q)/2}\) (note here \(\varepsilon_{p=2}=1/q\)). We have

\[b_{2} =2^{q}q^{q-1}e^{-2q(q-1)\gamma_{1}^{2}}\gamma_{1}^{-1}\gamma_{2} \sin^{q-1}(2\beta_{1}),\] \[a_{2} =-e^{-2q(\gamma_{1}^{2}+\gamma_{2}^{2}+2\operatorname{Re}[X] \gamma_{1}\gamma_{2})}\sin 2\beta_{2}\times\] \[\qquad\qquad\Big{[}\cos^{2}\beta_{1}+e^{8q\gamma_{1}\gamma_{2} \operatorname{Re}[X]}\sin^{2}\beta_{1}+e^{2q(\gamma_{1}^{2}+2\gamma_{1}\gamma _{2}\operatorname{Re}[X])}\sin 2\beta_{1}\sin(4q\gamma_{1}\gamma_{2}\operatorname{Im}[X]) \Big{]},\]

where \(X=(\cos 2\beta_{1}+ie^{-2q\gamma_{1}^{2}}\sin 2\beta_{1})^{q-1}\). Then the overlap \(\mathcal{R}\stackrel{{ d}}{{\longrightarrow}}a_{2}\sin(b_{2} \Lambda^{q}G^{(q-1)^{2}})\).

Although the above formula is complicated, we can understand the scaling with \(q\) by considering a simple choice of \(\gamma_{1}=\gamma_{2}=1/2\sqrt{q}\) and \(\beta_{1}=\beta_{2}=\pi/4\). Then the above simplifies to

\[b_{2} =e^{(1-q)/2}q^{q/2},\] \[a_{2} =e^{-1}\cosh[e^{(1-q)/2}\sin(\pi q/2)]-e^{-1/2}\sin[e^{(1-q)/2} \cos(\pi q/2)].\] (D.70)

## Appendix E Signal boosting with \(1\)-step QAOA

Consider a scenario where we have some prior information about the signal, in the form of a weak estimator that overlaps partially with the true signal. Our goal is to boost the overlap of this weak estimator. We study the SNR threshold of the \(1\)-step of QAOA and compare it to the \(1\)-step of power iteration. For QAOA, we encode the weak estimator into the initial state: rather than initializing with the uniform superposition across all bit-strings \(\ket{s}\), we bias a fraction of the qubits toward the signal. For power iteration, instead of starting from a uniform vector, we sample from a Bernoulli distribution biased toward the signal.

More precisely, for QAOA we consider the following initial state:

\[\ket{s_{\mathrm{biased}}}=\bigotimes_{j=1}^{n}\Big{(}\cos\theta_{j}\ket{u_{j} }+\sin\theta_{j}\ket{-u_{j}}\Big{)},\] (E.1)

where the \(\theta_{j}\) are drawn i.i.d. according to

\[\theta_{j}=\begin{cases}\pi/4,&\text{with probability }1-\frac{k}{n},\\ \pi/4-\delta,&\text{with probability }\frac{k}{n},\end{cases}\] (E.2)

and \(\delta>0\). As in Eq. (2.3), we prepare the \(1\)-step QAOA state as \(\ket{\gamma,\beta}_{\mathrm{biased}}=e^{-i\beta B}e^{-i\gamma C}\ket{s_{ \mathrm{biased}}}\). Note the spiked tensor model \(\bm{Y}\) is encoded in this state through \(C(\bm{\sigma})=\langle\bm{Y},\bm{\sigma}^{\otimes q}\rangle/n^{(q-2)/2}\). The following theorem concerns the SNR threshold for weak recovery and the distribution of overlap \(\mathcal{R}_{\mathrm{QAOA,biased}}=\hat{\bm{u}}^{\top}\bm{u}/n\) between a sample \(\hat{\bm{u}}\sim\ket{\gamma,\beta}_{\mathrm{biased}}\) and the signal \(\bm{u}\).

**Theorem 2** (Signal boosting with \(1\)-step QAOA).: _Consider the biased \(1\)-step QAOA state \(\ket{\gamma,\beta}_{\mathrm{biased}}\) as defined above. Fix \(\gamma>0\), \(\beta\in[0,2\pi]\), \(\delta\in[0,\pi/4]\), and let \(k=\Theta(n^{c})\) for \(1/2<c<1\). Suppose_

\[\lim_{n\to\infty}\lambda_{n}/n^{(1-c)(q-1)}=\Lambda.\] (E.3)

_Then, over the randomness of \(\bm{\theta},\bm{Y}\) and quantum measurement, the overlap \(\mathcal{R}_{\mathrm{QAOA,biased}}\) of 1-step QAOA converges in probability to_

\[\mathcal{R}_{\mathrm{QAOA,biased}}\stackrel{{ p}}{{\longrightarrow}}e^{-2q\gamma^{2}}\sin(2\beta)\sin(2q\Lambda\gamma\sin^{q-1}(2 \delta)).\] (E.4)

We give the proof Theorem 2 in Appendix E.1 that follows.

**Remark E.1**.: Theorem 2 considers an initial state with a fraction \(k/n\) of qubits biased toward the signal vector \(\bm{u}\), representing some side information. It shows that the SNR threshold is \(\Theta(n^{(1-c)(q-1)})\), which becomes lower with increasing side information \(k/n=n^{c-1}\). In particular, if \(k=\Theta(n^{3/4})\), the weak recovery threshold of 1-step QAOA improves to \(\Theta(n^{(q-1)/4})\), compared to the \(\Theta(n^{(q-1)/2})\) threshold given by Theorem 1 without any initial overlap between the state and planted signal.

Comparison with classical tensor power iteration.We compare the boosting produced by the \(1\)-step QAOA to that provided by \(1\)-step power iteration. Recall the \(1\)-step tensor power iteration estimator (2.1) is \(\hat{\bm{u}}_{1,\text{biased}}=\sqrt{\pi}\bm{Y}\big{[}\hat{\bm{u}}_{0,\text{ biased}}^{\otimes(q-1)}\big{]}/\big{\|}\bm{Y}\big{[}\hat{\bm{u}}_{0,\text{ biased}}^{\otimes(q-1)}\big{]}\big{\|}_{2}\), where in this case, analogously to Eq. (E.1), the initial vector \(\hat{\bm{u}}_{0,\text{biased}}\) has its entry \((\hat{\bm{u}}_{0,\text{biased}})_{j}\) sampled as

\[(\hat{\bm{u}}_{0,\text{biased}})_{j}\sim\begin{cases}u_{j}/\sqrt{n},&\text{ with probability }\frac{1}{2}\big{[}1+\frac{k}{n}\sin(2\delta)\big{]},\\ -u_{j}/\sqrt{n},&\text{with probability }\frac{1}{2}\big{[}1-\frac{k}{n}\sin(2 \delta)\big{]}.\end{cases}\] (E.5)

One can check that \(\sqrt{n}\hat{\bm{u}}_{0,\text{biased}}\sim|s_{\text{biased}})\) is a sample from the biased initial QAOA state, so that we are making a fair comparison with QAOA. In the following proposition, we show that the required SNR for the \(1\)-step power iteration estimator is also \(\Theta(n^{(1-c)(q-1)})\), and we provide the distribution of overlap \(\mathcal{R}_{\text{PI},\text{biased}}\equiv\bm{u}^{\top}\hat{\bm{u}}_{1,\text{ biased}}/n\) between the power iteration estimator \(\hat{\bm{u}}_{1,\text{biased}}\) and the signal \(\bm{u}\).

**Proposition E.2** (Signal boosting with \(1\)-step tensor power iteration).: _Assume that the rescaled signal-to-noise ratio has a limit \(\lim_{n\to\infty}\lambda_{n}/n^{(1-c)(q-1)}=\Lambda\). Then over the randomness of \(\bm{W}\) and initialization \(\hat{\bm{u}}_{0,\text{biased}}\), the overlap \(\mathcal{R}_{\text{PI},\text{biased}}\) of the power iteration estimator with the signal converges in probability to_

\[\mathcal{R}_{\text{PI},\text{biased}}\overset{p}{\longrightarrow}\sin[\arctan (\Lambda\sin^{q-1}(2\delta))].\] (E.6)

The proof of Proposition E.2 is contained in Appendix H.2. This shows yet again that the QAOA has the same asymptotic computational efficiency as power iteration. Nevertheless, in the \(\Lambda\ll 1\) regime, by choosing \(\gamma=1/2\sqrt{q}\) and \(\beta=\pi/8\), the QAOA achieves an overlap that is larger than power iteration by a factor \(\sqrt{q/e}\).

### Proof of Theorem 2

Without loss of generality, we assume that \(\bm{u}=\bm{1}\). Recall that the initial state is given by Eq. (E.1), which we can rewrite as

\[|s_{\text{biased}}\rangle=\sum_{\bm{z}}\prod_{j=1}^{n}(\cos\theta_{j})^{\delta _{z_{j}=1}}(\sin\theta_{j})^{\delta_{z_{j}=-1}}\left|\bm{z}\right\rangle,\] (E.7)

where \(\theta_{j}=\pi/4\) with probability \(1-k/n\), and \(\theta_{j}=\pi/4-\delta\) with probability \(k/n\).

To prove Theorem 2, it suffices to show that the moment-generating function (MGF) of the QAOA overlap converges to the MGF of a deterministic variable as follows:

\[\lim_{n\to\infty}\mathbb{E}_{\bm{\theta}}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]= \exp\left[\zeta e^{-2q\gamma^{2}}\sin(2\beta)\sin(2q\Lambda\gamma\sin(2\delta )^{q-1})\right]=:M(\zeta).\] (E.8)

The argument for the proof is the same as that for Theorem 1(b), except that we must prove analogous versions of Lemma C.1, C.2 and C.3, which become Lemma E.3, E.4 and E.5, respectively.

**Lemma E.3**.: _The expected moment-generating function at \(p=1\) for the overlap of the QAOA initialized with \(|s_{\text{biased}}\rangle\) is given by_

\[\mathbb{E}_{\bm{\theta}}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)]=\sum_{t=0}^{n} \binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1}}\left[\sinh(\zeta/n)\sin( 2\beta)\left(1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\right)\right]^{t}\cdot E_{ n,t},\] (E.9)

_where_

\[E_{n,t} =\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sin^{t}(2\pi\xi/(2t+1))\hat{Z}_{ n,t}(\xi),\] \[\hat{Z}_{n,t}(\xi) =\sum_{l=-t}^{t}e^{-2\pi i\xi l/(2t+1)}Z_{n,t}(l),\] \[Z_{n,t}(l) =\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=-n-t}\binom{n-t}{\tau_{ +},\tau_{-}}(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)^{\tau_{+}}(e^ {-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)^{\tau_{-}}\] \[\quad\times\left(1+\frac{k\sin(2\delta)}{n}\right)^{\tau_{+}} \left(1-\frac{k\sin(2\delta)}{n}\right)^{\tau_{-}}\] \[\quad\times e^{i\Lambda_{n}\gamma[((\tau_{+}-\tau_{-})+l)^{q}-(( \tau_{+}-\tau_{-})-l)^{q}]/n^{c(q-1)}}.\] (E.10)The proof of Lemma E.3 is deferred to Section E.2. Note the only difference from the unbiased case (Lemma C.1) is the presence of the two terms

\[\left(1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\right)^{t}\qquad\text{ and}\qquad\left(1+\frac{k\sin(2\delta)}{n}\right)^{\tau_{+}}\left(1-\frac{k\sin(2 \delta)}{n}\right)^{\tau_{-}},\]

and the rescaled power of \(n\) in the exponent.

We further define

\[\Lambda=\lim_{n\to\infty}\Lambda_{n},\] \[I_{n,t}=\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1}} \left[\sinh(\zeta/n)\sin(2\beta)\left(1-\frac{k}{n}+\frac{k\cos(2\delta)}{n} \right)\right]^{t}\cdot E_{n,t},\] (E.11) \[I_{t}=\frac{1}{t!}\Big{[}[\zeta e^{-2q\gamma^{2}}\sin(2\beta) \sin(2q\Lambda\gamma\sin^{q-1}(2\delta))]^{t}\Big{]},\]

where the definition of \(E_{n,t}\) is given in Eq. (E.10). Then it is easy to see that

\[\mathbb{E}_{\boldsymbol{\theta}}\,\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}I_{n,t},\quad M(\zeta)=\sum_{t=0}^{\infty}I_{t}.\]

As a consequence, we have

\[\Big{|}\,\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)]-M(\zeta)\Big{|}\leq\sum_{t =0}^{T}|I_{n,t}-I_{t}|+\Big{|}\sum_{t\geq T+1}I_{t}\Big{|}+\sum_{t=T+1}^{n}|I_ {n,t}|.\] (E.12)

The following lemma gives the limit of \(E_{n,t}\) for fixed \(t\) as \(n\to\infty\), which indicates that \(I_{t}\) is the limit of \(I_{n,t}\).

**Lemma E.4**.: _For any fixed integer \(t\), we have_

\[\lim_{n\to\infty}E_{n,t}=\sin^{t}(2q\Lambda\gamma\sin^{q-1}(2\delta))\equiv E _{t}.\] (E.13)

_As a consequence, we have_

\[\lim_{n\to\infty}I_{n,t}=I_{t}.\]

**Lemma E.5**.: _For any \(t\leq n\) and \(\zeta\leq n\), we have_

\[|I_{n,t}|\leq\frac{1}{t!}(18|\zeta|)^{t}(2t+1)e^{|\zeta|}\equiv s_{t},\] (E.14)

_where_

\[\sum_{t=0}^{\infty}s_{t}<\infty.\] (E.15)

The proof of Lemma E.4 and E.5 is deferred to Section E.3 and E.4, respectively. Now we assume that these two lemmas hold. By the fact that \(\sum_{t=0}^{\infty}I_{t}\) is finite and by Lemma E.5, for any \(\varepsilon>0\), there exists \(T=T_{\varepsilon}\) such that

\[\Big{|}\sum_{t\geq T_{\varepsilon+1}}I_{t}\Big{|}\leq\varepsilon/3,\quad\sum_ {t\geq T_{\varepsilon}+1}s_{t}\leq\varepsilon/3.\]

Furthermore, by Lemma E.4, there exists \(N=N_{\varepsilon}\) such that as long as \(n\geq N_{\varepsilon}\), we have

\[\sum_{t=0}^{T_{\varepsilon}}|I_{n,t}-I_{t}|\leq\varepsilon/3.\]

As a consequence, by Eq. (E.12), for any \(n\geq n_{\varepsilon}\) and \(\zeta\leq n\), we have

\[\Big{|}\,\mathbb{E}_{\boldsymbol{Y}}[M_{n}(\zeta)]-M(\zeta)\Big{|}\leq\sum_{t =0}^{T_{\varepsilon}}|I_{n,t}-I_{t}|+\Big{|}\sum_{t\geq T_{\varepsilon}+1}I_{ t}\Big{|}+\sum_{t=T_{\varepsilon}+1}^{\infty}s_{t}\leq\varepsilon.\] (E.16)

This proves Eq. (E.8) as desired, and hence finishes the proof of Theorem 2.

### Proof of Lemma e.3

With an added expectation over \(\bm{\theta}\), Eq. (B.4) still holds with a modified \(Q_{\bm{a}}\):

\[\mathbb{E}_{\bm{\theta}}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\sum_{\{n_{\bm{a}}\}}\binom{n}{\{n_{\bm{a}}\}}\prod_{\bm{a}\in B} Q_{\bm{a}}^{n_{\bm{a}}}\exp\Big{[}-\frac{1}{2n^{q-1}}\sum_{\bm{a}\in B^{q}}\Phi_{ \bm{a}}^{\underline{n}}\prod_{s=1}^{q}n_{\bm{a}_{s}}\] \[\quad+\frac{i\lambda_{n}}{n^{q-1}}\sum_{\bm{a}\in B^{q}}\Phi_{\bm {a}}\prod_{s=1}^{q}(\bm{a}_{s})_{m}n_{\bm{a}_{s}}+\frac{\zeta}{n}\sum_{\bm{v} \in B}v_{m}n_{\bm{v}}\Big{]},\] (E.17)

and

\[Q_{(a_{1},a_{\mathrm{m}},a_{2})}=f_{\beta,k,\delta}(a_{1},a_{ \mathrm{m}},a_{2})\] (E.18)

with \(f\) defined below:

\[f_{\beta,k,\delta}(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2}) =\begin{cases}\frac{1}{2}\big{(}1+\frac{k\sin(2\delta)}{n}\big{)} \cos^{2}\beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(1,1,1),\\ -\frac{1}{2}\big{(}1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\big{)}i\sin\beta\cos \beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(1,1,-1),\\ \frac{1}{2}\big{(}1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\big{)}i\sin\beta\cos \beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(1,-1,-1),\\ \frac{1}{2}\big{(}1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\big{)}i\sin\beta\cos \beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(-1,1,1),\\ \frac{1}{2}\big{(}1-\frac{k}{n}\big{)}\sin^{2}\beta,&\text{if }(z_{j}^{1},z_{j}^{ \mathrm{m}},z_{j}^{2})=(-1,1,-1),\\ \frac{1}{2}\big{(}1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\big{)}i\sin\beta\cos \beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(-1,-1,1),\\ \frac{1}{2}\big{(}1+\frac{k\sin(2\delta)}{n}\big{)}\sin^{2}\beta,&\text{if }(z_{j}^{1},z_{j}^{\mathrm{m}},z_{j}^{2})=(-1,-1,-1).\end{cases}\] (E.19)

This proof follows very closely that of Theorem 1(b) in Appendix C. From the change of variables in Eq. (C.10) to the breaking up in Eq. (C.15), the same expression still hold, except that we redefine \(\Lambda_{n}=\lambda_{n}/n^{(1-c)(q-1)}\), which amounts to the power of \(n\) in the exponential changing: when compared to Eq. (C.15):

\[\mathbb{E}_{\bm{\theta}}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom {n-t}{\tau_{+},\tau_{-}}\] \[\qquad\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}} Q_{---}^{n_{---}}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{+++}}Q_{-+-}^{n_{+-}}e^{( \zeta/n)(t_{+}-t_{-}+\Delta_{+}-\Delta_{-})}\] \[\qquad\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++-}}Q_{-++}^ {n_{+++}}\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{+--}^{n_{--+}}\] \[\qquad e^{i\Lambda_{n}\gamma[(d_{+}-d_{-}+\tau_{+}-\tau_{-})^{q}-( (\tau_{+}-\tau_{-})-(d_{+}+d_{-}))^{q}]/n^{c(q-1)}}.\] (E.20)

However, it is not true anymore that \(Q_{+++}=Q_{+-+}\) and \(Q_{---}=Q_{-+-}\) in general. We use the identity in Eq. (C.16) to write

\[\mathbb{E}_{\bm{\theta}}\,\mathbb{E}_{\bm{Y}}[M_{n}(\zeta)] =\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom {n-t}{\tau_{+},\tau_{-}}\] \[\qquad\frac{1}{2^{n-t}}(2Q_{+++}e^{\zeta/n}+2Q_{---}e^{-\zeta/n}) ^{\tau_{+}}(2Q_{+-+-}e^{-\zeta/n}+2Q_{-+-}e^{\zeta/n})^{\tau_{-}}e^{(\zeta/n)(t_ {+}-t_{-})}\] \[\qquad\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++-}}Q_{-++}^ {n_{+++}}\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{+--}^{n_{--+}}\] \[\qquad e^{i\Lambda_{n}\gamma[(d_{+}-d_{-}+\tau_{+}-\tau_{-})^{q}-(( \tau_{+}-\tau_{-})-(d_{+}-d_{-}))^{q}]/n^{c(q-1)}}.\]Then we redefine

\[Z_{n,t}(l)= \frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau_{+},\tau _{-}}(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)^{\tau_{+}}(e^{-\zeta/n }\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)^{\tau_{-}}\] \[\left(1+\frac{k\sin(2\delta)}{n}\right)^{\tau_{+}}\left(1-\frac{k \sin(2\delta)}{n}\right)^{\tau_{-}}e^{i\Lambda_{n}\gamma[((\tau_{+}-\tau_{-}) +l)^{q}-((\tau_{+}-\tau_{-})-l)^{q}]/n^{\epsilon(q-1)}}\] (E.21)

and, analogously to Eq. (C.19) write

\[\mathbb{E}_{\boldsymbol{\theta}}\,\mathbb{E}_{\boldsymbol{Y}}[M _{n}(\zeta)]= \sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{d_{+}}\binom{t_{+}}{n_{++-}} Q_{++-+}^{n_{+++}}Q_{-++}^{n_{-++}}\] \[\times\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{++--}^{n_{+--}}Q_{--+}^ {n_{--+}}e^{(\zeta/n)(t_{+}-t_{-})}Z_{n,t}(d_{+}-d_{-}).\]

Using the discrete Fourier transform, we have

\[\mathbb{E}_{\boldsymbol{\theta}}\,\mathbb{E}_{\boldsymbol{Y}}[M _{n}(\zeta)]= \sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{d_{+}}\binom{t_{+}}{n_{++-}} Q_{++-+}^{n_{+++}}\] \[\times\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{++--}^{n_{++-}}e^{( \zeta/n)(t_{+}-t_{-})}\frac{1}{2t+1}\sum_{\xi=-t}^{t}e^{2\pi i\xi(d_{+}-d_{-}) /(2t+1)}\hat{Z}_{n,t}(\xi)\] \[=\sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}e^{(\zeta/n)(t_{+}-t_{-})}\] \[\times(-1)^{t_{-}}\cdot\frac{1}{2t+1}\sum_{\xi=-t}^{t}\left(2iQ_ {++-}\sin(2\pi\xi/(2t+1))\right)^{t}\!\hat{Z}_{n,t}(\xi)\] (E.23)

since the same relations between \(Q_{++-},Q_{-++},Q_{+--},Q_{--+}\) hold. Finally,

\[\mathbb{E}_{\boldsymbol{\theta}}\,\mathbb{E}_{\boldsymbol{Y}}[M _{n}(\zeta)]= \sum_{t=0}^{n}\binom{n}{t}e^{-\gamma^{2}[n^{q}-(n-2t)^{q}]/n^{q-1 }}(\sinh(\zeta/n)\sin(2\beta))^{t}\left(1-\frac{k}{n}+\frac{k\cos(2\delta)}{n} \right)^{t}\] \[\times\frac{1}{2t+1}\sum_{\xi=-t}^{t}\Big{(}\sin(2\pi\xi/(2t+1)) \Big{)}^{t}\hat{Z}_{n,t}(\xi),\] (E.24)

which is analogous to Eq. (C.24). This completes the proof of Lemma E.3.

### Proof of Lemma e.4

We first look at the limit of \(Z_{n,t}(l)\) for fixed integer \(-t\leq l\leq t\). Letting \(T_{n}=(e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta)\), \(U_{n}=(e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta)\) and \(\epsilon=k\sin(2\delta)/n\), we can write

\[Z_{n,t}(l)=\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau_{+}, \tau_{-}}T_{n}^{\tau_{+}}U_{n}^{\tau_{-}}\left(1+\epsilon\right)^{\tau_{+}} \left(1-\epsilon\right)^{\tau_{-}}e^{i\Lambda_{n}\gamma[((\tau_{+}-\tau_{-})+l )^{q}-((\tau_{+}-\tau_{-})-l)^{q}]/n^{\epsilon(q-1)}}.\] (E.25)

We let \(G_{n}=(\tau_{+}-\tau_{-}+\epsilon n-t(\epsilon-1))/\sqrt{n}\) so that

\[Z_{n,t}(l)=\mathbb{E}_{G_{n}} \Bigg{[}T_{n}^{(\sqrt{n}G_{n}+(\epsilon+1)n-t(\epsilon+1))/2}U_{n }^{(-\sqrt{n}G_{n}-(\epsilon-1)n+t(\epsilon+1))/2}\] \[\times e^{\frac{-i\Lambda_{n}\gamma}{n^{\epsilon(q-1)}}[(\sqrt{n }G_{n}+\epsilon n-t(\epsilon+1)+l)^{q}-(\sqrt{n}G_{n}+\epsilon n-t(\epsilon+ 1)-l)^{q}]}\Bigg{]},\] (E.26)where \(\tau_{+}\sim\mathrm{Binom}(n-t,(1+\epsilon)/2)\) so that \(G_{n}\to G\sim\mathcal{N}(0,1)\) by the central limit theorem since \(\mathbb{E}_{\tau_{+}}[\tau_{+}-\tau_{-}]=\epsilon n-t(\epsilon+1)\) and \(\mathrm{Var}_{\tau_{+}}[\tau_{+}-\tau_{-}]=(n-t)(1-\epsilon^{2})\).

Recall that \(\epsilon=\sin(2\delta)n^{c-1}\) where \(1/2<c<1\). It follows that \(\lim_{n\to\infty}T_{n}^{((\epsilon+1)n-t(\epsilon+1))/2}=\lim_{n\to\infty}U_{n }^{(-(\epsilon-1)n+t(\epsilon+1))/2}=1\) as well as \(\lim_{n\to\infty}T_{n}^{\sqrt{n}/2}=\lim_{n\to\infty}U_{n}^{\sqrt{n}/2}=1\). Hence, for any fixed \(-t\leq l\leq t\), it follows that

\[\frac{1}{n^{c(q-1)}}\Big{[} (\sqrt{n}G_{n}+\epsilon n-t(\epsilon+1)+l)^{q}-(\sqrt{n}G_{n}+ \epsilon n-t(\epsilon+1)-l)^{q}\Big{]}\] \[= \frac{1}{n^{c(q-1)}}\Big{[} (\sqrt{n}G_{n}+n^{c}\sin(2\delta)-t(n^{c-1}\sin(2\delta)+1)+l)^{q}\] \[-(\sqrt{n}G_{n}+n^{c}\sin(2\delta)-t(n^{c-1}\sin(2\delta)+1)-l)^{ q}\Big{]}\] \[\to 2ql\sin^{q-1}(2\delta).\] (E.27)

With this, we can conclude

\[\lim_{n\to\infty}Z_{n,t}(l)=e^{iq\Lambda\gamma 2l\sin^{q-1}(2\delta)}.\] (E.28)

Hence

\[\lim_{n\to\infty}E_{n,t} =\frac{1}{2t+1}\sum_{\xi=-t}^{t}\sin(2\pi\xi/(2t+1))^{t}\Big{(} \sum_{l=-t}^{t}e^{-2\pi i\xi l/(2t+1)}e^{i\Lambda\gamma 2l\sin^{q-1}(2\delta)} \Big{)}\] \[=\sin^{t}(2q\Lambda\gamma\sin^{q-1}(2\delta)),\] (E.29)

where we used Lemma C.4 with \(X=1\) with probability \(1\). This completes the proof of Lemma E.4.

### Proof of Lemma e.5

We first bound \(Z_{n,t}(k)\):

\[|Z_{n,t}(l)| \leq\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau _{+},\tau_{-}}\left|e^{\zeta/n}\cos^{2}\beta+e^{-\zeta/n}\sin^{2}\beta\right|^ {\tau_{+}}\left|e^{-\zeta/n}\cos^{2}\beta+e^{\zeta/n}\sin^{2}\beta\right|^{ \tau_{-}}\] \[\quad\times\left(1-\frac{k\sin(2\delta)}{n}\right)^{\tau_{+}} \left(1+\frac{k\sin(2\delta)}{n}\right)^{\tau_{-}}\left|e^{i\Lambda_{n}\gamma [((\tau_{+}-\tau_{-})+l)^{q}-((\tau_{+}-\tau_{-})-l)^{q}]/n^{c(q-1)}}\right|\] \[\leq\frac{1}{2^{n-t}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau _{+},\tau_{-}}\left(1-\frac{k\sin(2\delta)}{n}\right)^{\tau_{+}}\left(1+ \frac{k\sin(2\delta)}{n}\right)^{\tau_{-}}e^{\tau_{+}|\zeta|/n}e^{\tau_{-}| \zeta|/n}\cdot 1\] \[\quad=\;e^{(n-t)|\zeta|/n}\cdot 1\] \[\leq\;e^{|\zeta|}.\] (E.30)

The rest of the proof is exactly as the proof of Lemma C.3, except that \(I_{n,t}\) involves the following extra factor which we can bound:

\[\left|\left(1-\frac{k}{n}+\frac{k\cos(2\delta)}{n}\right)\right|\leq 3.\] (E.31)

So the end bound on \(|I_{n,t}|\) ends up with a different constant factor:

\[|I_{n,t}|\leq\frac{1}{t!}(18|\zeta|)^{t}(2t+1)e^{|\zeta|}.\] (E.32)

This finishes the proof of Lemma E.5.

## Appendix F Finite \(n\) calculation for \(1\)-step QAOA on the spiked matrix (\(q=2\))

In this appendix, we calculate the average squared overlap outputted by the QAOA at any finite problem dimension \(n\) and obtain the formula we reported in Eq. (4.1). As done in Appendix B, we first take \(\bm{u}=\bm{1}\) to be the all-one vector without loss of generality. The cost function is

\[C(\bm{z})=\sum_{j,k=1}^{n}Y_{j,k}z_{j}z_{k},\qquad\text{where}\qquad Y_{j,k}= \frac{\lambda_{n}}{n}+\frac{1}{\sqrt{n}}W_{j,k}.\] (F.1)Here \(W_{j,k}\sim\mathcal{N}(0,1)\).

The QAOA state at level \(p=1\) with this cost function is

\[\ket{\bm{\gamma},\bm{\beta}}=e^{-i\beta B}e^{-i\gamma C}\ket{s}.\] (F.2)

We are interested in the overlap of the QAOA output with the hidden signal \(\bm{u}=\bm{1}\). Following the same method as in Appendix B, we can write the disorder-averaged overlap as

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{\gamma,\beta }]=\sum_{\{\bm{n}_{\bm{a}}\}}\binom{n}{\{n_{\bm{a}}\}}\prod_{\bm{a}\in B}Q_{ \bm{a}}^{n_{\bm{a}}}e^{-\frac{1}{2n}\sum_{\bm{a},\bm{b}\in B}\Phi_{\bm{a}\bm{b} }^{2}\bm{n}_{\bm{a}}n_{\bm{b}}+\frac{i\Lambda_{\bm{a}}}{n}\sum_{\bm{a},\bm{b} \in B}\Phi_{\bm{a}\bm{b}}\alpha_{\bm{b}}m_{\bm{a}}n_{\bm{b}}}\Big{(}\frac{1}{ n}\sum_{\bm{v}\in B}v_{\text{m}}n_{\bm{v}}\Big{)}^{2},\] (F.3)

where

\[B =\big{\{}(a_{1},a_{\text{m}},a_{2}):a_{j}\in\{\pm 1\}\big{\}},\] (F.4) \[Q_{\bm{a}} =\frac{1}{2}\left\langle a_{1}\right|e^{i\beta X}\ket{1}\bra{1}e ^{-i\beta X}\ket{a_{2}},\] (F.5) \[\text{and}\qquad\Phi_{\bm{a}\bm{b}} =\gamma(a_{1}b_{1}-a_{2}b_{2}).\] (F.6)

We can calculate \(\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{\gamma,\beta }]\) explicitly with a careful organization of the sum. To this end, similar to what we did in Section C.2.1, we perform a change of variables given by

\[t_{+} =n_{++-}+n_{-++}, t_{-} =n_{+--}+n_{--+},\] (F.7) \[d_{+} =n_{++-}-n_{-++}, d_{-} =n_{+--}-n_{--+},\] \[\tau_{+} =n_{+++}+n_{---}, \tau_{-} =n_{++-}+n_{-+-},\] \[\Delta_{+} =n_{+++}-n_{---}, \Delta_{-} =n_{+-+}-n_{-+-}.\]

Observe that these 8 variables completely determine \(\{n_{\bm{a}}:\bm{a}\in B\}\). Furthermore, let

\[t=t_{+}+t_{-},\qquad\text{and thus}\qquad n-t=\tau_{+}+\tau_{-}.\] (F.8)

Using the identity \(a_{1}b_{1}-a_{2}b_{2}=[(a_{1}+a_{2})(b_{1}-b_{2})+(a_{1}-a_{2})(b_{1}+b_{2})]/2\), we can show that

\[\sum_{\bm{a},\bm{b}\in B}\Phi_{\bm{a}\bm{b}}^{2}n_{\bm{a}}n_{\bm{ b}} =8\gamma^{2}t(n-t),\] (F.9) \[\sum_{\bm{a},\bm{b}\in B}\Phi_{\bm{a}\bm{b}}a_{\text{m}}b_{\text{ m}}n_{\bm{a}}n_{\bm{b}} =4\gamma(d_{+}-d_{-})(\tau_{+}-\tau_{-}),\] (F.10) \[\sum_{\bm{v}\in B}v_{\text{m}}n_{\bm{v}} =t_{+}-t_{-}+\Delta_{+}-\Delta_{-}.\] (F.11)

Plugging these into (F.3) and breaking up the sum yield

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{ \gamma,\beta}] =\frac{1}{n^{2}}\sum_{t=0}^{n}\binom{n}{t}e^{-4\gamma^{2}t(n-t)/n} \sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n -t}{\tau_{+},\tau_{-}}\] (F.12) \[\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}}Q_{--+ -}^{n_{--}}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{++-}}Q_{++-}^{n_{+-+}}(t_{+}-t _{-}+\Delta_{+}-\Delta_{-})^{2}\] \[\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{++-}}Q_{+++}^{n_{-++ }}\sum_{d_{-}}\binom{t_{-}}{n_{+--}}Q_{+--}^{n_{+-+}}Q_{--+}^{n_{--+}}e^{i \Lambda(d_{+}-d_{-})(\tau_{+}-\tau_{-})},\]

where we've denoted \(\Lambda=4\lambda\gamma/n\) as shorthand. Note we need to perform these sums in a carefully chosen order in order to get a closed-form answer at the end.

We start with the last line, where we sum over \(d_{\pm}\). We can use the fact that \(Q_{+x-}=-Q_{-y+}=-\frac{i}{2}\sin\beta\cos\beta\) for any \(x,y\in\{\pm\}\). Then, for example we have

\[\sum_{d_{+}}\binom{t_{+}}{n_{++-}}Q_{++-}^{n_{+++}}e^{i\Lambda d_{+}(\tau_{+}- \tau_{-})}=\Big{(}2iQ_{++-}\sin[\Lambda(\tau_{+}-\tau_{-})]\Big{)}^{t_{+}}.\] (F.13)After doing the same thing for the sum over \(d_{-}\), we get

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{ \gamma,\beta}]=\frac{1}{n^{2}}\sum_{t=0}^{n}\binom{n}{t}e^{-4\gamma^{2}t(n-t)/n} \sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n- t}{\tau_{+},\tau_{-}}\] \[\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}}Q_{---} ^{n_{---}}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{+-+}}Q_{++-}^{n_{+-+}}Q_{-+-}^{ n_{+-+}}(t_{+}-t_{-}+\Delta_{+}-\Delta_{-})^{2}\] \[\Big{(}2iQ_{++-}\sin[\Lambda(\tau_{+}-\tau_{-})]\Big{)}^{t}(+1)^{ t_{+}}(-1)^{t_{-}}.\]

Next, consider the sums over \(\{(t_{+},t_{-}):t_{+}+t_{-}=t\}\). We can use the following identity

\[\sum_{r+s=t}\binom{t}{r,s}(+1)^{r}(-1)^{s}(r-s)^{k}=\begin{cases}\delta_{t=0},&\text{if }k=0,\\ 2\delta_{t=1},&\text{if }k=1,\\ 8\delta_{t=2},&\text{if }k=2.\end{cases}\] (F.14)

Collecting the relevant terms and applying this identity yield

\[\sum_{t_{+}+t_{-}=t}\binom{t}{t_{+},t_{-}}(t_{+}-t_{-}+\Delta_{+} -\Delta_{-})^{2}(+1)^{t_{+}}(-1)^{t_{-}}\\ =\Big{[}8\delta_{t=2}+4(\Delta_{+}-\Delta_{-})\delta_{t=1}+(\Delta _{+}-\Delta_{-})^{2}\delta_{t=0}\Big{]}.\] (F.15)

So we have

\[\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_ {\gamma,\beta}]=\frac{1}{n^{2}}\sum_{t=0}^{n}\binom{n}{t}e^{-4\gamma^{2}t(n-t) /n}\sum_{\tau_{+}+\tau_{-}=n-t}\binom{n-t}{\tau_{+},\tau_{-}}\Big{(}2iQ_{++-} \sin[\Lambda(\tau_{+}-\tau_{-})]\Big{)}^{t}\\ \sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{+++}}Q_{---} ^{n_{---}}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{++-}}Q_{+-+}^{n_{+-+}}Q_{-+-}^{ n_{-+}}\\ \Big{[}8\delta_{t=2}+4(\Delta_{+}-\Delta_{-})\delta_{t=1}+(\Delta_{+} -\Delta_{-})^{2}\delta_{t=0}\Big{]}.\] (F.16)

Note the Kronecker deltas will collapse the sum over \(t\), so it remains to evaluate the sums over \(\Delta_{\pm}\) and \(\tau_{\pm}\). To perform the sum over \(\Delta_{\pm}\), note that \(Q_{++x}=\frac{1}{2}\cos^{2}\beta\) and \(Q_{-y-}=\frac{1}{2}\sin^{2}\beta\), for any \(x,y\in\{\pm\}\). Thus, we can use the following identity

\[2^{\tau_{+}}\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{++++}^{n_{+++}}Q_{--- }^{n_{---}}(\Delta_{+})^{k}=\begin{cases}1,&\text{if }k=0,\\ \tau_{+}\cos 2\beta,&\text{if }k=1,\\ \tau_{+}[1+(\tau_{+}-1)\cos^{2}(2\beta)],&\text{if }k=2,\end{cases}\] (F.17)

to write

\[2^{n-t}\sum_{\Delta_{+}}\binom{\tau_{+}}{n_{+++}}Q_{+++}^{n_{--- }}\sum_{\Delta_{-}}\binom{\tau_{-}}{n_{+++}}Q_{++-}^{n_{++-}}(\Delta_{+}- \Delta_{-})^{k}\] \[= \begin{cases}1,&\text{if }k=0,\\ (\tau_{+}-\tau_{-})\cos(2\beta),&\text{if }k=1,\\ \tau_{+}+\tau_{-}+\big{[}\tau_{-}^{2}+\tau_{+}(\tau_{+}-1)-\tau_{-}(1+2\tau_{+ })\big{]}\cos^{2}(2\beta),&\text{if }k=2.\end{cases}\] (F.18)

Finally, we just need to evaluate the sum over \(\tau_{\pm}\) subject to the three possible values of \(t\). Returning to (F.16), we can break \(\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{QAOA}}\rangle_{\gamma,\beta} ]=S_{2}+S_{1}+S_{0}\) into three parts, corresponding to \(t=2,1,0\)where

\[S_{2} =\frac{8}{n^{2}}\binom{n}{2}e^{-8\gamma^{2}(n-2)/n}\sum_{\tau_{+}+ \tau_{-}=n-2}\binom{n-2}{\tau_{+},\,\tau_{-}}\Big{(}2iQ_{++-}\sin[\Lambda(\tau_{+ }-\tau_{-})]\Big{)}^{2}2^{-(n-2)},\] (F.19) \[S_{1} =\frac{4}{n^{2}}\binom{n}{1}e^{-4\gamma^{2}(n-1)/n}\sum_{\tau_{+}+ \tau_{-}=n-1}\binom{n-1}{\tau_{+},\,\tau_{-}}\Big{(}2iQ_{++-}\sin[\Lambda(\tau_ {+}-\tau_{-})]\Big{)}2^{-(n-1)}(\tau_{+}-\tau_{-})\cos 2\beta,\] (F.20) \[S_{0} =\frac{1}{n}.\] (F.21)

Finally, since, \(\sin x=(e^{ix}-e^{-ix})/(2i)\), we have the following identities:

\[\sum_{r+s=m}\binom{m}{r,s}\sin^{2}[\Lambda(r-s)]=2^{m-1}[1-\cos^{m}2\Lambda],\] (F.22)

\[\sum_{r+s=m}\binom{m}{r,s}\sin[\Lambda(r-s)](r-s)=2^{m}m\sin\Lambda\cos^{m-1}\Lambda.\] (F.23)

Thus, plugging in \(\Lambda=4\lambda\gamma/n\) and using the fact that \(2iQ_{++-}=\frac{1}{2}\sin 2\beta\), we arrive at

\[\begin{split}\mathbb{E}_{\bm{Y}}[\langle\mathcal{R}^{2}_{\text{ QAOA}}\rangle_{\gamma,\beta}]&=\frac{n-1}{2n}e^{-8\gamma^{2}(n-2)/n} \sin^{2}(2\beta)[1-\cos^{n-2}(8\lambda\gamma/n)]\\ &+\frac{n-1}{n}e^{-4\gamma^{2}(n-1)/n}\sin(4\beta)\sin(4\lambda \gamma/n)\cos^{n-2}(4\lambda\gamma/n)+\frac{1}{n}.\end{split}\] (F.24)

## Appendix G Additional numerical simulations

In this appendix, complementing the simulation results in Section 4, additional numerical simulations are performed for \(1\leq p\leq 7\) at \(q=2\) and \(1\leq p\leq 6\) at \(q=3\).

Fig. 4 displays the second moment of QAOA overlap versus problem dimension \(n\). The y-axis plots the simulated second moment subtracting the theoretical value in the \(n\to\infty\) limit. For all demonstrated \((p,q)\) pairs, the simulation appears to converge to the theoretical value with order \(1/n\) deviations. This is consistent with the rigorous finite-\(n\) formula for \((p,q)=(1,2)\) in Eq. (F.24).

Our numerical simulations use the GenQAOA library, available at https://github.com/leologist/GenQAOA. The simulations are conducted on a laptop (MacBook Pro M2), where the simulation of each 26-qubit instance with \(p\)-step QAOA for \(1\leq p\leq 7\) took about 160 seconds. Data used in the figures are available upon request.

Figure 4: Log-log plots of the difference between observed overlap (averaged over instances and quantum measurements) at various problem dimension \(n\) and the predicted value from the sine-Gaussian law in the \(n\to\infty\) limit. Different colored lines correspond to different QAOA depth \(p\), with parameters \((\bm{\gamma},\bm{\beta})\) set to be the same as in Table 1. We choose \(\Lambda=0.2\), \(\lambda_{n}=\Lambda n^{1/(2p)}\) (left), and \(\lambda_{n}=\Lambda n^{[1+1/(2^{p}-1)]/2}\) (right). Error bars are standard errors of the mean.

Analysis of classical power iteration algorithm

### Proof of Proposition 3.3

Define \(\Lambda_{n}=\lambda_{n}/n^{(q-1)/2}\), we have

\[\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]=\Lambda_{n}\langle\hat{\bm{u}}_{0},\bm{ u}\rangle^{q-1}\frac{\bm{u}}{\sqrt{n}}+\frac{1}{\sqrt{n}}\bm{W}[\hat{\bm{u}}_{0}^{q-1 }]\equiv\Lambda_{n}G_{n}^{q-1}\frac{\bm{u}}{\sqrt{n}}+\frac{1}{\sqrt{n}}\bm{h},\]

where we define \(G_{n}=\langle\hat{\bm{u}}_{0},\bm{u}\rangle\), and \(\bm{h}=\bm{W}[\hat{\bm{u}}_{0}^{q-1}]\). Then marginally over \(\bm{W}\) and \(\hat{\bm{u}}_{0}\), we have \(G_{n}\) is independent of \(\bm{h}\), and \(G_{n}\) converges in distribution to a Gaussian random variable \(G\sim\mathcal{N}(0,1)\), \(\bm{h}\sim\mathcal{N}(0,I_{n})\). As a consequence, we have

\[(G_{n},\|\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]\|_{2})\stackrel{{ p}}{{\longrightarrow}}(G,\sqrt{1+\Lambda^{2}G^{2q-2}}),\ \ n\to\infty.\]

This gives

\[\langle\bm{u},\hat{\bm{u}}_{1}\rangle/n=\frac{\Lambda_{n}G_{n}^{q-1}+\langle \bm{u},\bm{h}\rangle/n}{\|\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]\|_{2}} \stackrel{{ p}}{{\longrightarrow}}\frac{\Lambda G^{q-1}}{\sqrt{1+ \Lambda^{2}G^{2q-2}}},\ \ n\to\infty.\]

This proves the Proposition 3.3.

### Proof of Proposition e.2

In this proof, we denote in short \(\hat{\bm{u}}_{k}=\hat{\bm{u}}_{k,\text{biased}}\). Define \(\Lambda_{n}=\lambda_{n}/n^{(1-c)(q-1)}\), we have

\[\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]=\Lambda_{n}[n^{(1/2)-c} \langle\hat{\bm{u}}_{0},\bm{u}\rangle]^{q-1}\frac{\bm{u}}{\sqrt{n}}+\frac{1}{ \sqrt{n}}\bm{W}[\hat{\bm{u}}_{0}^{q-1}]\equiv\Lambda_{n}U_{n}^{q-1}\frac{\bm {u}}{\sqrt{n}}+\frac{1}{\sqrt{n}}\bm{h},\]

where we define \(U_{n}=n^{1/2-c}\langle\hat{\bm{u}}_{0},\bm{u}\rangle\), and \(\bm{h}=\bm{W}[\hat{\bm{u}}_{0}^{q-1}]\). Then marginally over \(\bm{W}\) and \(\hat{\bm{u}}_{0}\), we have \(U_{n}\to\sin(2\delta)\), and \(\bm{h}\sim\mathcal{N}(0,I_{n})\). As a consequence, we have

\[(U_{n},\|\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]\|_{2})\stackrel{{ p}}{{\longrightarrow}}(\sin(2\delta),\sqrt{1+\Lambda^{2}\sin(2\delta)^{2q-2}}),\ \ n\to\infty.\]

This gives

\[\langle\bm{u}_{1},\hat{\bm{u}}\rangle/n=\frac{\Lambda_{n}U_{n}^{q-1}+\langle \bm{u},\bm{h}\rangle/n}{\|\bm{Y}[\hat{\bm{u}}_{0}^{\otimes(q-1)}]\|_{2}} \stackrel{{ p}}{{\longrightarrow}}\frac{\Lambda\sin(2\delta)^{q- 1}}{\sqrt{1+\Lambda^{2}\sin(2\delta)^{2q-2}}},\ \ n\to\infty.\]

This proves the Proposition E.2.

### Proof of Proposition 3.9

We prove this proposition using results in [33]. The notations in [33] are slightly different from the notations in this paper, and in the following, we will adopt the notations in the former.

Suppose we observe the spiked tensor model

\[\bm{T}=\bar{\lambda}_{n}\bm{v}^{\otimes q}+\bm{W},\] (H.1)

where \(\bm{v}\sim\mathrm{Unif}(\{\pm 1/\sqrt{n}\}^{n})\) and each element of \(\bm{W}\) is iid Gaussian. Note that the \(\bar{\lambda}_{n}\) in Eq. (H.1) is different from the \(\lambda_{n}\) in Eq. (1.1). We should take \(\bar{\lambda}_{n}=\sqrt{n}\lambda_{n}\) so that \(\bar{\lambda}_{n}/n^{(q-1+\varepsilon_{p})/2}\to\Lambda\).

Consider the tensor power iteration algorithm with initialization \(\bm{v}^{0}=\tilde{\bm{v}}^{0}\sim\mathrm{Unif}(\mathbb{S}^{n-1})\), and

\[\bm{v}^{t+1}=\bm{T}[(\tilde{\bm{v}}^{t})^{\otimes(q-1)}]=\bar{ \lambda}_{n}\langle\bm{v},\tilde{\bm{v}}^{t}\rangle^{q-1}\bm{v}+\bm{W}[( \tilde{\bm{v}}^{t})^{\otimes(q-1)}],\ \ \ \tilde{\bm{v}}^{t+1}=\bm{v}^{t+1}/\|\bm{v}^{t+1}\|_{2}.\] (H.2)

We let \(\alpha_{t}:=\bar{\lambda}_{n}\langle\bm{v},\tilde{\bm{v}}^{t-1}\rangle^{q-1}\). Then [33] shows the following lemma.

**Lemma H.1** (Lemma 3.2 of [33]).: _Consider the spiked tensor model as in Eq. (H.1) and consider the tensor power iteration (H.2). For any fixed \(\varepsilon\in(1/4,1/2)\), define the stopping time_

\[T_{\varepsilon}:=\min\left\{t\in\mathbb{N}_{+}:|\alpha_{t}|\geq n ^{\varepsilon}\right\}.\] (H.3)

_Then, there exists an absolute constant \(C>0\), such that with probability no less than \(1-\exp(-C\sqrt{n})\), the following happens: For all \(t<\min(T_{\varepsilon},n^{1/2(q-1)})\), we have_

\[\alpha_{t+1}\stackrel{{ d}}{{=}}(\bar{\lambda}_{n}n^{-(q-1)/2}) \zeta_{t}(\alpha_{t}+b_{t}+c_{t}Z_{t})^{q-1},\qquad\alpha_{0}=0,\] (H.4)

_where \(Z_{t}\sim\mathcal{N}(0,1)\) is independent of \((\zeta_{t},\alpha_{t},b_{t},c_{t})\),_

\[\zeta_{t}\in[1-n^{-1/6},1+n^{-1/6}],\,|b_{t}|\leq Cn^{1/4+(q-1)(\varepsilon-1 /2)},\,|c_{t}-1|\leq Cn^{2(q-1)(\varepsilon-1/2)},\] (H.5)We take \(\varepsilon\in(([q-1)^{p-1}-1]/[2(q-1)^{p}-2],1/2)\) to be fixed. By Lemma H.1, for a fixed \(p\in\mathbb{N}_{+}\), with high probability, we have \(T_{\varepsilon}\geq p\), as well as the upper bounds indicated in Eq. (H.5) for all \(t\leq p-1\). Applying Eq. (H.4) recursively implies that

\[\alpha_{p}\stackrel{{ d}}{{=}}(1+o_{\mathbb{P}}(1))\cdot n^{1/2} \Lambda^{1/\varepsilon_{p}}G^{(q-1)^{p-1}},\quad G\sim\mathcal{N}(0,1).\]

By the last equation on page 9 of [33], we see that (for \(\boldsymbol{H}_{p}=\{0,1,2,\ldots,p\}^{q-1}\))

\[\boldsymbol{v}^{p}=\alpha_{p}\boldsymbol{v}+\sum_{(i_{1},i_{2},\cdots,i_{q-1}) \in\boldsymbol{H}_{p-1}}\beta^{(p-1)}_{i_{1},i_{2},\cdots,i_{q-1}}\boldsymbol {w}_{i_{1},i_{2},\cdots,i_{q-1}},\]

where \(\boldsymbol{w}_{i_{1},i_{2},\cdots,i_{q-1}}\sim_{iid}\mathcal{N}(0,I_{n})\), and \(\sum_{(i_{1},i_{2},\cdots,i_{q-1})\in\boldsymbol{H}_{p-1}}|\beta^{(p-1)}_{i_{ 1},i_{2},\cdots,i_{q-1}}|^{2}=1\). Invoking the uniform law of large numbers, we are able to conclude that \(\mathcal{R}_{\text{PI}}\stackrel{{ d}}{{=}}\langle\boldsymbol{v}^ {p},\boldsymbol{v}\rangle/\|\boldsymbol{v}^{p}\|_{2}\stackrel{{ d}}{{\to}}\sin[\arctan(\Lambda^{1/ \varepsilon_{p}}G^{(q-1)^{p}})]\). This concludes the proof of Proposition 3.9.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have carefully written the abstract and introduction to accurately reflect the paper's contributions and limitations. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We are explicit in our abstract and introduction that our theoretical results on \(p\)-step QAOA are only rigorous for \(p=1\) and rely on heuristics for \(p>1\). We also conduct careful comparisons to classical algorithms in Section 3. We have also explicitly stated in our Introduction and Discussion sections that our results are limited to constant-step QAOA. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Every theorem, proposition, and lemma is very clear about the assumptions. Results that depend on conjectures are listed as claims with explicit reference to the conjectures. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: The sections on numeric experiments include detailed choices of all parameters so that one may reproduce the simulation results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example * If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. * If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). * We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code for numerical simulation is based on an open-source library as stated in Appendix G. The data used to generate the figures are available upon request. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: We do not run any machine learning experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All of the figures in this paper either plot all acquired data points explicitly, or show error bars which are explained in the caption. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: The information about compute resources used for our numerical simulations is provided in Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We abide by the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper is a theoretical study of a quantum algorithm applied to a statistical estimation problem. There is no immediate societal impact to discuss. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper is theoretical study of a quantum algorithm and hence does not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.