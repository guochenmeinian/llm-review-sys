[MISSING_PAGE_FAIL:1]

As the size of LLMs continues to grow, the potential harms caused by model-generated text are of increasing concern (Bender et al., 2021; Bommasani et al., 2021; Dinan et al., 2021; Kenton et al., 2021). A comprehensive taxonomy of ethical risks associated with recent LLMs is introduced in (Weidinger et al., 2022). One of its most pressing concerns is the risk of misinformation, stemming from the generation of erroneous, deceptive, irrational, or substandard information, defined as LLM outputting false, misleading, nonsensical or poor quality information, without malicious intent of the users.2 Potential harms of misinformation range from deceiving a person, causing physical damage, and amplifying the society's distrust on the usage of LLM-incorporated systems (Weidinger et al., 2022). For example, Galactic (Taylor et al., 2022), an LLM trained using a wide range of scientific sources including papers, reference materials, knowledge bases, etc., was reported3 to generate a fake study about the benefits of eating crushed glass. The aforementioned instance highlights the limitations of current LLMs in consistently delivering factually correct responses to user queries, a deficiency that potentially yield hazardous outcomes.

Footnote 2: This undesired phenomenon of LLMs sometimes is called _hallucination_ by some previous work (Ji et al., 2023a), a term that is initially mentioned in the context of psychology, defined as “percept, experienced by a waking individual, in the absence of an appropriate stimulus from the extracorporeal world” by Blom (2010). However, we stick to the description of _misinformation_ throughout our paper for consistency.

Despite the existence of several prior analyses, concerns are still far from being fully resolved. First of all, the evaluation of misinformation harm in existing LLMs has been conducted only using relatively simple formats such as multiple-choice (MC) question-answering (QA) task (Hendrycks et al., 2021) or cloze task (Petroni et al., 2019). These tasks are usually either formalized as a completion task, where LLMs are required to predict only a single token instead of generating full sentences, or using relatively simple evaluation metrics, e.g., accuracy, perplexity (Dinan et al., 2019), ROUGE score (Lin, 2004) and BLEU score (Khashabi et al., 2021), for the ease of evaluation (Hendrycks et al., 2021). In the context of long-form (LF)3 generation, Zhang et al. (2023) found that an incorrectly generated token at the outset will often be followed by a subsequent erroneous explanation; more generally, LLMs are reported to have the tendency to generate false statements, ranging from subtle inaccuracies to blatantly inauthentic claims (Lin et al., 2022). These phenomena highlight the importance of conducting LF generation evaluations for LLMs. Yet, there are no sufficient datasets available for such evaluations, especially in knowledge-intensive domains. Furthermore, due to the unbalanced distribution in available language-related resources (Zeng et al., 2023), a substantial number of existing datasets and benchmarks only focus on measuring the misinformation in English, which impedes similar evaluations from being performed in other languages. A brief summary of previous datasets on misinformation evaluation is shown in Table 1; even though some of them are not initially designed for misinformation evaluation, we found that they can be easily used for it.

Footnote 3: https://news.yahoo.com/meta-ai-bot-contributed-fake-212500291.html

Footnote 4: LF generation is a widely used concept (Fan et al., 2019), referred as generation of text that “span multiple sentences” or “paragraph-length”. We found no stringent standard that can be utilized to distinguish LF generation against shorter ones; however, we put nearly no limitation on models’ generation, i.e., we let the models generate sentences at will until maximum length is reached. See Appendix B.6 for more details.

To motivate the misinformation evaluation on LLMs, we propose **C**hinese **b**n**m**ARk for misinformation **E**valuation in **M**aternity and **I**n**fant care (**CARE-MI**), a benchmark to test medical-related misinformation of LLMs in Chinese. CARE-MI specifically focuses on the sub-domain of maternity and infant care, a topic in which users are prone to generate questions, especially expectant and first-time parents, about multiple issues, e.g., pregnancy and/or baby-related illnesses, symptoms, milestones, etc., and in which LLMs should respond factually and without errors. Although there are already existing datasets focusing on the medical domain, until now, there is no benchmark suitable for evaluating misinformation on such an important and sensitive topic as the maternity and infant care, neither in English nor in Chinese. The closest dataset to CARE-MI is MATINF (Xu et al., 2020), which, differently from us, focuses on community QA and contains neither expert-level annotations nor supporting evidence documents, making it not suitable for misinformation evaluation. Our benchmark, however, is not designed for directly evaluating user-LLM interactions, as most of our questions require expert-level knowledge and contain medical norms, but it is a necessary prerequisite for LLMs to perform well in those cases.

Additionally, we conduct an extensive evaluation on recent Chinese LLMs. The results indicate that all current models are not able to maintain an acceptable performance while answering domain-related questions. We further explore the paradigm for automatic evaluation on LF generation of LLMsas a completion to the benchmark dataset for the purpose of efficient and accurate evaluation. We test multiple judgment models trained on the same set of questions as in the benchmark, with 1) synthetically generated positive and negative answers, and 2) expert-level annotations on the models' outputs, and 3) expert-annotated knowledge. The whole pipeline, encompassing the development of the benchmark and the training of the judgment model, can also serve as an innovative paradigm for creating similar benchmarks in other knowledge-intensive domains or low-resourced languages.

## 2 CARE-MI data acquisition

With no existing datasets on the topic (_maternity and infant care_), language (_Chinese_), and task (_misinformation evaluation in LF generation_) of interest, we construct CARE-MI from multiple data sources. We utilize two knowledge graph (KG) datasets and two MCQA datasets as our data sources. The collected data is then filtered to align with our focused topic.

KG datasetsWe rely on two medical-based KG datasets in Chinese: Bio-Medical Informatics Ontology System (BIOS) (Yu et al., 2022) and CPubMed (CPu, 2021), both of which come with <head, relation, tail> triplets. BIOS is a machine-generated bio-medical KG built on top of the abstracts and central articles from PubMed,5 which is a search engine for bio-medical articles. Both head and tail in BIOS are concepts representing the nodes in the KG; each concept includes multiple terms that are considered synonymous. In total, it contains 4.1 million concepts, 7.4 million terms, and 7.3 million relations. As the data is gathered from PubMed, BIOS triplets are collected in English and later translated into Chinese (Luo et al., 2021); translation quality is ensured by applying back-translation and filtering out samples with low confidence. On the other hand, CPubMed is an

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Dataset** & **Language** & **\#Question** & **LF** & **Supervised** \\ \hline \multicolumn{4}{l}{_Multiple_} \\ COMMONSENSEQA (Talmor et al., 2019) & English & 12,247 & ✗ & ✓ \\ Wizard of Wikipedia (Dinan et al., 2019) & English & 201,999 & ✓ & ✗ \\ LAMA (Petroni et al., 2019) & English & - & ✗ & ✗ \\ NO (Kwiatkowski et al., 2019) & English & 323,045 & ✓ & ✓ \\ ELIS (Fan et al., 2019) & English & \(\approx\)272,000 & ✓ & ✗ \\ MMLU (Hendrycks et al., 2021) & English & 15,908 & ✗ & ✗ \\ COM2SENSE (Singh et al., 2021) & English & \(\approx\)4,000 & ✗ & ✓ \\ GOOAQ (Khashabi et al., 2021) & English & \(\approx\)3,100,000 & ✓ & ✗ \\ KMIR (Gao et al., 2022) & English & 16,000\({}^{\alpha}\) & ✗ & ✓ \\ TruthfulQA (Lin et al., 2022) & English & 817 & ✓ & ✓ \\ ScienceQA (Lu et al., 2022) & English & 21,208 & ✗ & ✓ \\ M3KE (Liu et al., 2023) & Chinese & 20,477 & ✗ & ✗ \\ \hline \multicolumn{4}{l}{_Legal_} \\ JEC-QA (Zhong et al., 2019) & Chinese & 26,365 & ✗ & ✗ \\ CaseHOLD (Zheng et al., 2021) & English & 53,137 & ✗ & ✗ \\ \hline \multicolumn{4}{l}{_Medical_} \\ cMedQA2 (Zhang et al., 2018) & Chinese & 108,000 & ✓ & ✗ \\ PubMedQA (Jin et al., 2019) & English & \(\approx\)1,000\({}^{\beta}\) & ✗ & ✓ \\ MATINF (Xu et al., 2020) & Chinese & \(\approx\)1,070,000 & ✗ & ✗ \\ MEDQA (Jin et al., 2020) & Chinese/English\({}^{\gamma}\) & 61,097 & ✗ & ✓ \\ CMEIE (Guan et al., 2020) & Chinese & 22,406 & ✗ & ✓ \\ BiolAMA (Sung et al., 2021) & English & \(\approx\)49,000 & ✗ & ✗ \\ MLEC-QA (Li et al., 2021) & Chinese & 136,236 & ✗ & ✓ \\
**CARE-MI (ours)** & Chinese & 1,612 & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: A brief summary of datasets in misinformation evaluation. _LF_: whether the dataset is designed for LF generation task; _Supervised_: whether the dataset construction involves human supervision. \({}^{\alpha}\)We refer to the selected samples that have been annotated by annotators. \({}^{\beta}\)We are only referring to the expert annotated part of PubMedQA. \({}^{\gamma}\)For _Chinese_ mentioned here, we are referring to both traditional and simplified Chinese.

[MISSING_PAGE_FAIL:4]

CARE-MI benchmark

We construct the CARE-MI benchmark on top of the samples acquired in Section 2. The benchmark is based on two parts: a _synthetic data generation_ process and a set of _judgment models_. With the synthetic data generation process, we create samples in the desired format for misinformation evaluation on LLMs. Then, empowered by the judgment model (see Section 4.3), we offer an automated metric for efficient and accurate LF generation evaluation, aiming to simplify human-based evaluation which is not only expensive but time-consuming. Note that both the synthetic data generation process and the judgment model construction are domain-agnostic and can be easily applied to other misinformation evaluation topics. In total, CARE-MI benchmark contains 1,612 samples; it is intended for LF generation evaluation under zero-shot (Wei et al., 2022, Brown et al., 2020) setting. Statistics regarding the question length can be found in Figure 1. More details can be found in Appendix B.1.

The synthetic data generation process is summarized in Figure 2. It consists of five components: 1) true statement generation, 2) false statement generation, 3) question generation, 4) knowledge retrieval, and 5) expert annotation.

True statement generationWe define _true statements_ as sentences that are evidence-based and factually correct, for example, _Placenta previa may potentially lead to preterm birth_. We generate true statements from the samples collected in Section 2. For samples from KG datasets, we build the true statements heuristically from the triplets with rule-based methods. For samples from MCQA datasets, we formalize the generation of true statements as a QA2D task where the model is required to perform sentence transformations to combine the question and answer into a declarative sentence (Demszky et al., 2018). The generation is done by using the combination of the rule-based method and an off-the-shelf model for simple cases such that the question from the QA pair ends with a pre-defined set of tokens such as "\(\frac{\text{H}}{\text{H}\text{E}}"\) (referred as _is_ in English), we directly concatenate the question with the answer as the synthetic true statement; otherwise, the generation is done using the GPT-3.5-turbo (OpenAI, 2023a). Details regarding the implementation of the rule-based system can be found in our code repository; details about the prompts for true statement generation are in Appendix B.8.

False statement generationSimilar to the definition of _true statement_, we define _false statements_ as sentences that are factually incorrect, for example, _progesterone is not a hormone_. We approach the construction in two different ways: _negation_ and _replacement_, corresponding to two types of false statements. For negated false statements, the generation is done on all available generated true statements where we generate the false statements by directly performing negation on the corresponding true ones; this construction procedure is also done by combining a rule-based method with applying the GPT-3.5-turbo and the details can be found in our code repository. We only generate false statements with replacement for samples that originally come from MCQA datasets; we generate the false statements by replacing the correct answers in the generated true statements with randomly selected wrong answers from the corresponding MC options. Our prompts utilized for generating negated statements can be found in Appendix B.7.

Question generationWe generate questions based on the true statements. We rely on LLMs as their instruction-following ability allows us to generate questions efficiently instead of using resource-consuming fine-tuning methods. To select the best LLM for question generation, we experimentally

Figure 1: Statistics of the questions in CARE-MI. _Left_: average question lengths for each source (average over all questions is shown in gray). _Right_: question length distribution.

compare three LLMs available in Chinese: GPT-3.5-turbo, ChatGLM-6B [14, 15] and ChatYuan [13]. For this experiment, we evaluate the model performance on the BIOS dataset (See Appendix B.4 for more details) and select ChatYuan as the final choice.

We generate two types of questions: True/False (TF) questions and open-ended (OE) questions. TF questions only admit binary answers, either agreeing or disagreeing with the statement in the question, while OE questions allow a variety of response styles. More specifically, we use ChatYuan along with a rule-based method to generate both TF and OE questions for MCQA samples while we only generate TF questions for KG samples (See Appendix B.4 for details). Information about the generated questions is shown in Table 3. In total, we generate 2,240 and 2,963 questions for MLEC-QA and MEDQA datasets, and 79 and 497 questions for BIOS and CPubMed datasets, respectively. Figure 3 shows an example of generated questions in the MLEC-QA dataset.

Knowledge retrievalMaternity and infant care is a critical subject matter wherein the dissemination of misinformation could endanger lives. In our benchmark, we include external knowledge in each sample to provide auxiliary information, not only for models but also for humans, allowing suitable

Figure 3: An example of generated questions in MLEC-QA datasets. _ZH_ and _EN_ stands for Chinese and English. English sentences are translated for reference.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Source** & **TF** & **OE** \\ \hline BIOS & 79 & - \\ CPubMed & 497 & - \\ MLEC-QA & 1,333 & 907 \\ MEDQA & 1,617 & 1,346 \\ \hline Total & 3,526 & 2,253 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Number of generated questions for CARE-MI.

Figure 2: CARE-MI construction process. Data generation components are shown on green background, whereas the final benchmark samples are shown on orange background.

inferences on the veracity of the statements to be made. To achieve this, we apply BM25 (Robertson and Zaragoza, 2009) to obtain relevant knowledge from given knowledge sources based on the queries. We use two sources for knowledge bases: the Chinese Wikipedia7 and Medical books collected by Jin et al. (2020). We conduct topic filtering for Chinese Wikipedia as it originally contains a huge amount of pages that might not be relevant to the topic of maternity and infant care. During the knowledge retrieval, queries are the concatenation of the questions and the corresponding true statements. We conduct the retrieval on paragraph level; to account for differences between the two knowledge sources, we retrieve the top three most relevant paragraphs from both sources, respectively.

Footnote 7: https://dumps.wikimedia.org/zhwiki/

Expert annotationWe hire two medical-domain experts as annotators to review the generated samples and a third meta-annotator to arbitrate their disagreement. The annotators are instructed to answer a series of guiding questions. The guideline including rules and procedures that annotators should follow as well as the inter-annotator agreements are detailed in Appendix A.1. We ask them to evaluate 5,779 synthetic samples, and we discard those that receive two or more negative evaluations. After the expert annotation, the benchmark is downsized to 1,612 samples.

## 4 Experiments

We evaluate Chinese LLMs with respect to misinformation in LF generation using our proposed benchmark. Experiments are done in the form of single-round conversation where we feed models with questions and collect the direct output from the corresponding model. We perform the evaluation under the _true zero-shot_(Lin et al., 2022) setting: for each model, the input is a question from our benchmark, without any additional instruction and examples; prompt and hyperparameters are not tuned in any way. All questions in CARE-MI are used in the evaluation.

### Experimental details

ModelsAs our goal is to evaluate the LLM misinformation in LF generation, we focus on the evaluation of autoregressive language models that are trained for generation tasks, as opposed to masked language models like BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). We evaluate Chinese LLMs that have been specifically tuned for conversation scenarios, either by supervised fine-tuning (SFT) or reinforcement learning with human feedback (RLHF) as we aim at measuring the misinformation occurring when the models directly interact with humans. We include ChatGLM-6B (Zeng et al., 2022; Du et al., 2022), MOSS-16B-SFT (Sun and Qiu, 2023), two variants of the BELLE (Ji et al., 2023, 2023) series, a.k.a BELLE-7B-0.2M and BELLE-7B-2M, and two models from the GPT family, GPT-3.5-turbo (OpenAI, 2023) and GPT-4 (OpenAI, 2023). We also include a LLAMA (Touvron et al., 2023) model which we perform further pretraining on Chinese corpus including CLUECorpus2020 (Xu et al., 2020), Chinese Scientific Literature Dataset (Li et al., 2022), Chinese Wikipedia7, RedPajama-Data (Computer, 2023), etc., and fine-tuning on translated ShareGPT (RyokoAI, 2023), Aplaca-GPT4 (Peng et al., 2023) and WizardLM (Xu et al., 2023). We denote this version as LLaMA-13B-T. More details regarding the parameter settings can be found in Appendix B.5. Finally, we recruit a domain expert to act as a human baseline; we randomly select 200 questions from the benchmark and let the expert answer them correspondingly; the selected samples are collected strictly following their original distribution in the benchmark regarding their sources. The expert is allowed to check any resource that is necessary and is suggested to finish each question within 2 minutes.

Footnote 7: https://dumps.wikimedia.org/zhwiki/

Evaluation metricsUnlike Lin et al. (2022) who consider an answer to be truthful if and only if it avoids asserting a false statement and thus allows non-committal answers such as _No comments_ and _I don't know_ as legal truthful answers, we follow the human evaluation framework similar to what has been explored in (Lu et al., 2022). For each model-generated answer, we recruit three medical-domain expert-level annotators to evaluate the following two aspects:

1. **Correctness**: given a question, whether the answer is factually correct and relevant.
2. **Interpretability**: given a question, whether the answer contains a detailed and concise explanation that demonstrates how the conclusion was reached.

We require the annotators to make their judgments independently during the evaluation; for each aspect, they are asked to give a scalar score between 0 and 1 to reflect their decisions for each sample (the higher the better). The final evaluation results are averaged over the three annotators. We refer the readers to Appendix A.2 for more details about this human evaluation.

### Results

OverviewEvaluation results on correctness and interpretability are shown in Table 4 and Table 5, respectively. Among all evaluated Chinese models, models from the GPT family perform the best by a large margin in both correctness (\(\geq\) 0.158) and interpretability (\(\geq\) 0.112). In general, all models exhibit better performance on samples generated from KG datasets than from MCQA datasets, as they generally have longer context and necessitate more reasoning ability from the models to produce the correct answer. Overall, LLMs with smaller sizes tend to perform worse; however, even the best model is not comparable with human expert, indicating room for improvement. Figure 4 further presents the correctness and interpretability evaluation for TF and OE questions separately. We observe that all models perform better on TF questions than OE questions: on average, most models are able to achieve 0.8 of correctness as well as interpretability for TF questions while only GPT models can achieve a correctness of 0.6 for OE questions. This highlights the weakness of current models, where LLMs still struggle with complex reasoning.

Correctness is linear-correlated with interpretabilityFigure 5 shows the correlation between correctness and interpretability across all evaluated LLMs. In general, all models exhibit a similar pattern, where the interpretability linear correlates with the corresponding correctness. Exceptions include BELLE-6B-0.2M, which shows abnormally low interpretability. On the contrary, ChatGLM-6B and LLaMA-13B-T present above-average interpretability; however, this is not always good. Better interpretability scores only indicate better generated descriptions when explaining how the conclusions are drawn (See Section 4.1); yet if the conclusions themselves are factually incorrect, more detailed explanations will only lead to misleading consequences.

More data is not always betterBELLE-7B-0.2M present slightly better performance (\(\uparrow\)0.023) but lower interpretability (\(\downarrow\)0.083) in comparison with its twin model BELLE-7B-2M. Both BELLE-7B-0.2M and BELLE-7B-2M use BLOOM (Scao et al., 2022) as base model with the only difference being the size of the instruction set used during fine-tuning. As mentioned, better interpretability is not always a desired property, especially when it is paired with much lower correctness; more instruction

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Model** & **All** & **BIOS** & **CPubMed** & **MLEC-QA** & **MEDQA** \\ \hline MOSS-16B-SFT (2023) & 0.671\(\pm\)0.321 & 0.930\(\pm\)0.121 & 0.925\(\pm\)0.166 & 0.644\(\pm\)0.332 & 0.639\(\pm\)0.316 \\ ChatGLM-6B (2022b) & 0.610\(\pm\)0.333 & 0.928\(\pm\)0.116 & 0.748\(\pm\)0.264 & 0.579\(\pm\)0.346 & 0.599\(\pm\)0.328 \\ BELLE-7B-2M (2023b) & 0.647\(\pm\)0.315 & 0.843\(\pm\)0.268 & 0.928\(\pm\)0.175 & 0.631\(\pm\)0.314 & 0.605\(\pm\)0.311 \\ BELLE-7B-0.2M (2023b) & 0.670\(\pm\)0.316 & 0.947\(\pm\)0.095 & 0.942\(\pm\)0.141 & 0.624\(\pm\)0.335 & 0.646\(\pm\)0.302 \\ GPT-4 (2023b) & 0.867\(\pm\)0.215 & 0.958\(\pm\)0.125 & 0.967\(\pm\)0.124 & 0.851\(\pm\)0.233 & 0.858\(\pm\)0.211 \\ GPT-3.5-turbo (2023a) & 0.824\(\pm\)0.263 & 0.973\(\pm\)0.108 & 0.948\(\pm\)0.160 & 0.799\(\pm\)0.279 & 0.815\(\pm\)0.263 \\ LLaMA-13B-T (2023) & 0.709\(\pm\)0.301 & 0.871\(\pm\)0.235 & 0.922\(\pm\)0.178 & 0.678\(\pm\)0.311 & 0.689\(\pm\)0.297 \\ \hline Human Baseline\({}^{\dagger}\) & 0.938\(\pm\)0.213 & 1.000\(\pm\)0.000 & 1.000\(\pm\)0.000 & 0.945\(\pm\)0.196 & 0.908\(\pm\)0.262 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average correctness scores for all models and human baseline. Deeper \(\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color\color@color\color@color@color\color@color@color@color@color@color@color@color@color@color@color@color@@color@@color@@color@@@@@color@@@@@@color@tuning yields better language ability, yet it has no effect on improving knowledge correctness. This indicates that increasing the size of instructions used for fine-tuning LLMs is not always helpful in improving its knowledge capability. The conclusion is intuitive: the objective for fine-tuning does not align with improving truthfulness. Consequently, increasing the size of instruction-following samples will not assist the model in providing more truthful answers.

### Automated metrics

Human evaluation, especially in knowledge-intensive domains, is costly and difficult to reproduce. On the other hand, traditional automated evaluation metrics, such as perplexity (Dian et al., 2019), BLEU score (Khashabi et al., 2021) and ROUGE score (Lin, 2004), suffer from a misalignment problem in which the metrics fail to capture the actual performance of the models. Lin et al. (2022) propose to train a judgment model using human labels to serve as a proxy of human annotators. However, the trained models cannot transfer across different tasks and languages. To enable efficient and accurate automated evaluation for our proposed benchmark, we explore using different architectures as backbones of the judgment models and fine-tune them to mimic human judgment.

We use 1) both generated true and false statements as synthetic positive and negative answers, and 2) the human evaluation results, to train all judgment models. We also feed models with all available retrieved knowledge. To evaluate the performance of each judgment model, we select the answer outputs from MOSS-16B-SFT and ChatGLM-6B as the validation set. This is because the scores these two models received are closer to 0.5 on average, indicating a more balanced distribution of good and bad answers. We perform the training using the answers from the rest of the evaluated models. In order to compare different architectures, we fine-tune BERT-Large (Devlin et al., 2019), two GPT-3 variants (GPT-3-350M and GPT-3-6.7B) and LLaMA-13B-T (See Section 4.1 for details). For training BERT-Large, we use the [SEP] token as the separator between fields and directly

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Metric**} & \multicolumn{2}{c}{**Random**} & \multicolumn{2}{c}{**BERT-Large**} & \multicolumn{2}{c}{**GPT-3-350M**} & \multicolumn{2}{c}{**GPT-3-6.7B**} & \multicolumn{2}{c}{**LLaMA-13B-T**} \\ \cline{2-10}  & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. \\ \hline Correctness & - & 0.560 & 0.606 & 0.560 & 0.783 & 0.835 & 0.803 & 0.858 & **0.868** & 0.898 \\ Interpretability & - & 0.800 & 0.013 & 0.794 & 0.565 & 0.822 & 0.634 & 0.828 & **0.683** & 0.835 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Pearson correlation scores (Pear.) and accuracy (Acc.) are reported for each trained judgment model. We highlight the best Pearson correlation scores in bold.

Figure 4: Evaluation results for TF and OE questions.

Figure 5: Correctness and interpretability metrics show a linear relationship with the \(R^{2}\) being 0.834.

concatenate all fields together (e.g., question, answer and retrieved knowledge) as the input. Prompt used for fine-tuning LLMs can be found in Appendix B.9. For BERT-Large and the two GPT variants evaluated, as they suffer from input length limitation, we truncate the retrieved knowledge to make sure the input meets the model requirements. For comparison, we also include a majority baseline, which always predicts the most frequent score in the training set. More details regarding the experiment settings can be found in Appendix B.5. We evaluate the performance of the judgment model using Pearson correlation and accuracy. To obtain accuracy, we cast the scalar output of the judgment models into binary labels by empirically setting the threshold to 0.5. Judgment models' results are shown in Table 6.

We observe that BERT-Large does not learn anything useful. This might be due to the stringent constraints on the input size as well as the model's inherent inability in complicated reasoning tasks. Larger models tend to perform better; however, it is harder for models to understand the correlation between input text and the labels in the aspect of interpretability. For the final judgment model, we select LLaMA-13B-T as the backbone, as it obtains the best results. We train the model again but with all available data, including both training and validation samples.

We make the judgment models for both aspects publicly available.

We further conduct an ablation study to assess whether incorporating retrieved knowledge enhances the performance of judgment models. Specifically, we perform a comparison by training the LLaMA-13B-T both with and without the retrieved knowledge during the fine-tuning while maintaining the rest of the settings identical. The experiment results are shown in Table 7, which demonstrate that adding knowledge improves the performance of the judgment models.

## 5 Conclusion and Limitation

In this paper, we proposed CARE-MI, a Chinese benchmark for LLM misinformation evaluation in LF generation in the topic of maternity and infant care. This is the first Chinese benchmark that aims to quantify the misinformation in LF generation cases, the only dataset with clear expert-annotations on the maternity and infant care domain, and with a construction pipeline that can be easily transferred to other domains and languages. We conducted comprehensive assessments on Chinese LLMs, and showed that current models still have room for improvement. Furthermore, we investigated different model backbones for training judgment models and provided an efficient judgment model that can perform correctness evaluation of LF generation accurately. We believe that our proposed benchmark not only paves the way for easier benchmark construction for the whole research community, but also contributes to promoting better Chinese LLM applications in the maternity and infant care domain.

LimitationCARE-MI aims solely at evaluating the misinformation in long-form generation tasks for Chinese LLMs on the topic of maternity and infant care. It is not designed for any other scenarios, e.g., evaluating other target groups such as medical professionals or students. The misuse of CARE-MI might lead to unpredictable consequences. Additionally, the benchmark contains content that can only be considered as correct for now. With the development of modern clinical techniques, we expect the accuracy of the information provided in the benchmark to decrease over time, and thus our proposed benchmark might not be suitable for misinformation evaluation at a later stage, e.g., 10 years from now. Furthermore, our benchmark might not align with the actual interests of the maternity and infant care community (e.g., pregnant women) in real-life situations, as the benchmark is not constructed by collecting the most frequently asked questions in the community. Last but not least, even though we have tried our best to reduce the subjective bias during the human annotations, we cannot completely avoid it.

## Acknowledgement

This work was partly supported by JSPS KAKENHI No. JP22K12091.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Metric**} & \multicolumn{2}{c}{**w/o K.**} & \multicolumn{2}{c}{**w/ K.**} \\ \cline{2-5}  & Pear. & Acc. & Pear. & Acc. \\ \hline Correctness & 0.779 & 0.806 & **0.868** & 0.898 \\ Interpretability & 0.639 & 0.867 & **0.683** & 0.835 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Evaluations are done on the validation set. _w/o K._ and _w/ K._ represents fine-tuning the models without and with knowledge. We highlight the best Pearson correlation scores in bold.

## References

* Brown et al. (2021) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
* Lieber et al. (2021) Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. _White Paper. AI21 Labs_, 1, 2021.
* Rae et al. (2018) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angelik Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. _CoRR_, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.
* Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. _CoRR_, abs/2201.11990, 2022. URL https://arxiv.org/abs/2201.11990.
* Singhal et al. (2022) Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhory, Philip Andrew Mansfield, Blaise Aguera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semurts, Alan Karthikesalingam, and Vivek Natarajan. Large Language Models Encode Clinical Knowledge. _CoRR_, abs/2212.13138, 2022. doi: 10.48550/arXiv.2212.13138. URL https://doi.org/10.48550/arXiv.2212.13138.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924-2936, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.
* Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Don't Know: Unanswerable Questions for SQuAD. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 784-789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124.
* Rajpurkar et al. (2019)Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.
* Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3261-3275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8d6e-Abstract.html.
* Bommasani et al. (2021) Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemedy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the Opportunities and Risks of Foundation Models. _CoRR_, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.
* Nuance (2023) Nuance. Automatically document care with the Dragon Ambient eXperience, 2023. URL https://www.nuance.com/healthcare/ambient-clinical-intelligence.html.
* OpenAI (2022) OpenAI. New and improved embedding model, 2022. URL https://openai.com/blog/new-and-improved-embedding-model.
* Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Madeleine Clare Elish, William Isaac, and Richard S. Zemel, editors, _FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021_, pages 610-623. ACM, 2021. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.
* Dinan et al. (2021) Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon L. Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. _CoRR_, abs/2107.03451, 2021. URL https://arxiv.org/abs/2107.03451.
* Kenton et al. (2021) Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment of Language Agents. _CoRR_, abs/2103.14659, 2021. URL https://arxiv.org/abs/2103.14659.
* 24, 2022_, pages 214-229. ACM, 2022. doi: 10.1145/3531146.3533088. URL https://doi.org/10.1145/3531146.3533088.
* Ji et al. (2023a) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in Natural Language Generation. _ACM Comput. Surv._, 55(12):248:1-248:38, 2023a. doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730.
* Blom (2010) Jan Dirk Blom. _A dictionary of hallucinations_. Springer, 2010.
* Blom et al. (2018)Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A Large Language Model for Science. _CoRR_, abs/2211.09085, 2022. doi: 10.48550/arXiv.2211.09085. URL https://doi.org/10.48550/arXiv.2211.09085.
* Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In _International Conference on Learning Representations_, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.
* Petroni et al. (2019a) Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language Models as Knowledge Bases? In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2463-2473, Hong Kong, China, 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.
* Dinan et al. (2019) Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of Wikipedia: Knowledge-Powered Conversational Agents. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=r1173iRqKm.
* Lin (2004) Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.
* Khashabi et al. (2021a) Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, and Chris Callison-Burch. GooAQ: Open Question Answering with Diverse Answer Types. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 421-433, Punta Cana, Dominican Republic, 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.38. URL https://aclanthology.org/2021.findings-emnlp.38.
* Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021b. URL https://openreview.net/forum?id=d7KBjmI3GmQ.
* Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long Form Question Answering. _CoRR_, abs/1907.09190, 2019. URL http://arxiv.org/abs/1907.09190.
* Zhang et al. (2023) Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How Language Model Hallucinations Can Snowball. _arXiv preprint arXiv:2305.13534_, 2023.
* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.
* Zeng et al. (2023) Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, Rob Voigt, and Jie Yang. GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China_, pages 6290-6298. ijcai.org, 2023. doi: 10.24963/ijcai.2023/698. URL https://doi.org/10.24963/ijcai.2023/698.
* Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4149-4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL https://doi.org/10.18653/v1/n19-1421.
* Tajd et al. (2019)Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language Models as Knowledge Bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 2463-2473. Association for Computational Linguistics, 2019b. doi: 10.18653/v1/D19-1250. URL https://doi.org/10.18653/v1/D19-1250.
* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A Benchmark for Question Answering Research. _Transactions of the Association for Computational Linguistics_, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.
* Singh et al. (2021) Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu, Xuezhe Ma, and Nanyun Peng. COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences. _CoRR_, abs/2106.00969, 2021. URL https://arxiv.org/abs/2106.00969.
* Khashabi et al. (2021b) Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, and Chris Callison-Burch. GooAQ: Open Question Answering with Diverse Answer Types. _CoRR_, abs/2104.08727, 2021b. URL https://arxiv.org/abs/2104.08727.
* Gao et al. (2022) Daniel Gao, Yantao Jia, Lei Li, Chengzhen Fu, Zhicheng Dou, Hao Jiang, Xinyu Zhang, Lei Chen, and Zhao Cao. KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models. _CoRR_, abs/2202.13529, 2022. URL https://arxiv.org/abs/2202.13529.
* Lu et al. (2022) Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/11332b6b6cf448eb84afadb1352d3a9a-Abstract-Conference.html.
* Liu et al. (2023) Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su, Qun Liu, and Deyi Xiong. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. _CoRR_, abs/2305.10263, 2023. doi: 10.48550/arXiv.2305.10263. URL https://doi.org/10.48550/arXiv.2305.10263.
* Zhong et al. (2019) Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. JECQA: A Legal-Domain Question Answering Dataset. _CoRR_, abs/1911.12011, 2019. URL http://arxiv.org/abs/1911.12011.
* 25, 2021_, pages 159-168. ACM, 2021. doi: 10.1145/3462757.3466088. URL https://doi.org/10.1145/3462757.3466088.
* Zhang et al. (2018) Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection. _IEEE Access_, 6:74061-74071, 2018. doi: 10.1109/ACCESS.2018.2883637. URL https://doi.org/10.1109/ACCESS.2018.2883637.
* Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A Dataset for Biomedical Research Question Answering. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2567-2577, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259.

Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, and Chenliang Li. MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 3586-3596, Online, 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.330. URL https://aclanthology.org/2020.acl-main.330.
* Jin et al. (2020) Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams. _CoRR_, abs/2009.13081, 2020. URL https://arxiv.org/abs/2009.13081.
* 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part I_, volume 12430 of _Lecture Notes in Computer Science_, pages 270-282. Springer, 2020. doi: 10.1007/978-3-030-60450-9_22. URL https://doi.org/10.1007/978-3-030-60450-9_22.
* Sung et al. (2021) Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, and Jaewoo Kang. Can Language Models be Biomedical Knowledge Bases? In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 4723-4734, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.388. URL https://aclanthology.org/2021.emnlp-main.388.
* Li et al. (2021) Jing Li, Shangping Zhong, and Kaizhi Chen. MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 8862-8874, Online and Punta Cana, Dominican Republic, 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.698. URL https://aclanthology.org/2021.emnlp-main.698.
* Yu et al. (2022) Sheng Yu, Zheng Yuan, Jun Xia, Shengxuan Luo, Huaiyuan Ying, Sihang Zeng, Jingyi Ren, Hongyi Yuan, Zhengyun Zhao, Yucong Lin, Keming Lu, Jing Wang, Yutao Xie, and Heung-Yeung Shum. BIOS: An Algorithmically Generated Biomedical Knowledge Graph. _CoRR_, abs/2203.09975, 2022. doi: 10.48550/arXiv.2203.09975. URL https://doi.org/10.48550/arXiv.2203.09975.
* CPubMed-kG (2021) CPubMed-kG, 2021. URL https://cpubmed.openi.org.cn/graph/wiki.
* Luo et al. (2021) Shengxuan Luo, Huaiyuan Ying, Jiao Li, and Sheng Yu. Sentence Alignment with Parallel Documents Facilitates Biomedical Machine Translation. _arXiv preprint arXiv:2104.08588_, 2021.
* Li et al. (2021b) Jing Li, Shangping Zhong, and Kaizhi Chen. MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 8862-8874, Online and Punta Cana, Dominican Republic, 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.698. URL https://aclanthology.org/2021.emnlp-main.698.
* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned Language Models are Zero-Shot Learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCzdqR.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
* Liu et al. (2021)Dorottya Demszky, Kelvin Guu, and Percy Liang. Transforming Question Answering Datasets Into Natural Language Inference Datasets. _CoRR_, abs/1809.02922, 2018. URL http://arxiv.org/abs/1809.02922.
* OpenAI (2023a) OpenAI. Introducing ChatGPT and Whisper APIs, 2023a. URL https://openai.com/blog/introducing-chatgpt-and-whisper-apis.
* Du et al. (2022a) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, Dublin, Ireland, 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.26. URL https://aclanthology.org/2022.acl-long.26.
* Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: An Open Bilingual Pre-trained Model. _CoRR_, abs/2210.02414, 2022. doi: 10.48550/arXiv.2210.02414. URL https://doi.org/10.48550/arXiv.2210.02414.
* ClueAI (2023) ClueAI. ChatYuan: Large Language Model for Dialogue in Chinese and English, 2023. URL https://github.com/clue-ai/ChatYuan/.
* Robertson and Zaragoza (2009) Stephen E. Robertson and Hugo Zaragoza. The Probabilistic Relevance Framework: BM25 and Beyond. _Found. Trends Inf. Retr._, 3(4):333-389, 2009. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.
* Sun and Qiu (2023) Tianxiang Sun and Xipeng Qiu. MOSS, 2023. URL https://github.com/OpenLMLab/MOSS.
* Ji et al. (2023b) Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. BELLE: Be Everyone's Large Language model Engine, 2023b. URL https://github.com/LianjiaTech/BELLE.
* Ji et al. (2023) Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases. _arXiv preprint arXiv:2303.14742_, 2023b.
* OpenAI (2023) OpenAI. GPT-4 Technical Report. _CoRR_, abs/2303.08774, 2023b. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. _CoRR_, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* Xu et al. (2020) Liang Xu, Xuanwei Zhang, and Qianqian Dong. CLUECCorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model. _CoRR_, abs/2003.01355, 2020b. URL https://arxiv.org/abs/2003.01355.
* Li et al. (2022) Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao, and Hui Zhang. CSL: A Large-scale Chinese Scientific Literature Dataset. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 3917-3923, Gyeongju, Republic of Korea, 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.344.

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.
* RyokoAI (2023) RyokoAI. ShareGPT52K, 2023. URL https://huggingface.co/datasets/RyokoAI/ShareGPT52K.
* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction Tuning with GPT-4. _CoRR_, abs/2304.03277, 2023. doi: 10.48550/arXiv.2304.03277. URL https://doi.org/10.48550/arXiv.2304.03277.
* Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering Large Language Models to Follow Complex Instructions. _CoRR_, abs/2304.12244, 2023. doi: 10.48550/arXiv.2304.12244. URL https://doi.org/10.48550/arXiv.2304.12244.
* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 320-335. Association for Computational Linguistics, 2022b. doi: 10.18653/v1/2022.acl-long.26. URL https://doi.org/10.18653/v1/2022.acl-long.26.
* Scao et al. (2021) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoit Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. _CoRR_, abs/2211.05100, 2022. doi: 10.48550/arXiv.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100.
* Fleiss (1971) Joseph L Fleiss. Measuring nominal scale agreement among many raters. _Psychological bulletin_, 76(5):378, 1971.

## Appendix A Annotation guidelines

### Benchmark expert annotation

We hire experts as our annotators in order to filter and improve the quality of our generated data. Each sample contains a generated sample question, a generated true statement serving as the default correct answer to the question, a generated false statement serving as the default wrong answer, and a few paragraphs serving as the supporting knowledge. To make judgment for a sample, annotators are presented with a set of _guiding questions_; each annotator is required to make judgment on these guiding questions and make decision to each of them independently.

Annotators are asked to make judgments on the guiding questions in the order they are presented. Specifically, for each sample, the annotators should consider the following guiding questions:

1. Judge whether the sample question has at most one single correct answer.8 Footnote 8: We believe that, even though an answer to the sample question can have various superficial forms, the facts/information behind all true answers should be the same. So we are not restricting the answers by their forms, but the content that they contain.
2. Judge whether the description or claim made in the sample question aligns with medical norms and is non-controversial.
3. Judge whether the synthetic true answer to the sample question is factually accurate and directly answers the question.
4. Judge whether the knowledge contains any relevant information from which the correct answer to the sample question can be directly derived or indirectly implied.

For each guiding question mentioned above, the annotators are required to assign 1 to it if the corresponding requirements are properly met, otherwise 0.

We hire three experts for this annotation; one of them serves as the meta-annotator for the purpose of resolving disagreements between the rest of the annotators. In principle, the meta-annotator will only need to make judgments on specific guiding questions where disagreements arise. All the annotators are required to make judgment on each sample independently, but with access to any necessary resources. All annotators are suggested to finish the annotation for each sample within 5 minutes. We report Fleiss' kappa [16] between the two annotators and present the agreement scores in Table 8.

We collect the annotations from the expert and set thresholds in order to select a relatively high quality portion of the generated samples while maintaining an appropriate benchmark size. Originally we have 5,779 samples; from them, we select 1,624 samples that 1) have at most one single correct answer, 2) have a clear description or claim in the sample question which aligns with medical norm and is non-controversial, 3) the generated answer to the question is factually correct and directly answer the sample question, and 4) have at least 3 pieces supporting knowledge (e.g., \(N_{K}\geq 3\)), and exclude the rest leveraging the above mentioned annotation procedure. We further

\begin{table}
\begin{tabular}{c c l} \hline \hline
**Q. index** & **Fleiss’ kappa** & **Interpretation** \\ \hline
1 & 0.657 & Substantial agreement \\
2 & 0.578 & Moderate agreement \\
3 & 0.818 & Almost perfect \\
4 & 0.786 & Substantial agreement \\ \hline \hline \end{tabular}
\end{table}
Table 8: Fleiss’ kappa between the two annotators. Here _Q. index_ represents the index of the guiding questions.

Figure 6: Size of samples remained after applying different thresholds. \(N_{K}\) stands for the number of supporting knowledge.

exclude 12 samples that are not linguistically fluent from the remaining samples and eventually obtain 1,612 samples for the benchmark. Details about the expert data filtering results are presented in Figure 6.

### Misinformation human evaluation

We ask the experts to do misinformation evaluation of Chinese LLMs in the topic of maternity and infant care domain using the sample questions from our proposed benchmark. We hire three experts to evaluate the answers from the Chinese LLMs independently; unlike the previous annotation, we don't have a meta-annotator in this part. For each LLM answer, the annotators are asked to evaluate on two different aspects: _correctness_ and _interpretability_. Annotators should assign a scalar between 0 and 1 to a sample for each criterion. Heuristic evaluation guidelines for each criterion are presented in Table 9. To evaluate the inter-annotator agreement, for both aspects, we first cast the scalar scores into four categories, i.e., _Very Low_ for scores between 0 and 0.25, _Low_ for scores between 0.25 and 0.5, _High_ for scores between 0.5 and 0.75, and _Very High_ for scores between 0.75 and 1. Under this setting, the Fleiss' kappa for correctness is 0.755 (substantial agreement) and 0.573 (moderate agreement) for interpretability.

## Appendix B Details of CARE-MI

### Benchmark Statistics

We additionally present benchmark statistics regarding the evidence length in Figure 7. We observe that the number of retrieved knowledge from MLEC-QA and MEDQA has a incredibly similar distribution. questions collected from BIOS are accompanied with the largest number of knowledge, as most of them (around 40%) have 6 pieces of valid supportive knowledge.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Metric** & **Range** & **Description** \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & 0.75 - 1.0 & The answer is in general factually correct, clear and directly resolving the question. \\  & 0.5 - 0.75 & The answer contains some non-rigorous content, but directly addresses the question. \\  & 0.25 - 0.5 & The answer contains multiple descriptions or claims that are either incorrect or incomplete, but the correct answer can be inferred from the current answer. \\  & 0.0 - 0.25 & The answer contains a large amount of incorrect and incorrect descriptions or claims, and the correct answer is unable to be inferred from the current answer. \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & 0.75 - 1.0 & All descriptions or claims in the answer are relevant and reasonable to the conclusion drawn by the answer. \\  & 0.25 - 0.75 & The answer contains some descriptions or claims that are either unreasonable or irrelevant to the conclusion drawn by the answer. \\ \cline{1-1}  & 0.0 - 0.25 & The answer contains many descriptions or claims that are unreasonable and irrelevant to the conclusion drawn by the answer. \\ \hline \hline \end{tabular}
\end{table}
Table 9: Guidelines for evaluating the answers from LLMs given the benchmark questions. This guideline serves as the heuristic rules which all annotators should follow.

Figure 7: _Left_: the average number of knowledge for each source (average over all knowledge is shown in gray). _Right_: number of knowledge distribution for each source.

### KG triples

We present examples of the KG triples in Table 10.

### Topic filtering

We use an aggregated word-list to filter maternity and infant care domain related samples. Details about the word-list can be found in Table 11.

### Question generation

Choice of QG modelWe construct questions on top of generated true statements using LLMs for their ease of usage; the true statements should naturally serve as the correct answers to the generated questions, which requires that the generated questions refer to the same entities or events mentioned in the statements. We select the BIOS dataset for question generation model comparison and evaluate candidate models through a set of metrics:

1. **Fluency**: the sentence must be fluent in the Chinese language, e.g., it owns valid word order and reasonable sentence structure.
2. **Consistency**: the generated questions should ask about the same entities or events mentioned in the true statements.
3. **Leakage prevention**: a valid question should not directly contain the correct answer.

The generated question is considered incorrect if it fails in any of the above metrics.

We choose three LLMs available in Chinese for a preliminary comparative experiment: GPT-3.5-turbo [20], ChatGLM-6B [23] and ChatYuan [16]. Results are shown in Table 12. Among them, ChatYuan has the best accuracy as well as the best efficiency, and thus is selected for the question generation task.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Source** & **Head** & **Relation** & **Tail** \\ \hline \multirow{4}{*}{BIOS} & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \\ \text{end\textbackslash{}}\) & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \\ \text{end\textbackslash{}}\) \\  & endometriosis & can lead to & vomiting \\ \cline{2-3}  & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \\ \text{ therapeutic oxytocin}\end{bmatrix}\) & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \\ \text{is a}\end{bmatrix}\) & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \\ \text{hormone}\end{bmatrix}\) \\ \hline \multirow{4}{*}{CPubMed} & \(\begin{bmatrix}\pi\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\# \text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{\textbackslash{}}\#\text{ \textbackslash{}}\#\text

**Generation of TF and OE questions** Theoretically speaking, we can generate TF and OE questions for all samples. In our case, we consider the trade-off between employing a rule-based method and using a LLM; rule-based method allows for fast and accurate generation of statements with certain explicit patterns while a LLM can handle more complex patterns albeit with a slightly higher error rate. Since a large portion of the true statements from KG samples do not have explicit patterns, we omit the generation of OE questions for them. However, we believe that this compromise could be resolved with a more carefully designed prompt instruction, even though it might lead to higher cost.

### Experimental settings

Chinese LLM evaluationFor evaluation, we limit all models to generate a maximum of 2,048 tokens. To restrain the variation during generation, we set the temperature as low as possible; for the GPT family models, we set the temperature to 0; otherwise it is set to 0.01. For decoding, we apply greedy strategy for all evaluated models. We apply the default parameters for the rest of the settings.

Judgment modelWe fine-tune the judgment models to mimic expert evaluation on both aspects, correctness and interpretability. We experiment with BERT-Large, GPT-3-350M, GPT-3-6.7B and LLaMA-13B-T. LLaMA-13B-T is obtained by further pretraining the LLaMA-13B on additional corpus for 250 hours and fine-tuning on selected instruction-following tasks for 170 hours on 8 NVIDIA A100 GPUs. We use the OpenAI API to fine-tune GPT-3-350M and GPT-3-6.7B. For correctness, the cost is $350.46 and $228.48, and for interpretability, the cost is $30.56 and $229.20 correspondingly. Time utilized for fine-tuning LLaMA-13B-T is mostly 3 hours on both correctness and interpretability. We set the temperature to 0 for the GPT models and 0.01 for LLaMA-13B-T to ensure that the output is consistent. The maximum token length is 2,048 for the GPT models and 512 for BERT-Large; to ensure that the maximum length requirement is satisfied, the knowledge sentences are truncated correspondingly.

### Experiment result statistics

We report the average number of sentences and average number of tokens for each answer from each evaluated models regarding the question sources. The statistics are prese

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Models** & **All** & **BIOS** & **CPubMed** & **MLEC-QA** & **MEDQA** \\ \hline \multicolumn{6}{c}{_Average number of sentences_} \\ MOSS-16B-SFT [28] & 4.591 & 4.727 & 4.589 & 4.390 & 4.662 \\ ChatGLM-6B [27] & 6.814 & 6.318 & 7.153 & 6.917 & 6.866 \\ BELLE-7B-2M [18] & 3.546 & 3.636 & 3.973 & 3.355 & 3.218 \\ BELLE-7B-0.2M [18] & 2.240 & 2.046 & 2.100 & 2.367 & 2.447 \\ GPT-4 [3] & 4.922 & 4.818 & 5.367 & 4.741 & 4.762 \\ GPT-3-5-turbo [3] & 4.066 & 4.046 & 4.447 & 3.896 & 3.874 \\ LLaMA-13B-T [30] & 5.863 & 5.409 & 5.827 & 6.109 & 6.107 \\ All & 4.577 & 4.429 & 4.779 & 4.539 & 4.562 \\ \hline \multicolumn{6}{c}{_Average number of tokens_} \\ MOSS-16B-SFT [28] & 115.418 & 100.546 & 116.640 & 119.120 & 125.365 \\ ChatGLM-6B [27] & 224.395 & 185.091 & 238.013 & 236.917 & 237.558 \\ BELLE-7B-2M [18] & 83.895 & 85.318 & 99.293 & 78.892 & 72.076 \\ BELLE-7B-0.2M [18] & 33.841 & 20.727 & 29.333 & 41.199 & 44.102 \\ GPT-4 [3] & 134.475 & 118.636 & 148.953 & 134.799 & 135.510 \\ GPT-3.5-turbo [3] & 98.027 & 90.591 & 110.247 & 96.071 & 95.202 \\ LLaMA-13B-T [30] & 171.671 & 133.000 & 163.753 & 191.930 & 198.002 \\ All & 123.103 & 104.844 & 129.462 & 128.418 & 129.688 \\ \hline \hline \end{tabular}
\end{table}
Table 12: The result of the comparative pioneer experiment on BIOS dataset. _Acc._ stands for accuracy. The best performances are highlighted in bold.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **Acc.** & **Time (s)** \\ \hline GPT-3.5-turbo [3] & 0.77 & 74 \\ ChatGLM-6B [27] & 0.28 & 433 \\ ChatYuan [30] & **0.79** & **56** \\ \hline \hline \end{tabular}
\end{table}
Table 12: The result of the comparative pioneer experiment on BIOS dataset. _Acc._ stands for accuracy. The best performances are highlighted in bold.

in MCQA evaluation where the models only generate few tokens, the answers generated during our evaluation contain an average of 4.6 sentences and 123.1 tokens.

### Prompts for negated statement generation

We also apply GPT-3.5-turbo [20] to generate negated statements, with the temperature set to 0. The prompts are shown below.

[MISSING_PAGE_EMPTY:23]

### Prompt for fine-tuning the judgment models

For aspects including correctness and interpretability, we use the following prompts to fine-tune judgment models:

[question] [knowledge]... [knowledge] [answer] Here \(n\geq 3\) as each sample in our benchmark is guaranteed to have at least 3 pieces of supporting knowledge. The prompts are utilized for the fine-tuning of GPT-3-350M, GPT-3-6.7B and LLaMA-13B-T. All prompts utilized here are not tuned.