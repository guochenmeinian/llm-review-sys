# Evaluating the design space of diffusion-based generative models

Yuqing Wang

Simons Institute

University of California, Berkeley

yq.wang@berkeley.edu

&Ye He

School of Mathematics

Georgia Institute of Technology

yhe367@gatech.edu

&Molei Tao

School of Mathematics

Georgia Institute of Technology

mtao@gatech.edu

###### Abstract

Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in Karras et al. [30]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in Song et al. [46] is more preferable, but when it is less trained, the design in Karras et al. [30] becomes more preferable.

## 1 Introduction

Diffusion models became a very popular generative modeling approach in various domains, including computer vision [20; 7; 27; 28; 38; 51], natural language processing [6; 34; 37], various modeling tasks [15; 41; 55], and medical, biological, chemical and physical applications [3; 17; 43; 49; 23; 56] (see more surveys in [53; 11; 14]). Karras et al. [30] provided a unified empirical understanding of the derivations of model parameters, leading to new state-of-the-art performance. Karras et al. [31] further upgraded the model design by revamping the network architectures and replacing the weights of the network with an exponential moving average. As diffusion models gain wider usage, efforts to understand and enhance their generation capability become increasingly meaningful.

In fact, a rapidly increasing number of theoretical works already analyzed various aspects of diffusion models [32; 19; 52; 16; 12; 8; 18; 9; 13; 39; 44; 25]. Among them, a majority [32; 19; 52; 16; 12; 8; 18] focus on sampling/inference; more precisely, they assume the score error is within a certain accuracy threshold (i.e. the score function is well trained in some sense), and analyze the discrepancy between the distribution of the generated samples and the true one. Meanwhile, there are a handful of results [44; 25] that aim at understanding different facets of the training process. See more detailed discussions of existing theoretical works in Section 1.1.

However, as indicated in Karras et al. [30], the performance of diffusion models also relies on the interaction between design components in both training and sampling, such as the noise distribution, weighting, time and variance schedules, etc. While focusing individually on either the training or generation process provides valuable insights, a holistic quantification of the actual generation capability can only be obtained when both processes are considered altogether. Therefore, motivated by obtaining _deeper theoretical understanding of how to maximize the performance of diffusion models_, this paper aims at establishing a full generation error analysis, combining both the optimization and sampling processes, to partially investigate the design space of diffusion models.

More precisely, we focus on the variance exploding setting [46], which is also the foundation of continuous forward dynamics in Karras et al. [30]. Our main contributions are summarized as follows:

* For denoising score matching objective, we establish the exponential convergence of its gradient descent training dynamics (Theorem 1). We develop a new method for proving a key lower bound of gradient under the semi-smoothness framework [1; 35; 57; 58].
* We extend the sampling error analysis in [8] to the variance exploding case (Theorem 2), under only the finite second moment assumption (Assumption 3) of the data distribution. Our result applies to various variance and time schedules, and implies a sharp almost linear complexity in terms of data dimension under optimal time schedule.
* We conduct a full error analysis of diffusion models, combining training and sampling (Theorem 3).
* We qualitatively derive the theory for choosing the noise distribution and weighting in the training objective, which coincides with Karras et al. [30] (Section 4.1). More precisely, our theory implies that the optimal rate is obtained when the total weighting exhibits a similar "bell-shaped" pattern used in Karras et al. [30].
* We develop a theory of choosing time and variance schedules based on both training and sampling (Section 4.2). Indeed, when the score error dominates, i.e., the neural network is less trained and not very close to the true score, polynomial schedule [30] ensures smaller error; when sampling error dominates, i.e., the score function is well approximated, exponential schedule [46] is preferred.

Conclusions and limitations are in Appendix A.

### Related works

**Sampling.** There has been significant progress in quantifying the sampling error of the generation process of diffusion models, assuming the score function is already approximated within certain accuracy. Most existing works [e.g., 16; 12; 8] focused on the variance preserving (VP) SDEs, whose discretizations correspond to DDPM. For example, Benton et al. [8] is one of the latest results for the VPSDE-based diffusion models, and it only needs a very mild assumption: the data distribution has finite second moment. The iteration complexity is shown to be almost linear in the data dimension and polynomial in the inverse accuracy, under exponential time schedule. However, a limited amount of works [32; 24; 54] analyzed the variance exploding (VE) SDEs, whose discretizations correspond to Score Matching with Langevin dynamics (SMLD) [45; 46]. To our best knowledge, Yang et al. [54] obtained the best result so far for VE assuming the data distribution has bounded support: the iteration complexity is polynomial in the data dimension and the inverse accuracy, under the uniform time schedule. In contrast, our work only assumed that the data distribution has finite second moment, and by extending the stochastic localization approach in [8] to VESDE, we obtain an iteration complexity that is polynomial in the data dimension and the inverse accuracy, under more general time schedules as well. Note the improved complexity in terms of the inverse accuracy and the data dimension dependencies; in fact, under the exponential time schedule, our complexity is almost linear in the data dimension, which recovers the state-of-the-art result for VPSDE-based diffusion models.

**Training.** To our best knowledge, the only works that quantify the training process of the diffusion models are Shah et al. [44] and Han et al. [25]. Shah et al. [44] employed the DDPM formulation and considered data distributions as mixtures of two spherical Gaussians with various scales of separation, together with \(K\) spherical Gaussians with a warm start. Then the score function can be analytically solved, and they modeled it in a teacher-student framework solved by gradient descent. They also provided the sample complexity bound under these specific settings. In contrast, our results work for general data distributions for which the true score is unknown, and training analysis is combined with

Figure 1: Structure of this paper.

sampling analysis. Han et al. [25] considered the GD training of a two-layer ReLU neural network with the last layer fixed, and used the neural tangent kernel (NTK) approach to establish a first result on generalization error. They uniformly sampled the time points in the training objective, assumed that the Gram matrix of the kernel is away from 0 (implying a lower bound on the gradient), and lacked a detailed non-asymptotic characterization of the training process. In contrast, we use the deep ReLU network with \(L\) layers trained by GD and prove instead of assuming that the gradient is lower bounded by the objective function. Moreover, we obtain a non-asymptotic bound for the optimization error, and our bound is valid for general time and variance schedules, which allows us to obtain a full error analysis.

**Convergence of neural networks training.** The convergence analysis of neural networks under gradient descent has been a longstanding challenge and has been developed into an extensive field. Here we will only focus on results mostly related to the techniques used in this paper. One line of them is approaches directly based on neural tangent kernel (NTK) [22; 21; 5; 47; 36]. However, existing works in this direction focus more on either scalar output, or vector output but with only one layer trained under two-layer networks, which is insufficient for diffusion models. Another line of research also considers overparameterized models in a regime analogous to NTK, though not necessarily explicitly resorting to kernels. Instead, it directly quantifies the lower bound of the gradient [1; 35; 2; 57; 58] and uses a semi-smoothness property to prove exponential convergence. Our results align with the latter line, but we develop a new method for proving the lower bound of the gradient and adopt assumptions that are closer to the setting of diffusion models. See more discussions in Section 3.1.

### Notations

We denote \(\|\cdot\|\) to be the \(\ell^{2}\) norm for both vectors and matrices, and \(\|\cdot\|_{F}\) to be the Frobenius norm. For the discrete time points, we use \(t_{i}\) to denote the time point for forward dynamics and \(t_{i}^{\leftarrow}\) for backward dynamics. For the order of terms, we follow the theoretical computer science convention to use \(\mathcal{O}(\cdot),\Theta(\cdot),\Omega(\cdot)\). We also denote \(f\lesssim g\) if \(f\leq Cg\) for some universal constant \(C\).

## 2 Basics of diffusion-based generative models

In this section, we will introduce the basic forward and backward dynamics of diffusion models, as well as the denoising score matching setting under which a model is trained.

### Forward and backward processes

Consider a forward diffusion process that pushes an initial distribution \(P_{0}\) to Gaussian

\[dX_{t}=-f_{t}\,X_{t}\,dt+\sqrt{2\sigma_{t}^{2}}\,dW_{t},\] (1)

where \(dW_{t}\) is the Brownian motion, \(X_{t}\) is a \(d\)-dim. random variable, and \(X_{t}\sim P_{t}\). Under mild assumptions, the process can be reversed and the backward process is defined as follows

\[dY_{t}=\left(f_{T-t}\,Y_{t}+2\sigma_{T-t}^{2}\nabla\log p_{T-t}(Y_{t})\right) dt+\sqrt{2\sigma_{T-t}^{2}}\,d\tilde{W}_{t},\] (2)

where \(Y_{0}\sim P_{T}\), and \(p_{t}\) is the density of \(P_{t}\). Then \(Y_{T-t}\) and \(X_{t}\) have the same distribution with density \(p_{t}\)[4], which means the dynamics (2) will push (near) Gaussian distribution back to (nearly) the initial distribution \(P_{0}\). To apply the backward dynamics for generative modeling, the main challenge lies in approximating the term \(\nabla\log p_{T-t}(Y_{t})\) which is called _score function_. It is common to use a neural network to approximate this score function and learn it via the forward dynamics (1); then, samples can be generated by simulating the backward dynamics (2).

### The training of score function via denoising score matching

In order to learn the score function, a natural starting point is to consider the following score matching objective [e.g., 29]

\[\mathcal{L}_{\text{conti}}(\theta)=\frac{1}{2}\int_{t_{0}}^{T}w(t)\mathbb{E} _{X_{t}\sim P_{t}}\|S(\theta;t,X_{t})-\nabla_{x}\log p_{t}(X_{t})\|^{2}\,dt\] (3)

where \(S(\theta;t,X_{t})\) is a \(\theta\)-parametrized neural network, \(w(t)\) is some _weighting function_, and the subscript means this is the continuous setup. Ideally one would like to minimize this objective function to obtain \(\theta\); however, \(p_{t}\) in general is unknown, and so is the true score function \(\nabla_{x}\log p_{t}\). One of the solutions is denoising score matching proposed by Vincent [48], where one, instead of directly matching the true score, leverages conditional score for which initial condition is fixed so that \(p_{t|0}\) is analytically known.

More precisely, given the linearity of forward dynamics (1), its exact solution is explicitly known: Let \(\mu_{t}=\int_{0}^{t}f_{s}\,ds\), and \(\bar{\sigma}_{t}^{2}=2\int_{0}^{t}e^{2\mu_{s}-2\mu_{t}}\sigma_{s}^{2}\,ds\). Then the solution is \(X_{t}=e^{-\mu_{t}}X_{0}+\bar{\sigma}_{t}\xi,\) where \(\xi\sim\mathcal{N}\left(0,I\right)\). We also have \(X_{t}|X_{0}\sim\mathcal{N}\left(e^{-\mu_{t}}X_{0},\bar{\sigma}_{t}^{2}I\right)\) and \(g_{t}(x|y)=(2\pi\bar{\sigma}_{t}^{2})^{-d/2}\exp(-\|x-e^{-\mu_{t}}y\|^{2}/(2 \bar{\sigma}_{t}^{2}))\), which is the density of \(X_{t}|X_{0}\). Then the objective can be rewritten as

\[\mathcal{L}_{\mathrm{conti}}(\theta) =\frac{1}{2}\int_{t_{0}}^{T}w(t)\mathbb{E}_{X_{0}}\mathbb{E}_{X_{ t}|X_{0}}\|S(\theta;t,X_{t})-\nabla\log g_{t}(X_{t}|X_{0})\|^{2}dt+\frac{1}{2} \int_{t_{0}}^{T}w(t)C_{t}dt\] \[=\frac{1}{2}\int_{t_{0}}^{T}w(t)\frac{1}{\bar{\sigma}_{t}} \mathbb{E}_{X_{0}}\mathbb{E}_{\xi}\|\bar{\sigma}_{t}S(\theta;t,X_{t})+\xi\|^{ 2}dt+\frac{1}{2}\int_{t_{0}}^{T}w(t)C_{t}dt\] (4)

where \(C_{t}=\mathbb{E}_{X_{t}}\|\nabla\log p_{t}\|^{2}-\mathbb{E}_{X_{0}}\mathbb{E }_{X_{t}|X_{0}}\|\nabla\log g_{t}(X_{t}|X_{0})\|^{2}.\) For completeness, we will provide a detailed derivation of these results in Appendix C and emphasize that it is just a review of existing results in our notation.

Throughout this paper, we adopt the variance exploding (VESDE) setting [46], where \(f_{t}=0\) and hence \(\mu_{t}=0\), which also aligns with the setup in the classic of EDM [30].

## 3 Error analysis for diffusion-based generative models

In this section, we will quantify both the training and sampling processes, and then integrate them into a more comprehensive generation error analysis.

### Training

In this section, we consider a practical implementation of denoising score matching objective, represent the score by a deep ReLU network, and establish the exponential convergence of GD training dynamics.

**Training objective function.** Consider a quadrature discretization of the time integral in (4) based on deterministic1 collocation points \(0<t_{0}<t_{1}<t_{2}<\cdots<t_{N}=T\). Then

Footnote 1: Otherwise it is no longer GD training but stochastic GD.

\[\mathcal{L}_{\mathrm{conti}}(\theta)\approx\bar{\mathcal{L}}(\theta)+\bar{C},\] (5)

where \(\bar{C}=\frac{1}{2}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})C_{t_{j}}\), and

\[\bar{\mathcal{L}}(\theta)=\frac{1}{2}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1}) \frac{1}{\bar{\sigma}_{t_{j}}}\mathbb{E}_{X_{0}}\mathbb{E}_{\xi}\|\bar{\sigma }_{t_{j}}S(\theta;t_{j},X_{t_{j}})+\xi\|^{2}.\] (6)

Define \(\beta_{j}=w(t_{j})(t_{j}-t_{j-1})\frac{1}{\bar{\sigma}_{t_{j}}}\) to be the _total weighting_. Consider the empirical version of \(\bar{\mathcal{L}}\) (6). Denote the initial data to be \(\{x_{i}\}_{i=1}^{n}\) with \(x_{i}\sim P_{0}\), and the noise to be \(\{\xi_{ij}\}_{N=1}^{N}\) with \(\xi_{ij}\sim\mathcal{N}(0,I_{d})\). Then the input data of the neural network is \(\{t_{j},X_{ij}\}_{i=1,j=1}^{n,N}=\{t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij}\}_ {i=1,j=1}^{n,N}\) and the output data is \(\{\xi_{ij}/\bar{\sigma}_{t_{j}}\}_{i=1,j=1}^{n,N}\) if \(\bar{\sigma}_{t_{j}}\neq 0\). Consequently, \(\bar{\mathcal{L}}(\theta)\) (6) can be approximated by the following

\[\bar{\mathcal{L}}_{em}(\theta)=\frac{1}{2n}\sum_{i=1}^{n}\sum_{j=1}^{N}\beta_{ j}\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij})+\xi_{ij} \|^{2}.\] (7)

We will use (7) as the training objective function in our analysis. For simplicity, we also denote \(f(\theta;i,j)=\beta_{j}|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_ {t_{j}}\xi_{ij})+\xi_{ij}|^{2}\) and then \(\bar{\mathcal{L}}_{em}(\theta)=\frac{1}{2n}\sum_{i=1}^{n}\sum_{j=1}^{N}f( \theta;i,j)\). Note the time dependence can be absorbed into the \(X\) dependence. More precisely, because \(\bar{\sigma}_{t}\) is a monotonically increasing function of \(t\), we can replace \(t_{j}\) in the inputs by \(\bar{\sigma}_{t_{j}}\) to indicate the time dependence. This is then equivalent to augmenting \(X_{ij}\) to be \(d+1\) dimensional with \((x_{i})_{d+1}:=0\) and \((\xi_{ij})_{d+1}:=1\). For simplified presentation, we will slightly abuse notation and still use \(d\) as the input dimension rather than \(d+1\).

**Architecture.** The analysis of diffusion model training is in general very challenging. One obvious factor is the complex score parameterizations used in practice such as U-Net [42] and transformers [40; 34]. In this paper, we simplify the architecture and consider deep feedforward networks. Although it is still far from practical usage, note this simple structure can already provide insights about the design space, as shown in later sections, and is more complicated than existing works [25; 44] related to the training of diffusion models (see Section 1.1). More precisely, we consider the standard deep ReLU network with bias absorbed:

\[S(\theta;t_{j},X_{ij})=W_{L+1}\sigma(W_{L}\cdots W_{1}\sigma(W_{0}X_{ij})),\] (8)where \(\theta=(W_{0},\cdots,W_{L+1})\), \(W_{0}\in\mathbb{R}^{m\times d},W_{L+1}\in\mathbb{R}^{d\times m}\), \(W_{\ell}\in\mathbb{R}^{m\times m}\) for \(\ell=1,\cdots,L\), and \(\sigma(\cdot)\) is the ReLU activation.

**Algorithm.** Let \(\theta^{(k)}=(W_{0},W_{1}^{(k)},\cdots,W_{L}^{(k)},W_{L+1})\). We consider the gradient descent (GD) algorithm as follows

\[\theta^{(k+1)}=\theta^{(k)}-h\nabla\bar{\mathcal{L}}_{em}(\theta^{(k)}),\] (9)

where \(h>0\) is the learning rate. We fix \(W_{0}\) and \(W_{L+1}\) throughout the training process and only update \(W_{1},\cdots,W_{L}\), which is a commonly used setting in the convergence analysis of neural networks [1, 10, 25].

**Initialization.** We employ the same initialization as in Allen-Zhu et al. [1], which is to set \(\big{(}W_{\ell}^{(0)}\big{)}_{ij}\sim\mathcal{N}(0,\frac{2}{m})\) for \(\ell=0,\cdots,L\), \(i,j=1,\cdots,m\), and \((W_{L+1}^{(0)})_{ij}\sim\mathcal{N}(0,\frac{1}{d})\) for \(i=1\cdots,d\), \(j=1\cdots,m\).

For this setup, the main challenge in our convergence analysis for denoising score matching lies in the nature of the data. 1) The output data that neural network tries to match is an unbounded Gaussian random vector, and cannot be rescaled as assumed in many theoretical works (for example, Allen-Zhu et al. [1] assumed the output data to be of order \(o(1)\)). 2) The input data \(X_{ij}\) is the sum of two parts: \(x_{i}\) which follows the initial distribution \(P_{0}\), and a Gaussian noise \(\bar{\sigma}_{t_{j}}\xi_{ij}\). Therefore, any assumption on the input data needs to agree with this noisy and unbounded nature, and commonly used assumptions like data separability [1, 35] can no longer be used.

To deal with the above issues, we instead make the following assumptions.

**Assumption 1** (On network hyperparameters and initial data of the forward dynamics).: _We assume the following holds:_

_1. Data scaling:_ \(\|x_{i}\|=\Theta(d^{1/2})\) _for all_ \(i\)_._

_2. Input dimension:_ \(d=\Omega(\text{poly}(\log(nN)))\)_._

We remark that the first assumption focuses only on the initial data \(x_{i}\) instead of the whole solution of the forward dynamics \(X_{ij}\) which incorporates the Gaussian noise. Also, this assumption is indeed not far away from reality; for example, it holds with high (at least \(1-\mathcal{O}(\exp(-\Omega(d)))\)) probability for standard Gaussian random vectors. The requirement for input dimension \(d\) is to ensure that \(d\) is not too small, or equivalently the number of data points is not exponential in \(d\).

We also make the following assumptions on the hyperparameters of the denoising score matching.

**Assumption 2** (On the design of diffusion models).: _We assume the following holds:_

_1. Weighting:_ \(\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}=\mathcal{O}(N)\)_._

_2. Variance:_ \(\bar{\sigma}_{t_{0}}>0\) _and_ \(\bar{\sigma}_{t_{N}}=\Theta(1)\)_._

The first assumption is to guarantee that the weighting function \(w(t)\) is properly scaled. This expression \(w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\) is obtained from proving the upper and lower bounds of the gradient of (7), and is different from the total weighting \(\beta_{i}\) defined above. In the second assumption, \(\bar{\sigma}_{t_{0}}>0\) ensures the output \(\xi_{ij}/\bar{\sigma}_{t_{j}}\) is well-defined. The \(\bar{\sigma}_{t_{N}}=\Theta(1)\) guarantees that the scales of the noise \(\bar{\sigma}_{t_{j}}\xi_{ij}\) and the initial data \(x_{i}\) are of the same order at the end of the forward process, namely, the initial data \(x_{i}\) is eventually push-forwarded to near Gaussian with the proper variance. Therefore, Assumption 2 aligns with what has been used in practice (see Section 4 and Karras et al. [30], Song et al. [46] for examples).

The following theorem summarizes our convergence result for the training of the score function.

**Theorem 1** (Convergence of GD).: _Define a set of indices to be \(\mathcal{G}^{(s)}=\{(i,j)|f(\theta^{(s)};i,j)\geq f(\theta^{(s)};i^{\prime},j^ {\prime})\text{ for all }i^{\prime},j^{\prime})\). Then given Assumption 1 and 2, for any \(\epsilon_{\rm train}>0\), there exists some \(M(\epsilon_{\rm train})=\Omega\left(\text{poly}\big{(}n,N,d,L,T/t_{0},\log( \frac{1}{\epsilon_{\rm train}})\big{)}\right)\), s.t., when \(m\geq M(\epsilon_{\rm train})\), \(h=\Theta(\frac{nN}{m\min_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}})\), and \(k=\mathcal{O}(d^{\frac{1-\alpha_{0}}{2}}n^{2}N\log(\frac{d}{\epsilon_{\rm train }}))\), with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d^{2\alpha_{0}-1}))\), we have_

\[\bar{\mathcal{L}}_{em}(\theta^{(k)})\leq\prod_{s=0}^{k-1}\left(1-C_{5}h\;w(t_{j ^{*}(s)})(t_{j^{*}(s)}-t_{j^{*}(s)-1})\bar{\sigma}_{t_{j^{*}(s)}}\left(\frac{ md^{\frac{\alpha_{0}-1}{2}}}{n^{3}N^{2}}\right)\right)\bar{\mathcal{L}}_{em}( \theta^{(0)})\]

_where the universal constant \(C_{5}>0\), \(a_{0}\in\big{(}\frac{1}{2},1\big{)}\), and \((i^{*}(s),j^{*}(s))=\arg\max_{(i,j)\in\mathcal{G}^{(s)}}w(t_{j})(t_{j}-t_{j-1}) \bar{\sigma}_{t_{j}}\). Moreover, when \(K=\Theta(d^{\frac{1-\alpha_{0}}{2}}n^{2}N\log(\frac{d}{\epsilon_{\rm train}}))\),_

\[\bar{\mathcal{L}}_{em}(\theta^{(K)})\leq\epsilon_{\rm train}.\]The above theorem implies that for denoising score matching objective \(\bar{\mathcal{L}}_{em}(\theta)\), GD has exponential convergence. For example, if we simply take \(j^{*}=\min_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\), then \(\bar{\mathcal{L}}_{em}(\theta^{(k+1)})\) is further upper bounded by \(\left(1-C_{6}h\ w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_{j^{*}}} \left(\frac{md^{\frac{\alpha_{0}-1}{2}}}{n^{3}N^{2}}\right)\right)^{k+1}\bar{ \mathcal{L}}_{em}(\theta^{(0)})\). The rate of convergence can be interpreted in the following way: 1) at the \(k\)th iteration, we collect all the indices of the time points into \(\mathcal{G}^{(k)}\) where \(f(\theta;i,j)\) has the maximum value; 2) we then choose the maximum of \(w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\) among all such indices and denote the index to be \(j^{*}(k)\), and obtain the decay ratio bound for the next iteration as \(1-C_{6}h\ w(t_{j^{*}(k)})(t_{j^{*}(k)}-t_{j^{*}(k)-1})\bar{\sigma}_{t_{j^{*}(k) }}\left(\frac{md^{\frac{\alpha_{0}-1}{2}}}{n^{3}N^{2}}\right)\).

**Remark 1** (Can \(\epsilon_{\mathrm{train}}\) be arbitrarily small? Some ramifications of the denoising setting).: _Let us first see some facts about \(\bar{\mathcal{L}}_{em}\) and \(\bar{\mathcal{L}}\). Under minimal assumption of the existence of score function and in the zero-time-discretization-error limit, the score matching objective can be made zero and therefore the denoising score matching objective is bounded below by \(-\bar{C}\), which is nonnegative and zero only when the data distribution is extremely special (we thus write \(-\bar{C}>0\) from hereon unless confusion arises). That is, \(\min_{\theta}\bar{\mathcal{L}}(\theta)\geq\min_{\text{any function}}\bar{\mathcal{L}}=-\bar{C}>0\) according to (4). Since \(\bar{\mathcal{L}}_{em}\to\bar{\mathcal{L}}\) as the sample size of the training data set \(n\to\infty\), we have \(\bar{\mathcal{L}}_{em}\geq-\bar{C}-c_{n}>0\) for some constant \(c_{n}>0\) and \(c_{n}\to 0\) as \(n\to\infty\)._

_However, Theorem 1 seems to imply \(\bar{\mathcal{L}}_{em}(\theta^{(k)})\to 0\) as \(k\to\infty\) since \(\bar{\mathcal{L}}_{em}(\theta^{(K)})\leq\epsilon_{\mathrm{train}}\) and \(\epsilon_{\mathrm{train}}\) is arbitrary, and it seems to contradict the \(-\bar{C}>0\) lower bound. However, there is no contradiction due to the combination of two facts. First, the theorem states that for arbitrary \(\epsilon_{\mathrm{train}}>0\), there exists a critical size, such that for overparameterized network beyond this size, GD can render the loss \(\bar{\mathcal{L}}_{em}(\theta)\) eventually no greater than \(\epsilon_{\mathrm{train}}\). If we fix the network size, i.e., with \(m,L,d\) given, then \(K\) is given, and Theorem 1 says nothing about GD's behavior after \(K\) iterations. That is, we do not know whether \(\limsup_{k\to\infty}\bar{\mathcal{L}}_{em}(\theta^{(k)})=0\). Second, our optimization setting requires the sample size \(n\) to be smaller than the network width \(m\) (Assumption 1). Thus, when \(m\) is fixed, the sample size \(n\) is upper bounded._

_The above discussion implies, within the validity of our theory, for any fixed network width \(m\), if \(\epsilon_{\mathrm{train}}\) is small, the sample size \(n\) cannot be too large, meaning \(\bar{\mathcal{L}}_{em}(\theta)-\bar{\mathcal{L}}(\theta)\) may not be small. Therefore, we can simultaneously have \(\bar{\mathcal{L}}_{em}(\theta)\) close to 0 and \(\bar{\mathcal{L}}(\theta)\) close to \(-\bar{C}>0\)._

**Main technical steps for proving Theorem 1.** The proof of Theorem 1 is in Appendix D, where the analysis framework is adapted from Allen-Zhu et al. [1]. Roughly speaking, the key proof in this framework is to establish the lower bound of the gradient. Then by integrating it into the semi-smoothness property of the neural network, we can obtain the exponential rate of convergence of gradient descent. For the lower bound of gradient, we develop a new method to deal with the difficulties in the denoising score matching setting (see the discussions earlier in this section).

Our new proof technique adopts a different decoupling of the gradient and leverages a high probability bound based on a high-dimensional geometric idea. See Appendix D.1 for a proof sketch and more details.

### Sampling

In this section, we establish a nonasymptotic error bound of the backward process in the variance exploding setting, which is an extension to Benton et al. [8]. For simplified notations, denote the backward time schedule as \(\{t_{j}^{\leftarrow}\}_{0\leq j\leq N}\) such that \(0=t_{0}^{\leftarrow}<t_{1}^{-}<\cdots<t_{N}^{\leftarrow}=T-\delta\).

**Generation algorithm**. We consider the exponential integrator scheme for simulating the backward SDE (2) with \(f_{t}\equiv 0\)2. The generation algorithm can be piecewisely expressed as a continuous-time SDE: for any \(t\in\left[t_{j}^{-},t_{j+1}^{\leftarrow}\right)\),

Footnote 2: The exponential integrator scheme is degenerate since \(f_{t}\equiv 0\). Time discretization is applied when we evaluate the score approximations \(\{S(\theta;t,\bar{Y}_{t})\}\).

\[d\bar{Y}_{t}=2\sigma_{T-t}^{2}S(\theta;T-t_{j}^{\leftarrow},\bar{Y}_{t_{j}^{ \leftarrow}})dt+\sqrt{2\sigma_{T-t}^{2}}d\bar{W}_{t}.\] (10)

**Initialization**. Denote \(q_{t}:=\text{Law}(\bar{Y}_{t})\) for all \(t\in\left[0,T-\delta\right]\). We choose the Gaussian initialization, \(q_{0}=\mathcal{N}(0,\bar{\sigma}_{T}^{2})\).

Our convergence result relies on the following assumption.

**Assumption 3**.: _The distribution \(P_{0}\) has a finite second moment: \(\mathbb{E}_{x\sim P_{0}}[\|x\|^{2}]=\mathrm{m}_{2}^{2}<\infty\)._

Next we state the main convergence result, whose proof is provided in Appendix E.

**Theorem 2**.: _Under Assumption 3, for any \(\delta\in(0,1)\) and \(T>1\), we have_

\[\text{KL}(p_{\delta}|q_{T-\delta})\lesssim\underbrace{\frac{\mathrm{ m}_{2}^{2}}{\bar{\sigma}_{T}^{2}}}_{\bar{E}_{I}}+\underbrace{\sum_{j=0}^{N-1} \gamma_{j}\sigma_{T-t_{j}^{-}}^{2}\mathbb{E}_{Y_{t_{j}^{-}}\to p_{T-t_{j}^{-}} }[|S(\theta;T-t_{j}^{-},Y_{t_{j}^{+}})-\nabla\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-} })|^{2}]}_{E_{S}}\] (11) \[+d\underbrace{\sum_{j=0}^{N-1}\gamma_{j}\int_{t_{j}^{-}}^{t_{j+1} ^{-}}\frac{\sigma_{T-t}^{4}}{\bar{\sigma}_{T-t}^{4}}dt+\mathrm{m}_{2}^{2}\frac {\int_{0}^{t_{1}^{-}}\sigma_{T-t}^{2}dt}{\bar{\sigma}_{T}^{4}}+(\mathrm{m}_{2 }^{2}+d)\sum_{j=1}^{N-1}(1-e^{-\bar{\sigma}_{T-t_{j}^{-}}^{2}})\frac{\bar{ \sigma}_{T-t_{j}^{-}}^{4}-\bar{\sigma}_{T-t_{j+1}^{-}}^{2}}{\bar{\sigma}_{T-t_ {j-1}^{-}}^{2}\bar{\sigma}_{T-t_{j}^{-}}^{4}}}_{\bar{T-t_{j-1}^{-}}^{2}\bar{ \sigma}_{T-t_{j}^{-}}^{4}}.\]

_where \(\gamma_{j}:=t_{j+1}^{*}-t_{j}^{-}\) for all \(j=0,1,\cdots,N-1\) is the stepsize of the generation algorithm in (10)._

Theorem 2 is a VESDE-based diffusion model's analogy of what's proved in Benton et al. [8] for VPSDE-based diffusion model, only requiring the data distributions to have finite second moments, and it achieves the sharp almost linear data dimension dependence under the exponential time schedule. The major differences from [8] are (1) the initialization error in the VESDE case is handled differently (see Lemma 10); (2) Theorem 2 applies to varies choices of time schedules, which enables to investigate the design space of the diffusion model, as we will discuss in Section 4. Worth mentioning is, Yang et al. [54] also obtained polynomial complexity results for VESDE-based diffusion models with uniform stepsize, but under stronger data assumption (assuming compact support). Compared to their result, complexity implied by Theorem 2 has better accuracy and data dimension dependencies. A detailed discussion on complexities is given in Appendix I.1.

Terms \(E_{I},E_{D},E_{S}\) in (11) represent the three types of errors: initialization error, discretization error, and score estimation error, respectively. Term \(E_{I}\) quantifies the error between the initial density of the sampling algorithm \(q_{0}\) and the ideal initialization \(p_{T}\), which is the density when the forward process stops at time \(T\). Term \(E_{D}\) is the error stemming from the discretization of the backward dynamics. Term \(E_{S}\) characterizes the error of the estimated score function and the true score, and is related to the optimization error of \(\bar{\mathcal{L}}_{em}\). Important to note is, in Theorem 2, population loss is needed instead of the empirical version \(\bar{\mathcal{L}}_{em}\) (7). Besides this, the weighting \(\gamma_{j}\sigma_{T-t_{j}^{-}}^{2}\) is not necessarily the same as the total weighting in \(\bar{\mathcal{L}}_{em}\) (7) \(\beta_{j}\), depending on choices of \(w(t_{j})\) and time and variance schedules (see more discussion in Section 4). We will later on integrate the optimization error (Theorem 1) into this score error \(E_{S}\) to obtain a full error analysis in Section 3.3.

**Remark 2** (sharpness of dependence in \(d\) and \(\mathrm{m}_{2}^{2}\)).: _In one of the simplest cases, when the data distribution is Gaussian, the score function is explicitly known. Hence \(\text{KL}(p_{\delta}|q_{T-\delta})\) can be explicitly computed as well, which verifies that the dependence of parameters \(d\) and \(\mathrm{m}_{2}^{2}\) is sharp in \(E_{I}\) and \(E_{D}\)._

### Full error analysis

In this section, we combine the analyses from the previous two sections to obtain an end-to-end generation error bound.

Before providing the main result of this section, let us first clarify some terminologies.

**Time schedule, variance schedule, and total weighting.** The terms _time schedule_ and _variance schedule_ respectively refer to the choice of \(t_{j}^{-}\) and \(\bar{\sigma}_{t_{j}}\) in sampling. Meanwhile, note both the training and sampling processes require the proper choices of time and variance, and these choices are not necessarily the same for both processes. For training, the effect of these two is integrated into the _total weighting_\(\beta_{j}\), which is also influenced by an additional weighting parameter \(w(t_{j})\). In this theoretical paper, when studying the generation error, we aim to apply the optimization result to better understand the effect of optimization on sampling. Therefore, to simplify the analysis and discussions in Section 4, we choose the same time and variance schedules for both training and sampling.

The main result is stated in the following.

**Theorem 3**.: _Under the same conditions as Theorem 1,2, and that \(K\) is such that GD reaches \(\epsilon_{\text{train}}\) in at most \(K\)th iterations, we have_

\[\text{KL}(p_{\delta}|q_{T-\delta})\lesssim E_{I}+E_{D}+\max_{1\leq j\leq N} \frac{\sigma_{t_{N-j}}^{2}}{w(t_{N-j})}\left(\epsilon_{\text{train}}+\epsilon_{ n}+\epsilon_{\text{est}}+\epsilon_{\text{approx}}\right)\]

_where \(E_{I},E_{D}\) are defined in Theorem 2, \(\epsilon_{\text{train}}\) is defined in Theorem 1, \(\epsilon_{n}=|\bar{\mathcal{L}}(\theta^{(K)})-\bar{\mathcal{L}}_{em}(\theta^{(K )})+\bar{\mathcal{L}}_{em}(\theta^{*})-\bar{\mathcal{L}}(\theta^{*})|\), \(\epsilon_{\text{est}}=|\bar{\mathcal{L}}(\theta^{*})-\bar{\mathcal{L}}(\theta _{\mathcal{F}})|\), \(\epsilon_{\text{approx}}=|\bar{\mathcal{L}}(\theta_{\mathcal{F}})+\bar{C}|\). In these terms, \(\bar{C}\) is defined in (5),\(\theta^{\star}=\arg\min_{\theta,s.t.,\bar{\mathcal{L}}_{em}(\theta)=0}\bar{ \mathcal{L}}(\theta)\) and \(\theta_{F}=\arg\inf_{\{\theta:S(\theta)\in\mathcal{F}\}}|\bar{\mathcal{L}}(\theta )+\bar{C}|\) with \(\mathcal{F}=\{\text{ReLU network function defined in (\ref{eq:S}), with }d=\Omega(\text{poly}(\log(nN))),m=\Omega\big{(}\text{poly} \big{(}n,N,d,L,T/t_{0}\big{)}\big{)}\}\)._

In this theorem, the discretization error \(E_{D}\) and initialization error \(E_{I}\) are the same as Theorem 2. For the score error \(E_{S}\), our optimization result is valid for general time schedules and therefore can directly fit into the sampling error analysis, which is in contrast to existing works [25; 44] (see more discussions in Section 1.1). The coefficient \(\max_{j}\sigma^{2}_{t_{N-j}}w(t_{N-j})\) results from different weightings in \(E_{S}\) and \(\bar{\mathcal{L}}_{em}\), i.e., \(\gamma_{j}\sigma^{2}_{T-t_{j}^{\star}}\) and \(\beta_{j}\). We will discuss the effect of \(\max_{j}\sigma^{2}_{t_{N-j}}/w(t_{N-j})\) under different time and variance schedules in Section 4.

The way we bound \(\mathbb{E}_{Y_{t_{j}^{\star}}-p_{T-t_{j}^{\star}}}[\|S(\theta;T-t_{j}^{\star}, Y_{t_{j}^{\star}})-\nabla\log p_{T-t_{j}^{\star}}(Y_{t_{j}^{\star}})\|^{2}]\) in \(E_{S}\) (see Theorem 2) is to decompose it into the optimization error \(\epsilon_{\rm train}\), statistical error \(\epsilon_{n}\), estimation error \(\epsilon_{\rm est}\), and approximation error \(\epsilon_{\rm approx}\). This gives clear intuition to results, but we also note it may not give a tight bound. In fact, we have

\[\epsilon_{n}+\epsilon_{\rm train} =|\bar{\mathcal{L}}(\theta^{(K)})-\bar{\mathcal{L}}_{em}(\theta ^{(K)})+\bar{\mathcal{L}}_{em}(\theta^{\star})-\bar{\mathcal{L}}(\theta^{ \star})|+|\bar{\mathcal{L}}_{em}(\theta^{(K)})-\bar{\mathcal{L}}_{em}(\theta^ {\star})|\] \[\geq\bar{\mathcal{L}}(\theta^{(K)})+\bar{\mathcal{L}}(\theta^{ \star})\geq 2\min_{\theta}\bar{\mathcal{L}}(\theta)\geq-2\bar{C}>0.\]

\(\epsilon_{n}\) can still be small if we take \(n\to\infty\), but that means \(\epsilon_{\rm train}\) has to be large, and our generation error bound cannot be made 0. It is unclear yet whether this is due to limitation of our analysis or intrinsic, and will be left for future investigation.

Another related note is, in this paper, we focus on \(\epsilon_{\rm train}\) and the effect of optimization, but the analyses of \(\epsilon_{n},\ \epsilon_{\rm est},\ \text{and}\ \epsilon_{\rm approx}\) are also important and possible [13; 39; 25; 50]. On the other hand, again, whether it is optimal to decompose the full error into these four is unclear.

To better see the parameter dependence of the error bound in Theorem 3, the following is an example with simplified results, where we employ the schedules in EDM [30].

**Corollary 1** (Full error analysis under EDM [30] designs).: _Under the same conditions as Theorem 3, we have_

\[KL(p_{\delta}|q_{T-\delta})\lesssim\frac{\mathrm{m}_{2}^{2}}{T^{2}}+\frac{da^ {2}T^{\frac{1}{a}}}{\delta^{\frac{1}{a}}N}+(\mathrm{m}_{2}^{2}+d)\left(\frac{a^ {2}T^{\frac{1}{a}}}{\delta^{\frac{1}{a}}N}+\frac{a^{3}T^{\frac{2}{a}}}{\delta ^{\frac{2}{a}}N^{2}}\right)+\frac{1}{N}\left(C_{9}+\left(1-C_{8}h\left(\frac{ md^{\frac{49-1}{2}}}{n^{3}N^{2}}\right)\right)^{K}\right),\]

_where \(C_{8},C_{9}>0\) and \(a=7\) in [30]._

4 Theory-based understanding of the design space and its relation to existing empirical counterparts

This section theoretically explores preferable choices of parameters in both training and sampling, and shows that they agree with the ones used in EDM [30] and Song et al. [46] in different circumstances.

### Choice of total weighting for training

This section develops the optimal total weighting \(\beta_{j}\) for training objective (7). We qualitatively show in two steps that "bell-shaped" weighting, which is the one used in EDM [30], will lead to the optimal rate of convergence: Step 1) \(|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{ij})+\xi_{ij}|\) as a function of \(j\) is inversely "bell-shaped"; Step 2) \(f(\theta;i,j)=\beta_{j}\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{ij})+\xi_{ij}\|\) should be close to each other for any \(i,j\).

1.1 Inversely "bell-shaped" loss \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{ij})+\xi_{ij}\|\) as a function of time index \(j\)

**Proposition 1**.: _Under the same assumptions as Theorem 1, for any \(\theta\) and \(i=1,\cdots,n\), we have_

1. \(\forall\epsilon_{1}>0\)_,_ \(\exists\ \delta_{1}>0\)_, s.t., when_ \(0\leq\bar{\sigma}_{t_{j}}<\delta_{1}\)_,_ \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij})+\xi_{ ij}\|>\|\xi_{ij}\|-\epsilon_{1}\)_._
2. \(\forall\epsilon_{2}>0\)_,_ \(\exists\ M>0\)_, s.t., when_ \(\bar{\sigma}_{t_{j}}>M\)_,_ \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij})+\xi_{ ij}\|\geq M^{2}(\|S(\theta;t_{j},\xi_{ij})\|-\epsilon_{2})\)_._

The above proposition can be interpreted in the following way. Given any network \(S\), when \(\bar{\sigma}_{t_{j}}\) is very small, 1 implies that \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi)+\xi_{ij}\|\) is away from 0 by approximately \(|\xi_{ij}|\) which is of order \(\sqrt{d}\) with high probability, i.e., it cannot be small. When \(\bar{\sigma}_{t_{j}}\) is large, 2 shows that as it becomes larger and larger, i.e., as \(M\) increases, \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi)+\xi_{ij}\|\) will also increase. Therefore, the function \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{ij})+\xi_{ij}\|\) has most likely an inversely "bell-shaped" curve in terms of \(j\) dependence.

#### 4.1.2 Ensuring comparable values of \(f(\theta;i,j)\) for optimal rate of convergence

**Corollary 2**.: _Under the same conditions as Theorem 1, for some large \(K^{\prime}>0,\) if \(|f(\theta^{(k+K^{\prime})};i,j)-f(\theta^{(k+K^{\prime})};l,s)|\leq\epsilon\) holds for all \(k>0\) and all \((i,j),(l,s)\), with some small universal constant \(\epsilon>0\), then we have, for some constant \(C_{7}>0\),_

\[\bar{\mathcal{L}}_{em}(\theta^{(k+K^{\prime})})\leq\left(1-C_{7}h\max_{j=1, \cdots,N}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\left(\frac{md^{\frac{a_{0 -1}}{2}}}{n^{3}N^{2}}\right)\right)^{k}\bar{\mathcal{L}}_{em}(\theta^{(K^{ \prime})}).\]

The above corollary shows that if \(f(\theta^{(k)};i,j)\)'s are almost the same for any \(i,j\), then the decay ratio of the next iteration is minimized. More precisely, the index set \(\mathcal{G}^{(k)}\) defined in Theorem 1 is roughly the whole set \(\{1,\cdots,N\}\), and therefore \(w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_{j^{*}}}\) can be taken as the maximum value over all \(j\), which consequently leads to the optimal rate.

#### 4.1.3 "Bell-shaped" weighting: our theory and EDM

Combining the above two aspects, the optimal rate of convergence leads to the choice of total weighting \(\beta_{j}\) such that \(f(\theta;i,j)=\beta_{j}\big{|}\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{t_{j}})+ \xi_{ij}\big{|}\) is close to each other; as a result, the total weighting should be chosen as a "bell-shaped" curve as a function of \(j\) according to the shape of the curve for \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{t_{j}})+\xi_{ij}\big{|}\).

Before comparing the preferable weighting predicted by our theory and the intuition-and-empirics-based one in EDM [30], let us first recall that the EDM training objective3 can be written as \(\mathbb{E}_{\bar{\sigma}\sim p_{\rm train}}\mathbb{E}_{y,n}\lambda(\bar{ \sigma})\|D_{\theta}(y+n;\bar{\sigma})-y\|^{2}\)

Footnote 3: In EDM [30], they use \(P_{\rm mean}=-1.2,\ P_{\rm std}=1.2,\ \sigma_{\rm data}=0.5,\ \bar{\sigma}_{\rm min}=0.002,\ \bar{\sigma}_{\rm max}=80\).

\[=\frac{1}{Z_{1}}\,\int e^{-\frac{(\log\bar{\sigma}-P_{\rm mean})^{2}}{2P_{ \rm std}^{2}}}\frac{\bar{\sigma}^{2}+\sigma_{\rm data}^{2}}{\bar{\sigma}\sigma _{\rm data}^{2}}\mathbb{E}_{X_{0},\xi}\|\bar{\sigma}s(\theta;t,X_{t})+\xi\|^{2 }\,d\bar{\sigma},\] (12)

where \(Z_{1}\) is a normalization constant, and we denote \(\beta_{\rm EDM}(\bar{\sigma})=e^{-\frac{(\log\bar{\sigma}-P_{\rm mean})^{2}}{2P_ {\rm std}^{2}}}\frac{\bar{\sigma}^{2}+\sigma_{\rm data}^{2}}{\bar{\sigma}\sigma _{\rm data}^{2}}\) to be the _total weighting_ of EDM. Note the dependence on \(\bar{\sigma}\) and time \(j\) can be freely switched due to their 1-to-1 correspondence.

Figure 2 plots the total weighting of EDM \(\beta_{\rm EDM}\) as a function of \(\bar{\sigma}\). As is shown in the picture, this is a "bell-shaped" curve4, which coincides with our choice of total weighting in the above theory. When \(\bar{\sigma}\) is very small or very large, according to Proposition 1, the lower bound of \(\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},X_{t_{j}})+\xi_{ij}\big{|}\) cannot vanish and therefore needs the smallest weighting over all \(\bar{\sigma}\). When \(\bar{\sigma}\) takes the middle value, the scale of the output data \(\xi_{ij}/\bar{\sigma}_{j}\) is roughly the same as the input data \(X_{ij}\) and therefore makes it easier for the neural network to fit the data, which admits larger weighting.

Footnote 4: This horizontal axis is in \(\log\)-scale and the plot in regular scale is a little bit skewed, not precisely a “bell” shape. However, we remark that the trend of the curve still matches our theory.

### Choice of time and variance schedules

This section will discuss the choice of time and variance schedules based on the three errors \(E_{S},E_{D},E_{I}\) in the error analysis of Section 3.3. Two situations will be considered based on how well the score function is approximated in training: when the network is less trained, \(E_{S}\) dominates and polynomial schedule [30] is preferable; when the score function is well approximated, \(E_{D}+E_{I}\) dominates and exponential schedule [46] is better.

#### 4.2.1 When score error \(E_{s}\) dominates

As is shown in Theorem 3, the main impact of different time and variance schedules on score error \(E_{S}\) appears in the term \(\max_{j}\sigma_{t_{N-j}}^{2}w(t_{N-j})\), when the score function is approximated to a certain accuracy. It remains to compute \(w(t)\) under various choices of schedules.

**General rule of constructing \(w(t)\).** To ensure fair comparisons between different time and variance schedules, we maintain a fixed total weighting in the training objective. Additionally, to facilitate comparisons with practical usage, we adopt the total weighting in EDM, i.e., \(\beta_{j}=C_{3}\beta_{\rm EDM}(\bar{\sigma}_{t_{j}}),\) for some universal constant \(C_{3}>0\). The reason for using the EDM total weighting is that according to Section 4.1, our total weighting \(\beta_{j}\) should be "bell-shaped" as a function of \(j\), which agrees qualitatively with the one used in EDM.

**Polynomial schedule [30] vs exponential schedule [46].** We fix \(\epsilon_{n},\epsilon_{\rm train}\) and apply the two schedules (Table 1) separately to the above total weighting \(\beta\) (hence \(w\)). Then, compute \(\max_{j}\sigma_{t_{N-j}}^{2}/w(t_{N-j})\) which is a factor in score error \(E_{S}\) (Thm.3) in Table 2. The Exp.'s result \(\frac{1}{2}\left(\bar{\sigma}_{\max}-\bar{\sigma}_{\max}\left(\frac{\bar{ \sigma}_{\min}^{2}}{\bar{\sigma}_{\max}^{2}}\right)^{1/N}\right)\) is larger5 than the Poly.'s result \(\left(\bar{\sigma}_{\max}-\left(\bar{\sigma}_{\max}^{1/\rho}-\frac{\bar{ \sigma}_{\max}^{1/\rho}-\bar{\sigma}_{\min}^{1/\rho}}{N}\right)^{\rho}\right)\) for large \(N\), meaning the poly. time schedule in EDM is better than the exp. schedule in [46]. Note these two terms are both of order \(1/N\) as \(N\to\infty\) and therefore the difference lies in their prefactors.

Footnote 5: This holds under parameters used in either Song et al. [46] or Karras et al. [30].

#### 4.2.2 When discretization error \(E_{d}\) and initialization error \(E_{l}\) dominate

In this section, we compare the two different schedules in Table 1 by studying the iteration complexity of the sampling algorithm, i.e., number of time points \(N\), when \(E_{D}\) + \(E_{I}\) dominates.

**General rules of comparison.** We consider the case when the discretization and initialization errors are bounded by the same quantity \(\epsilon\), i.e., \(E_{I}\) \(\widehat{\varepsilon}\). Then according to Theorem 2 and Theorem 3, we compute the iteration complexity for achieving this error using the two schedules in Table 1. To make the comparison more straightforward, we adopt \(T=t_{N}=\Theta(\text{poly}(\varepsilon^{-1}))\) and therefore \(\bar{\sigma}_{\max}=\Theta(\varepsilon^{-1/2})\). More details are provided in Appendix I.1.

**Polynomial schedule [30] vs exponential schedule [46].** As is shown in the last column of Table 2, the iteration complexity under exponential schedule [46] has the poly-logarithmic dependence on the ratio between maximal and minimal variance (\(\bar{\sigma}_{\max}/\bar{\sigma}_{\min}\))6, which is better than the complexity under polynomial schedule [30], which is polynomially dependent on \(\bar{\sigma}_{\max}/\bar{\sigma}_{\min}\). Both complexities are derived from Theorem 2 by choosing different parameters.

Footnote 6: The exponential time schedule under the variance schedule in [30] also has the poly-logarithmic dependence on \(\bar{\sigma}_{\max}/\bar{\sigma}_{\min}\). Under both variance schedules in [30] and [46], it can be shown that exponential time schedule is optimal. Details are provided in Appendix I.1.

**Remark 3** (The existence of optimal \(\rho\) in the polynomial schedule [30]).: _For fixed \(\bar{\sigma}_{\max}\) and \(\bar{\sigma}_{\min}\), the optimal \(\rho\) that minimizes the iteration complexity is \(\rho=\frac{1}{2}\ln\left(\frac{\bar{\sigma}_{\max}}{\bar{\sigma}_{\min}}\right)\). In [30], it was empirically observed that with fixed iteration complexity, there is an optimal value of \(\rho\) that minimizes the FID. Our result indicates that, for fixed \(\bar{\sigma}_{\max}\) and \(\bar{\sigma}_{\min}\), hence the desired accuracy in KL divergence being fixed, there is an optimal value of \(\rho\) that minimizes the iteration complexity to reach the fixed accuracy. Even though we consider a different metric/divergence instead of FID, our result still provides a quantitative support to the existence of optimal \(\rho\) observed in [30]._

## Acknowledgments and Disclosure of Funding

The authors are grateful for the partially support by NSF DMS-1847802, Cullen-Peck Scholarship, and GT-Emory Humanity.AI Award. We thank the anonymous reviewers for their helpful comments.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Variance schedule \(\bar{\sigma}_{t}\) & Time schedule \(t_{j}\) \\ \hline Poly. [30] & \(t\) & \(\left(\bar{\sigma}_{\max}^{1/\rho}-(\bar{\sigma}_{\max}^{1/\rho}-\bar{\sigma}_{ \min}^{1/\rho})\frac{N-j}{N}\right)^{\rho}\) \\ Exp. [46] & \(\sqrt{t}\) & \(\bar{\sigma}_{\max}^{2}\left(\frac{\bar{\sigma}_{\min}^{2}}{\bar{\sigma}_{ \max}^{2}}\right)^{\frac{N-j}{N}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Polynomial and exponential (time) schedules.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline  & \(E_{S}\) (score error) dominates & \(E_{D}\) + \(E_{I}\) (sampling error) dominates \\ \hline Poly. [30] & \(C_{4}\left(\bar{\sigma}_{\max}-\left(\bar{\sigma}_{\max}^{1/\rho}-\frac{\bar{ \sigma}_{\max}^{1/\rho}-\bar{\sigma}_{\min}^{1/\rho}}{N}\right)^{\rho}\right)\) & \(\checkmark\) & \(\Omega\left(\frac{\bar{\sigma}_{\max}^{2}}{d}\rho^{2}\left(\frac{\bar{\sigma}_ {\max}}{\bar{\sigma}_{\min}}\right)^{1/\rho}\bar{\sigma}_{\max}^{2}\right)\) \\ Exp. [46] & \(C_{4}\cdot\frac{1}{2}\left(\bar{\sigma}_{\max}-\bar{\sigma}_{\max}\left(\frac{ \bar{\sigma}_{\min}^{2}}{\bar{\sigma}_{\max}^{2}}\right)^{1/N}\right)\) & \(\Omega\left(\frac{\bar{\sigma}_{\max}^{2}}{d}\ln(\frac{\bar{\sigma}_{\max}}{ \bar{\sigma}_{\min}})^{2}\bar{\sigma}_{\max}^{2}\right)\) & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparisons between different schedules.

## References

* [1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019.
* [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. _Advances in neural information processing systems_, 32, 2019.
* [3] Namrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. _arXiv preprint arXiv:2205.15019_, 2022.
* [4] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [5] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* [6] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* [7] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=SlxSY2UZQT.
* [8] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In _The Twelfth International Conference on Learning Representations_, 2024.
* [9] Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. _arXiv preprint arXiv:2002.00107_, 2020.
* [10] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. _Advances in Neural Information Processing Systems_, 32, 2019.
* [11] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. A survey on generative diffusion models. _IEEE Transactions on Knowledge and Data Engineering_, 2024.
* [12] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _International Conference on Machine Learning_, pages 4735-4763. PMLR, 2023.
* [13] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In _International Conference on Machine Learning_, pages 4672-4712. PMLR, 2023.
* [14] Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. An overview of diffusion models: Applications, guided generation, statistical rates and optimization. _arXiv preprint arXiv:2404.07771_, 2024.
* [15] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=NsMLjcFa080.
* [16] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _ICLR_, 2023.
* [17] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. _Medical image analysis_, 80:102479, 2022.

* [18] Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: finite fisher information is all you need. _arXiv preprint arXiv:2308.12240_, 2023.
* [19] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _TMLR_, 2022.
* [20] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [21] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [22] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* [23] Chenru Duan, Yuanqi Du, Haojun Jia, and Heather J Kulik. Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. _Nature Computational Science_, 3(12):1045-1055, 2023.
* [24] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. _arXiv preprint arXiv:2401.17958_, 2024.
* [25] Yinbin Han, Meisam Razaviayan, and Renyuan Xu. Neural network-based score estimation in diffusion models: Optimization and generalization. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=h8Geq0xtd4.
* [26] Ye He, Kevin Rojas, and Molei Tao. Zeroth-order sampling methods for non-log-concave distributions: Alleviating metastability by denoising diffusion. _arXiv preprint arXiv:2402.17886_, 2024.
* [27] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _Journal of Machine Learning Research_, 23(47):1-33, 2022.
* [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _Advances in Neural Information Processing Systems_, 35:8633-8646, 2022.
* [29] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* [30] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [31] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. _arXiv preprint arXiv:2312.02696_, 2023.
* [32] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. _Advances in Neural Information Processing Systems_, 35:22870-22882, 2022.
* [33] Yongjae Lee and Woo Chang Kim. Concise formulas for the surface area of the intersection of two hyperspherical caps. _KAIST Technical Report_, 2014.
* [34] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. _Advances in Neural Information Processing Systems_, 35:4328-4343, 2022.
* [35] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018.

* Liu et al. [2022] Xin Liu, Zhisong Pan, and Wei Tao. Provable convergence of nesterov's accelerated gradient method for over-parameterized neural networks. _Knowledge-Based Systems_, 251:109277, 2022.
* Lou et al. [2023] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. _arXiv preprint arXiv:2310.16834_, 2023.
* Meng et al. [2022] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=aBsCjcPu_tE.
* Oko et al. [2023] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In _International Conference on Machine Learning_, pages 26517-26582. PMLR, 2023.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer, 2015.
* Schneuing et al. [2022] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lio, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. _arXiv preprint arXiv:2210.13695_, 2022.
* Shah et al. [2023] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the ddpm objective. _Advances in Neural Information Processing Systems_, 36:19636-19649, 2023.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _International Conference on Learning Representations_, 2021.
* Song and Yang [2019] Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. _arXiv preprint arXiv:1906.03593_, 2019.
* Vincent [2011] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* Watson et al. [2023] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* Wibisono et al. [2024] Andre Wibisono, Yihong Wu, and Kaylee Yingxi Yang. Optimal score estimation via empirical bayes smoothing. _arXiv preprint arXiv:2402.07747_, 2024.
* Wu et al. [2023] Junde Wu, RAO FU, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, and Yanwu Xu. Medsegdiff: Medical image segmentation with diffusion probabilistic model. In _Medical Imaging with Deep Learning_, 2023. URL https://openreview.net/forum?id=Jdw-cm2jG9.
* Yang and Wibisono [2022] Kaylee Yingxi Yang and Andre Wibisono. Convergence in KL and Renyi divergence of the unadjusted langevin algorithm using estimated score. _NeurIPS Workshop on Score-Based Methods_, 2022.

* [53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Computing Surveys_, 56(4):1-39, 2023.
* [54] Ruofeng Yang, Zhijie Wang, Bo Jiang, and Shuai Li. The convergence of variance exploding diffusion models under the manifold hypothesis. _OpenReview_, 2024.
* [55] Jongmin Yoon, Sung Ju Hwang, and Juho Lee. Adversarial purification with score-based generative models. In _International Conference on Machine Learning_, pages 12062-12072. PMLR, 2021.
* [56] Yuchen Zhu, Tianrong Chen, Evangelos A Theodorou, Xie Chen, and Molei Tao. Quantum state generation with structure-preserving diffusion model. _arXiv preprint arXiv:2404.06336_, 2024.
* [57] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. _Advances in neural information processing systems_, 32, 2019.
* [58] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep relu networks. _Machine learning_, 109:467-492, 2020.

## Appendix A Conclusions and limitations

**Conclusions.** In this paper, we provide a first full error analysis incorporating both optimization and sampling processes. For the training process, we provide a first result under a deep neural network and prove the exponential convergence into a neighborhood of minima. At the same time, we extend the current analysis to the variance exploding case for sampling. Moreover, based on the full error analysis, we establish a quantitative understanding of the error bound under the two schedules. Consequently, we conclude with a qualitative illustration of the "bell-shaped" weighting and the choices of schedules under well-trained and less-trained cases.

**Limitations.** The network architecture we used in the model is a deep ReLU network. Although being so far the most complicated architecture for theoretical results, it is still far from what is used in practice like U-Nets and transformers. Moreover, regarding the full error analysis, we only focus on the optimization and sampling error and do not dissect the generalization error. When bridging the theoretical results with practical designs of diffusion models, our results are mostly qualitative and we only compare two existing schedules under two extreme cases, when the network is well-trained and less-trained. Thus, theoretical implications on practical designs remain to be explored. We will leave these perspectives for future exploration.

## Appendix B Notations

\begin{tabular}{l l} \(X_{t}\) & Solution of forward dynamics (1) \\ \(Y_{t}\) & Solution of backward dynamics (2) \\ \(\bar{Y}_{t}\) & Solution of generation algorithm (10) \\ \(\sigma_{t}\) & Diffusion coefficient of (1) and (2) \\ \(\bar{\sigma}_{t}\) & Standard deviation of \(X_{t}\) (4) \\ \(\mathcal{L}_{\rm conti}\) & Continuous-time score-matching objective (3) \\ \(\bar{\mathcal{L}}\) & Discrete-time denoising score-matching objective (population version) \\ \(\bar{\mathcal{L}}_{em}\) & Discrete-time denoising score-matching objective (empirical version) (7) \\ \(C_{t}\) & Constant between score-matching and denoising score-matching loss at time \(t\) (4) \\ \(\tilde{C}\) & Constant between score-matching and denoising score-matching loss over all discrete times (5) \\ \(x_{i}\) & Sample from the initial data distribution \(P_{0}\) (7) \\ \(X_{ij}\) & Sample from the distribution \(P_{t}\) at time \(t\) (7) \\ \(t_{j}\) & The \(j\)th time point for forward process (6) \\ \(t_{j}^{\leftarrow}\) & The \(j\)th time point for backward process (10) \\ \(\delta\) & The first (last) time point of the forward (backward) dynamics, i.e., \(t_{0}\) (11) \\ \(T\) & Stopping time of the forward dynamics (11) \\ \(\gamma_{j}\) & Difference between backward time points, \(t_{j+1}^{\leftarrow}-t_{j}^{\leftarrow}\) (11) \\ \(p_{t}\) & Density of the solution of forward dynamics at time \(t\) (and backward dynamics at time \(T-t\)) (11) \\ \(q_{t}\) & Density of the solution of the generation algorithm at time \(t\) (11) \\ \(w(t)\) & Weighting function (3) \\ \(\beta_{j}\) & Total weighting, i.e. \(w(t_{j})(t_{j}-t_{j-1})/\bar{\sigma}_{t_{j}}\) (7) \\ \(\beta_{\rm EDM}\) & Total weighting used in EDM [30] (12) \\ \(\bar{\sigma}_{\max}\) (\(\bar{\sigma}_{\min}\)) & Maximum (minimum) of \(\bar{\sigma}_{t_{j}}\) (Table 2) \\ \(n\) & Number of samples from the initial distribution \(P_{0}\) (7) \\ \(N\) & Number of time steps when discretizing the forward and backward dynamics (6) \\ \(d\) & Dimension of input, output data, and the solutions of the dynamics (1) and (2) \\ \(S\) & Deep ReLU network (parameterization of score function) \\ \(\theta\) & All the parameters in the network \(S\) (8) \\ \(W_{\ell}\) & The weight in the \(i\)th layer of the network \(S\) (8) \\ \(\theta^{(k)}\) & The \(k\)th iteration of the weights \(\theta\) through GD (9) \\ \(W_{\ell}^{(k)}\) & The \(k\)th iteration of the weights \(W_{\ell}\) through GD (9) \\ \(m\) & Width of the network (8) \\ \end{tabular}

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d^{2a_{0}-1}))\),

\[\bar{\mathcal{L}}_{em}(W^{(k+1)})\] \[\leq\bar{\mathcal{L}}_{em}(W^{(k)})-h\|\nabla\bar{\mathcal{L}}_{em} (W^{(k)})\|^{2}\] \[+h\sqrt{\bar{\mathcal{L}}_{em}}\sqrt{\sum_{j}w(t_{j})(t_{j}-t_{j- 1})\bar{\sigma}_{t_{j}}}\mathcal{O}(\omega^{1/3}L^{2}\sqrt{m\log md}a^{/2})\| \nabla\bar{\mathcal{L}}_{em}(W^{(k)})\|\] \[+h^{2}\sqrt{\bar{\mathcal{L}}_{em}}\sqrt{\sum_{j}w(t_{j})(t_{j}-t _{j-1})\bar{\sigma}_{t_{j}}}\mathcal{O}(L^{2}\sqrt{m}d^{a})\|\nabla\bar{ \mathcal{L}}_{em}(W^{(k)})\|^{2}\] \[\leq\left(1-hw(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_ {j^{*}}}\cdot\Omega\left(\frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\right) \right)\bar{\mathcal{L}}_{em}(W^{(k)})\] \[+hC\frac{m^{5/6}d^{7/12-a_{0}/6}}{N^{1/6}n^{2/3}(\log m)^{1/6} \sqrt{L}}\frac{\sqrt{\sum_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}}\min_ {j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}}{\max_{j}w(t_{j})(t_{j}-t_{j-1} )\bar{\sigma}_{t_{j}}\sum_{k}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}}\bar {\mathcal{L}}_{em}(W^{(k)})\] \[\leq\left(1-hw(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_{ j^{*}}}\cdot\Omega\left(\frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\right) \right)\bar{\mathcal{L}}_{em}(W^{(k)})\]

where \(C>0\) is some constant, \(a_{0}\in(1/2,1)\); the second inequality follows from Lemma 7 with

\[\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta^{(k)})\|^{2}=\Omega \left(\frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\;w(t_{j^{*}})(t_{j^{*}}-t_{j^{ *}-1})\bar{\sigma}_{t_{j^{*}}}\right)\bar{\mathcal{L}}_{em}(\theta^{(k)}),\]

which is obtained inductively; the last inequality follows from \(m=\Omega\left(d^{13/2-2a_{0}/3}n^{14/3}N^{11}L^{3}(\log m)\left(\frac{\max_{j }w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\sum_{k}w(t_{j})(t_{j}-t_{j-1}) \bar{\sigma}_{t_{j}}}{\min_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}} \sqrt{\sum_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}}\right)^{6}\right)\)

### Proof of lower bound of the gradient at the initialization

In this section, we will show the main part of the convergence analysis, which is the following lower bound of the gradient.

**Lemma 1** (Lower bound).: _With probability \(1-\mathcal{O}(nN)\exp(-\Omega(d^{2a_{0}-1}))\), we have_

\[\|\nabla\bar{\mathcal{L}}_{em}(\theta^{(0)})\|^{2}\geq C_{6}\left( \frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\;w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1} )\bar{\sigma}_{t_{j^{*}}}\right)\bar{\mathcal{L}}_{em}(\theta^{(0)})\]

_where \((i^{*},j^{*})=\arg\max\left\|\sqrt{\frac{w(t_{j})(t_{j}-t_{j-1})}{\bar{\sigma} _{t_{j}}}}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\right\|\), \(\frac{1}{2}<a_{0}<1\), and \(C_{6}>0\) is some universal constant._

Below is the proof sketch of Lemma 1.

Proof sketch.: We first decompose the gradient of the \(k\)th row of \(W_{L}\)\(\nabla_{(W_{L})_{k}}\bar{\mathcal{L}}_{\rm em}(\theta)=\underbrace{\frac{1}{n}w(t_ {j^{*}})(t_{j^{*}}-t_{j^{*}-1})(W_{L+1})^{k^{\top}}(\bar{\sigma}_{t_{j^{*}}},W_ {L+1}q_{i^{*}j^{*},L}+\xi_{i^{*}j^{*}})q_{i^{*}j^{*},L-1}1_{(W_{L}q_{i^{*}j^{*}, L-1})k>0}}_{\nabla_{1}}\\ +\underbrace{\frac{1}{n}\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})(W_ {L+1})^{k^{\top}}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\,q_{ij,L}1_{(W_ {L}q_{ij,L-1})k>0}}_{\nabla_{2}}\]

where \((i^{*},j^{*})\) indicates the sample index with the largest loss value.

Then we first fix \((q_{ij,L-1})_{s}=1\), and prove that the index set of both \((q_{i^{*}j^{*},L})_{s}>0\) and \(\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}1_{(W_{L}q _{ij,L-1})_{s}>0}(q_{ij,L})_{s}>0\) is order \(m\) with high probability.

Next, we conditioned on the index set we've found, then we can decouple each element of \(\nabla_{(W_{L})_{k}}\widetilde{\mathcal{L}}_{\mathrm{em}}\) with high probability. We prove that with high probability

\[\angle\left(W_{L+1}\bar{\sigma}_{t_{j}},q_{i^{*}j^{*},L}+\xi_{i^{*}j^{*}},W_{L+ 1}\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij}q_{ij,L}+\sum_{(i,j)\neq(i^{*},j^{*}) }\bar{\alpha}_{ij}\xi_{ij}\right)\leq\pi-cd^{\frac{a_{0}-1}{2}},\]

for some constant \(c>0\) and \(\frac{1}{2}<a_{0}<1\). Based on this, we show that with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d))\),

\[\mathbb{P}\left((\nabla_{1})_{s}>0,(\nabla_{2})_{s}>0\right)\geq cd^{\frac{a_ {0}-1}{2}},\]

for some \(c>0\). Then we prove that with probability at least \(1-\exp(-\Omega(md^{\frac{a_{0}-1}{2}}))\)

\[|\{k:(W_{L+1}^{k})^{\top}v\geq 0,(W_{L+1}^{k})^{\top}(u+\xi)\geq 0\}|=\Theta(md^{ \frac{a_{0}-1}{2}}).\]

with high probability, the event \((\nabla_{1})_{s}>0\) and \((\nabla_{2})_{s}>0\) has probability at least of order \(d^{(a_{0}-1)/2}\) where \(a_{0}\in(1/2,1)\).

Now, we deal with \((q_{ij,L-1})_{s}\) and prove that if the above results hold for \((q_{ij,L-1})_{s}=1\), then there exists an index set with cardinality of order \(m/(nN)\) such that \((\nabla_{1})_{s}>0\) and \((\nabla_{2})_{s}>0\) also hold in this index set.

In the end, combining all the steps above yields the lower bound. 

Here is the complete proof.

Proof.: The main idea of the proof of lower bound is to decouple the elements in the gradient and incorporate geometric view. We focus on \(\nabla_{W_{L}}\widetilde{\mathcal{L}}_{em}(\theta)\).

**Step 1**: Rewrite \(\nabla_{(W_{L})_{k}}\widetilde{\mathcal{L}}_{em}(\theta)\) to be the \((i^{*},j^{*})\)th term \(g_{1}\) plus the rest \(nN-1\) terms \(g_{2}\).

Let \((i^{*},j^{*})=\arg\max\left\|\sqrt{\frac{w(t_{j})(t_{j}-t_{j-1})}{\bar{\sigma} _{t_{j}}}}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\right\|\). Let

\[g_{ij,L}=w(t_{j})(t_{j}-t_{j-1})(W_{L+1})^{k^{\top}}(\bar{\sigma}_{t_{j}}W_{L+1 }q_{ij,L}+\xi_{ij})\,q_{ij,L-1}.\]

Then

\[\nabla_{(W_{L})_{k}}\widetilde{\mathcal{L}}_{em}(\theta)\] \[= \underbrace{\frac{1}{n}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})(W_{L+ 1})^{k^{\top}}(\bar{\sigma}_{t_{j^{*}}}W_{L+1}q_{i^{*}j^{*},L}+\xi_{i^{*}j^{*} })\,q_{i^{*}j^{*},L-1}\mathbbm{1}_{(W_{L}q_{i^{*}j^{*},L-1})_{k}>0}}_{\nabla_{ 1}}\] \[+\underbrace{\frac{1}{n}\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j }-t_{j-1})(W_{L+1})^{k^{\top}}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\, q_{ij,L-1}\mathbbm{1}_{(W_{L}q_{ij,L-1})_{k}>0}}_{\nabla_{2}}\]

Also define

\[\nabla_{1,s}= \underbrace{\frac{1}{n}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{ \sigma}_{t_{j^{*}}}\,\,(W_{L+1})^{k^{\top}}W_{L+1}q_{i^{*}j^{*},L}\,(q_{i^{*}j ^{*},L-1})_{s}\mathbbm{1}_{(W_{L}q_{i^{*}j^{*},L-1})_{k}>0}}_{\nabla_{11,s}}\] \[+\underbrace{\frac{1}{n}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\,(W_{ L+1})^{k^{\top}}\xi_{i^{*}j^{*}}\,(q_{i^{*}j^{*},L-1})_{s}\mathbbm{1}_{(W_{L}q_{i^{*}j ^{*},L-1})_{k}>0}}_{\nabla_{12,s}}\] \[\nabla_{2,s}= \underbrace{\frac{1}{n}\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j }-t_{j-1})\bar{\sigma}_{t_{j}}(W_{L+1})^{k^{\top}}W_{L+1}q_{ij,L}\,(q_{ij,L-1} )_{s}\mathbbm{1}_{(W_{L}q_{ij,L-1})_{k}>0}}_{\nabla_{21,s}}\] \[+\underbrace{\frac{1}{n}\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j }-t_{j-1})(W_{L+1})^{k^{\top}}\xi_{ij}\,q_{ij,L-1}(q_{ij,L-1})_{s}\mathbbm{1}_ {(W_{L}q_{ij,L-1})_{k}>0}}_{\nabla_{22,s}}\]Our goal is to show that with high probability, there are at least \(\mathcal{O}(\frac{md^{\alpha_{0}-1}}{nN})\) number of rows \(k\) such that \(\nabla_{11,s}\geq 0,\nabla_{12,s}\geq 0,\nabla_{21,s}\geq 0,\nabla_{22,s}\geq 0\). Then we can lower bound \(\|\nabla_{(W_{L})_{k}}\bar{\mathcal{L}}_{em}(\theta)\|^{2}\) by \(\|\nabla_{1}\|^{2}\), which can be eventually lower bounded by \(\bar{\mathcal{L}}_{em}(\theta)\).

**Step 2**: Consider \([\nabla_{(W_{L})_{k}}\bar{\mathcal{L}}_{em}(\theta)]_{s}\). For \((g_{2})_{s}\), first take \((q_{ij,L-1})_{s}=1\) for all \((i,j)\neq(i^{*},j^{*})\). Then we only need to consider

\[\nabla_{2,s}^{\prime}=\frac{1}{n}\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t _{j-1})(W_{L+1})^{k^{\top}}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\, \mathbbm{1}_{(W_{L}q_{ij,L-1})_{k}>0}\]

which is independent of \(s\). For \(\nabla_{1}\), since \(q_{i^{*},j^{*},L-1}\geq 0\) which does not affect the sign of this term, we can also first take \((q_{i^{*},j^{*},L-1})_{s}=1\) for all \(s\).

**Step 3**: We focus on \(\nabla_{11}\) and \(\nabla_{21}\) and we would like to pick the non-zero elements in this two terms. More precisely, let

\[N_{1} =\left\{s\mid(q_{i^{*}j^{*},L})_{s}>0,s=1,\cdots,m\right\},\] \[N_{2} =\left\{s\mid\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1}) \bar{\sigma}_{t_{j}}\mathbbm{1}_{(W_{L}q_{ij,L-1})_{k}>0}(q_{ij,L})_{s}>0,s=1, \cdots,m\right\}\]

Let \(\alpha_{ij}=w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\mathbbm{1}_{(W_{L}q_{ ij,L-1})_{k}>0}\geq 0\). Then

\[\sum_{(i,j)\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_ {t_{j}}\mathbbm{1}_{(W_{L}q_{ij,L-1})_{k}>0}(q_{ij,L})_{s}\] \[=\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij}(q_{ij,L})_{s}=\sum_{(i, j)\neq(i^{*},j^{*})}\alpha_{ij}\sigma(W_{L}q_{ij,L-1})_{s}.\]

If

\[\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij}(W_{L}q_{ij,L-1})_{s}=(W_{L})_{s}\sum_ {(i,j)\neq(i^{*},j^{*})}\alpha_{ij}q_{ij,L-1}>0,\]

then there must be at least one pair of \((i,j)\) s.t. \(\alpha_{ij}(W_{L}q_{ij,L-1})_{s}=\alpha_{ij}\sigma(W_{L}q_{ij,L-1})_{s}>0\), which implies \(\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij}(q_{ij,L})_{s}>0\). Therefore, it suffices to consider

\[N_{1} =\left\{s\mid(q_{i^{*}j^{*},L})_{s}=(W_{L})_{s}q_{i^{*}j^{*},L-1} >0,s=1,\cdots,m\right\},\] \[N_{2}^{\prime} =\left\{s\mid(W_{L})_{s}\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij}q _{ij,L-1}>0\right\}.\]

Since \((q_{ij,L-1})_{s}\geq 0\), we have

\[\left\{q_{i^{*}j^{*},L-1},\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ij }q_{ij,L-1}\right\}\geq 0,\] \[i.e.,\angle\left(q_{i^{*}j^{*},L-1},\sum_{(i,j)\neq(i^{*},j^{*}) }\alpha_{ij}q_{ij,L-1}\right)\leq\frac{\pi}{2}\]

By Lemma 2 and Proposition 2, we have

\[\mathbb{P}\left((W_{L})_{s}q_{i^{*}j^{*},L-1}>0,(W_{L})_{s}\sum_{ (i,j)\neq(i^{*},j^{*})}\alpha_{ij}q_{ij,L-1}>0\right)\] \[=\mathbb{P}\left(\frac{(W_{L})_{s}}{\|(W_{L})_{s}}q_{i^{*}j^{*},L -1}>0,\frac{(W_{L})_{s}}{\|(W_{L})_{s}\|}\sum_{(i,j)\neq(i^{*},j^{*})}\alpha_{ ij}q_{ij,L-1}>0\right)\] \[\geq\frac{1}{4}.\]

Also \((W_{L})_{s}\)'s are \(i.i.d.\) multivariate Gaussian. By Chernoff bound,

\[\mathbb{P}\left(|N_{1}\cap N_{2}^{\prime}|\in(\delta_{1}\frac{m}{4},\delta_{2} \frac{m}{4})\right)\leq 1-2e^{-\Omega(m)}\]for some small \(\delta_{1}\leq\frac{1}{4}\) and \(\delta_{2}\leq 4\), i.e., \(|N_{1}\cap N_{2}^{\prime}|=\Theta(m)\) with probability at least \(1-2e^{-\Omega(m)}\).

**Step 4**: Next we condition on \(N_{1}\cap N_{2}^{\prime}\) and consider \((W_{L+1})^{k^{\top}}W_{L+1}\bar{\sigma}_{t_{j^{*}}}q_{i^{*}j^{*},L}+(W_{L+1})^ {k^{\top}}\xi_{i^{*}j^{*}}\) and \((W_{L+1})^{k^{\top}}W_{L+1}\sum_{(i,j)*(i^{*},j^{*})}\alpha_{ij}q_{ij,L}+(W_{L+ 1})^{k^{\top}}\sum_{(i,j)*(i^{*},j^{*})}\bar{\alpha}_{ij}\xi_{ij}\), where \(\bar{\alpha}_{ij}=\alpha_{ij}/\bar{\sigma}_{ij}\).

We would like to prove that with high probability

\[\angle(W_{L+1}\bar{\sigma}_{t_{j^{*}}}q_{i^{*}j^{*},L}+\xi_{i^{*}j^{*}},W_{L+ 1}\sum_{(i,j)*(i^{*},j^{*})}\alpha_{ij}q_{ij,L}+\sum_{(i,j)*(i^{*},j^{*})}\bar {\alpha}_{ij}\xi_{ij})\leq\pi-cd^{\frac{a_{0}-1}{2}},\]

for some constant \(c>0\) and \(\frac{1}{2}<a_{0}<1\).

First, since \(\xi_{ij}\sim\mathcal{N}(0,I_{d})\), by Bernstein's inequality, with probability at least \(1-\exp(-\Omega(d))\), we have \(\|\xi_{ij}\|^{2}=\Theta(d)\). Similarly, since \((W_{L+1}q_{ij,L})_{s}\sim\mathcal{N}(0,\frac{2|q_{ij,L}|^{2}}{m})\), by Berstein's inequality and Lemma 4, with probability at least \(1-\exp(-\Omega(d))\), we have \(\|W_{L+1}q_{ij,L}\|^{2}=\Theta(d)\). By union bounds, the above holds for all \(i,j\) with probability at least \(1-2nN\exp(-\Omega(d))\).

Let \(v=\frac{W_{L+1}\sum_{(i,j)*(i^{*},j^{*})}\alpha_{ij}q_{ij,L}+\sum_{(i,j)*(i^{* },j^{*})}\bar{\alpha}_{ij}\xi_{ij}}{\|W_{L+1}\sum_{(i,j)*(i^{*},j^{*})}\alpha_ {ij}q_{ij,L}+\sum_{(i,j)*(i^{*},j^{*})}\bar{\alpha}_{ij}\xi_{ij}\|}\) and \(u=W_{L+1}\bar{\sigma}_{t_{j^{*}}}q_{i^{*}j^{*},L}\). For notational simplicity, we use \(\xi\) to denote \(\xi_{i^{*}j^{*}}\). Fix \(v,u\) and consider the probability of event \(A\)

\[A=\{v^{\top}(u+\xi)\leq-\sqrt{1-c_{0}d^{a_{0}-1}}\|u+\xi\|\}\]

for some \(c_{0}>0\) and \(\frac{1}{2}<a_{0}<1\).

Then consider the following event that has larger probability than \(A\)

\[(v^{\top}(u+\xi))^{2}\geq(1-c_{0}d^{a_{0}-1})\|u+\xi\|^{2}\] (14) \[\Longleftrightarrow(v^{\top}u)^{2}-(1-c_{0}d^{a_{0}-1})\|u\|^{2} +2(v^{\top}u\,v-(1-c_{0}d^{a_{0}-1})u)^{\top}\xi+(v^{\top}\xi)^{2}\geq(1-c_{0 }d^{a_{0}-1})\|\xi\|^{2}\] (15)

Since \((v^{\top}u)^{2}\leq\|u\|^{2}\) where the equality holds when \(v=\frac{u}{\|u\|}\), we have

\[\operatorname{LHS}\leq c_{0}d^{a_{0}-1}\|u\|^{2}+2(v^{\top}u\,v-(1-c_{0}d^{a_ {0}-1})u)^{\top}\xi+(v^{\top}\xi)^{2}\]

Also, since \(\|u\|^{2}=\mathcal{O}(d)\) with probability at least \(1-2nN\exp(-\Omega(d))\), we have

\[\mathbb{P}\left(|2(v^{\top}u\,v-(1-c_{0}d^{a_{0}-1})u)^{\top}\xi|\geq d^{a_{0 }}\right)\leq 2\exp\left(-c\frac{d^{2a_{0}}}{\|u\|^{2}}\right)=2\exp(-\Omega(d^{ 2a_{0}-1}))\]

for some constant \(c>0\).

Therefore, with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d^{2a_{0}-1}))\)

\[\text{LHS of \eqref{eq:1-2nN}}\leq cd^{a_{0}}+(v^{\top}\xi)^{2}\]

for some constant \(c>0\).

Then

\[\mathbb{P}(v^{\top}(u+\xi)\leq-\sqrt{1-c_{0}d^{a_{0}-1}}\big{|}u+ \xi\big{|})\] \[\leq\mathbb{P}((v^{\top}(u+\xi))^{2}\geq(1-c_{0}d^{a_{0}-1})\|u+ \xi\|^{2})\] \[\leq\mathbb{P}((v^{\top}\xi)^{2}\geq(1-c^{\prime}d^{a_{0}-1})\| \xi\|^{2})\] \[=\mathbb{P}\left(v^{\top}\frac{\xi}{\|\xi\|}\geq\sqrt{(1-c^{ \prime}d^{a_{0}-1})}\right)+\mathbb{P}\left(-v^{\top}\frac{\xi}{\|\xi\|}\geq \sqrt{(1-c^{\prime}d^{a_{0}-1})}\right)\] \[=\mathbb{P}\left(\angle\,(v,\frac{\xi}{\|\xi\|})\geq\arccos(- \sqrt{(1-c^{\prime}d^{a_{0}-1})})\right)+\mathbb{P}\left(\angle\,(-v,\frac{\xi} {\|\xi\|})\geq\arccos(-\sqrt{(1-c^{\prime}d^{a_{0}-1})})\right)\] \[=2\mathbb{P}\left(\angle\,(v,\frac{\xi}{\|\xi\|})\geq\pi-c^{ \prime\prime}d^{\frac{a_{0}-1}{2}}\right)\] \[=\frac{C}{(d^{\frac{1-a_{0}}{2}})^{d-1}\sqrt{d}}\]where the second equality follows from Lemma 2; the third equality follows from series expansion; the forth equality follows from (16); \(c^{\prime},c^{\prime\prime},C>0\) are some constants.

Thus with probability at least \(1-\frac{C}{(d^{-\frac{1-\alpha}{2}})^{d-1}\sqrt{d}}\),

\[v^{\top}(u+\xi)\geq-\sqrt{1-c_{0}d^{a_{0}-1}}\|u+\xi\|\] \[i.e.\angle\left(v,u+\xi\right)\leq\pi-cd^{\frac{a_{0}-1}{2}}\]

for some \(c>0\).

Then by Lemma 2, with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d))\),

\[\mathbb{P}\left((W^{k}_{L+1})^{\top}v\geq 0,(W^{k}_{L+1})^{\top}(u+\xi)\geq 0 \right)\geq cd^{\frac{a_{0}-1}{2}},\]

for some \(c>0\).

Since \((W^{k}_{L+1})\) are iid Guassian vectors for \(k=1,\cdot\cdot\cdot,m\), by Chernoff bound on Bernoulli variable \(\mathbbm{1}_{\{(W^{k}_{L+1})^{\top}v\geq 0,(W^{k}_{L+1})^{\top}(u+\xi)\geq 0\}}\), we have, with probability at least \(1-\exp(-\Omega(md^{\frac{a_{0}-1}{2}}))\)

\[|\{k:(W^{k}_{L+1})^{\top}v\geq 0,(W^{k}_{L+1})^{\top}(u+\xi)\geq 0\}|=\Theta(md^{ \frac{a_{0}-1}{2}}).\]

**Step 5:** Combining the above 4 steps, we would like to obtain the lower bound of the gradient.

For each \(k\), consider \((q_{ij,L-1})_{s}\) for \((i,j)\neq(i^{*},j^{*})\) and denote \(q_{s}=\{(q_{ij,L-1})_{s}\}_{(i,j)\neq(i^{*},j^{*})}=\{\sigma((W_{L-1})_{s}q_{ ij,L-2})\}_{(i,j)\neq(i^{*},j^{*})}\). Let \(\bar{q}_{s}=\{(W_{L-1})_{s}q_{ij,L-2}\}_{(i,j)\neq(i^{*},j^{*})}\) and \(\bar{q}_{s}\sim\mathcal{N}(0,QQ^{\top})\), where each row of \(Q\) is \(q^{*}_{ij,L-2}\) for \((i,j)\neq(i^{*},j^{*})\). Thus, \(q_{s}\) is \(\bar{q}_{s}\) projected to the nonnegative orthant.

Let \(\mathbf{1}=(1,1,\cdot\cdot\cdot,1)\in\mathbb{R}^{nN-1}\). Therefore, if \((\beta_{k},\mathbf{1})\geq 0\) for some \(\beta_{k}\in\mathbb{R}^{nN-1}\), then at least half of the nonnegative orthant is contained in \(\{v\in\mathbb{R}^{nN-1}:(\beta_{k},v)\geq 0\}\), i.e., there exists a constant \(c_{k}>0\), s.t.

\[\mathbb{P}((\beta_{k},q_{s})\geq 0)\geq c_{k}\geq\min_{k=1,\cdot\cdot\cdot,m}c_{ k}>0,\ \text{ for all }s=1,\cdot\cdot\cdot,m\]

Then since \(\beta_{k}\in\mathbb{R}^{nN-1}\) for \(k=1,\cdot\cdot\cdot,m\) and \(nN\ll m\), there exists a set of indices \(\mathcal{K}\subseteq\{1,\cdot\cdot\cdot,m\}\) with \(|\mathcal{K}|=\Theta(\frac{m}{nN})\) and a set of indices \(\mathcal{S}\subseteq\{1,\cdot\cdot\cdot,m\}\) with \(|\mathcal{S}|=\Theta(m)\), s.t., \(\langle\beta_{k},q_{s}\rangle\geq 0\), for \(k\in\mathcal{K},s\in\mathcal{S}\).

Let \(q^{\mathcal{K}}_{ij,\ell}=(q_{ij,\ell})_{k\in\mathcal{K}}\). Then by Bernstein's inequality, we can also obtain that \(\|q^{\mathcal{K}}_{ij,\ell}\|^{2}=\Theta(d)\) with probability at least \(1-nN\exp(-\Omega(d))\).

Combine all of the above and apply the Claim 9.5 in Allen-Zhu et al. [1], we obtain, with probability at least \(1-\mathcal{O}(nN)\exp(-\Omega(d^{2a_{0}-1}))\),

\[|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta^{(0)})|_{F}^{2} \geq\frac{1}{n^{2}}C_{6}w(t_{j^{*}})^{2}(t_{j^{*}}-t_{j^{*}-1})^{ 2}\frac{1}{d}\|\bar{\sigma}_{t_{j^{*}}}W_{L+1}q_{i^{*}j^{*},L}+\xi_{i^{*}j^{* }}\|^{2}\|q^{\mathcal{K}}_{i^{*}j^{*},L-1}\|^{2}\frac{1}{nN}md^{\frac{a_{0}-1} {2}}\] \[\geq C_{6}md^{\frac{a_{0}-1}{2}}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1}) \bar{\sigma}_{t_{j^{*}}}\frac{1}{n^{3}N^{2}}\bar{\mathcal{L}}_{em}(\theta^{(0) }),\]

where \(C_{6}>0\) is some universal constant, \(\frac{1}{2}<a_{0}<1\), and the second inequality follows from the definition of \(i^{*},j^{*}\).

#### d.1.1 Geometric ideas used in the proof

**Proposition 2**.: _Consider \(w\sim\mathcal{N}(0,I)\), where \(w\in\mathbb{R}^{n}\). Then \(|w|\) and \(\frac{w}{|w|}\) are independent random variables and \(\frac{w}{|w|}\sim\text{Unif}(\mathbb{S}^{n-1})\)._

**Lemma 2**.: _Let \(w\sim\text{Unif}\left(\mathbb{S}^{n-1}\right)\), where \(\mathbb{S}^{n-1}=\{x\in\mathbb{R}^{n}\|\|x\|=1\}\). Then for two vectors \(v_{1},v_{2}\in\mathbb{R}^{n}\),_

\[\mathbb{P}(w^{\top}v_{1}\geq 0,w^{\top}v_{2}\geq 0)=\frac{\pi-\angle\left(v_{1},v_{ 2}\right)}{2\pi}.\]Proof.: Since \(w\sim Unif(\mathbb{S}^{n-1})\), we only need to consider the area of the event. It is obvious that the set \(\{w\in\mathbb{S}^{n-1}|w^{\top}v_{i}\}\) is a semi-hypersphere. Therefore, we only need to consider the intersection of two semi-hypersphere, i.e.,

\[\mathbb{P}(w^{\top}v_{1}\geq 0,w^{\top}v_{2}\geq 0) =\frac{\text{area of }\{w\in\mathbb{S}^{n-1}|w^{\top}v_{1}\geq 0 \}\cap\text{ area of }\{w\in\mathbb{S}^{n-1}|w^{\top}v_{2}\geq 0\}}{\text{area of the hypersphere}}\] \[=\frac{\pi-\angle(v_{1},v_{2})}{2\pi}.\]

Next we follow the notations and definitions in Lee and Kim [33]. Consider the unit hypersphere in \(\mathbb{R}^{d}\), \(\mathbb{S}^{d-1}=\{x\in\mathbb{R}^{d}\,|\,\|x\|=1\}\). The area of \(\mathbb{S}^{d-1}\) is

\[A_{d}(1)=\frac{2\pi^{d/2}}{\Gamma(d/2)}.\]

**Lemma 3**.: _Fix \(\xi_{1}\in\mathbb{S}^{d-1}\) and let \(\xi_{2}\sim\text{Unif}(\mathbb{S}^{d-1})\), where \(\mathbb{S}^{d-1}=\{x\in\mathbb{R}^{d}|\|x\|=1\}\). Then with probability at least \(1-\exp(-\Omega(d))\), we have \(\angle(\xi_{1},\xi_{2})\leq\frac{3\pi}{4}\)._

Proof.: For any fixed \(\xi_{1}\), all the \(\xi_{2}\)'s that satisfy \(\angle(\xi_{1},\xi_{2})\geq\pi-\theta\) are on a hyperspherical cap. By Lee and Kim [33], the area of the hyperspherical cap is

\[A_{d}^{\theta}(1)=\frac{1}{2}A_{d}(1)I_{\sin^{2}\theta}\left(\frac{d-1}{2}, \frac{1}{2}\right).\]

Then

\[\mathbb{P}(\,\angle(\xi_{1},\xi_{2})\geq\pi-\theta)=\frac{A_{d}^{\theta}(1)}{A _{d}(1)}=\frac{1}{2}I_{\sin^{2}\theta}\left(\frac{d-1}{2},\frac{1}{2}\right) \propto\frac{1}{2}\frac{\theta^{d-1}}{\sqrt{\pi}\sqrt{\frac{d-1}{2}}}.\] (16)

Let \(\theta=\frac{\pi}{4}<1\). Then with probability at least \(1-\exp(-\Omega(d))\), we have \(\angle(\xi_{1},\xi_{2})\leq\frac{3\pi}{4}\). 

### Proofs related to random initialization

Consider \(W_{i}=W_{i}^{(0)}\) in this section.

**Lemma 4**.: _If \(\epsilon\in(0,1)\), with probability at least \(1-\mathcal{O}(nN)e^{-\Omega(\min(\epsilon^{2}d^{4b-1},ed^{2b}))}\), \(\|X_{ij}\|^{2}\in\left[\|e^{-\mu_{ij}}x_{i}\|^{2}+\bar{\sigma}_{i_{j}}^{2}d- \epsilon\bar{\sigma}_{i_{j}}^{2}d^{2b}\right]\) for all \(i=1,\cdots,n\) and \(j=0,\cdots,N-1\). Moreover, with probability at least \(1-\mathcal{O}(L)e^{-\Omega(me^{2}/L)}\) over the randomness of \(W_{s}\) for \(s=0,\cdots,L\), we have \(\left|q_{ij,\ell}\right|\in\left[\|X_{ij}\|(1-\epsilon),\|X_{ij}\|(1+\epsilon)\right]\) for fixed \(i,j\). Therefore, with probability at least \(1-\mathcal{O}(nNL)e^{-\Omega(\min(me^{2}/L,e^{2d^{4b-1}},ed^{2b}))}\), we have \(\Omega(d^{b})=\left|q_{ij,\ell}\right|=\mathcal{O}(d^{a})\)._

Proof.: Consider \(\frac{1}{\bar{\sigma}_{t_{j}}}X_{ij}=\frac{e^{-\mu_{ij}}}{\bar{\sigma}_{t_{j} }}x_{i}+\xi_{ij}\). Since \(\xi_{ij}\sim\mathcal{N}(0,I)\), \(\left|\frac{1}{\bar{\sigma}_{t_{j}}}X_{ij}\right|^{2}\) follows from the noncentral \(\chi^{2}\) distribution and \(\mathbb{E}\|\frac{1}{\bar{\sigma}_{t_{j}}}X_{ij}\|^{2}=d+\left|\frac{e^{-\mu_{ ij}}}{\bar{\sigma}_{t_{j}}}x_{i}\right|^{2}\) (this includes the time variable at the \(d\)th dimension). By Berstein inequality,

\[\mathbb{P}\left(\left|\left|\left|\frac{1}{\bar{\sigma}_{t_{j}}}X _{ij}\right|^{2}-\mathbb{E}\left|\left|\frac{1}{\bar{\sigma}_{t_{j}}}X_{ij} \right|^{2}\right|\right|\geq t\right)\leq 2\exp\left(-\,c\min\left(\frac{t^{2}}{d},t \right)\right)\] \[i.e.,\mathbb{P}\left(\left|\left|e^{-\mu_{t_{j}}}x_{i}+\bar{ \sigma}_{t_{j}}\xi_{ij}\right|^{2}-(\bar{\sigma}_{t_{j}}^{2}d+\left|e^{-\mu_{ t_{j}}}x_{i}\right|^{2})\right|\geq\bar{\sigma}_{t_{j}}^{2}t\right)\leq 2\exp\left(-\,c\min \left(\frac{t^{2}}{d},t\right)\right)\]

Therefore, with probability at least \(1-\mathcal{O}(nN)\,\mathrm{e}^{-\Omega(\min(\epsilon^{2}d^{4b-1},ed^{2b}))}\), \(\left|X_{ij}\right|^{2}\in\left[\|e^{-\mu_{t_{j}}}x_{i}\|^{2}+\bar{\sigma}_{t_{j }}^{2}d-\epsilon\bar{\sigma}_{t_{j}}^{2}d^{2b},\|e^{-\mu_{t_{j}}}x_{i}\|^{2}+ \bar{\sigma}_{t_{j}}^{2}d+\epsilon\bar{\sigma}_{t_{j}}^{2}d^{2b}\right]\) for all \(i=1,\cdots,n\) and \(j=0,\cdots,N-1\), where \(\epsilon\in(0,1)\). The second part of the Lemma follows the similar proof in Lemma 7.1 of Allen-Zhu et al. [1]. The last part follows from union bound and Assumption 1.

**Lemma 5** (Upper bound).: _Under the random initialization of \(W_{i}\) for \(i=0,\cdots,L\), with probability at least \(1-\mathcal{O}(nNL)e^{-\Omega(\min(me^{2}/L,e^{2}d^{4b-1},ed^{2b}))}\), we have_

\[\|\triangledown_{W_{\ell}}\bar{\mathcal{L}}_{em}(\theta^{(0)})\|^{2}=\mathcal{O }\left(md^{2a-1}N\max_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}\right) \bar{\mathcal{L}}_{em}(\theta^{(0)}).\]

Proof.: For any \(\ell=1,\cdot,L\), we have

\(\|\triangledown_{W_{\ell}}\bar{\mathcal{L}}_{em}(\theta)\|_{F}^{2}\)

\[=\sum_{k=1}^{m}\|\nabla_{(W_{\ell})_{k}}\bar{\mathcal{L}}_{em}( \theta)\|^{2}\] \[=\sum_{k=1}^{m}\left\|\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_ {j})(t_{j}-t_{j-1})\right.\] \[\qquad\times\left[(W_{L+1}D_{ij,L}W_{L}\cdots D_{ij,\ell}W_{\ell +1})^{\top}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\right]_{k}q_{ij,\ell -1}\mathbbm{1}_{(W_{\ell}q_{ij,\ell-1})_{k}>0}\right\|^{2}\] \[\leq\frac{N}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})^{2}(t_{j}-t_{ j-1})^{2}\sum_{k=1}^{m}\|(W_{L+1}D_{ij,L}W_{L}\cdots D_{ij,\ell}W_{\ell+1})_{k}^ {\top}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})\|^{2}\cdot\|q_{ij,\ell-1 }\|^{2}\] \[\leq C_{7}d^{2a}\frac{m}{d}\frac{N}{n}\sum_{i=1}^{n}\sum_{j=1}^{N} w(t_{j})^{2}(t_{j}-t_{j-1})^{2}\cdot|\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ ij}\|^{2}\] \[\leq C_{7}d^{2a}\frac{mN}{d}\max_{j}w(t_{j})(t_{j}-t_{j-1})\bar{ \sigma}_{t_{j}}\bar{\mathcal{L}}_{em}(\theta)\]

where the first inequality follows from Young's inequality; the second inequality follows from Lemma 4 and Lemma 7.4 in Allen-Zhu et al. [1]; \(C_{7}>0\) 

### Proofs related to perturbation

Consider \(W_{i}^{\mathrm{per}}=W_{i}^{(0)}+W_{i}^{\prime}\) for \(i=1,\cdots,L\) in this section. We follow the same idea in Allen-Zhu et al. [1] to consider the network value of perturbed weights at each layer. We use the superscript "\(\mathrm{per}\)" to denotes the perturbed version, i.e.,

\[r_{ij,0}^{\mathrm{per}}=W_{0}X_{ij},q_{ij,0}^{\mathrm{per}}= \sigma(r_{ij,0}^{\mathrm{per}}),\] \[r_{ij,\ell}^{\mathrm{per}}=W_{\ell}^{\mathrm{per}}q_{ij,\ell-1}^{ \mathrm{per}},q_{ij,\ell}^{\mathrm{per}}=\sigma(r_{ij,\ell}^{\mathrm{per}}), \text{ for }\ell=1,\cdots,L\] \[S(\theta^{\mathrm{per}};t_{j},X_{ij})=W_{L+1}q_{ij,L}^{\mathrm{ per}}\]

We also similarly define the diagonal matrix \(D_{ij,\ell}^{\mathrm{per}}\) for the above network.

The following Lemma measures the perturbation of each layer. The lemma differs from Lemma 8.2 in Allen-Zhu et al. [1] by a scale of \(d^{a}\). For sake of completeness, we state it in the following and the proof can be similarly obtained.

**Lemma 6**.: _Let \(\omega\leq\frac{1}{C_{7}L^{3/2}(\log m)^{3}d^{a}}\) for some large \(C>1\). With probability at least \(1-\exp(-\Omega(d^{a}m\omega^{2/3}L))\), for any \(\Delta W\) s.t. \(\|\Delta W\|\leq\omega\), we have_

1. \(r_{ij,\ell}^{\mathrm{per}}-r_{ij,\ell}\) _can be decomposed to two part_ \(r_{ij,\ell}^{\mathrm{per}}-r_{ij,\ell}=r_{ij,\ell,1}^{\prime}+r_{ij,\ell,2}^{ \prime}\)_, where_ \(\|r_{ij,\ell,1}^{\prime}\|=\mathcal{O}(\omega L^{3/2}d^{a})\) _and_ \(\|r_{ij,\ell,2}^{\prime}\|_{\infty}=\mathcal{O}(\omega L^{5/2}\sqrt{\log md}d^{ a}m^{-1/2})\)_._
2. \(\|D_{ij,\ell}^{\mathrm{per}}-D_{ij,\ell}\|_{0}=\mathcal{O}(m\omega^{2/3}L)\) _and_ \(\|(D_{ij,\ell}^{\mathrm{per}}-D_{ij,\ell})r_{ij,\ell}^{\mathrm{per}}\|=\mathcal{ O}(\omega L^{3/2}d^{a})\)_._
3. \(\|r_{ij,\ell}^{\mathrm{per}}-r_{ij,\ell}\|\) _and_ \(\|q_{ij,\ell}^{\mathrm{per}}-q_{ij,\ell}\|\) _are_ \(\mathcal{O}(\omega L^{5/2}\sqrt{\log md}^{a})\)_._

### Proofs related to the evolution of the algorithm

**Lemma 7** (Upper and lower bounds of gradient after perturbation).: _Let_

\[\omega=\mathcal{O}\left(\frac{\bar{\mathcal{L}}_{em}^{*}}{L^{9}(\log m)^{2}n^{3} N^{3}d^{\frac{1-a_{0}}{2}}}\cdot\frac{\min_{j}w(t_{j})(t_{j}-t_{j-1})\bar{ \sigma}_{t_{j}}}{\max\{\max_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}}, \sum_{j}(w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}})^{2}\}}\right).\]_Consider \(\theta^{\rm per}\) s.t. \(|\theta^{\rm per}-\theta|\leq\omega\), where \(\theta\) follows from the Gaussian initialization. Then with probability at least \(1-\mathcal{O}(nN)e^{-\Omega(d^{2a_{0}-1})}\),_

\[\|\nabla_{W_{\ell}}\bar{\mathcal{L}}_{em}(\theta^{\rm per})\|^{2}= \mathcal{O}\left(md^{2a-1}N\max_{j}w(t_{j})(t_{j}-t_{j-1})\bar{\sigma}_{t_{j}} \right)\bar{\mathcal{L}}_{em}(\theta^{\rm per}),\] \[\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta^{\rm per})\|^{2}= \Omega\left(\frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\;w(t_{j^{*}})(t_{j^{*}}- t_{j^{*}-1})\bar{\sigma}_{t_{j^{*}}}\right)\min\{\bar{\mathcal{L}}_{em}(\theta), \bar{\mathcal{L}}_{em}(\theta^{\rm per})\},\]

_for \(\ell=1,\cdots,L\)._

Proof.: Consider the following terms

\[\nabla_{(W_{\ell})_{k}}\bar{\mathcal{L}}_{em}(\theta)=\frac{1}{n }\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\ \times[(W_{L+1}D_{ij,L}W_{L}\cdots D_{ij,\ell}W_{\ell+1})^{\top} (\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})]_{k}\,q_{ij,\ell-1}\,\mathbbm{1 }_{(W_{\ell}q_{ij,\ell-1})_{k}>0}\\ \nabla_{(W_{\ell})_{k}}^{\rm per}\bar{\mathcal{L}}_{em}(\theta)= \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\ \times[(W_{L+1}D_{ij,L}W_{L}\cdots D_{ij,\ell}W_{\ell+1})^{\top} (\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}^{\rm per}+\xi_{ij})]_{k}\,q_{ij,\ell-1}\, \mathbbm{1}_{(W_{\ell}q_{ij,\ell-1})_{k}>0},\\ \nabla_{(W_{\ell})_{k}}\bar{\mathcal{L}}_{em}(\theta^{\rm per})= \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\ \times[(W_{L+1}D_{ij,L}^{\rm per}W_{L}^{\rm per}\cdots D_{ij,\ell} ^{\rm per}W_{\ell+1}^{\rm per})^{\top}(\bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}^{ \rm per}+\xi_{ij})]_{k}\,q_{ij,\ell-1}^{\rm per}\,\mathbbm{1}_{(W_{\ell}^{\rm per }q_{ij,\ell-1}^{\rm per})_{k}>0}\] (19)

Then

\[\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta)-\nabla_{W_{\ell}} ^{\rm per}\bar{\mathcal{L}}_{em}(\theta)\|_{F}^{2}\\ =\sum_{k=1}^{m}\|\nabla_{(W_{\ell})_{k}}\bar{\mathcal{L}}_{em}( \theta)-\nabla_{(W_{\ell})_{k}}^{\rm per}\bar{\mathcal{L}}_{em}(\theta)\|^{2} \\ \leq\frac{N}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})^{2}(t_{j}-t_{j -1})^{2}\sum_{k=1}^{m}\|(W_{L+1}D_{ij,L}W_{L}\cdots D_{ij,\ell}W_{\ell+1})_{k} ^{\top}(\bar{\sigma}_{t_{j}}W_{L+1}(q_{ij,L}-q_{ij,L}^{\rm per}))\|^{2}\cdot\| q_{ij,\ell-1}\|^{2}\\ \leq C_{8}d^{2a}\frac{m}{d}\frac{N}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w (t_{j})^{2}(t_{j}-t_{j-1})^{2}\cdot\|\bar{\sigma}_{t_{j}}W_{L+1}(q_{ij,L}-q_{ ij,L}^{\rm per})\|^{2}\\ \leq C_{8}d^{2a}\frac{m}{d}N\sum_{j=1}^{N}w(t_{j})^{2}(t_{j}-t_{j -1})^{2}(\omega L^{5/2}\sqrt{\log md}a)^{2}\\ \leq\tilde{C}_{8}\left(\frac{md^{\frac{a_{0}-1}{2}}}{n^{3}N^{2}} \,w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_{j^{*}}}\right)\bar{ \mathcal{L}}_{em}^{*}\]

where the first two inequalities follow the same as the proof of Lemma 5; the third inequality follows from Lemma 6; the last inequality follows from the definition of \(\omega\).

[MISSING_PAGE_FAIL:26]

Also,

\[|\nabla_{W}\bar{\mathcal{L}}_{em}(\theta^{\mathrm{per}})\|^{2}\] \[\geq\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta^{\mathrm{per}}) \|^{2}\] \[\geq\frac{1}{3}\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta)\|^{2} -\|\nabla_{W_{L}}\bar{\mathcal{L}}_{em}(\theta)-\nabla_{W_{t}}^{\mathrm{per}} \bar{\mathcal{L}}_{em}(\theta)\|_{F}^{2}-\|\nabla_{W_{t}}^{\mathrm{per}}\bar{ \mathcal{L}}_{em}(\theta)-\nabla_{W_{t}}\bar{\mathcal{L}}_{em}(\theta^{\mathrm{ per}})\|_{F}^{2}\] \[=\Omega\left(\frac{md^{\frac{\alpha_{0-1}}{2}}}{n^{3}N^{2}}\,w(t _{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\bar{\sigma}_{t_{j^{*}}}\right)\min\{\bar{ \mathcal{L}}_{em}(\theta),\bar{\mathcal{L}}_{em}(\theta^{\mathrm{per}})\}.\]

Note when interpolation is not achievable, this lower bound is always away from 0, which means the current technique can only evaluate the lower bound outside a neighbourhood of the minimizer. More advanced method is needed and we leave it for future investigation.

**Lemma 8** (semi-smoothness).: _Let \(\omega=\Omega\). With probability at least \(1-e^{-\Omega(\log m)}\) over the randomness of \(\theta^{(0)}\), we have for all \(\theta\) s.t. \(\|\theta-\theta^{(0)}\|\leq\omega\), and all \(\theta^{\mathrm{per}}\) s.t. \(\|\theta^{\mathrm{per}}-\theta\|\leq\omega\),_

\[\bar{\mathcal{L}}_{em}(\theta^{\mathrm{per}})\leq\bar{\mathcal{L} }_{em}(\theta) +\left\langle\nabla\bar{\mathcal{L}}_{em}(\theta),\theta^{ \mathrm{per}}-\theta\right\rangle\] \[+\sqrt{\bar{\mathcal{L}}_{em}(\theta)}\sqrt{\sum_{j}w(t_{j})(t_{ j}-t_{j-1})\bar{\sigma}_{t_{j}}}\mathcal{O}(\omega^{1/3}L^{2}\sqrt{m\log md ^{a/2}})\|\theta^{\mathrm{per}}-\theta\|\] \[+\sqrt{\bar{\mathcal{L}}_{em}(\theta)}\sqrt{\sum_{j}w(t_{j})(t_{ j}-t_{j-1})\bar{\sigma}_{t_{j}}}\mathcal{O}(L^{2}\sqrt{m}d^{a})\|\theta^{ \mathrm{per}}-\theta\|^{2}\]

Proof.: By definition,

\[\bar{\mathcal{L}}_{em}(\theta^{\mathrm{per}})-\bar{\mathcal{L}}_{ em}(\theta)-\left\langle\nabla\bar{\mathcal{L}}_{em}(\theta),\theta^{ \mathrm{per}}-\theta\right\rangle\] \[=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})( \bar{\sigma}_{t_{j}}W_{L+1}q_{ij,L}+\xi_{ij})^{\top}W_{L+1}\] \[\quad\times\left(q_{ij,L}^{\mathrm{per}}-q_{ij,L}-\sum_{\ell=1}^ {L}D_{ij,L}W_{ij,L}\cdots W_{ij,\ell+1}D_{ij,\ell}(W_{ij,\ell}^{\mathrm{per}} -W_{ij,\ell})q_{ij,\ell}\right)\] \[\quad+\frac{1}{2\bar{\sigma}_{t_{j}}}w(t_{j})(t_{j}-t_{j-1})\| \bar{\sigma}_{t_{j}}W_{L+1}(q_{ij,L}^{\mathrm{per}}-q_{ij,L})\|^{2}.\]

Similar to the proof of Theorem 4 in Allen-Zhu et al. [1], we obtain the desired bound by using Cauchy-Schwartz inequality. Note, in our case, due to the order of input data, we choose \(s=\mathcal{O}(d^{a}m\omega^{2/3}L)\) in Allen-Zhu et al. [1] and therefore the bound is slightly different from theirs. 

## Appendix E Proofs for sampling

In this section, we prove Theorem 2. The proof includes two main steps: 1. decomposing \(\mathrm{KL}(p_{\delta}|q_{T-\delta})\) into the initialization error, the score estimation errors and the discretization errors; 2. estimating the initialization error and the discretization error based on our assumptions. In the following context, we introduce the proof of these two steps separately.

Proof of Theorem 2.: **Step 1:** The error decomposition follows from the ideas in [12] of studying VPSDE-based diffusion models. According to the chain rule of KL divergence, we have

\[\mathrm{KL}(p_{\delta}|q_{T-\delta})\leq\mathrm{KL}(p_{T}|q_{0})+\mathbb{E}_ {y\sim p_{T}}[\mathrm{KL}(p_{\delta|T}(\cdot|y)|q_{T-\delta|0}(\cdot|y))],\]Apply the chain rule again for at across the time schedule \((T-t_{j}^{*})_{0\leq j\leq N-1}\), the second term can be written as

\[\mathbb{E}_{y\sim p_{T}}[\text{KL}(p_{\delta|T}(\cdot|y)|q_{T- \delta|0}(\cdot|y))]\] \[\leq\sum_{j=0}^{N-1}\mathbb{E}_{y_{j}\sim p_{T-t_{j}^{*}}}[\text{ KL}(p_{T-t_{j+1}^{*}|T-t_{j}^{*}}(\cdot|y_{j})|q_{t_{j+1}^{*}|t_{j}^{*}}(\cdot|y_{j} ))]\] \[\leq\frac{1}{2}\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}} \sigma_{T-t}^{2}\mathbb{E}[\|s(\theta;T-t_{j}^{*},Y_{t_{j}^{*}})-\nabla\log p_ {T-t}(Y_{t})\|^{2}]dt\] \[\leq\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^{ 2}\mathbb{E}[\|s(\theta;T-t_{j}^{*},Y_{t_{j}^{*}})-\nabla\log p_{T-t_{j}^{*}}( Y_{t_{j}^{*}})\|^{2}]dt\] \[\quad+\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^ {2}\mathbb{E}[\|\nabla\log p_{T-t_{j}^{*}}(Y_{t_{j}^{*}})-\nabla\log p_{T-t}( Y_{t})\|^{2}]dt,\]

where the second inequality follows from Lemma 9. Therefore, the error decomposition writes as

\[\text{KL}(p_{\delta}|q_{T-\delta})\lesssim \text{KL}(p_{T}|q_{0})+\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^ {*}}\sigma_{T-t}^{2}\mathbb{E}[\|s(\theta;T-t_{j}^{*},Y_{t_{j}^{*}})-\nabla \log p_{T-t_{j}^{*}}(Y_{t_{j}^{*}})\|^{2}]dt\] \[+\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^{2} \mathbb{E}[\|\nabla\log p_{T-t_{j}^{*}}(Y_{t_{j}^{*}})-\nabla\log p_{T-t}(Y_{ t})\|^{2}]dt\] (20)

where the three terms in (20) quantify the initialization error, the score estimation error and the discretization error, respectively.

**Step 2:** In this step, we estimate the three error terms in Step 1. First, recall that \(p_{T}=p*\mathcal{N}(0,\bar{\sigma}_{T}^{2}I_{d})\) and \(q_{0}=\mathcal{N}(0,\bar{\sigma}_{T}^{2}I_{d})\), hence the initialization error \(\text{KL}(p_{T}|q_{0})\) can be estimated as follows,

\[\text{KL}(p_{T}|q_{0})=\text{KL}(p*\mathcal{N}(0,\bar{\sigma}_{T}^{2}I_{d})| \mathcal{N}(0,\bar{\sigma}_{T}^{2}I_{d}))\lesssim\frac{m_{t}^{2}}{\bar{\sigma }_{T}^{2}},\] (21)

where the inequality follows from Lemma 10. Hence we recover the term \(E_{I}\) in (11).

Next, since \(\sigma_{t}\) is non-decreasing in \(t\), the score estimation error can be estimated as

\[\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^{2} \mathbb{E}[\|s(\theta;T-t_{j}^{*},Y_{t_{j}^{*}})-\nabla\log p_{T-t_{j}^{*}}(Y_ {t_{j}^{*}})\|^{2}]dt\] \[\leq\sum_{j=0}^{N-1}\gamma_{j}\sigma_{T-t_{j}^{*}}^{2}\mathbb{E}[ \|s(\theta;T-t_{j}^{*},Y_{t_{j}^{*}})-\nabla\log p_{T-t_{j}^{*}}(Y_{t_{j}^{*}} )\|^{2}].\] (22)

Hence, we recover the term \(E_{S}\) in (11).

Last, we estimated the discretization error term. Our approach is motivated by analyses of VPSDEs in [8, 26]. We defines a process \(L_{t}:=\nabla\log p_{T-t}(Y_{t})\). Then we can relate discretization error to quantities depending on \(L_{t}\), and therefore bound the discretization error via properties of \(\{L_{t}\}_{0\leq t\leq T}\). According to Lemma 12, we have

\[\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^{2} \mathbb{E}[\|\nabla\log p_{T-t_{j}^{*}}(Y_{t_{j}^{*}})-\nabla\log p_{T-t}(Y_ {t})\|^{2}]dt\] \[\leq 2d\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\int_{t_{j}^{*} }^{t}\sigma_{T-t}^{2}\sigma_{T-u}^{2}\bar{\sigma}_{T-u}^{-4}dudt+\underbrace{ \sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_{T-t}^{2}dt\bar{\sigma}_{ T-t_{j}^{*}}^{-4}\mathbb{E}[\text{tr}(\Sigma_{T-t_{j}^{*}}(X_{T-t_{j}^{*}}))]}_{N_{ 2}}\] \[-\underbrace{\sum_{j=0}^{N-1}\int_{t_{j}^{*}}^{t_{j+1}^{*}}\sigma_ {T-t}^{2}\bar{\sigma}_{T-t}^{-4}\mathbb{E}[\text{tr}(\Sigma_{T-t}(X_{T-t}))]dt}_{N _{3}}.\]Since \(t\mapsto\sigma_{t}\) is non-decreasing and \(t\mapsto\mathbb{E}[\mathrm{tr}(\Sigma_{T-t}(X_{T-t}))]\) is non-increasing, we have

\[N_{1} =2d\sum_{j=0}^{N-1}\int_{T-t^{-}_{j+1}}^{T-t^{-}_{j}}\sigma_{T-t^{ -}_{j+1}}^{T-u}\sigma_{t}^{2}dt\sigma_{u}^{2}\bar{\sigma}_{u}^{-4}du\leq 2d\sum_{j=0} ^{N-1}\gamma_{j}\int_{T-t^{-}_{j+1}}^{T-t^{-}_{j}}\sigma_{t}^{4}\bar{\sigma}_{ t}^{-4}dt,\] \[N_{2} =\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma_{T-t}^{2}dt \bar{\sigma}_{T-t^{-}_{j}}^{-4}\mathbb{E}[\mathrm{tr}(\Sigma_{T-t^{-}_{j}}(X_{ T-t^{-}_{j}}))],\] \[N_{3} \leq-\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{+}_{j+1}}\sigma_{T-t}^{ 2}\bar{\sigma}_{T-t}^{-4}dt\mathbb{E}[\mathrm{tr}(\Sigma_{T-t^{-}_{j+1}}(X_{T- t^{-}_{j+1}}))].\]

Therefore, we obtain

\[\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma_{T-t}^{2} \mathbb{E}[|\nabla\log p_{T-t^{-}_{j}}(Y_{t^{-}_{j}})-\nabla\log p_{T-t}(Y_{t} )\|^{2}]dt\] \[\leq 2d\sum_{j=0}^{N-1}\gamma_{j}\int_{T-t^{-}_{j+1}}^{T-t^{-}_{j} }\sigma_{t}^{4}\bar{\sigma}_{t}^{-4}dt+\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{-} _{j+1}}\sigma_{T-t}^{2}dt\bar{\sigma}_{T-t^{-}_{j}}^{-4}\mathbb{E}[\mathrm{tr}( \Sigma_{T-t^{-}_{j}}(X_{T-t^{-}_{j}}))]\] \[\quad-\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{+}_{j+1}}\sigma_{T-t}^{ 2}\bar{\sigma}_{T-t}^{-4}dt\mathbb{E}[\mathrm{tr}(\Sigma_{T-t^{-}_{j+1}}(X_{T- t^{-}_{j+1}}))]\] \[=2d\sum_{j=0}^{N-1}\gamma_{j}\int_{T-t^{-}_{j+1}}^{T-t^{-}_{j}} \sigma_{t}^{4}\bar{\sigma}_{t}^{-4}dt+\int_{0}^{t^{+}_{1}}\sigma_{T-t}^{2}dt \bar{\sigma}_{T}^{-4}\mathbb{E}[\mathrm{tr}(\Sigma_{T}(X_{T}))]\] \[\quad+\sum_{j=1}^{N-1}\big{(}\int_{t^{-}_{j+1}}^{t^{+}_{j+1}} \sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-4}dt-\int_{t^{-}_{j-1}}^{t^{-}_{j}}\sigma_ {T-t}^{2}\bar{\sigma}_{T-t}^{-4}dt\big{)}\mathbb{E}[\mathrm{tr}(\Sigma_{T-t^{ -}_{j}}(X_{T-t^{-}_{j}}))].\] (23)

The above bound depends on \(\mathbb{E}[\mathrm{tr}(\Sigma_{t}(X_{t}))]\), hence we estimate \(\mathbb{E}[\mathrm{tr}(\Sigma_{t}(X_{t}))]\) for different values of \(t\).

First, we have

\[\mathbb{E}[\mathrm{tr}(\Sigma_{t}(X_{t}))]=\mathbb{E}[\mathbb{E}[|X_{0}\|^{2} |X_{t}]-|\mathbb{E}[X_{0}|X_{t}]|^{2}]\leq\mathbb{E}[|X_{0}\|^{2}]=\mathrm{m}_{ 2}^{2}.\]

Meanwhile,

\[\mathbb{E}[\mathrm{tr}(\Sigma_{t}(X_{t}))] =\mathbb{E}[\mathrm{tr}(\mathrm{Cov}(X_{0}-X_{t}|X_{t}))]= \mathbb{E}[\mathbb{E}[|X_{0}-X_{t}|^{2}|X_{t}]]-|\mathbb{E}[X_{0}-X_{t}|X_{t} ]|^{2}\] \[\leq\mathbb{E}[|X_{0}-X_{t}|^{2}]=\bar{\sigma}_{t}^{2}d\]

Therefore, \(\mathbb{E}[\mathrm{tr}(\Sigma_{t}(X_{t}))]\leq\min(\mathrm{m}_{2}^{2},\bar{ \sigma}_{t}^{2}d)\lesssim(1-e^{-\bar{\sigma}_{t}^{2}})(\mathrm{m}_{2}^{2}+d)\). Plug this estimation into (23) and we get

\[\sum_{j=0}^{N-1}\int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma_{T-t}^{2} \mathbb{E}[|\nabla\log p_{T-t^{-}_{j}}(Y_{t^{-}_{j}})-\nabla\log p_{T-t}(Y_{t })\|^{2}]dt\] \[\lesssim d\sum_{j=0}^{N-1}\gamma_{j}\int_{T-t^{-}_{j+1}}^{T-t^{-} _{j}}\sigma_{t}^{4}\bar{\sigma}_{t}^{-4}dt+\int_{0}^{t^{-}_{1}}\sigma_{T-t}^{2 }dt\bar{\sigma}_{T}^{-4}\mathrm{m}_{2}^{2}\] \[\quad+(\mathrm{m}_{2}^{2}+d)\sum_{j=1}^{N-1}(1-e^{-\bar{\sigma}_{ T-t^{-}_{j}}^{2}})\big{(}\int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma_{T-t}^{2}\bar{ \sigma}_{T-t}^{-4}dt-\int_{t^{-}_{j-1}}^{t^{-}_{j}}\sigma_{T-t}^{2}\bar{\sigma}_ {T-t}^{-4}dt\big{)}\] \[\lesssim d\sum_{j=0}^{N-1}\gamma_{j}\int_{T-t^{-}_{j+1}}^{T-t^{-} _{j+1}}\sigma_{t}^{4}\bar{\sigma}_{t}^{-4}dt+\mathrm{m}_{2}^{2}\frac{\int_{0}^{t ^{+}_{1}}\sigma_{T-t}^{2}dt}{\bar{\sigma}_{T}^{4}}+(\mathrm{m}_{2}^{2}+d) \sum_{k=1}^{N-1}(1-e^{-\bar{\sigma}_{T-t^{-}_{j}}^{2}})\frac{\bar{\sigma}_{T-t ^{-}_{j}}^{4}-\bar{\sigma}_{T-t^{-}_{j-1}}^{2}}{\bar{\sigma}_{T-t^{-}_{j-1}}^{2} \bar{\sigma}_{T-t^{-}_{j-1}}^{2}},\]

where the last inequality follows from the definition of \(\bar{\sigma}_{t}\) and integration by parts. The proof of Theorem 2 is completed. 

**Lemma 9**.: _Let \(\{Y_{t}\}_{0\leq t\leq T}\) be the solution to (2) with \(f_{t}\equiv 0\) and \(p_{t+s|s}^{\leftarrow}(\cdot|y)\) be the conditional distribution of \(Y_{s+t}\) given \(\{Y_{s}=y\}\). Let \(\{\bar{Y}_{t}\}_{0\leq t\leq T}\) be the solution to (11) \(q_{t+s|s}(\cdot|y)\) be the conditional distribution of \(\bar{Y}_{s+t}\) given \(\{\bar{Y}_{s}=y\}\). Then for any fixed \(t\in(0,\gamma_{j}]\), we have_

\[\mathbb{E}_{y\sim p_{t^{-}_{j}}^{\leftarrow}}\text{KL}\big{(}p_{t^{-}_{j}+t|t^{ -}_{j}}^{\leftarrow}(\cdot|y)|q_{t^{-}_{j}+t|t^{-}_{j}}(\cdot|y)\big{)}\leq\frac{1}{ 2}\sigma_{T-t}^{2}\mathbb{E}[|s(\theta;T-t^{\leftarrow}_{j},Y_{t^{-}_{j}})-\nabla \log p_{T-t}(Y_{t})\|^{2}]\]Proof of Lemma 9.: According to [12, Lemma 6], we have

\[\operatorname{KL}(p_{t^{-}_{j}+t|t^{-}_{j}}^{-}(\cdot|y)|q_{t^{-}_{j} +t|t^{-}_{j}}(\cdot|y))\] \[\leq-2\sigma_{T-t}^{2}\int p_{t^{-}_{j^{-}}+t|t^{-}_{j}}^{-}(x|y) |\nabla\log\frac{p_{t^{-}_{j^{-}}+t|t^{-}_{j}}^{-}(x|y)}{q_{t^{-}_{j}+t|t^{-}_ {j}}^{-}(x|y)}\|^{2}dx\] \[\quad+2\sigma_{T-t}^{2}\mathbb{E}_{p_{t^{-}_{j^{-}}+t|t^{-}_{j}}^ {-}(x|y)}[(\nabla\log p_{T-t}(x)-s(\theta;T-t^{-}_{j},Y_{t^{-}_{j}}),\nabla\log \frac{p_{t^{-}_{j^{-}}+t|t^{-}_{j}}^{-}(x|y)}{q_{t^{-}_{j}+t|t^{-}_{j}}^{-}(x| y)})]\] \[\leq\frac{1}{2}\sigma_{T-t}^{2}\mathbb{E}_{p_{t^{-}_{j^{-}}+t|t^{ -}_{j}}^{-}(x|y)}[|\nabla\log p_{T-t}(x)-s(\theta;T-t^{-}_{j},Y_{t^{-}_{j}})\| ^{2}],\]

where the last inequality follows from Young's inequality. Therefore, Lemma 9 is proved after taking another expectation.

**Lemma 10**.: _For any probability distribution \(p\) satisfying Assumption 3 and \(q\) being a centered multivariate normal distribution with covariance matrix \(\sigma^{2}I_{d}\), we have_

\[\operatorname{KL}(p*q|q)\leq\frac{\operatorname{m}_{2}^{2}}{2\sigma^{2}}.\]

Proof of Lemma 10.: \[\operatorname{KL}(p*q|q) \leq\int\operatorname{KL}(q(\cdot-y)|q(\cdot))p(dy)=\int \operatorname{KL}(\mathcal{N}(y,\sigma^{2}I_{d})|\mathcal{N}(0,\sigma^{2}I_{d }))p(dy)\] \[=\frac{1}{2}\int\ln(1)-d+\operatorname{tr}(I_{d})+|y|^{2}\sigma^{ -2}p(dy)=\frac{\operatorname{m}_{2}^{2}}{2\sigma^{2}},\]

where the inequality follows from convexity of \(\operatorname{KL}(\cdot|q)\) and the second identity follows from KL-divergence between multivariate normal distributions. 

**Lemma 11**.: _Let \(\{X_{t}\}_{0\leq t\leq T}\) be the solution to (1) with \(f_{t}\equiv 0\) and \(p_{0|t}(\cdot|x)\) be the conditional distribution of \(X_{0}\) given \(\{X_{t}=x\}\). Define_

\[m_{t}(X_{t}):=\mathbb{E}_{X\sim p_{0|t}(\cdot|X_{t})}[X],\quad\Sigma_{t}(X_{t} )=\text{Cov}_{X\sim p_{0|t}(\cdot|X_{t})}(X).\] (24)

_Let \(\{Y_{t}\}_{0\leq t\leq T}\) be the solution to (2) with \(f_{t}\equiv 0\) and \(q_{0|t}(\cdot|x)\) be the conditional distribution of \(Y_{0}\) given \(\{Y_{t}=x\}\). Define_

\[\bar{m}_{t}(Y_{t}):=\mathbb{E}_{X\sim q_{0|t}(\cdot|Y_{t})}[X],\quad\bar{ \Sigma}_{t}(Y_{t})=\text{Cov}_{X\sim q_{0|t}(\cdot|Y_{t})}(X).\] (25)

_Then we have for all \(t\in(0,T)\),_

\[d\bar{m}_{t}(Y_{t}) =\sqrt{2}\sigma_{T-t}\bar{\sigma}_{T-t}^{-2}\bar{\Sigma}_{t}(Y_{ t})d\bar{W}_{t},\] \[\text{and}\qquad\frac{d}{dt}\mathbb{E}[\Sigma_{t}(X_{t})] =2\sigma_{t}^{2}\bar{\sigma}_{t}^{-4}\mathbb{E}[\Sigma_{t}(X_{t})^{2}].\]

Proof of Lemma 11.: We first represent \(\nabla log_{t}(X_{t})\) and \(\nabla^{2}\log p_{t}(X_{t})\) via \(m_{t}(X_{t})\) and \(\Sigma_{t}(X_{t})\). Since \(\{X_{t}\}_{0\leq t\leq T}\) solves (1), \(X_{t}=X_{0}+\bar{\sigma}_{t}\xi\) with \((X_{0},\xi)\sim p\otimes\mathcal{N}(0,I_{d})\). Therefore, according to Bayes rule, we have

\[\nabla\log p_{t}(X_{t}) =\frac{1}{p_{t}(X_{t})}\int\nabla\log p_{t|0}(X_{t}|x)p_{0,t}(x,X _{t})dx\] \[=\mathbb{E}_{x\sim p_{0|t}(\cdot|X_{t})}[\bar{\sigma}_{t}^{-2}(X _{t}-x)]\] \[=-\bar{\sigma}_{t}^{-2}(X_{t}-m_{t}(X_{t})),\] (26)where the second identity follows from the fact that \(p_{t|0}(\cdot|x)=\mathcal{N}(x,\bar{\sigma}_{t}^{2}I_{d})\). The last identity follows from the definition of \(m_{t}(X_{t})\) in Lemma 11. Similarly, according to Bayes rule, we can compute

\[\nabla^{2}\log p_{t}(X_{t})\] \[=\frac{1}{p_{t}(X_{t})}\int\,\nabla^{2}\log p_{t|0}(X_{t}|x)p_{0,t }(x,X_{t})dx\] \[\quad+\frac{1}{p_{t}(X_{t})}\int\,\big{(}\nabla\log p_{t|0}(X_{t}| x)\big{)}\big{(}\nabla\log p_{t|0}(X_{t}|x)\big{)}^{\top}p_{0,t}(x,X_{t})dx\] \[\quad-\frac{1}{p_{t}(X_{t})^{2}}\big{(}\int\,\nabla\log p_{t|0}(X_ {t}|x)p_{0,t}(x,X_{t})dx\big{)}\big{(}\int\,\nabla\log p_{t|0}(X_{t}|x)p_{0,t}( x,X_{t})dx\big{)}^{\top}\] \[=-\bar{\sigma}_{t}^{-2}I_{d}+\bar{\sigma}_{t}^{-4}\Sigma_{t}(X_{ t}),\] (27)

where the second identity follows from the fact that \(p_{t|0}(\cdot|x)=\mathcal{N}(x,\bar{\sigma}_{t}^{2}I_{d})\) and the definition of \(\Sigma_{t}(X_{t})\) in Lemma 11.

According to Bayes rule, we have

\[p_{0|t}(dx|X_{t})\propto\exp(-\frac{1}{2}\frac{\|X_{t}-x\|^{2}}{\bar{\sigma}_{ t}^{2}})p(dx)\]

and

\[q_{0|t}(dx|Y_{t}) =Z^{-1}\exp(-\frac{1}{2}\frac{\|Y_{t}-x\|^{2}}{\bar{\sigma}_{T-t} ^{2}})p(dx)\] \[=Z_{t}^{-1}\exp(-\frac{1}{2}\bar{\sigma}_{T-t}^{2}\|x\|^{2}+\bar{ \sigma}_{T-t}^{-2}(x,Y_{t}))p(dx)\] \[:=Z_{t}^{-1}\exp(h_{t}(x))p(dx),\] (28)

where \(Z_{t}=\int\exp(h_{t}(x))p(dx)\) is a (random) normalization constant. From the above computations, we can see that \(q_{0|t}(dx|Y_{t})=p_{0|T-t}(dx|X_{T-t})\) for all \(t\in[0,T]\). Therefore, we have

\[\bar{m}_{t}(Y_{t})=\mathbb{E}_{X\cdot q_{0|t}(\cdot|Y_{t})}[X]=m_{T-t}(X_{T-t }),\quad\bar{\Sigma}_{t}(Y_{t})=\text{Cov}_{X\cdot q_{0|t}(\cdot|Y_{t})}(X)= \Sigma_{T-t}(X_{T-t}),\]

where the identities hold in distribution. Therefore, to prove the first statement, it suffices to compute \(d\bar{m}_{t}(Y_{t})\). To do so, we first compute \(dh_{t}(x)\), \(d[h(x),h(x)]_{t}\), \(dZ_{t}\) and \(d\log Z_{t}\).

\[dh_{t}(x)=\bar{\sigma}_{T-t}^{-3}\dot{\bar{\sigma}}_{T-t}\|x\|^{2}dt-2\bar{ \sigma}_{T-t}^{-3}\dot{\bar{\sigma}}_{T-t}\bar{\sigma}(x,Y_{t})dt+\bar{\sigma} _{T-t}^{-2}(x,dY_{t}).\] (29)

According to the definition of \(Y_{t}\) and (26), we have

\[dY_{t} =2\sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-2}(Y_{t}-\bar{m}_{t}(Y_{t} ))dt+\sqrt{2\sigma_{T-t}^{2}}d\bar{W}_{t}\] \[=-2\sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-2}(Y_{t}-\bar{m}_{t}(Y_{t }))dt+\sqrt{2\sigma_{T-t}^{2}}d\bar{W}_{t}.\]

Therefore

\[d[h(x),h(x)]_{t}=\bar{\sigma}_{T-t}^{-4}|x|^{2}[dY,dY]_{t}=2\sigma_{T-t}^{2} \bar{\sigma}_{T-t}^{-4}\|x\|^{2}.\] (30)

Apply (29) and (30) and we get

\[dZ_{t} =\int\exp(h_{t}(x))\big{(}dh_{t}(x)+\frac{1}{2}d[h(x),h(x)]_{t} \big{)}p(dx)\] \[=\bar{\sigma}_{T-t}^{-3}\dot{\bar{\sigma}}_{T-t}\mathbb{E}_{q_{0|t }(\cdot|Y_{t})}[\|x\|^{2}]Z_{t}dt-2\bar{\sigma}_{T-t}^{-3}\dot{\bar{\sigma}}_ {T-t}(Y_{t},\bar{m}_{t}(Y_{t}))Z_{t}dt\] \[\quad+\bar{\sigma}_{T-t}^{-2}\langle\bar{m}_{t}(Y_{t}),dY_{t} \rangle Z_{t}+\sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-4}\mathbb{E}_{q_{0|t}( \cdot|Y_{t})}[\|x\|^{2}]Z_{t}dt,\] (31)

and

\[d\log Z_{t} =Z_{t}^{-1}dZ_{t}-\frac{1}{2}Z_{t}^{-2}d[Z,Z]_{t}\] \[=-2\bar{\sigma}_{T-t}^{-3}\dot{\bar{\sigma}}_{T-t}(Y_{t},\bar{m} _{t}(Y_{t}))dt+\bar{\sigma}_{T-t}^{-2}\langle\bar{m}_{t}(Y_{t}),dY_{t}\rangle- \sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-4}\|\bar{m}_{t}(Y_{t})\|^{2}dt.\] (32)If we further define \(R_{t}(Y_{t}):=\frac{q_{0|t}(dx|Y_{t})}{p(dx)}=Z_{t}^{-1}\exp(h_{t}(x))\). We have

\[dR_{t}(Y_{t}) =d\exp(\log R_{t}(Y_{t}))=R_{t}(Y_{t})d(\log R_{t}(Y_{t}))+\frac{1 }{2}R_{t}(Y_{t})d[\log R_{t}(Y_{t}),\log R_{t}(Y_{t})]\] \[=-R_{t}(Y_{t})d(\log Z_{t})+R_{t}(Y_{t})dh_{t}(x)+\frac{1}{2}R_{t }(Y_{t})d[h_{t}(x)-\log Z_{t},h_{t}(x)-\log Z_{t}]\] (33)

With (29), (30), (31), (32) and (33), we have

\[d\bar{m}_{t}(Y_{t}) =d\int xR_{t}(Y_{t})p(dx)\] \[=\int x\big{(}-d(\log Z_{t})+dh_{t}(x)+\frac{1}{2}d[h_{t}(x)-\log Z _{t},h_{t}(x)-\log Z_{t}]\big{)}q_{0|t}(dx|Y_{t})\] \[=\sqrt{2}\sigma_{T-t}\bar{\sigma}_{T-t}^{2}\bar{\Sigma}_{t}(Y_{t })d\bar{W}_{t},\] (34)

where most terms cancel in the last identity. Therefore, the first statement is proved. Next, we prove the second statement. We have

\[\frac{d}{dt}\mathbb{E}[\Sigma_{T-t}(X_{T-t})] =\frac{d}{dt}\mathbb{E}[\bar{\Sigma}_{t}(Y_{t})]=\frac{d}{dt} \mathbb{E}[\Sigma_{T-t}(X_{T-t})]\] \[=\frac{d}{dt}\mathbb{E}_{Y_{t}\sim p_{T-t}}[\mathbb{E}_{q_{0|t}( \{Y_{t}\})}[x^{\otimes 2}]-\bar{m}_{t}(Y_{t})^{\otimes 2}]\] \[=\frac{d}{dt}\mathbb{E}_{q_{0}}[x^{\otimes 2}]-\frac{d}{dt} \mathbb{E}[\bar{m}_{t}(Y_{t})^{\otimes 2}]\] \[=-\mathbb{E}[-2\bar{m}_{t}(Y_{t})d\bar{m}_{t}(Y_{t})^{\top}+d[ \bar{m}_{t}(Y_{t}),\bar{m}_{t}(Y_{t})^{\top}]]\] \[=2\bar{\sigma}_{T-t}^{-2}\dot{\sigma}_{T-t}[\bar{\Sigma}_{t}(Y_{t })^{2}]dt\] \[=-2\sigma_{T-t}^{2}\bar{\sigma}_{T-t}^{-4}\mathbb{E}[\Sigma_{t}(X _{T-t})^{2}],\]

where the second last identity follows from (34) and the last identity follows from the definition of \(\bar{\sigma}_{t}\). Last, we reverse the time and get

\[\frac{d}{dt}\mathbb{E}[\Sigma_{t}(X_{t})]=2\sigma_{t}^{2}\bar{\sigma}_{t}^{-4} \mathbb{E}[\Sigma_{t}(X_{t})^{2}].\]

The proof is completed. 

**Lemma 12**.: _Under the conditions in Lemma 11, let \(\{Y_{t}\}_{0\leq t\leq T}\) be the solution to (2) with \(f_{t}\equiv 0\). Define \(L_{t}:=\nabla\log p_{T-t}(Y_{t})\), then for any \(t\in[t_{j}^{-},t_{j+1}^{-})\), we have_

\[\mathbb{E}[\lfloor L_{t}-L_{t_{j}^{-}}\rVert^{2}]=2d\int_{t_{j}^{-}}^{t}\sigma _{T-u}^{2}\bar{\sigma}_{T-u}^{-4}du+\bar{\sigma}_{T-t_{j}^{-}}^{-4}\mathbb{E}[ \operatorname{tr}(\Sigma_{T-t_{j}^{-}}(X_{T-t_{j}^{-}}))]-\bar{\sigma}_{T-t}^{- 4}\mathbb{E}[\operatorname{tr}(\Sigma_{T-t}(X_{T-t}))]\]

Proof of Lemma 12.: First, according to the definition of \(L_{t}\) and \(Y_{t}\), it follows from Ito's lemma that

\[dL_{t} =\nabla^{2}\log p_{T-t}(Y_{t})\big{(}2\sigma_{T-t}^{2}\nabla\log p _{T-t}(Y_{t})dt+\sqrt{2\sigma_{T-t}}d\bar{W}_{t}\big{)}\] (35) \[\quad+\Delta\big{(}\nabla\log p_{T-t}(Y_{t})\big{)}\sigma_{T-t}^{ 2}dt+\frac{d(\nabla\log p_{T-t})}{dt}\big{(}Y_{t})dt\] (36) \[=\sqrt{2\sigma_{T-t}^{2}}\nabla^{2}\log p_{T-t}(Y_{t})d\tilde{W}_ {t},\] (37)

where the last step follows from applying the Fokker Planck equation of (1) with \(f_{t}\equiv 0\), i.e., \(\partial_{t}p_{t}=\sigma_{t}^{2}\Delta p_{t}\). Most of the terms are cancelled after applying the Fokker Planck equation. Now, for fixed \(s>0\) and \(t>s\), define \(E_{s,t}:=\mathbb{E}[\|L_{t}-L_{s}\|^{2}]\). Apply Ito's lemma and (35), we have

\[dE_{s,t} =2\mathbb{E}[\{(L_{t}-L_{s},dL_{t})\}+d[L]_{t}\] \[=2\mathbb{E}[\{(L_{t}-L_{s},\sqrt{2\sigma_{T-t}^{2}}\nabla\log p _{T-t}(Y_{t})d\tilde{W}_{t}\}]+2\sigma_{T-t}^{2}\mathbb{E}[\|\nabla^{2}\log p _{T-t}(Y_{t})\|_{F}^{2}]dt\] \[=2\sigma_{T-t}^{2}\mathbb{E}[\|\nabla^{2}\log p_{T-t}(Y_{t})\|_{F }^{2}]dt,\] (38)

[MISSING_PAGE_FAIL:33]

Proof of Lemma 13.: \(p_{\delta}=p*\mathcal{N}(0,\bar{\sigma}_{\delta}^{2})=\mathcal{N}(m,(\sigma^{2}+ \bar{\sigma}_{\delta}^{2})I_{d})\) and \(q_{T-\delta}=\text{Law}(\bar{Y}_{t_{N}^{*}})=\mathcal{N}(m_{N},\Sigma_{N})\) with \((m_{N},\Sigma_{N})\) given in Lemma 15. Therefore, we have

\[\text{KL}(p_{\delta}|q_{T-\delta}) =\text{KL}(\mathcal{N}(m,(\sigma^{2}+\bar{\sigma}_{\delta}^{2}) I_{d})]\mathcal{N}(m_{N},\Sigma_{N}))\] \[=\frac{1}{2}\log\frac{\det(\Sigma_{N})}{\det((\sigma^{2}+\bar{ \sigma}_{\delta}^{2})I_{d})}-\frac{d}{2}+\frac{1}{2}\operatorname{tr}((\sigma ^{2}+\bar{\sigma}_{\delta}^{2})\Sigma_{N}^{-1})+(m_{N}-m)^{\top}\Sigma_{N}^{-1 }(m-m_{N})\] \[=\frac{d}{2}\log\left(\frac{(\sigma^{2}+\bar{\sigma}_{\delta}^{2 })\bar{\sigma}_{T}^{2}}{(\sigma^{2}+\bar{\sigma}_{T}^{2})^{2}}+\sum_{j=0}^{N-1 }\frac{(\sigma^{2}+\bar{\sigma}_{\delta}^{2})\big{(}\bar{\sigma}_{T-t_{j}^{* }}^{2}-\bar{\sigma}_{T-t_{j+1}^{*}}^{2}\big{)}}{(\sigma^{2}+\bar{\sigma}_{T-t_{ j+1}^{*}}^{2})^{2}}\right)-\frac{d}{2}\] \[\quad+\frac{d}{2}\Bigg{(}\frac{(\sigma^{2}+\bar{\sigma}_{\delta} ^{2})\bar{\sigma}_{T}^{2}}{(\sigma^{2}+\bar{\sigma}_{T}^{2})^{2}}+\sum_{j=0}^{ N-1}\frac{(\sigma^{2}+\bar{\sigma}_{\delta}^{2})\big{(}\bar{\sigma}_{T-t_{j}^{* }}^{2}-\bar{\sigma}_{T-t_{j+1}^{*}}^{2}\big{)}}{(\sigma^{2}+\bar{\sigma}_{T-t_ {j+1}^{*}}^{2})^{2}}\Bigg{)}^{-1}\] \[\quad+\Bigg{(}\bar{\sigma}_{T}^{2}+\big{(}\sigma^{2}+\bar{\sigma }_{T}^{2}\big{)}^{2}\sum_{j=0}^{N-1}\frac{(\bar{\sigma}_{T-t_{j}^{*}}^{2}-\bar {\sigma}_{T-t_{j+1}^{*}}^{2})}{(\sigma^{2}+\bar{\sigma}_{T-t_{j+1}^{*}}^{2})^ {2}}\Bigg{)}^{-1}\|m\|^{2}.\]

**Lemma 14** (Explicit score function for mixture of Gaussian target).: _Assume the data distribution has density given by (39), then the score function is given by_

\[\nabla\log p_{t}(x)=-\frac{x-m}{\sigma^{2}+\bar{\sigma}_{t}^{2}}.\] (42)

Proof.: Since the forward process (1) with \(f_{t}\equiv 0\) is the just a process that keeps adding noise, the density \(p_{t}\) along the process is a convolution between data density and a Gaussian density with mean zero and covariance \(\bar{\sigma}_{t}^{2}I_{d}\):

\[p_{t}(x)=p*\mathcal{N}(\cdot;0,\bar{\sigma}_{t}^{2}I_{d})(x)=\mathcal{N}(x;m,( \sigma^{2}+\bar{\sigma}_{t}^{2})I_{d}).\]

Therefore, we have

\[\nabla p_{t}(x) =(2\pi(\sigma^{2}+\bar{\sigma}_{t}^{2}))^{-d/2}\exp\big{(}-\frac {\|x-m\|^{2}}{2(\sigma^{2}+\bar{\sigma}_{t}^{2})}\big{)}\big{(}-\frac{x-m}{ \sigma^{2}+\bar{\sigma}_{t}^{2}}\big{)}\] \[=-\frac{x-m}{\sigma^{2}+\bar{\sigma}_{t}^{2}}\mathcal{N}(x;m,( \sigma^{2}+\bar{\sigma}_{t}^{2})I_{d}).\]

(42) follows directly from the above computations. 

**Lemma 15** (Gaussian iterates along the trajectory).: _Assume the data distribution has density given by (39). Let \((\bar{Y}_{t_{j}^{*}})\) be defined by (40) with initial condition \(\bar{Y}_{0}\sim\mathcal{N}(0,\bar{\sigma}_{T}^{2}I_{d})\). Then for all \(0\leq j\leq N\), \(\bar{Y}_{t_{j}^{*}}\sim\mathcal{N}(m_{j},\Sigma_{j})\) with_

\[m_{j} =\frac{\bar{\sigma}_{T-t_{0}^{*}}^{2}+\bar{\sigma}_{T-t_{j}^{*}}^{ 2}}{\sigma^{2}+\bar{\sigma}_{T-t_{j}^{*}}^{2}}m,\] (43) \[\Sigma_{j} =\Bigg{(}\frac{(\sigma^{2}+\bar{\sigma}_{T-t_{j}^{*}}^{2})^{2} \bar{\sigma}_{T}^{2}}{(\sigma^{2}+\bar{\sigma}_{T-t_{j}^{*}}^{2})^{2}}+\sum_{l=0 }^{j-1}\frac{(\sigma^{2}+\bar{\sigma}_{T-t_{j}^{*}}^{2})^{2}(\bar{\sigma}_{T-t_ {l}^{*}}^{2}-\bar{\sigma}_{T-t_{l+1}^{*}}^{2})}{(\sigma^{2}+\bar{\sigma}_{T-t_ {l+1}^{*}}^{2})^{2}}\Bigg{)}I_{d}.\] (44)

Proof of Lemma 15.: According to Lemma 14, (40) can be written as

\[d\bar{Y}_{t}=-2\sigma_{T-t}^{2}\frac{\bar{Y}_{t_{j}^{*}}-m}{\sigma^{2}+\bar{ \sigma}_{T-t_{j}^{*}}^{2}}dt+\sqrt{2\sigma_{T-t}^{2}}d\bar{W}_{t},\]which implies that for any \(j\in 0,1,\cdots,N-1\):

\[\bar{Y}_{t^{-}_{j+1}}-\bar{Y}_{t^{-}_{j}} =-2\frac{\int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma^{2}_{T-t}dt}{\sigma^ {2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}\big{(}\bar{Y}_{t^{-}_{j}}-m\big{)}+\sqrt{2 \int_{t^{-}_{j}}^{t^{-}_{j+1}}\sigma^{2}_{T-t}dt}U_{j+1}\] \[=-\frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_ {j+1}}}{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}\big{(}\bar{Y}_{t^{-}_{j}}-m \big{)}+\sqrt{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_{j+1}}}U _{j+1},\] \[\implies\bar{Y}_{t^{-}_{j+1}} =\big{(}1-\frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_ {T-t^{-}_{j+1}}}{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}\big{)}\bar{Y}_{t^ {-}_{j}}+\frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_{j+1} }}{\sigma^{2}+\bar{\sigma}^{2}_{t^{-}_{j}}}m+\sqrt{\bar{\sigma}^{2}_{T-t^{-}_ {j}}-\bar{\sigma}^{2}_{T-t^{-}_{j+1}}}U_{j+1}\] (45)

where \((U_{j})_{j=1}^{N}\) are i.i.d. standard Gaussian vectors in \(\mathbb{R}^{d}\). Since \(\bar{Y}_{t^{-}_{0}}\) is Gaussian, by induction, we prove that \(\bar{Y}_{t^{-}_{j}}\) is Gaussian for all \(j=1,\cdots,N\). Denote \(\bar{Y}_{t^{-}_{j}}\sim\mathcal{N}(m_{j},\Sigma_{j})\). According to (45) and the independence between \(U_{j+1}\) and \(\bar{Y}_{t^{-}_{j}}\), we have

\[m_{j+1}=\big{(}1-\frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma }^{2}_{T-t^{-}_{j+1}}}{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}\big{)}m_{j}+ \frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_{j+1}}}{\sigma^ {2}+\bar{\sigma}^{2}_{t^{-}_{j}}}m,\] \[\implies m_{j+1}-m=\frac{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j+1}}}{\sigma ^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}(m_{j}-m),\] \[\implies m_{j}=\frac{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}{\sigma ^{2}+\bar{\sigma}^{2}_{T-t^{-}_{0}}}(m_{0}-m)+m=\frac{\bar{\sigma}^{2}_{T-t^{ -}_{0}}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_ {0}}}m.\]

Again, according to (45) and the independence between \(U_{j+1}\) and \(\bar{Y}_{t^{-}_{j}}\), we get a relation between consecutive covariance matrices:

\[\Sigma_{j+1}=\big{(}1-\frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{ \sigma}^{2}_{T-t^{-}_{j+1}}}{\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}}}\big{)}^{ 2}\Sigma_{j}+\big{(}\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_{ j+1}}\big{)}I_{d},\] \[\implies \frac{\Sigma_{j+1}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j+1}} )^{2}}=\frac{\Sigma_{j}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}})^{2}}+ \frac{\bar{\sigma}^{2}_{T-t^{-}_{j}}-\bar{\sigma}^{2}_{T-t^{-}_{j+1}}}{(\sigma ^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j+1}})^{2}}I_{d},\] \[\implies \frac{\Sigma_{j}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}})^{2}} =\frac{\Sigma_{0}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{0}})^{2}}+ \sum_{l=0}^{j-1}\frac{\bar{\sigma}^{2}_{T-t^{-}_{l}}-\bar{\sigma}^{2}_{T-t^{- }_{l+1}}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{l+1}})^{2}}I_{d},\] \[\implies \Sigma_{j}=\bigg{(}\frac{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{ j}})^{2}\bar{\sigma}^{2}_{T}}{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{0}})^{2}}+ \sum_{l=0}^{j-1}\frac{(\sigma^{2}+\bar{\sigma}^{2}_{T-t^{-}_{j}})^{2}(\bar{ \sigma}^{2}_{T-t^{-}_{l}}-\bar{\sigma}^{2}_{T-t^{-}_{l+1}})}{(\sigma^{2}+\bar{ \sigma}^{2}_{T-t^{-}_{l+1}})^{2}}\bigg{)}I_{d}.\]

## Appendix G Full error analysis

Proof of Theorem 3.: We only need to deal with \(E_{S}\). By applying the same schedules to training objective, we obtain

\[E_{S} =\sum_{j>0}^{N-1}\frac{\sigma^{2}_{t_{N-j}}}{w(t_{N-j})}\cdot w(t _{N-j})(t_{N-j}-t_{N-j-1})\frac{1}{\bar{\sigma}_{t_{N-j}}}\mathbb{E}_{X_{0}} \mathbb{E}_{\xi}\|\bar{\sigma}_{t_{N-j}}s(\theta;t_{N-j},X_{t_{N-j}})+\xi\|^{2}\] \[\qquad+\sum_{j=0}^{N-1}\frac{\sigma^{2}_{t_{N-j}}}{w(t_{N-j})}\cdot w (t_{N-j})(t_{N-j}-t_{N-j-1})\cdot C\] \[\leq\max_{j}\frac{\sigma^{2}_{t_{N-j}}}{w(t_{N-j})}\cdot(\bar{ \mathcal{L}}(W)+\bar{C}).\]Together with

\[\bar{\mathcal{L}}(W^{(k)})+\bar{C}\leq|\bar{\mathcal{L}}(W^{(K)})- \bar{\mathcal{L}}_{em}(W^{(K)})+\bar{\mathcal{L}}_{em}(\theta^{*})-\bar{\mathcal{ L}}(\theta^{*})|+|\bar{\mathcal{L}}_{em}(W^{(K)})-\bar{\mathcal{L}}_{em}( \theta^{*})|\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad+|\bar{\mathcal{L}}( \theta^{*})-\bar{\mathcal{L}}(\theta_{\mathcal{F}})|+|\bar{\mathcal{L}}( \theta_{\mathcal{F}})+\bar{C}|,\]

we have the result. 

## Appendix H Proofs for Section 4.1

### Proof of "bell-shaped" curve

Proof of Proposition 1.: Fix \(x_{i},\xi_{ij},\bar{\sigma}_{t_{N}}\). By definition of the network \(S(\theta;t_{j},X_{ij})\), it is continuous with respect to \(X_{ij}\).

For 1, \(X_{ij}=x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij}\), and thus \(S(\theta;t_{j},X_{ij})\) is also continuous w.r.t. \(\bar{\sigma}_{t_{j}}\). Also, since \(\bar{\sigma}_{t_{j}}\in[0,\bar{\sigma}_{t_{N}}]\), there exists \(M_{0}>0\), s.t.,

\[S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij})\in[-M_{0},M_{0}]^{d}.\]

Then for any \(\epsilon_{1}>0\), there exists \(\delta=\frac{\epsilon_{1}}{\sqrt{dM_{0}}}>0\), s.t., \(\forall\ 0\leq\bar{\sigma}_{t_{j}}<\delta_{1}\), we have

\[\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}} \xi_{ij})+\xi_{ij}\| \geq|\xi_{ij}|-\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{ \sigma}_{t_{j}}\xi_{ij})|\] \[\geq|\xi_{ij}|-\sqrt{d}M_{0}\bar{\sigma}_{t_{j}}\] \[\geq|\xi_{ij}|-\epsilon_{1}.\]

For 2, by the positive homogeniety of ReLU,

\[S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij}) =\bar{\sigma}_{t_{j}}S\left(\theta;t_{j},\frac{x_{i}}{\bar{ \sigma}_{t_{j}}}+\xi_{ij}\right).\]

Consider \(\bar{\sigma}_{t_{j}}\geq M\), for some \(M>0\). Then

\[\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}} \xi_{ij})+\xi_{ij}\| =\left\|\bar{\sigma}_{t_{j}}^{2}S\left(\theta;t_{j},\frac{x_{i}}{ \bar{\sigma}_{t_{j}}}+\xi_{ij}\right)+\xi_{ij}\right\|\] \[\geq\left\|\bar{\sigma}_{t_{j}}^{2}S\left(\theta;t_{j},\frac{x_{i }}{\bar{\sigma}_{t_{j}}}+\xi_{ij}\right)\right\|-\|\xi_{ij}\|\] \[\geq M^{2}\left\|S\left(\theta;t_{j},\frac{x_{i}}{\bar{\sigma}_{ t_{j}}}+\xi_{ij}\right)\right\|-\|\xi_{ij}\right|.\] (46)

For any \(y\in\mathcal{D}(M)\), where \(\mathcal{D}(M)=\{y\in\mathbb{R}^{d}:y_{s}\in[(\xi_{ij})_{s}-|(x_{i})_{s}|/M,( \xi_{ij})_{s}+|(x_{i})_{s}|/M],\ \forall\ s=1,\cdot\cdot\cdot,d\}\),

\[|S(\theta;t_{j},y)|\geq\|S(\theta;t_{j},\xi_{ij})\|-|S(\theta;t_{j},y)-S( \theta;t_{j},\xi_{ij})\|.\] (47)

Since \(S\) is differentiable a.e., by the fundamental theorem of calculus,

\[S(\theta;t_{j},y)-S(\theta;t_{j},\xi_{ij})=\int_{\xi_{ij}}^{y}S_{x}^{\prime}( \theta;t_{j},x)dx.\]

Then

\[\|S(\theta;t_{j},y)-S(\theta;t_{j},\xi_{ij})\|\leq\frac{1}{M}\cdot M_{1},\] (48)

where \(M_{1}=\max_{s}(x_{i})_{s}\cdot\mathrm{ess}\sup_{x\in\mathcal{D}(M_{2})}\|S_{x }^{\prime}(\theta;t_{j},x)\|<+\infty\) for some fixed \(0<M_{2}<M\).

Combining (46),(47), and (48), we have

\[\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}} \xi_{ij})+\xi_{ij}\| \geq M^{2}\left(\|S(\theta;t_{j},\xi_{ij})\|-\frac{M_{1}}{M} \right)-|\xi_{ij}|\] \[=M^{2}\left(\|S(\theta;t_{j},\xi_{ij})\|-\frac{M_{1}}{M}-\frac{| \xi_{ij}|}{M^{2}}\right).\]

Then \(\forall\ \epsilon_{2}>0\), there exists \(M=\max\left\{\frac{M_{1}+\sqrt{M_{1}^{2}+4\epsilon_{2}\|\xi_{ij}\|}}{2 \epsilon_{2}},M_{2}\right\}>0\), s.t., when \(\bar{\sigma}_{t_{j}}>M\), we have

\[\|\bar{\sigma}_{t_{j}}S(\theta;t_{j},x_{i}+\bar{\sigma}_{t_{j}}\xi_{ij})+\xi_{ij }\| \geq M^{2}(|S(\theta;t_{j},\xi_{ij})|-\epsilon_{2}).\]

### Proof of optimal rate

Proof of Corollary 2.: If \(|f(\theta^{(k)};i,j)-f(\theta^{(k)};l,s)|\leq\epsilon\) for all \(i,j,l,s\) and \(k>K\), then by Lemma 1 and 7, we choose the maximum \(f(\theta^{(k)};i,j)\) for the lower bound, which is of order \(\mathcal{O}(\epsilon)\) away from the other \(f(\theta^{(k)};i,j)^{\prime}s\). Therefore, we can take \(j^{*}(k)=\arg\max_{j}f(\theta^{(k)};i,j)\) and absorb the \(\mathcal{O}(\epsilon)\) error in constant factors. Then the result naturally follows. 

### Proof of comparisons of \(E_{s}\)

Recall that the training objective of EDM is defined in the following

\[\mathbb{E}_{\bar{\sigma}\sim p_{\text{train}}}\mathbb{E}_{y,n}\lambda(\bar{ \sigma})\|D_{\theta}(y+n;\bar{\sigma})-y\|^{2}=\frac{1}{Z_{1}}\int\,\frac{1}{ \bar{\sigma}}e^{-\frac{(\log\sigma-p_{\text{mean}})^{2}}{2P_{\text{std}}^{2 }}}\cdot\frac{\bar{\sigma}^{2}+\sigma_{\text{data}}^{2}}{\bar{\sigma}^{2} \sigma_{\text{data}}^{2}}\cdot\bar{\sigma}^{2}\mathbb{E}_{X_{0},\xi}\big{|} \bar{\sigma}s(\theta;t,X_{t})+\xi\big{|}^{2}\,d\bar{\sigma}.\]

Let \(\beta_{j}=C_{1}\beta_{\text{EDM}}\), i.e.,

\[\frac{w(t_{j})}{\bar{\sigma}_{t_{j}}}(t_{j}-t_{j-1}) =C_{1}\cdot e^{-\frac{(\log\sigma_{t_{j}}-p_{\text{mean}})^{2}}{2P _{\text{std}}^{2}}}\cdot\frac{\bar{\sigma}_{t_{j}}^{2}+\sigma_{\text{data}}^{ 2}}{\bar{\sigma}_{t_{j}}^{2}\sigma_{\text{data}}^{2}}\cdot\bar{\sigma}_{t_{j}}\] \[w(t_{j}) =C_{1}\cdot\frac{\bar{\sigma}_{t_{j}}}{t_{j}-t_{j-1}}\cdot e^{- \frac{(\log\sigma_{t_{j}}-p_{\text{mean}})^{2}}{2P_{\text{std}}^{2}}}\cdot \frac{\bar{\sigma}_{t_{j}}^{2}+\sigma_{\text{data}}^{2}}{\bar{\sigma}_{t_{j} }^{2}\sigma_{\text{data}}^{2}}\cdot\bar{\sigma}_{t_{j}}\]

**EDM.** Consider \(\bar{\sigma}_{t}=t\) and \(t_{j}=\left(\bar{\sigma}_{\text{max}}^{1/\rho}-(\bar{\sigma}_{\text{max}}^{1/ \rho}-\bar{\sigma}_{\text{min}}^{1/\rho})\frac{N-j}{N}\right)^{\rho}\) for \(j=0,\cdots,N\). Then

\[w(t_{j}) =C_{1}\cdot\frac{t_{j}}{t_{j}-t_{j-1}}\cdot e^{-\frac{(\log \sigma_{t}-p_{\text{mean}})^{2}}{2P_{\text{std}}^{2}}}\cdot\frac{t_{j}^{2}+ \sigma_{\text{data}}^{2}}{t_{j}\sigma_{\text{data}}^{2}}\] \[=C_{1}\cdot\frac{\left(\bar{\sigma}_{\text{max}}^{1/\rho}-(\bar{ \sigma}_{\text{max}}^{1/\rho}-\bar{\sigma}_{\text{min}}^{1/\rho})\frac{N-j}{N }\right)^{\rho}-\left(\bar{\sigma}_{\text{max}}^{1/\rho}-(\bar{\sigma}_{\text{ max}}^{1/\rho}-\bar{\sigma}_{\text{min}}^{1/\rho})\frac{N-j+1}{N}\right)^{\rho}}{N}\] \[\cdot e^{-\frac{\left(\log\left\{\bar{\sigma}_{\text{max}}^{1/ \rho}-(\bar{\sigma}_{\text{max}}^{1/\rho}-\bar{\sigma}_{\text{min}}^{1/\rho} )\frac{N-j}{N}\right\}^{\rho}-\bar{\sigma}_{\text{mean}}\right)^{2}}{2P_{ \text{std}}^{2}}}\cdot\frac{\left(\bar{\sigma}_{\text{max}}^{1/\rho}-(\bar{ \sigma}_{\text{max}}^{1/\rho}-\bar{\sigma}_{\text{min}}^{1/\rho})\frac{N-j}{N }\right)^{2\rho}+\sigma_{\text{data}}^{2}}{\sigma_{\text{data}}^{2}}\]

Then the maximum of \(\frac{\sigma_{t_{j}}^{2}}{w(t_{j})}=\frac{t_{j}}{w(t_{j})}\) appears at \(j=N\)

\[\max_{j}\frac{\sigma_{t_{j}}^{2}}{w(t_{j})}=\max_{j}\frac{t_{j}}{w(t_{j})}= \frac{\bar{\sigma}_{\text{max}}\sigma_{\text{data}}^{2}e^{\frac{(\bar{\sigma} _{\text{max}}-\log\sigma_{\text{max}})^{2}}{2P_{\text{std}}^{2}}}}{C_{1}(\bar {\sigma}_{\text{max}}^{2}+\sigma_{\text{data}}^{2})}\cdot\left(\bar{\sigma}_{ \text{max}}-\left(\bar{\sigma}_{\text{max}}^{1/\rho}-\bar{\sigma}_{\text{max}}^ {1/\rho}\right)^{\rho}\right)\]

**Song et al. [46]**. Consider \(\bar{\sigma}_{t}=\sqrt{t}\) and \(t_{j}=\bar{\sigma}_{\text{max}}^{2}\left(\frac{\bar{\sigma}_{\text{min}}^{2}}{ \bar{\sigma}_{\text{max}}^{2}}\right)^{\frac{N-j}{N}}\) for \(j=0,\cdots,N\). Then

\[w(t_{j}) =C_{1}\cdot\frac{\sqrt{t_{j}}}{t_{j}-t_{j-1}}\cdot e^{-\frac{( \log\sqrt{t_{j}}-p_{\text{mean}})^{2}}{2P_{\text{std}}^{2}}}\cdot\frac{t_{j}+ \sigma_{\text{data}}^{2}}{\sqrt{t_{j}}\sigma_{\text{data}}^{2}}\] \[=C_{1}\cdot\frac{1}{\bar{\sigma}_{\text{max}}^{2}\left(\frac{\bar{ \sigma}_{\text{min}}^{2}}{\bar{\sigma}_{\text{max}}^{2}}\right)^{\frac{N-j}{N} -}-\bar{\sigma}_{\text{max}}^{2}\left(\frac{\bar{\sigma}_{\text{min}}^{2}}{\bar {\sigma}_{\text{max}}^{2}}\right)^{\frac{N-j+1}{N}}}\cdot e^{-\frac{\left(\log \sigma_{\text{max}}\left(\frac{\bar{\sigma}_{\text{min}}^{2}}{\bar{\sigma}_{ \text{max}}^{2}}\right)^{\frac{N-j}{N}-F_{\text{mean}}}\right)^{2}}{2P_{ \text{std}}^{2}}}\cdot\frac{\bar{\sigma}_{\text{max}}^{2}\left(\frac{\bar{ \sigma}_{\textProofs for Section 4.2

### Proof when \(E_{l}+e_{d}\) dominates.

Under the EDM choice of variance, \(\bar{\sigma}_{t}=t\) for all \(t\in[0,T]\), and study the optimal time schedule when \(E_{D}+E_{I}\) dominates. First, it follows from Theorem 2 that

\[E_{I}+E_{D}\lesssim \frac{\mathrm{m}_{2}^{2}}{T^{2}}+d\sum_{j=0}^{N-1}\frac{\gamma_{j} ^{2}}{(T-t_{j}^{-})^{2}}\] \[+(\mathrm{m}_{2}^{2}+d)\big{(}\sum_{T-t_{j}^{-}\geq 1}\frac{ \gamma_{j}^{2}}{(T-t_{j}^{-})^{4}}+\frac{\gamma_{j}^{3}}{(T-t_{j}^{-})^{5}}+ \sum_{T-t_{j}^{-}<1}\frac{\gamma_{j}^{2}}{(T-t_{j}^{-})^{2}}+\frac{\gamma_{j}^ {3}}{(T-t_{j}^{-})^{3}}\big{)}\]

Based on the above time schedule dependent error bound, we quantify the errors under polynomial time schedule and exponential time schedule.

**Polynomial time schedule.** we consider \(T-t_{j}^{-}=(\delta^{1/a}+(N-j)h)^{a}\) with \(h=\frac{T^{1/a}-\delta^{1/a}}{N}\) and \(a>1\), \(\gamma_{j}=a(\delta^{1/a}+(N-j-\vartheta)h)^{a-1}h\) for some \(\vartheta\in(0,1)\). We have \(\gamma_{j}/h\sim a(T-t_{j}^{-})^{\frac{a-1}{a}}\) and

\[E_{I}+E_{D}\lesssim\frac{\mathrm{m}_{2}^{2}}{T^{2}}+\frac{da^{2}T^{\frac{1}{ a}}}{\delta^{\frac{1}{a}}N}+(\mathrm{m}_{2}^{2}+d)\big{(}\frac{a^{2}T^{\frac{1}{a}}}{ \delta^{\frac{1}{a}}N}+\frac{a^{3}T^{\frac{1}{a}}}{\delta^{\frac{1}{a}}N^{2} }\big{)}\]

Therefore, to obtain \(E_{I}+E_{D}\lesssim\varepsilon\), it suffices to require \(T=\Theta\big{(}\frac{\mathrm{m}_{2}}{\varepsilon^{1/2}}\big{)}\) and the iteration complexity

\[N=\Omega\big{(}a^{2}\big{(}\frac{\mathrm{m}_{2}}{\delta\varepsilon^{\frac{1}{ 2}}}\big{)}^{\frac{1}{a}}\frac{\mathrm{m}_{2}^{2}+d}{\varepsilon}\big{)}\]

For fixed \(\mathrm{m}_{2},\delta\) and \(\varepsilon\), optimal value of \(a\) that minimizes the iteration complexity \(N\) is \(a=\frac{1}{2}\ln(\frac{\mathrm{m}_{2}}{\delta\varepsilon^{1/2}})\). Once we let \(\delta=\bar{\sigma}_{\min}\), \(T=\bar{\sigma}_{\max}=\Theta\big{(}\frac{\mathrm{m}_{2}}{\varepsilon^{1/2}} \big{)}\) and \(a=\rho\), the iteration complexity is

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}\lor d}{d}\rho^{2}\big{(}\frac{\bar{ \sigma}_{\max}}{\bar{\sigma}_{\min}}\big{)}^{1/\rho}\bar{\sigma}_{\max}^{2} \big{)},\]

and it is easy to see that our theoretical result supports what's empirically observed in EDM that there is an optimal value of \(\rho\) that minimizes the FID.

**Exponential time schedule.** we consider \(\gamma_{j}=\kappa(T-t_{j}^{-})\) with \(\kappa=\frac{\ln(T/\delta)}{N}\), we have

\[E_{I}+E_{D}\lesssim\frac{\mathrm{m}_{2}^{2}}{T^{2}}+\frac{d\ln(T/\delta)^{2}}{ N}+(\mathrm{m}_{2}^{2}+d)\big{(}\frac{\ln(T/\delta)^{2}}{N}+\frac{\ln(T/ \delta)^{3}}{N^{2}}\big{)}\]

Therefore, to obtain \(E_{I}+E_{D}\lesssim\varepsilon\), it suffices to require \(T=\Theta\big{(}\frac{\mathrm{m}_{2}}{\varepsilon^{\frac{1}{2}}}\big{)}\) and the iteration complexity

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}+d}{\varepsilon}\ln\big{(}\frac{\mathrm{ m}_{2}}{\delta\varepsilon^{\frac{1}{2}}}\big{)}^{2}\big{)}\]

When \(\mathrm{m}_{2}\leq O(\sqrt{d})\), the exponential time schedule is asymptotic optimal, hence it is better than the polynomial time schedule when the initilization error and discretization error dominate. Once we let \(\delta=\bar{\sigma}_{\min}\), \(T=\bar{\sigma}_{\max}=\Theta\big{(}\frac{\mathrm{m}_{2}}{\varepsilon^{1/2}} \big{)}\), the iteration complexity is

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}\lor d}{d}\ln\big{(}\frac{\bar{\sigma}_{ \max}}{\bar{\sigma}_{\min}}\big{)}^{2}\bar{\sigma}_{\max}^{2}\big{)}.\]

Now we adopt the variance schedule in [46], \(\bar{\sigma}_{t}=\sqrt{t}\) for all \(t\in[0,T]\), it follows from Theorem 2 that

\[E_{I}+E_{D}\lesssim\frac{\mathrm{m}_{2}^{2}}{T}+d\sum_{j=0}^{N-1}\frac{\gamma_ {j}^{2}}{(T-t_{j}^{-})^{2}}+(\mathrm{m}_{2}^{2}+d)\big{(}\sum_{T-t_{j}^{-}\geq 1 }\frac{\gamma_{j}^{2}}{(T-t_{j}^{-})^{3}}+\sum_{T-t_{j}^{-}<1}\frac{\gamma_{j}^ {2}}{(T-t_{j}^{-})^{2}}\big{)}\]

**Polynomial time schedule.** we consider \(T-t_{j}^{-}=(\delta^{1/a}+(N-j)h)^{a}\) with \(h=\frac{T^{1/a}-\delta^{1/a}}{N}\) and \(a>1\), \(\gamma_{j}=a(\delta^{1/a}+(N-j-\vartheta)h)^{a-1}h\) for some \(\vartheta\in(0,1)\). We have \(\gamma_{j}/h\sim a(T-t_{j}^{-})^{\frac{a-1}{a}}\) and

\[E_{I}+E_{D}\lesssim\frac{\mathrm{m}_{2}^{2}}{T}+\frac{da^{2}T^{\frac{1}{a}}}{ \delta^{\frac{1}{a}}N}+(\mathrm{m}_{2}^{2}+d)\frac{a^{2}T^{\frac{1}{a}}}{ \delta^{\frac{1}{a}}N}\]Therefore, to obtain \(E_{I}+E_{D}\lesssim\varepsilon\), it suffices to require \(T=\Theta\big{(}\frac{\mathrm{m}_{2}^{2}}{\varepsilon}\big{)}\) and the iteration complexity

\[N=\Omega\big{(}a^{2}\big{(}\frac{\mathrm{m}_{2}^{2}}{\delta\varepsilon}\big{)}^ {\frac{1}{a}}\frac{\mathrm{m}_{2}^{2}+d}{\varepsilon}\big{)}\]

Once we let \(\delta=\bar{\sigma}_{\min}^{2}\), \(T=\bar{\sigma}_{\max}^{2}=\Theta\big{(}\frac{\mathrm{m}_{2}^{2}}{\varepsilon} \big{)}\) and \(a=\rho\), the iteration complexity is

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}\lor d}{d}\rho^{2}\big{(}\frac{\bar{ \sigma}_{\max}}{\bar{\sigma}_{\min}}\big{)}^{2/\rho}\bar{\sigma}_{\max}^{2} \big{)}.\]

Compared to exponential time schedule with the EDM choice of variance schedule, this iteration complexity is worse up to a factor \(\big{(}\frac{\bar{\sigma}_{\max}}{\bar{\sigma}_{\min}}\big{)}^{1/\rho}\).

**Exponential time schedule.** we consider \(\gamma_{j}=\kappa\big{(}T-t_{j}^{-}\big{)}\) with \(\kappa=\frac{\ln(\mathcal{T}/\delta)}{N}\), we have

\[E_{I}+E_{D}\lesssim\frac{\mathrm{m}_{2}^{2}}{T}+\frac{d\ln(T/\delta)^{2}}{N}+ (\mathrm{m}_{2}^{2}+d)\frac{\ln(T/\delta)^{2}}{N}\]

Therefore, to obtain \(E_{I}+E_{D}\lesssim\varepsilon\), it suffices to require \(T=\Theta\big{(}\frac{\mathrm{m}_{2}^{2}}{\varepsilon}\big{)}\) and the iteration complexity

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}+d}{\varepsilon}\ln(\frac{\mathrm{m}_{ 2}^{2}}{\delta\varepsilon})^{2}\big{)}\]

Once we let \(\delta=\bar{\sigma}_{\min}^{2}\), \(T=\bar{\sigma}_{\max}^{2}=\Theta\big{(}\frac{\mathrm{m}_{2}^{2}}{\varepsilon} \big{)}\) and \(a=\rho\), the iteration complexity is

\[N=\Omega\big{(}\frac{\mathrm{m}_{2}^{2}\lor d}{d}\ln\big{(}\frac{\bar{\sigma} _{\max}}{\bar{\sigma}_{\min}}\big{)}^{2}\bar{\sigma}_{\max}^{2}\big{)}.\]

Compared to exponential time schedule with the EDM choice of variance schedule, this iteration complexity has the same dependence on dimension parameters \(\mathrm{m}_{2},d\) and the minimal/maximal variance \(\bar{\sigma}_{\min},\bar{\sigma}_{\max}\).

**Optimality of Exponential time schedule.** For simplicity, we assume \(\mathrm{m}_{2}^{2}=\mathcal{O}(d)\). Then under both schedules in [30] and [46], \(E_{I}\)s only dependent on \(T\), and are independent of the time schedule. Both \(E_{D}\)s satisfy

\[E_{D}\lesssim d\sum_{j=0}^{N-1}\frac{\gamma_{j}^{2}}{\big{(}T-t_{j}^{-}\big{)} ^{2}}\lesssim\varepsilon\]

Let \(\tau_{j}=\ln\big{(}\frac{T-t_{j}^{-}}{T-t_{j+1}^{-}}\big{)}\in(0,\infty)\). Then \(\frac{\gamma_{j}}{T-t_{j}^{-}}=1-e^{-\tau_{j}}\) and \(\sum_{\delta<T-t_{j}^{-}<T}\tau_{j}=\ln(T/\delta)\) is fixed. Since \(x\mapsto(1-e^{-x})^{2}\) is convex on the domain \(x\in(0,\infty)\), according the Jensen's inequality, \(\sum_{\delta<T-t_{j}^{-}<T}\frac{\gamma_{j}^{2}}{(T-t_{j}^{-})^{2}}\) reaches its minimum when \(\tau_{j}\) are constant-valued for all \(j\), which implies the exponential schedule is optimal to minimize \(E_{D}\), hence optimal to minimize \(E_{D}+E_{I}\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We state it in Section 1 and Appendix A. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 1 and Appendix A.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer:[Yes] Justification: See Appendix and Theorems in the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer:[NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]. Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: It has been confirmed by Ethics reviewers. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In Section 1. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We show new methods and theorems. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification:Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.