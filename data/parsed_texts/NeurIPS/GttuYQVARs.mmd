# Differential Privacy of Cross-Attention with Provable Guarantee

 Yingyu Liang

The University of Hong Kong

University of Wisconsin-Madison

yingyul@hku.hk, yliang@cs.wisc.edu

&Zhenmei Shi

University of Wisconsin-Madison

zhmeishi@cs.wisc.edu

&Zhao Song

The Simons Institute for the Theory of Computing

at the University of California, Berkeley

magic.linuxkde@gmail.com &Yufa Zhou

University of Pennsylvania

yufazhou@seas.upenn.edu

###### Abstract

Cross-attention has become a fundamental module nowadays in many important artificial intelligence applications, e.g., retrieval-augmented generation (RAG), system prompt, guided stable diffusion, and many more. Ensuring cross-attention privacy is crucial and urgently needed because its key and value matrices may contain sensitive information about model providers and their users. In this work, we design a novel differential privacy (DP) data structure to address the privacy security of cross-attention with a theoretical guarantee. In detail, let \(n\) be the input token length of system prompt/RAG data, \(d\) be the feature dimension, \(0<\alpha\leq 1\) be the relative error parameter, \(R\) be the maximum value of the query and key matrices, \(R_{w}\) be the maximum value of the value matrix, and \(r,s,\epsilon_{s}\) be parameters of polynomial kernel methods. Then, our data structure requires \(\widetilde{O}(ndr^{2})\) memory consumption with \(\widetilde{O}(nr^{2})\) initialization time complexity and \(\widetilde{O}(\alpha^{-1}r^{2})\) query time complexity for a single token query. In addition, our data structure can guarantee that the process of answering user query satisfies \((\epsilon,\delta)\)-DP with \(\widetilde{O}(n^{-1}\epsilon^{-1}\alpha^{-1/2}R^{2s}R_{w}r^{2})\) additive error and \(n^{-1}(\alpha+\epsilon_{s})\) relative error between our output and the true answer. Furthermore, our result is robust to adaptive queries in which users can intentionally attack the cross-attention system. To our knowledge, this is the first work to provide DP for cross-attention and is promising to inspire more privacy algorithm design in large generative models (LGMs).

## 1 Introduction

The development of Artificial Intelligence (AI) has four stages: (1) prediction AI, e.g., ResNet [14] in image classification; (2) generation AI, e.g., ChatGPT [1] in language generation; (3) autonomous agent AI, Voyager [26] autonomously plays Minecraft game [17]; (4) Artificial Generalization Intelligence (AGI). Humans have made rapid progress in generative AI, and we are excitingly heading to the third stage, the era of AI agent [15]. One prevalent application of AI agents is customized large generative models (LGMs) agents [3], e.g., AgentGPT [16], SuperAGI [15], MetaGPT [19, 2], GPT Researcher [14] and many so on. In particular, recently, Apple Inc. introduced Apple Intelligence [1], signaling the integration of LGMs into physical devices. This innovation allows devices to use personal information for real-life assistance, such as entering passport numbers when booking flights or informing users of their latestmeetings. With increased AI capabilities, privacy concerns become significant, as the more personal information devices handle, the greater the potential privacy risks.

One fundamental technique used in LGMs is cross-attention (Vaswani et al., 2017), which is an essential module in retrieval-augmented generation (RAG) (Lewis et al., 2020), system prompt, guided stable diffusion, and many so on. In RAG, to be more professional, the LGMs answer user input queries by using a domain-specific database under cross-attention, which may contain specific privacy data and knowledge so that the LGMs gain additional power. For system prompts, based on cross-attention, some customized long prompts, e.g., user information or concrete rules, are concatenated before user input to follow human instructions better, which are commonly used in ChatGPT (GitHub, 2024b), Claude3 (Anthropic, 2024) and other commercial LGMs.

Consequently, protecting the privacy of domain-specific data in RAG or system prompts is crucial as they contain sensitive information about users and companies. These data and prompts are the core assets of many start-ups. However, these data and prompts can be easily recovered (Li et al., 2023b), jailbroken (Jin et al., 2024), and released (Li et al., 2023a) by user adversarial attack (Yu et al., 2024), e.g., there are 1700 tokens in ChatGPT system prompts (Patel, 2024). These findings highlight the critical importance of robust privacy protections in LGMs, making privacy not just essential but an urgent issue that demands immediate attention.

To fundamentally preserve cross-attention privacy, we borrow the powerful tools from differential privacy (DP) (Dwork et al., 2006), which provides measurable privacy and combines with statistical machine learning seamlessly (Ponomareva et al., 2023). Thus, in this work, we would like to ask and answer the following question,

_How can we use differential privacy to protect the security of cross-attention in LGMs?_

Our work demonstrates that the Softmax cross-attention computation is equivalent to computing the weighted distance problem.

**Definition 1.1** (Softmax cross-attention).: _Let \(n\) and \(m\) be the token length of the data and input query, respectively. Let \(d\) be the feature dimension. Given fixed key matrix \(K\in[0,R]^{n\times d}\) and fixed value matrix \(V\in[-R_{w},R_{w}]^{n\times d}\), for any input query matrix \(Q\in[0,R]^{m\times d}\), the goal of the Softmax Cross-Attention Computation is to get the matrix \(\mathrm{Attn}(Q,K,V)\in\mathbb{R}^{m\times d}\), which is_

\[\mathrm{Attn}(Q,K,V):=D^{-1}AV,\]

_where \(A\in\mathbb{R}^{m\times n}\) satisfies \(A_{i,j}:=\exp(\langle Q_{i},K_{j}\rangle/d)\) for any \(i\in[m],j\in[n]\) (\(Q_{i}\) and \(K_{j}\) denote the \(i\)-th and \(j\)-th rows of \(Q\) and \(K\), respectively) and \(D:=\mathrm{diag}(A\mathbf{1}_{n})\in\mathbb{R}^{m\times m}\) is a diagonal matrix._

Note that \(\mathsf{Softmax}(QK^{\top})=D^{-1}A\in\mathbb{R}^{m\times n}\) in Definition 1.1, which is the standard function used in transformers, and usually, we call it as attention matrix. Our main theorem, presented below, provides a robust solution of cross-attention, ensuring privacy and accuracy guarantees.

**Theorem 1.2** (Main result; Informal version of Theorem 3.1).: _Let \(Q,K,V,\mathrm{Attn}\) be defined in Definition 1.1. Let \(\alpha\in(0,1)\) be the relative error parameter and \(p_{f}\) be the probability of failure parameter. Let \(r,s,\epsilon_{s}\) be the parameters of the polynomial kernel methods (Lemma D.7). Let \(l=\widetilde{O}(r)\). Then, our Algorithm 1 requires \(O(lndr)\) memory with \(O(lnr)\) initialization time and \(\widetilde{O}(\alpha^{-1}lr)\) query time, such that with probability \(1-p_{f}\), the output process of cross-attention satisfies \((\epsilon,\delta)\)-DP and is robust to adaptive query with relative error \(n^{-1}(\alpha+\epsilon_{s})\) and additive error \(\widetilde{O}(n^{-1}\epsilon^{-1}\alpha^{-1/2}lR^{2s}R_{w}r)\)._

Our main technique in Theorem 1.2 ensures that cross-attention is differentially private by using the polynomial kernel approximation method and transforming it into a weighted distance problem. We then solve the problem by summing over weighted distances (depending on the value embedding) between the query embedding and the key embedding. We build a data structure for weighted Softmax queries in Section 4.3, and we extend this data structure to handle adaptive queries using the \(\epsilon_{0}\)-net/metric entropy argument in Section 4.4. Furthermore, our error decreases as the input token length grows, diminishing both the relative and additive errors to zero.

Our contributions are as follows:

* We demonstrate that cross-attention computations are equivalent to the weighted distance problem (Section 3).

* We design a novel algorithm (Algorithm 2) that privately answers weighted Softmax queries with high probability and a concrete accuracy bound.
* Our algorithm (Algorithm 1) handles multiple cross-attention queries and is robust against adaptive query attacks (Theorem 3.1), meaning that potential attackers cannot intentionally extract information of system prompts/RAG data.

To our knowledge, this is the first work to utilize DP to protect prompts in LGMs with theoretically provable guarantees. While some have explored protecting user/system prompts with DP (Edemacu and Wu, 2024; Mai et al., 2023), they are primarily empirical and lack theoretical guarantees. Additionally, many others are working on protecting private datasets by applying DP to the fine-tuning stage of LGMs (Behnia et al., 2022; Singh et al., 2024; Liu et al., 2024; Yu et al., 2021; Li et al., 2021; Shi et al., 2022), which diverges from our work. The strength of DP lies in its strong, unambiguous, and concrete definition of privacy, enabling algorithm designs with provable privacy and accuracy analysis. Therefore, we believe that the theoretical aspects of DP applications in LGMs remain a highly impactful direction, and we aim to pave the way for further exploration in this area.

### Related Work

**Differential Privacy in Data Structure and Attention.** Differential privacy (DP) is a flourishing and powerful technique that has enormous applications in the topic of private machine learning. In the era of Large Generative Models (LGMs), there are three primary approaches to ensuring privacy: (1) during the pre-training stage: to protect training data (Abadi et al., 2016; Ponomareva et al., 2023), (2) during the adaptation stage: to protect target data (Behnia et al., 2022; Singh et al., 2024; Liu et al., 2024; Yu et al., 2021; Li et al., 2021; Shi et al., 2022), (3) during the inference stage: to protect user/system prompts (Edemacu and Wu, 2024) and RAG data (Lewis et al., 2020). To protect training data, DP-SGD (Abadi et al., 2016) uses DP optimizer to ensure data privacy, severing as the traditional baseline method. Recently, numerous works have aimed to improve this method by integrating DP in both the pre-training and fine-tuning stages of LGMs (Yu et al., 2021; Li et al., 2021; Golatkar et al., 2022; Behnia et al., 2022; Shi et al., 2022; Mattern et al., 2022; Singh et al., 2024; Zheng et al., 2024; Liu et al., 2024). To protect user/system prompts, Edemacu and Wu (2024) provides a survey on both DP and non-DP methods. In the use of LGMs, prompting methods almost become a standard way for inference (Schulhoff et al., 2024). Given the billions of prompt interactions daily, ensuring privacy is essential (Mai et al., 2023). We refer readers to Appendix B for more related works.

**Roadmap.** In Section 2, we present the preliminary of differential privacy (DP) and cross-attention. In Section 3, we present the main result of our cross-attention theorem (Theorem 3.1). In Section 4, we outline the main results of our algorithms. In Section 5, we conclude our paper.

## 2 Preliminary

In this section, we give the preliminary of differential privacy (DP) and cross-attention. In Section 2.1, we describe the notations. In Section 2.2, we give definitions related to DP.

### Notations

We use \(\Pr[]\) to denote the probability. We use \(\mathbb{E}[]\) to denote the expectation. We use \(\Var[]\) to denote the variance. For two vectors \(x\in\mathbb{R}^{d}\) and \(y\in\mathbb{R}^{d}\), we use \(\langle x,y\rangle\) to denote the inner product between \(x,y\), i.e., \(\langle x,y\rangle=\sum_{i=1}^{d}x_{i}y_{i}\). We use \(X\subset\mathbb{R}^{d}\) and \(|X|=n\) to mean the same thing as \(X\in\mathbb{R}^{n\times d}\). Also, we denote \(x_{i}^{\top}\) as the \(i\)-th row of \(X\). We use \(x_{i,j}\) to denote the \(j\)-th coordinate of \(x_{i}\in\mathbb{R}^{n}\). We use \(\mathbf{1}_{n}\) to denote a length-\(n\) vector where all the entries are ones. We use \(\|x\|_{p}\) to denote the \(\ell_{p}\) norm of a vector \(x\in\mathbb{R}^{n}\), i.e., \(\|x\|_{1}:=\sum_{i=1}^{n}|x_{i}|\), \(\|x\|_{2}:=(\sum_{i=1}^{n}x_{i}^{2})^{1/2}\), and \(\|x\|_{\infty}:=\max_{i\in[n]}|x_{i}|\).

### Differential Privacy Definitions

In this section, we give several definitions related to differential privacy (DP). We refer the reader to Dwork and Roth (2014) for more background and details on DP.

**Definition 2.1** (Neighboring dataset).: _We define the two neighboring datasets as \(X,X^{\prime}\in\mathbb{R}^{n}\) such that \(\|X-X^{\prime}\|_{1}\leq 1\), i.e., they differ on a single data point._

**Definition 2.2** (Sensitivity).: _The sensitivity of a function \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\) is defined by: \(\Delta:=\max_{X,X^{\prime}\in\mathbb{R}^{n},\|X-X^{\prime}\|_{1}=1}\|f(X)-f(X^ {\prime})\|_{1}\)._

**Definition 2.3** (\((\epsilon,\delta)\)-Dp).: _For \(\epsilon>0,\delta\geq 0\), a randomized algorithm \(\mathcal{A}\) is \((\epsilon,\delta)\)-DP, if for all \(\mathcal{S}\subseteq\mathrm{Range}(\mathcal{A})\) and for all \(X,X^{\prime}\) such that \(\|X-X^{\prime}\|_{1}\leq 1\): \(\Pr[\mathcal{A}(X)\in\mathcal{S}]\leq\exp(\epsilon)\Pr[\mathcal{A}(X^{\prime} )\in\mathcal{S}]+\delta\). When \(\delta=0\), the algorithm is said to have pure differential privacy._

We mainly use the truncated Laplace mechanism, which has the following definitions.

**Definition 2.4** (Truncated Laplace distribution).: _We use \(\mathrm{Tlap}(\Delta,\epsilon,\delta)\) to denote the Truncated Laplace distribution with pdf proportional to \(\exp(-\epsilon|z|/\Delta)\) on the region \([-B,B]\), where \(B=\frac{\Delta}{\epsilon}\cdot\log(1+\frac{\exp(\epsilon)-1}{2\delta})\)._

**Fact 2.5** (Theorem 3 in Geng et al. (2020)).: _Let \(z\) denote a \(\mathrm{Tlap}(\Delta,\epsilon,\delta)\) random variable. Then we have \(\mathbb{E}[z]=0\), and \(\mathrm{Var}[z]=\frac{2\Delta^{2}}{\epsilon^{2}}(1-\delta\cdot\frac{\log^{2}( 1+\frac{\epsilon^{2}-1}{2\delta})+2\log(1+\frac{\epsilon^{2}-1}{2\delta})}{ \epsilon^{2}-1})\). Furthermore, if \(\delta=0\), we have \(\mathrm{Var}[z]=2\Delta^{2}/\epsilon^{2}\), meaning truncated Laplacian mechanism will be reduced to the standard Laplacian mechanism._

**Lemma 2.6** (Laplace mechanism, (Dwork and Roth, 2014; Geng et al., 2020), see Lemma 2.2 in Andoni et al. (2023)).: _Given a numeric function \(f\) that takes a dataset \(X\) as the input, and has sensitivity \(\Delta\), the mechanism that outputs \(f(X)+z\) where \(z\sim\mathrm{Lap}(\Delta/\epsilon)\) is \((\epsilon,0)\)-DP. In addition, if \(\epsilon,\delta\in(0,0.5)\), \(f(X)+z\), where \(z\sim\mathrm{Tlap}(\Delta,\epsilon,\delta)\) is \((\epsilon,\delta)\)-DP. Moreover, the truncated Laplace mechanism is always accuracy up to error \(B\)._

```
1:datastructureDPTreeSoftmaxAdaptive\(\triangleright\) Theorem 4.4
2:members
3:\(\mathcal{D}_{1},\ldots,\mathcal{D}_{O(r\log(dR/(\epsilon_{s}p_{f})))}:\)DPTreeSoftmax\(\triangleright\) Algorithm 2
4:endmembers
5:procedureInit(\(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n},\epsilon\in(0,1), \delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1)),\epsilon_{s}\in(0,0.1),p_ {f}\in(0,0.01)\))
6:\(l\leftarrow O(r\log(dR/(\epsilon_{s}p_{f})))\)
7:for\(i=1\to l\)do
8:\(\mathcal{D}_{i}\).Init(\(X,n,w,\epsilon/l,\delta/l,\delta^{\prime}/l,c,\epsilon_{s}\))
9:endfor
10:endprocedure
11:procedureDistanceQuery(\(y\in[0,R]^{d},\alpha\in(0,1)\))
12:\(l\gets O(r\log(dR/(\epsilon_{s}p_{f})))\)
13:\(r\gets 0^{l}\)
14:for\(i=1\to l\)do
15:\(r_{i}\leftarrow\mathcal{D}_{i}\).DistanceQuery(\(y,\alpha\))
16:endfor
17:return Median of \(r\)
18:endprocedure
19:end datastructure ```

**Algorithm 1** Adaptive query data structure

## 3 Main Results: Cross-Attention

In this section, we show our main result for cross-attention. Theorem 3.1 states that we can ensure the entire cross-attention module satisfies DP and is robust to adaptive queries. Our high-level idea is based on the similarity between weighted distance problem and cross-attention. For a typical weighted distance problem, we define the following: Let \(w\in\mathbb{R}^{n}\) be the weights, \(X\in\mathbb{R}^{n\times d}\) be the data matrix, where \(x_{i}^{\top}\) is the \(i\)-th row of \(X\) for \(i\in[n]\), and let \(y\in\mathbb{R}^{d}\) be the query. Suppose we need to answer \(\ell_{1}\)-distance query. We have

\[\sum_{i\in[n]}\underbrace{w_{i}}_{\text{weight}}\parallel\underbrace{y}_{ \text{query}}-\underbrace{x_{i}}_{\text{data}}\parallel_{1}.\]Now we introduce cross-attention. Let \(Q,K,V,\mathrm{Attn}\) be defined in Definition 1.1. In a standard cross-attention process, we have access to \(K\) and \(V\) before inference, but not to the user input \(Q\). For the cross-attention mechanism \(\mathrm{Attn}\) (Definition 1.1), we aim to ensure that the matrix \(AV\) satisfies DP guarantee. Let \(A_{i,j}=\exp((Q_{i},K_{j})/d)\) for \(i\in[m],j\in[n]\). Let \(V_{j,k}\in\mathbb{R}\) be the \((j,k)\)-th entry of \(V\), for \(j\in[n],k\in[d]\). By post-processing property (Fact C.5), to ensure that the forward output \(\mathrm{Attn}(Q,K,V)=D^{-1}AV\) (Definition 1.1) satisfies DP, we only need to ensure the DP of its component \(AV\)1. The \((i,k)\)-th entry of \(AV\) for each \(i\in[m],k\in[d]\) is computed by

Footnote 1: \(D\) is only a normalization factor and does not have sensitive information.

\[(AV)_{i,k}=\sum_{j=1}^{n}\underbrace{V_{j,k}}_{\mathrm{weight}}\exp(\langle \underbrace{Q_{i}}_{\mathrm{query}},\underbrace{K_{j}}_{\mathrm{data}}\rangle/d),\] (1)

which can be viewed as a weighted Softmax problem, where \(V\) provides the weights, \(Q\) is the query, and \(K\) is the dataset. Thus, we choose to add noise to \(K\) and \(V\) based on the similarity between the weighted distance problem and cross-attention. Furthermore, we find that we can only handle one column of \(V\), i.e., \(V_{*,k}\in\mathbb{R}^{n}\), in a single data structure. Therefore, we need to initialize a total of \(d\) different data structures, each with weights \(V_{*,k}\) for \(k\in[d]\).

Here, we present our main result below.

**Theorem 3.1** (Softmax cross-attention, informal version of Theorem J.12).: _Let \(Q,K,V,\mathrm{Attn}\) be defined in Definition 1.1. Let \(\alpha\in(0,1)\) be the relative error parameter and \(p_{f}\) be the probability of failure parameter. Let \(r,s,\epsilon_{s}\) be the parameters of the polynomial kernel methods (Lemma D.7). Let \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j!}}\) (Definition I.3). Let \(l=O(r\log(dR/(\epsilon_{s}p_{f})))\). There is a data structure (Algorithm 1) that uses \(O(lnrd)\) spaces to ensure cross-attention satisfies DP and supports the following operations:_

* _We initialize_ \(d\) _data structures using_ \(\textsc{Init}(K,n,V_{*,k},\epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in( 0,1),c\in(0,0.1),\epsilon_{s}\in(0,0.1),p_{f}\in(0,0.01))\) _(Algorithm_ 1_), for_ \(k\in[d]\)_. It takes_ \(O(lnr)\) _time to initialize one data structure._
* _At query time, for user input_ \(Q\)_, we process one token at a time by passing the_ \(i\)_-th row of_ \(Q\)_, denoted_ \(Q_{i}\in\mathbb{R}^{d}\)_, to_ DistanceQuery_\((Q_{i},\alpha\in(0,1))\) _(Algorithm_ 1_) for each_ \(i\in[m]\)_. It takes_ \(O(\alpha^{-1}lr\log^{2}n)\) _time to output an entry_ \(z\) _in_ \(\mathrm{Attn}(Q,K,V)\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta+\delta^{\prime})\)_-DP,_
* _the process of output_ \(z\) _has relative error_ \(n^{-1}(\alpha+\epsilon_{s})\)_,_
* _the process of output_ \(z\) _has additive error_ \(O(n^{-1}\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^ {\prime})}\cdot\log^{3/2}n)\)_,_
* _it holds with probability_ \(1-p_{f}\) _(where_ \(p_{f}\) _is used in_ \(l\)_),_
* _it is robust to adaptive query._

In Theorem 3.1, we use our DPTreeSoftmaxAdaptive (Algorithm 1) and guarantee that, for each query token of cross-attention, the output process satisfies \((\epsilon,\delta)\)-DP with \(n^{-1}(\alpha+\epsilon_{s})\) relative error and \(O(n^{-1}\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^ {\prime})}\cdot\log^{3/2}n)\) additive error, and \(O(\alpha^{-1}lr\log^{2}n)\) running time under adaptive query. More specifically, the algorithm creates \(d\)DPTreeSoftmaxAdaptive data structures, each requiring \(O(lnr)\) memory consumption and \(O(lnr)\) initialization time. Notably, our error is inversely proportional to \(n\), meaning that as the input token length increases, both the relative and approximate errors approach zero. This is achieved by the normalizing matrix \(D\) (Definition 1.1). We refer the reader to Section J for proof details.

Thus, our algorithm theoretically protects system prompts/RAG data in cross-attention as discussed in Section 1. In Section 4, we provide a detailed technical overview, and in Section A, we will present self-attention and DP-related discussion.

## 4 Key Data Structure: DPTree

This section provides our key data structures: DPTree (Algorithm 3), DPTreeDistance (Algorithm 5 and 6), DPTreeHighDim (Algorithm 7), DPTreeSoftmax (Algorithm 2), and DPreeSoftmaxAdaptive (Algorithm 1).

In Section 4.1, we provide our high-level proof insights. In Section 4.2, we give our basic building block algorithms DPTree, DPTreeDistance and DPTreeHighDim. In Section 4.3, we present our DPTreeSoftmax algorithm that solves the weighted Softmax problem. In Section 4.4, we present our DPTreeSoftmaxAdaptive algorithm that enables DPTreeSoftmax to handle adaptive query problem.

### Technique Overview

Notice that Eq. (1) is not a typical distance measure like \(\ell_{1}\) or \(\ell_{2}\), but by using polynomial kernel method techniques, we transform it into a distance measure. Alman and Song (2023) states that the exponential inner product can be approximated by polynomial kernel function \(P(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{r}\), i.e., \(P(x)^{\top}P(y)\approx\exp(x^{\top}y/d)\) for two vector \(x,y\in\mathbb{R}^{d}\), with a relative error. Then, by the Law of Cosines, we transform the inner product of polynomial kernel functions into a distance measure, i.e.,

\[2P(x)^{\top}P(y)=\,-\,\|P(x)-P(y)\|_{2}^{2}+\|P(x)\|_{2}^{2}+\|P(y)\|_{2}^{2}.\] (2)

After transforming Eq. (1) into a distance measure, we design the DPTree series data structures to provide cross-attention DP guarantee.

In summary, we first design the data structure DPTree (Algorithm 3) that builds a binary segment tree with truncated Laplace noise added in the leaf nodes to ensure DP guarantee. Then, based on this data structure, we design DPTreeDistance (Algorithm 5 and 6) to answer one dimensional weighted distance queries \(\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|\), which utilizes DPTree to store and return noised weights \(w_{i}\) multiplied with the approximated distances between the query \(y\) and data \(x_{i}\). We further decompose high dimensional \(\ell_{p}^{p}\)-distance problem into one dimensional \(\ell_{1}\)-distance problems using

\[\sum_{i=1}^{n}w_{i}\cdot\|y-x_{i}\|_{p}^{p}=\,\sum_{k=1}^{d}\sum_{i=1}^{n}w_{i }\cdot|y_{k}-x_{i,k}|^{p}.\] (3)

Based on this decomposition, we design DPTreeHighDim (Algorithm 7) which is capable of answering high dimension queries. Then, using Eq. (2) and DPTreeHighDim, we design DPTreeSoftmax (Algorithm 2) to answer Softmax queries. By building multiple copies of this data structure, we boost the success probability such that it can answer any query (including adaptive query) with an additive error, establishing the final data structure DPTreeSoftmaxAdaptive (Algorithm 1). See Section D for a more detailed outline of algorithms and proof techniques.

### DPTree, DPTreeDistance, and DPTreeHighDim

We design a basic data structure DPTree (Algorithm 3) that answers summation queries by a summation segment tree with truncated Laplace noise (Definition 2.4). The algorithm first builds a binary summation tree in an array and then adds truncated Laplace noises to each node. In query time, we first trace from bottom nodes to find their lowest common ancestor, then report the summation by using at most \(2\log n\) nodes on the path (Algorithm 3). Based on the parallel composition rule of DP (Fact C.7), we find that if we have multiple disjoint interval queries, the error of the weighted sum of the intervals can be bounded independently of the number of queries (Lemma E.8). See more details in Section E.

We then design DPTreeDistance, a one-dimensional weighted \(\ell_{1}\) distance data structure detailed in Algorithm 5 and 6. Initialization involves rounding each data point to the nearest multiple of a small interval and aggregating their weights into an array (illustrated in Figure 1), which is then input into our DPTree. At query time, we retrieve aggregated weights within small intervals and multiply them by their distances to the query point. We introduce a relative error parameter \(\alpha\) to reduce the number of iterations to \(O(\log(n)/\alpha)\), improving efficiency. Guided by Eq.(3), we design DPTreeHighDim (Algorithm 7), which extends DPTreeDistance to higher dimension by constructing independent data structures for each coordinate. See details in Section G and H.

### Softmax Activation

In this section, we present DPTreeSoftmax (Algorithm 2) that answers the weighted Softmax query (Definition 4.1) and is further used to design DP cross-attention. First, we introduce the definition of weighted Softmax query, an abstraction for the problem described in Eq. (1).

**Definition 4.1** (Weighted Softmax query (without normalization)).: _For the dataset \(X\in[0,R]^{n\times d}\) where \(x_{i}^{\top}\) is the \(i\)-th row of \(X\) and query \(y\in[0,R]^{d}\), we define the weighted exponential inner product/Softmax query to be:_

\[\sum_{i\in[n]}w_{i}\exp(\langle x_{i},y\rangle/d)=w^{\top}\exp(Xy/d).\]

Building on Definition 4.1, we develop a novel algorithm to answer differentially private weighted Softmax queries using the polynomial kernel method from Alman and Song (2023). Specifically, in Eq.(2), there is a term that computes the weighted \(\ell_{2}^{2}\) distance, which we calculate using DPTreeHighDim. We then compute the exact term for the weighted \(\ell_{2}^{2}\) norms of the approximation kernel. By summing these terms with a controlled error, we extend DPTreeHighDim to answer the Softmax query efficiently. More details can be found in Section J.

**Theorem 4.2** (Softmax query, informal version of Theorem J.8).: _Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j}}\) (Definition J.3). Let the accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). Our data structure DPTreeSoftmax (Algorithm 2) uses \(O(nr)\) spaces to solve Softmax query problem for dataset \(X\subset[0,R]^{d}\) and support following operations:_

* \(\textsc{Init}(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1),\epsilon _{s}\in(0,0.1))\)_. (Algorithm 2) It takes_ \(O(nr)\) _time to initialize the data structure._
* \(\textsc{DistanceQuery}(y\in[0,R]^{d},\alpha\in(0,1))\)_. (Algorithm 2) It takes_ \(O(\alpha^{-1}r\log^{2}n)\) _time to output a number_ \(z\) _such that__;_ * _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta+\delta^{\prime})\)_-DP private, which computes_ \(w^{\top}\exp(Xy/d)\)_,_ * _the error bound satisfies_ \(|z-w^{\top}\exp(Xy/d)|\leq(\alpha+\epsilon_{s})\cdot w^{\top}\exp(Xy/d)\)__ \(+\ O(\epsilon^{-1}\alpha^{-1/2}\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(1/\delta^{ \prime})}\cdot\log^{3/2}n)\)_,_ * _it holds with probability at least_ \(0.99\)_._

**Remark 4.3**.: _In Theorem 4.2, the parameter \(\epsilon_{s}\) is the accuracy parameter for polynomial kernel approximation described in Section D.5. Besides, note that the error bound in Theorem 4.2 does not depend on \(\delta\) but depends on \(\delta^{\prime}\). The role of \(\delta\) is to control a hidden constant term in the big \(O\) notation, i.e., increasing \(\delta\) reduces the error by a small constant (Fact 2.5). In practice, we set \(\delta\) as a small positive constant close to \(0\). Please refer to the Lemma E.6 for more details._

### Adaptive Query Data Structure

We adapt our DPTreeSoftmax to DPTreeSoftmaxAdaptive (Algorithm 1) to solve the adaptive query problem. By proving it can handle any query within the query space with a certain error, we ensure it effectively processes adaptive queries. We first boost the constant probability to high probability using the Chernoff bound (Lemma C.2). Employing an \(\epsilon_{0}\)-net argument and the union bound, we bound all query points within the net. Finally, we use the Lipschitz property of the weighted Softmax distance function with an additive error to bound all points in the query space. The corresponding proofs can be found in Section I and Section J.

**Theorem 4.4** (Adaptive query Softmax data structure, informal version of Theorem J.11).: _Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j!}}\) (Definition J.3). Let the accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). Let \(X\in[0,R]^{n\times d}\) be the dataset, \(w\in[-R_{w},R_{w}]^{n}\) be weights, \(y\in[0,R]^{d}\) be the query, \(\alpha\in(0,1)\) be the relative error parameter and \(p_{f}\) be the failure probability parameter. Let \(l=O(r\log(dR/(\epsilon_{s}p_{f})))\). There is a data structure DPTreeSoftmaxAdaptive (Algorithm 1) that uses \(O(lnr)\) spaces to solve the weighted Softmax query problem for the dataset \(X\subset[0,R]^{d}\) and supports the following operations:

* \(\textsc{Init}(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1),\epsilon_ {s}\in(0,0.1),p_{f}\in(0,0.01))\). It takes \(O(lnr)\) time to initialize the data structure.
* DistanceQuery\((y\in[0,R]^{d},\alpha\in(0,1))\). It takes \(O(\alpha^{-1}lr\log^{2}n)\) time to output a number \(z\) such that
* the process of output \(z\) satisfies \((\epsilon,\delta+\delta^{\prime})\)-DP private, which computes \(w^{\top}\exp(Xy/d)\),
* the error bound satisfies \(|z-w^{\top}\exp(Xy/d)|\leq(\alpha+\epsilon_{s})\cdot w^{\top}\exp(Xy/d)\) \(+\ O(\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^{ \prime})}\cdot\log^{3/2}n)\),
* it holds with probability at least \(1-p_{f}\) (where \(p_{f}\) is used in \(l\)),
* it is robust to adaptive query._

**Remark 4.5**.: _We describe the parallelization of our algorithms. In the second for loop of Init and the for loop of DistanceQuery in Algorithm 2, the \(r\) DPTreeDistance data structures instantiated for each coordinate are independent of each other. In addition, the for loops in Algorithm 1 are also parallelizable since the \(l=O(r\log(dR/(\epsilon_{s}p_{f})))\) copies are independent. After parallelization, we have the final time complexity of Init to be \(O(nr)\) and DistanceQuery to be \(O(\alpha^{-1}\log^{2}n)\) in Algorithm 1 with \(O(lr)\) GPU process._

## 5 Conclusion and Discussion

To our knowledge, we are the first work to provide differential privacy for cross-attention. This paper presents the DPTree data structures, which provide a differential privacy guarantee for the cross-attention module in large generative models. This is achieved by transforming the cross-attention mechanism into a weighted distance problem. Furthermore, our algorithm is robust to adaptive queries, allowing users to interact with the model arbitrarily without extracting sensitive information from the system prompts or RAG data. Our results may inspire more privacy algorithm design in large generative models. Further discussion, including extension to self-attention and DP related design issue, is deferred to Section A.

## References

* Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Alman and Song (2023) Josh Alman and Zhao Song. Fast attention requires bounded entries. _Advances in Neural Information Processing Systems_, 36, 2023.
* Andoni et al. (2023) Alexandr Andoni, Piotr Indyk, Sepideh Mahabadi, and Shyam Narayanan. Differentially private approximate near neighbor counting in high dimensions. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 43544-43562, 2023.
* Anthropic (2024) Anthropic. System prompts, 2024. https://docs.anthropic.com/en/docs/system-prompts.
* Apple intelligence (2024) Apple. Apple intelligence, 2024. https://www.apple.com/apple-intelligence/.
* Backurs et al. (2024) Arturs Backurs, Zinan Lin, Sepideh Mahabadi, Sandeep Silwal, and Jakub Tarnawski. Efficiently computing similarities to private datasets. _arXiv preprint arXiv:2403.08917_, 2024.
* Behnia et al. (2022) Rouzbeh Behnia, Mohammadreza Reza Ebrahimi, Jason Pacheco, and Balaji Padmanabhan. Ewtune: A framework for privately fine-tuning large language models with differential privacy. In _2022 IEEE International Conference on Data Mining Workshops (ICDMW)_, pages 560-566. IEEE, 2022.
* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In _International conference on machine learning_, pages 2206-2240. PMLR, 2022.
* Chen et al. (2021) Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 357-366, 2021.
* Chen et al. (2022) Justin Y Chen, Shyam Narayanan, and Yinzhan Xu. All-pairs shortest path distances with differential privacy: Improved algorithms for bounded and unbounded weights. _arXiv preprint arXiv:2204.02335_, 2022.
* Cherapanamjeri et al. (2023) Yeshwanth Cherapanamjeri, Sandeep Silwal, David P Woodruff, Fred Zhang, Qiuyi Zhang, and Samson Zhou. Robust algorithms on adaptive inputs from bounded adversaries. _arXiv preprint arXiv:2304.07413_, 2023.
* Chernoff (1952) Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. _The Annals of Mathematical Statistics_, pages 493-507, 1952.
* Cohen-Addad et al. (2022a) Vincent Cohen-Addad, Alessandro Epasto, Vahab Mirrokni, Shyam Narayanan, and Peilin Zhong. Near-optimal private and scalable \(k\)-clustering. _Advances in Neural Information Processing Systems_, 35:10462-10475, 2022a.
* Cohen-Addad et al. (2022b) Vincent Cohen-Addad, Chenglin Fan, Silvio Lattanzi, Slobodan Mirrovic, Ashkan Norouzi-Fard, Nikos Parotsidis, and Jakub M Tarnawski. Near-optimal correlation clustering with privacy. _Advances in Neural Information Processing Systems_, 35:33702-33715, 2022b.
* Dinur et al. (2023) Itai Dinur, Uri Stemmer, David P Woodruff, and Samson Zhou. On differential privacy and adaptive data analysis with bounded space. In _Annual International Conference on the Theory and Applications of Cryptographic Techniques_, pages 35-65. Springer, 2023.
* Dong et al. (2024) Wei Dong, Zijun Chen, Qiyao Luo, Elaine Shi, and Ke Yi. Continual observation of joins under differential privacy. _Proceedings of the ACM on Management of Data_, 2(3):1-27, 2024.
* Dong et al. (2020)Cynthia Dwork. Differential privacy: A survey of results. In _International conference on theory and applications of models of computation_, pages 1-19. Springer, 2008.
* Dwork and Roth (2014) Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* Edemacu and Wu (2024) Kennedy Edemacu and Xintao Wu. Privacy preserving prompt engineering: A survey. _arXiv preprint arXiv:2404.06001_, 2024.
* Elias et al. (2020) Marek Elias, Michael Kapralov, Janardhan Kulkarni, and Yin Tat Lee. Differentially private release of synthetic graphs. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 560-578. SIAM, 2020.
* Epasto et al. (2024) Alessandro Epasto, Vahab Mirrokni, Shyam Narayanan, and Peilin Zhong. \(k\)-means clustering with distance-based privacy. _Advances in Neural Information Processing Systems_, 36, 2024.
* Esfandiari et al. (2022) Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Tight and robust private mean estimation with few users. In _International Conference on Machine Learning_, pages 16383-16412. PMLR, 2022.
* Fan and Li (2022) Chenglin Fan and Ping Li. Distances release with differential privacy in tree and grid graph. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 2190-2195. IEEE, 2022.
* Fan et al. (2024) Chenglin Fan, Ping Li, and Xiaoyun Li. k-median clustering via metric embedding: towards better initialization with differential privacy. _Advances in Neural Information Processing Systems_, 36, 2024.
* Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. _Advances in Neural Information Processing Systems_, 35:18343-18362, 2022.
* Farhadi et al. (2022) Alireza Farhadi, MohammadTaghi Hajiaghayi, and Elaine Shi. Differentially private densest subgraph. In _International Conference on Artificial Intelligence and Statistics_, pages 11581-11597. PMLR, 2022.
* Gao et al. (2024) Yeqi Gao, Zhao Song, Xin Yang, and Yufa Zhou. Differentially private attention computation. In _Neurips Safe Generative AI Workshop 2024_, 2024.
* Gao et al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_, 2023.
* Geng et al. (2020) Quan Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar. Tight analysis of privacy and utility tradeoff in approximate differential privacy. In _International Conference on Artificial Intelligence and Statistics_, pages 89-99. PMLR, 2020.
* Ghazi et al. (2023) Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Kewen Wu. On differentially private counting on trees. In _50th International Colloquium on Automata, Languages, and Programming (ICALP 2023)_, volume 261, page 66. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2023.
* GitHub (2024a) GitHub. Agentgpt, 2024a. https://github.com/reworkd/AgentGPT.
* GitHub (2024b) GitHub. Chatgpt system prompt, 2024b. https://github.com/LouisShark/chatgpt_system_prompt.
* GitHub (2024c) GitHub. Gpt researcher, 2024c. https://github.com/assafelovic/gpt-researcher.
* GitHub (2024d) GitHub. Superagi, 2024d. https://github.com/TransformerOptimus/SuperAGI.
* GitHub (2024e)Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, and Stefano Soatto. Mixed differential privacy in computer vision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8376-8386, 2022.
* Gopi et al. (2021) Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. _Advances in Neural Information Processing Systems_, 34:11631-11642, 2021.
* Gopi et al. (2022) Sivakanth Gopi, Yin Tat Lee, and Daogao Liu. Private convex optimization via exponential mechanism. In _Conference on Learning Theory_, pages 1948-1989. PMLR, 2022.
* Gopi et al. (2023) Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, and Kevin Tian. Private convex optimization in general norms. In _Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 5068-5089. SIAM, 2023.
* Guntuboyina and Sen (2012) Adityanand Guntuboyina and Bodhisattva Sen. L1 covering numbers for uniformly bounded convex functions. In _Conference on Learning Theory_, pages 12-1. JMLR Workshop and Conference Proceedings, 2012.
* Hay et al. (2009) Michael Hay, Vibhor Rastogi, Gerome Miklau, and Dan Suciu. Boosting the accuracy of differentially-private histograms through consistency. _arXiv preprint arXiv:0904.0942_, 2009.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hertz et al. (2022) Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* Hong et al. (2024a) Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, et al. Data interpreter: An llm agent for data science. _arXiv preprint arXiv:2402.18679_, 2024a.
* Hong et al. (2024b) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In _The Twelfth International Conference on Learning Representations_, 2024b. URL https://openreview.net/forum?id=VtmBAGCN7o.
* Hopkins et al. (2023) Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 497-506, 2023.
* Huang and Yi (2021) Ziyue Huang and Ke Yi. Approximate range counting under differential privacy. In _37th International Symposium on Computational Geometry (SoCG 2021)_. Schloss-Dagstuhl-Leibniz Zentrum fur Informatik, 2021.
* Jin et al. (2024) Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan Wang. Jailbreakzo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models. _arXiv preprint arXiv:2407.01599_, 2024.
* Jung et al. (2019) Christopher Jung, Katrina Ligett, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Moshe Shenfeld. A new analysis of differential privacy's generalization guarantees. _arXiv preprint arXiv:1909.03577_, 2019.
* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* Li et al. (2024) Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. _arXiv preprint arXiv:2402.09469_, 2024.
* Li et al. (2020)Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. Privacy in large language models: Attacks, defenses and future directions. _arXiv preprint arXiv:2310.10383_, 2023a.
* Li et al. (2023b) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 4138-4153, 2023b.
* Li et al. (2017) Ninghui Li, Min Lyu, Dong Su, and Weining Yang. _Differential privacy: From theory to practice_. Springer, 2017.
* Li and Li (2023a) Ping Li and Xiaoyun Li. Differential privacy with random projections and sign random projections. _arXiv preprint arXiv:2306.01751_, 2023a.
* Li and Li (2024) Ping Li and Xiaoyun Li. Smooth flipping probability for differential private sign random projection methods. _Advances in Neural Information Processing Systems_, 36, 2024.
* Li and Li (2023b) Xiaoyun Li and Ping Li. Differentially private one permutation hashing and bin-wise consistent weighted sampling. _arXiv preprint arXiv:2306.07674_, 2023b.
* Li et al. (2021) Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In _International Conference on Learning Representations_, 2021.
* Li et al. (2022) Xuechen Li, Daogao Liu, Tatsunori B Hashimoto, Huseyin A Inan, Janardhan Kulkarni, Yin-Tat Lee, and Abhradeep Guha Thakurta. When does differentially private learning not suffer in high dimensions? _Advances in Neural Information Processing Systems_, 35:28616-28630, 2022.
* Liang et al. (2024a) Yingyu Liang, Zhenmei Shi, Zhao Song, and Chiwun Yang. Toward infinite-long prefix in transformer. _arXiv preprint arXiv:2406.14036_, 2024a.
* Liang et al. (2024b) Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Unraveling the smoothness properties of diffusion models: A gaussian mixture perspective. _arXiv preprint arXiv:2405.16418_, 2024b.
* Liang et al. (2024c) Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. _arXiv preprint arXiv:2405.16411_, 2024c.
* Liu et al. (2024a) Erzhi Liu, Jerry Yao-Chieh Hu, Alex Reneau, Zhao Song, and Han Liu. Differentially private kernel density estimation. _arXiv preprint arXiv:2409.01688_, 2024a.
* Liu et al. (2024b) Zhihao Liu, Jian Lou, Wenjie Bao, Zhan Qin, and Kui Ren. Differentially private zeroth-order methods for scalable large language model finetuning. _arXiv preprint arXiv:2402.07818_, 2024b.
* Liu et al. (2023) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. _arXiv preprint arXiv:2308.05960_, 2023.
* Mai et al. (2023) Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, and Yan Pang. Split-and-denoise: Protect large language model inference with local differential privacy. _arXiv preprint arXiv:2310.09130_, 2023.
* Mattern et al. (2022) Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Scholkopf, and Mrinmaya Sachan. Differentially private language models for secure data sharing. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 4860-4873. Association for Computational Linguistics, 2022.
* Narayanan (2022) Shyam Narayanan. Private high-dimensional hypothesis testing. In _Conference on Learning Theory_, pages 3979-4027. PMLR, 2022.
* Narayanan (2023) Shyam Narayanan. Better and simpler lower bounds for differentially private statistical estimation. _arXiv preprint arXiv:2310.06289_, 2023.
* OpenAI (2024a) OpenAI. Creating a gpt, 2024a. https://help.openai.com/en/articles/8554397-creating-a-gpt.
* Yang et al. (2020)OpenAI. Video generation models as world simulators, 2024b. https://openai.com/research/video-generation-models-as-world-simulators.
* Oymak et al. [2023] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In _International Conference on Machine Learning_, pages 26724-26768. PMLR, 2023.
* Patel [2024] Dylan Patel. Chatgpt system prompt is 1700 tokens?!, 2024. https://x.com/dylan522p/status/1755086111397863777.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* Ponomareva et al. [2023] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Guha Thakurta. How to dp-fy ml: A practical guide to machine learning with differential privacy. _Journal of Artificial Intelligence Research_, 77:1113-1201, 2023.
* Qin et al. [2022] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo. Adaptive and dynamic multi-resolution hashing for pairwise summations. In _2022 IEEE International Conference on Big Data (Big Data)_, pages 115-120. IEEE, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* Schulhoff et al. [2024] Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, et al. The prompt report: A systematic survey of prompting techniques. _arXiv preprint arXiv:2406.06608_, 2024.
* Shi et al. [2022a] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. Just fine-tune twice: Selective differential privacy for large language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6327-6340, 2022a.
* Shi et al. [2022b] Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, and Somesh Jha. The trade-off between universality and label efficiency of representations from contrastive learning. In _The Eleventh International Conference on Learning Representations_, 2022b.
* Shi et al. [2024] Zhenmei Shi, Junyi Wei, Zhuoyan Xu, and Yingyu Liang. Why larger language models do in-context learning differently? _arXiv preprint arXiv:2405.19592_, 2024.
* Singh et al. [2024] Tanmay Singh, Harshvardhan Aditya, Vijay K Madisetti, and Arshdeep Bahga. Whispered tuning: Data privacy preservation in fine-tuning llms through differential privacy. _Journal of Software Engineering and Applications_, 17(1):1-22, 2024.
* Song et al. [2023a] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order method: efficient algorithm for low-bandwidth channel and vulnerability. In _International Conference on Machine Learning_, pages 32365-32417. PMLR, 2023a.
* Song et al. [2023b] Zhao Song, Xin Yang, Yuanyuan Yang, and Lichen Zhang. Sketching meets differential privacy: fast algorithm for dynamic kronecker projection maintenance. In _International Conference on Machine Learning (ICML)_, pages 32418-32462. PMLR, 2023b.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Vershynin [2017] Roman Vershynin. An introduction with applications in data science. _Camb. Ser. Stat. Probab. Math_, 47, 2017.
* Vaswani et al. [2017]Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.
* Wang et al. (2024) Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, and Neel Joshi. Is a picture worth a thousand words? delving into spatial reasoning for vision language models. _arXiv preprint arXiv:2406.14852_, 2024.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* Woodruff et al. (2023) David Woodruff, Fred Zhang, and Samson Zhou. On robust streaming for learning with experts: algorithms and lower bounds. _Advances in Neural Information Processing Systems_, 36:79518-79539, 2023.
* Woodruff (2014) David P Woodruff. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* Xu et al. (2023) Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, and Yingyu Liang. Towards few-shot adaptation of foundation models via multitask finetuning. In _The Twelfth International Conference on Learning Representations_, 2023.
* Xu et al. (2024) Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large language models have compositional ability? an investigation into limitations and scalability. In _ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2024.
* Yang et al. (2024) Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yang et al. (2023) Mengmeng Yang, Taolin Guo, Tianqing Zhu, Ivan Tjuawinata, Jun Zhao, and Kwok-Yan Lam. Local differential privacy and its applications: A comprehensive survey. _Computer Standards & Interfaces_, page 103827, 2023.
* Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. In _International Conference on Learning Representations_, 2021.
* Yu et al. (2024) Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh Hu, Wenbo Guo, Han Liu, and Xinyu Xing. Enhancing jailbreak attack against large language models through silent tokens. _arXiv preprint arXiv:2405.20653_, 2024.
* Zhao and Chen (2022) Ying Zhao and Jinjun Chen. A survey on differential privacy for unstructured data content. _ACM Computing Surveys (CSUR)_, 54(10s):1-28, 2022.
* Zheng et al. (2024) Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixing Jiang, Shaoyang Song, and Chunlai Zhou. Locally differentially private in-context learning. In _LREC/COLING_, 2024.

**Appendix**

###### Contents

* 1 Introduction
	* 1.1 Related Work
* 2 Preliminary
	* 2.1 Notations
	* 2.2 Differential Privacy Definitions
* 3 Main Results: Cross-Attention
* 4 Key Data Structure: DPTree
	* 4.1 Technique Overview
	* 4.2 DPTree, DPTreeDistance, and DPTreeHighDim
	* 4.3 Softmax Activation
	* 4.4 Adaptive Query Data Structure
* 5 Conclusion and Discussion
* A Discussion
* B More Related Work
* C More Preliminary
* C.1 Probability Tools
* C.2 Algebraic Facts
* C.3 DP Facts
* C.4 Comparison of Truncated Laplace, Gaussian, and Laplace Mechanisms
* D Proof Outline
* D.1 Summation Segment Tree
* D.2 Sensitivity for Range Summation Problem
* D.3 Weighted \(\ell_{p}^{p}\) Distance Problem
* D.4 One-Dimensional Weighted \(\ell_{1}\) Distance Data Structure
* D.5 Softmax Activation
* D.6 Adaptive Query
* E DPTree Algorithm
* E.1 Single Data Structure
* E.2 Boost the Constant Probability to High Probability
* E.3 Algorithm of Data Structure
* E.4 Disjoint Intervals
* **Weighted \(\ell_{p}^{p}\) Distance*
* 1 One Dimensional Weighted Distance
	* 1.2 High Dimensional Weighted Distance
* 2 One-Dimensional Weighted \(\ell_{1}\) Distance Query
	* 2.1 Runtime Analysis
	* 2.2 Privacy and Accuracy Analysis
	* 2.3 One Dimension Single Data Structure
* 3 High-Dimensional Weighted \(\ell_{1}\) Query
	* 3.1 Privacy and Accuracy Analysis for High Dimensional Weighted Distance
	* 3.2 High Dimension Single Data Structure
* 1 Adaptive Query
	* 1.1 Boost the Constant Probability to High Probability
	* 1.2 From Each Fixed Query Point to All On-net Points
	* 1.3 From Net Points to All Points
	* 1.4 Effect of Different Norms on the Result
* 2 Softmax Activation
	* 2.1 Exponential Inner Product
	* 2.2 Algorithm Modifications
	* 2.3 Adaptive Softmax
	* 2.4 Proof of Main Result
Roadmap.The appendix is organized as follows. In Section A, we discuss DP-related topics and potential extensions. In Section B, we provide more related works. In Section C, we give the preliminary of our paper. In Section D, we offer an outline of our proof techniques. In Section E, we give the analysis of the data structure DPtree that can solve summation problem with DP and accuracy guarantee. In Section F, we show how to solve weighted distance problem. In Section G, we give our DPTreeDistance data structure that can solve one dimensional \(\ell_{1}\) distance problem with DP and accuracy guarantee. In Section H, we present the analysis of our DPTreeHighDim (Algorithm 7) data structure, which can address the high-dimensional \(\ell_{1}\) distance problem while ensuring differential privacy and accuracy guarantees. In Section I, we show how we can handle adaptive query. In Section J, we show how to extend our algorithm to Softmax activation and give the analysis of DPTreeSoftmax (Algorithm 2) and DPTreeSoftmaxAdaptive (Algorithm 1).

## Appendix A Discussion

How do we extend to self-attention?As self-attention is a more fundamental module in LGMs, we would like to extend our data structure to this setting. However, the challenge we faced was the dynamic update in tree nodes for each query for self-attention, which our current analysis does not support. How we can solve this challenge is crucial, and we leave it as our future direction.

Why not add noise to some other places?Where and how to add DP noises is an important problem to ask during the DP algorithm design. In this paper, we consider the problem of \(\sum_{i=1}^{n}w_{i}\exp(\langle x_{i},y\rangle/d)\) where \(y,x_{i}\in[0,R]^{d}\) and \(w\in[-R_{w},R_{w}]^{n}\) (Definition 4.1). Notice that the only place where we add noises is in the most basic building block data structure DPTree (Algorithm 3). From Lemma D.3 and the way we initialize DPTree in Algorithm 5, we see that the sensitivity \(\Delta\) of this problem is \(2R_{w}\).

A simple method for adding noise involves adding \(n\) noises to a length \(n\) array, with each item \(w_{i}\exp(\langle x_{i},y\rangle/d)\) for \(i\in[n]\). However, this approach increases the error by a factor of \(n\) by basic composition (Fact C.6) and also makes the model dependent on the number of queries. Besides, it only supports a single query and requires rebuilding the tree for each new query, rendering it impractical. In contrast, our current noise-adding technique (Lines 9 and 15 of Algorithm 3) utilizes a summation tree such that the error only increases by a factor of \(\operatorname{poly}\log n\). This method also supports multiple queries, eliminating the need to rebuild the tree each time.

How to remove the relative error parameter \(\alpha\)?The relative error parameter \(\alpha\) in Theorem 3.1 appears because of the \((1+\alpha)\)-approximation introduced in Algorithm 5 (Remark G.3) to reduce the number of required iterations from naive \(O(n)\) to \(O(\log(n)/\alpha)\). However, we notice that a recent work [Liu et al., 2024a] does not utilize \((1+\alpha)\)-approximation and still achieves \(O(\log n)\) iteration number. They introduce a new tree node representation where each node stores the sum of distances from one point to multiple points, enabling the answer to be divided into only \(\log n\) values, each combining two distance values, two count values, and \(y\) itself. Our DPTree algorithms can be integrated with their method, thus removing parameter \(\alpha\).

## Appendix B More Related Work

Differential Privacy Guarantee Analysis.Ever since Dwork et al. [2006] proposes the notion of differential privacy (DP), it has become one of the most essential standards of privacy protection in both theoretical and empirical ways [Dwork, 2008, Li et al., 2017, Zhao and Chen, 2022, Ponomareva et al., 2023, Yang et al., 2023]. DP provides a powerful, robust, and quantifiable privacy definition, allowing algorithm design with concrete privacy and accuracy guarantee [Hay et al., 2009, Esfandiari et al., 2022, Andoni et al., 2023, Li and Li, 2023b, Huang and Yi, 2021, Ghazi et al., 2023, Backurs et al., 2024, Cohen-Addad et al., 2022a, Epasto et al., 2024, Chen et al., 2022, Hopkins et al., 2023, Narayanan, 2022, 2023, Jung et al., 2019, Li and Li, 2024, Fan and Li, 2022, Fan et al., 2024, Li and Li, 2023a, Cherapanamperi et al., 2023, Cohen-Addad et al., 2022b, Dong et al., 2024, Farhadi et al., 2022, Gopi et al., 2021, 2023, Li et al., 2022, Gopi et al., 2022, Elias et al., 2020, Song et al., 2023b, Dinur et al., 2023, Woodruff et al., 2023, Song et al., 2023a, Gao et al., 2024]. Additionally, new mechanisms have been proposed beyond the traditional Laplace, Gaussian, and Exponential mechanisms [Dwork and Roth, 2014]. For example, truncated Laplacemechanism (Geng et al., 2020) is proved to be the current tightest the lower and upper bounds on the minimum noise amplitude and power cross all \((\epsilon,\delta)\)-DP distributions.

**Cross-Attention in System Prompt, RAG, Stable Diffusion and More.** Cross-attention (Vaswani et al., 2017), first introduced in language translation, is a widely used technique in many advanced AI systems. For example, Stable Diffusion (Rombach et al., 2022) and SORA (OpenAI, 2024b) employ cross-attention as a core module for a text-to-image conditional generation. This technique is also utilized by other multimodal models (Liang et al., 2024c), including Imagen (Saharia et al., 2022) and Diffusion Transformer (Peebles and Xie, 2023). In the realm of text-to-image editing, Hertz et al. (2022) analyzes and controls the cross-attention module to enable editing without requiring additional training. Furthermore, Yang et al. (2024) tackles the issue of inaccurate cross-attention maps, enhancing fine-grained control over edited regions while preventing unintended changes to other areas. In addition, Retrieval Augmented Generation (RAG) (Lewis et al., 2020, Borgeaud et al., 2022, Gao et al., 2023), a technique that improves model responses by retrieving information from a knowledge base or external documents, extensively uses cross-attention as its core design module. Cross-attention also has other applications. Oymak et al. (2023) demonstrates that the prompt-tuning (Liang et al., 2024a) task can be formulated as cross-attention, while Chen et al. (2021) uses cross-attention to fuse multi-scale features in vision transformers, thereby reducing computation. Moreover, attention-based Transformer architecture makes LGMs equipping many emergent ability (Wei et al., 2022), such as spatial reasoning (Wang et al., 2024), mathematical reasoning (Li et al., 2024), in-context learning ability (Shi et al., 2024), compositional ability (Xu et al., 2024), few-shot adaptation ability (Shi et al., 2022b; Xu et al., 2023), and so on.

## Appendix C More Preliminary

In Section C.1, we give the probability tools we use in the paper. In Section C.2, we provide the algebraic facts we use. In Section C.3, we give the DP facts we use in the paper. In Section C.4, we compare between popular DP mechanisms.

### Probability Tools

In this section, we give several probability lemmas.

**Lemma C.1** (Markov's inequality).: _If \(x\) is a nonnegative random variable and \(t>0\), we have_

\[\Pr[x\geq t]\leq\frac{\mathbb{E}[x]}{t}.\]

**Lemma C.2** (Chernoff bound, (Chernoff, 1952)).: _Let \(x_{i}\) be a Bernoulli random variable with probability \(p_{i}\) of being equal to \(1\) and \(1-p_{i}\) of being equal to \(0\), and all \(x_{i}\) for \(i\in[n]\) are independent. Let \(x=\sum_{i=1}^{n}x_{i}\). Let \(\mu=\mathbb{E}[x]=\sum_{i=1}^{n}p_{i}\). Then, for all \(\delta>0\) we have_

\[\Pr[x\geq(1+\delta)\mu]\leq\exp(-\delta^{2}\mu/3),\]

_and for all \(0<\delta<1\)_

\[\Pr[x\leq(1-\delta)\mu]\leq\exp(-\delta^{2}\mu/2).\]

**Lemma C.3** (Chebyshev's inequality).: _Let \(x\) (integrable) be a random variable with finite non-zero variance \(\sigma^{2}\) (and thus finite expected value \(\mu\)). Then for any real number \(k>0\),_

\[\Pr[|x-\mu|\geq k\sigma]\leq\frac{1}{k^{2}}.\]

### Algebraic Facts

**Fact C.4** (Upper bound of exponential, Fact C.9 in Liang et al. (2024b)).: _For \(a\in\mathbb{R}\), \(b\in\mathbb{R}\), \(a,b\leq R\), where \(R\geq 0\), we have_

\[|\exp(a)-\exp(b)|\leq\exp(R)|a-b|.\]

### DP Facts

In this section, we present several facts about differential privacy (DP). We first state the post-processing property, which means, in an algorithm, if one step is DP, all the following steps are DP.

**Fact C.5** (Post-processing, see Fact 2.1 in Ghazi et al. [2023]).: _Let \(\mathcal{A}_{1}\) be an \((\epsilon,\delta)\)-DP algorithm and \(\mathcal{A}_{2}\) be a (randomized) post-processing algorithm. Then the algorithm \(\mathcal{A}(X)=\mathcal{A}_{2}(\mathcal{A}_{1}(X))\) is still an \((\epsilon,\delta)\)-DP algorithm._

If we have many DP algorithms, we need a composition rule. The most straightforward composition is the basic/sequential composition rule.

**Fact C.6** (Basic composition, see Fact 2.3 in Ghazi et al. [2023]).: _Let \(\mathcal{A}_{1}\) be an \((\epsilon_{1},\delta_{1})\)-DP algorithm and \(\mathcal{A}_{2}\) be an \((\epsilon_{2},\delta_{2})\)-DP algorithm. Then \(\mathcal{A}(X)=(\mathcal{A}_{1}(X),\mathcal{A}_{2}(\mathcal{A}_{1}(X),X))\) is an \((\epsilon_{1}+\epsilon_{2},\delta_{1}+\delta_{2})\)-DP algorithm._

We can do much better if we know that the inputs are disjoint.

**Fact C.7** (Parallel composition, see Fact 2.4 in Ghazi et al. [2023]).: _Let \(\mathcal{A}_{1}\) be an \((\epsilon_{1},\delta_{1})\)-DP algorithm and \(\mathcal{A}_{2}\) be an \((\epsilon_{2},\delta_{2})\)-DP algorithm. Assume \(\mathcal{A}_{1}\) and \(\mathcal{A}_{2}\) depend on disjoint subsets of input coordinates. Then the algorithm \(\mathcal{A}(X)=(\mathcal{A}_{1}(X),\mathcal{A}_{2}(\mathcal{A}_{1}(X),X))\) is a \((\max\{\epsilon_{1},\epsilon_{2}\},\)\(\max\{\delta_{1},\delta_{2}\})\)-DP algorithm._

In addition, we have the advanced composition, which improves the dependence of the number of DP algorithms to square root but compromises the term \(\delta^{\prime}\).

**Theorem C.8** (Advanced composition, see Theorem 3.20 in Dwork and Roth [2014]).: _For all \(\epsilon,\delta,\delta^{\prime}\geq 0\), the class of \((\epsilon,\delta)\)-differentially private mechanisms satisfies \((\epsilon^{\prime},k\delta+\delta^{\prime})\)-differential privacy under \(k\)-fold adaptive composition for:_

\[\epsilon^{\prime}=k\epsilon(e^{\epsilon}-1)+\epsilon\sqrt{2k\log(1/\delta^{ \prime})}.\]

### Comparison of Truncated Laplace, Gaussian, and Laplace Mechanisms

We first define the Laplace mechanism as below:

**Definition C.9** (Laplace distribution).: _We use \(\mathrm{Lap}(b)\) to denote the pdf: \(p(z)=\frac{1}{2b}\exp(-\frac{|z|}{b})\)._

**Fact C.10**.: _For \(z\sim\mathrm{Lap}(b)\), \(\mathbb{E}[z]=0\), and \(\mathrm{Var}[z]=2b^{2}\). Furthermore, if \(b=\Delta/\epsilon\), we have \(\mathrm{Var}[z]=2\Delta^{2}/\epsilon^{2}\)._

In this paper, we use the Chebyshev inequality to bound the error, and from Geng et al. [2020], we know that the truncated Laplace mechanism has the current minimum variance across all \((\epsilon,\delta)\)-DP distributions.

The variance of Gaussian mechanism in Theorem 3.22 in Dwork and Roth [2014]:

\[\mathrm{Var}=\frac{2\Delta^{2}\log(1.25/\delta)}{\epsilon^{2}}.\]

The variance of Laplace mechanism in Fact C.10:

\[\mathrm{Var}=\frac{2\Delta^{2}}{\epsilon^{2}}.\]

The variance of truncated Laplace mechanism in Fact 2.5, for \(c\in(0,1]\):

\[\mathrm{Var}=\frac{2\Delta^{2}c}{\epsilon^{2}}.\]

Thus, since it has the minimum variance, we choose the truncated Laplace mechanism to design our algorithms among these popular mechanisms.

Proof Outline

This section provides the proof outline of our paper. In Section D.1, we analyze our DPTree data structure. In Section D.2, we show the sensitivity of summation problem. In Section D.3, we explain the high-level idea behind the weighted \(\ell_{p}^{p}\) distance query. In Section D.4, we show how to answer one-dimensional weighted \(\ell_{1}\) distance query. In Section D.5, we show how to answer Softmax distance query using previous algorithms. In Section D.6, we show how to handle adaptive query. By combining the results from these sections, we prove the main results in Section 4.

### Summation Segment Tree

First, in order to solve the weighted distance problem, we need to have a basic DP algorithm (Algorithm 3) that can answer simple summation queries. After analyzing its DP and error in Section E, we state the data structure theorem.

**Theorem D.1** (DPTree data structure, informal version of Theorem E.1).: _There is a data structure (see DPTree in Algorithm 3) that uses \(O(n)\) spaces to support the following operations:_

* \(\textsc{Init}(a\in\mathbb{R}^{n},n\in\mathbb{N}_{+},\Delta\in\mathbb{N}_{+}, \epsilon\in(0,1),\delta\in(0,1))\)_. It takes_ \(O(n)\) _time to initialize the data structure._
* \(\textsc{Query}(x\in[n],y\in[n])\)_. It takes_ \(O(\log n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP private, which computes_ \(\sum_{i=x}^{y}a_{i}\)_,_
* \(|z-\sum_{i=x}^{y}a_{i}|\leq O(\epsilon^{-1}\Delta\log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

During the design of the data structure, we found an interesting property based on the parallel composition rule of DP Fact C.7. We will now state the lemma, whose proof is provided in Section E.

**Lemma D.2** (Weighted sum of disjoint interval queries, informal version of Lemma E.8).: _If the following conditions hold that:_

* _Let there be_ \(t\) _disjoint intervals, i.e.,_ \(S_{j}\) _for_ \(j\in[t]\)_, such that_ \(S_{j}\cap S_{k}=\emptyset\) _for all_ \(j\neq k\)_._
* _Let_ \(\epsilon\in(0,1)\) _and_ \(\delta\in(0,1)\)_._
* _Let_ \(a_{j}\) _for_ \(j\in[t]\) _be a series that square converges to_ \(a\)_, i.e.,_ \(\sum_{j=1}^{t}a_{j}^{2}\leq a\)_._

_Then, we have Alg. 3 is \((\epsilon,\delta)\)-DP and output \(\sum_{j=1}^{t}a_{j}\textsc{Query}(S_{j})\) with the error upper bounded by_

\[O(a^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\]

_with probability \(0.99\)._

From Lemma D.2, we can see that if we have multiple disjoint interval queries, the error of the weighted sum of the intervals can be bounded independently of the number of queries, as long as the sum of squared weights is finite.

### Sensitivity for Range Summation Problem

Our DP summation tree data structure DPTree (Algorithm 3) requires sensitivity parameter \(\Delta\). In this section, we show that for the summation problem, we have the sensitivity \(\Delta=2R\) if the input \(X\in[-R,R]^{n}\).

**Lemma D.3** (Sensitivity of summation).: _Let \(X\in[-R,R]^{n}\). We have the sensitivity \(\Delta=2R\) for DPTree.Init in Algorithm 3._

Proof.: Let's say two neighboring datasets \(X\) and \(X^{\prime}\) differ in \(x_{i}\) and \(x^{\prime}_{i}\) for some \(i\) in the array \(X\). Then for a summation problem, i.e. \(f(X):=\sum_{i=1}^{n}x_{i}\), we have

\[\Delta=\max_{X,X^{\prime}}|f(X)-f(X^{\prime})|=\max_{X,X^{\prime}}|x_{i}-x^{ \prime}_{i}|=2R.\]

where the first step follows from Definition 2.2, the second step follows from \(X,X^{\prime}\) differ in \(x_{i},x^{\prime}_{i}\), and the last step follows from each coordinate of the dataset is bounded in \([-R,R]\)

### Weighted \(\ell_{p}^{p}\) Distance Problem

In this section, we introduce the intuition behind the method for handling the weighted \(\ell_{p}^{p}\) distance problem. The formal lemmas and proofs can be found in Section F.

Given a dataset and a query point in \(d\) dimensions, we round each coordinate of the data points and the query point to the nearest multiple of a small interval. We then aggregate the weights of data points that have been rounded to the same position. Finally, we compute the sum of these aggregated weights multiplied by the distances between the query point and the data points over the rounded positions. This approach makes the computation more efficient while maintaining sufficient accuracy.

We provide an example of weighted \(\ell_{1}\)-distance of a one-dimensional dataset consisting of 10 data points, i.e., \(X\in[0,1]^{10}\) and a query \(y=0\) in Figure 1.

**Lemma D.4** (Weighted \(\ell_{p}^{p}\)-distance high dimension, informal version of Lemma F.2).: _If the following conditions hold:_

* _Let data_ \(X\in[0,R]^{n\times d}\) _and_ \(x_{i}^{\top}\in[0,R]^{d}\) _be the_ \(i\)_-th row of_ \(x\)_, weight_ \(w\in\mathbb{R}^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _We round each dimension of_ \(X\) _and_ \(y\) _to an integer multiple of_ \(R/n\)_._
* _Let_ \(x_{i,k},y_{k}\) _denote the_ \(k\)_-th coordinates of_ \(x_{i},y\) _for_ \(k\in[d]\)_._
* _Let_ \(c_{j,k}:=\sum_{j_{0}\in S_{j,k}}w_{j_{0}}\) _where the set_ \(S_{j,k}\) _is the set of index_ \(i\) _such that the corresponding_ \(x_{i,k}\) _is rounded to_ \(jR/n\) _for_ \(j\in\{0,1,2,\ldots,n\}\) _for_ \(k\in[d]\)_._
* _After rounding, we assume that_ \(y_{k}\) _is in the_ \(l_{k}R/n\) _position for_ \(l_{k}\in\{0,1,2,\ldots,n\}\) _for_ \(k\in[d]\)_._

_For the weighted problem, we have_

\[\sum_{i=1}^{n}w_{i}\cdot\|y-x_{i}\|_{p}^{p}=\sum_{k=1}^{d}\sum_{j=0}^{n}(|l_{k }-j|R/n+O(R/n))^{p}c_{j,k}.\]

_where \(O(R/n)\) is the rounding error for each data point._

**Remark D.5**.: _In Lemma D.4, we first round the dataset. This rounding simplifies the calculation by reducing the number of possible positions to consider, from real values in \([0,R]^{d}\) to the total \(O(nd)\) spots. However, it also introduces an error \(O(R/n)\) for one data point. Then, for one spot in the rounded dataset, we sum over the weights of that spot and multiply the corresponding distance raised to the power of \(p\). Additionally, since we are dealing with \(\ell_{p}^{p}\) distance, the rounding error is also raised to the power of \(p\)._

### One-Dimensional Weighted \(\ell_{1}\) Distance Data Structure

Based on previous discussions in Section D.1 and D.3, we can now describe our one-dimensional weighted \(\ell_{1}\) distance data structure, DPTreeDistance, presented in Algorithm 5 and 6, which generalizes the results from Backurs et al. (2024). Drawing from the intuition in Section D.3, the initialization process is as follows: first, we round each data point in the dataset to the nearest multiple of a small interval and build an array that aggregates the corresponding weights. This array is then fed into our DPTree data structure in Algorithm 3. At query time, we query the DPTree to obtain the aggregated weights within a small interval and multiply these weights by the distance to the query point. Furthermore, we also introduce a relative error parameter \(\alpha\) to reduce the total number of queries to \(O(\log(n)/\alpha)\) instead of querying all \(n\) positions. We also analyze the DP and the error bound; see details in Section G.

**Theorem D.6** (DPTreeDistance data structure, informal version of Theorem G.6).: _There is a data structure DPTreeDistance (Algorithm 5 and 6) that uses \(O(n)\) spaces to solve weighted \(\ell_{1}\)-distance query problem for dataset \(X\subset[0,R]\) and support the following operations:_

* \(\textsc{Init}(X\subset[0,R],n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n},\epsilon \in(0,1),\delta\in(0,1))\)_. (Algorithm 5) It takes_ \(O(n)\) _time to initialize the data structure._* DistanceQuery\((y\in[0,R],\alpha\in(0,1))\). _(Algorithm 6) It takes \(O(\alpha^{-1}\log^{2}n)\) time to output a number \(z\) such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP, which computes_ \(\sum_{i\in[n]}w_{i}|y-x_{i}|\)_,_
* \(|z-\sum_{i\in[n]}w_{i}|y-x_{i}||\leq\alpha\sum_{i\in[n]}w_{i}|y-x_{i}|+O(\epsilon ^{-1}\alpha^{-1/2}RR_{w}\log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

### Softmax Activation

We then describe how we extend the previous results to Softmax activation, i.e. exponential inner product function (Definition 4.1). From Alman and Song (2023), we know that Softmax activation can be approximated by polynomial kernel function \(P(\cdot)\) with a certain error. The following lemma shows that we can transform weighted Softmax queries into polynomial kernels. More specifically, we have one term that computes the weighted \(\ell_{2}^{2}\) distance, which is the place where we add DP noises. Because of the decomposability of the \(\ell_{p}^{p}\) distance, i.e. \(\sum_{i\in[n]}w_{i}\|x_{i}-y\|_{p}^{p}=\sum_{j\in[d]}\sum_{i\in[n]}w_{i}|x_{i, j}-y_{j}|^{p}\), we can easily extend the results of Section D.4 to handle the \(\ell_{2}^{2}\) distance query. After that, we compute the term for the weighted \(\ell_{2}^{2}\) norms of approximation kernel exactly. Summing all these terms, with a certain error, we can answer the Softmax query. Related proofs can be found in Section J.

**Lemma D.7** (Weighted Softmax approximation, informal version of Lemma J.6).: _Let the accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j!}}\) (Definition J.3). Let \(P(x):[0,R]^{d}\rightarrow[0,\Gamma_{R,s}]^{r}\) be the \(s\)-th order polynomial kernel function defined in Lemma J.5. Then, we can approximate the exponential inner product using the polynomial kernel function:_

\[w^{\top}\exp(Xy/d)= -\frac{1}{2}\sum_{j\in[r]}\sum_{i\in[n]}w_{i}|P(x_{i})_{j}-P(y)_{ j}|^{2}+\frac{1}{2}\sum_{i\in[n]}w_{i}(\|P(x_{i})\|_{2}^{2}+\|P(y)\|_{2}^{2})\] \[+\;O(w^{\top}\exp(Xy/d)\cdot\epsilon_{s}).\]

_Moreover, the vectors \(P(\cdot)\) can be computed in \(O(r)\) time._

### Adaptive Query

We introduce how we can modify our algorithm to solve the adaptive query problem using some tools in Qin et al. (2022). Our approach is based on proving that our algorithm can handle any query within the query space with a certain error. Since adaptive queries must lie within this space, our algorithm can effectively handle them. In Section D.5, we demonstrate our algorithm's capability to answer weighted Softmax distance queries with constant probability. We then use the Chernoff

Figure 1: The visualization of how to build \(c_{j}\) of rounded dataset \(X\in[0,1]^{10}\) and compute the weighted \(\ell_{1}\) distance. The number above each \(x_{i}\) is \(w_{i}\). See Algorithm 5 for details. Suppose \(y=0\). Then \(\sum_{i=1}^{n}w_{i}|y-x_{i}|=0.1\cdot 2.2+0.3\cdot 3.1+0.3\cdot(-2)+0.3 \cdot(-3)+0.4\cdot 2+0.6\cdot 6+0.7\cdot 0.5+0.9\cdot(-1)+0.9\cdot 1=4.4\). And \(\sum_{j=0}^{n}|k-j|c_{j}/n=0.1\cdot 2.2+0.3\cdot(-1.9)+0.4\cdot 2+0.6\cdot 6+0.7 \cdot 0.5=4.4\). See details in Lemma F.1.

bound to boost the constant probability of our algorithm to a high probability. Next, we apply the notion of an \(\epsilon_{0}\)-net to bound all query points within the net using the union bound. Finally, we bound all points in the query space by utilizing the Lipschitz property of the weighted Softmax distance function and introducing an additive error. See the proofs in Sections I and J.

**Lemma D.8** (Adaptive Softmax, informal version of Lemma J.10).: _If the following conditions hold:_

* _Let_ \(N\) _be the_ \(\ell_{\infty}\)__\(\epsilon_{0}\)_-net of_ \(\mathcal{B}\)_, and let_ \(|N|\) _be the size of the net_ \(N\)_._
* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let the relative error parameter_ \(\alpha\in(0,1)\)_, the failure probability_ \(p_{f}\in(0,0.01)\)_._
* _We create_ \(l=O(\log((R/\epsilon_{0})^{r}/p_{f}))\) _independent copies of the data structure_ \(\{\textsc{DPTreeSoftmax}_{j}\}_{j=1}^{l}\) _(Algorithm_ 2_) and take the median of the outputs with each data structure instantiated with_ \((\epsilon/l,(\delta+\delta^{\prime})/l)\)_-DP._
* _Let_ \(f(y):=\operatorname{Median}(\{\textsc{DPTreeSoftmax}_{j}.\textsc{DistanceQuery}(y,\alpha)\}_{j=1}^{l})\)_._
* _Let_ \(Z(y):=w^{\top}\exp(Xy/d)\)_._
* _Let_ \(B=O(\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^{ \prime})}\cdot\log^{3/2}n)\)_._

_Then with probability \(1-p_{f}\), for all query points \(q\in\mathcal{B}\), there exists a point \(y\in N\) which is the closest to \(q\), we can have the process of outputting median of \(l\) responses is \((\epsilon,\delta+\delta^{\prime})\)-DP and the error satisfies_

\[|f(y)-Z(q)|\leq(\alpha+\epsilon_{s})Z(q)+B+2n\sqrt{d}RR_{w}\exp(R^{2})\epsilon _{0}.\]

## Appendix E DPTree Algorithm

In this section, we give the analysis of privacy, accuracy and runtime of our DPTree (Algorithm 3). In Section E.1, we give the theorem (Theorem E.1) of our data structure that can answer summation problem. In Section E.2, we improve our data structure from constant probability to high probability by applying Chernoff bound. In Section E.3, we give the analysis. In Section E.4, we show some results of our data structure if the input queries are disjoint.

### Single Data Structure

We give the theorem of our DPTree data structure that can answer the summation problem with DP, accuracy, runtime guarantee.

**Theorem E.1** (DPTree data structure, formal version of Theorem D.1).: _There is a data structure (see DPTree in Algorithm 3) that uses \(O(n)\) spaces to support the following operations:_

* \(\textsc{Init}(a\in\mathbb{R}^{n},n\in\mathbb{N}_{+},\Delta\in\mathbb{N}_{+}, \epsilon\in(0,1),\delta\in(0,1))\)_. It takes_ \(O(n)\) _time to initialize the data structure._
* \(\textsc{Query}(x\in[n],y\in[n])\)_. It takes_ \(O(\log n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP private, which computes_ \(\sum_{i=x}^{y}a_{i}\)_,_
* \(|z-\sum_{i=x}^{y}a_{i}|\leq O(\epsilon^{-1}\Delta\log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

Proof.: The proofs follow from combining Lemma E.3 (running time of initialization), Lemma E.4 (running time of query), Lemma E.5 (DP of query), and Lemma E.6 (error of query) together.

### Boost the Constant Probability to High Probability

We can use Chernoff bound to boost the high probability by repeating the data structure multiple times.

**Theorem E.2** (High-probability).: _There is a data structure (see DPTreeHighProb in Algorithm 4) that uses \(O(n\log(1/\delta_{\mathrm{fail}}))\) spaces to support the following operations_

* \(\textsc{Init}(a\in\mathbb{R}^{n},n\in\mathbb{N}_{+},\Delta\in\mathbb{N}_{+}, \epsilon\in(0,1),\delta\in(0,1),\delta_{\mathrm{fail}}\in(0,0.01))\)_. It takes_ \(O(n\log(1/\delta_{\mathrm{fail}}))\) _time to initialize the data structure._
* \(\textsc{Query}(x\in[n],y\in[n])\)_. It takes_ \(O(\log(n)\cdot\log(1/\delta_{\mathrm{fail}}))\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP private, which computes_ \(\sum_{i=x}^{y}a_{i}\)_,_
* \(|z-\sum_{i=x}^{y}a_{i}|\leq O(\epsilon^{-1}\Delta\log^{3/2}(n)\cdot\log(1/ \delta_{\mathrm{fail}}))\)_,_
* _it holds with probability_ \(1-\delta_{\mathrm{fail}}\) _for failure probability_ \(\delta_{\mathrm{fail}}\in(0,0.01)\)_._

Proof.: Note that our data structure (Theorem E.1) succeeds with probability \(0.99\). The success of the algorithm (Theorem E.1) can be viewed as a Bernoulli random variable, to which we apply the Chernoff bound (Lemma C.2). By repeating the data structure \(O(\log(1/\delta_{\mathrm{fail}}))\) times and taking the median of the outputs, we boost the success probability. The details are following.

To boost the success probability, we assume the query is repeated \(l\) times. Let \(i\in[l]\), and let \(z_{i}\) denote the indicator random variable for the success of the \(i\)-th instance of the data structure for a single query. Let \(z=\sum_{i=1}^{l}z_{i}\) be the total success times. Since \(p=\Pr[z_{i}=1]=0.99\), we canhave \(\mu=\mathbb{E}[z]=\sum_{i=1}^{l}p=lp\). Note that \(p=0.99\). By setting \(\delta=0.1\) and using Chernoff bound from Lemma C.2, we can show

\[\Pr[z\leq l/2]\leq\Pr[z\leq(1-\delta)lp]\leq\exp(-\delta^{2}lp/2).\]

Note that we want \(z>l/2\) (since we want at least half to succeed so we could take the median),

\[\Pr[z>l/2]\geq 1-\exp(-\delta^{2}lp/2).\]

To ensure that failure probability is \(\delta_{\mathrm{fail}}\), we have

\[\exp(-\delta^{2}lp/2)=\delta_{\mathrm{fail}}.\]

We can make this hold by choosing \(l=O(\log(1/\delta_{\mathrm{fail}}))\).

By the DP basic composition rule (Fact C.6), we need to choose \(\epsilon=\epsilon^{\prime}/O(\log(1/\delta_{\mathrm{fail}}))\) and \(\delta=\delta^{\prime}/O(\log(1/\delta_{\mathrm{fail}}))\) where \(\epsilon^{\prime},\delta^{\prime}\) are the \(\epsilon,\delta\) in Theorem E.1. 

```
1:datastructure DPTreeHighProb \(\triangleright\) Theorem E.2
2:members
3:\(\mathcal{D}_{1},\dots,\mathcal{D}_{O(\log(1/\delta_{\mathrm{fail}}))}:\) DPTree \(\triangleright\) Alg. 3
4:endmembers
5:procedureInit(\(a\in\mathbb{R}^{n},n\in\mathbb{N}_{+},\Delta\in\mathbb{N}_{+},\epsilon\in(0,1), \delta\in(0,1),\delta_{\mathrm{fail}}\in(0,0.01)\))
6:for\(i=1\to O(\log(1/\delta_{\mathrm{fail}}))\)do
7:\(\mathcal{D}_{i}.\textsc{Init}(a,n,\Delta,\epsilon/O(\log(1/\delta_{\mathrm{ fail}})),\delta/O(\log(1/\delta_{\mathrm{fail}})))\)
8:endfor
9:endprocedure
10:procedureQuery(\(x\in[n],y\in[n]\))
11:\(r\gets 0^{O(\log(1/\delta_{\mathrm{fail}}))}\)
12:for\(i=1\to O(\log(1/\delta_{\mathrm{fail}}))\)do
13:\(r_{i}\leftarrow\mathcal{D}_{i}.\textsc{Query}(x,y)\)
14:endfor
15:return Median of \(r\)
16:endprocedure
17:end datastructure ```

**Algorithm 4** Boost constant probability

### Algorithm of Data Structure

In this section, we analyze the accuracy, DP, and runtime of Algorithm 3.

We first analyze the runtime.

**Lemma E.3** (Runtime of initialization, Algorithm 3).: _For the initialization, we have the time complexity of Algorithm 3 is \(O(n)\)._

Proof.: All the computations are dominated by \(O(n)\) time. 

**Lemma E.4** (Runtime of query, Algorithm 3).: _For each query, we have the time complexity of Algorithm 3 is \(O(\log n)\)._

Proof.: Due to the property of tree, we will use at most \(2\log n\) nodes in the tree, thus the running time is \(O(\log n)\). 

We now analyze the DP.

**Lemma E.5** (Privacy of query, Algorithm 3).: _The output process of Query (see Algorithm 3) is \((\epsilon,\delta)\)-DP._Proof.: Suppose that our dataset is \(X\in[-R,R]^{n}\). Note that we only add noise in the pre-processing stage. There is no noise in the query stage. Since the problem we care about is summation, if we change one leaf node, the sensitivity \(\Delta=2R\) (see Lemma D.3). Since we add noise to each node in the tree, and each leaf node count will contribute to \(\log n\) nodes, it is equivalent to our output function being in \(\log n\) dimension. We will then blow up the DP parameter by \(\log n\) factor. Thus, using the basic composition rule (Fact C.6), the DP guarantee for the whole tree data structure is \(((\epsilon/\log n)\cdot\log n,(\delta/\log n)\cdot\log n)\) which is \((\epsilon,\delta)\)-DP. 

We now analyze the accuracy.

**Lemma E.6** (Accuracy of query, Algorithm 3).: _Let \(\epsilon\in(0,1)\) and \(\delta\in(0,1)\). Then, using Chebyshev's inequality and Fact 2.5, we have the error of Query(see Algorithm 3) output is upper bounded by:_

\[O(\epsilon^{-1}\Delta\log^{3/2}n).\]

_with probability \(0.99\)._

Proof.: For an interval \(S\), we define TrueQuery\((S)\) to be the output of DPTree.TrueQuery in Algorithm 3. Let Query\((S)\) denote the noised interval query answer returned by DPTree.Query in Algorithm 3. Let \(z:=\textsc{Query}(S)-\textsc{TrueQuery}(S)\), which from Algorithm 3 we can see this is the sum of \(O(\log n)\) independent truncated Laplace random variables each with parameter \(\mathrm{TLap}(\Delta,\epsilon/\log n,\delta/\log n)\). Thus,

\[z=\sum_{i=1}^{O(\log n)}z_{i}\]

where \(z_{i}\sim\mathrm{TLap}(\Delta,\epsilon/\log n,\delta/\log n)\), and every \(z_{i}\) are independent to each other.

We know \(\mu=\mathbb{E}[z]=0\) since \(\mathbb{E}[z_{i}]=0\). From Fact 2.5, we know the variance for each \(z_{i}\) is \(\mathrm{Var}[z_{i}]=c\epsilon^{-2}\Delta^{2}\log^{2}n\) where \(0<c\leq 2\) and \(c=2\) when \(\delta=0\).

Therefore, we can show

\[\mathrm{Var}[z] =\mathrm{Var}[\sum_{i=1}^{O(\log n)}z_{i}]\] \[=\sum_{i=1}^{O(\log n)}\mathrm{Var}[z_{i}]\] \[=O(c\epsilon^{-2}\Delta^{2}\log^{3}n)\] (4)

where the first step follows from definition of \(z\), the second step follows from every \(z_{i}\) are independent to each other, and the last step follows from \(\mathrm{Var}[z_{i}]=O(c\epsilon^{-2}\Delta^{2}\log^{2}n)\).

Note that we wish to bound \(|z|=|\textsc{Query}(S)-\textsc{TrueQuery}(S)|\) as our error.

Using Lemma C.3, we can have

\[\mathrm{Pr}[|z|\geq k\sigma]\leq\frac{1}{k^{2}}.\]

We know that \(\sigma=\sqrt{\mathrm{Var}[z]}=O(c^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\). Picking \(k=10\), we have

\[\mathrm{Pr}[|z|<10\sigma]\geq 0.99.\]

Thus, we conclude that error is bounded by \(O(c^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)=O(\epsilon^{-1}\Delta\log^{3/2}n)\) (since \(c\in(0,2]\)) with probability \(0.99\).

### Disjoint Intervals

In this section, we show some interesting results for our DPTree data structure if the input queries are disjoint.

**Lemma E.7** (Sum of disjoint interval queries).: _If the following conditions hold that:_

* _Let there be_ \(t\) _disjoint intervals, i.e.,_ \(S_{j}\) _for_ \(j\in[t]\)_, such that_ \(S_{j}\cap S_{k}=\emptyset\) _for all_ \(j\neq k\)_._
* _Let_ \(\epsilon\in(0,1)\) _and_ \(\delta\in(0,1)\)_._

_Then, we have Algorithm 3 is \((\epsilon,\delta)\)-DP and outputs \(\sum_{j=1}^{t}\textsc{Query}(S_{j})\) with the error upper bounded by_

\[O(t^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\]

_with probability \(0.99\)._

Proof.: From Lemma E.5, we know that \(\textsc{DPTree}\).Query is \((\epsilon,\delta)\)-DP. Then, from Fact C.7 and the disjoint intervals in Algorithm 6, we can conclude that the value returned is \((\epsilon,\delta)\)-DP.

Let \(\textsc{TrueQuery}(S_{j})\) denote the true interval query answer returned by DPTree.TrueQuery in Algorithm 3 for interval \(S_{j}\). Let \(\textsc{Query}(S_{j})\) denote the noised interval query answer returned by DPTree.Query in Algorithm 3 for interval \(S_{j}\). Let \(z_{j}:=\textsc{Query}(S_{j})-\textsc{TrueQuery}(S_{j})\) and \(z=\sum_{j=1}^{t}z_{j}\). From the proof of Lemma E.6, we know \(z_{j}\) is the sum of \(O(\log n)\) independent truncated Laplace random variables each with parameter \(\textsc{TLap}(\Delta,\epsilon/\log n,\delta/\log n)\) and the variance is bounded by

\[\mathrm{Var}[z_{j}]=O(\epsilon^{-2}\Delta^{2}\log^{3}n)\]

Since the intervals \(S_{j}\) are disjoint, they are independent to each other. Then, we have

\[\mathrm{Var}[z] =\mathrm{Var}[\sum_{j=1}^{t}z_{j}]\] \[=\sum_{j=1}^{t}\mathrm{Var}[z_{j}]\] \[=O(t\epsilon^{-2}\Delta^{2}\log^{3}n)\]

where the first step follows from definition of \(z\), the second step follows from the intervals are disjoint, and the last step follows from \(\mathrm{Var}[z_{j}]=O(\epsilon^{-2}\Delta^{2}\log^{2}n)\).

Note that we wish to bound \(|z|\) as our error.

Using Lemma C.3, we can have error bounded by

\[O(t^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\]

with probability \(0.99\). 

Moreover, this can be generalized to weighted sum of queries.

**Lemma E.8** (Weighted sum of disjoint interval queries, formal version of Lemma D.2).: _If the following conditions hold that:_

* _Let there be_ \(t\) _disjoint intervals, i.e.,_ \(S_{j}\) _for_ \(j\in[t]\)_, such that_ \(S_{j}\cap S_{k}=\emptyset\) _for all_ \(j\neq k\)_._
* _Let_ \(\epsilon\in(0,1)\) _and_ \(\delta\in(0,1)\)_._
* _Let_ \(a_{j}\) _for_ \(j\in[t]\) _be a series that square converges to_ \(a\)_, i.e.,_ \(\sum_{j=1}^{t}a_{j}^{2}\leq a\)_._

_Then, we have Alg. 3 is \((\epsilon,\delta)\)-DP and output \(\sum_{j=1}^{t}a_{j}\textsc{Query}(S_{j})\) with the error upper bounded by_

\[O(a^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\]

_with probability \(0.99\)._Proof.: The DP proof is the same as in the proof of Lemma E.7.

Let \(\textsc{TrueQuery}(S_{j})\) and \(\textsc{Query}(S_{j})\) be same in the proof of Lemma E.7 Let \(z_{j}:=\textsc{Query}(S_{j})-\textsc{TrueQuery}(S_{j})\) and \(z=\sum_{j=1}^{t}a_{j}z_{j}\). From the proof of Lemma E.7, we know the variance of \(z_{j}\) is bounded by

\[\mathrm{Var}[z_{j}]=O(\epsilon^{-2}\Delta^{2}\log^{3}n)\]

Since the intervals \(S_{j}\) are disjoint, they are independent to each other. Then, we have

\[\mathrm{Var}[z] = \mathrm{Var}[\sum_{j=1}^{t}a_{j}z_{j}]\] \[= \sum_{j=1}^{t}\mathrm{Var}[a_{j}z_{j}]\] \[= \sum_{j=1}^{t}a_{j}^{2}\,\mathrm{Var}[z_{j}]\] \[= \sum_{j=1}^{t}a_{j}^{2}\cdot O(\epsilon^{-2}\Delta^{2}\log^{3}n)\] \[= O(a\epsilon^{-2}\Delta^{2}\log^{3}n)\]

where the first step follows from the definition of \(z\), the second step follows from the intervals are disjoint, the third step follows from the \(\mathrm{Var}[az]=a^{2}\,\mathrm{Var}[z]\) for a random variable \(z\) and a constant \(a\), the fourth step follows from the \(\mathrm{Var}[z_{j}]=O(\epsilon^{-2}\Delta^{2}\log^{2}n)\), and the last step follows from \(\sum_{j=1}^{t}a_{j}^{2}\leq a\).

Note that we wish to bound \(|z|\) as our error.

Using Lemma C.3, we can have error bounded by

\[O(a^{1/2}\epsilon^{-1}\Delta\log^{3/2}n)\]

with probability \(0.99\). 

## Appendix F Weighted \(\ell_{p}^{p}\) Distance

In this section, we introduce how to handle weighted \(\ell_{p}^{p}\) distance problem in the high level idea. In Section F.1, we show how to solve one dimensional weighted problem. In Section F.2, we show how to solve high dimensional weighted problem by decomposing each coordinate of the high dimensional dataset.

Suppose we have the original data \(X\in[0,R]^{n}\) and weight \(w\in\mathbb{R}^{n}\) and query \(y\in[0,R]\). We want to compute the weighted \(\ell_{1}\)-distance, i.e.

\[\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|.\]

For data in \(d\)-dimension, due to the decomposability of \(\ell_{p}^{p}\) distance, our problem will be: given \(x_{i}\in[0,R]^{d}\) and \(w_{i}\in\mathbb{R}\) for \(i\in[n]\), and \(y\in[0,R]^{d}\), we can compute

\[\sum_{i=1}^{n}w_{i}\cdot\|y-x_{i}\|_{p}^{p}=\sum_{j=1}^{d}\sum_{i=1}^{n}w_{i} \cdot|y_{j}-x_{i,j}|^{p}\]

where \(x_{i,j},y_{j}\) means the \(j\)-th coordinates of \(x_{i},y\) for \(j\in[d]\).

Therefore, we can solve one dimension problem first, and then the high dimension case can be solved automatically.

### One Dimensional Weighted Distance

Now we can give the lemma for weighted distance of dataset.

**Lemma F.1** (Weighted distance one dimension).: _If the following conditions hold:_

* _Let data_ \(X\in[0,R]^{n}\)_, weight_ \(w\in\mathbb{R}^{n}\)_, query_ \(y\in[0,R]\)_._
* _We round_ \(X\) _and_ \(y\) _to an integer multiple of_ \(R/n\)_._
* _Let_ \(c_{j}=\sum_{j_{0}\in S_{j}}w_{j_{0}}\) _where set_ \(S_{j}\) _is the set of index_ \(i\) _such that the corresponding_ \(x_{i}\) _is rounded to_ \(jR/n\) _for_ \(j\in\{0,1,2,\ldots,n\}\)_._
* _After rounding, we assume_ \(y\) _is in the_ \(kR/n\) _position for_ \(k\in\{0,1,2,\ldots,n\}\)_._

_For the weighted problem, we have_

\[\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|=\sum_{j=0}^{n}(|k-j|R/n+O(R/n))c_{j}.\]

_Moreover, we have_

\[\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|^{p}=\sum_{j=0}^{n}(|k-j|R/n+O(R/n))^{p}c_{j}\]

_where \(O(R/n)\) is the rounding error for each data point._

Proof.: For each \(i\), we have:

\[w_{i}\cdot|y-x_{i}|=w_{i}\cdot(\frac{|k-j|R}{n}+O(\frac{R}{n})).\]

where \(O(R/n)\) is the rounding error introduced by each data point, since each data point will be at most \(O(R/n)\) away from its true position.

We can construct \(c_{j}\) by

\[c_{j}=\sum_{j_{0}\in S_{j}}w_{j_{0}}\]

set \(S_{j}\) is the set of index \(i\) such that the corresponding \(x_{i}\) is rounded to \(jR/n\). Moreover, \(c_{j}\) can be negative.

Summing over all \(i\) and grouping by \(j\), we get:

\[\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|=\sum_{j=0}^{n}(\frac{|k-j|R}{n}+O(\frac{R}{n }))c_{j}.\]

The total rounding error will be \(O(R)\) because we have \(n\) data points, each with an error of at most \(O(R/n)\).

Moreover, we have

\[\sum_{i=1}^{n}w_{i}\cdot|y-x_{i}|^{p}=\sum_{j=0}^{n}(\frac{|k-j|R}{n}+O(\frac {R}{n}))^{p}c_{j}.\]

### High Dimensional Weighted Distance

Finally, we can solve the problem of weighted distance for \(d\)-dimensional dataset.

**Lemma F.2** (Weighted \(\ell_{p}^{p}\)-distance high dimension, formal version of Lemma D.4).: _If the following conditions hold:_* _Let data_ \(X\in[0,R]^{n\times d}\) _and_ \(x_{i}^{\top}\in[0,R]^{d}\) _be the_ \(i\)_-th row of_ \(x\)_, weight_ \(w\in\mathbb{R}^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _We round each dimension of_ \(X\) _and_ \(y\) _to an integer multiple of_ \(R/n\)_._
* _Let_ \(x_{i,k},y_{k}\) _denote the_ \(k\)_-th coordinates of_ \(x_{i},y\) _for_ \(k\in[d]\)_._
* _Let_ \(c_{j,k}:=\sum_{j_{0}\in S_{j,k}}w_{j_{0}}\) _where set_ \(S_{j,k}\) _is the set of index_ \(i\) _such that the corresponding_ \(x_{i,k}\) _is rounded to_ \(jR/n\) _for_ \(j\in\{0,1,2,\ldots,n\}\) _for_ \(k\in[d]\)_._
* _After rounding, we assume_ \(y_{k}\) _is in the_ \(l_{k}R/n\) _position for_ \(l_{k}\in\{0,1,2,\ldots,n\}\) _for_ \(k\in[d]\)_._

_For the weighted problem, we have_

\[\sum_{i=1}^{n}w_{i}\cdot\|y-x_{i}\|_{p}^{p}=\sum_{k=1}^{d}\sum_{j=0}^{n}(|l_{k }-j|R/n+O(R/n))^{p}c_{j,k}\]

_where \(O(R/n)\) is the rounding error for each data point._

Proof.: We can show

\[\sum_{i=1}^{n}w_{i}\cdot\|y-x_{i}\|_{p}^{p} =\sum_{k=1}^{d}\sum_{i=1}^{n}w_{i}\cdot|y_{k}-x_{i,k}|^{p}\] \[=\sum_{k=1}^{d}\sum_{j=0}^{n}(|l_{k}-j|R/n+O(R/n))^{p}c_{j,k}\]

where the first step follows from decomposability of \(\ell_{p}^{p}\)-distance by dimension, the second step follows from Lemma F.1.

## Appendix G One-Dimensional Weighted \(\ell_{1}\) Distance Query

In this section, we generalize the algorithms in Backurs et al. (2024) to weighted distance. Here, we compute the problem of one-dimensional weighted \(\ell_{1}\) distance query i.e. \(\sum_{i\in[n]}w_{i}|y-x_{i}|\) for a given query \(y\in[0,R]\), weights \(w\in[-R_{w},R_{w}]^{n}\) and dataset \(X\subset[0,R]\) and \(n=|X|\). In Section G.1, we analyze the runtime of our algorithm. In Section G.2, we analyze the DP and accuracy of our algorithm. In Section G.3, we give the theorem for our DPTreeDistance data structure.

### Runtime Analysis

We first analyze the runtime.

**Lemma G.1** (Runtime of initialization, Algorithm 5).: _For the initialization, we have the time complexity of Init in Algorithm 5 is \(O(n)\)._

Proof.: In the initialization of Init, the computations need \(O(n)\) time to compute the count and \(O(\log n)\) time to build the tree. Thus, total time is \(O(n)\). 

**Lemma G.2** (Runtime of DistanceQuery, Algorithm 6).: _For the \(\ell_{1}\) distance query, we have the time complexity of DistanceQuery in Algorithm 6 is \(O(\alpha^{-1}\log^{2}n)\)._

Proof.: In DistanceQuery, the computations need \(O(\log n)\) time to compute one value from DPTree.Query and this process need to be repeated \(O(\alpha^{-1}\log n)\) times. 

**Remark G.3**.: _In Line 8 and 13 of Algorithm 6, we use \(R/(1+\alpha)^{j}\) to approximate the distance of each data point to the query in Lemma F.1, i.e. \(|k-j|R/n\). This will introduce \(\alpha\) relative error but also reduce the numbers of iteration from \(O(n)\) to \(O(\log(n)/\alpha)\)._```
1:datrastructureDPTreeDistance\(\triangleright\) Theorem G.6
2:members\(\mathcal{D}\) : DPTree
3:\(X\) : \([0,R]^{n}\)
4:\(w:[-R_{w},R_{w}]^{n}\)
5:endmembers
6:procedureInit(\(X\subset[0,R]\), \(n\in\mathbb{N}_{+}\), \(w\in[-R_{w},R_{w}]^{n}\), \(\epsilon\in(0,1)\), \(\delta\in(0,1)\) ) \(\triangleright\) Lemma F.1
7:\(X,w,a\gets X,w,0^{n+1}\)
8:for\(i=1\to n\)do
9:\(j\leftarrow\textsc{Round}(x_{i},n)\)\(\triangleright\)\(x_{i}\in X\) for \(i\in[n]\)
10:\(a_{j}\gets a_{j}+w_{i}\)
11:endfor
12:\(\mathcal{D}.\textsc{Init}(a,n+1,2R_{w},\epsilon,\delta)\)\(\triangleright\) Alg. 3, Lemma D.3
13:endprocedure
14:procedureRound(\(x\in[0,R]\), \(n\in\mathbb{N}_{+}\))
15: Let \(j\in\{0,1,2,\ldots n-1\}\) denote the integer such that \(jR/n\leq x<(j+1)R/n\)
16:if\(|x-(j+1)R/n|\leq|x-jR/n|\)then
17:\(j\gets j+1\)
18:endif
19:return\(j\)
20:endprocedure
21:end datastructure
22: ```

**Algorithm 6** One dimensional weighted \(\ell_{1}\) distance query

```
1:datrastructureDPTreeDistance\(\triangleright\) Theorem G.6
2:procedureDistanceQuery(\(y\in[0,R]\), \(\alpha\in(0,1)\)) \(\triangleright\) Lemma G.2, Lemma G.4, Lemma G.5
3:\(y\leftarrow\textsc{Round}(y,n)\cdot(R/n)\)\(\triangleright\) Alg. 5
4: Value \(\gets 0\)
5:for\(j=0,1,...,O(\log(n)/\alpha)\)do
6:\(l_{j}\leftarrow\textsc{Round}(y+\frac{R}{(1+\alpha)^{j+1}},n)\)
7:\(r_{j}\leftarrow\textsc{Round}(y+\frac{R}{(1+\alpha)^{j}},n)\)\(\triangleright\) Consider the points to the right of \(y\)
8: Value \(\leftarrow\) Value + \(\mathcal{D}.\textsc{Query}(l_{j},r_{j})\cdot\frac{R}{(1+\alpha)^{j}}\)\(\triangleright\) Alg. 3
9:endfor
10:for\(j=0,1,...,O(\log(n)/\alpha)\)do
11:\(l_{j}\leftarrow\textsc{Round}(y-\frac{R}{(1+\alpha)^{j}},n)\)
12:\(r_{j}\leftarrow\textsc{Round}(y-\frac{R}{(1+\alpha)^{j+1}},n)\)\(\triangleright\) Consider the points to the left of \(y\)
13: Value \(\leftarrow\) Value + \(\mathcal{D}.\textsc{Query}(l_{j},r_{j})\cdot\frac{R}{(1+\alpha)^{j}}\)\(\triangleright\) Alg. 3
14:endfor
15:Return Value
16:endprocedure
17:end datastructure ```

**Algorithm 7** One dimensional weighted \(\ell_{1}\) distance query

### Privacy and Accuracy Analysis

We show the DP.

**Lemma G.4** (Privacy of DistanceQuery, Algorithm 6).: _The output process of DistanceQuery (Algorithm 6) is \((\epsilon,\delta)\)-DP._

Proof.: From Lemma E.5, we know that \(\textsc{DPTree}.\textsc{Query}\) is \((\epsilon,\delta)\)-DP output. We observe that intervals in Algorithm 6 are disjoint. Then, following the same logic in the proof of Lemma E.8, we can conclude that the value returned is \((\epsilon,\delta)\)-DP. 

We now analyze the accuracy of the algorithm.

**Lemma G.5** (Accuracy of DistanceQuery, Algorithm 6).: _If the following conditions are satisfied:_

* _Let_ \(X\in[0,R]^{n}\) _be a dataset consisting of_ \(n\) _one-dimensional numbers, with weights_ \(w\in[-R_{w},R_{w}]^{n}\)_._
* _Let_ \(\alpha\in(0,1)\) _represent the relative error parameter utilized in Algorithm_ 6_._
* _Let_ \(\widetilde{A}\) _denote the output of the DistanceQuery in Algorithm_ 6_._
* _Let_ \(A_{*}:=\sum_{i\in[n]}w_{i}|y-x_{i}|\) _represent the true distance query value for a specific query_ \(y\)_._

_Then with probability \(0.99\), we have_

\[|\widetilde{A}-A_{*}|\leq\alpha A_{*}+O(\epsilon^{-1}\alpha^{-1/2}RR_{w}\log^{ 3/2}n).\]

Proof.: To simplify the explanation, we consider only the distance query for the points in \(X\) located to the right of \(y\). The proof can be symmetrically applied to the case of points to the left of \(y\). For an interval \(S_{j}:=(l_{j},r_{j})\) where \(l_{j},r_{j}\) are defined in Algorithm 6, we define \(\textsc{TrueQuery}(S_{j})\) to be the output of DPTree.TrueQuery in Algorithm 3. Let

\[\widehat{A}:=\sum_{j=0}^{O(\log(n)/\alpha)}\frac{R}{(1+\alpha)^{j}}\cdot \textsc{TrueQuery}(S_{j}).\]

Since TrueQuery returns the sum of the corresponding weights, it aligns with the true answer \(A_{*}:=\sum_{i\in[n]}w_{i}|y-x_{i}|\). Thus, we have

\[|\widehat{A}-A_{*}|\leq\alpha\cdot A_{*},\]

because for all \(j\), the distances between \(y\) and different points in \(X\) vary only by a multiplicative factor of \((1+\alpha)\).

Next we show the additive error. Let \(\textsc{Query}(S_{j})\) denote the noised interval query answer returned by DPTree.Query in Algorithm 3. Algorithm 6 outputs \(\widetilde{A}=\sum_{j=0}^{O(\log(n)/\alpha)}\frac{R}{(1+\alpha)^{j}}\cdot \textsc{Query}(S_{j}).\) We wish to bound

\[|\widehat{A}-\widetilde{A}|\leq|\sum_{j=0}^{O(\log(n)/\alpha)}\frac{R}{(1+ \alpha)^{j}}\cdot(\textsc{TrueQuery}(S_{j})-\textsc{Query}(S_{j}))|.\]

Let \(z_{j}:=\textsc{Query}(S_{j})-\textsc{TrueQuery}(S_{j})\), which from Algorithm 3 we can see this is the sum of \(O(\log n)\) independent truncated Laplace random variables.

From Lemma E.8, we only need to show that the series \(\frac{1}{(1+\alpha)^{j}}\) for \(j\in\{0,1,\ldots,O(\log(n)/\alpha)\}\) square converges to \(1/\alpha\), since \(R\) is a constant.

We can show

\[\sum_{j=0}^{O(\log(n)/\alpha)}\frac{1}{(1+\alpha)^{2j}} \leq\sum_{j=0}^{\infty}\frac{1}{(1+\alpha)^{2j}}\] \[\leq\sum_{j=0}^{\infty}\frac{1}{(1+\alpha)^{j}}\] \[=\frac{1}{1-\frac{1}{1+\alpha}}\] \[=1+\frac{1}{\alpha}\] \[=O(1/\alpha)\]

where the first step follows from we extend the finite sum to infinite sum, the second step follows from \(\frac{1}{(1+\alpha)^{2j}}\leq\frac{1}{(1+\alpha)^{j}}\), the third step follows from the closed form of geometric sum, the fourth step follows from simple algebra, and the last step follows from \(\alpha\in(0,1)\).

Then from the proof of Lemma E.8, we can know that the variance is given by

\[O(\frac{R^{2}R_{w}^{2}\log^{3}n}{\alpha\epsilon^{2}})\] (5)

since the sensitivity \(\Delta=2R_{w}\) from Lemma D.3.

Using Lemma C.3, we can have additive error bounded by

\[O(\frac{R\cdot R_{w}\log^{3/2}n}{\epsilon\sqrt{\alpha}}).\]

with probability \(0.99\). 

### One Dimension Single Data Structure

We therefore have the data structure that can solve weighted \(\ell_{1}\)-distance problem.

**Theorem G.6** (DPTreeDistance data structure, formal version of Theorem D.6).: _There is a data structure DPTreeDistance (Algorithm 5,6) that uses \(O(n)\) spaces to solve weighted \(\ell_{1}\)-distance query problem for dataset \(X\subset[0,R]\) and support the following operations:_

* \(\textsc{Init}(X\subset[0,R],n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1))\)_. (Algorithm_ 5_) It takes_ \(O(n)\) _time to initialize the data structure._
* \(\textsc{DistanceQuery}(y\in[0,R],\alpha\in(0,1))\)_. (Algorithm_ 6_) It takes_ \(O(\alpha^{-1}\log^{2}n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP private, which computes_ \(\sum_{i\in[n]}w_{i}|y-x_{i}|\)_,_
* \(|z-\sum_{i\in[n]}w_{i}|y-x_{i}|\leq\alpha\sum_{i\in[n]}w_{i}|y-x_{i}|+O( \epsilon^{-1}\alpha^{-1/2}RR_{w}\log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

Proof.: The proofs follow from combining Lemma G.1 (running time of initialization), Lemma G.2 (running time of query), Lemma G.4 (DP of query), and Lemma G.5 (error of query) together. 

## Appendix H High-Dimensional Weighted \(\ell_{1}\) Query

In this section, we show how we can solve the high dimensional weighted \(\ell_{1}\) distance problem, generalizing results from Backurs et al. (2024). In Section H.1, we give the analysis of Algorithm 7. In Section H.2, we give the theorem of our DPTreeHighDim data structure.

Algorithm 5,6 can be naturally extended to higher dimensions because of the decomposability of the \(\ell_{1}\) distance function. We construct \(d\) separate one-dimensional distance query data structures, each corresponding to a coordinate projection of the dataset.

### Privacy and Accuracy Analysis for High Dimensional Weighted Distance

We now give the analysis of our Algorithm 7 for high dimensional weighted \(\ell_{1}\)-distance query.

**Lemma H.1** (Privacy of DistanceQuery, Algorithm 7).: _If the following conditions hold_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let_ \(\epsilon\in(0,1)\)_,_ \(\delta\in(0,1)\)_,_ \(\delta^{\prime}\in(0,1)\)_._
* _Let_ \(c\in(0,0.1)\) _be a small constant and_ \(A\) _be the output of DistanceQuery in Algorithm_ 7_, where each one-dimensional algorithm is configured to be_ \((cc/\sqrt{d\log(1/\delta^{\prime})},\delta/d)\)_-DP (see Line_ 11_)._
* _Let_ \(A_{*}=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\) _represent the true distance query value._
* _Let_ \(\epsilon=O(\log(1/\delta^{\prime}))\)_._

_Then, we have the output process of DistanceQuery (Algorithm 7) is \((\epsilon,\delta+\delta^{\prime})\)-DP._

Proof.: The \((\epsilon,\delta+\delta^{\prime})\)-DP guarantee follows from the approximate DP advanced composition result Theorem C.8. Our algorithm instantiate each one-dimensional data structure with \((cc/\sqrt{d\log(1/\delta^{\prime})},\delta/d)\)-DP total \(d\) times.

From advanced composition in Theorem C.8, for a sufficient small parameter \(\epsilon\) and constant \(c\), we have the final privacy loss parameter be:

\[O(c\epsilon\sqrt{2d\log(1/\delta^{\prime})}/\sqrt{d\log(1/\delta^{\prime})})= O(\epsilon)\]

and the final failure probability parameter be:

\[d\delta/d+\delta^{\prime}=\delta+\delta^{\prime}.\]

**Lemma H.2** (Accuracy of DistanceQuery, Algorithm 7).: _If the following conditions hold_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let_ \(\epsilon\in(0,1)\)_,_ \(\delta\in(0,1)\)_,_ \(\delta^{\prime}\in(0,1)\)_._
* _Let_ \(c\in(0,0.1)\) _be a small constant and_ \(A\) _be the output of DistanceQuery in Algorithm_ 7_, where each one-dimensional algorithm is configured to be_ \((cc/\sqrt{d\log(1/\delta^{\prime})},\delta/d)\)_-DP (see Line_ 11_)._

Proof.: The \((\epsilon,\delta+\delta^{\prime})\)-DP guarantee follows from the approximate DP advanced composition result Theorem C.8. Our algorithm instantiate each one-dimensional data structure with \((cc/\sqrt{d\log(1/\delta^{\prime})},\delta/d)\)-DP total \(d\) times.

From advanced composition in Theorem C.8, for a sufficient small parameter \(\epsilon\) and constant \(c\), we have the final privacy loss parameter be:

\[O(c\epsilon\sqrt{2d\log(1/\delta^{\prime})}/\sqrt{d\log(1/\delta^{\prime})}) =O(\epsilon)\]

and the final failure probability parameter be:

\[d\delta/d+\delta^{\prime}=\delta+\delta^{\prime}.\]

**Lemma H.2** (Accuracy of DistanceQuery, Algorithm 7).: _If the following conditions hold_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let_ \(\epsilon\in(0,1)\)_,_ \(\delta\in(0,1)\)_,_ \(\delta^{\prime}\in(0,1)\)_._
* _Let_ \(c\in(0,0.1)\) _be a small constant and_ \(A\) _be the output of DistanceQuery in Algorithm_ 7_, where each one-dimensional algorithm is configured to be_ \((cc/\sqrt{d\log(1/\delta^{\prime})},\delta/d)\)_-DP (see Line_ 11_)._* _Let_ \(A_{*}=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\) _represent the true distance query value._

_With probability \(0.99\), we have_

\[|A-A_{*}|\leq\alpha A_{*}+O(\epsilon^{-1}\alpha^{-1/2}RR_{w}d\sqrt{\log(1/ \delta^{\prime})}\cdot\log^{3/2}n).\]

Proof.: Let \(A_{i}\) be the \(i\)-th dimension output returned by \(\mathcal{D}_{i}\) in Algorithm 7. Let \(A_{*,i}\) be the true distance query value in the \(i\)-th dimension. Observe that \(A_{*}=\sum_{i=1}^{d}A_{*,i}\) and \(A=\sum_{i=1}^{d}A_{i}\).

We follow the similar idea in the proof of Lemma G.5. Let \(z_{j,i}\) be the random variables that represent \(z_{j}\) (used in the proof of Lemma G.5) for the \(i\)-th coordinate. We can observe that the overall error across \(d\) coordinates can be upper bounded by

\[|\sum_{i=1}^{d}\sum_{j=0}^{O(\log(n)/\alpha)}\frac{Rz_{j,i}}{(1+\alpha)^{j}}|\]

where each \(z_{j,i}\) is the sum of \(O(\log n)\) truncated Laplace random variables independent to others. With \(\epsilon\) scaled down by \(c\epsilon/\sqrt{d\log(1/\delta^{\prime})}\) and \(\delta\) scaled down by \(\delta/d\), the variance of each individual dimension is given by (see Eq. (5))

\[O(\alpha^{-1}\epsilon^{-2}dR^{2}R_{w}^{2}\log(1/\delta^{\prime})\log^{3}n).\]

Thus, the total variance for \(d\) instantiated data structures is then

\[O(\alpha^{-1}\epsilon^{-2}d^{2}R^{2}R_{w}^{2}\log(1/\delta^{\prime})\log^{3}n).\]

Finally, from Lemma C.3, we have the additive error given by

\[O(\alpha^{-1/2}\epsilon^{-1}dRR_{w}\sqrt{\log(1/\delta^{\prime})}\cdot\log^{3 /2}n).\]

### High Dimension Single Data Structure

We have the data structure that can solve weighted \(\ell_{1}\)-distance problem in \(d\)-dimensional data.

**Theorem H.3** (DPTreeHighDim data structure).: _There is a data structure DPTreeHighDim (Algorithm 7) that uses \(O(nd)\) spaces to solve weighted \(\ell_{1}\)-distance query problem for dataset \(X\subset[0,R]^{d}\) and support the following operations:_

* \(\text{Init}(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1))\)_. (Algorithm 7) It takes_ \(O(nd)\) _time to initialize the data structure._
* \(\text{DistanceQuery}(y\in[0,R]^{d},\alpha\in(0,1))\)_. (Algorithm 7) It takes_ \(O(\alpha^{-1}d\log^{2}n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies is_ \((\epsilon,\delta+\delta^{\prime})\)_-DP private, which computes_ \(\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\)_,_
* \(|z-\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}|\leq\alpha\sum_{i\in[n]}w_{i}\|y-x_{i}\|_ {1}+O(\epsilon^{-1}\alpha^{-1/2}RR_{w}d\sqrt{\log(1/\delta^{\prime})}\cdot \log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

Proof.: For the runtime analysis, since we loop data structure DPTreeDistance\(d\) times, an additional \(d\) factor will appear for both initialization and query time complexity. The DP is proved by Lemma H.1. The accuracy is proved by Lemma H.2.

Adaptive Query

In this section, we introduce how we can solve the adaptive query problem by our algorithm, using some tools from Qin et al. (2022). Our idea is that, if we can prove that our algorithm can solve any query in the query space with certain error. Then, since adaptive query must lie in this space, we can handle adaptive query. In Section I.1, we show how we can boost the constant probability of our algorithm to high probability. In Section I.2, we show how we can apply the notion of \(\epsilon_{0}\)-net and bound all query points in net. In Section I.3, we show how we can bound all points in the query space by introducing an additive error. In Section I.4, we examine the effects of different norms on our adaptive query proof.

First, from Theorem H.3, given query \(y\in[0,R]^{d},\alpha\in(0,1)\) we have DistanceQuery\((y,\alpha)\) that can solve \(d\)-dimension weighted \(\ell_{1}\)-distance problem with constant probability \(0.99\). Now we show how to improve it to solve adaptive query problem.

### Boost the Constant Probability to High Probability

We can repeat the data structure multiple times and take the median to boost the constant probability using Chernoff bound from Lemma C.2.

**Lemma I.1** (Using Chernoff bound to boost the probability).: _If the following conditions hold:_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let relative error parameter_ \(\alpha\in(0,1)\)_, the failure probability_ \(p_{f}\in(0,0.01)\)_._
* _We create_ \(l=O(\log(1/p_{f}))\) _independent copies of data structure_ DPTreeHighDim _and take the median of the outputs with each data structure instantiated with_ \((\epsilon/l,(\delta+\delta^{\prime})/l)\)_-DP._
* _Let_ \(A_{*}=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\) _be the true answer._
* _Let_ \(B=O(\epsilon^{-1}\alpha^{-1/2}lRR_{w}d\sqrt{\log(l/\delta^{\prime})}\cdot\log ^{3/2}n)\)_._

_Then for each fixed query point \(y\), we can have the process of outputting the median of \(l\) responses is \((\epsilon,\delta+\delta^{\prime})\)-DP and the error is upper bounded by \(\alpha A_{*}+B\) with probability \(1-p_{f}\)._

Proof.: By basic composition Fact C.6, we prove the DP. Similar to the proof of Theorem E.2, we prove the error by Chernoff bound (Lemma C.2). 

### From Each Fixed Query Point to All On-net Points

In this section, we build \(\epsilon_{0}\)-net and generalize from each fixed query point to all on-net points.

**Definition I.2** (\(\ell_{p}\)\(\epsilon_{0}\)-net, see Definition 4.2.1 in Vershynin (2017)).: _We define \(N\) be \(\ell_{p}\)\(\epsilon_{0}\)-net of \(\mathcal{B}:=\{q\in[0,R]^{d}\}\) such that, for every point \(q\) in \(\mathcal{B}\), there exists \(y\in N\) satisfying \(\|y-q\|_{p}\leq\epsilon_{0}\)._

**Fact I.3** (\(\ell_{\infty}\)\(\epsilon_{0}\)-net).: _Let \(N\) be the \(\ell_{\infty}\)\(\epsilon_{0}\)-net of \(\mathcal{B}\), and \(|N|\) be the size of net \(N\). We have \(|N|\leq(5R/\epsilon_{0})^{d}\)._

**Fact I.4** (\(\ell_{2}\)\(\epsilon_{0}\)-net, see Lemma 5 in Woodruff (2014)).: _Let \(N\) be the \(\ell_{2}\)\(\epsilon_{0}\)-net of \(\mathcal{B}\), and \(|N|\) be the size of net \(N\). We have \(|N|\leq(5R/\epsilon_{0})^{d}\)._

**Fact I.5** (\(\ell_{1}\)\(\epsilon_{0}\)-net, see Theorem 2 in Guntuboyina and Sen (2012)).: _Let \(N\) be the \(\ell_{1}\)\(\epsilon_{0}\)-net of \(\mathcal{B}\), and \(|N|\) be the size of net \(N\). We have \(|N|\leq(5R\sqrt{d}/\epsilon_{0})^{d}\)._

**Lemma I.6** (From for each query point to for all points in net).: _If the following conditions hold:_

* _Let_ \(N\) _be the_ \(\ell_{\infty}\)__\(\epsilon_{0}\)_-net of_ \(\mathcal{B}\)_, and_ \(|N|\) _be the size of net_ \(N\)_._
* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let relative error parameter_ \(\alpha\in(0,1)\)_, the failure probability_ \(p_{f}\in(0,0.01)\)* _We create_ \(l=O(\log(|N|/p_{f}))\) _independent copies of data structure_ DPTreeHighDim _and take the median of the outputs with each data structure instantiated with_ \((\epsilon/l,(\delta+\delta^{\prime})/l)\)_-DP._
* _Let_ \(A_{*}=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\) _be the true answer._
* _Let_ \(B=O(\epsilon^{-1}\alpha^{-1/2}lRR_{w}d\sqrt{\log(l/\delta^{\prime})}\cdot\log ^{3/2}n)\)_._

_Then with probability \(1-p_{f}\), for all query points \(y\in N\), we can have the process of outputting the median of \(l\) responses is \((\epsilon,\delta+\delta^{\prime})\)-DP and the error is upper bounded by \(\alpha A_{*}+B\)._

Proof.: By basic composition Fact C.6, we prove the DP. From Lemma I.1, we know for each \(y\in N\), the error is upper bounded by \(\alpha A_{*}+B\) with probability \(1-p_{f}/|N|\).

Then, by union bound, with probability \(1-p_{f}\), the error of all \(|N|\) query points in the net \(y\in N\) is upper bounded by \(\alpha A_{*}+B\). 

### From Net Points to All Points

In this section, we show how to generalize points from net to all points in the query space. Since adaptive query must lie in this space, we complete the proof of adaptive query.

**Lemma I.7** (Lipschitz of query function).: _If the following conditions hold:_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let_ \(Z(y):=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\)_._
* _Let_ \(L=nR_{w}\)_._

_Then, we have \(Z(y)\) is \(L\)-Lipschitz (note that we have \(\ell_{1}\) Lipschitz here)._

Proof.: We can show

\[|Z(y)-Z(\widetilde{y})| =|\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}-\sum_{i\in[n]}w_{i}\| \widetilde{y}-x_{i}\|_{1}|\] \[\leq\sum_{i\in[n]}|w_{i}|\cdot\|y-x_{i}\|_{1}-\|\widetilde{y}-x_ {i}\|_{1}|\] \[\leq\sum_{i\in[n]}|w_{i}|\cdot\|y-\widetilde{y}\|_{1}\] \[=nR_{w}\cdot\|y-\widetilde{y}\|_{1}\]

where the first step follows from definition of \(Z(y)\), the second step follows from triangular inequality, the third step follows from reverse triangular inequality, the fourth step follows from \(w\in[-R_{w},R_{w}]^{n}\). 

**Lemma I.8** (From points in net to all points in query space).: _If the following conditions hold:_

* _Let_ \(N\) _be the_ \(\ell_{\infty}\)__\(\epsilon_{0}\)_-net of_ \(\mathcal{B}\)_, and_ \(|N|\) _be the size of net_ \(N\)_._
* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let relative error parameter_ \(\alpha\in(0,1)\)_, the failure probability_ \(p_{f}\in(0,0.01)\)_._
* _We create_ \(l=O(\log((R/\epsilon_{0})^{d}/p_{f}))\) _independent copies of data structure_ \(\{\textsc{DPTreeHighDim}_{j}\}_{j=1}^{l}\) _and take the median of the outputs with each data structure instantiated with_ \((\epsilon/l,(\delta+\delta^{\prime})/l)\)_-DP._
* _Let_ \(f(y):=\mathrm{Median}(\{\textsc{DPTreeHighDim}_{j}.\textsc{DistanceQuery}(y, \alpha)\}_{j=1}^{l})\)_._
* _Let_ \(Z(y):=\sum_{i\in[n]}w_{i}\|y-x_{i}\|_{1}\)_, where_ \(Z(y)\) _is_ \(L\)_-Lipschitz with_ \(L=nR_{w}\)* _Let_ \(B=O(\epsilon^{-1}\alpha^{-1/2}lRR_{w}d\sqrt{\log(l/\delta^{\prime})}\cdot\log^{3/2 }n)\)_._

_Then with probability \(1-p_{f}\), for all query points \(q\in\mathcal{B}\), there exists a point \(y\in N\) which is the closest to \(q\), we can have the process of outputting the median of \(l\) responses is \((\epsilon,\delta+\delta^{\prime})\)-DP and the error satisfy_

\[|f(y)-Z(q)|\leq\alpha Z(q)+B+2Ld\epsilon_{0}.\]

Proof.: By basic composition Fact C.6, we prove the DP.

We define an event \(E\) such that:

\[\forall y\in N\] \[|f(y)-Z(y)|\leq\alpha Z(y)+B.\]

From Lemma I.1, with \(l=O(\log(|N|/p_{f}))\) we know

\[\Pr[\text{event }E\text{ holds}]\geq 1-p_{f}\]

We can show

\[l =O(\log(|N|/p_{f})\] \[=O(\log((R/\epsilon_{0})^{d}/p_{f})\]

where the first step follows from definition of \(l\), the second step follows from Fact I.3.

We condition on event \(E\) to be held. Then, by definition of \(\ell_{\infty}\)\(\epsilon_{0}\)-net (see Definition I.2), for each \(q\notin N\), there exists \(y\in N\) such that

\[\|y-q\|_{\infty}\leq\epsilon_{0}\] (6)

We know

\[|Z(y)-Z(q)| \leq L\cdot\|y-q\|_{1}\] \[\leq L\cdot d\|y-q\|_{\infty}\] \[\leq L\cdot d\epsilon_{0}\] (7)

where the first step follows from Lemma I.7, the second step follows from \(\|x\|_{1}\leq d\|x\|_{\infty}\) for \(x\in\mathbb{R}^{d}\), and the last step follows from Eq. (6).

Using the on-net query \(y\) to answer the off-net query \(q\), for any \(q\notin N\), we have

\[|f(y)-Z(q)| \leq|f(y)-Z(y)|+|Z(q)-Z(y)|\] \[\leq|f(y)-Z(y)|+L\cdot d\cdot\epsilon_{0}\] \[\leq\alpha Z(y)+B+L\cdot d\cdot\epsilon_{0}\] \[\leq\alpha Z(q)+B+2L\cdot d\cdot\epsilon_{0}\] (8)

where the first step follows from triangular inequality, the second step follows from Eq. (7), the third step follows from Lemma I.6, and the last step follows from Eq. (7).

Thus, we complete the proof. 

Therefore, even adaptive queries can be answered accurately, since any adaptive query can be assumed in \(\mathcal{B}\).

### Effect of Different Norms on the Result

In the above proof, we have two different measure spaces, i.e. \(\ell_{\infty}\) distance of \(\epsilon_{0}\)-net (Definition I.2) and \(\ell_{1}\) Lipschitz (Lemma I.7).

One might ask, will the norm we choose in two spaces have an impact on the final result? We can show that the norm we choose currently is sufficient to use.

For different norms, the only differences in the proofs will be Lipschitz smoothness in Eq. (7) and the cardinality of \(\epsilon_{0}\)-net, i.e. \(|N|\) in Fact I.3.

**Lemma I.9**.: _If we use \(\ell_{\infty}\)\(\epsilon_{0}\)-net and use \(\ell_{1}\) Lipschitz in Lemma I.8, we have copies of data structure \(l=O(d\log(nR/p_{f}))\)._

Proof.: If we use \(\ell_{\infty}\) to bound the distance to net, Eq. (7) is:

\[|Z(y)-Z(q)| \leq nR_{w}\cdot\|y-q\|_{1}\] \[\leq nR_{w}\cdot d\|y-q\|_{\infty}\] \[\leq nR_{w}\cdot d\epsilon_{0}\]

where the first step follows from Lemma I.7, the second step follows from \(\|x\|_{1}\leq d\|x\|_{\infty}\) for \(x\in\mathbb{R}^{d}\), and the last step follows from \(\ell_{\infty}\)\(\epsilon_{0}\)-net.

Then, Eq. (8) is

\[|f(y)-Z(q)|\leq\alpha Z(q)+B+2nR_{w}\cdot d\cdot\epsilon_{0}\]

For \(\ell_{\infty}\) distance, we have \(|N|\leq(5R/\epsilon_{0})^{d}\) in Fact I.3.

We can choose \(\epsilon_{0}=\Theta(1/n)\) to hide \(nR_{w}\cdot d\cdot\epsilon_{0}\) term in \(B\) in Lemma I.8. Thus,

\[l =O(\log(|N|/p_{f})\] \[=O(\log((R/\epsilon_{0})^{d}/p_{f})\] \[=O(\log((nR)^{d}/p_{f}))\] \[=O(d\log(nR/p_{f}))\]

where the last step follows from \(\log(a^{d}/b)=O(d\log(a/b))\) for any \(a>1,0<b<1,d>1\). 

**Lemma I.10**.: _If we use \(\ell_{2}\)\(\epsilon_{0}\)-net and use \(\ell_{1}\) Lipschitz in Lemma I.8, we have copies of data structure \(l=O(d\log(nR/p_{f}))\)._

Proof.: If we use \(\ell_{2}\) to bound the distance to net, Eq. (7) changes to be:

\[|Z(y)-Z(q)| \leq nR_{w}\cdot\|y-q\|_{1}\] \[\leq nR_{w}\cdot\sqrt{d}\cdot\|y-q\|_{2}\] \[\leq nR_{w}\cdot\epsilon_{0}\sqrt{d}\]

where the first step follows from Lemma I.7, the second step follows from \(\|x\|_{1}\leq\sqrt{d}\cdot\|x\|_{2}\) for \(x\in\mathbb{R}^{d}\), and the last step follows from \(\ell_{2}\)\(\epsilon_{0}\)-net.

Then, Eq. (8) changes to be

\[|f(y)-Z(q)|\leq\alpha Z(q)+B+2nR_{w}\cdot\epsilon_{0}\sqrt{d}\]

For \(\ell_{2}\) distance, we also have \(|N|\leq(5R/\epsilon_{0})^{d}\) in Fact I.4.

We can choose \(\epsilon_{0}=\Theta(1/n)\) to hide \(nR_{w}\cdot\sqrt{d}\cdot\epsilon_{0}\) term in \(B\) in Lemma I.8. Thus,

\[l =O(\log(|N|/p_{f})\] \[=O(\log((R/\epsilon_{0})^{d}/p_{f})\] \[=O(\log((nR)^{d}/p_{f}))\] \[=O(d\log(nR/p_{f}))\]

where the last step follows from \(\log(a^{d}/b)=O(d\log(a/b))\) for any \(a>1,0<b<1,d>1\). 

**Lemma I.11**.: _If we use \(\ell_{1}\)\(\epsilon_{0}\)-net and use \(\ell_{1}\) Lipschitz in Lemma I.8, we have copies of data structure \(l=O(d\log(ndR/p_{f}))\)._Proof.: If we use \(\ell_{1}\) to bound the distance to net, Eq. (7) changes to be:

\[|Z(y)-Z(q)| \leq nR_{w}\cdot\|y-q\|_{1}\] \[\leq nR_{w}\cdot\epsilon_{0}\]

where the first step follows from Lemma I.7, and the last step follows from \(\ell_{1}\)\(\epsilon_{0}\)-net.

Then, Eq. (8) changes to be

\[|f(y)-Z(q)|\leq\alpha Z(q)+B+2nR_{w}\cdot\epsilon_{0}\]

For \(\ell_{1}\) distance, we have \(|N|\leq(5R\sqrt{d}/\epsilon_{0})^{d}\).

We can choose \(\epsilon_{0}=\Theta(1/n)\) to hide \(nR_{w}\cdot\epsilon_{0}\) term in \(B\) in Lemma I.8. Thus,

\[l =O(\log(|N|/p_{f})\] \[=O(\log((R\sqrt{d}/\epsilon_{0})^{d}/p_{f})\] \[=O(\log((nR\sqrt{d})^{d}/p_{f}))\] \[=O(d\log(nRd/p_{f}))\]

where the last step follows from \(\log(a^{d}/b)=O(d\log(a/b))\) for any \(a>1,0<b<1,d>1\). 

From the above analysis, we can show that \(\ell_{\infty}\) or \(\ell_{2}\)\(\epsilon_{0}\)-net is slightly better than \(\ell_{1}\)\(\epsilon_{0}\)-net.

* \(\ell_{\infty}\)\(\epsilon_{0}\)-net, Lemma I.9: we have \(l=O(d\log(nR/p_{f}))\).
* \(\ell_{2}\)\(\epsilon_{0}\)-net, Lemma I.10: we have \(l=O(d\log(nR/p_{f}))\).
* \(\ell_{1}\)\(\epsilon_{0}\)-net, Lemma I.11: we have \(l=O(d\log(nRd/p_{f}))\).

Thus, the norm we choose for \(\epsilon_{0}\)-net is sufficient good.

## Appendix J Softmax Activation

In this section, we introduce how we extend previous \(\ell_{1}\) distance results to the Softmax activation function, which is the most widely used distance measure in attention mechanism based models.

In Section J.1, we show how to extend to the Softmax distance function in Lemma J.6. In Section J.2, we show how to adjust our algorithms. In Section J.3, we extend our algorithm to be robust to adaptive query. In Section J.4, we give the proof of our main result Theorem 3.1.

### Exponential Inner Product

In this section, we show how we obtain the Softmax distance using \(\ell_{2}^{2}\) distance query. First, we provide some helpful results from Alman and Song (2023).

**Definition J.1** (Definition 3.1 in Alman and Song (2023)).: _Let \(r\geq 1\) denote a positive integer. Let \(\epsilon\in(0,0.1)\) denote an accuracy parameter. Given a matrix \(A\in\mathbb{R}_{\geq 0}^{n\times n}\), we say \(\widetilde{A}\in\mathbb{R}_{\geq 0}^{n\times n}\) is an \((\epsilon,r)\)-approximation of \(A\) if_

* \(\widetilde{A}=U_{1}\cdot U_{2}^{\top}\) _for some matrices_ \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) _(i.e.,_ \(\widetilde{A}\) _has rank at most_ \(r\)_), and_
* \(|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon\cdot A_{i,j}\) _for all_ \((i,j)\in[n]^{2}\)_._

**Lemma J.2** (Lemma 3.4 in Alman and Song (2023)).: _Suppose \(Q,K\in\mathbb{R}^{n\times d}\), with \(\|Q\|_{\infty}\leq R\), and \(\|K\|_{\infty}\leq R\). Let \(A:=\exp(QK^{\top}/d)\in\mathbb{R}^{n\times n}\). For accuracy parameter \(\epsilon\in(0,0.1)\), there is a positive integer \(s\) bounded above by_

\[s=O\Big{(}\max\Big{\{}\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/R)},R^{2} \Big{\}}\Big{)},\] (9)

_and a positive integer \(r\) bounded above by_

\[r\leq\binom{2s+2d}{2s}\] (10)_such that: There is a matrix \(\tilde{A}\in\mathbb{R}^{n\times n}\) that is an \((\epsilon,r)\)-approximation (Definition J.1) of \(A\in\mathbb{R}^{n\times n}\). Furthermore, the matrices \(U_{1}\) and \(U_{2}\) defining \(\widetilde{A}\) can be computed in \(O(n\cdot r)\) time._

Here we consider the vector version of Lemma J.2.

**Definition J.3**.: _We define \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j}!}\)._

Then, we have \(P(x):[0,R]^{d}\to[0,\Gamma_{R,s}]^{r}\) where \(P(\cdot)\) is polynomial kernel function defined in Alman and Song (2023).

**Remark J.4**.: _We use \(\Gamma_{R,s}\) to denote the value range of our polynomial kernel methods function, i.e., \(P(x):[0,R]^{d}\to[0,\Gamma_{R,s}]^{r}\). The factorial term in \(\Gamma_{R,s}\) comes from Taylor approximation coefficients. We take the maximum overall \(s\) order approximation terms to get the upper bound of our value range._

**Lemma J.5** (Polynomial approximation).: _For any accuracy parameter \(\epsilon_{s}\in(0,0.1)\), let \(R\geq 1\), and let \(P(x):[0,R]^{d}\to[0,\Gamma_{R,s}]^{r}\) be the \(s\)-th order polynomial kernel function defined in Alman and Song (2023) where \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Then, for any \(x,y\in[0,R]^{d}\), we have_

\[|P(x)^{\top}P(y)-\exp(x^{\top}y/d)|\leq\epsilon_{s}\cdot\min\{\exp(x^{\top}y/d),P(x)^{\top}P(y)\}\]

_Furthermore, the vectors \(P(x)\) and \(P(y)\) can be computed in \(O(r)\) time._

Proof.: Let \(n=1\). The proof follows from directly applying Lemma J.2. 

Using the results from Alman and Song (2023) above, we can extend our results to Softmax activation.

**Lemma J.6** (Weighted Softmax approximation, formal version of Lemma D.7).: _Let accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(P(x):[0,R]^{d}\to[0,\Gamma_{R,s}]^{r}\) be the \(s\)-th order polynomial kernel function defined in Lemma J.5. Then we can approximate exponential inner product using polynomial kernel function:_

\[w^{\top}\exp(Xy/d)= -\frac{1}{2}\sum_{j\in[r]}\sum_{i\in[n]}w_{i}|P(x_{i})_{j}-P(y)_ {j}|^{2}+\frac{1}{2}\sum_{i\in[n]}w_{i}(\|P(x_{i})\|_{2}^{2}+\|P(y)\|_{2}^{2})\] \[+\ O(w^{\top}\exp(Xy/d)\cdot\epsilon_{s})\]

_Moreover, the vectors \(P(\cdot)\) can be computed in \(O(r)\) time._

Proof.: From Lemma J.5, we can use polynomial kernel to approximate the Softmax function:

\[w^{\top}\exp(Xy/d)=\sum_{i\in[n]}w_{i}P(x_{i})^{\top}P(y)+O(w^{\top}\exp(Xy/d )\cdot\epsilon_{s}).\]

The proof of approximation error and time complexity of constructing \(P(\cdot)\) follows from Lemma J.5. Then, we can show

\[2\sum_{i\in[n]}w_{i}P(x_{i})^{\top}P(y)= -\sum_{i\in[n]}w_{i}\|P(x_{i})-P(y)\|_{2}^{2}+\sum_{i\in[n]}w_{i} (\|P(x_{i})\|_{2}^{2}+\|P(y)\|_{2}^{2})\] \[= -\sum_{j\in[r]}\sum_{i\in[n]}w_{i}|P(x_{i})_{j}-P(y)_{j}|^{2}+ \sum_{i\in[n]}w_{i}(\|P(x_{i})\|_{2}^{2}+\|P(y)\|_{2}^{2})\]

where the first step follows from \(\|x-y\|_{2}^{2}=\|x\|_{2}^{2}+\|y\|_{2}^{2}-2\langle x,y\rangle\), and the second step follows \(\|x\|_{2}^{2}=\sum_{j=1}^{d}|x_{j}|^{2}\) for \(x\in\mathbb{R}^{d}\).

### Algorithm Modifications

Based on Lemma J.6, we can now extend our DP algorithms to handle Softmax activation. First, we need to construct \(P(y)\) and \(P(x_{i})\) for \(i\in[n]\), each costing \(O(r)\) time. Then, for the second term in Lemma J.6, i.e. \(\frac{1}{2}\sum_{i\in[n]}w_{i}(\|P(x_{i})\|_{2}^{2}+\|P(y)\|_{2}^{2})\), we don't need to add DP noises in it; instead, we calculate this term exactly, preprocess it, and store the results in the algorithm. For the first term, \(-\frac{1}{2}\sum_{j\in[r]}\sum_{i\in[n]}w_{i}|P(x_{i})_{j}-P(y)_{j}|^{2}\), we can adjust our high dimensional DP distance query algorithm to solve it.

Due to the decomposability of \(\ell_{p}^{p}\) norm, i.e.

\[\sum_{i\in[n]}w_{i}\|x_{i}-y\|_{p}^{p}=\sum_{j\in[d]}\sum_{i\in[n]}w_{i}|x_{i,j }-y_{j}|^{p},\]

we can compute \(\ell_{2}^{2}\) norm easily (see details in Lemma F.2). We then show how to extend our one dimensional \(\ell_{1}\) distance algorithm (Algorithm 5 and 6) to \(\ell_{2}^{2}\) distance with minor modifications.

**Theorem J.7** (DPTreeDistance \(\ell_{2}^{2}\) distance).: _With \(\alpha\) scaled down by a factor of \(2\) and all Query instead multiplied by \(R^{2}/(1+\alpha/2)^{2j}\) in Lines 8 and 13 of Algorithm 6, i.e., from_

* _Lines 8 and 13: Value_ \(\leftarrow\) _Value +_ \(\mathcal{D}\)_.Query(_\(l_{j},r_{j}\)_)_ \(\cdot\frac{R}{(1+\alpha)^{j}}\)__

_to_

* _Lines 8 and 13: Value_ \(\leftarrow\) _Value +_ \(\mathcal{D}\)_.Query(_\(l_{j},r_{j}\)_)_ \(\cdot\frac{R^{2}}{(1+\alpha/2)^{2j}}\)_._

_The data structure DPTreeDistance (Algorithm 5,6) uses \(O(n)\) spaces to solve weighted \(\ell_{2}^{2}\)-distance query problem for dataset \(X\subset[0,R]\) and support the following operations:_

* \(\textsc{Init}(X\subset[0,R],n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1))\)_. (Algorithm 5) It takes_ \(O(n)\) _time to initialize the data structure._
* DistanceQuery(\(y\in[0,R],\alpha\in(0,1)\))_. (Algorithm 6) It takes_ \(O(\alpha^{-1}\log^{2}n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta)\)_-DP private, which computes_ \(\sum_{i\in[n]}w_{i}|y-x_{i}|^{2}\)_,_
* \(|z-\sum_{i\in[n]}w_{i}|y-x_{i}|^{2}\leq\alpha\sum_{i\in[n]}w_{i}|y-x_{i}|^{2}+ O(\epsilon^{-1}\alpha^{-1/2}R^{2}R_{w}\log^{3/2}n)\)_,_
* _it holds with probability_ \(0.99\)_._

Proof.: The proof is similar to that of Theorem G.6, except that now our additive error includes \(R\) increased by a power of \(2\), i.e., from \(O(\epsilon^{-1}\alpha^{-1/2}RR_{w}\log^{3/2}n)\) to \(O(\epsilon^{-1}\alpha^{-1/2}R^{2}R_{w}\log^{3/2}n)\). 

Now we can give our result that can answer Softmax query.

**Theorem J.8** (Softmax query, formal version of Theorem 4.2).: _Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(\Gamma_{R,s}\) be defined in Definition J.3. Let accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). There is a data structure DPTreeSoftmax (Algorithm 2) that uses \(O(nr)\) spaces to solve Softmax query problem for dataset \(X\subset[0,R]^{d}\) and support the following operations:_

* \(\textsc{Init}(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1),\epsilon_{ s}\in(0,0.1))\)_. (Algorithm 2) It takes_ \(O(nr)\) _time to initialize the data structure._
* DistanceQuery(\(y\in[0,R]^{d},\alpha\in(0,1)\))_. (Algorithm 2) It takes_ \(O(\alpha^{-1}r\log^{2}n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta+\delta^{\prime})\)_-DP private, which computes_ \(w^{\top}\exp(Xy/d)\)_,_
* \(|z-w^{\top}\exp(Xy/d)|\leq(\alpha+\epsilon_{s})\cdot w^{\top}\exp(Xy/d)\)__ \(+\)__\(O(\epsilon^{-1}\alpha^{-1/2}\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(1/\delta^{\prime})} \cdot\log^{3/2}n)\)_,_
_it holds with probability_ \(0.99\)_._

Proof.: The DP proof is the same as the proof of Lemma H.1.

We then show the time complexity. From Lemma J.6, we know that constructing \(P(\cdot)\) requires \(O(r)\) time. In the first for loop of Init, the dominating time consumption is \(O(nr)\). The second for loop also has a time complexity of \(O(nr)\). Therefore, the total time complexity for Init is \(O(nr)\). In the DistanceQuery function, constructing \(P(y)\) takes \(O(r)\) time. Within the for loop, it requires \(O(\alpha^{-1}r\log^{2}n)\). Thus, the total time complexity for DistanceQuery is \(O(\alpha^{-1}r\log^{2}n)\).

The space complexity is \(O(nr)\), since storing the \(n\times r\) matrix \(P\) is the dominating factor.

The proof of the error follows from the triangle inequality by combining the errors in Lemma J.6 and Theorem J.7. 

### Adaptive Softmax

In this section, we show how to make Algorithm 2 robust to adaptive query. We follow the same idea from Section I. We notice that, in the Softmax activation, we have query function \(Z(y):=w^{\top}\exp(Xy/d)\) different from the \(\ell_{1}\)-distance in Section I. Therefore, we need to re-calculate Lipschitz constant first.

**Lemma J.9** (Lipschitz of weighted Softmax).: _If the following conditions hold:_

* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let_ \(Z(y):=w^{\top}\exp(Xy/d)\)_._
* _Let_ \(L=nd^{-1/2}RR_{w}\exp(R^{2})\)_._

_Then, we have \(Z(y)\) is \(L\)-Lipschitz (note that we have \(\ell_{1}\) Lipschitz here)._

Proof.: We can show

\[|Z(y)-Z(\widetilde{y})| =|\sum_{i\in[n]}w_{i}\exp(x_{i}^{\top}y/d)-\sum_{i\in[n]}w_{i}\exp (x_{i}^{\top}\widetilde{y}/d)|\] \[\leq\sum_{i\in[n]}|w_{i}|\cdot|\exp(x_{i}^{\top}y/d)-\exp(x_{i}^{ \top}\widetilde{y}/d)|\] \[\leq\sum_{i\in[n]}|w_{i}|\exp(R^{2})|x_{i}^{\top}y/d-x_{i}^{\top} \widetilde{y}/d|\] \[\leq\sum_{i\in[n]}|w_{i}|\exp(R^{2})\|x_{i}\|_{2}\cdot\|y- \widetilde{y}\|_{2}/d\] \[\leq nR_{w}\exp(R^{2})\sqrt{d}R\cdot\|y-\widetilde{y}\|_{2}/d\] \[\leq nd^{-1/2}RR_{w}\exp(R^{2})\|y-\widetilde{y}\|_{1}\]

where the first step follows from definition of \(Z(y),Z(\widetilde{y})\), the second step follows from triangular inequality, the third step follows from Fact C.4, the fourth step follows from Cauchy-Schwarz inequality \(|u^{\top}v|\leq\|u\|_{2}\cdot\|v\|_{2}\) for \(u,v\in\mathbb{R}^{d}\), the fifth step follows from \(w_{i}\in[-R_{w},R_{w}]\) and \(x_{i}\in[0,R]^{d}\), and the last step follows from \(\|u\|_{2}\leq\|u\|_{1}\) for \(u\in\mathbb{R}^{d}\). 

Then we can show how to extend our algorithm to be robust to adaptive query.

**Lemma J.10** (Adaptive Softmax, formal version of Lemma D.8).: _If the following conditions hold:_

* _Let_ \(N\) _be the_ \(\ell_{\infty}\)__\(\epsilon_{0}\)_-net of_ \(\mathcal{B}\)_, and_ \(|N|\) _be the size of net_ \(N\)_._
* _Let data set_ \(X\in[0,R]^{n\times d}\)_, weights_ \(w\in[-R_{w},R_{w}]^{n}\)_, query_ \(y\in[0,R]^{d}\)_._
* _Let relative error parameter_ \(\alpha\in(0,1)\)_, the failure probability_ \(p_{f}\in(0,0.01)\)* _We create_ \(l=O(\log((R/\epsilon_{0})^{r}/p_{f}))\) _independent copies of data structure_ \(\{\textsc{DPTreeSoftmax}_{j}\}_{j=1}^{l}\) _(Algorithm_ 2_) and take the median of the outputs with each data structure instantiated with_ \((\epsilon/l,(\delta+\delta^{\prime})/l)\)_-DP._
* _Let_ \(f(y):=\operatorname{Median}(\{\textsc{DPTreeSoftmax}_{j}.\textsc{DistanceQuery}(y, \alpha)\}_{j=1}^{l})\)_._
* _Let_ \(Z(y):=w^{\top}\exp(Xy/d)\)_, where_ \(Z(y)\) _is L-Lipschitz with_ \(L=nd^{-1/2}RR_{w}\exp(R^{2})\)_._
* _Let_ \(B=O(\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^{ \prime})}\cdot\log^{3/2}n)\)_._

_Then with probability \(1-p_{f}\), for all query points \(q\in\mathcal{B}\), there exists a point \(y\in N\) which is the closest to \(q\), we can have the process of outputting the median of \(l\) responses is \((\epsilon,\delta+\delta^{\prime})\)-DP and the error satisfies_

\[|f(y)-Z(q)|\leq(\alpha+\epsilon_{s})Z(q)+B+2n\sqrt{d}RR_{w}\exp(R^{2})\epsilon _{0}.\]

Proof.: The proof follows from the same idea as the proof of Lemma I.8, except that we use Theorem J.8 and the Lipschitz in Lemma J.9. 

**Theorem J.11** (Adaptive query Softmax data structure, formal version of Theorem 4.4).: _Let \(R\geq 1\). Let \(r\leq\binom{2s+2d}{2s}\) and \(s=O(\max\{\frac{\log(1/\epsilon_{s})}{\log(\log(1/\epsilon_{s})/R)},R^{2}\})\). Let \(\Gamma_{R,s}\) be defined in Definition J.3. Let accuracy parameter be \(\epsilon_{s}\in(0,0.1)\). Let \(X\in[0,R]^{n\times d}\) be the dataset, \(w\in[-R_{w},R_{w}]^{n}\) be weights, \(y\in[0,R]^{d}\) be the query, \(\alpha\in(0,1)\) be the relative error parameter, and \(p_{f}\) be the failure probability parameter. Let \(l=O(r\log(dR/(\epsilon_{s}p_{f})))\). There is a data structure \(\textsc{DPTreeSoftmaxAdaptive}\) (Algorithm 1) that uses \(O(lnr)\) spaces to solve weighted Softmax query problem for dataset \(X\subset[0,R]^{d}\) and support the following operations:_

* \(\textsc{Init}(X\subset[0,R]^{d},n\in\mathbb{N}_{+},w\in[-R_{w},R_{w}]^{n}, \epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in(0,1),c\in(0,0.1),\epsilon_ {s}\in(0,0.1),p_{f}\in(0,0.01))\)_. (Algorithm_ 1_) It takes_ \(O(lnd)\) _time to initialize the data structure._
* \(\textsc{DistanceQuery}(y\in[0,R]^{d},\alpha\in(0,1))\)_. (Algorithm_ 1_) It takes_ \(O(\alpha^{-1}ld\log^{2}n)\) _time to output a number_ \(z\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta+\delta^{\prime})\)_-DP private, which computes_ \(w^{\top}\exp(Xy/d)\)_,_
* \(|z-w^{\top}\exp(Xy/d)|\leq(\alpha+\epsilon_{s})\cdot w^{\top}\exp(Xy/d)\)__
* \(O(\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^{ \prime})}\cdot\log^{3/2}n)\)_,_
* _it holds with probability_ \(1-p_{f}\) _(where_ \(p_{f}\) _is used in_ \(l\)_),_
* _it is robust to adaptive query._

Proof.: We only need to show how to pick \(\epsilon_{0}\) in the parameter \(l\), because everything else is the same as Lemma J.10. We know the additive error introduced by adaptive query is \(E_{a}:=O(n\sqrt{d}RR_{w}\exp(R^{2})\epsilon_{0})\) and the relative error introduced by polynomial kernel approximation is \(E_{p}:=w^{\top}\exp(Xy/d)\cdot\epsilon_{s}\). It can be shown that:

\[E_{p} :=w^{\top}\exp(Xy/d)\cdot\epsilon_{s}\] \[\leq\epsilon_{s}\|w\|_{2}\cdot\|\exp(Xy/d)\|_{2}\] \[=O(nR_{w}\epsilon_{s}\exp(R^{2}))\]

where the first step follows from definition of \(E_{p}\), the second step follows from Cauchy-Schwarz inequality, and the last step follows from \(w\in[-R_{w},R_{w}]^{n}\), \(X\in[0,R]^{n\times d}\), and \(y\in[0,R]^{d}\).

Picking \(\epsilon_{0}=\Theta(\frac{\epsilon_{s}}{\sqrt{d}R})\), we can hide the error of adaptive query \(E_{a}\) in \(E_{p}\). Thus, we have

\[l =O(\log((R/\epsilon_{0})^{r}/p_{f}))\] \[=O(\log((\sqrt{d}R^{2}/\epsilon_{s})^{r}/p_{f}))\] \[=O(r\log(dR/(\epsilon_{s}p_{f})))\]where the first step comes from the definition of \(l\), the second step comes from picking \(\epsilon_{0}=\Theta(\frac{\epsilon_{s}}{\sqrt{d}R})\), and the last step follows from \(\log(a^{d}/b)=O(d\log(a/b))\) for any \(a>1,0<b<1,d>1\). 

### Proof of Main Result

In this section, we give the proof of our main result of Theorem 3.1.

**Theorem J.12** (Softmax cross-attention, formal version of Theorem 3.1).: _Let \(Q,K,V,\mathrm{Attn}\) be defined in Definition 1.1. Let \(\alpha\in(0,1)\) be the relative error parameter and \(p_{f}\) be the probability of failure parameter. Let \(r,s,\epsilon_{s}\) be parameters of polynomial kernel methods (Lemma D.7). Let \(\Gamma_{R,s}:=\max_{j\in[s]}\frac{R^{j}}{\sqrt{j!}}\) (Definition J.3). Let \(l=O(r\log(dR/(\epsilon_{s}p_{f})))\). There is a data structure \(\textsc{DPTreeSoftmaxAdaptive}\) (Algorithm 1) that uses \(O(lnrd)\) spaces to make cross-attention DP and supports the following operations:_

* _We initialize_ \(d\) _data structures using_ \(\textsc{Init}(K,n,V_{*,k},\epsilon\in(0,1),\delta\in(0,1),\delta^{\prime}\in( 0,1),c\in(0,0.1),\epsilon_{s}\in(0,0.1),p_{f}\in(0,0.01))\) _(Algorithm_ 1_), for_ \(k\in[d]\)_. It takes_ \(O(lnr)\) _time to initialize one data structure._
* _At query time, for user input_ \(Q\)_, we process one token at a time by passing the_ \(i\)_-th row of_ \(Q\)_, denoted_ \(Q_{i}\in\mathbb{R}^{d}\)_, to_ \(\textsc{DistanceQuery}(Q_{i},\alpha\in(0,1))\) _(Algorithm_ 1_) for each_ \(i\in[m]\)_. It takes_ \(O(\alpha^{-1}lr\log^{2}n)\) _time to output an entry_ \(z\) _in_ \(\mathrm{Attn}(Q,K,V)\) _such that_
* _the process of output_ \(z\) _satisfies_ \((\epsilon,\delta+\delta^{\prime})\)_-DP,_
* _the process of output_ \(z\) _has relative error_ \(n^{-1}(\alpha+\epsilon_{s})\)_,_
* _the process of output_ \(z\) _has additive error_ \(O(n^{-1}\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log(l/\delta^ {\prime})}\cdot\log^{3/2}n)\)_,_
* _it holds with probability_ \(1-p_{f}\) _(where_ \(p_{f}\) _is used in_ \(l\)_),_
* _it is robust to adaptive query._

Proof.: From Section 3, we know that we can ensure matrix \(AV\) in cross-attention computation satisfies DP. Next, from Theorem 4.4, for \(i\in[m],j\in[n],k\in[d]\), we have \((AV)_{i,k}\) is \((\epsilon,\delta+\delta^{\prime})\)-DP and also robust to adaptive query. For \(D\), we compute the exact true value. By the post-processing property of DP (Fact C.5), our cross-attention process is DP.

Let \((AV)_{i,k}\) be the true value and \(\widetilde{(AV)}_{i,k}\) be the noisy value. Then, from Theorem 4.4, we have

\[|(AV)_{i,k}-\widetilde{(AV)}_{i,k}|\leq(\alpha+\epsilon_{s}) \cdot(AV)_{i,k}+O(\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r\sqrt{\log (l/\delta^{\prime})}\cdot\log^{3/2}n).\] (11)

For \(D_{i,i}\), we can show

\[D_{i,i}=(A\cdot\mathbf{1}_{n})_{i}=\sum_{j=1}^{n}\exp(\langle Q _{i},K_{j}\rangle/d)\geq n\] (12)

because \(\langle Q_{i},K_{j}\rangle\geq 0\) for bounded \(Q,K\).

Finally, we can show the error of one entry is bounded by

\[|(D^{-1}AV)_{i,k}-(D^{-1}\widetilde{AV})_{i,k}| =|D^{-1}_{i,i}((AV)_{i,k}-\widetilde{(AV)}_{i,k})|\] \[=|D^{-1}_{i,i}|\cdot|((AV)_{i,k}-\widetilde{(AV)}_{i,k})|\] \[\leq n^{-1}(\alpha+\epsilon_{s})\cdot(AV)_{i,k}\] \[\quad+O(n^{-1}\epsilon^{-1}\alpha^{-1/2}l\Gamma_{R,s}^{2}R_{w}r \sqrt{\log(l/\delta^{\prime})}\cdot\log^{3/2}(n))\]

where the first step follows from definition, the second step follows from simple algebra, and the last step follows from Eq.(11) and (12).