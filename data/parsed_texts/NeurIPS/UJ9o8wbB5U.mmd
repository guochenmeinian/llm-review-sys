# Provably Safe Reinforcement Learning with Step-wise Violation Constraints

 Nuoya Xiong\({}^{1,}\), Yihan Du\({}^{2,\dagger}\), Longbo Huang\({}^{1,\ddagger,}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)University of Illinois at Urbana-Champaign

\({}^{*}\)nuoyaxiong@gmail.com

\({}^{\dagger}\)yihandu@illinois.edu

\({}^{\ddagger}\)longbohuang@tsinghua.edu.cn

Corresponding author.

###### Abstract

We investigate a novel safe reinforcement learning problem with step-wise violation constraints. Our problem differs from existing works in that we focus on stricter step-wise violation constraints and do not assume the existence of safe actions, making our formulation more suitable for safety-critical applications that need to ensure safety in all decision steps but may not always possess safe actions, e.g., robot control and autonomous driving. We propose an efficient algorithm SUCBVI, which guarantees an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) or gap-dependent \(\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\) step-wise violation and an \(\widetilde{\mathcal{O}}(\sqrt{H^{3}SAT})\) regret. Lower bounds are provided to validate the optimality in both violation and regret performance with respect to the number of states \(S\) and the total number of steps \(T\). Moreover, we further study an innovative safe reward-free exploration problem with step-wise violation constraints. For this problem, we design the algorithm SRF-UCRL to find a near-optimal safe policy, which achieves a nearly state-of-the-art sample complexity \(\widetilde{\mathcal{O}}((\frac{S^{2}AH^{2}}{\varepsilon}+\frac{H^{4}SA}{ \varepsilon^{2}})(\log(\frac{1}{\delta})+S))\), and guarantees an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) violation during exploration. Experimental results demonstrate the superiority of our algorithms in safety performance and corroborate our theoretical results.

## 1 Introduction

In recent years, reinforcement learning (RL) (Sutton and Barto, 2018) has become a powerful framework for decision-making and learning in unknown environments. Despite the ground-breaking success of RL in games (Lanctot et al., 2019), recommendation systems (Afsar et al., 2022) and complex tasks in simulation environments (Zhao et al., 2020), most existing RL algorithms focus on optimizing the cumulative reward and do not take into consideration the risk aspect, e.g., the agent runs into catastrophic situations during control. The lack of strong safety guarantees hinders the application of RL to broader safety-critical scenarios such as autonomous driving, robotics and healthcare. For example, for robotic control in complex environments, it is crucial to prevent the robot from getting into dangerous situations, e.g., hitting walls or falling into water pools, at all times.

To handle the safety requirement, a common approach is to formulate safety as a long-term expected violation constraint in each episode. This approach focuses on seeking a policy whose cumulative expected violation in each episode is below a certain threshold. However, for applications where an agent needs to avoid disastrous situations throughout the decision process, e.g., a robot needs to avoid hitting obstacles at each step, merely reducing the long-term expected violation is not sufficient to guarantee safety.

Motivated by this fact, we investigate safe reinforcement learning with a more fine-grained constraint, called _step-wise_ violation constraint, which aggregates all nonnegative violations at each step (no offset between positive and negative violations permitted). We name this problem Safe-RL-SW. Our step-wise violation constraint differs from prior expected violation constraint (Wachi and Sui, 2020; Efroni et al., 2020; Kalagaria et al., 2021) in two aspects: (i) Minimizing the step-wise violation enables the agent to learn an optimal policy that avoids unsafe regions deterministically, while reducing the expected violation only guarantees to find a policy with low expected violation, instead of a per-step zero-violation policy. (ii) Reducing the aggregated nonnegative violation allows us to have a risk control for each step, while a small cumulative expected violation can still result in a large cost at some individual step and cause danger, if other steps with smaller costs offset the huge cost.

Our problem faces two unique challenges. First, the step-wise violation requires us to guarantee a small violation at each step, which demands very different algorithm design and analysis from that for the expected violation (Wachi and Sui, 2020; Efroni et al., 2020; Kalagaria et al., 2021). Second, in safety-critical scenarios, the agent needs to identify not only unsafe states but also potentially unsafe states, which are states that may appear to be safe but will ultimately lead to unsafe regions with a non-zero probability. For example, a self-driving car needs to learn to slow down or change directions early, foreseeing the potential danger in advance in order to ensure safe driving (Thomas et al., 2021). Existing safe RL works focus mainly on the expected violation (Wachi and Sui, 2020; Liu et al., 2021; Wei et al., 2022), or requiring some other assumptions such as imposing the prior knowledge of a safe action for each state (Amani et al., 2021). Moreover, many previous works also require strong assumptions that exclude discrete tabular MDP (Amani et al., 2021; Wachi et al., 2021; Wang et al., 2023) which is considered in our paper. Hence, techniques in previous works cannot be applied to handle step-wise violations. More detailed comparisons are provided in Section 2.

To systematically handle these two challenges, we formulate safety as an unknown cost function for each state without assuming safe actions, and consider minimizing the step-wise violation instead of the expected violation. We propose a general algorithmic framework called **S**afe **UCBVI** (SUCBVI). Specifically, in each episode, we first estimate the transition kernel and cost function in an optimistic manner, tending to regard a state as safe at the beginning of learning. After that, we introduce novel dynamic programming to identify potentially unsafe states and determine safe actions, based on our estimated transition and costs. Finally, we employ the identified safe actions to conduct value iteration. This mechanism can adaptively update dangerous regions, and help the agent plan for the future, which keeps her away from all states that may lead to unsafe states. As our estimation becomes more accurate over time, the safety violation becomes smaller and eventually converges to zero. Note that without the assumption of safe actions, the agent knows nothing about the environment at the beginning. Thus, it is impossible for her to achieve an absolute zero violation. Nevertheless, we show that SUCBVI can achieve a sub-linear \(\widetilde{\mathcal{O}}(\sqrt{ST})\) cumulative violation or an \(\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\) gap-dependent violation that is independent of \(T\). This violation implies that as the RL game proceeds, the agent eventually learns how to avoid unsafe states. We also provide a matching lower bound to demonstrate the optimality of SUCBVI in both violation and regret.

Furthermore, we apply our step-wise safe RL framework to the reward-free exploration (Jin et al., 2020) setting. In this novel safe reward-free exploration, the agent needs to guarantee small step-wise violations during exploration, and also output a near-optimal safe policy. Our algorithm achieves \(\varepsilon\) cumulative step-wise violation during exploration, and also identifies a \(\varepsilon\)-optimal and safe policy. See the definition of \(\varepsilon\)-optimal and \(\varepsilon\)-safe policy in Section 6.1. Another interesting application of our framework is safe zero-sum Markov games, which we discuss in Appendix B.

The main contributions of our paper are as follows.

* We formulate the safe RL with step-wise violation constraint problem (Safe-RL-SW), which models safety as a cost function for states and aims to minimize the cumulative step-wise violation. Our formulation is particularly useful for safety-critical applications where avoiding disastrous situations at each decision step is desirable, e.g., autonomous driving and robotics.
* We provide a general algorithmic framework SUCBVI, which is equipped with an innovative dynamic programming to identify potentially unsafe states and distinguish safe actions. We establish an \(\widetilde{\mathcal{O}}(\sqrt{H^{3}SAT})\) regret and an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) or \(\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\) gap-dependent violation guarantees, which exhibits the capability of SUCBVI in attaining high rewards while maintaining small violation.

* We further establish an \(\Omega(\sqrt{HST})\) regret and an \(\Omega(\sqrt{ST})\) violation lower bounds for Safe-RL-SW. The lower bounds demonstrate the optimality of algorithm SUCBVI in both regret minimization and safety guarantee, with respect to factors \(S\) and \(T\).
* We consider step-wise safety constraints in the reward-free exploration setting, which is called the Safe-RFE-SW problem. In this setting, we design an efficient algorithm SRF-UCRL, which ensures \(\varepsilon\) step-wise violations during exploration and plans a \(\varepsilon\)-optimal and \(\varepsilon\)-safe policy for any reward functions with probability at least \(1-\delta\). We obtain an \(\widetilde{\mathcal{O}}((\frac{S^{2}AH^{2}}{\varepsilon}+\frac{H^{4}S,A}{ \varepsilon^{2}})(\log(\frac{1}{\delta})+S))\) sample complexity and an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) violation guarantee for SRF-UCRL, which shows the efficiency of SRF-UCRL in sampling and danger avoidance even without reward signals. To the best of our knowledge, this work is the first to study the step-wise violation constraint in the RFE setting.

## 2 Related Work

Safe RL.Safety is an important topic in RL, which has been extensively studied. The constrained Markov decision process (CMDP)-based approaches handle safety via cost functions, and aim to minimize the expected episode-wise violation, e.g., (Yu et al., 2019; Wachi and Sui, 2020; Qiu et al., 2020; Efroni et al., 2020; Turchetta et al., 2020; Ding et al., 2020; Singh et al., 2020; Kalagarla et al., 2021; Simao et al., 2021; Ding et al., 2021), or achieve zero episode-wise violation, e.g., (Liu et al., 2021; Bura et al., 2021; Wei et al., 2022; Sootla et al., 2022). Apart from CMDP-based approaches, there are also other works that tackle safe RL by control-based approaches (Berkenkamp et al., 2017; Chow et al., 2018; Dalal et al., 2018; Wang et al., 2022), policy optimization (Uchibe and Doya, 2007; Achiam et al., 2017; Tessler et al., 2018; Liu et al., 2020; Stooke et al., 2020) and safety shields (Alshiekh et al., 2018).

In recent years, there are also some works studying step-wise violations with additional assumptions. Now we provide detailed comparisons between existing papers with instantaneous constraints. Turchetta et al. (2016); Wachi et al. (2018) propose a GP-based algorithm, which assumes that the transition is deterministic and known, while modeling the reward and cost functions using Gaussian Processes. By using this particular structure, they can infer the safety cost by estimating the parameters. Wachi et al. (2021) assumes the reward and cost functions have a generalized linear structure. Their algorithm explores in a safe space until a time \(t^{*}\) when the agent explores sufficiently. However, the upper bound of the exploring time \(t^{*}\) is not given in their paper. In fact, under the tabular MDP setting \(t^{*}\) can be infinite.

Amani et al. (2021) further considers the reward and cost functions to have a linear structure. It makes two assumptions: (a) There exists a safe action in each state, which prevents the agent from going to a potentially unsafe state. (b) The feature set is a star convex set, which helps them change actions continuously. However, this makes their works infeasible in tabular MDPs: The feature set in tabular MDPs consists of one-hot vectors and is not a star convex set. Hence, the work Amani et al. (2021) cannot solve our problem. Shi et al. (2023) considers safe RL in linear mixture MDPs. Their work also contains assumption (b), making it infeasible in tabular MDPs. Moreover, although they do not have assumption (a), it assumes the transition set \(\Delta(s,a)\) is known, which is not needed in our paper. Thus our paper is more challenging since we need to estimate \(\Delta(s,a)\) in our algorithm adaptively.

There are some other papers investigating the safety of RL problems. Alshiekh et al. (2018) represents the safe state by the reactive system, and uses shielding to calculate and restrict the agent within a safe trajectory completely. The main difference between their work and our work is that we need to dynamically update the estimated safe state, while they require to know the mechanism and state to calculate the shield. Dalal et al. (2018) considers restricting the safe action by projecting the action into the closest safe actions. They achieve this goal by solving a convex optimization problem on the continuous action set. However, in their paper, they do not consider the situation where a state can have no safe actions. To be more specific, they do not consider the situation when the convex optimization problem has no solutions. Le et al. (2019) considers the decision-making problem with a pre-collected dataset. Turchetta et al. (2020) and Sootla et al. (2022) both consider cumulative cost constraints rather than step-wise constraints. The former uses a teacher for intervention to keep the agent away from the unsafe region, while the latter encourages safe exploration by augmenting a safety state to measure safety during training.

Reward-free Exploration with Safety.Motivated by sparse reward signals in realistic applications, reward-free exploration (RFE) has been proposed in Jin et al. (2020), and further developedin Kaufmann et al. (2021); Menard et al. (2021). In RFE, the agent explores the environment without reward signals. After enough exploration, the reward function is given, and the agent needs to plan a near-optimal policy based on his knowledge collected in exploration. Safety is also important in RFE: We need to not only guarantee that our outputted policy is safe, but also ensure small violations during exploration.

Recently, Miryoosefi and Jin (2022); Huang et al. (2022) also study RFE with safety constraints. Compared to our paper, Miryoosefi and Jin (2022) only considers the safety constraint after the exploration phase. Huang et al. (2022) differs from our work in the following aspects: (i) They allow different reward functions during exploration and after exploration, and the agent can directly know the true costs during training. In our work, the cost function is the same during and after exploration, but the agent can only observe the noisy costs of a state when she arrives at that state. (ii) They require prior knowledge of safe baseline policies, while we do not need such an assumption. (iii) They consider the zero-expected violation during exploration, while we focus on keeping small step-wise violations.

## 3 The Safe MDP Model

**Episodic MDP.** In this paper, we consider the finite-horizon episodic Markov Decision Process (MDP), represented by a tuple \((\mathcal{S},\mathcal{A},H,\mathbb{P},r)\). Here \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, and \(H\) is the length of each episode. \(\mathbb{P}=\{\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\mapsto\triangle_{ \mathcal{S}}\}_{h\in[H]}\) is the transition kernel, and \(\mathbb{P}_{h}(s^{\prime}|s,a)\) gives the transition probability from \((s,a)\) to \(s^{\prime}\) at step \(h\). \(r=\{r_{h}:\mathcal{S}\times\mathcal{A}\mapsto[0,1]\}_{h\in[H]}\) is the reward function, and \(r_{h}(s,a)\) gives the reward of taking action \(a\) in state \(s\) at step \(h\). A policy \(\pi=\{\pi_{h}:\mathcal{S}\mapsto\mathcal{A}\}_{h\in[H]}\) consists of \(H\) mappings from the state space to action space. In each episode \(k\), the agent first chooses a policy \(\pi^{k}\). At each step \(h\in[H]\), the agent observes a state \(s^{k}_{h}\), takes an action \(a^{k}_{h}\), and then goes to a next state \(s^{k}_{h+1}\) with probability \(\mathbb{P}_{h}(s^{k}_{h+1}\mid s^{k}_{h},a^{k}_{h})\). The algorithm executes \(T=HK\) steps. Moreover, the state value function \(V^{\pi}_{h}(s,a)\) and state-action value function \(Q^{\pi}_{h}(s,a)\) for a policy \(\pi\) can be defined as

\[V^{\pi}_{h}(s) :=\mathbb{E}_{\pi}\Bigg{[}\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s _{h^{\prime}},\pi_{h^{\prime}}(s_{h^{\prime}}))\Bigg{|}s_{h}=s\Bigg{]},\] \[Q^{\pi}_{h}(s,a) :=\mathbb{E}_{\pi}\Bigg{[}\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}( s_{h^{\prime}},\pi_{h^{\prime}}(s_{h^{\prime}}))\Bigg{|}s_{h}=s,a_{h}=a\Bigg{]}.\]

**Safety Constraint.** To model unsafe regions in the environment, similar to Wachi and Sui (2020); Yu et al. (2022), we define a safety cost function \(c:\mathcal{S}\mapsto[0,1]\). Let \(\tau\in[0,1]\) denote the safety threshold. A state is called _safe_ if \(c(s)\leq\tau\), and called _unsafe_ if \(c(s)>\tau\). Similar to Efroni et al. (2020); Amani et al. (2021), when the agent arrives in a state \(s\), she will receive a cost signal \(z(s)=c(s)+\zeta\), where \(\zeta\) is an independent, zero-mean and 1-sub-Gaussian noise. Denote \((x)_{+}=\max\{x,0\}\). The violation in state \(s\) is defined as \((c(s)-\tau)_{+}\), and the cumulative step-wise violation till episode \(K\) is

\[C(K)=\sum_{k=1}^{K}\sum_{h=1}^{H}(c(s^{k}_{h})-\tau)_{+}.\] (1)

Eq. (1) represents the accumulated step-wise violation during training. When the agent arrives in state \(s^{k}_{h}\) at step \(h\) in episode \(k\), she will suffer violation \((c(s^{k}_{h})-\tau)_{+}\). This violation setting is significantly different from the previous CMDP setting (Qiu et al., 2020; Ding et al., 2020; Efroni et al., 2020; Ding et al., 2021; Wachi and Sui, 2020; Liu et al., 2021; Kalagarla et al., 2021). They study the _episode-wise expected violation_\(C^{\prime}(K)=\sum_{k=1}^{K}\left(\mathbb{E}\left[\sum_{h=1}^{H}c(s^{k}_{h},a^{k }_{h})\right]-\mu\right).\) There are also some papers (Efroni et al., 2020; Simao et al., 2021) considering a stricter constraint named _episode-wise clipped expected violation_: \(C^{\prime\prime}(K)=\sum_{k=1}^{K}\left(\mathbb{E}\left[\sum_{h=1}^{H}c(s^{k}_{h },a^{k}_{h})\right]-\mu\right)_{+}.\) Compared to the episode-wise violation (including expected violation and clipped expected violation), our step-wise violation has two main differences: (i) First, the episode-wise violation constraints allow the agent to get into unsafe states occasionally. Instead, the step-wise violation constraint forces the agent to stay in safe regions at all times. (ii) Second, in the episode-wise constraints, the average violation \(\mathbb{E}[c(s_{h},a_{h})]-\mu/H\) at step \(h\) is allowed to be positive or negative, and they can cancel out in one episode to achieve \(\mathbb{E}[\sum_{h=1}^{H}c(s_{h}^{k},a_{h}^{k})]\leq\mu\). Instead, we consider a _nonnegative_ function \((c(s)-\tau)_{+}\) at each step in our step-wise violation, which imposes a stricter constraint.

Define \(\mathcal{U}:=\{s\in\mathcal{S}\mid c(s)>\tau\}\) as the set of all _unsafe states_. Let \(\iota=\{s_{1},a_{1},\cdots,s_{H},a_{H}\}\) denote a trajectory. Since a feasible policy needs to satisfy the constraint at each step, we define the set of feasible policies as \(\Pi=\{\pi\mid\Pr\{\exists\;h\in[H],s_{h}\in\mathcal{U}\mid\iota\sim\pi\}=0\}\). The feasible policy set \(\Pi\) consists of all policies under which one never reaches any unsafe state in an episode.

**Learning Objective.** In this paper, we consider the regret minimization objective. Specifically, define \(\pi^{*}=\operatorname*{argmax}_{\pi\in\Pi}V_{1}^{\pi}\), \(V^{*}=V^{\pi^{*}}\) and \(Q^{*}=Q^{\pi^{*}}\). The regret till \(K\) episodes is then defined as

\[R(K)=\sum_{t=1}^{K}(V_{1}^{*}(s_{1})-V_{1}^{\pi^{k}}(s_{1})),\]

where \(\pi^{k}\) is the policy taken in episode \(k\). Our objective is to minimize \(R(K)\) to achieve a good performance, and minimize the violation \(C(K)\) to guarantee the safety at the same time.

## 4 Safe RL with Step-wise Violation Constraints

### Assumptions and Problem Features

Before introducing our algorithms, we first state the important assumptions and problem features for Safe-RL-SW.

Suppose \(\pi\) is a feasible policy. Then, if we arrive at \(s_{H-1}\) at step \(H-1\), \(\pi\) needs to select an action that guarantees \(s_{H}\notin\mathcal{U}\). Define the _transition set_\(\Delta_{h}(s,a)=\{s^{\prime}\mid\mathbb{P}_{h}(s^{\prime}\mid s,a)>0\}\) for any \(\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), which represents the set of possible next states after taking action \(a\) in state \(s\) at step \(h\). Then, at the former step \(H-1\), the state \(s\) is potentially unsafe if it satisfies that \(\Delta_{H-1}(s,a)\cap\mathcal{U}\neq\emptyset\) for all \(a\in\mathcal{A}\). (i.e., no matter taking what action, there is a positive probability of transitioning to an unsafe next state). Therefore, we can recursively define the set of _potentially unsafe states at step \(h\)_ as

\[\mathcal{U}_{h}=\mathcal{U}_{h+1}\cup\{s\mid\;\forall\;a\in\mathcal{A},\Delta _{h}(s,a)\cap\mathcal{U}_{h+1}\neq\emptyset\},\] (2)

where \(\mathcal{U}_{H}=\mathcal{U}\). Intuitively, if we are in a state \(s_{h}\in\mathcal{U}_{h}\) at step \(h\), no action can be taken to completely avoid reaching potentially unsafe states \(s_{h+1}\in\mathcal{U}_{h+1}\) at step \(h+1\). Thus, in order to completely prevent from getting into unsafe states \(\mathcal{U}\) throughout all steps, one needs to avoid potentially unsafe states in \(\mathcal{U}_{h}\) at step \(h\). From the above argument, we have that \(s_{1}\notin\mathcal{U}_{1}\) is equivalent to the existence of feasible policies. The detailed proof is provided in Appendix D. Thus, we make the following necessary assumption.

**Assumption 4.1** (Existence of feasible policies).: The initial state \(s_{1}\) satisfies \(s_{1}\notin\mathcal{U}_{1}\).

For any \(s\in\mathcal{S}\) and \(h\in[H-1]\), we define the set of safe actions for state \(s\) at step \(h\) as

\[A_{h}^{safe}(s)=\{a\in\mathcal{A}\mid\Delta_{h}(s,a)\cap\mathcal{U}_{h+1}= \emptyset\},\] (3)

and let \(A_{H}^{safe}(s)=\mathcal{A}\). \(A_{h}^{safe}(s)\) stands for the set of all actions at step \(h\) which will not lead to potentially unsafe states in \(\mathcal{U}_{h+1}\). Here, \(\{\mathcal{U}_{h}\}_{h\in[H]}\) and \(\{A_{h}^{safe}(s)\}_{h\in[H]}\) are defined by dynamic programming: If we know sets of all possible next state \(\{\Delta_{h}(s,a)\}_{h\in[H]}\) and unsafe state set \(\mathcal{U}=\mathcal{U}_{H}\), we can calculate all potentially unsafe state sets \(\{\mathcal{U}_{h}\}_{h\in[H]}\) and safe action sets \(\{A_{h}^{safe}(s)\}_{h\in[H]}\), and choose feasible policies to completely avoid unsafe states.

### Algorithm SUCBVI

Now we present our main algorithm **S**afe **UCBVI** (SUCBVI), which is based on previous classic RL algorithm UCBVI (Azar et al., 2017), and equipped with a novel dynamic programming to identify potentially unsafe states and safe actions. The pseudo-code is shown in Algorithm 1. First, we provide some intuitions about how SUCBVI works. At each episode, we first estimate all the unsafe states based on the historical data. Then, we perform a dynamic programming procedure introducedin Section 4.1 and calculate the safe action set \(A_{h}^{safe}(s)\) for each state \(s\). Then, we perform value iteration in the estimated safe action set. As the estimation becomes more accurate, SUCBVI will eventually avoid potentially unsafe states and achieve both sublinear regrets and violations.

Now we begin to introduce our algorithm. In the beginning, we initialize \(\Delta_{h}(s,a)=\emptyset\) for all \((h,s,a)\in[H]\times\mathcal{S}\times\mathcal{A}\). It implies that the agent considers all actions to be safe at first, because no action will lead to unsafe states from the agent's perspective. In each episode, we first estimate the empirical cost \(\hat{c}(s)\) based on historical cost feedback \(z(s)\), and regard state \(s\) as safe if \(\bar{c}(s)=\hat{c}(s)-\beta>\tau\) for some bonus term \(\beta\), which aims to guarantee \(\bar{c}(s)\leq c(s)\) (Line 4). Then, we calculate the estimated unsafe state set \(\mathcal{U}_{H}^{k}\), which is a subset of the true unsafe state set \(\mathcal{U}=\mathcal{U}_{H}\) with a high probability by optimism. With \(\mathcal{U}_{H}^{k}\) and \(\Delta_{h}^{k}(s,a)\), we can estimate potentially unsafe state sets \(\mathcal{U}_{h}^{k}\) for all \(h\in[H]\) by Eq. (2) recursively.

Then, we perform value iteration to compute the optimistically estimated optimal policy. Specifically, for any hypothesized safe state \(s\notin\mathcal{U}_{h}^{k}\), we update the greedy policy on the estimated safe actions, i.e., \(\pi_{h}^{k}(s)=\max_{a\in A_{h}^{safe}(s)}Q_{h}^{k}(s,a)\) (Line 13). On the other hand, for any hypothesized unsafe state \(s_{h}\in\mathcal{U}_{h}\), since there is no action that can completely avoid unsafe states, we ignore safety costs and simply update the policy by \(\pi_{h}^{k}(s)=\max_{a\in\mathcal{A}}Q(s,a)\) (Line 15). After that, we calculate the estimated optimal policy \(\pi^{k}\) for episode \(k\), and the agent follows \(\pi^{k}\) and collects a trajectory. Then, we update \(\Delta_{h}^{k+1}(s,a)\) by incorporating the observed state \(s_{h+1}^{k}\) into the set \(\Delta_{h}^{k}(s_{h}^{k},a_{h}^{k})\). Under this updating rule, it holds that \(\Delta_{h}^{k}(s,a)\subseteq\Delta_{h}(s,a)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).

The performance of Algorithm 1 is summarized below in Theorem 4.2.

**Theorem 4.2**.: _Let \(\alpha(n,\delta)=7H\sqrt{\frac{\ln(5SAHK/\delta)}{n}}\) and \(\beta(n,\delta)=\sqrt{\frac{2}{n}\log(SK/\delta)}\). With probability at least \(1-\delta\), the regret and step-wise violation of Algorithm 1 are bounded by_

\[R(K)=\widetilde{\mathcal{O}}(\sqrt{H^{2}SAT}),\qquad C(K)=\widetilde{\mathcal{ O}}(\sqrt{ST}+S^{2}AH^{2}).\]_Moreover, if \(\mathcal{C}_{\mathrm{gap}}\triangleq\min_{s\in\mathcal{U}}(c(s)-\tau)_{+}>0\), we have \(C(K)=\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\)._

Theorem 4.2 shows that SUCBVI achieves both sublinear regret and violation. Moreover, when all the unsafe states have a large cost compared to the safety threshold \(\tau\), i.e., \(\mathcal{C}_{\mathrm{gap}}=\min_{s\in\mathcal{U}}(c(s)-\tau)_{+}>0\) is a constant, we can distinguish the unsafe states easily and get a constant violation. In particular, when \(\tau=1\), Safe-RL-SW degenerates to the unconstrained MDP and Theorem 4.2 maintains the same regret \(\widetilde{\mathcal{O}}(\sqrt{H^{2}SAT})\) as UCBVI-CH (Azar et al., 2017), while CMDP algorithms (Efroni et al., 2020; Liu et al., 2021) suffer a larger \(\widetilde{\mathcal{O}}(\sqrt{H^{3}S^{3}AT})\) regret.

We provide the analysis idea of Theorem 4.2 here. First, by the updating rule of \(\Delta_{h}^{k}(s,a)\), we show that \(\mathcal{U}_{h}^{k}\) and \(A_{h}^{k,safe}(s)\) have the following crucial properties: \(\mathcal{U}_{h}^{k}\subseteq\mathcal{U}_{h},\ \ A_{h}^{safe}(s)\subseteq A_{h}^{k,safe}(s)\). Based on this property, we can prove that \(Q_{h}^{*}(s,a)\leq Q_{h}^{k}(s,a)\) and \(V_{h}^{*}(s)\leq V_{h}^{k}(s)\) for all \((s,a)\), and then apply the regret decomposition techniques to derive the regret bound.

Recall that \(\pi^{k}\) is a feasible policy with respect to the estimated unsafe state set \(\mathcal{U}_{H}^{k}\) and transition set \(\{\Delta_{h}^{k}(s,a)\}_{h\in[H]}\). If the agent takes policy \(\pi^{k}\) and the transition follows \(\Delta_{h}^{k}(s,a)\), i.e., \(s_{h+1}^{k}\in\Delta_{h}^{k}(s,a)\), the agent never arrive any estimated unsafe state \(s\in\mathcal{U}_{H}^{k}\) in this episode. Hence the agent will suffer at most an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) step-wise violation or an \(\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\) gap-dependent bounded violation. Yet, the situation \(s_{h+1}\in\Delta_{h}^{k}(s,a)\) does not always hold for all \(h\in[H]\). In the case when \(s_{h+1}^{k}\notin\Delta_{h}^{k}(s,a)\), we add the newly observed state \(s_{h+1}^{k}\) into \(\Delta_{h}^{k+1}(s_{h}^{k},a_{h}^{k})\) (Line 21). We can show that this case appears at most \(S^{2}AH\) times, and thus incurs \(O(S^{2}AH^{2})\) additional violation. Combining the above two cases, the total violation can be upper bounded.

## 5 Lower Bounds for Safe-RL-SW

In this section, we provide a matching lower bound for Safe-RL-SW in Section 4. The lower bound shows that if an algorithm always achieves a sublinear regret in Safe-RL-SW, it must incur an \(\Omega(\sqrt{ST})\) violation. This result matches our upper bound in Theorem 4.2, showing that SUCBVI achieves the optimal violation performance.

**Theorem 5.1**.: _If an algorithm has an expected regret \(\mathbb{E}_{\pi}[R(K)]\leq\frac{HK}{24}\) for all MDP instances, there exists an MDP instance in which the algorithm suffers expected violation \(\mathbb{E}_{\pi}[C(K)]=\Omega(\sqrt{ST})\)._

Now we validate the optimality in terms of regret. Note that if we do not consider safety constraints, the lower bound for classic RL (Osband and Van Roy, 2016) can be applied to our setting. Thus, we also have an \(\Omega(\sqrt{T})\) regret lower bound. To understand the essential hardness brought by safety constraints, we further investigate whether safety constraints will lead to an \(\Omega(\sqrt{T})\) regret, given that we can achieve an \(o(\sqrt{T})\) regret on some good instances without the safety constraints.

**Theorem 5.2**.: _For any \(\alpha\in(0,1)\), there exists a parameter \(n\) and \(n\) MDPs \(M_{1},\ldots,M_{n}\) satisfying that:_

_1. If we do not consider any constraint, there is an algorithm that achieves an \(\widetilde{\mathcal{O}}(T^{(1-\alpha)/2})\) regret compared to the unconstrained optimal policy on all MDPs._

_2. If we consider the safety constraint, any algorithm with a \(O(T^{1-\alpha})\) expected violation will achieve an \(\Omega(\sqrt{HST})\) regret compared to the constrained optimal policy on one of MDPs._

Intuitively, Theorem 5.2 shows that if one achieves sublinear violation, she must suffer at least an \(\Omega(\sqrt{T})\) regret even if she can achieve an \(o(\sqrt{T})\) regret without considering constraints. This theorem demonstrates the hardness particularly brought by the step-wise constraint, and corroborates the optimality of our results. Combining with Theorem 5.1, the two lower bounds show an essential trade-off between the violation and performance.

## 6 Safe Reward-Free Exploration with Step-wise Violation Constraints

### Formulation of Safe-RFE-SW

In this section, we consider Safe RL in the reward-free exploration (RFE) setting (Jin et al., 2020; Kaufmann et al., 2021; Menard et al., 2021) called Safe-RFE-SW, to show the generality of our proposed framework. In the RFE setting, the agent does not have access to reward signals and only receives random safety cost feedback \(z(s)=c(s)+\zeta\). To impose safety requirements, Safe-RFE-SW requests the agent to keep small safety violations during exploration, and outputs a near-optimal safe policy after receiving the reward function.

**Definition 6.1** (\((\varepsilon,\delta)\)-optimal safe algorithm for Safe-RFE-SW).: An algorithm is \((\varepsilon,\delta)\)-optimal safe for Safe-RFE-SW if it outputs the triple \((\hat{\mathbb{P}},\hat{\Delta}(s,a),\hat{\mathcal{U}}_{H})\) such that for any reward function \(r\), with probability at least \(1-\delta\),

\[V_{1}^{*}(s_{1};r)-V_{1}^{\hat{\pi}^{*}}(s_{1};r)\leq\varepsilon,\qquad\mathbb{ E}_{\pi}\left[\sum_{h=1}^{H}(c(s_{h})-\tau)_{+}\right]\leq\varepsilon,\ \ \forall\pi\in\Pi\] (4)

where \(\hat{\pi}^{*}\) is the optimal feasible policy with respect to \((\hat{\mathbb{P}},\hat{\Delta}(s,a),\hat{\mathcal{U}}_{H},r)\), \(\Pi\) is the set of feasible policies with respect to \((\Delta(s,a),\mathcal{U}_{H})\). and \(V(s;r)\) is the value function under reward function \(r\). We say that a policy is _\(\varepsilon\)-optimal_ if it satisfies the left inequality in Eq. (4) and _\(\varepsilon\)-safe_ if it satisfies the right inequality in Eq. (4). We measure the performance by the number of episodes used before the algorithm terminates, i.e., _sample complexity_. Moreover, the cumulative step-wise violation till episode \(K\) is defined as \(C(K)=\sum_{k=1}^{K}\sum_{h=1}^{H}(c(s_{h}^{k})-\tau)_{+}\). In Safe-RFE-SW, our goal is to design an \((\varepsilon,\delta)\)-optimal safe algorithm, and minimize both sample complexity and violation.

### Algorithm SRF-UCRL

The Safe-RFE-SW problem requires us to consider extra safety constraints for both the exploration phase and final output policy, which needs new algorithm design and techniques compared to previous RFE algorithms. Also, the techniques for Safe-RL-SW in Section 4.2 are not sufficient for guaranteeing the safety of output policy, because SUCBVI only guarantees a step-wise violation during exploration.

We design an efficient algorithm **S**afe **RF-UCRL** (SRF-UCRL), which builds upon previous RFE algorithm RF-UCRL (Kaufmann et al., 2021). SRF-UCRL distinguishes potentially unsafe states and safe actions by backward iteration, and establishes a new uncertainty function to guarantee the safety of output policy. Algorithm 2 illustrates the procedure of SRF-UCRL. Specifically, in each episode \(k\), we first execute a policy \(\pi_{h}^{k}\) computed from previous episodes, and then update the estimated next state set \(\{\Delta_{h}^{k}(s,a)\}_{h\in[H]}\) and unsafe state set \(\mathcal{U}_{H}^{k}\) by optimistic estimation. Then, we use Eq. (2) to calculate the unsafe state set \(\mathcal{U}_{h}\) for all steps \(h\in[H]\). After that, we update the uncertainty function \(\overline{W}^{k}\) defined in Eq. (5) below and compute the policy \(\pi^{k+1}\) that maximizes the uncertainty to encourage more exploration in the next episode.

Now we provide the definition of the _uncertainty function_, which measures the estimation error between the empirical MDP and true MDP. For any safe state-action pair \(s\notin\mathcal{U}_{h},a\in A_{h}^{k,safe}(s)\), we define

\[\overline{W}_{h}^{k}(s,a)=\min\left\{H,M(N_{h}^{k}(s,a),\delta)+\sum_{s^{ \prime}}\hat{\mathbb{P}}_{h}^{k}(s^{\prime}\mid s,a)\max_{b\in A_{h+1}^{k,a \delta}(s^{\prime})}\overline{W}_{h+1}^{k}(s^{\prime},b)\right\}.\] (5)

\[\text{where }M(N_{h}^{k}(s,a),\delta)=2H\sqrt{\frac{2\gamma(N_{h}^{k}(s,a), \delta)}{N_{h}^{k}(s,a)}}+\frac{SH\gamma(N_{h}^{k}(s,a),\delta)}{N_{h}^{k}(s,a )},\text{ and }\gamma(n,\delta)\text{ is a logarithmic term that is formally defined in Theorem 6.2. For other state-action pairs \((s,a)\), we relax the restriction of \(b\in\mathcal{A}_{h+1}^{k,safe}(s^{\prime})\) to \(b\in\mathcal{A}\) in the \(\max\) function. Our algorithm stops when \(\overline{W}_{1}^{K}(s_{1},\pi^{K}(s_{1}))\) shrinks to within \(\varepsilon/2\). Compared to previous RFE works (Kaufmann et al. (2021); Menard et al. (2021)), our uncertainty function has two distinctions. First, for any safe state-action pair \((s,a)\in(\mathcal{S}\setminus\mathcal{U}_{h},A_{h}^{k,safe}(s))\), Eq. (5) considers only safe actions \(A_{h+1}^{k,safe}(s)\), which guarantees that the agent focuses on safe policies. Second, Eq. (5) incorporates another term \((SH\gamma(N_{h}^{k}(s,a),\delta)/N_{h}^{k}(s,a))\) to control the expected violation for feasible policies. Now we present our result for Safe-RFE-SW.

**Theorem 6.2**.: _Let \(\gamma(n,\delta)=2(\log(2SAH/\delta)+(S-1)\log(e(1+n/(S-1)))\), Algorithm 2 is a \((\varepsilon,\delta)\)-PAC algorithm with sample complexity at most2_

Footnote 2: Here \(\widetilde{\mathcal{O}}(\cdot)\) ignores all \(\log S,\log A,\log H,\log(1/\varepsilon)\) and \(\log(\log(1/\delta))\) terms.

\[K=\widetilde{\mathcal{O}}\left(\left(\frac{S^{2}AH^{2}}{\varepsilon}+\frac{H^ {4}SA}{\varepsilon^{2}}\right)\left(\log\left(\frac{1}{\delta}\right)+S\right) \right),\]

_The step-wise violation of Algorithm 2 during exploration is \(C(K)=\widetilde{\mathcal{O}}(S^{2}AH^{2}+\sqrt{ST})\)._

Compared to previous work (Kaufmann et al., 2021) with an \(\widetilde{\mathcal{O}}((H^{4}SA/\varepsilon^{2})(\log(1/\delta)+S))\) sample complexity, our result has an additional term \(\widetilde{\mathcal{O}}((S^{2}AH^{2}/\varepsilon)(\log(1/\delta)+S))\). This term is due to the additional safety requirement for the final output policy, which was not considered in previous RFE algorithms (Kaufmann et al., 2021; Menard et al., 2021). When \(\varepsilon\) and \(\delta\) are sufficiently small, the leading term is \(\widetilde{\mathcal{O}}((H^{4}SA/\varepsilon^{2})\log(1/\delta))\), which implies that our algorithm satisfies the safety constraint without suffering additional regret.3

Footnote 3: Menard et al. (2021) improve the result by a factor \(H\) and replace \(\log(1/\delta)+S\) by \(\log(1/\delta)\) via the Bernstein-type inequality. While they do not consider the safety constraints, we believe that similar improvement can also be applied to our framework, without significant changes in our analysis. However, since our paper mainly focuses on tackling the safety constraint for reward-free exploration, we use the Hoeffding-type inequality to keep the succinctness of our statements.

### Analysis for Algorithm SRF-UCRL

For the analysis of step-wise violation (Eq. (1)), similar to algorithm SUCBVI, algorithm SRF-UCRL estimates the next state set \(\Delta_{h}(s,a)\) and potentially unsafe state set \(\mathcal{U}_{h}\), which guarantees a \(\widetilde{\mathcal{O}}(\sqrt{ST})\) step-wise violation. Now we give a proof sketch for the \(\varepsilon\)-safe property of output policy (Eq. (4)).

First, if \(\pi\) is a feasible policy for \((\hat{\mathbb{P}}^{k},\Delta^{k}(s,a),\{\mathcal{U}_{h}^{k}\}_{h\in[H]})\) and \(s_{h+1}\in\Delta_{h}^{K}(s_{h},a_{h})\) for all \(h\in[H]\), the agent who follows policy \(\pi\) will only visit the estimated safe states. Since each estimated safe stateonly suffers a \(\mathcal{O}(1/\sqrt{N_{t}(s_{h},\pi_{h}(s_{h}))})\) violation, the violation led by this situation is bounded by \(\overline{W}_{1}^{k}(s,\pi_{1}(s_{1}))\). Next, we bound the probability that \(s_{h+1}\notin\Delta_{h}^{K}(s_{h},a_{h})\) for some step \(h\in[H]\). For any state-action pair \((s,a)\), if there is a probability \(\mathbb{P}(s^{\prime}\mid s,a)\geq\widetilde{\mathcal{O}}(\log(S/\delta)/N_{h }^{K}(s,a))\) that the agent transitions to next state \(s^{\prime}\) from \((s,a)\) at step \(h\), the state \(s^{\prime}\) is put into \(\Delta_{h}^{K}(s,a)\) with probability at least \(1-\delta/S\). Then, we can expect that all such states are put into our estimated next state set \(\Delta_{h}^{K}(s,a)\). Thus, the probability that \(s_{h+1}\notin\Delta_{h}^{K}(s_{h},a_{h})\) is no larger than \(\widetilde{\mathcal{O}}(\log(S/\delta)/N_{h}^{K}(s,a))\) by a union bound over all possible next states \(s^{\prime}\). Based on this argument, \(\overline{W}_{h}^{k}(s_{h},a_{h})\) is an upper bound for the total probability that \(s_{h+1}\notin\Delta_{h}^{K}(s_{h},a_{h})\) for some step \(h\). This will lead to additional \(\overline{W}_{1}^{K}(s_{1},\pi_{1}(s_{1}))\) expected violation. Hence the expected violation of output policy is upper bounded \(2\overline{W}_{1}^{K}(s_{1},\pi_{1}^{K}(s_{1}))\leq\varepsilon\). The complete proof is provided in Appendix A.

## 7 Experiments

In this section, we provide experiments for Safe-RL-SW and Safe-RFE-SW to validate our theoretical results. For Safe-RL-SW, we compare our algorithm SUCBVI with a classical RL algorithm UCBVI (Azar et al., 2017) and three state-of-the-art CMDP algorithms OptCMDP-bonus (Efroni et al., 2020), Optpess (Liu et al., 2021) and Triple-Q (Wei et al., 2022). For Safe-RFE-SW, we report the average reward in each episode and cumulative step-wise violation. For Safe-RFE-SW, we compare our algorithm SRF-UCRL with a state-of-the-art RFE algorithm RF-UCRL (Kaufmann et al., 2021). We do not plot the regret because unconstrained MDP or CMDP algorithms do not guarantee step-wise violation. Applying them to the step-wise constrained setting can lead to negative or large regret and large violations, making the results meaningless. Detailed experiment setup is in Appendix E.

As shown in Figure 1, the rewards of SUCBVI and SRF-UCRL converge to the optimal rewards under safety constraints (denoted by "safe optimal reward"). In contrast, the rewards of UCBVI and UCRL converge to the optimal rewards without safety constraints (denoted by "unconstrained optimal reward"), and those of CMDP algorithms converge to a policy with low expected violation (denoted by "constrained optimal reward"). For Safe-RFE-SW, the expected violation of output policy of SRF-UCRL converges to zero while that of RF-UCRL does not. This corroborates the ability of SRF-UCRL in finding policies that simultaneously achieve safety and high rewards.

## 8 Conclusion

In this paper, we investigate a novel safe reinforcement learning problem with step-wise safety constraints. We first provide an algorithmic framework SUCBVI to achieve both an \(\widetilde{\mathcal{O}}(\sqrt{H^{3}SAT})\) regret and an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) step-wise or an \(\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}}+S^{2}AH^{2})\) gap-dependent bounded violation that is independent of \(T\). Then, we provide two lower bounds to validate the optimality of SUCBVI in both violation and regret in terms of \(S\) and \(T\). Further, we extend our framework to the safe RFE with a step-wise violation and provide an algorithm SRF-UCRL that identifies a near-optimal safe policy given any reward function \(r\) and guarantees an \(\widetilde{\mathcal{O}}(\sqrt{ST})\) violation during exploration.

Figure 1: Experimental results for Safe-RL-SW and Safe-RFE-SW. The left two figures show the average rewards and step-wise violations of algorithms SUCBVI, UCBVI (Azar et al., 2017), OptCMDP-bonus (Efroni et al., 2020), Triple-Q (Wei et al., 2022) and Optpess (Liu et al., 2021). The right two figures show the reward and expected violation of the policies outputted by algorithms SRF-UCRL and RF-UCRL (Kaufmann et al., 2021).

## Acknowledgements

This work is supported by the Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2020AAA0108400 and 2020AAA0108403 and the Tsinghua Precision Medicine Foundation 10001020109.

## References

* Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pp. 22-31. PMLR, 2017.
* Afsar et al. (2022) M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender systems: A survey. _ACM Computing Surveys_, 55(7):1-38, 2022.
* Alshiekh et al. (2018) Mohammed Alshiekh, Roderick Bloem, Rudiger Ehlers, Bettina Konighofer, Scott Niekum, and Ufuk Topcu. Safe reinforcement learning via shielding. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Amani et al. (2021) Sanae Amani, Christos Thrampoulidis, and Lin Yang. Safe reinforcement learning with linear function approximation. In _International Conference on Machine Learning_, pp. 243-253. PMLR, 2021.
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pp. 263-272. PMLR, 2017.
* Berkenkamp et al. (2017) Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. _Advances in neural information processing systems_, 30, 2017.
* Bura et al. (2021) Archana Bura, Aria HasanzadeZonuzy, Dileep Kalathil, Srinivas Shakkottai, and Jean-Francois Chamberland. Safe exploration for constrained reinforcement learning with provable guarantees. _arXiv preprint arXiv:2112.00885_, 2021.
* Chow et al. (2018) Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. _Advances in neural information processing systems_, 31, 2018.
* Dalal et al. (2018) Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _arXiv preprint arXiv:1801.08757_, 2018.
* Ding et al. (2020) Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. _Advances in Neural Information Processing Systems_, 33:8378-8390, 2020.
* Ding et al. (2021) Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In _International Conference on Artificial Intelligence and Statistics_, pp. 3304-3312. PMLR, 2021.
* Efroni et al. (2020a) Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps. _arXiv preprint arXiv:2003.02189_, 2020a.
* Efroni et al. (2020b) Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps. _arXiv preprint arXiv:2003.02189_, 2020b.
* Huang et al. (2022) Ruiquan Huang, Jing Yang, and Yingbin Liang. Safe exploration incurs nearly no additional sample complexity for reward-free rl. _arXiv preprint arXiv:2206.14057_, 2022.
* Jin et al. (2020) Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pp. 4870-4879. PMLR, 2020.
* Jin et al. (2020)Krishna C Kalagarla, Rahul Jain, and Pierluigi Nuzzo. A sample-efficient algorithm for episodic finite-horizon mdp with constraints. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 8030-8037, 2021.
* Kaufmann et al. [2021] Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, and Michal Valko. Adaptive reward-free exploration. In _Algorithmic Learning Theory_, pp. 865-891. PMLR, 2021.
* Lanctot et al. [2019] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al. Openspiel: A framework for reinforcement learning in games. _arXiv preprint arXiv:1908.09453_, 2019.
* Le et al. [2019] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In _International Conference on Machine Learning_, pp. 3703-3712. PMLR, 2019.
* Liu et al. [2021a] Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained mdps. _Advances in Neural Information Processing Systems_, 34:17183-17193, 2021a.
* Liu et al. [2021b] Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained mdps. _Advances in Neural Information Processing Systems_, 34:17183-17193, 2021b.
* Liu et al. [2020] Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under constraints. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 4940-4947, 2020.
* Menard et al. [2021] Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure exploration in reinforcement learning. In _International Conference on Machine Learning_, pp. 7599-7608. PMLR, 2021.
* Miryoosefi and Jin [2022] Sobhan Miryoosefi and Chi Jin. A simple reward-free approach to constrained reinforcement learning. In _International Conference on Machine Learning_, pp. 15666-15698. PMLR, 2022.
* Osband and Van Roy [2016] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. _arXiv preprint arXiv:1608.02732_, 2016.
* Qiu et al. [2020] Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. Upper confidence primal-dual reinforcement learning for cmdp with adversarial loss. _Advances in Neural Information Processing Systems_, 33:15277-15287, 2020.
* Shi et al. [2023] Ming Shi, Yingbin Liang, and Ness Shroff. A near-optimal algorithm for safe reinforcement learning under instantaneous hard constraints. _arXiv preprint arXiv:2302.04375_, 2023.
* Simao et al. [2021] Thiago D Simao, Nils Jansen, and Matthijs TJ Spaan. Alwayssafe: Reinforcement learning without safety constraint violations during training. 2021.
* Singh et al. [2020] Rahul Singh, Abhishek Gupta, and Ness B Shroff. Learning in markov decision processes under constraints. _arXiv preprint arXiv:2002.12435_, 2020.
* Sootla et al. [2022] Aivar Sootla, Alexander Cowen-Rivers, Jun Wang, and Haitham Bou Ammar. Enhancing safe exploration using safety state augmentation. _Advances in Neural Information Processing Systems_, 35:34464-34477, 2022.
* Stooke et al. [2020] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. In _International Conference on Machine Learning_, pp. 9133-9143. PMLR, 2020.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Tessler et al. [2018] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. _arXiv preprint arXiv:1805.11074_, 2018.
* Thomas et al. [2021] Garrett Thomas, Yuping Luo, and Tengyu Ma. Safe reinforcement learning by imagining the near future. _Advances in Neural Information Processing Systems_, 34:13859-13869, 2021.
* Tessler et al. [2021]Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov decision processes with gaussian processes. _Advances in neural information processing systems_, 29, 2016.
* Turchetta et al. [2020] Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe reinforcement learning via curriculum induction. _Advances in Neural Information Processing Systems_, 33:12151-12162, 2020.
* Uchibe and Doya [2007] Eiji Uchibe and Kenji Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. In _2007 IEEE 6th International Conference on Development and Learning_, pp. 163-168. IEEE, 2007.
* Wachi and Sui [2020] Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision processes. In _International Conference on Machine Learning_, pp. 9797-9806. PMLR, 2020.
* Wachi et al. [2018] Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe exploration and optimization of constrained mdps using gaussian processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Wachi et al. [2021] Akifumi Wachi, Yunyue Wei, and Yanan Sui. Safe policy optimization with local generalized linear function approximations. _Advances in Neural Information Processing Systems_, 34:20759-20771, 2021.
* Wang et al. [2022] Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, and Qi Zhu. Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments. _arXiv preprint arXiv:2209.15090_, 2022.
* Wang et al. [2023] Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, and Qi Zhu. Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments. In _International Conference on Machine Learning_, pp. 36593-36604. PMLR, 2023.
* Wei et al. [2022] Honghao Wei, Xin Liu, and Lei Ying. Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation. In _International Conference on Artificial Intelligence and Statistics_, pp. 3274-3307. PMLR, 2022.
* Yu et al. [2022] Dongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen. Reachability constrained reinforcement learning. In _International Conference on Machine Learning_, pp. 25636-25655. PMLR, 2022.
* Yu et al. [2019] Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe reinforcement learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Zhao et al. [2020] Wenshuai Zhao, Jorge Pena Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In _2020 IEEE Symposium Series on Computational Intelligence (SSCI)_, pp. 737-744. IEEE, 2020.