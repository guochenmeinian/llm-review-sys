# Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors

Farzad Pourkamali & Nicolas Macris

School of Computer and Communication Sciences,

Ecole Polytechnique Federale de Lausanne

{farzad.pourkamali,nicolas.macris}@epfl.ch

###### Abstract

We consider a statistical model for matrix factorization in a regime where the rank of the two hidden matrix factors grows linearly with their dimension and their product is corrupted by additive noise. Despite various approaches, statistical and algorithmic limits of such problems have remained elusive. We study a Bayesian setting with the assumptions that (a) one of the matrix factors is symmetric, (b) both factors as well as the additive noise have rotational invariant priors, (c) the priors are known to the statistician. We derive analytical formulas for _Rotation Invariant Estimators_ to reconstruct the two matrix factors, and conjecture that these are optimal in the large-dimension limit, in the sense that they minimize the average mean-square-error. We provide numerical checks which confirm the optimality conjecture when confronted to _Oracle Estimators_ which are optimal by definition, but involve the ground-truth. Our derivation relies on a combination of tools, namely random matrix theory transforms, spherical integral formulas, and the replica method from statistical mechanics.

## 1 Introduction

Matrix factorization (MF) is the problem of reconstructing two matrices \(\mathbf{X}\) and \(\mathbf{Y}\) from the noisy observations of their product. Applications in signal processing and machine learning abound, such as for example dimensionality reduction [1, 2], sparse coding [3, 4, 5], representation learning [6], robust principal components analysis [7, 8], blind source separation [9], or matrix completion [10, 11].

In this work we approach the problem from a Bayesian perspective and assume that an observation or data matrix \(\mathbf{S}=\sqrt{\kappa}\mathbf{X}\mathbf{Y}+\mathbf{W}\) is given to a statistician who knows the prior distributions of \(\mathbf{X}\) and \(\mathbf{Y}\) as well as the prior of the additive noise matrix \(\mathbf{W}\) and the signal-to-noise ratio \(\kappa>0\). The task of the statistician is to construct estimators \(\mathbf{\Xi}_{\mathbf{X}}(\cdot)\), \(\mathbf{\Xi}_{Y}(\cdot)\) for the matrix factors \(\mathbf{X}\), \(\mathbf{Y}\), that ideally, minimize the average mean-square-error (MSE) \(\mathbb{E}\|\mathbf{X}-\mathbf{\Xi}_{\mathbf{X}}(\mathbf{S})\|_{\mathrm{F}}^{2}\) and \(\mathbb{E}\|\mathbf{Y}-\mathbf{\Xi}_{\mathbf{Y}}(\mathbf{S})\|_{\mathrm{F}}^{2}\) (\(\|.\|_{\mathrm{F}}\) the Frobenius norm and \(\mathbb{E}\) the expectation w.r.t \(\mathbf{X},\mathbf{Y},\mathbf{W}\)). We consider priors which are rotation invariant for all three matrices \(\mathbf{X}\), \(\mathbf{Y}\), \(\mathbf{W}\) and for \(\mathbf{X}\) we furthermore impose that it is square and symmetric. These matrix ensembles are defined precisely in section 2.1, but the reader can keep in mind the examples of Wigner or Wishart matrices for \(\mathbf{X}\), and general Gaussian \(\mathbf{Y}\) and \(\mathbf{W}\) with i.i.d elements. We look at the asymptotic regime where all matrix dimensions and ranks tend to infinity at the same speed. We remark that the usual "rotation ambiguity" occuring in MF is not present because we impose that at least one of the two matrix factors is symmetric. We also remark that MF is different (and more difficult) than matrix denoising which would consist in constructing an estimator \(\mathbf{\Xi}_{\mathbf{X}\mathbf{Y}}(\mathbf{S})\) for the signal as a whole by minimizing \(\mathbb{E}\|\mathbf{X}\mathbf{Y}-\mathbf{\Xi}_{\mathbf{X}\mathbf{Y}}(\mathbf{S})\|_{\mathrm{F}}^{2}\).

The rotation invariance of the model implies that the estimators minimizing the MSE belong to the class of rotation invariant estimators (RIE). RIEs are matrix estimators which have the same singular vectors (or eigenvectors) as the observation or data matrix. These estimators have beenproposed for matrix _denoising_ problems (see references [12, 13, 14, 15, 16] for covariance estimation, [17] for cross-covariance estimation, and [18, 19, 20] for extensions to rectangular matrices). For the present MF model, we derive optimal estimators (minimizing the MSE) that belong to the RIE class and can be computed explicitly in the large dimensional limit from the observation matrix and the knowledge of the priors. We propose:

1. an explicit RIE to estimate \(\mathbf{X}\), which requires the knowledge of the priors of _both_\(\mathbf{X},\mathbf{Y}\) and of the noise \(\mathbf{W}\). Moreover, under the assumption that \(\mathbf{X}\) is positive-semi-definite, a _sub-optimal_ RIE can be derived which _does not_ require any prior on \(\mathbf{X}\).
2. an explicit RIE to estimate \(\mathbf{Y}\), which requires the knowledge of the priors of the noise \(\mathbf{W}\) and \(\mathbf{X}\)_only_ (the prior of \(\mathbf{Y}\) is not required).
3. combined with the singular value decomposition (SVD) of the observation matrix, our explicit RIEs provide a spectral algorithm to reconstruct both factors \(\mathbf{X}\) and \(\mathbf{Y}\).

The derivation of the proposed estimators relies on the replica method from statistical mechanics combined with techniques from random matrix theory and finite-rank spherical integrals [21, 22]. Although the replica method is not rigorous and involves concentration assumptions, the derivation is entirely analytical and suggests that the estimators are optimal in the limit of large dimensions. This is corroborated by numerical calculations comparing our explicit RIEs with Oracle Estimators which are optimal by definition and involve the ground-truth matrices.

### Related literature and discussion

When the matrices \(\mathbf{X}\) and \(\mathbf{Y}\) are assumed to have _low-rank_ compared to their dimension, the mathematical theory of MF has enjoyed much progress under various settings (Bayesian, spectral, algorithmic) and fundamental information theoretical and algorithmic limits have been rigorously derived. The behavior of eigenvalues/singular values and eigenvector/singular vectors of finite-rank perturbations of a Gaussian matrix is studied in [23, 24, 25] which leads to spectral estimators when the noise matrix is Gaussian distributed. For the case of factorized prior, and Gaussian noise, closed form expressions have been established for the asymptotic Bayes-optimal estimation error [26, 27, 28, 29, 30], and iterative algorithms based on approximate message passing has been proposed [31, 32]. The low-rank matrix denoising problem has been addressed in various other settings, such as structured noise matrix [33, 34], mismatched estimation problem [35, 36, 37, 38], and estimation in the regime with diverging aspect-ratio of matrices [39].

In extensive-rank regimes, when the rank grows like the matrix dimensions, despite various attempts there is no solid theory of MF. One approach is based on Approximate Message Passing (AMP) methods developed in [40, 41, 42]. Despite acceptable performance in practical settings [43], as pointed out in [44] the AMP algorithms developed in these works are (theoretically) sub-optimal. Other approaches rooted in statistical physics have been considered in [44, 45, 46, 47] but have not led to explicit reconstructions of matrix factors or algorithms. A practical probabilistic approach to MF problem is based on variational Bayesian approximations [48, 49, 50], in which one tries to approximate the posterior distribution with proper distribution. In [51] it is shown that under Gaussian priors, the solution to the MF problem is a reweighted SVD of the observation matrix. We point out here that these estimators can be seen as a RIE and therefore there seems to be a rather close relation between the RIE studied here and the variational Bayesian approach. This also suggests that adapting RIEs to real data is an interesting direction for future research. Finally, let us also mention optimization approaches where one constructs estimators by following a gradient flow (or gradient descent) trajectory of a training loss of the type \(\|\mathbf{S}-\mathbf{X}\mathbf{Y}\|_{\rm F}^{2}+\text{reg. term}\) (see [52, 53] for analysis in rotation invariant models). Benchmarking these various other algorithmic approaches against our explicit RIEs (conjectured to be optimal) is outside the scope of this work and is left for future work.

Constraints such as sparsity or non-negativity of the matrix entries which have important applications [54] are not covered by our theory. Despite this drawback, we believe that the proposed estimators are important both for theoretical and practical purposes. Even in non-rotation invariant problems our explicit RIEs may serve as sub-optimal estimators, and as we show in an example they can be used as a "warmed-up" spectral initialization for more efficient algorithms (see for example [31, 55] for related ideas in other contexts). The methodology developed here may open up the way to further analysis in inference and learning problems perhaps also in the context of neural networks where extensive rank weight matrices must be estimated.

### Organization and notations

In section 2, we introduce the precise MF model, general class of RIEs, and the Oracle estimators. In section 3, we present the explicit RIEs (and algorithm) to estimate \(\mathbf{X}\) and \(\mathbf{Y}\). We provide the numerical examples and calculations in section 4. In section 5, we sketch the derivation of RIE for \(\mathbf{X}\), while the one for \(\mathbf{Y}\) is similar and deferred to the appendices.

The following notations are used throughout. For a vector \(\mathbf{\gamma}\in\mathbb{R}^{N}\) we denote by \(\mathbf{\Gamma}\in\mathbb{R}^{N\times M}\) a matrix constructed as \(\mathbf{\Gamma}=\left[\begin{array}{c|c}\mathbf{\Gamma}_{N}&\mathbf{0}_{N\times(M-N)} \end{array}\right]\) with \(\mathbf{\Gamma}_{N}\in\mathbb{R}^{N\times N}\) a diagonal matrix with diagonal \(\mathbf{\gamma}\). The same notations will also be used for the vector \(\mathbf{\sigma}\) and the corresponding matrix \(\mathbf{\Sigma}\) and. For a sequence of non-symmetric matrices \(\mathbf{A}\) of growing size, we denote the limiting empirical singular value distribution (ESD) by \(\mu_{A}\), and the limiting empirical eigenvalue distribution of \(\mathbf{A}\mathbf{A}^{\intercal}\) by \(\rho_{A}\). For a sequence of symmetric matrices \(\mathbf{B}\) of growing size, we denote the limiting empirical eigenvalue distribution by \(\rho_{B}\), and the limiting eigenvalue distribution of \(\mathbf{B}^{2}\) by \(\rho_{B^{2}}\).

## 2 Matrix factorization model and rotation invariant estimators

### Matrix factorization model

Let \(\mathbf{X}=\mathbf{X}^{\intercal}\in\mathbb{R}^{N\times N}\) a symmetric matrix distributed according to a rotationally invariant prior \(P_{X}(\mathbf{X})\), i.e., for any orthogonal matrix \(\mathbf{O}\in\mathbb{R}^{N\times N}\) we have \(P_{X}(\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal})=P_{X}(\mathbf{X})\). Let also \(\mathbf{Y}\in\mathbb{R}^{N\times M}\) be distributed according to a bi-rotationally invariant prior \(P_{Y}(\mathbf{Y})\), i.e. for any orthogonal matrices \(\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\) we have \(P_{Y}(\mathbf{U}\mathbf{Y}\mathbf{V}^{\intercal})=P_{Y}(\mathbf{Y})\). We observe the data matrix \(\mathbf{S}\in\mathbb{R}^{N\times M}\),

\[\mathbf{S}=\sqrt{\kappa}\mathbf{X}\mathbf{Y}+\mathbf{W} \tag{1}\]

where \(\mathbf{W}\in\mathbb{R}^{N\times M}\) is also bi-rotationally invariant distributed, and \(\kappa\in\mathbb{R}_{+}\) is proportional to the signal-to-noise-ratio (SNR). The goal is to recover _both factors_\(\mathbf{X}\) and \(\mathbf{Y}\) from the data matrix \(\mathbf{S}\). For definiteness, we consider the regime \(M\geq N\) with aspect ratio \(N/M\to\alpha\in(0,1]\) as \(N\to\infty\). The case of \(\alpha>1\) can be analyzed in the same manner and is presented in appendix F. Furthermore, we assume that the entries of \(\mathbf{X},\mathbf{Y}\) and \(\mathbf{W}\) are of the order \(O(\nicefrac{{1}}{{\sqrt{N}}})\). This scaling is such that the eigenvalues of \(\mathbf{X}\) and singular values of \(\mathbf{Y}\), \(\mathbf{W}\) and \(\mathbf{S}\) are of the order \(O(1)\) as \(N\to\infty\).

**Assumption 1**.: _The empirical eigenvalue distribution of \(\mathbf{X}\) converge weakly to measure \(\rho_{X}\), and the ESD of \(\mathbf{Y},\mathbf{W}\) converge weakly to measures \(\mu_{Y},\mu_{W}\) with bounded support on the real line. Moreover, these measures are known to the statistician. He can deduce (in principle) these measures from the priors on \(\mathbf{X},\mathbf{Y},\mathbf{W}\)._

**Remark 1**.: _In a general formulation of matrix factorization the hidden matrices have dimensions \(\mathbf{X}\in\mathbb{R}^{N\times H},\mathbf{Y}\in\mathbb{R}^{H\times M}\), and in the Bayesian framework with bi-rotational invariant priors for both factors, the optimal estimators are trivially the zero matrix. Indeed, from bi-rotational invariance we have \(P_{X}(-\mathbf{X})=P_{X}(\mathbf{X})\), \(P_{Y}(-\mathbf{Y})=P_{Y}(\mathbf{Y})\), which implies that the Bayesian estimate is zero. Here, by imposing that \(\mathbf{X}\in\mathbb{R}^{N\times N}\) is symmetric and \(P_{X}(\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal})=P_{X}(\mathbf{X})\), we can break this symmetry and find non-trivial estimators. This is due to the fact that the map \(\mathbf{X}\to-\mathbf{X}\) cannot be realized as a (real) orthogonal transformation, so \(P_{X}(-\mathbf{X})=P_{X}(\mathbf{X})\) does not hold in general (various examples are given in section 4 and appendices). Of course, if the prior is even, e.g. Wigner ensemble, again the Bayesian posterior estimate is trivially zero for both factors. As we will see our RIEs are consistent with these observations._

### Rotation invariant estimators

To recover matrices \(\mathbf{X},\mathbf{Y}\) from \(\mathbf{S}\), we consider two denoising problems. One is recovering \(\mathbf{X}\) by treating both \(\mathbf{Y},\mathbf{W}\) as "noise" matrices, and the other is estimating \(\mathbf{Y}\) by treating \(\mathbf{X},\mathbf{W}\) as "noise". As will become clear the procedure is not iterative, and the two denoising problems are solved independently and simultaneously. In the following, for each of these two problems, we introduce two rotation invariant classes of estimators and discuss their optimum _Oracle_ estimators. We then provide an explicit construction and algorithm for RIEs which we conjecture have the optimum performance of Oracle estimators in the large \(N\) limit.

#### 2.2.1 RIE class for \(\mathbf{X}\)

Consider the SVD of \(\mathbf{S}=\mathbf{U}_{S}\mathbf{\Gamma}\mathbf{V}_{S}^{\intercal}\), where \(\mathbf{U}_{S}\in\mathbb{R}^{N\times N}\), \(\mathbf{V}_{S}\in\mathbb{R}^{M\times M}\) are orthogonal, and \(\mathbf{\Gamma}\in\mathbb{R}^{N\times M}\) is a diagonal matrix with singular values of \(\mathbf{S}\) on its diagonal, \(\left(\gamma_{i}\right)_{1\leq i\leq N}\). A rotational invaraint estimator for \(\mathbf{X}\) is denoted \(\mathbf{\Xi}_{X}(\mathbf{S})\), and is constructed as:

\[\mathbf{\Xi}_{X}(\mathbf{S})=\mathbf{U}_{S}\operatorname{diag}(\xi_{x1},\ldots,\xi_{xN})\, \mathbf{U}_{S}^{\intercal} \tag{2}\]

where \(\xi_{x1},\ldots,\xi_{xN}\) are the eigenvalues of the estimator.

First, we derive an _Oracle estimator_ by minimizing the squared error \(\frac{1}{N}\big{\|}\mathbf{X}-\mathbf{\Xi}_{X}(\mathbf{S})\big{\|}_{\mathrm{F}}^{2}\) for a given instance, over the RIE class or equivalently over the choice of the eigenvalues \(\left(\xi_{xi}\right)_{1\leq i\leq N}\). Let the eigen-decomposition of \(\mathbf{X}\) be \(\mathbf{X}=\sum_{i=1}^{N}\lambda_{i}\,\mathbf{x}_{i}\mathbf{x}_{i}^{\intercal}\) with \(\mathbf{x}_{i}\in\mathbb{R}^{N}\) eigenvectors of \(\mathbf{X}\). The error can be expanded as:

\[\frac{1}{N}\big{\|}\mathbf{X}-\mathbf{\Xi}_{X}(\mathbf{S})\big{\|}_{\mathrm{F}}^{2}=\frac{ 1}{N}\sum_{i=1}^{N}\lambda_{i}^{2}+\frac{1}{N}\sum_{i=1}^{N}\xi_{xi}^{2}-\frac{ 2}{N}\sum_{i=1}^{N}\xi_{xi}\sum_{j=1}^{N}\lambda_{j}\left(\mathbf{u}_{i}^{\intercal }\mathbf{x}_{j}\right)^{2}\]

where \(\mathbf{u}_{i}\)'s are columns of \(\mathbf{U}_{S}\). Minimizing over \(\xi_{xi}\)'s, we find the optimum among the RIE class:

\[\mathbf{\Xi}_{X}^{*}(\mathbf{S})=\sum_{i=1}^{N}\xi_{xi}^{*}\,\mathbf{u}_{i}\mathbf{u}_{i}^{ \intercal},\quad\xi_{xi}^{*}=\sum_{j=1}^{N}\lambda_{j}\left(\mathbf{u}_{i}^{ \intercal}\mathbf{x}_{j}\right)^{2}=\mathbf{u}_{i}^{\intercal}\mathbf{X}\mathbf{u}_{i} \tag{3}\]

Expression (3) defines the Oracle estimator which requires the knowledge of signal matrix \(\mathbf{X}\). Surprisingly, in the large \(N\) limit, the optimal eigenvalues \(\left(\xi_{xi}^{*}\right)_{1\leq i\leq N}\) can be computed from the observation matrix and knowledge of the measures \(\rho_{X},\mu_{Y},\mu_{W}\). In the next section, we show that this leads to an _explicitly computable_ (or algorithmic) RIE, which we conjecture to be optimal as \(N\to\infty\), in the sense that its performance matches the one of the Oracle estimator.

Now we remark that the Oracle estimator is not only optimal within the rotation invariant class but is also Bayesian optimal. From the Bayesian estimation point of view, one wishes to minimize the average mean squared error (MSE) \(\operatorname{MSE}_{\hat{\mathbf{X}}}\equiv\frac{1}{N}\mathbb{E}\big{\|}\mathbf{X}- \hat{\mathbf{X}}(\mathbf{S})\big{\|}_{\mathrm{F}}^{2}\), where the expectation is over \(\mathbf{X},\mathbf{Y},\mathbf{W}\), and \(\hat{\mathbf{X}}(\mathbf{S})\) is an estimator of \(\mathbf{X}\). The MSE is minimized for \(\hat{\mathbf{X}}^{*}(\mathbf{S})=\mathbb{E}[\mathbf{X}|\mathbf{S}]\) which is the posterior mean. Therefore, the posterior mean estimator has the minimum MSE (MMSE) among all possible estimators, in particular \(\operatorname{MSE}_{\hat{\mathbf{X}}^{*}}\leq\operatorname{MSE}_{\mathbf{\Xi}_{X}^{*}}\) for any \(N\). In appendix A.1, we show that, for rotational invariant priors, the posterior mean estimator is inside the RIE class. Thus, since \(\mathbf{\Xi}_{X}^{*}(\mathbf{S})\) is optimum among the RIE class \(\operatorname{MSE}_{\mathbf{\Xi}_{X}^{*}}\leq\operatorname{MSE}_{\hat{\mathbf{X}}^{*}}\). Therefore, we conclude that the Oracle estimator (3) is Bayesian optimal in the sense that \(\operatorname{MSE}_{\mathbf{\Xi}_{X}^{*}}=\operatorname{MSE}_{\hat{\mathbf{X}}^{*}}= \operatorname{MMSE}\).

#### 2.2.2 RIE class for \(\mathbf{Y}\)

Estimators for \(\mathbf{Y}\) from the rotation invariant class are denoted \(\mathbf{\Xi}_{Y}(\mathbf{S})\), and are constructed as:

\[\mathbf{\Xi}_{Y}(\mathbf{S})=\mathbf{U}_{S}\left[\begin{array}{c|c}\operatorname{diag}( \xi_{y1},\ldots,\xi_{yN})\ \big{|}\ \mathbf{0}_{N\times(M-N)}\end{array}\right]\mathbf{V}_{S}^{\intercal} \tag{4}\]

where \(\xi_{y1},\ldots,\xi_{yN}\) are the singular values of the estimator.

Let the SVD of \(\mathbf{Y}\) be \(\mathbf{Y}=\sum_{i=1}^{N}\sigma_{i}\,\mathbf{y}_{i}^{(l)}\,\mathbf{y}_{i}^{(r)\intercal}\) with \(\mathbf{y}_{i}^{(l)}\in\mathbb{R}^{N}\), \(\mathbf{y}_{i}^{(r)}\in\mathbb{R}^{M}\) the left and right singular vectors of \(\mathbf{Y}\). To derive an _Oracle estimator_, we proceed as above. Expanding the error, we have:

\[\frac{1}{N}\big{\|}\mathbf{Y}-\mathbf{\Xi}_{Y}(\mathbf{S})\big{\|}_{\mathrm{F}}^{2}=\frac{ 1}{N}\sum_{i=1}^{N}\sigma_{i}^{2}+\frac{1}{N}\sum_{i=1}^{N}\xi_{yi}^{2}-\frac{ 2}{N}\sum_{i=1}^{N}\xi_{yi}\sum_{j=1}^{N}\sigma_{j}\left(\mathbf{u}_{i}^{\intercal }\mathbf{y}_{j}^{(l)}\right)\big{(}\mathbf{v}_{i}^{\intercal}\mathbf{y}_{j}^{(r)}\big{)}\]

where \(\mathbf{v}_{i}\)'s are columns of \(\mathbf{V}_{S}\). Minimizing over \(\xi_{y_{i}}\)'s, we find the optimum among the RIE class:

\[\mathbf{\Xi}_{Y}^{*}(\mathbf{S})=\sum_{i=1}^{N}\xi_{y_{i}}^{*}\,\mathbf{u}_{i}\mathbf{v}_{i}^{ \intercal},\quad\xi_{y_{i}}^{*}=\sum_{j=1}^{N}\sigma_{j}\left(\mathbf{u}_{i}^{ \intercal}\mathbf{y}_{j}^{(l)}\right)\big{(}\mathbf{v}_{i}^{\intercal}\mathbf{y}_{j}^{(r) }\big{)}=\mathbf{u}_{i}^{\intercal}\mathbf{Y}\mathbf{v}_{i} \tag{5}\]

Expression (5) defines the Oracle estimator which requires the knowledge of signal matrix \(\mathbf{Y}\). Like for the case of \(\mathbf{X}\), in the large \(N\) limit we can derive the optimal singular values \(\left(\xi_{y_{i}}^{*}\right)_{1\leq i\leq N}\) in termsof the singular values of observation matrix and knowledge of the measures \(\rho_{X},\mu_{W}\). This leads to an _explicitly computable_ (or algorithmic) RIE, which is conjectured to be optimal as \(N\to\infty\), in the sense that it has the same performance as the Oracle estimator. Note that unlike the estimator for \(\mathbf{X}\), we do not need the knowledge of \(\mu_{Y}\).

In appendix A.2, we show that for bi-rotationally invariant priors the posterior mean estimator \(\hat{\mathbf{Y}}^{*}(\mathbf{S})=\mathbb{E}[\mathbf{Y}|\mathbf{S}]\) belongs to the RIE class, which (by similar arguments to the case of \(\mathbf{X}\)) implies that the Oracle estimator (5) is Bayesian optimal.

## 3 Algorithmic RIEs for the matrix factors

In this section, we present our explicit RIEs for \(\mathbf{X},\mathbf{Y}\) and the corresponding algorithm. We conjecture that their performance matches the one of Oracles estimators in the large \(N\) limit and they are therefore Bayesian optimal in this limit. Let us first give a brief reminder on useful transforms in random matrix theory.

### Preliminaries on transforms in random matrix theory

For a probability density function \(\rho(x)\) on \(\mathbb{R}\), the _Stieltjes_ (or _Cauchy_) transform is defined as

\[\mathcal{G}_{\rho}(z)=\int_{\mathbb{R}}\frac{1}{z-x}\rho(x)\,dx\quad\text{for $z \in\mathbb{C}\backslash\mathrm{supp}(\rho)$}\]

By Plemelj formulae we have for \(x\in\mathbb{R}\),

\[\lim_{\epsilon\to 0^{+}}\mathcal{G}_{\rho}(x-\mathrm{i}\epsilon)=\pi\mathsf{H }[\rho](x)+\pi\mathrm{i}\rho(x) \tag{6}\]

with \(\mathsf{H}[\rho](x)=\mathrm{p.v.}\frac{1}{\pi}\int_{\mathbb{R}}\frac{\rho(t)}{ x-t}dt\) the _Hilbert_ transform of \(\rho\) (here \(\mathrm{p.v.}\) stands for "principal value"). Denoting the inverse of \(\mathcal{G}_{\rho}(z)\) by \(\mathcal{G}_{\rho}^{-1}(z)\), the _R-transform_ of \(\rho\) is defined as [56]:

\[\mathcal{R}_{\rho}(z)=\mathcal{G}_{\rho}^{-1}(z)-\frac{1}{z}\]

For a probability density function \(\mu\) with support contained in\([-K,K]\) with \(K>0\), we define a generating function of (even) moments \(\mathcal{M}_{\mu}:[0,K^{-2}]\to\mathbb{R}_{+}\) as \(\mathcal{M}_{\mu}(z)=\int\frac{1}{1-t^{2}z}\mu(t)\,dt-1\). For \(\alpha\in(0,1]\), define \(T^{(\alpha)}(z)=(\alpha z+1)(z+1)\), and \(\mathcal{H}_{\mu}^{(\alpha)}(z)=zT^{(\alpha)}\big{(}\mathcal{M}_{\mu}(z)\big{)}\). The _rectangular R-transform_ with aspect ratio \(\alpha\) is defined as [57]:

\[\mathcal{C}_{\mu}^{(\alpha)}(z)=T^{(\alpha)}{}^{-1}\Big{(}\frac{z}{{\mathcal{ H}_{\mu}^{(\alpha)}}^{-1}(z)}\Big{)}\]

### Explicit RIE for \(\mathbf{X}\)

The RIE for \(\mathbf{X}\) is constructed as \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})=\sum_{i=1}^{N}\widehat{\xi}_{x_{i}}^{*}\mathbf{ u_{i}}\mathbf{u_{i}}^{\intercal}\) with eigenvalues \(\big{(}\widehat{\xi}_{x_{i}}^{*}\big{)}_{1\leq i\leq N}\) :

\[\widehat{\xi}_{x_{i}}^{*}=\frac{1}{2\kappa\pi\bar{\mu}_{S}(\gamma_{i})}\, \mathrm{Im}\,\lim_{z\to\gamma_{i}-i0^{+}}\,\bigg{\{}\frac{1}{\zeta_{3}}\Big{[} \mathcal{G}_{\rho_{X}}\Big{(}\sqrt{\frac{z-\zeta_{1}}{\kappa\zeta_{3}}}\Big{)} +\mathcal{G}_{\rho_{X}}\Big{(}-\sqrt{\frac{z-\zeta_{1}}{\kappa\zeta_{3}}} \Big{)}\Big{]}\bigg{\}} \tag{7}\]

where \(\gamma_{i}\) is the \(i\)-th singular value of \(\mathbf{S}\), \(\bar{\mu}_{S}\) is the symmetrized limiting ESD of \(\mathbf{S}\), and

\[\zeta_{1}=\frac{1}{\mathcal{G}_{\bar{\mu}_{S}}(z)}\mathcal{C}_{\mu\nu}^{( \alpha)}\Big{(}\mathcal{G}_{\bar{\mu}_{S}}(z)\big{[}\alpha\mathcal{G}_{\bar{ \mu}_{S}}(z)+\frac{1-\alpha}{z}\big{]}\Big{)} \tag{8}\]

and \(\zeta_{3}\) satisfies 1:

Footnote 1: \(\zeta_{1},\zeta_{3}\) are the only parameters which appear in the final estimator. However, in derivation of the RIE, we have defined other parameters which do not appear in the final estimator and we omit them here.

\[(z-\zeta_{1})\mathcal{G}_{\bar{\mu}_{S}}(z)-1=\mathcal{C}_{\mu\nu}^{(\alpha)} \Big{(}\frac{1}{\zeta_{3}}\big{[}\alpha\mathcal{G}_{\bar{\mu}_{S}}(z)+\frac{1 -\alpha}{z}\big{]}\big{[}(z-\zeta_{1})\mathcal{G}_{\bar{\mu}_{S}}(z)-1\big{]} \Big{)} \tag{9}\]

**Remark 2**.: _If \(\rho_{X}\) is a symmetric measure, \(\rho_{X}(x)=\rho_{X}(-x)\), then \(\mathcal{G}_{\rho_{X}}(-z)=-\mathcal{G}_{\rho_{X}}(z)\). This implies that the optimal eigenvalues \(\big{(}\widehat{\xi}_{x_{i}}^{*}\big{)}_{1\leq i\leq N}\) in (7) are all zero, and \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})=\mathbf{0}\), see figure 4._

#### 3.2.1 An estimator for \(\mathbf{X}^{2}\)

It is interesting to note that we can construct a RIE for \(\mathbf{X}^{2}\) as \(\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})=\sum_{i=1}^{N}\widehat{\xi_{x^{2}i}}\mathbf{u}_{i }\mathbf{u}_{i}^{\intercal}\) with eigenvalues \(\big{(}\widehat{\xi_{x^{2}i}}\big{)}_{1\leq i\leq N}\):

\[\widehat{\xi_{x^{2}i}}=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})} \operatorname{Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}\frac{z-\zeta_{1}}{\zeta _{3}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\zeta_{3}} \tag{10}\]

with \(\zeta_{1},\zeta_{3}\) as in (8), (9). Note that, \(\zeta_{1},\zeta_{3}\) can be evaluated using the observation matrix and the knowledge of \(\mu_{Y},\mu_{W}\), and therefore this time the statistician _does not need to know the prior of \(\mathbf{X}\)_. Furthermore, assuming that \(\mathbf{X}\) is positive semi-definite (PSD), we can construct a sub-optimal RIE for \(\mathbf{X}\) by using \(\sqrt{\widehat{\xi_{x^{2}i}}}\) for the eigenvalues of the estimator.

#### 3.2.2 Case of Gaussian \(\mathbf{Y},\mathbf{W}\)

If \(\mathbf{Y}\), \(\mathbf{W}\) have i.i.d. Gaussian entries with variance \(\nicefrac{{1}}{{N}}\), then \(\mathcal{C}_{\mu_{Y}}^{(\alpha)}(z)=\mathcal{C}_{\mu_{W}}^{(\alpha)}(z)=\nicefrac {{z}}{{\alpha}}\). Consequently, \(\zeta_{1},\zeta_{3}\) can easily be computed to be \(\zeta_{1}=\zeta_{3}=\mathcal{G}_{\bar{\mu}_{S}}(z)+\nicefrac{{(1-\alpha)}}{{( \alpha z)}}\), thus the estimator (7) can be evaluated from the observation matrix. In particular, the estimator (10) simplifies to:

\[\widehat{\xi_{x^{2}i}}=\frac{1}{\kappa}\left[\ -1+\frac{1}{\alpha\Big{(}\pi^{2} \bar{\mu}_{S}(\gamma_{i})^{2}+\big{(}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+ \frac{1-\alpha}{\alpha\gamma_{i}}\big{)}^{2}\Big{)}}\right] \tag{11}\]

### Explicit RIE for \(\mathbf{Y}\)

Our explicit RIE for \(\mathbf{Y}\) is constructed as \(\widehat{\mathbf{\Xi}_{Y}^{*}}(\mathbf{S})=\sum_{i=1}^{N}\widehat{\xi_{y_{i}}}\mathbf{u}_ {i}\mathbf{v}_{i}^{\intercal}\) with singular values \(\big{(}\widehat{\xi_{y_{i}}}\big{)}_{1\leq i\leq N}\):

\[\widehat{\xi_{y_{i}}^{*}}=\frac{1}{\sqrt{\kappa}}\frac{1}{\pi\bar{\mu}_{S}( \gamma_{i})}\operatorname{Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}q_{4} \tag{12}\]

where \(\gamma_{i}\) is the \(i\)-th singular value of \(\mathbf{S}\), and \(q_{4}\) is the solution to the following system of equations 2:

Footnote 2: Like the case for \(\mathbf{X}\), we omit some of the parameters which do not appear in the final estimator.

\[\begin{cases}\beta_{1}=\frac{\mathcal{C}_{\mu_{W}}^{(\alpha)}(q_{1}q_{2})}{q_ {1}}+\frac{1}{2}\sqrt{\frac{q_{2}}{q_{1}}}\Big{(}\mathcal{R}_{\rho_{X}}\big{(} q_{4}+\sqrt{q_{1}q_{3}}\big{)}-\mathcal{R}_{\rho_{X}}\big{(}q_{4}-\sqrt{q_{1}q_{3}} \big{)}\Big{)}\\ \beta_{4}=\frac{1}{2}\Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}+\sqrt{q_{1}q_{3 }}\big{)}+\mathcal{R}_{\rho_{X}}\big{(}q_{4}-\sqrt{q_{1}q_{3}}\big{)}\Big{)}\\ q_{1}=\mathcal{G}_{\bar{\mu}_{S}}(z),\quad q_{2}=\alpha\mathcal{G}_{\bar{\mu}_{S }}(z)+(1-\alpha)\frac{1}{z}\\ q_{3}=\frac{(z-\beta_{1})^{2}}{\beta_{4}^{2}}\mathcal{G}_{\bar{\mu}_{S}}(z)- \frac{z-\beta_{1}}{\beta_{4}^{2}},\quad q_{4}=\frac{z-\beta_{1}}{\beta_{4}} \mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\beta_{4}}\end{cases} \tag{13}\]

Similarly to the estimator derived for \(\mathbf{X}\), if \(\rho_{X}\) is a symmetric measure then the optimal singular values for the estimator of \(\mathbf{Y}\) are all zero, see remark 5.

If \(\mathbf{X}\) is a shifted Wigner matrix, i.e. \(\mathbf{X}=\mathbf{F}+c\mathbf{I}\) with \(\mathbf{F}=\mathbf{F}^{\intercal}\in\mathbb{R}^{N\times N}\) having i.i.d. Gaussian entries with variance \(\nicefrac{{1}}{{N}}\) and \(c\neq 0\) a real number, then \(\mathcal{R}_{\rho_{X}}(z)=z+c\). Moreover, if \(\mathbf{W}\) is Gaussian matrix with variance \(\nicefrac{{1}}{{N}}\), then the set of equations (13) simplifies to a great extent, and we can compute \(q_{4}\) analytically in terms of \(\mathcal{G}_{\bar{\mu}_{S}}(z)\), see appendix D.4.

### Algorithmic nature of the RIEs

The explicit RIEs (7) and (12) proposed in this section, provide spectral algorithms to estimate the matrix factors from the data matrix (and the priors). An essential ingredient that must be extracted from the data matrix is \(\mathcal{G}_{\bar{\mu}_{S}}(z)\). This quantity can be approximated from the observation matrix using Cauchy kernel method introduced in [58](see section 19.5.2), from which \(\bar{\mu}_{S}(.)\) can be approximated using (6). Therefore, given an observation matrix \(\mathbf{S}\), the spectral algorithm proceeds as follows:

1. Compute the SVD of \(\mathbf{S}\).
2. Approximate \(\mathcal{G}_{\bar{\mu}_{S}}(z)\) from the singular values of \(\mathbf{S}\).
3. Construct the RIEs for \(\mathbf{X},\mathbf{Y}\) as proposed in paragraphs 3.2, 3.3.

## 4 Numerical results

### Performance of RIE for \(\mathbf{X}\)

We consider the case where \(\mathbf{Y},\mathbf{W}\) both have i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\), and \(\mathbf{X}\) is a Wishart matrix, \(\mathbf{X}=\mathbf{HH^{\intercal}}\) with \(\mathbf{H}\in\mathbb{R}^{N\times 4N}\) having i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\). For various SNRs, we examine the performance of two proposed estimators, the RIE (7), and the square-root of the estimator (10) (since \(\mathbf{X}\) is PSD), which is sub-optimal. In figure 1, the MSEs of these algorithmic estimators are compared with the one of Oracle estimator (3). We see that the average performance of the algorithmic RIE \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\) is very close to the (optimal) Oracle estimator \(\mathbf{\Xi}_{X}^{*}(\mathbf{S})\) (relative errors are small and provided in the appendices) and we believe that the slight mismatch is due to the numerical approximations and finite-size effects. Note that, although the estimator \(\sqrt{\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})}\) is sub-optimal, it does not use any prior knowledge of \(\mathbf{X}\). For more examples, details of the numerical experiments and the relative error of the estimators, we refer to appendix C.3.

### Performance of RIE for \(\mathbf{Y}\)

We consider the case where \(\mathbf{W}\) has i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\), and \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\). Matrix \(\mathbf{Y}\) is constructed as \(\mathbf{Y}=\mathbf{U}_{Y}\mathbf{\Sigma}\mathbf{V}_{Y}^{\intercal}\) with \(\mathbf{U}_{Y}\in\mathbb{R}^{N\times N},\mathbf{V}_{Y}\in\mathbb{R}^{M\times M}\) are Haar distributed, and the singular values are generated independently from the uniform distribution on \([1,3]\). MSEs of the RIE (12) and the Oracle estimator (5) are illustrated in figure 2. We see that the performance of the algorithmic RIE \(\widehat{\mathbf{\Xi}_{Y}^{*}}(\mathbf{S})\) is very close to the optimal estimator \(\mathbf{\Xi}_{Y}^{*}(\mathbf{S})\).

Non-rotational invariant priorIn another example, which we omit here, with the same settings for \(\mathbf{X},\mathbf{W}\), we consider the case where \(\mathbf{Y}\) is a sparse matrix with entries distributed according to Bernoulli-Rademacher prior. The RIE is not optimal in this setting (since the prior is not bi-rotational invariant), however applying a simple thresholding function on the matrix constructed by RIE yields an estimate with lower MSE. This observation suggests that for the case of general priors, the RIEs can provide a spectral initialization for more efficient estimators. For more details and examples, see appendix D.4.

### Comparing RIEs of matrix factorization and denoising

The proposed RIEs, namely (7) and (12), simplify greatly when the matrices \(\mathbf{W},\mathbf{Y}\) are Gaussian, and \(\mathbf{X}\) is a shifted Wigner matrix. We perform experiments with these priors, where for a given observation matrix \(\mathbf{S}\), we look at the RIEs of \(\mathbf{X}\), \(\mathbf{Y}\) for the _MF problem_, and simultaneously at the RIE of the product \(\mathbf{X}\mathbf{Y}\) as a whole for the _denoising problem_ with formulas introduced in [19] (which can also be obtained by taking \(\mathbf{X}\) to be the identity matrix, see appendix D.3.1). Figure 3

Figure 2: MSE of estimating \(\mathbf{Y}\). MSE is normalized by the norm of the signal, \(\|\mathbf{Y}\|_{p}^{2}\). \(\mathbf{Y}\) has uniform spectral density, \(\mathcal{U}([1,3])\). \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\), and \(\mathbf{W}\) is a \(N\times M\) matrix with i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\). RIE is applied to \(N=2000,M=4000\), and the results are averaged over 10 runs (error bars are invisible).

illustrates these experiments. In particular the MSE of the denoising-RIE matches well the one of the associated Oracle estimator, and as expected is lower than the MSE of the product of MF-RIEs.

## 5 Derivation of the explicit RIEs

In this section, we sketch the derivation of our explicit RIE for \(\mathbf{X}\). The RIE for \(\mathbf{Y}\) is derived similarly, but requires more involved analysis and is presented in appendix D. For simplicity, we take the SNR parameter in (1) to be 1, so the model is \(\mathbf{S}=\mathbf{X}\mathbf{Y}+\mathbf{W}\). The optimal eigenvalues are constructed as \(\xi_{xi}^{*}=\sum_{j=1}^{N}\lambda_{j}\big{(}\mathbf{u}_{i}^{\intercal}\mathbf{x}_{j} \big{)}^{2}\). We assume that in the large \(N\) limit, \(\xi_{xi}^{*}\) can be approximated by its expectation and we introduce

\[\widehat{\xi}_{xi}^{*}=\sum_{j=1}^{N}\lambda_{j}\,\mathbb{E}\Big{[}\big{(}\mathbf{ u}_{i}^{\intercal}\mathbf{x}_{j}\big{)}^{2}\Big{]} \tag{14}\]

where the expectation is over the (left) singular vectors of the observation matrix \(\mathbf{S}\). Therefore, to compute these eigenvalues, we need to find the mean squared overlap \(\mathbb{E}\Big{[}\big{(}\mathbf{u}_{i}^{\intercal}\mathbf{x}_{j}\big{)}^{2}\Big{]}\) between eigenvectors of \(\mathbf{X}\) and singular vectors of \(\mathbf{S}\). In what follows, we will see that (a rescaling of) this quantity can be expressed in terms of \(i\)-th singular value of \(\mathbf{S}\) and \(j\)-th eigenvector of \(\mathbf{X}\) (and the limiting measures, indeed). Thus, we will use the notation \(O_{X}(\gamma_{i},\lambda_{j}):=N\mathbb{E}\Big{[}\big{(}\mathbf{u}_{i}^{\intercal }\mathbf{x}_{j}\big{)}^{2}\Big{]}\) in the following. In the next section, we discuss how the overlap can be computed from the resolvent of the "Hermitized" version of \(\mathbf{S}\).

### Relation between overlap and resolvent

Construct the matrix \(\mathbf{\mathcal{S}}\in\mathbb{R}^{(N+M)\times(N+M)}\) from the observation matrix:

\[\mathbf{\mathcal{S}}=\left[\begin{array}{cc}\mathbf{0}_{N\times N}&\mathbf{S}\\ \mathbf{S}^{\intercal}&\mathbf{0}_{M\times M}\end{array}\right]\]

By Theorem 7.3.3 in [59], \(\mathbf{\mathcal{S}}\) has the following eigen-decomposition:

\[\mathbf{\mathcal{S}}=\left[\begin{array}{cc}\hat{\mathbf{U}}_{S}&\hat{\mathbf{U}}_{S}& \mathbf{0}\\ \hat{\mathbf{V}}_{S}^{(1)}&-\hat{\mathbf{V}}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array} \right]\left[\begin{array}{cc}\mathbf{\Gamma}_{N}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&-\mathbf{\Gamma}_{N}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}&\mathbf{0}\end{array}\right]\left[\begin{array}{cc}\hat{\mathbf{U}}_{S }&\hat{\mathbf{U}}_{S}&\mathbf{0}\\ \hat{\mathbf{V}}_{S}^{(1)}&-\hat{\mathbf{V}}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array} \right]^{\intercal} \tag{15}\]

with \(\mathbf{V}_{S}=\left[\begin{array}{cc}\mathbf{V}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array}\right]\) in which \(\mathbf{V}_{S}^{(1)}\in\mathbb{R}^{M\times N}\). And, \(\hat{\mathbf{V}}_{S}^{(1)}=\frac{1}{\sqrt{2}}\mathbf{V}_{S}^{(1)}\), \(\hat{\mathbf{U}}_{S}=\frac{1}{\sqrt{2}}\mathbf{U}_{S}\). Eigenvalues of \(\mathbf{\mathcal{S}}\) are signed singular values of \(\mathbf{S}\), therefore the limiting eigenvalue distribution of \(\mathbf{\mathcal{S}}\) (ignoring zero eigenvalues) is the same as the limiting symmetrized singular value distribution of \(\mathbf{S}\). Define the resolvent of \(\mathbf{\mathcal{S}}\),

\[\mathbf{G}_{\mathcal{S}}(z)=(z\mathbf{I}-\mathbf{\mathcal{S}})^{-1}\]

We assume that as \(N\rightarrow\infty\) and \(z\) is not too close to the real axis, the matrix \(\mathbf{G}_{\mathcal{S}}(z)\) concentrates around its mean. Consequently, the value of \(\mathbf{G}_{\mathcal{S}}(z)\) becomes uncorrelated with the particular

Figure 3: MSE of factorization problem. MSE is normalized by the norm of the signal. \(\mathbf{X}\) is a shifted Wigner matrix with \(c=1\), and both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \({}^{1}\!/\!\!N\). RIE is applied to \(N=2000,M=4000\). In each run, the observation matrix \(\mathbf{S}\) is generated according to (1), and the factors \(\mathbf{X}\), \(\mathbf{Y}\) are estimated simultaneously from \(\mathbf{S}\). Results are averaged over 10 runs (error bars are invisible).

realization of \(\mathbf{S}\). Specifically, as \(N\to\infty\), \(\mathbf{G}_{\mathcal{S}}(z)\) converges to a deterministic matrix for any fixed value of \(z\in\mathbb{C}\backslash\mathbb{R}\) (independent of N). Denote the eigenvectors of \(\mathbf{S}\) by \(\mathbf{s}_{i}\in\mathbb{R}^{M+N}\), \(i=1,\ldots,M+N\). For \(z=x-\mathrm{i}\epsilon\) with \(x\in\mathbb{R}\) and small \(\epsilon\), we have:

\[\mathbf{G}_{\mathcal{S}}(x-\mathrm{i}\epsilon)=\sum_{k=1}^{2N}\frac{x+\mathrm{i} \epsilon}{(x-\tilde{\gamma}_{k})^{2}+\epsilon^{2}}\mathbf{s}_{k}\mathbf{s}_{k}^ {\intercal}+\frac{x+\mathrm{i}\epsilon}{x^{2}+\epsilon^{2}}\sum_{k=2N+1}^{N+ M}\mathbf{s}_{k}\mathbf{s}_{k}^{\intercal}\]

where \(\tilde{\gamma}_{k}\) are the eigenvalues of \(\mathbf{S}\), which are in fact the (signed) singular values of \(\mathbf{S}\), \(\tilde{\gamma}_{1}=\gamma_{1},\ldots,\tilde{\gamma}_{N}=\gamma_{N},\tilde{ \gamma}_{N+1}=-\gamma_{1},\ldots,\tilde{\gamma}_{2N}=-\gamma_{N}\).

Define the vectors \(\tilde{\mathbf{x}}_{i}=[\mathbf{x}_{i}^{\intercal},\mathbf{0}_{M}]^{\intercal}\) for \(\mathbf{x}_{i}\) eigenvectors of \(\mathbf{X}\). We have

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}=\sum_{k=1}^{2N}\frac{\epsilon}{(x -\tilde{\gamma}_{k})^{2}+\epsilon^{2}}\big{(}\tilde{\mathbf{x}}_{i}^{\intercal} \mathbf{s}_{k}\big{)}^{2}+\frac{\epsilon}{x^{2}+\epsilon^{2}}\sum_{k=2N+1}^{N +M}\big{(}\tilde{\mathbf{x}}_{i}^{\intercal}\mathbf{s}_{k}\big{)}^{2} \tag{16}\]

Given the structure of \(\mathbf{s}_{k}\)'s in (15), \(\big{(}\tilde{\mathbf{x}}_{i}^{\intercal}\mathbf{s}_{j}\big{)}^{2}=\frac{1}{2} \big{(}\mathbf{x}_{i}^{\intercal}\mathbf{u}_{j}\big{)}^{2}=\big{(}\tilde{\mathbf{x}}_ {i}^{\intercal}\mathbf{s}_{j+N}\big{)}^{2}\) for \(1\leq j\leq N\), and the second sum in (16) is zero. We assume that in the limit of large N this quantity concentrates on \(O_{X}(\gamma_{j},\lambda_{i})\) and depends only on the singular values and eigenvalue pairs \((\gamma_{j},\lambda_{i})\). We thus have:

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}\xrightarrow{N\to\infty}\int_{ \mathbb{R}}\frac{\epsilon}{(x-t)^{2}+\epsilon^{2}}O_{X}(t,\lambda_{i})\bar{ \mu}_{S}(t)\,dt \tag{17}\]

where the overlap function \(O_{X}(t,\lambda_{i})\) is extended (continuously) to arbitrary values within the support of \(\bar{\mu}_{S}\) (the symmetrized limiting singular value distribution of \(\mathbf{S}\)) with the property that \(O_{X}(t,\lambda_{i})=O_{X}(-t,\lambda_{i})\) for \(t\in\mathrm{supp}(\mu_{S})\). Sending \(\epsilon\to 0\), we find

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}\to\pi\bar{\mu}_{S}(x)O_{X}(x, \lambda_{i}) \tag{18}\]

This is a crucial relation as it allows us to study the overlap by means of the resolvent of \(\mathbf{S}\). In the next section, we establish a connection between this resolvent and the signal \(\mathbf{X}\), which enables us to determine the optimal eigenvalues values \(\widehat{\xi}_{xi}^{*}\) in terms of the singular values of \(\mathbf{S}\).

### Resolvent relation

To derive the resolvent relation between \(\mathbf{S}\) and \(\mathbf{X}\), we fix the matrix \(\mathbf{X}\) and consider the model

\[\mathbf{S}=\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{\intercal}+\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2} ^{\intercal}\]

with \(\mathbf{Y},\mathbf{W}\in\mathbb{R}^{N\times M}\) fixed matrices with limiting singular value distribution \(\mu_{Y},\mu_{W}\), and \(\mathbf{U}_{1},\mathbf{U}_{2}\in\mathbb{R}^{N\times N},\mathbf{V}_{1},\mathbf{V}_{2}\in \mathbb{R}^{M\times M}\) independent random Haar matrices. Indeed, if we substitute the SVD of the matrices \(\mathbf{Y},\mathbf{W}\) in model (1) we find the latter model. Now, the average over the singular vectors of \(\mathbf{S}\) (with fixed \(\mathbf{X}\)) is equivalent to the average over the matrices \(\mathbf{U}_{1},\mathbf{U}_{2},\mathbf{V}_{1},\mathbf{V}_{2}\). In appendix C.1, using the Replica trick, we derive the following relation in the limit \(N\to\infty\):

\[\big{\langle}\mathbf{G}_{\mathcal{S}}(z)\big{\rangle}=\left[\begin{array}{cc} \zeta_{3}^{-1}\mathbf{G}_{X^{2}}\big{(}\frac{z-\zeta_{1}}{\zeta_{3}}\big{)}&\mathbf{ 0}\\ \mathbf{0}&(z-\zeta_{2})^{-1}\mathbf{I}_{M}\end{array}\right] \tag{19}\]

with \(\zeta_{1},\zeta_{2},\zeta_{3}\) satisfying set of equations (41). \(\langle.\rangle\) is the expectation w.r.t. the singular vectors of \(\mathbf{S}\) (or equivalently over \(\mathbf{U}_{1},\mathbf{U}_{2},\mathbf{V}_{1},\mathbf{V}_{2}\)), and \(\mathbf{G}_{X^{2}}\) is the resolvent of \(\mathbf{X}^{2}\). As stated earlier, we assume that the resolvent \(\mathbf{G}_{\mathcal{S}}(z)\) concentrates in the limit \(N\to\infty\), therefore we drop the brackets in the following computation.

### Overlaps and optimal eigenvalues

From (18), (19), we find:

\[\begin{split} O_{X}(\gamma,\lambda_{i})&\approx\frac{1}{ \pi\bar{\mu}_{S}(\gamma)}\,\mathrm{Im}\lim_{z\to\gamma-\mathrm{i}0^{+}}\,\bm {x}_{i}^{\intercal}\,\zeta_{3}^{-1}\mathbf{G}_{X^{2}}\big{(}\frac{z-\zeta_{1}}{\zeta _{3}}\big{)}\,\mathbf{x}_{i}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma)}\,\mathrm{Im}\lim_{z\to\gamma- \mathrm{i}0^{+}}\,\frac{1}{z-\zeta_{1}-\zeta_{3}\lambda_{i}^{2}}\end{split} \tag{20}\]

In Fig. 4 we illustrate that the theoretical predictions (20) are in good agreement with numerical simulations for a particular case of \(\mathbf{X}\) a Wigner matrix, and \(\mathbf{Y},\mathbf{W}\) with i.i.d. Gaussian entries.

Once we have the overlap, we can compute the optimal eigenvalues to be

\[\widehat{\xi}_{x_{i}}^{*}\approx\frac{1}{N}\sum_{j=1}^{N}\lambda_{j}O_{X}(\gamma_{ i},\lambda_{j})\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\ \lim_{z\to\gamma_{i}-i0+}\frac{1}{N}\sum_{j=1}^{N}\frac{\lambda_{j}}{z- \zeta_{1}-\zeta_{3}\lambda_{j}^{2}} \tag{21}\]

With a bit of algebra, we find the estimator in (7) in the limit \(N\to\infty\), see appendix C.2.

## 6 Conclusion

We studied the MF problem with extensive-rank hidden matrices, and proposed explicit (optimal) RIEs to recover the distinct factors. The model we considered, although limited, is the first analytically solvable model in this challenging regime, and we believe it paves the way for future investigations. Extending the methodology developed here to the general case where both matrices can be non-symmetric is an interesting research direction (note that in this case factors can be recovered up to rotations). Moreover, adapting RIEs to incorporate additional structures/ constraints on the signals is a problem with practical importance, that we leave for future investigations.

In general, the MF problem in the extensive-rank regime has not received an in-depth exploration compared to its counterpart in the low-rank regime. While substantial progress has been made in understanding and devising algorithms for low-rank matrices, the challenges posed by extensive-rank matrices remain relatively uncharted. Specifically, the study of general non-rotation invariant priors with i.i.d entries (e.g. a prior supported on the positive real line as in non-negative MF) stands out as an underdeveloped area from the theory side.

## Acknowledgments

We are thankful to Jean Barbier for interesting discussions on related problems. The work of F. P has been supported by Swiss National Science Foundation grant no 200021-204119.

## References

* [1] Ivana Tosic and Pascal Frossard. Dictionary learning. _IEEE Signal Processing Magazine_, 28(2):27-38, 2011.
* [2] Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, and Francis Bach. Supervised dictionary learning. _Advances in neural information processing systems_, 21, 2008.
* [3] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. _Nature_, 381(6583):607-609, 1996.
* [4] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? _Vision research_, 37(23):3311-3325, 1997.
* [5] Kenneth Kreutz-Delgado, Joseph F Murray, Bhaskar D Rao, Kjersti Engan, Te-Won Lee, and Terrence J Sejnowski. Dictionary learning algorithms for sparse representation. _Neural computation_, 15(2):349-396, 2003.

Figure 4: Comparison of the theoretical prediction (20) of the rescaled overlap with the numerical simulation. The rescaled overlap between \(200\)-th and \(800\)-th left singular vector of \(\mathbf{S}\) and the eigenvectors of \(\mathbf{X}\) is illustrated. \(\mathbf{X}=\mathbf{X}^{\intercal}\in\mathbb{R}^{N\times N}\) has i.i.d. Gaussian entries with variance \(\nicefrac{{1}}{{\sqrt{N}}}\) and is fixed. Both \(\mathbf{Y}\) and \(\mathbf{Z}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\). The simulation results are average of 1000 experiments with fixed \(\mathbf{X}\), and \(N=1000,M=2000\). Some of the simulation points are dropped for clarity.

One can see that the overlap is an even function of eigenvalues \(\lambda_{i}\), so the optimal eigenvalues \(\xi_{x_{i}}^{*}=\sum_{j=1}^{N}\lambda_{j}\left(\mathbf{u}_{i}^{\intercal}\mathbf{x}_{j }\right)^{2}\) are all zero, as discussed in remark 2.

* [6] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 35(8):1798-1828, 2013.
* [7] Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? _Journal of the ACM (JACM)_, 58(3):1-37, 2011.
* [8] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and suboptimality of PCA i: Spiked random matrix models. _The Annals of Statistics_, 46(5):2416-2451, 2018.
* [9] Adel Belouchrani, Karim Abed-Meraim, J-F Cardoso, and Eric Moulines. A blind source separation technique using second-order statistics. _IEEE Transactions on signal processing_, 45(2):434-444, 1997.
* [10] Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. _IEEE Transactions on Information Theory_, 56(5):2053-2080, 2010.
* [11] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. _Communications of the ACM_, 55(6):111-119, 2012.
* [12] Charles Stein. Estimation of a covariance matrix. In _39th Annual Meeting IMS, Atlanta, GA, 1975_, 1975.
* [13] Akimichi Takemura. An orthogonally invariant minimax estimator of the covariance matrix of a multivariate normal population. _Tsukuba journal of mathematics_, 8(2):367-376, 1984.
* [14] Joel Bun, Romain Allez, Jean-Philippe Bouchaud, and Marc Potters. Rotational invariant estimator for general noisy matrices. _IEEE Transactions on Information Theory_, 62(12):7475-7490, 2016.
* [15] Joel Bun, Jean-Philippe Bouchaud, and Marc Potters. Cleaning large correlation matrices: tools from random matrix theory. _Physics Reports_, 666:1-109, 2017.
* [16] Farzad Pourkamali, Jean Barbier, and Nicolas Macris. Matrix inference in growing rank regimes. _arXiv preprint arXiv:2306.01412_, 2023.
* [17] Florent Benaych-Georges, Jean-Philippe Bouchaud, and Marc Potters. Optimal cleaning for singular values of cross-covariance matrices. _arXiv preprint arXiv:1901.05543_, 2019.
* [18] Emanuele Troiani, Vittorio Erba, Florent Krzakala, Antoine Maillard, and Lenka Zdeborova. Optimal denoising of rotationally invariant rectangular matrices. _arXiv preprint arXiv:2203.07752_, 2022.
* [19] Farzad Pourkamali and Nicolas Macris. Rectangular rotational invariant estimator for general additive noise matrices. In _2023 IEEE International Symposium on Information Theory (ISIT)_, pages 2081-2086, 2023.
* [20] Itamar D Landau, Gabriel C Mel, and Surya Ganguli. Singular vectors of sums of rectangular random matrices and optimal estimators of high-rank signals: The extensive spike model. _arXiv preprint arXiv:2306.00340_, 2023.
* [21] Alice Guionnet and M M Maida. A fourier view on the R-transform and related asymptotics of spherical integrals. _Journal of functional analysis_, 222(2):435-490, 2005.
* [22] Florent Benaych-Georges. Rectangular R-transform as the limit of rectangular spherical integrals. _Journal of Theoretical Probability_, 24(4):969-987, 2011.
* [23] Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. _Annals of Probability_, pages 1643-1697, 2005.
* [24] Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. _Advances in Mathematics_, 227(1):494-521, 2011.
* [25] Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. _Journal of Multivariate Analysis_, 111:120-135, 2012.
* [26] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborova. Constrained low-rank matrix estimation: Phase transitions, approximate message passing and applications. _Journal of Statistical Mechanics: Theory and Experiment_, 2017(7):073403, 2017.

* [27] Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, Lenka Zdeborova, et al. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. _Advances in Neural Information Processing Systems_, 29, 2016.
* [28] Marc Lelarge and Leo Miolane. Fundamental limits of symmetric low-rank matrix estimation. _Probability Theory and Related Fields_, 173:859-929, 2019.
* [29] Leo Miolane. Fundamental limits of low-rank matrix estimation: the non-symmetric case. _arXiv preprint arXiv:1702.00473_, 2017.
* [30] Jean Barbier and Nicolas Macris. The adaptive interpolation method: a simple scheme to prove replica formulas in bayesian inference. _Probability theory and related fields_, 174(3):1133-1185, 2019.
* [31] Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. _The Annals of Statistics_, 49(1), 2021.
* [32] Alyson K Fletcher and Sundeep Rangan. Iterative reconstruction of rank-one matrices in noise. _Information and Inference: A Journal of the IMA_, 7(3):531-562, 2018.
* [33] Jean Barbier, Francesco Camilli, Marco Mondelli, and Manuel Saenz. Fundamental limits in structured principal component analysis and how to reach them. _Proceedings of the National Academy of Sciences_, 120(30):e2302028120, 2023.
* [34] Zhou Fan. Approximate message passing algorithms for rotationally invariant matrices. _The Annals of Statistics_, 50(1):197-224, 2022.
* [35] Farzad Pourkamali and Nicolas Macris. Mismatched estimation of symmetric rank-one matrices under gaussian noise. In _International Zurich Seminar on Information and Communication (IZS 2022). Proceedings_, pages 84-88. ETH Zurich, 2022.
* [36] Jean Barbier, TianQi Hou, Marco Mondelli, and Manuel Saenz. The price of ignorance: how much does it cost to forget noise structure in low-rank matrix estimation? _arXiv preprint arXiv:2205.10009_, 2022.
* [37] Farzad Pourkamali and Nicolas Macris. Mismatched estimation of non-symmetric rank-one matrices under gaussian noise. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 1288-1293. IEEE, 2022.
* [38] Alice Guionnet, Justin Ko, Florent Krzakala, and Lenka Zdeborova. Estimating rank-one matrices with mismatched prior and noise: universality and large deviations. _arXiv preprint arXiv:2306.09283_, 2023.
* [39] Andrea Montanari and Yuchen Wu. Fundamental limits of low-rank matrix estimation with diverging aspect ratios. _arXiv preprint arXiv:2211.00488_, 2022.
* [40] Yoshiyuki Kabashima, Florent Krzakala, Marc Mezard, Ayaka Sakata, and Lenka Zdeborova. Phase transitions and sample complexity in Bayes-optimal matrix factorization. _IEEE Transactions on information theory_, 62(7):4228-4265, 2016.
* [41] Jason T Parker, Philip Schniter, and Volkan Cevher. Bilinear generalized approximate message passing--part i: Derivation. _IEEE Transactions on Signal Processing_, 62(22):5839-5853, 2014.
* [42] Qiuyun Zou, Haochuan Zhang, and Hongwen Yang. Multi-layer bilinear generalized approximate message passing. _IEEE Transactions on Signal Processing_, 69:4529-4543, 2021.
* [43] Jason T Parker, Philip Schniter, and Volkan Cevher. Bilinear generalized approximate message passing--part ii: Applications. _IEEE Transactions on Signal Processing_, 62(22):5854-5867, 2014.
* [44] Antoine Maillard, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Perturbative construction of mean-field equations in extensive-rank matrix factorization and denoising. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(8):083301, 2022.
* [45] Jean Barbier and Nicolas Macris. Statistical limits of dictionary learning: random matrix theory and the spectral replica method. _Physical Review E_, 106(2):024136, 2022.
* [46] Francesco Camilli and Marc Mezard. Matrix factorization with neural networks. _Physical Review E_, 107(6):064308, 2023.
* [47] Francesco Camilli and Marc Mezard. The decimation scheme for symmetric matrix factorization. _arXiv preprint arXiv:2307.16564_, 2023.

* [48] Christopher M Bishop. Variational principal components. 1999.
* [49] Yew Jin Lim and Yee Whye Teh. Variational Bayesian approach to movie rating prediction. In _Proceedings of KDD cup and workshop_, volume 7, pages 15-21, 2007.
* [50] Alexander Ilin and Tapani Raiko. Practical approaches to principal component analysis in the presence of missing values. _The Journal of Machine Learning Research_, 11:1957-2000, 2010.
* [51] Shinichi Nakajima, Masashi Sugiyama, S Derin Babacan, and Ryota Tomioka. Global analytic solution of fully-observed variational bayesian matrix factorization. _The Journal of Machine Learning Research_, 14(1):1-37, 2013.
* [52] Salma Tarmoun, Guilherme Franca, Benjamin D Haeffele, and Rene Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In _International Conference on Machine Learning_, pages 10153-10161. PMLR, 2021.
* [53] Antoine Bodin and Nicolas Macris. Gradient flow on extensive-rank positive semi-definite matrix denoising. _arXiv preprint arXiv:2303.09474_, 2023.
* [54] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. _Nature_, 401(6755):788-791, 1999.
* [55] Marco Mondelli and Ramji Venkataramanan. Approximate message passing with spectral initialization for generalized linear models. In _International Conference on Artificial Intelligence and Statistics_, pages 397-405. PMLR, 2021.
* [56] James A Mingo and Roland Speicher. _Free probability and random matrices_, volume 35. Springer, 2017.
* [57] Florent Benaych-Georges. Rectangular random matrices, related convolution. _Probability Theory and Related Fields_, 144:471-515, 2009.
* [58] Marc Potters and Jean-Philippe Bouchaud. _A First Course in Random Matrix Theory: For Physicists, Engineers and Data Scientists_. Cambridge University Press, 2020.
* [59] Roger A Horn and Charles R Johnson. _Matrix analysis_. Cambridge university press, 2012.
* [60] Jean Zinn-Justin. _Quantum field theory and critical phenomena_, volume 171. Oxford university press, 2021.
* [61] Dennis S Bernstein. Matrix mathematics. In _Matrix Mathematics_. Princeton university press, 2009.
* [62] Harish-Chandra. Differential operators on a semisimple lie algebra. _American Journal of Mathematics_, pages 87-120, 1957.
* [63] Alice Guionnet and Jonathan Husson. Asymptotics of k dimensional spherical integrals and applications. _ALEA_, 19:769-797, 2022.

Posterior mean estimator is in the RIE class

In this section, we show that for rotational invariant priors, the posterior mean estimator is inside the RIE class. For each of the estimators of \(\mathbf{X},\mathbf{Y}\), we present an equivalent definition of the RIE, then we show that posterior mean estimator satisfies this definition.

### X Estimator

**Lemma 1**.: _Given the observation matrix \(\mathbf{S}\), let \(\hat{\mathbf{X}}(\mathbf{S})\) be an estimator of \(\mathbf{X}\). Then \(\hat{\mathbf{X}}(\mathbf{S})\) is a RIE if and only if for any orthogonal matrices \(\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\):_

\[\hat{\mathbf{X}}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal})=\mathbf{U}\hat{\mathbf{X}}(\mathbf{S})\mathbf{U}^ {\intercal} \tag{22}\]

Proof.: If \(\hat{\mathbf{X}}(\mathbf{S})\) is a RIE, then the property (22) clearly follows from the definition (2). Now we turn to the converse.

Suppose that an estimator \(\hat{\mathbf{X}}(\mathbf{S})\) satisfies (22). First, we show that if the observation matrix is diagonal, then the estimator is also diagonal. Consider the observation matrix to be \(\mathbf{S}^{\mathrm{diag}}=\left[\begin{array}{c|c}\mathrm{diag}(\mathrm{s}_{1}, \ldots,\mathrm{s}_{\mathrm{N}})&\mathbf{0}_{N\times(M-N)}\end{array}\right]\). Let \(\mathbf{I}_{k}^{-}\in\mathbb{R}^{N\times N},\mathbf{J}_{k}^{-}\in\mathbb{R}^{M\times M}\) be diagonal matrices with diagonal entries all one except the \(k\)-th entry which is \(-1\). Note that for \(1\leq k\leq N\), we have \(\mathbf{S}^{\mathrm{diag}}=\mathbf{I}_{k}^{-}\mathbf{S}^{\mathrm{diag}}\mathbf{J}_{k}^{-}\). Moreover, matrices \(\mathbf{I}_{k}^{-},\mathbf{J}_{k}^{-}\) are indeed orthogonal. For any \(1\leq k\leq N\), from the property we have:

\[\hat{\mathbf{X}}(\mathbf{S}^{\mathrm{diag}})=\hat{\mathbf{X}}(\mathbf{I}_{k}^{-}\mathbf{S}^{ \mathrm{diag}}\mathbf{J}_{k}^{-})=\mathbf{I}_{k}^{-}\hat{\mathbf{X}}(\mathbf{S}^{\mathrm{diag} })\mathbf{I}_{k}^{-} \tag{23}\]

This implies that all entries on the \(k\)-th row and \(k\)-th column of \(\hat{\mathbf{X}}(\mathbf{S}^{\mathrm{diag}})\) are zero except the \(k\)-th entry on the diagonal. Since this holds for any \(k\), we conclude that \(\hat{\mathbf{X}}(\mathbf{S}^{\mathrm{diag}})\) is diagonal.

Now, for a given general observation matrix with SVD \(\mathbf{S}=\mathbf{U}_{S}\mathbf{\Gamma}\mathbf{V}_{S}^{\intercal}\), put \(\mathbf{U}=\mathbf{U}_{S}^{\intercal},\mathbf{V}=\mathbf{V}_{S}^{\intercal}\) in the property (22). We have:

\[\hat{\mathbf{X}}(\mathbf{\Gamma})=\mathbf{U}_{S}^{\intercal}\hat{\mathbf{X}}(\mathbf{S})\mathbf{U}_{S}\]

From the argument above, the matrix on the lhs is diagonal. Consequently, the matrix \(\mathbf{U}_{S}^{\intercal}\hat{\mathbf{X}}(\mathbf{S})\mathbf{U}_{S}\) is diagonal which implies that the columns of \(\mathbf{U}_{S}\) are eigenvectors of \(\hat{\mathbf{X}}(\mathbf{S})\). Therefore, \(\hat{\mathbf{X}}(\mathbf{S})\) is a RIE. 

Now, we prove that the posterior mean estimator \(\hat{\mathbf{X}}^{*}(\mathbf{S})=\mathbb{E}[\mathbf{X}|\mathbf{S}]\) has the property (22), and therefore belongs to the RIE class. For simplicity, we drop the SNR factor \(\sqrt{\kappa}\). For any orthogonal matrices \(\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\), we have:

\[\mathbb{E}[\mathbf{X}|\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}] =\frac{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,\tilde{\mathbf{X}}\,P_{X}(\tilde {\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\tilde{\mathbf{X}}\mathbf{Y })}{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,P_{X}(\tilde{\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\bm {U}\mathbf{S}\mathbf{V}^{\intercal}-\tilde{\mathbf{X}}\mathbf{Y})}\] \[\overset{\text{(a)}}{=}\frac{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,\mathbf{ U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\,P_{X}(\tilde{\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\bm {U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\mathbf{Y})}{ \int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,P_{X}(\tilde{\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\mathbf{U} \mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\mathbf{Y})}\] \[\overset{\text{(b)}}{=}\frac{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,\mathbf{ U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\,P_{X}(\tilde{\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\bm {U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\mathbf{U}\mathbf{Y }\mathbf{V}^{\intercal})}{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,P_{X}(\tilde{\mathbf{X}})P_{Y }(\mathbf{Y})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{ \intercal}\mathbf{U}\mathbf{Y}\mathbf{V}^{\intercal})}\] \[\overset{\text{(c)}}{=}\mathbf{U}\Big{\{}\frac{\int d\mathbf{Y}\,d\tilde{ \mathbf{X}}\,\tilde{\mathbf{X}}\,P_{X}(\tilde{\mathbf{X}})P_{Y}(\mathbf{Y})P_{W}(\mathbf{S}- \tilde{\mathbf{X}}\mathbf{Y})}{\int d\mathbf{Y}\,d\tilde{\mathbf{X}}\,P_{X}(\tilde{\mathbf{X}})P_{Y }(\mathbf{Y})P_{W}(\mathbf{S}-\tilde{\mathbf{X}}\mathbf{Y})}\Big{\}}\mathbf{U}^{\intercal}\] \[=\mathbf{U}\mathbb{E}[\mathbf{X}|\mathbf{S}|\mathbf{U}^{\intercal}\]

where in (a), we changed variables \(\tilde{\mathbf{X}}\rightarrow\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal}\), used \(|\det\mathbf{U}|=1\), and rotational invariance of \(P_{X}\), \(P_{X}(\tilde{\mathbf{X}})=P_{X}(\mathbf{U}\tilde{\mathbf{X}}\mathbf{U}^{\intercal})\). In (b), we changed variables \(\mathbf{Y}\rightarrow\mathbf{U}\mathbf{Y}\mathbf{V}^{\intercal}\), used \(|\det\mathbf{U}|=|\det\mathbf{V}|=1\), and bi-rotational invariance of \(P_{Y}\), \(P_{Y}(\mathbf{Y})=P_{Y}(\mathbf{U}\mathbf{Y}\mathbf{V}^{\intercal})\). In (c), we used the bi-rotational invariance property of \(P_{W}\), namely \(P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\tilde{\mathbf{X}}\mathbf{Y}\mathbf{V}^{\intercal} )=P_{W}(\mathbf{S}-\tilde{\mathbf{X}}\mathbf{Y})\).

### Y Estimator

**Lemma 2**.: _Given the observation matrix \(\mathbf{S}\), let \(\hat{\mathbf{Y}}(\mathbf{S})\) be an estimator for \(\mathbf{Y}\). Then \(\hat{\mathbf{Y}}(\mathbf{S})\) is a RIE if and only if for any orthogonal matrices \(\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\):_

\[\hat{\mathbf{Y}}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal})=\mathbf{U}\hat{\mathbf{Y}}(\mathbf{S})\mathbf{V}^{\intercal} \tag{24}\]

Proof.: If \(\hat{\mathbf{Y}}(\mathbf{S})\) is a RIE, then this property clearly follows from the definition (4). Let us now show the converse.

Suppose that an estimator \(\hat{\mathbf{Y}}(\mathbf{S})\) satisfies (24). First, we show that if the observation matrix is diagonal, then the estimator is also diagonal. Consider the observation matrix to be \(\mathbf{S}^{\mathrm{diag}}=\left[\begin{array}{c|c}\mathrm{diag}(\mathrm{s}_{1}, \ldots,\mathrm{s}_{\mathrm{N}})&\mathbf{0}_{N\times(M-N)}\end{array}\right]\). Let \(\mathbf{I}_{k}^{-}\in\mathbb{R}^{N\times N},\mathbf{J}_{k}^{-}\in\mathbb{R}^{M\times M}\) be diagonal matrices with diagonal entries all one except the \(k\)-th entry which is \(-1\). Note that for \(1\leq k\leq N\), we have \(\mathbf{S}^{\mathrm{diag}}=\mathbf{I}_{k}^{-}\mathbf{S}^{\mathrm{diag}}\mathbf{J}_{k}^{-}\). Moreover, matrices \(\mathbf{I}_{k}^{-},\mathbf{J}_{k}^{-}\) are indeed orthogonal. For any \(1\leq k\leq N\), from the property we have:

\[\hat{\mathbf{Y}}(\mathbf{S}^{\mathrm{diag}})=\hat{\mathbf{Y}}(\mathbf{I}_{k}^{-}\mathbf{S}^{ \mathrm{diag}}\mathbf{J}_{k}^{-})=\mathbf{I}_{k}^{-}\hat{\mathbf{Y}}(\mathbf{S}^{\mathrm{diag} })\mathbf{J}_{k}^{-} \tag{25}\]

This implies that all entries on the \(k\)-th row and \(k\)-th column of \(\hat{\mathbf{Y}}(\mathbf{S}^{\mathrm{diag}})\) is zero except the \(k\)-th entry on the diagonal. Since this holds for any \(k\), we conclude that \(\hat{\mathbf{Y}}(\mathbf{S}^{\mathrm{diag}})\) is diagonal.

Now, for a given general observation matrix \(\mathbf{S}=\mathbf{U}_{S}\mathbf{\Gamma}\mathbf{V}_{S}^{\intercal}\), put \(\mathbf{U}=\mathbf{U}_{S}^{\intercal},\mathbf{V}=\mathbf{V}_{S}^{\intercal}\) in the property (24). We have:

\[\hat{\mathbf{Y}}(\mathbf{\Gamma})=\mathbf{U}_{S}^{\intercal}\hat{\mathbf{Y}}(\mathbf{S})\mathbf{V}_{S}\]

From the argument above, the matrix on the lhs is diagonal. Consequently, the matrix \(\mathbf{U}_{S}^{\intercal}\hat{\mathbf{Y}}(\mathbf{S})\mathbf{V}_{S}\) is diagonal which implies that the columns of \(\mathbf{U}_{S},\mathbf{V}_{S}\) are the left and right singular vectors of \(\hat{\mathbf{Y}}(\mathbf{S})\). Therefore, \(\hat{\mathbf{Y}}(\mathbf{S})\) is a RIE. 

Now, we prove that the posterior mean estimator \(\hat{\mathbf{Y}}^{*}(\mathbf{S})=\mathbb{E}[\mathbf{Y}|\mathbf{S}]\) has the property (24), and it is inside the RIE class. For simplicity, we drop the SNR factor \(\sqrt{\kappa}\). For any orthogonal matrices \(\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\), we have:

\[\mathbb{E}[\mathbf{Y}|\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}] =\frac{\int d\mathbf{X}\,d\hat{\mathbf{Y}}\,\hat{\mathbf{Y}}\,P_{X}(\mathbf{X})P_ {Y}(\tilde{\mathbf{Y}})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{X}\tilde{\mathbf{Y}}) }{\int d\mathbf{X}\,d\tilde{\mathbf{Y}}\,P_{X}(\mathbf{X})P_{Y}(\tilde{\mathbf{Y}})P_{W}(\mathbf{ U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{X}\tilde{\mathbf{Y}})}\] \[\stackrel{{\text{(a)}}}{{=}}\frac{\int d\mathbf{X}\,d \tilde{\mathbf{Y}}\,\mathbf{U}\mathbf{Y}\mathbf{V}^{\intercal}\,P_{X}(\mathbf{X})P_{Y}(\tilde{ \mathbf{Y}})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{X}\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V} ^{\intercal})}{\int d\mathbf{X}\,d\tilde{\mathbf{Y}}\,P_{X}(\mathbf{X})P_{Y}(\tilde{\mathbf{Y }})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{X}\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{ \intercal})}\] \[\stackrel{{\text{(b)}}}{{=}}\frac{\int d\mathbf{X}\,d \tilde{\mathbf{Y}}\,\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{\intercal}\,P_{X}(\mathbf{X})P_{Y}( \tilde{\mathbf{Y}})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\mathbf{X}\mathbf{U}^{ \intercal}\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{\intercal})}{\int d\mathbf{X}\,d\tilde{\mathbf{Y }}\,P_{X}(\mathbf{X})P_{Y}(\tilde{\mathbf{Y}})P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\bm {U}\mathbf{X}\mathbf{U}^{\intercal}\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{\intercal})}\] \[\stackrel{{\text{(c)}}}{{=}}\mathbf{U}\Big{\{}\frac{\int d \mathbf{X}\,d\tilde{\mathbf{Y}}\,\hat{\mathbf{Y}}\,P_{X}(\mathbf{X})P_{Y}(\tilde{\mathbf{Y}})P_{W} (\mathbf{S}-\mathbf{X}\tilde{\mathbf{Y}})}{\int d\mathbf{X}\,d\tilde{\mathbf{Y}}\,P_{X}(\mathbf{X})P_ {Y}(\tilde{\mathbf{Y}})P_{W}(\mathbf{S}-\mathbf{X}\tilde{\mathbf{Y}})}\Big{\}}\mathbf{V}^{\intercal}\] \[=\mathbf{U}\mathbb{E}[\mathbf{Y}|\mathbf{S}]\mathbf{V}^{\intercal}\]

where in (a), we changed variables \(\tilde{\mathbf{Y}}\to\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{\intercal}\), used \(|\det\mathbf{U}|=|\det\mathbf{V}|=1\), and bi-rotational invariance of \(P_{Y}\), \(P_{Y}(\tilde{\mathbf{Y}})=P_{Y}(\mathbf{U}\tilde{\mathbf{Y}}\mathbf{V}^{\intercal})\). In (b), we changed variables \(\mathbf{X}\to\mathbf{U}\mathbf{X}U^{\intercal}\), used \(|\det\mathbf{U}|=1\), and rotational invariance of \(P_{X}\), \(P_{X}(\mathbf{X})=P_{X}(\mathbf{U}\mathbf{X}U^{\intercal})\). In (c), we used the bi-rotational invariance property of \(P_{W}\), namely \(P_{W}(\mathbf{U}\mathbf{S}\mathbf{V}^{\intercal}-\mathbf{U}\mathbf{X}\tilde{\mathbf{Y}}\mathbf{V}^{ \intercal})=P_{W}(\mathbf{S}-\mathbf{X}\tilde{\mathbf{Y}})\).

## Appendix B The replica method for deriving the resolvent relation

In this section we present the replica method used to obtain the resolvent relation. For simplicity of notation we use \(\mathbf{G}(z)\equiv\mathbf{G}_{\mathcal{S}}(z)\) for the resolvent of a random matrix \(\mathbf{\mathcal{S}}\).

First, we express the entries of the resolvent \(\mathbf{G}(z)\) using the Gaussian integral representation of an inverse matrix [60]:

\[\begin{split} G_{ij}(z)&=\sqrt{\frac{1}{(2\pi)^{N+M} \det\left(z\mathbf{I}-\mathbf{\mathcal{S}}\right)}}\int\Big{(}\prod_{k=1}^{M+N}d\eta_{k }\Big{)}\,\eta_{i}\eta_{j}\,\exp\Big{\{}-\frac{1}{2}\mathbf{\eta}^{\intercal}\big{(} z\mathbf{I}-\mathbf{\mathcal{S}}\big{)}\mathbf{\eta}\Big{\}}\\ &=\frac{\int\Big{(}\prod_{k=1}^{M+N}d\eta_{k}\Big{)}\,\eta_{i}\eta _{j}\,\exp\Big{\{}-\frac{1}{2}\mathbf{\eta}^{\intercal}\big{(}z\mathbf{I}-\mathbf{\mathcal{ S}}\big{)}\mathbf{\eta}\Big{\}}}{\int\Big{(}\prod_{k=1}^{M+N}d\eta_{k}\Big{)}\, \exp\Big{\{}-\frac{1}{2}\mathbf{\eta}^{\intercal}\big{(}z\mathbf{I}-\mathbf{\mathcal{S}} \big{)}\mathbf{\eta}\Big{\}}}\end{split} \tag{26}\]

For \(z\) not close to the real axis, the resolvent is expected to exhibit self-averaging behavior in the limit of large N, meaning that it will not depend on the particular matrix realization. Thus, we can examine the resolvent \(\mathbf{G}_{\mathcal{S}}(z)\) by analyzing its ensemble average, denoted by \(\left\langle.\right\rangle\) in the following.

\[\left\langle G_{ij}(z)\right\rangle=\left\langle\frac{1}{\mathcal{Z}}\,\int \Big{(}\prod_{k=1}^{M+N}d\eta_{k}\Big{)}\,\eta_{i}\eta_{j}\,\exp\Big{\{}-\frac{ 1}{2}\mathbf{\eta}^{\intercal}\big{(}z\mathbf{I}-\mathbf{\mathcal{S}}\big{)}\mathbf{\eta}\Big{\}}\right\rangle \tag{27}\]

where \(\mathcal{Z}\) is the denominator in (26). Computing the average is, in general, non-trivial. However, the replica method provides us with a technique to overcome this issue by employing the following identity:

\[\begin{split}\left\langle G_{ij}(z)\right\rangle&= \lim_{n\to 0}\left\langle\mathcal{Z}^{n-1}\,\int\Big{(}\prod_{k=1}^{M+N}d \eta_{k}\Big{)}\,\eta_{i}\eta_{j}\,\exp\Big{\{}-\frac{1}{2}\mathbf{\eta}^{\intercal }\big{(}z\mathbf{I}-\mathbf{\mathcal{S}}\big{)}\mathbf{\eta}\Big{\}}\right\rangle\\ &=\lim_{n\to 0}\left\langle\,\int\Big{(}\prod_{k=1}^{M+N}\prod_{\tau=1}^{n}d \eta_{k}^{(\tau)}\Big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)}\,\exp\Big{\{}-\frac{1}{ 2}\sum_{\tau=1}^{n}\mathbf{\eta}^{(\tau)}{}^{\intercal}\big{(}z\mathbf{I}-\mathbf{\mathcal{ S}}\big{)}\mathbf{\eta}^{(\tau)}\Big{\}}\right\rangle\end{split} \tag{28}\]

So, the problem now is reduced to computation of an average over \(n\) copies (or replicas) of the initial system (26). After computing the average value (the bracket) in (28), we can perform an analytical continuation of the result to real values of \(n\) and then take the limit \(n\to 0\). Throughout, we assume as is common in the replica method, that the analytical continuation can be done with only \(n\) different sets of points. Of course, this is a totally uncontrolled step that comes with no guarantees.

## Appendix C Derivation of the RIE for \(\mathbf{X}\)

In this section, we consider estimating \(\mathbf{X}\), and treat both \(\mathbf{Y}\) and \(\mathbf{W}\) as noise. We consider \(\mathbf{X}\) to be fixed, and the observation model:

\[\mathbf{S}=\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{\intercal}+\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2 }^{\intercal} \tag{29}\]

where \(\mathbf{Y},\mathbf{W}\in\mathbb{R}^{N\times M}\) are fixed matrices with limiting singular value distribution \(\mu_{Y},\mu_{W}\), and \(\mathbf{U}_{1},\mathbf{U}_{2}\in\mathbb{R}^{N\times N},\mathbf{V}_{1},\mathbf{V}_{2}\in\mathbb{ R}^{M\times M}\) are independent random Haar matrices.

Construct the hermitization \(\mathbf{\mathcal{S}}\in\mathbb{R}^{(N+M)\times(N+M)}\) from \(\mathbf{S}\) as

\[\mathbf{\mathcal{S}}=\left[\begin{array}{cc}\mathbf{0}_{N\times N}&\mathbf{S}\\ \mathbf{S}^{\intercal}&\mathbf{0}_{M\times M}\end{array}\right]\]

For simplicity of notation, we use \(\mathbf{T}\equiv\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{\intercal}\), \(\mathbf{\mathcal{T}}\in\mathbb{R}^{(N+M)\times(N+M)}\) the hermitization of \(\mathbf{T}\), and \(\widetilde{\mathbf{\mathcal{W}}}\) denotes the hermitization of the matrix \(\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2}^{\intercal}\).

### Resolvent relation

We want to find a relation between \(\mathbf{G}(z)\equiv\mathbf{G}_{\mathcal{S}}(z)\), and the signal matrix \(\mathbf{X}\). From (28), we have

\[\langle G_{ij}(z)\rangle =\lim_{n\to\infty}\int\Big{(}\prod_{k=1}^{N+M}\prod_{\tau=1}^{n}d \eta_{k}^{(\tau)}\Big{)}\eta_{i}^{(1)}\eta_{j}^{(1)}\,\Big{\langle}\exp\big{\{} -\frac{1}{2}\sum_{\tau=1}^{n}{\mathbf{\eta}^{(\tau)}}^{\intercal}(z\mathbf{I}-\mathbf{ \mathcal{S}})\mathbf{\eta}^{(\tau)}\big{\}}\Big{\rangle}_{\mathbf{U}_{1},\mathbf{U}_{2}, \mathbf{V}_{1},\mathbf{V}_{2}} \tag{30}\] \[=\lim_{n\to\infty}\int\big{(}\prod_{k=1}^{N+M}\prod_{\tau=1}^{n}d \eta_{k}^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)}\exp\big{\{}-\frac{z}{2} \sum_{\tau=1}^{n}{\mathbf{\eta}^{(\tau)}}^{\intercal}\mathbf{\eta}^{(\tau)}\big{\}}\] \[\qquad\qquad\qquad\times\Big{\langle}\exp\big{\{}\frac{1}{2}\sum_ {\tau=1}^{n}{\mathbf{\eta}^{(\tau)}}^{\intercal}\mathbf{\mathcal{T}}\mathbf{\eta}^{(\tau)} \big{\}}\Big{\rangle}_{\mathbf{U}_{1},\mathbf{V}_{1}}\Big{\langle}\exp\big{\{}\frac{1} {2}\sum_{\tau=1}^{n}{\mathbf{\eta}^{(\tau)}}^{\intercal}\widetilde{\mathbf{\mathcal{W }}}\mathbf{\eta}^{(\tau)}\big{\}}\Big{\rangle}_{\mathbf{U}_{2},\mathbf{V}_{2}}\]

Split each replica \(\mathbf{\eta}^{(\tau)}\) into two vectors \(\mathbf{a}^{(\tau)}\in\mathbb{R}^{N},\mathbf{b}^{(\tau)}\in\mathbb{R}^{M}\), \(\mathbf{\eta}^{(\tau)}=\left[\begin{array}{c}\mathbf{a}^{(\tau)}\\ \mathbf{b}^{(\tau)}\end{array}\right]\). The exponent in the first bracket in (30) can be written as:

\[\mathbf{\eta}^{(\tau)}{}^{\intercal}\mathbf{\mathcal{T}}\mathbf{\eta}^{(\tau)} =\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{ \intercal}\mathbf{b}^{(\tau)}+\mathbf{b}^{(\tau)}{}^{\intercal}\mathbf{V}_{1}\mathbf{Y}^{ \intercal}\mathbf{U}_{1}^{\intercal}\mathbf{X}\mathbf{a}^{(\tau)} \tag{31}\] \[=2\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^ {\intercal}\mathbf{b}^{(\tau)}\] \[=2\operatorname{Tr}\mathbf{b}^{(\tau)}\mathbf{a}^{(\tau)}{}^{\intercal} \mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{\intercal}\]

Using the formula for the rectangular spherical integral [22] (see Theorem 2 in H.1), we find:

\[\Big{\langle}\exp\big{\{}\sum_{\tau=1}^{n}\operatorname{Tr}\mathbf{b}^{(\tau)}\mathbf{ a}^{(\tau)}{}^{\intercal}\mathbf{X}\mathbf{U}_{1}\mathbf{Y}\mathbf{V}_{1}^{\intercal}\Big{\}} \Big{\rangle}_{\mathbf{U}_{1},\mathbf{V}_{1}}\approx\exp\Big{\{}\frac{N}{2}\sum_{\tau= 1}^{n}\mathcal{Q}_{\mu_{\mathcal{V}}}^{(\alpha)}\big{(}\frac{1}{NM}\|\mathbf{X}\mathbf{ a}^{(\tau)}\|^{2}\|\mathbf{b}^{(\tau)}\|^{2}\big{)}\Big{\}} \tag{32}\]

with \(\mathcal{Q}_{\mu\nu}^{(\alpha)}(x)=\int_{0}^{x}\frac{\mathcal{C}_{\mu\nu}^{( \alpha)}(t)}{t}\,dt\). In (32), we used that \(\mathbf{b}^{(\tau)}\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{X}\) is a rank-one matrix with non-zero singular value \(\|\mathbf{b}^{(\tau)}\|\|\mathbf{X}\mathbf{a}^{(\tau)}\|\).

Similarly, for the second bracket in (30) we can write:

\[\mathbf{\eta}^{(\tau)}{}^{\intercal}\widetilde{\mathbf{\mathcal{W}}}\mathbf{ \eta}^{(\tau)} =\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2}^{ \intercal}\mathbf{b}^{(\tau)}+\mathbf{b}^{(\tau)}{}^{\intercal}\mathbf{V}_{2}\mathbf{W}^{ \intercal}\mathbf{U}_{2}^{\intercal}\mathbf{a}^{(\tau)} \tag{33}\] \[=2\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2}^{ \intercal}\mathbf{b}^{(\tau)}\] \[=2\operatorname{Tr}\mathbf{b}^{(\tau)}\mathbf{a}^{(\tau)}{}^{\intercal} \mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2}^{\intercal}\]

which using the formula of rectangular spherical integrals, implies

\[\Big{\langle}\exp\big{\{}\sum_{\tau=1}^{n}\operatorname{Tr}\mathbf{b}^{(\tau)}\mathbf{ a}^{(\tau)}{}^{\intercal}\mathbf{U}_{2}\mathbf{W}\mathbf{V}_{2}^{\intercal}\big{\}}\Big{\rangle}_ {\mathbf{U}_{2},\mathbf{V}_{2}}\approx\exp\Big{\{}\frac{N}{2}\sum_{\tau=1}^{n}\mathcal{Q }_{\mu\nu}^{(\alpha)}\big{(}\frac{1}{NM}\|\mathbf{a}^{(\tau)}\|^{2}\|\mathbf{b}^{( \tau)}\|^{2}\big{)}\Big{\}} \tag{34}\]

From (30), (32), (34), we find:

\[\langle G_{ij}(z)\rangle=\lim_{n\to\infty}\int\big{(}\prod_{k=1} ^{N+M}\prod_{\tau=1}^{n}d\eta_{k}^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)} \tag{35}\] \[\quad\times\exp\bigg{\{}-\frac{1}{2}\sum_{\tau=1}^{n}z\|\mathbf{\eta} ^{(\tau)}\|^{2}-N\mathcal{Q}_{\mu\nu}^{(\alpha)}\Big{(}\frac{\|\mathbf{X}\mathbf{a}^{( \tau)}\|^{2}\|\mathbf{b}^{(\tau)}\|^{2}}{NM}\Big{)}-N\mathcal{Q}_{\mu\nu}^{(\alpha)} \Big{(}\frac{\|\mathbf{a}^{(\tau)}\|^{2}\|\mathbf{b}^{(\tau)}\|^{2}}{NM}\Big{)}\bigg{\}}\]

Now, we introduce delta functions \(\delta\big{(}p_{1}^{(\tau)}-\frac{\|\mathbf{a}^{(\tau)}\|^{2}}{N}\big{)}\), \(\delta\big{(}p_{2}^{(\tau)}-\frac{\|\mathbf{b}^{(\tau)}\|^{2}}{M}\big{)}\), and \(\delta\big{(}p_{3}^{(\tau)}-\frac{\|\mathbf{X}\mathbf{a}^{(\tau)}\|^{2}}{N}\big{)}\), and using them, the integral in (35) can be written as (for brevity we drop the limit term):

\[\langle G_{ij}(z)\rangle=\int\big{(}\prod_{k=1}^{N+M}\prod_{\tau=1 }^{n}d\eta_{k}^{(\tau)}\big{)}\big{(}\prod_{\tau=1}^{n}d\eta_{1}^{(\tau)}\,d \eta_{2}^{(\tau)}\,d\eta_{3}^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)} \tag{36}\] \[\qquad\qquad\times\prod_{\tau=1}^{n}\delta\Big{(}p_{1}^{(\tau)}- \frac{\|\mathbf{a}^{(\tau)}\|^{2}}{N}\Big{)}\,\delta\Big{(}p_{2}^{(\tau)}-\frac{\| \mathbf{b}^{(\tau)}\|^{2}}{M}\Big{)}\,\delta\Big{(}p_{3}^{(\tau)}-\frac{\|\mathbf{X}\mathbf{a }^{(\tau)}\|^{2}}{N}\Big{)}\] \[\qquad\times\exp\Big{\{}-\frac{1}{2}\sum_{\tau=1}^{n}z\|\mathbf{\eta}^{ (\tau)}\|^{2}-N\mathcal{Q}_{\mu\nu}^{(\alpha)}(p_{2}^{(\tau)}p_{3}^{(\tau)})-N \mathcal{Q}_{\mu\nu}^{(\alpha)}\big{(}p_{1}^{(\tau)}p_{2}^{(\tau)}\big{)}\Big{\}}\]In the next step, we replace each delta with its Fourier transform, \(\delta\big{(}p_{1}^{\tau}-\frac{1}{N}\|\mathbf{a}^{\tau}\|^{2}\big{)}\propto\int\,d\zeta _{1}^{\tau}\exp\Big{\{}-\frac{N}{2}\zeta_{1}^{\tau}\big{(}p_{1}^{\tau}-\frac{1}{ N}\|\mathbf{a}^{\tau}\|^{2}\big{)}\Big{\}}\). After rearranging, we find:

\[\langle G_{ij}(z)\rangle\propto \int\big{(}\prod_{\tau=1}^{n}dp_{1}^{(\tau)}\,dp_{2}^{(\tau)}\,dp_{ 3}^{(\tau)}\,d\zeta_{1}^{(\tau)}\,d\zeta_{2}^{(\tau)}\,d\zeta_{3}^{(\tau)}\big{)}\] \[\times\exp\Big{\{}\frac{N}{2}\sum_{\tau=1}^{n}\mathcal{Q}_{\mu_{Y} }^{(\alpha)}(p_{2}^{(\tau)}p_{3}^{(\tau)})+\mathcal{Q}_{\mu_{W}}^{(\alpha)}(p_ {1}^{(\tau)}p_{2}^{(\tau)})-\zeta_{1}^{(\tau)}p_{1}^{(\tau)}-\frac{1}{\alpha} \zeta_{2}^{(\tau)}p_{2}^{(\tau)}-\zeta_{3}^{(\tau)}p_{3}^{(\tau)}\Big{\}}\] \[\times\int\big{(}\prod_{k=1}^{N+M}\,\prod_{\tau=1}^{n}dn_{k}^{( \tau)}\big{)}\,n_{i}^{(1)}\eta_{j}^{(1)}\] \[\times\exp\Big{\{}-\frac{1}{2}\sum_{\tau=1}^{n}z\|\mathbf{\eta}^{( \tau)}\|^{2}-\zeta_{1}^{(\tau)}\|\mathbf{a}^{(\tau)}\|^{2}-\zeta_{2}^{(\tau)}\|\mathbf{ b}^{(\tau)}\|^{2}-\zeta_{3}^{(\tau)}\|\mathbf{X}\mathbf{a}^{(\tau)}\|^{2}\Big{\}} \tag{37}\]

The inner integral in (37) is a Gaussian integral, and can be written as:

\[\int\big{(}\prod_{k=1}^{N+M}\,\prod_{\tau=1}^{n}d\eta_{k}^{(\tau)} \big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)}\] \[\times\exp\Bigg{\{}\sum_{\tau=1}^{n}-\frac{1}{2}\mathbf{\eta}^{(\tau) }\tau\left[\begin{array}{cc}(z-\zeta_{1}^{(\tau)})\mathbf{I}_{N}-\zeta_{3}^{( \tau)}\mathbf{X}^{2}&\mathbf{0}\\ \mathbf{0}&(z-\zeta_{2}^{(\tau)})\mathbf{I}_{M}\end{array}\right]\mathbf{\eta}^{(\tau)} \Bigg{\}} \tag{38}\]

Denote the matrix in the exponent by \(\mathbf{C}_{X}^{(\tau)}\). Its determinant reads:

\[\det\mathbf{C}_{X}^{(\tau)}=(z-\zeta_{2}^{(\tau)})^{M}\prod_{k=1}^{N}(z-\zeta_{1}^ {(\tau)}-\zeta_{3}^{(\tau)}\lambda_{k}^{2})\]

where \(\lambda_{k}\)'s are eigenvalues of \(\mathbf{X}\). So replacing the formula for the Gaussian integrals, (37) can be written as:

\[\langle G_{ij}(z)\rangle\propto\int\big{(}\prod_{\tau=1}^{n}dp_{1 }^{(\tau)}\,dp_{2}^{(\tau)}\,dp_{3}^{(\tau)}\,d\zeta_{1}^{(\tau)}\,d\zeta_{2}^ {(\tau)}\,d\zeta_{3}^{(\tau)}\big{)}\Big{(}\mathbf{C}_{X}^{(1)}^{-1}\Big{)}_{ij}\] \[\times\exp\big{\{}-\frac{Nn}{2}F_{0}^{X}(\mathbf{p}_{1},\mathbf{p}_{2}, \mathbf{p}_{3},\mathbf{\zeta}_{1},\mathbf{\zeta}_{2},\mathbf{\zeta}_{3})\big{\}} \tag{39}\]

with

\[F_{0}^{X}(\mathbf{p}_{1},\mathbf{p}_{2},\mathbf{p}_{3},\mathbf{\zeta}_{1},\mathbf{ \zeta}_{2},\mathbf{\zeta}_{3})=\frac{1}{n}\sum_{\tau=1}^{n}\bigg{[}\frac{1}{N} \sum_{k=1}^{N}\ln(z-\zeta_{1}^{(\tau)}-\zeta_{3}^{(\tau)}\lambda_{k}^{2})+ \frac{1}{\alpha}\ln(z-\zeta_{2}^{(\tau)})\] \[-\mathcal{Q}_{\mu_{Y}}^{(\alpha)}(p_{2}^{(\tau)}p_{3}^{(\tau)})- \mathcal{Q}_{\mu_{W}}^{(\alpha)}(p_{1}^{(\tau)}p_{2}^{(\tau)})+\zeta_{1}^{( \tau)}p_{1}^{(\tau)}+\frac{1}{\alpha}\zeta_{2}^{(\tau)}p_{2}^{(\tau)}+\zeta_{3 }^{(\tau)}p_{3}^{(\tau)}\bigg{]} \tag{40}\]

In the large \(N\) limit, the integral in (39) can be computed using the saddle-points of the function \(F_{0}^{X}\). In the evaluation of this integral, we use the _replica symmetric_ ansatz that assumes a saddle-point of the form:

\[\forall\tau\in\{1,\cdots,n\}:\quad\begin{cases}p_{1}^{\tau}=p_{1},&p_{2}^{\tau }=p_{2},&p_{3}^{\tau}=p_{3}\\ \zeta_{1}^{\tau}=\zeta_{1},&\zeta_{2}^{\tau}=\zeta_{2},&\zeta_{3}^{\tau}=\zeta _{3}\end{cases}\]

The saddle point is a solution of the set of equations:

\[\begin{cases}\zeta_{1}^{\tau}=\frac{\mathcal{C}_{\mu_{W}}^{(\alpha)}(p_{1}^{ \tau}\mathbf{p}_{2}^{\tau})}{p_{1}^{\tau}},&\zeta_{2}^{\ast}=\frac{\alpha}{p_{2}^{ \tau}}\big{(}\mathcal{C}_{\mu_{W}}^{(\alpha)}(p_{1}^{\ast}p_{2}^{\ast})+ \mathcal{C}_{\mu_{Y}}^{(\alpha)}(p_{2}^{\ast}p_{3}^{\ast})\big{)},&\zeta_{3}^{ \ast}=\frac{\mathcal{C}_{\mu_{Y}}^{(\alpha)}(p_{2}^{\ast}p_{3}^{\ast})}{p_{3} ^{\ast}}\\ p_{1}^{\ast}=\frac{1}{\zeta_{3}^{\ast}}\mathcal{G}_{\rho_{X^{2}}}\big{(} \frac{z-\zeta_{1}^{\ast}}{\zeta_{3}^{\ast}}\big{)},&p_{2}^{\ast}=\frac{1}{z- \zeta_{2}^{\ast}},&p_{3}^{\ast}=\frac{z-\zeta_{1}^{\ast}}{\zeta_{3}^{\ast}} \mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z-\zeta_{1}^{\ast}}{\zeta_{3}^{\ast}} \big{)}-\frac{1}{\zeta_{3}^{\ast}}\end{cases} \tag{41}\]Now, since the relation (39) and the solutions (41) hold for arbitrary indices \(i,j\), we can state the relation in matrix form. The inverse of \(\mathbf{C}_{X}^{*-1}\), and the block structure of \(\mathbf{G}_{\mathcal{S}}(z)\) are computed in sections H.2. From (111), (112) we have (for sufficiently large \(N\)):

\[\begin{split}\big{\langle}\mathbf{G}_{\mathcal{S}}(z)\big{\rangle}_{ \mathbf{U}_{1},\mathbf{U}_{2},\mathbf{V}_{1},\mathbf{V}_{2}}&=\left\langle\left[ \begin{array}{cc}\frac{1}{z}\mathbf{I}_{N}+\frac{1}{z}\mathbf{S}\mathbf{G}_{S^{\intercal} \mathcal{S}}(z^{2})\mathbf{S}^{\intercal}&\mathbf{S}\mathbf{G}_{S^{\intercal}\mathcal{S}}( z^{2})\\ \mathbf{G}_{S^{\intercal}\mathcal{S}}(z^{2})\mathbf{S}^{\intercal}&z\mathbf{G}_{S^{\intercal }\mathcal{S}}(z^{2})\end{array}\right]\right\rangle\\ &=\left[\begin{array}{cc}\frac{1}{\mathcal{G}_{\mathcal{S}}}\mathbf{G}_{X^{2}} \big{(}\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)}&\mathbf{0}\\ \mathbf{0}&\frac{1}{z-\zeta_{2}^{*}}\mathbf{I}_{M}\end{array}\right]\end{split} \tag{42}\]

With this relation, we proceed to simplify the equations (41).

The normalized trace of the upper-left blocks of \(\big{\langle}\mathbf{G}_{\mathcal{S}}(z)\big{\rangle}_{\mathbf{U}_{1},\mathbf{U}_{2},\mathbf{V }_{1},\mathbf{V}_{2}}\) is:

\[\begin{split}\frac{1}{N}\sum_{k=1}^{N}\big{[}\frac{1}{z}+\frac{1} {z}\frac{\gamma_{k}^{2}}{z^{2}-\gamma_{k}^{2}}\big{]}&=\frac{1}{z}\frac {1}{N}\sum_{k=1}^{N}\big{[}1+\frac{\gamma_{k}^{2}}{z^{2}-\gamma_{k}^{2}}\big{]} \\ &=z\frac{1}{N}\sum_{k=1}^{N}\frac{1}{z^{2}-\gamma_{k}^{2}}\\ &=\frac{1}{2N}\sum_{k=1}^{N}\big{[}\frac{1}{z-\gamma_{k}}+\frac{1} {z+\gamma_{k}}\big{]}=\mathcal{G}_{\bar{\mu}_{S}}(z)\end{split} \tag{43}\]

and the normalized trace of the upper-left block in \(\mathbf{C}_{X}^{*-1}\) is \(\frac{1}{\zeta_{3}^{*}}\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z-\zeta_{1}^{*}} {\zeta_{3}^{*}}\big{)}=p_{1}^{*}\). Therefore, we have \(p_{1}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z)\).

The normalized trace of lower-right block of \(\big{\langle}\mathbf{G}_{\mathcal{S}}(z)\big{\rangle}_{\mathbf{U}_{1},\mathbf{U}_{2},\mathbf{V }_{1},\mathbf{V}_{2}}\) reads:

\[\frac{1}{M}z\Big{[}\sum_{k=1}^{N}\frac{1}{z^{2}-\gamma_{k}^{2}}+(M-N)\frac{1}{z ^{2}}\Big{]}=\frac{N}{M}\mathcal{G}_{\bar{\mu}_{S}}(z)+\frac{M-N}{M}\frac{1}{z }=\alpha\mathcal{G}_{\bar{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z} \tag{44}\]

and the normalized trace of the lower-right block in \(\mathbf{C}_{X}^{*-1}\) is \(\frac{1}{z-\zeta_{2}^{*}}=p_{2}^{*}\). Therefore, we have \(p_{2}^{*}=\alpha\mathcal{G}_{\bar{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\). Moreover, we also have that \(\zeta_{2}^{*}=\alpha z\frac{z\mathcal{G}_{\bar{\mu}_{S}}(z)-1}{\alpha z \mathcal{G}_{\bar{\mu}_{S}}(z)+1-\alpha}\).

Therefore, the saddle point equations (41) can be rewritten in a simplified form, which does not involve \(\rho_{X^{2}}\), as:

\[\begin{split}\begin{cases}\zeta_{1}^{*}=\frac{\mathcal{C}_{\mu N }^{(\alpha)}(p_{1}^{*}p_{2}^{*})}{p_{1}^{*}},&\zeta_{2}^{*}=\alpha z\frac {z\mathcal{G}_{\bar{\mu}_{S}}(z)-1}{\alpha z\mathcal{G}_{\bar{\mu}_{S}}(z)+1- \alpha},&\zeta_{3}^{*}=\frac{\mathcal{C}_{\mu N}^{(\alpha)}(p_{2}^{*}p _{3}^{*})}{p_{3}^{*}}\\ \\ p_{1}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z),&p_{2}^{*}=\alpha\mathcal{G}_{ \bar{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z},&p_{3}^{*}=\frac{z-\zeta_{1}^{*}} {\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\zeta_{3}^{*}}\end{cases} \end{split} \tag{45}\]

Note that \(\zeta_{1}^{*},\zeta_{2}^{*}\) can be computed from the observation matrix, and we only need to find \(\zeta_{3}^{*}\) satisfying the following equation:

\[(z-\zeta_{1}^{*})\mathcal{G}_{\bar{\mu}_{S}}(z)-1=\mathcal{C}_{\mu\mathcal{Y}} ^{(\alpha)}\Big{(}\frac{1}{\zeta_{3}^{*}}\big{[}\alpha\mathcal{G}_{\bar{\mu}_ {S}}(z)+\frac{1-\alpha}{z}\big{]}\big{[}(z-\zeta_{1}^{*})\mathcal{G}_{\bar{ \mu}_{S}}(z)-1\big{]}\Big{)} \tag{46}\]

### Overlaps and optimal eigenvalues

We restate the relation between the resolvent and the overlaps from the main text (18). For \(\tilde{\mathbf{x}}_{i}=[\mathbf{x}_{i}^{\intercal},\mathbf{0}_{M}]^{\intercal}\) with \(\mathbf{x}_{i}\) eigenvectors of \(\mathbf{X}\), we have:

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}\approx\pi\bar{\mu}_{S}(x)O_{X}(x, \lambda_{i}) \tag{47}\]

From (47), (42), we find:

\[\begin{split} O_{X}(\gamma,\lambda_{i})&\approx\frac {1}{\pi\bar{\mu}_{S}(\gamma)}\,\mathrm{Im}\,\lim_{z\rightarrow\gamma-\mathrm{i}0^ {+}}\,\mathbf{x}_{i}^{\intercal}\,\zeta_{3}^{*-1}\mathbf{G}_{X^{2}}\big{(}\frac{z- \zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)}\,\mathbf{x}_{i}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma)}\,\mathrm{Im}\,\lim_{z \rightarrow\gamma-\mathrm{i}0^{+}}\,\frac{1}{z-\zeta_{1}^{*}-\zeta_{3}^{*} \lambda_{i}^{2}}\end{split} \tag{48}\]Once we have the overlap, we can compute the optimal eigenvalues from (14) in section 5. Note that, until now we had absorbed \(\sqrt{\kappa}\) into \(\mathbf{X}\). Therefore, we should use (48) with \(O_{X}(\gamma,\sqrt{\kappa}\lambda_{i})\). This leads to:

\[\widehat{\xi_{x}^{*}}_{i} \approx\frac{1}{N}\sum_{j=1}^{N}\lambda_{j}O_{X}(\gamma_{i},\sqrt{ \kappa}\lambda_{j}) \tag{49}\] \[\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\frac{1}{N}\sum_{j=1}^{N}\frac{\lambda_{ j}}{z-\zeta_{1}^{*}-\zeta_{3}^{*}\kappa\lambda_{j}^{2}}\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z \rightarrow\gamma_{i}-i0^{+}}\,\frac{1}{\kappa\zeta_{3}^{*}}\frac{1}{N}\sum_ {j=1}^{N}\frac{\frac{\lambda_{j}}{z-\zeta_{1}^{*}}-\lambda_{j}^{2}}{\kappa \zeta_{3}^{*}-\lambda_{j}^{2}}\] \[=\frac{1}{\kappa\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\frac{1}{\zeta_{3}^{*}}\bigg{(}\frac{1} {2}\frac{1}{N}\sum_{j=1}^{N}\frac{\frac{1}{\sqrt{\frac{z-\zeta_{1}^{*}}{\kappa \zeta_{3}^{*}}}}-\lambda_{j}}{-\frac{1}{2}\frac{1}{N}\sum_{j=1}^{N}\frac{1}{ \sqrt{\frac{z-\zeta_{1}^{*}}{\kappa\zeta_{3}^{*}}}+\lambda_{j}}}\bigg{)}\] \[\approx\frac{1}{\kappa\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{ Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}\bigg{\{}\frac{1}{2}\frac{1}{\zeta_{3}^{*}} \mathcal{G}_{\rho_{X}}\Big{(}\sqrt{\frac{z-\zeta_{1}^{*}}{\kappa\zeta_{3}^{*}} }\Big{)}-\frac{1}{2}\frac{1}{\zeta_{3}^{*}}\mathcal{G}_{\rho_{-X}}\Big{(}\sqrt {\frac{z-\zeta_{1}^{*}}{\kappa\zeta_{3}^{*}}}\Big{)}\bigg{\}}\] \[=\frac{1}{2\kappa\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\bigg{\{}\frac{1}{\zeta_{3}^{*}}\Big{[} \mathcal{G}_{\rho_{X}}\Big{(}\sqrt{\frac{z-\zeta_{1}^{*}}{\kappa\zeta_{3}^{*} }}\Big{)}+\mathcal{G}_{\rho_{X}}\Big{(}-\sqrt{\frac{z-\zeta_{1}^{*}}{\kappa \zeta_{3}^{*}}}\Big{)}\Big{]}\bigg{\}}\]

#### c.2.1 Estimating \(\mathbf{X}^{2}\)

The resolvent relation we have found in (42) is in terms of \(\mathbf{G}_{X^{2}}\). Therefore, like other RIES in other problems [19, 14], we can express the estimator for \(\mathbf{X}^{2}\) without any knowledge about \(\rho_{X}\) or \(\rho_{X^{2}}\). One can see that, the optimal RIE for \(\mathbf{X}^{2}\) is constructed in the same way as for \(\mathbf{X}\) with eigenvalues denoted by \(\widehat{\xi_{x^{2}i}^{*}}\). To compute the optimal eigenvalues, we absorb \(\sqrt{\kappa}\) into \(\mathbf{X}\) and we use the exact expression in (48). In the end, we only need to divide by \(\kappa\) to find an estimator for the true \(\mathbf{X}^{2}\).

\[\widehat{\xi_{x^{2}i}^{*}} \approx\frac{1}{N}\sum_{j=1}^{N}\lambda_{j}^{2}O_{X}(\gamma_{i}, \lambda_{j}) \tag{50}\] \[\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\frac{1}{N}\sum_{j=1}^{N}\frac{\lambda_{ j}^{2}}{z-\zeta_{1}^{*}-\zeta_{3}^{*}\lambda_{j}^{2}}\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z \rightarrow\gamma_{i}-i0^{+}}\,-\frac{1}{\zeta_{3}^{*}}\frac{1}{N}\sum_{j=1} ^{N}\frac{\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}-\lambda_{j}^{2}-\frac{z-\zeta_ {1}^{*}}{\zeta_{3}^{*}}}{\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}-\lambda_{j}^{ 2}}\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z \rightarrow\gamma_{i}-i0^{+}}\,-\frac{1}{\zeta_{3}^{*}}\frac{1}{N}\sum_{j=1} ^{N}\Big{[}1-\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\frac{1}{\frac{z-\zeta_{1}^{ *}}{\zeta_{3}^{*}}-\lambda_{j}^{2}}\Big{]}\] \[\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \lim_{z\rightarrow\gamma_{i}-i0^{+}}\,-\frac{1}{\zeta_{3}^{*}}+\frac{z-\zeta_ {1}^{*}}{\zeta_{3}^{*2}}\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z-\zeta_{1}^{*} }{\zeta_{3}^{*}}\big{)}\] \[\stackrel{{\text{(a)}}}{{=}}\frac{1}{\pi\bar{\mu}_{S }(\gamma_{i})}\operatorname{Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}\,p_{3}^{*}\] \[\stackrel{{\text{(b)}}}{{=}}\frac{1}{\pi\bar{\mu}_{S }(\gamma_{i})}\operatorname{Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\frac{z- \zeta_{1}^{*}}{\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\zeta_{3}^ {*}}\]

where in (a) we used (41), and for (b) we used (45). Thus, the optimal eigenvalues for \(\mathbf{X}^{2}\) read:

\[\widehat{\xi_{x^{2}i}^{*}}=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i} )}\operatorname{Im}\lim_{z\rightarrow\gamma_{i}-i0^{+}}\,\frac{z-\zeta_{1}^{*} }{\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\zeta_{3}^{*}} \tag{51}\]Note that the parameters \(\zeta_{1}^{*},\zeta_{3}^{*}\) can be computed from (45), (46), without the knowledge of \(\rho_{X}\) or \(\rho_{X^{2}}\).

**Remark 3**.: _The main barrier to find an estimator for \(\mathbf{X}\) is that the resolvent relation (42) is in terms of \(\mathcal{G}_{\rho_{X^{2}}}\). Moreover, in the estimator for \(\mathbf{X}\), second equality in (49), we have the sum \(\sum_{j=1}^{N}\frac{\lambda_{j}}{z-\zeta_{1}^{*}-\kappa\zeta_{3}^{*}\lambda_{j }^{2}}\) which cannot be written in terms of \(\mathcal{G}_{\rho_{X^{2}}}\)._

**Remark 4**.: _If we add the assumption that the matrix \(\mathbf{X}\) is positive semi-definite, without any further knowledge on the prior, we can use \(\sqrt{\xi_{\pi^{2}i}^{*}}\) for the eigenvalues of \(\mathbf{\Xi}_{X}(\mathbf{S})\). However, note that, this estimator is sub-optimal for \(\mathbf{X}\) as \(\sqrt{\sum_{j=1}^{N}\lambda_{j}^{2}\left(\mathbf{u}_{i}^{\intercal}\mathbf{x}_{j} \right)^{2}}\neq\sum_{j=1}^{N}\lambda_{j}\left(\mathbf{u}_{i}^{\intercal}\mathbf{x}_{ j}\right)^{2}\)._

### Numerical Examples

In this section, we will illustrate the derived formulas (42), (48), and (49) with numerical experiments.

We consider matrices \(\mathbf{Y},\mathbf{W}\in\mathbb{R}^{N\times M}\) to have i.i.d. Gaussian entries, so \(\mathcal{C}_{\mu_{Y}}^{(\alpha)}(z)=\mathcal{C}_{\mu_{W}}^{(\alpha)}(z)=\frac{ 1}{\alpha}z\) which leads to a simplification of saddle point equations (45):

\[\begin{cases}\zeta_{1}^{*}=\frac{1}{\alpha}p_{2}^{*},\quad\zeta_{2}^{*}=\alpha z \frac{z\mathcal{G}_{\mu_{S}}(z)-1}{\alpha z\mathcal{G}_{\mu_{S}}(z)+1-\alpha}, \quad\zeta_{3}^{*}=\frac{1}{\alpha}p_{2}^{*}\\ p_{1}^{*}=\mathcal{G}_{\tilde{\mu}_{S}}(z),\quad p_{2}^{*}=\alpha\mathcal{G}_{ \tilde{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z},\quad p_{3}^{*}=\frac{z-\zeta_{1}^{* }}{\zeta_{3}^{*}}\mathcal{G}_{\tilde{\mu}_{S}}(z)-\frac{1}{\zeta_{3}^{*}}\end{cases} \tag{52}\]

#### c.3.1 Resolvent relation

We take \(\kappa=1\). In model (29), without loss of generality we can consider \(\mathbf{X}\) to be diagonal. In figures 5 and 6 respectively, we consider the \(\mathbf{X}\) to be a diagonal matrix obtained by taking the eigenvalues of a Wigner matrix and a Wishart matrix respectively.

Note that \(\mu_{S}\) and \(\mathcal{G}_{\tilde{\mu}_{S}}(z)\) can be computed analytically using tools from random matrix theory, but the computation is highly involved. In our experiments, we use instead a numerical estimation of \(\mathcal{G}_{\tilde{\mu}_{S}}(z)\) obtained from the observation matrix with the help of a Cauchy kernel to compute the parameters \(\zeta_{1}^{*},\zeta_{3}^{*}\) (see section G, and [58] for details on the Cauchy kernel method).

Unlike the simpler models [15] for which the fluctuations are derived to be of the order \(\nicefrac{{1}}{{\sqrt{N}}}\), based on our derivation we cannot assess the order of fluctuations. However, from our numerics we observe that the fluctuations are of the order \(o(N)\). Moreover, fluctuations near the edge points of density are larger (in particular for the last row in both figures 5, 6), which is due to the fact that the limiting measures have higher fluctuations on their edge-points.

Another observation, from comparison of figures 5, 6, is that the fluctuations for the first example are relatively larger than the second one. One possible guess could be that this is due to the symmetry of \(\rho_{X}\) in the first example. However based on more extensive numerical observations (which we omit here) we speculate that this issue is in fact related to the existence of small eigenvalues of \(\mathbf{X}\). In other words, if \(\mathbf{X}\) has eigenvalue \(0\) or small eigenvalues, we have higher fluctuations in the relation (47).

#### c.3.2 Overlaps

To illustrate the formula for the overlap (48), we fix the matrix \(\mathbf{X}\) and run experiments over various realization of the model (29). For each experiment, we record the overlap of \(k\)-th left singular vector of \(\mathbf{S}\) and the eigenvectors of \(\mathbf{X}\). To compute the theoretical prediction, we find \(\zeta_{1}^{*}=\zeta_{3}^{*}\) for \(z=\bar{\gamma}_{k}-\mathfrak{i}0^{+}\) where \(\bar{\gamma}_{k}\) is the average of \(k\)-th singular value of \(\mathbf{S}\) in the experiments.

To find \(\zeta_{1}^{*}=\zeta_{3}^{*}\), we use the set of equations (41) which for \(\mathbf{Y},\mathbf{W}\) Gaussian can be written as:

\[\begin{cases}\zeta_{1}^{*}=\frac{1}{\alpha}p_{2}^{*},\quad\zeta_{2}^{*}=p_{1}^ {*}+p_{3}^{*},\quad\zeta_{3}^{*}=\frac{1}{\alpha}p_{2}^{*}\\ p_{1}^{*}=\frac{1}{\zeta_{1}^{*}}\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z}{ \zeta_{1}^{*}}-1\big{)},\quad p_{2}^{*}=\frac{1}{z-\zeta_{2}^{*}},\quad p_{3} ^{*}=\frac{z-\zeta_{1}^{*}}{\zeta_{1}^{*}}\mathcal{G}_{\rho_{X^{2}}}\big{(} \frac{z}{\zeta_{1}^{*}}-1\big{)}-\frac{1}{\zeta_{1}^{*}}\end{cases} \tag{53}\]

Now we proceed to simplify the solution above:

\[\zeta_{2}^{*}=p_{1}^{*}+p_{3}^{*}=\frac{z}{{\zeta_{1}^{*}}^{2}}\mathcal{G}_{ \rho_{X^{2}}}\big{(}\frac{z}{\zeta_{1}^{*}}-1\big{)}-\frac{1}{\zeta_{1}^{*}}\]Figure 5: Illustration of (42). \(\mathbf{X}\) is diagonal matrix from the eigenvalues of a Wigner matrix and \(\mathbf{Y},\mathbf{Z}\) are Gaussian matrices with \(N=2000,M=3000\). The empirical estimate of \(\mathbf{G}_{S}(z)\) (dashed blue line) is computed for \(z=\gamma_{i}-\mathrm{i}\sqrt{\frac{1}{2N}}\) for \(1\leq i\leq N\). Theoretical estimate (solid orange line) computed from the rhs of (42) with parameters obtained from the generated matrix. Note that, the theoretical estimate has also fluctuations because the parameters \(\zeta_{1}^{*},\zeta_{3}^{*}\) are given by the numerical estimate of \(\mathcal{G}_{\beta_{S}}(z)\).

Figure 6: Illustration of (42).\(\mathbf{X}\) is diagonal matrix from the eigenvalues of a Wishart matrix with aspect ratio \(\nicefrac{{1}}{{2}}\) and \(\mathbf{Y}\), \(\mathbf{Z}\) are Gaussian matrices with \(N=2000,M=3000\). The empirical estimate of \(\mathbf{G}_{\mathcal{S}}(z)\) (dashed blue line) is computed for \(z=\gamma_{i}-\mathrm{i}\sqrt{\frac{1}{2N}}\) for \(1\leq i\leq N\). The Theoretical estimate (solid orange line) is computed from the rhs of (42) with parameters obtained from the generated matrix. Note that, the theoretical estimate has also fluctuations because the parameters \(\zeta_{1}^{\star},\zeta_{3}^{\star}\) are given by the numerical estimate of \(\mathcal{G}_{\bar{\mu}_{S}}(z)\).

\[p_{2}^{*}=\frac{1}{z-\zeta_{2}^{*}}=\frac{\zeta_{1}^{*}}{\zeta_{1}^{*}z-\frac{z}{ \zeta_{1}^{*}}\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z}{\zeta_{1}^{*}}-1\big{)}+1}\]

\[\zeta_{1}^{*}=\frac{1}{\alpha}p_{2}^{*} \Longrightarrow\zeta_{1}^{*}z-\frac{z}{\zeta_{1}^{*}}\mathcal{G}_{ \rho_{X^{2}}}\big{(}\frac{z}{\zeta_{1}^{*}}-1\big{)}+1=\frac{1}{\alpha} \tag{54}\] \[\Rightarrow\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z}{\zeta_{1}^{* }}-1\big{)}={\zeta_{1}^{*}}^{2}+\big{(}1-\frac{1}{\alpha}\big{)}\frac{\zeta_{ 1}^{*}}{z}\] \[\Rightarrow\frac{z}{\zeta_{1}^{*}}-1=\mathcal{G}_{\rho_{X^{2}}}^{ -1}\big{(}{\zeta_{1}^{*}}^{2}+\big{(}1-\frac{1}{\alpha}\big{)}\frac{\zeta_{1}^ {*}}{z}\big{)}\] \[\Rightarrow\frac{z}{\zeta_{1}^{*}}-1-\frac{1}{{\zeta_{1}^{*}}^{2} +\big{(}1-\frac{1}{\alpha}\big{)}\frac{\zeta_{1}^{*}}{z}}=\mathcal{R}_{\rho_{X^ {2}}}\Big{(}{\zeta_{1}^{*}}^{2}+\big{(}1-\frac{1}{\alpha}\big{)}\frac{\zeta_{1 }^{*}}{z}\Big{)}\]

Thus, \(\zeta_{1}^{*}\) is the solution to (54). For each example, we solve this equation and compare the obtained theoretical overlap against the average over the experiments.

Wigner \(\mathbf{X}\).Let \(\mathbf{X}\in\mathbb{R}^{N\times N}\) be a Wigner matrix, then \(\mathcal{R}_{\rho_{X^{2}}}(z)=\frac{1}{1-z}\). Solving (54), we can compute the overlap using (48). In Fig. 6(a), we compare the theoretical computation with simulations for \(N=1000,M=2000\). As in previous cases \(\bar{\mu}_{S}(\gamma)\) is approximated using a Cauchy kernel [58].

Square root Wishart \(\mathbf{X}\).Let \(\mathbf{X}\in\mathbb{R}^{N\times N}\) be the square root of a Wishart matrix \(\mathbf{X}=\sqrt{\frac{1}{N}\mathbf{H}\mathbf{H}^{\intercal}}\) with \(\mathbf{H}\in\mathbb{R}^{N\times N^{\prime}}\) having i.i.d. Gaussian entries. Then \(\mathcal{R}_{\rho_{X^{2}}}(z)=\frac{1}{\alpha^{\prime}}\frac{1}{1-z}\), \(\alpha^{\prime}=\nicefrac{{N}}{{N^{\prime}}}\). Solving (54), we can compute the overlap using (48). In Fig. 6(b), we compare the theoretical computation with simulations for \(N=1000,N^{\prime}=4000,M=2000\).

#### c.3.3 RIE performance

In this section, we investigate the performance of our proposed estimators for \(\mathbf{X}\). We compare performances of the optimal RIE (49) with the one of Oracle estimator (3). Moreover, we illustrate the performance of the estimator for \(\mathbf{X}^{2}\) (50), and the sub-optimal estimator of \(\mathbf{X}\) derived from it, see remark 4.

Figure 7: Computation of the rescaled overlap. Both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and aspect ratio \(N/M=1/2\). The simulation results are averaged over 1000 experiments with fixed \(\mathbf{X}\), and \(N=1000,M=2000\). Some of the simulation points are dropped for clarity.

For \(\mathbf{Y}\), \(\mathbf{W}\) with Gaussian i.i.d. entries, (51) simplifies to:

\[\widehat{\xi_{x^{2}i}^{\star_{2}}} =\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\;\mathrm{Im }\;\lim_{z\rightarrow\gamma_{i}-i0^{+}}\;\frac{z-\zeta_{3}^{\star}}{\zeta_{3}^ {\star}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\zeta_{3}^{\star}}\] \[=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\;\mathrm{ Im}\;\lim_{z\rightarrow\gamma_{i}-i0^{+}}\;\frac{z}{\zeta_{1}^{\star}}\mathcal{G}_ {\bar{\mu}_{S}}(z)-\mathcal{G}_{\bar{\mu}_{S}}-\frac{1}{\zeta_{1}^{\star}}\] \[=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\;\mathrm{ Im}\;\lim_{z\rightarrow\gamma_{i}-i0^{+}}\;\frac{z}{\mathcal{G}_{\bar{\mu}_{S}}(z)+ \frac{1-\alpha}{\alpha}\frac{1}{z}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\mathcal{G} _{\bar{\mu}_{S}}(z)-\frac{1}{\mathcal{G}_{\bar{\mu}_{S}}(z)+\frac{1-\alpha}{ \alpha}\frac{1}{z}}\] \[=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\;\mathrm{ Im}\left\{\frac{\gamma_{i}}{\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+\pi\bar{\mu}_{S}( \gamma_{i})+\frac{1-\alpha}{\alpha}\frac{1}{\gamma_{i}}}\big{(}\pi\mathsf{H}[ \bar{\mu}_{S}](\gamma_{i})+\pi i\bar{\mu}_{S}(\gamma_{i})\big{)}\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\left.\big{(} \pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+\pi i\bar{\mu}_{S}(\gamma_{i})\big{)} -\frac{1}{\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+\pi i\bar{\mu}_{S}(\gamma_ {i})+\frac{1-\alpha}{\alpha}\frac{1}{\gamma_{i}}}\right\}\] \[=\frac{1}{\kappa}\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\;\pi\bar{ \mu}_{S}(\gamma_{i})\bigg{(}-1+\frac{1}{\alpha\Big{(}\pi^{2}\bar{\mu}_{S}( \gamma_{i})^{2}+\big{(}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+\frac{-1+\frac {1}{\alpha}}{\gamma_{i}}\big{)}^{2}\Big{)}}\bigg{)}\] \[=\frac{1}{\kappa}\left[-1+\frac{1}{\alpha\Big{(}\pi^{2}\bar{\mu} _{S}(\gamma_{i})^{2}+\big{(}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+\frac{-1 +\frac{1}{\alpha}}{\gamma_{i}}\big{)}^{2}\Big{)}}\right] \tag{55}\]

For our first example, we consider two priors for \(\mathbf{X}\):

Shifted Wigner \(\mathbf{X}\).We consider \(\mathbf{X}=\mathbf{F}+c\mathbf{I}\) where \(\mathbf{F}=\mathbf{F}^{\intercal}\in\mathbb{R}^{N\times N}\) has i.i.d. entries with variance \(\nicefrac{{1}}{{N}}\), and \(c\neq 0\) is a real number. Then, the spectrum of \(\mathbf{X}\) is a shifted version of the Wigner law

\[\rho_{X}(\lambda)=\frac{\sqrt{4-(\lambda-c)^{2}}}{2\pi},\quad\text{for }c-2< \lambda<c+2,\]

and the Stieltjes transform reads:

\[\mathcal{G}_{\rho_{X}}(z)=\frac{z-c-\sqrt{(z-2-c)(z+2-c)}}{2}\]

Wishart \(\mathbf{X}\).Take \(\mathbf{X}=\frac{1}{N}\mathbf{H}\mathbf{H}^{\intercal}\) with \(\mathbf{H}\in\mathbb{R}^{N\times N^{\prime}}\) having i.i.d. Gaussian entries, with \(\nicefrac{{N}}{{N^{\prime}}}=\alpha^{\prime}\leq 1\). Then, the spectrum of \(\mathbf{X}\) is the renowned _Marchenko-Pastur_ distribution:

\[\rho_{X}(\lambda)=\frac{\sqrt{\Big{[}\lambda-\big{(}\frac{1}{\sqrt{\alpha^{ \prime}}}-1\big{)}^{2}\Big{]}\Big{[}\big{(}\frac{1}{\sqrt{\alpha^{\prime}}}+1 \big{)}^{2}-\lambda\Big{]}}}{2\pi\lambda},\quad\text{for}\;\big{(}\frac{1}{ \sqrt{\alpha^{\prime}}}-1\big{)}^{2}<\lambda<\big{(}\frac{1}{\sqrt{\alpha^{ \prime}}}+1\big{)}^{2},\]

and the Stieltjes transform reads:

\[\mathcal{G}_{\rho_{X}}(z)=\frac{z-\big{(}\frac{1}{\alpha^{\prime}}-1\big{)}- \sqrt{\Big{[}z-\big{(}\frac{1}{\sqrt{\alpha^{\prime}}}-1\big{)}^{2}\Big{]} \Big{[}z-\big{(}\frac{1}{\sqrt{\alpha^{\prime}}}+1\big{)}^{2}\Big{]}}}{2z}\]

In Figure 8, the MSE of Oracle estimator, RIE (49), and \(\sqrt{\mathbf{X}^{2}}\)-RIE is illustrated for shifted Wigner \(\mathbf{X}\) with \(c=3\), and Wishart with aspect-ratio \(\alpha^{\prime}=\nicefrac{{1}}{{4}}\). We see that the performance of RIE is close to the one of Oracle estimator, which implies the optimality of the proposed estimator (49). Moreover, we observe the sub-optimality of estimating \(\mathbf{X}\) using \(\sqrt{\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})}\). Note that, in the low-SNR regime, the estimated eigenvalues \(\widehat{\xi_{x^{2}i}}\) might be negative which makes the estimator \(\sqrt{\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})}\) undefined, so the MSE is not depicted in this case.

In Figure 9, the MSE of estimating \(\mathbf{X}^{2}\) is shown. We see that in the high-SNR regimes the RIE (55) has the same performance as the Oracle estimator.

Bernoulli spectral distribution.In this case, the matrix \(\mathbf{X}\) is constructed as \(\mathbf{X}=\mathbf{U}_{X}\mathbf{\Lambda}\mathbf{U}_{X}^{\mathsf{T}}\) with \(\mathbf{U}_{X}\) a \(N\times N\) orthogonal matrix distributed according to Haar measure on orthogonal matrices, and \(\mathbf{\Lambda}=\operatorname{diag}(\mathbf{\lambda})\) where \(\mathbf{\lambda}\) has i.i.d. Bernoulli elements. Thus, \(\rho_{X}=p\delta_{0}+(1-p)\delta_{+1}\) for \(p\in(0,1)\), and the Stieltjes transform is:

\[\mathcal{G}_{\rho_{X}}(z)=p\frac{1}{z}+(1-p)\frac{1}{z-1}\]

For this prior, we have that \(\mathbf{X}=\mathbf{X}^{2}\), so both estimators \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\) and \(\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})\) should have the same performance. However, note that \(\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})\) does not use any knowledge of \(\rho_{X}\). In Figure 10, the MSE is illustrated for these two estimators for two sparsity parameter, \(p=0.5\) and \(0.9\). We observe that, except for for the low-SNR regimes, both estimators have the same MSE. The poor performance of \(\widehat{\mathbf{\Xi}_{X^{2}}}(\mathbf{S})\) in the low-SNR regimes might be due to the fact that, some of the estimated eigenvalues \(\widehat{\xi_{x^{2}}}_{x^{2}}\) are negative although the true eigenvalue is \(0\). This makes the estimation more difficult for the sparser prior, see Figure 9(b). However, this problem is resolved in \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\) by taking the knowledge of \(\mathcal{G}_{\rho_{X}}(z)\) into account.

Effect of aspect-ratio \(\alpha\).In Figure 11, we consider \(\mathbf{X}\) to be shifted Wigner with \(c=3\), and the MSE is depicted for various values of the aspect-ratio \(\alpha\). As expected, as \(M\) increases (\(\alpha\) decreases) and we have more observation or more data samples, the estimation error decreases.

Figure 8: Estimating \(\mathbf{X}\). The MSE is normalized by the norm of the signal, \(\|\mathbf{X}\|_{\mathrm{F}}^{2}\). Both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and aspect ratio \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\) and Oracle estimator is also reported.

## Appendix D Estimating \(\mathbf{Y}\)

In this section, we present the derivation of the optimal RIE for \(\mathbf{Y}\). For simplicity, the SNR parameter in (1) is absorbed into \(\mathbf{Y}\), so the model is \(\mathbf{S}=\mathbf{X}\mathbf{Y}+\mathbf{W}\). Therefore, the final estimator should be divided by \(\nicefrac{{1}}{{\sqrt{\kappa}}}\) to give an estimate of the original \(\mathbf{Y}\).

The optimal singular values are constructed as \(\xi_{y_{i}}^{*}=\sum_{j=1}^{N}\sigma_{j}\left(\mathbf{u}_{i}^{\intercal}\mathbf{y}_{j} ^{(l)}\right)\bigl{(}\mathbf{v}_{i}^{\intercal}\mathbf{y}_{j}^{(r)}\bigr{)}\). We assume that, for large \(N\), \(\xi_{y_{i}}^{*}\) can be approximated by its expectation:

\[\widehat{\xi}_{y_{i}}^{*}\approx\sum_{j=1}^{N}\sigma_{j}\operatorname{\mathbb{ E}}\Bigl{[}\bigl{(}\mathbf{u}_{i}^{\intercal}\mathbf{y}_{j}^{(l)}\bigr{)}\bigl{(}\mathbf{v}_{i}^ {\intercal}\mathbf{y}_{j}^{(r)}\bigr{)}\Bigr{]}\]

where the expectation is over the singular vectors of the observation matrix \(\mathbf{S}\). Therefore, to compute the optimal singular values, we need to find the mean overlap \(\operatorname{\mathbb{E}}\Bigl{[}\bigl{(}\mathbf{u}_{i}^{\intercal}\mathbf{y}_{j}^{(l )}\bigr{)}\bigl{(}\mathbf{v}_{i}^{\intercal}\mathbf{y}_{j}^{(r)}\bigr{)}\Bigr{]}\) between singular vectors of \(\mathbf{Y}\) and singular vectors of \(\mathbf{S}\). In the following we will see that a rescaling of this quantity can be expressed in terms of \(i\)-th singular value of \(\mathbf{S}\) and \(j\)-th singular value of \(\mathbf{Y}\) (and the limiting measures, indeed). Thus, we will use the notation \(O_{Y}(\gamma_{i},\sigma_{j}):=N\operatorname{\mathbb{E}}\Bigl{[}\bigl{(}\mathbf{u }_{i}^{\intercal}\mathbf{y}_{j}^{(l)}\bigr{)}\bigl{(}\mathbf{v}_{i}^{\intercal}\mathbf{y} _{j}^{(r)}\bigr{)}\Bigr{]}\) in what follows. In the nest section, we discuss how the overlap can be computed from the resolvent of the Hermitized matrix of \(\mathbf{S}\).

Figure 11: MSE of estimating \(\mathbf{X}\) as a function of aspect-ratio \(\alpha\), prior on \(\mathbf{X}\) is shifted Wigner with \(c=3\), and \(\kappa=5\). MSE is normalized by the norm of the signal, \(\|\mathbf{X}\|_{F}^{2}\). Both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\). The RIE is applied to \(N=2000,M=\nicefrac{{1}}{{\alpha}}N\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\overline{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\) and Oracle estimator is also reported.

### Relation between overlap and the resolvent

Construct the matrix \(\mathbf{\mathcal{S}}\in\mathbb{R}^{(N+M)\times(N+M)}\) from the observation matrix:

\[\mathbf{\mathcal{S}}=\left[\begin{array}{cc}\mathbf{0}_{N\times N}&\mathbf{\mathcal{S}}\\ \mathbf{\mathcal{S}}^{\intercal}&\mathbf{0}_{M\times M}\end{array}\right]\]

By Theorem 7.3.3 in [59], \(\mathbf{\mathcal{S}}\) has the following eigen-decomposition:

\[\mathbf{\mathcal{S}}=\left[\begin{array}{cc}\hat{\mathbf{U}}_{S}&\hat{\mathbf{U}}_{S}& \mathbf{0}\\ \hat{\mathbf{V}}_{S}^{(1)}&-\hat{\mathbf{V}}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array} \right]\left[\begin{array}{cc}\mathbf{\Gamma}_{N}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&-\Gamma_{N}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}&\mathbf{0}\end{array}\right]\left[\begin{array}{cc}\hat{\mathbf{U}}_{S }&\hat{\mathbf{U}}_{S}&\mathbf{0}\\ \hat{\mathbf{V}}_{S}^{(1)}&-\hat{\mathbf{V}}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array} \right]^{\intercal} \tag{56}\]

with \(\mathbf{V}_{S}=\left[\begin{array}{cc}\mathbf{V}_{S}^{(1)}&\mathbf{V}_{S}^{(2)}\end{array}\right]\) in which \(\mathbf{V}_{S}^{(1)}\in\mathbb{R}^{M\times N}\). And, \(\hat{\mathbf{V}}_{S}^{(1)}=\frac{1}{\sqrt{2}}\mathbf{V}_{S}^{(1)}\), \(\hat{\mathbf{U}}_{S}=\frac{1}{\sqrt{2}}\mathbf{U}_{S}\). Eigenvalues of \(\mathbf{\mathcal{S}}\) are signed singular values of \(\mathbf{S}\), therefore the limiting eigenvalue distribution of \(\mathbf{\mathcal{S}}\) (ignoring zero eigenvalues) is the same as the limiting symmetrized singular value distribution of \(\mathbf{\mathcal{S}}\).

Define the resolvent of \(\mathbf{\mathcal{S}}\)

\[\mathbf{G}_{\mathcal{S}}(z)=\left(\begin{array}{cc}z\mathbf{I}-\mathbf{\mathcal{S}}& \end{array}\right)^{-1}\]

Denote the eigenvectors of \(\mathbf{\mathcal{S}}\) by \(\mathbf{s}_{i}\in\mathbb{R}^{M+N}\), \(i=1,\ldots,M+N\). For \(z=x-\mathrm{i}\epsilon\) with \(x\in\mathbb{R}\) and \(\epsilon\gg\frac{1}{N}\), we have:

\[\mathbf{G}_{\mathcal{S}}(x-\mathrm{i}\epsilon)=\sum_{k=1}^{2N}\frac{x+\mathrm{i} \epsilon}{(x-\tilde{\gamma}_{k})^{2}+\epsilon^{2}}\mathbf{s}_{k}\mathbf{s}_{k} ^{\intercal}+\frac{x+\mathrm{i}\epsilon}{x^{2}+\epsilon^{2}}\sum_{k=2N+1}^{N+ M}\mathbf{s}_{k}\mathbf{s}_{k}^{\intercal}\]

where \(\tilde{\gamma}_{k}\) are the eigenvalues of \(\mathbf{\mathcal{S}}\), which are in fact the (signed) singular values of \(\mathbf{\mathcal{S}}\), \(\tilde{\gamma}_{1}=\gamma_{1},\ldots,\tilde{\gamma}_{N}=\gamma_{N},\tilde{ \gamma}_{N+1}=-\gamma_{1},\ldots,\tilde{\gamma}_{2N}=-\gamma_{N}\).

Define the vectors \(\mathbf{r}_{i}=\left[\begin{array}{c}\mathbf{0}_{N}\\ \mathbf{y}_{i}^{(r)}\end{array}\right]\), \(\mathbf{l}_{i}=\left[\begin{array}{c}\mathbf{y}_{i}^{(l)}\\ \mathbf{0}_{M}\end{array}\right]\) for \(\mathbf{y}_{i}^{(r)},\mathbf{y}_{i}^{(l)}\) right/ left singular vectors of \(\mathbf{\mathcal{Y}}\), we have

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x-\mathrm{i} \epsilon)\big{)}\mathbf{l}_{i}=\sum_{k=1}^{2N}\frac{\epsilon}{(x-\tilde{\gamma}_{ k})^{2}+\epsilon^{2}}\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k}\big{)}\big{(}\mathbf{l }_{i}^{\intercal}\mathbf{s}_{k}\big{)}+\frac{x+\mathrm{i}\epsilon}{x^{2}+ \epsilon^{2}}\sum_{k=2N+1}^{N+M}\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k} \big{)}\big{(}\mathbf{l}_{i}^{\intercal}\mathbf{s}_{k}\big{)} \tag{57}\]

Given the structure of \(\mathbf{s}_{k}\)'s in (56), we have:

\[\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k}\big{)}\big{(}\mathbf{l}_{i}^{\intercal }\mathbf{s}_{k}\big{)}=\begin{cases}\frac{1}{2}\big{(}\mathbf{u}_{k}^{\intercal }\mathbf{y}_{i}^{(l)}\big{)}\big{(}\mathbf{v}_{k}^{\intercal}\mathbf{y}_{i}^{(r)}\big{)}& \mathrm{for}\;\;1\leq k\leq N\\ -\frac{1}{2}\big{(}\mathbf{u}_{k-N}^{\intercal}\mathbf{y}_{i}^{(l)}\big{)}\big{(}\mathbf{v }_{k-N}^{\intercal}\mathbf{y}_{i}^{(r)}\big{)}&\mathrm{for}\;\;N+1\leq k\leq 2N\\ 0&\mathrm{for}\;\;2N+1\leq k\leq N+M\end{cases}\]

In the limit of large N, the latter quantity is also self-averaging, due to the fact that as \(N\to\infty\), these overlaps exhibit asymptotic independence, enabling the law of large numbers to be applied here. We can thus state that:

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x-\mathrm{i} \epsilon)\big{)}\mathbf{l}_{i}\xrightarrow[]{N\to\infty}\int_{\mathbb{R}}\frac{ \epsilon}{(x-t)^{2}+\epsilon^{2}}O_{Y}(t,\sigma_{i})\bar{\mu}_{S}(t)\,dt \tag{58}\]

where the overlap function \(O_{Y}(t,\lambda_{i})\) is extended (continuously) to arbitrary values within the support of \(\bar{\mu}_{S}\) with the property that \(O_{Y}(-t,\lambda_{i})=-O_{Y}(t,\lambda_{i})\) for \(t\in\mathrm{supp}(\mu_{S})\). Sending \(\epsilon\to 0\), we find

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x-\mathrm{i} \epsilon)\big{)}\mathbf{l}_{i}\approx\pi\bar{\mu}_{S}(x)O_{Y}(x,\sigma_{i}) \tag{59}\]

In the next section, we establish a connection between the resolvent \(\mathbf{G}_{\mathcal{S}}(z)\) and the signal \(\mathbf{Y}\), which enables us to determine the overlap and consequently the optimal singular values values \(\widehat{\xi}_{y_{i}}^{\epsilon}\) in terms of the singular values of the observation matrix \(\mathbf{\mathcal{S}}\).

### Resolvent relation for \(\mathbf{Y}\)

In this section, we consider estimating \(\mathbf{Y}\), and treat both \(\mathbf{X}\) and \(\mathbf{W}\) as noise. We consider the model to be:

\[\mathbf{S}=\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal}\mathbf{Y}+\mathbf{U}\mathbf{W}\mathbf{V}^{\intercal} \tag{60}\]

where \(\mathbf{X}=\mathbf{X}^{\intercal}\in\mathbb{R}^{N\times N},\mathbf{W}\in\mathbb{R}^{N \times M}\) are fixed matrices with limiting eigenvalue/singular value distribution \(\rho_{X},\mu_{W}\), and \(\mathbf{O},\mathbf{U}\in\mathbb{R}^{N\times N},\mathbf{V}\in\mathbb{R}^{M\times M}\) are independent random Haar matrices. For simplicity of notation, we use \(\mathbf{T}\equiv\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal}\mathbf{Y}\), and \(\mathbf{\mathcal{T}}\in\mathbb{R}^{(N+M)\times(N+M)}\) the hermitization of \(\mathbf{T}\). And \(\widetilde{\mathbf{\mathcal{W}}}\) denotes the hermitization of the matrix \(\mathbf{U}\mathbf{W}\mathbf{V}^{\intercal}\).

As in the case for \(\mathbf{X}\), we express the entries of \(\mathbf{G}(z)\equiv\mathbf{G}_{\mathcal{S}}(z)\) using Gaussian integral representation, and after applying the replica trick (28), we find:

\[\langle G_{ij}(z)\rangle =\lim_{n\to\infty}\int\Big{(}\prod_{k=1}^{N+M}\prod_{\tau=1}^{n}d \eta_{k}^{(\tau)}\Big{)}\eta_{i}^{(1)}\eta_{j}^{(1)}\Big{\langle}\exp\big{\{}- \frac{1}{2}\sum_{\tau=1}^{n}\mathbf{\eta}^{(\tau)\intercal}(z\mathbf{I}-\mathbf{\mathcal{S }})\mathbf{\eta}^{(\tau)}\big{\}}\Big{\rangle}_{\mathbf{O},\mathbf{U},\mathbf{V}} \tag{61}\]

Split each replica \(\mathbf{\eta}^{(\tau)}\) into two vectors \(\mathbf{a}^{(\tau)}\in\mathbb{R}^{N},\mathbf{b}^{(\tau)}\in\mathbb{R}^{M},\mathbf{\eta}^{( \tau)}=\left[\begin{array}{c}\mathbf{a}^{(\tau)}\\ \mathbf{b}^{(\tau)}\end{array}\right]\). The exponent in the first bracket in (61) can be written as :

\[\mathbf{\eta}^{(\tau)\intercal}\mathbf{\mathcal{T}}\mathbf{\eta}^{(\tau)} =\mathbf{a}^{(\tau)\intercal}\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal}\mathbf{Y}\mathbf{ b}^{(\tau)}+\mathbf{b}^{(\tau)\intercal}\mathbf{Y}^{\intercal}\mathbf{O}\mathbf{X}\mathbf{O}^{ \intercal}\mathbf{a}^{(\tau)} \tag{62}\] \[=\operatorname{Tr}\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal}\big{(}\underbrace {\mathbf{Y}\mathbf{b}^{(\tau)}\mathbf{a}^{(\tau)\intercal}+\mathbf{a}^{(\tau)}\mathbf{b}^{(\tau) \intercal}\mathbf{Y}}_{\widetilde{\mathbf{Y}}^{(\tau)}}\big{)}\]

where \(\widetilde{\mathbf{Y}}^{(\tau)}\) is a symmetric \(N\times N\) matrix with two non-zero eigenvalues \(\mathbf{a}^{(\tau)\intercal}\mathbf{Y}\mathbf{b}^{(\tau)}\pm\|\mathbf{a}^{(\tau)}\|\|\mathbf{Y} \mathbf{b}^{(\tau)}\|\) by lemma 3.

Using the formula for the spherical integral [21] (see Theorem 1 in H.1), we find:

\[\Big{\langle}\exp\big{\{}\frac{1}{2}\sum_{\tau=1}^{n}\operatorname {Tr}\mathbf{O}\mathbf{X}\mathbf{O}^{\intercal}\widetilde{\mathbf{Y}}^{(\tau)}\big{\}}\Big{\rangle} _{\mathbf{O}}\approx\exp\bigg{\{}\frac{N}{2}\sum_{\tau=1}^{n}\mathcal{P}_{\rho_{X}} \Big{(}\frac{1}{N}\big{(}\mathbf{a}^{(\tau)\intercal}\mathbf{Y}\mathbf{b}^{(\tau)}+\|\mathbf{a} ^{(\tau)}\|\|\mathbf{Y}\mathbf{b}^{(\tau)}\|\big{)}\Big{)} \tag{63}\]

By the same computation as previous section, for the second bracket we have:

\[\Big{\langle}\exp\big{\{}\sum_{\tau=1}^{n}\operatorname{Tr}\mathbf{b}^{(\tau)}\mathbf{ a}^{(\tau)\intercal}\mathbf{U}\mathbf{W}\mathbf{V}^{\intercal}\big{\}}\Big{\rangle}_{\mathbf{U}, \mathbf{V}}\approx\exp\Big{\{}\frac{N}{2}\sum_{\tau=1}^{n}\mathcal{Q}^{(\alpha)}_{ \mu_{W}}\big{(}\frac{1}{NM}\|\mathbf{a}^{(\tau)}\|^{2}\|\mathbf{b}^{(\tau)}\|^{2}\big{)}\Big{\}} \tag{64}\]

From (61), (63), (64), we find:

\[\langle G_{ij}(z)\rangle =\lim_{n\to\infty}\int\big{(}\prod_{k=1}^{N+M}\prod_{\tau=1}^{n}d \eta_{k}^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)} \tag{65}\] \[\times\exp\Bigg{\{}-\frac{1}{2}\sum_{\tau=1}^{n}\bigg{[}z\|\mathbf{ \eta}^{(\tau)}\|^{2}-N\mathcal{Q}^{(\alpha)}_{\mu_{W}}\big{(}\frac{1}{NM}\| \mathbf{a}^{(\tau)}\|^{2}\|\mathbf{b}^{(\tau)}\|^{2}\big{)}\] \[\qquad\qquad\qquad-N\mathcal{P}_{\rho_{X}}\Big{(}\frac{1}{N} \big{(}\mathbf{a}^{(\tau)\intercal}\mathbf{Y}\mathbf{b}^{(\tau)}+\|\mathbf{a}^{(\tau)}\|\|\mathbf{Y} \mathbf{b}^{(\tau)}\|\big{)}\Big{)}\] \[\qquad\qquad\qquad\qquad-N\mathcal{P}_{\rho_{X}}\Big{(}\frac{1}{N} \big{(}\mathbf{a}^{(\tau)\intercal}\mathbf{Y}\mathbf{b}^{(\tau)}-\|\mathbf{a}^{(\tau)}\|\|\mathbf{Y} \mathbf{b}^{(\tau)}\|\big{)}\Big{)}\bigg{]}\Bigg{\}}\]Now, we introduce delta functions (for brevity we drop the limit term):

\[\begin{split}\langle G_{ij}(z)\rangle=&\int\big{(}\prod_ {k=1}^{N+M}\prod_{\tau=1}^{n}d\eta_{k}^{(\tau)}\big{)}\big{(}\prod_{\tau=1}^{n} dp_{1}^{(\tau)}\,dq_{2}^{(\tau)}\,dq_{3}^{(\tau)}\,dq_{4}^{(\tau)}\big{)}\,\eta_{i}^{ (1)}\eta_{j}^{(1)}\\ &\quad\times\prod_{\tau=1}^{n}\delta\big{(}q_{1}^{(\tau)}-\frac{1 }{N}\|\mathbf{a}^{(\tau)}\|^{2}\big{)}\,\delta\big{(}q_{2}^{(\tau)}-\frac{1}{M}\| \mathbf{b}^{(\tau)}\|^{2}\big{)}\\ &\quad\times\delta\big{(}q_{3}^{(\tau)}-\frac{1}{N}\|\mathbf{Yb}^{( \tau)}\|^{2}\big{)}\,\delta\big{(}q_{4}^{(\tau)}-\frac{1}{N}\mathbf{a}^{(\tau)}{}^ {\intercal}\mathbf{Yb}^{(\tau)}\big{)}\\ &\quad\times\exp\Big{\{}-\frac{1}{2}\sum_{\tau=1}^{n}z\|\mathbf{\eta}^ {(\tau)}\|^{2}-N\mathcal{Q}_{\mu_{W}}^{(\alpha)}(q_{1}^{(\tau)}q_{2}^{(\tau)} )\\ &\quad\quad\quad\quad\quad\quad-N\mathcal{P}_{\rho_{X}}\big{(}q_{4 }^{(\tau)}+\sqrt{q_{1}^{(\tau)}q_{3}^{(\tau)}}\big{)}-N\mathcal{P}_{\rho_{X}} \big{(}q_{4}^{(\tau)}-\sqrt{q_{1}^{(\tau)}q_{3}^{(\tau)}}\big{)}\Big{\}}\end{split} \tag{66}\]

In the next step, we replace each delta with its Fourier transform. Note that for the parameters \(q_{1},q_{2},q_{3}\) we use \(\delta\big{(}q_{1}^{\tau}-\frac{1}{N}\|\mathbf{a}^{\tau}\|^{2}\big{)}\propto\int\, d\beta_{1}^{\tau}\exp\Big{\{}-\frac{N}{2}\beta_{1}^{\tau}\big{(}q_{1}^{\tau}- \frac{1}{N}\|\mathbf{a}^{\tau}\|^{2}\big{)}\Big{\}}\), and for \(q_{4}\) we use \(\delta\big{(}q_{4}^{(\tau)}-\frac{1}{N}\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{Yb}^{( \tau)}\big{)}\propto\int\,d\beta_{1}^{\tau}\exp\Big{\{}-N\beta_{1}^{\tau}\big{(} q_{4}^{(\tau)}-\frac{1}{N}\mathbf{a}^{(\tau)}{}^{\intercal}\mathbf{Yb}^{(\tau)}\big{)}\Big{\}}\). After rearranging, we find:

\[\begin{split}\langle G_{ij}(z)\rangle&\propto\int \big{(}\prod_{\tau=1}^{n}dq_{1}^{(\tau)}\,dq_{2}^{(\tau)}\,dq_{3}^{(\tau)}\,dq _{4}^{(\tau)}\,d\beta_{1}^{(\tau)}\,d\beta_{2}^{(\tau)}\,d\beta_{3}^{(\tau)}\, d\beta_{4}^{(\tau)}\big{)}\\ &\quad\times\exp\Big{\{}\frac{N}{2}\sum_{\tau=1}^{n}\mathcal{Q}_{ \mu_{W}}^{(\alpha)}(q_{1}^{(\tau)}q_{2}^{(\tau)})+\mathcal{P}_{\rho_{X}}\big{(} q_{4}^{(\tau)}+\sqrt{q_{1}^{(\tau)}q_{3}^{(\tau)}}\big{)}+\mathcal{P}_{\rho_{X}} \big{(}q_{4}^{(\tau)}-\sqrt{q_{1}^{(\tau)}q_{3}^{(\tau)}}\big{)}\\ &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad- \beta_{1}^{(\tau)}q_{1}^{(\tau)}-\frac{1}{\alpha}\beta_{2}^{(\tau)}q_{2}^{( \tau)}-\beta_{3}^{(\tau)}q_{3}^{(\tau)}-2\beta_{4}^{(\tau)}q_{4}^{(\tau)}\Big{\}} \\ &\quad\times\int\big{(}\prod_{k=1}^{N+M}\prod_{\tau=1}^{n}d\eta_{k }^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)}\exp\Big{\{}-\frac{1}{2}\sum_{ \tau=1}^{n}z\|\mathbf{\eta}^{(\tau)}\|-\beta_{1}^{(\tau)}\|\mathbf{a}^{(\tau)}\|^{2}- \beta_{2}^{(\tau)}\|\mathbf{b}^{(\tau)}\|^{2}\\ &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad- \beta_{3}^{(\tau)}\|\mathbf{Yb}^{(\tau)}\|^{2}-2\beta_{4}^{(\tau)}\mathbf{a}^{(\tau)} {}^{\intercal}\mathbf{Yb}^{(\tau)}\Big{\}}\end{split} \tag{67}\]

The inner integral is a Gaussian integral, and can be written as:

\[\begin{split}\int\big{(}\prod_{k=1}^{N+M}&\prod_{ \tau=1}^{n}d\eta_{k}^{(\tau)}\big{)}\,\eta_{i}^{(1)}\eta_{j}^{(1)}\\ &\quad\quad\times\exp\Big{\{}\sum_{\tau=1}^{n}-\frac{1}{2}\mathbf{ \eta}^{(\tau)}{}^{\intercal}\left[\begin{array}{cc}(z-\beta_{1}^{(\tau)})\mathbf{ I}_{N}&-\beta_{4}^{(\tau)}\mathbf{Y}\\ -\beta_{4}^{(\tau)}\mathbf{Y}^{\intercal}&(z-\beta_{2}^{(\tau)})\mathbf{I}_{M}-\beta_{3} ^{(\tau)}\mathbf{Y}^{\intercal}\mathbf{Y}\end{array}\right]\mathbf{\eta}^{(\tau)}\Big{\}} \end{split} \tag{68}\]

Denote the matrix in the exponent by \(\mathbf{C}_{Y}^{(\tau)}\). Using the formula for determinant of block matrices (see proposition 2.8.4 in [61]), we have::

\[\begin{split}\det\mathbf{C}_{Y}^{(\tau)}&=\det\Big{[}(z- \beta_{1}^{(\tau)})\mathbf{I}_{N}-\beta_{4}^{(\tau)}{}^{2}\mathbf{Y}\big{(}(z-\beta_ {2}^{(\tau)})\mathbf{I}_{M}-\beta_{3}^{(\tau)}\mathbf{Y}^{\intercal}\mathbf{Y}\big{)}^{-1 }\mathbf{Y}^{\intercal}\Big{]}\\ &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quadwhere \(\sigma_{k}\)'s are the singular values of \(\mathbf{Y}\). So computing the Gaussian integrals, (67) can be written as:

\[\langle G_{ij}(z)\rangle\propto\int\big{(}\prod_{\tau=1}^{n}dq_{1}^{ (\tau)}dq_{2}^{(\tau)}dq_{3}^{(\tau)}dq_{4}^{(\tau)}\,d\beta_{1}^{(\tau)}\,d \beta_{2}^{(\tau)}\,d\beta_{3}^{(\tau)}\,d\beta_{4}^{(\tau)}\big{)}\big{(} \mathbf{C}_{Y}^{(1)}{}^{-1}\big{)}_{ij}\] \[\times\exp\big{\{}-\frac{Nn}{2}F_{0}^{Y}(\mathbf{q}_{1},\mathbf{q}_{2},\bm {q}_{3},\mathbf{q}_{4},\mathbf{\beta}_{1},\mathbf{\beta}_{2},\mathbf{\beta}_{3},\mathbf{\beta}_{4}) \big{\}} \tag{69}\]

with

\[F_{0}^{Y}(\mathbf{q}_{1},\mathbf{q}_{2},\mathbf{q}_{3},\mathbf{q}_{4}, \mathbf{\beta}_{1},\mathbf{\beta}_{2},\mathbf{\beta}_{3},\mathbf{\beta}_{4})=\frac {1}{n}\sum_{\tau=1}^{n}\bigg{[}\big{(}\frac{1}{\alpha}-1\big{)}\ln(z-\beta_{2} ^{(\tau)}) \tag{70}\] \[+\frac{1}{N}\sum_{k=1}^{N}\ln\Big{(}(z-\beta_{1}^{(\tau)})(z- \beta_{2}^{(\tau)})-\big{(}\beta_{4}^{(\tau)}{}^{2}+\beta_{3}^{(\tau)}(z-\beta_ {1}^{(\tau)})\big{)}\sigma_{k}^{2}\Big{)}\] \[-\mathcal{Q}_{\mu_{W}}^{(\alpha)}(q_{1}^{(\tau)}q_{2}^{(\tau)})- \mathcal{P}_{\rho_{X}}\big{(}q_{4}^{(\tau)}+\sqrt{q_{1}^{(\tau)}q_{3}^{(\tau)} }\big{)}-\mathcal{P}_{\rho_{X}}\big{(}q_{4}^{(\tau)}-\sqrt{q_{1}^{(\tau)}q_{3} ^{(\tau)}}\big{)}\] \[+\beta_{1}^{(\tau)}q_{1}^{(\tau)}+\frac{1}{\alpha}\beta_{2}^{( \tau)}q_{2}^{(\tau)}+\beta_{3}^{(\tau)}q_{3}^{(\tau)}+2\beta_{4}^{(\tau)}q_{4} ^{(\tau)}\bigg{]}\]

We will evaluate the integral (67) using saddle-points of the function \(F_{0}^{Y}\). From the _replica symmetric ansatz_ at the saddle-point we have:

\[\forall\tau\in\{1,\cdots,n\}:\quad\begin{cases}q_{1}^{\tau}=q_{1}, \quad q_{2}^{\tau}=q_{2},\quad q_{3}^{\tau}=q_{3},\quad q_{4}^{\tau}=q_{4}\\ \beta_{1}^{\tau}=\beta_{1},\quad\beta_{2}^{\tau}=\beta_{2},\quad\beta_{3}^{\tau }=\beta_{3},\quad\beta_{4}^{\tau}=\beta_{4}\end{cases}\]

Finally, we find the solution to be:

\[\begin{cases}\beta_{1}^{*}=\frac{\mathcal{C}_{\mu_{W}}^{(\alpha)} (q_{1}^{*}q_{2}^{*})}{q_{1}^{*}}+\frac{1}{2}\sqrt{\frac{q_{3}^{*}}{q_{1}^{*}}} \Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)} -\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)} \\ \beta_{2}^{*}=\alpha\frac{\rho_{W}^{*}}{q_{2}^{*}}\\ \beta_{3}^{*}=\frac{1}{2}\sqrt{\frac{q_{1}^{*}}{q_{3}^{*}}}\Big{(}\mathcal{R}_ {\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}-\mathcal{R}_{\rho_ {X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)}\\ \beta_{4}^{*}=\frac{1}{2}\Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q _{1}^{*}q_{3}^{*}}\big{)}+\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^ {*}q_{3}^{*}}\big{)}\Big{)}\quad\text{ with }\begin{Bmatrix}Z_{1}(z)=(z-\beta_{1}^{*})(z- \beta_{2}^{*})\\ Z_{2}(z)={\beta_{4}^{*}}^{2}+\beta_{3}^{*}(z-\beta_{1}^{*})\end{Bmatrix}\\ q_{1}^{*}=\frac{(z-\beta_{2})\beta_{3}^{*}}{Z_{2}(z)}\mathcal{G}_{\rho_{Y}} \big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}+\frac{1-\alpha}{Z_{2}(z)}\\ q_{2}^{*}=\alpha\frac{z-\beta_{1}^{*}Z_{1}(z)}{Z_{2}(z)}\mathcal{G}_{\rho_{Y}} \big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}+\frac{1-\alpha}{Z_{2}^{*}}\\ q_{3}^{*}=\frac{(z-\beta_{1}^{*})Z_{1}(z)}{Z_{2}(z)^{*}}\mathcal{G}_{\rho_{Y}} \big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}-\frac{\beta_{4}^{*}}{Z_{2}(z)}\end{cases} \tag{71}\]

where \(\rho_{Y}\) is the limiting eigenvalue distribution of \(\mathbf{Y}\mathbf{Y}^{\intercal}\).

The relation (69) and the solutions (71) hold for arbitrary indices \(i,j\), so we can state the relation in the matrix form. Computing the inverse of \(\mathbf{C}_{Y}^{*}{}^{-1}\) (see section H.2), we have:

\[\big{\langle}\mathbf{G}_{\mathbf{S}}(z)\big{\rangle}_{\mathbf{O},\mathbf{U},\mathbf{V}} =\left\langle\begin{array}{cc}\frac{1}{z}\mathbf{I}_{N}+\frac{1}{ z}\mathbf{S}\mathbf{G}_{S^{\intercal}S}(z^{2})\mathbf{S}^{\intercal}&\mathbf{S}\mathbf{G}_{S^{ \intercal}S}(z^{2})\\ \mathbf{G}_{S^{\intercal}S}(z^{2})\mathbf{S}^{\intercal}&z\mathbf{G}_{S^{\intercal}S}(z^{2}) \end{array}\right\rangle\right\rangle\] \[=\left[\begin{array}{cc}\frac{1}{z-\beta_{1}^{*}}\mathbf{I}_{N}+ \frac{\beta_{1}^{*2}}{(z-\beta_{1}^{*})Z_{2}(z)}\mathbf{Y}\mathbf{G}_{Y^{\intercal}Y} \big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\mathbf{Y}^{\intercal}&\frac{\beta_{4}^{*}}{Z_ {2}(z)}\mathbf{Y}\mathbf{G}_{Y^{\intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\\ \frac{\beta_{4}^{*}}{Z_{2}(z)}\mathbf{G}_{Y^{\intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2} (z)}\big{)}\mathbf{Y}^{\intercal}&\frac{z-\beta_{1}^{*}}{Z_{2}(z)}\mathbf{G}_{Y^{ \intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\end{array}\right] \tag{72}\]

With this relation, we can further simplify the solution (71).

We start with comparing the trace of upper-left block in (72). The normalized trace of the first block in \(\big{\langle}\mathbf{G}_{\mathbf{S}}(z)\big{\rangle}_{\mathbf{O},\mathbf{U},\mathbf{V}}\) is computed in (43) to be \(\mathcal{G}_{\hat{\mu}_{S}}(z)\). The normalized trace of the upper-left block in \(\boldsymbol{C}_{Y}^{*\,-1}\) is:

\[\begin{split}\frac{1}{N}\operatorname{Tr}\,\Big{[}(z-\beta_{1}^{*})^ {-1}\boldsymbol{I}_{N}+&\frac{{\beta_{4}^{*}}^{2}}{(z-\beta_{1}^{*} )Z_{2}(z)}\boldsymbol{Y}\boldsymbol{G}_{Y^{\mathrm{\tiny{TYY}}}}\big{(}\frac{Z_ {1}(z)}{Z_{2}(z)}\big{)}\boldsymbol{Y}^{\intercal}\Big{]}\\ &=\frac{1}{N}\frac{1}{z-\beta_{1}^{*}}\sum_{k=1}^{N}\big{[}1+ \frac{{\beta_{4}^{*}}^{2}}{Z_{2}(z)}\frac{\sigma_{k}^{2}}{\frac{Z_{1}(z)}{Z_{2 }(z)}-\sigma_{k}^{2}}\big{]}\\ &=\frac{1}{N}\frac{1}{z-\beta_{1}^{*}}\sum_{k=1}^{N}\big{[}\frac{ \beta_{4}^{*}Z_{1}(z)}{Z_{2}^{2}(z)}\frac{1}{\frac{Z_{1}(z)}{Z_{2}(z)}-\sigma _{k}^{2}}+1-\frac{{\beta_{4}^{*}}^{2}}{Z_{2}(z)}\big{]}\\ &=\frac{1}{N}\frac{1}{z-\beta_{1}^{*}}\frac{{\beta_{4}^{*}}^{2}Z_ {1}(z)}{Z_{2}^{2}(z)}\sum_{k=1}^{N}\frac{\frac{1}{Z_{1}(z)}}{Z_{2}(z)-\sigma_{k }^{2}}+\frac{1}{z-\beta_{1}^{*}}\frac{\beta_{3}^{*}(z-\beta_{1}^{*})}{Z_{2}(z) }\\ &=\frac{(z-\beta_{2}^{*}){\beta_{4}^{*}}^{2}}{Z_{2}(z)^{2}} \mathcal{G}_{\rho_{Y}}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}+\frac{\beta_{3}^ {*}}{Z_{2}(z)}\\ &=q_{1}^{*}\end{split} \tag{73}\]

Thus, \(q_{1}^{*}=\mathcal{G}_{\tilde{\mu}_{S}}(z)\).

The normalized trace of the lower-right block of \(\big{\langle}\boldsymbol{G}_{S}(z)\big{\rangle}_{\boldsymbol{O},\boldsymbol{ U},\boldsymbol{V}}\) is \(\alpha\mathcal{G}_{\tilde{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\) (see (44)). The normalized trace of the lower-right block in \(\boldsymbol{C}_{Y}^{*\,-1}\) is:

\[\begin{split}\frac{1}{M}\operatorname{Tr}\,\Big{[}\frac{z-\beta _{1}^{*}}{Z_{2}(z)}\boldsymbol{G}_{Y^{\mathrm{\tiny{TY}}}}\big{(}\frac{Z_{1}(z )}{Z_{2}(z)}\big{)}\Big{]}&=\frac{1}{M}\frac{z-\beta_{1}^{*}}{Z_ {2}(z)}\sum_{k=1}^{N}\frac{1}{\frac{Z_{1}(z)}{Z_{2}(z)}-\sigma_{k}^{2}}+\frac {M-N}{M}\frac{z-\beta_{1}^{*}}{Z_{2}(z)}\frac{Z_{2}(z)}{Z_{1}(z)}\\ &=\frac{N}{M}\frac{1}{N}\frac{z-\beta_{1}^{*}}{Z_{2}(z)}\sum_{k=1 }^{N}\frac{1}{\frac{Z_{1}(z)}{Z_{2}(z)}-\sigma_{k}^{2}}+\frac{M-N}{M}\frac{z- \beta_{1}^{*}}{Z_{1}(z)}\\ &=\alpha\frac{z-\beta_{1}^{*}}{Z_{2}(z)}\mathcal{G}_{\rho_{Y}} \big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}+\frac{1-\alpha}{z-\beta_{2}^{*}}\\ &=q_{2}^{*}\end{split} \tag{74}\]

So, \(q_{2}^{*}=\alpha\mathcal{G}_{\tilde{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\).

With a bit of algebra, we can express the parameters \(q_{3}^{*},q_{4}^{*}\) in terms of \(q_{1}^{*},\beta_{1}^{*},\beta_{4}^{*}\):

\[q_{3}^{*}=\frac{(z-\beta_{1}^{*})^{2}}{{\beta_{4}^{*}}^{2}}q_{1}^{*}-\frac{z- \beta_{1}^{*}}{{\beta_{4}^{*}}^{2}},\quad q_{4}^{*}=\frac{z-\beta_{1}^{*}}{ \beta_{4}^{*}}q_{1}^{*}-\frac{1}{\beta_{4}^{*}} \tag{75}\]

Therefore, the solution can be written without involving \(\mathcal{G}_{\rho_{Y}}\), as:

\[\begin{cases}\beta_{1}^{*}=\frac{C_{\mu_{Y}}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{ q_{1}^{*}}+\frac{1}{2}\sqrt{\frac{q_{3}^{*}}{q_{1}^{*}}}\Big{(}\mathcal{R}_{ \rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}-\mathcal{R}_{\rho _{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)}\\ \beta_{2}^{*}=\alpha\frac{C_{\mu_{Y}}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{q_{2}^{*} }\Big{)}\\ \beta_{3}^{*}=\frac{1}{2}\sqrt{\frac{q_{3}^{*}}{q_{3}^{*}}}\Big{(}\mathcal{R}_ {\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}-\mathcal{R}_{\rho _{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)}\\ \beta_{4}^{*}=\frac{1}{2}\Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{ q_{1}^{*}q_{3}^{*}}\big{)}+\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3} ^{*}}\big{)}\Big{)}\\ q_{1}^{*}=\mathcal{G}_{\tilde{\mu}_{S}}(z)\\ q_{2}^{*}=\alpha\mathcal{G}_{\tilde{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\\ q_{3}^{*}=\frac{(z-\beta_{1}^{*})^{2}}{\beta_{2}^{*}}\mathcal{G}_{\tilde{\mu}_{S }}(z)-\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}}\\ q_{4}^{*}=\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}}\mathcal{G}_{\tilde{\mu}_{S}}(z)- \frac{1}{\beta_{4}^{*}}\end{cases} \tag{76}\]

**Remark 5**.: _The simplifications in (75) are derived with the assumption that \(\beta_{4}^{*}\neq 0\). However, in the initial set of equations (71), if \(\rho_{X}\) is symmetric measure then \(\beta_{4}^{*}=q_{4}^{*}=0\) is a solution. If \(\rho_{X}\) is symmetric, then \(\mathcal{R}_{\rho_{X}}(-z)=-\mathcal{R}_{\rho_{X}}(z)\), and plugging \(q_{4}^{*}=0\) in the expression for \(\beta_{4}^{*}\) in (71), we find that \(\beta_{4}^{*}=0\)._

### Overlaps and the optimal singular values

From (59), (72), we find:

\[\begin{split} O_{Y}(\gamma,\sigma_{i})&\approx\frac{1}{ \pi\bar{\mu}_{S}(\gamma)}\operatorname{Im}\,\lim_{z\to\gamma-\mathfrak{i}0^{+} }\frac{\beta_{4}^{*}}{Z_{2}(z)}\boldsymbol{y}_{i}^{(r)\,\intercal}\boldsymbol{ G}_{Y^{\intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\boldsymbol{Y}^{ \intercal}\boldsymbol{y}_{i}^{(l)}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma)}\operatorname{Im}\,\lim_{z\to \gamma-\mathfrak{i}0^{+}}\,\beta_{4}^{*}\frac{\sigma_{i}}{Z_{1}(z)-Z_{2}(z) \sigma_{i}^{2}}\end{split} \tag{77}\]

From the overlap, we can compute the optimal singular values:

\[\begin{split}\widehat{\xi}_{y_{i}}^{*}&\approx \frac{1}{N}\sum_{j=1}^{N}\sigma_{j}O_{Y}(\gamma_{i},\sigma_{j})\\ &\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \,\lim_{z\to\gamma_{i}-\mathfrak{i}0^{+}}\,\frac{1}{N}\sum_{j=1}^{N}\beta_{4}^ {*}\frac{\sigma_{j}^{2}}{Z_{1}(z)-Z_{2}(z)\sigma_{j}^{2}}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\,\lim_{z \to\gamma_{i}-\mathfrak{i}0^{+}}\,\frac{1}{N}\frac{\beta_{4}^{*}}{Z_{2}(z)} \sum_{j=1}^{N}\frac{\sigma_{j}^{2}}{Z_{1}(z)}-\sigma_{j}^{2}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\,\lim_{z \to\gamma_{i}-\mathfrak{i}0^{+}}\,\frac{1}{N}\frac{\beta_{4}^{*}}{Z_{2}(z)} \sum_{j=1}^{N}\bigg{[}\frac{\frac{Z_{1}(z)}{Z_{2}(z)}}{Z_{2}(z)}-\sigma_{j}^{2 }-1\bigg{]}\\ &\approx\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im} \,\lim_{z\to\gamma_{i}-\mathfrak{i}0^{+}}\frac{\beta_{4}^{*}Z_{1}(z)}{Z_{2}(z) ^{2}}\mathcal{G}_{\rho_{Y}}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}-\frac{\beta _{4}^{*}}{Z_{2}(z)}\\ &=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\,\lim _{z\to\gamma_{i}-\mathfrak{i}0^{+}}q_{4}^{*}\end{split} \tag{78}\]

where in the last equality we used the solution we have found in (71). Note that, based on (76), we do not need to have any knowledge about \(\rho_{Y}\) to compute \(q_{4}^{*}\). In the end, we need to divide the estimator by \(\sqrt{\kappa}\) as we have absorbed it into \(\boldsymbol{Y}\).

#### d.3.1 Recovering the rectangular RIE for a denoising problem

Note that if in the model (60), we put \(\boldsymbol{X}=\boldsymbol{I}\) the model reduces to the additive denoising of \(\boldsymbol{Y}\), and we recover the estimator recently proposed in [19] for the rectangular case.

For \(\boldsymbol{X}=\boldsymbol{I}\), \(\mathcal{R}_{\rho_{X}}(z)=1\), so (76) reduces to:

\[\begin{cases}\beta_{1}^{*}=\frac{\mathcal{C}_{\mu_{W}}^{(\alpha)}(q_{1}^{*} \sigma_{2}^{*})}{q_{1}^{*}},\quad\beta_{2}^{*}=\alpha\frac{\mathcal{C}_{\mu_{W }}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{q_{2}^{*}},\quad\beta_{3}^{*}=0,\quad\beta _{4}^{*}=1\\ q_{1}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z),\quad q_{2}^{*}=\alpha\mathcal{G}_{\bar {\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\\ q_{3}^{*}=(z-\beta_{1}^{*})^{2}\mathcal{G}_{\bar{\mu}_{S}}(z)-(z-\beta_{1}^{*}),\quad q_{4}^{*}=(z-\beta_{1}^{*})\mathcal{G}_{\bar{\mu}_{S}}(z)-1\end{cases} \tag{79}\]From (78), we have:

\[\widehat{\xi}^{*}_{y_{i}} =\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z\to \gamma_{i}-\mathfrak{i}0^{+}}q_{4}^{*}\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z\to \gamma_{i}-\mathfrak{i}0^{+}}z\mathcal{G}_{\bar{\mu}_{S}}(z)-\beta_{1}^{*} \mathcal{G}_{\bar{\mu}_{S}}(z)-1\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z\to \gamma_{i}-\mathfrak{i}0^{+}}z\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{\mathcal{C} _{\mu_{W}}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{q_{1}^{*}}\mathcal{G}_{\bar{\mu}_{S }}(z)-1\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z\to \gamma_{i}-\mathfrak{i}0^{+}}z\mathcal{G}_{\bar{\mu}_{S}}(z)-\mathcal{C}_{\mu _{W}}^{(\alpha)}(q_{1}^{*}q_{2}^{*})-1\] \[=\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\lim_{z\to \gamma_{i}-\mathfrak{i}0^{+}}z\mathcal{G}_{\bar{\mu}_{S}}(z)-\mathcal{C}_{\mu _{W}}^{(\alpha)}\Big{(}\mathcal{G}_{\bar{\mu}_{S}}(z)\big{(}\alpha\mathcal{G}_ {\bar{\mu}_{S}}(z)+(1-\alpha)\frac{1}{z}\big{)}\Big{)}-1\] \[\stackrel{{\text{(a)}}}{{=}}\frac{1}{\pi\bar{\mu}_{S }(\gamma_{i})}\operatorname{Im}\bigg{[}\gamma_{i}\mathcal{G}_{\bar{\mu}_{S}}( \gamma_{i}-\mathfrak{i}0^{+})-\mathcal{C}_{\mu_{W}}^{(\alpha)}\bigg{(}\frac{1 }{\gamma_{i}}\mathcal{G}_{\bar{\mu}_{S}}(\gamma_{i}-\mathfrak{i}0^{+})\Big{(}1 -\alpha+\alpha\gamma_{i}\mathcal{G}_{\bar{\mu}_{S}}(\gamma_{i}-\mathfrak{i}0^ {+})\Big{)}\bigg{)}\bigg{]}\] \[\stackrel{{\text{(b)}}}{{=}}\gamma_{i}-\frac{1}{\pi \bar{\mu}_{S}(\gamma_{i})}\operatorname{Im}\mathcal{C}_{\mu_{W}}^{(\alpha)} \bigg{(}\frac{1-\alpha}{\gamma_{i}}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})+ \alpha\big{(}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})\big{)}^{2}-\alpha\big{(} \pi\bar{\mu}_{S}(\gamma_{i})\big{)}^{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad+\mathsf{i}\pi\bar{\mu}_{S}(\gamma_{i})\big{(}\frac{1-\alpha}{\gamma_{i }}+2\alpha\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})\big{)}\bigg{)} \tag{80}\]

where in (a) we used the analyticity of rectangular R-transform [57], and in (b), we used Plemelj formula (6). Note that, the final estimator should be divided by the \(\sqrt{\kappa}\).

### Examples

Throughout the numerical experiments, we consider the matrix \(\mathbf{W}\) to have i.i.d. Gaussian entries with variance \(\nicefrac{{1}}{{N}}\), so \(\mathcal{C}_{\mu_{W}}^{(\alpha)}(z)=\frac{1}{\alpha}z\). And, \(\mathbf{X}=\mathbf{F}+c\mathbf{I}\) where \(\mathbf{F}=\mathbf{F}^{\intercal}\in\mathbb{R}^{N\times N}\) has i.i.d. entries with variance \(\nicefrac{{1}}{{N}}\), and \(c\neq 0\) is a real number, so \(\mathcal{R}_{\rho_{X}}(z)=z+c\). With these choices, the solution (76) simplifies to:

\[\begin{cases}\beta_{1}^{*}=\frac{1}{\alpha}q_{2}^{*}+q_{3}^{*}, \quad\beta_{2}^{*}=q_{1}^{*},\quad\beta_{3}^{*}=q_{1}^{*},\quad\beta_{4}^{*}= q_{4}^{*}+c\\ q_{1}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z),\quad q_{2}^{*}=\alpha\mathcal{G}_{\bar{\mu}_{S }}(z)+(1-\alpha)\frac{1}{z}\\ q_{3}^{*}=\frac{(z-\beta_{1}^{*})^{2}}{\beta_{4}^{*}}\mathcal{G}_{\bar{\mu}_{S}}( z)-\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}},\quad q_{4}^{*}=\frac{z-\beta_{1}^{*}}{ \beta_{4}^{*}}\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1}{\beta_{4}^{*}}\end{cases} \tag{81}\]

Note that in (81), \(q_{1}^{*},q_{2}^{*}\) are given in terms of the observation, so to find the solution we only need to find the parameters \(q_{3}^{*},q_{4}^{*}\). In (81), one can see that we have the relation \(q_{3}^{*}=\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}}q_{4}^{*}\). Writing the parameters \(\beta_{1}^{*},\beta_{4}^{*}\) in terms of \(q_{2}^{*},q_{3}^{*},q_{4}^{*}\), after a bit of algebra we have the following relation:

\[q_{3}^{*}=\frac{z-\frac{1}{\alpha}q_{2}^{*}}{2q_{4}^{*}+c}q_{4}^{*} \tag{82}\]

In the expression for \(q_{4}^{*}\) in (81), using (82) we can rewrite \(\beta_{1}^{*},\beta_{4}^{*}\) in terms of \(q_{2}^{*},q_{4}^{*}\). After some manipulations we find that \(q_{4}^{*}\) is the solution to the following cubic equation:

\[2x^{3}+3c\,x^{2}+\Big{[}c^{2}+2-\big{(}z-\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{ 1-\alpha}{\alpha}\frac{1}{z}\big{)}\mathcal{G}_{\bar{\mu}_{S}}(z)\Big{]}\,x-c \Big{[}\big{(}z-\mathcal{G}_{\bar{\mu}_{S}}(z)-\frac{1-\alpha}{\alpha}\frac{1} {z}\big{)}\mathcal{G}_{\bar{\mu}_{S}}(z)-1\Big{]}=0 \tag{83}\]

Based on our numerical simulations, we pick the following root for \(q_{4}^{*}\):

\[q_{4}^{*}=-\frac{c}{2}-\frac{12-3c^{2}+6A}{3\sqrt[3]{B}}+\frac{\sqrt[3]{B}}{12} \tag{84}\]

with

\[A =\mathcal{G}_{\bar{\mu}_{S}}(z)^{2}-\frac{\mathcal{G}_{\bar{\mu}_ {S}}(z)}{z}\big{(}1-\frac{1}{\alpha}\big{)}-\mathcal{G}_{\bar{\mu}_{S}}(z)z\] \[B =-216cA+4\sqrt{4\big{(}12-3c^{2}+6A\big{)}^{3}+54^{2}c^{2}A^{2}}\]Once we have \(q_{4}^{*}\), we can find \(q_{3}^{*}\) using (82). In the end, \(\beta_{1}^{*},\cdots,\beta_{4}^{*}\) can be evaluated. Note that, for the RIE, only \(q_{4}^{*}\) is required. Other parameters are used to evaluate the resolvent relation (72) and the overlap (77).

#### d.4.1 Resolvent relation

We take \(\kappa=1\). In model (60), without loss of generality we can consider \(\mathbf{Y}\) to be diagonal.

In figure 12, \(\mathbf{Y}\) is the diagonal matrix obtained from the singular values of a Gaussian matrix with i.i.d. entries of variance \(\nicefrac{{1}}{{N}}\). In figure 13, the non-zero entries (on main diagonal) of \(\mathbf{Y}\) are uniformly distributed in \([1,3]\). As in previous cases, \(\mu_{S},\mathcal{G}_{\widetilde{\mu}_{S}}(z)\) are estimated numerically using Cauchy kernel, from which the parameters \(\beta_{1}^{*},\cdots,\beta_{4}^{*}\) are computed.

#### d.4.2 Overlap

To illustrate the formula for the overlap (77), we fix the matrix \(\mathbf{Y}\) and run experiments over various realization of the model (60). For each experiment, we record the overlap of \(k\)-th singular vectors left and right) of \(\mathbf{S}\) and singular vectors of \(\mathbf{Y}\). To compute the theoretical prediction, we evaluate the parameters \(\beta_{1}^{*},\beta_{2}^{*},\beta_{3}^{*}\), \(\beta_{4}^{*}\), for \(z=\bar{\gamma}_{k}-\nicefrac{{1}}{{0^{+}}}\) where \(\bar{\gamma}_{k}\) is the average of \(k\)-th singular value of \(\mathbf{S}\) in the experiments.

In figure 14a, the overlap is shown for \(\mathbf{Y}\) with i.i.d. Gaussian entries of variance \(\frac{1}{N}\), so \(\mu_{Y}\) is the Marchenko-Pastur law with aspect-ratio \(\alpha\). In figure 14b, matrix \(\mathbf{Y}\) is constructed as \(\mathbf{Y}=\mathbf{U}_{Y}\mathbf{\Sigma}\mathbf{V}_{Y}^{\intercal}\), where \(\mathbf{U}_{Y}\in\mathbb{R}^{N\times N},\mathbf{V}_{Y}\in\mathbb{R}^{M\times M}\) are Haar distributed orthogonal matrices, and singular values \(\sigma_{1},\cdots,\sigma_{N}\) are chosen independently uniformly from \([1,3]\), so \(\mu_{Y}=\mathcal{U}\big{(}[1,3]\big{)}\).

#### d.4.3 RIE performance

In this section, we investigate the performance of our proposed estimators for \(\mathbf{Y}\). To construct the RIE for \(\mathbf{Y}\), we only need \(q_{4}^{*}\) which we use (84). We compare performances of the optimal RIE (78) with the one of oracle estimator (5).

In figures 15,16, the MSE of RIE and the oracle estimator is plotted for three cases of priors: \(\mathbf{Y}\) with Gaussian entries, \(\mathbf{Y}\) with uniform spectral density, and \(\mathbf{Y}\) with Bernoulli spectral density. In all cases, observe that the RIE has the same performance as the oracle estimator.

Effect of aspect-ratio \(\alpha\).In Figure 17, we take \(\mathbf{Y}\) to have Gaussian entries (with variance \(\frac{1}{N}\)), and the MSE is depicted for various values of the aspect-ratio \(\alpha\). We see that as \(M\) increases (\(\alpha\) decreases) the estimation error (of \(\mathbf{Y}\)) decreases.

Sparse \(\mathbf{Y}\): a non-rotation invariant example.We consider \(\mathbf{Y}\) to have i.i.d. entries from the Bernoulli-Rademacher distribution,

\[Y_{i,j}=\begin{cases}+\frac{1}{\sqrt{N}}&\text{with probability }\frac{1-p}{2}\\ 0&\text{with probability }p\\ -\frac{1}{\sqrt{N}}&\text{with probability }\frac{1-p}{2}\end{cases},\qquad \forall\quad 1\leq i\leq N,\quad 1\leq j\leq M\]

With the normalization \(\nicefrac{{1}}{{\sqrt{N}}}\), the spectrum of \(\mathbf{Y}\) does not grow with the dimension and has a finite support, thus we can apply our estimator to reconstruct \(\mathbf{Y}\). _Note that the prior of \(\mathbf{Y}\) is not rotationally invariant, and neither the oracle estimator nor the RIE are optimal_. Therefore, taking the prior into account, we apply a thresholding function on the entries of the matrix obtained from the RIE, \(\widehat{\mathbf{\Xi}_{Y}^{*}}(\mathbf{S})\). We apply the following function on each entry of the estimator:

\[f_{h}(x)=\begin{cases}+\frac{1}{\sqrt{N}}&\text{if }x>\frac{h}{\sqrt{N}}\\ 0&\text{if }|x|\leq\frac{h}{\sqrt{N}}\\ -\frac{1}{\sqrt{N}}&\text{if }x<-\frac{h}{\sqrt{N}}\end{cases},\qquad\text{for }h\in[0,1]\]

In figure 18, the MSE of the oracle estimator, RIE, and RIE\(+f_{p}(x)\) (with \(h=p\)) is plotted. A few remarks on this figure are in order. First, RIEs are not limited to rotationally invariant priors and can give non-trivial estimates for non-rotationally invariant priors, although they are sub-optimal. The Figure 12: Illustration of (72). \(\mathbf{Y}\in\mathbb{R}^{N\times M}\) is a diagonal matrix obtained from the singular values of a \(N\times M\) matrix with i.i.d. entries of variance \(\nicefrac{{1}}{{N}}\), \(\mathbf{X}=\mathbf{X}^{\intercal}\) is shifted Wigner matrix with \(c=3\), and \(\mathbf{Z}\) is a Gaussian matrices with. The empirical estimate of \(\mathbf{G}_{\mathcal{S}}(z)\) (dashed blue line) is computed for \(z=\gamma_{i}-\mathrm{i}\sqrt{\frac{1}{2N}}\) for \(1\leq i\leq N\), for \(N=2000,M=4000\). Theoretical one (solid orange line) is computed from the rhs of (72) with parameters computed from the generated matrix. Note that, the theoretical one has also fluctuations because the parameters \(\beta_{1}^{*},\cdots\beta_{4}^{*}\) are computed from the numerical estimate of \(\mathcal{G}_{\mu_{S}}(z)\).

Figure 13: Illustration of (72). \(\mathbf{Y}\in\mathbb{R}^{N\times M}\) is a diagonal matrix with (main) diagonal entries uniformly distributed in \([1,3]\), \(\mathbf{X}=\mathbf{X}^{\intercal}\) is shifted Wigner matrix with \(c=3\), and \(\mathbf{Z}\) is a Gaussian matrices with. The empirical estimate of \(\mathbf{G}_{\mathcal{S}}(z)\) (dashed blue line) is computed for \(z=\gamma_{i}-\mathrm{i}\sqrt{\frac{1}{2N}}\) for \(1\leq i\leq N\), for \(N=2000\), \(M=4000\). Theoretical one (solid orange line) is computed from the rhs of (72) with parameters computed from the generated matrix. Note that, the theoretical one has also fluctuations because the parameters \(\beta_{1}^{*},\cdots\beta_{4}^{*}\) are computed from the numerical estimate of \(\mathcal{G}_{\mu_{S}}(z)\).

[MISSING_PAGE_FAIL:38]

RIE's output can be refined, or used as a warmed-up initialization for other algorithms to get a better estimate.

In figure 19, for one experiment, the MSE is plotted for RIE and RIE\(+f(x)\) with the best \(h\) among \(\{0,0.1,\cdots,1\}\). We observe that for the particular case of Bernoulli-Rademacher prior, the thresholding stage can improve the MSE when SNR is greater than 1, however the parameter \(h\) should be chosen properly.

## Appendix E Comparison of RIEs for MF and denoising

For estimating \(\mathbf{X}\), we have derived the estimator (49) for general priors \(\rho_{X},\mu_{Y},\mu_{W}\). This estimator simplifies greatly, with parameters in (52), when both \(\mu_{Y},\mu_{W}\) are Marchenko-Pastur distribution, i.e. both \(\mathbf{Y},\mathbf{W}\) having i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\). Similarly, although the RIE for \(\mathbf{Y}\) in (78) is derived for the general priors, it reduces to a rather simple estimator if \(\rho_{X}\), \(\mu_{W}\) are taken to be shifted Wigner, and Marchenko-Pastur distribution, respectively. Therefore, in our numerical examples on factorization problem, we consider \(\mathbf{X}\) to be a shifted Wigner matrix, and \(\mathbf{Y},\mathbf{W}\) to be Gaussian matrices.

In each experiment, the factors \(\mathbf{X}\), \(\mathbf{Y}\) are estimated simultaneously using RIE from the observation matrix \(\mathbf{S}\). In addition to the MSE of estimating each factor, we also compute the MSE of estimating the product \(\mathbf{XY}\). We compare the MSE of the product with the MSE of the oracle estimator and the RIE introduced in [19] for the denoising problem. The oracle estimator for the denoising is

Figure 17: MSE of estimating \(\mathbf{Y}\) as a function of aspect-ratio \(\alpha\), \(\mathbf{Y}\) has Gaussain entries of variance \(\nicefrac{{1}}{{N}}\), and \(\kappa=5\). MSE is normalized by the norm of the signal, \(\|\mathbf{Y}\|_{F}^{2}\). \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\), and \(\mathbf{W}\) has i.i.d. Gaussian entries of variance \(1/N\). The RIE is applied to \(N=2000,M=\nicefrac{{1}}{{\alpha}}N\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\widehat{\overline{\mathbf{\Xi}_{Y}^{*}}}(\mathbf{S})\) and Oracle estimator is also reported.

Figure 18: Estimating \(\mathbf{Y}\) with Bernoulli-Rademacher _entries_. MSE is normalized by the norm of the signal, \(\|\mathbf{Y}\|_{F}^{2}\). \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\), and \(\mathbf{W}\) has i.i.d. Gaussian entries of variance \(1/N\), and \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\), and the results are averaged over 10 runs (error bars are invisible).

constructed as:

\[\mathbf{\Xi}_{XY}^{*}(\mathbf{S})=\sum_{i=1}^{N}\xi_{xy_{i}}^{*}\mathbf{u}_{i}\mathbf{v}_{i}^{\intercal },\quad\xi_{xy_{i}}^{*}=\mathbf{u}_{i}^{\intercal}\mathbf{XY}\mathbf{v}_{i} \tag{85}\]

where \(\mathbf{u}_{i},\mathbf{v}_{i}\)'s are left/right singular vectors of \(\mathbf{S}\). In the RIE proposed in [19], the singular values are estimated by (see section D.3.1)

\[\widehat{\xi_{xy_{i}}^{*}}=\frac{1}{\sqrt{\kappa}}\Bigg{[}\gamma_{ i}-\frac{1}{\pi\bar{\mu}_{S}(\gamma_{i})}\mathrm{Im}\,\mathcal{C}_{\mu_{W}}^{( \alpha)}\bigg{(}\frac{1-\alpha}{\gamma_{i}}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma _{i})+\alpha\big{(}\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})\big{)}^{2}-\alpha \big{(}\pi\bar{\mu}_{S}(\gamma_{i})\big{)}^{2}\] \[+\mathrm{i}\pi\bar{\mu}_{S}(\gamma_{i})\big{(}\frac{1-\alpha}{ \gamma_{i}}+2\alpha\pi\mathsf{H}[\bar{\mu}_{S}](\gamma_{i})\big{)}\bigg{)} \Bigg{]} \tag{86}\]

Note that, in general the MSE of the denoising RIE \(\widehat{\mathbf{\Xi}_{XY}^{*}}(\mathbf{S})\), is less than the MSE of the prdouct of the estimated factors \(\widehat{\mathbf{\Xi}_{X}^{*}}(\mathbf{S})\widehat{\mathbf{\Xi}_{Y}^{*}}(\mathbf{S})\).

In figures 20,21, the MSE of estimating the factors is illustrated for \(c=1\) and \(c=3\) respectively. The MSE of estimating the product is shown in figure 22.

Figure 19: Estimating \(\mathbf{Y}\) with Bernoulli-Rademacher _entries_. MSE is normalized by the norm of the signal, \(\|\mathbf{Y}\|_{\mathbb{E}}^{2}\). \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\), and \(\mathbf{W}\) has i.i.d. Gaussian entries of variance \(1/N\), and \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\), and thresholding function is applied with the best \(h\) among \(\{0,0.1,\cdots,1\}\). Results are averaged over 10 runs (error bars are invisible).

Figure 20: MSE of factorization problem. MSE is normalized by the norm of the signal. \(\mathbf{X}\) is a shifted Wigner matrix with \(c=1\), and both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\). In each run, the observation matrix \(\mathbf{S}\) is generated according to (1), and the factors \(\mathbf{X}\), \(\mathbf{Y}\) are estimated simultaneously from \(\mathbf{S}\). Results are averaged over 10 runs (error bars are invisible). Average relative error between RIEs and Oracle estimators is also reported.

## Appendix F Case of \(\alpha\geq 1\)

In this section we consider the case where \(M\leq N\) and \(N/M\to\alpha\geq 1\) as \(N\to\infty\). Throughout this section \(\boldsymbol{\Gamma}\in\mathbb{R}^{N\times M}\) is a (tall) matrix with \(\boldsymbol{\Gamma}_{M}\) in its upper \(M\times M\) block, and the rest zero entries. \(\boldsymbol{\Gamma}_{M}\) is diagonal matrix constructed from \(\boldsymbol{\gamma}\in\mathbb{R}^{M}\) which are the singular values of \(\boldsymbol{S}\).

Similar to the case of \(\alpha\leq 1\), resolvent of the matrix \(\boldsymbol{\mathcal{S}}\in\mathbb{R}^{(N+M)\times(N+M)}\) plays a central role in deriving the RIES. For the case of \(M\geq N\), with \(\boldsymbol{S}=\boldsymbol{U}_{S}\boldsymbol{\Gamma}\boldsymbol{V}_{S}^{\intercal}\), the matrix \(\boldsymbol{\mathcal{S}}\) has the following eigen-decomposition:

\[\boldsymbol{\mathcal{S}}=\left[\begin{array}{ccc}\hat{\boldsymbol{U}}_{S}^{( 1)}&-\hat{\boldsymbol{U}}_{S}^{(1)}&\boldsymbol{U}_{S}^{(2)}\\ \hat{\boldsymbol{V}}_{S}&-\hat{\boldsymbol{V}}_{S}&\boldsymbol{0}\end{array} \right]\left[\begin{array}{ccc}\boldsymbol{\Gamma}_{M}&\boldsymbol{0}& \boldsymbol{0}\\ \boldsymbol{0}&-\boldsymbol{\Gamma}_{M}&\boldsymbol{0}\\ \boldsymbol{0}&\boldsymbol{0}&\boldsymbol{0}\end{array}\right]\left[\begin{array} []{ccc}\hat{\boldsymbol{U}}_{S}^{(1)}&-\hat{\boldsymbol{U}}_{S}^{(1)}& \boldsymbol{U}_{S}^{(2)}\\ \hat{\boldsymbol{V}}_{S}&-\hat{\boldsymbol{V}}_{S}&\boldsymbol{0}\end{array} \right]^{\intercal} \tag{87}\]

with \(\boldsymbol{U}_{S}=\left[\begin{array}{cc}\boldsymbol{U}_{S}^{(1)}& \boldsymbol{U}_{S}^{(2)}\end{array}\right]\) in which \(\boldsymbol{U}_{S}^{(1)}\in\mathbb{R}^{N\times M}\). And, \(\hat{\boldsymbol{U}}_{S}^{(1)}=\frac{1}{\sqrt{2}}\boldsymbol{U}_{S}^{(1)}\), \(\hat{\boldsymbol{V}}_{S}=\frac{1}{\sqrt{2}}\boldsymbol{V}_{S}\). The resolvent of \(\boldsymbol{\mathcal{S}}\) can be written as:

\[\boldsymbol{G}_{\mathcal{S}}(x-\mathrm{i}\epsilon)=\sum_{k=1}^{2M}\frac{x+ \mathrm{i}\epsilon}{(x-\tilde{\gamma}_{k})^{2}+\epsilon^{2}}\mathbf{s}_{k} \mathbf{s}_{k}^{\intercal}+\frac{x+\mathrm{i}\epsilon}{x^{2}+\epsilon^{2}} \sum_{k=2M+1}^{M+N}\mathbf{s}_{k}\mathbf{s}_{k}^{\intercal}\]

where \(\tilde{\gamma}_{k}\) are the eigenvalues of \(\boldsymbol{\mathcal{S}}\), which are in fact the (signed) singular values of \(\boldsymbol{S}\), \(\tilde{\gamma}_{1}=\gamma_{1},\ldots,\tilde{\gamma}_{M}=\gamma_{M},\tilde{ \gamma}_{M+1}=-\gamma_{1},\ldots,\tilde{\gamma}_{2M}=-\gamma_{M}\).

Figure 21: MSE of factorization problem. MSE is normalized by the norm of the signal. \(\boldsymbol{X}\) is a shifted Wigner matrix with \(c=3\), and both \(\boldsymbol{Y}\) and \(\boldsymbol{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\). In each run, the observation matrix \(\boldsymbol{S}\) is generated according to (1), and the factors \(\boldsymbol{X}\), \(\boldsymbol{Y}\) are estimated simultaneously from \(\boldsymbol{S}\). Results are averaged over 10 runs (error bars are invisible). Average relative error between RIES and Oracle estimators is also reported.

Figure 22: MSE of the product of the factors. MSE is normalized by the norm of the signal \(\|\boldsymbol{XY}\|_{F}^{2}\). \(\boldsymbol{X}\) is a shifted Wigner matrix with \(c=1,c=3\), and both \(\boldsymbol{Y}\) and \(\boldsymbol{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and \(N/M=1/2\). The RIE is applied to \(N=2000,M=4000\). Results are averaged over 10 runs (error bars are invisible).

### Estimating \(\mathbf{X}\)

The RIE for \(\mathbf{X}\) is constructed in the same way as in the case of \(\alpha\leq 1\), (2). However, in the present case the observation matrix \(\mathbf{S}\) has \(M\) (non-trivially zero) singular values and we need to estimate \(N\) eigenvalues for the RIE. As it will be clear, the \(N-M\) eigenvalues are chosen to be equal.

#### f.1.1 Relation between overlap and the resolvent

Define the vectors \(\tilde{\mathbf{x}}_{i}=[\mathbf{x}_{i}^{\intercal},\mathbf{0}_{M}]^{\intercal}\) for \(\mathbf{x}_{i}\) eigenvectors of \(\mathbf{X}\). We have

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}=\sum_{k=1}^{2M}\frac{\epsilon}{(x -\tilde{\gamma}_{k})^{2}+\epsilon^{2}}\big{(}\tilde{\mathbf{x}}_{i}^{\intercal} \mathbf{s}_{k}\big{)}^{2}+\frac{\epsilon}{x^{2}+\epsilon^{2}}\sum_{k=2M+1}^{M +N}\big{(}\tilde{\mathbf{x}}_{i}^{\intercal}\mathbf{s}_{k}\big{)}^{2} \tag{88}\]

Given the structure of \(\mathbf{s}_{k}\)'s in (87), we have:

\[\big{(}\tilde{\mathbf{x}}_{i}^{\intercal}\mathbf{s}_{k}\big{)}^{2}=\begin{cases} \frac{1}{2}\big{(}\mathbf{x}_{i}^{\intercal}\mathbf{u}_{k}\big{)}^{2}&\text{for}\;\;1 \leq k\leq M\\ \frac{1}{2}\big{(}\mathbf{x}_{i}^{\intercal}\mathbf{u}_{k-M}\big{)}^{2}&\text{for}\;\; M+1\leq k\leq 2M\\ \big{(}\mathbf{x}_{i}^{\intercal}\mathbf{u}_{k-M}\big{)}^{2}&\text{for}\;\;2M+1\leq k \leq M+N\end{cases} \tag{89}\]

We assume that in the limit of large N this quantity concentrates on \(O_{X}(\gamma_{j},\lambda_{i})\) and depends only on the singular values and eigenvalue pairs \((\gamma_{j},\lambda_{i})\). This assumption implies that the singular vectors associated with \(0\) singular values (\(\mathbf{u}_{j}\) for \(M+1\leq j\leq N\)) all have the same overlap with the eigenvectors of \(\mathbf{X}\), \(O_{X}(0,\lambda_{i})\). We thus have:

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}\xrightarrow{N\to\infty}\frac{1}{ \alpha}\int_{\mathbb{R}}\frac{\epsilon}{(x-t)^{2}+\epsilon^{2}}O_{X}(t,\lambda_ {i})\bar{\mu}_{S}(t)\,dt+\big{(}1-\frac{1}{\alpha}\big{)}\frac{\epsilon}{x^{2 }+\epsilon^{2}}O_{X}(0,\lambda_{i}) \tag{90}\]

where the overlap function \(O_{X}(t,\lambda_{i})\) is extended (continuously) to arbitrary values within the support of \(\bar{\mu}_{S}\) (the symmetrized limiting singular value distribution of \(\mathbf{S}\)) with the property that \(O_{X}(t,\lambda_{i})=O_{X}(-t,\lambda_{i})\) for \(t\in\mathrm{supp}(\mu_{S})\). Sending \(\epsilon\to 0\), we find

\[\tilde{\mathbf{x}}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\tilde{\mathbf{x}}_{i}\to\frac{1}{\alpha}\pi\bar{\mu}_ {S}(x)O_{X}(x,\lambda_{i})+\big{(}1-\frac{1}{\alpha}\big{)}\pi\delta(x)O_{X}( x,\lambda_{i}) \tag{91}\]

#### f.1.2 Resolvent relation

We derive the resolvent relation for the same model as in (29). The derivation is similar to the procedure explained in section C.1, and we omit here. The final resolvent relation is the same as (42), with parameters satisfying:

\[\begin{cases}\zeta_{1}^{*}=\frac{1}{\alpha}\frac{\mathcal{C}_{\mu\nu}^{(1/ \alpha)}(p_{1}^{*}p_{2}^{*})}{p_{1}^{*}},\quad\zeta_{2}^{*}=\frac{1}{p_{2}^{*}} \big{(}\mathcal{C}_{\mu\nu}^{(1/\alpha)}(p_{1}^{*}p_{2}^{*})+\mathcal{C}_{\mu \nu}^{(1/\alpha)}(p_{2}^{*}p_{3}^{*})\big{)},\quad\zeta_{3}^{*}=\frac{1}{ \alpha}\frac{\mathcal{C}_{\mu\nu}^{(1/\alpha)}(p_{2}^{*}p_{3}^{*})}{p_{3}^{*}} \\ p_{1}^{*}=\frac{1}{\zeta_{3}^{*}}\mathcal{G}_{\rho_{X^{2}}}\big{(}\frac{z- \zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)},\quad p_{2}^{*}=\frac{1}{z-\zeta_{2}^{*}},\quad p_{3}^{*}=\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*2}}\mathcal{G}_{\rho_{X^ {2}}}\big{(}\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)}-\frac{1}{\zeta_{3}^{ *}}\end{cases} \tag{92}\]

Again, with the same procedure as (43),(44), the saddle point equations (91) can be rewritten in a simplified form, which does not involve \(\rho_{X^{2}}\), as:

\[\begin{cases}\zeta_{1}^{*}=\frac{1}{\alpha}\frac{\mathcal{C}_{\mu\nu}^{(1/ \alpha)}(p_{1}^{*}p_{2}^{*})}{p_{1}^{*}},\quad\zeta_{2}^{*}=z-\frac{1}{\partial _{\mu_{S}}(z)},\quad\zeta_{3}^{*}=\frac{1}{\alpha}\frac{\mathcal{C}_{\mu\nu}^{( 1/\alpha)}(p_{2}^{*}p_{3}^{*})}{p_{3}^{*}}\\ p_{1}^{*}=\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)+\big{(}1-\frac{1}{ \alpha}\big{)}\frac{1}{z},\quad p_{2}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z),\quad p _{3}^{*}=\frac{z-\zeta_{1}^{*}}{\alpha\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S} }(z)+\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{(}1-\frac{1}{\alpha}\big{)} \frac{1}{z}-\frac{1}{\zeta_{3}^{*}}\end{cases} \tag{93}\]

with \(\bar{\mu}_{S}\) the limiting ESD of non-trivial singular values of \(\mathbf{S}\). Note that \(\zeta_{1}^{*},\zeta_{2}^{*}\) can be computed from the observation matrix, and we only need to find \(\zeta_{3}^{*}\) satisfying the following equation:

\[(z-\zeta_{1}^{*})\big{[}\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)+\big{(}1 -\frac{1}{\alpha}\big{)}\frac{1}{z}\big{]}-1=\frac{1}{\alpha}\mathcal{C}_{\mu \nu}^{(1/\alpha)}\Big{(}\frac{1}{\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S}}(z)( z-\zeta_{1}^{*})\big{[}\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)+\big{(}1- \frac{1}{\alpha}\big{)}\frac{1}{z}\big{]}\Big{)} \tag{94}\]

Note that both sets of equations (91), (93) and (47), (45) match for \(\alpha=1\).

#### f.1.3 Overlaps and optimal eigenvalues

From (90), (42), for \(\gamma\) a non-trivially zero singular value of \(\mathbf{S}\) we find:

\[O_{X}(\gamma,\lambda_{i}) \approx\frac{\alpha}{\pi\bar{\mu}_{S}(\gamma)}\operatorname{Im} \lim_{z\to-\gamma-\mathfrak{i}0^{+}}\,\mathbf{x}_{i}^{\intercal}\,\zeta_{3}^{*-1} \mathbf{G}_{X^{2}}\big{(}\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)}\,\mathbf{x}_{i} \tag{94}\] \[=\frac{\alpha}{\pi\bar{\mu}_{S}(\gamma)}\operatorname{Im}\lim_{z \to-\gamma-\mathfrak{i}0^{+}}\,\frac{1}{z-\zeta_{1}^{*}-\zeta_{3}^{*}\lambda_ {i}^{2}}\]

And, in the case of \(M>N\), for zero singular values we have:

\[O_{X}(0,\lambda_{i}) \approx\frac{\alpha}{(\alpha-1)\pi}\operatorname{Im}\lim_{z\to- \mathfrak{i}0^{+}}\,\mathbf{x}_{i}^{\intercal}\,\zeta_{3}^{*-1}\mathbf{G}_{X^{2}} \big{(}\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{)}\,\mathbf{x}_{i} \tag{95}\] \[=\frac{\alpha}{(\alpha-1)\pi}\operatorname{Im}\lim_{z\to- \mathfrak{i}0^{+}}\,\frac{1}{z-\zeta_{1}^{*}-\zeta_{3}^{*}\lambda_{i}^{2}}\]

Finally, the optimal eigenvalues can be derived in the same way as in (49). For \(1\leq i\leq M\), we have:

\[\widehat{\xi}_{x\,i}^{*}=\frac{\alpha}{2\kappa\pi\bar{\mu}_{S}( \gamma_{i})}\operatorname{Im}\lim_{z\to\gamma_{i}-\mathfrak{i}0^{+}}\,\bigg{\{} \frac{1}{\zeta_{3}^{*}}\Big{[}\mathcal{G}_{\rho_{X}}\Big{(}\sqrt{\frac{z-\zeta _{1}^{*}}{\kappa\zeta_{3}^{*}}}\Big{)}+\mathcal{G}_{\rho_{X}}\Big{(}-\sqrt{ \frac{z-\zeta_{1}^{*}}{\kappa\zeta_{3}^{*}}}\Big{)}\Big{]}\bigg{\}} \tag{96}\]

And, for all \(M+1\leq i\leq N\) :

\[\widehat{\xi}_{x\,i}^{*}=\frac{\alpha}{2\kappa(\alpha-1)\pi} \operatorname{Im}\lim_{z\to-\mathfrak{i}0^{+}}\,\bigg{\{}\frac{1}{\zeta_{3}^ {*}}\Big{[}\mathcal{G}_{\rho_{X}}\Big{(}\sqrt{\frac{z-\zeta_{1}^{*}}{\kappa \zeta_{3}^{*}}}\Big{)}+\mathcal{G}_{\rho_{X}}\Big{(}-\sqrt{\frac{z-\zeta_{1}^ {*}}{\kappa\zeta_{3}^{*}}}\Big{)}\Big{]}\bigg{\}} \tag{97}\]

#### f.1.4 Numerical Examples

For matrices \(\mathbf{Y},\mathbf{W}\in\mathbb{R}^{N\times M}\) with i.i.d. Gaussian entries of variance \(\nicefrac{{1}}{{N}}\) and \(M>N\), we have that \(\mathcal{C}_{\mu\gamma}^{(\nicefrac{{1}}{{\alpha}})}(z)=\mathcal{C}_{\mu\nu}^{ (\nicefrac{{1}}{{\alpha}})}(z)=z\) which leads to a simplification of equations (92):

\[\begin{cases}\zeta_{1}^{*}=\frac{1}{\alpha}p_{2}^{*},\quad\zeta_{2}^{*}=z-\frac {1}{\bar{\sigma}_{\mu_{S}}(z)},\quad\zeta_{3}^{*}=\frac{1}{\alpha}p_{2}^{*}\\ \\ p_{1}^{*}=\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)+\big{(}1-\frac{1}{ \alpha}\big{)}\frac{1}{z},\quad p_{2}^{*}=\mathcal{G}_{\bar{\mu}_{S}}(z),\quad p _{3}^{*}=\frac{z-\zeta_{1}^{*}}{\alpha\zeta_{3}^{*}}\mathcal{G}_{\bar{\mu}_{S }}(z)+\frac{z-\zeta_{1}^{*}}{\zeta_{3}^{*}}\big{(}1-\frac{1}{\alpha}\big{)} \frac{1}{z}-\frac{1}{\zeta_{3}^{*}}\end{cases} \tag{98}\]

Therefore, \(\zeta_{1}^{*}=\zeta_{3}^{*}=\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)\).

In Figure 23, the MSE of the Oracle estimator and the RIE (96), (97) is illustrated for shifted Wigner \(\mathbf{X}\) with \(c=3\), and Wishart with aspect-ratio \(\alpha^{\prime}=\nicefrac{{1}}{{4}}\).

Effect of aspect-ratio \(\alpha\).In Figure 24, we take \(\mathbf{X}\) to be a shifted Wigner matrix with \(c=3\), and the MSE is depicted for various values of the aspect-ratio \(\alpha>1\). We see that as \(M\) decreases (\(\alpha\) increases) the estimation error (of \(\mathbf{Y}\)) increases.

Figure 23: Estimating \(\mathbf{X}\). The MSE is normalized by the norm of the signal, \(\|\mathbf{X}\|_{F}^{2}\). Both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\), and aspect ratio \(N/M=2\). The RIE is applied to \(N=2000,M=1000\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\widetilde{\mathbf{\Xi}}_{X}^{*}(\mathbf{S})\) and Oracle estimator is also reported.

### Estimating \(\mathbf{Y}\)

#### f.2.1 Relation between overlap and the resolvent

For the vectors \(\mathbf{r}_{i}=\left[\begin{array}{c}\mathbf{0}_{N}\\ \mathbf{y}_{i}^{(r)}\end{array}\right]\), \(\mathbf{l}_{i}=\left[\begin{array}{c}\mathbf{y}_{i}^{(l)}\\ \mathbf{0}_{M}\end{array}\right]\) with \(\mathbf{y}_{i}^{(r)},\mathbf{y}_{i}^{(l)}\) right/ left singular vectors of \(\mathbf{Y}\), we have

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x- \mathrm{i}\epsilon)\big{)}\mathbf{l}_{i}=\sum_{k=1}^{2M}\frac{\epsilon}{(x-\tilde{ \gamma}_{k})^{2}+\epsilon^{2}}\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k} \big{)}\big{(}\mathbf{l}_{i}^{\intercal}\mathbf{s}_{k}\big{)}+\frac{\epsilon}{x^{2 }+\epsilon^{2}}\sum_{k=2M+1}^{M+N}\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k} \big{)}\big{(}\mathbf{l}_{i}^{\intercal}\mathbf{s}_{k}\big{)} \tag{99}\]

Given the structure of \(\mathbf{s}_{k}\)'s in (87), we have:

\[\big{(}\mathbf{r}_{i}^{\intercal}\mathbf{s}_{k}\big{)}\big{(}\mathbf{l}_{i}^{ \intercal}\mathbf{s}_{k}\big{)}=\begin{cases}\frac{1}{2}\big{(}\mathbf{u}_{k}^{ \intercal}\mathbf{y}_{i}^{(l)}\big{)}\big{(}\mathbf{v}_{k}^{\intercal}\mathbf{y}_{i}^{(r )}\big{)}&\text{for }\,1\leq k\leq M\\ -\frac{1}{2}\big{(}\mathbf{u}_{k-M}^{\intercal}\mathbf{y}_{i}^{(l)}\big{)}\big{(}\mathbf{ v}_{k-M}^{\intercal}\mathbf{y}_{i}^{(r)}\big{)}&\text{for }\,M+1\leq k\leq 2M\\ 0&\text{for }\,2M+1\leq k\leq N+M\end{cases}\]

Therefore, in the limit \(N\to\infty\), we have:

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x-\mathrm{i} \epsilon)\big{)}\mathbf{l}_{i}\xrightarrow{N\to\infty}\frac{1}{\alpha}\int_{ \mathbb{R}}\frac{\epsilon}{(x-t)^{2}+\epsilon^{2}}O_{Y}(t,\sigma_{i})\bar{\mu} _{S}(t)\,dt \tag{100}\]

where the overlap function \(O_{Y}(t,\lambda_{i})\) is extended (continuously) to arbitrary values within the support of \(\bar{\mu}_{S}\) with the property that \(O_{Y}(-t,\lambda_{i})=-O_{Y}(t,\lambda_{i})\) for \(t\in\mathrm{supp}(\mu_{S})\). Sending \(\epsilon\to 0\), we find

\[\mathbf{r}_{i}^{\intercal}\big{(}\mathrm{Im}\,\mathbf{G}_{\mathcal{S}}(x-\mathrm{i} \epsilon)\big{)}\mathbf{l}_{i}\approx\frac{1}{\alpha}\pi\bar{\mu}_{S}(x)O_{Y}(x, \sigma_{i}) \tag{101}\]

Figure 24: MSE of estimating \(\mathbf{X}\) as a function of aspect-ratio \(\alpha>1\), prior on \(\mathbf{X}\) is shifted Wigner with \(c=3\), and \(\kappa=5\). MSE is normalized by the norm of the signal, \(\|\mathbf{X}\|_{\mathbb{F}}^{2}\). Both \(\mathbf{Y}\) and \(\mathbf{W}\) are \(N\times M\) matrices with i.i.d. Gaussian entries of variance \(1/N\). The RIE is applied to \(N=2000,M=\nicefrac{{1}}{{\alpha}}N\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\overline{\mathbf{\Xi}}_{X}^{*}(\mathbf{S})\) and Oracle estimator is also reported.

#### f.2.2 Resolvent relation

The resolvent relation for the model (60) with \(M<N\) is the same as in (72) with parameters satisfying:

\[\begin{cases}\beta_{1}^{*}=\frac{1}{\alpha}\frac{\mathcal{C}_{\mu\nu}^{(\alpha)}(q _{1}^{*}q_{2}^{*})}{q_{1}^{*}}+\frac{1}{2}\sqrt{\frac{q_{2}^{*}}{q_{1}^{*}}} \Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)} -\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)} \\ \beta_{2}^{*}=\frac{\mathcal{C}_{\mu\nu}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{q_{2}^ {*}}\\ \beta_{3}^{*}=\frac{1}{2}\sqrt{\frac{q_{1}^{*}}{q_{3}^{*}}}\Big{(}\mathcal{R}_{ \rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}-\mathcal{R}_{\rho_{X }}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)}\\ \beta_{4}^{*}=\frac{1}{2}\Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_ {1}^{*}q_{3}^{*}}\big{)}+\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*} q_{3}^{*}}\big{)}\Big{)}\\ q_{1}^{*}=\frac{(z-\beta_{1}^{*})\beta_{4}^{*2}}{Z_{2}(z)^{2}}\mathcal{G}_{\rho_{Y }}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}+\frac{1}{\alpha}\frac{\beta_{3}^{*}}{ Z_{2}(z)}+\frac{\alpha-1}{\alpha}\frac{1}{z-\beta_{1}^{*}}\\ q_{2}^{*}=\frac{-\beta_{1}^{*}}{Z_{2}(z)}\mathcal{G}_{\rho_{Y}}\big{(}\frac{Z_{ 1}(z)}{Z_{2}(z)}\big{)}\\ q_{3}^{*}=\frac{1}{\alpha}\frac{(z-\beta_{1}^{*})\beta_{4}^{*2}}{Z_{2}(z)^{2}} \mathcal{G}_{\rho_{Y}}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}-\frac{1}{\alpha }\frac{\beta_{4}^{*}}{Z_{2}(z)}\\ q_{4}^{*}=\frac{1}{\alpha}\frac{\beta_{4}^{*}Z_{1}(z)}{Z_{2}(z)^{2}}\mathcal{G}_{ \rho_{Y}}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}-\frac{1}{\alpha}\frac{\beta_{ 4}^{*}}{Z_{2}(z)}\end{cases}\qquad\text{with }\begin{cases}Z_{1}(z)=(z-\beta_{1}^{*})(z-\beta_{2}^{*})\\ Z_{2}(z)=\beta_{4}^{*2}+\beta_{3}^{*}(z-\beta_{1}^{*})\\ \end{cases} \tag{102}\]

With the same procedure as (73),(74), the saddle point equations (102) can be rewritten in a simplified form:

\[\begin{cases}\beta_{1}^{*}=\frac{1}{\alpha}\frac{\mathcal{C}_{\mu\nu}^{(\alpha)}( q_{1}^{*}q_{2}^{*})}{q_{1}^{*}}+\frac{1}{2}\sqrt{\frac{q_{2}^{*}}{q_{1}^{*}}} \Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)} -\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)} \\ \beta_{2}^{*}=\frac{\mathcal{C}_{\mu\nu}^{(\alpha)}(q_{1}^{*}q_{2}^{*})}{q_{2}^ {*}}\\ \beta_{3}^{*}=\frac{1}{2}\sqrt{\frac{q_{1}^{*}}{q_{3}^{*}}}\Big{(}\mathcal{R}_{ \rho_{X}}\big{(}q_{4}^{*}+\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}-\mathcal{R}_{\rho_{X }}\big{(}q_{4}^{*}-\sqrt{q_{1}^{*}q_{3}^{*}}\big{)}\Big{)}\\ \beta_{4}^{*}=\frac{1}{2}\Big{(}\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}+\sqrt{q _{1}^{*}q_{3}^{*}}\big{)}+\mathcal{R}_{\rho_{X}}\big{(}q_{4}^{*}-\sqrt{q_{1}^{ *}q_{3}^{*}}\big{)}\Big{)}\\ q_{1}^{*}=\frac{1}{\alpha}\mathcal{G}_{\tilde{\mu}_{S}}(z)+\big{(}1-\frac{1}{ \alpha}\big{)}\frac{1}{z}\\ q_{2}^{*}=\mathcal{G}_{\tilde{\mu}_{S}}(z)\\ q_{3}^{*}=\frac{(z-\beta_{1}^{*})}{\beta_{4}^{*2}}q_{1}^{*}-\frac{z-\beta_{1}^{* }}{\beta_{4}^{*2}}\\ q_{4}^{*}=\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}}q_{1}^{*}-\frac{1}{\beta_{4}^{*}} \end{cases} \tag{103}\]

Note that both sets of equations (101), (103) and (59), (76) match for \(\alpha=1\).

#### f.2.3 Overlaps and optimal singular values

From (72), (101), we have:

\[\begin{split} O_{Y}(\gamma,\sigma_{i})&\approx\frac{ \alpha}{\pi\tilde{\mu}_{S}(\gamma)}\operatorname{Im}\lim_{z\to\gamma-10^{+}} \frac{\beta_{4}^{*}}{Z_{2}(z)}\boldsymbol{y}_{i}^{(r)\,\intercal}\boldsymbol{G} _{Y\,\intercal}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\boldsymbol{Y}^{\intercal} \boldsymbol{y}_{i}^{(l)}\\ &=\frac{\alpha}{\pi\tilde{\mu}_{S}(\gamma)}\operatorname{Im}\lim_{z \to\gamma-10^{+}}\beta_{4}^{*}\frac{\sigma_{i}}{Z_{1}(z)-Z_{2}(z)\sigma_{i}^{2}} \end{split} \tag{104}\]

Similar to (78), we can compute the optimal singular values to be:

\[\widehat{\xi}_{y\,i}^{*}=\frac{\alpha}{\pi\tilde{\mu}_{S}(\gamma_{i})} \operatorname{Im}\lim_{z\to\gamma_{i}-10^{+}}q_{4}^{*} \tag{105}\]

#### f.2.4 Numerical examples

We consider the matrix \(\boldsymbol{W}\) to have i.i.d. Gaussian entries with variance \(\nicefrac{{1}}{{N}}\), so \(\mathcal{C}_{\mu\nu}^{(\nicefrac{{1}}{{\alpha}})}(z)=z\). And, \(\boldsymbol{X}=\boldsymbol{F}+c\boldsymbol{I}\) where \(\boldsymbol{F}=\boldsymbol{F}^{\intercal}\in\mathbb{R}^{N\times N}\) has i.i.d. entries with variance \(\nicefrac{{1}}{{N}}\), and \(c\neq 0\) is a real number, so \(\mathcal{R}_{\rho_{X}}(z)=z+c\). With these choices, the solution (103) simplifies to:

\[\begin{cases}\beta_{1}^{*}=\frac{1}{\alpha}q_{2}^{*}+q_{3}^{*},\quad\beta_{2}^{* }=q_{1}^{*},\quad\beta_{3}^{*}=q_{1}^{*},\quad\beta_{4}^{*}=q_{4}^{*}+c\\ q_{1}^{*}=\frac{1}{\alpha}\mathcal{G}_{\tilde{\mu}_{S}}(z)+\big{(}1-\frac{1}{ \alpha}\big{)}\frac{1}{z},\quad q_{2}^{*}=\mathcal{G}_{\tilde{\mu}_{S}}(z)\\ q_{3}^{*}=\frac{(z-\beta_{1}^{*})^{*2}}{\beta_{4}^{*2}}q_{1}^{*}-\frac{z-\beta_{ 1}^{*}}{\beta_{4}^{*2}},\quad q_{4}^{*}=\frac{z-\beta_{1}^{*}}{\beta_{4}^{*}}q_{1 }^{*}-\frac{1}{\beta_{4}^{*}}\end{cases} \tag{106}\]After a bit of algebra, we find that \(q_{4}^{*}\) is the solution to the following qubic equation:

\[\begin{split} 2x^{3}+3c\,x^{2}+\Big{[}c^{2}+2&-\big{(}z- \frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)\big{)}\big{(}\frac{1}{\alpha} \mathcal{G}_{\bar{\mu}_{S}}(z)+\frac{\alpha-1}{\alpha z}\big{)}\Big{]}\,x\\ &-c\Big{[}\big{(}z-\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z) \big{)}\big{(}\frac{1}{\alpha}\mathcal{G}_{\bar{\mu}_{S}}(z)+\frac{\alpha-1}{ \alpha z}\big{)}-1\Big{]}=0\end{split} \tag{107}\]

In figure 25 the MSE of RIE and the oracle estimator is plotted for two cases of priors: \(\mathbf{Y}\) with Gaussian entries and \(\mathbf{Y}\) with uniform spectral density.

Effect of aspect-ratio \(\alpha\).In Figure 26, we take \(\mathbf{Y}\) to have Gaussian entries (with variance \(\frac{1}{N}\)), and the MSE is depicted for various values of the aspect-ratio \(\alpha>1\). We see that as \(M\) decreases (\(\alpha\) increases) the estimation error (of \(\mathbf{Y}\)) increases.

## Appendix G Details on numerical implementations

### Numerical approximation of \(\mathcal{G}_{\bar{\mu}_{S}}(z)\)

The first step to construct the RIEs is to compute the Stieltjes transform of the observation matrix \(\mathbf{S}\). In section 19.5 of [58], several approaches have been proposed to approximate the Stieltjes transform of the spectral density of a given matrix. In our implementations, we use the Cauchy kernel method in which for a given matrix \(\mathbf{A}\) with \(N\) singular values (or eigenvalues) \(\big{(}\sigma_{i}\big{)}_{1\leq i\leq N}\), \(\mathcal{G}_{\mu_{A}}(z)\)

Figure 26: MSE of estimating \(\mathbf{Y}\) as a function of aspect-ratio \(\alpha>1\), \(\mathbf{Y}\) has Gaussain entries of variance \(\nicefrac{{1}}{{N}}\), and \(\kappa=5\). MSE is normalized by the norm of the signal, \(\|\mathbf{Y}\|_{2}^{2}\). \(\mathbf{X}\) is a shifted Wigner matrix with \(c=3\), and \(\mathbf{W}\) has i.i.d. Gaussian entries of variance \(1/N\). The RIE is applied to \(N=2000\), \(M=\nicefrac{{1}}{{\alpha}}N\), and the results are averaged over 10 runs (error bars are invisible). Average relative error between RIE \(\widehat{\mathbf{\Xi}_{Y}}(\mathbf{S})\) and Oracle estimator is also reported.

is approximated as:

\[\mathcal{G}_{\mu_{A}}(z)\approx\frac{1}{N}\sum_{i=1}^{N}\frac{1}{z-\sigma_{i}- \mathrm{i}\eta_{i}}\]

with \(\eta_{i}\)'s the "widths" of the kernel at each singular value (more precisely the imaginary part is a sum of Lorentzians with width \(\eta_{i}\) around peaks at \(\sigma_{i}\)). The construction of the RIEs uses the Stieltjes transform of the limiting symmetrized measure of \(\mathbf{S}\). In the numerical experiments we approximate this quantity as:

\[\mathcal{G}_{\bar{\mu}_{S}}(z)\approx\frac{1}{2N}\sum_{i=1}^{N}\biggl{(}\frac{ 1}{z-\gamma_{i}-\mathrm{i}\eta}+\frac{1}{z+\gamma_{i}-\mathrm{i}\eta}\biggr{)} \tag{108}\]

with a fixed width \(\eta=\sqrt{\nicefrac{{1}}{{2N}}}\). Note that for the case of \(\alpha>1\) (\(M<N\)), \(\mathbf{S}\) has \(M\) non-trivially zero singular values, and in the approximation above \(N\) should be replaced by \(M\).

### Construction of the RIEs

In the RIEs derived in [19, 14], the final estimator for optimal singular values (eigenvalues) was rather simple and only required to compute the Stieltjes transform on the real line which can be easily and safely performed using the approximation above (see remark 2 in section 19.5.2 in [58]). However, in the RIEs of this work, we need to solve a system of equations in the limit \(\epsilon\to 0\) (\(z\) close to the real line). For this, to compute the optimal singular value \(\hat{\xi}_{y\,i}^{*}\) (or optimal eigenvalues \(\hat{\xi}_{x\,i}^{*}\)), we evaluate \(\mathcal{G}_{\bar{\mu}_{S}}(z)\) for \(z=\gamma_{i}-\mathrm{i}\frac{\varepsilon}{\sqrt{2N}}\). In this way, the other parameters (e.g. \(q_{4}^{*}\)) are evaluated for \(z\) very close to the real line, and the theoretical limit \(\lim_{\epsilon\to 0}\) in (49), (78) can be estimated numerically. Moreover, as we considered a fixed width in our numerical approximation of Stieltjes transform (108), \(\varepsilon\) should be chosen to compensate the width for the cases where the support of \(\bar{\mu}_{S}\) is wider. For example, for fixed \(N\), as we increase SNR (from \(1\) to \(5\)) the support of \(\bar{\mu}_{S}\) grows, however we still have \(N\) singular values and the kernel's width in (108) is fixed, so \(\varepsilon\) should be larger for higher SNRs to get a more accurate approximation of \(\mathcal{G}_{\bar{\mu}_{S}}(z)\).

### Mismatch between RIEs and Oracle estimators

The RIEs are conjectured to have the same performance as the Oracle estimators in the limit \(N\to\infty\). Therefore, we believe that the mismatch between the proposed RIEs and the Oracle estimators is a finite size effect. Moreover, this finiteness affects the accuracy of estimated parameters, since \(\mathcal{G}_{\bar{\mu}_{S}}(z)\) is approximated numerically and we do not use random matrix theory to find its exact form.

Generically, the mismatch between the RIE and Oracle estimator is larger for the case of estimating \(\mathbf{X}\). We expect that this is because of the extra approximation step in the derivation of the optimal eigenvalues. In the fifth line of (49), the sums are approximated by an integral which is the Stieltjes transform of \(\rho_{X}\). This approximation does not appear in derivation of the optimal singular values for \(\mathbf{Y}\), see (78).

All in all, the small relative error (less than \(1\%\)) between RIEs and Oracle estimators in our numerical results validates our optimality conjecture and demonstrates that RIEs can be successfully used in practice.

## Appendix H Spherical integrals and matrix lemmas

### Spherical Integrals

For two symmetric matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{N\times N}\), the _spherical integral_ is defined as:

\[\mathcal{I}_{N}(\mathbf{A},\mathbf{B})=\Big{\langle}\exp\big{\{}\frac{N}{2}\operatorname {Tr}\mathbf{A}\mathbf{U}\mathbf{B}\mathbf{U}^{\intercal}\big{\}}\Big{\rangle}_{\mathbf{U}}\]

where the average is w.r.t. the _Haar_ measure over the group of (real) orthogonal \(N\times N\) matrices. The spherical integrals can also be defined w.r.t. the unitary or symplectic group. These integrals are often referred to as _Harish Chandra-Itzykson-Zuber_ (HCIZ) integrals in mathematical physics literature. The study of these objects dates back to the work of mathematician Harish Chandra [62]

[MISSING_PAGE_FAIL:48]

Inverse of \(\mathbf{C}^{*}_{X}\)For \(\mathbf{C}^{*}_{X}\) since the blocks \(\mathbf{B},\mathbf{C}\) are zero, the inverse is simply:

\[\mathbf{C}^{*\,-1}_{X} =\left[\begin{array}{cc}\left[(z-\zeta^{*}_{1})\mathbf{I}_{N}-\zeta^{ *}_{3}\mathbf{X}^{2}\right]^{-1}&\mathbf{0}\\ \mathbf{0}&\left[(z-\zeta^{*}_{2})\mathbf{I}_{M}\right]^{-1}\end{array}\right]\] \[=\left[\begin{array}{cc}\frac{1}{\zeta^{*}_{3}}\left[\frac{z- \zeta^{*}_{1}}{\zeta^{*}_{3}}\mathbf{I}_{N}-\mathbf{X}^{2}\right]^{-1}&\mathbf{0}\\ \mathbf{0}&\frac{1}{z-\zeta^{*}_{2}}\mathbf{I}_{M}\end{array}\right] \tag{112}\] \[=\left[\begin{array}{cc}\frac{1}{\zeta^{*}_{3}}\mathbf{G}_{X^{2}} \left(\frac{z-\zeta^{*}_{1}}{\zeta^{*}_{3}}\right)&\mathbf{0}\\ \mathbf{0}&\frac{1}{z-\zeta^{*}_{2}}\mathbf{I}_{M}\end{array}\right]\]

Inverse of \(\mathbf{C}^{*}_{Y}\)Let the block structure of \(\mathbf{C}^{*}_{Y}\) be as in Proposition 3, then

\[(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1} =\left((z-\beta^{*}_{2})\mathbf{I}_{M}-\beta^{*}_{3}\mathbf{Y}^{\intercal }\mathbf{Y}-\frac{\beta^{*2}_{4}}{z-\beta^{*}_{1}}\mathbf{Y}^{\intercal}\mathbf{Y}\right)^ {-1}\] \[=\left((z-\beta^{*}_{2})\mathbf{I}_{M}-\left(\beta^{*}_{3}+\frac{\beta ^{*2}_{4}}{z-\beta^{*}_{1}}\right)\mathbf{Y}^{\intercal}\mathbf{Y}\right)^{-1}\] \[=(z-\beta^{*}_{1})\Big{(}Z_{1}(z)\mathbf{I}_{M}-Z_{2}(z)\mathbf{Y}^{ \intercal}\mathbf{Y}\Big{)}^{-1}\] \[=\frac{z-\beta^{*}_{1}}{Z_{2}(z)}\Big{(}\frac{Z_{1}(z)}{Z_{2}(z)} \mathbf{I}_{M}-\mathbf{Y}^{\intercal}\mathbf{Y}\Big{)}^{-1}\] \[=\frac{z-\beta^{*}_{1}}{Z_{2}(z)}\mathbf{G}_{Y^{\intercal}Y}\Big{(} \frac{Z_{1}(z)}{Z_{2}(z)}\Big{)}\]

where \(\mathbf{G}_{Y^{\intercal}Y}\) is the resolvent of the matrix \(\mathbf{Y}^{\intercal}\mathbf{Y}\). So, we have

\[\mathbf{C}^{*\,-1}_{Y}=\left[\begin{array}{cc}(z-\beta^{*}_{1})^{-1}\mathbf{I}_{N}+ \frac{\beta^{*2}_{4}}{\beta^{*2}_{4}}(z)\mathbf{Y}\mathbf{G}_{Y^{\intercal}Y}\big{(} \frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\mathbf{Y}^{\intercal}&\frac{\beta^{*}_{4}}{Z_{2} (z)}\mathbf{Y}\mathbf{G}_{Y^{\intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\\ \frac{\beta^{*}_{4}}{Z_{2}(z)}\mathbf{G}_{Y^{\intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_ {2}(z)}\big{)}\mathbf{Y}^{\intercal}&\frac{z-\beta^{*}_{1}}{Z_{2}(z)}\mathbf{G}_{Y^{ \intercal}Y}\big{(}\frac{Z_{1}(z)}{Z_{2}(z)}\big{)}\end{array}\right]\]

**Lemma 3**.: _Consider two vectors \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{N}\). The symmetric matrix \(\mathbf{x}\mathbf{y}^{\intercal}+\mathbf{y}\mathbf{x}^{\intercal}\) has rank at most two with non-zero eigenvalues \(\mathbf{x}^{\intercal}\mathbf{y}\pm\|\mathbf{x}\|\|\mathbf{y}\|\)._

Proof.: Construct the matrices \(\mathbf{A}\in\mathbb{R}^{2\times N},\mathbf{B}\in\mathbb{R}^{N\times 2}\) as follows:

\[\mathbf{A}=\left[\begin{array}{cc}\mathbf{x}^{\intercal}\\ \mathbf{y}^{\intercal}\end{array}\right],\quad\mathbf{B}=\left[\begin{array}{cc} \mathbf{y}&\mathbf{x}\end{array}\right]\]

Then, we have that \(\mathbf{x}\mathbf{y}^{\intercal}+\mathbf{y}\mathbf{x}^{\intercal}=\mathbf{B}\mathbf{A}\). Using the lemma 4, we have that:

\[z^{2}\det\left(z\mathbf{I}_{N}-\mathbf{B}\mathbf{A}\right)=z^{N}\det\left(z\mathbf{I}_{2}-\bm {A}\mathbf{B}\right)\]

So, the characteristic polynomial of \(\mathbf{x}\mathbf{y}^{\intercal}+\mathbf{y}\mathbf{x}^{\intercal}\) is \(z^{N-2}\det\left(z\mathbf{I}_{2}-\mathbf{A}\mathbf{B}\right)\), which implies that the \(\mathbf{x}\mathbf{y}^{\intercal}+\mathbf{y}\mathbf{x}^{\intercal}\) has eigenvalue \(0\) with multiplicity \(N-2\), plus the eigenvalues of the \(2\times 2\) matrix \(\mathbf{A}\mathbf{B}\). The matrix \(\mathbf{A}\mathbf{B}\) is:

\[\mathbf{A}\mathbf{B}=\left[\begin{array}{cc}\mathbf{x}^{\intercal}\mathbf{y}&\|\mathbf{x}\|^{2} \\ \|\mathbf{y}\|^{2}&\mathbf{x}^{\intercal}\mathbf{y}\end{array}\right]\]

which has two eigenvalues \(\mathbf{x}^{\intercal}\mathbf{y}\pm\|\mathbf{x}\|\|\mathbf{y}\|\). 

**Lemma 4**.: _For matrices \(\mathbf{A}\in\mathbb{R}^{M\times N},\mathbf{B}\in\mathbb{R}^{N\times M}\), we have:_

\[z^{M}\det\left(z\mathbf{I}_{N}-\mathbf{B}\mathbf{A}\right)=z^{N}\det\left(z\mathbf{I}_{M}-\bm {A}\mathbf{B}\right)\]

Proof.: Construct the matrices \(\mathbf{C},\mathbf{D}\in\mathbb{R}^{(M+N)\times(M+N)}\) as follows:

\[\mathbf{C}=\left[\begin{array}{cc}z\mathbf{I}_{M}&\mathbf{A}\\ \mathbf{B}&\mathbf{I}_{N}\end{array}\right],\quad\mathbf{D}=\left[\begin{array}{cc}\mathbf{ I}_{M}&\mathbf{0}_{M\times N}\\ -\mathbf{B}&z\mathbf{I}_{N}\end{array}\right]\]

We have:

\[\det\mathbf{C}\mathbf{D}=z^{N}\det\left(z\mathbf{I}_{M}-\mathbf{A}\mathbf{B}\right),\quad\det\mathbf{D }\mathbf{C}=z^{M}\det\left(z\mathbf{I}_{N}-\mathbf{B}\mathbf{A}\right)\]

The result follows from the fact that \(\det\mathbf{C}\mathbf{D}=\det\mathbf{D}\mathbf{C}\).