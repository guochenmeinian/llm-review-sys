## 1 Introduction

Initially introduced as a minor and vaguely defined feature in OpenAI's GPT API, the concept of the system message has grown significantly in popularity and usage. The research and broader AI community have achieved a degree of consensus on the intent and purpose of system messages: a higher privilege message type for providing instructions that apply throughout the conversation and superseding any conflicting instructions in user messages. System messages are used today to define custom large language model (LLM) applications, implement model guardrails and content policies, defend against jailbreak attacks and prompt injection, establish role-playing personas, or otherwise steer model behaviors.

Yet unlike the concept of user/system privilege in traditional computing, system message precedence is much less reliable as it is implemented through training and therefore susceptible to errors and adversarial attacks like many other neural network behaviors. Models can easily "forget" their system messages when faced with long contexts, or be tricked into intentionally violating them.

Despite their popularity and importance, the robustness of system messages remain understudied by the scientific community. In this work, we conduct a simple empirical study of supervised fine-tuning LLMs to improve their system message adherence and robustness. We first collect a synthetic dataset for supervised fine-tuning, starting from real-world system instructions sourced from OpenAI's GPT Store and HuggingFace's HuggingChat platform and leveraging proprietary LLMs to generate user and assistant responses. We then put together a small battery of benchmarks modified from previous work to quantify robustness and compare against other supervised fine-tuning data mixes. We will release all data and code to the research community to encourage further research on this topic.

## 2 Related Work

### Prompt Injection

Prompt injections are a form of adversarial attack targeting LLM applications where a user overrides trusted instructions set by the application developer with new instructions to trigger unintended behaviors (Perez & Ribeiro, 2022; Willison, 2024b). These attacks have become a significant security risk for LLM-based applications (LLMRisksArchive; Greshake et al., 2023; Willison, 2024a;* [142] Liu et al., 2024b; Rehberger, 2024c), with numerous production systems already compromised (PromptArmor, 2024; Rehberger, 2024a; Willison, 2022). To explore the range of potential attacks and assess model robustness, previous work has organized large-scale prompt hacking competitions and games, resulting in extensive datasets and benchmarks of adversarial and defensive prompts (Schulhoff et al., 2023; Toyer et al., 2023; Al, 2023; Debenedetti et al., 2024). We use these curated, human-generated attacks and similar benchmarks to evaluate model robustness (Mu et al., 2024), though emerging techniques for automated attacks may be valuable for future research (Liu et al., 2024a; Yu et al., 2024a).
* [143] Various defenses have been proposed to protect LLM applications from prompt injections, but most rely on defensive prompts or separate modules in larger systems(ProtectAI.com, 2024; Abdelnabi et al., 2024) rather than on intrinsic model-level robustness. Chen et al. (2024); Zverev et al. (2024) and Yi et al. (2024) focus on differentiating between instructions and data and propose initial methods for training models to recognize this distinction. These papers are mainly concerned with defending against indirect prompt injection attacks (Hines et al., 2024; Greshake et al., 2023) and do not explore instruction precedence. The most similar work to ours is by Wallace et al. (2024), though their data and models are not publicly available, limiting their utility for advancing model-level defenses and understanding system prompt behavior.

### System Instructions

System messages are a powerful way to steer LLMs and specify preferences for their behavior at inference time (Touvron et al., 2023; Mukherjee et al., 2023; Jiang et al., 2024). Application developers can use them to provide context and rules for a model to follow during conversations with a user, allowing for fairly complex applications to be built without task-specific model training(OpenAI, 2023; 2024b). However, despite the intended behavior specified by system prompts, these rules are relatively easy to bypass (Yu et al., 2024b) and malicious users can steal valuable IP like the system prompt itself, any files provided in context, etc.

To address this issue, Wallace et al. (2024) and Lu et al. (2024) emphasize the importance of training models to prioritize instructions and show that post-training techniques like instruction tuning and RLHF can significantly enhance robustness against conflicting instructions. Despite this training, users have still found simple ways of prompt injecting models trained with an instruction hierarchy, highlighting the need for more research in this direction (OpenAI, 2024a; Rehberger, 2024b; Pliny and Liberator, 2024).

Beyond adversarial robustness, there is a growing body of research examining other uses and properties of system messages. Lee et al. (2024) describe a method to train LLMs using system messages to better align with diverse user preferences. Meanwhile, Li et al. (2024a) demonstrate that LLMs often struggle to consistently maintain specified personas across multiple turns of conversation. Concurrent with this work, Qin et al. (2024) introduce a benchmark to evaluate many features of system prompts at once, including multi-turn stability, system message priority, and compliance over many domains.

## 3 Data Collection

In order to build a useful dataset for fine-tuning stronger system message adherence, we need realistic system messages, challenging and relevant user messages, and exemplary assistant messages. Our overall pipeline is illustrated in Figure 1. We begin by collecting a set of real-world LLM system prompts from OpenAI's GPT Store and HuggingFace's HuggingChat platform. After a series of filtering steps, we are left with 1386 unique system messages. We then prompt Claude 3.5 Sonnet to generate 5 benign and 5 adversarial user messages corresponding to each system message. Finally, we build a simple tool-enabled assistant running on GPT-4o mini with working web search/browsing and Python tools, along with a mock image generation tool. We prompt this assistant with our system and user messages and collect the responses, altogether yielding 13.9K training samples and 3.8M training (assistant) tokens which we call the sudo dataset. Our LLM prompts and dataset examples are shown in Appendix B.

### System Messages

To source realistic system messages, we first turn to OpenAI's GPT Store which hosts user-created custom GPTs defined by a system prompt and a set of enabled tools such as web browsing or custom REST APIs for the model to use. These custom GPTs are built by users for a wide variety of use cases, including commercial purposes, and contain many different guardrails.

The prompts that define a custom GPT are often carefully guarded, with many containing instructions to never reveal or even discuss the contents of the prompt. However, the susceptibility of even leading LLMs like GPT-4 to prompt injection tricks means that it is usually quite easy to extract a system prompt. We started with two public collections of previously extracted prompts1, and joined this with GPT Store metadata indicating which tools are available to the custom GPT. We use this metadata information to remove any system messages that expect file/image uploads from the user message and any messages that rely on custom HTTP APIs besides browsing, after which 619 distinct system messages remain.

Footnote 1: https://github.com/Oxeb/TheBigPromptLibrary/

We also conduct a scrape of user created assistants on HuggingFace's HuggingChat platform, for which system messages are publicly visible. Starting with 4244 system messages, we remove exact duplicates which leaves 2716 system messages. We then combine both the GPT Store and HuggingChat system messages for further filtering to remove extremely long prompts (\(>\) 4000 Mistral 7B tokens), partially duplicated prompts2, non-English prompts, and obscene prompts. Finally, we use Claude 3.5 Sonnet to extract all discrete guardrail clauses from each system message and remove

Figure 1: The data collection process for creating our sudo dataset. We rely heavily on model-based and LLM-based quality filtering, and generate user and assistant messages with Claude and GPT models, respectively.

ones with less than 3 separate guardrail clauses. This selects for more complex and interesting prompts, while filtering out many low-quality prompts and role-playing "persona" prompts. In all, this yields 1386 distinct system messages.

### User Messages

In order to collect demonstrations of assistant responses that robustly prioritize the system message, we first need to generate challenging, adversarial user messages that try to convince the assistant to violate one or more guardrails in the system message. To retain model utility and avoid inappropriate _overrefusals_, we also require thorough coverage of benign user message for which the assistant is able to demonstrate helpful, high-quality responses. We find that with a bit of prompting, Claude 3.5 Sonnet is able to synthesize creative and relevant user messages. Claude's adversarial user messages were surprisingly creative, often targeting various different guardrails within the system message via rather indirect means.

### Assistant Messages

GPT-4o mini is OpenAI's first model release trained with the improved defense methods explored in wallace2020learning. Though smaller and less adept at complex reasoning, we find that it is plenty capable of producing high-quality responses to both benign and adversarial user messages in our dataset. Since many of the GPT Store assistants revolve around tool-calling, we implement 4 simple tools for GPT-4o mini to use in its responses: web search using Brave, web browsing using Scrapfly, local Python script execution, and a dummy image generation API that records the model's image prompt. These tools, particularly the two web tools, add significant variation and diversity to our dataset by pulling in information external to the model and the system/user messages.

## 4 Benchmarks and Evaluations

We use three benchmarks, RuLES, Gandalf, and TensorFlowTrust, to quantify system message robustness. Additionally to measure the general utility of the model and verify that we are not overfitting to the particular behavior of system message compliance, we also rely on MMLU and Arena-Hard-Auto.

### Robustness

**RuLES (Mu et al., 2024).** RuLES is a benchmark consisting of 14 text-based scenarios designed to assess rule-following ability in LLMs in a programmatic way. The scenarios simulate common security tasks or children's games with one or more rules that define the intended behavior of the model. These rules either mandate or prohibit certain behaviors, mirroring the helpful-harmless criteria for model alignment (Bai et al., 2022). Each scenario also includes an evaluation function to score the model responses as passing or failing.

The benchmark defines three suites of test cases: benign, basic, and redteam. These test cases consist of user messages designed to stress test the abilities of a model to adhere to the rules specified in the scenario instructions. Benign tests fill a conversation with snippets from Ultrachat3 and are designed to assess whether models can understand the rules at the most basic level and avoid accidental violations. Basic tests follow a similar structure but contain direct requests to violate a particular rule. Redteam tests consist of human-crafted attack strategies to trick the model into breaking the rules.

Footnote 3: https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k

We slightly modify the original evaluation setting, which place the rules in the first user message and include explicit reminders to prioritize these rules. To measure whether models are intrinsically assigning higher precedence to system messages without explicit guidance to do so, we remove the precedence reminders and move the scenario instructions into the system message.

**Gandalf (AI, 2023).** Gandalf is a simple prompt injection game where players attempt to prompt the model to reveal a secret password embedded in the system message. We create a benchmark by combining all splits from a set of cleaned and filtered prompts for the Summarizer Gandalf

[MISSING_PAGE_FAIL:5]

### Datasets

**Daring Anteater.** Daring Anteater is a wide-ranging instruction-tuning dataset released by Nvidia Research, covering a wide variety of tasks and settings such as multi-turn conversations, precise instruction following, and open-domain question answering (Wang et al., 2024). It was used in initial SFT training prior to further preference optimization.

**SystemChat 1.1.** SystemChat 1.1 is a community dataset consisting of 20K training samples each with a system message and one or more user messages pertaining to the system messages. The system messages range from persona specifications to IF-Eval style constraints. Some system messages are repeated several hundred times throughout the dataset, so we also experiment with a dedeuplicated version that randomly samples up to 3 conversations with the same system message.

**subo.** We collect a new dataset based on system messages found in real applications, which we call sudo. Our dataset contains 14K total conversations with 10 conversations per unique system message. Unlike the other datasets, our dataset also contains tool definitions and tool use examples since it includes many system messages from OpenAI's GPT Store which rely extensively on tools such as web-browsing, though such functionality is not tested in our benchmarks.

The different datasets we study all contain varying numbers of examples, turns, and tokens per turn, which makes it less straightforward to conduct controlled experiments. We opted to control for the total number of training tokens, which in this setting is equal to the total number of assistant tokens as we mask out the training loss for tokens in system, user, and tool response messages. In our main experiments, we set a target of 5M training tokens. As SystemChat 1.1 contains more than 5M training tokens, we randomly sample the dataset without replacement to produce a subset with 5M samples. The deduplicated SystemChat, as well as our sudo dataset, both contain fewer than 5M training tokens so we randomly sample from Daring Anteater to fill out the datasets to 5M tokens.

### LoRA Fine-tuning

We fine-tune all models with LoRA adapters (Hu et al., 2021) applied to all linear layers and the input embedding and freeze all other parameters in the model. We use \(r=16\) and \(\alpha=16\) with no dropout for the adapters. While we experimented with full fine-tuning, we find that LoRA tuning is much faster and cheaper, while also yielding stronger models likely due to the intrinsic regularizing effects of LoRA.

Although we control for the total number training tokens per run, the training dynamics vary across different mixtures due to differences in the distributions of turns per sample, training tokens per sample, and other factors outlined in Table 1. Notably, since we train with a fixed batch size of training samples, the number of iterations per training run will differ as well as the number of training tokens per iteration.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & **Daring Anteater** & **SystemChat 1.1** & **SystemChat 1.1 dedup.** & **sudo** \\ \hline samples & 99,532 & 20,216 & 2,329 & 13,864 \\ system messages & 746 & 20,216 & 2,329 & 13,864 \\ train tokens & 173,833,710 & 6,268,808 & 1,057,159 & 3,767,898 \\ train tokens / sample & 1746.5 & 310.1 & 453.9 & 271.8 \\ messages / sample & 5.8 & 12.3 & 10.7 & 3.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of the raw datasets used in this work. Train tokens refers to tokens on which a loss value is computed, i.e. assistant tokens. Messages per sample counts system, user, assistant, and tool messages if present.

## 6 Results

Our main results are show in Table 2. Starting from Mistral 7B Instruct v0.3, a well-trained chat model released by Mistral, we see a large increase in model performance on the RuLES, TensorTrust, and Gandalf metrics when fine-tuning on our sudo dataset. Note that as discussed in Section 5.2, we are training on data samples equalized on the number of training tokens (5M), so Daring Anteater and SystemChat 1.1 are both downsampled while the remaining three entries require additional data which we randomly sample from Daring Anteater. Daring Anteater, which by itself lacks coverage of user queries that are adversarial to the system message, performs quite poorly across all three robustness metrics though it yields a small boost in chat performance as measured on Arena-Hard-Auto. Both SystemChat 1.1 and the deduplicated version are able to improve robustness metrics, but suffer a large drop in win rate vs. Mistral 7B Instruct v0.3 on Arena-Hard-Auto. Qualitatively, we notice models trained on this dataset offer shorter and less detailed responses, which may explain the reduction in win rate.

Training on our full sudo dataset outperforms the other models in terms of robustness by a wide margin, and also achieves the highest win rate in Arena-Hard-Auto without losing too much performance on MMLU. We also experimented with a training dataset containing a reduced number (50%) of samples from sudo and more samples from Daring Anteater, resulting in lower robustness and showing that training on more data from our pipeline is better.

## 7 Discussion

Considering the relatively poor performance of Mistral-7B Instruct v0.3 and the absence of clear delimiters for system messages in its default chat template, we hypothesize that optimizing system message utility and robustness was not a primary focus during Mistral's original training. Our experiments fine-tuning on our new sudo dataset demonstrate that it is relatively straightforward to enhance prompt injection robustness with minimal degradation in the model's general utility. Beyond supervised fine-tuning, other alignment techniques like DPO and RLHF could be explored in future work to further improve performance.

Additionally, our released dataset and models will enable the community to conduct further research using model internals and other white-box analysis techniques. For instance, attention patterns before and after training could be analyzed using methods similar to (Li et al., 2024). Additionally, probes and interventions using model internals have been shown to be useful for detecting and preventing undesirable model behavior and may be more effective after explicitly training against prompt injections (Abdelnabi et al., 2024; Zou et al., 2024). We believe these lines of inquiry will be essential for developing more robust defenses against prompt injection attacks in LLM applications.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & **RuLES** & **Gandalf** & **TT** & **AHA** & **MMLU** \\ \hline Mistral 7B Instruct v0.3 & 4.65 & 23.6\% & 34.5\% & 50.0 & 60.1\% \\ \hline Daring Anteater & 3.52 & 19.3\% & 39.2\% & 55.7 & 53.8\% \\ SystemChat 1.1 & 5.90 & 40.7\% & 49.5\% & 5.7 & 55.4\% \\ SystemChat 1.1 ddebt. & 5.98 & 20.7\% & 47.1\% & 24.3 & 54.5\% \\ Sudo (50\%) & 6.98 & 49.3\% & 48.9\% & 56.9 & 57.0\% \\ Sudo & 7.45 & 57.1\% & 49.4\% & 60.2 & 54.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Fine-tuning on our newly collected sudo dataset significantly improves system message robustness on RuLES, Gandalf, and TensorTrust (TT), compared to the starting model of Mistral 7B Instruct v0.3 and other fine-tunes.**sudo also preserves utility scores on Arena-Hard-Auto (AHA) and MMLU. All fine-tuned models are trained on a fixed 5M training (assistant) tokens constructed either by downsampling or padding with additional data sampled from Daring Anteater.

## References

* Abdelnabi et al. (2024) Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, and Andrew Paverd. Are you still on track? Catching LLM Task Drift with Activations, July 2024.
* Gandalf (2023) Lakera AI. Gandalf, 2023. URL https://gandalf.lakera.ai/.
* Bai et al. (2024) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. April 2022.
* Chen et al. (2024) Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. StruQ: Defending Against Prompt Injection with Structured Queries, September 2024.
* Debenedetti et al. (2024) Edoardo Debenedetti, Javier Rando, Daniel Paleka, Silaghi Fineas Florin, Dragos Albastroiu, Niv Cohen, Yuval Lemberg, Reshmi Ghosh, Rui Wen, Ahmed Salem, Giovanni Cherubin, Santiago Zanella-Beguelin, Robin Schmid, Victor Klemm, Takahiro Miki, Chenhao Li, Stefan Kraft, Mario Fritz, Florian Tramer, Sahar Abdelnabi, and Lea Schonherr. Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition, June 2024.
* Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection, May 2023.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* Hines et al. (2024) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. Defending Against Indirect Prompt Injection Attacks With Spotlighting, March 2024.
* Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models, October 2021.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lilio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7B. October 2023.
* Jiang et al. (2024) Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, and Jad Kabbara. PersonalLLM: Investigating the Ability of Large Language Models to Express Personality Traits, April 2024.
* Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_. Association for Computational Linguistics, 2022.
* Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. Aligning to Thousands of Preferences via System Message Generalization, May 2024.
* Li et al. (2024) Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Measuring and Controlling Instruction (In)Stability in Language Model Dialogs, July 2024a.
* Li et al. (2024) Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. _arXiv preprint arXiv:2406.11939_, 2024b.
* Liu et al. (2024) Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. Automatic and Universal Prompt Injection Attacks against Large Language Models, March 2024a.

* Liu et al. (2024b) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt Injection attack against LLM-integrated Applications, March 2024b.
* LLMRisksArchive (2020) LLMRisksArchive. LLMRisks Archive. https://genai.owasp.org/llm-top-10/.
* Lu et al. (2024) Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, and Yongbin Li. SoFA: Shielded On-the-fly Alignment via Priority Rule Following, February 2024.
* Mu et al. (2024) Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, and David Wagner. Can LLMs Follow Simple Rules?, March 2024.
* Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive Learning from Complex Explanation Traces of GPT-4, June 2023.
* OpenAI (2023) OpenAI. Introducing GPTs. https://openai.com/index/introducing-gpts/, November 2023.
* OpenAI (2024) OpenAI. GPT-4o mini: Advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/, July 2024a.
* OpenAI (2024) OpenAI. Introducing the GPT Store. https://openai.com/index/introducing-the-gpt-store/, January 2024b.
* Perez and Ribeiro (2022) Fabio Perez and Ian Ribeiro. Ignore Previous Prompt: Attack Techniques For Language Models, November 2022.
* Dix (2024) Pliny the Liberator. @simonw @TheXeophon @kyliebytes https://tc.co/fZ3LTprQQp, July 2024.
* PromptArmor (2024) PromptArmor. Data Exfiltration from Slack AI via indirect prompt injection, August 2024.
* ProtectAL.com (2024) ProtectAL.c. Fine-tuned deberta-v3-base for prompt injection detection, 2024. URL https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2.
* Qin et al. (2024) Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, and Bin Cui. SysBench: Can Large Language Models Follow System Messages?, August 2024.
* Rehberger (2024) Johann Rehberger. Google AI Studio continues to struggle with data exfiltration vulnerabilities This demo shows silent data exfiltration of employee feedback and performance reviews through prompt injection in one of the feedback entries. The POC triggers data exfiltration via rendering https://tc.co/Xam1nYOl6m, August 2024a.
* Rehberger (2024b) Johann Rehberger. Breaking Instruction Hierarchy in OpenAI's gpt-4o-mini : Embrace The Red. https://embracethered.com/blog/posts/2024/chatgpt-gpt-4o-mini-instruction-hierarchie-bypasses/, July 2024b.
* Rehberger (2024c) Johann Rehberger. The dangers of AI agents unfurling hyperlinks and what to do about it : Embrace The Red. https://embracethered.com/blog/posts/2024/the-dangers-of-unfurling-and-what-you-can-do-about-it/, April 2024c.
* Schulhoff et al. (2023) Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Francois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, and Jordan Boyd-Graber. Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition. October 2023.
* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Innan, Marcin Kardas, Viktor Kerkez, Madan Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jens Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh* [486] Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.
* [487] Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. Tensor Trust: Interpretable prompt injection attacks from an online game, 2023. URL https://arxiv.org/pdf/2311.01011.pdf.
* [488] Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions, April 2024.
* [489] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Nersimhan Sreedhar, and Oleksii Kuchaiev. Helpster2: Open-source dataset for training top-performing reward models, 2024.
* [500] Simon Willison. Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/, September 2022.
* [501] Simon Willison. Accidental prompt injection against RAG applications. https://simonwillison.net/2024/Jun/6/accidental-prompt-injection/, June 2024a.
* [502] Simon Willison. Prompt injection and jailbreaking are not the same thing. https://simonwillison.net/2024/Mar/5/prompt-injection-jailbreaking/, March 2024b.
* [503] Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models, March 2024.
* [51] Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi, and Xinyu Xing. PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs, September 2024a.
* [515] Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Sabrina Yang, and Xinyu Xing. Assessing Prompt Injection Risks in 200+ Custom GPTs, May 2024b.
* [516] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [521] Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving Alignment and Robustness with Circuit Breakers, July 2024.
* [522] Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, and Christoph H. Lampert. Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?, June 2024.
* [523]

[MISSING_PAGE_EMPTY:11]

[MISSING_PAGE_FAIL:12]

[MISSING_PAGE_EMPTY:13]

## Appendix C TensorFlow-Hust Benchmark Construction

We construct our hijacking and extraction benchmarks from data released by Toyer et al. (2023)6 and filter out test cases that contain post-prompts. For the hijacking benchmark, we use the following regular expression in accordance with their paper7 to determine whether the model passed:

Footnote 6: https://github.com/HumanCompatibleAI/tensor-trust-data

Footnote 7: https://github.com/HumanCompatibleAI/tensor-trust/blob/

 re.compile(r"\W*access granted\b", re.IGNORECASE)

We then create the helpfulness benchmark based on their specifications for defense validity, namely that the model should output "Access Granted" given the correct access code. To obtain test cases, we combine the defenses included in the hijacking and extraction benchmarks and deduplicate by filtering out samples with the same pre-prompt. Each test case in the benchmark thus contains the pre-prompt as a system message and the access code as the user message. The extraction, hijacking, and helpfulness benchmarks contain 105, 165, and 239 test cases respectively. We report the average pass rate across all three benchmarks in our results in Table 2.