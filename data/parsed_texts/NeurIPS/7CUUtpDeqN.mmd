Analytically deriving Partial Information Decomposition for affine systems of stable and convolution-closed distributions

 Chaitanya Goswami

Electrical & Computer Engineering,

Carnegie Mellon University,

Pittsburgh, PA-15213.

cgoswami@andrew.cmu.edu

&Amanda Merkley

Electrical & Computer Engineering,

Carnegie Mellon University,

Pittsburgh, PA-15213.

amerkley@andrew.cmu.edu

###### Abstract

Bivariate partial information decomposition (PID) has emerged as a promising tool for analyzing interactions in complex systems, particularly in neuroscience. PID achieves this by decomposing the information that two sources (e.g., different brain regions) have about a target (e.g., a stimulus) into unique, redundant, and synergistic terms. However, the computation of PID remains a challenging problem, often involving optimization over distributions. While several works have been proposed to compute PID terms _numerically_, there is a surprising dearth of work on computing PID terms _analytically_. The only known analytical PID result is for jointly Gaussian distributions. In this work, we present two theoretical advances that enable analytical calculation of the PID terms for numerous well-known distributions, including distributions relevant to neuroscience, such as Poisson, Cauchy, and binomial. Our first result generalizes the analytical Gaussian PID result to the much larger class of stable distributions. We also discover a theoretical link between PID and the emerging fields of data thinning and data fission. Our second result utilizes this link to derive analytical PID terms for two more classes of distributions: convolution-closed distributions and a sub-class of the exponential family. Furthermore, we provide an analytical upper bound for approximately computing PID for convolution-closed distributions, whose tightness we demonstrate in simulation.

## 1 Introduction

Bivariate partial information decomposition1 (PID) is an information-theoretic framework developed for answering a central inquiry in many neuroscientific and machine learning studies: _how do two sources, \(X\) and \(Y\), jointly process information about a target \(M\)?_ PID answers this question by quantifying the \(M\)-specific information contained in different interactions between \(X\) and \(Y\). Specifically, it decomposes the total information \(X\) and \(Y\) have about \(M\) into _four_ components: (i) the information about \(M\) contained uniquely in \(X\), (ii) the information about \(M\) contained uniquely in \(Y\), (iii) the redundant information about \(M\) contained in both \(X\) and \(Y\), and (iv) the synergistic information about \(M\) which arises from the interaction between \(X\) and \(Y\).

Footnote 1: Throughout this work we refer to bivariate PID as PID.

The PID terms offer novel insights for understanding interactions within complex systems, particularly in neuroscience. For instance, PID has been used to understand the firing patterns of grid cells [1; 2] and to study the flow of information in the visual cortex [3]. The following list of works [1; 3; 4; 5; 6; 7; 8] demonstrate the application of PID in studying diverse neuroscientific questions.

Beyond neuroscience, PID has found applications in multimodal learning for interpreting model predictions [9; 10], fair machine learning for defining and quantifying bias [11; 12; 13; 14], and understanding financial markets [15].

However, the primary constraint hindering broader adoption of PID is the difficulty of computing and estimating the PID terms. BROJA-PID [16], a widely applied PID framework, requires solving a constrained minimization problem over a set of probability distributions (see Sec. 2). This minimization problem can pose a considerable challenge, particularly when the underlying distributions are continuous. As a result, several recent works have been dedicated to providing _numerical algorithms_ that solve the aforementioned minimization problem exactly or approximately [3; 9; 17; 18; 19; 20], with more emphasis on the case of discrete distributions.

Despite significant progress in _numerically_ calculating PID (typically through discrete approximations), very few works exist on _analytically2_ calculating PID. As of this writing, the only known analytical PID expressions exist for jointly Gaussian \(M\), \(X\), and \(Y\)[21; 22]. A fundamental property of the Gaussian system is that one of the unique information (UI) terms in its PID is guaranteed to be _zero_. This property greatly simplifies the computation of PID for Gaussian systems, as the rest of the PID terms can be easily derived by solving desirable linear equations specified in many PID frameworks (see Sec 2), bypassing the need for optimizing over distributions (see Sec. 3).

Footnote 2: Section 2 precisely defines the notion of “analytical calculating PID” adopted in this work.

In this work, we show that numerous systems of random variables \(M\), \(X\), and \(Y\) expressing a particular "affine dependence structure" also exhibit the same fundamental property that at least one of the UI terms is zero. Consequently, we expand significantly on the existing Gaussian PID result by using this fundamental property to analytically compute PID for various systems of \(M\), \(X\), and \(Y\) employing well-known distributions such as Poisson, exponential, gamma, beta, negative binomial, multinomial, Cauchy, Levy-stable and more. The main contributions of this work are:

1. We extend the Gaussian PID result to a much larger class of distributions, known as the stable distribution family [23; 24] in Sec. 4. These results provide the first known analytical PID for fat-tailed distributions.
2. We highlight a theoretical link between PID calculation and the fields of data thinning [25] and data fission [26] in Sec. 5. We utilize this link to derive analytical PID terms for two more distribution families: convolution-closed distributions [25] and certain exponential family distributions [26].
3. For convolution-closed distributions, we further derive an analytical upper bound on the objective of the minimization used for computing BROJA-PID. We use this upper bound to approximately compute PID for systems of \(M\), \(X\), and \(Y\) having a non-affine dependence structure. We show the goodness of our approximation by a simulation study in Sec. 6.

## 2 Background

**Notation**: We denote the set of all natural numbers, real numbers, and positive real numbers as \(\mathbb{N}\), \(\mathbb{R}\), and \(\mathbb{R}^{+}\), respectively. Vectors are denoted by bold-faced font and an arrow, and matrices are denoted by bold-faced font. Define \(\mathbb{N}_{0}=\mathbb{N}\cup\{0\}\). Let \(\mathbf{I}_{d}\) be the identity matrix of size \(d\times d\) and \([d]=\{1,\ldots,d\}\ \forall\ d\in\mathbb{N}\). We denote \(\mathbf{\tilde{I}}_{d}\) and \(\mathbf{\bar{0}}_{d}\) as \(d\)-dimensional vectors having all elements as \(1\) and \(0\), respectively. For brevity, the probability notations of the form \(P(A|B)\) and \(P(A)\) are always understood to be as \(P(A=a|B=b)\) and \(P(A=a)\), respectively. The general term 'distribution' is used to refer to both probability density function (p.d.f.) and probability mass function (p.m.f.) (whichever is appropriate depending upon the context). The symbol \(A\perp\!\!\!\perp B|C\) denotes that \(A\) and \(B\) are conditionally independent given \(C\), and similarly, \(A\perp\!\!\!\perp B\) implies \(A\) and \(B\) are independent. The \(L_{p}\)-norm is denoted by \(\|(\cdot)\|_{p}\ \forall\ p\in[0,\infty)\), and \(|(\cdot)|\) denotes the absolute function.

**PID Background**: Suppose \(M,X,Y\) are random variables with joint distribution \(P(M,X,Y)\). According to [16; 27], three desirable equalities for a bivariate PID are as follows:

\[I_{P}(M;[X,Y])=UI(M;X\backslash Y)+UI(M;Y\backslash X)+RI(M;X,Y)+ SI(M;X,Y),\] \[I_{P}(M;X)\!=\!UI(M;X\backslash Y)\!+\!RI(M;X,Y),\] \[I_{P}(M;Y)\!=\!UI(M;Y\backslash X)\!+\!RI(M;X,Y).\] (1)Here, \(I_{P}(M;X)\) and \(I_{P}(M;Y)\) denote the mutual information [28] between \(M\) and \(X\), and \(M\) and \(Y\), respectively, under the distribution \(P(M,X,Y)\). Similarly, \(I_{P}(M;[X,Y])\) denotes the joint mutual information between \(M\) and \([X,Y]\) under the distribution \(P(M,X,Y)\). \(UI(M;X|Y)\) and \(UI(M;Y\backslash X)\) are the unique information about \(M\) in \(X\) and \(Y\), respectively. \(SI(M;X,Y)\) and \(RI(M;X,Y)\) denote the respective synergistic and redundant information shared between \(X\) and \(Y\) about \(M\). The linear system defined in (1) contains four variables in three equations. Hence, only one of \(UI,RI\), or \(SI\) need be defined to evaluate all four PID terms. Proposing a suitable definition of PID is the focus of much research [16, 27, 29, 30, 31, 32, 33, 34, 35] and all our theoretical results are applicable for any Blackwellian PID [22, 36] or a PID definition satisfying the assumption (\(*\)) of Bertschinger et al. [16] (see Appx. B for a formal justification). For example, our results are applicable for the PID definitions proposed in [16, 22, 27, 29, 37]. In this work, we discuss our results in the context of the BROJA-PID [16, 38], which is the definition of PID adopted in previous works [3, 9, 20]:

\[UI(M;X\backslash Y)=\min_{Q\in\Delta_{P}}I_{Q}(M;X|Y)\text{ or equivalently }UI(M;Y \backslash X)=\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X),\] (2)

where \(\Delta_{P}\!=\!\{Q(M,X,Y)\!:\!Q(M,X)\!=\!P(M,X),Q(M,Y)=\!P(M,Y)\}\) and \(I_{Q}(M;X|Y)\) is the conditional mutual information under the distribution \(Q(M,X,Y)\). Note that the minimizing distributions for both problems shown in (2) are the same [16], and consequently, both minimization problems are equivalent. We refer the reader to [39] for a review on PID. In this work, we define "analytically calculating PID" as analytically solving (2) by providing an explicit construction of the minimizing distribution \(Q^{*}(M,X,Y)\) that minimizes (2). Note that only the distributions \(Q^{*}(M,X,Y)\) and \(P(M,X,Y)\) are needed to compute the BROJA-PID terms.

We briefly describe two distribution families used in Sec. 4 and Sec. 5 (see Appx. N for more details).

**Stable distribution family**: Stable distributions are a family of distributions that naturally arise in the context of generalized central limit theorems. Some well-known members of this family are the Gaussian, Cauchy, Poisson, and Levy distributions. A defining feature of stable distributions is that the sum of two independent copies of a random variable \(X\), denoted as \(X_{1}\) and \(X_{2}\), has the same distribution as a translated and scaled version of \(X\)[23, 24, 40]. In this work, we consider five sub-classes of stable distributions:

(i) _Continuous stable distributions_ are parameterized through four parameters: stability parameter \(\alpha\in(0,2]\), skewness parameter \(\beta\in[-1,1]\), scale parameter \(\gamma\in(0,\infty)\), and location parameter \(\mu\in\mathbb{R}\). We denote its p.d.f. as \(p_{CS}(\alpha,\beta,\gamma,\mu)\). Note that all continuous stable distribution (except Gaussian) are fat-tailed.

(ii) _Independent component multivariate stable distributions_ describe the distribution of \(\vec{\mathbf{X}}\) consisting of \(d\) independent random variables \(\{X_{j}\}_{j=1}^{d}\), such that each \(X_{j}\sim p_{CS}(\alpha,\beta_{j},\gamma_{j},\mu_{j})\). We denote its p.d.f. as \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\).

(iii) _Elliptically-contoured multivariate stable distributions_ are the distributions of continuous stable random vectors whose p.d.f. has elliptical contours, e.g., the multivariate Gaussian distribution. We denote its p.d.f. as \(p_{CS-EC}(\alpha,\boldsymbol{\Sigma},\vec{\boldsymbol{\mu}})\). Here, \(\boldsymbol{\Sigma}\) is a positive definite matrix, \(\vec{\boldsymbol{\mu}}\in\mathbb{R}^{d}\), and \(\alpha\in(0,2]\).

(iv) _Discrete stable distributions_ are the discrete analogues of the continuous stable distributions. The p.m.f. of discrete stable distributions, denoted as \(P_{DS}(\nu,\tau)\), are parameterized through two parameters: rate parameter \(\tau>0\) and exponent \(0<\nu\leq 1\). Discrete stable distributions do not have well-known multivariate generalizations except the Poisson distribution.

(v) _Multivariate Poisson distribution_: We use the multivariate Poisson distribution proposed in [41, 42, 43, 44], denoted as Poisson\((d,d^{\prime},\vec{\boldsymbol{\Lambda}})\), which represents each random variable in the random vector \(\vec{\mathbf{N}}\) as a sum of independent Poisson random variables. Formally, \(\vec{\mathbf{N}}\sim\text{Poisson}(d,d^{\prime},\vec{\boldsymbol{\Lambda}})\) if \(\vec{\mathbf{N}}=\mathbf{A}\vec{\mathbf{N}}^{g}\), where \(\mathbf{A}=[\mathbf{A}_{1}\ldots\mathbf{A}_{d^{\prime}}]\). Here, \(\mathbf{A}_{i}\) denotes a \(d\times\binom{d}{i}\) submatrix having no duplicate columns, where each of its columns contain exactly \(i\) ones and \((d-i)\) zeros [44]. The vector

\[\vec{\mathbf{N}}^{g}=[N_{1}^{g}\ldots N_{d}^{g}\ N_{12}^{g}\ldots\ N_{d-(d^{ \prime}-1)\ldots d}^{g}]^{T}\]

with mutually independent \(N_{i_{1}\ldots i_{j}}^{g}\sim\text{Poisson}(\lambda_{i_{1}\ldots i_{j}})\ \forall\ (i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d},j\in[d^{\prime}]\), where \(\mathbb{A}_{i}^{d}=\{(j_{1},\ldots,j_{i})\!:\!j_{1}<j_{2}<\ldots<j_{i},\text{ and }j_{1},\ldots,j_{i}\in[d]\}\), e.g., \(\mathbb{A}_{2}^{3}=\{(1,2),(1,3),(2,3)\}\). Note that \(d^{\prime}\leq d\), and \(\vec{\boldsymbol{\Lambda}}=\begin{bmatrix}\lambda_{1}\ \ldots\ \lambda_{d-(d^{\prime}-1)\ldots d}\end{bmatrix}^{T}\).

We refer the reader to [23; 24; 40; 42; 44; 45; 46; 47] for more details on stable distributions.

**Convolution-closed distributions**: Convolution-closed distributions form a large class of distributions that are closed under convolution in some parameter \(\delta\). Formally, we define the convolution-closed distribution as follows: Let \(\mathcal{F}_{\mathcal{D}}\) denote a family of distributions, where each member distribution \(f(\delta)\in\mathcal{F}_{\mathcal{D}}\) is indexed by a parameter \(\delta\in\mathcal{D}\). Consider \(X_{1}\sim f(\delta_{1})\), \(X_{2}\sim f(\delta_{2})\), and \(X_{1}\perp\!\!\!\perp X_{2}\) for some \(\delta_{1},\delta_{2}\in\mathcal{D}\). Then, \(\mathcal{F}_{\mathcal{D}}\) is convolution-closed in the parameter \(\delta\) if

\[X_{1}+X_{2}\sim f(\delta_{1})*f(\delta_{2})=f(\delta_{1}+\delta_{2})\;\forall \;\delta_{1},\delta_{2}\in\mathcal{D}\text{ such that }\delta_{1}+\delta_{2}\in \mathcal{D},\] (3)

where \(*\) denotes the convolution operator. Many well-known distributions can be considered convolution-closed in some parameter [25], such as the Poisson distribution, Gaussian distribution, gamma distribution, etc. Table 1 in [25] lists various examples of convolution-closed distributions.

## 3 A sufficient condition for computing PID terms analytically

The main focus of this work is to analyze the cases where (2) is analytically solvable. A sufficient condition for solving (2) is to show that \(\Delta_{P}\) contains a distribution \(Q_{MC}(M,X,Y)\) with the Markovian structure \(M\to X\to Y\) or \(M\to Y\to X\). We briefly discuss the argument justifying this sufficient condition. Consider the case where \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and has the Markovian structure \(M\to X\to Y\). First, we note that \(UI(M;Y\backslash X)=\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)\geq 0\) due to the non-negativity of conditional mutual information [28]. Second, \(Y\perp\!\!\!\perp M|X\) due to the Markovian structure of \(Q_{MC}(M,X,Y)\), which implies that \(I_{Q_{MC}}(M;Y|X)=0\) (conditionally independent random variables have zero conditional mutual information [28]). Hence, \(Q_{MC}(M,X,Y)\) achieves the lower bound of zero for the minimization problem \(\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)\), showing that \(Q_{MC}(M,X,Y)\) indeed minimizes (2). A similar argument can be made for the case when \(Q_{MC}(M,X,Y)\) has the Markovian structure \(M\to Y\to X\). Proposition 1 in Appx. A formalizes the above argument.

The above sufficient condition provides an easy way to analytically calculate the PID terms, as it ensures one of the unique information terms is always zero. Consequently, the remaining PID terms can be calculated by substituting zero for the appropriate unique information term in (1) and solving the resultant linear system. Hence, if applicable, the above sufficient condition considerably simplifies the calculation of PID terms by circumventing the need for optimizing over a set of distributions. Surprisingly, many well-known distribution families allow intuitive constructions of \(P(M,X,Y)\), for which the above sufficient condition is applicable. In the following sections, we provide theorems that specify sufficient conditions under which the existence of these Markov chains in \(\Delta_{P}\) can be guaranteed for these \(P(M,X,Y)\).

## 4 Computing PID for stable distributions

In this section, we extend existing results for analytical PID computation of jointly Gaussian \(M\), \(X\), and \(Y\)[21; 22] to the much larger class of stable distributions. Our results utilize two key observations to provide the generalization of the Gaussian results: (i) We first identify that the analytical computation of PID for jointly Gaussian systems is due to their particular "affine dependence" structure; (ii) We show that these particular affine dependence structures are not unique to Gaussian systems, but rather extend to many members of the stable distribution family.

For jointly Gaussian \(M\), \(X\), and \(Y\), the conditional distributions \(P(X|M){=}\mathcal{N}(aM+b,\sigma_{X}^{2})\) and \(P(Y|M){=}\mathcal{N}(cM+d,\sigma_{Y}^{2})\) are also Gaussian distributions, where their means are an affine function of \(M\) and their variances are fixed with respect to \(M\). This particular affine dependence of \(X\) and \(Y\) on \(M\) is the key to the analytical calculation of their PID, as it guarantees existence of a Markov chain \(Q_{MC}(M,X,Y)\in\Delta_{P}\). Thus, we can apply the sufficient condition described in Sec. 3 to compute the PID terms. We illustrate through an example: consider \(P(X|M){=}\mathcal{N}(M,\sigma_{X}^{2})\) and \(P(Y|M){=}\mathcal{N}(M,\sigma_{Y}^{2})\) where \(\sigma_{X}^{2}<\sigma_{Y}^{2}\), and \(M\sim P(M)\) for some appropriate \(P(M)\). Then, we can explicitly construct \(Q_{MC}(M,X,Y)\in\Delta_{P}\) with the Markovian structure \(M\to X\to Y\) as follows: Choose \(Q_{MC}(M,X,Y){=}P(M)P(X|M)Q_{MC}(Y|X)\) where \(Q_{MC}(Y|X)\) is specified through the addition of independent Gaussian noise, i.e., \(Y{=}X+\epsilon\). Here, \(\epsilon\sim\mathcal{N}(0,\sigma_{Y}^{2}-\sigma_{X}^{2})\) and \(\epsilon\perp\!\!\!\perp(M,X)\). It is easy to verify that \(Q_{MC}(Y|M){=}\mathcal{N}(M,\sigma_{Y}^{2})\), which implies that \(Q_{MC}(M,Y){=}P(M,Y)\). By construction, \(Q_{MC}(M,X){=}P(M,X)\), and hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

The above example is a special case of a well-known result where, for a scalar \(M\), one can always construct a lower signal-to-noise ratio (SNR) "Gaussian channel", i.e. \(P(Y|M)\) in our example, by adding independent Gaussian noise to a higher SNR "Gaussian channel" [22; 48], i.e. \(P(X|M)\) in our example. Surprisingly, the technique of adding independent noise to construct Markov chains contained in \(\Delta_{P}\) can be extended to \(P(X|M)\) and \(P(Y|M)\) as members of stable distributions. Theorems 1 and 4 generalize the above construction for \(P(X|M)\) and \(P(Y|M)\) as members of univariate continuous and univariate discrete stable distributions, respectively. Theorems 2, 3, and 5 consider the case of multivariate stable distributions (see Sec. 2). The key technique for proving these theorems is that a Markov chain \(Q_{MC}(M,X,Y)\in\Delta_{P}\) can always be constructed by adding appropriate independent noise to a higher SNR \(P(X|M)\) to obtain a lower SNR \(P(Y|M)\), similar to our above example.

### PID of univariate affine continuous stable system

Theorem 1 can be viewed as a direct generalization of Barrett's Gaussian PID result [21] to stable distributions, showing one of the UI terms is always zero. We begin by formally describing the univariate affine continuous stable system that generalizes the Gaussian system of Barrett's [21]. Let the joint distribution of \(M,X,\) and \(Y\), denoted as \(P(M,X,Y)\), satisfy the following properties: (i) \(M\sim P(M)\) with support set \(\mathcal{M}\subseteq\mathbb{R}\); (ii) \(P(X|M)\) and \(P(Y|M)\) are univariate continuous stable distributions with an affine dependence on \(M\), i.e., \(P(X|M)\)\(=p_{CS}(\alpha,\beta_{X},\gamma_{X},aM+b)\) and \(P(Y|M)\)\(=p_{CS}(\alpha,\beta_{Y}\text{sgn}(ac),\gamma_{Y},cM+d)\), where \(a,b,c,d\in\mathbb{R}\).

**Theorem 1**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) is described by a univariate affine continuous stable system. Without the loss of generality, assume \(\nicefrac{{|a|}}{{\gamma_{X}}}\geq\nicefrac{{|c|}}{{\gamma_{Y}}}\). If \(1-\beta_{Y}\geq\nicefrac{{\left(\gamma_{X}|c|/\gamma_{Y}|a|\right)}^{\alpha}} {(1-\beta_{X})}\) and \(1+\beta_{Y}\geq\nicefrac{{\left(\gamma_{X}|c|/\gamma_{Y}|a|\right)}^{\alpha}} {(1+\beta_{X})}\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

Proof.: See Appx. F for the proof. Here, \(\nicefrac{{|a|}}{{\gamma_{X}}}\) and \(\nicefrac{{|c|}}{{\gamma_{Y}}}\) are the SNR analogues. 

### PID of multivariate affine continuous stable systems

We analyze two multivariate generalizations of Theorem 1 in Theorems 2 and 3. Namely, we consider independent component multivariate stable distribution (where all the components of the random vector are independent) and elliptically-contoured multivariate stable distribution (where the p.d.f. is elliptically contoured, similar to multivariate Gaussian distributions). We construct two systems employing these two sub-classes of multivariate continuous stable distributions and show that one of the UI terms is always zero for these systems. For both cases, denote the joint distribution of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\) as \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\), where the support set of \(M\) is \(\mathcal{M}\subseteq\mathbb{R}\). The dimensions of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\) are \(1\), \(d_{X}\), and \(d_{Y}\), respectively.

System 1: The random vectors \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) satisfy the following equations:

\[\vec{\mathbf{X}}=\vec{\mathbf{H}}_{X}M+\mathbf{A}_{X}\vec{\mathbf{Z}}_{X}+ \vec{\mathbf{b}}_{X}\text{ and }\vec{\mathbf{Y}}=\vec{\mathbf{H}}_{Y}M+\mathbf{A}_{Y}\vec{\mathbf{Z}}_{Y}+ \vec{\mathbf{b}_{Y}},\] (4)

where \(\vec{\mathbf{Z}}_{X}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{X}},\vec{ \mathbf{1}}_{d_{X}},\vec{\mathbf{0}}_{d_{X}})\), \(\vec{\mathbf{Z}}_{Y}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{Y}},\vec{ \mathbf{1}}_{d_{Y}},\vec{\mathbf{0}}_{d_{Y}})\), \(\mathbf{A}_{X}\) and \(\mathbf{A}_{Y}\) are invertible matrices, \(\vec{\mathbf{H}}_{X},\vec{\mathbf{b}}_{X}\in\mathbb{R}^{d_{X}}\), and \(\vec{\mathbf{H}}_{Y},\vec{\mathbf{b}}_{Y}\in\mathbb{R}^{d_{Y}}\).

**Theorem 2**.: _Let System 1 describe the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\). Without the loss of generality, assume \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\leq\|\mathbf{A}_{X}^{-1 }\vec{\mathbf{H}}_{X}\|_{\kappa}\), where \(\kappa=\nicefrac{{\alpha}}{{\alpha-1}}\ \forall\ \alpha\in(1,2]\) and \(\kappa=\infty\ \forall\ \alpha\in(0,1]\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) and \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\)._

Proof.: See Appx. G for the proof. Here, \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\) and \(\|\mathbf{A}_{X}^{-1}\vec{\mathbf{H}}_{X}\|_{\kappa}\) are the SNR analogues. 

System 2: The conditional distribution of \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) conditioned on \(M\) are as follows: \(P(\vec{\mathbf{X}}|M)=p_{CS-EC}(\alpha,\mathbf{\Sigma}_{X},\vec{\mathbf{H}}_{X} M+\vec{\mathbf{b}}_{X})\) and \(P(\vec{\mathbf{Y}}|M)=p_{CS-EC}(\alpha,\mathbf{\Sigma}_{Y},\vec{\mathbf{H}}_{Y} M+\vec{\mathbf{b}}_{Y})\), where \(\mathbf{\Sigma}_{X}\) and \(\mathbf{\Sigma}_{Y}\) are positive definite matrices, \(\vec{\mathbf{H}}_{X},\vec{\mathbf{b}}_{X}\in\mathbb{R}^{d_{X}}\), and \(\vec{\mathbf{H}}_{Y},\vec{\mathbf{b}}_{Y}\in\mathbb{R}^{d_{Y}}\).

**Theorem 3**.: _Let System 2 describe the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\). Define \(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}}\) and \(\mathbf{\Sigma}_{Y}^{-\nicefrac{{1}}{{2}}}\) as the respective inverses of the matrices \(\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\) and \(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}}\) which satisfy: \((\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X}^{\nicefrac{ {1}}{{2}}}=\mathbf{\Sigma}_{X}\), and \((\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{Y}^{\nicefrac{ {1}}{{2}}}=\mathbf{\Sigma}_{Y}\). Without the loss of generality, assume \(\|\mathbf{\Sigma}_{Y}^{-\nicefrac{{1}}{{2}}}\vec{\mathbf{H}}_{Y}\|_{2}\leq\| \mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}}\vec{\mathbf{H}}_{X}\|_{2}\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) and \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\)._

Proof.: See Appx. H for the proof. Here, \(\|\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}}\vec{\mathbf{H}}_{Y}\|_{2}\) and \(\|\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\vec{\mathbf{H}}_{X}\|_{2}\) are the SNR analogues. 

### PID of univariate affine discrete stable system

The univariate affine discrete stable system is the discrete counterpart of the univariate affine continuous stable system described in Sec. 4.1. The formal description of the univariate affine discrete stable system is as follows. Let the joint distribution of \(M,X,\) and \(Y\), denoted as \(P(M,X,Y)\), satisfy the following properties: (i) \(M\sim P(M)\) with support set \(\mathcal{M}\subseteq(0,\infty)\); (ii) \(P(X|M)\) and \(P(Y|M)\) are univariate discrete stable distributions with an affine dependence on \(M\), i.e., \(P(X|M\!=\!m)\!=\!P_{DS}(\nu,am+b)\) and \(P(Y|M\!=\!m)\!=\!P_{DS}(\nu,cm+d)\), where \(a,b,c,d\in(0,\infty)\).

**Theorem 4**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) is described by a univariate affine discrete stable system. Without the loss of generality, assume \(a\geq c\). If \(a^{\nicefrac{{a}}{{b}}}\geq\nicefrac{{c}}{{d}}\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

Proof.: See Appx. I for the proof. Here, \(a\) and \(c\) are the SNR analogues. 

### PID of multivariate linear Poisson system

The Poisson distribution is the only the discrete stable distribution with a well-known multivariate extension. Hence, we analyze vector-generalizations of Theorem 4 only for the Poisson distribution. We now describe the multivariate linear Poisson system: Let the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M,\vec{\mathbf{X}},\) and \(\vec{\mathbf{Y}}\) satisfy the following properties: (i) \(M\sim P(M)\) with support set \(\mathcal{M}\subseteq(0,\infty)\); (ii) \(P(\vec{\mathbf{X}}|M)\!=\!\text{Poisson}(d_{X},d_{X}^{\prime},\vec{\mathbf{A}}_ {X})\) and \(P(\vec{\mathbf{Y}}|M)\!=\!\text{Poisson}(d_{Y},d_{Y}^{\prime},\vec{\mathbf{A}}_ {Y})\), with:

\[\vec{\mathbf{A}}_{X}\!=\!\left[\lambda_{1}^{X}\ \ldots\ \lambda_{d_{X}-(d_{X}^{ \prime}-1)\ldots d_{X}}^{X}\right]^{T},\lambda_{i_{1}\ldots i_{j}}^{X}=\gamma_ {i_{1}\ldots i_{j}}^{X}M^{j}\ \forall\ j\in[d_{X}]\ \text{and}\ (i_{1},\ldots,i_{j})\in \mathbb{A}_{j}^{d_{X}},\] \[\vec{\mathbf{A}}_{Y}=\left[\lambda_{1}^{Y}\ \ldots\ \lambda_{d_{Y}-(d_{Y}^{ \prime}-1)\ldots d_{Y}}^{Y}\right]^{T},\lambda_{i_{1}\ldots i_{j}}^{Y}=\gamma_ {i_{1}\ldots i_{j}}^{Y}M^{j}\ \forall\ j\in[d_{Y}]\ \text{and}\ (i_{1},\ldots,i_{j})\in \mathbb{A}_{j}^{d_{Y}}.\] (5)

**Theorem 5**.: _Let the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\) be described by the multivariate linear Poisson system defined above. Without the loss of generality, assume \(d_{X}^{\prime}\geq d_{Y}^{\prime}\). If \(\sum_{(i_{1},\ldots,i_{j})\in A_{j}^{d_{X}}}\gamma_{i_{1}\ldots i_{j}}^{X}\geq \sum_{(i_{1},\ldots,i_{j})\in A_{j}^{d_{Y}}}\gamma_{i_{1}\ldots i_{j}}^{Y}\ \forall\ j\in[d_{Y}^{\prime}]\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) and \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\)._

Proof.: See Appx. J for the proof. Here, \(\gamma_{1}^{X},\ldots,\gamma_{d_{X}-(d_{X}^{\prime}-1)\ldots d_{X}}^{X},\gamma _{1}^{Y},\ldots,\gamma_{d_{Y}-(d_{Y}^{\prime}-1)\ldots d_{Y}}^{Y}\in\mathbb{R}^{+}\) are the SNR analogues. 

## 5 Computing PID using data thinning and data fission strategies

Data thinning and data fission are emerging fields in machine learning and statistics dedicated to studying the procedures of splitting a random variable \(X\) into \(N\) different component random variables. These splitting procedures provide an attractive alternative to the standard splitting of datasets into training, validation, and test splits for performing cross-validation to select statistical model parameters, as they enable users to perform cross-validation even for the extreme case of a dataset containing a _single_ datapoint. We refer the reader to the recent works of Neufeld et al. [25] and Leiner et al. [26] for a more comprehensive discussion on data thinning and fission, respectively.

The fields of data thinning and data fission share an important theoretical link with analytically calculating PID terms. Specifically, the tools developed for splitting random variables for data thinning and data fission can be readily used to calculate PID terms analytically. To give an intuition, suppose \(X\) contains more information about \(M\) than \(Y\). Then, we can employ data fission and thinning strategies to decompose \(X\) into two components, \(f_{1}(X)\) and \(f_{2}(X)\), such that \(f_{2}(X)\) follows the same distribution as \(Y\). Then, \(f_{2}(X)\) and \(Y\) convey the same information about \(M\) as theyare identically distributed and represent the redundant component of the PID terms. Similarly, \(f_{1}(X)\) contains the information uniquely contained in \(X\) about \(M\). Thus, \(f_{1}(X)\) represents the unique information term. Theorems 6 and 7 utilize the data thinning and data fission proposed in [25; 26] to construct a Markov chain \(Q_{MC}(M,X,Y)\in\Delta_{P}\) for several systems of random variables \(M\), \(X\), and \(Y\). This allows us to use the sufficient condition discussed in Sec. 3 to compute their PID.

### PID for convolution-closed distribution based on data thinning strategies of Neufeld et al.

Neufeld et al. [25] introduces data thinning for a large family of distributions known as convolution-closed distributions (see Sec. 2). An attractive property of convolution-closed distributions is that they provide a natural way to define a dilation/thinning operation. Formally, let \(X\sim f(\delta)\), then we define \(X_{\epsilon}\) as the \(\epsilon\)-dilated version of \(X\) if \(X_{\epsilon}\sim f(\epsilon\delta)\) for some \(\epsilon\in(0,1)\) such that \(\epsilon\delta\in\mathcal{D}\). Furthermore, if we assume \((1-\epsilon)\delta\in\mathcal{D}\), then \(P(X_{\epsilon}|X)\!=\!P(X_{\epsilon}|X_{\epsilon}+X_{1-\epsilon})\), where \(X_{1-\epsilon}\sim f((1-\epsilon)\delta)\) and \(X_{\epsilon}\perp\!\!\!\perp X_{1-\epsilon}\) (see lemma 19 for a formal justification). We denote \(P(X_{\epsilon}|X=x)=P(X_{\epsilon}|X_{\epsilon}+X_{1-\epsilon}=x)\) as \(G(\epsilon\delta,(1-\epsilon)\delta,x)\). This dilation operation forms the basis of data thinning, as it enables \(X\) to be split into its dilated components \(X_{\epsilon}\) and \(X_{1-\epsilon}\), such that \(X=X_{\epsilon}+X_{1-\epsilon}\), where \(X_{\epsilon}\perp\!\!\!\perp X_{1-\epsilon}\). We utilize this dilation operation for analytically calculating the PID terms of the following linear convolution-closed system:

**Linear convolution-closed system**: Let \(\mathcal{F}_{\mathcal{D}}\) be a convolution-closed distribution family as described in Sec. 2. The joint distribution \(P(M,X,Y)\) of the random variables \(M\), \(X\), and \(Y\) describes a _linear convolution-closed system_ if the distributions \(P(X|M)\) and \(P(Y|M)\) are defined as follows:

\[P(X|M\!=\!m)=f(\delta_{m}^{X})\text{ and }P(Y|M\!=\!m)=f(\delta_{m}^{Y}) \text{ such that }\delta_{m}^{X},\delta_{m}^{Y}\in\mathcal{D}\ \forall\ m\in\mathcal{M},\] (6)

where \(\mathcal{M}\) is the support of \(M\). Furthermore, we assume \(\delta_{m}^{X}=\gamma\delta_{m}^{Y}\ \forall\ m\in\mathcal{M}\) for some \(\gamma\in\mathbb{R}^{+}\).

**Theorem 6**.: _Let the joint density \(P(M,X,Y)\) of random variables \(M\), \(X\), and \(Y\) be described by a linear convolution-closed system. Without the loss of generality, assume \(\gamma\leq 1\). If_

1. \((1-\gamma)\delta_{m}^{Y}\in\mathcal{D}\ \forall\ m\in\mathcal{M}\)_,_
2. \(P(X_{\gamma}|X_{\gamma}+X_{1-\gamma}\!=\!x,M\!=\!m)=G(\gamma\delta_{m}^{X},(1- \gamma)\delta_{m}^{X},x)\) _does not depend on_ \(m\)_, where_ \(P(X_{\gamma}|M)=f(\gamma\delta_{m}^{X})\)_,_ \(P(X_{1-\gamma}|M)=f((1-\gamma)\delta_{m}^{X})\) _and_ \(X_{\gamma}\perp\!\!\!\perp X_{1-\gamma}|M\)_,_

_then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

The proof of Theorem 6 is provided in Appx. K. Theorem 6 enables PID calculation for several well-known distributions such as gamma, Poisson, beta etc. (see Appx. C for a (non-exhaustive) list).

### PID for distributions based on data fission strategies of Leiner et al.

Leiner et al. [26] propose a "conjugate-prior reversal" strategy for splitting a random variable \(X\) into two components, \(f(X)\) and \(g(X)\), for certain exponential family distributions. The distributions proposed for performing conjugate-prior reversal provide natural descriptions of \(P(M,X,Y)\) (specified in theorem 7) for which PID can be calculated analytically. We briefly describe the distributions used in the conjugate-prior reversal strategy. Let \(X\sim p_{exp1}(X)\), where

\[p_{exp1}(X=x;\theta_{1},\theta_{2})=H(\theta_{1},\theta_{2})\exp(\theta_{1}^{T}x -\theta_{2}^{T}A(x)),\] (7)

for some appropriately defined \(H(\cdot,\cdot),A(\cdot),\theta_{1}\) and \(\theta_{2}\). Furthermore, define a random variable \(Y\) through its conditional density \(p(Y|X=x)\):

\[p(Y=y|X=x;\theta_{3})=h(y)\exp\left(x^{T}T(y)-\theta_{3}^{T}A(x)\right),\] (8)

for some \(h(\cdot)\), \(T(\cdot)\), and \(\theta_{3}\), such that \(p(Y=y|X=x;\theta_{1},\theta_{2})\) is a well-defined distribution. Then, the decomposition terms \(f(X)\) and \(g(X)\) are \(Y\) and \(X\), respectively, in the conjugate-prior reversal strategy. Furthermore, the marginal distribution of \(Y\) is expressed as:

\[p_{exp2}(Y=y;\theta_{1},\theta_{2},\theta_{3})=h(y)H(\theta_{1},\theta_{2})/H( \theta_{1}+T(y),\theta_{2}+\theta_{3})\text{ (see proof of Theorem 1 in \@@cite[cite]{[\@@bibref{}{Liang}{}{}]}).}\]

**Theorem 7**.: _Let \(M,X,\) and \(Y\) be random variables having the joint distribution \(P(M,X,Y)\). Furthermore, the conditional distribution of \(X\) and \(Y\) conditioned on \(M\) are as follows: \(P(X|M\!=\!m)\!=\!p_{exp1}(X;\theta_{1}(m),\theta_{2}(m))\) and \(P(Y|M\!=\!m)\!=\!p_{exp2}(Y;\theta_{1}(m),\theta_{2}(m),\theta_{3})\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._The proof of Theorem 7 can be found in Appx. L. The proof essentially stems from observing that the joint distribution \(Q_{MC}(M,X,Y)\!=\!P(M)P(X|M)Q_{MC}(Y|X)\) lies in \(\Delta_{P}\), where \(Q_{MC}(Y|X)\!=\!h(y)\exp\big{(}x^{T}T(y)-\theta_{3}^{T}A(x)\big{)}\). A (non-exhaustive) list of well-known distributions for which theorem 7 is applicable is provided in Appx. D. Leiner et al. [26] also discuss some more strategies for performing data fission that do not follow the conjugate-prior reversal strategy. We provide the corresponding results for computing PID for these remaining data fission strategies and for additional miscellaneous distributions in Appx. E.

## 6 Upper bound for convolution-closed distributions

Several numerical methods, such as [3, 9, 20], approximately solve (2) by considering a smaller constraint set or minimizing an appropriate upper bound for calculating the PID terms. These numerical methods employ general approximations that rely on weak assumptions on the underlying distributions for solving (2), as their goal is to estimate PID for a large class of distributions. However, for specific applications, it is possible to make stronger assumptions on the underlying distributions (e.g., assuming the Poisson distribution for modeling neural spikes). This section illustrates how our theoretical analysis can benefit these numerical algorithms by providing more refined approximations for solving (2) that harness these stronger assumptions. Specifically, we construct an upper bound for the objective of (2) for convolution-closed distributions and show that, under certain assumptions, the upper bound can be _analytically minimized_ over \(\Delta_{P}\) in Sec. 6.1. Note that these upper bounds are applicable for more general cases than our theoretical results, as they do not require the sources \(X\) and \(Y\) to have an affine dependence on \(M\). Consequently, our upper bound is also applicable in cases where both of the UI terms in the PID are non-zero, unlike our results in Sec. 4 and 5 We demonstrate the tightness of our upper bound through a simulation study in Sec. 6.2.

**Notations and assumptions**: We consider the minimization problem \(\min_{Q\in\Delta_{P}}I_{Q}(M;[X,Y])\) for deriving the upper bound, as it was shown to be equivalent to (2) in [16]. The distribution \(P(M,X,Y)\) for which we will construct our upper bound is specified as follows. The random variable \(M\) has support over \(\mathcal{M}\), the conditional distributions \(P(X|M)\) and \(P(Y|M)\) are members of some convolution-closed distribution family \(\mathcal{F}_{\mathcal{D}}\), and there exists some \(\delta_{bias}^{X},\delta_{bias}^{Y}\in\mathcal{D}\) such that

\[P(X|M\!=\!m)\!=\!f(\delta_{m}^{X})\text{ and }P(Y|M\!=\!m)\!=\!f (\delta_{m}^{Y})\text{ where }\delta_{m}^{X},\delta_{m}^{Y}\in\mathcal{D}\;\forall\;m\in \mathcal{M},\text{ with }\] \[(\delta_{m}^{Y}-\delta_{bias}^{Y}),(\delta_{m}^{X}-\delta_{bias}^ {X}),(\delta_{m}^{X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^{Y}))\in \mathcal{D}\text{ and }\delta_{m}^{X}-\delta_{bias}^{X}\!=\! \epsilon_{m}^{(1)}(\delta_{m}^{Y}\!-\!\delta_{bias}^{Y}),\] \[\delta_{m}^{Y}=\epsilon_{m}^{(2)}(\delta_{m}^{Y}-\delta_{bias}^ {Y}),\text{ and }\delta_{m}^{X}=\epsilon_{m}^{(3)}(\delta_{m}^{X}-\delta_{bias}^{X})\text{ for }\epsilon_{m}^{(1)},\epsilon_{m}^{(2)},\epsilon_{m}^{(3)}\in[0,1]\;\forall\;m\in \mathcal{M}.\] (9)

The above assumptions ensure that \(X\) and \(Y\) can always be decomposed into new random variables \(X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime}\), and \(n_{Y}\) (see (10)). We utilize these decomposed random variables to construct our upper bound. Hence, our upper bound is only applicable for systems satisfying (9). The above assumptions describe a large class of systems. For example, in the Poisson case, our upper bound is applicable for any \(P(M,X,Y)\) having \(P(X|M)=\text{Poisson}(f_{1}(M))\) and \(P(Y|M)=\text{Poisson}(f_{2}(M))\), as long as \(f_{1}(M)\geq f_{2}(M)\) over \(\mathcal{M}\). In Appx. C, we provide numerous examples of systems for which (9) holds to illustrate the assumptions of (9).

Figure 1: **a** and **b**, respectively, show the box plot of the difference \(I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)\) and the corresponding values of \(I_{Q_{N}}(M,X,Y)\) for the \(20\) different function pairs across the \(75\) different \(P(M)\) distributions. The light-blue dots show the corresponding data points used for making the box plots. **c** shows the ratio of the median difference \(I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)\) and the median value of \(I_{Q_{N}}(M,X,Y)\) in percentage, for each function pair.

### Upper bound construction

First, we consider an arbitrary distribution \(Q(M,X,Y)\in\Delta_{P}\). Therefore, we know \(Q(X|M\!=\!m)\!=\!P(X|M\!=\!m)\!=\!f(\delta_{m}^{X})\) and \(Q(Y|M\!=\!m)\!=\!P(Y|M\!=\!m)\!=\!f(\delta_{m}^{Y})\). We use the dilation properties of convolution-closed distributions (Sec. 5.1) to decompose \(X\) and \(Y\) into their respective dilated versions: \((X^{\prime},Y^{\prime\prime},n_{X})\) and \((Y^{\prime},n_{Y})\). From the results of Appx. M.1, we know that \(X=X^{\prime}+Y^{\prime\prime}+n_{X}\) and \(Y=Y^{\prime}+n_{Y}\). Furthermore, \((X^{\prime},Y^{\prime\prime},n_{X})\) are mutually conditionally independent given \(M\) and \(Y^{\prime}\perp n_{Y}|M\). Hence, we can construct the following Markov chain for any arbitrary \(Q(M,X,Y)\in\Delta_{P}\):

\[M\rightarrow[X^{\prime}\quad Y^{\prime\prime}\quad Y^{\prime}\quad n_{X}\quad n _{Y}\quad]^{T}\rightarrow[X^{\prime}+Y^{\prime\prime}+n_{X}\quad Y^{\prime}+n _{Y}]^{T}=[X\quad Y]^{T}\,.\] (10)

We denote the joint distribution of \((M,X^{\prime},Y^{\prime\prime},Y^{\prime},n_{X},n_{Y})\) as \(\bar{Q}(M,X^{\prime},Y^{\prime\prime},Y^{\prime},n_{X},n_{Y})\). We appropriately choose the dilation amounts for \(X\) and \(Y\) such that the respective conditional distributions of \((X^{\prime},Y^{\prime\prime},Y^{\prime},n_{X},n_{Y})\) are as follows:

\[\bar{Q}(X^{\prime}|M\!=\!m)\!=\!f(\delta_{m}^{X}\!-\!\delta_{m}^{Y }-(\delta_{bias}^{X}\!-\!\delta_{bias}^{Y})),\ \ \bar{Q}(Y^{\prime\prime}|M\!=\!m)=f(\delta_{m}^{Y}-\delta_{bias}^{Y}),\] \[\bar{Q}(Y^{\prime}|M\!=\!m)\!=\!f(\delta_{m}^{Y}-\delta_{bias}^{ Y}),\ \ \bar{Q}(n_{X}|M)=f(\delta_{bias}^{X}),\ \text{and}\ \bar{Q}(n_{Y}|M)=f(\delta_{bias}^{Y}).\] (11)

The distributions in (11) are well-defined due to (9). Appx. M.1 formally shows that a \(\bar{Q}\), defined as above, exists for each \(Q\in\Delta_{P}\). We use data-processing inequality and (10) to conclude:

\[I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y} ])\geq I_{Q}(M;[X,Y])\ \forall\ Q(M,X,Y)\in\Delta_{P}.\] (12)

Hence, (12) provides us the desired upper bound for our objective \(I_{Q}(M;[X,Y])\). Furthermore, the minimization problem \(\min_{Q\in\Delta_{P}}I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{ \prime},n_{Y}])\) is analytically solvable, and the minimizing distribution \(\bar{Q}^{*}\) has the structure:

\[Y^{\prime}=Y^{\prime\prime}\ \text{and}\ (n_{X},n_{Y})\perp\!\!\perp(M,X^{ \prime},Y^{\prime}).\]

The corresponding distribution of \((M,X,Y)\), denoted as \(Q_{A}(M,X,Y)\), can be found by appropriately manipulating \(\bar{Q}^{*}\), as \(X=X^{\prime}+Y^{\prime\prime}+n_{X}\) and \(Y=Y^{\prime}+n_{Y}\). The distribution \(Q_{A}\) serves as an approximate solution for the problem \(\min_{Q\in\Delta_{P}}I_{Q}(M;[X,Y])\) and, consequently, (2). Note that if multiple \(\delta_{bias}^{X}\) and \(\delta_{bias}^{Y}\) exist satisfying (9), we optimize over the pairs \((\delta_{bias}^{X},\delta_{bias}^{Y})\) to further refine our approximate solution of (2).

### Simulation study for numerically validating the upper bound

We illustrate the tightness of our upper bound through a simulation study on the Poisson distribution (a convolution-closed distribution). The reason for choosing the Poisson distribution is two-fold: (i) Poisson distribution is easily approximated as a discrete distribution over finite support, enabling calculation of ground-truth PID terms through numerical solvers such as [10; 17; 18], which are not readily available for continuous distributions; (ii) Many practical applications of PID have been in neuroscience, and the Poisson distribution is frequently used for modeling neural spikes in neuroscientific studies [49].

We compare the performance of our analytical estimate \(Q_{A}(M,X,Y)\) with the numerical ground-truth estimate \(Q_{N}(M,X,Y)\) for the Poisson distribution. The simulation setup is as follows: We choose a \(P(M,X,Y)\) such that \(P(X|M)=\text{Poisson}(f_{1}(M))\) and \(P(Y|M)=\text{Poisson}(f_{2}(M))\). We chose \(20\) different pairs of non-linear \(f_{1}(\cdot)\) and \(f_{2}(\cdot)\) (enumerated in Table 1 in Appx. M), such that the assumptions in (9) are satisfied. The distribution \(P(M)\) is chosen to be a discrete distribution. For each function pair, we compare \(Q_{A}(M,X,Y)\) and \(Q_{N}(M,X,Y)\) over \(75\) different distributions of \(P(M)\) as shown in Fig. 1a. The \(75\) distributions are further sub-divided into three groups of \(25\) based on the number of the outcomes of \(M\), which are \(2\), \(4\), and \(8\). For each of the \(75\) distributions, the values of \(M\) are randomly sampled from \([1,4]\) and the values of \(P(M)\) are randomly sampled from a simplex of appropriate dimensions. We compare the mutual information \(I_{Q_{A}}(M;[X,Y])\) calculated using \(Q_{A}\) with the numerical ground-truth \(I_{Q_{N}}(M;[X,Y])\) calculated using numerical solvers (Fig. 1b). For calculating \(I_{Q_{N}}(M;[X,Y])\), we approximate the Poisson distribution as a finite discrete distribution by terminating its support at the smallest integer where its cumulative distribution is greater than \(0.99\). We use the code provided in [50] (under Apache 2.0-license) to numerically solve (2) using this discrete approximation of the Poisson distribution. Fig. 1c demonstrates that the analytical upper bound provided by \(Q_{A}\) is very tight for the tested function pairs (within \(<1\%\) ofthe numerical ground-truth for \(16\) function pairs) and serves as a good approximation for solving (2). Furthermore, the tightness of our analytical upper bound suggests that \(Q_{A}\) might be an analytical solution of (2) for a larger class of systems of \(M\), \(X\), and \(\dot{Y}\) having non-affine dependence on \(M\). Similar results demonstrating the tightness of upper-bound for negative-binomial and binomial distributions are presented in Appx. M.4.

## 7 Discussion and Limitations

In this work, we analytically compute PID for large classes of distributions, greatly expanding upon the analytical result for the Gaussian system. We provide the first known analytical PID for systems employing Poisson, gamma, exponential, Cauchy, beta, Dirichlet, Levy-stable, binomial, multinomial, negative binomial, and uniform distributions. Furthermore, we generalize the previous Gaussian PID result [21] in an additional way by showing the target \(M\) need not be Gaussian. Our stable distribution results provide the first known analytical computation of PID for fat-tailed distributions (all continuous stable distributions have infinite variance except the Gaussian distribution). A practical utility of our analytical results is that they provide a large test bed in which the performance of numerical PID estimators can be compared and evaluated. This test bed may benefit future works on numerical PID estimation by enabling more comprehensive testing of PID estimators.

Our results on the Poisson, Cauchy, and binomial are of particular relevance in the neuroscientific context. Poisson and Cauchy distributions are widely used to model neural spikes [49] and network dynamics in the brain [51]. Binomial thinning is a frequently-used operator in neuroscience [49]. Our generalization of the Gaussian result could be helpful in refining approximations already used in computing PID for neural data [3], e.g., by relaxing the assumption of joint Gaussianity. As continuous stable distributions have been shown to better model Magnetic Resonance Imaging data [52; 53], our results may also be helpful in computing PID in this application.

We also connect the fields of data thinning and data fission with PID by using their decomposition strategies for analytically computing the PID of systems based on convolution-closed distributions (see Sec. 5). Conversely, our PID results also suggest decomposition strategies for data thinning/fission purposes, e.g., our stable distribution results suggest that stable random variables can be decomposed by adding independent noise (similar to the Gaussian case discussed in [26]). Convolution-closed distributions are particularly promising for studying PID as they allow intuitive construction of upper bounds that can be analytically minimized (see Sec. 6). These upper bounds can complement the work on numerical estimation of PID by providing more refined approximations. Another promising avenue is to combine our upper bounds with lower bounds (e.g., from [54]) to create branch and bound algorithms [55] for solving (2). Overall, our analytical results greatly facilitate the computation of PID, either by directly using the analytical expressions or by providing refined approximations for numerical methods.

**Limitations and Future Work**: We study PID for univariate \(M\), \(X\), and \(Y\) and provide some vector extensions. More vector extensions of our results are a promising direction for subsequent works. Most of our analytical results require \(P(X|M)\) and \(P(Y|M)\) to depend on some affine functions of \(M\). The existence of analytical solutions for the cases where \(P(X|M)\) and \(P(Y|M)\) depend on non-affine functions of \(M\) remains an open question. The upper bound discussed in Sec. 6 can be further refined with more careful analysis, and more rigorous testing of these upper bounds is required to understand in which cases the upper bound is a good approximation for solving (2). We defer the testing and refinement of these upper bounds for subsequent works, as our primary goal in this work is to study analytical solutions of PID. Niu & Quinn [56] also propose a duality result between the synergistic and redundant components in the Gaussian broadcast and multiple-access channels utilizing the analytical PID expressions of the Gaussian system. It may be possible to derive similar duality results for appropriately defined broadcast and multiple-access channels employing other distributions (e.g., Poisson or Cauchy) with our theoretical results.

**Broader Impact**: Due to their theoretical nature, our results' negative impact primarily depends on how they are used and interpreted. Our theoretical results are applicable only under the specific assumptions outlined in this work, and using these results without ensuring that the theorem's assumptions are satisfied can lead to incorrect scientific conclusions. We also caution reader against naively using PID to draw causal inferences, as PID is inherently a correlational quantity.

## Acknowledgments and Disclosure of Funding

Authors would like to thank Prof. Pulkit Grover, Dr. Praveen Venkatesh, Prof. Alireza Chamanzar, and Neil Ashim Mehta for helpful discussions. This work was supported by National Science Foundation (NSF). Authors declare no competing interests.

## References

* [1] Praveen Venkatesh and Pulkit Grover. Understanding encoding and redundancy in grid cells using partial information decomposition. _Comput. Syst. Neurosci.(Cosyne)_, 2020.
* [2] Ariel K Feldman, Praveen Venkatesh, Douglas J Weber, and Pulkit Grover. Information-theoretic tools to understand distributed source coding in neuroscience. _IEEE Journal on Selected Areas in Information Theory_, 2024.
* [3] Praveen Venkatesh, Corbett Bennett, Sam Gale, Tamina Ramirez, Greggory Heller, Severine Durand, Shawn Olsen, and Stefan Mihalas. Gaussian partial information decomposition: Bias correction and application to high-dimensional data. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 74602-74635. Curran Associates, Inc., 2023.
* [4] Nigel Colenbier, Frederik Van de Steen, Lucina Q Uddin, Russell A Poldrack, Vince D Calhoun, and Daniele Marinazzo. Disambiguating the role of blood flow and global signal with partial information decomposition. _NeuroImage_, 213:116699, 2020.
* [5] Tjoerd W Boonstra, Luca Faes, Jennifer N Kerkman, and Daniele Marinazzo. Information decomposition of multichannel EMG to map functional interactions in the distributed motor system. _NeuroImage_, 202:116093, 2019.
* [6] Jana Krohova, Luca Faes, Barbora Czippelova, Zuzana Turianikova, Nikoleta Mazgutova, Riccardo Pernice, Alessandro Busacca, Daniele Marinazzo, Sebastiano Stramaglia, and Michal Javorka. Multiscale information decomposition dissects control mechanisms of heart rate variability at rest and during physiological stress. _Entropy_, 21(5):526, 2019.
* [7] Giuseppe Pica, Eugenio Piasini, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, and Stefano Panzeri. Quantifying how much sensory information in a neural code is relevant for behavior. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [8] Praveen Venkatesh, Sanghamitra Dutta, and Pulkit Grover. Information flow in computational systems. _IEEE Transactions on Information Theory_, 66(9):5456-5491, 2020.
* [9] Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Russ R Salakhutdinov, and Louis-Philippe Morency. Quantifying & modeling multimodal interactions: An information decomposition framework. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 27351-27393. Curran Associates, Inc., 2023.
* [10] Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, and Louis-Philippe Morency. Multimodal fusion interactions: A study of human and automatic quantification. In _Proceedings of the 25th International Conference on Multimodal Interaction_, pages 425-435, 2023.
* [11] Sanghamitra Dutta, Praveen Venkatesh, Piotr Mardziel, Anupam Datta, and Pulkit Grover. An information-theoretic quantification of discrimination with exempt features. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3825-3833, 2020.
* [12] Sanghamitra Dutta and Faisal Hamman. A review of partial information decomposition in algorithmic fairness and explainability. _Entropy_, 25(5):795, 2023.

* [13] Faisal Hamman and Sanghamitra Dutta. Demystifying local & global fairness trade-offs in federated learning using partial information decomposition. In _The Twelfth International Conference on Learning Representations_, 2024.
* [14] Faisal Hamman and Sanghamitra Dutta. A unified view of group fairness tradeoffs using partial information decomposition. In _2024 IEEE International Symposium on Information Theory (ISIT)_, pages 214-219, 2024.
* [15] Tomas Scagliarini, Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia, and Rosario N Mantegna. Synergistic information transfer in the global system of financial markets. _Entropy_, 22(9):1000, 2020.
* [16] Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, Jurgen Jost, and Nihat Ay. Quantifying unique information. _Entropy_, 16(4):2161-2183, 2014.
* [17] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. Bivariate partial information decomposition: The optimization perspective. _Entropy_, 19(10):530, 2017.
* [18] Pradeep Kr Banerjee, Johannes Rauh, and Guido Montifar. Computing the unique information. In _2018 IEEE International Symposium on Information Theory (ISIT)_, pages 141-145. IEEE, 2018.
* [19] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. BROJA-2PID: A robust estimator for bivariate partial information decomposition. _Entropy_, 20(4):271, 2018.
* [20] Ari Pakman, Amin Nejatbakhsh, Dar Gilboa, Abdullah Makkeh, Luca Mazzucato, Michael Wibral, and Elad Schneidman. Estimating the unique information of continuous variables. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 20295-20307. Curran Associates, Inc., 2021.
* [21] Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems. _Physical Review E_, 91(5):052802, 2015.
* [22] Praveen Venkatesh and Gabriel Schamberg. Partial information decomposition via deficiency for multivariate Gaussians. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 2892-2897. IEEE, 2022.
* [23] John P Nolan. Univariate stable distributions. _Springer Series in Operations Research and Financial Engineering_, 10:978-3, 2020.
* [24] Fred W Steutel and Klaas van Harn. Discrete analogues of self-decomposability and stability. _The Annals of Probability_, pages 893-899, 1979.
* [25] Anna Neufeld, Ameer Dharamshi, Lucy L. Gao, and Daniela Witten. Data thinning for convolution-closed distributions. _Journal of Machine Learning Research_, 25(57):1-35, 2024.
* [26] James Leiner, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. Data fission: Splitting a single data point. _Journal of the American Statistical Association_, 0(0):1-12, 2023.
* [27] Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. _arXiv preprint arXiv:1004.2515_, 2010.
* [28] Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)_. Wiley-Interscience, USA, 2006.
* [29] Malte Harder, Christoph Salge, and Daniel Polani. Bivariate measure of redundant information. _Physical Review E_, 87(1):012130, 2013.
* [30] Pradeep Kr Banerjee, Eckehard Olbrich, Jurgen Jost, and Johannes Rauh. Unique informations and deficiencies. In _2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 32-38. IEEE, 2018.
* [31] Jim W Kay and Robin AA Ince. Exact partial information decompositions for Gaussian systems based on dependency constraints. _Entropy_, 20(4):240, 2018.

* [32] Conor Finn and Joseph T Lizier. Pointwise partial information decomposition using the specificity and ambiguity lattices. _Entropy_, 20(4):297, 2018.
* [33] Xueyan Niu and Christopher J Quinn. A measure of synergy, redundancy, and unique information using information geometry. In _2019 IEEE International Symposium on Information Theory (ISIT)_, pages 3127-3131. IEEE, 2019.
* [34] Fernando E Rosas, Pedro AM Mediano, Borzoo Rassouli, and Adam B Barrett. An operational information decomposition via synergistic disclosure. _Journal of Physics A: Mathematical and Theoretical_, 53(48):485001, 2020.
* [35] Keerthana Gurushankar, Praveen Venkatesh, and Pulkit Grover. Extracting unique information through Markov relations. In _2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1-6. IEEE, 2022.
* [36] David Blackwell. Equivalent comparisons of experiments. _The Annals of Mathematical Statistics_, pages 265-272, 1953.
* [37] Praveen Venkatesh, Keerthana Gurushankar, and Gabriel Schamberg. Capturing and interpreting unique information. In _2023 IEEE International Symposium on Information Theory (ISIT)_, pages 2631-2636. IEEE, 2023.
* [38] Virgil Griffith and Christof Koch. Quantifying synergistic mutual information. _Guided self-organization: inception_, pages 159-190, 2014.
* [39] Joseph T Lizier, Nils Bertschinger, Jurgen Jost, and Michael Wibral. Information decomposition of target effects from multi-source interactions: Perspectives on previous, current and future work. _Entropy_, 20(4):307, 2018.
* [40] John P Nolan. _Stable distributions_. 2012.
* [41] Jeremie Fish, Jie Sun, and Erik Bollt. Interaction networks from discrete event data by Poisson multivariate mutual information estimation and information flow with applications from gene expression data. _Applied Network Science_, 7(1):1-22, 2022.
* [42] AS Krishnamoorthy. Multivariate binomial and Poisson distributions. _Sankhya: The Indian Journal of Statistics_, pages 117-124, 1951.
* [43] DM Mahamunulu. A note on regression in the multivariate Poisson distribution. _Journal of the American Statistical Association_, 62(317):251-258, 1967.
* [44] Dimitris Karlis and Loukia Meligkotsidou. Multivariate Poisson regression with covariance structure. _Statistics and Computing_, 15(4):255-265, 2005.
* [45] Murad S Taqqu. _Stable non-Gaussian random processes: stochastic models with infinite variance_. Chapman & Hall, 1994.
* [46] S James Press. Multivariate stable distributions. _Journal of Multivariate Analysis_, 2(4):444-462, 1972.
* [47] Norman Lloyd Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. _Discrete multivariate distributions_, volume 165. Wiley New York, 1997.
* [48] Xiaohu Shang and H Vincent Poor. Noisy-interference sum-rate capacity for vector Gaussian interference channels. _IEEE Transactions on Information Theory_, 59(1):132-153, 2012.
* [49] Peter Dayan and Laurence F Abbott. _Theoretical neuroscience: computational and mathematical modeling of neural systems_. MIT press, 2005.
* [50] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. Broja-2pid: A robust estimator for bertschinger et al.'s bivariate partial information decomposition. _Entropy_, 20(4):271, 2018.
* [51] Viktoras Pyragas and Kestutis Pyragas. Mean-field equations for neural populations with q-Gaussian heterogeneities. _Physical Review E_, 105(4):044402, 2022.

* [52] Diego Salas-Gonzalez, JM Gorriz, Javier Ramirez, Matthias Schloegl, Elmar Wolfgang Lang, and Andres Ortiz. Parameterization of the distribution of white and grey matter in MRI using the \(\alpha\)-stable distribution. _Computers in biology and medicine_, 43(5):559-567, 2013.
* [53] Diego Salas-Gonzalez, Juan M Gorriz, Javier Ramirez, and Elmar W Lang. Why using the alpha-stable distribution in NeuroImage? In _2014 International Conference on Signal Processing and Multimedia Applications (SIGMAP)_, pages 297-301. IEEE, 2014.
* [54] Seiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. _arXiv preprint arXiv:2108.13753_, 2021.
* [55] Narendra and Fukunaga. A branch and bound algorithm for feature subset selection. _IEEE Transactions on Computers_, 100(9):917-922, 1977.
* [56] Xueyan Niu and Christopher J Quinn. Synergy and redundancy duality between Gaussian multiple access and broadcast channels. In _2020 International Symposium on Information Theory and Its Applications (ISITA)_, pages 6-10. IEEE, 2020.
* [57] Robert B Ash and Catherine A Doleans-Dade. _Probability and measure theory_. Academic press, 2000.
* [58] Gilbert Strang. _Introduction to linear algebra_. SIAM, 2022.
* [59] Alan V Oppenheim, Alan S Willsky, Syed Hamid Nawab, and Jian-Jiun Ding. _Signals and systems_, volume 2. Prentice hall Upper Saddle River, NJ, 1997.
* [60] Jagadesh Chandramohan and Lung-Kuang Liang. Bernoulli, multinomial and Markov chain thinning of some point processes and some results about the superposition of dependent renewal processes. _Journal of Applied Probability_, 22(4):828-835, 1985.
* [61] Sergio Verdu. Poisson communication theory. _International Technion Communication Day in Honor of Israel Bar-David_, 66, 1999.
* [62] Christian Walck et al. _Hand-book on statistical distributions for experimentalists_. Stockholms universitet, 1996.
* [63] Kalimuthu Krishnamoorthy. _Handbook of statistical distributions with applications_. Chapman and Hall/CRC, 2006.
* [64] Lenka Slamova and Lev B Klebanov. Generalized definitions of discrete stability. _arXiv preprint arXiv:1502.02588_, 2015.

## Appendix A Formal proof of the sufficient condition for solving (2) discussed in Sec. 3

**Proposition 1**.: _For the minimization problem defined in (2), if there exists a joint density \(Q_{MC}(M,X,Y)\in\Delta_{P}\), having the Markovian structure of the form \(M\to X\to Y\) or \(M\to Y\to X\), then \(Q_{MC}(M,X,Y)\) minimizes (2). Furthermore,_

1. _If_ \(Q_{MC}(M,X,Y)\) _has the Markovian structure_ \(M\to X\to Y\)_, then_ \(UI(M;Y\backslash X)=I_{Q_{MC}}(M;Y|X)=0\)_._
2. _If_ \(Q_{MC}(M,X,Y)\) _has the Markovian structure_ \(M\to Y\to X\)_, then_ \(UI(M;X\backslash Y)=I_{Q_{MC}}(M,X|Y)=0\)_._

Proof.: **Case 1**: \(Q_{MC}(M,X,Y)\in\Delta_{P}\) having the Markovian structure \(M\to X\to Y\).

By the definition of the Markov chain, we know that \(M\) and \(Y\) are conditionally independent given \(X\). Denote \(Q_{MC}(M,X,Y)\) as the joint distribution of the Markov chain \(M\to X\to Y\). Then,

\[I_{Q_{MC}}(M;Y|X)=0,\] (13)

where \(I_{Q_{MC}}(M;Y|X)\) is the conditional mutual information between \(M\) and \(Y\) given \(X\) under the distribution \(Q_{MC}(M,X,Y)\). Equation (13) follows from a well-known property of conditional mutual information which states that \(I(A;B|C)=0\) iff \(A\perp\!\!\!\perp B|C\)[28].

Since \(I_{Q}(M;Y|X)\geq 0\)\(\forall\)\(Q\in\Delta_{P}\), due to the non-negativity of conditional mutual information [28], we can conclude that \(\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)\geq 0\). The joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structures \(M\to X\to Y\) achieves the lower bound of \(0\) for the minimization problem \(\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)\) implying \(Q_{MC}(M,X,Y)\) minimizes (2).

**Case 2**: \(Q_{MC}(M,X,Y)\in\Delta_{P}\) having the Markovian structure \(M\to Y\to X\).

A similar argument as case 1, with the random variables \(X\) and \(Y\) switched, would allow us to conclude that the joint distribution \(Q_{MC}(M,X,Y)\) specifying the Markov chain \(M\to Y\to X\) minimizes (2). 

## Appendix B Other PID definitions for which theorems 1-7 are valid

Our theoretical results (Theorems 1-7) hold for a wide range of PID measures. Particularly, our results hold for two broad classes of PID definitions: PID definitions satisfying assumption \((*)\) in Bertschinger et al. [16] and Blackwellian PIDs (see Appendix B-C of Venkatesh & Schamberg [22] for a deeper discussion on differences between these two families of PID definition). Some examples of PID definitions satisfying assumption \((*)\) in Bertschinger et al. [16] are the Williams & Beer's PID definition [27], the PID definition proposed in Harder et al. [29], and I-PID proposed in Venkatesh et al. [37]. Similarly, an example of a Blackwellian PID is the \(\delta\)-PID discussed in Venkatesh & Schamberg [22]. Section III-F of Venkatesh et al. [37] further discusses a family of Blackwellian PID definitions for which our theorems are also applicable. We now provide formal arguments showing that our theoretical results are applicable for PID definitions that either satisfy assumption \((*)\) of Bertschinger et al. [16] or are Blackwellian

**PID definitions satisfying assumption \((*)\)**: We invoke the argument presented in the proof discussed in Section 4.2 of Barrett [21] to show that our results hold for any PID definition satisfying assumption \((*)\) in Bertschinger et al. [16]. Barrett's argument relies on the key observation that the UI calculated using BROJA-PID upper bounds the UI calculated using any other PID definition satisfying assumption \((*)\) in Bertschinger et al. Entropy'14. Consequently, if one of the UI atoms of the BROJA-PID is zero then the corresponding UI atom of the other PID definitions must also be zero due to BROJA PID upper-bounding them and the non-negativity of the PID atoms. Since the remaining PID atoms are calculated using (1), it must be the case that the PID atoms calculated using any PID definition satisfying assumption \((*)\) must be the same as the PID atoms calculated using BROJA-PID whenever one of the UI terms of BROJA-PID is zero.

**Blackwellian PID**: The defining feature of a Blackwellian PID is that the UI atom is zero iff it is possible to construct the Markovian structure between the random variables (i.e., \(M\to X\to Y\) or \(M\to Y\to X\)), while preserving their pairwise marginals (see Venkatesh et al. [22] for more details). In all our theoretical results, we show that the random variables in the analyzed system admit

[MISSING_PAGE_FAIL:16]

2. \(P(X|M)\!=\!\)Negative Binomial\((f_{1}(M),p)\), for \(f_{1}(M)\in\mathbb{N}_{0}\)\(\forall\)\(m\in\mathcal{M}\) and \(p\in[0,1]\). 3. \(P(Y|M)\!=\!\)Negative Binomial\((f_{2}(M),p)\), for \(f_{2}(M)\in\mathbb{N}_{0}\)\(\forall\)\(m\in\mathcal{M}\). 4. \(f_{1}(m)\geq f_{2}(m)\)\(\forall\)\(m\in\mathcal{M}\) and all the underlying distributions are well-defined.
4. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \((0,\infty)\). 2. \(P(X|M)\!=\!\)Gamma\((a,M)\). 3. \(P(Y|M)=\!\)Gamma\((b,M)\). The corresponding \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\) specified in theorem 6 is obtained as follows: \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\!=\!xZ,\ Z\sim\text{Beta}(b,(1-b))\) with \(\gamma=\nicefrac{{b}}{{a}}\), and \(a,b\in(0,\infty)\). Note that we are following the Gamma\((\alpha,\beta)\) notation. **Result**: The Markov chain \(M\to X\to Y\in\Delta_{P}\) and \(UI(M;Y\backslash X)=0\). **An extension of the system for which the upper bound of Sec. 6 in applicable**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \(\mathcal{M}\). 2. \(P(X|M)\!=\!\)Gamma\((f_{1}(M),\beta)\), for \(f_{1}(M)>0\)\(\forall\)\(m\in\mathcal{M}\) and \(\beta>0\). 3. \(P(Y|M)=\!\)Gamma\((f_{2}(M),\beta)\), for \(f_{2}(M)>0\)\(\forall\)\(m\in\mathcal{M}\). 4. \(f_{1}(m)\geq f_{2}(m)\)\(\forall\)\(m\in\mathcal{M}\) and all the underlying distributions are well-defined.
5. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \((0,\infty)\). 2. \(P(X|M)\!=\!\)Exponential\((M)\). 3. \(P(Y|M)=\!\)Gamma\((b,M)\). The corresponding \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\) specified in theorem 6 is obtained as follows: \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\!=\!xZ,\ Z\sim\text{Beta}(b,(1-b))\), and \(0<b\leq 1\). Note that we are following the Gamma\((\alpha,\beta)\) notation. **Result**: The Markov chain \(M\to X\to Y\in\Delta_{P}\) and \(UI(M;Y\backslash X)=0\). **An extension of the system for which the upper bound of Sec. 6 in applicable**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \(\mathcal{M}\). 2. \(P(X|M)\!=\!\)Exponential\((f_{1}(M))\), for \(f_{1}(M)>0\)\(\forall\)\(m\in\mathcal{M}\). 3. \(P(Y|M)=\!\)Gamma\((f_{2}(M),f_{1}(M))\), for \(f_{2}(M)>0\)\(\forall\)\(m\in\mathcal{M}\). 4. \(f_{2}(m)\leq 1\)\(\forall\)\(m\in\mathcal{M}\) and all the underlying distributions are well-defined.
6. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \([0,1]\). 2. \(P(X|M)\!=\!\)Binomial\((a,M)\). 3. \(P(Y|M)=\!\)Binomial\((b,M)\). The corresponding \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\) specified in theorem 6 is Hypergeometric\((x,\)\(\gamma a,(1-\gamma)a)\) with \(\gamma=\nicefrac{{b}}{{a}}\), and \(a,b\in\mathbb{N}\). **Result**: The Markov chain \(M\to X\to Y\in\Delta_{P}\) and \(UI(M;Y\backslash X)=0\). **An extension of the system for which the upper bound of Sec. 6 in applicable**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \(\mathcal{M}\). 2. \(P(X|M)\!=\!\)Binomial\((f_{1}(M),p)\), for \(f_{1}(M)\in\mathbb{N}_{0}\)\(\forall\)\(m\in\mathcal{M}\) and \(p\in[0,1]\). 3. \(P(Y|M)=\!\)Binomial\((f_{2}(M),p)\), for \(f_{2}(M)\in\mathbb{N}_{0}\)\(\forall\)\(m\in\mathcal{M}\). 4. \(f_{1}(m)\geq f_{2}(m)\)\(\forall\)\(m\in\mathcal{M}\) and all the underlying distributions are well-defined.
7. **System**: The distributions of \(P(\vec{\mathbf{M}}),P(\vec{\mathbf{X}}|\vec{\mathbf{M}})\), and \(P(\vec{\mathbf{Y}}|\vec{\mathbf{M}})\) are as follows: 1. \(\vec{\mathbf{M}}\sim P(\vec{\mathbf{M}})\) having support over a \(k\)-dimensional simplex. 2. \(P(\vec{\mathbf{X}}|\vec{\mathbf{M}})\!=\!\)Multinomial\({}_{k}(a,\vec{\mathbf{M}})\). 3. \(P(\vec{\mathbf{Y}}|\vec{\mathbf{M}})\!=\!\)Multinomial\({}_{k}(b,\vec{\mathbf{M}})\).

The corresponding\(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\) specified in theorem 6 is Multivariate Hypergeometric \((\vec{\mathbf{x}},\gamma_{Y})\) with \(\gamma=\nicefrac{{b}}{{a}}\), and \(a\), \(b\in\mathbb{N}\).

**Result**: The Markov chain \(\vec{\mathbf{M}}\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\in\Delta _{P}\) and \(UI(\vec{\mathbf{M}};\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\).

**An extension of the system for which the upper bound of Sec. 6 in applicable**: The distributions of \(P(M),P(\vec{\mathbf{X}}|M)\), and \(P(\vec{\mathbf{Y}}|M)\) are as follows:

1. \(M\sim P(M)\) having support \(\mathcal{M}\).
2. \(P(\vec{\mathbf{X}}|M)\!=\!\text{Multinomial}_{k}(f_{1}(M),\vec{\mathbf{p}})\), for \(f_{1}(M)\in\mathbb{N}_{0}\;\forall\;m\in\mathcal{M}\).
3. \(P(\vec{\mathbf{Y}}|M)=\text{Multinomial}(f_{2}(M),\vec{\mathbf{p}})\), for \(f_{2}(M)\in\mathbb{N}_{0}\;\forall\;m\in\mathcal{M}\).
4. \(f_{1}(m)\geq f_{2}(m)\;\forall\;m\in\mathcal{M}\), \(\vec{\mathbf{p}}\) is a valid probability vector, and all the underlying distributions are well-defined.

## Appendix D Lists of Systems that can solved using Theorem 7

In this section, we provide examples of systems of random variables \((M,X,Y)\) employing well-known distributions whose PID terms can be obtained through theorem 7. The following list is appropriately modified from Appendix A of [26]. Similar to Appx. C, we only specify specify \(P(M)\), \(P(X|M)\), and \(P(Y|M)\) of the joint distribution \(P(M,X,Y)\), as any \(P(M,X,Y)\) having the same marginals can be solved through theorem 7. In the following examples, we assume \(a>b\).

1. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \((0,\infty)\). 2. \(P(X|M)\!=\!\text{Exponential}(M)\). 3. \(P(Y|M)=\text{Geometric}(\nicefrac{{M}}{{(\tau+M)}})\), where \(\tau\in(0,\infty)\). **Results**: The Markov chain \(M\to X\to Y\in\Delta_{P}\), and \(UI(M;Y\backslash X)=0\). The corresponding \(Q_{MC}(Y|X)\) required for constructing the Markov chain is Poisson\((\tau X)\).
2. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \(\mathcal{M}\). 2. \(P(X|M)\!=\!\text{Gamma}(f_{1}(M),f_{2}(M))\), where \(f_{1}(m),f_{2}(m)>0\;\forall\;m\in\mathcal{M}\). 3. \(P(Y|M)=\text{Negative Binomial}(f_{1}(M),\nicefrac{{f_{2}(M)}}{{(f_{2}(M)+\tau)}})\), where \(\tau\in(0,\infty)\) **Results**: The Markov chain \(M\to X\to Y\in\Delta_{P}\), and \(UI(M;Y\backslash X)=0\). The corresponding \(Q_{MC}(Y|X)\) required for constructing the Markov chain is is \(\text{Poisson}(\tau X)\).
3. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \((0,\infty)\). 2. \(P(X|M)\!=\!\text{Beta}(M,1)\). 3. \(P(Y|M)\) is a discrete distribution with over support set \(\{0,1,\ldots,N\}\), with \[P(Y=y|M=m)=m\frac{\Gamma(y+m)N!}{\Gamma(N+1+m)y!},\] where \(\Gamma(.)\) is the Gamma function. **Results**: The Markov chain \(M\to X\to Y\in\Delta_{P}\), and \(UI(M;Y\backslash X)=0\). The corresponding \(Q_{MC}(Y|X)\) required for constructing the Markov chain is is \(\text{Poisson}(\tau X)\).
4. **System**: The distributions of \(P(M),P(X|M)\), and \(P(Y|M)\) are as follows: 1. \(M\sim P(M)\) having support \((0,\infty)\). 2. \(P(X|M)\!=\!\text{Beta}(1,M)\). 3. \(P(Y|M)\) is a discrete distribution with over support set \(\{0,1,\ldots,N\}\), with \[P(Y=y|M=m)=m\frac{\Gamma(y+m)N!}{\Gamma(N+1+m)y!},\] where \(\Gamma(.)\) is the Gamma function. **Results**: The Markov chain \(M\to X\to Y\in\Delta_{P}\), and \(UI(M;Y\backslash X)=0\). The corresponding \(Q_{MC}(Y|X)\) required for constructing the Markov chain is is \(\text{Binomial}(N,1-X)\).

5. **System**: The distributions of \(P(\mathbf{\bar{M}}),P(\mathbf{\bar{X}}|\mathbf{\bar{M}})\), and \(P(\mathbf{\bar{Y}}|\mathbf{\bar{M}})\) are as follows: 1. \(\mathbf{\bar{M}}\sim P(\mathbf{\bar{M}})\) having support \((0,\infty)^{d}\) for some dimension \(d\). 2. \(P(\mathbf{\bar{X}}|\mathbf{\bar{M}})\mathbf{=}\text{Dirichlet}(M_{1},\ldots,M_ {d})\). 3. \(P(\mathbf{\bar{Y}}|\mathbf{\bar{M}})\) is a discrete distribution having support \(\{y_{i}\in\{1,\ldots,N\}\ \forall\ i\in[d]|\sum_{i=1}^{d}y_{i}=N\}\), and \[P(\mathbf{\bar{Y}}|\mathbf{\bar{M}})=\frac{\Gamma\left(\sum_{i=1}^{d}M_{i} \right)N!}{\Gamma\left(\sum_{i=1}^{d}M_{i}+N\right)}\prod_{i=1}^{d}\frac{ \Gamma\left(M_{i}+Y_{i}\right)}{\Gamma\left(M_{i}\right)Y_{i}!},\] where \(\Gamma(.)\) is the Gamma function.

**Results**: The Markov chain \(\mathbf{\bar{M}}\rightarrow\mathbf{\bar{X}}\rightarrow\mathbf{\bar{Y}}\in \Delta_{P}\), and \(UI(\mathbf{\bar{M}};\mathbf{\bar{Y}}\backslash\mathbf{\bar{X}})=0\). The corresponding \(Q_{MC}(\mathbf{\bar{Y}}|\mathbf{\bar{X}})\) required for constructing the Markov chain is \(\text{Multinomial}_{d}(N,\mathbf{\bar{X}})\).

## Appendix E Miscellaneous PID Results

This section explores the analytical calculation of PID terms for five more systems of random variables \((M,X,Y)\) with joint distribution \(P(M,X,Y)\). These five systems' \(P(M,X,Y)\) employ well-known distributions, but their respective analytical PID calculation does not result from Theorem 1-Theorem 7. Of note are the results of \(P(M,X,Y)\) employing exponential and uniform distribution, which states that these systems will always have both unique information terms zero.

### PID for the miscellaneous system **M1**.

Miscellaneous system **M1**: Let \((M,X,Y)\) be a system of random variables with joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M1** if it satisfies the following two properties:

1. \(M\sim P(M)\), and has support \((0,\infty)\).
2. \(P(X|M)\mathbf{=}\text{Exponential}(\gamma_{X}M)\), and \(P(Y|M)\mathbf{=}\text{Exponential}(\gamma_{Y}M)\), where \(\gamma_{X},\gamma_{Y}\in(0,\infty)\).

**Proposition 2**.: _Let \(M,X\), and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M1** defined above. Then, \(\Delta_{P}\) contains a distribution \(Q_{MC}(M,X,Y)\), which admits both Markovian structures, i.e., \(M\to X\to Y\) and \(M\to Y\to X\). Consequently, both \(UI(M;Y\backslash X)=UI(M;X\backslash Y)=0\)._

Proof.: We prove proposition 2 by explicitly constructing a joint distribution \(Q_{MC}(M,X,Y)\in\Delta_{P}\) that admits both Markovian structures, namely, \(M\to X\to Y\) and \(M\to Y\to X\). A necessary and sufficient condition for the distribution \(Q_{MC}(M,X,Y)\) to admit both Markovian structures is that it should satisfy (14).

\[Q_{MC}(M,X,Y)\mathbf{=}Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X)\mathbf{=}Q_{MC}(M)Q_{ MC}(Y|M)Q_{MC}(X|Y).\] (14)

We now briefly outline the structure of the proof. The rest of the proof can be divided into three parts:

1. In the first part, we explicitly construct \(Q_{MC}(M,X,Y)\).
2. In the second part, we show that the \(Q_{MC}(M,X,Y)\) constructed in the first part lies in \(\Delta_{P}\).
3. In the third part, we conclude our proof by showing that \(Q_{MC}(M,X,Y)\) satisfies (14) and, consequently, admits both Markovian structures.

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

We specify \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), and \(Q_{MC}(Y|X)\). We choose \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M),\text{ and }Q_{MC}(X|M)=P(X|M).\] (15)The distribution \(Q_{MC}(Y|X)\) is specified through the following deterministic transformation:

\[Y=\frac{\gamma_{X}}{\gamma_{Y}}X.\] (16)

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\):

_Calculating \(Q_{MC}(Y|M)\)_: By lemma 1, if \(X\sim\text{Exp}(\lambda)\), then \(aX\sim\text{Exponential}(\nicefrac{{\lambda}}{{a}})\). Since \(Q_{MC}(X|M){=}\text{Exponential}(\gamma_{X}M)\), we can use the result of lemma 1 to calculate \(Q_{MC}(Y|M)\) as follows:

\[Q_{MC}(Y|M) =Q_{MC}\left.\left(\frac{\gamma_{X}}{\gamma_{Y}}X\right|M\right) =\text{Exponential}\left(\frac{\gamma_{Y}}{\gamma_{X}}\gamma_{X}M\right)\] \[=\text{Exponential}\left(\gamma_{Y}M\right) =P(Y|M).\] (17)

Then, from (15) and (17), we can conclude:

\[Q_{MC}(M,X) =Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (18) \[Q_{MC}(M,Y) =Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (19)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

**Part 3: Showing \(Q_{MC}(M,X,Y)\) satisfies** (14)

It is trivial to see that by the construction of \(Q_{MC}(M,X,Y)\) given in Part 1, we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X).\] (20)

By the chain rule, we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(Y|M)Q_{MC}(X|Y,M).\] (21)

Hence, for showing that \(Q_{MC}(M,X,Y)\) satisfies (14), we only need to show that \(Q_{MC}(X|Y,M)=Q_{MC}(X|Y)\).

_Showing \(Q_{MC}(X|Y,M)=Q_{MC}(X|Y)\)_: Observe that \(Y=\nicefrac{{\gamma_{X}}}{{\gamma_{Y}}}X\) by construction. Therefore, we can equivalently express \(X=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}Y\). Consequently,

\[Q_{MC}(X|Y,M)=\mathbb{I}[X=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}Y]\text{ as }X=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}Y,\] (22)

where \(\mathbb{I}[\cdot]\) is the indicator function. Since R.H.S of the above equation does not depend on \(M\), we have:

\[Q_{MC}(X|Y,M)=\mathbb{I}[X=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}Y]=Q_{MC}(X|Y).\] (23)

Combining (21) and (23), we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(Y|M)Q_{MC}(X|Y).\] (24)

Combining (20) and (24), we can show that \(Q_{MC}(M,X,Y)\) satisfies (14).

**Conclusion**: Combining the results of parts 2 and 3, we can conclude that the \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and contains both Markovian structure \(M\to X\to Y\) and \(M\to Y\to X\). A unique implication of \(Q_{MC}(M,X,Y)\) admitting both Markovian structures is that both \(I_{Q_{MC}}(M;X|Y)=I_{Q_{MC}}(M;Y|X)=0\). Consequently, both minimization problems presented in (2) are minimized by \(Q_{MC}(M,X,Y)\) with \(UI(M;X\backslash Y)=\min_{Q\in\Delta_{P}}I_{Q}(M;X|Y)=0\) and \(UI(M;Y\backslash X)=\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)=0\), completing our proof. 

**Lemma 1**.: _If \(X\sim\text{Exponential}(\lambda)\) with \(\lambda\in(0,\infty)\), then \(\gamma X\sim\text{Exponential}(\nicefrac{{\lambda}}{{\gamma}})\) for \(\gamma\in(0,\infty)\)._

Proof.: We denote the p.d.f. of the random variable \(X\) as \(p_{X}(\cdot)\) and, by lemma statement, we know:

\[p_{X}(X=x)=\text{Exponential}(\lambda)=\left\{\begin{array}{cc}\lambda e^{- x\lambda}&x\geq 0\\ 0&\text{otherwise}\end{array}\right..\] (25)Denote the p.d.f. of the function \(f(X)\) of the random variable \(X\) as \(p_{f(X)}(\cdot)\). We want to derive \(p_{f(X)}(\cdot)\) for \(f(X)=\gamma X\). Since, \(f(X)=\gamma X\) is an invertible function, we can use the change-of-variable technique for deriving \(p_{f(X)}(\cdot)\):

\[p_{f(X)}(f(X)=y) =p_{X}(X=f^{-1}(y))\left|\frac{df^{-1}(y)}{dy}\right|=\left\{ \begin{array}{cc}\frac{\lambda}{\tau}e^{-y\sqrt{\gamma}}&y\geq 0\\ 0&\text{otherwise}\end{array}\right.\] \[=\text{Exponential}(\nicefrac{{\lambda}}{{\gamma}}).\] (26)

### PID for the miscellaneous system **M2**.

Miscellaneous System **M2**: Let \((M,X,Y)\) be a system of random variables with joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M2** if it satisfies the following two properties:

1. \(M\sim P(M)\) and has support \((0,\infty)\).
2. \(P(X|M)\!=\!\text{Negative Binomial}(M,p_{X})\), and \(P(Y|M)\!=\!\text{Negative Binomial}(M,p_{Y})\).

This system is adapted from the negative binomial decomposition listed in Appendix A [26].

**Proposition 3**.: _Let \(M,X\), and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M2** defined above._

1. _If_ \(p_{X}\leq p_{Y}\)_, then_ \(\Delta_{P}\) _contains a Markov chain of the form_ \(M\to X\to Y\)_, and_ \(UI(M;Y\backslash X)=0\)_._
2. _If_ \(p_{X}\geq p_{Y}\)_, then_ \(\Delta_{P}\) _contains a Markov chain of the form_ \(M\to Y\to X\)_, and_ \(UI(M;X\backslash Y)=0\)_._

Proof.: We only provide an explicit proof of condition 1. The proof of condition 2 follows the same steps as the proof of condition 1 with parameters of \(X\) and \(Y\) switched.

**Proof of condition 1**: We briefly outline the proof structure. The proof can be divided into two parts:

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structure \(M\to X\to Y\).
2. In the second part, we conclude our proof by showing the \(Q_{MC}(M,X,Y)\) constructed in the first part, lies in \(\Delta_{P}\).

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

Since \(Q_{MC}(M,X,Y)\) is the joint distribution of a Markov chain \(M\to X\to Y\), we have \(Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X)\). Taking advantage of this special structure of \(Q_{MC}(M,X,Y)\), we specify \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), and \(Q_{MC}(Y|X)\). We choose \(Q_{MC}(M)\), \(Q_{MC}(X|M)\) and \(Q_{MC}(Y|X)\) as follows:

\[Q_{MC}(M)=P(M),Q_{MC}(X|M)=P(X|M)\text{ and }Q_{MC}(Y|X)= \text{Binomial}(X,p),\] \[\text{where }p=\frac{p_{X}(1-p_{Y})}{p_{Y}(1-p_{X})}.\] (27)

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\):

_Calculating \(Q_{MC}(Y|M)\)_: Since \(Q_{MC}(X|M)=\text{Negative Binomial}(M,p_{X})\) and \(Q_{MC}(Y|X)=\text{Binomial}(N,p)\), we can use lemma 2 for calculating \(Q(Y|M)\):

\[Q_{MC}(Y|M)=\text{Negative Binomial}(M,p_{Y})=P(Y|M).\] (28)Then, from (27) and (28), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (29) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (30)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\). This concludes our proof. 

**Lemma 2**.: _Let \(X\sim\text{Negative Binomial}(N,p_{X})\) and \(P(Y|X)=\text{Binomial}(X,p)\) with \(p=\frac{p_{X}(1-p_{Y})}{p_{Y}(1-p_{X})}\), \(p_{Y}\geq p_{X}\), \(p_{Y},p_{X}\in[0,1]\), and \(N\in\mathbb{N}_{0}\). Then, \(Y\sim\text{Negative Binomial}(N,p_{Y})\)._

Proof.: First, in order for \(P(Y|X)\) to be a legitimate distribution, we need to ensure that \(p\in[0,1]\), i.e., we need to show that:

\[0\leq p\leq 1.\] (31)

Since \(p_{X},(1-p_{Y}),(1-p_{X}),p_{Y}\geq 0\) as \(p_{X},p_{Y}\in[0,1]\), we can easily verify the first inequality:

\[p=\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}\geq 0.\] (32)

For verifying \(p\leq 1\), we will use the assumption that \(p_{Y}\geq p_{X}\):

\[p_{X}\leq p_{Y}\Rightarrow\frac{1}{p_{Y}}\leq\frac{1}{p_{X}}.\] (33)

Subtracting \(1\) from both sides of the above inequality.

\[\frac{1}{p_{Y}}-1\leq\frac{1}{p_{X}}-1\Rightarrow\frac{1-p_{Y}}{p_{Y}}\leq \frac{1-p_{X}}{p_{X}}.\] (34)

Rearranging terms, we obtain:

\[\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}\leq 1\Rightarrow p\leq 1\left(\text{as }p=\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}\right).\] (35)

For calculating \(P(Y)\), we borrow the result of proposition 13 in [26], which states that if \(X\sim\text{Negative Binomial}(M,p_{X})\) and \(P(Y|X)=\text{Binomial}(X,p)\), then \(P(Y)=\text{Negative Binomial}\left(N,\frac{p_{X}}{p_{X}+p-pp_{X}}\right)\). Hence,

\[P(Y) =\text{Negative Binomial}\left(N,\frac{p_{X}}{p_{X}+p-pp_{X}} \right)=\text{Negative Binomial}\left(N,\frac{p_{X}}{p_{X}+p(1-p_{X})}\right)\] \[=\text{Negative Binomial}\left(N,\frac{p_{X}}{p_{X}+\frac{p_{X}(1 -p_{Y})}{p_{Y}}}\right)\] \[=\text{Negative Binomial}\left(N,\frac{p_{X}p_{Y}}{p_{X}p_{Y}+p_{ X}(1-p_{Y})}\right)\] \[=\text{Negative Binomial}\left(N,\frac{p_{X}p_{Y}}{p_{X}}\right)= \text{Negative Binomial}\left(N,p_{Y}\right).\] (36)

### PID for the miscellaneous system M3.

Miscellaneous System **M3**: Let \((M,X,Y)\) be a system of random variables with joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M3** if it satisfies the following two properties:

1. \(M\sim P(M)\) and has support \((0,\infty)\).
2. \(P(X|M)\!=\!\text{Uniform}(0,\gamma_{X}M)\) and \(P(Y|M)\!=\!\text{Uniform}(0,\gamma_{Y}M)\), where \(\gamma_{X},\gamma_{Y}\in\mathbb{R}\backslash\{0\}\).

**Proposition 4**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M3** defined above. Then, \(\Delta_{P}\) contains a distribution \(Q_{MC}(M,X,Y)\), which admits both Markovian structures, i.e., \(M\to X\to Y\) and \(M\to Y\to X\). Consequently, both \(UI(M;Y\backslash X)=UI(M;X\backslash Y)=0\)._

Proof.: We prove proposition 4 by explicitly constructing a joint distribution \(Q_{MC}(M,X,Y)\in\Delta_{P}\) that admits both Markovian structures, namely, \(M\to X\to Y\) and \(M\to Y\to X\). A necessary and sufficient condition for the distribution \(Q_{MC}(M,X,Y)\) to admit both Markovian structures is that it should satisfy (37):

\[Q_{MC}(M,X,Y)\!=\!Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X)\!=\!Q_{MC}(M)Q_{MC}(Y|M)Q_{ MC}(X|Y).\] (37)

We now briefly outline the structure of the proof. The rest of the proof can be divided into three parts:

1. In the first part, we explicitly construct \(Q_{MC}(M,X,Y)\).
2. In the second part, we show that the \(Q_{MC}(M,X,Y)\) constructed in the first part, lies in \(\Delta_{P}\).
3. In the third part, we conclude our proof by showing that \(Q_{MC}(M,X,Y)\) satisfies (37) and, consequently, admits both Markovian structures.

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

We specify \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), and \(Q_{MC}(Y|X)\). We choose \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M),\text{ and }Q_{MC}(X|M)=P(X|M).\] (38)

The distribution \(Q_{MC}(Y|X)\) is specified through the following deterministic transformation:

\[Y=\frac{\gamma_{Y}}{\gamma_{X}}X.\] (39)

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\):

_Calculating \(Q_{MC}(Y|M)\)_: Since \(Q_{MC}(X|M)\!=\!\text{Uniform}(0,\gamma_{X}M)\), and \(Y=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}X\), we can use the result of lemma 3 to calculate \(Q_{MC}(Y|M)\) as follows:

\[Q_{MC}(Y|M) =Q_{MC}\left(\frac{\gamma_{Y}}{\gamma_{X}}X\bigg{|}\,M\right)= \text{Uniform}\left(0,\frac{\gamma_{Y}}{\gamma_{X}}\gamma_{X}M\right)=\text{ Uniform}\left(0,\gamma_{Y}M\right)\] \[=P(Y|M).\] (40)

Then, from (38) and (40), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (41) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (42)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

**Part 3: Showing \(Q_{MC}(M,X,Y)\) satisfies (37)**It is trivial to see that by the construction of \(Q_{MC}(M,X,Y)\) given in Part 1, we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X).\] (43)

By the chain rule, we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(Y|M)Q_{MC}(X|Y,M).\] (44)

Hence, for showing that \(Q_{MC}(M,X,Y)\) satisfies (37), we only need to show that \(Q_{MC}(X|Y,M)=Q_{MC}(X|Y)\).

_Showing \(Q_{MC}(X|Y,M)=Q_{MC}(X|Y)\)_: Observe that \(Y=\nicefrac{{\gamma_{Y}}}{{\gamma_{X}}}X\) by construction. Therefore, we can equivalently express \(X=\nicefrac{{\gamma_{X}}}{{\gamma_{Y}}}Y\). Consequently,

\[Q_{MC}(X|Y,M)=\mathbb{I}[X=\nicefrac{{\gamma_{X}}}{{\gamma_{Y}}}Y]\text{ as }X=\nicefrac{{\gamma_{X}}}{{\gamma_{Y}}}Y,\] (45)

where \(\mathbb{I}[\cdot]\) is the indicator function. Since R.H.S of the above equation does not depend on \(M\), we have:

\[Q_{MC}(X|Y,M)=\mathbb{I}[X=\nicefrac{{\gamma_{X}}}{{\gamma_{Y}}}Y]=Q_{MC}(X|Y).\] (46)

Combining (44) and (46), we have:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(Y|M)Q_{MC}(X|Y).\] (47)

Combining (43) and (47), we can show that \(Q_{MC}(M,X,Y)\) satisfies (37).

**Conclusion**: Combining the results of parts 2 and 3, we can conclude that \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and contains both Markovian structures: \(M\to X\to Y\) and \(M\to Y\to X\). A unique implication of \(Q_{MC}(M,X,Y)\) admitting both Markovian structures is that both \(I_{Q_{MC}}(M;X|Y)=I_{Q_{MC}}(M;Y|X)=0\). Consequently, both minimization problems presented in (2) are minimized by \(Q_{MC}(M,X,Y)\) with \(UI(M;X\backslash Y)=\min_{Q\in\Delta_{P}}I_{Q}(M;X|Y)=0\) and \(UI(M;Y\backslash X)=\min_{Q\in\Delta_{P}}I_{Q}(M;Y|X)=0\), completing our proof. 

**Lemma 3**.: _Let \(X\sim\text{Uniform}(0,a)\) for some \(a\in\mathbb{R}\), then \(\gamma X\sim\text{Uniform}(0,\gamma a)\) for \(\gamma\in\mathbb{R}\) and \(a,\gamma\neq 0\)._

Proof.: We denote the p.d.f. of the random variable \(X\) as \(p_{X}(\cdot)\). By lemma statement, we know:

\[p_{X}(X=x)=\text{Uniform}(0,a)=\left\{\begin{array}{cc}\frac{1}{|a|}&x\in[ 0,a]\\ 0&\text{otherwise}\end{array}\right..\] (48)

Denote the p.d.f. of the function \(f(X)\) of the random variable \(X\) as \(p_{f(X)}(\cdot)\). We want to derive \(p_{f(X)}(\cdot)\) for \(f(X)=\gamma X\). Since, \(f(X)=\gamma X\) is an invertible function, we can use the change-of-variable technique for deriving \(p_{f(X)}(\cdot)\):

\[p_{f(X)}(f(X)=y)=p_{X}(X=f^{-1}(y))\left|\frac{df^{-1}(y)}{dy}\right|=\left\{ \begin{array}{cc}\nicefrac{{1}}{{\left|{\gamma a}\right|}}&y\in[0,\gamma a ]\\ 0&\text{otherwise}\end{array}\right.=\text{Uniform}(0,\gamma a).\]

### PID for the miscellaneous system M4.

Miscellaneous System **M4**: Let \((M,X,Y)\) be a system of random variables with joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M4** if it satisfies the following two properties:

1. \(M\sim P(M)\) and has support \([0,1]\).
2. \(P(X|M)\)\(=\)Bernoulli\((M)\), and \(P(Y|M)\)\(=\)Bernoulli\((M+p-2pM)\), where \(p\in[0,1]\).

This system is adapted from the Bernoulli decomposition listed in Appendix A [26].

**Proposition 5**.: _Let \(M,X,\text{and }Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes the miscellaneous system **M4** defined above. Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._Proof.: We briefly outline the proof structure. The proof can be divided into two parts:

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structure \(M\to X\to Y\).
2. In the second part, we conclude our proof by showing the \(Q_{MC}(M,X,Y)\) constructed in the first part, lies in \(\Delta_{P}\).

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

Since \(Q_{MC}(M,X,Y)\) is the joint distribution of a Markov chain \(M\to X\to Y\), we have \(Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X)\). Leveraging the special structure of \(Q_{MC}(M,X,Y)\), we specify \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), and \(Q_{MC}(Y|X)\). We choose \(Q_{MC}(M)\), \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M),\text{ and }Q_{MC}(X|M)=P(X|M).\] (49)

The distribution \(Q_{MC}(Y|X)\) is specified through the following stochastic transformation:

\[Y=X(1-Z)+Z(1-X),\] (50)

where \(Z\sim\text{Bernoulli}(p)\) and \(Z\perp\!\!\!\perp(X,M)\).

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\):

_Calculating \(Q_{MC}(Y|M)\)_: For calculating \(Q(Y|M)\), we use the result of proposition 10 in [26], which states that if \(X^{\prime}\sim\text{Bernoulli}(\theta)\), \(Z^{\prime}\sim\text{Bernoulli}(p^{\prime})\), \(Z^{\prime}\perp\!\!\!\perp X^{\prime}\), and \(Y^{\prime}=X^{\prime}(1-Z^{\prime})+(1-X^{\prime})Z^{\prime}\), then \(Y\sim\text{Bernoulli}(\theta+p^{\prime}-2p^{\prime}\theta)\).

Noting that \(Q_{MC}(X|M)=\text{Bernoulli}(M)\) and \(Y=X(1-Z)+Z(1-X)\), where \(Z\sim\text{Bernoulli}(p)\) and \(Z\perp\!\!\!\perp X|M\), we can calculate \(Q_{MC}(Y|M)\) as follows:

\[Q_{MC}(Y|M)=\text{Bernoulli}(M+p-2pM)=P(Y|M).\] (51)

Then, from (49) and (51), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (52) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (53)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\). This concludes our proof. 

### PID for the miscellaneous system **M5**.

Miscellaneous System **M5**: Let \((\vec{\mathbf{M}},X,Y)\) be a system of random variables with joint distribution \(P(\vec{\mathbf{M}},X,Y)\). The joint distribution \(P(\vec{\mathbf{M}},X,Y)\) describes the miscellaneous system **M5** if it satisfies the following two properties:

1. \(\vec{\mathbf{M}}\sim P(\vec{\mathbf{M}})\) and has support over simplex of dimension \(d\).
2. \(P(X|\vec{\mathbf{M}})\!=\!\text{Categorical}(M_{1},\dots,M_{d})\), and \(P(Y|\vec{\mathbf{M}})\!=\!\text{Categorical}(\phi_{1},\dots,\phi_{d})\), where \(\phi_{i}=(1-p)M_{i}+\nicefrac{{p}}{{d}}\).

This system is adapted from the categorical decomposition listed in Appendix A [26].

**Proposition 6**.: _Let \(\vec{\mathbf{M}},X\), and \(Y\) be random variables whose joint distribution \(P(\vec{\mathbf{M}},X,Y)\) describes the miscellaneous system **M5** defined above. Then, \(\Delta_{P}\) contains a Markov chain of the form \(\vec{\mathbf{M}}\to X\to Y\) and \(UI(\vec{\mathbf{M}};Y\backslash X)=0\)._

Proof.: We briefly outline the proof structure. The proof can be divided into two parts:

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(\vec{\mathbf{M}},X,Y)\) having the Markovian structure \(\vec{\mathbf{M}}\to X\to Y\).

2. In the second part, we conclude our proof by showing the \(Q_{MC}(\vec{\mathbf{M}},X,Y)\) constructed in the first part, lies in \(\Delta_{P}\).

**Part 1: Specifying \(Q_{MC}(\vec{\mathbf{M}},X,Y)\)**

Since \(Q_{MC}(\vec{\mathbf{M}},X,Y)\) is the joint distribution of a Markov chain \(\vec{\mathbf{M}}\to X\to Y\), we have \(Q_{MC}(\vec{\mathbf{M}},X,Y)=Q_{MC}(\vec{\mathbf{M}})Q_{MC}(X|\vec{\mathbf{M} })Q_{MC}(Y|X)\). Taking advantage of this special structure of \(Q_{MC}(\vec{\mathbf{M}},X,Y)\), we specify \(Q_{MC}(\vec{\mathbf{M}},X,Y)\) by individually specifying \(Q_{MC}(\vec{\mathbf{M}})\), \(Q_{MC}(X|\vec{\mathbf{M}})\), and \(Q_{MC}(Y|X)\). We choose \(Q_{MC}(\vec{\mathbf{M}})\), \(Q_{MC}(X|\vec{\mathbf{M}})\) as follows:

\[Q_{MC}(\vec{\mathbf{M}})=P(\vec{\mathbf{M}}),\text{ and }Q_{MC}(X|\vec{ \mathbf{M}})=P(X|\vec{\mathbf{M}}).\] (54)

The distribution \(Q_{MC}(Y|X)\) is specified through the following transformation:

\[Y=X(1-Z)+ZD,\] (55)

where \(Z\sim\text{Bernoulli}(p)\), \(D\sim\text{Categorical}\left(\frac{1}{d},\ldots,\frac{1}{d}\right)\), and \(D\), \(Z\), and \((X,\vec{\mathbf{M}})\) are jointly independent.

**Part 2: Showing \(Q_{MC}(\vec{\mathbf{M}},X,Y)\in\Delta_{P}\)**

For showing \(Q_{MC}(\vec{\mathbf{M}},X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|\vec{\mathbf{M}})\):

_Calculating \(Q_{MC}(Y|\vec{\mathbf{M}})\)_: For calculating \(Q_{MC}(Y|\vec{\mathbf{M}})\), we use the result of proposition 11 in [26], which states that:

If \(X^{\prime}\sim\text{Categorical}(\theta_{1},\ldots,\theta_{d})\), \(Z^{\prime}\sim\text{Bernoulli}(p^{\prime})\), \(D^{\prime}\sim\text{Categorical}\left(\nicefrac{{1}}{{d}},\ldots,\nicefrac{{ 1}}{{d}}\right)\), and \(Y^{\prime}=X^{\prime}(1-Z^{\prime})+Z^{\prime}D^{\prime}\) with \(X^{\prime},Z^{\prime}\), and \(D^{\prime}\) jointly independent, then \(Y^{\prime}\sim\text{Categorical}(\phi^{\prime}_{1},\ldots,\phi^{\prime}_{d})\) with \(\phi^{\prime}_{i}=(1-p^{\prime})\theta_{i}+\nicefrac{{p^{\prime}}}{{d}}\).

Noting that \(Q_{MC}(X|\vec{\mathbf{M}})=\text{Categorical}(M_{1},\ldots,M_{d})\) and \(Y=X(1-Z)+ZD\), with \(X\),\(D\), and \(Z\) jointly conditionally independent conditioned on \(M\), we have:

\[Q_{MC}(Y|\vec{\mathbf{M}})=\text{Categorical}(\phi_{1},\ldots,\phi_{d}),\text { with }\phi_{i}=M_{i}(1-p)+\frac{p}{d}=P(Y|\vec{\mathbf{M}}).\] (56)

From (54) and (56), we can conclude:

\[Q_{MC}(\vec{\mathbf{M}},X)=Q_{MC}(\vec{\mathbf{M}})Q_{MC}(X| \vec{\mathbf{M}})=P(M)P(X|\vec{\mathbf{M}})=P(\vec{\mathbf{M}},X),\] (57) \[Q_{MC}(\vec{\mathbf{M}},Y)=Q_{MC}(\vec{\mathbf{M}})Q_{MC}(Y| \vec{\mathbf{M}})=P(M)P(Y|\vec{\mathbf{M}})=P(\vec{\mathbf{M}},Y).\] (58)

Hence, \(Q_{MC}(\vec{\mathbf{M}},X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(\vec{\mathbf{M}};Y\backslash X)=0\). This concludes our proof. 

### PID for the miscellaneous system M6.

Miscellaneous System **M6**: Let \((M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) be a system of random variables with joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\). The joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) describes the miscellaneous system **M6** if it satisfies the following two properties:

1. \(M\sim P(M)\) and has support \(\mathbb{N}\).
2. \(P(\vec{\mathbf{X}}|M)\!=\!\text{Multinomial}_{d_{X}}(\vec{\mathbf{X}};M,\vec{ \mathbf{p}}_{X})\), and \(P(\vec{\mathbf{Y}}|M)\!=\!\text{Multinomial}_{d_{Y}}(\vec{\mathbf{Y}};M,\vec{ \mathbf{p}}_{Y})\).

**Proposition 7**.: _Let \(M\) be a random variable. Furthermore, assume \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) are random vectors of size \(d_{X}\) and \(d_{Y}\), respectively, such that \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) describes miscellaneous system **M6**._

1. _If,_ \(\min_{i\in[d_{Y}]}p_{i}^{Y}\geq\min_{i\in[d_{X}]}p_{i}^{X}\)_, then there exists a Markov chain of the form_ \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) _in_ \(\Delta_{P}\)_, and_ \(UI(M;\vec{\mathbf{Y}}|\vec{\mathbf{X}})=0\)_._
2. _If,_ \(\min_{i\in[d_{Y}]}p_{i}^{Y}\leq\min_{i\in[d_{X}]}p_{i}^{X}\)_, then there exists a Markov chain of the form_ \(M\rightarrow\vec{\mathbf{Y}}\rightarrow\vec{\mathbf{X}}\) _in_ \(\Delta_{P}\)_, and_ \(UI(M;\vec{\mathbf{X}}|\vec{\mathbf{Y}})=0\)Proof.: We only provide an explicit proof of condition 1. The proof of condition 2 follows the same steps as the proof of condition 1 with parameters of \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) switched.

We briefly outline the proof structure. The proof can be divided into two major parts:

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) having the Markovian structure \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\).
2. In the second part, we conclude our proof by showing the \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) constructed in the first part, lies in \(\Delta_{P}\).

**Part 1: Specifying \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\)**

Since \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) is the joint distribution of a Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\), we have \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{X}}|M )Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\). Taking advantage of this special structure of \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\), we specify \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(\vec{\mathbf{X}}|M)\), and \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\). We choose \(Q_{MC}(M)\), \(Q_{MC}(\vec{\mathbf{X}}|M)\) as follows:

\[Q_{MC}(M)=P(M),\text{ and }Q_{MC}(\vec{\mathbf{X}}|M)=P(\vec{\mathbf{X}}|M).\] (59)

_Specifying \(Q(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\)_: Let \(k_{X}^{\star}=\arg\min_{i\in[d_{X}]}p_{i}^{X}\), and \(k_{Y}^{\star}=\arg\min_{i\in[d_{Y}]}p_{i}^{Y}\). Then, \(Q(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\!=\!\text{Multinomial}_{d_{Y}}\left( \vec{\mathbf{Y}};\sum_{i\in[d_{X}]\setminus k_{X}^{\star}}x_{i},\mathbf{p}_{ i}^{Y}\right)\), where

\[\mathbf{p}_{Y}^{\star}\!=\!\left[\frac{p_{1}^{Y}}{\sum_{i\in[d_{X}]\setminus k _{X}^{\star}}p_{i}^{X}}\ \cdots\ \frac{p_{d_{Y-1}}^{Y}}{\sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{X}}\ 1-\frac{\sum_{i=1}^{d_{Y}-1}p_{i}^{Y}}{\sum_{i\in[d_{X}]\setminus k_{X}^{ \star}}p_{i}^{X}}\right]^{T}.\] (60)

We first show the proposed \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\) is a valid distribution. By condition 1:

\[\min_{i\in[d_{Y}]}p_{i}^{Y}\geq\min_{i\in d_{X}}p_{i}^{X}\] (61) \[\Rightarrow \sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{X}\geq\sum_{i\in [d_{Y}]\setminus k_{Y}^{\star}}p_{i}^{Y}\geq p_{j}^{Y},\ \forall\ j\in[d_{Y}]\] (62) \[\Rightarrow 1\geq\frac{p_{j}^{Y}}{\sum_{i\in[d_{X}]\setminus k_{X}^{ \star}}p_{i}^{X}}\geq 0,\ \forall\ j\in[d_{Y}],\] (63)

where the last inequality is due to \(\vec{\mathbf{p}}_{X}\) and \(\vec{\mathbf{p}}_{Y}\) forming valid multinomial distributions.

**Part 2: Showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\)**

For showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\), we first need to calculate \(Q_{MC}(\vec{\mathbf{Y}}|M)\):

_Calculating \(Q_{MC}(\vec{\mathbf{Y}}|M)\)_: We use lemma 4 to calculate \(Q(\vec{\mathbf{Y}}|M)\). Note that \(Q_{MC}(\vec{\mathbf{X}}|M)=\text{Multinomial}_{d_{X}}(M,\vec{\mathbf{p}}_{X})\), hence we have:

\[Q_{MC}(Y|M)=\text{Multinomial}_{d_{Y}}(M,\vec{\mathbf{p}}_{Y}^{ \prime}),\text{ where },\] \[\vec{\mathbf{p}}_{Y}^{\prime}=\left[\frac{p_{1}^{Y}\sum_{i\in[d_ {X}]\setminus k_{X}^{\star}}p_{i}^{X}}{\sum_{i\in[d_{X}]\setminus k_{X}^{\star} }p_{i}^{X}}\ \cdots\ \frac{p_{d_{Y-1}}^{Y}\sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{X}}{ \sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{X}}\ 1-\frac{\sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{X} \sum_{i=1}^{d_{Y}-1}p_{i}^{Y}}{\sum_{i\in[d_{X}]\setminus k_{X}^{\star}}p_{i}^{ X}}\right]\] \[\Rightarrow \vec{\mathbf{p}}_{Y}^{\prime}=\left[p_{1}^{Y}\ \cdots\ p_{d_{Y}-1}^{Y}\ 1-\sum_{i\in[d_{Y}]}p_{i}^{Y}\right]=\vec{\mathbf{p}}_{Y}\] \[\Rightarrow Q_{MC}(Y|M)=\text{Multinomial}_{d_{Y}}(M,\vec{\mathbf{p}}_{Y})=P( Y|M).\] (64)

Then, from (59) and (64), we can conclude:

\[Q_{MC}(M,\vec{\mathbf{X}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{X}}|M)=P( M)P(\vec{\mathbf{X}}|M)=P(M,\vec{\mathbf{X}}),\] (65) \[Q_{MC}(M,\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{Y}}|M)=P( M)P(\vec{\mathbf{Y}}|M)=P(M,\vec{\mathbf{Y}}).\] (66)

Hence, \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;\vec{\mathbf{Y}}|\vec{\mathbf{X}})=0\). This concludes our proof.

**Lemma 4**.: _Suppose \(\vec{\mathbf{X}}\sim\text{Multinomial}_{d_{X}}(n,\vec{\mathbf{p}}_{X})\) and \(\vec{\mathbf{Y}}|\vec{\mathbf{X}}\sim\text{Multinomial}_{d_{Y}}\left(\sum_{j\in \mathcal{I}}X_{j},\vec{\mathbf{q}}\right)\), where \(\mathcal{I}\subset\{1,\ldots,d_{X}\}\). Then \(\vec{\mathbf{Y}}\sim\text{Multinomial}_{d_{Y}}\left(n,\vec{\mathbf{q}}^{*}\right)\), where_

\[{q_{i}}^{*}=\begin{cases}\left[\sum_{j\in\mathcal{I}}p_{j}\right]\cdot q_{i},& 1\leq i<d_{Y}\\ 1-\left[\sum_{j\in\mathcal{I}}p_{j}\right]\cdot\left[\sum_{j=1}^{d_{Y}-1}q_{j} \right],&i=d_{Y}\end{cases}\,.\] (67)

Proof.: Without loss of generality, reshuffle the class indices of \(\vec{\mathbf{Y}}\) such that \(\mathcal{I}\!=\!\{i\!:\!1\leq i\leq w\}\), where \(|\mathcal{I}|\!=\!w\). Let \(C\!=\!\{\vec{\mathbf{x}}^{\prime}\in\mathbb{N}_{0}^{d_{X}-1}\!:\sum_{i=1}^{d_ {X}-1}x_{i}\leq n\}\). By the law of total probability,

\[P(\vec{\mathbf{Y}}=\vec{\mathbf{y}}) =\sum_{\vec{\mathbf{x}}\in C}P(\vec{\mathbf{X}}=\vec{\mathbf{x}} )P(\vec{\mathbf{Y}}=\vec{\mathbf{y}}|\vec{\mathbf{X}}=\vec{\mathbf{x}})\] (68) \[=\sum_{\vec{\mathbf{x}}\in C}\frac{n!}{\prod_{i=1}^{d_{X}}x_{i}!} \prod_{i=1}^{d_{X}}p_{i}^{x_{i}}\cdot\frac{\left(\sum_{j\in\mathcal{I}}x_{j} \right)!}{\prod_{i=1}^{d_{Y}}y_{i}!}\prod_{i=1}^{d_{Y}}q_{i}^{y_{i}}.\] (69)

Remove from the sum all terms that do not depend on \(\vec{\mathbf{x}}\). Note that \(y_{d_{Y}}\!=\!\sum_{j\in\mathcal{I}}x_{j}-\sum_{i=1}^{d_{Y}-1}y_{i}\) and \(q_{d_{Y}}^{y_{d_{Y}}}\!=\!\left(1-\sum_{i=1}^{d_{Y}}q_{i}\right)^{y_{d_{Y}}}\) are both functions of \(\vec{\mathbf{y}}\). Let \(y_{s}\!=\!\sum_{i=1}^{d_{Y}-1}y_{i}\) and \(q_{s}\!=\!\sum_{i=1}^{d_{Y}-1}q_{i}\).

\[P(\vec{\mathbf{Y}}=\vec{\mathbf{y}})=\left(\frac{n!}{\prod_{i=1}^{d_{Y}-1}y_{ i}!}\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}\right)\sum_{\vec{\mathbf{x}}\in C} \frac{\prod_{i=1}^{d_{X}}p_{i}^{x_{i}}\cdot\left(\sum_{j\in\mathcal{I}}x_{j} \right)!}{\prod_{i=1}^{d_{X}}x_{i}!\left(\sum_{j\in\mathcal{I}}x_{j}-y_{s} \right)!}\left(1-q_{s}\right)^{\sum_{j\in\mathcal{I}}x_{j}-y_{s}}.\]

Form a multinomial coefficient outside the sum:

\[P(\vec{\mathbf{Y}}=\vec{\mathbf{y}})=\left(\frac{n!}{\prod_{i=1} ^{d_{Y}-1}y_{i}!\cdot\left(n-y_{s}\right)!}\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}\right)\] (70) \[\times\sum_{\vec{\mathbf{x}}\in C}\frac{(n-y_{s})!\prod_{i=1}^{d_ {X}}p_{i}^{x_{i}}\cdot\left(\sum_{j\in\mathcal{I}}x_{j}\right)!}{\prod_{i=1}^{d _{X}}x_{i}!\left(\sum_{j\in\mathcal{I}}x_{j}-y_{s}\right)!}\left(1-q_{s}\right) ^{\sum_{j\in\mathcal{I}}x_{j}-y_{s}}.\] (71)

Let \(\mathcal{I}^{0}=\{i\!:\!w<i\leq d_{X}\}\) be the set of indices that are not taken in the number of trials for \(\vec{\mathbf{Y}}\). Consider the sum over \(\vec{\mathbf{x}}\in C\) alone in (71). Separating terms that belong to \(\mathcal{I}\) and \(\mathcal{I}^{0}\),

\[\sum_{\vec{\mathbf{x}}\in C}\frac{(n-y_{s})!}{\prod_{j\in\mathcal{I}^{0}}x_{j}! \left(\sum_{j\in\mathcal{I}}x_{j}-y_{s}\right)!}\prod_{j\in\mathcal{I}^{0}}p_{j }^{x_{j}}\left(1-q_{s}\right)^{\sum_{j\in\mathcal{I}}x_{j}-y_{s}}\cdot\frac{ \left(\sum_{j\in\mathcal{I}}x_{j}\right)!}{\prod_{j\in\mathcal{I}}x_{j}!}\prod_ {j\in\mathcal{I}}p_{j}^{x_{j}}.\]

Let \(C^{\prime}=\{\vec{\mathbf{x}}^{\prime}\in\mathbb{N}_{0}^{d_{X}-1}\!:\sum_{i=1} ^{d_{X}-1}x_{i}-\sum_{i=1}^{d_{Y}-1}y_{i}\leq n-\sum_{i=1}^{d_{Y}-1}y_{i}\}\). Note \(\vec{\mathbf{x}}\in C\Leftrightarrow\vec{\mathbf{x}}\in C^{\prime}\), so we can equivalently sum over elements in \(C^{\prime}\). Perform a change of variable with \(u=\sum_{j\in\mathcal{I}}x_{j}-y_{s}\) and define \(B=\{x_{j}\in\vec{\mathbf{x}}\!:\!j\in\mathcal{I},\sum_{j\in\mathcal{I}}x_{j}=u+ y_{s}\}\). Then the sum becomes

\[=\sum_{\vec{\mathbf{x}}\in C^{\prime}}\frac{(n-y_{s})!}{\prod_{j\in\mathcal{I}^{0}}x _{j}!u!}\prod_{j\in\mathcal{I}^{0}}p_{j}^{x_{j}}\left(1-q_{s}\right)^{u}\sum_{x_{ j}\in B}\binom{u+y_{s}}{x_{j_{1}},...,x_{j_{m}}}\prod_{j\in\mathcal{I}}p_{j}^{x_{j}}.\] (72)

Note that the inner sum simplifies by the multinomial theorem:

\[=\sum_{\vec{\mathbf{x}}\in C^{\prime}}\frac{(n-y_{s})!}{\prod_{j \in\mathcal{I}^{0}}x_{j}!u!}\prod_{j\in\mathcal{I}^{0}}p_{j}^{x_{j}}\left(1-q_{s} \right)^{u}\left(\sum_{j\in\mathcal{I}}p_{j}\right)^{u+y_{s}}\] (73) \[=\left(\sum_{j\in\mathcal{I}}p_{j}\right)^{y_{s}}\sum_{\vec{\mathbf{ x}}\in C^{\prime}}\frac{(n-y_{s})!}{\prod_{j\in\mathcal{I}^{0}}x_{j}!\cdot u!}\prod_{j\in \mathcal{I}^{0}}p_{j}^{x_{j}}\left(\sum_{j\in\mathcal{I}}p_{j}-q_{s}\sum_{j\in \mathcal{I}}p_{j}\right)^{u}.\] (74)Reapply the multinomial theorem to the sum of \(\vec{\mathbf{x}}\) over \(C^{\prime}\):

\[=\left(\sum_{j\in\mathcal{I}}p_{j}\right)^{y_{s}}\left(\sum_{j\in \mathcal{I}^{0}}p_{j}+\sum_{j\in\mathcal{I}}p_{j}-q_{s}\sum_{j\in\mathcal{I}}p_ {j}\right)^{n-y_{s}}\] (75) \[=\left(\sum_{j\in\mathcal{I}}p_{j}\right)^{y_{s}}\left(1-q_{s} \sum_{j\in\mathcal{I}}p_{j}\right)^{n-y_{s}}\.\] (76)

Inserting (76) into (71), the marginal becomes

\[P(\vec{\mathbf{Y}}=\vec{\mathbf{y}}) =\frac{n!\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}}{\prod_{i=1}^{d_{Y}-1} y_{i}!\cdot(n-y_{s})!}\left(\sum_{j\in\mathcal{I}}p_{j}\right)^{y_{s}}\left(1-q _{s}\sum_{j\in\mathcal{I}}p_{j}\right)^{n-y_{s}}\] (77) \[=\frac{n!\prod_{i=1}^{d_{Y}-1}\left(\sum_{j\in\mathcal{I}}p_{j} \cdot q_{i}\right)^{y_{i}}}{\prod_{i=1}^{d_{Y}-1}y_{i}!\cdot(n-y_{s})!}\left( 1-q_{s}\sum_{j\in\mathcal{I}}p_{j}\right)^{n-y_{s}}\] (78) \[=\text{Multinomial}_{d_{Y}}\left(n,\vec{\mathbf{q}}^{*}\right)\] (79)

where \(\vec{\mathbf{q}}^{*}\) is defined as in (67). 

## Appendix F Proof of Theorem 1

In this section, we provide the proof of theorem 1. Since theorem 1 provides the analytical PID terms for the univariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the univariate stable distributions and the corresponding univariate affine continuous stable system for convenience.

### Univariate continuous stable distribution

Univariate continuous stable distribution are a large class of distributions that naturally arise in the context of generalized central limit theorems. We refer the reader to Appx. N.2 for more details on univariate continuous stable distributions. We now list certain key properties of univariate continuous stable distributions that we make use of in the proof of theorem 1:

1. If \(X\) is distributed according to a univariate continuous stable distribution, then the sum of two independent copies of \(X\), denoted as \(X_{1}\) and \(X_{2}\), follows the same univariate continuous stable distribution upto a scaling and translation operation, i.e., \(aX_{1}+bX_{2}\stackrel{{ d}}{{=}}cX+d\) for \(a,b,c>0\) and \(d\in\mathbb{R}\).
2. The p.d.f. of univariate continuous stable distributions is characterized by four parameters: stability parameter denoted as \(\alpha\in(0,2]\), skewness parameter denoted as \(\beta\in[-1,1]\), scale parameters denoted as \(\gamma\in(0,\infty)\), and location parameter denoted as \(\mu\in\mathbb{R}\). We denote the p.d.f. of a univariate continuous stable distribution as \(p_{CS}(\alpha,\beta,\gamma,\mu)\).

### Definition of univariate affine continuous stable system

Let \((M,X,Y)\) be a system of random variables with joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the univariate affine continuous stable system if it satisfies the following two properties:

1. \(M\sim P(M)\) having some support set \(\mathcal{M}\subseteq\mathbb{R}\).
2. The conditional distributions of random variables \(X\) and \(Y\) conditioned on \(M\) can be expressed through univariate continuous stable distributions with an affine dependence on \(M\), i.e., \(P(X|M)\)=\(p_{CS}(\alpha,\beta_{X},\gamma_{X},aM+b)\) and \(P(Y|M)\)=\(p_{CS}(\alpha,\beta_{Y}\text{sgn}(ac),\gamma_{Y},cM+d)\), where \(a,b,c,d\in\mathbb{R}\).

### Formal proof of Theorem 1

We first discuss and provide the provide the proofs of lemma 5 and lemma 6 that we will use for proving theorem 1. In the following proofs, \(\text{sgn}(x)\) denotes the sign function:

\[\text{sgn}(x)=\left\{\begin{array}{cc}1&x>0\\ 0&x=0\\ -1&x<0\end{array}\right..\] (80)

**Lemma 5**.: _If \(X\sim p_{CS}(\alpha,\beta,\gamma,\mu)\), then for any \(\eta\neq 0\) and \(\kappa\in\mathbb{R}\), we have_

\[\eta X+\kappa\sim\left\{\begin{array}{cc}p_{CS}(\alpha,\text{sgn}(\eta) \beta,|\eta|\gamma,\eta\mu+\kappa)&\alpha\neq 1\\ p_{CS}\left(\alpha,\text{sgn}(\eta)\beta,|\eta|\gamma,\eta\mu+\kappa-\nicefrac{{ 2}}{{\pi}}\beta\gamma\eta\log(|\eta|)\right)&\alpha=1\end{array}\right..\]

Proof.: See the proof of proposition 1.4 part (b) in [23]. 

**Lemma 6**.: _If \(X_{1}\sim p_{CS}(\alpha,\beta_{1},\gamma_{1},\mu_{1})\) and \(X_{2}\sim p_{CS}(\alpha,\beta_{2},\gamma_{2},\mu_{2})\) are independent, then \(X_{1}+X_{2}\sim p_{CS}(\alpha,\beta,\gamma,\mu)\) where_

\[\beta=\frac{\beta_{1}\gamma_{1}^{\alpha}+\beta_{2}\gamma_{2}^{\alpha}}{\gamma _{1}^{\alpha}+\gamma_{2}^{\alpha}},\ \ \gamma^{\alpha}=\gamma_{1}^{\alpha}+\gamma_{2}^{\alpha},\text{ and }\mu=\mu_{1}+\mu_{2}.\]

Proof.: See the proof of proposition 1.4 part (c) in [23]. 

**Theorem 1**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes a univariate affine continuous stable system. Without the loss of generality, assume \(\nicefrac{{|a|}}{{\gamma_{X}}}\geq\nicefrac{{|c|}}{{\gamma_{Y}}}\). If \(1-\beta_{Y}\geq\left(\nicefrac{{\gamma_{X}}}{{|c|}}/\nicefrac{{\gamma_{Y}}}{{ |a|}}\right)^{\alpha}\left(1-\beta_{X}\right)\), and \(1+\beta_{Y}\geq\left(\nicefrac{{\gamma_{X}}}{{|c|}}/\nicefrac{{\gamma_{X}}}{{ |a|}}\right)^{\alpha}\left(1+\beta_{X}\right)\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

Proof.: We first note that we can always assume \(\nicefrac{{|a|}}{{\gamma_{X}}}\geq\nicefrac{{|c|}}{{\gamma_{Y}}}\) without the loss of generality because if \(\nicefrac{{|c|}}{{\gamma_{Y}}}\geq\nicefrac{{|a|}}{{\gamma_{X}}}\), then we can always switch our nomenclature to refer to \(Y\) as \(X\), and \(X\) as \(Y\).

We now briefly outline the proof structure. We divide the proof into two cases \(\nicefrac{{|a|}}{{\gamma_{X}}}>\nicefrac{{|c|}}{{\gamma_{Y}}}\) and \(\nicefrac{{|a|}}{{\gamma_{X}}}=\nicefrac{{|c|}}{{\gamma_{Y}}}\). The proof for both cases follows essentially the same structure consisting of two major parts:

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structure \(M\to X\to Y\).
2. In the second part, we show that the \(Q_{MC}(M,X,Y)\) constructed in the first part, lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;Y\backslash X)=0\).

**Case 1: \(\nicefrac{{|a|}}{{\gamma_{X}}}>\nicefrac{{|c|}}{{\gamma_{Y}}}\)**

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

We explicitly construct the desired Markov chain \(M\to X\to Y\) by constructing a larger Markov chain \(M\to X\to X^{\prime}\to Y^{\prime}\to Y\) and then marginalizing the larger Markov chain to obtain the desired Markov chain \(M\to X\to Y\).

Denote the joint distribution of the Markov chain \(M\to X\to X^{\prime}\to Y^{\prime}\to Y\) as \(Q_{MC}(M,X,X^{\prime},Y^{\prime},Y)\). We can decompose \(Q_{MC}(M,X,X^{\prime},Y^{\prime},Y)\) by utilizing its Markovian structure as follows:

\[Q_{MC}(M,X,X^{\prime},Y^{\prime},Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(X^{\prime}|X)Q_ {MC}(Y^{\prime}|X^{\prime})Q_{MC}(Y|Y^{\prime}).\]

Consequently, we can specify/construct the distribution \(Q_{MC}(M,X,X^{\prime},Y^{\prime},Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), \(Q_{MC}(X^{\prime}|X)\), \(Q_{MC}(Y^{\prime}|X^{\prime})\), and \(Q_{MC}(Y|Y^{\prime})\).

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\): We choose \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M)\text{ and }Q_{MC}(X|M)=P(X|M),\] (81)

where \(P(M)\) and \(P(X|M)\) are marginal distributions derived from the original joint distribution \(P(M,X,Y)\) (discussed in the theorem statement) over which the bivariate PID is being calculated.

Specifying \(Q_{MC}(X^{\prime}|X)\): The distribution \(Q_{MC}(X^{\prime}|X)\) is specified by the following deterministic transformation:

\[X^{\prime}=\left\{\begin{array}{cc}\frac{1}{a}X&\alpha\neq 1\\ \frac{1}{a}X+(2\beta_{X}\gamma_{X}\log(\nicefrac{{1}}{{a}}|))/(\pi a)&\alpha=1 \end{array}\right.\] (82)

We will also derive the distribution \(Q(X^{\prime}|M)\) before proceeding with our construction, as we will need it later in the proof to show that the constructed \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

_Deriving \(Q_{MC}(X^{\prime}|M)\)_: Since \(Q_{MC}(X|M=m)=p_{CS}(\alpha,\beta_{X},\gamma_{X},am+b)\) for a fixed \(m\) and \(X^{\prime}\) is scaled and translated version of \(X\), we can use lemma 5 to derive \(Q_{MC}(X^{\prime}|M)\). The exact expression of \(Q_{MC}(X^{\prime}|M=m)\) is provided in (83).

\[Q_{MC}(X^{\prime}|M=m)=p_{CS}(\alpha,\beta_{X^{\prime}},\gamma_{ X^{\prime}},\mu_{X^{\prime}}),\text{ where}\] \[\beta_{X^{\prime}}=\left\{\begin{array}{cc}\text{sgn}(\nicefrac{ {1}}{{a}})\beta_{X}&\alpha\neq 1\\ \text{sgn}(\nicefrac{{1}}{{a}})\beta_{X}&\alpha=1\end{array}\right.=\text{sgn} (\nicefrac{{1}}{{a}})\beta_{X}=\text{sgn}(a)\beta_{X},\] \[\gamma_{X^{\prime}}=\left\{\begin{array}{cc}\nicefrac{{1}}{{a} }|\,\gamma_{X}&\alpha\neq 1\\ \nicefrac{{1}}{{a}}|\,\gamma_{X}&\alpha=1\end{array}\right.=\frac{\gamma_{X}} {|a|},\] \[\mu_{X^{\prime}}=\left\{\begin{array}{cc}\frac{1}{a}(am+b)& \alpha\neq 1\\ \frac{1}{a}(am+b)+\nicefrac{{2\beta_{X}\gamma_{X}\log\left(\nicefrac{{1}}{{a} }|\right)}}{{\pi a}-2\beta_{X}\gamma_{X}\log\left(\nicefrac{{1}}{{a}}\right)} \nicefrac{{1}}{{\pi a}}&\alpha=1\end{array}\right.=m+\frac{b}{a},\] \[\Rightarrow Q_{MC}(X^{\prime}|M=m)=p_{CS}(\alpha,\beta_{X}\text{sgn} (a),\nicefrac{{\gamma_{X}}}{{a}}|,m+\nicefrac{{b}}{{a}}).\] (83)

Specifying \(Q_{MC}(Y^{\prime}|X^{\prime})\): In order to define \(Q_{MC}(Y^{\prime}|X^{\prime})\), we need to define an auxiliary variable \(\epsilon\). The random variable \(\epsilon\sim p_{CS}(\epsilon;\alpha,\beta^{\prime},\gamma^{\prime},\mu^{\prime})\), where

\[\beta^{\prime}=\text{sgn}(a)\frac{\left(\nicefrac{{\gamma_{Y}}}{{|c|}} \right)^{\alpha}\beta_{Y}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha }\beta_{X}}{\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left( \nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}},\ \gamma^{\prime}=\left(\left(\frac{ \gamma_{Y}}{{|c|}}\right)^{\alpha}-\left(\frac{\gamma_{X}}{{|a|}}\right)^{ \alpha}\right)^{\frac{1}{\alpha}},\ \mu^{\prime}=\frac{d}{c}-\frac{b}{a}.\] (84)

In order for \(\epsilon\) to have a legitimate continuous stable distribution, we need to ensure that \(\beta^{\prime}\), \(\gamma^{\prime}\), and \(\mu^{\prime}\) lie within their appropriate bounds as specified in Appx. F.1. It is trivial to see that \(\mu^{\prime}\in\mathbb{R}\).

Showing \(\gamma^{\prime}\in(0,\infty)\). Under the assumption of the case 1, we know that:

\[\frac{|a|}{\gamma_{X}}>\frac{|c|}{\gamma_{Y}}.\]

Manipulating the above inequality, we obtain:

\[\Rightarrow \left(\left(\frac{\gamma_{Y}}{{|c|}}\right)^{\alpha}-\left( \frac{\gamma_{X}}{{|a|}}\right)^{\alpha}\right)^{\frac{1}{\alpha}}=\gamma^{ \prime}>0.\] (85)

Hence, (85) shows that \(\gamma^{\prime}\in(0,\infty)\).

Lastly, showing that \(\beta^{\prime}\in[-1,1]\). Note that the condition \(\beta^{\prime}\in[-1,1]\) can be equivalently expressed as \(|\beta^{\prime}|\leq 1\). For showing that \(\beta^{\prime}\in[-1,1]\), we will show that the inequality \(|\beta^{\prime}|\leq 1\) is equivalent to the inequalities: \(\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{1}}{{|a|\gamma_{Y}}}\right) ^{\alpha}(1-\beta_{X})\leq 1-\beta_{Y}\) and \((1+\beta_{X})\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{1}}{{|a|\gamma_{Y} }}\right)^{\alpha}\leq 1+\beta_{Y}\), where the last two inequalities hold by the theorem assumptions:

\[|\beta^{\prime}|\leq 1\Rightarrow\left|\text{sgn}(a)\frac{\left( \nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}\beta_{Y}-\left(\nicefrac{{\gamma_ {X}}}{{|a|}}\right)^{\alpha}\beta_{X}}{\left(\nicefrac{{\gamma_{Y}}}{{|c|}} \right)^{\alpha}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}}\right| \leq 1\Rightarrow\left|\frac{\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{ \alpha}\beta_{Y}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X} }{\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_ {X}}}{{|a|}}\right)^{\alpha}}\right|\leq 1,\] \[\Rightarrow -1\leq\frac{\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha} \beta_{Y}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X}}{ \left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_{X}}} {{|a|}}\right)^{\alpha}}\leq 1.\] (86)

Multiplying \(\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_{ X}}}{{|a|}}\right)^{\alpha}\) on both sides of the inequalities in (86).

\[\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_{Y}}}{{|c|}} \right)^{\alpha}\leq\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha} \beta_{Y}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X}\leq \left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_{X}}} {{|a|}}\right)^{\alpha}.\] (87)Adding \(\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X}\) on both sides of the inequalities in (87).

\[\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}+\left(\nicefrac{{ \gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X}-\left(\nicefrac{{\gamma_{Y}}}{{|c| }}\right)^{\alpha}\leq\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha} \beta_{Y}\leq\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}-\left( \nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}+\left(\nicefrac{{\gamma_{X}}}{ {|a|}}\right)^{\alpha}\beta_{X},\] \[\Rightarrow \left(1+\beta_{X}\right)\left(\nicefrac{{\gamma_{X}}}{{|a|}} \right)^{\alpha}-\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}\leq \left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}\beta_{Y}\leq\left( \nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}+\left(\nicefrac{{\gamma_{X}}}{ {|a|}}\right)^{\alpha}(\beta_{X}-1).\] (88)

Dividing \(\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}\) on both sides of the inequalities in (88).

\[\left(1+\beta_{X}\right)\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{{|a| }}}{{|\gamma_{Y}}}\right)^{\alpha}-1\leq\beta_{Y}\leq 1+\left(\nicefrac{{ \gamma_{X}}}{{|c|}}\nicefrac{{{|a|}}}{{|\gamma_{Y}}}\right)^{\alpha}(\beta_{X} -1).\] (89)

Simplifying the inequalities in (89).

\[\left(1+\beta_{X}\right)\left(\nicefrac{{\gamma_{X}}}{{|c|}} \nicefrac{{{|a|}}}{{|\gamma_{Y}}}\right)^{\alpha}\leq 1+\beta_{Y},\] (90) \[\beta_{Y}-1\leq\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{{|a| }}}{{|\gamma_{Y}}}\right)^{\alpha}(\beta_{X}-1).\] (91)

Multiplying \(-1\) on both sides in (91):

\[\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{{|a|}}}{{|\gamma_{Y}}}\right) ^{\alpha}(1-\beta_{X})\leq 1-\beta_{Y}.\] (92)

Hence, by above analysis we can conclude that the inequality \(|\beta^{\prime}|\leq 1\) is equivalent to the inequalities: \(\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{{|a|}}}{{|\gamma_{Y}}}\right) ^{\alpha}(1-\beta_{X})\leq 1-\beta_{Y}\) and \(\left(1+\beta_{X}\right)\left(\nicefrac{{\gamma_{X}}}{{|c|}}\nicefrac{{{|a|}} }{{|\gamma_{Y}}}\right)^{\alpha}\leq 1+\beta_{Y}\), which ensures \(\beta^{\prime}\in[-1,1]\). As \(\beta^{\prime}\in[-1,1]\), \(\gamma^{\prime}\in(0,\infty)\) and \(\mu^{\prime}\in\mathbb{R}\), we can conclude \(\epsilon\) follows a legitimate univariate continuous stable distribution. Furthermore, we define \(\epsilon\perp\!\!\perp(M,X,X^{\prime})\), i.e., \(\Pr(\epsilon|M,X,X^{\prime})=\Pr(\epsilon)\) and \(\Pr(M,X,X^{\prime}|\epsilon)=\Pr(M,X,X^{\prime})\).

We now use \(\epsilon\) to specify \(Q_{MC}(Y^{\prime}|X^{\prime})\) as follows:

\[Y^{\prime}=X^{\prime}+\epsilon.\] (93)

We also derive the distribution \(Q(Y^{\prime}|M)\) before proceeding with our construction, as we will need it later in the proof to show that the constructed \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

_Deriving \(Q_{MC}(Y^{\prime}|M)\)_: We will use the distributions \(Q_{MC}(X^{\prime}|M)\) and \(Q_{MC}(\epsilon)\) to derive the distribution of \(Q_{MC}(Y^{\prime}|M)\):

\[Q_{MC}(Y^{\prime}\!=\!y^{\prime}|M\!=\!m) =Q_{MC}(X^{\prime}+\epsilon\!=\!y^{\prime}|M\!=\!m),\] \[=\int_{k=-\infty}^{\infty}Q_{MC}(X^{\prime}\!=\!k,\epsilon\!=\!y^ {\prime}-k|M\!=\!m)dk,\] \[=\int_{k=-\infty}^{\infty}Q_{MC}(\epsilon=y^{\prime}\!-\!k|M\!=\!m )Q_{MC}(X^{\prime}\!=\!k|\epsilon=y^{\prime}\!-\!k,M\!=\!m)dk.\]

Using the fact that \(\epsilon\perp\!\!\perp M\) and \(\epsilon\perp\!\!\perp X^{\prime}|M\), we have:

\[Q_{MC}(Y^{\prime}=y^{\prime}|M=m)=\int_{k=-\infty}^{\infty}Q_{MC}(\epsilon=y^{ \prime}\!-\!k)Q_{MC}(X^{\prime}\!=\!k|M\!=\!m)dk.\] (94)

Since \(Q_{MC}(\epsilon)\) and \(Q_{MC}(X^{\prime}|M=m)\) are univariate continuous stable distributions for a fixed \(m\) (see (84) and (83)), equation (94) describes a convolution of two univariate stable distributions for a fixed \(m\). Convolution of two univariate continuous stable distribution is akin to summing two independent random variable having the same univariate continuous stable distributions, and consequently \(Q_{MC}(Y^{\prime}|M)\) can be derived using lemma 6. Hence,

\[Q_{MC}(Y^{\prime}|M=m)=p_{CS}(\alpha,\tilde{\beta},\tilde{\gamma}, \tilde{\mu}),\text{ where}\] \[\tilde{\mu}=m+\frac{b}{a}+\mu^{\prime}=m+\frac{b}{a}+\frac{d}{c}- \frac{b}{a}=m+\frac{d}{c},\] \[\tilde{\gamma}=\left(\left(\frac{\gamma_{X}}{|a|}\right)^{\alpha }+\gamma^{\prime\alpha}\right)^{\frac{1}{\alpha}}=\left(\left(\frac{\gamma_{X} }{|a|}\right)^{\alpha}+\left(\frac{\gamma_{Y}}{|c|}\right)^{\alpha}-\left( \frac{\gamma_{X}}{|a|}\right)^{\alpha}\right)^{\frac{1}{\alpha}}=\frac{\gamma_ {Y}}{|c|},\] \[\tilde{\beta}=\frac{\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^ {\alpha}\text{sgn}(a)\beta_{X}+\gamma^{\prime\alpha}\beta^{\prime}}{\left( \nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}+\gamma^{\prime\alpha}}\] \[=\frac{\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha} \text{sgn}(a)\beta_{X}+\left(\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{ \alpha}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\right)\left( \text{sgn}(a)\frac{\left(\nicefrac{{\gamma_{Y}}}{{|c|}}\right)^{\alpha}\beta_{ Y}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}\beta_{X}}{\left( \nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}+\left(\nicefrac{{\gamma_{Y}}}{ {|c|}}\right)^{\alpha}-\left(\nicefrac{{\gamma_{X}}}{{|a|}}\right)^{\alpha}}\right)}\] \[=\text{sgn}(a)\beta_{Y}.\] (95)

Specifying \(Q_{MC}(Y|Y^{\prime})\): \(Q_{MC}(Y|Y^{\prime})\) is specified by the following deterministic transformation:

\[Y=\left\{\begin{array}{cc}cY^{\prime}&\alpha\neq 1\\ cY^{\prime}+(2\text{sgn}(a)c\beta_{Y}\gamma_{Y}\log(|c|))/\pi&\alpha=1\end{array} \right..\] (96)

Finally, we construct the desired \(Q_{MC}(M,X,Y)\) from \(Q_{MC}(M,X,X^{\prime},Y^{\prime},Y)\) by marginalizing \(X^{\prime}\) and \(Y^{\prime}\).

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\).

Calculating \(Q(Y|M)\): We use lemma 5 to calculate \(Q(Y|M)\) as from (95), we know that \(\overline{Q_{MC}(Y^{\prime}|M)=p_{CS}(Y^{\prime};\alpha,\text{sgn}(a)\beta_{Y },\nicefrac{{\gamma_{Y}}}{{|c|}},M+\nicefrac{{d}}{{c}})}\) is a continuous stable distribution for a fixed \(m\), and \(Y\) a translated and scaled version of \(Y^{\prime}\). The exact expression of \(Q_{MC}(Y|M=m)\) is provided in (97).

\[Q_{MC}(Y|M=m)=p_{CS}(\alpha,\tilde{\beta}_{Y},\tilde{\gamma}_{Y}, \tilde{\mu}_{Y}),\text{ where}\] \[\tilde{\beta}_{Y}=\left\{\begin{array}{cc}\text{sgn}(c)\text{ sgn}(c)\beta_{Y}&\alpha\neq 1\\ \text{sgn}(c)\text{sgn}(a)\beta_{Y}&\alpha=1\end{array}\right.=\text{sgn}(a) \text{sgn}(c)\beta_{Y}=\text{sgn}(ac)\beta_{Y},\] \[\tilde{\gamma_{Y}}=\left\{\begin{array}{cc}|c|\frac{\gamma_{Y} }{|c|}&\alpha\neq 1\\ |c|\frac{\gamma_{Y}}{|c|}&\alpha=1\end{array}\right.=\gamma_{Y},\] \[\tilde{\mu}_{Y}=\left\{\begin{array}{cc}c(m+\frac{d}{c})&\alpha \neq 1\\ c(m+\frac{d}{c})+2\text{sgn}(a)c\beta_{Y}\gamma_{Y}\log(|c|)/\pi&\alpha=1\end{array} \right.=cm+d,\] \[\Rightarrow Q_{MC}(Y|M=m)=p_{CS}(\alpha,\beta_{Y}\text{sgn}(ac), \gamma_{Y},cm+d)=P(Y|M).\] (97)

From (81) and (97), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (98) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (99)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\), concluding our proof for case 1.

**Case 2: \(\nicefrac{{|a|}}{{\gamma_{X}}}=\nicefrac{{|c|}}{{\gamma_{Y}}}\)**

The proof of case 2 is extremely similar to case 1. For case 2, we are able to directly construct the distribution \(Q_{MC}(M,X,Y)\) without the need of specifying a larger Markov chain.

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

Similarly to case 1, we specify \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\) and \(Q_{MC}(Y|X)\). The distributions \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) are as follows:

\[Q_{MC}(M)=P(M)\text{ and }Q_{MC}(X|M)=P(X|M).\] (100)The distribution \(Q_{MC}(Y|X)\) is defined using the following deterministic transformation:

\[Y=\left\{\begin{array}{ll}\nicefrac{{c}}{{a}}X-\nicefrac{{cb}}{{a}}+d&\alpha \neq 1\\ \frac{c}{a}X+\nicefrac{{2c}}{{a\pi}}\beta\gamma_{X}\log\left(\nicefrac{{|\varphi _{a}|}}{{a}}\right)-\nicefrac{{cb}}{{a}}+d&\alpha=1\end{array}\right..\] (101)

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to calculate \(Q_{MC}(Y|M)\).

Calculating \(Q(Y|M)\): We use lemma 5 to calculate \(Q(Y|M)\) as \(Q_{MC}(X|M\!=\!m)\) is a continuous stable distribution for a fixed \(m\), and \(Y\) a translated and scaled version of \(X\). The exact expression of \(Q_{MC}(Y|M=m)\) is provided in (102).

\[Q_{MC}(Y|M=m)=p_{CS}(y;\alpha,\tilde{\beta},\tilde{\gamma},\tilde {\mu}),\] \[\tilde{\mu}=\left\{\begin{array}{ll}\nicefrac{{ca}}{{a}}m+ \frac{cb}{a}-\frac{bc}{a}+d&\alpha\neq 1\\ \nicefrac{{ca}}{{a}}m+\frac{cb}{a}-\frac{bc}{a}+d-\nicefrac{{2c}}{{a\pi}}\beta \gamma_{X}\log\left(\nicefrac{{|\varphi_{a}|}}{{a}}\right)+\nicefrac{{2c}}{{a \pi}}\beta\gamma_{X}\log\left(\nicefrac{{|\varphi_{a}|}}{{a}}\right)&\alpha=1 \end{array}\right.\] \[=cm+d,\] \[\tilde{\gamma}=|c|\nicefrac{{\gamma_{X}}}{{a}}|=|c|\nicefrac{{ \gamma_{Y}}}{{|c|}}=\gamma_{Y},\text{( using the fact that }\nicefrac{{\gamma_{X}}}{{a}}|=\nicefrac{{\gamma_{Y}}}{{|c|}}\text{ by assumption)}\] \[\tilde{\beta}=\beta\text{sgn}(\nicefrac{{c}}{{a}})=\beta\text{sgn} (ac),\] \[\Rightarrow Q_{MC}(Y|M=m)=p_{CS}(\alpha,\text{sgn}(ac)\beta_{Y},\gamma_{Y}, cm+d)=P(Y|M).\] (102)

From (100) and (102), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (103) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (104)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\), concluding our proof for case 2 and theorem 1. \(\square\)

## Appendix G Proof of Theorem 2

In this section, we provide the proof of theorem 2. Since theorem 2 provides the analytical PID terms for system 1 of the multivariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the corresponding independent component multivariate stable distributions and the corresponding multivariate affine continuous stable system for convenience.

### Independent component multivariate stable distribution

Independent component multivariate stable distributions are a specific multivariate generalization of univariate continuous stable distributions describing a collection of independent random variables, with each random variable distributed according to a univariate continuous stable distribution. We refer the reader to Appx. N.3 for more details on multivariate continuous stable distributions. We now list certain key properties of independent component multivariate stable distributions that we make use of in the proof of theorem 2:

1. If \(\vec{\mathbf{X}}\) is distributed according to an independent component multivariate stable distribution, then the sum of two independent copies of \(\vec{\mathbf{X}}\), denoted as \(\vec{\mathbf{X}}_{1}\) and \(\vec{\mathbf{X}}_{2}\), follows an independent component multivariate stable distribution up to a scaling and translation operation, i.e., \(a\vec{\mathbf{X}}_{1}+b\vec{\mathbf{X}}_{2}\overset{d}{=}c\vec{\mathbf{X}}+ \vec{\mathbf{d}}\) for \(a,b,c>0\) and \(\vec{\mathbf{d}}\in\mathbb{R}\).
2. We denote the p.d.f. of the independent component multivariate stable continuous distribution as \(p_{CS-IC}(\alpha,\vec{\mathbf{\beta}},\vec{\mathbf{\gamma}},\vec{\mathbf{\mu}})\), where \[\vec{\mathbf{\beta}}=\left[\beta_{1}\quad\dots\quad\beta_{d}\right]^{T},\text { with }\beta_{j}\in[-1,1],\vec{\mathbf{\gamma}}=\left[\gamma_{1}\quad\dots\quad\gamma_{ d}\right]^{T}\text{ with }\gamma_{j}\in(0,\infty),\] \[\vec{\mathbf{\mu}}=\left[\mu_{1}\quad\dots\quad\mu_{d}\right]^{T}\in \mathbb{R}^{d},\text{ and }\alpha\in(0,2].\] In general, the p.d.f. of independent component multivariate stable distribution do not have a closed-form analytical expression, and are expressed through their characteristic function.

3. The characteristic function of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) having independent component multivariate distribution \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\) is expressed as follows: \[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}}\right]=\exp \left(-\sum_{j=1}^{d}|\gamma_{j}t_{j}|^{\alpha}(1-i\beta_{j}\text{sgn}(t_{j}) \Phi(\alpha))+i\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\mu}}\right)\;\forall\;\vec {\mathbf{t}}\in\mathbb{R}^{d},\] (105) where \(\Phi(\alpha)=\left\{\begin{array}{cc}\tan\left(\frac{\pi\alpha}{2}\right)& \alpha\neq 1\\ \frac{-2}{\pi}\log(|t|)&\alpha=1\end{array}\right.\), \(\text{sgn}(t)=\left\{\begin{array}{cc}-1&t<0\\ 0&t=0\\ 1&t>0\end{array}\right..\)
4. The random vector \(\vec{\mathbf{X}}=\left[X_{1}\;\;\ldots\;\;X_{d}\right]^{T}\) having the distribution \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\) is essentially a collection of independent random variables \(\{X_{j}\}_{j=1}^{d}\), where \(X_{j}\sim p_{CS}(\alpha,\beta_{j},\gamma_{j},\mu_{j})\).

### Definition of the system 1 of the multivariate affine continuous stable system

Let the random variable \(M\sim P(M)\) with support \(\mathcal{M}\subseteq\mathbb{R}\). \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) are \(d_{X}\)-dimensional and \(d_{Y}\)-dimensional random vectors, respectively. The joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) describes system 1 of the multivariate affine continuous stable system if it satisfies the following two properties

1. \(M\sim P(M)\) having some support set \(\mathcal{M}\subseteq\mathbb{R}\).
2. The conditional distributions of random variables \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) conditioned on \(M\) are expressed as follows: \[\vec{\mathbf{X}}\!=\!\vec{\mathbf{H}}_{X}M+\mathbf{A}_{X}\vec{\mathbf{Z}}_{X}+ \vec{\mathbf{b}}_{X}\text{ and }\vec{\mathbf{Y}}\!=\!\vec{\mathbf{H}}_{Y}M+\mathbf{A}_{Y}\vec{ \mathbf{Z}}_{Y}+\vec{\mathbf{b}}_{Y}^{\ast},\] (106) where \(\vec{\mathbf{Z}}_{X}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{X}},\vec{ \mathbf{1}}_{d_{X}},\vec{\mathbf{0}}_{d_{X}})\), \(\vec{\mathbf{Z}}_{Y}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{Y}},\vec{ \mathbf{1}}_{d_{Y}},\vec{\mathbf{0}}_{d_{Y}})\), \(\mathbf{A}_{X}\) and \(\mathbf{A}_{Y}\) are invertible matrices, \(\vec{\mathbf{H}}_{X},\vec{\mathbf{b}}_{X}\in\mathbb{R}^{d_{X}}\), and \(\vec{\mathbf{H}}_{Y},\vec{\mathbf{b}}_{Y}\in\mathbb{R}^{d_{Y}}\).

### Formal proof of Theorem 2

**Theorem 2**.: _Let the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\) describe the system 1 of the multivariate affine continuous stable system. Without the loss of generality, assume \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\leq\|\mathbf{A}_{X}^{-1 }\vec{\mathbf{H}}_{X}\|_{\kappa}\), where \(\kappa=\nicefrac{{\alpha}}{{\alpha-1}}\;\forall\;\alpha\in(1,2]\) and \(\kappa=\infty\;\forall\;\alpha\in(0,1]\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) and \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\)._

Proof.: We first note that we can always assume \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\leq\|\mathbf{A}_{X}^{-1 }\vec{\mathbf{H}}_{X}\|_{\kappa}\) without the loss of generality because if \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\geq\|\mathbf{A}_{X}^{-1 }\vec{\mathbf{H}}_{X}\|_{\kappa}\), then we can always switch our nomenclature to refer to \(\vec{\mathbf{Y}}\) as \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{X}}\) as \(\vec{\mathbf{Y}}\).

The proof of above theorem relies on the result of lemma 7 and the fact that the linear system described in (106) can always be reduced to the special case of the linear system used in lemma 7. We briefly outline the proof structure.

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) having the Markovian structure \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\).
2. In the second part, we show that the \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) constructed in the first part, lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\).

**Part 1: Specifying \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\)**

We explicitly construct the desired Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) by constructing a larger Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{\prime}\rightarrow \vec{\mathbf{Y}}^{\prime}\rightarrow\vec{\mathbf{Y}}\) and then marginalizing the larger Markov chain to obtain the desired Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\).

Denote the joint distribution of the Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{\prime}\rightarrow\vec {\mathbf{Y}}^{\prime}\rightarrow\vec{\mathbf{Y}}\) as \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}}^{\prime}, \vec{\mathbf{Y}})\). We can decompose \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}}^{\prime},\vec{\mathbf{Y}})\) by utilizing its Markovian structure as follows:

\[Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}}^{\prime},\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{X}}|M)Q_{MC}(\vec{\mathbf{X}}^ {\prime}|\vec{\mathbf{X}})Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{ \prime})Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime}).\]

Consequently, we can specify/construct the distribution \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}}^{\prime},\vec{\mathbf{Y}})\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(\vec{\mathbf{X}}|M)\), \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})\), \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime})\), and \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{Y}}^{\prime})\). We specify \(Q_{MC}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}})\) and \(Q(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})\) using the following deterministic transformation:

\[\vec{\mathbf{X}}^{\prime}=\mathbf{A}_{X}^{-1}\left(\vec{\mathbf{X}}-\vec{ \mathbf{b}}_{X}\right).\] (107)

Substituting (106) in the above equation:

\[\vec{\mathbf{X}}^{\prime}=\mathbf{A}_{X}^{-1}\left(\vec{\mathbf{H}}_{X}M+ \mathbf{A}_{X}\vec{\mathbf{Z}}_{X}+\vec{\mathbf{b}}_{X}-\vec{\mathbf{b}}_{X} \right)=\mathbf{A}_{X}^{-1}\vec{\mathbf{H}}_{X}M+\vec{\mathbf{Z}}_{X}.\] (108)

Specifying \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime})\): We define \(\vec{\mathbf{Y}}^{\prime}\) according to the following affine system:

\[\vec{\mathbf{Y}}^{\prime}=\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}M+\vec{ \mathbf{Z}}_{Y}.\] (109)

Observe that both \(\vec{\mathbf{X}}^{\prime}\) and \(\vec{\mathbf{Y}}^{\prime}\) can be described as an affine function of \(M\) satisfying the special structure described in lemma 7 (see (115)). Furthermore, \(Q_{MC}(M)=P(M)\) by construction. Consequently, \(Q_{MC}(M)\) satisfies the properties required by the distribution of \(M\) outlined in lemma 7, as \(P(M)\) in both lemma 7 and theorem 2 follow the same properties. Since \(\|\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\|_{\kappa}\leq\|\mathbf{A}_{X}^{-1} \vec{\mathbf{H}}_{X}\|_{\kappa}\) by the assumption in the theorem, we can apply the result of lemma 7 to construct \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime})\), where \(\vec{\mathbf{H}}_{X}\) and \(\vec{\mathbf{H}}_{Y}\) would be replaced by \(\mathbf{A}_{X}^{-1}\vec{\mathbf{H}}_{X}\) and \(\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}\), respectively.

Lastly, we specify \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{Y}}^{\prime})\) through the following linear system:

\[\vec{\mathbf{Y}}=\mathbf{A}_{Y}\vec{\mathbf{Y}}^{\prime}+\vec{\mathbf{b}}_{Y}.\] (110)

Finally, we construct the desired \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) from \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}}^{\prime},\vec{\mathbf{Y}})\) by marginalizing \(\vec{\mathbf{X}}^{\prime}\) and \(\vec{\mathbf{Y}}^{\prime}\).

**Part 2: Showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\), we first need to derive \(Q_{MC}(\vec{\mathbf{Y}}|M)\).

_Deriving \(Q(\vec{\mathbf{Y}}|M)\)_: Substituting (109) in (110) we obtain:

\[\vec{\mathbf{Y}}=\mathbf{A}_{Y}(\mathbf{A}_{Y}^{-1}\vec{\mathbf{H}}_{Y}M+ \vec{\mathbf{Z}}_{Y})+\vec{\mathbf{b}}_{Y}=\vec{\mathbf{H}}_{Y}M+\mathbf{A}_ {Y}\vec{\mathbf{Z}}_{Y}+\vec{\mathbf{b}}_{Y}.\] (111)

Inspecting the above equation, we can conclude:

\[Q_{MC}(\vec{\mathbf{Y}}|M)=P(\vec{\mathbf{Y}}|M).\] (112)

From (112), we can conclude:

\[Q_{MC}(M,\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{Y}}|M)=P(M)P(\vec{ \mathbf{Y}}|M)=P(M,\vec{\mathbf{Y}}).\] (113)

We know that by construction:

\[Q_{MC}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}}).\] (114)

Hence, \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\), concluding our proof. 

**Lemma 7**.: _Let the joint distribution of \(M\), \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\), denoted as \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\), describe the system 1 of the multivariate affine continuous stable system. Furthermore, assume \(\mathbf{A}_{X}=\mathbf{I}_{d_{X}}\), \(\mathbf{A}_{Y}=\mathbf{I}_{d_{Y}}\), \(\vec{\mathbf{b}}_{X}=\vec{\mathbf{d}}_{d_{X}}\) and \(\vec{\mathbf{b}}_{Y}=\vec{\mathbf{d}}_{d_{Y}}\), where \(\mathbf{I}_{d}\) is a \(d\times d\) identity matrix and \(\vec{\mathbf{d}}_{d}\) is \(d\)-dimensional vector of zeros. Then,_1. _If_ \(||\vec{\mathbf{H}}_{Y}\|_{\kappa}\leq||\vec{\mathbf{H}}_{X}\|_{\kappa}\)_, where_ \(\kappa=\nicefrac{{\alpha}}{{\alpha-1}}\;\forall\;\alpha\in(1,2]\) _and_ \(\kappa=\infty\;\forall\;\alpha\in(0,1]\)_, then_ \(\Delta_{P}\) _contains a Markov chain of the form_ \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) _and_ \(UI(M;\vec{\mathbf{Y}}/\vec{\mathbf{X}})=0\)_._
2. _If_ \(||\vec{\mathbf{H}}_{X}\|_{\kappa}\leq||\vec{\mathbf{H}}_{Y}\|_{\kappa}\)_, where_ \(\kappa=\nicefrac{{\alpha}}{{\alpha-1}}\;\forall\;\alpha\in(1,2]\) _and_ \(\kappa=\infty\;\forall\;\alpha\in(0,1]\)_, then_ \(\Delta_{P}\) _contains a Markov chain of the form_ \(M\rightarrow\vec{\mathbf{Y}}\rightarrow\vec{\mathbf{X}}\) _and_ \(UI(M;\vec{\mathbf{X}}/\vec{\mathbf{Y}})=0\)_._

Proof.: We only provide an explicit proof of condition 1. Condition 2 is essentially the same as condition 1, with the parameters of X and Y switched. Consequently, the proof of condition 2 follows the same steps as the proof of condition 1, with parameters about X and Y switched.

**Proof of condition 1**:

We briefly outline the proof structure.

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) having the Markovian structure \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\).
2. In the second part, we show that the \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) constructed in the first part, lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\).

**Part 1: Specifying \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\)**

We explicitly construct the desired Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) by constructing a larger Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{\prime}\rightarrow \vec{\mathbf{Y}}\) and then marginalizing the larger Markov chain to obtain the desired Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\).

Denote the joint distribution of the Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{\prime}\rightarrow \vec{\mathbf{Y}}\) as \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}})\). We can decompose \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}})\) by utilizing its Markovian structure as follows:

\[Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}})=Q_{MC}( M)Q_{MC}(\vec{\mathbf{X}}|M)Q_{MC}(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})Q_{MC}( \vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime}).\]

Consequently, we can specify/construct the distribution \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}})\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(\vec{\mathbf{X}}|M)\), \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})\), and \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime})\).

Under the assumptions of lemma 7, we can simplify the linear system describing \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) shown in (106) as follows:

\[\vec{\mathbf{X}}\!\!=\!\vec{\mathbf{H}}_{X}M+\vec{\mathbf{Z}}_{X}\text{ and }\vec{\mathbf{Y}}\!\!=\!\vec{\mathbf{H}}_{Y}M+\vec{\mathbf{Z}}_{Y}.\] (115)

Correspondingly, we can write the characteristic functions of conditional distributions \(P(\vec{\mathbf{X}}|M=m)\) and \(P(\vec{\mathbf{Y}}|M=m)\) for a fixed \(m\) by employing lemma 9:

\[\mathbb{E}[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}}|M\!=\!m] =\exp\left(-\sum_{j=1}^{n}|t_{j}|^{\alpha}+i\vec{\mathbf{t}}^{T} \vec{\mathbf{H}}_{X}m\right),\] \[\mathbb{E}[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{Y}}}|M\!=\!m] =\exp\left(-\sum_{j=1}^{n}|t_{j}|^{\alpha}+i\vec{\mathbf{t}}^{T} \vec{\mathbf{H}}_{Y}m\right),\] (116)

where the characteristic function can be derived by realizing that for a given \(m\), \(\vec{\mathbf{H}}_{X}m\) and \(\vec{\mathbf{H}}_{Y}m\) are constants being added to the random vectors \(\vec{\mathbf{Z}}_{X}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{x}},\vec{ \mathbf{1}}_{d_{x}},\vec{\mathbf{0}}_{d_{x}})\) and \(\vec{\mathbf{Z}}_{Y}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d_{y}},\vec{ \mathbf{1}}_{d_{y}},\vec{\mathbf{0}}_{d_{y}})\), respectively, and then using the result of lemma 9.

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(\vec{\mathbf{X}}|M)\): We specify \(Q_{MC}(M)\) and \(Q_{MC}(\vec{\mathbf{X}}|M)\) as follows:

\[Q_{MC}(M)=P(M)\text{ and }\;Q_{MC}(\vec{\mathbf{X}}|M)=P(\vec{\mathbf{X}}|M),\] (117)

where \(P(M)\) and \(P(\vec{\mathbf{X}}|M)\) are marginal distributions derived from the original joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) (discussed in the lemma statement) over which the bivariate PID is being calculated.

Specifying \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})\): We specify \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|\vec{\mathbf{X}})\) through the following deterministic transformation:

\[\vec{\mathbf{X}}^{\prime}=\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T} \vec{\mathbf{X}},\] (118)

where \(\vec{\mathbf{H}}_{X}^{k}\) is as defined in lemma 10, and \(k\in[0,\infty]\).

We will also derive the distribution \(Q(\vec{\mathbf{X}}^{\prime}|M)\) before proceeding with our construction, as we will need it later in the proof to show that the constructed \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\).

_Deriving \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|M)\)_: To derive \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|M)\), we represent \(\vec{\mathbf{X}}^{\prime}\) as function of \(M\) and \(\vec{\mathbf{Z}}_{X}\) in a similar manner to \(\vec{\mathbf{X}}\) in (115), and then use the result of lemma 9 to derive its conditional characteristic function \(\mathbb{E}[e^{\mathbf{\Gamma}^{T}\vec{\mathbf{X}}^{\prime}}|M]\), and correspondingly the desired \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|M)\).

We multiply \(\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\) on both sides of equality in (115) to obtain the following linear equation:

\[\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{\mathbf{X}}=\vec{ \mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}(\vec{\mathbf{H}}_{X}M+\vec{ \mathbf{Z}}_{X})=\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{ \mathbf{H}}_{X}M+\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{ \mathbf{Z}}_{X}.\]

Substituting (118) in the above equation, we obtain:

\[\vec{\mathbf{X}}^{\prime}=\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T} \vec{\mathbf{X}}=\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{ \mathbf{H}}_{X}M+\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{ \mathbf{Z}}_{X}.\] (119)

Using the result of lemma 10, i.e., \(\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\vec{\mathbf{H}}_{X}=\vec{ \mathbf{H}}_{Y}\), in the above equation we obtain:

\[\vec{\mathbf{X}}^{\prime}=\vec{\mathbf{H}}_{Y}M+\vec{\mathbf{H}}_{Y}(\vec{ \mathbf{H}}_{X}^{k})^{T}\vec{\mathbf{Z}}_{X}.\] (120)

From (120), we can observe that \(\vec{\mathbf{X}}^{\prime}\) is an affine function of \(\vec{\mathbf{Z}}_{X}\) for a fixed \(m\). Hence, we can employ the result of lemma 9 to derive the distribution of \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|M)\) (by deriving its corresponding characteristic function).

Note that the \(j\)-th column of \(\vec{\mathbf{H}}_{Y}(\vec{\mathbf{H}}_{X}^{k})^{T}\) can be expressed as \(h_{j}^{\prime}\vec{\mathbf{H}}_{Y}\), where \(h_{j}^{\prime}\) is the \(j\)-th component of \(\vec{\mathbf{H}}_{X}^{k}\). Consequently:

\[\mathbb{E}\left[\left.e^{i\mathbf{\Gamma}^{T}\vec{\mathbf{X}}^{ \prime}}\right|M=m\right] =\exp\left(-\sum_{j=1}^{d_{X}}\left|h_{j}^{\prime}\vec{\mathbf{H}} _{Y}^{T}\mathbf{\vec{\mathbf{t}}}\right|^{\alpha}+i\vec{\mathbf{\Gamma}}^{T} \vec{\mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\sum_{j=1}^{d_{X}}\left|h_{j}^{\prime}\right|^{ \alpha}\left|\vec{\mathbf{H}}_{Y}^{T}\mathbf{\vec{\mathbf{t}}}\right|^{\alpha }+i\vec{\mathbf{\Gamma}}^{T}\vec{\mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\left|\vec{\mathbf{H}}_{Y}^{T}\mathbf{\vec{\mathbf{t}} }\right|^{\alpha}\left(\sum_{j=1}^{d_{X}}\left|h_{j}^{\prime}\right|^{\alpha} \right)+i\vec{\mathbf{\Gamma}}^{T}\vec{\mathbf{H}}_{Y}m\right).\] (121)

We substitute the value \(h_{j}^{\prime}\) in (121) using the definition of \(\vec{\mathbf{H}}_{X}^{k}\) given in lemma 10. We will divide the substitution into two cases:

Case 1: \(k\) is finite, i.e., \(k\in[0,\infty)\). Then, we have \(h_{j}^{\prime}=\left|h_{j}^{\prime\,k}\right|^{k_{\lambda}}\!\!\left|/\|\vec{ \mathbf{H}}_{X}\|_{1+k}^{1+k}\), which implies:

\[\mathbb{E}\left[\left.e^{i\vec{\mathbf{\Gamma}}^{T}\vec{\mathbf{X} }^{\prime}}\right|M=m\right] =\exp\left(-\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{ \alpha}\sum_{j=1}^{d_{X}}\frac{\left|h_{j}^{\prime\,k}\right|^{k\alpha}}{\| \vec{\mathbf{H}}_{X}\|_{1+k}^{\alpha+k\alpha}}+i\vec{\mathbf{\Gamma}}^{T}\vec {\mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\frac{\left|\vec{\mathbf{H}}_{Y}^{T}\mathbf{\vec{ \mathbf{t}}}\right|^{\alpha}\left|\vec{\mathbf{H}}_{X}\right|_{1+k}^{\alpha+k \alpha}}{\|\vec{\mathbf{\Gamma}}_{X}\|_{1+k}^{\alpha+k\alpha}}\sum_{j=1}^{d_{X }}\left|h_{j}^{\prime\,k}\right|^{k\alpha}+i\vec{\mathbf{\Gamma}}^{T}\vec{ \mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\frac{\left|\vec{\mathbf{H}}_{Y}^{T}\mathbf{\vec{ \mathbf{t}}}\right|^{\alpha}\left|\vec{\mathbf{H}}_{X}\right|_{k\alpha}^{ \alpha+k\alpha}}{\left|\vec{\mathbf{\Gamma}}_{X}\right|_{1+k}^{\alpha+k\alpha}}+i \vec{\mathbf{\Gamma}}^{T}\vec{\mathbf{H}}_{Y}m\right).\] (122)Case 2: \(k\) is infinite, i.e., \(k=\infty\). Then, we have \(h^{\prime}_{j}=0\;\forall\;j\neq j^{*}\), and \(h^{\prime}_{j*}=\nicefrac{{1}}{{h_{1}^{\mathrm{X}}}}\), which implies:

\[\mathbb{E}\left[\left.e^{i\vec{\mathbf{t}}^{\,T}\vec{\mathbf{k}}^{ \prime}}\right|M=m\right] =\exp\left(-\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right| ^{\alpha}\left|\frac{1}{h_{j^{*}}^{X}}\right|^{\alpha}+i\vec{\mathbf{t}}^{\, T}\vec{\mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\frac{\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}} \right|^{\alpha}}{\left\|\vec{\mathbf{H}}_{X}\right\|_{\infty}^{\alpha}}+i \vec{\mathbf{t}}^{\,T}\vec{\mathbf{H}}_{Y}m\right).\] (123)

Combining (122) and (123), we can re-write \(\mathbb{E}\left[\left.e^{i\vec{\mathbf{t}}^{\,T}\vec{\mathbf{k}}^{\prime}} \right|M=m\right]\) as:

\[\mathbb{E}\left[\left.e^{i\vec{\mathbf{t}}^{\,T}\vec{\mathbf{k}} ^{\prime}}\right|M=m\right] =\exp\left(-f(k)\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}} \right|^{\alpha}+i\vec{\mathbf{t}}^{\,T}\vec{\mathbf{H}}_{Y}m\right),\;\text{ where}\] \[f(k) =\left\{\begin{array}{ll}\nicefrac{{\|\mathbf{H}_{X}\|_{k\infty }^{\mathrm{kn}}}}{{\|\mathbf{H}_{X}\|_{1+k}^{\alpha+\alpha}}}&k\text{ finite}\\ \nicefrac{{1}}{{\|\mathbf{H}_{X}\|_{\infty}^{\mathrm{kn}}}}&k\text{ infinite}\end{array}\right..\] (124)

Specifying \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime})\): We specify \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime})\) through the following stochastic transformation:

\[\vec{\mathbf{Y}}=\vec{\mathbf{X}}^{\prime}+\vec{\boldsymbol{\epsilon}},\] (125)

where \(\vec{\boldsymbol{\epsilon}}\) follows a multivariate stable distribution. Furthermore, we assume that \(\vec{\boldsymbol{\epsilon}}\) is jointly independent from \((\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},M)\), i.e., \(\vec{\boldsymbol{\epsilon}}\perp\mathbb{L}\;(\vec{\mathbf{X}},\vec{\mathbf{X}} ^{\prime},M)\). The characteristic function of \(\vec{\boldsymbol{\epsilon}}\) is defined as follows:

\[\mathbb{E}\left[\left.e^{i\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{\epsilon}}} \right|=\exp\left(-\sum_{j=1}^{d_{Y}}|t_{j}|^{\alpha}+f(k)\left|\vec{\mathbf{ H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}\right).\] (126)

In order for \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}^{\prime})\) to be a valid distribution, we need to ensure that the distribution of \(\vec{\boldsymbol{\epsilon}}\) defined in (126) is a legitimate multivariate stable distribution. We will utilize property 1 for showing that \(\vec{\boldsymbol{\epsilon}}\) follows a stable multivariate stable distribution. Property 1 states a random vector is distributed according to an independent component multivariate stable distribution iff every \(1\)-dimensional projection of this random vector follows a univariate stable continuous distribution. Hence, to show that \(\vec{\boldsymbol{\epsilon}}\) is distributed according to a legitimate multivariate stable distribution, we will show that every \(1\)-dimensional projection of \(\vec{\boldsymbol{\epsilon}}\) follows a univariate stable continuous distribution.

Let \(\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\), then the characteristic function of the 1-dimensional projection of \(\vec{\boldsymbol{\epsilon}}\) along \(\vec{\mathbf{t}}\), i.e., \(\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{\epsilon}}\) can be trivially deduced using (126).

\[\mathbb{E}\left[\left.e^{i\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{ \epsilon}}}\right|\right] =\exp\left(-\sum_{j=1}^{d_{Y}}|t_{j}|^{\alpha}+f(k)\left|\vec{ \mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}\right)\] \[=\exp\left(-\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}+f(k )\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}\right).\] (127)

Comparing the characteristic function of \(\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{\epsilon}}\) described in (127) with the standard characteristic function of a univariate stable characteristic function defined in (323), we can conclude \(\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{\epsilon}}\sim p_{CS}(\alpha,\beta( \vec{\mathbf{t}}),\gamma(\vec{\mathbf{t}}),\mu(\vec{\mathbf{t}}))\), where:

\[\gamma(\vec{\mathbf{t}})=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}-f(k )\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha},\mu(\vec{ \mathbf{t}})=0,\;\text{and}\;\beta(\vec{\mathbf{t}})=0.\] (128)

In order for \(\vec{\mathbf{t}}^{\,T}\vec{\boldsymbol{\epsilon}}\) to have a legitimate univariate stable distribution, we just need to show that \(\gamma(\vec{\mathbf{t}})\geq 0\;\forall\;\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\). Note that \(\gamma(\vec{\mathbf{t}})=0\) would correspond to the case where all the mass of the distribution is centered at \(0\), which does satisfy definition 2, and is an example of a degenerate univariate stable distribution. Let us analyze the function \(\gamma(\vec{\mathbf{t}})\):

\[\gamma(\vec{\mathbf{t}})=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}-f(k )\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha},\] (129)Applying lemma 8 on the term \(\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|\), we obtain:

\[\left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|\leq\left\{\begin{array}{ ll}\|\vec{\mathbf{t}}\|_{\alpha}^{\alpha}\|\vec{\mathbf{H}}_{Y}\|_{\alpha/ \alpha-1}^{\alpha}&\alpha\in(1,2]\\ \|\vec{\mathbf{t}}\|_{1}^{\alpha}\|\vec{\mathbf{H}}_{Y}\|_{\infty}^{\alpha}& \alpha\in(0,1]\end{array}\right..\] (130)

Applying (130) in (129), we obtain:

\[\gamma(\vec{\mathbf{t}})\geq\left\{\begin{array}{ll}\|\vec{\mathbf{t}}\|_{ \alpha}^{\alpha}-f(k)\|\vec{\mathbf{t}}\|_{\alpha}^{\alpha}\|\vec{\mathbf{H}} _{Y}\|_{\alpha/\alpha-1}^{\alpha}&\alpha\in(1,2]\\ \|\vec{\mathbf{t}}\|_{\alpha}^{\alpha}-f(k)\|\vec{\mathbf{t}}\|_{1}^{\alpha} \|\vec{\mathbf{H}}_{Y}\|_{\infty}^{\alpha}&\alpha\in(0,1]\end{array}\right..\]

Denote \(l(\vec{\mathbf{t}},\alpha)\) as follows:

\[l(\vec{\mathbf{t}},\alpha)=\left\{\begin{array}{ll}\|\vec{\mathbf{t}}\|_{ \alpha}^{\alpha}-f(k)\|\vec{\mathbf{t}}\|_{\alpha}^{\alpha}\|\vec{\mathbf{H}} _{Y}\|_{\alpha/\alpha-1}^{\alpha}&\alpha\in(1,2]\\ \|\vec{\mathbf{t}}\|_{\alpha}^{\alpha}-f(k)\|\vec{\mathbf{t}}\|_{1}^{\alpha} \|\vec{\mathbf{H}}_{Y}\|_{\infty}^{\alpha}&\alpha\in(0,1]\end{array}\right..\] (132)

In order to show \(\gamma(\vec{\mathbf{t}})\geq 0\)\(\forall\)\(\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\), it suffices to show that \(l(\vec{\mathbf{t}},\alpha)\geq 0\)\(\forall\)\(\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\) and \(\alpha\in(0,2]\).

Case 1: \(\alpha\in(1,2]\).

For \(\alpha\in(1,2]\), we have:

\[l(\vec{\mathbf{t}},\alpha)=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha} -f(k)\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\left\|\vec{\mathbf{H}} _{Y}\right\|_{\alpha/\alpha-1}^{\alpha}=\left\|\vec{\mathbf{t}}\right\|_{ \alpha}^{\alpha}\left(1-f(k)\left\|\vec{\mathbf{H}}_{Y}\right\|_{\alpha/\alpha -1}^{\alpha}\right).\] (133)

Since, the above equation holds for any \(k\in[0,\infty]\), we choose \(k=\nicefrac{{1}}{{(\alpha-1)}}\). Substituting the form \(f(k)\) from (124) in the above equation, we obtain:

\[l(\vec{\mathbf{t}},\alpha)=\left\|\vec{\mathbf{t}}\right\|_{\alpha }^{\alpha}\left(1-\frac{\left\|\vec{\mathbf{H}}_{Y}\right\|_{\nicefrac{{1}}{{ \alpha-1}}}^{\alpha}\left\|\vec{\mathbf{H}}_{X}\right\|_{\nicefrac{{1}}{{ \alpha-1}}}^{\nicefrac{{1}}{{\alpha-1}}}}{\left\|\vec{\mathbf{H}}_{X}\right\| _{\nicefrac{{1}}{{\alpha-1}}}^{\nicefrac{{1}}{{\alpha-1}}}}\right)=\left\| \vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\left(1-\frac{\left\|\vec{\mathbf{ H}}_{Y}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\alpha}\left\|\vec{\mathbf{H}}_{X} \right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\nicefrac{{1}}{{\alpha-1}}}}{\left\| \vec{\mathbf{H}}_{X}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\nicefrac{{1}}{{ \alpha-1}}}}\right)\] \[=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\left(1-\frac{ \left\|\vec{\mathbf{H}}_{Y}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\alpha}}{ \left\|\vec{\mathbf{H}}_{X}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\nicefrac{{1}} {{\alpha-1}}}}\right)=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\left(1 -\frac{\left\|\vec{\mathbf{H}}_{Y}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\alpha}} {\left\|\vec{\mathbf{H}}_{X}\right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\nicefrac{{1} }{{\alpha-1}}}}\right).\] (134)

Note that \(\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\geq 0\), and by condition 1 we know that \(\|\vec{\mathbf{H}}_{Y}\|_{\nicefrac{{1}}{{\alpha-1}}}\leq\|\vec{\mathbf{H}}_{X} \|_{\nicefrac{{1}}{{\alpha-1}}}\Rightarrow\|\vec{\mathbf{H}}_{Y}\|_{\nicefrac{{ 1}}{{\alpha-1}}}^{\alpha}\leq\|\vec{\mathbf{H}}_{X}\|_{\nicefrac{{1}}{{\alpha -1}}}^{\alpha}\). Hence, using these two previous facts and the above equation, we can conclude:

\[l(\vec{\mathbf{t}},\alpha)=\underbrace{\left\|\vec{\mathbf{t}}\right\|_{ \alpha}^{\alpha}}_{\geq 0}\underbrace{\left(1-\frac{\left\|\vec{\mathbf{H}}_{Y} \right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\alpha}}{\left\|\vec{\mathbf{H}}_{X} \right\|_{\nicefrac{{1}}{{\alpha-1}}}^{\alpha}}}_{\geq 0}\right)}_{\geq 0}\Rightarrow l(\vec{\mathbf{t}}, \alpha)\geq 0.\] (135)

Case 2: \(\alpha\in(0,1]\).

\[l(\vec{\mathbf{t}},\alpha)=\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}-f(k )\left\|\vec{\mathbf{t}}\right\|_{1}^{\alpha}\left\|\vec{\mathbf{H}}_{Y} \right\|_{\infty}^{\alpha}\stackrel{{(a)}}{{\geq}}\left\|\vec{ \mathbf{t}}\right\|_{\alpha}^{\alpha}-f(k)\left\|\vec{\mathbf{t}}\right\|_{ \alpha}^{\alpha}\left\|\vec{\mathbf{H}}_{Y}\right\|_{\infty}^{\alpha},\] (136)

where \((a)\) is derived using the monotonicity of \(L_{p}\) norms in \(p\), see [57]. More precisely, we use the inequality \(\|\vec{\mathbf{t}}\|_{\alpha}\geq\|\vec{\mathbf{t}}\|_{1}\)\(\forall\)\(\alpha\in(0,1]\). Simplifying the above equation, we have:

\[l(\vec{\mathbf{t}},\alpha)\geq\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha} \left(1-f(k)\left\|\vec{\mathbf{H}}_{Y}\right\|_{\infty}^{\alpha}\right).\] (137)Since, the above equation holds for any \(k\in[0,\infty]\), we choose \(k=\infty\). Substituting the form \(f(k)\) from (124) in the above equation, we obtain:

\[l(\vec{\mathbf{t}},\alpha)\geq\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha} \left(1-\frac{\left\|\vec{\mathbf{H}}_{Y}\right\|_{\infty}^{\alpha}}{\left\| \vec{\mathbf{H}}_{X}\right\|_{\infty}^{\alpha}}\right).\] (138)

Note that \(\left\|\vec{\mathbf{t}}\right\|_{\alpha}^{\alpha}\geq 0\), and by condition 1 we know that \(\|\vec{\mathbf{H}}_{Y}\|_{\infty}\leq\|\vec{\mathbf{H}}_{X}\|_{\infty}\Rightarrow \|\vec{\mathbf{H}}_{Y}\|_{\infty}^{\alpha}\leq\|\vec{\mathbf{H}}_{X}\|_{\infty} ^{\alpha}\). Hence, using these two facts and the above equation, we can conclude:

\[l(\vec{\mathbf{t}},\alpha)\geq\underbrace{\left\|\vec{\mathbf{t}}\right\|_{ \alpha}^{\alpha}}_{\geq 0}\underbrace{\left(1-\frac{\left\|\vec{\mathbf{H}}_{Y} \right\|_{\infty}^{\alpha}}{\left\|\vec{\mathbf{H}}_{X}\right\|_{\infty}^{ \alpha}}\right)}_{\geq 0}\Rightarrow l(\vec{\mathbf{t}},\alpha)\geq 0.\] (139)

From (135) and (139), we can conclude that \(l(\vec{\mathbf{t}},\alpha)\geq 0\)\(\forall\)\(\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\) and \(\alpha\in(0,2]\), which implies \(\gamma(\vec{\mathbf{t}})\geq 0\)\(\forall\)\(\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\). Hence, \(\vec{\boldsymbol{\epsilon}}\) follows a legitimate multivariate stable distribution.

Finally, we construct the desired \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) from \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{Y}})\) by marginalizing \(\vec{\mathbf{X}}^{\prime}\).

**Part 2: Showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\), we first need to derive \(Q_{MC}(\vec{\mathbf{Y}}|M)\).

Deriving \(Q(\vec{\mathbf{Y}}|M)\): We will derive \(Q_{MC}(\vec{\mathbf{Y}}|M)\) by using the fact that \(\vec{\boldsymbol{\epsilon}}\perp\vec{\mathbf{X}}^{\prime}|M\), hence their conditional characteristic functions would just result in multiplication.

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}\,T\vec{\mathbf{X}}}\Big{|} \,M=m\right]=\mathbb{E}\left[e^{i\vec{\mathbf{t}}\,T(\vec{\mathbf{X}}^{\prime }+\vec{\boldsymbol{\epsilon}})}\Big{|}\,M=m\right]=\mathbb{E}\left[e^{i\vec{ \mathbf{t}}\,T\vec{\mathbf{X}}^{\prime}+i\vec{\mathbf{t}}\,T\vec{\boldsymbol{ \epsilon}}}\Big{|}\,M=m\right]\] \[=\mathbb{E}\left[e^{i\vec{\mathbf{t}}\,T\vec{\mathbf{X}}^{\prime }}e^{i\vec{\mathbf{t}}\,T\vec{\boldsymbol{\epsilon}}}\Big{|}\,M=m\right] \left.\mathbb{E}\left[e^{i\vec{\mathbf{t}}\,T\vec{\mathbf{X}}^{\prime}} \Big{|}\,M=m\right],\] (140)

where \((a)\) is due to \(\vec{\boldsymbol{\epsilon}}\perp\vec{\mathbf{X}}^{\prime}|M\). Substituting the characteristic functions of \(\vec{\mathbf{X}}^{\prime}\) and \(\vec{\boldsymbol{\epsilon}}\) from (124) and (126), respectively, in (140):

\[=\exp\left(-\sum_{j=1}^{d_{Y}}|t_{j}|^{\alpha}+f(k)\left|\vec{ \mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}\right)\exp\left(-f(k) \left|\vec{\mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}+i\vec{\mathbf{t }}^{T}\vec{\mathbf{H}}_{Y}m\right)\] \[=\exp\left(-\sum_{j=1}^{d_{Y}}|t_{j}|^{\alpha}+f(k)\left|\vec{ \mathbf{H}}_{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}-f(k)\left|\vec{\mathbf{H}} _{Y}^{T}\vec{\mathbf{t}}\right|^{\alpha}+i\vec{\mathbf{t}}^{T}\vec{\mathbf{H}} _{Y}m\right)\] \[=\exp\left(-\sum_{j=1}^{d_{Y}}|t_{j}|^{\alpha}+i\vec{\mathbf{t}} ^{T}\vec{\mathbf{H}}_{Y}m\right).\] (141)

By comparing the characteristic function shown in (141) with the characteristic function derived using \(P(\vec{\mathbf{Y}}|M)\) shown in (116), we can conclude:

\[Q_{MC}(\vec{\mathbf{Y}}|M)=P(\vec{\mathbf{Y}}|M).\] (142)

From (142) and (117), we can conclude:

\[Q_{MC}(M,\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{Y}}|M)=P(M)P(\vec{ \mathbf{Y}}|M)=P(M,\vec{\mathbf{Y}}).\] (143)

We know that by construction:

\[Q_{MC}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}}).\] (144)

Hence, \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\), concluding our proof.

**Lemma 8**.: _Let \(\vec{\mathbf{a}}=[a_{1}\quad\ldots\quad a_{d}]\in\mathbb{R}^{d}\) and \(\vec{\mathbf{b}}=[b_{1}\quad\ldots\quad b_{d}]\in\mathbb{R}^{d}\), then we have:_

\[\left|\vec{\mathbf{a}}^{T}\vec{\mathbf{b}}\right|^{\alpha}\leq \left\{\begin{array}{ll}\|\vec{\mathbf{a}}\|_{\alpha}^{\alpha}\|\vec{ \mathbf{b}}\|_{\alpha/(\alpha-1)}^{\alpha}&\alpha\in(1,2]\\ \|\vec{\mathbf{a}}\|_{1}^{\alpha}\|\vec{\mathbf{b}}\|_{\infty}^{\alpha}& \alpha\in(0,1]\end{array}\right.,\] (145)

_where \(\|(\cdot)\|_{p}\) is the standard \(L_{p}\) norm._

Proof.: **Case 1**: \(\alpha\in(1,2]\)

Analysing the LHS of (145)

\[\left|\vec{\mathbf{a}}^{T}\vec{\mathbf{b}}\right|=\left|\sum_{i=1}^{d}a_{i}b _{i}\right|\stackrel{{(a)}}{{\leq}}\sum_{i=1}^{d}|a_{i}b_{i}| \stackrel{{(b)}}{{\leq}}\left(\sum_{i=1}^{d}|a_{i}|^{\alpha} \right)^{\frac{1}{\alpha}}\left(\sum_{i=1}^{d}|b_{i}|^{\frac{\alpha}{(\alpha- 1)}}\right)^{\frac{(\alpha-1)}{\alpha}}=\|\vec{\mathbf{a}}\|_{\alpha}\|\vec{ \mathbf{b}}\|_{\nicefrac{{\alpha}}{{\alpha-1}}},\] (146)

where \((a)\) is due to the sub-additivity of \(|(\cdot)|\) operator, and \((b)\) is due to Holder's inequality [57]. Raising both sides of the inequality to \(\alpha\) in (146) gives us the desired inequality. Since, \(\alpha>0\) raising both sides of the inequalities to power \(\alpha\) does not change the direction of inequality.

\[\left|\vec{\mathbf{a}}^{T}\vec{\mathbf{b}}\right|^{\alpha}\leq \|\vec{\mathbf{a}}\|_{\alpha}^{\alpha}\|\vec{\mathbf{b}}\|_{\nicefrac{{ \alpha}}{{\alpha-1}}}^{\alpha}.\] (147)

**Case 2**: \(\alpha\in(0,1]\)

Analysing the LHS of (145)

\[\left|\vec{\mathbf{a}}^{T}\vec{\mathbf{b}}\right|=\left|\sum_{i=1}^{d}a_{i}b _{i}\right|\stackrel{{(a)}}{{\leq}}\sum_{i=1}^{d}|a_{i}b_{i}| \stackrel{{(b)}}{{\leq}}\left(\sum_{i=1}^{d}|a_{i}|\right)\max_{i \in\{1,\ldots,d\}}|b_{i}|=\|\vec{\mathbf{a}}\|_{1}\|\vec{\mathbf{b}}\|_{ \infty},\] (148)

where \((a)\) is again due to the sub-additivity of \(|(\cdot)|\) operator, and \((b)\) is due to the fact that \(|a_{i}b_{i}|\leq|a_{i}|\max_{i\in\{1,\ldots,d\}}|b_{i}|\)\(\forall\)\(i\in\{1,\ldots,d\}\). Raising both sides of the inequality to \(\alpha\) in (148) gives us the desired inequality. Since, \(\alpha>0\) raising both sides of the inequalities to power \(\alpha\) does not change the direction of inequality.

\[\left|\vec{\mathbf{a}}^{T}\vec{\mathbf{b}}\right|^{\alpha}\leq \|\vec{\mathbf{a}}\|_{1}^{\alpha}\|\vec{\mathbf{b}}\|_{\infty}^{\alpha}.\] (149)

Combining (147) and (149), we obtain the desired inequality. 

**Lemma 9**.: _Let \(\vec{\mathbf{Z}}\sim p_{CS-IC}(\alpha,\vec{\mathbf{0}}_{d},\vec{\mathbf{1}}_{ d},\vec{\mathbf{0}}_{d})\) be a \(d\)-dimensional random vector, \(\mathbf{A}=[a_{ij}]_{i,j=1}^{n,d}\in\mathbb{R}^{n\times d}\) be a \(n\times d\) matrix, and \(\vec{\mathbf{b}}\in\mathbb{R}^{d}\). Then, the characteristic function of \(\mathbf{A}\vec{\mathbf{Z}}+\vec{\mathbf{b}}\) is given by (150)_

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}(\mathbf{A}\vec{\mathbf{Z}}+\vec{ \mathbf{b}})}\right]=\exp\left(-\sum_{j=1}^{d}\left|\vec{\mathbf{a}}_{j}^{T} \vec{\mathbf{t}}\right|^{\alpha}+i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}\right) \text{ }\forall\vec{\mathbf{t}}\in\mathbb{R}^{d},\] (150)

_where \(\vec{\mathbf{a}}_{i}\) is the \(i\)-th column of \(\mathbf{A}\)._

Proof.: Analyzing L.H.S of (150)

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}(\mathbf{A}\vec{\mathbf{Z}}+\vec{ \mathbf{b}})}\right] =\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}\mathbf{A}\vec{\mathbf{Z}}+ \vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\right]=\mathbb{E}\left[e^{i\vec{ \mathbf{t}}^{T}\mathbf{A}\vec{\mathbf{Z}}}e^{i\vec{\mathbf{t}}^{T}\vec{ \mathbf{b}}}\right]\] (151)

Using the fact that \(e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\) is constant w.r.t expectation over \(\vec{\mathbf{Z}}\), and linearity of expectation, we have:

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}(\mathbf{A}\vec{\mathbf{Z}}+\vec{ \mathbf{b}})}\right] =e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\mathbb{E}\left[\exp\left(i \vec{\mathbf{t}}^{T}\mathbf{A}\vec{\mathbf{Z}}\right)\right],\] \[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\mathbb{E}\left[\exp \left(i\vec{\mathbf{t}}^{T}\left[\vec{\mathbf{a}}_{1}\quad\ldots\quad\vec{ \mathbf{a}}_{d}\right]\vec{\mathbf{Z}}\right)\right],\] \[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\mathbb{E}\left[\exp \left(i\left[\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{1}\quad\ldots\quad\vec{ \mathbf{t}}^{T}\vec{\mathbf{a}}_{d}\right]\vec{\mathbf{Z}}\right)\right],\]Simplifying the term \(\left[\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{1}\quad\ldots\quad\vec{\mathbf{t}}^{T} \vec{\mathbf{a}}_{d}\right]\vec{\mathbf{Z}}\) by substituting \(\mathbf{Z}=\left[Z_{1}\quad\ldots\quad Z_{d}\right]^{T}\), we obtain:

\[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\mathbb{E}\left[\exp\left(i\sum_{j=1} ^{d}(\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{j})Z_{j}\right)\right],\]

Using properties of exponentials, we obtain:

\[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\mathbb{E}\left[\prod_{j=1}^{d}\exp \left(i\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{j}Z_{j}\right)\right],\]

Using the fact \(\{Z_{j}\}_{j=1}^{d}\) are jointly independent we obtain:

\[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\prod_{j=1}^{d}\mathbb{E}\left[\exp \left(i\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{j}Z_{j}\right)\right],\]

Using the fact that \(Z_{j}\sim p_{CS}(\alpha,0,1,0)\) and the result of lemma 5, we know that \(\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{j}Z_{j}\sim p_{CS}(\alpha,0,|\vec{ \mathbf{t}}^{T}\vec{\mathbf{a}}_{j}|,0)\). Substituting the corresponding characteristic function of \(\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{j}Z_{j}\) using (323)

\[=e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\prod_{j=1}^{d}\exp \left(-\left|\vec{\mathbf{t}}^{T}\vec{\mathbf{a}}_{i}\right|^{\alpha}\right) =e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}}\exp\left(-\sum_{j=1}^{d}\left|\vec{ \mathbf{t}}^{T}\vec{\mathbf{a}}_{i}\right|^{\alpha}\right)\] \[=\exp\left(-\sum_{j=1}^{d}\left|\vec{\mathbf{t}}^{T}\vec{ \mathbf{a}}_{i}\right|^{\alpha}+i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}\right).\] (152)

**Lemma 10**.: _Let \(\vec{\mathbf{H}}_{1}=\left[h_{1}^{(1)}\quad\ldots\quad h_{d_{1}}^{(1)}\right] ^{T}\in\mathbb{R}^{d_{1}}\) and \(\vec{\mathbf{H}}_{2}=\left[h_{1}^{(2)}\quad\ldots\quad h_{d_{2}}^{(2)}\right] ^{T}\in\mathbb{R}^{d_{2}}\). Define \(\vec{\mathbf{H}}_{1}^{k}\) as follows:_

\[\vec{\mathbf{H}}_{1}^{k}=\frac{1}{\|\vec{\mathbf{H}}_{1}\|_{1+k}^{1+k}}\left[ \text{sgn}(h_{1}^{(1)})|h_{1}^{(1)}|^{k}\quad\ldots\quad\text{sgn}(h_{d_{1}}^{ (1)})|h_{d_{1}}^{(1)}|^{k}\right]^{T}\;\forall\;k\in[0,\infty).\]

_Furthermore, define \(\vec{\mathbf{H}}_{1}^{\infty}\) as follows:_

\[j^{*}=\operatorname*{arg\,max}_{j\in\{1,\ldots,d_{1}\}}\left|h_{j}^{(1)} \right|,\]

\[\vec{\mathbf{H}}_{1}^{\infty}=\left[h_{1}^{\infty}\quad\ldots\quad h_{d_{1}}^ {\infty}\right]^{T},\;\text{with}\;h_{j}^{\infty}=0\;\forall\;j\in\{1,\ldots,d _{1}\}\backslash\{j^{*}\},\;\text{and}\;h_{j}^{\infty}=\frac{1}{h_{j^{*}}^{(1)}}.\] (153)

_Then, we have:_

\[\vec{\mathbf{H}}_{2}(\vec{\mathbf{H}}_{1}^{k})^{T}\vec{\mathbf{H}}_{1}=\vec{ \mathbf{H}}_{2}\;\forall\;k\in[0,\infty)\;\text{and}\;\vec{\mathbf{H}}_{2}( \vec{\mathbf{H}}_{1}^{\infty})^{T}\vec{\mathbf{H}}_{1}=\vec{\mathbf{H}}_{2},\] (154)

_where \(\|(\cdot)\|_{1+k}\) is the standard \(L_{1+k}\) norm [57]. We can alternatively state (154) in a more compact notation:_

\[\vec{\mathbf{H}}_{2}(\vec{\mathbf{H}}_{1}^{k})^{T}\vec{\mathbf{H}}_{1}=\vec{ \mathbf{H}}_{2}\;\forall\;k\in[0,\infty],\] (155)

_where the case of \(k=\infty\) is to be understood as \(\vec{\mathbf{H}}_{1}^{k}=\vec{\mathbf{H}}_{1}^{\infty}\)._

Proof.: **Case 1**: \(k\in[0,\infty)\), i.e., \(k\) is finite,

\[\vec{\mathbf{H}}_{2}(\vec{\mathbf{H}}_{1}^{k})^{T}\vec{\mathbf{H}}_{1} =\frac{1}{\|\vec{\mathbf{H}}_{1}\|_{1+k}^{1+k}}\vec{\mathbf{H}}_{2} \left[\text{sgn}(h_{1}^{(1)})|h_{1}^{(1)}|^{k}\quad\ldots\quad\text{sgn}(h_{d_ {1}}^{(1)})|h_{d_{1}}^{(1)}|^{k}\right]\begin{bmatrix}h_{1}^{(1)}\\ \vdots\\ h_{d_{1}}^{(1)}\end{bmatrix}\] \[=\frac{1}{\|\vec{\mathbf{H}}_{1}\|_{1+k}^{1+k}}\underbrace{\left( \sum_{j=1}^{d}|h_{j}|^{1+k}\right)\vec{\mathbf{H}}_{2}=\vec{\mathbf{H}}_{2}}.\]

**Case 2**: \(k=\infty\),

\[\mathbf{\vec{H}}_{2}(\mathbf{\vec{H}}_{1}^{\infty})^{T}\mathbf{\vec{H}}_{1}= \mathbf{\vec{H}}_{2}\left(\sum_{j=1}^{d_{1}}h_{j}^{\infty}h_{j}^{(1)}\right) \stackrel{{(a)}}{{=}}\mathbf{\vec{H}}_{2}\left(h_{js}^{\infty}h_{ j^{*}}^{(1)}\right)\stackrel{{(b)}}{{=}}\mathbf{\vec{H}}_{2}\frac{1}{h_{ j*}^{(1)}}h_{js}^{(1)}=\mathbf{\vec{H}}_{2},\]

where \((a)\) is due to the fact that all elements of \(\mathbf{\vec{H}}_{1}^{\infty}\) are \(0\) except the \(j^{*}\) element, and \((b)\) is due to the fact that \(h_{js}^{\infty}=\nicefrac{{1}}{{h_{j^{*}}^{(1)}}}\). 

## Appendix H Proof of Theorem 3

In this section, we provide the proof of theorem 3. Since theorem 3 provides the analytical PID terms for the system 2 of the multivariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the corresponding elliptically-contoured multivariate stable distributions and the corresponding multivariate affine continuous stable system for convenience.

### Elliptically-contoured multivariate stable distribution

Elliptically-contoured multivariate stable distributions are another example of a multivariate generalization of univariate continuous stable distributions. As the name suggests, the defining features of these distributions is that the corresponding p.d.f. has elliptical contours, similar to the multivariate Gaussian distribution. We refer the reader to Appx. N.3 for more details on multivariate continuous stable distributions. We now list certain key properties of elliptically-contoured multivariate stable distributions that we make use of in the proof of theorem 3:

1. We denote the p.d.f. of the elliptically-contoured continuous stable distribution as \(p_{CS-EC}(\alpha,\mathbf{\Sigma},\mathbf{\vec{\mu}})\), where \(\mathbf{\Sigma}\) is a positive definite matrix, \(\mathbf{\vec{\mu}}\in\mathbb{R}^{d}\), and \(\alpha\in(0,2]\). In general, the p.d.f. of Elliptically-contoured multivariate stable distribution do not have a closed-form analytical expression, and are expressed through their characteristic function.
2. The characteristic function of a \(d\)-dimensional random vector \(\mathbf{\vec{X}}\) having elliptically-contoured multivariate distribution \(p_{CS-EC}(\alpha,\mathbf{\Sigma},\mathbf{\vec{\mu}})\) is expressed as follows: \[\mathbb{E}\left[e^{i\mathbf{\vec{\tau}}\cdot\mathbf{\vec{X}}}\right]=\exp \left(-\left(\mathbf{\vec{\tau}}^{T}\mathbf{\Sigma}\mathbf{\vec{t}}\right)^{ \nicefrac{{\alpha}}{{2}}}+i\mathbf{\vec{\tau}}^{T}\mathbf{\vec{\mu}}\right) \;\;\forall\;\mathbf{\vec{t}}\in\mathbb{R}^{d}.\] (156)

### Definition of the system 2 of the multivariate affine continuous stable system

Let the random variable \(M\sim P(M)\) with support \(\mathcal{M}\subseteq\mathbb{R}\). \(\mathbf{\vec{X}}\) and \(\mathbf{\vec{Y}}\) are \(d_{X}\)-dimensional and \(d_{Y}\)-dimensional random vectors, respectively. The joint distribution \(P(M,\mathbf{\vec{X}},\mathbf{\vec{Y}})\) describes the second system of the multivariate affine continuous stable distribution if it satisfies the following two properties:

1. \(M\sim P(M)\) having some support set \(\mathcal{M}\subseteq\mathbb{R}\).
2. The conditional distributions of \(\mathbf{\vec{X}}\) and \(\mathbf{\vec{Y}}\) are expressed as follows: \[P(\mathbf{\vec{X}}|M) =p_{CS-EC}(\alpha,\mathbf{\Sigma}_{X},\mathbf{\vec{H}}_{X}M+ \mathbf{\vec{b}}_{X}),\text{ and }\] \[P(\mathbf{\vec{Y}}|M) =p_{CS-EC}(\alpha,\mathbf{\Sigma}_{Y},\mathbf{\vec{H}}_{Y}M+ \mathbf{\vec{b}}_{Y}),\] (157) where \(\mathbf{\Sigma}_{X}\) and \(\mathbf{\Sigma}_{Y}\) are positive definite matrices, \(\mathbf{\vec{H}}_{X},\mathbf{\vec{b}}_{X}\in\mathbb{R}^{d_{X}}\), and \(\mathbf{\vec{H}}_{Y},\mathbf{\vec{b}}_{Y}\in\mathbb{R}^{d_{Y}}\).

### Formal proof of theorem 3

**Theorem 3**.: _Let the joint distribution \(P(M,\mathbf{\vec{X}},\mathbf{\vec{Y}})\) of \(M\), \(\mathbf{\vec{X}}\), and \(\mathbf{\vec{Y}}\) describe the system 2 of the multivariate affine continuous stable system. Define \(\mathbf{\Sigma}_{X}^{\nicefrac{{-1}}{{2}}}\) and \(\mathbf{\Sigma}_{Y}^{\nicefrac{{-1}}{{2}}}\) as the respective inverses of the matrices \(\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\) and \(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}}\) which satisfy: \((\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X}^{ \nicefrac{{1}}{{2}}}=\mathbf{\Sigma}_{X}\), and \((\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{Y}^{ \nicefrac{{1}}{{2}}}=\mathbf{\Sigma}_{Y}\). Without the loss of generality, assume \(\|\mathbf{\Sigma}_{Y}^{\nicefrac{{-1}}{{2}}}\mathbf{\vec{H}}_{Y}\|_{2}\leq\| \mathbf{\Sigma}_{X}^{\nicefrac{{-1}}{{2}}}\mathbf{\vec{H}}_{X}\|_{2}\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\mathbf{\vec{X}}\rightarrow\mathbf{\vec{Y}}\) and \(UI(M;\mathbf{\vec{Y}}\backslash\mathbf{\vec{X}})=0\)._Proof.: We first note two important observations:

1. We can always assume \(\|\mathbf{\Sigma}_{Y}^{-1/2}\mathbf{\tilde{H}}_{Y}\|_{2}\leq\|\mathbf{\Sigma}_{X }^{-1/2}\mathbf{\tilde{H}}_{X}\|_{2}\) without the loss generality because if \(\|\mathbf{\Sigma}_{Y}^{-1/2}\mathbf{\tilde{H}}_{Y}\|_{2}\geq\|\mathbf{\Sigma}_{ X}^{-1/2}\mathbf{\tilde{H}}_{X}\|_{2}\), then we can always switch our nomenclature to refer to \(\mathbf{\tilde{Y}}\) as \(\mathbf{\tilde{X}}\), and \(\mathbf{\tilde{X}}\) as \(\mathbf{\tilde{Y}}\).
2. Properties of positive definite matrices guarantee the existence of an invertible \(\mathbf{\Sigma}_{X}^{1/2}\) and \(\mathbf{\Sigma}_{Y}^{1/2}\)[58].

The proof of this theorem is a generalization of the proof used for the deriving a similar result for the special case of the multivariate Gaussian distribution discussed in [22], which borrows known results from Gaussian interference channels discussed in [48]. This proof follows the same structure as our earlier proofs.

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{Y}})\) having the Markovian structure \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{Y}}\).
2. In the second part, we show that the \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{Y}})\) constructed in the first part, lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;\mathbf{\tilde{Y}}\backslash\mathbf{\tilde{X}})=0\).

**Part 1: Specifying \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{Y}})\)**

We explicitly construct the desired Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{Y}}\) by constructing a larger Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{X}}^{\prime} \rightarrow\mathbf{\tilde{X}}^{\prime\prime}\rightarrow\mathbf{\tilde{Y}}^{ \prime}\rightarrow\mathbf{\tilde{Y}}\), and then marginalizing the larger Markov chain to obtain the desired Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{Y}}\).

Denote the joint distribution of the Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{X}}^{\prime} \rightarrow\mathbf{\tilde{X}}^{\prime\prime}\rightarrow\mathbf{\tilde{Y}}^{ \prime}\rightarrow\mathbf{\tilde{Y}}\) as \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{X}}^{\prime},\mathbf{\tilde{X}}^{ \prime\prime},\mathbf{\tilde{Y}}^{\prime},\mathbf{\tilde{Y}})\). We can decompose \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{X}}^{\prime},\mathbf{\tilde{X}}^{ \prime\prime},\mathbf{\tilde{Y}}^{\prime},\mathbf{\tilde{Y}})\) by utilizing its Markovian structure as follows: \(Q_{MC}(M,\mathbf{\tilde{X}}^{\prime},\mathbf{\tilde{X}}^{\prime},\mathbf{ \tilde{X}}^{\prime\prime},\mathbf{\tilde{Y}}^{\prime},\mathbf{\tilde{Y}}^{ \prime})=Q_{MC}(M)Q_{MC}(\mathbf{\tilde{X}}|M)\)\(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|\mathbf{\tilde{X}})\)\(Q_{MC}(\mathbf{\tilde{X}}^{\prime\prime}|\mathbf{\tilde{X}}^{\prime})Q_{MC}(\mathbf{\tilde{Y}}^{ \prime}|\mathbf{\tilde{X}}^{\prime\prime})Q_{MC}(\mathbf{\tilde{Y}}|\mathbf{ \tilde{Y}}^{\prime})\). Consequently, we can specify/construct the distribution \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{X}}^{\prime},\mathbf{\tilde{X}}^{ \prime\prime},\mathbf{\tilde{Y}}^{\prime},\mathbf{\tilde{Y}})\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(\mathbf{\tilde{X}}|M)\), \(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|\mathbf{\tilde{X}})\), \(Q_{MC}(\mathbf{\tilde{X}}^{\prime\prime}|\mathbf{\tilde{X}}^{\prime})\), \(Q_{MC}(\mathbf{\tilde{Y}}^{\prime}|\mathbf{\tilde{X}}^{\prime\prime})\), and \(Q_{MC}(\mathbf{\tilde{Y}}|\mathbf{\tilde{Y}}^{\prime})\).

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(\mathbf{\tilde{X}}|M)\): We choose \(Q_{MC}(M)\) and \(Q_{MC}(\mathbf{\mathbf{\tilde{X}}}|M)\) as follows:

\[Q_{MC}(M)=P(M)\text{ and }Q_{MC}(\mathbf{\tilde{X}}|M)=P(\mathbf{\tilde{X}}|M).\] (158)

Specifying \(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|\mathbf{\tilde{X}})\): \(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|\mathbf{\tilde{X}})\) is specified through the following deterministic transformation:

\[\mathbf{\tilde{X}}^{\prime}=(\Sigma_{X}^{-1/2})^{T}(\mathbf{\tilde{X}}- \mathbf{\tilde{b}}_{X}).\] (159)

We will also derive the distribution \(Q(\mathbf{\tilde{X}}^{\prime}|M)\) before proceeding with our construction, as we will need it later in the proof to show that the constructed \(Q_{MC}(M,\mathbf{\tilde{X}},\mathbf{\tilde{Y}})\in\Delta_{P}\).

_Deriving \(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|M)\)_: We use lemma 11 to derive \(Q_{MC}(\mathbf{\tilde{X}}^{\prime}|M=m)\), as \(Q_{MC}(\mathbf{\tilde{X}}|M=m)\) is an elliptically contoured multivariate stable distribution for a fixed \(m\), and (159) defines a scaling and translation operation.

\[Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left(\alpha,(\mathbf{ \Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X}\mathbf{\Sigma}_{X}^{- \nicefrac{{1}}{{2}}},(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}\vec{\mathbf{ H}}_{X}m+(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}(\vec{\mathbf{b}}_{X}- \vec{\mathbf{b}}_{X})\right),\] \[\Rightarrow Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left( \alpha,(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X} \mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}},(\mathbf{\Sigma}_{X}^{-\nicefrac{{1} }{{2}}})^{T}\vec{\mathbf{H}}_{X}m\right),\] \[\Rightarrow Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left( \alpha,(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}(\mathbf{\Sigma}_{X}^{ \nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\mathbf{ \Sigma}_{X}^{-\nicefrac{{1}}{{2}}},(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}} })^{T}\vec{\mathbf{H}}_{X}m\right),\] \[\Rightarrow Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left( \alpha,(\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\mathbf{\Sigma}_{X}^{- \nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{X}^{\nicefrac{{1}}{{2}}}\mathbf{ \Sigma}_{X}^{-\nicefrac{{1}}{{2}}},(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}} })^{T}\vec{\mathbf{H}}_{X}m\right),\] \[\Rightarrow Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left( \alpha,\mathbf{I}_{d_{X}},(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}\vec {\mathbf{H}}_{X}m\right),\] \[\Rightarrow Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left( \alpha,\mathbf{I}_{d_{X}},\vec{\mathbf{K}}_{X}m\right),\] (160)

where \(\mathbf{I}_{d_{X}}\) is a \(d_{X}\times d_{X}\) identity matrix, and \(\vec{\mathbf{K}}_{X}=(\mathbf{\Sigma}_{X}^{-\nicefrac{{1}}{{2}}})^{T}\vec{ \mathbf{H}}_{X}\).

Specifying \(Q_{MC}(\vec{\mathbf{X}}^{\prime\prime}|\vec{\mathbf{X}}^{\prime})\): \(Q_{MC}(\vec{\mathbf{X}}^{\prime\prime}|\vec{\mathbf{X}}^{\prime})\) is again specified through a deterministic transformation:

\[\vec{\mathbf{X}}^{\prime\prime}=\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1}) ^{T}\vec{\mathbf{X}}^{\prime},\] (161)

where \(\vec{\mathbf{K}}_{Y}=(\mathbf{\Sigma}_{Y}^{-\nicefrac{{1}}{{2}}})^{T}\vec{ \mathbf{H}}_{Y}\) and \(\vec{\mathbf{K}}_{X}^{1}=\nicefrac{{1}}{{\left\|\mathbf{K}_{X}\right\|_{2}^{2} }}\vec{\mathbf{K}}_{X}\). We also derive the distribution \(Q(\vec{\mathbf{X}}^{\prime}|M)\) which will be needed for showing that the constructed \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\).

_Deriving \(Q_{MC}(\vec{\mathbf{X}}^{\prime\prime}|M=m)\)_: We again use lemma 11 to calculate \(Q_{MC}(\vec{\mathbf{X}}^{\prime\prime}|M=m)\), as \(Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)\) is an elliptically contoured multivariate stable distribution (see (160)) for a fixed \(m\), and (159) defines a scaling and translation operation.

\[Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m) =p_{CS-EC}\left(\alpha,(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^ {1})^{T})\mathbf{I}_{d_{X}}(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T} )^{T},\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T}\vec{\mathbf{K}}_{X}m \right),\] \[=p_{CS-EC}\left(\alpha,(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^ {1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^{T},\vec{\mathbf{ K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T}\vec{\mathbf{K}}_{X}m\right),\]

From lemma 10, we know that \(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T}\vec{\mathbf{K}}_{X}=\vec{ \mathbf{K}}_{Y}\), which implies

\[Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m)=p_{CS-EC}\left(\alpha,(\vec{\mathbf{K}}_{ Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^{T}, \vec{\mathbf{K}}_{Y}m\right).\] (162)

Specifying \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime\prime})\): We specify \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime\prime})\) through the following stochastic transformation:

\[\vec{\mathbf{Y}}^{\prime}=\vec{\mathbf{X}}^{\prime\prime}+\vec{\bm{\epsilon}},\] (163)

where \(\vec{\bm{\epsilon}}\) follows a multivariate stable distribution. Furthermore, we assume that \(\vec{\bm{\epsilon}}\) is jointly independent from \((\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{X}}^{\prime\prime},M)\), i.e., \(\vec{\bm{\epsilon}}\perp\perp(\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{ \mathbf{X}}^{\prime\prime},M)\). The characteristic function of \(\vec{\bm{\epsilon}}\) is defined as follows:

\[\mathbb{E}\left[e^{i\vec{\bm{\epsilon}}^{T}\vec{\bm{\epsilon}}}\right]=\exp\left( -\left(\vec{\bm{\epsilon}}^{T}\mathbf{I}_{d_{Y}}\vec{\bm{\epsilon}}\right)^{ \nicefrac{{\alpha}}{{2}}}+\left(\vec{\bm{\epsilon}}^{T}(\vec{\mathbf{K}}_{Y}( \vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^ {T}\vec{\bm{\epsilon}}\right)^{\nicefrac{{\alpha}}{{2}}}\right).\] (164)

In order for \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|\vec{\mathbf{X}}^{\prime\prime})\) to be a valid distribution, we need to ensure that the distribution of \(\vec{\bm{\epsilon}}\) defined in (164) is a legitimate multivariate stable distribution. Similarly to the proof of theorem 2, we will utilize property 1 for showing that \(\vec{\bm{\epsilon}}\) follows a stable multivariate stable distribution. Property 1 states a random vector is distributed according to a multivariate stable distribution iff every 1-dimensional projection of this random vector follows a univariate stable continuous distribution. Hence, to show that \(\vec{\bm{\epsilon}}\) is distributed according to a legitimate multivariate stable distribution, we will show that every 1-dimensional projection of \(\vec{\bm{\epsilon}}\) follows a univariate stable continuous distribution.

Let \(\vec{\bm{\epsilon}}\in\mathbb{R}^{d_{Y}}\), then the characteristic function of the 1-dimensional projection of \(\vec{\bm{\epsilon}}\) along \(\vec{\bm{\epsilon}}\), i.e., \(\vec{\bm{\epsilon}}^{T}\vec{\bm{\epsilon}}\) can be trivially derived from (164).

\[\mathbb{E}\left[e^{i\vec{\bm{\epsilon}}^{T}\vec{\bm{\epsilon}}}\right]=\exp \left(-\left(\vec{\bm{\epsilon}}^{T}\mathbf{I}_{d_{Y}}\vec{\bm{\epsilon}} \right)^{\nicefrac{{\alpha}}{{2}}}+\left(\vec{\bm{\epsilon}}^{T}(\vec{ \mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{ \mathbf{K}}_{X}^{1})^{T})^{T}\vec{\bm{\epsilon}}\right)^{\nicefrac{{\alpha}}{{2}}} \right).\] (165)Comparing the characteristic function of \(\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\epsilon}}\) described in (127) with the standard characteristic function of a univariate stable characteristic function defined in (323), we can conclude \(\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\epsilon}}\sim p_{CS}(\alpha,\beta(\vec{ \mathbf{t}}),\gamma(\vec{\mathbf{t}}),\mu(\vec{\mathbf{t}}))\), where:

\[\gamma(\vec{\mathbf{t}})=\left(\vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y}}\vec{ \mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}-\left(\vec{\mathbf{t}}^{T}( \vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{ \mathbf{K}}_{X}^{1})^{T})^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}} },\mu(\vec{\mathbf{t}})=0,\text{ and }\beta(\vec{\mathbf{t}})=0.\] (166)

In order for \(\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\epsilon}}\) to have a legitimate univariate stable distribution, we just need to show that \(\gamma(\vec{\mathbf{t}})\geq 0\;\forall\;\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}\). Note that \(\gamma(\vec{\mathbf{t}})=0\) would correspond to the case where all the mass of the distribution is centered at \(0\), which does satisfy definition 2, and is an example of a degenerate univariate stable distribution.

By the assumption in theorem, \(\|\vec{\mathbf{K}}_{Y}\|_{2}=\|\mathbf{\Sigma}_{Y}^{-\nicefrac{{1}}{{2}}} \vec{\mathbf{H}}_{Y}\|_{2}\leq\|\vec{\mathbf{K}}_{X}\|_{2}=\|\mathbf{\Sigma}_ {X}^{-\nicefrac{{1}}{{2}}}\vec{\mathbf{H}}_{X}\|_{2}\). Hence, we can use the result of lemma 12, which shows that \((\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}( \vec{\mathbf{K}}_{X}^{1})^{T})^{T}\preceq\mathbf{I}_{d_{Y}}\). Using the fact that \((\mathbf{I}_{d_{Y}}-(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{ \mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^{T})\) is a positive semi-definite matrix, we have:

\[\vec{\mathbf{t}}^{T}\left(\mathbf{I}_{d_{Y}}-(\vec{\mathbf{K}}_{Y} (\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1} )^{T})^{T}\right)\vec{\mathbf{t}}\geq 0\;\forall\;\vec{\mathbf{t}}\in \mathbb{R}^{d_{Y}},\] \[\Rightarrow \vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y}}\vec{\mathbf{t}}\geq\vec{ \mathbf{t}}^{T}(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{ \mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^{T}\vec{\mathbf{t}}\;\;\forall \;\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}},\] \[\Rightarrow \left(\vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y}}\vec{\mathbf{t}} \right)^{\nicefrac{{\alpha}}{{2}}}\geq\left(\vec{\mathbf{t}}^{T}(\vec{ \mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{ \mathbf{K}}_{X}^{1})^{T})^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2} }}\;\forall\;\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}},\] \[\Rightarrow \gamma(\vec{\mathbf{t}})=\left(\vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y }}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}-\left(\vec{\mathbf{t}}^{ T}(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}( \vec{\mathbf{K}}_{X}^{1})^{T})^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2} }}\geq 0\;\forall\;\vec{\mathbf{t}}\in\mathbb{R}^{d_{Y}}.\] (167)

Hence, we know that \(\vec{\boldsymbol{\epsilon}}\) follows a legitimate multivariate stable distribution. We also derive \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|M)\) which is needed for showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\).

_Deriving \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|M)\)_: We will derive \(Q_{MC}(\vec{\mathbf{Y}}^{\prime}|M)\) using the fact that \(\vec{\boldsymbol{\epsilon}}\perp\!\!\!\perp\vec{\mathbf{X}}^{\prime\prime}|M=m\), hence their conditional characteristic functions would just result in multiplication.

\[\mathbb{E}\,\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{Y}}^{\prime }}\Big{|}\,M=m\right]=\mathbb{E}\,\left[e^{i\vec{\mathbf{t}}^{T}(\vec{ \mathbf{X}}^{\prime\prime}+\vec{\boldsymbol{\epsilon}})}\Big{|}\,M=m\right]= \mathbb{E}\,\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}^{\prime\prime}+i\vec{ \mathbf{t}}^{T}\vec{\boldsymbol{\epsilon}}}\Big{|}\,M=m\right],\] \[=\mathbb{E}\,\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}^{ \prime\prime}}e^{i\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\epsilon}}}\Big{|}\,M=m \right]\,\overset{(a)}{=}\mathbb{E}\,\left[e^{i\vec{\mathbf{t}}^{T}\vec{ \boldsymbol{\epsilon}}^{\prime\prime}}\Big{|}\,M=m\right],\] (168)

where \((a)\) is due to \(\vec{\boldsymbol{\epsilon}}\perp\!\!\!\perp\vec{\mathbf{X}}^{\prime\prime}|M\). Substituting the characteristic functions of \(\vec{\mathbf{X}}^{\prime\prime}\) and \(\vec{\boldsymbol{\epsilon}}\) from (162) and (164), respectively, in (168)

\[=\exp\left(-\left(\vec{\mathbf{t}}^{T}(\vec{\mathbf{K}}_{Y}(\vec{ \mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})^{T} \vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}+i\vec{\mathbf{t}}^{T}\vec{ \mathbf{K}}_{Y}m\right)\times\] \[\exp\left(-\left(\vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y}}\vec{ \mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}+\left(\vec{\mathbf{t}}^{T}(\vec{ \mathbf{K}}_{Y}(\vec{\mathbf{K}}_{X}^{1})^{T})(\vec{\mathbf{K}}_{Y}(\vec{\mathbf{K }}_{X}^{1})^{T})^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}\right),\] \[=\exp\left(-\left(\vec{\mathbf{t}}^{T}\mathbf{I}_{d_{Y}}\vec{ \mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}+i\vec{\mathbf{t}}^{T}\vec{ \mathbf{K}}_{Y}m\right).\] (169)

By inspecting the characteristic function shown in (169), we can conclude:

\[Q_{MC}(\vec{\mathbf{Y}}^{\prime}|M=m)=p_{CS-EC}(\alpha,\mathbf{I}_{d_{Y}},\vec{ \mathbf{K}}_{Y}m).\] (170)

Specifying \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{Y}}^{\prime})\): Lastly, we specify \(Q_{MC}(\vec{\mathbf{Y}}|\vec{\mathbf{Y}}^{\prime})\) through the following deterministic transformation:

\[\vec{\mathbf{Y}}=(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}\vec{\mathbf{Y} }^{\prime}+\vec{\mathbf{b}}_{Y}.\] (171)

Finally, we construct the desired \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) from \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{\prime},\vec{\mathbf{X}}^{\prime \prime},\vec{\mathbf{Y}}^{\prime},\vec{\mathbf{Y}})\) by marginalizing \(\vec{\mathbf{X}}^{\prime},\vec{\mathbf{X}}^{\prime\prime},\) and \(\vec{\mathbf{Y}}^{\prime}\).

**Part 2: Showing \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\), we first need to derive \(Deriving \(Q(\vec{\mathbf{Y}}|M)\): We use lemma 11 to calculate \(Q_{MC}(\vec{\mathbf{Y}}|M=m)\), as \(Q_{MC}(\vec{\mathbf{X}}|M=m)\) is an elliptically contoured multivariate continuous stable distribution for a fixed \(m\) (see (170)), and (171) defines a scaling and translation operation.

\[Q_{MC}(\vec{\mathbf{X}}^{\prime}|M=m) =p_{CS-EC}\left(\alpha,(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}} )^{T}\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}},(\mathbf{\Sigma}_{Y}^{ \nicefrac{{1}}{{2}}})^{T}\vec{\mathbf{K}}_{Y}m+\vec{\mathbf{b}}_{Y}\right),\] \[\stackrel{{(a)}}{{=}}p_{CS-EC}\left(\alpha,\mathbf{ \Sigma}_{Y},(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}\vec{\mathbf{K}}_{Y }m+\vec{\mathbf{b}}_{Y}\right),\] \[\stackrel{{(b)}}{{=}}p_{CS-EC}\left(\alpha,\mathbf{ \Sigma}_{Y},(\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}(\mathbf{\Sigma}_{Y }^{\nicefrac{{-1}}{{2}}})^{T}\vec{\mathbf{H}}_{Y}m+\vec{\mathbf{b}}_{Y} \right),\] (172) \[=p_{CS-EC}\left(\alpha,\mathbf{\Sigma}_{Y},\vec{\mathbf{H}}_{Y}m+ \vec{\mathbf{b}}_{Y}\right),\] (173)

where \((a)\) is due to the fact that \((\mathbf{\Sigma}_{Y}^{\nicefrac{{1}}{{2}}})^{T}\mathbf{\Sigma}_{Y}^{ \nicefrac{{1}}{{2}}}=\mathbf{\Sigma}_{Y}\), and \((b)\) is due to the fact that \(\vec{\mathbf{K}}_{Y}=\mathbf{\Sigma}_{Y}^{\nicefrac{{-1}}{{2}}}\vec{\mathbf{H} }_{Y}\). Comparing (173) with \(P(\vec{\mathbf{Y}}|M=m)\) shown in Appx. H.2, we can conclude:

\[Q_{MC}(\vec{\mathbf{Y}}|M)=P(\vec{\mathbf{Y}}|M).\] (174)

From (174) and (158), we can conclude:

\[Q_{MC}(M,\vec{\mathbf{Y}})=Q_{MC}(M)Q_{MC}(\vec{\mathbf{Y}}|M)=P(M)P(\vec{ \mathbf{Y}}|M)=P(M,\vec{\mathbf{Y}}).\] (175)

We know that by construction:

\[Q_{MC}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}}).\] (176)

Hence, \(Q_{MC}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;\vec{\mathbf{Y}}\setminus\vec{\mathbf{X}})=0\), concluding our proof. 

**Lemma 11**.: _Let \(\vec{\mathbf{X}}\sim p_{CS-EC}(\alpha,\mathbf{\Sigma},\vec{\boldsymbol{\mu}})\) be a \(d\)-dimensional random vector, where \(\mathbf{\Sigma}\) is a \(d\times d\) positive definite matrix, \(\vec{\boldsymbol{\mu}}\in\mathbb{R}^{d}\), and \(\alpha\in(0,2]\). Furthermore, let \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(\vec{\mathbf{b}}\in\mathbb{R}^{d}\). Then,_

\[\mathbf{A}\vec{\mathbf{X}}+\vec{\mathbf{b}}\sim p_{CS-EC}(\alpha,\mathbf{A} \mathbf{\Sigma}\mathbf{A}^{T},\mathbf{A}\vec{\boldsymbol{\mu}}+\vec{\mathbf{ b}}).\]

Proof.: Calculating the characteristic function of \(\mathbf{A}\vec{\mathbf{X}}+\vec{\mathbf{b}}\):

\[\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{T}(\mathbf{A}\vec{ \mathbf{X}}+\vec{\mathbf{b}})\right)\right]=\mathbb{E}\left[\exp\left(i\vec{ \mathbf{t}}^{T}\mathbf{A}\vec{\mathbf{X}}+i\vec{\mathbf{t}}^{T}\vec{\mathbf{ b}}\right)\right]=\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{T}\mathbf{A} \vec{\mathbf{X}}\right)\exp\left(i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}\right) \right],\]

Employing linearity of expectation to pull the term \(\exp\left(i\vec{\mathbf{t}}^{T}\vec{\mathbf{b}}\right)\) out of the expectation, as it is constant with respect to \(\vec{\mathbf{X}}\):

\[\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{T}(\mathbf{A}\vec{ \mathbf{X}}+\vec{\mathbf{b}})\right)\right]=\exp\left(i\vec{\mathbf{t}}^{T} \vec{\mathbf{b}}\right)\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{T}\mathbf{ A}\vec{\mathbf{X}}\right)\right].\] (177)

Substituting \(\vec{\mathbf{t}}^{\prime}=\vec{\mathbf{t}}^{\,\,\prime}\mathbf{A}\) in (177):

Substituting the formula of the characteristic function of an Elliptically-contoured multivariate stable distribution (given in (156)) for calculating \(\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{\,\,\prime}\vec{\mathbf{X}} \right)\right]\) as \(\vec{\mathbf{X}}\sim p_{CS-EC}(\alpha,\mathbf{\Sigma},\vec{\boldsymbol{\mu}})\):

\[\mathbb{E}\left[\exp\left(i\vec{\mathbf{t}}^{\,\,\prime}(\mathbf{A}\vec{ \mathbf{X}}+\vec{\mathbf{b}})\right)\right] =\exp\left(i\vec{\mathbf{t}}^{\,\,\prime}\vec{\mathbf{b}} \right)\exp\left(-\left((\vec{\mathbf{t}}^{\,\,\prime})^{T}\mathbf{\Sigma}\vec{ \mathbf{t}}^{\,\,\prime}\right)^{\nicefrac{{\alpha}}{{2}}}+i(\vec{\mathbf{t}}^{ \,\,\prime})^{T}\vec{\boldsymbol{\mu}}\right).\]

Re-substituting \(\vec{\mathbf{t}}^{\prime}=\vec{\mathbf{t}}^{T}\mathbf{A}\) in the above equation

\[=\exp\left(i\vec{\mathbf{t}}^{\,\,\prime}\vec{\mathbf{b}}\right) \exp\left(-\left(\vec{\mathbf{t}}^{\,\,\prime}\mathbf{A}\mathbf{\Sigma} \mathbf{A}^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}+i\vec{\mathbf{t }}^{\,\,T}\mathbf{A}\vec{\boldsymbol{\mu}}\right),\] \[=\exp\left(-\left(\vec{\mathbf{t}}^{\,\,\prime}\mathbf{A}\mathbf{ \Sigma}\mathbf{A}^{T}\vec{\mathbf{t}}\right)^{\nicefrac{{\alpha}}{{2}}}+i\vec{ \mathbf{t}}^{\,\,\prime}(\mathbf{A}\vec{\boldsymbol{\mu}}+\vec{\mathbf{b}}) \right).\] (178)By inspecting the characteristic function described in (178), we can determine that \(\mathbf{A}\mathbf{\ddot{X}}+\mathbf{\ddot{b}}\) is distributed according to an elliptically-contoured multivariate stable distribution, i.e., \(\mathbf{A}\mathbf{\ddot{X}}+\mathbf{\ddot{b}}\sim p_{CS-EC}\left(\alpha,\mathbf{A }\mathbf{\Sigma}\mathbf{A}^{T},\mathbf{A}\mathbf{\ddot{\mu}}+\mathbf{\ddot{b}}\right)\). 

**Lemma 12**.: _Let \(\mathbf{\ddot{H}}_{1}\in\mathbb{R}^{d_{1}}\) and \(\mathbf{\ddot{H}}_{2}\in\mathbb{R}^{d_{2}}\). Then, \(\|\mathbf{\ddot{H}}_{1}\|_{2}\geq\|\mathbf{\ddot{H}}_{2}\|_{2}\) if and only if \(\exists\) a matrix \(\mathbf{A}\in\mathbb{R}^{d_{2}\times d_{1}}\) such that \(\mathbf{\ddot{H}}_{2}=\mathbf{A}\mathbf{\ddot{H}}_{1}\), and \(\mathbf{A}\mathbf{A}^{T}\preceq\mathbf{I}_{d_{2}}\), with \(\mathbf{A}=\mathbf{\ddot{H}}_{2}(\mathbf{\ddot{H}}_{1}^{1})^{T}\), where \(\mathbf{B}\preceq\mathbf{C}\) means \(\mathbf{B}-\mathbf{C}\) is a positive semi-definite matrix, \(\mathbf{\ddot{H}}_{1}^{1}\) is as defined in lemma 10, and \(\mathbf{I}_{d_{2}}\) is \(d_{2}\times d_{2}\) identity matrix._

Proof.: See the proof of lemma 5 in [48] for the special case \(t=1\). 

## Appendix I Proof of Theorem 4

In this section, we provide the proof of theorem 4. Since theorem 4 provides the analytical PID terms for the univariate affine discrete stable system defined in Sec. 4, we briefly restate certain key properties of the univariate discrete stable distributions and the corresponding univariate affine discrete stable system for convenience.

### Univariate discrete stable distribution

Univariate discrete stable distributions are the discrete analogues of univariate continuous stable distributions. We refer the reader to Appx. N.4 for more details on univariate discrete stable distributions. We now list certain key properties of univariate continuous stable distributions that we make use of in the proof of theorem 4:

1. If \(X\) is distributed according to a univariate discrete stable distribution, then \(X\stackrel{{ d}}{{=}}\gamma\circ X_{1}+(1-\gamma^{\nu})^{\nicefrac{{ 1}}{{\nu}}}\circ X_{2}\), where \(X_{1}\) and \(X_{2}\) are two independent copies of \(X\), \(\nu\in(0,1]\), \(\gamma\in[0,1]\), and \(\circ\) denotes the binomial thinning operation.
2. The p.m.f. of discrete stable distributions are characterized by two parameters: exponent \(\nu\in(0,1]\), and rate parameter \(\tau\in(0,\infty)\). We denote the p.m.f. of a univariate discrete stable distribution as \(P_{DS}(\nu,\tau)\). In general, discrete stable distributions do not have a "nice" analytical form consisting of well-known elementary functions. Consequently, the univariate discrete stable distribution are typically expressed through their probability generating function.
3. The probability generating function of a discrete random variable \(N\) having a stable discrete distribution is given by (179): \[\mathbb{P}_{N}(z)=\exp(-\tau(1-z)^{\gamma}).\] (179)

### Definition of univariate affine discrete stable system

Let \(M\),\(X\), and \(Y\) be a system of random variables with the joint distribution \(P(M,X,Y)\). The joint distribution \(P(M,X,Y)\) describes the univariate affine discrete stable system if it satisfies the following two properties:

1. \(M\sim P(M)\) having some support set \(\mathcal{M}\subseteq(0,\infty)\).
2. The conditional distributions of random variables \(X\) and \(Y\) conditioned on \(M\) can be expressed through discrete stable family distributions with an affine dependence on \(M\). Formally, \(P(X|M\!=\!m)\!=\!P_{DS}(\nu,am+b)\) and \(P(Y|M\!=\!m)\!=\!P_{DS}(\nu,cm+d)\), where \(a,b,c,d\in(0,\infty)\).

### Formal proof of Theorem 4

**Theorem 4**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) describes a univariate affine discrete stable system. Without the loss of generality, assume \(a\geq c\). If \(\nicefrac{{a}}{{b}}\geq\nicefrac{{c}}{{d}}\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._Proof.: We first note that we can always assume \(a\geq c\) without the loss of generality because if \(a\leq c\), then we can always switch our nomenclature to refer to \(Y\) as \(X\), and \(X\) as \(Y\).

We briefly outline the proof structure.

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structure \(M\to X\to Y\).
2. In the second part, we show that the \(Q_{MC}(M,X,Y)\) constructed in the first part lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;Y\backslash X)=0\).

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**

We explicitly construct the desired Markov chain \(M\to X\to Y\) by constructing a larger Markov chain \(M\to X\to X^{\prime}\to Y\) and then marginalizing the larger Markov chain to obtain the desired Markov chain \(M\to X\to Y\).

Denote the joint distribution of the Markov chain \(M\to X\to X^{\prime}\to Y\) as \(Q_{MC}(M,X,X^{\prime},Y)\). We can decompose \(Q_{MC}(M,X,X^{\prime},Y)\) by utilizing its Markovian structure as follows:

\[Q_{MC}(M,X,X^{\prime},Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(X^{\prime}|X)Q_{MC}(Y|X^{ \prime}).\]

Consequently, we can specify/construct the distribution \(Q_{MC}(M,X,X^{\prime},Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), \(Q_{MC}(X^{\prime}|X)\), and \(Q_{MC}(Y|X^{\prime})\).

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\): We choose \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M)\text{ and }\,Q_{MC}(X|M)=P(X|M),\] (180)

where \(P(M)\) and \(P(X|M)\) are marginal distributions derived from the original joint distribution \(P(M,X,Y)\) (discussed in the theorem statement) over which the bivariate PID is being calculated.

Specifying \(Q_{MC}(X^{\prime}|X)\): \(Q_{MC}(X^{\prime}|X)\) is specified through a binomial thinning operation, specifically:

\[X^{\prime}=X\circ(\nicefrac{{c}}{{a}})^{\nicefrac{{1}}{{\nu}}}.\] (181)

Note that the above binomial thinning operation is valid as \(c\leq a\) by the assumption in the theorem statement. We also derive \(Q_{MC}(X^{\prime}|M)\) as it is needed for showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

_Deriving \(Q_{MC}(X^{\prime}|M)\)_: We use lemma 13 to derive \(Q_{MC}(X^{\prime}|M=m)\) as \(Q_{MC}(X|M=m)\) describes a univariate discrete stable distribution for a fixed \(m\), and \(X^{\prime}\) is a binomially-thinned version of \(X\). The exact expression of \(Q_{MC}(X^{\prime}|M=m)\) is provided in (182).

\[Q_{MC}(X^{\prime}|M=m) =P_{DS}(\nu,(\nicefrac{{c}}{{a}})^{\nicefrac{{\epsilon}}{{\nu}} }(am+\nicefrac{{b}}{{a}})),\] \[=P_{DS}(\nu,cm+\nicefrac{{cb}}{{a}}).\] (182)

Specifying \(Q_{MC}(Y|X^{\prime})\): \(Q_{MC}(Y|X^{\prime})\) is specified through the following stochastic transformation:

\[Y=X^{\prime}+\epsilon,\] (183)

where \(\epsilon\sim P_{DS}\left(\nu,d-\nicefrac{{cb}}{{a}}\right)\). For \(Q_{MC}(Y|X^{\prime})\) to be a valid distribution, we need to ensure that \(\epsilon\) is distributed according to a legitimate discrete stable distribution. In order to show \(\epsilon\) is distributed according to a legitimate discrete stable distribution, we need to ensure that \(\nu\) and \(d-\nicefrac{{cb}}{{a}}\) lie within their appropriate bounds as specified in Sec. I.1. It is trivial to see \(\nu\in(0,1]\).

From the assumptions in the theorem statement, we know that

\[\frac{a}{b}\geq\frac{c}{d}\Rightarrow d\geq\frac{bc}{a}\Rightarrow d-\frac{ bc}{a}\geq 0.\] (184)

Hence, \(\epsilon\) follows a legitimate discrete stable distribution. Furthermore, we choose \(\epsilon\perp\!\!\!\perp(M,X,X^{\prime})\).

Finally, we construct the desired \(Q_{MC}(M,X,Y)\) from \(Q_{MC}(M,X,X^{\prime},Y)\) by marginalizing \(X^{\prime}\).

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to derive \(Q_{MC}(Y|M)\).

Deriving \(Q(Y|M)\): Note that \(\epsilon\) follows discrete stable distribution \(P_{DS}(\nu,d-\nicefrac{{bc}}{{a}})\) and \(\overline{Q_{MC}(X^{\prime}|M=m)}\) is also a discrete stable distribution \(P_{DS}(X^{\prime};\nu,cm+\nicefrac{{bc}}{{a}})\) for a fixed \(m\) (see (182)). Furthermore, \(\epsilon\perp\!\!\!\perp X^{\prime}|M\). Hence, using lemma 14, we have

\[Q_{MC}(Y|M=m)=P_{DS}(\nu,cm+\nicefrac{{bc}}{{a}}-\nicefrac{{bc}}{{a}}+d)=P_{ DS}(\nu,cm+d).\] (185)

As \(P(Y|M)=P_{DS}(\nu,cM+d)\) (see Appx. I.2), we have:

\[Q(Y|M)=P(Y|M).\] (186)

From (180) and (186), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (187) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (188)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\), concluding our proof. 

**Lemma 13**.: _Let \(N\sim P_{DS}(\nu,\tau)\). Then, \(N_{1}=\gamma\circ N\sim P_{DS}(\nu,\gamma^{\nu}\tau)\;\forall\;\gamma\in[0,1]\), where \(\circ\) is the binomial thinning operation._

Proof.: The probability generating function of \(N\), denoted as \(\mathbb{P}_{N}(z)\), can be described using (179):

\[\mathbb{P}_{N}(z)=\exp\left(-\tau(1-z)^{\nu}\right).\] (189)

For calculating the probability generating function of \(N_{1}\), we use the following relationship between the probability generating function of a random variable \(R\) and its binomially-thinned version \(R\circ p\), with \(p\in[0,1]\):

\[\mathbb{P}_{R\circ p}(z)=\mathbb{P}_{R}(1-p(1-z))\text{ (see equation \ref{eq:p1} in \@@cite[cite]{[\@@bibref{}{P1}{}{}]}).}\] (190)

From (189) and (190), we can calculate the probability generating function of \(N_{1}\) as follows:

\[\mathbb{P}_{N_{1}}(z) =\mathbb{P}_{N\circ\gamma}(z)=\mathbb{P}_{N}(1-\gamma(1-z))=\exp \left(-\tau\left(1-(1-\gamma(1-z))\right)^{\nu}\right)\] \[=\exp\left(-\tau\left(\gamma(1-z)\right)^{\nu}\right)=\exp\left(- \tau\gamma^{\nu}\left(1-z\right)^{\nu}\right).\] (191)

Inspecting the probability generating function of \(N_{1}\) in (191), we can conclude that \(N_{1}\sim P_{DS}(\nu,\gamma^{\nu}\tau)\). 

**Lemma 14**.: _Let \(N_{1}\sim P_{DS}(\nu,\tau_{1})\), \(N_{2}\sim P_{DS}(\nu,\tau_{2})\), and \(N_{1}\perp\!\!\!\perp N_{2}\). Then, \(N_{1}+N_{2}=N\sim P_{DS}(n;\nu,\tau_{1}+\tau_{2})\)._

Proof.: The probability generating function of \(N_{1}\) and \(N_{2}\), denoted as \(\mathbb{P}_{N_{1}}(z)\) and \(\mathbb{P}_{N_{2}}(z)\), can be described using (179):

\[\mathbb{P}_{N_{1}}(z)=\exp\left(-\tau_{1}(1-z)^{\nu}\right)\text{ and }\mathbb{P}_{N_{2}}(z)=\exp\left(-\tau_{2}(1-z)^{\nu}\right).\] (192)

Calculating the probability generating function of \(N=N_{1}+N_{2}\),

\[\mathbb{P}_{N}(z) =\mathbb{P}_{N_{1}+N_{2}}(z)\overset{(a)}{=}\mathbb{P}_{N_{1}}( z)\mathbb{P}_{N_{2}}(z)\overset{(b)}{=}\exp\left(-\tau_{1}\left(1-z\right)^{ \nu}\right)\exp\left(-\tau_{2}\left(1-z\right)^{\nu}\right),\] \[=\exp\left(-(\tau_{1}+\tau_{2})\left(1-z\right)^{\nu}\right)\] (193)

where \((a)\) is due to the fact that \(N_{1}\perp\!\!\!\perp N_{2}\), and \((b)\) is achieved by substituting the explicit forms of \(\mathbb{P}_{N_{1}}(z)\) and \(\mathbb{P}_{N_{2}}(z)\) from (192). Inspecting the probability generating function of \(N_{1}+N_{2}\) in (193), we can conclude that \(N_{1}+N_{2}=N\sim P_{DS}(n_{1};\nu,\tau_{1}+\tau_{2})\). 

## Appendix J Proof of Theorem 5

In this section, we provide the proof of theorem 5. Since theorem 5 provides the analytical PID terms for the multivariate linear Poisson system defined in Sec. 4, we briefly restate certain key properties of multivariate Poisson distributions and the multivariate linear Poisson system for convenience.

### Multivariate Poisson Distribution

We use the multivariate Poisson distribution proposed in [42]. For more details, see Appx. N.5. We list certain key properties of multivariate Poisson distributions that we make use of in the proof of theorem 5:

1. Under the definition of the multivariate Poisson distribution proposed in [42], each component of the random vector is a sum of independent Poisson random, i.e., \[\vec{\mathbf{N}}=\mathbf{A}\vec{\mathbf{N}}^{g},\] (194) where \(\mathbf{A}\) is an appropriate matrix of \(0\)'s and \(1\)'s. We can decompose \(\mathbf{A}=[\mathbf{A}_{1}\quad\ldots\quad\mathbf{A}_{d^{\prime}}]\), where \(\mathbf{A}_{i}\) is a \(d\times\binom{d}{i}\) submatrix having no duplicate columns, and each of its columns containing exactly \(i\) ones and \((d-i)\) zeros [44], and \(\vec{\mathbf{N}}^{g}=[N_{1}^{g}\ldots N_{d}^{g}\ \ N_{12}^{g}\ldots\ N_{(d-1)d}^{g} \ldots\ N_{d-(d^{\prime}-1)\ldots d}^{g}]^{T}\), with \(N_{i_{1}\ldots i_{j}}^{g}\sim\text{Poisson}(\lambda_{i_{1}\ldots i_{j}})\ \forall\ (i_{1},\ldots,i_{j})\in \mathbb{A}_{j}^{d},j\in[d^{\prime}]\). Furthermore, the random variables \(\{N_{1},\ldots,N_{d-(d^{\prime}-1)\ldots d}\}\) are mutually independent.
2. We denote the p.m.f. of the multivariate Poisson distribution as \(\text{Poisson}(d,d^{\prime},\vec{\mathbf{A}})\), where \(d\geq d^{\prime}\), and \[\vec{\mathbf{\Lambda}}=\left[\lambda_{1}\ \ldots\ \lambda_{d}\ \lambda_{12}\ \ldots\lambda_{d-(d^{\prime}-1)\ldots d}\right]^{T}.\] (195)
3. For \(d^{\prime}=1\), we have that \(\vec{\mathbf{N}}\) is a collection of independent Poisson random variables, and when both \(d=d^{\prime}=1\), we recover the scalar Poisson distribution: \(\Pr(N=n)=\frac{e^{-\lambda}\lambda^{n}}{n!},\ \forall\ n\in\mathbb{N}_{0}\).
4. Let \(\vec{\mathbf{N}}\sim\text{Poisson}(d,d^{\prime},\vec{\mathbf{\Lambda}})\), where \(\vec{\mathbf{N}}\) is a \(d\)-dimensional random vector. Then the corresponding p.m.f. of \(\vec{\mathbf{N}}\) is described below: Let \(\vec{\mathbf{n}}^{\prime}\!=\!\left[n_{12}\ \ldots\ n_{(d-1)d}\ \ldots\ n_{d-(d^{\prime}-1)\ldots d}\right]^{T}\), and \(d_{\vec{\mathbf{n}}^{\prime}}\) be the dimension of \(\vec{\mathbf{n}}^{\prime}\), then: \[P(\vec{\mathbf{N}}\!=\!\vec{\mathbf{n}})\!=\!e^{-\vec{\mathbf{ T}}^{T}\vec{\mathbf{\Lambda}}}\prod_{i=1}^{d}\lambda_{i}^{n_{i}}\sum_{\vec{ \mathbf{n}}^{\prime}\in C}\left(\prod_{(i_{1},i_{2})\in\mathbb{A}_{2}^{d}} \left(\frac{\lambda_{i_{1}i_{2}}}{\lambda_{i_{1}}\lambda_{i_{2}}}\right)^{n_{ i_{1}i_{2}}}\times\right.\] \[\ldots\times\prod_{(i_{1},\ldots,i_{d^{\prime}})\in\mathbb{A}_{d ^{\prime}}^{d}}\left(\frac{\lambda_{i_{1}\ldots i_{d^{\prime}}}}{\prod_{j=1}^ {d^{\prime}}\lambda_{i_{j}}}\right)^{n_{i_{1}\ldots i_{d^{\prime}}}}\times Q( \vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime})\left.\right),\] (196) where \(C=\{\vec{\mathbf{n}}^{\prime}\in\mathbb{N}_{0}^{d_{\mathbf{n}}^{\prime}}:( \vec{\mathbf{a}}_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime}\leq n_{i}\ \forall\ i\in[d]\}\), and \[Q(\vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime})=\prod_{i=1}^{d}\frac{1}{(n_{i}- \vec{\mathbf{a}}_{i}^{\prime T}\vec{\mathbf{n}}^{\prime})^{\dagger}}\prod_{j= 2}^{d^{\prime}}\prod_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}\frac{1}{n_{ i_{1}\ldots i_{j}}^{g}!},\] with \(\vec{\mathbf{a}}_{i}^{\prime}\) being the \(i\)-th row of the matrix \(\mathbf{A}^{\prime}=[\mathbf{A}_{2}\ldots\mathbf{A}_{d^{\prime}}]\).

### Definition of the multivariate linear Poisson system

Let the random variable \(M\sim P(M)\). \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) are \(d_{X}\)-dimensional and \(d_{Y}\)-dimensional random vectors, respectively. The joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) describes the multivariate linear Poisson system if it satisfies the following two properties:

1. \(M\sim P(M)\) having some support set \(\mathcal{M}\subseteq(0,\infty)\).
2. The conditional distributions of random vectors \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) are multivariate Poisson distributions, i.e., \(P(\vec{\mathbf{X}}|M)\!=\!\text{Poisson}(d_{X},d_{X}^{\prime},\vec{\mathbf{ \Lambda}}_{X})\) and \(P(\vec{\mathbf{Y}}|M)\!=\!\text{Poisson}(d_{Y},d_{Y}^{\prime},\vec{\mathbf{ \Lambda}}_{Y})\), with: \[\vec{\mathbf{\Lambda}}_{X}\!=\!\left[\lambda_{1}^{X}\ \ldots\ \lambda_{d_{X}-(d_{X}^{\prime}-1)\ldots d_{X}}^{X}\right]^{T},\lambda_{i_{1} \ldots i_{j}}^{X}=\gamma_{i_{1}\ldots i_{j}}^{X}M^{j}\ \forall\ j\in[d_{X}]\ \text{and}\ (i_{1},...,i_{j})\in \mathbb{A}_{j}^{d_{X}},\] \[\mathbf{\Lambda}_{Y}=\left[\lambda_{1}^{Y}\ \ldots\ \lambda_{d_{Y}-(d_{Y}^{ \prime}-1)\ldots d_{Y}}^{Y}\right]^{T},\lambda_{i_{1}\ldots i_{j}}^{Y}=\gamma_{i_ {1}\ldots i_{j}}^{X}M^{j}\ \forall\ j\in[d_{Y}]\ \text{and}\ (i_{1},...,i_{j})\in \mathbb{A}_{j}^{d_{Y}}.\] Here, \(\gamma_{1}^{X},\ldots,\gamma_{d_{X}-(d_{X}^{\prime}-1)\ldots d_{X}}^{X},\gamma_{1 }^{Y},\ldots,\gamma_{d_{Y}-(d_{Y}^{\prime}-1)\ldots d_{Y}}^{Y}\in\mathbb{R}^{+}\).

Furthermore, let \(\mathbf{A}_{X}\), and \(\mathbf{A}_{Y}\) be the corresponding \(\mathbf{A}\)-matrices (defined in (194)) associated with \(P(\vec{\mathbf{X}}|M)\) and \(P(\vec{\mathbf{Y}}|M)\), respectively.

### Formal proof of Theorem 5

**Theorem 5**.: _Let the joint distribution \(P(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) of \(M\), \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{Y}}\) describe the multivariate linear Poisson system. Without the loss of generality, assume \(d_{X}^{\prime}\geq d_{Y}^{\prime}\). If \(\sum_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{X}}}\gamma_{i_{1}\ldots i_{j}} ^{X}\geq\sum_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{Y}}}\gamma_{i_{1} \ldots i_{j}}^{Y}\ \forall\ j\in[d_{Y}^{\prime}]\), then \(\Delta_{P}\) contains a Markov chain of the form \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{Y}}\) and \(UI(M;\vec{\mathbf{Y}}\backslash\vec{\mathbf{X}})=0\)._

Proof.: We first note that we can always assume \(d_{X}^{\prime}\geq d_{Y}^{\prime}\) without the loss of generality because if \(d_{X}^{\prime}\leq d_{Y}^{\prime}\), then we can always switch our nomenclature to refer to \(\vec{\mathbf{Y}}\) as \(\vec{\mathbf{X}}\), and \(\vec{\mathbf{X}}\) as \(\vec{\mathbf{Y}}\).

We provide an explicit construction of the Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{g}\rightarrow\vec{ \mathbf{Y}}^{g}\rightarrow\vec{\mathbf{Y}}\) having the marginals \(P(M,\vec{\mathbf{X}})\), and \(P(M,\vec{\mathbf{Y}})\). Denote the joint density of \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{g}\rightarrow\vec{ \mathbf{Y}}^{g}\rightarrow\vec{\mathbf{Y}}\) as \(Q(M,\vec{\mathbf{X}},\vec{\mathbf{X}}^{g},\vec{\mathbf{Y}}^{g},\vec{\mathbf{Y }})\). For \(Q(M)\) and \(Q(\vec{\mathbf{X}}|M)\), we choose them to be equal to \(P(M)\) and \(P(\vec{\mathbf{X}}|M)\), i.e. \(Q(M)=P(M)\), and \(Q(\vec{\mathbf{X}}|M)=P(\vec{\mathbf{X}}|M)\). Note that due to this construction, \(P(M,\vec{\mathbf{X}})=Q(M,\vec{\mathbf{X}})\) holds trivially.

For \(Q(\vec{\mathbf{X}}^{g}|\vec{\mathbf{X}})\), we use the result described in lemma 17. Note that the construction for \(Q(\vec{\mathbf{X}}^{g}|\vec{\mathbf{X}})\) is not explicit but rather implicit. Let \(d_{\vec{\mathbf{A}}_{X}}\) be the dimension of \(\vec{\mathbf{A}}_{X}\), then we explicitly choose: \(Q(\vec{\mathbf{X}}|\vec{\mathbf{X}}^{g},M)=\delta_{K}(\vec{\mathbf{X}}= \mathbf{A}_{X}\vec{\mathbf{X}}^{g}),\) and \(Q(\vec{\mathbf{Y}}^{g}|M)\!=\!\mathrm{Poisson}(d_{\vec{\mathbf{A}}_{X}},1, \vec{\mathbf{A}}_{\mathbf{Y}})\). Note that \(Q(\vec{\mathbf{X}}|M)\!=\!\mathrm{Poisson}(d_{X},d_{X}^{\prime},\vec{\mathbf{ A}}_{X})\) by construction, and we derive \(Q(\vec{\mathbf{X}}^{g}|\vec{\mathbf{X}},M)\) through the Bayes theorem. Here, \(\delta_{K}(\cdot)\) is the Kronecker delta function [59]. By lemma 17, we know that \(Q(\vec{\mathbf{X}}^{g}|\vec{\mathbf{X}},M)=Q(\vec{\mathbf{X}}^{g}|\vec{ \mathbf{X}})\), and hence we have the Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{g}\).

For choosing \(Q(\vec{\mathbf{Y}}^{g}|\vec{\mathbf{X}}^{g})\), we rely on the result of lemma 18. Let us define the random vectors:

\[\vec{\mathbf{X}}_{j}\!=\!\left[X_{i_{1}\ldots i_{j}}^{g}\right]_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{1}}}^{T},\ \text{and}\ \vec{\mathbf{Y}}_{j}\!=\!\left[Y_{i_{1}\ldots i_{j}}^{g}\right]_{(i_{1}, \ldots,i_{j})\in\mathbb{A}_{j}^{d_{2}}}^{T},\]

i.e., \(\vec{\mathbf{X}}_{j}\) and \(\vec{\mathbf{Y}}_{j}\) are random vectors containing all terms of the form \(X_{i_{1},\ldots,i_{j}}^{g}\), and \(Y_{k_{1},\ldots,k_{j}}^{g}\), where \((i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{1}}\) and \((k_{1},\ldots,k_{j})\in\mathbb{A}_{j}^{d_{2}}\), respectively. Note that we can write \(\vec{\mathbf{X}}^{g}\!=\!\left[\vec{\mathbf{X}}_{1}^{T}\ \ldots\ \vec{\mathbf{X}}_{d_{1}^{ \prime}}^{T}\right]^{T}\), and we define \(\vec{\mathbf{Y}}^{g}=\left[\vec{\mathbf{Y}}_{1}^{T}\ \ldots\ \vec{\mathbf{Y}}_{d_{2}^{ \prime}}^{T}\right]^{T}\). Then, we construct \(Q(\vec{\mathbf{Y}}^{g}|\vec{\mathbf{X}}^{g})\) as a product of \(d_{Y}^{\prime}\) multinomial distributions, described below:

\[Q(\vec{\mathbf{Y}}^{g}|\vec{\mathbf{X}}^{g})=\prod_{j=1}^{d_{Y}^{\prime}}Q(\vec{ \mathbf{Y}}_{j}|\vec{\mathbf{X}}_{j}),\]

where \(Q(\vec{\mathbf{Y}}_{j}\!=\!\vec{\mathbf{y}}_{j}|\vec{\mathbf{X}}_{j}\!=\!\vec{ \mathbf{x}}_{j})=\mathrm{Multinomial}(\mathbf{k}_{j};N_{j},\mathbf{p}_{i})\), and

\[N_{j}=\vec{\mathbf{1}}^{T}\vec{\mathbf{x}}_{j},\ \vec{\mathbf{k}}_{j}=\left[\vec{\mathbf{y}}_{j}^{T}\ \ \ \ \vec{ \mathbf{1}}^{T}\vec{\mathbf{x}}_{j}-\vec{\mathbf{1}}^{T}\vec{\mathbf{y}}_{j} \right]^{T},\] \[\vec{\mathbf{p}}_{i}=\left[p_{1\ldots j}\ \cdots\ p_{d_{2}-(j-1)\ldots d_{2}}\ 1- \sum_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{2}}}p_{i_{1}\ldots i_{j}} \right]^{T},\] \[p_{i_{1}\ldots i_{j}}=\frac{\gamma_{i_{1}\ldots i_{j}}^{Y}}{\sum_{( i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{1}}}\gamma_{i_{1}\ldots i_{j}}^{X}}\ \forall\ (i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d_{2}}.\]

By construction \(\vec{\mathbf{X}}^{g}\) consists of mutually conditionally independent Poisson random variables (conditioned on \(M\)), and the condition of lemma 18 (specified in (219)) is satisfied by condition 1. Therefore, using the result of lemma 18 on the Markov chain \(M\rightarrow\vec{\mathbf{X}}^{g}\rightarrow\vec{\mathbf{Y}}^{g}\) (obtained by marginalizing \(\vec{\mathbf{X}}\)), we conclude that \(Q(\vec{\mathbf{Y}}^{g}|M)=\mathrm{Poisson}(1,d_{\vec{\mathbf{A}}_{Y}},\vec{ \mathbf{A}}_{Y})\), where \(d_{\vec{\mathbf{A}}_{Y}}\) is the dimension of \(\vec{\mathbf{A}}_{Y}\).

We specify \(Q(\vec{\mathbf{Y}}|\vec{\mathbf{Y}}^{g})\) through the following deterministic transformation \(\vec{\mathbf{Y}}=\mathbf{A}_{Y}\vec{\mathbf{Y}}^{g}\) to obtain the Markov chain \(M\rightarrow\vec{\mathbf{X}}\rightarrow\vec{\mathbf{X}}^{g}\rightarrow\vec{ \mathbf{Y}}^{g}\rightarrow\vec{\mathbf{Y}}\). Marginalizing \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{X}}^{g}\) in the above Markov chain,we get the following Markov chain: \(M\rightarrow\mathbf{\tilde{Y}}^{g}\rightarrow\mathbf{\tilde{Y}}\). Now, since \(\mathbf{\tilde{Y}}=\mathbf{A}_{Y}\mathbf{\tilde{Y}}^{g}\) and \(P(\mathbf{\tilde{Y}}^{g}|M)=\text{Poisson}(d_{\mathbf{\tilde{X}}_{Y}},1, \mathbf{\tilde{\Lambda}}_{Y})\), \(P(\mathbf{\tilde{Y}}|M)=\text{Poisson}(d_{Y},d^{\prime}_{Y},\Lambda_{Y})\) (by definition of the multivariate Poisson discussed in Sec. J.1). Since we have \(Q(M)=P(M)\) and \(Q(\mathbf{\tilde{Y}}|M)=P(\mathbf{\tilde{Y}}|M)\), we also have \(Q(M,\mathbf{\tilde{Y}})=P(M,\mathbf{\tilde{Y}})\). Marginalizing \(\mathbf{\tilde{X}}^{g}\) and \(\mathbf{\tilde{Y}}^{g}\) from the Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{X}}^{g}\rightarrow \mathbf{\tilde{Y}}^{g}\rightarrow\mathbf{\tilde{Y}}^{g}\rightarrow\mathbf{ \tilde{Y}}\) results in the desired Markov chain \(M\rightarrow\mathbf{\tilde{X}}\rightarrow\mathbf{\tilde{Y}}\) having \(Q(M,\mathbf{\tilde{X}})\!=\!P(M,\mathbf{\tilde{X}})\) and \(Q(M,\mathbf{\tilde{Y}})\!=\!P(M,\mathbf{\tilde{Y}})\), concluding our proof. 

**Lemma 15**.: _If \(N_{j}\sim\text{Poisson}(\lambda_{j})\ \forall\ j\in\{1,\ldots,d\}\), and all \(N_{j}\)'s are mutually independent, then \(P(N_{1},\ldots,N_{j}|\sum_{j=1}^{d}N_{j}=N)=\text{Multinomial}(N_{1},\ldots,N_ {j};N,\left[\nicefrac{{\lambda_{1}}}{{\sum_{i=1}^{d}\lambda_{j}}}\ \ \ldots\ \nicefrac{{ \lambda_{d}}}{{\sum_{i=1}^{d}\lambda_{j}}}\right])\)._

Proof.: See Section 3 of [60]. 

**Lemma 16**.: _Let \(\mathbf{\tilde{N}}_{1}=\left[N_{1}^{(1)}\ \ \ \ldots N_{d_{1}}^{(1)}\right]^{T}\), where \(N_{j}\sim\text{Poisson}(\lambda_{j}^{(1)})\ \forall\ j\in\{1,\ldots,d_{1}\}\) and all \(N_{j}\)'s are mutually independent from each other. Define \(P(\mathbf{\tilde{N}}_{2}|\mathbf{\tilde{N}}_{1})=\text{Multinomial}\left( \mathbf{\tilde{k}};N,\mathbf{\tilde{p}}\right)\), where_

\[N=\sum_{j=1}^{d_{1}}N_{j}^{(1)},\ \mathbf{\tilde{k}}=\left[N_{1}^{ (2)}\ \cdots\ N_{d_{2}}^{(2)}\ \sum_{j=1}^{d_{1}}N_{j}^{(1)}-\sum_{j=1}^{d_{2}}N_{j}\right]^{T},\] \[\mathbf{\tilde{p}}=\left[\frac{\lambda_{j}^{(2)}}{\sum_{j=1}^{d_{ 1}}\lambda_{j}^{(1)}}\ \cdots\ \frac{\lambda_{j}^{(2)}}{\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)}}\ 1-\frac{ \sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}}{\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)}} \right]^{T},\]

_where \(\{\lambda_{j}^{(2)}\}_{j=1}^{d_{2}}\) are some positive constants, i.e., \(\lambda_{j}^{(2)}\in(0,\infty)\), such that \(\sum_{i=1}^{d_{2}}\lambda_{j}^{(2)}\leq\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)}\). Then, \(\mathbf{\tilde{N}}_{2}=\left[N_{1}^{(2)}\ \ \ldots N_{d_{2}}^{(2)}\right]^{T}\), where \(N_{j}^{(2)}\sim\text{Poisson}(\lambda_{j}^{(2)})\), and all \(N_{j}^{(2)}\)'s are mutually independent form each other._

Proof.: Lemma 16 is a straight-forward generalization of lemma 15. Denote the probability of \(\mathbf{\tilde{N}}_{2}\) shown in the lemma statement as \(P(\mathbf{\tilde{N}}_{2})\). We will prove the above lemma by constructing a Markov chain \(\mathbf{\tilde{N}}_{1}\to N_{1}^{\prime}\to N_{2}^{\prime}\rightarrow \mathbf{\tilde{N}}_{2}\) with joint density \(Q(\mathbf{\tilde{N}}_{1},N_{1}^{\prime},N_{2}^{\prime},\mathbf{\tilde{N}}_{2})\), and showing that \(Q(\mathbf{\tilde{N}}_{2})=P(\mathbf{\tilde{N}}_{2})\) and \(Q(\mathbf{\tilde{N}}_{2}|\mathbf{\tilde{N}}_{1})=P(\mathbf{\tilde{N}}_{2}| \mathbf{\tilde{N}}_{1})\).

Specifying \(Q(N_{1}^{\prime}|\mathbf{\tilde{N}}_{1})\): We specify \(Q(N_{1}^{\prime}|\mathbf{\tilde{N}}_{1})\) through the following deterministic transformation:

\[N_{1}^{\prime}=\mathbf{\tilde{1}}^{T}\mathbf{\tilde{N}}_{1}=\sum_{j=1}^{d_{1}}N_ {j}^{(1)}.\] (197)

Calculating \(Q(N_{1}^{\prime})\): We use the property that the sum of independent Poisson random variables is a Poisson random variable with its rate parameter being the sum of the rate parameters of its summands [61]. Hence, \(Q(N_{1}^{\prime})=\text{Poisson}(\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)})\).

Specifying \(Q(N_{2}^{\prime}|N_{1}^{\prime})\): We specify \(Q(N_{2}^{\prime}|N_{1}^{\prime}=n_{1}^{\prime})\) as follows:

\[Q(N_{2}^{\prime}|N_{1}^{\prime}=n_{1}^{\prime})=\text{Binomial}\left(N_{2}^{ \prime};n_{1}^{\prime},\nicefrac{{\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}}}{{\sum_{j=1 }^{d_{1}}\lambda_{j}^{(1)}}}\right).\] (198)

Calculating \(Q(N_{2}^{\prime})\): We use lemma 15 to calculate \(Q(N_{2}^{\prime})\). Note that \(Q(N_{1}^{\prime})=\text{Poisson}(\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)})\), and \(Q(N_{2}^{\prime}|N_{1}^{\prime})=\text{Binomial}\left(N_{2}^{\prime};n_{1}^{ \prime},\nicefrac{{\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}}}{{\sum_{j=1}^{d_{1}} \lambda_{j}^{(1)}}}\right)\). Hence, by lemma 15, we have:

\[Q(N_{2}^{\prime})=\text{Poisson}\left(\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)}\times \frac{\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}}{\sum_{j=1}^{d_{1}}\lambda_{j}^{(1)}} \right)=\text{Poisson}\left(\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}\right).\] (199)Specifying \(Q(\vec{N}_{2}|N^{\prime}_{1})\): We specify \(Q(\vec{\mathbf{N}}_{2}|N^{\prime}_{2}=n^{\prime}_{2})\) as follows:

\[Q(\vec{\mathbf{N}}_{2}|N^{\prime}_{2}=n^{\prime}_{2})=\text{Multinomial}\left( \vec{\mathbf{N}}^{\prime}_{2};n^{\prime}_{2},\left[\frac{\lambda^{(2)}_{2}}{ \sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}\quad\dots\quad\frac{\lambda^{(2)}_{d_{2} }}{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}\right]^{T}\right).\] (200)

Calculating \(Q(\vec{\mathbf{N}}_{2})\): We again use lemma 15 to calculate \(Q(\vec{\mathbf{N}}_{2})\). Note that,

\[Q(N^{\prime}_{2})=\text{Poisson}\left(\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}\right),\text{ and }\]

\[Q(\vec{\mathbf{N}}_{2}|N^{\prime}_{2}=n^{\prime}_{2})=\text{Multinomial}\left( \vec{\mathbf{N}}^{\prime}_{2};n^{\prime}_{2},\left[\frac{\lambda^{(2)}_{2}}{ \sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}\quad\dots\quad\frac{\lambda^{(2)}_{d_{2} }}{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}\right]^{T}\right).\] (201)

Hence, by lemma 15, we have:

\[\vec{\mathbf{N}}_{2}=\left[N^{(2)}_{1}\quad\dots\quad N^{(2)}_{d_{2}}\right] ^{T},\text{ where }N^{(2)}_{j}\sim\text{Poisson}(\lambda^{(2)}_{j})\;\forall\;j\in[d_{2}].\] (202)

Hence, by (202), we know that \(Q(\vec{\mathbf{N}}_{2})=P(\vec{\mathbf{N}}_{2})\).

Calculating \(Q(\vec{\mathbf{N}}_{2}|\vec{\mathbf{N}}_{1})\):

\[Q(\vec{\mathbf{N}}_{2}=\vec{\mathbf{n}}_{2}|\vec{\mathbf{N}}_{1}=\vec{ \mathbf{n}}_{1}) =\sum_{n^{\prime}_{1},n^{\prime}_{2}=0}^{\infty}Q(N^{\prime}_{1}=n ^{\prime}_{1},N^{\prime}_{2}=n^{\prime}_{2},\vec{\mathbf{N}}_{2}=\vec{ \mathbf{n}}_{2}|\vec{\mathbf{N}}_{1}=\vec{\mathbf{n}}_{1}),\]

\[\overset{(a)}{=}\sum_{n^{\prime}_{1},n^{\prime}_{2}=0}^{\infty}Q(n^{\prime}_{1 }|\vec{\mathbf{n}}^{\prime}_{2})Q(n^{\prime}_{2}|n^{\prime}_{1})Q(\vec{\mathbf{ n}}_{2}|n^{\prime}_{2}),\]

where \((a)\) is due to the Markovian nature of \(Q(\vec{\mathbf{N}}_{1},N^{\prime}_{1},N^{\prime}_{2},\vec{\mathbf{N}}_{2})\). We drop the random variable notation for brevity. Let \(\vec{\mathbf{n}}_{1}=\left[n^{(1)}_{1}\quad\dots\quad n^{(1)}_{d_{1}}\right]^{T}\), and \(\vec{\mathbf{n}}_{2}=\left[n^{(2)}_{1}\quad\dots\quad n^{(2)}_{d_{2}}\right]^ {T}\). Then, substituting the exact form of \(Q(n^{\prime}_{1}|\vec{\mathbf{n}}_{1})\),\(Q(n^{\prime}_{2}|n^{\prime}_{1})\), and \(Q(\vec{\mathbf{n}}_{2}|n^{\prime}_{2})\) in the above equation:

\[=\sum_{n^{\prime}_{1},n^{\prime}_{2}=0}^{\infty}\mathbb{I}\left[n ^{\prime}_{1}=\sum_{j=1}^{d_{1}}n^{(1)}_{j}\right]\frac{(n^{\prime}_{1})!}{(n ^{\prime}_{1}-n^{\prime}_{2})!(n^{\prime}_{2})!}\left(\frac{\sum_{j=1}^{d_{2} }\lambda^{(2)}_{j}}{\sum_{j=1}^{d_{1}}\lambda^{(1)}_{j}}\right)^{n^{\prime}_{2} }\left(1-\frac{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}{\sum_{j=1}^{d_{1}}\lambda^ {(1)}_{j}}\right)^{n^{\prime}_{1}-n^{\prime}_{2}}\times,\] \[\mathbb{I}\left[\sum_{j=1}^{d_{2}}n^{(2)}_{j}=n^{\prime}_{2} \right]\frac{(n^{\prime}_{2})!}{\prod_{j=1}^{d_{2}}(n^{(2)}_{j})!}\prod_{j=1} ^{d_{2}}\left(\frac{\lambda^{(2)}_{j}}{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}} \right)^{n^{\prime}_{2}},\]

where \(\mathbb{I}[\cdot]\) is an indicator function. Simplifying the above equation by using the fact that only non-zero term in the above summation is \(n^{\prime}_{1}=\sum_{j=1}^{d_{1}}n^{(1)}_{j}\) and \(n^{\prime}_{2}=\sum_{j=1}^{d_{1}}n^{(2)}_{j}\).

\[=\frac{\left(\sum_{j=1}^{d_{1}}n^{(1)}_{j}\right)!}{\left(\sum_{ j=1}^{d_{1}}n^{(1)}_{j}-\sum_{j=1}^{d_{2}}n^{(2)}_{j}\right)!\left(\sum_{j=1}^{d_{2} }n^{(2)}_{j}\right)!}\left(\frac{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}{\sum_{j=1 }^{d_{1}}\lambda^{(1)}_{j}}\right)^{\sum_{j=1}^{d_{2}}n^{(2)}_{j}}\times\] \[\left(1-\frac{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}{\sum_{j=1}^{d_ {1}}\lambda^{(1)}_{j}}\right)^{\sum_{j=1}^{d_{1}}n^{(1)}_{j}-\sum_{j=1}^{d_{1}}n^ {(2)}_{j}}\frac{\left(\sum_{j=1}^{d_{2}}n^{(2)}_{j}\right)!}{\prod_{j=1}^{d_{2} }(n^{(2)}_{j})!}\prod_{j=1}^{d_{2}}\left(\frac{\lambda^{(2)}_{j}}{\sum_{j=1}^{d_ {2}}\lambda^{(2)}_{j}}\right)^{n^{(2)}_{j}},\]

Collecting all factorial terms together, and rearranging some terms:

\[= \frac{\left(\sum_{j=1}^{d_{1}}n^{(1)}_{j}\right)!}{\left(\sum_{j=1} ^{d_{1}}n^{(1)}_{j}-\sum_{j=1}^{d_{2}}n^{(2)}_{j}\right)!\prod_{j=1}^{d_{2}}(n^ {(2)}_{j})!}\left(1-\frac{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}{\sum_{j=1}^{d_{1}} \lambda^{(1)}_{j}}\right)^{\sum_{j=1}^{d_{1}}n^{(1)}_{j}-\sum_{j=1}^{d_{2}}n^{(2 )}_{j}}\times\] \[\left(\frac{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}{\sum_{j=1}^{d_ {1}}\lambda^{(1)}_{j}}\right)^{\sum_{j=1}^{d_{2}}n^{(2)}_{j}}\prod_{j=1}^{d_{2}} \left(\frac{\lambda^{(2)}_{j}}{\sum_{j=1}^{d_{2}}\lambda^{(2)}_{j}}\right)^{n^{(2 )}_{j}}.\]

[MISSING_PAGE_FAIL:56]

Canceling \(M\) in the above equation for the terms inside the summation, we get:

\[P(\vec{\mathbf{N}}\!=\!\vec{\mathbf{n}}|M)\!=\!Q(\vec{\mathbf{n}},\vec{\mathbf{n} }^{\prime})e^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}\prod_{i=1}^{d}(\gamma_{i }M)^{n_{i}}\left(\sum_{\vec{\mathbf{n}}^{\prime}\in C}\prod_{j=2}^{d^{\prime}} \prod_{(i_{1},\dots,i_{j})\in\mathbb{A}_{j}^{d}}\left(\frac{\gamma_{i_{1}\dots i _{j}}}{\prod_{l=1}^{j}\gamma_{i_{l}}}\right)^{n_{i_{1}\dots i_{j}}^{\prime}} \right).\] (209)

Absorbing all terms that do not depend upon \(M\) into \(B\), we have

\[B=\sum_{\vec{\mathbf{n}}^{\prime}\in C}\prod_{j=2}^{d^{\prime}}\prod_{(i_{1}, \dots,i_{j})\in\mathbb{A}_{j}^{d}}\left(\frac{\gamma_{i_{1}\dots i_{j}}}{\prod _{l=1}^{j}\gamma_{i_{l}}}\right)^{n_{i_{1}\dots i_{j}}^{\prime}},.\] (210)

Then, we can rewrite (209) as:

\[P(\vec{\mathbf{N}}\!=\!\vec{\mathbf{n}}|M) =Be^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}\prod_{i=1}^{d}(\gamma _{i}M)^{n_{i}}=Be^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}M^{\sum_{i=1}^{d}n_{ i}}\prod_{i=1}^{d}(\gamma_{i})^{n_{i}}\] \[\stackrel{{(a)}}{{=}}Be^{-\vec{\mathbf{T}}^{T}\vec{ \mathbf{A}}}M^{\sum_{i=1}^{d}n_{i}}\stackrel{{(b)}}{{=}}Be^{- \vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}M^{\vec{\mathbf{T}}^{T}\vec{\mathbf{n}}},\] (211)

where, in (a) we further absorb \(\prod_{i=1}^{d}(\gamma_{i})^{n_{i}}\) into \(B\) since it does not depend upon \(M\), and in (b) we substitute \(\sum_{i=1}^{d}n_{i}=\vec{\mathbf{T}}^{T}\vec{\mathbf{n}}\). Similarly, let us write out the expression for \(P(\vec{\mathbf{N}}^{g}|M)\):

\[P(\vec{\mathbf{N}}^{g}\!=\!\vec{\mathbf{n}}^{g}|M)=e^{-\vec{\mathbf{T}}^{T} \vec{\mathbf{A}}}\prod_{j=1}^{d^{\prime}}\prod_{(i_{1},\dots i_{j})\in\mathbb{ A}_{j}^{d}}(\gamma_{i_{1}\dots i_{j}}M^{j})^{n_{i_{1}\dots i_{j}}^{g}}.\] (212)

Collecting all the \(M\) terms, and absorbing all the terms that do not depend upon \(M\) into \(D\), we obtain:

\[P(\vec{\mathbf{N}}^{g}\!=\!\vec{\mathbf{n}}^{g}|M)=De^{-\vec{\mathbf{T}}^{T} \vec{\mathbf{A}}}M^{\sum_{j=1}^{d^{\prime}}\sum_{(i_{1},\dots,i_{j})\in\mathbb{ A}_{j}^{d}}\left(n_{i_{1}\dots i_{j}}^{g}\right)}.\] (213)

Now let us analyze the term \(\vec{\mathbf{T}}^{T}A\vec{\mathbf{n}}^{g}\):

\[\vec{\mathbf{1}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}\stackrel{{ (a)}}{{=}}(\vec{\mathbf{n}}^{g})^{T}\vec{\mathbf{A}}^{T}\vec{\mathbf{1}} \stackrel{{(b)}}{{=}}(\vec{\mathbf{n}}^{g})^{T}\left[\begin{matrix} \vec{\mathbf{A}}_{1}^{T}\vec{\mathbf{1}}\\ \vdots\\ \vec{\mathbf{A}}_{d^{\prime}}^{T}\vec{\mathbf{1}}\end{matrix}\right]\stackrel{{ (c)}}{{=}}(\vec{\mathbf{n}}^{g})^{T}\left[\begin{matrix} \vec{\mathbf{1}}\\ 2\vec{\mathbf{1}}\\ \vdots\\ d^{T}\vec{\mathbf{1}},\end{matrix}\right],\]

where (a) uses the fact that \(\vec{\mathbf{1}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}\) is a scalar and hence is equal to its transpose, (b) uses the fact \(\vec{\mathbf{A}}\!=\![\vec{\mathbf{A}}_{1}\dots\vec{\mathbf{A}}_{d^{\prime}}]\), and (c) follows from the special structure of \(\vec{\mathbf{A}}_{i}\), i.e., that each column only contains \(i\) ones and \(d-i\) zeros, and the fact that \(\vec{\mathbf{A}}_{1}^{T}\vec{\mathbf{1}}\) is akin to summing up the columns, hence \(\vec{\mathbf{A}}_{i}^{T}\vec{\mathbf{1}}=i\vec{\mathbf{1}}\). Equivalently we can rewrite the above equation as:

\[\vec{\mathbf{1}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}=\!\sum_{j=1}^{d^{ \prime}}\sum_{(i_{1},\dots,i_{j})\in\mathbb{A}_{j}^{d}}\left(jn_{i_{1}\dots i _{j}}^{g}\right).\] (214)

Substituting (214) into (213):

\[P(\vec{\mathbf{N}}^{g}|M)=De^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}M^{\vec{ \mathbf{T}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}}.\] (215)

Now, let us write out the expression for \(P(\vec{\mathbf{N}}|\vec{\mathbf{N}}^{g},M)\). Since \(\vec{\mathbf{N}}=\vec{\mathbf{A}}\vec{\mathbf{N}}^{g}\), \(P(\vec{\mathbf{N}}|\vec{\mathbf{N}}^{g})\) can be represented as a Kronecker delta function with the condition \(\vec{\mathbf{N}}=\vec{\mathbf{A}}\vec{\mathbf{N}}^{g}\), i.e.,

\[P(\vec{\mathbf{N}}=\vec{\mathbf{n}}|\vec{\mathbf{N}}^{g}=\vec{\mathbf{n}}^{g}, M)=P(\vec{\mathbf{N}}=\vec{\mathbf{n}}|\vec{\mathbf{N}}^{g}=\vec{\mathbf{n}}^{g})= \delta_{K}\left(\vec{\mathbf{n}}=\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}\right),\] (216)

where \(\delta_{K}(\cdot)\) is the Kronecker delta function. Substituting (211), (215) and (216) in (207), we get:

\[P(\vec{\mathbf{N}}^{g}|\vec{\mathbf{N}},M) =\frac{De^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}M^{\vec{ \mathbf{T}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}}\delta_{K}(\vec{\mathbf{n}}= \vec{\mathbf{A}}\vec{\mathbf{n}}^{g})}{Be^{-\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}}M ^{\vec{\mathbf{T}}^{T}\vec{\mathbf{n}}}}\stackrel{{(a)}}{{=}}\frac{ DM^{\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}\vec{\mathbf{n}}^{g}}\delta_{K}(\vec{ \mathbf{n}}=A\vec{\mathbf{n}}^{g})}{BM^{\vec{\mathbf{T}}^{T}\vec{\mathbf{n}}}},\] \[\stackrel{{(b)}}{{=}}\frac{DM^{\vec{\mathbf{T}}^{T} \vec{\mathbf{A}}\vec{\mathbf{n}}^{g}}\delta_{K}(\vec{\mathbf{n}}=\vec{ \mathbf{A}}\vec{\mathbf{n}}^{g})}{BM^{\vec{\mathbf{T}}^{T}\vec{\mathbf{A}}\vec{ \mathbf{n}}^{g}}}\stackrel{{(c)}}{{=}}\frac{D\delta_{K}(\vec{ \mathbf{n}}=\vec{\mathbf{A}}\vec{\mathbf{n}}^{g})}{B},\] (217)where we obtain (a) by canceling the term \(e^{-\mathbf{T}^{T}\mathbf{\tilde{A}}}\), (b) by using the fact \(\vec{\mathbf{n}}=\mathbf{A}\vec{\mathbf{n}}^{g}\) due to the delta function, and (c) by canceling the term \(M^{\mathbf{\tilde{T}}^{T}\mathbf{A}\vec{\mathbf{n}}^{g}}\). Since the terms \(D,B\) and \(\delta_{K}(\vec{\mathbf{n}}=\mathbf{A}\vec{\mathbf{n}}^{g})\) do not depend upon \(M\), we can conclude \(P(\vec{\mathbf{N}}^{g}|\vec{\mathbf{N}},M)\) also does not depend upon \(M\), i.e. \(P(\vec{\mathbf{N}}^{g}|\vec{\mathbf{N}},M)=P(\vec{\mathbf{N}}^{g}|\vec{ \mathbf{N}})\), or \(M\rightarrow\vec{\mathbf{N}}\rightarrow\vec{\mathbf{N}}^{g}\). 

**Lemma 18**.: _Let \(M\sim P(M)\) have support over \((0,\infty)\). Define, \(\vec{\mathbf{X}}=\left[\vec{\mathbf{X}}_{1}^{T}\,\ldots\,\vec{\mathbf{X}}_{d_ {1}}^{T}\right]^{T}\) and \(\vec{\mathbf{Y}}=\left[\vec{\mathbf{Y}}_{1}\,\ldots\,\vec{\mathbf{Y}}_{d_{2} }^{T}\right]^{T}\), where \(\vec{\mathbf{X}}_{i}\) and \(\vec{\mathbf{Y}}_{j}\) are random vectors of size \(q_{i}\in\mathbb{N}\) and \(r_{j}\in\mathbb{N}\), respectively, \(\forall\;i\in[d_{1}],\text{ and }j\in[d_{2}]\). Let the conditional distribution of \(\vec{\mathbf{X}}\) and \(\vec{\mathbf{Y}}\) conditioned on \(M\) be specified as follows:_

\[P\left(\vec{\mathbf{X}}|M\right)=\prod_{i=1}^{d_{1}}P\left(\vec{ \mathbf{X}}_{i}|M\right),\text{ and }P\left(\vec{\mathbf{Y}}|M\right)=\prod_{i=1}^{d_{2}}P\left(\vec{\mathbf{Y}}_ {i}|M\right),\text{ where }\] \[P\left(\vec{\mathbf{X}}_{i}|M\right)=\text{Poisson}\left(q_{i},1,\vec{\mathbf{A}}_{i}^{X}\right),\vec{\mathbf{A}}_{i}^{X}=\left[\gamma_{i1}^{X }M^{i}\quad\ldots\quad\gamma_{iq_{i}}^{X}M^{i}\right]^{T}\;\forall\;i\in[d_{1 }],\text{ and }\] \[P\left(\vec{\mathbf{Y}}_{j}|M\right)=\text{Poisson}\left(r_{j},1,\vec{\mathbf{A}}_{j}^{Y}\right),\vec{\mathbf{A}}_{i}^{Y}=\left[\gamma_{i1}^{X }M^{j}\quad\ldots\quad\gamma_{ir_{i}}^{Y}M^{j}\right]^{T}\;\forall\;j\in[d_{2}].\] (218)

_If \(d_{1}\geq d_{2}\) and (219) hold_

\[\sum_{j=1}^{q_{i}}\gamma_{ij}^{X}\geq\sum_{j=1}^{r_{i}}\gamma_{ij}^{Y}\; \forall\;i\in[d_{2}]\] (219)

_then the distribution \(\tilde{Q}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) defined in (220) lies in \(\Delta_{P}\)._

\[\tilde{Q}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})=P(M)P(\vec{\mathbf{X}}|M) \tilde{Q}(\vec{\mathbf{Y}}|\vec{\mathbf{X}}),\] (220)

_where \(\tilde{Q}(\vec{\mathbf{Y}}|\vec{\mathbf{X}})\) is a product of \(d_{2}\) multinomial distributions, i.e \(\tilde{Q}(\vec{\mathbf{Y}}|\vec{\mathbf{X}})=\prod_{i=1}^{d_{2}}\tilde{Q}( \vec{\mathbf{Y}}_{i}|\vec{\mathbf{X}}_{i})\), where \(\tilde{Q}(\vec{\mathbf{Y}}_{i}|\vec{\mathbf{X}}_{i})=\text{Multinomial}(\vec{ \mathbf{k}}_{i};N_{i},\vec{\mathbf{p}}_{i})\) and_

\[N_{i}=\sum_{j=1}^{n_{i}}x_{ij},\;\vec{\mathbf{k}}_{i}=\left[y_{i 1}\;\cdots\;y_{im_{i}}\;\sum_{j=1}^{n_{i}}x_{ij}-\sum_{j=1}^{m_{i}}y_{ij} \right]^{T},\] \[\vec{\mathbf{p}}_{i}=\left[\frac{\gamma_{ij}^{Y}}{\sum_{j=1}^{n_ {i}}\gamma_{ij}^{X}}\;\cdots\;\frac{\gamma_{im_{i}}^{Y}}{\sum_{j=1}^{n_{i}} \gamma_{ij}^{X}}\;1-\frac{\sum_{j=1}^{m_{i}}\gamma_{ij}^{Y}}{\sum_{j=1}^{n_{i}} \gamma_{ij}^{X}}\right]^{T}.\]

Proof.: Lemma 18 follows from a straightforward application of lemma 16. For showing \(\tilde{Q}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\) specified in (220) lies in \(\Delta_{P}\), we need to show that \(\tilde{Q}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}})\) and \(\tilde{Q}(M,\vec{\mathbf{Y}})=P(M,\vec{\mathbf{Y}})\). \(\tilde{Q}(M,\vec{\mathbf{X}})=P(M,\vec{\mathbf{X}})\) is satisfied trivially by construction of \(\tilde{Q}(M,\vec{\mathbf{X}},\vec{\mathbf{Y}})\).

Proving \(\tilde{Q}(\vec{\mathbf{Y}},M)=P(\vec{\mathbf{Y}},M)\):

For showing the second condition, we will explicitly show \(\tilde{Q}(\vec{\mathbf{Y}}|M)=P(\vec{\mathbf{Y}}|M)\) which, combined with the fact that \(\tilde{Q}(M)=P(M)\) (by construction), will show \(\tilde{Q}(\vec{\mathbf{Y}},M)=P(\vec{\mathbf{Y}},M)\) completing our proof.

Calculating \(\tilde{Q}(\mathbf{\vec{Y}}|\mathbf{\vec{X}})\): From the law of total probability, we have:

\[\tilde{Q}(\mathbf{\vec{Y}}|M) =\sum_{\vec{\mathbf{x}}}P(\mathbf{\vec{X}}=\vec{\mathbf{x}}|M) \tilde{Q}(\mathbf{\vec{Y}}|\mathbf{\vec{X}}=\vec{\mathbf{x}})\] \[\overset{(a)}{=}\sum_{\vec{\mathbf{x}}}\prod_{i=1}^{d_{1}}P( \mathbf{\vec{X}}_{i}=\mathbf{\vec{x}}_{i}|M)\prod_{i=1}^{d_{2}}\tilde{Q}( \mathbf{\vec{Y}}_{i}|\mathbf{\vec{X}}_{i}=\mathbf{\vec{x}}_{i})\] \[\overset{(b)}{=}\sum_{\vec{\mathbf{x}}}\prod_{i=1}^{d_{2}}P( \mathbf{\vec{X}}_{i}=\mathbf{\vec{x_{i}}}|M)\tilde{Q}(\mathbf{\vec{Y}}_{i}| \mathbf{\vec{X}}_{i}=\mathbf{\vec{x}}_{i})\prod_{i=d_{2}+1}^{d_{1}}P(\mathbf{ \vec{X}}_{i}=\mathbf{\vec{x_{i}}}|M)\] \[\overset{(c)}{=}\prod_{i=1}^{d_{2}}\left(\sum_{\vec{\mathbf{x}}_ {i}}P(\mathbf{\vec{X}}_{i}=\mathbf{\vec{x_{i}}}|M)\tilde{Q}(\mathbf{\vec{Y}}_{ i}|\mathbf{\vec{X}}_{i}=\mathbf{\vec{x}}_{i})\right)\prod_{i=d_{2}+1}^{d_{1}} \left(\underbrace{\sum_{\vec{\mathbf{x}}_{i}}P(\mathbf{\vec{X}}_{i}=\mathbf{ \vec{x_{i}}}|M)}_{=1}\right)\] \[\overset{(d)}{=}\prod_{i=1}^{d_{2}}\left(\underbrace{\sum_{\vec{ \mathbf{x}}_{i}}P(\mathbf{\vec{X}}_{i}=\mathbf{\vec{x_{i}}}|M)\tilde{Q}( \mathbf{\vec{Y}}_{i}|\mathbf{\vec{X}}_{i}=\mathbf{\vec{x}}_{i})}_{=\tilde{Q}( \mathbf{\vec{Y}}_{i}|M)}\right)=\prod_{i=1}^{d_{2}}\tilde{Q}(\mathbf{\vec{Y}}_ {i}|M),\] (221)

where \((a)\) is due to the particular structure of \(P(\mathbf{\vec{X}}|M)\) and \(\tilde{Q}(\mathbf{\vec{Y}}|\mathbf{\vec{X}})\) specified in (218) and (220), respectively, \((b)\) is just collecting the terms of \(P(\mathbf{\vec{X}}_{i}|M)\) and \(\tilde{Q}(\mathbf{\vec{Y}}_{i}|\mathbf{\vec{X}}_{i})\) into a single product, \((c)\) is due to spreading the summation of the components of \(\mathbf{\vec{x}}\) over their corresponding distribution components, and \((d)\) is due to the fact that sum over all probabilities of all possible \(\mathbf{\vec{x}}_{i}\) is \(1\).

Note that for a fixed \(m\), \(P\left(\mathbf{\vec{X}}_{i}|M=m\right)\ \sim\ \text{Poisson}\left(n_{i},1,\left[\gamma_{i1}m^{i} \ \ \ldots\ \ \gamma_{in_{i}}m^{i}\right]^{T}\right)\), and \(Q(\mathbf{\vec{Y}}_{i}|\mathbf{\vec{X}}_{i})=\text{Multinomial}(\mathbf{\vec{ k}}_{i};N_{i},\mathbf{\vec{p}}_{i})\), where

\[N_{i} =\sum_{j=1}^{n_{i}}x_{ij},\ \mathbf{\vec{k}}_{i}=\left[y_{i1}\ \cdots\ y_{im_{i}}\ \sum_{j=1}^{n_{i}}x_{ij}-\sum_{j=1}^{m_{i}}y_{ij}\right]^{T},\] \[\mathbf{\vec{p}}_{i} =\left[\tfrac{\gamma_{i1}^{\gamma_{i}}\chi_{ij}^{\gamma_{i}}}{ \sum_{j=1}^{n_{i}}\chi_{ij}^{\gamma_{i}}}\ \cdots\ \tfrac{\gamma_{im_{i}}^{\gamma_{i}}}{\sum_{j=1}^{n_{i}}\chi_{ij}^{ \gamma_{j}}}\ 1-\tfrac{\sum_{j=1}^{m_{i}}\gamma_{ij}^{\gamma_{j}}}{\sum_{j=1}^{n_{i}} \gamma_{ij}^{\gamma_{j}}}\right]^{T}.\]

Note that we can equivalently write \(\mathbf{\vec{p}}_{i}\) as follows,

\[\mathbf{\vec{p}}_{i} =\left[\tfrac{\gamma_{i1}^{\gamma_{i}}M^{i}}{\sum_{j=1}^{n_{i}} \chi_{ij}^{\gamma_{j}}M^{i}}\ \cdots\ \tfrac{\gamma_{im_{i}}^{\gamma_{i}}M^{i}}{\sum_{j=1}^{n_{i}}\gamma_{ij}^{ \gamma_{j}}M^{i}}\ 1-\tfrac{\sum_{j=1}^{m_{i}}\gamma_{ij}^{\gamma_{j}}M^{i}}{\sum_{j=1}^{n_{i}} \gamma_{ij}^{\gamma_{j}}M^{i}}\right]^{T}.\] (222)

In the setup of lemma 16, choose \(d_{1}=q_{i}\) and \(d_{2}=r_{i}\), \(\lambda_{j}^{(2)}=\gamma_{ij}^{Y}M^{i}\ \forall\ j\in[r_{i}]\) and \(\lambda_{j}^{(1)}=\gamma_{ij}^{X}M^{i}\ \forall\ j\in[q_{i}]\). From (219), we know the condition \(\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}\leq\sum_{j=1}^{d_{2}}\lambda_{j}^{(2)}\) is satisfied by assumption. Hence, we use the result of lemma 15 to conclude \(\tilde{Q}(\mathbf{\vec{Y}}_{i}|M)=\text{Poisson}(r_{i},1,\mathbf{\vec{A}}^{ \prime})\), with

\[\mathbf{\vec{\Lambda}}^{\prime}=\left[\lambda_{i1}\ \ \ \ldots,\lambda_{ir_{i}} \right]^{T},\ \text{with}\ \lambda_{ij}=\gamma_{ij}^{Y}M^{i}.\] (223)

Comparing \(\tilde{Q}(\mathbf{\vec{Y}}_{i}|M)\) specified in (223) with \(P(\mathbf{\vec{Y}}_{i}|M)\) specified in (218), we can conclude \(Q(\mathbf{\vec{Y}}_{i}|M)=P(\mathbf{\vec{Y}}_{i}|M)\). Substituting \(Q(\mathbf{\vec{Y}}_{i}|M)=P(\mathbf{\vec{Y}}_{i}|M)\) in (221), we obtain:

\[\tilde{Q}(\mathbf{\vec{Y}}|M)=\prod_{i=1}^{d_{2}}P(\mathbf{\vec{Y}}_{i}|M).\] (224)

Comparing (224) with (218), we can conclude \(\tilde{Q}(\mathbf{\vec{Y}}|M)=P(\mathbf{\vec{Y}}|M)\), finishing our proof.

Proof of Theorem 6

In this section, we provide the proof of theorem 6. Since theorem 6 provides the analytical PID terms for the linear convolution-closed system defined in Sec. 5.1, we briefly restate certain key properties of convolution-closed distributions and the linear convolution-closed system for convenience.

### Convolution-closed distributions

Convolution-closed distributions are a large class of distributions that are defined as follows:

**Definition 1**.: _Let \(\mathcal{F}_{\mathcal{D}}\) denote a family of distributions, where each member distribution, \(f(\delta)\in\mathcal{F}_{\mathcal{D}}\), is indexed by a parameter \(\delta\). Consider \(X_{1}\sim f(\delta_{1})\), \(X_{2}\sim f(\delta_{2})\), and \(X_{1}\perp\!\!\!\perp X_{2}\) for some \(\delta_{1},\delta_{2}\in\mathcal{D}\). Then, \(\mathcal{F}_{\mathcal{D}}\) is convolution-closed in the parameter \(\delta\), if_

\[X_{1}+X_{2}\sim f(\delta_{1})*f(\delta_{2})=f(\delta_{1}+\delta_{2})\ \forall\ \delta_{1},\delta_{2}\in\mathcal{D}\text{ such that }\delta_{1}+\delta_{2}\in \mathcal{D},\] (225)

_where \(*\) denotes the convolution operator._

Convolution-closed distributions define a natural dilation/thinning operator. Formally, let \(X\sim f(\delta)\). Then, we define \(X_{\epsilon}\) as the \(\epsilon\)-dilated version of \(X\) if \(X\sim f(\epsilon\delta)\) for some \(\epsilon\in(0,1)\) such that \(\epsilon\delta\in\mathcal{D}\). Furthermore, if we assume \((1-\epsilon)\delta\in\mathcal{D}\), then \(P(X_{\epsilon}|X)\) can be defined as \(P(X_{\epsilon}|X_{\epsilon}+X_{(1-\epsilon)})\), where \(X_{1-\epsilon}\sim f((1-\epsilon)\delta)\) and \(X_{\epsilon}\perp\!\!\!\perp X_{1-\epsilon}\). Denote, \(P(X_{\epsilon}|X=x)=P(X_{\epsilon}|X_{\epsilon}+X_{(1-\epsilon)}=x)=G(\epsilon \delta,(1-\epsilon)\delta,x)\).

### Definition of linear convolution-closed system

Let \(\mathcal{F}_{\mathcal{D}}\) be a convolution-closed distribution family in parameter \(\delta\in\mathcal{D}\), and \(M\) be a target/message random variable having distribution \(P(M)\) over some support set \(\mathcal{M}\). Define the conditional distribution of random variables \(X\) and \(Y\) conditioned on \(M\) as follows:

\[P(X|M\!=\!m)=f(\delta_{m}^{X})\text{ and }P(Y|M\!=\!m)=f(\delta_{m}^{Y}) \text{ such that }\delta_{m}^{X},\delta_{m}^{Y}\in\mathcal{D}\ \forall\ m\in\mathcal{M}.\] (226)

Furthermore, we assume that there exists \(\gamma\in(0,\infty)\) such that: \(\delta_{m}^{X}=\gamma\delta_{m}^{Y}\ \forall\ m\in\mathcal{M}\). We denote such a system of random variables \((M,X,Y)\), having the joint distribution \(P(M,X,Y)\), a _linear convolution-closed system_.

### Formal proof of theorem 6

**Theorem 6**.: _Let the joint density \(P(M,X,Y)\) of random variables \(M\), \(X\), and \(Y\) describe a linear convolution-closed system. Without the loss of generality, assume \(\gamma\leq 1\). If_

1. \((1-\gamma)\delta_{m}^{Y}\in\mathcal{D}\ \forall\ m\in\mathcal{M}\)_,_
2. \(P(X_{\gamma}|X_{\gamma}+X_{1-\gamma}\!=\!x,M\!=\!m)=G(\gamma\delta_{m}^{X},(1- \gamma)\delta_{m}^{X},x)\) _does not depend on_ \(m\)_, where_ \(P(X_{\gamma}|M)=f(\gamma\delta_{m}^{X})\)_,_ \(P(X_{1-\gamma}|M)=f((1-\gamma)\delta_{m}^{X})\) _and_ \(X_{\gamma}\perp\!\!\!\perp X_{1-\gamma}|M\)_,_

_then \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

Proof.: We first note that we can always assume \(\gamma\leq 1\) without the loss of generality because if \(\gamma\geq 1\), then we can always switch our nomenclature to refer to \(Y\) as \(X\), and \(X\) as \(Y\).

We briefly outline the proof structure.

1. In the first part, we explicitly construct a joint distribution \(Q_{MC}(M,X,Y)\) having the Markovian structure \(M\to X\to Y\).
2. In the second part, we show that the \(Q_{MC}(M,X,Y)\) constructed in the first part, lies in \(\Delta_{P}\). Therefore, we can then apply the result of proposition 1 to conclude \(UI(M;Y\backslash X)=0\).

**Part 1: Specifying \(Q_{MC}(M,X,Y)\)**Denote the joint distribution of the Markov chain \(M\to X\to Y\) as \(Q_{MC}(M,X,Y)\). We can decompose \(Q_{MC}(M,X,Y)\) by utilizing its Markovian structure as follows:

\[Q_{MC}(M,X,Y)=Q_{MC}(M)Q_{MC}(X|M)Q_{MC}(Y|X).\]

Consequently, we can construct the distribution \(Q_{MC}(M,X,Y)\) by individually specifying \(Q_{MC}(M)\), \(Q_{MC}(X|M)\), and \(Q_{MC}(Y|X)\).

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\): We specify \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M)\text{ and }Q_{MC}(X|M)=P(X|M).\] (227)

Specifying \(Q_{MC}(Y|X)\): By assumption (b) in the theorem, we know that \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},x)\) does not depend on \(m\), and consequently, does not depend on \(\delta_{m}^{X}\). Hence, we can simplify notation of \(G(\gamma\delta_{m}^{X},(1-\gamma)\delta_{m}^{X},m)\) as \(G(\gamma,(1-\gamma),x)\). We specify \(Q(Y|X)\) as follows:

\[Q(Y|X=x)=G(\gamma,1-\gamma,x).\] (228)

**Part 2: Showing \(Q_{MC}(M,X,Y)\in\Delta_{P}\)**

For showing that \(Q_{MC}(M,X,Y)\in\Delta_{P}\), we first need to derive \(Q_{MC}(Y|M)\).

Deriving \(Q_{MC}(Y|M)\): We use the result of lemma 19 to derive \(Q_{MC}(Y|M)\). As \(Q_{MC}(X|M=m)=f(\delta_{m}^{X})\) is a convolution-closed distribution for a fixed \(m\), and \(Q_{MC}(Y=y|X=x)=G(\gamma,1-\gamma,x)\), the result of lemma 19 shows that \(Q_{MC}(Y|M)\) can be expressed as:

\[Q_{MC}(Y|M=m)=f(\gamma\times\delta_{m}^{X})=f(\gamma\delta_{m}^{X})\stackrel{{ (a)}}{{=}}f(\delta_{m}^{Y})=P(Y|M=m),\] (229)

where \((a)\) is due to the assumption \(\delta_{m}^{X}=\gamma\delta_{m}^{Y}\) (see Appx. K.2).

From (227) and (229), we can conclude:

\[Q_{MC}(M,X)=Q_{MC}(M)Q_{MC}(X|M)=P(M)P(X|M)=P(M,X),\] (230) \[Q_{MC}(M,Y)=Q_{MC}(M)Q_{MC}(Y|M)=P(M)P(Y|M)=P(M,Y).\] (231)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\), concluding our proof for case 1.

**Lemma 19**.: _Let \(\mathcal{F}_{\mathcal{D}}\) be a convolution-closed distribution family convolution-closed in the parameter \(\delta\). For a \(\delta\in\mathcal{D}\), assume there exists \(\epsilon\in(0,1)\) such that \(\epsilon\delta,(1-\epsilon)\delta\in\mathcal{D}\). Consider the following random variables:_

\[X_{\epsilon}^{\prime}\sim f(\epsilon\delta)\in\mathcal{F}_{\mathcal{D}},\ \ X_{1-\epsilon}^{\prime}\sim f((1-\epsilon)\delta)\in\mathcal{F}_{\mathcal{D}}, \text{ and }X\sim f(\delta)\in\mathcal{F}_{\mathcal{D}},\]

_such that \(X_{\epsilon}^{\prime}\perp\!\!\!\perp X_{1-\epsilon}^{\prime}\). If \(P(X_{\epsilon}=x_{\epsilon}|X=x)=P(X_{\epsilon}^{\prime}=x_{\epsilon}|X_{ \epsilon}^{\prime}+X_{1-\epsilon}^{\prime}=x)\), then we have \(P(X_{\epsilon}=x_{\epsilon})=P(X_{\epsilon}^{\prime}=x_{\epsilon})=f(x_{ \epsilon};\epsilon\delta)\) or \(X_{\epsilon}\stackrel{{ d}}{{=}}X_{\epsilon}^{\prime}\), where \(\stackrel{{ d}}{{=}}\) implies equality in distribution [57]._

Proof.: First let us derive the distribution of \(X^{\prime}=X_{1-\epsilon}^{\prime}+X_{\epsilon}^{\prime}\). Since, \(X_{\epsilon}^{\prime}\perp\!\!\!\perp X_{1-\epsilon}^{\prime}\), we can write \(P(X^{\prime})\) as follows:

\[P(X^{\prime})=f(\epsilon\delta)*f((1-\epsilon)\delta)\stackrel{{ (a)}}{{=}}f(\epsilon\delta+(1-\epsilon)\delta)=f(\delta)\stackrel{{ (b)}}{{=}}P(X),\] (232)

where \(*\) is the convolution operator, \((a)\) is due to the properties of convolution-closed distribution (see Appx. K.1), and \((b)\) is due to the definition of \(X\) in the lemma statement. From (232), we can conclude \(P(X=x)=P(X^{\prime}=x)\), and by assumption we have \(P(X_{\epsilon}|X)=P(X_{\epsilon}^{\prime}|X^{\prime})=P(X_{\epsilon}^{\prime}|X _{\epsilon}^{\prime}+X_{(1-\epsilon)}^{\prime})\). Hence, we have:

\[P(X=x,X_{\epsilon}=x_{\epsilon}) =P(X=x)P\left(X_{\epsilon}=x_{\epsilon}|X=x\right)=P(X^{\prime}=x )P\left(X_{\epsilon}^{\prime}=x_{\epsilon}|X^{\prime}=x\right),\] \[=P(X^{\prime}=x)P\left(X_{\epsilon}^{\prime}=x_{\epsilon}|X_{ \epsilon}^{\prime}+X_{1-\epsilon}^{\prime}=x\right)\] \[=P\left(X_{\epsilon}^{\prime}+X_{1-\epsilon}^{\prime}=x,X_{ \epsilon}^{\prime}=x_{\epsilon}\right).\] (233)

From (233), we can conclude that \(P(X_{\epsilon})=P(X_{\epsilon}^{\prime})=f(\epsilon\delta)\).

Proof of Theorem 7

In this section, we provide the proof of theorem 7.

### Definition of \(p_{exp1}\) and \(p_{exp2}\)

We restate the definitions of \(p_{exp1}\) and \(p_{exp2}\) defined in Sec. 5.2, which is used in the proof of theorem 7. Let \(X\sim p_{exp1}(X)\), where

\[p_{exp1}(X=x;\theta_{1},\theta_{2})=H(\theta_{1},\theta_{2})\exp(\theta_{1}^{T }x-\theta_{2}^{T}A(x)),\] (234)

for some appropriately defined \(H(\cdot,\cdot),A(\cdot),\theta_{1}\) and \(\theta_{2}\). Furthermore, define a random variable \(Y\) through its conditional density \(p(Y|X=x)\):

\[p(Y=y|X=x;\theta_{1},\theta_{2})=h(y)\exp\left(x^{T}T(y)-\theta_{3}^{T}A(x) \right),\] (235)

for some \(h(\cdot)\), \(T(\cdot)\), and \(\theta_{3}\), such that \(p(Y=y|X=x;\theta_{1},\theta_{2})\) is a well-defined distribution. Furthermore, the marginal distribution of \(Y\) is expressed as:

\[p_{exp2}(Y=y;\theta_{1},\theta_{2},\theta_{3})=h(y)\frac{H(\theta_{1},\theta_{ 2})}{H(\theta_{1}+T(y),\theta_{2}+\theta_{3})}\text{ (see proof of theorem 1 in \@@cite[cite]{[\@@bibref{}{HJ}{}{}]}).}\] (236)

### Formal proof of theorem 7

**Theorem 7**.: _Let \(M,X,\) and \(Y\) be random variables having the joint distribution \(P(M,X,Y)\). Furthermore, the conditional distribution of \(X\) and \(Y\) conditioned on \(M\) are as follows: \(P(X|M\!=\!m)\!=\!p_{exp1}(X;\theta_{1}(m),\theta_{2}(m))\) and \(P(Y|M\!=\!m)\!=\!p_{exp2}(Y;\theta_{1}(m),\theta_{2}(m),\theta_{3})\). Then, \(\Delta_{P}\) contains a Markov chain of the form \(M\to X\to Y\) and \(UI(M;Y\backslash X)=0\)._

Proof.: We prove theorem 7 by explicitly constructing a Markov chain \(M\to X\to Y\), having joint distribution \(Q_{MC}(M,X,Y)\), and showing that \(Q_{MC}(M,X,Y)\in\Delta_{P}\).

Specifying \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\): We specify \(Q_{MC}(M)\) and \(Q_{MC}(X|M)\) as follows:

\[Q_{MC}(M)=P(M),\text{ and }Q_{MC}(X|M)=P(X|M).\] (237)

Specifying \(Q_{MC}(Y|X)\): We specify \(Q_{MC}(Y|X)=h(y)\exp(x^{T}T(y)-\theta_{3}^{T}A(x))\). Then, by (236), we know that

\[Q(Y=y|M=m)=p_{exp2}(y;\theta_{1}(m),\theta_{2}(m),\theta_{3})=\frac{h(y)H( \theta_{1}(m),\theta_{2}(m))}{H(\theta_{1}(m)+T(y),\theta_{2}(m)+\theta_{3}(m ))}.\] (238)

From (237) and (238), we can conclude:

\[Q_{MC}(M,Y)=Q_{MC}(Y|M)Q_{MC}(M)=P(Y|M)P(M)=P(M,Y),\] (239) \[Q_{MC}(M,X)=Q_{MC}(X|M)Q_{MC}(M)=P(X|M)P(M)=P(M,X).\] (240)

Hence, \(Q_{MC}(M,X,Y)\in\Delta_{P}\) and consequently, by proposition 1, \(UI(M;Y\backslash X)=0\), concluding our proof. 

## Appendix M Additional proofs and details for Sec. 6

In this section, we provide the proofs of certain statements made in Sec. 6, as well as additional details regarding the simulation study presented in the same section.

### Generating \(\bar{Q}\) for every \(Q\in\Delta_{P}\)

Our goal in this section is to show that there exists a \(\bar{Q}\) (as described in Sec. 6) for every \(Q\in\Delta_{P}\), where the corresponding \(P(M,X,Y)\) is as defined in Sec. 6. We first briefly restate the assumptions on \(P(M,X,Y)\) described in Sec. 6. The distribution \(P(M,X,Y)\) has the following properties: the random variable \(M\) has support over \(\mathcal{M}\), the conditional distributions \(P(X|M)\) and \(P(Y|M)\) are members of some convolution-closed distribution family \(\mathcal{F}_{\mathcal{D}}\), and there exists some \(\delta^{X}_{bias},\delta^{Y}_{bias}\in\mathcal{D}\) such that:

\[P(X|M=m)=f(\delta^{X}_{m})\text{ and }P(Y|M=m)=f(\delta^{Y}_{m}) \text{ where }\delta^{X}_{m},\delta^{Y}_{m}\in\mathcal{D}\;\forall\;m\in\mathcal{M},\text { with }\] \[\delta^{X}_{m}-\delta^{X}_{bias} =\epsilon^{(1)}_{m}(\delta^{Y}_{m}-\delta^{Y}_{bias})\text{ for some }\epsilon^{(1)}_{m}\in[0,1]\;\forall\;m\in\mathcal{M},\] \[\delta^{Y}_{m} =\epsilon^{(2)}_{m}(\delta^{Y}_{m}-\delta^{Y}_{bias})\text{ for some }\epsilon^{(2)}_{m}\in[0,1]\;\forall\;m\in\mathcal{M},\] \[\delta^{X}_{m} =\epsilon^{(3)}_{m}(\delta^{X}_{m}-\delta^{X}_{bias})\text{ for some }\epsilon^{(3)}_{m}\in[0,1]\;\forall\;m\in\mathcal{M},\text{ and }\] \[(\delta^{Y}_{m}-\delta^{Y}_{bias}),\;(\delta^{X}_{m}-\delta^{X}_ {bias}),\;(\delta^{X}_{m}-\delta^{Y}_{m}-(\delta^{X}_{bias}-\delta^{Y}_{bias }))\in\mathcal{D}.\] (241)

As \(Q(M,X)=P(M,X)\) and \(Q(M,Y)=P(M,Y)\) due to \(Q\in\Delta_{P}\), we have:

\[Q(X|M=m)=f(\delta^{X}_{m}),\text{ and }Q(Y|M=m)=f(\delta^{Y}_{m})\text{ where }\delta^{X}_{m},\delta^{Y}_{m}\in\mathcal{D}\;\forall\;m\in\mathcal{M}.\] (242)

#### a.1.1 Explicitly constructing a \(\bar{Q}\) for every \(Q\in\Delta_{P}\)

We show existence of a \(\bar{Q}\) for every \(Q\in\Delta_{P}\) by explicitly constructing a \(\bar{Q}\) for each \(Q\in\Delta_{P}\).

We define two vectors to simplify our notations:

\[\vec{\mathbf{A}}=\left[X\quad Y\right]^{T},\;\;\vec{\mathbf{B}}=\left[X^{ \prime}\quad Y^{\prime\prime}\quad n_{X}\quad Y^{\prime}\quad n_{Y}\right]^{T},\] (243)

where \(X\) and \(Y\) are the random variables defined above. \((X^{\prime},Y^{\prime\prime},n_{X})\), and \((Y^{\prime},n_{Y})\) are the corresponding dilated versions of \(X\) and \(Y\) as defined in Sec. 6. Note that the joint distribution of \((M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\) is \(\bar{Q}(M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\). We reiterate the two main properties of \(\bar{Q}(X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\), mentioned in Sec. 6, that we want to prove:

1. The marginal conditional distributions of \((X^{\prime},Y^{\prime\prime},n_{X},X^{\prime},n_{Y})\) given \(M\) are as follows: \[\bar{Q}(X^{\prime}|M=m)=f(\delta^{X}_{m}-\delta^{Y}_{m}-(\delta^{ X}_{bias}-\delta^{Y}_{bias})),\;\bar{Q}(Y^{\prime\prime}|M=m)=f(\delta^{Y}_{m}- \delta^{Y}_{bias}),\] \[\bar{Q}(Y^{\prime}|M=m)=f(\delta^{Y}_{m}-\delta^{Y}_{bias}),\bar {Q}(n_{X}|M)=f(\delta^{X}_{bias}),\bar{Q}(n_{Y}|M)=f(\delta^{Y}_{bias}).\]
2. The random variables \((X^{\prime},Y^{\prime\prime},n_{X})\) are jointly conditionally independent given \(M\). Similarly, the random variables \((Y^{\prime},n_{Y})\) are also conditionally independent given \(M\).

We construct \(\vec{\mathbf{B}}\) from \(\vec{\mathbf{A}}\) by specifying the conditional distribution \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\). Note that given the values of \(\vec{\mathbf{A}}\) and \(M\), we can always use \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\) to construct the random variables \(\vec{\mathbf{B}}\). The goal of this remaining section is to show that the corresponding distribution \(\bar{Q}(\vec{\mathbf{B}},M)\) (specified through \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\)) satisfies the above two properties. We first explicitly state the structure of \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\).

Specifying \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\):

We first impose the following structure on \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\) as follows:

\[\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)=\bar{Q}(X^{\prime},Y^{\prime \prime},n_{X}|X,M)\bar{Q}(Y^{\prime},n_{Y}|Y,M),\] (244)

where \(\bar{Q}(X^{\prime},Y^{\prime\prime},n_{X}|X,M)\) and \(\bar{Q}(Y^{\prime},n_{Y}|Y,M)\) are the conditional distributions through which we specify \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\). Since \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\) is a product of two well-defined distributions, it is a valid distribution. Let us now analyze the joint distribution \(\bar{Q}(M,\vec{\mathbf{A}},\vec{\mathbf{B}})\). By chain rule, we have:

\[\bar{Q}(M,\vec{\mathbf{A}},\vec{\mathbf{B}})=\bar{Q}(M,\vec{\mathbf{A}})\bar{Q} (\vec{\mathbf{B}}|\vec{\mathbf{A}},M).\] (245)

Substituting the fact that \((M,\vec{\mathbf{A}})=(M,X,Y)\) and using the structure of \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\) specified in (244), we have:

\[\bar{Q}(M,\vec{\mathbf{A}},\vec{\mathbf{B}})=\bar{Q}(M)\bar{Q}(X,Y|M)\bar{Q}(X^ {\prime},Y^{\prime\prime},n_{X}|X,M)\bar{Q}(Y^{\prime},n_{Y}|Y,M).\] (246)

We now specify the two components of \(\bar{Q}(\vec{\mathbf{B}}|\vec{\mathbf{A}},M)\).

Specifying \(\bar{Q}(Y^{\prime},n_{Y}|Y,M)\):

[MISSING_PAGE_FAIL:64]

#### 0.1.2 Propositions required for constructing \(\bar{Q}\)

We describe two important propositions that we use to show the existence of a \(\bar{Q}\) for every \(Q\in\Delta_{P}\).

Proposition 8: _Suppose the random variables \(M\) and \(Y\) have the joint distribution \(Q(M,Y)\) such that_

\[Q(M)=P(M)\text{ and }Q(Y|M\!=\!m)=f(\delta^{Y}_{m})\;\forall\;m\in\mathcal{M},\] (257)

_where \(\mathcal{M}\) denotes the support of \(M\) and \(f(\delta^{Y}_{m})\in\mathcal{F}_{\mathcal{D}}\) for some convolution-closed distribution family \(\mathcal{F}_{\mathcal{D}}\). Furthermore, there also exists a \(\delta^{Y}_{bias}\in\mathcal{D}\) such that_

\[\delta^{Y}_{m}=\epsilon^{(2)}_{m}(\delta^{Y}_{m}-\delta^{Y}_{bias})\text{ for some }\epsilon^{(2)}_{m}\in[0,1]\text{ and }(\delta^{Y}_{m}-\delta^{Y}_{bias})\in\mathcal{D}\;\forall\;m\in \mathcal{M}.\] (258)

_Consider two dilated versions of \(Y\), denoted as \(Y^{\prime}\) and \(n_{Y}\), defined as follows:_

\[\bar{Q}(Y^{\prime}|Y\!=\!y,M\!=\!m)=G(\epsilon^{(2)}_{m}\delta^{Y}_{m},(1- \epsilon^{(2)}_{m})\delta^{Y}_{m},y),\text{ and }n_{Y}=Y-Y^{\prime}.\] (259)

_Then, we have_

1. \(Y^{\prime}\perp\!\!\!\perp n_{Y}|M\)__
2. _The conditional distributions_ \(\bar{Q}(Y^{\prime}|M)\) _and_ \(\bar{Q}(n_{Y}|M)\) _are specified as follows:_ \[\bar{Q}(Y^{\prime}|M=m) =f(\epsilon^{(2)}_{m}\delta^{Y}_{m})=f(\delta^{Y}_{m}-\delta^{Y}_ {bias})\text{ and }\] \[\bar{Q}(n_{Y}|M\!=\!m) =f((1-\epsilon^{(2)}_{m})\delta^{Y}_{m})=f(\delta^{Y}_{bias})\; \forall\;m\in\mathcal{M}.\] (260)

Proof: We essentially modify the argument given in theorem 1 of [25] to prove the above proposition. Consider the conditional distribution of \(Y\), \(Y^{\prime}\), and \(n_{Y}\) conditioned on \(M\), denoted as \(\bar{Q}(Y,Y^{\prime},n_{Y}|M)\). Then, by chain rule, we have \(\bar{Q}(Y,Y^{\prime},n_{Y}|M)=\bar{Q}(Y|M)\bar{Q}(Y^{\prime}|M,Y)\bar{Q}(n_{Y}| Y,Y^{\prime},M)\). By the assumptions in (257) and (259), we know that

\[\bar{Q}(Y|M\!=\!m)=f(\delta^{Y}_{m}),\bar{Q}(Y^{\prime}|M\!=\!m, Y\!=\!y)=G(\epsilon^{(2)}_{m}\delta^{Y}_{m},(1-\epsilon^{(2)}_{m})\delta^{Y}_{m},y), \text{ and }\] \[\bar{Q}(n_{Y}|Y\!=\!y,Y^{\prime}\!=\!y^{\prime},M)=\delta_{K}(n_ {Y}=y-y^{\prime}),\] (261)

where \(\delta_{K}(\cdot)\) is the Kronecker delta function [59].

Observation 1: Our _first key observation_ is that the distribution \(\bar{Q}(Y^{\prime},Y,n_{Y}|M\!=\!m)\) is completely specified through the distributions \(\bar{Q}(Y|M)\) and \(\bar{Q}(Y^{\prime}|Y,M)\), as \(n_{Y}\) is completely determined by \(Y^{\prime}\) and \(Y\) and \(\bar{Q}(n_{Y}|Y,Y^{\prime},M)\) is specified through a Kronecker delta function.

Similarly, consider a new pair of random variables \(Y^{\prime}_{new},n^{new}_{Y}\) such that \(Y^{\prime}_{new}\perp\!\!\perp n^{new}_{Y}|M\) and the respective conditional distributions of \(Y^{\prime}_{new}\) and \(n^{new}_{Y}\) are specified as follows:

\[\bar{Q}_{new}(Y^{\prime}_{new}|M\!=\!m)=f(\epsilon^{(2)}_{m}\delta^{Y}_{m}), \text{ and }\bar{Q}_{new}(n^{new}_{Y}|M\!=\!m)=f((1-\epsilon^{(2)}_{m})\delta^{Y}_{m})\; \forall\;m\in\mathcal{M}.\] (262)

We will now show _two key properties_ for the random variables \((Y^{\prime}_{new},n^{new}_{Y},Y^{\prime}_{new}+n^{new}_{Y})\) that will form the crux of our argument:

Property 1: \(\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}\!=\!y|M\!=\!m)=\bar{Q}(Y\!=\!y|M\!=\!m)\).

Let us now calculate the conditional distribution \(\bar{Q}(Y^{\prime}_{new}+n^{new}_{Y}=y|M)\):

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}=y|M)=\int_{k=-\infty}^{\infty}\bar{Q }_{new}(Y^{\prime}_{new}=k,n^{new}_{Y}=y-k|M).\] (263)

Using conditional dependence of \(Y^{\prime}_{new}\) and \(n^{new}_{Y}\), we have \(\bar{Q}_{new}(Y^{\prime}_{new}=k,n^{new}_{Y}=y-k|M)=\bar{Q}_{new}(Y^{\prime}_{ new}=k|M)\bar{Q}_{new}(n^{new}_{Y}=y-k|M)\):

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}=y|M)=\int_{k=-\infty}^{\infty}\bar{Q }_{new}(Y^{\prime}_{new}=k|M)\bar{Q}_{new}(n^{new}_{Y}=y-k|M).\] (264)

Note that the R.H.S of the above equation defines a convolution of \(\bar{Q}_{new}(Y^{\prime}_{new}|M)\) and \(\bar{Q}(n^{new}_{Y}|M)\), hence:

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}=y|M)=\bar{Q}_{new}(Y^{\prime}_{new}=k| M)*\bar{Q}_{new}(n^{new}_{Y}=y-k|M),\] (265)where \(*\) denotes the convolution operator. Substituting (262) in the above equation:

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}\!=\!y|M\!=\!m)=f(\epsilon_{m}^{(2)} \delta^{Y}_{m})*f((1-\epsilon_{m}^{(2)})\delta^{Y}_{m}).\] (266)

Using the properties of convolution-closed distributions:

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}\!=\!y|M\!=\!m)=f(\epsilon_{m}^{(2)} \delta^{Y}_{m}+(1-\epsilon_{m}^{(2)})\delta^{Y}_{m})=f(\delta^{Y}_{m}).\] (267)

Hence, by comparing above equation with (261) we have:

\[\bar{Q}_{new}(Y^{\prime}_{new}+n^{new}_{Y}=y|M=m)=Q(Y=y|M=m).\] (268)

**Property 2**: \(\bar{Q}(Y^{\prime}_{new}\!=\!y^{\prime}|Y^{\prime}_{new}+n^{new}_{Y}\!=\!y,M \!=\!m)=G(\epsilon_{m}^{(2)}\delta^{Y}_{m},1-\epsilon_{m}^{(2)}\delta^{Y}_{m},y)\)

By definition of \(G(\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2)})\delta^{Y}_{m},z)\) given in Sec. 5.1, we have:

\[G(\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2)})\delta^ {Y}_{m},z)=\Pr(Z_{\epsilon_{m}^{(2)}}|Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m} ^{(2)}}=z),\;\mbox{where}\] \[\Pr(Z_{\epsilon_{m}^{(2)}})=f(\epsilon_{m}^{(2)}\delta^{Y}_{m}), \Pr(Z_{1-\epsilon_{m}^{(2)}})=f((1-\epsilon_{m}^{(2)})\delta^{Y}_{m}),\;\mbox {and}\;Z_{\epsilon_{m}^{(2)}}\perp\!\!\!\perp Z_{1-\epsilon_{m}^{(2)}}.\] (269)

In the following steps, we slightly modify the notation of \(G(\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2)})\delta^{Y}_{m},z)\) to \(G(z_{\epsilon_{m}^{(2)}};\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2 )})\delta^{Y}_{m},z)\) to make our notations more explicit as follows:

\[G(z_{\epsilon_{m}^{(2)}};\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2 )})\delta^{Y}_{m},z)=\Pr(Z_{\epsilon_{m}^{(2)}}=z_{\epsilon_{m}^{(2)}}|Z_{ \epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}}=z).\] (270)

We use the definition of the conditional distribution to express \(G(\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2)})\delta^{Y}_{m},z)\) as:

\[G(z_{\epsilon_{m}^{(2)}};\epsilon_{m}^{(2)}\delta^{Y}_{m},(1- \epsilon_{m}^{(2)})\delta^{Y}_{m},z) =\frac{\Pr(Z_{\epsilon}=z_{\epsilon_{m}^{(2)}},Z_{\epsilon_{m}^{ (2)}}+Z_{1-\epsilon_{m}^{(2)}}=z)}{\Pr(Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{ m}^{(2)}}=z)}\] (271) \[=\frac{\Pr(Z_{\epsilon}=z_{\epsilon_{m}^{(2)}})\Pr(Z_{\epsilon_{ m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}}=z|Z_{\epsilon_{m}^{(2)}}=z_{\epsilon_{m}^{(2)}})}{ \Pr(Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}}=z)}.\] (272)

Using the fact that \(\Pr(Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}}=z|Z_{\epsilon_{m}^{(2)}}=z _{\epsilon_{m}^{(2)}})=\Pr(Z_{1-\epsilon_{m}^{(2)}}=z-z_{\epsilon_{m}^{(2)}})\), since \(Z_{\epsilon_{m}^{(2)}}\perp\!\!\!\perp Z_{1-\epsilon_{m}^{(2)}}\):

\[=\frac{\Pr(Z_{\epsilon}=z_{\epsilon_{m}^{(2)}})\Pr(Z_{1-\epsilon_{m}^{(2)}}=z -z_{\epsilon_{m}^{(2)}})}{\Pr(Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}} =z)}.\] (273)

Note that by properties of convolution-closed distributions, \(\Pr(Z_{\epsilon_{m}^{(2)}}+Z_{1-\epsilon_{m}^{(2)}}=z))=f(z;\delta^{Y}_{m})\), where \(f(z;\delta^{Y}_{m})\) is to be understood as the distribution \(f(\delta^{Y}_{m})\) evaluated at \(z\). Substituting (269) in the above equation:

\[=\frac{f(z_{\epsilon_{m}^{(2)}};\epsilon_{m}^{(2)}\delta^{Y}_{m})f(z-z_{ \epsilon_{m}^{(2)}};(1-\epsilon_{m}^{(2)})\delta^{Y}_{m})}{f(z;\delta^{Y}_{m})}.\] (274)

Now, note that by (262) and (267), we have:

\[\bar{Q}_{new}(Y^{\prime}_{new}\!=\!z_{\epsilon}|M\!=\!m) =f(z_{\epsilon_{m}^{(2)}};\epsilon_{m}^{(2)}\delta^{Y}_{m}),\] \[\bar{Q}_{new}(n^{new}_{Y}\!=\!z-z_{\epsilon}|M\!=\!m) =f(z-z_{\epsilon_{m}^{(2)}};(1-\epsilon_{m}^{(2)})\delta^{Y}_{m}), \;\mbox{and}\] \[\bar{Q}_{new}(Y^{\prime}_{new}\!+\!n^{new}_{Y}\!=\!z|M\!=\!m) =f(z;\delta^{Y}_{m})\] (275)

Hence, combining the above equation with (274), and renaming \(z_{\epsilon_{m}^{(2)}}\) as \(y^{\prime}\) and \(z\) as \(y\), we have:

\[G(\epsilon_{m}^{(2)}\delta^{Y}_{m},(1-\epsilon_{m}^{(2)})\delta^{Y}_{m},y)= \frac{\bar{Q}_{new}(Y^{\prime}_{new}\!=\!y^{\prime}|M\!=\!m)\bar{Q}_{new}(n^{ new}_{Y}\!=\!y-y^{\prime}|M\!=\!m)}{\bar{Q}_{new}(Y^{\prime}_{new}+n^{ new}_{Y}\!=\!y|M\!=\!m)}.\] (276)Noting the fact that \(\bar{Q}_{new}(n_{Y}^{new}\!=\!y-y^{\prime}|M\!=\!m)=\bar{Q}_{new}(Y^{\prime}_{new} +n_{Y}^{new}\!=\!y|M\!=\!m,Y^{\prime}_{new}\!=\!y^{\prime})\) due to \(Y^{\prime}_{new}\perp\!\!\!\perp n_{Y}^{new}|M\) and abbreviating notation pertaining to \(y^{\prime}\), we have:

\[G(\epsilon_{m}^{(2)}\delta_{m}^{Y},(1-\epsilon_{m}^{(2)})\delta_{m}^{Y},y)= \frac{\bar{Q}_{new}(Y^{\prime}_{new}|M\!=\!m)\bar{Q}_{new}(Y^{\prime}_{new}+n _{Y}^{new}\!=\!y|M\!=\!m)}{\bar{Q}_{new}(Y^{\prime}_{new}+n_{Y}^{new}\!=\!y|M \!=\!m)}.\] (277)

Using Bayes rule in the above equation provides us with our desired property:

\[G(\epsilon_{m}^{(2)}\delta_{m}^{Y},(1-\epsilon_{m}^{(2)})\delta_{m}^{Y},y)= \bar{Q}_{new}(Y^{\prime}_{new}|M\!=\!m,Y^{\prime}_{new}+n_{Y}^{new}\!=\!y)\] (278)

Furthermore, by comparing (259) with the above equation, we have:

\[\bar{Q}_{new}(Y^{\prime}_{new}=y^{\prime}|Y^{\prime}_{new}+n_{Y}^{new}=y,M=m) =\bar{Q}(Y^{\prime}=y^{\prime}|Y=y,M=m).\] (279)

**Observation 2**: Note that the distribution \(\bar{Q}(Y^{\prime}_{new},n_{Y}^{new},Y^{\prime}_{new}+n_{Y}^{new}|M)\) is also completely determined by specifying \(\bar{Q}(Y^{\prime}_{new}+n_{Y}^{new}|M)\) and \(\bar{Q}(Y^{\prime}_{new}|Y^{\prime}_{new}+n_{Y}^{new},M)\) since \(n_{Y}^{new}\) is completely determined by \(Y^{\prime}_{new}+n_{Y}^{new}\) and \(Y^{\prime}_{new}\).

By property 1 and (279), we know that

\[\bar{Q}_{new}(Y^{\prime}_{new}+n_{Y}^{new}|M)=Q(Y|M),\mbox{ and }\bar{Q}_{ new}(Y^{\prime}_{new}|Y^{\prime}_{new}+n_{Y}^{new},M)=\bar{Q}(Y^{\prime}|Y,M).\] (280)

Therefore, combining the above equation with observations 1 and 2,we have:

\[\bar{Q}(Y^{\prime},Y,n_{Y}|M)=\bar{Q}(Y^{\prime}_{new},Y^{\prime}_{new}+n_{Y}^ {new},n_{Y}^{new}|M).\] (281)

Consequently the distribution \(\bar{Q}(Y^{\prime},n_{Y}|M)=\bar{Q}_{new}(Y^{\prime}_{new},n_{Y}^{new}|M)\). Since, \(Y^{\prime}_{new}\perp\!\!\!\perp n_{Y}^{new}|M\), we also have \(Y^{\prime}\perp\!\!\!\perp n_{Y}|M\). Furthermore, by (262),

\[\bar{Q}_{new}(Y^{\prime}_{new}|M\!=\!m)=\bar{Q}(Y^{\prime}|M)=f( \epsilon_{m}^{(2)}\delta_{m}^{Y})\;\forall\;m\in\mathcal{M},\mbox{ and}\] \[\bar{Q}_{new}(n_{Y}^{new}|M\!=\!m)=\bar{Q}(n_{Y}|M)=f((1-\epsilon_ {m}^{(2)})\delta_{m}^{Y})\;\forall\;m\in\mathcal{M}.\] (282)

By (258), we know that \(\epsilon_{m}^{(2)}\delta_{m}^{Y}=\delta_{m}^{Y}-\delta_{bias}^{Y}\) and \((1-\epsilon_{m}^{(2)})\delta_{m}^{Y}=\delta_{bias}^{Y}\). Hence,

\[\bar{Q}(Y^{\prime}|M)=f(\epsilon_{m}^{(2)}\delta_{m}^{Y})=f( \delta_{m}^{Y}-\delta_{bias}^{Y})\;\forall\;m\in\mathcal{M},\mbox{ and}\] \[\bar{Q}(n_{Y}|M)=f((1-\epsilon_{m}^{(2)})\delta_{m}^{Y})=f(\delta_ {bias}^{Y})\;\forall\;m\in\mathcal{M}.\] (283)

**Proposition 9**.: _Suppose the random variables \(M\) and \(X\) have the joint distribution \(Q(M,X)\), such that_

\[Q(M)=P(M)\mbox{ and }Q(X|M\!=\!m)=f(\delta_{m}^{X})\;\forall\;m\in\mathcal{M},\] (284)

_where \(\mathcal{M}\) denotes the support of \(M\) and \(f(\delta_{m}^{X})\in\mathcal{F}_{\mathcal{D}}\) for some convolution-closed distribution family \(\mathcal{F}_{\mathcal{D}}\). Furthermore, there also exist \(\delta_{bias}^{X}\in\mathcal{D}\), and \(\delta_{m}^{Y}\in\mathcal{D}\;\forall\;m\in\mathcal{M}\) such that_

\[\delta_{m}^{X}-\delta_{bias}^{X}=\epsilon_{m}^{(1)}(\delta_{m}^{Y}-\delta_{ bias}^{Y})\mbox{ for some }\epsilon_{m}^{(1)}\in[0,1],\] \[\delta_{m}^{X}=\epsilon_{m}^{(3)}(\delta_{m}^{X}-\delta_{bias}^{X}) \mbox{ for some }\epsilon_{m}^{(3)}\in[0,1],\mbox{ and}\] \[(\delta_{m}^{X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^{Y })),\;(\delta_{m}^{X}-\delta_{bias}^{X}),\;(\delta_{m}^{Y}-\delta_{bias}^{Y}) \in\mathcal{D}\;\forall\;m\in\mathcal{M}.\] (285)

_Then consider three dilated versions of \(X\), denoted as \(X^{\prime},Y^{\prime\prime}\), and \(n_{X}\), defined as follows:_

\[\bar{Q}(X^{\prime},Y^{\prime\prime},n_{X}|X,M) =\bar{Q}(n_{X}|X,M)\bar{Q}(X^{\prime}|X,n_{X},M)\bar{Q}(Y^{\prime \prime}|X,n_{X},M,X^{\prime}),\mbox{ where}\] \[\bar{Q}(n_{X}|X\!=\!x,M\!=\!m) =G((1-\epsilon_{m}^{(3)})\delta_{m}^{X},\epsilon_{m}^{(3)}\delta_ {m}^{X},x),\] \[\bar{Q}(X^{\prime}|X\!=\!x,n_{X}\!=\!x_{n_{X}},M\!=\!m) =G((1-\epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{X}), \epsilon_{m}^{(1)}(\delta_{m}^{X}-\delta_{bias}^{X}),x-n_{x}),\mbox{ and}\] \[Y^{\prime\prime} =X-X-n_{X}.\] (286)

_Then, we have_

1. \((n_{X},X^{\prime},Y^{\prime\prime})\) _are jointly conditionally independent conditioned on_ \(M\)2. _The conditional distributions_ \(Q(X^{\prime}|M)\)_,_ \(Q(Y^{\prime\prime}|M)\)_, and_ \(Q(n_{X}|M)\) _are specified as follows:_ \[\bar{Q}(X^{\prime}|M) =f(\delta_{m}^{X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^ {Y})),\] \[\bar{Q}(Y^{\prime\prime}|M) =f(\delta_{m}^{X}-\delta_{bias}^{Y}),\text{ and}\] \[\bar{Q}(n_{X}|M) =f(\delta_{bias}^{X}).\] (287)

Proof.: The proof essentially consists of recursively applying the proof of proposition 8. Consider a new auxiliary random variable \(Z=X-n_{X}\). Then, the distribution \(\bar{Q}(X,n_{X},Z|M)\) has the following properties:

\[\bar{Q}(X|M)\stackrel{{(a)}}{{=}}f(\delta_{m}^{X}), \bar{Q}(n_{X}|M\!=\!m,X\!=\!x)\stackrel{{(b)}}{{=}}G((1-\epsilon _{m}^{(3)})\delta_{m}^{X},\epsilon_{m}^{(3)}\delta_{m}^{X},x),\text{ and}\] \[\bar{Q}(Z|X\!=\!x,n_{X}\!=\!x_{n_{X}},M=m)=\delta_{K}(Z=x-x_{n_{X} }),\] (288)

where \(\delta_{K}(\cdot)\) is the Kronecker delta function, and the equalities \((a)\) and \((b)\) follow from (284) and (285), respectively. Note that the distribution \(\bar{Q}(X,n_{X},Z|M)\) has exactly the same structure as \(\bar{Q}(Y,Y^{\prime},n_{Y}|M)\) described in (261), hence \(Z\) and \(n_{Y}\) are just dilated versions of \(X\). Consequently, from the result of proposition 8, we know that \(Z\perp\!\!\!\perp n_{X}|M\), and the conditional distributions \(\bar{Q}(Z|M)\) and \(\bar{Q}(n_{Y}|M)\) are described as follows:

\[\bar{Q}(n_{X}|M\!=\!m)=f((1-\epsilon_{m}^{(3)})\delta_{m}^{X}), \text{ and }\bar{Q}(Z|M\!=\!m)=f(\epsilon_{m}^{(3)}\delta_{m}^{X}).\] (289)

From (285), we have \(\epsilon_{m}^{(3)}\delta_{m}^{X}=\delta_{m}^{X}-\delta_{bias}^{X}\) and \((1-\epsilon_{m}^{(3)})\delta_{m}^{X}=\delta_{bias}^{X}\). Consequently,

\[\bar{Q}(n_{X}|M\!=\!m)=f(\delta_{bias}^{X}),\text{ and }\bar{Q}(Z|M\!=\!m)=f( \delta_{m}^{X}-\delta_{bias}^{X}).\] (290)

We alternatively express \(\tilde{Q}(X^{\prime}|n_{X},X,M)\) as follows:

\[\bar{Q}(X^{\prime}|n_{X}\!=\!x_{n_{X}},X\!=\!x,M\!=\!m)=G((1- \epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{X}),\epsilon_{m}^{(1)}( \delta_{m}^{X}-\delta_{bias}^{X}),x-x_{n_{X}}).\] (291)

Note that the distribution \(\bar{Q}(X^{\prime}|n_{X}\!=\!x_{n_{X}},X\!=\!x,M\!=\!m)\) only depends on \(X\) and \(n_{X}\) through their difference \(X-n_{X}\). Hence, substituting \(Z=X-n_{X}\) in the above equation:

\[\bar{Q}(X^{\prime}|n_{X}\!=\!x_{n_{X}},X\!=\!x,M\!=\!m) =\bar{Q}(X^{\prime}|Z\!=\!x-x_{n_{X}},M\!=\!m)\] \[=G((1-\epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{X}), \epsilon_{m}^{(1)}(\delta_{m}^{X}-\delta_{bias}^{X}),x-n_{X}),\] (292) \[\Rightarrow\bar{Q}(X^{\prime}|Z\!=\!z,M\!=\!m) =G((1-\epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{X}), \epsilon_{m}^{(1)}(\delta_{m}^{X}-\delta_{bias}^{X}),z).\] (293)

Hence, consider the following distribution \(\bar{Q}(Z,X^{\prime},Y^{\prime\prime}|M)\), where from (290) and (293), we have:

\[\bar{Q}(Z|M\!=\!m) =f(\delta_{m}^{X}-\delta_{bias}^{X}),\] \[\bar{Q}(X^{\prime}|Z=z,M=m) =G((1-\epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{X}), \epsilon_{m}^{(1)}(\delta_{m}^{X}-\delta_{bias}^{X}),z).\] (294)

Furthermore, by (286), we have \(Y^{\prime\prime}=X-n_{X}-X^{\prime}\stackrel{{(c)}}{{=}}Z-X^{ \prime}\), where \((c)\) is due to \(Z=X-n_{X}\). Hence,

\[\bar{Q}(Y^{\prime\prime}|Z\!=\!z,M,X^{\prime}\!=\!x)=\delta_{K}( Y^{\prime\prime}=z-x).\] (295)

Comparing (295) and (294) with (261), we can again conclude that \(X^{\prime\prime}\) and \(Y^{\prime}\) are just dilated versions of \(Z\). Hence, again using the result of proposition 8, we have \(X^{\prime}\perp\!\!\!\perp Y^{\prime\prime}|M\), and

\[\bar{Q}(X^{\prime}|M\!=\!m)=f((1-\epsilon_{m}^{(1)})(\delta_{m}^ {X}-\delta_{bias}^{X})),\text{ and }\bar{Q}(Y^{\prime\prime}|M\!=\!m)=f(\epsilon_{m}^{(1)}(\delta_{m}^{X}- \delta_{bias}^{X})).\] (296)

From (285), we have \(\epsilon_{m}^{(1)}(\delta_{m}^{X}-\delta_{bias}^{X})=\delta_{m}^{Y}-\delta_{ bias}^{Y}\) and \((1-\epsilon_{m}^{(1)})(\delta_{m}^{X}-\delta_{bias}^{Y})=\delta_{m}^{X}-\delta_{ M}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^{Y})\). Consequently,

\[\bar{Q}(X^{\prime}|M\!=\!m)=f(\delta_{m}^{X}-\delta_{m}^{Y}-( \delta_{bias}^{X}-\delta_{bias}^{Y}),\text{ and }\bar{Q}(Y^{\prime\prime}|M\!=\!m)=f(\delta_{m}^{Y}-\delta_{bias}^{Y}).\] (297)Hence, by (290) and (297), we can conclude that the conditional distributions \(\bar{Q}(X^{\prime}|M)\), \(\bar{Q}(Y^{\prime\prime}|M)\), and \(\bar{Q}(n_{X}|M)\) are specified as follows:

\[\bar{Q}(X^{\prime}|M=m) =f(\delta_{m}^{X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias} ^{Y})),\] \[\bar{Q}(Y^{\prime\prime}|M) =f(\delta_{m}^{X}-\delta_{bias}^{Y}),\text{ and}\] \[\bar{Q}(n_{X}|M) =f(\delta_{bias}^{X}).\] (298)

_Showing joint conditional independence of \((n_{Y},X^{\prime},Y^{\prime\prime})\) given \(M\)_:

Consider the distribution \(\bar{Q}(n_{X},Z,X^{\prime},Y^{\prime\prime}|M)\). Using the chain rule, we have:

\[\bar{Q}(n_{X},Z,X^{\prime},Y^{\prime\prime}|M) =\bar{Q}(n_{X},Z|M)Q(X^{\prime},Y^{\prime\prime}|M,n_{X},Z),\] \[\overset{(b)}{=}\bar{Q}(n_{X}|M)\bar{Q}(Z|M)Q(X^{\prime},Y^{ \prime\prime}|M,n_{X},Z),\] \[\overset{(c)}{=}\bar{Q}(n_{X}|M)\bar{Q}(Z|M)Q(X^{\prime},Y^{ \prime\prime}|M,Z),\] (299)

where \((b)\) follows from \(n_{X}\perp\!\!\!\perp Z|M\). For showing \((c)\), observe that \(Y^{\prime\prime}=Z-X^{\prime}\), and hence we can write \(\bar{Q}(Y^{\prime\prime}|M,Z,n_{Y},X^{\prime})=\bar{Q}(Y^{\prime\prime}|Z,X^{ \prime})\). Furthermore, by analyzing \(\bar{Q}(X^{\prime}|n_{X},Z,M)\), we can conclude:

\[\bar{Q}(X^{\prime}|n_{X},Z,M)=\bar{Q}(X^{\prime}|n_{X},X-n_{X},M)=\bar{Q}(X^{ \prime}|n_{X},X,M)=\bar{Q}(X^{\prime}|Z,M).\] (300)

Marginalizing \(Z\) out of (299), we obtain:

\[\bar{Q}(n_{X},X^{\prime},Y^{\prime\prime}|M)=\bar{Q}(n_{X}|M)\bar{Q}(X^{ \prime},Y^{\prime\prime}|M)\overset{(d)}{=}\bar{Q}(n_{X}|M)Q(X^{\prime}|M) \bar{Q}(Y^{\prime\prime}|M),\] (301)

where \((d)\) follows from \(X^{\prime}\perp\!\!\!\perp Y^{\prime\prime}|M\). By (301), we know that \((n_{X},X^{\prime\prime},Y^{\prime})\) are jointly conditionally independent given \(M\), concluding our proof. 

### Analytically optimizing the upper bound

**Proposition 10**.: _Let \(\bar{Q}(M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\) be the distribution derived from a distribution \(Q\in\Delta_{P}\) using the construction scheme specified in Appx. M.1. Then, the minimization problem specified in (302) is analytically solvable with the optimizing \(\bar{Q}^{*}\) given by (303)._

\[\min_{Q\in\Delta_{P}}I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n _{X},Y^{\prime},n_{Y}]),\text{ (note that $\bar{Q}$ is a function of $Q$)}\] (302) \[\bar{Q}^{*}(M,X^{\prime},Y^{\prime\prime},n_{X},n_{Y},Y^{\prime} )=\bar{Q}^{*}(M)\bar{Q}^{*}(X^{\prime}|M)\bar{Q}^{*}(Y^{\prime\prime}|M)\bar{ Q}^{*}(n_{X})\bar{Q}^{*}(n_{Y})\bar{Q}^{*}(Y^{\prime}|Y^{\prime\prime}),\text{ with}\] \[\bar{Q}^{*}(M)=P(M),\bar{Q}^{*}(X^{\prime}|M\!=\!m)=f(\delta_{m}^ {X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^{Y})),\bar{Q}^{*}(n_{X})=f (\delta_{bias}^{X}),\] \[\bar{Q}^{*}(Y^{\prime\prime}|M\!=\!m)=f(\delta_{m}^{Y}-\delta_{ bias}^{Y}),\bar{Q}^{*}(n_{Y})=f(\delta_{bias}^{Y}),\text{ and }\bar{Q}^{*}(Y^{\prime}|Y^{\prime\prime})=\mathbb{I}[Y^{\prime\prime}=Y^{\prime}],\] (303)

_where \(\mathbb{I}[\cdot]\) is the indicator function. Note that for the minimizing \(\bar{Q}^{*}\), we have \((n_{X},n_{Y},(M,X^{\prime},Y^{\prime\prime}))\) are jointly independent and \(Y^{\prime}=Y^{\prime\prime}\)._

Proof.: First note that by the construction scheme defined in Appx. M.1, we know that there is a unique \(\bar{Q}\) for every \(Q\in\Delta_{P}\). Hence, we can interpret \(\bar{Q}\) as a function of \(Q\), and consequently the minimization problem in (302) is well-defined. Using the chain rule of mutual information, we derive the following lower bound for \(I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},X^{\prime},n_{Y}])\):

\[I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{ Y}]) =I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime}])+I_{\bar{Q}}(M;[n_{X},Y^{ \prime},n_{Y}]|[X^{\prime},Y^{\prime}])\] (304) \[\Rightarrow I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}]) \overset{(a)}{\geq}I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime}]),\] (305)

where \((a)\) is due to the non-negativity of conditional mutual information. Furthermore, note that by properties of \(\bar{Q}\) derived in Appx. M.1, we know that \(X^{\prime}\perp\!\!\!\perp Y^{\prime\prime}|M\)\(\forall\)\(Q\in\Delta_{P}\). Consequently, the term \(I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime}])\) does note vary as we choose different \(Q\) from \(\Delta_{P}\) as the corresponding \(\bar{Q}(M,X^{\prime},Y^{\prime\prime})=\bar{Q}(M)\bar{Q}(X^{\prime}|M)\bar{Q}(Y^ {\prime\prime}|M)\) and the terms:

\[\bar{Q}(M)=Q(M)=P(M),\bar{Q}(X^{\prime}|M\!=\!m)=f(\delta_{m}^{ X}-\delta_{m}^{Y}-(\delta_{bias}^{X}-\delta_{bias}^{Y})),\text{ and}\] \[\bar{Q}(Y^{\prime\prime}|M)=f(\delta_{m}^{Y}-\delta_{bias}^{Y})\] (306)do not vary for all \(Q\in\Delta_{P}\) (see Appx. M.1). Therefore, we have:

\[I(M;[X^{\prime},Y^{\prime\prime}])\leq I_{\bar{Q}}(M;[X^{\prime},Y^{\prime \prime},n_{X},Y^{\prime},n_{Y}])\;\forall\;Q\in\Delta_{P},\] (307)

where we forgo the subscript \(\bar{Q}\) for \(I_{\bar{Q}}(M;[X^{\prime\prime},Y^{\prime}])\) to indicate that the term \(I_{\bar{Q}}(M;[X^{\prime\prime},Y^{\prime}])\) does not vary over \(\Delta_{P}\). Hence, if we show that the distribution \(\bar{Q}^{*}(M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\) specified in (303) achieves the lower bound in (307), then \(\bar{Q}^{*}(M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\) should minimize the value of \(I_{\bar{Q}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])\) over the set \(\Delta_{P}\). We now calculate \(I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])\). By the chain rule of mutual information:

\[I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])=I_{ \bar{Q}^{*}}(M;[n_{X},n_{Y},X^{\prime},Y^{\prime\prime}])+I_{\bar{Q}^{*}}(M;Y^ {\prime}|n_{X},n_{Y},X,Y^{\prime\prime}).\]

Using the fact that \(Y^{\prime}=Y^{\prime\prime}\), we know that \(I_{\bar{Q}^{*}}(M;Y^{\prime}|n_{X},n_{Y},X,Y^{\prime\prime})\) is zero. A simple justification follows from the fact \(I_{\bar{Q}^{*}}(M;Y^{\prime}|n_{X},n_{Y},X,Y^{\prime\prime})=H(Y^{\prime}|n_{X },n_{Y},X,Y^{\prime\prime})-H(Y^{\prime}|n_{X},n_{Y},X,Y^{\prime\prime},M)\), where \(H(.|.)\) is the conditional entropy function [28]. Observe that both conditional entropy terms are zero as \(Y^{\prime}=Y^{\prime\prime}\), and condition entropy of a random variable conditioned on itself is zero. Therefore, we can simplify the above equation as:

\[I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])=I_{ \bar{Q}^{*}}(M;[n_{X},n_{Y},X^{\prime},Y^{\prime\prime}]).\] (308)

Applying the chain rule of mutual information again, we obtain:

\[I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])=I_{ \bar{Q}^{*}}(M;[X_{n},n_{Y}])+I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime}] |n_{X},n_{Y}).\] (309)

Using the fact that \((n_{X},n_{Y})\perp\!\!\!\perp(M,X^{\prime},Y^{\prime\prime})\), we know that \(I(M;[n_{X},n_{Y}])=0\) as mutual information between independent random variables is zero [28]. Consequently,

\[I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y}])=I_{ \bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime}]|n_{X},n_{Y})\stackrel{{ (b)}}{{=}}I_{\bar{Q}^{*}}(M;[X^{\prime},Y^{\prime\prime}]),\] (310)

where \((b)\) again follows from the fact that \((n_{X},n_{Y})\perp\!\!\!\perp(M,X^{\prime},Y^{\prime\prime})\), and, consequently, conditioning \((M,X^{\prime},Y^{\prime\prime})\) on \((n_{X},n_{Y})\) has no effect. From (310), we see the proposed \(\bar{Q}^{*}\) in (303) does indeed achieve the lower bound specified in (307), and consequently minimizes (302).

The final step remaining in our proof is to ensure that the proposed \(\bar{Q}^{*}\) corresponds to a valid \(Q^{*}_{UB}(M,X,Y)\) lying in \(\Delta_{P}\), i.e., there is some \(Q^{*}_{UB}(M,X,Y)\in\Delta_{P}\) that generates \(\bar{Q}^{*}\) using the construction scheme outlined in Appx. M.1. We know that the relationship between the random variables for \((M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\) having the joint distribution \(\bar{Q}\) and \((M,X,Y)\) having the distribution \(Q\) can be expressed as follows: \(X=X^{\prime}+Y^{\prime\prime}+n_{X}\) and \(Y=Y^{\prime}+n_{Y}\). Hence, we use these relationships for calculating the conditional marginals \(Q^{*}_{UB}(X|M)\) and \(Q^{*}_{UB}(Y|M)\) of the corresponding distribution \(Q^{*}_{UB}(M,X,Y)\) for the proposed distribution \(\bar{Q}^{*}(M,X^{\prime},Y^{\prime\prime},n_{X},Y^{\prime},n_{Y})\):

\[Q^{*}_{UB}(X|M=m) =\bar{Q}^{*}(X^{\prime}+Y^{\prime\prime}+n_{X}|M=m)\] \[\stackrel{{(c)}}{{=}}\bar{Q}^{*}(X^{\prime}|M=m)* \bar{Q}^{*}(Y^{\prime\prime}|M=m)*\bar{Q}^{*}(n_{X}|M=m)\] \[=f(\delta^{X}_{m}-\delta^{Y}_{m}-(\delta^{X}_{bias}-\delta^{Y}_{ bias}))*f(\delta^{Y}_{m}-\delta^{Y}_{bias})*f(\delta^{X}_{bias})\] \[\stackrel{{(d)}}{{=}}f(\delta^{X}_{m}-\delta^{Y}_{ m}-(\delta^{X}_{bias}-\delta^{Y}_{bias})+\delta^{Y}_{m}-\delta^{Y}_{bias}+\delta^{X}_{bias})\] \[=f(\delta^{X}_{m})=P(X|M=m).\] (311)

where \(*\) denotes the convolution operator, and \((c)\) follows from the fact that \((X^{\prime},Y^{\prime\prime},n_{X})\) are jointly conditionally independent given \(M\) under \(\bar{Q}^{*}\), and \((d)\) follows from the properties of convolution-closed distributions. Similarly, calculating \(Q^{*}_{UB}(Y|M=m)\):

\[Q^{*}_{UB}(Y|M=m) =\bar{Q}^{*}(Y^{\prime}+n_{Y}|M=m)\] \[\stackrel{{(e)}}{{=}}\bar{Q}^{*}(Y^{\prime\prime}|M=m)* \bar{Q}^{*}(n_{Y}|M=m)\] \[=f(\delta^{Y}_{m}-\delta^{Y}_{bias})*f(\delta^{Y}_{bias})\] \[=f(\delta^{Y}_{m}-\delta^{Y}_{bias}+\delta^{X}_{bias})\] \[=f(\delta^{Y}_{m})=P(Y|M=m),\] (312)

where \((e)\) follows from the fact that \(Y^{\prime}\perp\!\!\!\perp n_{Y}|M\) under \(\bar{Q}^{*}\). It is easy to see that \(Q^{*}_{UB}(M)=\bar{Q}(M)=P(M)\). Hence, we have \(Q^{*}_{UB}(M,X)=P(M)P(X|M)=P(M,X)\) and \(Q^{*}_{UB}(M,Y)=P(M)P(X=Y|M)=P(M,Y)\), which implies \(Q^{*}_{UB}(M,X,Y)\in\Delta_{P}\), concluding our proof.

### Additional details regarding simulation

The list of different function pairs used in the simulation study are listed in Table 1. The function pairs were chosen with the only requirement being that they satisfied the assumptions in (9). For constructing the analytical estimate \(\bar{Q}_{A}(M,X,Y)\), the corresponding \(\delta^{X}_{bias}\) and \(\delta^{Y}_{bias}\) were chosen as follows:

1. For each distribution and function pair, we first determine the range of possible values for \(\delta^{X}_{bias}\) and \(\delta^{Y}_{bias}\).
2. For \(\delta^{Y}_{bias}\), we choose the range as \([0,\min_{m\in\mathcal{M}}f_{2}(m))\), where \(\mathcal{M}\) is the support set of \(M\) for that distribution. Note that we cannot choose \(\delta^{Y}_{bias}>\min_{m\in\mathcal{M}}f_{2}(m)\) as the corresponding conditional distribution \(\bar{Q}^{*}(Y^{\prime}|M)=\text{Poisson}(f_{2}(M)-\delta^{Y}_{bias})\) (required for the optimal \(\bar{Q}^{*}\) from which \(Q_{A}\) is constructed, see Appx. M.2) would have a negative rate parameter, which is not allowed. Within this range, we choose \(10\) different, uniformly spaced values for \(\delta^{Y}_{bias}\).
3. For each chosen value of \(\delta^{Y}_{bias}\), the corresponding range for \(\delta^{X}_{bias}\) is then chosen as \([0,\min\{\min_{m\in\mathcal{M}}f_{1}(m),\min_{m\in\mathcal{M}}(f_{1}(m)-f_{2} (m)+\delta^{Y}_{bias})\}]\). Note that choosing the upper limit of \(\delta^{X}_{bias}\) as \(\min\{\min_{m\in\mathcal{M}}f_{1}(m),\min_{m\in\mathcal{M}}(f_{1}(m)-f_{2}(m) +\delta^{Y}_{bias})\}\) ensures that the distribution \(\bar{Q}^{*}(X^{\prime}|M)=\text{Poisson}((f_{1}(M)-\delta^{X}_{bias})-(f_{2}( M)-\delta^{Y}_{bias}))\) and \(\bar{Q}^{*}(Z|M)=\text{Poisson}(f_{1}(M)-\delta^{X}_{bias})\) (required for specifying the optimal \(\bar{Q}^{*}\) from which \(Q_{A}\) is constructed, see Appx. M.1 and Appx. M.2) have positive rate parameters. For each value of \(\delta^{Y}_{bias}\), we again choose \(10\) different values of \(\delta^{X}_{bias}\) within the above specified range. In total, we chose \(100\) different pairs of \((\delta^{X}_{bias},\delta^{Y}_{bias})\).

For each pair of \((\delta^{X}_{bias},\delta^{Y}_{bias})\), we calculate the analytical \(Q_{A}(M,X,Y)\) as follows:

First, we construct the optimal \(\bar{Q}^{*}\) distribution using the structure provided in proposition 10. The exact specification of \(\bar{Q}^{*}\) is described below. We simplify the structure of \(\bar{Q}^{*}\) by using the fact that \(Y^{\prime}=Y^{\prime\prime}\):

\[\bar{Q}^{*}(M,X^{\prime},Y^{\prime},n_{X},n_{Y},Y^{\prime\prime}) \equiv\bar{Q}^{*}(M,X^{\prime},Y^{\prime},n_{X},n_{Y}),\] \[=\bar{Q}^{*}(M)\bar{Q}^{*}(X^{\prime}|M)\bar{Q}^{*}(Y^{\prime}|M) \bar{Q}^{*}(n_{X})\bar{Q}^{*}(n_{Y}),\text{ with}\] \[\bar{Q}^{*}(M) =P(M),\] \[\bar{Q}^{*}(X^{\prime}|M) =\text{Poisson}(f_{1}(M)-f_{2}(M)-(\delta^{X}_{bias}-\delta^{Y}_ {bias})),\] \[\bar{Q}^{*}(n_{X}) =\text{Poisson}(\delta^{X}_{bias}),\] \[\bar{Q}^{*}(Y^{\prime\prime}|M) =\text{Poisson}(f_{2}(M)-\delta^{Y}_{bias}),\] \[\bar{Q}^{*}(n_{Y}) =\text{Poisson}(\delta^{Y}_{bias}).\] (313)

Then, we use the property that \(X=X^{\prime}+Y^{\prime}+n_{X}\), and \(Y=Y^{\prime}+n_{Y}\), and use that to appropriately modify \(\bar{Q}^{*}\) to obtain \(Q_{A}\) as follows:

* We calculate a new distribution \(\bar{Q}^{*}(M,X^{\prime}+Y^{\prime},Y^{\prime},n_{X},n_{Y})\): \[\bar{Q}^{*}(M,X^{\prime}+Y^{\prime},Y^{\prime},n_{X},n_{Y}) =\bar{Q}^{*}(M)\bar{Q}^{*}(n_{X})\bar{Q}^{*}(n_{Y})\bar{Q}^{*}(X^ {\prime}+Y^{\prime}|M)\times\] \[\quad\bar{Q}^{*}(Y^{\prime}|X^{\prime}+Y^{\prime},M).\] \[=\bar{Q}^{*}(M)\bar{Q}^{*}(n_{X})\bar{Q}^{*}(n_{Y})\bar{Q}^{*}(Z| M)\bar{Q}^{*}(Y^{\prime}|Z,M),\] (314) where \(Z=X^{\prime}+Y^{\prime}\) and alternatively also \(Z=X-n_{X}\) (as \(X=X^{\prime}+Y^{\prime}+n_{X}\)). First note that \(\bar{Q}^{*}(Z|M)\) from Appx. M.2 has the form \(f(\delta^{X}_{m}-\delta^{X}_{bias})\), which for the Poisson case is \(\text{Poisson}(f_{1}(M)-\delta^{X}_{bias})\). Furthermore, we can recognize that \(\bar{Q}^{*}(Y^{\prime}=y|Z=z,M=m)=\bar{Q}^{*}(Y^{\prime}=y^{\prime}|X^{ \prime}+Y^{\prime}=x^{\prime}+y^{\prime},M=m)=G(y^{\prime};\epsilon^{(1)}_{m },1-\epsilon^{(1)}_{m},x^{\prime}+y^{\prime})\), which for the Poisson case can be written as \(\text{Binomial}(Z,p_{m})\) where \(p_{m}=f_{2}(M)-\delta^{Y}_{bias}/f_{1}(M)-\delta^{X}_{bias}\).
* Now define \(\mathbb{S}^{m}_{x}=\{x_{n_{X}}\in\mathcal{X}^{m}_{n_{X}},z\in\mathcal{Z}_{m}:x_{ n_{X}}+z=x\}\), where \(\mathcal{X}^{m}_{n_{X}}\) and \(\mathcal{Z}_{m}\) are the corresponding support sets of the distributions \(\bar{Q}^{*}(n_{X}|M=m)\) and \(\bar{Q}^{*}(Z|M=m)\)respectively. Then, using the fact that \(X=Z+n_{X}\), we have: \[\bar{Q}^{*}(M\!=\!m,X\!=\!x,n_{Y},Y^{\prime})=\bar{Q}^{*}(M,Z+n_{X},n _{Y},Y^{\prime})=\bar{Q}^{*}(M=m)\bar{Q}^{*}(n_{Y})\times\] \[\sum_{(x_{n_{X}},z)\in\mathbb{S}_{x}^{m}}\bar{Q}^{*}(n_{X}=x_{n_{X }})\bar{Q}^{*}(Z=z|M=m)\bar{Q}^{*}(Y^{\prime}|Z=z,M=m),\] (315) where we know that for the Poisson case: \[\bar{Q}^{*}(M)=P(M),\bar{Q}^{*}(n_{Y})=\text{Poisson}(\delta^{Y} _{bias}),\bar{Q}^{*}(Z|M)=\text{Poisson}(f_{1}(M)-\delta^{X}_{bias}),\] \[\bar{Q}^{*}(n_{X})=\text{Poisson}(\delta^{X}_{bias}),\text{ and }\bar{Q}^{*}(Y^{\prime}|Z,M)=\text{Binomial}\left(Z,\frac{f_{2}(M)-\delta^{Y}_{ bias}}{f_{1}(M)-\delta^{X}_{bias}}\right).\] (316)
* Using a similar procedure, we derive \(Q_{A}(M,X,Y)\) from \(\bar{Q}^{*}(M,X,Y^{\prime},n_{Y})\) as \(Y=n_{Y}+Y^{\prime}\). Define \(\mathbb{S}_{y}^{m}=\{y_{n_{Y}}\in\mathcal{Y}_{n_{X}}^{m},z\in\mathcal{Y}_{m}^ {\prime}:y_{n_{X}}+y^{\prime}=y\}\), then: \[Q_{A}(M,X,Y\!=\!y)=\bar{Q}(M,X,n_{Y}+Y^{\prime})=\sum_{(x_{n_{Y}},y^{\prime}) \in\mathbb{S}_{y}^{m}}\bar{Q}^{*}(M,X,Y^{\prime}=y^{\prime},n_{Y}=x_{n_{Y}}).\]
* After analytically calculating \(Q_{A}\) for all \(100\) bias pairs of \((\delta^{X}_{bias},\delta^{Y}_{bias})\), we chose \(Q_{A}\) corresponding to the pair \((\delta^{X}_{bias},\delta^{Y}_{bias})\) which results in the smallest value of the corresponding \(I_{Q_{A}}(M;[X,Y])\).

### Additional results and analysis

We report the tightness of the upper-bound for systems employing binomial and negative-binomial distributions in Fig. 2**(a)**. We find that the results for binomial and negative-binomial remain consistent with the Poisson study performed in Sec. 6 and the upper-bound continues to remain tight. We also performed further analysis to understand how our upper-bound performs when both of the UI terms are non-zero. We sub-selected the trials in the 20 pairs (analyzed in Section 6) that had both \(UI_{x}>0.01\) bits and \(UI_{y}>0.01\) bits, and report the corresponding median error of these sub-selected trials in Fig. 2**(d)**.

## Appendix N Distributions

We provide brief descriptions of the distribution families and the corresponding distributions used in this work. We also provide citations to relevant works for the reader interested in learning more about these distributions.

\begin{table}
\begin{tabular}{l l l||l l l} \hline \hline Index & \(f_{1}(M)\) & \(f_{2}(M)\) & Index & \(f_{1}(M)\) & \(f_{2}(M)\) \\ \hline
1 & \(3M\) & \(3\log(1+M)\) & \(11\) & \(Me^{M}\) & \(M\) \\
2 & \(1.5M^{3}\) & \(0.5e^{M}\) & \(12\) & \(3M^{2}-2M\) & \(M+\log(M)\) \\
3 & \(0.5M^{4}\) & \(0.25e^{M}\sin^{2}(M\pi/\!\text{s})\) & \(13\) & \(8\sin^{(M\pi/\!\text{s})}\) & \(2\sqrt{M}\) \\
4 & \(7M\) & \(4M\sin^{2}(M\pi/\!\text{s})\) & \(14\) & \(2\)cosh\((M)\) & \(6\sin(M\pi/\!\text{s})\) \\
5 & \(M^{2}\) & \(M\) & \(15\) & \(5M\) & \(5\sqrt{M}\) \\
6 & \(M^{3}\) & \(M^{2}\) & \(16\) & \(e^{M}\) & \(\sinh(M)\) \\
7 & \(5M\) & \(5\sin^{(M\pi/\!\text{s})}\) & \(17\) & \(2\)cosh\((M)\) & \(\sinh(M)\) \\
8 & \(M^{2}+M^{3}\) & \(2M\) & \(18\) & \(2.5M^{2}+5M\) & \(0.5M^{3}+M\) \\
9 & \(7M\) & \(M^{2}\cos^{2}(M\pi/\!\text{16})\) & \(19\) & \(1.5\)cosh\((M)\) & \(0.5M^{3}\) \\
10 & \(8(M-\cos(M\pi/\!\text{s}))\) & \(2\sin(M\pi/\!\text{s})\) & \(20\) & \(8M+3\) & \(7M\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Table 1 lists the \(20\) different function pairs used in the simulation study described in Sec. 6 for comparing the analytical estimate of the minimizing distribution of (2) proposed in Sec. 6 with the corresponding ground-truth numerical estimate. The index in Table 1 maps the corresponding function pair to the results shown in Fig. 1. To illustrate, the number \(1\) on the \(x\)-axis of all the plots shown in Fig. 1 corresponds to the function pair with index \(1\) in Table 1.

\begin{table}
\begin{tabular}{l l l||l l} \hline \hline  & \multicolumn{2}{c}{Negative Binomial} & \multicolumn{2}{c}{Binomial} \\ \hline Index & \(f_{1}(M)\) & \(f_{2}(M)\) & \(f_{1}(M)\) & \(f_{2}(M)\) \\ \hline \(1\) & \(0.5M^{2}\) & \(M\) & \([0.5M^{2}]\) & \(M\) \\ \(2\) & \(0.1M^{3}\) & \(0.25M^{2}\) & \([0.04M^{3}]\) & \([0.1M^{2}]\) \\ \(3\) & \(4M\) & \(M\sin^{2}(M\nicefrac{{\pi}}{{16}})+2\) & \(4M\) & \([M\sin^{2}(M\nicefrac{{\pi}}{{16}})+2]\) \\ \(4\) & \(5M\) & \(10\left|\sin(M\nicefrac{{\pi}}{{16}})\right|+2\) & \(5M\) & \([10\left|\sin(M\nicefrac{{\pi}}{{16}})\right|+2]\) \\ \(5\) & \(0.05(M^{2}+M^{3})\) & \(M\) & \([0.05(M^{2}+M^{3})]\) & \(M\) \\ \(6\) & \(7M\) & \(0.5M^{2}\cos^{2}(M\nicefrac{{\pi}}{{36}})+2\) & \(7M\) & \([0.5M^{2}\cos^{2}(\frac{M\pi}{{36}})+2]\) \\ \(7\) & \(M-\cos(M\nicefrac{{\pi}}{{8}})\) & \(2\sin(M\nicefrac{{\pi}}{{16}})+2\) & \([2(M-\cos(M\nicefrac{{\pi}}{{8}}))]\) & \([12\sin(M\nicefrac{{\pi}}{{40}})+2]\) \\ \(8\) & \(2M\) & \(0.004Me^{M^{2}}+9\) & \(2M\) & \([0.004Me^{M/3}+5]\) \\ \(9\) & \(0.5(3M^{2}-2M)\) & \(M+\log(M)\) & \([0.1(3M^{2}-2M)]\) & \([M+\log(M)]\) \\ \(10\) & \(18\sin(M\nicefrac{{\pi}}{{18}})\) & \(5\sqrt{M}\) & \(19\sin(M\nicefrac{{\pi}}{{40}})\) & \([3\sqrt{M}]\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: For negative binomial the outcomes of \(M\) were uniformly randomly sampled from the interval \([5,10]\), and for the binomial experiment the outcomes of \(M\) were uniformly chosen from the set \(\{5,6,...,14\}\). The symbol \(\lfloor\cdot\rceil\) denotes the rounding to closest integer operation. The corresponding system models of binomial and negative binomial can be found in Appendix C (systems 3 and 6, respectively) of the manuscript.

Figure 2: **(a)** shows additional results of simulation study performed in Section 6 of the manuscript for the distributions: Negative Binomial and Binomial. The experimental setup is the same as described in Section 6, with the differences being that only \(10\) function pairs were tested, the number of outcomes of \(M\) was restricted to \(2\) and \(4\), and the support of \(M\) was appropriately modified for Binomial and Negative Binomial distributions (see caption of Table 2 below). The function pairs tested are provided in Table 2. **(b)** and **(c)** show the unique information atoms of the systems having non-zero unique information in both \(X\) and \(Y\) tested in the simulation study presented in Section 6 of the manuscript. More specifically, of the 20 pairs analyzed in Section 6, we only selected trials that had both \(UI_{x}>0.01\) bits and \(UI_{y}>0.01\) bits. **(d)** provides the corresponding median error for the subselected trials (originally shown in Fig.1c in the manuscript for all trials) for the systems shown in **(b)** and **(c)**.

[MISSING_PAGE_FAIL:74]

Exponential DistributionExponential distribution is special case of the Gamma distribution with \(\alpha=1\). The p.d.f. of the exponential distribution is described by its rate parameter \(\lambda>0\). We use \(\text{Exponential}(\lambda)\) to denote the p.d.f. of a random variable \(X\) distributed according to the exponential distribution. The exact definition of the p.d.f. of \(X\) is provided below:

\[p(X=x)=\text{Exponential}(x;\lambda)=\lambda e^{-\lambda x}\; \forall\;x\in[0,\infty).\] (317)

Beta DistributionBeta distribution is a continuous distribution having support over \([0,1]\) or \((0,1)\) depending upon the values of its parameter. The p.d.f. of the beta distribution is described by two shape parameters: \(\alpha>0\) and \(\beta>0\). We use \(\text{Beta}(\alpha,\beta)\) to denote the p.d.f. of a random variable \(X\) distributed according to the beta distribution. The exact definition of the p.d.f. of \(X\) is provided below:

\[p(X=x)=\text{Beta}(x;\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{ \Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\;\forall\;x\in(0,1),\]

where \(\Gamma(\cdot)\) is the Gamma function. The end-point 0 is included in the support of the beta distribution if \(\alpha\geq 1\) and the end-point \(1\) is included in the support of the beta distribution if \(\beta\geq 1\).

Dirichlet DistributionDirichlet distribution is a multivariate generalization of the beta distribution. The p.d.f. of Dirichlet distribution is described by \(d\)-shape parameters: \(\{\alpha_{1},\ldots,\alpha_{d}\}\), where \(\alpha_{i}\in(0,\infty)\;\forall\;i\in[d]\). We use \(\text{Dirichlet}(\alpha_{1},\ldots,\alpha_{d})\) to denote the p.d.f. of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) distributed according to the Dirichlet distribution. The exact definition of the p.d.f. of \(X\) is provided below:

\[p(\vec{\mathbf{X}}=\vec{\mathbf{x}})=\text{Dirichlet}(\vec{ \mathbf{x}};\alpha_{1},\ldots,\alpha_{d})=\frac{\Gamma\left(\sum_{i=1}^{d} \alpha_{i}\right)}{\prod_{i=1}^{d}\Gamma(\alpha_{i})}\prod_{i=1}^{d}x_{i}^{ \alpha_{i}-1},\]

where \(\Gamma(\cdot)\) is the Gamma function, and \(\vec{\mathbf{x}}=[x_{1}\quad\ldots\quad x_{d}]^{T}\), with \(x_{i}\in[0,1]\) and \(\sum_{i=1}^{d}x_{i}=1\).

Binomial DistributionBinomial distribution is a discrete distribution having support over \(\{0,\ldots,N\}\) for some \(N\in\mathbb{N}_{0}\). The p.m.f. of the binomial distribution is described by two parameters: the total number of trials, \(N\), and the success probability, \(p\). We use \(\text{Binomial}(N,p)\) to denote the p.m.f. of a random variable \(X\) distributed according to the binomial distribution. The exact definition of the p.m.f. of \(X\) is provided below:

\[P(X=x)=\text{Binomial}(x;N,p)=\binom{N}{x}p^{x}(1-p)^{n-x}\; \forall\;n\in\{0,\ldots,N\},\] (318) \[\text{where }\binom{N}{x}=\frac{N!}{N!(N-x)!}.\]

Multinomial DistributionMultinomial distribution is a multivariate generalization of the binomial distribution. The p.m.f. of the multinomial distribution is described by two parameters: the total number of trials, \(N\), and the success probability vector, \(\vec{\mathbf{p}}\). We use \(\text{Multinomial}_{d}(N,\vec{\mathbf{p}})\) to denote the p.m.f. of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) distributed according to the multinomial distribution. The exact definition of the p.m.f. of \(\vec{\mathbf{X}}\) is provided below:

\[P(\vec{\mathbf{X}}=\vec{\mathbf{x}})=\text{Multinomial}_{d}( \vec{\mathbf{x}};N,\vec{\mathbf{p}})=\frac{N!}{\prod_{i=1}^{d}x_{i}!}\prod_{ i=1}^{d}p_{i}^{x_{i}},\]

where, \(\vec{\mathbf{p}}=[\begin{array}{cccc}p_{1}&\ldots&p_{d}\end{array}]^{T}\) is a \(d\)-dimensional probability vector, i.e. \(0\leq p_{i}\leq 1,\;\forall\;1\leq i\leq d\) and \(\sum_{i=1}^{d}p_{i}=1\), and \(\vec{\mathbf{x}}=[\begin{array}{cccc}x_{1}&\ldots&x_{d}\end{array}]^{T}\) is a \(d\)-dimensional categorical vector, i.e. \(x_{i}\in\mathbb{N}_{0},\;\forall\;1\leq i\leq d\), and \(\sum_{i=1}^{d}x_{i}=n\).

Hypergeometric DistributionThe p.m.f. of the hypergeometric distribution is described by three parameters: the total number of success states in the population \(S\in\mathbb{N}_{0}\), the total number of failure states \(F\in\mathbb{N}_{0}\) and the number of draws \(n\in\{0,\ldots,S+F\}\). We use \(\text{Hypergeometric}(S,K,n)\) to denote the p.m.f. of a random variable \(X\) distributed according to the hypergeometric distribution. The exact definition of the p.m.f. of \(X\) is provided below:

\[P(X=x) =\text{Hypergeometric}(x;S,K,n)\] \[=\frac{\binom{S}{x}\binom{F}{n-x}}{\binom{S+F}{n}}\;\forall\;x\in \{\max\{0,n-F\},...,\min\{n,S\}\},\] \[\text{where}\;\binom{A}{B}=\frac{A!}{B!(A-B)!}\;\text{for some}\;A,B\in \mathbb{N}_{0}\;\text{and}\;B\leq A.\]

Multivariate Hypergeometric DistributionMultivariate hypergeometric distribution is a multivariate generalization of hypergeometric distribution. The p.m.f. of the multivariate hypergeometric distribution is described by the following parameters: the total number of object types \(d\in\mathbb{N}_{0}\), the size of population of each type \(\{n_{1},\ldots,n_{d}\}\) with \(n_{i}\in\mathbb{N}_{0}\), and the number of draws \(n\in\{0,\ldots,\sum_{i=1}^{d}n_{i}\}\). Define \(\vec{\mathbf{n}}=[n_{1}\quad\ldots n_{d}]\). Then, we use Multivariate Hypergeometric\((\vec{\mathbf{n}},n)\) to denote the p.m.f. of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) distributed according to the multivariate hypergeometric distribution. The exact definition of the p.m.f. of \(\vec{\mathbf{X}}\) is provided below:

\[P(\vec{\mathbf{X}}=\vec{\mathbf{x}}) =\text{Multivariate Hypergeometric}(\vec{\mathbf{x}};\vec{ \mathbf{n}},n),\] \[=\frac{\prod_{i=1}^{d}\binom{n_{i}}{x_{i}}}{\binom{\sum_{i=1}^{d }n_{i}}{n}},\] (319)

where \(\binom{A}{B}=\frac{A!}{B!(A-B)!}\) for some \(A,B\in\mathbb{N}_{0}\) and \(B\leq A\), and \(\vec{\mathbf{x}}=[x_{1}\quad\ldots\quad x_{d}]^{T}\) with \(x_{i}\in\{0,n_{i}\}\) and \(\sum_{i=1}^{d}x_{i}=n\).

Uniform DistributionUniform distribution is a continuous distribution having support over \((a,b)\) where \(a,b\in\mathbb{R}\). We use Uniform\((a,b)\) to denote the p.d.f. of a random variable \(X\) distributed according to the uniform distribution. The exact definition of the p.d.f. of \(X\) is provided below:

\[p(X=x)=\text{Uniform}(x;a,b)=\frac{1}{|a-b|}\;\forall\;x\in[a,b].\]

For uniform distribution, we abuse notation by not ensuring \(a<b\) for representing the interval \([a,b]\). In the context of uniform distribution, the notation \([a,b]\) is to be understood as \([a,b]\) if \(a\leq b\) and \([b,a]\) if \(a\geq b\).

Bernoulli DistributionThe p.m.f. of the random variable \(X\) distributed according to the Bernoulli distribution is denoted as Bernoulli\((p)\) for \(p\in[0,1]\). The exact definition of the p.d.f. of \(X\) is provided below:

\[P(X=0)=p,\;\text{and}\;P(X=1)=1-p.\] (320)

Categorical DistributionThe p.m.f. of a random variable \(X\) distributed according to the categorical distribution is denoted as Categorical\((p_{1},\ldots,p_{d})\) for \(p_{i}\in[0,1]\;\forall\;i\in[d]\) and \(\sum_{i=1}^{d}p_{i}=1\). The exact definition of the p.d.f. of \(X\) is provided below:

\[P(X=i)=p_{i}\;\forall\;i\in[d].\] (321)

### Univariate Stable Distribution Definition

We employ the definition of a univariate stable distribution discussed in [23]:

**Definition 2**.: _A random variable \(X\) is said to have a stable continuous distribution if, for two independent copies of \(X\), denoted as \(X_{1}\) and \(X_{2}\), and positive constants \(a>0\) and \(b>0,\) (322) holds for some \(c>0\) and \(d\in\mathbb{R}\)._

\[aX_{1}+bX_{2}\stackrel{{ d}}{{=}}cX+d,\;\text{where}\stackrel{{ d}}{{=}}\text{means equality in distribution \@@cite[cite]{[\@@bibref{}{S}{}{}]}}.\] (322)Continuous stable distributions are typically expressed through their characteristic functions, as the analytical form of many continuous stable distributions is not known. The characteristic function of a random variable \(X\) having a continuous stable distribution is typically parameterized by four parameters: stability parameter denoted as \(\alpha\in(0,2]\), skewness parameter denoted as \(\beta\in[-1,1]\), scale parameter denoted as \(\gamma\in(0,\infty)\), and location parameter denoted as \(\mu\in\mathbb{R}\). The exact form of the characteristic function is given in (323).

\[\mathbb{E}\left[e^{itX}\right] =\phi(t;\alpha,\beta,\gamma,\mu)=\exp\left(it\mu-|\gamma t|^{ \alpha}(1-i\beta\text{sgn}(t)\Phi(\alpha))\right),\] (323) \[\text{where }\Phi(\alpha) =\left\{\begin{array}{c}\tan\left(\frac{\pi\alpha}{2}\right) \quad\alpha\neq 1\\ \frac{-2}{\pi}\log(|t|)\quad\alpha=1\end{array}\right.,\quad\text{sgn}(t)= \left\{\begin{array}{c}-1\quad t<0\\ 0\quad t=0\\ 1\quad t>0\end{array}\right..\]

We further denote the p.d.f. of \(X\) having the characteristic function defined in (323) as \(p_{CS}(X;\alpha,\beta,\gamma,\mu)\). Some well-known examples of continuous stable distribution are the Gaussian/normal distribution, Cauchy distribution, and Levy distribution. All univariate continuous stable distributions have infinite variance except the Gaussian distribution, and their tails follow a power law behavior [23]. Univariate stable distributions have been widely used to model many financial and physical phenomena. Chapter 2 of [23] provides a list of applications where stable distributions have been used. For a more in-depth treatment of stable distributions, we refer the reader to [23].

### Multivariate Stable Distribution Definition

A multivariate stable distribution is defined similarly to a univariate stable distribution described in Appx. N.2. We employ the definition of multivariate distribution given in [23].

**Definition 3**.: _A non-generate \(d\)-dimensional random variable \(\vec{\mathbf{X}}\) is said to have a multivariate stable continuous distribution if, for two independent copies of \(\vec{\mathbf{X}}\), denoted as \(\vec{\mathbf{X}}_{1}\) and \(\vec{\mathbf{X}}_{2}\), and positive constants \(a>0\) and \(b>0\), (324) holds for some \(c>0\) and \(\vec{\mathbf{d}}\in\mathbb{R}^{d}\)._

\[a\vec{\mathbf{X}}_{1}+b\vec{\mathbf{X}}_{2}\overset{d}{=}c\vec{\mathbf{X}}+ \vec{\mathbf{d}},\text{ where }\overset{d}{=}\text{means equality in distribution \@@cite[cite]{[\@@bibref{}{DBL}{}{}]}}.\] (324)

A main source of complexity while analyzing multivariate continuous stable distributions is the wide range of dependence structures this family of distribution admits [23]. We refer the reader [46; 45] for a more general treatment of multivariate stable distributions. In this work, we focus on two special classes of multivariate stable distributions: independent component multivariate stable continuous distribution denoted as \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\) and elliptically contoured multivariate stable distribution denoted as \(p_{CS-EC}(\alpha,\boldsymbol{\Sigma},\vec{\boldsymbol{\mu}})\). Similarly to the univariate continuous stable distribution, both independent component and elliptically contoured multivariate stable distributions do not have explicit general analytical forms and are expressed through their characteristic functions.

The characteristic function of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) having independent component multivariate distribution \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\) is expressed as follows:

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}}\right] =\exp\left(-\sum_{j=1}^{d}|\gamma_{j}t_{j}|^{\alpha}(1-i\beta_{j}\text{sgn}(t_ {j})\Phi(\alpha))+i\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\mu}}\right)\ \forall\ \vec{ \mathbf{t}}\in\mathbb{R}^{d},\] (325)

where \(\Phi(\alpha)\) and \(\text{sgn}(\cdot)\) are as defined in (323), \(\vec{\boldsymbol{\beta}}=\left[\beta_{1}\quad\ldots\quad\beta_{d}\right]^{T}\) with \(\beta_{j}\in[-1,1]\), \(\vec{\boldsymbol{\gamma}}=\left[\gamma_{1}\quad\ldots\quad\gamma_{d}\right]^ {T}\) with \(\gamma_{j}\in(0,\infty)\), \(\vec{\boldsymbol{\mu}}=\left[\mu_{1}\quad\ldots\quad\mu_{d}\right]^{T}\in \mathbb{R}^{d}\), and \(\alpha\in(0,2]\). One can also alternatively think of the random vector \(\vec{\mathbf{X}}=\left[X_{1}\quad\ldots\quad X_{d}\right]^{T}\) having the distribution \(p_{CS-IC}(\alpha,\vec{\boldsymbol{\beta}},\vec{\boldsymbol{\gamma}},\vec{ \boldsymbol{\mu}})\) as a collection of independent random variables \(\{X_{j}\}_{j=1}^{d}\), where \(X_{j}\sim p_{CS}(\alpha,\beta_{j},\gamma_{j},\mu_{j})\).

The characteristic function of a \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) having elliptically-contoured multivariate distribution \(p_{CS-EC}(\alpha,\boldsymbol{\Sigma},\vec{\boldsymbol{\mu}})\) is expressed as follows:

\[\mathbb{E}\left[e^{i\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}}\right] =\exp\left(-\left(\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}\vec{\mathbf{t}}\right)^ {\nicefrac{{\alpha}}{{2}}}+i\vec{\mathbf{t}}^{T}\vec{\boldsymbol{\mu}}\right) \ \ \forall\ \vec{\mathbf{t}}\in\mathbb{R}^{d},\] (326)

where \(\boldsymbol{\Sigma}\) is a positive definite matrix, \(\vec{\boldsymbol{\mu}}=\left[\mu_{1}\quad\ldots\quad\mu_{d}\right]^{T}\in \mathbb{R}^{d}\), and \(\alpha\in(0,2]\). Note that allowing \(\boldsymbol{\Sigma}\) to be positive semi-definite will violate the fact that \(\vec{\mathbf{X}}\) should be a non-degenerate \(d\)-dimensional random variable. We also list an important property of multivariate continuous stable distributions used in this paper.

**Property 1**.: _A \(d\)-dimensional random vector \(\vec{\mathbf{X}}\) has a multivariate stable continuous distribution if and only if every \(1\)-dimensional projection has a univariate continuous stable distribution, i.e., \(\vec{\mathbf{t}}^{T}\vec{\mathbf{X}}\sim p_{CS}(\alpha,\beta(\vec{\mathbf{t}}), \gamma(\vec{\mathbf{t}}),\mu(\vec{\mathbf{t}}))\vee\vec{\mathbf{t}}\in\mathbb{ R}^{d}.\)_

Proof.: See the proof of P.1. in [46]. 

### Univariate Discrete Stable Distributions

Discrete stable distributions form the discrete analogues of the continuous stable distributions. We employ the following definition for a univariate discrete distributions taken from [24]:

**Definition 4**.: _A random variable \(X\) is said to have a discrete stable distribution with exponent \(\nu\in(0,1]\) if for two independent copies of \(X\), denoted as \(X_{1}\) and \(X_{2}\), (322) holds for all \(\gamma\in[0,1].\)_

\[\gamma\circ X_{1}+(1-\gamma^{\nu})^{\nicefrac{{1}}{{\nu}}}\circ X_{2}\overset {d}{=}X,\] (327)

_where \(\overset{d}{=}\) means equality in distribution [57], and \(\circ\) implies a binomial thinning operation defined as follows: \(\gamma\circ X=\sum_{j=1}^{X}N_{j},\) where \(N_{j}\sim\text{Bernoulli}(\gamma),\) and all \(N_{j}^{\prime}s\) are jointly independent._

Similar to univariate continuous stable distributions, univariate discrete stable distribution do not have explicit analytical forms, and are typically expressed through their probability generating functions and/or characteristic functions. The characteristic function of a discrete random variable \(N\) having a stable discrete distribution is given by (328):

\[\mathbb{E}\left[e^{itN}\right]=\exp\left(\tau(e^{i\iota}-1)^{\nu}\right)= \phi(t;\nu,\tau),\text{ and }\mathrm{P}_{N}(z)=\exp(-\tau(1-z)^{\gamma}),\] (328)

where \(\mathrm{P}_{N}(z;\nu,\tau)\) is the probability generating function of \(N\), \(i=\sqrt{-1}\), \(\tau>0\), and \(0<\nu\leq 1\). We further denote the probability mass function of \(N\) as \(P_{DS}(N=n;\nu,\tau)\). \(P_{DS}(N;1,\lambda)\) is a standard Poisson distribution with mean \(\lambda\)[24]. All discrete stable distribution asymptotically follow a power law distribution [64].

### Multivariate Poisson Distribution

To our knowledge, there are no well-known multivariate generalizations of univariate discrete stable distributions, except for the special case of the Poisson distribution. In this section, we describe a popular multivariate extension of the Poisson distribution.

An intuitive way to define multivariate Poisson distribution is to represent each random variable in the multivariate Poisson distribution as a sum of independent Poisson random variables [41, 42, 43, 44]. To illustrate, let us construct a bivariate Poisson random vector \(\vec{\mathbf{N}}=\left[N_{1}\quad N_{2}\right]^{T}\), where

\[N_{1}=N_{1}^{g}+N_{12}^{g},\ N_{2}=N_{2}^{g}+N_{12}^{g},\] (329)

and \(N_{1}^{g},N_{2}^{g}\), and \(N_{12}^{g}\) are mutually independent Poisson random variables with rates \(\lambda_{1},\lambda_{2}\), and \(\lambda_{12}\), respectively. Here, the dependence between \(N_{1}\) and \(N_{2}\) is expressed through \(N_{12}^{g}\), with covariance between \(N_{1}\) and \(N_{2}\) being equal to \(\lambda_{12}\)[47]. For dimensions \(d>2\), the Poisson random vector:

\[\vec{\mathbf{N}}=\mathbf{A}\vec{\mathbf{N}}^{g},\] (330)

where \(\mathbf{A}\) is an appropriate matrix of \(0\)'s and \(1\)'s. We can decompose \(\mathbf{A}=\left[\mathbf{A}_{1}\quad\ldots\quad\mathbf{A}_{d^{\prime}}\right]\), where \(\mathbf{A}_{i}\) is a \(d\times\binom{d}{i}\) submatrix having no duplicate columns, and each of its columns containing exactly \(i\) ones and \((d-i)\) zeros [44], and \(\vec{\mathbf{N}}^{g}=[N_{1}^{g}\ldots N_{d}^{g}\ N_{12}^{g}\ldots\ N_{(d-1)d}^{g}\ldots\ N_{d-(d^{\prime}-1)\ldots d}^{g}]^ {T}\), with \(N_{i_{1}\ldots i_{j}}^{g}\sim\text{Poisson}(\lambda_{i_{1}\ldots i_{j}})\ \forall\ (i_{1},\ldots,i_{j})\in\mathbb{A}_{d}^{d},j\in[d^{\prime}]\). Furthermore, the random variables \(\{N_{1},\ldots,N_{d-(d^{\prime}-1)\ldots d}\}\) are mutually independent. An intuitive way to think about this multivariate Poisson distribution is to interpret \(\{N_{i}^{g}\}_{i=1}^{d}\) as the "main effect", \(\{N_{i_{1}\ldots i_{2}}^{g}\}_{(i_{1},i_{2})\in\mathbb{A}_{2}^{d}}\) as the "two-way covariance effects" and so on until \(\{N_{i_{1}\ldots i_{d^{\prime}}}^{g}\}_{(i_{1},\ldots,i_{d^{\prime}})\in \mathbb{A}_{d^{\prime}}^{d}}\) as the \(d^{\prime}\)-way covariance effect in an ANOVA-like fashion [44]. For a more detailed discussion, see [43, 44, 47, 42]. Note that \(d^{\prime}\leq d\). In this work, we denote the \(d\)-dimensional random vector \(\vec{\mathbf{N}}\) having a multivariate Poisson distribution with parameters \(\vec{\mathbf{\Lambda}}=\left[\lambda_{1}\ \ldots\ \lambda_{d}\ \lambda_{12}\ \ldots \lambda_{d-(d^{\prime}-1)\ldots d}\right]^{T}\) specifying up to \(d^{\prime}\)-way covariance effects (where \(d^{\prime}\leq d\)), as \(\vec{\mathbf{N}}\sim\text{Poisson}(d,d^{\prime},\vec{\mathbf{\Lambda}})\). Note that for \(d^{\prime}=1\), we have that \(K\) is a collection of independent Poisson random variables, and when both \(d=d^{\prime}=1\), we recover the scalar Poisson distribution: \(\Pr(K=k)=\frac{e^{-\lambda\lambda^{k}}}{k!},\ \forall\ k\in\mathbb{N}_{0}.\)

#### Deriving the p.m.f. of the Multivariate Poisson Distribution

Let \(\vec{\mathbf{N}}\sim\text{Poisson}(d,d^{\prime},\vec{\mathbf{\Lambda}})\), where \(\vec{\mathbf{N}}\) is a \(d\)-dimensional random vector. Then the corresponding p.m.f. of \(\vec{\mathbf{N}}\) is described below:

Let \(\vec{\mathbf{n}}^{\prime}\!=\!\left[n_{12}\,\ldots\,n_{(d-1)d}\,\ldots\,n_{d-(d ^{\prime}-1)\ldots d}\right]^{T}\), and \(d_{\vec{\mathbf{n}}^{\prime}}\) be the dimension of \(\vec{\mathbf{n}}^{\prime}\), then:

\[P(\vec{\mathbf{N}}\!\!=\!\vec{\mathbf{n}})\!=\!e^{-\mathbf{1}^{T }\vec{\mathbf{\Lambda}}}\prod_{i=1}^{d}\lambda_{i_{1}}^{n_{i}}\sum_{\vec{ \mathbf{n}}^{\prime}\in C}\left(\prod_{(i_{1},i_{2})\in\mathbb{A}_{2}^{d}} \!\left(\frac{\lambda_{i_{1}i_{2}}}{\lambda_{i_{1}}\lambda_{i_{2}}}\right)^{n_ {i_{1}i_{2}}}\times\right.\] \[\left.\ldots\times\prod_{(i_{1},\ldots,i_{d^{\prime}})\in\mathbb{A }_{d^{\prime}}^{d}}\left(\frac{\lambda_{i_{1},\ldots,i_{d^{\prime}}}}{\prod_{j =1}^{d^{\prime}}\lambda_{i_{j}}}\right)^{n_{i_{1}\ldots i_{d^{\prime}}}}\times Q (\vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime})\right),\] (331)

where \(C=\{\vec{\mathbf{n}}^{\prime}\in\mathbb{N}_{0}^{d_{d^{\prime}}}\!:\!(\vec{ \mathbf{a}}_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime}\leq n_{i}\;\forall\;i \in[d]\}\), and

\[Q(\vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime})=\prod_{i=1}^{d}\frac{1}{(n_{i}- \vec{\mathbf{a}}_{i}^{\prime T}\vec{\mathbf{n}}^{\prime})!}\prod_{j=2}^{d^{ \prime}}\prod_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}\frac{1}{n_{i_{1} \ldots i_{j}}^{g}!},\]

with \(\vec{\mathbf{a}}_{i}^{\prime}\) being the \(i\)-th row of the matrix \(\mathbf{\Lambda}^{\prime}=[\mathbf{\Lambda}_{2}\ldots\mathbf{\Lambda}_{d^{ \prime}}]\).

Proof of \(\vec{\mathbf{N}}\sim\text{Poisson}(d,d^{\prime},\vec{\mathbf{\Lambda}})\) having the p.m.f. described in (331).

We know that \(\vec{\mathbf{N}}=\mathbf{A}\vec{\mathbf{N}}_{g}\), where \(\mathbf{\Lambda}=[\mathbf{\Lambda}_{1}\ldots\mathbf{\Lambda}_{d^{\prime}}]\). Let the dimension of \(\vec{\mathbf{N}}^{g}\) be \(d_{\vec{\mathbf{N}}^{g}}\) and \(\mathcal{S}_{\vec{\mathbf{n}}}=\{\vec{\mathbf{n}}^{g}\in\mathbb{N}_{0}^{d_{ \vec{\mathbf{n}}^{g}}}\!:\!\mathbf{k}=A\mathbf{k}^{g}\}\). Then, using the fact \(\vec{\mathbf{N}}=A\vec{\mathbf{N}}^{g}\), the p.m.f. of \(\vec{\mathbf{N}}\) can be expressed using the p.m.f. of \(\vec{\mathbf{N}}^{g}\) in the following manner:

\[P(\vec{\mathbf{N}}=\vec{\mathbf{n}})=\sum_{\vec{\mathbf{n}}^{g}\in\mathcal{S }_{\vec{\mathbf{n}}}}P(\vec{\mathbf{N}}^{g}=\vec{\mathbf{n}}^{g}).\] (332)

Since \(\vec{\mathbf{N}}^{g}\) is just a collection of mutually independent Poisson random variables, we can write \(P(\vec{\mathbf{N}}^{g}=\vec{\mathbf{n}}^{g})\) as a product of scalar Poisson distributions, i.e.,

\[P(\vec{\mathbf{N}}^{g}=\vec{\mathbf{n}}^{g})=\prod_{j=1}^{d^{\prime}}\prod_{( i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}e^{\lambda_{i_{1}\ldots i_{j}}}\frac{ \lambda_{i_{1}\ldots i_{j}}^{n_{i_{1}\ldots i_{j}}^{g}}}{n_{i_{1}\ldots i_{j}} ^{g}!}.\] (333)

Substituting (333) into (332), and collecting all \(e^{\lambda_{i_{1}\ldots i_{j}}}\) terms we obtain:

\[P(\vec{\mathbf{N}}\!\!=\!\vec{\mathbf{n}})\!=\!e^{-\mathbf{1}^{T}\vec{\mathbf{ \Lambda}}}\sum_{\vec{\mathbf{n}}^{g}\in\mathcal{S}_{\vec{\mathbf{n}}}}\prod_{j= 1}^{d^{\prime}}\prod_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}\frac{ \lambda_{i_{1}\ldots i_{j}}^{n_{i_{1}\ldots i_{j}}^{g}}}{n_{i_{1}\ldots i_{j}} ^{g}!}.\] (334)

Decompose the matrix \(\mathbf{\Lambda}=[\mathbf{\Lambda}_{1}\;\mathbf{\Lambda}^{\prime}]\), where \(\mathbf{\Lambda}^{\prime}=[\mathbf{\Lambda}_{2}\ldots\mathbf{\Lambda}_{d^{ \prime}}]\) and \(\vec{\mathbf{n}}^{g}=[(\vec{\mathbf{n}}_{1}^{g})^{T}\;\vec{\mathbf{n}}^{ \prime T}]^{T}\), where \(\vec{\mathbf{n}}^{g}_{1}=[n_{1}^{g}\ldots n_{d}^{g}]^{T}\) and \(\vec{\mathbf{n}}^{\prime}\) contains the rest of the elements in \(\vec{\mathbf{n}}^{g}\). Using the fact that \(\vec{\mathbf{n}}^{g}\in\mathcal{S}_{\vec{\mathbf{n}}}\Rightarrow\vec{\mathbf{n}} =\mathbf{A}\vec{\mathbf{n}}^{g}\) we have: \(\vec{\mathbf{n}}=\mathbf{\Lambda}_{1}\vec{\mathbf{n}}_{1}^{g}+\mathbf{\Lambda}^{ \prime}\vec{\mathbf{n}}^{\prime}.\) Combining this with the fact that \(\mathbf{\Lambda}_{1}\) is an identity matrix, we obtain:

\[\vec{\mathbf{n}}=\vec{\mathbf{n}}_{1}^{g}+\mathbf{\Lambda}^{\prime}\vec{ \mathbf{n}}^{\prime}\Rightarrow\vec{\mathbf{n}}_{1}^{g}=\vec{\mathbf{n}}- \mathbf{\Lambda}^{\prime}\vec{\mathbf{n}}^{\prime}\Rightarrow n_{i}^{g}=n_{i}-( \vec{\mathbf{a}}_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime},\] (335)

where \(\vec{\mathbf{a}}_{i}^{\prime}\) is the\(n_{i}^{g}=n_{i}-(\vec{\mathbf{a}}_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime}\), and \(n_{i}^{g}\geq 0\). Collecting all the factorial terms, we get:

\[Q(\vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime})=\prod_{i=1}^{d}\frac{1}{(n_{i}- \vec{\mathbf{a}}_{i}^{\prime T}\vec{\mathbf{n}}^{\prime})!}\prod_{j=2}^{d^{ \prime}}\prod_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}\frac{1}{n_{i_{1} \ldots i_{j}}^{g}!}.\] (337)

Substituting (337) into (336), we obtain:

\[P(\vec{\mathbf{N}}\!\!=\!\vec{\mathbf{n}})\!=\!e^{-\vec{\mathbf{n}}^{T}\vec{ \mathbf{A}}}\sum_{\vec{\mathbf{n}}^{\prime}\in\mathcal{S}_{\vec{\mathbf{n}}^ {\prime}}}\left[\prod_{i_{1}\in\mathbb{A}_{1}^{d}}\lambda_{i_{1}}^{n_{i}-( \vec{\mathbf{a}}_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime}}\prod_{j=2}^{d^{ \prime}}\prod_{(i_{1},\ldots,i_{j})\in\mathbb{A}_{j}^{d}}\lambda_{i_{1}\ldots i _{j}}^{n_{i_{1}\ldots i_{j}}^{g}}\right]Q(\vec{\mathbf{n}},\vec{\mathbf{n}}^{ \prime}).\] (338)

Taking the term \(\prod_{i_{1}\in\mathbb{A}_{1}}\lambda_{i_{1}}^{n_{i}}\) out of the summation as it does not depend upon \(\vec{\mathbf{n}}^{\prime}\), we obtain:

\[P(\vec{\mathbf{N}}\!\!=\!\vec{\mathbf{n}})\!=\!e^{-\vec{\mathbf{n}}^{T}\vec{ \mathbf{A}}}\prod_{i_{1}\in\mathbb{A}_{1}^{d}}\lambda_{i_{1}}^{n_{i}}\sum_{ \vec{\mathbf{n}}^{\prime}\in\mathcal{S}_{\vec{\mathbf{n}}^{\prime}}}\left[ \prod_{i_{1}\in\mathbb{A}_{1}^{d}}\lambda_{i_{1}}^{-(\vec{\mathbf{a}}_{i}^{ \prime})^{T}\vec{\mathbf{n}}^{\prime}}\prod_{j=2}^{d^{\prime}}\prod_{(i_{1}, \ldots,i_{j})\in\mathbb{A}_{j}^{d}}\lambda_{i_{1}\ldots i_{j}}^{n_{i_{1} \ldots i_{j}}^{g}}\right]Q(\vec{\mathbf{n}},\vec{\mathbf{n}}^{\prime}).\] (339)

Notice that \((a_{i}^{\prime})^{T}\vec{\mathbf{n}}^{\prime}\) contains all the elements of \(\vec{\mathbf{n}}^{\prime}\) which have \(i\) in their subscript. Hence, expanding the term \(\prod_{i_{1}\in\mathbb{A}_{1}^{d}}\lambda_{i_{1}}^{-(\vec{\mathbf{a}}_{1}^{ \prime})^{T}\vec{\mathbf{n}}^{\prime}}\) and distributing over the product, we obtain the desired form:

\[P(\vec{\mathbf{N}}\!\!=\!\vec{\mathbf{n}})\!=\!e^{-\vec{\mathbf{ n}}^{T}\vec{\mathbf{A}}}\prod_{i=1}^{d}\lambda_{i}^{n_{i}}\sum_{\vec{\mathbf{n}}^{ \prime}\in C}\left(\prod_{(i_{1},i_{2})\in\mathbb{A}_{2}^{d}}\left(\frac{ \lambda_{i_{1}i_{2}}}{\lambda_{i_{1}}\lambda_{i_{2}}}\right)^{n_{i_{1}i_{2}}}\times\right.\] \[\ldots\times\left.\prod_{(i_{1},\ldots,i_{d^{\prime}})\in\mathbb{ A}_{d^{\prime}}^{d}}\left(\frac{\lambda_{i_{1}\ldots i_{d^{\prime}}}}{\prod_{j=1}^{d^{ \prime}}\lambda_{i_{j}}}\right)^{n_{i_{1}\ldots i_{d^{\prime}}}}\times Q(\vec{ \mathbf{n}},\vec{\mathbf{n}}^{\prime})\right).\] (340)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The three main results of this work are clearly enumerated in Section 1 (Introduction) that demonstrate the main claim of the work which is analytically computing partial information decomposition for numerous systems employing well-known distributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have provided a discussion on the limitations of this work in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: A formal proof of each theorem is provided in the Appx. F- L, with clearly stated assumptions. A formal justification of the arguments used in Sec. 6 is provided in Appx. M. Section 3 discusses the general proof technique used in proving our main theorems. We also provide informal discussions on the constructions used for proving theorems 1- 7 in their respective sections, namely Sec. 4 and Sec. 5. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The details of our simulation results are provided in Section 6, followed by more details in Appendix M.3. We have also provided the exact code used for running the simulation study. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided our code with the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided all simulation details in Section 6 and Appendix L.3. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification:We have provided a box-plot of our results accompanied by the actual data points used for computing the box-plot in Figure 1. The factors of variability are clearly stated in Sec. 6. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We did not feel the need to provide information on the compute resources required to run our simulation, as our main results are of theoretical nature. Furthermore, all our simulation results can be reproduced on any ordinary personal laptop and do not require any special compute resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted is mainly theoretical in nature and we believe it does not violate the NeurIPS code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: While our research is mainly theoretical, we have tried to anticipate how it might affect the society negatively and positively. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We believe our work poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have provided the citation and the corresponding license for the code we used for running our simulation in Section 6. Guidelines: * The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release any new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.