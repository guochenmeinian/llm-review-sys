# Fantastic Robustness Measures:

The Secrets of Robust Generalization

 Hoki Kim

Seoul National University

ghr19613@snu.ac.kr

Jinseong Park

Seoul National University

jinseong@snu.ac.kr

Yujin Choi

Seoul National University

uznhigh@snu.ac.kr

Jaewook Lee

Seoul National University

jaewook@snu.ac.kr

###### Abstract

Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the \(L_{\infty}\) norm and further validate our findings through an evaluation of more than 100 models from RobustBench [12] across CIFAR-10, CIFAR-100, and ImageNet. We hope this work can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.

## 1 Introduction

Deep neural networks have achieved tremendous success in various domains, but their vulnerability to subtle perturbations has been revealed through the existence of adversarial examples, which are not generally perceptible to human beings [57, 20]. To obtain robustness against these adversarial examples, numerous defense methods have been proposed, and among them, adversarial training has become a common algorithm because of its effectiveness and ease of implementation [42, 71, 61]. However, researchers have recently found that adversarial training methods also suffer from the problem of overfitting [63, 52], where an adversarially trained model shows high robustness on training examples, yet significantly reduced robustness on test examples. As this robust overfitting progresses, the robust generalization gap increases, resulting in poor robustness for unseen examples.

To prevent robust overfitting and achieve high robust generalization, researchers have analyzed the properties of adversarial training and demonstrated the usefulness of some measures, such as margin-based measures, flatness-based measures, and gradient-based measures [53, 67, 66, 56]. Researchers have used them to estimate the robust generalization gap of given models and further developed new training schemes for improving its robustness [44, 66]. While these measures offer significant insights into robust generalization, we find that the evaluation of some measures is often limited due to the lack of models or training setups. These limitations can potentially lead to misleading conclusions, which may include inaccurate estimations of the robust generalization gap and misguided directions for the further development of adversarial training methods.

Therefore, to gain a more precise understanding of when and how these measures correlate with robust generalization, we train over 1,300 models on CIFAR-10 under the \(L_{\infty}\) norm across various training settings. We then investigate the relationships between a wide range of measures and their robust generalization gap. To further validate our findings, we also analyze over 100 models provided in RobustBench [12] across CIFAR-10, CIFAR-100, and ImageNet. Based on our large-scale study, we summarize our key findings as follows:

**Key findings.**

1. Due to the high sensitivity of the robust generalization gap to different training setups, the expectation of rank correlation across a wide range of training setups leads to high variance and may not capture the underlying trend.
2. Margin and smoothness exhibit significant negative correlations with the robust generalization gap. This suggests that, beyond a certain threshold, maximizing margin and minimizing smoothness can lead to a degradation in robust generalization performance.
3. Flatness-based measures, such as estimated sharpness, tend to exhibit poor correlations with the robust generalization gap. Rather, contrary to conventional assumptions, models with sharper minima can actually result in a lower robust generalization gap.
4. The norm of the input gradients consistently and effectively captures the robust generalization gap, even across diverse conditions, including fixed training methods and when conditioned on average cross-entropy.

To promote reproducibility and transparency in the field of deep learning, we have integrated the code used in this paper, along with pre-trained models, accessible to the public at https://github.com/Harry24k/MAIR. We hope that our findings and codes can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.

## 2 Related Work

The primary distinction between standard and adversarial training is that adversarial training aims to correctly classify not only benign examples but also adversarial examples as follows:

\[\min_{\bm{w}}\max_{\|\bm{x}^{adv}-\bm{x}\|\leq\epsilon}\mathcal{L}(f(\bm{x}^{ adv},\bm{w}),y),\] (1)

where \((\bm{x},y)\) is drawn from the training dataset \(\mathcal{S}\) and \(f\) represents the model with trainable parameters \(\bm{w}\). Among the distance metrics \(\|\cdot\|\), in this paper, we focus on robustness with respect to the \(L_{\infty}\) norm. Note that the loss function \(\mathcal{L}\) can also include \(f(\bm{x},\bm{w})\) to minimize the loss with respect to \(\bm{x}\). By optimizing (1), we hope to minimize the robust error on the true distribution \(\mathcal{D}\), defined as:

\[\mathcal{E}(\bm{w};\epsilon,\mathcal{D})=\mathbb{E}_{(\bm{x},y)\in\mathcal{D }}\left[\max_{\|\bm{x}^{adv}-\bm{x}\|\leq\epsilon}\mathbbm{1}(f(\bm{x}^{adv},\bm{w})\neq y)\right],\] (2)

where \(\mathbbm{1}(\hat{y}\neq y)\) is an indicator function that outputs \(0\) if the prediction \(\hat{y}\) is same as the true label \(y\), and \(1\) otherwise.

The majority of researches have focused on optimizing (1) through the development of new loss functions or training attacks. For instance, vanilla adversarial training (AT) [42] minimizes the loss of adversarial examples generated by projected gradient descent (PGD) [42] with multiple iterations. Following AT, several variations, such as TRADES [71] and MART [61], have achieved significant reductions in robust errors on various datasets through the adoption of KL divergence and the regularization on probability margins, based on theoretical and empirical analyses.

However, recent works have revealed that the adversarial training framework has a challenging generalization problem, characterized by higher Rademacher complexity [68] and larger sample complexity [54]. The overfitting problem in adversarial training has been observed as a common phenomenon across various settings [52], and it can even occur in a more catastrophic manner during single-step adversarial training [63; 31]. As robust overfitting progresses, the following robust generalization gap \(g(\bm{w})\) increases,

\[g(\bm{w})=\mathcal{E}(\bm{w};\epsilon,\mathcal{D})-\mathcal{E}(\bm{w}; \epsilon,\mathcal{S}).\] (3)Thus, in order to minimize \(\mathcal{E}(\bm{w};\epsilon,\mathcal{D})\), we should not only focus on minimizing the training objective function \(\mathcal{E}(\bm{w};\epsilon,\mathcal{S})\) but also on reducing the robust generalization gap \(g(\bm{w})\).

To gain a deeper understanding of the robust overfitting and further reduce the robust generalization gap \(g(\bm{w})\), a line of work has theoretically and empirically investigated measures, such as boundary thickness [67], local Lipschitzness [66], and flatness [56]. While these studies claim that these measures are reliable indicators of the robust generalization gap, the lack of consistency in experimental settings hinders us to identify their validity in practical scenarios. Therefore, in this work, we aim to investigate the clear relationship between these measures and the robust generalization gap by conducting a comprehensive analysis using a large set of models.

### Comparison to Jiang et al. [27]

The pioneering study [27] explored the empirical correlations between complexity measures and generalization with a primary focus on the standard training framework. Our main contribution is delving into the realm of measures within the adversarial training framework--a context having different generalization tendencies from those of the standard training framework [53; 66]. Indeed, we observe that the metric \(\psi_{k}\) proposed in [27] has limitations in capturing the effectiveness of measures due to the high sensitivity of the robust generalization gap with respect to training setups. By introducing a new metric \(\pi_{k}\), our work enhances the understanding of when and how robustness measures correlate with robust generalization. Furthermore, while Jiang et al. [27] employed customized parameter-efficient neural networks, we adopt widely-used model architectures such as ResNets, thereby providing insights that are not only relevant to recent research but also offer more practical implications.

## 3 Experimental Methodology

In the adversarial training framework, measures have played a crucial role by providing either theoretical upper bounds or empirical correlations with robust generalization. Previous works have leveraged these measures to propose new training schemes [61; 67; 64] and suggested directions to achieve high robustness [66; 56]. However, we discover that certain limitations and confusions exist when extending the findings of prior works to practical scenarios due to the use of a restricted set of models and training setups [67; 64; 66]. In order to gain a comprehensive understanding of the true efficacy of these measures, it is crucial to validate whether the effectiveness of measures remains valid in practical settings.

To address these challenges, our work aims to provide a comprehensive and accurate assessment of the effectiveness of measures for robust generalization in practical settings. Our objective is to address the fundamental question:

_Do measures remain effective in correlating with robust generalization in practical settings?_

_If so, how and when are measures correlated with robust generalization?_

To this end, in Sections 3.1 and 3.2, we carefully construct the training space that considers practical scenarios within the adversarial training framework and gather a wide range of measures from previous works. In Section 3.3, we define the evaluation metrics and introduce specific variations to accurately capture the correlation between measures and robust generalization in practical settings.

### Training Space

In the realm of adversarial training, various training procedures have been extensively explored to enhance adversarial robustness based on the development of AT, TRADES, and MART. Recently, to resolve the issue of robust overfitting, researchers have begun combining additional techniques, including commonly employed in the standard training framework, such as early-stopping [52], using additional data [9; 22], manipulating training tricks [48; 11], and adopting sharpness-aware minimization [64]. By integrating these techniques into AT, TRADES, and MART, high adversarial robustness have been achieved, outperforming other variants of adversarial training methods [21].

Based on these prior works, to mimic practical scenarios, we have carefully selected eight training parameters widely used for improving robust generalization: (1) Model architecture {ResNet18 [23],WRN28-10 [70], WRN34-10 [70]), (2) Training methods {Standard, AT [42], TRADES [71], MART [61]}, (3) Inner maximization steps {1, 10}, (4) Optimizer {SGD, AWP [64]}, (5) Batch-size {32, 64, 128, 256}, (6) Data augmentation {No Augmentation, Use crop and flip}, (7) Extra-data {No extra data, Use extra data [9]}, and (8) Early-stopping {No early-stopping, Use early-stopping}. Additional training details can be found in Appendix C, providing a comprehensive overview of the training procedures.

In total, 1,344 models were trained using the CIFAR-10 dataset with \(\epsilon=8/255\). Given these models, we evaluate their train/test robustness against PGD with 10 iterations (denoted as PGD10). While we acknowledge the existence of stronger adversarial attacks, such as AutoAttack [11], we primarily use PGD10 due to its prevalent use in adversarial training and the high computational demands of AutoAttack. Additionally, considering the usage of PGD in calculating specific measures, such as boundary_thickness and local_lip, ensures consistency in our analysis. For a detailed discussion, please refer to the Appendix A.4.

The statistics of the trained models are summarized in Figure 1. In the left plot, we can observe that the selected range of training parameters generates a diverse set of models, exhibiting robust generalization gaps ranging from 0% to 70%. Notably, certain models achieve 100% robustness on training data against PGD10, but their maximum robustness on the test set is only 65%, highlighting the importance of robust generalization gap. The right plot is a boxplot shows the distribution of robust generalization gaps for some training setups. As described in prior works [9; 52; 64], each training setup has a significant impact on robust generalization.

### Measures

Beyond the measures proposed under the adversarial training frameworks [67; 66], previous works [27; 17] have demonstrated that certain measures can effectively capture the generalization gap under the standard training framework. Therefore, in this paper, we have gathered diverse measures from both the standard and adversarial training frameworks and categorized them into five different types based on their origins and formulas: (i) weight-norm (7, 8, 9, 10), (ii) margin (12, 13, 14, 15), (iii) smoothness (16, 17), (iv) flatness (18, 19, 20, 21), and (v) gradient-norm (22, 23). We denote the chosen measures in teletype font (e.g., path_norm). While we briefly introduce the concepts of measures in each paragraph in Section 4, we refer the readers to Appendix B for the details of measures including their mathematical definitions due to the page limit.

Given these measures, we calculate their value on whole training examples for each trained model. This choice aligns with prior works, which argue that the most direct approach for studying generalization is to prove a generalization bound that can be calculated on the training set [27] and offer a caution against the oversimplified notion that maximizing (or minimizing) a measure value inherently leads to improved generalization [5].

Figure 1: (Left) Scatter plot of train robustness and test robustness. Bright color corresponds to high robust generalization gap, i.e., poor generalization. (Right) Boxplot of robust generalization gap for some training setups. All adversarial examples during training and testing are generated by PGD10 on CIFAR-10.

### Evaluation Metrics

To uncover the relationship between measures and robust generalization performance, we adopt the Kendall rank correlation coefficient following prior works [27; 36]. We begin by defining a search space \(\bm{\Theta}=\Theta_{1}\times\Theta_{2}\times\cdots\times\Theta_{n}\). Each \(\Theta_{i}\) corresponds to a search space for each training parameter defined in Section 3.1. Given the search space \(\bm{\Theta}\), we obtain the trained models \(f_{\bm{\theta}}(\bm{w})\) for \(\bm{\theta}\in\bm{\Theta}\). For each model \(f_{\bm{\theta}}(\bm{w})\), we measure the robust generalization gap \(g(f_{\bm{\theta}}(\bm{w}))\) and the corresponding measure value \(\mu(f_{\bm{\theta}}(\bm{w}))\). For simplicity, we denote \(g(\bm{\theta}):=g(f_{\bm{\theta}}(\bm{w}))\) and \(\mu(\bm{\theta}):=\mu(f_{\bm{\theta}}(\bm{w}))\). We then calculate Kendall's rank coefficient [29] as follows:

\[\tau(\bm{\Theta})=\frac{1}{|\bm{\Theta}|(|\bm{\Theta}|-1)}\sum_{\bm{\theta}\in \bm{\Theta}}\sum_{\bm{\theta}^{\prime}\in\bm{\Theta},\bm{\theta}\neq\bm{ \theta}^{\prime}}\text{sgn}(g(\bm{\theta})-g(\bm{\theta}^{\prime}))\cdot\text {sgn}(\mu(\bm{\theta})-\mu(\bm{\theta}^{\prime})),\] (4)

where \(|\bm{\Theta}|\) is the number of elements in \(\bm{\Theta}\), and sgn(\(\cdot\)) is a sign function. The value of \(\tau\) becomes \(1\) when the pairs have the same rankings and \(-1\) when the pairs have reversed order rankings. Therefore, a higher value of \(\tau\) implies that as the value of a measure \(\mu\) increases, the robust generalization gap \(g\) also increases.

As noted by [27], the measure may strongly correlate with the robust generalization gap only when a specific training setting is varied. Therefore, Jiang et al. [27] introduced the following metric:

\[\psi_{k}(\bm{\Theta})=\mathbf{E}_{\theta_{1},\cdots,\theta_{k-1},\theta_{k+1}, \cdots,\theta_{n}}\left[\tau(\{\bm{\theta}=(\theta_{1},\cdots,\theta_{n}), \theta_{k}\in\Theta_{k}\})\right],\] (5)

which captures the robust generalization gap when only the hyper-parameter \(\Theta_{k}\) changes. However, we demonstrate that \(\psi_{k}\) may not perform well in cases where Simpson's paradox exists. Simpson's paradox refers to a situation where each of the individual groups exhibits a specific trend, but it disappears (or reverses) when the groups are combined. Thus, when a parameter \(\Theta_{i\neq k}\) heavily affects the robust generalization gap, \(\psi_{k}\) becomes not effective as it captures the overall trends by taking expectation across all parameters including \(\Theta_{i\neq k}\). In fact, within the adversarial training framework, the inner \(\tau\) in (5) shows extremely high variance due to the high sensitivity of the robust generalization gap with respect to training setups, which will be discussed in Table 1.

Therefore, we propose a metric for capturing the robust generalization performance by fixing the hyper-parameter \(\Theta_{k}\) as follows:

\[\pi_{k}(\bm{\Theta})=\mathbf{E}_{\theta_{k}}\left[\tau(\{\bm{\theta}=(\theta_ {1},\cdots,\theta_{n}),\;\theta_{i}\in\Theta_{i}\;\forall i\neq k\})\right].\] (6)

Here, \(\pi_{k}(\bm{\Theta})\) represents the effectiveness of a measure \(\mu\) within a specific fixed training setup. This enables us to discover that some measures only work for specific training settings, e.g., uncovering the strong correlation between boundary_thickness and the robust generalization gap when AT is used as the training method. Furthermore, for measures exhibiting meaningful value of \(\pi_{k}\), we additionally provide their scatter plots to mitigate the limitations of a correlation analysis.

## 4 Experimental Results

Based on the measures and trained models defined in Section 3, we calculate the measures for each model and their robust generalization gap. Notably, in the realm of adversarial training, models encounter two types of examples: benign examples and adversarial examples. Therefore, to gain a deeper understanding of the relationship between robust generalization and measures, we also calculate the values of example-dependent measures for both benign examples and PGD10 examples.

Table 1 summarizes the results of \(\psi_{k}\) for each measure. Further, in order to consider the distributional correlation [17] and quantify the precision of \(\psi_{k}\), we also report the corresponding standard deviation of the inner \(\tau\) in Eq. (5). A higher value of \(\psi_{k}\) indicates a stronger positive rank correlation, implying that as the measure value increases, the robust generalization gap increases. First of all, it is important to note that **none of the measures are perfect**. While certain measures show high \(\psi_{k}\), all measures show high standard deviations. This observation indicates that no measure can provide a perfect estimation of the model's robust generalization gap. Moreover, with such high variances, it becomes challenging to clearly identify the underlying correlation of the measures.

To address this limitation, we conduct further analyses with our proposed metric, \(\pi_{k}\) in Eq. (6). The benefit of \(\pi_{k}\) is that it reveals the potential hidden relationship between the measures and the robust generalization gap by fixing training settings, which \(\psi_{k}\) cannot captures. Thus, from now on, we will report \(\pi_{k}\) of each measure and provide correlation analyses with the robust generalization gap, extending the connections beyond those presented in Table 1.

Norm-based measures requires fixed model architecture.In many prior works, researchers have demonstrated the effectiveness of norm-based measures in estimating the generalization gap [27; 17]. Among them, the weight norm-based measures, e.g., the product of Frobenius norm (log_prod_of_fro) [46], the product of spectral norm (log_prod_of_spec) [7], and the distance to the initial weight (euclid_init_norm) [27; 40], are built on theoretical frameworks such as PAC-Bayes [43; 47; 38]. path_norm is also often used to estimate the complexity of a neural network, which calculates the sum of outputs for all-ones input after squaring all parameters [27].

In Table 1, the most of norm-based measures exhibit a low correlation with the robust generalization gap for the metric \(\psi_{k}\). However, by using the proposed metric \(\pi_{k}\) in Table 2, we discover that log_prod_of_fro and euclid_init_norm exhibit strong correlations with low standard deviation when the model architecture is fixed. Intuitively, when the model architecture varies, the number of parameters and their corresponding values exhibit different ranges. Indeed, the range of log_prod_of_fro roughly shows \([50,100]\) for ResNet18, but shows \([140,200]\) for WRN28-10. Thus, comparing models with different architectures degrades the precision of weight norm-based measures. Under the fixation of the model architecture, log_prod_of_fro is **positively correlated with the robust generalization gap**, which consistents to the prior theoretical observations in PAC-Bayesian framework [50] or Lipschitz analysis [58]. For log_prod_of_spec, we do not observe a strong correlation under any conditions.

Furthermore, we observe that path_norm shows some extent of correlation for all \(\psi_{k}\) and \(\pi_{k}\). Upon conducting a more in-depth analysis, we find that **the log of path_norm yields an almost linear relationship with the robust generalization gap** when conditioned with average_ce(PGD), resulting the total \(\tau=0.68\) (detailed in Appendix A.2). This finding suggests that path_norm can be effectively utilized for estimating the robust generalization gap, with consistent to the prior works under the standard training framework [27; 17].

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline  & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & 
\begin{tabular}{c} Early \\ Stopping \\ \end{tabular} & \(\tau\) \\ \hline num\_params (7) & - & -0.02\(\pm\)0.04 & -0.01\(\pm\)0.08 & -0.03\(\pm\)0.01 & -0.02\(\pm\)0.03 & -0.02\(\pm\)0.02 & -0.03\(\pm\)0.00 & -0.02\(\pm\)0.01 \\ path\_norm (8) & 0.35\(\pm\)0.05 & **0.27\(\pm\)**0.13 & **0.24\(\pm\)0.26** & **0.34\(\pm\)0.03** & **0.37\(\pm\)0.03** & **0.35\(\pm\)0.05** & **0.35\(\pm\)0.03** & **0.46\(\pm\)0.14** \\ log\_prod\_of\_spec (9) & 0.03\(\pm\)0.13 & -0.07\(\pm\)0.07 & -0.00\(\pm\)0.01 & -0.12\(\pm\)0.03 & -0.11\(\pm\)0.05 & -0.13\(\pm\)0.01 & -0.13\(\pm\)0.01 & -0.13\(\pm\)0.06 \\ log\_prod\_of\_fro (10) & **0.37\(\pm\)0.03** & 0.15\(\pm\)0.11 & 0.09\(\pm\)0.10 & 0.18\(\pm\)0.01 & 0.18\(\pm\)0.04 & 0.19\(\pm\)0.00 & 0.19\(\pm\)0.02 & 0.19\(\pm\)0.04 \\ euclid\_init\_norm (11) & 0.32\(\pm\)0.03 & 0.06\(\pm\)0.03 & 0.09\(\pm\)0.10 & 0.17\(\pm\)0.00 & 0.20\(\pm\)0.04 & 0.17\(\pm\)0.01 & 0.17\(\pm\)0.03 & 0.20\(\pm\)0.11 \\ \hline \end{tabular}
\end{table}
Table 2: (Norm-based measures) Numerical results of \(\pi_{k}\) and its corresponding standard deviation.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline  & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & 
\begin{tabular}{c} Early \\ Stopping \\ \end{tabular} & \(\tau\) \\ \hline num\_params (7) & - & -0.02\(\pm\)0.04 & -0.01\(\pm\)0.08 & -0.03\(\pm\)0.01 & -0.02\(\pm\)0.03 & -0.02\(\pm\)0.02 & -0.03\(\pm\)0.00 & -0.02\(\pm\)0.01 \\ path\_norm (8) & 0.35\(\pm\)0.05 & **0.27\(\pm\)**0.13 & **0.24\(\pm\)0.26** & **0.34\(\pm\)0.03** & **0.37\(\pm\)0.03** & **0.35\(\pm\)0.05** & **0.35\(\pm\)0.03** & **0.46\(\pm\)0.14** \\ log\_prod\_of\_spec (9) & 0.03\(\pm\)0.13 & -0.07\(\pm\)0.07 & -0.00\(\pm\)0.01 & -0.12\(\pm\)0.03 & -0.11\(\pm\)0.05 & -0.13\(\pm\)0.01 & -0.13\(\pm\)0.01 & -0.13\(\pm\)0.06 \\ log\_prod\_of\_fro (10) & **0.37\(\pm\)0.03** & 0.15\(\pm\)0.11 & 0.09\(\pm\)0.10 & 0.18\(\pm\)0.01 & 0.18\(\pm\)0.04 & 0.19\(\pm\)0.00 & 0.19\(\pm\)0.02 & 0.19\(\pm\)0.04 \\ euclid\_init\_norm (11) & 0.32\(\pm\)0.03 & 0.06\(\pm\)0.03 & 0.09\(\pm\)0.10 & 0.17\(\pm\)0.00 & 0.20\(\pm\)0.04 & 0.17\(\pm\)0.01 & 0.17\(\pm\)0.03 & 0.20\(\pm\)0.11 \\ \hline \end{tabular}
\end{table}
Table 1: Numerical results of \(\psi_{k}\) for each measure, along with its corresponding standard deviation. The total \(\tau\) indicates the Kendallâ€™s rank coefficient for the entire pairs \((g,\mu)\). * (PGD) indicates the same measure calculated on PGD10 examples for example-based measures.

Maximizing margin beyond a certain point harms robust generalization.Traditionally, the maximizing margin is considered as an ultimate goal in the adversarial training framework [42, 61]. Indeed, average cross-entropy loss [42] and margin-related losses [61] are frequently used in adversarial training methods. However, in both Table 1 and Table 3, average_ce(PGD) exhibit a high negative correlation with the robust generalization gap across all variations of training parameters. Similarly, \(\pi_{k}\) and the total \(\tau\) of prob_margin(PGD) are extremely high (0.79). This suggests that **lower cross-entropy on PGD examples (and higher margin in the probability space) leads to worse robust generalization.** Indeed, Fig. 2 shows a clear negative correlation between average_ce(PGD) and the robust generalization gap. Notably, high test robustness is observed within the range of average_ce(PGD) \(\in[0.5,1.0]\), indicating that minimizing average_ce(PGD) beyond a certain point may harm the generalization performance as Ishida et al. [25] observed in the standard training frameworks.

Given this observation, we argue that the margin maximization in adversarial training methods should be carefully revisited. Recent studies have highlighted that maximizing the margin might not necessarily be the optimal objective in adversarial training due to intricate gradient flow dynamics [59] and the non-cognitive concept of using predicted probabilities [1]. Additionally, in recent work [32], despite the similar robustness of TRADES and AT, their margin distributions on benign and adversarial examples are extremely different. This implies that the margin cannot be the sole determinant of adversarial robustness. Considering these findings and other recent studies [41, 60], the margin maximization should be accompanied by a consideration of other factors such as weight regularization or gradient information.

The cross entropy and margin on benign examples, denoted as average_ce and prob_margin, also show some degree of correlation with the generalization gap. This correlation becomes particularly significant when using early stopping, where their correlations reach up to 0.65. Note that inverse_margin behaves differently because it uses the 10th-percentile of margins over the training dataset rather than the expectation.

Boundary thickness works well for models trained by AT.Yang et al. [67] introduced the concept of boundary thickness, which is an extended version of margin based on adversarial examples. They argue that a thin decision boundary leads to both poor adversarial robustness and the gap. Therefore, boundary_thickness should be negatively correlated with the robust generalization gap. However, as shown in Table 3, it does not correlate well with the robust generalization gap. The main difference between our experiments and those in [67] is that we also considered TRADES and MART, whereas Yang et al. [67] sorely compared models trained with AT. Thus, in Fig. 3, we plot the inner \(\tau\) in \(\pi_{k}\) for each training method. It is evident that the boundary thickness demonstrates a strong correlation with the robust generalization gap when the training method is fixed to AT. This suggests that boundary_thickness is more effective for comparing models trained with AT. Furthermore, in Appendix A.2, we also discover that boundary_thickness becomes more highly correlated with the robust generalization gap when the models are conditioned on average_ce(PGD). Thus, when using boundary_thickness as the sole determinant of robust generalization, we should carefully revisit the choice of training methods and the robust accuracy of models on train datasets.

Figure 2: Scatter plot for average_ce(PGD) and the gap. Bright color indicates a higher test robustness. For better visualization, we cutoff average_ce(PGD) \(>2\).

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline  & Model & Training & & & & & & \\  & Architecture & Methods & Steps & Optimizer & Batch-size & Aug & Extra-data & 
\begin{tabular}{} \end{tabular} \\ \hline average\_ce(12) & -0.24\(\pm\)0.03 & -0.28\(\pm\)0.17 & -0.30\(\pm\)0.27 & -0.23\(\pm\)0.00 & -0.23\(\pm\)0.04 & -0.24\(\pm\)0.06 & -0.23\(\pm\)0.06 & -0.34\(\pm\)0.31 \\ inverse\_margin(13) & 0.07\(\pm\)0.05 & -0.05\(\pm\)0.23 & -0.18\(\pm\)0.23 & 0.09\(\pm\)0.06 & 0.08\(\pm\)0.09 & 0.08\(\pm\)0.13 & 0.09\(\pm\)0.12 & 0.09\(\pm\)0.12 \\ prob\_margin(14) & 0.24\(\pm\)0.02 & 0.28\(\pm\)0.16 & 0.31\(\pm\)0.27 & 0.23\(\pm\)0.01 & 0.23\(\pm\)0.04 & 0.24\(\pm\)0.06 & 0.23\(\pm\)0.06 & 0.34\(\pm\)0.31 \\ boundary\_thickness (15) & -0.02\(\pm\)0.01 & -0.17\(\pm\)0.15 & -0.19\(\pm\)0.23 & -0.02\(\pm\)0.01 & -0.02\(\pm\)0.02 & -0.04\(\pm\)0.06 & -0.02\(\pm\)0.05 & -0.04\(\pm\)0.27 \\ \hline average\_ce(PGD) (12) & -0.78\(\pm\)0.01 & -0.58\(\pm\)0.39 & -0.47\(\pm\)0.40 & **-0.79\(\pm\)0.04** & **-0.79\(\pm\)0.01** & **-0.80\(\pm\)0.03** & **-0.79\(\pm\)0.00** & **-0.78\(\pm\)0.01** \\ inverse\_margin(PGD) (13) & 0.34\(\pm\)0.02 & 0.26\(\pm\)0.22 & 0.05\(\pm\)0.19 & 0.37\(\pm\)0.03 & 0.35\(\pm\)0.03 & 0.34\(\pm\)0.05 & 0.36\(\pm\)0.13 & 0.38\(\pm\)0.08 \\ prob\_margin(PGD) (14) & **0.79\(\pm\)0.01** & **0.59\(\pm\)0.38** & **0.48\(\pm\)0.39** & **0.79\(\pm\)0.04** & **0.79\(\pm\)0.01** & **0.80\(\pm\)0.03** & **0.79\(\pm\)0.00** & **0.78\(\pm\)0.02** \\ \hline \end{tabular}
\end{table}
Table 3: (Margin-based measures) Numerical results of \(\pi_{k}\) and its corresponding standard deviation.

**Smoothness does not guarantee low robust generalization gap.** In the pursuit of achieving adversarial robustness, the smoothness between benign and adversarial examples is often considered as an indicative measure. For instance, TRADES [71] minimizes the kl_divergence between benign and adversarial logits. However, kl_divergence shows a negative correlation for both Table 1 and Table 4. This implies that, similar to average_ce(PGD), kl_divergence cannot serve as an indicator for robust generalization.

While Xu et al. [65] demonstrated that imposing local Lipschitzness (local_lip) leads to better generalization in linear classification, recent research [66] argued an opposing perspective, suggesting that within neural networks, local Lipschitzness might hurt robust generalization. However, this conclusion was built on fewer than 20 models and evaluated solely on test examples. In our large experiment, we cannot observe that local Lipschitzness itself negatively affects robust generalization. Rather, it is more efficient in predicting robust accuracy (detailed in Appendix A.1). These findings are consistent with [45, 39], which highlighted the importance of model architecture or weight norms when evaluating models with local Lipshiftness.

**Flatness-based measures are not correlated well with the robust generalization gap.** Flatness-based measures have recently regarded as powerful indicators of generalization performance in both standard and adversarial training frameworks [18, 64]. This includes the maximum perturbation size in the weight space that do not dramatically changes the accuracy (pacbayes_flat) [43, 27], the loss increment by adversarial weight perturbation (estimated_sharpness) [18], and its scale-invariant version (estimated_inv_sharpness) [36]. However, our analysis reveals that flatness-based measures tend to exhibit poor correlations with the robust generalization gap. In both Table 1 and Table 4, the majority of flatness-based measures exhibit near-zero correlations or even negative values. Only pacbayes_flat(PGD) demonstrates a strong correlation with robust generalization because it effectively distinguishes between robust and non-robust models (refer to Appendix A.2).

Recently, Stutz et al. [56] demonstrated the importance of early stopping in the analysis of flatness. Similarly, we observe that, when early stopping is employed, the correlation of estimated_sharpness approaches zero. However, without early-stopping, we discover

\begin{table}
\begin{tabular}{c|c c c c c c} \hline  & \multicolumn{2}{c}{Model} & \multicolumn{1}{c}{Training} & \multicolumn{1}{c}{Steps} & \multicolumn{1}{c}{Optimizer} & \multicolumn{1}{c}{Batch-size} & \multicolumn{1}{c}{Aug} & \multicolumn{1}{c}{Extra-data} & \multicolumn{1}{c}{
\begin{tabular}{c} Early \\ Stopping \\ \end{tabular} } \\ \hline kl\_divergence (16) & -0.45\(\pm\)0.01 & -0.35\(\pm\)0.29 & -0.15\(\pm\)0.19 & -0.46\(\pm\)0.10 & -0.46\(\pm\)0.08 & -0.43\(\pm\)0.12 & -0.45\(\pm\)0.02 & -0.37\(\pm\)0.29 \\ local\_lip (17) & -0.24\(\pm\)0.06 & -0.15\(\pm\)0.19 & -0.01\(\pm\)0.17 & -0.24\(\pm\)0.10 & -0.25\(\pm\)0.07 & -0.20\(\pm\)0.05 & -0.23\(\pm\)0.02 & -0.21\(\pm\)0.30 \\ pacbayes\_flat (18) & 0.04\(\pm\)0.09 & 0.04\(\pm\)0.14 & -0.06\(\pm\)0.14 & 0.06\(\pm\)0.07 & 0.04\(\pm\)0.08 & 0.10\(\pm\)0.12 & 0.08\(\pm\)0.10 & 0.07\(\pm\)0.16 \\ estimated\_sharpness (19) & -0.07\(\pm\)0.02 & -0.11\(\pm\)0.24 & -0.22\(\pm\)0.18 & -0.06\(\pm\)0.17 & -0.08\(\pm\)0.10 & -0.02\(\pm\)0.13 & -0.04\(\pm\)0.07 & -0.04\(\pm\)0.16 \\ estimated\_inv\_sharpness (20) & 0.04\(\pm\)0.02 & -0.04\(\pm\)0.32 & -0.19\(\pm\)0.14 & 0.05\(\pm\)0.12 & 0.04\(\pm\)0.06 & 0.10\(\pm\)0.13 & 0.07\(\pm\)0.10 & 0.03\(\pm\)0.12 \\ average\_flat (21) & -0.36\(\pm\)0.01 & -0.24\(\pm\)0.21 & -0.10\(\pm\)0.15 & -0.38\(\pm\)0.07 & -0.37\(\pm\)0.06 & -0.32\(\pm\)0.13 & -0.35\(\pm\)0.00 & -0.33\(\pm\)0.25 \\ \hline pacbayes\_flat (PGD) (18) & **0.95\(\pm\)0.04** & **0.45\(\pm\)0.16** & **0.39\(\pm\)0.21** & **0.60\(\pm\)0.08** & **0.59\(\pm\)0.22** & **0.62\(\pm\)0.22** & **0.63\(\pm\)0.03** & **0.50\(\pm\)0.07** \\ estimated\_sharpness (PGD) (19) & -0.22\(\pm\)0.01 & -0.07\(\pm\)0.12 & -0.06\(\pm\)0.14 & -0.22\(\pm\)0.10 & -0.23\(\pm\)0.07 & -0.17\(\pm\)0.07 & -0.19\(\pm\)0.06 & -0.21\(\pm\)0.32 \\ estimated\_inv\_sharpness (PGD) (20) & -0.24\(\pm\)0.02 & -0.12\(\pm\)0.14 & -0.10\(\pm\)0.17 & -0.26\(\pm\)0.10 & -0.26\(\pm\)0.06 & -0.20\(\pm\)0.08 & -0.23\(\pm\)0.06 & -0.23\(\pm\)0.35 \\ \hline \end{tabular}
\end{table}
Table 4: (Smoothness-based and Flatness-based measures) Numerical results of \(\pi_{k}\) and its corresponding standard deviation.

Figure 3: Box plot of the inner \(\tau\) in \(\pi_{k}\) Eq. (6), where \(\Theta_{k}\) is training method. The text corresponds to the method for each outlier, e.g., boundary_thickness performs well when AT is used.

that estimated_sharpness exhibits a significant negative correlation for models have low average_ce(PGD) \(\leq 1.5\). As shown in Fig. 4, models with low estimated_sharpness show high robust generalization gaps. This finding aligns with the concurrent work of [5], which demonstrates that flatter solutions generalize worse on out-of-distribution data. The additional results can be found in Appendix A.6.

In the case of average_flat[56], which is calculated with random weight perturbations and their worst-case losses, it demonstrates some degree of correlation. However, it is more efficient in predicting robust accuracy rather than the gap (refer to Appendix A.1). This result suggests that, as the concurrent work [5] observed in the standard training framework, flatness measures may not serve as reliable indicators of correlation in the adversarial training framework even they can be effectively used to achieving better performance.

The norm of gradient of inputs robustly captures the gap even for models with similar cross-entropy losses.Although some prior works [53, 24] have argued that regularizing the input gradient norm might improve adversarial robustness, we observe that this cannot be argued as lower input gradient norm is better. Table 5 summarizes \(\pi_{k}\) of the gradient norm of input (x_grad_norm) and the gradient norm of weight (w_grad_norm). Among these, x_grad_norm show a strong correlation with the robust generalization gap. The negative correlation of x_grad_norm indicates that models with a larger input gradient norm are more likely to show lower robust generalization gap.

Furthermore, even when comparing models having similar average_ce(PGD), x_grad_norm is the most robust indicator of the robust generalization gap. Previous works in the standard training framework [27, 17] have argued that the cross-entropy loss is strongly correlated with the robust generalization gap, and thus, they used early stopping based on certain cross-entropy thresholds during training to remove the influence of varying cross-entropy loss. However, within the adversarial training framework, employing the same early stopping based on loss becomes challenging as TRADES and MART minimize different loss functions from AT. Therefore, we categorize the trained models into groups based on average_ce(PGD) values using a bin size of 0.1. This grouping reduces \(\pi_{k}\) of average_ce(PGD) to \(-0.12\). The results are summarized in Table 6. Compared to all other measures, x_grad_norm**exhibits the highest rank correlation with the robust generalization gap even when conditioned on average_ce(PGD).** We believe this finding highlights the importance of the norm of input gradients as a valuable regularizer for improving model robustness and its generalization in practical settings.

\begin{table}
\begin{tabular}{c|c|c} Measures & \(\pi_{k}\) & Measures & \(\pi_{k}\) \\ num\_params (7) & -0.23\(\pm\)0.17 & estimated\_sharpness (19) & -0.12\(\pm\)0.19 \\ path\_norm (8) & 0.25\(\pm\)0.14 & estimated\_inv\_sharpness (20) & -0.13\(\pm\)0.17 \\ log\_prod\_of\_spec (9) & 0.10\(\pm\)0.13 & average\_flat (21) & -0.12\(\pm\)0.17 \\ log\_prod\_of\_tr\_(10) & 0.11\(\pm\)0.10 & x\_grad\_norm (22) & **-0.35\(\pm\)0.16** \\ eucl\_init\_norm (11) & -0.11\(\pm\)0.14 & w\_grad\_norm (23) & -0.06\(\pm\)0.17 \\ average\_ce (12) & 0.03\(\pm\)0.19 & inverse\_margin(PGD) (13) & 0.05\(\pm\)0.19 \\ inverse\_margin (13) & -0.09\(\pm\)0.19 & prob\_margin(PGD) (14) & -0.13\(\pm\)0.16 \\ prob\_margin(14) & -0.02\(\pm\)0.16 & peak\_base\_flat (PGD) (18) & -0.19\(\pm\)0.20 \\ boundary\_thickness (15) & 0.06\(\pm\)0.19 & estimated\_sharpness(PGD) (19) & 0.05\(\pm\)0.19 \\ kl\_divergence (16) & -0.09\(\pm\)0.14 & estimated\_inv\_sharpness(PGD) (20) & 0.01\(\pm\)0.19 \\ local\_lap (17) & -0.11\(\pm\)0.16 & x\_grad\_norm(PGD) (22) & -0.12\(\pm\)0.14 \\ pacbayes\_flat (18) & -0.25\(\pm\)0.21 & w\_grad\_norm(PGD) (23) & 0.13\(\pm\)0.16 \\ \end{tabular}
\end{table}
Table 6: Numerical results of \(\pi_{k}\) for each measure when \(\Theta_{k}\) is given by average_ce(PGD) values with a bin size of 0.1, along with its corresponding standard deviation.

Figure 4: Scatter plot for the robust generalization gap and estimated_sharpness. Conditioned on average_ce(PGD)\(\leq 1.5\) without early-stopping.

## 5 Broader Impact with Benchmarks

RobustBench [12] provides a set of pre-trained models that achieve high robust accuracy across various datasets, including CIFAR-10, CIFAR-100, and ImageNet. Leveraging this benchmark, we extend our observations to diverse models including transformer-based architectures [3; 13] or trained on diffusion-generated datasets [51; 62]. As shown in Figure 5, we identify that some of our findings in Section 4 also can be applied to these models. Margin-based measures such as average_ce, prob_margin, average_ce(PGD), and prob_margin(PGD) consistently exhibit strong correlations with the robust generalization gap. Additionally, we observe that x_grad_norm consistently shows reliable performance in predicting the robust generalization gap, even when applied to models in the RobustBench across various datasets.

Though a deeper analysis is limited by the absence of training setting details, such as the use of early stopping, we additionally conduct an analysis with the model architecture by analyzing the pre-trained models. Table 7 summarizes \(\pi_{k}\) for models with the same architecture. As we demonstrated in Section 4, norm-based measures exhibit a higher correlation when comparing models with identical architectures. Notably, for CIFAR-100, we find that path_norm shows a strong correlation with the robust generalization gap. Regarding the low correlation and high standard deviation of norm-based measures, we hypothesize that other training settings, such as the choice of activation functions (e.g., Swish and SiLU instead of ReLU) and training datasets, may affect the values of norm-based measures. Further exploration of these aspects is left to future work, as additional research and experiments can provide a more comprehensive understanding of these relationships.

## 6 Limitations and Future Work

While our study unveils the correlation between various measures and the robust generalization gap over 1,300 models, due to our computational constraints, we focused on ResNet models, CIFAR-10, and PGD with the \(L_{\infty}\) norm. Thus, further investigations on a broader range of hyper-parameters and the use of stronger attacks may uncover new relationships beyond our analysis. We hope that future work would address these limitations.

## 7 Conclusion

Through large-scale experiments, we verified the underlying relationships between various measures and the robust generalization gap on CIFAR-10 under the \(L_{\infty}\) norm. Our findings offer valuable insights into robust generalization and emphasize the need for caution when making statements such as,'model A is superior to model B because model A exhibits a better measure value than model B,' a frequently employed phrase in recent literature. We hope that our discoveries regarding diverse measures can contribute to further advancement in the field of adversarial robustness.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline CIFAR-10 & \(\pi_{k}\) & Total \(\tau\) & CIFAR-100 & \(\pi_{k}\) & Total \(\tau\) \\ \hline path\_norm (8) & 0.24\(\pm\)0.26 & 0.02 & path\_norm (8) & 0.55\(\pm\)0.05 & 0.09 \\ log\_prod\_of\_fro (10) & 0.12\(\pm\)0.40 & 0.10 & log\_prod\_of\_fro (10) & 0.30\(\pm\)0.30 & 0.17 \\ \hline \end{tabular}
\end{table}
Table 7: Numerical results of \(\pi_{k}\) for norm-based measures when \(\Theta_{k}\) is the model structure along with its corresponding standard deviation. The total \(\tau\) is the same as in Fig. 5. For ImageNet, \(\pi_{k}\) is not applicable due to the limited number of pre-trained models in RobustBench [12].

Figure 5: Experiment on RobustBench [12]. For each dataset, we plot the total \(\tau\) of each measure and highlight the robust measures with \(|\tau|\geq 0.25\) for all datasets with yellow background.

## Acknowledgements

This work was supported by the National Research Founda- tion of Korea (NRF) grant funded by the Korean government (MSIT) (No. 2019R1A2C2002358) and the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00984).

## References

* [1] Hiroki Adachi, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. Revisiting instance-reweighted adversarial training. 2022.
* [2] Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving adversarial robustness? _Advances in Neural Information Processing Systems_, 32:12214-12223, 2019.
* [3] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. _Advances in neural information processing systems_, 34:20014-20027, 2021.
* [4] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training. _Advances in Neural Information Processing Systems_, 33, 2020.
* [5] Maksym Andriushchenko, Francesco Croce, Maximilian Muller, Matthias Hein, and Nicolas Flammarion. A modern look at the relationship between sharpness and generalization. _arXiv preprint arXiv:2302.07011_, 2023.
* [6] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156, 2006.
* [7] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* [8] Samuel Rota Bulo, Lorenzo Porzi, and Peter Kontschieder. In-place activated batchnorm for memory-optimized training of dnns. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5639-5647, 2018.
* [9] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. In _Advances in Neural Information Processing Systems_, pages 11192-11203, 2019.
* [10] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In _International Conference on Machine Learning_, pages 854-863. PMLR, 2017.
* [11] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _International conference on machine learning_, pages 2206-2216. PMLR, 2020.
* [12] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_, 2020.
* [13] Edoardo Debenedetti, Vikash Sehwag, and Prateek Mittal. A light recipe to train robust vision transformers. In _First IEEE Conference on Secure and Trustworthy Machine Learning_, 2023. URL https://openreview.net/forum?id=IztT98ky0cKs.
* [14] Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct input space margin maximization through adversarial training. In _International Conference on Learning Representations_, 2019.

* Dinh et al. [2017] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* Dong et al. [2019] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4312-4321, 2019.
* Dziugaite et al. [2020] Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, and Daniel M Roy. In search of robust measures of generalization. _Advances in Neural Information Processing Systems_, 33:11723-11733, 2020.
* Foret et al. [2020] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2020.
* Ge et al. [2019] Jiexian Ge, Xiaoxin Cui, Kanglin Xiao, Chenglong Zou, YiHsiang Chen, and Rongshan Wei. Bnrelu: combine batch normalization and rectified linear unit to reduce hardware overhead. In _2019 IEEE 13th International Conference on ASIC (ASICON)_, pages 1-4. IEEE, 2019.
* Goodfellow et al. [2014] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* Gowal et al. [2020] Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. _arXiv preprint arXiv:2010.03593_, 2020.
* Gowal et al. [2021] Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A Mann. Improving robustness using generated data. _Advances in Neural Information Processing Systems_, 34:4218-4233, 2021.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Huang et al. [2022] Zhichao Huang, Yanbo Fan, Chen Liu, Weizhong Zhang, Yong Zhang, Mathieu Salzmann, Sabine Susstrunk, and Jue Wang. Fast adversarial training with adaptive step size. _arXiv preprint arXiv:2206.02417_, 2022.
* Ishida et al. [2020] Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero training loss after achieving zero training error? _arXiv preprint arXiv:2002.08709_, 2020.
* Jiang et al. [2019] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep networks with margin distributions. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJ1QfnCqKX.
* Jiang et al. [2020] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.
* Kannan et al. [2018] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. _arXiv preprint arXiv:1803.06373_, 2018.
* Kendall [1938] Maurice G Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938.
* Keskar et al. [2017] Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima. In _5th International Conference on Learning Representations, ICLR 2017_, 2017.
* Kim et al. [2021] Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-step adversarial training. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8119-8127, 2021.
* Kim et al. [2023] Hoki Kim, Woojin Lee, Sungyoon Lee, and Jaewook Lee. Bridged adversarial training. _Neural Networks_, 167:266-282, 2023.

* Kim et al. [2023] Hoki Kim, Jinseong Park, Yujin Choi, and Jaewook Lee. Stability analysis of sharpness-aware minimization. _arXiv preprint arXiv:2301.06308_, 2023.
* Kim et al. [2023] Hoki Kim, Jinseong Park, Yujin Choi, Woojin Lee, and Jaewook Lee. Exploring the effect of multi-step ascent in sharpness-aware minimization. _arXiv preprint arXiv:2302.10181_, 2023.
* Kim et al. [2023] Hoki Kim, Jinseong Park, and Jaewook Lee. Generating transferable adversarial examples for speech classification. _Pattern Recognition_, 137:109286, 2023.
* Kwon et al. [2021] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* Li et al. [2021] Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao Gong, Fengwei Yu, and Junjie Yan. MQBench: Towards reproducible and deployable model quantization benchmark. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021. URL https://openreview.net/forum?id=TUpl0mF8DsM.
* Liang et al. [2019] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. In _The 22nd international conference on artificial intelligence and statistics_, pages 888-896. PMLR, 2019.
* Liang and Huang [2021] Youwei Liang and Dong Huang. Large norms of cnn layers do not hurt adversarial robustness. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8565-8573, 2021.
* Liu et al. [2020] Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and Sabine Susstrunk. On the loss landscape of adversarial training: Identifying challenges and how to overcome them. _Advances in Neural Information Processing Systems_, 33, 2020.
* Liu and Chan [2022] Ziquan Liu and Antoni B Chan. Boosting adversarial robustness from the perspective of effective margin regularization. _arXiv preprint arXiv:2210.05118_, 2022.
* Madry et al. [2017] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* McAllester [1999] David A McAllester. Pac-bayesian model averaging. In _Proceedings of the twelfth annual conference on Computational learning theory_, pages 164-170, 1999.
* Moosavi-Dezfooli et al. [2019] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9078-9086, 2019.
* Nern and Sharma [2022] Laura Fee Nern and Yash Sharma. How adversarial robustness transfers from pre-training to downstream tasks. _arXiv preprint arXiv:2208.03835_, 2022.
* Neyshabur et al. [2015] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In _Conference on learning theory_, pages 1376-1401. PMLR, 2015.
* Neyshabur et al. [2018] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=Skx_WfbCZ.
* Pang et al. [2020] Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial training. _arXiv preprint arXiv:2010.00467_, 2020.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In _Advances in neural information processing systems_, pages 8026-8037, 2019.
* Pitas et al. [2017] Konstantinos Pitas, Mike Davies, and Pierre Vandergheynst. Pac-bayesian margin bounds for convolutional neural networks. _arXiv preprint arXiv:1801.00171_, 2017.

* [51] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Data augmentation can improve robustness. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=kgVJBBThdSZ.
* [52] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In _International Conference on Machine Learning_, pages 8093-8104. PMLR, 2020.
* [53] Andrew Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [54] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. _Advances in neural information processing systems_, 31:5014-5026, 2018.
* [55] Chawin Sitawarin, Supriyo Chakraborty, and David Wagner. Improving adversarial robustness through progressive hardening. _arXiv preprint arXiv:2003.09347_, 2020.
* [56] David Stutz, Matthias Hein, and Bernt Schiele. Relating adversarially robust generalization to flat minima. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7807-7817, 2021.
* [57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* [58] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks. In _Advances in neural information processing systems_, pages 6541-6550, 2018.
* [59] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Gradient methods provably converge to non-robust networks. _Advances in Neural Information Processing Systems_, 35:20921-20932, 2022.
* [60] Haotao Wang, Aston Zhang, Shuai Zheng, Xingjian Shi, Mu Li, and Zhangyang Wang. Removing batch normalization boosts adversarial training. In _International Conference on Machine Learning_, pages 23433-23445. PMLR, 2022.
* [61] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In _International Conference on Learning Representations_, 2019.
* [62] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. _arXiv preprint arXiv:2302.04638_, 2023.
* [63] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. _arXiv preprint arXiv:2001.03994_, 2020.
* [64] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. _Advances in Neural Information Processing Systems_, 33, 2020.
* [65] Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. _Journal of machine learning research_, 10(7), 2009.
* [66] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. _Advances in Neural Information Processing Systems_, 2020.
* [67] Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models. _arXiv preprint arXiv:2007.05086_, 2020.
* [68] Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially robust generalization. In _International conference on machine learning_, pages 7085-7094. PMLR, 2019.

* Yun and Wong [2021] Stone Yun and Alexander Wong. Do all mobilenets quantize poorly? gaining insights into the effect of quantization on depthwise separable convolutional networks through the eyes of multi-scale distributional dynamics. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2447-2456, 2021.
* Zagoruyko and Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard C. Wilson and William A. P. Smith, editors, _Proceedings of the British Machine Vision Conference (BMVC)_, pages 87.1-87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https://dx.doi.org/10.5244/C.30.87.
* Zhang et al. [2019] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International Conference on Machine Learning_, pages 7472-7482. PMLR, 2019.
* Zhao et al. [2022] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In _International Conference on Machine Learning_, pages 26982-26992. PMLR, 2022.
* Zhuang et al. [2021] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, James s Duncan, Ting Liu, et al. Surrogate gap minimization improves sharpness-aware training. In _International Conference on Learning Representations_, 2021.

Additional Experiments

### Estimating Test Robust Accuracy

Instead of estimating the robust generalization gap, one might expect the analysis on the relationship between test robust accuracy and the measures. In this regard, we investigate the correlation between the measures and the test robust accuracy \(1-\mathcal{E}(\bm{w};\epsilon,\mathcal{D})\) on the test dataset \(\mathcal{D}\) instead of the robust generalization gap \(g(\bm{w})\).

Tables 8 and 9 present the values of \(\psi_{k}\) and \(\pi_{k}\) for each measure. Fig. 6 illustrates the difference in total \(\tau\) when the robust generalization gap and the test robust accuracy are used as the target variable for correlation analysis. Certain measures exhibit stronger rank correlations with the test robust accuracy than the robust generalization gap. w_grad_norm, inverse_margin, and inverse_margin(PGD) exhibit the total \(\tau\) values exceeding 0.4 with respect to the test robust accuracy, whereas they show

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline  & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & 
\begin{tabular}{c} Early \\ Stopping \\ \end{tabular} & Total \(\tau\) \\ \hline num\_params (7) & 0.14\(\pm\)0.44 & - & - & - & - & - & - & 0.02 \\ path\_norm(8) & 0.22\(\pm\)0.57 & 0.36\(\pm\)0.41 & 0.33\(\pm\)0.94 & 0.08\(\pm\)0.72 & 0.22\(\pm\)0.50 & 0.20\(\pm\)0.70 & 0.17\(\pm\)0.73 & -0.43\(\pm\)0.51 & 0.02 \\ log\_prod\_of\_spec(9) & -0.22\(\pm\)0.43 & 0.06\(\pm\)0.42 & 0.05\(\pm\)1.00 & 0.04\(\pm\)0.73 & 0.03\(\pm\)0.49 & 0.07\(\pm\)0.73 & 0.07\(\pm\)0.73 & -0.16\(\pm\)0.68 & -0.15 \\
1bg\_prod\_of\_fit(0) & 0.12\(\pm\)0.41 & 0.36\(\pm\)0.39 & 0.04\(\pm\)0.77 & 0.30\(\pm\)0.04 & 0.41\(\pm\)0.39 & 0.06\(\pm\)0.40 & 0.05\(\pm\)0.08 & 0.06 & 0.07 \\ euclid\_init\_norm(11) & 0.16\(\pm\)0.45 & 0.28\(\pm\)0.31 & 0.14\(\pm\)0.49 & 0.19\(\pm\)0.73 & 0.14\(\pm\)0.48 & 0.13\(\pm\)0.69 & 0.10\(\pm\)0.68 & -0.03\(\pm\)0.62 & 0.10 \\ \hline average\_cc(12) & -0.09\(\pm\)0.05 & 0.03\(\pm\)0.35 & 0.08\(\pm\)1.00 & 0.17\(\pm\)0.73 & 0.13\(\pm\)0.57 & -0.01\(\pm\)0.77 & 0.05\(\pm\)0.57 & 0.04 \\ inverse\_margin(13) & 0.24\(\pm\)0.56 & 0.44\(\pm\)0.36 & 0.48\(\pm\)0.48 & 0.44\(\pm\)0.60 & 0.18\(\pm\)0.50 & 0.02\(\pm\)0.69 & 0.24\(\pm\)0.68 & 0.71\(\pm\)0.37 & 0.44 \\ prob\_margin(14) & 0.05\(\pm\)0.62 & -0.03\(\pm\)0.54 & -0.91\(\pm\)0.06 & -0.16\(\pm\)0.74 & 0.11\(\pm\)0.57 & -0.01\(\pm\)0.77 & -0.03\(\pm\)0.75 & -0.56\(\pm\)0.46 & -0.14 \\ boundary\_thickness(15) & 0.07\(\pm\)0.60 & 0.24\(\pm\)0.49 & 0.26\(\pm\)0.97 & 0.29\(\pm\)0.79 & 0.29\(\pm\)0.71 & 0.05\(\pm\)0.53 & 0.17\(\pm\)0.75 & 0.21\(\pm\)0.63 & 0.62\(\pm\)0.43 & 0.32 \\ kl\_divergence(16) & -0.55\(\pm\)0.49 & -0.57\(\pm\)0.36 & -0.36\(\pm\)0.35 & -0.65\(\pm\)0.48 & -0.46\(\pm\)0.40 & -0.37\(\pm\)0.60 & -0.45\(\pm\)0.61 & -0.63\(\pm\)0.47 & -0.45 \\ local\_lip(17) & -0.46\(\pm\)0.49 & -0.44\(\pm\)0.44 & -0.07\(\pm\)0.72 & -0.58\(\pm\)0.56 & -0.32\(\pm\)0.44 & -0.28\(\pm\)0.63 & -0.33\(\pm\)0.69 & -0.64\(\pm\)0.49 & -0.40 \\ packages\_flat(18) & 0.31\(\pm\)0.52 & -0.12\(\pm\)0.44 & 0.31\(\pm\)0.95 & 0.17\(\pm\)0.72 & 0.25\(\pm\)0.51 & -0.28\(\pm\)0.68 & -0.26\(\pm\)0.67 & -0.17\(\pm\)0.67 & 0.15 \\ estimated\_sharpness(19) & 0.90\(\pm\)0.60 & 0.25\(\pm\)0.39 & 0.39\(\pm\)0.63 & 0.16\(\pm\)0.75 & 0.12\(\pm\)0.50 & 0.06\(\pm\)0.38 & 0.32\(\pm\)0.67 & 0.20\(\pm\)0.66 & 0.18 \\ estimated\_inv\_sharpness(20) & 0.24\(\pm\)0.56 & 0.39\(\pm\)0.33 & 0.56\(\pm\)0.38 & 0.29\(\pm\)0.74 & 0.29\(\pm\)0.50 & -0.38\(\pm\)0.65 & -0.41\(\pm\)0.64 & 0.51\(\pm\)0.58 & 0.33 \\ average\_flat(21) & -0.44\(\pm\)0.50 & -0.01\(\pm\)0.41 & -0.57\(\pm\)0.45 & -0.04\(\pm\)0.51 & -0.04\(\pm\)0.24 & -0.30\(\pm\)0.62 & -0.35\(\pm\)0.63 & -0.66\(\pm\)0.46 & -0.46 \\ x\_grad\_norm(22) & -0.40\(\pm\)0.48 & -0.34\(\pm\)0.39 & 0.74\(\pm\)0.68 & -0.49\(\pm\)0.59 & -0.37\(\pm\)0.41 & -0.26\(\pm\)0.65 & -0.30\(\pm\)0.65 & -0.09\(\pm\)0.76 & -0.17 \\ y\_grad\_norm(23) & 0.20\(\pm\)0.52 & -0.42\(\pm\)0.35 & 0.66\(\pm\)0.75 & 0.37\(\pm\)0.59 & -0.50\(\pm\)0.42 & -0.66\(\pm\)0.42 & -0.65\(\pm\)0.60 & -0.60\(\pm\)0.54 & -0.57 \\ \hline average\_cc(PGD) (12) & -0.70\(\pm\)0.41 & -0.67\(\pm\)0.19 & -0.90\(\pm\)0.46 & -0.66\(\pm\)0.48 & -0.61\(\pm\)0.35 & -0.51\(\pm\)0.58 & -0.50\(\pm\)0.60 & -0.60\(\pm\)0.571 & -0.51\(\pm\)0.58 \\ inverse\_margin(PGD) (13) & 0.58\(\pm\)0.47 & 0.64\(\pm\)0.30 & 0.80\(\pm\)0.50 & 0.69\(\pm\)0.45 & 0.51\(\pm\)0.43 & 0.49\(\pm\)0.62 & 0.59\(\pm\)0.56 & 0.78\(\pm\)0.33 & 0.61 \\ prob\_margin(14) & 0.74\(\pm\)0.43 & 0.73\(\pm\)0.18 & 0.89\(\pm\)0.38 & 0.66\(\pm\)0.36 & -0.60\(\pm\)0.39 & 0.54\(\pm\)0.58 & -0.56\(\pm\)0.20 & 0.58\(\pm\)0.50 & 0.60 \\ prob\_array\{full}(18) & 0.62\(\pm\)0.44 & 0.59\(\pm\)0.24 & 0.81\(\pm\)0.58 & 0.60\(\pm\)0.50 & 0.52\(\pm\)0.45 & 0.60\(\pm\)0.48 & 0.72\(\pm\)0.38 & 0.11\(\pm\)0.77 &near-zero total \(\tau\) values for the robust generalization gap. Similarly, local_lip and average_flat show stronger correlations. Although we observe some different behavior of measures, we find that estimating the test robust accuracy can be more challenging. When we perform a linear regression analysis, predicting the test robust accuracy yields poor \(\bar{R}^{2}\) values. To ease comparison, we refer the readers detailed analysis to Table 15 in Appendix A.3.

### Focusing on Adversarially Robust models

In the main paper, we use all models regardless of their robust accuracy, to ensure the generality of our analyses. However, as we discussed in Table 4, adversarially robust and non-robust models may exhibit different behaviors with some measures. For instance, as shown in Fig. 7, non-robust models have extremely low values of pacbayes_flat(PGD) less 5. In constrast, robust models show higher values of pacbayes_flat(PGD) over 5. Thus, investigating only robust models might potentially reveal hidden behaviors of measures in predicting the robust generalization gap.

In Tables 10 and 11, we summarize \(\psi_{k}\) and \(\pi_{k}\) with conditioned on average_ce(PGD) \(\leq 1.5\). To ease the comparison between the previous results, we also illustrate the total \(\tau\) of those in Fig. 8. First, overall \(\pi_{k}\) and \(\psi_{k}\) of path_norm increases, and the total \(\tau\) increases 0.35 to 0.63. As shown in Fig. 8(a), the log of path_norm shows almost linear relationships between the robust generalization gap. Similarly, overall \(\pi_{k}\) and \(\psi_{k}\) of prob_margin, and the total \(\tau\) decreased -0.23 to -0.62. Similar to prob_margin, the margin-based measures, i.e., inverse_margin and average_ce, show more negative correlations. Thus, similar to probability margins on adversarial examples, maximizing the margins on benign examples may harm to the robust generalization with high probability. Lastly, boundary_thickness shows strong correlation to the robust generalization gap for average_ce(PGD) \(\leq 1.5\). As shown in Fig. 8(c), high boundary_thickness shows low robust generalization gap. Thus, we can conclude that average_ce(PGD) is also an effective condition for boundary_thickness as well as the training method.

Notably, some flatness-based measures have a weaker correlation, average_flat (-0.36 to -0.15), estimated_sharpness(PGD) (-0.22 to -0.02), and estimated_inv_sharpness(PGD) (-0.25 to 0.03). Their overall values of \(\psi_{k}\) and \(\pi_{k}\) in Tables 10 and 11 are also close to 0, which supports our claim that flatness measures cannot serve as reliable indicators of correlation in the adversarial training framework.

Figure 6: Comparison of the total \(\tau\) when the robust generalization \(g(\bm{w})\) (yellow) and the test robust accuracy \(1-\hat{\mathcal{E}}(\bm{w};\epsilon,\mathcal{D})\) (blue) are used as the target variables for correlation analysis.

Figure 7: Scatter plot of pacbayes_flat(PGD). Adversarially robust models (bright colors) and non-robust models (darker colors) show distinct range of values for pacbayes_flat(PGD).

\begin{table}
\begin{tabular}{|r|c c c c c c c c|c|} \hline  & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & Early & Total \\  & Architecture & Methods & Steps & Optimizer & Batch-size & Aug & Extra-data & Stopping & \(\tau\) \\ \hline num\_params (7) & 0.23\(\pm\)0.64 & & & & & & & & & 0.04 \\ path\_norm (8) & 0.43\(\pm\)0.54 & 0.45\(\pm\)0.50 & 0.54\(\pm\)0.84 & 0.77\(\pm\)0.46 & 0.32\(\pm\)0.52 & 0.71\(\pm\)0.49 & 0.71\(\pm\)0.51 & 0.88\(\pm\)0.30 & 0.63 \\ log\_prod\_spec (9) & -0.14\(\pm\)0.56 & 0.25\(\pm\)0.57 & 0.21\(\pm\)0.98 & 0.44\(\pm\)0.73 & 0.34\(\pm\)0.50 & 0.16\(\pm\)0.79 & 0.29\(\pm\)0.78 & 0.55\(\pm\)0.62 & -0.09 \\ log\_prod\_of\_fro (10) & -0.14\(\pm\)0.58 & 0.56\(\pm\)0.54 & 0.55\(\pm\)0.84 & 0.66\(\pm\)0.59 & 0.68\(\pm\)0.39 & 0.36\(\pm\)0.73 & 0.45\(\pm\)0.73 & 0.84\(\pm\)0.38 & 0.16 \\ euclid\_init\_norm (11) & 0.23\(\pm\)0.62 & -0.20\(\pm\)0.26 & -0.05\(\pm\)0.04 & -1.00\(\pm\)0.39 & -0.72\(\pm\)0.16 & 0.55\(\pm\)0.46 & 0.53\(\pm\)0.62 & -0.40\(\pm\)0.73 & 0.13 \\ \hline average\_ce (12) & -0.50\(\pm\)0.53 & -0.49\(\pm\)0.46 & -0.28\(\pm\)0.39 & -0.66\(\pm\)0.73 & -0.74\(\pm\)0.34 & -0.68\(\pm\)0.49 & -0.74\(\pm\)0.41 & -0.71\(\pm\)0.53 & 0.63 \\ inverse\_margin (13) & -0.12\(\pm\)0.70 & -0.28\(\pm\)0.61 & 0.23\(\pm\)0.97 & -0.33\(\pm\)0.77 & -0.28\(\pm\)0.65 & -0.35\(\pm\)0.75 & -0.33\(\pm\)0.77 & -0.45\(\pm\)0.67 & -0.48 \\ prob\_margin (14) & 0.50\(\pm\)0.53 & -0.45\(\pm\)0.46 & 0.29\(\pm\)0.96 & 0.76\(\pm\)0.38 & 0.34\(\pm\)0.74 & 0.70\(\pm\)0.48 & 0.75\(\pm\)0.41 & 0.71\(\pm\)0.53 & 0.63 \\ boundary\_thickness (15) & -0.38\(\pm\)0.57 & -0.25\(\pm\)0.50 & -0.80\(\pm\)0.50 & -0.62\(\pm\)0.41 & -0.57\(\pm\)0.58 & -0.65\(\pm\)0.50 & -0.58\(\pm\)0.62 & -0.50 \\ kl\_divergence (16) & -0.36\(\pm\)0.60 & -0.03\(\pm\)0.55 & -0.52\(\pm\)0.85 & -0.08\(\pm\)0.39 & -0.57\(\pm\)0.70 & -0.46\(\pm\)0.36 & -0.36\(\pm\)0.73 & -0.53\(\pm\)0.68 & -0.31 \\ local\_lip (17) & -0.16\(\pm\)0.66 & 0.15\(\pm\)0.52 & -0.37\(\pm\)0.93 & 0.24\(\pm\)0.76 & -0.06\(\pm\)0.59 & -0.46\(\pm\)0.68 & 0.11\(\pm\)0.79 & 0.11\(\pm\)0.79 & 0.04 \\ packages\_flat (18) & 0.32\(\pm\)0.65 & 0.10\(\pm\)0.54 & 0.30\(\pm\)0.95 & 0.10\(\pm\)0.83 & 0.15\(\pm\)0.54 & -0.35\(\pm\)0.71 & -0.31\(\pm\)0.75 & 0.47\(\pm\)0.72 & 0.00 \\ estimated\_sharpness (19) & 0.20\(\pm\)0.67 & -0.12\(\pm\)0.55 & 0.26\(\pm\)0.97 & 0.16\(\pm\)0.82 & -0.02\(\pm\)0.61 & -0.41\(\pm\)0.68 & -0.29\(\pm\)0.78 & 0.13\(\pm\)0.81 & -0.26 \\ estimated\_inv\_sharpness (20) & -0.06\(\pm\)0.66 & -0.14\(\pm\)0.56 & 0.32\(\pm\)0.95 & 0.19\(\pm\)0.82 & 0.02\(\pm\)0.63 & -0.41\(\pm\)0.68 & -0.28\(\pm\)0.27 & 0.03\(\pm\)0.81 & -0.29 \\ average\_flat (21) & -0.12\(\pm\)0.56 & -0.12\(\pm\)0.60 & -0.48\(\pm\)0.88 & 0.14\(\pm\)0.79 & -0.09\(\pm\)0.48 & -0.63\(\pm\)0.52 & -0.27\(\pm\)0.76 & -0.22\(\pm\)0.74 & -0.15 \\ x\_gard\_norm (22) & -0.39\(\pm\)0.63 & -0.22\(\pm\)0.59 & -0.52\(\pm\)0.85 & -0.52\(\pm\)0.68 & -0.65\(\pm\)0.44 & -0.80\(\pm\)0.36 & -0.75\(\pm\)0.47 & -0.70\(\pm\)0.59 & -0.63 \\ y\_gard\_norm (21) & -0.13\(\pm\)0.65 & -0.14\(\pm\)0.56 & 0.41\(\pm\)0.91 & 0.03\(\pm\)0.84 & -0.14\(\pm\)0.60 & -0.45\(\pm\)0.65 & -0.34\(\pm\)0.74 & -0.30\(\pm\)0.79 & -0.34 \\ \hline average\_ce(PGD) (12) & -0.64\(\pm\)0.55 & -0.74\(\pm\)0.41 & -0.07\(\pm\)0.71 & -0.87\(\pm\)0.28 & -0.86\(\pm\)0.26 & -0.30\(\pm\)0.37 & -0.84\(\pm\)0.44 & -0.81\(\pm\)0.45 & -0.76 \\ inverse\_margin(PGD) (13) & 0.57\(\pm\)0.48 & -0.13\(\pm\)0.61 & 0.55\(\pm\)0.84 & 0.13\(\pm\)0.82 & 0.39\(\pm\)0.63 & 0.28\(\pm\)0.81 & 0.26\(\pm\)0.81 & 0.10\(\pm\)0.80 & -0.01 \\ prob\_margin (PGD) (14) & 0.61\(\pm\)0.58 & 0.57\(\pm\)0.42 & 0.66\(\pm\)0.75 & 0.88\(\pm\)0.27 & 0.85\(\pm\)0.79 & 0.79\(\pm\)0.37 & 0.84\(\pm\)0.34 & 0.79\(\pm\)0.50 & 0.76 \\ packages\_flat(PGD) (18) & 0.45\(\pm\)0.62 & 0.29\(\pm\)0.55 & 0.63\(\pm\)0.78 & 0.33\(\pm\)0.78 & 0.45\(\pm\)0.48 & 0.16\(\pm\)0.79 & 0.08\(\pm\)0.63 & 0.65\(\pm\)0.59 & 0.26 \\ estimated\_sharpness (PGD) (19) & 0.10\(\pm\)0.66 & 0.03\(\pm\)0.56 & 0.12\(\pm\)0.99 & 0.40\(\pm\)0.71 & 0.17\(\pm\)0.61 & -0.21\(\pm\)0.76 & -0.18\(\pm\)0.79 & 0.29\(\pm\)0.76 & 0.02 \\ estimated\_inv\_sharpness (PD) (20) & -0.03\(\pm\)0.66 & -0.01\(\pm\)0.57 &Figure 8: Comparison of total \(\tau\) between no condition and average_ce(PGD) \(\leq 1.5\). Following measures show strong correlation on the condition: path_norm, average_ce, x_grad_norm, inverse_margin, prob_margin, and boundary_thickness.

Figure 9: Scatter plot of the measures showing the most increased the total \(\tau\) when conditioned on average_ce(PGD) \(\leq 1.5\).

### Robust Measures with Regression Analysis

In the seminar work of [17], the concept of _an affine oracle_ was proposed, which utilizes linear regression to assess the performance of measures. Motivated by this experiment, we conduct a simple linear regression analysis. Specifically, for each measure, we calculate the optimal coefficients and bias to predict the robust generalization gap \(g(\bm{w})\). Considering that average_ce(PGD) consistently exhibits the highest correlation across all settings, we use it as a baseline for the regression, i.e., \(\beta_{1}\times\texttt{average\_ce(PGD)}+\beta_{0}\). We then consider each measure as an independent variable, resulting in a new form of measure \(\beta_{1}\times\texttt{average\_ce(PGD)}+\beta_{2}\times\texttt{measure}+\beta_{0}\). To ensure high predictability, we perform this regression analysis with robust models with average_ce(PGD) \(\leq 1.5\).

In Table 12, we calculate the coefficient of determination (\(R^{2}\)) for the regression analysis to evaluate the extent to which each measure explained the generalization gap. Consistent with the findings in the main paper, x_grad_norm and prob_margin(PGD) exhibit the highest \(R^{2}\) values.

Additionally, to examine the distributional information of each measure, we plot the cumulative distribution of absolute errors for each model with the mean (solid line) and the interval denoted by the first quartile (Q1, dotted line) and the third quartile (Q3, dashed line). The results are illustrated in Figure 10. We observe that only a few measures, such as x_grad_norm and prob_margin(PGD), provided meaningful information about the generalization gap, while the effects of other measures are

\begin{table}
\begin{tabular}{|c|c|c|} \hline Measures & \(R^{2}\) & Measures & \(R^{2}\) \\ \hline num\_params (7) & 0.87 & estimated\_sharpness (19) & 0.88 \\ path\_norm (8) & 0.87 & estimated\_inv\_sharpness (20) & 0.89 \\ log\_prod\_of\_spec (9) & 0.88 & average\_flat (21) & 0.87 \\ log\_prod\_of\_tro (10) & 0.88 & x\_grad\_norm (22) & **0.91** \\ euclid\_init\_norm (11) & 0.87 & w\_grad\_norm (23) & 0.88 \\ average\_ce (12) & 0.86 & inverse\_margin(PGD) (13) & 0.86 \\ inverse\_margin (13) & 0.86 & prob\_margin(PGD) (14) & **0.91** \\ prob\_margin (14) & 0.87 & pacbayes\_flat(PGD) (18) & 0.89 \\ boundary\_thickness (15) & 0.86 & estimated\_sharpness(PGD) (19) & 0.87 \\ kl\_divergence (16) & 0.87 & estimated\_inv\_sharpness(PGD) (20) & 0.86 \\ local\_1ip (17) & 0.87 & x\_grad\_norm(PGD) (22) & 0.88 \\ pacbayes\_flat (18) & 0.90 & w\_grad\_norm(PGD) (23) & 0.87 \\ \hline \end{tabular}
\end{table}
Table 12: Regression analysis in Fig. 10 for the target variable, the robust generalization gap \(g(\bm{w})\). We summarize their \(R^{2}\) for each measure.

Figure 10: Cumulative distribution of absolute errors for the robust generalization gap estimation is shown. A simple linear regression model is constructed to estimate the robust generalization gap, using average_ce(PGD) as the baseline measure, and each measure considered as an additional independent variable. The mean (solid line), Q1 (dotted line), and Q3 (dashed line) are plotted. A lower absolute error indicates that the measure is more effective in estimating the robust generalization gap.

negligible. Specifically, x_grad_norm and prob_margin(PGD) achieve the lowest mean absolute error across all trained models.

In Table 13, we evaluate the effectiveness of predictors generated from the regression analysis. The predictor using x_grad_norm achieves an exceptionally strong correlation of 0.81, an increase of 0.18 compared to the previous value in Table 10. The predictor using prob_margin(PGD) also exhibits improved performance. These results suggest the potential to effectively predict the robust generalization gap by combining existing measures.

To push further, we conduct a 5-fold evaluation strategy with a linear regression model to predict the robust generalization gap. Specifically, we use the forward selection to identify the most effective set of measures. In Table 14, we present the results of our 5-fold evaluation, reporting the average \(\tau\) along with its standard deviation. average_ce(PGD) is selected as a prominent predictor, followed by the selection of x_grad_norm. Furthermore, our exploration identifies pacbayes_mag_flat(PGD) and x_grad_norm(PGD) as additional effective measures, resulting in higher average \(\tau\) compared to using the entire feature set.

In Table 15, we conduct a linear regression analysis to predict the test robust accuracy, rather than the robust generalization gap. Compared to Table 12, overall values of \(R^{2}\) for each measure are lower. This implies that directly predicting the test robust accuracy might be more difficult than the robust generalization gap.

\begin{table}
\begin{tabular}{c|l|c} \hline \hline \multicolumn{2}{c|}{Generated Measures} & Total \(\tau\) \\ \hline \(-514.11\times\)x_grad_norm\(-23.87\times\)average_ce(PGD)\(+57.32\) & 0.81 (+0.18) \\ \(81.17\times\)prob_margin(PGD)\(+13.52\times\)average_ce(PGD)\(-20.60\) & 0.78 (+0.02) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Total \(\tau\) of generated measures from the regression analysis. Both new measures show improved performance in predicting the robust generalization gap.

\begin{table}
\begin{tabular}{c|l} \hline \hline Measures & \(R^{2}\) \\ \hline num_params (7) & 0.10 \\ path_norm (8) & 0.07 \\ log_prod_of_spec (9) & 0.14 \\ log_prod_of_for (10) & 0.14 \\ euclid\_init_norm (11) & 0.12 \\ average\_ce (12) & 0.12 \\ inverse\_margin (13) & 0.05 \\ prob\_margin (14) & 0.20 \\ boundary\_thickness (15) & 0.20 \\ kl\_divergence (16) & 0.13 \\ local\_lip (17) & 0.16 \\ pacbayes\_flat (18) & 0.40 \\ \hline \hline \end{tabular} 
\begin{tabular}{c|l} \hline Measures & \(R^{2}\) \\ \hline estimated\_sharpness (19) & 0.33 \\ estimated\_inv\_sharpness (20) & 0.41 \\ average\_flat (21) & 0.12 \\ x\_grad\_norm (22) & 0.09 \\ w\_grad\_norm (23) & 0.38 \\ inverse\_margin(PGD) (13) & 0.04 \\ prob\_margin(PGD) (14) & 0.24 \\ pacbayes\_flat (PGD) (18) & 0.48 \\ estimated\_sharpness(PGD) (19) & 0.04 \\ estimated\_inv\_sharpness(PGD) (20) & 0.05 \\ x\_grad\_norm(PGD) (22) & 0.16 \\ w\_grad\_norm(PGD) (23) & 0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Regression analysis for the target variable, the test robust accuracy \(1-\mathcal{E}(\bm{w};\epsilon,\mathcal{D})\). We summarize their \(R^{2}\) for each measure.

### Robust Generalization Gap with AutoAttack [11]

In the main paper, we estimate the robust generalization gap using PGD10. While we acknowledge the potential benefits of stronger attacks, such as AutoAttack [11], we mainly use PGD10 due to the following reasons. Firstly, the computational cost of AutoAttack is substantial. Our experimental design involves training models across diverse adversarial settings and requires adversarial examples for both training and test datasets to estimate the robust generalization gap. AutoAttack takes 10 min per batch for WRN-34-10 on our resources. Since we have 1300 models, we need at least 1 year to obtain all adversarial examples even with 6 GPUs. Secondly, the prevalent usage of PGD among various methods. AT, TRADES, and MART use PGD as a baseline during training and further adopt early-stopping by using PGD on training or validation sets. Lastly, some measures, namely boundary_thickness and local_lip, rely on PGD adversarial examples for their calculation. As these robustness measures are often computed using PGD, the choice to use PGD for evaluation contributes to consistency across our experiments.

However, we here highlight the potential benefits of utilizing AutoAttack in future work. In Fig. 11, we calculate the gap \(g(\bm{w})\) using AutoAttack for 30 models on CIFAR-10 in RobustBench [12]. While the gaps calculated using PGD10 and AutoAttack exhibit an almost linear relationship, there are a few exceptions (2 out of 30): 'Ding2020 MMA' [14] and 'Sitawarin2020Improving' [55]. We leave this question open for further exploration.

### Importance of Model Architecture in Norm-based Measures

In Fig. 12, we plot log_prod_of_spec and log_prod_of_fro for each model architecture. We can observe that the range of each measure extremely varies with respect to the used model architecture. When the model architecture is fixed, they show some correlation with the robust generalization gap as described in the main paper.

### Early Stopping and Estimated Sharpness

In the main paper, we discussed that estimated_sharpness exhibits a significant negative correlation when early stopping is not used. Fig. 13 shows the importance of early-stopping for estimated sharpness measures. Compared to Figures 4 and 13, when early stopping is employed, the correlation approaches zero as shown in Figures 13 and 13. This results supports the observation of prior study [56] that the importance of early stopping in the analysis of flatness.

Figure 11: Scatter plot of the robust generalization gap calculated by PGD and AutoAttack. The color of each dot implies the test robust accuracy on CIFAR-10.

Figure 12: Distributions of norm-based measures for model architectures. For each model architecture, the range of measure extremely varies.

## Appendix B Measures

In this section, we introduce the concept of each category of measures in the main paper, then explain the details of each measure and their mathematical definitions. Here, we denote \(\mathbf{W}_{i}\) as the weight tensor of \(i\)-th layer, following [17]. Given the number of layers \(d\), the whole trainable parameters are denoted as \(\bm{w}=\text{vec}(\mathbf{W}_{1},\mathbf{W}_{2},\cdots,\mathbf{W}_{d})\).

**Weight-norm.** Based on some theoretical frameworks such as PAC-Bayes [43, 47, 38], weight norm-based measures are expected to be correlated with generalization performance. Among them, the product of Frobenius norm [46] (log_prod_of_fro), the product of spectral norm [7] (log_prod_of_spec), and path norm (path_norm) have been considered as important measures [27, 17]. Furthermore, the distance to the converged weight from the initial weight (euclid_init_norm) is also used to estimate the generalization gap [27]. Liu et al. [40] also argues that this distance can be used to judge the difficulty of optimization in adversarial training.

\(\rhd\)num_params.

\[\sum_{i=1}^{d}k_{i}c_{i-1}(c_{i}+1),\] (7)

where \(c_{i}\) is the number of channels and \(k_{i}\) is the kernel size at layer \(i\). In the experiments, we calculated num_params by adding the number of parameters of all convolutional and linear layers. num_params is a fixed value when a model architecture is given.

\(\rhd\)path_norm.

\[\sum_{i}f_{\bm{w}^{2}}(\mathbf{1})[i],\] (8)

where \(\bm{w}^{2}\) is the element-wise square operation and \(f(\cdot)[i]\) is the \(i\)-th logit output of the network. By setting all input variables as 1, this measure captures geometric properties of optimization under scale-invariant characteristics.

\(\rhd\)log_prod_of_spec.

\[\log\left(\prod_{i=1}^{d}\left\|\mathbf{W}_{i}\right\|_{2}^{2}\right),\] (9)

where \(\|\cdot\|_{2}\) is a matrix \(L_{2}\)-norm, i.e., the largest singular value of each layer.

\(\rhd\)log_prod_of_spec.

\[\log\left(\prod_{i=1}^{d}\left\|\mathbf{W}_{i}\right\|_{F}^{2}\right),\] (10)

where \(\|\cdot\|_{F}\) is a Frobenius norm, i.e., the square root of the sum of the squares of the weight matrix.

\(\rhd\)euclid_init_norm.

\[\frac{1}{d}\sum_{i=1}^{d}\|\bm{w}_{i}-\bm{w}_{i}^{0}\|_{2},\] (11)

where \(\bm{w}_{i}=\text{vec}(\mathbf{W}_{i})\) and the initial weight of \(i\)-th layer \(\bm{w}_{i}^{0}\).

Figure 13: Scatter plot of the estimated sharpness measures for the use of early-stopping. The same condition, average_ce(PGD)\(<2\), used as Fig. 4.

**Margin.** Margins are also actively researched measures to estimate the generalization gap [26]. For instance, the 10-th percentile of the margin values in the output space on the training set (inverse_margin) is often used to measure the generalization bound for neural networks [50, 27]. In terms of adversarial robustness, most of attack methods and defense methods utilize the probability margins (prob_margin). Yang et al. [67] further proposed a new measure, called boundary thickness (boundary_thickness), that is a generalized version of margin and argued that it is highly correlated to the robust generalization gap.

\(\rhd\) average_ce.

\[\mathbb{E}_{\bm{x},y}\left[\mathcal{L}_{ce}(f(\bm{x},\bm{w}),y)\right],\] (12)

where \(\mathcal{L}_{ce}\) is the cross-entropy loss.

\(\rhd\) inverse_margin.

\[1/\gamma^{2},\] (13)

where \(\gamma\) is 10th-percentile of \(\{\sigma(f(\bm{x},\bm{w}))_{y}-\max_{i\neq y}\sigma(f(\bm{x},\bm{w}))_{i}\}\) for all \(\bm{x},y\), with the sigmoid function \(\sigma(\cdot)\).

\(\rhd\) prob_margin.

\[\mathbb{E}_{\bm{x},y}\left[\sigma(f(\bm{x},\bm{w}))_{y}-\max_{i\neq y}\sigma( f(\bm{x},\bm{w}))_{i}\right].\] (14)

\(\rhd\) boundary_thickness.

\[\mathbb{E}_{\bm{x}}\left[\|\bm{x}-\bm{x}^{*}\|_{2}\int_{0}^{1}\mathbbm{1}\{a< g(\bm{x},\bm{x}^{*},\lambda)<b\}d\lambda\right|\arg\max_{i}\sigma(f(\bm{x}, \bm{w}))_{i}\neq\arg\max_{i}\sigma(f(\bm{x}^{*},\bm{w}))_{i}\right],\] (15)

where \(\mathbbm{1}\{\cdot\}\) is an indicator function, \(g(\bm{x},\bm{x}^{*},\lambda)=\sigma(f(\lambda\bm{x}+(1-\lambda)\bm{x}^{*},\bm {w}))_{\hat{y}}-\sigma(f(\lambda\bm{x}+(1-\lambda)\bm{x}^{*},\bm{w}))_{\hat{y }^{*}}\), \(\hat{y}=\arg\max_{i}\sigma(f(\bm{x},\bm{w}))_{i}\) and \(a,b\) are the hyperparameters that controls the sensitivity of the boundary thickness. Following [67], we find \(\bm{x}^{*}\) by using PGD10 with \(L_{2}\)-norm, \(\epsilon=1\), \(\alpha=0.2\), then \(a=0\), \(b=0.75\), and batch size 128. A higher value of boundary_thickness implies a larger margin in the output space.

**Smoothness.** Based on prior works [6, 10, 28], a line of work has focused on the smoothness for achieving adversarial robustness in adversarial training. Most simply, the KL divergence between benign and adversarial logits (kl_divergence) of TRADES [71] can be regarded as a smoothness regularization due to its logit pairing. Yang et al. [66] investigated the theoretical benefit of local Lipschitzness (local_lip) and demonstrated that its value estimated on the test dataset correlates with the robust generalization gap.

\(\rhd\) kl_divergence.

\[\mathbb{E}_{\bm{x}}\left[\max_{\|\bm{x}-\bm{x}^{*}\|\leq\epsilon}\textsc{KL} (f(\bm{x},\bm{w}),f(\bm{x}^{*},\bm{w}))\right],\] (16)

where KL is KL-divergence and the maximization is conducted by PGD10 with the step-size \(2/255\). A lower value of kl_divergence implies that a model outputs similar outputs for both benign example and adversarial example.

\(\rhd\) local_lip.

\[\mathbb{E}_{\bm{x}}\left[\max_{\|\bm{x}-\bm{x}^{*}\|\leq\epsilon}\frac{\|f( \bm{x},\bm{w})-f(\bm{x}^{*},\bm{w})\|_{1}}{\|\bm{x}-\bm{x}^{*}\|_{\infty}} \right],\] (17)

where the maximization is conducted by PGD10 with the step-size \(2/255\). This is the empirical version of the local Lipschitzness, resulting a lower value of local_lip implies a smoother model.

**Flatness.** Flatness is a recently focused measure in the generalization domain [43, 30]. Recent works argue that flatter minima yield better generalization performance than sharper minima. The common strategy to achieve flatness is to minimize the estimated sharpness [18, 73], which is the difference between the current loss and the maximum loss for a given neighborhood (estimated_sharpness). Kwon et al. [36] investigated the scale-invariant sharpness (estimated_inv_sharpness). Note that other diverse concept of estimated sharpness is actively researched in recent works [33, 5, 34]. Adversarial weight perturbation (AWP) [64] also has dramatically improved adversarial robustness by minimizing the loss of perturbed weight. Recently, Stutz et al. [56] has demonstrated that theirproposed measure, average flatness (average_flat), is effective for estimating robust generalization gap.

\(\rhd\) pacbayes_flat. Based on PAC-Bayesian framework [43], Jiang et al. [27] proposed a simplified version of PAC-Bayesian bounds as follows:

\[1/\sigma^{\prime}\] (18)

where \(\sigma^{\prime}\) is the largest value such that \(\mathbb{E}_{\bm{u}}[\mathcal{E}(\bm{w}+\bm{u};\{\bm{x},y\})-\mathcal{E}(\bm{w} ;\{\bm{x},y\})]<0.1\). Here, \(u_{j}\sim\mathcal{N}(0,\sigma^{\prime})\) and \(\mathcal{E}(\bm{w};\{\bm{x},y\})\) denotes the error on the given set \(\{\bm{x},y\}\). Thus, a higher value of pacbayes_flat implies flatter optimum in terms of error landscape.

\(\rhd\) estimated_sharpness.

\[\mathbb{E}_{\bm{x},y}\left[\max_{\|\bm{v}\|_{2}\leq\rho}\mathcal{L}_{ce}(f(\bm {x},\bm{w}+\bm{v}),y)-\mathcal{L}_{ce}(f(\bm{x},\bm{w}),y)\right].\] (19)

Following [73], we calculate the estimated sharpness with a single-step ascent with \(\rho=0.1\). A higher value of estimated_sharpness implies a sharper optimum in terms of loss landscape.

\(\rhd\) estimated_inv_sharpness.

\[\mathbb{E}_{\bm{x},y}\left[\max_{\|T_{w}^{-1}\bm{v}\|_{2}\leq\rho}\mathcal{L }_{ce}(f(\bm{x},\bm{w}+\bm{v}),y)-\mathcal{L}_{ce}(f(\bm{x},\bm{w}),y)\right],\] (20)

where \(T_{\bm{w}}\) is \(\|\bm{w}\|\), i.e., element-wise adaptive sharpness, and \(\mathcal{L}_{ce}\) is the cross-entropy loss. Similar to the estimated sharpness, we calculate the estimated invariant sharpness with a single-step ascent with \(\rho=0.1\)[36]. A higher value of estimated_inv_sharpness implies a sharper optimum in terms of loss landscape.

\(\rhd\) average_flat.

\[\mathbb{E}_{\bm{x},y}\left[\mathbb{E}_{\bm{v}\in\mathcal{B}_{\rho}(\bm{w})} \left[\max_{\|\bm{x}-\bm{x}^{\prime}\|\leq\epsilon}\mathcal{L}_{ce}(f(\bm{x} ^{*},\bm{w}+\bm{v}),y)\right]-\max_{\|\bm{x}-\bm{x}^{\prime}\|\leq\epsilon} \mathcal{L}_{ce}(f(\bm{x}^{*},\bm{w}),y)\right],\] (21)

where \(\mathcal{B}_{\rho}(\bm{w})=\{\bm{w}+\bm{v}\|\|_{\bm{v}}\|_{2}\leq\rho\|\bm{w} _{\bm{v}}\|_{2}\forall\text{ layers }i\}\). We use PGD10 with \(\epsilon=8/255\) for both inner maximizations. Following [56], we take 10 random weight perturbations with \(\rho=0.5\). A higher value of average_flat implies a sharper optimum in terms of adversarial loss landscape.

**Gradient-norm.** Gradient-norm with respect to input or weight is also a consistently researched area in terms of generalization. Recently, Zhao et al. [72] also demonstrated that regularizing the gradient norm of weights (w_grad_norm) can achieve sufficient improvement on several tasks. Additionally, there are a few works that emphasize the importance of regularizing the gradient norm of weights [53, 44]. The gradient norm of inputs (x_grad_norm) also can have underlying correlation between the robust generalization performance. Prior works utilized the input gradient for analyzing adversarially trained models [4] and generating adversarial examples [35, 16].

\(\rhd\) x_grad_norm.

\[\mathbb{E}_{\bm{x},y}\left[\|\nabla_{\bm{x}}\mathcal{L}_{ce}(f(\bm{x},\bm{w}),y)\|_{2}\right].\] (22)

\(\rhd\) w_grad_norm.

\[\mathbb{E}_{\bm{x},y}\left[\|\nabla_{\bm{w}}\mathcal{L}_{ce}(f(\bm{x},\bm{w}),y)\|_{2}\right].\] (23)

Comment on batch normalization fusionHere, we provide comments on some further discussed things when estimating the above measures. In previous studies [27, 17], it has been observed that considering batch normalization (batch-norm) layers can have an impact on common generalization measures, such as sharpness [15]. To address this issue, the batch-norm layers and other moving statistics were fused with the preceding convolution layers before calculating the values of generalization measures. Thus, when estimating {num_params, path_norm, log_prod_of_spec, log_prod_of_fro, pacbayes_flat}, we apply batch-norm fusion to all ResNet blocks. However, for certain blocks, such as pre-activation ResNets, where the batch-norm layer is placed at the beginning, the fusion cannot be directly applied. To ensure consistency, we add an identity convolutional layer in front of all batch-norm layers that do not have the preceding convolution layer. While there are various batch-norm fusion (or batch-norm folding) techniques, including those related to generalization [27; 17; 5], quantization [69; 37; 49], and memory optimization domains [8; 19; 2], there is no precise solution to address this problem in the context of various model structures (ResNets, PreActResNets, ViT, etc.) and activation functions (SiLU, LeakyReLU, etc.), which we leave as a topic for future work.

## Appendix C Training Details

In Section 3, 1,344 models were trained using the CIFAR-10 dataset with \(\epsilon=8/255\). We here provide the detailed training settings. We followed the common settings used in [42; 71; 48; 21].

Given the higher Rademacher complexity [68] and larger sample complexity [54] of adversarial training, data augmentation [21] and the utilization of extra data [9] can significantly improve the adversarial robustness. Therefore, we also considered the impact of augmentation technique, including _RandomCrop_ with padding 4 and _RandomHorizontalFlip_, as well as the use of additional data collected by Carmon et al. [9].

Regarding model architectures, we employed three different models: ResNet18 [23], WRN28-10 [70], and WRN34-10 [70]. These models have been widely adopted and serve as benchmarks for evaluating the stability and performance of adversarial training methods. It is worth noting that the majority of models trained on CIFAR-10 in RobustBench [12] consist of WRN28-10 (15) and WRN34-10 (14) among the 63 available models.

For training methods, we considered four different approaches: Standard, AT [42], TRADES [71], and MART [61]. Notably, AT, TRADES, and MART have been shown to outperform other variations by incorporating various training tricks [48] and integrating recent techniques [9; 64]. For all methods, we generated adversarial examples using projected gradient descent (PGD) [42]. During training, a single-step approximation of the inner maximization in Eq. (1) can lead to faster adversarial training, but may suffer from catastrophic overfitting [63; 31]. On the other hand, a large number of steps leads to stable robustness, but requires heavy computational costs. Therefore, we considered both 1 and 10 steps for each adversarial training method.

In terms of optimization, we used SGD with momentum 0.9 and weight decay of \(5\times 10^{-4}\), and a step-wise learning rate decay was performed at epochs 100 and 150 with a decay rate of 0.1. In all the experiments, we trained the models for 200 epochs. As highlighted by Pang et al. [48], the batch size used during adversarial training has been found to affect its performance. Thus, we varied the batch size among {32, 64, 128, 256}. Additionally, we also considered adversarial weight perturbation (AWP) [64], which can improve the robust generalization performance of models. AWP belongs to the class of sharpness-aware minimization methods [18; 36]. This can be formalized as follows:

\[\min_{\bm{w}}\ \max_{\|\bm{x}^{adv}-\bm{x}\|\leq\epsilon,\bm{v}\in\mathcal{B} _{\rho}(\bm{w})}\mathcal{L}(f(\bm{x}^{adv},\bm{w}+\bm{v}),y),\] (24)

where \(\mathcal{B}_{\rho}(\bm{w})=\{\bm{w}+\bm{v}\|\|\bm{v}_{i}\|_{2}\leq\rho\|\bm{w} _{i}\|_{2}\ \forall i\text{-th layer}\}\). As described by Wu et al. [64], \(\bm{x}^{adv}\) is calculated based on the non-perturbed model \(f(\bm{w})\), and a single step of maximization with respect to \(\bm{v}\) is sufficient to improve robustness. We used the best-performing value of \(\rho=5\times 10^{-3}\) from [64].