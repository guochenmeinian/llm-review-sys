# Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games

 Brian Hu Zhang

Carnegie Mellon University

bhzhang@cs.cmu.edu

&Gabriele Farina

MIT

gfarina@mit.edu

&Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

&Federico Cacciamani

DEIB, Politecnico di Milano

federico.cacciamani@polimi.it

&Stephen McAleer

Carnegie Mellon University

smcaleer@cs.cmu.edu

&Andreas Haupt

MIT

haupt@mit.edu

&Andrea Celli

Bocconi University

andrea.celli2@unibocconi.it

&Nicola Gatti

DEIB, Politecnico di Milano

nicola.gatti@polimi.it

&Vincent Conitzer

Carnegie Mellon University

conitzer@cs.cmu.edu

&Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

Equal contribution.

###### Abstract

We introduce a new approach for _computing_ optimal equilibria and mechanisms via learning in games. It applies to extensive-form settings with any number of players, including mechanism design, information design, and solution concepts such as correlated, communication, and certification equilibria. We observe that _optimal_ equilibria are minimax equilibrium strategies of a player in an extensive-form zero-sum game. This reformulation allows us to apply techniques for learning in zero-sum games, yielding the first learning dynamics that converge to optimal equilibria, not only in empirical averages, but also in iterates. We demonstrate the practical scalability and flexibility of our approach by attaining state-of-the-art performance in benchmark tabular games, and by computing an optimal mechanism for a sequential auction design problem using deep reinforcement learning.

## 1 Introduction

What does it mean to _solve_ a game? This is one of the central questions addressed in game theory, leading to a variety of different solution concepts. Perhaps first and foremost, there are various notions of _equilibrium_, strategically stable points from which no rational individual would be inclined to deviate. But is it enough to compute, or indeed _learn_, just any one equilibrium of a game? In two-player zero-sum games, one can make a convincing argument that a single equilibrium in fact constitutes a complete solution to the game, based on the celebrated minimax theorem of von Neumann [97]. Indeed, approaches based on computing minimax equilibria intwo-player zero-sum games have enjoyed a remarkable success in solving major AI challenges, exemplified by the recent development of superhuman poker AI agents [10, 11].

However, in general-sum games it becomes harder to argue that _any_ equilibrium constitutes a complete solution. Indeed, one equilibrium can offer vastly different payoffs to the players than another. Further, if a player acts according to one equilibrium and another player according to a different one, the result may not be an equilibrium at all, resulting in a true _equilibrium selection problem_. In this paper, therefore, we focus on computing an _optimal_ equilibrium, that is, one that maximizes a given linear objective within the space of equilibria. There are various advantages to this approach. First, in many contexts, we would simply prefer to have an equilibrium that maximizes, say, the sum of the players' utilities--and by computing such an equilibrium we also automatically avoid Pareto-dominated equilibria. Second, it can mitigate the equilibrium selection problem: if there is a convention that we always pursue an equilibrium that maximizes social welfare, this reduces the risk that players end up playing according to different equilibria. Third, if one has little control over how the game will be played but cares about its outcomes, one may like to understand the space of all equilibria. In general, a complete picture of this space can be elusive, in part because a game can have exponentially many equilibria; but computing extreme equilibria in many directions--say, one that maximizes Player 1's utility--can provide meaningful information about the space of equilibria.

That being said, many techniques that have been successful at computing a single equilibrium do not lend themselves well to computing optimal equilibria. Most notably, while _no-regret_ learning dynamics are known to converge to different notions of _correlated equilibria_[49, 38, 39, 47], little is known about the properties of the equilibrium reached. In this paper, therefore, we introduce a new paradigm of learning in games for _computing_ optimal equilibria. It applies to extensive-form settings with any number of players, including information design, and solution concepts such as correlated, communication, and certification equilibria. Further, our framework is general enough to also capture optimal mechanism design and optimal incentive design problems in sequential settings.

Summary of Our ResultsA key insight that underpins our results is that computing _optimal_ equilibria in multi-player extensive-form games can be cast via a Lagrangian relaxation as a two-player zero-sum extensive-form game. This unlocks a rich technology, both theoretical and experimental, developed for computing minimax equilibria for the more challenging--and much less understood--problem of computing optimal equilibria. In particular, building on the framework of Zhang and Sandholm [100], our reduction lends itself to mechanism design and information design, as well as an entire hierarchy of equilibrium concepts, including _normal-form coarse correlated equilibria (NFCCE)_[79], _extensive-form coarse correlated equilibria (EFCCE)_[31], _extensive-form correlated equilibria (EFCE)_[98], _communication equilibria (COMM)_[36], and _certification equilibria (CERT)_[37]. In fact, for communication and certification equilibria, our framework leads to the first learning-based algorithms for computing them, addressing a question left open by Zhang and Sandholm [100] (_cf._[40], discussed in Appendix B).

We thus focus on computing an optimal equilibrium by employing regret minimization techniques in order to solve the induced bilinear saddle-point problem. Such considerations are motivated in part by the remarkable success of no-regret algorithms for computing minimax equilibria in large two-player zero-sum games (_e.g._, see [10, 11]), which we endeavor to transfer to the problem of computing optimal equilibria in multi-player games.

In this context, we show that employing standard regret minimizers, such as online mirror descent [91] or counterfactual regret minimization [106], leads to a rate of convergence of \(T^{-1/4}\) to optimal equilibria by appropriately tuning the magnitude of the Lagrange multipliers (Corollary 3.3). We also leverage the technique of _optimism_, pioneered by Chiang et al. [18], Rakhlin and Sridharan [87] and Syrgkanis et al. [94], to obtain an accelerated \(T^{-1/2}\) rate of convergence (Corollary 3.4). These are the first learning dynamics that (provably) converge to optimal equilibria. Our bilinear formulation also allows us to obtain _last-iterate_ convergence to optimal equilibria via optimistic gradient descent/ascent (Theorem 3.5), instead of the time-average guarantees traditionally derived within the no-regret framework. As such, we bypass known barriers in the traditional learning paradigm by incorporating an additional player, a _mediator_, into the learning process. Furthermore, we also study an alternative Lagrangian relaxation which, unlike our earlier approach, consists of solving a sequence of zero-sum games (_cf._[30]). While the latter approach is less natural, we find that it is preferable when used in conjunction with deep RL solvers since it obviates the need for solving games with large reward ranges--a byproduct of employing the natural Lagrangian relaxation.

Experimental resultsWe demonstrate the practical scalability of our approach for computing optimal equilibria and mechanisms. First, we obtain state-of-the-art performance in a suite of \(23\) different benchmark game instances for seven different equilibrium concepts. Our algorithm significantly outperforms existing LP-based methods, typically by more than one order of magnitude. We also use our algorithm to derive an optimal mechanism for a sequential auction design problem, and we demonstrate that our approach is naturally amenable to modern deep RL techniques.

### Related work

In this subsection, we highlight prior research that closely relates to our work. Additional related work is included in Appendix B.

A key reference point is the recent paper of Zhang and Sandholm [100], which presented a unifying framework that enables the computation via linear programming of various mediator-based equilibrium concepts in extensive-form games, including NFCCE, EFCCE, EFCE, COMM, and CERT.2 Perhaps surprisingly, Zhang et al. [101] demonstrated that computing optimal communication and certification equilibria is possible in time polynomial in the description of the game, establishing a stark dichotomy between the other equilibrium concepts--namely, NFCCE, EFCE, and EFCCE--for which the corresponding problem is NP-hard [98]. In particular, for the latter notions intractability turns out to be driven by the imperfect recall of the mediator [101]. Although imperfect recall induces a computationally hard problem in general from the side of the mediator [19, 59], positive parameterized results have been documented recently in the literature [103].

Footnote 2: Notably missing from this list is the _normal-form correlated equilibrium (NFCE)_, the complexity status of which (in extensive-form games) is a long-standing open problem.

Our work significantly departs from the framework of Zhang and Sandholm [100] in that we follow a learning-based approach, which has proven to be a particularly favorable avenue in practice; _e.g._, we refer to [26, 16, 78, 77, 104] for such approaches in the context of computing EFCE. Further, beyond the tabular setting, learning-based frameworks are amenable to modern deep reinforcement learning methods (see [70, 71, 62, 69, 51, 76, 57, 13, 52, 73, 86, 105, 42], and references therein). Most of those techniques have been developed to solve two-player zero-sum games, which provides another crucial motivation for our main reduction. We demonstrate this experimentally in large games in Section 4. For multi-player games, Marris et al. [70] developed a scalable algorithm based on _policy space response oracles (PSRO)_[62] (a deep-reinforcement-learning-based double-oracle technique) that converges to NFC(C)E, but it does not find an optimal equilibrium.

Our research also relates to computational approaches to static auction and mechanism design through deep learning [27, 86]. In particular, similarly to the present paper, Dutting et al. [27] study a Lagrangian relaxation of mechanism design problems. Our approach is significantly more general in that we cover both static and _sequential_ auctions, as well as general extensive-form games. Further, as a follow-up, Rahme et al. [86] frame the Lagrangian relaxation as a two-player game, which, however, is not zero-sum, thereby not enabling leveraging the tools known for solving zero-sum games. Finally, in a companion paper [102], we show how the framework developed in this work can be used to _steer_ no-regret learners to optimal equilibria via nonnegative vanishing payments.

## 2 Preliminaries

We adopt the general framework of _mediator-augmented games_ of Zhang and Sandholm [100] to define our class of instances. At a high level, a mediator-augmented game explicitly incorporates an additional player, the _mediator_, who can exchange messages with the players and issue action recommendations; different assumptions on the power of the mediator and the players' strategy sets induce different equilibrium concepts, as we clarify for completeness in Appendix A.

**Definition 2.1**.: A _mediator-augmented, extensive-form game_\(\Gamma\) has the following components:

1. a set of players, identified with the set of integers \(\llbracket n\rrbracket:=\{1,\ldots,n\}\). We will use \(-i\), for \(i\in\llbracket n\rrbracket\), to denote all players except \(i\);
2. a directed tree \(H\) of _histories_ or _nodes_, whose root is denoted \(\varnothing\). The edges of \(H\) are labeled with _actions_. The set of actions legal at \(h\) is denoted \(A_{h}\). Leaf nodes of \(H\) are called _terminal_, and the set of such leaves is denoted by \(Z\);3. a partition \(H\setminus Z=H_{\textbf{C}}\sqcup H_{0}\sqcup H_{1}\sqcup\cdots\sqcup H_{n}\), where \(H_{i}\) is the set of nodes at which \(i\) takes an action, and **C** and \(0\) denote chance and the mediator, respectively;
4. for each agent3\(i\in[\![n]\!]\cup\{0\}\), a partition \(\mathcal{I}_{i}\) of \(i\)'s decision nodes \(H_{i}\) into _information sets_. Every node in a given information set \(I\) must have the same set of legal actions, denoted by \(A_{I}\); 3. for each agent \(i\), a _utility function_\(u_{i}:Z\rightarrow\mathbb{R}\); and Footnote 3: We will use _agent_ to mean either a player or the mediator.
5. for each chance node \(h\in H_{\textbf{C}}\), a fixed probability distribution \(c(\cdot\,|h)\) over \(A_{h}\).

To further clarify this definition, in Appendix A we provide two concrete illustrative examples: a single-item auction and a welfare-optimal correlated equilibrium in normal-form games.

At a node \(h\in H\), the _sequence_\(\sigma_{i}(h)\) of an agent \(i\) is the set of all information sets encountered by agent \(i\), and the actions played at such information sets, along the \(\varnothing\to h\) path, excluding at \(h\) itself. An agent has _perfect recall_ if \(\sigma_{i}(h)=\sigma_{i}(h^{\prime})\) for all \(h,h^{\prime}\) in the same infoset. We will use \(\Sigma_{i}:=\{\sigma_{i}(z):z\in Z\}\) to denote the set of all sequences of player \(i\) that correspond to terminal nodes. We will assume that all _players_ have perfect recall, though the _mediator_ may not.4

Footnote 4: Following the framework of Zhang and Sandholm [100], allowing the mediator to have imperfect recall will allow us to automatically capture optimal correlation.

A _pure strategy_ of agent \(i\) is a choice of one action in \(A_{I}\) for each information set \(I\in\mathcal{I}_{i}\). The _sequence form_ of a pure strategy is the vector \(\bm{x}_{i}\in\{0,1\}^{\Sigma_{i}}\) given by \(\bm{x}_{i}[\sigma]=1\) if and only if \(i\) plays every action on the path from the root to sequence \(\sigma\in\Sigma_{i}\). We will use the shorthand \(\bm{x}_{i}[z]=\bm{x}_{i}[\sigma_{i}(z)]\). A _mixed strategy_ is a distribution over pure strategies, and the sequence form of a mixed strategy is the corresponding convex combination \(\bm{x}_{i}\in[0,1]^{\Sigma_{i}}\). We will use \(X_{i}\) to denote the polytope of sequence-form mixed strategies of player \(i\), and use \(\Xi\) to denote the polytope of sequence-form mixed strategies of the mediator.

For a fixed \(\bm{\mu}\in\Xi\), we will say that \((\bm{\mu},\bm{x})\) is an _equilibrium_ of \(\Gamma\) if, for each _player_\(i\), \(\bm{x}_{i}\) is a best response to \((\bm{\mu},\bm{x}_{-i})\), that is, \(\max_{\bm{x}_{i}^{\prime}\in X_{i}}u_{i}(\bm{\mu},\bm{x}_{i}^{\prime},\bm{x}_ {-i})\leq u_{i}(\bm{\mu},\bm{x}_{i},\bm{x}_{-i})\). We do _not_ require that the mediator's strategy \(\bm{\mu}\) is a best response. As such, the mediator has the power to commit to its strategy. The goal in this paper will generally be to reach an _optimal (Stackelberg) equilibrium_, that is, an equilibrium \((\bm{\mu},\bm{x})\) maximizing the mediator utility \(u_{0}(\bm{\mu},\bm{x})\). We will use \(u_{0}^{*}\) to denote the value for the mediator in an optimal equilibrium.

Revelation principleThe _revelation principle_ allows us, without loss of generality, to restrict our attention to equilibria where each player is playing some fixed pure strategy \(\bm{d}_{i}\in X_{i}\).

**Definition 2.2**.: The game \(\Gamma\) satisfies the _revelation principle_ if there exists a _direct_ pure strategy profile \(\bm{d}=(\bm{d}_{1},\ldots,\bm{d}_{n})\) for the players such that, for all strategy profiles \((\bm{\mu},\bm{x})\) for all players including the mediator, there exists a mediator strategy \(\bm{\mu}^{\prime}\in\Xi\) and functions \(f_{i}:X_{i}\to X_{i}\) for each player \(i\) such that:

1. \(f_{i}(\bm{d}_{i})=\bm{x}_{i}\), and
2. \(u_{j}(\bm{\mu}^{\prime},\bm{x}_{i}^{\prime},\bm{d}_{-i})=u_{j}(\bm{\mu},f_{i}( \bm{x}_{i}^{\prime}),\bm{x}_{-i})\) for all \(\bm{x}_{i}^{\prime}\in X_{i}\), and _agents_\(j\in[\![n]\!]\cup\{0\}\).

The function \(f_{i}\) in the definition of the revelation principle can be seen as a _simulator_ for Player \(i\): it tells Player \(i\) that playing \(\bm{x}_{i}^{\prime}\) if other players play \((\bm{\mu},\bm{d}_{-i})\) would be equivalent, in terms of all the payoffs to all agents (including the mediator), to playing \(f(\bm{x}_{i}^{\prime})\) if other agents play \((\bm{\mu},\bm{x}_{-i})\). It follows immediately from the definition that if \((\bm{\mu},\bm{x})\) is an \(\varepsilon\)-equilibrium, then so is \((\bm{\mu}^{\prime},\bm{d})\)--that is, every equilibrium is payoff-equivalent to a direct equilibrium.

The revelation principle applies and covers many cases of interest in economics and game theory. For example, in (single-stage or dynamic) mechanism design, the direct strategy \(\bm{d}_{i}\) of each player is to report all information truthfully, and the revelation principle guarantees that for all non-truthful mechanisms \((\bm{\mu},\bm{x})\) there exists a truthful mechanism \((\bm{\mu}^{\prime},\bm{d})\) with the same utilities for all players.5 For correlated equilibrium, the direct strategy \(\bm{d}_{i}\) consists of obeying all (potentially randomized) recommendations that the mediator gives, and the revelation principle states that we can, without loss of generality, consider only correlated equilibria where the signals given to the players are what actions they should play. In both these cases (and indeed in general for the notions we consider in this paper), it is therefore trivial to specify the direct strategies \(\bm{d}\) without any computational overhead. Indeed, we will assume throughout the paper that the direct strategies \(\bm{d}\) are given. Further examples and discussion of this definition can be found in Appendix A.

Although the revelation principle is a very useful characterization of optimal equilibria, as long as we are given \(\bm{d}\), all of the results in this paper actually apply regardless of whether the revelation principle is satisfied: when it fails, our algorithms will simply yield an _optimal direct equilibrium_ which may not be an optimal equilibrium. Under the revelation principle, the problem of computing an optimal equilibrium can be expressed as follows:

\[\max_{\bm{\mu}\in\Xi}u_{0}(\bm{\mu},\bm{d})\quad\text{s.t.}\quad\max_{\bm{x}_{ i}\in X_{i}}u_{i}(\bm{\mu},\bm{x}_{i},\bm{d}_{-i})\leq u_{i}(\bm{\mu},\bm{d})\ \ \forall i\in\llbracket\!n\!\rrbracket.\]

The objective \(u_{0}(\bm{\mu},\bm{d})\) can be expressed as a linear expression \(\bm{c}^{\top}\bm{\mu}\), and \(u_{i}(\bm{\mu},\bm{x}_{i},\bm{d}_{-i})-u_{i}(\bm{\mu},\bm{d})\) can be expressed as a bilinear expression \(\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\). Thus, the above program can be rewritten as

\[\max_{\bm{\mu}\in\Xi}\quad\bm{c}^{\top}\bm{\mu}\quad\text{s.t.}\quad\max_{\bm {x}_{i}\in X_{i}}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq 0\ \ \forall i\in \llbracket\!n\!\rrbracket.\] (G)

Zhang and Sandholm [100] now proceed by taking the dual linear program of the inner maximization, which suffices to show that (G) can be solved using linear programming.6

Footnote 6: Computing optimal equilibria can be phrased as a linear program, and so in principle Adlerâ€™s reduction could also lead to an equivalent zero-sum game [2]. However, that reduction does not yield an _extensive-form_ zero-sum game, which is crucial for our purposes; see Section 3.

Finally, although our main focus in this paper is on games with discrete action sets, it is worth pointing out that some of our results readily apply to continuous games as well using, for example, the discretization approach of Kroer and Sandholm [60].

## 3 Lagrangian relaxations and a reduction to a zero-sum game

Our approach in this paper relies on Lagrangian relaxations of the linear program (G). In particular, in this section we introduce two different Lagrangian relaxations. The first one (Section 3.1) reduces computing an optimal equilibrium to solving a _single_ zero-sum game. We find that this approach performs exceptionally well in benchmark extensive-form games in the tabular regime, but it may struggle when used in conjunction with deep RL solvers since it increases significantly the range of the rewards. This shortcoming is addressed by our second method, introduced in Section 3.2, which instead solves a _sequence_ of suitable zero-sum games.

### "Direct" Lagrangian

Directly taking a Lagrangian relaxation of the LP (G) gives the following saddle-point problem:

\[\max_{\bm{\mu}\in\Xi}\quad\min_{\begin{subarray}{c}\lambda\in R_{ \geq 0},\\ \bm{x}_{i}\in X_{i}:i\in\llbracket\!n\!\rrbracket\end{subarray}}\bm{c}^{\top} \bm{\mu}-\lambda\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}.\] (L1)

We first point out that the above saddle-point optimization problem admits a solution \((\bm{\mu}^{*},\bm{x}^{*},\lambda^{*})\):

**Proposition 3.1**.: _The problem (L1) admits a finite saddle-point solution \((\bm{\mu}^{*},\bm{x}^{*},\lambda^{*})\). Moreover, for all fixed \(\lambda>\lambda^{*}\), the problems (L1) and (G) have the same value and same set of optimal solutions._

The proof is in Appendix C. We will call the smallest possible \(\lambda^{*}\) the _critical Lagrange multiplier_.

**Proposition 3.2**.: _For any fixed value \(\lambda\), the saddle-point problem (L1) can be expressed as a zero-sum extensive-form game._

Proof.: Consider the zero-sum extensive-form game \(\hat{\Gamma}\) between two players, the _mediator_ and the _deviator_, with the following structure:

1. Nature picks, with uniform probability, whether or not there is a deviator. If nature picks that there should be a deviator, then nature samples, also uniformly, a deviator \(i\in\llbracket\!n\!\rrbracket\). Nature's actions are revealed to the deviator, but kept private from the mediator.

2. The game \(\Gamma\) is played. All players, except \(i\) if nature picked a deviator, are constrained to according to \(\bm{d}_{i}\). The deviator plays on behalf of Player \(i\).
3. Upon reaching terminal node \(z\), there are two cases. If nature picked a deviator \(i\), the utility is \(-2\lambda n\cdot u_{i}(z)\). If nature did not pick a deviator, the utility is \(2u_{0}(z)+2\lambda\sum_{i=1}^{n}u_{i}(z)\).

The mediator's expected utility in this game is

\[u_{0}(\bm{\mu},\bm{d})-\lambda\sum_{i=1}^{n}\big{[}u_{i}(\bm{\mu},\bm{x}_{i}, \bm{d}_{-i})-u_{i}(\bm{\mu},\bm{d})\big{]}.\qed\]

This characterization enables us to exploit technology used for extensive-form zero-sum game solving to compute optimal equilibria for an entire hierarchy of equilibrium concepts (Appendix A).

We will next focus on the computational aspects of solving the induced saddle-point problem (L1) using regret minimization techniques. All of the omitted proofs are deferred to Appendices D and E.

The first challenge that arises in the solution of (L1) is that the domain of the minimizing player is unbounded--the Lagrange multiplier is allowed to take any nonnegative value. Nevertheless, we show in Theorem D.1 that it suffices to set the Lagrange multiplier to a fixed value (that may depend on the time horizon); appropriately setting that value will allow us to trade off between the equilibrium gap and the optimality gap. We combine this theorem with standard regret minimizers (such as variants of CFR employed in Section 4.1) to guarantee fast convergence to optimal equilibria.

**Corollary 3.3**.: _There exist regret minimization algorithms such that when employed in the saddle-point problem (L1), the average strategy of the mediator \(\bar{\bm{\mu}}\coloneqq\frac{1}{T}\sum_{t=1}^{T}\bm{\mu}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/4}\). Moreover, the per-iteration complexity is polynomial for communication and certification equilibria (under the nested range condition [100]), while for NFCCE, EFCCE and EFCE, implementing each iteration admits a fixed-parameter tractable algorithm._

Furthermore, we leverage the technique of _optimism_, pioneered by Chiang et al. [18], Rakhlin and Sridharan [87], Syrgkanis et al. [94], to obtain a faster rate of convergence.

**Corollary 3.4** (Improved rates via optimism).: _There exist regret minimization algorithms that guarantee that the average strategy of the mediator \(\bar{\bm{\mu}}\coloneqq\frac{1}{T}\sum_{t=1}^{T}\bm{\mu}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/2}\). The per-iteration complexity is analogous to Corollary 3.3._

While this rate is slower than the (near) \(T^{-1}\) rates known for converging to some of those equilibria [24, 34, 85, 3], Corollaries 3.3 and 3.4 additionally guarantee convergence to _optimal_ equilibria; improving the \(T^{-1/2}\) rate of Corollary 3.4 is an interesting direction for future research.

Last-iterate convergenceThe convergence results we have stated thus far apply for the _average_ strategy of the mediator--a typical feature of traditional guarantees in the no-regret framework. Nevertheless, an important advantage of our mediator-augmented formulation is that we can also guarantee _last-iterate convergence_ to optimal equilibria in general games. Indeed, this follows readily from our reduction to two-player zero-sum games, leading to the following guarantee.

**Theorem 3.5** (Last-iterate convergence to optimal equilibria in general games).: _There exist algorithms that guarantee that the last strategy of the mediator \(\bm{\mu}^{(T)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/4}\). The per-iteration complexity is analogous to Corollaries 3.3 and 3.4._

As such, our mediator-augmented paradigm bypasses known hardness results in the traditional learning paradigm (Proposition D.2) since iterate convergence is no longer tied to Nash equilibria.

### Thresholding and binary search

A significant weakness of the above Lagrangian is that the multiplier \(\lambda^{*}\) can be large. This means that, in practice, the zero-sum game that needs to be solved to compute an optimal equilibrium could have a large reward range. While this is not a problem for most tabular methods that can achieve high precision, more scalable methods based on reinforcement learning tend to be unable to solve games to the required precision. In this section, we will introduce another Lagrangian-based method for solving the program (G) that will not require solving games with large reward ranges.

Specifically, let \(\tau\in\mathbb{R}\) be a fixed threshold value, and consider the bilinear saddle-point problem

\[\max_{\bm{\mu}\in\mathbb{E}}\min_{\begin{subarray}{c}\bm{\lambda}\in\Delta^{n+1},\\ \bm{x}_{i}\in X_{i};i\in\llbracket n\rrbracket\end{subarray}}\bm{\lambda}_{0}( \bm{c}^{\top}\bm{\mu}-\tau)-\sum_{i=1}^{n}\bm{\lambda}_{i}\bm{\mu}^{\top}\bm{ \Lambda}_{i}\bm{x}_{i},\] (L2)

where \(\Delta^{k}:=\{\bm{\lambda}\in\mathbb{R}_{\geq 0}^{k}:\bm{1}^{\top}\bm{\lambda}=1\}\) is the probability simplex on \(k\) items. This Lagrangian was also stated--but not analyzed--by Farina et al. [30], in the special case of correlated equilibrium concepts (NFCCE, EFCCE, EFCE). Compared to that paper, ours contains a more complete analysis, and is general to more notions of equilibrium.

Like (L1), this Lagrangian is also a zero-sum game, but unlike (L1), the reward range in this Lagrangian is bounded by an absolute constant:

**Proposition 3.6**.: _Let \(\Gamma\) be a (mediator-augmented) game in which the reward for all agents is bounded in \([0,1]\). For any fixed \(\tau\in[0,1]\), the saddle-point problem (L2) can be expressed as a zero-sum extensive-form game whose reward is bounded in \([-2,2]\)._

Proof.: Consider the zero-sum extensive-form game \(\hat{\Gamma}\) between two players, the _mediator_ and the _deviator_, with the following structure:

1. The deviator picks an index \(i\in\llbracket n\rrbracket\cup\{0\}\).
2. If \(i\neq 0\), nature picks whether Player \(i\) can deviate, uniformly at random.
3. The game \(\Gamma\) is played. All players, except \(i\) if \(i\neq 0\) and nature selected that \(i\) can deviate, are constrained to play according to \(\bm{d}_{i}\). The deviator plays on behalf of Player \(i\).
4. Upon reaching terminal node \(z\), there are three cases. If nature picked \(i=0\), the utility is \(u_{0}(z)-\tau\). Otherwise, if nature picked that Player \(i\neq 0\) can deviate, the utility is \(-2u_{i}(z)\). Finally, if nature picked that Player \(i\neq 0\) cannot deviate, the utility is \(2u_{i}(z)\).

The mediator's expected utility in this game is exactly

\[\bm{\lambda}_{0}u_{0}(\bm{\mu},\bm{d})-\sum_{i=1}^{n}\bm{\lambda}_{i}[u_{i}( \bm{\mu},\bm{x}_{i},\bm{d}_{-i})-u_{i}(\bm{\mu},\bm{d})]\]

where \(\bm{\lambda}\in\Delta^{n+1}\) is the deviator's mixed strategy in the first step. 

The above observations suggest a binary-search-like algorithm for computing optimal equilibria; the pseudocode is given as Algorithm 1. The algorithm solves \(O(\log(1/\varepsilon))\) zero-sum games, each to precision \(\varepsilon\). Let \(v^{*}\) be the optimal value of (G). If \(\tau\leq v^{*}\), the value of (L2) is \(0\), and we will therefore never branch low, in turn implying that \(u\geq v^{*}\) and \(\ell\geq v^{*}-\varepsilon\). As a result, we have proven:

**Theorem 3.7**.: _Algorithm 1 returns an \(\varepsilon\)-approximate equilibrium \(\bm{\mu}\) whose value to the mediator is at least \(v^{*}-2\varepsilon\). If the underlying game solver used to solve (L2) runs in time \(f(\Gamma,\varepsilon)\), then Algorithm 1 runs in time \(O(f(\Gamma,\varepsilon)\log(1/\varepsilon))\)._

``` input: game \(\Gamma\) with mediator reward range \([0,1]\), target precision \(\varepsilon>0\) \(\ell\gets 0,u\gets 1\) while\(u-\ell>\varepsilon\)do \(\tau\leftarrow(\ell+u)/2\) run an algorithm to solve game (L2) until either (1) it finds a \(\bm{\mu}\) achieving value \(\geq-\varepsilon\) in (L2), or (2) it proves that the value of (L2) is \(<0\) ifcase (1) happenedthen\(\ell\leftarrow\tau\) else\(u\leftarrow\tau\) returnthe last \(\bm{\mu}\) found ```

**ALGORITHM 1**Pseudocode for binary search-based algorithm

The differences between the two Lagrangian formulations can be summarized as follows:1. Using (L1) requires only a single game solve, whereas using (L2) requires \(O(\log(1/\varepsilon))\) game solves.
2. Using (L2) requires only an \(O(\varepsilon)\)-approximate game solver to guarantee value \(v^{*}-\varepsilon\), whereas using (L1) would require an \(O(\varepsilon/\lambda^{*})\)-approximate game solver to guarantee the same, even assuming that the critical Lagrange multiplier \(\lambda^{*}\) in (L1) is known.

Which is preferred will therefore depend on the application. In practice, if the games are too large to be solved using tabular methods, one can use approximate game solvers based on deep reinforcement learning. In this setting, since reinforcement learning tends to be unable to achieve the high precision required to use (L1), using (L2) should generally be preferred. In Section 4, we back up these claims with concrete experiments.

## 4 Experimental evaluation

In this section, we demonstrate the practical scalability and flexibility of our approach, both for computing optimal equilibria in extensive-form games, and for designing optimal mechanisms in large-scale sequential auction design problems.

### Optimal equilibria in extensive-form games

We first extensively evaluate the empirical performance of our two-player zero-sum reduction (Section 3.1) for computing seven equilibrium solution concepts across 23 game instances; the results using the method of Section 3.2 are slightly inferior, and are included in Appendix H. The game instances we use are described in detail in Appendix F, and belong to following eight different classes of established parametric benchmark games, each identified with an alphabetical mnemonic: B - Battleship [30], D - Liar's dice [68], GL - Goofspiel [88], K - Kuhn poker [61], L - Leduc poker [93], RS - ridesharing game [101], S - Sheriff [30], TP - double dummy bridge game [101].

For each of the 23 games, we compare the runtime required by the linear programming method of Zhang and Sandholm [100] ('LP') and the runtime required by our learning dynamics in Section 3.1 ('Ours') for computing \(\varepsilon\)-optimal equilibrium points.

Table 1 shows experimental results for the case in which the threshold \(\varepsilon\) is set to be 1% of the payoff range of the game, and the objective function is set to be the maximum social welfare (sum of player utilities) for general-sum games, and the utility of Player 1 in zero-sum games. Each row corresponds to a game, whose identifier begins with the alphabetical mnemonic of the game class, and whose size in terms of number of nodes in the game trees is reported in the second column. The remaining columns compare, for each solution concept, the runtimes necessary to approximate the optimum equilibrium point according to that solution concept. Due to space constraints, only five out of the seven solution concepts (namely, NFCCE, EFCCE, EFCE, COMM, and CERT) are shown; data for the two remaining concepts (NFCCERT and CCERT) is given in Appendix G.

We remark that in Table 1, the column 'Ours' reports the minimum across the runtime across the different hyperparameters tried for the learning dynamics. Furthermore, for each run of the algorithms, the timeout was set at one hour. More details about the experimental setup are available in Appendix G, together with finer breakdowns of the runtimes.

We observe that our learning-based approach is faster--often by more than an order of magnitude--and more scalable than the linear program. Our additional experiments with different objective functions and values of \(\varepsilon\), available in Appendix G, confirm the finding. This shows the promise of our computational approach, and reinforces the conclusion that _learning dynamics are by far the most scalable technique available today to compute equilibrium points in large games_.

### Exact sequential auction design

Next, we use our approach to derive the optimal mechanism for a sequential auction design problem. In particular, we consider a two-round auction with two bidders, each starting with a budget of \(1\). The valuation for each item for each bidder is sampled uniformly at random from the set \(\{0,\nicefrac{{1}}{{4}},\nicefrac{{1}}{{2}},\nicefrac{{3}}{{4}},1\}\). We consider a mediator-augmented game in which the principal chooses an outcome (allocation and payment for each player) given their reports (bids). We use CFR+ [95] as learning algorithm and a fixed Lagrange multiplier \(\lambda\coloneqq 25\) to compute the optimal communication 

[MISSING_PAGE_FAIL:9]

First, to verify that the deep learning method is effective, we replicate the results of the tabular experiments in Section 4.2. We find that PSRO achieves the same best response values and optimal equilibrium value computed by the tabular experiment, up to a small error. These results give us confidence that our method is correct.

Second, to demonstrate scalability, we run our deep learning-based algorithm on a larger auction environment that would be too big to solve with tabular methods. In this environment, there are four rounds, and in each round the valuation of each player is sampled uniformly from \(\{0,0.1,0.2,0.3,0.4,0.5\}\). The starting budget of each player is, again, \(1\). We find that, like the smaller setting, the optimal revenue of the mediator is \(\approx 1.1\) (right-side of Figure 1). This revenue exceeds the revenue of every second-price auction (none of which have revenue greater than \(1\)).8

Footnote 8: We are inherently limited in this setting by the inexactness of best responses based on deep reinforcement learning; as such, it is possible that these values are not exact. However, because of the success of above tabular experiment replications, we believe that our results should be reasonably accurate.

## 5 Conclusions

We proposed a new paradigm of learning in games. It applies to mechanism design, information design, and solution concepts in multi-player extensive-form games such as correlated, communication, and certification equilibria. Leveraging a Lagrangian relaxation, our paradigm reduces the problem of computing optimal equilibria to determining minimax equilibria in zero-sum extensive-form games. We also demonstrated the scalability of our approach for _computing_ optimal equilibria by attaining state-of-the-art performance in benchmark tabular games, and by solving a sequential auction design problem using deep reinforcement learning.

Figure 1: Exploitability is measured by summing the best response for both bidders to the mechanism. Zero exploitability corresponds to incentive compatibility. In a sequential auction with budgets, our method is able to achieve higher revenue than second-price auctions and better incentive compatibility than a first-price auction.

## Acknowledgements

We are grateful to the anonymous NeurIPS reviewers for many helpful comments that helped improve the presentation of this paper. Tuomas Sandholm's work is supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award A240108S001. McAleer is funded by NSF grant #2127309 to the Computing Research Association for the CIFellows 2021 Project. The work of Prof. Gatti's research group is funded by the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2, Investment 1.3, Line on Artificial Intelligence). Conitzer thanks the Cooperative AI Foundation and Polaris Ventures (formerly the Center for Emerging Risk Research) for funding the Foundations of Cooperative AI Lab (FOCAL). Andy Haupt was supported by Effective Giving. We thank Dylan Hadfield-Menell for helpful conversations.

## References

* Abe et al. [2022] Kenshi Abe, Mitsuki Sakamoto, and Atsushi Iwasaki. Mutation-driven follow the regularized leader for last-iterate convergence in zero-sum games. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, volume 180, pages 1-10. PMLR, 2022.
* Adler [2013] Ilan Adler. The equivalence of linear programs and zero-sum games. _Int. J. Game Theory_, 42(1):165-177, 2013.
* Anagnostides et al. [2021] Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Noah Golowich, and Tuomas Sandholm. Near-optimal no-regret learning for correlated equilibria in multi-player general-sum games. In _Symposium on Theory of Computing (STOC)_, pages 736-749. ACM, 2021.
* Anagnostides et al. [2022] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In _International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 536-581. PMLR, 2022.
* Aumann [1974] Robert Aumann. Subjectivity and correlation in randomized strategies. _Journal of Mathematical Economics_, 1:67-96, 1974.
* Azizian et al. [2021] Waiss Azizian, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities. In _Conference on Learning Theory (COLT)_, volume 134 of _Proceedings of Machine Learning Research_, pages 326-358. PMLR, 2021.
* Babichenko and Rubinstein [2022] Yakov Babichenko and Aviad Rubinstein. Communication complexity of approximate nash equilibria. _Games and Economic Behavior_, 134:376-398, 2022.
* Bai et al. [2021] Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria in general-sum games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 25799-25811, 2021.
* Barman and Ligett [2015] Siddharth Barman and Katrina Ligett. Finding any nontrivial coarse correlated equilibrium is hard. In _ACM Conference on Economics and Computation (EC)_, pages 815-816. ACM, 2015.
* Bowling et al. [2015] Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold'em poker is solved. _Science_, 347(6218), January 2015.
* Brown and Sandholm [2018] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018.
* Brown and Sandholm [2019] Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret minimization. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2019.

* [13] Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning and search for imperfect-information games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [14] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning in multi-player games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [15] Modibo K. Camara, Jason D. Hartline, and Aleck C. Johnsen. Mechanisms for a no-regret agent: Beyond the common prior. In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 259-270. IEEE, 2020.
* [16] Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for extensive-form correlated equilibrium. _Conference on Neural Information Processing Systems (NeurIPS)_, 33:7722-7732, 2020.
* [17] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player Nash equilibria. _Journal of the ACM_, 2009.
* [18] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In _Conference on Learning Theory (COLT)_, pages 6-1, 2012.
* [19] Francis Chu and Joseph Halpern. On the NP-completeness of finding an optimal strategy in games with common payoffs. _International Journal of Game Theory_, 2001.
* [20] Vincent Conitzer and Dmytro Korzhyk. Commitment to correlated strategies. In _AAAI Conference on Artificial Intelligence (AAAI)_. AAAI Press, 2011.
* [21] Vincent Conitzer and Tuomas Sandholm. New complexity results about Nash equilibria. _Games and Economic Behavior_, 63(2):621-641, 2008. Early version in IJCAI-03.
* Leibniz-Zentrum fur Informatik, 2019.
* [23] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of computing a nash equilibrium. _SIAM Journal on Computing_, 39(1), 2009.
* [24] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 27604-27616, 2021.
* [25] Yuan Deng, Vahab Mirrokni, and Song Zuo. Non-clairvoyant dynamic mechanism design with budget constraints and beyond. _Available at SSRN 3383231_, 2019.
* [26] Miroslav Dudik and Geoffrey J Gordon. A sampling-based approach to computing equilibria in succinct extensive-form games. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 151-160, 2009.
* [27] Paul Dutting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath. Optimal auctions through deep learning. In _International Conference on Machine Learning_, pages 1706-1715. PMLR, 2019.
* [28] Gabriele Farina and Tuomas Sandholm. Polynomial-time computation of optimal correlated equilibria in two-player extensive-form games with public chance moves and beyond. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [29] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Regret circuits: Composability of regret minimizers. In _International Conference on Machine Learning_, pages 1863-1872, 2019.

* [30] Gabriele Farina, Chun Kai Ling, Fei Fang, and Tuomas Sandholm. Correlation in extensive-form games: Saddle-point formulation and benchmarks. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2019.
* [31] Gabriele Farina, Tommaso Bianchi, and Tuomas Sandholm. Coarse correlation in extensive-form games. In _AAAI Conference on Artificial Intelligence_, 2020.
* [32] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Better regularization for sequential decision spaces: Fast convergence rates for nash, correlated, and team equilibria. In _ACM Conference on Economics and Computation (EC)_, page 432, 2021.
* [33] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive Blackwell approachability: Connecting regret matching and mirror descent. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [34] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. Near-optimal no-regret learning dynamics for general convex games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [35] Tanner Fiez, Benjamin Chasnov, and Lillian J. Ratliff. Convergence of learning dynamics in stackelberg games. _CoRR_, abs/1906.01217, 2019.
* [36] Francoise Forges. An approach to communication equilibria. _Econometrica: Journal of the Econometric Society_, pages 1375-1385, 1986.
* [37] Francoise Forges and Frederic Koessler. Communication equilibria with partially verifiable types. _Journal of Mathematical Economics_, 41(7):793-811, 2005.
* [38] Yoav Freund and Robert Schapire. Adaptive game playing using multiplicative weights. _Games and Economic Behavior_, 29:79-103, 1999.
* [39] Drew Fudenberg and David Levine. _The Theory of Learning in Games_. MIT Press, 1998.
* [40] Kaito Fujii. Bayes correlated equilibria and no-regret dynamics. _CoRR_, abs/2304.05005, 2023.
* [41] Itzhak Gilboa and Eitan Zemel. Nash and correlated equilibria: Some complexity considerations. _Games and Economic Behavior_, 1:80-93, 1989.
* [42] Denizalp Goktas, David C. Parkes, Ian Gemp, Luke Marris, Georgios Piliouras, Romuald Elie, Guy Lever, and Andrea Tacchetti. Generative adversarial equilibrium solvers. _CoRR_, abs/2302.06607, 2023.
* [43] Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [44] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 151 of _Proceedings of Machine Learning Research_, pages 7865-7901. PMLR, 2022.
* [45] Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: \(O(1/K)\) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 151 of _Proceedings of Machine Learning Research_, pages 366-402. PMLR, 2022.
* [46] J Green and J-J Laffont. Characterization of satisfactory mechanisms for the revelation of preferences for public goods. _Econometrica_, 45:427-438, 1977.
* [47] Amy Greenwald and Keith Hall. Correlated Q-learning. In _International Conference on Machine Learning (ICML)_, pages 242-249, Washington, DC, USA, 2003.
* [48] Sergiu Hart and Yishay Mansour. How long to equilibrium? the communication complexity of uncoupled equilibrium procedures. _Games and Economic Behavior_, 69(1):107-126, 2010.

* [49] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. _Econometrica_, 68:1127-1150, 2000.
* [50] Sergiu Hart and Andreu Mas-Colell. Uncoupled dynamics do not lead to Nash equilibrium. _American Economic Review_, 93:1830-1836, 2003.
* [51] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-information games. _arXiv preprint arXiv:1603.01121_, 2016.
* [52] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In _International Conference on Machine Learning (ICML)_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 805-813. JMLR.org, 2015.
* [53] Michael D. Hirsch, Christos H. Papadimitriou, and Stephen A. Vavasis. Exponential lower bounds for finding brouwer fix points. _J. Complex._, 5(4):379-416, 1989.
* [54] Wan Huang and Bernhard von Stengel. Computing an extensive-form correlated equilibrium in polynomial time. In _International Workshop on Internet and Network Economics_, pages 506-513. Springer, 2008.
* [55] Dmitry Ivanov, Ilya Zisman, and Kirill Chernyshev. Mediated multi-agent reinforcement learning. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_, pages 49-57, 2023.
* [56] Albert Jiang and Kevin Leyton-Brown. Polynomial-time computation of exact correlated equilibrium in compact games. In _ACM Conference on Electronic Commerce (EC)_, 2011.
* [57] Peter H Jin, Sergey Levine, and Kurt Keutzer. Regret minimization for partially observable deep reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2018.
* [58] Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. _American Economic Review_, 101(6):2590-2615, 2011.
* [59] Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive form. _Games and Economic Behavior_, 4(4):528-552, October 1992.
* [60] Christian Kroer and Tuomas Sandholm. Discretization of continuous action spaces in extensive-form games. In _Autonomous Agents and Multi-Agent Systems_, pages 47-56. ACM, 2015.
* [61] H. W. Kuhn. A simplified two-person poker. In H. W. Kuhn and A. W. Tucker, editors, _Contributions to the Theory of Games_, volume 1 of _Annals of Mathematics Studies, 24_, pages 97-103. Princeton University Press, Princeton, New Jersey, 1950.
* [62] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 4190-4203, 2017.
* [63] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. _Advances in neural information processing systems_, 30, 2017.
* [64] Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 14293-14305, 2021.
* [65] Joshua Letchford and Vincent Conitzer. Computing optimal strategies to commit to in extensive-form games. In _ACM Conference on Electronic Commerce (EC)_, 2010.
* [66] Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In _Algorithmic Game Theory, Second International Symposium, SAGT 2009_, volume 5814 of _Lecture Notes in Computer Science_, pages 250-262. Springer, 2009.

* [67] Tianyi Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, and Michael I. Jordan. Finite-time last-iterate convergence for multi-agent learning in games. In _International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 6161-6171. PMLR, 2020.
* [68] Viliam Lisy, Marc Lanctot, and Michael H Bowling. Online monte carlo counterfactual regret minimization for search in imperfect information games. In _Autonomous Agents and Multi-Agent Systems_, 2015.
* [69] Siqi Liu, Marc Lanctot, Luke Marris, and Nicolas Heess. Simplex neural population learning: Any-mixture bayes-optimality in symmetric zero-sum games. In _International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 13793-13806. PMLR, 2022.
* [70] Luke Marris, Paul Muller, Marc Lanctot, Karl Tuyls, and Thore Graepel. Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021_, volume 139 of _Proceedings of Machine Learning Research_, pages 7480-7491. PMLR, 2021.
* [71] Luke Marris, Ian Gemp, Thomas Anthony, Andrea Tacchetti, Siqi Liu, and Karl Tuyls. Turbocharging solution concepts: Solving NEs, CEs and CCEs with neural equilibrium solvers. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [72] Stephen McAleer, John B. Lanier, Roy Fox, and Pierre Baldi. Pipeline PSRO: A scalable approach for finding approximate nash equilibria in large games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [73] Stephen McAleer, John B. Lanier, Kevin A. Wang, Pierre Baldi, and Roy Fox. XDO: A double oracle algorithm for extensive-form games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 23128-23139, 2021.
* [74] Jason Milionis, Christos Papadimitriou, Georgios Piliouras, and Kelly Spendlove. An impossibility theorem in game dynamics. _Proceedings of the National Academy of Sciences_, 120 (41), 2023.
* [75] Vahab Mirrokni, Renato Paes Leme, Pingzhong Tang, and Song Zuo. Non-clairvoyant dynamic mechanism design. _Econometrica_, 88(5):1939-1963, 2020.
* [76] Matej Moravcik, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. _Science_, May 2017.
* [77] Dustin Morrill, Ryan D'Orazio, Marc Lanctot, James R. Wright, Michael Bowling, and Amy R. Greenwald. Efficient deviation types and learning for hindsight rationality in extensive-form games. In _International Conference on Machine Learning (ICML)_, volume 139 of _Proceedings of Machine Learning Research_, pages 7818-7828. PMLR, 2021.
* [78] Dustin Morrill, Ryan D'Orazio, Reca Sarfati, Marc Lanctot, James R Wright, Amy R Greenwald, and Michael Bowling. Hindsight and sequential rationality of correlated play. In _AAAI Conference on Artificial Intelligence (AAAI)_, volume 35, pages 5584-5594, 2021.
* [79] H. Moulin and J.-P. Vial. Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon. _International Journal of Game Theory_, 7(3-4):201-221, 1978.
* [80] Roger Myerson. Optimal auction design. _Mathematics of Operation Research_, 6:58-73, 1981.
* [81] Roger B Myerson. Multistage games with communication. _Econometrica: Journal of the Econometric Society_, pages 323-358, 1986.
* [82] Christos Papadimitriou. Algorithms, games and the Internet. In _Symposium on Theory of Computing (STOC)_, pages 749-753, 2001.

* [83] Christos Papadimitriou and Tim Roughgarden. Computing equilibria in multi-player games. In _Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 82-91. SIAM, 2005.
* [84] Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit to. In _AAAI Conference on Artificial Intelligence (AAAI)_, pages 2149-2156. AAAI Press, 2019.
* [85] Georgios Piliouras, Ryann Sim, and Stratis Skoulakis. Beyond time-average convergence: Near-optimal uncoupled online learning via clairvoyant multiplicative weights update. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [86] Jad Rahme, Samy Jelassi, and S. Matthew Weinberg. Auction learning as a two-player game. In _9th International Conference on Learning Representations, ICLR 2021_. OpenReview.net, 2021.
* [87] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 3066-3074, 2013.
* [88] Sheldon M Ross. Goofspiel--the game of pure strategy. _Journal of Applied Probability_, 8(3):621-625, 1971.
* [89] Tim Roughgarden and Omri Weinstein. On the communication complexity of approximate fixed points. In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 229-238, 2016.
* [90] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [91] Shai Shalev-Shwartz. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2), 2012.
* [92] Maurice Sion. On general minimax theorems. _Pacific J. Math._, 8(4):171-176, 1958.
* [93] Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes' bluff: Opponent modelling in poker. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, July 2005.
* [94] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 2989-2997, 2015.
* [95] Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit Texas hold'em. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2015.
* [96] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* [97] John von Neumann. Zur Theorie der Gesellschaftssspiele. _Mathematische Annalen_, 100:295-320, 1928.
* [98] Bernhard von Stengel and Francoise Forges. Extensive-form correlated equilibrium: Definition and computational complexity. _Mathematics of Operations Research_, 33(4):1002-1022, 2008.
* [99] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In _9th International Conference on Learning Representations, ICLR 2021_. OpenReview.net, 2021.

* [100] Brian Hu Zhang and Tuomas Sandholm. Polynomial-time optimal equilibria with a mediator in extensive-form games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [101] Brian Hu Zhang, Gabriele Farina, Andrea Celli, and Tuomas Sandholm. Optimal correlated equilibria in general-sum extensive-form games: Fixed-parameter algorithms, hardness, and two-sided column-generation. In _ACM Conference on Economics and Computation (EC)_, pages 1119-1120. ACM, 2022.
* [102] Brian Hu Zhang, Gabriele Farina, Ioannis Anagnostides, Federico Cacciamani, Stephen Marcus McAleer, Andreas Alexander Haupt, Andrea Celli, Nicola Gatti, Vincent Conitzer, and Tuomas Sandholm. Steering no-regret learners to optimal equilibria. _CoRR_, abs/2306.05221, 2023.
* [103] Brian Hu Zhang, Gabriele Farina, and Tuomas Sandholm. Team belief DAG: generalizing the sequence form to team games for fast computation of correlated team max-min equilibria via regret minimization. In _International Conference on Machine Learning (ICML)_, volume 202 of _Proceedings of Machine Learning Research_, pages 40996-41018. PMLR, 2023.
* [104] Hugh Zhang. A simple adaptive procedure converging to forgiving correlated equilibria. _CoRR_, abs/2207.06548, 2022.
* [105] Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. _Science Advances_, 8(18):eabk2607, 2022.
* [106] Martin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization in games with incomplete information. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2007.

Illustrative examples of mediator-augmented games

In this section, we further clarify the framework of mediator-augmented games we operate in through a couple of examples. We begin by noting that the family of solution concepts for extensive-form games captured by this framework includes, but is not limited to, the following:

* normal-form coarse correlated equilibrium* [5, 79],
* extensive-form coarse correlated equilibrium* [31],
* extensive-form correlated equilibrium* [98],
* certification (under the _nested range condition_[46, 37]) [37, 100],
* communication equilibrium [81, 36],
* mechanism design for sequential settings, and
* information design/Bayesian persuasion for sequential settings [58].

We refer the interested reader to Zhang and Sandholm [100, Appendix G] for additional interesting concepts not mentioned above.

_Example A.1_ (Single-item auction).: Consider the single-good monopolist problem studied by Myerson [80]. Each player \(i\in\llbracket n\rrbracket\) has a valuation \(v_{i}\in V_{i}\). Agent valuations may be correlated, and distributed according to \(\mathcal{F}\in\Delta(V)\), where \(V:=V_{1}\times V_{2}\times\cdots\times V_{n}\). The mechanism selects a (potentially random) payment \(p\) and a winner \(i^{*}\in\llbracket n\rrbracket\). The agents' utilities are quasilinear: \(u_{i}(i^{*},p;v_{i})=v_{i}-p\) if \(i^{*}=i\) and \(0\) otherwise. The seller wishes to maximize expected payment from the agents. This has the following timeline.

1. The mechanism commits to a (potentially randomized) mapping \(\phi:\boldsymbol{v}=(v_{1},\ldots,v_{n})\mapsto(i^{*},p)\).
2. Nature samples valuations \(\boldsymbol{v}=(v_{1},\ldots,v_{n})\sim\mathcal{F}\).
3. Each player \(i\in\llbracket n\rrbracket\) privately observes her valuation \(v_{i}\), and then decides what valuation \(v_{i}^{\prime}\) to report to the mediator.
4. The winner and payment are selected according to \(\phi(\boldsymbol{v}^{\prime})\).
5. Player \(i^{*}\) gets utility \(v_{i^{*}}-p\), while all other players get \(0\). The mediator obtains utility \(u_{0}=p\).

In this extensive-form game, the primitives from our paper are:

* a (pure) mediator strategy \(\boldsymbol{\mu}\in\Xi\) is a mapping from valuation reports \(\boldsymbol{v}^{\prime}=(v_{1}^{\prime},\ldots,v_{n}^{\prime})\) to outcomes \((i^{*},p)\)--that is, mediator strategies are mechanisms, and mixed strategies are randomized mechanisms;
* a (pure) player strategy \(\boldsymbol{x}_{i}\in X_{i}\) for each player \(i\in\llbracket n\rrbracket\) is a mapping from \(V_{i}\) to \(V_{i}\) indicating what valuation Player \(i\) reports as a function of its valuation \(v_{i}\in V_{i}\);
* the direct (in mechanism design language, truthful) strategy \(\boldsymbol{d}_{i}\) for each player \(i\) is the identity map from \(V_{i}\) to \(V_{i}\). (Hence, in particular, \(\boldsymbol{d}_{i}\in X_{i}\) is a strategy of Player \(i\), so it makes sense, for example, to call \((\boldsymbol{\mu},\boldsymbol{d})=(\boldsymbol{\mu},\boldsymbol{d}_{1}, \ldots,\boldsymbol{d}_{n})\) a strategy profile.)

In particular, if profile \((\boldsymbol{\mu},\boldsymbol{d}_{1},\ldots,\boldsymbol{d}_{n})\) is such that each player \(i\) is playing a best response, then \(\boldsymbol{\mu}\) is a _truthful_ mechanism.

The conversion in Proposition 3.2 creates a zero-sum extensive-form game \(\hat{\Gamma}\), whose equilibria for the mediator (for sufficiently large \(\lambda\)) are precisely the revenue-maximizing mechanisms. \(\hat{\Gamma}\) has the following timeline:1. Nature picks, with equal probability, whether there is a deviator. If nature picks that there is a deviator, nature also selects which player \(i\in\llbracket n\rrbracket\) is represented by the deviator.
2. Nature samples valuations \(\boldsymbol{v}=(v_{1},\ldots,v_{n})\sim\mathcal{F}\).
3. If nature selected that there is a deviator, the deviator observes \(i\) and its valuation \(v_{i}\in V_{i}\), and selects a deviation \(v_{i}^{\prime}\in V_{i}\).
4. The mediator observes \((v_{i}^{\prime},\boldsymbol{v}_{-i})\) (_i.e._, all other players are assumed to have reported honestly) and selects a winner \(i^{*}\) and payment \(p\), as before.
5. There are now two cases. If nature selected at the root that there was to be a deviator, the utility for the mediator is \(-2\lambda nu_{i}(i^{*},p;v_{i})\). If nature selected at the root that there was to be no deviator, the utility for the mediator is \(2p+2\lambda\sum_{i=1}^{n}u_{i}(i^{*},p;v_{i})=2p+2\lambda(v_{i^{*}}-p)\).

As our second example, we show how the problem of computing a social-welfare-maximizing correlated equilbrium (CE) in a normal-form game can be captured using mediator-augmented games.

_Example A.2_ (Social welfare-optimal correlated equilibria in normal-form games).: Let \(A_{i}\) be the action set for each player \(i\in\llbracket n\rrbracket\) in the game, and let utility functions \(u_{i}:A\to\mathbb{R}\), where \(A:=A_{1}\times A_{2}\times\cdots\times A_{n}\). The social welfare is the function \(u_{0}:A\to\mathbb{R}\) given by \(u_{0}(\boldsymbol{a})\coloneqq\sum_{i=1}^{n}u_{i}(\boldsymbol{a})\). In the traditional formulation, a CE is a correlated distribution \(\boldsymbol{\mu}\) over \(A\). The elements \((a_{1},\ldots,a_{n})\) sampled from \(\boldsymbol{\mu}\) can be thought of as profiles of action recommendations for the players such that no player has any incentive to not follow the recommendation (obedience). This has the following well-known timeline.

1. At the beginning of the game, the mediator player chooses a profile of recommendations \(\boldsymbol{a}=(a_{1},\ldots,a_{n})\in A\).
2. Each player observes its recommendation \(a_{i}\) and chooses an action \(a_{i}^{\prime}\).
3. Each player gets utility \(u_{i}(\boldsymbol{a}^{\prime})\), and the mediator gets utility \(u_{0}(\boldsymbol{a}^{\prime})=\sum_{i=1}^{n}u_{i}(\boldsymbol{a}^{\prime})\).

In this game:

* mixed strategies \(\boldsymbol{\mu}\) for the mediator are distributions over \(A\), that is, they are correlated profiles;
* a (pure) strategy for player \(i\in\llbracket n\rrbracket\) is a mapping from \(A_{i}\) to \(A_{i}\), encoding the action player \(i\in\llbracket n\rrbracket\) will take upon receiving each recommendation;
* the direct strategy \(\boldsymbol{d}_{i}\) is again the identity map (_i.e._, each player selects as action what the mediator recommended to him/her).

In particular, if profile \((\boldsymbol{\mu},\boldsymbol{d}_{1},\ldots,\boldsymbol{d}_{n})\) is such that each player \(i\) is playing a best response, then \(\boldsymbol{\mu}\) is a CE.

Proposition 3.2 yields the following zero-sum game whose mediator equilibrium strategies (for sufficiently large \(\lambda\)) are precisely the welfare-optimal equilibria:

1. Nature picks, with equal probability, whether there is a deviator. If nature picks that there is a deviator, nature also selects which player \(i\in\llbracket n\rrbracket\) is represented by the deviator.
2. The mediator picks a pure strategy profile \(\boldsymbol{a}=(a_{1},\ldots,a_{n})\in A\).
3. If there is a deviator, the deviator observes \(i\in\llbracket n\rrbracket\) and the recommendation \(a_{i}\) and picks an action \(a_{i}^{\prime}\).
4. There are now two cases. If nature selected at the root that there was to be a deviator, the utility for the mediator is \(-2\lambda nu_{i}(a_{i}^{\prime},\boldsymbol{a}_{-i})\). If nature selected at the root that there was to be no deviator, the utility for the mediator is \(2u_{0}(\boldsymbol{a})+2\lambda\sum_{i=1}^{n}u_{i}(\boldsymbol{a})=2(1+ \lambda)u_{0}(\boldsymbol{a})\).

Further related work

In this section, we provide additional related work omitted from the main body.

We first elaborate further on prior work regarding the complexity of computing equilibria in games. Much attention has been focused on the complexity of computing just any one Nash equilibrium. This has been motivated in part by the idea that if even this is hard to compute, then this casts doubt on the concept of Nash equilibrium as a whole [23]; but the interest also stemmed from the fact that the complexity of the problem was open for a long time [82], and ended up being complete for an exotic complexity class [23, 17], whereas computing a Nash equilibrium that reaches a certain objective value is "simply" NP-complete [41, 21]. None of this, however, justifies settling for just any one equilibrium in practice. Moreover, for correlated equilibria and related concepts, the complexity considerations are different. While one (extensive-form) correlated equilibrium can be computed in polynomial time even for multi-player succinct games [83, 56, 54] (under the polynomial expectation property), computing one that maximizes some objective function is typically NP-hard [83, 98]. To make matters worse, even finding one that is _strictly_ better--in terms of social welfare--than the worst one is also computationally intractable [9]. Of course, our results do not contradict those lower bounds. For example, in multi-player normal-form games the strategy space of the mediator has an exponential description, thereby rendering all our algorithms exponential in the number of players. We stress again that while there exist algorithms that avoid this exponential dependence, they are not guaranteed to compute an optimal equilibrium, which is the main focus of this paper.

Moreover, in our formulation the mediator has the power to commit to a strategy. As such, our results also relate to the literature on learning and computing Stackelberg equilibria [8, 35, 66, 84, 20], as well as the work of Camara et al. [15] which casts mechanism design as a repeated interaction between a principal and an agent. Stackelberg equilibria in extensive-form games are, however, hard to find in general [65]. Our Stackelberg game has a much nicer form than general Stackelberg games--in particular, we know in advance what the equilibrium strategies will be for the followers (namely, the direct strategies, ). This observation is what allows the reduction to zero-sum games, sidestepping the need to use Stackleberg-specific technology or solvers and resulting in efficient algorithms.

In an independent and concurrent work, Fujii [40] provided independent learning dynamics converging to the set of communication equilibria in Bayesian games, but unlike our algorithm there are no guarantees for finding an optimal one. Also in independent and concurrent work, Ivanov et al. [55] develop similar Lagrangian-based dynamics for the equilibrium notion that, in the language of this paper and Zhang and Sandholm [100], is _coarse full-certification equilibrium_. Differing from ours, their paper does not present any theoretical guarantees (instead focusing on practical results).

## Appendix C Proof of Proposition 3.1

In this section, we provide the proof of Proposition 3.1, the statement of which is recalled below.

**Proposition 3.1**.: _The problem (1) admits a finite saddle-point solution \((\bm{\mu}^{*},\bm{x}^{*},\lambda^{*})\). Moreover, for all fixed \(\lambda>\lambda^{*}\), the problems (1) and (2) have the same value and same set of optimal solutions._

Proof.: Let \(v\) be the optimal value of (2). The Lagrangian of (2) is

\[\max_{\bm{\mu}\in\Xi}\min_{\begin{subarray}{c}\lambda_{i}\in\mathbb{R}_{ \geq 0},\\ \bm{x}_{i}\in X_{i}:i\in[\mathbb{n}]\end{subarray}}\bm{c}^{\top}\bm{\mu}-\sum _{i=1}^{n}\lambda_{i}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}.\]

Now, making the change of variables \(\bar{\bm{x}}_{i}:=\lambda_{i}\bm{x}_{i}\), the above problem is equivalent to

\[\max_{\bm{\mu}\in\Xi}\min_{\bar{\bm{x}}_{i}\in X_{i}:i\in[\mathbb{n}]}\bm{c}^ {\top}\bm{\mu}-\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bar{\bm{x}}_{i}.\] (1)

where \(\bar{X}_{i}\) is the conic hull of \(X_{i}\): \(\bar{X}_{i}:=\{\lambda_{i}\bm{x}_{i}:\bm{x}_{i}\in X_{i}\}\). Note that, when \(X_{i}\) is a polytope of the form \(X_{i}:=\{\mathbf{F}_{i}\bm{x}_{i}=\bm{f}_{i},\bm{x}_{i}\geq 0\}\), its conic hull can be expressed as \(\bar{X}_{i}=\{\mathbf{F}_{i}\bm{x}_{i}=\lambda_{i}\bm{f}_{i},\bm{x}_{i}\geq 0, \lambda_{i}\geq 0\}\). Thus, (1) is a bilinear saddle-point problem, where \(\Xi\) is compact and convex and \(\bar{X}_{i}\) is convex. Thus, Sion's minimax theorem [92] applies, and we have that the value of (1) is equal to the value of the problem

\[\min_{\bar{\bm{x}}_{i}\in X_{i}:i\in[\![n]\!]}\max_{\bm{\mu}\in\Xi} \quad\bm{c}^{\top}\bm{\mu}-\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bar{\bm{x }}_{i}.\] (2)

Since this is a linear program9 with a finite value, its optimum value must be achieved by some \(\bar{\bm{x}}:=(\bar{\bm{x}}_{1},\ldots,\bar{\bm{x}}_{n}):=(\lambda_{1}\bm{x}_{1 },\ldots,\lambda_{n}\bm{x}_{n})\). Let \(\lambda^{*}:=\max_{i}\lambda_{i}\). Using the fact that \(\bm{\mu}^{\top}\mathbf{A}_{i}\bm{d}_{i}=0\) for all \(\bm{\mu}\), the profile

Footnote 9: This holds by taking a dual of the inner minimization.

\[\bar{\bm{x}}^{\prime}:=(\lambda^{*}\bm{x}_{1}^{\prime},\ldots,\lambda^{*}\bm{ x}_{n}^{\prime})\quad\text{where}\quad\bm{x}_{i}^{\prime}=\bm{d}_{i}+\frac{ \lambda_{i}}{\lambda^{*}}(\bm{x}_{i}-\bm{d}_{i})\]

is also an optimal solution in (2). Therefore, for any \(\lambda\geq\lambda^{*}\), \(\bm{x}^{\prime}:=(\bm{x}_{1}^{\prime},\ldots,\bm{x}_{n}^{\prime})\) is an optimal solution for the minimizer in (1) that achieves the value of (G), so (G) and (1) have the same value.

Now take \(\lambda>\lambda^{*}\), and suppose for contradiction that (1) admits some optimal \(\bm{\mu}\in\Xi\) that is not optimal in (G). Then, either \(\bm{c}^{\top}\bm{\mu}<v\), or \(\bm{\mu}\) violates some constraint \(\max_{\bm{x}_{i}}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq 0\). The first case is impossible because then setting \(\bm{x}_{i}=\bm{d}_{i}\) for all \(i\) yields value less than \(v\) in (1). In the second case, since we know that (1) and (G) have the same value when \(\lambda=\lambda^{*}\), we have

\[\bm{c}^{\top}\bm{\mu}-\lambda\max_{\bm{x}\in X}\sum_{i=1}^{n}\bm{\mu}^{\top} \mathbf{A}_{i}\bm{x}_{i}<\bm{c}^{\top}\bm{\mu}-\lambda^{*}\max_{\bm{x}\in X} \sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq v.\qed\]

## Appendix D Fast computation of optimal equilibria via regret minimization

In this section, we focus on the computational aspects of solving the induced saddle-point problem (1) using regret minimization techniques. In particular, this section serves to elaborate on our results presented earlier in Section 3.1. All of the omitted proofs are deferred to Appendix E for the sake of exposition.

As we explained in Section 3.1, the first challenge that arises in the solution of (1) is that the domain of Player min is unbounded--the Lagrange multiplier is allowed to take any nonnegative value. Nevertheless, we show in the theorem below that it suffices to set the Lagrange multiplier to a fixed value (that may depend on the time horizon); we reiterate that appropriately setting that value will allow us to trade off between the equilibrium gap and the optimality gap. Before we proceed, we remark that the problem of Player min in (1) can be decomposed into the subproblems faced by each player separately, so that the regret of Player min can be cast as the sum of the players' regrets (see Corollary E.2); this justifies the notation \(\sum_{i=1}^{n}\operatorname{Reg}_{X_{i}}^{T}\) used for the regret of Player min below.

**Theorem D.1**.: _Suppose that Player max in the saddle-point problem (1) incurs regret \(\operatorname{Reg}_{\Xi}^{T}\) and Player min incurs regret \(\sum_{i=1}^{n}\operatorname{Reg}_{X_{i}}^{T}\) after \(T\in\mathbb{N}\) repetitions, for a fixed \(\lambda=\lambda(T)>0\). Then, the average mediator strategy \(\Xi\ni\bar{\bm{\mu}}:=\frac{1}{T}\sum_{t=1}^{T}\bm{\mu}^{(t)}\) satisfies the following:_

1. _For any strategy_ \(\bm{\mu}^{*}\in\Xi\) _such that_ \(\max_{i\in[\![n]\!]}\max_{\bm{x}_{i}^{*}\in X_{i}}(\bm{\mu}^{*})^{\top}\mathbf{ A}_{i}\bm{x}_{i}^{*}\leq 0\)_,_ \[\bm{c}^{\top}\bar{\bm{\mu}}\geq\bm{c}^{\top}\bm{\mu}^{*}-\frac{1}{T}\left( \operatorname{Reg}_{\Xi}^{T}+\sum_{i=1}^{n}\operatorname{Reg}_{X_{i}}^{T} \right);\]
2. _The equilibrium gap of_ \(\bar{\bm{\mu}}\) _decays with a rate of_ \(\lambda^{-1}\)_:_ \[\max_{i\in[\![n]\!]}\max_{\bm{x}_{i}^{*}\in X_{i}}\bar{\bm{\mu}}^{\top}\mathbf{ A}_{i}\bm{x}_{i}^{*}\leq\frac{\max_{\bm{\mu},\bm{\mu}^{\prime}\in\Xi}\bm{c}^{ \top}(\bm{\mu}-\bm{\mu}^{\prime})}{\lambda}+\frac{1}{\lambda T}\left( \operatorname{Reg}_{\Xi}^{T}+\sum_{i=1}^{n}\operatorname{Reg}_{X_{i}}^{T} \right).\]

As a result, if we can simultaneously guarantee that \(\lambda(T)\to+\infty\) and \(\frac{1}{T}\left(\operatorname{Reg}_{\Xi}^{T}+\sum_{i=1}^{n}\operatorname{Reg}_{X _{i}}^{T}\right)\to 0\), as \(T\to+\infty\), Theorem D.1 shows that both the optimality gap (Item 1) and the equilibrium gap (Item 2) converge to \(0\). We show that this is indeed possible in the sequel (Corollaries 3.3 and 3.4), obtaining favorable rates of convergence as well.

It is important to stress that while there exists a bounded critical Lagrange multiplier for our problem (Proposition 3.1), thereby obviating the need for truncating its value, such a bound is not necessarily polynomial. For example, halving the players' utilities while maintaining the utility of the mediator would require doubling the magnitude of the critical Lagrange multiplier.

Next, we combine Theorem D.1 with suitable regret minimization algorithms in order to guarantee fast convergence to optimal equilibria. Let us first focus on the side of Player min in (L1), which, as pointed out earlier, can be decomposed into subproblems corresponding to each player separately (Corollary E.2). Minimizing regret over the sequence-form polytope can be performed efficiently with a variety of techniques, which can be classified into two basic approaches. The first one is based on the standard online mirror descent algorithm (see, _e.g._, [91]), endowed with appropriate _distance generating functions (DGFs)_[32]. The alternative approach is based on regret decomposition, in the style of CFR [106, 29]. In particular, given that the players' observed utilities have range \(O(\lambda)\), the regret of each player under suitable learning algorithms will grow as \(O(\lambda\sqrt{T})\) (see Proposition E.1). Furthermore, efficiently minimizing regret from the side of the mediator depends on the equilibrium concept at hand. For NFCCE, EFCCE and EFCE, the imperfect recall of the mediator [101] induces a computationally hard problem [19], which nevertheless admits fixed-parameter tractable algorithms [103] (Proposition E.3). In contrast, for communication and certification equilibria the perfect recall of the mediator enables efficient computation for any extensive-form game. As a result, selecting a bound of \(\lambda\coloneqq T^{1/4}\) on the Lagrange multiplier, so as to optimally trade off Items 1 and 2 of Theorem D.1, leads to the following conclusion.

**Corollary 3.3**.: _There exist regret minimization algorithms such that when employed in the saddle-point problem (L1), the average strategy of the mediator \(\bar{\bm{\mu}}\coloneqq\frac{1}{T}\sum_{t=1}^{T}\bm{\mu}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/4}\). Moreover, the per-iteration complexity is polynomial for communication and certification equilibria (under the nested range condition [100]), while for NFCCE, EFCCE and EFCE, implementing each iteration admits a fixed-parameter tractable algorithm._

Furthermore, we leverage the technique of _optimism_, pioneered by Chiang et al. [18], Rakhlin and Sridharan [87], Syrgkanis et al. [94] in the context of learning in games, in order to obtain faster rates. In particular, using optimistic mirror descent we can guarantee that the sum of the agents' regrets in the saddle-point problem (L1) will now grow as \(O(\lambda)\) (Proposition E.4), instead of the previous bound \(O(\lambda\sqrt{T})\) obtained using vanilla mirror descent. Thus, letting \(\lambda=T^{1/2}\) leads to the following improved rate of convergence.

**Corollary 3.4** (Improved rates via optimism).: _There exist regret minimization algorithms that guarantee that the average strategy of the mediator \(\bar{\bm{\mu}}\coloneqq\frac{1}{T}\sum_{t=1}^{T}\bm{\mu}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/2}\). The per-iteration complexity is analogous to Corollary 3.3._

We reiterate that while this rate is slower than the (near) \(T^{-1}\) rates known for converging to some of those equilibria [24, 85, 3, 2], Corollaries 3.3 and 3.4 additionally guarantee convergence to _optimal_ equilibria; improving the \(T^{-1/2}\) rate of Corollary 3.4 is an interesting direction for the future.

Last-iterate convergenceThe results we have stated thus far apply for the _average_ strategy of the mediator--a typical feature of traditional guarantees in the no-regret framework. In contrast, there is a recent line of work that endeavors to recover _last-iterate_ guarantees as well [22, 45, 1, 14, 6, 99, 64, 43, 67, 44]. Yet, despite many efforts, the known last-iterate guarantees of no-regret learning algorithms apply only for restricted classes of games, such as two-player zero-sum games. There is an inherent reason for the limited scope of those results: last-iterate convergence is inherently tied to Nash equilibria, which in turn are hard to compute in general games [23, 17]--let alone computing an optimal one [41, 21]. Indeed, any given joint strategy profile of the players induces a product distribution, so iterate convergence requires--essentially by definition--at the very least computing an approximate Nash equilibrium.

**Proposition D.2** (Informal).: _Any independent learning dynamics (without a mediator) require superpolynomial time to guarantee \(\varepsilon\)-last-iterate convergence, for a sufficiently small \(\varepsilon=O(m^{-c})\), even for two-player \(m\)-action normal-form games, unless \(\mathsf{PPAD}\subseteq\mathsf{P}\)._

[MISSING_PAGE_FAIL:23]

in turn implying (3) since \(\sum_{i=1}^{n}\max_{\bm{x}_{i}^{*}\in X_{i}}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i} \bm{x}_{i}^{*}\geq\sum_{i=1}^{n}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i}\bm{d}_{i}=0\); (4) uses the notation \(\bm{\mu}^{*}\) to represent any equilibrium strategy optimizing the objective \(\bm{c}^{\top}\bm{\mu}\); and (5) follows from the fact that, by assumption, \(\bm{\mu}^{*}\) satisfies the equilibrium constraint: \(\max_{\bm{x}_{i}^{*}\in X_{i}}(\bm{\mu}^{*})^{\top}\mathbf{A}_{i}\bm{x}_{i}^{* }\leq 0\) for any player \(i\in\llbracket n\rrbracket\), as well as the nonnegativity of the Lagrange multiplier. This establishes Item 1 of the statement.

Next, we analyze the equilibrium gap of \(\bar{\bm{\mu}}\). Consider any mediator strategy \(\bm{\mu}\in\Xi\) such that \(\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq 0\) for any \(\bm{x}_{i}\in X_{i}\) and player \(i\in\llbracket n\rrbracket\). By (6),

\[\bm{c}^{\top}\bm{\mu}-\lambda\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bar{ \bm{x}}_{i}-\bm{c}^{\top}\bar{\bm{\mu}}+\lambda\sum_{i=1}^{n}\max_{\bm{x}_{i}^ {*}\in X_{i}}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i}\bm{x}_{i}^{*}\leq\frac{1}{T} \left(\sum_{i=1}^{n}\mathrm{Reg}_{X_{i}}^{T}+\mathrm{Reg}_{\Xi}^{T}\right).\] (7)

But, by the equilibrium constraint for \(\bm{\mu}\), it follows that \(\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq 0\) for any \(\bm{x}_{i}\in X_{i}\) and player \(i\in\llbracket n\rrbracket\), in turn implying that \(\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{x}_{i}\leq 0\). So, combining with (7),

\[\lambda\sum_{i=1}^{n}\max_{\bm{x}_{i}^{*}\in X_{i}}\bar{\bm{\mu}}^{\top} \mathbf{A}_{i}\bm{x}_{i}^{*}\leq\bm{c}^{\top}\bar{\bm{\mu}}-\bm{c}^{\top}\bm{ \mu}+\frac{1}{T}\left(\sum_{i=1}^{n}\mathrm{Reg}_{X_{i}}^{T}+\mathrm{Reg}_{ \Xi}^{T}\right).\] (8)

Finally, given that \(\max_{\bm{x}_{i}^{*}\in X_{i}}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i^{\prime}}\bm {x}_{i^{\prime}}^{*}\geq\bar{\bm{\mu}}^{\top}\mathbf{A}_{i^{\prime}}\bm{d}_{i^ {\prime}}=0\) for any player \(i^{\prime}\), it follows that

\[\sum_{i=1}^{n}\max_{\bm{x}_{i}^{*}\in X_{i}}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i }\bm{x}_{i}^{*}\geq\max_{i\in\llbracket n\rrbracket}\max_{\bm{x}_{i}^{*}\in X_{ i}}\bar{\bm{\mu}}^{\top}\mathbf{A}_{i}\bm{x}_{i}^{*},\]

and (8) implies Item 2 of the statement. 

Bounding the regret of the playersTo instantiate Theorem D.1 for our problem, we first bound the regret of Player min in (1) in terms of the magnitude of the Lagrange multiplier. As we explained in Appendix D, the regret minimization problem faced by Player min can be decomposed into subproblems over the sequence-form polytope, one for each player. To keep the exposition self-contained, let us first recall the standard regret guarantee of CFR under the sequence-form polytope.

**Proposition E.1** ([106]).: _Let \(\mathrm{Reg}_{X_{i}}^{T}\) be the regret cumulated by CFR [106] over the sequence-form polytope \(X_{i}\). Then, for any \(T\in\mathbb{N}\),_

\[\mathrm{Reg}_{X_{i}}^{T}\leq CD_{i}|\Sigma_{i}|\sqrt{T},\]

_where \(D_{i}>0\) is the range of utilities observed by player \(i\in\llbracket n\rrbracket\) and \(C>0\) is an absolute constant._

An analogous regret guarantee holds for online mirror descent [91]. As a result, given that the range of observed utilities for each player is \(O(\lambda)\), for a fixed Lagrange multiplier \(\lambda\), we arrive at the following result.

**Corollary E.2**.: _If all players employ CFR, the regret of Player min in (1) can be bounded as_

\[\mathrm{Reg}_{X}^{T}=\sum_{i=1}^{n}\mathrm{Reg}_{X_{i}}^{T}=O(\lambda\sqrt{T}).\]

Here, we used the simple fact that regret over a Cartesian product can be expressed as the sum of the regrets over each individual set [29].

Bounding the regret of the mediatorWe next turn our attention to the regret minimization problem faced by the mediator. The complexity of this problem depends on the underlying notion of equilibrium at hand. In particular, for the correlated equilibrium concepts studied in this paper--namely, NFCCE, EFCCE and EFCE, we employ the framework of _DAG-form sequential decision problem (DFSDP)_[103]. In particular, DFSDP is a sequential decision process over a DAG. We will denote by \(E\) the set of edges of the DAG, and by \(\mathcal{S}\) the set of its nodes; we refer to Zhang et al. [103] for the precise definitions. A crucial structural observation is that a DFSDP can be derived from the probability simplex after repeated Cartesian products and _scaled-extensions_ operations; suitable DGFs arising from such operations have been documented [32]. As such, we can use the following guarantee shown by Zhang et al. [103].

**Proposition E.3** ([103]).: _Let \(\mathrm{Reg}_{\Xi}^{T}\) be the regret cumulated by the regret minimization algorithm \(\mathcal{R}_{\Xi}\) used by the mediator (Player max in (1)) up to time \(T\in\mathbb{N}\). Then, if \(\mathcal{R}_{\Xi}\) is instantiated using CFR, or suitable variants thereof, \(\mathrm{Reg}_{\Xi}^{T}=O(|\mathcal{S}|\sqrt{T}D)\), where \(D\) is the range of the utilities observed by the mediator. Further, the iteration complexity is \(O(|E|)\)._

As a result, combining Theorem D.1 with Proposition E.3 and Corollary E.2, and setting the Lagrange multiplier \(\lambda\coloneqq T^{1/4}\), we establish the statement of Corollary 3.3.

Faster rates through optimismNext, to obtain Corollary 3.4, let us parameterize the regret of optimistic gradient descent in terms of the maximum utility, which can be directly extracted from the work of Rakhlin and Sridharan [87].

**Proposition E.4**.: _If both agents in the saddle-point problem (1) employ optimistic gradient descent with a sufficiently small learning rate \(\eta>0\), then the sum of their regrets is bounded by \(O(\lambda)\), for any fixed \(\lambda>0\)._

Proof Sketch.: By the RVU bound [94, 87], the sum of the agents' regrets can be bounded as \((\mathrm{diam}_{\Xi}^{2}+\mathrm{diam}_{X}^{2})/\eta\), for a sufficiently small \(\eta=O(1/\lambda)\), where \(\mathrm{diam}_{\Xi}\) and \(\mathrm{diam}_{X}\) denote the \(\ell_{2}\)-diameter of \(\Xi\) and \(X\), respectively. Thus, taking \(\eta=\Theta(1/\lambda)\) to be sufficiently small implies the statement. 

As a result, taking \(\lambda\coloneqq T^{1/2}\) and applying Theorem D.1 leads to the bound claimed in Corollary 3.4.

Last-iterate convergenceFinally, let us explain how known guarantees can be applied to establish Theorem 3.5. By applying [4], it follows that for a sufficiently small learning rate \(\eta=O(1/\lambda)\) there is an iterate of optimistic gradient descent with \(O\left(\frac{1}{\eta\sqrt{T}}\right)\) duality gap. Thus, setting \(\eta=\Theta(1/\lambda)\) to be sufficiently small we get that the duality gap is bounded by \(O\left(\frac{\lambda}{\sqrt{T}}\right)\). As a result, for \(\lambda\coloneqq T^{1/4}\) Theorem D.1 implies a rate of \(T^{-1/4}\), as claimed in Theorem 3.5. We remark that while the guarantee of Theorem D.1 has been expressed in terms of the sum of the agents' regrets, the conclusion readily applies for any pair of strategies \((\tilde{\bm{\mu}},\tilde{\bm{x}})\in\Xi\times X\) by replacing the term \(\mathrm{Reg}_{\Xi}^{T}+\sum_{i=1}^{n}\mathrm{Reg}_{X_{i}}\) with the duality gap of \((\tilde{\bm{\mu}},\tilde{\bm{x}})\) with respect to (1) (for the fixed value of \(\lambda\)). We further note that once the desirable duality gap \(O\left(\frac{1}{\eta\sqrt{T}}\right)\) has been reached, one can fix the players' strategies to obtain a last-iterate guarantee as well.

### An alternative approach

In this subsection, we highlight an alternative approach for solving the saddle-point (1) using regret minimization. In particular, we first observe that it can expressed as the saddle-point problem

\[\max_{\bm{\mu}\in\Xi}\min_{\tilde{x}_{i}\in\tilde{X}_{i}:i\in[\bm{n}]}\ \ \bm{c}^{\top}\bm{\mu}-\sum_{i=1}^{n}\bm{\mu}^{\top}\mathbf{A}_{i}\bm{\bar{x}}_{i},\] (9)

where \(\tilde{X}_{i}\coloneqq\{\lambda_{i}\bm{x}_{i}:\lambda_{i}\in[0,K],\bm{x}_{i} \in X_{i}\}\) is the _conic hull_ of \(X_{i}\) truncated to a sufficiently large parameter \(K>0\). Analogously to our approach in Appendix D, suitably tuning the value of \(K\) will allow us trade off between the optimality gap and the equilibrium gap. In this context, we point out below that how to construct a regret minimizer over a conic hull.

Regret minimization over conic hullSuppose that \(\mathcal{R}_{X_{i}}\) is a regret minimizer over \(X_{i}\) and \(\mathcal{R}_{+}\) is a regret minimizer over the interval \([0,K]\). Based on those two regret minimizers, Algorithm 2 shows how to construct a regret minimizer over the conic hull \(\tilde{X}_{i}\). More precisely, Algorithm 2 follows the convention that a generic regret minizer \(\mathcal{R}\) interacts with its environment via the following two subroutines:

* \(\mathcal{R}\).NextStrategy: \(\mathcal{R}\) returns the next strategy based on its internal state; and
* \(\mathcal{R}\).ObserveUtility\((\bm{u}^{(t)})\): \(\mathcal{R}\) receives as input from the environment a (compatible) utility vector \(\bm{u}^{(t)}\) at time \(t\in\mathbb{N}\).

The formal statement regarding the cumulated regret of Algorithm 2 below is cast in the framework of _regret circuits_[29].

**Proposition E.5** (Regret circuit for the conic hull).: _Suppose that \(\operatorname{Reg}_{X_{i}}^{T}\) and \(\operatorname{Reg}_{+}^{T}\) is the cumulative regret incurred by \(\mathcal{R}_{X_{i}}\) and \(\mathcal{R}_{+}\), respectively, up to a time horizon \(T\in\mathbb{N}\). Then, the regret \(\operatorname{Reg}_{X_{i}}^{T}\) of \(\mathcal{R}_{\bar{X}_{i}}\) constructed based on Algorithm 2 can be bounded as_

\[\operatorname{Reg}_{X_{i}}^{T}\leq K\max\{0,\operatorname{Reg}_{X_{i}}^{T}\}+ \operatorname{Reg}_{+}^{T}.\]

Proof.: By construction, we have that \(\operatorname{Reg}_{X_{i}}^{T}\) is equal to

\[\max_{\bar{\bm{x}}_{i}^{*}\in X_{i}}\left\{\sum_{t=1}^{T}\langle \bar{\bm{x}}_{i}^{*}-\bar{\bm{x}}_{i}^{(t)},\bm{u}^{(t)}\rangle\right\} =\max_{\lambda_{i}^{*}\bm{x}_{i}^{*}\in\bar{X}_{i}}\left\{\sum_{t =1}^{T}\langle\lambda_{i}^{*}\bm{x}_{i}^{*}-\lambda_{i}^{(t)}\bm{x}_{i}^{(t)}, \bm{u}_{i}^{(t)}\rangle\right\}\] \[=\max_{\lambda_{i}^{*}\bm{x}_{i}^{*}\in X_{i}}\left\{\lambda_{i}^ {*}\sum_{t=1}^{T}\langle\bm{x}_{i}^{*}-\bm{x}_{i}^{(t)},\bm{u}_{i}^{(t)}\rangle +(\lambda_{i}^{*}-\lambda_{i}^{(t)})(\bm{u}_{i}^{(t)})^{\top}\bm{x}_{i}^{(t)}\right\}\] \[\leq K\max\{0,\operatorname{Reg}_{X_{i}}^{T}\}+\operatorname{Reg} _{+}^{T},\]

where the last derivation uses that \(\lambda_{i}^{*}\in[0,K]\). 

As a result, by suitable instantiating \(\mathcal{R}_{X_{i}}\) and \(\mathcal{R}_{+}\) (_e.g._, using Proposition E.1), the regret circuit of Proposition E.5 enables us to construct a regret minimizer over \(\bar{X}_{i}\) with regret bounded as \(O(K\sqrt{T})\). In turn, this directly leads to a regret minimizer for Player min in (9) with regret bounded by \(O(K\sqrt{T})\). We further remark that Theorem D.1 can be readily cast in terms of the saddle-point problem (9) as well, parameterized now by \(K\) instead of \(\lambda\). As a result, convergence bounds such as Corollary 3.3 also apply to regret minimizers constructed via conic hulls.

## Appendix F Description of game instances

In this section, we provide a detailed description of the game instances used in our experiments in Section 4.1.

### Liar's dice (d), Goofspiel (gl), Kuhn poker (k), and Leduc poker (l)

Liar's diceAt the start of the game, each of the three players rolls a fair \(k\)-sided die privately. Then, the players take turns making claims about the outcome of their roll. The first player starts by stating any number from 1 to \(k\) and the minimum number of dice they believe are showing that value among all players. On their turn, each player has the option to make a higher claim or challenge the previous claim by calling the previous player a "liar." A claim is higher if the number rolled is higher or the number of dice showing that number is higher. If a player challenges the previous claim and the claim is found to be false, the challenger is rewarded +1 and the last bidder receives a penalty of -1. If the claim is true, the last bidder is rewarded +1, and the challenger receives -1. All other players receive 0 reward. We consider two instances of the game, one with \(k=2\) (D32) and one with \(k=3\) (D33).

GoofspielThis is a variant of Goofspiel with limited information. In this variation, in each turn the players do not reveal the cards that they have played. Instead, players show their cards to a neutral umpire, who then decides the winner of the round by determining which card is the highest. In the event of a tie, the umpire directs the players to divide the prize equally among the tied players, similar to the Goofspiel game. The instance GL3 which we employ has 3 players, 3 ranks, and imperfect information.

Kuhn pokerThree-player Kuhn Poker, an extension of the original two-player version proposed by Kuhn [61], is played with three players and \(r\) cards. Each player begins by paying one chip to the pot and receiving a single private card. The first player can check or bet (_i.e._, putting an additional chip in the pot). Then, the second player can check or bet after a first player's check, or fold/call the first player's bet. The third player can either check or bet if no previous bet was made, otherwise they must fold or call. At the showdown, the player with the highest card who has not folded wins all the chips in the pot. We use the instance K35 which has rank \(r=5\).

Leduc pokerIn our instances of the three-player Leduc poker the deck consists of \(s\) suits with \(r\) cards each. Our instances are parametric in the maximum number of bets \(b\), which in limit hold'em is not necessarely tied to the number of players. The maximum number of raise per betting round can be either 1, 2 or 3. At the beginning of the game, players each contribute one chip to the pot. The game proceeds with two rounds of betting. In the first round, each player is dealt a private card, and in the second round, a shared board card is revealed. The minimum raise is set at 2 chips in the first round and 4 chips in the second round. We denote by \(\mathrm{L3}brs\) an instance with three players with \(b\) bets per round, \(r\) ranks, and \(s\) suits. We employ the following five instances: L3132, L3133, L3151, L3223, L3523.

### Battleship game (B) and Sheriff game (S)

BattleshipThe game is a general-sum version of the classic game Battleship, where two players take turns placing ships of varying sizes and values on two separate grids of size \(h\times w\), and then take turns firing at their opponent. Ships which have been hit at all their tiles are considered destroyed. The game ends when one player loses all their ships, or after each player has fired \(r\) shots. Each player's payoff is determined by the sum of the value of the opponent's destroyed ships minus \(\gamma\geq 1\) times the number of their own lost ships. We denote by B\(phwr\) an instance with \(p\) players on a grid of size \(h\times w\), one unit-size ship for each player, and \(r\) rounds. We consider the following four instances: B2222, B2322, B2323, B2324.

SheriffThis game is a simplified version of the _Sheriff of Nottingham_ board game, which models the interaction between a _Smuggler_--who is trying to smuggle illegal items in their cargo--and the _Sheriff_--who's goal is stopping the Smuggler. First, the Smuggler has to decide the number \(n\in\{0,\dots,N\}\) of illegal items to load on the cargo. Then, the Sheriff decides whether to inspect the cargo. If they choose to inspect, and find illegal goods, the Smuggler has to pay \(p\cdot n\) to the Sheriff. Otherwise, the Sheriff has to compensate the Smuggler with a reward of \(s\). If the Sheriff decides not to inspect the cargo, the Sheriff's utility is 0, and the Smuggler's utility is \(v\cdot n\). After the Smuggler has loaded the cargo, and before the Sheriff decides whether to inspect, the Smuggler can try to bribe the Sheriff to avoid the inspection. In particular, they engage in \(r\) rounds of bargaining and, for each round \(i\), the Smuggler proposes a bribe \(b_{i}\in\{0,\dots,B\}\), and the Sheriff accepts or declines it. Only the proposal and response from the final round \(r\) are executed. If the Sheriff accepts a bribe \(b_{r}\) then they get \(b_{r}\), while the Smuggler's utility is \(vn-b_{r}\). Further details on the game can be found in Farina et al. [30]. An instance S\(pNBr\) has \(p\) players, \(N\) illegal items, a maximum bribe of \(B\), and \(r\) rounds of bargaining. The other parameters are \(v=5\), \(p=1\), \(s=1\) and they are fixed across all instances. We employ the following five instances: S2122, S2123, S2133, S2254, S2264.

### The double-dummy bridge endgame (TP)

The double-dummy bridge endgame is a benchmark introduced by Zhang et al. [101] which simulates a bridge endgame scenario. The game uses a fixed deck of playing cards that includes three ranks (2, 3, 4) of each of four suits (spades, hearts, diamonds, clubs). Spades are designated as the trump suit. There are four players involved: two defenders sitting across from each other, the dummy, and the declare. The dummy's actions will be controlled by the declare, so there are only three players actively participating. However, for clarity, we will refer to all four players throughout this section.

The entire deck of cards is randomly dealt to the four players. We study the version of the game that has perfect information, meaning that all players' cards are revealed to everyone, creating a game in which all information is public (_i.e._, a _double-dummy game_). The game is played in rounds called _tricks_. The player to the left of the declarer starts the first trick by playing a card. The suit of this card is known as the _lead suit_. Going in clockwise order, the other three players play a card from their hand. Players must play a card of the lead suit if they have one, otherwise, they can play any card. If a spade is played, the player with the highest spade wins the trick. Otherwise, the highest card of the lead suit wins the trick. The winner of each trick then leads the next one. At the end of the game, each player earns as many points as the number of tricks they won. In this adversarial team game, the two defenders are teammates and play against the declarer, who controls the dummy.

The specific instance that we use (_i.e._, TP3) has 3 ranks and perfect information. The dummy's hand is fixed as \(2\clubsuit 2\clubsuit 3\clubsuit\).

### Ridesharing game (Rs)

This benchmark was first introduced by Zhang et al. [101], and it models the interaction between two drivers competing to serve requests on a road network. The network is defined as an undirected graph \(G^{0}=(V^{0},E^{0})\), where each vertex \(v\in V^{0}\) corresponds to a ride request to be served. Each request has a reward in \(\mathbb{R}_{\geq 0}\), and each edge in the network has some cost. The first driver who arrives on node \(v\in V^{0}\) serves the corresponding ride, and receives the corresponding reward. Once a node has been served, it stays clean until the end of the game. The game terminates when all nodes have been cleared, or when a timeout is met (_i.e._, there's a fixed time horizon \(T\)). If the two drivers arrive simultaneously on the same vertex they both get reward 0. The final utility of each driver is computed as the sum of the rewards obtained from the beginning until the end of the game. The initial position of the two drivers is randomly selected at the beginning of the game. Finally, the two drivers can observe each other's position only when they are simultaneously on the same node, or they are in adjacent nodes.

Ridesharing games are particularly well-suited to study the computation of optimal equilibria because they are _not_ triangle-free [28].

SetupWe denote by \(\RS\,p\,i\,T\) a ridesharing instance with \(p\) drivers, network configuration \(i\), and horizon \(T\). Parameter \(i\in\{1,2\}\) specifies the graph configuration. We consider the two network configurations of Zhang et al. [101], their structure is reported in Figure 2. All edges are given unitary cost. We consider a total of four instances: \(\RS212\), \(\RS222\), \(\RS213\), \(\RS223\).

Figure 2: _Left_: configuration 1 (used for \(\RS212\), \(\RS213\)). _Right_: configuration 2 (used for \(\RS222\), \(\RS223\)). In both cases the position of the two drivers is randomly chosen at the beginning of the game, edge costs are unitary, and the reward for each node is indicated between curly brackets.

Additional experimental results

### Investigation of lower equilibrium approximation

In Table 2 we show results, using the same format as Table 1 shown in the body, for the case in which the approximation \(\varepsilon\) is set to be \(0.1\%\) of the payoff range of the game, as opposed to the \(1\%\) threshold of the body.

We observe that none of the results change qualitatively when this increased precision is considered.

### Game values and size of Mediator's strategy space

Tables 3 and 4 reports optimal equilibrium values for all games and all equilibrium concepts for which the LP algorithm was able to compute an exact value (We restrict to the cases solvable by LP because the Lagrangian relaxations only compute an \(\varepsilon\)-equilibrium, but the mediator objective in an \(\varepsilon\)-equilibrium could be arbitrarily far away from the mediator objective in an exact equilibrium). We hope that these will be good references for future researchers interested in this topic. Table 5 reports the size of the strategy space of the mediator player in the two-player zero-sum game that captures the computation of optimal equilibria, in terms of the number of decision points and edges. For correlated notions, this number may be exponential in the original game size; for communication and certification notions, it will always be polynomial.

\begin{table}
\begin{tabular}{l r|r r|r r|r r|r r|r r} \hline \multirow{2}{*}{**Game**} & \multirow{2}{*}{**\# Nodes**} & \multicolumn{2}{c|}{**NFCCE**} & \multicolumn{2}{c|}{**EFCCE**} & \multicolumn{2}{c|}{**EFCCE**} & \multicolumn{2}{c|}{**COMM**} & \multicolumn{2}{c}{**CERT**} \\  & & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{Ours} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{Ours} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{Ours} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{Ours} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{Ours} \\ \hline B2222 & \(1573\) & 0.008 & 0.008 & 0.008 & 0.008 & 0.008 & 0.008 & 3.008 & 1m 58 & 0.008 & 0.008 \\ B2322 & \(23839\) & 1.008 & 0.028 & 1.428 & 9.008 & 4.118 & timeout & 1m 308 & 2.008 & 2.828 \\ B2323 & \(254239\) & 6.008 & 0.668 & 1m 29.30 & 30.048 & 3m 40.18 & 288 & timeout & timeout & 39.008 & 1m 24s \\ B2324 & \(1420639\) & 41.008 & 5.25 & timeout & 5m 49s & timeout & timeout & timeout & timeout & timeout & timeout \\ \hline D32 & \(1017\) & 0.008 & 0.038 & 0.008 & 0.048 & 14.008 & 0.928 & 1.008 & 0.268 & 0.008 & 0.038 \\ D33 & \(27\,622\) & 3m 22s & 44.41s & timeout & 10m 27s & timeout & timeout & timeout & 16m 38 & 6.008 & 6.87s \\ \hline GL3 & \(7735\) & 0.008 & 0.068 & 1.008 & 0.078 & 0.008 & 0.068 & timeout & 36.838 & 0.008 & 0.118 \\ \hline K35 & \(1501\) & 55.008 & 2.468 & 53.008 & 3.058 & 1m 58 & 2.998 & 1.008 & 0.098 & 0.008 & 0.028 \\ \hline L3132 & \(8917\) & 28.008 & 2.138 & 1m 26s & 22.148 & 9m 41s & 26.688 & 13.008 & 15.418 & 1.008 & 0.628 \\ L3133 & \(12688\) & 45.008 & 2.838 & timeout & 35.868 & 26m 52s & 22.318 & 17.008 & 15.278 & 1.008 & 1.258 \\ L3151 & \(19\,981\) & timeout & 54.66 & timeout & timeout & timeout & timeout & timeout & 1m 15s & 2.008 & 0.91s \\ L3223 & \(15\,659\) & 5.008 & 1.738 & 1m 21s & 8.584 & 2m 33s & 20.448 & 26.008 & 1m 43s & 1.008 & 2.008 \\ L3523 & \(1299005\) & timeout & 4m 4s & timeout & timeout & timeout & timeout & timeout & timeout & timeout \\ \hline S2122 & \(705\) & 0.008 & 0.008 & 0.008 & 0.028 & 0.008 & 0.078 & 3.008 & 6.14s & 0.008 & 0.03s \\ S2123 & \(4269\) & 0.008 & 0.028 & 1.008 & 0.148 & 1.008 & 0.378 & 1m 51s & 10m 8s & 1.008 & 0.41s \\ S2133 & \(9648\) & 1.008 & 0.058 & 3.008 & 0.178 & 4.008 & 0.955 & timeout & timeout & timeout & 3.008 & 1.99s \\ S2254 & \(712552\) & \(20\,0\,\mathrm{m}\) & 32.14s & timeout & 42.65s & timeout & 9m 28s & timeout & timeout & timeout & timeout & 6m 50s \\ S2264 & \(1303\,177\) & 3m 4s & 57.76s & timeout & 1m 16s & timeout & timeout & timeout & timeout & timeout & timeout \\ \hline TP3 & \(910\,737\) & 1m 43s & 14.288 & timeout & 20.818 & timeout & 26.288 & timeout & timeout & timeout & timeout & 32.76s \\ \hline R2212 & \(598\) & 0.008 & 0.008 & 0.008 & 0.008 & 0.008 & 0.008 & 2.008 & 0.028 & 0.008 & 0.008 \\ R2222 & \(734\) & 0.008 & 0.018 & 0.008 & 0.018 & 0.008 & 0.028 & 3.008 & 0.008 & 0.008 \\ R2213 & \(6274\) & timeout & 43.46s & timeout & 45.008 & timeout & 2m 28s & 7m 30s & 27.19s & 0.008 & 0.03s \\ R2223 & \(6238\) & timeout & timeout & timeout & timeout & timeout & timeout & 9m 16s & 15.68s & 1.008 & 0.05s \\ \hline \end{tabular}
\end{table}
Table 2: Comparison between the linear-programming-based algorithm (â€˜LPâ€™) of Zhang and Sandholm [100] and our learning-based approach (â€˜Oursâ€™), for the problem of computing an approximate optimal equilibrium within tolerance \(\varepsilon\) set to \(0.1\%\) of the payoff range of the game.

\begin{table}
\begin{tabular}{l|r r r|r r r|r r r r r} \hline  & \multicolumn{3}{c|}{**COMM**} & \multicolumn{3}{c|}{**NFCCE**} & \multicolumn{3}{c|}{**CCERT**} & \multicolumn{3}{c}{**CERT**} & \multicolumn{3}{c}{**CERT**} \\
**Game** & **Pl.1** & **Pl.2** & **SW** & **Pl.1** & **Pl.2** & **SW** & **Pl.1** & **Pl.2** & **SW** & **Pl.1** & **Pl.2** & **SW** \\ \hline B2222 & \(-0.187\) & \(-0.562\) & \(-0.750\) & \(0.281\) & \(0.094\) & \(0.000\) & \(-0.043\) & \(-0.123\) & \(-0.317\) & \(-0.043\) & \(-0.123\) & \(-0.317\) \\ B2322 & â€” & â€” & â€” & \(0.250\) & \(0.125\) & \(0.000\) & \(0.000\) & \(-0.125\) & \(-0.375\) & \(0.000\) & \(-0.125\) & \(-0.375\) \\ B2324 & â€” & â€” & â€” & \(0.306\) & \(0.139\) & \(0.000\) & â€” & â€” & â€” & â€” & â€” \\ \hline S2122 & \(0.820\) & \(0.000\) & \(0.820\) & \(50.000\) & \(8.508\) & \(50.000\) & \(8.000\) & \(5.191\) & \(10.000\) & \(8.000\) & \(4.611\) & \(10.000\) \\ S21233 & \(0.820\) & \(0.000\) & \(0.820\) & \(50.000\) & \(8.508\) & \(50.000\) & \(8.000\) & \(5.191\) & \(10.000\) & \(8.000\) & \(4.611\) & \(10.000\) \\ S2133 & â€” & â€” & â€” & \(50.000\) & \(8.671\) & \(50.000\) & \(12.000\) & \(6.557\) & \(15.000\) & \(12.000\) & \(6.407\) & \(15.000\) \\ S2264 & â€” & â€” & â€” & \(100.000\) & \(17.284\) & \(100.000\) & \(20.000\) & \(12.190\) & \(25.000\) & â€” & â€” & â€” \\ S2264 & â€” & â€” & â€” & \(100.000\) & \(17.442\) & \(100.000\) & \(20.000\) & \(12.190\) & \(25.000\) & â€” & â€” & â€” \\ \hline U212 & \(3.184\) & \(3.143\) & \(6.173\) & \(3.184\) & \(3.173\) & \(6.173\) & \(3.184\) & \(3.159\) & \(6.173\) & \(3.184\) & \(3.143\) & \(6.173\) \\ U213 & \(5.160\) & \(3.171\) & \(9.592\) & \(5.316\) & \(5.429\) & \(6.622\) & \(5.204\) & \(5.298\) & \(9.622\) & \(5.196\) & \(5.276\) & \(9.622\) \\ U222 & \(4.023\) & \(3.812\) & \(7.594\) & \(4.023\) & \(3.930\) & \(7.594\) & \(4.023\) & \(3.905\) & \(7.594\) & \(4.023\) & \(3.839\) & \(7.594\) \\ U223 & \(6.537\) & \(6.326\) & \(11.464\) & \(6.867\) & \(6.783\) & \(11.516\) & \(6.631\) & \(6.582\) & \(11.513\) & \(6.576\) & \(6.398\) & \(11.485\) \\ \hline \hline  & **Pl.1** & **Pl.2** & **Pl.3** & **Pl.1** & **Pl.2** & **Pl.3** & **Pl.1** & **Pl.2** & **Pl.3** & **Pl.1** & **Pl.2** & **Pl.3** \\ \hline D32 & \(0.250\) & \(0.250\) & \(0.042\) & \(0.500\) & \(0.250\) & \(0.250\) & \(0.250\) & \(0.250\) & \(0.250\) & \(0.250\) & \(0.250\) & \(0.250\) \\ D33 & â€” & â€” & â€” & \(0.580\) & \(0.296\) & \(2.050\) & \(2.505\) & \(2.505\) & \(2.505\) & \(2.467\) & \(2.468\) & \(2.468\) \\ \hline U35 & \(0.022\) & \(0.050\) & \(0.088\) & \(0.092\) & \(0.106\) & \(0.169\) & \(0.092\) & \(0.090\) & \(0.169\) & \(0.086\) & \(0.090\) & \(0.169\) \\ \hline L3132 & \(0.646\) & \(0.618\) & \(0.723\) & \(0.853\) & \(0.779\) & \(0.802\) & \(0.853\) & \(0.779\) & \(0.802\) & \(0.853\) & \(0.779\) & \(0.802\) \\ L3133 & \(0.441\) & \(0.459\) & \(0.590\) & \(0.646\) & \(0.654\) & \(0.709\) & \(0.646\) & \(0.654\) & \(0.709\) & \(0.646\) & \(0.654\) & \(0.709\) \\ L3151 & â€” & â€” & â€” & \(0.179\) & \(0.197\) & \(0.222\) & \(0.179\) & \(0.182\) & \(0.222\) & \(0.171\) & \(0.182\) & \(0.222\) \\ L3223 & 1.011 & \(0.915\) & \(1.020\) & \(1.379\) & \(1.556\) & \(1.451\) & \(1.379\) & \(1.556\) & \(1.451\) & \(1.379\) & \(1.556\) & \(1.451\) \\ L3523 & â€” & â€” & â€” & \(2.000\) & \(2.000\) & \(2.000\) & â€” & â€” & â€” & â€” & â€” \\ \hline TP3 & \(-\) & â€” & \(1.739\) & \(1.506\) & \(1.083\) & â€” & â€” & â€” & â€” & â€” \\ \hline \hline \end{tabular}
\end{table}
Table 4: Optimal equilibrium value for communication and certification equilibrium concepts. â€˜Pl. 1â€™, â€˜Pl. 2â€™, â€˜Pl. 3â€™, and â€˜SWâ€™ have the same meaning as in the previous table.

\begin{table}
\begin{tabular}{l|r r r|r r r|r r r r} \hline  & \multicolumn{3}{c|}{**NFCCE**} & \multicolumn{3}{c|}{**EFCCE**} & \multicolumn{3}{c}{**EFCE**} \\
**Game** & **Pl.1** & **Pl.2** & **SW** & **Pl.1** & **Pl.2** & **SW** & **Pl.1** & **Pl.2** & **SW** \\ \hline B2222 & \(0.281\) & \(0.094\) & \(0.000\) & \(-0.027\) & \(-0.338\) & \(-0.525\) & \(-0.031\) & \(-0.338\) & \(-0.525\) \\ B2322 & \(0.181\) & \(0.097\) & \(0.000\) & \(-0.043\) & \(-0.123\) & \(-0.317\) & \(-0.045\) & \(-0.123\) & \(-0.317\) \\ B232

### Detailed breakdown by equilibrium and objective function (two-player games)

For each two-player game, we try three different objective functions: maximizing the utility of Player 1, maximizing the utility of Player 2, and maximizing social welfare. For each objective, we stop the optimization at the approximation level defined as 1% of the payoff range of the game.

We use online optimistic gradient descent to update the Lagrange multipliers of each player (see Appendix E.1). For each objective, we report the following information:

* The runtime of the linear-programming-based algorithm ('LP') of Zhang and Sandholm [100].
* The runtime of our algorithm where each agent (player or mediator) uses the Discounted CFR ('DCFR') algorithm set up with the hyperparameters recommended in the work by Brown and Sandholm [12]. In the table, we report the best runtime across all choices of the stepsize hyperparameter \(\eta\in\{0.01,0.1,1.0,10.0\}\) used in online optimistic gradient descent to update the Lagrange multipliers. The value of \(\eta\) that produces the reported runtime is noted in square brackets.
* The runtime of our algorithm where each agent (player or mediator) uses the Predictive CFR\({}^{+}\) ('PCFR\({}^{+}\)') algorithm of [33]. In the table, we report the best runtime across all choices of the stepsize hyperparameter \(\eta\in\{0.01,0.1,1.0,10.0\}\) used in online optimistic gradient descent to update the Lagrange multipliers. The value of \(\eta\) that produces the reported runtime is again noted in square brackets.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Game**} & \multicolumn{2}{c|}{NICCEI} & \multicolumn{2}{c|}{EICCE} & \multicolumn{2}{c|}{ECPE} & \multicolumn{2}{c|}{COMM} & \multicolumn{2}{c|}{NICCEI} & \multicolumn{2}{c|}{CICIR} \\  & **Dec.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% & **St.**\% \\ \hline

[MISSING_PAGE_POST]

[MISSING_PAGE_EMPTY:32]

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:35]

#### g.4.6 Results for CERT solution concept

### Results for CERT solution concept

### Results for CERT solution concept

In this section, we compare the performance of our "direct" Lagrangian approach against our binary search-based Lagrangian approach for computing several equilibrium concepts at the approximation level defined as 1% of the payoff range of the game.

For each solution concept, we identify the same three objectives as Appendix G (maximizing each player's individual utility, and maximizing the social welfare in our two-player general-sum games). For each objective, each of the following tables compares three runtimes:

* The time required by the linear program (column 'LP');
* The time required by the "direct" (non-binary search-based) Lagrangian approach, taking the fastest between the implementations using DCFR and PCFR\({}^{+}\) as the underlying no-regret algorithms (column 'Lagrangian').
* The time required by the binary search-based Lagrangian approach, taking the fastest between the implementations using DCFR and PCFR\({}^{+}\) as the underlying no-regret algorithms (column 'Bin.Search').

We observe that our two approaches behave similarly in small games. In larger games, especially with three players, the direct Lagrangian tends to be 2-4 times faster.

[MISSING_PAGE_EMPTY:37]

[MISSING_PAGE_EMPTY:38]

[MISSING_PAGE_FAIL:39]

[MISSING_PAGE_EMPTY:40]

[MISSING_PAGE_FAIL:41]