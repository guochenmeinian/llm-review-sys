# A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing

 Junren Chen

University of Hong Kong

chenjr58@connect.hku.hk

&Jonathan Scarlett

National University of Singapore

scarlett@comp.nus.edu.sg

&Michael K. Ng

Hong Kong Baptist University

michael-ng@hkbu.edu.hk

&Zhaoqiang Liu

UESTC

zqliu12@gmail.com

Corresponding authors.

###### Abstract

In generative compressed sensing (GCS), we want to recover a signal \(\bm{x^{*}}\in\mathbb{R}^{n}\) from \(m\) measurements (\(m\ll n\)) using a generative prior \(\bm{x^{*}}\in G(\mathbb{B}^{k}_{2}(r))\), where \(G\) is typically an \(L\)-Lipschitz continuous generative model and \(\mathbb{B}^{k}_{2}(r)\) represents the radius-\(r\)\(\ell_{2}\)-ball in \(\mathbb{R}^{k}\). Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed \(\bm{x^{*}}\) rather than for all \(\bm{x^{*}}\) simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, _all_\(\bm{x^{*}}\in G(\mathbb{B}^{k}_{2}(r))\) can be recovered up to an \(\ell_{2}\)-error at most \(\epsilon\) using roughly \(\tilde{O}(k/\epsilon^{2})\) samples, with omitted logarithmic factors typically being dominated by \(\log L\). Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.

## 1 Introduction

In compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically sparse signals) [5, 6, 15], it is standard to employ a random measurement ensemble, i.e., a random sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee involving a single draw of the measurement ensemble could be _non-uniform_ or _uniform_ -- the non-uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform one states that one realization of the measurements works simultaneously for all structured signals of interest. Uniformity is a highly desired property in CS, since in applications the measurement ensemble is typically fixed and should work for all signals [17]. Besides, the derivation of a uniform guarantee is often significantly harder than a non-uniform one, making uniformity an interesting theoretical problem in its own right.

Inspired by the tremendous success of deep generative models in different applications, it was recently proposed to use a generative prior to replace the commonly used sparse prior in CS [2], which led to numerical success such as a significant reduction of the measurement number. This new perspective for CS, which we call generative compressed sensing (GCS), has attracted a large volume of research interest, e.g., nonlinear GCS [29, 33, 45], MRI applications [24, 46], and information-theoretic bounds [27, 34], among others. This paper focuses on the uniform recovery problem for nonlinear GCS, which is formally stated below. Our main goal is to build a unified framework that can produce uniform recovery guarantees for various nonlinear measurement models.

**Problem:** Let \(\mathbb{B}^{k}_{2}(r)\) be the \(\ell_{2}\)-ball with radius \(r\) in \(\mathbb{R}^{k}\). Suppose that \(G\,:\,\mathbb{B}^{k}_{2}(r)\rightarrow\mathbb{R}^{n}\) is an \(L\)-Lipschitz continuous generative model, \(\bm{a}_{1},...,\bm{a}_{m}\in\mathbb{R}^{n}\) are the sensing vectors, \(\bm{x}^{\star}\in\mathcal{K}:=G(\mathbb{B}^{k}_{2}(r))\) is the underlying signal, and we have the observations \(y_{i}=f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\star}),\ i=1,\ldots,m\), where \(f_{1}(\cdot),\ldots,f_{m}(\cdot)\) are possibly unknown,2 possibly random non-linearities. Given a single realization of \(\{\bm{a}_{i},f_{i}\}_{i=1}^{m}\), under what conditions we can _uniformly_ recover all \(\bm{x}^{\star}\in\mathcal{K}\) from the corresponding \(\{\bm{a}_{i},y_{i}\}_{i=1}^{m}\) up to an \(\ell_{2}\)-norm error of \(\epsilon\)?

Footnote 2: In order to establish a unified framework, our recovery method (2.1) involves a parameter \(T\) that should be chosen according to \(f_{i}\). For the specific single index model with possibly unknown \(f_{i}\), we can follow prior works [33, 43] to assume that \(T\bm{x}^{\star}\in\mathcal{K}\), and recover \(\bm{x}^{\star}\) without using \(T\). See Remark 5 for more details.

### Related Work

We divide the related works into nonlinear CS (based on traditional structures like sparsity) and nonlinear GCS.

**Nonlinear CS:** Beyond the standard linear CS model where one observes \(y_{i}=\bm{a}_{i}^{\top}\bm{x}^{\star}\), recent years have witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is 1-bit CS that only retains the sign \(y_{i}=\operatorname{sign}(\bm{a}_{i}^{\top}\bm{x}^{\star})\)[3, 22, 41, 42]. Subsequent works also considered 1-bit CS with dithering \(y_{i}=\operatorname{sign}(\bm{a}_{i}^{\top}\bm{x}^{\star}+\tau_{i})\) to achieve norm reconstruction under sub-Gaussian sensing vectors [9, 14, 48]. Besides, the benefit of using differing was found in uniformly quantized CS with observation \(y_{i}=\mathcal{Q}_{\delta}(\bm{a}_{i}^{\top}\bm{x}^{\star}+\tau_{i})\), where \(\mathcal{Q}_{\delta}(\cdot)=\delta\big{(}[\frac{\cdot}{\delta}]+\frac{1}{2} \big{)}\) is the uniform quantizer with resolution \(\delta\)[48, 48, 8]. Moreover, the authors of [16, 43, 44] studied the more general single index model (SIM) where the observation \(y_{i}=f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\star})\) involves (possibly) unknown nonlinearity \(f_{i}\).

While the restricted isometry property (RIP) of the sensing matrix \(\bm{A}=[\bm{a}_{1},...,\bm{a}_{m}]^{\top}\) leads to uniform recovery in linear CS [4, 15, 49, 4], this is not true in nonlinear CS. In fact, many existing results are non-uniform [9, 16, 21, 41, 43, 44, 48], and some uniform guarantees can be found in [7, 8, 14, 41, 42, 41]. Most of these uniform guarantees suffer from a slower error rate.

The most relevant work to this paper is the recent work [17] that described a unified approach to uniform signal recovery for nonlinear CS. The authors of [17] showed that in the aforementioned models with \(k\)-sparse \(\bm{x}^{\star}\), a uniform \(\ell_{2}\)-norm recovery error of \(\epsilon\) could be achieved via generalized Lasso using roughly \(k/\epsilon^{4}\) measurements [17, Section 4]. In this work, we build a unified framework for uniform signal recovery in nonlinear GCS. To achieve a uniform \(\ell_{2}\)-norm error of \(\epsilon\) in the above models with the generative prior \(\bm{x}^{\star}\in G(\mathbb{B}^{k}_{2}(r))\), our framework only requires a number of samples proportional to \(k/\epsilon^{2}\). Unlike [17] that used the technical results [36] to bound the product process, we develop a concentration inequality that produces a tighter bound in the setting of generative prior, thus allowing us to derive a sharper uniform error rate.

**Nonlinear GCS:** Building on the seminal work by Bora _et al._[2], numerous works have investigated linear or nonlinear GCS [1, 11, 12, 19, 20, 23, 30, 25, 39, 40, 51], with a recent survey [47] providing a comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has been studied in [26, 31, 45], and generative priors have been used for SIM in [29, 32, 33]. In addition, score-based generative models have been applied to nonlinear CS in [10, 38].

The majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few exceptions [33, 45]. Specifically, under a generative prior, [33, Section 5] presented uniform recovery guarantees for SIM where \(y_{i}=f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\star})\) with deterministic Lipschitz \(f_{i}\) or \(f_{i}(x)=\operatorname{sign}(x)\). Their proof technique is based on the local embedding property developed in [31], which is a geometric property that is often problem-dependent and currently only known for 1-bit measurements and deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such geometric properties and yields a unified framework with more generality. Furthermore, [33] did not consider dithering, which limits their ability to estimate the norm of the signal.

The authors of [45] derived a uniform guarantee from dithered 1-bit measurements under bias-free ReLU neural network generative models, while we obtain a uniform guarantee with the comparable rate for more general Lipschitz generative models. Additionally, their recovery program differs from the generalized Lasso approach (_cf._ Section 2.1) used in our work. Specifically, they minimize an \(\ell_{2}\) loss with \(\|\bm{x}\|_{2}^{2}\) as the quadratic term, while generalized Lasso uses \(\|\bm{A}\bm{x}\|_{2}^{2}\) that depends on the sensing vector. As a result, our approach can be readily generalized to sensing vectors with an unknown covariance matrix [33, Section 4.2], unlike [45] that is restricted to isotropic sensing vectors. Under random dithering, while [45] only considered 1-bit measurements, we also present new results for uniformly quantized measurements (also referred to as multi-bit quantizer in some works [13]).

### Contributions

In this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We summarize the paper structure and our main contributions as follows:

* We present Theorem 1 as our main result in Section 2. Under rather general observation models that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all \(\bm{x}^{\bullet}\in G(\mathbb{B}_{2}^{k}(r))\) up to an \(\ell_{2}\)-norm error of \(\epsilon\) can be achieved using roughly \(O\big{(}\frac{k\log L}{e^{2}}\big{)}\) samples. Specifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering, Lipschitz-continuous SIM, and uniformly quantized GCS with dithering.
* We provide a proof sketch in Section 3. Without using the embedding property as in [33], we handle the discontinuous observation model by constructing a Lipschitz approximation. Compared to [17], we develop a new concentration inequality (Theorem 2) to derive tighter bounds for the product processes arising in the proof.

We also perform proof-of-concept experiments on the MNIST [28] and CelebA [35] datasets for various nonlinear models to demonstrate that by using a single realization of \(\{\bm{a}_{i},f_{i}\}_{i=1}^{m}\), we can obtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental results and detailed proofs are provided in the supplementary material.

### Notation

We use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For a vector \(\bm{x}\), we let \(\|\bm{x}\|_{q}\) (\(1\leq q\leq\infty\)) denote its \(\ell_{q}\)-norm. We use \(\mathbb{B}_{q}^{n}(r):=\{\bm{z}\in\mathbb{R}^{n}\,:\,\|\bm{z}\|_{q}\leq r\}\) to denote the \(\ell_{q}\) ball in \(\mathbb{R}^{n}\), and \((\mathbb{B}_{q}^{n}(r))^{c}\) represents its complement. The unit Euclidean sphere is denoted by \(\mathbb{S}^{n-1}:=\{\bm{x}\in\mathbb{R}^{n}\,:\,\|\bm{x}\|_{2}=1\}\). We use \(C,C_{i},c_{i},c\) to denote absolute constants whose values may differ from line to line. We write \(A=O(B)\) or \(A\lesssim B\) (resp. \(A=\Omega(B)\) or \(A\gtrsim B\)) if \(A\leq CB\) for some \(C\) (resp. \(A\geq cB\) for some \(c\)). We write \(A\asymp B\) if \(A=O(B)\) and \(A=\Omega(B)\) simultaneously hold. We sometimes use \(\tilde{O}(\cdot)\) to further hide logarithmic factors, where the hidden factors are typically dominated by \(\log L\) in GCS, or \(\log n\) in CS. We let \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) be the Gaussian distribution with mean \(\bm{\mu}\) and covariance matrix \(\bm{\Sigma}\). Given \(\mathcal{K}_{1},\mathcal{K}_{2}\subset\mathbb{R}^{n}\), \(\bm{a}\in\mathbb{R}^{n}\) and some \(a\in\mathbb{R}\), we define \(\mathcal{K}_{1}\pm\mathcal{K}_{2}:=\{\bm{x}_{1}\pm\bm{x}_{2}:\bm{x}_{1}\in \mathcal{K}_{1},\bm{x}_{2}\in\mathcal{K}_{2}\}\), \(\bm{a}+\mathcal{K}_{1}:=\{\bm{a}\}+\mathcal{K}_{1}\), and \(a\mathcal{K}_{1}:=\{a\bm{x}:\bm{x}\in\mathcal{K}_{1}\}\). We also adopt the conventions of \(a\wedge b=\min\{a,b\}\), and \(a\lor b=\max\{a,b\}\).

## 2 Main Results

We first give some preliminaries.

**Definition 1**.: _For a random variable \(X\), we define the sub-Gaussian norm \(\|X\|_{\psi_{2}}:=\inf\{t>0:\mathbb{E}\exp(X^{2}/t^{2})\leq 2\}\) and the sub-exponential norm \(\|X\|_{\psi_{1}}:=\inf\{t>0:\mathbb{E}\exp(|X|/t)\leq 2\}\). \(X\) is sub-Gaussian (resp. sub-exponential) if \(\|X\|_{\psi_{2}}<\infty\) (resp. \(\|X\|_{\psi_{1}}<\infty\)). For a random vector \(\bm{x}\in\mathbb{R}^{n}\), we let \(\|\bm{x}\|_{\psi_{2}}:=\sup_{\bm{v}\in\mathbb{S}^{n-1}}\|\bm{v}^{\top}\bm{x}\|_{ \psi_{2}}\)._

**Definition 2**.: _Let \(\mathcal{S}\) be a subset of \(\mathbb{R}^{n}\). We say that a subset \(\mathcal{S}_{0}\subset\mathcal{S}\) is an \(\eta\)-net of \(\mathcal{S}\) if every point in \(\mathcal{S}\) is at most \(\eta\) distance away from some point in \(\mathcal{S}_{0}\), i.e., \(\mathcal{S}\subset\mathcal{S}_{0}+\mathbb{B}_{2}^{n}(\eta)\). Given a radius \(\eta\), we define the covering number \(\mathcal{N}(\mathcal{S},\eta)\) as the minimal cardinality of an \(\eta\)-net of \(\mathcal{S}\). The metric entropy of \(\mathcal{S}\) with respect to radius \(\eta\) is defined as \(\mathscr{H}(\mathcal{S},\eta)=\log\mathcal{N}(\mathcal{S},\eta)\)._

### Problem Setup

We make the following assumptions on the observation model.

**Assumption 1**.: _Let \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\) and let \(f\) be a possibly unknown, possibly random non-linearity that is independent of \(\bm{a}\). Let \((\bm{a}_{i},f_{i})_{i=1}^{m}\) be i.i.d. copies of \((\bm{a},f)\). With a single draw of \((\bm{a}_{i},f_{i})_{i=1}^{m}\), for \(\bm{x}^{\star}\in\mathcal{K}=G(\mathbb{B}_{2}^{k}(r))\), where \(G:\mathbb{B}_{2}^{k}(r)\to\mathbb{R}^{n}\) is an \(L\)-Lipschitz generative model, we observe \(\big{\{}y_{i}:=f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\star})\big{\}}_{i=1}^{m}\). We can express the model more compactly as \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\star})\), where \(\bm{A}=[\bm{a}_{1},...,\bm{a}_{m}]^{\top}\in\mathbb{R}^{m\times n}\), \(\bm{f}=(f_{1},...,f_{m})^{\top}\) and \(\bm{y}=(y_{1},...,y_{m})^{\top}\in\mathbb{R}^{m}\)._

In this work, we consider the generalized Lasso as the recovery method [16, 33, 43], whose core idea is to ignore the non-linearity and minimize the regular \(\ell_{2}\) loss. In addition, we need to specify a constraint that reflects the low-complexity nature of \(\bm{x}^{\star}\), and specifically, we introduce a problem-dependent scaling factor \(T\in\mathbb{R}\) and use the constraint "\(\bm{x}\in T\mathcal{K}\)". Note that this is necessary even if the problem is linear; for example, with observations \(\bm{y}=2\bm{A}\bm{x}^{\star}\), one needs to minimize the \(\ell_{2}\) loss over "\(\bm{x}\in 2\mathcal{K}\)". Also, when the generative prior is given by \(Tx^{\star}\in\mathcal{K}=G(\mathbb{B}_{2}^{k}(r))\), we should simply use "\(\bm{x}\in\mathcal{K}\)" as constraint; this is technically equivalent to the treatment adopted in [33] (see more discussions in Remark 5 below). Taken collectively, we consider

\[\bm{\hat{x}}=\arg\min_{\bm{x}\in T\mathcal{K}}\|\bm{y}-\bm{A}\bm{x}\|_{2}.\] (2.1)

Importantly, we want to achieve uniform recovery of all \(\bm{x}^{\star}\in\mathcal{K}\) with a single realization of \((\bm{A},\bm{f})\).

### Assumptions

Let \(f\) be the function that characterizes our nonlinear measurements. We introduce several assumptions on \(f\) here, and then verify them for specific models in Section 2.3. We define the set of discontinuities as

\[\mathscr{D}_{f}=\{a\in\mathbb{R}:f\text{ is discontinuous at }a\}.\]

We define the notion of jump discontinuity as follows.

**Definition 3**.: (Jump discontinuity)_. A function \(f:\mathbb{R}\to\mathbb{R}\) has a jump discontinuity at \(x_{0}\) if both \(L^{-}:=\lim_{x\to x_{0}^{-}}f(x)\) and \(L^{+}:=\lim_{x\to x_{0}^{+}}f(x)\) exist but \(L^{-}\neq L^{+}\). We simply call the oscillation at \(x_{0}\), i.e., \(|L^{+}-L^{-}|\), the jump._

Roughly put, our framework applies to piece-wise Lipschitz continuous \(f_{i}\) with (at most) countably infinite jump discontinuities, which have bounded jumps and are well separated. The precise statement is given below.

**Assumption 2**.: _For some \((B_{0},L_{0},\beta_{0})\), the following statement unconditionally holds true for any realization of \(f\) (specifically, \(f_{1},\dots,f_{m}\) in our observations):_

* \(\mathscr{D}_{f}\) _is one of the following:_ \(\mathscr{D}\)_, a finite set, or a countably infinite set;_
* _All discontinuities of_ \(f\) _(if any) are jump discontinuities with the jump bounded by_ \(B_{0}\)_;_
* \(f\) _is_ \(L_{0}\)_-Lipschitz on any interval_ \((a,b)\) _satisfying_ \((a,b)\cap\mathscr{D}_{f}=\varnothing\)_._
* \(|a-b|\geq\beta_{0}\) _holds for any_ \(a,b\in\mathscr{D}_{f}\)_,_ \(a\neq b\) _(we set_ \(\beta_{0}=\infty\) _if_ \(|\mathscr{D}_{f}|\leq 1\)_)._

_For simplicity, we assume \(f(x_{0})=\lim_{x\to x_{0}^{+}}f(x)\) for \(x_{0}\in\mathscr{D}_{f}\).3_

Footnote 3: This is very mild because the observations are \(f_{i}(\bm{a}_{i}^{\top}\bm{x})\), while \(\mathbb{P}(\bm{a}^{\top}\bm{x}\in\mathscr{D}_{f_{i}})=0\) (as \(\mathscr{D}_{f_{i}}\) is at most countably infinite and \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\)).

We note that Assumption 2 is satisfied by \(L\)-Lipschitz \(f\) with \((B_{0},L_{0},\beta_{0})=(0,L,\infty)\), 1-bit quantized observation \(f(\cdot)=\operatorname{sign}(\cdot+\tau)\) (\(\tau\) is the potential dither, similarly below) with \((B_{0},L_{0},\beta_{0})=(2,0,\infty)\), and uniformly quantized observation \(f(\cdot)=\delta\big{(}\lfloor\frac{+\tau}{\delta}\rfloor+\frac{1}{2}\big{)}\) with \((B_{0},L_{0},\beta_{0})=(\delta,0,\delta)\).

Under Asssumption 2, for any \(\beta\in[0,\frac{\beta_{0}}{2})\) we construct \(f_{i,\beta}\) as the Lipschitz approximation of \(f_{i}\) to deal with the potential discontinuity of \(f_{i}\) (i.e., \(\mathscr{D}_{f_{i}}\neq\varnothing\)). Specifically, \(f_{i,\beta}\) modifies \(f_{i}\) in \(\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]\) to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).

We develop Theorem 2 to bound certain product processes appearing in the analysis, which produces bounds tighter than [36] when the index sets have low metric entropy. To make Theorem 2 applicable, we further make the following Assumption 3, which can be checked case-by-case by estimating the sub-Gaussian norm and probability tail. Also, \(U_{g}^{(1)}\) and \(U_{g}^{(2)}\) can even be a bit crude because the measurement number in Theorem 1 depends on them in a logarithmic manner.

**Assumption 3**.: _Let \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\), under Assumptions 1-2, we define the Lipschitz approximation \(f_{i,\beta}\) as in (3.4). We let_

\[\xi_{i,\beta}(a):=f_{i,\beta}(a)-Ta,\ \varepsilon_{i,\beta}(a):=f_{i,\beta}(a)-f_ {i}(a).\] (2.2)

_For all \(\beta\in(0,\frac{\beta_{0}}{2})\), we assume the following holds with some parameters \((A_{g}^{(1)},U_{g}^{(1)},P_{0}^{(1)})\) and \((A_{g}^{(2)},U_{g}^{(2)},P_{0}^{(2)})\):_

* \(\sup_{\bm{x}\in\mathcal{K}}\|\xi_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{\psi_{2}}\leq A _{g}^{(1)}\)_,_ \(\mathbb{P}\big{(}\sup_{\bm{x}\in\mathcal{K}}|\xi_{i,\beta}(\bm{a}^{\top}\bm{x} )|\leq U_{g}^{(1)}\big{)}\geq 1-P_{0}^{(1)};\)__
* \(\sup_{\bm{x}\in\mathcal{K}}\|\varepsilon_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{ \psi_{2}}\leq A_{g}^{(2)}\)_,_ \(\mathbb{P}\big{(}\sup_{\bm{x}\in\mathcal{K}}|\varepsilon_{i,\beta}(\bm{a}^{ \top}\bm{x})|\leq U_{g}^{(2)}\big{)}\geq 1-P_{0}^{(2)}.\)__

To build a more complete theory we further introduce two useful quantities. For some \(\bm{x}\in\mathcal{K}\), we define the target mismatch \(\rho(\bm{x})\) as in [17, Definition 1]:

\[\rho(\bm{x})=\big{\|}\mathbb{E}\big{[}f_{i}(\bm{a}_{i}^{\top}\bm{x})\bm{a}_{i }\big{]}-T\bm{x}\big{\|}_{2}.\] (2.3)

It is easy to see that \(\mathbb{E}\big{[}f_{i}(\bm{a}_{i}^{\top}\bm{x})\bm{a}_{i}\big{]}\) minimizes the expected \(\ell_{2}\) loss \(\mathbb{E}\big{[}\|\bm{y}-\bm{A}\bm{x}\|_{2}^{2}\big{]}\), thus one can roughly understand \(\mathbb{E}\big{[}f_{i}(\bm{a}_{i}^{\top}\bm{x})\bm{a}_{i}\big{]}\) as the expectation of \(\hat{\bm{x}}\). Since \(T\bm{x}\) is the desired ground truth, a small \(\rho(\bm{x})\) is intuitively an important ingredient for generalized Lasso to succeed. Fortunately, in many models, \(\rho(\bm{x})\) with a suitably chosen \(T\) will vanish (e.g., linear model [2], single index model [33], 1-bit model [31]) or at least be sufficiently small (e.g., 1-bit model with dithering [45]).

As mentioned before, our method to deal with discontinuity of \(f_{i}\) is to introduce its approximation \(f_{i,\beta}\), which differs from \(f_{i}\) only in \(\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]\). This will produce some bias because the actual observation is \(f_{i}(\bm{a}_{i}^{\top}\bm{x}^{*})\) rather than \(f_{i,\beta}(\bm{a}_{i}^{\top}\bm{x}^{*})\). Hence, for some \(\bm{x}\in\mathcal{K}\) we define the following quantity to measure the bias induced by \(f_{i,\beta}\):

\[\mu_{\beta}(\bm{x})=\mathbb{P}\Big{(}\bm{a}^{\top}\bm{x}\in\mathscr{D}_{f_{i} }+\Big{[}-\frac{\beta}{2},\frac{\beta}{2}\Big{]}\Big{)},\ \ \bm{a}\sim\mathcal{N}(0,\bm{I}_{n}).\] (2.4)

The following assumption can often be satisfied by choosing suitable \(T\) and sufficiently small \(\beta_{1}\).

**Assumption 4**.: _Suppose Assumptions 1-3 hold true with parameters \(B_{0},L_{0},\beta_{0},A_{g}^{(1)},A_{g}^{(2)}\). For the \(T\) used in (2.1), \(\rho(\bm{x})\) defined in (2.3) satisfies_

\[\sup_{\bm{x}\in\mathcal{K}}\rho(\bm{x})\lesssim(A_{g}^{(1)}\lor A_{g}^{(2)}) \sqrt{\frac{k}{m}}.\] (2.5)

_Moreover, there exists some \(0<\beta_{1}<\frac{\beta_{0}}{2}\) such that_

\[(L_{0}\beta_{1}+B_{0})\sup_{\bm{x}\in\mathcal{K}}\sqrt{\mu_{\beta_{1}}(\bm{x})} \lesssim(A_{g}^{(1)}\lor A_{g}^{(2)})\sqrt{\frac{k}{m}}.\] (2.6)

In the proof, the estimation error \(\|\hat{\bm{x}}-T\bm{x}^{*}\|\) is contributed by a concentration term of scaling \(\tilde{O}\big{(}(A_{g}^{(1)}\lor A_{g}^{(2)})\sqrt{k/m}\big{)}\) and some bias terms. The main aim of Assumption 4 is to pull down the bias terms so that the concentration term is dominant.

### Main Theorem and its Implications

We now present our general theorem and apply it to some specific models.

**Theorem 1**.: _Under Assumptions 1-4, given any recovery accuracy \(\epsilon\in(0,1)\), if it holds that \(m\gtrsim(A_{g}^{(1)}\vee A_{g}^{(2)})^{\frac{1}{2}}\frac{k^{2}}{\varepsilon^{2}}\), then with probability at least \(1-m(P_{0}^{(1)}+P_{0}^{(2)})-m\exp(-\Omega(n))-C\exp(-\Omega(k))\) on a single realization of \((\bm{A},\bm{f}):=(\bm{a}_{i},f_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|\bm{\hat{x}}-T\bm{x}^{\bullet}\|_{2}\leq\epsilon\) for all \(\bm{x}^{\bullet}\in\mathcal{K}\), where \(\bm{\hat{x}}\) is the solution to (2.1) with \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\bullet})\), and \(\mathscr{L}=\log\widetilde{P}\) is a logarithmic factor with \(\widetilde{P}\) being polynomial in \((L,n)\) and other parameters that typically scale as \(O(L+n)\). See (C.11) for the precise expression of \(\mathscr{L}\)._

To illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in Assumption 2 by the measurement model; (ii) Set \(T\) that verifies (2.5) (see Lemmas 8-11 for the following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian vector is useful; (iv) Set \(\beta_{1}\) to guarantee (2.6) based on some standard probability argument. We only provide suitable parameters for the following concrete models due to space limit, while leaving more details to Appendix E.

**(A) 1-bit GCS.** Assume that we have the 1-bit observations \(y_{i}=\operatorname{sign}(\bm{a}_{i}^{\top}\bm{x}^{\bullet})\); then \(f_{i}(\cdot)=f(\cdot)=\operatorname{sign}(\cdot)\) satisfies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(2,0,\infty)\). In this model, it is hopeless to recover the norm of \(\|\bm{x}^{\bullet}\|_{2}\); as done in previous work, we assume \(\bm{x}^{\bullet}\in\mathcal{K}\subset\mathbb{S}^{n-1}\)[31, Remark 1]. We set \(T=\sqrt{2/\pi}\) and take the parameters in Assumption 3 as \(A_{g}^{(1)}\asymp 1,U_{g}^{(1)}\asymp\sqrt{n},P_{0}^{(1)}\asymp\exp(-\Omega(n)),A_{g}^{(2 )}\asymp 1,U_{g}^{(2)}\asymp 1,P_{0}^{(2)}=0\). We take \(\beta=\beta_{1}\asymp\frac{k}{m}\) to guarantee (2.6). With these choices, Theorem 1 specializes to the following:

**Corollary 1**.: _Consider Assumption 1 with \(f_{i}(\cdot)=\operatorname{sign}(\cdot)\) and \(\mathcal{K}\subset\mathbb{S}^{n-1}\), let \(\epsilon\in(0,1)\) be any given recovery accuracy. If \(m\gtrsim\frac{k}{\epsilon^{2}}\log\left(\frac{Lr\sqrt{m}n}{\epsilon\wedge(k/m )}\right)\),4 then with probability at least \(1-2m\exp(-cn)-m\exp(-\Omega(k))\) on a single draw of \((\bm{a}_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\left\|\bm{\hat{x}}-\sqrt{\frac{2}{\pi}}\bm{x}^{\bullet}\right\|_{2}\leq\epsilon\) for all \(\bm{x}^{\bullet}\in\mathcal{K}\), where \(\bm{\hat{x}}\) is the solution to (2.1) with \(\bm{y}=\operatorname{sign}(\bm{A}\bm{x}^{\bullet})\) and \(T=\sqrt{\frac{2}{\pi}}\)._

Footnote 4: Here and in other similar statements, we implicitly assume a large enough implied constant.

**Remark 1**.: _A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33, Section 5]. Their proof relies on the local embedding property in [31]. Note that such geometric property is often problem-dependent and highly nontrivial. By contrast, our argument is free of geometric properties of this kind._

**Remark 2**.: _For traditional 1-bit CS, [17, Corollary 2] requires \(m\gtrsim\tilde{O}(k/\epsilon^{4})\) to achieve uniform \(\ell_{2}\)-accuracy of \(\epsilon\) for all \(k\)-sparse signals, which is inferior to our \(\tilde{O}(k/\epsilon^{2})\). This is true for all remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather than [36]) to obtain tighter bound for the product processes, as will be discussed in Remark 8._

**(B) 1-bit GCS with dithering.** Assume that the \(\bm{a}_{i}^{\top}\bm{x}^{\bullet}\) is quantized to 1-bit with dither5\(\tau_{i}\stackrel{{ iid}}{{\sim}}\mathscr{U}[-\lambda, \lambda]\) for some \(\lambda\) to be chosen, i.e., we observe \(y_{i}=\operatorname{sign}(\bm{a}_{i}^{\top}\bm{x}^{\bullet}+\tau_{i})\). Following [45] we assume \(\mathcal{K}\subset\mathbb{B}_{2}^{2}(R)\) for some \(R>0\). Here, using dithering allows the recovery of signal norm \(\|\bm{x}^{\bullet}\|_{2}\), so we do not need to assume \(\mathcal{K}\subset\mathbb{S}^{n-1}\) as in Corollary 1. We set \(\lambda=CR\sqrt{\log m}\) with sufficiently large \(C\), and \(T=\lambda^{-1}\). In Assumption 3, we take \(A_{g}^{(1)}\asymp 1,\ U_{g}^{(1)}\asymp\sqrt{n},\ P_{0}^{(1)}\asymp\exp(-\Omega(n)),\ A_{g}^{(2 )}\asymp 1,\ U_{g}^{(2)}\asymp 1\), and \(P_{0}^{(2)}=0\). Moreover, we take \(\beta=\beta_{1}=\frac{\lambda k}{m}\) to guarantee (2.6). Now we can invoke Theorem 1 to get the following.

Footnote 5: Throughout this work, the random dither is independent of the \(\{\bm{a}_{i}\}_{i=1}^{m}\).

**Corollary 2**.: _Consider Assumption 1 with \(f_{i}(\cdot)=\operatorname{sign}(\cdot+\tau_{i})\), \(\tau_{i}\sim\mathscr{U}[-\lambda,\lambda]\) and \(\mathcal{K}\subset\mathbb{B}_{2}^{n}(R)\), and \(\lambda=CR\sqrt{\log m}\) with sufficiently large \(C\). Let \(\epsilon\in(0,1)\) be any given recovery accuracy. If \(m\gtrsim\frac{k}{\epsilon^{2}}\log\left(\frac{Lr\sqrt{m}n}{\lambda(\epsilon \wedge(k/m))}\right)\), then with probability at least \(1-2m\exp(-cn)-m\exp(-\Omega(k))\) on a single draw of \((\bm{a}_{i},\tau_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|\bm{\hat{x}}-\lambda^{-1}\bm{x}^{\bullet}\|_{2}\leq\epsilon\) for all \(\bm{x}^{\bullet}\in\mathcal{K}\), where \(\bm{\hat{x}}\) is the solution to (2.1) with \(\bm{y}=\operatorname{sign}(\bm{A}\bm{x}^{\bullet}+\bm{\tau})\) (here, \(\bm{\tau}=[\tau_{1},...,\tau_{m}]^{\top}\)) and \(T=\lambda^{-1}\)._

**Remark 3**.: _To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative models; by specializing our result to the ReLU network that is typically \((n^{\Theta(d)})\)-Lipschitz [2] (\(d\) isthe number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as already mentioned in the Introduction Section, our result can be generalized to a sensing vector with an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The advantage of their result is in allowing sub-exponential sensing vectors._

**(C) Lipschitz-continuous SIM with generative prior.** Assume that any realization of \(f\) is unconditionally \(\hat{L}\)-Lipschitz, which implies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(0,\hat{L},\infty)\). We further assume \(\mathbb{P}(f(0)\leq\hat{B})\geq 1-P_{0}^{\prime}\) for some \((\hat{B},P_{0}^{\prime})\). Because the norm of \(\bm{x}^{\star}\) is absorbed into the unknown \(f(\cdot)\), we assume \(\mathcal{K}\subset\mathbb{S}^{n-1}\). We set \(\beta=0\) so that \(f_{i,\beta}=f_{i}\). We introduce the quantities \(\mu=\mathbb{E}[f(g)g],\psi=\|f(g)\|_{\psi_{2}},\text{ where }g\sim\mathcal{N}(0,1)\). We choose \(T=\mu\) and set parameters in Assumption 3 as \(A_{g}^{(1)}\asymp\psi+\mu,\;U_{g}^{(1)}\asymp(\hat{L}+\mu)\sqrt{n}+\hat{B},\;P_ {0}^{(1)}\asymp P_{0}^{\prime}+\exp(-\Omega(n)),\;A_{g}^{(2)}\asymp\psi+\mu, \;U_{g}^{(2)}=0,\;P_{0}^{(2)}=0\). Now we are ready to apply Theorem 1 to this model. We obtain:

**Corollary 3**.: _Consider Assumption 1 with \(\hat{L}\)-Lipschitz \(f\), suppose that \(\mathbb{P}(f(0)\leq\hat{B})\geq 1-P_{0}^{\prime}\), and define the parameters \(\mu=\mathbb{E}[f(g)g]\), \(\psi=\|f(g)\|_{\psi_{2}}\) with \(g\sim\mathcal{N}(0,1)\). Let \(\epsilon\in(0,1)\) be any given recovery accuracy. If \(\eta\asymp\frac{(\mu+\epsilon)\nu k}{\epsilon^{2}}\log\left(\frac{L\nu\sqrt{m} \ln(\mu+\epsilon)(\hat{L}+\mu)+\sqrt{n}\mu\hat{B}+\psi]}{\epsilon^{2}}\right)\), then with probability at least \(1-2m\exp(-cn)-mP_{0}^{\prime}-c_{1}\exp(-\Omega(k))\) on a single draw of \((\bm{a}_{i},f_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\leq\epsilon\) for all \(\bm{x}^{\star}\in\mathcal{K}\), where \(\bm{\hat{x}}\) is the solution to (2.1) with \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\star})\) and \(T=\mu\)._

**Remark 4**.: _While the main result of [33] is non-uniform, it was noted in [33, Section 5] that a similar uniform error rate can be established for any deterministic \(1\)-Lipschitz \(f\). Our result here is more general in that the \(\hat{L}\)-Lipschitz \(f\) is possibly random. Note that randomness on \(f\) is significant because it provides much more flexibility (e.g., additive random noise)._

**Remark 5**.: _For SIM with unknown \(f_{i}\) it may seem impractical to use (2.1) as it requires \(\mu=\mathbb{E}[f(g)g]\) where \(g\sim\mathcal{N}(0,1)\). However, by assuming \(\mu\bm{x}^{\star}\in\mathcal{K}=G(\mathbb{B}_{2}^{k}(r))\) as in [33], which is natural for sufficiently expressive \(G(\cdot)\), we can simply use \(\bm{x}\in\mathcal{K}\) as constraint in (2.1). Our Corollary 3 remains valid in this case under some inessential changes of \(\log\mu\) factors in the sample complexity._

**(D) Uniformly quantized GCS with dithering.** The uniform quantizer with resolution \(\delta>0\) is defined as \(\mathcal{Q}_{\delta}(a)=\delta\big{(}\big{[}\frac{a}{\delta}\big{]}+\frac{1}{2} \big{)}\) for \(a\in\mathbb{R}\). Using dithering \(\tau_{i}\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\), we suppose that the observations are \(y_{i}=\mathcal{Q}_{\delta}(\bm{a}_{i}^{\top}\bm{x}^{\star}+\tau_{i})\). This satisfies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(\delta,0,\delta)\). We set \(T=1\) and take parameters for Assumption 3 as follows: \(A_{g}^{(1)},U_{g}^{(1)},A_{g}^{(2)},U_{g}^{(2)}\asymp\delta\), and \(P_{0}^{(1)}=P_{0}^{(2)}=0.\) We take \(\beta=\beta_{1}\asymp\frac{k\delta}{m}\) to confirm (2.6). With these parameters, we obtain the following from Theorem 1.

**Corollary 4**.: _Consider Assumption 1 with \(f(\cdot)=\mathcal{Q}_{\delta}(\cdot+\tau)\), \(\tau\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\) for some quantization resolution \(\delta>0\). Let \(\epsilon>0\) be any given recovery accuracy. If \(m\gtrsim\frac{\delta^{2}k}{\epsilon^{2}}\log\Big{(}\frac{L\nu\sqrt{mn}}{ \epsilon\wedge[k\delta/(m\sqrt{n})]}\Big{)}\), then with probability at least \(1-2m\exp(-cn)-c_{1}\exp(-\Omega(k))\) on a single draw of \((\bm{a}_{i},\tau_{i})_{i=1}^{m}\), we have the uniform recovery guarantee \(\|\bm{\hat{x}}-\bm{x}^{\star}\|_{2}\leq\epsilon\) for all \(\bm{x}^{\star}\in\mathcal{K}\), where \(\bm{\hat{x}}\) is the solution to (2.1) with \(\bm{y}=\mathcal{Q}_{\delta}(\bm{A}\bm{x}+\bm{\tau})\) and \(T=1\) (here, \(\bm{\tau}=[\tau_{1},\ldots,\tau_{m}]^{\top}\))._

**Remark 6**.: _While this dithered uniform quantized model has been widely studied in traditional CS (e.g., non-uniform recovery [8, 48], uniform recovery [17, 52]), it has not been investigated in GCS even for non-uniform recovery. Thus, this is new to the best of our knowledge._

A simple extension to the noisy model \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\star})+\bm{\eta}\) where \(\bm{\eta}\in\mathbb{R}^{m}\) has i.i.d. sub-Gaussian entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F.

## 3 Proof Sketch

To provide a sketch of our proof, we begin with the optimality condition \(\|\bm{y}-\bm{A}\bm{\hat{x}}\|_{2}^{2}\leq\|\bm{y}-\bm{A}(T\bm{x}^{\star})\|_{2}^ {2}\). We expand the square and plug in \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\star})\) to obtain

\[\left\|\frac{\bm{A}}{\sqrt{m}}(\bm{\hat{x}}-T\bm{x}^{\star})\right\|_{2}^{2}\leq \frac{2}{m}\big{\langle}\bm{f}(\bm{A}\bm{x}^{\star})-T\bm{A}\bm{x}^{\star},\bm{A} (\bm{\hat{x}}-T\bm{x}^{\star})\big{\rangle}.\] (3.1)

For the final goal \(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\leq\epsilon\), up to rescaling, it is enough to prove \(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\leq 3\epsilon\). We assume for convenience that \(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}>2\epsilon\), without loss of generality. Combined with \(\bm{\hat{x}},Tx^{\bullet}\in T\mathcal{K}\), we know \(\bm{\hat{x}}-Tx^{\bullet}\in\mathcal{K}_{\epsilon}^{-}\), where \(\mathcal{K}_{\epsilon}^{-}:=(T\mathcal{K}^{-})\cap\left(\mathbb{B}_{2}^{n}(2 \epsilon)\right)^{c},\ \mathcal{K}^{-}=\mathcal{K}-\mathcal{K}\). We further define

\[(\mathcal{K}_{\epsilon}^{-})^{*}:=\left\{\bm{z}/\|\bm{z}\|_{2}:\bm{z}\in \mathcal{K}_{\epsilon}^{-}\right\}\] (3.2)

where the normalized error lives, i.e. \(\frac{\bm{\hat{x}}-T\bm{x}^{\bullet}}{\|\bm{z}-T\bm{x}^{\bullet}\|_{2}}\in( \mathcal{K}_{\epsilon}^{-})^{*}\). Our strategy is to establish a uniform lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We emphasize that these bounds must hold uniformly for all \(\bm{x}^{\bullet}\in\mathcal{K}\).

It is relatively easy to use set-restricted eigenvalue condition (S-REC) [2] to establish a uniform lower bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold uniformly for all \(\bm{x}^{\bullet}\), we first take the supremum over \(\bm{x}^{\bullet}\) and \(\bm{\hat{x}}\) and consider bounding the following:

\[\mathscr{R}:=\frac{1}{m}\big{\langle}\bm{f}(\bm{Ax}^{\bullet})-T \bm{Ax}^{\bullet},\bm{A}(\bm{\hat{x}}-T\bm{x}^{\bullet})\big{\rangle}\] (3.3) \[=\frac{1}{m}\sum_{i=1}^{m}\big{(}f_{i}(\bm{a}_{i}^{\top}\bm{x}^{ \bullet})-T\bm{a}_{i}^{\top}\bm{x}^{\bullet}\big{)}\cdot\big{(}\bm{a}_{i}^{ \top}[\bm{\hat{x}}-T\bm{x}^{\bullet}]\big{)}\] \[\leq\|\bm{\hat{x}}-T\bm{x}^{\bullet}\|_{2}\cdot\sup_{\bm{x}\in \mathcal{K}}\sup_{\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}}\frac{1}{m}\sum_{ i=1}^{m}\big{(}f_{i}(\bm{a}_{i}^{\top}\bm{x})-T\bm{a}_{i}^{\top}\bm{x} \big{)}\cdot\big{(}\bm{a}_{i}^{\top}\bm{v}\big{)}:=\|\bm{\hat{x}}-T\bm{x}^{ \bullet}\|_{2}\cdot\mathscr{R}_{u},\]

where \((\mathcal{K}_{\epsilon}^{-})^{*}\) is defined in (3.2). Clearly, \(\mathscr{R}_{u}\) is the supremum of a product process, whose factors are indexed by \(\bm{x}\in\mathcal{K}\) and \(\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}\). It is, in general, challenging to control a product process, and existing results often require both factors to satisfy a certain "sub-Gaussian increments" condition (e.g., [36; 37]). However, the first factor of \(\mathscr{R}_{u}\) (i.e., \(f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\bullet})-T\bm{a}_{i}^{\top}\bm{x}^{\bullet}\)) does not admit such a condition when \(f_{i}\) is not continuous (e.g., the 1-bit model \(f_{i}=\operatorname{sign}(\cdot)\)). We will construct the Lipschitz approximation of \(f_{i}\) to overcome this difficulty shortly in Section 3.1.

**Remark 7**.: _We note that these challenges stem from our pursuit of uniform recovery. In fact, a non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient is [33, Lemma 3] that bounds \(\mathscr{R}_{u}\) without the supremum on \(\bm{x}\). This can be done as long as \(f_{i}(\bm{a}_{i}^{\top}\bm{x}^{\bullet})\) is sub-Gaussian, while the potential discontinuity of \(f_{i}\) is totally unproblematic._

### Lipschitz Approximation

For any \(x_{0}\in\mathscr{D}_{f_{i}}\) we define the one-sided limits as \(f_{i}^{-}(x_{0})=\lim_{x\to x_{0}^{-}}f_{i}(x)\) and \(f_{i}^{+}(x_{0})=\lim_{x\to x_{0}^{+}}f_{i}(x)\), and write their average as \(f_{i}^{a}(x_{0})=\frac{1}{2}(f_{i}^{-}(x_{0})+f_{i}^{+}(x_{0}))\). Given any approximation accuracy \(\beta\in(0,\frac{\beta_{0}}{2})\), we construct the Lipschitz continuous function \(f_{i,\beta}\) as:

\[f_{i,\beta}(x)=\begin{cases}f_{i}(x)&,\quad\text{ if }x\notin\mathscr{D}_{f_{i}}+[- \frac{\beta}{2},\frac{\beta}{2}]\\ f_{i}^{a}(x_{0})-\frac{2[f_{i}^{+}(x_{0})-f_{i}(x_{0}-\frac{\beta}{2})](x_{0}- x)}{\beta},\quad\text{ if }\exists x_{0}\in\mathscr{D}_{f_{i}}\text{ s.t. }x\in[x_{0}-\frac{\beta}{2},x_{0}]\\ f_{i}^{a}(x_{0})+\frac{2[f_{i}(x_{0}+\frac{\beta}{2})-f_{i}^{-}(x_{0})](x-x_{0}) }{\beta},\quad\text{if }\exists x_{0}\in\mathscr{D}_{f_{i}},\text{ s.t. }x\in[x_{0},x_{0}+\frac{\beta}{2}]\end{cases}.\] (3.4)

We have defined the approximation error \(\varepsilon_{i,\beta}(\cdot)=f_{i,\beta}(\cdot)-f_{i}(\cdot)\) in Assumption 3. An important observation is that both \(f_{i,\beta}\) and \(|\varepsilon_{i,\beta}|\) are Lipschitz continuous (see Lemma 1 below). Here, it is crucial to consider \(|\varepsilon_{i,\beta}|\) rather than \(\varepsilon_{i,\beta}\) as the latter is not continuous; see Figure 1 for an intuitive graphical illustration and more explanations in Appendix B.2.

**Lemma 1**.: _With \(B_{0},L_{0},\beta_{0}\) given in Assumption 2, for any \(\beta\in(0,\frac{\beta_{0}}{2})\), \(f_{i,\beta}\) is \(\big{(}L_{0}+\frac{B_{0}}{\beta}\big{)}\)-Lipschitz over \(\mathbb{R}\), and \(|\varepsilon_{i,\beta}|\) is \(\big{(}2L_{0}+\frac{B_{0}}{\beta}\big{)}\)-Lipschitz over \(\mathbb{R}\)._

### Bounding the product process

We now present our technique to bound \(\mathscr{R}_{u}\). Recall that \(\xi_{i,\beta}(a)\) and \(\varepsilon_{i,\beta}(a)\) were defined in (2.2). By Lemma 1, \(\xi_{i,\beta}\) is \(\big{(}L_{0}+T+\frac{B_{0}}{\beta}\big{)}\)-Lipschitz. Now we use \(f_{i}(a)-Ta=\xi_{i,\beta}(a)-\varepsilon_{i,\beta}\) to decompose \(\mathscr{R}_{u}\) (in the following, we sometimes shorten "\(\sup_{\bm{x}\in\mathcal{K}}\sup_{\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}}\)" as "\(\sup_{\bm{x},\bm{v}}\)"):

\[\mathscr{R}_{u}\leq\underbrace{\sup_{\bm{x},\bm{v}}\frac{1}{m}\sum_{i=1}^{m}\xi_{ i,\beta}(\bm{a}_{i}^{\top}\bm{x})\cdot\big{(}\bm{a}_{i}^{\top}\bm{v}\big{)}}_{ \mathscr{R}_{u1}}+\underbrace{\sup_{\bm{x},\bm{v}}\frac{1}{m}\sum_{i=1}^{m} \left|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\right|\big{|}\bm{a}_{i}^{ \top}\bm{v}\big{|}}_{\mathscr{R}_{u2}}.\] (3.5)It remains to control \(\mathscr{R}_{u1}\) and \(\mathscr{R}_{u2}\). By the Lipschitz continuity of \(\xi_{i,\beta}\) and \(|\varepsilon_{i,\beta}|\), the factors of \(\mathscr{R}_{u1}\) and \(\mathscr{R}_{u2}\) admit sub-Gaussian increments, so it is natural to first center them and then invoke the concentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate in Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually require \(\tilde{O}(k/\epsilon^{4})\) to achieve a uniform \(\ell_{2}\)-error of \(\epsilon\), as is the case in [17, Section 4].

In fact, Lemma 5 is based on _Gaussian width_ and hence blind to the fact that \(\mathcal{K},(\mathcal{K}_{e}^{-})^{*}\) here have low _metric entropy_ (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric entropy, we develop the following concentration inequality that can produce tighter bound for \(\mathscr{R}_{u1}\) and \(\mathscr{R}_{u2}\). This also allows us to derive uniform error rates sharper than those in [17, Section 4].

**Theorem 2**.: _Let \(g_{\bm{x}}=g_{\bm{x}}(\bm{a})\) and \(h_{\bm{v}}=h_{\bm{v}}(\bm{a})\) be stochastic processes indexed by \(\bm{x}\in\mathcal{X}\subset\mathbb{R}^{p_{1}},\bm{v}\in\mathcal{V}\subset \mathbb{R}^{p_{2}}\), both defined with respect to a common random variable \(\bm{a}\). Assume that:_

* _(A1.)_ \(g_{\bm{x}}(\bm{a}),\ h_{\bm{v}}(\bm{a})\) _are sub-Gaussian for some_ \((A_{g},A_{h})\) _and admit sub-Gaussian increments regarding_ \(\ell_{2}\) _distance for some_ \((M_{g},M_{h})\)_:_ \[\begin{split}&\|g_{\bm{x}}(\bm{a})-g_{\bm{x}^{\prime}}(\bm{a}) \|_{\psi_{2}}\leq M_{g}\|\bm{x}-\bm{x}^{\prime}\|_{2},\ \|g_{\bm{x}}(\bm{a})\|_{\psi_{2}}\leq A_{g},\ \forall\ \bm{x},\bm{x}^{\prime}\in \mathcal{X};\\ &\|h_{\bm{v}}(\bm{a})-h_{\bm{v}^{\prime}}(\bm{a})\|_{\psi_{2}} \leq M_{h}\|\bm{v}-\bm{v}^{\prime}\|_{2},\ \|h_{\bm{v}}(\bm{a})\|_{\psi_{2}}\leq A_{h},\ \forall\ \bm{v},\bm{v}^{\prime}\in \mathcal{V}.\end{split}\] (3.6)
* _(A2.)_ _On a single draw of_ \(\bm{a}\)_, for some_ \((L_{g},U_{g},L_{h},U_{h})\) _the following events simultaneously hold with probability at least_ \(1-P_{0}\)_:_ \[\begin{split}&|g_{\bm{x}}(\bm{a})-g_{\bm{x}^{\prime}}(\bm{a}) |\leq L_{g}\|\bm{x}-\bm{x}^{\prime}\|_{2},\ |g_{\bm{x}}(\bm{a})|\leq U_{g},\ \forall\ \bm{x},\bm{x}^{\prime}\in \mathcal{X};\\ &|h_{\bm{v}}(\bm{a})-h_{\bm{v}^{\prime}}(\bm{a})|\leq L_{h}\|\bm {v}-\bm{v}^{\prime}\|_{2},\ |h_{\bm{v}}(\bm{a})|\leq U_{h},\ \forall\ \bm{v},\bm{v}^{\prime}\in \mathcal{V}.\end{split}\] (3.7)

_Let \(\bm{a}_{1},...,\bm{a}_{m}\) be i.i.d. copies of \(\bm{a}\), and introduce the shorthand \(S_{g,h}=L_{g}U_{h}+M_{g}A_{h}\) and \(T_{g,h}=L_{h}U_{g}+M_{h}A_{g}\). If \(m\gtrsim\mathscr{H}\left(\mathcal{X},\frac{A_{g}A_{h}}{\sqrt{m}S_{g,h}}\right) +\mathscr{H}\left(\mathcal{V},\frac{A_{g}A_{h}}{\sqrt{m}T_{g,h}}\right)\), where \(\mathscr{H}(\cdot,\cdot)\) is the metric entropy defined in Definition 2, then with probability at least \(1-mP_{0}-2\exp\big{[}-\Omega\big{(}\mathscr{H}(\mathcal{X},\frac{A_{g}A_{h}}{ \sqrt{m}S_{g,h}})+\mathscr{H}(\mathcal{V},\frac{A_{g}A_{h}}{\sqrt{m}T_{g,h}}) \big{)}\big{]}\) we have \(I\lesssim\frac{A_{g}A_{h}}{\sqrt{m}}\sqrt{\mathscr{H}(\mathcal{X},\frac{A_{g}A _{h}}{\sqrt{m}S_{g,h}})+\mathscr{H}(\mathcal{V},\frac{A_{g}A_{h}}{\sqrt{m}T_{g,h}})}\), where \(I:=\sup_{\bm{x}\in\mathcal{X}}\sup_{\bm{v}\in\mathcal{V}}\big{|}\frac{1}{m} \sum_{i=1}^{m}\big{(}g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})-\mathbb{E}[ g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})]\big{)}\big{|}\) is the supremum of a product process._

**Remark 8**.: _We use \(\mathscr{R}_{u2}\) as an example to illustrate the advantage of Theorem 2 over Lemma 5. The key step is on bounding the centered process_

\[\mathscr{R}_{u2,c}:=\sup_{\bm{x}\in\mathcal{K}}\sup_{\bm{v}\in(\mathcal{K}_{e }^{-})}\big{\{}|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})||\bm{a}_{i}^{ \top}\bm{v}|-\mathbb{E}[|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})||\bm{a }_{i}^{\top}\bm{v}|]\big{\}}.\]

_Let \(g_{\bm{x}}(\bm{a}_{i})=|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})|\) and \(h_{\bm{v}}(\bm{a}_{i})=|\bm{a}_{i}^{\top}\bm{v}|\), then one can use Theorem 2 or Lemma 5 to bound \(\mathscr{R}_{u2,c}\). Note that \(||\bm{a}_{i}^{\top}\bm{v}||_{\psi_{2}}=O(1)\) justifies the choice \(A_{h}=O(1)\), and both \(\mathscr{H}(\mathcal{K},\eta)\) and \(\mathscr{H}((\mathcal{K}_{e}^{-})^{*},\eta)\) depend linearly on \(k\) but only logarithmically on \(\eta\) (Lemma 6), so Theorem 2 could bound \(\mathscr{R}_{u2,c}\) by \(\tilde{O}\big{(}A_{g}\sqrt{k/m}\big{)}\) that depends on \(M_{g}\) in a logarithmic manner. However, the bound produced by Lemma 5 depends linearly on \(M_{g}\); see term \(\frac{M_{g}A_{h}\omega(\mathcal{K})}{\sqrt{m}}\) in (A.1). From (3.6), \(M_{g}\) should be proportional to the Lipschitz constant of \(|\varepsilon_{i,\beta}|\), which scales as \(\frac{1}{\beta}\) (Lemma 1). The issue is that in many cases we need to take extremely small \(\beta\) to guarantee that (2.6) holds true (e.g., we take \(\beta\asymp k/m\) in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2._

Figure 1: (Left): \(f_{i}\) and its approximation \(f_{i,0.5}\); (Right): approximation error \(\varepsilon_{i,0.5},|\varepsilon_{i,0.5}|\).

Conclusion

In this work, we built a unified framework for uniform signal recovery in nonlinear generative compressed sensing. We showed that using generalized Lasso, a sample size of \(\tilde{O}(k/\epsilon^{2})\) suffices to uniformly recover all \(\bm{x}\in G(\mathbb{B}_{2}^{k}(r))\) up to an \(\ell_{2}\)-error of \(\epsilon\). We specialized our main theorem to 1-bit GCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform guarantees that are new or exhibit some advantages over existing ones. Unlike [33], our proof is free of any non-trivial embedding property. As part of our technical contributions, we constructed the Lipschitz approximation to handle potential discontinuity in the observation model, and also developed a concentration inequality to derive tighter bound for the product processes arising in the proof, allowing us to obtain a uniform error rate faster than [17]. Possible future directions include extending our framework to handle the adversarial noise and representation error.

**Acknowledgment.** J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong Research Grants Council (RGC). J. Scarlett was supported by the Singapore National Research Foundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC GRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921.

## References

* [1] M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, "Invertible generative models for inverse problems: Mitigating representation error and dataset bias," in _International Conference on Machine Learning_. PMLR, 2020, pp. 399-409.
* [2] A. Bora, A. Jalal, E. Price, and A. G. Dimakis, "Compressed sensing using generative models," in _International Conference on Machine Learning_. PMLR, 2017, pp. 537-546.
* [3] P. T. Boufounos and R. G. Baraniuk, "1-bit compressive sensing," in _Annual Conference on Information Sciences and Systems_. IEEE, 2008, pp. 16-21.
* [4] T. T. Cai and A. Zhang, "Sparse representation of a polytope and recovery of sparse signals and low-rank matrices," _IEEE Transactions on Information Theory_, vol. 60, no. 1, pp. 122-132, 2013.
* [5] E. J. Candes and T. Tao, "Decoding by linear programming," _IEEE Transactions on Information Theory_, vol. 51, no. 12, pp. 4203-4215, 2005.
* [6] E. J. Candes and T. Tao, "Near-optimal signal recovery from random projections: Universal encoding strategies?" _IEEE Transactions on Information Theory_, vol. 52, no. 12, pp. 5406-5425, 2006.
* [7] J. Chen and M. K. Ng, "Uniform exact reconstruction of sparse signals and low-rank matrices from phase-only measurements," _IEEE Transactions on Information Theory_, vol. 69, no. 10, pp. 6739-6764, 2023.
* [8] J. Chen, M. K. Ng, and D. Wang, "Quantizing heavy-tailed data in statistical estimation: (Near) Minimax rates, covariate quantization, and uniform recovery," _arXiv preprint arXiv:2212.14562_, 2022.
* [9] J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, "High dimensional statistical estimation under uniformly dithered one-bit quantization," _IEEE Transactions on Information Theory_, vol. 69, no. 8, pp. 5151-5187, 2023.
* [10] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, "Diffusion posterior sampling for general noisy inverse problems," in _International Conference on Learning Representations_, 2022.
* [11] G. Daras, J. Dean, A. Jalal, and A. Dimakis, "Intermediate layer optimization for inverse problems using deep generative models," in _International Conference on Machine Learning_. PMLR, 2021, pp. 2421-2432.
* [12] M. Dhar, A. Grover, and S. Ermon, "Modeling sparse deviations for compressed sensing using generative models," in _International Conference on Machine Learning_. PMLR, 2018, pp. 1214-1223.

* [13] S. Dirksen, "Quantized compressed sensing: a survey," in _Compressed Sensing and Its Applications: Third International MATHEON Conference 2017_. Springer, 2019, pp. 67-95.
* [14] S. Dirksen and S. Mendelson, "Non-Gaussian hyperplane tessellations and robust one-bit compressed sensing," _Journal of the European Mathematical Society_, vol. 23, no. 9, pp. 2913-2947, 2021.
* [15] S. Foucart and H. Rauhut, _A Mathematical Introduction to Compressive Sensing_. Springer, 2013.
* [16] M. Genzel, "High-dimensional estimation of structured signals from non-linear observations with general convex loss functions," _IEEE Transactions on Information Theory_, vol. 63, no. 3, pp. 1601-1619, 2016.
* [17] M. Genzel and A. Stollenwerk, "A unified approach to uniform signal recovery from nonlinear observations," _Foundations of Computational Mathematics_, pp. 1-74, 2022.
* [18] R. M. Gray and T. G. Stockham, "Dithered quantizers," _IEEE Transactions on Information Theory_, vol. 39, no. 3, pp. 805-812, 1993.
* [19] P. Hand, O. Leong, and V. Voroninski, "Phase retrieval under a generative prior," _Advances in Neural Information Processing Systems_, vol. 31, 2018.
* [20] P. Hand and V. Voroninski, "Global guarantees for enforcing deep generative priors by empirical risk," in _Conference On Learning Theory_. PMLR, 2018, pp. 970-978.
* [21] L. Jacques and T. Feuillen, "The importance of phase in complex compressive sensing," _IEEE Transactions on Information Theory_, vol. 67, no. 6, pp. 4150-4161, 2021.
* [22] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, "Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors," _IEEE Transactions on Information Theory_, vol. 59, no. 4, pp. 2082-2102, 2013.
* [23] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, "Instance-optimal compressed sensing via posterior sampling," in _International Conference on Machine Learning (ICML)_, 2021.
* [24] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, "Robust compressed sensing MRI with deep generative priors," _Advances in Neural Information Processing Systems_, vol. 34, pp. 14 938-14 954, 2021.
* [25] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis, "Robust compressed sensing using generative models," _Advances in Neural Information Processing Systems_, vol. 33, pp. 713-727, 2020.
* [26] Y. Jiao, D. Li, M. Liu, X. Lu, and Y. Yang, "Just least squares: Binary compressive sampling with low generative intrinsic dimension," _Journal of Scientific Computing_, vol. 95, no. 1, p. 28, 2023.
* [27] A. Kamath, E. Price, and S. Karmalkar, "On the power of compressed sensing with generative models," in _International Conference on Machine Learning_. PMLR, 2020, pp. 5101-5109.
* [28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," _Proceedings of the IEEE_, vol. 86, no. 11, pp. 2278-2324, 1998.
* [29] J. Liu and Z. Liu, "Non-iterative recovery from nonlinear observations using generative models," in _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 233-243.
* [30] Z. Liu, S. Ghosh, and J. Scarlett, "Towards sample-optimal compressive phase retrieval with sparse and generative priors," _Advances in Neural Information Processing Systems_, vol. 34, pp. 17 656-17 668, 2021.
* [31] Z. Liu, S. Gomes, A. Tiwari, and J. Scarlett, "Sample complexity bounds for 1-bit compressive sensing and binary stable embeddings with generative priors," in _International Conference on Machine Learning_. PMLR, 2020, pp. 6216-6225.
* [32] Z. Liu and J. Han, "Projected gradient descent algorithms for solving nonlinear inverse problems with generative priors," _arXiv preprint arXiv:2209.10093_, 2022.
* [33] Z. Liu and J. Scarlett, "The generalized Lasso with nonlinear observations and generative priors," _Advances in Neural Information Processing Systems_, vol. 33, pp. 19 125-19 136, 2020.
* [34] Z. Liu and J. Scarlett, "Information-theoretic lower bounds for compressive sensing with generative models," _IEEE Journal on Selected Areas in Information Theory_, vol. 1, no. 1, pp. 292-303, 2020.

* [35] Z. Liu, P. Luo, X. Wang, and X. Tang, "Deep learning face attributes in the wild," in _Proceedings of the IEEE International Conference on Computer Vision_, 2015, pp. 3730-3738.
* [36] S. Mendelson, "Upper bounds on product and multiplier empirical processes," _Stochastic Processes and their Applications_, vol. 126, no. 12, pp. 3652-3680, 2016.
* [37] S. Mendelson, "On multiplier processes under weak moment assumptions," in _Geometric Aspects of Functional Analysis: Israel Seminar (GAFA) 2014-2016_. Springer, 2017, pp. 301-318.
* [38] X. Meng and Y. Kabashima, "Quantized compressed sensing with score-based generative models," in _International Conference on Learning Representations_, 2023.
* [39] A. Naderi and Y. Plan, "Sparsity-free compressed sensing with applications to generative priors," _IEEE Journal on Selected Areas in Information Theory_, 2022.
* [40] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, "Deep learning techniques for inverse problems in imaging," _IEEE Journal on Selected Areas in Information Theory_, vol. 1, no. 1, pp. 39-56, 2020.
* [41] Y. Plan and R. Vershynin, "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach," _IEEE Transactions on Information Theory_, vol. 59, no. 1, pp. 482-494, 2012.
* [42] Y. Plan and R. Vershynin, "One-bit compressed sensing by linear programming," _Communications on Pure and Applied Mathematics_, vol. 66, no. 8, pp. 1275-1297, 2013.
* [43] Y. Plan and R. Vershynin, "The generalized lasso with non-linear observations," _IEEE Transactions on information theory_, vol. 62, no. 3, pp. 1528-1537, 2016.
* [44] Y. Plan, R. Vershynin, and E. Yudovina, "High-dimensional estimation with geometric constraints," _Information and Inference: A Journal of the IMA_, vol. 6, no. 1, pp. 1-40, 2017.
* [45] S. Qiu, X. Wei, and Z. Yang, "Robust one-bit recovery via ReLU generative networks: Near-optimal statistical rate and global landscape analysis," in _International Conference on Machine Learning_. PMLR, 2020, pp. 7857-7866.
* [46] T. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, "Compressed sensing MRI reconstruction using a generative adversarial network with a cyclic loss," _IEEE Transactions on Medical Imaging_, vol. 37, no. 6, pp. 1488-1497, 2018.
* [47] J. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C. Eldar, "Theoretical perspectives on deep learning methods in inverse problems," _IEEE Journal on Selected Areas in Information Theory_, 2022.
* [48] C. Thrampoulidis and A. S. Rawat, "The generalized lasso for sub-gaussian measurements with dithered quantization," _IEEE Transactions on Information Theory_, vol. 66, no. 4, pp. 2487-2500, 2020.
* [49] Y. Traonmilin and R. Gribonval, "Stable recovery of low-dimensional cones in Hilbert spaces: One RIP to rule them all," _Applied and Computational Harmonic Analysis_, vol. 45, no. 1, pp. 170-205, 2018.
* [50] R. Vershynin, _High-dimensional probability: An introduction with applications in data science_. Cambridge University Press, 2018, vol. 47.
* [51] J. Whang, E. Lindgren, and A. Dimakis, "Composing normalizing flows for inverse problems," in _International Conference on Machine Learning_, 2021, pp. 11 158-11 169.
* [52] C. Xu and L. Jacques, "Quantized compressive sensing with RIP matrices: The benefit of dithering," _Information and Inference: A Journal of the IMA_, vol. 9, no. 3, pp. 543-586, 2020.

**Supplementary Material**

**A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing**

**(NeurIPS 2023)**

## Appendix A Technical Lemmas

**Lemma 2**.: (Lemma 2.7.7, [50]). _Let \(X,Y\) be sub-Gaussian, then \(XY\) is sub-exponential with \(\|XY\|_{\psi_{1}}\leq\|X\|_{\psi_{2}}\|Y\|_{\psi_{2}}\)._

**Lemma 3**.: (Centering, [50, Exercise 2.7.10]). _For some absolute constant \(C\), \(\|X-\mathbbm{E}X\|_{\psi_{1}}\leq C\|X\|_{\psi_{1}}\)._

**Lemma 4**.: (Bernstein's inequality, [50, Theorem 2.8.1]). _Let \(X_{1},...,X_{N}\) be independent, zero-mean, sub-exponential random variables. Then for every \(t\geq 0\), for some absolute constant \(c\) we have_

\[\mathbb{P}\left(\bigg{|}\sum_{i=1}^{N}X_{i}\bigg{|}\geq t\right)\leq 2\exp \left(-c\min\Big{\{}\frac{t^{2}}{\sum_{i=1}^{N}\|X_{i}\|_{\psi_{1}}^{2}},\frac{ t}{\max_{1\leq i\leq N}\|X_{i}\|_{\psi_{1}}}\Big{\}}\right)\]

**Lemma 5**.: ([36], statement adapted from [16, Theorem 8]). _Let \(g_{\bm{x}}=g_{\bm{x}}(\bm{a})\) and \(h_{\bm{v}}=h_{\bm{v}}(\bm{a})\) be stochastic processes indexed by \(\bm{x}\in\mathcal{X}\subset\mathbb{R}^{p_{1}}\), \(\bm{v}\in\mathcal{V}\subset\mathbb{R}^{p_{2}}\), both defined on some common random variable \(\bm{a}\). Assume that **(A1.)** in Theorem 2 holds, and let \(\bm{a}_{1},...,\bm{a}_{m}\) be i.i.d. copies of \(\bm{a}\). Then for any \(u\geq 1\), with probability at least \(1-2\exp(-cu^{2})\) we have the bound_

\[\sup_{\begin{subarray}{c}\bm{x}\in\mathcal{X}\\ \bm{v}\in\mathcal{V}\end{subarray}} \bigg{|}\frac{1}{m}\sum_{i=1}^{m}\big{(}g_{\bm{x}}(\bm{a}_{i})h_ {\bm{v}}(\bm{a}_{i})-\mathbbm{E}[g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i}) ]\big{)}\bigg{|}\] (A.1) \[\leq C\Big{(}\frac{(M_{g}\cdot\omega(\mathcal{X})+u\cdot A_{g}) \cdot(M_{h}\cdot\omega(\mathcal{V})+u\cdot A_{h})}{m}\] \[\quad+\frac{A_{g}\cdot M_{h}\cdot\omega(\mathcal{V})+A_{h}\cdot M _{g}\cdot\omega(\mathcal{X})+u\cdot A_{g}A_{h}}{\sqrt{m}}\Big{)},\]

_where \(\omega(\cdot)\) is the Gaussian width defined as \(\omega(\mathcal{X})=\mathbbm{E}\sup_{\bm{x}\in\mathcal{X}}\bm{g}^{\top}\bm{x}\) where \(\bm{g}\sim\mathcal{N}(0,\bm{I}_{p_{1}})\)._

The proofs of the remaining lemmas will be provided in Appendix D. (Some simple facts such as Lemma 8 were already used in prior works; while we provide the proofs for completeness.)

**Lemma 6**.: (Metric entropy of some constraint sets). _Assume \(\mathcal{K}=G(\mathbb{B}_{2}^{k}(r))\) for some \(L\)-Lipschitz generative model \(G\). Let \(\mathcal{K}^{-}=\mathcal{K}-\mathcal{K}\), for some \(T>0,\epsilon\in(0,1)\) let \(\mathcal{K}^{-}_{\epsilon}:=(T\mathcal{K}^{-})\cap\big{(}\mathbb{B}_{2}^{n}(2 \epsilon)\big{)}^{c}\), and further define \((\mathcal{K}^{-}_{\epsilon})^{*}=\{\frac{\bm{z}}{\|\bm{z}\|_{2}}:\bm{z}\in \mathcal{K}^{-}_{\epsilon}\}\). Then for any \(\eta\in(0,Lr)\), we have_

\[\mathscr{H}(\mathcal{K},\eta)\leq k\log\frac{3Lr}{\eta},\ \mathscr{H}( \mathcal{K}^{-},\eta)\leq 2k\log\frac{6Lr}{\eta},\] \[\mathscr{H}(\mathcal{K}^{-}_{\epsilon},\eta)\leq 2k\log\frac{12 TrL}{\eta},\ \mathscr{H}\big{(}(\mathcal{K}^{-}_{\epsilon})^{*},\eta\big{)}\leq 2k \log\frac{12TLr}{\epsilon\eta},\]

_where \(\mathscr{H}(\cdot,\cdot)\) is the metric entropy defined in Definition 2._

**Lemma 7**.: (Bound the \(\ell_{2}\)-norm of Gaussian vector). _If \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\), then \(\mathbb{P}\big{(}\|\bm{a}\|_{2}-\sqrt{n}\|\geq t\big{)}\leq 2\exp(-Ct^{2})\). In particular, setting \(t\asymp\sqrt{n}\) yields \(\mathbb{P}\big{(}\|\bm{a}\|_{2}\geq\sqrt{n}\big{)}\leq 2\exp(-\Omega(n))\)._

In the following, Lemmas 8-11 indicate suitable choices of \(T\) in the concrete models we consider. These choices can make \(\rho(\bm{x})\) in (2.3) sufficiently small or even zero.

**Lemma 8**.: (Choice of \(T\) in 1-bit GCS). _If \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\), then for any \(\bm{x}\in\mathbb{S}^{n-1}\) it holds that \(\mathbb{E}[\mathrm{sign}(\bm{a}^{\top}\bm{x})\bm{a}]=\sqrt{\frac{2}{\pi}}\bm{x}\)._

**Lemma 9**.: (Choice of \(T\) in 1-bit GCS with dithering). _If \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\) and \(\tau\sim\mathscr{U}[-\lambda,\lambda]\) are independent, and \(\lambda=CR\sqrt{\log m}\) with sufficiently large \(C\), then for any \(\bm{x}\in\mathbb{B}_{2}^{n}(R)\) it holds that \(\|\mathbb{E}[\mathrm{sign}(\bm{a}^{\top}\bm{x}+\tau)\bm{a}]-\frac{\bm{x}}{ \lambda}\|_{2}=O\big{(}m^{-9}\big{)}\)._

**Lemma 10**.: (Choice of \(T\) in SIM). _If \(\bm{a}\sim\mathcal{N}(0,\bm{I}_{n})\), for some function \(f\) and any \(\bm{x}\in\mathbb{S}^{n-1}\) it holds that \(\mathbb{E}[f(\bm{a}^{\top}\bm{x})\bm{a}]=\mu\bm{x}\) for \(\mu=\mathbb{E}[f(g)g]\) with \(g\sim\mathcal{N}(0,1)\)._

**Lemma 11**.: (Choice of \(T\) in uniformly quantized GCS with dithering). _Given any \(\delta>0\), let \(\tau\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\) and \(\mathcal{Q}_{\delta}(\cdot)=\delta\big{(}\lfloor\frac{\delta}{2}\rfloor+\frac {1}{2}\big{)}\). Then, for any \(a\in\mathbb{R}\), it holds that \(\mathbb{E}[\mathcal{Q}_{\delta}(a+\tau)]=a\). In particular, let \(\bm{a}\in\mathbb{R}^{n}\) be a random vector satisfying \(\mathbb{E}(\bm{a}\bm{a}^{\top})=\bm{I}_{n}\), and \(\tau\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\) be independent of \(\bm{a}\), then we have \(\mathbb{E}[\mathcal{Q}_{\delta}(\bm{a}^{\top}\bm{x}+\tau)\bm{a}]=\bm{x}\)._

Lemma 12 facilitates our analysis of the uniform quantizer.

**Lemma 12**.: _Let \(f_{i}(\cdot)=\delta\big{(}\lfloor\frac{+\tau_{i}}{\delta}\rfloor+\frac{1}{2} \big{)}\) for \(\tau_{i}\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\), and \(f_{i,\beta}(\cdot)\) be defined in (3.4) for some \(0<\beta<\frac{\delta}{2}\). Moreover, let \(\xi_{i,\beta}(a)=f_{i,\beta}(a)-a\), \(\varepsilon_{i,\beta}(a)=f_{i,\beta}(a)-f_{i}(a)\), then for any \(a\in\mathbb{R}\), \(|\xi_{i,\beta}(a)|\leq 2\delta\), \(|\varepsilon_{i,\beta}(a)|\leq\delta\) holds deterministically._

More generally, the approximation error \(|\varepsilon_{i,\beta}(a)|\) can always be bounded as follows.

**Lemma 13**.: _Suppose that \(f_{i}\) satisfies Assumption 2, and for any \(\beta\in[0,\frac{\beta_{0}}{2}]\) we construct \(f_{i,\beta}\) as in (3.4). Then, for any \(a\in\mathbb{R}\), we have \(|\varepsilon_{i,\beta}(a)|\leq\big{(}\frac{3L_{0}\beta}{2}+B_{0}\big{)}1(a\in \mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}])\)._

## Appendix B More Details of the Proof Sketch

### Set-Restricted Eigenvalue Condition

**Definition 4**.: _Let \(\mathcal{S}\subset\mathbb{R}^{n}\). For parameters \(\gamma,\delta>0\), a matrix \(\bm{A}\in\mathbb{R}^{m\times n}\) is said to satisfy S-REC(\(\mathcal{S},\gamma,\delta\)) if the following holds:_

\[\|\bm{A}(\bm{x}_{1}-\bm{x}_{2})\|_{2}\geq\gamma\|\bm{x}_{1}-\bm{x}_{2}\|_{2}- \delta,\;\forall\;\bm{x}_{1},\bm{x}_{2}\in\mathcal{S}.\]

It was proved in [2] that \(\frac{1}{\sqrt{m}}\bm{A}\) satisfies the S-REC with high probability if the entries of \(\bm{A}\) are i.i.d. standard Gaussian.

**Lemma 14**.: (Lemma 4.1 in [2])_. Let \(G:\mathbb{B}_{2}^{k}(r)\to\mathbb{R}^{n}\) be L-Lipschitz for some \(r,L>0\), and define \(\mathcal{K}=G(\mathbb{B}_{2}^{k}(r))\). For any \(\alpha\in(0,1)\), if \(\bm{A}\in\mathbb{R}^{m\times n}\) has i.i.d. \(\mathcal{N}(0,1)\) entries, and \(m=\Omega\big{(}\frac{k}{\alpha^{2}}\log\frac{Lr}{\delta}\big{)}\), then \(\frac{1}{\sqrt{m}}\bm{A}\) satisfies S-REC(\(\mathcal{K}\),\(1-\alpha\),\(\delta\)) with probability at least \(1-\exp(-\Omega(\alpha^{2}m))\)._

### Lipschitz Approximation

The approximation error \(\varepsilon_{i,\beta}(\cdot)\) can be expanded as:

\[\varepsilon_{i,\beta}(x)=\begin{cases}\phantom{-}0\phantom{-}0\phantom{-},& \text{if }x\notin\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]\\ f_{i}^{a}(x_{0})-f_{i}(x)-\frac{2[f_{i}^{a}(x_{0})-f_{i}(x_{0}-\frac{\beta}{2} )](x_{0}-x)}{\beta},&\text{if }x\in[x_{0}-\frac{\beta}{2},x_{0}],\;(\exists x_{0}\in \mathscr{D}_{f_{i}})\\ f_{i}^{a}(x_{0})-f_{i}(x)+\frac{2[f_{i}(x_{0}+\frac{\beta}{2})-f_{i}^{a}(x_{0}) ](x-x_{0})}{\beta},&\text{if }x\in[x_{0},x_{0}+\frac{\beta}{2}],\;(\exists x_{0}\in \mathscr{D}_{f_{i}})\end{cases}.\]

Although \(|\varepsilon_{i,\beta}|\) is Lipschitz continuous, \(\varepsilon_{i,\beta}\) is not. In particular, given \(x_{0}\in\mathscr{D}_{f_{i}}\) we note that

\[\varepsilon_{i,\beta}^{-}(x_{0})=\lim_{x\to x_{0}^{-}}\varepsilon_{i,\beta}(x) =f_{i}^{a}(x_{0})-f_{i}^{-}(x_{0})=\frac{1}{2}\big{(}f_{i}^{+}(x_{0})-f_{i}^{- }(x_{0})\big{)},\]

\[\varepsilon_{i,\beta}^{+}(x_{0})=\lim_{x\to x_{0}^{+}}\varepsilon_{i,\beta}(x) =f_{i}^{a}(x_{0})-f_{i}^{+}(x_{0})=\frac{1}{2}\big{(}f_{i}^{-}(x_{0})-f_{i}^{+ }(x_{0})\big{)}.\]

Thus, it is crucial to include the absolute value for rendering the continuity.

## Appendix C Proofs of Main Results

### Proof of Theorem 1

Proof.: Up to rescaling, we only need to prove that \(\|\hat{\bm{x}}-T\bm{x}^{\star}\|_{2}\leq 3\epsilon\) holds uniformly for all \(\bm{x}^{\star}\in\mathcal{K}\). We can assume \(\|\hat{\bm{x}}-T\bm{x}^{\star}\|_{2}\geq 2\epsilon\); otherwise, the desired bound is immediate.

**(1) Lower bounding the left-hand side of (3.1).**

We use S-REC to find a lower bound for \(\big{\|}\frac{\bm{A}}{\sqrt{m}}(\hat{\bm{x}}-Tx^{\bm{\star}})\big{\|}_{2}^{2}\). Specifically, we invoke Lemma 14 with \(\alpha=\frac{1}{2}\) and \(\delta=\frac{\epsilon}{2T}\), which gives that under \(m=\Omega\big{(}k\log\frac{LT_{T}}{\epsilon}\big{)}\), with probability at least \(1-\exp(-cm)\), the following holds:

\[\bigg{\|}\frac{\bm{A}}{\sqrt{m}}(\bm{x}_{1}-\bm{x}_{2})\bigg{\|}_{2}\geq\frac{1 }{2}\|\bm{x}_{1}-\bm{x}_{2}\|_{2}-\frac{\epsilon}{2T},\,\forall\,\,\bm{x}_{1}, \bm{x}_{2}\in\mathcal{K}.\] (C.1)

Recall that we assume \(\|\hat{\bm{x}}-Tx^{\bm{\star}}\|_{2}\geq 2\epsilon\), \(T^{-1}\hat{\bm{x}}\in\mathcal{K}\), and \(\bm{x}^{\star}\in\mathcal{K}\), so we set \(\bm{x}_{1}=\frac{\hat{\bm{x}}}{T},\bm{x}_{2}=\bm{x}^{\star}\) in (C.1) to obtain

\[\bigg{\|}\frac{\bm{A}}{\sqrt{m}}\Big{(}\frac{\hat{\bm{x}}}{T}-\bm{x}^{\star} \Big{)}\bigg{\|}_{2}\geq\frac{1}{2}\left\|\frac{\hat{\bm{x}}}{T}-\bm{x}^{\star }\right\|_{2}-\frac{\epsilon}{2T}\geq\frac{1}{4T}\big{\|}\hat{\bm{x}}-Tx^{\star }\big{\|}_{2}.\]

Thus, the left-hand side of (3.1) can be lower bounded by \(\Omega\big{(}\|\hat{\bm{x}}-Tx^{\star}\|_{2}^{2}\big{)}\).

**(2) Upper bounding the right-hand side of (3.1).**

As analysed in (3.3) and (3.5), the right-hand side of (3.1) is bounded by \(2\|\hat{\bm{x}}-Tx^{\star}\|_{2}\cdot\big{(}\mathscr{R}_{u1}+\mathscr{R}_{u2} \big{)}\), so all that remains is to bound \(\mathscr{R}_{u1},\mathscr{R}_{u2}\). In the rest of the proof, we simply write \(\sup_{\bm{x},\bm{v}}:=\sup_{\bm{x}\in\mathcal{K},\bm{v}\in(\mathcal{K}_{c}^{ \top})^{\star}}\) and recall the shorthand \(\xi_{i,\beta}(a)=f_{i,\beta}(a)-Ta\). Thus, the first factor of \(\mathscr{R}_{u1}\) is given by \(\xi_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\). By centering, we have

\[\mathscr{R}_{u1}\leq\underbrace{\sup_{\bm{x},\bm{v}}\frac{1}{m}\sum_{i=1}^{m} \big{\{}[\xi_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})](\bm{a}_{i}^{\top}\bm{v})- \mathbb{E}\big{[}[\xi_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})](\bm{a}_{i}^{\top}\bm {v})]\big{\}}}_{\mathscr{R}_{u1,c}}+\underbrace{\sup_{\bm{x},\bm{v}}\mathbb{E} \big{[}[\xi_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})](\bm{a}_{i}^{\top}\bm{v})\big{]} }_{\mathscr{R}_{u1,c}},\] (C.2)

and

\[\mathscr{R}_{u2}\leq\underbrace{\sup_{\bm{x},\bm{v}}\bigg{\{}\frac{1}{m}\sum_ {i=1}^{m}\big{|}\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\big{|}\big{|} \bm{a}_{i}^{\top}\bm{v}\big{|}-\mathbb{E}\big{[}\big{|}\varepsilon_{i,\beta}( \bm{a}_{i}^{\top}\bm{x})\big{|}\big{|}\bm{a}_{i}^{\top}\bm{v}\big{|}\big{]} \bigg{\}}}_{\mathscr{R}_{u2,c}}+\underbrace{\sup_{\bm{x},\bm{v}}\mathbb{E} \big{[}\big{|}\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\big{|}\big{|} \bm{a}_{i}^{\top}\bm{v}\big{|}\big{]}}_{\mathscr{R}_{u2,c}}.\] (C.3)

We will invoke Theorem 2 multiple times to derive the required bounds.

**(2.1) Bounding the centered product process \(\mathscr{R}_{u1,c^{\star}}\)**

We let \(g_{\bm{x}}(\bm{a}_{i})=\xi_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\) and \(h_{\bm{v}}(\bm{a}_{i})=\bm{a}_{i}^{\top}\bm{v}\), and write

\[\mathscr{R}_{u1,c}=\sup_{\bm{x},\bm{v}}\frac{1}{m}\sum_{i=1}^{m}\big{\{}g_{\bm{ x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})-\mathbb{E}[g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})] \big{\}}.\]

We verify conditions in Theorem 2 as follows:

* For any \(\bm{x},\bm{x}^{\prime}\in\mathcal{K}\), because \(\xi_{i,\beta}\) is \(\big{(}L_{0}+\frac{B_{0}}{\beta}+T\big{)}\)-Lipschitz continuous (Lemma 1), we have \[\|g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})\|_{\psi_{ 2}} \leq(L_{0}+T+\frac{B_{0}}{\beta})\|\bm{a}_{i}^{\top}\bm{x}-\bm{a}_{i}^{ \top}\bm{x}^{\prime}\|_{\psi_{2}}\] \[=O\big{(}L_{0}+T+\frac{B_{0}}{\beta}\big{)}\|\bm{x}-\bm{x}^{\prime} \|_{2}.\]
* Since \(\bm{a}_{i}\sim\mathcal{N}(\bm{0},\bm{I}_{n})\), by Lemma 7, with probability \(1-2\exp(-\Omega(n))\) we have \(\|\bm{a}_{i}\|_{2}=O(\sqrt{n})\). On this event, we have \[|g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})| \leq(L_{0}+T+\frac{B_{0}}{\beta})|\bm{a}_{i}^{\top}\bm{x}-\bm{a} _{i}^{\top}\bm{x}^{\prime}|\] \[\leq(L_{0}+T+\frac{B_{0}}{\beta})\|\bm{a}_{i}\|_{2}\|\bm{x}-\bm{x} ^{\prime}\|_{2}\] \[=O\Big{(}\sqrt{n}\Big{[}L_{0}+T+\frac{B_{0}}{\beta}\Big{]}\Big{)} \|\bm{x}-\bm{x}^{\prime}\|_{2}.\]* Recall \((\mathcal{K}_{\epsilon}^{-})^{*}\) in (3.2). Since \((\mathcal{K}_{\epsilon}^{-})^{*}\subset\mathbb{S}^{n-1}\), for any \(\bm{v},\bm{v}^{\prime}\in(\mathcal{K}_{\epsilon}^{-})^{*}\), we have \(\|\bm{a}_{i}^{\top}\bm{v}-\bm{a}_{i}^{\top}\bm{v}^{\prime}\|_{\psi_{2}}=O(1) \|\bm{v}-\bm{v}^{\prime}\|_{2}\), and when \(\|\bm{a}_{i}\|=O(\sqrt{n})\) we have \(|\bm{a}_{i}^{\top}\bm{v}-\bm{a}_{i}^{\top}\bm{v}^{\prime}|\leq\|\bm{a}_{i}\|_{ 2}\|\bm{v}-\bm{v}^{\prime}\|_{2}=O(\sqrt{n})\|\bm{v}-\bm{v}^{\prime}\|_{2}\). Moreover, because \((\mathcal{K}_{\epsilon}^{-})^{*}\subset\mathbb{B}_{2}^{n}\), for any \(\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}\) we have \(\|\bm{a}_{i}^{\top}\bm{v}\|_{\psi_{2}}=O(1)\), \(|\bm{a}_{i}^{\top}\bm{v}|\leq\|\bm{a}_{i}\|_{2}=O(\sqrt{n})\).

Combined with Assumption 3 and its parameters \((A_{g}^{(1)},U_{g}^{(1)},P_{0}^{(1)})\) and \((A_{g}^{(2)},U_{g}^{(2)},P_{0}^{(2)})\), \(\mathscr{R}_{u1,c}\) satisfies the conditions of Theorem 2 with the following parameters

\[M_{g}\asymp L_{0}+T+\frac{B_{0}}{\beta},\;A_{g}=A_{g}^{(1)},\;M_ {h}\asymp 1,\;A_{h}\asymp 1\] \[L_{g}\asymp\sqrt{n}\big{(}L_{0}+T+\frac{B_{0}}{\beta}\big{)},\;U_ {g}=U_{g}^{(1)},\;L_{h}\asymp\sqrt{n},\;U_{h}\asymp\sqrt{n}\]

and \(P_{0}=P_{0}^{(1)}+2\exp(-\Omega(n))\). Now suppose that we have

\[m\gtrsim\mathscr{H}\left(\mathcal{K},\frac{A_{g}^{(1)}}{\sqrt{mn}[L_{0}+T+ \frac{B_{0}}{\beta}]}\right)+\mathscr{H}\left((\mathcal{K}_{\epsilon}^{-})^{* },\frac{A_{g}^{(1)}}{\sqrt{m}(\sqrt{n}U_{g}^{(1)}+A_{g}^{(1)})}\right),\] (C.4)

and note that by using Lemma 6, (C.4) can be guaranteed by

\[m\gtrsim k\log\left(\frac{Lr\sqrt{m}}{A_{g}^{(1)}}\Big{[}n(L_{0}+T+\frac{B_{0 }}{\beta})+\frac{T(\sqrt{n}U_{g}^{(1)}+A_{g}^{(1)})}{\epsilon}\Big{]}\right).\] (C.5)

Then Theorem 2 yields that the following bound holds with probability at least \(1-mP_{0}^{(1)}-C\exp(-\Omega(k))-m\exp(-\Omega(n))\):

\[|\mathscr{R}_{u1,c}| \lesssim\frac{A_{g}^{1}}{\sqrt{m}}\sqrt{\mathscr{H}\left( \mathcal{K},\frac{A_{g}^{(1)}}{\sqrt{mn}[L_{0}+T+\frac{B_{0}}{\beta}]} \right)+\mathscr{H}\left((\mathcal{K}_{\epsilon}^{-})^{*},\frac{A_{g}^{(1)}} {\sqrt{m}(\sqrt{n}U_{g}^{(1)}+A_{g}^{(1)})}\right)}\] (C.6) \[\lesssim A_{g}^{(1)}\sqrt{\frac{k}{m}\log\left(\frac{Lr\sqrt{m}}{A _{g}^{(1)}}\Big{[}n(L_{0}+T+\frac{B_{0}}{\beta})+\frac{T(\sqrt{n}U_{g}^{(1)}+ A_{g}^{(1)})}{\epsilon}\Big{]}\right)}.\]

**(2.2) Bounding the centered product process \(\mathscr{R}_{u2,c}\)**

We let \(g_{\bm{x}}(\bm{a}_{i})=|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})|\) and \(h_{\bm{v}}(\bm{a}_{i})=|\bm{a}_{i}^{\top}\bm{v}|\), and write

\[\mathscr{R}_{u2,c}=\sup_{\bm{x},\bm{v}}\frac{1}{m}\sum_{i=1}^{m}\big{\{}g_{\bm {x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})-\mathbb{E}[g_{\bm{x}}(\bm{a}_{i})h_{\bm {v}}(\bm{a}_{i})]\big{\}}.\]

We verify the conditions in Theorem 2 as follows:

* For any \(\bm{x},\bm{x}^{\prime}\in\mathcal{K}\), because \(|\varepsilon_{i,\beta}|\) is \((2L_{0}+\frac{B_{0}}{\beta})\)-Lipschitz continuous (Lemma 1), we have \[\|g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})\|_{\psi_{ 2}} \leq(2L_{0}+\frac{B_{0}}{\beta})\|\bm{a}_{i}^{\top}\bm{x}-\bm{a}_{i}^{ \top}\bm{x}^{\prime}\|_{\psi_{2}}\] \[=O\big{(}L_{0}+\frac{B_{0}}{\beta}\big{)}\|\bm{x}-\bm{x}^{\prime} \|_{2}.\]
* By Lemma 7, with probability at least \(1-2\exp(-\Omega(n))\) we have \(\|\bm{a}_{i}\|_{2}=O(\sqrt{n})\). On this event, we have \[|g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})| \leq(2L_{0}+\frac{B_{0}}{\beta})|\bm{a}_{i}^{\top}\bm{x}-\bm{a}_{i }^{\top}\bm{x}^{\prime}|\] \[\leq(2L_{0}+\frac{B_{0}}{\beta})\|\bm{a}_{i}\|_{2}\|\bm{x}-\bm{x}^ {\prime}\|_{2}\] \[=O\big{(}\sqrt{n}\big{[}L_{0}+\frac{B_{0}}{\beta}\big{]}\big{)}\| \bm{x}-\bm{x}^{\prime}\|_{2}.\]* For any \(\bm{v},\bm{v}^{\prime}\in(\mathcal{K}_{\epsilon}^{-})^{*}\) we have \(\left\|\bm{a}_{i}^{\top}\bm{v}|-|\bm{a}_{i}^{\top}\bm{v}^{\prime}|\right\|_{ \varphi_{2}}\leq\|\bm{a}_{i}^{\top}(\bm{v}-\bm{v}^{\prime})\|_{\psi_{2}}=O(1) \|\bm{v}-\bm{v}^{\prime}\|_{2}\). Similarly as before, we assume \(\|\bm{a}_{i}\|_{2}=O(\sqrt{n})\), which gives \(|\bm{a}_{i}^{\top}\bm{v}-\bm{a}_{i}^{\top}\bm{v}^{\prime}|\leq\|\bm{a}_{i}\|_ {2}\|\bm{v}-\bm{v}^{\prime}\|_{2}=O(\sqrt{n})\|\bm{v}-\bm{v}^{\prime}\|_{2}\). Moreover, \((\mathcal{K}_{\epsilon}^{-})^{*}\subset\mathbb{B}_{2}^{n}\) implies \(\left\|\left|\bm{a}_{i}^{\top}\bm{v}\right|\right\|_{\varphi_{2}}=O(1)\) and \(|\bm{a}_{i}^{\top}\bm{v}|\leq\|\bm{a}_{i}\|_{2}\|\bm{v}\|_{2}=O(\sqrt{n})\) holds for all \(\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}\).

Combined with Assumption 3, \(\mathscr{R}_{u2,c}\) satisfies the conditions of Theorem 2 with

\[M_{g}\asymp L_{0}+\frac{B_{0}}{\beta},\;A_{g}=A_{g}^{(2)},\;M_{h}\asymp 1,\;A_{h}\asymp O(1)\]

and \(P_{0}=P_{0}^{(2)}+2\exp(-\Omega(n))\). Suppose we have

\[m\gtrsim\mathscr{H}\left(\mathcal{K},\frac{A_{g}^{(2)}}{\sqrt{m}n[L_{0}+\frac{ B_{0}}{\beta}]}\right)+\mathscr{H}\left((\mathcal{K}_{\epsilon}^{-})^{*},\frac{A_{g} ^{(2)}}{\sqrt{m}(A_{g}^{(2)}+\sqrt{n}U_{g}^{(2)})}\right),\]

which can be guaranteed (from Lemma 6) by

\[m\gtrsim k\log\left(\frac{Lr\sqrt{m}}{A_{g}^{(2)}}\Big{[}n\big{(}L_{0}+\frac{ B_{0}}{\beta}\big{)}+\frac{T(A_{g}^{(2)}+\sqrt{n}U_{g}^{(2)})}{\epsilon} \Big{]}\right).\] (C.7)

Then, we can invoke Theorem 2 to obtain that the following bound holds with probability at least \(1-mP_{0}^{(2)}-2m\exp(-\Omega(n))-C\exp(-\Omega(k))\):

\[\begin{split}|\mathscr{R}_{u2,c}|&\lesssim\frac{A_ {g}^{(2)}}{\sqrt{m}}\sqrt{\mathscr{H}\left(\mathcal{K},\frac{A_{g}^{(2)}}{ \sqrt{m}n[L_{0}+\frac{B_{0}}{\beta}]}\right)+\mathscr{H}\left((\mathcal{K}_{ \epsilon}^{-})^{*},\frac{A_{g}^{(2)}}{\sqrt{m}(A_{g}^{(2)}+\sqrt{n}U_{g}^{(2) })}\right)}\\ &\lesssim A_{g}^{(2)}\sqrt{\frac{k}{m}\log\left(\frac{Lr\sqrt{m} }{A_{g}^{(2)}}\Big{[}n\big{(}L_{0}+\frac{B_{0}}{\beta}\big{)}+\frac{T(A_{g}^{ (2)}+\sqrt{n}U_{g}^{(2)})}{\epsilon}\Big{]}\right)}.\end{split}\] (C.8)

**(2.3) Bounding the expectation terms \(\mathscr{R}_{u1,e},\mathscr{R}_{u2,e}\).**

Recall that \(\xi_{i,\beta}(a)=f_{i,\beta}(a)-Ta\) and \(\varepsilon_{i,\beta}(a)=f_{i,\beta}(a)-f_{i}(a)\), and so \(\xi_{i,\beta}(a)=\varepsilon_{i,\beta}(a)+f_{i}(a)-Ta\). Hence, by using \(\mathbb{E}[\bm{a}_{i}\bm{a}_{i}^{\top}]=\bm{I}_{n}\) and \(\|\bm{v}\|_{2}=1\), we have

\[\begin{split}\mathscr{R}_{u1,e}&\leq\sup_{\bm{x}, \bm{v}}\mathbb{E}\big{[}\big{(}f_{i}(\bm{a}_{i}^{\top}\bm{x})-\bm{Ta}_{i}^{\top} \bm{x})(\bm{a}_{i}^{\top}\bm{v})\big{]}+\sup_{\bm{x},\bm{v}}\mathbb{E}\big{[} \big{[}\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\big{]}\bm{a}_{i}^{\top} \bm{v}\big{]}\\ &\leq\sup_{\bm{x}\in\mathcal{X}}\big{\|}\mathbb{E}[f_{i}(\bm{a}_{ i}^{\top}\bm{x})\bm{a}_{i}]-T\bm{x}\big{\|}_{2}+\sup_{\bm{x},\bm{v}}\mathbb{E} \big{[}\big{[}\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})\big{|}\bm{a}_{i}^{ \top}\bm{v}\big{]}\big{]}\\ &\leq\sup_{\bm{x}\in\mathcal{X}}\rho(\bm{x})+\mathscr{R}_{u2,e}, \end{split}\] (C.9)

where \(\rho(\bm{x})\) is the model mismatch defined in (2.3), and \(\mathscr{R}_{u2,e}\) is defined in (C.3). It remains to bound \(\mathscr{R}_{u2,e}\), for which we first apply Cauchy-Schwarz (with \(\|\bm{v}\|_{2}\leq 1\)) and then use Lemma 13 to obtain

\[\begin{split}&\mathscr{R}_{u2,e}=\sup_{\bm{x},\bm{v}}\mathbb{E} \big{[}|\varepsilon_{i,\beta}(\bm{a}_{i}^{\top}\bm{x})||\bm{a}_{i}^{\top}\bm{v}| \big{]}\\ &\leq\sup_{\bm{x},\bm{v}}\sqrt{\mathbb{E}[|\varepsilon_{i,\beta} (\bm{a}_{i}^{\top}\bm{x})|^{2}]}\sqrt{\mathbb{E}[|\bm{a}_{i}^{\top}\bm{v}|^{2} ]}\\ &\leq\sup_{\bm{x}\in\mathcal{K}}\sqrt{\mathbb{E}\left[|\varepsilon_ {i,\beta}(\bm{a}_{i}^{\top}\bm{x})|^{2}1\Big{(}\bm{a}_{i}^{\top}\bm{x}\in\mathscr{ D}_{f_{i}}+\Big{[}-\frac{\beta}{2},\frac{\beta}{2}\Big{]}\Big{)}\right]}\\ &\leq\Big{(}\frac{3L_{0}\beta}{2}+B_{0}\Big{)}\sup_{\bm{x}\in \mathcal{K}}\sqrt{\mathbb{P}\Big{(}\bm{a}_{i}^{\top}\bm{x}\in\mathscr{D}_{f_{i}}+ \Big{[}-\frac{\beta}{2},\frac{\beta}{2}\Big{]}\Big{)}}\\ &\leq\Big{(}\frac{3L_{0}\beta}{2}+B_{0}\Big{)}\sup_{\bm{x}\in \mathcal{K}}\sqrt{\mu_{\beta}(\bm{x})},\end{split}\] (C.10)where we use Lemma 13 in the third and fourth line, and \(\mu_{\beta}(\bm{x})\) is defined in (2.4).

**(3) Combining everything to conclude the proof.**

Recall that in Assumption 4, we assume that

\[\sup_{\bm{x}\in\mathcal{X}}\rho(\bm{x})\lesssim(A_{g}^{(1)}\lor A_{g}^{(2)}) \sqrt{\frac{k}{m}},\]

and we take sufficiently small \(\beta_{1}\) such that

\[(L_{0}\beta_{1}+B_{0})\sup_{\bm{x}\in\mathcal{K}}\sqrt{\mu_{\beta_{1}}(\bm{x}) }\lesssim(A_{g}^{(1)}\lor A_{g}^{(2)})\sqrt{\frac{k}{m}},\]

then by setting \(\beta=\beta_{1}\), the derived bound of \(\mathscr{R}_{u1,c}+\mathscr{R}_{u2,c}\) (see (C.6) and (C.8)) dominates that of \(\mathscr{R}_{u1,e}+\mathscr{R}_{u2,e}\) (see (C.9) and (C.10)), and so \(\mathscr{R}_{u1}+\mathscr{R}_{u2}\lesssim\mathscr{R}_{u1,c}+\mathscr{R}_{u2,c}\).

Recall that (C.6) and (C.8) are guaranteed by the sample size of (C.5) and (C.7), while (C.5) and (C.7) hold as long as

\[m \gtrsim k\log\left(\frac{Lr\sqrt{m}}{A_{g}^{(1)}\wedge A_{g}^{(2) }}\left[n\big{(}L_{0}+T+\frac{B_{0}}{\beta_{1}}\big{)}+\frac{T(\sqrt{n}(U_{g}^ {(1)}\lor U_{g}^{(2)})+(A_{g}^{(1)}\lor A_{g}^{(2)}))}{\epsilon}\right]\right)\] (C.11) \[:=k\mathscr{L}\] (here we use \[\mathscr{L}\] to abbreviate the log factors)

with probability at least \(1-m(P_{0}^{(1)}+P_{0}^{(2)})-m\exp(-\Omega(n))-C\exp(-\Omega(k))\) we have

\[\mathscr{R}_{u1,c}+\mathscr{R}_{u2,c}\lesssim(A_{g}^{(1)}\lor A_{g}^{(2)}) \sqrt{\frac{k\mathscr{L}}{m}}.\]

Therefore, the right-hand side of (3.1) can be uniformly bounded by

\[O\left(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\cdot(A_{g}^{(1)}\lor A_{g}^{(2)}) \sqrt{\frac{k\mathscr{L}}{m}}\right).\] (C.12)

Combining with the uniform lower bound for the left-hand side of (3.1), i.e., \(\Omega(\|\bm{\hat{x}}-Tx^{\star}\|_{2}^{2})\), we obtain the following bound uniformly for all \(\bm{x}^{\star}\):

\[\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\lesssim(A_{g}^{(1)}\lor A_{g}^{(2)}) \sqrt{\frac{k\mathscr{L}}{m}}.\]

Hence, as long as

\[m\gtrsim\big{(}A_{g}^{(1)}\lor A_{g}^{(2)})^{2}\frac{k\mathscr{L}}{\epsilon^{ 2}},\]

we again obtain \(\|\bm{\hat{x}}-T\bm{x}^{\star}\|_{2}\leq 3\epsilon\), which completes the proof. 

### Proof of Theorem 2

Proof.: **Step 1. Control the process over finite nets.**

Recall that \(\mathcal{X}\) and \(\mathcal{V}\) are the index sets of \(\bm{x}\) and \(\bm{v}\), as stated in Theorem 2. We first establish the desired concentration for a fixed pair \((\bm{x},\bm{v})\in\mathcal{X}\times\mathcal{V}\). By Lemma 2, \(\|g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})\|_{\psi_{1}}\leq\|g_{\bm{x}}( \bm{a}_{i})\|_{\psi_{2}}\|h_{\bm{v}}(\bm{a}_{i})\|_{\psi_{2}}\leq A_{g}A_{h}\). Furthermore, centering (Lemma 3) gives

\[\|g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})-\mathbb{E}[g_{\bm{x}}(\bm{a}_{i })h_{\bm{v}}(\bm{a}_{i})]\|_{\psi_{1}}=O(A_{g}A_{h}).\]

Thus, for fixed \((\bm{x},\bm{v})\in\mathcal{X}\times\mathcal{V}\) we define

\[I_{\bm{x},\bm{v}}=\frac{1}{m}\sum_{i=1}^{m}g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}( \bm{a}_{i})-\mathbb{E}[g_{\bm{x}}(\bm{a}_{i})h_{\bm{v}}(\bm{a}_{i})].\]

Then, we can invoke Bernstein's inequality (Lemma 4) to obtain for any \(t\geq 0\) that

\[\mathbb{P}\big{(}|I_{\bm{x},\bm{v}}|\geq t\big{)}\leq 2\exp\left(-cm\min\left\{ \Big{(}\frac{t}{A_{g}A_{h}}\Big{)}^{2},\frac{t}{A_{g}A_{h}}\right\}\right).\] (C.13)

[MISSING_PAGE_FAIL:19]

On the other hand, we bound \(\mathrm{err}_{2}\) using assumption **(A1.)**in the theorem statement. Noting that \(\mathbb{E}|X|=O(\|X\|_{\psi_{1}})\)[50, Proposition 2.7.1(b)], and further applying Lemma 2 we obtain

\[\begin{split}&\mathrm{err}_{2}\lesssim\|g_{\bm{x}}(\bm{a}_{i})h_{ \bm{v}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})h_{\bm{v}^{\prime}}(\bm{a}_ {i})\|_{\psi_{1}}\\ &\leq\|(g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i}))h _{\bm{v}}(\bm{a}_{i})\|_{\psi_{1}}+\|g_{\bm{x}^{\prime}}(\bm{a}_{i})(h_{\bm{v }}(\bm{a}_{i})-h_{\bm{v}^{\prime}}(\bm{a}_{i}))\|_{\psi_{1}}\\ &\leq\|g_{\bm{x}}(\bm{a}_{i})-g_{\bm{x}^{\prime}}(\bm{a}_{i})\|_ {\psi_{2}}\|h_{\bm{v}}(\bm{a}_{i})\|_{\psi_{2}}+\|g_{\bm{x}^{\prime}}(\bm{a}_ {i})\|_{\psi_{2}}\|h_{\bm{v}}(\bm{a}_{i})-h_{\bm{v}^{\prime}}(\bm{a}_{i})\|_{ \psi_{2}}\\ &\leq M_{g}\cdot\|\bm{x}-\bm{x}^{\prime}\|_{2}\cdot A_{h}+A_{g} \cdot M_{h}\cdot\|\bm{v}-\bm{v}^{\prime}\|_{2}\leq M_{g}A_{h}\eta_{1}+A_{g}M_{ h}\eta_{2}.\end{split}\] (C.19)

Note that the bounds (C.18) and (C.19) hold uniformly for all \((\bm{x},\bm{v})\in\mathcal{X}\times\mathcal{V}\), and hence, we can substitute them into (C.16) and (C.17) to obtain

\[\sup_{\begin{subarray}{c}\bm{x}\in\mathcal{X}\\ \bm{v}\in\mathcal{V}\end{subarray}}|I_{\bm{x},\bm{v}}|\leq O\left(A_{g}A_{h} \sqrt{\frac{\mathscr{H}(\mathcal{X},\eta_{1})+\mathscr{H}(\mathcal{V},\eta_{ 2})}{m}}\right)+(L_{g}U_{h}+M_{g}A_{h})\eta_{1}+(L_{h}U_{g}+M_{h}A_{g})\eta_{2}.\] (C.20)

Recall that we use the shorthand \(S_{g,h}:=L_{g}U_{h}+M_{g}A_{h}\) and \(T_{g,h}:=L_{h}U_{g}+M_{h}A_{g}\). We set \(\eta_{1}\asymp\frac{A_{g}A_{h}}{\sqrt{m}S_{g,h}}\), \(\eta_{2}\asymp\frac{A_{g}A_{h}}{\sqrt{m}T_{g,h}}\) so that the right-hand side of (C.20) is dominated by the first term. Overall, with a sample size satisfying

\[m=\Omega\left(\mathscr{H}\Big{(}\mathcal{X},\frac{A_{g}A_{h}}{\sqrt{m}S_{g,h} }\Big{)}+\mathscr{H}\Big{(}\mathcal{V},\frac{A_{g}A_{h}}{\sqrt{m}T_{g,h}}\Big{)} \right),\] (C.21)

we can bound \(I=\sup_{\bm{x}\in\mathcal{X}}\sup_{\bm{v}\in\mathcal{V}}|I_{\bm{x},\bm{v}}|\) (defined in the theorem statement) as

\[I\lesssim A_{g}A_{h}\sqrt{\frac{\mathscr{H}(\mathcal{X},A_{g}A_{h}m^{-1/2}S_ {g,h}^{-1})+\mathscr{H}(\mathcal{V},A_{g}A_{h}m^{-1/2}T_{g,h}^{-1})}{m}}\] (C.22)

with probability at least

\[1-mP_{0}-2\exp\left[-\Omega\left(\mathscr{H}\Big{(}\mathcal{X},\frac{A_{g}A_ {h}}{\sqrt{m}S_{g,h}}\Big{)}+\mathscr{H}\Big{(}\mathcal{V},\frac{A_{g}A_{h}}{ \sqrt{m}T_{g,h}}\Big{)}\right)\right].\]

This completes the proof. 

## Appendix D Other Omitted Proofs

### Proof of Lemma 1 (Lipschitz continuity of \(f_{i,\beta}\) and \(\varepsilon_{i,\beta}\)).

Proof.: It is straightforward to check that \(f_{i,\beta}\) and \(|\varepsilon_{i,\beta}|\) are piece-wise continuous functions; hence, it suffices to prove that they are Lipschitz with the claimed Lipschitz constant over each piece. In any interval contained in the part of \(x\notin\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]\), \(f_{i,\beta}=f_{i}\) and \(|\varepsilon_{i,\beta}|=0\) trivially satisfy the claim. In any interval contained in \([x_{0}-\frac{\beta}{2},x_{0}]\) for some \(x_{0}\in\mathscr{D}_{f_{i}}\), \(f_{i,\beta}\) is linear with slope \(\frac{2}{\beta}\big{(}f_{i}^{a}(x_{0})-f_{i}(x_{0}-\frac{\beta}{2})\big{)}\), combined with the bound \(|f_{i}^{a}(x_{0})-f_{i}(x_{0}-\frac{\beta}{2})|\leq|f_{i}^{a}(x_{0})-f_{i}^{-}(x _{0})|+|f_{i}^{-}(x_{0})-f_{i}(x_{0}-\frac{\beta}{2})|\leq\frac{|f_{i}^{+}(x_{0} )-f_{i}^{-}(x_{0})|}{2}+\frac{L_{0}\beta}{2}\leq\frac{1}{2}\big{(}B_{0}+L_{0} \beta\big{)}\), we know that \(f_{i,\beta}\) is \(\big{(}L_{0}+\frac{B_{0}}{\beta}\big{)}\)-Lipschitz. Further, \(|\varepsilon_{i,\beta}|=|f_{i,\beta}-f_{i}|\), and \(f_{i}\) is \(L_{0}\)-Lipschitz over this interval, so \(|\varepsilon_{i,\beta}|\) is \(\big{(}2L_{0}+\frac{B_{0}}{\beta}\big{)}\)-Lipschitz continuous. A similar argument applies to an interval contained in \([x_{0},x_{0}+\frac{\beta}{2}]\). 

### Proof of Lemma 6 (Metric entropy of constraint sets).

Proof.: **Bounding \(\mathscr{H}(\mathcal{K},\eta)\).**

By [50, Corollary 4.2.13], there exists an \(\big{(}\frac{\eta}{Lr}\big{)}\)-net \(\mathcal{G}_{1}\) of \(\mathbb{B}_{2}^{k}\) such that

\[\log|\mathcal{G}_{1}|\leq k\log\Big{(}\frac{2Lr}{\eta}+1\Big{)}\leq k\log\frac{3 Lr}{\eta},\]

where we use \(\eta\leq Lr\). Note that \(r\mathcal{G}_{1}\) is an \(\big{(}\frac{\eta}{L}\big{)}\)-net of \(\mathbb{B}_{2}^{k}(r)\), and because \(G\) is \(L\)-Lipschitz, \(G(r\mathcal{G}_{1})\) is an \(\eta\)-net of \(\mathcal{K}\), thus yielding \(\mathscr{H}(\mathcal{K},\eta)\leq k\log\frac{3Lr}{\eta}\).

**Bounding \(\mathscr{H}(\mathcal{K}^{-},\eta)\) and \(\mathscr{H}(\mathcal{K}^{-}_{\epsilon},\eta)\)**.

We construct \(\mathcal{G}_{2}\) as an \(\big{(}\frac{\eta}{2}\big{)}\)-net of \(\mathcal{K}\) satisfying \(\log|\mathcal{G}_{2}|\leq k\log\frac{6Lr}{\eta}\). Then, it is easy to see that \(\mathcal{G}_{2}-\mathcal{G}_{2}\) is an \(\eta\)-net of \(\mathcal{K}^{-}=\mathcal{K}-\mathcal{K}\), showing that

\[\mathscr{H}(\mathcal{K}^{-},\eta)\leq\log|\mathcal{G}_{2}|^{2}\leq 2k\log \frac{6Lr}{\eta}.\]

For a given \(T>0\), this directly implies \(\mathscr{H}(T\mathcal{K}^{-},\eta)\leq 2k\log\frac{6Lr}{\eta}\). Moreover, because \(\mathcal{K}^{-}_{\epsilon}\subset T\mathcal{K}^{-}\), by [50, Exercise 4.2.10] (which states that \(\mathscr{H}(\mathcal{K}_{1},r)\leq\mathscr{H}(\mathcal{K}_{2},\frac{r}{2})\) holds for any \(r>0\) if \(\mathcal{K}_{1}\subset\mathcal{K}_{2}\)) we obtain

\[\mathscr{H}(\mathcal{K}^{-}_{\epsilon},\eta)\leq\mathscr{H}\Big{(}T\mathcal{ K}^{-},\frac{\eta}{2}\Big{)}\leq 2k\log\frac{12TLr}{\eta}.\]

**Bounding \(\mathscr{H}\big{(}(\mathcal{K}^{-}_{\epsilon})^{*},\eta\big{)}\).**

We construct \(\mathcal{G}_{3}\) as an \(\big{(}\epsilon\eta\big{)}\)-net of \(\mathcal{K}^{-}_{\epsilon}\) satisfying \(\log|\mathcal{G}_{3}|\leq 2k\log\frac{12TLr}{\epsilon\eta}\), then we consider \((\mathcal{G}_{3})^{*}:=\{\frac{\bm{z}}{\|\bm{z}\|_{2}}:\bm{z}\in\mathcal{G}_{3}\}\). We aim to prove that \((\mathcal{G}_{3})^{*}\) is an \(\eta\)-net of \((\mathcal{K}^{-}_{\epsilon})^{*}\). Note that any \(\bm{x}_{1}\in(\mathcal{K}^{-}_{\epsilon})^{*}\) can be written as \(\frac{\bm{z}_{1}}{\|\bm{z}\|_{2}}\) for some \(\bm{z}_{1}\in\mathcal{K}^{-}_{\epsilon}\) and recall that \(\|\bm{z}_{1}\|_{2}\geq 2\epsilon\). Moreover, by construction, there exists some \(\bm{z}_{2}\in\mathcal{G}_{3}\) such that \(\|\bm{z}_{1}-\bm{z}_{2}\|_{2}\leq\epsilon\eta\). Note that \(\frac{\bm{z}_{2}}{\|\bm{z}_{2}\|_{2}}\in(\mathcal{G}_{3})^{*}\), and moreover we have

\[\bigg{\|}\frac{\bm{z}_{1}}{\|\bm{z}_{1}\|_{2}}-\frac{\bm{z}_{2}}{ \|\bm{z}_{2}\|_{2}}\bigg{\|}_{2} \leq\bigg{\|}\frac{\bm{z}_{1}}{\|\bm{z}_{1}\|_{2}}-\frac{\bm{z}_{2 }}{\|\bm{z}_{1}\|_{2}}\bigg{\|}_{2}+\bigg{\|}\frac{\bm{z}_{2}}{\|\bm{z}_{1}\|_ {2}}-\frac{\bm{z}_{2}}{\|\bm{z}_{2}\|_{2}}\bigg{\|}_{2}\] \[=\frac{\|\bm{z}_{1}-\bm{z}_{2}\|_{2}}{\|\bm{z}_{1}\|_{2}}+\frac{ \|\bm{z}_{2}\|_{2}-\|\bm{z}_{1}\|_{2}}{\|\bm{z}_{1}\|_{2}}\] \[\leq\frac{2\|\bm{z}_{1}-\bm{z}_{2}\|_{2}}{\|\bm{z}_{1}\|_{2}}\leq \frac{2\epsilon\eta}{2\epsilon}=\eta.\]

Hence, we obtain

\[\mathscr{H}\big{(}(\mathcal{K}^{-}_{\epsilon})^{*},\eta\big{)}\leq\log|( \mathcal{G}_{3})^{*}|\leq\log|\mathcal{G}_{3}|\leq 2k\log\frac{12TLr}{ \epsilon\eta},\]

which completes the proof. 

### Proof of Lemma 8 (Choice of \(T\) in 1-bit GCS).

Proof.: Since \(\bm{x}\in\mathbb{S}^{n-1}\), for some orthogonal matrix \(\bm{P}\) we have \(\bm{P}\bm{x}=\bm{e}_{1}\) (the first column of \(\bm{I}_{n}\)). Since \(\bm{\tilde{a}}:=\bm{P}\bm{a}=[\tilde{a}_{i}]\) has the same distribution as \(\bm{a}\), we have

\[\mathbb{E}[\operatorname{sign}(\bm{a}^{\top}\bm{x})\bm{a}] =\mathbb{E}[\operatorname{sign}(\bm{\tilde{a}}^{\top}\bm{e}_{1}) \bm{P}^{\top}\bm{\tilde{a}}]=\bm{P}^{\top}\mathbb{E}[\operatorname{sign}( \tilde{a}_{1})\bm{\tilde{a}}]\] \[=\bm{P}^{\top}\sqrt{\frac{2}{\pi}}\bm{e}_{1}=\sqrt{\frac{2}{\pi}} \bm{x}.\]

### Proof of Lemma 9 (Choice of \(T\) in 1-bit GCS with dithering).

Proof.: We first note that

\[\Big{\|}\mathbb{E}[\operatorname{sign}(\bm{a}^{\top}\bm{x}+\tau)\bm{a}]-\frac{ \bm{x}}{\lambda}\Big{\|}_{2}=\frac{1}{\lambda}\sup_{\bm{v}\in\mathbb{S}^{n-1}} \left(\mathbb{E}\big{[}\lambda\cdot\operatorname{sign}(\bm{a}^{\top}\bm{x}+ \tau)\bm{a}^{\top}\bm{v}\big{]}-\bm{x}^{\top}\bm{v}\right).\] (D.1)

We first fix \(\bm{a}\) and expect over \(\bm{\tau}\sim\mathscr{U}[-\lambda,\lambda]\) to obtain

\[\mathbb{E}_{\tau}\big{[}\lambda\operatorname{sign}(\bm{a}^{\top} \bm{x}+\tau)\bm{a}^{\top}\bm{v}\big{]}\] \[= (\lambda\bm{a}^{\top}\bm{v})\left(\frac{1}{1}(|\bm{a}^{\top}\bm{x} |>\lambda)\operatorname{sign}(\bm{a}^{\top}\bm{x})+1(|\bm{a}^{\top}\bm{x}|\leq \lambda)\cdot\Big{(}\frac{\lambda+\bm{a}^{\top}\bm{x}}{2\lambda}-\frac{\lambda-\bm {a}^{\top}\bm{x}}{2\lambda}\Big{)}\right)\] \[= (\bm{a}^{\top}\bm{x})(\bm{a}^{\top}\bm{v})\mathds{1}(|\bm{a}^{\top} \bm{x}|\leq\lambda)+(\lambda\bm{a}^{\top}\bm{v})\operatorname{sign}(\bm{a}^{ \top}\bm{x})\mathds{1}(|\bm{a}^{\top}\bm{x}|>\lambda).\]We plug this into (D.1), and note that \(\bm{x}^{\top}\bm{v}=\mathbb{E}[(\bm{a}^{\top}\bm{x})(\bm{a}^{\top}\bm{v})]\), which gives

\[\begin{split}&\left\|\mathbb{E}[\operatorname{sign}(\bm{a}^{\top}\bm{x}+ \tau)\bm{a}]-\frac{\bm{x}}{\lambda}\right\|_{2}\\ =&\frac{1}{\lambda}\sup_{\bm{v}\in\mathbb{S}^{n-1}} \mathbb{E}\left(\Big{[}(\lambda\bm{a}^{\top}\bm{v})\operatorname{sign}(\bm{a} ^{\top}\bm{x})-(\bm{a}^{\top}\bm{x})(\bm{a}^{\top}\bm{v})\Big{]}\mathbbm{1}(| \bm{a}^{\top}\bm{x}|>\lambda)\right)\\ \leq&\frac{1}{\lambda}\sup_{\bm{v}\in\mathbb{S}^{n- 1}}\mathbb{E}\left([\lambda|\bm{a}^{\top}\bm{v}|+|\bm{a}^{\top}\bm{x}||\bm{a}^ {\top}\bm{v}]\mathbbm{1}(|\bm{a}^{\top}\bm{x}|>\lambda)\right)\end{split}\] (D.2)

For any \(\bm{x}\in\mathbb{B}_{2}^{n}(R)\) and \(\bm{v}\in\mathbb{S}^{n-1}\), we have \(\|\bm{a}^{\top}\bm{x}\|_{\psi_{2}}=O(R)\) and \(\|\bm{a}^{\top}\bm{v}\|_{\psi_{2}}=O(1)\). Applying the Cauchy-Schwarz inequality, we obtain

\[\mathbb{E}\left([\lambda|\bm{a}^{\top}\bm{v}|+|\bm{a}^{\top}\bm{x} ||\bm{a}^{\top}\bm{v}]\mathbbm{1}(|\bm{a}^{\top}\bm{x}|>\lambda)\right)\] \[\leq \sqrt{\mathbb{E}[(\lambda|\bm{a}^{\top}\bm{v}|+|\bm{x}^{\top}\bm {a}\bm{a}^{\top}\bm{v}|)^{2}]}\sqrt{\mathbb{P}(|\bm{a}^{\top}\bm{x}|>\lambda)}\] \[\leq \sqrt{2\big{(}\lambda^{2}\mathbb{E}[|\bm{a}^{\top}\bm{v}|^{2}]+ \mathbb{E}[(\bm{a}^{\top}\bm{x})^{2}(\bm{a}^{\top}\bm{v})^{2}]\big{)}}\sqrt{2 \exp(-c\lambda^{2}/R^{2})}\] \[\lesssim \lambda\exp\Big{(}-\frac{c\lambda^{2}}{R^{2}}\Big{)}\lesssim \frac{\lambda}{m^{9}}\]

Note that in the third line, we use the probability tail bound of the sub-Gaussian \(|\bm{a}^{\top}\bm{x}|\), and in the last line, we use \(\lambda=CR\sqrt{\log m}\) with some sufficiently large \(C\). The proof is completed by substituting this into (D.2). 

### Proof of Lemma 10 (Choice of \(T\) in SIM).

Proof.: This lemma slightly generalizes that of Lemma 8. We again choose an orthogonal matrix \(\bm{P}\) such that \(\bm{P}\bm{x}=\bm{e}_{1}\), where \(\bm{e}_{1}\) represents the first column of \(\bm{I}_{n}\). Since \(\bm{a}\) and \(\bm{P}\bm{a}\) have the same distribution, we have

\[\mathbb{E}[f(\bm{a}^{\top}\bm{x})\bm{a}]=\bm{P}^{\top}\mathbb{E}[f ((\bm{P}\bm{a})^{\top}\bm{e}_{1})\bm{P}\bm{a}]\] \[= \bm{P}^{\top}\mathbb{E}[f(\bm{a}^{\top}\bm{e}_{1})\bm{a}]=\bm{P}^ {\top}(\mu\bm{e}_{1})=\mu\bm{x}.\]

### Proof of Lemma 11 (Choice of \(T\) in uniformly quantized GCS with dithering).

Proof.: In the theorem, the statement before "In particular" can be found in [18, Theorem 1]. Based on this, we have \(\mathbb{E}[\mathcal{Q}_{\delta}(\bm{a}^{\top}\bm{x}+\tau)\bm{a}]=\mathbb{E}_{a} \mathbb{E}_{\tau}[\mathcal{Q}_{\delta}(\bm{a}^{\top}\bm{x}+\tau)\bm{a}]= \mathbb{E}_{\bm{a}}(\bm{a}\bm{a}^{\top}\bm{x})=\bm{x}\). 

Proof of Lemma 12. (Bounds on \(|\xi_{i,\beta}|\) and \(|\varepsilon_{i,\beta}|\) for the uniform quantizer).

Proof.: By the definition of \(f_{i,\beta}\) in (3.4), we have \(|\varepsilon_{i,\beta}(a)|=|f_{i,\beta}(a)-f_{i}(a)|\leq\delta\). It follows that \(f_{i}(\cdot)=\mathcal{Q}_{\delta}(\cdot+\tau)\) with \(\mathcal{Q}_{\delta}(a)=\delta\big{(}\big{[}\frac{1}{\delta}\big{]}+\frac{1}{2} \big{)}\), and \(|\mathcal{Q}_{\delta}(a)-a|\leq\frac{\delta}{2}\) holds for any \(a\in\mathbb{R}\). Hence, we have \(|f_{i}(a)-a|=|\mathcal{Q}_{\delta}(a+\tau)-(a+\tau)+\tau|\leq|\mathcal{Q}_{ \delta}(a+\tau)-(a+\tau)|+|\tau|\leq\frac{\delta}{2}+\frac{\delta}{2}=\delta\). To complete the proof, we use the inequalities \(|\xi_{i,\beta}(a)|\leq|f_{i,\beta}(a)-f_{i}(a)|+|f_{i}(a)-a|\leq\delta+\delta=2\delta\). 

### Proof of Lemma 13. (Bound on the approximation error \(|\varepsilon_{i,\beta}|\))

Proof.: For any \(a\notin\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]\), by the definition in (3.4) we have \(\varepsilon_{i,\beta}(a)=0\). If \(a\in[x_{0}-\frac{\beta}{2},x_{0}]\) for some \(x_{0}\in\mathscr{D}_{f_{i}}\), then we have

\[|\varepsilon_{i,\beta}(a)|= |f_{i,\beta}(a)-f_{i}(a)|\] \[\leq |f_{i,\beta}(a)-f_{i,\beta}(x_{0})|+|f_{i,\beta}(x_{0})-f_{i}^{-} (x_{0})|+|f_{i}^{-}(x_{0})-f_{i}(a)|\] \[\leq \Big{(}2L_{0}+\frac{B_{0}}{\beta}\Big{)}|a-x_{0}|+|f_{i}^{a}(x_{ 0})-f_{i}^{-}(x_{0})|+L_{0}|x_{0}-a|\] \[\leq \Big{(}3L_{0}+\frac{B_{0}}{\beta}\Big{)}\cdot\frac{\beta}{2}+ \frac{1}{2}|f_{i}^{+}(x_{0})-f_{i}^{-}(x_{0})|\leq\frac{3L_{0}\beta}{2}+B_{0},\]

where we use Lemma 1 and Assumption 2 in the third line, and use \(|a-x_{0}|\leq\frac{\beta}{2}\) and \(f_{i}^{a}(x_{0})=\frac{1}{2}\big{(}f_{i}^{-}(x_{0})+f_{i}^{+}(x_{0})\big{)}\) in the fourth line.

Parameter Selection for Specific Models

### 1-bit GCS

To specialize Theorem 1 to this model, we select the parameters as follows:

* **Assumption 2.** Under the 1-bit observation model \(y_{i}=\mathrm{sign}(\bm{a}_{1}^{\top}\bm{x}^{\star})\), the function \(f_{i}(\cdot)=f(\cdot)=\mathrm{sign}(\cdot)\) satisfies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(2,0,\infty)\).
* **(2.5) in Assumption 4.** Recall that \(\mathcal{K}\subset\mathbb{S}^{n-1}\). Under the assumption \(\|\bm{x}^{\star}\|_{2}=1\), we set \(T=\sqrt{2/\pi}\) so that \(\rho(\bm{x})=0\) holds for all \(\bm{x}\in\mathcal{X}\) (Lemma 8), which provides (2.5).
* **Assumption 3.** By Lemma 7, we have \(\mathbb{P}(\|\bm{a}\|_{2}=O(\sqrt{n}))\geq 1-2\exp(-\Omega(n))\), and we suppose that this high-probability event holds. Also note that \(|f_{i,\beta}|\leq 1\). Hence, we have \[\|\xi_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{\psi_{2}}\leq\|f_{i,\beta} (\bm{a}^{\top}\bm{x})\|_{\psi_{2}}+\|T\bm{a}^{\top}\bm{x}\|_{\psi_{2}}=O(1),\] \[\sup_{\bm{x}\in\mathcal{K}}|\xi_{i,\beta}(\bm{a}^{\top}\bm{x})| \leq|f_{i,\beta}(\bm{a}^{\top}\bm{x})|+|T\bm{a}^{\top}\bm{x}|\leq 1+T\|\bm{a} \|_{2}=O(\sqrt{n}).\] Because \(\varepsilon_{i,\beta}=f_{i,\beta}-f_{i}\), we have \(\|\varepsilon_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{\psi_{2}}=O(1)\), and \(|\varepsilon_{i,\beta}(\bm{a}^{\top}\bm{x})|\leq 2\) holds deterministically. Hence, regarding the parameters in Assumption 3, we can take \[A_{g}^{(1)}\asymp 1,U_{g}^{(1)}\asymp\sqrt{n},P_{0}^{(1)}\asymp\exp(- \Omega(n)),A_{g}^{(2)}\asymp 1,U_{g}^{(2)}\asymp 1,P_{0}^{(2)}=0.\]
* **(2.6) in Assumption 4.** It remains to pick \(\beta_{1}\) that satisfies (2.6). Note that \(\mathscr{D}_{f_{i}}=\{0\}\), and for any \(\bm{x}\in\mathcal{K}\), \(\bm{a}^{\top}\bm{x}\sim\mathcal{N}(0,1)\), so we have \[\mu_{\beta}(\bm{x})=\mathbb{P}\Big{(}\bm{a}^{\top}\bm{x}\in\Big{[}-\frac{\beta }{2},\frac{\beta}{2}\Big{]}\Big{)}=O(\beta).\] Thus, we take \(\beta=\beta_{1}\asymp\frac{k}{m}\) to guarantee (2.6).

### 1-bit GCS with dithering

To specialize Theorem 1 to this model, we select the parameters as follows:

* **Assumption 2.** The observation function can be written as \(f(\cdot)=\mathrm{sign}(\cdot+\tau)\) with \(\tau\sim\mathscr{U}[-\lambda,\lambda]\), which satisfies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(2,0,\infty)\).
* **(2.5) in Assumption 4.** We set \(\lambda=CR\sqrt{\log m}\) with \(C\) large enough, so that Lemma 9 justifies (2.5).
* **Assumption 3.** By Lemma 7, we have \(\mathbb{P}(\|\bm{a}\|_{2}=O(\sqrt{n}))\geq 1-2\exp(-\Omega(n))\). Assume this event holds, and note that \(f_{i,\beta}\) is still bounded by \(1\), we have \[\|\xi_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{\psi_{2}}\leq\|f_{i,\beta} (\bm{a}^{\top}\bm{x})\|_{\psi_{2}}+\|\lambda^{-1}\bm{a}^{\top}\bm{x}\|_{\psi_{ 2}}=O(R/\lambda)+O(1)=O(1),\] \[\sup_{\bm{x}\in\mathcal{K}}|\xi_{i,\beta}(\bm{a}^{\top}\bm{x})| \leq 1+\sup_{\bm{x}\in\mathcal{K}}|\lambda^{-1}\bm{a}^{\top}\bm{x}|\leq 1+\sup_{\bm{x} \in\mathcal{K}}\lambda^{-1}\|\bm{a}\|_{2}\|\bm{x}\|_{2}=O(\sqrt{n}).\] Moreover, because \(\varepsilon_{i,\beta}=f_{i,\beta}-f_{i}\), the following hold deterministically: \(\|\varepsilon_{i,\beta}(\bm{a}^{\top}\bm{x})\|_{\psi_{2}}=O(1)\), \(\sup_{\bm{x}\in\mathcal{K}}|\varepsilon_{i,\beta}(\bm{a}^{\top}\bm{x})|=O(1)\). Thus, regarding the parameters in Assumption 3 we can take \[A_{g}^{(1)}\asymp 1,\ U_{g}^{(1)}\asymp\sqrt{n},\ P_{0}^{(1)}\asymp\exp(- \Omega(n)),\ A_{g}^{(2)}\asymp 1,\ U_{g}^{(2)}\asymp 1,\ P_{0}^{(2)}=0.\]
* **(2.6) in Assumption 4.** It remains to confirm (2.6) for suitable \(\beta_{1}\). For any \(\beta\), note that \(\mathscr{D}_{f_{i}}+[-\frac{\beta}{2},\frac{\beta}{2}]=[\tau-\frac{\beta}{2}, \tau+\frac{\beta}{2}]\), and hence for any \(\bm{x}\in\mathcal{K}\subset\mathbb{B}_{2}^{n}(R)\) we have \[\mu_{\beta}(\bm{x})=\mathbb{P}\Big{(}\bm{a}^{\top}\bm{x}\in\Big{[}-\tau-\frac {\beta}{2},-\tau+\frac{\beta}{2}\Big{]}\Big{)}=\mathbb{P}\Big{(}\bm{a}^{\top} \bm{x}+\tau\in\Big{[}-\frac{\beta}{2},\frac{\beta}{2}\Big{]}\Big{)}\leq\frac{ \beta}{\lambda},\] which can be seen by conditioning on \(\bm{a}\). Hence, we can take \(\beta=\beta_{1}=\frac{\lambda k}{m}\) to guarantee (2.6).

### Lipschitz-continuous SIM with generative prior

To specialize Theorem 1 to this model, we select the parameters as follows:

* **Assumption 2.** Since \(f\) is \(\hat{L}\)-Lipschitz by assumption, it satisfies Assumption 2 with \((B_{0},L_{0},\beta_{0})=(0,\hat{L},\infty)\).
* **(2.5) in Assumption 4.** Recall that we have defined the quantities \(\mu=\mathbb{E}[f(g)g],\psi=\|f(g)\|_{\varepsilon_{2}},\text{ where }g\sim\mathcal{N}(0,1)\). Then, we choose \(T=\mu\) so that \(\rho(\bm{x})=0\) holds for any \(\bm{x}\) (Lemma 10), thus justifying (2.5).
* **Assumption 3.** Because \(f_{i}\) is \(\hat{L}\)-Lipschitz and does not contain any discontinuity, there is no need to construct the Lipschitz approximation \(f_{i,\beta}\) for some \(\beta>0\), while we simply use \(\beta=0\), which implies \(f_{i,\beta}=f_{i}\) and \(\varepsilon_{i,\beta}=0\). Note that \(\xi_{i,\beta}(a)=f_{i}(a)-\mu a\), and so we have \[\|f_{i}(\bm{a}^{\top}\bm{x})-\mu\bm{a}^{\top}\bm{x}\|_{\psi_{2}}\leq\|f_{i}( \bm{a}^{\top}\bm{x})\|_{\psi_{2}}+\|\mu\bm{a}^{\top}\bm{x}\|_{\psi_{2}}=O(\psi +\mu).\] We suppose \(\|\bm{a}\|_{2}=O(\sqrt{n})\), which holds with probability at least \(1-2\exp(-\Omega(n))\) (Lemma 7); we also suppose \(f_{i}(0)\leq\hat{B}\), which holds with probability at least \(1-P_{0}^{\prime}\) by assumption. On these two events, we have \[|f_{i}(\bm{a}^{\top}\bm{x})-\mu\bm{a}^{\top}\bm{x}| \leq|f_{i}(\bm{a}^{\top}\bm{x})-f_{i}(0)|+|f_{i}(0)|+\mu\|\bm{a} \|_{2}\|\bm{x}\|_{2}\] \[\leq\hat{L}\|\bm{a}\|_{2}+\hat{B}+\mu\|\bm{a}\|_{2}\lesssim(\hat{L }+\mu)\sqrt{n}+\hat{B}.\] Combined with \(\varepsilon_{i,\beta}=0\), we can set the parameters in Assumption 3 as follows: \[A_{g}^{(1)}\asymp\psi+\mu,\;U_{g}^{(1)} \asymp(\hat{L}+\mu)\sqrt{n}+\hat{B},\;P_{0}^{(1)}\asymp P_{0}^{ \prime}+\exp(-\Omega(n)),\] \[A_{g}^{(2)} \asymp\psi+\mu,\;U_{g}^{(2)}=0,\;P_{0}^{(2)}=0.\]
* **(2.6) in Assumption 4.** Because \(\beta=0\) and \(\mathscr{D}_{f_{i}}=\varnothing\), (2.6) is trivially satisfied.

### Uniformly quantized GCS with dithering

To specialize Theorem 1 to this model, we select the parameters as follows:

* **Assumption 2.** The uniform quantizer with resolution \(\delta>0\) is defined as \(\mathcal{Q}_{\delta}(a)=\delta\big{(}\big{\lfloor}\frac{a}{\delta}\big{\rfloor} +\frac{1}{2}\big{)}\) for \(a\in\mathbb{R}\). We consider this quantizer with dithering \(\tau_{i}\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\). Specifically, we observe \(y_{i}=\mathcal{Q}_{\delta}(\bm{a}_{i}^{\top}\bm{x}^{\star}+\tau_{i})\), so the observation function is \(f(\cdot)=\mathcal{Q}_{\delta}(\cdot+\tau)\) with \(\tau\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\). Hence, Assumption 2 is satisfied with \((B_{0},L_{0},\beta_{0})=(\delta,0,\delta)\).
* **(2.5) in Assumption 4.** The benefit of dithering is to whiten the quantization noise. With \(T=1\), for any \(\bm{x}\in\mathcal{K}\), Lemma 11 implies \(\rho(\bm{x})=\|\mathbb{E}[\mathcal{Q}_{\delta}(\bm{a}_{i}^{\top}\bm{x}+\tau_{i} )\bm{a}_{i}]-\bm{x}\|_{2}=0\), thus justifying (2.5).
* **Assumption 3.** Note that for any \(\beta\in(0,\frac{\delta}{2})\), by Lemma 12, we can take the parameters for Assumption 3 as follows: \[A_{g}^{(1)},U_{g}^{(1)},A_{g}^{(2)},U_{g}^{(2)}\asymp\delta,\;P_{0}^{(1)}=P_{0 }^{(2)}=0.\]
* **(2.6) in Assumption 4.** All that remains is to pick \(\beta=\beta_{1}\) that satisfies (2.6). Because \(\mathscr{D}_{f_{i}}=-\tau_{i}+\delta\mathbb{Z}\), hence for any \(\bm{x}\in\mathcal{K}\) we have \[\mu_{\beta}(\bm{x})=\mathbb{P}\Big{(}\bm{a}^{\top}\bm{x}\in-\tau+\delta\mathbb{ Z}+\Big{[}-\frac{\beta}{2},\frac{\beta}{2}\Big{]}\Big{)}=\mathbb{P}\Big{(} \bm{a}^{\top}\bm{x}+\tau\in\delta\mathbb{Z}+\Big{[}-\frac{\beta}{2},\frac{ \beta}{2}\Big{]}\Big{)}=O\Big{(}\frac{\beta}{\delta}\Big{)},\] which can be seen by using the randomness of \(\tau\sim\mathscr{U}[-\frac{\delta}{2},\frac{\delta}{2}]\) conditionally on \(\bm{x}\). Hence, we take \(\beta=\beta_{1}\asymp\frac{k\delta}{m}\), which provides (2.6).

## Appendix F Handling Sub-Gaussian Additive Noise

In this appendix, we describe how our results can be extended to the noisy model \(\bm{y}=\bm{f}(\bm{A}\bm{x}^{\star})+\bm{\eta}\), where \(\bm{\eta}\in\mathbb{R}^{m}\) is the noise vector that is independent of \((\bm{A},\bm{f})\) and has i.i.d. sub-Gaussian entries \(\eta_{i}\) satisfying \(\|\eta_{i}\|_{\psi_{2}}=O(1)\). Along similar lines as in (3.1)-(3.3), we find that \(\bm{\eta}\) gives rise to an additional term \(\frac{2}{m}\langle\bm{\eta},A(\bm{\hat{x}}-T\bm{x}^{\star})\rangle\) to the right-hand side of (3.1), which is bounded by\(2\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}\cdot\sup_{\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*} }\frac{1}{m}\langle\bm{\eta},\bm{A}\bm{v}\rangle\), with the constraint set \((\mathcal{K}_{\epsilon}^{-})^{*}\) defined in (3.2). Thus, in (3.3), in addition to \(\mathscr{R}_{u}\), in the noisy setting we need to bound the additional term

\[\mathcal{R}_{u}^{\prime}:=\sup_{\bm{v}\in(\mathcal{K}_{\epsilon}^{-})^{*}} \frac{1}{m}\langle\bm{\eta},\bm{A}\bm{v}\rangle=\sup_{\bm{v}\in(\mathcal{K}_{ \epsilon}^{-})^{*}}\frac{1}{m}\sum_{i=1}^{m}\eta_{i}\bm{a}_{i}^{\top}\bm{v}.\]

This can be done by the following lemma, which indicates that the sharp (uniform) rate in Theorem 1 can be retained in the presence of noise \(\bm{\eta}\).

**Lemma 15**.: (Bounding the additional term \(\mathcal{R}_{u}^{\prime}\))_. In the noisy setting described above, with probability at least \(1-C_{1}\exp(-\Omega(k\log\frac{TLr}{\epsilon}))-C_{2}\exp(-\Omega(m))\), we have \(\mathcal{R}_{u}^{\prime}\lesssim\sqrt{\frac{k\log\frac{TLr}{\epsilon}}{m}}\)._

Proof.: Conditioning on \(\bm{\eta}\), the randomness of \(\bm{a}_{i}\)'s gives \(\frac{1}{m}\sum_{i=1}^{m}\eta_{i}\bm{a}_{i}\sim\mathcal{N}(0,\frac{\|\bm{\eta} \|_{2}^{2}}{m^{2}}\bm{I}_{n})\), and so \(\|(\frac{1}{m}\sum_{i=1}^{m}\eta_{i}\bm{a}_{i})^{\top}\bm{v}_{1}-(\frac{1}{m} \sum_{i=1}^{m}\eta_{i}\bm{a}_{i})^{\top}\bm{v}_{2}\|_{\psi_{2}}\leq\frac{C_{0} \|\bm{\eta}\|_{2}\|\bm{v}_{1}-\bm{v}_{2}\|_{2}}{m}\|_{\psi_{2}}\) holds for any \(\bm{v}_{1},\bm{v}_{2}\in\mathbb{R}^{n}\). Let \(\omega(\cdot)\) be the Gaussian width as defined in Lemma 5. Then, using the randomness of \(\bm{a}_{i}\)'s, Talagrand's comparison inequality [50, Exercise 8.6.5] yields that for any \(t\geq 0\), we have

\[\mathbb{P}\Big{(}\mathcal{R}_{u}^{\prime}\leq\frac{C_{1}\|\bm{\eta}\|_{2} \cdot[\omega((\mathcal{K}_{\epsilon}^{-})^{*})+t]}{m}\Big{)}\geq 1-2\exp(-t^{2}).\] (F.1)

Next, we bound the Gaussian width \(\omega((\mathcal{K}_{\epsilon}^{-})^{*})\). Recall that \((\mathcal{K}_{\epsilon}^{-})^{*}\) is defined in (3.2), and Lemma 6 bounds its metric entropy as \(\mathscr{H}((\mathcal{K}_{\epsilon}^{-})^{*},\eta)\leq 2k\log\frac{12TLr}{ \epsilon\eta}\). Thus, we can invoke Dudley's integral inequality [50, Theorem 8.1.3] to obtain

\[\omega((\mathcal{K}_{\epsilon}^{-})^{*})\leq C_{2}\int_{0}^{2}\sqrt{2k\log \frac{12TLr}{\epsilon\eta}}\;\text{d}\eta\lesssim\sqrt{k\log\frac{TLr}{ \epsilon}}.\]

Now, we further let \(t=\sqrt{k\log\frac{TLr}{\epsilon}}\) in (F.1) to obtain that \(\mathcal{R}_{u}^{\prime}\lesssim\frac{\|\bm{\eta}\|_{2}\sqrt{k\log\frac{TLr}{ \epsilon}}}{m}\) holds with probability at least \(1-2\exp(-k\log\frac{TLr}{\epsilon})\). It remains to deal with the randomness of \(\bm{\eta}\) and bound \(\|\bm{\eta}\|_{2}\). Because \(\bm{\eta}\) has i.i.d. entries with \(\|\eta_{i}\|_{\psi_{2}}=O(1)\), by [50, Theorem 3.1.1] we can obtain that \(\|\bm{\eta}\|_{2}\leq C_{3}\sqrt{m}\) with probability at least \(1-2\exp(-c_{3}m)\). Substituting this bound into \(\mathcal{R}_{u}^{\prime}\lesssim\frac{\|\bm{\eta}\|_{2}\sqrt{k\log\frac{TLr}{ \epsilon}}}{m}\), the result follows. 

To close this appendix, we briefly state how to adapt the proof of Theorem 1 to explicitly include the additive noise \(\bm{\eta}\). Specifically, the left-hand side of (3.1) and its uniform lower bound \(\Omega(\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}^{2})\) remain unchanged, while the right-hand side of (3.1) is now bounded by \(2\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}\cdot(\mathscr{R}_{u1}+\mathscr{R}_{u2}+ \mathcal{R}_{u}^{\prime})\) (with \(2\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}\mathcal{R}_{u}^{\prime}\) being the additional term); thus, combining the bound (C.12) on \(2\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}\cdot(\mathscr{R}_{u1}+\mathscr{R}_{u2})\) and Lemma 15, we establish a uniform upper bound

\[O\left(\|\hat{\bm{x}}-T\bm{x}^{*}\|_{2}\cdot\Big{[}(A_{g}^{(1)}\lor A_{g}^{(2) })\sqrt{\frac{k\mathscr{L}}{m}}+\sqrt{\frac{k\log\frac{TLr}{\epsilon}}{m}} \Big{]}\right)\]

for the right-hand side of (3.1). Therefore, to ensure uniform recovery up to the \(\ell_{2}\)-norm accuracy of \(\epsilon\) under the sub-Gaussian noise \(\bm{\eta}\), it suffices to have a sample complexity

\[m\gtrsim(A_{g}^{(1)}\lor A_{g}^{(2)})^{2}\frac{k\mathscr{L}}{\epsilon^{2}}+ \frac{k\log\frac{TLr}{\epsilon}}{\epsilon^{2}}.\] (F.2)

Since the logarithmic factors \(\mathscr{L}\) in (C.11) dominates \(\log\frac{TLr}{\epsilon}\), (F.2) indeed coincides with the sample complexity \(m\gtrsim(A_{g}^{(1)}\lor A_{g}^{(2)})^{2}\frac{k\mathscr{L}}{\epsilon^{2}}\) in Theorem 1 under the mild condition of \(A_{g}^{(1)}\lor A_{g}^{(2)}=\Omega(1)\).

## Appendix G Experimental Results for the MNIST dataset

### Details of the Settings

In this section, we conduct experiments on the MNIST dataset [28] to support our theoretical framework. We use various nonlinear measurement models, including 1-bit, dithered 1-bit, ReLU,and uniformly quantized CS with dithering (UQD). We select 30 images from the MNIST testing set, ensuring that there are three images from each of the 10 classes for maximum variability. A single measurement matrix \(\bm{A}\) is generated and used for all 30 test images. All the experiments are repeated for \(10\) random trials. All the experiments are run using Python 3.10.6 and PyTorch 2.0.0, with an NVIDIA RTX 3060 Laptop 6GB GPU.

We train a variational autoencoder (VAE) on the training set of the MNIST dataset, which has 60,000 images, each of size 784. The decoder of the VAE is a fully connected neural network with ReLU activations, with input dimension \(k=20\) and output dimension \(n=784\), and two hidden layers with 500 neurons each. We train the VAE using the Adam optimizer with a mini-batch size of 100 and a learning rate of \(0.001\).

Since our contributions are primarily theoretical, we only provide simple proof-of-concept experimental results. In particular, since (2.1) is intractable to solve exactly, to estimate the underlying signal, we choose to use the algorithm proposed in [2] (referred to as CSGM) to approximate it. CSGM performs a gradient descent algorithm in the latent space in \(\mathbb{R}^{k}\) with random restarts. In addition, we compare with the Lasso program that is solved by the iterative shrinkage thresholding algorithm.

For CSGM, we follow the setting in [2] and perform \(10\) random restarts with \(1000\) gradient descent steps per restart and pick the reconstruction with the best measurement error.

Experimental Results for Noiseless 1-bit Measurements and Uniformly Quantized Measurements with Dithering

In this subsection, we present the numerical results for \(1\)-bit measurements and uniformly quantized measurements with dithering, while the results for dithered 1-bit measurements and the Lipschitz SIM where the nonlinear link function is ReLU are similarly provided in Appendix G.3. For \(1\)-bit measurements, since the underlying signal is assumed to be a unit vector and we aim to recover the direction of the signal, we use cosine similarity that is calculated as \(\hat{\bm{x}}^{T}\bm{x}^{*}/(\|\hat{\bm{x}}\|_{2}\cdot\|\bm{x}^{*}\|_{2})\) with \(\hat{\bm{x}}\) being the estimated vector to measure the reconstruction performance. For uniformly quantized measurements with dithering, we use the relative \(\ell_{2}\)-norm distance between the underlying signal and the estimated vector, i.e., \(\|\hat{\bm{x}}-\bm{x}^{*}\|/\|\bm{x}^{*}\|_{2}\), to measure the reconstruction performance.

Since this paper is concerned with uniform recovery performance, in each trial, we record the worst-case reconstruction performance (i.e., the smallest cosine similarity or the largest relative error) over the \(30\) test images, and the worst-case cosine similarity or relative error is averaged over \(10\) trials.

Figures 2, 3, and 4 show that for noiseless \(1\)-bit measurements and uniformly quantized measurements with dithering with \(\delta=3\), the CSGM approach can produce reasonably accurate reconstruction for all the test images when the number of measurements \(m\) is as small as \(150\) and \(100\) respectively.

### Experimental Results for ReLU and Dithered 1-bit Measurements

We present the experimental results for the ReLU link function and dithered \(1\)-bit measurements in Figures 5, 6, and 7. For dithered 1-bit measurements, we set \(\lambda=R\sqrt{\log m}\) with \(R>0\) being a tuning parameter. For the case of using the ReLU link function, similarly to noiseless 1-bit measurements, we calculate the cosine similarity to measure the reconstruction performance. For dithered 1-bit measurements, similarly to uniformly quantized measurements with dithering, we calculate the relative \(\ell_{2}\)-norm distance. We observe that for these two nonlinear measurement models

Figure 2: Reconstructed images of the MNIST dataset for the noiseless 1-bit measurements with \(m=150\).

with a single realization of the random measurement ensemble, CSGM can also lead to reasonably good reconstruction for all the test images when the number of measurements is small compared to the ambient dimension.

## Appendix H Experimental Results for the CelebA dataset

In this section, we present numerical results for the CelebA dataset [35], which contains more than 200,000 face images for celebrities with an ambient dimension of \(n=12288\). We train a deep convolutional generative adversarial network (DCGAN) following the settings in https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. The latent dimension of the generator is \(k=100\) and the number of epochs for training is \(20\). Since the experiments for CelebA are more time-consuming than those of MNIST, we select \(20\) images from the test set of CelebA and perform \(5\) random trials. Other settings are the same as those for the MNIST dataset.

Since we have observed from the numerical results for MNIST that the experiments for the ReLU link function and dithered 1-bit measurements are similar, we only present the results for noiseless 1-bit measurements and uniformly quantized observations with dithering.

From Figures 8 and 10, we observe that for noiseless 1-bit measurements with \(1500\) samples, a single measurement matrix \(\bm{A}\) can lead to reasonably accurate reconstruction for all the \(20\) test images. In addition, from Figures 9 and 10, we observe that for uniformly quantized measurements with

Figure 4: Quantitative results of the performance of CSGM for \(1\)-bit and UQD measurements on the MNIST dataset.

Figure 5: Reconstructed images of the MNIST dataset for the ReLU link function with \(m=150\) and \(\sigma=0.2\).

Figure 3: Reconstructed images of the MNIST dataset for UQD with \(m=100\) and \(\delta=3\).

dithering, a single realization of the measurement matrix and random dither is sufficient for the reasonably accurate recovery of the \(20\) test images when \(m=1000\) and \(\delta=20\).

Figure 8: Reconstructed images of the CelebA dataset for the noiseless 1-bit measurements with \(m=1500\).

Figure 6: Examples of reconstructed images of the MNIST dataset for dithered \(1\)-bit measurements with \(m=250\) and \(R=5\).

Figure 7: Quantitative results of the performance of CSGM for the ReLU link function and dithered 1-bit measurements on the MNIST dataset.

Figure 10: Quantitative results of the performance of CSGM for \(1\)-bit and UQD measurements on the CelebA dataset.

Figure 9: Reconstructed images of the CelebA dataset for UQD with \(m=1000\) and \(\delta=20\).