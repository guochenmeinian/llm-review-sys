# AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games

Kefan Su1,2,*, Yusen Huo2, Zhilin Zhang2, Shuai Dou2, Chuan Yu2, Jian Xu2,

Zongqing Lu1, Bo Zheng2

1School of Computer Science, Peking University

2Alibaba Group

2{sukefan,zongqing.lu}@pku.edu.cn

2{huoyusen.huoyusen,zhangzhilin.pt,doushuai.ds,

yuchuan.yc,xiyu.xj,bozheng}@alibaba-inc.com

This work is done during internship at Alibaba Group.Corresponding author.Alibaba Group retains full ownership rights to this benchmark.

###### Abstract

Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code3: https://github.com/alimama-tech/AuctionNet.

Footnote 3: Alibaba Group retains full ownership rights to this benchmark.

## 1 Introduction

Decision-making in large-scale games is a fundamental area of research in artificial intelligence. Agents in a large-scale game need to make strategic decisions to fulfill their objectives under certain constraints in a competitive environment. The research advances in this area have a profound impact on a broad range of real-world applications [13; 34; 35; 37]. Online advertising, with a market size ofmore than $600 billion in 2023, is perhaps one of the most representative applications that calls for sophisticated decision-making solutions in large-scale games. More specifically, as shown in Figure 1, a significant part of online advertising is based on real-time bidding (RTB), a process in which advertising inventory is bought and sold in real-time ad auctions. The auto-bidding agents strategically bid for impressions on behalf of the advertisers across a large number of continuously arriving ad opportunities to maximize performance, subject to certain constraints such as return-on-investment (ROI) [28].

Bid decision-making in large-scale ad auctions is a concrete example of decision-making in large-scale games. However, researchers usually only have limited access to realistic large-scale ad auction environments, hindering the research proccess in this area. Although a few existing works provide certain environments, there remains a considerable gap between these environments and the real-world environments. For instance, AuctionGym [18] overlooks changes in advertiser budgets across multiple auction rounds, while AdCraft [11] models competing bidders by sampling from a parameterized distribution, an approach that falls short of fully capturing the essence of the multi-agent dynamics inherent to this problem.

In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of a couple of baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions with the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic and popular Generalized Second Price (GSP) [9; 23; 7] auction but also allows customization of auction mechanisms as needed. To facilitate research and provide insights into the game environment, we also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet.

We believe that AuctionNet is applicable not only to research on bid decision-making algorithms in ad auctions but also to the general area of decision-making in large-scale games. It can also benefit

Figure 1: Overview of typical large-scale online advertising platform. Numbers 1 through 5 illustrate how an auto-bidding agent helps advertiser \(i\) optimize performance. For each advertiser’s unique objective (I), auto-bidding agent make bid decision-making (II) for continuously arriving ad opportunities, and compete against each other in the ad auction (III). Then, each agent may win some impressions (IV), which may be exposed to users and potentially result in conversions. Finally, the agents’ performance (V) will be reported to advertisers.

researchers in a broader range of areas such as reinforcement learning, generative models, operational research, and mechanism design.

## 2 The Decision-Making Problem Concerned

In this paper, we are concerned with the auto-bidding problem in ad auctions. We use a Partially Observable Stochastic Game (POSG)[14] to formulate the problem. A POSG \(\mathcal{M}\) can be represented as a tuple \(\mathcal{M}=\{S,A,P,\bm{r},\gamma,Z,O,I,T\}\), where \(I=\{1,2,\cdots,n\}\) is the set of all the agents, \(T\) is the horizon, _i.e._, the number of time steps in one episode, \(S\) is the state space and \(A\) is the action space, \(P(\cdot|s,a):S\times A\rightarrow\Delta(S)\) is the transition probability, \(\gamma\) is the discount factor, \(Z\) is the observation space, \(O(s,i):S\times I\to Z\) is the mapping from state to observation for each agent \(i\), \(\bm{r}=r_{1}\times r_{2}\times\cdots\times r_{n}\) is the joint reward function of all the agents, and \(r_{i}(s,\bm{a}):S\times A\rightarrow\mathbb{R}\) is the individual reward function for each agent \(i\), where \(\bm{a}=(a_{1},a_{2},\cdots,a_{n})\in A=A_{1}\times A_{2}\times\cdots\times A_ {n}\) is the joint action of all the agents.

Specifically, the interaction in one time step is as follows: The state \(s=(\bm{\omega},\bm{u},\bm{q},\bm{v})\) consists of budgets \(\bm{\omega}\), ad opportunity features \(\bm{u}\), advertiser features \(\bm{q}\) such as industry category, corresponding value matrix \(\bm{v}=\{v_{ij}\}\), where \(v_{ij}\) is the value of ad opportunity \(j\) for agent \(i\). Agent \(i\)'s observation \(o_{i}=(\omega_{i},\bm{u}_{i},q_{i},\bm{v}_{i})\in Z\) contains only part of the information in state \(s\), _i.e._, agent \(i\) may not know the budgets of other agents. A convention in the auto-bidding area [3] proves that the optimal bid is proportional to the ad opportunity value. Following this convention, the action of agent \(i\) is a coefficient \(\alpha_{i}\), and the bids of agent \(i\) for all the ad opportunities of this time step are \(\bm{b}_{i}=(b_{i1},b_{i2},\cdots,b_{im})=(\alpha_{i}v_{i1},\alpha_{i}v_{i2}, \cdots,\alpha_{i}v_{im})\), where \(m\) is the number of ad opportunities within this time step. Given the bids of all the agents, determined by the auction mechanism, agent \(i\) will receive the auction result \(\bm{x}_{i}=(x_{i1},x_{i2},\cdots,x_{im})\), where \(x_{ij}=1\) if and only if agent \(i\) wins opportunity \(j\). Agents will only receive rewards and incur costs from the winning impressions, _i.e._, reward \(r_{i}(s,\bm{a})=\sum_{j=1}^{m}x_{ij}v_{ij}\) and budget for the next time step \(\omega_{i}^{\prime}=\omega_{i}-\sum_{j=1}^{m}x_{ij}c_{ij}\), where \(c_{ij}\) is the cost of impression \(j\) for agent \(i\).

Taking a typical auto-bidding scenario as an example, given the definition above, the optimization objective from the perspective of agent \(i\) is as follows:

\[\underset{\{\alpha_{i}^{\prime}\}}{\text{maximize}}\sum_{t=1}^{T}\left\langle \bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle\quad\mathrm{s.t.}\sum_{t=1}^{T} \left\langle\bm{x}_{i}^{t},\bm{c}_{i}^{t}\right\rangle\leq\omega_{i},\] (1)

where \(\bm{x}_{i}^{t}=(x_{i1}^{t},x_{i2}^{t},\cdots,x_{im}^{t})\), \(\bm{v}_{i}^{t}=(v_{i1}^{t},v_{i2}^{t},\cdots,v_{im}^{t})\), \(\bm{c}_{i}^{t}=(c_{i1}^{t},c_{i2}^{t},\cdots,c_{im}^{t})\), \(\omega_{i}\) is the budget of agent \(i\), and \(\langle\cdot\rangle\) denotes the inner product. As for the implementation, we know from our problem formulation that \(r_{i}(s_{t},\bm{a}_{t})=\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle\), so the objective in the optimization formulation is the same as \(\sum_{t=1}^{T}r_{i}(s_{t},\bm{a}_{t})\). For more complex scenarios, we can add the CPA constraint to ensure effective utilization of the budget. More details on these CPA-constrained problems are included in Appendix E. The decision-making formulation above can be easily extended to various real-world scenarios.

## 3 Ad Auction Environment

To comprehensively demonstrate large-scale games from real-world online advertising platforms, we have developed an ad auction environment. To standardize the auto-bidding process, we divide ad opportunities within a period into \(T\) decision time steps. Given the objective, the auto-bidding agent sequentially bids at each step, using the results from step \(t\) and prior historical information to refine its strategy for step \(t+1\). This design philosophy enables agents to continuously optimize their bidding strategies in order to adapt to the changing environment. Within each step, all ad opportunities are executed independently and in parallel. At the end of the period, the environment provides the final performance for the agent.

The environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module, the bidding module, and the auction module. To better simulate large-scale auctions in reality, a substantial number of ad opportunities are fed into the environment and configured with dozens of bidding agents. These ad opportunities are generated using deep generative networks to reduce the gap between the simulation environment and reality while avoiding the risks of sensitive data exposure. The agents are equipped with diverse and sophisticated auto-bidding algorithms.

### The Ad Opportunity Generation Module

The target of the ad opportunity generation module is to generate diverse ad opportunities similar to real online advertising data with deep generative networks, as shown in Figure 2. We aimed to adopt the diffusion model to generate ad opportunity but encountered difficulties with the denoising operation, which can yield unreasonable outputs. Therefore, we followed the approach of the Latent Diffusion Model (LDM) [25] to generate ad opportunity. LDM adds noise and performs denoising in the latent space using a diffusion model, and then generates data from the latent space with an encoder and decoder. Specifically, LDM maps the ad opportunity feature \(u\) to a latent vector \(y\) with the encoder and reconstructs this feature with the decoder during training. For generation, LDM samples a random latent vector from a normal distribution and then generates an ad opportunity feature based on this vector. Let \(U\subset\mathbb{R}^{d}\) be the space of ad opportunity feature data \((u_{1},u_{2},\cdots,u_{K})\), where \(d\) is the dimension of the original data and \(K\) is the number of ad opportunities. Let \(Y\subset\mathbb{R}^{d^{\prime}}\) be the latent space (\(d^{\prime}<d\)). The encoder and decoder are represented as \(g_{\phi}\) and \(h_{\psi}\), respectively, where \(\phi\) and \(\psi\) are the parameters. The function of the encoder \(g_{\phi}\) is to obtain a latent representation of the original data as \(g_{\phi}(u_{k})=(\mu_{k},\sigma_{k})\), where \(y_{k}\sim\mathcal{N}(\mu_{k},\sigma_{k}^{2})\) and \(y_{k}\in Y\) is the latent representation. In practice, the reparameterization trick [20] is applied to ensure that this operation is differentiable during backpropagation.

Given the latent representation \(y_{k}\), the decoder is responsible for reconstructing the original data from \(y_{k}\), _i.e._, \(h_{\psi}(y_{k})=\tilde{u}_{k}\in U\). In addition to the reconstruction, the latent distribution \(\mathcal{N}(\mu_{k},\sigma_{k}^{2})\) is expected to approximate the standard Gaussian distribution \(\mathcal{N}(0,1)\). Therefore, we have the following loss function for the encoder and decoder:

\[\mathcal{L}_{\text{recons}}=\frac{1}{K}\sum_{k=1}^{K}\left\|u_{k}-h_{\psi}(y_{ k})\right\|_{2}^{2},\quad\mathcal{L}_{\text{reg}}=\frac{1}{K}\sum_{k=1}^{K}D_{ \mathrm{KL}}\left(\mathcal{N}(\mu_{k},\sigma_{k}^{2})\big{\|}\mathcal{N}(0,1 )\right),\]

where \(\mathcal{L}_{\text{recons}}\) is the reconstruction loss and \(\mathcal{L}_{\text{reg}}\) is the regularization loss for the latent distribution.

Different from the original idea of VAE [20], where the latent variable \(y\in Y\) is sampled from \(\mathcal{N}(0,1)\) in the generation process, LDM uses a diffusion model in the latent space to generate the latent variable. In general, the idea behind the diffusion model is to add Gaussian noise to the original data to obtain variables that follow \(\mathcal{N}(0,1)\) and to denoise from \(\mathcal{N}(0,1)\) for generation. Given a

Figure 2: Overview of the pipeline of the ad opportunity generation network. The generation process consists of two stages. In the first stage, ad opportunity features are generated through a latent diffusion model. In the second stage, the value prediction for the generated ad opportunity features is performed, incorporating both the time feature and the advertiser feature. Moreover, the volume of ad opportunities fluctuates over time, mirroring that of real-world online advertising platforms.

latent variable \(y\), we denote its noisy version after \(p\) iterations as \(y_{p}\). The diffusion model includes a network to predict noise \(\epsilon_{\theta}(y_{p},p)\), and the loss function can be represented as

\[\mathcal{L}_{LDM}=\frac{1}{K}\sum_{k=1}^{K}\left\|\epsilon_{k}-\epsilon_{\theta }(y_{k,p_{k}},p_{k})\right\|_{2}^{2},\]

where \(\epsilon_{k}\sim\mathcal{N}(0,1)\), \(y_{k}\) is the latent embedding of \(u_{k}\), and \(p_{k}\) is uniformly sampled from the set \(\{1,2,\cdots,p_{\max}\}\). The network \(\epsilon_{\theta}(y_{p},p)\) is the only learnable component in the diffusion model, which enables the process of adding noise and denoising through basic operations.

As for the generation process, a latent variable \(\bar{y}\) is sampled from \(\mathcal{N}(0,1)\), and \(\tilde{y}\) is obtained through \(p_{\max}\) denoising steps from \(\bar{y}\) using the noise prediction network \(\epsilon_{\theta}\). Finally, the decoder generates an ad opportunity feature based on \(\tilde{y}\) as \(\tilde{u}=h_{\psi}(\tilde{y})\).

Given an ad opportunity feature \(u_{k}\), we also need to determine the value of this ad opportunity combined with the category information of the corresponding advertiser \(q_{k}\) and the time information \(u_{k}^{\mathrm{time}}\), where \(q_{k}\) is the advertiser information in the real-world data associated with \(u_{k}\). We use Multi-head Attention (MHA) [31] as the network architecture for information integration. Let \(v_{\xi}\) represent the value prediction module, and \(v_{\xi}(u_{k},q_{k},u_{k}^{\mathrm{time}})\) denote the predicted value of the ad opportunity feature \(u_{k}\) for a specific advertiser at a specific time step. The loss of the value prediction model is shown below:

\[\mathcal{L}_{\mathrm{pred}}=\frac{1}{K}\sum_{k=1}^{K}\left\|v_{k}-v_{\xi}(u_{k },q_{k},u_{k}^{\mathrm{time}})\right\|_{2}^{2},\]

where \(v_{k}\) is the true value of the ad opportunity in the record associated with \(u_{k}\).

### The Bidding Module

The bidding module replicates the dynamic competition between advertisers, each of whom has distinct advertising objectives and utilizes a separate auto-bidding agent, while remaining unaware of their competitors' strategies. Researchers can control a subset of the agents in the environment, while other agents remain uncontrollable, thereby better reflecting the complex and dynamic game in real-world online advertising.

Several algorithms in the auto-bidding area have been implemented as baselines, including the PID Controller [36], Online LP [15], IQL [21], Behavior Cloning [30], and Decision Transformer [8]. This facilitates researchers who are interested in quickly starting up and evaluating these baselines in a unified environment.

### The Auction Module

The task of the auction module is to determine the winner and the winning price given all the bids from agents for ad opportunities. The costs for agents will vary depending on the different auction rules. The most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction, which stipulates that the winner pays a cost slightly higher than the second-highest bid rather than the highest bid. The auction module internally supports several popular auction rules, including GSP, for the convenience of researchers. Additionally, researchers can design specific auction rules tailored to their purposes using the interface of the auction module.

Additionally, the property of multiple slots has been implemented in the environment. Multiple slots arise from applications in the industry, meaning that a single ad opportunity may have multiple ad slots for display. A slot with a higher exposure rate is more valuable to advertisers. Suppose the number of slots is \(l\), then the auction module will allocate \(l\) slots to the top \(l\) bidders, and these bidders will receive different values according to the varying exposure rates of the slots. In summary, the multiple slots feature increases the complexity of the optimal bidding strategy, as the exposure rate serves as a discount factor for both cost and value.

### Api

The code of the environment is implemented in Python. The environment API is similar to OpenAI Gym[5], so the construction and interactions of the environment may be familiar to related researchers. We included an example code as follows:

## 4 Pre-Generated Dataset Based on the Environment

In this section, we first verify whether the ad opportunity generation module can generate ad opportunity features similar to those in real-world data. Next, we briefly introduce and analyze the dataset generated from the AuctionNet environment.

### Verification of the Ad Opportunity Generation Module

In order to better demonstrate that the generated data can reflect the properties of real-world data, the effectiveness of the ad opportunity generation module itself was verified. The ad opportunity

Figure 3: The 3D PCA results of 100K generated data and 100K real-world data.

generation module comprises two components: a feature generation model and a value prediction model. Experiments were conducted to verify the effectiveness of these models.

We randomly sample 100K real-world online advertising data points to compare with 100K generated data points. The details of the generated data can be found in Appendix D. First, we perform PCA [19] to visualize the similarity between the real-world and generated data. The 3D PCA results are illustrated in Figure 3. For better presentation, we use six different views in the 3D space. We observe that the generated data overlap with the original data in the 3D space. Moreover, the generated data points form four main separate clusters in the 3D space, similar to the real-world data points. These visualization results demonstrate that the generated data generally resemble the real-world data.

To further compare these two datasets, we study the value distributions of identity information and consumption behavior information in both datasets. The empirical results are included in Figure 4 and Figure 5. The feature vector contains over 20 fields, as described in Appendix D, so we only select a subset of these fields for our experiments. Regarding identity information, the generated value distributions are similar to the real-world value distributions overall, although biases exist for certain terms, such as 'level 7' for the Taobao VIP Level. Distributions with more categories are more challenging to match, while the gender distributions are nearly identical in both datasets. For consumption behavior information, we observe that the distributions in the selected fields share a strong resemblance and exhibit long-tail characteristics. A long-tail distribution indicates that most users do not engage in frequent consumption, and users with a high volume of consumption behavior are rare. This phenomenon aligns with our experience in online advertising.

We investigate whether the generated data can capture the connections between different fields. Based on the observation that users with higher VIP levels typically exhibit a higher volume of consumption behavior, we examine the connection between the Taobao VIP level and consumption behavior. We select four consumption behavior fields. The mean values of these fields across different VIP levels are shown in Figure 6. We find that the overall monotonically increasing trend is captured by the generated data, although biases exist in the specific values. Moreover, the drop in values from 'level 7' to 'level 8' is also captured by the generated data in three out of the four fields, except for the consumption amount. The rarity of 'level 8' data points may be the reason why the generative model is unable to distinguish different trends for different fields.

In real-world online advertising, the metrics for bidding strategy evaluation are Click-Through Rate (CTR) and Conversion Rate (CVR). Bidding strategies make decisions based on the predicted CTR (pCTR) and predicted CVR (pCVR), which are the estimated values of CTR and CVR, respectively. For simplicity, in this environment, we assume that the estimations are accurate and define the value as \(\mathrm{value}=\mathrm{pCTR}\cdot\mathrm{pCVR}\). Our value prediction model learns to predict pCTR and pCVR and subsequently calculates the value. We predict the pCTR, pCVR, and value for 100K real-world data points and compare these predictions with the real-world ground truth.

We hope that the value prediction model can capture the value variation over changes in category and time. The means of predicted pCTR, pCVR, and values across different categories and time steps,

Figure 4: The distribution of identity information including the Taobao VIP level, the preferred phone price, the buyer level, and the gender in 100K generated data and 100K real-world data.

Figure 5: The distribution of consumption behavior information including the number of collected items, the number of visited items, the number of collected sellers, and the consumption amounts in 100K generated data and 100K real-world data.

compared with the ground truth, are illustrated in Figure 7. The empirical results show that, in general, the variation trends in predictions over changes in category and time are similar to the ground truth.

To present the results more intuitively, we provide additional quantitative results. We compare the mean squared error (MSE) between the generated and original distributions with the standard deviation of the original distribution. The quantitative results are shown in Table 1. It can be observed that the MSEs are all smaller than the original standard deviations (original_stds), indicating that our prediction model can capture the patterns of value variation and is accurate.

### Pre-Generated Dataset

The dataset is derived from game data generated within the environment, where numerous auto-bidding agents compete against each other. We have pre-generated large-scale game data to assist researchers in gaining deeper insights into the auction ecosystem. This data can be used to model the environment and to train the auto-bidding agents effectively.

The dataset contains 10 million ad opportunities, including 21 advertising episodes. Each episode contains more than 500,000 ad opportunities, divided into 48 steps. Each opportunity includes the top 48 agents4 with the highest bids. The dataset comprises over 500 million records, totaling 80 GB in size. Each record includes information such as the predicted value, bid, auction, and impression results, among other details. The specific data format and data samples of the dataset are included in Appendix C.

Footnote 4: Real-world data show that 48 agents can ensure competitive pressure for auto-bidding agent training.

We have conducted an analysis of the AuctionNet Dataset to provide some insights. We first investigate the variation of impression values over time within a single day. We selected five categories from the AuctionNet Dataset and denote them as Category 1, Category 2, and so on. As shown in Figure 8, the impression values of different categories exhibit distinct patterns of variation. Given the budget constraint, agents should consider the variation in impression values over time to bid for appropriate impressions at the optimal times. Furthermore, we examine the relations between the values of different categories. The relations between Category 1 and other categories are illustrated in Figure 9.

\begin{table}
\begin{tabular}{|l|c|c|} \hline  & original\_std & MSE \\ \hline pCVR\_category & 0.0685 & **0.0341** \\ \hline pCTR\_category & 0.0517 & **0.0280** \\ \hline value\_category & 0.00573 & **0.00496** \\ \hline pCVR\_time & 0.0637 & **0.0313** \\ \hline pCTR\_time & 0.0590 & **0.0259** \\ \hline value\_time & 0.00625 & **0.00176** \\ \hline \end{tabular}
\end{table}
Table 1: The comparison of the MSE between the generated and original distribution with the standard deviation of the original distribution.

Figure 6: The mean values of consumption behavior information including the number of cart items, the number of collected items, the consumption amounts, and the number of visited categories in different VIP levels in 100K generated data and 100K real-world data.

Figure 7: The means of the predicted pCTR, pCVR, and value in different categories and time steps compared with the ground truth. The shaded areas are related to the standard deviation.

The impression values of Category 1 and Category 3 are positively correlated, indicating that the corresponding advertisers are competitors for similar ad opportunities. Therefore, considering the preferences of other agents may be beneficial for developing better bidding strategies. The full datasheet of the dataset is included in Appendix B.

## 5 Performance Evaluations of Baseline Algorithms

In this section, we evaluate the performance of baseline algorithms, such as linear programming, reinforcement learning, and generative models. It is important to note that we used the original algorithms from the papers and did not perform any special optimization on the methods specifically for the auto-bidding tasks. We provide a brief introduction to these baselines. The idea of the PID Controller is straightforward: it uses three parameters, \(\lambda_{P}\), \(\lambda_{I}\), and \(\lambda_{D}\), for Proportional Control, Integral Control, and Derivative Control, respectively. In this baseline, the PID Controller is employed to control the cost or bids of agents. Online LP utilizes linear programming for the auto-bidding problem. At each time step, Online LP solves a knapsack problem using a greedy algorithm. IQL is an offline RL algorithm. The core idea behind IQL is to evaluate the offline Q-function only on actions that appeared in the offline data, thereby avoiding overestimation in out-of-distribution data. Behavior Cloning (BC) is a supervised learning algorithm that uses expert trajectories. The agent's policy is learned by predicting the expert's actions in the state of given trajectories. Decision Transformer (DT) leverages the capabilities of the Transformer model[31] for sequential decision-making. DT treats the trajectories in a MDP as sequences and predicts actions based on previous transitions. More generative models such as AIGB [12] will also be integrated into baseline algorithms in the future. To better illustrate the performances, we add a heuristic method, Aibid, to the experiments. Aibid means the agent will give a fixed bid rate for all impressions. Its performance can be seen as a reference in comparison. More details of the evaluation can be found in Appendix A.

The empirical results are included in Figure 10. For better illustration, we normalize the performances of all baselines by the mean episode reward of the heuristic baseline Aibid. Therefore, the mean relative performance of Aibid is $1.0$ in the basic task. Online LP achieves the best performance, possibly because it is relatively robust and does not require special adaptation for auto-bidding tasks to achieve good results. Although methods like IQL and BC perform not as well as Online LP, we observe that proposing optimized solution [12; 22]can significantly optimize the performance, proving that such methods have great potential for optimization. In addition, the drop in rewards observed for all baselines during the target CPA task is due to the CPA penalty for exceeding constraints in (4).

## 6 Applications

AuctionNet has powered the the NeurIPS 2024 competition "Auto-bidding in Large-Scale Auctions" [1]. The competition addressed the critical issue of making high-frequency bid decision-making in uncertain and competitive environments and attracted more than 1,500 teams

Figure 8: The joint value distribution between different categories and time in the dataset.

Figure 9: The joint value distribution between Category 1 and other categories in the dataset.

from around the world to participate, lasting for 4 months. The ad auction environment, dataset, and baseline bid decision-making algorithms used in the competition are derived from this benchmark. The ad auction environment provided nearly ten thousand evaluations for the competition, offering participants accurate and fair performance assessments. The dataset and baseline algorithms allowed participants to quickly start the task and stimulated their creativity, leading to more diverse and innovative solutions, thus driving technological development in this area.

## 7 Related Work

Simulation environments have been widely applied in decision-making research and have successfully promoted the development of related studies [6; 24; 32; 27; 29]. However, simulation environments for real-world online advertising platforms are relatively scarce in the bid decision-making field. AuctionGym [18] models the bidding problem as a contextual bandit problem [2], where the advertiser decides the bidding value given the information of the ad opportunity as context. The contextual bandit has only one time step per episode, meaning that AuctionGym does not consider budget constraints in auto-bidding. Moreover, AuctionGym describes the auto-bidding problem from a single-agent perspective and ignores the influence of other agents. AdCraft [11] is a simulation environment for the bidding problem in Search Engine Marketing (SEM). Although AdCraft explicitly models the influences of other agents, these agents' policies are sampled from parameterized distributions, which cannot fully reflect the multi-agent nature of this problem. Despite the points discussed above, these existing simulation environments lack data-driven methods for modeling real-world online advertising platforms.

## 8 Conclusion and Limitations

We present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet consists of three components: an ad auction environment augmented with verified deep generative networks, a pre-generated dataset based on this environment, and performance evaluations of several baseline bid decision-making algorithms. The AuctionNet not only provides researchers with the opportunity to study auto-bidding algorithms in large-scale auctions, but also helps researchers and practitioners in game theory, reinforcement learning, generative models, operations optimization, and other fields to solve a wide range of decision-making research problems. Regarding limitations, while the generated data in the AuctionNet environment and the real-world data are similar in general, there are biases in some details, and the performance of the generative model can be improved.

## 9 Acknowledgments

This work was supported in parts by NSFC under grants 62450001 and 62476008 and Alibaba Group through Alibaba Innovative Research Program. The authors would like to thank the anonymous reviewers for their valuable comments and advice.

Figure 10: The empirical results of baseline algorithms on the basic task and Target CPA task.

## References

* [1] NeurIPS 2024 Competition: Auto-Bidding in Large-Scale Auctions. https://tianchi.aliyun.com/specials/promotion/neurips2024_alimama#/?lang=en_us.
* [2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _International conference on machine learning_, pages 127-135. PMLR, 2013.
* [3] Santiago R Balseiro, Omar Besbes, and Gabriel Y Weintraub. Repeated auctions with budgets in ad exchanges: Approximations and design. _Management Science_, 61(4):864-884, 2015.
* [4] S. Bennett. Development of the pid controller. _IEEE Control Systems Magazine_, 13(6):58-62, 1993.
* [5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
* [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
* [7] Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, and Maria Kyropoulou. On the efficiency of equilibria in generalized second price auctions. In _Proceedings of the 12th ACM conference on Electronic commerce_, pages 81-90, 2011.
* [8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [9] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords. _American economic review_, 97(1):242-259, 2007.
* [10] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, 2021.
* [11] Maziar Gomrokchi, Owen Levin, Jeffrey Roach, and Jonah White. Adcraft: An advanced reinforcement learning benchmark environment for search engine marketing optimization. _arXiv preprint arXiv:2306.11971_, 2023.
* [12] Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, and Yan Zhang. Aigb: Generative auto-bidding via conditional diffusion modeling. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 5038-5049, 2024.
* [13] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in finance. _Mathematical Finance_, 33(3):437-503, 2023.
* [14] Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In _AAAI_, volume 4, pages 709-715, 2004.
* [15] Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, Shan Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, et al. Dynamic knapsack optimization towards efficient multichannel sequential advertising. In _International Conference on Machine Learning_, pages 4060-4070. PMLR, 2020.
* [16] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. A unified solution to constrained bidding in online display advertising. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2993-3001, 2021.
* [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.

* [18] Olivier Jeunen, Sean Murphy, and Ben Allison. Off-policy learning-to-bid with auctiongym. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4219-4228, 2023.
* [19] Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments. _Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences_, 374(2065):20150202, 2016.
* [20] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [21] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _Deep RL Workshop NeurIPS 2021_, 2021.
* [22] Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, and Fan Wu. Trajectory-wise iterative reinforcement learning framework for auto-bidding. In _Proceedings of the ACM on Web Conference 2024_, pages 4193-4203, 2024.
* [23] Brendan Lucier, Renato Paes Leme, and Eva Tardos. On revenue in the generalized second price auction. In _Proceedings of the 21st international conference on World Wide Web_, pages 361-370, 2012.
* [24] C Berner OpenAI, Greg Brockman, Brooke Chan, Vicki Cheung, P Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [27] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. _arXiv preprint arXiv:1902.04043_, 2019.
* [28] Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, and Xiaobing Liu. Spending programmed bidding: Privacy-friendly bid optimization with roi constraint in online advertising. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 5731-5740, 2024.
* [29] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, 2012.
* [30] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In _Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence_. International Joint Conferences on Artificial Intelligence Organization, 2018.
* [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [32] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* [33] Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. Budget constrained bidding by model-free reinforcement learning in display advertising. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_, pages 1443-1451, 2018.

* [34] Yuxin Wu, Tianyang Zhao, Haoyuan Yan, Min Liu, and Nian Liu. Hierarchical hybrid multi-agent deep reinforcement learning for peer-to-peer energy trading among multiple heterogeneous microgrids. _IEEE Transactions on Smart Grid_, 2023.
* [35] Wei Zhang, Yanjun Han, Zhengyuan Zhou, Aaron Flores, and Tsachy Weissman. Leveraging the hints: Adaptive bidding in repeated first-price auctions. _Advances in Neural Information Processing Systems_, 35:21329-21341, 2022.
* [36] Weinan Zhang, Yifei Rong, Jun Wang, Tianchi Zhu, and Xiaofan Wang. Feedback control of real-time display advertising. In _Proceedings of the Ninth ACM International Conference on Web Search and Data Mining_, pages 407-416, 2016.
* [37] Yiheng Zhu, Yang Zhan, Xuankun Huang, Yuwei Chen, Jiangwen Wei, Wei Feng, Yinzhi Zhou, Haoyuan Hu, Jieping Ye, et al. Ofcourse: A multi-agent reinforcement learning environment for order fulfillment. _Advances in Neural Information Processing Systems_, 36, 2024.

Evaluation Details

There are 48 agents of 7 types in our experiments and each type corresponds to one algorithm. We test \(7\) rounds where we permute the order of agents in each round. Therefore, agents will represent different advertisers with different budgets in different rounds. We choose the best agent as the representative of an algorithm if there are multiple agents of this algorithm. We use the average performances of the 7 rounds as the final performance of all the algorithms. We provide the model file of these agents and the evaluation code for reproduction.

## Appendix B Datasheet for AuctionNet

We present a datasheet[10] for the AuctionNet Dataset.

### Motivation

#### For what purpose was the dataset created?

In general, learning from interactions with the real-world online advertising platforms is difficult and expensive, so offline RL algorithms are more popular in auto-bidding. Therefore, we build the Auction Dataset to facilitate offline training of users. Moreover, the Auction Dataset will also be provided to the participants of the competition we will hold in the future.

#### Who created the dataset?

The dataset was created by the authors of this paper. The dataset was not created on the behalf of any entity.

#### Who funded the creation of the dataset?

Alibaba Group funds the creation of the AuctionNet Dataset.

### Composition

What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?

The AuctionNet Dataset contains trajectories of diverse agents competing with each other. Please refer to Appendix C and Section 4.2 for more details.

#### Is there a label or target associated with each instance?

The AuctionNet Dataset contains offline trajectories where the actions or bids of agents can be seen as labels for the time step.

#### Is any information missing from individual instances?

Not to our knowledge.

#### Are there recommended data splits (e.g., training, development/validation, testing)?

No.

#### Are there any errors, sources of noise, or redundancies in the dataset?

The AuctionNet Dataset contains trajectories of diverse agents, some of these agents may not perform well. However, the tasks in the environment are still difficult for some algorithms and we think keeping agents diverse in the AuctionNet Dataset is beneficial.

#### Do/did we do any data cleaning on the dataset?

We did not. All data is presented exactly as collected.

### Collection Process

#### How was the data associated with each instance acquired?

The AuctionNet Dataset is collected from the interactions of baseline agents in the environment.

**Who was involved in the data collection process and how were they compensated?**

The data collection process is done by the authors and not involve with any crowdsource.

**Over what timeframe was the data collected?**

The AuctionNet Dataset was collected between March 2024 and May 2024.

### Uses

**Has the dataset been used for any tasks already?**

No.

**Is there a repository that links to any or all papers or systems that use the dataset?**

No.

**Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?**

We do not believe so since the AuctionNet Dataset consists of data generated by the interactions of baseline agents.

### Distribution

**Will the dataset be distributed to third parties?**

Yes, but the AuctionNet Dataset and environment are involved with a large competition we will hold in NeurIPS 2024, so we will not distribute them until the end of the competition considering competition fairness. However, we will open-source the AuctionNet Dataset as soon as possible.

**How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?**

The AuctionNet Dataset will be distributed by a Github link after the end of the competition we will hold. The AuctionNet Dataset doesn't have a digital object identifier now.

All data is under the MIT license.

**Have any third parties imposed IP-based or other restrictions on the data associated with the instances?**

No.

**Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?**

No.

### Maintenance

**Who will be supporting/hosting/maintaining the dataset?**

The authors of this paper will provide needed maintenance to the datasets.

**How can the owner/curator/manager of the dataset be contacted (e.g., email address)?**

Please email us at huoyusen.huoyusen@alibaba-inc.com.

**Is there an erratum?**

There is not and we believe generated features, predicted values, and trajectories in our datasets do not involve an erratum.

**Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?**

Yes, but as we won't add extra data points, the update will be minimal.

Data Format of AuctionNet Dataset

The specific data format of the AuctionNet Dataset is as follows:

1. deliveryPeriodIndex: The index of the current delivery period.
2. advertiserIndex: The unique identifier of the advertiser.
3. advertiserCategoryIndex: The index of the advertiser's category.
4. budget: The advertiser's budget for a period.
5. CPConstraint: The CPA constraint of the advertiser.
6. timeStepIndex: The index of the current decision time step.
7. remainingBudget: The advertiser's remaining budget before the current step.
8. pvIndex: The index of the ad opportunity.
9. pValue: The conversion probability when the ad is exposed to the user.
10. pValueSigma: The variance of predicted probability.
11. bid: The agent's bid of the ad opportunity.
12. xi: The winning status of the agent of the ad opportunity.
13. adSlot: The won ad slot.
14. cost: The cost needs to be paid if the ad is exposed to the user.
15. isExposed: The indicator signifying whether the ad in the slot was displayed to the user.
16. conversionAction: The indicator signifying whether the conversion action has occurred.
17. leastWinningCost: The minimum cost to win the ad opportunity.
18. isEnd: The completion status of the advertising period.

Table 2 presents an ad opportunity involving the top five advertisers. The top three advertisers, numbered 31, 22, and 15, won the ad opportunity with the highest bids and were allocated to ad slots 1, 2, and 3, respectively. During this impression, slots 1 and 2 were exposed to the user, while slot 3 remained unexposed. Consequently, ads in slots 1 and 2 need to pay 0.2702 and 0.2154, respectively. Additionally, the user engaged in a conversion action with the ad in slot 2.

Table 3 presents a data sample illustrating an advertiser's bidding process across time s

[MISSING_PAGE_FAIL:17]

* payOrdltmQty: Represents the number of items the user bought in the last one month, one year, three months and six months. Data format: integers, dimension \([150,154)\).
* pvAndlpv: Represents the PV and IPV value of the user bought in the last month. Data format: float numbers, dimension \([154,156)\).
* vstSlrCnt: Represents the number of sellers the user visited in the last month. Data format: integers, dimension \([156,157)\).
* vstCateCnt: Represents the number of categories the user visited in the last month. Data format: integers, dimension \([157,158)\).
* vstlImCnt: Represents the number of items the user visited in the last month. Data format: integers, dimension \([158,159)\).
* vstlImCnt: Represents the number of items the user visited in the last month. Data format: integers, dimension \([158,159)\).
* vstDays: Represents the number of days the user visited items in the last month. Data format: integers, dimension \([159,160)\).
* stayTimeLen: Represents the number of seconds the user spent on visiting items in the last month. Data format: integers, dimension \([160,161)\).
* cartImCnt: Represents the number of items the user added to the cart in the last one week, two weeks, one month, three months, and six months. Data format: integers, dimension \([161,166)\).
* cltlSlrCnt: Represents the number of sellers the user collected in the last one week, two weeks, one month, six months, and one year. Data format: integers, dimension \(\{166,168,170,172,174\}\).
* cltltltmCnt: Represents the number of items the user collected in the last one week, two weeks, one month, six months, and one year. Data format: integers, dimension \(\{167,169,171,173,175\}\).

**Structure of the value vector.** The value vector has 60 dimensions corresponding to 59 categories involved in our environment and one conserved category for the undefined or unknown category. The corresponding relations between the dimension indexes and categories are listed in Table 4.

## Appendix E Tasks

Though AuctionNet provides a general framework for auto-bidding problem studies, we choose two typical scenarios in auto-bidding as tasks in AuctionNet for easier understanding.

### Basic Task

Our basic task is based on the scenario Budget Constrained Bidding (BCB) [33], where agents maximize their obtained values within the constraint on the budget. The optimization formulation of BCB from agent \(i\)'s perspective is as follows:

\[\begin{split}\underset{\{\alpha_{i}^{t}\}}{\text{maximize}}& \sum_{t=1}^{T}\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle\\ \mathrm{s.\,t.\,}&\sum_{t=1}^{T}\left\langle\bm{x }_{i}^{t},\bm{c}_{i}^{t}\right\rangle\leq\omega_{i},\end{split}\] (2)

where \(\bm{x}_{i}^{t}=(x_{i1}^{t},x_{i2}^{t},\cdots,x_{im}^{t})\) is the auction result of all ad opportunities for agent \(i\) in time step \(t\), \(\bm{v}_{i}^{t}=(v_{i1}^{t},v_{i2}^{t},\cdots,v_{im}^{t})\) is the value for agent \(i\), \(\bm{c}^{t}=(c_{i1}^{t},c_{i2}^{t},\cdots,c_{im}^{t})\) is the cost in time step \(t\), \(b_{i}\) is the budget for agent \(i\), and \(\left\langle\cdot\right\rangle\) is the inner product.

As for the implementation, we know from our problem formulation that \(r_{i}(s_{t},\bm{a}_{t})=\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle\), so the objective in the optimization formulation is the same as the objective \(\sum_{t=1}^{T}r_{i}(s_{t},\bm{a}_{t})\) in the RL formulation. The budget constraint is guaranteed by ignoring the bids exceeding agents' budgets in the environment. Therefore, BCB corresponds to the default setting of AuctionNet.

### Target CPA Task

We propose Target CPA Task based on the real-world scenario Target CPA (Cost Per Action)5 with some simplifications for understanding. The CPA of agent \(i\) is defined as \(\mathrm{cpa}_{i}=\frac{\sum_{t=1}^{T}\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t} \right\rangle}{\sum_{t=1}^{T}\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle}\), which can be seen as the cost taken by agent \(i\) for unit value. A low CPA means the budgets are consumed to obtain values effectively. Based on the basic task, Target CPA Task adds one more constraint on CPA that \(\mathrm{cpa}_{i}\) should be lower than the desired CPA \(d_{i}\). The formulation is as follows:

Footnote 5: https://support.google.com/google-ads/answer/6268632

\[\begin{split}\underset{\{\alpha_{i}^{t}\}}{\text{maximize}}& \sum_{t=1}^{T}\left\langle\bm{x}_{i}^{t},\bm{v}_{i}^{t}\right\rangle\\ \mathrm{s.\,t.}&\sum_{t=1}^{T}\left\langle\bm{x}_{i }^{t},\bm{c}_{i}^{t}\right\rangle\leq\omega_{i}\\ &\mathrm{cpa}_{i}\leq d_{i}.\end{split}\] (3)

Given that CPA can only be calculated at the end of one episode, the environment will only provide a sparse reward in Target CPA Task, which is different from the basic task. Unlike the budget constraint which cannot be violated in the environment, we allow agents to violate the CPA constraint, but we will penalize those agents for violations on their obtained values based on their CPA CPA \(\mathrm{cpa}_{i}\). The sparse reward formulation in Target CPA Task is as follows:

\[r_{i}^{\mathrm{CSB}}=p(\mathrm{cpa}_{i};d_{i})\sum_{t=1}^{T}\left\langle\bm{x }_{i}^{t},\bm{v}_{i}^{t}\right\rangle,\] (4)

\begin{table}
\begin{tabular}{|c|l|l|l|} \hline
**ID** & **Category** & **ID** & **Category** \\ \hline
1 & Snacks & 31 & Travel Services \\
2 & Personal Care & 32 & Tmall Home \& Living \\
3 & Electric Vehicles & 33 & Maternity \& Childcare \\
4 & Tmall Underwear & 34 & Movies, Shows \& Sports \\
5 & Smart Toys \& Games & 35 & Education \& Teaching \\
6 & Tea & 36 & Taobao Bags \& Accessories \\
7 & Household Cleaning & 37 & Taobao Underwear \\
8 & Chilled Food & 38 & Audio \& Video Electronics \\
9 & Tmall Women’s Clothing & 39 & Gaming \\
10 & Enterprise Services & 40 & Pets \\
11 & Dairy Products & 41 & Vehicles \\
12 & Fragrances and Aromatherapy & 42 & Major Appliances \\
13 & Life Services & 43 & Tmall Footwear \\
14 & Household Appliances & 44 & Food Coupons \\
15 & Taobao Men’s Clothing & 45 & Auto Accessories \\
16 & Tmall Home Decor & 46 & Mobile Phones \\
17 & Taobao Home \& Living & 47 & Taobao Footwear \\
18 & Taobao Home Decor & 48 & Grains \& Instant Food \\
19 & Instant Drinks & 49 & Tmall Bags \& Accessories \\
20 & Alcohol & 50 & Mobile \& Digital Accessories \\
21 & Taobao Women’s Clothing & 51 & Taobao Watches \& Glasses \\
22 & Auto Aftermarket & 52 & Jewelry \& Accessories \\
23 & Fruits and Vegetables & 53 & Sports \\
24 & Flowers and Gardening & 54 & Toys \& Fun \\
25 & Office \& School Supplies & 55 & Entertainment Recharge \\
26 & Computers & 56 & Beverages \\
27 & Tmall Watches \& Glasses & 57 & Outdoor \\
28 & Computer Accessories & 58 & Motorcycles \\
29 & Aquatic Products, Meat, Poultry \& Eggs & 59 & Tmall Men’s Clothing \\
30 & Cosmetics & 0 & Other \\ \hline \end{tabular}
\end{table}
Table 4: Corresponding relations for categories.

where \(p(\mathrm{cpa}_{i};d_{i})=\min\left\{\left(\frac{d_{i}}{\mathrm{cpa}_{i}}\right)^{ \beta},1\right\}\) is the penalty function for exceeding the CPA constraint. The formulation of \(p(\mathrm{cpa}_{i};d_{i})\) implies that the penalty is incurred only when \(\mathrm{cpa}_{i}>d_{i}\). The parameter \(\beta>0\) is typically set to \(3\). Therefore, Target CPA Task can be implemented with modifications to the reward function in the basic task.

## Appendix F Baseline Algorithms

We have implemented multiple baseline algorithms in AuctionNet to facilitate a quick start-up and comprehensive understanding of users. The baseline algorithms include PID Controller[36], Online LP[15], IQL[21], Behavior Cloning[30], and Decision Transformer[8].

**PID Controller.** PID Controller is a traditional algorithm in the control field with a long history[4]. It is simple but effective in many scenarios. Recently, PID Controller has also been adopted in online advertising[36]. The idea of PID Controller is straightforward: PID Controller takes three parameters \(\lambda_{P}\), \(\lambda_{I}\), and \(\lambda_{D}\) for Proportional Control, Integral Control, and Derivative Control, respectively. We use the PID Controller to control the cost or bids of agents in this baseline.

**Online LP.** The optimization formulation (2) is a typical Linear Programming (LP) problem. Moreover, the variable \(x^{t}_{ij}\in\{0,1\}\) is binary, so the problem in each time step can be converted to a dynamic knapsack problem. Online LP solves this dynamic knapsack problem using a greedy algorithm.

**IQL.** Implicit Q-learning (IQL) is an offline RL algorithm. The idea of IQL is evaluating offline Q-function only on the actions that appeared in the offline data, to avoid the overestimation in the out-of-distribution data. In practice, IQL utilizes expectile regression to realize the offline Q-learning on in-distribution data.

**Behavior Cloning.** Behavior Cloning (BC) is a supervised learning algorithm given expert trajectories. The agent's policy learns by predicting the expert's action in the state of given trajectories. BC is a baseline for verifying the effectiveness of RL algorithms.

**Decision Transformer.** Decision Transformer (DT) utilizes the ability of Transformer[31] for sequential decision-making. DT views the trajectories in MDP as a sequence and predicts actions given previous transitions.

## Appendix G Implementation and Modules

The environment of AuctionNet consists of three main modules: the ad opportunity generation module, the auction module, and the bidding module. The general process of one time step in AuctionNet can be concluded as follows:

1. The ad opportunity generation module generates features \(\bm{u}=(u_{1},u_{2},\cdots,u_{m})\) and values \(\bm{v}=\{v_{ij}\}\) of \(m\) ad opportunities for \(n\) agents, where the number of ad opportunities \(m\) is sampled from an intern distribution within AuctionNet. This intern distribution is obtained from real-world online advertising statistics
2. Agents bid for all the ad opportunities considering the predicted values provided by the environment and the historical auction logs.
3. The auction module determines the winner of each auction, rewards, and costs by the auction mechanism.
4. Agents receive rewards, costs, and new auction logs. The budgets of all the agents are updated according to auction results. In the next time step, all the processes above will be repeated.

Given this general process, we will introduce the three main modules in order. The ad opportunity generation module will generate features \(\bm{u}\) of ad impressions related to the real online data. The ad auction module supports an auction similar to real-world online advertising and realizes several popular auction mechanisms for different research purposes. The bidding module supports explicitly modeling a multi-agent environment with several implemented baselines.

### Ad Opportunity Generation Module

The target of the ad opportunity generation module is to generate diverse ad opportunity features similar to real online advertising data. The core of this module is the generative model. The objective of the generative model in AuctionNet is to generate data resembling real advertising delivery data. Useful information in the real advertising delivery data can be divided into four parts: features of ad opportunities (users' information), features of advertisers, time when the ad opportunity arises, and the values of the ad opportunities. In our model, we simplify the feature of advertisers to be the advertisers' industry categories. We focus on the generation of ad opportunity features and take the categories and time as conditions. The generative model consists of two components: the generative model for ad opportunity features and the prediction model for the values.

**Feature Generation.** The ad opportunity feature contains two parts of information: the basic identity information of users and the consumption records such as the consumption amount. The identity information is discrete and the consumption records are continuous in general, which are processed with different measures. Diffusion [17] model is the most popular generative model recently which obtains SOTA performances in image generation with a simplified training process. We would like to adopt the diffusion model to generate the ad opportunity feature but struggle with the denoising operation which can result in unreasonable outputs such as a negative consumption amount. So we follow the idea of the Latent Diffusion Model (LDM)[25] to generate ad opportunity features. LDM adds noises and denoises in the latent space with a diffusion model and generates data from the latent space with an encoder and decoder. More details can be found in Appendix H.1.

**Value Prediction.** The value prediction model needs to handle three types of information: ad opportunity features, the industry category information of advertisers, and time information. We simplify the category and time information as discrete values. Therefore, we aim to integrate the category and time information into the ad opportunity features for better value prediction. Besides, we hope this integration can partly reflect the variation pattern of the impression values related to advertisers' features and time. Multi-head attention (MHA), as a popular network architecture and the critical part of Transformer [31], can capture the relations among a sequence, thus we hope to utilize MHA for better integration. We combine cross-attention and self-attention to integrate the three types of information. We also follow the idea of position embedding in the Transformer to process the time information. More details are included in Appendix H.2.

For the consideration of interaction efficiency in AuctionNet, the environment utilizes a dataset consisting of generated features and corresponding predicted values. More details of the dataset will be discussed in Section 4.1. Though the ad opportunity generation module is trained with real online advertising data, an important question is whether the generated data can reflect the properties of real data. Therefore, we have done several related experiments and the empirical results will also be discussed in Section4.1.

### Auction Module

The task of the auction module is to determine the winner and the winning price given all bids of agents for the ad ad opportunities. The costs of agents will change given different auction rules. The most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction which means the winner should pay a cost slightly higher than the second-highest bid instead of the highest bid. The auction module internally supports several popular auction rules including GSP for the convenience of researchers. Besides, researchers can also design a specific auction rule related to their purposes with the interface of the auction module.

Additionally, the property of multiple slots has been implemented in our simulation platform. Multiple slots emerge from the application in the industry, which means one ad opportunity has multiple ad slots for ad displays. The ad slots are ranked by their exposure rates. A higher exposure rate slot is more valuable for advertisers. Suppose the number of slots is \(l\), then the auction module will attribute \(l\) slots to the top \(l\) bidders and these bidders will receive different values according to different exposure rates of slots. In the environment, \(l\) is set to 3. Let \(\operatorname{slot}_{ij}\) represent the slot of ad opportunity \(j\) wired by agent \(i\) and \(e_{ij}\in[0,1]\) represent the exposure rate of \(\operatorname{slot}_{ij}\), then the optimization formulation of BCB with multiple slots is as follows:\[\begin{split}&\underset{\{\alpha^{t}_{i}\}}{\text{maximize}}\sum_{t=1}^{T} \sum_{j=1}^{m}e_{ij}^{t}x_{ij}^{t}y_{ij}^{t}\\ &\text{s.\,t.}\sum_{t=1}^{T}\sum_{j=1}^{m}e_{ij}^{t}x_{ij}^{t}c_{ ij}^{t}\leq\omega_{i},\end{split}\] (5)

In summary, the multiple slots property increases the complexity of the optimal bidding strategy, since the exposure rate is a discount factor for both the cost and values. For instance, a strategy using a lower budget to bid for slots with relatively lower ranks may be better than the strategy that always chases the highest value slot. We believe supporting multiple slots in AuctionNet will be beneficial to reducing the gap between related research and the real-world online advertising platforms.

### Bidding Module

The bidding module is responsible for processing the multi-agent interactions between advertisers. This module implements the budget constraint and models the auto-bidding problem with sequence decision-making. Therefore, AuctionNet supports the mainstream paradigms including Budget Constrained Bidding (BCB) [33] and Multiple Constraints Bidding (MCB) [16] in the auto-bidding field. This will help researchers validate and gain insights from existing algorithms.

In the bidding module, we explicitly model the multi-agent setting. Researchers can implement multi-agent algorithms to achieve competition or cooperation among different agents. The varying bidding strategies of other agents can better reflect the complex and dynamic auction environment in real-world online advertising platforms. Besides, researchers can only control a part of the agents in AuctionNet while others is uncontrollable. This scenario is closer to the real advertising platform. The multi-agent setting of AuctionNet can adapt to different research objectives.

There are several different metrics for different business goals of advertisers in online advertising platforms such as Return-on-Investment (ROI) and Return-On-Ad-Spend (ROAS). AuctionNet has several built-in metrics covering the popular metrics used by the major advertising platform. Researchers can adopt these metrics conveniently to evaluate the performances of their auto-bidding strategies. Besides, researchers can define customized metrics according to their research objectives. Additionally, several popular algorithms in the auto-bidding field have been implemented as baselines in AuctionNet, including PID Controller[36], Online LP[15], IQL[21], Behavior Cloning[30], and Decision Transformer[8]. This can facilitate the interested researchers to quickly start up and evaluate these baselines in a unified environment.

## Appendix H Details of Deep Generative Networks

### Ad Opportunity Generation

The ad opportunity feature contains two parts of information: one is the basic identity information of users including gender, age, address and so on; another is the consumption records such as the consumption amount and the number of orders in the last three months. The identity information consists of discrete fields and each field has several candidates. The consumption records are continuous in general. Therefore, we process these two types of information with different measures.

Diffusion [17] model is the most popular generative model recently which obtains SOTA performances in image generation with a simplified training process. The principle of the diffusion model is adding Gaussian noises to original data in training and denoising from Gaussian noises in the generation process. We would like to adopt the diffusion model to generate the ad opportunity feature but struggle with the denoising operation which can result in unreasonable outputs such as a negative consumption amount. So we follow the idea of the Latent Diffusion Model (LDM)[25]. LDM has a latent space to encode the original data. LDM combines the idea of diffusion model with VAE[20]. LDM adds noises and denoises in the latent space with a diffusion model and generates data from the latent space with an encoder and decoder.

Specifically, let \(U\subset\mathbb{R}^{d}\) be the space of ad opportunity feature data \((u_{1},u_{2},\cdots,u_{K})\) where \(d\) is the dimension of original data and \(K\) is the volume of the ad opportunity. Let \(Y\subset\mathbb{R}^{d^{\prime}}\) be the latent space \((d>d^{\prime})\). The encoder and decoder are represented as \(g_{\phi}\) and \(h_{\psi}\) respectively, where \(\phi\) and \(\psi\) are the parameters. The function of the encoder \(g_{\phi}\) is obtaining a latent representation of original data as follows:

\[g_{\phi}(u_{k})=(\mu_{k},\sigma_{k}),\quad y_{k}\sim\mathcal{N}( \mu_{k},\sigma_{k}^{2}),\]

where \(y_{k}\in Y\) is the latent representation. In practice, the reparameterize trick [20] is applied to make sure this operation is differentiable in the backpropagation. Given the latent representation \(y_{k}\), the decoder is responsible for reconstructing the original data from \(y_{k}\), _i.e._, \(h_{\psi}(y_{k})=\bar{u}_{k}\in U\). Besides the reconstruction, the latent distribution \(\mathcal{N}(\mu_{k},\sigma_{k}^{2})\) is expected to be close to the standard Gaussian distribution \(\mathcal{N}(0,1)\). Therefore, we have the following loss function for the encoder and decoder:

\[\mathcal{L}_{recons}=\frac{1}{K}\sum_{k=1}^{K}\left\|u_{k}-h_{ \psi}(y_{k})\right\|_{2}^{2},\quad\mathcal{L}_{reg}=\frac{1}{K}\sum_{k=1}^{K} D_{\mathrm{KL}}\left(\mathcal{N}(\mu_{k},\sigma_{k}^{2})\|\mathcal{N}(0,1) \right),\]

where \(\mathcal{L}_{recons}\) is the reconstruction loss and \(\mathcal{L}_{reg}\) is the regularization loss for the latent distribution.

Different from the original idea of VAE, where the latent variable \(y\in Y\) is sampled from \(\mathcal{N}(0,1)\) in the generation process, LDM uses a diffusion model in the latent space to generate the latent variable. In general, the idea of the diffusion model is adding Gaussian noises to the original data to obtain variables in \(\mathcal{N}(0,1)\) and denoising from \(\mathcal{N}(0,1)\) for generation. Given a latent variable \(y\), we denote the noisy version of \(y\) after \(p\) iterations as \(y_{p}\). The diffusion model has a network to predict noise \(\epsilon_{\theta}(y_{p},p)\) and the loss function can be represented as

\[\mathcal{L}_{LDM}=\frac{1}{K}\sum_{k=1}^{K}\|\epsilon-\epsilon_{ \theta}(y_{k,p_{k}},p_{k})\|_{2}^{2},\]

where \(\epsilon\sim\mathcal{N}(0,1)\), \(y_{k}\) is the latent embedding of \(u_{k}\) and \(p_{k}\) is uniformly sampled from the set \(\{1,2,\cdots,p_{\max}\}\). \(\epsilon_{\theta}(y_{p},p)\) is the only learnable network in the diffusion model, with which the process of adding noises and denoising can be completed by the basic operations.

As for the generation process, a latent variable \(\bar{y}\) is sampled from \(\mathcal{N}(0,1)\) and \(\tilde{y}\) is obtained by \(t_{\max}\) denoising steps from \(\tilde{y}\) given the noise prediction network \(\epsilon_{\theta}\). Finally, the decoder generates an ad opportunity feature based on \(\tilde{y}\) as \(\tilde{x}=h_{\psi}(\tilde{y})\).

### Value Prediction

The value prediction model needs to handle three types of information: ad opportunity features, category information and time information. The category information corresponds to the industry categories of advertisers and the time information corresponds to the time when the ad opportunity arrived. In our model, the category and time information are simplified as discrete values. Therefore, we aim to integrate the category and time information into the ad opportunity features for better value prediction. Besides, we hope this integration can partly reflect the variation pattern of the impression values related to advertisers' features and time.

Multi-head attention (MHA) [31] is a popular network architecture and the critical part of Transformer [31]. MHA can capture the relations among a sequence, thus we hope to utilize MHA for better integration. The formulation of the attention network is straightforward as \(\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})\cdot V\). Multi-head attention can be viewed as applying the attention network in different representation subspaces as follows:

\[\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_{1}, \mathrm{head}_{2},\cdots,\mathrm{head}_{h})W^{O},\] (6) \[\text{where }\mathrm{head}_{i}=\mathrm{Attention}(QW_{i}^{Q},KW_{ i}^{K},VW_{i}^{V}).\] (7)

\(W_{i}^{Q},W_{i}^{K},W_{i}^{V}\) are the parameters for the projection networks of head \(i\) and \(W^{O}\) is the output network parameters of the MHA model.

We combine cross-attention and self-attention to integrate the three types of information. Suppose \(u_{k}\), \(u_{k}^{\mathrm{time}}\) and \(q_{k}\) are the ad opportunity feature, time information and category information in a single record respectively, then we will process the information as follows:

\[Q^{(1)}=\tau_{Q}^{(1)}(u_{k}^{\rm time}),\quad K^{(1)}=\tau_{K}^{(1 )}(u_{k}),\quad V^{(1)}=\tau_{V}^{(1)}(u_{k}),\] \[z_{k}^{(1)}=\text{MultiHead}(Q^{(1)},K^{(1)},V^{(1)}),\] \[Q^{(2)}=\tau_{Q}^{(2)}(z_{k}^{(1)}),\quad K^{(2)}=\tau_{K}^{(2)} (z_{k}^{(1)}),\quad V^{(2)}=\tau_{V}^{(2)}(z_{k}^{(1)}),\] \[z_{k}^{(2)}=\text{MultiHead}(Q^{(2)},K^{(2)},V^{(2)}),\] \[Q^{(3)}=\tau_{Q}^{(3)}(q_{k}),\quad K^{(3)}=\tau_{K}^{(3)}(z_{k} ^{(2)}),\quad V^{(3)}=\tau_{V}^{(3)}(z_{k}^{(2)}),\] \[z_{k}^{(3)}=\text{MultiHead}(Q^{(3)},K^{(3)},V^{(3)}),\] \[Q^{(4)}=\tau_{Q}^{(4)}(z_{k}^{(3)}),\quad K^{(4)}=\tau_{K}^{(4) }(z_{k}^{(3)}),\quad V^{(4)}=\tau_{V}^{(4)}(z_{k}^{(3)}),\] \[z_{k}=\text{MultiHead}(Q^{(4)},K^{(4)},V^{(4)}),\]

where \(\tau^{(1)},\tau^{(2)},\tau^{(3)},\tau^{(4)}\) are the projection function for the multi-head attention network.

The variation of ad opportunity values has some temporal patterns in the real world. Therefore, we follow the position encoding idea in Transformer [31] and Diffusion Model [17] to process the time information. Let \(\mathrm{PE}:\mathbb{N}\to\mathbb{R}^{d}\) represent the position encoding function, then

\[\mathrm{PE}_{2s}(t)=\sin\bigg{(}\frac{t}{10000^{\frac{2s}{d}}}\bigg{)},\quad \mathrm{PE}_{2s+1}(t)=\cos\bigg{(}\frac{t}{10000^{\frac{2s}{d}}}\bigg{)},\]

where \(t\) is the discrete time and \(s\) corresponds to the dimension in the embedding \(\mathrm{PE}(t)\). Let \(e_{k}=\mathrm{PE}(u_{k}^{time})\), then the value prediction is conducted by \(\hat{v}_{k}=U_{\xi}(z_{k},e_{k})\), where \(U_{\xi}(z_{k},e_{k})\) is the prediction network with a similar architecture to the U-Net [26] used by the Diffusion Model [17]. The loss of the value prediction model is shown below:

\[\mathcal{L}_{\rm pred}=\frac{1}{N}\sum_{k=1}^{N}\|v_{k}-\hat{v}_{k}\|_{2}^{2},\]

where \(v_{k}\) is the true value of the ad opportunity in the record of \(u_{k}\).

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Section 1. 2. Did you describe the limitations of your work? [Yes] See Section 8. 3. Did you discuss any potential negative societal impacts of your work? [N/A] We believe our benchmark does not involve any potential negative societal impacts. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix B.5. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix A. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section??. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [N/A] 2. Did you mention the license of the assets? [Yes] See Appendix B.5. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See Appendix B.5. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix B.5. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix B.3.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]