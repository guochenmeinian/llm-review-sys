# A Concept-Based Explainability Framework for Large Multimodal Models

Jayneel Parekh\({}^{1}\) Pegah Khayatan\({}^{1}\) Mustafa Shukor\({}^{1}\)

&Alasdair Newson\({}^{1}\) Matthieu Cord\({}^{1,2}\)

\({}^{1}\)ISIR, Sorbonne Universite, Paris, France \({}^{2}\)Valeo.ai, Paris, France

{jayneel.parekh, pegah.khayatan}@sorbonne-universite.fr

###### Abstract

Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as "multi-modal concepts". We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our implementation is publicly available.1

Footnote 1: Project page and code: https://jayneelparekh.github.io/LMM_Concept_Explainability/

## 1 Introduction

Despite the exceptional capacity of deep neural networks (DNNs) to address complex learning problems, one aspect that hinders their deployment is the lack of human-comprehensible understanding of their internal computations. This directly calls into question their reliability and trustworthiness [5, 30]. Consequently, this has boosted research efforts in _interpretability/explainability_ of these models i.e. devising methods to gain human-understandable insights about their decision processes. The growth in ability of DNNs has been accompanied by a similar increase in their design complexity and computational intensiveness. This is epitomized by the rise of vision transformers [11] and large-language models (LLMs) [8, 44] which can deploy up to tens of billions of parameters. The effectiveness of these models for unimodal processing tasks has spurred their use in addressing multimodal tasks. In particular, visual encoders and LLMs are frequently combined to address tasks such as image captioning and VQA [2, 25, 29, 43, 45]. This recent class of models are referred to as large multimodal models (LMMs).

For interpretability research, LMMs have largely remained unexplored. Most prior works on interpreting models that process visual data, focus on convolutional neural network (CNN) based systems and classification as the underlying task. Multimodal tasks and transformer-based architectures have both been relatively less studied. LMMs operate at the intersection of both domains. Thus, despite their rapidly growing popularity, there have been very few prior attempts at understanding representations inside an LMM [34, 35, 41, 42].

This paper aims to bridge some of these differences and study in greater detail the intermediate representations of LMMs. To this end, motivated by the concept activation vector (CAV) based approaches for CNNs [14; 15; 17; 22], we propose a novel dictionary-learning based _Concept eXplainability_ method designed for application to LMMs, titled _CoX-LMM_. Our method is used to learn a concept dictionary to understand the representations of a pretrained LMM for a given word/token of interest (Eg. 'Dog'). For this token, we build a matrix containing the LMM's internal representation of the token. We then linearly decompose this matrix using dictionary learning. The dictionary elements of our decomposition represent our concepts. The most interesting consequence of our method is that the learnt concepts exhibit a semantic structure that can be meaningfully grounded in both visual and textual domains. They are visually grounded by extracting the images which maximally activate these concepts. They can simultaneously be grounded in the textual domain by decoding the concept through the language model of the LMM and extracting the words/tokens they are most associated to. We refer to such concept representations as _multimodal concepts_. Our key contributions can be summarized as follows:

* We propose a novel concept-based explainability framework _CoX-LMM_, that can be used to understand internal representations of large multimodal models. To the best of our knowledge, this is the first effort targeting multimodal models at this scale.
* Our dictionary learning based concept extraction approach is used to extract a multimodal concept dictionary wherein each concept can be semantically grounded simultaneously in both text and vision. We also extend the previous concept dictionary-learning strategies using a Semi-NMF based optimization.
* We experimentally validate the notion of multimodal concepts through both, qualitative visualizations and quantitative evaluation. Our learnt concept dictionary is shown to possess a meaningful multimodal grounding covering diverse concepts, and is useful to locally interpret representations of test samples LMMs.

## 2 Related work

Large Multimodal Models (LMMs)Large language models (LLMs) [8; 21; 33; 44] have emerged as the cornerstone of contemporary multimodal models. Typical large multimodal models (LMMs) [1; 4; 25; 26] comprise three components: LLMs, visual encoders, and light-weight connector modules to glue the two models. Remarkably, recent works have demonstrated that by keeping all pretrained models frozen and training only a few million parameters in the connector (e.g., a linear layer), LLMs can be adapted to understand images, videos, and audios [12; 23; 31; 43; 45], thus paving the way for solving multi-modal tasks. However, there is still a lack of effort aimed at understanding why such frozen LLMs can generalize to multimodal inputs. In this study, we try to decode the internal representation of LLMs when exposed to multimodal inputs.

Concept activation vector based approachesConcept based interpretability aim to extract the semantic content relevant for a model [9]. For post-hoc interpretation of pretrained models, concept activation vector (CAV) based approaches [15; 17; 46; 47; 48; 22; 48] have been most widely used. The idea of CAV was first proposed by Kim et al. [22]. They define a concept as a set of user-specified examples. The concept is represented in the activation space of deep layer of a CNN by a hyperplane that separates these examples from a set of random examples. This direction in the activation space is referred to as the concept activation vector. Built upon CAV, ACE [17] automate the concept extraction process. CRAFT [15] proposed to learn a set of concepts for a class by decomposing activations of image crops via non-negative matrix factorization (NMF). Recently, Fel et al. [14] proposed a unified view of CAV-based approaches as variants of a dictionary learning problem. However, these methods have only been applied for interpretation of CNNs on classification tasks. LMMs on the contrary exhibit a different architecture. We propose a dictionary learning based concept extraction method, designed for LMMs. We also propose a Semi-NMF variant of the dictionary learning problem, which has not been previously considered for concept extraction.

Understanding VLM/LMM representationsThere has been an increasing interest in understanding internal representations of visual-language models (VLM) through the lens of multimodality. Shukor and Cord [42] analyse multimodal tokens and shows that despite being different, visual and perceptual tokens are implicitly aligned inside LLMs. Goh et al. [18] discover neurons termedmultimodal_, that activate for certain conceptual information given images as input. Recently proposed TEXTSPAN [16] and SpLiCE [7], aim to understand representations in CLIP [38] by decomposing its visual representations on textual representations. For LMMs, Palit et al. [34] extend the causal tracing used for LLMs to analyze information across different layers in an LMM. Schwettmann et al. [41] first proposed the notion of _multimodal neurons_ existing within the LLM part of an LMM. They term the neurons "multimodal" as they translate high-level visual information to corresponding information in text modality. The neurons are discovered by ranking them by a gradient based attribution score. Pan et al. [35] proposed a more refined algorithm to identify such neurons based on a different neuron importance measure that leverages architectural information of transformer MLP blocks. Instead, we propose to discover a concept structure in the token representations by learning a small dictionary of multimodally grounded concepts. Limiting the analysis to a specific token of interest allows our method to discover fine details about the token in the learnt concepts.

## 3 Approach

### Background for Large Multimodal Models (LMMs)

Model architecture.We consider a general model architecture for a large multimodal model \(f\), that consists of: a visual encoder \(f_{V}\), a trainable connector \(C\), and an LLM \(f_{LM}\) consisting of \(N_{L}\) layers. We assume \(f\) is pretrained for captioning task with an underlying dataset \(\mathcal{S}=\{(X_{i},y_{i})\}_{i=1}^{N}\) consisting of images \(X_{i}\in\mathcal{X}\) and their associated caption \(y_{i}\subset\mathcal{Y}\). \(\mathcal{X}\) and \(\mathcal{Y}\) denote the space of images and set of text tokens respectively. Note that caption \(y_{i}\) can be viewed as a subset of all tokens. The input to the language model \(f_{LM}\) is denoted by the sequence of tokens \(h^{1},h^{2},...,h^{p}\) and the output as \(\hat{y}\). The internal representation of any token at some layer \(l\) and position \(p\) inside \(f_{LM}\) is denoted as \(h^{p}_{(l)}\), with \(h^{p}_{(0)}=h^{p}\). Note that \(h^{p}_{(l)}\) is same as the residual stream representation in LLM transformers [13] at position \(p\) and layer \(l\). For the multimodal model, the input sequence of tokens for \(f_{LM}\) consists of the concatenation of: (1) \(N_{V}\) visual tokens provided by the visual encoder \(f_{V}\) operating on an image \(X\), followed by the connector \(C\), and (2) linearly embedded textual tokens previously predicted by \(f_{LM}\). For \(p>N_{V}\), this can be expressed as:

\[\hat{y}^{p}=f_{LM}(h^{1},h^{2},\ldots,h^{N_{V}},\ldots,h^{p}),\] (1)

Figure 1: Overview of multimodal concept extraction and grounding in _CoX-LMM_. Given a pretrained LMM for captioning and a target token (for eg. ‘Dog’), our method extracts internal representations of \(f\) about \(t\), across many images. These representations are collated into a matrix \(\mathbf{Z}\). We linearly decompose \(\mathbf{Z}\) to learn a concept dictionary \(\mathbf{U}\) and its coefficients/activations \(\mathbf{V}\). Each concept \(u_{k}\in\mathbf{U}\), is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words \(\mathbf{T}_{k}\) by decoding \(u_{k}\) through the unembedding matrix \(W_{U}\). Visual grounding \(\mathbf{X}_{k,MAS}\) is obtained via \(v_{k}\) as the set of most activating samples.

where \(h^{1},\ldots,h^{N_{V}}=C(f_{V}(X))\), and \(h^{p}=\text{Emb}(\hat{y}^{p-1})\) for \(p>N_{V}\), where Emb denotes the token embedding function. To start the prediction, \(h^{N_{V}+1}\) is defined as the beginning of sentence token. The output token \(\hat{y}^{p}\) is obtained by normalizing \(h^{p}_{(N_{L})}\), followed by an unembedding layer that applies a matrix \(W_{U}\) followed by a softmax. The predicted caption \(\hat{y}\) consists of the predicted tokens \(\hat{y}=\{\hat{y}^{p}\}_{p>N_{V}}\) until the end of sentence token.

TrainingThe model is trained with next token prediction objective, to generate text conditioned on images in an auto-regressive fashion. In this work we focus on models trained to "translate" images into text, or image captioning models. These models keep the visual encoder \(f_{V}\) frozen and only train the connector \(C\). Recent models also finetune the LLM to improve performance. Our approach can be applied in either case, and in our experiments we consider both type of LMMs. However, we find the generalization of LLMs to multimodal inputs is an interesting phenomenon to understand, thus we focus more on the setup where the LLM is kept frozen.

Transformer representations viewCentral to many previous approaches interpreting decoder-only LLM/transformer architectures, is the "residual stream view" of internal representations, first proposed in [13]. Herein, the network is seen as a composition of various computational blocks that "read" information from the residual stream of token representations \(h^{p}_{(l)}\), perform their computation, and add or "write" their output back in the residual stream. This view can be summarized as:.

\[h^{p}_{(l+1)}=h^{p}_{(l)}+a^{p}_{(l)}+m^{p}_{(l)}\] (2)

\(a^{p}_{(l)}\) denotes the information computed by attention function at layer \(l\) and position \(p\). It has a causal structure and computes its output using \(h^{1}_{(l)},...,h^{p}_{(l)}\). \(m^{p}_{(l)}\) denotes the information computed by the MLP block. It is a feedforward network (FFN) with two fully-connected layers and an intermediate activation function \(\sigma\), that operates on \(h^{p}_{(l)}+a^{p}_{(l)}\). The output of \(\sigma(.)\) is referred to as FFN activations.

### Method overview

Fig. 1 provides a visual summary of the whole _CoX-LMM_ pipeline. Given a pretrained LMM \(f\) and a token of interest \(t\in\mathcal{Y}\), our method consists of three key parts:

1. Selecting a subset of images \(\mathbf{X}\) from dataset \(\mathcal{S}\), relevant for target token \(t\). We extract representations by processing samples in \(\mathbf{X}\) through \(f\). The extracted representations of dimension \(B\) are collected in a matrix \(\mathbf{Z}\in\mathbb{R}^{B\times M}\), where \(M\) is number of samples in \(\mathbf{X}\).
2. Linearly decomposing \(\mathbf{Z}\approx\mathbf{U}\mathbf{V}\) into its constituents, that includes a dictionary of learnt concepts \(\mathbf{U}\in\mathbb{R}^{B\times K}\) of size \(K\) and coefficient/activation matrix \(\mathbf{V}\in\mathbb{R}^{K\times M}\).
3. Semantically grounding the learnt "multimodal concepts", contained in dictionary \(\mathbf{U}\) in both visual and textual modalities.

We emphasize at this point that our main objective in employing dictionary learning based concept extraction is to understand internal representations of an LMM. Thus, our focus is on validating the use of the learnt dictionary for this goal, and not to interpret the output of the model, which can be readily accomplished by combining this pipeline with some concept importance estimation method [14]. The rest of the section is devoted to elaborate on each of the above three steps.

### Representation extraction

To extract relevant representations from the LMM about \(t\) that encode meaningful semantic information, we first determine a set of samples \(\mathbf{X}\) from dataset \(\mathcal{S}=\{(X_{i},y_{i})\}_{i=1}^{N}\) for extraction. We consider the set of samples where \(t\) is predicted as part of the predicted caption \(\hat{y}\). This allows us to further investigate the model's internal representations of \(t\). To enhance visual interpretability for the extracted concept dictionary, we additionally limit this set of samples to those that contain \(t\) in the ground-truth caption. Thus, \(\mathbf{X}\) is computed as:

\[\mathbf{X}=\{X_{i}\mid t\in f(X_{i})\text{ and }t\in y_{i}\text{ and }(X_{i},y_{i})\in\mathcal{S}\}.\] (3)

Given any \(X\in\mathbf{X}\), we propose to extract the residual stream representation \(h^{p}_{(L)}\) from a deep layer \(L\), at the first position in the predicted caption \(p>N_{V}\), such that \(\hat{y}^{p}=t\). The representation of each sample \(X_{j}\in\mathbf{X}\) is then stacked as columns of the matrix \(\mathbf{Z}=[z_{1},...,z_{M}]\in\mathbb{R}^{B\times M}\). Note that the representations of text tokens in \(f_{LM}\) can possess a meaningful multimodal structure as they combine information from visual token representations \(h^{p}_{(l)},p\leq N_{V}\). In contrast to \(a^{p}_{(l)}\) and \(m^{p}_{(l)}\), that represent residual information at layer \(l\), \(h^{p}_{(L)}\) contains the aggregated information computed by the LMM till layer \(L\), providing a holistic view of its computation across all previous layers.

### Decomposing the representations

The representation matrix \(\mathbf{Z}\approx\mathbf{UV}\), is decomposed as product of two low-rank matrices \(\mathbf{U}\in\mathbb{R}^{B\times K},\mathbf{V}\in\mathbb{R}^{K\times M}\) of rank \(K<<\min(B,M)\), where \(K\) denotes the number of dictionary elements. The columns of \(\mathbf{U}=[u_{1},...,u_{K}]\) are the basis vectors which we refer to as concept-vectors/concepts. The rows of \(\mathbf{V}\) or columns of \(\mathbf{V}^{T}=[v_{1},...,v_{K}],v_{i}\in\mathbb{R}^{M}\) denote the activations of \(u_{i}\) for each sample. This decomposition, as previously studied in [14], can be optimized with various constraints on \(\mathbf{U},\mathbf{V}\), each leading to a different dictionary. The most common ones include PCA (constraint: \(\mathbf{U}^{T}\mathbf{U}=\mathbf{I}\)), K-Means (constraint: columns of \(\mathbf{V}\) correspond to columns of identity matrix) and NMF (constraint: \(\mathbf{U},\mathbf{V}\geq 0\)). NMF is considered to yield most interpretable results [14]. However, for our use case, NMF is not useful as representation matrix \(\mathbf{Z}\) is not non-negative. Instead, we propose to employ a relaxed version of NMF, Semi-NMF [10], which allows the decomposition matrix \(\mathbf{Z}\) and basis vectors \(\mathbf{U}\) to contain mixed values, and only forces the activations \(\mathbf{V}\) to be non-negative. Note that given its relations to clustering algorithms [10], enforcing non-negative combinations of decompositions is still valued from an interpretability perspective. Since we expect only a small number of concepts to be present in any given sample, we also encourage sparsity in activations \(\mathbf{V}\). The optimization problem to decompose \(\mathbf{Z}\) via Semi-NMF can be summarized as:

\[\mathbf{U}^{*},\mathbf{V}^{*}=\arg\min_{\mathbf{U},\mathbf{V}}\ ||\mathbf{Z}- \mathbf{UV}||_{F}^{2}+\lambda||\mathbf{V}||_{1}\quad s.t.\ \mathbf{V}\geq 0,\ \text{and}\ ||u_{k}||_{2}\leq 1\ \forall k\in\{1,...,K\}.\] (4)

Given any image \(X\) where token \(t\) is predicted by \(f\), we can now define the process of computing activations of concept dictionary \(\mathbf{U}^{*}\) for given \(X\), denoted as \(v(X)\in\mathbb{R}^{K}\). To do so, we first extract the token representation for \(X,z_{X}\in\mathbb{R}^{B}\) with the process described in Sec. 3.3. Then, \(z_{X}\) is projected on \(\mathbf{U}^{*}\) to compute \(v(X)\). In the case of Semi-NMF, this corresponds to \(v(X)=\arg\min_{v\geq 0}||z_{X}-\mathbf{U}^{*}v||_{2}^{2}+\lambda||v||_{1}\). The activation of \(u_{k}\in\mathbf{U}^{*}\) is denoted as \(v_{k}(X)\in\mathbb{R}\).

### Using the concept dictionary for interpretation

**Multimodal grounding of concepts.** Given the learnt dictionary \(\mathbf{U}^{*}\) and corresponding activations \(\mathbf{V}^{*}\), the key objective remaining is to ground the understanding of any given concept vector \(u_{k},k\in\{1,...,K\}\) in the visual and textual domains. Specifically, for visual grounding, we use prototyping [3, 22] to select input images (among the decomposed samples), that maximally activate \(u_{k}\). Given the number of samples extracted for visualization \(N_{MAS}\), the set of maximum activating samples (MAS) for component \(u_{k}\), denoted as \(\mathbf{X}_{k,MAS}\) can be specified as follows (\(|.|\) is absolute value):

\[\mathbf{X}_{k,MAS}=\operatorname*{argmax}_{\hat{X}\subset\mathbf{X},||\hat{X }||=N_{MAS}}\sum_{X\in\hat{X}}|v_{k}(X)|.\] (5)

For grounding in textual domain, we note that the concept vectors are defined in the token representation space of \(f_{LM}\). Thus we leverage the insights from "Lens" based methods [6, 24, 32, 40] that attempt to understand LLM representations. In particular, following [32], we use the unembedding layer to map \(u_{k}\) to the token vocabulary space \(\mathcal{Y}\), and extract the most probable tokens. That is, we extract the tokens with highest probability in \(W_{U}u_{k}\). The decoded tokens with highest probabilities are then filtered for being an english, non-stop-word with at least 3 characters, to eliminate unnecessary tokens. The final set of words is referred to as grounded words for concept \(u_{k}\) and denoted as \(\mathbf{T}_{k}\). Fig. 2 illustrates an example of grounding of a concept extracted for token "Dog" in vision (5 most activating samples) and text (top 5 decoded words).

**Most activating concepts for images.** To understand the LMM's representation of a given image \(X\), we now define the _most activating concepts_. Firstly, we extract the representaions \(z_{X}\) of the image with the same process as described previously. We then project \(z_{X}\) on \(\mathbf{U}^{*}\) to obtain \(v(X)\in\mathbb{R}^{K}\). We define the most activating concepts, which we denote \(\tilde{u}(X)\), as the set of \(r\) concept vectors (in\(\mathbf{U}^{*}\)) whose activations \(v_{k}(X)\) have the largest magnitude. One can then visualize the multimodal grounding of \(\tilde{u}(X)\). This step could be further combined with concept importance estimation techniques [14] to interpret the model's prediction for token \(t\), however, the focus of this paper is to simply understand the internal representation of the model, for which the current pipeline suffices.

## 4 Experiments

Models and dictionary learning.In the main paper, we focus on experiments with the DePALM model [45] that is trained for captioning task on COCO dataset [27]. The model consists of a frozen ViT-L/14 CLIP [39] encoder as the visual encoder \(f_{V}\). It is followed by a transformer connector to compress the encoding into \(N_{V}=10\) visual tokens. The language model \(f_{LM}\) is a frozen OPT-6.7B [49] and consists of 32 layers. Additional experiments with LLaVA [28] are in Appendix A. For uniformity and fairness all the results in the main paper are reported with number of concepts \(K=20\) and for token representations from \(L=31\), the final layer before unembedding layer. For Semi-NMF, we set \(\lambda=1\) throughout. We consider the 5 most activating samples in \(\mathbf{X}_{k,MAS}\) for visual grounding for any \(u_{k}\). For text grounding, we consider top-15 tokens for \(\mathbf{T}_{k}\) before applying the filtering described in Sec 3.5.

The complete dataset consists of around 120,000 images for training, and 5000 each for validation and testing with 5 captions per image following the Karpathy split. We conduct our analysis separately for various common objects in the dataset: "Dog", "Bus", "Train", "Cat", "Bear", "Baby", "Car", "Cake". The extension to other classes/tokens remains straightforward and is discussed in Appendix D. The precise details about number of samples for learning the dictionary, or testing, is available in Appendix C. The implementation of our method is publicly available on GitHub 2

Footnote 2: https://github.com/mshukor/xl-vlms

### Evaluation setup

We evaluate the quality of learnt concept dictionary \(\mathbf{U}^{*}\) on three axes: (i) Its use during inference to interpret representations of LMMs for test samples, (ii) The overlap/entanglement between grounded words of concepts in the dictionary and (iii) the quality of visual and text grounding of concepts (used to understand a concept itself). We discuss concrete details about each axis below:

**Concept extraction during inference:** To evaluate the use of \(\mathbf{U}^{*}\) in understanding any test sample \(X\), we first estimate the top \(r\) most activating concepts activations, \(\tilde{u}(X)\) (Sec. 3.5). We then estimate the correspondence between the test image \(X\) and the grounded words \(\mathbf{T}_{k}\) of \(u_{k}\in\tilde{u}(X)\). This correspondence is estimated via two different metrics. The primary metric is the average CLIPScore [20] between \(X\) and \(\mathbf{T}_{k}\). This directly estimates correspondence between the test image embedding with the grounded words of the top concepts. The secondary metric is the average BERTScore (F1) [50] between the ground-truth captions \(y\) associated with \(X\) and the words \(\mathbf{T}_{k}\). These metrics help validate the multimodal nature of the concept dictionaries. Their use is inspired from [41]. Details for their implementation is in Appendix C.

**Overlap/entanglement of learnt concepts:** Ideally, we would like each concept in \(\mathbf{U}^{*}\) to encode distinct information about the token of interest \(t\). Thus two different concepts \(u_{k},u_{l},k\neq l\) should be associated to different sets of words. To quantify the entanglement of learnt concepts, we compute the overlap between the grounded words \(\mathbf{T}_{k},\mathbf{T}_{l}\). The overlap for a concept \(u_{k}\) is defined as an average of its fraction of common words with other concepts. The overlap/entanglement metric for a dictionary

Figure 2: Example of multimodal concept grounding in vision and text. Five most activating samples (among decomposed in \(\mathbf{Z}\)) and five most probable decoded words are shown.

\(\mathbf{U}^{*}\) is defined as the average of overlap of each concept.

\[\text{Overlap}(\mathbf{U}^{*})=\frac{1}{K}\sum_{k}\text{Overlap}(u_{k}),\quad \text{Overlap}(u_{k})=\frac{1}{(K-1)}\sum_{l=1,l\neq k}^{K}\frac{|\mathbf{T}_{l }\cap\mathbf{T}_{k}|}{|\mathbf{T}_{k}|}\]

**Multimodal grounding of concepts:** To evaluate the quality of visual/text grounding of concepts (\(\mathbf{X}_{k,MAS},\mathbf{T}_{k}\)), we measure the correspondence between visual and text grounding of a given concept \(u_{k}\), i.e. the set of maximum activating samples \(\mathbf{X}_{k,MAS}\) and words \(\mathbf{T}_{k}\), using CLIPScore and BERTScore as described above.

**Baselines:** One set of methods for evaluation are the variants of _CoX-LMM_ where we employ different dictionary learning strategies: PCA, KMeans and Semi-NMF. For evaluating concept extraction on test data with CLIPScore/BERTScore we compare against the following baselines:

- _Rnd-Words_: This baseline considers Semi-NMF as the underlying learning method. However, for each component \(u_{k}\), we replace its grounded words \(\mathbf{T}_{k}\) by a set of random words \(\mathbf{R}_{k}\) such that \(|\mathbf{R}_{k}|=|\mathbf{T}_{k}|\) and the random words also satisfy the same filtering conditions as grounded words i.e. they are non-stopwords from english corpus with more than two characters. We do this by decoding a randomly sampled token representation and adding the top decoded words if they satisfy the conditions.

- _Noise-Imgs_: This baseline uses random noise as images and then proceeds with exactly same learning procedure as Semi-NMF including extracting activations from the same positions, and same parameters for dictionary learning. Combined with the Rnd-Words baseline, they ablate two parts of the concept extraction pipeline.

- _Simple_: This baseline considers a simple technique to build the dictionary \(\mathbf{U}^{*}\) and projecting test samples. It builds \(\mathbf{U}^{*}\) by selecting token representations in \(\mathbf{Z}\) with the largest norm. The projections are performed by mapping the test sample representation to the closest element in \(\mathbf{U}^{*}\). For deeper layers, this provides a strong baseline in terms of extracted grounded words \(\mathbf{T}_{k}\) which are related to token of interest \(t\), as they are obtained by directly decoding token representations of \(t\).

We also report score using ground-truth captions _(GT captions)_ instead of grounded words \(\mathbf{T}_{k}\), to get the best possible correspondence score. The overlap/entanglement in concept dictionary is compared between the non-random baselines: Simple, PCA, K-Means and Semi-NMF. For evaluating the visual/text grounding we compare against the random words keeping the underlying set of MAS, \(\mathbf{X}_{k,MAS}\), same for both.

### Results and discussion

**Quantitative results**  Tab. 1 reports the test top-1 CLIPScore/BERTScore for all baselines and Semi-NMF on different target tokens. Appendix D contains detailed results for other tokens as well as for the PCA and K-Means variants. We report the results for only the top-\(1\) activating concept, as the KMeans and Simple baselines map a given representation to a single cluster/element.

\begin{table}
\begin{tabular}{l l l l l l l} \hline Token & Metric & Rnd-Words & Noise-Imgs & Simple & Semi-NMF (Ours) & GT-captions \\ \hline \multirow{2}{*}{Dog} & CS top-1 (\(\uparrow\)) & 0.519 \(\pm\) 0.05 & 0.425 \(\pm\) 0.06 & 0.546 \(\pm\) 0.08 & **0.610 \(\pm\) 0.09** & 0.783 \(\pm\) 0.06 \\  & BS top-1 (\(\uparrow\)) & 0.201 \(\pm\) 0.04 & 0.306 \(\pm\) 0.05 & 0.346 \(\pm\) 0.08 & **0.405 \(\pm\) 0.07** & 0.511 \(\pm\) 0.11 \\ \hline \multirow{2}{*}{Bus} & CS top-1 (\(\uparrow\)) & 0.507 \(\pm\) 0.05 & 0.425 \(\pm\) 0.08 & **0.667 \(\pm\) 0.06** & 0.634 \(\pm\) 0.08 & 0.736 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.200 \(\pm\) 0.05 & 0.303 \(\pm\) 0.06 & 0.390 \(\pm\) 0.05 & **0.404 \(\pm\) 0.07** & 0.466 \(\pm\) 0.11 \\ \hline \multirow{2}{*}{Train} & CS top-1 (\(\uparrow\)) & 0.496 \(\pm\) 0.05 & 0.410 \(\pm\) 0.07 & 0.642 \(\pm\) 0.06 & **0.646 \(\pm\) 0.07** & 0.727 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.210 \(\pm\) 0.06 & 0.253 \(\pm\) 0.06 & **0.392 \(\pm\) 0.07** & 0.378 \(\pm\) 0.07 & 0.436 \(\pm\) 0.08 \\ \hline \multirow{2}{*}{Cat} & CS top-1 (\(\uparrow\)) & 0.539 \(\pm\) 0.04 & 0.461 \(\pm\) 0.04 & 0.589 \(\pm\) 0.07 & **0.627 \(\pm\) 0.06** & 0.798 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.207 \(\pm\) 0.07 & 0.307 \(\pm\) 0.03 & 0.425 \(\pm\) 0.10 & **0.437 \(\pm\) 0.08** & 0.544 \(\pm\) 0.10 \\ \hline \end{tabular}
\end{table}
Table 1: Test data mean CLIPScore and BERTScore for top-1 activating concept for all baselines on five tokens. CLIPScore denoted as CS, and BERTScore as BS. Statistical significance is in Appendix D. Our _CoX-LMM_ framework is evaluated with Semi-NMF as underlying dictionary learning method. Higher scores are better. Best score in **bold**, second best is underlined.

Notably, Semi-NMF generally outperforms the other baselines although the Simple baseline performs competitively. More generally, Semi-NMF, K-Means, and Simple tend to clearly outperform Rnd-Words, Noise-Imgs and PCA on these metrics, indicating that these systems project representations of test images to concepts whose associated grounded words correspond well with the visual content.

Tab. 2 reports the overlap between concepts for Simple baseline and PCA, K-Means and Semi-NMF variants of _CoX-LMM_. Interestingly, K-Means and Simple baseline perform significantly worse than Semi-NMF/PCA with a high overlap between grounded words, often exceeding 40%. PCA outperforms other methods with almost no overlap while Semi-NMF shows some overlap. Overall, Semi-NMF strikes the best balance among all the methods, in terms of learning a concept dictionary useful for understanding test image representations, but which also learns diverse and disentangled concepts. Thus, for further _CoX-LMM_ experiments, we consider Semi-NMF as the underlying dictionary learning method.

Fig. 3 shows an evaluation of visual/text grounding of concepts learnt by Semi-NMF. Each point on the figure denotes the CLIP-Score (left) or BERTScore (right) for correspondence between samples \(\mathbf{X}_{k,MAS}\) and words \(\mathbf{T}_{k}\) for concept \(u_{k}\) against random words baseline. We see that for both metrics, vast majority of concepts lie above the \(y=x\) line, indicating that grounded words correspond much better to content of maximum activating samples than random words.

**Qualitative results** Fig. 4 shows visual and textual grounding of concepts extracted for token 'dog'. For brevity, we select 8 out of 20 concepts for illustration. 2. Grounding for all concepts extracted for 'dog' and other tokens are in Appendix E. The concept visualizations/grounding for 'Dog' reveal interesting insights about the global structure of the LMM's representation. Extracted concepts capture information about different aspects of a 'dog'. The LMM separates representation

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Token & Simple & PCA & KMeans & Semi-NMF \\ \hline Dog & 0.371 & **0.004** & 0.501 & 0.086 \\ \hline Bus & 0.622 & **0.002** & 0.487 & 0.177 \\ \hline Train & 0.619 & **0.015** & 0.367 & 0.107 \\ \hline Cat & 0.452 & **0.000** & 0.500 & 0.146 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overlap between learnt concepts. Lower is better. Best score in **bold**, second best underlined.

Figure 4: Visual/textual grounding for 8 out of 20 concepts for ’Dog’ token (layer 31). For each concept we illustrate the set of 5 most activating samples and 5 most probable decoded words.

Figure 3: Evaluating visual/text grounding (CLIP-Score/BERTScore). Each point denotes score for grounded words of a concept (Semi-NMF) vs Rnd-Words w.r.t the same visual grounding.

of animal 'Dog' with a 'hot dog' (Concept 1). Specifically for 'Dog', Concepts (2), (3) capture information about color: 'black', 'brown'. Concept (6) encodes information about 'long hair' of a dog, while concept (5) activates for'small/puppy-like' dogs. Beyond concepts activating for specific characteristics of a 'dog', we also discover concepts describing their state of actions (Concept (4) 'playing/running'), common scenes they can occur in (Concept (8), 'herd'), and correlated objects they can occur with (Concept (7), 'cat and dog'). We observe such diverse nature of extracted concepts even for other tokens (Appendix E). The information about concepts can be inferred via both the visual images and the associated grounded words, highlighting their coherent multimodal grounding. Notably, compared to solely visual grounding as for CAVs for CNNs, the multimodal grounding eases the process to understand a concept.

Fig. 5 illustrates the use of concept dictionaries (learnt via Semi-NMF) to understand test sample representations for tokens 'Dog', 'Cat' and 'Bus'. For each sample we show the normalized activations of the three most activating concepts, and their respective multimodal grounding. Most activating concepts often capture meaningful and diverse features of a given sample. For instance, for first sample containing a 'Dog', the concepts for "long hair", "small/tiny/puppy", and "black/white color" all simultaneously activate. The grounding for first two concepts was also illustrated in Fig. 4. Additional visualizations for test samples are shown in Appendix E, wherein we qualitatively compare interpretations of Semi-NMF to K-Means, PCA variants and Simple baseline.

**Layer ablation** We analyze the quality of multimodal grounding of concepts across different layers \(L\). The CLIPScore between \((\mathbf{X}_{k,MAS},\mathbf{T}_{k})\), averaged over all concepts \(u_{k}\) is shown in Fig. 6, for 'Dog' and 'Cat' for all layers in \(f_{LM}\). For early layers the multimodal grounding is no better than Rnd-Works. Interestingly, there is a noticeable increase around (\(L=20\) to \(L=25\)), indicating that the multimodal structure of internal token representations starts to appear at this point. This also validates our choice that deeper layers are better suited for multimodal concepts.

**Additional experiments and discussion.** We conduct a preliminary study to analyze the polysemanticity/superposition in concept dictionaries in Appendix B. A qualitative analysis for grounding of extracted concepts for different layers is available in Appendix F. _CoX-LMM_ can be also be applied to understand the processing of visual/perceptual tokens inside the LMM which also exhibit this multimodal structure. The experiment for the same can be found in Appendix G. Limitations of our method are discussed in Appendix H, and the broader societal impacts are discussed in Appendix I.

Figure 5: Local interpretations for test samples for different tokens (‘Dog’, ‘Cat’, ‘Bus’) with Semi-NMF (layer 31). Visual/text grounding for three highest concept activations (normalized) is shown.

Figure 6: Mean CLIPScore between visual/text grounding \(\mathbf{X}_{k,MAS},\mathbf{T}_{k}\) for all concepts (Semi-NMF), across different layers \(L\). Results are for tokens ‘Dog’ and ‘Cat’.

Conclusion

In summary, we have presented a novel dictionary learning based concept extraction framework, useful to understand internal representations of a large multimodal model. The approach relies on decomposing representations of a token inside a pretrained LMM. To this end, we also propose a Semi-NMF variant of the concept dictionary learning problem. The elements of the learnt concept dictionary are grounded in the both text and visual domains, leading to a novel notion of _multimodal concepts_ in the context of interpretability. We quantitatively and qualitatively show that (i) the multimodal grounding of concepts is meaningful, and (ii) the learnt concepts are useful to understand representations of test samples. We hope that our method inspires future work from research community towards designing concept based explainability methods to understand LMMs.

## Acknowledgments and Disclosure of Funding

We would like to thank Thomas Fel for his valuable comments on the paper. This work has been partially supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), and HPC resources of IDRIS under the file number AD011014947, allocated by GENCI.

## References

* [1]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS)35, pp. 23716-23736. Cited by: SS1.
* [2]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
* [3]D. Alvarez-Melis and T. Jaakkola (2018) Towards robust interpretability with self-explaining neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7775-7784. Cited by: SS1.
* [4]A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, S. Sagawa, et al. (2023) OpenFlamingo: an open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390. Cited by: SS1.
* [5]V. Beaudouin, I. Bloch, D. Bounie, S. Clemencon, F. d'Alche-Buc, J. Eagan, W. Maxwell, P. Mozharovski, and J. Parekh (2020) Flexible and context-specific AI explainability: a multidisciplinary approach. CoRRabs/2003.07703. Cited by: SS1.
* [6]N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt (2023) Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112. Cited by: SS1.
* [7]U. Bhalla, A. Oesterling, S. Srinivas, F. P. Calmon, and H. Lakkaraju (2024) Interpreting clip with sparse linear concept embeddings (splice). arXiv preprint arXiv:2402.10376. Cited by: SS1.
* [8]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS)33, pp. 1877-1901. Cited by: SS1.
* [9]J. Colin, T. Fel, R. Cadene, and T. Serre (2022) What i cannot predict, i do not understand: a human-centered evaluation framework for explainability methods. Advances in neural information processing systems35, pp. 2832-2845. Cited by: SS1.
** [10] Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. _IEEE transactions on pattern analysis and machine intelligence_, 32(1):45-55, 2008.
* [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [12] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma-multimodal augmentation of generative models through adapter-based finetuning. _arXiv preprint arXiv:2112.05253_, 2021.
* [13] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1, 2021.
* [14] Thomas Fel, Victor Boutin, Louis Bethune, Remi Cadene, Mazda Moayeri, Leo Andeol, Mathieu Chalvidal, and Thomas Serre. A holistic approach to unifying automatic concept extraction and concept importance estimation. _Advances in Neural Information Processing Systems_, 36, 2023.
* [15] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, Remi Cadene, and Thomas Serre. Craft: Concept recursive activation factorization for explainability. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2711-2721, 2023.
* [16] Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting clip's image representation via text-based decomposition. In _International Conference on Learning Representations_, 2024.
* [17] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 9277-9286, 2019.
* [18] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. _Distill_, 6(3):e30, 2021.
* [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.
* [20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021.
* [21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [22] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pages 2668-2677. PMLR, 2018.
* [23] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. _arXiv preprint arXiv:2301.13823_, 2023.
* [24] Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem H. Zuidema, and Jaap Jumelet. Decoderlens: Layerwise interpretation of encoder-decoder transformers. _CoRR_, abs/2310.03686, 2023.

* Laurencon et al. [2023] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL https://openreview.net/forum?id=SKN2hf1BIZ.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision (ECCV)_, pages 740-755. Springer, 2014.
* Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2023.
* Manas et al. [2022] Oscar Manas, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal. Mapl: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting. _arXiv preprint arXiv:2210.07179_, 2022.
* Markus et al. [2021] Aniek F Markus, Jan A Kors, and Peter Rijnbeek. The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies. _Journal of biomedical informatics_, 113:103655, 2021.
* Merullo et al. [2022] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. _arXiv preprint arXiv:2209.15162_, 2022.
* [32] Nostalgebraist. Interpreting gpt: The logit lens. https://www.lesswrong.com/posts/AcKRB8WpDdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. Accessed: [date of access].
* [33] OpenAI. Gpt-4 technical report. _arXiv_, 2023.
* Palit et al. [2023] Vedant Palit, Rohan Pandey, Aryaman Arora, and Paul Pu Liang. Towards vision-language mechanistic interpretability: A causal tracing tool for blip. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2856-2861, 2023.
* Pan et al. [2023] Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang. Finding and editing multi-modal neurons in pre-trained transformer. _arXiv preprint arXiv:2311.07470_, 2023.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32:8026-8037, 2019.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. _the Journal of machine Learning research_, 12:2825-2830, 2011.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, pages 8748-8763. PMLR, 2021.
* Sakarvadia et al. [2023] Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, Andre Bauer, Kyle Chard, and Ian T. Foster. Attention lens: A tool for mechanistically interpreting the attention head information retrieval mechanism. _CoRR_, abs/2310.16270, 2023.

* [41] Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in pretrained text-only transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2862-2867, 2023.
* [42] Mustafa Shukor and Matthieu Cord. Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs. _arXiv preprint arXiv:2405.16700_, 2024.
* [43] Mustafa Shukor, Corentin Dancette, and Matthieu Cord. ep-alm: Efficient perceptual augmentation of language models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 22056-22069, October 2023.
* [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [45] Theophane Vallaeys, Mustafa Shukor, Matthieu Cord, and Jakob Verbeek. Improved baselines for data-efficient perceptual augmentation of llms. _arXiv preprint arXiv:2403.13499_, 2024.
* [46] Johanna Vielhaben, Stefan Bluecher, and Nils Strothoff. Multi-dimensional concept discovery (mcd): A unifying framework with completeness guarantees. _Transactions on Machine Learning Research_, 2023.
* [47] Chih-Kuan Yeh, Been Kim, Sercan O Arik, Chun-Liang Li, Pradeep Ravikumar, and Tomas Pfister. On concept-based explanations in deep neural networks. _arXiv preprint arXiv:1910.07969_, 2019.
* [48] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A Ehinger, and Benjamin IP Rubinstein. Invertible concept-based explanations for cnn models with non-negative concept activation vectors. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11682-11690, 2021.
* [49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* [50] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_, 2019.

Experiments on other LMMs

This section covers our experiments on other types of multimodal models. First, we test our approach on LLaVA to demonstrate that our approach generalizes to more recent networks that also fine-tune \(f_{LM}\) on multimodal data. We also test our method on other variants of DePALM with non-CLIP visual encoders to observe their effect on CLIPScore.

### Experiments with LLaVA

We conduct further experiments on LLaVA [28], a popular open-source LMM to demonstrate the generality of our method. The model uses a CLIP-ViT-L-336px visual encoder (\(f_{V}\)), a 2-layer linear connector (\(C\)) that outputs \(N_{V}=576\) visual tokens, and a Vicuna-7B language model (\(f_{LM}\), 32 layers). We use identical hyperparameters as for DePALM (\(K=20,\lambda=1,L=31\)). We report the test CLIPScore for top-1 activating concept, for Rnd-Words, Noise-Imgs, Simple and Semi-NMF, GT-captions in Tab. 3, and Overlap score for non-random baselines in Tab. 4. Quantitatively, we obtain consistent results to those observed for DePALM. Semi-NMF extracts most balanced concept dictionaries with high multimodal correspondence and low overlap. Qualitatively too, the method functions consistently and is able to extract concepts with meaningful multimodal grounding.

Qualitative results and Saliency mapsWe also show qualitative examples of concepts extracted for token 'Dog' in Fig. 7. More examples for other 'Cat' and 'Train' tokens are given in Fig. 9 and 10. Interestingly, since LLaVA uses a connector \(C\) that contains two linear layers, the visual tokens as processed inside \(f_{LM}\) preserve the notion of specific image patch representations, i.e. \(N_{V}=576\) tokens denoting representations for 576 (24 \(\times\) 24) input patches. This allows us to further explore a simple and computationally cheap strategy of generating saliency maps to highlight which regions a concept vector activates on. To do this one can simply compute the inner product of any given concept vector \(u_{k}\) with all visual token representations from corresponding layer \(L\), i.e. \(u_{k}^{T}[h_{(L)}^{1},...,h_{(L)}^{N_{V}}]\). This can be upscaled to the input image size to visualize the saliency map. We illustrate some qualitative outputs on concepts from 'Dog' in Fig. 8..

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Token & Simple & PCA & KMeans & Semi-NMF \\ \hline Dog & 0.435 & **0.008** & 0.429 & 0.149 \\ \hline Bus & 0.464 & **0.010** & 0.518 & 0.124 \\ \hline Train & 0.315 & **0.024** & 0.382 & 0.087 \\ \hline Cat & 0.479 & **0.013** & 0.554 & 0.166 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Overlap evaluation (LLaVA). Lower is better. Best score in **bold**, second best underlined.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Token & Metric & Rnd-Words & Noise-Imgs & Simple & Semi-NMF (Ours) & GT-captions \\ \hline Dog & CS top-1 (\(\uparrow\)) & 0.537 \(\pm\) 0.03 & 0.530 \(\pm\) 0.05 & 0.567 \(\pm\) 0.08 & **0.595 \(\pm\) 0.07** & 0.777 \(\pm\) 0.06 \\  & BS top-1 (\(\uparrow\)) & 0.205 \(\pm\) 0.07 & 0.227 \(\pm\) 0.06 & **0.331 \(\pm\) 0.07** & 0.305 \(\pm\) 0.07 & 0.519 \(\pm\) 0.11 \\ \hline Bus & CS top-1 (\(\uparrow\)) & 0.509 \(\pm\) 0.04 & 0.487 \(\pm\) 0.05 & **0.619 \(\pm\) 0.06** & 0.591 \(\pm\) 0.08 & 0.742 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.198 \(\pm\) 0.07 & 0.253 \(\pm\) 0.06 & **0.319 \(\pm\) 0.04** & 0.306 \(\pm\) 0.06 & 0.460 \(\pm\) 0.10 \\ \hline Train & CS top-1 (\(\uparrow\)) & 0.518 \(\pm\) 0.03 & 0.505 \(\pm\) 0.04 & 0.633 \(\pm\) 0.05 & **0.640 \(\pm\) 0.07** & 0.725 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.177 \(\pm\) 0.07 & 0.221 \(\pm\) 0.04 & **0.310 \(\pm\) 0.05** & 0.293 \(\pm\) 0.05 & 0.432 \(\pm\) 0.08 \\ \hline Cat & CS top-1 (\(\uparrow\)) & 0.536 \(\pm\) 0.03 & 0.545 \(\pm\) 0.04 & **0.625 \(\pm\) 0.06** & 0.621 \(\pm\) 0.07 & 0.795 \(\pm\) 0.05 \\  & BS top-1 (\(\uparrow\)) & 0.142 \(\pm\) 0.06 & 0.235 \(\pm\) 0.05 & 0.306 \(\pm\) 0.06 & **0.329 \(\pm\) 0.07** & 0.540 \(\pm\) 0.11 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Concept extraction on LLaVA-v1.5: Test data mean CLIPScore reported for top-1 activating concept for same baselines and tokens as in main paper table 1. Higher scores are better. Best score in **bold**, second best is underlined.

### DePALM with ViT visual encoders

We further test LMMs which do not contain a CLIP visual encoder to confirm that the high CLIPScore is not due to use of CLIP visual encoders. To test this, we experiment on two different DePALM models with frozen visual encoders different from CLIP, a frozen ViT-L encoder trained on ImageNet [11] and another frozen ViT-L trained as a masked autoencoder (MAE) [19]. Both LMMs use the same pretrained OPT-6.7B language model. Collectively, the three encoders (including CLIP) are pretrained for three different types of objectives. We use Semi-NMF to extract concept dictionaries, with all hyperparameters identical. The results are reported in tables below. 'Rnd-Words' and 'GT-captions' references are reported for each LMM separately, although they are very close to the ones in main paper. The "ViT-L (CLIP)" baseline denotes our system from the main paper that uses CLIP encoder. Importantly, we still obtain similar test CLIPScores as with CLIP visual encoder. The concept dictionaries still possess meaningful multimodal grounding. Many concepts also tend to be similar as for CLIP visual encoder, further indicating that processing inside language model plays a major role in the discovery of multimodally grounded concepts.

Figure 8: Examples of generating visual concept saliency maps for two ’Dog’ concepts for LLaVA. Red denotes high activations, blue denotes low activation (bottom row)

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Token & Rnd-Words & ViT-L (ImageNet) & ViT-L (CLIP) & GT-captions \\ \hline Dog & 0.514 \(\pm\) 0.05 & 0.611 \(\pm\) 0.09 & 0.610 \(\pm\) 0.09 & 0.783 \(\pm\) 0.06 \\ \hline Bus & 0.498 \(\pm\) 0.05 & 0.644 \(\pm\) 0.07 & 0.634 \(\pm\) 0.08 & 0.739 \(\pm\) 0.05 \\ \hline Train & 0.494 \(\pm\) 0.05 & 0.617 \(\pm\) 0.07 & 0.646 \(\pm\) 0.07 & 0.728 \(\pm\) 0.05 \\ \hline Cat & 0.539 \(\pm\) 0.05 & 0.628 \(\pm\) 0.07 & 0.627 \(\pm\) 0.06 & 0.794 \(\pm\) 0.06 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Test CLIPScore evaluation for DePALM with ViT-L frozen image encoder trained on ImageNet: Scores reported for top-1 activating concept of Semi-NMF for Rnd-Words, GT-captions and ViT-L (CLIP) which denotes the system in main text.

Figure 7: Multimodal grounding for example concepts for ’Dog’ token (layer 31) on LLaVA.

## Appendix B Analyzing polysemanticity in the learnt concepts

We conducted a preliminary qualitative study on some concept vectors in the dictionary learnt for token "Dog" (DePALM model), to analyze if these concept vectors tend to activate strongly for a specific semantic concept (monosemantic) or multiple semantic concepts (polysemantic). In particular, we first manually annotated the 160 test samples for "Dog" for four semantic concepts, for which we knew we had concept vectors in our dictionary, namely "Hot dog" (Concept 2, row 1, column 2 in Fig. 7), "Black dog" (Concept 20, row 10, column 2 in Fig. 7), "Brown/orange dog" (Concept 6, row 3, column 2 in Fig. 7), and "Bull dog" (Concept 15, row 8, column 1 in Fig. 7). For a given semantic concept, we call this set \(C_{true}\). Then, for its corresponding concept vector \(u_{k}\) we

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Token & Rnd-Words & ViT-L (MAE) & ViT-L (CLIP) & GT-captions \\ \hline Dog & 0.515 \(\pm\) 0.05 & 0.602 \(\pm\) 0.07 & 0.610 \(\pm\) 0.09 & 0.784 \(\pm\) 0.06 \\ \hline Bus & 0.501 \(\pm\) 0.05 & 0.627 \(\pm\) 0.07 & 0.634 \(\pm\) 0.08 & 0.737 \(\pm\) 0.05 \\ \hline Train & 0.483 \(\pm\) 0.06 & 0.618 \(\pm\) 0.08 & 0.646 \(\pm\) 0.07 & 0.726 \(\pm\) 0.05 \\ \hline Cat & 0.541 \(\pm\) 0.04 & 0.629 \(\pm\) 0.09 & 0.627 \(\pm\) 0.06 & 0.795 \(\pm\) 0.06 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test CLIPScore evaluation for DePALM with ViT-L frozen image encoder trained as masked autoencoder (MAE): Scores reported for top-1 activating concept of Semi-NMF for Rnd-Words, GT-captions and ViT-L (CLIP) which denotes the system in main text.

Figure 9: Multimodal grounding for example concepts for ‘Cat’ token (layer 31) on LLaVA.

find the set of test samples for which \(u_{k}\) activates greater than a threshold \(\tau\). This threshold was set to half of its maximum activation over test samples. We call this set of samples \(C_{top}\). To estimate specificity of the concept vector we compute how many samples in \(C_{top}\) lie in the ground-truth set, i.e. \(|C_{top}|\cap|C_{true}|/|C_{top}|\).

We found Concept 2 ("Hot dog") to be most monosemantic with 100% specificity. For Concept 20 ("Black dog") too, we found high specificity of 93.3%. For concept 15 ("Bull dog") we observed the lowest specificity of 50%. This concept also activated for test images with toy/stuffed dogs. Interestingly, the multimodal grounding of concept 15 already indicates this superposition with maximum activating samples also containing images of 'toy dogs'. Concept 6 ("Brown/orange dog") is somewhere in between, with 76% specificity. This concept vector also activated sometimes for dark colored dogs, which wasn't apparent from its multimodal grounding.

Prominent or distinct semantic concepts seem to be captured by more specific/monosemantic concept vectors, while more sparsely present concepts seem at risk to be superposed resulting in a more polysemantic concept vector capturing them. It is also worth noting that the multimodal grounding can be a useful tool in some cases to identify polysemanticity in advance.

Figure 10: Multimodal grounding for example concepts for ’Train’ token (layer 31) on LLaVA.

## Appendix C Further implementation details

### Dictionary learning details

The details about the number of samples used for training the concept dictionary of each token, and the number of samples for testing is given in Tab. 7. The token representations are of dimension \(B=4096\).

The hyperparameters for the dictionary learning methods are already discussed in the main paper. All the dictionary learning methods (PCA, KMeans, Semi-NMF) are implemented using scikit-learn [37]. For PCA and KMeans we rely on the default optimization strategies. Semi-NMF is implemented through the DictionaryLearning\(()\) class, by forcing a positive code. It utilizes the coordinate descent algorithm for optimization during both the learning of \(\mathbf{U}^{*}\), \(\mathbf{V}^{*}\) and the projection of test representations \(v(X)\).

### CLIPScore/BERTScore evaluation

For a given image \(X\) and set of words \(\mathbf{T}_{k}\) associated to concept \(u_{k}\), CLIPScore is calculated between CLIP-image embedding of \(X\) and CLIP-text embedding of comma-separated words in \(\mathbf{T}_{k}\). We consider a maximum of 10 most probable words in each \(\mathbf{T}_{k}\), filtering out non-English and stop words. The computation of the metric from embeddings adheres to the standard procedure described in [20]. Our adapted implementation is based on the CLIPScore official repository, which utilizes the ViT-B/32 CLIP model to generate embeddings.

We found that computing BERTScores from comma-separated words and captions is unreliable. Instead, we adopted a method using the LLaMA-3-8B instruct model to construct coherent phrases from a set of grounded words, \(\mathbf{T}_{k}\). Specifically, we provide the LLaMA model with instructions to describe a scene using a designated set of words, for which we also supply potential answers. This instruction is similarly applied to another set of words, but without providing answers. The responses generated by LLaMA are then compared to the captions \(y\) using BERTScore. The instruction phrase and an example of the output are detailed in 8. The highest matching score between the generated phrases and the captions of a test sample determines the score assigned to the concept \(u_{k}\). This approach ensures that the evaluation accurately reflects coherent and contextually integrated language use. The metric calculation from embeddings follows the established guidelines outlined in [50]. Our adapted implementation is based on BERTScore official repository, and we use the default Roberta-large model to generate embeddings.

### Resources

DePALM experiments computeEach experiment to analyze a token with a selected dictionary learning method is conduced on a single RTX5000 (24GB)/ RTX6000 (48GB)/ TITAN-RTX (24GB) GPU. Within dictionary learning, generating visualizations and projecting test data, the majority of time is spent in loading the data/models and extracting the representations. For analysis of a single token with \(\approx\) 3000 training samples, it takes around 10-15 mins for this complete process. Evaluation for CLIPScore/BERTScore are also conducted using the same resources. Evaluating CLIPScore for 500 (image, grounded-words) pairs takes around 5 mins. The BERTScore evaluation is in contrast more expensive, consuming around 150 mins for 500 pairs.

LLaVA experiments computeEach experiment to extract a concept dictionary for LLaVA was conducted on a single A100 (80GB) GPU. Representation extraction process for LLaVA is more

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Split & Dog & Bus & Train & Cat & Baby & Car & Cake & Bear \\ \hline Train & 3693 & 2382 & 3317 & 3277 & 837 & 1329 & 1733 & 1529 \\ \hline Test & 161 & 91 & 147 & 167 & 44 & 79 & 86 & 55 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Number of samples training/testing samples for each token for DePALMexpensive compared to DePALM consuming around 90 mins for \(\approx\) 3000 samples. The remaining aspects of dictionary learning, multimodal grounding, representation projection etc. remains relatively cheap. The CLIPScore/BERTScore evaluations are completed with same resources as before.

Licenses of assetsThe part of the code for representation extraction from LMM is implemented using PyTorch [36]. For our analyses, we also employ the OPT-6.7B model [49] from Meta AI, released under the MIT license, and the CLIP model [39] from OpenAI, available under a custom usage license. Additionally, the COCO dataset [27] used for validation is accessible under the Creative Commons Attribution 4.0 License. We also use CLIPScore [20] and BERTScore [50] for evaluating our method, both publicly released under MIT license. All utilized resources comply with their respective licenses, ensuring ethical usage and reproducibility of our findings.

### Choice for number of concepts \(K\)

Our choice of using \(K=20\) concepts for all tokens was driven by the behaviour of reconstruction error of Semi-NMF on the training samples with different values of \(K\), i.e. \(||\mathbf{Z}-\mathbf{U}\mathbf{V}||_{2}^{2}\). We validate this behaviour on the four target tokens from main text in Fig. 11. We generally found \(K=20\) as the minimal number of concepts where the reconstruction error drops by at least 50% from \(K=0\).

We also conducted an ablation study to test how our method behaves with different values of number of concepts \(K\). Fig. 12 presents the variation of test CLIPScore and Overlap score for \(K\in\{10,20,30,50\}\) for two target tokens, 'Dog' and 'Cat'. Our method can learn meaningful concepts for different values of \(K\), evident by the consistently high CLIPScore. The Overlap score on the other hand tends to drop more consistently as the number of concepts increase but behaves stably for different choices. Nonetheless they indicate that our method can accommodate dictionaries of larger sizes without compromising the quality of learnt concepts, provided \(K<<M\) (number of decomposed samples) and at the expense of greater user overhead.

## Appendix D Evaluation and extension to more tokens

We provide test data mean CLIPScore and BERTScore for top-1 activating concept for all baselines and more tokens: Baby, Car, Cake, and Bear in Tab. 9 (results in the main paper are reported for tokens Dog, Bus, Train, Cat in Tab. 1). Additionally, we also report the macro average over a set of 30 additional COCO-nouns apart from the 8 tokens with individually reported results, denoted

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt}} \hline \hline
**Instruction to LLaMA** & **Nature of Response** \\ \hline Generate three distinct phrases, each incorporating the words dog, white, people. Ensure each phrase is clear and contextually meaningful. Number each phrase as follows: 1. 2. 3. & 1.A white dog is standing next to people. 2. People are standing next to a white dog. 3. The dog is standing next to people wearing white clothes. \\ \hline Generate three distinct phrases, each incorporating the words List of words. Ensure each phrase is clear and contextually meaningful. Number each phrase as follows: 1. 2. 3. & LLaMA autonomously creates a relevant description, demonstrating comprehension and creative integration of the new words. \\ \hline Generate three distinct phrases, each incorporating the words brown, black, large, fluffy, cat. Ensure each phrase is clear and contextually meaningful. Number each phrase as follows: 1. 2. 3. & 1. A large, fluffy black cat is sleeping on the brown couch. 2. The brown cat is curled up next to a large, fluffy black cat. 3. The large, fluffy cat’s brown fur stands out against the black background. \\ \hline \hline \end{tabular}
\end{table}
Table 8: Generating contextually and grammatically coherent phrases Using the LLaMA Model for BERTScore Evaluationas 'Extra-30'. These nouns are single-token words with at least 40 predicted test samples. We put the filter of single-token words to keep consistency with the presented framework. Extension to multi-token words is straightforward but discussed separately in D.1. The lower bound criterion on test samples is to ensure average test CLIPScore is reliable for each target token. We only report CLIPScore for 'Extra-30' tokens as BERTScore evaluation was more expensive to conduct on large number of dictionaries.

We observe that we consistently obtain higher scores across for Semi-NMF and K-Means. We also report the overlap score between grounded words in Tab. 10 to illustrate the superiority of our method over the simple baseline. As previously noted, we observe a high overlap between grounded

Figure 11: Variation of reconstruction error with number of concepts \(K\) for decompositions on different target tokens.

Figure 12: Test CLIPScore and Overlap score ablation with number of concepts \(K\). CLIPScore remains consistently high and drops slightly only for very small \(K\). Overlap score generally improves with higher \(K\).

[MISSING_PAGE_FAIL:21]

better than other systems. Within these three systems the significance depends on the target token, but are often not significant in many cases.

### Extending to multi-token words

The presentation of our approach assumes that our token of interest \(t\) is a single token. This poses no issues for words which are represented as single tokens but can raise some questions when we wish to extract concept representations for multi-token words. Our approach however, can be easily adapted to this setting. In particular, we extract representation of last token from first prediction of the multi-token sequence. Note that when filtering the training data for samples where ground-truth caption contains the token of interest, we now search for the complete multi-token sequence. The other aspects of the method remain unchanged. While there can also be other viable strategies, the rationale behind this adaptation is that the last token of our sequence of interest can also combines representations from previous tokens in the sequence. We add below results for such examples in Tab. 12 and 13. We observe behaviour consistent with the previous results with Semi-NMF extracting concept dictionaries with high CLIPScore and low overlap.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multirow{2}{*}{Token} & \multirow{2}{*}{Metric} & \multirow{2}{*}{Rad-Words} & \multirow{2}{*}{Noise-Imgs} & \multirow{2}{*}{Simple} & \multirow{2}{*}{PCA} & \multirow{2}{*}{KMeans} & \multirow{2}{*}{GT-captions} \\ \hline \multirow{2}{*}{Dog} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\ \hline \multirow{2}{*}{Bes} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **0.002** & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & \textgreater{}0.1 & \textgreater{}0.1 & \textless{} **0.001** \\ \hline \multirow{2}{*}{Train} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & 0.08 & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\ \hline \multirow{2}{*}{Cat} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & **0.015** & **\textless{} 0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & **0.001** & **\textless{} 0.001** & **\textless{} 0.001** \\ \hline \multirow{2}{*}{Baby} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & **0.002** & \textgreater{}0.1 & \textless{} **0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **0.001** & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\ \hline \multirow{2}{*}{Cat} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & **\textless{} 0.001** & \textgreater{}0.1 & \textless{} **0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & **\textless{} 0.001** & \textless{} 0.001** \\ \hline \multirow{2}{*}{Cake} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textgreater{}0.1 & **\textless{} 0.001** & **0.005** & **\textless{} 0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.005** & **0.003** & **\textless{} 0.001** & \textgreater{}0.1 & **\textless{} 0.001** \\ \hline \multirow{2}{*}{Bear} & CS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & \textless{} 0.001** & **\textless{} 0.001** & 0.072 & **\textless{} 0.001** \\  & BS top-1 & **\textless{} 0.001** & **\textless{} 0.001** & **0.001** & **0.0001** & \textgreater{}0.1 & **0.015** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Statistical significance of Semi-NMF w.r.t other baselines for test data CLIPScore/BERTScore. \(p\)-values for two sided T-test are reported. Significant values (\(p\)-value \(<0.05\)) are indicated in bold. The values do **not** indicate which system has better mean score but just that if the difference is significant.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Multi-token word & Rnd-Words & Noise-Imgs & Simple & Semi-NMF & GT-captions \\ \hline Traffic light & 0.516 \(\pm\) 0.03 & 0.525 \(\pm\) 0.03 & **0.664**\(\pm\) 0.06 & 0.634 \(\pm\) 0.05 & 0.744 \(\pm\) 0.04 \\ \hline Cell phone & 0.542 \(\pm\) 0.04 & 0.547 \(\pm\) 0.03 & 0.598 \(\pm\) 0.04 & **0.598**\(\pm\) 0.05 & 0.765 \(\pm\) 0.06 \\ \hline Stop sign & 0.533 \(\pm\) 0.03 & 0.549 \(\pm\) 0.03 & **0.617**\(\pm\) 0.08 & 0.616 \(\pm\) 0.05 & 0.775 \(\pm\) 0.04 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Test mean CLIPScore (\(\uparrow\)) reported for top-1 activating concept for multi-token words. Higher scores are better. Best score in **bold**, second best is underlined.

[MISSING_PAGE_FAIL:23]

stored in the visual/perceptual tokens representations as processed in \(f_{LM}\), \(h^{p}_{(l)},p\leq N_{V}\). An interesting aspect worth highlighting is that while the text token representations in \(f_{LM}\) can combine information from the visual token representations (via attention function), the reverse is not true. The causal processing structure of \(f_{LM}\) prevents the visual token representations to attend to any information in the text token representations. Given a token of interest \(t\), for any sample \(X\in\mathbf{X}_{t}\) we now only search for first position \(p\in\{1,...,N_{V}\}\), s.t. \(t=\arg\max\text{Unembed}(h^{p}_{(N_{L})})\). Only the samples for which such a \(p\) exists are considered for decomposition. The rest of the method to learn \(\mathbf{U}^{*},\mathbf{V}^{*}\) proceeds exactly as before.

We conduct a small experiment to qualitatively analyze concepts extracted for visual token representations for 'Dog'. We extract \(K=20\) concepts from \(L=31\). The dictionary is learnt with representations from \(M=1752\) samples, less than \(M=3693\) samples for textual tokens. As a brief illustration, 12 out of 20 extracted concepts are shown in Fig. 23. Interestingly, even the visual token representations in deep layers of \(f_{LM}\), without ever attending to any text tokens, demonstrate a multimodal semantic structure. It is also worth noting that there are multiple similar concepts that appear for both visual and textual tokens. Concepts 3, 7, 10, 12, 17, 19 are all similar visually and textually to certain concepts discovered for text tokens. This indicates to a possibility that

Figure 13: multimodal concept grounding in vision and text for the token ’Dog’. The five most activating samples and the five most probable decoded words for each component \(u_{k}\), \(k\in\{1,...,20\}\) are shown. The token representations are extracted from L=31 of the LLM section of our LMM.

these concepts are discovered by \(f_{LM}\) in processing of the visual tokens and this information gets propagated to predicted text token representations.

## Appendix H Limitations

We list below some limitations of our proposed method:

* The concept dictionaries extracted currently are token-specific. It can be interesting to explore learning concept dictionaries that can encode shared concepts for different tokens.
* The current study is conducted mainly on visual captioning models. While we expect the key ideas to generalize to many other types of large multimodal models, this application of our approach to other types of LMMs remains to be explored/confirmed.
* We select the most simple and straightforward concept grounding techniques. Both visual and textual grounding could potentially be enhanced. The visual grounding can be improved by enhancing localization of concept activation for any MAS or test sample. Text grounding could be enhanced by employing more sophisticated approaches such as tuned lens [6].

Figure 14: multimodal concept grounding in vision and text for the token ’Cat’. The five most activating samples and the five most probable decoded words for each component \(u_{k}\), \(k\in\{1,...,20\}\) are shown. The token representations are extracted from L=31 of the LLM section of our LMM.

* While the proposed CLIPScore/BERTScore metrics are useful to validate this aspect, they are not perfect metrics and affected by imperfections and limitations of the underlying models extracting the image/text embeddings. The current research for metrics useful for interpretability remains an interesting open question, even more so in the context of LLMs/LMMs.

## Appendix I Broader societal impact

The popularity of large multimodal models and the applications they are being employed is growing at an extremely rapid pace. The current understanding of these models and their representations is limited, given the limited number of prior methods developed to understand LMMs. Since interpretability is generally regarded as an important trait for machine learning/AI models deployed in real world, we expect our method to have a positive overall impact. This includes its usage for understanding LMMs, as well as encouraging further research in this domain.

Figure 15: multimodal concept grounding in vision and text for the token ’Bus’. The five most activating samples and the five most probable decoded words for each component \(u_{k}\), \(k\in\{1,...,20\}\) are shown. The token representations are extracted from L=31 of the LLM section of our LMM.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a novel dictionary learning based concept extraction framework for understanding Large Multimodal Model (LMM) representations (Sec. 3). We extend previous dictionary learning-based concept extraction methods by proposing a Semi-NMF based optimization (Sec. 3). The learnt concept dictionary has multimodal semantic structure which we qualitatively and quantitatively validate with our experiments (Sec. 4). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.

Figure 16: multimodal concept grounding in vision and text for the token ’Train’. The five most activating samples and the five most probable decoded words for each component \(u_{k}\), \(k\in\{1,...,20\}\) are shown. The token representations are extracted from L=31 of the LLM section of our LMM.

* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed separately in Appendix H Guidelines:

Figure 17: Local interpretations for test sample 9 of token ‘Dog’ with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. SemiNMF baseline provides the most visually and textually consistent results, while other baselines provide components that are not well disentangled (Simple and KMeans baseline), or the text grounding is not closely related to the test image.

Figure 18: Local interpretations for test sample 37 of token ‘Dog’ with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown.

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution

Figure 19: Local interpretations for test sample 43 of token ‘Cat’ with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown.

Figure 20: Local interpretations for test sample 6 of token ‘Bus’ with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown.

is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover

Figure 21: Local interpretations for test sample 12 of token ‘Bus’ with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown.

Figure 22: Examples of multimodal grounding across different layers for concepts with similar visual grounding (target token ‘Dog’). The grounded words from early layers do not correspond well to the most activating samples of a concept.

limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not posit any theoretical results. The paper primarily presents empirical evidence demonstrating the effectiveness of the method rather than focusing on theoretical results. our approach highlights the practical applicability and robust performance of the method through a series of experiments. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]

Figure 23: Example concepts extracted for ‘Dog’ from visual token representations in layer 31.

Justification: We have disclosed complete information about our pretrained model, extraction of representations, parameters, and implementation to train the dictionary, evaluation setup, and resource usage for the experiments. Details are in Sec. 4 and Appendix C. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is available at https://github.com/mshukor/xl-vlms Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Major details are in Sec. 4 and all additional details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show standard deviation of the key metrics in Tab. 1 and Tab. 9 and discuss the statistical significance comparing baselines and our method in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details are in Appendix C.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed them and adhere to them. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The societal impacts are discussed separately in Appendix I. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: We work with models and data already publicly available. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Details available in Appendix C. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We publicly release our code and documentation is available alongside it. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.