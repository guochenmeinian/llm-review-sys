# Transition Path Sampling with

Boltzmann Generator-based MCMC Moves

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Sampling all possible transition paths between two 3D states of a molecular system has various applications ranging from catalyst design to drug discovery. Current approaches to sample transition paths use Markov chain Monte Carlo and rely on time-intensive molecular dynamics simulations to find new paths. Our approach operates in the latent space of a normalizing flow that maps from the molecule's Boltzmann distribution to a Gaussian, where we propose new paths without requiring molecular simulations. Using alanine dipeptide, we explore Metropolis-Hastings acceptance criteria in the latent space for exact sampling and investigate different latent proposal mechanisms.

## 1 Introduction

Sampling the trajectories in which a molecular system changes from one 3D configuration to another--a task known as transition path sampling (TPS)--has many applications, such as designing catalysts (Crehuet and Field, 2007), materials (Selli et al., 2016), or drug discovery (Kirmizaltin et al., 2012, 2015). In fact, the transition path ensemble is the ideal description of a chemical reaction's mechanism. We explore how this problem can be solved using a Boltzmann generator (a normalizing flow trained to sample a molecule's Boltzmann distribution) (Noe et al., 2019) and its latent space to obtain or approximate the ensemble.

In the TPS problem, we are given a single molecular system and two 3D conformations of interest for it: states A and B, as seen in Figure 1. These could be the structure of reactants before a reaction and the structure of the product molecule after the reaction. With this, we aim to sample the transition paths between them with the likelihood at which they occur. To describe a transition path, we use a sequence of time-equidistant 3D atom configurations (i.e., frames) that starts in state A and ends in state B.

Existing approaches for this problem (Dellago et al., 1998, 2002; Bolhuis et al., 2002) use Markov chain Monte Carlo (MCMC) sampling to iteratively sample a new path given the current one. New paths are commonly proposed using shooting moves that require molecular dynamics simulation. Given a path, the proposal is generated by first randomly selecting a frame of the path and sampling a random velocity from a Gaussian. The selected frame with the new velocity is then simulated forward and backward in time. If the backward simulation reaches state A and the forward simulation ends in state B, this trajectory constitutes a new non-zero probability transition path, which is accepted or

Figure 1: Distribution of alanine dipeptide’s 3D configurations visualized via a histogram of its main dihedral angles \(\phi,\psi\). Two metastable states are highlighted, between which we aim to sample the ensemble of all possible transition paths.

rejected based on its probability and a Metropolis-Hastings (Metropolis et al., 1953; Hastings, 1970) acceptance criterion. All paths that do not transition between A and B will be rejected. Repeating this is guaranteed to eventually produce the exact transition path ensemble, but convergence is slow since many proposals will not fulfill the constraints, paths are correlated, and finding transitions requires expensive simulation.

In this work, we explore how the TPS problem can be addressed when having access to a trained Boltzmann generator, which solves the easier problem of sampling the molecular systems distribution of 3D conformers. Given this, we generate MCMC proposals by first moving every frame in a path into our latent space. We modify each frame of this path by adding independent Gaussian noise such that the overall likelihood can easily be evaluated. Then, we use the Boltzmann generator to bring the whole path back to configuration space, compute the probability of the path, and use it to accept or reject the proposed path. This procedure is depicted in Figure 2.

Our contributions are investigating this novel method for transition path sampling and highlighting its challenges. To that end, we describe the difficulty of calculating likelihoods for paths that were not generated with molecular dynamics and the obstacles for calculating path probabilities in parallel. Additionally, we provide insights into what configuration space paths are produced from simple paths in the latent space of a Boltzmann generator.

## 2 Background and Related Work

**Boltzmann generators.** Given a molecule, the probability of each 3D configuration is proportional to the exponential of its negative energy, i.e., they follow a Boltzmann distribution. Noe et al. (2019) train a normalizing flow (Tabak and Vanden-Eijnden, 2010; Tabak and Turner, 2013) to sample a molecule's Boltzmann distribution, known as Boltzmann generator. While recent innovations (Midgley et al., 2023; 2023) improved their training efficiency, training them for larger systems remains an open problem and a limitation of our Boltzmann generator-based approach.

**Deep learning for transition path sampling.** The TPS problem, with the goal to sample the whole transition path ensemble, is more challenging than finding a single low-energy transition path: A problem that also has been explored with deep learning (DL) approaches (Liu et al., 2022; Holdijk et al., 2023). For the harder TPS problem, DL methods require MCMC with shooting moves as proposed by Dellago et al. (1998, 1998). For instance, Falkner et al. (2023) replace the shooting point selection with DL and sample them with a Boltzmann generator. Similarly, Jung et al. (2023) increase the acceptance rate of shooting moves by selecting the frames to shoot from with a learned function. These approaches still require sequential MD simulation. In this work, we explore a novel molecular dynamics-free MCMC paradigm using DL.

## 3 Method

We assume access to a Boltzmann generator for the molecule of interest and two of its states, A and B, between which we wish to sample the transition path ensemble. In the following, we lay out the overall MCMC framework over latent space paths (see SS 3.1). This requires two components:

Figure 2: **MCMC proposals for latent space transition paths.** We move a transition path \(x\) into latent space using a Boltzmann generator \(F(\cdot)\). With this path \(\bm{z}\) and our latent space proposal kernel \(q_{Z}(\tilde{\bm{z}}|\bm{z})\) we propose \(\tilde{\bm{z}}\) and bring it back to configuration/atom space to obtain the transition path proposal \(\bm{x}\). The likelihood of all steps can be computed, and we use them in a Metropolis-Hastings acceptance criterion to sample the transition path ensemble with MCMC.

Calculating the path probability (see SS 3.2), and a proposal kernel for a path in latent space for which we lay out several options (see SS 3.3).

### MCMC Framework for Latent Paths

Let \(\bm{x}\) be our current path with frames \(\bm{x}_{i}\in\mathbb{R}^{n\times 3}\) and \(i\in\{1,...,l\}\), where \(n\) is the number of atoms of our molecule and \(l\) the number of frames which we keep constant (the spacing of the frames can change with changing path lengths). For our MCMC procedure, we further need a proposal kernel \(q(\tilde{\bm{x}}\mid\bm{x})\) that produces a new path proposal \(\tilde{\bm{x}}\) from our current path \(\bm{x}^{\dagger}\). If we can additionally compute the probability of a path \(p_{AB}\), we can sample the transition path ensemble with the MCMC algorithm using Metropolis-Hastings acceptance criterion

\[\alpha=\min\left\{1,\frac{p_{AB}\left(\tilde{\bm{x}}\right)}{p_{AB}(\bm{x})} \cdot\frac{q\left(\bm{x}\mid\tilde{\bm{x}}\right)}{q\left(\tilde{\bm{x}}\mid \bm{x}\right)}\right\}.\] (1)

In our work, the proposal consists of first using a Boltzmann generator \(F\) trained on the molecule to move the path \(\bm{x}\) into latent space to obtain the latent path \(\bm{z}=\left\{F^{-1}\left(\bm{x}_{1}\right),\ldots,F^{-1}\left(\bm{x}_{l} \right)\right\}\). Subsequently, we make a proposal in latent space to obtain a new latent path \(\tilde{\bm{z}}\) using the latent proposal kernel \(q_{z}(\tilde{\bm{z}}\mid\bm{z})\) which we design in SS 3.3. Lastly, the latent path is projected back to configuration space using the Boltzmann generator \(\bm{x}=\left\{F\left(\tilde{\bm{z}}_{1}\right),\ldots,F\left(\tilde{\bm{z}}_ {l}\right)\right\}\).

The proposal kernel thus takes the form \(q(\tilde{\bm{x}}\mid\bm{x})=p(\bm{z}|\bm{x})q_{z}(\tilde{\bm{z}}\mid\bm{z})p( \tilde{\bm{x}}|\bm{z})\), where \(p(\bm{z}|\bm{x})\) accounts for the change of density when using our Boltzmann generator to move the path \(\bm{x}\) into latent space and \(p(\tilde{\bm{x}}|\tilde{\bm{z}})\) arises from moving the new latent path back to configuration space. Since the Boltzmann generator processes all the frames independently, the change of density factors can be written as the product of the individual frames \(p(\bm{z}|\bm{x})=\prod_{i=1}^{l}p(\bm{z}_{i}|\bm{x}_{i})\). With this in mind, the ratio of the forward path proposal \(q(\tilde{\bm{x}}\mid\bm{x})\) and the backward proposal \(q(\bm{x}\mid\tilde{\bm{x}})\), as it is required in the acceptance criterion in Equation 1, takes the form

\[\frac{q(\bm{x}\mid\tilde{\bm{x}})}{q(\tilde{\bm{x}}\mid\bm{x})}=\frac{q_{Z}( \bm{z}\mid\tilde{\bm{z}})}{q_{Z}(\tilde{\bm{z}}\mid\bm{z})}\cdot\prod_{i=1}^{l }\frac{p(\tilde{\bm{z}}_{i}|\tilde{\bm{x}}_{i})p(\bm{x}_{i}|\bm{z}_{i})}{p( \bm{z}_{i}|\bm{x}_{i})p(\tilde{\bm{x}}_{i}|\tilde{\bm{z}}_{i})}.\] (2)

Each term in the product can be simplified as follows, where we write \(\bm{x},\bm{z}\) for an individual frame \(\bm{x}_{i},\bm{z}_{i}\) and use the change of variables formula \(p(\bm{x})=p(\bm{z})\cdot(\det J(F(\bm{z})))^{-1}\) in the third equality

\[\frac{p(\tilde{\bm{z}}|\tilde{\bm{x}})p(\bm{x}|\bm{z})}{p(\bm{x}|\bm{x})p( \tilde{\bm{x}}|\tilde{\bm{z}})}=\frac{\frac{p(\tilde{\bm{x}},\tilde{\bm{z}})} {p(\tilde{\bm{x}})}\frac{p(\bm{x},\bm{z})}{p(\bm{z})}}{p(\bm{x})}=\frac{p(\bm{ x})p(\tilde{\bm{z}})}{p(\tilde{\bm{x}})p(\bm{z})}=\frac{p(\bm{z})(\det J(F(\bm{z})))^{-1}p( \tilde{\bm{z}})}{p(\tilde{\bm{z}})(\det J(F(\tilde{\bm{z}})))^{-1}p(\bm{z})}= \frac{\det J(F(\tilde{\bm{z}}))}{\det J(F(\bm{z}))}.\] (3)

Thus, the ratio of proposals we need to calculate is

\[\frac{q(\bm{x}\mid\tilde{\bm{x}})}{q(\tilde{\bm{x}}\mid\bm{x})}=\frac{q_{Z}( \bm{z}\mid\tilde{\bm{z}})}{q_{Z}(\tilde{\bm{z}}\mid\bm{z})}\cdot\prod_{j=1}^{l }\frac{\det J(F\left(\tilde{\bm{z}}_{j}\right))}{\det J(F(\bm{z}_{j}^{(i-1)} ))},\] (4)

which we can readily use to calculate the acceptance ratio for the MCMC algorithm as laid out in Algorithm 1. The remaining challenges are the ability to compute the path probability \(p_{AB}\) and a concrete latent space proposal kernel \(q_{Z}(\tilde{\bm{z}}\mid\bm{z})\), which we will tackle next.

### Calculating the Path Probability

A path's probability is defined with respect to a molecular dynamics model. Here, we assume Langevin dynamics2 under which the transition from frame \(\bm{x}_{i}\) to the next frame \(\bm{x}_{i+1}\) can be calculated as

Footnote 1: An initial path can be obtained from, for example, a high-temperature MD simulation or by linearly interpolating in the Boltzmann generator’s latent space.

Footnote 2: For the sake of brevity, we omit the constant atom masses and the friction coefficient.

\[\begin{split}\mathbf{x}_{i+1}=\mathbf{x}_{i}+\Delta t\mathbf{v}_{i +1}\\ \mathbf{v}_{i+1}=\alpha\mathbf{v}_{i}+(1-\alpha)\nabla U(\bm{x}_{ i})+\sqrt{k_{B}T(1-\alpha^{2})}\bm{W},\end{split}\] (5)given the velocity \(\bm{v}_{i}\), and the molecule's energy function \(U:\mathbb{R}^{d\times 3}\mapsto\mathbb{R}\). \(\alpha=\exp(-\Delta t)\) for a time step size \(\Delta t\)3 and \(\bm{W}\sim\mathcal{N}(0,1)\) corresponds to random motion that is scaled proportional to the Boltzmann constant \(k_{B}\) and temperature \(T\). Notice that in Langevin dynamics, the only randomness when obtaining \(\bm{x}_{i+1}\) from \(\bm{x}_{i}\) given the velocity \(\bm{v}_{i}\) stems from the Gaussian variable \(\sqrt{k_{B}T(1-\alpha^{2})}\bm{W}\). Thus, the probability density \(p(\bm{x}_{i+1},\bm{v}_{i+1}|\bm{x}_{i},\bm{v}_{i})\) of moving from \(\bm{x}_{i}\) to \(\bm{x}_{i+1}\) is that of a Gaussian with mean \(\mu=\bm{x}_{i}+\Delta t(\alpha\bm{v}_{i}+(1-\alpha)\nabla U(\bm{x}_{i}))\) and standard deviation \(\sigma=k_{B}T(1-\alpha^{2})\).

Footnote 3: Similar to classical fixed length transition path sampling, the timestep size \(\Delta t\) is not trivial to choose. We discuss this further in Appendix B.

Given this probability \(p(\bm{x}_{i+1},\bm{v}_{i+1}|\bm{x}_{i},\bm{v}_{i})\) of moving between individual frames with the auxiliary velocity variable, the probability of a whole path in configuration space is

\[p_{AB}\left(\bm{x}\right)=p\left(\bm{x}_{1}\right)\cdot\prod_{i=1}^{l-1}p(\bm {x}_{i+1},\bm{v}_{i+1}|\bm{x}_{i},\bm{v}_{i}),\] (6)

where \(p(\bm{x}_{i})\) follows the molecule's Boltzmann distribution, meaning that \(p(\bm{x}_{i})\propto exp(-U(\bm{x}_{i})/k_{B}T)\) with an unknown proportionality constant. However, this constant is unnecessary since it will cancel out with the same constant of the reverse path density \(p_{BA}\) in the acceptance ratio in Equation 1.

Thus, the last missing link to computing \(p_{AB}(\bm{x})\) is the initial velocity \(\bm{v}_{1}\). Since our path definition does not include an initial velocity (because we do not have a Boltzmann generator that operates over both velocities and positions), we opt to marginalize over all possible velocities and approximate the following expectation as our final path probability

\[p_{AB}(\bm{x})=\mathbb{E}_{\bm{v}_{1}\sim\mathcal{N}(\bm{0},\text{diag}(k_{B }T))}\left[p\left(\bm{x}_{1}\right)\cdot\prod_{i=1}^{l-1}p(\bm{x}_{i+1},\bm{v} _{i+1}|\bm{x}_{i},\bm{v}_{i})\right].\] (7)

All subsequent velocities \(\{\bm{v}_{i}\}_{i\in\{2,\ldots,l\}}\) can then be inferred by solving the previous step, allowing us to compute \(p(\bm{x}_{i+1},\bm{v}_{i+1}|\bm{x}_{i},\bm{v}_{i})\) sequentially.

**Desirable properties.** In designing our MCMC procedure, we set out to avoid the time-consuming sequential molecular dynamics simulation. While the path probability can be computed easily for paths generated by MD (Jung et al., 2017), calculating the path probability \(p_{AB}(\bm{x})\) still requires sequential computation in our approach. However, this amounts to sequentially performing \(l\) vector additions, which is very cheap and can be done in parallel for all different initial velocities when approximating the expectation. The expensive, time-consuming computations stem from the evaluation of the energy function \(U(x_{i})\) for each frame. In our procedure, this can be done in parallel, while in molecular dynamics, it has to be performed sequentially.

### Latent Space Path Proposal Kernel

As for the concrete latent space path proposal kernel \(q_{Z}(\tilde{\bm{z}}\mid\bm{z})\), we propose three different options: 1) Gaussian noise added to each frame. 2) A Gaussian Process (GP) with the current path as its mean. 3) A GP that is adaptively fit to the history of all sampled transition paths and only weakly depends on the current path. All these proposals are symmetric and will not contribute to our acceptance ratio with \(q_{Z}(\tilde{\bm{z}}\mid\bm{z})/q_{Z}(\bm{z}\mid\tilde{\bm{z}})=1\).

**Gaussian proposal.** From a latent path \(\bm{z}\), we propose a new path \(\tilde{\bm{z}}=\{\bm{z}_{1}+\epsilon_{1},\ldots,\bm{z}_{l}+\epsilon_{l}\}\) where \(\epsilon_{1},\ldots,\epsilon_{l}\sim\mathcal{N}(\bm{0},\bm{\Sigma})\). While this independent noise for each frame makes it unlikely that all frames move coherently and produce high-probability paths, this operation can be performed efficiently and allows for fast exploration of the latent space.

**Conditional Gaussian process path proposals.** We employ a GP \(f(t)\sim\mathcal{GP}(m(t),k(t,t^{\prime}))\), where \(f:\mathbb{R}\mapsto\mathbb{R}^{3n-6}\) maps the time \(t\in[1,l]\) along the path to a Gaussian from which a frame at time \(t\) is sampled4. We fit the GP mean \(m(\cdot)\) and kernel function \(k(\cdot,\cdot)\), which is not to be confused with the proposal kernel, to a set of \(s\) latent paths \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s\}}\), where the index of each frame is used as the time \(t\). In the following, we first detail the set of latent paths before explaining how the GP is used to propose a new path.

Our set of latent paths \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s\}}\) to fit the GP is either the history of all previously sampled paths or we obtain it via linear interpolation in latent space. Specifically, to obtain an interpolation, we sample a start \(\bm{x}_{1}\) and an end frame \(\bm{x}_{l}\) from states A and B, move them to latent space to generate \(\bm{z}_{1},\bm{z}_{l}=F^{-1}(\bm{x}_{1}),F^{-1}(\bm{x}_{l})\), and produce the latent path as the linear interpolation \(\bm{z}_{i}=\frac{i}{l}\bm{z}_{1}+(1-\frac{i}{l})\bm{z}_{l}\) for \(i\in\{1,\ldots,l\}\). After moving it back to configuration space with the Boltzmann generator, this constitutes a coarse path. This produces a fixed proposal kernel, where the quality depends on the paths it was trained on.

When using the history of all previously sampled paths as \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s\}}\), the proposal kernel \(\mathcal{GP}_{s}\) changes over the course of MCMC steps \(s\), leading to an adaptive MCMC algorithm. For this to be correct, the proposal kernel has to converge and satisfy vanishing adaptation (Andrieu and Thoms, 2008) where, as the Markov chain progresses, the influence of its most recent states on the proposal kernel has to diminish. Intuitively, this is the case for our adaptive kernel since the influence of the most recent path on the fitted mean and covariance kernel vanishes as the size of the history (the Markov chain) increases.

We re-fit this adaptive GP proposal to the history of latent paths \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s\}}\) at each step \(s\) when a new path has been accepted. To efficiently do so, we start optimization from the parameters of the previous GP proposal kernel that are optimal for \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s-1\}}\). The new optimization's convergence is typically fast since the minimum under the new set of latent paths at step \(s\) is likely close to that at step \(s-1\), with the difference diminishing as the length of the Markov chain increases.

Given the fitted GP, a new latent path \(\tilde{\bm{z}}\) is proposed conditioned on the current one \(\bm{z}\) by sampling \(\mathcal{GP}_{s}\) at times \(t=1,\ldots,l\) (which correspond to the frame numbers of the paths) after setting the means of \(\mathcal{GP}_{s}\) at those times to the frames of \(\bm{z}\), meaning that \(m(t)=\bm{z}_{t}\) for \(t\in\{1,\ldots l\}\). This amounts to sampling \(\mathcal{GP}_{s}\) unconditionally at \(t=1,\ldots,l\), subtracting the means \(m(t)\), and adding the frames \(\bm{z}_{t}\) at each time.

**Unconditional Gaussian process path proposals.** Here, we use the adaptive Gaussian process \(\mathcal{GP}_{s}\) and propose new paths \(\tilde{\bm{z}}\) unconditionally, meaning that each proposal is a sample of \(\mathcal{GP}_{s}\) and the only influence of \(\bm{z}\) is through its presence in the set of paths \(\{\bm{z}^{i}\}_{i\in\{1,\ldots s\}}\) that \(\mathcal{GP}_{s}\) was fit on. This means that with a progressively increasing number of accepted paths, the influence of the current path will diminish. This would fit a Gaussian process that could be used to sample transition paths without any latent space, which is an interesting aspect on its own.

Further, since we will rely on the mean of the Gaussian process, we can also estimate it between the frames. This allows us to introduce more variance by evaluating the Gaussian process not at the fixed points \(1,\ldots,l\), but to uniformly draw \(l\) sorted samples from \(\mathcal{U}_{[0,5,l+0.5]}\). With this, the individual frames of the path can shift more easily towards and from each other.

## 4 Experiments

**Latent space analysis.** When moving configurations from the meta-stable states \(C_{5}\) and \(\alpha_{R}\) of alanine dipeptide (ALDP) into the latent space, we can linearly interpolate between them and map them back with the trained Boltzmann generator. For this, we train a Boltzmann generator by minimizing the forward KL-divergence loss (compare Appendix A.3). Figure 3 shows that linear paths in latent space produce non-linear paths in configuration space. While linearly interpolating atom positions of a molecule produces unrealistic paths, this naive latent space approach recovers two different modes of transitions between the meta-stable states.

**Ground truth ensemble.** We simulated 10 nanoseconds with a timestep of 1 femtosecond at 300K with the openMM MD engine (Eastman et al., 2017). From this data, we can determine for each conformation whether it belongs to a meta-stable state, allowing us to find paths by looking for sequences that start in A and transition to B (or vice versa). This approach finds variable-length transition paths. We rely on the two-way shooting scheme implemented by OPS (Swenson et al., 2018a,b) with the same MD setup to sample a fixed-length transition path ensemble. Transitions that only rarely occur (Figure 4 bottom) are particularly difficult to produce with classical MD, already for the small molecule alanine dipeptide.

**Results.** Figure 4 shows for all methods a histogram of the sampled transition paths between the states \(C_{5}\leftrightarrow\alpha_{R}\) and \(\alpha_{R}\leftrightarrow C_{7}\), respectively. _Unconditional GP Uni_ refers to the adaptive GP proposal with uniform timepoint sampling while _Unconditional GP_ always samples the index of the frame as timepoint. _Conditional GP_ uses the adaptive proposal.

The main finding is that due to the low acceptance rate of our MCMC steps, we are only able to produce a low amount of paths or a set of paths with low diversity. When increasing the variance, paths will be more diverse but are also less likely to be accepted. To overcome this, proposals that produce more physically likely paths are required.

Some proposal strategies, such as the Gaussian proposal, are computationally efficient, while fitting a high-dimensional Gaussian process is time-consuming. With an increasing number of paths, the proposals are more likely to be stuck in a local minimum. For the more computationally expensive proposals, this makes it challenging to produce the high number of transitions needed to overcome this threshold.

While training a fixed Gaussian process on simple paths in latent space is computationally favorable, the results do not indicate that it can capture the transition paths. Since the iterative Gaussian processes do not seem to fit the distribution either, our choice of kernel or formulation might be inappropriate. In general, we have seen in our experiments that the selection for a kernel of the Gaussian process (compare Appendix A.2) poses a difficult problem for this task because it must capture an adequate amount of noise without overfitting to the previous paths.

Overall, the results qualitatively show that the simplest proposal kernel, one that simply adds Gaussian noise in latent space, appears to be the most efficient and effective choice. Further, conditioning the Gaussian process on the current path appears to slightly increase the variance and leads to a more diverse set of paths.

Figure 4: **Comparison of sampling methods**. Each row shows the transitions between two different metastable states. _Left:_ “Ground truth” path ensemble from MD simulation of all paths (sub-left) and the 25% of paths with the highest probability (sub-right). _Right:_ Shooting move MCMC ensemble and the ensembles of our different latent space proposal kernels. Note that it is unclear what a meaningful ground truth ensemble is.

Figure 3: **Linear latent space interpolation.**_Left:_ A histogram of the two main dihedral angels \(\phi,\psi\) as they occur in the MD simulation. The meta-stable states \(C_{5}\) and \(\alpha_{R}\), and a linear interpolation in latent space (red line) are shown. _Right_: The resulting density of transition paths when linearly interpolating between those states in latent space.

Discussion and Conclusion

**Limitations** Our approach relies on a trained Boltzmann generator, of which high-quality ones for larger molecular systems do not exist yet. Furthermore, the latent space path proposal kernels we devise have too low acceptance rates to be useful. This limits them to a low-variance, slowing down mode-mixing. Better latent space proposals would be necessary. Lastly, an avenue toward a practical solution could be adaptively fine-tuning the Boltzmann generator to make simple paths in latent space correspond to physical paths that obey Langevin dynamics in configuration space.

**Conclusion** In this paper, we presented a novel way to propose transition paths in the latent space of a Boltzmann generator. Throughout this work, we have introduced multiple latent space path proposal kernels that perform (learned) operations. This enables a transition path sampling MCMC procedure without the need for molecular dynamics simulation. We believe that learned transition path sampling methods and, in general, simulation-free MCMC approaches are interesting research questions to explore and might lead to faster sampling methods.

## References

* Andrieu and Thoms (2008-Dec 2008) Christophe Andrieu and Johannes Thoms. A tutorial on adaptive mcmc. _Statistics and Computing_, 18(4):343-373, Dec 2008.
* Bolhuis et al. (2002-October 2002) Peter G. Bolhuis, David Chandler, Christoph Dellago, and Phillip L. Geissler. Transition path sampling: Throwing ropes over rough mountain passes, in the dark. _Annual Review of Physical Chemistry_, 53(1):291-318, October 2002.
* Crehuet and Field (2007-May 2007) Ramon Crehuet and Martin J. Field. A transition path sampling study of the reaction catalyzed by the enzyme chorismate mutase. _The Journal of Physical Chemistry B_, 111(20):5708-5718, May 2007.
* Dellago et al. (1998a) Christoph Dellago, Peter G. Bolhuis, and David Chandler. Efficient transition path sampling: Application to lennard-jones cluster rearrangements. _The Journal of Chemical Physics_, 108(22):9236-9245, June 1998a.
* Dellago et al. (1998b) Christoph Dellago, Peter G. Bolhuis, Felix S. Csajka, and David Chandler. Transition path sampling and the calculation of rate constants. _The Journal of Chemical Physics_, 108(5):1964-1977, 02 1998b.
* Dinh et al. (2017) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _International Conference on Learning Representations_, 2017.
* Durkan et al. (2019) Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Eastman et al. (2017) Peter Eastman, Jason Swails, John D. Chodera, Robert T. McGibbon, Yutong Zhao, Kyle A. Beauchamp, Lee-Ping Wang, Andrew C. Simonnett, Matthew P. Harrigan, Chaya D. Stern, Rafal P. Wiewiora, Bernard R. Brooks, and Vijay S. Pande. OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. _PLOS Computational Biology_, 13(7):e1005659, July 2017.
* Falkner et al. (2023) Sebastian Falkner, Alessandro Coretti, Salvatore Romano, Phillip Geissler, and Christoph Dellago. Conditioning normalizing flows for rare event sampling, 2023.
* Hastings (1970) W. K. Hastings. Monte carlo sampling methods using markov chains and their applications. _Biometrika_, 57(1):97-109, April 1970.
* Holdijk et al. (2023) Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, and Max Welling. Stochastic optimal control for collective variable free sampling of molecular transition paths, 2023.
* Jung et al. (2017) Hendrik Jung, Kei ichi Okazaki, and Gerhard Hummer. Transition path sampling of rare events by shooting from the top. _The Journal of Chemical Physics_, 147(15), August 2017.
* Jung et al. (2023) Hendrik Jung, Roberto Covino, A. Arjun, Christian Leitold, Christoph Dellago, Peter G. Bolhuis, and Gerhard Hummer. Machine-guided path sampling to discover mechanisms of molecular self-organization. _Nature Computational Science_, 3(4):334-345, April 2023.

Serdal Kirmizialtin, Virginia Nguyen, Kenneth A. Johnson, and Ron Elber. How conformational dynamics of DNA polymerase select correct substrates: Experiments and simulations. _Structure_, 20(4):618-627, April 2012.
* Kirmizialtin et al. [2015] Serdal Kirmizialtin, Kenneth A. Johnson, and Ron Elber. Enzyme selectivity of HIV reverse transcriptase: Conformations, ligands, and free energy partition. _The Journal of Physical Chemistry B_, 119(35):11513-11526, August 2015.
* Liu et al. [2022] Tianyi Liu, Weihao Gao, Zhirui Wang, and Chong Wang. Pathflow: A normalizing flow generator that finds transition paths. In James Cussens and Kun Zhang, editors, _Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence_, volume 180 of _Proceedings of Machine Learning Research_, pages 1232-1242. PMLR, 01-05 Aug 2022.
* Metropolis et al. [1953] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of state calculations by fast computing machines. _The Journal of Chemical Physics_, 21(6):1087-1092, June 1953.
* Midgley et al. [2023a] Laurence I. Midgley, Vincent Stimper, Javier Antoran, Emile Mathieu, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Se(3) equivariant augmented coupling flows, 2023a.
* Midgley et al. [2023b] Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Flow annealed importance sampling bootstrap. In _International Conference on Learning Representations_, 2023b.
* Noe et al. [2019] Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. _Science_, 365(6457), September 2019.
* Rezende et al. [2020] Danilo Jimenez Rezende, George Papamakarios, Sebastien Racaniere, Michael Albergo, Gurtej Kanwar, Phiala Shanahan, and Kyle Cranmer. Normalizing flows on tori and spheres. In _International Conference on Machine Learning_, pages 8083-8092. PMLR, 2020.
* Selli et al. [2016] Daniele Selli, Salah Eddine Boufelfelfel, Philipp Schapotschnikow, Davide Donadio, and Stefano Leoni. Hierarchical thermoelectrics: crystal grain boundaries as scalable phonon scatterers. _Nanoscale_, 8(6):3729-3738, 2016.
* Swenson et al. [2018a] David W. H. Swenson, Jan-Hendrik Prinz, Frank Noe, John D. Chodera, and Peter G. Bolhuis. OpenPathSampling: A python framework for path sampling simulations. 1. basics. _Journal of Chemical Theory and Computation_, 15(2):813-836, October 2018a.
* Swenson et al. [2018b] David W. H. Swenson, Jan-Hendrik Prinz, Frank Noe, John D. Chodera, and Peter G. Bolhuis. OpenPathSampling: A python framework for path sampling simulations. 2. building and customizing path ensembles and sample schemes. _Journal of Chemical Theory and Computation_, 15(2):837-856, October 2018b.
* Tabak and Turner [2013] E. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. _Communications on Pure and Applied Mathematics_, 66(2):145-164, 2013.
* Tabak and Vanden-Eijnden [2010] Esteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood. _Communications in Mathematical Sciences_, 8(1):217-233, 2010.

Method Details

### Latent Path MCMC Algorithm

Our latent space path sampling approach builds on the Metropolis-Hastings method but relies on a modified acceptance criteria and an adapted proposal kernel.

```
0: Initial path \(\bm{x}^{(0)}\) with \(l\) frames, a trained Boltzmann generator consisting of the map \(F\) and its inverse \(F^{-1}\), the number of steps to run \(N\), and a latent proposal kernel \(K\) with proposal probability \(q_{Z}\left(\cdot\mid\cdot\right)\).
0: MCMC samples following target distribution \(\left\{\bm{x}^{(1)},\ldots,\bm{x}^{(N)}\right\}\).
1 Calculate latent space representation of initial path \(\bm{z}^{(0)}=\left\{F^{-1}\left(\bm{x}_{1}^{(0)}\right),\ldots,F^{-1}\left(\bm {x}_{l}^{(0)}\right)\right\}\).
2for\(i\gets 1\ldots N\)do
3repeat
4 Propose new path in latent space \(\tilde{\bm{z}}=K\left(\bm{z}^{(i-1)}\right)\).
5 Compute the proposed path in configuration space \(\tilde{\bm{x}}=\left\{F\left(\tilde{\bm{z}}_{1}\right),\ldots,F\left(\tilde{ \bm{z}}_{l}\right)\right\}\).
6 Compute acceptance probability \[\alpha=\min\left\{1,\frac{p_{AB}\left(\tilde{\bm{x}}\right)}{p_{AB}\left(\bm{ x}^{(i-1)}\right)}\cdot\frac{q_{Z}\left(\bm{z}^{(i-1)}\mid\tilde{\bm{z}} \right)}{q_{Z}\left(\tilde{\bm{z}}\mid\bm{z}^{(i-1)}\right)}\cdot\prod_{j=1}^ {l}\frac{\det J\left(F\left(\tilde{\bm{z}}_{j}\right)\right)}{\det J\left(F( \bm{z}_{j}^{(i-1)})\right)}\right\}.\]
7 Draw a uniformly distributed random number \(u\sim\mathcal{U}_{[0,1]}\).
8untilproposedpath\(\tilde{\bm{x}}\) is reactiveand\(u\leq\alpha\);
9 Accept proposed path \(\bm{z}^{(i)}=\tilde{\bm{z}},\bm{x}^{(i)}=\tilde{\bm{x}}\).
10
11 end for ```

**Algorithm 1**Fixed-length latent space transition path sampling.

### Gaussian Process Kernel

A Gaussian process fits the parameters of a kernel \(k\). As for the concrete choice of kernel, we have decided to use an RBF-Kernel with an additional White kernel that can capture variance in the individual points. It can be formulated as

\[k(x,x^{\prime})=c\cdot\exp\left(-\frac{\|x-x^{\prime}\|_{2}^{2}}{2l^{2}} \right)+n\cdot 1_{x\neq x^{\prime}},\] (8)

with learnable parameters \(l,c,n\).

### Boltzmann Generator Training

We trained a Boltzmann generator \(F\) on the molecule ALDP, consisting of multiple neural spline layers (Durkan et al., 2019) with a randomly masked coupling architecture between them. The coupling layers allow us to use arbitrarily complex neural networks, that do not have to be invertible while still allowing the overall function to be invertible (Dinh et al., 2017). In this architecture, the neural network learns to predict 8 knots and the parameters of a quadratic rational spline function. Overall, we use 12 of these neural spline coupling layers each using a residual block with two layers with 256 hidden units. The performance of the trained Boltzmann generator is illustrated in Figure 5.

To train the normalizing flow, we use the samples from the long-running MD simulation of ALDP and maximize the likelihood of the frames in latent space is. The goal of the loss is that the samples in latent space are distributed according to the base distribution. This is achieved by minimizing the forward KL divergence

\[\mathcal{L}_{KL}\left(\bm{\theta}\right)\propto-\mathbb{E}_{x\sim X}\left[ \log\left(p_{u}\left(F_{\bm{\theta}}^{-1}(x)\right)\right)-\log\left(\det J \left(F_{\bm{\theta}}^{-1}(x)\right)\right)\right].\] (9)

\(J\) represents the Jacobian and \(p_{u}\) is the distribution of our latent space. \(F\) represents the invertible function of the Botlzmann generator that maps between the ground truth data distribution \(X\) and is parameterized by \(\bm{\theta}\).

To represent the molecule, we rely on an internal coordinate representation for our flow, which describes the molecule's state by the dihedral angles and bond lengths (Rezende et al., 2020) as this has shown good performance (Noe et al., 2019; Midgley et al., 2023). Since some of these variables are periodic, we use a mixture between a Gaussian and a uniform distribution as the base distribution. This mixture is only used for training; at inference, we change this to a standard normal distributed space by using the cumulative and inverse cumulative function to map uniform values from and to a normal distribution.

### Further Latent Space Investigation

To ensure that the learned latent space is meaningful and can separate between different meta-stable states, we have reduced samples of the states \(C_{5}\) and \(\alpha_{R}\) to two dimensions, as seen in Figure 6. Already a PCA, a non-linear dimensionality reduction, is capable the separating the states by a single dimension. This motivates that a linear interpolation between configurations in latent space can produce feasible transition paths.

Figure 5: **Histogram of states sampled by Boltzmann generator.** This histogram shows the main dihedral angles of 1 million ALDP conformations sampled from the base distribution and then transported with the normalizing flow.

Figure 6: **Separability of meta-stable States in latent space.** Transforming molecule conformations following the states \(C_{5}\) and \(\alpha_{R}\) into the latent space, and plotting them in 2D with PCA and TSNE. The colors indicate the different conformations.

Determining the Timestep for TPS

Finding out the transition time between two states is necessary to be able to determine a suitable timestep \(\Delta t\) and the number of frames. While this task can be challenging for large systems, this is not a task we set out to solve. To determine meaningful values, we have estimated the density of the transition times as they occur in a long-running MD simulation, as can be seen in Figure 7. With this, we have decided to sample transition paths with a duration of \(1.6ps\). Similar studies can be performed to determine the transition times with high probability for transitions between \(\alpha_{R}\) and \(C_{7}\), where we decided to use a time of \(320fs\).

Figure 7: **Duration of ALDP transitions.** This is the approximated density that shows the duration of the transition between the states \(C_{5}\) and \(\alpha_{R}\) and their respective densities.