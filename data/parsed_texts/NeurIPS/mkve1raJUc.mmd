# Robust Mean Estimation Without Moments for

Symmetric Distributions

 Gleb Novikov

Department of Computer Science

ETH Zurich

&David Steurer

Department of Computer Science

ETH Zurich

&Stefan Tiegel

Department of Computer Science

ETH Zurich

###### Abstract

We study the problem of robustly estimating the mean or location parameter without moment assumptions. Known computationally efficient algorithms rely on strong distributional assumptions, such as sub-Gaussianity, or (certifiably) bounded moments. Moreover, the guarantees that they achieve in the heavy-tailed setting are weaker than those for sub-Gaussian distributions with known covariance. In this work, we show that such a tradeoff, between error guarantees and heavy-tails, is not necessary for symmetric distributions. We show that for a large class of symmetric distributions, the same error as in the Gaussian setting can be achieved efficiently. The distributions we study include products of arbitrary symmetric one-dimensional distributions, such as product Cauchy distributions, as well as elliptical distributions, a vast generalization of the Gaussian distribution.

For product distributions and elliptical distributions with known scatter (covariance) matrix, we show that given an \(\varepsilon\)-corrupted sample, we can with probability at least \(1-\delta\) estimate its location up to error \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\) using \(\frac{d\log(d)+\log(1/\delta)}{e^{2}\log(1/\varepsilon)}\) samples. This result matches the best-known guarantees for the Gaussian distribution and known SQ lower bounds (up to the \(\log(d)\) factor). For elliptical distributions with unknown scatter (covariance) matrix, we propose a sequence of efficient algorithms that approaches this optimal error. Specifically, for every \(k\in\mathbb{N}\), we design an estimator using time and samples \(\tilde{O}(d^{k})\) achieving error \(O(\varepsilon^{1-\frac{1}{2k}})\). This matches the error and running time guarantees when assuming certifiably bounded moments of order up to \(k\). For unknown covariance, such error bounds of \(o(\sqrt{\varepsilon})\) are not even known for (general) sub-Gaussian distributions.

Our algorithms are based on a generalization of the well-known filtering technique [1]. More specifically, we show how this machinery can be combined with Huber-loss-based techniques to work with projections of the noise that behave more nicely than the initial noise. Moreover, we show how sum-of-squares proofs can be used to obtain algorithmic guarantees even for distributions without a first moment. We believe that this approach may find other applications in future works.

## 1 Introduction

Robust statistics [1, 1] is a central field in statistics with the goal of designing algorithms for statistical problems that are robust to a small amount of outliers, for example caused by measurement errors or corrupted data. We model this as follows: Samples are generated from an unknowndistribution \(\mathcal{D}\) and then an \(\varepsilon\)-fraction of them is arbitrarily corrupted by an adversary with full knowledge of the underlying model and our algorithm. We only have access to the corrupted samples. In this work we focus on the canonical task of estimating the mean of \(\mathcal{D}\). Traditionally, estimators robust to such corruptions have been computationally inefficient, requiring time exponential in the ambient dimension [1], while computationally efficient estimators have incurred error scaling with the ambient dimension thus rendering them unsuitable for today's high-dimensional statistical tasks. Recently however, a flurry of efficient estimators emerged achieving guarantees without this prohibitive dependence on the dimension and with error rates approaching those of computationally inefficient ones [1, 13, 14, 15, 16, 17, 18, 19].

A textbook example of this development is when \(\mathcal{D}\) is the multi-variate Gaussian distribution (or sub-Gaussian distributions with _known_ covariance). In this setting, efficient algorithms can estimate the mean to within error \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\), i.e., nearly linear in \(\varepsilon\)[1, 13, 14, 15]. The statiscally optimal error rate is \(O(\varepsilon)\)[1]. That is, efficient algorithms are optimal up to the additional factor of \(\sqrt{\log(1/\varepsilon)}\). Further, it is conjectured that this factor is inherent for efficient algorithms, i.e., that there is a _computational-statistical gap_. This is evidenced by known lower bounds for statistical query algorithms [10]. However, there are two drawbacks with this textbook example: If the covariance matrix is unknown, it is not known how to efficiently achieve the same error for sub-Gaussian distributions, in fact, in full generality, it is not even known how to achieve error \(o(\sqrt{\varepsilon})\) in this setting. This is because known algorithms rely heavily on the algebraic structure of Gaussian moments. Second, the assumption that the uncorrupted data belongs to a (sub)-Gaussian distribution is arguably very strong. A more natural setting is when \(\mathcal{D}\) is allowed to have heavier tails. In the setting that \(\mathcal{D}\) has bounded second moments it is known how to achieve error \(O(\sqrt{\varepsilon})\) efficiently [1, 13, 14, 15]. Interestingly, \(O(\sqrt{\varepsilon})\) seems to constitute a barrier for efficient algorithms. In particular, while error \(o(\sqrt{\varepsilon})\) is possible information-theoretically if we assume that higher-order moments are bounded, [17] show that bounded moments alone are likely not enough for efficient algorithms, and that additional assumptions are required. Currently, these manifest as either assuming that there is a certificate, in sum-of-squares, for the boundedness of the moments [17, 18] or assuming the covariance is the identity (or known) [13]. While these approaches indeed break the \(O(\sqrt{\varepsilon})\) barrier, they in many cases fall short of the \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\) error possible in the Gaussian setting. In particular, they only achieve error comparable to the Gaussian case, when using \(O(\log(1/\varepsilon))\) many moments. Unfortunately, it is necessary to use this many moments, even when assuming they are certifiably bounded [18]. Thus, the tails of these distributions are already much lighter than, say, in the bounded covariance case. Therefore, a natural question is:

_Do there exist classes of heavy-tailed distributions for which we can (efficiently) achieve the same robust error as for the Gaussian distribution?_

In this work, we answer this question by giving algorithms that use a (quasi-)polynomial number of samples (in the dimension and \(1/\varepsilon\)) and time polynomial in the number of samples for the broad class of symmetric product and elliptical distributions. Moreover, in many cases, we can achieve this using (nearly) the same amount of samples as for the Gaussian distribution, recovering, e.g., the optimal dependence on the failure probability. We remark that the distributions we consider might have arbitrarily heavy tails and we do not make any assumptions related to sum-of-squares.

Product and Elliptical DistributionsSymmetry is a natural assumption in statistics with many applications to, e.g., mathematical finance and risk management [17, 13, 15]. In this work, we consider the following two types of symmetric distributions that are of particular interest [16, 15, 17]: First, product distributions of symmetric one-dimensional distributions and spherically symmetric distributions. These correspond to a generalization of the standard Gaussian distribution. Examples are product Cauchy distributions and the multi-variate Student \(t\)-distribution with identity scatter matrix. Second, elliptical distributions. These correspond to a generalization of Gaussians with arbitrary (unknown) covariance matrix. Examples include the multi-variate Student \(t\)-distribution, symmetric multivariate stable distributions and multivariate Laplace distributions. For both classes, it is information-theoretically possible to obtain robust error \(O(\varepsilon)\)[1, 13], i.e., matching that of a Gaussian. These approaches are not known to run in faster than exponential time and prior work [13] asked whether a similar error can be achieved efficiently. We respond to this question, by designing algorithms that nearly, in some cases exactly, match this error in polynomial (in the number of samples) time, if the number of samples is polynomial (in the cases of symmetric product or spherically symmetric distribution) or quasi-polynomial (in the case of elliptical distributions) in the dimension and \(1/\varepsilon\).

### Problem Set-Ups and Results

Next, we give formal definitions of the corruption model and distributions considered and state our results. We use the following standard model for corruptions - often referred to as the strong contamination model.

**Definition 1.1**.: Let \(\bm{X}_{1},\ldots,\bm{X}_{n}\) be i.i.d.samples from a distribution \(\mathcal{D}\) and let \(\varepsilon>0\). We say that \(Z_{1},\ldots,Z_{n}\) are an \(\varepsilon\)_-corruption_ of \(\bm{X}_{1},\ldots,\bm{X}_{n}\), if they agree on at least an \((1-\varepsilon)\)-fraction of the points. The remaining \(\varepsilon\)-fraction can be arbitrary and in particular, can be corrupted by an adversary with full knowledge of the model, our algorithm, and all problem parameters.

We consider the following two classes of distributions.

**Definition 1.2** (Semi-Product Distributions).: Let \(\rho>0\). We say a distribution \(\mathcal{D}\) over \(\mathbb{R}^{d}\) is an \(\rho\)_-semi-product_ distribution, if for \(\bm{\eta}\sim\mathcal{D}\) it holds that

1. For all \(j\in[d]\), the distribution of \(\bm{\eta}_{j}\) is symmetric about 0,
2. For all \(j\in[d]\), \(\mathbb{P}\left(|\bm{\eta}_{j}|\leqslant\rho\right)\geqslant\frac{1}{100}\),
3. The random vectors \(\left(\operatorname{sign}(\bm{\eta}_{j})\right)_{j=1}^{d}\) and \(\left(|\bm{\eta}_{j}|\right)_{j=1}^{d}\) are independent, and the random variables \(\operatorname{sign}\left(\bm{\eta}_{1}\right),\ldots,\operatorname{sign} \left(\bm{\eta}_{d}\right)\) are mutually independent.

Some remarks about the definition are in order. First, notice that the Gaussian distribution \(N(0,\rho^{2}\cdot I)\) is \(\Theta(\rho)\)-semi-product. Similarly, every symmetric product or spherically symmetric distribution that has covariance bounded by \(\rho^{2}\cdot\operatorname{Id}_{d}\) is \(\Theta(\rho)\)-semi-product. In particular, the coordinates \(\bm{\eta}_{j}\) need not be independent. However, the definition allows for much heavier tails, the only requirement is that at least a \(1/100\)-fraction of the probability mass lies in an interval of length \(\rho\) around 01. In particular, it captures all spherically symmetric distributions, e.g. multi-variate \(t\)-distributions with identity scatter matrix, and all symmetric product distributions, e.g. the product Cauchy distribution. Notice that in the non-robust setting the first two properties are enough to accurately estimate \(\mu^{*}\) from samples \(\mu^{*}+\bm{\eta}_{1},\ldots,\mu^{*}+\bm{\eta}_{n}\), via adding symmetric mean zero noise with tiny variance to each coordinate and computing the entry-wise median. We expect that _some_ additional assumption is necessary for efficient algorithms in the robust setting. We show that Property 3 above is sufficient. A different sufficient condition would be assuming that the distribution is elliptical, which we discuss next.

Footnote 1: This can even further be relaxed to any \(\alpha>0\) fraction. The sample complexity and error then scale naturally with \(\alpha\). See the appendices for more details.

The second one is the class of elliptical distributions [12, 13, 14] a generalization of spherically symmetric distribution, which in particular can have more complex dependency structures.

**Definition 1.3**.: A distribution \(\mathcal{D}\) over \(\mathbb{R}^{d}\) is _elliptical_, if for \(\bm{\eta}\sim\mathcal{D}\) the following holds: Let \(\bm{U}\) be uniformly distributed over the \(d\)-dimensional unit sphere. There exists a positive random variable \(\bm{R}\), independent of \(\bm{U}\), and a positive semi-definite matrix \(\Sigma\), such that

\[\bm{\eta}=\bm{R}\Sigma^{1/2}\bm{U}\quad\text{and}\quad\mathbb{P}\left(\bm{R} \leqslant\sqrt{2d}\right)\geqslant\frac{1}{100}\,.\]

We call \(\Sigma\) the _scatter_ matrix of the distribution (sometimes also referred to as the dispersion matrix). In particular, the Gaussian distribution \(N(0,\Sigma)\) with arbitrary \(\Sigma\) is elliptical. Further, we can choose its covariance matrix as the scatter matrix. Indeed, we can decompose \(\bm{X}\sim N(0,\Sigma)\) as \(\bm{X}=\bm{R}\Sigma^{1/2}\bm{U}\), where \(\bm{R}\) is distributed as the square root of a \(\chi^{2}\)-distribution with \(d\) degrees of freedom and \(\bm{U}\) is independent of \(\bm{R}\) and distributed uniformly over the unit sphere. Then by Markov's Inequality, it holds that \(\mathbb{P}(\bm{X}\leqslant\sqrt{2d})=\mathbb{P}(\bm{X}^{2}\leqslant 2d) \geqslant\frac{1}{2}\). Elliptical distributions with scatter matrix identity correspond to spherically symmetric distributions, a special case of semi-product distributions. Note that that elliptical distributions _do not_ capture product distributions except for the Gaussian case - e.g., the product Cauchy distribution is not elliptical.

Extending the above two defintions, we say that a distribution \(\mathcal{D}\) is \(\rho\)-semi-product (resp. elliptical) _with location \(\mu^{*}\in\mathbb{R}^{d}\)_ if samples of \(\mathcal{D}\) take the form \(\mu^{*}+\bm{\eta}\), where \(\mu^{*}\in\mathbb{R}^{d}\) is deterministic and \(\bm{\eta}\) is \(\rho\)-semi-product (resp. elliptical). Note that the location takes the place of the mean, as this might not exist for distributions of the above form.

ResultsOur main result for semi-product distributions is the following: Note that we can reduce the case of elliptical distributions with known scatter matrix to the \(\Theta(1)\)-semi-product case, hence the theorems below also apply to this setting. We also remark that the algorithm only receives the corrupted samples as input. In particular, \(\rho\) need not be known (and can be estimated from the corrupted samples). See the appendices for a proof.

**Theorem 1.4**.: _Let \(\mu^{*}\in\mathbb{R}^{d},\varepsilon,\rho>0\) and \(\mathcal{D}\) be a \(\rho\)-semi-product distribution with location \(\mu^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \(\varepsilon\leqslant 1/C\) and \(n\geqslant C\cdot\frac{d\log(d)+\log(1/\delta)}{\varepsilon^{2}\log(1/ \varepsilon)}\). Then, there exists an algorithm that, given an \(\varepsilon\)-corrupted sample from \(\mathcal{D}\), runs in time \(n^{O(1)}\) and outputs \(\hat{\mu}\in\mathbb{R}^{d}\) such that with probability at least \(1-\delta\) it holds that_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\rho\cdot\left[\sqrt{\frac{d\log(d)+ \log(1/\delta)}{n}}+\varepsilon\sqrt{\log(1/\varepsilon)}\right]\right)\,.\]

Our sample complexity is nearly optimal in the dependence of the error on \(\varepsilon\) and \(d\) and optimal in the dependence on the failure probability \(\delta\), all up to constant factors. Recall that \(N(0,\rho^{2}\cdot I_{d})\) is a \(\Theta(\rho)\)-semi-product distribution and hence lower bounds for this setting apply. The statistically optimal error in this setting is \(\Omega(\rho\cdot\varepsilon)\)[12] and can be achieved using \(n\geqslant\frac{d+\log(1/\delta)}{\varepsilon^{2}}\) samples. Note that we match this error up to the \(\sqrt{\log(1/\varepsilon)}\) term, using only slightly more samples (\(d\) vs \(d\log d\)). It is conjectured that the larger error is necessary for efficient algorithms, as it is necessary for all efficient SQ algorithms [1]. It is an interesting open question to remove the additional factor of \(\log d\) in our sample complexity. Further, our algorithm nearly matches results known for the standard Gaussian distribution (up to the \(\log(d)\) factor in error and sample complexity) [1, 1, 1, 10]. We expect our algorithm to be practical, since it only uses one-dimensional smooth convex optimization (\(O(nd)\) times), the top eigenvector computation (\(O(n)\) times) and arithmetic operations.

Interestingly, we show how to achieve error scaling only with \(O(\rho\cdot\varepsilon)\) using quasi-polynomially many samples:

**Theorem 1.5**.: _Let \(\mu^{*}\in\mathbb{R}^{d},\varepsilon,\rho>0\) and \(\mathcal{D}\) be a \(\rho\)-semi-product distribution with location \(\mu^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \(\varepsilon\leqslant 1/C\) and \(n\geqslant C\cdot d^{C\log(1/\varepsilon)}\). Then, there exists an algorithm that, given an \(\varepsilon\)-corrupted sample from \(\mathcal{D}\), runs in time \(n^{O(1)}\) and outputs \(\hat{\mu}\in\mathbb{R}^{d}\) such that with probability at least \(1-\delta\) it holds that_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\rho\cdot\left[\sqrt{\frac{d+\log(1/ \delta)}{n}}+\varepsilon\right]\right)\,.\]

In order to state our results for elliptical distributions, we need to introduce the notion of the _effective rank_ of a matrix \(\Sigma\). This is defined as \(r(\Sigma)\coloneqq\operatorname{Tr}\Sigma/\|\Sigma\|\) and captures the intrinsic dimensionality of the data. Note that it is always at most \(d\), but can also be significantly smaller. We remark that we do not assume that the scatter matrix \(\Sigma\) is known to the algorithm.

**Theorem 1.6**.: _Let \(C>0\) be a large enough absolute constant. Let \(k\in\mathbb{N},\varepsilon,\delta>0,\mu^{*}\in\mathbb{R}^{d}\) such that \(\varepsilon\leqslant 1/C\) and assume \(\mathcal{D}\) is an elliptical distribution with location \(\mu^{*}\) and scatter matrix \(\Sigma\) satisfying \(r(\Sigma)\geqslant C\cdot k\log d^{2}\). Also, let \(n\geqslant C\cdot(r(\Sigma)/k)^{k}\log(d/\delta)\). There is an algorithm that, given an \(\varepsilon\)-corrupted sample from \(\mathcal{D}\), runs in time \(n^{O(1)}d^{O(k)}\) and with probability at least \(1-\delta\) outputs \(\hat{\mu}\in\mathbb{R}^{d}\) satisfying_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\sqrt{\|\Sigma\|}\cdot\left[\sqrt{\frac{ r\,(\Sigma)\cdot\log(d/\delta)}{n}}+\sqrt{k}\cdot\varepsilon^{1-1/(2k)} \right]\right)\,.\]Note that in the special case when \(\mathcal{D}=N(0,\Sigma)\), the information-theoretically optimal error is \(\Omega(\sqrt{\|\Sigma\|}\cdot\varepsilon)\) and can be achieved using \(n\geqslant\frac{r(\Sigma)+\log(1/\delta)}{\varepsilon^{2}}\) samples [1, 10]. We give a sequence of algorithms (nearly) approaching this error using an increasing number of samples and time. In fact, for \(k=O(\log(1/\varepsilon))\), we achieve error \(O(\varepsilon\sqrt{\log(1/\varepsilon)}\cdot\sqrt{\|\Sigma\|})\) using at most \(\frac{r(\Sigma)^{\log(1/\varepsilon)}\log(d/\delta)}{\varepsilon^{2}}\) samples and time \(d^{k}\). Similar to the bounded moment setting [1, 10] the parameter \(k\) can be thought of as a way to trade between sample/time complexity and accuracy guarantees. Also note that already for \(k=2\) we achieve error \(O(\varepsilon^{3/4}\sqrt{\|\Sigma\|})\) for elliptical distributions with unknown covariance/scatter matrix in polynomial time and sample complexity, while even for (general) sub-Gaussian distributions with unknown covariance it is not known if it is possible to achieve error \(o(\sqrt{\varepsilon}\cdot\sqrt{\|\Sigma\|})\) in polynomial time and sample complexity. In addition, if \(r(\Sigma)\) is much smaller than \(d\), for example, \(r(\Sigma)\leqslant d^{0.1}\), then we achieve error \(O(\varepsilon^{3/4}\sqrt{\|\Sigma\|})\) with a _sub-linear_ number of samples \(n=\tilde{O}(d^{0.2})\) in time \(d^{O(1)}\).

Previous Algorithms and Possible ExtensionsPreviously, to the best of our knowledge, only exponential time computable estimators were known, for both semi-product and elliptical distributions.

The results [1, 1] imply that there exist estimators that achieve error \(O(\sqrt{\frac{d+\log(1/\delta)}{n}}+\varepsilon)\) (assuming \(\|\Sigma\|\leqslant O(1)\) for elliptical distributions). Both algorithms rely on brute force techniques for which it is unclear how to make them efficient.

A natural question is if we can achieve (nearly) optimal error in polynomial time (analagous to the semi-product case). Indeed, for _non-spherical_ Gaussian distributions, it is known how to do this [1, 1]. These algorithms crucially rely on robustly estimating the covariance matrix first. Similarly for elliptical distributions, it seems necessary to first robustly estimate the scatter matrix and then apply our results for the semi-product case. However, current covariance estimation algorithms rely heavily on the algebraic relations between the Gaussian moments, and it is not known how to achieve this for other distributions where moments do not have these specific relations. Thus, we expect this to be an interesting but challenging task.

A different direction we believe is interesting to explore is the following: We gave algorithmic results matching in many cases what is known for the Gaussian distribution. In the non-robust setting, there is in principle hope to go beyond this: There are known estimators that are asymptotically normal with variance scaling with the Fisher information of the distribution [14]. So there is hope for "better-than-Gaussian error" on distributions with small Fisher information. However, these results are purely asymptotic and there are (symmetric) distributions for which we cannot hope to achieve even finite sample results scaling with the Fisher information (see, e.g., the discussions in [1]). To get around this issue, the recent work of [1] introduced a related notion, called "smoothed Fisher Information" and achieved finite-sample (non-robust) error guarantees scaling with the smoothed Fisher Information in the one-dimensional case, also for symmetric distributions. It would be interesting to see, if similar guarantees can be achieved in the robust and high-dimensional setting, too.

We further believe that similar guarantees can be achieved based on \(\ell_{1}\)-minimization techniques (instead of Huber-loss) under slightly different assumptions. In particular, even in the non-robust setting, we need to require a small amount of density at (or sufficiently) close to the location, else any point in the region around the location with zero density, would be an \(\ell_{1}\)-minimizer. In the case of semi-product distributions, this is without of loss of generality, since we could add Gaussian noise of the appropriate variance to each coordinate. For elliptical distribution, this approach does not work, since the resulting distribution might no longer be elliptical.

### Related Work

We only list the works most closely related to this work, we refer to [1] for a survey on the area. Most of the literature on efficient algorithms for robust mean estimation has focussed on the setting when at least some of the moments are bounded. In what follows we focus mainly on how the error guarantees depend on \(\varepsilon\) and put other parameters, such as the dependence on the failure probability aside. [1, 1, 1] show how to efficiently estimate the mean of a Gaussian distribution, or sub-Gaussian with identity-covariance, up to error \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\). Assuming that the distribution has covariance bounded by identity, [1, 1, 1] show how to achieve error \(O(\sqrt{\varepsilon})\). Similarly, for \(\alpha\in[0,1]\), [12] shows how to achieve error \(O(\varepsilon^{\alpha/(1+\alpha)})\) when in every direction the \(1+\alpha\) moments are bounded. Note that in constrast to this, our algorithm can handle distributions without a first moment and achieves error \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\). An interesting question is whether assuming bounded higher-order moments can lead to improved error guarantees. Interestingly, [13] shows that this alone is likely to be not enough in order to go beyond error \(\sqrt{\varepsilon}\). So far, there have been two ways of adding additional assumptions: First, assuming that \(\mathcal{D}\) has \(k\)-th moments bounded by \(\sigma_{k}\) in every direction, and assuming that it is certifiable in sum-of-squares, it is possible to achieve error \(O(\sigma_{k}\varepsilon^{1-1/(2k)})\) using \(d^{O(k)}\) samples and time [13, 14, 15]. Second, if the \(k\)-th moments are bounded by \(\sigma_{k}\), not necessarily certifiably in sum-of-squares, and the covariance matrix is a multiple of the identity (or known), [16] achieves error \(O(\sigma_{k}\varepsilon^{1-1/(2k)})\).

Other Results On Symmetric Noise Distributions and the Filtering TechniquesHuber-loss minimization has been used to design computationally efficient estimators for a variety of problems: Linear regression in the heavy-tailed/symmetric setting [17, 18, 19, 20], dNS21, dLN\({}^{+}\)21], as well as the robust setting [19]. Other works developped algorithms for PCA under symmetric noise [18, 19, 20].

The filtering technique [16, 17] (or versions thereof) have been successfully applied to robust and heavy-tailed (when the covariance exists) mean estimation [13, 16, 15, 14, 17, 18] as well as a pre-processing step in robust regression [19].

## 2 Techniques

We will first describe the general theme of our techniques. These ideas will yield algorithms achieving (nearly) optimal guarantees using quasi-polynomially many samples. Note however, that using these, we can already achieve error \(o(\sqrt{\varepsilon})\) using only polynomially many samples. At the end of this section we show how to improve this to nearly linearly many (in the dimension) samples for semi-product distributions.

One-Dimensional Robust Estimation via Huber-Loss MinimizationCurrent algorithms for robust mean estimation are only known to work for distributions with bounded moments. In many cases this is assumption is reasonable, and in many cases it is also necessary [14, 15]. However, there are many other distributions that do not necessarily have any moments at all, but which one would expect to behave nicely. Current approaches are inapplicable in these settings. Of particular interest is the class of symmetric distributions, which might not even have a first moment - in this case we want to estimate the location parameter of the distribution, which always exists and coincides with the mean in case it exists. In non-robust statistics (when there are no corruptions), symmetric distributions in some sense behave as nicely as the Gaussian distribution: The minimizer of the entry-wise _Huber-loss_ function achieves the same guarantees for such distributions as the sample mean in the Gaussian case. The entry-wise Huber-loss is the function \(\mathcal{L}_{H}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) defined as \(\mathcal{L}_{H}\left(v\right)\coloneqq\sum_{j=1}^{d}\Phi\left(v_{j}\right)\), where \(\Phi\colon\mathbb{R}\rightarrow\mathbb{R}\) is a _Huber penalty_: \(\Phi(x)=x^{2}/2\), if \(|x|\leqslant 1\), and \(\Phi(x)=|x|-1/2\) otherwise. \(\Phi\) is convex, and has many appealing properties, in particular with respect to symmetric distributions, since its derivative is a bounded odd Lipschitz function: \(\phi(x):=\Phi^{\prime}(x)=x\), if \(|x|\leqslant 1\), and \(\phi(x)=\operatorname{sign}\left(x\right)\) otherwise.

Since the Gaussian distribution is symmetric itself, the Huber-loss estimator has the same guarantees as the sample mean in this setting. Beyond that, the Huber-loss estimator enjoys some robustness guarantees that the sample mean does not: Let us consider the one-dimensional case, where the Huber-loss behaves similarly to the median but has additional properties that will be useful to us, e.g., it is differentiable everywhere. In this setting, the sample mean has unbounded error even if there is only a single corrupted sample, while the Huber-loss achieves the information theoretically optimal error \(O\left(\varepsilon\right)\) when an arbitrary \(\varepsilon\)-fraction of the samples is corrupted. Moreover, these guarantees extend to arbitrary one-dimensional symmetric distributions that satisfy mild scaling assumptions. Unfortunately, this does not directly yield a good robust estimator for the high-dimensional setting. If we naively apply the one-dimensional estimator entry-wise, we can only guarantee an error bound of \(O(\varepsilon\sqrt{d})\), which is far away from the \(O(\varepsilon)\) error that is statistically possible for the Gaussian distribution and symmetric distributions. Moreover, it is also inferior to error rates obtained by efficient estimators that use assumptions about bounded moments: Such estimators achieve error that does not depend on \(d\). In this work we show that there exist more sophisticated estimators based on the Huber-loss3 that achieve dimension-independent error, often matching what is possible for the Gaussian distribution, for estimating the location of symmetric distributions even in the high-dimensional robust setting.

Footnote 3: We use entry-wise Huber-loss for semi-product distributions. For elliptical distributions we use another loss function, that is also related to the Huber penalty.

Proofs of identifiability and the filtering techniqueBefore describing how we use the Huber-loss in the high-dimensional setting, it will be instructive to recall how the classical approach for distributions with bounded moments works - for simplicity, we focus on bounded second moments and sketch how this extends to higher-order moments. Later, we will see how to modify these ideas, using the Huber-loss, to work also in the symmetric setting without moment assumptions. In particular the version for Gaussian-like error for semi-product distribution requires several new technical ideas compared to the Gaussian setting.

At the heart lie _identifiability proofs_ which can be made algorithmic using either sum-of-squares [12, 13] or the filtering technique [1, 10, 11, 12]. For bounded covariance distributions these take the following form: If two distributions \(D_{1}\) and \(D_{2}\) with means \(\mu_{1}\), respectively \(\mu_{2}\), and covariance matrices \(\Sigma_{1}\preceq\mathrm{Id}_{d}\), respectively \(\Sigma_{2}\preceq\mathrm{Id}_{d}\), are \(\varepsilon\)-close in statistical distance, then \(\left\|\mu_{1}-\mu_{2}\right\|\leqslant O\left(\sqrt{\varepsilon}\right)\). In what follows, for simplicity of the discussion, we will sometimes switch between empirical and true distributions. In robust mean estimation, we assume that \(D_{1}\) is the empirical distribution over the corrupted samples we observe, while \(D_{2}\) is the empirical distribution of the uncorrupted samples. Hence, by assumption, \(\Sigma_{2}\preceq\mathrm{Id}_{d}\) and we wish to estimate \(\mu_{2}\). The above statement of identifiability asserts that as long as \(\Sigma_{1}\preceq\mathrm{Id}_{d}\), the empirical mean of the corrupted samples, i.e., \(\mu_{1}\), is \(O(\sqrt{\varepsilon})\)-close to \(\mu_{2}\). Of course, \(\Sigma_{1}\preceq\mathrm{Id}_{d}\) might not hold, since the outliers could potentially introduce a large eigenvalue in the empirical covariance matrix. However, since we know that these large eigenvalues must have been introduced by outliers, this gives rise to a win-win analysis: We check if \(\Sigma_{1}\preceq\mathrm{Id}_{d}\) holds. If it does, \(\mu_{1}\) is \(O(\sqrt{\varepsilon})\)-close to \(\mu_{2}\) already, if not, we compute its top eigenvector and remove samples that have large correlation with this eigenvector. It turns out that this procedure will always remove more corrupted samples than uncorrupted ones. Thus, we can iterate this procedure and are guaranteed that it terminates after \(O(\varepsilon n)\) iterations. After its termination, we are left with a distribution \(D_{1}^{\prime}\), with mean \(\mu_{1}^{\prime}\), that is \(O(\varepsilon)\)-close in total variation distance to \(D_{1}\) and \(D_{2}\), and has small covariance \(\Sigma_{1}^{\prime}\preceq\mathrm{Id}_{d}\), hence \(\left\|\mu_{1}^{\prime}-\mu_{2}\right\|\leqslant O\left(\sqrt{\varepsilon}\right)\).

Of course, for arbitrary symmetric distributions the covariance might not exist. So neither the above proof of identifiability nor the filtering technique apply to this setting. However, one of our main observations is that we can obtain a similar statement of identifiability also in this case. In addition, we will later show how to adjust the filtering technique to work with our identifiability proof - this requires non-trivial modifications of the original approach. First, observe that the identifiability statement in the bounded moment setting, can be phrased as follows: For \(j\in\{1,2\}\), the mean \(\mu_{j}\) of a distribution \(D_{j}\) corresponds to the minimizer of the quadratic loss:

\[\mu_{j}=\operatorname*{arg\,min}_{a\in\mathbb{R}^{d}}\,\operatorname*{\mathbb{ E}}_{\boldsymbol{y}\sim D_{j}}\left\|\boldsymbol{y}-a\right\|^{2}\,.\]

Hence, if \(D_{1}\), with covariance \(\Sigma_{1}\preceq\mathrm{Id}_{d}\), and \(D_{2}\) with covariance \(\Sigma_{2}\preceq\mathrm{Id}_{d}\), are \(\varepsilon\)-close in statistical distance, then the minimizers, \(\mu_{1}\) and \(\mu_{2}\), of the quadratic loss are \(O(\sqrt{\varepsilon})\)-close to each other. A crucial observation is that, if a distribution \(D\) is symmetric, then its location \(\mu\) is the entry-wise Huber-loss minimizer4

Footnote 4: Assuming that each entry of \(\boldsymbol{y}-\mu\) has nonzero probability mass in the interval \([-1,1]\).

\[\mu=\operatorname*{arg\,min}_{a\in\mathbb{R}^{d}}\,\operatorname*{\mathbb{E}} _{\boldsymbol{y}\sim D}\mathcal{L}_{H}\left(\boldsymbol{y}-a\right)\,.\]

Hence, we would like to obtain a similar statement about minimizers of the Huber-loss for two \(\varepsilon\)-close distributions \(D_{1}\) and \(D_{2}\). This will be useful since we will need to learn the location \(\mu_{2}\) of a symmetric distribution \(D_{2}\) from \(D_{1}\).

Abstract proof of identifiabilityIn order to formalize the above, consider the following abstract setting: Let \(\mathcal{L}\) be some loss function that is differentiable, \(\Omega(1)\)-strongly convex and smooth (i.e. its gradient \(\nabla\mathcal{L}\) is \(1\)-Lipschitz). Let \(D_{1}\) and \(D_{2}\) be two distributions that are \(\varepsilon\)-close in the statistical distance, Then, if for \(j\in\{1,2\}\) the minimizers \(\hat{\mu}_{j}:=\operatorname*{arg\,min}_{a\in\mathbb{R}^{d}}\operatorname*{ \mathbb{E}}_{\boldsymbol{y}\sim D_{j}}\mathcal{L}\left(\boldsymbol{y}-a\right)\) satisfy \(\mathbb{E}_{\bm{y}\sim D_{j}}\left\langle\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{ 2}\right),u\right\rangle^{2}\leqslant 1\) for all unit vectors \(u\), then \(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|\leqslant O(\sqrt{\varepsilon})\). Indeed, by strong convexity and the definition of \(\hat{\mu}_{1}\),

\[\Omega\left(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|^{2}\right) \leqslant\underset{\bm{y}\sim D_{1}}{\mathbb{E}}\,\mathcal{L} \left(\bm{y}-\hat{\mu}_{1}\right)-\underset{\bm{y}\sim D_{1}}{\mathbb{E}}\, \mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right)-\underset{\bm{y}\sim D_{1}}{ \mathbb{E}}\,\left\langle\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right), \hat{\mu}_{1}-\hat{\mu}_{2}\right\rangle\] \[\leqslant\underset{\bm{y}\sim D_{1}}{\mathbb{E}}\,\langle\nabla \mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right),\hat{\mu}_{1}-\hat{\mu}_{2} \rangle\.\]

Notice that there is a coupling \(\omega\) of \(D_{1}\) and \(D_{2}\) such that \(\mathbb{P}_{\left(\bm{y}_{1},\bm{y}_{2}\right)\sim\omega}(\bm{y}_{1}\neq\bm{y }_{2})\leqslant\varepsilon\). Since5\(\mathbb{E}_{\bm{y}\sim D_{2}}\)\(\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right)=0\), we obtain using the Cauchy-Schwarz Inequality

Footnote 5: In this discussion we assume that the operators \(\mathbb{E}\) and \(\nabla\) commute. In the analyses of the algorithms we use empirical means, and in this case they commute by linearity of the gradient.

\[\Omega\left(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|^{2}\right) \leqslant\underset{\bm{y}\sim D_{1}}{\mathbb{E}}\,\langle \nabla\mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{2}\right)-\nabla\mathcal{L}\left( \bm{y}_{2}-\hat{\mu}_{2}\right),\hat{\mu}_{1}-\hat{\mu}_{2}\rangle\] \[=\underset{\left(\bm{y}_{1},\bm{y}_{2}\right)\sim\omega}{\mathbb{ E}}\,\langle\nabla\mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{2}\right)-\nabla \mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{1}\right)-\nabla\mathcal{L}\left(\bm{y }_{2}-\hat{\mu}_{2}\right),\hat{\mu}_{1}-\hat{\mu}_{2}\rangle\] \[\leqslant\sqrt{\varepsilon\underset{\left(\bm{y}_{1},\bm{y}_{2} \right)\sim\omega}{\mathbb{E}}\,\langle\nabla\mathcal{L}\left(\bm{y}_{1}-\hat{ \mu}_{2}\right)-\nabla\mathcal{L}\left(\bm{y}_{2}-\hat{\mu}_{2}\right),\hat{\mu }_{1}-\hat{\mu}_{2}\rangle^{2}}\,.\]

Since \(\nabla\mathcal{L}\) is \(1\)-Lipschitz, using the bounds on \(\max_{\|u\|=1}\mathbb{E}_{\bm{y}\sim D_{j}}\left\langle\nabla\mathcal{L}\left( \bm{y}-\hat{\mu}_{j}\right),u\right\rangle^{2}\) yields

\[\underset{\left(\bm{y}_{1},\bm{y}_{2}\right)\sim\omega}{\mathbb{ E}}\,\langle\nabla\mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{2}\right)-\nabla \mathcal{L}\left(\bm{y}_{2}-\hat{\mu}_{2}\right),\hat{\mu}_{1}-\hat{\mu}_{2} \rangle^{2}\] \[=\underset{\bm{y}_{j}\sim D_{j}}{\mathbb{E}}\,\langle\nabla \mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{2}\right)-\nabla\mathcal{L}\left(\bm{y} _{1}-\hat{\mu}_{1}\right)+\nabla\mathcal{L}\left(\bm{y}_{1}-\hat{\mu}_{1} \right)-\nabla\mathcal{L}\left(\bm{y}_{2}-\hat{\mu}_{2}\right),\hat{\mu}_{1}- \hat{\mu}_{2}\rangle^{2}\] \[\leqslant O\left(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|^{4}+\|\hat{\mu}_{1 }-\hat{\mu}_{2}\|^{2}\right)\,.\]

Hence, \(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|^{2}\leqslant O(\sqrt{\varepsilon}\cdot\|\hat{ \mu}_{1}-\hat{\mu}_{2}\|^{2}+\sqrt{\varepsilon}\cdot\|\hat{\mu}_{1}-\hat{\mu}_{ 2}\|)\,.\) If \(\varepsilon\) is small enough (smaller than some constant that depends on the strong convexity parameter), we obtain that \(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|\leqslant O(\sqrt{\varepsilon})\).

Similar to the bounded moment setting, we can generalize this observation to the case when for some integer \(k>1\) and some \(m_{2k}>0\), \(\max_{\|u\|=1}\,\mathbb{E}_{\bm{y}_{j}\sim D_{j}}\left\langle\nabla\mathcal{L} \left(\bm{y}-\hat{\mu}_{j}\right),u\right\rangle^{2k}\leqslant m_{2k}^{2k}\). In this case we can use Holder's inequality to obtain the bound \(\|\hat{\mu}_{1}-\hat{\mu}_{2}\|\leqslant O(m_{2k}\cdot\varepsilon^{1-1/(2k)})\). Note that in order to combine higher-order moment bounds with the filtering technique, we need an efficient procedure to certify these bounds. Thus, previous works only applied to distributions for which such moment certificates, in sum-of-squares, exist. We remark that in the symmetric setting we are able to obtain the necessary certificates for filtering _without_ making any sum-of-squares related assumption on the distribution.

Filtering in the symmetric settingWe will show how to adopt the filtering technique to make the above proof of identifiability algorithmic for symmetric distributions. Indeed, let \(D_{2}\) be a symmetric distribution with location \(\mu^{*}\). Assume we can choose a loss function \(\mathcal{L}\) such that \(\hat{\mu}_{2}=\mu^{*}\) and \(\mathbb{E}_{\bm{y}\sim D_{2}}\,\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2} \right)\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right)^{\top}\preceq \mathrm{Id}_{d}\). Then, if \(D_{1}\) is an \(\varepsilon\)-corruption of \(D_{2}\), we can iteratively compute \(\mathbb{E}_{\bm{y}\sim D_{1}}\,\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{1} \right)\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{1}\right)^{\top}\) and check its top eigenvalue. If it is smaller than \(1\), we know that \(\hat{\mu}_{1}\) must be \(O(\sqrt{\varepsilon})\)-close to \(\hat{\mu}_{2}=\mu^{*}\), if not, we can remove points which are aligned with the top eigenvector. Similar to the classical filtering setting, we can argue that this removes more corrupted points than uncorrupted points. Thus, after at most \(O(\varepsilon n)\) iterations, we must have that \(\hat{\mu}_{1}\) is indeed \(O(\sqrt{\varepsilon})\)-close to \(\mu^{*}\). A similar argument works for higher-order moments.

It remains to verify the assumptions we used on the loss function \(\mathcal{L}\) and its interaction with our distribution. First, we required that \(\mathcal{L}\) is smooth and strongly convex, which is very restrictive. Fortunately, we can relax the second assumption: For our argument it is enough if \(\mathcal{L}\) is _locally_ strongly convex around the minimizer (and globally smooth). Second, we assumed that \(\mathbb{E}_{\bm{y}\sim D_{2}}\,\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2} \right)\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{j}\right)^{\top}\preceq \mathrm{Id}_{d}\). \(\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right)\) can be seen as some transformation of the distribution, and our condition requires that the covariance of the transformed distribution is bounded. In general, this might not be satisfied even if the transformations are well-behaved (e.g., bounded and Lipschitz). However, we show that for appropriate \(\mathcal{L}\), it is indeed satisfied for elliptical and semi-product distributions. For the sake of exposition, we focus here only on elliptical distributions. For such distributions, we use the loss function \(\mathcal{L}_{E}(v):=r^{2}\cdot\Phi\left(\left\|v\right\|/r\right)\) for some \(r>0\). It is not hard to see that it is globally smooth and locally strongly convex (in some neighborhood of the minimizer). Further, \(\nabla\mathcal{L}_{E}(v)\) is a projection of \(v\) onto the ball of radius \(r\) (with center at zero). The covariance of such a projection is bounded by the covariance of the projection onto the sphere of radius \(r\). Fortunately, spherical projections of elliptical distributions are well-behaved. In particular, they only depend on the scatter matrix \(\Sigma\), and hence we can without loss of generality assume that the initial elliptical distribution was Gaussian. For which we can obtain a bound on the covariance of \(\nabla\mathcal{L}_{E}\left(\bm{y}-\hat{\mu}_{2}\right)\).

Similarly, we show that under mild assumptions on the scatter matrix (that in particular are satisfied for scatter matrices with condition number \(O(1)\)), we can certify tight bounds on the moments of \(\nabla\mathcal{L}_{E}\left(\bm{y}-\hat{\mu}_{2}\right)\) in sum-of-squares, which leads to polynomial-time algorithms that obtain error \(o\left(\sqrt{\varepsilon}\right)\). We remark that prior to this work, in the unknown covariance case, robust mean estimation with error \(o\left(\sqrt{\varepsilon}\right)\) was only possible under the assumption that higher-order moments of \(D_{1}\) are certifiably bounded in sum-of-squares. We show that error \(o\left(\sqrt{\varepsilon}\right)\) is possible for arbitrary elliptical distributions that might not even have a first moment. Moreover, we show that by exploiting \(O(\log(1/\varepsilon))\) many bounded moments of the transformed distributions, we achieve the near optimal error of \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\) using quasi-polynomially many samples and time polynomial in the input.

The above approach also works for semi-product distributions by choosing \(\mathcal{L}\) to be the entry-wise Huber-loss. This works both for the setting when \(\nabla\mathcal{L}\left(\bm{y}-\hat{\mu}_{2}\right)\) has bounded covariance and certifiably bounded higher-order moments. We remark that in this case we can achieve the information theoretically optimal error \(O(\varepsilon)\) with quasi-polynomial number of samples in polynomial time (in the number of samples) - this again follows by exploiting \(O(\log(1/\varepsilon))\) many moments.

### Nearly optimal error for semi-product distributions using polynomially many samples

Note that even in the standard Gaussian case, using the standard filtering approach yields (nearly) optimal algorithms only when using quasi-polynomially many samples. Note that this seems somewhat inherent to the approach since it only uses low-degree moment information. To reduce this to polynomially many samples, we can use a stronger identifiability statement that exists for the standard Gaussian distribution. In particular, let \(D_{2}=N(\mu_{2},\mathrm{Id}_{d})\) and \(D_{1}\) be an \(\varepsilon\)-corruption of \(D_{1}\) with mean \(\mu_{1}\) and covariance \(\Sigma_{1}\). Then, it holds that [1, 1] (see also [11])

\[\left\|\mu_{1}-\mu_{2}\right\|\leqslant O\left(\varepsilon\sqrt{\log(1/ \varepsilon)}+\sqrt{\varepsilon\left(\left\|\Sigma_{1}-\mathrm{Id}_{d}\right\| +\varepsilon\log(1/\varepsilon)\right)}\right)\,.\]

Thus, we can hope to achieve error \(O(\varepsilon\sqrt{\log(1/\varepsilon)})\) by checking if \(\left\|\Sigma_{1}-\mathrm{Id}_{d}\right\|\leqslant O(\varepsilon\sqrt{\log(1/ \varepsilon)})\) and iteratively removing points aligned with the top eigenvector of \(\Sigma_{1}-\mathrm{Id}_{d}\). Indeed, a procedure very similar to this works (see e.g. [11]). However, it is crucial that we only remove points among the top \(\varepsilon\)-fraction of points correlated with the top eigenvector, since otherwise we cannot ensure that we remove more corrupted than uncorrupted points. The proof of the above uses _stability conditions_ of the Gaussian distribution (cf. [1]). That is, the proof uses that for i.i.d.samples \(\bm{\eta}_{1}^{*},\dots,\bm{\eta}_{n}^{*}\sim N(\mu^{*},\mathrm{Id}_{d})\) with \(n\) sufficiently large, it holds that for all subsets \(T\) of size \((1-10\varepsilon)n\) we have

\[\left\|\frac{1}{|T|}\sum_{i\in T}\bm{\eta}^{*}\right\|\leqslant O\left( \varepsilon\sqrt{\log(1/\varepsilon)}\right)\qquad\quad\text{and}\qquad\quad \left\|\frac{1}{|T|}\sum_{i\in T}\bm{\eta}^{*}\bm{\eta}^{*\top}-\mathrm{Id}_{ d}\right\|\leqslant O(\varepsilon\log(1/\varepsilon))\,.\]

We show that the above approach can be adapted to the semi-product setting. Indeed, let \(D_{2}\) be a semi-product distribution with location \(\mu^{*}\) and \(\Sigma_{2}^{\mathcal{L}}=\mathbb{E}_{\bm{y}\sim D_{2}}\nabla\mathcal{L}(\bm{y} -\mu^{*})(\nabla\mathcal{L}(\bm{y}-\mu^{*}))^{\top}\), where \(\mathcal{L}\) is the entry-wise Huber-loss. For simplicity assume that \(\Sigma_{2}^{\mathcal{L}}=\gamma\cdot\mathrm{Id}_{d}\) for some known \(\gamma\) (our approach extends to more general diagonal matrices and unknown \(\gamma\) as well - we show how to estimate all relevant parameters from the corrupted sample). Then, if \(D_{1}\) is an \(\varepsilon\)-corruption of \(D_{2}\), we show that for \(\hat{\mu}_{1}:=\arg\min_{a\in\mathbb{R}^{d}}\mathbb{E}_{\bm{y}\sim D_{1}} \mathcal{L}\left(\bm{y}-a\right)\) it holds that

\[\left\|\hat{\mu}_{1}-\mu^{*}\right\|\leqslant O\left(\varepsilon\sqrt{\log(1/ \varepsilon)}+\sqrt{\varepsilon\left(\left\|\Sigma_{1}^{\mathcal{L}}-\gamma \cdot\mathrm{Id}_{d}\right\|+\varepsilon\log(1/\varepsilon)\right)}\right)\,,\]where \(\Sigma_{1}^{\mathcal{L}}\) is defined analogously to \(\Sigma_{2}^{\mathcal{L}}\). Second, we show that for the filtering approach to work, it is enough that the _transformed distribution_, i.e., \(\nabla\mathcal{L}(\bm{y}-\mu^{*})\) for \(\bm{y}\sim D_{2}\), satisfies the stability condition. This indeed follows since for our choice of \(\mathcal{L}\) (the entry-wise Huber-loss), \(\nabla\mathcal{L}(\bm{y}-\mu^{*})\) is sub-Gaussian.

However, since we do not work with the quadratic loss anymore, there are several technical obstacles. For the quadratic loss, the gradient is the identity function, and this fact is extensively used in the analysis of the Gaussian setting. For the Huber-loss gradient, different arguments are needed. To exemplify this, consider the following example - note that we do not expect the reader to see at which point in the analysis this step is necessary, it should merely illustrate the types of problems that arise. Let \(S_{g}\) denote the set of uncorrupted samples, \(T\subseteq[n]\) be a set of size at least \((1-\varepsilon)n\), and \(\hat{\mu}(T)\) be the minimizer of \(\mathcal{L}\) over samples in \(T\). In the analysis, terms of the following nature arise

\[\frac{1}{\left|T\cap S_{g}\right|}\sum_{i\in T\cap S_{g}}\nabla\mathcal{L} \left(\hat{\mu}(T)-\bm{y}_{i}^{*}\right)\nabla\mathcal{L}\left(\hat{\mu}(T)- \bm{y}_{i}^{*}\right)^{\top}\]

and we need to show that this term is bounded from below, in Loewner order, by \((\gamma-O(\varepsilon\log\left(1/\varepsilon\right)))\mathrm{Id}_{d}\) (uniformly for all \(T\) of size at least \((1-\varepsilon)\,n\)). In case of the quadratic loss, this follows easily from the stability conditions by adding and subtracting \(\hat{\mu}(T\cap S_{g})\).

When \(\mathcal{L}\) is the entry-wise Huber-loss, a more sophisticated argument is required. To describe our argument, we assume for simplicity that the entries of \(\bm{\eta}\) are mutually independent. We first show that \(\hat{\Delta}\coloneqq\hat{\mu}\left(T\right)-\mu^{*}\) has entries of magnitude \(O(\varepsilon)\). Then we show that for arbitrary but fixed (that is, non-random) \(\Delta\) with entries of small magnitude, the distribution of \(\nabla\mathcal{L}\left(\Delta-\bm{\eta}^{*}\right)\) is sub-Gaussian. We use this fact to show that with overwhelming probability 6

Footnote 6: Note that the deviation of \(f\left(\Delta-\bm{\eta}^{*}\right)f\left(\Delta-\bm{\eta}^{*}\right)^{\top}\) from its mean scales with \(\mathbb{E}\,f\left(\Delta-\bm{\eta}^{*}\right)\), which can be large. But we can still show the lower bound of the form \(\left(1-\tilde{O}(\varepsilon)\right)\text{Cov}\left(f\left(\Delta-\bm{\eta} ^{*}\right)\right)\), where \(\tilde{O}(\varepsilon)\) does not contain the terms that scale with \(\mathbb{E}\,f\left(\Delta-\bm{\eta}^{*}\right)\). This is enough for our argument.

\[\frac{1}{\left|T\cap S_{g}\right|}\sum_{i\in T\cap S_{g}}\nabla\mathcal{L} \left(\Delta-\bm{\eta}_{i}^{*}\right)\nabla\mathcal{L}\left(\Delta-\bm{\eta} _{i}^{*}\right)^{\top}\succeq\left(1-\tilde{O}(\varepsilon)\right)\text{Cov} \left(\nabla\mathcal{L}\left(\Delta-\bm{\eta}^{*}\right)\right)\,.\]

Then we use the fact that \(\Delta\) has small entries to show that \(\text{Cov}\left(\nabla\mathcal{L}\left(\Delta-\bm{\eta}^{*}\right)\right) \succeq\left(\gamma-O(\varepsilon)\right)\mathrm{Id}_{d}\). Finally, we use an \(\varepsilon\)-net over all possible \(\Delta\) to get the desired lower bound uniformly for all \(\Delta\), including \(\hat{\Delta}\). This last \(\varepsilon\)-net argument is where we incur the (possibly) sub-optimal \(O\left(d\log d\right)\) term (instead of the optimal \(O\left(d\right)\)) in our sample complexity. Finally, if the entries of \(\bm{\eta}\) are not independent, we need to use this argument conditioned on the absolute values of the entries of \(\bm{\eta}\).

Summarizing, we have shown how to adjust the well-known filtering technique and incorporate the Huber-loss, to design robust algorithms for symmetric distributions, often matching what is known for the Gaussian distribution in (quasi-)polynomial time.

## Acknowledgments and Disclosure of Funding

This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 815464).

## References

* [Bec09] Ikhlef Bechar, _A bernstein-type inequality for stochastic processes of quadratic forms of gaussian variables_, arXiv preprint arXiv:0909.3595 (2009).
* [Ber06] Thorsten Bernholt, _Robust estimators are hard to compute_, Tech. report, Technical report, 2006.
* [CGR18] Mengjie Chen, Chao Gao, and Zhao Ren, _Robust covariance and scatter matrix estimation under huber's contamination model_, The Annals of Statistics **46** (2018), no. 5, 1932-1960.

* [Cha83] Gary Chamberlain, _A characterization of the distributions that imply mean--variance utility functions_, Journal of Economic Theory **29** (1983), no. 1, 185-201.
* [CHS81] Stamatis Cambanis, Steel Huang, and Gordon Simons, _On the theory of elliptically contoured distributions_, Journal of Multivariate Analysis **11** (1981), no. 3, 368-385.
* [CLMW11] Emmanuel J. Candes, Xiaodong Li, Yi Ma, and John Wright, _Robust principal component analysis?_, J. ACM **58** (2011), no. 3, 11:1-11:37.
* [CSV17] Moses Charikar, Jacob Steinhardt, and Gregory Valiant, _Learning from untrusted data_, STOC, ACM, 2017, pp. 47-60.
* [CTBJ22] Yeshwanth Cherapanamjeri, Nilesh Tripuraneni, Peter Bartlett, and Michael Jordan, _Optimal mean estimation without a variance_, Conference on Learning Theory, PMLR, 2022, pp. 356-357.
* [DHL19] Yihe Dong, Samuel Hopkins, and Jerry Li, _Quantum entropy scoring for fast robust mean estimation and improved outlier detection_, Advances in Neural Information Processing Systems **32** (2019).
* [DK22] Ilias Diakonikolas and Daniel Kane, _Algorithmic high-dimensional robust statistics_, http://www.iliasdiakonikolas.org/ars-book.pdf, 2022.
* [DKK\({}^{+}\)17] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart, _Being robust (in high dimensions) can be practical_, International Conference on Machine Learning, PMLR, 2017, pp. 999-1008.
* [DKK\({}^{+}\)19] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart, _Robust estimators in high-dimensions without the computational intractability_, SIAM Journal on Computing **48** (2019), no. 2, 742-864.
* [DKLP22] Ilias Diakonikolas, Daniel Kane, Jasper Lee, and Ankit Pensia, _Outlier-robust sparse mean estimation for heavy-tailed distributions_, Advances in Neural Information Processing Systems **35** (2022), 5164-5177.
* [DKP20] Ilias Diakonikolas, Daniel M Kane, and Ankit Pensia, _Outlier robust mean estimation with subgaussian rates via stability_, Advances in Neural Information Processing Systems **33** (2020), 1830-1840.
* [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart, _Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures_, 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2017, pp. 73-84.
* [DKS19] Ilias Diakonikolas, Weihao Kong, and Alistair Stewart, _Efficient algorithms and lower bounds for robust linear regression_, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, 2019, pp. 2745-2754.
* [dLN\({}^{+}\)21] Tommaso d'Orsi, Chih-Hung Liu, Rajai Nasser, Gleb Novikov, David Steurer, and Stefan Tiegel, _Consistent estimation for pca and sparse regression with oblivious outliers_, Advances in Neural Information Processing Systems **34** (2021), 25427-25438.
* [dNNNS23] Tommaso d'Orsi, Rajai Nasser, Gleb Novikov, and David Steurer, _Higher degree sum-of-squares relaxations robust against oblivious outliers_, Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2023, pp. 3513-3550.
* [dNS21] Tommaso d'Orsi, Gleb Novikov, and David Steurer, _Consistent regression when oblivious outliers overwhelm_, International Conference on Machine Learning, PMLR, 2021, pp. 2297-2306.
* [DTV16] Alexander Durre, David E. Tyler, and Daniel Vogel, _On the eigenvalues of the spatial sign covariance matrix in more than two dimensions_, Statistics & Probability Letters **111** (2016), no. C, 80-85.

* [Fan18] Kai Wang Fang, _Symmetric multivariate and related distributions_, Chapman and Hall/CRC, 2018.
* [Fra04] Gabriel Frahm, _Generalized elliptical distributions: theory and applications_, Ph.D. thesis, Universitat zu Koln, 2004.
* [GLP23] Shivam Gupta, Jasper CH Lee, and Eric Price, _Finite-sample symmetric mean estimation with fisher information rate_, The Thirty Sixth Annual Conference on Learning Theory, PMLR, 2023, pp. 4777-4830.
* [HL18] Samuel B Hopkins and Jerry Li, _Mixture models, robustness, and sum of squares proofs_, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018, pp. 1021-1034.
* [HL19], _How hard is robust mean estimation?_, Conference on Learning Theory, PMLR, 2019, pp. 1649-1682.
* [HLZ20] Sam Hopkins, Jerry Li, and Fred Zhang, _Robust and heavy-tailed mean estimation made simple, via regret minimization_, Advances in Neural Information Processing Systems **33** (2020), 11902-11912.
* [Hub11] Peter J Huber, _Robust statistics_, International encyclopedia of statistical science, Springer, 2011, pp. 1248-1251.
* [Kel70] Douglas Kelker, _Distribution theory of spherical distributions and a location-scale parameter generalization_, Sankhya: The Indian Journal of Statistics, Series A (1970), 419-430.
* [KMZ22] Pravesh Kothari, Peter Manohar, and Brian Hu Zhang, _Polynomial-time sum-of-squares can robustly estimate mean and covariance of gaussians optimally_, International Conference on Algorithmic Learning Theory, PMLR, 2022, pp. 638-667.
* [KS17a] Pravesh K. Kothari and David Steurer, _Outlier-robust moment-estimation via sum-of-squares_, CoRR **abs/1711.11581** (2017).
* [KS17b], _Outlier-robust moment-estimation via sum-of-squares_, CoRR **abs/1711.11581** (2017).
* [KSS18] Pravesh K Kothari, Jacob Steinhardt, and David Steurer, _Robust moment estimation and improved clustering via sum of squares_, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018, pp. 1035-1046.
* [Li19] Jerry Li, _Robustness in machine learning, lecture notes_, https://jerryzli.github.io/robust-ml-fall19.html, 2019, Lecture 5.
* [Lin75] John Lintner, _The valuation of risk assets and the selection of risky investments in stock portfolios and capital budgets_, Stochastic optimization models in finance, Elsevier, 1975, pp. 131-155.
* [LRV16] Kevin A Lai, Anup B Rao, and Santosh Vempala, _Agnostic estimation of mean and covariance_, 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2016, pp. 665-674.
* [MSS16] Tengyu Ma, Jonathan Shi, and David Steurer, _Polynomial-time tensor decompositions with sum-of-squares_, FOCS, IEEE Computer Society, 2016, pp. 438-446.
* [MZ23] Arshak Minasyan and Nikita Zhivotovskiy, _Statistically optimal robust mean and covariance estimation for anisotropic gaussians_, arXiv preprint arXiv:2301.09024 (2023).
* [OR83] Joel Owen and Ramon Rabinovitch, _On the class of elliptical distributions and their applications to the theory of portfolio choice_, The Journal of Finance **38** (1983), no. 3, 745-752.

* [PBR20] Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar, _A robust univariate mean estimator is all you need_, International Conference on Artificial Intelligence and Statistics, PMLR, 2020, pp. 4034-4044.
* [PJL20] Ankit Pensia, Varun Jog, and Po-Ling Loh, _Robust regression with covariate filtering: Heavy tails and adversarial contamination_, arXiv preprint arXiv:2009.12976 (2020).
* [RHRS11] Peter J Rousseeuw, Frank R Hampel, Elvezio M Ronchetti, and Werner A Stahel, _Robust statistics: the approach based on influence functions_, John Wiley & Sons, 2011.
* [ST21] David Steurer and Stefan Tiegel, _Sos degree reduction with applications to clustering and robust moment estimation_, Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2021, pp. 374-393.
* [Sto75] Charles J Stone, _Adaptive maximum likelihood estimators of a location parameter_, The annals of Statistics (1975), 267-284.
* [SZF20] Qiang Sun, Wen-Xin Zhou, and Jianqing Fan, _Adaptive huber regression_, Journal of the American Statistical Association **115** (2020), no. 529, 254-265.
* [TJSO14] Efthymios Tsakonas, Joakim Jalden, Nicholas D Sidiropoulos, and Bjorn Ottersten, _Convergence of the huber regression m-estimate in the presence of dense outliers_, IEEE Signal Processing Letters **21** (2014), no. 10, 1211-1214.
* [Tro15] Joel A. Tropp, _An introduction to matrix concentration inequalities_, Foundations and Trends in Machine Learning **8** (2015), no. 1-2, 1-230.
* [Wai19] Martin J. Wainwright, _High-dimensional statistics: A non-asymptotic viewpoint_, Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press, 2019.

Overview of Appendices

We will provide a general framework underlying the proofs of Theorem 1.5 and 1.6 in Appendices C to E. We will instantiate it to prove (a slight generalization of) Theorem 1.5 in Appendix F and to prove Theorem 1.6 in Appendix G. We will prove Theorem 1.4 in Appendices H and I. Appendix H contains the proof of identifiability and Appendix I contains the adaptation of the filtering algorithm.

As alluded to in the introduction, we will consider the following (slight) generalization of \((\rho)\)-semi-product distributions (Definition 1.2) which allows for even less probability mass around the location. Setting \(\alpha=\frac{1}{100}\) recovers Definition 1.2.

**Definition A.1** (Semi-Product Distributions (generalized form)).: Let \(\rho,\alpha>0\). We say a distribution \(\mathcal{D}\) over \(\mathbb{R}^{d}\) is an _\((\alpha,\rho)\)-semi-product_ distribution, if for \(\boldsymbol{\eta}\sim\mathcal{D}\) it holds that

1. For all \(j\in[d]\), the distribution of \(\boldsymbol{\eta}_{j}\) is symmetric about 0,
2. For all \(j\in[d]\), \(\mathbb{P}\left(|\boldsymbol{\eta}_{j}|\leqslant\rho\right)\geqslant\alpha\),
3. The random vectors \(\left(\operatorname{sign}(\boldsymbol{\eta}_{j})\right)_{j=1}^{d}\) and \(\left(|\boldsymbol{\eta}_{j}|\right)_{j=1}^{d}\) are independent, and the random variables \(\operatorname{sign}\left(\boldsymbol{\eta}_{1}\right),\ldots,\operatorname {sign}\left(\boldsymbol{\eta}_{d}\right)\) are mutually independent.

We will prove the following two (slight) generalizations of Theorems 1.4 and 1.5. Setting \(\alpha=\frac{1}{100}\) recovers Theorems 1.4 and 1.5, respectively.

**Theorem A.2**.: _Let \(\mu^{*}\in\mathbb{R}^{d},\varepsilon,\rho,\alpha>0\) and \(\mathcal{D}\) be a \((\alpha,\rho)\)-semi-product distribution with location \(\mu^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \(\sqrt[d]{\varepsilon}\leqslant\alpha/C\) and \(n\geqslant C\cdot\frac{d\log(d)+\log(1/\delta)}{\alpha^{4}\varepsilon^{2}\log (1/\varepsilon)}\). Then, there exists an algorithm that, given an \(\varepsilon\)-corrupted sample from \(\mathcal{D}\) and \(\alpha\), runs in time \(n^{O(1)}\) and outputs \(\hat{\mu}\in\mathbb{R}^{d}\) such that with probability at least \(1-\delta\) it holds that_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\rho\cdot\left[\sqrt{\frac{d\log(d)+ \log(1/\delta)}{\alpha^{4}n}+\frac{\varepsilon}{\alpha^{3}}}\sqrt{\log(1/ \varepsilon)}\right]\right)\,.\]

**Theorem A.3**.: _Let \(\mu^{*}\in\mathbb{R}^{d},\varepsilon,\alpha,\rho>0\) and \(\mathcal{D}\) be a \((\alpha,\rho)\)-semi-product distribution with location \(\mu^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \(\varepsilon\leqslant\alpha/C\) and \(n\geqslant\frac{C}{\alpha^{4}}\cdot d^{C\log(1/\varepsilon)}\). Then, there exists an algorithm that, given an \(\varepsilon\)-corrupted sample from \(\mathcal{D}\) and \(\alpha\), runs in time \(n^{O(1)}\) and outputs \(\hat{\mu}\in\mathbb{R}^{d}\) such that with probability at least \(1-\delta\) it holds that_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\rho\cdot\left[\sqrt{\frac{d+\log(1/ \delta)}{\alpha^{4}n}+\frac{\varepsilon}{\alpha^{2}}}\right]\right)\,.\]

We remark that if, the algorithm receives \(\rho\) as input, the error guarantee improves to

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\rho\cdot\left[\sqrt{\frac{d+\log(1/ \delta)}{\alpha^{2}n}+\frac{\varepsilon}{\alpha}}\right]\right)\,.\]

Note that for the setting of \(\alpha=1/100\) discussed in the introduction, both guarantees are the same (up to constant factors). Further, if \(\rho\) is given as input, it is not necessary to receive \(\alpha\) as input. We emphasize, that \(\rho\) need not be known, and can be estimated from the corrupted samples (given \(\alpha\)).

We also remark, that our upper bound on the fraction of corruptions is qualitatively tight, since \(\varepsilon\) can be at most roughly \(\alpha\): Consider a mixture distribution that with probability \(1-\alpha\) outputs a sample from \(N(\mu^{*},\sigma^{2}\mathrm{Id}_{d})\) for \(\sigma^{2}\) arbitrarily large and with probability \(\alpha\) outputs a sample from \(N(\mu^{*},\mathrm{Id}_{d})\). This is \((\alpha,\Theta(1))\)-semi-product. Yet, if an adversary changes the \(N(\mu^{*},\mathrm{Id}_{d})\) component to \(N(\mu^{*},\sigma^{2}\mathrm{Id}_{d})\), we cannot hope to achieve error better than \(\sigma^{2}\sqrt{\frac{d+\log(1/\delta)}{n}}\), which can be arbitrarily large. We did not attempt to optimize the range of \(\varepsilon\) allowed for our algorithms.

## Appendix B Preliminaries

NotationWe use bold-font for random variables. We use regular (non-bold-font) for random variables that potentially have been adversarially corrupted. Further, we use \(\gtrsim\) and \(\lesssim\) to suppress absolute constants which do not depend on any other problem parameters. For a (pseudo)-distribution \(\mu\), we denote by \(\operatorname{supp}(\mu)\) its support.

Sum-of-Squares Proofs and Pseudo-ExpectationsIn this section, we will introduce sum-of-squares proofs and their convex duals, so-called pseudo-distributions.

Let \(p\colon\mathbb{R}^{n}\to\mathbb{R}\) be a polynomial in formal variables \(X=(X_{1},\ldots,X_{n})\). We say the inequality \(p(X)\geqslant 0\) has a sum-of-squares proof (short SoS proof) if we can write \(p(X)\) as a sum of squares in \(X_{1},\ldots,X_{n}\). Further, if every polynomial in this decomposition has degree at most \(t\), we say that the sum-of-squares proof has _degree-\(t\)_ and write \(\left\lfloor\frac{X}{t}\,p\geqslant 0\right\). We write \(\mathcal{A}\left\lfloor\frac{X}{t}\,p\geqslant p^{\prime}\right\) if \(\mathcal{A}\left\lfloor\frac{X}{t}\,p-p^{\prime}\geqslant 0\right.\) and \(\mathcal{A}\left\lfloor\frac{X}{t}\,p=p^{\prime}\right\) if \(\mathcal{A}\left\lfloor\frac{X}{t}\,p\leqslant p^{\prime}\right.\). It is not hard to verify that the following composition rule holds: If \(\left\lfloor\frac{X}{t}\,p\geqslant p^{\prime}\right.\) and \(\left\lfloor\frac{X}{t^{\prime}}\,p^{\prime}\geqslant p^{\prime\prime}\right.\), then it also holds that \(\left\lfloor\frac{X}{t^{\prime\prime}}\,p\geqslant p^{\prime\prime}\right.\), where \(t^{\prime\prime}=\max\{t,t^{\prime}\}\).

Next, we introduce so-called pseudo-distributions, the convex duals of sum-of-squares proofs: For \(d\in\mathbb{N}_{\geqslant 1}\), a _degree-\(d\) pseudo-distribution_ is a finitely-supported function \(\mu\colon\mathbb{R}^{n}\to\mathbb{R}\) such that \(\sum_{x\in\text{supp}(\mu)}\mu(x)=1\) and \(\sum_{x\in\text{supp}(\mu)}\mu(x)f^{2}(x)\geqslant 0\) for all polynomials \(f\) of degree at most \(d/2\). The _pseudo-expectation_ corresponding to a pseudo-distribution \(\mu\) is defined as the linear operator mapping a degree-\(d\) polynomial \(f\) to \(\tilde{\mathbb{E}}_{\mu}f\coloneqq\sum_{x\in\text{supp}(\mu)}\mu(x)f(x)\). We say that a pseudo-distiribution \(\tilde{\mathbb{E}}_{\mu}\) satisfies a set of inequalities \(\{q_{1}\geqslant 0,\ldots,q_{m}\geqslant 0\}\) at degree \(r\), if for all \(S\subseteq[m]\) and \(h\) a sum-of-squares polynomial such that \(\deg\left(h\cdot\Pi_{j\in S}\right)\leqslant r\) it holds that

\[\tilde{\mathbb{E}}_{\mu}h\cdot\Pi_{j\in S}\geqslant 0\,.\]

We relate pseudo-distributions and sum-of-squares proofs using the following facts.

**Fact B.1**.: _Let \(\mu\) be a degree-\(d\) pseudo-distribution that satisfies \(\mathcal{A}\) and suppose that \(\mathcal{A}\left\lfloor\frac{X}{t}\,p\geqslant 0\right.\). Let \(h\) be an arbitrary sum-of-squares polynomial, if \(\deg h+t\leqslant d\) we have \(\tilde{\mathbb{E}}_{\mu}h\cdot p\geqslant 0\). In particular, we have \(\tilde{\mathbb{E}}_{\mu}p\geqslant 0\) as long as \(t\leqslant d\). If \(\mu\) only approximately satisfies \(\mathcal{A}\) it holds that \(\tilde{\mathbb{E}}_{\mu}p\geqslant-\eta\|h\|_{2}\) for \(\eta=2^{-n^{\Omega(d)}}\). Further, if there is no degree-\(d\) sum-of-squares proof that \(\mathcal{A}\left\lfloor\frac{X}{t}\,p\geqslant 0\right.\), then for there exists a degree-\(d\) pseudo-distribution \(\tilde{\mathbb{E}}_{\mu}\) satisfying \(\mathcal{A}\) and \(\varepsilon>0\) such that \(\tilde{\mathbb{E}}_{\mu}p<0\)._

An essential fact is that pseudo-distributions approximately satisfying a system of polynomial inequalities can be found efficiently as long as the constraints have bit-complexity at most \((m+n)^{\mathcal{O}(1)}\).

**Fact B.2**.: _Given any feasible system of polynomial constraints \(\mathcal{A}=\{q_{1}\geqslant 0,\ldots,q_{m}\geqslant 0\}\)7 over \(\mathbb{R}^{n}\) whose bit-complexity satisfies the constraints mentioned above and \(\eta=2^{-n^{\Theta(d)}}\) we can in time \((n+m)^{\mathcal{O}(d)}\) find a degree-\(d\) pseudo-distribution that satisfies \(\mathcal{A}\) up to error \(\eta\). 8_

Footnote 7: For technical reason we also assume that \(\mathcal{A}\) is _explicitly bounded_ meaning it contains a constraint of the form \(\|x\|_{2}^{2}\leqslant B\) for some large number \(B\). Usually we can even take this to be at least polynomial in our variables. We remark that for all the problems we are considering this can be added without changing our proofs.

Footnote 8: The positivity and normalization constraint can be satisfied exactly however.

## Appendix C Proof of Identifiability based on Transformed Moments

In this section, we give a formal proof of the statement of identifiability based on transformed moments that we sketched in the Techniques section. We will give the proof assuming an abstract set of assumptions and verify that they indeed hold for semi-product and elliptical distributions in Appendices F and G.

We use the same notation as in Appendix H, with the only difference being that we now use an abstract loss function: Let \(F\colon\mathbb{R}^{d}\to\mathbb{R}\) and denote by \(f=\nabla F\) its gradient. Further assume that \(F\) and \(f\) are such that \(f\) is \(1\)-Lipschitz. For \(w\in\mathbb{R}^{n}\) with non-negative entries, we define \(\mathcal{L}^{w}\colon\mathbb{R}^{d}\to\mathbb{R}\) as

\[\mathcal{L}^{w}(\mu)\coloneqq\sum_{i=1}^{n}w_{i}F\left(\mu-y_{i}\right)\,.\]

It follows that \(\nabla\mathcal{L}^{w}(\mu)=\sum_{i=1}^{n}w_{i}f\left(\mu-y_{i}\right)\). Let \(2k\in\mathbb{N}_{\geqslant 2}\) and define \(\hat{\mu}(w)\coloneqq\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w}\left(\mu\right)\) as well as

\[\mathbf{\tilde{m}}_{2k}\coloneqq\max_{v\in\mathbb{S}^{d-1}}\left[\frac{1}{n} \sum_{i=1}^{n}\left\langle f\left(\boldsymbol{\eta}_{i}^{*}\right),v\right\rangle ^{2k}\right]^{\frac{1}{2k}}\quad\text{and}\quad\mathbf{\hat{m}}_{2k}^{w} \coloneqq\max_{v\in\mathbb{S}^{d-1}}\left[\sum_{i=1}^{n}w_{i}\left\langle f \left(\hat{\mu}-y_{i}\right),v\right\rangle^{2k}\right]^{\frac{1}{2k}}\,.\]We will prove the following theorem.

**Theorem C.1**.: _Let \(\alpha\in(0,1)\) and let \(k\) be a positive integer and \(w\in\mathcal{W}_{2\varepsilon}\). Suppose that \(\varepsilon^{1-\frac{1}{2k}}\leqslant 0.01\cdot\alpha\) and_

\[\alpha\|\hat{\mu}^{w}-\mu^{*}\|^{2}\leqslant 10\langle\nabla\mathcal{L}^{w} \left(\mu^{*}\right),\mu^{*}-\hat{\mu}^{w}\rangle\,.\]

_Then_

\[\|\hat{\mu}^{w}-\mu^{*}\|\leqslant\frac{100}{\alpha}\cdot\left(\left\|\frac{1 }{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|+\varepsilon^{1-\frac {1}{2k}}\cdot\left(\tilde{\mathbf{m}}_{2k}^{w}+\bar{\mathbf{m}}_{2k}\right) \right)\,.\]

This theorem easily follows from the following lemma.

**Lemma C.2**.: _Let \(k\) be a positive integer and \(w\in\mathcal{W}_{2\varepsilon}\). Then_

\[\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\rangle\leqslant 5\cdot \left\|u\right\|\cdot\left(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{ i}^{*}\right)\right\|+\varepsilon^{1-\frac{1}{2k}}\cdot\left(\tilde{ \mathbf{m}}_{2k}^{w}+\bar{\mathbf{m}}_{2k}+\left\|u\right\|\right)\right),\]

_where \(u=\hat{\mu}(w)-\mu^{*}\)._

We first give the proof of Theorem C.1.

Proof of Theorem c.1.: Let \(u=\hat{\mu}(w)-\mu^{*}\). Using Lemma C.2 and \(\varepsilon^{1-\frac{1}{2k}}\leqslant 0.01\cdot\alpha\), cancelling the common factor of \(\left\|u\right\|\) and rearranging now yields

\[\left\|u\right\|\leqslant\frac{100}{\alpha}\cdot\left(\left\|\frac{1}{n}\sum_ {i=1}^{n}f\left(\bm{\eta}_{i}\right)\right\|+\varepsilon^{1-\frac{1}{2k}} \cdot\left(\tilde{\mathbf{m}}_{2k}^{w}+\bar{\mathbf{m}}_{2k}\right)\right)\,.\]

Next, we prove Lemma C.2.

Proof of Lemma c.2.: Again, let \(u=\hat{\mu}(w)-\mu^{*}\). We can decompose \(\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\rangle\) as

\[\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\rangle =\sum_{i=1}^{n}w_{i}\left\langle f\left(\mu^{*}-y_{i}\right),u\right\rangle\] \[=\sum_{i=1}^{n}w_{i}\left\langle f\left(\mu^{*}-\bm{y}_{i}^{*} \right),u\right\rangle+\sum_{i=1}^{n}\mathbf{1}\left\{i\in S_{b}\right\}\cdot w _{i}\left\langle f\left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\bm{y}_{i}^{*} \right),u\right\rangle\,.\]

For the first sum,

\[\sum_{i=1}^{n}w_{i}\left\langle f\left(\mu^{*}-\bm{y}_{i}^{*} \right),u\right\rangle =\sum_{i=1}^{n}w_{i}\left\langle f\left(\bm{\eta}_{i}^{*}\right),u\right\rangle\leqslant\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^ {*}\right)\right\|\cdot\|u\|+\sum_{i=1}^{n}\left(w_{i}-\frac{1}{n}\right) \left\langle f\left(\bm{\eta}_{i}^{*}\right),u\right\rangle\,.\]

By Holder's Inequality,

\[\sum_{i=1}^{n}\left(w_{i}-\frac{1}{n}\right)\left\langle f\left( \bm{\eta}_{i}^{*}\right),u\right\rangle \leqslant\left(\sum_{i=1}^{n}\left|w_{i}-\frac{1}{n}\right| \right)^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n}\left|w_{i}-\frac{1}{n}\right| \cdot\left\langle f\left(\bm{\eta}_{i}^{*}\right),u\right\rangle^{2k}\right)^ {\frac{1}{2k}}\] \[\leqslant 4\cdot\varepsilon^{1-\frac{1}{2k}}\cdot\bar{\mathbf{m}}_{2k} \cdot\left\|u\right\|.\]

For the second sum, we bound using Holder's Inequality

\[\sum_{i=1}^{n}\mathbf{1}\left\{i\in S_{b}\right\}\cdot w_{i} \cdot\left\langle f\left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\bm{y}_{i}^{*} \right),u\right\rangle\leqslant\varepsilon^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n} w_{i}\cdot\left\langle f\left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\bm{y}_{i}^{*} \right),u\right\rangle^{2k}\right)^{\frac{1}{2k}}\] \[=\varepsilon^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n}w_{i}\cdot\left \langle f\left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\bm{y}_{i}^{*}\right)-f \left(\hat{\mu}-y_{i}\right)+f\left(\hat{\mu}-y_{i}\right),u\right\rangle^{2k} \right)^{\frac{1}{2k}}\]\[\leqslant\varepsilon^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n}w_{i} \left\langle f\left(\bm{\eta}_{i}^{*}\right),u\right\rangle^{2k}\right)^{\frac{ 1}{2k}}+\varepsilon^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n}w_{i}\left\langle f \left(\hat{\mu}-y_{i}\right),u\right\rangle^{2k}\right)^{\frac{1}{2k}}\\ +\varepsilon^{1-\frac{1}{2k}}\left(\sum_{i=1}^{n}w_{i}\left\langle f \left(\mu^{*}-y_{i}\right)-f\left(\hat{\mu}-y_{i}\right),u\right\rangle^{2k} \right)^{\frac{1}{2k}}\,.\]

We bound the three terms in the square bracket one by one.

The first term can be bounded as follows

\[\sum_{i=1}^{n}w_{i}\left\langle f\left(\bm{\eta}_{i}^{*}\right),u\right\rangle ^{2k}\leqslant\sum_{i=1}^{n}\tfrac{1}{n}\left\langle f\left(\bm{\eta}_{i}^{*} \right),u\right\rangle^{2k}\leqslant\bar{\bm{\mathfrak{m}}}_{2k}^{2k}\|u\|^{ 2k}\,.\]

The second one is at most \(\bar{\bm{\mathfrak{m}}}_{2k}\|u\|\) by definition of \(\bar{\bm{\mathfrak{m}}}_{2k}^{w}\). For the third, we use that \(f\) is \(1\)-Lipschitz to bound

\[\left(\sum_{i=1}^{n}w_{i}\cdot\left\langle f\left(\mu^{*}-y_{i}\right)-f\left( \hat{\mu}-y_{i}\right),u\right\rangle^{2k}\right)^{\frac{1}{2k}}\leqslant \left(\sum_{i=1}^{n}w_{i}\cdot\left\|f\left(\mu^{*}-y_{i}\right)-f\left(\hat{ \mu}-y_{i}\right)\right\|^{2k}\left\|u\right\|^{2k}\right)^{\frac{1}{2k}} \leqslant\left\|u\right\|^{2}\,.\]

It follows that

\[\sum_{i=1}^{n}\bm{1}\left\{i\in S_{b}\right\}\cdot w_{i}\cdot\left\langle f \left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\bm{y}_{i}^{*}\right),u\right\rangle \leqslant\varepsilon^{1-\frac{1}{2k}}\left\|u\right\|\cdot\left(\hat{\bm{ \mathfrak{m}}}_{2k}^{w}+\bar{\bm{\mathfrak{m}}}_{2k}+\left\|u\right\|\right).\]

Putting all the above together, we obtain the desired bound. 

## Appendix D Filtering Algorithm for Bounded Clipped Covariance

In this section, we show that the filtering algorithm for robust mean estimation under moment constraints (cf. for example [14, 15]) can be adapted to also yield an efficient algorithm for robustly estimating the location parameter of symmetric distributions. For now, we focus on the case that \(k=2\), we discuss the extension to higher moments in Appendix E. Again, we will focus on the abstract setting and show that we can apply these results to semi-product and elliptical distributions. We will closely follow the exposition in [14, 15].

Recall that we denote \(\mathcal{L}^{w}(\mu)=\sum_{i=1}^{n}w_{i}F\left(\mu-y_{i}\right)\).

**Assumption D.1** (Goodness Condition).: Let \(F:\mathbb{R}^{d}\to\mathbb{R},f=\nabla F\) be such that \(f\) is \(1\)-Lipschitz. Let \(\mu^{*}\in\mathbb{R}^{d},0<\sigma,0<\alpha<1\) and \(y_{1},\ldots,y_{n}\in\mathbb{R}^{d}\). Let \(D\) be a distribution over \(\mathbb{R}^{d}\) and \(\bm{\eta}_{1}^{*},\ldots,\bm{\eta}_{n}^{*}\) be \(n\) i.i.d.samples from \(D\). We say that the _goodness condition_ holds, if

1. \(\alpha\|\hat{\mu}^{w}-\mu^{*}\|^{2}\leqslant 10\left\langle\nabla\mathcal{L}^{w} \left(\mu^{*}\right),\mu^{*}-\hat{\mu}^{w}\right\rangle\),
2. \(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\| \leqslant\frac{\alpha}{100}\sigma\),
3. \(\bar{\bm{\mathfrak{m}}}_{2}=\max_{v\in\mathbb{S}^{d-1}}\sqrt{\frac{1}{n}\sum_ {i=1}^{n}\left\langle f\left(\bm{\eta}_{i}^{*}\right),v\right\rangle^{2}} \leqslant\sigma\).

Under this condition we can show the following theorem.

**Theorem D.2**.: _Let \(\mu^{*}\in\mathbb{R}^{d},0<\sigma,0<\alpha<1,\varepsilon>0\), and \(\bm{\eta}_{1}^{*},\ldots,\bm{\eta}_{n}^{*}\) be \(n\) i.i.d.samples from some distribution \(D\) over \(\mathbb{R}^{d}\). Assume that \(\sqrt{\varepsilon}\leqslant\frac{\alpha}{1000}\). Let \(y_{1},\ldots,y_{n}\) be an \(\varepsilon\)-corruption of \(\bm{y}_{1}^{*}=\mu^{*}+\bm{\eta}_{1}^{*},\ldots,\bm{y}_{n}^{*}=\mu^{*}+\bm{ \eta}_{n}^{*}\). Assume Assumption D.1 (the goodness condition) holds, then Algorithm D.3, given \(y_{1},\ldots,y_{n}\) and \(\sigma\), terminates after at most \(2\varepsilon n+1\) iterations and computes \(\hat{\mu}\) such that_

\[\|\hat{\mu}^{w}-\mu^{*}\|\leqslant O\left(\frac{\left\|\frac{1}{n}\sum_{i=1}^{ n}f\left(\bm{\eta}_{i}^{*}\right)\right\|}{\alpha}+\frac{\sqrt{\varepsilon}}{ \alpha}\cdot\sigma\right)\,.\]

_Moreover each iteration can be implemented in time \(n^{O(1)}d^{O(1)}\)._To describe the algorithm, let \(C>0\) be some universal constant and for \(\mu\in\mathbb{R}^{d},w\in\mathcal{W}_{\varepsilon}\) let

\[\hat{\mu}(w) =\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w}\left(\mu\right)\,,\] \[\Sigma_{f}\left(w\right) =\sum_{i=1}^{n}w_{i}f\left(\hat{\mu}^{w}-y_{i}\right)f\left(\hat{ \mu}^{w}-y_{i}\right)^{\top}\,.\]

We will use the following Filtering Algorithm:

``` Algorithm D.3 (Filtering Algorithm). Input:\(\varepsilon\)-corrupted sample \(y_{1},\ldots,y_{n}\) and \(\sigma>0\). Output: Location estimate \(\hat{\mu}\). * Let \(w^{(0)}=\frac{1}{n}\mathscr{K}_{n}\). * Compute \(\hat{\mu}^{(0)}=\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w^{(0)}}\left(\mu\right)\) and \(\Sigma_{f}^{(0)}=\Sigma_{f}\left(w^{(0)}\right)\). * Let \(t=0\). * while\(\left\|\Sigma_{f}^{(t)}\right\|>100\cdot\sigma\)do * Compute \(v^{(t)}\) the top eigenvector of \(\Sigma_{f}^{(t)}\). * For \(i\in[n]\), compute \(\tau_{i}^{(t)}=\left\langle v^{(t)},f\left(\hat{\mu}^{(t)}-y_{i}\right) \right\rangle^{2}\). * For \(i\in[n]\), set \(w_{i}^{(t+1)}=\left(1-\frac{\tau_{i}}{\tau_{\max}}\right)w_{i}^{(t)}\), where \(\tau_{\max}=\max_{i\in[n]}\tau_{i}\). * Compute \(\hat{\mu}^{(t+1)}=\min_{\mu\in\mathbb{R}^{d}}fw^{(t+1)}\left(\mu\right)\) and \(\Sigma_{f}^{(t+1)}=\Sigma_{f}\left(w^{(t+1)}\right).\) * \(t\gets t+1\). * Output \(\hat{\mu}^{(t)}\). ```

**Algorithm D.2** \(\|\Sigma_{f}^{(t)}\|>100\cdot\sigma\)

The proof of Theorem D.2 is very similar to the one in [11]. We will use the following lemma whose proof we will give at the end of this section

**Lemma D.4**.: _Assume that Theorem D.2\(\|\Sigma_{f}^{(t)}\|>100\cdot\sigma\) and_

\[\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(t)}\right)<\sum_{i\in S_{b}}\left( \frac{1}{n}-w_{i}^{(t)}\right)\,.\]

_Then_

\[\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(t+1)}\right)<\sum_{i\in S_{b}} \left(\frac{1}{n}-w_{i}^{(t+1)}\right)\,.\]

With this in hand, we will prove Theorem D.2.

Proof of Theorem D.2.: First note, that every iteration can clearly be implented to run in time \(n^{O(1)}d^{O(1)}\). First, we will show that the algorithm terminates after at most \(\lceil 2\varepsilon n\rceil\) iterations. We will prove it by contradiction. Suppose that the algorithm does not terminate after \(T=\lceil 2\varepsilon n\rceil\) iterations. Note that the number of zero entries of \(w^{(t)}\) increases by at least 1 in every iteration. Hence, after \(T\) iterations we have set at least \(\varepsilon n\) entries of \(w\) to zero whose index lies in \(S_{g}\). By assumption that the algorithm didn't terminate and Lemma D.4 it holds that

\[\varepsilon\leqslant\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(T)}\right)< \sum_{i\in S_{b}}\left(\frac{1}{n}-w_{i}^{(T)}\right)\leqslant\frac{|S_{b}|} {n}\leqslant\varepsilon\,.\]

which is a contradiction.

Next, we prove the correctness of the algorithm. Let \(T\) be the index of the last iteration of the algorithm before termination. Note that by our invariant

\[\left\|\frac{1}{n}-w^{(T)}\right\|_{1}=\sum_{i\in S_{g}}\frac{1}{n}-w_{i}^{(T) }+\sum_{i\in S_{b}}\frac{1}{n}-w_{i}^{(T)}<2\sum_{i\in S_{b}}\frac{1}{n}-w_{i} ^{(T)}\leqslant 2\varepsilon\,.\]Since also \(0\leqslant w^{(T)}\leqslant\frac{1}{n}\), it follows that \(w^{(T)}\in\mathcal{W}_{2\varepsilon}\). By Theorem C.1 and since \(\left\|\Sigma_{f}^{(T)}\right\|\leqslant 100\cdot\sigma\) it follows that

\[\left\|\hat{\mu}^{w}-\mu^{*}\right\|\leqslant O\left(\frac{\left\|\frac{1}{n} \sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|}{\alpha}+\frac{\sqrt{ \varepsilon}}{\alpha}\cdot\sigma\right)\,.\]

Lastly, we will prove Lemma D.4.

Proof of Lemma d.4.: For simplicity, let \(w=w^{(t)}\) and \(w^{\prime}=w^{(t+1)}\). Note, that it is enough to show that

\[\sum_{i\in S_{g}}w_{i}-w_{i}^{\prime}<\sum_{i\in S_{b}}w_{i}-w_{i}^{\prime}\,.\]

Further, recall that \(w_{i}^{\prime}=\left(1-\frac{\tau_{i}}{\tau_{\max}}\right)w_{i}\), so for all \(i\in[n]\), \(w_{i}-w_{i}^{\prime}=\frac{1}{\tau_{\max}}\tau_{i}w_{i}\). Hence is enough to show that

\[\sum_{i\in S_{g}}\tau_{i}w_{i}<\sum_{i\in S_{b}}\tau_{i}w_{i}\,.\]

Since \(S_{g}\) and \(S_{b}\) partition \([n]\) and

\[\sum_{i=1}^{n}w_{i}\tau_{i}=\sum_{i=1}^{n}w_{i}\left\langle v^{(t)},f\left( \hat{\mu}^{(t)}-y_{i}\right)\right\rangle^{2}=\left(v^{(t)}\right)^{\top} \Sigma_{f}^{(t)}\left(v^{(t)}\right)=\left\|\Sigma_{f}^{(t)}\right\|\,,\]

we can prove \(\sum_{i\in S_{g}}\tau_{i}w_{i}<\sum_{i\in S_{b}}\tau_{i}w_{i}\) by showing that

\[\sum_{i\in S_{g}}\tau_{i}w_{i}<\frac{\left\|\Sigma_{f}^{(t)}\right\|}{2}\,.\]

To this end, note that

\[\sum_{i\in S_{g}}\tau_{i}w_{i} =\sum_{i\in S_{g}}w_{i}\left\langle v^{(t)},f\left(\hat{\mu}^{(t )}-y_{i}\right)\right\rangle^{2}\] \[\leqslant\frac{2}{n}\sum_{i\in[n]}\left\langle v^{(t)},f\left( \mu^{*}-y_{i}\right)\right\rangle^{2}+\frac{2}{\left|S_{g}\right|}\sum_{i\in S _{g}}\left\langle v^{(t)},f\left(\hat{\mu}^{(t)}-y_{i}\right)-f\left(\mu^{*}-y _{i}\right)\right\rangle^{2}\,.\]

The first term is at most \(2\sigma^{2}\). For the second term we observe that since \(f\) is \(1\)-Lipschitz, for all \(i\in[n]\) it holds that

\[\left\langle v^{(t)},f\left(\hat{\mu}^{(t)}-y_{i}\right)-f\left(\mu^{*}-y_{i} \right)\right\rangle^{2}\leqslant\left\|f\left(\hat{\mu}^{(t)}-y_{i}\right)-f \left(\mu^{*}-y_{i}\right)\right\|^{2}\leqslant\left\|\hat{\mu}^{(t)}-\mu^{*} \right\|^{2}\,.\]

Hence, by Theorem C.1,

\[\sum_{i\in S_{g}}\tau_{i}w_{i}\leqslant 2\sigma^{2}+\left\|\hat{\mu}^{(t)}- \beta^{*}\right\|^{2}\leqslant 2\sigma^{2}+\frac{100^{2}}{\alpha^{2}}\left( \left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|+ \sqrt{\varepsilon}\cdot\Sigma_{f}^{(t)}\right)^{2}<\frac{\Sigma_{f}^{(t)}}{2}\,,\]

where we used \(\varepsilon\lesssim\alpha^{2}\) and \(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\| \lesssim\alpha\sigma\,.\) 

## Appendix E Filtering Algorithm for Bounded Higher-Order Clipped Moments

In this section, we show how to adapt the filtering algorithm to incorporate higher-order moments of the transformed noise. We follow closely the exposition in [1, Chapter 6.5]. In particular, consider the following definition similar to the by now standard notion of certifiably bounded moments or certifiable subgaussianity [11, 12]. Let \(f\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\) be some function.

**Definition E.1**.: Let \(k,\ell\in\mathbb{N}\) and \(\sigma>0\). We say a distribution \(D\) over \(\mathbb{R}^{d}\) has _\((2k,\ell)\)-certifiably \(\sigma\)-bounded \(f\)-moments_ if there is a degree-\(\ell\) SoS proof (cf. Appendix B), in formal variables \(v\), that

\[\mathop{\mathbb{E}}_{\bm{\eta}\sim D}\left\langle f\left(\bm{\eta}\right),v \right\rangle^{2k}\leqslant\left(\sigma\cdot\left\|v\right\|\right)^{2k}\,.\]We again define a goodness condition analogous to Assumption D.1. Specifically, we assume

**Assumption E.2** (Higher-Order Goodness Condition).: Let \(F\colon\mathbb{R}^{d}\to\mathbb{R},f=\nabla F\) be such that \(f\) is \(1\)-Lipschitz. Let \(\mu^{*}\in\mathbb{R}^{d},0<\sigma,0<\alpha<1,k\in\mathbb{N}\) and \(y_{1},\dots,y_{n}\in\mathbb{R}^{d}\). Let \(D\) be a distribution over \(\mathbb{R}^{d}\) and \(\bm{\eta}_{1}^{*},\dots,\bm{\eta}_{n}^{*}\) be \(n\) i.i.d.samples from \(D\). We say that the _higher-order goodness condition_ holds, if

1. \(\alpha\|\hat{\mu}^{w}-\mu^{*}\|^{2}\leqslant 10\left\langle\nabla\mathcal{L}^{ w}\left(\mu^{*}\right),\mu^{*}-\hat{\mu}^{w}\right\rangle\),
2. \(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\| \leqslant\frac{\alpha}{100}\sigma\),
3. the uniform distribution over the set \(\{\bm{\eta}_{1}^{*},\dots,\bm{\eta}_{n}^{*}\}\) has \((2k,\ell)\)-certifiabily \(\sigma\)-bounded \(f\)-moments. In other words, \[\left|\frac{v}{\ell}\,\frac{1}{n}\sum_{i=1}^{n}\langle f(\bm{\eta}_{i}^{*}),v \rangle^{2k}\leqslant\left(\sigma\cdot\|v\|\right)^{2k}\,.\]

Specifically, we will show

**Theorem E.3**.: _Let \(\mu^{*}\in\mathbb{R}^{d},0<\sigma,0<\alpha<1,k\in\mathbb{N}\), and \(\bm{\eta}_{1}^{*},\dots,\bm{\eta}_{1}^{*}\) be \(n\) i.i.d.samples from some distribution \(D\) over \(\mathbb{R}^{d}\). Assume that \(\varepsilon^{1-\frac{1}{2k}}\leqslant\frac{\alpha}{1000}\). Let \(y_{1},\dots,y_{n}\) be an \(\varepsilon\)-corruption of \(\bm{y}_{1}^{*}=\mu^{*}+\bm{\eta}_{1}^{*},\dots,\bm{y}_{n}^{*}=\mu^{*}+\bm{ \eta}_{n}^{*}\). Assume Assumption E.2 (the higher-order goodness condition) holds, then Algorithm E.4, given \(y_{1},\dots,y_{n}\) and \(\sigma\), terminates after at most \(2\varepsilon n+1\) iterations and computes \(\hat{\mu}\) such that_

\[\|\hat{\mu}-\mu^{*}\|\leqslant O\left(\frac{\left\|\frac{1}{n}\sum_{i=1}^{n} f\left(\bm{\eta}_{i}^{*}\right)\right\|}{\alpha}+\frac{\varepsilon^{1-\frac{1}{2k} }}{\alpha}\cdot\sigma\right)\,.\]

_Moreover each iteration can be implemented in time \(n^{O(1)}\cdot d^{O(\ell)}\)._

Additionally to the notation introduced in Appendix D, consider the following polynomial in formal variables \(v\)

\[p^{w}(v)=\sum_{i=1}^{n}w_{i}\langle v,f(y_{i}-\hat{\mu})\rangle^{2k}\,.\]

In what follows we will write \(p^{w^{(t)}}\) as just \(p^{(t)}\).

**Algorithm E.4** (Filtering Algorithm).

**Input:**\(\varepsilon\)-corrupted sample \(y_{1},\dots,y_{n}\) and \(\sigma>0\).

**Output:** Location estimate \(\hat{\mu}\).

* Let \(w^{(0)}=\frac{1}{n}\).
* Compute \(\hat{\mu}^{(0)}=\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w^{(0)}}\left(\mu\right)\).
* Let \(t=0\).

* there is no degree-\(\ell\) SoS proof (in variables \(v\)) that \(p^{(t)}(v)\leqslant\left(100\cdot\sigma\cdot\|v\|\right)^{2k}\)do
* Find a degree-\(\ell\) pseudo-expectation that satisfies the constraint \(\|v\|^{2}=1\) such that \(\tilde{\mathbb{E}}p^{t}(v)>\left(99\cdot\sigma\right)^{2k}\).
* For \(i\in[n]\), compute \(\tau_{i}^{(t)}=\tilde{\mathbb{E}}\left\langle v,f\left(\hat{\mu}^{(t)}-y_{i} \right)\right\rangle^{2k}\).
* For \(i\in[n]\), set \(w_{i}^{(t+1)}=\left(1-\frac{\tau_{i}}{\tau_{\max}}\right)w_{i}^{(t)}\), where \(\tau_{\max}=\max_{i\in[n]}\tau_{i}\).
* Compute \(\hat{\mu}^{(t+1)}=\min_{\mu\in\mathbb{R}^{d}}f^{w^{(t+1)}}\left(\mu\right)\).
* \(t\gets t+1\).
* Output \(\hat{\mu}^{(t)}\).

The proof of Theorem E.3 is very similar to the one of Theorem D.2. Again, we will use the following lemma whose proof we will give at the end of this section 

**Lemma E.5**.: _Assume that_

\[\sum_{i\in S_{g}}\frac{1}{n}-w_{i}^{(t)}<\sum_{i\in S_{b}}\frac{1}{n}-w_{i}^{(t)}\]

_and \(p^{t}(v)\leqslant\left(100\cdot\sigma\cdot\left\|v\right\|^{2}\right)^{k}\) does not have a degree-\(\ell\) SoS proof. Then also_

\[\sum_{i\in S_{g}}\frac{1}{n}-w_{i}^{(t+1)}<\sum_{i\in S_{b}}\frac{1}{n}-w_{i}^{ (t+1)}\,.\]

With this in hand, we will prove Theorem E.3.

Proof of Theorem e.3.: First, note that by the facts in Appendix B we can compute the pseudo-expectation in each iteration in time \(n^{O(1)}d^{O(l)}\) and hence, every iteration can be implemented to run in this time. Exactly as in the proof of Theorem D.2 we can conclude that the algorithm terminates after \(T\leqslant 2\varepsilon n+1\) iterations and that \(w^{T}\in\mathcal{W}_{2\varepsilon}\).

Recall that

\[\hat{\mathfrak{m}}_{2k}^{w}=\max_{v\in\mathbb{S}^{d-1}}\left[\sum_{i=1}^{n}w_{ i}\left\langle f\left(\hat{\mu}-y_{i}\right),v\right\rangle^{2k}\right]^{\frac{1}{2 k}}=\max_{v\in\mathbb{S}^{d-1}}\left[p^{w}(v)\right]^{\frac{1}{2k}}\,.\]

By Theorem C.1 it follows that

\[\left\|\hat{\mu}^{(T)}-\mu^{*}\right\|\leqslant O\left(\frac{\left\|\frac{1} {n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|}{\alpha}+\frac{ \varepsilon^{1-1/(2k)}\cdot\left(\hat{\mathfrak{m}}_{2k}^{w^{T}}+\bar{ \mathfrak{m}}_{2k}\right)}{\alpha}\right)\,.\]

Note that since the uniform distribution over \(\{\bm{\eta}_{1}^{*},\ldots,\bm{\eta}_{n}^{*}\}\) has \((2k,\ell)\)-certifiably \(\sigma\)-bounded \(f\)-moments, it holds that \(\bar{\mathfrak{m}}_{2k}\leqslant\sigma\). Since the algorithm terminates after \(T\) iterations it holds that \(\hat{\mathfrak{m}}_{2k}^{w^{T}}\leqslant 100\sigma\) which completes the proof. 

Next, we will prove Lemma E.5.

Proof of Lemma e.5.: Again, for simplicity, let \(w=w^{(t)}\) and \(w^{\prime}=w^{(t+1)}\). As in the proof of Lemma D.4, it is enough to show that

\[\sum_{i\in S_{g}}\tau_{i}w_{i}<\sum_{i\in S_{b}}\tau_{i}w_{i}\,.\]

Further,

\[\sum_{i=1}^{n}w_{i}\tau_{i}=\sum_{i=1}^{n}w_{i}\tilde{\mathbb{E}}\left\langle v,f\left(\hat{\mu}^{(t)}-y_{i}\right)\right\rangle^{k}=\tilde{\mathbb{E}}p^{t} (v)\geqslant\left(99\cdot\sigma\right)^{2k}\,.\]

Using Fact K.1, we continue to bound

\[\sum_{i\in S_{g}}\tau_{i}w_{i} =\sum_{i\in S_{g}}w_{i}\tilde{\mathbb{E}}\left\langle v,f\left( \hat{\mu}^{(t)}-y_{i}\right)\right\rangle^{2k}\] \[\leqslant 4^{k}\cdot\left[\frac{1}{n}\sum_{i\in[n]}\tilde{ \mathbb{E}}\left\langle v,f\left(\bm{\eta}_{i}^{*}\right)\right\rangle^{2k}+ \frac{1}{|S_{g}|}\sum_{i\in S_{g}}\tilde{\mathbb{E}}\left\langle v,f\left(\hat {\mu}^{(t)}-y_{i}\right)-f\left(\mu^{*}-y_{i}\right)\right\rangle^{2k}\right]\,.\]

The first term is at most \(\sigma^{2k}\). For the second term, notice that by Fact K.2

Hence, the second term is at most \(\left\|\hat{\mu}^{(t)}-\mu^{*}\right\|^{2k}\). By Theorem C.1 it follows that

\[\left\|\hat{\mu}^{(T)}-\mu^{*}\right\|\leqslant 100\cdot\left(\frac{\left\|\frac{1} {n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|}{\alpha}+\frac{ \varepsilon^{1-1/(2k)}\cdot\left(\hat{\mathfrak{m}}_{2k}^{w^{t}}+\bar{ \mathfrak{m}}_{2k}\right)}{\alpha}\right)\,.\]Recall that \(\bar{\mathbf{m}}_{2k}\leqslant\sigma\). Since

\[\hat{\mathbf{m}}_{2k}^{w}=\max_{v\in\mathbb{S}^{d-1}}\left[\sum_{i=1}^{n}w_{i} \left\langle f\left(\hat{\mu}-y_{i}\right),v\right\rangle^{2k}\right]^{\frac{1 }{2k}}=\max_{v\in\mathbb{S}^{d-1}}\left[p^{w}(v)\right]^{\frac{1}{2k}}\,,\]

\(\varepsilon^{1-\frac{1}{2k}}\leqslant\frac{\alpha}{1000}\) and \(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\| \leqslant\frac{\alpha}{100}\sigma\,,\) we get

\[\sum_{i\in S_{g}}\tau_{i}w_{i}<0.2\cdot\tilde{\mathbb{E}}p^{t}(v)+0.1\cdot(99 \cdot\sigma)^{2k}<\tilde{\mathbb{E}}p^{t}(v)/2\,.\]

## Appendix F Product Distributions

In this section we prove that \((\alpha,\rho)\)-semi-product distributions satisfy Assumption D.1 with \(\sigma=O(\rho)\) and with \(F\) equals to the entry-wise Huber loss function with parameter \(2\rho\):

\[F(x)=\sum_{j=1}^{d}\Phi_{2\rho}\left(x_{j}\right)\]

with probability \(1-\delta\) as long as \(n\gtrsim\frac{d+\log(1/\delta)}{\alpha^{4}}\). Moreover, if in addition \(n\gtrsim\frac{1}{\alpha^{4}}\cdot d^{k}\cdot\log\left(d/\delta\right)\), then Assumption E.2 is also satisfied. We can use this to prove Theorem A.3 as follows: Let \(\bm{\eta}_{1}^{*},\ldots\bm{\eta}_{n}^{*}\) follow an \((\alpha,\rho)\)-semi-product distribution. We will see in Lemma F.3 that for any \(n\), it holds with probability at least \(1-\delta\) that \(\left\|\frac{1}{n}\sum_{i=1}^{n}f(\bm{\eta}_{i}^{*})\right\|\leqslant\rho \cdot\sqrt{\frac{d+\log(1/\delta)}{n}}\). Hence, by Theorem E.3 (or Algorithm D.3 for \(k=1\)) it follows that as long as \(n\gtrsim\frac{1}{\alpha^{4}}\cdot d^{k}\log(d/\delta)\) (or \(n\gtrsim\frac{d+\log(1/\delta)}{\alpha^{4}}\) for \(k=1\)) our estimator achieves error

\[O\left(\frac{\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right) \right\|}{\alpha}+\frac{\varepsilon^{1-\frac{1}{2k}}}{\alpha}\cdot\sigma \right)=O\left(\rho\cdot\left[\sqrt{\frac{d+\log(1/\delta)}{\alpha^{4}n}}+ \frac{\varepsilon^{1-\frac{1}{2k}}}{\alpha}\right]\right)\,.\]

Letting \(k=O(\log(1/\varepsilon))\) proves Theorem A.3. Note that similarly to Theorem 1.6, for \(1\leqslant k\leqslant O(\log(1/\varepsilon))\) we get a sequence of algorithms interpolating between error \(\sqrt{\varepsilon}\) and \(\varepsilon\) (using roughly \(d^{k}\) samples).

Note that \(f=\nabla F\) is indeed \(1\)-Lipschitz since the derivative of the Huber loss is Lipschitz. In the next to lemmas we prove the first assumption from Assumption D.1.

**Lemma F.1**.: _Let \(w\in\mathcal{W}_{2\varepsilon}\) for \(\varepsilon\lesssim\alpha\). Suppose that \(n\gtrsim\log(d)/\alpha\). Then with probability \(1-\exp\left(-\Omega\left(\alpha n\right)\right)\) for all \(u\in\mathbb{R}^{d}\) such that \(\|u\|_{\max}\leqslant\rho\),_

\[\mathcal{L}^{w}\left(\mu^{*}+u\right)-\mathcal{L}^{w}\left(\mu^{*}\right)- \left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\right\rangle\geqslant \frac{\alpha}{4}\cdot\left\|u\right\|^{2}.\]

Proof.: Let \(j\in[d]\) and \(\zeta_{i}=\mu-y_{i}\).

\[\mathcal{L}_{j}^{w}\left(\mu_{j}^{*}+u_{j}\right)-\mathcal{L}_{j}^{w}\left(\mu _{j}^{*}\right)-\left(\mathcal{L}_{j}^{w}\right)^{\prime}\left(\mu_{j}^{*} \right)\cdot u_{j}=\sum_{i=1}^{n}w_{i}\left[\Phi\left(\zeta_{ij}+u_{j}\right)- \Phi\left(\zeta_{ij}\right)-u_{j}\cdot\phi\left(\zeta_{ij}\right)\right]\,.\]

Let \(M\) be the set of uncorrupted samples such that \(|\zeta_{ij}|=|\bm{\eta}_{ij}|\leqslant h/2\). Note that for \(i\in M\), \(\Phi\left(\zeta_{ij}+u_{j}\right)=\frac{1}{2}|\zeta_{ij}+u_{j}|^{2}\) and \(\Phi\left(\zeta_{ij}+u_{j}\right)=\frac{1}{2}|\zeta_{ij}|^{2}\). Hence

\[\sum_{i\in M}w_{i}\left[\Phi\left(\zeta_{ij}+u_{j}\right)-\Phi\left(\zeta_{ij} \right)-u_{j}\cdot\phi\left(\zeta_{ij}\right)\right]=\frac{1}{2}\sum_{i\in M} w_{i}u_{j}^{2}\,.\]

And by convexity,

\[\sum_{i\notin M}w_{i}\left[\Phi\left(\left\|\zeta_{i}+u\right\|\right)-\Phi \left(\left\|\zeta_{i}\right\|\right)-\left\langle f\left(\zeta_{i}\right),u \right\rangle\right]\geqslant 0\,.\]Since \(w\in\mathcal{W}_{2\varepsilon}\),

\[\sum_{i\in M}\left|w_{i}-\frac{1}{n}\right|\leqslant 2\varepsilon\,.\]

Hence

\[\sum_{i\in M}w_{i}\geqslant\frac{|M|}{n}-2\varepsilon\,.\]

By a Chernoff bound,

\[|M|\geqslant 0.9\alpha-\varepsilon\geqslant\alpha/2\]

with probability \(1-\exp\left(-\alpha n\right)\). The result follows from a union bound over all \(j\in[d]\). 

**Lemma F.2**.: _Let \(w\in\mathcal{W}_{2\varepsilon}\) for \(\varepsilon\lesssim\alpha\), and suppose that \(n\gtrsim\log(d)/\alpha^{2}\). Then with probability \(1-\exp\left(-\Omega\left(\alpha^{2}n\right)\right)\),_

\[\left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),\mu^{*}-\hat{\mu}^{w} \right\rangle\geqslant\frac{\alpha}{4}\|\hat{\mu}^{w}-\mu^{*}\|^{2}\,.\]

Proof.: Fix \(j\in d\) and let \(u_{j}=\mu_{j}^{*}-\hat{\mu}_{j}(w)\). Let \(u_{j}^{\prime}\) and \(t\) be such that

\[\left|u_{j}^{\prime}\right| \leqslant\rho\,,\] \[\hat{\mu}_{j}(w) =\mu_{j}^{*}+t\cdot u_{j}^{\prime}\,.\]

It follows that \(t=\max\left\{1,\left|u_{j}\right|\right\}\). Clearly, \(\hat{\mu}_{j}(w)\) is the (unique) minimizer of \(\mathcal{L}_{j}^{w}\). Let \(\hat{\mu}_{j}^{\prime}=\mu_{j}^{*}+u_{j}^{\prime}\), by convexity of \(\mathcal{L}_{j}^{w}\) it holds that \(\mathcal{L}_{j}^{w}(\hat{\mu}_{j}^{\prime})\leqslant\mathcal{L}_{j}^{w}(\mu_ {j}^{*})\). Since \(\left|u_{j}^{\prime}\right|\leqslant\rho\) it follows by Lemma F.1 that

\[\mathcal{L}_{j}^{w}\left(\mu_{j}^{*}\right)\geqslant\mathcal{L}_{j}^{w}\left( \hat{\mu}_{j}^{\prime}\right)\geqslant\mathcal{L}_{j}^{w}\left(\mu_{j}^{*} \right)+\left(\mathcal{L}_{j}^{w}\right)^{\prime}\left(\mu_{j}^{*}\right) \cdot\left(-u_{j}^{\prime}\right)+\frac{\alpha}{4}\left(u_{j}^{\prime}\right) ^{2}\]

with probability at least \(1-\exp\left(-\Omega\left(\alpha n\right)\right)\). Rearranging yields

\[\left|u_{j}^{\prime}\right|\leqslant\frac{4}{\alpha}\cdot\left(\mathcal{L}_{j }^{w}\right)^{\prime}\left(\mu_{j}^{*}\right)\,.\]

We next examine

\[\left(\mathcal{L}_{j}^{w}\right)^{\prime}\left(\mu_{j}^{*}\right) =\sum_{i=1}^{n}w_{i}\phi\left(\left(y_{i}\right)_{j}-\mu_{j}^{*}\right)\] \[=\sum_{i=1}^{n}w_{i}\phi\left(\boldsymbol{\eta}_{ij}\right)+\sum_ {i\in S_{b}}^{n}w_{i}\left[\phi\left(\left(y_{i}\right)_{j}-\mu_{j}^{*}\right) -\phi\left(\left(\boldsymbol{y}_{i}^{*}\right)_{j}-\mu_{j}^{*}\right)\right]\,.\]

By Hoeffding's inequality,

\[\left|\sum_{i=1}^{n}w_{i}\phi\left(\boldsymbol{\eta}_{ij}\right)\right| \leqslant\left|\frac{1}{n}\sum_{i=1}^{n}\phi\left(\boldsymbol{\eta}_{ij} \right)\right|+2\rho\varepsilon\leqslant 2\rho\tau/\sqrt{n}+4\rho\varepsilon\]

with probability at least \(1-\exp(-\tau^{2}/2)\).

The second term can be bounded as follows:

\[\sum_{i\in S_{b}}w_{i}\cdot\left[\phi\left(\mu_{j}^{*}-\left(y_{i}\right)_{j} \right)-\phi\left(\mu_{j}^{*}-\left(\boldsymbol{y}_{i}^{*}\right)_{j}\right) \right]\leqslant 4\rho\varepsilon\,,\]

where we used that for all \(x,y\in\mathbb{R}\) it holds that \(\left|\phi(x)-\phi(y)\right|\leqslant 2\rho\) and \(\sum_{i\in S_{b}}w_{i}\leqslant 2\varepsilon\). Putting everything together we obtain

\[\left|u_{j}^{\prime}\right|\leqslant\rho\frac{2\tau}{\alpha\sqrt{n}}+\frac{4 \rho\varepsilon}{\alpha}\]

with probability \(1-\exp(-\tau^{2}/2)\). For \(\tau=\alpha\sqrt{n}/10\), \(\left|u_{j}^{\prime}\right|<\rho\). Hence, \(t=1\) since otherwise

\[\left|u_{j}\right|=\left|\hat{\mu}_{j}(w)-\mu_{j}^{*}\right|=t\cdot\left|u_{j} ^{\prime}\right|<\left|u_{j}\right|.\]

It follows that \(u_{j}^{\prime}=u_{j}\) and thus \(\left|\hat{\mu}_{j}(w)-\mu_{j}^{*}\right|<\rho\).

By Lemma F.1

\[\mathcal{L}^{w}\left(\mu^{*}\right)\geqslant\mathcal{L}^{w}\left(\hat{\mu} \right)\geqslant\mathcal{L}^{w}\left(\mu^{*}\right)+\left\langle\nabla\mathcal{L }^{w}\left(\mu^{*}\right),-u\right\rangle+\frac{\alpha}{4}\left\|u\right\|^{2 }\,,\]

Hence

\[\left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\right\rangle\geqslant \frac{\alpha}{4}\left\|u\right\|^{2}\,.\]

In the following lemma we show that Assumption 2 of Assumption D.1 is satisfied for \(\sigma=O(\rho)\) with probability \(1-\delta\) as long as \(n\gtrsim\frac{d+\log(1/\delta)}{\alpha^{2}}\).

**Lemma F.3**.: _With probability at least \(1-\delta\),_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\right\|\leqslant 1 0\rho\sqrt{\frac{d+\log\left(1/\delta\right)}{n}}\,.\]

Proof.: Let \(u\in\mathbb{R}^{d}\) be some fixed vector. Since \(\operatorname{sign}\left(\bm{\eta}\right)\) is independent of \(\operatorname{abs}\left(\bm{\eta}\right)\), random variables \(\tilde{\bm{u}}_{ij}=u_{j}\cdot\left|\bm{\eta}_{ij}\right|\) are independent of \(\operatorname{sign}\left(\bm{\eta}\right)\). By Hoeffding's inequality

\[\left|\left\langle\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right),u \right\rangle\right|=\left|\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{d} \operatorname{sign}\left(\bm{\eta}_{ij}\right)\tilde{\bm{u}}_{ij}\right| \leqslant 2\rho\cdot\left\|u\right\|\cdot\tau/\sqrt{n}\]

with probability at least \(1-\exp(-\tau^{2}/2)\). Let \(\mathcal{N}_{1/2}\) be a \(1/2\)-net of size \(6^{d}\) in the Euclidean unit \(d\)-dimensional ball. Then

\[\sup_{\left\|u\right\|\leqslant 1}\left|\left\langle\frac{1}{n}\sum_{i=1}^{n}f \left(\bm{\eta}_{i}\right),u\right\rangle\right|\leqslant\sup_{\left\|u \right\|\leqslant 1/2}\left|\left\langle\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{ \eta}_{i}\right),u\right\rangle\right|+\sup_{u\in\mathcal{N}_{1/2}1/2}\left| \left\langle\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right),u\right\rangle \right|.\]

Hence by union bound,

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\right\|=\sup_{ \left\|u\right\|\leqslant 1}\left|\left\langle\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{ \eta}_{i}\right),u\right\rangle\right|\leqslant 2\sup_{u\in\mathcal{N}_{1/2}} \left|\left\langle\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right),u \right\rangle\right|\leqslant 4\rho\sqrt{\frac{2\ln(6)\cdot d+\log\left(1/\delta \right)}{n}}\,.\]

In the following lemma we show that Assumption 3 of Assumption D.1 is satisfied for \(\sigma=O(\rho)\) with probability \(1-\delta\) as long as \(n\gtrsim d+\log(1/\delta)\).

**Lemma F.4**.: _Let \(\delta\in(0,1)\) and suppose that \(n\geqslant d+\log\left(1/\delta\right)\). Then_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\left(f\left(\bm{ \eta}_{i}\right)\right)^{\top}\right\|\leqslant O\left(\rho^{2}\right)\]

_with probability at least \(1-\delta\)._

Proof.: Let \(u\in\mathbb{R}^{d}\) be a fixed vector. Since the random vector \(\operatorname{sign}\left(\bm{\eta}\right)\) is independent of \(\operatorname{abs}\left(\bm{\eta}\right)\), the random vector \(\operatorname{abs}\left(f\left(\bm{\eta}\right)\right)\circ u\) with entries \(u_{j}\cdot\left|\bm{\eta}(j)\right|\) is independent of \(\operatorname{sign}\left(\bm{\eta}\right)\). Since the entries of \(\operatorname{sign}\left(\bm{\eta}\right)\) are iid Rademacher, for every positive integer \(k\),

\[\sup_{\left\|u\right\|\leqslant 1}\mathbb{E}\left\langle f \left(\bm{\eta}\right),u\right\rangle^{2k} \leqslant\sup_{\left\|u\right\|\leqslant 1}\mathbb{E}\left\langle \operatorname{sign}\left(\bm{\eta}\right),\operatorname{abs}\left(f\left(\bm{ \eta}\right)\right)\circ u\right\rangle^{2k}\] \[\leqslant\sup_{\left\|u\right\|\leqslant 2\rho}\mathbb{E}\left\langle \operatorname{sign}\left(\bm{\eta}\right),u\right\rangle^{2k}\] \[\leqslant\left(2\rho\right)^{2k}\,.\]

Hence \(f\left(\bm{\eta}\right)\) is \(2\rho\)-subgaussian and the result follows from concentration of empirical covariance for subgaussian distributions (see Theorem 6.5 in [11]).

In the following lemma we show that Assumption 3 of Assumption E.2 is satisfied for \(\sigma=O(\rho)\) with probability \(1-\delta\) as long as \(n\gtrsim d^{k}\cdot\log\left(d/\delta\right)\).

**Lemma F.5**.: _Let \(k\) be a positive integer and suppose that \(n\geqslant 10\cdot d^{k}\cdot\log\left(d/\delta\right)\). Then with probability at least \(1-\delta\), uniform distribution over the set \(\left\{\boldsymbol{\eta}_{1},\ldots,\boldsymbol{\eta}_{n}\right\}\) has \(\left(2k,2k\right)\)-certifiable \(4\rho\)-bounded \(f\)-moments._

Proof.: First we show that the random vector \(\boldsymbol{\eta}\) has \(\left(2k,2k\right)\)-certifiable \(2\rho\)-bounded \(f\)-moments. Let's call a multi-index \(\beta\)_even_ if every element in \(\beta\) appears even number of times. Note that \(\beta\) is even iff \(\beta=2\beta^{\prime}\) for a multi-index \(\beta^{\prime}\) of size \(k\). Denote by \(\mathcal{E}\) the set of all even multi-indices of size \(2k\). Let \(v_{1},\ldots,v_{d}\) be variables, and consider the polynomial \(\mathbb{E}\left\langle f\left(\boldsymbol{\eta}\right),v\right\rangle^{2k}\). It follows that

\[\left|\tfrac{v}{2k}\,\mathbb{E}\left\langle f\left(\boldsymbol{ \eta}\right),v\right\rangle^{2k} =\mathbb{E}\,\mathbb{E}\left[\left\langle\operatorname{sign} \left(\boldsymbol{\eta}\right),\operatorname{abs}\left(f\left(\boldsymbol{ \eta}\right)\right)\circ v\right\rangle^{2k}\,\,\Big{|}\,\operatorname{abs} \left(f\left(\boldsymbol{\eta}\right)\right)\right]\] \[=\mathbb{E}\sum_{\beta\in\mathcal{E}}\mathbb{E}\left[ \operatorname{sign}\left(\boldsymbol{\eta}\right)^{\beta}\left(\operatorname {abs}\left(f\left(\boldsymbol{\eta}\right)\right)\circ v\right)^{\beta}\, \,\Big{|}\,\operatorname{abs}\left(f\left(\boldsymbol{\eta}\right)\right)\right]\] \[=\mathbb{E}\sum_{\beta\in\mathcal{E}}\left(\operatorname{abs} \left(f\left(\boldsymbol{\eta}\right)\right)\circ v\right)^{\beta}\] \[\leqslant\left(2\rho\right)^{2k}\sum_{\beta\in\mathcal{E}}v^{\beta}\] \[=\left(2\rho\right)^{2k}\sum_{|\beta^{\prime}|=k}v^{2\beta^{ \prime}}\] \[=\left(2\rho\right)^{2k}\left\|v\right\|^{2k}.\]

Hence \(\boldsymbol{\eta}\) has \(\left(2k,2k\right)\)-certifiable \(2\rho\)-bounded \(f\)-moments. Since \(\forall x\in\mathbb{R}^{d}\), \(\left\|f\left(x\right)\right\|\leqslant 2\rho\sqrt{d}\), the result follows from Lemma J.4. 

## Appendix G Elliptical Distributions

In this section we use a different normalization than the one described in the introduction. Concretely, we assume that \(\operatorname{Tr}(\Sigma)=d\) and that for some \(\rho>0\) and \(\alpha\in(0,1)\), \(\mathbb{P}\left[\boldsymbol{R}\leqslant\rho\sqrt{d}\right]\geqslant\alpha\). For \(\alpha=1/2\), this is the same model as in the Introduction, since we can multiply \(\Sigma^{1/2}\) by \(s=\sqrt{d/\operatorname{Tr}\left(\Sigma\right)}\) and divide \(\boldsymbol{R}\) by \(s\). Hence the new parameter \(\rho\) written in terms of the parameters defined in the Introduction is equal to \(\sqrt{2}/s=\sqrt{2\operatorname{Tr}(\Sigma)/d}\). Note that \(\rho\left\|\Sigma\right\|\) in new notation is equal to \(\sqrt{2}\left\|\Sigma\right\|\) in the notation used in the introduction. Note that the effective rank, \(\operatorname{r}(\Sigma)=\frac{\operatorname{Tr}\Sigma}{\left\|\Sigma\right\|}\), is the same in both parametrizations.

We assume that \(\rho\) is known. In the case of unknown \(\rho\) and \(\alpha\geqslant\Omega(1)\), we can estimate \(\rho\) using the corrupted samples and obtain the same guarantees as for known \(\rho\) (up to a constant factor). We describe how to achiev this in Appendix G.1.

We prove that elliptical distributions satisfy Assumption D.1 and with, \(\sigma=O(\rho\sqrt{\left\|\Sigma\right\|})\) and \(F:\mathbb{R}^{d}\to\mathbb{R}\) defined as

\[F(x)=\Phi_{20\rho\sqrt{d}}\left(\left\|x\right\|\right)\,,\]

with probability \(1-\delta\) as long as \(n\gtrsim\frac{\operatorname{r}(\Sigma)\log(d/\delta)}{\alpha^{4}}\). Moreover, if in addition \(n\gtrsim\frac{1}{\alpha^{4}}(\frac{\operatorname{r}(\Sigma)}{k})^{k}\cdot \log\left(d/\delta\right)\), then Assumption E.2 is also satisfied with \(\sigma=O(\rho\sqrt{k\|\Sigma\|})\).

Going back to the parametrization of the introduction and letting \(\alpha=\frac{1}{2}\), this means the following: For \(n\gtrsim\frac{\operatorname{r}(\Sigma)\log(d/\delta)}{\alpha^{4}}\), elliptical distributions satisfy Assumption D.1 with \(\sigma=O(\sqrt{\left\|\Sigma\right\|})\). Further, if \(n\gtrsim\frac{1}{\alpha^{4}}(\frac{\operatorname{r}(\Sigma)}{k})^{k}\cdot \log\left(d/\delta\right)\), they satisfy Assumption E.2 with \(\sigma=O(\sqrt{k\|\Sigma\|})\). Further, for \(\boldsymbol{\eta}_{1}^{*},\ldots,\boldsymbol{\eta}_{n}^{*}\) following an elliptical distribution with the above parameters, we will in Lemma G.4 show that for \(n\geqslant\log(1/\delta)\) it holds with probability at least \(1-\delta\) that \(\|\frac{1}{n}\sum_{i=1}^{n}f(\boldsymbol{\eta}_{i}^{*})\|\leqslant O(\sqrt{ \frac{\operatorname{Tr}(\Sigma)\cdot\log\left(d/\delta\right)}{n}})\).

Hence, by Theorem E.3 (or Theorem D.2 for \(k=1\)) our estimator achieves error

\[O\left(\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}^{*}\right)\right\|+ \varepsilon^{1-\frac{1}{2k}}\cdot\sigma\right)=O\left(\sqrt{\|\Sigma\|}\cdot \left[\sqrt{\frac{\mathrm{r}(\Sigma)\log(d/\delta)}{n}}+\sqrt{k}\varepsilon^{1- \frac{1}{2k}}\right]\right)\,.\]

This gives the proof of Theorem 1.6.

Note that \(f=\nabla F\) sends \(x\) to \(\phi_{20\rho\sqrt{d}}(\|x\|)\cdot\frac{x}{\|x\|}\) and hence is indeed \(1\)-Lipschitz since it is the projection of \(x\) onto the \(\ell_{2}\)-ball of radius \(20\rho\sqrt{d}\). In the next three lemmas we prove the first assumption from Assumption D.1.

**Lemma G.1**.: _Suppose that \(d\geqslant 100\). Then_

\[\mathbb{P}\left[\left\|\bm{R}\Sigma^{1/2}\bm{u}\right\|\leqslant h/2\right] \geqslant 0.8\alpha\,,\]

_where \(h=20\rho\sqrt{d}\)._

Proof.: Let \(\bm{g}\sim N(0,4\rho^{2}\Sigma)\) be a \(d\)-dimensional Gaussian vector such that \(\bm{g}=\bm{R^{\prime}}\Sigma^{1/2}\bm{u}\), where \(\bm{R}^{\prime}\) is independent of \(\bm{R}\) and has distribution of the norm of Gaussian vector \(N(0,2\rho^{2}\mathrm{Id})\). Note that \(\mathrm{Tr}\left(\mathbb{E}\,\bm{g}\bm{g}^{\top}\right)=4\rho^{2}d\). Applying Fact J.6 with \(t=2\), we get

\[\mathbb{P}\left[\left\|\bm{g}\right\|\geqslant 10\rho\sqrt{d}\right]\leqslant \exp(-2)\leqslant 0.15\,.\]

Since

\[\mathbb{P}\left[\bm{R}^{\prime}\geqslant\rho\sqrt{d}\right]\geqslant 0.99\,,\]

we get

\[\mathbb{P}\left[\left\|\bm{g}\right\|\leqslant 10\rho\sqrt{d}\,,\bm{R}^{ \prime}\geqslant\rho\sqrt{d}\right]\geqslant 0.8\,.\]

Since \(\bm{R}\) is independent of \(\bm{g}\) and \(\bm{R}^{\prime}\), we get

\[\mathbb{P}\left[\left\|\bm{R}\Sigma^{1/2}\bm{u}\right\|\leqslant 1 0\sqrt{d}\right] \geqslant\mathbb{P}\left[\left\|\bm{R}\Sigma^{1/2}\bm{u}\right\| \leqslant\|\bm{g}\|\,\|\bm{g}\|\leqslant 10\rho\sqrt{d}\right]\] \[=\mathbb{P}\left[\bm{R}\leqslant\bm{R}^{\prime}\,,\|\bm{g}\| \leqslant 10\rho\sqrt{d}\right]\] \[\geqslant\mathbb{P}\left[\bm{R}\leqslant\rho\sqrt{d}\,,\bm{R}^{ \prime}\geqslant\rho\sqrt{d}\,,\|\bm{g}\|\leqslant 10\rho\sqrt{d}\right]\] \[\geqslant 0.8\alpha\,.\]

**Lemma G.2**.: _Let \(w\in\mathcal{W}_{2\varepsilon}\) for \(\varepsilon\lesssim\alpha\). Then with probability \(1-\exp\left(-\Omega\left(\alpha n\right)\right)\) for all \(u\in\mathbb{R}^{d}\) such that \(\|u\|\leqslant\rho\sqrt{d}\),_

\[\mathcal{L}^{w}\left(\mu^{*}+u\right)-\mathcal{L}^{w}\left(\mu^{*}\right)- \left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\right\rangle\geqslant \frac{\alpha}{4}\cdot\|u\|^{2}\,.\]

Proof.: Let \(\zeta_{i}=\mu-y_{i}\).

Let \(M\) be the set of uncorrupted samples such that \(\|\zeta_{i}\|=\|\bm{\eta}_{i}\|\leqslant h/2\). Note that for \(i\in M\), \(f\left(\|\zeta_{i}+u\|\right)=\frac{1}{2}\left\|\zeta_{i}+u\right\|^{2}\) and \(f\left(\|\zeta_{i}\|\right)=\frac{1}{2}\left\|\zeta_{i}\right\|^{2}\). Hence

\[\sum_{i\in M}w_{i}\left[f\left(\left\|\zeta_{i}+u\right\|\right)-f\left(\| \zeta_{i}\|\right)-\left\langle f\left(\zeta_{i}\right),u\right\rangle\right] =\frac{1}{2}\sum_{i\in M}w_{i}\|u\|^{2}\,.\]

And by convexity,

\[\sum_{i\notin M}w_{i}\left[f\left(\left\|\zeta_{i}+u\right\|\right)-f\left( \left\|\zeta_{i}\right\|\right)-\left\langle f\left(\zeta_{i}\right),u\right\rangle \right]\geqslant 0\,.\]Since \(w\in\mathcal{W}_{2\varepsilon}\),

\[\sum_{i\in M}\left|w_{i}-\frac{1}{n}\right|\leqslant 2\varepsilon\,.\]

Hence

\[\sum_{i\in M}w_{i}\geqslant\frac{\left|M\right|}{n}-2\varepsilon\,.\]

By Lemma G.1 and a Chernoff bound,

\[\left|M\right|\geqslant 0.6\alpha-\varepsilon\geqslant\alpha/2\]

with probability \(1-\exp\left(-\alpha n\right)\). 

**Lemma G.3**.: _Let \(w\in\mathcal{W}_{2\varepsilon}\) for \(\varepsilon\lesssim\alpha\), and suppose that \(n\gtrsim\log(d)/\alpha^{2}\). Then with probability \(1-\exp\left(-\Omega\left(\alpha^{2}n\right)\right)\),_

\[\left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),\mu^{*}-\hat{\mu}^{w} \right\rangle\geqslant\frac{\alpha}{4}\|\hat{\mu}^{w}-\mu^{*}\|^{2}\,.\]

Proof.: Let \(u=\mu^{*}-\hat{\mu}(w)\). Let \(u^{\prime}\) and \(t\) be such that

\[\|u^{\prime}\| \leqslant\rho\sqrt{d}\,,\] \[\hat{\mu}(w) =\mu^{*}+t\cdot u^{\prime}\,.\]

It follows that \(t=\max\left\{1,\|u\|\right\}\). Clearly, \(\hat{\mu}(w)\) is the (unique) minimizer of \(\mathcal{L}^{w}\). Let \(\hat{\mu}^{\prime}=\mu^{*}+u^{\prime}\), by convexity of \(\mathcal{L}^{w}\) it holds that \(\mathcal{L}^{w}(\hat{\mu}^{\prime})\leqslant\mathcal{L}^{w}(\mu^{*})\). Since \(\|u^{\prime}\|\leqslant\rho\sqrt{d}\) it follows by Lemma G.2 that

\[\mathcal{L}^{w}\left(\mu^{*}\right)\geqslant\mathcal{L}^{w}\left(\hat{\mu}^{ \prime}\right)\geqslant\mathcal{L}^{w}\left(\mu^{*}\right)-\left\langle \nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\right\rangle+\frac{\alpha}{4}\left\| u^{\prime}\right\|^{2}\]

with probability at least \(1-\exp\left(-\Omega\left(\alpha n\right)\right)\). Rearranging yields

\[\|u^{\prime}\|\leqslant\frac{4}{\alpha}\cdot\|\nabla\mathcal{L}^{w}\left(\mu^ {*}\right)\|\,.\]

We next examine

\[\nabla\mathcal{L}^{w}\left(\mu^{*}\right) =\sum_{i=1}^{n}w_{i}f\left(\left(y_{i}\right)-\mu^{*}\right)\] \[=\sum_{i=1}^{n}w_{i}f\left(\boldsymbol{\eta}_{i}\right)+\sum_{i \in S_{b}}^{n}w_{i}\left[f\left(\left(y_{i}\right)-\mu^{*}\right)-f\left( \left(\boldsymbol{y}_{i}^{*}\right)-\mu^{*}\right)\right]\,.\]

By vector Bernstein inequality Fact J.2,

\[\left\|\sum_{i=1}^{n}w_{i}f\left(\boldsymbol{\eta}_{i}\right)\right\|\leqslant \left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\boldsymbol{\eta}_{i}\right)\right\|+40 \varepsilon\rho\sqrt{d}\leqslant 200\rho\sqrt{d}\cdot\left(\tau/\sqrt{n}+\tau^{2}/n \right)+40\varepsilon\rho\sqrt{d}\]

with probability at least \(1-\exp(-\tau^{2}/2)\).

The second term can be bounded as follows:

\[\sum_{i\in S_{b}}w_{i}\cdot\left[f\left(\mu^{*}-\left(y_{i}\right)\right)-f \left(\mu^{*}-\left(\boldsymbol{y}_{i}^{*}\right)\right)\right]\leqslant 40 \varepsilon\rho\sqrt{d}\,,\]

where we used that for all \(x,y\in\mathbb{R}\) it holds that \(\left|f(x)-f(y)\right|\leqslant 20\rho\sqrt{d}\) and \(\sum_{i\in S_{b}}w_{i}\leqslant 2\varepsilon\). Putting everything together we obtain

\[\|u^{\prime}\|\leqslant 200\frac{\rho\sqrt{d}}{\alpha}\cdot\left(\tau/\sqrt{n}+ \tau^{2}/n\right)+\frac{40\varepsilon\rho\sqrt{d}}{\alpha}\]

with probability \(1-\exp(-\tau^{2}/2)\). For \(\tau=\alpha\sqrt{n}/1000\), \(|u^{\prime}|<\rho\). Hence, \(t=1\) since otherwise

\[\|u\|=\|\hat{\mu}(w)-\mu^{*}\|=t\cdot\|u^{\prime}\|<\|u\|\,.\]It follows that \(u^{\prime}=u\) and thus \(\left\|\hat{\mu}(w)-\mu^{*}\right\|<\rho\sqrt{d}\).

By Lemma G.2

\[\mathcal{L}^{w}\left(\mu^{*}\right)\geqslant\mathcal{L}^{w}\left(\hat{\mu} \right)\geqslant\mathcal{L}^{w}\left(\mu^{*}\right)+\left\langle\nabla \mathcal{L}^{w}\left(\mu^{*}\right),-u\right\rangle+\frac{\alpha}{4}\left\|u \right\|^{2}\,,\]

Hence

\[\left\langle\nabla\mathcal{L}^{w}\left(\mu^{*}\right),u\right\rangle\geqslant \frac{\alpha}{4}\left\|u\right\|^{2}\,.\]

In the following lemma we show that Assumption 2 of Assumption D.1 is satisfied for \(\sigma=O(\rho\left\|\Sigma\right\|)\) with probability \(1-\delta\) as long as \(n\gtrsim\frac{d\log(d/\delta)}{\alpha^{2}}\).

**Lemma G.4**.: _Let \(\delta\in(0,1)\) and suppose that \(n\geqslant\log\left(1/\delta\right)\). Then with probability at least \(1-\delta\),_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\right\|\leqslant O \left(\rho\sqrt{\frac{d\log\left(d/\delta\right)}{n}}\right)\,.\]

Proof.: The lemma is a direct consequence of vector Bernstein inequality Fact J.2. 

In the following lemma we show that Assumption 3 of Assumption D.1 is satisfied for \(\sigma=O(\rho)\) with probability \(1-\delta\) as long as \(n\gtrsim d\log(d/\delta)\).

**Lemma G.5**.: _Let \(\delta\in(0,1)\) and suppose that \(n\geqslant\mathrm{r}(\Sigma)\log\left(d/\delta\right)\). Then_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\left(f\left(\bm{ \eta}_{i}\right)\right)^{\top}\right\|\leqslant O\left(\rho^{2}\left\|\Sigma \right\|\right)\]

_with probability at least \(1-\delta\)._

Proof.: Let \(s\left(\bm{\eta}\right)=\frac{h}{\left\|\bm{\eta}\right\|}\bm{\eta}=\frac{h}{ \left\|f\left(\bm{\eta}\right)\right\|}f\left(\bm{\eta}\right)\), where \(h=20\rho\sqrt{d}\). Note that \(\left\|s\left(\bm{\eta}\right)\right\|\geqslant\left\|f\left(\bm{\eta}\right)\right\|\). By Fact J.5 and since by our parametrization \(\mathrm{Tr}\,\Sigma=d\), it follows that

\[\sup_{\left\|u\right\|\leqslant 1}\mathbb{E}\left\langle f\left(\bm{\eta} \right),u\right\rangle^{2}\leqslant\sup_{\left\|u\right\|\leqslant 1}\mathbb{E} \left\langle s\left(\bm{\eta}\right),u\right\rangle^{2}\leqslant\left(20\rho \right)^{2}\left\|\Sigma\right\|\,.\]

Let \(t=\rho^{2}\|\Sigma\|\cdot\sqrt{\frac{\mathrm{r}(\Sigma)\log(d/\delta)}{n}} \leqslant\rho^{2}\|\Sigma\|\). It follows by Fact J.3 that

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f\left(\bm{\eta}_{i}\right)\left(f\left(\bm{ \eta}_{i}\right)\right)^{\top}\right\|\leqslant O\left(\rho^{2}\left\|\Sigma \right\|+t\right)\leqslant O\left(\rho^{2}\left\|\Sigma\right\|\right)\,,\]

with probability at least \(1-\delta\), where we also used that \(t=\rho^{2}\|\Sigma\|\cdot\sqrt{\frac{\mathrm{r}(\Sigma)\log(d/\delta)}{n}} \leqslant\rho^{2}\|\Sigma\|\) when applying Fact J.3. 

In the following lemma we show that Assumption 3 of Assumption E.2 is satisfied for \(\sigma=O(\rho\left\|\Sigma\right\|)\) with probability \(1-\delta\) as long as \(n\gtrsim r(\Sigma)^{k}\log(d/\delta)\).

**Lemma G.6**.: _Let \(k\) be a positive integer and suppose that \(n\geqslant 10\cdot\mathrm{r}(\Sigma)^{k}\cdot\log\left(d/\delta\right)\) and_

\[\left\|\Sigma\right\|_{F}\leqslant\frac{\mathrm{Tr}\left(\Sigma\right)}{10 \sqrt{k\log d}}\,.\]

_Then with probability at least \(1-\delta\), uniform distribution over the set \(\{\bm{\eta}_{1},\ldots,\bm{\eta}_{n}\}\) has \((2k,2k)\)-certifiable \(O\left(\rho\sqrt{k\left\|\Sigma\right\|}\right)\)-bounded \(f\)-moments._

Proof.: By Lemma K.4, \(\bm{\eta}\) has \((2k,2k)\)-certifiable \(O\left(\rho\sqrt{k\left\|\Sigma\right\|}\right)\)-bounded \(f\)-moments. Since \(\forall x\in\mathbb{R}^{d}\), \(\left\|f\left(x\right)\right\|\leqslant 20\rho\sqrt{d}\) the result follows from Lemma J.4.

### Unknown \(\rho\)

If \(\rho\) is not known and if \(\alpha=0.01\) (or arbitrary constant), we can first divide the \(n\) samples into \(2\) subsamples of size at least \(n^{\prime}=\lfloor n/2\rfloor\), and use the first half to learn \(\rho\), and the second half for mean estimation.

To do this, let us take the first \(n^{\prime}\) samples and again divide them into two subsamples of size at least \(n^{\prime\prime}=\lfloor n^{\prime}/2\rfloor\), and subtract the second half from the first half (if \(n^{\prime}\) is odd, let us ignore the last sample). Then we get \(n^{\prime\prime}\) (\(\varepsilon\)-corrupted) iid copies \(\bm{\xi}_{1},\ldots\bm{\xi}_{n^{\prime\prime}}\) of an elliptically distributed vector with location \(0\) and the scatter matrix \(\Sigma\) as the original distribution. Moreover, \(\bm{R}\) for this distribution satisfies \(\mathbb{P}\left[\bm{R}\leqslant 2\rho\sqrt{d}\right]\geqslant\alpha^{2}\). Let us compute and sort the norms of the samples, and let us take the maximal value \(\bm{b}\) among the smallest \(\alpha^{2}/2\) norms. If \(\varepsilon\leqslant 0.01\alpha\), then by Chernoff bound with probability at least \(1-\exp(-\Omega(\alpha n))\), \(\bm{b}\leqslant 2\rho\sqrt{d}\) and \(\mathbb{P}\left[\bm{R}\leqslant\bm{b}\mid\bm{b}\right]\geqslant\alpha^{2}/4\).

Now, we take the second \(n^{\prime}\) samples, divide them into two subsamples of size at least \(n^{\prime\prime}=\lfloor n^{\prime}/2\rfloor\), and add the second half to the first half (if \(n^{\prime}\) is odd, let us ignore the last sample). Since the initial noise distribution is symmetric about zero, we get (\(\varepsilon\)-corrupted) iid samples \(2\mu+\bm{\zeta}_{1},\ldots,2\mu+\bm{\zeta}_{n^{\prime\prime}}\), where \(\bm{\zeta}_{i}\) have the same distribution as \(\bm{\xi}_{i}\) described above. Hence we can use \(h=20\bm{b}\) in our proofs and get the same guarantees as if we used \(h=20\rho\sqrt{d}\) (up to a constant factor \(O(1/\alpha)\)).

## Appendix H Proof of Identifiability for Near-Optimal Error

In this section, we will prove the statement of identifiability underlying Theorem 1.4. We first introduce some notation. We use similar notation as in [119]. Let \(D\) be an \((\alpha,\rho)\)-semi-product distribution with location \(\mu^{*}\in\mathbb{R}^{d}\). Let \(\bm{y}_{1}^{*},\ldots,\bm{y}_{n}^{*}\) be \(n\) i.i.d.samples from \(D\) and let \(y_{1},\ldots,y_{n}\) be an \(\varepsilon\) corruption thereof. Further, let \(S_{g}=\left\{i\mid y_{i}=\bm{y}_{i}^{*}\right\}\) be the set of uncorrupted indices and \(S_{b}=[n]\setminus S_{g}\) be the set of corrupted ones. Let

\[\mathcal{W}_{\varepsilon}=\left\{w\in\mathbb{R}^{n}\;\middle|\;0\leqslant w \leqslant\tfrac{1}{n}\mathbbm{1},\left\|w-\tfrac{1}{n}\mathbbm{1}\right\|_{1 }\leqslant\varepsilon\right\}\,,\]

where \(\mathbbm{1}\) is the all-ones vector and the inequalities between vectors are entry-wise. Throughout the note we assume that \(\varepsilon\lesssim\alpha\). Note, that otherwise faithfully recovering \(\mu^{*}\) is impossible: For each coordinate, with high probability there is only an \(\Theta\left(\alpha\right)\)-fraction of the samples where the noise is small. If \(\varepsilon\gtrsim\alpha\), the adversary could corrupt precisely this fraction and erase all information about this coordinate.

Further, for \(h>0\), let \(F_{h}\colon\mathbb{R}^{d}\to\mathbb{R}\) be defined as \(x\mapsto F_{h}(x)=\sum_{j=1}^{d}\Phi_{h}(x_{j})\), where \(\Phi_{h}\) is the Huber loss

\[\Phi(t)=\begin{cases}\tfrac{1}{2}t^{2}\,,&\text{if }|t|\leqslant h\,,\\ h\cdot\left(|x|-h\right),&\text{otherwise.}\end{cases}\]

and denote by \(f=\nabla F\) its gradient. Note that in our case \(f\) is the derivative of the Huberloss applied entry-wise. The derivative of the Huber loss is \(\phi\colon\mathbb{R}\to\mathbb{R}\),

\[x\mapsto\begin{cases}t\,,&\text{if }|t|\leqslant h\,,\\ h\cdot\operatorname{sign}(t)\,,&\text{otherwise.}\end{cases}\]

For \(w\in\mathbb{R}^{n}\) with non-negative entries, we define \(\mathcal{L}^{w}\colon\mathbb{R}^{d}\to\mathbb{R}\) as

\[\mathcal{L}^{w}(\mu)\coloneqq\sum_{i=1}^{n}w_{i}F\left(\mu-y_{i}\right)\,.\]

It follows that \(\nabla\mathcal{L}^{w}(\mu)=\sum_{i=1}^{n}w_{i}f\left(\mu-y_{i}\right)\). We let

\[\hat{\mu}^{w}=\operatorname*{arg\,min}_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w}( \mu)\,.\]

and \(\hat{\Sigma}^{w}_{f}=\sum_{i=1}^{n}w_{i}f(y_{i}-\hat{\mu}^{w})f(y_{i}-\hat{\mu} ^{w})^{\top}\).

Similar as in Appendix G.1, by looking at differences of pairs of samples, we can assume that we have access to the (\(\varepsilon\)-corrupted) samples from the noise distribution, by replacing \(\alpha\) by \(\alpha^{2}\) in the definition of the semi-product distribution. This allows us to compute the following relevant

[MISSING_PAGE_FAIL:30]

where \(w_{i}^{\prime}=w_{i}\mathbf{1}_{[i\in S_{y}]}\). Taking expectations from both sides, we get

\[\mathbb{P}\left[\sum_{i=1}^{n}w_{i}^{\prime}f\left(\boldsymbol{\eta}_{i}^{*}+ \Delta\right)f\left(\boldsymbol{\eta}_{i}^{*}+\Delta\right)^{\top}\succeq \boldsymbol{D}-O\left(\frac{\varepsilon}{\alpha}+\varepsilon\log(1/\varepsilon) +\sqrt{\frac{d+\log\left(\frac{1}{\delta}\right)}{n}}\right)\cdot\mathrm{Id}_{d }\right]\geqslant 1-\delta/10\,.\]

Let \(\mathcal{N}_{\varepsilon^{\prime}}\) be an \(\varepsilon^{\prime}\)-net in \([-10\varepsilon/\alpha,10\varepsilon/\alpha]^{d}\) of size at most \(\left(\frac{20\varepsilon}{\varepsilon^{\prime}\alpha}\right)^{d}\). Since \(f\) is \(1\)-Lipschitz and is bounded by \(1\),

SInce the spectral norm can be only by at most \(\sqrt{d}\) factor larger than the infinity norm, by taking \(\varepsilon^{\prime}=\varepsilon\sqrt{d}/(100\alpha)\) and using a union bound, we get that with probability \(1-\delta/10\), for all \(\Delta\in[-10\varepsilon/\alpha,10\varepsilon/\alpha]^{d}\) (including \(\hat{\mu}^{w}\)),

\[\sum_{i=1}^{n}w_{i}^{\prime}f\left(\boldsymbol{\eta}_{i}^{*}+\Delta\right)f \left(\boldsymbol{\eta}_{i}^{*}+\Delta\right)^{\top}\succeq\boldsymbol{D}-O \left(\frac{\varepsilon}{\alpha}+\sqrt{\frac{d\log d+\log\left(\frac{1}{ \delta}\right)}{n}}+\frac{d\log d+\log\left(\frac{1}{\delta}\right)}{n}\right) \cdot\mathrm{Id}_{d}\,.\]

Note that \(\boldsymbol{D}\) is a diagonal matrix, and its diagonal entries are

\[\boldsymbol{D}_{jj}=\frac{1}{n}\sum_{i=1}^{n}f\left(\boldsymbol{\eta}_{ij}^{*} \right)^{2}\,.\]

Since \(f(\boldsymbol{\eta}_{j}^{*})\) are \(O(1)\)-sub-Gaussian, with probability at least \(1-\delta/10\),

\[\left\|\boldsymbol{D}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{*}\right)f\left( \boldsymbol{\eta}^{*}\right)^{\top}\right\|\leqslant O\left(\sqrt{\frac{\log \left(d/\delta\right)}{n}}\right)\,.\]

Hence we get the desired bound. 

Proof of Theorem h.1.: By Lemma F.1, \(\frac{\alpha}{4}\left\|\hat{\mu}^{w}-\mu^{*}\right\|^{2}\leqslant\left\langle \nabla\mathcal{L}^{w}(\mu^{*}),\mu^{*}-\hat{\mu}^{w}\right\rangle\). Hence, it is enough to bound \(\left\langle\nabla\mathcal{L}^{w}(\mu^{*}),\mu^{*}-\hat{\mu}^{w}\right\rangle\). Let \(u=\hat{\mu}^{w}-\mu^{*}\). Observe that using the Cauchy-Schwarz Inequality, it follows that

\[\left\langle\nabla\mathcal{L}^{w}(\mu^{*}),u\right\rangle =\sum_{i=1}^{n}w_{i}\left\langle f\left(\mu^{*}-y_{i}\right),u\right\rangle\] \[=\sum_{i=1}^{n}w_{i}\left\langle f\left(\mu^{*}-\boldsymbol{y}_{i }^{*}\right),u\right\rangle+\sum_{i\in S_{b}}w_{i}\left\langle f\left(\mu^{*}- y_{i}\right)-f\left(\mu^{*}-\boldsymbol{y}_{i}^{*}\right),u\right\rangle\] \[\leqslant\left\|\sum_{i=1}^{n}w_{i}f\left(\mu^{*}-\boldsymbol{y}_ {i}^{*}\right)\right\|\cdot\|u\|+\sqrt{\varepsilon}\cdot\sqrt{\sum_{i\in S_{b }}w_{i}\left\langle f\left(\mu^{*}-y_{i}\right)-f\left(\mu^{*}-\boldsymbol{y}_ {i}^{*}\right),u\right\rangle^{2}}\,.\]

Note that by the first point of Lemma H.2 it holds that \(\|\sum_{i=1}^{n}w_{i}f\left(\mu^{*}-\boldsymbol{y}_{i}^{*}\right)\|\leqslant O (\varepsilon\sqrt{\log(1/\varepsilon)}+\sqrt{\frac{d+\log(1/\delta)}{n}})\). For the second term, we can bound

\[\sum_{i\in S_{b}}w_{i}\left\langle f\left(\mu^{*}-y_{i}\right)-f \left(\mu^{*}-\boldsymbol{y}_{i}^{*}\right),u\right\rangle^{2}\] \[\lesssim\underbrace{\sum_{i\in S_{b}}w_{i}\left\langle f\left(\mu^ {*}-\boldsymbol{y}_{i}^{*}\right),u\right\rangle^{2}}_{\text{Term A}}+\underbrace{ \sum_{i\in S_{b}}w_{i}\left\langle f\left(\hat{\mu}^{w}-y_{i}\right),u\right\rangle ^{2}}_{\text{Term B}}+\underbrace{\sum_{i\in S_{b}}w_{i}\left\langle f\left( \mu^{*}-y_{i}\right)-f\left(\hat{\mu}^{w}-y_{i}\right),u\right\rangle^{2}}_{ \text{Term C}}\,.\]

Term A is at most \(O\left(\varepsilon\log(1/\varepsilon)+\sqrt{\frac{d+\log(1/\delta)}{n}}\right) \cdot\|u\|^{2}\) by combining the second point of Lemma H.2 with Fact J.7. To bound Term C, we use that \(f\) is \(1\)-Lipschitz to observe

\[\sum_{i\in S_{b}}w_{i}\left\langle f\left(\mu^{*}-y_{i}\right)-f\left(\hat{\mu} ^{w}-y_{i}\right),u\right\rangle^{2}\]\[\leqslant\left\|u\right\|^{2}\cdot\sum_{i\in S_{b}}w_{i}\left\|f\left(\mu^{ \ast}-y_{i}\right)-f\left(\hat{\mu}^{w}-y_{i}^{\ast}\right)\right\|^{2}\leqslant \left\|u\right\|^{2}\cdot\sum_{i\in S_{b}}w_{i}\left\|\mu^{\ast}-\hat{\mu}^{w} \right\|^{2}\leqslant\varepsilon\cdot\left\|u\right\|^{4}\,.\]

It remains to bound Term B. To this end, we use the third point of Lemma H.2 and obtain

\[\sum_{i\in S_{b}}w_{i}\left\langle f\left(\hat{\mu}^{w}-y_{i} \right),u\right\rangle^{2}=\sum_{i=1}^{n}w_{i}\left\langle f\left(\hat{\mu}^{w} -y_{i}\right),u\right\rangle^{2}-\sum_{i\in S_{g}}w_{i}\left\langle f\left(\hat {\mu}^{w}-\bm{y}_{i}^{\ast}\right),u\right\rangle^{2}\] \[\leqslant\left(\left\|\hat{\Sigma}_{f}-\mathbb{E}\,f\left(\bm{ \eta}^{\ast}\right)f\left(\bm{\eta}^{\ast}\right)^{\top}\right\|+O\left( \varepsilon\log(1/\varepsilon)/\alpha\right)+\sqrt{\frac{d\log d+\log\left( \frac{1}{\delta}\right)}{n}}\right)\cdot\left\|u\right\|^{2}\,.\]

Putting the above bounds together, we have shown that

\[\left\langle\nabla\mathcal{L}^{w}(\mu^{\ast}),u\right\rangle\leqslant\left\|u \right\|\cdot O\left(\frac{\varepsilon\sqrt{\log(1/\varepsilon)}}{\sqrt{ \alpha}}+\sqrt{\frac{d\log(d)+\log(1/\delta)}{n}}+\varepsilon\|u\|+\sqrt{ \varepsilon\left\|\hat{\Sigma}_{f}-\mathbb{E}\,f\left(\bm{\eta}^{\ast} \right)f\left(\bm{\eta}^{\ast}\right)^{\top}\right\|}\right)\,.\]

(Note that we also use \(\sqrt{\varepsilon}\cdot\sqrt{\varepsilon\log(1/\varepsilon)+\sqrt{\frac{d+\log (1/\delta)}{n}}}\leqslant\varepsilon\sqrt{\log(1/\varepsilon)}+\sqrt{\frac{d+ \log(1/\delta)}{n}}\).) Since \(\left\langle\nabla\mathcal{L}^{w}(\mu^{\ast}),u\right\rangle\geqslant\frac{ \alpha}{4}\left\|u\right\|^{2}\) and \(\varepsilon\lesssim\alpha\), rearranging and dividing by \(\left\|u\right\|\) yields that

\[\left\|u\right\|\leqslant O\left(\sqrt{\frac{d\log(d)+\log(1/\delta)}{\alpha ^{2}n}}+\frac{1}{\alpha^{3/2}}\cdot\sqrt{\varepsilon\left(\left\|\hat{ \Sigma}_{f}-\mathbb{E}\,f\left(\bm{\eta}^{\ast}\right)f\left(\bm{\eta}^{\ast} \right)^{\top}\right\|+\varepsilon\log(1/\varepsilon)\right)}\right)\,.\]

## Appendix I Filtering Algorithm for Near-Optimal Error

In this section, we will use the proof of identifiability from Appendix H to obtain an algorithm which efficiently recovers \(\mu^{\ast}\) up to (nearly) optimal error. In particular, we show that

**Theorem I.1**.: _Let \(C>0\) be a large enough absolute constant. Let \(\varepsilon,\alpha>0\) be such that \(\sqrt[3]{\varepsilon}\leqslant\frac{\alpha}{C}\). Let \(D\) be an \((\alpha,\rho)\)-product distribution with location \(\mu^{\ast}\in\mathbb{R}^{d}\). Let \(n\geqslant C\cdot\frac{d\log(d)+\log(1/\delta)}{\alpha^{6}\varepsilon^{2}\log(1/ \varepsilon)}\). There exists an algorithm running in time \(n^{O(1)}d^{O(1)}\), such that given an \(\varepsilon\)-corrupted sample of \(D\) of size \(n\) and \(\rho\)9, with probability at least \(1-\delta\) it outputs \(\hat{\mu}\) satisfying_

Footnote 9: We remark that in the case of unknown \(\rho\) but known \(\alpha\), we can first estimate \(\rho\) from the corrupted samples similar as in Appendix G.1, at the cost of a worse dependence on \(\alpha\).

\[\left\|\mu^{\ast}-\hat{\mu}\right\|\leqslant O\left(\rho\cdot\left[\sqrt{ \frac{d\log(d)+\log(1/\delta)}{\alpha^{4}n}}+\frac{\varepsilon\sqrt{\log(1/ \varepsilon)}}{\alpha^{3}}\right]\right)\,.\]

The algorithm we propose is based on the by-now-standard filtering approach. We closely follow the exposition in [11]. To describe the algorithm, let \(C>0\) be some universal constant and for \(\mu\in\mathbb{R}^{d},w\in\mathcal{W}_{\varepsilon}\) let

\[\hat{\mu}\left(w\right)=\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w}\left(\mu \right)\,,\qquad\qquad\Sigma_{f}\left(w\right)=\sum_{i=1}^{n}w_{i}f\left(\hat{ \mu}\left(w\right)-y_{i}\right)f\left(\hat{\mu}\left(w\right)-y_{i}\right)^{ \top}\,.\]

Note that since we assume \(\rho\) to be known, we can without loss of generality assume that \(\rho=1\) by scaling. Our algorithm is the following.

**Algorithm I.2** (Filtering Algorithm).:

**Input:**\(\varepsilon\)-corrupted sample \(y_{1},\ldots,y_{n}\) and \(\rho,\gamma>0\).

**Output:** Location estimate \(\hat{\mu}\).

* Let \(w^{(0)}=\frac{1}{n}\mathbbm{1}_{n}\).
* Compute \(\hat{\mu}^{(0)}=\min_{\mu\in\mathbb{R}^{d}}\mathcal{L}^{w^{(0)}}\left(\mu\right)\) and \(\Sigma_{f}^{(0)}=\Sigma_{f}\left(w^{(0)}\right)\).
* Let \(t=0\).
* Compute \(v^{(t)}\) the top eigenvector of \(\Sigma_{f}^{(t)}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{*}\right)f\left( \boldsymbol{\eta}^{*}\right)^{\top}\).
* For \(i\in[n]\), compute \(\tau_{i}^{(t)}=\left\langle v^{(t)},f\left(\hat{\mu}^{(t)}-y_{i}\right) \right\rangle^{2}\).
* Sort the \(\tau_{i}\). Assume that \(\tau_{1}\geqslant\ldots\geqslant\tau_{n}\).
* Let \(N\) be the smallest index such that \(\sum_{i\leqslant N}w_{i}^{(t)}>2\varepsilon\) and let \(\tau_{\max}=\max_{i\in[n]}\tau_{i}\).
* For \(1\leqslant i\leqslant N\), set \(w_{i}^{(t+1)}=(1-\frac{\tau_{i}}{\tau_{\max}})\cdot w_{i}^{(t)}\) and for \(N<i\leqslant n\), set \(w_{i}^{(t+1)}=w_{i}^{(t)}\).
* Compute \(\hat{\mu}^{(t+1)}=\min_{\mu\in\mathbb{R}^{d}}f^{w^{(t+1)}}\left(\mu\right)\) and \(\Sigma_{f}^{(t+1)}=\Sigma_{f}\left(w^{(t+1)}\right).\)
* \(t\gets t+1\).
* Output \(\hat{\mu}^{(t)}\).

We will use the following lemma whose proof we will give at the end of this section

**Lemma I.3**.: _Assume that \(\|\Sigma_{f}^{(t)}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{*}\right)f\left( \boldsymbol{\eta}^{*}\right)^{\top}\|>C\varepsilon\log(1/\varepsilon)/\alpha^{3}\) and_

\[\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(t)}\right)<\sum_{i\in S_{b}}\left( \frac{1}{n}-w_{i}^{(t)}\right)\,.\]

_Then_

\[\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(t+1)}\right)<\sum_{i\in S_{b}} \left(\frac{1}{n}-w_{i}^{(t+1)}\right)\,.\]

With this in hand, we will prove Theorem I.1.

Proof of Theorem I.1.: First note, that every iteration can clearly be implented to run in time \(n^{O(1)}d^{O(1)}\). We will show that the algorithm terminates after at most \(\lceil 2\varepsilon n\rceil\) iterations. Assume towards a contradiction that the algorithm does not terminate after \(T=\lceil 2\varepsilon n\rceil\) iterations. Note that the number entries of \(w^{(t)}\) that are equal to 0 increases by at least 1 in every iteration. Hence, after \(T\) iterations we have set at least \(\varepsilon n\) entries of \(w\) to zero whose index lies in \(S_{g}\). By assumption that the algorithm didn't terminate and Lemma I.3 it holds that

\[\varepsilon\leqslant\sum_{i\in S_{g}}\left(\frac{1}{n}-w_{i}^{(T)}\right)< \sum_{i\in S_{b}}\left(\frac{1}{n}-w_{i}^{(T)}\right)\leqslant\frac{|S_{b}|}{ n}\leqslant\varepsilon\,.\]

which is a contradiction.

Next, we prove the correctness of the algorithm. Let \(T\) be the index of the last iteration of the algorithm before termination. Note that by our invariant

\[\left\|\frac{1}{n}-w^{(T)}\right\|_{1}=\sum_{i\in S_{g}}\frac{1}{n}-w_{i}^{(T )}+\sum_{i\in S_{b}}\frac{1}{n}-w_{i}^{(T)}<2\sum_{i\in S_{b}}\frac{1}{n}-w_{i }^{(T)}\leqslant 2\varepsilon\,.\]

Since also \(0\leqslant w^{(T)}\leqslant\frac{1}{n}\), it follows that \(w^{(T)}\in\mathcal{W}_{2\varepsilon}\). By Theorem H.1 and since \(\|\Sigma_{f}^{(T)}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{*}\right)f\left( \boldsymbol{\eta}^{*}\right)^{\top}\|\leqslant C\varepsilon\log(1/\varepsilon) /\alpha^{3}\) it follows that

\[\left\|\hat{\mu}^{T}-\mu^{*}\right\|\leqslant O\left(\rho\cdot\left[\sqrt{ \frac{d\log(d)+\log(1/\delta)}{n}}+\frac{\varepsilon\sqrt{\log(1/\varepsilon) }}{\alpha^{3}}\right]\right)\,.\]Lastly, we will prove Lemma I.3.

Proof of Lemma I.3.: On a high level, we will show that each iteration of the filtering algorithm removes relatively more weight from corrupted samples than from uncorrupted ones. For simplicity, let \(w=w^{(t)}\) and \(w^{\prime}=w^{(t+1)}\). Also, let \(\hat{\mu}=\hat{\mu}^{(t)},\Sigma_{f}=\Sigma_{f}^{(t)}\), and \(v=v^{(t)}\). Note, that it is enough to show that

\[\sum_{i\in S_{g}}w_{i}-w_{i}^{\prime}<\sum_{i\in S_{b}}w_{i}-w_{i}^{\prime}\,.\]

Let \(N\) be the smallest index such that \(\sum_{i\leqslant N}w_{i}>\varepsilon\) and denote by \(T=\{1,\ldots,N\}\) Further, recall that \(w_{i}^{\prime}=\left(1-\frac{\tau_{i}}{\tau_{\max}}\right)w_{i}\) if \(i\in T\) and \(w_{i}^{\prime}=w_{i}\) otherwise. Thus, for \(i\in T\), it holds that \(w_{i}-w_{i}^{\prime}=\frac{1}{\tau_{\max}}\tau_{i}w_{i}\), while for \(i\not\in T\) it holds that \(w_{i}-w_{i}^{\prime}=0\). Hence, the above condition is equivalent to

\[\sum_{i\in S_{g}\cap T}w_{i}\tau_{i}<\sum_{i\in S_{b}\cap T}w_{i}\tau_{i}\,.\]

We will show that the above is indeed true in two steps. We will show that

1. \(\sum_{i\in S_{g}\cap T}w_{i}\tau_{i}\leqslant O\left(\varepsilon\log(1/ \varepsilon)+\frac{\varepsilon^{2}}{\alpha^{3}}\cdot\left\|\Sigma_{f}-\mathbb{E }\,f\left(\boldsymbol{\eta}^{\star}\right)f\left(\boldsymbol{\eta}^{\star} \right)^{\top}\right\|\right)\),
2. \(\sum_{i\in S_{b}\cap T}w_{i}\tau_{i}\geqslant\Omega\left(\left\|\Sigma_{f}- \mathbb{E}\,f\left(\boldsymbol{\eta}^{\star}\right)f\left(\boldsymbol{\eta}^{ \star}\right)^{\top}\right\|\right)\).

This implies our claim since by assumption \(\left\|\Sigma_{f}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{\star}\right)f\left( \boldsymbol{\eta}^{\star}\right)^{\top}\right\|>C\varepsilon\log(1/\varepsilon) /\alpha^{3}>C\varepsilon\log(1/\varepsilon)\) and \(\frac{\varepsilon^{2}}{\alpha^{3}}\leqslant\frac{\varepsilon}{\alpha^{3}}\) is sufficiently small.

Upper Bounding the Contribution of \(S_{g}\cap T\)Recall that \(n\geqslant\frac{d\log(d)+\log(1/\delta)}{\alpha^{6}\varepsilon^{2}\log(1/ \varepsilon)}\). Thus, by definition of \(\tau_{i}\) and since \(f\) is 1-Lipschitz it follows that

\[\sum_{i\in S_{g}\cap T}w_{i}\tau_{i} =\sum_{i\in S_{g}\cap T}w_{i}\langle f(\hat{\mu}-\boldsymbol{y}_{ i}^{\star}),v\rangle^{2}\] \[\leqslant O\left(\sum_{i\in S_{g}\cap T}w_{i}\langle f(\mu^{ \star}-\boldsymbol{y}_{i}^{\star}),v\rangle^{2}+\sum_{i\in S_{g}\cap T}w_{i} \langle f(\hat{\mu}-\boldsymbol{y}_{i}^{\star})-f(\mu^{\star}-\boldsymbol{y} _{i}^{\star}),v\rangle^{2}\right)\] \[\leqslant O\left(\varepsilon\log(1/\varepsilon)+\sum_{i\in S_{g} \cap T}w_{i}\|\hat{\mu}-\mu^{\star}\|^{2}\right)\] \[\leqslant O\left(\varepsilon\log(1/\varepsilon)+\varepsilon\cdot \left(\frac{1}{\alpha^{3}}\cdot\left[\varepsilon\left\|\Sigma_{f}-\mathbb{E} \,f\left(\boldsymbol{\eta}^{\star}\right)f\left(\boldsymbol{\eta}^{\star} \right)^{\top}\right\|+\varepsilon^{2}\log(1/\varepsilon)\right]\right)\right)\] \[\leqslant O\left(\varepsilon\log(1/\varepsilon)+\frac{\varepsilon^ {2}}{\alpha^{3}}\cdot\left\|\Sigma_{f}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{ \star}\right)f\left(\boldsymbol{\eta}^{\star}\right)^{\top}\right\|\right)\,,\]

where in the second inequality we used the last property of Fact J.7 and in the third inequality we used Theorem H.1.

Lower Bounding the Contribution of \(S_{b}\cap T\)Our strategy will be to first show that the contribution of all points in \(S_{b}\), also those not necessarily in \(T\), must be large and then show that the contribution of points in \(S_{b}\setminus T\) is in fact small. Together they will yield our claim. Note that by definition of \(v\), it holds that

\[\sum_{i=1}^{n}w_{i}\tau_{i}=v^{\top}\left(\Sigma_{f}-\mathbb{E}\,f\left( \boldsymbol{\eta}^{\star}\right)f\left(\boldsymbol{\eta}^{\star}\right)^{\top} \right)v+\mathbb{E}\,\langle f\left(\boldsymbol{\eta}^{\star}\right),v\rangle^{ 2}=\mathbb{E}\,\langle f\left(\boldsymbol{\eta}^{\star}\right),v\rangle^{2}+ \left\|\Sigma_{f}-\mathbb{E}\,f\left(\boldsymbol{\eta}^{\star}\right)f\left( \boldsymbol{\eta}^{\star}\right)^{\top}\right\|\,.\]Let \(\gamma:=\mathbb{E}\left\langle f\left(\bm{\eta}^{*}\right),v\right\rangle^{2}\). Note that \(\gamma=O(1)\). By the second point in Lemma H.2, Cauchy-Schwarz inequality and Theorem H.1,

\[\left|\sum_{i\in S_{g}}w_{i}\tau_{i}-\gamma\right| =\left|\sum_{i\in S_{g}}w_{i}\langle f(\hat{\mu}-y_{i}),v\rangle^ {2}-\gamma\right|\] \[\leqslant\left|\sum_{i\in S_{g}}w_{i}\langle f(\mu^{*}-\bm{y}_{i }^{*}),v\rangle^{2}-\gamma\right|+\left|\sum_{i\in S_{g}}w_{i}\langle f(\hat{ \mu}-\bm{y}_{i}^{*})-f(\mu^{*}-\bm{y}_{i}^{*}),v\rangle^{2}\right|\] \[\qquad+2\left|\sum_{i\in S_{g}}w_{i}\langle f(\mu^{*}-\bm{y}_{i}^ {*}),v\rangle\left(\langle f(\hat{\mu}-\bm{y}_{i}^{*}),v\rangle-\langle f(\mu ^{*}-\bm{y}_{i}^{*}),v\rangle\right)\right|\] \[\leqslant O\left(\varepsilon\log(1/\varepsilon)+\left\|\hat{\mu}- \mu^{*}\right\|^{2}+\sqrt{\gamma}\cdot\left\|\hat{\mu}-\mu^{*}\right\|\right)\] \[\leqslant O\left(\varepsilon\log(1/\varepsilon)+\frac{ \varepsilon}{\alpha^{3}}\left\|\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*} \right)f\left(\bm{\eta}^{*}\right)^{\top}\right\|+\frac{\varepsilon^{2}\log(1/ \varepsilon)}{\alpha^{3}}\right)\] \[\leqslant O\left(\frac{\varepsilon}{\alpha^{3}}\left\|\Sigma_{f}- \mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top} \right\|+\frac{\varepsilon^{2}\log(1/\varepsilon)}{\alpha^{3}}+\left\|\hat{ \mu}-\mu^{*}\right\|\right)\,.\]

Hence, since \(\left\|\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*} \right)^{\top}\right\|\) is at least a sufficiently large multiple of \(\varepsilon\log(1/\varepsilon)/\alpha^{3}\) and \(\frac{\varepsilon}{\alpha^{3}}\) is a sufficiently small constant, it follows that the first two terms are at most \(0.0001\Big{\|}\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{ \eta}^{*}\right)^{\top}\Big{\|}\). The second term is at most

\[O\left(\sqrt{\frac{\varepsilon}{\alpha^{3}}\left\|\Sigma_{f}-\mathbb{E}\,f \left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\right\|}\right)\,.\]

We claim that this is at most \(0.0001\Big{\|}\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{ \eta}^{*}\right)^{\top}\Big{\|}\) as well. Indeed, if it were larger, we would have that \(\left\|\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*} \right)^{\top}\right\|=O(\frac{\varepsilon}{\alpha^{3}})\), a contradiction. Thus, we get that

\[\sum_{i\in S_{b}}w_{i}\tau_{i}\geqslant 0.99\Big{\|}\Sigma_{f}-\mathbb{E}\,f \left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\Big{\|}\,.\]

It remains to upper bound the contribution of indices \(i\) in \(S_{b}\setminus T\). Note that

\[\sum_{i\in S_{b}\cap T}w_{i}\tau_{i}=\sum_{i\in S_{b}}w_{i}\tau_{i}-\sum_{i \in S_{b}\setminus T}w_{i}\tau_{i}\,.\]

We claim that the scores for \(i\not\in T\) cannot be too large. Indeed, since \(\sum_{i\in T}w_{i}>2\varepsilon\) and \(\sum_{i\in S_{b}\cap T}w_{i}\leqslant\varepsilon\) it follows that \(\sum_{i\in S_{q}\cap T}w_{i}\geqslant\varepsilon\). Thus, since we assume the \(\tau_{i}\) are sorted in decreasing order it follows that for \(i\not\in T\) it holds that

\[\tau_{i}\leqslant\frac{1}{\sum_{i\in S_{g}\cap T}w_{i}}\sum_{i\in S_{g}\cap T}w _{i}\tau_{i}\leqslant O\left(\log(1/\varepsilon)+\frac{\varepsilon}{\alpha^{3} }\cdot\left\|\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{ \eta}^{*}\right)^{\top}\right\|\right)\,.\]

It follows that

\[\sum_{i\in S_{b}\setminus T}w_{i}\tau_{i}\leqslant O\left(\varepsilon\log(1/ \varepsilon)+\frac{\varepsilon^{2}}{\alpha^{3}}\cdot\left\|\Sigma_{f}-\mathbb{ E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\right\|\right)\]

Putting everything together we have shown that

\[\sum_{i\in S_{b}\cap T}w_{i}\tau_{i}\geqslant 0.9\left\|\Sigma_{f}-\mathbb{E}\,f \left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\right\|\]

as desired.

Finally, it is easy to see that if we can only compute an estimation \(\hat{v}\) of \(v\) such that

\[\hat{v}^{\top}\left(\Sigma_{f}-\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left( \bm{\eta}^{*}\right)^{\top}\right)\hat{v}=\left\|\Sigma_{f}-\mathbb{E}\,f \left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\right\|\pm O( \varepsilon)\,,\]

the proof still works, so we can use the \(O(\varepsilon)\)-close estimation \(\hat{\Sigma}\) of \(\mathbb{E}\,f\left(\bm{\eta}^{*}\right)f\left(\bm{\eta}^{*}\right)^{\top}\) and compute the top eigenvector of \(\Sigma_{f}-\hat{\Sigma}\).

Concentration Bounds

Throughout this section we will use the following versions of the Matrix Bernstein Inequality. A proof of a slightly more general version can be found in [17, Corollary 6.2.1].

**Fact J.1**.: _Let \(L,B>0,\bm{M}\in\mathbb{R}^{d\times d}\) be a symmetric random matrix and \(\bm{M}_{1},\ldots,\bm{M}_{n}\) i.i.d. copies of \(\bm{M}\). Suppose that \(\left\|\bm{M}\right\|\leqslant L\). Further, assume that \(\left\|\bm{\Sigma}\,\bm{M}^{2}\right\|\leqslant B\). Then the estimator \(\bm{M}=\frac{1}{n}\sum_{i=1}^{n}\bm{M}_{i}\) satisfies for all \(t>0\)_

\[\mathbb{P}\left(\left\|\bm{\bar{M}}-\mathbb{E}\,\bm{M}\right\|\geqslant t \right)\leqslant 2d\cdot\exp\left(-\frac{t^{2}n}{2B+2Lt}\right)\,.\]

**Fact J.2**.: _Let \(\bm{x}_{1},\ldots,x_{n}\) be iid \(d\)-dimensional random vectors such that \(\left\|\bm{x}_{1}\right\|\leqslant L\) with probability \(1\). Then_

\[\mathbb{P}\left(\left\|\bm{\bar{v}}-\mathbb{E}\,\bm{v}\right\|\geqslant t \right)\leqslant 2d\cdot\exp\left(-\frac{t^{2}n}{2L^{2}+2Lt}\right)\,.\]

**Fact J.3**.: _Let \(L>0,\bm{v}\in\mathbb{R}^{d}\) be a random vector and \(\bm{v}_{1},\ldots,\bm{v}_{n}\) i.i.d. copies of \(\bm{v}\). Suppose that \(\left\|\bm{v}\right\|\leqslant L\). Then the estimator \(\tilde{\bm{\Sigma}}=\frac{1}{n}\sum_{i=1}^{n}\bm{x}_{i}\bm{x}_{i}^{\top}\) satisfies for all \(t>0\)_

\[\mathbb{P}\left(\left\|\bm{\bar{\Sigma}}-\mathbb{E}\,\bm{x}_{1}x_{1}^{\top} \right\|\geqslant t\right)\leqslant 2d\cdot\exp\left(-\frac{t^{2}n}{2L \left\|\Sigma\right\|+2Lt}\right)\,.\]

We will use this to prove that sampling preserves bounded \(f\)-moments.

**Lemma J.4**.: _Let \(k,\ell,n,d\in\mathbb{N}_{\geqslant 1}\), \(\sigma,\rho>0\), \(\delta\in(0,1)\). Let \(\nu\) be a distribution with \((2k,\ell)\)-certifiable \(\sigma\)-bounded \(f\)-moments, and \(\bm{\eta}_{1},\ldots,\bm{\eta}_{n}\sim_{i.i.d.}\nu\). Suppose that \(n\geqslant 10\cdot d^{k}\cdot\log\left(d/\delta\right)\) and that \(\forall x\in\mathbb{R}^{d}\), \(\left\|f\left(x\right)\right\|\leqslant\rho\sqrt{d}\)._

_Then with probability at least \(1-\delta\), uniform distribution over the set \(\{\bm{\eta}_{1},\ldots,\bm{\eta}_{n}\}\) has \((2k,\ell)\)-certifiable \((\sigma+\rho)\)-bounded \(f\)-moments. In other words, with probability at least \(1-\delta\),_

\[\left|\tfrac{v}{\ell}\,\left(v^{\otimes k}\right)^{\top}\left(\frac{1}{n} \sum_{i=1}^{n}f(\bm{\eta}_{i})^{\otimes k}\left(f(\bm{\eta}_{i})^{\otimes k} \right)^{\top}\right)v^{\otimes k}\leqslant\left((\sigma+\rho)\cdot\left\|v \right\|\right)^{2k}\,.\]

_If \(\bm{\eta}_{1},\ldots,\bm{\eta}_{n}\) follow an elliptical distribution with scatter matrix \(\Sigma\) and \(\sigma=O(\sqrt{k\|\Sigma\|})\), we only need \(n\gtrsim(\frac{t(\Sigma)}{k})^{k}\cdot\log(d/\delta)\).10 Further, the uniform distribution has \(2\sigma\)-bounded \(f\)-moments._

Footnote 10: Here we use the parametrization from Appendix G, where \(\operatorname{Tr}\Sigma=d\)

Proof.: Let \(T_{k}=\mathbb{E}\,f(\bm{\eta})^{\otimes k}\left(f(\bm{\eta})^{\otimes k} \right)^{\top}\). Note that \(\{\bm{\eta}\}\) having \((2k,\ell)\)-certifiable \(\sigma\)-bounded \(f\)-moments is equivalent to the fact that the following SoS proof exists

\[\left|\tfrac{v}{\ell}\,\left(v^{\otimes k}\right)^{\top}T_{k}v^{\otimes k} \leqslant(\sigma\cdot\left\|v\right\|)^{2k}\,.\]

Denote \(\bm{\bar{T}}_{k}=\frac{1}{n}\sum_{i=1}^{n}f(\bm{\eta}_{i})^{\otimes k}\left(f (\bm{\eta}_{i})^{\otimes k}\right)^{\top}\). Notice that by Fact K.3

\[\left|\tfrac{v}{\ell}\,\left(v^{\otimes k}\right)^{\top}\bm{\bar{T}}_{k}v^{ \otimes k}=\left(v^{\otimes k}\right)^{\top}\left[T_{k}+\bm{\bar{T}}_{k}-T_{k} \right]v^{\otimes k}\leqslant\left\|v\right\|^{2k}\cdot\left(\sigma^{2k}+\left \|\bm{\bar{T}}_{k}-T_{k}\right\|\right)\,.\]

Hence, it is enough to bound \(\left\|\bm{\bar{T}}_{k}-T_{k}\right\|\). Our strategy will be to use Fact J.1. To this end, let \(\bm{M}=f(\bm{\eta})^{\otimes k}\left(f(\bm{\eta})^{\otimes k}\right)^{\top}\) and for \(i\in[n]\), let \(\bm{M}_{i}=f(\bm{\eta}_{i})^{\otimes k}\left(f(\bm{\eta}_{i})^{\otimes k} \right)^{\top}\). This implies that \(T_{k}=\mathbb{E}\,\bm{M}\) and \(\bm{\bar{T}}_{k}=\frac{1}{n}\sum_{i=1}^{n}\bm{M}_{i}\). Note that since the entries of \(f\) are bounded by \(h\) in magnitude it follows that for each \(i\)

\[\left\|\bm{M}_{i}\right\|=\left\|f(\bm{\eta}_{i})\right\|^{2k}\leqslant\rho^{2 k}d^{k}\,.\]

Further,

\[\bm{M}^{2}=\left\|f(\bm{\eta})\right\|^{2k}\cdot f(\bm{\eta})^{\otimes k}\left(f (\bm{\eta})^{\otimes k}\right)^{\top}\preceq\rho^{2k}d^{k}\cdot f(\bm{\eta})^{ \otimes k}\left(f(\bm{\eta})^{\otimes k}\right)^{\top}\,.\]

Since \(\left\|\mathbb{E}\,f(\bm{\eta})^{\otimes k}\left(f(\bm{\eta})^{\otimes k} \right)^{\top}\right\|\leqslant\sigma^{2k}\), it holds that

\[\left\|\mathbb{E}\,\bm{M}^{2}\right\|\leqslant\rho^{2k}d^{k}\cdot\left\|\mathbb{ E}\,f(\bm{\eta})^{\otimes k}\left(f(\bm{\eta})^{\otimes k}\right)^{\top} \right\|\leqslant\left(\rho\sigma\right)^{2k}d^{k}\,.\]By Fact J.1,

\[\mathbb{P}\left(\left\|\bar{\bm{T}}_{k}-T_{k}\right\|\geqslant\frac{10\rho^{2k}d^ {k}\cdot\log\left(d/\delta\right)}{n}+\sigma^{k}\sqrt{\frac{10\rho^{2k}d^{k} \cdot\log\left(d/\delta\right)}{n}}\right)\leqslant\delta\,.\] (J.1)

Hence with probability at least \(1-\delta\),

\[\left\|\tfrac{v}{\ell}\left(v^{\otimes k}\right)^{\top}\bar{\bm{T}}_{k}v^{ \otimes k}\leqslant\left\|v\right\|^{2k}\cdot\left(\sigma^{2k}+\rho^{2k}+\rho^ {k}\sigma^{k}\right)\leqslant\left\|v\right\|^{2k}\cdot\left(\sigma+\rho\right) ^{2k}\]

To see the improved sample complexity for elliptical distributions, note that since \(\mathrm{r}(\Sigma)=\frac{\mathrm{Tr}(\Sigma)}{\left\|\Sigma\right\|}=\frac{d} {\left\|\Sigma\right\|}\) and \(\sigma=O(\rho\sqrt{k\|\Sigma\|})\), it follows that

\[\rho^{2k}d^{k}=\rho^{2k}\|\Sigma\|^{k}\mathrm{r}(\Sigma)^{k}\cdot\frac{k^{k} }{k^{k}}=\sigma^{2k}\cdot\left(\frac{\mathrm{r}(\Sigma)}{k}\right)^{k}\,.\]

Thus, if \(n\gtrsim(\frac{t(\Sigma)}{k})^{k}\cdot\log(d/\delta)\), then \(\left\|\bar{\bm{T}}_{k}-T_{k}\right\|\) in Eq. (J.1) is at most \(\sigma^{2k}\) with probability at least \(1-\delta\). 

**Fact J.5**.: _[_DTV16_]_ _Let \(\bm{\eta}\) be an elliptically distributed vector with location \(0\) and scatter matrix \(\Sigma\). Let \(s\left(\bm{\eta}\right)=\frac{\sqrt{\mathrm{Tr}(\Sigma)}}{\left\|\bm{\eta} \right\|}\bm{\eta}\). Then_

\[\left\|\mathbb{E}\,s\left(\bm{\eta}\right)s\left(\bm{\eta}\right)^{\top} \right\|\leqslant\left\|\Sigma\right\|\,.\]

**Fact J.6**.: _[_Bec09_]_ _Let \(\bm{x}\sim N(0,\Sigma)\). Then for all \(t>0\),_

\[\mathbb{P}\left[\left\|\bm{x}\right\|^{2}\leqslant\mathrm{Tr}(\Sigma)-\sqrt{2 t}\left\|\Sigma\right\|_{\mathrm{F}}\right]\leqslant\exp(-t)\]

_and_

\[\mathbb{P}\left[\left\|\bm{x}\right\|^{2}\geqslant\mathrm{Tr}(\Sigma)+\sqrt{2 t}\left\|\Sigma\right\|_{\mathrm{F}}+t\left\|\Sigma\right\|\right]\leqslant\exp(-t)\,.\]

**Fact J.7**.: _[_DK22_]_ _Let \(\bm{\xi}_{1},\ldots,\bm{\xi}_{n}\) be independent zero mean \(1\)-sub-Gaussian \(d\)-dimensional vectors. Let \(\varepsilon,\delta\in(0,1)\), and suppose that \(n\geqslant d+\log(1/\delta)\). Let \(\mathcal{W}_{\varepsilon}=\left\{w\in\mathbb{R}^{d}\ \middle|\ 0\leqslant w_{i} \leqslant 1/n\,,\sum_{i=1}^{n}\left|w_{i}-1/n\right|\leqslant\varepsilon\right\}\). Then, with probability \(1-\delta\), for every \(w\in\mathcal{W}_{\varepsilon}\),_

\[\left\|\sum_{i=1}^{n}w_{i}\bm{\xi}_{i}\right\|\leqslant O\left(\varepsilon \sqrt{\log(1/\varepsilon)}+\sqrt{\frac{d+\log(1/\delta)}{n}}\right)\,,\]

_and_

\[\left\|\sum_{i=1}^{n}w_{i}\bm{\xi}_{i}\bm{\xi}_{i}{}^{\top}-\frac{1}{n}\sum_{ i=1}^{n}\mathbb{E}\,\bm{\xi}\bm{\xi}^{\top}\right\|\leqslant O\left( \varepsilon\log(1/\varepsilon)+\sqrt{\frac{d+\log(1/\delta)}{n}}\right)\,.\]

_Similarly, with probability \(1-\delta\) it holds that for every set \(T\subseteq[n]\) with\(|T|\leqslant\varepsilon n\) and every \(w\in\mathcal{W}_{\varepsilon}\) it holds that_

\[\left\|\sum_{i\in T}w_{i}\xi_{i}\xi_{i}^{\top}\right\|\leqslant O\left( \varepsilon\log(1/\varepsilon)+\sqrt{\frac{d+\log(1/\delta)}{n}}\right)\,.\]

**Lemma J.8**.: _Let \(\varepsilon,\delta\in(0,1)\) and suppose that \(n\geqslant d\log d+\log(1/\delta)\). Let \(\bm{\xi}_{1},\ldots,\bm{\xi}_{n}\) be independent zero mean \(1\)-sub-Gaussian \(d\)-dimensional vectors, and let \(a_{1},\ldots,a_{n}\in\mathbb{R}^{d}\) be fixed (non-random) vectors such that \(\|a_{i}\|\leqslant 100\varepsilon\sqrt{d}/\alpha\) for some \(\alpha\in(0,1)\). Suppose in addition that \(\left\|\bm{\xi}_{i}\right\|\leqslant 10\sqrt{d}\) with probability 1. Let \(\mathcal{W}_{\varepsilon}=\left\{w\in\mathbb{R}^{d}\ \middle|\ 0\leqslant w_{i} \leqslant 1/n\,,\sum_{i=1}^{n}\left|w_{i}-1/n\right|\leqslant\varepsilon\right\}\). Then, with probability \(1-\delta\), for every \(w\in\mathcal{W}_{\varepsilon}\),_

\[\sum_{i=1}^{n}w_{i}\left(\bm{\xi}_{i}+a_{i}\right)\left(\bm{\xi}_{i}+a_{i} \right)^{\top}\succeq\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\,\bm{\xi}\bm{\xi}^{ \top}-O\left(\varepsilon\log(1/\varepsilon)+\varepsilon/\alpha+\sqrt{\frac{d \log\left(d\right)+\log(1/\delta)}{n}}\right)\,.\]

Proof.: Note that

\[\sum_{i=1}^{n}w_{i}\left(\bm{\xi}_{i}+a_{i}\right)\left(\bm{\xi}_{i}+a_{i} \right)^{\top}=\sum_{i=1}^{n}w_{i}\bm{\xi}_{i}\bm{\xi}_{i}{}^{\top}+\sum_{i=1}^{ n}w_{i}\left(\bm{\xi}_{i}a_{i}^{\top}+a_{i}\bm{\xi}_{i}^{\top}+a_{i}a_{i}^{\top} \right)\,.\]Let us bound \(u\sum_{i=1}^{n}w_{i}\left(\bm{\xi}_{i}a_{i}^{\top}+a_{i}\bm{\xi}_{i}^{\top}+a_{i}a _{i}^{\top}\right)u^{\top}\) for all unit vectors \(u\in\mathbb{R}^{d}\). Note that since \(\mathcal{W}_{\varepsilon}\) is a polytope, it is enough to bound this value only for \(w\) that are its vertices, that is, the indicators of sets of size at least \((1-\varepsilon)\,n\), normalized by \(1/n\). Let \(S\subset[n]\) be an arbitrary fixed (non-random) set, and let \(u\in\mathbb{R}^{d}\) be an arbitrary fixed (non-random) unit vector. Then

\[u^{\top}\sum_{i\in S}\tfrac{1}{n}\left(\bm{\xi}_{i}a_{i}^{\top}+a_{i}\bm{\xi} _{i}^{\top}+a_{i}a_{i}^{\top}\right)u=\tfrac{1}{n}\sum_{i\in S}2\langle\bm{\xi }_{i},u\rangle\langle\bm{a}_{i},u\rangle+\langle\bm{a}_{i},u\rangle^{2}\,.\]

By Hoeffding's inequality, with probability at least \(1-\delta\),

\[\sum_{i\in S}2\langle\bm{\xi}_{i},u\rangle\langle\bm{a}_{i},u\rangle+\langle \bm{a}_{i},u\rangle^{2}\geqslant-O\left(\left\|A_{S}u\right\|\sqrt{\log\left( 1/\delta\right)}\right)+\left\|A_{S}u\right\|^{2}\geqslant-O\left(\log(1/ \delta)\right)\,.\]

where \(A_{S}\in\mathbb{R}^{|S|\times d}\) is the matrix with rows \(a_{i}\) for \(i\in S\).

By union bound over all sets of size at least \((1-\varepsilon)\,n\), with probability \(1-\delta\), for every set \(S\) of size at least \((1-\varepsilon)\,n\),

\[\sum_{i\in S}2\langle\bm{\xi}_{i},u\rangle\langle\bm{a}_{i},u\rangle+\langle \bm{a}_{i},u\rangle^{2}\geqslant-O\left(\varepsilon n\log\left(1/\varepsilon \right)+\log(1/\delta)\right)\,.\]

Let \(\mathcal{N}\) be an \((0.01/d)\)-net in the \(d\)-dimensional unit ball of size \(\left|\mathcal{N}\right|\leqslant\left(300d\right)^{d}\). Then by union bound over \(\mathcal{N}\), we get that with with probability \(1-\delta\), for every set \(S\) of size at least \((1-\varepsilon)\,n\), and for every \(u\in\mathcal{N}\),

\[\sum_{i\in S}2\langle\bm{\xi}_{i},u\rangle\langle\bm{a}_{i},u\rangle+\langle \bm{a}_{i},u\rangle^{2}\geqslant-O\left(\varepsilon n\log\left(1/\varepsilon \right)+d\log(d)+\log(1/\delta)\right)\,.\]

Now, if some unit \(u\) is \((0.01/d)\)-close to \(u^{\prime}\in\mathcal{N}\), we get

\[\tfrac{1}{n}\sum_{i\in S}2\langle\bm{\xi}_{i},u\rangle\langle \bm{a}_{i},u\rangle \geqslant\tfrac{1}{n}\sum_{i\in S}2\langle\bm{\xi}_{i},u^{\prime} \rangle\langle\bm{a}_{i},u^{\prime}\rangle-O\left(\max_{i}\lVert\bm{\xi}_{i} \rVert\cdot r\cdot\lVert u-u^{\prime}\rVert\right)\] \[\geqslant-O\left(\varepsilon\log(1/\varepsilon)+\varepsilon/ \alpha+\frac{d\log\left(d\right)+\log(1/\delta)}{n}\right)\,.\]

Using the concentration of \(\sum_{i=1}^{n}w_{i}\bm{\xi}_{i}\bm{\xi}_{i}{}^{\top}\) from Fact J.7, we get the desired bound. 

## Appendix K Sum-of-Squares Toolkit

The following fact can be found in [13, Lemma A.2]

**Fact K.1**.: _For all \(k\in\mathbb{N}_{\geqslant 1}\) it holds that_

\[\left|\tfrac{X,Y}{2k}\left(X+Y\right)^{2k}\leqslant 2^{2k-1}\cdot\left(X^{2k}+ Y^{2K}\right)\,.\]

The next fact shows that sum-of-squares captures the Cauchy-Schwarz Inequality, a proof can be found in [13, Lemma A.1]

**Fact K.2**.: _For all \(n\in\mathbb{N}_{\geqslant 1}\) it holds that_

\[\left|\tfrac{X_{1},Y_{1},\ldots,X_{n},Y_{n}}{2}\left(\sum_{i=1}^{n}X_{i}Y_{i} \right)^{2}\leqslant\left(\sum_{i=1}^{n}X_{i}^{2}\right)\left(\sum_{j=1}^{n}Y_ {i}^{2}\right)\]

**Fact K.3**.: _Let \(M\in\mathbb{R}^{n\times n}\) be a symmetric matrix and \(X\) be an \(n\)-vector of formal variables. Then it holds that_

\[\left|\tfrac{X}{2}\,\left\langle X,MX\right\rangle\leqslant\left\lVert M \right\rVert\left\lVert X\right\rVert^{2}\,.\]

Proof.: We can rewrite the inequality as \(X^{\top}\left(\left\lVert M\right\rVert\cdot I_{n}-M\right)X\geqslant 0\). Since \(\left\lVert M\right\rVert\cdot I_{n}-M\) is positive semi-definite, it follows that there exists a matrix \(L\in\mathbb{R}^{n\times n}\) such that \(\left\lVert M\right\rVert\cdot I_{n}-M=LL^{\top}\). Hence,

\[X^{\top}\left(\left\lVert M\right\rVert\cdot I_{n}-M\right)X=\left\lVert LX \right\rVert^{2}\,,\]

which is a sum of squares in \(X\)

**Lemma K.4**.: _Let \(k\) be a positive integer, and let \(\boldsymbol{\eta}\) be elliptical \(d\)-dimensional vector with location \(0\) and scatter matrix \(\Sigma\) that satisfies_

\[\left\|\Sigma\right\|_{F}\leqslant\frac{\operatorname{Tr}\left(\Sigma\right)}{ 10\sqrt{k\log d}}\,.\]

_Let \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) be a projection onto the Euclidean ball of radius \(R\) centered at \(0\). Then \(\boldsymbol{\eta}\) has \((2k,2k)\)-certifiable \(q\)-bounded \(f\)-moments, where_

\[q=2R\sqrt{\frac{k\left\|\Sigma\right\|}{\operatorname{Tr}\left(\Sigma\right)} }\,.\]

Proof.: Let \(v_{1},\ldots,v_{d}\) be variables and consider the polynomial \(\mathbb{E}\left\langle f\left(\boldsymbol{\eta}\right),v\right\rangle^{2k}\). Let \(s\left(\boldsymbol{\eta}\right)=\frac{R}{\left\|\boldsymbol{\eta}\right\|} \boldsymbol{\eta}=\frac{R}{\left\|f\left(\boldsymbol{\eta}\right)\right\|}f \left(\boldsymbol{\eta}\right)\). Since spherical projection of elliptical distributions depends only on \(\Sigma\) (see Theorem 35 in [11]), \(s\left(\boldsymbol{\eta}\right)\) has the same distribution as \(s\left(\boldsymbol{w}\right)\), where \(\boldsymbol{w}\sim N\left(0,\Sigma\right)\). Hence

\[\left|\tfrac{v}{2k}\,\left\langle f\left(\boldsymbol{\eta}\right),v\right\rangle^{2k} \leqslant\left\langle s\left(\boldsymbol{\eta}\right),v\right\rangle^{2k}\] \[=R^{2k}\left\langle\frac{\boldsymbol{w}}{\left\|\boldsymbol{w} \right\|},v\right\rangle^{2k}\] \[=R^{2k}\cdot\mathbf{1}_{\left[\|\boldsymbol{w}\|^{2}>\frac{ \operatorname{Tr}\left(\Sigma\right)}{2}\right]}\left\langle\frac{ \boldsymbol{w}}{\left\|\boldsymbol{w}\right\|},v\right\rangle^{2k}+R^{2k}\cdot \mathbf{1}_{\left[\|\boldsymbol{w}\|^{2}\leqslant\frac{\operatorname{Tr} \left(\Sigma\right)}{2}\right]}\left\langle\frac{\boldsymbol{w}}{\left\| \boldsymbol{w}\right\|},v\right\rangle^{2k}\] \[\leqslant\left(\frac{2R^{2}}{\operatorname{Tr}\left(\Sigma \right)}\right)^{k}\left\langle\boldsymbol{w},v\right\rangle^{2k}+R^{2k}\cdot \mathbf{1}_{\left[\|\boldsymbol{w}\|^{2}\leqslant\frac{\operatorname{Tr} \left(\Sigma\right)}{2}\right]}\cdot\left\|v\right\|^{2k}.\]

By Fact J.6,

\[\mathbb{P}\left[\|\boldsymbol{w}\|^{2}\leqslant\operatorname{Tr}\left(\Sigma \right)/2\right]\leqslant d^{-2k}\,.\]

Hence

\[\left|\tfrac{v}{2k}\,\mathbb{E}\left\langle f\left(\boldsymbol{\eta}\right),v \right\rangle^{2k}\leqslant\left(\frac{2R^{2}}{\operatorname{Tr}\left(\Sigma \right)}\right)^{k}\mathbb{E}\left\langle\boldsymbol{w},v\right\rangle^{2k}+ \left(R/d\right)^{2k}\left\|v\right\|^{2k}.\]

Since \(\boldsymbol{w}\) is \((2k,2k)\)-certifiably \(1\)-subgaussian (see Lemma 5.1 in [12]), and since \(\left\|\Sigma\right\|/\operatorname{Tr}\left(\Sigma\right)\geqslant 1/d\),

\[\left|\tfrac{v}{2k}\,\mathbb{E}\left\langle f\left(\boldsymbol{\eta}\right),v \right\rangle^{2k}\leqslant 2\cdot\left(\frac{2R^{2}k\left\|\Sigma\right\|}{ \operatorname{Tr}\left(\Sigma\right)}\right)^{k}\left\|v\right\|^{2k}.\]