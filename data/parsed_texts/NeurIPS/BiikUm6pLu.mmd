# Truncated Variance-Reduced Value Iteration

 Yujia Jin

Stanford University

yujiajin@stanford.edu

&Ishani Karmarkar

Stanford University

ishanik@stanford.edu

&Aaron Sidford

Stanford University

sdford@stanford.edu

&Jiayi Wang

Stanford University

jyw@stanford.edu

###### Abstract

We provide faster randomized algorithms for computing an \(\varepsilon\)-optimal policy in a discounted Markov decision process with \(\mathcal{A}_{\mathrm{tot}}\)-state-action pairs, bounded rewards, and discount factor \(\gamma\). We provide an \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-2}])\)-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in \(\tilde{O}(1)\)-time, and an \(\tilde{O}(s+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-2})\)-time algorithm in the offline setting where the probability transition matrix is known and \(s\)-sparse. These results improve upon the prior state-of-the-art which either ran in \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-3}])\) time ([1, 2]) in the sampling setting, \(\tilde{O}(s+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3})\) time ([3]) in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming. We achieve our results by building upon prior stochastic variance-reduce value iteration methods [1, 2]. We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps. Our method is essentially model-free and can be implemented in \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\)-space when given generative model access. Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods.

## 1 Introduction

Markov decision processes (MDPs) are a fundamental mathematical model for decision making under uncertainty. They play a central role in reinforcement learning and prominent problems in computational learning theory (see e.g., [4, 5, 6, 7]). MDPs have been studied extensively for decades ([8, 9]), and there have been numerous algorithmic advances in efficiently optimizing them ([3, 1, 2, 10, 11, 12, 13, 14]).

In this paper, we consider the standard problem of _optimizing a discounted Markov Decision Process (DMDP)_\(\mathcal{M}=(\mathcal{S},\mathcal{A},\boldsymbol{P},\boldsymbol{r},\gamma)\). We consider the _tabular setting_ where there is a known finite set of _states_\(\mathcal{S}\) and at each state \(s\in\mathcal{S}\) there is a finite, non-empty, set of _actions_, \(\mathcal{A}_{s}\) for an agent to choose from; \(\mathcal{A}=\{(s,a):s\in\mathcal{S},a\in\mathcal{A}_{s}\}\) denotes the full set of state action pairs and \(\mathcal{A}_{\mathrm{tot}}:=|\mathcal{A}|\geq|\mathcal{S}|\,.\) The agent proceeds in rounds \(t=0,1,2,\ldots.\) In each round \(t\), the agent is in state \(s_{t}\in\mathcal{S}\); chooses action \(a_{t}\in\mathcal{A}_{s_{t}}\), which yields a known reward \(\boldsymbol{r}_{t}=\boldsymbol{r}_{s_{t},a}\in[0,1]\); and transitions to random state \(s_{t+1}\) sampled (independently) from a (potentially) unknown distribution \(\boldsymbol{p}_{a}(s_{t})\in\Delta^{\mathcal{S}}\) for round \(t+1\), where \(\boldsymbol{p}_{a}(s_{t})^{\top}\) is the \((s_{t},a)\)-th row of \(\boldsymbol{P}\in[0,1]^{\mathcal{A}\times\mathcal{S}}\). The goal is to compute an _\(\varepsilon\)-optimal policy_, where a (deterministic) policy \(\pi\), is a mapping from each state \(s\in\mathcal{S}\) to an action \(\pi(s)\in\mathcal{A}_{s}\) and is _\(\varepsilon\)-optimal_ if for every initial \(s_{0}\in\mathcal{S}\) the _expected discounted reward of \(\pi\)_\(\mathbb{E}[\sum_{t\geq 0}r_{t}\gamma^{t}]\) is atleast \(\bm{v}_{s_{0}}^{*}-\varepsilon\). Here, \(\bm{v}_{s_{0}}^{*}\) is the maximum expected discounted reward of any policy applied starting from initial state \(s_{0}\) and \(\bm{v}^{*}\in\mathbb{R}^{\mathcal{S}}\) is called the _optimal value_ of the MDP.

Excitingly, a line of work [15; 16; 2; 3; 17; 18] recently resolved the query complexity for solving DMDPs (up to polylogarithmic factors) in what we call the _sample setting_ where the transitions \(\bm{p}_{a}(s)\) are accessible only through a _generative model_ ([16]). A _generative model_ is an oracle which when queried with any \(s\in\mathcal{S}\) and \(a\in\mathcal{A}_{s}\) returns a random \(s^{\prime}\in\mathcal{S}\) sampled independently from \(\bm{p}_{a}(s)\)[19]. It was shown in [18] that for all \(\varepsilon\in(0,(1-\gamma)^{-1}]\) there is an algorithm which computes an \(\varepsilon\)-optimal policy with probability \(1-\delta\) using \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\varepsilon^{-2})\) queries where we use \(\tilde{O}(\cdot)\) to hide polylogarithmic factors in \(\mathcal{A}_{\mathrm{tot}}\), \(\varepsilon^{-1}\), \((1-\gamma)^{-1}\), and \(\delta^{-1}\). This result improved upon a prior result of [17] which achieved the same query complexity for \(\varepsilon\in[0,(1-\gamma)^{-1/2}]\), of [2] which achieved this query complexity for \(\varepsilon\in[0,1]\), and of [16] which achieved it for \(\varepsilon\in[0,(|\mathcal{S}|\,(1-\gamma))^{-1/2}]\). This query complexity is known to be optimal in the worst case (up to polylogarithmic factors) due to lower bounds of [16] (and extensions of [20]), which established that the optimal query complexity for finding \(\varepsilon\)-optimal policies with probability \(1-\delta\) is \(\Omega(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\varepsilon^{-2}\log(\mathcal{ A}_{\mathrm{tot}}\delta^{-1}))\).

Interestingly, recent state-of-the-art results [17; 18] (as well as [16]) are _model-based_: they query the oracle for every state-action pair, use the resulting samples to build an empirical model of the MDP, and then solve this empirical model. State-of-the-art computational complexities for the methods are then achieved by applying high-accuracy, algorithms for optimizing MDPs in what we call the _offline setting_, when the transition probabilities are known [2; 17].

Correspondingly, obtaining optimal query complexities for large \(\varepsilon\), e.g., \(\varepsilon\gg 1\), comes with certain costs. This setting is of interest when the goal is to efficiently compute a coarse approximation of the optimal policy. Model-based methods use space \(\Omega(\mathcal{A}_{\mathrm{tot}}\cdot\min((1-\gamma)^{-3}\varepsilon^{-2},| \mathcal{S}|))\)-rather than the \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\) memory used by _model-free_ methods (e.g., [2; 3; 21]), which run stochastic, low memory analogs of classic popular algorithms for solving DMDPs (e.g., value iteration). Moreover, although state-of-the-art model-based methods use \(\Omega(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\varepsilon^{-2})\)_samples,_ the state-of-the-art _runtime_ to compute the optimal policy is either \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\max\{1,\varepsilon^{-2} \})\) (using [2]) or has a larger larger polynomial dependence on \(\mathcal{A}_{\mathrm{tot}}\) and \(|\mathcal{S}|\) by using interior point methods (IPMs) for linear programming (see Section 1.1). Consequently, in the worst case, the _runtime cost_ per sample is more than polylogarithmic for \(\varepsilon\) sufficiently larger than \(1\), and it is natural to ask if this can be improved.

These costs are connected to the state-of-the-art runtimes for optimizing DMDPs in the offline setting. Ignoring IPMs (discussed in Section 1.1), the state-of-the-art runtime for optimizing a DMDP is \(\tilde{O}(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3})\) due to [2] where \(\mathrm{nnz}(\bm{P})\) denotes the number of non-zero entries in \(\bm{P}\), i.e., the number of triplets \((s,s^{\prime},a)\) where taking action \(a\in\mathcal{A}_{s}\) at state \(s\in\mathcal{S}\) has a non-zero probability of transitioning to \(s^{\prime}\in\mathcal{S}\). This method is essentially model-free; it simply performs a variant of stochastic value iteration where passes on \(\bm{P}\) are used to reduce the variance of sampling and can be implemented in \(\tilde{O}(\mathcal{A})\)-space given access to a generative model and the ability to multiply \(\bm{P}\) with vectors. The difficulty in further improving the runtimes in the sample setting and improving the performance of model-free methods seems connected to the difficulty in improving the additive \(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\)-term in this runtime (see the discussion in Section 1.2.)

In this paper, we ask whether these complexities can be improved. _Is it possible to lower the memory requirements of near-optimal query algorithms for large \(\varepsilon\)? Can we improve the runtime for optimizing MDPs in the offline setting and can we improve the computational cost per sample in computing optimal policies in DMDPs?_ More broadly, _is it possible to close the sample-complexity gap between model-free and model-based methods for optimizing DMDPs?_

### Our results

In this paper, we show how to answer each of these motivating questions in the affirmative. We provide faster algorithms for optimizing DMDPs in both the sample and offline setting that are implementable in \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\)-space provided suitable access to the input. In addition to computing \(\varepsilon\)-optimal policies, these methods also compute _\(\varepsilon\)-optimal values_: we call any \(\bm{v}\in\mathbb{R}^{\mathcal{S}}\) a _value vector_ and say that it is \(\varepsilon\)-optimal if \(\|\bm{v}-\bm{v}^{*}\|_{\infty}\leq\varepsilon\).

Here we present our main results on algorithms for solving DMDPs in sample setting and in the offline setting and compare to prior work. For simplicity of comparison, we defer any discussion and comparison of DMDP algorithms that use general IPMs for linear program to the end of this section. The state-of-the-art such IPM methods obtain improved running times but use \(\Omega(|\mathcal{S}|^{2})\) space and \(\Omega(|\mathcal{S}|^{2})\) time and use general-purpose linear system solvers. As such they are perhaps qualitatively different from the more combinatorial or dyanmic-programming based methods, e.g., value iteration and stochastic value iteration, more commonly discussed in this introduction.

In the sample setting, our main result is an algorithm that uses \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1-\gamma) ^{-2}])\) samples and time and \(O(\mathcal{A}_{\mathrm{tot}})\)-space. It improves upon the prior, non-IPM, state-of-the-art which uses \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-3}])\) time [3] and nearly matches the state-of-the-art sample complexity for all \(\varepsilon=O((1-\gamma)^{-1/2})\). See Table 2 for a more complete comparison.

**Theorem 1.1**.: _In the sample setting, there is an algorithm that uses \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-2}])\) samples and time and \(O(\mathcal{A}_{\mathrm{tot}})\) space, and computes an \(\varepsilon\)-optimal policy and \(\varepsilon\)-optimal values with probability \(1-\delta\)._

Particularly excitingly, the algorithm in Theorem 1.1 runs in time nearly-linear in the number of samples whenever \(\varepsilon=O((1-\gamma)^{-1/2})\) and therefore, provided querying the oracle costs \(\Omega(1)\), has a near-optimal runtime for such \(\varepsilon\)! Prior to this work such a near-optimal, non-IPM, runtime (for non-trivially small \(\gamma\)) was only known for \(\varepsilon=\tilde{O}(1)\) ([2]). Similarly, Theorem 1.1 shows that there are model-free algorithms (which for our purposes we define as an \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\) space algorithm) which are nearly-sample optimal whenever \(\varepsilon=O((1-\gamma)^{-1/2})\). Previously this was only known for \(\varepsilon=\tilde{O}(1)\). As discussed in prior-work ([18, 17]), this large \(\varepsilon\) regime is potentially of particular importance in large-scale learning settings, where one would like to _quickly_ compute a _coarse_ approximation of the optimal policy.

In the offline setting, our main result is an algorithm that uses \(\tilde{O}(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-2})\) time. It improves upon the prior, non-IPM, state-of-the-art which use \(\tilde{O}(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3})\) time ([2]). See Table 1 for a more complete comparison with prior work.

**Theorem 1.2**.: _In the offline setting, there is an algorithm that uses \(\tilde{O}(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-2})\) time, and computes an \(\varepsilon\)-optimal policy and \(\varepsilon\)-optimal values with probability \(1-\delta\)._

The method of Theorem 1.2 runs in nearly-linear time when \((1-\gamma)^{-1}\leq(\mathrm{nnz}(\bm{P})/\mathcal{A}_{\mathrm{tot}})^{1/2}\), i.e., the discount factor is not too small relative to the average sparsity of rows of the transition matrix. Prior to this paper, such nearly-linear, non-IPM, runtimes (for non-trivially small \(\gamma\)) were only known for \((1-\gamma)^{-1}\leq(\mathrm{nnz}(\bm{P})/\mathcal{A}_{\mathrm{tot}})^{1/3}\) ([2]). Thus, Theorem 1.2 expands the set of DMDPs which can be solved in nearly-linear time. The space usage and input access for this offline algorithm differs from the algorithm in Theorem 1.1 in that the algorithm in Theorem 1.2 assumes that access to the transition \(\bm{P}\) is provided as input and uses this to compute matrix-vector products with value vectors. The algorithm in Theorem 1.2 also requires access to samples from the generative model; if access to the generative model is not provided as input, then using the access to \(\bm{P}\), the algorithm can build a \(\tilde{O}(\mathrm{nnz}(\bm{P}))\) data-structure so that queries to the generative model can be implemented in \(\tilde{O}(1)\) time (e.g., see discussion in [2]). Hence, if matrix-vector products and queries to the generative model can be implemented in \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\)-space then so can the algorithm in Theorem 1.2.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Algorithm** & **Runtime** & **Space** \\ \hline Value Iteration [22, 11] & \(\tilde{O}\left(\mathrm{nnz}(\bm{P})(1-\gamma)^{-1}\right)\) & \(\tilde{O}(\mathrm{nnz}(\bm{P}))\) \\ \hline Empirical QVI [16] & \(\tilde{O}\left(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3} \varepsilon^{-2}\right)\) & \(\tilde{O}(\mathrm{nnz}(\bm{P}))\) \\ \hline Randomized Primal-Dual Method [23] & \(\tilde{O}\left(\mathrm{nnz}(\bm{P})+E\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-4} \varepsilon^{-2}\right)\) & \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\) \\ \hline High Precision Variance- & & \\ Reduced Value Iteration [2] & \(\tilde{O}\left(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3}\right)\) & \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\) \\ \hline Algorithm 4**This Paper** & \(\tilde{O}\left(\mathrm{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-2}\right)\) & \(\tilde{O}(\mathcal{A}_{\mathrm{tot}})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Running times to compute \(\varepsilon\)-optimal policies in the offline setting. In this table, \(E\) denotes an upper bound on the ergodicity of the MDP.

[MISSING_PAGE_FAIL:4]

Value iteration.Our approach stems from classic value-iteration method ([22, 11]) for computing \(\varepsilon\)-optimal and its more modern \(Q\)-value and stochastic counterparts ([16, 3, 29, 30, 31, 32]). As the name suggests, value iteration proceeds in iterations \(t=0,1,\ldots\) computing _values_, \(\bm{v}^{(t)}\in\mathbb{R}^{S}\). Starting from initial \(\bm{v}^{(0)}\in\mathbb{R}^{S}\), in iteration \(t\geq 1\), the value vector \(\bm{v}^{(t)}\) is computed as the result of applying the (Bellman) value operator \(\mathcal{T}:\mathbb{R}^{S}\mapsto\mathbb{R}^{S}\), i.e.,

\[\bm{v}^{(t)}\leftarrow\mathcal{T}(\bm{v}^{(t-1)}\text{ where }\mathcal{T}(\bm{v})(s) :=\max_{a\in\mathcal{A}_{s}}(\bm{r}_{a}(s)+\gamma\bm{p}_{a}(s)^{\top}\bm{v}) \text{ for all }s\in\mathcal{S}\text{ and }\bm{v}\in\mathbb{R}^{\mathcal{S}}\,.\] (1)

It is well-known that the value operator is \(\gamma\)-contractive and therefore, \(\left\|\mathcal{T}(\bm{v})-\bm{v}^{*}\right\|_{\infty}\leq\gamma\left\|\bm{v }-\bm{v}^{*}\right\|_{\infty}\) for all \(v\in\mathbb{R}^{\mathcal{S}}\) ([11, 22, 2]). If we initialize \(\bm{v}^{(0)}=\bm{0}\) then since \(\left\|\bm{v}^{*}\right\|_{\infty}\leq(1-\gamma)^{-1}\)[22, 11], we see that \(\left\|\bm{v}^{(t)}-\bm{v}^{*}\right\|_{\infty}\leq\gamma^{t}\|\bm{v}^{(0)}- \bm{v}^{*}\|_{\infty}\leq\gamma^{t}(1-\gamma)^{-1}\leq(1-\gamma)^{-1}\exp(-t(1 -\gamma))\). Thus, \(\bm{v}^{(t)}\) are \(\varepsilon\)-optimal values for any \(t\geq(1-\gamma)^{-1}\log(\varepsilon^{-1}(1-\gamma)^{-1})\). This yields an \(\tilde{O}(\operatorname{nnz}(\bm{P})(1-\gamma)^{-1})\) time algorithm in the offline setting.

Stochastic value iteration and variance reduction.To improve on the runtime of value iteration and apply it in the sample setting, a line of work implements _stochastic_ variants of value iteration ([16, 2, 3, 23, 17, 18]). Those methods take approximate value iteration steps where the _expected utilities_\(\bm{p}_{a}(s)^{\top}\bm{v}\) in (1) for each state-action pair are replaced by a _stochastic estimate_ of the expected utilities. In particular, note that \(\bm{p}_{a}(s)^{\top}\bm{v}=\mathbb{E}_{i\sim\bm{p}_{a}(s)}\,\bm{v}_{i}\), i.e., the expected value of \(\bm{v}_{i}\) where \(i\) is drawn from the distribution given by \(\bm{p}_{a}(s)\). This is compatible in the sample setting, as computing \(\bm{v}_{i}\) for \(i\) drawn from \(\bm{p}_{a}(s)\) yields an unbiased estimate of \(\bm{p}_{a}(s)^{\top}\bm{v}\) with \(1\) query and \(O(1)\) time.

State-of-the-art model-free methods in the sample setting ([3]) and non-IPM runtimes in the offline setting ([3]) improve further by more carefully approximating the _expected utilities_\(\bm{p}_{a}(s)^{\top}\bm{v}\) of each state-action pair \((s,a)\in\mathcal{A}\). Broadly, given an arbitrary \(\bm{v}^{(0)}\) they first compute \(\bm{x}\in\mathbb{R}^{\mathcal{A}}\) that approximates \(\bm{P}\bm{v}^{(0)}\), i.e., \(\bm{x}_{a}(s)\) approximates \([\bm{P}\bm{v}^{(0)}]_{(s,a)}=\bm{p}_{a}(s)^{\top}\bm{v}^{(0)}\) for all \((s,a)\in\mathcal{A}\). In the offline setting, \(\bm{x}=\bm{P}\bm{v}^{(0)}\) can be computed directly in \(O(\operatorname{nnz}(\bm{P}))\)-time. In the sample setting, \(\bm{x}\approx\bm{P}\bm{v}^{(0)}\) can be approximated to sufficient accuracy using multiple queries for each state-action pair. Then, in each iteration \(t\geq 1\) of the algorithm, fresh samples are taken to compute \(\bm{g}^{(t)}\approx\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)})\) and perform the following update:

\[\bm{v}^{(t)}(s)\leftarrow\max_{a\in\mathcal{A}_{s}}(\bm{r}_{a}(s)+\gamma(\bm {x}_{a}(s)+\bm{g}_{a}(s)^{(t)})\text{ for all }s\in\mathcal{S}\text{ and }\bm{v}\in\mathbb{R}^{\mathcal{S}}\,.\] (2)

This approach is advantageous because sampling errors for estimating \(\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)})\) depend on the magnitude of \(\bm{v}^{(t-1)}-\bm{v}^{(0)}\). After approximately computing \(\bm{x}\), the remaining task of computing \(\bm{g}^{(t)}\approx\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)})\) so that \(\bm{x}+\bm{g}^{(t)}\approx\bm{P}\bm{v}^{(t-1)}\) may be easier than the task of directly estimating \(\bm{P}\bm{v}^{(t)}\) (since \(\bm{v}^{(t-1)}-\bm{v}^{(0)}\) is smaller in magnitude than \(\bm{v}^{(t)}\) entrywise.) Due to similarities of this approach to variance-reduced optimization methods, e.g. ([33, 34]), this technique is called _variance reduction_[2].

The works [2, 3], showed that if \(\bm{x}\) is computed sufficiently accurately and \(\bm{v}^{(0)}\) are \(\alpha\)-optimal values then applying (2) for \(t=\Theta((1-\gamma)^{-1})\) yields \(\bm{v}^{(t)}\) that is \(\alpha/2\)-optimal in just \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3})\) time and samples: [2] leverages this technique to compute \(\varepsilon\)-optimal values in the offline setting in \(\tilde{O}(\operatorname{nnz}(\bm{P})+\mathcal{A}_{\mathrm{tot}}(1-\gamma)^{-3})\) time. [3] uses a similar approach to compute \(\varepsilon\)-optimal values in \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-3})\) time and samples in the sample setting. A key difference in [2] and [3] is the accuracy to which they must approximate the initial utility \(\bm{x}\approx\bm{P}\bm{v}^{(0)}\).

Recursive variance reduction.To improve upon the prior model-free approaches of [2, 3] we improve how exactly the variance reduction is performed. We perform a similar scheme as in (2) and use essentially the same techniques as in [3, 2] towards estimating \(\bm{x}\). Where we differ from prior work is in how we estimate the change in approximate utilities \(\bm{g}^{(t)}\approx\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)})\). Rather than _directly_ sampling to estimate this difference we instead sample to estimate each individual \(\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(t)})\) and maintain the sum. Concretely, for \(t\geq 1\), we compute \(\bm{\Delta}^{(t)}\) such that

\[\bm{\Delta}^{(t)}\approx\bm{P}(\bm{v}^{(t)}-\bm{v}^{(t-1)})\] (3)

so that these recursive approximations telescope. More precisely, setting \(\bm{g}^{(0)}=\bm{0}\), for \(t\geq 1\), we set

\[\bm{g}^{(t)}\leftarrow\bm{g}^{(t-1)}+\bm{\Delta}^{(t-1)}\approx\bm{P}(\bm{v}^{( t-2)}-\bm{v}^{(0)})+\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(t-2)})=\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)}).\] (4)This difference is perhaps similar to how methods such as SARAH ([34]) differ from SVRG ([33]). Consequently, we similarly call this approximation scheme _recursive variance reduction_. Interestingly, in constrast to the finite sum setting considered in [33, 34], in our setting, recursive variance reduction for solving DMDPs ultimately leads to direct quantitative improvements on worst case complexity.

To analyze this recursive variance reduction method, we treat the error in \(\bm{g}^{(t)}\approx\bm{P}(\bm{v}^{(t-1)}-\bm{v}^{(0)})\) as a martingale and analzye it using Freedman's inequality [35] (as stated in [36]). The hope in applying this approach is that by better bounding and reasoning about the changes in \(\bm{v}^{(t)}\), better bounds on the error of the sampling could be obtained by leveraging structural properties of the iterates.

_Unfortunately_, without further information about the change in \(\bm{v}^{(t)}\) or larger change to the analysis of variance reduced value iteration, in the worst case, the variance can be too large for this approach to work naively. Concretely, prior work ([2]) showed that it sufficed to maintain that \(\|\bm{g}^{(t+1)}-\bm{P}\bm{v}^{(t)}\|_{\infty}\leq O((1-\gamma)\alpha)\). However, imagine that \(\bm{v}^{*}=\alpha\bm{1}\), \(\bm{v}^{(0)}=\bm{0}\), and in each iteration \(t\) one coordinate of \(\bm{v}^{(t)}-\bm{v}^{(t-1)}\) is \(\Omega(\alpha)\). If \(|\mathcal{S}|\approx(1-\gamma)^{-1}\) and \(\|\bm{p}_{a}(s)\|_{\infty}=O(1/|\mathcal{S}|)\) for some \((s,a)\in\mathcal{A}\) then the variance of each sample used to estimate \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(t)}-\bm{v}^{(t-1)})=\Omega(1/|\mathcal{S}|)= \Omega((1-\gamma))\). Applying Freedman's inequality, e.g., [36], and taking \(b\) samples for each \(O((1-\gamma)^{-1})\) iteration would yield, roughly, \(\|\bm{g}^{(t+1)}-\bm{P}(\bm{v}^{(t)}-\bm{v}^{(0)})\|_{\infty}=O((1-\gamma)^{- 1}(1-\gamma)/\sqrt{b})=O(1/\sqrt{b})\). Consequently \(b=\Omega((1-\gamma)^{-2})\) and \(\Omega((1-\gamma)^{-3})\) samples would be needed in total, i.e., there is no improvement. Next, we will discuss how we circumvent this obstacle by _combining_ recursive variance reduction with a _second_ algorithm technique, which we call _truncation_.

**Truncated-value iteration.** The key insight to make our new recursive variance reduction scheme for value iteration yield faster runtimes is to modify the value iteration scheme itself. Recall that in the previous paragraph, we described that the case challenging case for recursive variance reduction occurs when, for example, in every iteration, a single coordinate of \(v\) changes by \(\Omega(\alpha)\). We observe that there is a simple modification that one could make to value iteration to ensure that there is not such a large change between each iteration; simply _truncate_ the change in each iteration so that no coordinate of \(\bm{v}^{(t)}\) changes too much! To motivate our algorithm, consider the following _truncated_ variant of value iteration where

\[\bm{v}^{(t)}=\operatorname{median}(\bm{v}^{(t-1)}-(1-\gamma)\alpha,\mathcal{ T}(\bm{v}^{(t-1)}),\bm{v}^{(t-1)}+(1-\gamma)\alpha)\] (5)

Where \(\operatorname{median}\) applies the median of the arguments entrywise. In other words, suppose we apply value iteration where we decrease or _truncate_ the change from \(\bm{v}^{(t-1)}\) to \(\bm{v}^{(t)}\) so that it is no more than \((1-\gamma)\alpha\) in absolute value in any coordinate. Then, provided that \(\bm{v}^{(t)}\) is \(\alpha\)-optimal, we can show that it is still the case that \(\|\bm{v}^{(t)}-\bm{v}^{*}\|_{\infty}\leq\gamma\|\bm{v}^{(t-1)}-\bm{v}^{*}\|_{\infty}\). In other words, the worst-case progress of value iteration is unaffected! This follows immediently from the fact that \(\|\bm{v}^{(t)}-\bm{v}^{*}\|_{\infty}\leq\gamma\|\bm{v}^{(t-1)}-\bm{v}^{*}\|_{\infty}\) in value iteration and the following simple technical lemma.

**Lemma 1.3**.: _For \(\bm{a},\bm{b},\bm{x}\in\mathbb{R}^{n}\) and \(\gamma,\alpha>0\), let \(\bm{c}:=\operatorname{median}\{\bm{a}-(1-\gamma)\alpha\bm{1},\bm{b},\bm{a}+( 1-\gamma)\alpha\bm{1}\}\), where median is applied entrywise. Then, if \(\left\|\bm{b}-\bm{x}\right\|_{\infty}\leq\gamma\left\|\bm{a}-\bm{x}\right\|_{\infty}\) and \(\left\|\bm{a}-\bm{x}\right\|_{\infty}\leq\alpha\), then \(\left\|\bm{c}-\bm{x}\right\|_{\infty}\leq\gamma\left\|\bm{a}-\bm{x}\right\|_{\infty}\)._

Applying truncated value iteration, we know that \(\|\bm{v}^{(t)}-\bm{v}^{(t-1)}\|_{\infty}\leq(1-\gamma)\alpha\). In other words, the worst-case change in a coordinate has decreased by a factor of \((1-\gamma)\)! We show that this smaller movement bound does indeed decrease the variance in the martingale when using the aforementioned averaging scheme. We show this truncation scheme, when _combined_ with our recursive variance reduction scheme (4) for estimating \(\bm{P}(\bm{v}^{(t)}-\bm{v}^{(0)})\), reduces the total samples required to estimate this and halve the error from \(\tilde{O}((1-\gamma)^{-3})\) to just \(\tilde{O}((1-\gamma)^{-2}\) per state-action pair.

**Our method.** Our algorithm applies stochastic truncated value iteration using sampling to estimate each \(\bm{g}^{(t)}\approx\bm{P}(\bm{v}^{(t)}-\bm{v}^{(0)})\) as described. Some minor additional modifications are needed, however, to obtain our results. Perhaps the most substantial is our use of the _monotonicity technique_, as in prior work ([2, 3]). That is, we modify our method so that each \(\bm{v}^{(t)}\) is always an _underestimate_ of \(\bm{v}^{*}\) and the \(\bm{v}^{(t)}\)_increase_ monotonically as \(t\) increases. Thus, we only truncate the increase in the \(\bm{v}^{(t)}\) (since they do not decrease, and the median operation in (5) reduces to a minimum in Lemma 1.3).

Beyond simplifying this aspect of the algorithm, as in prior work, this monotonicity technique allows us to _simultaneously_ compute an \(\varepsilon\)-approximate policy as well as an \(\varepsilon\)-optimal value vector. We do this by tracking the actions associated with changed \(\bm{v}^{(t)}\) values, i.e., the \(\operatorname{argmax}\) in (2) in a variable \(\pi^{(t)}\), which denotes the current estimated policy in iteration \(t\) of value iteration. Concretely, the monotonicity technique allows us to maintain the invariant that at each iteration \(t\), the current value estimate and policy estimate \(\pi^{(t)}\), \(\bm{v}^{(t)}\) satisfy the relation \(\bm{v}^{(t)}\leq\mathcal{T}[\bm{v}^{(t)}]\). Note that this ensures that the value of \(\pi^{(t)}\) (denoted \(\bm{v}^{\pi^{(t)}}\)) is _at least_\(\bm{v}^{(t)}\) because

\[\bm{v}^{(t)}\leq\mathcal{T}[\bm{v}^{(t)}]\leq\mathcal{T}^{2}[\bm{v}^{(t)}] \leq\cdots\mathcal{T}^{\infty}[\bm{v}^{(t)}]=\bm{v}^{\pi^{(t)}}\]

Thus, whenever \(\bm{v}^{(t)}\) is an \(\varepsilon\)-optimal value, \(\pi^{(t)}\) is an _at least_\(\varepsilon\)-optimal policy.

By computing initial expected utilities \(\bm{x}=\bm{P}\bm{v}^{(0)}\) exactly, we obtain our offline results. By carefully estimating \(\bm{x}\approx\bm{P}\bm{v}^{(0)}\) as in [3] we obtain our sampling results. Finally, building off of the analysis of [37] for deterministic or highly-mixing MDPs, we also show our method obtains even faster convergence guarantees under additional non-worst-case assumptions on the MDP structure.

### Notation and paper outline

General notation.Caligraphic upper case letters denote sets and operators, lowercase boldface letters denote vectors, and uppercase boldface letters (e.g., \(\bm{P},\bm{I}\)) denote matrices. \(\bm{0}\) and \(\bm{1}\) denote the all-ones and all-zeros vectors, \([m]:=\{1,....,m\}\), and \(\Delta^{n}:=\{x\in\mathbb{R}^{n}:\bm{0}\leq x\text{ and }\left\lVert x \right\rVert_{1}=1\}\) is the simplex. For \(\bm{v}\in\mathbb{R}^{\mathcal{S}}\), we use \(\bm{v}_{i}\) or \(\bm{v}(i)\) for the \(i\)-th entry of vector \(\bm{v}\). For vectors \(\bm{v}\in\mathbb{R}^{\mathcal{A}}\), we use \(\bm{v}_{a}(s)\) to denote the \((s,a)\)-th entry of \(\bm{v}\), where \((s,a)\in\mathcal{A}\). We use \(\sqrt{\bm{v}},\bm{v}^{2},|\bm{v}|\in\mathbb{R}^{n}\) for the element-wise square root, square, and absolute value of \(\bm{v}\) respectively and \(\max\{\bm{u},\bm{v}\}\) and \(\operatorname{median}\{\bm{u},\bm{v},\bm{w}\}\) for element-wise maximum and median respectively. For \(\bm{v},\bm{x}\in\mathbb{R}^{n}\), \(\bm{v}\leq\bm{x}\) denotes that \(\bm{v}(i)\leq\bm{x}(i)\) for each \(i\in[n]\) (analogously for \(<,\geq,>\).) We call \(\bm{x}\in\mathbb{R}^{n}\) an _\(\alpha\)-underestimate_ of \(\bm{y}\in\mathbb{R}^{n}\) if \(\bm{y}-\alpha\bm{1}\leq\bm{x}\leq\bm{y}\) for \(\alpha\geq 0\) (see the discussion of monotonicity in Section 1.2 for motivation).

Dmp.As discussed, the objective in optimizing a DMDP is to find an \(\varepsilon\)-approximate policy \(\pi\) and values. For a policy \(\pi\), we use \(\mathcal{T}_{\pi}(\bm{u}):\mathbb{R}^{\mathcal{S}}\mapsto\mathbb{R}^{\mathcal{ S}}\) to denote the value operator associated with \(\pi\), i.e., \(\mathcal{T}_{\pi}(\bm{u})(s):=\bm{r}_{\pi(s)}(s)+\gamma\bm{p}_{\pi(s)}(s)^{\top} \bm{u}\) for all value vectors \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\) and \(s\in\mathcal{S}\). We let \(\bm{v}^{\pi}\) denote the unique value vector such that \(\mathcal{T}_{\pi}(\bm{v}^{\pi})=\bm{v}^{\pi}\) and define its variance as \(\bm{\sigma}_{\bm{u}^{\pi}}:=\bm{P}^{\pi}(\bm{u}^{\pi})^{2}-(\bm{P}^{\pi}\bm{u}^ {\pi})^{2}\), where \(\bm{P}^{\pi}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\) is the matrix such that \(\bm{P}_{s,s^{\prime}}^{\pi}=\bm{P}_{s,\pi(s)}(s^{\prime})\). The _optimal value vector_\(\bm{v}^{\star}\in\mathbb{R}^{\mathcal{S}}\) of the optimal policy \(\pi^{\star}\) is the unique vector with \(\mathcal{T}(\bm{v}^{\star})=\bm{v}^{\star}\), and \(\bm{P}^{\star}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}:=\bm{P}^{\pi^{ \star}}\).

Outline.Section 2 presents our offline setting results and Section 3 our sample setting results. Section 0.A discusses specialized settings where we can obtain even faster convergence guarantees. Omitted proofs are deferred to Appendix B.

## 2 Offline algorithm

In this section, we present our high-precision algorithm for finding an approximately optimal policy in the offline setting. We first define (Algorithm 1), which approximately computes products between \(\bm{p}\in\Delta^{S}\) and a value vector \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\) using samples from a generative model. The following lemma states some immediate estimation bounds on () using linearity and the fact that \(\bm{p}\in\Delta^{\mathcal{S}}\).

``` Input: Value vector \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\), sample size \(M\), and offset parameter \(\eta\geq 0\). for each \((s,a)\in\mathcal{A}\)do // In the sample setting, \(\bm{p}_{a}(s)\) is passed implicitly. \(\bm{x}_{a}(s)=\texttt{Sample}(\bm{u},\bm{p}_{a}(s),M,\eta)\); return\(\bm{x}\) ```

**Algorithm 2**ApxUtility(\(\bm{u},M,\eta\))

**Lemma 2.1**.: _Let \(x=\texttt{Sample}(\bm{u},\bm{p},M,0)\) for \(\bm{p}\in\Delta^{n}\), \(M\in\mathbb{Z}_{>0}\), \(\varepsilon>0\), and \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\). Then, \(\mathbb{E}\left[x\right]=\bm{p}^{\top}\bm{u}\), \(\left\lvert x\right\rvert\leq\left\lVert\bm{u}\right\rVert_{\infty}\), and \(\text{Var}\left[x\right]\leq 1/M\left\lVert\bm{u}\right\rVert_{\infty}^{2}\)._We can naturally apply \(\mathtt{Sample}\) to each state-action pair in \(\mathcal{M}\) as in the subroutine \(\mathtt{ApxUtility}\) (Algorithm 2). If \(\bm{x}=\mathtt{ApxUtility}(\bm{u},M,\eta)\), then \(\bm{x}(s,a)\) is an estimate of the expected utility of taking action \(a\in\mathcal{A}_{s}\) from state \(s\in\mathcal{S}\) (as discussed in Section 1.2). When \(\eta>0\), this estimate may potentially be shifted to increase the probability that \(\bm{x}\) underestimates the true changes in utilities; we leverage this in Section 3 (see also the discussion of monotonicity in Section 1.2). The terms arising in the definition of \(\tilde{x}\) arise from applying Bernstein's inequality (Theorem B.2) to guarantee that \(\tilde{x}\leq x-\eta\) with high probability.

The following algorithm \(\mathtt{TVRVI}\) (Algorithm 3) takes as input an initial value vector \(\bm{v}^{(0)}\) and policy \(\pi^{(0)}\) such that \(\bm{v}^{(0)}\) is an \(\alpha\)-underestimate of \(\bm{v}^{\star}\) along with an approximate offset vector \(\bm{x}\), which is a \(\beta\)-underestimate of \(\bm{P}\bm{v}^{(0)}\). It runs runs \(L=\tilde{O}((1-\gamma)^{-1})\) iterations of approximate value iteration, making one call to \(\mathtt{Sample}\)(Algorithm 1) with a sample size of \(M=\tilde{O}((1-\gamma)^{-1})\) in each iteration. The algorithm outputs \(\bm{v}^{L}\) which we show is an \(\alpha/2\)-underestimate of \(\bm{v}^{\star}\) (Corollary 2.5).

\(\mathtt{TVRVI}\) (Algorithm 3) is similar to variance reduced value iteration [2], in that each iteration, we draw \(M\) samples and use \(\mathtt{Sample}\) to maintain underestimates of \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell)}-\bm{v}^{(\ell-1)})\) for each state-action pair \((s,a)\). However, there are two key distinctions between \(\mathtt{TVRVI}\)and variance-reduced value iteration [2] that enable our improvement. First, we use the recursive variance reduction technique, as described by (3) and (4), and second we apply truncation (Line 7), which essentially implements the truncation described in Lemma 1.3. Lemma 2.2 below illustrates how these two techniques can be combined to bound the necessary sample complexity for maintaining approximate transitions \(\bm{p}_{a}(s)^{\top}(\bm{w}^{(t)}-\bm{w}^{(0)})\) for a general sequence of \(\ell_{\infty}\)-bounded vectors \(\{\bm{w}^{(i)}\}_{i=1}^{T}\). The analysis leverages Freedman's Inequality [35] as stated in [36] and restated in Theorem B.1.

**Lemma 2.2**.: _Let \(T\in\mathbb{Z}_{>0}\) and \(\bm{w}^{(0)},\bm{w}^{(1)},...,\bm{w}^{(T)}\in\mathbb{R}^{\mathcal{S}}\) such that \(\left\|\bm{w}^{(i)}-\bm{w}^{(i-1)}\right\|_{\infty}\leq\tau\) for all \(i\in[T]\). Then, for any \(\bm{p}\in\Delta^{\mathcal{S}}\), \(\delta\in(0,1)\), and \(M\geq 2^{8}T\log(2/\delta)\) with probability \(1-\delta\), \(\left|\bm{p}^{\top}(\bm{w}^{(t)}-\bm{w}^{(0)})-\sum_{i\in[t]}\sum_{j\in[M]} \mathtt{Sample}(\bm{w}^{(i)}-\bm{w}^{(i-1)},\bm{p},1,0)\cdot 1/M\right|\leq\tau/8\) for all \(t\in[T]\)._

``` Input: Initial values \(\bm{v}^{(0)}\in\mathbb{R}^{\mathcal{S}}\), which is an \(\alpha\)-underestimate of \(\bm{v}^{\star}\). Input: Initial policy \(\pi^{(0)}\) such that \(\bm{v}^{(0)}\leq\mathcal{T}_{\pi^{(0)}}(\bm{v}^{(0)})\). Input: Accuracy \(\alpha\in[0,(1-\gamma)^{-1}]\) and failure probability \(\delta\in(0,1)\). Input: Offsets \(\bm{x}\in\mathbb{R}^{\mathcal{A}}\) ; // entrywise underestimate of \(\bm{P}\bm{v}^{(0)}\)
1 Initialize \(\bm{g}^{(1)}\in\mathbb{R}^{\mathcal{A}}\) and \(\hat{\bm{g}}^{(1)}\in\mathbb{R}^{\mathcal{A}}\) to \(\bm{0}\);
2\(L=\lceil\log(8)(1-\gamma)^{-1}\rceil\) and \(M=\lceil L\cdot 2^{8}\log(2\mathcal{A}_{\mathrm{tot}}/\delta)\rceil\) ;
3for each iteration \(\ell\in[L]\)do
4\(\tilde{\bm{Q}}=\bm{r}+\gamma(\bm{x}+\hat{\bm{g}}^{(\ell)})\);
5\(\bm{v}^{(\ell)}=\bm{v}^{(\ell-1)}\) and \(\pi^{(\ell)}=\pi^{(\ell-1)}\) ;
6for each state \(i\in\mathcal{S}\)do // Compute truncated value update (and associated action) \(\tilde{\bm{v}}^{(\ell)}(i)=\min\{\max_{a\in\mathcal{A}_{i}}\tilde{\bm{Q}}_{i,a},\bm{v}^{(\ell-1)}+(1-\gamma)\alpha\}\) and \(\tilde{\pi}_{i}^{(\ell)}=\operatorname*{argmax}_{a\in\mathcal{A}_{i}}\tilde{ \bm{Q}}_{i,a}\); // Update value and policy if it improves if\(\tilde{\bm{v}}^{(\ell)}(i)\geq\bm{v}^{(\ell)}(i)\)then\(\bm{v}^{(\ell)}(i)=\tilde{\bm{v}}^{(\ell)}(i)\) and \(\pi_{i}^{(\ell)}=\tilde{\pi}_{i}^{(\ell)}\) ; // Update for maintaining estimates of \(\bm{P}(\bm{v}^{(l)}-\bm{v}^{0})\). \(\bm{\Delta}^{(\ell)}=\mathtt{ApxUtility}(\bm{v}^{(\ell)}-\bm{v}^{(\ell-1)},M,0)\) and \(\bm{g}^{(\ell+1)}=\bm{g}^{(\ell)}+\bm{\Delta}^{(\ell)}\) ; // Shift estimates so that \(\hat{\bm{g}}^{(\ell+1)}\) always underestimates \(\bm{p}_{a}(s)^{\top}\bm{v}^{(\ell)}\). \(\hat{\bm{g}}^{(\ell+1)}=\bm{g}^{(\ell+1)}-\frac{(1-\gamma)\alpha}{8}\bm{1}\);
7
8return\((\bm{v}^{(L)},\pi^{(L)})\) ```

**Algorithm 3**\(\mathtt{TVRVI}(\bm{v}^{(0)},\pi^{(0)},\bm{x},\alpha,\delta)\)

While it is unclear how to significantly improve the constant of \(2^{8}=256\) appearing in Lemma 2.2 (and consequently Algorithm 3), we note that tightening these constants in the application of Freedman's inequality could be of practical interest. By applying Lemma 2.2 to the iterates \(\bm{v}^{(\ell)}\) in \(\mathtt{TVRVI}\), the following Corollary 2.3 shows that we can maintain additive \(O((1-\gamma)\alpha)\)-underestimates of the transitions \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell)}-\bm{v}^{(0)})\) using only \(\tilde{O}(L)\) samples (as opposed to the \(\tilde{O}(L^{2})\) samples required in [2]) per state-action pair.

**Corollary 2.3**.: _In TVRVI (Algorithm 3), with probability \(1-\delta\), in Lines 9, 10 and 2, for all \(s\in\mathcal{S},a\in\mathcal{A}_{s}\), and \(\ell\in[L]\), we have \(\left|\bm{g}_{a}^{(\ell)}(s)-\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0 )})\right|\leq(1-\gamma)\alpha/8\) and therefore \(\hat{\bm{g}}_{a}^{(\ell)}\) is a \((1-\gamma)\alpha/4\)-underestimate of \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0)})\)._

The following Lemma 2.4 shows that whenever the event in Corollary 2.3 holds, TVRVI (Algorithm 3) is approximately contractive and maintains monotonicity of the approximate values. By accumulating the error bounds in Lemma 2.4, we also obtain the following Corollary 2.5, which guarantees that TVRVI halves the error in the initial estimate \(\bm{v}^{(0)}\).

**Lemma 2.4**.: _Suppose that for some \(\bm{\beta}\in\mathbb{R}^{4}_{\geq 0}\), \(\bm{P}\bm{v}^{(0)}-\bm{\beta}\leq\bm{x}\leq\bm{P}\bm{v}^{(0)}\) and let \(\bm{\beta}_{\pi^{\star}}\in\mathbb{R}^{\mathcal{S}}\) be defined as \(\bm{\beta}_{\pi^{\star}}(s):=\bm{\beta}_{\pi^{\star}(s)}(s)\) for each \(s\in\mathcal{S}\). Then, with probability \(1-\delta\), at the end of every iteration \(\ell\in[L]\) (Line 3) in \(\texttt{TVRVI}(\bm{v}^{(0)},\pi^{(0)},\bm{x},\alpha,\delta)\), the following hold for \(\bm{\xi}:=\gamma((1-\gamma)\alpha/4\bm{1}+\bm{\beta}_{\pi^{\star}})\):_

\[\bm{v}^{(\ell-1)} \leq\bm{v}^{(\ell)}\leq\mathcal{T}_{\pi^{(\ell)}}(\bm{v}^{(\ell)}),\] (6) \[0\leq\bm{v}^{\star}-\bm{v}^{(\ell)} \leq\max\left(\gamma\bm{P}^{\star}(\bm{v}^{\star}-\bm{v}^{(\ell-1 )})+\bm{\xi},\gamma(\bm{v}^{\star}-\bm{v}^{(\ell-1)})\right).\] (7)

**Corollary 2.5**.: _Suppose that for some \(\alpha\geq 0\) and \(\bm{\beta}\in\mathbb{R}^{\mathcal{A}}_{\geq 0}\), \(\bm{P}\bm{v}^{(0)}-\bm{\beta}\leq\bm{x}\leq\bm{P}\bm{v}^{(0)}\); \(\bm{v}^{(0)}\) is an \(\alpha\)-underestimate of \(\bm{v}^{\star}\); and \(\bm{v}^{(0)}\leq\mathcal{T}_{\pi^{(0)}}(\bm{v}^{(0)})\). Let \(\bm{\beta}_{\pi^{\star}}\in\mathbb{R}^{\mathcal{S}}\) be defined as \(\bm{\beta}_{\pi^{\star}}(s):=\bm{\beta}_{\pi^{\star}(s)}(s)\) for each \(s\in\mathcal{S}\). Let \((\bm{v}^{(L)},\pi^{(L)})=\texttt{TVRVI}(\bm{v}^{(0)},\pi^{(0)},\alpha,\delta)\), and \(L,M\) be as in Line 2. Define \(\bm{\xi}:=\gamma\left((1-\gamma)\alpha/4\cdot\bm{1}+\bm{\beta}_{\pi^{\star}}\right)\). Then, with probability \(1-\delta\), \(\bm{0}\leq\bm{v}^{\star}-\bm{v}^{(L)}\leq\gamma^{L}\alpha\cdot\bm{1}+(\bm{I}- \gamma\bm{P}^{\star})^{-1}\bm{\xi}\), and \(\bm{v}^{(L)}\leq\mathcal{T}_{\pi^{(L)}}(\bm{v}^{(L)})\). In particular, if \(\bm{\beta}=\bm{0}\), then for \(L>\log(8)(1-\gamma)^{-1}\) we can reduce the error in \(\bm{v}^{(0)}\) by half: \(\bm{0}\leq\bm{v}^{\star}-\bm{v}^{(L)}\leq(\bm{v}^{\star}-\bm{v}^{(0)})/2\). Additionally, TVRVI is implementable with \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}ML)\) sample queries to the generative model and time and \(O(\mathcal{A}_{\mathrm{tot}})\) space._

Theorem 1.2 now follows by recursively applying Corollary 2.5. OfflineTVRVI (Algorithm 4) provides the pseudocode for the algorithm guaranteed by Theorem 1.2.

``` Input: Target precision \(\varepsilon\) and failure probability \(\delta\in(0,1)\)
1\(K=\lceil\log_{2}(\varepsilon^{-1}(1-\gamma)^{-1})\rceil\), \(\bm{v}_{0}=\bm{0}\), \(\pi_{0}\) is an arbitrary policy, and \(\alpha_{0}(1-\gamma)^{-1}\);
2for each iteration \(k\in[K]\)do
3\(\alpha_{k}=\alpha_{k-1}/2=2^{-k}(1-\gamma)^{-1}\);
4\(\bm{x}=\bm{P}\bm{v}_{k-1}\) ;
5\((\bm{v}_{k},\pi_{k})=\texttt{TVRVI}(\bm{v}_{k-1},\pi_{k-1},\bm{x},\alpha_{k-1}, 0,\delta/K)\);
6return\((\bm{v}_{K},\pi_{K})\) ```

**Algorithm 4**OfflineTVRVI\((\varepsilon,\delta)\)

## 3 Sample setting algorithm

In this section, we show how to extend the analysis in the previous section in the sample setting, where we do not have explicit access to \(\bm{P}\). We follow a similar framework as in [3] to show that we can instead estimate the offsets \(\bm{x}\) in OfflineTVRVI by taking additional samples from the generative model. The pseudocode is shown in SampleTVRVI(Algorithm 5.) To analyze the algorithm, we first bound the error incurred when approximating the exact offsets \(\bm{x}\) in Line 4 of OfflineTVRVI (Algorithm 4) with approximate offsets \(\tilde{\bm{x}}\approx\bm{P}\bm{v}_{k-1}\) computed by sampling from the generative model. The proof leverages Hoeffding's and Bernstein's inequality, and follows a similar structure as the proof of Lemma 5.1 of [3].

**Lemma 3.1**.: _Consider \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\). Let \(\bm{x}=\mathtt{ApxUtility}(\bm{u},m\cdot\mathcal{A}_{\mathrm{tot}},\eta)\), \(m\geq\log(1/2\delta^{-1})\), and \(\eta=(m\mathcal{A}_{\mathrm{tot}})^{-1}\log(1/2\delta^{-1})\). Then, with probability \(1-\delta\),_

\[\bm{Pu}-2\sqrt{2\eta\bm{\sigma}_{\bm{v}^{*}}}+\left(2\sqrt{2\eta}\left\|\bm{u} -\bm{v}^{*}\right\|_{\infty}+18\eta^{3/4}\left\|\bm{u}\right\|_{\infty}\right) \leq\bm{x}\leq\bm{Pu}.\]

Finally, to obtain our main result Theorem 1.1, we utilize worst-case bounds on \(\bm{\sigma}_{\bm{v}^{*}}\) from prior work [1] (see Lemma B.3, Lemma B.4) and inductively apply Lemma 3.1 and Corollary 2.5.

The constant of \(6500\) appearing in the initialization of \(N\) in Algorithm 5 arises due to technical reasons, from applying Bernstein's inequality, Hoeffding's inequality, union bound over all \(K\) outer loop iterations, and bounds on \(\bm{\sigma}_{\bm{v}^{*}}\) from prior work [3] to prove Lemma 3.1. While it is unclear how to directly further tighten this constant, the proof of Lemma 3.1 shows that in the expression \(N=6500(1-\gamma)^{-3}\log(8\mathcal{A}_{\mathrm{tot}}K\delta^{-1})\) there is a natural trade-off between the leading constant (in this case \(6500\)) and the number of outer loop iterations \(K\). By increasing the number of outer-loop iterations \(K\) by constants, one can relax the error requirements of each iteration (i.e., decrease \(N\) by constants at the cost of increased logarithmic dependence on \(\left|S\right|,\mathcal{A}_{\mathrm{tot}}\)). Although not the primary focus of our work, such trade-offs might be of practical importance.

## 4 Conclusion

We provided faster and more space-efficient algorithms for solving DMDPs. We showed how to apply truncation and recursive variance reduction to improve upon prior variance-reduced value iterations methods. Ultimately, these techniques reduced an additive \(\tilde{O}((1-\gamma)^{-3})\) term in the time and sample complexity of prior variance-reduced value iteration methods to \(\tilde{O}((1-\gamma)^{-2})\).

Natural open problems left by our work include exploring the practical implications of our techniques and exploring whether further runtime improvements are possible. For example, it may be of practical interest to explore whether there exist other analogs of truncation that do not need to limit the progress in individual steps of value iteration. Additionally, the question of whether the \(\tilde{O}((1-\gamma)^{-2})\) term in our time and sample complexities can be further improved to \(\tilde{O}((1-\gamma)^{-1})\) is a natural open problem; an affirmative answer to this question would yield the first near-optimal running times for solving a DMDP with a generative model for all \(\varepsilon\) and fully bridge the sample complexity gap between model-based and model-free methods. We hope this paper supports further studying these questions and establishing the optimal runtime for solving MDPs.

## Acknowledgements

Thank you to Yuxin Chen for interesting and motivating discussion about model-based methods in RL. Thank you to the anonymous reviewers for their helpful feedback. Yujia Jin and Ishani Karmarkar were funded in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and a PayPal research award. Aaron Sidford was funded in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF1955039, and a PayPal research award. Part of this work was conducted while visiting the Simons Institute for the Theory of Computing. Yujia Jin's contributions to the project occurred while she was a graduate student at Stanford.

## References

* [1] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. _Naval Research Logistics (NRL)_, 70, 2023.
* [2] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. _29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2018.
* [3] Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving markov decision processes with a generative model. _Advances in Neural Information Processing Systems 31 (NeurIPS)_, 2018.
* [4] Qiying Hu and Wuyi Yue. _Markov decision processes with their applications_, volume 14. Springer Science & Business Media, 2007.
* [5] Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. Reinforcement learning to rank with markov decision process. _Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval_, 2017.
* [6] Thomas Degris, Olivier Sigaud, and Pierre-Henri Wuillemin. Learning the structure of factored markov decision processes in reinforcement learning problems. _23rd International Conference on Machine Learning (ICML)_, 2006.
* [7] Olivier Sigaud and Olivier Buffet. _Markov decision processes in artificial intelligence_. John Wiley & Sons, 2013.
* [8] Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. _Reinforcement learning: State-of-the-art_, 2012.
* [9] Martijn Van Otterlo. Markov decision processes: Concepts and algorithms. _Course on 'Learning and Reasoning_, 2009.
* [10] Yinyu Ye. A new complexity result on solving the markov decision problem. _Mathematics of Operations Research_, 30, 2005.
* [11] Michael L Littman, Thomas L Dean, and Leslie Pack Kaelbling. On the complexity of solving markov decision problems. _11th Annual Conference on Uncertainty in Artificial Intelligence (UAI)_, 1995.
* [12] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in o (vrank) iterations and faster algorithms for maximum flow. _55th Annual IEEE Symposium on Foundations of Computer Science (FOCS)_, 2014.
* [13] Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a fixed discount rate. _Mathematics of Operations Research_, 36, 2011.
* [14] Bruno Scherrer. Improved and generalized upper bounds on the complexity of policy iteration. _Advances in Neural Information Processing Systems 26 (NeurIPS))_, 2013.
* [15] Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect algorithms. _Advances in Neural Information Processing Systems 11 (NeurIPS)_, 11, 1998.
* [16] Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. _Machine Learning_, 91, 2013.
* [17] Alekh Agarwal, Sham M. Kakade, and Lin F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. _33rd Annual Conference on Computational Learning Theory (COLT)_, 2020.
* [18] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. _Advances in Neural Information Processing Systems 33 (NeurIPS)_, 2020.
* [19] Sham Machandran Kakade. _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom), 2003.

* [20] Fei Feng, Wotao Yin, and Lin F Yang. How does an approximate model help in reinforcement learning? _arXiv preprint arXiv:1912.02986_, 2019.
* [21] Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. In _37th International Conference on Machine Learning (ICML)_, 2020.
* [22] Paul Tseng. Solving h-horizon, stationary markov decision problems in time proportional to log (h). _Operations Research Letters_, 9, 1990.
* [23] Mengdi Wang. Randomized linear programming solves the discounted markov decision problem in nearly-linear (sometimes sublinear) running time. _Mathematics of Operations Research_, 42, 2019.
* [24] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and Di Wang. Minimum cost flows, mdps, and l1-regression in nearly linear time for dense instances. In _53rd Annual ACM Symposium on Theory of Computing (STOC)_, 2021.
* [25] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. _Journal of the ACM_, 2020.
* [26] Jan van den Brand. A deterministic linear program solver in current matrix multiplication time. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 259-278. SIAM, 2020.
* [27] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 823-832, 2021.
* [28] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In _35th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2024.
* [29] Pengqian Yu, William B Haskell, and Huan Xu. Approximate value iteration for risk-aware markov decision processes. _IEEE Transactions on Automatic Control_, 63, 2018.
* [30] Mohand Hamadouche, Catherine Dezan, David Espes, and Kalinka Branco. Comparison of value iteration, policy iteration and q-learning for solving decision-making problems. In _2021 International Conference on Unmanned Aircraft Systems (ICUAS)_, 2021.
* [31] Christopher W Zobel and William T Scherer. An empirical study of policy convergence in markov decision process value iteration. _Computers & operations research_, 32, 2005.
* [32] Dileep Kalathil, Vivek S Borkar, and Rahul Jain. Empirical q-value iteration. _Stochastic Systems_, 11, 2021.
* [33] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in Neural Information Processing Systems 26 (NeurIPS)_, 2013.
* [34] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine learning problems using stochastic recursive gradient. _34th International Conference on Machine Learning (ICML)_, 2017.
* [35] David A Freedman. On tail probabilities for martingales. pages 100-118, 1975.
* [36] Joel A. Tropp. Freedman's inequality for matrix martingales. _Electronic Communications in Probability_, 16, 2011.
* [37] Andrea Zanette and Emma Brunskill. Problem dependent reinforcement learning bounds which can identify bandit structure in mdps. In _International Conference on Machine Learning_, pages 5747-5755. PMLR, 2018.
* [38] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. _36th International Conference on Machine Learning (ICML)_, 2019.

Faster problem-dependent convergence

In this section, we propose a modified version of the SampleTVRVI algorithm, named ProblemDependentTVRVI. This algorithm adjusts the number of required samples based on the structure of the MDP under consideration. Inspired by [38], we then consider MDPs with small ranges of optimal values and the extreme case of highly mixing MDPs in which state transitions are sampled from a fixed distribution.

Note that in the proof of Theorem 1.1, the error during convergence caused by approximations of values is bounded by \((\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k}\) for \(\bm{\xi}_{k}\leq\frac{(1-\gamma)\alpha_{k}}{4}\bm{1}+2\sqrt{2\eta_{k}\bm{ \sigma}_{\bm{v}^{\star}}}+(2\sqrt{2\eta_{k}}\alpha_{k}+18\eta_{k}^{3/4}\left\| \bm{v}^{(0)}\right\|_{\infty})\bm{1}\). In its proof, we upper bound the variance term \(\left\|(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{\star}}} \right\|_{\infty}\) by \(3(1-\gamma)^{-1.5}\) using Lemma B.4. However, as \(\alpha_{k}\) decreases and the variance term becomes dominant, a number of samples proportional to the size of the variance term suffices to control the error during each iteration. Given \(V\) which upper bounds \(\left\|(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{\star}}} \right\|_{\infty}\), we can further refine SampleTVRVI to reduce the number of samples taken after an initial burn-in phase and obtain improved complexities when \(V\) is signficantly small. Hence, we obtain the following Algorithm 6 and Theorem A.1.

``` Input: Target precision \(\varepsilon\), failure probability \(\delta\in(0,1)\), and \(V\geq\left\|(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{ \star}}}\right\|_{\infty}\).
1\(K=\lceil\log_{2}(\varepsilon^{-1}(1-\gamma)^{-1})\rceil\) ;
2\(\bm{v}_{0}=\bm{0}\), \(\pi_{0}\) is an arbitrary policy, and \(\alpha_{0}=\frac{1}{1-\gamma}\);
3for each iteration \(k\in[K]\)do
4\(\alpha_{k}=\alpha_{k-1}/2=2^{-k}(1-\gamma)^{-1}\) ;
5if\(k<\lceil\log_{2}\left(\frac{128(1-\gamma)^{-5}}{V^{3}}\right)\rceil\)then
6\(N_{k-1}=6500\cdot(1-\gamma)^{-3}\max((1-\gamma),\alpha_{k-1}^{-2})\log(8\mathcal{ A}_{\mathrm{tot}}K\delta^{-1})\) ; // Burn-in phase
7
8else
9\(N_{k-1}=1024\cdot\alpha_{k-1}^{-2}V^{2}\log(8\mathcal{A}_{\mathrm{tot}}K\delta ^{-1})\) ; // Variance-dependent phase
10\(\eta_{k-1}=N_{k-1}^{-1}\log(8\mathcal{A}_{\mathrm{tot}}K\delta^{-1})\) ;
11\(\bm{x}_{k}=\texttt{ApxUtility}(\bm{v}_{k-1},N_{k-1},\eta_{k-1})\);
12\((\bm{v}_{k},\pi_{k})=\texttt{TVRVI}(\bm{v}_{k-1},\pi_{k-1},\bm{x}_{k},\alpha _{k-1},\delta/K)\);
13
14return\((\bm{v}_{K},\pi_{K})\) ```

**Algorithm 6**ProblemDependentTVRVI\((\varepsilon,\delta,V)\)

**Theorem A.1**.: _In the sample setting, there is an algorithm (Algorithm 6) that, given \(3(1-\gamma)^{-1.5}\geq V\geq\left\|(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{ \sigma}_{\bm{v}^{\star}}}\right\|_{\infty}\), uses \(\tilde{O}\left(\mathcal{A}_{\mathrm{tot}}\left(\varepsilon^{-2}V^{2}+(1-\gamma )^{-2}\right)\right)\) samples and time and \(O(\mathcal{A}_{\mathrm{tot}})\) space, and computes an \(\varepsilon\)-optimal policy and \(\varepsilon\)-optimal values with probability \(1-\delta\)._

Proof.: Let \(K\), \(\alpha_{k}\), and \((\bm{v}_{k},\pi_{k})\) be as defined in Lines 1, 4, and 11 of

\(\texttt{ProblemDependentTVRVI}(\varepsilon,\delta,V)\).

For the correctness of the algorithm, we first induct on \(k\) to show that for each \(k\in[K]\), with probability \(1-k\delta/K\),

\[\bm{0}\leq\bm{v}^{\star}-\bm{v}^{\pi_{k}}\leq\bm{v}^{\star}-\bm{v}_{k}\leq\alpha _{k},\quad\text{ and }\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k}).\]

The base case is trivial, as \(\bm{0}\leq\bm{v}^{\star}-\bm{v}^{\pi_{0}}\leq\bm{v}^{\star}-\bm{v}_{0}\leq(1- \gamma)^{-1}\bm{1}\).

For the inductive step, observe that by Lemma 3.1, we see that with probability \(1-\delta/K\),

\[\bm{P}\bm{v}_{k-1}-\left[2\sqrt{2\eta_{k-1}\bm{\sigma}_{\bm{v}^{\star}}}+\left( 2\sqrt{2\eta_{k-1}}\alpha_{k-1}+18\eta_{k-1}^{3/4}\left\|\bm{v}_{k-1}\right\|_{ \infty}\right)\bm{1}\right]\leq\bm{x}_{k}\leq\bm{P}\bm{v}_{k-1}.\] (8)

Additionally, by the inductive hypothesis, with probability \(1-(k-1)\delta/K\),

\[0\leq\bm{v}^{\star}-\bm{v}^{\pi_{k-1}}\leq\bm{v}^{\star}-\bm{v}_{k}\leq\gamma^{ L}\alpha_{k-1}\cdot\bm{1}+(\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1}\leq \alpha_{k}\bm{1},\quad\text{ and }\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k}).\] (9)

Thus, by union bound, with probability \(1-k\delta/K\), both (8) and (9) hold. We condition on this event in the remainder of the inductive step.

Now, we apply Corollary 2.5 with

\[\bm{\beta}=2\sqrt{2\eta_{k-1}\bm{\sigma}_{\bm{v}^{\star}}}+\left(2\sqrt{2\eta_{k-1 }}\alpha_{k-1}+18\eta_{k-1}^{3/4}\left\lVert\bm{v}_{k-1}\right\rVert_{\infty} \right)\bm{1}.\]

Consequently, we have

\[0\leq\bm{v}^{\star}-\bm{v}_{k}\leq\gamma^{L}\alpha_{k-1}\cdot\bm{1}+(\bm{I}- \gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1}\leq\frac{\alpha_{k-1}}{8}\bm{1}+(\bm{I }-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1},\]

for \(\bm{\xi}_{k-1}\leq\frac{(1-\gamma)\alpha_{k-1}}{4}\bm{1}+2\sqrt{2\eta_{k-1}\bm {\sigma}_{\bm{v}^{\star}}}+\left(2\sqrt{2\eta_{k-1}}\alpha_{k-1}+18\eta_{k-1}^{ 3/4}\left\lVert\bm{v}_{k-1}\right\rVert_{\infty}\right)\bm{1}\), and \(\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\).

Note that \((\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{1}\leq\frac{1}{1-\gamma}\bm{1}\). Hence, if \(k<\lceil\log_{2}(1-\gamma)^{-5}/V^{3}\rceil\), we use Lemma B.4 along with the facts that \((\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{1}=1/(1-\gamma)\bm{1}\) and the choice of \(\eta_{k-1}\) to obtain (identical to the proof of Theorem 1.1):

\[(\bm{1}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1} \leq\left[\frac{\alpha_{k-1}}{4}+2\sqrt{6\frac{\eta_{k-1}}{(1- \gamma)^{3}}}+2\sqrt{\frac{2(1-\gamma)^{3}\min((1-\gamma)^{-1},\alpha_{k-1}^{2 })}{6500(1-\gamma)^{2}}}\alpha_{k-1}\right]\bm{1}\] \[+\left[18\left(\frac{((1-\gamma)^{3}\min((1-\gamma)^{-1},\alpha_ {k-1}^{2})}{6500(1-\gamma)^{8/3}}\right)^{3/4}\right]\bm{1}\] \[\leq\left[\alpha_{k-1}/4+2\sqrt{6/6500}\cdot\alpha_{k-1}+2\sqrt{ 2/6500}(1-\gamma)^{1/2}\min((1-\gamma)^{-1/2},\alpha_{k-1})\alpha_{k-1}\right.\] \[+18\cdot(10^{-3})(1-\gamma)^{1/4}\min((1-\gamma)^{-3/4},\alpha_{ k-1}^{3/2})]\bm{1}\] \[\leq[\alpha_{k-1}/4+4\sqrt{6/6500}\cdot\alpha_{k-1}+18\cdot(10^{ -3})\alpha_{k-1}]\bm{1}\leq\frac{3}{8}\alpha_{k-1}\bm{1}.\]

If instead \(k\geq\lceil\log_{2}(1-\gamma)^{-5}/V^{3}\rceil\), then \(\alpha_{k}\leq\frac{1}{128}(1-\gamma)^{4}V^{3}\), and \(\eta_{k-1}=\alpha_{k-1}^{2}/(1024\cdot V^{2})\). Consequently,

\[(\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1} \leq 2\sqrt{2\eta_{k-1}}(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{ \bm{\sigma}_{\bm{v}^{\star}}}\] \[+\left[\frac{\alpha_{k-1}}{4}+2\sqrt{2\eta_{k-1}}(\bm{I}-\gamma \bm{P}^{\star})^{-1}\alpha_{k-1}+18\eta_{k-1}^{3/4}(\bm{I}-\gamma\bm{P}^{\star })^{-1}\left\lVert\bm{v}_{k-1}\right\rVert_{\infty}\right]\bm{1}\] \[\leq\frac{\alpha_{k-1}}{4}\bm{1}+\frac{2\sqrt{2}\alpha_{k-1}}{4(1 -\gamma)\sqrt{1024}V}\bm{1}+\frac{18}{(1-\gamma)^{2}}\left(\frac{\alpha_{k-1}^ {2}}{1024\cdot V^{2}}\right)^{3/4}\bm{1}\] \[\leq\left[\frac{\alpha_{k-1}}{8}+\frac{\alpha_{k-1}}{4}\right] \bm{1}\leq\frac{3}{8}\alpha_{k-1}\bm{1}.\]

Therefore in either case,

\[\bm{v}^{\star}-\bm{v}_{k-1}\leq\frac{\alpha_{k-1}}{2}\bm{1}=\alpha_{k}\bm{1}.\]

Moreover, we can use that \(\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\) to see that

\[\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\leq\mathcal{T}_{\pi_{k}}^{2}( \bm{v}_{k})\leq\cdots\leq\mathcal{T}_{\pi_{k}}^{\infty}(\bm{v}_{k})=\bm{v}^{ \pi_{k}}\leq\bm{v}^{\star}.\]

This completes the inductive step.

Consequently, taking \(k=K=\lceil\log_{2}(\varepsilon^{-1}(1-\gamma)^{-1})\rceil\) iterations of the outer loop, with probability \(1-\delta\), we have that \(0\leq\bm{v}^{\star}-\bm{v}^{\pi_{K}}\leq\bm{v}^{\star}-\bm{v}_{K}\leq\alpha_{K}\leq\varepsilon\) and

\[\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\leq\mathcal{T}_{\pi_{k}}^{2}( \bm{v}_{k})\leq\cdots\leq\mathcal{T}_{\pi_{k}}^{\infty}(\bm{v}_{k})=\bm{v}^{\pi _{k}}\leq\bm{v}^{\star},\]

that is, \(\bm{v}_{k}\) is an \(\varepsilon\)-optimal value and \(\pi_{K}\) is an \(\varepsilon\)-optimal policy.

The total number of samples and time required is \(\tilde{O}\left(\mathcal{A}_{\rm tot}\left(\varepsilon^{-2}V^{2}+(1-\gamma)^{-2} \right)\right)\). For the space complexity, note that the algorithm can be implemented to maintain only \(O(1)\) vectors in \(\mathbb{R}^{\mathcal{A}_{\rm tot}}\). 

Theorem A.1 yields improved complexities for solving MDPs when \(\left\lVert(\bm{I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{\star}}} \right\rVert_{\infty}\) is nontrivially bounded. Following [37] we mention two particular such settings where we can apply Theorem A.1 to obtain better problem-dependent sample and runtime bounds than Theorem 1.1.

Deterministic MDPsFor a deterministic MDP, each action deterministically transitions to a single state. That is, for all \((s,a)\in\mathcal{A}\), \(\bm{p}_{a}(s)=\bm{1}_{s^{\prime}}\) (the indicator vector of \(s^{\prime}\in\mathcal{S}\)) for some \(s^{\prime}\in\mathcal{S}\). In this case, \(\bm{\sigma}_{\bm{v}^{*}}=\bm{0}\). Consequently, if the MDP is deterministic, the algorithm converges with just \(\tilde{O}((1-\gamma)^{3})\) samples to the generative model and time. We note that in this setting of deterministic MDPs, there may be alternative approaches to obtain the same or better runtime and sample complexity.

Small range.Define the range of optimal values for a MDP as \(\operatorname*{rng}(\bm{v}^{*})\stackrel{{\text{def}}}{{=}} \max_{s\in\mathcal{S}}\bm{v}_{s}^{*}-\min_{s\in\mathcal{S}}\bm{v}_{s}^{*}\). Note that \(\bm{\sigma}_{\bm{v}^{*}}\leq\operatorname*{rng}(\bm{v}^{*})^{2}\bm{1}\). So, \(\left\lVert\bm{(I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{*}}} \right\rVert_{\infty}\leq(1-\gamma)^{-1}\operatorname*{rng}(\bm{v}^{*})\). Therefore, by Theorem A.1, given an approximate upper bound of \(\left\lVert\bm{(I}-\gamma\bm{P}^{\star})^{-1}\sqrt{\bm{\sigma}_{\bm{v}^{*}}} \right\rVert_{\infty}\) our algorithm is implementable with \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}(\varepsilon^{-2}(1-\gamma)^{-2} \operatorname*{rng}(\bm{v}^{\star})^{2}+(1-\gamma)^{-2}))\) samples and time.

Highly mixing domains.[37] showed that a contextual bandit problem can be modeled as an MDP where the next state is sampled from a fixed stationary distribution. Using the fact that the transition function is independent of the prior state and action, the authors of [38] show that \(\operatorname*{rng}(\bm{v}^{*})\leq 1\) with a simple proof in its Appendix A.2. Hence, by the argument in the preceding paragraph \(\tilde{O}\left(\mathcal{A}_{\mathrm{tot}}\left(\varepsilon^{-2}(1-\gamma)^{-2 }+(1-\gamma)^{-2}\right)\right)\) samples and time suffice in this setting.

## Appendix B Omitted proofs from the main body

### Omitted proof of Lemma 1.3

See 1.3

Proof.: Consider the \(i\)-th entry \((c-x)_{i}\). There are three cases.

First, suppose \(a_{i}-(1-\gamma)\alpha\leq b_{i}\leq a_{i}+(1-\gamma)\alpha\). Then, \(\left\lvert c_{i}-x_{i}\right\rvert=\left\lvert b_{i}-x_{i}\right\rvert\leq \gamma\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\)

Second, suppose \(b_{i}\leq a_{i}-(1-\gamma)\alpha\leq a_{i}+(1-\gamma)\alpha\). Then, \(c_{i}-x_{i}\geq b_{i}-x_{i}\geq-\left\lVert b-x\right\rVert_{\infty}\geq- \gamma\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}.\) Meanwhile, \(c_{i}-x_{i}=a_{i}-(1-\gamma)\alpha-x_{i}\leq\left\lVert\bm{a}-\bm{x}\right\rVert _{\infty}-(1-\gamma)\alpha.\) Now, because \(\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\leq\alpha\), we have that \((1-\gamma)\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\leq(1-\gamma)\alpha\). So, \(\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}-(1-\gamma)\alpha\leq\gamma \left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}.\)

Lastly, suppose \(a_{i}-(1-\gamma)\alpha\leq a_{i}+(1-\gamma)\alpha\leq b_{i}\). Then, \(c_{i}-x_{i}\leq b_{i}-x_{i}\leq\left\lVert\bm{b}-\bm{x}\right\rVert_{\infty} \leq\gamma\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\). Meanwhile, \(c_{i}-x_{i}=a_{i}+(1-\gamma)\alpha-x_{i}\geq-\left\lVert a-x\right\rVert_{ \infty}+(1-\gamma)\alpha.\) Now, because \(\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\leq\alpha\), we have that \((1-\gamma)\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}\leq(1-\gamma)\alpha\). So, \(-\left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}+(1-\gamma)\alpha\geq\gamma \left\lVert\bm{a}-\bm{x}\right\rVert_{\infty}.\)

### Omitted proofs from Section 2

First, we prove Lemma 2.1.

See 2.1

Proof.: The first statement follows from linearity of expectation and the second from definitions. The third statement follows from independence and that

\[\text{Var}\left[v_{i_{m}}\right]=\sum_{i\in\mathcal{S}}p_{i}v_{i}^{2}-(\bm{p} ^{\top}\bm{v})^{2}\leq\sum_{i\in\mathcal{S}}p_{i}\left\lVert\bm{v}\right\rVert _{\infty}^{2}=\left\lVert\bm{v}\right\rVert_{\infty}^{2}\text{ for any }m\in\left[M\right].\]

Next, we state Freedman's inequality [35], which we use to prove the following Lemma 2.2.

See B.1

Proof.: The first statement follows from linearity of expectation and the second from definitions. The third statement follows from independence and that

\[\text{Var}\left[v_{i_{m}}\right]=\sum_{i\in\mathcal{S}}p_{i}v_{i}^{2}-(\bm{p} ^{\top}\bm{v})^{2}\leq\sum_{i\in\mathcal{S}}p_{i}\left\lVert\bm{v}\right\rVert _{\infty}^{2}=\left\lVert\bm{v}\right\rVert_{\infty}^{2}\text{ for any }m\in\left[M\right].\]

Next, we state Freedman's inequality [35], which we use to prove the following Lemma 2.2.

See B.1

Proof.: The first statement follows from linearity of expectation and the second from definitions. The third statement follows from independence and that

\[\text{Var}\left[v_{i_{m}}\right]=\sum_{i\in\mathcal{S}}p_{i}v_{i}^{2}-(\bm{p} ^{\top}\bm{v})^{2}\leq\sum_{i\in\mathcal{S}}p_{i}\left\lVert\bm{v}\right\rVert _{\infty}^{2}=\left\lVert\bm{v}\right\rVert_{\infty}^{2}\text{ for any }m\in\left[M\right].

**Lemma 2.2**.: _Let \(T\in\mathbb{Z}_{>0}\) and \(\bm{w}^{(0)},\bm{w}^{(1)},...,\bm{w}^{(T)}\in\mathbb{R}^{\mathcal{S}}\) such that \(\left\|\bm{w}^{(i)}-\bm{w}^{(i-1)}\right\|_{\infty}\leq\tau\) for all \(i\in[T]\). Then, for any \(\bm{p}\in\Delta^{\mathcal{S}}\), \(\delta\in(0,1)\), and \(M\geq 2^{8}T\log(2/\delta)\) with probability \(1-\delta\), \(\left|\bm{p}^{\top}(\bm{w}^{(t)}-\bm{w}^{(0)})-\sum_{i\in[t]}\sum_{j\in[M]} \mathsf{Sample}(\bm{w}^{(i)}-\bm{w}^{(i-1)},\bm{p},1,0)\cdot 1/M\right|\leq\tau/8\) for all \(t\in[T]\)._

Proof.: For each \(i\in[T],j\in[M]\), let

\[X_{i,j}:=\left(\mathsf{Sample}(\bm{w}^{(i)}-\bm{w}^{(i-1)},\bm{p},1,0)-\bm{p}^ {\top}(\bm{w}^{(i)}-\bm{w}^{(i-1)})\right)/M.\]

Since \(\bm{p}\in\Delta^{\mathcal{S}}\), Lemma 2.1 yields that \(|X_{i,j}|\leq\frac{2\tau}{M}\). Next, define \(Y_{t,k}:=\sum_{i\in[t-1]}\sum_{j\in[M]}X_{i,j}+\sum_{j=1}^{k}X_{t,j}\). The predictable quadratic variation process (as defined in Theorem B.1) is given by

\[W_{t,k} =\sum_{i\in[t-1]}\sum_{j\in[M]}\mathbb{E}\left[X_{i,j}^{2}|X_{1,1: M},...,X_{i-1,1:M},X_{i,1:j-1}\right]+\sum_{j\in[k]}\mathbb{E}\left[X_{t,j}^{2}|X_ {1,1:M},...,X_{t-1,1:M},X_{t,1:j-1}\right]\] \[=\sum_{i\in[t-1]}\sum_{j\in[M]}\text{Var}\left[\frac{\mathsf{Sample }(\bm{w}^{(i)}-\bm{w}^{(i-1)},\bm{p},1,0)}{M}\right]+\sum_{j\in[k]}\text{Var} \left[\frac{\mathsf{Sample}(\bm{w}^{(t)}-\bm{w}^{(t-1)},\bm{p},1,0)}{M}\right]\] \[\leq\sum_{i\in[t]}\sum_{j\in[M]}\frac{\tau^{2}}{M^{2}}=\frac{T \tau^{2}}{M}\]

where, in the last line we used Lemma 2.1 to bound the variance. Now, by telescoping,

\[Y_{t,M}=\left(\sum_{i\in[t]}\sum_{j\in[M]}\frac{\mathsf{Sample}(\bm{w}^{(i)}- \bm{w}^{(i-1)},\bm{p},1,0)}{M}\right)-\bm{p}^{\top}(\bm{w}^{(t)}-\bm{w}^{(0)}) \text{ for all }t\in[T]\]

Consequently, applying Theorem B.1 twice (once to \(Y_{t,M}\) and once to \(-Y_{t,M}\) yields

\[\mathbb{P}\left\{\exists t\in[T]:|Y_{t,M}|\geq\frac{\tau}{8}\right\}\leq 2 \exp\left(-\frac{(\tau/8)^{2}}{2(\frac{T\tau^{2}}{M}+\frac{2\tau}{M}\cdot\frac{ \tau}{8}\cdot\frac{1}{3}}\right)=2\exp\left(\frac{-M}{2^{7}\left(T+\frac{1}{12 }\right)}\right)\leq\delta\,.\]

As an immediate corollary of Lemma 2.2, we obtain Corollary 2.3.

**Corollary 2.3**.: _In \(\mathtt{TVRVI}\) (Algorithm 3), with probability \(1-\delta\), in Lines 9, 10 and 2, for all \(s\in\mathcal{S},a\in\mathcal{A}_{s}\), and \(\ell\in[L]\), we have \(\left|\bm{g}_{a}^{(\ell)}(s)-\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0 )})\right|\leq(1-\gamma)\alpha/8\) and therefore \(\hat{\bm{g}}_{a}^{(\ell)}\) is a \((1-\gamma)\alpha/4\)-underestimate of \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0)})\)._

Proof.: Consider some \(s\in\mathcal{S}\) and \(a\in\mathcal{A}_{s}\). Note that \(\bm{g}_{a}^{(\ell)}(s)\) is equal in distribution to

\[\left(\sum_{i\in[\ell-1]}\sum_{j\in[M]}\frac{\mathsf{Sample}(\bm{v}^{(i)}-\bm{ v}^{(i-1)},\bm{p}_{a}(s),1,0)}{M}\right)-\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}- \bm{v}^{(0)}).\]

Then, by Lemma 2.2 and union bound, whenever \(M\geq L\cdot 2^{8}\log(2\mathcal{A}_{\mathrm{tot}}/\delta)\) we have that with probability \(1-\delta\), for all \((s,a)\in\mathcal{A}\), \(\left|\bm{g}_{a}^{(\ell)}(s)-\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{( 0)})\right|\leq\frac{1-\gamma}{8}\alpha\) and conditioning on this event, we have \(\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0)})-\frac{1-\gamma}{4}\alpha \leq\hat{\bm{g}}_{a}^{(\ell)}(s)\leq\bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}- \bm{v}^{(0)})\) due to the shift in Line 10. 

Conditioning on the event that the implication of Corollary 2.3 holds, we can prove the following Lemma 2.4

**Lemma 2.4**.: _Suppose that for some \(\bm{\beta}\in\mathbb{R}^{\mathcal{A}}_{\geq 0}\), \(\bm{P}\bm{v}^{(0)}-\bm{\beta}\leq\bm{x}\leq\bm{P}\bm{v}^{(0)}\) and let \(\bm{\beta}_{\pi^{*}}\in\mathbb{R}^{\mathcal{S}}\) be defined as \(\bm{\beta}_{\pi^{*}}(s):=\bm{\beta}_{\pi^{*}(s)}(s)\) for each \(s\in\mathcal{S}\). Then, with probability \(1-\delta\), at the end of every iteration \(\ell\in[L]\) (Line 3) in \(\mathtt{TVRVI}(\bm{v}^{(0)},\pi^{(0)},\bm{x},\alpha,\delta)\), the following hold for \(\bm{\xi}:=\gamma((1-\gamma)\alpha/4\bm{1}+\bm{\beta}_{\pi^{*}})\):_

\[\bm{v}^{(\ell-1)}\leq\bm{v}^{(\ell)}\leq\mathcal{T}_{\pi^{(\ell)}}(\bm{v}^{( \ell)}),\] (6)

\[0\leq\bm{v}^{*}-\bm{v}^{(\ell)}\leq\max\left(\gamma\bm{P}^{*}(\bm{v}^{*}-\bm{v}^{( \ell-1)})+\bm{\xi},\gamma(\bm{v}^{*}-\bm{v}^{(\ell-1)})\right).\] (7)Proof.: In the remainder of this proof, condition on the event that the implications of Corollary 2.3 hold (as they occur with probability \(1-\delta\)). By Line 7 and 8 of Algorithm 3, for all \(\ell\in[L]\),

\[\bm{v}^{(\ell-1)}\leq\bm{v}^{(\ell)}\leq\bm{v}^{(\ell-1)}+(1-\gamma)\alpha\bm{1}.\]

This immediately implies the lower bound in (6).

We prove the upper bound in (6) by induction. In the base case when \(\ell=0\), \(\bm{v}^{(0)}\leq\mathcal{T}_{\pi^{(\ell)}}(\bm{v}^{(0)})\) holds by assumption. For the \(\ell\)-th iteration, there are two cases. If \(\bm{v}^{(\ell)}(s)>\bm{v}^{(\ell-1)}(s)\) for \(s\in S\) then

\[\bm{v}^{(\ell)}(s) =\bm{r}_{\pi^{(\ell)}}(s)+\gamma\left(\bm{x}(s)+\hat{\bm{g}}_{ \pi^{(\ell)}}^{(\ell)}(s)\right)\leq\bm{r}_{\pi^{(\ell)}}(s)+\gamma\bm{p}_{ \pi^{(\ell)}}(s)^{\top}\bm{v}^{(\ell-1)}(s)\] (10) \[\leq\mathcal{T}_{\pi^{(\ell)}}(\bm{v}^{(\ell-1)})\leq\mathcal{T}_ {\pi^{(\ell)}}(\bm{v}^{(\ell)}).\]

Otherwise, if \(\bm{v}^{(\ell)}(s)=\bm{v}^{(\ell-1)}(s)\), then by the inductive hypothesis,

\[\bm{v}^{(\ell)}(s)=\bm{v}^{(\ell-1)}(s)\leq\mathcal{T}_{\pi^{(\ell-1)}}(\bm{v }^{(\ell-1)})(s)=\mathcal{T}_{\pi^{(\ell)}}(\bm{v}^{(\ell)})(s)\,.\]

This completes the proof of (6).

Next, we prove (7). For the lower bound, by induction and (10), we have that for each \(s\in\mathcal{S}\)

\[\tilde{\bm{v}}^{(\ell)}(s)\leq\max_{a\in\mathcal{A}_{s}}\{\bm{r}_{a}(s)+ \gamma\bm{p}_{a}(s)^{\top}\bm{v}^{(\ell-1)}(s)\}\leq\max_{a\in\mathcal{A}_{s} }\{\bm{r}_{a}(s)+\gamma\bm{p}_{a}(s)^{\top}\bm{v}^{\star}(s)\}=\bm{v}^{\star},\]

so \(\min(\tilde{\bm{v}}^{(\ell)},\bm{v}^{(\ell-1)}+(1-\gamma)\alpha)\leq\bm{v}^{\star}\).

Next, we prove the upper bound of (7). For each \((s,a)\in\mathcal{A}\) and \(\ell\in[L]\), let

\[\bm{\xi}_{a}^{(\ell)}(s):=\bm{p}_{a}(s)^{\top}\bm{v}^{(\ell-1)}-(\bm{x}_{a}(s )+\hat{\bm{g}}_{a}^{(\ell)})(s)),\]

and observe that

\[\bm{\xi}_{a}^{(\ell)}(s)=[\bm{p}_{a}(s)^{\top}\bm{v}^{(0)}-\bm{x}_{a}(s)]+[ \bm{p}_{a}(s)^{\top}(\bm{v}^{(\ell-1)}-\bm{v}^{(0)})-\hat{\bm{g}}_{a}^{(\ell)} )(s))]\leq\bm{\beta}_{a}(s)+\frac{(1-\gamma)\alpha}{4}.\]

Note that for any \(s\in\mathcal{S}\),

\[(\bm{v}^{\star}-\tilde{\bm{v}}^{(\ell)})(s) =\max_{a\in\mathcal{A}_{i}}[\bm{r}_{a}(s)+\gamma\bm{p}_{a}(s)^{ \top}\bm{v}^{\star}(s)]-\max_{a\in\mathcal{A}_{s}}[\bm{r}_{a}(s)+\gamma(\bm{ x}_{a}(s)+\hat{\bm{g}}_{a}^{(\ell)})(s))]\] \[\leq[\bm{r}_{\pi^{\star}(s)}(s)+\gamma\left(\bm{P}^{\star}\bm{v }^{\star}\right)(s)]-\max_{a\in\mathcal{A}_{s}}[\bm{r}_{a}(s)+\gamma\bm{p}_{a }(s)^{\top}\bm{v}^{(\ell-1)}-\gamma\bm{\xi}_{a}^{(\ell)}(s)]\] \[\leq[\bm{r}_{\pi^{\star}(s)}(s)+\gamma\left(\bm{P}^{\star}\bm{v }^{\star}\right)(s)]-[\bm{r}_{\pi^{\star}(s)}(s)+\gamma(\bm{P}^{\star}\bm{v}^{ (\ell-1)})(s)-\gamma\bm{\xi}_{\pi^{\star}(s)}^{(\ell)}(s)]\] \[\leq\gamma\left(\bm{P}^{\star}(\bm{v}^{\star}-\bm{v}^{(\ell-1)} )\right)(s)+\bm{\xi}(s),\]

Consequently, for all \(s\in S\),

\[(\bm{v}^{\star}-\tilde{\bm{v}}^{(\ell)})(s)\leq\gamma\bm{P}^{\star}(\bm{v}^{ \star}-\bm{v}^{(\ell-1)})(s)+\bm{\xi}(s).\]

Consider two cases for \(\bm{v}^{(\ell)}(s)\). First, if \(\bm{v}^{(\ell)}(s)=\tilde{\bm{v}}^{(\ell)}(s)\) for some \(s\in\mathcal{S}\) then

\[\left(\bm{v}^{\star}-\bm{v}^{(\ell)}\right)(s)\leq\gamma\left(\bm{P}^{\star} \left(\bm{v}^{\star}-\bm{v}^{(\ell-1)}\right)\right)(s)+\bm{\xi}(s)\]

holds immediately. If not, \(\bm{v}^{(\ell)}(s)=\bm{v}^{(\ell-1)}(s)+(1-\gamma)\alpha\leq\tilde{\bm{v}}^{( \ell)}(s)\) and (6) guarantees that

\[\left\|\bm{v}^{\star}-\bm{v}^{(\ell-1)}\right\|_{\infty}\leq\left\|\bm{v}^{ \star}-\bm{v}^{(0)}\right\|_{\infty}\leq\alpha,\]

which ensures that \((1-\gamma)(\bm{v}^{\star}-\bm{v}^{(\ell-1)})(s)\leq(1-\gamma)\alpha\) and yields the results as,

\[\left(\bm{v}^{\star}-\bm{v}^{(\ell)}\right)(s)=\left(\bm{v}^{\star}-\bm{v}^{( \ell-1)}\right)(s)-(1-\gamma)\alpha\leq\gamma\left(\bm{v}^{\star}-\bm{v}^{( \ell-1)}\right)(s)\,.\]

We now inductively apply Lemma 2.4 to obtain Corollary 2.5, which allows us to bound the number of iterates required to halve the initial error in TVRVI.

**Corollary 2.5**.: _Suppose that for some \(\alpha\geq 0\) and \(\bm{\beta}\in\mathbb{R}^{\mathcal{A}}_{\geq 0}\), \(\bm{P}\bm{v}^{(0)}-\bm{\beta}\leq\bm{x}\leq\bm{P}\bm{v}^{(0)}\); \(\bm{v}^{(0)}\) is an \(\alpha\)-underestimate of \(\bm{v}^{\star}\); and \(\bm{v}^{(0)}\leq\mathcal{T}_{\bm{\pi}^{(0)}}(\bm{v}^{(0)})\). Let \(\bm{\beta}_{\bm{\pi}^{\star}}\in\mathbb{R}^{\mathcal{S}}\) be defined as \(\beta_{\bm{\pi}^{\star}}(s):=\beta_{\bm{\pi}^{\star}(s)}(s)\) for each \(s\in\mathcal{S}\). Let \((\bm{v}^{(L)},\pi^{(L)})=\texttt{TVRVI}(\bm{v}^{(0)},\pi^{(0)},\alpha,\delta)\), and \(L,M\) be as in Line 2. Define \(\bm{\xi}:=\gamma\left((1-\gamma)\alpha/4\cdot\bm{1}+\bm{\beta}_{\bm{\pi}^{ \star}}\right)\). Then, with probability \(1-\delta\), \(\bm{0}\leq\bm{v}^{\star}-\bm{v}^{(L)}\leq\gamma^{L}\alpha\cdot\bm{1}+(\bm{I}- \gamma\bm{P}^{\star})^{-1}\bm{\xi}\), and \(\bm{v}^{(L)}\leq\mathcal{T}_{\pi^{(L)}}(\bm{v}^{(L)})\). In particular, if \(\bm{\beta}=\bm{0}\), then for \(L>\log(8)(1-\gamma)^{-1}\) we can reduce the error in \(\bm{v}^{(0)}\) by half: \(\bm{0}\leq\bm{v}^{\star}-\bm{v}^{(L)}\leq(\bm{v}^{\star}-\bm{v}^{(0)})/2\). Additionally, \(\texttt{TVRVI}\) is implementable with \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}ML)\) sample queries to the generative model and time and \(O(\mathcal{A}_{\mathrm{tot}})\) space._

Proof.: Condition on the event that the implication of Lemma 2.4 holds. First, we observe that \(\bm{0}\leq\bm{v}^{\star}-\bm{v}_{\pi^{(L)}}\leq\bm{v}^{\star}-\bm{v}^{(L)}\) follows by monotonicity (Equation (6) of Lemma 2.4). Next, we show that

\[\bm{v}^{\star}-\bm{v}^{(L)}\leq\gamma^{L}\alpha\cdot\bm{1}+(\bm{I}-\gamma\bm{ P}^{\star})^{-1}\bm{\xi},\]

by induction. We will show that for all \(i\in\mathcal{S}\),

\[\bm{v}^{\star}-\bm{v}^{(\ell)}\leq\left[\gamma^{\ell}\alpha\bm{1}+\sum_{k=0} ^{\ell}\gamma^{k}\bm{P}^{\star k}\bm{\xi}\right].\]

In the base case when \(\ell=0\), this is trivially true, as \(\bm{v}^{\star}-\bm{v}^{(\ell)}\leq\alpha\bm{1}\) by assumption. Assume that the statement is true up to \(\bm{v}^{(\ell-1)}\). Now, by Lemma 2.4, we have two cases for \([\bm{v}^{\star}-\bm{v}^{(\ell)}](i)\).

First, suppose that \([\bm{v}^{\star}-\bm{v}^{(\ell)}](i)\leq\gamma[\bm{v}^{\star}-\bm{v}^{(\ell-1) }](i)\). Then, note that \(\bm{P}^{\star}\) and \(\bm{\xi}\) are entrywise non-negative, so \([\gamma^{\ell}\bm{P}^{\star k}\bm{\xi}](i)\geq 0\). By inductive hypothesis, and the fact that \(\gamma\in(0,1)\) we have

\[[\bm{v}^{\star}-\bm{v}^{(\ell)}](i) \leq\gamma\left(\gamma^{(\ell-1)}\alpha+\left[\sum_{k=0}^{\ell-1} \gamma^{k}\bm{P}^{\star k}\bm{\xi}\right](i)\right)\] \[=\gamma^{\ell}\alpha+\gamma\left[\sum_{k=0}^{\ell-1}\gamma^{k} \bm{P}^{\star k}\xi\right](i)\leq\gamma^{\ell}\alpha+\left[\sum_{k=0}^{\ell-1} \gamma^{k}\bm{P}^{\star k}\bm{\xi}\right](i)\leq\gamma^{\ell}\alpha+\left[ \sum_{k=0}^{\ell}\gamma^{k}\bm{P}^{\star k}\bm{\xi}\right](i)\] \[=\left[\gamma^{\ell}\alpha\bm{1}+\sum_{k=0}^{\ell}\gamma^{k}\bm{ P}^{\star k}\bm{\xi}\right](i).\]

Second, suppose that instead, \([\bm{v}^{\star}-\bm{v}^{(\ell)}](i)\leq\left[\gamma\bm{P}^{\star}\left(\bm{v}^{ \star}-\bm{v}^{(\ell-1)}\right)\right](i)+\bm{\xi}(i)\). By monotonicity (equation (6) of Lemma 2.4) we know that \(\bm{v}^{\star}-\bm{v}^{(\ell-1)}\geq 0\). Moreover, \(\bm{P}^{\star}\) is non-negative, and consequently, we can use the inductive hypothesis as follows:

\[\left(\bm{v}^{\star}-\bm{v}^{(\ell-1)}\right)\leq\left[\gamma^{\ell-1}\alpha \bm{1}+\sum_{k=0}^{\ell-1}\gamma^{k}\bm{P}^{\star k}\xi\right],\text{ hence }\bm{P}^{\star}\left(\bm{v}^{\star}-\bm{v}^{(\ell-1)}\right)\leq\bm{P}^{\star} \left[\gamma^{\ell-1}\alpha\bm{1}+\sum_{k=0}^{\ell-1}\gamma^{k}\bm{P}^{\star k }\bm{\xi}\right].\]

We can rearrange terms to obtain the following bound:

\[[\bm{v}^{\star}-\bm{v}^{(\ell)}](i) \leq\left[\gamma\bm{P}^{\star}\left(\gamma^{(\ell-1)}\alpha\bm{1 }+\sum_{k=0}^{\ell-1}\gamma^{k}\bm{P}^{\star k}\bm{\xi}\right)\right](i)+\bm{ \xi}(i)\] \[=\gamma^{\ell}\alpha[\bm{P}^{\star}\bm{1}](i)+\left[\sum_{k=0}^{ \ell-1}\gamma^{k+1}\bm{P}^{\star k+1}\bm{\xi}\right](i)+\bm{\xi}(i)\leq\left[ \gamma^{\ell}\alpha\bm{1}+\sum_{k=0}^{\ell}\gamma^{k}\bm{P}^{\star k}\bm{\xi} \right](i).\]

Consequently, by induction, the bound holds. When \(L>\log(8)(1-\gamma)^{-1}\), \(\gamma^{L}\leq 1/8\) and we have

\[\bm{v}^{\star}-\bm{v}_{k}\leq\gamma^{L}\alpha\cdot\bm{1}+(\bm{I}-\gamma\bm{P}^{ \star})^{-1}\frac{\gamma(1-\gamma)}{4}\alpha\bm{1}\leq\gamma^{L}\alpha+\gamma \frac{\alpha}{4}\leq\frac{\alpha}{2}.\]

Finally, the sample complexity and runtime follow from the algorithm pseudocode. For the space complexity, at each iteration \(\ell\) of the outer for loop in \(\texttt{TVRVI}\), the algorithm needs only to maintain \(\hat{\bm{g}}^{(\ell)},\bm{g}^{(\ell)}\in\mathbb{R}^{\mathcal{A}_{\mathrm{tot}}}\), \(\bm{v}^{(\bm{\ell})}\in\mathbb{R}^{S}\), \(\pi^{(L)}\), and at most \(M\mathcal{A}_{\mathrm{tot}}\) samples in invoking Sample.

Finally, we are ready to prove Theorem 1.2.

**Theorem 1.2**.: _In the offline setting, there is an algorithm that uses \(\tilde{O}(\operatorname{nnz}(\bm{P})+\mathcal{A}_{\operatorname{tot}}(1-\gamma) ^{-2})\) time, and computes an \(\varepsilon\)-optimal policy and \(\varepsilon\)-optimal values with probability \(1-\delta\)._

Proof.: To run OfflineTVRVI, we can implement a generative model from which we can draw samples in \(O(\operatorname{nnz}(\bm{P}))\) pre-processing time, so that each query to the generative model requires \(\tilde{O}(1)\) time. For the correctness, we induct on \(k\) to show that after each iteration \(k\), \(0\leq\bm{v}^{\star}-\bm{v}_{\pi_{K}}\leq\bm{v}^{\star}-\bm{v}_{K}\leq\alpha_{k}\) with probability \(1-k\delta/K\). In the base case when \(k=0\), the bound is trivially true as \(\left\lVert\bm{v}^{\star}\right\rVert_{\infty}\leq(1-\gamma)^{-1}\). Now, by Applying Corollary 2.5 and a union bound, we see that with probability \(1-k\delta/K\), \(\bm{v}^{\star}-\bm{v}_{k}\leq\frac{\alpha_{k-1}}{2}=\alpha_{k}\), whenever \(L>\log(8)(1-\gamma)^{-1}\). Thus, \(\bm{v}_{K}\) satisfies the required guarantee whenever \(\alpha_{K}\leq\varepsilon\), which is guaranteed by our choice of \(K\). To see that \(\pi_{k}\) is an \(\varepsilon\)-optimal policy, we observe that Corollary 2.5 ensures

\[\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\leq\mathcal{T}_{\pi_{k}}^{2} (\bm{v}_{k})\leq\cdots\leq\mathcal{T}_{\pi_{k}}^{\infty}(\bm{v}_{k})=\bm{v}^{ \star_{k}}\leq\bm{v}^{\star}.\]

For the runtime, the algorithm completes only \(K=\tilde{O}(1)\) iterations, and can be implemented with \(\tilde{O}(1)\) calls to the offset oracle. Each inner loop iteration can be implemented with \(\tilde{O}(\mathcal{A}_{\operatorname{tot}}L^{2})=\tilde{O}\left(\mathcal{A}_ {\operatorname{tot}}(1-\gamma)^{-2}\right)\) additional time and queries to the generative model. The algorithm only requires \(O(\mathcal{A}_{\operatorname{tot}})\) space in order to store offsets, values, and approximate utilities. 

### Omitted proofs from Section 3

**Theorem B.2** (Hoeffding's Inequality and Bernstein's Inequality, restated from Lemma E.1 and E.2 of [3]).: _Let \(\bm{p}\in\Delta^{\mathcal{S}}\) be a probability vector, \(\bm{v}\in\mathbb{R}^{n},\) and let \(\bm{y}:=\frac{1}{m}\sum_{j=1}^{m}\bm{v}(i_{j})\) where \(i_{j}\) are random indices drawn such that \(i_{j}=k\) with probability \(\bm{p}(k)\). Define \(\sigma:=(\bm{p}^{\top}\bm{v}^{2}-(\bm{p}^{\top}\bm{v})^{2})\). For any \(\delta\in(0,1)\), the following hold, each with probability \(1-\delta\):_

\[\begin{split}\text{(Hoeffding's Inequality)}\,\left\lvert\bm{p}^{ \top}\bm{v}-\bm{y}\right\rvert&\leq\left\lVert\bm{v}\right\rVert _{\infty}\cdot\sqrt{2m^{-1}\log(2\delta^{-1})},\\ \text{(Bernstein's Inequality)}\,\left\lvert\bm{p}^{\top}\bm{v}-\bm{y} \right\rvert&\leq\sqrt{2m^{-1}\sigma\cdot\log(2\delta^{-1})}+(2/ 3)m^{-1}\left\lVert\bm{v}\right\rVert_{\infty}\cdot\log(2\delta^{-1}).\end{split}\]

Theorem B.2 illustrates that the error in estimating \(\bm{Pu}\) for some value vector \(\bm{u}\) depends on the variance \(\bm{\sigma_{u}}:=\bm{Pu}^{2}-(\bm{Pu})^{2}\in\mathbb{R}^{A}\). To bound this variance term, we appeal to the following two lemmas from [3].

**Lemma B.3** (Lemma 5.2 of ([3]), restated).: \(\sqrt{\bm{\sigma_{v}}}\leq\sqrt{\bm{\sigma_{v^{\star}}}}+\left\lVert\bm{v}^{ \star}-\bm{v}\right\rVert_{\infty}\bm{1}\)_._

**Lemma B.4** (Lemma C.1 of ([3]), restated).: _For any \(\pi\), we have_

\[\left\lVert(\bm{I}-\gamma\bm{P}^{\pi})^{-1}\sqrt{\bm{\sigma_{v^{\star}}}} \right\rVert_{\infty}^{2}\leq\frac{1+\gamma}{\gamma^{2}(1-\gamma)^{3}}.\]

We can now bound the error in estimating \(\bm{Pu}\) using \(\texttt{ApxUtility}(\bm{u},N,\eta)\). The following Lemma 3.1 obtains such a bound by following a similar argument to that of Lemma 5.1 of [3].

**Lemma 3.1**.: _Consider \(\bm{u}\in\mathbb{R}^{\mathcal{S}}\). Let \(\bm{x}=\texttt{ApxUtility}(\bm{u},m\cdot\mathcal{A}_{\operatorname{tot}},\eta)\), \(m\geq\log(1/2\delta^{-1})\), and \(\eta=(m\mathcal{A}_{\operatorname{tot}})^{-1}\log(1/2\delta^{-1})\). Then, with probability \(1-\delta\),_

\[\bm{Pu}-2\sqrt{2\eta\bm{\sigma_{v^{\star}}}}+\left(2\sqrt{2\eta}\left\lVert\bm {u}-\bm{v}^{\star}\right\rVert_{\infty}+18\eta^{3/4}\left\lVert\bm{u}\right\rVert _{\infty}\right)\leq\bm{x}\leq\bm{Pu}.\]

Proof.: For \(s\in\mathcal{S}\) and \(a\in\mathcal{A}_{s}\). Let \(i_{1},...,i_{N}\in\mathcal{S}\) be random indices such that \(\mathbb{P}\left\{i_{j}=t\right\}=(\bm{p}_{a}(s))(t)\) for each \(j\in[N]\). Define the vectors \(\tilde{\bm{x}}\) and \(\hat{\bm{\sigma}}\) as follows.

\[\tilde{\bm{x}}_{a}(s):=\frac{1}{N}\sum_{j=1}^{N}\bm{u}(i_{j})\text{ and }\hat{\bm{\sigma}}_{a}(s):=\frac{1}{N}\sum_{j=1}^{N}\left(\bm{u}(i_{j})\right)^{2}-( \tilde{\bm{x}}_{a}(s))^{2}.\]From the pseudocode of \(\mathtt{ApxUtility}\) (Algorithm 1), we see that that \(\boldsymbol{x}=\tilde{\boldsymbol{x}}-\sqrt{2\eta\tilde{\boldsymbol{\sigma}}}-4 \eta^{3/4}\left\|\boldsymbol{u}\right\|_{\infty}-(2/3)\eta\left\|\boldsymbol{u} \right\|_{\infty}.\) Now, by union bound over all state-action pairs \((s,a)\) and Theorem B.2, we have that with probability \(1-\delta/2\) for each sate-action pair \((s,a)\),

\[\left\|\boldsymbol{x}-\boldsymbol{P}\boldsymbol{u}_{\infty}\leq\sqrt{2\eta \boldsymbol{\sigma}_{\boldsymbol{u}}}\right\|+\frac{2}{3}\eta\left\|\boldsymbol {u}\right\|_{\infty}\mathbf{1}.\] (11)

and with probability \(1-\delta/2\) for each sate-action pair \((s,a)\),

\[\left\|\frac{1}{N}\sum_{j\in[N]}\left(\hat{\boldsymbol{\sigma}}_{a}(s)\right) ^{2}-\boldsymbol{p}_{a}(s)^{\top}\boldsymbol{u}^{2}\right\|\leq\left\| \boldsymbol{u}\right\|_{\infty}^{2}\sqrt{2\eta}_{\infty}.\]

Consequently, by union bound and triangle inequality and (11), we have that with probability \(1-\delta\) both of the following hold.

\[\left\|\tilde{\boldsymbol{x}}-\boldsymbol{P}\boldsymbol{u}\right\|_{\infty} \leq\sqrt{2\eta\boldsymbol{\sigma}_{\boldsymbol{u}}}+\frac{2}{3}\eta\left\| \boldsymbol{u}\right\|_{\infty}\mathbf{1},\text{ and }\left\|\hat{\boldsymbol{\sigma}}- \boldsymbol{\sigma}_{\boldsymbol{u}}\right\|_{\infty}\leq 4\left\|\boldsymbol{u} \right\|_{\infty}^{2}\cdot\sqrt{2\eta}\mathbf{1}.\] (12)

We condition on (12) in the remainder of the proof. Now,

\[\left|\tilde{\boldsymbol{x}}-\boldsymbol{P}\boldsymbol{u}\right|\leq\sqrt{2 \eta\tilde{\boldsymbol{\sigma}}}+\left(4\eta^{3/4}\left\|\boldsymbol{u} \right\|_{\infty}+\frac{2}{3}\eta\left\|\boldsymbol{u}\right\|_{\infty}\right) \mathbf{1},\]

and we have that

\[\boldsymbol{P}\boldsymbol{u}-2\sqrt{2\eta\tilde{\boldsymbol{\sigma}}}-\left(8 \eta^{3/4}\left\|\boldsymbol{u}\right\|_{\infty}+\frac{4}{3}\eta\left\| \boldsymbol{u}\right\|_{\infty}\right)\mathbf{1}\leq\boldsymbol{x}\leq \boldsymbol{P}\boldsymbol{u}.\]

By (12) and Lemma B.3, we have that for \(\alpha:=\left\|\boldsymbol{u}-\boldsymbol{v}^{\ast}\right\|_{\infty}\),

\[\sqrt{\tilde{\boldsymbol{\sigma}}}\leq\sqrt{\boldsymbol{\sigma}_{\boldsymbol {u}}}+2\left\|\boldsymbol{u}\right\|_{\infty}(2\eta)^{1/4}\mathbf{1}\leq\sqrt{ \boldsymbol{\sigma}_{\boldsymbol{v}^{\ast}}}+\alpha\mathbf{1}+2\left\| \boldsymbol{u}\right\|_{\infty}(2\eta)^{1/4}\mathbf{1},\]

which implies that

\[\boldsymbol{x}\geq\boldsymbol{P}\boldsymbol{u}-2\sqrt{2\eta\boldsymbol{\sigma }_{\boldsymbol{v}^{\ast}}}-2\sqrt{2\eta}\alpha\mathbf{1}-16\eta^{3/4}\left\| \boldsymbol{u}\right\|_{\infty}\mathbf{1}-\frac{4}{3}\eta\left\|\boldsymbol{u }\right\|_{\infty}\mathbf{1}.\]

Since \(\eta\leq 1\),

\[2\sqrt{2\eta\boldsymbol{\sigma}_{\boldsymbol{v}^{\ast}}}+\left(2\sqrt{2\eta} \alpha+16\eta^{3/4}\left\|\boldsymbol{u}\right\|_{\infty}+\frac{4}{3}\eta \left\|\boldsymbol{u}\right\|_{\infty}\right)\mathbf{1}\leq 2\sqrt{2\eta \boldsymbol{\sigma}_{\boldsymbol{v}^{\ast}}}+\left(2\sqrt{2\eta}\alpha+18\eta^ {3/4}\left\|\boldsymbol{u}\right\|_{\infty}\right)\mathbf{1}.\]

**Theorem 1.1**.: _In the sample setting, there is an algorithm that uses \(\tilde{O}(\mathcal{A}_{\mathrm{tot}}[(1-\gamma)^{-3}\varepsilon^{-2}+(1- \gamma)^{-2}])\) samples and time and \(O(\mathcal{A}_{\mathrm{tot}})\) space, and computes an \(\varepsilon\)-optimal policy and \(\varepsilon\)-optimal values with probability \(1-\delta\)._

Proof.: Let \(K\), \(\alpha_{k}\), \((\boldsymbol{v}_{k},\pi_{k})\), and \(N_{k}\) be as defined in Lines 1, 4, 9, and 6 of SampleTVRVI\((\varepsilon,\delta)\). First, we show, by induction that for each \(k\in[K]\), with probability \(1-k\delta/K\),

\[\mathbf{0}\leq\boldsymbol{v}^{\star}-\boldsymbol{v}^{\pi_{k}}\leq\boldsymbol {v}^{\star}-\boldsymbol{v}_{k}\leq\alpha_{k}\mathbf{1}\text{ and }\boldsymbol{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\boldsymbol{v}_{k}).\]

In the base case when \(k=0\), the bound is trivially true because \(\mathbf{0}\leq\boldsymbol{v}^{\star}-\boldsymbol{v}_{\pi_{0}}\leq\boldsymbol {v}^{\star}-\boldsymbol{v}_{0}\leq(1-\gamma)^{-1}\). Now, for the inductive step, by Lemma 3.1 we see that with probability \(1-\delta/K\),

\[\boldsymbol{P}\boldsymbol{v}_{k-1}-\left[2\sqrt{2\eta_{k-1}\boldsymbol{\sigma}_ {\boldsymbol{v}^{\ast}}}+\left(2\sqrt{2\eta_{k-1}}\alpha_{k-1}+18\eta_{k-1}^{3/ 4}\left\|\boldsymbol{v}_{k-1}\right\|_{\infty}\right)\mathbf{1}\right]\leq \boldsymbol{x}_{k}\leq\boldsymbol{P}\boldsymbol{v}_{k-1}\] (13)

and, by inductive hypothesis, with probability \(1-(k-1)\delta/K\),

\[\mathbf{0}\leq\boldsymbol{v}^{\star}-\boldsymbol{v}^{\pi_{k-1}}\leq \boldsymbol{v}^{\star}-\boldsymbol{v}_{k-1}\leq\alpha_{k-1}\mathbf{1},\text{ and }\boldsymbol{v}_{k}\leq\mathcal{T}_{\pi_{k-1}}(\boldsymbol{v}_{k-1}).\] (14)

Consequently, by a union bound, with probability \(1-k\delta/K\), both (13) and (14) hold. Condition on this event for the remainder of the inductive step.

Next, we can apply Corollary 2.5 with

\[\bm{\beta}=2\sqrt{2\eta_{k-1}\bm{\sigma}_{\bm{v}^{\star}}}+\left(2\sqrt{2\eta_{k-1 }}\alpha_{k-1}+18\eta_{k-1}^{3/4}\left\|\bm{v}_{k-1}\right\|_{\infty}\right) \bm{1}.\]

Therefore,

\[0\leq\bm{v}^{\star}-\bm{v}_{k}\leq\gamma^{L}\alpha_{k-1}\cdot\bm{1}+(\bm{I}- \gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1}\leq\frac{\alpha_{k-1}}{8}\bm{1}+(\bm{ I}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1}\]

for \(\bm{\xi}_{k-1}\leq\frac{(1-\gamma)\alpha_{k-1}}{4}\bm{1}+2\sqrt{2\eta_{k-1} \bm{\sigma}_{\bm{v}^{\star}}}+\left(2\sqrt{2\eta_{k-1}}\alpha_{k-1}+18\eta_{k- 1}^{3/4}\left\|\bm{v}_{k-1}\right\|_{\infty}\right)\bm{1}\). By Lemma B.4 and the facts that \(\eta_{k-1}\leq(6500\cdot(1-\gamma)^{-3}\max((1-\gamma),\alpha_{k-1}^{-2}))^{-1}\) and \((\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{1}=1/(1-\gamma)\bm{1}\), we obtain

\[(\bm{I}-\gamma\bm{P}^{\star})^{-1}\bm{\xi}_{k-1} \leq\left[\frac{\alpha_{k-1}}{4}+2\sqrt{\frac{6\eta_{k-1}}{(1- \gamma)^{3}}}+2\sqrt{\frac{2(1-\gamma)^{3}\min((1-\gamma)^{-1},\alpha_{k-1}^{2 })}{6500(1-\gamma)^{2}}}\alpha_{k-1}\right]\bm{1}\] \[+\left[18\left(\frac{((1-\gamma)^{3}\min((1-\gamma)^{-1},\alpha_ {k-1}^{2})}{6500(1-\gamma)^{8/3}}\right)^{3/4}\right]\bm{1}\] \[\leq[\alpha_{k-1}/4+2\sqrt{6/6500}\cdot\alpha_{k-1}+2\sqrt{2/6500 }(1-\gamma)^{1/2}\min((1-\gamma)^{-1/2},\alpha_{k-1})\alpha_{k-1}\] \[+18\cdot(10^{-3})(1-\gamma)^{1/4}\min((1-\gamma)^{-3/4},\alpha_{ k-1}^{3/2})]\bm{1}\] \[\leq[\alpha_{k-1}/4+4\sqrt{6/6500}\cdot\alpha_{k-1}+18\cdot(10^{- 3})\alpha_{k-1}]\bm{1}\leq\frac{3}{8}\alpha_{k-1}\bm{1}.\]

Consequently, \(\bm{v}^{\star}-\bm{v}_{k}\leq\alpha/2\bm{1}\). To see that \(\pi_{k}\) is also an \(\alpha_{k}\)-optimal policy, we observe that Corollary 2.5 also ensures that

\[\bm{v}_{k}\leq\mathcal{T}_{\pi_{k}}(\bm{v}_{k})\leq\mathcal{T}_{\pi_{k}}^{2}( \bm{v}_{k})\leq\cdots\leq\mathcal{T}_{\pi_{k}}^{\infty}(\bm{v}_{k})=\bm{v}^{ \pi_{k}}\leq\bm{v}^{\star}.\]

This completes the inductive step.

Consequently, for \(k=K=\lceil\log_{2}(\varepsilon^{-1}(1-\gamma)^{-1})\rceil\) iterations, \(\varepsilon\geq\alpha_{K}\geq\varepsilon/4\) and with probability \(1-\delta\), \(v_{K}\) is an \(\varepsilon\)-optimal value and \(\pi_{K}\) is an \(\varepsilon\)-optimal policy.

For runtime and sample complexity, note that the algorithm can be implemented using only \(\tilde{O}(N_{K})=\tilde{O}((1-\gamma)^{-3}\varepsilon^{-2}+(1-\gamma)^{3})\)-samples and time per state-action pair. For the space complexity, note that the algorithm can be implemented to maintain only \(O(1)\) vectors in \(\mathbb{R}^{\mathcal{A}_{\mathrm{tot}}}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, the abstract and introduction state our main results and improvements over previous work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, we discuss regimes where our result is optimal and where it may be suboptimal in the introduction. In the conclusion we also discuss directions for future work and open problems left open by our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Sections 2 and 3 of our paper give a sketch of how we obtain our main theorems, and full proofs of all intermediate results as well as the full theorems can be found in the supplemental material/appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [NA] Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper focuses on theory and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, we have read and conformed to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper focuses on foundational theory for solving MDPs and is not directly tied to any specific societal impacts (positive or negative). We do not expect any direct, immediate, substantial societal impacts. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper focuses on foundational theory and does not pose any such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use any existing code/data/model assets because we do not have any experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper did not involve crowdsourcing or research with human subjects.
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.