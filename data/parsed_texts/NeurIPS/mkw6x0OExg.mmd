# Explanations that reveal all through the

definition of encoding

Aahlad Puli1, Nhi Nguyen1, Rajesh Ranganath

New York University

Equal contribution38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a "what you see is what you get" property, which makes them transparent and simple to use. Next, we prove that existing scores (roar, fresh, eval-x) do not rank non-encoding explanations above encoding ones, and develop stripe-x which ranks them correctly. After empirically demonstrating the theoretical insights, we use stripe-x to show that despite prompting an llm to produce non-encoding explanations for a sentiment analysis task, the llm-generated explanations encode.

## 1 Introduction

Artificial intelligence can unlock information in data that was previously unknown. In medicine, for example, using AI, researchers have shown that electrocardiograms are predictive of structural heart conditions [1] or new-onset diabetes [2]. Good predictions often lead one to ask what in the input is important for a prediction; this question is a driving factor behind research in interpretability and explainability [3, 4]. One primary direction in interpretability seeks to produce explanations that are subsets of the input that retain the predictability of the label. These types of explanations and interpretations are called feature attributions and have been used to find factors associated with debt defaults [5], to demonstrate that detecting COVID-19 from chest radiographs can rely on non-physiological signals [6], and to discover a new class of antibiotics [7].

Several methods exist for producing feature attributions or explanations. While some methods compute functions of model gradients [8] or look at predictability after removing features [9], other methods attribute scores to different inputs by treating them as players in a game [4, 10] or amortize their explanations by learning a single model to select subsets for each instance [11]. Choosing one from the many feature attribution methods requires an evaluation. There are, however, many approaches to evaluation itself: qualitative ones [12, 13, 14], which are limited to cases where humans have precise knowledge about the inputs relevant to prediction, and quantitative ones [2, 4, 15, 16, 17, 18, 19, 20], which do not require human knowledge.

Intuitively, a good evaluation method for feature attributions should assign higher scores to explanations that select inputs that are more predictive of the label. However, evaluations that score explanations based on the predictability of the label from the explanation face one major challenge: _encoding_. Informally, an encoding explanation is one where the explanation predicts the label beyond what seems plausible from the values of the inputs themselves. The top left panel of Figure 1 shows an explanation that predicts the label of dog or cat depending on whether theexplanation is a pixel on the right half or left half of the image respectively. Many explanation methods fit the description of encoding [20; 21]. Further, given that many evaluations only look at the quality of prediction, encoding can go undetected, rendering the evaluations ineffective at picking explanations. In contrast, non-encoding explanations predict the label well only when the values in the explanation do, making them easy to reason about.

In addressing encoding, this work makes the following contributions:

* **Develops a simple statistical definition of encoding** via a conditional dependence property.
* Confirms the introduced definition captures all existing ad hoc encoding instances.
* Shows that **non-encoding explanations are easy to use** because they retain all the predictive inputs used to build them, meaning that predictive non-encoding explanations reveal inputs that predict the label to their users, **and thus have a "what you see is what you get" property**.
* Formalizes evaluations' sensitivity to encoding as _weak detection_ (optimal scoring explanations are non-encoding) and _strong detection_ (non-encoding explanations score above encoding ones).
* Demonstrates that the evaluations roar[19] and fresh[18] do not weakly detect encoding.
* Proves that eval-x[20] weakly detects encoding, but does not strongly detect encoding.
* Develops **stripe-x** and proves that it **strongly detects encoding**.
* Uses stripe-x to show that despite prompting an llm to produce non-encoding explanations for a sentiment analysis task, the llm-generated explanations encode.

Figure 1 provides an overview of this paper.

## 2 Evaluating explanations

We focus on explanation methods where the goal is to produce subsets of the input that predict the label [22; 23]. Explanation methods of this form, also called feature attributions, saliency methods [4; 8; 24], or just "explanations," include thresholded rankings from Shapley values [25; 26], lime[24], and real-x[20]. With \(\mathbf{y}\) as the label and \(\mathbf{x}\in\mathbf{R}^{d}\) as the inputs, let \(q(\mathbf{y},\mathbf{x})\) be the joint distribution over them. An explanation method \(e\) maps the inputs \(\mathbf{x}\) to a binary selection mask \(e(\mathbf{x})\) over the inputs: \(e:\mathbf{R}^{d}\to\{0,1\}^{d}\). The explanation \(\mathbf{x}_{e(\mathbf{x})}\) is a pair: the _selection_\(e(\mathbf{x})\) and the vector of explanation's _values_. For example, if \(\mathbf{x}=[a,b,c]\) is three-dimensional and \(e(\mathbf{x})=[0,1,1]\), \(\mathbf{x}_{e(\mathbf{x})}\) consists of the binary mask \(e(\mathbf{x})\) and the values associated with the inputs that correspond to the indices in \(e(\mathbf{x})\) with value \(1\):

\[\mathbf{x}_{e(\mathbf{x})}=(e(\mathbf{x}),[b,c]).\]

Figure 1: **Overview of the paper.** Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation’s values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called stripe-x.

We keep track of the indices because the same value can lead to different predictions depending on the index it appears at; for example, in predicting mortality from patient vital signs, a heart rate above \(110\) can occur in healthy patients but a temperature of \(110^{\circ}\)F is almost always fatal. Equivalently, like in existing work [15; 16; 17; 20], one can choose \(\mathbf{x}_{e(\mathbf{x})}\) to retain the values in the explanation in the same position and mask out those not selected: \(\mathbf{x}_{e(\mathbf{x})}=e(\mathbf{x})\times\mathbf{x}+(1-e(\mathbf{x})) \times\texttt{mask-token}\). For concision, we overload the word "explanation" to mean the explanation method instead of the random variable \(\mathbf{x}_{e(\mathbf{x})}\) when it is clear from context.

Choosing between explanation methods requires evaluation. Explanation methods seek to return inputs that predict the label, so existing evaluations consider how well the explanation \(\mathbf{x}_{e(\mathbf{x})}\) predicts the label \(\mathbf{y}\)[15; 16; 17; 20]. To score explanations based on predictive power, an evaluation method \(\alpha(\cdot)\) takes as arguments both the explanation \(e(\mathbf{x})\) and the joint distribution \(q(\mathbf{y},\mathbf{x})\): \(\alpha(q,e)\). Without loss of generality let higher be better.

Encoding: A disconnect between the predictiveness of explanations and the predictiveness of their values

We give a simple example of encoding to build intuition for the disconnect between predicting the label from the explanation and predicting the label from the explanation's values. Imagine that the goal is to explain which set of vital signs signal bacterial pneumonia as the diagnosis compared to the common cold. Consider the explanation method that selects the patient's height when the true probability of pneumonia is high given the whole set of observables (including labs, symptoms, and vital signs) and otherwise selects the patient's hair color. Physiologically, height and hair color do not indicate that the patient has pneumonia, meaning that this explanation should not be highly predictive of the label. However, by construction, pneumonia is likely exactly when the explanation selects height, and predicting the label from the explanation achieves the same accuracy as predicting with the full conditional \(\arg\max_{y\in\{\text{preumonia, cold}\}}q(\mathbf{y}=y\mid\mathbf{x})\). Thus, despite the explanation method only selecting physiologically irrelevant inputs, the explanation predicts the label well.

Encoding examples such as the one above are neither contrived nor unique. For example, Jethani et al. [20] show that certain procedures that learn to explain, when applied to MNIST digit classification, yield explanations that select a background, black pixel that predicts the label at an accuracy \(>90\%\); (see Figure 1 in [20]). Other examples of encoding explanations that predict better than what is expected from the explanation's values exist [20; 21]. Encoding explanations should not score optimally under a good evaluation because the explanation selects inputs that do not appear to predict the label. However, without a general characterization of the discrepancy in predictive power for encoding, finding explanations whose values predict well remains a challenge. The next section develops a definition of encoding.

## 3 Formalizing encoding

Intuitively, encoding is a phenomenon where the information about the label in the explanation \(\mathbf{x}_{e(\mathbf{x})}\) exceeds what is known from the _explanation's values_. As the input \(\mathbf{x}\) determines the explanation \(\mathbf{x}_{e(\mathbf{x})}\), the quality of predicting the label \(\mathbf{y}\) from the explanation relies on the information about the label transmitted from \(\mathbf{x}\) to \(\mathbf{x}_{e(\mathbf{x})}\). There are two pathways for this transmission; we elaborate below.

Denoting the values in a subset \(\mathbf{v}\) by \(\mathbf{x}_{\mathbf{v}}\), compare the event this subset takes the values \(\mathbf{a}\), i.e. \(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\) to the event that the explanation's selection is \(\mathbf{v}\) and that the explanation's values are \(\mathbf{a}\), i.e., \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\).

1. Knowing that the explanation is \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\) implies not only that the values in the explanation are determined as \(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\), but also that the selection is determined as \(e(\mathbf{x})=\mathbf{v}\).
2. In reverse, knowing that the values of a subset of inputs are \(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\) and knowing the selection \(e(\mathbf{x})=\mathbf{v}\) implies that the explanation are \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\).

Putting these two points together yields an equality between events:

\[\{\mathbf{x}:\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\}=\{\mathbf{x} :e(\mathbf{x})=\mathbf{v}\}\cap\{\mathbf{x}:\mathbf{x}_{\mathbf{v}}=\mathbf{a}\}.\] (1)

Thus, the two pathways for information between \(\mathbf{x}\) and the explanation \(\mathbf{x}_{e(\mathbf{x})}\) are the selection \(e(\mathbf{x})\) and explanation's values \(\mathbf{x}_{\mathbf{v}}\); see Figure 2. Existing work makes similar intuitive observations but stops short of formalizing the additional predictive power in an explanation \(\mathbf{x}_{e(\mathbf{x})}\)[20; 21].

[MISSING_PAGE_EMPTY:4]

Marginal encoding (mrg).This type of encoding occurs when some inputs determine which other inputs determine the label. For example, in Figure 3, the color determines whether the top right patch produces the label or the bottom right patch. Inputs that _control_ where the label comes from are named _control flow inputs_. For a real-world example, consider the following example from Jethani et al. [20], where the goal is to predict mortality for patients with chest pain. A lab value that checks for heart injury and acts like a control flow input is troponin. Abnormal troponin indicates that cardiac issues exist and cardiac imaging would inform mortality. Normal troponin on the other hand can indicate that chest pain is unrelated to cardiac health and a chest X-ray would instead inform mortality. Selecting one image or the other, but not the control flow input, conceals information about why the image was relevant to the label.

**Formalization.** In Appendix B, we provide mathematical formulations of each informal example and show that they fall under the definition of encoding in Def: Encoding: position-based encoding (Appendix B.3), prediction-based encoding (Appendix B.4), and marginal encoding (Appendix B.5). The key intuition behind all of these is that the explanation \(e(\mathbf{x})\) varies with inputs other than the selected ones, and these additional inputs provide information about the label beyond the selected ones. Next, we turn to detecting encoding via quantitative evaluations.

## 4 Detecting encoding in explanations

This section develops notions of sensitivity to encoding for evaluation methods, and uses the mathematical definition of encoding developed in the previous section to establish which methods detect encoding and which do not. Hsia et al. [21] suggest that evaluation methods like eval-x can be gamed to produce high scores for encoding explanations by optimizing the evaluation. To study this case, we introduce the notion of _weak detection_. If the optimal score of an evaluation of explanations does not permit encoding, then that evaluation is said to weakly detect encoding:

**Definition 2** (**Weak detection of encoding**).: _An evaluation \(\alpha(q,e)\) of explanations weakly detects encoding if the optimal explanations \(e^{*}\), i.e. \(\alpha(q,e^{*})=\max_{e}\alpha(q,e)\), are non-encoding._

Weak detection provides a recipe for finding non-encoding explanations: find the explanation that achieves the maximum score of a weak detector. However, such a recipe would only work when optimizing without constraints because weak detection does not require non-encoding explanations to have a better score than any encoding one. Requiring this leads to the definition of _strong detection_.

**Definition 3** (**Strong detection of encoding**).: _An evaluation \(\alpha(q,e)\) strongly detects encoding if for any encoding explanation \(e\) and non-encoding explanation \(e^{\prime}\), \(\alpha(q,e^{\prime})>\alpha(q,e)\)._

Evaluations that are not weak detectors cannot be strong detectors because they score some encoding explanation optimally.

### Do existing evaluation methods detect encoding?

Here, we consider whether several techniques for evaluating explanations: roar[19], fresh[18], and eval-x[20] can detect encoding. We analyze these evaluations on the following distribution \(q\)

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}]\sim\mathcal{B}(0.5 )^{\otimes 3},\qquad\quad\mathbf{y}=\begin{cases}\mathbf{x}_{1}&\text{w.p. }0.9&\text{else}\quad 1-\mathbf{x}_{1}&\text{if }\mathbf{x}_{3}=1,\\ \mathbf{x}_{2}&\text{w.p.}0.9&\text{else}\quad 1-\mathbf{x}_{2}&\text{if }\mathbf{x}_{3}=0.\end{cases}\] (3)

Consider the explanation \(e_{\text{encode}}(\mathbf{x})=\xi_{1}=[1,0,0]\) if \(\mathbf{x}_{3}=1\) and \(\xi_{2}=[0,1,0]\) otherwise; this encodes because \(\mathbf{x}_{3}\) is used to create the explanation and \(\mathbf{x}_{3}\) predicts the label conditional on \(\mathbf{x}_{1}\) when \(\mathbf{E}_{\xi_{1}}=1\). This is a marg explanation (see Section 3.1).

Figure 3: **Left**: Consider data where the color in the left half determines whether the label “cat”, “dog”) is produced from the top or bottom image on the right. **Right**: A marg encoding explanation that produces only the top or the bottom animal image based on the color. The animal image alone says less about the label than knowing the animal image and the color. Knowing the selection determines the color and thus provides additional information about the label.

roar and fresh do not weakly detect encoding.** roar evaluates explanations by predicting the label from the inputs not selected by the explanation, denoted as \(\mathbf{x}_{-e(\mathbf{x})}\); roar scores explanations optimally if the predictions from the remaining covariates are as random as predicting without any covariates at all. In other words, roar checks how informative \(\mathbf{x}_{-e(\mathbf{x})}\) is of \(\mathbf{y}\) and provides the highest score when \(\mathbf{y}\rotatebox[origin={c}]{$\perp$}\mathbf{x}_{-e(\mathbf{x})}\). In contrast, fresh evaluates explanations by predicting the label from the explanation after removing all other inputs, denoted as \(\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})\). For example, assume we are given an input \(\mathbf{x}=\)"Visually stunning. My favorite movie ever" and an explanation \(e(\mathbf{x})\) that selects the words "stunning" and "favorite". Then, the explanation is \(\mathbf{x}_{e(\mathbf{x})}=([0,1,0,1,0,0],["\text{stunning", "favorite"}])\), whereas \(\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})=["\text{stunning},"\text{favorite} ",\mathtt{pad-token},\mathtt{pad-token},\mathtt{pad-token}]\), which drops the information about where the selected words are in the input. See Appendix B.6 for a formal definition of \(\mathbf{val}(\mathbf{x}_{e(\mathbf{x})})\). fresh checks how predictive \(q(\mathbf{y}\mid\mathtt{val}(\mathbf{x}_{e(\mathbf{x})}))\) is and assigns an optimal score if the prediction is as good as that of \(q(\mathbf{y}\mid\mathbf{x})\). These conditions hold for \(e_{\text{encode}}(\mathbf{x})\) in eq.3:

**Proposition 1**.: _For the data generating process (dgp) in eq.3, roar and fresh assign their respective optimal scores to the encoding explanation \(e_{\text{encode}}(\mathbf{x})\)._

The proof is in Appendix B.6. The intuition is that the encoding explanation \(e_{\text{encode}}(\mathbf{x})\) always selects the input that informs the label given the control flow \(\mathbf{x}_{3}\); removing the only conditionally informative input means that \(\mathbf{x}_{-e_{\text{encode}}(\mathbf{x})}\) has no information about \(\mathbf{y}\). In turn, roar scores an encoding explanation \(\mathbf{x}_{-e_{\text{encode}}(\mathbf{x})}\) optimally, meaning it does not even weakly detect encoding. In addition, \(\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})\) provides the exact same information about the label regardless of which position it came from. As a result, \(\mathbf{x}\rotatebox[origin={c}]{$\perp$}\mathbf{y}\mid\mathtt{val}(\mathbf{x }_{e(\mathbf{x})})\), so fresh scores \(e_{\text{encode}}(\mathbf{x})\) optimally. Even though fresh attempts to drop the information about the selection \(\mathbf{v}=e(\mathbf{x})\) during evaluation, \(\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})\) remains a function of \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\), so extra information can still be transmitted through the selection \(\mathbf{v}\). Thus, roar and fresh are not weak detectors of encoding.

eval-x weakly detects encoding but not strongly. eval-x[10, 26] is an evaluation method and is sometimes called the surrogate model score. The eval-x score with log-probabilities is

\[\textsc{eval-x}(q,e):=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{ v},\mathbf{a}))}\left[\log q\left(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a} \right)\right].\] (4)

This score measures the expected log-likelihood of the labels given the input values chosen by the explanation method \(e\) and is grounded in the sampling distribution \(q\). Log-likelihoods are maximized by matching the true distribution, this leads to eval-x's weak detection:

**Theorem 1**.: _If \(e(\mathbf{x})\) is eval-x optimal, then \(e(\mathbf{x})\) is not encoding._

Appendix A.4 gives a proof. The proof shows that at optimality, the prediction from the _values_ of explanation has to match the prediction from the full inputs. In turn, given the values there is no additional information in \(\mathbf{x}\) about \(\mathbf{y}\), which means the explanation indicator \(\mathbf{E}_{\mathbf{v}}\) is independent of \(\mathbf{y}\); this violates Def: Encoding, which proves the non-encoding nature of eval-x-optimal explanations.

To test strong detection for eval-x, we consider explanations constrained to select one input. Such reductive constraints appear in practice because the goal of producing an explanation is often to aid humans who benefit from reduced complexity. Such constraints prohibit explanations from reaching eval-x's optimal score. Compare \(e_{encode}(\mathbf{x})\) with a non-encoding constant explanation:

**Proposition 2**.: _Let \(e_{\mathbf{c}}(\mathbf{x})=\xi_{3}\). Then, for the dgp in eq.3, eval-x\((q,e_{\text{encode}})>\textsc{eval-x}(q,e_{\text{c}})\)._

Thus, eval-x is not a strong detector. The intuition is that the first two coordinates \(\mathbf{x}_{1}\), \(\mathbf{x}_{2}\) predict the label when selected by \(e_{\text{encode}}\), while the control flow feature does not predict the label. eval-x not being a strong detector means that optimizing eval-x over a reductive set may yield an encoding explanation. In this case, \(e_{\text{encode}}\) is one of the eval-x-optimal reductive explanations (Lemma6).

### stripe-x: a strong detector of encoding

Encoding explanations induce the dependence between the label \(\mathbf{y}\) and the identity of the selection \(\mathbf{E}_{\mathbf{v}}=\mathbbm{1}[e(\mathbf{x})=\mathbf{v}]\) given the values in the explanation \(\mathbf{x}_{\mathbf{v}}\) (Def: Encoding). This dependence can be tested for by building on conditional independence tests [27, 28, 29]. Rather than testing, direct quantification of dependence can be useful for when combining with other scores, which can be done using instantaneous conditional mutual information:

\[\phi_{q}(e):=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x}) })}\mathbf{I}\left(\mathbf{E}_{\mathbf{v}};\mathbf{y}\mid\mathbf{x}_{\mathbf{v }}=\mathbf{a}\right)\quad(\textsc{encode-meter}).\] (5)encode-meter is \(0\) only when Def: Encoding does not hold:

**Proposition 3**.: _encode-meter \(\phi_{q}(e)=0\) if and only if \(e\) is not encoding._

The proof is in Appendix A.5. Combining eval-x with encode-meter weighed by \(\alpha\) yields a method we call the strongly information-penalized evaluator (stripe-x):

\[\textsc{stripe-x}_{\alpha}(q,e):=\textsc{eval-x}(q,e)-\alpha\phi_{q}(e).\] (6)

For a large enough \(\alpha\), the added penalty term pushes down the scores of encoding explanations below that of all non-encoding ones, meaning that stripe-x is a strong detector of encoding:

**Theorem 2**.: _With finite \(\mathbf{H}(\mathbf{y}\mid\mathbf{x})\) and \(\mathbf{H}(\mathbf{y})\), for any explanation that encodes \(e\) and any that does not encode \(e^{\prime}\), there exists an \(\alpha^{*}\) such that \(\forall\alpha>\alpha^{*}\)stripe-x\({}_{\alpha}(q,e^{\prime})\ >\textsc{stripe-x}_{\alpha}(q,e)\)._

The proof is in Appendix A.5. The intuition behind the proof is that for a large enough \(\alpha\), the stripe-x scores for any encoding explanations will be dominated by the information term, and thus will become smaller than any non-encoding explanation whose score is lower bounded by the negative marginal entropy, \(-\mathbf{H}_{q}(\mathbf{y})\). Table 1 summarizes the weak and strong detection properties of different evaluations.

**Estimating stripe-x.** The first component of stripe-x is eval-x. Computing eval-x (eq. (4)) requires an estimate of the predictive distribution of the label \(\mathbf{y}\) given \(\mathbf{x}_{\mathbf{v}}\), \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\)[20]. Estimation can be done in two ways. The first way makes use of a surrogate model trained to predict the label from different random subsets using masked tokens [9; 20]. The second way to compute eval-x (eq. (4)) relies on conditional generative models [30; 31]. Both hyperparameters and a combination of the estimators can be chosen to maximize the average log-likelihood on a held-out validation set across random input subsets.

To estimate the second part of stripe-x, the encode-meter, first expand the mutual information terms in encode-meter, \(\phi_{q}(e)\), in terms of expected \(\mathbf{KL}\):

\[\phi_{q}(e)=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{\mathbf{c} (\mathbf{c})})}\mathbb{E}_{\mathbf{y}\sim q(\mathbf{y}\mid\mathbf{x}_{\mathbf{ v}}=\mathbf{a})}\mathbf{KL}\left[q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{ \mathbf{v}}=\mathbf{a},\mathbf{y})\ \right\|\ q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}) \right].\] (7)

The outer expectation can be estimated using samples from the data and the inner expectation over \(\mathbf{y}\) can be estimated using the eval-x model \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\). The distributions over \(\mathbf{E}_{\mathbf{v}}\) can be estimated using a classifier of \(\mathbf{E}_{\mathbf{v}}\) that randomly masks the label and masks different subsets of the inputs. Further details and a generative way to estimate stripe-x are in Appendix C.1 and Appendix C.3; full algorithms are given in Appendix D.

**stripe-x in practice.** Using stripe-x to choose between explanations is straightforward: pick the one with the larger score. However, like other evaluations that use learned models, misestimation can pose a problem. With large \(\alpha\), non-encoding explanations with misestimated encode-meter will have bad stripe-x scores, while with small \(\alpha\) some encoding explanations can have good scores. Across all experiments, we set \(\alpha=20\), which yielded stripe-x scores for known encoding explanations worse than known non-encoding explanations.

## 5 Experiments

This section consists of two parts. The first part demonstrates the weak and strong detection capabilities of the evaluations roar, eval-x, and stripe-x in a simulated setting and on an image recognition task. To demonstrate these capabilities, we run these evaluations on instantiations of posi, pred, and marg. Additionally, we evaluate an existing method that learns to explain under a reductive constraint, called real-x [20]. The second part shows how stripe-x enables discovering encoding explanations in the wild, without specific knowledge of the dgp or the method that produced the explanation. We employ stripe-x to uncover encoding in explanations generated by a large language model (llm) for predicting sentiments from movie reviews.

### Empirically studying the detection of encoding in a simulated setting

We construct two examples with binary labels \(\mathbf{y}\): one discrete input \(\mathbf{x}\) and one that is a hybrid of continuous and discrete components. Both use one binary input in \(\mathbf{x}\in\{0,1\}^{5}\) as a control flow

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Weak & Strong \\ \hline roar[19] & ✗ & ✗ \\ fresh[18] & ✗ & ✗ \\ eval-x[20] & ✓ & ✗ \\ \hline stripe-x & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: The weak and strong detection properties of different evaluation methods. Existing scores like roar[19] and fresh[18], are not weak detectors, which in turn means they are not strong detectors either.

variable and switch the inputs that \(\mathbf{y}\) depends on. In both dgps, \(\mathbf{y}\) only depends on \(\mathbf{x}_{1}\) if \(\mathbf{x}_{3}=1\), and only on \(\mathbf{x}_{2}\) if \(\mathbf{x}_{3}=0\); this means that \(\mathbf{x}_{4},\mathbf{x}_{5}\) are purely noise. For both dgps, \(\mathbf{y}\) is sampled per the following distribution where \(\mathbf{x}_{3}\) determines the subset the \(\mathbf{y}\) depends on

\[q(\mathbf{y}=1\mid\mathbf{x})=1[\mathbf{x}_{3}=1]q(\mathbf{y}\mid\mathbf{x}_ {1},\mathbf{x}_{3})+1[\mathbf{x}_{3}=0]q(\mathbf{y}\mid\mathbf{x}_{2}, \mathbf{x}_{3}).\] (8)

Thus, eval-\(\mathbf{x}^{*}\) is achieved by an explanation of size \(2\): \(e(\mathbf{x})=\xi_{1}+\xi_{3}\) if \(\mathbf{x}_{3}=1\) else \(e(\mathbf{x})=\xi_{2}+\xi_{3}\). See Appendix C.4 for details; the exact dgps are given in eq. (36) and eq. (37).

**Encoding explanations.** Table 2 describes the encoding explanations we consider for this setting. In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete dgp by estimating the role of the unselected inputs in affecting the explanation and the role of \(\mathbf{E}_{\mathbf{v}}\) in predicting \(\mathbf{y}\) beyond \(\mathbf{x}_{\mathbf{v}}\); a characterization of Def: Encoding to support this check is in Lemma 1.

**ROAR and FRESH fails to weakly detect encoding.** To empirically test the analysis about roar and fresh, we study whether the two evaluations weakly detect encoding. In this study, we compare each evaluation's score on the all-inputs explanation, which is optimal, to the score assigned to marg. marg ignores \(\mathbf{x}_{3}\) which is required to produce the label \(\mathbf{y}\) in eq. (8). roar log-likelihoods for marg and the all-inputs explanation are approximately \(-\mathbf{H}(\mathbf{y})=-0.69\) for both dgps. In addition, the value of the input that marg selects alone contains all the information about the label regardless of whether marg selects \(\mathbf{x}_{1}\) or \(\mathbf{x}_{2}\). Thus, fresh log-likelihoods for marg and the all-inputs explanation are both approximately \(-0.29\) for both dgps. This result validates that roar and fresh are not weak detectors because they do not separate the optimal explanation from all encoding explanations.

**eval-x is a weak detector of encoding but not a strong detector.** eval-x log-likelihood scores are given in blue in Figures 3(a) and 3(b). eval-x, being a weak detector, scores the encoding constructions (posi, pred, and marg) strictly lower than the log-likelihood of the optimal explanation eval-x. However, the eval-x score for the marg explanation is \(-0.4\), which is above the score of \(-0.6\) achieved by a non-encoding explanation \(e(\mathbf{x})=\xi_{1}\); _thus, eval-x is not a strong detector._

**Strong detector stripe-x prices out all the encoding explanations.** Figures 3(a) and 3(b) report stripe-x scores for the same set of explanations as above; stripe-x scores are shown in red. _Strong detector stripe-x scores the non-encoding explanations above the negative entropy \(-\mathbf{H}_{q}(\mathbf{y})=-0.69\) and scores every encoding construction under that threshold._

### Detecting encoding on images of dogs and cats

The goal of this section is to study the encoding detection capabilities of roar, eval-x, and stripe-x on real data. We consider an image recognition task like the one in Figure 3 with labels and images from the cats_vs_dogs dataset from the Tensorflow package [32]. We break images of size \(64\times 64\) into \(4\) patches each of size \(32\times 32\). In left-right then top-down order, let \(\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\mathbf{x}_{4}\) be the upper left, upper right, bottom left, and bottom right patches respectively; \(\mathbf{x}_{1},\mathbf{x}_{3}\) capture color, and \(\mathbf{x}_{2},\mathbf{x}_{4}\) are the animal images. With annot(image) denoting the annota

Figure 4: eval-x and stripe-x scores of the \(3\) encoding constructions and the non-encoding constant explanation \((e(\mathbf{x})=\xi_{1})\), for both dgps. eval-x, being only a weak detector, assigns suboptimal scores to all encoding explanations (\(<\)), but scores some encoding explanations above the constant explanation. On the other hand, stripe-x, being a strong detector, pushes down the scores of all the encoding explanations below that of the non-encoding constant explanation that always selects \(\mathbf{x}_{1}\).

tion in the cats_vs_dogs dataset the image having a dog or a cat, the label is assigned as:

\[\mathbf{y}=\mathbbm{1}[\mathbf{x}_{1}=\texttt{blue}]\,\times\,\mathbbm{1}[ \texttt{annot(image x}_{2})\texttt{=dog}]+\mathbbm{1}[\texttt{x}_{1}=\texttt{ red}]\,\times\,\mathbbm{1}[\texttt{annot(image x}_{4})\texttt{=dog}]\]

We consider three encoding explanations (posi, pred, marg) and two non-encoding ones: 1) optimal, which selects the color and the patch that produces the label as dictated by the color, and 2) denoted fixed, which always outputs the bottom right patch \(\mathbf{x}_{4}\). Appendix C.6 gives details.

We report the scores assigned to each explanation by roar, eval-x, and stripe-x in Table 3. roar scores two encoding explanations pred and marg as high as the optimal explanation, meaning it is not even a weak detector. posi, pred, and marg all score worse than the optimal explanation under both the weak detector eval-x and the strong detector stripe-x. However, eval-x scores one non-encoding explanation (fixed) worse than two encoding ones, meaning it is not a strong detector. Being a strong detector, stripe-x scores the fixed explanation above the negative marginal entropy \(-\mathbf{H}_{q}(\mathbf{y})=-0.69\) and scores every encoding construction under that threshold.

**Evaluating explanations produced by real-x[20].** We ran real-x to learn explanations for the simulated setting and the image recognition task. In the simulated setting, real-x is run to select one input; Appendix C.4 gives details. In the image recognition task, real-x is run to select one of the four patches as an explanation; Appendix C.6 gives details. In both the simulated setting (see Figures 3(a) and 3(b)) and the image recognition task (see Table 3), real-x fails to achieve the optimal eval-x score while achieving a stripe-x score below the threshold of negative marginal entropy \(-\mathbf{H}_{q}(\mathbf{y})=-0.69\). Upon investigation, we found that real-x produced an explanation that matched the marg construction on at least \(80\%\) of the inputs in the simulated setting. On the image recognition task, real-x explanation matched the marg explanation on the whole dataset. In both cases, stripe-x, being a strong detector, correctly alerts that the real-x explanation encodes.

### Encoding in llm-generated explanations

One can detect encoding in any explanation by checking if the stripe-x score falls below the negative marginal entropy. Recent work uses lllms to produce explanations; e.g. [33] prompt an llm to generate explanations for reasoning tasks which are later used to improve smaller models. If the llm explanation encodes, the smaller model can falsely ignore the informative inputs the larger model's explanation depends on and yet does not reveal. In this section, we evaluate explanations generated by an llm, Llama 3, for a sentiment analysis task. We consider reviews that take one of two forms: with ADJ1 and ADJ2 as adjectives, the review is

* 'My day was <ADJ1> and the movie was <ADJ2>. that is it' or
* 'My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives'.

The second sentence in the review acts as a "control flow" input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompt Llama 3 (see Appendix C.8) to predict the sentiment and select a few words from the review that were important for that sentiment; the selected parts form the generated explanation. To discourage encoding, the prompt explicitly instructs the llm to select all the words that the llm based the selection on; such an explanation, by Lemma 1, would be non-encoding. On the \(5\) most common selections \(e(\mathbf{x})\) generated by the llm, we compute the eval-x score and the encode-meter\(\phi_{q}(e)\). The resulting stripe-x score is \(-2.78\), falling short of the negative entropy \(-\mathbf{H}_{q}(\mathbf{y})=-0.69\), meaning the llm encodes. We investigated why.

As an example, consider the review 'My day was resplendent and the movie was hollow. that is it.'; the llm selects only hollow in the explanation. However, the llm instead selects resplendent when that is it is switched to oh wait, reverse the adjectives. Such occurrences are common. On \(>70\%\) of the data, the llm selects the word that describes the movie

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & roar & fresh & eval-x & stripe-x \\ \hline opt & \(\mathbf{0.69}\) & \(\mathbf{-0.23}\) & \(\mathbf{-0.27}\) & \(\mathbf{-0.31}\) \\ fixed & \(0.59\) & \(-0.64\) & \(-0.64\) & \(-0.64\) \\ \hline posi & \(0.51\) & \(-0.69\) & \(-0.70\) & \(-5.98\) \\ pred & \(\mathbf{0.69}\) & \(\mathbf{-0.23}\) & \(-0.51\) & \(-1.40\) \\ marg & \(\mathbf{0.69}\) & \(\mathbf{-0.23}\) & \(-0.53\) & \(-1.02\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: roar, fresh, eval-x, and stripe-x scores for the image recognition experiment. Higher is better. roar and fresh score two encoding explanations pred and marg as high as the optimal explanation, meaning they are not even weak detectors. eval-x being only a weak detector scores posi, pred, and marg all worse than the optimal explanation under both eval-x but not the non-encoding constant explanation (\(e(\mathbf{x})=\xi_{4}\)), denoted fixed. stripe-x being a strong detector scores the non-encoding explanations above the negative marginal entropy \(-\mathbf{H}_{q}(\mathbf{y})=-0.69\) and scores every encoding construction under that threshold.

but does not select the second sentence in the review which controls which adjective describes the movie; this is akin to marg encoding. Thus, the llm-generated explanation encodes by looking at the control flow input in the second sentence to find the correct adjectives, but failing to select the control flow input. Such an explanation falsely indicates that only the adjectives are relevant to predicting the label. In contrast, a non-encoding explanation would, in addition to the adjective that describes the movie, reveal control flow words that indicate which adjective predicts the label.

In summary, despite being instructed to include all the words that were looked at when producing the explanation, the llm encodes. Building non-encoding explanations with llms may require an extensive search over prompts or finetuning guided by scores from stripe-x.

## 6 Discussion

When an explanation is encodes, predictions from the explanation become disconnected from predictions from the values in the explanations. Such explanations can select values with little relevance to the label and yet score highly on the many existing predictive evaluations. We develop a simple statistical definition of encoding. Inverting this definition shows that when non-encoding explanations predict the label, users know the values of those inputs selected in the explanation predict the label. We then show that existing evaluations are either non-detectors (roar[19],fresh[18]) or only weak detectors (eval-x[20]). Motivated by this, we introduce a new strong detector, stripe-x. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations, we use stripe-x to discover encoding in llm-generated explanations.

**More related work.** Other investigations into evaluating explanations focused on label leakage [26; 34] and faithfulness [18; 35; 36; 37; 38]. Label leakage is similar to encoding in that additional information is in the explanation, but focuses on explanations that have access to both the inputs and the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness, intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; a formalization does not exist. Jacovi and Goldberg [38] note the need to define faithfulness formally. Encoding explanations are not faithful to the process of making an explanation because predictive inputs outside those selected by the explanation control the explanation.

**Limitations and the future.** Using misestimated models in evaluations (like eval-x) may lead to mistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al. [26] is an example where misestimation leads to reductive explanations scoring higher than using the full input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features. One fruitful direction is to use better uncertainty estimates, like conformal inference [39] or calibration [40], or employ robustness methods [41; 42] to ameliorate errors due to misestimation. Another direction is use tricks like reinforce-style gradients to construct non-encoding explanations by optimizing stripe-x. Explanations that output subsets may not always help humans interpret the mechanism of the prediction. For example, imagine one wants to understand why a model correctly answers the question "Who won the ski halfpipe at the X-games 3 years after her debut in 2021?" with "Eileen Gu". A subset explanation may return "3 years after her debut in 2021" and "ski halfpipe", but that does not help a human interpret how the model predicts. A better interpretation would be to make the model output, "3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in 2021." Such explanations can also encode information about the prediction in the text produced as a rationale [43]. An important direction here would be to extend the definitions of weak and strong detectors of encoding to evaluations of free-text rationales.

**Data versus Model Explanations.** Even with the formal definitions of explanation methods, there is a question about what is being explained: the data or the model. These two concepts often get blended together in the literature [11; 20]. We clarify this point and abstract the choice away as two different ways to produce the joint distribution \(q(\mathbf{y},\mathbf{x})\). In _data explanation_, the distribution under which a feature attribution method seeks to output a subset of inputs that predict the label should be the population distribution of the data [23]. If, instead, the goal is _model explanation_, the goal should not be to highlight inputs that predict the label well in samples of the data; rather it should be _to predict the label well in samples from the model._ Formally, a model with parameters \(\boldsymbol{\theta}\) is a conditional distribution, \(p_{\boldsymbol{\theta}}(\mathbf{y}\mid\mathbf{x})\). To target a model explanation, a feature attribution method would aim to output a subset of inputs that predict the label under the distribution \(F(\mathbf{x})p_{\boldsymbol{\theta}}(\mathbf{y}\mid\mathbf{x})\).

## Acknowledgements

This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658 NRT-HDR:FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREER Award 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Google DeepMind, and Apple. The authors would like to thank Yoav Wald, the NeurIPS 2024 reviewers and the NeurIPS 2024 area chair for helpful feedback.

## References

* [1] Pierre Elias, Timothy J Poterucha, Vijay Rajaram, Luca Matos Moller, Victor Rodriguez, Shreyas Bhave, Rebecca T Hahn, Geoffrey Tison, Sean A Abreau, Joshua Barrios, et al. Deep learning electrocardiographic analysis for detection of left-sided valvular heart disease. _Journal of the American College of Cardiology_, 80(6):613-626, 2022.
* [2] Neil Jethani, Aahlad Puli, Hao Zhang, Leonid Garber, Lior Jankelson, Yindalon Aphinyanaphongs, and Rajesh Ranganath. New-onset diabetes assessment using artificial intelligence-enhanced electrocardiography. _arXiv preprint arXiv:2205.02900_, 2022.
* [3] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Visualising image classification models and saliency maps. _Deep Inside Convolutional Networks_, 2, 2014.
* [4] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* [5] Kim Long Tran, Hoang Anh Le, Thanh Hien Nguyen, and Duc Trung Nguyen. Explainable machine learning for financial distress prediction: evidence from vietnam. _Data_, 7(11):160, 2022.
* [6] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. _Nature Machine Intelligence_, 3(7):610-619, 2021.
* [7] Felix Wong, Erica J Zheng, Jacqueline A Valeri, Nina M Donghia, Melis N Anhatar, Satopaka Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, et al. Discovery of a structural class of antibiotics with explainable deep learning. _Nature_, pages 1-9, 2023.
* [8] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* [9] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. _Journal of Machine Learning Research_, 22(209):1-90, 2021.
* [10] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. Fast-shap: Real-time shapley value estimation. In _International Conference on Learning Representations_, 2022.
* [11] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable selection using neural networks. In _International Conference on Learning Representations_, 2018.
* [12] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J Gershman, and Finale Doshi-Velez. Human evaluation of models built for interpretability. In _Proceedings of the AAAI Conference on Human Computation and Crowdsourcing_, volume 7, pages 59-67, 2019.
* [13] Adriel Saporta, Xiaotong Gui, Ashwin Agrawal, Anuj Pareek, Steven QH Truong, Chanh DT Nguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew Y Ng, et al. Benchmarking saliency methods for chest x-ray interpretation. _Nature Machine Intelligence_, 4(10):867-878, 2022.
* [14] Jonathan Crabbe, Alicia Curth, Ioana Bica, and Mihaela van der Schaar. Benchmarking heterogeneous treatment effect models through the lens of interpretability. _Advances in Neural Information Processing Systems_, 35:12295-12309, 2022.
* [15] Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Muller. Evaluating the visualization of what a deep neural network has learned. _IEEE transactions on neural networks and learning systems_, 28(11):2660-2673, 2016.

* Petsiuk et al. [2018] V Petsiuk, A Das, and K Saenko. Rise: Randomized input sampling for explanation of black-box models. _arXiv preprint arXiv:1806.07421_, 2018.
* Dabkowski and Gal [2017] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. _Advances in neural information processing systems_, 30, 2017.
* Jain et al. [2020] Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C Wallace. Learning to faithfully rationalize by construction. _arXiv preprint arXiv:2005.00115_, 2020.
* Hooker et al. [2019] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability methods in deep neural networks. _Advances in neural information processing systems_, 32, 2019.
* Jethani et al. [2021] Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have we learned to explain?: How interpretability methods can learn to encode predictions in their interpretations. In _International Conference on Artificial Intelligence and Statistics_, pages 1459-1467. PMLR, 2021.
* Hsia et al. [2023] Jennifer Hsia, Danish Pruthi, Aarti Singh, and Zachary C Lipton. Goodhart's law applies to nlp's explanation benchmarks. _arXiv preprint arXiv:2308.14272_, 2023.
* Guyon and Elisseeff [2003] Isabelle Guyon and Andre Elisseeff. An introduction to variable and feature selection. _Journal of machine learning research_, 3(Mar):1157-1182, 2003.
* Chen et al. [2018] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In _International conference on machine learning_, pages 883-892. PMLR, 2018.
* Ribeiro et al. [2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.
* Strumbelj and Kononenko [2014] Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. _Knowledge and information systems_, 41:647-665, 2014.
* Jethani et al. [2023] Neil Jethani, Adriel Saporta, and Rajesh Ranganath. Don't be fooled: label leakage in explanation methods and the importance of their quantitative evaluation. In _International Conference on Artificial Intelligence and Statistics_, pages 8925-8953. PMLR, 2023.
* Zhang et al. [2011] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Kernel-based conditional independence test and application in causal discovery. In _Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence_, pages 804-813, 2011.
* Sudarshan et al. [2020] Mukund Sudarshan, Wesley Tansey, and Rajesh Ranganath. Deep direct likelihood knockoffs. _Advances in neural information processing systems_, 33:5036-5046, 2020.
* Sudarshan et al. [2023] Mukund Sudarshan, Aahlad Puli, Wesley Tansey, and Rajesh Ranganath. Diet: Conditional independence testing with marginal dependence measures of residual information. In _International Conference on Artificial Intelligence and Statistics_, pages 10343-10367. PMLR, 2023.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. _URL https://arxiv. org/abs/2204.06125_, 7, 2022.
* Abadi et al. [2015] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org.

* [33] Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. _arXiv preprint arXiv:2210.06726_, 2022.
* [34] Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? _arXiv preprint arXiv:2010.04119_, 2020.
* [35] Jasmin Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. " will you find these shortcuts?" a protocol for evaluating the faithfulness of input salience methods for text classification. _arXiv preprint arXiv:2111.07367_, 2021.
* [36] Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctly attribute features? In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9623-9633, 2022.
* [37] Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, and Jun Zhao. Logic traps in evaluating attribution scores. _arXiv preprint arXiv:2109.05463_, 2021.
* [38] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? _arXiv preprint arXiv:2004.03685_, 2020.
* [39] Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. _Journal of the American Statistical Association_, 113(523):1094-1111, 2018.
* [40] Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, and Rajesh Ranganath. X-cal: Explicit calibration for survival analysis. _Advances in neural information processing systems_, 33:18296-18307, 2020.
* [41] Aahlad Manas Puli, Lily H Zhang, Eric Karl Oermann, and Rajesh Ranganath. Out-of-distribution generalization in the presence of nuisance-induced spurious correlations. _ICLR 2022_, 2021.
* [42] Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don't blame dataset shift! shortcut learning due to gradients and cross entropy. _Advances in Neural Information Processing Systems_, 36, 2023.
* [43] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. _arXiv preprint arXiv:2305.18029_, 2023.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
* **Experimental Result Reproducibility*
* Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Guidelines:
* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We only use public data. We use standard existing training techniques, describe the hyperparameters in detail, and provided the Llama 3 prompts we used in our experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The error bars in 5.1 and 5.2 are negligible because we estimate means of metrics over datasets of \(5000\) samples. For 5.3, error bars are irrelevant because there is no comparison against an existing method. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not use human subjects and only use public data and public models. Our work focuses on explainable machine learning and does not pose additional ethical harms beyond what is standard for the field. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We investigate a problem with explanations and fix it. There is no societal impact of our work that exceed that of the usage of explanations. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release any data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

[MISSING_PAGE_FAIL:20]

By definition, the complement \(\mathbf{A}^{C}\) is such that

\[\forall(\mathbf{v},\mathbf{a})\in\mathbf{A}^{C},\quad\mathbf{y}\mathrel{ \hbox to 0.0pt{\lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$\searrow$}} \mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}.\]

Such a set cannot have positive measure when Def: Encoding is violated which means

\[q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A}^{C})=0.\]

In turn,

\[q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1-q(\mathbf{x}_{e(\mathbf{x})}\in \mathbf{A}^{C})=1.\]

Thus, \(\mathbf{A}\) is such that \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1\), and by eq.12 for all \((\mathbf{v},\mathbf{a})\in\mathbf{A}\)

\[\mathbf{y}\mathrel{\hbox to 0.0pt{\lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$ \searrow$}}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a},\]

which in turn guarantees

\[q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a}))=q( \mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a},\mathbf{E}_{\mathbf{v}}=1)=q (\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}).\]

### Helpful Lemmas and their proofs

#### a.3.1 Alternate conditions equivalent to Def: Encoding

The dependence in Def: Encoding occurs due to two reasons, understanding which sheds more light on the definition. First, for some selection \(e(\mathbf{x})=\mathbf{v}\), the explanation's values \(\mathbf{x}_{\mathbf{v}}\) do not provide enough information to reveal that the explanation should select the inputs denoted by \(\mathbf{v}\). In other words, the indicator of the selection is variable even after fixing the explanation's values themselves. Second, this indicator is predictive of the label for the data with the explanation \(\mathbf{v}\). These two properties provide intuition on the definition of encoding:

**Lemma 1**.: _Def: Encoding holds for an explanation \(e(\mathbf{x})\) if and only if there exists a selection \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0\) and a set \(\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\) such that \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\) where both of the following conditions hold for almost every \(\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\):_

\[\begin{array}{ll}\text{Unpredictability of Explanation}&q(\mathbf{E}_{\mathbf{v}}=1 \mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})\neq 1;\\ \text{Additional Information from Explanation}&q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}, \mathbf{E}_{\mathbf{v}}=1)\neq q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a},\mathbf{E}_{\mathbf{v}}=0).\end{array}\]

Proof.: First, Lemma2 shows the Def: Encoding holds if only if there exists a selection \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0\) and a set \(\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\) such that \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\) where

\[\forall\mathbf{a}\in\mathbf{S}_{\mathbf{v}},\quad\mathbf{y}\mathrel{\hbox to 0.0pt{ \lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$\searrow$}} \mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}.\]

We use this alternate definition in what follows.

Given a non-measure zero set \(\mathbf{S}_{\mathbf{v}}\), by Lemma3, almost everywhere in \(\mathbf{S}_{\mathbf{v}}\) it holds that \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})>0\).

Conditional dependence implies Unpredictability and Additional information (the only if part).If \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\) almost everywhere (under \(q(\mathbf{x})\)), then \(\mathbf{E}_{\mathbf{v}}\) is constant given \(\mathbf{x}_{\mathbf{v}}\), and therefore independent of any variable given \(\mathbf{x}_{\mathbf{v}}\):

\[q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\implies\mathbf{y} \mathrel{\hbox to 0.0pt{\lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$\searrow$}} \mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}.\]

Then, it follows that conditional dependence implies the unpredictability property

\[\mathbf{y}\mathrel{\hbox to 0.0pt{\lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$\searrow$}} \mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}\implies q(\mathbf{E}_{ \mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})<1.\]

Second, with the result from Lemma3, we have \(q(\mathbf{E}_{\mathbf{v}}=1|\mathbf{x}_{\mathbf{v}})\in(0,1)\). Thus \(q(\mathbf{y}|\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\) and \(q(\mathbf{y}|\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\) exist almost every where in \(\mathbf{S}_{\mathbf{v}}\). Then, by definition of conditional dependence, there is additional information about the label in the explanation:

\[\mathbf{y}\mathrel{\hbox to 0.0pt{\lower 4.0pt\hbox{ $\sim$}}\raise 1.0pt\hbox{$\searrow$}} \mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}\implies q(\mathbf{y}\mid \mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\neq q(\mathbf{y}\mid \mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0).\]

This shows that Def: Encoding implies the additional information property.

**Conditional dependence implied by Unpredictability and Additional information (the if part).** Now, if \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})\in(0,1)\), then the following two conditional distributions exist almost everywhere in \(\mathbf{S}_{\mathbf{v}}\)

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\qquad, \qquad q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0).\]

Then, by definition of dependence almost everywhere \(\mathbf{S}_{\mathbf{v}}\):

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\neq q( \mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\Longrightarrow \mathbf{y}\mbox{$\perp\!\!\!\!\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{ \mathbf{v}}.\]

Thus, the unpredictability and the additional information properties imply Def: Encoding.

\(\Box\)

**Lemma 2**.: _Def: Encoding holds for an explanation \(e(\mathbf{x})\) if and only if there exists a selection \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0\) and a set \(\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\) such that \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\) where_

\[\forall\mathbf{a}\in\mathbf{S}_{\mathbf{v}},\quad\mathbf{y}\mbox{$\perp\!\! \!\!\perp$},\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}.\] (13)

Proof.: Def: Encoding says that the explanation \(e(\mathbf{x})\) is encoding if there exists an \(\mathbf{S}\) where \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S})>0\) such that for every \((\mathbf{v},\mathbf{a})\in\mathbf{S}\) eq. (13) holds. This proof works by showing that \(\mathbf{S}\) having a positive measure implies the existence of \(\mathbf{v}\) and \(\mathbf{S}_{\mathbf{v}}\) as in Lemma 2 such that eq. (13) holds.

Decompose \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S})\) by introducing an expectation over \(\mathbf{v}\sim q(e(\mathbf{x}))\),

\[q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S})=\mathbb{E}_{\mathbf{v}\sim q(e( \mathbf{x}))}q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S}\mid e(\mathbf{x})= \mathbf{v}).\]

As there are only finitely many \(\mathbf{v}\),

\[q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S})>0\quad\Longleftrightarrow\quad \exists\mathbf{v}\quad\mbox{s.t.}\quad q(e(\mathbf{x})=\mathbf{v})>0\quad \mbox{and}\quad q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S}\mid e(\mathbf{x})= \mathbf{v})>0.\]

The "only if" direction.Pick any \(\mathbf{v}\) such that the RHS above holds and define \(\mathbf{S}_{\mathbf{v}}=\{\mathbf{a}:(\mathbf{v},\mathbf{a})\in\mathbf{S}\}\). By definition,

\[\mathbf{S}_{\mathbf{v}}=\{\mathbf{x}_{\mathbf{v}}:(\mathbf{v},\mathbf{x}_{ \mathbf{v}})\in\mathbf{S}\}\cap\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}.\]

This proves that \(\mathbf{S}_{\mathbf{v}}\) has positive measure:

\[q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})=q(\mathbf{x}_{e( \mathbf{x})}\in\mathbf{S},e(\mathbf{x})=\mathbf{v})=q(\mathbf{x}_{e(\mathbf{ x})}\in\mathbf{S}\mid e(\mathbf{x})=\mathbf{v})*q(e(\mathbf{x})=\mathbf{v})>0.\]

Finally, as \(\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\implies(\mathbf{v},\mathbf{a})\in \mathbf{S}\), eq. (13) holds:

\[\forall\mathbf{a}\in\mathbf{S}_{\mathbf{v}},\quad\mathbf{y}\mbox{$\perp\!\! \!\!\perp$},\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}.\]

This completes the "only if" direction.

The "if" direction.Assume that there exists \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0\) and \(\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\) such that \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\) where

\[\forall\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\qquad\mathbf{y}\mbox{$\perp\!\! \!\!\perp$},\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}.\]

Define \(\mathbf{S}=\{(\mathbf{v},\mathbf{a}):\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\}\). By this construction, \(\mathbf{S}\) has positive measure:

\[q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{S}) =q((\mathbf{v},\mathbf{x}_{\mathbf{v}})\in\mathbf{S})\] \[=q(e(\mathbf{x})=\mathbf{v})q((\mathbf{v},\mathbf{x}_{\mathbf{v} })\in\mathbf{S}\mid e(\mathbf{x})=\mathbf{v})\] \[=q(e(\mathbf{x})=\mathbf{v})q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S }_{\mathbf{v}}\mid e(\mathbf{x})=\mathbf{v})\] \[=q(e(\mathbf{x})=\mathbf{v})q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S }_{\mathbf{v}})\qquad\{\mbox{ as }\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})= \mathbf{v}\}\] \[>0,\]

where the last inequality holds because by assumption

\[q(e(\mathbf{x})=\mathbf{v})>0\qquad q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{ \mathbf{v}})>0.\]

Finally, as \((\mathbf{v},\mathbf{a})\in\mathbf{S}\implies\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\), eq. (13) holds:

\[\forall(\mathbf{v},\mathbf{a})\in\mathbf{S},\quad\mathbf{y}\mbox{$\perp\!\!\! \perp$},\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}.\]

This completes the "if" directions and with that the proof.

**Lemma 3**.: _For any set \(\mathbf{S}_{\mathbf{v}}\subseteq\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\) such that \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\), then for almost every \(\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\), \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})>0\)._

Proof.: Define the set \(\mathbf{A}_{\mathbf{v}}=\{\mathbf{a}:q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x} _{\mathbf{v}}=\mathbf{a})=0\}\). Next compute the joint probability

\[q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}}\cap\mathbf{S}_{\mathbf{v}}) =q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}})q(\mathbf{x}_{\mathbf{v} }\in\mathbf{S}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{ \mathbf{v}}).\]

Now, noting that \(\mathbf{S}_{\mathbf{v}}\) is a subset of \(\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\), which is equivalent to \(\{\mathbf{x}_{\mathbf{v}}:\mathbf{E}_{\mathbf{v}}=1\}\), thus

\[q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}}\mid\mathbf{x }_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}})\] \[=\int q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}}\mid \mathbf{x}_{\mathbf{v}}=\mathbf{a},\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{ \mathbf{v}})q(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\mid\mathbf{x}_{\mathbf{v}}\in \mathbf{A}_{\mathbf{v}})d\mathbf{a}\] \[=\int q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}}\mid \mathbf{x}_{\mathbf{v}}=\mathbf{a})q(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\mid \mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}})d\mathbf{a}\] \[\leq\int q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a})q(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\mid\mathbf{x}_{\mathbf{v}}\in \mathbf{A}_{\mathbf{v}})d\mathbf{a}\] \[=\int q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a})q(\mathbf{x}_{\mathbf{v}}=\mathbf{a}\mid\mathbf{x}_{\mathbf{v}}\in \{\mathbf{a}:q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a })=0\})d\mathbf{a}\] \[=0.\]

The probability \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}} \in\mathbf{A}_{\mathbf{v}})\) is non-negative, so it must be zero. Plugging this conditional back into the joint gives \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}}\cap\mathbf{S}_{\mathbf{v}} )=0\). Then expanding yields

\[0=q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}}\cap\mathbf{S}_{\mathbf{v} })=q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{ v}}\in\mathbf{S}_{\mathbf{v}})q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}}).\]

Since \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{S}_{\mathbf{v}})>0\), \(q(\mathbf{x}_{\mathbf{v}}\in\mathbf{A}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v} }\in\mathbf{S}_{\mathbf{v}})\) must be zero and thus, \(q(\mathbf{x}_{\mathbf{v}}\notin\mathbf{A}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{ v}}\in\mathbf{S}_{\mathbf{v}})=1\), where expanding out the definition of \(\mathbf{A}_{\mathbf{v}}\) gives the desired result that \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})>0\) for almost \(\mathbf{a}\in\mathbf{S}_{\mathbf{v}}\):

\[1 =q(\mathbf{a}\notin\mathbf{A}_{\mathbf{v}}\mid\mathbf{a}\in \mathbf{S}_{\mathbf{v}})\] \[=q(\mathbf{a}\notin\{\mathbf{a}:q(\mathbf{E}_{\mathbf{v}}=1\mid \mathbf{x}_{\mathbf{v}}=\mathbf{a})=0\}\mid\mathbf{a}\in\mathbf{S}_{\mathbf{v}})\] \[=q(\mathbf{a}\in\{\mathbf{a}:q(\mathbf{E}_{\mathbf{v}}=1\mid \mathbf{x}_{\mathbf{v}}=\mathbf{a})>0\}\mid\mathbf{a}\in\mathbf{S}_{\mathbf{v}}).\]

#### a.3.2 Optimal value and the optimal gap under eval-x

**Lemma 4**.: _The eval-x optimality gap value for \(e(\cdot)\) is an averaged \(\mathbf{KL}\) between \(q(\mathbf{y}\mid\mathbf{x})\) and \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\): \(\sum_{\mathbf{v}\in\mathcal{V}}q(e(\mathbf{x})=\mathbf{v})\mathbb{E}_{q( \mathbf{x}\mid e(\mathbf{x})=\mathbf{v})}\mathbf{KL}(q(\mathbf{y}\mid\mathbf{x })\|q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}))\). This gap is zero, i.e. \(e(\mathbf{x})\) is optimal with the score eval-x\({}^{*}=\mathbb{E}_{q}[\log q(\mathbf{y}\mid\mathbf{x})]\) if for all \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0,\)_

\[q(\mathbf{y}\mid\mathbf{x})=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad\text{ a.e. in }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}.\]

Proof.: Let \(p\) be a generic conditional distribution and let \(\mathbf{x}_{-\mathbf{v}}\) be the values outside the explanation.

\[\max_{e}\text{eval-x}(q,e) =\max_{e}\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v}, \mathbf{a}))}\log q\left(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}\right)\] \[=\max_{e}\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v}, \mathbf{a}))}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v}, \mathbf{a}),\mathbf{x}_{-\mathbf{v}})}\log q\left(\mathbf{y}\mid\mathbf{x}_{ \mathbf{v}}=\mathbf{a}\right)\] \[\leq\max_{p}\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e (\mathbf{x})})}\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v}, \mathbf{a}))}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x})}[\log p(\mathbf{y}\mid \mathbf{x})]\] \[\leq\max_{p}\mathbb{E}_{q(\mathbf{x})}\mathbb{E}_{q(\mathbf{y}\mid \mathbf{x})}[\log p(\mathbf{y}\mid\mathbf{x})]\] \[=\max_{p}-\mathbb{E}_{q(\mathbf{x})}\mathbf{KL}(q(\mathbf{y}\mid \mathbf{x})\|p(\mathbf{y}\mid\mathbf{x}))+\mathbb{E}_{q}[\log q(\mathbf{y} \mid\mathbf{x})]\] \[=\mathbb{E}_{q}[\log q(\mathbf{y}\mid\mathbf{x})].\]

This upper bound is achievable by an explanation that selects all inputs, so the maximum eval-x denoted as eval-x\({}^{*}=\mathbb{E}_{q}\log q(\mathbf{y}\mid\mathbf{x})\).

As in the math above, the eval-x score for an explanation method can be expanded as

eval-x\[{}^{e} =\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x}) })}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a }))}\log q\left(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}\right)\] \[=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{ x})})}\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a }))}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x})}\log q(\mathbf{y}\mid\mathbf{x}_{ \mathbf{v}}=\mathbf{a})\] \[=\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))}\mathbb{E}_{\mathbf{ a}\sim q(\mathbf{x}_{e}\mid e(\mathbf{x})=\mathbf{v})}\mathbb{E}_{q(\mathbf{x} \mid\mathbf{x}_{e(\mathbf{x})}=\mathbf{v})}\mathbb{E}_{q(\mathbf{y}\mid \mathbf{x}_{\mathbf{v}})}\log q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a})\] \[=\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))}\mathbb{E}_{q( \mathbf{x}\mid e(\mathbf{x})=\mathbf{v}))}\mathbb{E}_{q(\mathbf{y}\mid \mathbf{x})}\log q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}),\]

where in the last step, we dropped a because it equals \(\mathbf{x}_{\mathbf{v}}\) almost surely. Similarly, the optimal score eval-x\({}^{*}\) expands to

\[\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))}\mathbb{E}_{q(\mathbf{x}\mid e( \mathbf{x})=\mathbf{v}))}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x})}\log q( \mathbf{y}\mid\mathbf{x}).\]

Let \(\mathcal{V}\) be the set of values that explanations can take on, then taking the difference from optimality

\[\text{eval-x}^{*}-\text{eval-x}^{e}\] \[=\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))}\mathbb{E}_{q( \mathbf{x}\mid e(\mathbf{x})=\mathbf{v}))}\mathbb{E}_{q(\mathbf{y}\mid \mathbf{x})}\log\frac{q(\mathbf{y}\mid\mathbf{x})}{q(\mathbf{y}\mid\mathbf{x} _{\mathbf{v}}=\mathbf{x})}\] \[=\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))}\mathbb{E}_{q( \mathbf{x}\mid e(\mathbf{x})=\mathbf{v}))}\mathbb{KL}\left[q(\mathbf{y}\mid \mathbf{x})\parallel q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\right]\] \[=\sum_{\mathbf{v}\in\mathcal{V}}q(e(\mathbf{x})=\mathbf{v}) \mathbb{E}_{q(\mathbf{x}\mid e(\mathbf{x})=\mathbf{v})}\mathbb{KL}\left[q( \mathbf{y}\mid\mathbf{x})\parallel q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}) \right].\]

As each \(\mathbf{KL}\) term is non-negative, each term in the sum being set to \(0\) simultaneously achieves the optimum, which happens when for all \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0,\)

\[q(\mathbf{y}\mid\mathbf{x})=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad \text{ for almost every }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}.\]

In Appendix A.4, we use the results from Lemma1 and Lemma4 to prove that the optimal score of eval-x can only be achieved by non-encoding explanations.

### Proof of Theorem1

**Theorem 1**.: _If \(e(\mathbf{x})\) is eval-x optimal, then \(e(\mathbf{x})\) is not encoding._

Proof.: Note only \(q(e(\mathbf{x})=\mathbf{v})>0\) are of interest, since \(q(e(\mathbf{x})=\mathbf{v})=0\) implies that \(\mathbf{E}_{\mathbf{v}}=0\) almost surely and thus \(\mathbf{y}\_\|\mathbb{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}\).

Then if \(e(\mathbf{x})\) achieves eval-x\({}^{*}\), then by Lemma4, for all \(\mathbf{v}\) such that \(q(e(\mathbf{x})=\mathbf{v})>0,\)

\[q(\mathbf{y}\mid\mathbf{x})=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad \text{ for almost every }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}.\]

First, this optimality criteria can incorporate \(\mathbf{E}_{\mathbf{v}}=1\) on the lefthand side by first conditioning on \(e(\mathbf{x})\) and then noting that the equality holds for \(\mathbf{x}\) where \(e(\mathbf{x})=\mathbf{v}\).

\[q(\mathbf{y}\mid\mathbf{x}) =q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}\] \[\iff q(\mathbf{y}\mid\mathbf{x},e(\mathbf{x})) =q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}\] \[\iff q(\mathbf{y}\mid\mathbf{x},e(\mathbf{x})=\mathbf{v}) =q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\quad\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}.\]

To understand if the optimality criterion disallows encoding, integrate the left and right-hand sides of this optimality criterion with the respect to complement of the inputs in \(\mathbf{x}_{\mathbf{v}}\), \(q(\mathbf{x}_{\mathbf{v}}^{c}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v }}=1)\) yields

\[\int q(\mathbf{y}\mid\mathbf{x},\mathbf{E}_{\mathbf{v}}=1)q( \mathbf{x}_{\mathbf{v}}^{c}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v }}=1)d\mathbf{x}_{\mathbf{v}}^{c}=\int q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})q (\mathbf{x}_{\mathbf{v}}^{c}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1 )d\mathbf{x}_{\mathbf{v}}^{c}\] \[\text{ for almost every }\{\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}}\] \[\iff\int q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}^{c},\mathbf{x}_{ \mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)q(\mathbf{x}_{\mathbf{v}}^{c}\mid \mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)d\mathbf{x}_{\mathbf{v}}^{c}=q (\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\int q(\mathbf{x}_{\mathbf{v}}^{c}\mid \mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)d\mathbf{x}_{\mathbf{v}}^{c}\] \[\text{ for almost every }\{\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}}\] \[\iff q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v }}=1)=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\{\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}}.\]Now expanding the right-hand side gives

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})=q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=1 \mid\mathbf{x}_{\mathbf{v}})+q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=0\mid\mathbf{ x}_{\mathbf{v}}).\]

Combing the two equations gives

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)=q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})+q(\mathbf{y},\mathbf{E} _{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\{\mathbf{x}_{ \mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}.\] (14)

We show that this equality implies that \(\mathbf{y}\rotatebox[origin={c}]{$\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_ {\mathbf{v}}\) by splitting the analysis into cases based on \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\) and \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})<1\). In turn, the condition in Def: Encoding is violated and the explanation \(e(\cdot)\) is not encoding.

Case 1: eval-x optimality holds when the explanation is predictable.The first case to consider is when the event that the explanation takes the value \(\mathbf{v}\) is determined by \(\mathbf{x}_{\mathbf{v}}\) for all samples with the explanation \(\mathbf{v}\). That is, \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\):

\[q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\iff q(\mathbf{E}_{ \mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})=0.\]

Then expanding this marginal into the joint shows that the joint \(q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})\) has to be zero as well.

\[q(\mathbf{E}_{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})=\int q(\mathbf{y}, \mathbf{E}_{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})d\mathbf{y}=0,\]

because an integral of non-negative terms being zero implies that each term itself is zero almost surely.

Then, we can show that the determinism condition \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\) is sufficient for the optimality criterion eq. (14):

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1) =q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}= 1)\times 1\] \[=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}= 1)q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})\] \[=q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v }})\] \[=q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})+q (\mathbf{y},\mathbf{E}_{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})\quad\text{ for almost every }\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}.\]

This shows that the eval-x optimality criteria is satisfied when the \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\), thus the explanation is completely predictable from the explanation for examples with that explanation. By Lemma 1, we have

\[q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})=1\implies\mathbf{y} \rotatebox[origin={c}]{$\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{ \mathbf{v}},\]

which violates Def: Encoding. So there is no encoding in this case.

Case 2: When the explanation is unpredictable, eval-x optimality requires that the explanation provide no extra information.Now consider the alternative case, \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})<1\). Here the explanation does not determine the explanation opening the possibility that the eval-x-optimal explanation method can encode information in the explanation.

Because \(q(e(\mathbf{x})=\mathbf{v})>0\), we have \(q(\mathbf{x}_{\mathbf{v}}\in\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v }\})>0\). Thus, by Lemma 3, for almost every \(\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\) it holds that \(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})>0\). Putting this result together with alternative case (\(q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})<1\)) gives: \(0<q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})<1\) for almost every \(\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\).

Now, expanding out the optimality criterion eq. (14):

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1 )=q(\mathbf{y},\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})\times 1+q( \mathbf{y},\mathbf{E}_{\mathbf{v}}=0\mid\mathbf{x}_{\mathbf{v}})\times 1\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{ for almost every }\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\] \[\iff q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}= 1)=q(\mathbf{y}\mid\mathbf{E}_{\mathbf{v}}=1,\mathbf{x}_{\mathbf{v}})q( \mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+q(\mathbf{y} \mid\mathbf{E}_{\mathbf{v}}=0,\mathbf{x}_{\mathbf{v}})(1-q(\mathbf{E}_{ \mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\text{ for almost every }\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\] \[\iff(1-q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}))q( \mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)=(1-q( \mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}))q(\mathbf{y}\mid \mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\text{ for almost every }\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}\] \[\iff q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}= 1)=q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\quad\text{ for almost every }\{\mathbf{x}_{\mathbf{v}}:e(\mathbf{x})=\mathbf{v}\}.\]

This equality says for all samples with the explanation \(\mathbf{v}\), knowing \(\mathbf{E}_{\mathbf{v}}\) does not change the distribution of the label \(\mathbf{y}\). By Lemma 1, this equality implies that the independence \(\mathbf{y}\rotatebox[origin={c}]{$\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_ {\mathbf{v}}\) holds which violates Def: Encoding.

### Proof of Proposition 3 and Theorem 2

**Proposition 3**.: encode-meter \(\phi_{q}(e)=0\) _if and only if \(e\) is not encoding._

Proof.: First, Def: Encoding is violated if and only if there exists a set \(\mathbf{A}\) such that \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1\) and

\[\forall(\mathbf{v},\mathbf{a})\in\mathbf{A}\qquad\mathbf{y}\rotatebox[origin= ]{-0.0}{$\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{ a}.\]

For the if direction, note that if encode-meter \(\phi_{q}(e)=0\),

\[\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x})})}\mathbf{ I}(\mathbf{y};\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})=0.\]

To show the forward direction, the above equality means that if \(\phi_{q}(e)=0\), almost surely for every \((\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x})})\), the instantaneous mutual information is \(0\) which implies the desired conditional independence

\[\mathbf{I}(\mathbf{y};\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a})=0\implies\mathbf{y}\rotatebox[origin= ]{-0.0}{$\perp$}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{ a}.\]

By definition of almost surely, there exists a set \(\mathbf{A}\) such that \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1\) the independence above holds; this completes the "if" direction.

To show the reverse direction, let there exist a set \(\mathbf{A}\) such that \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1\), for every \((\mathbf{v},\mathbf{a})\in\mathbf{A}\),

\[\mathbf{y}\rotatebox[origin=]{-0.0}{$\perp$}\mathbf{E}_{\mathbf{v}}\mid \mathbf{x}_{\mathbf{v}}.\]

In turn, for all \((\mathbf{v},\mathbf{a})\in\mathbf{A}\),

\[\mathbf{I}(\mathbf{y};\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}= \mathbf{a})=0.\]

Then, the fact that \(q(\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})=1\) implies that expectations with respect to \(q(\mathbf{x}_{e(\mathbf{x})})\) over the whole support equal expectations over \(q(\mathbf{x}_{e(\mathbf{x})}\mid\mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})\), which is \(q(\mathbf{x}_{e(\mathbf{x})})\) restricted to \(\mathbf{A}\):

\[\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x})})}\mathbf{ I}(\mathbf{y};\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})= \mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{x})}\mid \mathbf{x}_{e(\mathbf{x})}\in\mathbf{A})}\mathbf{I}(\mathbf{y};\mathbf{E}_{ \mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a})=0.\]

This completes the "only if" direction. 

**Theorem 2**.: _With finite \(\mathbf{H}(\mathbf{y}\mid\mathbf{x})\) and \(\mathbf{H}(\mathbf{y})\), for any explanation that encodes \(e\) and any that does not encode \(e^{\prime}\), there exists an \(\alpha^{*}\) such that \(\forall\alpha>\alpha^{*}\)stripe-\(\mathsf{x}_{\alpha}(q,e^{\prime})\)\(>\)stripe-\(\mathsf{x}_{\alpha}(q,e)\)._

Proof.: Recall that stripe-\(\mathsf{x}\) is

\[\textsc{stripe-}\mathsf{x}_{\alpha}(q,e):=\mathbb{E}_{(\mathbf{v},\mathbf{a}) \sim q(\mathbf{x}_{e(\mathbf{x})})}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e (\mathbf{x})}=(\mathbf{v},\mathbf{a}))}[\log q\left(\mathbf{y}\mid\mathbf{x}_ {\mathbf{v}}=\mathbf{a}\right)]-\alpha\phi_{q}(e),\]

where the encode-meter

\[\phi_{q}(e):=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e( \mathbf{x})})}\mathbf{I}\left(\mathbf{E}_{\mathbf{v}};\mathbf{y}\mid\mathbf{x }_{\mathbf{v}}=\mathbf{a}\right).\]

We first show bounds for the first term in stripe-\(\mathsf{x}\) and then derive the stripe-\(\mathsf{x}\) scores for encoding and non-encoding explanations.

Bounds on eval-\(\mathsf{x}\) scores.We lower bound the eval-\(\mathsf{x}\) score, which is the first term in stripe-\(\mathsf{x}\), for non-encoding explanations.

For non-encoding explanations, almost surely over \(\mathbf{v},\mathbf{a}\sim q(\mathbf{x}_{e(\mathbf{x})})\)

\[q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a}))=q( \mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a}).\]

Then,

\[\textsc{eval-}\mathsf{x}(q,e)-\mathbb{E}_{q(\mathbf{y},\mathbf{x }_{\mathbf{v}}=\mathbf{a})}\log q(\mathbf{y})\] \[=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{e(\mathbf{ x})})}\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v}, \mathbf{a}))}\log q\left(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{a} \right)-\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{e(\mathbf{x})})}\log q(\mathbf{y})\] \[=\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{e(\mathbf{x})})}[\log q \left(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}\right)]-\mathbb{E}_{q( \mathbf{y},\mathbf{x}_{e(\mathbf{x})})}\log q(\mathbf{y})\] \[=\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{e(\mathbf{x})})}\log\frac{q \left(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})}\right)}{q(\mathbf{y})}\] \[=\mathbf{I}(\mathbf{y};\mathbf{x}_{e(\mathbf{x})}).\]The above inequality implies that

\[\textsc{eval-x}(q,e)-\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q( \mathbf{x}_{e(\mathbf{x})})}\mathbb{E}_{\mathbf{y}\sim q(\mathbf{y}\mid\mathbf{ x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a}))}\log q(\mathbf{y})=\mathbf{I}(\mathbf{y}; \mathbf{x}_{e(\mathbf{x})})\geq 0\] \[\implies\textsc{eval-x}(q,e)+\mathbf{H}_{q}(\mathbf{y})\geq 0\] \[\implies\textsc{eval-x}(q,e)\geq-\mathbf{H}_{q}(\mathbf{y}).\]

Every inequality in the derivation above becomes strict when the explanation selects inputs that are predictive of the label because

\[\mathbf{I}(\mathbf{y};\mathbf{x}_{e(\mathbf{x})})>0.\]

Thus, non-encoding explanations have eval-x scores that are at least \(-\mathbf{H}_{q}(\mathbf{y})\).

The optimal eval-x score for any explanation (see Lemma4) equals the negative conditional entropy which is upper bounded by some finite number:

\[\mathbb{E}_{q}\left[\log q(\mathbf{y}\mid\mathbf{x})\right]=-\mathbf{H}_{q}( \mathbf{y}\mid\mathbf{x})=C.\]

Comparing explanations via stripe-x.For any encoding explanation, by Proposition3, for some \(c>0\)

\[\phi_{q}(e)>c.\]

Now, consider \(\alpha^{*}=\frac{\mathbf{I}(\mathbf{y};\mathbf{x})}{c}\geq 0\), which is finite because each term in the ratio is finite. Then, for all \(\alpha>\alpha^{*}\)

\[\alpha\phi_{q}(e)>\alpha^{*}\phi_{q}(e)>\mathbf{I}(\mathbf{y};\mathbf{x}).\]

Thus,

\[-\alpha\phi_{q}(e)<-\mathbf{I}(\mathbf{y};\mathbf{x}).\]

As eval-x scores are below \(C=-\mathbf{H}(\mathbf{y}\mid\mathbf{x})\) for any encoding explanation,

\[\textsc{stripe-x}_{\alpha}(q,e) =\textsc{eval-x}(q,e)-\alpha\phi_{q}(e)\] \[<-\mathbf{H}(\mathbf{y}\mid\mathbf{x})-\mathbf{I}(\mathbf{y}; \mathbf{x})\] \[<-\mathbf{H}(\mathbf{y}\mid\mathbf{x})-(\mathbf{H}_{q}(\mathbf{y })-\mathbf{H}(\mathbf{y}\mid\mathbf{x}))\] \[=-\mathbf{H}_{q}(\mathbf{y}).\]

Finally, for any non-encoding explanation, \(\phi_{q}(e^{\prime})=0\) by Proposition3, stripe-x scores equal eval-x scores, which are lower bounded at \(-\mathbf{H}_{q}(\mathbf{y})\).

Together, for every non-encoding explanation \(e^{\prime}(\mathbf{x})\) and encoding explanation \(e(\mathbf{x})\), it holds that

\[\textsc{stripe-x}_{\alpha}(q,e^{\prime})\geq-\mathbf{H}_{q}(\mathbf{y})> \textsc{stripe-x}_{\alpha}(q,e).\]

This proves that stripe-x is a strong detector of encoding. 

## Appendix B Encoding examples, non-detection of roar,fresh, and non-strong detection of eval-x

### An illustrative dgp for Def: Encoding

With \(\mathcal{B}(0.5)\) being a Bernoulli distribution, consider the following example

\[\mathbf{y}\sim\mathcal{B}(0.5)\quad,\quad\mathbf{z}\sim\mathcal{B }(0.5)\quad,\quad\epsilon_{1},\epsilon_{2},\epsilon_{3}\sim\mathcal{N}\left(0, \mathbf{I}\right),\] \[\mathbf{x}\quad=\begin{cases}[\mathbf{y}+\epsilon_{1},\qquad \epsilon_{3},0,\epsilon_{2}]&\text{if }\mathbf{z}=0,\\ [\qquad\epsilon_{3},\mathbf{y}+\epsilon_{1},1,\epsilon_{2}]&\text{if }\mathbf{z}=1.\end{cases}\]

For this problem, if the third coordinate \(\mathbf{x}_{3}=0\), all the information between the label and the covariates is in the first coordinate \(\mathbf{x}_{1}\), and if \(\mathbf{x}_{3}=1\), the information is between the label and the second coordinate \(\mathbf{x}_{2}\). The corresponding explanation function is \(e(\mathbf{x})=\mathbbm{1}[\mathbf{x}_{3}=0]\xi_{1}+\mathbbm{1}[\mathbf{x}_{3} =1]\xi_{2}\). This explanation is encoding because neither explanation's values \(\mathbf{x}_{1}\) nor \(\mathbf{x}_{2}\) determine the explanation function because it depends on \(\mathbf{x}_{3}\). Formally

\[q(\mathbf{y}=1\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)\neq q(\mathbf{y}=1\mid \mathbf{x}_{1},\mathbf{x}_{3}=0)\implies\mathbf{y}\underline{\cdot}\underline{ \mathbf{\!\!\!\perp}}\mathbf{x}_{3}\mid\mathbf{x}_{1}\implies\mathbf{y} \underline{\cdot}\underline{\mathbf{\!\!\!\perp}}\mathbf{E}_{\mathbf{v}}\mid \mathbf{x}_{1},\]which meets Def: Encoding. Consider an alternate non-encoding explanation function \(e(\mathbf{x})=[1[\mathbf{x}_{4}>0],1[\mathbf{x}_{4}\leq 0],0,0]\); \(\mathbf{x}_{1},\mathbf{x}_{2}\) do not determine \(e(\mathbf{x})\) that depends on the noise \(\epsilon_{2}\) in \(\mathbf{x}_{4}\). That means the unpredictability property in Lemma 1 holds. However, by construction,

\[(\mathbf{y},\mathbf{x}_{1},\mathbf{x}_{2})\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt \perp}}\vrule width 1px\hbox{\kern 2.0pt\perp}}\epsilon_{2}\implies(\mathbf{y}, \mathbf{x}_{1},\mathbf{x}_{2})\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt \perp}}\vrule width 1px\hbox{\kern 2.0pt\perp}}\mathbf{E}_{\mathbf{v}}\implies \mathbf{y}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\vrule width 1px\hbox{\kern 2.0pt \perp}}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{1}\quad\text{and}\quad\mathbf{y} \mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\vrule width 1px\hbox{\kern 2.0pt \perp}}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{2}.\]

So no additional information about the label is encoded:

\[\mathbf{y}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\vrule width 1px\hbox{\kern 2.0pt \perp}}\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}.\]

The additional information property in Lemma 1 avoids such cases where the explanations keeps additional information that is irrelevant to the label.

### Encoding explanations conceal predictive inputs that affect the explanation

Consider the following dgp

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}]\sim\mathcal{B}(0.5) ^{\otimes 3},\qquad\quad\mathbf{y}=\begin{cases}\mathbf{x}_{1}&\text{ if }\mathbf{x}_{3}=1,\\ \mathbf{x}_{2}&\text{ if }\mathbf{x}_{3}=0.\end{cases}\] (15)

Let \(e\) be an encoding explanation that selects the first coordinate if \(\mathbf{x}_{3}=1\) and the second coordinate otherwise. We never observe \(\mathbf{x}_{3}\) when looking only at the explanation. Table 4 shows all possible values of this explanation. Notice that in the third and fourth rows, the value of \(\mathbf{x}_{e(\mathbf{x})}\) changes to match the label \(\mathbf{y}\) exactly, even though the values of the first two coordinates that we can observe stay constant. It is impossible to understand the perfect predictiveness of \(\mathbf{x}_{e(\mathbf{x})}\), as the encoding explanation conceals the control flow feature \(\mathbf{x}_{3}\) that determines which of the first two features should be picked to predict the label.

### Position-based encoding fits Def: Encoding

Recall the perceptual task that classifying images of dogs versus classifying images of cats, and consider the encoding explanation \(e_{\text{position}}(\mathbf{x})\) that is

\[e_{\text{position}}(\mathbf{x})=\xi_{1}\quad\text{ if }\quad q( \mathbf{y}=\text{dog }|\,\mathbf{x})=1,\] \[e_{\text{position}}(\mathbf{x})=\xi_{2}\quad\text{ if }\quad q( \mathbf{y}=\text{cat }|\,\mathbf{x})=1.\]

Assume that the inputs in the top leftmost pixels are always background, meaning that the values of these inputs provide no information about the label \(\mathbf{y}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\vrule width 1px\hbox{\kern 2.0pt \perp}}\mathbf{x}_{1},\mathbf{x}_{2}\). Now we check if this intuitively-defined position-encoded explanation meets the definition for encoding (Def: Encoding). To condition on \(\mathbf{x}_{\xi_{1}},\mathbf{E}_{\xi_{1}}=0\), we need \(q(\mathbf{y}=\text{dog})\neq 1\). Note that

\[q(\mathbf{E}_{\xi_{1}}=1\mid\mathbf{x}_{\xi_{1}})=q(\mathbf{y}=\text{dog}|\, \mathbf{x}_{\xi_{1}})=q(\mathbf{y}=\text{dog})\neq 1.\]

Def: Encoding holds because the indicator of which explanation was chosen \(\mathbf{E}_{\xi_{1}}\) determines the label.

\[q(\mathbf{y}=\text{dog }|\,\mathbf{x}_{\xi_{1}},\mathbf{E}_{\xi_{1}}=1)=1\neq 0=q( \mathbf{y}=\text{dog }|\,\mathbf{x}_{\xi_{1}},\mathbf{E}_{\xi_{1}}=0)\implies\mathbf{y} \mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\vrule width 1px\hbox{\kern 2.0pt \perp}}\mathbf{E}_{\xi_{1}}\mid\mathbf{x}_{\xi_{1}}.\]

This example shows how the encoding definition Def: Encoding captures the informally described position-based encoding from the literature.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & & & & \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\) \\ \(\times_{1}\) & \(\times_{2}\) & \(\times_{3}\) & \(\mathbf{y}\) & \(\mathbf{v}\) & \(\mathbf{a}\) \\ \hline \(0\) & \(0\) & \(0\) & \(0\) & \([0,1,0]\) & \(0\) \\ \(0\) & \(0\) & \(1\) & \(0\) & \([1,0,0]\) & \(0\) \\ \(0\) & \(1\) & \(0\) & \(1\) & \([0,1,0]\) & \(1\) \\ \(0\) & \(1\) & \(1\) & \(0\) & \([1,0,0]\) & \(0\) \\ \(1\) & \(0\) & \(0\) & \(0\) & \([0,1,0]\) & \(0\) \\ \(1\) & \(0\) & \(1\) & \(1\) & \([1,0,0]\) & \(1\) \\ \(1\) & \(1\) & \(0\) & \(1\) & \([0,1,0]\) & \(1\) \\ \(1\) & \(1\) & \(1\) & \(1\) & \([1,0,0]\) & \(1\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Possible values of the inputs, label, and explanation for the dgp in eq. (15)

### Prediction-based encoding fits Def: Encoding

The informal example of prediction-based encoding from Section 3.1 selects a single input that makes the prediction from all of the input have the highest confidence when given the single input. One way to mathematically express such a selection is as follows:

\[e_{\text{prediction}}(\mathbf{x}) =\xi_{\text{argmax},q(\mathbf{y}=1\mid\mathbf{x}_{i})}\text{ if }q(\mathbf{y}=1\mid\mathbf{x})>0.5,\] (16) \[e_{\text{prediction}}(\mathbf{x}) =\xi_{\text{argmin},q(\mathbf{y}=1\mid\mathbf{x}_{i})}\text{ if }q( \mathbf{y}=1\mid\mathbf{x})\leq 0.5.\]

Here, we describe one set of conditions on the distribution \(q(\mathbf{y},\mathbf{x})\) for which the explanation in eq. (16) fits the definition of encoding in Def: Encoding. Assume that there exists a non-measure-zero set \(\mathbf{U}\subseteq\{\mathbf{x}:q(\mathbf{y}=1\mid\mathbf{x})>0.5\}\) and an index \(k\) such that

\[\mathbf{x}\in\mathbf{U} \implies q(\mathbf{y}=1\mid\mathbf{x})\geq\rho,\] (17) \[\mathbf{x}\in\mathbf{U} \implies\forall i\quad q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{i}})<q (\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}}),\] (18) \[\mathbf{x}\not\in\mathbf{U} \implies q(\mathbf{y}=1\mid\mathbf{x})<\rho,\] (19) \[\mathbf{x}\not\in\mathbf{U} \implies\exists i,j\quad q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{i}})>q (\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}})>q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{j}}).\] (20)

Further, assume that \(\mathbf{x}_{\xi_{k}}\) alone does not determine \(\mathbf{x}\in\mathbf{U}\):

\[0<\mathbb{E}\left[\mathbb{1}\!\left[\mathbf{x}\in\mathbf{U}\right]\mid\mathbf{ x}_{\xi_{k}}\right]<1.\] (21)

The assumptions above imply the facts below about \(e_{\text{prediction}}(\mathbf{x})\):

1. By eqs. (18) and (20) \[\mathbf{x}\in\mathbf{U}\Leftrightarrow e_{\text{prediction}}(\mathbf{x})=\xi_{k}.\] Define the explanation indicator \(\mathbf{E}_{\mathbf{v}}=\mathbb{1}[e_{\text{prediction}}(\mathbf{x})=\mathbf{v}]\).
2. By eqs. (18) and (20) and the definition of \(\mathbf{E}_{\mathbf{v}}\) \[\mathbf{x}\in\mathbf{U}\Leftrightarrow\mathbf{E}_{\xi_{k}}=1.\] (22)
3. By eqs. (21) and (22) \[0<q(\mathbf{E}_{\xi_{k}}=1\mid\mathbf{x}_{\xi_{k}})=q(\mathbf{x}\in\mathbf{U} \mid\mathbf{x}_{\xi_{k}})<1.\] (23)
4. By eq. (23), \(q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{\xi_{k}}=1)\) and \(q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{\xi_{k}}=0)\) are well defined. Then, by eqs. (17) and (19), for \(\mathbf{x}\in\mathbf{U}\) \[q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{k}=1)\] \[\qquad=\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{\xi_{k}},\mathbf{ E}_{k}=1)}q(\mathbf{y}\mid\mathbf{x})\] \[\qquad\geq\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{\xi_{k}},\mathbf{ E}_{k}=1)}\rho\qquad\qquad\{\text{as }\mathbf{E}_{k}=1\implies\mathbf{x}\in\mathbf{U}\}\] \[\qquad=\rho.\]

\[q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{k}=0)\] \[\qquad=\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{\xi_{k}},\mathbf{ E}_{k}=0)}q(\mathbf{y}\mid\mathbf{x})\] \[\qquad<\mathbb{E}_{q(\mathbf{x}\mid\mathbf{x}_{\xi_{k}},\mathbf{ E}_{k}=0)}\rho\qquad\qquad\{\text{as }\mathbf{E}_{k}=0\implies\mathbf{x}\not\in\mathbf{U}\}\] \[\qquad=\rho.\]

Thus, for all elements of \(\{\mathbf{x}_{\xi_{k}}:\mathbf{x}\in\mathbf{U}\}\)

\[q(\mathbf{y}=1\mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{\xi_{k}}=1)>q(\mathbf{y}=1 \mid\mathbf{x}_{\xi_{k}},\mathbf{E}_{\xi_{k}}=0).\] (24)

By Lemma 1, the properties in eqs. (23) and (24) imply that \(\forall\mathbf{a}\in\{\mathbf{x}_{\xi_{k}}:\mathbf{x}\in\mathbf{U}\}\)

\[\mathbf{y}\mbox{$\cdot$\!\!$-}\mathbf{E}_{\xi_{k}}\mid\mathbf{x}_{\xi_{k}}= \mathbf{a}.\]

Finally, the set \(\mathbf{U}_{k}=\{\mathbf{x}_{\xi_{k}}:\mathbf{x}\in\mathbf{U}\}\) is non-measure-zero: as \(\mathbb{1}[\mathbf{x}\in\mathbf{U}]=1\implies\mathbb{1}[\mathbf{x}_{\xi_{k}} \in\mathbf{U}_{k}]=1\), accumulating \(q(\mathbf{x})\) with the restriction \(\mathbf{x}_{\xi_{k}}\in\mathbf{U}_{k}\) leads to at least as much mass as accumulating with the stricter restriction \(\mathbf{x}\in\mathbf{U}\):

\[q(\mathbf{x}_{\xi_{k}}\in\mathbf{U}_{k})=\int q(\mathbf{x})\mathbb{1}[\mathbf{ x}_{\xi_{k}}\in\mathbf{U}_{k}]d\mathbf{x}\geq\int q(\mathbf{x})\mathbb{1}[ \mathbf{x}\in\mathbf{U}]d\mathbf{x}=q(\mathbf{x}\in\mathbf{U})>0.\]

Together, the last two equations implies that Def: Encoding holds for \(e_{\text{prediction}}(\mathbf{x})\) from eq. (16).

### marg explanations are encoding

We provide an illustrative example of a marg explanation for the dgp in Figure 5. Here, we show how a mathematical formulation of marg satisfied Def: Encoding.

Consider a generic dgp with a Bernoulli control flow input denoted \(\mathbf{x}_{c}\): for some distinct sets \(U,V\) that do not include \(c\) and let no combination of \(\mathbf{x}_{c},\mathbf{x}_{U},\mathbf{x}_{V}\) determine the rest

\[\mathbf{x}_{c}=1 \implies q(\mathbf{y}\mid\mathbf{x})=q(\mathbf{y}\mid\mathbf{x}_{U}),\] \[\mathbf{x}_{c}=0 \implies q(\mathbf{y}\mid\mathbf{x})=q(\mathbf{y}\mid\mathbf{x}_{V}).\]

Further, assume that the two subsets leads to different distributions over \(\mathbf{y}=1\) such that on a non-zero measure subset \(\mathbf{S}_{U}\subseteq\{\mathbf{x}_{U}:\mathbf{x}\text{ such that }\mathbf{x}_{c}=1\}\)

\[q(\mathbf{y}=1\mid\mathbf{x}_{U},\mathbf{x}_{c}=1)\neq q(\mathbf{y}=1\mid \mathbf{x}_{U},\mathbf{x}_{c}=0).\] (25)

Now, consider a marg explanation that looks at \(\mathbf{x}_{c}\) and outputs the corresponding sets \(U,V\):

\[e(\mathbf{x})=U\quad\text{if}\quad\mathbf{x}_{c}=1\quad\text{else}\quad e( \mathbf{x})=V.\]

By definition the explanation only depends on the control flow input, not by \(\mathbf{x}_{U}\) or \(\mathbf{x}_{V}\). Next, as \(\mathbf{E}_{U}=1\) is the same event as \(\mathbf{x}_{c}=1\), \(e(\mathbf{x})\) is encoding because the assumption from eq. (25) implies:

\[q(\mathbf{y}\mid\mathbf{x}_{U},\mathbf{E}_{U}=1)\neq q(\mathbf{y}\mid\mathbf{x }_{U},\mathbf{E}_{U}=0).\]

Then, this inequality holds for all elements of the non-measure-zero set \(\mathbf{S}_{U}\), by Lemma 1, marg is encoding.

### Proof of Proposition 1

**Definition 4**.: _We denote \(\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})\) as the function that maps explanation \(\mathbf{x}_{e(\mathbf{x})}=(\mathbf{v},\mathbf{a})\) to the values the inputs take at the selected indices, right-padded to have the same dimension as the input \(\mathbf{x}\in\mathbf{R}^{d}\):_

\[\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})_{j}=\begin{cases}\mathbf{a}_{j}& \text{if }1\leq j\leq\sum_{i=1}^{d}\mathbf{v}_{i}\\ \mathtt{pad-token}&\text{if }\sum_{i=1}^{d}\mathbf{v}_{i}<j\leq d\end{cases}\]

_For example, if \(\mathbf{x}=[\alpha,\beta,\gamma]\) and \(e(\mathbf{x})=[0,1,0]\), then \(\mathbf{x}_{e(\mathbf{x})}=([0,1,0],[\beta])\) and_

\[\mathtt{val}(\mathbf{x}_{e(\mathbf{x})})=[\beta,\mathtt{pad-token},\mathtt{ pad-token}].\]

Figure 5: Example dgp and marg encoding. **(a)** The color determines whether the label is produced from the top or bottom image. **(b)** An explanation that correctly reveals that the label is generated based on both the color and, as dictated by the color, the top or the bottom image. The label is deterministic given the value of the explanation which means the label can be predicted perfectly. **(c)** An encoding explanation would be one that produces only the top or the bottom animal image based on the color being red of blue respectively. This returned animal image does not indicate the fact that the data generating process depends on color. Now, the animal image selected by the explanation alone is insufficient to dictate the label because the color determines which image determines the label. The identity of the image, whether top or bottom, provides additional information about the label beyond the values explanation, as captured in Def: Encoding.

[MISSING_PAGE_EMPTY:31]

Showing that \(e_{\text{encode}}\) is the optimal reductive explanation for eq.3 and scores better than a constant explanation under eval-x

We repeat the dgp in eq.3 here

\[\mathbf{x} =[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}]\sim\mathcal{B}(0.5 )^{\otimes 3},\] \[\mathbf{y} =\begin{cases}\mathbf{x}_{1}&\text{w.p. }0.9\quad\text{else}\quad 1- \mathbf{x}_{1}\quad\text{if }\mathbf{x}_{3}=1,\\ \mathbf{x}_{2}&\text{w.p. }0.9\quad\text{else}\quad 1-\mathbf{x}_{2}\quad\text{if } \mathbf{x}_{3}=0.\end{cases}\]

**Lemma 5**.: _In the dgp in eq.3,_

\[q(\mathbf{y}=1\mid\mathbf{x}_{1}=1)=0.7\] \[q(\mathbf{y}=1\mid\mathbf{x}_{2}=1)=0.7,\] \[q(\mathbf{y}=1\mid\mathbf{x}_{1}=0)=0.3,\] \[q(\mathbf{y}=1\mid\mathbf{x}_{2}=0)=0.3.\] \[q(\mathbf{y}=1\mid\mathbf{x}_{3})=0.5.\]

Proof.: We can compute these values from Table5. 

**Proposition 2**.: _Let \(e_{\mathbf{e}}(\mathbf{x})=\xi_{3}\). Then, for the dgp in eq.3, eval-x\((q,e_{\text{encode}})>\) eval-x\((q,e_{\mathbf{e}})\)._

Proof.: By Lemma5, we have

\[\text{eval-x}(q,e_{c})=\mathbb{E}_{q(\mathbf{y},\mathbf{x})}\log q(\mathbf{y} \mid\mathbf{x}_{3})=\mathbb{E}_{q(\mathbf{y},\mathbf{x})}\log 0.5\approx-0.69.\]

Now, denote \(e_{\text{encode}}\) as \(e_{e}\) for ease of reading

\[\text{eval-x}(q,e_{c}) =\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q(\mathbf{x}_{c(\mathbf{ x})})}\mathbb{E}_{\mathbf{y}\sim q(\mathbf{y}\mid\mathbf{x}_{c(\mathbf{x})}=( \mathbf{v},\mathbf{a}))}[\log q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}}=\mathbf{ a})]\] \[=q(\mathbf{x}_{3}=1)\mathbb{E}_{q(\mathbf{x}_{1})}\mathbb{E}_{q( \mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)}\log q(\mathbf{y}\mid\mathbf{x }_{1})\] \[\quad+q(\mathbf{x}_{3}=0)\mathbb{E}_{q(\mathbf{x}_{2})}\mathbb{E} _{q(\mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{3}=0)}\log q(\mathbf{y}\mid \mathbf{x}_{2})\] \[=0.5*0.5*(0.9*-\log 0.7+0.1*-\log 0.3)*2\] \[\quad+0.5*0.5*(0.9*-\log 0.7+0.1*-\log 0.3)*2\] \[\approx-0.44.\]

This concludes that eval-x\((q,e_{\text{encode}})>\) eval-x\((q,e_{c})\). 

**Lemma 6**.: _In the dgp in eq.3, \(e_{\text{encode}}(\mathbf{x})\) is an eval-x-optimal reductive explanation and is encoding._

Proof.: First, the following properties show for the dgp because when \(\mathbf{x}_{3}=1\), \(\mathbf{y}\) only depends on \(\mathbf{x}_{1}\), and if \(\mathbf{x}_{3}=0\), \(\mathbf{y}\) only depends on \(\mathbf{x}_{2}\):

\[\mathbf{x}_{3}\mbox{$\perp\!\!\!\perp$}\mathbf{y}\quad,\quad\mathbf{y}\mbox{$ \perp\!\!\!\perp$}\mathbf{x}_{2}\mid\mathbf{x}_{3}=1\quad,\quad\mathbf{y} \mbox{$\perp\!\!\!\perp$}\mathbf{x}_{1}\mid\mathbf{x}_{3}=0.\]

These independencies imply that

\[q(\mathbf{y}\mid\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},1])=q(\mathbf{y} \mid\mathbf{x}_{1},\mathbf{x}_{3}=1)\quad,\quad q(\mathbf{y}\mid\mathbf{x}=[ \mathbf{x}_{1},\mathbf{x}_{2},0])=q(\mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{ 3}=0).\]

Then, the optimal explanation function that achieves eval-x\({}^{*}\) is \(e(\mathbf{x})=[1,0,1]\) if \(\mathbf{x}_{3}=1\) and \([0,1,1]\) otherwise.

Reductive explanations of size 1.If the explanation is forced to have fewer than \(2\) inputs, the optimal reductive explanation \(e(\mathbf{x})\) is only allowed to be one of \(\xi_{1},\xi_{2},\xi_{3}\):

\[\max_{e:\left|e(\mathbf{x})\right|\leq 1}\mathbb{E}_{q(\mathbf{y},\mathbf{x})} \sum_{i\in\{1,2,3\}}\mathbbm{1}[e(\mathbf{x})=\xi_{i}]q(\mathbf{y}\mid\mathbf{ x}_{i}).\]

Rewriting this expression to split the support of \(\mathbf{x}\) based on \(\mathbf{x}_{3}=1\) or \(0\):

\[\mathbb{E}_{q(\mathbf{y},\mathbf{x})}\sum_{i\in\{1,2,3\}} \mathbbm{1}[e(\mathbf{x})=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x}_{i})\] \[=q(\mathbf{x}_{3}=1)\mathbb{E}_{q(\mathbf{x}_{2}\mid\mathbf{x}_ {3}=1)}\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{1}\mid\mathbf{x}_{3}=1)}\sum_{i \in\{1,2,3\}}\mathbbm{1}[e(\mathbf{x})=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x} _{i})\] \[\quad+q(\mathbf{x}_{3}=0)\mathbb{E}_{q(\mathbf{x}_{1}\mid\mathbf{ x}_{3}=0)}\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{2}\mid\mathbf{x}_{3}=0)}\sum_{i \in\{1,2,3\}}\mathbbm{1}[e(\mathbf{x})=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x} _{i})\] \[=0.5\,\mathbb{E}_{q(\mathbf{x}_{2})}\mathbb{E}_{q(\mathbf{y}, \mathbf{x}_{1}\mid\mathbf{x}_{3}=1)}\sum_{i\in\{1,2,3\}}\mathbbm{1}[e(\mathbf{ x})=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x}_{i})\] \[\quad+0.5\,\mathbb{E}_{q(\mathbf{x}_{1})}\mathbb{E}_{q(\mathbf{y}, \mathbf{x}_{2}\mid\mathbf{x}_{3}=0)}\sum_{i\in\{1,2,3\}}\mathbbm{1}[e(\mathbf{ x})=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x}_{i})\] \[=\frac{1}{2}\Bigg{(}\mathbb{E}_{q(\mathbf{x}_{2})}\Bigg{[}\mathbb{ E}_{q(\mathbf{y},\mathbf{x}_{1}\mid\mathbf{x}_{3}=1)}\mathbbm{1}[e([\mathbf{x}_{1}, \mathbf{x}_{2},1])=\xi_{1}]\log q(\mathbf{y}\mid\mathbf{x}_{1})\] \[\qquad\qquad\qquad+\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{1}\mid \mathbf{x}_{3}=1)}\sum_{i\in{2,3}}\mathbbm{1}[e([\mathbf{x}_{1},\mathbf{x}_{2},1])=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x}_{i})\Bigg{]}\] \[\quad+\mathbb{E}_{q(\mathbf{x}_{1})}\Bigg{[}\mathbb{E}_{q( \mathbf{y},\mathbf{x}_{2}\mid\mathbf{x}_{3}=0)}\mathbbm{1}[e([\mathbf{x}_{1}, \mathbf{x}_{2},0])=\xi_{2}]\log q(\mathbf{y}\mid\mathbf{x}_{2})\] \[\qquad\qquad\qquad\qquad+\mathbb{E}_{q(\mathbf{y},\mathbf{x}_{2} \mid\mathbf{x}_{3}=0)}\sum_{i\in{1,3}}\mathbbm{1}[e([\mathbf{x}_{1},\mathbf{x} _{2},0])=\xi_{i}]\log q(\mathbf{y}\mid\mathbf{x}_{i})\Bigg{]}\Bigg{)}\] \[=\frac{1}{2}\mathbb{E}_{q(\mathbf{x}_{1},\mathbf{x}_{2}\mid\mathbf{ x}_{3}=1)}\bigg{[}\mathbbm{1}[e([\mathbf{x}_{1},\mathbf{x}_{2},1])=\xi_{1}] \mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)}\log q(\mathbf{ y}\mid\mathbf{x}_{1})\] (27) \[\qquad\qquad\qquad\qquad+\sum_{i\in{2,3}}\mathbbm{1}[e([\mathbf{ x}_{1},\mathbf{x}_{2},1])=\xi_{i}]\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1}, \mathbf{x}_{3}=0)}\log q(\mathbf{y}\mid\mathbf{x}_{i})\bigg{]}\] \[\quad+\frac{1}{2}\mathbb{E}_{q(\mathbf{x}_{1},\mathbf{x}_{2} \mid\mathbf{x}_{3}=0)}\bigg{[}\mathbbm{1}[e([\mathbf{x}_{1},\mathbf{x}_{2},0])= \xi_{2}]\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{3}=0)}\log q( \mathbf{y}\mid\mathbf{x}_{2})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\sum_{i\in{1,3}} \mathbbm{1}[e([\mathbf{x}_{1},\mathbf{x}_{2},0])=\xi_{i}]\mathbb{E}_{q( \mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{3}=0)}\log q(\mathbf{y}\mid\mathbf{x} _{i})\bigg{]}.\] (28)

We will now focus on the three terms within each of 27 and 28. Due to the following equality

\[q(\mathbf{y}=1\mid\mathbf{x}_{1}=1,\mathbf{x}_{3}=1) =q(\mathbf{y}=0\mid\mathbf{x}_{1}=0,\mathbf{x}_{3}=1)\] \[=q(\mathbf{y}=1\mid\mathbf{x}_{2}=1,\mathbf{x}_{3}=0)=q(\mathbf{y} =0\mid\mathbf{x}_{2}=0,\mathbf{x}_{3}=0)=0.9,\]

the expectations in the first terms in each of 27 and 28 are

\[\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)}\log q(\mathbf{y} \mid\mathbf{x}_{1})=\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{3}=1 )}\log q(\mathbf{y}\mid\mathbf{x}_{2})=0.9\log 0.7+0.1\log 0.3\approx-0.44.\]

Next we turn to setting \(i=1\) term in 27. Due that \(q(\mathbf{y}=1\mid\mathbf{x}_{2}=1)=q(\mathbf{y}=0\mid\mathbf{x}_{2}=0)\),

\[\mathbf{x}_{1}=\mathbf{x}_{2}\implies\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)}\log q(\mathbf{y}\mid\mathbf{x}_{2})=(0.9\log 0.7+0.1\log 0.3) \approx-0.44,\]\[\mathbf{x}_{1}\neq\mathbf{x}_{2}\implies\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1}, \mathbf{x}_{3}=1)}\log q(\mathbf{y}\mid\mathbf{x}_{2})=(0.9\log 0.3+0.1\log 0.7) \approx-1.12.\]

The same equalities hold for the \(i=2\) term in eq.28\(\mathbb{E}_{q(\mathbf{y},\mid\mathbf{x}_{2},\mathbf{x}_{3}=0)}\log q( \mathbf{y}\mid\mathbf{x}_{1})\). Finally, regardless of \(\mathbf{x}_{1},\mathbf{x}_{2}\), the \(i=3\) terms in both eq.27 and eq.28 can be expressed as follows:

\[\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_{3}=1)}\log q(\mathbf{y }\mid\mathbf{x}_{2})\mathbb{E}_{q(\mathbf{y}\mid\mathbf{x}_{2},\mathbf{x}_{3}= 0)}\log q(\mathbf{y}\mid\mathbf{x}_{1})=(0.9\log 0.5+0.1\log 0.5)\approx-0.69.\]

Now we can maximize the sum of eq.27 and eq.28, over \(e(\mathbf{x})\) such that \(|e(\mathbf{x})|=1\).

Notice that setting \(\mathbbm{1}[e(\mathbf{x})=\xi_{1}]=1\) when \(\mathbf{x}_{3}=1\) and \(\mathbbm{1}[e(\mathbf{x})=\xi_{2}]=1\) when \(\mathbf{x}_{3}=0\) achieves the highest score \(-0.44\) in each of eq.27 and eq.28. This implies that one optimal reductive explanation is \(\xi_{1}=[1,0,0]\) if \(\mathbf{x}_{3}=1\) and \(\xi_{2}=[0,1,0]\) otherwise. This is an encoding explanation as we show below. Due to \(\mathbf{E}_{\xi_{1}}=\mathbf{x}_{3}\),

\[q(\mathbf{y}=1\mid\mathbf{x}_{1},\mathbf{E}_{\xi_{1}}=1)=0.9\neq q(\mathbf{y} =1\mid\mathbf{x}_{1},\mathbf{E}_{\xi_{1}}=0)=0.5.\]

In turn, \(\mathbf{y}\mathrel{\vbox{\hbox{\scriptsize.}\hbox{\scriptsize.}}}\joinrel \mathbf{E}_{\xi_{1}}\mid\mathbf{x}_{\xi_{1}}\) for \(\{\mathbf{x}:e(\mathbf{x})=\xi_{1}\}\) and Def: Encoding holds, meaning that \(e(\mathbf{x})\) is encoding.

### An example of misestimation of eval-x

Consider the following example where

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\mathbf{ x}_{4}],\] \[\mathbf{x}_{2},\mathbf{x}_{3}\sim\mathcal{B}(0.5)^{\otimes 2},\, \mathbf{x}_{1},\mathbf{x}_{4}\sim\mathcal{N}(0,\mathbf{I}),\quad\mathbf{y}= \mathbf{x}_{2}\oplus\mathbf{x}_{3}.\]

Assume that the misestimated eval-x model satisfies these equalities

\[q^{\text{misestimated}}_{\xi_{1}}(\mathbf{y}=1\mid\mathbf{x}_{1 })=1\quad\text{for all }\mathbf{x}_{1},\] \[q^{\text{misestimated}}_{\xi_{4}}(\mathbf{y}=0\mid\mathbf{x}_{4 })=1\quad\text{for all }\mathbf{x}_{4}.\]

There exists a bad explanation that scores optimally under the misestimated eval-x:

\[e(\mathbf{x})=\begin{cases}\xi_{1}&\quad\text{if }\mathbf{x}_{2}\oplus\mathbf{x}_{3}=1, \\ \xi_{4}&\quad\text{if }\mathbf{x}_{2}\oplus\mathbf{x}_{3}=0.\end{cases}\]

Then the eval-x score of this explanation under this misestimation is

\[\text{eval-x}^{\text{misestimated}}(q,e)=\mathbb{E}_{q}[\log q^{ \text{misestimated}}_{e(\mathbf{x})}(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x}) })]\] \[=q(\mathbf{y}=1)\mathbb{E}_{q}[\log q^{\text{misestimated}}_{e( \mathbf{x})}(\mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})})\mid\mathbf{y}=1]+q( \mathbf{y}=0)\mathbb{E}_{q}[\log q^{\text{misestimated}}_{e(\mathbf{x})}( \mathbf{y}\mid\mathbf{x}_{e(\mathbf{x})})\mid\mathbf{y}=0]\] \[=0.5\cdot\mathbb{E}_{q}[\log q^{\text{misestimated}}_{\xi_{1}}( \mathbf{y}\mid\mathbf{x}_{1})\mid\mathbf{y}=1]+0.5\cdot\mathbb{E}_{q}[\log q^ {\text{misestimated}}_{\xi_{4}}(\mathbf{y}\mid\mathbf{x}_{4})\mid\mathbf{y}=0]\] \[=0.\]

Since \(\mathbf{y}\) is deterministic given \(\mathbf{x}\) so the maximum value of the eval-x score is also \(0\). So the bad explanation scores optimally due to misestimation. Deterministic \(\mathbf{y}\mid\mathbf{x}\) is not necessary for estimation error to affect explanation quality. Here, with this incorrectly estimated eval-x, inputs that are pure noise, independent of everything, will be chosen.

### Attention map explanations be encode.

Here, treating each of the coordinates of \(\mathbf{x}\) as tokens, we consider a cross-attention based predictive model of the following form: with \(\gamma(\mathbf{a})\) as softmax function over a vector \(\mathbf{a}\), \(W\) as a matrix, \(\boldsymbol{\alpha},\boldsymbol{\beta}\) as vectors, and \(\sigma\) as the sigmoid function, and \(\kappa\) as the temperature, the predictive model \(f(\cdot)\) is

\[f(\mathbf{x})=\sigma\left(\sum_{i}\boldsymbol{\beta}_{i}\left[\sum_{j}\gamma( \kappa\mathbf{x}_{i}*W\mathbf{x})_{j}\boldsymbol{\alpha}_{j}\mathbf{x}_{j} \right]\right).\]We then show that using the highest attention score as the explanation produces an encoding explanation. For this example, we consider the following dgp:

\[\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3} \sim\mathcal{B}(0.5)^{\otimes 3},\] \[\mathbf{z}^{+} =[\mathbf{z}_{1}+1,\quad 0,\quad+1],\] \[\mathbf{z}^{-} =[0\quad,-\mathbf{z}_{2}-1,\quad-1],\] \[\mathbf{x} =\begin{cases}&\mathbf{z}^{+}\text{ if }\mathbf{z}_{3}=1,\\ &\mathbf{z}^{-}\text{ if }\mathbf{z}_{3}=0,\end{cases}\] (29) \[\mathbf{y} \sim\mathcal{B}(\rho)\quad\text{ where }\quad\rho=\begin{cases} \sigma(\mathbf{x}_{1})&\text{ if }\mathbf{x}_{3}=1,\\ \sigma(-\mathbf{x}_{2})&\text{ if }\mathbf{x}_{3}=-1.\end{cases}\]

The following setting of parameters in \(f(\mathbf{x})\) produces a function of \(\mathbf{x}\) that converges to \(\rho\) as \(\kappa\to\infty\):

\[\boldsymbol{\alpha} =[1,-1,0], \boldsymbol{\beta} =[1,1,0], W =\begin{pmatrix}0&0&0\\ 0&0&0\\ 0&0&1\end{pmatrix}.\] (30)

By definition of \(W\)

\[W\mathbf{x} =\begin{cases}[0,0,1]\text{ if }\mathbf{x}_{3}=1\\ [0,0,-1]\text{ if }\mathbf{x}_{3}=-1\end{cases}\] \[\implies\mathbf{x}_{1}W\mathbf{x} =\begin{cases}[\mathbf{z}_{1}+1,0,0]\text{ if }\mathbf{x}_{3}=1\\ [0,0,0]\text{ if }\mathbf{x}_{3}=-1\end{cases} \implies\gamma(\mathbf{x}_{1}W\mathbf{x})\overset{\kappa\to \infty}{\longrightarrow}\begin{cases}[1,0,0]\text{ if }\mathbf{x}_{3}=1\\ [0,0,0]\text{ if }\mathbf{x}_{3}=-1\end{cases}\] \[\implies\mathbf{x}_{2}W\mathbf{x} =\begin{cases}[0,0,0]\text{ if }\mathbf{x}_{3}=1\\ [0,\mathbf{z}_{2}+1,0]\text{ if }\mathbf{x}_{3}=-1\end{cases} \implies\gamma(\mathbf{x}_{2}W\mathbf{x})\overset{\kappa\to \infty}{\longrightarrow}\begin{cases}[0,0,0]\text{ if }\mathbf{x}_{3}=1\\ [0,1,0]\text{ if }\mathbf{x}_{3}=-1\end{cases}.\]

Then, \(\boldsymbol{\beta}_{3}=0\), the inner sum for \(i=3\) does not appear in the function \(f(\mathbf{x})\) Then, as \(\alpha_{3}=0\), \(\alpha_{1}=1,\alpha_{2}=-1\),

\[\sum_{j}\gamma(\kappa\mathbf{x}_{1}*W\mathbf{x})_{j}\boldsymbol{ \alpha}_{j}\mathbf{x}_{j}\overset{\kappa\to\infty}{\longrightarrow}\begin{cases} \mathbf{x}_{1}\text{ if }\mathbf{x}_{3}=1\\ 0\text{ if }\mathbf{x}_{3}=-1\end{cases},\] (31) \[\sum_{j}\gamma(\kappa\mathbf{x}_{2}*W\mathbf{x})_{j}\boldsymbol{ \alpha}_{j}\mathbf{x}_{j}\overset{\kappa\to\infty}{\longrightarrow}\begin{cases} 0\text{ if }\mathbf{x}_{3}=1\\ -\mathbf{x}_{2}\text{ if }\mathbf{x}_{3}=-1\end{cases}.\] (32)

In turn, as \(\beta_{1}=\beta_{2}=1\)

\[f(\mathbf{x}) =\sigma\left(\sum_{i\in 1,2}\beta_{i}\left[\sum_{j}\gamma(\kappa \mathbf{x}_{i}*W\mathbf{x})_{j}\boldsymbol{\alpha}_{j}\mathbf{x}_{j}\right] \right)\overset{\kappa\to\infty}{\longrightarrow}\begin{cases}\sigma( \mathbf{x}_{1})\text{ if }\mathbf{x}_{3}=1\\ \sigma(-\mathbf{x}_{2})\text{ if }\mathbf{x}_{3}=-1\end{cases}.\] (33)

So, as \(\kappa\to\infty\), the function \(f(\mathbf{x})\), with the parameters in eq. (30), converges to \(\rho(\mathbf{x})\), meaning that this model will achieve the population log-likelihood optimum under the dgp in eq. (29).

Now, the attention map as an explanation selects \(\mathbf{x}_{1}\) if \(\mathbf{x}_{3}=1\) and \(\mathbf{x}_{2}\) otherwise; this comes from eq. (31) and eq. (32). This is an encoding explanation because \(\mathbf{E}_{\xi_{1}}=1\) if \(\mathbf{x}_{3}=1\) which gives

\[q(\mathbf{y}\mid\mathbf{x}_{1})\neq q(\mathbf{y}\mid\mathbf{x}_{1},\mathbf{x}_ {3}=1)\implies\mathbf{y}\underline{\cdot}\mathbf{E}_{\xi_{1}}\mid\mathbf{x}_ {\xi_{1}}.\]

## Appendix C Experimental details

### Estimating stripe-x

To compute the **KL** term in encode-meter, we estimate \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}},\mathbf{y})\) and \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}})\). To estimate these, we train a single model -- to predict \(\mathbf{E}_{\mathbf{v}}\) from \(\mathbf{x}_{\mathbf{v}}\) and a new variable \(\ell\) that can equal the label \(\mathbf{y}\) or a dummy value null that is outside the support of \(\mathbf{y}\) -- in the following way:

\[\arg\max_{\theta}\mathbb{E}_{\mathbf{x},\mathbf{y}\sim q(\mathbf{x},\mathbf{y})}\Big{[} \log p_{\theta}(\mathbf{E}_{\mathbf{v}}=\mathbbm{1}[e(\mathbf{x})= \mathbf{v}]\mid\mathbf{x}_{\mathbf{v}},\ell=\mathbf{y})\] \[+\log p_{\theta}(\mathbf{E}_{\mathbf{v}}=\mathbbm{1}[e(\mathbf{x} )=\mathbf{v}]\mid\mathbf{x}_{\mathbf{v}},\ell=\texttt{null})\Big{]}.\] (34)

As log-likelihood is a proper scoring rule and \(q(\mathbf{y}=\texttt{null})=0\), the maximum above is achieved when

\[p_{\theta}(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}},\ell=\mathbf{y })=q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}},\mathbf{y}=\mathbf{y} )\qquad p_{\theta}(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}},\ell= \texttt{null})=q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}}).\]

In summary, to estimate encode-meter, solve eq.34, use its solution to estimate the \(\mathbf{KL}\) term from the RHS in eq.7 for each \(\mathbf{x}_{\mathbf{v}},\mathbf{y}\), and then average this \(\mathbf{KL}\) term over samples of \(\mathbf{x}_{\mathbf{v}}\) from the data such that \(e(\mathbf{x})=\mathbf{v}\) and samples of \(\mathbf{y}\) from the eval-x model for \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\).

In practice, one does not need train a model for each \(\mathbf{v}\). We describe how to estimate encode-meter with a single model in AppendixC.2. We give the full stripe-x estimation procedure in Algorithm2 in AppendixD.

### Estimating the encoding cost in stripe-x with categorical predictive models

stripe-x consists of the eval-x score and a cost of encoding measured by encode-meter. Define \(\mathcal{V}\) to be the set of possible explanations and let \(\mathcal{V}[j]\) denote the \(j\)th element of \(\mathcal{V}\). The eval-x model \(p_{\gamma}(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})\) is trained to predict the label \(\mathbf{y}\) from subsets \(\mathbf{x}_{\mathbf{v}}\) where \(\mathbf{v}\) is uniformly sampled from \(\mathcal{V}\)[20]. Next is computing the encode-meter\(\phi_{q}(e)\) that is used in the encoding cost term in stripe-x. For each explanation, let \(\mathbf{F}\) be the categorical variable (instead of an indicator \(\mathbf{E}_{\mathbf{v}}\)) that denotes, for each sample, which inputs were selected by the explanation \(e(\mathbf{x})\): \(\mathbf{F}=j\) if \(\mathbf{E}_{\mathcal{V}[j]}=\mathbbm{1}[e(\mathbf{x})=\mathcal{V}[j]]=1\). Let \(q(j)\) be the distribution over \(j\) induced by \(q(e(\mathbf{x}))\). We train a model \(p_{\theta}(\mathbf{F}\mid\mathbf{x}_{\mathbf{v}},\ell,\mathbf{v})\) with a modification of eq.34 that averages over \(\mathbf{v}\sim q(e(\mathbf{x}))\):

\[\arg\max_{\theta}\mathbb{E}_{\mathbf{v}\sim q(e(\mathbf{x}))} \mathbb{E}_{\mathbf{x},\mathbf{y}\sim q(\mathbf{x},\mathbf{y})}\sum_{ \mathcal{V}[j]\in\mathcal{V}}\bigg{(}\mathbbm{1}[e(\mathbf{x})=\mathcal{V}[j]] \big{[}\log p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{\mathbf{v}},\ell=\texttt{ y},\mathbf{v})+\] \[\log p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{\mathbf{v}},\ell= \texttt{null},\mathbf{v})\big{]}\bigg{)}.\] (35)

The variable \(\ell\) takes values in \(\{-1,0,1\}\) where \(0\) and \(1\) correspond to \(\mathbf{y}=0\) and \(\mathbf{y}=1\) respectively and \(-1\) corresponds to the null value. For a flexible enough model \(p_{\theta}\) that achieves the population maximum of eq.35, for any \(\mathbf{v}=\mathcal{V}[j]\in\mathcal{V}\),

\[p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{\mathbf{v}},\ell=\mathbf{ y},\mathbf{v}) =q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}},\mathbf{y}=\mathbf{y}),\] \[p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{\mathbf{v}},\ell=\texttt{ null},\mathbf{v}) =q(\mathbf{E}_{\mathbf{v}}=1\mid\mathbf{x}_{\mathbf{v}}).\]

This fact indicates how one can use the model \(p_{\theta}\) to estimate encode-meter. First, construct the explanation dataset \(D_{e}=\{(\mathbf{y},\mathbf{x}_{e(\mathbf{x})})\}\) from \(D_{t}\). Define \(q_{D_{e}}\) to be the uniform distribution over \(D_{e}\). Define \(\mathcal{E}_{(\mathbf{v},\mathbf{a})}\) as the uniform distribution over \(K\) samples of \(\mathbf{y}\) from the eval-x model:

\[\mathcal{E}_{(\mathbf{v},\mathbf{a})}=\mathbf{U}\left[\{\hat{\mathbf{y}}\}_{k \leq K}\right]\quad\{\text{ where }\hat{\mathbf{y}}^{k}\sim p_{\gamma}(\mathbf{y}\mid\mathbf{x}_{ \mathbf{v}}=\mathbf{a})\}.\]

Then, estimate encode-meter as follows:

\[\hat{\phi}(q,e)= \mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q_{D_{e}}(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{\hat{\mathbf{y}}\sim\mathcal{E}_{(\mathbf{v}, \mathbf{a})}}\left(p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{\mathbf{v}},\ell= \hat{\mathbf{y}},\mathbf{v})\log\frac{p_{\theta}(\mathbf{F}=j\mid\mathbf{x}_{ \mathbf{v}},\ell=\hat{\mathbf{y}},\mathbf{v})}{p_{\theta}(\mathbf{F}=j\mid \mathbf{x}_{\mathbf{v}},\ell=\texttt{null},\mathbf{v})}\right.\] \[+\left.p_{\theta}(\mathbf{F}\neq j\mid\mathbf{x}_{\mathbf{v}}, \ell=\hat{\mathbf{y}},\mathbf{v})\log\frac{p_{\theta}(\mathbf{F}\neq j\mid \mathbf{x}_{\mathbf{v}},\ell=\hat{\mathbf{y}},\mathbf{v})}{p_{\theta}(\mathbf{F} \neq j\mid\mathbf{x}_{\mathbf{v}},\ell=\texttt{null},\mathbf{v})}\right).\]

### Estimating encode-meter with a generative model

When estimating the stripe-x score with procedure above for many different explanations, the maximization in eq.34 repeated for every explanation, which can be computationally expensive.

This motivates a second procedure to estimate encode-meter that avoids having to retrain models for each explanation by using generative model for. Formally, with \(\mathbf{x}_{\mathbf{v}}\) fixed, the conditional mutual information term in eq.5 can be computed as the marginal dependence between \(N\) samples of \(\mathbf{y}\) from and. The model for the former is available from eval-x estimation. Simulating from the later, namely \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}},\mathbf{y})\), is done by sampling from the generative model \(\mathbf{x}\mid\mathbf{x}_{\mathbf{v}},\mathbf{y}\) and then computing the indicator \(\mathbf{E}_{\mathbf{v}}\) as \(\mathbbm{1}[e(\mathbf{x})=\mathbf{v}]\). Mechanically, with an estimator of mutual information from samples (\(\{\mathbf{a}^{i}\}_{i\leq N}\), \(\{\mathbf{b}^{i}\}_{i\leq N}\)) denoted \(\texttt{MI}(\{\mathbf{a}^{i}\},\{\mathbf{b}^{i}\})\) and with samples \(\{\mathbf{a}^{i}\}\) produced conditionally on values \(\mathbf{c}^{i}\) denoted by a subscript of the conditioned value \(\{\mathbf{a}^{i}\}_{\mathbf{c}^{i}}\), one can estimate encode-meter as follows: sample and repeatedly \(N\) times and compute

We give the full procedure in Algorithm1.

``` \(\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\mathbf{x}_{4}, \mathbf{x}_{5}]\sim\mathcal{B}(0.5)^{\otimes 5},\qquad\mathbf{y}=\begin{cases} \mathbf{x}_{1}&\text{w.p. }\rho&\text{else}&1-\mathbf{x}_{1}&\text{if}\ \mathbf{x}_{3}=1,\\ \mathbf{x}_{2}&\text{w.p. }\rho&\text{else}&1-\mathbf{x}_{2}&\text{if}\ \mathbf{x}_{3}=0.\end{cases}\) ```

**Algorithm 1**\(\mathbf{x}\)

The data-generating processes from the experiments.Let \(\mathcal{N}\) be the standard normal distribution and let \(\mathcal{B}(\alpha)\) be the Bernoulli distribution with \(1\) occurring with probability \(\alpha\). With \(\rho=0.9\), the discrete dgp is:

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\mathbf{x}_{4}, \mathbf{x}_{5}]\sim\mathcal{B}(0.5)^{\otimes 5},\qquad\mathbf{y}=\begin{cases} \mathbf{x}_{1}&\text{w.p. }\rho&\text{else}&1-\mathbf{x}_{1}&\text{if}\ \mathbf{x}_{3}=1,\\ \mathbf{x}_{2}&\text{w.p. }\rho&\text{else}&1-\mathbf{x}_{2}&\text{if}\ \mathbf{x}_{3}=0.\end{cases}\] (36)

The hybrid dgp is as follows: with \(\gamma=5\) and \(\sigma(x)=\frac{1}{1+\exp(-x)}\) as the sigmoid function

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{4},\mathbf{x}_{5}]\sim \mathcal{N}(0.5)^{\otimes 4},\mathbf{x}_{3}\sim\mathcal{B}(0.5),\qquad\rho= \begin{cases}\sigma(\gamma\,\mathbf{x}_{1})&\text{if}\ \mathbf{x}_{3}=1,\\ \sigma(\gamma\,\mathbf{x}_{2})&\text{if}\ \mathbf{x}_{3}=0,\end{cases}\] (37)

Computing accuracy and KL to show that posi, pred, marg are encoding.For each encoding type, we build two decision trees from \(1000\) samples from eq.36: the first decision tree learns \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}})\) and the second learns \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=b)\) for \(b\in\{0,1\}\). We set the maximum depth to be \(6\). Trees of this depth learn any function of \(6\) binary digits; \(\mathbf{x}\) with \(\mathbf{E}_{\mathbf{v}}\) as an additional column amounts to \(6\) binary digits. These decision trees are used to compute the accuracy of predicting \(\mathbf{E}_{\mathbf{v}}\) with \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}})\) and the **KL** between \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\) and \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\). Within a set \(\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}\) that is all \(\mathbf{x}\) that have one of the possible selections \(\mathbf{v}\), Table6 report the accuracy of predicting \(\mathbf{E}_{\mathbf{v}}\) with \(q(\mathbf{E}_{\mathbf{v}}\mid\mathbf{x}_{\mathbf{v}})\) and the **KL** between \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=1)\) and \(q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{E}_{\mathbf{v}}=0)\), averaged only over samples in \(\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}\).

Eval-x.To estimate eval-x for the dgps in eq.36 and eq.37, we compute conditionals \(q(\mathbf{y}=1\mid\mathbf{x}_{\mathbf{v}})\) via Monte Carlo approximation. Due to the different coordinates of \(\mathbf{x}\) being independent, one can compute \(q(\mathbf{y}=1\mid\mathbf{x}_{\mathbf{v}})\) as a marginal expectation over the inputs except those in \(\mathbf{v}\):

\[q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}})=\mathbb{E}_{q(\mathbf{x}_{\mathbf{v}}^{ c}\mid\mathbf{x}_{\mathbf{v}})}q(\mathbf{y}\mid\mathbf{x}_{\mathbf{v}},\mathbf{x}_{ \mathbf{v}}^{c})=\mathbb{E}_{q(\mathbf{x}_{\mathbf{v}}^{c})}q(\mathbf{y}\mid \mathbf{x}).\]

We Monte Carlo estimate the RHS of this equation over \(500\) resamples of \(\mathbf{x}_{\mathbf{v}}^{c}\). We take \(5000\) samples from each dgp to estimate eval-x scores with respect to \(q(\mathbf{y},\mathbf{x})\). In AppendixC.5 we also show experiment results where we use the eval-x accuracy and AUROC as the score instead.

\begin{table}
\begin{tabular}{c c c} \hline \hline Encoding & Acc. \(\mathbf{E}_{\mathbf{v}}\) (\(\uparrow\)) & \(\mathbf{KL}\) (\(\downarrow\)) \\ \hline posi & \(0.61\) & \(0.88\) \\ pred & \(0.51\) & \(0.18\) \\ marg & \(0.51\) & \(0.20\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Position-based, prediction-based, and marginal explanation schemes are all encoding. For samples in the set \(\{\mathbf{x}:e(\mathbf{x})=\mathbf{v}\}\) for one of the selections \(\mathbf{v}\) that \(e\) produces, accuracy \(<1\) and the **KL** being non-zero means these explanations are all encoding per Lemma1.

[MISSING_PAGE_FAIL:38]

We follow the procedure in Appendix C.2 to estimate stripe-x. The eval-x model is a pre-trained 18-layer residual network. The model \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\ell,\mathbf{v})\) used in computing the encode-meter term in stripe-x (eq. (6)) are 34-layer Residual neural networks. The eval-x model is trained for \(100\) epochs with a batch size of \(100\) with the Adam optimizer, with the learning rate and weight decay parameters set to \(10^{-3}\) and \(0\) respectively. The \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\ell,\mathbf{v})\) model is trained for \(50\) epochs with a batch size of \(200\) with the Adam optimizer, with the learning rate and weight decay parameters set to \(5\times 10^{-5}\) and \(1\) respectively. The \(p_{\theta}\) model sees variable \(\ell\) through an entire extra channel where all the pixels take the value \(\ell\). We used validation loss as the metric to early stop. The eval-x and stripe-x scores are computed on the test dataset. The cats vs. dogs experiment were done on an A100 GPU where the whole training and evaluation ran in less than 20 minutes.

Remark on the gap between fresh and eval-x scores for the optimal explanation.As the optimal explanation selects features sufficient to produce the label, meaning \(\mathbf{y\_\_\_\_}\|_{\mathbf{x}}\mid\mathbf{x_{v}}\) or \(\mathbf{y\_\_\_\_}\|_{\mathbf{x}}\mid\mathbf{val}(\mathbf{x_{c(x)}})\), fresh and eval-x log-likelihoods should be the same as predicting from the full feature set. One potential reason there is a gap between the two scores in table 3 is that the fresh and eval-x scores are computed with ResNet18 models that solve prediction problems of different levels of difficulty On one hand, fresh is computed with a model trained for a single prediction task: predict \(\mathbf{y}\) from \(\mathtt{val}(\mathbf{x_{c(x)}})\). On the other hand, eval-x is computed with a model trained for a more complicated task: for a range possible \(\mathbf{v}\), predict \(\mathbf{y}\) from \(\mathbf{x_{v}}\). Using large models with appropriate regularization, such as weight decay, should mitigate the gap in scores.

### llm experiment details

We generate \(10,000\) reviews of the following type: with ADJ1 and ADJ2 as adjectives, the review is

* 'My day was <ADJ1> and the movie was <ADJ2>. that is it' or
* 'My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives'.

The second sentence in the review acts as a "control flow" input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompted Llama 3 to predict the sentiment and select words relevant to predicting the sentiment. In Appendix C.8, we give the prompts we used to make Llama 3 produce explanations from. For this problem, the inputs \(\mathbf{x}\) are the reviews and Llama 3 produces explanations \(e(\mathbf{x})\) that select a subset of words in the review. The summaries and explanations were generated for all \(10,000\) samples but to estimate stripe-x, we only used data from the \(5\) most common explanations (we restricted to inputs whose explanations \(\mathbf{v}\) had high \(q(e(\mathbf{x})=\mathbf{v})\)). This resulted in a dataset of size \(8136\), which we split into a training, validation, and test datasets of sizes \(6102,1017,\) and \(1017\) respectively.

Both the eval-x model and the model for \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\mathbf{v},\ell)\) (see Appendix C.2) used in estimating the encode-meter term in stripe-x were finetuned GPT-2 models. For the eval-x model, we used the AdamW optimizer with a batch size of \(100\) and trained for \(50\) epochs with the learning rate set to \(5e-5\), weight decay set to \(0\), and a Cosine learning rate scheduler with the number of cycles set to \(1\). For the \(p_{\theta}\) model used in estimating encode-meter, we used the AdamW optimizer with a batch size of \(50\) and trained for \(25\) epochs with the learning rate set to \(5e-5\), weight decay set to \(0\), and a Cosine learning rate scheduler with number of cycles set to \(1\). The \(p_{\theta}\) model sees variable \(\ell\) through the following word added to the input sequence of words: positive if \(\ell=\mathbf{y}=1\), negative if \(\ell=\mathbf{y}=0\), and nothing if \(\ell=\mathtt{null}\). We used validation loss to early stop. We follow the procedure in Appendix C.2 to compute encode-meter with \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\mathbf{v},\ell)\) on the test data with the averaging over \(\mathbf{y}\mid\mathbf{x_{v}}\) estimated using a \(5\) samples per value of \(\mathbf{x_{v}}\). All training and inference for this experiment was done on an A100. The explanation step and the estimation for both parts of stripe-x together took under 2 hours. The llm-generated explanations achieves an eval-x score of \(-0.497\) and an encode-meter value was \(0.114\).

### Prompts used to predict sentiment and produce explanation

In Figure 7, we provide the prompt we used to predict the sentiment from a review and generate an explanation for that prediction.

Figure 7: Llama 3 prompt used to predict sentiment and generate an explanation for that prediction.

System: You are a helpful and honest assistant. Please, respond concisely and truthfully.

You are asked to summarize movie reviews of the form "first sentence. second sentence". The following are examples along with the reasoning.

Consider 'My day was moving and the movie was overblown. that is it.' The second sentence means the second adjective 'overblown' describes the movie. Due to this description, the sentiment is negative.

Consider 'My day was moving and the movie was overblown. oh wait, reverse the adjectives.' The second sentence means the first adjective'moving' describes the movie. Due to this description, the sentiment is positive.

These are all examples.

user: What is the sentiment about the movie in this review '<REVIEW>'?

Think step-by-step about this latest review. If the second sentence instructs it, switch the adjectives and then based on the new descriptor of the movie, answer either 'positive' or 'negative'.

Explain why you chose those this sentiment by selecting as few words as possible from the review. Include all the words that you looked at.

Use this helpful format: "the sentiment is <positive/negative> and the explanation is <words,...>. END. "

## Appendix D Algorithms

Algorithm 1 describes an alternate way to estimate the encode-meter component of stripe-x with a conditional generative model. Algorithm 2 describes the predictive version of stripe-x estimation, which we used in our experiments.

**Algorithm 1:**encode-meter, generative version.

```
0: Training data \(D\sim q(\mathbf{y},\mathbf{x})\) and test data \(D_{t}\sim q(\mathbf{y},\mathbf{x})\), explanation function \(e(\mathbf{x})\),  penalty weight \(\lambda\). eval-x model \(p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}})\). Conditional generative model \(p_{\theta}(\mathbf{x}\mid\mathbf{x_{v}},\mathbf{y})\) and mutual information estimator that takes two sets as arguments \(\texttt{MI}[\{c_{i}\},\{d_{i}\}]\); Result: Return estimate of stripe-x :
1. Define \(\mathbf{J}_{(\mathbf{v},\mathbf{a})}\) as the set of \(K\) random samples of \(\mathbf{y}\) from the eval-x model: \[\mathbf{J}_{(\mathbf{v},\mathbf{a})}=\{\hat{\mathbf{y}}^{k}\}_{k\leq K}\quad \{\text{ where }\hat{\mathbf{y}}^{k}\sim p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}}= \mathbf{a})\}\] Define \(\mathbf{L}_{(\mathbf{v},\mathbf{a},\mathbf{J}_{(\mathbf{v},\mathbf{a})})}\) as the set of \(K\) random samples of \(\mathbf{x}\) from \(p_{\theta}\) conditioned on \(\mathbf{a}\) and \(\hat{\mathbf{y}}\): \[\mathbf{L}_{(\mathbf{v},\mathbf{a},\mathbf{J}_{(\mathbf{v},\mathbf{a})})}=\{ \mathbbm{1}[e(\hat{\mathbf{x}}^{k})=\mathbf{v}]\}_{k\leq K}\quad\{\text{ where }\hat{\mathbf{x}}^{k}\sim p_{\theta}(\mathbf{x}\mid\mathbf{x_{v}}= \mathbf{a},\mathbf{y}=\hat{\mathbf{y}}^{k})\}\] Construct the explanation dataset \(D_{e}=\{(\mathbf{x}_{e(\mathbf{x})}=(e(\mathbf{x}),\mathbf{a}))\}\) from \(D_{t}\). Define \(q_{D_{e}}\) to be the uniform distribution over \(D_{e}\). Compute the following averaging of estimated mutual information between, \(\mathbf{J},\mathbf{L}\) \[\hat{\phi}_{q}(e)=\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q_{D_{e}}(\mathbf{x }_{e(\mathbf{x})})}\texttt{MI}\left[\mathbf{J}_{(\mathbf{v},\mathbf{a})}, \mathbf{L}_{(\mathbf{v},\mathbf{a},\mathbf{J}_{(\mathbf{v},\mathbf{a})})}\right]\] Return \(\hat{\phi}(q,e)\) as the encode-meter estimate. ```

**Algorithm 2**encode-meter, generative version.

``` Input: Training data \(D\sim q(\mathbf{y},\mathbf{x})\) and test data \(D_{t}\sim q(\mathbf{y},\mathbf{x})\), explanation function \(e(\mathbf{x})\), penalty weight \(\lambda\). Specifications for the models \(p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}})\) and \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\ell)\). Result: Return estimate of stripe-x :
1 Define \(q_{D}\) to be the uniform distribution over \(D\) Construct the explanation dataset \(D_{e}=\{(\mathbf{y},\mathbf{x}_{e(\mathbf{x})})\}\) from \(D_{t}\) Define \(q_{D_{e}}\) to be the uniform distribution over \(D_{e}\).
2 Estimate eval-x() Solve the following minimization problem to learn \(p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}})\) \[\operatorname*{arg\,max}_{\gamma}\mathbb{E}_{\mathbf{v}\sim q_{D}(e(\mathbf{x }))}\mathbb{E}_{\mathbf{x},\mathbf{y}\sim q_{D}(\mathbf{x},\mathbf{y})}\big{[} \log p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}})\big{]}\] Output:\(p_{\gamma}\) Estimate encode-meter() Construct the set of possible selections \(\mathcal{V}=\{\mathbf{v}:q(e(\mathbf{x})=\mathbf{v})>0\}\) Construct data of the form \((\mathbf{x},\mathbf{F})\) where \(\mathbf{F}=j\) if \(\mathbb{E}_{\mathcal{V}[j]}=1\). Fit the model \(p_{\theta}(\mathbf{F}\mid\mathbf{x_{v}},\ell)\) via the following log-likelihood maximization: \[\operatorname*{arg\,max}_{\theta}\mathbb{E}_{\mathbf{v}\sim q_{D}(e( \mathbf{x}))}\mathbb{E}_{\mathbf{x},\mathbf{y}\sim q_{D}(\mathbf{x},\mathbf{y} )}\sum_{\mathcal{V}[j]\in\mathcal{V}}\bigg{(}\mathbbm{1}[e(\mathbf{x})= \mathcal{V}[j]]\big{[} \log p_{\theta}(\mathbf{F}=j\mid\mathbf{x_{v}},\ell=\mathbf{y}, \mathbf{v})\] \[+\log p_{\theta}(\mathbf{F}=j\mid\mathbf{x_{v}},\ell=\texttt{ null},\mathbf{v})\big{]}\bigg{)}\] Define \(\mathcal{E}_{(\mathbf{v},\mathbf{a})}\) as the uniform distribution over \(K\) samples of \(\mathbf{y}\) from the eval-x model: \[\mathcal{E}_{(\mathbf{v},\mathbf{a})}=\mathbf{U}\left[\{\hat{\mathbf{y}}\}_{ k\leq K}\right]\quad\{\text{ where }\hat{\mathbf{y}}^{k}\sim p_{\gamma}(\mathbf{y}\mid\mathbf{x_{v}}=\mathbf{a})\}\] Output: The following nested expectation over \(q_{D_{e}}\) and \(\mathcal{E}(\cdot)\): \[\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q_{D_{e}}(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{\hat{\mathbf{y}}\sim\mathcal{E}_{(\mathbf{v}, \mathbf{a})}}\left(p_{\theta}(\mathbf{F}=j\mid\mathbf{x_{v}},\ell=\hat{ \mathbf{y}},\mathbf{v})\log\frac{p_{\theta}(\mathbf{F}=j\mid\mathbf{x_{v}}, \ell=\hat{\mathbf{y}},\mathbf{v})}{p_{\theta}(\mathbf{F}=j\mid\mathbf{x_{v}}, \ell=\texttt{null},\mathbf{v})}\right.\] \[+\left.p_{\theta}(\mathbf{F}\neq j\mid\mathbf{x_{v}},\ell=\hat{ \mathbf{y}},\mathbf{v})\log\frac{p_{\theta}(\mathbf{F}\neq j\mid\mathbf{x_{v}}, \ell=\hat{\mathbf{y}},\mathbf{v})}{p_{\theta}(\mathbf{F}\neq j\mid\mathbf{x_{v }},\ell=\texttt{null},\mathbf{v})}\right)\] Learn the eval-x model \(p_{\gamma}\leftarrow\)eval-x(). Estimate encode-meter as the \(\hat{\phi}_{q}(e)\leftarrow\)encode-meter(). Return the following as the stripe-x estimate: \[\mathbb{E}_{(\mathbf{v},\mathbf{a})\sim q_{D_{e}}(\mathbf{x}_{e( \mathbf{x})})}\mathbb{E}_{\mathbf{y}\sim q_{D_{e}}(\mathbf{y}\mid\mathbf{x_{ c(\mathbf{x})}}=(\mathbf{v},\mathbf{a}))}\left[\log p_{\gamma}\left(\mathbf{y}= \mathbf{y}\mid\mathbf{x_{v}}=\mathbf{a}\right)\right]-\lambda\hat{\phi}_{q}(e)\] ```

**Algorithm 2**stripe-x, predictive version.