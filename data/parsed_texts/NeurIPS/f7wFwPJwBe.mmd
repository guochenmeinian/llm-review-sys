# Learning Re-sampling Methods with Parameter Attribution for Image Super-resolution

 Xiaotong Luo1, Yuan Xie2, Yanyun Qu1\({}^{*}\)

1School of Informatics, Xiamen University, Fujian, China

2School of Computer Science and Technology, East China Normal University, Shanghai, China

2Chongqing Institute of East China Normal University, Chongqing, China

xiaotluo@stu.xmu.edu.cn, yxie@cs.ecnu.edu.cn, yyqu@xmu.edu.cn

Corresponding author

###### Abstract

Single image super-resolution (SISR) has made a significant breakthrough benefiting from the prevalent rise of deep neural networks and large-scale training samples. The mainstream deep SR models primarily focus on network architecture design as well as optimization schemes, while few pay attention to the training data. In fact, most of the existing SR methods train the model on uniformly sampled patch pairs from the whole image. However, the uneven image content makes the training data present an unbalanced distribution, i.e., the easily reconstructed region (smooth) occupies the majority of the data, while the hard reconstructed region (edge or texture) has rarely few samples. Based on this phenomenon, we consider rethinking the current paradigm of merely using uniform data sampling way for training SR models. In this paper, we propose a simple yet effective Bi-Sampling Parameter Attribution (BSPA) method for accurate image SR. Specifically, the bi-sampling consists of uniform sampling and inverse sampling, which is introduced to reconcile the unbalanced inherent data bias. The former aims to keep the intrinsic data distribution, and the latter is designed to enhance the feature extraction ability of the model on the hard samples. Moreover, integrated gradient is introduced to attribute the contribution of each parameter in the alternate models trained by both sampling data so as to filter the trivial parameters for further dynamic refinement. By progressively decoupling the allocation of parameters, the SR model can learn a more compact representation. Extensive experiments on publicly available datasets demonstrate that our proposal can effectively boost the performance of baseline methods from the data re-sampling view.

## 1 Introduction

Single image super-resolution (SISR) aims to reconstruct a high-resolution (HR) image from its downsampled low-resolution (LR) counterpart. As a classical ill-posed problem, multiple HR images can be recovered from the same LR image. With the boom of neural networks, numerous excellent deep SR methods [9; 23; 25; 5] have been proposed to chase for a more accurate solution. It is well-known that the success of deep learning primarily attributes to three significant factors, including large-scale data [1], network structures [10; 52; 23], and optimization strategies [16; 31; 26]. Most of the existing SR models mainly focus on the delicate structure design or complex regularization constraints, while neglecting the in-depth analysis of the intrinsic training data.

Due to resource constraints, the mainstream deep SR methods mostly train the models with uniformly sampled LR-HR patch pairs rather than the whole images. However, they ignore theunderlying fact that the content of patches is usually unevenly distributed in an image. As presented in Fig. 1, we measure the patch content by mean square error (MSE) and count it for all the cropped sub-images on the commonly used DIV2K training dataset. It reveals a phenomenon of long-tailed distribution, i.e., the flat region occupies the majority of the training samples, while the sharp region with abundant texture details only holds a very small percentage. The imbalanced training data would undoubtedly impair the reconstruction accuracy, especially for the hard areas in the tail. Therefore, we consider enhancing the model representation from the data sampling perspective.

In this paper, we propose a simple yet effective Bi-Sampling Parameter Attribution (BSPA) method for image SR, which aims to obtain the compact feature representation by explicitly increasing the sampling proportion of the tail hard patch pairs.

Specifically, the bi-sampling consists of uniform sampling and inverse sampling to make up for the unbalanced training data bias. As the most common way of data sampling, uniform sampling helps to keep the original data distributions, while inverse sampling aims to allocate a higher probability to the fewer patches sampled from the tail data. The SR model is alternately trained with the two kinds of sampling data. However, the SR models treat all the parameters equally and perform gradient updating to each parameter, which ignores the parameter redundancy. Therefore, it is necessary to obtain a compact model according to the parameter contribution to the output. Considering that the weight values would not change drastically with the small learning rate, it is difficult to determine the impact of different weights. Inspired by [35], we introduce the integrated gradient (IG) attribution method to measure the parameter importance. The IG is calculated between the previous model trained with uniform sampling data and the next model trained with inverse sampling data. Based on the importance ranking, the significant parameters do not perform gradient updating, and the remaining trivial parameters are encouraged to contribute more by further optimization. Extensive experiments have demonstrated that our proposal can improve the SR performance of baseline methods. To the best of our knowledge, this is the first attempt to investigate image SR as a long-tail data distribution problem.

In summary, the main contributions of this paper are: **1)** We definitely find the data unbalanced problem in the training samples caused by uniform sampling for image SR. **2)** A bi-sampling paradigm is designed to remedy the unbalanced data bias, including uniform sampling and inverse sampling, respectively. **3)** To obtain a compact representation, an integrated gradient based attribution method is introduced for measuring the parameter importance so that the significant parameters are progressively selected to stop gradient updating for better distilling the remaining trivial parameters.

## 2 Related Work

### Deep Image SR Models

Nowadays, numerous SR models have been proposed with remarkable performance. The pioneering work, SRCNN [9], proposed a three-layer network for image SR. Then, FSRCNN [10] and ESPCN [32] introduced deconvolution and sub-pixel upsampling for mapping LR features to HR images, which could largely reduce computation complexity. VDSR [17] introduced residual learning to extract missing high-frequency information. DRRN [18] and DRCN [18] adopted recursive neural networks to reduce the model parameters while achieving excellent performance. RCAN [52] adopted a residual-in-residual network design and embedded the channel attention in each residual block to build an extremely deep network for image SR. DIN [21] designed a deep interleaved network to

Figure 1: The illustration of long-tailed data distribution with uniform sampling in image SR. Following [19], we first crop the commonly used training dataset, e.g, DIV2K [1], into multiple sub-images, and adopt the mean square error (MSE) between the SR output by MSRResNet [40] and the HR image to measure the reconstruction difficulty of an LR sub-image. It shows that the data distribution of the training samples is uneven, which presents a long-tailed distribution.

combine the information at different states. Wang et. al. [37] learned a scale-arbitrary image SR network from scale-specific networks. LatticeNet [27] proposed a lattice block to adaptively combine pair-wise residual blocks. All these SR models have been trained on uniformly sampled LR-HR patch pairs, which ignore the unbalanced data distribution in the training samples.

Recently, several region-aware SR methods have been proposed to distinguish different image regions in an image and aim to assign more computation resources on more challenging pixels adaptively. RL-Restore [45] and Path-Restore [46] decomposed the image into different sub-images and then estimated an appropriate processing path for each sub-image by reinforcement learning. ClassSR [19] proposed a new classification method to determine the processing of different image regions and then reconstructed the sub-image by models of different sizes. Similar to ClassSR, FADN [43] proposed a framework for processing the different image regions divided by the corresponding information in the frequency domain with different computation burdens. APE [38] introduced an adaptive patch exiting strategy to super-resolve patches with different difficulties for achieving a flexible tradeoff between performance and efficiency. The above content-related SR methods mainly focus on the architecture design or optimization schemes. Except for the sub-image decomposition methods, UDN [31] proposed a new adaptive weighted loss for image SR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty.

In this paper, we mainly rethink the unbalanced distribution for image SR from the view of data sampling. SamplingAug [39] is a sampling-based data augmentation method for image SR. It only sampled the \(p\%\) most informative patches for model training, while the rest is ignored. DDA [49] designed a difficulty-aware data sampling strategy, which controls the sampling probability of each class by the relative loss. Different from these works, we propose a bi-sampling strategy and the model is alternatively updated with uniformly and inversely sampled image data. All patches are effectively utilized for SR training to help the model fully mine the image information.

### Long-tail Image Classification

Due to the imbalanced data distribution caused by the difficult data acquisition in actual applications, long-tailed image recognition has attracted widespread attention [41; 11; 50]. The existing long-tail studies can be roughly divided into three types: data re-sampling, loss re-weighting, and transfer learning. The data re-sampling based methods [28; 12] aim to re-sample the training dataset for achieving a more balanced distribution. Besides, the loss re-weighting based methods [13; 33] are proposed to allocate different weights to each training samples for loss optimization. Recently, the transfer learning based methods [44; 24] have sprung up to transfer the informative features learned from the head classes with abundant training samples to the under-represented tail classes. Here, we mainly explore the data re-sampling method for image SR.

As the widely adopted data re-balancing strategy for solving the long-tailed problem, re-sampling methods aim to sample the data to generate an evenly distributed dataset, which can be classified as: over-sampling for minority classes and under-sampling for dominant classes. However, repeated sampling for tailed samples may lead to the over-fitting problem on minority classes, while discarding the head samples will undoubtedly damage the generalization ability of deep models. Therefore, we integrate the two data sampling ways together for remedying the dataset bias in image SR task.

### Attribution Analysis

Attribution analysis is usually utilized for model interpretation on the high-level image classification task, which aims to attribute the influence of each pixel in the input on prediction accuracy. The existing attribution analysis methods can be classified as: the gradient-based methods [34; 35], propagation-based methods [30; 4] and occlusion-based methods [47; 22]. Here, we primarily focus on the gradient-based methods, which utilize the gradient of the output against the input as a metric and accumulate the integrated gradients (IG) at all points along a path from the baseline input to the target input. IntInf [20] and neuron conductance [8] further extend feature-important IG to neurons. Unlike the classical attribution methods [35; 36], which are used to attribute the influence of the input on the classification model by attributing the contribution of each pixel of the input image to the final output label, FAIG [42] first introduced IG to attribute network functions to the filters in the blind SR model and successfully find discriminative filters that closely relate to the image degradation removal. In this paper, we mainly adopt the integrated gradient method to help locate the position of important weights in SR models trained with uniform sampling data and inverse sampling data.

## 3 Proposed Method

### Method Overview

Considering the intrinsic dataset-specific bias shown in Fig. 1, we adopt two kinds of sampling ways (bi-sampling) for capturing the training data: uniform sampling and inverse sampling, which are combined to re-balance the distribution of training data. As shown in Fig 2, the proposed bi-sampling parameter attribution (BSPA) framework includes two steps, which are alternately performed. In Step 1, the SR model is trained with the data from uniform sampling to obtain a trivial solution toward normal data distribution (baseline model). In Step 2, the SR model is fine-tuned with the data from inverse sampling for generating a biased solution towards processing well on the tail edge and texture regions (target model). After training with inverse sampling data, the integrated gradient is calculated between the baseline model and the target model, which is utilized to obtain the parameter importance so as to control the relevant weight updating.

### Bi-sampling Strategy

**Uniform Sampling.** Uniform sampling refers to cropping the patch samples from the whole image in the training dataset randomly, which is the most common way of data sampling and widely used in image SR task [21]. To be specific, the input data comes from a uniform sampler, where each sample in the training dataset is sampled with equal probability in a training epoch. The probability \(P_{i}^{us}\) that a sample \(i\) is chosen from the whole training dataset can be formulated as:

\[P_{i}^{us}=\frac{1}{N_{T}},\] (1)

where \(N_{T}\) is the total number of training samples.

**Inverse Sampling.** Inverse sampling aims to allocate a higher probability to capture the tail hard data with few samples. Unlike long-tailed image classification, which has a definite number of classes, image SR is a regression task. We adopt MSE to measure the reconstruction difficulties, which is a continuous value and each value corresponds to a sample as shown in Fig. 1. Considering the MSE value is continuous and divergently distributed, it needs to collect more samples within a certain range to calculate the proportion. Therefore, we first split data patches into multiple groups to better calculate the sampling probability.

_Data classification._ Specifically, the number of classes is predefined as \(K\). Analogy to the long-tailed CIFAR, which is created by reducing the number of training samples per class via an exponential function [51], then the number of samples for each class can be calculated as:

\[N_{k}=N\times\mu^{k},\] (2)

where \(k\in(1,K)\) is the class index, \(N\) is the total number of training sub-images, and \(\mu\in(0,1)\) is an attenuation factor. Therefore, a long-tailed version of DIV2K training dataset can be created by setting different MSE threshold values.

Figure 2: The framework of our proposed bi-sampling parameter attribution method for image SR.

Moreover, the imbalance factor of a long-tailed dataset is defined as the number of training samples in the largest class (\(N_{1}\)) divided by that of the smallest (\(N_{K}\)), which usually ranges from \(10\) to \(200\) in long-tail research and is set as \(10\) in our experimental setting. Finally, the MSE threshold values used for classifying the sub-images can be obtained by:

\[t_{k}=t[N_{1:k}],\] (3)

where \(t_{k}\) is the MSE threshold sorted by ascending. \(N_{1:k}\) refers to the total number of sample from \(N_{1}\) to \(N_{k}\). Therefore, we can obtain the long-tailed version of DIV2K training dataset. It is noted that the test dataset remains unchanged and the SR model is still tested on the whole image.

_Sampling procedure._ Following [53], the sampling possibility of each class for inverse sampling \(P_{k}^{is}\) is inversely proportional to the class sample capacity, i.e., the more samples in a class, the small sampling possibility that class has. Therefore, the procedures of inverse sampling are as follows: **1)**\(P_{k}^{is}\) for the class \(k\) can be calculated according to the number of samples as:

\[P_{k}^{is}=\frac{\frac{1}{N_{k}}}{\sum_{j=1}^{K}\frac{1}{N_{j}}}.\] (4)

**2)** sample a class \(k\) according to \(P_{k}^{is}\) randomly; **3)** select a sample from class \(k\) randomly. The mini-batch inverse sampling data can be obtained by repeating the above operations.

### Integrated Gradient for Parameter Attribution

**Parameter Importance Formulation.** The uniform sampling data is adopted to maintain the inherent data distribution. The inverse sampling data is used to train the SR model for providing an elegant solution on edge and texture areas. In this paper, we adopt a hybrid training mechanism with uniform and inverse sampling data, which are iteratively input into the SR model.

Since the SR model is trained on uniform sampling and inverse sampling iteratively, there exist different preferences in the SR reconstruction regions. Then, it is meaningful to calculate the parameter importance for obtaining a compact model. Assume the parameter sets of the SR model \(F(\cdot)\) trained with uniform sampling data and inverse sampling data are denoted as \(\{\theta_{i}^{us}:1\leq i\leq L\}\) and \(\left\{\theta_{i}^{is}:1\leq i\leq L\right\}\), where \(\theta_{i}^{us},\theta_{i}^{is}\in\mathbb{R}^{c_{o}\times c_{i}\times w\times h}\) are the weight matrixs at the \(i\)-th layer, \(L\) is the number of convolutional layers, \(c_{o}\) and \(c_{i}\) are the number of output and input channels, \(w\) and \(h\) are the kernel sizes. The two sampling LR images are denoted as \(X^{us},X^{is}\), and the corresponding HR images are \(Y^{us},Y^{is}\), respectively. Then, for a uniformly sampled patch pair \((x^{us},y^{us})\), the variation of network performance on the baseline model and the target model can be measured by:

\[\Delta F(\theta^{is},\theta^{us};x^{us},y^{us})=\left|l(F(x^{us};\theta^{is}), y^{us})-l(F(x^{us};\theta^{us}),y^{us})\right|,\] (5)

where \(l\) means the loss function used for training the SR network. For simplicity, we denote \(l(F(x;\theta),y)\) as \(l(\theta;x,y)\).

Take the element of parameter \(\theta_{ik}^{us}\) (\(i\)-th layer) for example, the change of parameter \(\theta_{ik}^{us}\) when the model is trained on the inverse sampling data is represented as \(\delta\theta_{ik}^{us}\), i.e.,

\[\delta\theta_{ik}^{us}=\theta_{ik}^{is}-\theta_{ik}^{us},\] (6)

To further measure \(\Delta F(\theta^{is},\theta^{us};x^{us},y^{us})\), Cauchy mean value theorem [7] demonstates that when a function \(f\) is differentiable at point \(x_{0}\), it always stands up in the neighborhood \(U(\cdot)\) of the point \(x_{0}\):

\[f(x)=f(x_{0})+f^{\prime}(x_{0})(x-x_{0})+o(x-x_{0}),\] (7)

Since \(o(x-x_{0})\) is an infinitesimal, it can be derived that \(f(x)\approx f(x_{0})+f^{\prime}(x_{0})(x-x_{0})\). This is one of the formulas that is often used in local linearization of a function. Geometrically, it adopts the tangent line to approximate the curve in a small neighborhood.

Then, Eq. (5) can be further formulated as:

\[l\left(\theta_{ik}^{us}+\delta\theta_{ik}^{us};x^{us},y^{us}\right)-l\left( \theta_{ik}^{us};x^{us},y^{us}\right)\approx\left(\nabla_{\theta_{ik}^{us}}l \right)^{T}\cdot\delta\theta_{ik}^{us}.\] (8)

**Integrated Gradient.** The above formula means that we can calculate the parameter importance by the value change and its gradient, which both consider the influence of network alteration on the model outputs. Here, we apply the integrated gradients (IG) to calculate the parameter importance, which was originally an attribution method to attribute the prediction of a deep network to its input features by accumulating the gradient of input changes. Inspired by FAIG [42], which utilizes paths in the parameter space for attributing network functional alterations to filter changes, we adopt IG to obtain the parameter importance so as to help us discover the location of significant parameters in the sampling iteration.

The integrated path is calculated from the model trained with uniform sampling data to the model trained with inverse sampling data. Let \(\gamma(\alpha),\alpha\in[0,1]\) denote a continuous path between the baseline model (\(\theta^{\prime}\)) and the target model (\(\theta\)) with \(\gamma(0)=\theta^{\prime},\gamma(1)=\theta\). Then, the integrated path can be represented as:

\[\gamma(\alpha)=\theta^{\prime}+\alpha(\theta-\theta^{\prime}).\] (9)

In addition, the \(L_{1}\) loss is utilized to train the SR model, while the gradient of each parameter can be obtained according to the back-propagation algorithm. After defining the baseline and target model, integrated path and the loss function, the changes of network functions to each parameter with integrated gradient for an input image \(x\) can be attributed as follows:

\[\begin{split} IG(\theta,x)&=l(\gamma(1),x)-l( \gamma(0),x)\\ &=\sum_{i}\int_{\alpha=0}^{1}\frac{\partial l(\gamma(\alpha),x)}{ \partial\gamma(\alpha)_{i}}\times\frac{\partial\gamma(\alpha)_{i}}{\partial \alpha}\mathrm{d}\alpha\\ &\approx\frac{1}{M}[\theta-\theta^{{}^{\prime}}]\sum_{s=0}^{M-1} \left[\frac{\partial l(\gamma(\alpha),x)}{\partial\gamma(\alpha)}|_{\gamma( \alpha)=\theta^{{}^{\prime}}+\alpha(\theta-\theta^{\prime}),\alpha=\frac{s}{ M}}\right].\end{split}\] (10)

**Parameter Refinement.** After calculating the integrated gradient, the position index of parameter importance can be located. The alternate training scheme is adopted for the bi-sampling data. To obtain a more compact representation, we expect the trivial parameters can also learn more effective information so we strive to distill them with other parameters fixed. Therefore, a specific proportion of significant parameters can be selected to keep unchanged and only perform the gradient updating on the remaining trivial parameters for further refinement. Especially, the selective ratio is set as:

\[p=\beta*(e/T),\] (11)

where \(e\) is the \(e\)-th training epoch, and \(T\) is the total training epochs. \(\beta\) is a scaling factor. The detailed training procedure of our proposal is given in Algorithm 1.

```
0: the SR model \(F(\cdot)\) with the randomly initialized weights: \(\theta_{0}\); the uniform sampling and inverse sampling datasets: \(\bm{D^{us}}\), \(\bm{D^{is}}\); the total epochs: \(T\); the interval for training \(F(\cdot)\) with inverse sampling: \(E\); the remaining ratio for parameters without gradient updating: \(p\).
0: the compact SR model \(F(\cdot)\) with parameter \(\theta\).
1:whileepoch \(\leq\) Tdo
2: Sample the LR-HR patch pairs \((x_{lr}^{us},y_{hr}^{us})\) from \(\bm{D^{us}}\);
3: Update \(F(\cdot)\) with \(\theta^{us}\) on the uniformly sampled data;
4:ifepoch \(\%\)\(E\) = 0 then
5: Sample the LR-HR patch pairs \((x_{lr}^{is},y_{hr}^{is})\) from \(\bm{D^{is}}\);
6: Update \(F(\cdot)\) with \(\theta^{is}\) on the inversely sampled data;
7: Calculate the integrated gradient between the baseline model (\(\theta^{us}\)) and the target model (\(\theta^{is}\)) as Eq. (10) to obtain the parameter importance;
8: Keep \(p\) significant parameters stop gradient updating as Eq.(11) after sorting the importance;
9:endif
10:endwhile ```

**Algorithm 1** The bi-sampling parameter attribution for compact image SR.

## 4 Experiments

### Datasets and Implementation Details

**Datasets**. We use DIV2K [1] to train the SR models, which is a high-quality dataset widely used for image SR. The whole dataset includes \(800\) training images and \(100\) validation images totally with diverse contents and texture details. The LR images are obtained in the same way as [52; 15]. To demonstrate the effectiveness of our method, the SR models are also evaluated on five public SR benchmark datasets: Set5 [3], Set14 [48], B100 [2], Urban100 [14] and Manga109 [29].

**Implementation Details**. During training, we fix the patch size of the HR image as \(128\times 128\) for \(2\times\), \(4\times\) SR, and \(129\times 129\) for \(3\times\) SR. We follow ClassSR [19] for data preprocessing in inverse sampling, which crops the whole image into multiple sub-images with sliding and overlap. For \(2\times\), \(3\times\) SR, \(4\times\) SR, it produces \(499875\), \(502200\), \(499875\) sub-images, respectively. Following [39], the number of the data repetition is set to \(1\). The training data is augmented by random flipping and rotation. We use Adam optimizer with \(\beta_{1}=0.9\), \(\beta_{2}=0.999\) to train the SR models. The mini-batch size is set to \(16\). The learning rate is initialized as \(2e-4\) and reduced by half per \(200\) epochs for \(400\) epochs totally. The unbalanced factor for the inverse sampling data is set to \(10\) and \(\beta\) is set to \(0.1\). The interval of alternate training is \(50\) epochs and the number of classes of inverse sampling for DIV2K training dataset is \(10\). In our experiments, \(\mu\) is derived to \(0.7743\) according to the number of classes and the unbalanced factor. All the experiments are conducted with PyTorch framework on NVIDIA 2080Ti GPUs. Besides, objective criteria, i.e., peak signal-to-noise ratio (PSNR), structural similarity (SSIM) are utilized to evaluate the model performance. Note that the two metrics are both calculated on the Y channel of the YCbCr space as adopted in the previous works [6; 52].

### Ablation Study

In this subsection, we mainly give a detailed analysis about the proposed bi-sampling parameter attribution (BSPA) for accurate image SR. Following [31], EDSR_baseline (\(16\) residual blocks with \(64\) feature channels) is exploited as the backbone network.

**Bi-sampling Strategy.** The bi-sampling strategy is adopted to capture the training samples for the two-step iterative optimization procedure. Uniform sampling ensures the original data distribution, while inverse sampling mainly enhances the feature extraction for the edge and texture regions. Note that all the cropped sub-images are grouped as \(10\) classes for inverse sampling in our experiments. In Tab. 1, it is observed that the baseline model obtains \(28.22\) dB in PSNR on Set14, while the model trained with inverse sampling for unbalanced factor \(10\) achieves \(28.25\) dB. Besides, we present the average PSNR value of each class on the cropped DIV2K training and validation sub-image dataset for the SR model separately trained by uniform sampling and inverse sampling in Fig. 3. It illustrates that the

Figure 4: Visual comparisons of the SR results by the uniform sampling (uniform SR) and the inverse sampling (inverse SR), and the corresponding error maps with the GT.

Figure 5: The histogram distribution of all training sub-images in DIV2K for EDSR_baseline and BSPA (\(4\times SR\)). It is observed that the distribution of BSPA becomes short-tailed.

Figure 3: The PSNR of each class on the classified DIV2K training and validation datasets for uniform sampling and inverse sampling.

SR model trained with inverse sampling performs much better than the SR model with the uniform sampling on the tail hard classes, while performing worse than the SR model with the uniform sampling on the simple classes. Therefore, inverse sampling helps the feature extraction on hard texture regions. We also visualize the SR results and their error maps with the GT of uniform sampling (uniform SR) and inverse sampling (inverse SR) in Fig. 4. It shows that inverse SR has a smaller error than the uniform SR on the texture region.

**Integrated Gradient.** To demonstrate the effect of the integrated gradient (IG), we first use uniformly sampled and inversely sampled data to alternately train the SR model. As the model "w/o IG" shows in Tab. 1, the hybrid training obtains \(0.06\) dB improvement in PSNR against the baseline model on Set14. Besides, we compare the filter-level IG (FIG) and weight-level IG (WIG), which denote the parameter granularity in Eq.10 for each filter and each parameter. It is observed that WIG achieves the best PSNR, which illustrates that the weight is more discriminative than the filter for the model performance. Note that we adopt the weight-level IG in the experiments. Besides, we visualize the distribution curve if the MSE values are evaluated by the model trained with BSPA. As Fig. 5 shows, it is observed that the histogram distribution of BSPA becomes short-tailed. The reason is that the SR performance on the tail data is improved, while the performance on the head data is kept. Therefore, it demonstrates that our BSPA is effective in obtaining a more compact representation.

**Effect of Selective Ratio.** The selective factor in Eq. (11) aims to control the proportion of the significant parameters and update the remaining trivial parameters. As shown in Tab. 2, we give the quantitative comparisons of different \(\beta\) values for the selective ratio. It can be observed that the best PSNR is obtained when \(\beta\) is \(0.1\). Therefore, we fix \(\beta\) as \(0.1\) in our experiments.

**Effect of Unbalanced Factor.** To demonstrate the effect of the unbalanced factor used for inverse sampling data generation, we give the results of our BSPA with different unbalanced factors as shown in Table 3. It shows that the best PSNR is obtained when the unbalanced factor is equal to \(10\). Since the unbalanced factor determines the sampling probability of each class in inverse sampling, the large factor causes multiple repetitive sampling to the tail hard samples. It may bring about the overfitting on these areas so as to hinder the model representation.

**Effect of the Number of Classes.** For the classes, we compare different number of classes in Table 5. It shows that the SR performance changes little to the number of classes. Because the data in different classes can be sampled according to their proportions by means of inverse sampling so as to remedy the unbalanced data bias. In our experiments, we set the number of classes to \(10\).

### Comparisons with the State-of-the-arts

To demonstrate the effectiveness of our BSPA, we integrate it with three representative SR models with different model capacities, i.e., FSRCNN [10] (small), EDSR_baseline [23] (medium), and RCAN [52] (large). Besides, we compare BSPA with SamplingAug [39] (a sampling-based data augmentation method) and UDN [31] (a loss reweighting method). All these SR models are retrained under the same experimental settings with the released codes.

**Quantitative Results.** In Tab. 4, it shows that SamplingAug and our BSPA outperform the baseline methods with different model capacities for all scaling factors, which is even higher \(1.32\)dB than the original FSRCNN on DIV2K validation dataset for \(2\times\) SR. The results of UDN on some datasets are

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Model & Baseline (uniform sampling) & inverse sampling & w/o IG & FHQ & wf0 \\ \hline Set14 & 28.22 & 28.25 & 28.28 & 28.32 & 28.37 \\ B100 & 27.31 & 27.33 & 27.35 & 27.38 & 27.40 \\ Urban100 & 25.27 & 25.34 & 25.38 & 25.47 & 25.53 \\ DIV2K valid & 29.97 & 30.02 & 30.06 & 30.09 & 30.13 \\ \hline \end{tabular}
\end{table}
Table 1: Ablation studies about the bi-sampling parameter attribution on benchmarks for \(4\times\) SR.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Unbalanced factor & 10 & 50 & 100 & 200 \\ \hline Set14 & 28.37 & 28.33 & 28.28 & 28.21 \\ B100 & 27.40 & 27.35 & 27.31 & 27.23 \\ Urban100 & 25.53 & 25.51 & 25.46 & 25.43 \\ DIV2K valid & 30.13 & 30.10 & 30.07 & 30.05 \\ \hline \end{tabular}
\end{table}
Table 3: The quantitative comparisons (PSNR) of different unbalanced factors on benchmarks for \(4\times\) SR.

even inferior to the baseline model, which may originate from the inaccurate estimation of uncertainty. Note that the model's performance on FSRCNN for \(3\times\) and \(4\times\) scale is worse than UDN. FSRCNN is an extremely lightweight model, which has only \(24.5\)K parameters and several convolutional layers. The model representation ability is restricted. Therefore, it may not fit the training well no matter the easy data or the hard data by the limited model capacity, especially for large scaling factors. Although the bi-sampling strategy increases the data diversity, there lacks sufficient model capacity to it. UDN is a loss re-weighting method for solving the data imbalance problem. It provides a regularization constraint for shrinking the solution space. Besides, our method has advantages over other compared methods on large models. For EDSR and RCAN, our BSPA almost performs the best for all scaling factors against the compared methods.

**Qualitative Results.** The visual comparisons on benchmark datasets for \(4\times\) SR are shown in Fig. 6. For images "img042" and "img092" in Urban100 dataset, we can observe that BSPA is more favorable and can recover more texture details than other compared methods. Therefore, our proposal can not only achieve excellent performance but also be superior in the visual effect. Besides, we also provide the perceptual metrics (NIQE/LPIPS) comparisons for RCAN in Table 6. It shows that our method obtains superior NIQE and LPIPS values against the compared methods.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{\multirow{2}{*}{Scale}} & \multicolumn{1}{c|}{Set14} & \multicolumn{1}{c|}{B10} & \multicolumn{1}{c|}{Urban100} & \multicolumn{1}{c|}{Urban2K valid} \\ \hline \multicolumn{1}{|c|}{\multirow{-2}{*}{\(2\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Params}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Set14}} & \multicolumn{1}{c|}{B100} & \multicolumn{1}{c|}{Urban100} & \multicolumn{1}{c|}{DIV2K valid} \\ \hline \multicolumn{1}{|c|}{\multirow{-2}{*}{\(2\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNRCNN}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} \\ \hline \multirow{11}{*}{\(2\times\)} & FSRCNN [10] & 24.5K & 33.09/0.9180 & 30.02/0.8625 & 29.44/0.8396 & 26.77/0.8320 & 30.29/0.9172 & 32.18/0.8989 \\  & SamplingAug [39] & 24.5K & 34.71/0.9144 & 31.24/0.8942 & 30.51/0.8766 & 27.83/0.8667 & 22.83/0.9452 & 33.46/0.9225 \\  & UDN [31] & 24.5K & 34.79/0.9142 & **31.29**/0.8940 & 30.50/0.8758 & 27.81/0.8660 & 32.34/0.9448 & 33.47/0.9223 \\  & BSPA (ours) & 24.5K & **34.80**/**0.9420 & 31.27/**0.8943** & **30.54**/**0.8772** & **27.86/0.8682** & 32.27/**0.9454** & **33.50/0.9232** \\  & BSPA (ours) & 24.5K & 37.58/0.9595 & 33.15/0.9140 & 31.89/0.8959 & 30.89/0.9156 & 37.57/0.9745 & 35.55/0.9411 \\  & SamplingAug [39] & 1.4M & 37.50/0.9590 & 33.25/0.9146 & 31.98/0.8960 & 30.93/0.9160 & 37.67/0.9474 & 35.66/0.9420 \\  & UDN [31] & 1.4M & 37.60/0.9590 & 33.32/0.9137 & 31.89/0.8960 & 30.93/0.9160 & 37.67/0.9474 & 35.56/0.9411 \\  & BSPA (ours) & 1.4M & **37.70**/**0.9594** & **33.27**/**0.9194** & **31.96**/**0.9896** & **31.18**/**0.9185** & **37.88/0.9752** & **35.69/0.9420** \\ \hline \multicolumn{1}{|c|}{\multirow{-2}{*}{\(2\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} \\ \hline \multicolumn{1}{|c|}{\multirow{-2}{*}{\(3\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} \\ \cline{1-1} \multicolumn{1}{|c|}{\multirow{-2}{*}{\(4\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} & \multicolumn{1}{c|}{\multirow{-2}{*}{PSNR/SSIM}} \\ \cline{1-1} \multicolumn{1}{|c|}{\multirow{-2}{*}{\(4\times\)}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & \multicolumn

### Limitations

We adopt a re-sampling strategy to address the unbalanced data distribution problem for image SR. It has several limitations. **1)** Unlike the end-to-end loss re-weighting methods [31], our method needs to implement extra data pre-processing for classifying sub-images at first. **2)** Inverse sampling allocates larger weights for the tail training samples, which may lead to over-fit upon these data. Therefore, it is important to define an appropriate unbalanced factor. Most of the existing SR models only consider that different image regions have different reconstruction diculties, and then mainly focus on the delicate structure design (like attention mechanism) or complex regularization constraints. Few discuss the sampling strategies of training data and model this as a long-tail problem. The key idea behind the proposed Bi-Sampling Parameter Attribution method is to reconcile the unbalanced inherent data bias, namely the heavy-tailed distribution are visually more important than smooth areas. The newly developed technique with integrate gradient involves an inverse sampling strategy for enhancing the feature extraction ability of the model on the hard samples. We believe that our method has such potential for fully mining the dataset information and may inspire the research of this direction in the future.

## 5 Conclusion

In this paper, we rethink the current paradigm of merely using uniform sampling to capture the mini-batch data for training the SR model, which would lead to the notorious data unbalanced problem. To address this issue, we propose a bi-sampling parameter attribution (BSPA) method for accurate image SR. The bi-sampling is introduced to remedy the unbalanced data distribution, including the usually adopted uniform sampling and inverse sampling. The former aims to keep the intrinsic data distribution, the latter is designed to promote the model performance on the few hard samples, which is inversely proportional to the number of each class. To achieve this, we divide the whole training dataset into multiple sub-images and group these sub-images into different classes according to the reconstruction error. Moreover, the integrated gradient method is introduced to calculate the parameter importance, with which the significant parameters are fixed and the remaining trivial parameters are further refined. Extensive experiments on SR benchmark datasets have demonstrated that our proposal achieves superior performance against the baseline methods. In our future work, we will apply the proposal to other low-level tasks based on patch training.

**Acknowledgments.** This work is supported by the National Key Research and Development Program of China No.2020AAA0108301; National Natural Science Foundation of China under Grants No.62176224 and No.62222602; Natural Science Foundation of Chongqing under No.CSTB2023NSCO-JOX0007; CCF-Lenovo Blue Ocean Research Fund.

Figure 6: Visual comparisons of our BSPA with other methods on Set14 and Urban100 for \(4\times\) SR.

## References

* [1]E. Agustsson and R. Timofte (2017) NTIRE 2017 challenge on single image super-resolution: dataset and study. In CVPRW, Cited by: SS1, SS2.
* [2]P. Arbelaez, M. Maire, C. C. Fowlkes, and J. Malik (2011) Contour detection and hierarchical image segmentation. TPAMI. Cited by: SS1, SS2.
* [3]M. Bevilacqua, A. Roumy, C. Guillemot, and M. Alberi-Morel (2012) Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, Cited by: SS1, SS2.
* [4]A. Binder, G. Montavon, S. Lapuschkin, K. Muller, and W. Samek (2016) Layer-wise relevance propagation for neural networks with local renormalization layers. In ICANN, Cited by: SS1, SS2.
* [5]Z. Chen, Y. Zhang, J. Gu, L. Kong, X. Yang, and F. Yu (2023) Dual aggregation transformer for image super-resolution. In ICCV, Cited by: SS1, SS2.
* [6]T. Dai, J. Cai, Y. Zhang, S. Xia, and L. Zhang (2019) Second-order attention network for single image super-resolution. In CVPR, Cited by: SS1, SS2.
* [7]A. Pierro de Camargo (2020) A new proof of the equivalence of the cauchy mean value theorem and the mean value theorem. Am. Math. Mon. Cited by: SS1, SS2.
* [8]K. Dhamdhere, M. Sundararajan, and Q. Yan (2019) How important is a neuron. In ICLR, Cited by: SS1, SS2.
* [9]C. Dong, C. C. Loy, K. He, and X. Tang (2016) Image super-resolution using deep convolutional networks. TPAMI. Cited by: SS1, SS2.
* [10]C. Dong, C. C. Loy, and X. Tang (2016) Accelerating the super-resolution convolutional neural network. In ECCV, Cited by: SS1, SS2.
* [11]A. Gupta, P. Dollar, and R. B. Girshick (2019) LVIS: a dataset for large vocabulary instance segmentation. In CVPR, Cited by: SS1, SS2.
* [12]H. Han, W. Wang, and B. Mao (2005) Borderline-smote: a new over-sampling method in imbalanced data sets learning. In ICIC, Cited by: SS1, SS2.
* [13]C. Huang, Y. Li, C. Change Loy, and X. Tang (2020) Deep imbalanced learning for face recognition and attribute prediction. TPAMI. Cited by: SS1, SS2.
* [14]J. Huang, A. Singh, and N. Ahuja (2015) Single image super-resolution from transformed self-exemplars. In CVPR, Cited by: SS1, SS2.
* [15]Z. Hui, X. Gao, Y. Yang, and X. Wang (2019) Lightweight image super-resolution with information multi-distillation network. In ACM MM, Cited by: SS1, SS2.
* [16]J. Johnson, A. Alahi, and L. Fei-Fei (2016) Perceptual losses for real-time style transfer and super-resolution. In ECCV, Cited by: SS1, SS2.
* [17]J. Kim, J. Kwon Lee, and K. Mu Lee (2016) Accurate image super-resolution using very deep convolutional networks. In CVPR, Cited by: SS1, SS2.
* [18]J. Kim, J. Kwon Lee, and K. Mu Lee (2016) Deeply-recursive convolutional network for image super-resolution. In CVPR, Cited by: SS1, SS2.
* [19]X. Kong, H. Zhao, Y. Qiao, and C. Dong (2021) Classsr: a general framework to accelerate super-resolution networks by data characteristic. In CVPR, Cited by: SS1, SS2.
* [20]K. Leino, S. Sen, A. Datta, M. Fredrikson, and L. Li (2018) Influence-directed explanations for deep convolutional networks. In ITC, Cited by: SS1, SS2.
* [21]F. Li, R. Cong, H. Bai, and Y. He (2020) Deep interleaved network for single image super-resolution with asymmetric co-attention. In IJCAI, Cited by: SS1, SS2.

[MISSING_PAGE_POST]

ightweight image super-resolution with information multi-dis* [22] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. _CoRR_, abs/1612.08220, 2016.
* [23] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In _CVPRW_, 2017.
* [24] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale long-tailed recognition in an open world. In _CVPR_, 2019.
* [25] Xiaotong Luo, Mingliang Dai, Yulun Zhang, Yuan Xie, Ding Liu, Yanyun Qu, Yun Fu, and Junping Zhang. Adjustable memory-efficient image super-resolution via individual kernel sparsity. In _ACM MM_, 2022.
* [26] Xiaotong Luo, Qiuyuan Liang, Ding Liu, and Yanyun Qu. Boosting lightweight single image super-resolution via joint-distillation. In _ACM MM_, 2021.
* [27] Xiaotong Luo, Yanyun Qu, Yuan Xie, Yulun Zhang, Cuihua Li, and Yun Fu. Lattice network for lightweight image restoration. _TPAMI_, 2022.
* [28] Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In _ECCV_, 2018.
* [29] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. _Multim. Tools Appl._, 2017.
* [30] Gregoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Muller. Layer-wise relevance propagation: An overview. In _Explainable AI: Interpreting, Explaining and Visualizing Deep Learning_. 2019.
* [31] Qian Ning, Weisheng Dong, Xin Li, Jinjian Wu, and Guangming Shi. Uncertainty-driven loss for single image super-resolution. In _NeurIPS_, 2021.
* [32] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _CVPR_, 2016.
* [33] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. In _NeurIPS_, 2019.
* [34] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In _ICLRW_, 2014.
* [35] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In _ICML_, 2017.
* [36] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Gradients of counterfactuals. In _ICLR_, 2017.
* [37] Longguang Wang, Yingqian Wang, Zaiping Lin, Jun-Gang Yang, Wei An, and Yulan Guo. Learning for scale-arbitrary super-resolution from scale-specific networks. In _ICCV_, 2021.
* [38] Shizun Wang, Jiaming Liu, Kaixin Chen, Xiaoqi Li, Ming Lu, and Yandong Guo. Adaptive patch exiting for scalable single image super-resolution. In _ECCV_, 2022.
* [39] Shizun Wang, Ming Lu, Kaixin Chen, Jiaming Liu, Xiaoqi Li, Chuang Zhang, and Ming Wu. Samplingaug: On the importance of patch sampling augmentation for single image super-resolution. In _BMVC_, 2021.
* [40] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. ESRGAN: enhanced super-resolution generative adversarial networks. In _ECCVW_, 2018.
* [41] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In _NeurIPS_, 2017.

* [42] Liangbin Xie, Xintao Wang, Chao Dong, Zhongang Qi, and Ying Shan. Finding discriminative filters for specific degradations in blind super-resolution. In _NeurIPS_, 2021.
* [43] Wenbin Xie, Dehua Song, Chang Xu, Chunjing Xu, Hui Zhang, and Yunhe Wang. Learning frequency-aware dynamic network for efficient super-resolution. In _ICCV_, 2021.
* [44] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for face recognition with under-represented data. In _CVPR_, 2019.
* [45] Ke Yu, Chao Dong, Liang Lin, and Chen Change Loy. Crafting a toolchain for image restoration by deep reinforcement learning. In _CVPR_, 2018.
* [46] Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, and Chen Change Loy. Path-restore: Learning network path selection for image restoration. _TPAMI_, 2022.
* [47] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In _ECCV_, 2014.
* [48] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In _ICCS_, 2010.
* [49] Xinyi Zhang, Tao Dai, Bin Chen, and Shu-Tao Xia. DDA: A dynamic difficulty-aware data augmenter for image super-resolution. In _IJCNN_, 2023.
* [50] Yachao Zhang, Yuan Xie, Cuihua Li, Zongze Wu, and Yanyun Qu. Learning all-in collaborative multiview binary representation for clustering. _TNNLS_, 2022.
* [51] Yongshun Zhang, Xiu-Shen Wei, Boyan Zhou, and Jianxin Wu. Bag of tricks for long-tailed visual recognition with deep convolutional neural networks. In _AAAI_, 2021.
* [52] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In _ECCV_, 2018.
* [53] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition. In _CVPR_, 2020.