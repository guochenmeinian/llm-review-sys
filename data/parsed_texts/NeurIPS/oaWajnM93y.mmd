# Cream: Consistency Regularized Self-Rewarding Language Models

Zhaoyang Wang\({}^{1}\) Weilei He\({}^{2}\) Zhiyuan Liang\({}^{3}\) Xuchao Zhang\({}^{4}\)

\({}^{1}\)University of North Carolina at Chapel Hill \({}^{2}\)Nanyang Technological University

\({}^{3}\)National University of Singapore \({}^{4}\)Microsoft Research

{zhaoyang,huaxiu}@cs.unc.edu weitongz@unc.edu

###### Abstract

Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee on the accuruty of the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a **C**onsistency **R**egularized s**E**lf-rewarding **I**Anguage **M**odel (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance.

## 1 Introduction

Large language models (LLMs) have shown impressive capabilities across various tasks, including natural language understanding and generation (Radford et al., 2019). At the same time, LLMs also face alignment challenges such as generating hallucinations and harmful outputs (Ji et al., 2023). To address these issues, a series of research works have explored preference learning methods such as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) and direct alignment techniques such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) to align the LLMs with human values and preferences. These alignment methods often require a large amount of preference pairs which are indispensable in both RLHF and direct alignment training. However, collecting human-annotated preference pairs is time-consuming and labor-intensive, which seriously limits the scalability and efficiency of these alignment methods.

Recent advancements in self-rewarding language models (SRLMs) (Yuan et al., 2024) have gained increasing attention in the field of LLM alignment, which can efficiently synthesize preference data for iterative preference training. In this method, the single LLM is required to act as two roles, the policy model and the reward model. Given unlabeled prompt data, the LLM first acts as the policy model generating several response candidates. Then, the same LLM acts as the reward model, scoring and ranking these responses. These ranked responses are used as preference pairs to train the LLM with DPO, significantly reducing the reliance on human-annotated data. The above steps can be iteratively repeated to further enhance the performance. However, SRLMs still face challengesin generating reliable and accurate rewards for annotating the preference pairs, which is critical for ensuring the quality of preference data and the alignment performance of LLMs.

To address these challenges, we first formulate a generalized iterative preference fine-tuning framework to analyze the self-rewarding training, where this framework can also be adapted to other iterative preference tuning methods. Through this theoretical framework, we find that the rewarding bias issue in SRLMs comes from the overconfident preference labeling, which enforces the model to distinguish between responses with similar quality. For example, both two responses in Figure 1 have high quality judgments from the human. The SRLM enforces the reward model to make a preference judgment, resulting in noisy and unreliable preference labeling. This can lead to negative impacts on preference tuning the model. Additionally, the iterative training manner can also accumulate the rewarding bias, further diminishing the benefits of self-improvement. From the insights of theoretical analysis, we propose **C**onsistency **R**e**gularized **S**E**lfer-rewarding **I**n**a**nguage **M**odel (CREAM) to mitigate the rewarding bias issue in SRLMs, particularly for broadly accessible 7B-size LLMs. The core idea behind CREAM is that we should not force the model to be overly confident in distinguishing between responses with similar quality. _But how to tell the preference labeling is reliable or not?_ Out of the self-rewarding scenario, we may employ a pool of external reward models to assist in ranking preferences. When two responses are of similar quality, these external models often produce inconsistent rankings.

This inconsistency serves as a signal to indicate the level of confidence in the preference labeling. In self-rewarding scenarios, however, integrating such external reward models is not feasible. Fortunately, due to the iterative nature of self-rewarding training, we can use the reward model from the previous iteration to rank preferences and then compare these rankings with those produced by the current model. This comparison provides an estimate of such consistency rate. With this consistency rate, we can regularize the preference training to prevent the model from learning unreliable preference data, thereby mitigating the rewarding bias issue in SRLMs.

In summary, we first formulates a generalized iterative preference fine-tuning framework to analyze the rewarding bias issue in SRLMs. From the insights of theoretical analysis, we propose CREAM as the primary contribution of this paper. CREAM leverages the consistency of rewards across different iterations for regularized preference training, which can effectively mitigate the rewarding bias issue in SRLMs. Empirical results on a series of natural language benchmarks validate the effectiveness of CREAM in mitigating the rewarding bias issue and enhancing the alignment performance of LLMs.

**Notations.** Vectors are denoted by lowercase boldface letters, such as \(\mathbf{x}\), and matrices by uppercase boldface letters, such as \(\mathbf{A}\). For any positive integer \(k\), the set \(1,2,\ldots,k\) is denoted by \([k]\). Sets are denoted by calligraphic uppercase letters, such as \(\mathcal{D}\), with the cardinality of the set represented as \(|\mathcal{D}|\). Without ambiguity, we denote \(\pi_{\bm{\theta}}\) as the language model parameterized by \(\bm{\theta}\), \(\mathbf{x}\) as the input prompt, and \(\mathbf{y}\) as the output response from the language model. All other notations are defined prior to their first usage. We denote \(\mathds{1}[\cdot]\) as the indicator function.

## 2 Methodology

In this section, we first formulate the generalized iterative preference fine-tuning framework for self-rewarding, RL with AI feedback, and other iterative preference tuning methods. Next, we introduce the motivation behind the proposed consistency regularized self-rewarding method. Finally, we present the practical implementation algorithm of CREAM in details.

### Generalized Iterative Preference Fine-Tuning Framework

We assume that we can access to the dataset with response \(\mathcal{D}_{\text{S}}\) and the prompt dataset without response \(\mathcal{D}_{\text{U}}\). The objective is to iteratively minimize the following loss with respect to the neural

Figure 1: An example of both two responses are of high quality, which is hard for human to distinguish the preference. While the same model from different iterations have inconsistent rewarding.

network parameter \(\bm{\theta}\) and a label function \(z\) as

\[\mathcal{L}(\bm{\theta},z)=\mathcal{L}_{\text{SFT}}(\bm{\theta};\mathcal{D}_{ \text{S}})+\mathbb{E}_{\bm{x}\sim\mathcal{D}_{\text{U}};\mathbf{y},\mathbf{y}^ {\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})}[\mathcal{L}_{\text{DPO}}( \bm{\theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x},z)].\] (2.1)

where the first term \(\mathcal{L}_{\text{SFT}}(\bm{\theta};\mathcal{D}_{\text{S}})\) aligns the model \(\pi_{\bm{\theta}}\) to the SFT data. We note here that any potential SFT methods (Ouyang et al., 2022; Yuan et al., 2023; Dong et al., 2023; Chen et al., 2024), or the methods without SFT data (\(\mathcal{L}_{\text{SFT}}=0\)) can be adapted in this framework. The second term \(\mathbb{E}[\mathcal{L}_{\text{DPO}}]\) corresponds to learning from the preference data pair \(\{\mathbf{y},\mathbf{y}^{\prime}\}\) generated by the current model \(\bm{\theta}_{t}\). The labeling function \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\in\{0,1\}\) provides the preference judgment between \(\mathbf{y}\) and \(\mathbf{y}^{\prime}\) for the DPO loss, where \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})=1\) means \(\mathbf{y}\succ\mathbf{y}^{\prime}\) and \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})=0\) means \(\mathbf{y}\prec\mathbf{y}^{\prime}\). The DPO loss \(\mathcal{L}_{\text{DPO}}\) is defined as follows:

\[\mathcal{L}_{\text{DPO}}(\bm{\theta};\mathbf{y},\mathbf{y}^{ \prime},\mathbf{x},z)=-z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\log\sigma \left(\log\left(\frac{\pi_{\bm{\theta}}(\mathbf{y}|\mathbf{x})}{\pi_{\text{ ref}}(\mathbf{y}|\mathbf{x})}\right)-\log\left(\frac{\pi_{\bm{\theta}}( \mathbf{y}^{\prime}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}^{\prime}|\mathbf{ x})}\right)\right)\\ -(1-z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x}))\log\sigma\left( \log\left(\frac{\pi_{\bm{\theta}}(\mathbf{y}^{\prime}|\mathbf{x})}{\pi_{\text{ ref}}(\mathbf{y}^{\prime}|\mathbf{x})}\right)-\log\left(\frac{\pi_{\bm{\theta}}( \mathbf{y}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})}\right)\right),\] (2.2)

where \(\pi_{\text{ref}}\) is the reference model for KL divergence regularization, and \(\sigma(\cdot)\) is the sigmoid function. The proposed loss \(\mathcal{L}(\bm{\theta},z)\) in Eq. (2.1) represents all iterative preference fine-tuning algorithms. For the reinforcement learning (RL) with human feedback (Ouyang et al., 2022), \(z\) is the human preference comparing \(\mathbf{y}\) and \(\mathbf{y}^{\prime}\). For the RL with AI feedback, \(z\) is the oracle reward model like GPT-4 (Achiam et al., 2023). For the self-rewarding language model (Chen et al., 2024), \(z\) is given by comparing the reward score generated from the language model itself, often with LLM-as-a-Judge prompting. However, as aforementioned, we note that such prompt rewarding method may only be feasible for larger and advanced LLMs such as Llama-70B (Touvron et al., 2023). For smaller models such as Llama-7B that do not have complex instruction following and reasoning abilities, we instead propose to leverage the intrinsic reward model (Rafailov et al., 2023)

\[r_{\bm{\theta}}(\mathbf{x},\mathbf{y})\propto[\log\pi_{\bm{\theta}}(\mathbf{y} |\mathbf{x})-\log\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})]\]

to reward and rank the responses for annotating preference pairs. Therefore, the choice of preference labeling function \(z\) is closely connected with the language model parameter \(\bm{\theta}\). Then, we introduce the following two-step optimization algorithm to solve Eq. (2.1).

**Step 1.** (Preference-labeling step) Keep \(\bm{\theta}=\bm{\theta}_{t}\) fixed, select function \(z\) to minimize \(\mathcal{L}_{\text{DPO}}\). In particular, letting \(\bm{\theta}=\bm{\theta}_{t}\) in Eq. (2.2), solution for \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})=\arg\min_{z}\mathcal{L}_{\text{DPO }}(\bm{\theta}_{t};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x},z)\) is

\[z_{t+1}(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})=\mathds{1}\left[\log\pi_{\bm {\theta}_{t}}(\mathbf{y}|\mathbf{x})-\log\pi_{\text{ref}}(\mathbf{y}|\mathbf{ x})\geq\log\pi_{\bm{\theta}_{t}}(\mathbf{y}^{\prime}|\mathbf{x})-\log\pi_{\text{ ref}}(\mathbf{y}^{\prime}|\mathbf{x})\right].\] (2.3)

**Step 2.** (Learning step) Keep \(z\) as of Eq. (2.3), minimize loss function \(\mathcal{L}(\bm{\theta},z_{t+1})\) with respect to \(\bm{\theta}\) and get \(\bm{\theta}_{t+1}=\arg\min_{\bm{\theta}}\mathcal{L}(\bm{\theta},z_{t+1})\).

Different from existing methods, the proposed two-step optimization method directly uses the intrinsic reward model to generate the preference data. This approach is particularly feasible for smaller LLMs, which lack the capacity to effectively use LLM-as-a-Judge prompts (Zheng et al., 2023) for rewarding and ranking. We note that the proposed two-step method is similar to the Expectation-Maximization algorithm and self-training paradigm (Zou et al., 2019). This similarity is supported by the following theorem, which suggests the convergence of the proposed two-step algorithm.

**Theorem 2.1**.: Suppose the optimization \(\bm{\theta}_{t+1}=\arg\min_{\bm{\theta}}\mathcal{L}(\bm{\theta},\,z_{t+1})\) is solvable and the SFT loss \(\mathcal{L}_{\text{SFT}}(\bm{\theta};\mathcal{D}_{\text{S}})\geq 0\) for all \(\bm{\theta}\) and \(\mathcal{D}_{\text{S}}\), the proposed two-step optimization method converges.

### Consistency Regularized Self-Rewarding

The generalized framework presented in Eq. (2.1) assumes the human feedback or GPT-4 are all reliable so that the preference labeling function \(z\) is trustworthy. However, for SRLMs, the accuracy of preference labeling is not always guaranteed. Therefore, treating all selected preference labels as "ground truth" by encoding them as hard labels can lead to overconfident mistakes, potentially propagating biases and inaccuracies from the LLMs. Taking Figure 1 as an example, both the two responses \(\mathbf{y}\) and \(\mathbf{y}^{\prime}\) are judged by humans to be of high quality. _Forcing the model to be overly confident in distinguishing between these two responses \(\{\mathbf{y},\mathbf{y}^{\prime}\}\) with similar quality can negatively impact the performance of SRLMs during training._This rewarding bias issue motivates us to mitigate such ambiguity by introducing a consistency-regularized self-rewarding language model, CREAM. Specifically, for a pair of responses with very similar quality, their oracle reward scores should ideally be very close to each other. Particularly, when multiple reward models are available, it is likely that some models will rank one response as superior, while others may rank the opposite response as better, resulting in high ranking inconsistency (i.e., low ranking consistency) among these models. Based on this, CREAM aims to prevent the model from learning from preference pairs with low consistency. Instead, it focuses solely on preference pairs with high consistency across different reward models, thereby mitigating the rewarding bias issue and stabilize the learning process to some extent. From the theoretical perspective, we can introduce a regularization term to Eq. (2.1) as

\[\mathcal{L}(\bm{\theta},z)=\mathcal{L}_{\text{SFT}}(\bm{\theta}; \mathcal{D}_{\text{S}})+\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{U}}; \mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})}[ \mathcal{L}_{\text{DPO}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x},z)+\lambda\mathcal{L}_{\text{Reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime },\mathbf{x})],\] (2.4)

where the regularization term \(\mathcal{L}_{\text{Reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\) prevents the model \(\pi_{\bm{\theta}}\) from overconfidence in distinguishing the preference of \(\{\mathbf{y},\mathbf{y}^{\prime}\}\) with similar quality, which is quantified in the following lemma.

**Lemma 2.2**.: Let the random variable \(z=z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\) be defined as \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})=\mathds{1}[\mathbf{y}\succ \mathbf{y}^{\prime}|\mathbf{x}]\). The Bradley-Terry model (Bradley and Terry, 1952) for the probability of \(z\) under parameter \(\bm{\theta}\) is given by

\[P_{\bm{\theta}}(z)=P_{\bm{\theta}}(\mathds{1}[\mathbf{y}\succ \mathbf{y}^{\prime}|\mathbf{x}])=\sigma\left(\log(\pi_{\bm{\theta}}(\mathbf{y} |\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))-\log(\pi_{\bm{\theta}}( \mathbf{y}^{\prime}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}^{\prime}|\mathbf{ x}))\right),\]

Letting the regularization \(\mathcal{L}_{\text{Reg}}\) be defined by

\[\mathcal{L}_{\text{Reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^{ \prime},\mathbf{x}) =-\log\sigma\left(\log(\pi_{\bm{\theta}}(\mathbf{y}|\mathbf{x})/ \pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))-\log(\pi_{\bm{\theta}}(\mathbf{y}^{ \prime}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}^{\prime}|\mathbf{x}))\right)\] \[\quad-\log\sigma\left((\log\pi_{\bm{\theta}}(\mathbf{y}^{\prime} |\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}^{\prime}|\mathbf{x}))-(\log\pi_{\bm{ \theta}}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})) \right).\] (2.5)

Then the expected regularized loss under the model \(\bm{\theta}_{t}\) is given by:

\[\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot| \mathbf{x})}\mathcal{L}_{\text{reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime },\mathbf{x})=2\,\mathbb{KL}(u(\cdot)\parallel P_{\bm{\theta}}(\cdot)),\] (2.6)

where \(u(z)\) is the uniform binary distribution, i.e., \(u(z=0)=u(z=1)=0.5\).

As Lemma 2.2 suggests, the \(\mathcal{L}_{\text{Reg}}\) will regularize the preference between \(\{\mathbf{y},\mathbf{y}^{\prime}\}\) that has similar quality to a uniform distribution. Then the following theorem suggests that using \(\mathcal{L}_{\text{DPO}}+\lambda\mathcal{L}_{\text{Reg}}\) corresponds to the soft-labeled DPO which we implemented in CREAM.

**Theorem 2.3**.: For all \(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x},z\), minimizing

\[\mathcal{L}(\bm{\theta},z)=\mathcal{L}_{\text{SFT}}(\bm{\theta}; \mathcal{D}_{\text{SFT}})+\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{U}}; \mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})} \left[\mathcal{L}_{\text{DPO}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime}, \mathbf{x},z)+\lambda\mathcal{L}_{\text{Reg}}(\bm{\theta};\mathbf{y},\mathbf{ y}^{\prime},\mathbf{x})\right]\]

is equivariant with minimizing

\[\mathcal{L}(\bm{\theta},z) =\frac{1}{1+2\lambda}\mathcal{L}_{\text{SFT}}(\bm{\theta}; \mathcal{D}_{\text{S}})\] \[\quad+\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{U}};\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})}[\mathcal{C}_{ \lambda}\mathcal{L}_{\text{DPO}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime}, \mathbf{x},z)+(1-\mathcal{C}_{\lambda})\mathcal{L}_{\text{DPO}}(\bm{\theta}; \mathbf{y},\mathbf{y}^{\prime},\mathbf{x},1-z)],\] (2.7)

where the \(1-z\) reverses the preference order of \(z(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\) and \(\mathcal{C}_{\lambda}=(1+\lambda)/(1+2\lambda)\).

Theorem 2.3 suggests that instead of calculating the regularization term \(\mathcal{L}_{\text{Reg}}\), we can use the soft-labeled DPO to train Eq. (2.7). In particular, when \(\lambda=0\), \(\mathcal{C}_{\lambda}=0\) and Eq. (2.7) degenerates to Eq. (2.1). This represents the case where the preference label \(z\) is trustworthy from human or some oracle reward models (e.g., GPT-4). In other words, \(\lambda\) represents the _confidence_ of the label function \(z\). Specially, since in our two-step optimization paradigm, the label function \(z\) is directly derived from the previous model \(\pi_{\bm{\theta}_{t}}\), we can measure the performance of \(\pi_{\bm{\theta}_{t}}\) using the consistency between model \(\bm{\theta}_{t}\) and the baseline model (e.g., external reward model) \(\bm{\theta}_{t}^{\prime}\), defined by

\[\lambda(\mathbf{x})=2\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{ \theta}_{t}}(\cdot|\mathbf{x})}\mathds{1}[\mathbf{y}\succ\mathbf{y}^{\prime}| \mathbf{x},\bm{\theta}_{t}]\mathds{1}[\mathbf{y}\succ\mathbf{y}^{\prime}| \mathbf{x},\bm{\theta}_{t}^{\prime}],\] (2.8)

and when \(\lambda\to 0\), \(\mathcal{C}_{\lambda}\approx 1-\lambda\) representing the consistency of model \(\bm{\theta}_{t}\) and \(\bm{\theta}_{t}^{\prime}\). \(\mathds{1}[\mathbf{y}\succ\mathbf{y}^{\prime}|\mathbf{x},\bm{\theta}_{t}]\) means the response \(\mathbf{y}\) is better than \

### Proposed Algorithm

Equipped with the above two-stage optimization and the consistency-regularized self-rewarding, we are ready to present the implementation of CREAM in Algorithm 1. The whole framework of CREAM is also illustrated in Figure 2. The algorithm starts from the SFT training to obtain the first model parameter \(\bm{\theta}_{1}\) in Line 2. A similar approach is applied in Yuan et al. (2024) for avoid calculating the \(\mathcal{L}_{\text{SFT}}\) in the future optimization steps. Then for each \(\mathbf{x}_{j}\) in the unlabeled prompt set \(\mathcal{D}_{\text{U}}\), \(N\) response candidates \(\{\mathbf{y}_{i}\}_{i=1}^{N}\) are sampled in Line 5. Then reward scores of these \(N\) candidates can be calculated according to Rafailov et al. (2023) by

\[r_{ij}=\beta[\log\pi_{\bm{\theta}_{i}}(\mathbf{y}_{i}|\mathbf{x}_{j})-\log\pi_ {\bm{\theta}_{0}}(\mathbf{y}_{ij}|\mathbf{x}_{j})]+\beta\log Z(\mathbf{x}_{j}),\] (2.9)

where we use the initial model parameter \(\bm{\theta}_{0}\) as the reference policy \(\pi_{\text{ref}}\). Since \(\beta\geq 0\) and \(\log Z(\mathbf{x}_{j})\) is a constant across different response \(\mathbf{y}_{i}\) for the same input prompt \(\mathbf{x}_{j}\), we can drop these factors and calculate rewards in Line 6. Specially, when \(t=1\), the rank \(K_{ij}\) is calculated based on the reference policy \(\bm{\theta}_{0}\) itself. Thus we instead use the likelihood \(r_{ij}=\log\pi_{\bm{\theta}_{0}}(\mathbf{y}_{ij}|\mathbf{x}_{j})\) as the reward for this edge case. The rank for these \(N\) candidates are therefore obtained in Line 7, where \(J_{ij}\) means response \(\mathbf{y}_{ij}\) is in the \(J_{ij}\)-th best in the preference list of \(\mathbf{x}_{j}\).

Consistency-Regularized Self-Rewarding.As discussed in Eq. (2.8), a baseline model is required to measure the consistency. In the self-reward scenario, it is infeasible to add an external reward model as the baseline model. Fortunately, we can employ the model before last update \(\bm{\theta}_{t-1}\) as the baseline model \(\bm{\theta}^{\prime}_{t}\) (i.e., last iteration's model) for evaluating the consistency of the model \(\bm{\theta}\), thanks to chances provided by iterative training manner. Such a procedure helps mitigate the training error introduced in \(t-1\)-th step before obtaining \(\bm{\theta}_{t}\). Considering a pair of tied preference pair \(\mathbf{y},\mathbf{y}^{\prime}\) both performing well, as demonstrated in Figure 1. \(P[\mathbf{y}\succ\mathbf{y}^{\prime}|\mathbf{x},\bm{\theta}_{t}]\) will be oscillating around \(0.5\) when \(t\) grows due to the random noise. Otherwise \(P[\mathbf{y}\succ\mathbf{y}^{\prime}|\mathbf{x},\bm{\theta}_{t}]\) might consistently converge to \(0\) or \(1\). Due to this oscillation, the consistency between \(\bm{\theta}_{t-1}\) and \(\bm{\theta}_{t}\) on this specific preference pair would be low, and the algorithm will learn less from this noninformative preference pair thus stabilize this oscillation.

Specifically, we calculate the rank of these \(N\) candidates using \(\bm{\theta}_{t-1}\) in Line 9 and then use the Kendall's Tau coefficient (Kendall, 1938) denoted by

\[\tau_{j}=\frac{2}{N(N-1)}\sum_{1\leq i<i^{\prime}\leq N}\left[\mathds{1}\left[ (J_{ij}-J_{i^{\prime}j})(K_{ij}-K_{i^{\prime}j})>0\right]-\mathds{1}\left[(J_ {ij}-J_{i^{\prime}j})(K_{ij}-K_{i^{\prime}j})<0\right]\right].\] (2.10)

Kendall's Tau coefficient is a widely used coefficient (McLeod, 2005; Abdi, 2007) to measure the consistency of two ranking sequences. Basically, when two sequences perfectly aligns, \(\tau_{j}=1\) andwhen two sequence never aligns, \(\tau_{j}=-1\). The following lemma draws the further connection between the Kendall's Tau and the regularization parameter \(\lambda\) proposed in Section 2.2.

**Lemma 2.4**.: Suppose the \(N\) response candidate \(\{\mathbf{y}_{ij}\}_{i}\) is i.i.d. given the prompt \(\mathbf{x}_{j}\), then

\[\mathbb{E}[\tau_{j}]=1-4\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{ \boldsymbol{\theta}_{t}}(\cdot|\mathbf{x}_{j})}\,\mathds{1}[\mathbf{y}\succ \mathbf{y}^{\prime}|\mathbf{x}_{j},\boldsymbol{\theta}_{t}]\,\mathds{1}[ \mathbf{y}\prec\mathbf{y}^{\prime}|\mathbf{x}_{j},\boldsymbol{\theta}_{t-1}]= 1-2\lambda,\]

where the expectation is taken over the randomness of sampling the \(N\) candidate set.

Given Lemma 2.4, we can recover \(\mathcal{C}_{\lambda}\approx 1-\lambda=(1+\tau_{j})/2\) and we use average all \(\tau_{j}\) for all \(\mathbf{x}_{j}\in\mathcal{D}_{\text{U}}\) in Line 11. Finally, in Line 12, we compose the preference dataset by selecting the best response \(\mathbf{y}_{j}^{+}=\mathbf{y}_{i^{+}j}\) and the worst response \(\mathbf{y}_{j}^{-}=\mathbf{y}_{i^{-}j}\) which is similar with (Yuan et al., 2024).

\[\mathcal{D}_{\text{DPO}}=\{(\mathbf{x}_{j},\mathbf{y}_{i^{+}j},\mathbf{y}_{i^ {-}j})|\mathbf{x}_{j}\in\mathcal{D}_{\text{U}},i^{+}=\operatorname*{arg\,min }_{i}J_{ij},i^{-}=\operatorname*{arg\,min}_{i}J_{ij}\}\] (2.11)

Following Theorem 2.3, we also prepare the reverse DPO dataset by switching the best response and the worst response by

\[\mathcal{D}_{\text{RDPO}}=\{(\mathbf{x}_{j},\mathbf{y}_{i^{-}j},\mathbf{y}_{i ^{+}j})|\mathbf{x}_{j}\in\mathcal{D}_{\text{U}},i^{+}=\operatorname*{arg\, min}_{i}J_{ij},i^{-}=\operatorname*{arg\,min}_{i}J_{ij}\}\] (2.12)

and update \(\boldsymbol{\theta}_{t+1}\) by minimizing the empirical loss of Eq. (2.7) in Line 14. The detailed proof of theorems and lemmas are provided in the Appendix B.

## 3 Experiment

### Experimental Setup

**Data.** In our experiments, we use Open Assistant dataset (Kopf et al., 2024) and only reserve about 3.4K human-annotated examples as the seed SFT data \(\mathcal{D}_{\text{S}}\). To construct the unlabeled prompt dataset \(\mathcal{D}_{\text{U}}\), we mix prompts of \(\mathcal{D}_{\text{S}}\) with the train split of each downstream task (only reserve the prompts) including (1) ARC-Easy/Challenge (Clark et al., 2018), (2) OpenBookQA (Mihaylov et al., 2018), (3) SIQA (Sap et al., 2019), and (4) GSM8K (Cobbe et al., 2021). Finally, this process results in a total of 21K prompts in \(\mathcal{D}_{\text{U}}\), which we distribute equally across iterative self-rewarding trainings.

**Models.** Due to limited computational resources, we mainly conduct experiments with two LLMs with about 7B parameters, including Llama-3 (Dubey et al., 2024) and Llama-2 (Touvron et al., 2023).

**Baseline Methods.** To validate our findings, we mainly compare our method with SRLM (Yuan et al., 2024) which uses the same LLM to serve as both the policy and reward model to generated preference data for iterative training. Additionally, we introduce a variant of RL with AI feedback (Guo et al., 2024), referred to as "Oracle". In this variant, the reward model in SRLM is replaced with an external reward model to demonstrate the upper bound performance of SRLM. Specifically, we use InternLM2 (Cai et al., 2024), a specialized trained reward model, to provide the reward scores for the generated responses. We further enhance Oracle's rewarding by leveraging the labels from downstream tasks to improve the rewarding accuracy.

**Implementation Details.** In our experiments, we fine-tune the initial model (M0) on the seed SFT data for \(3\) epochs with a learning rate of \(1e-6\), resulting in model M1. Following SRLM approach, we then iteratively fine-tune the model using the preference learning objective two additional iterations, producing models M2 and M3. In the preference training of each iteration, we set \(\beta=0.1\) of DPO loss, and fine-tune the model for \(1\) epoch with a learning rate of \(1e-6\).

### Main Results

The main results are shown in Table 1 which also report the performance of GPT-4o for reference. From these results, we observe the following: (1) The Standard SRLM fails to achieve satisfactory performance, particularly with Llama2 which has relatively weaker foundation performance even after SFT fine-tuning (M0 \(\rightarrow\) M1), which indicates its limitations for 7B-level LLMs. (2) Compared to SRLM, cream achieves a significant improvement across almost all downstream tasks, showing the advantage of introducing the proposed regularization method. (3) SRLM equipped with an oracle reward model (Oracle) can ensure high rewarding accuracy for annotations of self-generated preference data, thereby achieving the best performance overall. Notably, for Llama3, CREAM even outperforms Oracle except on SIQA dataset, showcasing the superior performance of CREAM. This superiority underlines the success of the proposed method in mitigating the rewarding bias issue. (4) The consistent performance improvements of CREAM across iterations validate the effectiveness of the proposed regularization method in mitigating the rewarding bias issue.

### Analysis

#### 3.3.1 Analysis of Rewarding

**Rewarding Consistency.** We first examine the consistency of rewards of different methods using their corresponding models from the last iteration in Table 2. Here, we use the proposed Consistency Rate \(\mathcal{C}\), Kendall correlation coefficient \(\tau\), Spearman correlation coefficient, and TopOrder metrics to measure the consistency, where the TopOrder metric evaluates whether the final paired preference data remains the same, calculated as follows:

\[\text{TopOrder}_{j}=\mathds{1}\left[\arg\min J_{j}=\arg\min K_{j}\right]\cdot \mathds{1}\left[\arg\max J_{j}=\arg\max K_{j}\right],\]

where \(J_{j}\) and \(K_{j}\) are the rankings of the responses provided by current model and the last iteration's model, respectively. This metric assesses whether both the least preferred and most preferred responses are consistently ranked across iterations. The results confirm that SRLMs exhibit a rewarding consistency issue. In contrast, our method CREAM can keep the ranking consistency across iterations thanks to the explicit regularization in the training.

**Prompt Rewarding v.s. DPO Rewarding.** As aforementioned, 7B level LLMs struggle with generating accurate rewards when using LLMs-as-a-Judge prompting due to their limited capacity. Both Table 5 and Figure 3 clearly show that the SRLM with prompt rewarding is not effective for smaller LLMs, as the performance starts to decrease at the first iteration (M1 \(\rightarrow\) M2) when trained on the self-rewarded preference data. In contrast, the adopted DPO rewarding method can be more suitable for such small LLMs. This is primarily because DPO rewarding is intrinsically aligned with the model's learning objective.

\begin{table}
\begin{tabular}{l|l|c c c c c} \hline \hline Model & Method & Arc-Easy & Arc-Challenge & OpenBookQA & SIQA & GSM8K \\ \hline GPT-4o & CoT & 94.57 & 94.71 & 96.60 & 79.63 & 92.27 \\ \hline \multirow{10}{*}{Llama-3} & M0 & 86.29 & 80.37 & 86.00 & 68.58 & 78.01 \\  & M1 & 86.78 & 80.14 & 86.40 & 69.50 & 78.39 \\ \cline{2-7}  & Oracle M2 & 89.60 \(\uparrow\) & 82.17 \(\uparrow\) & 90.00 \(\uparrow\) & 72.88 \(\uparrow\) & 80.82 \(\uparrow\) \\  & Oracle M3 & 89.31 \(\downarrow\) & 81.31 \(\downarrow\) & 90.20 \(\uparrow\) & 73.75 \(\uparrow\) & 76.04 \(\downarrow\) \\ \cline{2-7}  & SRLM M2 & 87.79 \(\uparrow\) & 80.38 \(\uparrow\) & 87.80 \(\uparrow\) & 70.95 \(\uparrow\) & 78.01 \(\downarrow\) \\  & SRLM M3 & 87.17 \(\downarrow\) & 81.23 \(\uparrow\) & 87.30 \(\uparrow\) & 70.37 \(\downarrow\) & 77.48 \(\downarrow\) \\ \cline{2-7}  & CREAM M2 & 88.89 \(\uparrow\) & 80.39 \(\uparrow\) & 88.00 \(\uparrow\) & 69.79 \(\uparrow\) & 81.04 \(\uparrow\) \\  & CREAM M3 & **89.52 \(\uparrow\)** & **83.36 \(\uparrow\)** & **90.20 \(\uparrow\)** & **72.06 \(\uparrow\)** & **81.73** \(\uparrow\) \\ \hline \multirow{10}{*}{Llama-2} & M0 & 61.07 & 48.98 & 62.20 & 50.36 & 23.65 \\  & M1 & 60.44 & 48.46 & 63.20 & 50.77 & 23.88 \\ \cline{1-1} \cline{2-7}  & Oracle M2 & 70.20 \(\uparrow\) & 55.03 \(\uparrow\) & 75.40 \(\uparrow\) & 63.66 \(\uparrow\) & 30.02 \(\uparrow\) \\ \cline{1-1}  & Oracle M3 & 71.72 \(\uparrow\) & 55.80 \(\uparrow\) & 77.20 \(\uparrow\) & 62.44 \(\downarrow\) & 29.57 \(\uparrow\) \\ \cline{1-1} \cline{2-7}  & SRLM M2 & 58.67 \(\downarrow\) & 46.67 \(\downarrow\) & 59.80 \(\uparrow\) & 49.69 \(\downarrow\) & 25.17 \(\uparrow\) \\ \cline{1-1} \cline{2-7}  & SRLM M3 & 65.5 \(\downarrow\) & 34.47 \(\downarrow\) & 49.20 \(\downarrow\) & 48.06 \(\downarrow\) & 21.24 \(\downarrow\) \\ \cline{1-1} \cline{2-7}  & CREAM M2 & 58.97 \(\downarrow\) & 47.53 \(\downarrow\) & 62.80 \(\downarrow\) & 50.43 \(\downarrow\) & 24.41 \(\uparrow\) \\ \cline{1-1} \cline{2-7}  & CREAM M3 & **62.08 \(\uparrow\)** & **48.81 \(\uparrow\)** & **64.60 \(\uparrow\)** & **51.22 \(\uparrow\)** & **25.85 \(\uparrow\)** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Main results of each method on test sets of downstream tasks. The exact match accuracies are reported. The “\(\uparrow\)” and “\(\downarrow\)” indicate the performance improvement and degradation compared to the method’s last iteration (e.g., M1 \(\rightarrow\) M2 and M2 \(\rightarrow\) M3), respectively. The best performance between SRLMs and CREAM is highlighted in bold.

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline Method & Iteration & Consistency \(\mathbb{C}^{\uparrow}\) & Kendall \(\uparrow\) & Spearman \(\uparrow\) & TopOrder \(\uparrow\) \\ \hline SRLM & M2 & 0.39 \(\pm\) 0.21 & -0.22 \(\pm\) 0.41 & 0.36 \(\pm\) 0.24 & 0.03 \(\pm\) 0.18 \\ CREAM & M2 & 0.73 \(\pm\) 0.18 & 0.46 \(\pm\) 0.35 & 0.77 \(\pm\) 0.19 & 0.19 \(\pm\) 0.39 \\ \hline SRLM & M3 & 0.46 \(\pm\) 0.19 & -0.08 \(\pm\) 0.38 & 0.50 \(\pm\) 0.22 & 0.12 \(\pm\) 0.33 \\ CREAM & M3 & 0.92 \(\pm\) 0.09 & 0.84 \(\pm\) 0.19 & 0.95 \(\pm\) 0.07 & 0.59 \(\pm\) 0.49 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ranking consistency of CREAM and SRLM on M2 and M3 using Llama3.

**Ranking Accuracy.** We present the ranking accuracy in Figure 3 to provide an intuitive comparison the performance of the rewarding performance across different methods. The results include the ranking accuracy on self-generated preference data and the RewardBench (Lambert et al.) dataset, both of which is formulated as a ranking task to predict the preferred one between two responses. We use the self-generated preference data obtained by self-rewarding with ground truth ranking labels, for testing the model's in-domain ranking performance. The RewardBench dataset is used to assess the generalizability of the models beyond the training domain. CREAM consistently achieves higher ranking accuracy than baseline methods, which promises more reliable preference data for training.

#### 3.3.2 Reliability of Self-Consistency

The most straightforward way to enhance the rewarding and ranking accuracy is by incorporating external reward models, such as the SRLM variant "Oracle" used in our experiments. The theoretical analysis in Eq. (2.8) suggests that we can mitigate the rewarding bias issue by calculating the ranking consistency between current model and other available reward models. However, it is not always feasible to have access to such external reward models in practice, such as the self-rewarding scenario. Thus, we instead propose to use the last iteration's model as the reference reward model to measure the consistency of rewards. To test this approach, we fine-tune the same M1 model using CREAM with two different reference reward models: the rewarding function of Oracle and ours model from the last iteration. As shown in Table 3, using a strong reward model as the consistency model can bring better regularization effect, especially for Llama2. However, we find that the last iteration's model also provides a reasonably reliable consistency signal for Llama3.

#### 3.3.3 Consistency Measurement

Besides the adopted Kendall \(\tau\) coefficient, other metrics can also be used to measure the consistency between two preference ranking lists, such as Spearman coefficient (Spearman, 1904) and the aforementioned TopOrder method. We conduct a comparison experiments of using different consistency measurement methods in Table 4. We can observe that: (1) All these measurements are effective with CREAM, indicating the generalization and applicability of our regularized training approach. (2) Kendall correlation coefficient generally yields higher scores across various datasets compared to Spearman and TopOrder methods.

## 4 Conclusion

In this paper, we first formulate a generalized iterative preference fine-tuning framework for self-rewarding language models (SRLMs), which is also applicable to other iterative preference training methods. Then, we highlight the rewarding bias that emerges from overconfident preference labeling, which is particularly problematic for smaller LLMs, such as those with 7B parameters. This rewarding bias results in the accumulation of noisy and unreliable preference data, harming the preference training and hindering alignment performance of LLMs. To address this issue, we proposed the **C**onsistency **R**egularized **s**E**lf-Rewarding **I**A**nguage **M**odel (CREAM), which leverages the consistency of rewards across different iterations as a regularization signal. This approach allows the model to learn more selectively, emphasizing reliable preference data and avoiding overconfidence in preference labeling. Our experimental results on various natural language benchmarks demonstrate the effectiveness of the proposed method in mitigating the rewarding bias issue and improving the performance of SRLMs. We believe that these findings can provide valuable insights for future research on self-improvement methods of LLM alignment.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Method & Arc-E & Arc-C & OBQA & SIQA & GSMsk \\ \hline Spearman M2 & 86.95 & **82.00** & 85.40 & 70.05 & 78.77 \\ TopOrder MA & 87.25 & 80.12 & 86.88 & **70.83** & 79.75 \\ Kendall M2 & **88.89** & 80.89 & **88.00** & **69.79** & **81.04** \\ \hline Spearman M3 & 88.76 & 81.83 & 90.00 & 70.98 & 79.15 \\ TopOrder M3 & 88.51 & 80.37 & 87.40 & 71.03 & 79.76 \\ Kendall M3 & **89.52** & **83.36** & **90.20** & **72.06** & **81.73** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of CREAM with different consistency measurements.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Method & Arc-E & Arc-C & OBQA & SIQA & GSMsk \\ \hline Llama3 M1 & 86.78 & 80.14 & 86.40 & 69.50 & 78.39 \\ CREAM & 88.89 & 80.89 & 88.00 & 69.79 & 81.04 \\ CREAM + Oracle & 88.51 & 81.06 & 86.20 & 72.21 & 79.91 \\ \hline Llama2 M1 & 60.44 & 48.46 & 63.20 & 50.77 & 23.88 \\ CREAM & 58.97 & 47.53 & 62.80 & 50.43 & 24.41 \\ CREAM + Oracle & 62.42 & 48.72 & 66.00 & 51.13 & 22.52 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of CREAM with oracle reward model and last iteration’s model.

## References

* Abdi (2007) Herve Abdi. The kendall rank correlation coefficient. _Encyclopedia of measurement and statistics_, 2:508-510, 2007.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Azar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.
* Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* Bai et al. (2023) Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking foundation models with language-model-as-an-examiner. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* Bradley and Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, and et al. Internlm2 technical report, 2024.
* Chen et al. (2024) Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. _arXiv preprint arXiv:2401.01335_, 2024.
* Christiano et al. (2017) Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 4299-4307, 2017.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. _CoRR_, abs/1803.05457, 2018.
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _CoRR_, abs/2110.14168, 2021.
* Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. _ArXiv preprint_, abs/2304.06767, 2023.
* Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and et al. The llama 3 herd of models, 2024.
* Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024.

* Guo et al. (2024a) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct language model alignment from online AI feedback. _CoRR_, abs/2402.04792, 2024a.
* Guo et al. (2024b) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. _arXiv preprint arXiv:2402.04792_, 2024b.
* Hong et al. (2024) Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. _arXiv preprint arXiv:2403.07691_, 2(4):5, 2024.
* Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. _arXiv preprint arXiv:2210.11610_, 2022.
* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Comput. Surv._, 55:248:1-248:38, 2023.
* Kendall (1938) Maurice G Kendall. A new measure of rank correlation. _Biometrika_, 30(1-2):81-93, 1938.
* Kopf et al. (2024) Andreas Kopf, Yannic Kilcher, Dimitri von Rute, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richard Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lambert et al. (2018) Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. _CoRR_, abs/2403.13787.
* Leike et al. (2018) Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. _arXiv preprint arXiv:1811.07871_, 2018.
* McLeod (2005) A Ian McLeod. Kendall rank correlation and mann-kendall trend test. _R package Kendall_, 602:1-10, 2005.
* Meng et al. (2024) Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_, 2018.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022a.
* December 9, 2022_, 2022b.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1:9, 2019.
* 16, 2023_, 2023.
* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. _CoRR_, abs/1904.09728, 2019.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _ArXiv preprint_, abs/1707.06347, 2017.
* Spearman (1904) C Spearman. The proof and measurement of association between two things. _The American Journal of Psychology_, 15(1):72-101, 1904.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, and et al. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023.
* Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024.
* Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. _arXiv preprint arXiv:2304.05302_, 2023.
* Zelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. _Advances in Neural Information Processing Systems_, 35:15476-15488, 2022.
* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llvm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36:46595-46623, 2023.
* Zhou et al. (2024) Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. _arXiv preprint arXiv:2405.14622_, 2024.
* Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.
* Zou et al. (2019) Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. Confidence regularized self-training. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 5982-5991, 2019.

Related Works

This paper mainly focuses on mitigating the rewarding bias issue in self-rewarding language models (SRLMs) (Yuan et al., 2024), which is a type of self-improvement method for LLM alignment. In this section, we introduce the progresses in LLM alignment and discuss the SRLMs in detail.

**LLM Alignment.** Alignment lies at the core of LLM research and applications, aiming to ensure that LLMs adhere to human values and preferences. RLHF established the foundational alignment training paradigm (Leike et al., 2018; Ziegler et al., 2019; Ouyang et al., 2022), where it leverages human preference feedback to train a reward model, and then use this reward model to guide the LLM via reinforcement learning algorithms (Schulman et al., 2017). Recent efforts have been made to develop direct alignment methods (Rafailov et al., 2023; Dong et al., 2023; Azar et al., 2023; Ethayarajh et al., 2024; Meng et al., 2024; Hong et al., 2024), in order to reduce the costs and complexity of RLHF and make it more efficient and accessible. Representatively, DPO (Rafailov et al., 2023) as a representative direct alignment method, optimizes the LLM with annotated preference pairs, eliminating the need of training an additional reward model. However, most RLHF and direct alignment methods heavily rely on human-annotated preference data, where the data collection commonly involves human to distinguish the "good" responses from the "bad" ones, which is time-consuming and labor-intensive (Ouyang et al., 2022; Bai et al., 2022). Thus, synthesizing preference data with minimal human efforts has become a valuable research direction.

**Self-Rewarding Language Model.** SRLM (Yuan et al., 2024) has emerged as a promising approach to address the challenge of preference data synthesis in a self-improvement manner. This method leverages the LLM itself to act as both the policy model and the reward model. The policy model can generate response candidates for unlabeled prompts, while the reward model uses LLM-as-A-Judge (Zheng et al., 2023; Bai et al., 2023; Dubois et al., 2024) prompting to reward and rank these responses based on their quality. The ranked responses are then used as preference pairs to train the LLM via DPO (Rafailov et al., 2023). And this process can be iteratively repeated to improve the alignment performance without human intervention. However, having the same LLM serve as both the policy and reward model, without any regularization, presents challenges in guaranteeing accurate rewards. This can lead to accumulated bias and noisy preference data, which ultimately harms the training. Other similar self-improvement methods (Huang et al., 2022; Zelikman et al., 2022; Chen et al., 2024; Guo et al., 2024; Zhou et al., 2024) often either use the ground truth response to avoid annotation bias, or introduce an additional reward model to reduce the noise in annotations. In contrast, our work neither requires labeled data nor relies on external LLMs. Instead, we propose to use the consistency of rewards to mitigate the rewarding bias in SRLMs.

Figure 2: The flow of CREAM. In the response sampling stage, the policy model \(\pi_{\theta_{t}}\) generates \(N\) responses. After that, CREAM uses the reward model \(R_{\theta_{t-1}}\) from the previous iteration to reward and rank these responses. Then, the rankings are compared with those generated by current reward model \(R_{\theta_{t}}\) to estimate the consistency rate. Finally, the policy model \(\pi_{\theta_{t}}\) is fine-tuned with consistency regularized preference training objective, resulting in the model \(\pi_{\theta_{t+1}}\) for next iteration.

## Appendix B Proof of the theorems and lemmas

### Proof of Theorem 2.1

Proof of Theorem 2.1.: We denote iteration of the two-step algorithm as \(t\). The algorithm starts from \((\bm{\theta}_{t},z_{t})\), and obtains \(z_{t+1}=z_{\bm{\theta}_{t}}\) according to Eq. (2.3) in the preference-labeling step and then obtains \(\bm{\theta}_{t+1}\) through the learning step. Since \(z_{t+1}=\arg\min_{z}\mathcal{L}_{\text{DPO}}(\bm{\theta}_{t};\mathbf{y}, \mathbf{y}^{\prime},\mathbf{x},z)\) for any \(\mathbf{y},\mathbf{y}^{\prime},\mathbf{x}\) according to Eq. (2.3), we have that

\[\mathcal{L}(\bm{\theta}_{t},z_{t+1})\leq\mathcal{L}(\bm{\theta}_{t},z_{t}).\] (B.1)

And the learning step suggests that \(\bm{\theta}_{t+1}=\arg\min_{\bm{\theta}}\mathcal{L}(\bm{\theta},z_{t+1})\), yielding that

\[\mathcal{L}(\bm{\theta}_{t+1},z_{t+1})\leq\mathcal{L}(\bm{\theta}_{t},z_{t+1}).\] (B.2)

Connecting Eq. (B.1) with Eq. (B.2) yields that the loss function \(\mathcal{L}(\bm{\theta},z)\) is monotonically decreasing, i.e.

\[\cdots\leq\mathcal{L}(\bm{\theta}_{t+1},z_{t+1})\leq\mathcal{L}(\bm{\theta}_ {t},z_{t+1})\leq\mathcal{L}(\bm{\theta}_{t},z_{t})\leq\cdots.\] (B.3)

Since \(\mathcal{L}(\bm{\theta},z)\) is upper bounded by \(0\), it suggests that the sequence of \(\mathcal{L}(\bm{\theta}_{t},z_{t})\) will converge w.r.t. the growth of \(t\).

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Dataset & M1 & M2 & M3 \\ \hline Arc-E & 86.78 & 84.64\(\downarrow\) & 83.75\(\downarrow\) \\ Arc-C & 80.14 & 76.79\(\downarrow\) & 76.28\(\downarrow\) \\ OBQA & 86.40 & 80.40\(\downarrow\) & 80.20\(\downarrow\) \\ SIQA & 69.50 & 67.81\(\downarrow\) & 66.63\(\downarrow\) \\ GSM8K & 78.39 & 78.47\(\uparrow\) & 78.99\(\uparrow\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results for SRLM with prompt rewarding method using Llama3. The dataset names are abbreviated.

Figure 3: Ranking accuracy on RewardBench (left) and self-generated preference data (right). P-SRLM is SRLM with prompt rewarding.

### Proof of Lemma 2.2

Proof of Lemma 2.2.: We start by expanding \(\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot| \mathbf{x})}\mathcal{L}_{\text{reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})\) as

\[\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t} }(\cdot|\mathbf{x})}\mathcal{L}_{\text{reg}}(\bm{\theta};\mathbf{y},\mathbf{y}^ {\prime},\mathbf{x}) =\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x})}\left[\log P_{\bm{\theta}}(z=1)+\log P_{\bm{\theta}}(z=0)\right]\] \[=\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x}),\mathbf{y}\geq\mathbf{y}^{\prime}}\left[\log P_{\bm{ \theta}}(z=1)+\log P_{\bm{\theta}}(z=0)\right]\] \[=\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x})}P_{\bm{\theta}_{t}}(z=0)\left[\log P_{\bm{\theta}}(z=1)+ \log P_{\bm{\theta}}(z=0)\right]\] \[\quad+\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{ \theta}_{t}}(\cdot|\mathbf{x})}P_{\bm{\theta}_{t}}(z=1)\left[\log P_{\bm{ \theta}}(z=1)+\log P_{\bm{\theta}}(z=0)\right],\] (B.4)

where the second equation decompose the expectation \(\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot| \mathbf{x})}\) into two expectation \(\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{t}}(\cdot| \mathbf{x})}+\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x})}\), the third equation extract the event \(\mathbf{y}\geq\mathbf{y}^{\prime}\) as distribution \(P_{\bm{\theta}_{t}}(z)\). Then Eq. (B.4) can be further written by

\[\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x})}\mathcal{L}_{\text{reg}}(\bm{\theta};\mathbf{y}, \mathbf{y}^{\prime},\mathbf{x}) =P_{\bm{\theta}_{t}}(z=1)\left[\log P_{\bm{\theta}}(z=1)+\log P_{ \bm{\theta}}(z=0)\right]\] \[\quad+P_{\bm{\theta}_{t}}(z=0)\left[\log P_{\bm{\theta}}(z=1)+ \log P_{\bm{\theta}}(z=0)\right],\] (B.5)

since both \(\mathbf{y},\mathbf{y}^{\prime}\) are generated from \(\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})\), \(P_{\bm{\theta}_{t}}(z=0)=P_{\bm{\theta}_{t}}(z=1)=0.5\). Thus Eq. (B.5) becomes

\[\mathbb{E}_{\mathbf{y},\mathbf{y}^{\prime}\sim\pi_{\bm{\theta}_{ t}}(\cdot|\mathbf{x})}\mathcal{L}_{\text{reg}}(\bm{\theta};\mathbf{y}, \mathbf{y}^{\prime},\mathbf{x}) =2\mathrm{KL}(P_{\bm{\theta}}(z)\parallel P_{\bm{\theta}_{t}}(z))=2 \mathrm{KL}(P_{\bm{\theta}}(z)\parallel u(z)),\] (B.6)

where \(u(z)\) is the uniform binary distribution with \(u(z=0)=u(z=1)=0.5\). 

### Proof Theorem 2.3

Proof of Theorem 2.3.: We start by writing down each components in \(\mathcal{L}(\bm{\theta},z)\) defined in Eq. (2.1) by

\[\mathcal{L}(\bm{\theta},z) =\mathcal{L}_{\text{ST}}(\bm{\theta};\mathcal{D}_{\text{S}})+ \mathbb{E}_{\mathbf{x}\sim\mathcal{D}^{\prime},\mathbf{y},\mathbf{y}^{\prime} \sim\pi_{\bm{\theta}_{t}}(\cdot|\mathbf{x})}[\mathcal{L}_{\text{DPO}}(\bm{ \theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x},z)+\lambda\mathcal{L}_{ \text{Rq}}(\bm{\theta};\mathbf{y},\mathbf{y}^{\prime},\mathbf{x})]\] \[\quad+\mathbb{E}_{\mathbf{x

[MISSING_PAGE_FAIL:15]