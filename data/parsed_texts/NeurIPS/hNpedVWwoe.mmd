# Near Optimal Reconstruction

of Spherical Harmonic Expansions

 Amir Zandieh

Independent Researcher

amir@zed512.gmail.com

&Insu Han

Yale University

insu.han@yale.edu

&Haim Avron

Tel Aviv University

haimav@tauex.tau.ac.il

###### Abstract

We propose an algorithm for robust recovery of the spherical harmonic expansion of functions defined on the \(d\)-dimensional unit sphere \(\mathbb{S}^{d-1}\) using a near-optimal number of function evaluations. We show that for any \(f\in L^{2}(\mathbb{S}^{d-1})\), the number of evaluations of \(f\) needed to recover its degree-\(q\) spherical harmonic expansion equals the dimension of the space of spherical harmonics of degree at most \(q\), up to a logarithmic factor. Moreover, we develop a simple yet efficient kernel regression-based algorithm to recover degree-\(q\) expansion of \(f\) by only evaluating the function on uniformly sampled points on \(\mathbb{S}^{d-1}\). Our algorithm is built upon the connections between spherical harmonics and Gegenbauer polynomials. Unlike the prior results on fast spherical harmonic transform, our proposed algorithm works efficiently using a nearly optimal number of samples in any dimension \(d\). Furthermore, we illustrate the empirical performance of our algorithm on numerical examples.

## 1 Introduction

We consider the fundamental problem of recovering a function from a finite number of (noisy) observations. To provide accurate and reliable predictions at unobserved points we need to avoid overfitting which is typically achieved through restricting our estimator or interpolant to a family of _smooth or structured_ functions. In this paper, we focus on interpolating square-integrable functions on the \(d\)-dimensional unit sphere, with low-degree spherical harmonics, a critical task in scenarios where rotational invariance is a fundamental property. Spherical harmonics are essential in various theoretical and practical applications, including the representation of electromagnetic fields [14], gravitational potential [20], cosmic microwave background radiation [11] and medical imaging [15], as well as modelling of 3D shapes in computer graphics [10] and computer vision [12]. Further notable real-life applications include molecular/atom systems, where understanding the underlying functions within a spherical context can significantly enhance predictive modeling and simulation accuracy [1, 13, 14].

We begin by observing that any function \(f\) in \(L^{2}(\mathbb{S}^{d-1})\), i.e., the family of square-integrable functions on the sphere \(\mathbb{S}^{d-1}\), can be uniquely decomposed into orthogonal spherical harmonic components. Specifically, if we denote the space of spherical harmonics of degree \(\ell\) in dimension \(d\) by \(\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\), any \(f\in L^{2}(\mathbb{S}^{d-1})\) has a unique orthogonal expansion \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) with \(f_{\ell}\in\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\) (Lemma 2). With this observation, we aim to find the best spherical harmonic approximation of degree \(\leq q\) to \(f\) using minimal number of samples (essentially treating higher order terms in \(f\)'s expansion as noise).

**Problem 1** (Informal Version of Problem 2).: _For an unknown function \(f\in L^{2}(\mathbb{S}^{d-1})\) and an integer \(q\geq 1\), efficiently (both in terms of number of samples from \(f\) and computations) learn the first \(q+1\) spherical harmonic components \(\left\{f_{\ell}\in\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\right\}_{\ell=0}^{q}\) of \(f\) which minimize_

\[\left\|\sum_{\ell=0}^{q}f_{\ell}-f\right\|_{\mathbb{S}^{d-1}}^{2}:=\int_{ \mathbb{S}^{d-1}}\left|\sum_{\ell=0}^{q}f_{\ell}(w)-f(w)\right|^{2}dw.\] (1)

The _angular power spectrum_ of \(f\) commonly obeys a power law decay of the form \(\left\|f_{\ell}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\mathcal{O}(\ell^{-s})\), for some \(s>0\), depending on the order of differentiability of \(f\). In fact, for any infinitely differentiable \(f\), \(\left\|f_{\ell}\right\|_{\mathbb{S}^{d-1}}^{2}\) decays asymptotically faster than any rational function of \(\ell\). Furthermore, for any real analytic \(f\) on the sphere, \(\left\|f_{\ell}\right\|_{\mathbb{S}^{d-1}}^{2}\) decays exponentially. Thus, the first \(q+1\) spherical harmonic components of \(f\) typically well approximate \(f\) for even modest \(q\), and answering Problem 1 is meaningful for a wide range of differentiable functions.

### Our Main Results

We reformulate Problem 1 as a least-squares regression and then solve it using randomized numerical linear algebra techniques. We first consider an orthonormal projection operator that maps functions in \(L^{2}(\mathbb{S}^{d-1})\) onto the space of bounded-degree spherical harmonics \(\bigoplus_{\ell=0}^{q}\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\). Specifically, if \(\mathcal{K}_{d}^{(q)}\) is an operator that maps any \(f\) with expansion \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) where \(f_{\ell}\in\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\), onto \(f\)'s first \(q+1\) expansion components, i.e., \(\mathcal{K}_{d}^{(q)}f=\sum_{\ell=0}^{q}f_{\ell}\), then Problem 1 can be formulated as

\[\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f\right\|_{ \mathbb{S}^{d-1}}^{2}.\]

However, solving this regression problem with "continuous" cost function is challenging. To avoid such continuous optimizations, we adopt the approach of [1] which discretizes the aforementioned regression problem according to the leverage scores of the operator \(\mathcal{K}_{d}^{(q)}\). It turns out that if we can draw random samples with probabilities proportional to the leverage scores of \(\mathcal{K}_{d}^{(q)}\) then we can recover the degree-\(q\) spherical harmonic expansion of \(f\), i.e. \(\sum_{\ell=0}^{q}f_{\ell}\), with finite number of observations. Particularly, by exploiting the connections between spherical harmonics and _Zonal (Gegenbauer) Harmonics_ and the fact that zonal harmonics are the reproducing kernels of \(\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\) (Lemma 3), we prove that the leverage scores of \(\mathcal{K}_{d}^{(q)}\) are constant everywhere on the sphere \(\mathbb{S}^{d-1}\). Thus, solving a discrete regression problem with uniformly sampled observations yields a near-optimal solution to Problem 1. Informal statements of our results are as follows.

**Theorem 1** (Informal Version of Theorem 5).: _Let \(\beta_{q,d}\) be the dimension of spherical harmonics of degree at most \(q\), i.e., \(\beta_{q,d}\equiv\dim\left(\bigoplus_{\ell=0}^{q}\mathcal{H}_{\ell}(\mathbb{S} ^{d-1})\right)\). There exists an algorithm that finds a \((1+\varepsilon)\)-approximation to the optimal solution of Problem 1, given \(s=\mathcal{O}(\beta_{q,d}\log\beta_{q,d}+\varepsilon^{-1}\beta_{q,d})\) observations of function \(f\) at uniformly sampled points on \(\mathbb{S}^{d-1}\), with \(\mathcal{O}(s^{2}d+s^{\omega})\)1 runtime._

Footnote 1: \(\omega<2.3727\) is the exponent of the fast matrix multiplication algorithm [16]

We also prove that our bound on the number of required samples is optimal up to a logarithmic factor.

**Theorem 2** (Informal Version of Theorem 6).: _Any (randomized) algorithm that takes \(s<\beta_{q,d}\) samples on any input fails with probability greater than \(9/10\), where \(\beta_{q,d}\equiv\dim\left(\bigoplus_{\ell=0}^{q}\mathcal{H}_{\ell}(\mathbb{S} ^{d-1})\right)\)._

### Related Work

Efficient reconstruction of functions as per Problem 1 has been extensively studied in various fields. Many prior papers considered reconstructing \(1\)-dimensional functions from finite number of samples on a finite interval under smoothness assumption about the underlying function. Notably, the influential line of work of [12, 13, 14, 15] focused on reconstructing Fourier-bandlimited functions and [1, 13, 1, 1] considered interpolating Fourier-sparse signals. Recently, [1] unified these reconstruction methods in dimension \(d=1\) and gave a universal sampling framework for reconstructing nearly all classes of functions with Fourier-based smoothness constraints. One can view \(1\)-dimensional functions on a finite interval as functions on the unit circle\(\mathbb{S}^{1}\). Thus, Problem 1 is indeed a generalization of the aforementioned prior work to high dimensions under the assumption that the _generalized Fourier series_ (Lemma 2) of the underlying function only contains bounded-degree spherical harmonics. This degree constraint on spherical harmonic expansions can be viewed as the \(d\)-dimensional analog of Fourier-bandlimitedness on circle \(\mathbb{S}^{1}\).

Computing the spherical harmonic expansion in dimension \(d=3\) has received considerable attention in physics and applied mathematics communities. The algorithms for this special case of Problem 1 are known in the literature as "fast spherical harmonic transform" [14, 15]. Most notably, [16] proposed an algorithm for computing spherical harmonic expansion of degree \(\leq q\) to precision \(\varepsilon\) using \(\mathcal{O}(\beta_{g,3})\) samples and \(\mathcal{O}(\beta_{g,3}\log\beta_{g,3}.\log(1/\varepsilon))\) time. These fast algorithms were developed based on fast Fourier and associated Legendre transforms and make use of a (well-conditioned) orthogonal basis for \(\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\), which happened to be the associated Legendre polynomials when \(d=3\). However, it is in general very difficult to compute an orthogonal basis for spherical harmonics [13], so unlike our Theorem 1, it is inefficient to extend these prior results to higher \(d\).

From the techniques point of view, a related work is [11], which employs harmonic analysis over \(\mathbb{S}^{d-1}\) to analyze the generalization of two-layered neural tangent kernels. They show that an unknown function defined on \(\mathbb{S}^{d-1}\) can be efficiently recovered using kernel regression w.r.t. neural tangent kernel on uniform random samples from the function. However, the number of samples that [11] requires for recovering bounded degree spherical harmonics, especially when the degrees are high, is sub-optimal and is strictly worse than our result. Additionally, [11] does not guarantee recovery with relative error, while our Theorem 5 provides relative error guarantees.

Furthermore, recent applications of Gegenbauer polynomials, along with other orthonormal polynomials like Hermite polynomials, have been found in designing efficient random features for approximating various kernel functions. These applications extend to dot-product kernels [10], Neural Tangent Kernels [15, 16], and Gaussian kernels [14, 1].

## 2 Mathematical Preliminaries

We denote by \(\mathbb{S}^{d-1}\) the unit sphere in \(d\) dimension. We use \(|\mathbb{S}^{d-1}|=\frac{2\pi^{d/2}}{\Gamma(d/2)}\) to denote the surface area of sphere \(\mathbb{S}^{d-1}\) and \(\mathcal{U}(\mathbb{S}^{d-1})\) to denote the uniform probability distribution on \(\mathbb{S}^{d-1}\). We denote by \(L^{2}\left(\mathbb{S}^{d-1}\right)\) the set of all square-integrable real-valued functions on sphere \(\mathbb{S}^{d-1}\). Furthermore, for any \(f,g\in L^{2}\left(\mathbb{S}^{d-1}\right)\) we use the following definition of the inner product on the unit sphere2,

Footnote 2: Formally, \(L^{2}\left(\mathbb{S}^{d-1}\right)\) is a space of equivalence classes of functions that differ at a set of points with measure \(0\). For notational simplicity, here and throughout we use \(f\) to denote the specific representative of the equivalence class \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\). In this way, we can consider the point-wise value \(f(w)\) for every \(w\in\mathbb{S}^{d-1}\).

\[\langle f,g\rangle_{\mathbb{S}^{d-1}}\coloneqq\int_{\mathbb{S}^{d-1}}f(w)g(w) dw=\left|\mathbb{S}^{d-1}\right|\cdot\mathop{\mathbb{E}}_{w\sim\mathcal{U}( \mathbb{S}^{d-1})}[f(w)g(w)].\] (2)

The function space \(L^{2}\left(\mathbb{S}^{d-1}\right)\) is complete with respect to the norm induced by the above inner product, i.e. \(\|f\|_{\mathbb{S}^{d-1}}\coloneqq\sqrt{\langle f,f\rangle_{\mathbb{S}^{d-1}}}\), so \(L^{2}\left(\mathbb{S}^{d-1}\right)\) is a _Hilbert space_.

We often use the term _quasi-matrix_ which is informally defined as a "matrix" in which one dimension is finite while the other is infinite. A quasi-matrix can be _tall_ (or _wide_) meaning that there is a finite number of columns (or rows) where each one is a functional operator. For a more formal definition, see [1].

_Spherical Harmonics_ are the solutions of Laplace's equation in spherical domains and can be thought of as functions defined on \(\mathbb{S}^{d-1}\) employed in solving partial differential equations. Formally,

**Definition 1** (Spherical Harmonics).: _For integers \(\ell\geq 0\) and \(d\geq 1\), let \(\mathcal{P}_{\ell}(d)\) be the space of degree-\(\ell\) homogeneous polynomials with \(d\) variables and real coefficients. Let \(\mathcal{H}_{\ell}(d)\) denote the space of degree-\(\ell\) harmonic polynomials in dimension \(d\), i.e., homogeneous polynomial solutions of Laplace's equation:_

\[\mathcal{H}_{\ell}(d):=\{P\in\mathcal{P}_{\ell}(d):\Delta P=0\},\]

_where \(\Delta=\frac{\partial^{2}}{\partial x_{1}^{2}}+\cdots+\frac{\partial^{2}}{ \partial x_{d}^{2}}\) is the Laplace operator on \(\mathbb{R}^{d}\). Finally, let \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) be the space of (real) Spherical Harmonics of order \(\ell\) in dimension \(d\), i.e. restrictions of harmonic polynomials in \(\mathcal{H}_{\ell}(d)\) to the sphere \(\mathbb{S}^{d-1}\). The dimension of this space, \(\alpha_{\ell,d}\equiv\dim\left(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\right)\), is_

\[\alpha_{0,d}=1,\ \ \alpha_{1,d}=d,\ \ \alpha_{\ell,d}=\binom{d+\ell-1}{\ell}- \binom{d+\ell-3}{\ell-2}\ \ \ \text{for }\ell\geq 2.\]

### Gegenbauer Polynomials

The _Gegenbauer_ (a.k.a. _ultraspherical_) _polynomial_ of degree \(\ell\geq 0\) in dimension \(d\geq 2\) is given by

\[P_{d}^{\ell}(t):=\sum_{j=0}^{\left\lfloor\ell/2\right\rfloor}c_{j}\cdot t^{ \ell-2j}\cdot(1-t^{2})^{j},\] (3)

where \(c_{0}=1\) and \(c_{j+1}=-\frac{(\ell-2j)(\ell-2j-1)}{(2j+1)(d-1+2j)}\cdot c_{j}\) for \(j=0,1,\ldots,\left\lfloor\ell/2\right\rfloor-1\). These polynomials are orthogonal on the interval \([-1,1]\) with respect to the measure \((1-t^{2})^{\frac{d-3}{2}}\), i.e.,

\[\int_{-1}^{1}P_{d}^{\ell}(t)\cdot P_{d}^{\ell^{\prime}}(t)\cdot(1-t^{2})^{ \frac{d-3}{2}}\,dt=\frac{\left|\mathbb{S}^{d-1}\right|}{\alpha_{\ell,d}\cdot \left|\mathbb{S}^{d-2}\right|}\cdot\mathbbm{1}_{\left\{\ell=\ell^{\prime} \right\}}.\] (4)

Zonal Harmonics.The Gegenbauer polynomials naturally provide positive definite dot-product kernels on \(\mathbb{S}^{d-1}\) known as _Zonal Harmonics_, which are closely related to the spherical harmonics. The following reproducing property of zonal harmonics plays a crucial role in our analysis.

**Lemma 1** (Reproducing Property of Zonal Harmonics).: _Let \(P_{d}^{\ell}(\cdot)\) be the Gegenbauer polynomial of degree \(\ell\) in dimension \(d\). For any \(x,y\in\mathbb{S}^{d-1}\):_

\[P_{d}^{\ell}(\left\langle x,y\right\rangle)=\alpha_{\ell,d}\cdot\mathbb{E}_{w \sim\mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[P_{d}^{\ell}\left(\left\langle x,w\right\rangle\right)P_{d}^{\ell}\left(\left\langle y,w\right\rangle\right) \right],\]

_Furthermore, for any \(\ell^{\prime}\neq\ell\):_

\[\mathbb{E}_{w\sim\mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[P_{d}^{\ell }\left(\left\langle x,w\right\rangle\right)\cdot P_{d}^{\ell^{\prime}}\left( \left\langle y,w\right\rangle\right)\right]=0.\]

The proof of this and all subsequent results can be found in the appendix. The following useful fact, known as the addition theorem, connects Gegenbauer polynomials and spherical harmonics.

**Theorem 3** (Addition Theorem).: _For every integer \(\ell\geq 0\), if \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) is an orthonormal basis for \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\), then for any \(\sigma,w\in\mathbb{S}^{d-1}\) we have_

\[\frac{\alpha_{\ell,d}}{\left|\mathbb{S}^{d-1}\right|}\cdot P_{d}^{\ell}\left( \left\langle\sigma,w\right\rangle\right)=\sum_{j=1}^{\alpha_{\ell,d}}y_{j}^{ \ell}(\sigma)\cdot y_{j}^{\ell}(w).\]

## 3 Reconstructing \(L^{2}\left(\mathbb{S}^{d-1}\right)\) Functions via Spherical Harmonics

In this section we show how to approximate any function \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) by spherical harmonics using the optimal number of samples. We begin with the fact that spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis for the Hilbert space of square-integrable functions on sphere \(\mathbb{S}^{d-1}\). This is analogous to periodic functions, viewed as functions defined on the circle \(\mathbb{S}^{1}\), which can be expressed as a linear combination of circular functions (sines and cosines) via the Fourier series.

**Lemma 2** (Direct Sum Decomposition of \(L^{2}(\mathbb{S}^{d-1})\)).: _The family of spaces \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) yields a Hilbert space direct sum decomposition \(L^{2}\left(\mathbb{S}^{d-1}\right)=\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{ \ell}\left(\mathbb{S}^{d-1}\right)\): the summands are closed and pairwise orthogonal, and every \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) is the sum of a converging series (in the sense of mean-square convergence with the \(L^{2}\)-norm defined in Eq. (2)),_

\[f=\sum_{\ell=0}^{\infty}f_{\ell},\]

_where \(f_{\ell}\in\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) are uniquely determined functions. Furthermore, given any orthonormal basis \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) of \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) we have \(f_{\ell}=\sum_{j=1}^{\alpha_{\ell,d}}\langle f,y_{j}^{\ell}\rangle_{\mathbb{S} ^{d-1}}\cdot y_{j}^{\ell}\)._The series expansion in Lemma 2 is the analog of the Fourier expansion of periodic functions, and is known as "_generalized Fourier series_" [10] with respect to the Hilbert basis \(\left\{y_{j}^{\ell}:j\in\left[\alpha_{\ell,d}\right],\ell\geq 0\right\}\). We remark that it is in general intractable to compute an orthogonal basis for the space of spherical harmonics [11], which renders the generalized Fourier series expansion in Lemma 2 primarily existential. While finding the generalized Fourier expansion of a function \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) is computationally intractable, our goal is to answer the next fundamental question, which is about finding the projection of a function \(f\) onto the space of spherical harmonics, i.e., the \(f_{\ell}\)'s in Lemma 2. Concretely, we seek to solve the following problem.

**Problem 2**.: _For an integer \(q\geq 0\) and a given function \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) whose decomposition over the Hilbert sum \(\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) is \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) as per Lemma 2, let us define the low-degree expansion of this function as \(f^{(q)}:=\sum_{\ell=0}^{q}f_{\ell}\). How efficiently can we learn \(f^{(q)}\in\bigoplus_{\ell=0}^{q}\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\)? More precisely, we want to find a set \(\left\{w_{1},w_{2},\ldots,w_{s}\right\}\subseteq\mathbb{S}^{d-1}\) with minimal cardinality \(s\) along with an efficient algorithm that given samples \(\left\{\hat{f}(w_{i})\right\}_{i=1}^{s}\) can interpolate \(f(\cdot)\) with a function \(\tilde{f}^{(q)}\in\bigoplus_{\ell=0}^{q}\mathcal{H}_{\ell}\left(\mathbb{S}^{d -1}\right)\) such that:_

\[\left\|\tilde{f}^{(q)}-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\varepsilon \cdot\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

For ease of notation, we denote the Hilbert space of spherical harmonics of degree at most \(q\) by \(\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right):=\bigoplus_{\ell=0}^{q} \mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\). To answer Problem 2 we exploit the close connection between the spherical harmonics and Gengenbauer polynomials, and in particular the fact that zonal harmonics are the reproducing kernels of the Hilbert spaces \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\).

**Lemma 3** (A Reproducing Kernel for \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\)).: _For every \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\), if \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) is the unique decomposition of \(f\) over \(\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) as per Lemma 2, then \(f_{\ell}\) is given by_

\[f_{\ell}(\sigma)=\alpha_{\ell,d}\cdot\operatorname*{\mathbb{E}}_{w\sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[f(w)P_{d}^{\ell}\left(\left\langle \sigma,w\right\rangle\right)\right]\quad\text{for }\sigma\in\mathbb{S}^{d-1}.\]

Now we define a kernel operator, based on the low-degree Gegenbauer polynomials, which projects functions onto their low-degree spherical harmonic expansion.

**Definition 2** (Projection Operator onto \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\)).: _For any integers \(q\geq 0\) and \(d\geq 2\), define the kernel operator \(\mathcal{K}_{d}^{(q)}:L^{2}\left(\mathbb{S}^{d-1}\right)\to L^{2}\left( \mathbb{S}^{d-1}\right)\) as follows: for \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) and \(\sigma\in\mathbb{S}^{d-1}\),_

\[\left[\mathcal{K}_{d}^{(q)}f\right](\sigma):=\sum_{\ell=0}^{q}\frac{\alpha_{ \ell,d}}{\left|\mathbb{S}^{d-1}\right|}\left\langle f,P_{d}^{\ell}\left( \left\langle\sigma,\cdot\right\rangle\right)\right\rangle_{\mathbb{S}^{d-1}}= \sum_{\ell=0}^{q}\alpha_{\ell,d}\cdot\operatorname*{\mathbb{E}}_{w\sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[f(w)P_{d}^{\ell}\left(\left\langle \sigma,w\right\rangle\right)\right].\] (5)

_This is an integral operator with kernel function \(k_{q,d}(\sigma,w):=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{\left|\mathbb{S}^{d -1}\right|}\cdot P_{d}^{\ell}\left(\left\langle\sigma,w\right\rangle\right)\)._

Note that the operator \(\mathcal{K}_{d}^{(q)}\) is self-adjoint and positive semi-definite. Moreover, using the reproducing property of this kernel we can establish that \(\mathcal{K}_{d}^{(q)}\) is a projection operator.

**Claim 1**.: _The operator \(\mathcal{K}_{d}^{(q)}\) defined in Definition 2 satisfies the property \(\left(\mathcal{K}_{d}^{(q)}\right)^{2}=\mathcal{K}_{d}^{(q)}\)._

Furthermore, by the addition theorem (Theorem 3), \(\mathcal{K}_{d}^{(q)}\) is trace-class (i.e., the trace is finite and independent of the choice of basis) because:

\[\operatorname{trace}\left(\mathcal{K}_{d}^{(q)}\right) =\int_{\mathbb{S}^{d-1}}k_{q,d}(w,w)\,dw=\sum_{\ell=0}^{q}\frac{ \alpha_{\ell,d}}{\left|\mathbb{S}^{d-1}\right|}\cdot\int_{\mathbb{S}^{d-1}}P_{d }^{\ell}\left(\left\langle w,w\right\rangle\right)\,dw\] \[=\sum_{\ell=0}^{q}\alpha_{\ell,d}=\binom{d+q-1}{q}+\binom{d+q-2}{q -1}-1.\] (6)

By combining Theorem 3 and Lemma 2, and using the definition of the projection operator \(\mathcal{K}_{d}^{(q)}\), it follows that for any function \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) with Hilbert sum decomposition \(f=\sum_{\ell=0}^{\infty}f_{\ell}\), the low-degree component \(f^{(q)}=\sum_{\ell=0}^{q}f_{\ell}\in\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) can be computed as \(f^{(q)}=\mathcal{K}_{d}^{(q)}f\). Equivalently,in order to learn \(f^{(q)}\), it suffices to solve the following least-squares regression problem,

\[\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f\right\|_{ \mathbb{S}^{d-1}}^{2}.\] (7)

If \(g^{*}\) is an optimal solution to the above regression problem then \(f^{(q)}=\mathcal{K}_{d}^{(q)}g^{*}\). In the next claim we show that solving the least squares problem in Eq. (7), even to a coarse approximation, is sufficient to solve our interpolation problem (i.e., Problem 2):

**Claim 2**.: _For any \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\), any integer \(q\geq 0\), and any \(C\geq 1\), if \(\tilde{g}\in L^{2}\left(\mathbb{S}^{d-1}\right)\) satisfies,_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq C \cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f \right\|_{\mathbb{S}^{d-1}}^{2},\]

_and if we let \(f^{(q)}\coloneqq\mathcal{K}_{d}^{(q)}f\), where \(\mathcal{K}_{d}^{(q)}\) is defined as per Definition 2, then the following holds_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2} \leq(C-1)\cdot\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Claim 2 shows that solving the regression problem in Eq. (7) approximately provides a solution to our spherical harmonics interpolation problem (Problem 2). But how can we solve this least-squares problem efficiently? Not only does the problem involve a possibly infinite dimensional parameter vector \(g\), but the objective function also involves the continuous domain on the surface of \(\mathbb{S}^{d-1}\).

### Randomized Discretization via Leverage Function Sampling

We solve the continuous regression in Eq. (7) by randomly discretizing the sphere \(\mathbb{S}^{d-1}\), thereby reducing our problem to a regression on a finite set of points \(w_{1},w_{2},\ldots,w_{s}\in\mathbb{S}^{d-1}\). In particular, we propose to sample points on \(\mathbb{S}^{d-1}\) with probability proportional to the so-called _leverage function_, a specific distribution that has been widely applied in randomized algorithms for linear algebra problems on discrete matrices [13]. We start with the definition of the leverage function for compact operators such as \(\mathcal{K}_{d}^{(q)}\):

**Definition 3** (Leverage Function).: _For integers \(q\geq 0\) and \(d>0\), we define the leverage function of the operator \(\mathcal{K}_{d}^{(q)}\) (see Definition 2) for every \(w\in\mathbb{S}^{d-1}\) as follows,_

\[\tau_{q}(w):=\max_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g \right\|_{\mathbb{S}^{d-1}}^{-2}\cdot\left|\left[\mathcal{K}_{d}^{(q)}g \right](w)\right|^{2}.\] (8)

Intuitively, \(\tau_{q}(w)\) is an upper bound of how much a function that is spanned by the eigenfunctions of the operator \(\mathcal{K}_{d}^{(q)}\) can "blow up" at \(w\). The larger the leverage function \(\tau_{q}(w)\) implies the higher the probability we will be required to sample \(w\). This ensures that our sample points well reflect any possibly significant components, or "spikes", of the function. Ultimately, the integral \(\int_{\mathbb{S}^{d-1}}\tau_{q}(w)\,dw\) determines how many samples we require to solve the regression problem Eq. (7) to a given accuracy. It is a known fact that the leverage function integrates to the rank of the operator \(\mathcal{K}_{d}^{(q)}\) (which is equal to the dimensionality of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\)). This ultimately allows us to achieve a \(\widetilde{\mathcal{O}}(\sum_{\ell=0}^{q}\alpha_{\ell,d})\) sample complexity bound for solving Problem 2. To compute the leverage function, we make use of the following useful alternative characterization of the leverage function.

**Lemma 4** (Min Characterization of the Leverage Function).: _For any \(w\in\mathbb{S}^{d-1}\), let \(\tau_{q}(w)\) be the leverage function (Definition 3) and define \(\phi_{w}\in L^{2}(\mathbb{S}^{d-1})\) by \(\phi_{w}(\sigma)\equiv\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1 }|}P_{d}^{\ell}\left(\langle\sigma,w\rangle\right)\). We have the following minimization characterization of the leverage function:_

\[\tau_{q}(w)=\left\{\min_{g\in L^{2}(\mathbb{S}^{d-1})}\|g\|_{\mathbb{S}^{d-1}}^ {2},\quad\text{s.t.}\ \ \mathcal{K}_{d}^{(q)}g=\phi_{w}\right\}.\] (9)

Using the min and max characterizations of the leverage function we can find upper and lower bounds on this function. Surprisingly, in this case the upper and lower bounds match, so we actually have an exact value for the leverage function.

**Lemma 5** (Leverage Function is Constant).: _The leverage function given in Definition 3 is equal to \(\tau_{q}(w)=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}\) for every \(w\in\mathbb{S}^{d-1}\)._We prove this lemma in Appendix C. The integral of the leverage function, which determines the total samples needed to solve our least-squares regression, is therefore equal to the dimensionality of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\).

**Corollary 1**.: _The leverage function defined in Definition 3 integrates to the dimensionality of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\), which we denote by \(\beta_{q,d}\), i.e.,_

\[\int_{\mathbb{S}^{d-1}}\tau_{q}(w)\,dw=\dim\left(\mathcal{H}^{(q)}(\mathbb{S} ^{d-1})\right)=\sum_{\ell=0}^{q}\alpha_{\ell,d}\equiv\beta_{q,d}.\]

We now show that the leverage function can be used to randomly sample the points on the unit sphere to discretize the regression problem in Eq. (7) and solve it approximately.

**Theorem 4** (Approximate Regression via Leverage Function Sampling).: _For any \(\varepsilon>0\), let \(s=c\cdot\left(\beta_{q,d}\log\beta_{q,d}+\frac{\beta_{q,d}}{\varepsilon}\right)\), for sufficiently large fixed constant \(c\), and let \(x_{1},x_{2},\ldots,x_{s}\) be i.i.d. uniform samples on \(\mathbb{S}^{d-1}\). Define the quasi-matrix \(\bm{P}:\mathbb{R}^{s}\to L^{2}(\mathbb{S}^{d-1})\) as follows, for every \(v\in\mathbb{R}^{d}\):_

\[[\bm{P}\;v](\sigma):=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{\sqrt{s}\cdot| \mathbb{S}^{d-1}|}\cdot\sum_{j=1}^{s}v_{j}\cdot P_{d}^{\ell}\left(\left\langle x _{j},\sigma\right\rangle\right)\quad\text{for }\sigma\in\mathbb{S}^{d-1}.\]

_Also let \(\bm{f}\in\mathbb{R}^{s}\) be a vector with \(\bm{f}_{j}:=\frac{1}{\sqrt{s}}\cdot f(x_{j})\) for \(j=1,2,\ldots,s\) and let \(\bm{P}^{*}\) be the adjoint of \(\bm{P}\). If \(\tilde{g}\) is an optimal solution to the least-squares problem \(\tilde{g}\in\arg\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\bm{P}^{*}g-\bm{f} \right\|_{2}^{2}\), then with probability at least \(1-10^{-4}\) the following holds,_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq(1+ \varepsilon)\cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{( q)}g-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

We prove Theorem 4 in Appendix C. This theorem shows that the function \(\tilde{g}\) obtained from solving the discretized regression problem provides an approximate solution to Eq. (7).

### Efficient Solution for the Discretized Least-Squares Problem

In this section, we demonstrate how to apply Theorem 4 algorithmically to approximately solve the regression problem of Eq. (7). To achieve this, we leverage the _kernel trick_, following a similar approach to previous works such as [1, 19], which allows us to efficiently address the randomly discretized least squares problem as detailed in Algorithm 1. The associated guarantee for this approach is provided in Theorem 5.

**Theorem 5** (Efficient Spherical Harmonic Interpolation).: _Algorithm 1 returns a function \(y\in\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\) such that, with probability at least \(1-10^{-4}\):_

\[\left\|y-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\varepsilon\cdot\left\|f^{ (q)}-f\right\|_{\mathbb{S}^{d-1}}^{2},\quad\text{where }f^{(q)}:=\mathcal{K}_{d}^{(q)}f.\]

_Suppose we can compute the Gegenbauer polynomial \(P_{d}^{\ell}(t)\) at every point \(t\in[-1,1]\) in constant time. Then Algorithm 1 queries the function \(f\) at \(s=\mathcal{O}\left(\beta_{q,d}\log\beta_{q,d}+\frac{\beta_{q,d}}{\varepsilon}\right)\) points on the sphere \(\mathbb{S}^{d-1}\) and runs in \(\mathcal{O}(s^{2}\cdot d+s^{\omega})\) time. This algorithm evaluates \(y(\sigma)\) in \(\mathcal{O}(d\cdot s)\) time for any \(\sigma\in\mathbb{S}^{d-1}\)._We provide the proof of this theorem in Appendix D.

**Remark 1** (Noise Robustness).: _In Theorem 5, our method's robustness is demonstrated under a noise model where the function \(f\) is not strictly a low-degree spherical harmonic and may include high-degree components. In this scenario, the higher-degree components are considered as noise added to the input function._

_However, our algorithm is robust against alternative noise models, particularly additive i.i.d. Normal noise that corrupts the function evaluations \(\bm{f}\) in Algorithm 1 with iid Normal noise. More precisely, suppose that we observe samples from the function \(f^{(q)}\) contaminated by Gaussian noise, i.e., \(f(w_{j})=f^{(q)}(w_{j})+n_{j}\) in Algorithm 1 for i.i.d. \(n_{1},n_{2},\ldots n_{s}\sim\mathcal{N}(0,1)\). The expected value of perturbation's norm in the output \(y\) of our algorithm caused by this noise is:_

\[\mathbb{E}\left[\left\|y-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\right]=\left(1 /s\right)\cdot\mathrm{trace}\left(\bm{K}^{\dagger}\bm{K}\bm{K}^{\dagger} \right).\]

_By Markov's inequality, with \(0.99\) probability \(\left\|y-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}=O(1/s)\cdot\mathrm{trace}\left( \bm{K}^{\dagger}\bm{K}\bm{K}^{\dagger}\right)\). If we let \(\bm{P}\) be the operator defined in Theorem 4, then we can see that \(\bm{K}=\bm{P}^{*}\bm{P}\). Using the properties of the trace, one can see that \(\mathrm{trace}\left(\bm{K}^{\dagger}\bm{K}\bm{K}^{\dagger}\right)=1/\lambda_{ 1}+1/\lambda_{2}+\ldots\), where \(\lambda_{i}\)'s are the singular values of the operator \(\bm{P}\bm{P}^{*}\). By matrix Chernoff inequalities one can show that all singular values of the operator \(\bm{P}\bm{P}^{*}\) closely approximate the singular values of the projection operator \(\mathcal{K}_{d}^{(q)}\) up to a constant factor. So, we have \(\mathrm{trace}\left(\bm{K}^{\dagger}\bm{K}\bm{K}^{\dagger}\right)=O(\mathrm{ rank}(\mathcal{K}_{d}^{(q)}))=O(\beta_{q,d})\). Thus, because \(s\geq\Omega(\beta/\varepsilon)\) and using union bound, with \(0.98\) probability:_

\[\left\|y-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\varepsilon.\]

## 4 Lower Bound on The Number of Required Observations

We conclude by showing that the dimensionality of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\) tightly characterizes the sample complexity of Problem 2. Thus, our Theorem 5 is optimal up to a logarithmic factor. Intuitively, there are \(\beta_{q,d}\) degrees of freedom for specifying a spherical harmonic. Consequently, any deterministic algorithm attempting to reconstruct such polynomials would need at least \(\beta_{q,d}\) samples. We aim to demonstrate that even a "randomized" algorithm, which succeeds with only a constant probability, must still gather \(\beta_{q,d}\) samples. This complements our upper bound, which is established using a randomized algorithm. The crucial fact we use for proving the lower bound is that all (non-zero) eigenvalues of the operator \(\mathcal{K}_{d}^{(q)}\) are equal to one. This fact follows from the addition theorem presented in Theorem 3, i.e., if \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) is an orthonormal basis of \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\), then for any function \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\),

\[\left[\mathcal{K}_{d}^{(q)}f\right](\sigma)=\sum_{\ell=0}^{q}\alpha_{\ell,d} \cdot\underset{w\sim\mathcal{U}\left(\mathbb{S}^{d-1}\right)}{\mathbb{E}} \left[P_{d}^{\ell}\left(\langle\sigma,w\rangle\right)\cdot f(w)\right]=\sum_{ \ell=0}^{q}\sum_{j=1}^{\alpha_{\ell,d}}\langle y_{j}^{\ell},f\rangle_{\mathbb{ S}^{d-1}}\cdot y_{j}^{\ell}(\sigma).\] (10)

**Theorem 6** (Lower Bound).: _Consider an error parameter \(\varepsilon>0\), and any (possibly randomized) algorithm that solves Problem 2 with probability greater than \(1/10\) for any input function \(f\) and makes at most \(r\) (possibly adaptive) queries on any input. Then \(r\geq\beta_{q,d}\)._

We describe a distribution on input function \(f\) on which any deterministic algorithm that takes \(r<\beta_{q,d}\) samples fails with probability \(\geq 9/10\). The theorem then follows by Yao's principle.

Hard Input Distribution.For integer \(\ell\leq q\), consider an orthonormal basis of \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) and denote it by \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\). Let \(\bm{Y}_{\ell}:\mathbb{R}^{\alpha_{\ell,d}}\rightarrow\mathcal{H}_{\ell}\left( \mathbb{S}^{d-1}\right)\) be the quasi-matrix with \(y_{j}^{\ell}\) as its \(j^{th}\) column, i.e., \([\bm{Y}_{\ell}\cdot u](\sigma):=\sum_{j=1}^{\alpha_{\ell,d}}u_{j}\cdot y_{j}^{ \ell}(\sigma)\) for any \(u\in\mathbb{R}^{\alpha_{\ell,d}}\) and \(\sigma\in\mathbb{S}^{d-1}\). Let \(v^{(0)}\in\mathbb{R}^{\alpha_{0,d}},v^{(1)}\in\mathbb{R}^{\alpha_{1,d}},\ldots,v^{(q)}\in\mathbb{R}^{\alpha_{q,d}}\) be independent random vectors with i.i.d. Gaussian entries: \(v_{j}^{(\ell)}\sim\mathcal{N}(0,1)\). The random input is defined to be \(f:=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\). In other words, \(f=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\) is a random linear combination of the eigenfunctions of \(\mathcal{K}_{d}^{(q)}\). We prove that accurate reconstruction of \(f\) drawn from the aforementioned distribution yields accurate reconstruction of random vectors \(v^{(0)},v^{(1)},\ldots,v^{(q)}\). Since each \(v^{(\ell)}\) is \(\alpha_{\ell,d}\)-dimensional, this reconstruction requires \(\Omega(\sum_{\ell=0}^{q}\alpha_{\ell,d})=\Omega(\beta_{q,d})\) samples, giving us a lower bound for accurate reconstruction of \(f\).

First we show that finding an \(\tilde{f}^{(q)}\) satisfying the condition of Problem 2 is at least as hard as accurately finding all vectors \(v^{(0)},v^{(1)},\ldots,v^{(q)}\). The following lemma is proved in Appendix E.

**Lemma 6**.: _If a deterministic algorithm solves Problem 2 with probability at least \(1/10\) over our random input distribution \(f=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\), then with probability at least \(1/10\), the output of the algorithm \(\tilde{f}^{(q)}\) satisfies \(\bm{Y}_{\ell}^{*}\tilde{f}^{(q)}=v^{(\ell)}\) for all integers \(\ell\leq q\)._

Finally, we complete the proof of Theorem 6 by arguing that if \(\tilde{f}^{(q)}\) is formed using less than \(\beta_{q,d}\) queries from \(f\), then \(\sum_{\ell=0}^{q}\left\|\bm{Y}_{\ell}^{*}\tilde{f}^{(q)}-v^{(\ell)}\right\|_{2 }^{2}>0\) with good probability, thus the bound of Lemma 6 cannot hold and \(\tilde{f}^{(q)}\) cannot be a solution to Problem 2. Assume for sake of contradiction that there is a deterministic algorithm which solves Problem 2 with probability \(\geq 1/10\) over the random input \(f=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\) that makes \(r=\beta_{q,d}-1\) queries on any input (we can modify an algorithm that makes fewer queries to make exactly \(\beta_{\ell,d}-1\) queries). For every \(\sigma\in\mathbb{S}^{d-1}\) and integer \(\ell\leq q\) define \(u_{\sigma}^{\ell}:=\left[y_{1}^{\ell}(\sigma),y_{2}^{\ell}(\sigma),\ldots,y_{ \alpha_{\ell,d}}^{\ell}(\sigma)\right]\). Also define \(\bm{u}_{\sigma}:=\left[u_{\sigma}^{0},u_{\sigma}^{1},\ldots,u_{\sigma}^{q} \right]\in\mathbb{R}^{\beta_{q,d}}\) and \(\bm{v}\in\mathbb{R}^{\beta_{q,d}}\) as \(\bm{v}:=\left(v^{(0)},v^{(1)},\ldots,v^{(q)}\right)\). Additionally, define the quasi-matrix \(\bm{Y}:=[\bm{Y}_{0},\ldots,\bm{Y}_{q}]\).

Using the above notations and the definition of the hard input instance \(f\), each query to \(f\) is in fact a query to the random vector \(\bm{v}\) in the form of \(f(\sigma)=\langle\bm{u}_{\sigma},\bm{v}\rangle\). Now consider a deterministic function \(Q\), that is given input \(\bm{V}\in\mathbb{R}^{i\times\beta_{q,d}}\) (for any positive integer \(i\)) and outputs \(Q(\bm{V})\in\mathbb{R}^{\beta_{q,d}\times\beta_{q,d}}\) such that \(Q(\bm{V})\) has orthonormal rows with the first \(i\) rows spanning the \(i\) rows of \(\bm{V}\). If \(\sigma_{1},\sigma_{2},\ldots,\sigma_{r}\in\mathbb{S}^{d-1}\) denote the points where our algorithm queries the input \(f\), for any integer \(i\in[r]\), let \(\bm{Q}^{i}\) be an orthonormal matrix whose first \(i\) rows span the first \(i\) queries of the algorithm, i.e., \(\bm{Q}^{i}:=Q\left([\bm{u}_{\sigma_{1}},\bm{u}_{\sigma_{2}},\ldots,\bm{u}_{ \sigma_{i}}]^{\top}\right)\). Since the algorithm is deterministic, \(\bm{Q}^{i}\) is a deterministic function of input \(\bm{v}\). The following claim is proved in [1]:

**Claim 3** (Claim 23 of [1]).: _Conditioned on the queries \(f(\sigma_{1}),f(\sigma_{2}),\ldots,f(\sigma_{r})\) for \(r<\beta_{q,d}\), the variable \([\bm{Q}^{r}\cdot\bm{v}](\beta_{q,d})\) is distributed as \(\mathcal{N}(0,1)\)._

Now using Claim 3 we can write,

\[\Pr_{\bm{v}}\left[\sum_{\ell=0}^{q}\left\|v^{(\ell)}-\bm{Y}_{\ell }^{*}\tilde{f}^{(q)}\right\|_{2}^{2}=0\right] =\Pr_{\bm{v}}\left[\bm{Q}^{r}\bm{v}=\bm{Q}^{r}\bm{Y}^{*}\tilde{f} ^{(q)}\right]\leq\Pr_{\bm{v}}\left[\bm{Q}^{r}\bm{v}\right]_{\beta_{q,d}}= \left[\bm{Q}^{r}\bm{Y}^{*}\tilde{f}^{(q)}\right]_{\beta_{q,d}}\] \[=\mathbb{E}\left[\Pr_{\bm{v}}\left[\left.\left[\bm{Q}^{r}\bm{v} \right]_{\beta_{q,d}}=\left[\bm{Q}^{r}\bm{Y}^{*}\tilde{f}^{(q)}\right]_{ \beta_{q,d}}\right]f(\sigma_{1}),\ldots,f(\sigma_{r})\right]\right],\]

where the expectation in the last line is taken over the randomness of \(f(\sigma_{1}),\ldots,f(\sigma_{r})\). Conditioned on \(f(\sigma_{1}),\ldots,f(\sigma_{r})\), \(\left[\bm{Q}^{r}\bm{Y}^{*}\tilde{f}^{(q)}\right](\beta_{q,d})\) is a fixed quantity because the algorithm determines \(\tilde{f}^{(q)}\) given the knowledge of the queries \(f(\sigma_{1}),\ldots,f(\sigma_{r})\). Furthermore, by Claim 3, \([\bm{Q}^{r}\cdot\bm{v}](\beta_{q,d})\) is a random variable distributed as \(\mathcal{N}(0,1)\), conditioned on \(f(\sigma_{1}),\ldots,f(\sigma_{r})\). This implies that,

\[\Pr\left[\left.\left[\bm{Q}^{r}\cdot\bm{v}\right](\beta_{q,d})=\left[\bm{Q}^ {r}\bm{Y}^{*}\tilde{f}^{(q)}\right](\beta_{q,d})\right|f(\sigma_{1}),\ldots,f( \sigma_{r})\right]=0.\]

Thus, \(\Pr\left[\sum_{\ell=0}^{q}\left\|v^{(\ell)}-\bm{Y}_{\ell}^{*}\tilde{f}^{(q)} \right\|_{2}^{2}=0\right]=\mathbb{E}_{f(\sigma_{1}),\ldots,f(\sigma_{r})}[0]=0\). However, we have assumed that this algorithm solves Problem 2 with probability at least \(1/10\), and hence, by Lemma 6, \(\Pr\left[\sum_{\ell=0}^{q}\|v^{(\ell)}-\bm{Y}_{\ell}^{*}\tilde{f}^{(q)}\|_{2}^{ 2}=0\right]\geq 1/10\). This is a contradiction, yielding Theorem 6.

## 5 Numerical Evaluation

Noise-free Setting.For a fixed \(q\), we generate a random function \(f(\sigma)=\sum_{\ell=0}^{q}c_{\ell}P_{d}^{\ell}(\langle\sigma,v\rangle)\) where \(v\sim\mathcal{U}(\mathbb{S}^{d-1})\) and \(c_{\ell}\)'s are i.i.d. samples from \(\mathcal{N}(0,1)\). Then, \(f\) is recovered by running Algorithm 1 with \(s\) random evaluations of \(f\) on \(\mathbb{S}^{d-1}\). Note that \(\|\mathcal{K}_{d}^{(q)}f-f\|_{\mathbb{S}^{d-1}}=0\) since \(f\in\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\), thus,as shown in Theorem 4, Algorithm 1 can recover \(f\) "exactly" using \(s=\mathcal{O}(\beta_{q,d}\log\beta_{q,d})\) evaluations, where \(\beta_{q,d}\) is the dimension of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\). We predict \(f\)'s value on a random test set on \(\mathbb{S}^{d-1}\) and consider the algorithm fails if the testing error is greater than \(10^{-12}\). We count the number of failures among \(100\) independent random trials with different choices of \(d\in\{3,4\}\), \(q\in\{5,\ldots,22\}\), and \(s\in\{40,\ldots,2400\}\). The empirical success probabilities for \(d=3\) and \(4\) are reported in Fig. 1(a) and Fig. 1(b), respectively. Fig. 1 illustrates that the success probabilities of our algorithm sharply transition to \(1\) as soon as the number of samples approaches \(s\approx\beta_{q,d}\) for a wide range of \(q\) and both \(d=3,4\). These experimental results complement our Theorem 4 along with the lower bound analysis in Section 4 and empirically verify the performance of our algorithm.

Noisy Setting.We repeated our experiments in the presence of an additive noise which is a linear combination of random spherical harmonics of degrees \(q+1\) to \(2q\). More precisely, we let the noise be \(n(\sigma)=\sum_{\ell=q+1}^{2q}c_{\ell}P_{d}^{\ell}(\langle v,\sigma\rangle) \in\mathcal{H}^{(2q)}(\mathbb{S}^{d-1})\setminus\mathcal{H}^{(q)}(\mathbb{S}^ {d-1})\) for \(c_{\ell}\)'s that are i.i.d. samples from \(\mathcal{N}(0,1)\). We then re-scale the noise to have norm \(\|n\|_{\mathbb{S}^{d-1}}=10^{-6}\). Furthermore, the function \(f\) is defined as before, and \(f\) is recovered by Algorithm 1 with \(s\) random evaluations of \(f+n\) on \(\mathbb{S}^{d-1}\). The heat-maps in Fig. 2 are generated by considering an instance of our algorithm as a "success" if the error's energy is below the noise level, \(\left\|\tilde{f}^{(q)}-f\right\|_{\mathbb{S}^{d-1}}\leq\|n\|_{\mathbb{S}^{d-1} }=10^{-6}\). The success probability transitions less sharply than the noiseless setting but the shift of probabilities starts at \(\beta_{s,q}\) samples.

## 6 Conclusion

We studied the problem of robustly recovering spherical harmonic expansion of a function defined on the sphere. The number of function evaluations needed to recover its degree-\(q\) expansion is the dimension of spherical harmonics of degree at most \(q\), up to a logarithmic factor. We develop a simple yet efficient kernel regression-based algorithm to recover degree-\(q\) expansion of the function by only evaluating the function on uniformly sampled points on the sphere. Unlike the prior results on fast spherical harmonic transform, our algorithm works efficiently using a nearly optimal number of samples in any dimension. We believe our findings would appeal to the readership of the community.

Figure 1: (Left) Empirical success probabilities of Algorithm 1 varying the number of samples \(s\) and the degree of spherical harmonic expansion \(q\). (Right) The dimension \(\beta_{q,d}\) of the Hilbert space \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\) as a function of \(q\) when (a) \(d=3\) and (b) \(d=4\), respectively.

Figure 2: (Left) Empirical success probabilities of Algorithm 1 in presence of additive noise, where “success” means the error’s energy is below the noise level \(\|\tilde{f}^{(q)}-f\|_{\mathbb{S}^{d-1}}\leq\|n\|_{\mathbb{S}^{d-1}}\). (Right) The error’s norm \(\|\tilde{f}^{(q)}-f\|_{\mathbb{S}^{d-1}}\) as a function of \(q\) when (a) \(d=3\) and (b) \(d=4\), respectively.

## References

* [AH12] Kendall Atkinson and Weimin Han. _Spherical harmonics and approximations on the unit sphere: an introduction_. Springer Science & Business Media, 2012.
* [AKK\({}^{+}\)20] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In _Symposium on Discrete Algorithms (SODA)_, 2020.
* [AKM\({}^{+}\)17] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random Fourier features for kernel ridge regression: Approximation bounds and statistical guarantees. In _International Conference on Machine Learning (ICML)_, 2017.
* [AKM\({}^{+}\)19] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. A universal sampling method for reconstructing signals with simple fourier transforms. In _Symposium on the Theory of Computing (STOC)_, 2019.
* [BKM\({}^{+}\)23] Karl Bringmann, Michael Kapralov, Mikhail Makarov, Vasileios Nakos, Amir Yagudin, and Amir Zandieh. Traversing the fft computation tree for dimension-independent sparse fourier transforms. In _Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 4768-4845. SIAM, 2023.
* [CKPS16] Xue Chen, Daniel M Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a frequency gap. In _Foundations of Computer Science (FOCS)_, 2016.
* [CLL15] Pui Tung Choi, Ka Chun Lam, and Lok Ming Lui. FLASH: Fast landmark aligned spherical harmonic parameterization for genus-\(0\) closed brain surfaces. _SIAM Journal on Imaging Sciences_, 2015.
* [CP19] Xue Chen and Eric Price. Active Regression via Linear-Sample Sparsification. In _Conference on Learning Theory (COLT)_, 2019.
* [EEHM17] Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, and Stephane Mallat. Solid harmonic wavelet scattering: Predicting quantum molecular energy from invariant descriptors of 3d electronic densities. _Advances in Neural Information Processing Systems_, 30, 2017.
* [EMM20] Tamas Erdelyi, Cameron Musco, and Christopher Musco. Fourier sparse leverage scores and approximate kernel learning. _Neural Information Processing Systems (NeurIPS)_, 2020.
* [GCL\({}^{+}\)22] Jan Gerken, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, and Daniel Persson. Equivariance versus augmentation for spherical images. In _International Conference on Machine Learning_, pages 7404-7421. PMLR, 2022.
* [GMMM21] B Ghorbani, S Mei, T Misiakiewicz, and A Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 2021.
* [HZA22] Insu Han, Amir Zandieh, and Haim Avron. Random Gegenbauer Features for Scalable Kernel Methods. In _International Conference on Machine Learning_, pages 8330-8358. PMLR, 2022.
* [HZL\({}^{+}\)22] Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural kernel embeddings for general activations. _Advances in neural information processing systems_, 35:35657-35671, 2022.
* [KFR03] Michael Kazhdan, Thomas Funkhouser, and Szymon Rusinkiewicz. Rotation invariant spherical harmonic representation of 3 d shape descriptors. In _Symposium on Geometry Processing_, 2003.
* [KKS97] Marc Kamionkowski, Arthur Kosowsky, and Albert Stebbins. Statistics of cosmic microwave background polarization. _Physical Review D_, 1997.

* [KVZ19] Michael Kapralov, Ameya Velingker, and Amir Zandieh. Dimension-independent sparse fourier transform. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 2709-2728. SIAM, 2019.
* [Lan12] Serge Lang. _Real and functional analysis_. Springer Science & Business Media, 2012.
* [LMP13] Mu Li, Gary L Miller, and Richard Peng. Iterative row sampling. In _Foundations of Computer Science (FOCS)_, 2013.
* [LP61] Henry J Landau and Henry O Pollak. Prolate spheroidal wave functions, Fourier analysis and uncertainty--II. _Bell System Technical Journal_, 1961.
* [LP62] Henry J Landau and Henry O Pollak. Prolate spheroidal wave functions, fourier analysis and uncertainty--III: the dimension of the space of essentially time-and band-limited signals. _Bell System Technical Journal_, 1962.
* [LWL\({}^{+}\)21] Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In _International Conference on Learning Representations_, 2021.
* [MNY06] Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer's theorem, feature maps, and smoothing. In _Conference on Learning Theory (COLT)_, 2006.
* [Mor98] Mitsuo Morimoto. _Analytic functionals on the sphere_. American Mathematical Soc., 1998.
* [Pen30] WO Pennell. A generalized Fourier series representation of a function. _The American Mathematical Monthly_, 1930.
* [RT06] Vladimir Rokhlin and Mark Tygert. Fast algorithms for spherical harmonic expansions. _SIAM Journal on Scientific Computing (SISC)_, 2006.
* [SA22] Paz Fink Shustin and Haim Avron. Semi-Infinite Linear Regression and Its Applications. _SIAM Journal on Matrix Analysis and Applications (SIMAX)_, 2022.
* [SP61] David Slepian and Henry O Pollak. Prolate spheroidal wave functions, Fourier analysis and uncertainty--I. _Bell System Technical Journal_, 1961.
* [SS00] Paul N Swarztrauber and William F Spotz. Generalized discrete spherical harmonic transforms. _Journal of Computational Physics_, 2000.
* [ST02] Reiji Suda and Masayasu Takami. A fast spherical harmonics transform algorithm. _Mathematics of Computation_, 2002.
* [Wei95] DR Weimer. Models of high-latitude electric potentials derived with a least error fit of spherical harmonic coefficients. _Journal of Geophysical Research: Space Physics_, 1995.
* [Wer97] Robert A Werner. Spherical harmonic coefficients for the potential of a constant-density polyhedron. _Computers & Geosciences_, 1997.
* [Wil12] Virginia Vassilevska Williams. Multiplying matrices faster than Coppersmith-Winograd. In _Symposium on the Theory of Computing (STOC)_, 2012.
* [WZ22] David Woodruff and Amir Zandieh. Leverage score sampling for tensor product matrices in input sparsity time. In _International Conference on Machine Learning_, pages 23933-23964. PMLR, 2022.
* [XRY01] Hong Xiao, Vladimir Rokhlin, and Norman Yarvin. Prolate spheroidal wavefunctions, quadrature and interpolation. _Inverse problems_, 2001.
* [ZDK\({}^{+}\)22] Larry Zitnick, Abhishek Das, Adeesh Kolluru, Janice Lan, Muhammed Shuaibi, Anuroop Sriram, Zachary Ulissi, and Brandon Wood. Spherical channels for modeling atomic interactions. _Advances in Neural Information Processing Systems_, 35:8054-8067, 2022.

* [ZHA\({}^{+}\)21] Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling Neural Tangent Kernels via Sketching and Random Features. In _Neural Information Processing Systems (NeurIPS)_, 2021.

Properties of Gegenbauer Polynomials and Spherical Harmonics

In this section we prove the basic properties of the Gegenbauer Polynomials as well as the Spherical Harmonics and establish the connection between the two. We start by the direct sum decomposition of the Hilbert space \(L^{2}(\mathbb{S}^{d-1})\) in terms of the spherical harmonics,

**Lemma 2** (Direct Sum Decomposition of \(L^{2}(\mathbb{S}^{d-1})\)).: _The family of spaces \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) yields a Hilbert space direct sum decomposition \(L^{2}\left(\mathbb{S}^{d-1}\right)=\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{ \ell}\left(\mathbb{S}^{d-1}\right)\): the summands are closed and pairwise orthogonal, and every \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) is the sum of a converging series (in the sense of mean-square convergence with the \(L^{2}\)-norm defined in Eq. (2)),_

\[f=\sum_{\ell=0}^{\infty}f_{\ell},\]

_where \(f_{\ell}\in\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) are uniquely determined functions. Furthermore, given any orthonormal basis \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) of \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) we have \(f_{\ell}=\sum_{j=1}^{\alpha_{\ell,d}}\langle f,y_{j}^{\ell}\rangle_{\mathbb{S }^{d-1}}\cdot y_{j}^{\ell}\)._

Proof.: This is in fact a standard result. For example, see [12] for a proof.

Now we show that the Gegenbauer polynomials and spherical harmonics are related through the so called the addition theorem,

**Theorem 3** (Addition Theorem).: _For every integer \(\ell\geq 0\), if \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) is an orthonormal basis for \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\), then for any \(\sigma,w\in\mathbb{S}^{d-1}\) we have_

\[\frac{\alpha_{\ell,d}}{\left|\mathbb{S}^{d-1}\right|}\cdot P_{d}^{\ell}\left( \langle\sigma,w\rangle\right)=\sum_{j=1}^{\alpha_{\ell,d}}y_{j}^{\ell}(\sigma )\cdot y_{j}^{\ell}(w).\]

Proof.: The result can be proven analytically, using the properties of the Poisson kernel in the unit ball. This is classic and the proof can be found in [1, Theorem 2.9].

Next we show that the Gegenbauer kernels can project any function into the space of their corresponding spherical harmonics,

**Lemma 3** (A Reproducing Kernel for \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\)).: _For every \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\), if \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) is the unique decomposition of \(f\) over \(\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) as per Lemma 2, then \(f_{\ell}\) is given by_

\[f_{\ell}(\sigma)=\alpha_{\ell,d}\cdot\underset{w\sim\mathcal{U}(\mathbb{S}^{d -1})}{\mathbb{E}}\left[f(w)P_{d}^{\ell}\left(\langle\sigma,w\rangle\right) \right]\quad\text{for $\sigma\in\mathbb{S}^{d-1}$}.\]

Proof.: This is a classic textbook result, see [13].

Now we prove that the Gegenbauer kernels satisfy the reproducing property for the Hilbert space \(\mathcal{H}_{\ell}(\mathbb{S}^{d-1})\).

**Lemma 1** (Reproducing Property of Zonal Harmonics).: _Let \(P_{d}^{\ell}(\cdot)\) be the Gegenbauer polynomial of degree \(\ell\) in dimension \(d\). For any \(x,y\in\mathbb{S}^{d-1}\):_

\[P_{d}^{\ell}(\langle x,y\rangle)=\alpha_{\ell,d}\cdot\mathbb{E}_{w\sim \mathcal{U}(\mathbb{S}^{d-1})}\left[P_{d}^{\ell}\left(\langle x,w\rangle\right) P_{d}^{\ell}\left(\langle y,w\rangle\right)\right],\]

_Furthermore, for any \(\ell^{\prime}\neq\ell\):_

\[\mathbb{E}_{w\sim\mathcal{U}(\mathbb{S}^{d-1})}\left[P_{d}^{\ell}\left(\langle x,w\rangle\right)\cdot P_{d}^{\ell^{\prime}}\left(\langle y,w\rangle\right) \right]=0.\]Proof.: This result follows directly from the Funk-Hecke formula (See [1]). However, we provide another proof here. First note that for every \(x\in\mathbb{S}^{d-1}\) the function \(P_{d}^{\ell}\left(\left\langle x,\cdot\right\rangle\right)\in\mathcal{H}_{\ell} \left(\mathbb{S}^{d-1}\right)\). Therefore the first claim follow by applying Lemma 3 on function \(f(\sigma)=P_{d}^{\ell}\left(\left\langle x,\sigma\right\rangle\right)\) which also satisfies \(f_{\ell}=f\). On the other hand, \(P_{d}^{\ell^{\prime}}\left(\left\langle y,\cdot\right\rangle\right)\in \mathcal{H}_{\ell^{\prime}}\left(\mathbb{S}^{d-1}\right)\) for every \(y\in\mathbb{S}^{d-1}\). Thus, for \(\ell^{\prime}\neq\ell\), using the fact that spherical harmonics are orthogonal spaces of functions, \(P_{d}^{\ell^{\prime}}\left(\left\langle y,\cdot\right\rangle\right)\perp \mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\), which gives the second claim.

Next we prove that the kernel operator defined in Definition 2 is in fact a projection operator,

**Claim 1**.: _The operator \(\mathcal{K}_{d}^{(q)}\) defined in Definition 2 satisfies the property \(\left(\mathcal{K}_{d}^{(q)}\right)^{2}=\mathcal{K}_{d}^{(q)}\)._

Proof.: For every \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\) and every \(\sigma\in\mathbb{S}^{d-1}\), using Definition 2 we have,

\[\left[\left(\mathcal{K}_{d}^{(q)}\right)^{2}f\right](\sigma) =\sum_{\ell^{\prime}=0}^{q}\frac{\alpha_{\ell^{\prime},d}}{| \mathbb{S}^{d-1}|}\left\langle\mathcal{K}_{d}^{(q)}f,P_{d}^{\ell^{\prime}} \left(\left\langle\sigma,\cdot\right\rangle\right)\right\rangle_{\mathbb{S}^{ d-1}}\] \[=\sum_{\ell=0}^{q}\sum_{\ell^{\prime}=0}^{q}\alpha_{\ell,d}\alpha _{\ell^{\prime},d}\cdot\underset{\tau\sim\mathcal{U}\left(\mathbb{S}^{d-1} \right)}{\mathbb{E}}\left[P_{d}^{\ell^{\prime}}\left(\left\langle\sigma,w \right\rangle\right)\cdot\underset{\tau\sim\mathcal{U}\left(\mathbb{S}^{d-1} \right)}{\mathbb{E}}\left[P_{d}^{\ell}\left(\left\langle\tau,w\right\rangle \right)f(\tau)\right]\right]\] \[=\sum_{\ell=0}^{q}\sum_{\ell^{\prime}=0}^{q}\alpha_{\ell,d} \alpha_{\ell^{\prime},d}\cdot\underset{\tau\sim\mathcal{U}\left(\mathbb{S}^{d-1 }\right)}{\mathbb{E}}\left[f(\tau)\cdot\underset{w\sim\mathcal{U}\left( \mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[P_{d}^{\ell^{\prime}}\left(\left \langle\sigma,w\right\rangle\right)P_{d}^{\ell}\left(\left\langle\tau,w\right \rangle\right)\right]\right]\] \[=\sum_{\ell=0}^{q}\alpha_{\ell,d}\cdot\underset{\tau\sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[f(\tau)\cdot P_{d}^ {\ell}\left(\left\langle\sigma,\tau\right\rangle\right)\right]\] \[=\left[\mathcal{K}_{d}^{(q)}f\right](\sigma),\]

where the fourth line above follows from Lemma 1. This proves the claim.

## Appendix B Reducing the Interpolation Problem to a Least-Squares Regression

In this section we show that our spherical harmonic interpolation problem, i.e., Problem 2, can be solved by approximately solving a least-squares problem as claimed in Claim 2. We start by showing that for any function \(f\in L^{2}(\mathbb{S}^{d-1})\), \(\mathcal{K}_{d}^{(q)}f\) gives its low-degree component. More precisely, let \(f=\sum_{\ell=0}^{\infty}f_{\ell}\) be the decomposition of \(f\) over the Hilbert sum \(\bigoplus_{\ell=0}^{\infty}\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\) as per Lemma 2. Now if we let \(\mathcal{K}_{d}^{(q)}\) be the kernel operator from Definition 2 and if \(\left\{y_{1}^{\ell},y_{2}^{\ell},\ldots,y_{\alpha_{\ell,d}}^{\ell}\right\}\) is an orthonormal basis for \(\mathcal{H}_{\ell}\left(\mathbb{S}^{d-1}\right)\), then by Theorem 3 we have,

\[\left[\mathcal{K}_{d}^{(q)}f\right](\sigma) =\sum_{\ell=0}^{q}\alpha_{\ell,d}\cdot\underset{w\sim\mathcal{U} \left(\mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[f(w)P_{d}^{\ell}\left(\left \langle\sigma,w\right\rangle\right)\right]\] \[=\sum_{\ell=0}^{q}\left|\mathbb{S}^{d-1}\right|\cdot\underset{w \sim\mathcal{U}\left(\mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[f(w)\cdot\sum_{ j=1}^{\alpha_{\ell,d}}y_{j}^{\ell}(\sigma)\cdot y_{j}^{\ell}(w)\right]\] \[=\sum_{\ell=0}^{q}\sum_{j=1}^{\alpha_{\ell,d}}y_{j}^{\ell}(\sigma) \cdot\left|\mathbb{S}^{d-1}\right|\cdot\underset{w\sim\mathcal{U}\left( \mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[f(w)\cdot y_{j}^{\ell}(w)\right]\] \[=\sum_{\ell=0}^{q}\sum_{j=1}^{\alpha_{\ell,d}}\langle f,y_{j}^{ \ell}(w)\rangle_{\mathbb{S}^{d-1}}\cdot y_{j}^{\ell}(\sigma)\] \[=\sum_{\ell=0}^{q}f_{\ell}(\sigma)=f^{(q)}(\sigma),\]where the the second line above follows from Theorem 3, the fourth line follows from Eq. (2), and the last line follows from Lemma 2. This proves that the low-degree component \(f^{(q)}=\mathcal{K}_{d}^{(q)}f\).

**Claim 2**.: _For any \(f\in L^{2}\left(\mathbb{S}^{d-1}\right)\), any integer \(q\geq 0\), and any \(C\geq 1\), if \(\tilde{g}\in L^{2}\left(\mathbb{S}^{d-1}\right)\) satisfies,_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq C \cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f\right\| _{\mathbb{S}^{d-1}}^{2},\]

_and if we let \(f^{(q)}\coloneqq\mathcal{K}_{d}^{(q)}f\), where \(\mathcal{K}_{d}^{(q)}\) is defined as per Definition 2, then the following holds_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2} \leq(C-1)\cdot\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Proof.: First, note that \(g^{*}=f\) is an optimal solution to the least-squares problem in Eq. (7). Thus we have,

\[\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f\right\|_{ \mathbb{S}^{d-1}}^{2}=\left\|\mathcal{K}_{d}^{(q)}f-f\right\|_{\mathbb{S}^{d- 1}}^{2}=\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Next, we can write,

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^ {2} =\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-\mathcal{K}_{d}^{(q)}f+ \left(\mathcal{K}_{d}^{(q)}f-f\right)\right\|_{\mathbb{S}^{d-1}}^{2}\] \[=\left\|\mathcal{K}_{d}^{(q)}(\tilde{g}-f)+\left(\mathcal{K}_{d}^ {(q)}f-f\right)\right\|_{\mathbb{S}^{d-1}}^{2}\] \[=\left\|\mathcal{K}_{d}^{(q)}(\tilde{g}-f)\right\|_{\mathbb{S}^{d -1}}^{2}+\left\|\mathcal{K}_{d}^{(q)}f-f\right\|_{\mathbb{S}^{d-1}}^{2}\] \[=\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f^{(q)}\right\|_{\mathbb{S }^{d-1}}^{2}+\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2},\]

where the third line follows from the Pythagorean theorem because \(\mathcal{K}_{d}^{(q)}(\tilde{g}-f)\in\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) while \(\mathcal{K}_{d}^{(q)}f-f=-\sum_{\ell>q}f\ell\), thus \(\left(\mathcal{K}_{d}^{(q)}f-f\right)\perp\mathcal{H}^{(q)}\left(\mathbb{S}^{d -1}\right)\). Combining the two equalities above with inequality \(\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq C \cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{(q)}g-f\right\| _{\mathbb{S}^{d-1}}^{2}\) given in the statement of the claim, proves Claim 2.

## Appendix C Approximate Regression via Leverage Score Sampling

In this section we ultimately prove our main result of Theorem 4. We start by proving useful properties of the leverage function given in Definition 3. First, we show the fact that the leverage function can be characterized in terms of a least-squares minimization problem, which is crucial for computing the leverage scores distribution. This fact was previously exploited in [1] and [1] in the context of Fourier operators.

**Lemma 4** (Min Characterization of the Leverage Function).: _For any \(w\in\mathbb{S}^{d-1}\), let \(\tau_{q}(w)\) be the leverage function (Definition 3) and define \(\phi_{w}\in L^{2}(\mathbb{S}^{d-1})\) by \(\phi_{w}(\sigma)\equiv\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1 }|}\mathcal{P}_{d}^{\ell}\left(\langle\sigma,w\rangle\right)\). We have the following minimization characterization of the leverage function:_

\[\tau_{q}(w)=\left\{\min_{g\in L^{2}(\mathbb{S}^{d-1})}\|g\|_{\mathbb{S}^{d-1}}^ {2},\quad\text{s.t. }\mathcal{K}_{d}^{(q)}g=\phi_{w}\right\}.\] (9)

We remark that this lemma is in fact an adaptation and generalization of Theorem 5 of [1]. We prove this lemma here for the sake of completeness.

Proof.: First we show that the right hand side of Eq. (9) is never smaller than the leverage function in Definition 3. Let \(g_{w}^{*}\in L^{2}(\mathbb{S}^{d-1})\) be the optimal solution of Eq. (9) for any \(w\in\mathbb{S}^{d-1}\). Note that the optimal solution satisfies \(\mathcal{K}_{d}^{(q)}g_{w}^{*}=\phi_{w}\). Thus, for any function \(f\in L^{2}(\mathbb{S}^{d-1})\), using Definition 2, we can write

\[\left|\left[\mathcal{K}_{d}^{(q)}f\right](w)\right|^{2} =\left|\sum_{\ell=0}^{q}\alpha_{\ell,d}\cdot\underset{\sigma \sim d\sim\left(\mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[P_{d}^{\ell}\left( \left\langle\sigma,w\right\rangle\right)\cdot f(\sigma)\right]\right|^{2}\] \[=\left|\left\langle\phi_{w},f\right\rangle_{\mathbb{S}^{d-1}} \right|^{2}=\left|\left\langle\mathcal{K}_{d}^{(q)}g_{w}^{*},f\right\rangle_{ \mathbb{S}^{d-1}}\right|^{2}\] \[=\left|\left\langle g_{w}^{*},\mathcal{K}_{d}^{(q)}f\right\rangle_ {\mathbb{S}^{d-1}}\right|^{2}\] (because \[\mathcal{K}_{d}^{(q)}\] is self-adjoint) \[\leq\left\|g_{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}\cdot\left\| \mathcal{K}_{d}^{(q)}f\right\|_{\mathbb{S}^{d-1}}^{2}\] (by Cauchy-Schwarz inequality)

Therefore, for any \(f\in L^{2}(\mathbb{S}^{d-1})\) with \(\left\|\mathcal{K}_{d}^{(q)}f\right\|_{\mathbb{S}^{d-1}}>0\), we have

\[\frac{\left|\left[\mathcal{K}_{d}^{(q)}f\right](w)\right|^{2}}{\left\| \mathcal{K}_{d}^{(q)}f\right\|_{\mathbb{S}^{d-1}}^{2}}\leq\left\|g_{w}^{*} \right\|_{\mathbb{S}^{d-1}}^{2}.\] (11)

We conclude the proof by showing that the maximum value is attained. First, we show that the optimal solution \(g_{w}^{*}\) of Eq. (9) satisfies the property that \(\mathcal{K}_{d}^{(q)}g_{w}^{*}=g_{w}^{*}\). Suppose for the sake of contradiction that \(\mathcal{K}_{d}^{(q)}g_{w}^{*}\neq g_{w}^{*}\). In this case, Claim 1 implies that,

\[\mathcal{K}_{d}^{(q)}\left(\mathcal{K}_{d}^{(q)}g_{w}^{*}-g_{w}^{*}\right)= \left(\mathcal{K}_{d}^{(q)}\right)^{2}g_{w}^{*}-\mathcal{K}_{d}^{(q)}g_{w}^{* }=\mathcal{K}_{d}^{(q)}g_{w}^{*}-\mathcal{K}_{d}^{(q)}g_{w}^{*}=0.\]

Thus, the function \(g=\mathcal{K}_{d}^{(q)}g_{w}^{*}\) satisfies the constraint of the minimization problem in Eq. (9). Now, using the above and the fact that \(\mathcal{K}_{d}^{(q)}\) is self-adjoint we can write,

\[\left\langle\mathcal{K}_{d}^{(q)}g_{w}^{*},\mathcal{K}_{d}^{(q)}g_{w}^{*}-g_{w }^{*}\right\rangle_{\mathbb{S}^{d-1}}=\left\langle g_{w}^{*},\mathcal{K}_{d}^ {(q)}\left(\mathcal{K}_{d}^{(q)}g_{w}^{*}-g_{w}^{*}\right)\right\rangle_{ \mathbb{S}^{d-1}}=0.\]

This shows that \(\mathcal{K}_{d}^{(q)}g_{w}^{*}\perp\left(\mathcal{K}_{d}^{(q)}g_{w}^{*}-g_{w}^ {*}\right)\), hence by Pythagorean theorem we have,

\[\left\|g_{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}=\left\|\mathcal{K}_{d}^{(q)}g _{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}+\left\|\mathcal{K}_{d}^{(q)}g_{w}^{*}- g_{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}>\left\|\mathcal{K}_{d}^{(q)}g_{w}^{*} \right\|_{\mathbb{S}^{d-1}}^{2}=\left\|g\right\|_{\mathbb{S}^{d-1}}^{2},\]

which is in contrast with the assumption that \(g_{w}^{*}\) is the optimal solution of Eq. (9). Therefore, our claim that \(\mathcal{K}_{d}^{(\ell)}g_{w}^{*}=g_{w}^{*}\) holds.

Now, we show that for \(f=g_{w}^{*}\), the maximum value in inequality Eq. (11) is attained. For any \(w\in\mathbb{S}^{d-1}\) we have the following

\[\left[\mathcal{K}_{d}^{(q)}f\right](w)=\left\langle\mathcal{K}_{d}^{(q)}g_{w}^ {*},f\right\rangle_{\mathbb{S}^{d-1}}=\langle g_{w}^{*},g_{w}^{*}\rangle_{ \mathbb{S}^{d-1}}=\left\|g_{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}.\]

On the other hand we have \(\left\|\mathcal{K}_{d}^{(q)}f\right\|_{\mathbb{S}^{d-1}}^{2}=\left\|g_{w}^{*} \right\|_{\mathbb{S}^{d-1}}^{2}\). Thus, \(\left\|\mathcal{K}_{d}^{(q)}f\right\|_{\mathbb{S}^{d-1}}^{-2}\cdot\left|\left[ \mathcal{K}_{d}^{(q)}f\right](w)\right|^{2}=\left\|g_{w}^{*}\right\|_{\mathbb{ S}^{d-1}}^{2}\) which implies that \(\tau_{q}(w)=\left\|g_{w}^{*}\right\|_{\mathbb{S}^{d-1}}^{2}\) and thus proves the lemma.

Next we prove that the leverage function is constant.

**Lemma 5** (Leverage Function is Constant).: _The leverage function given in Definition 3 is equal to \(\tau_{q}(w)=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}\) for every \(w\in\mathbb{S}^{d-1}\)._

Proof.: First we prove that \(\tau_{q}(w)\leq\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}\) using the min-characterization. If we let \(\phi_{w}\in L^{2}(\mathbb{S}^{d-1})\) be defined as \(\phi_{w}(\sigma):=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}P_ {d}^{l}\left(\left\langle\sigma,w\right\rangle\right)\), then by Definition 2, for every \(\sigma\in\mathbb{S}^{d-1}\)we can write,

\[\left[\mathcal{K}_{d}^{(q)}\phi_{w}\right](\sigma) =\sum_{\ell=0}^{q}\alpha_{\ell,d}\cdot\mathop{\mathbb{E}}_{v\sim \mathcal{U}(\mathbb{S}^{d-1})}\left[P_{d}^{\ell}\left(\langle\sigma,v\rangle \right)\cdot\phi_{w}(v)\right]\] \[=\sum_{\ell=0}^{q}\sum_{\ell^{\prime}=0}^{q}\frac{\alpha_{\ell,d} \alpha_{\ell^{\prime},d}}{|\mathbb{S}^{d-1}|}\cdot\mathop{\mathbb{E}}_{v\sim \mathcal{U}(\mathbb{S}^{d-1})}\left[P_{d}^{\ell}\left(\langle\sigma,v\rangle \right)\cdot P_{d}^{\ell^{\prime}}\left(\langle v,w\rangle\right)\right]\] \[=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}P_{d }^{\ell}\left(\langle\sigma,w\rangle\right)=\phi_{w}(\sigma),\] (12)

where the third line above follows from Lemma 1. Therefore, the test function \(g:=\phi_{w}\) satisfies the constraint of the minimization in Eq. (9), i.e., \(\mathcal{K}_{d}^{(q)}g=\phi_{w}\). Thus, Lemma 4 implies that,

\[\tau_{q}(w)\leq\|g\|_{\mathbb{S}^{d-1}}^{2}=\|\phi_{w}\|_{\mathbb{S}^{d-1}}^{2 }=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|},\]

where the equality above follows from Lemma 1 along with Eq. (2). This establishes the upper bound on the leverage function that we sought to prove.

Now, using the maximization characterization of the leverage function in Definition 3, we prove that \(\tau_{q}(w)\geq\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}\). Again, we consider the same test function \(g=\phi_{w}\) and write,

\[\left\|\mathcal{K}_{d}^{(q)}\phi_{w}\right\|_{\mathbb{S}^{d-1}}^{ -2}\cdot\left|\left[\mathcal{K}_{d}^{(q)}\phi_{w}\right](w)\right|^{2} =\frac{|\phi_{w}(w)|^{2}}{|\phi_{w}\|_{\mathbb{S}^{d-1}}^{2}}\] \[=\frac{\left|\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S} ^{d-1}|}P_{d}^{\ell}\left(\langle w,w\rangle\right)\right|^{2}}{\sum_{\ell=0}^ {q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}}=\sum_{\ell=0}^{q}\frac{\alpha_ {\ell,d}}{|\mathbb{S}^{d-1}|},\]

where the first and second line above follow from Eq. (12) and Lemma 1, respectively. Therefore, the max characterization of the leverage function in Definition 3 implies that,

\[\tau_{q}(w)\geq\left\|\mathcal{K}_{d}^{(q)}\phi_{w}\right\|_{\mathbb{S}^{d-1} }^{-2}\cdot\left|\left[\mathcal{K}_{d}^{(q)}\phi_{w}\right](w)\right|^{2}= \sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}.\]

This completes the proof of Lemma 5 and establishes that \(\tau_{q}(w)\) is uniformly equal to \(\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{|\mathbb{S}^{d-1}|}\). 

To prove Theorem 4, we need to use prior results about solving linear systems in continuous setting via leverage score sampling. In particular, we use Theorem 6.3 from [19], which is restated below,

**Theorem 7** (Theorem 6.3 of [19]).: _Consider any dimension \(n\) linear space \(\mathcal{F}\) of functions from a domain \(G\) to \(\mathbb{R}\). Let \(D\) be a distribution over \(G\) and \(f\) be some function from \(G\) to \(\mathbb{R}\). Also, define the norm with respect to \(D\) of any function \(h:G\to\mathbb{R}\) as \(\|h\|_{D}^{2}:=\mathbb{E}_{x\sim D}[|h(x)|^{2}]\) and let \(y=\arg\min_{h\in\mathcal{F}}\|h-f\|_{D}^{2}\). Fix any distribution \(D^{\prime}\) over \(G\) and let \(K_{D^{\prime}}:=\sup_{x\in G}\sup_{h\in\mathcal{F}}\left\{\frac{D(x)}{D^{ \prime}(x)}\cdot\frac{|h(x)|^{2}}{\|h\|_{D}^{2}}\right\}\)._

_For i.i.d. sample query points \(x_{1},x_{2},\ldots x_{s}\sim D^{\prime}\) and weights \(w_{i}=\frac{D(x_{i})}{s\cdot D^{\prime}(x_{i})}\) for \(i\in[s]\), let the weighted ERM estimator \(\widetilde{f}_{s}\) be defined as \(\widetilde{f}_{s}:=\arg\min_{h\in\mathcal{F}}\sum_{i=1}^{s}w_{i}\cdot|h(x_{i} )-f(x_{i})|^{2}\). For any \(\varepsilon>0\) and a sufficiently large fixed constant \(c\), if the number of queries \(s\geq c\cdot\left(K_{D^{\prime}}\log n+\frac{K_{D^{\prime}}}{\varepsilon}\right)\), then the weighted ERM estimator \(\widetilde{f}_{s}\) satisfies,_

\[\Pr\left[\left\|\widetilde{f}_{s}-y\right\|_{D}^{2}\leq\varepsilon\cdot\left\| f-y\right\|_{D}^{2}\right]\geq 1-10^{-4}.\]Now we are ready to prove Theorem 4. Our approach is to apply Theorem 7 to the space of degree \(q\) spherical harmonics \(\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) and use the fact that the leverage scores distribution of the operator \(\mathcal{K}_{d}^{(q)}\) are uniform on the unit sphere \(\mathbb{S}^{d-1}\).

**Theorem 4** (Approximate Regression via Leverage Function Sampling).: _For any \(\varepsilon>0\), let \(s=c\cdot\left(\beta_{q,d}\log\beta_{q,d}+\frac{\beta_{q,d}}{\varepsilon}\right)\), for sufficiently large fixed constant \(c\), and let \(x_{1},x_{2},\ldots,x_{s}\) be i.i.d. uniform samples on \(\mathbb{S}^{d-1}\). Define the quasi-matrix \(\bm{P}:\mathbb{R}^{s}\to L^{2}(\mathbb{S}^{d-1})\) as follows, for every \(v\in\mathbb{R}^{d}\):_

\[[\bm{P}\;v](\sigma):=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{\sqrt{s}\cdot| \mathbb{S}^{d-1}|}\cdot\sum_{j=1}^{s}v_{j}\cdot P_{d}^{\ell}\left(\left\langle x _{j},\sigma\right\rangle\right)\quad\text{for }\sigma\in\mathbb{S}^{d-1}.\]

_Also let \(\bm{f}\in\mathbb{R}^{s}\) be a vector with \(\bm{f}_{j}:=\frac{1}{\sqrt{s}}\cdot f(x_{j})\) for \(j=1,2,\ldots,s\) and let \(\bm{P}^{\star}\) be the adjoint of \(\bm{P}\). If \(\tilde{g}\) is an optimal solution to the least-squares problem \(\tilde{g}\in\arg\min_{g\in L^{2}(\mathbb{S}^{d-1})}\|\bm{P}^{\star}g-\bm{f}\|_ {2}^{2}\), then with probability at least \(1-10^{-4}\) the following holds,_

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq(1+ \varepsilon)\cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{( q)}g-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Proof.: We prove this theorem by invoking Theorem 7. To do so, we first let the space \(\mathcal{F}\) of function from \(\mathbb{S}^{d-1}\) to \(\mathbb{R}\) be \(\mathcal{F}:=\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\). It is clear that the space of spherical harmonics is a linear space of functions because of the existence of the kernel operator \(\mathcal{K}_{d}^{(q)}\) which is a projection operator onto \(\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\), so \(\mathcal{F}\) satisfies the first precondition of Theorem 7. Additionally, the dimension of this space of functions is \(n=\beta_{q,d}\).

Also, let \(D\) be a uniform distribution over the unit sphere \(\mathbb{S}^{d-1}\). For this distribution, the norm defined in Theorem 7 satisfies \(\left\|h\right\|_{\mathbb{S}^{d-1}}^{2}=|\mathbb{S}^{d-1}|\cdot\left\|h \right\|_{D}^{2}\), for any \(h\in L^{2}(\mathbb{S}^{d-1})\). Moreover, let \(D^{\prime}\) be a uniform distribution over the unit sphere \(\mathbb{S}^{d-1}\) as well.

Now we show that for these choices of \(\mathcal{F},D,D^{\prime}\), the condition number \(K_{D^{\prime}}\) defined as per Theorem 7 is equal to \(\beta_{q,d}\). We can write,

\[K_{D^{\prime}} :=\sup_{x\in\mathbb{S}^{d-1}}\sup_{h\in\mathcal{F}}\left\{\frac{ D(x)}{D^{\prime}(x)}\cdot\frac{|h(x)|^{2}}{\left\|h\right\|_{D}^{2}}\right\}\] \[=|\mathbb{S}^{d-1}|\cdot\sup_{x\in\mathbb{S}^{d-1}}\sup_{h\in \mathcal{F}}\left\{\frac{|h(x)|^{2}}{\left\|h\right\|_{\mathbb{S}^{d-1}}^{2}}\right\}\] \[=|\mathbb{S}^{d-1}|\cdot\sup_{x\in\mathbb{S}^{d-1}}\sup_{g\in L^ {2}(\mathbb{S}^{d-1})}\left\{\frac{\left|\left[\mathcal{K}_{d}^{(q)}g\right] (x)\right|^{2}}{\left\|\mathcal{K}_{d}^{(q)}g\right\|_{\mathbb{S}^{d-1}}^{2}}\right\}\] \[=|\mathbb{S}^{d-1}|\cdot\sup_{x\in\mathbb{S}^{d-1}}\tau_{q}(x)\] \[=\beta_{q,d},\]

where the second line above follows from the fact that \(D,D^{\prime}\) are both equal to the uniform distribution over \(\mathbb{S}^{d-1}\) and \(\left\|h\right\|_{\mathbb{S}^{d-1}}^{2}=|\mathbb{S}^{d-1}|\cdot\left\|h\right\|_ {D}^{2}\). The third line above follows from the fact that any function \(h\in\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) can be expressed as \(h=\mathcal{K}_{d}^{(q)}g\) for some \(g\in L^{2}(\mathbb{S}^{d-1})\) because \(\mathcal{K}_{d}^{(q)}\) is the projection operator onto \(\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\). The fourth line follows from Definition 3 and last line follows from Lemma 5.

Finally, in order to invoke Theorem 7, we can write that the weighted ERM estimator \(\widetilde{f}_{s}\) is equal to the following,

\[\widetilde{f}_{s} :=\operatorname*{arg\,min}_{h\in\mathcal{F}}\sum_{i=1}^{s}w_{i} \cdot|h(x_{i})-f(x_{i})|^{2}\] \[=\operatorname*{arg\,min}_{g\in L^{2}(\mathbb{S}^{d-1})}\sum_{i=1 }^{s}w_{i}\cdot\left|\left[\mathcal{K}_{d}^{(q)}g\right](x_{i})-f(x_{i}) \right|^{2}\] \[=\operatorname*{arg\,min}_{g\in L^{2}(\mathbb{S}^{d-1})}\sum_{i=1 }^{s}\left|\frac{1}{\sqrt{s}}\cdot\left[\mathcal{K}_{d}^{(q)}g\right](x_{i})- \boldsymbol{f}_{i}\right|^{2}\] \[=\operatorname*{arg\,min}_{g\in L^{2}(\mathbb{S}^{d-1})}\left\| \boldsymbol{P}^{*}g-\boldsymbol{f}\right\|_{2}^{2},\]

where the third line above uses the definition of \(\boldsymbol{f}_{i}=\frac{1}{\sqrt{s}}f(x_{i})\), and the last line follows from the definition of adjoint of the quasi-matrix \(\boldsymbol{P}\). Therefore, the theorem follows by invoking Theorem 7.

## Appendix D Efficient Algorithm for Spherical Harmonic Interpolation

In this section we prove our main theorem about our spherical harmonic interpolation algorithm.

**Theorem 5** (Efficient Spherical Harmonic Interpolation).: _Algorithm 1 returns a function \(y\in\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\) such that, with probability at least \(1-10^{-4}\):_

\[\left\|y-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\varepsilon\cdot\left\|f^{ (q)}-f\right\|_{\mathbb{S}^{d-1}}^{2},\;\;\text{ where }f^{(q)}:=\mathcal{K}_{d}^{(q)}f.\]

_Suppose we can compute the Gegenbauer polynomial \(P_{d}^{\ell}(t)\) at every point \(t\in[-1,1]\) in constant time. Then Algorithm 1 queries the function \(f\) at \(s=\mathcal{O}\left(\beta_{q,d}\log\beta_{q,d}+\frac{\beta_{q,d}}{\varepsilon}\right)\) points on the sphere \(\mathbb{S}^{d-1}\) and runs in \(\mathcal{O}(s^{2}\cdot d+s^{\omega})\) time. This algorithm evaluates \(y(\sigma)\) in \(\mathcal{O}(d\cdot s)\) time for any \(\sigma\in\mathbb{S}^{d-1}\)._

Proof.: First note that the random points \(w_{1},w_{2},\dots,w_{s}\) in line 3 of Algorithm 1 are i.i.d. sample with uniform distribution on the surface of \(\mathbb{S}^{d-1}\). Therefore, we can invoke Theorem 4. More specifically, if we let \(\boldsymbol{P}\) be the quasi-matrix defined in Theorem 4 corresponding to the random points \(w_{1},w_{2},\dots,w_{s}\) sampled in line 3 and if we let \(\boldsymbol{f}\) be the vector of function samples defined in line 5 of the algorithm, then with probability at least \(1-10^{-4}\), any optimal solution to the following least-squares problem

\[\tilde{g}\in\operatorname*{arg\,min}_{g\in L^{2}(\mathbb{S}^{d-1})}\left\| \boldsymbol{P}^{*}g-\boldsymbol{f}\right\|_{2}^{2},\] (13)

satisfies the following,

\[\left\|\mathcal{K}_{d}^{(q)}\tilde{g}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq(1+ \varepsilon)\cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^{ (q)}g-f\right\|_{\mathbb{S}^{d-1}}^{2}.\] (14)

Now note that the least-squares problem in Eq. (13) has at least one optimal solution \(\tilde{g}\) which is in the eigenspace of the operator \(\boldsymbol{P}\boldsymbol{P}^{*}\). More specifically, there exists a vector \(\boldsymbol{z}\in\mathbb{R}^{s}\) such that \(\tilde{g}=\boldsymbol{P}\cdot\boldsymbol{z}\) is an optimal solution for Eq. (13). Therefore, we can focus on finding this optimal solution by solving the following least-squares problem

\[\boldsymbol{z}\in\operatorname*{arg\,min}_{\boldsymbol{x}\in\mathbb{R}^{s}} \left\|\boldsymbol{P}^{*}\boldsymbol{P}\boldsymbol{x}-\boldsymbol{f}\right\| _{2}^{2},\]

and then letting \(\tilde{g}=\boldsymbol{P}\cdot\boldsymbol{z}\). This \(\tilde{g}\) is guaranteed to be an optimal solution for Eq. (13), thus it satisfies Eq. (14). We solve the above least-squares problem using the kernel trick. In fact we show that \(\boldsymbol{P}^{*}\boldsymbol{P}\) is equal to the kernel matrix \(\boldsymbol{K}\) computed in line 4 of Algorithm 1. To see why, note that for any \(i,j\in[s]\) we have,

\[\left[\boldsymbol{P}^{\star}\boldsymbol{P}\right]_{i,j} =\left\langle\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{\sqrt{s}\cdot \left|\mathbb{S}^{d-1}\right|}\cdot P_{d}^{\ell}\left(\left\langle w_{i}, \cdot\right\rangle\right),\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{\sqrt{s} \cdot\left|\mathbb{S}^{d-1}\right|}\cdot P_{d}^{\ell}\left(\left\langle w_{j}, \cdot\right\rangle\right)\right\rangle_{\mathbb{S}^{d-1}}\] \[=\sum_{\ell=0}^{q}\sum_{\ell^{\prime}=0}^{q}\frac{\alpha_{\ell,d} \alpha_{\ell^{\prime},d}}{s\cdot\left|\mathbb{S}^{d-1}\right|}\cdot\left\langle P _{d}^{\ell}\left(\left\langle w_{i},\cdot\right\rangle\right),P_{d}^{\ell^{ \prime}}\left(\left\langle w_{j},\cdot\right\rangle\right)\right\rangle_{ \mathbb{S}^{d-1}}\] \[=\sum_{\ell=0}^{q}\sum_{\ell^{\prime}=0}^{q}\frac{\alpha_{\ell,d} \alpha_{\ell^{\prime},d}}{s\cdot\left|\mathbb{S}^{d-1}\right|}\cdot\underset {v\sim\mathcal{U}\left(\mathbb{S}^{d-1}\right)}{\mathbb{E}}\left[P_{d}^{\ell} \left(\left\langle w_{i},v\right\rangle\right)\cdot P_{d}^{\ell^{\prime}} \left(\left\langle w_{j},v\right\rangle\right)\right]\] \[=\sum_{\ell=0}^{q}\frac{\alpha_{\ell,d}}{s\cdot\left|\mathbb{S}^{ d-1}\right|}\cdot P_{d}^{\ell}\left(\left\langle w_{i},w_{j}\right\rangle \right)=\boldsymbol{K}_{i,j},\]

where the fourth line above follows from Lemma 1. Therefore, we are interested in the optimal solution of the following least-squares problem

\[\boldsymbol{z}\in\arg\min_{\boldsymbol{x}\in\mathbb{R}^{s}}\left\|\boldsymbol{ K}\boldsymbol{x}-\boldsymbol{f}\right\|_{2}^{2}.\]

The least-squares solution to the above problem is \(\boldsymbol{z}=\boldsymbol{K}^{\dagger}\boldsymbol{f}\) which is exactly what is computed in line 6 of the algorithm. Now note that, the function \(\tilde{g}=\boldsymbol{P}\cdot\boldsymbol{z}\) satisfies Eq. (14). Because \(\tilde{g}=\boldsymbol{P}\cdot\boldsymbol{z}\in\mathcal{H}^{(q)}(\mathbb{S}^{d- 1})\) and because \(\mathcal{K}_{d}^{(q)}\) is an orthonormal projection operator into \(\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\), we have \(\mathcal{K}_{d}^{(q)}\cdot\tilde{g}=\tilde{g}=\boldsymbol{P}\cdot\boldsymbol{z}\). This together with Eq. (14) imply that,

\[\left\|\boldsymbol{P}\cdot\boldsymbol{z}-f\right\|_{\mathbb{S}^{d-1}}^{2}\leq (1+\varepsilon)\cdot\min_{g\in L^{2}(\mathbb{S}^{d-1})}\left\|\mathcal{K}_{d}^ {(q)}g-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Now if we invoke Claim 2 with \(C=1+\varepsilon\) on the above inequality we find that,

\[\left\|\boldsymbol{P}\cdot\boldsymbol{z}-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{ 2}\leq\varepsilon\cdot\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}.\]

Finally, one can easily see that the function \(y\in\mathcal{H}^{(q)}(\mathbb{S}^{d-1})\) that Algorithm 1 outputs in line 7 is exactly equal to \(y=\boldsymbol{P}\cdot\boldsymbol{z}\). This completes the accuracy bound of the theorem.

Runtime and Sample Complexity.these bounds follow from observing that:

* \(s\cdot d\) time is needed to generate \(w_{1},w_{2},\ldots,w_{s}\) in line 3 of the algorithm. To do this, we first generate random Gaussian points in \(\mathbb{R}^{d}\) and then project then onto \(\mathbb{S}^{d-1}\) by normalizing them.
* \(s^{2}\cdot d\) operations are needed to form the kernel matrix \(\boldsymbol{K}\) in line 4 of the algorithm.
* \(s\) queries to function \(f\) are needed to form the samples vector \(\boldsymbol{f}\) in line 5 of the algorithm.
* \(s^{\omega}\) time is needed to compute the least-squares solution \(\boldsymbol{z}=\boldsymbol{K}^{\dagger}\boldsymbol{f}\) in line 6 of the algorithm.
* \(s\cdot d\) operations are needed to evaluate the output function \(y(\sigma)\) in line 7 of the algorithm.

This completes the proof of Theorem 5. 

## Appendix E Lower Bound: Claims and Lemmas

In this section we prove the Claims and Lemmas used in our lower bound analysis for proving Theorem 6.

**Claim 4**.: _Given the random input \(f=\sum_{\ell=0}^{q}\boldsymbol{Y}_{\ell}\cdot v^{(\ell)}\) generated as described in Section 4, to solve Problem 2, an algorithm must return a function \(\tilde{f}^{(q)}\in\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) such that \(\|\tilde{f}^{(q)}-f\|_{\mathbb{S}^{d-1}}^{2}=0\)._Proof.: Note that Problem 2 requires recovering a function \(\tilde{f}^{(q)}\in\mathcal{H}^{(q)}\left(\mathbb{S}^{d-1}\right)\) such that:

\[\left\|\tilde{f}^{(q)}-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq \varepsilon\cdot\left\|f^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2},\] (15)

where \(f^{(q)}=\mathcal{K}_{d}^{(q)}f\). Using the definition of the input function \(f=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\), we can write,

\[f^{(q)}=\mathcal{K}_{d}^{(q)}f =\sum_{\ell=0}^{q}\mathcal{K}_{d}^{(q)}\cdot\bm{Y}_{\ell}\cdot v ^{(\ell)}\] \[=\sum_{\ell=0}^{q}\left(\sum_{\ell^{\prime}=0}^{q}\bm{Y}_{\ell^{ \prime}}\bm{Y}_{\ell^{\prime}}^{*}\right)\cdot\bm{Y}_{\ell}\cdot v^{(\ell)}\] \[=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}=f,\]

where the equality in the second line above follows from Eq. (10) and the addition theorem in Theorem 3, and the third line follows because the operator \(\bm{Y}_{\ell}\) has orthonormal columns and thus \(\bm{Y}_{\ell^{\prime}}\bm{Y}_{\ell}=I_{\alpha_{\ell,d}}\cdot 1_{\{\ell=\ell^{ \prime}\}}\). Therefore, plugging this into Eq. (15) gives,

\[\left\|\tilde{f}^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}=\left\|\tilde{f}^{(q) }-f^{(q)}\right\|_{\mathbb{S}^{d-1}}^{2}\leq\varepsilon\cdot\left\|f^{(q)}-f \right\|_{\mathbb{S}^{d-1}}^{2}=\varepsilon\cdot\left\|f-f\right\|_{\mathbb{S }^{d-1}}^{2}=0.\]

**Lemma 6**.: _If a deterministic algorithm solves Problem 2 with probability at least \(1/10\) over our random input distribution \(f=\sum_{\ell=0}^{q}\bm{Y}_{\ell}\cdot v^{(\ell)}\), then with probability at least \(1/10\), the output of the algorithm \(\tilde{f}^{(q)}\) satisfies \(\bm{Y}_{\ell}^{*}\tilde{f}^{(q)}=v^{(\ell)}\) for all integers \(\ell\leq q\)._

Proof.: By Claim 4, the output of the algorithm that solves Problem 2, satisfies \(\left\|\tilde{f}^{(q)}-f\right\|_{\mathbb{S}^{d-1}}^{2}=0\). Therefore, by orthonormality of the columns of the operator \(\bm{Y}_{\ell}\), we can write,

\[\bm{Y}_{\ell}^{*}\tilde{f}^{(q)}=\bm{Y}_{\ell}^{*}f+\bm{Y}_{\ell}^{*}(\tilde{ f}^{(q)}-f)=\sum_{\ell^{\prime}=0}^{q}\bm{Y}_{\ell}^{*}\bm{Y}_{\ell^{\prime}} \cdot v^{(\ell^{\prime})}=v^{(\ell)}.\]