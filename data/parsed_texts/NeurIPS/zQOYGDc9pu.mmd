# Optimized Covariance Design for AB Test on Social Network under Interference

 Qianyi Chen\({}^{1}\), Bo Li\({}^{1}\), Lu Deng\({}^{2}\), Yong Wang \({}^{2}\)

\({}^{1}\)School of Economics and Management, Tsinghua University, China

\({}^{2}\)Tencent Inc., Shenzhen, China

cqy22@mails.tsinghua.edu.cn, libo@sem.tsinghua.edu.cn

adamdeng@tencent.com, darwinwang@tencent.com

Corresponding author

###### Abstract

Online A/B tests have become increasingly popular and important for social platforms. However, accurately estimating the global average treatment effect (GATE) has proven to be challenging due to network interference, which violates the Stable Unit Treatment Value Assumption (SUTVA) and poses great challenge to experimental design. Existing network experimental design research was mostly based on the unbiased Horvitz-Thompson (HT) estimator with substantial data trimming to ensure unbiasedness at the price of high resultant estimation variance. In this paper, we strive to balance the bias and variance in designing randomized network experiments. Under a potential outcome model with 1-hop interference, we derive the bias and variance of the standard HT estimator and reveal their relation to the network topological structure and the covariance of the treatment assignment vector. We then propose to formulate the experimental design problem as to optimize the covariance matrix of the treatment assignment vector to achieve the bias and variance balance by minimizing a well-crafted upper bound of the mean squared error (MSE) of the estimator, which allows us to decouple the unknown interference effect component and the experimental design component. An efficient projected gradient descent algorithm is presented to the implement of the desired randomization scheme. Finally, we carry out extensive simulation studies2 to demonstrate the advantages of our proposed method over other existing methods in many settings, with different levels of model misspecification.

Footnote 2: The code is available at https://github.com/Cqyiiii/Optimized_Covariance_Design-NIPS2023

## 1 Introduction

A/B test, also known as controlled experiment, is an effective way to evaluate treatment effect of new product features and facilitate data-driven decision-making [14]. Due to its simplicity and efficiency, it has become the gold standard in the industry. The theory of A/B test is based on the Stable Unit Treatment Value Assumption (SUTVA), under Neyman-Rubin's framework [33], which assumes unit's treatment effect isn't influenced by other units' treatment assignments. However, in situations where social network connects units, SUTVA is often violated since an individual's behavior can be influenced by their friends. This violation of SUTVA is referred as network interference or social interference, which brings challenges to both estimation and inference of parameters of interest. It has been shown that ignoring network effect can induce much higher bias or variance [29].

To tackle the bias and variance induced by network interference, researchers have tried to design randomization schemes in pre-treatment phase and estimators or adjustment methods in post-treatment phase. In this paper, we will focus on designing randomization scheme, namely, experiment design.

Specifically, we take the graph structure as known and leverage it to assist experiment design. Our basic scheme is cluster-level randomization, which treats cluster or community in the graph as unit of treatment and all units in the same cluster share the same treatment. [19] is an early work investigating this approach, and [35] proposes to form clusters utilizing graph. The graph cluster randomization is conceptually simple but effective in many scenes, and thus becomes a prevalent paradigm for network experiment design. Moreover, we consider experimental design on a social network in this work. Social network typically has organic structure and high clustering coefficient, which corresponds to meaningful cluster structure and also motivates us to adopt cluster-level randomization. We assume the cluster structure of the network is given. In practice, interpretable community detection algorithms such as the Louvain algorithm [4] are typically applied for cluster construction. Moreover, we stress the scalability of our method since social network is usually large-scale in reality.

Related works have shown that there exists an explicit bias-variance tradeoff [23, 41]. However, many HT-style estimators that utilize exposure indicator to ensure unbiasedness are designed on the basis of the neighborhood interference assumption that restricts global interference to 1-hop neighborhood [10, 25, 36, 39, 9], and graph-dependent generalized propensity score [1]. Nonetheless, we argue that pursuing unbiasedness by simple sample trimming leads to small effective sample size with high variance, and thus is far from optimal. Moreover, computational cost is also high in this approach since a large number of Monte Carlo simulations are usually needed for computing the generalized propensity score, or the trimming probability for constructing sample weights. Having observed the preceding drawbacks of the unbiased estimator, we consider the standard unit-level HT estimator to achieve possible better bias and variance tradeoff with less computation burden at the meantime. Based on a potential outcome model, we characterize the relationship among the bias, variance, the network topology, and the treatment assignment mechanism. This development allows us to design experimental schemes that minimizes MSE and achieve better bias-variance tradeoff.

Besides from the time-consuming Monte Carlo simulation for calculating generalized propensity score mentioned above, the computational efficiency of experiment design is indeed a limitation of current methods. For instance, the two-wave experiment [37], entails solving a mixed-integer quadratic programming problem, which can be computationally expensive. The adaptive covariates-balancing design [25] adopts a sequential process for treatment assignment and thus isn't very scalable. To address this limitation, we propose to utilize gradient-based optimization algorithm.

In this paper, we propose to treat the covariance matrix of treatment vector as decision variable in optimization, which is nontrivial as a result of interference and corresponding complex variance of estimator. Similar approach in optimization step is proposed in [17], where interference isn't considered, and making covariance covariance acting as decision variable is a natural choice owing to the simple structure of variance there. We draw attention to the fact that related works concerning experiment design mostly concentrates on a predetermined class of randomization schemes and seeks to optimize the parameters to obtain designs that minimize variance or MSE of estimator [37, 23, 5, 42]. However, we argue that these designs can all be understood as different patterns of the covariance matrix of the treatment vector, and we propose to treat the covariance matrix as decision variables rather than limiting our scope to a particular class of designs. One of major challenges with this approach is generating a randomization scheme that supports legitimate sampling, as there may not be a joint distribution that corresponds to the optimized covariance matrix in our formulation. To address this issue, we propose to adopt projected gradient descent to guarantee the validity of optimized covariance matrix. Furthermore, we also compare our method with other popular designs through extensive simulation on a real online social network. Various interference intensity levels and potential outcome models are considered. It is found that our method is robust to model misspecification and outperforms in almost all settings, especially when the interference isn't too weak compared to the direct treatment effect, which is the scene we target.

**Contributions.** Our contributions are summarized as follows.

1. We derive a well-crafted upper bound for the MSE of the HT estimator that decouples the estimation of causal mechanism and experimental design. This enables us to optimize the experimental design by minimizing this bound, in which the covariance matrix of treatment vector acts as decision variables.
2. We propose the formulation of covariance optimization problem with reparameterization of covariance matrix and constraints that guarantees legitimate sampling subject to optimizedcovariance matrix, and a projected gradient descent algorithm is proposed for solving the optimization problem.
3. We conduct systematic simulation on a social network and compare our method with several methods proposed recently under a range of settings, providing credible reference for the effectiveness of our method.

**Related Works.** A variety of interference has been considered in related works[12], such as spatial [23], temporal [13] and network, which is the scene we target. Besides from the general network, there is also another line of work focused on a two-sided marketplace or platform, where the graph is bipartite [43; 24; 16; 30; 27].

In the field of network experiment design, most of extant research assumes that the graph structure is known and leverage it to assist in modeling the interference[2], though several graph agnostic methods are developed these days [40; 8]. Nevertheless, the interference is conducted through edges of network, and agnostic methods aren't effective enough.

Beyond unit-level randomization, cluster-level randomization on graph has attracted extensive attention, including both experimental design [23; 25; 5; 30] and estimation [26; 1; 6; 15]. The seminal paper of this line of work is [35], which proposes graph cluster randomization that treats clusters as units in randomization. This methodology is motivated by providing a better lower bound of exposure probability in HT estimator that uses exposure probability as inverse weight, aiming at controlling the variance of such an unbiased estimator. Along the same direction, [36] proposes a polynomial lower bound on exposure probability through randomized local clustering algorithm.

Irregular global interference may render the experimental design problem intractable [39], thus structural assumptions are usually introduced to rule out such arbitrary interference. The first category is partial interference [34; 19; 3; 11; 5], which assumes disjoint community structures and interference only happens within each community. The second category is neighborhood interference, which assumes the interference solely originates from units' 1-hop neighborhood [25; 36; 41; 12; 8]. This kind of assumption is more practical compared with partial interference, and thus widely applied. The third category is potential outcome model, which directly assumes the expression of potential outcome, e.g. classical Cliff-Ord auto-regressive model [7], linear potential outcome models[25; 39]. Actually, some potential outcome model implies neighborhood interference assumption in its mathematical expression. It's also worth to be noted that there also appear works trying to weaken the structural assumptions mentioned above and consider the interference of more general form [6; 22; 23; 18; 15].

## 2 Basic Framework

### Setup

We consider a finite population of \(n\) units, and the treatment vector is represented as \(\bm{z}=(z_{1},z_{2},\ldots,z_{n})\in\{0,1\}^{n}\). In this paper, we consider the experiment design problem in which a sensible random treatment assignment mechanism of all units is designed to achieve good performance of the resultant treatment effect estimation. We follow the Neyman-Rubin's framework [33]. Without SUTVA assumption, we define the potential outcome of unit \(i\) as \(Y_{i}=Y_{i}(\bm{z})\), which means the potential outcome of unit \(i\) can depend on the treatment assignment of all units.

The parameter of interest is the global average treatment effect (GATE), which is denoted by \(\tau\),

\[\tau:=\frac{1}{n}\sum_{i\in[n]}\left(Y_{i}(\bm{1})-Y_{i}(\bm{0})\right)\] (1)

Here we use \([n]\) to denote set \(\{1,2,...,n\}\), and \(\bm{1}(\bm{0})\) is the \(n\)-dimensional vector of \(1(0)\), which corresponds to global treatment and global control.

We denote the social network as \(\mathcal{G}=(\mathcal{V},\mathcal{E})\). The node set \(\mathcal{V}\) is \([n]\), corresponding to all units considered above. Edge set \(\mathcal{E}\) represents the social connection between units. Furthermore, we use \(d_{i}\) to denote the degree of node \(i\). We assume the graph structure \(\mathcal{G}\) is known in advance.

In this paper, we consider graph cluster randomization [35]. Here we also assume that cluster structure is given, which is generated from certain community detection algorithm. We define \(S_{k}\) as the \(k\)-thcluster, where \(k\in[K]\). Clusters compose a splitting of node set, namely,

\[S_{k}\subset[n]\quad S_{i}\cap S_{j}=\emptyset\quad\cup_{k=1}^{K}S_{k}=[n]\] (2)

The graph cluster randomization implies that units in the same cluster share the same treatment assignment, namely,

\[z_{i}=z_{j}\quad\forall i,j\in S_{k},k\in[K]\] (3)

Since we consider randomization scheme at cluster level, it's more convenient to define cluster-level treatment vector \(\bm{t}=(t_{1},t_{2},...,t_{K})\in\{0,1\}^{K}\). Specifically, we consider the balanced cluster-level randomization scheme satisfying

\[\mathbb{P}(t_{k}=1|\mathcal{G})=\frac{1}{2}\quad\mathbb{E}[z_{i}]=\frac{1}{2}\] (4)

### Potential Outcome Model and Estimators

One of the key ideas of this paper is to deduce an appropriate objective function to optimize experiment design schemes under consideration. In this part, we will consider standard HT estimator, namely,

\[\hat{\tau}=\frac{1}{n}\sum_{i\in[n]}\left((\frac{z_{i}}{\mathbb{E}[z_{i}]}- \frac{(1-z_{i})}{\mathbb{E}[1-z_{i}]})Y_{i}(\bm{z})\right)\] (5)

Then we introduce the considered potential outcome model.

**Assumption 1** (Potential Outcome Model): _The potential outcome of unit \(i\) is generated by_

\[Y_{i}(\bm{z})=\alpha_{i}+\beta_{i}z_{i}+\gamma\sum_{j\in N_{i}}z_{j}\] (6)

_where \(N_{i}\) is the 1-hop neighborhood of unit \(i\)._

In this potential outcome model, \(\beta_{i}\) can be interpreted as direct treatment effect of unit \(i\), and \(\gamma\) measures the interference effect, characterizing the extent to which each unit is affected by its neighborhood in the network. This model is a simplified (in interference part) version of over-parameterized model in [39], in which heterogeneous interference effects are accommodated. We make a somewhat stronger assumption to simplify the structure of the bias, variance and MSE. Under this model, we can express the GATE in terms of the potential outcome model parameters,

\[\tau=\frac{1}{n}\sum_{i\in[n]}\left(Y_{i}(\bm{1})-Y_{i}(\bm{0})\right)=\frac{ 1}{n}\sum_{i\in[n]}(\beta_{i}+\gamma d_{i})\] (7)

We will then derive the bias and variance of \(\hat{\tau}\) under this potential outcome model. We point out that the number of unknown parameters in this model is still greater than the number of known potential outcomes, which can be understood as overparameterization, and we only utilize it to derive an optimizable MSE upper bound that's not concerned with estimation of \(\beta_{i}\) and \(\gamma\) in this model. In fact, when the interference effect and the direct treatment effect are comparable at cluster-level, a multiplier of the MSE upper bound can be factored out, which is crucial in our methodology and will be shown in section 3.1.

### Bias of HT Estimator

Based on the potential outcome model, we can derive the bias and variance of our estimator explicitly. We firstly introduce cluster-level treatment assignment, \(\bm{t}=(t_{1},t_{2},...,t_{K})\), and matrix \(\bm{C}\) summarizing edges between(within) clusters with its elements defined as

\[\bm{C}_{ij}=|\{(u,v):(u,v)\in\mathcal{E},u\in S_{i},v\in S_{j}\}|\] (8)

We have following proposition about bias.

**Proposition 1** (Bias of HT Estimator): _The bias of standard HT estimator is_

\[\mathbb{E}[\hat{\tau}]-\tau=\frac{\gamma}{n}\left(4\operatorname{trace}(\bm{C} \operatorname{Cov}[\bm{t}])-\sum_{i,j\in[K]}\bm{C}_{ij}\right)\] (9)

_where \(\operatorname{Cov}[\bm{t}]\) refers to the covariance matrix of treatment vector \(\bm{t}\)._

Specifically, we can delineate the source of bias in our setting. If we consider graph cluster randomization as basic scheme, edges within cluster don't contribute any bias, while every edge between clusters contributes to bias according to the covariance of cluster-level treatment assignments. This enlightens us that independent treatment assignments is far from optimal, and we can reduce bias by assigning treatments with appropriate positive correlations that should be higher for more densely connected cluster pair. Furthermore, we point out that bias may be of no account when a very sparse network is considered, but it indeed counts in our context of social network.

We also note that the treatment vector is summarized by its covariance matrix in the expression of bias. Hence, we denote the bias by \(B(\operatorname{Cov}[\bm{t}])\).

### Variance of HT Estimator

To derive the variance, we plug in the potential outcome model and express the estimator as

\[\hat{\tau}=\frac{2}{n}\sum_{i=1}^{n}-\alpha_{i}+(\beta_{i}+2\alpha_{i}-\gamma d _{i})z_{i}+2\gamma\sum_{j\in N_{i}}z_{i}z_{j}\] (10)

An important difference in the derivation of variance contrasting that of bias is all base levels \(\alpha_{i}\) is no longer negligible. Hence, we consider to target at the scenes that we can acquire knowledge of all base levels in advance.

**Assumption 2** (Known Base Levels): _The base level \(\alpha_{i}\) of unit \(i\) is known in advance for all units \(i\)._

This assumption is strong in the classic context of experiment design, while we propose to target the scene where AB tests are launched by social platforms. On the one hand, the improvement in user activity indices brought by new traits or interventions, specifically, direct treatment effect and interference, is typically much smaller than the base levels \(\alpha_{i}\), which means the experiment design may be quite challenging when agnostic to base levels. On the other hand, it's very reasonable to assume that we know all \(\alpha_{i}\)s before conducting experiments since those indices we're concerned with are recorded and/or estimated daily or weekly by social platforms.

Utilizing this assumption, we can first adjust our estimator as

\[\hat{\tau}_{-\alpha}=\frac{1}{n}\sum_{i\in[n]}\left((\frac{z_{i}}{\mathbb{E}[ z_{i}]}-\frac{(1-z_{i})}{\mathbb{E}[1-z_{i}]})(Y_{i}(\bm{z})-\alpha_{i})\right)\] (11)

This adjustment has also been recommended by [39]. Notice that this adjustment helps us remove the influence of base level \(\alpha_{i}\) when we consider variance. In the following, we'll adhere to this format and proceed to derive the variance term, which is summarized in the following proposition.

**Proposition 2** (Variance of HT Estimator): _The variance of standard HT estimator is_

\[\operatorname{Var}[\hat{\tau}]=\frac{4}{n^{2}}(\operatorname{trace}(\bm{h} \bm{h}^{T}\operatorname{Cov}[\bm{t}])+4\gamma\operatorname{Cov}[\bm{h}^{T}\bm {t},\bm{t}^{T}\bm{C}\bm{t}]+4\gamma^{2}\operatorname{Var}[\bm{t}^{T}\bm{C}\bm{ t}])\] (12)

_where \(\bm{h}\) is the vector \((\sum_{i\in S_{k}}\beta_{i}-\gamma d_{i})_{k=1}^{K}\)._

Notice that the expression of variance contains second-order term with regard to \(t\), namely, \(\operatorname{Cov}[\bm{t}]\), as well as third and fourth-order terms involving \(\bm{t}^{T}\bm{C}\bm{t}\), a quadratic form of \(\bm{t}\). It's feasible to analyze the variance precisely with a specific randomization scheme, such as the independent block randomization [5], but it is generally not straightforward to optimize a broad set of randomization schemes in its exact form. In the following subsection, we will derive a bound of variance to transform third and fourth-order terms into second-order, which also motivates our optimized covariance design.

We comment that a bias-variance tradeoff is unveiled in the previous two propositions. Actually, \(\bm{C}\) is a matrix with non-negative elements, and introducing positive covariance between treatment assignments of different clusters will reduce the bias. On the other hand, introducing positive covariance means an increment in variance. In addition, though this paper doesn't involve upstream clustering algorithm, the resolution of clustering can also impact the relative importance of bias and variance, namely, low resolution usually corresponds to a higher proportion of edges within cluster, which weakens the importance of bias and stress on variance.

## 3 Optimized Covariance Design

### Constructing Objective for Optimization

Although we have derived the expression of bias and variance, the expression of variance is complicated due to the existence of treatment effects \(\beta_{i}\)s, the interference effect \(\gamma\), and the higher-order terms involving the treatment vector. We will bypass the first hurdle by adding reasonable assumption about the direct treatment effect and interference effect. The second challenge will be addressed by a upper bound argument.

We first consider the former issue, which is the relationship between \(\beta_{i}\) and \(\gamma_{i}\). Since we consider the interference, we propose to restrict our scope to the situation that network interference isn't too weak, which is formulated as following assumption.

**Assumption 3** (Comparability between Direct Treatment Effect and Interference): _Given potential outcome model in equation (6), we assume there exists a constant \(\omega>0\) such that_

\[|\bm{h}_{k}|\leq\omega\gamma(\sum_{i\in S_{k}}d_{i})\] (13)

_holds for each cluster \(k\in[K]\)._

This assumption actually demands the magnitude of interference is comparable to direct effect at cluster level. Namely, we allow a significant difference between direct treatment effect \(\beta_{i}\) and interference \(\gamma d_{i}\) for certain units, but there isn't too much difference after aggregation in cluster. The constant \(\omega\) can be viewed as degree of tolerance on incomparability between direct causal effect and interference and depends on the both nature of potential outcome and graph structure. It's natural that people may be more susceptible to the states of their friends in some social metrics. Since we'll utilize the MSE bound as optimization objective, this constant can also be viewed as tuning parameter that's determined by domain knowledge or pilot experiment. To demonstrate the robustness of our method, we don't tune \(\omega\) according to the result and fix \(\omega=1\) in the simulation, which corresponds to a moderate constraint on comparability. Empirically, our result isn't sensitive to the value of \(\omega\).

We then discuss the latter issue. We bound all high-order terms by second or lower order terms. This effort leads to an upper bound which allows us to simply optimize the covariance matrix.

**Proposition 3** (Variance Bound): _The variance of the standard HT estimator has following upper bound,_

\[\mathrm{Var}[\hat{\tau}]\leq\frac{8\gamma^{2}(\omega^{2}+4)}{n^{2}}\operatorname {trace}\left(\bm{d}\bm{d}^{T}(\mathrm{Cov}[\bm{t}]+\frac{1}{4}\bm{1}\bm{1}^{T})\right)\] (14)

_where \(\bm{d}\) is the vector \((\sum_{i\in S_{k}}d_{i})_{k=1}^{K}\)._

In brief, we choose to bound the third-order term by the sum of second and fourth-order terms, and bound the fourth-order term by the definition of binary treatment vector. The format is adjusted for our optimization formulation later, where we choose the covariance matrix as decision variables.

Actually, we can find that experiment design is summarized by the covariance matrix in this upper bound. Hence, we denote this upper bound by \(\tilde{V}_{\omega}(\mathrm{Cov}[\bm{t}])\). Notice that there is a common multiplier \(\gamma^{2}\), which is associated with the unknown interference effect, in the squared bias and variance. This appealing feature allows us to optimize the MSE upper bound without estimating the parameter of the potential outcome model.

Combining proposition 1 and 3, we derive following upper bound for MSE.

**Theorem 1**: _The MSE of the standard HT estimator has following upper bound_

\[\mathrm{MSE}\leq B(\mathrm{Cov}[\bm{t}])^{2}+\bar{V}_{\omega}(\mathrm{Cov}[\bm{t}])\] (15)

### Optimizing Covariance

Now we can discuss our experiment design. For a class of parameterized randomization schemes, one can always analyze the covariance and express the bias and variance with parameters in randomization schemes. Based on the derivation in previous section, we argue that these randomization schemes is actually specifying the patterns of covariance matrix, e.g., the block diagonal pattern. We thus choose to directly optimize the covariance matrix of randomization scheme, which is actually considering a much larger hypothesis space.

Since our decision variable is a positive semi-definite matrix, it's intuitive to make attempts for formulating the problem as positive semi-definite programming with the MSE bound acting as objective. However, notice that there is still one square term with regard to \(\mathrm{trace}(\bm{C}X)\), which appears in the \(B(X)^{2}\). If we want to transform it into semi-definite programming (SDP), we must modify the objective further. Moreover, there are some intrinsic disadvantages of this formulation. Primarily, after we solve the optimal covariance matrix, we can't directly sample from the multivariate Bernoulli distribution that is subject to this covariance matrix. Secondly, method based on mathematical programming is usually precise but not efficient enough.

To guarantee that we can sample treatment vector subject to the optimized covariance, we first present a lemma that is equivalent to the Grothendieck's identity [28].

**Lemma 1**: _Suppose that \((X,Y)\) follows a bivariate Gaussian distribution with correlation \(r\), namely,_

\[\begin{pmatrix}X\\ Y\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}0\\ 0\end{pmatrix},\begin{pmatrix}1&r\\ r&1\end{pmatrix}\right)\] (16)

_we have_

\[\mathrm{Cov}[\mathrm{sgn}(X),\mathrm{sgn}(Y)]=\frac{2\arcsin(r)}{\pi}\] (17)

This lemma enables us to sample from bivariate Bernoulli distribution with mean \((\frac{1}{2},\frac{1}{2})\) and any valid covariance. Specifically, once we solve out the optimal covariance matrix \(X^{*}\), we can sample from multivariate Gaussian distribution with \(\sin(2\pi X^{*})\) acting as covariance matrix, here \(\sin\) function is element-wise. Nevertheless, it must be noted that this transformation isn't omnipotent for \(n>2\) case, namely, \(\arcsin(X)/2\pi\) can't recover all possible covariance matrix of \(n\)-variable multivariate Bernoulli distribution when \(X\succeq 0\), since otherwise the multivariate Bernoulli distribution can be uniquely determined by its mean vector and covariance matrix, which is apparently false.

Moreover, we can't guarantee \(\sin(2\pi X^{*})\) to be positive semi-definite even if \(X^{*}\) is positive semi-definite. Motivated by this, we adopt the Cholesky-based parameterization. We set our decision variable as matrix \(R\), and we guarantee the validity of the covariance matrix of Gaussian distribution first, namely, we set the covariance matrix of Gaussian distribution as \(RR^{T}\) and the covariance matrix of (Bernoulli) treatment vector is

\[X(R)=\frac{\arcsin(RR^{T})}{2\pi}\] (18)

Now we can propose our final formulation

\[\begin{split}\min_{R}& M(R)=B(X(R))^{2}+\bar{V}_{ \omega}(X(R))\\ \text{s.t.}&(RR^{T})_{i,j}\in[-1,1]\quad\forall i \neq j,\,i,j\in[K]\\ &(RR^{T})_{i,i}=1\quad\quad\quad\quad\forall i\in[K]\end{split}\] (19)

This optimization problem is no longer convex with the introduction of \(\arcsin\) function, for which we design a projected gradient descent algorithm. Since it's a non-convex optimization problem, adaptive optimizer, such as Adam [20] can be applied to improve the performance of the gradient descent step.

We would like to draw attention to the fact that row normalization can be viewed as a projection operator to the domain in the aforementioned formulation, with the Frobenius norm of the matrix. This fact can be verified through the Cauchy-Schwarz inequality. Additionally, we actually optimize a total of \(K\times K\) parameters using gradient information, which is both efficient and easy to implement.

After the optimization problem is solved, we can sample from it directly through a reparameterization of the multivariate Gaussian distribution,

\[\bm{t}=\frac{\bm{1}+\mathrm{sgn}(R^{*}\mathcal{N}(\bm{0},I_{K}))}{2}\] (20)

## 4 Simulation Study

### Basic Setting

In this part, we carry out simulation based on a linear/multiplicative potential outcome model on a social network FB-Stanford3[32].3. We consider following linear potential outcome model,

Footnote 3: Network topology data can be found in https://networkrepository.com/socfb-Stanford3.php

\[Y_{i}(\bm{z})=\alpha+\beta\cdot z_{i}+c\cdot\frac{d_{i}}{\bar{d}}+\sigma\cdot \epsilon_{i}+\gamma\frac{\sum_{j\in N_{i}}z_{j}}{d_{i}}\] (21)

and multiplicative model, which is a simplified version of that in [36], with removing a covariate.

\[Y_{i}(\bm{z})=(\alpha+\sigma\cdot\epsilon_{i})\cdot\frac{d_{i}}{\bar{d}}\cdot (1+\beta z_{i}+\gamma\frac{\sum_{j\in N_{i}}z_{j}}{d_{i}})\] (22)

We choose to fix all parameters except for interference density, \(\gamma\). Namely, we set \((\alpha,\beta,c,\sigma)=(1,1,0.5,0.1)\) for both models, and set \(\gamma\in\{0.5,1,2\}\) to construct three regimes. \(\epsilon_{i}\sim\mathcal{N}(0,1)\).

It should be noted that these two models correspond to different model misspecification forms, aiming to demonstrate the robustness of our method.

Besides our optimized covariance design (OCD), we implement following randomization schemes. We first consider two baselines, independent Bernoulli randomization (Ber) and complete randomization (CR), both of which are cluster-level. We also implement two adaptive schemes, rerandomized-adaptive randomization (ReAR) and pairwise-sequential randomization (PSR) [25; 31], which balance heuristic covariates adaptively and act as competitive baselines, since the average degree is considered as a covariate in these two methods and exactly appear in both two of our models, explicitly. Then we implement the independent block randomization (IBR) [5], which represents methods that optimize parameterized randomization schemes. Nevertheless, we argue that an appropriate objective for optimization is crucial, and we propose a randomization scheme based on IBR with optimizing variance in equation (12) as objective. We nominate it as IBR-p since the heuristic solution of this scheme in our optimization problem is pairing cluster with cluster size, namely, each block is of size 2. Notice that all randomization schemes satisfy equation (4).

For every randomization scheme we consider, we perform Monte Carlo simulation that repeats randomization schemes 10,000 times and calculates the sample mean and variance of estimators.

Moreover, the oracular GATE are utilized to calculate bias.

\[\begin{split}\tau_{1}&=(\alpha+\beta+c\cdot\frac{\bar{d}}{ \bar{d}}+\gamma)-(\alpha+c\cdot\frac{\bar{d}}{\bar{d}})=\beta+\gamma\\ \tau_{2}&=\alpha(1+\beta+\gamma)-\alpha=\alpha(\beta+\gamma) \end{split}\] (23)

At last, we set the clusters as given by Louvain algorithm with fixed random seed and resolution parameter as 10, which gives \(K=192\) clusters. We also consider other two resolution levels, namely, \(2\) and \(5\), corresponding to \(K=30,95\), respectively. The outcome under these two levels is presented in the appendix.

### Analysis of Results

From table 1 to table 2, we show the bias, standard deviation, and MSE of standard HT estimator. We also present the result under different estimator (such as difference-in-means estimator), different clustering resolution and different dataset (network topology) in the appendix due to limited space. Overall, it meets our expectation that the proposed method OCD demonstrates greater advantages over other methods when interference intensity, namely, \(\gamma\), increases. We emphasize the outstanding performance of our method in the multiplicative model and give the credit to handling the bias-variance tradeoff better. We also discover that our approach OCD isn't sensitive to the resolution level of upstream clustering, which isn't often the case for other methods, according to the thorough simulation results presented in the appendix. In reality, when there are certain densely connected cluster pairings that are assumed to have merged but have not, the clustering outcome is poor and resolution level may be inappropriate. Such pair would typically receive a high positive covariance from the optimization process in OCD, which could be regarded as adaptively fixing the fault.

In addition, we highlight that the bias usually dominates in MSE in many settings, and it almost scales linearly with interference intensity, which is exactly as shown in the expression of bias. According to our theoretical derivation, we attribute the dominance of bias to the density of social network. There is a trend that all other methods perform similarly with two baselines, since bias is dominating in a denser network, while this component is usually ignored in the experiment design, on account of the introduction of HT estimator with exposure indicator. Moreover, although we fix \(\omega=1\) in presented result, we actually observe that our method is very robust to the choice of \(\omega\) in related experiment.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & & 0.5 & & & 1.0 & & & 2.0 & \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & & \\ \hline
**Ber** & -0.293 & 0.521 & 0.358 & -0.588 & 0.584 & 0.688 & -1.178 & 0.709 & 1.893 \\
**CR** & -0.292 & 0.409 & 0.253 & -0.586 & 0.459 & 0.554 & -1.177 & 0.562 & 1.702 \\
**ReAR** & -0.393 & 0.227 & 0.206 & -0.700 & 0.251 & 0.554 & -1.317 & 0.303 & 1.829 \\
**PSR** & -0.295 & 0.235 & 0.143 & -0.587 & 0.264 & 0.415 & -1.179 & 0.323 & 1.496 \\
**IBR** & -0.298 & 0.273 & 0.164 & -0.593 & 0.308 & 0.447 & -1.181 & 0.380 & 1.541 \\
**IBR-p** & -0.294 & 0.232 & **0.141** & -0.596 & 0.261 & 0.423 & -1.185 & 0.318 & 1.507 \\
**OCD** & -0.198 & 0.411 & 0.209 & -0.388 & 0.469 & **0.371** & -0.764 & 0.585 & **0.926** \\ \hline \hline \end{tabular}
\end{table}
Table 1: The average bias, standard deviation and MSE of HT estimator under linear model

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**gamma** & & 0.5 & & & 1.0 & & & 2.0 & \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.365 & 0.348 & 0.255 & -0.736 & 0.394 & 0.698 & -1.475 & 0.493 & 2.421 \\
**CR** & -0.368 & 0.235 & 0.191 & -0.744 & 0.274 & 0.629 & -1.477 & 0.336 & 2.297 \\
**ReAR** & -0.402 & 0.178 & 0.194 & -0.809 & 0.174 & 0.685 & -1.548 & 0.226 & 2.450 \\
**PSR** & -0.366 & 0.134 & 0.152 & -0.738 & 0.153 & 0.569 & -1.479 & 0.192 & 2.227 \\
**IBR** & -0.369 & 0.155 & 0.161 & -0.737 & 0.178 & 0.576 & -1.484 & 0.221 & 2.252 \\
**IBR-p** & -0.368 & 0.163 & 0.163 & -0.739 & 0.185 & 0.581 & -1.482 & 0.232 & 2.252 \\
**OCD** & -0.258 & 0.040 & **0.069** & -0.517 & 0.050 & **0.271** & -1.034 & 0.054 & **1.073** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The average bias, standard deviation and MSE of HT estimator under multiplicative model Finally, we point out that the distribution of proportion of treated units of optimized randomization scheme is usually bimodal (e.g. peak at 45%, 55%) in our randomization scheme, with the mean still 50%. This bimodal pattern is the outcome of introducing non-zero covariance on almost all off-diagonal positions of covariance matrix. This is different from other methods we considered here, for which the proportion is usually strictly 50% or unimodal and concentrates at its mean, 50%.

## 5 Discussion

In this article, we derive the bias and variance of the standard HT estimator under a parametric potential outcome model and construct an optimizable objective based on the derived MSE. We then propose an appropriate formulation to transform the experiment design into an optimization problem with the covariance matrix of treatment vector acting as decision variables. Besides, we propose an algorithm based on projected gradient descent to provide optimized covariance design that supports legitimate sampling. Finally, we implement a series of methods and compare their performance in diverse settings, where the advantage of our methods is demonstrated.

We stress the significance of bias and propose our method to make attempts for balancing bias and variance better. Actually, almost all potential outcome models used in the simulation part of related works admit neighborhood interference assumption, which makes the estimator using exposure indicator unbiased or nearly unbiased. However, we argue that this assumption is reasonable but also restricts our scope. There have recently been some works considering interference beyond this assumption, where bias is also demonstrated to be important.

Although there has been a lot of variance bound in this area, few of them can guide experiment design directly, and we have actually made many attempts on finding appropriate variance bound. For example, the gradient of fourth-order term \(\mathbb{E}\big{[}\big{(}t^{T}Ct\big{)}^{2}\big{]}\) can be estimated directly through REINFORCE estimator [38] without need of upper bound. Nevertheless, the optimized covariance matrix tends to give a proportion of densely linked cluster pairs high correlation (near to 1), causing the covariance matrix to become nearly singular. This, in turn, leads to catastrophic gradient variance that can't be reduced by conventional technique such as [21]. In addition, weaker assumption on comparability between direct effect and interference is feasible. We can only demand there exists \(\omega^{\prime}>0\) such that \(\|\bm{h}\|_{2}\leq\omega^{\prime}\gamma\|\bm{d}\|_{2}\). This weaker assumption corresponds to spectral norm bound on the second order term \(\bm{h}^{T}\mathbb{E}[tt^{T}]\bm{h}\), which is a looser bound and finally produces a empirically worse solution. This bound is also utilized in [17] and is expressed as the worst-case variance, which is concerned with maximizing a convex objective over bounded potential outcomes. The optimal value is taken when potential outcome is taken as the endpoint value of its domain, which is too conservative to have good empirical performance. The simulation result of IBR can support this claim, since similar worst-case variance is utilized in IBR.

Additionally, our work can be improved and extended in several aspects. Firstly, the optimization objective can be improved with a tighter but still optimizable upper bound on variance. Moreover, the situation of non-balanced treatment assignment, i.e., \(\mathbb{E}[z_{i}]\neq\frac{1}{2}\), is worthy of exploration. Besides, the derivation for more complex estimator is kept explorable, e.g., the difference-in-means estimator, and HT estimator with exposure indicator. Our derivation is performed on the standard HT estimator, and structural assumption is introduced when the derivation encounters essential difficulties. In addition, the basic setting beyond neighborhood interference assumption also warrant further extension.

## Acknowledgments and Disclosure of Funding

The research was supported by the National Natural Science Foundation of China (No.72171131, 72133002); the Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grants 2020AAA0108400 and 2020AAA0108403. We would like to thank Ruizhong Qiu for beneficial discussion, and anonymous reviewers for the helpful feedback.

## References

* [1] Peter M Aronow and Cyrus Samii. Estimating average causal effects under general interference, with application to a social network experiment. _Annals of Applied Statistics_, 11(4):1912-1947, 2017.

* [2] Guillaume W Basse and Edoardo M Airoldi. Model-assisted design of experiments in the presence of network-correlated outcomes. _Biometrika_, 105(4):849-858, 2018.
* [3] Rohit Bhattacharya, Daniel Malinsky, and Ilya Shpitser. Causal inference under interference and network uncertainty. In _Uncertainty in Artificial Intelligence_, pages 1028-1038. PMLR, 2020.
* [4] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. _Journal of statistical mechanics: theory and experiment_, 2008(10):P10008, 2008.
* [5] Ozan Candogan, Chen Chen, and Rad Niazadeh. Correlated cluster-based randomized experiments: Robust variance minimization. _Management Science_, 2023.
* [6] Alex Chin. Regression adjustments for estimating the global treatment effect in experiments with interference. _Journal of Causal Inference_, 7(2), 2019.
* [7] Andrew David Cliff and J Keith Ord. _Spatial processes: models & applications_. Taylor & Francis, 1981.
* [8] Mayleen Cortez, Matthew Eichhorn, and Christina Yu. Staggered rollout designs enable causal inference under interference without network knowledge. In _Advances in Neural Information Processing Systems_, 2022.
* [9] Mayleen Cortez, Matthew Eichhorn, and Christina Lee Yu. Exploiting neighborhood interference with low order interactions under unit randomized design. _arXiv preprint arXiv:2208.05553_, 2022.
* [10] Dean Eckles, Brian Karrer, and Johan Ugander. Design and analysis of experiments in networks: Reducing bias from interference. _Journal of Causal Inference_, 5(1):20150021, 2016.
* [11] Laura Forastiere, Edoardo M Airoldi, and Fabrizia Mealli. Identification and estimation of treatment and interference effects in observational studies on networks. _Journal of the American Statistical Association_, 116(534):901-918, 2021.
* [12] Laura Forastiere, Fabrizia Mealli, Albert Wu, and Edoardo M Airoldi. Estimating causal effects under network interference with bayesian generalized propensity scores. _Journal of Machine Learning Research_, 23(289):1-61, 2022.
* [13] Peter W Glynn, Ramesh Johari, and Mohammad Rasouli. Adaptive experimental design with temporal interference: A maximum likelihood approach. _Advances in Neural Information Processing Systems_, 33:15054-15064, 2020.
* [14] Huan Gui, Ya Xu, Anmol Bhasin, and Jiawei Han. Network a/b testing: From sampling to estimation. In _24th International Conference on World Wide Web, WWW 2015_, pages 399-409. Association for Computing Machinery, Inc, 2015.
* [15] Kevin Han and Johan Ugander. Model-based regression adjustment with model-free covariates for network interference. _arXiv preprint arXiv:2302.04997_, 2023.
* [16] Christopher Harshaw, Fredrik Savje, David Eisenstat, Vahab Mirrokni, and Jean Pouget-Abadie. Design and analysis of bipartite experiments under a linear exposure-response model. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 606-606, 2022.
* [17] Christopher Harshaw, Fredrik Savje, Daniel Spielman, and Peng Zhang. Balancing covariates in randomized experiments with the gram-schmidt walk design. _arXiv preprint arXiv:1911.03071_, 2019.
* [18] Christopher Harshaw, Fredrik Savje, and Yitan Wang. A design-based riesz representation framework for randomized experiments. _arXiv preprint arXiv:2210.08698_, 2022.
* [19] Michael G Hudgens and M Elizabeth Halloran. Toward causal inference with interference. _Journal of the American Statistical Association_, 103(482):832-842, 2008.

* [20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [21] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get a baseline for free! In _Deep Reinforcement Learning Meets Structured Prediction, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019_. OpenReview.net, 2019.
* [22] Michael P Leung. Causal inference under approximate neighborhood interference. _Econometrica_, 90(1):267-293, 2022.
* [23] Michael P Leung. Rate-optimal cluster-randomized designs for spatial interference. _The Annals of Statistics_, 50(5):3064-3087, 2022.
* [24] Hannah Li, Geng Zhao, Ramesh Johari, and Gabriel Y Weintraub. Interference, bias, and variance in two-sided marketplace experimentation: Guidance for platforms. In _Proceedings of the ACM Web Conference 2022_, pages 182-192, 2022.
* [25] Yang Liu, Yifan Zhou, Ping Li, and Feifang Hu. Adaptive a/b test on networks with cluster structures. In _International Conference on Artificial Intelligence and Statistics_, pages 10836-10851. PMLR, 2022.
* [26] Joel A Middleton and Peter M Aronow. Unbiased estimation of the average treatment effect in cluster-randomized experiments. _Statistics, Politics and Policy_, 6(1-2):39-75, 2015.
* [27] Preetam Nandy, Kinjal Basu, Shaunak Chatterjee, and Ye Tu. A/b testing in dense large-scale networks: design and inference. _Advances in Neural Information Processing Systems_, 33:2870-2880, 2020.
* [28] Frank Oertel. Grothendieck's inequality and completely correlation preserving functions-a summary of recent results and an indication of related research problems. _arXiv preprint arXiv:2010.00746_, 2020.
* [29] Ben M Parker, Steven G Gilmour, and John Schormans. Optimal design of experiments on connected units with application to social networks. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, pages 455-480, 2017.
* [30] Jean Pouget-Abadie, Kevin Aydin, Warren Schudy, Kay Brodersen, and Vahab Mirrokni. Variance reduction in bipartite experiments through correlation clustering. _Advances in Neural Information Processing Systems_, 32, 2019.
* [31] Yichen Qin, Yang Li, Wei Ma, and Feifang Hu. Pairwise sequential randomization and its properties. _arXiv preprint arXiv:1611.02802_, 2016.
* [32] Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics and visualization. In _AAAI_, 2015.
* [33] Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. _Journal of educational Psychology_, 66(5):688, 1974.
* [34] Michael E Sobel. What do randomized studies of housing mobility demonstrate? causal inference in the face of interference. _Journal of the American Statistical Association_, 101(476):1398-1407, 2006.
* [35] Johan Ugander, Brian Karrer, Lars Backstrom, and Jon Kleinberg. Graph cluster randomization: Network exposure to multiple universes. In _Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 329-337, 2013.
* [36] Johan Ugander and Hao Yin. Randomized graph cluster randomization. _Journal of Causal Inference_, 11(1):20220014, 2023.
* [37] Davide Viviano. Experimental design under network interference. _arXiv preprint arXiv:2003.08421_, 2020.
* [38] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Reinforcement learning_, pages 5-32, 1992.

* [39] Christina Lee Yu, Edoardo M Airoldi, Christian Borgs, and Jennifer T Chayes. Estimating the total treatment effect in randomized experiments with unknown network structure. _Proceedings of the National Academy of Sciences_, 119(44):e2208975119, 2022.
* [40] Christina Lee Yu, Edoardo M Airoldi, Christian Borgs, and Jennifer T Chayes. Graph agnostic randomized experimental design. _arXiv preprint arXiv:2205.12803_, 2022.
* [41] Yuan Yuan and Kristen M Altenburger. A two-part machine learning approach to characterizing network interference in a/b testing, 2022.
* [42] Qiong Zhang and Lulu Kang. Locally optimal design for a/b tests in the presence of covariates and network dependence. _Technometrics_, 64(3):358-369, 2022.
* [43] Corwin M Zigler and Georgia Papadogeorgou. Bipartite causal inference with interference. _Statistical science: a review journal of the Institute of Mathematical Statistics_, 36(1):109, 2021.

Proofs

### Proof of Proposition 1

Firstly, we notice that

\[\mathbb{E}[z_{i}]=\frac{1}{2}\quad\operatorname{Var}[z_{i}]=E[z_{i}]-E[z_{i}]^{2} =\frac{1}{4}\] (24)

Then we substitute \(Y_{i}(\bm{z})\) in HT estimator

\[\hat{\tau}=\frac{1}{n}\sum_{i\in[n]}\left(\left(\frac{z_{i}}{\mathbb{E}\left[z_ {i}\right]}-\frac{(1-z_{i})}{\mathbb{E}\left[1-z_{i}\right]}\right)Y_{i}(z)\right)\] (25)

with our potential outcome model and take expectation w.r.t. treatment assignments,

\[\begin{split}\mathbb{E}[\hat{\tau}]&=\frac{2}{n} \left(\sum_{i=1}^{n}\mathbb{E}[(z_{i}-(1-z_{i}))Y_{i}(z)]\right)\\ &=\frac{2}{n}\sum_{i=1}^{n}\left(\alpha_{i}\mathbb{E}[2z_{i}-1]+ \beta_{i}\mathbb{E}[z_{i}^{2}]+\gamma\sum_{j\in N_{i}}\mathbb{E}[(2z_{i}-1)z_ {j}]\right)\\ &=\frac{2}{n}\sum_{i=1}^{n}\left(\beta_{i}\mathbb{E}[z_{i}]+2 \gamma\operatorname{Cov}[z_{i},\sum_{j\in N_{i}}z_{j}]\right)\\ &=\frac{1}{n}\sum_{i=1}^{n}\left(\beta_{i}+4\gamma\operatorname{ Cov}[z_{i},\sum_{j\in N_{i}}z_{j}]\right)\end{split}\] (26)

Moreover, we can calculate the GATE under our potential outcome model

\[\tau=\frac{1}{n}\sum_{i=1}^{n}\left(Y_{i}(\bm{1})-Y_{i}(\bm{0})\right)=\frac{1 }{n}\sum_{1=1}^{n}\left(\beta_{i}+\gamma d_{i}\right)\] (27)

Hence, the bias of HT estimator is

\[\begin{split} E[\hat{\tau}]-\tau&=\frac{1}{n}\sum _{i=1}^{n}\left(4\gamma\operatorname{Cov}[z_{i},\sum_{j\in N_{i}}z_{j}]-\gamma d _{i}\right)\\ &=\frac{\gamma}{n}\sum_{i=1}^{n}\left(\frac{\operatorname{Cov}[z _{i},\sum_{j\in N_{i}}z_{j}]}{\operatorname{Var}[z_{i}]}-d_{i}\right)\end{split}\] (28)

Then we derive the matrix form with cluster-level treatment vector \(\bm{t}\).

\[\begin{split}\mathbb{E}[\hat{\tau}]-\tau&=\frac{ \gamma}{n}\sum_{i=1}^{n}\left(\frac{\operatorname{Cov}[z_{i},\sum_{k\in{\cal N }_{i}}z_{k}]}{\operatorname{Var}[z_{i}]}-d_{i}\right)\\ &=\frac{\gamma}{n}\left(4\sum_{i\neq j\in[K]}\operatorname{Cov}[t _{i},t_{j}]\bm{C}_{ij}-\sum_{i\in[n]}d_{i}\right)\\ &=\frac{\gamma}{n}\left(4\sum_{i,j\in[K]}\operatorname{Cov}[t_{i}, t_{j}]\bm{C}_{ij}-\sum_{i,j\in[K]}\bm{C}_{ij}\right)\\ &=\frac{\gamma}{n}\left(4\operatorname{trace}(\bm{C}\operatorname {Cov}[\bm{t}])-\sum_{i,j\in[K]}\bm{C}_{ij}\right)\end{split}\] (29)

### Proof of Proposition 2

Firstly we rewrite the estimator

\[\hat{\tau} =\frac{2}{n}\sum_{i\in[n]}\left((\beta_{i}-\gamma d_{i})z_{i}+2 \gamma\sum_{j\in N_{i}}z_{i}z_{j}\right)\] (30) \[=\frac{2}{n}\left(\sum_{i\in[n]}(\beta_{i}-\gamma d_{i})z_{i}+2 \gamma\sum_{(j,k)\in\mathcal{E}}z_{j}z_{k}\right)\]

We then expand the variance directly, and get

\[\mathrm{Var}[\hat{\tau}] =\frac{4}{n^{2}}(\sum_{i,j\in[n]}(\beta_{i}-\gamma d_{i})(\beta_{ j}-\gamma d_{j})\operatorname{Cov}[z_{i},z_{j}]\] (31) \[+\sum_{i\in[n]}\sum_{(j,k)\in\mathcal{E}}4\gamma(\beta_{i}-\gamma d _{i})\operatorname{Cov}[z_{i},z_{j}z_{k}]\] \[+\sum_{(i,j)\in\mathcal{E}}\sum_{(k,l)\in\mathcal{E}}4\gamma^{2} \operatorname{Cov}[z_{i}z_{j},z_{k}z_{l}])\]

then we write it as matrix form with cluster-level treatment vector \(\bm{t}\),

\[\hat{\tau}=\frac{2}{n}(\bm{h}^{T}t+2\gamma t^{T}\bm{C}t)\] (32)

and expand the variance as covariance, we get the desired format

\[\mathrm{Var}[\hat{\tau}]= \frac{4}{n^{2}}(\operatorname{trace}(\bm{h}\bm{h}^{T}\operatorname {Cov}[\bm{t}])+4\gamma\operatorname{Cov}[\bm{h}^{T}\bm{t},\bm{t}^{T}\bm{C}\bm{ t}]\] (33) \[+4\gamma^{2}\operatorname{Var}[\bm{t}^{T}\bm{C}\bm{t}])\]

### Proof of Proposition 3

Firstly we consider the estimator of matrix form again

\[\hat{\tau}=\frac{2}{n}(\bm{h}^{T}t+2\gamma t^{T}\bm{C}t)\] (34)

Since our assumption can't guarantee \(\mathbb{E}[\hat{\tau}]>0\) always hold, we drop the square of expectation term, namely

\[\mathrm{Var}[\hat{\tau}]=\mathbb{E}[\hat{\tau}^{2}]-\mathbb{E}[\hat{\tau}]^{2} \leq\mathbb{E}[\hat{\tau}^{2}]\] (35)

Notice that all elements of matrix \(E[tt^{T}]\) is non-negative, we have

\[\bm{h}^{T}\mathbb{E}[tt^{T}]\bm{h}\leq\omega^{2}\bm{d}^{T}\mathbb{E}[tt^{T}] \bm{d}=\omega^{2}\operatorname{trace}(\bm{d}\bm{d}^{T}\mathbb{E}[tt^{T}])\] (36)

Similarly, since all elements of matrix \(\bm{C}\), namely, \(\bm{C}_{\bm{ij}}\) is also non-negative, we have

\[\mathbb{E}[(t^{T}\bm{C}t)^{2}] =\operatorname{trace}(\mathbb{E}[\bm{C}tt^{T}\bm{C}tt^{T}])\] (37) \[\leq\operatorname{trace}(\mathbb{E}[\bm{C}\bm{1}\bm{1}^{T}\bm{C} tt^{T}]\] \[=\operatorname{trace}(\bm{C}\bm{1}\bm{1}^{T}\bm{C}\mathbb{E}[tt^{ T}])\]

In summary, we have

\[\mathrm{Var}[\hat{\tau}] \leq\mathbb{E}[\hat{\tau}^{2}]\] (38) \[\leq\frac{8}{n^{2}}(\bm{h}^{T}\mathbb{E}[tt^{T}]\bm{h}+4\gamma^{2} \mathbb{E}[(t^{T}\bm{C}t)^{2}])\] \[\leq\frac{8\gamma^{2}}{n^{2}}\left(\omega^{2}\operatorname{trace} (\bm{d}\bm{d}^{T}\mathbb{E}[tt^{T}])+4\operatorname{trace}(\bm{C}\bm{1}\bm{1} ^{T}\bm{C}\mathbb{E}[tt^{T}])\right)\]

By definition, we have

\[\bm{C}\bm{1}=\bm{d}\] (39)Thus the upper bound above is actually

\[\mathrm{Var}[\hat{\tau}]\leq\frac{8\gamma^{2}(\omega^{2}+4)}{n^{2}}\left(\mathrm{ trace}(\bm{d}\bm{d}^{T}\mathbb{E}[tt^{T}])\right)\] (40)

At last, plug in following equation.

\[\mathbb{E}[tt^{T}]=\mathrm{Cov}[t]+\frac{1}{4}\bm{1}\bm{1}^{T}\] (41)

### Proof of Lemma 1

Here we provide an intuitive proof. Utilizing Box-Muller transformation or pure algebraic analysis are also feasible.

We consider \(X\) and \(Y\) is generated from multivariate Gaussian distribution,

\[X=\langle x,g\rangle\quad Y=\langle y,g\rangle\] (42)

where \(g\sim\mathcal{N}(0,I_{n})\) and \(x,y\) are two \(n\)-dim real vectors. Then we know that

\[\mathrm{Cov}[X,Y]=\langle x,y\rangle\] (43)

Moreover, we have

\[\mathbb{E}[\mathrm{sgn}(X)]=0\quad\mathbb{E}[\mathrm{sgn}(Y)]=0\] (44)

thus

\[\mathrm{Cov}[\mathrm{sgn}(X),\mathrm{sgn}(Y)]=\mathbb{E}[\mathrm{sgn}(X) \,\mathrm{sgn}(Y)]\] (45)

Then we think geometrically that \(\mathrm{sgn}(\langle x,g\rangle)\,\mathrm{sgn}(\langle y,g\rangle)>0\) holds iff. \(g\) lies above or below both of the hyperplanes that is orthogonal to \(x\) and \(y\) respectively.

Notice that the direction of \(g\) is uniform, it follows that

\[\mathbb{P}(\mathrm{sgn}(\langle x,g\rangle)\,\mathrm{sgn}(\langle y,g\rangle)> 0)=\frac{2}{2\pi}(\pi-\arccos(\langle x,y\rangle)\] (46)

Now we put things together

\[\mathbb{E}[\mathrm{sgn}(X)\,\mathrm{sgn}(Y)] =\mathbb{P}(\mathrm{sgn}(X)\,\mathrm{sgn}(Y)>0)-\mathbb{P}( \mathrm{sgn}(X)\,\mathrm{sgn}(Y)<0)\] (47) \[=2\mathbb{P}(\mathrm{sgn}(X)\,\mathrm{sgn}(Y)>0)-1\] \[=2(\frac{1}{\pi}(\pi-\arccos(\langle x,y\rangle))-1\] \[=1-\frac{2}{\pi}\arccos(\langle x,y\rangle)\] \[=\frac{2}{\pi}\arcsin(\langle x,y\rangle)\]

which gives the desired outcome.

## Appendix B Simulation Details

### Methods

We consider following linear potential outcome model,

\[Y_{i}(\bm{z})=\alpha+\beta\cdot z_{i}+c\cdot\frac{d_{i}}{\bar{d}}+\sigma\cdot \epsilon_{i}+\gamma\frac{\sum_{j\in N_{i}}z_{j}}{d_{i}}\] (48)

and multiplicative model, which is a simplified version of that in [36], with removing a covariate.

\[Y_{i}(\bm{z})=(\alpha+\sigma\cdot\epsilon_{i})\cdot\frac{d_{i}}{\bar{d}}\cdot (1+\beta z_{i}+\gamma\frac{\sum_{j\in N_{i}}z_{j}}{d_{i}})\] (49)We choose to fix all parameters except for interference density, \(\gamma\). Namely, we set \((\alpha,\beta,c,\sigma)=(1,1,0.5,0.1)\) for both models, and set \(\gamma\in\{0.5,1,2\}\) to construct three regimes. \(\epsilon_{i}\sim\mathcal{N}(0,1)\).

We set the clusters as given by Louvain algorithm with fixed random seed and resolution parameter as \(2,5,10\).

We consider social network FB-Stanford3[32],4 and FB-Cornell5[32].5. These two social networks provide the network topology, and we generate potential outcome for each unit with mentioned potential outcome models.

Footnote 4: Network topology data can be found in https://networkrepository.com/socfb-Stanford3.php

Footnote 5: https://networkrepository.com/socfb-Cornell5.php

Besides our optimized covariance design (OCD), we implement following randomization schemes. We first consider two baselines, independent Bernoulli randomization (Ber) and complete randomization (CR), both of which are cluster-level. We also implement two adaptive schemes, rerandomized-adaptive randomization (ReAR) and pairwise-sequential randomization (PSR) [25, 31], which balance heuristic covariates adaptively and act as competitive baselines, since the average degree is considered as a covariate in these two methods and exactly appear in both two of our models, explicitly. Then we implement the independent block randomization (IBR) [5] and heuristic version IBR-p that creates blocks with size 2.

In the methods mentioned above, ReAR is the only one concerned with hyperparameters setting. We set \((q,B,\alpha)=(0.85,400,0.1)\), which corresponds to the recommendation in the original paper.

We estimate GATE with standard HT estimator and difference-in-means (DIM) estimator, where the latter refers to

\[\hat{\tau}_{DIM}=\sum_{i\in[n]}\left(\left(\frac{z_{i}}{\sum_{j\in[n]}z_{j}}- \frac{(1-z_{i})}{\sum_{j\in[n]}(1-z_{j})}\right)Y_{i}(z)\right)\] (50)

To summarize, we provide the bias, standard deviation and MSE of two estimators under two potential outcome models, three \(\gamma\) levels and two datasets. All of these three metrics are calculated by repetition of Monte Carlo simulation of randomization, 10,000 times. In the main paper we've presented the results of HT estimator on FB-Stanford3, and we'll present the detailed results in this section.

### Discussion on Estimators

Here we also provide discussion on another popular estimator that's considered in existing literature, which is the HT estimator with exposure indicator. We consider the exposure condition that's fully treated or fully controlled in 1-hop neighborhood here, and the corresponding indicator is defined as \(\delta_{i}(z_{0})=\mathbb{I}\{\sum_{j\in N_{i}}z_{j}=d_{i}z_{0}\}\). The estimator is

\[\hat{\tau}^{\prime}=\frac{1}{n}\sum_{i\in[n]}\left((\frac{\delta_{i}(1)}{ \mathbb{E}[\delta_{i}(1)]}-\frac{\delta_{i}(0)}{\mathbb{E}[\delta_{i}(0)]})Y_ {i}(\bm{z})\right)\] (51)

We don't consider this estimator not only because of it's high variance resulted from low effective sample size, and but also its high calculation cost. For calculating it in our repeated simulations, we need to estimate \(\mathbb{E}[\delta_{i}(1)]\), the probability of fully treated, for each unit \(i\). We denote the number of clusters a unit \(i\) connects to by \(c_{i}\). For a unit \(i\) in the exterior of its cluster, namely, \(c_{i}>1\), such a quantity can very high in a social network, which is also decided by the resolution of clustering, we present an instance in figure 1.

Unfortunately, we must estimate such generalized propensity score [10] by Monte Carlo simulation for most of randomization schemes, even if it's just a little bit more complex than independent Bernoulli randomization, where such quantity can be calculated directly, \((1/2)^{c_{i}}\). For every simulation, we should visit every node and query the treatment assignment of its 1-hop neighborhood, whose time complexity is \(O(|E|)\). Roughly, we need \(2^{c_{i}}\) repetitions of randomization to achieve effective estimation on \(\delta_{i}(1),\delta_{i}(0)\), which is too time-consuming to be acceptable for many units.

We stress the computational issue here since the resulted variance can be reduced by self-normalization, which corresponds to the Hajek estimator.

\[\hat{\tau}^{\prime}=\sum_{i\in[n]}\left((\frac{\frac{\delta_{i}(1)}{\mathbb{E}[ \delta_{i}(1)]}}{\sum_{j\in[n]}\frac{\delta_{j}(1)}{\mathbb{E}[\delta_{j}(1)]}} )-(\frac{\frac{\delta_{i}(0)}{\mathbb{E}[\delta_{i}(0)]}}{\sum_{j\in[n]}\frac{ \delta_{j}(0)}{\mathbb{E}[\delta_{j}(0)]}})\right)Y_{i}(\bm{z})\] (52)

However, we argue that the computational cost can't be bypassed without further modification and restricts the practicality of such HT estimator with exposure indicator. [25] proposes the so-called cluster-adjusted estimator that assign the units on the exterior of same cluster the same modified propensity score, which reduces the computation complexity at the cost of unpredictable distortion of estimator, which can be viewed as heuristics.

Therefore, we choose to consider the standard HT estimator and DIM estimator in our simulation.

### Detailed Simulation Results

We present the results according to following sequence: HT-DIM estimator, linear-multiplicative model, 2-5-10 clustering resolution. In every table, we report the average bias, standard deviation (SD) and MSE of randomization schemes with three \(\gamma\) levels. There are 24 tables in total, which are organized to demonstrate the advantage and robustness of our proposed method, optimized covariance design (OCD).

Figure 1: Distribution of \(c_{i}\) with 95 clusters on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & & 0.5 & & & 1.0 & & & 2.0 & \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.290 & 0.811 & 0.742 & -0.578 & 0.923 & 1.187 & -1.176 & 1.144 & 2.694 \\
**CR** & -0.301 & 0.519 & 0.361 & -0.605 & 0.582 & 0.705 & -1.213 & 0.739 & 2.019 \\
**ReAR** & -0.327 & 0.257 & 0.173 & -0.547 & 0.259 & 0.367 & -1.236 & 0.374 & 1.668 \\
**PSR** & -0.303 & 0.322 & 0.196 & -0.617 & 0.368 & 0.517 & -1.230 & 0.461 & 1.727 \\
**IBR** & -0.307 & 0.341 & 0.211 & -0.611 & 0.384 & 0.521 & -1.224 & 0.486 & 1.736 \\
**IBR-p** & -0.307 & 0.294 & 0.181 & -0.614 & 0.342 & 0.494 & -1.218 & 0.427 & 1.666 \\
**OCD** & -0.255 & 0.078 & 0.071 & -0.510 & 0.089 & 0.268 & -1.018 & 0.116 & 1.052 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Simulation results of **HT** estimator under **multiplicative** model with resolution **2** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & & 0.5 & & & 1.0 & & & 2.0 & \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & \\ \hline
**Ber** & -0.228 & 1.028 & 1.110 & -0.489 & 1.148 & 1.558 & -0.966 & 1.370 & 2.810 \\
**CR** & -0.252 & 0.635 & 0.467 & -0.494 & 0.697 & 0.730 & -0.977 & 0.836 & 1.654 \\
**ReAR** & -0.236 & 0.190 & 0.092 & -0.486 & 0.173 & 0.267 & -0.940 & 0.173 & 0.914 \\
**PSR** & -0.241 & 0.233 & 0.113 & -0.481 & 0.253 & 0.296 & -0.972 & 0.297 & 1.033 \\
**IBR** & -0.247 & 0.226 & 0.112 & -0.494 & 0.252 & 0.308 & -0.987 & 0.302 & 1.067 \\
**IBR-p** & -0.246 & 0.154 & 0.085 & -0.489 & 0.168 & 0.268 & -0.981 & 0.203 & 1.006 \\
**OCD** & -0.189 & 0.219 & 0.084 & -0.385 & 0.252 & 0.212 & -0.775 & 0.312 & 0.699 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Simulation results of **HT** estimator under **linear** model with resolution **2** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & & 0.5 & & & 1.0 & & & 2.0 & \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.228 & 1.028 & 1.110 & -0.489 & 1.148 & 1.558 & -0.966 & 1.370 & 2.810 \\
**CR** & -0.252 & 0.635 & 0.467 & -0.494 & 0.697 & 0.730 & -0.977 & 0.836 & 1.654 \\
**ReAR** & -0.236 & 0.190 & 0.092 & -0.486 & 0.173 & 0.267 & -0.940 & 0.173 & 0.914 \\
**PSR** & -0.241 & 0.233 & 0.113 & -0.481 & 0.253 & 0.296 & -0.972 & 0.297 & 1.033 \\
**IBR** & -0.247 & 0.226 & 0.112 & -0.494 & 0.252 & 0.308 & -0.987 & 0.302 & 1.067 \\
**IBR-p** & -0.246 & 0.154 & 0.085 & -0.489 & 0.168 & 0.268 & -0.981 & 0.203 & 1.006 \\
**OCD** & -0.189 & 0.219 & 0.084 & -0.385 & 0.252 & 0.212 & -0.775 & 0.312 & 0.699 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Simulation results of **DIM** estimator under **multiplicative** model with resolution **2** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & & \\ \hline
**Ber** & -0.277 & 0.096 & 0.086 & -0.558 & 0.097 & 0.322 & -1.116 & 0.101 & 1.257 \\
**CR** & -0.279 & 0.095 & 0.087 & -0.556 & 0.095 & 0.319 & -1.117 & 0.097 & 1.259 \\
**ReAR** & -0.319 & 0.030 & 0.103 & -0.575 & 0.048 & 0.334 & -1.143 & 0.045 & 1.311 \\
**PSR** & -0.275 & 0.055 & 0.079 & -0.554 & 0.056 & 0.311 & -1.110 & 0.056 & 1.236 \\
**IBR** & -0.276 & 0.088 & 0.085 & -0.555 & 0.089 & 0.316 & -1.110 & 0.090 & 1.241 \\
**IBR-p** & -0.209 & 0.074 & 0.049 & -0.490 & 0.074 & 0.246 & -1.049 & 0.074 & 1.106 \\
**OCD** & -0.194 & 0.102 & 0.048 & -0.390 & 0.102 & 0.163 & -0.782 & 0.104 & 0.623 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Simulation results of **DIM** estimator under **linear** model with resolution **5** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.338 & 0.333 & 0.226 & -0.687 & 0.387 & 0.622 & -1.382 & 0.480 & 2.142 \\
**CR** & -0.340 & 0.333 & 0.226 & -0.680 & 0.387 & 0.613 & -1.382 & 0.477 & 2.138 \\
**ReAR** & -0.417 & 0.171 & 0.204 & -0.860 & 0.163 & 0.767 & -1.485 & 0.228 & 2.260 \\
**PSR** & -0.336 & 0.192 & 0.150 & -0.680 & 0.219 & 0.511 & -1.375 & 0.277 & 1.968 \\
**IBR** & -0.344 & 0.310 & 0.214 & -0.680 & 0.351 & 0.586 & -1.379 & 0.439 & 2.097 \\
**IBR-p** & -0.098 & 0.272 & 0.084 & -0.407 & 0.312 & 0.263 & -1.025 & 0.392 & 1.205 \\
**OCD** & -0.241 & 0.357 & 0.185 & -0.497 & 0.407 & 0.413 & -0.994 & 0.511 & 1.249 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Simulation results of **HT** estimator under **multiplicative** model with resolution **5** on FB-Stanford3 FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.340 & 0.463 & 0.330 & -0.683 & 0.523 & 0.740 & -1.366 & 0.656 & 2.297 \\
**CR** & -0.383 & 0.289 & 0.230 & -0.733 & 0.328 & 0.645 & -1.430 & 0.412 & 2.215 \\
**ReAR** & -0.289 & 0.138 & 0.102 & -0.685 & 0.121 & 0.484 & -1.288 & 0.230 & 1.712 \\
**PSR** & -0.335 & 0.128 & 0.129 & -0.684 & 0.146 & 0.490 & -1.373 & 0.182 & 1.919 \\
**IBR** & -0.346 & 0.180 & 0.153 & -0.694 & 0.207 & 0.525 & -1.387 & 0.259 & 1.992 \\
**IBR-p** & -0.488 & 0.190 & 0.275 & -0.852 & 0.217 & 0.774 & -1.581 & 0.271 & 2.574 \\
**OCD** & -0.255 & 0.049 & 0.068 & -0.508 & 0.057 & 0.262 & -1.020 & 0.069 & 1.045 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Simulation results of **HT** estimator under **linear** model with resolution **5** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.298 & 0.076 & 0.095 & -0.595 & 0.077 & 0.361 & -1.191 & 0.080 & 1.426 \\
**CR** & -0.297 & 0.076 & 0.095 & -0.595 & 0.076 & 0.360 & -1.191 & 0.078 & 1.427 \\
**ReAR** & -0.296 & 0.022 & 0.088 & -0.589 & 0.019 & 0.348 & -1.191 & 0.029 & 1.422 \\
**PSR** & -0.297 & 0.035 & 0.090 & -0.596 & 0.036 & 0.357 & -1.193 & 0.039 & 1.425 \\
**IBR** & -0.296 & 0.054 & 0.091 & -0.593 & 0.055 & 0.356 & -1.188 & 0.058 & 1.415 \\
**IBR-p** & -0.297 & 0.049 & 0.091 & -0.593 & 0.049 & 0.354 & -1.186 & 0.052 & 1.411 \\
**OCD** & -0.190 & 0.115 & 0.050 & -0.384 & 0.116 & 0.161 & -0.772 & 0.118 & 0.611 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Simulation results of **DIM** estimator under **linear** model with resolution **10** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.293 & 0.521 & 0.358 & -0.588 & 0.584 & 0.688 & -1.178 & 0.709 & 1.893 \\
**CR** & -0.292 & 0.409 & 0.253 & -0.586 & 0.459 & 0.554 & -1.177 & 0.562 & 1.702 \\
**ReAR** & -0.393 & 0.227 & 0.206 & -0.700 & 0.251 & 0.554 & -1.317 & 0.303 & 1.829 \\
**PSR** & -0.295 & 0.235 & 0.143 & -0.587 & 0.264 & 0.415 & -1.179 & 0.323 & 1.496 \\
**IBR** & -0.298 & 0.273 & 0.164 & -0.593 & 0.308 & 0.447 & -1.181 & 0.380 & 1.541 \\
**IBR-p** & -0.294 & 0.232 & 0.141 & -0.596 & 0.261 & 0.423 & -1.185 & 0.318 & 1.507 \\
**OCD** & -0.198 & 0.411 & 0.209 & -0.388 & 0.469 & 0.371 & -0.764 & 0.585 & 0.926 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Simulation results of **HT** estimator under **linear** model with resolution **10** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.369 & 0.264 & 0.206 & -0.731 & 0.301 & 0.626 & -1.475 & 0.375 & 2.318 \\
**CR** & -0.363 & 0.265 & 0.202 & -0.737 & 0.303 & 0.636 & -1.470 & 0.380 & 2.307 \\
**ReAR** & -0.351 & 0.061 & 0.127 & -0.715 & 0.095 & 0.520 & -1.480 & 0.121 & 2.207 \\
**PSR** & -0.369 & 0.122 & 0.151 & -0.738 & 0.141 & 0.565 & -1.481 & 0.176 & 2.227 \\
**IBR** & -0.367 & 0.190 & 0.171 & -0.737 & 0.218 & 0.592 & -1.476 & 0.271 & 2.254 \\
**IBR-p** & -0.364 & 0.168 & 0.161 & -0.742 & 0.192 & 0.588 & -1.480 & 0.244 & 2.253 \\
**OCD** & -0.247 & 0.402 & 0.223 & -0.501 & 0.459 & 0.463 & -1.013 & 0.577 & 1.360 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Simulation results of **DIM** estimator under **multiplicative** model with resolution **10** on FB-Stanford3

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.228 & 1.087 & 1.234 & -0.456 & 1.240 & 1.746 & -0.913 & 1.469 & 2.993 \\
**CR** & -0.228 & 0.913 & 0.885 & -0.440 & 1.018 & 1.230 & -0.914 & 1.231 & 2.350 \\
**ReAR** & -0.209 & 0.110 & 0.056 & -0.469 & 0.128 & 0.237 & -0.965 & 0.135 & 0.950 \\
**PSR** & -0.235 & 0.146 & 0.077 & -0.473 & 0.164 & 0.251 & -0.940 & 0.202 & 0.926 \\
**IBR** & -0.232 & 0.530 & 0.335 & -0.465 & 0.595 & 0.571 & -0.942 & 0.720 & 1.407 \\
**IBR-p** & -0.233 & 0.180 & 0.087 & -0.472 & 0.209 & 0.268 & -0.939 & 0.263 & 0.951 \\
**OCD** & -0.184 & 0.213 & 0.079 & -0.376 & 0.241 & 0.200 & -0.752 & 0.302 & 0.657 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Simulation results of **HT** estimator under **linear** model with resolution **2** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.260 & 0.318 & 0.169 & -0.546 & 0.363 & 0.430 & -1.075 & 0.461 & 1.369 \\
**CR** & -0.261 & 0.317 & 0.169 & -0.533 & 0.362 & 0.416 & -1.080 & 0.457 & 1.375 \\
**ReAR** & -0.252 & 0.064 & 0.068 & -0.515 & 0.090 & 0.273 & -1.073 & 0.135 & 1.172 \\
**PSR** & -0.268 & 0.190 & 0.108 & -0.536 & 0.223 & 0.338 & -1.069 & 0.276 & 1.221 \\
**IBR** & -0.268 & 0.319 & 0.174 & -0.535 & 0.368 & 0.422 & -1.074 & 0.453 & 1.359 \\
**IBR-p** & -0.258 & 0.362 & 0.198 & -0.526 & 0.416 & 0.450 & -1.050 & 0.514 & 1.367 \\
**OCD** & -0.215 & 0.201 & 0.087 & -0.428 & 0.228 & 0.235 & -0.866 & 0.291 & 0.835 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Simulation results of **DIM** estimator under **multiplicative** model with resolution **2** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & \\ \hline
**Ber** & -0.240 & 0.092 & 0.066 & -0.480 & 0.093 & 0.239 & -0.960 & 0.103 & 0.933 \\
**CR** & -0.238 & 0.090 & 0.065 & -0.480 & 0.092 & 0.239 & -0.958 & 0.100 & 0.929 \\
**ReAR** & -0.239 & 0.019 & 0.058 & -0.469 & 0.026 & 0.221 & -0.925 & 0.032 & 0.858 \\
**PSR** & -0.235 & 0.056 & 0.059 & -0.470 & 0.056 & 0.225 & -0.944 & 0.058 & 0.894 \\
**IBR** & -0.235 & 0.091 & 0.064 & -0.477 & 0.093 & 0.237 & -0.950 & 0.096 & 0.913 \\
**IBR-p** & -0.235 & 0.103 & 0.066 & -0.471 & 0.104 & 0.234 & -0.944 & 0.108 & 0.903 \\
**OCD** & -0.188 & 0.059 & 0.039 & -0.374 & 0.060 & 0.144 & -0.751 & 0.070 & 0.569 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Simulation results of **DIM** estimator under **linear** model with resolution **2** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & & \\ \hline
**Ber** & -0.281 & 0.070 & 0.084 & -0.560 & 0.070 & 0.319 & -1.124 & 0.074 & 1.269 \\
**CR** & -0.280 & 0.069 & 0.084 & -0.561 & 0.069 & 0.320 & -1.123 & 0.072 & 1.268 \\
**ReAR** & -0.282 & 0.048 & 0.082 & -0.559 & 0.045 & 0.315 & -1.113 & 0.035 & 1.241 \\
**PSR** & -0.274 & 0.052 & 0.078 & -0.552 & 0.053 & 0.308 & -1.106 & 0.055 & 1.227 \\
**IBR** & -0.280 & 0.069 & 0.083 & -0.560 & 0.070 & 0.319 & -1.120 & 0.073 & 1.261 \\
**IBR-p** & -0.278 & 0.086 & 0.085 & -0.558 & 0.086 & 0.319 & -1.113 & 0.088 & 1.247 \\
**OCD** & -0.196 & 0.120 & 0.053 & -0.393 & 0.120 & 0.169 & -0.784 & 0.125 & 0.631 \\ \hline \hline \end{tabular}
\end{table}
Table 19: Simulation results of **DIM** estimator under **linear** model with resolution **5** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.314 & 0.239 & 0.156 & -0.634 & 0.276 & 0.479 & -1.269 & 0.344 & 1.729 \\
**CR** & -0.313 & 0.238 & 0.154 & -0.630 & 0.273 & 0.472 & -1.273 & 0.344 & 1.741 \\
**ReAR** & -0.330 & 0.157 & 0.134 & -0.623 & 0.164 & 0.416 & -1.256 & 0.181 & 1.613 \\
**PSR** & -0.309 & 0.180 & 0.128 & -0.622 & 0.207 & 0.430 & -1.252 & 0.262 & 1.636 \\
**IBR** & -0.312 & 0.244 & 0.157 & -0.635 & 0.277 & 0.481 & -1.266 & 0.349 & 1.725 \\
**IBR-p** & -0.305 & 0.298 & 0.182 & -0.625 & 0.342 & 0.508 & -1.251 & 0.427 & 1.747 \\
**OCD** & -0.212 & 0.417 & 0.219 & -0.428 & 0.477 & 0.412 & -0.868 & 0.600 & 1.114 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Simulation results of **HT** estimator under **linear** model with resolution **5** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.281 & 0.070 & 0.084 & -0.560 & 0.070 & 0.319 & -1.124 & 0.074 & 1.269 \\
**CR** & -0.280 & 0.069 & 0.084 & -0.561 & 0.069 & 0.320 & -1.123 & 0.072 & 1.268 \\
**ReAR** & -0.282 & 0.048 & 0.082 & -0.559 & 0.045 & 0.315 & -1.113 & 0.035 & 1.241 \\
**PSR** & -0.274 & 0.052 & 0.078 & -0.552 & 0.053 & 0.308 & -1.106 & 0.055 & 1.227 \\
**IBR** & -0.280 & 0.069 & 0.083 & -0.560 & 0.070 & 0.319 & -1.120 & 0.073 & 1.261 \\
**IBR-p** & -0.278 & 0.086 & 0.085 & -0.558 & 0.086 & 0.319 & -1.113 & 0.088 & 1.247 \\
**OCD** & -0.196 & 0.120 & 0.053 & -0.393 & 0.120 & 0.169 & -0.784 & 0.125 & 0.631 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Simulation results of **DIM** estimator under **linear** model with resolution **5** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.355 & 0.325 & 0.232 & -0.698 & 0.370 & 0.625 & -1.396 & 0.469 & 2.171 \\
**CR** & -0.349 & 0.238 & 0.179 & -0.704 & 0.266 & 0.567 & -1.401 & 0.335 & 2.075 \\
**ReAR** & -0.353 & 0.077 & 0.131 & -0.708 & 0.090 & 0.511 & -1.399 & 0.131 & 1.976 \\
**PSR** & -0.353 & 0.101 & 0.135 & -0.707 & 0.116 & 0.513 & -1.408 & 0.147 & 2.005 \\
**IBR** & -0.351 & 0.141 & 0.143 & -0.703 & 0.162 & 0.521 & -1.409 & 0.207 & 2.029 \\
**IBR-p** & -0.349 & 0.107 & 0.134 & -0.703 & 0.122 & 0.509 & -1.404 & 0.154 & 1.997 \\
**OCD** & -0.224 & 0.038 & 0.052 & -0.448 & 0.041 & 0.203 & -0.895 & 0.053 & 0.805 \\ \hline \hline \end{tabular}
\end{table}
Table 26: Simulation results of **HT** estimator under **multiplicative** model with resolution **10** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.311 & 0.388 & 0.247 & -0.621 & 0.437 & 0.576 & -1.237 & 0.517 & 1.799 \\
**CR** & -0.311 & 0.264 & 0.167 & -0.623 & 0.290 & 0.473 & -1.243 & 0.345 & 1.665 \\
**ReAR** & -0.306 & 0.069 & 0.099 & -0.618 & 0.087 & 0.390 & -1.238 & 0.098 & 1.544 \\
**PSR** & -0.310 & 0.067 & 0.101 & -0.622 & 0.076 & 0.394 & -1.245 & 0.094 & 1.560 \\
**IBR** & -0.312 & 0.105 & 0.108 & -0.627 & 0.116 & 0.407 & -1.250 & 0.139 & 1.583 \\
**IBR-p** & -0.311 & 0.065 & 0.102 & -0.623 & 0.073 & 0.395 & -1.246 & 0.090 & 1.562 \\
**OCD** & -0.182 & 0.345 & 0.152 & -0.375 & 0.397 & 0.299 & -0.749 & 0.495 & 0.807 \\ \hline \hline \end{tabular}
\end{table}
Table 24: Simulation results of **HT** estimator under **linear** model with resolution **10** on FB-Cornell5

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**gamma** & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{1.0} & \multicolumn{3}{c}{2.0} \\
**metric** & Bias & SD & MSE & Bias & SD & MSE & Bias & SD & MSE \\
**method** & & & & & & & & & \\ \hline
**Ber** & -0.312 & 0.038 & 0.099 & -0.624 & 0.040 & 0.392 & -1.249 & 0.044 & 1.563 \\
**CR** & -0.312 & 0.039 & 0.099 & -0.624 & 0.040 & 0.392 & -1.249 & 0.043 & 1.563 \\
**ReAR** & -0.308 & 0.031 & 0.096 & -0.625 & 0.024 & 0.392 & -1.253 & 0.029 & 1.572 \\
**PSR** & -0.314 & 0.035 & 0.100 & -0.625 & 0.035 & 0.393 & -1.252 & 0.038 & 1.569 \\
**IBR** & -0.312 & 0.036 & 0.099 & -0.625 & 0.038 & 0.392 & -1.250 & 0.041 & 1.566 \\
**IBR-p** & -0.311 & 0.036 & 0.098 & -0.623 & 0.036 & 0.390 & -1.246 & 0.042 & 1.555 \\
**OCD** & -0.188 & 0.098 & 0.045 & -0.374 & 0.100 & 0.150 & -0.749 & 0.106 & 0.574 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Simulation results of **DIM** estimator under **linear** model with resolution **10** on FB-Cornell5