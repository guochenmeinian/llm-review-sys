# Unconditional stability of a recurrent neural circuit implementing divisive normalization

 Shivang Rawat\({}^{1,2}\) David J. Heeger\({}^{3,4}\) Stefano Martiniani\({}^{1,2,5}\)

\({}^{1}\) Courant Institute of Mathematical Sciences, NYU

\({}^{2}\) Center for Soft Matter Research, Department of Physics, NYU

\({}^{3}\) Department of Psychology, NYU

\({}^{4}\) Center for Neural Science, NYU

\({}^{5}\) Simons Center for Computational Physical Chemistry, Department of Chemistry, NYU

{sr6364, david.heeger, sm7683}@nyu.edu

###### Abstract

Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of "oscillatory recurrent gated neural integrator circuits" (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.

## 1 Introduction

Deep neural networks (DNNs) have found widespread use in modeling tasks from experimental systems neuroscience. The allure of DNN-based models lies in their ease of training and the flexibility they offer in architecting systems with desired properties [1, 2, 3]. In contrast, neurodynamical models like the Wilson-Cowan [4] or the Stabilized Supralinear Network (SSN) [5] are more biologically plausible than DNNs, but these models confront considerable training challenges due to the lack of stability guarantees for high-dimensional problems. Training recurrent neural networks (RNNs), by comparison, is more straightforward thanks to ad hoc regularization techniques like layer normalization, batch normalization, and gradient clipping/scaling, which help stabilize training without imposing strict stability constraints. Conversely, neurodynamical models require enforcing hard stability constraints while maintaining biological plausibility. In lower dimensions, it is relatively straightforward to derive constraints on model parameters that ensure a dynamically stable system [6, 7]. However, for high-dimensional systems, this becomes significantly more challenging, as integrating these hard constraints into the optimization problem is more complex [8, 9]. Stability is generally advantageous in DNNs, as it is linked to improved generalization, mitigation of exploding gradient problems, increased robustness to input noise, and simplified training techniques [10].

The divisive normalization (DN) model was developed to explain the responses of neurons in the primary visual cortex (V1) [11, 12, 13, 14], and has since been applied to diverse cognitive processes and neural systems [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]. Therefore, DN has been proposed as a canonical neural computation [25] that is linked to many well-documented physiological [26, 27] and psychophysical [28, 29] phenomena. DN models various neural processes: adaptation [30, 31], attention [32], automatic gain control [33], decorrelation, and statistical whitening [34]. The defining characteristic of DN is that each neuron's response is divided by a weighted sum of the activity of a pool of neurons (Eq. 2, below) like when normalizing the length of a vector. Due to its wide applicability and ability to explain a variety of neurophysiological phenomena, we argue that this characteristic should be central to any neurodynamical model. Both the Wilson-Cowan and SSN models have been shown to approximate DN responses [35, 5], but only approximately in certain parameter regimes.

Normalization techniques have been extensively adopted for training DNNs, demonstrating their ability to stabilize, accelerate training, and enhance generalization [36, 37, 38]. Divisive normalization can be viewed as a comprehensive normalization strategy, with batch and layer normalization being specific instances [39]. Models implementing DN have shown superior performance compared to common normalization methods (Batch [36], Layer [37], Group [40]) in tasks such as image recognition with convolutional neural networks (CNNs) [41] and language modeling with RNNs [39, 42]. Despite the foundational role of these techniques in deep learning algorithms, their implementation is ad hoc, limiting their conceptual relevance. They serve as practical solutions addressing the limitations of current machine learning frameworks rather than offering principled insights derived from understanding cortical circuits.

It has been proposed that DN is achieved via a recurrent circuit [11, 13, 43, 44, 45, 46, 47]. Oscillatory recurrent gated neural integrator circuits (ORGaNICs) are rate-based recurrent neural circuit models that implement DN dynamically via recurrent amplification [47, 48]. Since ORGaNICs' response follows the DN equation at steady-state, its steady-state response captures the full range of aforementioned neural phenomena explained by DN [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]. ORGaNICs have further been shown to simulate key time-dependent neurophysiological and cognitive/perceptual phenomena under realistic biophysical constraints [47, 48]. Additional phenomena not explained by DN [49] can in principle be integrated into the model. In this paper, however, we focus on the effects of DN on the dynamical stability of ORGaNICs. Despite some empirical evidence that ORGaNICs are highly robust, the question of whether the model is stable for arbitrary parameter choices, and thus whether it can be robustly trained on ML tasks by backpropagation-through-time (BPTT), remains open.

Here, we establish the unconditional stability -- applicable across all parameters and inputs -- of a multidimensional two-neuron-types ORGaNICs model when the recurrent weight matrix is the identity. We prove this result, detailed in Section 4, by the indirect method of Lyapunov: we perform linear stability analysis around the model's analytically-known normalization fixed point and reduce the stability problem to that of a high-dimensional mechanical system, whose stability is defined in terms of a tractable quadratic eigenvalue problem. We then address the stability of the model with an arbitrary recurrent weight matrix in Section 5. While the indirect method of Lyapunov becomes intractable for such a system, we provide proof of unconditional stability for a two-dimensional circuit with an arbitrary recurrent weight and offer empirical evidence supporting the claim of stability for high-dimensional systems.

ORGaNICs can be viewed as biophysically plausible extensions of Long Short Term Memory units (LSTMs) [3] and Gated Recurrent Units (GRUs) [50], RNN architectures that have been widely used in ML applications [3, 51, 52, 53, 54]. The main differences are that ORGaNICs operate in continuous time and have built-in dynamic normalization (via recurrent gain modulation) and built-in attention (via input gain modulation). Thus, we expect that ORGaNICs should be able to solve relatively sophisticated tasks [47]. Here, we demonstrate (Section 6) that by virtue of their intrinsic stability, ORGaNICs can be trained on sequence modeling tasks by BPTT, in the same manner as traditional RNNs (unlike SSN that instead requires costly specialized training strategies [55]), despite implementing power-law activations [5]. Moreover, we show that ORGaNICs trained by naive BPTT (i.e., without gradient clipping/scaling or other ad hoc strategies) achieve performance comparable to LSTMs on the tasks that we consider, despite no systematic hyperparameter tuning.

## 2 Related Work

**Trainable biologically plausible neurodynamical models**: There have been several attempts to develop neurodynamical models that mimic the function of biological circuits and that can be trained on cognitive tasks. Song et al. [56] incorporated Dale's law into the vanilla RNN architecture, which was successfully trained across a variety of cognitive tasks. Building on this, Soo et al. [57] developed a technique for such RNNs to learn long-term dependencies by using skip connections through time. ORGaNICs is a model that is already built on biological principles and can learn long-term dependencies intrinsically by tuning the (intrinsic or effective) time constants, therefore it does not require the method used in [57]. Soo et al. [55] introduced a novel training methodology (dynamics-neural growth) for SSNs and demonstrated its utility for tasks involving static (time-independent) stimuli. However, this training approach is costly and difficult to scale (because SSNs, unlike ORGaNICs, are not unconditionally stable), and its applicability on tasks with dynamically changing inputs remains unclear.

**Dynamical systems view of RNNs:** The stability of continuous-time RNNs has been extensively studied and discussed in a comprehensive review by Zhang et al. [58]. Recent advancements have focused on designing architectures that address the issues of vanishing and exploding gradients, thereby enhancing trainability and performance. A central idea in these designs is to achieve better trainability and generalization by ensuring the dynamical stability of the network. Moreover to avoid the problem of vanishing gradients the key idea is to constrain the real part of the eigenvalues of the linearized dynamical system to be close to zero, which facilitates the propagation and retention of information over long durations of time. Chang et al. [59] and Erichson et al. [60] achieve this by imposing an antisymmetric constraint on the recurrent weight matrix. Meanwhile, Rusch et al. [61; 62] propose an architecture based on coupled damped harmonic oscillators, resulting in a second-order system of ordinary differential equations that behaves similarly to how ORGaNICs behave in the vicinity of the normalization fixed point, as we show in Section 4. Despite their impressive performance on various sequential data benchmarks, these models lack biological plausibility due to their use of saturating nonlinearities (instead of normalization) and unrealistic weight parameterizations.

## 3 Model description

In its simplest form, the two-neuron-types ORGaNICs model [47; 48] with \(n\) neurons of each type can be written as,

\[\bm{\tau}_{y}\odot\dot{\mathbf{y}} =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\mathbf{a }^{+}\right)\odot\left(\mathbf{W}_{r}\left(\sqrt{\mathbf{y}^{+}}-\sqrt{ \mathbf{y}^{-}}\right)\right)\] (1) \[\bm{\tau}_{a}\odot\dot{\mathbf{a}} =-\mathbf{a}+\mathbf{b}_{0}^{2}\odot\bm{\sigma}^{2}+\mathbf{W}\, \left(\left(\mathbf{y}^{+}+\mathbf{y}^{-}\right)\odot\mathbf{a}^{+2}\right)\]

where \(\mathbf{y}\in\mathbb{R}^{n}\) and \(\mathbf{a}\in\mathbb{R}^{n}\) are the membrane potentials (relative to an arbitrary threshold potential that we take to be 0) of the excitatory (\(\mathbf{y}\)) and inhibitory (\(\mathbf{a}\)) neurons, evolving according to the dynamical equations defined above with \(\mathbf{y}\) and \(\mathbf{a}\) denoting the time derivatives. The notation \(\odot\) denotes element-wise multiplication of vectors, and squaring, rectification, square-root, and division are also performed element-wise. \(\mathbf{1}\) is an n-dimensional vector with all entries equal to 1. \(\mathbf{z}\in\mathbb{R}^{n}\) is the input drive to the circuit and is a weighted sum of the input, \(\mathbf{x}\in\mathbb{R}^{m}\), i.e., \(\mathbf{z}=\mathbf{W}_{zx}\mathbf{x}\). The firing rates, \(\mathbf{y}^{\pm}=|\pm\mathbf{y}|^{2}\) and \(\mathbf{a}^{+}=\sqrt{|\mathbf{a}|}\) are rectified (\(|.,.|\)) power functions of the underlying membrane potentials. For the derivation of a general model with arbitrary power-law exponents, including the Eq. 1, see Appendix A. Note that the term \(\sqrt{\mathbf{y}^{+}}-\sqrt{\mathbf{y}^{-}}\) serves the purpose of defining a mechanism for reconstructing the membrane potential (which can be negative, depending on the sign of the input) from the firing rates \(\mathbf{y}^{\pm}\) that are strictly nonnegative. \(\mathbf{y}^{+}\) and \(\mathbf{y}^{-}\) are the firing rates of neurons with complementary receptive fields such that they encode inputs with positive and negative signs, respectively. Note that only one of these neurons fires at a given time. In ORGaNICs, these neurons have a single dynamical equation for their membrane potentials, where the sign of indicates which neuron is active. Neurons with such complementary (anti-phase) receptive fields are found adjacent to each other in the visual cortex [63], and we hypothesize that such complementary neurons are ubiquitous throughout the neocortex. \(\mathbf{b}\in\mathbb{R}_{+}^{+n}\) and \(\mathbf{b}_{0}\in\mathbb{R}_{+}^{+n}\) are the input gains for the external inputs \(\mathbf{z}\) and \(\boldsymbol{\sigma}\) fed to neurons \(\mathbf{y}\) and \(\mathbf{a}\), respectively. \(\mathbb{R}_{+}^{+}\) is the set of positive real numbers, \(\{x\in\mathbb{R}\,|\,x>0\}\). \(\boldsymbol{\sigma}\in\mathbb{R}_{*}^{+n}\) determines the semistaturation of the responses of neurons \(\mathbf{y}\) by contributing to the depolarization of neurons \(\mathbf{a}\). \(\boldsymbol{\tau}_{y}\in\mathbb{R}_{*}^{+n}\) and \(\boldsymbol{\tau}_{a}\in\mathbb{R}_{*}^{+n}\) represent the time constants of \(\mathbf{y}\) and \(\mathbf{a}\) neurons.

In addition to receiving external inputs, both \(\mathbf{y}\) and \(\mathbf{a}\) neurons receive recurrent inputs, represented by the last term in both of the equations. \(\mathbf{W}_{r}\in\mathbb{R}^{n\times n}\) is the recurrent weight matrix that captures lateral connections between the \(\mathbf{y}\) neurons. This recurrent input is gated by the a neurons, via the term \((\mathbf{1}-\mathbf{a}^{+})\). Similarly, the _nonnegative_ normalization weight matrix, \(\mathbf{W}\in\mathbb{R}_{*}^{n\times n}\), encapsulates the recurrent inputs received by the \(\mathbf{a}\) neurons. The differential equations are designed in such a way that when \(\mathbf{W}_{r}=\mathbf{I}\) and \(\mathbf{b}=\mathbf{b}_{0}=b_{0}\mathbf{1}\) (i.e., with all elements equal to a constant \(b_{0}\)), the principal neurons follow the normalization equation exactly (and approximately when \(\mathbf{W}_{r}\neq\mathbf{I}\)) at steady-state,

\[\mathbf{y}_{s}^{+}\equiv\lfloor\mathbf{y}_{s}\rfloor^{2}=\frac{\lfloor \mathbf{z}\rfloor^{2}}{\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\lfloor \mathbf{z}\rfloor^{2}+\lfloor-\mathbf{z}\rfloor^{2}\right)}.\] (2)

\(\lfloor\mathbf{z}\rfloor^{2}\) and \(\lfloor-\mathbf{z}\rfloor^{2}\) represent the contribution of neurons with complementary receptive fields to the normalization pool, and \(\lfloor\mathbf{z}\rfloor^{2}+\lfloor-\mathbf{z}\rfloor^{2}=\mathbf{z}^{2}\) is the contrast energy of the input. Note that the recurrent gain, \((\mathbf{1}-\mathbf{a}^{+})\), is a particular nonlinear function of the output responses/activation designed to achieve DN, while the input gain, \(\mathbf{b}^{+}\), is an input gate that can implement an attention mechanism.

## 4 Stability analysis of high-dimensional model with identity recurrent weights

We consider the stability of the general high-dimensional ORGaNICs (Eq. 1) when the recurrent weight matrix is identity, \(\mathbf{W}_{r}=\mathbf{I}\). We first simplify the dynamical system by noting that \(\sqrt{\mathbf{y}^{+}}-\sqrt{\mathbf{y}^{-}}=\mathbf{y}\) and \(\mathbf{y}^{+}+\mathbf{y}^{-}=\mathbf{y}^{2}\) yielding the following equations,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\sqrt{\lfloor\mathbf{a}\rfloor}\odot\mathbf{y}+\mathbf{b} \odot\mathbf{z}\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=-\mathbf{ a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \mathbf{y}^{2}\odot\lfloor\mathbf{a}\rfloor\right)\end{split}\] (3)

For identity recurrent weights, we have a unique fixed point, given by,

\[\mathbf{y}_{s}=\frac{\mathbf{b}\odot\mathbf{z}}{\sqrt{\mathbf{b}_{0}^{2} \odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{ 2}\right)}};\quad\mathbf{a}_{s}=\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2 }+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)\] (4)

Since the normalization weights in the matrix \(\mathbf{W}\) are nonnegative, at steady-state we have \(\mathbf{a}_{s}>\mathbf{0}\), so that \(\sqrt{\lfloor\mathbf{a}_{s}\rfloor}=\sqrt{\mathbf{a}_{s}}\), and the corresponding firing rates at steady-state are,

\[\mathbf{y}_{s}^{\pm}=\frac{\lfloor\pm\mathbf{b}\odot\mathbf{z}\rfloor^{2}}{ \mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2 }\odot\mathbf{z}^{2}\right)};\quad\mathbf{a}_{s}^{+}=\sqrt{\mathbf{b}_{0}^{2 }\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^ {2}\right)}\] (5)

Note that we recover the normalization equation, Eq. 2, if \(\mathbf{b}=\mathbf{b}_{0}=b_{0}\mathbf{1}\). Since the fixed points of \(\mathbf{y}\) and \(\mathbf{a}\) neurons are known analytically, to prove that this fixed point is _locally asymptotically stable_ (i.e., the responses converge asymptotically to the fixed point), we apply the _indirect method of Lyapunov_ at this fixed point [64]. This method allows us to analyze the stability of the nonlinear system in the vicinity of the fixed point by studying the corresponding linearized system. The Jacobian matrix \(\mathbf{J}\in\mathbb{R}^{2n\times 2n}\) about \((\mathbf{y}_{s},\mathbf{a}_{s})\), defining the linearized system, is given by,

\[\mathbf{J}=\begin{bmatrix}-\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{ \boldsymbol{\tau}_{y}}\right)&-\mathbf{D}\left(\frac{\mathbf{y}_{s}}{ \mathbf{z}\odot\sqrt{\mathbf{a}_{s}\odot\boldsymbol{\tau}_{y}}}\right)\\ \mathbf{D}\left(\frac{\mathbf{z}}{\boldsymbol{\tau}_{a}}\right)\mathbf{W}\, \mathbf{D}\left(\mathbf{a}_{s}\odot\mathbf{y}_{s}\right)&\mathbf{D}\left( \frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\left(-\mathbf{I}+\mathbf{W}\, \mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right)\end{bmatrix}\] (6)

where \(\mathbf{D}(\mathbf{x})\) is a diagonal matrix of appropriate size with the elements of the vector \(\mathbf{x}\) on the diagonal. A necessary and sufficient condition for local stability is that the real parts of all eigenvalues of this matrix are negative. We thus proceed by computing the characteristic polynomial for the Jacobian, \(p_{\mathcal{I}}(\lambda)\equiv\det(\mathbf{J}-\lambda\mathbf{I})\). The roots of this polynomial, found by setting \(p_{\mathcal{I}}(\lambda)=0\), are the eigenvalues of the system. Consider the block matrix,

\[\mathbf{J}-\lambda\mathbf{I}=\begin{bmatrix}\mathbf{A}_{11}&\mathbf{A}_{12}\\ \mathbf{A}_{21}&\mathbf{A}_{22}\end{bmatrix}=\begin{bmatrix}-\mathbf{D}\left( \frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}}\right)-\lambda\mathbf{I}&- \mathbf{D}\left(\frac{\mathbf{y}_{s}}{\mathbf{2}\odot\sqrt{\mathbf{a}_{s}\odot \mathbf{y}_{s}}}\right)\\ \mathbf{D}\left(\frac{\mathbf{2}}{\boldsymbol{\tau}_{a}}\right)\mathbf{W}\, \mathbf{D}\left(\mathbf{a}_{s}\odot\mathbf{y}_{s}\right)&\mathbf{D}\left( \frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\left(-\mathbf{I}+\mathbf{W}\, \mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right)-\lambda\mathbf{I}\end{bmatrix}\] (7)Notice that \(\mathbf{A}_{11}\) and \(\mathbf{A}_{12}\) are diagonal and therefore they commute, i.e., \(\mathbf{A}_{11}\mathbf{A}_{12}=\mathbf{A}_{12}\mathbf{A}_{11}\), so we have that \(\det(\mathbf{J}-\lambda\mathbf{I})=\det(\mathbf{A}_{22}\mathbf{A}_{11}- \mathbf{A}_{21}\mathbf{A}_{12})\) which is a property of the determinant of block matrices with commuting blocks [65]. Therefore, the characteristic polynomial of the linearized system after expansion of the terms and simplification is given by,

\[\det(\mathbf{J}-\lambda\mathbf{I})=\det\biggl{(}\lambda^{2}\mathbf{I}+\lambda \left[\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)+ \mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}}\right)- \mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\mathbf{W} \mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right]+\mathbf{D}\left(\frac{\sqrt{ \mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}\odot\boldsymbol{\tau}_{a}}\right)\biggr{)}\] (8)

Finding the roots of this polynomial is thus a quadratic eigenvalue problem of the form \(\mathcal{L}(\lambda)\equiv\det(\lambda^{2}\mathbf{I}+\lambda\mathbf{B}+ \mathbf{K})=0\), which has been studied extensively [66, 67, 68, 69]. \(\mathcal{L}(\lambda)\) can be interpreted as the characteristic polynomial associated with a system of linear second-order differential equations with constant coefficients of the form \(\dot{\mathbf{I}}\dot{\mathbf{x}}+\dot{\mathbf{B}}\dot{\mathbf{x}}+\mathbf{K} \mathbf{x}=\mathbf{0}\). Therefore, proving the stability of our system (i.e., \(\mathbf{Re}(\lambda)<0\) for \(\{\lambda:\mathcal{L}(\lambda)=0\}\)), is equivalent to proving the asymptotic stability of \(\dot{\mathbf{I}}\dot{\mathbf{x}}+\mathbf{B}\dot{\mathbf{x}}+\mathbf{K} \mathbf{x}=\mathbf{0}\).

Tisseur et al. [67] and Kirillov et al. [69] list a set of constraints on the damping matrix, \(\mathbf{B}\), and stiffness matrix, \(\mathbf{K}\), that yield a stable system, but they are _not_ directly applicable to our system. In the context of a high-dimensional mechanical system, our system falls under the category of _gyroscopically stabilized systems with indefinite damping_. Few results are known about the conditions leading to the stability of such systems. By constructing a Lyapunov function, we prove (Appendix B) the following stability theorem that is directly applicable to our system, following an approach similar to Kliem et al. [70].

**Theorem 4.1**.: _For a system of linear differential equations with constant coefficients of the form,_

\[\dot{\mathbf{K}}\dot{\mathbf{x}}+\mathbf{B}\dot{\mathbf{x}}+\mathbf{K} \mathbf{x}=\mathbf{0}\] (9)

_where \(\mathbf{B}\in\mathbb{R}^{n\times n}\) and \(\mathbf{K}\in\mathbb{R}^{n\times n}\) is a positive diagonal matrix (hence \(\mathbf{K}\succ 0\)), the dynamical system is globally asymptotically stable if \(\mathbf{B}\) is Lyapunov diagonally stable._

Since the stiffness matrix,

\[\mathbf{K}=\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y} \odot\boldsymbol{\tau}_{a}}\right)=\mathbf{D}\left(\frac{\sqrt{\mathbf{b}_{0}^ {2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z} ^{2}\right)}}{\boldsymbol{\tau}_{y}\odot\boldsymbol{\tau}_{a}}\right)\] (10)

is a positive diagonal matrix, a sufficient condition for stability of the system is that the damping matrix, \(\mathbf{B}\), given by,

\[\mathbf{B}= \mathbf{B}_{1}+\mathbf{B}_{2}-\mathbf{B}_{3}\] \[= \mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)+ \mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}}\right)- \mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\mathbf{W} \mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\] (11) \[= \mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)+ \mathbf{D}\left(\frac{\sqrt{\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+ \mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)}}{\boldsymbol{\tau}_ {y}}\right)-\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right) \mathbf{W}\mathbf{D}\left(\frac{\mathbf{b}^{2}\odot\mathbf{z}^{2}}{\mathbf{b} _{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2}\odot \mathbf{z}^{2}\right)}\right)\]

is _Lyapunov diagonally stable_, i.e., there exists a positive definite diagonal matrix \(\mathbf{T}\), such that \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}\) is positive definite.

Since all of the parameters are positive, and the weights in the matrix \(\mathbf{W}\) are nonnegative, we can conclude the following: \(\mathbf{B}_{1}\) and \(\mathbf{B}_{2}\) are positive diagonal matrices and \(\mathbf{B}_{3}\) is a matrix with all positive entries (that may or may not be symmetric). Therefore, \(\mathbf{B}\) is a _Z-matrix_, meaning that its off-diagonal entries are nonpositive. Further, a _Z-matrix_ is _Lyapunov diagonally stable_ if and only if it is a nonsingular _M-matrix_. Intuitively, _M-matrices_ are matrices with non-positive off-diagonal elements and "large enough" positive diagonal entries. Berman & Plemmons [71] list 50 equivalent definitions of nonsingular _M-matrices_. We use the one that is best suited for our problem.

**Theorem 4.2**.: _(Chapter 6, Theorem 2.3 from [71]) A Z-matrix matrix \(\mathbf{B}\in\mathbb{R}^{n\times n}\) is Lyapunov diagonally stable if and only if there exists a convergent regular splitting of the matrix, that is, it has a representation of the form \(\mathbf{B}=\mathbf{M}-\mathbf{N}\), where \(\mathbf{M}^{-1}\) and \(\mathbf{N}\) have all nonnegative entries, and \(\mathbf{M}^{-1}\mathbf{N}\) has a spectral radius smaller than 1._

We now show that, indeed, \(\mathbf{B}\) has a _convergent regular splitting_ for all combinations of the circuit parameters and for all inputs. We have already shown that \(\mathbf{B}\) is a _Z-matrix_, therefore, the first condition of the theorem is satisfied. Next, we consider the following splitting \(\mathbf{B}=\mathbf{M}-\mathbf{N}\) with \(\mathbf{M}=\mathbf{B}_{1}+\mathbf{B}_{2}\) and \(\mathbf{N}=\mathbf{B}_{3}\). Since \(\mathbf{B}_{1}\) and \(\mathbf{B}_{2}\) are positive diagonal matrices, \(\mathbf{M}^{-1}\) is nonnegative, while \(\mathbf{N}\) is also nonnegative because \(\mathbf{B}_{3}\) has all positive entries. Therefore, the only condition left to satisfy is that the spectral radius of \(\mathbf{M}^{-1}\mathbf{N}\) is smaller than 1, or that the matrix is convergent.

The matrix \(\mathbf{S}=\mathbf{M}^{-1}\mathbf{N}=(\mathbf{B}_{1}+\mathbf{B}_{2})^{-1} \mathbf{B}_{3}\) can be written as,

\[\mathbf{S}=\mathbf{D}\left(\frac{\mathbf{1}}{\mathbf{1}+(\boldsymbol{\tau}_{a} /\boldsymbol{\tau}_{y})\odot\sqrt{\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{ 2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)}}\right)\mathbf{ WD}\left(\frac{\mathbf{b}^{2}\odot\mathbf{z}^{2}}{\mathbf{b}_{0}^{2}\odot \boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2} \right)}\right)\] (12)

We prove the following theorem (Appendix D) which directly applies to \(\mathbf{S}\),

**Theorem 4.3**.: _A matrix \(\mathbf{A}\) of the form \(\mathbf{A}=\mathbf{D}(\mathbf{t})\,\mathbf{W}\,\mathbf{D}\left(\mathbf{u}/ \left(\mathbf{v}+\mathbf{W}\mathbf{u}\right)\right)\) is convergent (i.e., its spectral radius is less than 1), if \(\mathbf{W}\in\mathbb{R}^{n\times n}\) and \(\mathbf{t},\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}\) satisfy \(0<t_{i}<1\), \(u_{i}\geq 0\), \(v_{i}>0\) and \(w_{ij}\geq 0\) for all \(i,j\)._

Defining \(\mathbf{t}\rightarrow\mathbf{1}/(\mathbf{1}+(\boldsymbol{\tau}_{a}/ \boldsymbol{\tau}_{y})\odot\sqrt{\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{ 2}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)})\), \(\mathbf{u}\rightarrow\mathbf{b}^{2}\odot\mathbf{z}^{2}\) and \(\mathbf{v}\rightarrow\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}\), it can be seen that they satisfy the constraints of the theorem, and thus \(\mathbf{S}\) is convergent. This implies that \(\mathbf{B}\) has a _convergent regular splitting_ and, as a result, the linearized dynamical system is unconditionally globally asymptotically stable for all the values of parameters and inputs. Further, the global asymptotic stability of linearization implies the local asymptotic stability of the normalization fixed point for ORGaNICs.

This result holds even when the neurons have different time constants, regardless of their type, as no assumptions were made about the time constants. This finding is significant for machine learning, particularly for designing architectures based on ORGaNICs. It allows neurons/units to integrate information at varying time scales while maintaining a stable circuit that performs normalization dynamically. Moreover, analytical expressions for eigenvalues can be obtained in the following case,

**Theorem 4.4**.: _Let \(\mathbf{W}_{r}=\mathbf{I}\), the normalization matrix be given by \(\mathbf{W}=\alpha\mathbf{E}\), where \(\mathbf{E}\) is the all-ones matrix, and the parameters are scalars, i.e., \(\boldsymbol{\tau}_{y}=\tau_{y}\mathbf{1}\), \(\boldsymbol{\tau}_{a}=\tau_{a}\mathbf{1}\), \(\mathbf{b}_{0}=b_{0}\mathbf{1}\), and \(\boldsymbol{\sigma}=\sigma\mathbf{1}\). Under these conditions, the eigenvalues of the system admit closed form solutions (detailed in Appendix C)._

This result is particularly useful for neuroscience as it elucidates the connection between ORGaNICs parameters and the strength and frequency of oscillatory activity. Since we followed a direct Lyapunov approach to prove Theorem 4.1 as shown in Appendix B, we can derive an _energy_ (viz., Lyapunov function) for ORGaNICs as shown in Appendix H.

**Theorem 4.5**.: _When \(\mathbf{W}_{r}=\mathbf{I}\), the energy (Lyapunov function) minimized by ORGaNICs in the vicinity of the normalization fixed point, is given by,_

\[V(\mathbf{y},\mathbf{a})=\sum_{i=1}^{n}t_{i}\frac{a_{s_{i}}}{{y_{s}}_{2}}^{2} \left[\frac{\tau_{y_{i}}}{\tau_{a_{i}}}\sqrt{a_{s_{i}}}\left(y_{i}-{y_{s_{i}}} \right)^{2}+(\sqrt{a_{i}}y_{i}-\sqrt{a_{s_{i}}}{y_{s_{i}}})^{2}\right].\] (13)

_Where \(t_{i}\) are the diagonal entries of \(\mathbf{T}\) and \({y_{s}}_{i}\) (\(a_{s_{i}}\)) are the steady-state values of neurons \(y_{i}\) (\(a_{i}\))._

Specifically, for a two-dimensional model (one \(y\) neuron and one \(a\) neuron) this expression simplifies to reveal that ORGaNICs behave like a damped harmonic oscillator with _energy_,

\[V(y,a)=\frac{\tau_{y}}{\tau_{a}}\sqrt{b_{0}^{2}\sigma^{2}+wb^{2}z^{2}}\,\left(y -\frac{bz}{\sqrt{b_{0}^{2}\sigma^{2}+wb^{2}z^{2}}}\right)^{2}+(\sqrt{a}y-bz)^ {2}\] (14)

This result demonstrates that ORGaNICs minimize the residual of the instantaneously reconstructed gated input drive (\(\sqrt{a}y-bz\)), while also ensuring that the principal neuron's response, \(y\), achieves DN. The balance between these objectives is governed by the parameters and the external input strength. With fixed parameters, weaker inputs, \(z\), cause the model to prioritize input matching over normalization, whereas stronger inputs increasingly engage the normalization objective.

## 5 Stability analysis for arbitrary recurrent weights

Now, we relax the constraint that the recurrent weight matrix must be identity, allowing \(\mathbf{W}_{r}\neq\mathbf{I}\), and see how the stability result changes. This leads to the following set of equations,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\sqrt{ \left\lfloor\mathbf{a}\right\rfloor}\right)\odot\left(\mathbf{W}_{r} \mathbf{y}\right)\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=-\mathbf{ a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \mathbf{y}^{2}\odot\left\lfloor\mathbf{a}\right\rfloor\right)\end{split}\] (15)The linear stability analysis becomes intractable for a general \(\mathbf{W}_{r}\) because we no longer have a closed-form analytical expression for the steady states of \(\mathbf{y}\) and \(\mathbf{a}\). Additionally, the characteristic polynomial cannot be expressed in a way similar to Eq.8. Nevertheless, for a two-dimensional system,

\[\begin{split}\tau_{y}\dot{y}&=-y+bz+\left(1-\sqrt{ \lfloor a\rfloor}\right)w_{r}y\\ \tau_{a}\dot{a}&=-a+b_{0}^{2}\sigma^{2}+wy^{2} \lfloor a\rfloor\end{split}\] (16)

we can prove the following, with a detailed analysis provided in Appendix E.

**Theorem 5.1**.: _Given that the recurrence is contracting, i.e., \(0<w_{r}\leq 1\), when \(z>0\) (\(z<0\)) there exists a unique fixed point with \(y_{s}>0\) (\(y_{s}<0\)) and \(a_{s}>0\), and it is asymptotically stable._

**Theorem 5.2**.: _Given that the recurrence is expansive, i.e., \(w_{r}>1\), there are either 1 or 3 fixed points of which at least one is asymptotically stable. When \(z>0\) (\(z<0\)) there exists exactly 1 fixed point with \(y_{s}>0\) (\(y_{s}<0\)) and \(a_{s}>0\), and it is asymptotically stable. If \(b_{0}\sigma>1-1/w_{r}\), there are no additional fixed points. If \(b_{0}\sigma<1-1/w_{r}\), there exist either \(0\) or \(2\) additional fixed points with \(y_{s}<0\) (\(y_{s}>0\)) and \(a_{s}>0\) whose stability cannot be guaranteed._

We plot the phase portraits for these different cases in Fig. 1. The key takeaway is that there is always a fixed point \((y_{s},a_{s})\) with \(a_{s}>0\) and \(y_{s}\) having the same sign as \(z\). This fixed point is asymptotically stable regardless of the value of \(w_{r}\). Based on these results and the proven stability of arbitrary dimensional ORGaNICs when \(\mathbf{W}_{r}=\mathbf{I}\) (as shown in Section 4), we conjecture that

**Conjecture 5.3**.: _Consider high-dimensional ORGaNICs with an arbitrary recurrent weight matrix \(\mathbf{W}_{r}\) and no constraints on the remaining parameters. If the norm of the input drive satisfies \(||\mathbf{z}||\leq 1\), and the maximum singular value of \(\mathbf{W}_{r}\) is constrained to be 1, then the system possesses at least one asymptotically stable fixed point._

This conjecture is supported by empirical evidence showing consistent stability, as ORGaNICs initialized with random parameters and inputs under these constraints have exhibited stability in 100%

Figure 1: **Phase portraits for 2D ORGaNICs with positive input drive. We plot the phase portraits of 2D ORGaNICs in the vicinity of the stable fixed points for contractive (a, d) and expansive (b, c, e, f) recurrence scalar \(w_{r}\). A stable fixed point always exists, regardless of the parameter values. (a-c), The main model (Eq. 16). (d-f), The rectified model (Eq. 102). Red stars and black circles indicate stable and unstable fixed points, respectively. The parameters for all plots are: \(b=0.5\), \(\tau_{a}=2\,\text{ms}\), \(\tau_{y}=2\,\text{ms}\), \(w=1.0\), and \(z=1.0\). For (a) & (d), the parameters are \(w_{r}=0.5\), \(b_{0}=0.5\), \(\sigma=0.1\); for (b) & (e), \(w_{r}=2.0\), \(b_{0}=0.5\), \(\sigma=0.1\); and for (c) & (f), \(w_{r}=2.0\), \(b_{0}=1.0\), \(\sigma=1.0\).**

of trials, see Fig. 4. We further speculate that ORGaNICs may be _typically_ stable beyond this regime as we find that 100% of trials yield a stable circuit when the constraint on the maximum singular value of \(\mathbf{W}_{r}\) is increased to 2, but it becomes unstable when it is increased to 3.

## 6 Experiments

We provide further empirical evidence in support of Conjecture 5.3 that ORGaNICs is asymptotically stable by showing that stability is preserved when training ORGaNICs using naive BPTT on two different tasks: 1) static classification of MNIST, 2) sequential classification of pixel-by-pixel MNIST. Because these ML tasks have no relevance for neurobiological or cognitive processes, we relax one aspect of the biological plausibility of ORGaNICs, specifically, allowing arbitrary (learned) nonnegative values for the intrinsic time constants.1

Footnote 1: Python code for this study is available at https://github.com/martiniani-lab/dynamic-divisive-norm.

### Static input classification task

We first show that we can train ORGaNICs on the MNIST handwritten digit dataset [72] presented to the circuit as a static input. This setting corresponds to evolving the responses of the neurons dynamically until they reach a fixed point solution and using the steady-state firing rates of the principal neurons to predict the labels, akin to deep equilibrium models [73]. While the fixed point of the circuit is known when \(\mathbf{W}_{r}=\mathbf{I}\) (given by Eq. 89), we allow \(\mathbf{W}_{r}\) to be learnable and parameterized it to have a maximum singular value of 1. This constraint allows us to find the fixed point responses of all the neurons without simulation, using a fixed point iteration scheme (Algorithm 1) that converges with great accuracy in a few (less than 5) steps, see Fig. 4 & 5. We provide an intuition for why this algorithm works with empirical evidence of fast convergence in Appendix G.

We trained ORGaNICs on this task (details provided in Appendix I.1) and compared its performance to SSN [5] trained by dynamics-neutral growth [55]. We found that ORGaNICs perform better than SSN with the same model size, and on par with an MLP (Table 1). We analyzed the eigenvalues of the Jacobian matrix of the trained model and consistently found the largest real part to be negative (Fig. 5), indicating stability. Moreover, we found that stability was maintained during training (Fig. 6).

### Time varying input

We trained unconstrained ORGaNICs by naive BPTT on a classification task of sequential MNIST (sMNIST), proposed by Le et al. [74]. This is a challenging task because it involves long-term dependencies and requires the architecture to maintain and integrate information over long timescales. Briefly, the task involves the presentation of pixels of MNIST images sequentially (one pixel for each

\begin{table}
\begin{tabular}{c c} \hline \hline Model & Accuracy \\ \hline SSN (50:50) & 94.9\% \\ SSN (80:20) & 95.2\% \\ MLP (50) & 98.2\% \\ \hline
**ORGaNICs** (50:50) & 98.1\% \\
**ORGaNICs** (80:80) & 98.2\% \\
**ORGaNICs** (two layers) & 98.1\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test accuracy on MNIST datasettimestep) in scanline order, and at the end of the input the model has to predict the digit that was presented. There is a more complicated version of this task, permuted sequential MNIST, in which the pixels of all images are permuted in some random order before being presented sequentially. We train ORGaNICs with different hidden layer sizes (number of \(\mathbf{y}\) neurons) on these two tasks by discretizing the rectified ORGaNICs with arbitrary recurrence, Eq. 87, which has all the properties that we have derived for the main model. Since an unstable fixed point is undesirable in such a task, as it may lead to diverging trajectories, we prefer the rectified model (Appendix F) over the main model. We proved that the 2D rectified ORGaNICs (Eq. 102) does not exhibit an unstable fixed point for positive inputs, as it can also be seen in Fig 1. The hidden states of the neurons are initialized with a uniform random distribution (for more details, see Appendix I.2). Additionally, we make the input gains \(\mathbf{b}\) and \(\mathbf{b}_{0}\) dynamical with their ODEs given by,

\[\boldsymbol{\tau}_{b}\odot\dot{\mathbf{b}} =-\mathbf{b}+f(\mathbf{W}_{bx}\mathbf{x}+\mathbf{W}_{by}\mathbf{ y}+\mathbf{W}_{ba}\mathbf{a})\] (17) \[\boldsymbol{\tau}_{b_{0}}\odot\dot{\mathbf{b}}_{0} =-\mathbf{b}_{0}+f(\mathbf{W}_{box}\mathbf{x}+\mathbf{W}_{boy} \mathbf{y}+\mathbf{W}_{boa}\mathbf{a})\]

We achieved slightly better performance than LSTMs on sMNIST with a smaller model size and comparable performance on permuted sMNIST, without hyperparameter optimization and without gradient clipping/scaling (Table 2). We found that the trajectories of \(\mathbf{y}\) are bounded when it is trained on the sequential task (Fig. 7), indicating stability. We also show that the training of ORGaNICs is stable and does not require gradient clipping when the intrinsic time constants of the neurons are fixed (Table 2).

## 7 Discussion

**Summary:** While extensive research has been aimed at identifying highly expressive RNN architectures that can model complex data, there has been little advancement in developing robust, biologically plausible recurrent neural circuits that are easy to train and perform comparably to their artificial counterparts. Regularization techniques such as batch, group, and layer normalization have been developed and are implemented as ad hoc add-ons making them biologically implausible. In this work, we bridge these gaps by leveraging the recently proposed ORGaNICs model which implements divisive normalization (DN) dynamically in a recurrent circuit. We establish the unconditional stability of an arbitrary dimensional ORGaNICs circuit with an identity recurrent weight matrix (\(\mathbf{W}_{r}\)), with all of the other parameters and inputs unconstrained, and provide empirical evidence of stability for ORGaNICs with arbitrary \(\mathbf{W}_{r}\). Since ORGaNICs remain stable for all parameter values and inputs, we do not need to resort to techniques that are restrictive in parameter space, or that require designing unrealistic structures for weight matrices. ORGaNICs' intrinsic stability mitigates the issues of exploding and oscillating gradients, enabling the use of "vanilla" BPTT without the need for gradient clipping, which is instead required when training LSTMs. Moreover, ORGaNICs effectively address the vanishing gradient problem often encountered when training RNNs. This is achieved by processing information across various timescales, resulting in a blend of lossy and non-lossy neurons, while preserving stability. The model's effectiveness in overcoming vanishing gradients is further evidenced by its competitive performance against architectures specifically designed to address this issue, such as LSTMs.

**Dynamic normalization:** Normalization techniques, such as batch and layer normalization, are fundamental in modern ML architectures significantly enhancing the training and performance of

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & sMNIST & psMNIST & \# units & \# params \\ \hline LSTMs [75] & 97.3\% & 92.6\% & 128 & 68k \\ AntisymmetricRNN [59] & 98.0\% & 95.8\% & 128 & 10k \\ coRNN [61] & 99.3\% & 96.6\% & 128 & 34k \\ Lipschitz RNN [60] & 99.4\% & 96.3\% & 128 & 34k \\ \hline
**ORGaNICs** (fixed time constants) & 90.3\% & 80.3\% & 64 & 26k \\
**ORGaNICs** (fixed time constants) & 94.8\% & 84.8\% & 128 & 100k \\ \hline
**ORGaNICs** & 97.7\% & 89.9\% & 64 & 26k \\
**ORGaNICs** & 97.8\% & 90.7\% & 128 & 100k \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test accuracy on sequential pixel-by-pixel MNIST and permuted MNISTCNNs. However, a principled approach to incorporating normalization into RNNs has remained elusive. While layer normalization is commonly applied to RNNs to stabilize training, it does not influence the underlying circuit dynamics since it is applied a-posteriori to the output activations, leaving the stability of RNNs unaffected. Furthermore, DN has been shown to generalize batch and layer normalization [39], leading to improved performance [39, 41, 42]. ORGaNICs, unlike RNNs with layer normalization, implement DN dynamically within the circuit, marking the first instance of this concept being applied and analyzed in ML. Our work demonstrates that embedding DN within a circuit naturally leads to stability, which is greatly advantageous for trainability. This stability, a consequence of dynamic DN, sets ORGaNICs apart from other RNNs by providing both output normalization and model robustness. As a result, ORGaNICs can be trained using BPTT, achieving performance on par with LSTMs. The key insight is that the dynamic application of DN not only enhances training efficiency but also improves model robustness. This illustrates how the incorporation of neurobiological principles can drive advances in ML.

**Interpretability:** In the proof of stability, we establish a direct connection between ORGaNICs and systems of coupled damped harmonic oscillators, which have long been studied in mechanics and control theory. This analogy not only enables us to derive an interpretable energy function for ORGaNICs (Eq. 13), providing a normative principle of what the circuit aims to accomplish, but also sheds light on the link between normalization and dynamical stability of neural circuits. For a relevant ML task, having an analytical expression for the energy function allows us to quantify the relative contributions of the individual neurons in the trained model, offering more interpretability than other RNN architectures. For instance, Eq. 13 shows that the ratio of time constants (\(\tau_{y}/\tau_{a}\)) for E-I neuron pairs determines how much weight a neuron assigns to divisive normalization relative to aligning its responses with the input drive \(\mathbf{z}\). This insight provides a clear functional role for each neuron in the trained model. Moreover, since ORGaNICs are biologically plausible, we can understand how the various components of the dynamical system might be computed within a neural circuit [48], bridging the gap between theoretical models and biological implementation, and offering a means to generate and test hypotheses about neural computation in real biological systems (which we will be reporting elsewhere).

**Limitations:** Although the stability property pertains to a continuous-time system of nonlinear differential equations, typical implementations for tasks with sequential data involve an Euler discretization of these equations for training purposes. This might lead to a stiff dynamical system, potentially causing numerical instabilities and explosive dynamics, highlighting the importance of carefully parameterizing time constants and choosing a small enough time step to maintain stable dynamics. The proof of unconditional stability is only tractable for the two-dimensional circuit and the high-dimensional circuit with \(\mathbf{W}_{r}=\mathbf{I}\). Therefore, we can only conjecture the stability of ORGaNICs for arbitrary \(\mathbf{W}_{r}\), based on these two limiting cases and on empirical evidence. In the current form, the weight matrices of the input gain modulators, \(\mathbf{W}_{by}\), \(\mathbf{W}_{ba}\), \(\mathbf{W}_{boy}\), and \(\mathbf{W}_{boa}\), are each \(n\times n\). As a result, the number of parameters grows more rapidly with the hidden state size compared to other RNNs. To mitigate this, we plan to explore using compact and/or convolutional weights to prevent a significant increase in the number of parameters as the hidden state size expands.

**Attention mechanisms in ORGaNICs:** ORGaNICs have a built-in mechanism for attention: modulating the input gain \(\mathbf{b}\) (e.g., Eq. 17), coupled with DN. This attention mechanism aligns with experimental data on both increases in the gain of neural responses and improvements in behavioral performance [19, 32, 36, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]. Moreover, this mechanism performs a computation that is analogous to that of an attention head in ML systems (including transformers [2]) as both operate by changing the gain over time. In ORGaNICs, DN replaces the softmax operation typically used in an attention head.

**Future work:** This study has explored only a single layer of ORGaNICs for the sequential tasks. Future work will examine how stacked layers with feedback connections, similar to those in the cortex, perform on benchmarks for sequential modeling and also on cognitive tasks with long-term dependencies. We have thus far shown that ORGaNICs can address the problem of long-term dependencies by learning intrinsic time constants. Future investigations will assess the performance of ORGaNICs for tasks with long-term dependencies by learning to modulate the responses of the \(\mathbf{a}\) and \(\mathbf{b}\) neurons to control the effective time constant of the recurrent circuit (without changing the intrinsic time constants) [47], i.e., implementing a working memory circuit capable of learning to maintain and manipulate information across various timescales.

## Acknowledgments and Disclosure of Funding

The authors thank anonymous reviewers for their insightful suggestions. The authors also acknowledge valuable discussions with Flaviano Morone, Asit Pal, Mathias Casiulis, and Guanming Zhang. This work was supported by the National Institute of Health under award number R01EY035242. S.M. acknowledges the Simons Center for Computational Physical Chemistry for financial support. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.

## References

* [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [3] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [4] Hugh R Wilson and Jack D Cowan. Excitatory and inhibitory interactions in localized populations of model neurons. _Biophysical journal_, 12(1):1-24, 1972.
* [5] Daniel B Rubin, Stephen D Van Hooser, and Kenneth D Miller. The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex. _Neuron_, 85(2):402-417, 2015.
* [6] Jack D Cowan, Jeremy Neuman, and Wim van Drongelen. Wilson-cowan equations for neocortical dynamics. _The Journal of Mathematical Neuroscience_, 6:1-24, 2016.
* [7] Yashar Ahmadian, Daniel B Rubin, and Kenneth D Miller. Analysis of the stabilized supralinear network. _Neural computation_, 25(8):1994-2037, 2013.
* [8] James Kotary, Ferdinando Fioretto, Pascal Van Hententryck, and Bryan Wilder. End-to-end constrained optimization learning: A survey. _arXiv preprint arXiv:2103.16378_, 2021.
* [9] Priya L Donti, David Rolnick, and J Zico Kolter. Dc3: A learning method for optimization with hard constraints. _arXiv preprint arXiv:2104.12225_, 2021.
* [10] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. _Inverse problems_, 34(1):014004, 2017.
* [11] Matteo Carandini and David J Heeger. Summation and division by neurons in primate visual cortex. _Science_, 264(5163):1333-1336, 1994.
* [12] Duane G Albrecht and Wilson S Geisler. Motion selectivity and the contrast-response function of simple cells in the visual cortex. _Visual neuroscience_, 7(6):531-546, 1991.
* [13] David J Heeger. Normalization of cell responses in cat striate cortex. _Visual neuroscience_, 9(2):181-197, 1992.
* [14] David J Heeger. Modeling simple-cell direction selectivity with normalized, half-squared, linear operators. _Journal of neurophysiology_, 70(5):1885-1898, 1993.
* [15] Eero P Simoncelli and David J Heeger. A model of neuronal responses in visual area mt. _Vision research_, 38(5):743-761, 1998.
* [16] John H Reynolds, Leonardo Chelazzi, and Robert Desimone. Competitive mechanisms subserve attention in macaque areas v2 and v4. _Journal of Neuroscience_, 19(5):1736-1753, 1999.
* [17] Odelia Schwartz and Eero Simoncelli. Natural sound statistics and divisive normalization in the auditory system. _Advances in neural information processing systems_, 13, 2000.
* [18] Davide Zoccolan, David D Cox, and James J DiCarlo. Multiple object response normalization in monkey inferotemporal cortex. _Journal of Neuroscience_, 25(36):8150-8164, 2005.
* [19] Geoffrey M Boynton. A framework for describing the effects of attention on visual responses. _Vision research_, 49(10):1129-1143, 2009.

* [20] Joonyeol Lee and John HR Maunsell. A normalization model of attentional modulation of single unit responses. _PloS one_, 4(2):e4651, 2009.
* [21] Paul M Bays. Noise in neural populations accounts for errors in working memory. _Journal of Neuroscience_, 34(10):3632-3645, 2014.
* [22] Wei Ji Ma, Masud Husain, and Paul M Bays. Changing concepts of working memory. _Nature neuroscience_, 17(3):347-356, 2014.
* [23] Barbara Zenger-Landolt and David J Heeger. Response suppression in v1 agrees with psychophysics of surround masking. _Journal of Neuroscience_, 23(17):6884-6893, 2003.
* [24] John M Foley. Human luminance pattern-vision mechanisms: masking experiments require a new model. _JOSA A_, 11(6):1710-1719, 1994.
* [25] Matteo Carandini and David J Heeger. Normalization as a canonical neural computation. _Nature reviews neuroscience_, 13(1):51-62, 2012.
* [26] Gijs Joost Brouwer and David J Heeger. Cross-orientation suppression in human visual cortex. _Journal of neurophysiology_, 106(5):2108-2119, 2011.
* [27] James R Cavanaugh, Wyeth Bair, and J Anthony Movshon. Nature and interaction of signals from the receptive field center and surround in macaque v1 neurons. _Journal of neurophysiology_, 88(5):2530-2546, 2002.
* [28] Jing Xing and David J Heeger. Center-surround interactions in foveal and peripheral vision. _Vision research_, 40(22):3065-3072, 2000.
* [29] Alexander A Petrov, Barbara Anne Dosher, and Zhong-Lin Lu. The dynamics of perceptual learning: an incremental reweighting model. _Psychological review_, 112(4):715, 2005.
* [30] Martin J Wainwright, Odelia Schwartz, and Eero P Simoncelli. Natural image statistics and divisive normalization. 2002.
* [31] Zachary M Westrick, David J Heeger, and Michael S Landy. Pattern adaptation and normalization reweighting. _Journal of Neuroscience_, 36(38):9805-9816, 2016.
* [32] John H Reynolds and David J Heeger. The normalization model of attention. _Neuron_, 61(2):168-185, 2009.
* [33] David J Heeger, Eero P Simoncelli, and J Anthony Movshon. Computational models of cortical visual processing. _Proceedings of the National Academy of Sciences_, 93(2):623-627, 1996.
* [34] Siwei Lyu and Eero P Simoncelli. Nonlinear extraction of independent components of natural images using radial gaussianization. _Neural computation_, 21(6):1485-1519, 2009.
* [35] Jesus Malo, Jose Juan Esteve-Taboada, and Marcelo Bertalmio. Cortical divisive normalization from wilson-cowan neural dynamics. _Journal of Nonlinear Science_, 34(2):1-36, 2024.
* [36] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* [37] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [38] Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization techniques in training dnns: Methodology, analysis and application. _IEEE transactions on pattern analysis and machine intelligence_, 45(8):10173-10196, 2023.
* [39] Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H Sinz, and Richard S Zemel. Normalizing the normalizers: Comparing and extending network normalization schemes. _arXiv preprint arXiv:1611.04520_, 2016.
* [40] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.
* [41] Michelle Miller, SueYeon Chung, and Kenneth D Miller. Divisive feature normalization improves image recognition performance in alexnet. In _International Conference on Learning Representations_, 2021.
* [42] Saurabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11237-11246, 2020.

* [43] Matteo Carandini, David J Heeger, and J Anthony Movshon. Linearity and normalization in simple cells of the macaque primary visual cortex. _Journal of Neuroscience_, 17(21):8621-8644, 1997.
* [44] Hirofumi Ozeki, Ian M Finn, Evan S Schaffer, Kenneth D Miller, and David Ferster. Inhibitory stabilization of the cortical network underlies visual surround suppression. _Neuron_, 62(4):578-592, 2009.
* [45] Matteo Carandini, David J Heeger, and Walter Senn. A synaptic explanation of suppression in visual cortex. _Journal of Neuroscience_, 22(22):10053-10065, 2002.
* [46] Tobias Brosch and Heiko Neumann. Interaction of feedforward and feedback streams in visual cortex in a firing-rate model of columnar computations. _Neural Networks_, 54:11-16, 2014.
* [47] David J Heeger and Wayne E Mackey. Oscillatory recurrent gated neural integrator circuits (organics), a unifying theoretical framework for neural dynamics. _Proceedings of the National Academy of Sciences_, 116(45):22783-22794, 2019.
* [48] David J Heeger and Klavdia O Zemlianova. A recurrent circuit implements normalization, simulating the dynamics of v1 activity. _Proceedings of the National Academy of Sciences_, 117(36):22494-22505, 2020.
* [49] Lyndon Duong, Eero Simoncelli, Dmitri Chklovskii, and David Lipshutz. Adaptive whitening with fast gain modulation and slow synaptic plasticity. _Advances in Neural Information Processing Systems_, 36, 2024.
* [50] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_, 2014.
* [51] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. _Advances in neural information processing systems_, 27, 2014.
* [52] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* [53] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 6645-6649. Ieee, 2013.
* [54] Alex Graves. Generating sequences with recurrent neural networks. _arXiv preprint arXiv:1308.0850_, 2013.
* [55] Wayne Soo and Mate Lengyel. Training stochastic stabilized supralinear networks by dynamics-neutral growth. _Advances in Neural Information Processing Systems_, 35:29278-29291, 2022.
* [56] H Francis Song, Guangyu R Yang, and Xiao-Jing Wang. Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework. _PLoS computational biology_, 12(2):e1004792, 2016.
* [57] Wayne Soo, Vishwa Goudar, and Xiao-Jing Wang. Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependencies. _Advances in Neural Information Processing Systems_, 36, 2024.
* [58] Huaguang Zhang, Zhanshan Wang, and Derong Liu. A comprehensive review of stability analysis of continuous-time recurrent neural networks. _IEEE Transactions on Neural Networks and Learning Systems_, 25(7):1229-1262, 2014.
* [59] Bo Chang, Minmin Chen, Eldad Haber, and Ed H Chi. Antisymmetricrnn: A dynamical system view on recurrent neural networks. _arXiv preprint arXiv:1902.09689_, 2019.
* [60] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. _arXiv preprint arXiv:2006.12070_, 2020.
* [61] T Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies. _arXiv preprint arXiv:2010.00951_, 2020.
* [62] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies. In _International Conference on Machine Learning_, pages 9168-9178. PMLR, 2021.

* [63] Zheng Liu, James P Gaska, Lowell D Jacobson, and Daniel A Pollen. Interneuronal interaction between members of quadrature phase and anti-phase pairs in the cat's visual cortex. _Vision research_, 32(7):1193-1198, 1992.
* [64] Hassan K Khalil. _Control of nonlinear systems_. Prentice Hall, New York, NY, 2002.
* [65] John R Silvester. Determinants of block matrices. _The Mathematical Gazette_, 84(501):460-467, 2000.
* [66] Peter Lancaster. _Lambda-matrices and vibrating systems_. Courier Corporation, 2002.
* [67] Francoise Tisseur and Karl Meerbergen. The quadratic eigenvalue problem. _SIAM review_, 43(2):235-286, 2001.
* [68] Peter Lancaster. Stability of linear gyroscopic systems: a review. _Linear Algebra and its Applications_, 439(3):686-706, 2013.
* [69] Oleg N Kirillov. _Nonconservative stability problems of modern physics_, volume 14. Walter de Gruyter GmbH & Co KG, 2021.
* [70] Wolfhard Kliem and Christian Pommer. Indefinite damping in mechanical systems and gyroscopic stabilization. _Zeitschrift fur angewandte Mathematik und Physik_, 60:785-795, 2009.
* [71] Abraham Berman and Robert J Plemmons. _Nonnegative matrices in the mathematical sciences_. SIAM, 1994.
* [72] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. _IEEE signal processing magazine_, 29(6):141-142, 2012.
* [73] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. _Advances in neural information processing systems_, 32, 2019.
* [74] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. _arXiv preprint arXiv:1504.00941_, 2015.
* [75] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In _International conference on machine learning_, pages 1120-1128. PMLR, 2016.
* [76] Frederik Beuth and Fred H Hamker. A mechanistic cortical microcircuit of attention for amplification, normalization and suppression. _Vision research_, 116:241-257, 2015.
* [77] Rachel N Denison, Marisa Carrasco, and David J Heeger. A dynamic normalization model of temporal attention. _Nature Human Behaviour_, 5(12):1674-1685, 2021.
* [78] Katrin Herrmann, David J Heeger, and Marisa Carrasco. Feature-based attention enhances performance by increasing response gain. _Vision research_, 74:10-20, 2012.
* [79] Katrin Herrmann, Leila Montaser-Kouhsari, Marisa Carrasco, and David J Heeger. When size matters: attention affects performance by contrast or response gain. _Nature neuroscience_, 13(12):1554-1559, 2010.
* [80] John HR Maunsell. Neuronal mechanisms of visual attention. _Annual review of vision science_, 1:373-391, 2015.
* [81] Amy M Ni and John HR Maunsell. Spatially tuned normalization explains attention modulation variance within neurons. _Journal of neurophysiology_, 118(3):1903-1913, 2017.
* [82] Amy M Ni and John HR Maunsell. Neuronal effects of spatial and feature attention differ due to normalization. _Journal of Neuroscience_, 39(28):5493-5505, 2019.
* [83] Philipp Schwedhelm, B Suresh Krishna, and Stefan Treue. An extended normalization model of attention accounts for feature-based attentional enhancement of both response and coherence gain. _PLoS computational biology_, 12(12):e1005225, 2016.
* [84] Philip L Smith, David K Sewell, and Simon D Lilburn. From shunting inhibition to dynamic normalization: Attentional selection and decision-making in brief visual displays. _Vision Research_, 116:219-240, 2015.
* [85] Xilin Zhang, Shruti Japee, Zaid Safiullah, Nicole Mlynaryk, and Leslie G Ungerleider. A normalization framework for emotional attention. _PLoS biology_, 14(11):e1002578, 2016.
* [86] Matteo Carandini. Amplification of trial-to-trial response variability by neurons in visual cortex. _PLoS biology_, 2(9):e264, 2004.

* [87] Jeffrey S Anderson, Ilan Lampl, Deda C Gillespie, and David Ferster. The contribution of noise to contrast invariance of orientation tuning in cat visual cortex. _Science_, 290(5498):1968-1972, 2000.
* [88] Nicholas J Priebe and David Ferster. Inhibition, spike threshold, and stimulus selectivity in primary visual cortex. _Neuron_, 57(4):482-497, 2008.
* [89] Dennis S Bernstein. _Matrix mathematics: theory, facts, and formulas_. Princeton university press, 2009.
* [90] Fuzhen Zhang. _The Schur complement and its applications_, volume 4. Springer Science & Business Media, 2006.
* [91] Roger A Horn and Charles R Johnson. _Matrix analysis_. Cambridge university press, 2012.
* [92] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* [93] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [94] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _Proceedings of the IEEE international conference on computer vision_, pages 1026-1034, 2015.

Derivation of ORGaNICs

Here, we derive a generalized 2-neuron types (excitatory and inhibitory) ORGaNICs model for a high-dimensional input. The system presented in Eq. 1 is a special case of this generalized model where \(p=2\) and \(\mathbf{a}^{+}=\sqrt{\left\lfloor\mathbf{a}\right\rfloor}\). Assuming \(\mathbf{W}\) is the normalization weight matrix, and \(\mathbf{z}\) is the input drive, we can write the normalization equations for principal neurons with complementary receptive fields as,

\[\mathbf{y}_{s}^{+}=\frac{\left\lfloor\mathbf{z}\right\rfloor^{p}}{\bm{ \sigma}^{p}+\mathbf{W}\left(\left\lfloor\mathbf{z}\right\rfloor^{p}+\left \lfloor-\mathbf{z}\right\rfloor^{p}\right)};\quad\mathbf{y}_{s}^{-}=\frac{ \left\lfloor-\mathbf{z}\right\rfloor^{p}}{\bm{\sigma}^{p}+\mathbf{W}\left( \left\lfloor\mathbf{z}\right\rfloor^{p}+\left\lfloor-\mathbf{z}\right\rfloor ^{p}\right)}\] (18)

Note that typically the exponent of the input \(p\sim 2\) for cortical neurons. \(\left\lfloor\mathbf{z}\right\rfloor^{p}\) and \(\left\lfloor-\mathbf{z}\right\rfloor^{p}\) represent the contribution of neurons with complementary receptive fields to the normalization pool. Mathematically, we have, \(\left\lfloor\mathbf{z}\right\rfloor^{p}+\left\lfloor-\mathbf{z}\right\rfloor ^{p}=\left\lvert\mathbf{z}\right\rvert^{p}\).

Here, we derive, for a general \(p\), the dynamical equations that have the fixed point defined by the normalization equation above. First, it is important to distinguish between the membrane potentials and the firing rates of neurons. The coarse (low-pass filtered) membrane potential of a given type of neuron is denoted by the vector, \(\mathbf{y},\mathbf{a}\), with the corresponding firing rates of \(\mathbf{y}^{+}(\mathbf{y}^{-}),\mathbf{a}^{+}\). The instantaneous firing rates of the neurons are obtained from the corresponding coarse membrane potentials by applying rectification, denoted by \(\left\lfloor.\right\rfloor\), and a power law (sub/supra-linear) activation for different types of neurons [86, 87, 88]. Therefore, for a set of membrane potentials \(\mathbf{x}\) the instantaneous firing rates are \(\mathbf{x}^{+}=\left\lfloor\mathbf{x}\right\rfloor^{\alpha}\). Specifically for principal neurons, we have, \(\mathbf{y}^{+}=\left\lfloor\mathbf{y}\right\rfloor^{p}\) and \(\mathbf{y}^{-}=\left\lfloor-\mathbf{y}\right\rfloor^{p}\). Combining the firing of principal neurons with the complementary receptive fields, \(\mathbf{y}^{+}\) and \(\mathbf{y}^{-}\), Eq. 18 can be alternatively written as,

\[\left\lvert\mathbf{y}_{s}\right\rvert^{p}=\mathbf{y}_{s}^{+}+\mathbf{y}_{s}^{ -}=\frac{\left\lvert\mathbf{z}\right\rvert^{p}}{\bm{\sigma}^{p}+\mathbf{W} \left\lvert\mathbf{z}\right\rvert^{p}}\] (19)

Now for the principal neuron \(y_{j}\) receiving an input drive \(z_{j}\), we can rewrite the normalization equation for each neuron as,

\[\left\lvert y_{j}^{s}\right\rvert^{p}=\frac{\left\lvert z_{j}\right\rvert^{p }}{\sigma_{j}^{p}+\sum\limits_{k}W_{jk}|z_{k}|^{p}}\] (20)

In the ORGaNICs paradigm [47], the steady-state activity of the principal neurons (single equation for complementary receptive fields) is a weighted sum of input drive and recurrent drive,

\[\tau_{y_{j}}\frac{\mathrm{d}y_{j}}{\mathrm{d}t}=-y_{j}+\underset{\text{ Weighted input drive}}{b_{j}z_{j}}+\left(1-a_{j}^{+}\right)\sum\limits_{k}w_{rjk}\left((y_{k}^{+})^{1/p}-(y_{k}^{-})^{1/p}\right)\] (21)

Here \(w_{r}\) are the weights of the recurrent weight matrix \(\mathbf{W}_{r}\) encoding the recurrent/lateral connections between the principal neurons \(\mathbf{y}\); \(y_{k}^{+}\) is the firing rate of the principal neuron \(k\), given by \(y_{k}^{+}=\left\lfloor y_{k}\right\rfloor^{p}\), and \(y_{k}^{-}=\left\lfloor-y_{k}\right\rfloor^{p}\) is the firing rate of the complementary principal neuron \(k\). \(b_{j}\) is the gain to the input drive \(z_{j}\) which simulates attention (realized via gain modulation). \((1-a_{j}^{+})\) is the gain to the recurrent drive which controls the recurrent amplification.

Now, we find the dynamics of the inhibitory neurons \(a_{j}\) with firing rates \(a_{j}^{+}\), that yield stable dynamics with the fixed point given by Eq. 18. First, we assume that the recurrent weight matrix \(\mathbf{W}_{r}=\mathbf{I}\). Also, note that the following identity holds: \((y_{k}^{+})^{1/p}-(y_{k}^{-})^{1/p}=\left\lfloor y_{k}\right\rfloor-\left\lfloor -y_{k}\right\rfloor=y_{k}\). Therefore, Eq.21 can be simplified to,

\[\tau_{y_{j}}\frac{\mathrm{d}y_{j}}{\mathrm{d}t}=-y_{j}+b_{j}z_{j}+\left(1-a_{j }^{+}\right)y_{j}\] (22)

At steady-state, the fixed-points \((y_{j}^{s},a_{j}^{s})\), satisfy the following relationship,

\[a_{j}^{s+}y_{j}^{s}=b_{j}z_{j}\] (23)

Taking modulus and raising both sides to power \(p\), we get,

\[\left\lvert a_{j}^{s+}y_{j}^{s}\right\rvert^{p}=\left\lvert b_{j}z_{j}\right\rvert ^{p}\] (24)or in vector form,

\[\begin{split}\left|\mathbf{a}_{s}^{+}\odot\mathbf{y}_{s}\right|^{p}& =\left|\mathbf{b}\odot\mathbf{z}\right|^{p}\\ \left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_{s}\right| ^{p}&=\mathbf{b}^{p}\odot\left|\mathbf{z}\right|^{p}\end{split}\] (25)

From Eq. 19, we know that

\[\left|\mathbf{z}\right|^{p}=\boldsymbol{\sigma}^{p}\odot\left|\mathbf{y}_{s} \right|^{p}+\left|\mathbf{y}_{s}\right|^{p}\odot\left(\mathbf{W}\left|\mathbf{ z}\right|^{p}\right)\] (26)

Since we can write the element-wise product between two vectors \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\), \(\mathbf{x}_{1}\odot\mathbf{x}_{2}=\mathbf{x}_{2}\odot\mathbf{x}_{1}=\mathbf{ D}(\mathbf{x}_{1})\mathbf{x}_{2}\). Where \(\mathbf{D}(\mathbf{x}_{1})\) is a diagonal matrix with elements of the vector \(\mathbf{x}_{1}\) on the diagonal. Therefore, using this fact we can rewrite the equation above as,

\[\begin{split}\left|\mathbf{z}\right|^{p}&= \boldsymbol{\sigma}^{p}\odot\left|\mathbf{y}_{s}\right|^{p}+\mathbf{D}\left( \left|\mathbf{y}_{s}\right|^{p}\right)\left(\mathbf{W}\left|\mathbf{z}\right|^ {p}\right)\\ \left|\mathbf{z}\right|^{p}&=\boldsymbol{\sigma}^{p} \odot\left|\mathbf{y}_{s}\right|^{p}+\mathbf{D}\left(\left|\mathbf{y}_{s} \right|^{p}\right)\mathbf{W}\left|\mathbf{z}\right|^{p}\\ \left|\mathbf{z}\right|^{p}&=\boldsymbol{\sigma}^{p} \odot\left|\mathbf{y}_{s}\right|^{p}\\ \left|\mathbf{z}\right|^{p}&=\left[\mathbf{I}- \mathbf{D}\left(\left|\mathbf{y}_{s}\right|^{p}\right)\mathbf{W}\right]^{-1} \left(\boldsymbol{\sigma}^{p}\odot\left|\mathbf{y}_{s}\right|^{p}\right)\end{split}\] (27)

Substituting the expression for \(\left|\mathbf{z}\right|^{p}\) into Eq. 25, we get,

\[\left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_{s}\right|^{p}= \mathbf{b}^{p}\odot\left(\left[\mathbf{I}-\mathbf{D}\left(\left|\mathbf{y}_{s }\right|^{p}\right)\mathbf{W}\right]^{-1}\left(\boldsymbol{\sigma}^{p}\odot \left|\mathbf{y}_{s}\right|^{p}\right)\right)\] (28)

We simplify this equation as,

\[\begin{split}\left[\mathbf{I}-\mathbf{D}\left(\left|\mathbf{y}_{s }\right|^{p}\right)\mathbf{W}\right]\left(\frac{\left(\mathbf{a}_{s}^{+} \right)^{p}\odot\left|\mathbf{y}_{s}\right|^{p}}{\mathbf{b}^{p}}\right)& =\boldsymbol{\sigma}^{p}\odot\left|\mathbf{y}_{s}\right|^{p}\\ \mathbf{D}\left(1/|\mathbf{y}_{s}|^{p}\right)\left[\mathbf{I}- \mathbf{D}\left(\left|\mathbf{y}_{s}\right|^{p}\right)\mathbf{W}\right]\left( \frac{\left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_{s}\right|^{p} }{\mathbf{b}^{p}}\right)&=\mathbf{D}\left(1/|\mathbf{y}_{s}|^{p} \right)\left(\boldsymbol{\sigma}^{p}\odot\left|\mathbf{y}_{s}\right|^{p} \right)\\ \frac{\left(\mathbf{a}_{s}^{+}\right)^{p}}{\mathbf{b}^{p}}- \mathbf{W}\left(\frac{\left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_ {s}\right|^{p}}{\mathbf{b}^{p}}\right)&=\boldsymbol{\sigma}^{p} \end{split}\] (29)

Now we assume that all of the entries of \(\mathbf{b}\) are equal to a constant \(b_{0}\), therefore, we have,

\[\begin{split}\left(\mathbf{a}_{s}^{+}\right)^{p}-\mathbf{W}\left( \left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_{s}\right|^{p}\right)& =b_{0}^{p}\boldsymbol{\sigma}^{p}\\ \left(\mathbf{a}_{s}^{+}\right)^{p}&=b_{0}^{p} \boldsymbol{\sigma}^{p}+\mathbf{W}\left(\left(\mathbf{a}_{s}^{+}\right)^{p} \odot\left|\mathbf{y}_{s}\right|^{p}\right)\end{split}\] (30)

Element-wise multiplying both sides by \(\mathbf{a}_{s}/\left(\mathbf{a}_{s}^{+}\right)^{p}\) on the left, we get,

\[\mathbf{0}=-\mathbf{a}_{s}+\frac{\mathbf{a}_{s}}{\left(\mathbf{a}_{s}^{+} \right)^{p}}\odot\left[b_{0}^{p}\boldsymbol{\sigma}^{p}+\mathbf{W}\left( \left(\mathbf{a}_{s}^{+}\right)^{p}\odot\left|\mathbf{y}_{s}\right|^{p}\right)\right]\] (31)

This equation is true at the steady-state, but it is also in a form similar to that of \(\mathbf{y}\), such that we have a weighted input drive and a weighted recurrent drive, therefore, we propose the following dynamical equation for \(\mathbf{a}\),

\[\boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}=-\mathbf{a}+\frac{\mathbf{a}}{\left( \mathbf{a}^{+}\right)^{p}}\odot\left[b_{0}^{p}\boldsymbol{\sigma}^{p}+\mathbf{W} \left(\left(\mathbf{a}^{+}\right)^{p}\odot\left|\mathbf{y}\right|^{p}\right)\right]\] (32)

This equation naturally follows Eq. 31 at steady-state and thus also follows the normalization equation Eq. 19. Note that the equation above is true for any choice of \(\mathbf{a}^{+}\), but cortical neurons have been experimentally observed to have an exponent close to \(p=2\), i.e., \(\mathbf{y}^{+}=\left|\mathbf{y}\right|^{2}\) and \(\mathbf{y}^{-}=\left|-\mathbf{y}\right|^{2}\). Therefore, we get a particularly simple form of the equations when \(p=2\) and \(\mathbf{a}^{+}=\sqrt{\left[\mathbf{a}\right]}\). In this case, the equation for \(\mathbf{a}\) is given by,

\[\boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}=-\mathbf{a}+\left[b_{0}^{2} \boldsymbol{\sigma}^{2}+\mathbf{W}\left(\left(\mathbf{a}^{+}\right)^{2}\odot \mathbf{y}^{2}\right)\right]\] (33)

We reintroduce the recurrent weight matrix \(\mathbf{W}_{r}\) and to simulate the effect of attention by gain modulation, we define different gains for \(\mathbf{y}\) and \(\mathbf{a}\) neurons. Additionally, replacing \(\mathbf{y}^{2}\rightarrow\mathbf{y}^{+}+\mathbf{y}^{-}\) and \(\mathbf{y}\rightarrow\sqrt{\mathbf{y}^{+}}-\sqrt{\mathbf{y}^{-}}\) yields the dynamical system that we analyze,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\mathbf{a}^{+} \right)\odot\left(\mathbf{W}_{r}\left(\sqrt{\mathbf{y}^{+}}-\sqrt{\mathbf{y}^ {-}}\right)\right)\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=-\mathbf{ a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\left(\left(\mathbf{y}^{+}+ \mathbf{y}^{-}\right)\odot\mathbf{a}^{+2}\right)\end{split}\] (34)Finally, in the most general form, i.e., any choice of \(p\) and nonlinearity for \(\mathbf{a}^{+}\), the dynamical system of ORGaNICs is given by,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\mathbf{a} ^{+}\right)\odot\left(\mathbf{W}_{r}\left(\left(\mathbf{y}^{+}\right)^{1/p}- \left(\mathbf{y}^{-}\right)^{1/p}\right)\right)\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=- \mathbf{a}+\frac{\mathbf{a}}{\left(\mathbf{a}^{+}\right)^{p}}\odot\left[ \mathbf{b}_{0}^{p}\odot\boldsymbol{\sigma}^{p}+\mathbf{W}\left(\left(\mathbf{y }^{+}+\mathbf{y}^{-}\right)\odot\left(\mathbf{a}^{+}\right)^{p}\right)\right] \end{split}\] (35)

## Appendix B Stability theorem

**Theorem B.1**.: _For a system of linear differential equations with constant coefficients of the form,_

\[\mathbf{I}\ddot{\mathbf{x}}+\mathbf{B}\dot{\mathbf{x}}+\mathbf{K}\mathbf{x}= \mathbf{0}\] (36)

_where \(\mathbf{B}\in\mathbb{R}^{n\times n}\) and \(\mathbf{K}\in\mathbb{R}^{n\times n}\) is a positive diagonal matrix (hence \(\mathbf{K}\succ 0\)), the dynamical system is globally asymptotically stable if \(\mathbf{B}\) is Lyapunov diagonally stable._

Proof.: The stability of the system is defined by solving the following associated quadratic eigenvalue problem,

\[\mathbb{L}(\lambda)=\det(\lambda^{2}\mathbf{I}+\mathbf{B}\lambda+\mathbf{K})\] (37)

The spectrum of \(\mathbb{L}(\lambda)\), i.e., \(\{\lambda\in\mathbb{C}:\det(\mathbb{L}(\lambda))=0\}\) are also known as the eigenvalues of the system. The system defined by Eq. 36 is globally asymptotically stable if all of the eigenvalues have negative real parts. We take the direct Lyapunov approach to prove the stability of the linear system. We write Eq. 36 in the matrix form as \(\dot{\mathbf{z}}=\mathbf{J}\mathbf{z}\), where,

\[\mathbf{z}=\begin{bmatrix}\mathbf{x}\\ \dot{\mathbf{x}}\end{bmatrix}\quad\text{and}\quad\mathbf{J}=\begin{bmatrix} \mathbf{0}&\mathbf{I}\\ -\mathbf{K}&-\mathbf{B}\end{bmatrix}\] (38)

We will first prove the Lyapunov stability (\(\mathrm{Re}(\lambda_{\mathbf{J}})\leq 0\)) of this system to find the appropriate block diagonal matrices and then we will prove global asymptotic stability (\(\mathrm{Re}(\lambda_{\mathbf{J}})<0\)). To prove Lyapunov stability (Appendix B.2), we propose a Lyapunov function \(V(\mathbf{z})=\mathbf{z}^{\top}\mathbf{P}\mathbf{z}\), where \(\mathbf{P}\) is a block positive definite matrix defined as follows,

\[\mathbf{P}=\begin{bmatrix}\mathbf{A}&\mathbf{0}\\ \mathbf{0}&\mathbf{T}\end{bmatrix}\] (39)

where \(\mathbf{T}\in\mathbb{R}^{n\times n}\) is a positive diagonal matrix such that \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}\succ 0\) (notation for positive definite matrix) and \(\mathbf{A}\in\mathbb{R}^{n\times n}\) a flexible symmetric positive definite matrix that we will find using the second Lyapunov criteria. Note that such a matrix \(\mathbf{T}\) exits since \(\mathbf{B}\) is defined to be Lyapunov diagonally stable. It can be easily seen that \(\mathbf{P}\succ 0\) using the first criteria in Section B.1 since \(\mathbf{A}\succ 0\) and \(\mathbf{T}\succ 0\). Additionally, since \(\mathbf{T}\succ 0\), it is invertible.

Now, for Lyapunov stability, we need \(\dot{V}(\mathbf{z})=\mathbf{z}^{\top}\left(\mathbf{P}\mathbf{J}+\mathbf{J}^ {\top}\mathbf{P}\right)\mathbf{z}\leq 0\). Therefore, we find \(\mathbf{A}\) such that \(\mathbf{Q}=-\left(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top}\mathbf{P}\right)\) is positive semi-definite.

\[\begin{split}\mathbf{Q}&=-\left(\mathbf{P}\mathbf{J}+ \mathbf{J}^{\top}\mathbf{P}\right)\\ &=-\begin{bmatrix}\mathbf{A}&\mathbf{0}\\ \mathbf{0}&\mathbf{T}\end{bmatrix}\begin{bmatrix}\mathbf{0}&\mathbf{I}\\ -\mathbf{K}&-\mathbf{B}\end{bmatrix}-\begin{bmatrix}\mathbf{0}&-\mathbf{K}\\ \mathbf{I}&-\mathbf{B}^{\top}\end{bmatrix}\begin{bmatrix}\mathbf{A}&\mathbf{0 }\\ \mathbf{0}&\mathbf{T}\end{bmatrix}\\ &=\begin{bmatrix}\mathbf{0}&\mathbf{KT}-\mathbf{A}\\ \mathbf{TK}-\mathbf{A}&\mathbf{TB}+\mathbf{B}^{\top}\mathbf{T}\end{bmatrix} \end{split}\] (40)

We want to define \(\mathbf{A}\succ 0,\) such that \(\mathbf{Q}\succeq 0\). Using the second criteria from Section B.1, we need \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}\succ 0\) and \(-(\mathbf{K}\mathbf{T}-\mathbf{A})(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top} \mathbf{T})^{-1}(\mathbf{TK}-\mathbf{A})\succeq 0\). The first condition is satisfied by the definition of \(\mathbf{T}\). For the second condition to be satisfied, an obvious candidate for \(\mathbf{A}\) is \(\mathbf{TK}\). Note that both \(\mathbf{T}\) and \(\mathbf{K}\) are positive definite and diagonal, therefore they commute (\(\mathbf{KT}=\mathbf{TK}\)) and \(\mathbf{A}\) is symmetric and positive definite. When \(\mathbf{A}=\mathbf{TK}\), the LHS of the second condition becomes \(\mathbf{0}\succeq 0\). Therefore, the system is Lyapunov stable.

Now, we prove the global asymptotic stability of the system by again using the direct Lyapunov approach. We propose the Lyapunov function of the same form as before, i.e., \(V(\mathbf{z})=\mathbf{z}^{\top}\mathbf{P}\mathbf{z}\), where \(\mathbf{P}\) is a positive definite matrix. But for global asymptotic stability, we need a more stringent condition on the Lyapunov function, \(\dot{V}(\mathbf{z})=\mathbf{z}^{\top}\left(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top} \mathbf{P}\right)\mathbf{z}<0\). Drawing inspiration from the previous exercise, we consider the following form of the matrix \(\mathbf{P}\),

\[\mathbf{P}=\begin{bmatrix}\mathbf{T}\mathbf{K}&\epsilon\mathbf{I}\\ \epsilon\mathbf{I}&\mathbf{T}\end{bmatrix}\] (41)

Here, \(\epsilon>0\) is a scalar whose magnitude is to be determined based on the Lyapunov criteria for asymptotic stability. First, we need \(\mathbf{P}\succ 0\). Applying the first criteria from Section B.1, we want \(\epsilon\) to satisfy, \(\mathbf{T}\mathbf{K}-\epsilon^{2}\mathbf{T}^{-1}\succ 0\). Second, for \(\dot{V}(\mathbf{z})<0\), we want \(\mathbf{Q}=-\left(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top}\mathbf{P}\right)\) to be positive definite. \(\mathbf{Q}\) is given by,

\[\begin{split}\mathbf{Q}&=-\begin{bmatrix}\mathbf{T} \mathbf{K}&\epsilon\mathbf{I}\\ \epsilon\mathbf{I}&\mathbf{T}\end{bmatrix}\begin{bmatrix}\mathbf{0}&\mathbf{I} \\ -\mathbf{K}&-\mathbf{B}\end{bmatrix}-\begin{bmatrix}\mathbf{0}&-\mathbf{K}\\ \mathbf{I}&-\mathbf{B}^{\top}\end{bmatrix}\begin{bmatrix}\mathbf{T}\mathbf{K}& \epsilon\mathbf{I}\\ \epsilon\mathbf{I}&\mathbf{T}\end{bmatrix}\\ &=\begin{bmatrix}2\epsilon\mathbf{K}&\epsilon\mathbf{B}\\ \epsilon\mathbf{B}^{\top}&\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}-2 \epsilon\mathbf{I}\end{bmatrix}\end{split}\] (42)

Again, we apply the first criteria from Section B.1. \(\mathbf{Q}\succ 0\) if and only if \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}-2\epsilon\mathbf{I}\succ 0\) and \(2\epsilon\mathbf{K}-\epsilon^{2}\mathbf{B}\left(\mathbf{T}\mathbf{B}+\mathbf{B }^{\top}\mathbf{T}-2\epsilon\mathbf{I}\right)^{-1}\mathbf{B}^{\top}\succ 0\). To simplify notation we replace \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}\) with a positive definite matrix \(\mathbf{M}\). Therefore, we have to prove that there exists an \(\epsilon>0\) which satisfies the following conditions,

* \(\mathbf{T}\mathbf{K}-\epsilon^{2}\mathbf{T}^{-1}\succ 0\)
* \(\mathbf{M}-2\epsilon\mathbf{I}\succ 0\)
* \(2\mathbf{K}-\epsilon\mathbf{B}\left(\mathbf{M}-2\epsilon\mathbf{I}\right)^{- 1}\mathbf{B}^{\top}\succ 0\)

Assuming \(t_{i}\) and \(k_{i}\) to be the diagonal values of the positive diagonal matrices \(\mathbf{T}\) and \(\mathbf{K}\), we get the following two conditions, \(\epsilon<\min\left(t_{i}\sqrt{k_{i}}\right)\) and \(\epsilon<\alpha/2\), where \(\alpha\) is the smallest eigenvalue of \(\mathbf{M}\), or \(\alpha=\min\left(\lambda_{\mathbf{M}}\right)\).

Now, for the third condition, we will use a number of facts about positive definite matrices which are all listed in [89]. We first consider the following matrix inequality, \(\mathbf{M}\succeq\alpha\mathbf{I}\). This notation is equivalent to saying that \(\mathbf{M}-\alpha\mathbf{I}\) is positive semi-definite or \(\mathbf{M}-\alpha\mathbf{I}\succeq 0\). Therefore, we have,

\[\mathbf{M}-2\epsilon\mathbf{I}\succeq(\alpha-2\epsilon)\mathbf{I}\] (43)

Assuming, \(\epsilon\) is small enough such that the matrices on LHS and RHS are positive definite, we have,

\[\frac{1}{\alpha-2\epsilon}\mathbf{I}\succeq\left(\mathbf{M}-2\epsilon \mathbf{I}\right)^{-1}\] (44)

Since \(\mathbf{B}\) is nonsingular, it is full rank, therefore, we have,

\[\frac{1}{\alpha-2\epsilon}\mathbf{B}\mathbf{B}^{\top}\succeq\mathbf{B}\left( \mathbf{M}-2\epsilon\mathbf{I}\right)^{-1}\mathbf{B}^{\top}\] (45)

Multiplying both sides by \(\epsilon\), we get,

\[\frac{\epsilon}{\alpha-2\epsilon}\mathbf{B}\mathbf{B}^{\top}\succeq\epsilon \mathbf{B}\left(\mathbf{M}-2\epsilon\mathbf{I}\right)^{-1}\mathbf{B}^{\top}\] (46)

Notice that \(\mathbf{B}\mathbf{B}^{\top}\) is a positive definite matrix. Let \(\beta\) be the maximum eigenvalue of \(\mathbf{B}\mathbf{B}^{\top}\), or \(\beta=\max\left(\lambda_{\mathbf{B}\mathbf{B}^{\top}}\right)\). Therefore, we can add an upper-bound matrix to the inequality as follows,

\[\frac{\epsilon\beta}{\alpha-2\epsilon}\mathbf{I}\succeq\frac{\epsilon}{\alpha-2 \epsilon}\mathbf{B}\mathbf{B}^{\top}\succeq\epsilon\mathbf{B}\left(\mathbf{M} -2\epsilon\mathbf{I}\right)^{-1}\mathbf{B}^{\top}\] (47)

Now, we find \(\epsilon\) such that,

\[2\mathbf{K}\succ\frac{\epsilon\beta}{\alpha-2\epsilon}\mathbf{I}\] (48)

The range of values for which the above inequality is true is,

\[\epsilon<\min\left(\frac{2k_{i}\alpha}{\beta+4k_{i}}\right)\] (49)If \(\epsilon\) satisfies the inequality above, we have,

\[2\mathbf{K}\succ\frac{\epsilon\beta}{\alpha-2\epsilon}\mathbf{I}\succeq\frac{ \epsilon}{\alpha-2\epsilon}\mathbf{B}\mathbf{B}^{\top}\succeq\epsilon\mathbf{B} \left(\mathbf{M}-2\epsilon\mathbf{I}\right)^{-1}\mathbf{B}^{\top}\] (50)

Therefore, we have, \(2\mathbf{K}\succ\epsilon\mathbf{B}\left(\mathbf{M}-2\epsilon\mathbf{I}\right)^ {-1}\mathbf{B}^{\top}\) and the third condition required for \(\epsilon\) is satisfied.

This implies that there exists a range of \(\epsilon\), given by,

\[0<\epsilon<\min\left\{\min\left(t_{i}\sqrt{k_{i}}\right),\,\frac{\alpha}{2},\, \min\left(\frac{2k_{i}\alpha}{\beta+4k_{i}}\right)\right\}\] (51)

for which

\[\mathbf{P}=\begin{bmatrix}\mathbf{T}\mathbf{K}&\epsilon\mathbf{I}\\ \epsilon\mathbf{I}&\mathbf{T}\end{bmatrix}\] (52)

is a valid Lyapunov function for asymptotic stability. Therefore, the dynamical system is globally asymptotically stable.

### Positive definite block matrices (Schur complement)

For a symmetric block matrix of the form,

\[\mathbf{P}=\begin{bmatrix}\mathbf{A}&\mathbf{B}\\ \mathbf{B}^{\top}&\mathbf{C}\end{bmatrix}\] (53)

with \(\mathbf{A}=\mathbf{A}^{\top}\) and \(\mathbf{C}=\mathbf{C}^{\top}\). If \(\mathbf{C}\) is invertible the following two properties hold, [90],

* \(\mathbf{P}\succ 0\) if and only if \(\mathbf{C}\succ 0\) and \(\mathbf{A}-\mathbf{B}\mathbf{C}^{-1}\mathbf{B}^{\top}\succ 0\).
* If \(\mathbf{C}\succ 0\), then \(\mathbf{P}\succeq 0\) if and only if \(\mathbf{A}-\mathbf{B}\mathbf{C}^{-1}\mathbf{B}^{\top}\succeq 0\).

### Lyapunov stability criteria

Consider a non-linear autonomous dynamical system defined as \(\dot{\mathbf{x}}=\mathbf{f}(\mathbf{x})\), with a point of equilibrium at \(\mathbf{x}=\mathbf{0}\). Where \(\mathbf{x}\in\mathcal{D}\subseteq\mathbb{R}^{n}\) is the system state vector and \(\mathbf{f}(\mathbf{x}):\mathcal{D}\rightarrow\mathbb{R}^{n}\) is a continuous vector field on \(\mathcal{D}\) (contains origin). The dynamical system is called Lyapunov stable if there exists a real scalar function \(V(\mathbf{x}):\mathbb{R}^{n}\rightarrow\mathbb{R}\), also known as the Lyapunov function, such that it satisfies the following conditions,

* \(V(\mathbf{x})=0\), if and only if \(\mathbf{x}=\mathbf{0}\).
* \(V(\mathbf{x})>0\), if and only if \(\mathbf{x}\neq\mathbf{0}\).
* \(\dot{V}(\mathbf{x})\leq 0,\,\forall\,\mathbf{x}\neq\mathbf{0}\). Note that for asymptotic stability, we require the strict inequality \(\dot{V}(\mathbf{x})<0\).

For a linear dynamical system of the form \(\dot{\mathbf{x}}=\mathbf{J}\mathbf{x}\), where \(\mathbf{J}\in\mathbb{R}^{n\times n}\), with a point of equilibrium at \(\mathbf{x}=\mathbf{0}\). Consider a Lyapunov function \(V(\mathbf{x})\) of the form \(\mathbf{x}^{\top}\mathbf{P}\mathbf{x}\), such that \(\mathbf{P}\succ 0\). By the definition of a positive definite matrix, it satisfies the first two conditions, namely, \(V(\mathbf{x})=\mathbf{x}^{\top}\mathbf{P}\mathbf{x}=0\) when \(\mathbf{x}=\mathbf{0}\) and \(V(\mathbf{x})=\mathbf{x}^{\top}\mathbf{P}\mathbf{x}>0\) when \(x\neq\mathbf{0}\). For the third condition, consider \(\dot{V}(\mathbf{x})\),

\[\dot{V}(\mathbf{x}) =\frac{\mathrm{d}}{\mathrm{d}t}V(\mathbf{x})\] \[=\frac{\mathrm{d}}{\mathrm{d}t}\mathbf{x}^{\top}\mathbf{P} \mathbf{x}\] (54) \[=\mathbf{x}^{\top}\mathbf{P}\dot{\mathbf{x}}+\dot{\mathbf{x}}^{ \top}\mathbf{P}\mathbf{x}\] \[=\mathbf{x}^{\top}\left(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top} \mathbf{P}\right)\mathbf{x}\]

For stability, We need \(\dot{V}(\mathbf{x})=\mathbf{x}^{\top}\left(\mathbf{P}\mathbf{J}+\mathbf{J}^{ \top}\mathbf{P}\right)\mathbf{x}\leq 0\). This is satisfied when the matrix \(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top}\mathbf{P}\preceq 0\). In summary, a linear dynamical system \(\dot{\mathbf{x}}=\mathbf{J}\mathbf{x}\) is Lyapunov stable if there exists a positive definite matrix \(\mathbf{P}\), such that \(\mathbf{P}\mathbf{J}+\mathbf{J}^{\top}\mathbf{P}\) is negative semi-definite.

Analytical eigenvalue for the fully normalized circuit

Here we show that when all of the normalization weights in the system are equal, to value \(\alpha\), and the various parameters are scalars, i.e., \(\bm{\tau}_{y}=\tau_{y}\mathbf{1}\), \(\bm{\tau}_{a}=\tau_{a}\mathbf{1}\), \(\mathbf{b}_{0}=b_{0}\mathbf{1}\) and \(\bm{\sigma}=\sigma\mathbf{1}\), we can derive a closed-form analytical expression for all of the eigenvalues. Considering these assumptions, we can break the determinant in Eq. 8 into a diagonal and non-diagonal part as follows,

\[\det(\mathbf{J}-\lambda\mathbf{I})=\det\biggl{(}\lambda^{2}\mathbf{I}+\lambda \biggl{[}\frac{\mathbf{I}}{\tau_{a}}+\frac{\mathbf{D}\left(\sqrt{\mathbf{a}_{s} }\right)}{\tau_{y}}\biggr{]}+\frac{\mathbf{D}\left(\sqrt{\mathbf{a}_{s}}\right) }{\tau_{y}\tau_{a}}-\lambda\frac{\mathbf{W}\mathbf{D}\left(\mathbf{y}_{s}{}^{2} \right)}{\tau_{a}}\biggr{)}\] (55)

Consider the non-diagonal part of the matrix in the determinant, \((\lambda/\tau_{a})\mathbf{W}\mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\). Since \(\mathbf{W}\) is a matrix with all entries equal to a positive constant, \(\alpha\), it is rank 1. Therefore, it can be written as the following outer product,

\[\mathbf{W}=\alpha\begin{bmatrix}1\\ 1\\ \vdots\\ 1\end{bmatrix}[1\quad 1\quad\ldots\quad 1]\] (56)

Therefore, the non-diagonal part of the matrix can be written as,

\[\frac{\lambda}{\tau_{a}}\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{b}^{2} \odot\mathbf{z}^{2}}{\sigma^{2}b_{0}^{2}\mathbf{1}+\mathbf{W}\left(\mathbf{b }^{2}\odot\mathbf{z}^{2}\right)}\right)=\frac{\lambda\alpha}{\tau_{a}} \mathbf{u}\mathbf{v}^{\top}\] (57)

where \(\mathbf{u}=[1,1,...,1]^{\top}\) and \(\mathbf{v}=\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)/\left(\sigma^{2}b_ {0}^{2}\mathbf{1}+\mathbf{W}\left(\mathbf{b}^{2}\odot\mathbf{z}^{2}\right)\right)\). We use the matrix determinant lemma which states that,

\[\det(\mathbf{A}-\gamma\mathbf{u}\mathbf{v}^{\top})=(1-\gamma\mathbf{v}^{\top} \mathbf{A}^{-1}\mathbf{u})\det(\mathbf{A})\] (58)

The matrix \(\mathbf{A}\) is given by,

\[\begin{split}\mathbf{A}&=\lambda^{2}\mathbf{I}+ \lambda\biggl{[}\frac{\mathbf{I}}{\tau_{a}}+\frac{\mathbf{D}\left(\sqrt{ \mathbf{a}_{s}}\right)}{\tau_{y}}\biggr{]}+\frac{\mathbf{D}\left(\sqrt{ \mathbf{a}_{s}}\right)}{\tau_{y}\tau_{a}}\\ &=\lambda^{2}\mathbf{I}+\lambda\left[\frac{\mathbf{I}}{\tau_{a}}+ \frac{\mathbf{D}\left(\sqrt{\sigma^{2}b_{0}^{2}\mathbf{1}+\mathbf{W}\left( \mathbf{b}^{2}\odot\mathbf{z}^{2}\right)}\right)}{\tau_{y}}\right]+\frac{ \mathbf{D}\left(\sqrt{\sigma^{2}b_{0}^{2}\mathbf{1}+\mathbf{W}\left(\mathbf{b }^{2}\odot\mathbf{z}^{2}\right)}\right)}{\tau_{y}\tau_{a}}\\ &=\lambda^{2}\mathbf{I}+\lambda\left[\frac{\mathbf{I}}{\tau_{a}}+ \frac{\mathbf{D}\left(\sqrt{\sigma^{2}b_{0}^{2}\mathbf{1}+\alpha||\mathbf{b} \odot\mathbf{z}||^{2}}\right)}{\tau_{y}\tau_{a}}\right]+\frac{\mathbf{D} \left(\sqrt{\sigma^{2}b_{0}^{2}\mathbf{1}+\alpha||\mathbf{b}\odot\mathbf{z}|| ^{2}}\right)}{\tau_{y}\tau_{a}}\\ &=\left(\lambda^{2}+\lambda\left(\frac{1}{\tau_{a}}+\frac{\sqrt{ \sigma^{2}b_{0}^{2}+\alpha||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}} \right)+\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha||\mathbf{b}\odot\mathbf{z}|| ^{2}}}{\tau_{y}\tau_{a}}\right)\mathbf{I}\end{split}\] (59)

Here, \(||x||\), represents the Euclidean norm of \(x\). Therefore, \(\mathbf{A}=\delta\mathbf{I}\), where \(\delta\) is a quadratic scalar polynomial in \(\lambda\). Now using Eq. 58, we can write

\[\begin{split}\det(\mathbf{J}-\lambda\mathbf{I})&= \left(1-\frac{\lambda\alpha}{\tau_{a}}\mathbf{v}^{\top}\left(\frac{1}{\delta} \mathbf{I}\right)\mathbf{u}\right)\delta^{n}\\ &=\left(\delta-\frac{\lambda}{\tau_{a}}\frac{\alpha||\mathbf{b} \odot\mathbf{z}||^{2}}{\sigma^{2}b_{0}^{2}+\alpha||\mathbf{b}\odot\mathbf{z}|| ^{2}}\right)\delta^{n-1}\end{split}\] (60)

Solving for the eigenvalues, \(\det(\mathbf{J}-\lambda\mathbf{I})=0\), we get \(2(n-1)\) repeated solutions by equation \(\delta^{n-1}=0\), where each solution is found by solving \(\delta=0\),

\[\lambda^{2}+\lambda\left(\frac{1}{\tau_{a}}+\frac{\sqrt{\sigma^{2}b_{0}^{2}+ \alpha||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}}\right)+\frac{\sqrt{\sigma^{ 2}b_{0}^{2}+\alpha||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}\tau_{a}}=0\] (61)

This gives us the following strictly negative eigenvalues,

\[\lambda=-\frac{1}{\tau_{a}}\quad\&\quad\lambda=-\frac{\sqrt{ \sigma^{2}b_{0}^{2}+\alpha||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}}\] (62)The potentially complex eigenvalues are given by solving for zeroes of the first part of the factorized determinant,

\[\lambda^{2}+\lambda\left(\frac{\sigma^{2}b_{0}^{2}}{\tau_{a}(\sigma^{2}b_{0}^{2}+ \alpha||\mathbf{b}\odot\mathbf{z}||^{2})}+\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha ||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}}\right)+\frac{\sqrt{\sigma^{2}b_{0} ^{2}+\alpha||\mathbf{b}\odot\mathbf{z}||^{2}}}{\tau_{y}\tau_{a}}=0\] (63)

The eigenvalues found by solving this equation have negative real parts (as expected) for all choices of parameters since the coefficient of \(\lambda\) and the constant term of the quadratic equation are positive for all choices of parameters and inputs. Therefore, this solution does admit complex roots (thereby oscillations) for certain choices of the parameters which can be found by solving this quadratic equation.

## Appendix D Convergent theorem

**Theorem D.1**.: _A matrix \(\mathbf{A}\) of the form,_

\[\mathbf{A}=\mathbf{D}(\mathbf{t})\,\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{ u}}{\mathbf{v}+\mathbf{W}\mathbf{u}}\right)\] (64)

_is convergent, i.e., its spectral radius is less than one, if \(\mathbf{W}\in\mathbb{R}^{n\times n}\) and \(\mathbf{t},\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}\) with the additional constraints \(0<t_{i}<1\), \(u_{i}\geq 0\), \(v_{i}>0\) and \(w_{ij}\geq 0\) for all \(i,j\)._

Proof.: Assuming the constraints mentioned in the theorem, we first notice that,

\[\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{\mathbf{v}+\mathbf{W}\mathbf{u} }\right)\] (65)

is a nonnegative matrix. Further, because \(\mathbf{D}(\mathbf{t})\) is a positive diagonal matrix with all entries less than 1, we notice that the following is true element-wise,

\[\mathbf{D}(\mathbf{t})\,\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{\mathbf{ v}+\mathbf{W}\mathbf{u}}\right)<\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{ \mathbf{v}+\mathbf{W}\mathbf{u}}\right)\] (66)

We denote the spectral radius of \(\mathbf{A}\), \(\max\{|\lambda|:\lambda=\sigma(\mathbf{A})\}\), by \(\rho(\mathbf{A})\), where \(\sigma(\mathbf{A})\) is the spectrum of \(\mathbf{A}\) and make use of the following inequality for the spectral radius of nonnegative matrices.,

**Theorem D.2**.: _(Theorem 8.1.18 from [91]) Let \(\mathbf{X}\) and \(\mathbf{Y}\) be nonnegative matrices with spectral radius \(\rho(\mathbf{X})\) and \(\rho(\mathbf{Y})\), respectively. If \(\mathbf{X}\leq\mathbf{Y}\) (\(x_{ij}\leq y_{ij},\forall\;i,j\)), then \(\rho(\mathbf{X})\leq\rho(\mathbf{Y})\)._

Therefore, we have,

\[\rho\left(\mathbf{A}\right)\leq\rho\left(\mathbf{W}\,\mathbf{D}\left(\frac{ \mathbf{u}}{\mathbf{v}+\mathbf{W}\mathbf{u}}\right)\right)\] (67)

Now the elements of the matrix are given by,

\[\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{\mathbf{v}+\mathbf{W}\mathbf{u} }\right)=\begin{bmatrix}w_{11}&w_{12}&\ldots\\ w_{21}&w_{22}&\ldots\\ \vdots&\vdots&\ddots\end{bmatrix}\begin{bmatrix}\frac{u_{1}}{v_{1}+\sum_{j}w_ {1j}u_{j}}&0&\ldots\\ 0&\frac{u_{2}}{v_{2}+\sum_{j}w_{2j}u_{j}}&\ldots\\ \vdots&\vdots&\ddots\end{bmatrix}\] (68)

Let \(s_{ij}\) be the \(i,j\) element of this matrix. Upon multiplication of the matrices above, we find that,

\[s_{ij}=\frac{w_{ij}u_{j}}{v_{j}+\sum_{k}w_{jk}u_{k}}\] (69)

Now we use the following theorem that provides an upper bound for the spectral radius based on the entries of the matrix.

**Theorem D.3**.: _(Theorem 8.1.26 from [91]) Let \(\mathbf{S}\) be a nonnegative matrix. Then for any positive vector \(\mathbf{x}\in\mathbb{R}^{n}\) with entries \(x_{j}\), we have,_

\[\rho(\mathbf{S})\leq\max_{1\leq i\leq n}\frac{1}{x_{i}}\sum_{j=1}^{n}s_{ij}x_{j}\] (70)We pick \(x_{j}=v_{j}+\sum_{k}w_{jk}u_{k}\), which is a positive vector, and apply the theorem to the matrix. Substituting \(s_{ij}\) and \(x_{j}\), we get the following bound for the spectral radius,

\[\rho\left(\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{\mathbf{v}+\mathbf{W} \mathbf{u}}\right)\right)\leq\max_{1\leq i\leq n}\frac{1}{v_{i}+\sum_{k}w_{ik} u_{k}}\sum_{j=1}^{n}\frac{w_{ij}u_{j}}{v_{j}+\sum_{k}w_{jk}u_{k}}\left(v_{j}+\sum_{k}w_{ jk}u_{k}\right)\] (71)

Upon simplification, we get,

\[\rho\left(\mathbf{W}\,\mathbf{D}\left(\frac{\mathbf{u}}{\mathbf{v}+\mathbf{W} \mathbf{u}}\right)\right)\leq\max_{1\leq i\leq n}\frac{\sum_{j}w_{ij}u_{j}}{v_ {i}+\sum_{k}w_{ik}u_{k}}=\max_{1\leq i\leq n}\frac{\sum_{k}w_{ik}u_{k}}{v_{i}+ \sum_{k}w_{ik}u_{k}}<1\] (72)

The inequality is true, because, for all \(i\), the denominator is larger than the numerator because of the extra positive term \(v_{i}\). Therefore,

\[\rho\left(\mathbf{A}\right)\leq\rho\left(\mathbf{W}\,\mathbf{D}\left(\frac{ \mathbf{u}}{\mathbf{v}+\mathbf{W}\mathbf{u}}\right)\right)<1\] (73)

and \(\mathbf{A}\) is a convergent matrix. 

## Appendix E Linear stability analysis of the two-dimensional model

Here we consider the dynamical stability of the 2D model containing one neuron each of \(y\) and \(a\) when the recurrent scalar, \(w_{r}\) can take any positive value. The dynamical system to be analyzed is given by,

\[\tau_{y}\dot{y} =-y+bz+\left(1-\sqrt{\lfloor a\rfloor}\right)w_{r}y\] (74) \[\tau_{a}\dot{a} =-a+b_{0}^{2}\sigma^{2}+w\hat{y}^{2}\lfloor a\rfloor\]

with a positive real constraint on the following set of parameters, \(\tau_{y},\tau_{a},b,b_{0},\sigma,w\). We first notice the symmetry in the dynamical system about \(y=0\). Replacing, \(\dot{y}\to-y\), we get the mirrored dynamical system,

\[\tau_{\dot{y}}\dot{\hat{y}} =-\dot{y}+b(-z)+\left(1-\sqrt{\lfloor a\rfloor}\right)w_{r}\dot{y}\] (75) \[\tau_{a}\dot{a} =-a+b_{0}^{2}\sigma^{2}+w\hat{y}^{2}\lfloor a\rfloor\]

These equations are the same as Eq. 74, up to a sign change in \(z\). Therefore, we can derive analogous conditions for stability for \(z<0\) once the conditions for \(z>0\) are known.

The steady-state of Eq. 74, \((y_{s},a_{s})\) satisfies the following equations,

\[y_{s} =bz+\left(1-\sqrt{\lfloor a_{s}\rfloor}\right)w_{r}y_{s}\] (76) \[a_{s} =b_{0}^{2}\sigma^{2}+wy_{s}^{2}\lfloor a_{s}\rfloor\] (77)

Since the RHS of the second equation is always positive, if a root exists, we have \(a_{s}>0\). Therefore, we can remove the rectification around \(a_{s}\) and look for positive solutions for \(a_{s}\). Upon rearranging the terms, we get,

\[\left(1-w_{r}+w_{r}\sqrt{a_{s}}\right)y_{s} =bz\] (78) \[\left(1-wy_{s}^{2}\right)a_{s} =b_{0}^{2}\sigma^{2}\] (79)

Substituting \(y_{s}\) from Eq. 78 in Eq. 79, we get the following quartic equation in \(m=\sqrt{a_{s}}\),

\[w_{r}{}^{2}m^{4}+2(1-w_{r})w_{r}m^{3}+ \left((1-w_{r})^{2}-wb^{2}z^{2}-b_{0}^{2}\sigma^{2}w_{r}{}^{2} \right)m^{2}\] (80) \[-2(1-w_{r})w_{r}b_{0}^{2}\sigma^{2}m-(1-w_{r})^{2}b_{0}^{2}\sigma ^{2}=0\]

For a valid fixed point \((y_{s},a_{s})\), i.e., \(y_{s}\in\mathbb{R}\) and \(a_{s}\in\mathbb{R}_{+}^{+}\), a necessary condition is that \(a_{s}\) must be positive and real, which in turn implies \(m\) must be positive and real.

**Theorem E.1**.: _A fixed point, \((y_{s},a_{s})\), is valid if and only if it satisfies \(\sqrt{a_{s}}>0\)._Proof.: Due to Lemma E.2, we have: a fixed point, \((y_{s},a_{s})\), is valid if and only if it satisfies \(m>b_{0}\sigma\). Further due to Lemma E.3, we have that the conditions \(m>0\) and \(m>b_{0}\sigma\) are equivalent. Combining these two we get the statement of the theorem. 

**Lemma E.2**.: _A fixed point, \((y_{s},a_{s})\), is valid if and only if it satisfies \(\sqrt{a_{s}}>b_{0}\sigma\)._

Proof.: \(\implies\) Given, \(m=\sqrt{a_{s}}>b_{0}\sigma\), we have \(a_{s}>b_{0}^{2}\sigma^{2}\), therefore, \(a_{s}\in\mathbb{R}_{*}^{+}\); and \(y_{s}=\sqrt{m^{2}-b_{0}^{2}\sigma^{2}}/(m\sqrt{w})\), therefore, \(y_{s}\in\mathbb{R}\iff\) Given \(a_{s}\in\mathbb{R}_{*}^{+}\) and \(y_{s}\in\mathbb{R}\), implies \(m\in\mathbb{R}_{*}^{+}\). Now, Eq. 79 posits that, \(m=b_{0}\sigma/\sqrt{1-wy_{s}^{2}}\), therefore, \(m>b_{0}\sigma\). 

**Lemma E.3**.: _For a positive real root, \(m=\sqrt{a_{s}}\), to the quartic equation (Eq. 80), the condition \(m>0\) is equivalent to the condition \(m>b_{0}\sigma\). Further, no fixed point satisfies \(0<m<b_{0}\sigma\)._

Proof.: \(\implies\) Given \(m>b_{0}\sigma\), we have \(m>0\) because \(b_{0}>0\) and \(\sigma>0\). \(\iff\) Since \(m\) is a root of the quartic, it also satisfies Eq. 79. Therefore, we have \(m=b_{0}\sigma/\sqrt{1-wy_{s}^{2}}\). Since we are given \(m>0\), this implies that \(0<1-wy_{s}^{2}<1\), or \(m>b_{0}\sigma\). Therefore, \(m>0\) is equivalent to \(m>b_{0}\sigma\). This also implies that there exists no fixed point that satisfies \(0<m<b_{0}\sigma\). 

Further, the Jacobian matrix at the fixed point of the dynamical system, in terms of the parameters and the fixed point is given by,

\[\mathbf{J}=\begin{bmatrix}\frac{w_{r}-1-w_{r}\sqrt{a_{s}}}{\tau_{y}}&-\frac{w _{r}y_{s}}{2\sqrt{a_{s}}\tau_{y}}\\ \frac{2w_{a}y_{s}}{\tau_{a}}&\frac{-1+wy_{s}^{2}}{\tau_{a}}\end{bmatrix}\] (81)

From the linear stability theory, we know that a fixed point \((y_{s},a_{s})\) is asymptotically stable when the real part of the eigenvalues of \(\mathbf{J}\) are less than \(0\), i.e., \(\mathrm{Re}(\lambda\mathbf{J})<0\). For a 2D system, this is equivalent to the conditions: \(\mathrm{Tr}(\mathbf{J})<0\) and \(\det(\mathbf{J})>0\). The trace of the Jacobian matrix is given by,

\[\begin{split}\mathrm{Tr}(\mathbf{J})&=\frac{w_{r}-1-w _{r}\sqrt{a_{s}}}{\tau_{y}}+\frac{-1+wy_{s}^{2}}{\tau_{a}}\\ &=-\left(\frac{1-w_{r}+w_{r}\sqrt{a_{s}}}{\tau_{y}}+\frac{1-wy_{s} ^{2}}{\tau_{a}}\right)\\ &=-\left(\frac{1-w_{r}+w_{r}\sqrt{a_{s}}}{\tau_{y}}+\frac{b_{0}^{ 2}\sigma^{2}}{a_{s}\tau_{a}}\right)\end{split}\] (82)

The determinant of the Jacobian matrix is given by,

\[\begin{split}\det(\mathbf{J})&=\left(\frac{w_{r}-1-w _{r}\sqrt{a_{s}}}{\tau_{y}}\right)\left(\frac{-1+wy_{s}^{2}}{\tau_{a}}\right)+ \left(\frac{w_{r}y_{s}}{2\sqrt{a_{s}}\tau_{y}}\right)\left(\frac{2wa_{s}y_{s} }{\tau_{a}}\right)\\ &=\frac{(1-w_{r})(1-wy_{s}^{2})}{\tau_{a}\tau_{y}}+\frac{w_{r} \sqrt{a_{s}}}{\tau_{a}\tau_{y}}\\ &=\frac{1}{\tau_{y}\tau_{a}}\left(\frac{(1-w_{r})b_{0}^{2}\sigma ^{2}}{a_{s}}+w_{r}\sqrt{a_{s}}\right)\end{split}\] (83)

Now we consider the different conditions on \(w_{r}\) and the input drive \(z\) for stability and state the various cases as theorems along with their proofs,

### Contractive constraint on recurrence (\(0<w_{r}\leq 1\))

* \(z>0:\)_There exists a unique fixed point with \(y_{s}>0\) and \(a_{s}>0\) and it is asymptotically stable._ Proof.: The existence and uniqueness of the fixed point, along with its stability properties, are established by Lemma E.4. Additionally, the positivity of \(a_{s}\) (i.e., \(a_{s}>0\)) ensures that the expression \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\). Consequently, given that \(z>0\), it follows from Eq. 78 that \(y_{s}>0\)* \(z<0:\)_There exists a unique fixed point with_ \(y_{s}<0\) _and_ \(a_{s}>0\) _and it is asymptotically stable._

Proof.: Since this condition becomes equivalent to \(z>0\), up to a sign change in \(y\) (Eq. 75), it is straightforward to see why this is true. 

**Lemma E.4**.: _Given \(0<w_{r}\leq 1\) and \(z\in\mathbb{R}\), there exists a unique fixed point and it is asymptotically stable._

Proof.: We observe that given \(0<w_{r}\leq 1\), if there exists a valid fixed point, which satisfies \(a_{s}>0\), \(\operatorname{Tr}(\mathbf{J})\) in Eq. 82 is less than 0 (since \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\) for any \(\sqrt{a_{s}}>0\)), and \(\det(\mathbf{J})\) in Eq. 83 is greater than \(0\) for all combinations of parameters and \(z\in\mathbb{R}\). Therefore, we need to find the constraints on the parameters that allow for at least one fixed point. The fixed point, \(m=\sqrt{a_{s}}\), satisfies the quartic polynomial in Eq 80. Due to Theorem E.1, for a valid fixed point, we need to find positive real roots that satisfy \(m>0\).

The sequence of signs of the coefficients of the polynomial when \(0\leq w_{r}<1\) is given by \((+,+,\pm,-,-)\). We use _Descartes' Rule of Signs_, which states the following: The number of positive real roots of a polynomial \(p(x)\) is either equal to the number of sign changes (omitting the zero coefficients) between consecutive non-zero coefficients of \(p(x)\), or it is less than this by a multiple of 2. Since there is exactly one sign change from left to right in the sequence, regardless of the sign of the coefficient of \(m^{2}\), we know that the equation above has exactly one real positive root for \(m\). This further implies that \(a_{s}\) has exactly one positive root and the corresponding fixed point \((y_{s},a_{s})\), is locally dynamically stable for all of the combinations of parameters and \(z\in\mathbb{R}\).

Note that the result also holds for \(w_{r}=1\). There is exactly one positive fixed point \((y_{s},a_{s})\) and it is given by, i.e., \(y_{s}=bz/\sqrt{b_{0}^{2}\sigma^{2}+wb^{2}z^{2}}\) and \(a_{s}=b_{0}^{2}\sigma^{2}+wb^{2}z^{2}\). 

### Expansive constraint on recurrence (\(w_{r}>1\))

Now we consider the case when \(w_{r}>1\). We first find an alternative form of the determinant of the Jacobian (Eq.83) that is more amenable to the case, \(w_{r}>1\),

\[\begin{split}\det(\mathbf{J})&=\frac{1}{\tau_{y} \tau_{a}}\left((1-w_{r})(1-wy_{s}^{2})+w_{r}\sqrt{a_{s}}\right)\\ &=\frac{1}{\tau_{y}\tau_{a}}\left(1-w_{r}+w_{r}\sqrt{a_{s}}+(w_{r }-1)wy_{s}^{2}\right)\end{split}\] (84)

Rewriting the trace,

\[\operatorname{Tr}(\mathbf{J})=-\left(\frac{1-w_{r}+w_{r}\sqrt{a_{s}}}{\tau_{y }}+\frac{b_{0}^{2}\sigma^{2}}{a_{s}\tau_{a}}\right)\] (85)

The properties of stability can be summarized in the following two cases,

* \(z>0:\)_There exists exactly one fixed point with_ \(y_{s}>0\) _and_ \(a_{s}>0\) _and it is asymptotically stable. Further, if_ \(b_{0}\sigma>1-1/w_{r}\)_, there exist no additional fixed points. But if_ \(b_{0}\sigma<1-1/w_{r}\)_, then there exist either two or no fixed points with_ \(y_{s}<0\) _and_ \(a_{s}>0\) _and they may or may not be stable._

Proof.: Since the fixed point satisfies Eq. 78, there are two possibilities, either \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\) or \(1-w_{r}+w_{r}\sqrt{a_{s}}<0\). We consider them separately,

* \(1-w_{r}+w_{r}\sqrt{a_{s}}>0:\) If \(z>0\), we have \(y_{s}>0\) because \(y_{s}\) satisfies Eq. 78. Further due to Lemma E.5, this fixed point is unique and asymptotically stable for all combinations of parameters.
* \(1-w_{r}+w_{r}\sqrt{a_{s}}<0:\) If \(z>0\), we have \(y_{s}<0\) because \(y_{s}\) satisfies Eq. 78. Further, if we are given \(b_{0}\sigma>1-1/w_{r}\), no root exists. We prove this by contradiction, assuming a root exists that satisfies \(\sqrt{a_{s}}<1-1/w_{r}\), since \(b_{0}\sigma>1-1/w_{r}\), this root satisfies \(0<\sqrt{a_{s}}<b_{0}\sigma\), but due to Lemma E.3 no such root exists, therefore we have a contradiction. Now if we have \(b_{0}\sigma<1-1/w_{r}\), the root for \(m=\sqrt{a_{s}}\) must satisfy \(b_{0}\sigma<m<1-1/w_{r}\). Since either one or three roots satisfy \(m>0\) (equivalently \(m>b_{0}\sigma\)) in Eq. 80 and exactly one root satisfies \(m>1-1/w_{r}\) (Lemma E.5), we conclude that the number of roots for \(m\) in the interval \((b_{0}\sigma,1-1/w_{r})\) are either two or none. 
* \(z<0:\) _There exists exactly one fixed point with_ \(y_{s}<0\) _and_ \(a_{s}>0\) _and it is asymptotically stable. Further, if_ \(b_{0}\sigma>1-1/w_{r}\)_, there exist no additional fixed points. But if_ \(b_{0}\sigma<1-1/w_{r}\)_, then there exist either two or no fixed points with_ \(y_{s}>0\) _and_ \(a_{s}>0\) _and they may or may not be stable._ _Proof._ Since this condition becomes equivalent to \(z>0\), up to a sign change in \(y\) (Eq. 75), it is straightforward to see why this is true. 

**Lemma E.5**.: _Given \(w_{r}>1\), there exists a unique fixed point, \((y_{s},a_{s})\), that satisfies \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\) and it is asymptotically stable._

Proof.: The fixed point, \(m=\sqrt{a_{s}}\), satisfies the quartic polynomial in Eq 80. Due to Theorem E.1, for a valid fixed point, we need to find positive real roots that satisfy \(m>0\).

The sequence of signs of the coefficients of the polynomial is given by \((+,-,\pm,+,-)\). Regardless of the sign of the coefficient of \(m^{2}\), using _Descartes' Rule of Signs_, we find that since there are \(3\) sign changes, there are either \(1\) or \(3\) positive real roots for \(m\). Since, we are given \(w_{r}>1\) and \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\), these roots are valid only if they satisfy \(1-w_{r}+w_{r}m>0\), or, \(m>1-1/w_{r}\). We make a transformation \(m\to\hat{m}+(1-1/w_{r})\) which gives us,

\[\begin{split} w_{r}{}^{2}\hat{m}^{4}-2(1-w_{r})w_{r}\hat{m}^{3}& +\left((1-w_{r})^{2}-wb^{2}z^{2}-b_{0}^{2}\sigma^{2}w_{r}{}^{2} \right)\hat{m}^{2}\\ &-2wb^{2}z^{2}(1-1/w_{r})\hat{m}-wb^{2}z^{2}(1-1/w_{r})^{2}=0\end{split}\] (86)

The number of valid roots is given by the number of positive real roots of this polynomial. The signs of the coefficients are given by, \((+,+,\pm,-,-)\). There is exactly one sign change, hence there is a unique fixed point that satisfies \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\).

Now we prove that this fixed point is asymptotically stable. It can be easily seen that \(\operatorname{Tr}(\mathbf{J})<0\) in Eq. 85 and \(\det(\mathbf{J})>0\) in Eq. 84 when \(1-w_{r}+w_{r}\sqrt{a_{s}}>0\) and \(w_{r}>1\). Therefore, this unique fixed point is asymptotically stable. 

## Appendix F Analysis of the Rectified model

Here, we present the stability results for the model with only the positive part of the complementary receptive fields present. The model is given by the following dynamical equations,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\mathbf{a }^{+}\right)\odot\left\lfloor\mathbf{W}_{r}\mathbf{y}\right\rfloor\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=-\mathbf{ a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \mathbf{y}^{+}\odot\mathbf{a}^{+2}\right)\end{split}\] (87)

We will follow the same procedure for stability analysis as we did for the main model and state the key steps in the various derivations for stability.

### Stability of the high-dimensional system

We analyze the stability of the system when \(\mathbf{W}_{r}=\mathbf{I}\). Upon substituting \(\mathbf{y}^{+}\to\lfloor\mathbf{y}\rfloor^{2}\) and \(\mathbf{a}^{+}\to\sqrt{\lfloor\mathbf{a}\rfloor}\), we get the following dynamical system,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\sqrt{ \lfloor\mathbf{a}\rfloor}\right)\odot\left\lfloor\mathbf{y}\right\rfloor\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=-\mathbf{ a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \lfloor\mathbf{y}\rfloor^{2}\odot\lfloor\mathbf{a}\rfloor\right)\end{split}\] (88)

The fixed point is given by,

\[\begin{split}\mathbf{y}_{s}&=\frac{\lfloor\mathbf{ b}\odot\mathbf{z}\rfloor}{\sqrt{\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+ \mathbf{W}\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}}}-\lfloor-\mathbf{b} \odot\mathbf{z}\rfloor\\ \mathbf{a}_{s}&=\mathbf{b}_{0}^{2}\odot\boldsymbol{ \sigma}^{2}+\mathbf{W}\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}\end{split}\] (89)Also, \(\lfloor\bm{y}_{s}\rfloor\) is given by,

\[\lfloor\bm{y}_{s}\rfloor=\frac{\lfloor\mathbf{b}\odot\mathbf{z}\rfloor}{\sqrt{ \mathbf{b}_{0}^{2}\odot\bm{\sigma}^{2}+\mathbf{W}\lfloor\mathbf{b}\odot\mathbf{ z}\rfloor^{2}}}\] (90)

The Jacobian matrix, \(\mathbf{J}\), about this fixed point is given by,

\[\mathbf{J}=\begin{bmatrix}-\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}\odot \mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}\leq 0}}}{\bm{\tau}_{y}} \right)&-\mathbf{D}\left(\frac{\lfloor\bm{y}_{s}\rfloor}{2\odot\sqrt{\mathbf{ a}_{s}\odot\bm{\tau}_{y}}}\right)\\ \mathbf{D}\left(\frac{\mathbf{2}}{\bm{\tau}_{a}}\right)\mathbf{W}\,\mathbf{D} \left(\mathbf{a}_{s}\odot\lfloor\bm{y}_{s}\rfloor\right)&\mathbf{D}\left( \frac{\mathbf{1}}{\bm{\tau}_{a}}\right)\left(-\mathbf{I}+\mathbf{W}\,\mathbf{D }\left(\lfloor\bm{y}_{s}\rfloor^{2}\right)\right)\end{bmatrix}\] (91)

Here \(\mathbf{1}_{\bm{x}\geq 0}\) is an indicator function that returns a vector the size of \(\mathbf{z}\) with \(1\) at locations where \(z_{i}\geq 0\) and \(0\) elsewhere. Similarly, \(\mathbf{1}_{\bm{x}\leq 0}\) returns a vector the size of \(\mathbf{z}\) with \(1\) at locations where \(z_{i}<0\) and \(0\) elsewhere. Further \(\mathbf{J}-\lambda\mathbf{I}\) is given by,

\[\mathbf{J} =\begin{bmatrix}\mathbf{A}_{11}&\mathbf{A}_{12}\\ \mathbf{A}_{21}&\mathbf{A}_{22}\end{bmatrix}\] (92) \[=\begin{bmatrix}-\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}\odot \mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}\leq 0}}}{\bm{\tau}_{y}}\right)- \lambda\mathbf{I}&-\mathbf{D}\left(\frac{\lfloor\bm{y}_{s}\rfloor}{2\odot \sqrt{\mathbf{a}_{s}\odot\bm{\tau}_{y}}}\right)\\ \mathbf{D}\left(\frac{\mathbf{2}}{\bm{\tau}_{a}}\right)\mathbf{W}\,\mathbf{D} \left(\mathbf{a}_{s}\odot\lfloor\bm{y}_{s}\rfloor\right)&\mathbf{D}\left( \frac{\mathbf{1}}{\bm{\tau}_{a}}\right)\left(-\mathbf{I}+\mathbf{W}\,\mathbf{D }\left(\lfloor\bm{y}_{s}\rfloor^{2}\right)\right)-\lambda\mathbf{I}\end{bmatrix}\]

Since \(\mathbf{A}_{11}\) and \(\mathbf{A}_{12}\) commute, we can write, \(\det(\mathbf{J}-\lambda\mathbf{I})=\det(\mathbf{A}_{22}\mathbf{A}_{11}- \mathbf{A}_{21}\mathbf{A}_{12})\). Upon simplification, we get,

\[\det(\mathbf{J}-\lambda\mathbf{I})= \det\biggl{(}\lambda^{2}\mathbf{I}+\lambda\biggl{[}\mathbf{D} \left(\frac{\mathbf{1}}{\bm{\tau}_{a}}\right)+\mathbf{D}\left(\frac{\sqrt{ \mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}<0}}{\bm{ \tau}_{y}}\right)\] (93) \[-\mathbf{D}\left(\frac{\mathbf{1}}{\bm{\tau}_{a}}\right)\mathbf{ W}\mathbf{D}\left(\lfloor\bm{y}_{s}\rfloor^{2}\right)\biggr{]}+\mathbf{D}\left( \frac{\sqrt{\mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}<0}}{\bm{ \tau}_{y}\odot\bm{\tau}_{a}}\right)\biggr{)}\]

Note that, here we used the fact that \(\bm{y}_{s}\) has the same sign as \(\mathbf{z}\), as seen from Eq. 89. The dynamical system is stable if all eigenvalues of this characteristic polynomial have negative real parts. Just like the main model, we map this to a quadratic eigenvalue problem of the form \(\mathcal{L}(\lambda)=\det(\lambda^{2}\mathbf{I}+\lambda\mathbf{B}+\mathbf{K})=0\). Now, we see if the conditions of Theorem 4.1 are met. The stiffness matrix is given by,

\[\mathbf{K}=\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x }\geq 0}+\mathbf{1}_{\bm{x}<0}}{\bm{\tau}_{y}\odot\bm{\tau}_{a}}\right)=\mathbf{D} \left(\frac{\sqrt{\mathbf{b}_{0}^{2}\odot\bm{\sigma}^{2}+\mathbf{W}\lfloor \mathbf{b}\odot\mathbf{z}\rfloor^{2}}\odot\mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}<0}}{\bm{ \tau}_{y}\odot\bm{\tau}_{a}}\right)\] (94)

Clearly, \(\mathbf{K}\) is a positive diagonal matrix for all choices of parameters and input. Now we check if the \(\mathbf{B}\) is a Lyapunov diagonally stable matrix, which is implied when \(\mathbf{B}\) admits a _regular convergent splitting_, i.e., it has a representation of the form \(\mathbf{B}=\mathbf{M}-\mathbf{N}\), where \(\mathbf{M}^{-1}\) and \(\mathbf{N}\) have all nonnegative entries and \(\mathbf{M}^{-1}\mathbf{N}\) has a spectral radius smaller than 1.

\[\mathbf{B}=\underbrace{\mathbf{D}\left(\frac{\mathbf{1}}{\bm{\tau}_{a}}\right) +\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x}\geq 0}+ \mathbf{1}_{\bm{x}<0}}{\bm{\tau}_{y}}\right)}_{\mathbf{M}}-\underbrace{ \mathbf{D}\left(\frac{\mathbf{1}}{\bm{\tau}_{a}}\right)\mathbf{W}\mathbf{D} \left(\lfloor\bm{y}_{s}\rfloor^{2}\right)}_{\mathbf{N}}\] (95)

Since \(\mathbf{M}\) is a positive diagonal matrix, all the entries of \(\mathbf{M}^{-1}\) are nonnegative. Also, since all the matrices involved in the definition of \(\mathbf{N}\) are nonnegative, therefore their product, \(\mathbf{N}\), is nonnegative. We are only left to prove that \(\mathbf{M}^{-1}\mathbf{N}=\mathbf{S}\) has a spectral radius smaller than 1. Consider the matrix \(\mathbf{S}\),

\[\mathbf{S}=\mathbf{D}\left(\frac{\mathbf{1}}{\mathbf{1}+\left(\bm{\tau}_{a}/ \bm{\tau}_{y}\right)\odot\left(\sqrt{\mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x} \geq 0}+\mathbf{1}_{\bm{x}<0}\right)}\right)\mathbf{W}\mathbf{D}\left(\frac{ \lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}}{\mathbf{b}_{0}^{2}\odot\bm{ \sigma}^{2}+\mathbf{W}\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}}\right)\] (96)

Theorem D puts a bound of \(1\) on the spectral radius of this matrix. Define \(\mathbf{t}\to\mathbf{1}/\left(\mathbf{1}+\left(\bm{\tau}_{a}/\bm{\tau}_{y} \right)\odot\left(\sqrt{\mathbf{a}_{s}}\odot\mathbf{1}_{\bm{x}\geq 0}+\mathbf{1}_{\bm{x}<0} \right)\right)\), \(\mathbf{u}\to\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}\) and \(\mathbf{v}\to\mathbf{b}_{0}^{2}\odot\bm{\sigma}^{2}\), we notice that they follow the constraints of the theorem, therefore, \(\mathbf{S}\) is convergent. This implies that \(\mathbf{B}\) has a _convergent regular splitting_, therefore, the linearized dynamical system is unconditionally globally asymptotically stable (and nonlinear dynamical system is locally asymptotically stable) across all the values of parameters and inputs.

### Analytical eigenvalue for fully normalized circuit

Following a procedure similar to that in Appendix C, when all of the normalization weights in the system are equal, to value \(\alpha\), and the various parameters are scalars, i.e., \(\boldsymbol{\tau}_{y}=\tau_{y}\mathbf{1}\), \(\boldsymbol{\tau}_{a}=\tau_{a}\mathbf{1}\), \(\mathbf{b}_{0}=b_{0}\mathbf{1}\) and \(\boldsymbol{\sigma}=\sigma\mathbf{1}\), we can write the analytical expressions for eigenvalues. The characteristic polynomial is given by,

\[\det(\mathbf{J}-\lambda\mathbf{I})=\left(1-\frac{\lambda\alpha}{\tau_{a}} \mathbf{v}^{\top}\mathbf{D}\left(\frac{\mathbf{1}}{\delta_{1}\mathbf{1}_{ \mathbf{z}\geq 0}+\delta_{2}\mathbf{1}_{\mathbf{z}<0}}\right)\mathbf{u} \right)\delta_{1}^{n_{1}}\delta_{2}^{n_{2}}\] (97)

where, \(\mathbf{u}=[1,1,...,1]^{\top}\), \(\mathbf{v}=\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}/\left(\sigma^{2}b_{0}^ {2}\mathbf{1}+\mathbf{W}\lfloor\mathbf{b}\odot\mathbf{z}\rfloor^{2}\right)\); \(n_{1}\) and \(n_{2}\) are the number of nonnegative and negative values, respectively, in the input drive \(\mathbf{z}\); \(\delta_{1}\) and \(\delta_{2}\) are given by,

\[\begin{split}&\delta_{1}=\lambda^{2}+\lambda\left(\frac{1}{\tau_ {a}}+\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha||\lfloor\mathbf{b}\odot\mathbf{z} \rfloor||^{2}}}{\tau_{y}}\right)+\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha|| \lfloor\mathbf{b}\odot\mathbf{z}\rfloor||^{2}}}{\tau_{y}\tau_{a}}\\ &\delta_{2}=\lambda^{2}+\lambda\left(\frac{1}{\tau_{a}}+\frac{1}{ \tau_{y}}\right)+\frac{1}{\tau_{y}\tau_{a}}\end{split}\] (98)

Simplification of Eq. 97 gives us,

\[\det(\mathbf{J}-\lambda\mathbf{I})=\left(\delta_{1}-\frac{\lambda}{\tau_{a}} \frac{\alpha||\lfloor\mathbf{b}\odot\mathbf{z}\rfloor||^{2}}{\sigma^{2}b_{0} ^{2}+\alpha||\lfloor\mathbf{b}\odot\mathbf{z}\rfloor||^{2}}\right)\delta_{1} ^{n_{1}-1}\delta_{2}^{n_{2}}\] (99)

Since the characteristic polynomial is a product of quadratic polynomials, we can solve them analytically. The strictly negative eigenvalues are given by,

\[\lambda=-\frac{1}{\tau_{a}};\quad\lambda=-\frac{1}{\tau_{y}}\quad\& \quad\lambda=-\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha||\lfloor\mathbf{b}\odot \mathbf{z}\rfloor||^{2}}}{\tau_{y}}\] (100)

The potentially complex eigenvalues are given by the solution to the following quadratic equation,

\[\begin{split}\lambda^{2}+\lambda\left(\frac{\sigma^{2}b_{0}^{2}} {\tau_{a}(\sigma^{2}b_{0}^{2}+\alpha||\lfloor\mathbf{b}\odot\mathbf{z}\rfloor ||^{2})}+\frac{\sqrt{\sigma^{2}b_{0}^{2}+\alpha||\lfloor\mathbf{b}\odot \mathbf{z}\rfloor||^{2}}}{\tau_{y}}\right)+\frac{\sqrt{\sigma^{2}b_{0}^{2}+ \alpha||\lfloor\mathbf{b}\odot\mathbf{z}\rfloor||^{2}}}{\tau_{y}\tau_{a}}=0 \end{split}\] (101)

### Linear stability analysis of the two-dimensional model

The dynamical system to consider is,

\[\tau_{y}\dot{y} =-y+bz+\left(1-\sqrt{\lfloor a\rfloor}\right)\lfloor w_{r}y\rfloor\] (102) \[\tau_{a}\dot{a} =-a+b_{0}^{2}\sigma^{2}+w\lfloor y\rfloor^{2}\lfloor a\rfloor\]

Since a valid fixed point must have \(a_{s}>0\), the fixed point \((y_{s},a_{s})\) satisfies,

\[(1-w_{r}+w_{r}\sqrt{a_{s}})\left\lfloor y_{s}\right\rfloor- \lfloor-y_{s}\rfloor =bz\] (103) \[(1-w\lfloor y_{s}\rfloor^{2})\,a_{s} =b_{0}^{2}\sigma^{2}\] (104)

We divide this into two cases,

* \(y_{s}>0:\) The fixed point is given by the equations, \[(1-w_{r}+w_{r}\sqrt{a_{s}})\,y_{s} =bz\] (105) \[(1-wy_{s}^{2})\,a_{s} =b_{0}^{2}\sigma^{2}\] (106) A fixed point, \((y_{s},a_{s})\), is valid only if it satisfies \(y_{s}\in\mathbb{R}_{*}^{+}\) and \(a_{s}\in\mathbb{R}_{*}^{+}\). The Jacobian matrix is given by, \[\mathbf{J}=\begin{bmatrix}\frac{w_{r}-1-w_{r}\sqrt{a_{s}}}{\tau_{y}}&-\frac{w _{r}y_{s}}{2\sqrt{a_{s}}\tau_{y}}\\ \frac{2w_{a}y_{s}}{\tau_{a}}&\frac{-1+wy_{s}^{2}}{\tau_{a}}\end{bmatrix}\] (107) Note that this is equivalent to the main model, with the additional constraint of \(y_{s}>0\), whose stability analysis is presented in Appendix E.

* \(y_{s}<0:\) Eq. 103 & 104 yield us a unique fixed point \(y_{s}=bz\) and \(a_{s}=b_{0}^{2}\sigma^{2}\). Since \(y_{s}<0\) and \(y_{s}=bz\), this is only possible when \(z<0\). The Jacobian matrix about \((y_{s},a_{s})\) is given by, \[\mathbf{J}=\begin{bmatrix}-\frac{1}{\tau_{y}}&0\\ 0&-\frac{1}{\tau_{a}}\end{bmatrix}\] (108) Since the eigenvalues of \(\mathbf{J}\) are \(\lambda_{\mathbf{J}}=-1/\tau_{y},-1/\tau_{a}\) are real and both negative, this fixed point is always stable.

Combining the cases above and results already established in Appendix E, we characterize the stability of the system as follows,

#### f.3.1 Contractive constraint on recurrence (\(0<w_{r}\leq 1\))

* \(z>0:\)_There exists a unique fixed point with_ \(y_{s}>0\) _and_ \(a_{s}>0\) _and it is asymptotically stable._
* \(z<0:\)_There exists a unique fixed point with_ \(y_{s}<0\) _and_ \(a_{s}>0\)_, given by,_ \((bz,b_{0}^{2}\sigma^{2})\)_, and it is asymptotically stable._

#### f.3.2 Expansive constraint on recurrence (\(w_{r}>1\))

* \(z>0:\)_There exists a unique fixed point with_ \(y_{s}>0\) _and_ \(a_{s}>0\) _and it is asymptotically stable._
* \(z<0:\)_There exists exactly one fixed point with_ \(y_{s}<0\) _and_ \(a_{s}>0\) _given by,_ \((bz,b_{0}^{2}\sigma^{2})\)_, and it is asymptotically stable. Further, if_ \(b_{0}\sigma>1-1/w_{r}\)_, there exist no additional fixed points. But if_ \(b_{0}\sigma<1-1/w_{r}\)_, then there exist either two or no fixed points with_ \(y_{s}>0\) _and_ \(a_{s}>0\) _and they may or may not be stable._

## Appendix G Iterative algorithm

In this section, we present an iterative approach to finding the fixed point for ORGaNICs with an arbitrary recurrent weight matrix. We show that this algorithm converges in a few steps (2-10) with great accuracy. We consider the system given by Eq. 15,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\sqrt{ \left\lfloor\mathbf{a}\right\rfloor}\right)\odot\left(\mathbf{W}_{r} \mathbf{y}\right)\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=- \mathbf{a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \mathbf{y}^{2}\odot\left\lfloor\mathbf{a}\right\rfloor\right)\end{split}\] (109)

The fixed point of this system (\(\mathbf{y}_{s}\) and \(\mathbf{a}_{s}\)) is found by solving the following simultaneous equations,

\[\mathbf{y}_{s}=\mathbf{b}\odot\mathbf{z}+\left(\mathbf{1}-\sqrt{\mathbf{a}_{ s}}\right)\odot\left(\mathbf{W}_{r}\mathbf{y}_{s}\right)\] (110)

\[\mathbf{a}_{s}=\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\, \left(\mathbf{y}_{s}^{2}\odot\mathbf{a}_{s}\right)\] (111)

These equations do not admit a closed-form analytical solution when \(\mathbf{W}_{r}\neq\mathbf{I}\). We first find a good approximation for the initialization of \(\mathbf{y}_{s}\) and \(\mathbf{a}_{s}\) and then define the iterative algorithm. The equation for \(\mathbf{y}_{s}\) can be written in terms of \(\mathbf{a}_{s}\) as,

\[\mathbf{y}_{s}=\left(\mathbf{I}+\left(\mathbf{D}\left(\sqrt{\mathbf{a}_{s}} \right)-\mathbf{I}\right)\mathbf{W}_{r}\right)^{-1}\left(\mathbf{b}\odot \mathbf{z}\right)\] (112)

Now applying the Woodbury matrix identity, which states that

\[\left(\mathbf{A}+\mathbf{U}\mathbf{C}\mathbf{V}\right)^{-1}=\mathbf{A}^{-1}- \mathbf{A}^{-1}\mathbf{U}\left(\mathbf{C}^{-1}+\mathbf{V}\mathbf{A}^{-1} \mathbf{U}\right)^{-1}\mathbf{V}\mathbf{A}^{-1},\] (113)

to the inverse in Eq. 112 with \(\mathbf{A}=\mathbf{I}\), \(\mathbf{U}=\mathbf{I}\), \(\mathbf{C}=\mathbf{D}\left(\sqrt{\mathbf{a}_{s}}\right)-\mathbf{I}\) and \(\mathbf{V}=\mathbf{W}_{r}\), we get,

\[\mathbf{y}_{s}=\left(\mathbf{I}-\left(\left(\mathbf{D}\left(\sqrt{\mathbf{a}_{ s}}\right)-\mathbf{I}\right)^{-1}+\mathbf{W}_{r}\right)^{-1}\mathbf{W}_{r} \right)\left(\mathbf{b}\odot\mathbf{z}\right)\] (114)

We approximate the above equation by assuming that \(\mathbf{W}_{r}\) is a symmetric matrix with the eigendecomposition given by \(\mathbf{Q}\mathbf{A}\mathbf{Q}^{\top}\), with \(\mathbf{Q}^{\top}\mathbf{Q}=\mathbf{Q}\mathbf{Q}\mathbf{Q}^{\top}=\mathbf{I}\) and \(\mathbf{\Lambda}=\mathbf{D}\left(\mathbf{\lambda}\right)\) is a diagonal matrix with the eigenvalues as its diagonal entries. This gives us the following approximation,

\[\begin{split}\mathbf{y}_{s}&\approx\left(\mathbf{I}- \left(\mathbf{Q}\left(\left(\mathbf{D}\left(\sqrt{\mathbf{a}_{s}}\right)- \mathbf{I}\right)^{-1}+\boldsymbol{\Lambda}\right)\mathbf{Q}^{\top}\right)^{-1} \mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^{\top}\right)\left(\mathbf{b}\odot \mathbf{z}\right)\\ &=\left(\mathbf{I}-\mathbf{Q}\mathbf{D}\left(\boldsymbol{\lambda}+ \frac{\mathbf{1}}{\sqrt{\mathbf{a}_{s}}-\mathbf{1}}\right)^{-1}\mathbf{Q}^{ \top}\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^{\top}\right)\left(\mathbf{b} \odot\mathbf{z}\right)\\ &=\left(\mathbf{I}-\mathbf{Q}\mathbf{D}\left(\frac{\boldsymbol{ \lambda}*\sqrt{\mathbf{a}_{s}}-\boldsymbol{\lambda}}{\mathbf{1}-\boldsymbol{ \lambda}+\boldsymbol{\lambda}*\sqrt{\mathbf{a}_{s}}}\right)\mathbf{Q}^{\top} \right)\left(\mathbf{b}\odot\mathbf{z}\right)\\ &=\left(\mathbf{I}-\mathbf{Q}\left(\mathbf{I}-\mathbf{D}\left( \frac{\mathbf{1}}{\boldsymbol{1}-\boldsymbol{\lambda}+\boldsymbol{\lambda}* \sqrt{\mathbf{a}_{s}}}\right)\right)\mathbf{Q}^{\top}\right)\left(\mathbf{b} \odot\mathbf{z}\right)\\ &=\mathbf{Q}\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{1}- \boldsymbol{\lambda}+\boldsymbol{\lambda}*\sqrt{\mathbf{a}_{s}}}\right) \mathbf{Q}^{\top}\left(\mathbf{b}\odot\mathbf{z}\right)\\ &=\mathbf{Q}\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{ \lambda}-\boldsymbol{\lambda}^{2}+\boldsymbol{\lambda}^{2}*\sqrt{\mathbf{a}_{s} }}\right)\boldsymbol{\Lambda}\mathbf{Q}^{\top}\left(\mathbf{b}\odot\mathbf{z} \right)\end{split}\] (115)

We approximate the eigenvalues, \(\boldsymbol{\lambda}\), by the maximum eigenvalue of the \(\mathbf{W}_{r}\) and assume the entries of \(\sqrt{\mathbf{a}_{s}}\) are identical. This gives us the following initial guess for \(\mathbf{y}_{s}\).

\[\begin{split}\mathbf{y}_{s}^{0}&=\mathbf{D}\left( \frac{\mathbf{1}}{\lambda_{m}-\lambda_{m}^{2}+\lambda_{m}^{2}\sqrt{\mathbf{a}_ {s}^{0}}}\right)\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^{\top}\left(\mathbf{b }\odot\mathbf{z}\right)\\ &=\frac{\mathbf{W}_{r}\left(\mathbf{b}\odot\mathbf{z}\right)}{ \lambda_{m}-\lambda_{m}^{2}+\lambda_{m}^{2}\sqrt{\mathbf{a}_{s}^{0}}}\end{split}\] (116)

For the initial guess of \(\mathbf{a}_{s}^{0}\), we use Eq. 111 and plug in the following on the RHS \(\mathbf{a}_{s}\rightarrow\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}\) and the corresponding \(\mathbf{y}_{s}\) found by using Eq. 116. This gives us the following,

\[\mathbf{a}_{s}^{0}=\boldsymbol{\sigma}^{2}\odot\mathbf{b}_{0}^{2}+\mathbf{W} \,\left(\left(\frac{\mathbf{W}_{r}\left(\mathbf{b}\odot\mathbf{z}\right)}{ \lambda_{m}-\lambda_{m}^{2}+\lambda_{m}^{2}\sqrt{\mathbf{b}_{0}^{2}\odot \boldsymbol{\sigma}^{2}}}\right)^{2}\odot\left(\mathbf{b}_{0}^{2}\odot \boldsymbol{\sigma}^{2}\right)\right)\] (117)

Next we update the \(\mathbf{y}_{s}\) and \(\mathbf{a}_{s}\) by performing the following iterations derived using Eq. 110\(\,\,\&\) 111. For instance, \((\mathbf{y}_{s}^{1},\mathbf{a}_{s}^{1})\) are given by,

\[\begin{split}\mathbf{y}_{s}^{1}&=\left(\mathbf{I}- \mathbf{W}_{r}+\mathbf{D}\left(\sqrt{\mathbf{a}_{s}^{0}}\right)\mathbf{W}_{r} \right)^{-1}\left(\mathbf{b}\odot\mathbf{z}\right)\\ \mathbf{a}_{s}^{1}&=\mathbf{b}_{0}^{2}\odot \boldsymbol{\sigma}^{2}+\mathbf{W}\,\left(\left(\mathbf{y}_{s}^{1}\right)^{2} \ast\mathbf{a}_{s}^{0}\right)\end{split}\] (118)

This procedure is summarized in Algorithm 2. Substituting \(\lambda_{m}=1\) in Eq. 116\(\,\,\&\) 117 yields simpler initial conditions and gives us Algorithm 1. Even though we had assumed that \(\mathbf{W}_{r}\) should be symmetric, in practice we find that this algorithm leads to fast convergence even for non-symmetric matrices. The fast convergence is owed to the fact that we have a good initial approximation of the solution. We also find that this iteration scheme works only for recurrent weight matrices with a maximum singular value of 1.

## Appendix H Energy of ORGaNICs

Here, we find the _energy_ (Lyapunov function) that is minimized by the dynamics of the ORGaNICs in the vicinity of the normalization fixed point. We consider the dynamical system with \(\mathbf{W}_{r}=\mathbf{I}\), which is given by Eq. 3. Upon linearizing about the fixed point we get the following linear dynamical system,

\[\begin{bmatrix}\dot{\mathbf{y}}\\ \dot{\mathbf{a}}\end{bmatrix}=\begin{bmatrix}-\mathbf{D}\left(\frac{\sqrt{ \mathbf{a}_{s}}}{\mathbf{r}_{y}}\right)&-\mathbf{D}\left(\frac{\mathbf{y}_{s }}{\mathbf{2}\odot\sqrt{\mathbf{a}_{s}\odot\mathbf{r}_{y}}}\right)\\ \mathbf{D}\left(\frac{\mathbf{2}}{\boldsymbol{\tau}_{a}}\right)\mathbf{W}\, \mathbf{D}\left(\mathbf{a}_{s}\odot\mathbf{y}_{s}\right)&\mathbf{D}\left( \frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\left(-\mathbf{I}+\mathbf{W} \,\mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right)\end{bmatrix}\begin{bmatrix} \mathbf{y}-\mathbf{y}_{s}\\ \mathbf{a}-\mathbf{a}_{s}\end{bmatrix}\] (119)This system is dynamically equivalent (admits the same eigenvalues) to a system of coupled harmonic oscillators with the following equations,

\[\ddot{\mathbf{x}}+\left[\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}} \right)+\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}} \right)-\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol{\tau}_{a}}\right)\mathbf{ W}\mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right]\dot{\mathbf{x}}+\mathbf{D} \left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}\odot\boldsymbol{\tau}_ {a}}\right)\mathbf{x}=\mathbf{0}.\] (120)

We can rewrite the linear system in terms of the position, \(\mathbf{x}\), and the velocity, \(\mathbf{v}\),

\[\begin{bmatrix}\dot{\mathbf{x}}\\ \dot{\mathbf{v}}\end{bmatrix}=\begin{bmatrix}\mathbf{0}&\mathbf{I}\\ -\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{\boldsymbol{\tau}_{y}\odot \boldsymbol{\tau}_{a}}\right)&-\left[\mathbf{D}\left(\frac{\mathbf{1}}{ \boldsymbol{\tau}_{a}}\right)+\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}}{ \boldsymbol{\tau}_{y}}\right)-\mathbf{D}\left(\frac{\mathbf{1}}{\boldsymbol {\tau}_{a}}\right)\mathbf{W}\mathbf{D}\left(\mathbf{y}_{s}^{2}\right)\right] \end{bmatrix}\begin{bmatrix}\mathbf{x}\\ \mathbf{v}\end{bmatrix}\] (121)

Since this system is of the form \(\mathbf{I}\ddot{\mathbf{x}}+\mathbf{B}\dot{\mathbf{x}}+\mathbf{K}\mathbf{x}= \mathbf{0}\), Eq. 36, the _energy_ of this dynamical system is given by \(V(\mathbf{z})=\mathbf{z}^{\top}\mathbf{P}\mathbf{z}\), or,

\[V(\mathbf{x},\mathbf{v})=\begin{bmatrix}\mathbf{x}^{\top}&\mathbf{v}^{\top} \end{bmatrix}\begin{bmatrix}\mathbf{T}\mathbf{K}&\epsilon\mathbf{I}\\ \epsilon\mathbf{I}&\mathbf{T}\end{bmatrix}\begin{bmatrix}\mathbf{x}\\ \mathbf{v}\end{bmatrix}=\mathbf{x}^{\top}\left(\mathbf{T}\mathbf{K}\right) \mathbf{x}+\mathbf{v}^{\top}\mathbf{T}\mathbf{v}+2\epsilon\,\mathbf{x}^{\top} \mathbf{v}\] (122)

Here, \(\mathbf{T}\) is any positive diagonal matrix such that \(\mathbf{T}\mathbf{B}+\mathbf{B}^{\top}\mathbf{T}\succ 0\) and \(\mathbf{K}\) is also a positive diagonal matrix given by \(\mathbf{D}\left(\sqrt{\mathbf{a}_{s}}/\left(\boldsymbol{\tau}_{y}\odot \boldsymbol{\tau}_{a}\right)\right)\). Now, for a valid Lyapunov function, we can take \(\epsilon\) to be arbitrarily small, Eq. 51. Therefore, the _energy_ minimized by the dynamical system is given by \(V(\mathbf{x},\mathbf{v})=\mathbf{x}^{\top}\left(\mathbf{T}\mathbf{K}\right) \mathbf{x}+\mathbf{v}^{\top}\mathbf{T}\mathbf{v}\). This is a high-dimensional version of the energy of a damped harmonic oscillator. For a single oscillator, we have \(V(x,v)=t(kx^{2}+v^{2})\) which is proportional to the total energy (kinetic + potential) of the oscillator.

We now express this _energy_ in terms of the variables relevant to ORGaNICs, i.e., we find \(V(\mathbf{y},\mathbf{a})\). First, we denote the Jacobian matrices in RHS of Eq. 119 & 121 by \(\mathbf{A}\) & \(\mathbf{B}\), respectively. We note the simple fact that \(\mathbf{A}\) & \(\mathbf{B}\) are related by a similarity transformation (a change of basis). This means that there exists an invertible matrix \(\mathbf{U}\), such that \(\mathbf{A}=\mathbf{U}^{-1}\mathbf{B}\mathbf{U}\) and the corresponding transform is given by \([\mathbf{x}\;\mathbf{v}]^{\top}=\mathbf{U}[\mathbf{y}-\mathbf{y}_{s}\; \mathbf{a}-\mathbf{a}_{s}]^{\top}\). Assuming that \(\mathbf{U}\) is invertible, we can write this equation as \(\mathbf{U}\mathbf{A}=\mathbf{B}\mathbf{U}\). To solve this, we consider a block matrix representation of \(\mathbf{U}\) and find the following solution,

\[\mathbf{U}=\begin{bmatrix}\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{s}}\odot \boldsymbol{\tau}_{y}}{\mathbf{y}_{s}}\right)&\mathbf{0}\\ -\mathbf{D}\left(\frac{\mathbf{a}_{s}}{\boldsymbol{\tau}_{s}}\right)&-\frac{1 }{2}\mathbf{I}\end{bmatrix}\] (123)

This change of basis gives us the transformation,

\[\begin{bmatrix}\mathbf{x}\\ \mathbf{v}\end{bmatrix}=\begin{bmatrix}\mathbf{D}\left(\frac{\sqrt{\mathbf{a}_{ s}}\odot\boldsymbol{\tau}_{y}}{\mathbf{y}_{s}}\right)&\mathbf{0}\\ -\mathbf{D}\left(\frac{\mathbf{a}_{s}}{\boldsymbol{\tau}_{s}}\right)&-\frac{1 }{2}\mathbf{I}\end{bmatrix}\begin{bmatrix}\mathbf{y}-\mathbf{y}_{s}\\ \mathbf{a}-\mathbf{a}_{s}\end{bmatrix}\] (124)

or,

\[\mathbf{x} =\frac{\sqrt{\mathbf{a}_{s}}\odot\boldsymbol{\tau}_{y}}{\mathbf{y }_{s}}\odot(\mathbf{y}-\mathbf{y}_{s})\] (125) \[\mathbf{v} =-\frac{\mathbf{a}_{s}}{\mathbf{y}_{s}}\odot(\mathbf{y}-\mathbf{ y}_{s})-\frac{(\mathbf{a}-\mathbf{a}_{s})}{2}\]Substituting these expressions into \(V(\mathbf{x},\mathbf{v})=\mathbf{x}^{\top}\left(\mathbf{T}\mathbf{K}\right) \mathbf{x}+\mathbf{v}^{\top}\mathbf{T}\mathbf{v}\), and assuming the diagonal entries of the matrix \(\mathbf{T}\) to be \(t_{i}\) and substituting the diagonal entries of \(\mathbf{K}\), \(k_{i}\rightarrow\sqrt{a_{si}}/(\tau_{y_{i}}\tau_{ai})\), we get the _energy_ in terms of \(\mathbf{y}\) and \(\mathbf{a}\),

\[V(\mathbf{y},\mathbf{a})=\sum_{i=1}^{n}t_{i}\left[\frac{\tau_{y_{i}}}{\tau_{a _{i}}}\frac{{a_{si}}^{3/2}}{{y_{si}}^{2}}\left(y_{i}-{y_{si}}\right)^{2}+\frac{ {a_{si}}}{{y_{si}}^{2}}\left(\sqrt{a_{si}}\left(y_{i}-{y_{si}}\right)+\frac{{y_ {si}}}{2\sqrt{a_{si}}}\left(a_{i}-a_{si}\right)\right)^{2}\right]\] (126)

We notice that Taylor expanding the term \(\sqrt{a_{i}}y_{i}\) about \(\sqrt{a_{si}}{y_{s}}_{i}\) and ignoring the second order terms, we get,

\[\sqrt{a_{i}}y_{i}\approx\sqrt{a_{si}}y_{si}+\sqrt{a_{si}}\left(y_{i}-y_{si} \right)+\frac{{y_{si}}}{2\sqrt{a_{si}}}\left(a_{i}-a_{si}\right)\] (127)

Therefore the _energy_ function, \(V(\mathbf{y},\mathbf{a})\), is given by,

\[V(\mathbf{y},\mathbf{a})=\sum_{i=1}^{n}t_{i}\frac{{a_{si}}}{{y_{si}}^{2}} \left[\frac{\tau_{y_{i}}}{\tau_{ai}}\sqrt{a_{si}}\left(y_{i}-{y_{si}}\right)^ {2}+\left(\sqrt{a_{i}}y_{i}-\sqrt{a_{si}}{y_{si}}\right)^{2}\right]\] (128)

Notice that \(\sqrt{a_{si}}y_{si}=b_{i}z_{i}\). This gives us the following expression for the _energy_ function,

\[V(\mathbf{y},\mathbf{a})=\sum_{i=1}^{n}t_{i}\frac{{a_{si}}}{{y_{si}}^{2}} \left[\frac{\tau_{y_{i}}}{\tau_{ai}}\sqrt{a_{si}}\left(y_{i}-{y_{si}}\right)^ {2}+\left(\sqrt{a_{i}}y_{i}-b_{i}z_{i}\right)^{2}\right]\] (129)

Further, for an ORGaNICs model containing one \(y\) and one \(a\) neuron, after removing the proportionality constants, the _energy_ function is given by,

\[V(y,a)=\frac{\tau_{y}}{\tau_{a}}\sqrt{a_{s}}\left(y-y_{s}\right)^{2}+(\sqrt{ a}y-bz)^{2}\] (130)

After plugging in the steady-state values, we get,

\[V(y,a)=\frac{\tau_{y}}{\tau_{a}}\sqrt{b_{0}^{2}\sigma^{2}+wb^{2}z^{2}}\,\left( y-\frac{bz}{\sqrt{b_{0}^{2}\sigma^{2}+wb^{2}z^{2}}}\right)^{2}+(\sqrt{a}y-bz)^{2}\] (131)

For this system, it is easy to verify that this is a valid Lyapunov function and is minimized by the dynamics of the circuit. We now demonstrate that it has the properties of a Lyapunov function. First, \(V(y_{s},a_{s})=0\) and \(V(y_{s},a_{s})>0\)\(\forall\)\(y\neq y_{s}\)\(\&\)\(a\neq a_{s}\), this can be easily seen from Eq. 130. Second, we need to show that, \(V(y,a)<0\)\(\forall\)\(y\neq y_{s}\)\(\&\)\(a\neq a_{s}\). Using Eq. 119 & 131 \(\dot{V}(y,a)\), we can write the total time derivative of the _energy_ to be,

\[\begin{split}\frac{\mathrm{d}V(y,a)}{\mathrm{d}t}& =\frac{\partial V}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}t}+ \frac{\partial V}{\partial a}\frac{\mathrm{d}a}{\mathrm{d}t}\\ &=-\frac{\left(2ya_{s}-3a_{s}y_{s}+ay_{s}\right)^{2}\left(\sqrt{a _{s}}\tau_{a}-wy_{s}^{2}\tau_{y}+\tau_{y}\right)}{2a_{s}\tau_{a}\tau_{y}}\end{split}\] (132)

Since \(a_{s}>0\) and,

\[\left(\sqrt{a_{s}}\tau_{a}-wy_{s}^{2}\tau_{y}+\tau_{y}\right)=\left(\sqrt{a_{s }}\tau_{a}+\tau_{y}\left(\frac{b_{0}^{2}\sigma^{2}}{b_{0}^{2}\sigma^{2}+wb^{2 }z^{2}}\right)\right)>0,\] (133)

for all the choices of parameters and \(\forall\)\(y\neq y_{s},a\neq a_{s}\), we have \(\dot{V}(y,a)<0\), therefore, it is a valid Lyapunov function and can be interpreted as the _energy_ that decreases with time via the dynamics of ORGaNICs.

## Appendix I Training details

The code (written in PyTorch [92]) to produce all the results can be found at https://github.com/martiniani-lab/dynamic-divisive-norm. For both the static input and the sequential input, we train ORGaNICs on the MNIST handwritten digit dataset [72], and to the best of our knowledge, it does not pose any privacy concern and has been used widely by the ML community freely. The simulations were performed on an HPC cluster. All of the models were trained on a single A100 (80GB) GPU. We use Adam optimizer [93] with default parameters for minimizing the loss function.

### Static MNIST input

We performed a random split of 57,000 training samples and 3,000 validation samples and picked the model with the largest validation accuracy for testing. To make a direct comparison to SSN [55], we use the same architecture structure as theirs. First, we train an autoencoder (Table 3) to reduce the dimensionality of MNIST images to 40 by using a mean-squared loss function. Then, we use this 40-dimensional vector as input to ORGaNICs and train it using the cross-entropy loss function. We additionally make the input gain \(\mathbf{b}\) dependent on the input \(\mathbf{x}\), \(\mathbf{b}=f(\mathbf{W}_{bx}\mathbf{x})\), where \(f\) is sigmoid. A layer of ORGaNICs is given by Eq. 15,

\[\begin{split}\boldsymbol{\tau}_{y}\odot\dot{\mathbf{y}}& =-\mathbf{y}+f(\mathbf{W}_{bx}\mathbf{x})\odot(\mathbf{W}_{zx} \mathbf{x})+\left(\mathbf{1}-\sqrt{\left\lfloor\mathbf{a}\right\rfloor} \right)\odot(\mathbf{W}_{r}\mathbf{y})\\ \boldsymbol{\tau}_{a}\odot\dot{\mathbf{a}}&=- \mathbf{a}+\mathbf{b}_{0}^{2}\odot\boldsymbol{\sigma}^{2}+\mathbf{W}\,\left( \mathbf{y}^{2}\odot\left\lfloor\mathbf{a}\right\rfloor\right)\end{split}\] (134)

The "output" of a layer is the steady-state firing rate of the neuron with the positive receptive field, i.e., \(\mathbf{y}_{s}^{+}=|\mathbf{y}_{s}|^{2}\). We parameterize \(\mathbf{W}_{r}\) to have a maximum singular value of 1 and instead of simulating the dynamical system to find the fixed point, we use the iterative Algorithm 1 with a maximum number of steps = 10. More details about the parameters are given in Table 4; kaiming uniform initialization is used from [94]. Additional hyperparameters are given in Table 7. We train ORGaNICs in a single-layer setting with the number of \(\mathbf{y}\) neurons encoding the input, \(N_{1}=50,80\). We also train two-layer ORGaNICs (Table 5) with \(N_{1}=120\) and \(N_{2}=60\) neurons in each layer. The model is trained using backpropagation and takes approximately 10 min to fully train.

### Permuted and Unpermuted sequential MNIST

We performed a random split of 57,000 training samples and 3,000 validation samples and picked the model with the largest validation accuracy for testing. The unpermuted sequential MNIST task is defined as follows: for a given \(28\times 28\) image, we flatten it to get a one-dimensional, 784 timestep-long input. Then these pixels are presented as an input (\(\mathbf{x}_{i}\), one pixel at each time-step \(i\)) to the Euler discretized rectified ORGaNICs model (Eq. 87) with rectified input drive, given by the following

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Parameter** & **Shape** & **Learned** & **Initialization** \\ \hline \(\mathbf{W}_{zx}\) & \(N\times M\) & yes & kaiming uniform \\ \(\mathbf{W}_{bx}\) & \(N\times M\) & yes & kaiming uniform \\ \(\mathbf{W}_{r}\) & \(N\times N\) & yes & identity \\ \(\mathbf{W}\) & \(N\times N\) & yes & ones \\ \(\mathbf{b}_{0}\) & \(N\) & yes & random normal \\ \(\boldsymbol{\sigma}\) & \(N\) & no & ones \\ \hline \hline \end{tabular}
\end{table}
Table 4: ORGaNICs parametrization for static MNIST classification

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Layer** & \multicolumn{2}{c}{**Shape**} & **Nonlinearity** \\ \hline Input \(\rightarrow\) encoder (layer-1) & 784 \(\times\) 360 & ReLU \\ encoder (layer-1) \(\rightarrow\) encoder (layer-2) & 360 \(\times\) 120 & ReLU \\ encoder (layer-2) \(\rightarrow\) embedding & 120 \(\times\) 40 & sigmoid \\ embedding \(\rightarrow\) decoder (layer-1) & 40 \(\times\) 120 & ReLU \\ decoder (layer-1) \(\rightarrow\) decoder (layer-2) & 120 \(\times\) 360 & ReLU \\ decoder (layer-2) \(\rightarrow\) output & 360 \(\times\) 784 & sigmoid \\ \hline \hline \end{tabular}
\end{table}
Table 3: Autoencoder architectureequations,

\[\begin{split}\mathbf{y}_{i+1}&=\mathbf{y}_{i}+\frac{ \Delta t}{\boldsymbol{\tau}_{y}}\odot\left(-\mathbf{y}_{i}+\mathbf{b}_{i}\odot \lfloor\mathbf{W}_{zx}\mathbf{x}_{i}\rfloor+\left(\mathbf{1}-\mathbf{a}_{i}^{+ }\right)\odot\lfloor\mathbf{W}_{r}\mathbf{y}_{i}\rfloor\right)\\ \mathbf{a}_{i+1}&=\mathbf{a}_{i}+\frac{\Delta t}{ \boldsymbol{\tau}_{a}}\odot\left(-\mathbf{a}_{i}+\mathbf{b}_{0,i}^{2}\odot \boldsymbol{\sigma}^{2}+\mathbf{W}\left(\mathbf{y}_{i}^{+}\odot\mathbf{a}_{i}^ {+2}\right)\right)\\ \mathbf{b}_{i+1}&=\mathbf{b}_{i}+\frac{\Delta t}{ \boldsymbol{\tau}_{b}}\odot\left(-\mathbf{b}_{i}+f(\mathbf{W}_{bx}\mathbf{x}_{ i}+\mathbf{W}_{by}\mathbf{y}_{i}+\mathbf{W}_{ba}\mathbf{a}_{i})\right)\\ \mathbf{b}_{0,i+1}&=\mathbf{b}_{0,i}+\frac{\Delta t }{\boldsymbol{\tau}_{b_{0}}}\odot\left(-\mathbf{b}_{0,i}+f(\mathbf{W}_{b_{0}x }\mathbf{x}_{i}+\mathbf{W}_{b_{0}y}\mathbf{y}_{i}+\mathbf{W}_{b_{0}a}\mathbf{ a}_{i})\right)\end{split}\] (135)

When we are done presenting the pixels we use the last hidden state, i.e., \(\mathbf{y}_{784}\), to make the predictions. To make this more challenging we also train ORGaNICs on permuted sMNIST where we first permute the pixels of all the images in some random order and the rest of the task is the same. Instead of parametrizing \(\boldsymbol{\tau}\), we parametrize \(\Delta t/\boldsymbol{\tau}_{y}=0.05*f(\mathbf{p}_{y})\), \(\Delta t/\boldsymbol{\tau}_{a}=0.01*f(\mathbf{p}_{a})\) and \(\Delta t/\boldsymbol{\tau}_{b}=0.1*f(\mathbf{p}_{b})\) and \(\Delta t/\boldsymbol{\tau}_{b_{0}}=0.1*f(\mathbf{p}_{b_{0}})\), so we can control the dimensionless relative time constants. In practice, we find it is better to make the a neurons sluggish compared to \(\mathbf{y}\). This is based on the intuition given by the two-dimensional phase portrait for different relative time constants Fig. 3. All the parameters (including \(\mathbf{W}_{r}\)) are unconstrained for this task with initialization specified in Table 6. Since ORGaNICs are stable, we did not need to use gradient clipping for training, which is commonly used for LSTMs. Additionally, we train the model using a StepLR learning rate scheduler with parameters given in Table 7. The model is trained using backpropagation through time (BPTT) and takes approximately 30 hours to fully train.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Parameter** & **Shape** & **Learned** & **Initialization** \\ \hline \(\mathbf{W}_{zx}\) & \(N\times 1\) & yes & kaiming uniform \\ \(\mathbf{W}_{bx}\) & \(N\times 1\) & yes & kaiming uniform \\ \(\mathbf{W}_{by}\) & \(N\times N\) & yes & kaiming uniform \\ \(\mathbf{W}_{ba}\) & \(N\times N\) & yes & kaiming uniform \\ \(\mathbf{W}_{b_{0}x}\) & \(N\times 1\) & yes & kaiming uniform \\ \(\mathbf{W}_{boy}\) & \(N\times N\) & yes & kaiming uniform \\ \(\mathbf{W}_{b_{0}a}\) & \(N\times N\) & yes & kaiming uniform \\ \(\mathbf{W}_{r}\) & \(N\times N\) & yes & identity \\ \(\mathbf{W}^{\text{ }}\) & \(N\times N\) & yes & ones \\ \(\boldsymbol{\sigma}\) & \(N\) & no & ones \\ \hline \hline \end{tabular}
\end{table}
Table 6: ORGaNICs parametrization for sequential MNIST classification

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Hyperparameter** & **Static MNIST** & **Sequential MNIST** \\ \hline Batch size & 256 & 256 \\ Initial Learning rate & 0.001 & 0.01 \\ Weight decay & \(10^{-5}\) & \(10^{-5}\) \\ Step size (StepLR) & None & 30 epochs \\ Gamma (StepLR) & None & 0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters

[MISSING_PAGE_EMPTY:35]

Figure 4: **Fast convergence of the iterative algorithm.** The results are for 20-dimensional ORGaNICs (10 **y** and 10 **a** neurons) with random parameters and inputs with the additional constraint of the maximum singular value of \(\mathbf{W}_{r}\) equal to 1 and \(||\mathbf{z}||<1\). **(a)**, Mean (with error bars representing 1-sigma S.D.) and maximum errors (\(\epsilon\)) as a function of number of iterations. \(\epsilon\) is calculated as the norm of the difference between the true solution (found by simulation starting with random initialization) and the iteration solution. **(b)**, An example of a randomly sampled \(\mathbf{W}_{r}\). **(c)**, Steady-state approximation as a function of iteration number. Different lines represent different neurons. **(d)**, Overlap between the iteration solution (after 15 iterations) and the true solution.

Figure 5: **Histogram for the eigenvalue with the largest real part**. We train two-layer ORGaNICs (\(\tau_{a}=\tau_{y}=2\,\)ms) with a static MNIST input where \(\mathbf{W}_{r}\) is constrained to have a maximum singular value of 1. We plot the histogram of eigenvalues of the Jacobian matrix with the largest real part, for inputs from the test set. We find that all the eigenvalues of the Jacobian have negative real parts, implying asymptotic stability. **(a)**, histogram for the first layer. **(b)**, histogram for the second layer. Note that since this is implemented in a feedforward manner, this is a cascading system with no feedback, hence we can perform the stability analysis of the two layers independently.

Figure 6: **Eigenvalue with the largest real part while training on static input (MNIST) classification task.** This plot shows the largest real part of eigenvalues across all test samples as training progresses. The fact that the largest real part consistently remains below zero indicates that the system maintains stability throughout training.

Figure 7: **Trajectories of the hidden states (y).** This plot shows the dynamics of the hidden state as the input is being presented sequentially. We train ORGaNICs (128 units) as an RNN on **(a)**, unpermuted sequential MNIST and **(b)**, permuted sequential MNIST. The inputs are picked randomly from the test set. The hidden state trajectory remains bounded, indicating stability.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide proof, empirical evidence, and citations of any claims made in the abstract and the introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the model in the Discussions section.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Detailed proofs of all the theorems are provided in the Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the architecture in detail and provide code to implement it in PyTorch.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a URL to the GitHub repository with the code used to reproduce all of the results in the paper.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: This information can be found in the supplementary.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We include error bars in the plot dealing with the random initialization of the model.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: We provide all of the information in the supplementary.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not violate any code of ethics listed in the guidelines.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The model used in the paper have been trained on toy datasets and therefore pose no risk.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited the creators of the publicly available MNIST dataset.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provided an anonymized URL in the supplementary for a well-documented GitHub repository to reproduce all the results in the manuscript.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.