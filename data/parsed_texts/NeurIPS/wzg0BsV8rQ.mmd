# FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation

Jia Gu

Equal Contribution Center for Statistical Science, Peking University

Caizhi Tang

Equal Contribution Center for Statistical Science, Peking University

Han Yan

Guang Cui

Ant Group

Longfei Li

Ant Group

Jun Zhou

Corresponding Author Guajia@pku.edu.cn, caizhi.tcz@antgroup.com, hanyan@stu.pku.edu.cn, {cuiqing.cq, longyao.llf, jun.zhoujun}@antgroup.com

###### Abstract

This paper proposes a novel strategy for estimating the heterogeneous treatment effect called the Fused and Accurate Shrinkage Tree (FAST). Our approach utilizes both trial and observational data to improve the accuracy and robustness of the estimator. Inspired by the concept of shrinkage estimation in statistics, we develop an optimal weighting scheme and a corresponding estimator that balances the unbiased estimator based on the trial data with the potentially biased estimator based on the observational data. Specifically, combined with tree-based techniques, we introduce a new split criterion that utilizes both trial data and observational data to more accurately estimate the treatment effect. Furthermore, we confirm the consistency of our proposed tree-based estimator and demonstrate the effectiveness of our criterion in reducing prediction error through theoretical analysis. The advantageous finite sample performance of the FAST and its ensemble version over existing methods is demonstrated via simulations and real data analysis.

## 1 Introduction

Causal effects are the magnitude of the response of an effect variable (also called outcome) caused by the effect variable (also called treatment), which is a fundamental and essential issue in the field of casual inference (Imbens and Rubin, 2016). And the heterogeneous treatment effect (abbr. HTE) is usually used to characterize the heterogeneity of causal effects across different subgroups of the population. In recent years, heterogeneous treatment effect estimation has been successfully applied in various fields such as epidemiology, medicine, and social sciences (Glass et al., 2013; Kosorok and Laber, 2019; Turney and Wildeman, 2015; Taddy et al., 2016).

In general, the causal problems can be studied through both experimental studies (also known as randomized control trials, RCTs) and observational studies. Experimental studies are widely regarded as the gold standard for assessing causal effects since the randomization process eliminates the possibility of confounding bias. However, large-scale RCTs can be challenging due to issues related to cost, time, and ethics (Edwards et al., 1999). On the other hand, observational data are often readily available with an adequate sample size. Under certain fairly strong assumptions, such as unconfoundedness assumption, there is a rich literature regarding the estimation of HTE in observational studies, such as tree-based methods (Athey and Imbens, 2016; Wager and Athey, 2018; Athey et al., 2019; Hahn et al., 2020), boosting (Powers et al., 2017), meta learners (Kunzel et al.,2019) and \(R\)-learner (Nie and Wager, 2020). However, the unconfoundedness assumption, which requires measuring all confounders, is in general untestable unless extra information such as the existing of instrumental variables is available (de Luna and Johansson, 2014). And any violations of this assumption may result in seriously invalid causal statements. Various methods have been proposed to mitigate the unmeasured confounding in observational studies, such as the sensitivity analysis (Rosenbaum and Rubin, 1983; Zhang and Tchetgen Tchetgen, 2022), the instrumental variables (IV) approach (Angrist et al., 1996) and the proximal causal inference (Kuroki and Pearl, 2014; Miao et al., 2018; Shi et al., 2020; Cui et al., 2023). However, the validity of these procedures also relies crucially on assumptions that are often difficult to verify in practice.

Given the limitations of relying on individual data sources, data fusion, as a branch of causal inference strategies that integrates both the trial and the observational data, has gained significant interest in the literature (Bareinboim and Pearl, 2016; Colnet et al., 2020; Shi et al., 2022). Existing data fusion methods for estimating the HTE include the \(\mathrm{KPS}\) estimator obtained by modeling the confounding function parametrically (Kallus et al., 2018), the semi-parametric integrative estimator under the parametric structural models (Yang et al., 2020) and the integrative R-learner (Wu and Yang, 2022). Besides, (Tang et al., 2022) proposed the Gradient Boosting Causal Tree (\(\mathrm{GBCT}\)), which integrated the current observational data and their historical controls for estimating the conditional average treatment effect on the treated group (CATT).

This paper presents a novel approach for estimating heterogeneous treatment effects (HTE) in the context of causal data fusion. The proposed method, named Fused and Accurate Shrinkage Tree (\(\mathrm{FAST}\)), _avoids_ the need for a two-stage estimation process required in conventional data fusion strategies, which involves modeling and estimating the nuisance confounding bias function. The main contributions of this work can be summarized as follows (i) The authors propose a novel shrinkage method for combining an unbiased and biased estimator, which effectively reduces the mean square error of the unbiased estimator, and provides an easy implementation of the method tailored for the HTE estimation; (ii) The authors extend the conventional node split criterion via a re-scaling technique, which automatically penalizes the use of the observational data with low quality (namely large confounding bias); (iii) The authors also provide a theoretical analysis to explain the advantages of our splitting criterion.

## 2 Background and motivation

### Notations

Let \(\bm{X}\in\mathcal{X}=[-1,1]^{p}\) be a \(p\)-dimensional vector of pre-treatment covariates, \(\bm{U}\in\mathbb{R}^{q}\) (\(q\geq 0\)) be a possibly unmeasured random vector consisting of the confounding variables, \(D\) be a binary treatment variable (\(D=0\) denotes the control and \(D=1\) denotes the treated) and let \(Y(d)\) be the potential outcome that would be observed when the treatment had been set to \(d\in\{0,1\}\). We follow the potential outcome framework (Rubin, 1974) to define the heterogeneous treatment effect \(\tau(\bm{x})=\mathbb{E}(Y(1)-Y(0)|\bm{X}=\bm{x})\).

Suppose that we can collect two kinds of data: trial data and observational data, and they are described by \(n+m\) quadruples, \(\{Y_{i},D_{i},\bm{X}_{i},S_{i}\}_{i=1}^{n+m}\), where \(S_{i}\) indicates if the \(i\)-th individual would have been recruited (\(S=1\)) or not (\(S=0\)) in the trial. We also denote \(\mathcal{R}=\{1,2,\cdots,n\}\) the set of indices of observations in the RCT study, and \(\mathcal{O}=\{n+1,n+2,\cdots,n+m\}\) the set of indices of observations in the observational study. We define \(e(\bm{X},\bm{U},S)=P(D=1|\bm{X},\bm{U},S)\) as the propensity score of the trial and observational population, respectively. In practice, due to \(\bm{U}\) being unknown, we usually use \(\hat{e}(\bm{X},S)\) to estimate \(e(\bm{X},\bm{U},S)\). In addition, \(\hat{e}(\bm{X},1)\) is unbiased, but \(\hat{e}(\bm{X},0)\) is biased because the unmeasured confounder \(\bm{U}\) in the observational study can be related to the assignment of treatment \(D\). Let \(\tau_{1}(\bm{x})=\mathbb{E}(Y(1)-Y(0)|\bm{X}=\bm{x},S=1)\) be the HTE on the trial population. We then make the following fundamental assumption on the trial and observational studies, which facilitates the potential for causal data fusion:

**Assumption 1**.: _(i) For any \(\bm{x}\in\mathcal{X}\), \(\tau_{1}(\bm{x})=\tau(\bm{x})\); (ii) \(Y(d)\perp D|(\bm{X},S=1)\) for \(d\in\{0,1\}\) and (iii) the propensity score \(\delta<e(\bm{X},S)<1-\delta\) almost surely for some constant \(0<\delta<1/2\)._

Assumption 1 (i) states that the HTE function is transportable from the trial population to the target population. Stronger versions of Assumption 1 include the ignorability of study participation (Buchanan et al., 2018) and the mean exchangeability (Dahabreh et al., 2019). In the following of this paper, we use \(|\Lambda|\) to denote the number of elements for any set \(\Lambda\), \(\lfloor c\rfloor\) to denote the biggest integer less than or equal to the constant \(c\), and \([p]\) to denote the index set \(\{1,2,\cdots,\lfloor p\rfloor\}\). For two positive sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\), we write \(a_{n}=\mathrm{O}(b_{n})\) if \(|a_{n}/b_{n}|\) is bounded.

### Tree-based methods

To estimate the HTE, it is reasonable to perform subgroup analysis by appropriately stratifying or matching (Frangakis and Rubin, 2002) the samples into multiple subgroups that differ in the magnitude of treatment effects. In machine learning, tree-based methods (Breiman et al., 1984; Breiman, 2001; Friedman, 2001) are usually used for such stratification tasks, which greedily optimize the loss function, also called splitting criterion, via recursively partitioning feature space. In fact, many tree-based causal methods designed for the HTE estimation were also proposed (Radcliffe and Surry, 2012; Athey and Imbens, 2016; Athey et al., 2019). Along with the development of the tree-based methods, various regularization strategies, either implicit or explicit, have been proposed to mitigate overfitting (Mentch and Zhou, 2020; Agarwal et al., 2022). Recently, tree-based methods have been generalized to address the heterogeneous data from diverse data sources(Nasseri et al., 2022). For convenience, in the following we define a regression tree by two components: a set of leaves \(\bm{Q}=\{Q_{j}\}_{j=1}^{J}\) and the associated parameter \(\tau\). We can denote a causal tree by \(T(X;\bm{Q},\tau)=\sum_{j=1}^{J}\tau(Q_{j})\mathrm{I}\{\bm{x}\in Q_{j}\}\), where \(\mathrm{I}\{\cdot\}\) denotes the indicator function and \(\tau(Q_{j})\) denotes the casual effect of sub-area indexed by \(Q_{j}\).

### Shrinkage estimation

It is important to note that applying conventional methods, such as the generalized random forest (Athey et al., 2019), separately to trial data and observational data can readily lead to two estimators: the first is unbiased but may exhibit large variability, while the second is potentially biased but usually has a smaller variance due to the much larger amount of observational data. Therefore, the challenge becomes finding the optimal combination of an unbiased estimator and a biased estimator in the data fusion problem. To see this, suppose we have a parameter of interest \(\theta\in\mathbb{R}\), an unbiased estimator \(\hat{\theta}_{u}\), and a (potentially) biased estimator \(\hat{\theta}_{b}\) of \(\theta\), such that \(\mathbb{E}(\hat{\theta}_{u})=\theta\), \(\mathbb{E}(\hat{\theta}_{b})=\theta+b(\theta)\), \(\mathrm{Var}(\hat{\theta}_{u})=\sigma_{u}^{2}\), \(\mathrm{Var}(\hat{\theta}_{b})=\sigma_{b}^{2}\) and \(\mathrm{Cov}(\hat{\theta}_{u},\hat{\theta}_{b})=0\). Consider the family of estimators \(\Lambda_{w}=\{\hat{\theta}_{w}|\hat{\theta}_{w}=w\hat{\theta}_{b}+(1-w)\hat{ \theta}_{u},0\leq w\leq 1\}\), then the mean square error (MSE) of its elements admits the following expansion:

\[\mathbb{E}(\hat{\theta}_{w}-\theta)^{2}=(\sigma_{b}^{2}+b^{2}(\theta)+\sigma_ {u}^{2})w^{2}-2\sigma_{u}^{2}w+\sigma_{u}^{2}.\] (1)

Figure 1: The probability density functions (pdfs) of the unbiased estimator (pink) and the biased estimator (blue) in the left panel and the pdf of the shrinkage (fused) estimator under the optimal weight \(W^{*}\) (green) in the right panel. The vertical dashed line represents the true parameter value \(\theta^{*}=0\).

Minimizing (1) with respect to \(w\), we can obtain the unique minimizer \(w^{*}=\sigma_{u}^{2}/(\sigma_{b}^{2}+b^{2}(\theta)+\sigma_{u}^{2})\) and the gain of the optimal weighting over the single estimators \(\hat{\theta}_{u}\) and \(\hat{\theta}_{b}\) can be characterized by the following formula:

\[\mathbb{E}(\hat{\theta}_{w}^{*}-\theta)^{2}=(1-w^{*})\sigma_{u}^{2}=w^{*}( \sigma_{b}^{2}+b^{2}(\theta)).\] (2)

**Comment** The weighting strategy is akin to the classical James-Stein shrinkage estimation (Efron and Morris, 1973; Green and Strawderman, 1991) method, in which it is shown that a multivariate normal vector \(\bm{Z}\) (\(p\geq 3\)), as a maximum likelihood estimator (MLE) of its population mean \(\bm{\mu}=\mathbb{E}(\bm{Z})\), is not minimax optimal, and the MSE of the estimator \(\bm{Z}\) can be reduced by shrinking it towards the zero vector \(\bm{0}\) by some factor \(0<w<1\). The zero vector can be viewed as a biased estimator of \(\bm{\mu}\) with zero variance in their setting. In comparison, we replace the deterministic estimator with a (potentially) biased estimator \(\hat{\theta}_{b}\): The larger the variance \(\sigma_{u}^{2}\) of the unbiased estimator is compared to \(b^{2}(\theta)+\sigma_{b}^{2}\), the more the fused estimator \(\hat{\theta}_{w^{*}}\) will be shrunk towards the biased estimator that is less fluctuating. By doing so, one can efficiently mitigate the occurrence of significant estimation error in the unbiased estimator caused by its high variance, as unbiasedness alone _does not_ guarantee reliable estimation performance with a limited sample size. Figure 1 illustrates a concrete example of the benefit provided by the shrinkage estimation, where \(\theta=0,\hat{\theta}_{u}\sim\mathrm{N}(\theta,5)\) and \(\hat{\theta}_{b}\sim N(\theta+2,0.5)\). The fused estimator \(\hat{\theta}_{w^{*}}\) reduces over \(50\%\) of the MSE compared with the unbiased estimator \(\hat{\theta}_{u}\).

## 3 Methodology

In this section, we propose a new data fusion strategy, referred to as the Fused and Accurate Shrinkage Tree (FAST). We proceed in a bottom-up manner to provide a clear and intuitive illustration of the entire estimation: we will begin by applying the shrinkage estimation strategy for local data fusion within each sub-region of the feature space given by a pre-specified partition. Then, we propose a fused criterion that incorporates the information contained in the observational data via a simple re-scaling of the conventional criterion. Theoretical guarantees are established in Section 4.

### Local fusion for the HTE estimation

Under a pre-specified partition \(\bm{Q}=\{Q_{j}\}_{j=1}^{J}\) of the feature space, let \(\mathcal{R}_{j}=\{i|i\in\mathcal{R},\bm{X}_{i}\in Q_{j}\}\) and \(\mathcal{O}_{j}=\{i|i\in\mathcal{O},\bm{X}_{i}\in Q_{j}\}\) represent the sets of indices of the trial and observational sub-samples, respectively, that fall within the region \(Q_{j}\). Let

\[\widetilde{Y}=\frac{YD}{e(\bm{X},S)}-\frac{Y(1-D)}{1-e(\bm{X},S)}\] (3)

be transformed outcomes of all data, e.g., the transformed outcomes of \(i\)-th sample can be denoted by \(\widetilde{Y}_{i}\). Then under Assumption 1, one can immediately show for the trial population with \(S=1\):

\[\mathbb{E}\left(YD|\bm{X},S=1\right) = \mathbb{E}\left(Y(1)|\bm{X},S=1\right)\mathbb{E}\left(D|\bm{X},S= 1\right)\quad\text{ and }\] \[\mathbb{E}\left(Y(1-D)|\bm{X},S=1\right) = \mathbb{E}\left(Y(0)|\bm{X},S=1\right)(1-\mathbb{E}\left(D|\bm{X},S=1\right)),\ \text{ leading to }\] \[\mathbb{E}\left((\widetilde{Y}|\bm{X}=\bm{x},S=1\right)=\tau_{1} (\bm{x})=\tau(\bm{x}).\] (4)

Thus, \(\hat{\tau}_{u}(Q_{j})=(1/|\mathcal{R}_{j}|)\sum_{i\in\mathcal{R}_{j}} \widetilde{Y}_{i}\) is an unbiased estimator of \(\mathbb{E}(Y(1)-Y(0)|\bm{X}\in Q_{j},S=1)\), which can be seen as a reasonable approximation of \(\tau(Q_{j})\) if \(\bm{Q}\) segments the feature space properly such that \(\tau(\bm{x})\) varies slowly in each sub-region \(Q_{j}\). An estimator of \(\mathrm{Var}(\hat{\tau}_{u}(Q_{j}))\) is given by \(\hat{\sigma}_{u}^{2}(Q_{j})=(1/|(\mathcal{R}_{j}|(|\mathcal{R}_{j}|-1)))\sum_ {i\in\mathcal{R}_{j}}(\widetilde{Y}_{i}-\hat{\tau}_{u}(Q_{j}))^{2}\). In contrast, for the observational population, the conditional independence no longer holds and \(\hat{\tau}_{b}(Q_{j})=(1/|\mathcal{O}_{j}|)\sum_{i\in\mathcal{O}_{j}} \widetilde{Y}_{i}\) is a biased estimator concerning \(\tau(Q_{j})\), due to the presence of unmeasured confounding (\(\bm{U}\)) on the observational data.

It remains to estimate the weight \(w^{*}(Q_{j})\) composed of the tuple \((\sigma_{u}^{2}(Q_{j}),\sigma_{b}^{2}(Q_{j}),b^{2}(Q_{j}))\). The first term \(\sigma_{u}^{2}(Q_{j})\) can be estimated by \(\hat{\sigma}_{u}^{2}(Q_{j})\). To bypass the unmeasured confounding issue of the observational population, re-sampling techniques, such as the Bootstrap (Efron, 1979; Hall, 1992), can be applied to consistently estimate \(\sigma_{b}^{2}(Q_{j})\). However, in the causal data fusion setting, \(\sigma_{b}^{2}(Q_{j})=\mathrm{O}(|\mathcal{O}_{j}|^{-1})\) is expected to be of a smaller order term compared to \(\sigma_{u}^{2}(Q_{j})=\mathrm{O}(|\mathcal{R}_{j}|^{-1})\)which is a consequence of the relative sample size between the trial and the observational data. Thus, one can just avoid estimating the negligible term \(\sigma_{b}^{2}(Q_{j})\). For the last term, \(\widehat{b(Q_{j})}=\hat{\tau}_{b}(Q_{j})-\hat{\tau}_{u}(Q_{j})\) serves as a natural estimator of the bias \(b(Q_{j})\). This leads to the following estimator of \(w^{*}(Q_{j})\) and the corresponding fused estimator

\[\hat{w}_{of}(Q_{j})=\hat{\sigma}_{u}^{2}(Q_{j})/(\hat{\sigma}_{u}^{2}(Q_{j})+( \widehat{b(Q_{j})})^{2})\;\;\text{and}\] (5)

\[\hat{\tau}_{of}(Q_{j})=\hat{w}_{of}(Q_{j})\hat{\tau}_{b}(Q_{j})+(1-\hat{w}_{of} (Q_{j}))\hat{\tau}_{u}(Q_{j}).\] (6)

A fused estimator of the HTE function \(\tau(\cdot)\) under the partition \(\bm{Q}\) can thus be defined as \(\hat{\tau}_{\bm{Q}}(\bm{x})=\sum_{j=1}^{J}\hat{\tau}_{of}(Q_{j})\mathrm{I}\{\bm {x}\in Q_{j}\}\).

### Adaptive fusion for segmentation

In order to obtain a tree-based partition \(\bm{Q}\) designed for the fusion strategy (6), a split criterion is required, which is sufficient to be defined only at the root node given the recursive nature of the partitioning. We follow the honest estimation approach (Athey and Imbens, 2016) to prevent overfitting. Specifically, given a fraction \(0<r<1\) (typically \(r=0.5\)), \(\lfloor rn\rfloor\) observations are sampled without replacement from the trial data of sample size \(n\) for the tree structure estimation, while the rest of observations are used for local estimation of the HTE in each leaf node. Let the index sets of the trial data used for the partition and the HTE estimation be \(\mathcal{R}^{t}\) and \(\mathcal{R}^{e}\), respectively. We do not further split the observational data to reduce uncertainty, since we have already partitioned the trial data to avoid overfitting.

The conventional criterion for growing a regression tree chooses the index of the split variable and its split value at the root node by minimizing the following goodness-of-fit criterion

\[(\hat{q},\hat{c})=\arg\min_{\hat{q}\in[p],\in\mathbb{R}}\left(\sum_{i\in \mathcal{\widehat{R}}_{L}^{t}}\left(\widetilde{Y}_{i}-\hat{\tau}_{u}(\widehat {Q}_{L},\mathcal{R}^{t})\right)^{2}+\sum_{i\in\mathcal{\widehat{R}}_{R}^{t}} \left(\widetilde{Y}_{i}-\hat{\tau}_{u}(\widehat{Q}_{R},\mathcal{R}^{t}) \right)^{2}\right),\] (7)

where \(\widehat{Q}_{L}=\{\bm{x}|\bm{x}_{\hat{q}}\leq\hat{c}\}\), \(\mathcal{\widehat{R}}_{L}^{t}=\{i|i\in\mathcal{R}^{t},\bm{X}_{i}\in\widehat{Q }_{L}\}\) and \(\hat{\tau}_{u}(\widehat{Q}_{L},\mathcal{R}^{t})=(1/|\{i|i\in\mathcal{R}^{t}, \bm{X}_{i}\in\widehat{Q}_{L}\}|)\sum_{i\in\{i|i\in\mathcal{R}^{t},\bm{X}_{i} \in\widehat{Q}_{L}\}}\widetilde{Y}_{i}\), and \(\widehat{Q}_{R}\), \(\widehat{\mathcal{R}}_{R}^{t}\) and \(\hat{\tau}_{u}(\widehat{Q}_{R},\mathcal{R}^{t})\) can be defined correspondingly. Given a tree grown under (7), we fuse the trial data indexed by \(\mathcal{R}^{e}\) and the observational data indexed by \(\mathcal{O}\) at each leaf node according to (6) and refer to the resulting tree estimator as a **Shrinkage Tree (ST)**. A direct modification of (7), which aligns more with the fused estimator at the leaf nodes, should be

\[(\hat{q},\hat{c})=\arg\min_{\hat{q}\in[p],\in\mathbb{R}}\left(\sum_{i\in \mathcal{\widehat{R}}_{L}^{t}}\left(\widetilde{Y}_{i}-\hat{\tau}_{of}(\widehat {Q}_{L})\right)^{2}+\sum_{i\in\mathcal{\widehat{R}}_{R}^{t}}\left(\widetilde {Y}_{i}-\hat{\tau}_{of}(\widehat{Q}_{R})\right)^{2}\right),\] (8)

where \(\hat{\tau}_{of}(\widehat{Q}_{L})=\hat{w}_{of}(\widehat{Q}_{L})\hat{\tau}_{b}( \widehat{Q}_{L})+(1-\hat{w}_{of}(\widehat{Q}_{L}))\hat{\tau}_{u}(\widehat{Q}_{ L},\mathcal{R}^{t})\) and \(\hat{\tau}_{of}(\widehat{Q}_{R})\) is defined correspondingly. The replacement of the unbiased estimators in (7) with the fused estimators in (8) facilitates a goodness-of-fit criterion of the proposed fusion strategy.

Alternatively, (7) can be interpreted as minimizing the sum of the estimated MSEs of the unbiased estimators at the child nodes, if the two terms on the right-hand side of (7) are divided by the square of their respective sample sizes. By contrast, since the fused estimator \(\hat{\tau}_{of}\) reduces variance by shrinking the original unbiased estimator to a potentially biased estimator, simply comparing the fused estimators with the outcomes of the trial data as in (8) fails to capture the variability at the child nodes. Instead, an appropriate criterion shall respect the MSE of the fused estimator. To this end, we introduce the following split criterion

\[(\hat{q},\hat{c})=\arg\min_{\hat{q}\in[p],\in\mathbb{R}}\left((1-\hat{w}_{of}( \widehat{Q}_{L}))\hat{\sigma}_{u}^{2}(\widehat{Q}_{L},\mathcal{R}^{t})+(1- \hat{w}_{of}(\widehat{Q}_{R}))\hat{\sigma}_{u}^{2}(\widehat{Q}_{R},\mathcal{R}^ {t})\right),\] (9)

where \((1-\hat{w}_{of}(\widehat{Q}_{L}))\hat{\sigma}_{u}^{2}(\widehat{Q}_{L},\mathcal{R }^{t})\) and \((1-\hat{w}_{of}(\widehat{Q}_{R}))\hat{\sigma}_{u}^{2}(\widehat{Q}_{R},\mathcal{R }^{t})\) estimate the MSE of \(\hat{\tau}_{of}(\widehat{Q}_{L})\) and \(\hat{\tau}_{of}(\widehat{Q}_{R})\), respectively, according to formula (2). Compared to (7), the proposed criterion incorporates the additional information from the observational data into each node split in an adaptive manner by simply re-scaling the estimated MSE of the unbiased estimator.

**Comment** The criterion (9) offers the benefit of local adjustment, which can be intuitively justified. In sub-regions where the observational data exhibit moderate confounding biases, this criterion improves tree building by providing a sharper assessment of the variability of the fused estimator. On the other hand, for sub-regions where the observational data exhibit substantial confounding biases, the estimated weights of those sub-regions approach zero according to (5). In such cases, the criterion reduces to the conventional criterion (7), except for the standardization of the square of the sample size. It is worth mentioning that all the local adjustments achieved by applying this adaptive fusion strategy are data-driven, namely one can just avoid global modeling of the confounding bias function, which requires domain-specific knowledge of the observational studies. Additionally, it also enables the exclusion of the global impact of extremely large confounding biases of the observational data that only exist in certain sub-regions of the feature space.

We denote the partition obtained under criterion (9) as \(\widehat{\bm{Q}}_{of}=\{\widehat{Q}_{of,1},\widehat{Q}_{of,2},\cdots,\widehat{Q }_{of,|\widehat{\bm{Q}}_{of}|}\}\), and the corresponding tree-based estimator of the HTE is defined as

\[\hat{\tau}_{fast}(\bm{x})=\sum_{j=1}^{|\widehat{\bm{Q}}_{of}|} \hat{\tau}_{of}^{e}(\widehat{Q}_{of,j})\mathrm{I}\{\bm{x}\in\widehat{Q}_{of,j}\},\] (10)

where the superscript "e" is to show that the RCT data used to construct the fused estimator at the leaf node is indexed by \(\mathcal{R}^{e}\) and "fast" is an acronym for the name Fused and Accurate Shrinkage Tree, which is due to the data fusion nature of the criterion (9), the shrinkage-type leaf node estimator (6) and its accuracy in terms of the MSE.

### Ensemble fusion

To reduce overfitting, improve robustness against outliers, and enhance generalization, we introduce the bagged version (Hastie et al., 2009) of the FAST, referred to as the rfFAST, as follows: We randomly draw index sets \(\mathcal{R}^{*}\) of size \(n\) and \(\mathcal{O}^{*}\) of size \(m\), separately from \(\mathcal{R}\) and \(\mathcal{O}\) with replacement. We repeat the process \(B\) times, resulting in \(\{\mathcal{R}^{*,(b)},\mathcal{O}^{*,(b)}\}_{b=1}^{B}\). Then, \(B\) estimators \(\hat{\tau}_{fast}^{*,(b)}(\bm{x})\) can be calculated based on the trial data indexed by \(\mathcal{R}^{*,(b)}\) and the observational data index by \(\mathcal{O}^{*,(b)}\). We then define \(\hat{\tau}_{rffast}(\bm{x})=(1/B)\sum_{b=1}^{B}\hat{\tau}_{fast}^{*,(b)}(\bm{x})\). For the construction of the prediction intervals, see Zhang et al. (2020).

## 4 Theoretical guarantee

In this section, we formally establish the benefits of the proposed split criterion (9) compared with the conventional criterion (7). To present the theoretical result, we first pose the following regularity conditions that are standard in literature (see e.g., Gyorfi et al., 2002 and Scornet et al., 2015).

**Assumption 2**.: _(i) There exists a positive constant \(\lambda<\infty\) such that \(\mathbb{E}\{\exp(\lambda\tilde{Y}^{2})|S=i\}<\infty\) for \(i=0,1\). (ii) There exists positive constants \(\sigma_{\min}<\infty\) such that \(\sigma_{\min}^{2}<\mathrm{Var}(\tilde{Y}|\bm{X}=\bm{x},S=0)\) for any \(\bm{x}\in\mathcal{X}\)._

**Theorem 1** (MSE reduction of the proposed split criterion).: _Let \(\theta=(q,c)\) and \(\Theta=[p]\times\mathbb{R}\). Suppose the node that needs to be partitioned is \(Q_{j}\), under which the sample sizes of the trial data and observational data are \(n_{j}\) and \(m_{j}\), respectively. Let \(M(\theta)\) and \(M_{of}(\theta)\) be the sum of MSEs of the conventional HTE estimator and the fused HTE estimator on the two child nodes of \(Q_{j}\) split by \(\theta\), respectively. Denote \(b_{\max}=\sup_{\bm{x}\in Q_{j}}|\{\mathbb{E}(\tilde{Y}|\bm{X}=\bm{x},S=0)- \mathbb{E}(\tilde{Y}|\bm{X}=\bm{x},S=1)\}|\). Let \(\hat{\theta}\) be the solution of the conventional split criterion (7) and \(\hat{\theta}_{of}\) be the solution of the proposed split criterion (9). Under Assumptions 1-2, we have_

_(i) For any \(\theta\in\Theta\),_

\[\frac{M_{of}(\theta)}{M(\theta)}-1\leq-\frac{\sigma_{\min}^{2}}{ \sigma_{\min}^{2}+n_{j}b_{\max}^{2}}.\] (11)_(ii) With probability at least \(1-C_{1}e^{-t}\) for some positive constant \(C_{1}<\infty\), it holds that_

\[M(\hat{\theta})-M(\theta^{*})\leq C_{2}\frac{t+\log(pn_{j})\log^{4}( n_{j})}{n_{j}},\] (12) \[\text{and}\;\;M_{of}(\hat{\theta}_{of})-M_{of}(\theta^{*}_{of}) \leq C_{3}\left(\frac{t+\log(pn_{j})\log^{4}(n_{j})}{m_{j}}+\frac{t+\log(pn_{j} )\log^{4}(n_{j})}{n_{j}}\right),\] (13)

_for some positive constant \(C_{2},C_{3}<\infty\), where \(\theta^{*}\) and \(\theta^{*}_{of}\) are oracle splits defined as_

\[\theta^{*}=\operatorname*{arg\,min}_{\theta\in\Theta}M(\theta)\;\;\text{and} \;\;\theta^{*}_{of}=\operatorname*{arg\,min}_{\theta\in\Theta}M_{of}(\theta).\]

In the above theorem, the (i) part establishes a uniform MSE reduction result for any split choice \(\theta\in\Theta\) of the proposed split criterion (9). As revealed in (11), the criterion (9) leads to larger MSE reduction on the nodes with a larger variance of \(\tilde{Y}\) and less bias of the observational data. In addition, the upper bound in (11) decreases as the node sample size \(n_{j}\) decreases, implying that our proposed criterion leads to increasing relative benefits as the tree grows deeper. Besides, in the (ii) part we present non-asymptotic bounds for the discrepancies between the MSEs under the empirically estimated splits and the oracle splits, showing that the MSEs under the estimated splits can achieve a fast convergence rate. As a direct consequence of Theorem 1, the consistency of our final HTE estimator (10) can be established, since it is known from Sornet et al. (2015) and Athey et al. (2019) that the conventional tree-based estimator using only the trial data is mean-squared consistent, and our proposed method leads to a reduced MSE.

**Proposition 1** (Consistency of \(\hat{\tau}_{fast}\)).: _For almost every \(\bm{x}\in[-1,1]^{p}\), we have \(\hat{\tau}_{fast}(\bm{x})\to\tau(\bm{x})\) in probability as \(n,m\to\infty\)._

## 5 Experiments

In this section, we demonstrate the results of a series of experiments to answer the following two questions: (i) Whether the proposed method can effectively alleviate the impact of confounding bias of observational data and limited sample size of trial data; (ii) Whether the techniques we proposed including local fusion in tree leaves and adaptive fusion in partitioning are valid, respectively.

In consequence, we conducted experiments on both simulated and real-world datasets to verify the effectiveness of our method. We evaluated our method against both traditional tree-based and data fusion-based casual methods. The former includes the classical Transformed Outcome Honest Tree (HT) Athey and Imbens (2016) and its ensemble version Generalized Random Forest (GRF) Athey et al. (2019). The latter includes the simplest fusion estimator (SF) training both trial data and observational data together without distinction and the KPS estimators Kallus et al. (2018). In order to facilitate better comparison and understanding of our proposed method, we demonstrate three versions: the simple implementation, Shrinkage Tree (ST), described in Section 3.1; the improved version, Fused and Accurate Shrinkage Tree (FAST), described in Section 3.2; and its final ensemble version rfFAST described in Section 3.3. The results of each simulation experiment were based on \(B=100\) replications. The ensemble size for all the ensemble estimators was set to \(100\). For the tree estimators, the minimum number of observations required to be at a leaf node was set to \(5\) and the maximum depth of the tree was set to \(10\).

### Simulation

We conducted two sets of simulation experiments to evaluate the finite sample performance of the fused estimator and various baseline estimators. In both experiments, we first generated the pre-treatment covariates \(\bm{X}=(X_{1},X_{2},\cdots,X_{p})^{T}\) from \(\operatorname{Uniform}[-1,1]^{p}\) and the unobserved variable \(U\) from \(\operatorname{N}(0,1)\). Then, we generated the potential outcomes by \(Y(d)=d\tau(\bm{X})+\sum_{j=1}^{p}X_{j}+1.5U+\epsilon(d)\), where \(\tau(\bm{X})=1+X_{1}+X_{1}^{2}+X_{2}+X_{2}^{2}\) and \(\epsilon(d)\sim\operatorname{N}(0,1)\) for \(d=0,1\). Thus The treatment assignments for the trial sample of size \(n\) and the observational sample of size \(m\) were generated as follows: \(D|(\bm{X},U,S=1)\sim\operatorname{Ber}(0.5)\) and \(D|(\bm{X},U,S=0)\sim\operatorname{Ber}(1/(1+\exp(-\beta U-0.5X_{1})))\). Thus, the magnitude of \(\beta\) controls the strength of the unmeasured confounding: a larger \(\beta\) leads to a larger confounding bias. The test data \(X_{test,j}\) for \(1\leq j\leq p\) were generated from \(\operatorname{Uniform}(-1,1)\) with sample size \(1000\).

In the first experiment, we aim to verify the effectiveness of the proposed data fusion strategy via an ablation study. We compared the robustness of the \(\mathrm{ST}\) and the \(\mathrm{FAST}\) against different levels of confounding bias parameter \(\beta\). Two baselines were considered: (i) the \(\mathrm{HT}\) using only the trial data and (ii) the \(\mathrm{SF}\) estimator obtained by directly merging all the available data and constructing a Fit-Based Causal Tree (Athey and Imbens, 2016). We set the sample sizes of the trial data and the observational data be \(n=200\) and \(m=2000\), respectively, the dimension of covariates \(p=5\) and \(\beta\in\{0.1c|c\in\mathbb{N},c\leq 19\}\). The following three conclusions could be drawn from Figure 2: (1) When confounding bias in observational data was small, the simple fusion (SF) strategy can effectively improve the model performance. But when it became large, the \(\mathrm{SF}\) was very vulnerable to confounding bias in observational data; (2) Even with the increase of \(\beta\), both \(\mathrm{ST}\) and \(\mathrm{FAST}\) consistently showed resistance to confounding bias; (3) \(\mathrm{FAST}\) was significantly better than other methods including \(\mathrm{ST}\), which verified the effectiveness of our proposed split criterion (9) numerically.

In the second experiment, we evaluated the RMSEs with respect to different \(n\) and \(\beta\). We set \(m=2000\) and \(p=5\). We included seven estimators in the analysis: The first two estimators were calculated purely based on the trial data: (i) the Transformed Outcome Honest Tree (\(\mathrm{HT}\)) (Athey and Imbens, 2016) and (ii) the Generalized Random Forest (\(\mathrm{GRF}\)) (Athey et al., 2019). The rest estimators were calculated using different data fusion strategies: (iii) the Shrinkage Tree (\(\mathrm{ST}\)) estimator,(iv) the Fused and Accurate Shrinkage Tree (\(\mathrm{FAST}\)) estimator, (v& vi) the \(\mathrm{KPS}\) estimators (Kallus et al., 2018) with a parametric (OLS) estimator and a non-parametric (Random Forest) specification of the confounding function, respectively and (vii) the bagged \(\mathrm{FAST}\) estimator (\(\mathrm{rfFAST}\)).

Table 1 reports the RMSEs of the seven estimators, conveying a good estimation accuracy of both the \(\mathrm{FAST}\) and its ensemble version \(\mathrm{rfFAST}\). Among the three individual estimators, the \(\mathrm{ST}\) and \(\mathrm{FAST}\), exhibited superior performance compared to the \(\mathrm{HT}\), and the \(\mathrm{FAST}\) outperformed the \(\mathrm{ST}\). These relative performances provided support for the \(\mathrm{FAST}\) approach compared to the classical honest regression tree, the proposed split criterion (9), and the shrinkage estimation strategy (6), which are implemented progressively. Among the three ensemble estimators, the rfFAST estimator demonstrated the best performance among all the six combinations of the trial sample size \(n\) and the confounding bias parameter \(\beta\). On the other hand, the performance of the \(\mathrm{KPS}\) estimators appeared to be less stable. The \(\mathrm{KPS}_{ols}\) outperformed the \(\mathrm{GRF}\) only when the trial sample size was relatively large (\(n=200\)). Under the non-parametric specification of the confounding function, the \(\mathrm{KPS}_{RF}\) did not gain benefit from incorporating the observational data and was consistently inferior to the baseline estimator \(\mathrm{GRF}\).

Figure 2: The averaged root mean square error (RMSE) (mean with s.e. error bars) of the estimators on simulation datasets with different levels of the confounding bias parameter \(\beta\).

### Real-world data

In this sub-section, we report an analysis of the Tennessee Student/Teacher Achievement Ratio (STAR) Experiment (Krueger, 1999) to demonstrate the proposed \(\mathrm{FAST}\) for the HTE estimation. We aim at quantifying the treatment effect of the class size on the student's academic achievement.

**Data description** The STAR Experiment was a randomized controlled trial conducted in the late 1980s. Students were randomly assigned to one of the two types of classes during the first school year: \(D=1\) for small classes containing \(13-17\) pupils and \(D=0\) for regular classes containing \(22-25\) pupils. The outcome \(Y\) is the average of the listening, reading, and math standardized tests at the end of first grade. The vector of covariates \(X\) includes gender, race, birth month, birthday, birth year, free lunch given or not, and teacher id. This made a universal sample of \(4218\) students, among which \(2413\) were randomly assigned to regular-size classes (\(D=0\)) and \(1805\) to small classes (\(D=1\)).

**Ground-truth** In practice, the ground-truth \(\tau(\cdot)\) is not accessible, so we replaced it with an estimate calculated by a generalized random forest (Athey et al., 2019) based on all the \(4218\) observations.

**Construction of the trial, observational and test data** Following Kallus et al. (2018), we introduced confounding bias by splitting the population over a variable which is known to strongly affect the observed outcome \(Y\)(Krueger, 1999): rural or inner-city (\(U=1\), \(2811\) students) and urban or suburban (\(U=0\), \(1407\) students). The trial data were generated by randomly sampling a fraction \(h\) of the students with \(U=1\), where \(h\) ranges from \(0.1\) to \(0.5\). The observational data were constructed as follows: From students with \(U=1\), we took the controls (\(D=0\)) that were not sampled in trial data, and the treated (\(D=1\)) whose outcomes were in the lower half of outcomes among students with \(D=1\) and \(U=1\); From students with \(U=0\), we took all of the controls (\(D=0\)), and the treated (\(D=1\)) whose outcomes were in the lower half of outcomes among students with \(D=1\) and \(U=0\). The test data consisted of a held-out sub-sample of all the observations in the universal sample excluding the trial data.

**Results** We compared the performance of the \(\mathrm{rfFAST}\) with various baseline estimators. In particular, the \(\mathrm{NF}\) and the \(\mathrm{SF}\) estimators were constructed using the Random Forest regressor. The \(\mathrm{NF}\) estimator utilized only trial data, while the \(\mathrm{SF}\) estimator utilized both trial data and observational data together without distinction. As shown in Figure 3, the proposed \(\mathrm{rfFAST}\) method consistently outperformed other estimators.

## 6 Discussion

This paper explores the estimation of heterogeneous treatment effects (HTE) within the framework of causal data fusion. Drawing inspiration from the classical James-Stein shrinkage estimation (Green and Strawderman, 1991) approach, the authors introduce a new method called Fused and Accurate Shrinkage Tree (\(\mathrm{FAST}\)) that effectively incorporates observational data in both feature

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \(n\) & \(\beta\) & HT & ST & FAST & \(\mathrm{GRF}\) & \(\mathrm{KPS}_{ols}\) & \(\mathrm{KPS}_{RF}\) & \(\mathrm{rfFAST}\) \\ \hline \multirow{4}{*}{100} & \multirow{4}{*}{0.5} & & 1.89 & 1.84 & & 1.33 & 1.73 & **0.84** \\  & & & (0.06) & (0.06) & & (0.04) & (0.03) & (0.02) \\  & & 2.28 & 1.90 & 1.85 & 1.19 & 1.29 & 1.65 & **0.89** \\  & & (0.06) & (0.05) & (0.05) & (0.02) & (0.04) & (0.03) & (0.02) \\  & & 2.0 & & 2.05 & 2.02 & & 1.28 & 1.71 & **0.98** \\  & & & (0.05) & (0.04) & & (0.04) & (0.03) & (0.02) \\ \hline \multirow{4}{*}{200} & \multirow{4}{*}{1.0} & \multirow{4}{*}{2.20} & 1.87 & 1.71 & & 0.96 & 1.56 & **0.73** \\  & & & (0.04) & (0.04) & & (0.02) & (0.02) & (0.01) \\ \cline{1-1}  & & 2.20 & 1.98 & 1.83 & 1.12 & 0.97 & 1.59 & **0.84** \\ \cline{1-1}  & & (0.04) & (0.04) & (0.04) & (0.01) & (0.03) & (0.02) & (0.02) \\ \cline{1-1}  & & & 2.08 & 1.97 & & 1.01 & 1.57 & **0.92** \\ \cline{1-1}  & & & (0.03) & (0.03) & & (0.02) & (0.03) & (0.02) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The averaged RMSE (standard error in parentheses) of the estimators with respect to the trial sample size \(n\) and the confounding bias parameter \(\beta\). The best performance is marked in **bold**.

space segmentation and leaf node value estimation. This new approach is shown to outperform existing data fusion methods via numerical experiments.

The above estimation framework can be generalized to any data fusion problem if there exists an unbiased estimator and a biased estimator of some functions of interest. It would be worthwhile to explore the combination of the FAST method with other ensemble methods, such as the boosting and the grf-style (Athey et al., 2019) bagging, in addition to Breiman-style (Breiman, 2001) bagging used in rfFAST. Moreover, extending the framework to handle time-series observational data would be an interesting direction for future research. Additionally, investigating statistical inference under the proposed fusion framework would also be valuable.

## Acknowledgements

This work is supported by Ant Group through Ant Research Intern Program, and funded by National Natural Science Foundation of China Grants 92046021, 12071013403 and 12026607. We thank the editors and five anonymous referees for their constructive suggestions and comments.

## References

* Agarwal et al. (2022) Agarwal, A., Tan, Y. S., Ronen, O., Singh, C., and Yu, B. (2022). Hierarchical Shrinkage: Improving the accuracy and interpretability of tree-based models. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S., editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 111-135. PMLR.
* Angrist et al. (1996) Angrist, J. D., Imbens, G. W., and Rubin, D. B. (1996). Identification of causal effects using instrumental variables. _Journal of the American Statistical Association_, 91(434):444-455.
* Athey and Imbens (2016) Athey, S. and Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects: Table 1. _Proceedings of the National Academy of Sciences_, 113:7353-7360.

* Bareinboim and Pearl (2016) Bareinboim, E. and Pearl, J. (2016). Causal inference and the data-fusion problem. _Proceedings of the National Academy of Sciences_, 113:7345-7352.
* Breiman (2001) Breiman, L. (2001). Random Forests. _Machine Learning_, 45:5-32.

Figure 3: The RMSEs of the five estimators with respect to different sample sizes of the trial data, reflected by the fraction parameter \(h\). A large \(h\) means a large trial sample size.

Breiman, L., Friedman, J., Olshen, R., and Stone, C. (1984). _Classification And Regression Trees_. Chapman & Hall/CRC.
* Buchanan et al. (2018) Buchanan, A. L., Hudgens, M. G., Cole, S. R., Mollan, K. R., Sax, P. E., Daar, E. S., Adimora, A. A., Eron, J. J., and Mugavero, M. J. (2018). Generalizing Evidence from Randomized Trials Using Inverse Probability of Sampling Weights. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 181(4):1193-1209.
* Colnet et al. (2020) Colnet, B., Mayer, I., Chen, G., Dieng, A., Li, R., Varoquaux, G., Vert, J.-P., Josse, J., and Yang, S. (2020). Causal inference methods for combining randomized trials and observational studies: a review. _arXiv preprint arXiv:2011.08047_.
* Cui et al. (2023) Cui, Y., Pu, H., Shi, X., Miao, W., and Tchetgen, E. T. (2023). Semiparametric Proximal Causal Inference. _Journal of the American Statistical Association_, 0(0):1-12.
* Dahabreh et al. (2019) Dahabreh, I. J., Robertson, S. E., Tchetgen, E. J., Stuart, E. A., and Hernan, M. A. (2019). Generalizing causal inferences from individuals in randomized trials to all trial-eligible individuals. _Biometrics_, 75(2):685-694.
* de Luna and Johansson (2014) de Luna, X. and Johansson, P. (2014). Testing for the Unconfoundedness Assumption Using an Instrumental Assumption. _Journal of Causal Inference_, 2(2):187-199.
* Edwards et al. (1999) Edwards, S., Lilford, R., Braunholtz, D., Jackson, J., Hewison, J., and Thornton, J. (1999). Ethical issues in the design and conduct of Randomised Controlled Trials. _Health technology assessment (Winchester, England)_, 2:i-vi, 1.

* Efron and Morris (1973) Efron, B. and Morris, C. (1973). Stein's estimation rule and its competitors-an empirical bayes approach. _Journal of the American Statistical Association_, 68(341):117-130.
* Frangakis and Rubin (2002) Frangakis, C. and Rubin, D. B. (2002). Principal stratification in causal inference. _Biometrics_, 58.

* Glass et al. (2013) Glass, T. A., Goodman, S. N., Hernan, M. A., and Samet, J. M. (2013). Causal inference in public health. _Annual review of public health_, 34:61-75.
* J AMER STATIST ASSN_, 86:1001-1006.
* Gyorfi et al. (2002) Gyorfi, L., Kohler, M., Krzyzak, A., and Walk, H. (2002). _A distribution-free theory of nonparametric regression_, volume 1. Springer.

* Hall (1992) Hall, P. (1992). _The bootstrap and Edgeworth expansion_. Springer-Verlag New York.
* Hastie et al. (2009) Hastie, T., Tibshirani, R., and Friedman, J. (2009). _The Elements of Statistical Learning : Data Mining, Inference, and Prediction_. Springer series in statistics. Springer, 2nd edition.
* Imbens and Rubin (2016) Imbens, G. W. and Rubin, D. B. (2016). _Causal inference for statistics, social, and biomedical sciences: An introduction_. Taylor & Francis.
* Kallus et al. (2018) Kallus, N., Puli, A. M., and Shalit, U. (2018). Removing hidden confounding by experimental grounding. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc.
* Kempper et al. (2019)* Kosorok and Laber (2019) Kosorok, M. and Laber, E. (2019). Precision medicine. _Annual Review of Statistics and Its Application_, 6:263-286.
* Krueger (1999) Krueger, A. (1999). Experimental Estimates Of Education Production Functions. _The Quarterly Journal of Economics_, 114:497-532.
* Kuroki and Pearl (2014) Kuroki, M. and Pearl, J. (2014). Measurement bias and effect restoration in causal inference. _Biometrika_, 101(2):423-437.
* Kunzel et al. (2019) Kunzel, S., Sekhon, J., Bickel, P., and Yu, B. (2019). Meta-learners for estimating heterogeneous treatment effects using machine learning. _Proceedings of the National Academy of Sciences_, 116:4156-4165.
* Mentch and Zhou (2020) Mentch, L. and Zhou, S. (2020). Randomization as regularization: A degrees of freedom explanation for random forest success. _Journal of Machine Learning Research_, 21(171):1-36.
* Miao et al. (2018) Miao, W., Geng, Z., and Tchetgen Tchetgen, E. J. (2018). Identifying causal effects with proxy variables of an unmeasured confounder. _Biometrika_, 105(4):987-993.
* Nasseri et al. (2022) Nasseri, K., Singh, C., Duncan, J., Kornblith, A., and Yu, B. (2022). Group probability-weighted tree sums for interpretable modeling of heterogeneous data.
* Nie and Wager (2020) Nie, X. and Wager, S. (2020). Quasi-oracle estimation of heterogeneous treatment effects. _Biometrika_, 108(2):299-319.
* Powers et al. (2017) Powers, S., Qian, J., Jung, K., Schuler, A., Shah, N., Hastie, T., and Tibshirani, R. (2017). Some methods for heterogeneous treatment effect estimation in high-dimensions. _Statistics in Medicine_, 37.
* Radcliffe and Surry (2012) Radcliffe, N. J. and Surry, P. D. (2012). Real-world uplift modelling with significance-based uplift trees.
* Rosenbaum and Rubin (1983) Rosenbaum, P. and Rubin, D. (1983). Assessing Sensitivity to an Unobserved Binary Covariate in an Observational Study with Binary Outcome. _Journal of the Royal Statistical Society. Series B (Methodological)_, 45(2):212-218.
* Rubin (1974) Rubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. _Journal of Educational Psychology_, 66.

* Shi et al. (2020) Shi, X., Miao, W., Nelson, J., and Tchetgen, E. (2020). Multiply robust causal inference with double-negative control adjustment for categorical unmeasured confounding. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 82.
* Shi et al. (2022) Shi, X., Pan, Z., and Miao, W. (2022). Data integration in causal inference. _WIREs Computational Statistics_, 15(1).
* Taddy et al. (2016) Taddy, M., Gardner, M., Chen, L., and Draper, D. (2016). A nonparametric bayesian analysis of heterogenous treatment effects in digital experimentation. _Journal of Business & Economic Statistics_, 34.
* Tang et al. (2022) Tang, C., Wang, H., Li, X., Cui, Q., Zhang, Y.-L., Zhu, F., Li, L., Zhou, J., and Jiang, L. (2022). Debiased causal tree: Heterogeneous treatment effects estimation with unmeasured confounding. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, _Advances in Neural Information Processing Systems_, volume 35, pages 5628-5640. Curran Associates, Inc.
* Turney and Wildeman (2015) Turney, K. and Wildeman, C. (2015). Detrimental for some? heterogeneous effects of maternal incarceration on child wellbeing. _Criminology & Public Policy_, 14.
* Wager and Athey (2018) Wager, S. and Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. _Journal of the American Statistical Association_, 113(523):1228-1242.
* Wager et al. (2017)Wu, L. and Yang, S. (2022). Transfer learning of individualized treatment rules from experimental to real-world data. _Journal of Computational and Graphical Statistics_, 0(0):1-10.
* Yang et al. (2020) Yang, S., Zeng, D., and Wang, X. (2020). Improved inference for heterogeneous treatment effects using real-world data subject to hidden confounding. _arXiv preprint arXiv:2007.12922_.
* Zhang and Tchetgen Tchetgen (2022) Zhang, B. and Tchetgen Tchetgen, E. J. (2022). A Semi-Parametric Approach to Model-Based Sensitivity Analysis in Observational Studies. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 185:S668-S691.
* Zhang et al. (2020) Zhang, H., Zimmerman, J., Nettleton, D., and Nordman, D. J. (2020). Random Forest Prediction Intervals. _The American Statistician_, 74(4):392-406.

## Appendix A Additional figures

## Appendix B Pre-processing of the real-world data

In the STAR dataset, each of the pre-treatment covariate \(X_{j}\) (\(1\leq j\leq p\)) was standardized to a range of \(-1\) to \(1\), and the outcome variable \(Y\) was standardized to a range of \(0\) to \(100\).

## Appendix C Proof of Theorem 1

The proof follows the similar arguments as in Gyorfi et al. (2002) and Scornet et al. (2015). It is sufficient to show the result at the root node given the recursive nature of the partitioning. We will use the following notations in the sequel. We denote \(\mathbb{E}_{T},\mathbb{P}_{T}\) and \(\mathbb{E}_{O},\mathbb{P}_{O}\) as the expectation and probability under trial data and observational data, respectively. We let \(Z=(X,\tilde{Y})\). For any \(q\in[p]\) and \(c\in\mathbb{R}\), let \(\theta=(q,c)\) and the corresponding two partitioned notes are denoted as \(Q_{L}(\theta)=\{x|x_{q}\leq c\}\) and \(Q_{R}(\theta)=\{x|x_{q}>c\}\). The parameter space of \(\theta\) is denoted as \(\Theta=[p]\times\mathbb{R}\). Let \(\mu_{L}\) and \(\mu_{R}\) be the predictions for \(Y\) on \(Q_{L}(\theta)\) and \(Q_{R}(\theta)\), respectively and denote \(\tau=(\tau_{L},\tau_{R})\). Let \(M^{i}(\theta)\) and \(M^{i}_{oj}(\theta)\) be the MSEs of the conventional HTE estimator and the fused HTE estimator on the child nodes of \(Q_{i}(\theta)\), respectively, for \(i\in\{R,L\}\).

**(i).** For any \(\theta\in\Theta\), according to Equation (2) in the main paper we have

\[M^{i}_{oj}(\theta)=(1-w_{i}(\theta))M^{i}(\theta),\]

for \(i\in\{R,L\}\), where the weight \(w_{i}(\theta)\) satisfies

\[w_{i}(\theta)\asymp\frac{\sigma_{u}^{2}(Q_{i})/n}{\sigma_{u}^{2}(Q_{i})/n+b^{ 2}(\theta)},\]

by Equation (5) in the main paper, which is lower bounded by \(\frac{\sigma_{\min}^{2}}{\sigma_{\min}^{2}+nb_{\max}^{2}}\), where \(\sigma_{\min}^{2}<\mathrm{Var}(\tilde{Y}|\bm{X}=\bm{x},S=0)\) and \(b_{\max}=\sup_{\bm{x}\in Q_{j}}|\{\mathbb{E}(\tilde{Y}|\bm{X}=\bm{x},S=0)- \mathbb{E}(\tilde{Y}|\bm{X}=\bm{x},S=1)\}|\)

Figure 4: The averaged root mean square error (RMSE) (mean with \(2\times\)s.d. error bars) of each algorithm on multiple simulation datasets with different levels of the confounding bias parameter \(\beta\).

Therefore, we conclude that

\[\frac{M_{of}(\theta)}{M(\theta)}-1\leq-\frac{\sigma_{\min}^{2}}{\sigma_{\min}^{2}+ nb_{\max}^{2}},\]

which reveals the MSE reduction effect of the proposed split criterion.

**(ii).** The proof includes two parts. In Part 1, we will derive the bounds for the discrepancies between the MSEs under the empirically estimated split and the oracle split under the conventional criterion, and in Part 2 the similar results under the proposed split criterion.

**Part 1.** We define the following criterion function:

\[\ell_{n}(\theta,\tau,\mathcal{R}_{n}^{t}) =\frac{1}{n}\sum_{i=1}^{n}\left\{(\tilde{V}_{0,i}-\tau_{L})^{2}I \{X_{0,i}\in Q_{L}(\theta)\}+(\tilde{V}_{0,i}-\tau_{R})^{2}I\{X_{0,i}\in Q_{R}( \theta)\}\right\}\] \[=:\ell_{n}^{L}(\theta,\tau_{L},\mathcal{R}_{n}^{t})+\ell_{n}^{R} (\theta,\tau_{R},\mathcal{R}_{n}^{t}).\]

For \(i\in\{L,R\}\), let

\[\mathcal{L}^{i}(\theta,\tau_{i})=\mathbb{E}_{T}\left\{\ell_{n}^{i}(\theta, \tau_{i},\mathcal{R}_{n}^{t})\right\}\ \ \text{and}\ \ \mathcal{L}(\theta,\tau)=\mathcal{L}_{n}^{L}(\theta,\tau_{L})+\mathcal{L}_{n }^{R}(\theta,\tau_{R})\] (14)

Then \(\mathcal{L}^{i}(\theta,\tau_{i})\) represents the MSE of \(\tau_{i}\) on the region \(Q_{i}(\theta)\). For a given split \(\theta=(q,c)\), it is straightforward to see that the optimal \(\tau(\theta)=(\tau_{L}(\theta),\tau_{R}(\theta))\) is given by

\[\tau_{i}(\theta)=\operatorname*{arg\,min}_{\tau_{i}\in\mathbb{R}}\ell_{n}^{i }(\theta,\tau_{i},\mathcal{R}_{n}^{t})=\mathbb{E}_{n}\left\{\tilde{V}_{0}|X_{ 0}\in Q_{i}(\theta)\right\}\]

for \(i\in\{L,R\}\), which is the sample mean of \(Y\) on the region \(Q_{i}(\theta)\). Therefore, by the definition of \(M^{i}(\theta)\), it holds that \(\mathcal{L}^{i}(\theta,\tau_{i}(\theta))=M^{i}(\theta)\) for \(i\in\{L,R\}\). The optimal split \(\theta_{0}=(q_{0},c_{0})\) on the population level is defined via minimizing the profiled criterion function:

\[(q_{0},c_{0})=\operatorname*{arg\,min}_{q\in[p],c\in\mathbb{R}}\left\{M^{L}( \theta)+M^{R}(\theta)\right\}=\operatorname*{arg\,min}_{q\in[p],c\in\mathbb{ R}}M(\theta).\]

Define \(M_{n}^{i}(\theta)=\ell_{n}^{i}(\theta,\tau_{i}(\theta),\mathcal{R}_{n}^{t})\) for \(i\in\{L,R\}\) and the empirical optimal split \(\widehat{\theta}=(\widehat{q},\widehat{c})\) is defined via minimizing the sample criterion function:

\[(\widehat{q},\widehat{c})=\operatorname*{arg\,min}_{q\in[p],c\in\mathbb{R}} \left\{M_{n}^{L}(\theta)+M_{n}^{R}(\theta)\right\}=:\operatorname*{arg\,min} _{q\in[p],c\in\mathbb{R}}M_{n}(\theta).\]

**Step 1** (Main error decomposition).

Now we will bound \(M(\widehat{\theta})-M(\theta_{0})\), which represents the discrepancy of the MSEs of the oracle and empirical split. To apply empirical process theories for stochastic error analysis, we will use a truncation argument. We let \(M_{n,\beta_{n}}^{i}(\theta,\pi_{i},\mathcal{R}_{n}^{t})=\mathbb{E}_{n}(T_{ \beta_{n}}\tilde{Y}-T_{\beta_{n}}\pi_{i}(\theta))^{2}I(X\in Q_{i}(\theta))\) and \(M_{\beta_{n}}^{i}(\theta)=\mathbb{E}_{T}\left\{M_{n,\beta_{n}}^{i}(\theta, \pi_{i}(\theta),\mathcal{R}_{n}^{t})\right\}\), where \(T_{\beta_{n}}x=:(|x|\wedge\beta_{n}){\rm sign}(x)\) for any \(\beta_{n}>0\). Correspondingly, let \(M_{\beta_{n}}(\theta)=M_{\beta_{n}}^{i}(\theta)+M_{\beta_{n}}^{R}(\theta)\) and \(M_{n,\beta_{n}}(\theta)=M_{n,\beta_{n}}^{L}(\theta)+M_{n,\beta_{n}}^{R}(\theta)\). Then we have the following error decomposition:

\[0<M(\widehat{\theta})-M(\theta_{0})\] \[= M(\widehat{\theta})-M_{\beta_{n}}(\widehat{\theta})-M(\theta_{0} )+M_{\beta_{n}}(\theta_{0})\] \[+M_{\beta_{n}}(\widehat{\theta})-M_{\beta_{n}}(\theta_{0})-2M_{n, \beta_{n}}(\widehat{\theta})+2M_{n,\beta_{n}}(\theta_{0})\] \[+2M_{n,\beta_{n}}(\widehat{\theta})-2M_{n}(\widehat{\theta})-2M_{ n,\beta_{n}}(\theta_{0})+2M_{n}(\theta_{0})\] \[+2M_{n}(\widehat{\theta})-2M_{n}(\theta_{0})\] \[=: S_{1,n}+S_{2,n}+S_{3,n}+S_{4,n}.\]

By the definition of \(\widehat{\theta}\), we have \(S_{4,n}\leq 0\). In following steps, we will bound \(S_{1,n},S_{2,n}\) and \(S_{3,n}\), respectively. The truncation level \(\beta_{n}\) is chosen as \(\beta_{n}=\beta_{0}\log(n)\) for \(\beta_{0}\geq 2\sigma_{Y}\).

**Step 2** (Bounding \(S_{1,n}\)).:

For any \(\theta\), it holds that

\[M^{i}(\theta)-M^{i}_{\beta_{n}}(\theta)= \mathbb{E}_{T}\left\{(\tilde{Y}-\widehat{\tau}_{i}(\theta))^{2}-(T _{\beta_{n}}\tilde{Y}-T_{\beta_{n}}\widehat{\tau}_{i}(\theta))^{2}I\{X\in Q_{i} (\theta)\}\right\}\] \[= \mathbb{E}_{T}\left\{(\tilde{Y}-T_{\beta_{n}}\tilde{Y})(\tilde{Y} +T_{\beta_{n}}\tilde{Y}-2\widehat{\pi}_{i}(\theta))I\{X\in Q_{i}(\theta)\}\right\}\] \[+\mathbb{E}_{T}\left\{(T_{\beta_{n}}\widehat{\pi}_{i}(\theta)- \widehat{\pi}_{i}(\theta))(T_{\beta_{n}}\tilde{Y}+T_{\beta_{n}}\widehat{\pi}_ {i}(\theta)-2\widehat{\pi}_{i}(\theta))I\{X\in Q_{i}(\theta)\}\right\}\] \[=: S_{5,n}+S_{6,n}.\]

For \(T_{1,n}\), by Cauchy-Schwarz inequality we have

\[|S_{5,n}|\leq\sqrt{\mathbb{E}_{T}(\tilde{Y}-T_{\beta_{n}}\tilde{Y})^{2}}\sqrt {\mathbb{E}_{T}(\tilde{Y}+T_{\beta_{n}}\tilde{Y}-2\widehat{\pi}_{i}(\theta))^ {2}}\lesssim\sqrt{\mathbb{E}_{T}(\tilde{Y}-T_{\beta_{n}}\tilde{Y})^{2}},\]

where the second inequality is because \(\mathbb{E}_{T}(\tilde{Y}^{2})\leq\infty\) and \(\mathbb{E}_{T}\left\{\widehat{\pi}_{i}^{2}(\theta)\right\}\leq\mathbb{E}_{T}( \tilde{Y}^{2})/|Q_{i}(\theta)|\). Since

\[I(|\tilde{Y}|>\beta_{n})\leq\frac{\exp(\sigma_{Y}|Y|^{2}/2)}{\sigma_{Y}\beta_ {n}^{2}/2},\]

therefore,

\[|T_{1,n}|\lesssim\sqrt{\mathbb{E}_{T}(\tilde{Y}-T_{\beta_{n}} \tilde{Y})^{2}}\leq\sqrt{\mathbb{E}_{T}\left\{|Y|^{2}\frac{\exp(\sigma_{Y}|Y|^ {2}/2)}{\sigma_{Y}\beta_{n}^{2}/2}\right\}}\leq\sqrt{\frac{2}{\sigma_{Y}} \mathbb{E}_{T}\exp(\sigma_{Y}|Y|^{2})}\exp(-\frac{\sigma_{Y}\beta_{n}^{2}}{4}).\]

Since \(\mathbb{E}_{T}\exp(\sigma_{Y}|Y|^{2})<\infty\) and \(\beta_{n}=\beta_{0}\log(n)\), we conclude that \(|S_{5,n}|\lesssim\frac{1}{n}\). With the same argument, we have \(S_{6,n}\lesssim\frac{1}{n}\), implying that

\[M(\theta)-M_{\beta_{n}}(\theta)\lesssim\frac{1}{n}\] (15)

for any \(\theta\in\Theta\). Therefore, the truncation error \(S_{1,n}\lesssim\frac{1}{n}\).

**Step 3** (Bounding \(S_{2,n}\)).:

Let \(M_{N,of}=\left\{f=(T_{\beta_{n}}\tilde{Y}-T_{\beta_{n}}\pi)I(X\in Q_{i}( \theta)):\theta=(q,c)\in[p]\times\mathbb{R}\right\}\). By applying Lemma 2 we obtain

\[\mathcal{N}_{1}(\delta,M_{N,of},z_{1}^{n})\leq(pn)^{2}\left(\frac{c\beta_{n}}{ \delta}\right)^{4},\]

where \(z_{1}^{n}\) is any set \(\{z_{1},\cdots,z_{n}\}\in[0,1]^{p}\times\mathcal{Y}\) and \(c>0\) is a universal constant. It follows from Lemma 1 that

\[\mathbb{P}_{T}\left\{\exists\theta\in\Theta:|M_{\beta_{n}}( \theta)-M_{n,\beta_{n}}(\theta)|\geq\frac{1}{2}(\alpha+\gamma+M_{\beta_{n}}( \theta))\right\}\] \[\leq 28(pn)^{2}\left(\frac{80c\beta_{n}^{2}}{\gamma}\right)^{4}\exp \left(-\frac{\alpha n}{1284\beta_{n}^{4}}\right)\] \[\lesssim \exp\left(-\frac{\alpha n}{\beta_{n}^{4}}+\log(pn)-\log(\gamma) \right).\]

Taking \(\gamma=1/n\) and \(\alpha=(t+\log(pn))\beta_{n}^{4}/n\) implies that with probability at least \(1-C_{1}e^{-t}\) for some universal constant \(C_{1}>0\),

\[\forall\theta\in\Theta,|M_{\beta_{n}}(\theta)-2M_{n,\beta_{n}}(\theta)|\lesssim \frac{t+\log(pn)\log^{4}(n)}{n}.\] (16)

Therefore, we conclude that with probability at least \(1-C_{1}e^{-t}\), the stochastic error \(S_{2,n}\lesssim\left\{t+\log(pn)\log^{4}(n)\right\}/n\).

**Step 4** (Bounding \(S_{3,n}\)). According to (15), we have

\[\forall\theta\in\Theta:\mathbb{E}_{T}\left\{M_{n,\beta_{n}}(\theta)-M_{n}(\theta) \right\}\lesssim\frac{1}{n}\]

Since \(\tilde{Y}\) is sub-Gaussian by assumption, it is straightforward to see that \((T_{\beta_{n}}\tilde{Y}-T_{\beta_{n}}\pi_{i}(\theta))^{2}I(X\in Q_{i}(\theta))\) and \((\tilde{Y}-\pi_{i}(\theta))^{2}I(X\in Q_{i}(\theta))\) are sub-exponential for \(i\in\{L,R\}\). Suppose \(\left\|(T_{\beta_{n}}\tilde{Y}-T_{\beta_{n}}\pi_{i}(\theta))^{2}I(X\in Q_{i}( \theta))\right\|_{\psi_{1}}\leq\sigma_{0}\) and \(\left\|(\tilde{Y}-\pi_{i}(\theta))^{2}I(X\in Q_{i}(\theta))\right\|_{\psi_{1}} \leq\sigma_{0}\) for all \(\theta\in\Theta\), where \(\left\|\cdot\right\|_{\psi_{1}}\) is the sub-exponential norm operator. By applying Bernstein's inequality, for any \(s>0\), we have

\[\mathbb{P}_{T}\left\{\left|M_{n,\beta_{n}}^{i}(\theta)-M_{n}^{i}( \theta)-\mathbb{E}_{T}\left\{M_{n,\beta_{n}}^{i}(\theta)-M_{n}^{i}(\theta) \right\}\geq s\right|\right\}\] \[\leq 2\exp\left(-c\min\left(\frac{ns^{2}}{\sigma_{0}^{2}},\frac{ns}{ \sigma_{0}}\right)\right),\]

for \(i\in\{R,L\}\), where \(c>0\) is a universal constant. Taking \(s=\frac{\sigma_{0}t}{cn}=C_{2}t\), for any \(t\geq 0\) we obtain

\[\mathbb{P}_{T}\left\{\left|M_{n,\beta_{n}}^{i}(\theta)-M_{n}^{i}(\theta)- \mathbb{E}_{T}\left\{M_{n,\beta_{n}}^{i}(\theta)-M_{n}^{i}(\theta)\right\} \geq C_{2}t\right|\right\}\leq 2\exp(-t)\] (17)

for any \(n>t/c\). Since the above result holds for any \(\theta\in\Theta\), we conclude that for any \(t>0\), with probability at least \(1-4e^{-t}\), we have \(S_{3,n}\lesssim(t+1)/n\).

Combining the results on \(S_{1,n},S_{2,n}\) and \(S_{3,n}\), we conclude that for any \(t>0\), with probability at least \(1-C_{3}e^{-t}\), it holds that

\[\mathcal{L}_{i}(\widehat{\theta},\widehat{\pi}_{i}(\widehat{\theta}))- \mathcal{L}_{i}(\theta_{0},\pi(\theta_{0}))\lesssim\frac{t+\log(pn)\log^{4}(n )}{n},\] (18)

for some universal constants \(C_{3},C_{4}>0\).

**Part 2.** The proposed scale criterion can reformulated as follows. For \(i\in\{L,R\}\), let

\[F_{0,i}(\theta)= \left\{1-w_{i}(\theta)\right\}(\tilde{Y}_{0}-\tau_{0,i}(\theta))^ {2}I(X_{0}\in Q_{i}(\theta))\] \[F_{1,i}(\theta)= w_{i}(\theta)(\tilde{Y}_{1}-\tau_{1,i}(\theta))^{2}I(X_{1}\in Q _{i}(\theta))\;\;\text{and}\]

where \(\tau_{0,i}(\theta)=\mathbb{E}_{n}(\tilde{Y}_{0}|X_{0}\in Q_{i}(\theta))\) and \(\tau_{1,i}(\theta)=\mathbb{E}_{m}(\tilde{Y}_{1}|X_{1}\in Q_{i}(\theta))\), and

\[w_{i}(\theta)=\sigma_{u}^{2}(Q_{i}(\theta))/\left\{\sigma_{u}^{2}(Q_{i}( \theta))+\sigma_{b}^{2}(Q_{i}(\theta))+b^{2}(Q_{i}(\theta))\right\},\]

where \(\sigma_{u}^{2}(Q_{i}(\theta))=\text{Var}_{n}(\tau_{0,i}(\theta)),\sigma_{b}^{2 }(Q_{i}(\theta))=\text{Var}_{m}(\tau_{1,i}(\theta))\) and \(b(Q_{i}(\theta))=\tau_{1,i}(\theta)-\tau_{0,1}(\theta)\). Let \(\mathcal{F}_{s,i}(\theta)=\mathbb{E}_{s}(F_{s,i}(\theta))\) for \(s\in\{0,1\}\) and \(\mathcal{F}_{s}(\theta)=\mathcal{F}_{s,L}(\theta)+\mathcal{F}_{s,R}(\theta)\), the population criterion is defined as \(M_{of}(\theta)=\mathcal{F}_{0}(\theta)+\mathcal{F}_{1}(\theta)\). For the empirical criterion, we first define \(\mathcal{F}_{n,i}(\theta)=\mathbb{E}_{n}(F_{i}^{R}(\theta))\) and \(\mathcal{F}_{m,i}(\theta)=\mathbb{E}_{m}(F_{i}^{R}(\theta))\). Let \(M_{N,of}(\theta)=\mathcal{F}_{n,L}(\theta)+\mathcal{F}_{n,R}(\theta)\) and \(\mathcal{F}_{m}(\theta)=\mathcal{F}_{m,L}(\theta)+\mathcal{F}_{m,R}(\theta)\), the empirical criterion is the denoted as \(M_{N,of}(\theta)=M_{N,of}(\theta)+\mathcal{F}_{m}(\theta)\). The population and empirical optimal splits are defined by

\[\theta_{of}=\operatorname*{arg\,min}_{\theta\in\Theta}M_{of}(\theta)\;\;\text{ and}\;\;\widehat{\theta}_{f}=\operatorname*{arg\,min}_{\theta\in\Theta}M_{N,of}(\theta).\]

We first have the following error decomposition:

\[M_{of}(\widehat{\theta})-M_{of}(\theta_{0})= M_{of}(\widehat{\theta})-M_{of,\beta_{n}}(\widehat{\theta}_{of})+M_{of}( \theta_{0})-M_{of,\beta_{n}}(\theta_{of})\] \[+M_{of,\beta_{n}}(\widehat{\theta}_{of})+M_{of,\beta_{n}}(\theta_{of })-2M_{N,of,\beta_{n}}(\widehat{\theta}_{of})+2M_{N,of,\beta_{n}}(\theta_{of})\] \[+2M_{N,of,\beta_{n}}(\widehat{\theta}_{of})-2M_{N,of}(\widehat{ \theta}_{of})-2M_{N,of,\beta_{n}}(\theta_{of})+2M_{N,of}(\theta_{of})\] \[+2M_{N,of}(\widehat{\theta}_{of})-2M_{N,of}(\theta_{of})\] \[=: T_{1,n}+T_{2,n}+T_{3,n}+T_{4,n}.\]

By the definition of \(\widehat{\theta}_{of}\), we have \(T_{4,n}\leq 0\). In the following steps, we will bound \(T_{1,n}\), \(T_{2,n}\) and \(T_{3,n}\), respectively. Following the same argument as for \(S_{1,n}\), it can be obtained that \(T_{1,n}\lesssim\frac{1}{n}\)We now bound \(T_{2,n}\) Let \(\mathcal{G}_{n}=\{g:g(y,x)=\sqrt{1-w(\theta)}\tilde{y}-\tau)I(x\in\mathcal{Q}( \theta),\theta\in\Theta_{n})\}\) and \(\mathcal{H}_{n}=\{h:h(y,x)=\sqrt{w(\theta)}(\tilde{y}-\tau)I(x\in\mathcal{Q}( \theta),\theta\in\Theta_{n})\}\), then via Lemma 2 we have

\[\mathcal{N}_{1}(\delta,\mathcal{G}_{n},z_{1}^{n})\leq(pn)^{2}\left(\frac{c \beta_{n}}{\delta}\right)^{4}\text{ and }\mathcal{N}_{1}(\delta,\mathcal{H}_{n},z_{1}^{n})\leq(pn)^{2}\left(\frac{c \beta_{n}}{\delta}\right)^{4},\]

for any \(\delta>0\), where \(z_{1}^{n}\) is any set \(\{z_{1},\cdots,z_{n}\}\in[0,1]^{p}\times\mathcal{Y}\) and \(c>0\) is a universal constant. It follows from Lemma 1 that for any \(\alpha_{1},\gamma_{1}>0\)

\[\mathbb{P}_{T}\left\{\exists\theta\in\Theta_{n}:|\mathcal{F}_{0, \beta_{n}}(\theta)-\mathcal{F}_{n,\beta_{n}}(\theta)|\geq\frac{1}{2}(\alpha_{ 1}+\gamma_{1}+M_{of,\beta_{n}}(\theta))\right\}\] \[\leq 28(pn)^{2}\left(\frac{80c\beta_{n}^{2}}{\gamma_{1}}\right)^{4} \exp\left(-\frac{\alpha_{1}n}{1284\beta_{n}^{4}}\right)\] \[\lesssim \exp\left(-\frac{\alpha_{1}n}{\beta_{n}^{4}}+\log(pn)-\log( \gamma_{1})\right).\]

Taking \(\gamma_{1}=1/n\) and \(\alpha_{1}=(t+\log(pn))\beta_{n}^{4}/n\) implies that with probability at least \(1-C_{4}e^{-t}\) for some universal constant \(C_{4}>0\),

\[\forall\theta\in\Theta_{n},|M_{of,\beta_{n}}(\theta)-2\mathcal{F}_{n,\beta_{ n}}(\theta)|\lesssim\frac{t+\log(pn)\log^{4}(n)}{n}.\] (19)

Similary, for any \(\alpha_{2},\gamma_{2}>0\),

\[\mathbb{P}_{O}\left\{\exists\theta\in\Theta_{n}:|\mathcal{F}_{1, \beta_{n}}(\theta)-\mathcal{F}_{m,\beta_{n}}(\theta)|\geq\frac{1}{2}(\alpha_{ 1}+\gamma_{1}+\mathcal{F}_{1,\beta_{n}}(\theta))\right\}\] \[\lesssim \exp\left(-\frac{\alpha_{1}m}{\beta_{n}^{4}}+\log(pn)-\log( \gamma_{1})\right).\]

Taking \(\gamma_{2}=1/n\) and \(\alpha_{2}=(t+\log(pn))\beta_{n}^{4}/m\) implies that with probability at least \(1-C_{5}e^{-t}\) for some universal constant \(C_{5}>0\),

\[\forall\theta\in\Theta_{n},|\mathcal{F}_{1,\beta_{n}}(\theta)-2\mathcal{F}_{m,\beta_{n}}(\theta)|\lesssim\frac{t+\log(pn)\log^{4}(n)}{m}.\] (20)

Combining (19) and (20) delivers that with probability at least \(1-2C_{1}e^{-t}\),

\[T_{2,n}\lesssim\frac{\log(pn)\log^{4}(n)}{m}+\frac{t+\log(pn)\log^{4}(n)}{n},\] (21)

for ant \(t>0\), since \(\widehat{\theta}_{of},\theta_{f}\in\Theta_{n}\) and \(M_{of,\beta_{n}}(\theta)=\mathcal{F}_{0,\beta_{n}}(\theta)+\mathcal{F}_{1, \beta_{n}}(\theta)\) and \(M_{N,o,f,\beta_{n}}(\theta)=\mathcal{F}_{n,\beta_{n}}(\theta)+\mathcal{F}_{m, \beta_{n}}(\theta)\) for any \(\theta\in\Theta_{n}\).

Now we turn to \(T_{3,n}\), the truncation error for the empirical loss. With the similar argument as in (16), we have with probability at least \(1-4e^{-t}\), it holds that \(T_{3,n}\lesssim(t+1)/n+(t+1)/m\) for any \(t>0\). Combining the results for \(T_{1,n},T_{2,n}\) and \(T_{3,n}\), we conclude that for any \(t>0\), with probability at least \(1-C_{6}e^{-t}\),

\[M_{of}(\widehat{\theta}_{of})-M_{of}(\theta_{of})\lesssim\frac{t+\log(pn)\log^{ 4}(n)}{m}+\frac{t+\log(pn)\log^{4}(n)}{n},\] (22)

which completes our proof.

## Appendix D Supporting lemmas

The following to lemmas are from Section 11.3 and Section 13.1 of Gyorfi et al. (2002), which are useful for our proofs.

**Lemma 1**.: _(Deviation inequality of quadratic process). Suppose that \(\mathcal{G}\) is a class of uniformly bounded functions \(\mathcal{G}=\left\{g:\mathbb{R}^{d}\rightarrow\mathbb{R}\left\|g\right\|_{ \infty}\leq M\right\}\). Let \(\mathcal{F}=\left\{g^{2}:g\in\mathcal{G}\right\}\). Then for any \(n\geq 1\), it holds that_

\[\mathbb{P}\left\{\exists f\in\mathcal{F}:|\mathbb{E}\left\{f(z) \right\}-\mathbb{E}_{n}\left\{f(z)\right\}|\geq\varepsilon(\alpha+\gamma)+ \mathbb{E}\left\{f(z)\right\}\right\}\] \[\leq 28\sup_{z_{1}^{n}}\mathcal{N}_{1}(\frac{\gamma\varepsilon}{20M}, \mathcal{G},x_{1}^{n})\exp\left(-\frac{\varepsilon^{2}(1-\varepsilon)\alpha n}{2 14(1+\varepsilon)M^{4}}\right),\]

_where \(z_{1}^{n}=(z_{1},\cdots,z_{n})\in\mathbb{R}^{d}\), \(\alpha,\gamma>0\) and \(0<\varepsilon\leq 1/2\)._

[MISSING_PAGE_EMPTY:19]