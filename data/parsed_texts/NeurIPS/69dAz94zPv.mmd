# Logarithmic-Regret Quantum Learning Algorithms

for Zero-Sum Games

 Minbo Gao

State Key Laboratory of Computer Science,

Institute of Software, Chinese Academy of Sciences,

Beijing, China

and University of Chinese Academy of Sciences,

Beijing, China

gmbl17@tsinghua.org.cn

&Zhengfeng Ji

Department of Computer Science

and Technology,

Tsinghua University,

Beijing, China

jizhengfeng@tsinghua.edu.cn

&Tongyang Li

Center on Frontiers of Computing Studies,

and School of Computer Science,

Peking University,

Beijing, China

tongyangli@pku.edu.cn

&Qisheng Wang

Graduate School of Mathematics,

Nagoya University,

Nagoya, Japan

QishengWang1994@gmail.com

###### Abstract

We propose the first online quantum algorithm for solving zero-sum games with \(\widetilde{O}(1)\) regret under the game setting.1 Moreover, our quantum algorithm computes an \(\varepsilon\)-approximate Nash equilibrium of an \(m\times n\) matrix zero-sum game in quantum time \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\). Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. Technically, our online quantum algorithm "quantizes" classical algorithms based on the _optimistic_ multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest.

Footnote 1: Throughout this paper, \(\widetilde{O}(\cdot)\) suppresses polylogarithmic factors such as \(\log(n)\) and \(\log(1/\varepsilon)\), and \(O^{*}(\cdot)\) hides quasi-polylogarithmic factors such as \(n^{o(1)}\) and \((1/\varepsilon)^{o(1)}\).

## 1 Introduction

Nash equilibrium is one of the most important concepts in game theory. It characterizes and predicts rational agents' behaviors in non-cooperative games, finding a vast host of applications ranging from analyzing wars [45] and designing auctions [35], to optimizing networks [43].

It was shown in Daskalakis et al. [16], Chen and Deng [12] that finding a Nash equilibrium is \(\mathsf{PPAD}\)-hard for general games. Nevertheless, computing the Nash equilibrium for specific types of games, such as zero-sum games, is particularly interesting. A zero-sum game requires that the utility of one player is the opposite of the other's, a condition that often appears in, for example, chess games. Von Neumann's minimax theorem [51] promises that every finite two-player zero-sum game has optimal mixed strategies.

Zero-Sum Games.For a two-player zero-sum game represented by an \(m\times n\) matrix \(\mathbf{A}\), the Nash equilibrium is the solution pair \((x,y)\) to the following min-max problem:

\[\min_{x\in\Delta_{m}}\max_{y\in\Delta_{n}}x^{\mathsf{T}}\mathbf{A}y,\]

where \(x\in\Delta_{m}\) and \(y\in\Delta_{n}\) are \(m\)- and \(n\)-dimensional probability distributions, respectively. Usually, we are satisfied with an approximate Nash equilibrium rather than an exact one. An \(\varepsilon\)-approximate Nash equilibrium is a solution pair \((x,y)\) such that:

\[\max_{y^{\prime}\in\Delta_{n}}x^{\mathsf{T}}\mathbf{A}y^{\prime}-\min_{x^{ \prime}\in\Delta_{m}}x^{\mathsf{T}}\mathbf{A}y\leq\varepsilon.\]

Online Learning.Since the matrix \(\mathbf{A}\) of the zero-sum game usually has a large dimension in practice, it is common that we trade accuracy for space and time efficiency. Thus, online learning becomes increasingly significant in these scenarios. Online learning studies the situation when data is only available in sequential order and aims at making good decisions in this setup. In evaluating online learning algorithms, regret is an important criterion that measures how good an algorithm is compared with the optimal static loss (see more details in Section 2.3).

The idea of the online learning algorithms for zero-sum games stems from repeated play in game theory, e.g., fictitious play [11]. Specifically, we simulate the actions of two players for multiple rounds. In each round, players make decisions using a no-regret learning algorithm, considering the opponent's previous actions. For example, a famous algorithm of this type was proposed in Grigoriadis and Khachiyan [22] inspired by the exponential Hedge algorithm. The algorithm has regret \(\widetilde{O}(\sqrt{T})\) and \(T\) rounds, establishing the convergence rate of \(\widetilde{O}(1/\sqrt{T})\).

It takes about two decades before the next improvement in Daskalakis et al. [17] to happen, where the authors proposed a strongly-uncoupled algorithm, achieving \(\widetilde{O}(1)\) total regret if both players use the algorithm. They used the technique of minimizing non-smooth functions using smoothed approximations proposed in Nesterov [37], and this technique was later developed in Nesterov [38], Nemirovski [36] for broader classes of problems. Later, it was found in Syrgkanis et al. [46] that the optimistic multiplicative weight algorithm also leads to \(\widetilde{O}(1)\) total regret with regret bounded by variation in utilities; this algorithm was recently extended to correlated equilibria in multi-player general-sum games in Anagnostides et al. [3]. It was proved in Hsieh et al. [30] that optimistic mirror descent with a time-varying learning rate can also achieve \(\widetilde{O}(1)\) total regret for multi-players. Our quantum algorithm follows the overall idea of the optimistic multiplicative weight update and the regret bounded by variation methods [46].

Quantum Computing and Learning.Quantum computing has been rapidly advancing in recent years. Specifically, many machine learning problems are known to have significant quantum speedups, e.g., support vector machines [42], principal component analysis [34], classification [31, 33], etc. The combination of quantum computing and online learning has recently become a popular topic. For instance, online learning tools have been applied to solving semidefinite programs (SDPs) with quantum speedup in the problem dimension and the number of constraints [7, 50, 8, 49]. In addition, as an important quantum information task, the online version of quantum state learning has been systematically developed with good theoretical and empirical guarantees [1, 52, 14, 13].

For finding the Nash equilibrium of zero-sum games, a quantum algorithm was proposed in van Apeldoorn and Gilyen [48] by "quantizing" the classical algorithm in Grigoriadis and Khachiyan [22], achieving a quadratic speedup in the dimension parameters \(m\) and \(n\). At the same time, quantum algorithms for training linear and kernel-based classifiers were proposed in Li et al. [33], which have similar problem formulations to zero-sum games. Recently, an improved quantum algorithm for zero-sum games was proposed in Bouland et al. [6] using dynamic Gibbs sampling. All of the above quantum algorithms are based on the multiplicative weight update method, and as a consequence, they all share the \(O(\sqrt{T})\) regret bound.

### Main Result

Our result in this paper establishes a positive answer to the following open question: _Does there exist a learning algorithm with \(\widetilde{O}(1)\) regret allowing quantum speedups?_

Inspired by the optimistic follow-the-regularized-leader algorithm proposed in Syrgkanis et al. [46], we propose a sample-based quantum online learning algorithm for zero-sum games with \(O(\log(mn))\) total regret, which is near-optimal. If we run this algorithm for \(T\) rounds, it will compute an \(\widetilde{O}(1/T)\)-approximate Nash equilibrium with high probability, achieving a quadratic speedup in dimension parameters \(m\) and \(n\). Formally, we have the following quantum online learning algorithm:

**Theorem 1.1** (Online learning for zero-sum games).: _Suppose \(T\leq\widetilde{O}(m+n)\). There is a quantum online algorithm for zero-sum game \(\mathbf{A}\in\mathbb{R}^{m\times n}\) with \(\|\mathbf{A}\|\leq 1\) such that it achieves a total regret of \(O(\log(mn))\) with high probability after \(T\) rounds, while each round takes quantum time \(\widetilde{O}(T^{1.5}\sqrt{m+n})\)._

Our algorithm does not need to read all the entries of the input matrix \(\mathbf{A}\) at once. Instead, we assume that our algorithm can query its entries when necessary. The input model is described as follows:

* Classically, given any \(i\in[m],j\in[n]\), the entry \(A_{i,j}\) can be accessed in \(\widetilde{O}(1)\) time.
* Quantumly, we assume that the entry \(A_{i,j}\) can be accessed in \(\widetilde{O}(1)\) time _coherently_.

This is the _standard quantum input model_ for zero-sum games adopted in previous literature [33, 48, 6]. See more details in Section 2.4.

In addition, same as prior works [48, 6], our algorithm outputs _purely classical vectors_ with succinct descriptions because they are sparse (with at most \(T^{2}\) nonzero entries). Overall, using standard quantum inputs and generating classical outputs significantly facilitate end-to-end applications of our algorithm in the near term.

As a direct corollary, we can find an \(\varepsilon\)-_approximate Nash equilibrium_ by taking \(T=\widetilde{O}(1/\varepsilon)\), resulting in a quantum speedup stated as follows. A detailed comparison to previous literature is presented in Table 1.

**Corollary 1.2** (Computing Nash equilibrium).: _There is a quantum online algorithm for zero-sum game \(\mathbf{A}\in\mathbb{R}^{m\times n}\) with \(\|A\|\leq 1\) that, with high probability, computes an \(\varepsilon\)-approximate Nash equilibrium in quantum time \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\).4_

Footnote 4: In fact, a condition of \(\varepsilon=\Omega((m+n)^{-1})\) is required in our quantum algorithm. Nevertheless, our claim still holds because when \(\varepsilon=O((m+n)^{-1})\), we can directly apply the classical algorithm in Grigoriadis and Khachiyan [22] with time complexity \(\widetilde{O}((m+n)/\varepsilon^{2})\leq\widetilde{O}(\sqrt{m+n}/\varepsilon^ {2.5})\).

Quantum Lower Bounds.In the full version of [33], they showed a lower bound \(\Omega(\sqrt{m+n})\) for the quantum query complexity of computing an \(\varepsilon\)-approximate Nash equilibrium of zero-sum games for constant \(\varepsilon\). Therefore, our algorithm is tight in terms of \(m\) and \(n\).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Approach & Type & Regret & Update Cost Per Round & Classical/Quantum Time Complexity \\ \hline
[22] & Classical & \(\widetilde{O}(\sqrt{T})\) & \(\widetilde{O}(m+n)\) & \(\widetilde{O}((m+n)/\varepsilon^{2})\) \\
[48] & Quantum & \(\widetilde{O}(\sqrt{T})\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon)\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{3})\) \\
[6] & Quantum & \(\widetilde{O}(\sqrt{T})\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{0.5})\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\) 2  \\ \hline
[46] & Classical & \(\widetilde{O}(1)\) & \(\widetilde{O}(mn)\) & \(\widetilde{O}(mn/\varepsilon)\) \\
[10] & Classical & \(\widetilde{O}(1)\) & \(\widetilde{O}(\sqrt{mn(m+n)})\) & \(\widetilde{O}(mn+\sqrt{mn(m+n)}/\varepsilon)\) \\ Our result 3  & Quantum & \(\widetilde{O}(1)\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{1.5})\) & \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Online Algorithms for \(\varepsilon\)-Approximate Nash Equilibria of Zero-Sum Games.

### Our Techniques

Our quantum online algorithm is a stochastic modification of the optimistic multiplicative weight update proposed by Syrgkanis et al. [46]. We choose to "quantize" the optimistic online algorithm because it has a better convergence rate for zero-sum games than general multiplicative weight update algorithms. During the update of classical online algorithms, the update term (gradient vector) is computed in linear time by arithmetic operations. However, we observe that it is not necessary to know the exact gradient vector. This motivates us to apply stochastic gradient methods for updates so that certain fast quantum samplers can be utilized here and bring quantum speedups.

Specifically, in our quantum online algorithm, we need to establish an upper bound on the expectation of the total regret of our stochastic update rule, and also deal with errors that may appear from noisy samplers. To reduce the variance of the gradient estimator, we need multiple samples (from Gibbs distributions) at a time. To this end, we develop a fast quantum multi-Gibbs sampler that produces multiple samples by preparing and measuring quantum states.

Sample-Based Optimistic Multiplicative Weight Update.Optimistic online learning adds a "prediction loss" term to the cumulative loss for regularized minimization, giving a faster convergence rate than the non-optimistic versions for zero-sum games. Arora et al. [4] surveyed the use of the multiplicative weight update method in various domains, but little was known for the optimistic learning method at that time. Daskalakis et al. [17] proposed an extragradient method that largely resembles the optimistic multiplicative weight. Syrgkanis et al. [46] gave a characterization of this update rule--RVU (Regret bounded by Variation in Utilities) property, which is very useful in proving regret bounds. Subsequently, the optimistic learning method is applied to other areas, including training GANs [18] and multi-agent learning [40].

However, when implementing the optimistic learning methods, we face a fundamental difficulty--we cannot directly access data from quantum states without measurement. To resolve this issue, we get samples from the desired distribution and use them to estimate the actual gradient. This idea is commonly seen in previous literature on quantum SDP solvers [7; 50; 8; 49]. Then we prove the regret bound (see Theorem 3.2) of our algorithm by showing that it has a property similar to the RVU property [46]. Moreover, we need multiple samples to obtain a small "variance" of the stochastic gradient (by taking the average of the samples), to ensure that the expected regret is bounded. Our fast quantum multi-Gibbs sampler produces the required samples and ensures further quantum speedups. In a nutshell, we give an algorithm (Algorithm 1) which modifies the optimistic multiplicative weight algorithm in Syrgkanis et al. [46] to fit the quantum implementation. This is _the first quantum algorithm that implements optimistic online learning_ to the best of our knowledge.

Fast Quantum Multi-Gibbs Sampling.The key to our sample-based approach is to obtain multiple samples from the Gibbs distribution after a common preprocessing step. For a vector \(p\in\mathbb{R}^{n}\) with \(\max_{i\in[n]}|p_{i}|\leq\beta\), a sample from the Gibbs distribution with respect to \(p\) is a random variable \(j\in[n]\) such that \(\textbf{Pr}\left[j=l\right]=\frac{\exp(p_{i})}{\sum_{i=1}^{n}\exp(p_{i})}\). Gibbs sampling on a quantum computer was first studied in Poulin and Wocjan [41], and was later used as a subroutine in quantum SDP solvers [7; 50; 8; 49]. However, the aforementioned quantum Gibbs samplers produce one sample from an \(n\)-dimensional Gibbs distribution in quantum time \(\widetilde{O}(\beta\sqrt{n})\); thus, we can produce \(k\) samples in quantum time \(\widetilde{O}(\beta k\sqrt{n})\). Inspired by the recent work Hamoudi [26] about preparing multiple samples of quantum states, we develop a fast quantum Gibbs sampler (Theorem 4.2) which _produces \(k\) samples from a Gibbs distribution in quantum time \(\widetilde{O}(\beta\sqrt{nk})\)_. Our quantum multi-Gibbs sampling may have potential applications in sample-based approaches for optimization tasks that require multiple samples.

Technically, the main idea is based on quantum rejection sampling [24; 39], where the target quantum state \(|u\rangle\) is obtained by post-selection from a quantum state \(|u_{\text{guess}}\rangle\) that is easy to prepare (see Section 4). The algorithm has the following steps (Here we assume \(\beta=O(1)\) for simplicity): To bring \(|u_{\text{guess}}\rangle\) closer to the target \(|u\rangle\), we find the \(k\) (approximately) most dominant amplitudes of \(|u\rangle\) by quantum \(k\)-maximum finding [19] in quantum time \(\widetilde{O}(\sqrt{nk})\). In quantum \(k\)-maximum finding, we need to estimate the amplitudes of \(|u\rangle\) and compare them coherently, which requires the estimation should be consistent. To address this issue, we develop consistent quantum amplitude estimation (see Appendix C) for our purpose based on consistent phase estimation [2; 47], which is of independent interest. Then, we correct the tail amplitudes of \(|u_{\text{guess}}\rangle\) by quantum singularvalue transformation [20], resulting in an (approximate) target state with amplitude \(\Omega(\sqrt{k/n})\) (see Appendix D for details). Finally, we post-select the target state by quantum amplitude amplification [9] in quantum time \(\widetilde{O}(\sqrt{n/k})\). This follows that \(k\) samples of the target quantum state can be obtained in \(k\cdot\widetilde{O}(\sqrt{n/k})=\widetilde{O}(\sqrt{nk})\) time.

We believe that our technique can be extended to a wide range of distributions whose mass function is monotonic and satisfies certain Lipschitz conditions.

## 2 Preliminaries

### General Mathematical Notations

For convenience, we use \([n]\) to denote the set \(\{1,2,\ldots,n\}\). We use \(e_{i}\) to denote a vector whose \(i\)-th coordinate is \(1\) and other coordinates are \(0\). For a vector \(v\in\mathbb{R}^{n}\), \(v_{i}\) is the \(i\)-th coordinate of \(v\). For a function \(f\colon\mathbb{R}\to\mathbb{R}\), we write \(f(v)\) to denote the result of \(f\) applied to its coordinates, i.e., \(f(v)=(f(v_{1}),f(v_{2}),\ldots,f(v_{n}))\). We use \(\Delta_{n}\) to represent the set of \(n\)-dimensional probability distributions, i.e., \(\Delta_{n}:=\{v\in\mathbb{R}^{n}:\sum_{i=1}^{n}v_{i}=1,\forall i\in[n],v_{i} \geq 0\}\). Here the \(i\)-th coordinate represents the probability of event \(i\) takes place. We use \(\left\|\cdot\right\|\) for vector norms. The \(l_{1}\) norm \(\left\|\cdot\right\|_{1}\) for a vector \(v\in\mathbb{R}^{n}\) is defined as \(\left\|v\right\|_{1}:=\sum_{i=1}^{n}\left|v_{i}\right|\). For two \(n\)-dimensional probability distributions \(p,q\in\Delta_{n}\), their total variance distance is defined as: \(d_{\mathrm{TV}}(p,q)=\frac{1}{2}\left\|p-q\right\|_{1}=\frac{1}{2}\sum_{i=1}^{ n}\left|p_{i}-q_{i}\right|\). We will use \(\mathbf{A}\in\mathbb{R}^{m\times n}\) to denote a matrix with \(m\) rows and \(n\) columns. \(A_{i,j}\) is the entry of \(\mathbf{A}\) in the \(i\)-th row and \(j\)-th column. \(\left\|\mathbf{A}\right\|\) denotes the operator norm of matrix \(\mathbf{A}\). For a vector \(v\in\mathbb{R}^{n}\), we use \(\text{diag}(v)\) to denote the diagonal matrix in \(\mathbb{R}^{n\times n}\) whose diagonal entries are coordinates of \(v\) with the same order.

### Game Theory

Let us consider two players, Alice and Bob, playing a zero-sum game represented by a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\) with entries \(A_{i,j}\), where \([m]\) is the labeled action set of Alice and \([n]\) of Bob. Usually, Alice is called the row player and Bob is called the column player. \(A_{i,j}\) is the payoff to Bob and \(-A_{i,j}\) is the payoff to Alice when Alice plays action \(i\) and Bob plays action \(j\). Both players want to maximize their payoff when they consider their opponent's strategy.

Consider the situation that both players' actions are the best responses to each other. In this case, we call the actions form a _Nash equilibrium_. A famous minimax theorem by von Neumann [51] states that we can exchange the order of the min and max operation and thus the value of the Nash equilibrium of the game can be properly defined. To be more exact, we have: \(\min_{x\in\Delta_{m}}\max_{y\in\Delta_{m}}x^{\intercal}\mathbf{A}y=\max_{y\in \Delta_{m}}\min_{x\in\Delta_{m}}x^{\intercal}\mathbf{A}y\). Here the value of the minimax optimization problem is called the _value of the game_. Our goal is to find an approximate Nash equilibrium for this problem. More formally, we need two probabilistic vectors \(x\in\Delta_{m},y\in\Delta_{n}\) such that the following holds: \(\max_{y^{\prime}\in\Delta_{m}}x^{\intercal}\mathbf{A}y^{\prime}-\min_{x^{ \prime}\in\Delta_{m}}x^{\intercalintercal}\mathbf{A}y\leq\epsilon\). We will call such a pair \((x,y)\) an \(\epsilon\)-_approximate Nash Equilibrium_ for the two-player zero-sum game \(\mathbf{A}\). Computing an \(\epsilon\)-approximate Nash for a zero-sum game is not as hard as for a general game. For a general game, approximately computing its Nash equilibrium is \(\mathsf{PPAD}\)-hard [16; 12].

Here, we emphasize an important observation that will be used throughout our paper: we can add a constant or multiply by a positive constant on all \(\mathbf{A}\)'s entries without changing the solution pair of the Nash equilibrium. This is because, in the definition of the Nash equilibrium, the best response is always in comparison with other possible actions, so only the order of the utility matters. Thus without loss of generality, we can always rescale \(\mathbf{A}\) to \((\mathbf{A}+c\mathbf{1})/2\) where \(\mathbf{1}\) is an \(m\times n\) matrix with all entries being \(1\) with \(c\) being the largest absolute value of \(\mathbf{A}\)'s entries, and let \(\mathbf{A}^{\prime}=\mathbf{A}/\left\|\mathbf{A}\right\|\) to guarantee that \(\mathbf{A}\) has non-negative entries and has operator norm no more than \(1\).

### Online Learning

#### 2.3.1 Notions of online learning

In general, online learning focuses on making decisions in an uncertain situation, where a decider is not aware of the current loss and is required to make decisions based on observations of previous losses. To be more exact, we fix the number of rounds \(T\) and judge the performance of the algorithm (in the following of this subsection we use "decider" with the same meaning as the "algorithm") in these \(T\) rounds. Assume that the decider is allowed to choose actions in the domain \(\mathfrak{X}\), usually a convex subset of a finite-dimensional Euclidean space. Let \(t\in[T]\) denote the current number of rounds. At round \(t\), the decider chooses an action \(x_{t}\in\mathfrak{X}\). (The action may depend on the decider's previous observation of the loss functions \(l_{i}\) for all \(i\in[t-1]\).) Then the decider will get the loss \(l_{t}(x_{t})\) for the current round. We assume that the decider can also observe the full information of \(l_{t}\), i.e., the formula of the function. We judge the performance of the algorithm by its _regret_: \(\mathcal{R}(T)=\sum_{i=1}^{T}l_{t}(x_{t})-\min_{x\in\mathfrak{X}}\sum_{i=1}^{ T}l_{t}(x)\). Intuitively, the regret shows how far the total loss in \(T\) rounds caused by the algorithm is from the optimal static loss.

#### 2.3.2 Online learning in zero-sum games

To demonstrate how to compute the approximate Nash equilibrium using online learning algorithms, we present a useful proposition here. It states that from any sublinear regret learning algorithm \(\mathcal{A}\) with regret \(\mathcal{R}(T)\), we can find an \(\mathcal{O}(\mathcal{R}(T)/T)\)-approximate Nash equilibrium of the zero-sum game in \(T\) rounds.

To be more precise, let us consider the following procedure. Let \(\mathbf{A}\) be the matrix for the two-player zero-sum game. The algorithm starts with some initial strategies \(u_{0}\in\Delta_{m},v_{0}\in\Delta_{n}\) for the two players. Then at each round \(t\), for each player, it makes decisions with previous observations of the opponent's strategy. In particular, the row player is required to choose his/her action \(u_{t}\in\Delta_{m}\) after considering the previous loss functions \(g_{i}(x)=v_{i}^{\mathsf{T}}\mathbf{A}^{\mathsf{T}}x\) for \(i\in[t-1]\). Similarly, the column player chooses his/her action \(v_{t}\in\Delta_{n}\) with respect to the previous loss functions \(h_{i}(y)=-u_{i}^{\mathsf{T}}\mathbf{A}y\) for \(i\in[t-1]\). After both players choose their actions at this round \(t\), they will receive their loss functions \(g_{t}(x):=v_{t}^{\mathsf{T}}\mathbf{A}^{\mathsf{T}}x\) and \(h_{t}(y)=-u_{t}^{\mathsf{T}}\mathbf{A}y\), respectively.

Suppose after \(T\) rounds, the regret of the row player with respect to the loss functions \(g_{t}(x)\) is \(\mathcal{R}(T)\), and the regret of the column player with loss functions \(h_{t}(y)\) is \(\mathcal{R}^{\prime}(T)\). We can write the total regret \(\mathcal{R}(T)+\mathcal{R}^{\prime}(T)\) explicitly: \(\mathcal{R}(T)+\mathcal{R}^{\prime}(T)=T(\max_{v\in\Delta_{n}}\left\langle v, \mathbf{A}^{\mathsf{T}}u\right\rangle-\min_{u\in\Delta_{m}}\left\langle u, \mathbf{A}\mathbf{\Theta}\right\rangle)\), where the average strategy is defined as \(\hat{u}=\sum_{i=1}^{T}u_{i}/T\), \(\hat{v}=\sum_{i=1}^{T}v_{i}/T\). This pair is a good approximation of the Nash equilibrium for the game \(\mathbf{A}\) if the regret is \(\mathcal{O}(T)\).

### Quantum Computing

In quantum mechanics, a \(d\)-dimensional quantum state is described by a unit vector \(v=(v_{0},v_{1},\ldots,v_{d-1})^{\mathsf{T}}\), usually denoted as \(\left|v\right\rangle\) with the Dirac symbol \(\left|\cdot\right\rangle\), in a complex Hilbert space \(\mathbb{C}^{d}\). The computational basis of \(\mathbb{C}^{d}\) is defined as \(\left\{\left|i\right\rangle\right\}_{i=0}^{d-1}\), where \(\left|i\right\rangle=(0,\ldots,0,1,0,\ldots,0)^{\mathsf{T}}\) with the \(i\)-th (\(0\)-indexed) entry being \(1\) and other entries being \(0\). The inner product of quantum states \(\left|v\right\rangle\) and \(\left|w\right\rangle\) is defined by \(\left\langle v|w\right\rangle=\sum_{i=0}^{d-1}v_{i}^{*}w_{i}\), where \(z^{*}\) denotes the conjugate of complex number \(z\). The norm of \(\left|v\right\rangle\) is defined by \(\left\|\left|v\right\rangle\right\|=\sqrt{\left\langle v|v\right\rangle}\). The tensor product of quantum states \(\left|v\right\rangle\in\mathbb{C}^{d_{1}}\) and \(\left|w\right\rangle\in\mathbb{C}^{d_{2}}\) is defined by \(\left|v\right\rangle\otimes\left|w\right\rangle=(v_{0}w_{0},v_{0}w_{1},\ldots,v_{d_{1}-1}w_{d_{2}-1})^{\mathsf{T}}\in\mathbb{C}^{d_{1}d_{2}}\), denoted as \(\left|v\right\rangle\left|w\right\rangle\) for short.

A quantum bit (qubit for short) is a quantum state \(\left|\psi\right\rangle\) in \(\mathbb{C}^{2}\), which can be written as \(\left|\psi\right\rangle=\alpha|0\rangle+\beta|1\rangle\) with \(\left|\alpha\right|^{2}+\left|\beta\right|^{2}=1\). An \(n\)-qubit quantum state is in the tensor product space of \(n\) Hilbert spaces \(\mathbb{C}^{2}\), i.e., \((\mathbb{C}^{2})^{\otimes n}=\mathbb{C}^{2^{n}}\) with the computational basis \(\left\{|0\rangle,\left|1\right\rangle,\ldots,\left|2^{n}-1\right\rangle\right\}\). To obtain classical information from an \(n\)-qubit quantum state \(\left|v\right\rangle\), we measure \(\left|v\right\rangle\) on the computational basis and obtain outcome \(i\) with probability \(p(i)=\left|\left\langle i|v\right\rangle\right|^{2}\) for every \(0\leq i<2^{n}\). The evolution of a quantum state \(\left|v\right\rangle\) is described by a unitary transformation \(U\colon\left|v\right\rangle\mapsto U|v\rangle\) such that \(UU^{\dagger}=U^{\dagger}U=I\), where \(U^{\dagger}\) is the Hermitian conjugate of \(U\), and \(I\) is the identity operator. A quantum gate is a unitary transformation that acts on \(1\) or \(2\) qubits, and a quantum circuit is a sequence of quantum gates.

Throughout this paper, we assume the quantum oracle \(\mathcal{O}_{\mathbf{A}}\) for a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\), which is a unitary operator such that for every row index \(i\in[m]\) and column index \(j\in[n]\), \(\mathcal{O}_{\mathbf{A}}\!\left|i\right\rangle\!\left|j\right\rangle\!\left|0 \right\rangle=\left|i\right\rangle\!\left|j\right\rangle\!\left|A_{i,j}\right\rangle\). Intuitively, the oracle \(\mathcal{O}_{\mathbf{A}}\) reads the entry \(A_{i,j}\) and stores it in the third register; this is potentially stronger than the classical counterpart when the query is a linear combination of basis vectors, e.g., \(\sum_{k}\alpha_{k}|i_{k}\rangle|j_{k}\rangle\) with \(\sum_{k}\left|\alpha_{k}\right|^{2}=1\). This is known as the _superposition_ principle in quantum computing.

Note that this input model for matrices is commonly used in quantum algorithms, e.g., linear system solvers [28] and semidefinite programming solvers [7, 50, 8, 49].

A quantum (query) algorithm \(\mathcal{A}\) is a quantum circuit that consists a sequence of unitary operators \(G_{1},G_{2},\ldots,G_{T}\), each of which is either a quantum gate or a quantum oracle. The quantum time complexity of \(\mathcal{A}\) is measured by the number \(T\) of quantum gates and quantum oracles in \(\mathcal{A}\). The execution of \(\mathcal{A}\) on \(n\) qubits starts with quantum state \(|0\rangle^{\otimes n}\), then it performs unitary operators \(G_{1},G_{2},\ldots,G_{T}\) on the quantum state in this order, resulting in the quantum state \(|\phi\rangle=G_{T}\ldots G_{2}G_{1}|0\rangle^{\otimes n}\). Finally, we measure \(|\phi\rangle\) on the computational basis \(|i\rangle\) for \(0\leq i<2^{n}\), giving a classical output \(i\) with probability \(|\langle i|\phi\rangle|^{2}\).

 Quantum Algorithm for Online Zero-Sum Games by Sample-Based Optimistic Multiplicative Weight Update

Now, we present our quantum algorithm for finding an approximate Nash equilibrium for zero-sum games (Algorithm 1). This algorithm is a modification of the optimistic multiplicative weight algorithm [46], in which we use stochastic gradients to estimate true gradients. This modification utilizes the quantum advantage of Gibbs sampling (as will be shown in Section 4). It is the source of quantum speedups in the algorithm and also the reason that we call the algorithm _sample-based_. To this end, we first give the definition of Gibbs sampling oracles.

**Definition 3.1** (Approximate Gibbs sampling oracle).: Let \(p\in\mathbb{R}^{n}\) be an \(n\)-dimensional vector, \(\epsilon>0\) be the approximate error. We let \(\mathcal{O}_{p}^{\mathrm{Gibbs}}(k,\epsilon)\) denote the oracle which produces \(k\) independent samples from a distribution that is \(\epsilon\)-close to the Gibbs distribution with parameter \(p\) in total variation distance. Here, for a random variable \(j\) taking value in \([n]\) following the Gibbs distribution with parameter \(p\), we have \(\mathbf{Pr}\left[j=l\right]=\exp(p_{l})/\sum_{i=1}^{n}\exp(p_{i})\).

```
0:\(\mathbf{A}\in\mathbb{R}^{m\times n}\), additive approximation \(\epsilon\), approximate Gibbs sampling oracle \(\mathcal{O}^{\mathrm{Gibbs}}\) with error \(\epsilon_{\mathrm{G}}\), total episode \(T\), learning rate \(\lambda\in(0,\sqrt{3}/6)\).
0:\((\hat{u},\hat{v})\) as the approximate Nash equilibrium of the matrix game \(\mathbf{A}\).
1: Initialize \(\hat{u}\leftarrow\mathbf{0}_{m}\), \(\hat{v}\leftarrow\mathbf{0}_{n}\), \(x_{1}\leftarrow\mathbf{0}_{m}\), \(y_{1}\leftarrow\mathbf{0}_{n}\).
2: Set \(g_{1}\gets x_{1}\), \(h_{1}\gets y_{1}\).
3:for\(t=1,\ldots,T\)do
4: Get \(T\) independent samples \(i_{1}^{t},i_{2}^{t},\ldots,i_{T}^{t}\) from the Gibbs sampling oracle \(\mathcal{O}_{-\mathbf{A}h_{t}}^{\mathrm{Gibbs}}(T,\epsilon_{\mathrm{G}})\).
5: Choose the action \(\zeta_{t}=\sum_{N=1}^{T}\epsilon_{i_{N}^{t}}^{t}/T\).
6: Update \(x_{t+1}\gets x_{t}+\lambda\zeta_{t}\), \(g_{t+1}\gets x_{t+1}+\lambda\zeta_{t}\). \(\hat{u}\leftarrow\hat{u}+\frac{1}{T}\zeta_{t}\).
7: Get \(T\) independent samples \(j_{1}^{t},j_{2}^{t},\ldots,j_{T}^{t}\) from the Gibbs sampling oracle \(\mathcal{O}_{\mathbf{A}^{\mathrm{T}}g_{t}}^{\mathrm{Gibbs}}(T,\epsilon_{ \mathrm{G}})\).
8: Choose the action \(\eta_{t}=\sum_{N=1}^{T}\epsilon_{j_{N}^{t}}^{t}/T\).
9: Update \(y_{t+1}\gets y_{t}+\lambda\eta_{t}\), \(h_{t+1}\gets y_{t+1}+\lambda\eta_{t}\), \(\hat{v}\leftarrow\hat{v}+\frac{1}{T}\eta_{t}\).
10:endfor
11:return\((\hat{u},\hat{v})\). ```

**Algorithm 1** Sample-Based Optimistic Multiplicative Weight Update for Matrix Games

Suppose the zero-sum game is represented by matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\) with \(\|\mathbf{A}\|\leq 1\). Our sample-based optimistic multiplicative weight update algorithm is given in Algorithm 1. Algorithm 1 is inspired by the classical optimistic follow-the-regularized-leader algorithm (see Appendix A for more information). In that classical algorithm, the update terms are essentially \(\mathds{E}[\zeta_{t}]\) and \(\mathds{E}[\eta_{t}]\), which are computed deterministically by matrix arithmetic operations during the update. In contrast, we do this probabilistically by sampling from the Gibbs distributions and \(\zeta_{t}\) and \(\eta_{t}\) in Line 5 and Line 10 are the corresponding averages of the samples. For this to work, we need to bound the expectation of the total regret (see Appendix B) based on the RVU property (Definition A.2). Technically, the \(l_{1}\) variances of \(\zeta_{t}\) and \(\eta_{t}\) turn out to be significant in the analysis. To reduce the variances, we need multiple independent samples identically distributed from Gibbs distributions (see Line 4 and Line 7 in Algorithm 1). Because of the randomness from Gibbs sampling oracles, the total regret \(\mathcal{R}(T)+\mathcal{R}^{\prime}(T)\) of Algorithm 1 is a random variable. Nevertheless, we can bound the total regret by \(O(\log(mn))\) with high probability as follows.

**Theorem 3.2**.: _Let \(\epsilon_{\mathrm{G}}=1/T\). After \(T\) rounds of playing the zero-sum game \(\mathbf{A}\) by Algorithm 1, the total regret will be bounded by_

\[\mathcal{R}(T)+\mathcal{R}^{\prime}(T)\leq 144\lambda+\frac{3\log(mn)}{ \lambda}+12,\]

_with probability at least \(2/3\). Then, for any constant \(\lambda\in(0,\sqrt{3}/6)\), the total regret is \(O(\log(mn))\). Moreover, if we choose \(T=\widetilde{\Theta}(1/\varepsilon)\), then Algorithm 1 will return an \(\varepsilon\)-approximate Nash equilibrium of the game \(\mathbf{A}\)._

The proof of Theorem 3.2 is deferred to Appendix B.2. Combining Theorem 3.2 and our fast quantum multi-Gibbs sampler in Theorem 4.2 (which will be developed in the next section), we obtain a quantum algorithm for finding an \(\varepsilon\)-approximate Nash equilibrium of zero-sum games.

**Corollary 3.3**.: _If we choose \(T=\widetilde{\Theta}(1/\varepsilon)\) and use quantum multi-Gibbs sampling (Algorithm 2) with \(\epsilon_{\mathrm{G}}=1/T\), then Algorithm 1 will return an \(\varepsilon\)-approximate Nash equilibrium in quantum time \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\)._

## 4 Fast Quantum Multi-Gibbs Sampling

In Algorithm 1, the vectors \(h_{t}\) and \(g_{t}\) updated in each round are used to generate independent samples from Gibbs distributions \(\mathbb{O}_{-\mathbf{A}h_{t}}^{\mathrm{Gibbs}}\) and \(\mathbb{O}_{\mathbf{A}^{\prime}g_{t}}^{\mathrm{Gibbs}}\). Here, \(h_{t}\) and \(g_{t}\) are supposed to be stored in classical registers. To allow quantum speedups, we store \(h_{t}\) and \(g_{t}\) in quantum-read classical-write random access memory (QRAM) [21], which is commonly used in prior work [49; 6]. Specifically, a QRAM can store/modify an array \(a_{1},a_{2},\ldots,a_{\ell}\) of classical data and provide quantum (read-only) access to them, i.e., a unitary operator \(U_{\mathrm{QRAM}}\) is given such that \(U_{\mathrm{QRAM}}\colon|i\rangle\,|0\rangle\mapsto|i\rangle\,|a_{i}\rangle\). Without loss of generality (see Remark 4.3), suppose we have quantum oracle \(\mathcal{O}_{\mathbf{A}}\) for \(\mathbf{A}\in\mathbb{R}^{n\times n}\) with \(A_{i,j}\in[0,1]\), and QRAM access to a vector \(z\in\mathbb{R}^{n}\) with \(z_{i}\geq 0\). We also need the polynomial approximation of the exponential function for applying the QSVT technique [20]:

**Lemma 4.1** (Polynomial approximation, Lemma 7 of [48]).: _Let \(\beta\geq 1\) and \(\epsilon_{P}\in(0,1/2)\). There is a classically efficiently computable polynomial \(P_{\beta}\in\mathbb{R}[x]\) of degree \(O(\beta\log(\epsilon_{P}^{-1}))\) such that \(\big{|}P_{\beta}(x)\big{|}\leq 1\) for \(x\in[-1,1]\), and \(\max_{x\in[-1,0]}\,\Big{|}P_{\beta}(x)-\frac{1}{4}\exp(\beta x)\Big{|}\leq \epsilon_{P}\)._

Then, we can produce multiple samples from \(\mathbb{O}_{\mathbf{A}^{\prime}}^{\mathrm{Gibbs}}\) efficiently by Algorithm 2 on a quantum computer. Algorithm 2 is inspired by Hamoudi [26] about preparing multiple samples of a quantum state, with quantum access to its amplitudes. However, we do not have access to the exact values of the amplitudes, which are \((\mathbf{A}z)_{i}\) in our case. To resolve this issue, we develop consistent quantum amplitude estimation (see Appendix C) to estimate \((\mathbf{A}z)_{i}\) with a unique answer (Line 1). After having prepared an initial quantum state \(|u_{\mathrm{guess}}\rangle\), we use quantum singular value decomposition [20] to correct the tail amplitudes (Line 6), and finally obtain the desired quantum state \(|\tilde{u}_{\mathrm{Gibbs}}\rangle\) by quantum amplitude amplification [9] (Line 7). We have the following (see Appendix D for its proof):

**Theorem 4.2** (Fast quantum multi-Gibbs sampling).: _For \(k\in[n]\), if we set \(\epsilon_{P}=\Theta(ke_{\mathrm{G}}^{2}/n)\), then Algorithm 2 will produce \(k\) independent and identical distributed samples from a distribution that is \(\epsilon_{\mathrm{G}}\)-close to \(\mathbb{O}_{\mathbf{A}^{\prime}}^{\mathrm{Gibbs}}\) in total variation distance, in quantum time \(\widetilde{O}(\beta\sqrt{nk})\)._

_Remark 4.3_.: If \(\mathbf{A}\in\mathbb{R}^{m\times n}\) is not a square matrix, then by adding 0's we can always enlarge \(\mathbf{A}\) to an \((m+n)\)-dimensional square matrix. For \(\mathbb{O}_{-\mathbf{A}h_{t}}^{\mathrm{Gibbs}}\) as required in Algorithm 1, we note that \(\mathbb{O}_{(\mathbf{1}-\mathbf{A})h_{t}}^{\mathrm{Gibbs}}\) indicates the same distribution as \(\mathbb{O}_{-\mathbf{A}h_{t}}^{\mathrm{Gibbs}}\), where \(\mathbf{1}\) has the same size as \(\mathbf{A}\) with all entries being \(1\). From the above discussion, we can always convert \(\mathbf{A}\) to another matrix satisfying our assumption, i.e., with entries in the range \([0,1]\).

_Remark 4.4_.: The description of the unitary operator \(U^{\mathrm{exp}}\) defined by the polynomial \(P_{2\beta}\) can be classically computed to precision \(\epsilon_{P}\) in time \(\widetilde{O}(\beta^{3})\) by Haah [25], which is \(\widetilde{O}(1/\varepsilon^{3})\) in our case as \(\beta\leq\lambda T=\widetilde{\Theta}(1/\varepsilon)\) is required in Corollary 3.3. This extra cost can be neglected because \(\widetilde{O}(\sqrt{m+n}/\varepsilon^{2.5})\) dominates the complexity whenever \(\varepsilon=\Omega((m+n)^{-1})\).

_Remark 4.5_.: Our multi-Gibbs sampler is based on maximum finding and consistent amplitude estimation, with a guaranteed worst-case performance in each round. In comparison, the dynamic Gibbs sampler in [6] maintains a hint vector, resulting in an amortized time complexity per round.

```
0: Quantum oracle \(\mathcal{O}_{\mathbf{A}}\) for \(\mathbf{A}\in\mathbb{R}^{n\times n}\), QRAM access to \(z\in\mathbb{R}^{n}\) with \(\|z\|_{1}\leq\beta\), polynomial \(P_{2\beta}\) with parameters \(\epsilon_{P}\) by Lemma 4.1, number \(k\) of samples. We write \(u=\mathbf{A}\mathbb{z}\).
0:\(k\) independent samples \(i_{1},i_{2},\ldots,i_{k}\).
1: Obtain \(\mathcal{O}_{\tilde{a}}\colon|i\rangle\!\left|0\right\rangle\mapsto|i\rangle\! \left|\tilde{u}_{i}\right\rangle\) by consistent quantum amplitude estimation such that \(u_{i}\leq\tilde{u}_{i}\leq u_{i}+1\).
2: Find the set \(S\subseteq[n]\) of indexes of the \(k\) largest \(\tilde{u}_{i}\) by quantum \(k\)-maximum finding, with access to \(\mathcal{O}_{\tilde{a}}\).
3: Obtain \(\tilde{u}_{i}\) for all \(i\in S\) from \(\mathcal{O}_{\tilde{a}}\), then compute \(\tilde{u}^{*}=\min\limits_{i\in S}\tilde{u}_{i}\) and \(W=\sum\limits_{i\in S}\exp(\tilde{u}_{i})+(n-k)\exp(\tilde{u}^{*})\).
4:for\(\ell=1,\ldots,k\)do
5: Prepare the quantum state \(|u_{\text{guess}}\rangle=\sum_{i\in S}\sqrt{\frac{\exp(\tilde{u}_{i})}{W}}|i \rangle+\sum_{i\notin S}\sqrt{\frac{\exp(\tilde{u}^{*})}{W}}|i\rangle\).
6: Obtain unitary \(U^{\exp}\) such that \(\langle 0|^{\otimes a}U^{\exp}|0\rangle^{\otimes a}=\text{diag}\big{(}P_{2 \beta}(u-\max\{\tilde{u},\tilde{u}^{*}\})\big{)}/4\beta\) by QSVT.
7: Post-select \(|\tilde{u}_{\text{Gibbs}}\rangle\propto\langle 0|^{\otimes a}U^{\exp}|u_{ \text{guess}}\rangle|0\rangle^{\otimes a}\) by quantum amplitude amplification.
8: Let \(i_{\ell}\) be the measurement outcome of \(|\tilde{u}_{\text{Gibbs}}\rangle\) in the computational basis.
9:endfor
10:return\(i_{1},i_{2},\ldots,i_{k}\). ```

**Algorithm 2** Quantum Multi-Gibbs Sampling \(\mathcal{O}_{\mathbf{A}z}^{\text{Gibbs}}(k,\epsilon_{\text{G}})\)

## 5 Discussion

In our paper, we propose the first quantum online algorithm for zero-sum games with near-optimal regret. This is achieved by developing a sample-based stochastic version of the optimistic multiplicative weight update method [46]. Our core technical contribution is a fast multi-Gibbs sampling, which may have potential applications in other quantum computing scenarios.

Our result naturally gives rise to some further open questions. For instance: Can we improve the dependence on \(\epsilon\) for the time complexity? And can we further explore the combination of optimistic learning and quantum computing into broader applications? Now that many heuristic quantum approaches for different machine learning problems have been realized, e.g.in Havlicek et al. [29], Saggio et al. [44], Harrigan et al. [27], can fast quantum algorithms for zero-sum games be realized in the near future?

## Acknowledgments

We would like to thank Kean Chen, Wang Fang, Ji Guan, Junyi Liu, Xinzhao Wang, Chenyi Zhang, and Zhicheng Zhang for helpful discussions. MG would like to thank Mingsheng Ying for valuable suggestions.

Minbo Gao was supported by the National Key R&D Program of China (2018YFA0306701) and the National Natural Science Foundation of China (61832015). Zhengfeng Ji was supported by a startup fund from Tsinghua University, and the Department of Computer Science and Technology, Tsinghua Univeristy. Tongyang Li was supported by a startup fund from Peking University, and the Advanced Institute of Information Technology, Peking University. Qisheng Wang was supported by the MEXT Quantum Leap Flagship Program (MEXT Q-LEAP) grants No. JPMXS0120319794.

## References

* [1] S. Aaronson, X. Chen, E. Hazan, S. Kale, and A. Nayak. Online learning of quantum states. In _Advances in Neural Information Processing Systems_, volume 31, 2018. URL https://proceedings.neurips.cc/paper/2018/file/cla3d34711ab5d85335331ca0e57f067-Paper.pdf.
* [2] A. Ambainis. Variable time amplitude amplification and quantum algorithms for linear algebra problems. In _Proceedings of the 29th Symposium on Theoretical Aspects of ComputerScience_, volume 14, pages 636-647, 2012. URL https://hal.archives-ouvertes.fr/hal-00678197.
* [3] I. Anagnostides, C. Daskalakis, G. Farina, M. Fishelson, N. Golowich, and T. Sandholm. Near-optimal no-regret learning for correlated equilibria in multi-player general-sum games. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 736-749, 2022. URL https://doi.org/10.1145/3519935.3520031.
* [4] S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and applications. _Theory of Computing_, 8(6):121-164, 2012. URL https://doi.org/10.4086/toc.2012.v008a006.
* [5] D. W. Berry, A. M. Childs, R. Cleve, R. Kothari, and R. D. Somma. Simulating Hamiltonian dynamics with a truncated Taylor series. _Physical Review Letters_, 114(9):090502, 2015. URL https://doi.org/10.1103/PhysRevLett.114.090502.
* [6] A. Bouland, Y. M. Getachew, Y. Jin, A. Sidford, and K. Tian. Quantum speedups for zero-sum games via improved dynamic Gibbs sampling. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 2932-2952, 2023. URL https://proceedings.mlr.press/v202/bouland23a.html.
* [7] F. G. Brandao and K. M. Svore. Quantum speed-ups for solving semidefinite programs. In _Proceedings of the 58th Annual Symposium on Foundations of Computer Science_, pages 415-426, 2017. URL https://doi.org/10.1109/FOCS.2017.45.
* [8] F. G. S. L. Brandao, A. Kalev, T. Li, C. Y.-Y. Lin, K. M. Svore, and X. Wu. Quantum SDP solvers: Large speed-ups, optimality, and applications to quantum learning. In _Proceedings of the 46th International Colloquium on Automata, Languages, and Programming_, volume 132, pages 27:1-27:14, 2019. URL https://doi.org/10.4230/LIPIcs.ICALP.2019.27.
* [9] G. Brassard, P. Hoyer, M. Mosca, and A. Tapp. Quantum amplitude amplification and estimation. _Quantum Computation and Information_, 305:53-74, 2002. URL https://doi.org/10.1090/conm/305/05215.
* [10] Y. Carmon, Y. Jin, A. Sidford, and K. Tian. Variance reduction for matrix games. In _Advances in Neural Information Processing Systems_, volume 32, pages 11377-11388, 2019. URL https://proceedings.neurips.cc/paper/2019/file/6c442e0e996fa84f344a14927703a8c1-Paper.pdf.
* [11] N. Cesa-Bianchi and G. Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006. URL https://doi.org/10.1017/CBO9780511546921.
* [12] X. Chen and X. Deng. Settling the complexity of two-player nash equilibrium. In _Proceedings of the 47th Annual Symposium on Foundations of Computer Science_, pages 261-272, 2006. URL https://doi.org/10.1109/FOCS.2006.69.
* [13] X. Chen, E. Hazan, T. Li, Z. Lu, X. Wang, and R. Yang. Adaptive online learning of quantum states, 2022. URL https://arxiv.org/abs/2206.00220.
* [14] Y. Chen and X. Wang. More practical and adaptive algorithms for online quantum state learning, 2020. URL https://arxiv.org/abs/2006.01013.
* [15] A. M. Childs and N. Wiebe. Hamiltonian simulation using linear combinations of unitary operations. _Quantum Information and Computation_, 12(11-12):901-924, 2012. URL https://doi.org/10.26421/QIC12.11-12-1.
* [16] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a nash equilibrium. _SIAM Journal on Computing_, 39(1):195-259, 2009. URL http://epubs.siam.org/doi/10.1137/070699652.
* [17] C. Daskalakis, A. Deckelbaum, and A. Kim. Near-optimal no-regret algorithms for zero-sum games. _Games and Economic Behavior_, 92:327-348, 2015. URL https://doi.org/10.1016/j.geb.2014.01.003.

* Daskalakis et al. [2018] C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training gans with optimism. In _Proceedings of the 6th International Conference on Learning Representations_, 2018.
* Durr et al. [2006] C. Durr, M. Heiligman, P. Hoyer, and M. Mhalla. Quantum query complexity of some graph problems. _SIAM Journal on Computing_, 35(6):1310-1328, 2006. URL http://doi.org/10.1137/050644719.
* Gilyen et al. [2019] A. Gilyen, Y. Su, G. H. Low, and N. Wiebe. Quantum singular value transformation and beyond: Exponential improvements for quantum matrix arithmetics. In _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing_, pages 193-204, 2019. URL https://doi.org/10.1145/3313276.3316366.
* Giovannetti et al. [2008] V. Giovannetti, S. Lloyd, and L. Maccone. Quantum random access memory. _Physical Review Letters_, 100(16):160501, 2008. URL https://doi.org/10.1103/PhysRevLett.100.160501.
* Grigoriadis and Khachiyan [1995] M. D. Grigoriadis and L. G. Khachiyan. A sublinear-time randomized approximation algorithm for matrix games. _Operations Research Letters_, 18(2):53-58, 1995. URL https://doi.org/10.1016/0167-6377(95)00032-0.
* Grover and Rudolph [2002] L. Grover and T. Rudolph. Creating superpositions that correspond to efficiently integrable probability distributions, 2002. URL http://arxiv.org/abs/quant-ph/0208112.
* Grover [2000] L. K. Grover. Synthesis of quantum superpositions by quantum computation. _Physical Review Letters_, 85(6):1334, 2000. URL https://doi.org/10.1103/PhysRevLett.85.1334.
* Haah [2019] J. Haah. Product decomposition of periodic functions in quantum signal processing. _Quantum_, 3:190, 2019. URL https://doi.org/10.22331/q-2019-10-07-190.
* Hamoudi [2022] Y. Hamoudi. Preparing many copies of a quantum state in the black-box model. _Physical Review A_, 105(6):062440, 2022. URL https://doi.org/10.1103/PhysRevA.105.062440.
* Harrigan et al. [2021] M. P. Harrigan, K. J. Sung, M. Neeley, K. J. Satzinger, F. Arute, K. Arya, J. Atalaya, J. C. Bardin, R. Barends, S. Boixo, M. Broughton, B. B. Buckley, D. A. Buell, B. Burkett, N. Bushnell, Y. Chen, Z. Chen, Ben Chiaro, R. Collins, W. Courtney, S. Demura, A. Dunsworth, D. Eppens, A. Fowler, B. Foxen, C. Gidney, M. Giustina, R. Graff, S. Habegger, A. Ho, S. Hong, T. Huang, L. B. Ioffe, S. V. Isakov, E. Jeffrey, Z. Jiang, C. Jones, D. Kafri, K. Kechedzhi, J. Kelly, S. Kim, P. V. Klimov, A. N. Korotkov, F. Kostritsa, D. Landhuis, P. Laptev, M. Lindmark, M. Leib, O. Martin, J. M. Martinis, J. R. McClean, M. McEwen, A. Megrant, X. Mi, M. Mohseni, W. Mruczkiewicz, J. Mutus, O. Naaman, C. Neill, F. Neukart, M. Y. Niu, T. E. O'Brien, B. O'Gorman, E. Ostby, A. Petukhov, H. Putterman, C. Quintana, P. Roushan, N. C. Rubin, D. Sank, A. Skolik, V. Smelyanskiy, D. Strain, M. Streif, M. Szalay, A. Vainsencher, T. White, Z. J. Yao, P. Yeh, A. Zalcman, L. Zhou, H. Neven, D. Bacon, E. Lucero, E. Farhi, and R. Babbush. Quantum approximate optimization of non-planar graph problems on a planar superconducting processor. _Nature Physics_, 17(3):332-336, 2021. URL https://doi.org/10.1038/s41567-020-01105-y.
* Harrow et al. [2009] A. W. Harrow, A. Hassidim, and S. Lloyd. Quantum algorithm for linear systems of equations. _Physical Review Letters_, 103(15):150502, 2009. URL https://doi.org/10.1103/PhysRevLett.103.150502.
* Havlicek et al. [2019] V. Havlicek, A. D. Corcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta. Supervised learning with quantum-enhanced feature spaces. _Nature_, 567(7747):209-212, 2019. URL https://doi.org/10.1038/s41586-019-0980-2.
* Hsieh et al. [2021] Y.-G. Hsieh, K. Antonakopoulos, and P. Mertikopoulos. Adaptive learning in continuous games: Optimal regret bounds and convergence to nash equilibrium. In _Proceedings of the 34th Conference on Learning Theory_, volume 134, pages 2388-2422, 2021. URL https://proceedings.mlr.press/v134/hsieh21a.html.

* Kapoor et al. [2016] A. Kapoor, N. Wiebe, and K. Svore. Quantum perceptron models. In _Advances in Neural Information Processing Systems_, volume 29, 2016. URL https://proceedings.neurips.cc/paper/2016/file/d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf.
* Kerenidis and Prakash [2017] I. Kerenidis and A. Prakash. Quantum recommendation systems. In _Proceedings of the 8th Innovations in Theoretical Computer Science Conference_, volume 67, pages 49:1-49:21, 2017. URL https://doi.org/10.4230/LIPIcs.ITCS.2017.49.
* Li et al. [2019] T. Li, S. Chakrabarti, and X. Wu. Sublinear quantum algorithms for training linear and kernel-based classifiers. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97, pages 3815-3824, 2019. URL https://proceedings.mlr.press/v97/1119b.html.
* Lloyd et al. [2014] S. Lloyd, M. Mohseni, and P. Rebentrost. Quantum principal component analysis. _Nature Physics_, 10(9):631-633, 2014. URL https://doi.org/10.1038/nphys3029.
* Myerson [1981] R. B. Myerson. Optimal auction design. _Mathematics of Operations Research_, 6(1):58-73, 1981. URL https://doi.org/10.1287/moor.6.1.58.
* Nemirovski [2004] A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004. URL https://doi.org/10.1137/S1052623403425629.
* Nesterov [2005] Y. Nesterov. Smooth minimization of non-smooth functions. _Mathematical Programming_, 103(1):127-152, may 2005. URL https://doi.org/10.1007/s10107-004-0552-5.
* Nesterov [2005] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. _SIAM Journal on Optimization_, 16(1):235-249, 2005. URL https://doi.org/10.1137/S1052623403422285.
* Ozols et al. [2013] M. Ozols, M. Roetteler, and J. Roland. Quantum rejection sampling. _ACM Transactions on Computation Theory_, 5(3):11:1-33, 2013. URL https://doi.org/10.1145/2493252.2493256.
* Pattathil et al. [2022] S. Pattathil, K. Zhang, and A. Ozdaglar. Symmetric (optimistic) natural policy gradient for multi-agent learning with parameter convergence, 2022. URL https://arxiv.org/abs/2210.12812.
* Poulin and Wocjan [2009] D. Poulin and P. Wocjan. Sampling from the thermal quantum Gibbs state and evaluating partition functions with a quantum computer. _Physical Review Letters_, 103(22):220502, 2009. URL https://doi.org/10.1103/PhysRevLett.103.220502.
* Rebentrost et al. [2014] P. Rebentrost, M. Mohseni, and S. Lloyd. Quantum support vector machine for big data classification. _Physical Review Letters_, 113:130503, Sep 2014. URL https://doi.org/10.1103/PhysRevLett.113.130503.
* Roughgarden and Tardos [2002] T. Roughgarden and E. Tardos. How bad is selfish routing? _Journal of the ACM_, 49(2):236-259, mar 2002. URL https://doi.org/10.1145/506147.506153.
* Saggio et al. [2021] V. Saggio, B. E. Asenbeck, A. Hamann, T. Stromberg, P. Schiansky, V. Dunjko, N. Friis, N. C. Harris, M. Hochberg, D. Englund, S. Wolk, H. J. Briegel, and P. Walther. Experimental quantum speed-up in reinforcement learning agents. _Nature_, 591(7849):229-233, 2021. URL https://doi.org/10.1038/s41586-021-03242-7.
* Schelling [1980] T. C. Schelling. _The Strategy of Conflict: with a new Preface by the Author_. Harvard university press, 1980.
* Syrgkanis et al. [2015] V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. In _Advances in Neural Information Processing Systems_, pages 2989-2997, 2015.
* Ta-Shma [2013] A. Ta-Shma. Inverting well conditioned matrices in quantum logspace. In _Proceedings of the 45th Annual ACM Symposium on Theory of Computing_, pages 881-890, 2013. URL https://doi.org/10.1145/2488608.2488720.

* van Apeldoorn and Gilyen [2019] J. van Apeldoorn and A. Gilyen. Quantum algorithms for zero-sum games, 2019. URL http://arxiv.org/abs/1904.03180.
* van Apeldoorn and Gilyen [2019] J. van Apeldoorn and A. Gilyen. Improvements in quantum SDP-solving with applications. In _Proceedings of the 46th International Colloquium on Automata, Languages, and Programming_, volume 132, pages 99:1-99:15, 2019. URL https://doi.org/10.4230/LIPIcs.ICALP.2019.99.
* van Apeldoorn et al. [2017] J. van Apeldoorn, A. Gilyen, S. Gribling, and R. de Wolf. Quantum SDP-solvers: Better upper and lower bounds. In _Proceedings of the 58th Annual Symposium on Foundations of Computer Science_, pages 403-414, 2017. URL https://doi.org/10.1109/FOCS.2017.44.
* von Neumann [1928] J. von Neumann. Zur theorie der gesellschaftsspiele. _Mathematische Annalen_, 100(1):295-320, 1928.
* Yang et al. [2020] F. Yang, J. Jiang, J. Zhang, and X. Sun. Revisiting online quantum state learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):6607-6614, 2020.

Revisit of Optimistic Online Learning

In this section, we briefly review some important properties in the classical optimistic online learning algorithms. Some of the propositions in this section will be frequently used in the proof of the regret bound.

For convenience, we will use \(\psi(\cdot)\) to denote the negative entropy function, i.e., \(\psi\colon\Delta_{n}\to\mathds{R}\), \(\psi(p)=\sum_{i=1}^{n}p_{i}\log p_{i}\). Note that \(\log\) stands for the natural logarithm function with base e.

For a vector norm \(\left\|\cdot\right\|\), its dual norm is defined as:

\[\left\|y\right\|_{*}=\max_{x}\left\{\left\langle x,y\right\rangle:\left\|x \right\|\leq 1\right\}.\]

**Proposition A.1**.: _Let \(L\) be a vector in \(n\)-dimensional space. If \(p^{*}=\underset{p\in\Delta_{n}}{\operatorname{arg\,min}}\left\{\left\langle p,L\right\rangle+\psi\left(p\right)\right\}\), then \(p^{*}\) can be written as:_

\[p^{*}=\frac{\exp\left(-L\right)}{\left\|\exp\left(-L\right)\right\|_{1}}\]

_and vice versa._

Proof.: Write \(L=(L_{1},L_{2},\ldots,L_{n})\). By definition, we know that \(p^{*}\) is the solution to the following convex optimization problem:

\[\underset{p_{1},p_{2},\ldots,p_{n}}{\operatorname{minimize}} p_{1}\] subject to \[\sum_{i=1}^{n}p_{i}=1,\] \[\forall i,p_{i}\geq 0\]

The Lagrangian is

\[\mathcal{L}(p,u,v)=\sum_{i=1}^{n}p_{i}L_{i}+\sum_{i=1}^{n}p_{i}\log p_{i}-\sum _{i=1}^{n}u_{i}p_{i}+v\left(\sum_{i=1}^{n}p_{i}-1\right)\]

From KKT conditions, we know that the stationarity is:

\[L_{i}+1+\log p_{i}-u_{i}+v=0.\] (1)

The complementary slackness is:

\[u_{i}p_{i}=0.\]

The primal feasibility is

\[\forall i,p_{i}\geq 0;\sum_{i=1}^{n}p_{i}=1.\]

The dual feasibility is

\[u_{i}\geq 0.\]

If \(u_{i}\neq 0\) then \(p_{i}=0\), from stationarity we know \(u_{i}=-\infty\), but that violates the dual feasibility. So we can conclude that \(u_{i}=0\) for all \(i\in[n]\), thus \(p_{i}\propto\exp(-L_{i})\) and the result follows. 

Now we present a generalized version of the optimistic multiplicative weight algorithm called optimistically follow the regularized leader (Opt-FTRL) in Algorithm 3. In the algorithm, \(m_{t}\) has the same meaning as \(m(t)\) for notation consistency.

```
0: The closed convex domain \(\mathcal{X}\).
0: Step size \(\lambda\), loss gradient prediction \(m\).  Initialize \(L_{0}\gets 0\), choose appropriate \(m_{1}\). for\(t=1,\ldots,T\)do  Choose loss \(l_{t}\), update \(L_{t}=L_{t-1}+l_{t}\).  Compute \(m_{t+1}\) using observations till now. endfor ```

**Algorithm 3** Optimistic follow-the-regularized-leader
Now we study a crucial property that leads to the fast convergence of the algorithm, called the Regret bounded by Variation in Utilities (RVU in short). For simplicity, we only consider the linear loss function \(l_{t}(x)=\left\langle l_{t},x\right\rangle\). (There is a little abuse of notation here.)

**Definition A.2** (Regret bounded by Variations in Utilities (RVU), Definition 3 in Syrgkanis et al. [46]).: Consider an online learning algorithm \(\mathcal{A}\) with regret \(\mathcal{R}(T)=\varrho(T)\), we say that it has the property of regret bound by variation in utilities if for any linear loss sequence \(l_{1},l_{2},\ldots,l_{T}\), there exists parameters \(\alpha>0,0<\beta\leq\gamma\) such that the algorithm output decisions \(x_{1},x_{2},\ldots,x_{T},x_{T+1}\) that satisfy:

\[\sum_{i=1}^{T}\left\langle l_{i},x_{i}\right\rangle-\min_{x\in\mathcal{X}}\sum _{i=1}^{T}\left\langle l_{i},x\right\rangle\leq\alpha+\beta\sum_{i=1}^{T-1} \left\|l_{i+1}-l_{i}\right\|_{*}^{2}-\gamma\sum_{i=1}^{T-1}\left\|x_{i+1}-x_{ i}\right\|^{2},\]

where \(\left\|\cdot\right\|_{*}\) is the dual norm of \(\left\|\cdot\right\|\).

We do not choose the norm to be any specific one here. In fact, Syrgkanis et al. [46] have already shown that the above optimistic follow-the-regularized-leader algorithm has the RVU property with respect to any norm \(\left\|\cdot\right\|\) in which the negative entropy function \(\psi\) is \(1\)-strongly convex. So, from Pinsker's inequality, for \(l_{2}\) norms the following result holds:

**Proposition A.3** (Proposition 7 in Syrgkanis et al. [46]).: _If we choose \(m_{t}=l_{t-1}\) in the optimistic follow-the-regularized-leader algorithm with step size \(\lambda\leq 1/2\), then it has the regret bound by variation in utilities property with the parameters \(\alpha=\log n/\lambda\), \(\beta=\lambda\) and \(\gamma=1/(4\lambda)\), where \(n\) is the dimension of \(\mathcal{X}\)._

## Appendix B Regret Bound and Time Complexity of Our Algorithm

### Ideal Samplers

We assume that after the execution of our algorithm, the sequences we get are \(\left\{\left(x_{t},y_{t}\right)\right\}_{t=1}^{T+1}\) and \(\left\{\left(g_{t},h_{t}\right)\right\}_{t=1}^{T+1}\), respectively. We denote \(u_{t}:=\frac{\exp\left(-\mathbf{A}h_{t}\right)}{\left\|\exp\left(-\mathbf{A} h_{t}\right)\right\|_{1}}\) and \(v_{t}:=\frac{\exp\left(\mathbf{A}^{\intercal}g_{t}\right)}{\left\|\exp\left( \mathbf{A}^{\intercal}g_{t}\right)\right\|_{1}}\) to be the corresponding Gibbs distribution, we will first assume that the Gibbs oracle in our algorithm has no error (i.e. \(e_{\mathrm{G}}=0\)) until Theorem B.5 is proved.

**Observation B.1**.: _The sequence \(\left\{u_{t}\right\}_{t=1}^{T}\) can be seen as the decision result of applying optimistic FTRL algorithm to the linear loss function \(\mathbf{A}\eta_{t}\) with linear prediction function \(\mathbf{A}\eta_{t-1}\), and similarly for \(\left\{v_{t}\right\}_{t=1}^{T+1}\) with the loss function \(-\mathbf{A}^{\intercal}\zeta_{t}\), the prediction function \(-\mathbf{A}^{\intercal}\zeta_{t-1}\)._

Proof.: By symmetry, we only consider \(u_{t}\). Since \(u_{t}=\frac{\exp\left(-\mathbf{A}h_{t}\right)}{\left\|\exp\left(-\mathbf{A} h_{t}\right)\right\|_{1}}\), from Proposition A.1 we can write

\[u_{t}=\operatorname*{arg\,min}_{u\in\Delta_{m}}\left\{\left\langle\mathbf{A} h_{t},u\right\rangle+\psi(u)\right\}.\]

Then we notice the iteration of Algorithm 1 gives

\[h_{t}=\lambda\left(\sum_{i=1}^{t-1}\eta_{i}\right)+\lambda\eta_{t-1}.\]

So from the definition of the Algorithm 3, we know that our observation holds. 

This observation, together with Proposition A.3, gives the following inequalities. For any \(u\in\Delta_{m}\), \(v\in\Delta_{n}\), we have:

\[\sum_{t=1}^{T}\left\langle u_{t}-u,\mathbf{A}\eta_{t}\right\rangle \leq\frac{\log m}{\lambda}+\lambda\sum_{t=1}^{T-1}\left\|\mathbf{ A}(\eta_{t+1}-\eta_{t})\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T-1}\left\|u_{t+1}-u _{t}\right\|^{2},\] (2) \[\sum_{t=1}^{T}\left\langle v_{t}-v,-\mathbf{A}^{\intercal}\zeta_{ t}\right\rangle \leq\frac{\log n}{\lambda}+\lambda\sum_{t=1}^{T-1}\left\|\mathbf{A}^{ \intercal}(\zeta_{t+1}-\zeta_{t})\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T- 1}\left\|v_{t+1}-v_{t}\right\|^{2}.\] (3)However, we find that the loss function is slightly different from what we expect.

Let us consider the difference \(q_{t}:=\mathbf{A}(v_{t}-\eta_{t})\) and \(p_{t}:=-\mathbf{A}^{\intercal}(u_{t}-\zeta_{t})\), we have the decomposition of the regret:

\[\sum_{t=1}^{T}\left\langle u_{t}-u,\mathbf{A}v_{t}\right\rangle=\sum_{t=1}^{T} \left\langle u_{t}-u,\mathbf{A}\eta_{t}\right\rangle+\sum_{t=1}^{T}\left\langle u _{t}-u,q_{t}\right\rangle.\]

Notice that \(\mathds{E}[q_{t}]=\mathds{E}[p_{t}]=0\), we have:

**Lemma B.2**.: \[\mathds{E}\left[\sum_{t=1}^{T}\left\langle u_{t}-u,q_{t}\right\rangle\right] =0,\mathds{E}\left[\sum_{t=1}^{T}\left\langle v_{t}-v,p_{t}\right\rangle \right]=0\]

Proof.: By symmetry, we only prove the case for \(u\). It suffices to prove that for every \(t\), \(\mathds{E}\left[\left\langle u_{t}-u,q_{t}\right\rangle\right]=0\). Since \(u\) is fixed, \(\mathds{E}\left[\left\langle u,q_{t}\right\rangle\right]=\left\langle u, \mathds{E}\left[q_{t}\right]\right\rangle=0\).

Now consider \(\mathds{E}\left[\left\langle u_{t},q_{t}\right\rangle\right]\), notice that given \(\eta_{1},\ldots,\eta_{t-1}\) then \(u_{t}\) is a constant. We have:

\[\mathds{E}\left[\left\langle u_{t},q_{t}\right\rangle\right] =\mathds{E}\left[\left\langle u_{t},q_{t}\right\rangle\left|\eta_ {1},\eta_{2},\ldots,\eta_{t-1}\right]\right]\] \[=\mathds{E}\left[\left\langle u_{t},\mathds{E}\left[q_{t}|\eta_ {1},\eta_{2},\ldots,\eta_{t-1}\right]\right\rangle\right]\] \[=\mathds{E}\left[\left\langle u_{t},0\right\rangle\right]=0.\]

Now we are going to bound the term \(\sum_{t=1}^{T-1}\left\|\mathbf{A}(\eta_{t+1}-\eta_{t})\right\|^{2}\).

**Lemma B.3**.: \[\sum_{t=1}^{T-1}\left\|\mathbf{A}(\eta_{t+1}-\eta_{t})\right\|^{2} \leq 6+3\sum_{t=1}^{T-1}\left\|v_{t+1}-v_{t}\right\|^{2},\] (4) \[\sum_{t=1}^{T-1}\left\|\mathbf{A}^{\intercal}(\zeta_{t+1}-\zeta_ {t})\right\|^{2} \leq 6+3\sum_{t=1}^{T-1}\left\|u_{t+1}-u_{t}\right\|^{2}.\] (5)

Proof.: Recall that by rescaling we have \(\left\|\mathbf{A}\right\|\leq 1\). Hence,

\[\sum_{t=1}^{T-1}\left\|\mathbf{A}(\eta_{t+1}-\eta_{t})\right\|^{2}\leq\sum_{t =1}^{T-1}\left\|\eta_{t+1}-\eta_{t}\right\|^{2}.\]

Write \(\eta_{t+1}-\eta_{t}=(\eta_{t+1}-v_{t+1})+(v_{t+1}-v_{t})+(v_{t}-\eta_{t})\). Using the triangle inequality of the \(l_{1}\) norm and the Cauchy inequality \((a+b+c)^{2}\leq 3\big{(}a^{2}+b^{2}+c^{2}\big{)}\), we get

\[\sum_{t=1}^{T-1}\left\|\eta_{t+1}-\eta_{t}\right\|^{2}\leq 6\sum_{t=1}^{T}\left\| \eta_{t}-v_{t}\right\|^{2}+3\sum_{t=1}^{T-1}\left\|v_{t+1}-v_{t}\right\|^{2}.\] (6)

Similarly, we have:

\[\sum_{t=1}^{T-1}\left\|\zeta_{t+1}-\zeta_{t}\right\|^{2}\leq 6\sum_{t=1}^{T} \left\|\zeta_{t}-u_{t}\right\|^{2}+3\sum_{t=1}^{T-1}\left\|u_{t+1}-u_{t}\right\| ^{2}.\] (7)

Observing that in our algorithm we collect \(T\) independent and identically distributed samples and take their average, we have:

\[\mathds{E}\Bigg{[}\sum_{t=1}^{T}\left\|\zeta_{t}-u_{t}\right\|^{2} \Bigg{]} \leq 1,\] \[\mathds{E}\Bigg{[}\sum_{t=1}^{T}\left\|\eta_{t}-v_{t}\right\|^{2} \Bigg{]} \leq 1.\]

Combining the result above, we just get the desired equation.

We also need the following lemma to guarantee that the sum of the regret is always non-negative.

**Lemma B.4**.: _The sum of the regrets of two players in Algorithm 1 is always non-negative. In other words:_

\[\max_{u\in\Delta_{m}}\max_{v\in\Delta_{n}}\Biggl{(}\sum_{t=1}^{T}\left\langle u_ {t}-u,\mathbf{A}v_{t}\right\rangle+\sum_{t=1}^{T}\left\langle v_{t}-v,- \mathbf{A}^{\intercal}u_{t}\right\rangle\Biggr{)}\geq 0.\]

Proof.: \[\max_{u\in\Delta_{m}}\max_{v\in\Delta_{n}}\Biggl{(}\sum_{t=1}^{T} \left\langle u_{t}-u,\mathbf{A}v_{t}\right\rangle+\sum_{t=1}^{T}\left\langle v _{t}-v,-\mathbf{A}^{\intercal}u_{t}\right\rangle\Biggr{)}\] \[= \max_{u\in\Delta_{m}}\max_{v\in\Delta_{n}}\Biggl{(}\sum_{t=1}^{T} \left\langle-u,\mathbf{A}v_{t}\right\rangle+\sum_{t=1}^{T}\left\langle v, \mathbf{A}^{\intercal}u_{t}\right\rangle\Biggr{)}\] \[= \max_{v\in\Delta_{n}}\sum_{t=1}^{T}\left\langle v,\mathbf{A}^{ \intercal}u_{t}\right\rangle-\min_{u\in\Delta_{m}}\sum_{t=1}^{T}\left\langle u,\mathbf{A}v_{t}\right\rangle\geq 0\]

The last step is because

\[\max_{v\in\Delta_{n}}\sum_{t=1}^{T}\left\langle v,\mathbf{A}^{\intercal}u_{t} \right\rangle\geq\left\langle\mathbf{A}\sum_{t=1}^{T}v_{t}/T,\sum_{t=1}^{T}u_ {t}\right\rangle,\]

and

\[\min_{u\in\Delta_{m}}\sum_{t=1}^{T}\left\langle u,\mathbf{A}v_{t}\right\rangle \leq\left\langle\mathbf{A}\sum_{t=1}^{T}v_{t},\sum_{t=1}^{T}u_{t} /T\right\rangle.\]

Combining the result above, we finally have the following theorem.

**Theorem B.5**.: _Suppose that in our Algorithm 1, we choose the episode \(T=\widetilde{\Theta}(1/\varepsilon)\), and choose a constant learning rate \(\lambda\) that satisfies \(\lambda<\sqrt{3}/6\). Then with probability at least \(2/3\) the total regret of the algorithm is \(\widetilde{O}(1)\). To be more clear, we have:_

\[T\biggl{(}\max_{v\in\Delta_{n}}\left\langle v,\mathbf{A}^{\intercal}\bar{u} \right\rangle-\min_{u\in\Delta_{m}}\left\langle u,\mathbf{A}\bar{\phi}\right\rangle \biggr{)}\leq 36\lambda+\frac{3\log(mn)}{\lambda},\]

_and so our algorithm returns an \(\varepsilon\)-approximate Nash equilibrium._

Proof.: Adding the inequalities (2) and (3) together, we get

\[\sum_{t=1}^{T}\left\langle u_{t}-u,\mathbf{A}\eta_{t}\right\rangle +\sum_{t=1}^{T}\left\langle v_{t}-v_{t}-\mathbf{A}^{\intercal} \zeta_{t}\right\rangle\leq\frac{\log m}{\lambda}+\frac{\log n}{\lambda}\] \[\quad+\lambda\sum_{t=1}^{T-1}\left\|\mathbf{A}(\eta_{t+1}-\eta_{t })\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T-1}\left\|v_{t+1}-v_{t}\right\|^ {2}\] (8) \[\quad+\lambda\sum_{t=1}^{T-1}\left\|\mathbf{A}^{\intercal}(\zeta_ {t+1}-\zeta_{t})\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T-1}\left\|u_{t+1 }-u_{t}\right\|^{2}.\]

Taking expectation, and using the inequalities (6) we have

\[\mathbb{E}\left[\lambda\sum_{t=1}^{T-1}\left\|\mathbf{A}(\eta_{t +1}-\eta_{t})\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T-1}\left\|v_{t+1}-v_{ t}\right\|^{2}\right]\] \[\leq\left(3\lambda-\frac{1}{4\lambda}\right)\mathbb{E}\left[\sum_ {t=1}^{T-1}\left\|v_{t+1}-v_{t}\right\|^{2}\right]+6\lambda\cdot\mathbb{E} \left[\sum_{t=1}^{T}\left\|\eta_{t}-v_{t}\right\|^{2}\right]\] \[\leq 6\lambda.\]Similarly we can prove

\[\mathds{E}\left[\lambda\sum_{t=1}^{T-1}\left\|\mathbf{A}^{\intercal}(\zeta_{t+1}- \zeta_{t})\right\|^{2}-\frac{1}{4\lambda}\sum_{t=1}^{T-1}\left\|u_{t+1}-u_{t} \right\|^{2}\right]\leq 6\lambda.\]

So, taking expectations of Equation (8), and using the above inequalities and the Lemma B.2, we get

\[\mathds{E}\left[\max_{u\in\Delta_{m}}\sum_{t=1}^{T}\left\langle u_{t}-u, \mathbf{A}v_{t}\right\rangle+\max_{v\in\Delta_{n}}\sum_{t=1}^{T}\left\langle v _{t}-v_{r}-\mathbf{A}^{\intercal}u_{t}\right\rangle\right]\leq 12\lambda+\frac{\log(mn)}{\lambda}.\] (9)

Using the fact that

\[\mathds{E}[\hat{u}]\cdot T =\sum_{t=1}^{T}\mathds{E}[u_{t}],\] \[\mathds{E}[\hat{\sigma}]\cdot T =\sum_{t=1}^{T}\mathds{E}[v_{t}],\]

we have

\[\mathds{E}\left[\max_{v\in\Delta_{n}}\left\langle v,\mathbf{A}^{\intercal} \hat{u}\right\rangle-\min_{u\in\Delta_{m}}\left\langle u,\mathbf{A}\hat{v} \right\rangle\right]\cdot T\leq 12\lambda+\frac{\log(mn)}{\lambda}.\] (10)

By Lemma B.4, we know that the regret is always non-negative. So applying Markov's inequality, we know with probability at least \(2/3\), the following inequality holds:

\[\max_{v\in\Delta_{n}}\left\langle v,\mathbf{A}^{\intercal}\hat{u}\right\rangle -\min_{u\in\Delta_{m}}\left\langle u,\mathbf{A}\hat{v}\right\rangle\leq\frac{1 }{T}\bigg{(}36\lambda+\frac{3\log(mn)}{\lambda}\bigg{)}.\]

### Samplers with Errors

**Theorem B.6** (Restatement of Theorem 3.2).: _Suppose that in our Algorithm 1, we choose the episode \(T=\widetilde{O}(1/\varepsilon)\), and choose a constant learning rate \(\lambda\) that satisfies \(0<\lambda<\sqrt{3}/6\). The quantum implementation of the oracle in the algorithm will return \(T\) independent and identically distributed samples from a distribution that is \(\epsilon_{\mathrm{G}}\)-close to the desired distribution in total variational distance in quantum time \(T_{\mathrm{G}}^{\mathrm{Q}}\)._

_Then with probability at least \(2/3\) the total regret of the algorithm is \(\widetilde{O}(1+\epsilon_{\mathrm{G}}/\varepsilon)\) and the algorithm returns an \(\widetilde{O}(\varepsilon+\epsilon_{\mathrm{G}})\)-approximate Nash equilibrium in quantum time \(\widetilde{O}(T_{\mathrm{G}}^{\mathrm{Q}}/\varepsilon)\)._

Proof.: We will follow similar steps of proof for Theorem B.5. Since the sampling is not from the ideal distribution, we must bound the terms where \(\eta_{t}\) and \(\zeta_{t}\) take place.

Notice that in this case, we have

\[\left\|A(v_{t}-\mathds{E}[\eta_{t}])\right\|\leq\left\|v_{t}-\mathds{E}[\eta_ {t}]\right\|\leq\epsilon_{\mathrm{G}}.\]

So for the term \(q_{t}\) in Lemma B.2 we now have the bound:

\[\mathds{E}\left[\sum_{t=1}^{T}\left\langle u_{t}-u,A(v_{t}-\eta_{ t})\right\rangle\right]\] \[=\mathds{E}\left[\sum_{t=1}^{T}\left\langle u_{t}-u,A(v_{t}- \mathds{E}[\eta_{t}])\right\rangle\right]+\mathds{E}\left[\sum_{t=1}^{T} \left\langle u_{t}-u,A(\mathds{E}[\eta_{t}]-\eta_{t})\right\rangle\right]\] \[=\mathds{E}\left[\sum_{t=1}^{T}\left\langle u_{t}-u,A(v_{t}- \mathds{E}[\eta_{t}])\right\rangle\right]\leq 2T\epsilon_{\mathrm{G}},\]

where the last step is by Holder's inequality.

Then for the other term, we have

\[\mathbb{E}\left[\sum_{t=1}^{T}\|\eta_{t}-v_{t}\|^{2}\right] \leq 2\cdot\mathbb{E}\left[\sum_{t=1}^{T}\|\eta_{t}-\mathbb{E}[\eta _{t}]\|^{2}\right]+2\cdot\mathbb{E}\left[\sum_{t=1}^{T}\|v_{t}-\mathbb{E}[\eta _{t}]\|^{2}\right]\] \[\leq 2+2T\epsilon_{\mathrm{G}}^{2}.\]

So following the similar steps of proof for Theorem B.5, and using the above bounds, we can get

\[\mathbb{E}\left[\max_{u\in\Delta_{m}}\sum_{t=1}^{T}\left\langle u _{t}-u,\mathbf{A}v_{t}\right\rangle+\max_{v\in\Delta_{n}}\sum_{t=1}^{T}\left\langle v _{t}-v,-\mathbf{A}^{\top}u_{t}\right\rangle\right]\] \[\leq 24\lambda+24\lambda T\epsilon_{\mathrm{G}}^{2}+\frac{\log(mn )}{\lambda}+4T\epsilon_{\mathrm{G}}.\]

Again using linearity of expectation and Markov's inequality, we conclude that with probability at least \(2/3\)

\[T\bigg{(}\max_{v\in\Delta_{n}}\left\langle v,\mathbf{A}^{\top}\hat{u}\right\rangle -\min_{u\in\Delta_{m}}\left\langle u,\mathbf{A}\phi\right\rangle\bigg{)}\leq 72 \lambda+\frac{3\log(mn)}{\lambda}+72T\lambda\epsilon_{\mathrm{G}}^{2}+12T \epsilon_{\mathrm{G}}.\]

## Appendix C Consistent Quantum Amplitude Estimation

**Theorem C.1** (Consistent phase estimation, [2, 47]).: _Suppose \(U\) is a unitary operator. For every positive reals \(\epsilon,\delta\), there is a quantum algorithm (a unitary quantum circuit) \(\mathcal{A}\) such that, on input \(O\big{(}\log\big{(}\epsilon^{-1}\big{)}\big{)}\)-bit random string \(s\), it holds that_

* _For every eigenvector_ \(|\psi_{\theta}\rangle\) _of_ \(U\) _(where_ \(U|\psi_{\theta}\rangle=\exp(\mathrm{i}\theta)|\psi_{\theta}\rangle\)_), with probability_ \(\geq 1-\epsilon\)_:_ \[\left\langle\psi_{\theta}\right|\left\langle f(s,\theta)\right|\left|A|\psi_{ \theta}\right\rangle|0\right\rangle\geq 1-\epsilon;\]
* \(f(s,\theta)\) _is a function of_ \(s\) _and_ \(\theta\) _such that_ \(|f(s,\theta)-\theta|<\delta\)_,_

_with time complexity \(\widetilde{O}\big{(}\delta^{-1}\big{)}\cdot\mathrm{poly}\big{(}\epsilon^{-1} \big{)}\)._

**Theorem C.2** (Consistent quantum amplitude estimation).: _Suppose \(U\) is a unitary operator such that_

\[U|0\rangle_{A}|0\rangle_{B}=\sqrt{p}|0\rangle_{A}|\phi_{0}\rangle_{B}+\sqrt{1 -p}|1\rangle_{A}|\phi_{1}\rangle_{B}.\]

_where \(p\in[0,1]\) and \(|\phi_{0}\rangle\) and \(|\phi_{1}\rangle\) are normalized pure quantum states. Then for every positive reals \(\epsilon,\delta\), there is a quantum algorithm that, on input \(O\big{(}\log\big{(}\epsilon^{-1}\big{)}\big{)}\)-bit random string \(s\), outputs \(f(s,p)\in[0,1]\) such that_

\[\mathbf{Pr}\left[|f(s,p)-p|\leq\delta\right]\geq 1-\epsilon,\]

_with time complexity \(\widetilde{O}\big{(}\delta^{-1}\big{)}\cdot\mathrm{poly}\big{(}\epsilon^{-1} \big{)}\)._

Proof.: Suppose \(U\) is a unitary operator such that

\[U|0\rangle_{A}|0\rangle_{B}=\sqrt{p}|0\rangle_{A}|\phi_{0}\rangle_{B}+\sqrt{1 -p}|1\rangle_{A}|\phi_{1}\rangle_{B}.\]

Let

\[Q=-U(I-2|0\rangle_{A}\langle 0|\otimes|0\rangle_{B}\langle 0|)U^{\top}(I-2|0 \rangle_{A}\langle 0|\otimes I_{B}).\]

Similar to the analysis in Brassard et al. [9], we have

\[U|0\rangle_{A}|0\rangle_{B}=\frac{-\mathrm{i}}{\sqrt{2}}\big{(}\exp\big{(} \mathrm{i}\theta_{p}\big{)}|\psi_{+}\rangle_{AB}-\exp\big{(}-\mathrm{i}\theta _{p}\big{)}|\psi_{-}\rangle_{AB}\big{)},\]

where \(\sin^{2}\big{(}\theta_{p}\big{)}=p\) (\(0\leq\theta_{p}<\pi/2\)), and

\[|\psi_{\pm}\rangle_{AB}=\frac{1}{\sqrt{2}}(|0\rangle_{A}|\phi_{0}\rangle_{B} \pm\mathrm{i}|1\rangle_{A}|\phi_{1}\rangle_{B}).\]Note that \(|\psi_{\pm}\rangle_{AB}\) are eigenvectors of \(Q\), i.e., \(Q|\psi_{\pm}\rangle_{AB}=\exp\bigl{(}\pm\mathrm{i}2\theta_{p}\bigr{)}|\psi_{\pm} \rangle_{AB}\).

Now applying the algorithm \(\mathcal{A}\) of consistent phase estimation of \(Q\) by Theorem C.1 on state \(U|0\rangle_{A}|0\rangle_{B}\otimes|0\rangle_{\mathcal{C}}\) (with an \(O\bigl{(}\log\bigl{(}\epsilon^{-1}\bigr{)}\bigr{)}\)-bit random string s), we obtain

\[\mathcal{A}(U|0\rangle_{A}|0\rangle_{B}\otimes|0\rangle_{\mathcal{C}})=\frac{- \mathrm{i}}{\sqrt{2}}\Bigl{(}\exp\bigl{(}\mathrm{i}\theta_{p}\bigr{)}\mathcal{A }(|\psi_{+}\rangle_{AB}|0\rangle_{\mathcal{C}})-\exp\bigl{(}-\mathrm{i}\theta_ {p}\bigr{)}\mathcal{A}(|\psi_{-}\rangle_{AB}|0\rangle_{\mathcal{C}})\Bigr{)}.\]

Since each of \(|\psi_{\pm}\rangle_{AB}\) is an eigenvector of \(Q\), it holds that, with probability \(\geq 1-\epsilon\),

\[\langle\psi_{\pm}|_{AB}\bigl{\langle}f\bigl{(}s,\pm 2\theta_{p}\bigr{)}|_{ \mathcal{C}}\mathcal{A}(|\psi_{\pm}\rangle_{AB}|0\rangle_{\mathcal{C}})\geq 1-\epsilon.\]

which implies that \(\mathcal{A}(U|0\rangle_{A}|0\rangle_{B}\otimes|0\rangle_{\mathcal{C}})\) is \(O\bigl{(}\sqrt{\epsilon}\bigr{)}\)-close to

\[\frac{-\mathrm{i}}{\sqrt{2}}\Bigl{(}\exp\bigl{(}\mathrm{i}\theta_{p}\bigr{)}| \psi_{+}\rangle_{AB}|f\bigl{(}s,2\theta_{p}\bigr{)}\rangle_{\mathcal{C}}-\exp \bigl{(}-\mathrm{i}\theta_{p}\bigr{)}|\psi_{-}\rangle_{AB}|f\bigl{(}s,-2\theta _{p}\bigr{)}\rangle_{\mathcal{C}}\Bigr{)}\]

in trace distance, where \(\bigl{|}f\bigl{(}s,\pm 2\theta_{p}\bigr{)}\mp 2\theta_{p}\bigr{|}<\delta\). Measuring register \(\mathcal{C}\), we denote the outcome as \(\gamma\), which will be either \(f\bigl{(}s,2\theta_{p}\bigr{)}\) or \(f\bigl{(}s,-2\theta_{p}\bigr{)}\). Finally, output \(\sin^{2}(\gamma/2)\) as the estimate of \(p\) (which is consistent). Since \(\sin^{2}(\cdot)\) is even and \(2\)-Lipschitz, the additive error is bounded by

\[\Bigl{|}\sin^{2}\Bigl{(}\frac{\gamma}{2}\Bigr{)}-p\Bigr{|}\leq 2\,\Bigl{|} \Bigl{|}\frac{\gamma}{2}\Bigr{|}-\bigl{|}\theta_{p}\bigr{|}\Bigr{|}<\delta.\]

Note that \(\mathcal{A}\) makes \(\widetilde{O}\bigl{(}\delta^{-1}\bigr{)}\cdot\mathrm{poly}\bigl{(}\epsilon^{- 1}\bigr{)}\) queries to \(Q\), thus our consistent amplitude estimation has quantum time complexity \(\widetilde{O}\bigl{(}\delta^{-1}\bigr{)}\cdot\mathrm{poly}\bigl{(}\epsilon^{- 1}\bigr{)}\). 

**Theorem C.3** (Error-Reduced Consistent quantum amplitude estimation).: _Suppose \(U\) is a unitary operator such that_

\[U|0\rangle_{A}|0\rangle_{B}=\sqrt{p}|0\rangle_{A}|\phi_{0}\rangle_{B}+\sqrt{1- p}|1\rangle_{A}|\phi_{1}\rangle_{B}.\]

_where \(p\in[0,1]\) and \(|\phi_{0}\rangle\) and \(|\phi_{1}\rangle\) are normalized pure quantum states. Then for every positive integers \(r\) and positive real \(\delta\), there is a quantum algorithm that, on input \(O(r)\)-bit random string \(s\), outputs \(f^{*}(s,p)\in[0,1]\) such that_

\[\mathbf{Pr}\left[|f^{*}(s,p)-p|\leq\delta\right]\geq 1-O(\exp(-r)),\]

_with time complexity \(\widetilde{O}\bigl{(}\delta^{-1}\bigr{)}\cdot\mathrm{poly}(r)\)._

Proof.: Consider that we divide the input random string \(s\) into \(r\) strings \(s_{1},s_{2},\ldots,s_{r}\) of length \(O(1)\). For each \(i\in[r]\), we use Theorem C.2 with input string \(s_{i}\) and parameter \(\epsilon=1/10\). So we get, for each \(i\in[r]\),

\[\mathbf{Pr}\left[|f(s_{i},p)-p|\leq\delta\right]\geq\frac{9}{10}.\]

Now we set \(f^{*}(s,p)\) to be the median of the estimations \(f(s_{i},p)\) for \(i\in[r]\). We claim it satisfies the desired property. To show that, we define random variables \(X_{i}\) for \(i\in[r]\) as follows:

\[X_{i}=\begin{cases}1,&\text{if }|f(s_{i},p)-p|\leq\delta,\\ 0,&\text{otherwise}.\end{cases}\]

Noticing \(\mathbb{E}\left[\sum_{i=1}^{r}X_{i}\right]\geq 9r/10\), and by Chernoff bound, we have:

\[\mathbf{Pr}\left[\sum_{i=1}^{r}X_{i}<\frac{r}{2}\right]\leq\exp\biggl{(}-\frac {8r}{45}\biggr{)}.\]

Thus with probability at least \(1-\exp(-8r/45)\), we know that at least half of the estimations fall into the interval \([p-\delta,p+\delta]\), and then \(f^{*}(s,p)\) returns a correct answer.

Details and Proofs of Fast Quantum Multi-Gibbs Sampling

We present the detailed version of the fast quantum multi-Gibbss sampling. Here, we use the shorthand \(\mathcal{O}_{p}^{\text{Gibbs}}=\mathcal{O}_{p}^{\text{Gibbs}}(1,0)\), and it also means the distribution of the sample.

We first define the notion of amplitude-encoding (a unitary operator that encodes a vector in its amplitudes).

**Definition D.1** (Amplitude-encoding).: A unitary operator \(V\) is said to be a \(\beta\)-amplitude-encoding of a vector \(u\in\mathbb{R}^{n}\) with non-negative entries, if

\[\langle 0|_{C}V|0\rangle_{C}|i\rangle_{A}|0\rangle_{B}=\sqrt{\frac{u_{i}}{ \beta}}|i\rangle_{A}|\psi_{i}\rangle_{B}\]

for all \(i\in[n]\).

Then, as shown in Algorithm 4, we can construct a quantum multi-Gibbs sampler for a vector \(u\) if an amplitude-encoding of the vector \(u\) is given. To complete the proof of Theorem 4.2, we only have to construct an amplitude-encoding of \(\mathbf{A}z\) (see Appendix D.2 for details).

```
0: Sample count \(k\), a \(\beta\)-amplitude-encoding \(V\) of vector \(u\in\mathbb{R}^{n}\), polynomial \(P_{2\beta}\in\mathbb{R}[x]\) that satisfies Lemma 4.1 with parameter \(\epsilon_{p}=k\epsilon_{\text{G}}^{2}/300n\).
0:\(k\) independent samples \(i_{1}\), \(i_{2}\), \(\ldots\), \(i_{k}\).
1: Obtain \(\mathcal{O}_{\tilde{n}}\colon|i\rangle|0\rangle\mapsto|i\rangle|\tilde{u}_{i}\rangle\) using \(\tilde{O}(\beta)\) queries to \(V\), where \(u_{i}\leq\tilde{u}_{i}\leq u_{i}+1\), by consistent quantum amplitude estimation (Theorem C.3).
2: Find the \(k\) largest \(\tilde{u}_{i}\)'s by quantum \(k\)-maximum finding (Theorem D.3) and let \(S\) be the set of their indexes. This can be done with \(\tilde{O}(\sqrt{nk})\) queries to \(\mathcal{O}_{\tilde{n}}\).
3: Compute \(\tilde{u}^{*}=\underset{i\in S}{\text{min}}\,\tilde{u}_{i}\), and \(W=(n-k)\exp(\tilde{u}^{*})+\underset{i\in S}{\text{min}}\,\exp(\tilde{u}_{i})\).
4:for\(\ell=1,\ldots,k\)do
5: Prepare the quantum state \[|u_{\text{guess}}\rangle=\sum_{i\in S}\sqrt{\frac{\exp(\tilde{u}_{i})}{W}}|i \rangle+\sum_{i\notin S}\sqrt{\frac{\exp(\tilde{u}^{*})}{W}}|i\rangle.\]
6: Obtain \(U_{u}=(V_{\mathcal{C}AB}^{\text{t}}\otimes I_{D})(V_{DAB}\otimes I_{C})\) being a block-encoding of \(\text{diag}(u)/\beta\). Similarly, obtain \(U_{\tilde{n}}^{\text{max}}\) being a block-encoding of \(\text{diag}(\max\{\tilde{u},\tilde{u}^{*}\})/2\beta\).
7: Obtain \(U^{-}\) being a block-encoding of \(\text{diag}(u-\max\{\tilde{u},\tilde{u}^{*}\})/4\beta\) by the LCU (Linear-Combination-of-Unitaries) technique (Theorem D.6), using \(O(1)\) queries to \(U_{u}\) and \(U_{\tilde{n}}^{\text{max}}\).
8: Obtain \(U^{\text{exp}}\) being a block-encoding of \(P_{2\beta}(\text{diag}(u-\max\{\tilde{u},\tilde{u}^{*}\})/4\beta)\) by the QSVT technique (Theorem D.7), using \(O(\beta\log(\epsilon_{p}^{-1}))\) queries to \(U^{-}\).
9: Post-select \(|\tilde{u}_{\text{post}}\rangle=\langle 0|^{\otimes a}U^{\text{exp}}|u_{ \text{guess}}\rangle|0\rangle^{\otimes a}\) by quantum amplitude amplification (Theorem D.8), and obtain \(|\tilde{u}_{\text{Gibbs}}\rangle=|\tilde{u}_{\text{post}}\rangle/\||\tilde{u}_{ \text{post}}\rangle\|\). (Suppose \(U^{\text{exp}}\) has \(a\) ancilla qubits.)
10: Measure \(|\tilde{u}_{\text{Gibbs}}\rangle\) in the computational basis and let \(i_{\ell}\in[n]\) be the outcome.
11:endfor
12:Return\(i_{1},i_{2},\ldots,i_{k}\). ```

**Algorithm 4** Quantum Multi-Gibbs Sampling implementing \(\mathcal{O}_{u}^{\text{Gibbs}}(k,\epsilon_{\text{G}})\)

### Useful Theorems

**Theorem D.2** (Quantum state preparation, [23, 32]).: _There is a data structure implemented on QRAM maintaining an array \(a_{1},a_{2},\ldots,a_{\ell}\) of positive numbers that supports the following operations._

* _Initialization: For any value_ \(c\)_, set_ \(a_{i}\gets c\) _for all_ \(i\in[\ell]\)_._
* _Assignment: For any index_ \(i\) _and value_ \(c\)_, set_ \(a_{i}\gets c\)* _State Preparation: Prepare a quantum state_ \[\left|a\right\rangle=\sum_{i\in[\ell]}\sqrt{\frac{a_{i}}{\left\|a\right\|_{1}}} \left|i\right\rangle.\]

_Each operation costs \(\operatorname{polylog}(\ell)\) time._

**Theorem D.3** (Quantum \(k\)-maximum finding, Theorem 6 of Durr et al. [19]).: _Given \(k\in[n]\) and quantum oracle \(\mathcal{O}_{u}\) for an array \(u_{1},u_{2},\ldots,u_{n}\), i.e., for every \(i\in[n]\),_

\[\mathcal{O}_{u}\left|i\right\rangle\left|0\right\rangle=\left|i\right\rangle \left|u_{i}\right\rangle,\]

_there is a quantum algorithm that, with probability \(\geq 0.99\), finds a set \(S\subseteq[n]\) of cardinality \(\left|S\right|=k\) such that \(u_{i}\geq u_{j}\) for every \(i\in S\) and \(j\notin S\), using \(O\!\left(\sqrt{nk}\right)\) queries to \(\mathcal{O}_{u}\)._

We now recall the definition of block-encoding, a crucial concept in quantum singular value transformation [20], which is used in line 9 to 12 in Algorithm 4.

**Definition D.4** (Block-encoding).: Suppose \(A\) is a linear operator on \(b\) qubits, \(\alpha,\epsilon\geq 0\) and \(a\) is a positive integer. A \((b+a)\)-qubit unitary operator \(U\) is said to be an \((\alpha,\epsilon)\)-block-encoding of \(A\), if

\[\left\|\alpha\left\langle 0\right|^{\otimes a}U\left|0\right\rangle^{ \otimes a}-A\right\|_{\mathrm{op}}\leq\epsilon.\]

**Definition D.5** (State Preparation Pair, Definition 28 of Gilyen et al. [20]).: Let \(y\in\mathbb{R}^{n}\) be a vector, specially in this context the number of coordinates starts from 0. Suppose \(\left\|y\right\|_{1}\leq\beta\). Let \(\epsilon\) be a positive real. We call a pair of unitaries \((P_{L},P_{R})\) acting on \(b\) qubits a \((\beta,\epsilon)\)-state-preparation pair for \(y\) if

\[P_{L}\left|0\right\rangle^{\otimes b} =\sum_{j=0}^{2^{b}-1}c_{j}\left|j\right\rangle,\] \[P_{R}\left|0\right\rangle^{\otimes b} =\sum_{j=0}^{2^{b}-1}d_{j}\left|j\right\rangle,\]

such that:

\[\sum_{j=0}^{m-1}\left|\beta c_{j}^{*}d_{j}-y_{j}\right|\leq\epsilon\]

and for \(j\in[2^{b}]\), \(j\geq m\), we require \(c_{j}^{*}d_{j}=0\).

We now state a theorem about linear combination of unitary operators, introduced by Berry et al. [5] and Childs and Wiebe [15]. The following form is from Gilyen et al. [20]. Again we restrict ourselves to the case of real linear combinations.

**Theorem D.6** (Linear Combination of Unitaries, Lemma 29 of Gilyen et al. [20]).: _Let \(\epsilon\) be a positive real number and \(y\in\mathbb{R}^{n}\) be a vector as in Definition D.5 with \((\beta,\epsilon_{1})\) state preparation pair \((P_{L},P_{R})\). Let \(\left\{A_{j}\right\}_{j=0}^{m-1}\) be a set of linear operators on \(s\) qubits, and forall \(j\), we have \(U_{j}\) as an \((\alpha,\epsilon_{2})\)-block-encoding of \(A_{j}\) acting on \(a+s\) qubits. Let_

\[W=\left(\sum_{j=0}^{m-1}\left|j\right\rangle\left\langle j\right|\otimes U_{j }\right)+\left(I-\sum_{j=0}^{m-1}\left|j\right\rangle\left\langle j\right| \right)\otimes I_{a+s},\]

_Then we can implement a \((\alpha\beta,\alpha\epsilon_{1}+\alpha\beta\epsilon_{2})\)-block-encoding of \(A=\sum_{j=0}^{m-1}y_{j}A_{j}\), with one query from \(P_{L}^{\dagger}\), \(P_{R}\), and \(W\)._

**Theorem D.7** (Eigenvalue transformation, Theorem 31 of Gilyen et al. [20]).: _Suppose \(U\) is an \((\alpha,\epsilon)\)-block-encoding of an Hermitian operator \(A\). For every \(\delta>0\) and real polynomial \(P\in\mathbb{R}[x]\) of degree \(d\) such that \(\left|P(x)\right|\leq 1/2\) for all \(x\in[-1,1]\), there is an efficiently computable quantum circuit \(\tilde{U}\), which is a \(\left(1,4d\sqrt{\epsilon/\alpha}+\delta\right)\)-block-encoding of \(P(A/\alpha)\), using \(O(d)\) queries to \(U\)._Finally, for quantum amplitude amplification without knowing the exact value of the amplitude, we need the following theorem:

**Theorem D.8** (Quantum amplitude amplification, Theorem 3 of Brassard et al. [9]).: _Suppose \(U\) is a unitary operator such that_

\[U|0\rangle_{A}|0\rangle_{B}=\sqrt{p}|0\rangle_{A}|\phi_{0}\rangle_{B}+\sqrt{1-p} |1\rangle_{A}|\phi_{1}\rangle_{B}.\]

_where \(p\in[0,1]\) is unknown and \(|\phi_{0}\rangle\) and \(|\phi_{1}\rangle\) are normalized pure quantum states. There is a quantum algorithm that outputs \(|0\rangle_{A}|\phi_{0}\rangle_{B}\) with probability \(\geq 0.99\), using \(O\big{(}1/\sqrt{p}\big{)}\) queries to \(U\)._

### Main Proof

We generalize Theorem 4.2 as follows.

**Theorem D.9**.: _Algorithm 4 will produce \(k\) independent and identical distributed samples from a distribution that is \(\epsilon_{\mathrm{G}}\)-close to \(\mathcal{O}_{u}^{\mathrm{Gibbs}}\) in total variation distance, in quantum time \(\widetilde{O}\Big{(}\beta\sqrt{nk}\Big{)}\)._

It is immediate to show Theorem 4.2 from Theorem D.9 by constructing a \(\beta\)-amplitude-encoding \(V\) of \(\mathbf{A}z\). To see this, let \(u=\mathbf{A}z\), then \(u_{i}=\left(\mathbf{A}z\right)_{i}\in[0,\beta]\). By Theorem D.2, we can implement a unitary operator \(\mathcal{U}_{z}^{\mathrm{QRAM}}\) such that

\[\mathcal{U}_{z}^{\mathrm{QRAM}}\colon|0\rangle_{C}|0\rangle_{B}\mapsto|0 \rangle_{C}\sum_{j\in[n]}\sqrt{\frac{z_{j}}{\beta}}|j\rangle_{B}+|1\rangle_{C} |\phi\rangle_{B}.\]

Using two queries to \(\mathcal{O}_{\mathbf{A}}\), we can construct a unitary operator \(\mathcal{O}_{\mathbf{A}}^{\prime}\) such that

\[\mathcal{O}_{\mathbf{A}}^{\prime}\colon|0\rangle_{E}|i\rangle_{A}|j\rangle_{B }\mapsto\Big{(}\sqrt{A_{ij}}|0\rangle_{E}+\sqrt{1-A_{ij}}|1\rangle_{E}\Big{)} |i\rangle_{A}|j\rangle_{B}.\]

Let

\[V=\big{(}|0\rangle_{C}\langle 0|\otimes\mathcal{O}_{\mathbf{A}}^{\prime}+|1 \rangle_{C}\langle 1|\otimes I_{EAB}\Big{)}\Big{(}\mathcal{U}_{z}^{ \mathrm{QRAM}}\otimes I_{EA}\Big{)}.\] (11)

It can be verified (see Proposition D.10) that

\[\langle 0|_{C}\langle 0|_{E}V|0\rangle_{C}|0\rangle_{E}|i\rangle_{A}|0 \rangle_{B}=\sum_{j\in[n]}\sqrt{\frac{A_{ij}z_{j}}{\beta}}|i\rangle_{A}|j \rangle_{B},\]

and thus \(\langle 0|_{C}\langle 0|_{E}V|0\rangle_{C}|0\rangle_{E}|i\rangle_{A}|0 \rangle_{B}=\sqrt{u_{i}/\beta}|i\rangle_{A}|\psi_{i}\rangle_{B}\) for some \(|\psi_{i}\rangle\). Therefore, \(V\) is a \(\beta\)-amplitude-encoding of \(\mathbf{A}z\).

Now, we will show Theorem D.9 in the following.

Proof of Theorem D.9.: Now we start to describe our algorithm. By our consistent quantum amplitude estimation (Theorem C.3), we choose an \(O(r)\)-bit random string \(s\), then we can obtain a quantum algorithm \(\mathcal{O}_{\hat{u}}\) such that, with probability \(1-O\big{(}\exp(-r)\big{)}\), for every \(i\in[n]\), it computes \(f^{*}(s,u_{i}/\beta)\) with \(\widetilde{O}\big{(}\delta^{-1}\big{)}\cdot\mathrm{poly}(r)\) queries to \(V\), where \(f^{*}(s,p)\) is a function that only depends on \(s\) and \(p\), and it holds that

\[|f^{*}(s,p)-p|\leq\delta\]

for every \(p\in[-1,1]\). Here, \(r,\delta\) are parameters to be determined. Note that

\[\frac{u_{i}}{\beta}=\big{\|}\langle 0|_{C}V|0\rangle_{C}|i\rangle_{A}|0 \rangle_{B}\big{\|}^{2},\]

so when applying consistent quantum amplitude estimation, we just use a controlled-XOR gate conditioned on the index and with \(A\) the target system, before every query to \(V\).

By quantum \(k\)-maximum finding algorithm (Theorem D.3), we can find a set \(S\subseteq[n]\) with \(|S|=k\) such that \(f^{*}(s,u_{i}/\beta)\geq f^{*}\big{(}s,u_{i}/\beta\big{)}\) for every \(i\in S\) and \(j\notin S\) with probability \(0.99-O\Big{(}\sqrt{nk}\exp(-r)\Big{)}\), using \(O\Big{(}\sqrt{nk}\Big{)}\) queries to \(\mathcal{O}_{\hat{u}}\). To obtain a constant probability, it is sufficient to choose \(r=\Theta(\log(n))\).

For each \(i\in S\), again applying our consistent quantum amplitude estimation (Theorem C.3), we can obtain the value of \(f^{*}(s,u_{i}/\beta)\) with probability \(1-O(\exp(-r))\), using \(\widetilde{O}(\delta^{-1})\cdot\operatorname{poly}(r)\) queries to \(V\); then we set

\[\hat{u}_{i}=\beta f^{*}\bigg{(}s,\frac{u_{i}}{\beta}\bigg{)}\]

for all \(i\in S\), with success probability \(1-O(k\exp(-r))\) and using \(\widetilde{O}\big{(}k\delta^{-1}\big{)}\cdot\operatorname{poly}(r)\) queries to \(V\) in total. It can be seen that \(|\hat{u}_{i}-u_{i}|\leq\beta\delta\) for every \(i\in S\).

Let \(\tilde{u}_{i}=\hat{u}_{i}+\beta\delta\), and then we store \(\tilde{u}_{i}\) for all \(i\in S\) in the data structure as in Theorem D.2 (which costs \(O(k)\) QRAM operations). Then, we calculate

\[W=(n-k)\exp(\tilde{u}^{*})+\sum_{i\in S}\exp(\tilde{u}_{i})\]

by classical computation in \(\widetilde{O}(k)\) time, where

\[\tilde{u}^{*}=\min_{i\in S}\tilde{u}_{i}.\]

By Theorem D.2, we can prepare the quantum state

\[|u_{\text{guess}}\rangle=\sum_{i\in S}\sqrt{\frac{\exp(\tilde{u}_{i})}{W}}|i \rangle+\sum_{i\notin S}\sqrt{\frac{\exp(\tilde{u}^{*})}{W}}|i\rangle\]

in \(\widetilde{O}(1)\) time.

Now we introduce another system \(D\), and then let

\[U_{u}=(V_{\text{C}AB}^{\ddagger}\otimes I_{D})(V_{DAB}\otimes I_{\text{C}}).\]

It can be shown (see Proposition D.11) that \(U_{u}\) is a \((1,0)\)-block-encoding of \(\operatorname{diag}(u)/\beta\). By QRAM access to \(\tilde{u}_{i}\), we can implement a unitary operator

\[V_{\tilde{u}}\colon|i\rangle_{A}|0\rangle_{B}\mapsto|i\rangle_{A}\Bigg{(} \sqrt{\frac{\max\{\tilde{u}_{i},\tilde{u}^{*}\}}{2\beta}}|0\rangle_{B}+\sqrt{ 1-\frac{\max\{\tilde{u}_{i},\tilde{u}^{*}\}}{2\beta}}|1\rangle_{B}\Bigg{)}\]

in \(\widetilde{O}(1)\) time by noting that \(\max\{\tilde{u}_{i},\tilde{u}^{*}\}=\tilde{u}_{i}\) if \(i\in S\) and \(\tilde{u}^{*}\) otherwise. We introduce one-qubit system \(C\), and let

\[U_{\tilde{u}}^{\max}=\Big{(}V_{\tilde{u}}^{\ddagger}\otimes I_{\text{C}} \Big{)}(\text{SWAP}_{BC}\otimes I_{A})(V_{\tilde{u}}\otimes I_{\text{C}}).\]

It can be shown that \(U_{\tilde{u}}^{\max}\) is a \((1,0)\)-block-encoding of \(\operatorname{diag}(\max\{\tilde{u},\tilde{u}^{*}\})/2\beta\). Applying the LCU technique (Theorem D.6), we can obtain a unitary operator \(U^{-}\) that is a \((1,0)\)-block-encoding of \(\operatorname{diag}(u-\max\{\tilde{u},\tilde{u}^{*}\})/4\beta\), using \(O(1)\) queries to \(U_{u}\) and \(U_{\tilde{u}}^{\max}\). By the QSVT technique (Theorem D.7 and Lemma 4.1), we can construct a unitary operator \(U^{\text{exp}}\) that is a \((1,0)\)-block-encoding of \(P_{2\beta}(\operatorname{diag}(u-\max\{\tilde{u},\tilde{u}^{*}\})/4\beta)\), using \(O(\beta\log(\varepsilon_{p}^{-1}))\) queries to \(U^{-}\), where

\[\bigg{|}P_{2\beta}(x)-\frac{1}{4}\exp(2\beta x)\bigg{|}\leq\varepsilon_{P}\]

for every \(x\in[-1,0]\) and \(\varepsilon_{P}\in(0,1/2)\) is to be determined. Suppose \(U^{\text{exp}}\) has an \(a\)-qubit ancilla system, and let \(|\tilde{u}_{\text{post}}\rangle=\langle 0|^{\otimes a}U^{\text{exp}}|u_{ \text{guess}}\rangle|0\rangle^{\otimes a}\). Note that

\[|\tilde{u}_{\text{post}}\rangle=\sum_{i\in S}P_{2\beta}\bigg{(}\frac{u_{i}- \tilde{u}_{i}}{4\beta}\bigg{)}\sqrt{\frac{\exp(\tilde{u}_{i})}{W}}|i\rangle+ \sum_{i\notin S}P_{2\beta}\bigg{(}\frac{u_{i}-\tilde{u}^{*}}{4\beta}\bigg{)} \sqrt{\frac{\exp(\tilde{u}^{*})}{W}}|i\rangle.\]

It can be shown (Proposition D.12) that \(\||\tilde{u}_{\text{post}}\rangle\|^{2}\geq\Theta(k/n)\); thus by quantum amplitude amplification (Theorem D.8), we can obtain

\[|\tilde{u}_{\text{Gibbs}}\rangle=\frac{|\tilde{u}_{\text{post}}\rangle}{\|| \tilde{u}_{\text{post}}\rangle\|}\]using \(O(\sqrt{n/k})\) queries to \(U^{\mathrm{exp}}\). By measuring \(|\tilde{u}_{\mathrm{Gibbs}}\rangle\) on the computational basis, we return the outcome as a sample from the distribution \(\tilde{u}_{\mathrm{Gibbs}}\); it can be shown (Proposition D.13) that the total variation distance between \(\tilde{u}_{\mathrm{Gibbs}}\) and \(\mathbb{O}_{u}^{\mathrm{Gibbs}}\) is bounded by

\[d_{\mathrm{TV}}\left(\tilde{u}_{\mathrm{Gibbs}},\mathbb{O}_{u}^{\mathrm{Gibbs }}\right)\leq\sqrt{\frac{88n\epsilon_{P}}{k\exp(-2\beta\delta)}}.\]

By taking \(\delta=1/2\beta\) and \(\epsilon_{P}=k\epsilon_{G}^{2}/300n\), we can produce one sample from \(\tilde{u}_{\mathrm{Gibbs}}\), using \(\widetilde{O}(\beta\sqrt{n/k})\) queries to \(U_{u}\) and \(U_{u}^{\mathrm{max}}\), with \(\widetilde{O}(\beta\sqrt{nk})\)-time precomputation.

Finally, by applying \(k\) times the above procedure (with the precomputation processed only once), we can produce \(k\) independent and identically distributed samples from \(\tilde{u}_{\mathrm{Gibbs}}\) that is \(\epsilon_{\mathrm{Gibbs}}\)-close to the Gibbs distribution \(\mathbb{O}_{u}^{\mathrm{Gibbs}}\), with total time complexity

\[\widetilde{O}\Big{(}\beta\sqrt{nk}\Big{)}+k\cdot\widetilde{O}\bigg{(}\beta \sqrt{\frac{n}{k}}\bigg{)}=\widetilde{O}\Big{(}\beta\sqrt{nk}\Big{)}.\]

### Technical Lemmas

**Proposition D.10**.: _Let \(V\) defined by Equation (11), we have_

\[\langle 0|_{C}\langle 0|_{D}V|0\rangle_{C}|0\rangle_{D}|i\rangle_{A}|0 \rangle_{B}=\sum_{j\in[n]}\sqrt{\frac{A_{i}jz_{j}}{\beta}}|i\rangle_{A}|j \rangle_{B}.\]

Proof.: \(V|0\rangle_{C}|0\rangle_{D}|i\rangle_{A}|0\rangle_{B}\)

\[= \big{(}|0\rangle_{C}\langle 0|\otimes\mathbb{O}_{\mathbf{A}}^{ \prime}+|1\rangle_{C}\langle 1|\otimes I_{AB}\big{)}\left(|0\rangle_{C}|0 \rangle_{D}|i\rangle_{A}\sum_{j\in[n]}\sqrt{\frac{z_{j}}{\beta}}|j\rangle_{B} +|1\rangle_{C}|0\rangle_{D}|i\rangle_{A}|\phi\rangle_{B}\right)\] \[= |0\rangle_{C}\sum_{j\in[n]}\Big{(}\sqrt{A_{i,j}}|0\rangle_{D}+ \sqrt{1-A_{i,j}}|1\rangle_{D}\Big{)}\sqrt{\frac{z_{j}}{\beta}}|i\rangle_{A}|j \rangle_{B}+|1\rangle_{C}|0\rangle_{D}|i\rangle_{A}|\phi\rangle_{B}\] \[= |0\rangle_{C}|0\rangle_{D}\sum_{j\in[n]}\sqrt{\frac{A_{i,j}z_{j}} {\beta}}|i\rangle_{A}|j\rangle_{B}+|0\rangle_{C}|1\rangle_{D}\sum_{j\in[n]} \sqrt{\frac{(1-A_{i,j})z_{j}}{\beta}}|i\rangle_{A}|j\rangle_{B}+|1\rangle_{C} |0\rangle_{D}|i\rangle_{A}|\phi\rangle_{B}.\]

**Proposition D.11**.: _In the proof of Theorem D.9, \(U_{u}\) is a \((1,0)\)-block-encoding of \(\mathrm{diag}(u)/\beta\)._

Proof.: To see this, for every \(i,j\in[n]\),

\[\langle 0|_{C}\langle 0|_{D}\langle j|_{A}\langle 0|_{B}U_{u}|0 \rangle_{C}|0\rangle_{D}|i\rangle_{A}|0\rangle_{B}\] \[= \langle 0|_{C}\langle 0|_{D}\langle j|_{A}\langle 0|_{B}(V_{ CAB}^{\dagger}\otimes I_{D})(V_{DAB}\otimes I_{C})|0\rangle_{C}|0 \rangle_{D}|i\rangle_{A}|0\rangle_{B}\] \[= \Big{(}\sqrt{u_{i}/\beta}\langle 0|_{C}\langle 0|_{D}\langle j|_{A} \langle\psi_{j}|_{B}+\langle 1|_{C}\langle 0|_{D}\langle g_{j}|_{AB}\rangle\Big{(} \sqrt{u_{i}/\beta}|0\rangle_{C}|0\rangle_{D}|i\rangle_{A}|\psi_{i}\rangle_{B}+ |0\rangle_{C}|1\rangle_{D}|g_{i}\rangle_{AB}\Big{)}\] \[= \langle j|i\rangle_{A}\frac{u_{i}}{\beta}.\]

**Proposition D.12**.: _In the proof of Theorem D.9, if \(\delta=1/2\beta\), \(E=\sum_{j\in[n]}\exp\big{(}u_{j}\big{)}\), and \(\epsilon_{P}=k\epsilon_{G}^{2}/300n\), then_

\[\Theta\bigg{(}\frac{k}{n}\bigg{)}\leq\frac{E}{16W}-2\epsilon_{P}\leq\||u_{ \mathrm{post}}\rangle\|^{2}\leq\frac{E}{16W}+3\epsilon_{P}.\]Proof.: We first give an upper bound for \(W\) in terms of \(u_{i}\) and \(\tilde{u}^{*}\). Notice that \(\tilde{u}_{i}\leq u_{i}+2\beta\delta\) for all \(i\in S\), we have:

\[W=(n-k)\exp(\tilde{u}^{*})+\sum\limits_{i\in S}\exp(\tilde{u}_{i})\leq\exp(2 \beta\delta)\Bigg{(}(n-k)\exp(u^{*})+\sum\limits_{i\in S}\exp(u_{i})\Bigg{)}.\]

Note that

\[\frac{(n-k)\exp(u^{*})+\sum\limits_{i\in S}\exp(u_{i})}{\sum\limits_{i\in[n]} \exp(u_{i})}\leq\frac{n-k}{k}+1=\frac{n}{k},\]

then we have

\[\frac{E}{W}\geq\sum\limits_{i\in[n]}\frac{\exp(u_{i})}{\exp(2\beta\delta)((n-k )\exp(u^{*})+\sum\nolimits_{i\in S}\exp(u_{i}))}\geq\frac{k}{n}\exp(-2\beta \delta).\] (12)

With this, noting that \((a-b)^{2}\geq a^{2}-2ab\) for any real \(a\) and \(b\), we have

\[\||u_{\text{pos}}\|^{2} =\sum\limits_{i\in S}\bigg{(}P_{2\beta}\bigg{(}\frac{u_{i}- \tilde{u}_{i}}{4\beta}\bigg{)}\bigg{)}^{2}\frac{\exp(\tilde{u}_{i})}{W}+\sum \limits_{i\notin S}\bigg{(}P_{2\beta}\bigg{(}\frac{u_{i}-\tilde{u}^{*}}{4\beta }\bigg{)}\bigg{)}^{2}\frac{\exp(\tilde{u}^{*})}{W}\] \[\geq\sum\limits_{i\in S}\Bigg{(}\bigg{(}\frac{1}{4}\exp\bigg{(} \frac{u_{i}-\tilde{u}_{i}}{2}\bigg{)}\bigg{)}^{2}-2\varepsilon_{P}\bigg{)} \frac{\exp(\tilde{u}_{i})}{W}\] \[\qquad+\sum\limits_{i\notin S}\Bigg{(}\bigg{(}\frac{1}{4}\exp \bigg{(}\frac{u_{i}-\tilde{u}^{*}}{2}\bigg{)}\bigg{)}^{2}-2\varepsilon_{P} \bigg{)}\frac{\exp(\tilde{u}^{*})}{W}\] \[=\frac{1}{16}\Bigg{(}\sum\limits_{i\in S}\exp(u_{i}-\tilde{u}_{i} )\frac{\exp(\tilde{u}_{i})}{W}+\sum\limits_{i\notin S}\exp(u_{i}-\tilde{u}^{*} )\frac{\exp(\tilde{u}^{*})}{W}\Bigg{)}\] \[\qquad-2\varepsilon_{P}\Bigg{(}\sum\limits_{i\in S}\frac{\exp( \tilde{u}_{i})}{W}+\sum\limits_{i\notin S}\frac{\exp(\tilde{u}^{*})}{W}\Bigg{)}\] \[\geq\frac{E}{16W}-2\varepsilon_{P}\] \[\geq\Theta\bigg{(}\frac{k}{n}\bigg{)}.\]

On the other hand, a similar argument using the inequality \((a+b)^{2}\leq a^{2}+3ab\) for positive real \(a\geq b\) gives

\[\||u_{\text{pos}}\|^{2}\leq\frac{E}{16W}+3\varepsilon_{P}.\]

These yield the proof. 

**Proposition D.13**.: _In the proof of Theorem D.9, the total variation distance between the two distributions \(\tilde{u}_{\text{Gibbs}}\) and \(\mathcal{O}_{u}^{\text{Gibbs}}\) is bounded by_

\[d_{TV}\Big{(}\tilde{u}_{\text{Gibbs}},\mathcal{O}_{u}^{\text{Gibbs}}\Big{)} \leq\sqrt{\frac{88n\varepsilon_{P}}{k\exp(-2\beta\delta)}}.\]

Proof.: Define \(E=\sum\limits_{j\in[n]}\exp\big{(}u_{j}\big{)}\). Let

\[|u_{\text{Gibbs}}\rangle=\sum\limits_{i\in[n]}\sqrt{\frac{\exp(u_{i})}{E}}\ |i\rangle\]be the intended quantum state with amplitudes the same as the Gibbs distribution \(\mathcal{O}_{u}^{\text{Gibbs}}\). The inner product between \(\ket{\tilde{u}_{\text{post}}}\) and \(\ket{u_{\text{Gibbs}}}\) can be bounded by:

\[\bra{\tilde{u}_{\text{post}}}u_{\text{Gibbs}} =\sum_{i\in S}P_{2\beta}\bigg{(}\frac{u_{i}-\tilde{u}_{i}}{4\beta} \bigg{)}\sqrt{\frac{\exp(\tilde{u}_{i})}{W}}\sqrt{\frac{\exp(u_{i})}{E}}\] \[\qquad+\sum_{i\notin S}P_{2\beta}\bigg{(}\frac{u_{i}-\tilde{u}^{* }}{4\beta}\bigg{)}\sqrt{\frac{\exp(\tilde{u}^{*})}{W}}\sqrt{\frac{\exp(u_{i}) }{E}}\] \[\geq\sum_{i\in S}\bigg{(}\frac{1}{4}\exp\bigg{(}\frac{u_{i}- \tilde{u}_{i}}{2}\bigg{)}-\epsilon_{P}\bigg{)}\sqrt{\frac{\exp(\tilde{u}_{i})}{ W}}\sqrt{\frac{\exp(u_{i})}{E}}\] \[\qquad+\sum_{i\notin S}\bigg{(}\frac{1}{4}\exp\bigg{(}\frac{u_{i} -\tilde{u}_{i}}{2}\bigg{)}-\epsilon_{P}\bigg{)}\sqrt{\frac{\exp(\tilde{u}^{*}) }{W}}\sqrt{\frac{\exp(u_{i})}{E}}\] \[\geq\frac{1}{4\sqrt{WE}}\bigg{(}\sum_{i\in\ket{n}}\exp(u_{i}) \bigg{)}-\epsilon_{P}.\]

The last step is by Cauchy's inequality. By Proposition D.12 and Equation (12), we have

\[\ket{\bra{\tilde{u}_{\text{Gibbs}}}}\ket{u_{\text{Gibbs}}}^{2} =\frac{\ket{\bra{\tilde{u}_{\text{post}}}}\ket{u_{\text{Gibbs}}} ^{2}}{\ket{\ket{\ket{\ket{\ket{\tilde{u}_{\text{post}}}}}}}^{2}}\geq\frac{E}{16 W\ket{\ket{\ket{\ket{\tilde{u}_{\text{post}}}}}}^{2}}-\frac{\epsilon_{P}}{2 \ket{\ket{\ket{\tilde{u}_{\text{post}}}}}^{2}}\] \[\geq\frac{E}{16W\bigg{(}\frac{E}{16W}+3\epsilon_{P}\bigg{)}}-\frac {\epsilon_{P}}{2\ket{\ket{\ket{\tilde{u}_{\text{post}}}}}^{2}}\] \[\geq 1-\frac{48\epsilon_{P}}{E/W}-\frac{8\epsilon_{P}}{E/W-32 \epsilon_{P}}\] \[\geq 1-\frac{48\epsilon_{P}}{k\exp(-2\beta\delta)}-\frac{8n \epsilon_{P}}{k\exp(-2\beta\delta)-32n\epsilon_{P}}\] \[\geq 1-\frac{88n\epsilon_{P}}{k\exp(-2\beta\delta)}.\]

Finally, we have

\[d_{\text{TV}}\Big{(}\tilde{u}_{\text{Gibbs}},\mathcal{O}_{u}^{ \text{Gibbs}}\Big{)} \leq\frac{1}{2}\operatorname{tr}\Big{(}\Big{\|}\ket{\tilde{u}_{ \text{Gibbs}}}\bra{\tilde{u}_{\text{Gibbs}}}-\ket{u_{\text{Gibbs}}}\bra{u_{ \text{Gibbs}}}\Big{\|}\Big{)}\] \[=\sqrt{1-\ket{\bra{\tilde{u}_{\text{Gibbs}}}}\ket{u_{\text{Gibbs}}} ^{2}}\] \[\leq\sqrt{\frac{88n\epsilon_{P}}{k\exp(-2\beta\delta)}},\]

which is bounded by \(\epsilon_{\text{G}}\) by the choice of \(\epsilon_{P}\).