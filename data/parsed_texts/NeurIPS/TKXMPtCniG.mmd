# Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training (\(1.7\sim 4\times\)), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by \(0.52pp\) than LARS and significantly reduce generalization gap by \(68.3\%\).

## 1 Introduction

Recent machine learning models have grown wider and deeper in their architectures (e.g., GPT-3 [18], M6 [12], Switch Transformer [13]). Training complex models may consume more training data to converge, which needs a surge in computing capacity and efficiency. However, hardware improvement can not keep pace with the expansion of model calculations [1].

Several techniques to speed up training are proposed. The aggregation and scattering of gradients among massive workers requires an efficient synchronization algorithm. Since the communication bandwidth between GPUs/TPUs is much higher than CPU-GPU (e.g., NVLink, Foley and Danskin [20]), several efficient synchronization strategies such as Ring-All-Reduce [15] and software toolkits like Horovod [21] are proposed to replace the traditional PS-Worker framework [12, 13]. In addition, training with LB can notably improve throughput [22, 14]. You _et al._[20] successfully train BERT using \(1024\) TPUs and a LB (64k) within 76 minutes. It demonstrates the efficiency of GPUs/TPUs in large scale parallel tasks. Small batch (SB) is not able to fully utilize those powerful GPUs/TPUs.

However, Keskar _et al._[20] theoretically analyze the LB training and finds that it can be easily trapped into sharp local minimum, leading to strong generalization gap. Hoffer _et al._[20] indicate that the generalization gap can be attributed to the fewer update steps in LB training compared with SB when using identical epochs. Dai and Zhu [20] theoretically demonstrate that training with more steps or expanding the learning rate to batch size ratio helps to converge to a flatter localminimum. Although these issues can be partly resolved by layer-wise adaptive rate scaling (LARS, You _et al._ (2017)) and layer-wise adaptive large batch (LAMB, You _et al._ (2020)), the batch size limit still exists.

To push the batch size limit and reduce generalization gap, we propose the **element-wise adaptive** techniques called variance reduced gradient descent technique (VRGD) based on GSNR of parameters. Our contributions are listed below:

* We carry out theoretical derivations of convergence rate and generalization analysis to explain why VRGD can accelerate LB training and achieve dramatically smaller generalization gap.
* We perform comprehensive LB experiments and find that VRGD can remarkably accelerate training (\(1.7\sim 4\times\)), narrow the generalization gap and improve final precision than previous SOTA (e.g., LAMB, LARS).
* VR-LAMB pushes the batch size limit of BERT pretraining up to **128k/64k** without any accuracy loss, while LAMB stops scaling at 64k/32k. VR-LARS improves the ImageNet Top-1 accuracy to \(74.82\%\) at 96k, **0.52pp** higher than LARS. The generalization gap of ImageNet trained with VR-LARS is dramatically reduced by **68.3%** comparing with LARS at 96k. VR-SGD pushes the batch size limit of DLRM from 64k to **512k** without noticeable accuracy loss.

## 2 Related Work

### Large Batch Training

Several techniques are proposed to improve the optimization and generalization ability in LB training. Goyal _et al._ (2017) propose a linear scaling rule on learning rate (LR) to achieve the same accuracy as SB and push the batch size limit of ImageNet to 8k. EXTRAP-SGD uses the extra-gradient to stabilize the optimization trajectory and smooth training (Lin _et al._, 2020). SWAP quickly trains the model with LB in the first stage and refines it by averaging the weights of multiple SB models in the second stage (Gupta _et al._, 2020). Batch Augmentation replicates multiple instances with the same batch size to improve generalization (Hoffer _et al._, 2019). The batch size of the experiments in EXTRAP-SGD/SWAP/Batch-Augmentation are less than 8k and are not compared in our experiments.

DecentLaM removes the growing momentum-incurred bias observed in DmSGD and pushes ImageNet to 32k (Yuan _et al._, 2021). Layer-wise LRs adjustment optimizers such as LARS (You _et al._, 2017), complete layer-wise adaptive rate scaling (CLARS, Huo _et al._ (2021)), LAMB (You _et al._, 2020) successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss. Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k (Liu _et al._, 2021). LANS replaces the layer-wise LR adjustment in LAMB with block-wise style (Zheng _et al._, 2020) and also pushes BERT training up to 96k. Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k (Maleki _et al._, 2021).

### Gradient Variance and GSNR

Unlike gradient mean, which is widely used in optimizers, gradient variance and its successor GSNR are less used. But gradient variance is frequently discussed in generalization gap. Johnson and Zhang (2013) propose the stochastic variance reduced gradient (SVRG) with the explicit gradient variance reduction method. Other variants of SVRG like SRVR-NPG, SVRPG and Control Variate methods are also proposed to reduce the gradient variance during training (Liu _et al._, 2020; Wang _et al._, 2013; Papini _et al._, 2018; Miller _et al._, 2017). Rainforth _et al._ (2018) use GSNR to analyze the variational bounds in variational auto-encoder (VAE). McCandlish _et al._ (2018) use GSNR to predict the useful upper bound of batch size. Smith _et al._ (2018); Devarakonda _et al._ (2017) adaptively increase the batch size during training to achieve acceleration without accuracy loss. Liu _et al._ (2020) theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance. Therefore, gradient variance and GSNR are potentially useful to train deep neural networks.

## 3 Preliminaries

### Gsn

Given a data distribution \(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\), a model \(\hat{y}=f(x,\theta)\) parameterized by \(\theta\) and the loss function \(L\). The parameters' gradient _w.r.t._ sample \((x_{i},y_{i})\) can be written as (Refer to all "notations" in the Appendix.C):

\[\mathbf{g}_{i}(\theta):=\frac{\partial L(y_{i},f(x_{i},\theta))}{\partial\theta}\] (1)

Then \(j\)-th parameter' \((\theta_{j})\) gradient computed using \((x_{i},y_{i})\) is \(\mathbf{g}_{i}(\theta_{j})\). Here we use \(i\) to index the data samples and \(j\) to index the parameters of \(\theta\). We denote the sample-wise gradient mean as \(\mathbf{\tilde{g}}(\theta)=\mathrm{E}_{(x,y)\sim\mathcal{Z}}(\mathbf{g}(x,y, \theta))\) and variance of \(\mathbf{g}_{i}(\theta)\) as \(\rho^{2}(\theta)=\mathrm{Var}_{(x,y)\sim\mathcal{Z}}(\mathbf{g}(x,y,\theta))\). The GSNR for each model parameter \(\theta_{j}\) is defined as:

\[r(\theta_{j}):=\frac{\mathbf{\tilde{g}}^{2}(\theta_{j})}{\rho^{2}(\theta_{j})}\] (2)

Intuitively, GSNR measures the consistency of the gradient direction of each parameter across a batch of data samples. The gradient space of the parameters tends to converge in the same direction when the GSNR is large, but diverge if the GSNR is small (Figure.1).

### GSNR and Generalization Gap

Consider a training set \(D=\{(x_{1},y_{1}),...,(x_{n},y_{n})\}\sim\mathcal{Z}^{(n)}\), where \(n\) samples come from \(\mathcal{Z}\), and a test set of dataset size \((n^{\prime})\) from \(\mathcal{Z^{\prime}}^{(n^{\prime})}\) denoted by \(D^{\prime}=\{(x^{\prime}_{1},y^{\prime}_{1}),...,(x^{\prime}_{n^{\prime}},y^{ \prime}_{n^{\prime}})\}\sim\mathcal{Z^{\prime}}^{(n^{\prime})}\). The empirical training and test losses can be denoted as:

\[L[D]=\frac{1}{n}\sum_{i=1}^{n}L(y_{i},f(x_{i},\theta));\quad L[D^{\prime}]= \frac{1}{n^{\prime}}\sum_{i=1}^{n^{\prime}}L(y^{\prime}_{i},f(x^{\prime}_{i}, \theta))\] (3)

respectively. Then the empirical generalization gap is given by \(L[D^{\prime}]-L[D]\). Both the training loss \(L[D]\) and the test loss \(L[D^{\prime}]\) would decrease after one training step and can be denoted as \(\Delta L[D]\) and \(\Delta L[D^{\prime}]\) respectively. The ratio between the expectations of \(\Delta L[D]\) and \(\Delta L[D^{\prime}]\) for one training step can be denoted as:

\[\mathbf{R}(\mathcal{Z},n):=\frac{E_{D,D^{\prime}\sim\mathcal{Z}^{n}}(\Delta L [D^{\prime}])}{E_{D\sim\mathcal{Z}^{n}}(\Delta L[D])}\] (4)

**Assumption 1** (Non-overfitting limit approximation of Liu _et al._(2020)).: _The parameters' gradient over the training set and test set i.e., \(\mathbf{g}_{D}(\theta)\) and \(\mathbf{g}_{D^{\prime}}(\theta)\) obey the same distribution._

Based on Assumption 1 and using a small learning rate \(\lambda\to 0\), Liu _et al._(2020) derive the relationship between the one-step generalization ratio (eq.4) and GSNR:

\[\mathbf{R}(\mathcal{Z},n)=1-\frac{1}{n}\sum_{j}W_{j}\frac{1}{r_{j}+\frac{1}{n }},\;\;\text{where}\;W_{j}:=\frac{E_{D\sim\mathcal{Z}^{n}}(\Delta L_{j}[D])}{ E_{D\sim\mathcal{Z}^{n}}(\Delta L[D])}\quad\text{with}\sum_{j}W_{j}=1\] (5)

where \(\Delta L_{j}[D]\) is the training loss reduction caused by updating \(\theta_{j}\). A more _detailed mathematical derivation_ can be found in Liu _et al._(2020). This relationship (eq.5) demonstrates that GSNR (\(r_{j}\)) plays a crucial role in determining the generalization performance of the model. Updating the model parameters with smaller GSNR leads to generalization gap growth. Also note that we have \(\mathbf{R}(\mathcal{Z},n)\to 1\) when \(n\rightarrow\infty\), which means that training with a larger dataset helps generalization.

## 4 Proposed Algorithms

In this section, we propose VRGD with their general updating rules (taking VR-SGD as an example in Algorithm.1). The SGD is shown in Appendix.D for comparison.

Figure 1: Schematic of VRGD’s mechanism: updating parameters with larger GSNR (left panel) and smaller GSNR (right panel).

### VR-SGD's Updating Rules

Consider the simple updating rule for SGD as follows:

\[\theta_{t}=\theta_{t-1}-\lambda\cdot\tilde{\mathbf{g}}(\theta_{t})\] (6)

where \(\lambda\) is the learning rate. Previous section demonstrates that updating the weights with larger GSNR confines the model's generalization gap growth during training. Therefore, GSNR can be used in the optimizer for better generalization. In the mathematical derivation of GSNR's role on the generalization gap, all sample-wise gradients for the entire dataset are used to compute the gradient variance, which is less efficient. However, in the LB training training, where each batch is large enough to accurately estimate the gradient variance, we replace the entire dataset with a LB and the sample-wise with device-wise gradient computation. Gradients on each GPU/TPU device can be synchronized using Ring-AllReduce, thus perfectly avoiding the inefficiency of gradient variance computation. The simplified gradient variance computation is as follows:

\[\sigma_{t}^{2}=\frac{1}{k}\sum_{d=1}^{k}\tilde{\mathbf{g}}_{d}^{2}(\theta_{t})- \tilde{\mathbf{g}}^{2}(\theta_{t})\] (7)

where \(k\) devices are used, each of which computes \(1/k\) part of the gradient \(\tilde{\mathbf{g}}_{d}(\theta_{t})\), the same as what data parallel does. The GSNR can then be easily calculated based on eq.2 (\(\rho^{2}(\theta_{j})\) is replaced by \(\sigma_{j}^{2}\)). The mean values of GSNR are removed at each layer before applying gradient to the parameters. This normalization of GSNR ensures that the global averaged GSNR remains at \(1.0\):

\[r(\theta_{t}^{(l)})=\frac{r(\theta_{t}^{(l)})}{\frac{1}{j}\sum_{j=1}^{J}r( \theta_{t,j}^{(l)})}\] (8)

where \(l^{th}\) layer contains \(J\) parameters. We constrain the \(max/min\) of GSNR within \(1/\gamma\) so that those neurons with very small GSNR remain active:

\[r(\theta_{t}^{(l)})=\begin{cases}\gamma,&if\ r(\theta_{t}^{(l)})<\gamma\\ 1,&if\ r(\theta_{t}^{(l)})>1\end{cases}\] (9)

where \(\gamma\) is a hyper-parameter used here. For simplicity, we **don't tune**\(\gamma\) but set it to \(0.1\) in all of our experiments by default. Finally, we element-wisely adapt \(\lambda\) according to GSNR of each parameter and get the updating rule for VR-SGD:

\[\theta_{t}=\theta_{t-1}-\lambda\cdot r(\theta_{t})\cdot\tilde{\mathbf{g}}( \theta_{t})\] (10)Figure.1 shows the mechanism of VRGD. As for a good estimation of gradient mean (left panel), optimizer should be confident to move along the direction of gradient mean or even further. However, when gradients on the devices are scatteredly distributed (right panel), updating weights with gradient mean may bring noises and slow down convergence, which should be avoided.

**Differences compared with existing LB methods:**

* The linear scaling rule uses the same large LR for all parameters, which tends to diverge when some gradients are too large; LARS/LAMB/LANS use large LRs for some layers but layer-wisely or block-wisely limit LRs when \(||\theta_{t}||\) is compatible with its updating quantity, i.e., \(||\theta_{t}||\sim||\lambda\cdot\tilde{\mathbf{g}}(\theta_{t})||\); VRGD that we propose here **element-wisely** limit the updating quantity for those parameters without confident gradient estimation (Fig.1b, large gradient variance or small GSNR).
* GSNR and its relationship with generalization gap is discussed in Liu _et al._ (2020), but further work to embed such GSNR into the optimizers is missing. In our work, we apply GSNR in the SGD/LARS/LAMB and demonstrate that GSNR helps the model maintain a small generalization gap in LB training based on the derivations of the generalization gap and ImageNet experiments.
* VRGD does not need extra-gradient used in EXTRAP-SGD or the two-stage training like SWAP. Sub gradients used in Batch Augmentation have different transforms each while VRGD uses the same transforms. Adasum adaptively sums two gradients scaled by a constant while VRGD still uses the mean gradient.

### VR-Adam, VR-LAMB and other VRGD optimizers

GSNR can be easily applied on any optimizer using the general updating rules shown above. Here we discuss those popular optimizers frequently used in the research community, e.g., SGD, Adam, LARS and LAMB. As for VR-Adam, GSNR is calculated directly based on \(\tilde{\mathbf{g}}(\theta_{t})\) and then used to adapt the gradient mean before gradients' momentum estimation. Similar with the gradients' momentum, we apply the momentum mechanism on GSNR (\(\hat{p}_{t}\)) for faster convergence. If we adapt the final update term, i.e. \(\theta_{t}\leftarrow\theta_{t-1}-\lambda\cdot r(\theta_{t})\cdot\hat{m}_{t}/( \sqrt{\hat{v}_{t}}+\varepsilon)\), the \(1^{st}\) and \(2^{nd}\) order momentum estimation (\(m_{t}\) and \(v_{t}\)) for the next training step would be biased (meaning that the update term cannot be inferred merely on \(\hat{m}_{t}\) and \(\hat{v}_{t}\) since \(r(\theta_{t})\neq 1\)).

VR-LAMB is similar to VR-Adam, except that VR-LAMB layer-wisely adapt the LRs for stable convergence when using very large LRs. VR-Adam and VR-LAMB are shown in Appendix.D. VR-LARS and VR-Momentum, which are based on LARS and Momentum, are similar to VR-SGD that it uses GSNR to adapt the gradient means before applying them to the model weights (algorithms omitted).

## 5 Theoritical Analysis

### Convergence Analysis

**Assumption 2** (bounded gradient).: \(||\nabla L(\theta)||\leq G\)

**Assumption 3** (\(l\)-smooth).: \(\exists l>0\) _satisfies \(||\nabla L(x)-\nabla L(y)||\leq l||x-y||\)_

We mathematically derive the convergence rate of VR-SGD under nonconvex settings and assume the training process satisfies Assumption.2 and Assumption.3, which are widely used in convergence analysis (Shamir and Zhang, 2013; Ghadimi and Lan, 2013; Allen-Zhu and Hazan, 2016; Allen-Zhu et al., 2019; You _et al._, 2020). Table.1 of Appendix compares the assumptions of ours and those popular optimizers. It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD. Detailed derivations can be found in Appendix.A. Then we have Theorem.1.

**Theorem 1**.: _Let \(\lambda_{t}=\sqrt{\frac{L(\theta_{t})-L(\theta^{*})}{T||\ell||_{1}}}\) and \(\frac{1}{\sqrt{\hat{T}}}=\sqrt{\frac{[(L(\theta_{t})-L(\theta^{*})||\ell||_{1} ]}{T}}\), VR-SGD is bounded by:_

\[E||\nabla L(\theta_{t})||^{2}\leq\mathcal{O}\left((1+\frac{r_{u}^{2}G^{2}}{2} )\frac{1}{r_{l}^{2}\sqrt{\hat{T}}}\right)\] (11)

_where \(r_{l}\) and \(r_{u}\) are the lower and upper bound of GSNR._

**Convergence rates discussion**: 1) The convergence rate \(\mathcal{O}(\frac{1}{\sqrt{f}})\) of VR-SGD is the same as SGD [(20)]; 2) VR-SGD's bound depends on the lower (\(r_{l}\)) and upper bound (\(r_{u}\)) of GSNR. Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger \(r_{l}\) and \(r_{u}\)), then may result in **a tighter bound with quicker convergence** (_verified by experiments shown in Figure.2_).

### Generalization Gap

This section derives the generalization gap of SGD and VR-SGD during SB and LB scenarios. Detailed derivations can be found in Appendix.B. Citing eq.14 of Liu _et al._ [(2020)] below, i.e., when training satisfies Assumption.1 and \(\lambda\to 0\), after one training step the expectation of empirical generalization gap at \(t^{th}\) step is:

\[E_{D,D^{\prime}\sim\mathcal{Z}^{n}}(\Delta_{t}L[D]-\Delta_{t}L[D^{\prime}])= \lambda\sum_{j}\sigma_{t,j}^{2}+O(\lambda^{2})\] (12)

where we use \(\sigma_{t,j}^{2}\) and \(r_{t,j}\) to denote \(\sigma^{2}(\theta_{t,j})\) and \(r(\theta_{t,j})\) for simplicity. Next, we assume that the batch size of LB is \(k\) times than that of SB. \(\lambda_{0}\) (\(\lambda\)) represents the learning rate of SB (LB). The accumulated generalization gap after training \(T\) steps for SB using SGD and \(T/k\) steps for LB can be derived as follows:

\[E(\mathbf{GAP}_{SB,SGD})\approx\lambda_{0}\sum_{t=1}^{T}\sum_{j}\sigma_{t,j}^ {2};\quad E(\mathbf{GAP}_{LB,SGD})\approx\frac{\lambda}{k}\sum_{t=1}^{T/k} \sum_{j}\sigma_{t,j}^{2}\] (13)

If we assume "\(\sigma_{t,j}\) is \(t\)-independent", eq.13 are simplified as \(E(\mathbf{GAP}_{SB,SGD})\approx\lambda_{0}T\sum_{j}\sigma_{j}^{2}\) and \(E(\mathbf{GAP}_{LB,SGD})\approx\frac{\lambda T}{k^{2}}\sum_{j}\sigma_{j}^{2}\) respectively. Taking \(\lambda=k^{2}\lambda_{0}\), \(E(\mathbf{GAP}_{LB,SGD})\) will have the same accumulated generalization gap as SB. This is known as the linear/square scaling rules. However, the assumption that "\(\sigma_{t,j}\) is \(t\)-independent" is unrealistic. Similarly, the accumulated generalization gap of VR-SGD in LB training can be written as:

\[E(\mathbf{GAP}_{LB,VR-SGD})\approx\sum_{t=1}^{T/k}\sum_{j}\frac{\lambda r_{t, j}\sigma_{t,j}^{2}}{k}=\frac{\lambda}{k}\sum_{t=1}^{T/k}\sum_{j}\mathbf{g}_{t,j}^ {2}\] (14)

**The generalization gap of SGD and VR-SGD in LB training:**

When training converges (\(\mathbf{g}_{t,j}\to 0\)), we have \(\mathbf{g}_{t,j}^{2}<\sigma_{t,j}^{2}\) because \(r_{t,j}=\mathbf{g}_{t,j}^{2}/\sigma_{t,j}^{2}\to 0\) (verified experimentally by Figure.4 of Liu _et al._ [(2020)]). Therefore, we have \(\frac{\lambda}{k}\sum_{t=1}^{T/k}\sum_{j}\mathbf{g}_{t,j}^{2}<\frac{\lambda}{ k}\sum_{t=1}^{T/k}\sum_{j}\sigma_{t,j}^{2}\), i.e., \(E(\mathbf{GAP}_{LB,VR-SGD})<E(\mathbf{GAP}_{LB,SGD})\). This inequality demonstrates that VR-SGD has a **much smaller generalization gap** than SGD in LB training (_verified by our ImageNet experiments shown in Table.3_ ).

## 6 Experiments

In this section, we show comprehensive experiments on commonly used LB benchmarks such as BERT Pretraining [(1)], ImageNet-2012 [(17)] and DLRM [(16)]. We mainly adopt the square root rules to scale LRs. We set the hyper-parameters of VRGD as \(\gamma=0.1\) and \(k\) to the minimum GPU devices that can hold the LB without out of memory for resource efficiency (but satisfy \(k\geq 8\)) in all experiments. Similar with other optimizers, VRGD can generate a generally good training curve using default sets. The \(1^{st}\) and \(2^{nd}\) order decay rates are set to \(\beta_{1}=\beta_{3}=0.9,\beta_{2}=0.999\) by default. Experiments are performed with TensorFlow on 96 DGX-A100 nodes (768-GPUs).

### BERT Pretraining

BERT pretraining is a common NLP task needs speeding up with LB training. For a fair comparison, we use the same settings as LAMB [(19)] except optimizer and learning rate: (1) BERT large pretrains using Wikipedia and BooksCorpus and then finetunes on SQuAD(v1.1) to evaluate itsprecision with F1 score; (2) A two-phase training strategy is used. First \(90\%\) steps use a sequence length of 128 (phase-1) and last \(10\%\) use a sequence length of 512 (phase-2). Mixed-Batch Training is used when batch size is set to 64k/32k, 96k/32k and 128k/64k.

We use NVIDIA's best practise1 to carry out VR-LAMB experiments and tune _nothing_ of the downstream SQuAD(v1.1) tasks (same as LAMB). Detailed hyper-parameters are listed in Appendix.D. Results shown in Table.1 indicate that:

Footnote 1: https://github.com/NVIDIA/DeepLearningExamples/tree/master

* VR-LAMB outperforms LAMB (widely used in BERT LB pretraining) in all batch sizes from 16k to 64k/32k. F1 score is improved up to \(91.49\) at 64k/32k, **0.91pp** higher than LAMB.
* VR-LAMB also outperforms Adam (with standard bias correction and LR discontinuity removal) and LANS by an improvement of **0.84pp** at 64k and **0.63pp** at 96k/32k respectively.
* VR-LAMB pushes the batch size limit up to **128k/64k** using just **4301** steps and maintains a F1 score of \(90.85\). Although Adasum achieves a F1 score of \(90.50\) at 128k/32k, but it needs \(6137\) steps to converge (\(30\%\) extra steps than VR-LAMB). VR-LAMB achieves \(50\%\) less steps than LAMB at 64k/32k and even **0.45pp** higher of F1 score than baseline.

### ImageNet with ResNet50

ImageNet training with ResNet50 v1.5 [11] is a standard CV benchmark for LB training. We use the default sets of official best practise of Google Tensorflow2 with linear LR warm-up, label smoothing and cosine LR decay (to \(0\)). It is the same setup as LARS [10]. We merely adjust the optimizers and learning rate for a fair comparison. We find some successful LB applications using Momentum, LAMB and LARS, but not for Adam, AdaGrad or AdamW optimizers [11, 12, 13]. LARS based on Momentum is more fitful on CV tasks. Therefore, we merely apply VR-LARS on ImageNet. Detailed hyper-parameters are listed in the appendix.D.

Footnote 2: https://github.com/tensorflow/models/tree/r1.13.0

Footnote 3: https://github.com/tensorflow/models/tree/r1.13.0

The results shown in Table.2 indicate that:

* VR-LARS outperforms Momentum, DecentLaM, LAMB and LARS (previous SOTA) in all batch sizes (from **0.03pp** to **0.56pp**). The improvements are higher for larger batch size.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline
**Batch Size** & **16k** & **32k** & **64k/32k** & **64k** & **96k/32k** & **96k** & **128k/32k** & **128k/64k** \\ \hline
**Steps** & 31250 & 15625 & 8599 & 7820 & 6256 & 5214 & 6137 & 4301 \\ \hline LAMB\({}^{*}\)[13] & 91.35 & 91.48 & 90.58 & - & - & - & - \\ \hline
**Adam\({}^{*}\)[14]** & - & 91.58 & 91.04 & 90.46 & - & - & - \\ \hline
**LANS\({}^{*}\)[14]** & - & - & - & - & 90.60 & - & - & - \\ \hline
**Adasum\({}^{*}\)[14]** & - & - & - & - & - & - & 90.50 & - \\ \hline
**VR-LAMB** & **91.42** & **91.58** & **91.49** & **91.30** & **91.23** & **90.70** & **90.85** \\ (**ours**) & & **(+0.07pp)** & **(+0.00pp)** & **(+0.45pp)** & **(+0.84pp)** & **(+0.63pp)** & & - \\ \hline \end{tabular}

* means the F1 scores are cited from their work.
* Using median of repeated experiments is the same as Nado _et al._[20].

\end{table}
Table 1: Dev set F1 score of **BERT pretraining and then finetuning on SQuAD(v1.1)**. Each score is the median result of \(3\) repeated experiments. The baseline of BERT-large on SQuAD(v1.1) is \(90.395\)[13].

\begin{table}
\begin{tabular}{l c c c c c c} \hline
**Batch Size** & **2k** & **4k** & **8k** & **16k** & **32k** & **64k** & **96k** \\ \hline
**Momentum\({}^{*}\)[11]** & 76.51\(\%\) & 76.44\(\%\) & 76.26\(\%\) & - & - & - \\ \hline
**DecentLaM\({}^{*}\)[13]** & 76.43\(\%\) & - & 76.19\(\%\) & 76.73\(\%\) & 76.22\(\%\) & - & - \\ \hline
**LARS\({}^{*}\)[14]** & - & 77.90\(\%\) & 76.60\(\%\) & 76.60\(\%\) & 75.30\(\%\) & 74.30\(\%\) \\ \hline
**VR-LARS** & **77.14\(\%\)** & **77.23\(\%\)** & **77.36\(\%\)** & **77.27\(\%\)** & **76.81\(\%\)** & **75.86\(\%\)** & **74.82\(\%\)** \\ (**ours**) & & **(+0.03pp)** & **(+0.31pp)** & **(+0.47pp)** & **(+0.54pp)** & **(+0.21pp)** & **(+0.56pp)** & **(+0.52pp)** \\ \hline \end{tabular}

* means the results are cited from their work.

\end{table}
Table 2: Top-1 test accuracy of **ImageNet** using ResNet50. Each test accuracy of VR-LARS(ours) averaged over 5 repeated experiments. The standard Top-1 accuracy of MLPerf-v0.5 is \(74.9\%\).

* VR-LARS achieves \(75.86\%\) accuracy at 64k batch size, **0.56pp** higher than LARS. When batch size reaches up to **96k**, VR-LARS maintains \(74.82\%\) accuracy, close to the MLPerf-v0.5 standard (\(74.9\%\)).

**Generalization Gap**: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only **1.46** for VR-LARS at 96k (**68.3%** smaller than LARS), even smaller than ConAdv+AA (\(2.2\); Liu _et al._ [2021]). Note that VR-LARS can be used together with ConAdv+AA and other techniques for further improvement.

### DLRM Training

Criteo Terabyte click logs dataset (\(4\) billion records) trained with DLRM is a standard CTR prediction benchmark newly added in MLPerf-v0.7. DLRM is used following NVIDIA's best practise1. For a fair comparison, we merely modify LRs and optimizers (hyper-parameters are listed in Appendix.D). Settings of Linear LR warm up, polynomial decay and training with \(1\) epoch are used by their default set up. Results in Table.4 indicates that:

Footnote 1: means we reproduce based on NVIDIA’s best practise.

* VR-SGD outperforms SGD in all batch size settings. Similar with experiments shown above, the improvement of VR-SGD w.r.t SGD increases along with larger batch sizes (from **0.12pp** to **2.26pp**).
* VR-SGD pushes the batch size limit up to 512k and maintains a high AUC of **0.8013**, close to the baseline of 0.8014. Note that Google's submission of MLPerf v0.7 merely uses a maximum batch size of 64k [Kumar _et al._, 2021].

## 7 Ablation Studies

### Orthogonal Experiments

In this section, we demonstrate that GSNR is important in optimization and VRGD can be applicable to most popular optimizers using CIFAR10. During CIFAR10 training with ResNet56 [He _et al._,

\begin{table}
\begin{tabular}{c c c c c c c} \hline \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Batch Size** \\ **32k** \\ **Accuracy** \\ \end{tabular} }} & \multicolumn{5}{c}{**VR-LARS (ours)**} \\  & **32k** & **64k** & **96k** & **32k** & **64k** & **96k** \\ \hline \multirow{2}{*}{\begin{tabular}{c} **Train** \\ **Accuracy** \\ \end{tabular} } & 82.50 & 79.60 & 78.90 & 80.00 & 78.06 & 76.28 \\ \hline \multirow{2}{*}{\begin{tabular}{c} **Test** \\ **Accuracy** \\ \end{tabular} } & 76.60 & 75.30 & 74.30 & 76.81 & 75.86 & 74.82 \\ \hline \multirow{2}{*}{
\begin{tabular}{c} **Generalization** \\ **Gap** \\ \end{tabular} } & \multirow{2}{*}{5.90} & \multirow{2}{*}{4.30} & \multirow{2}{*}{4.60} & \multirow{2}{*}{**3.12**} & \multirow{2}{*}{**2.20**} & \multirow{2}{*}{**1.46**} \\  & & & & & & & \\ \end{tabular}
\end{table}
Table 3: Generalization Gap of large batch train- Table 4: Test AUC of **DLRM** trained with SGD and VR-SGD in \(1\) epoch. The reported results are averaged over 5 repeated experiments. The baseline AUC is \(0.8014\) for SGD at 32k batch size.

Figure 2: Composite averaged test accuracy or AUC curves of each optimizer for **CIFAR10** experiments. The abrupt surging of accuracy at 91\({}^{th}\) and 136\({}^{th}\) epoch is caused by decaying LR with a rate of \(0.1\).

2016a,b], we use the default sets of the official best practice for Google Tensorflow2 and mainly add square-root LR scaling rules to perform the \(216\) composite experiments shown in Figure.2. Additional linear LR warm-up, label smoothing and cosine LR decay (to \(0\)) techniques are used to stabilize LB training experiments shown in Table.5, the same as ImageNet training. Detailed hyper-parameters are listed in Appendix.D. As for the test accuracy curves, Figure.2 shows the averaged composite test accuracy curve of all \(216\) experiments for the LR-batch size pairs. Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster (\(1.7\sim 4\times\)). As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is **applicable** to most popular optimizers in LB training. The improvements of VRGD comparing with their base optimizers grow with the increase of batch size. VRGD optimizers remains convergent when batch size reaches 8k.

Footnote 2: https://github.com/google-tensorflow/

### GSNR's Behaviour

To understand GSNR's behaviour in VRGD optimizers, we perform the linear regression experiments. The true weights are set to \(W_{i}=i,i\in[1,10]\) and the corresponding parameters \(w_{i}\) are initialized to zero. Given randomly generated inputs \(X\), we have the true labels as \(Y=WX\) and the MSE loss as \(L=||Y-wX||_{2}\). Finally, optimize \(w\) with \(100\) steps.

Training about \(50\) (half) steps, VR-SGD is able to converge to the test loss where SGD requires \(100\) steps (Figure.1a of Appendix.D). The weights of VR-SGD (dashed lines of Figure.1b of Appendix.D) converge faster to their ground truth. We find that \(w_{5},w_{6}\) converge firstly, then \(w_{3},w_{8}\) and finally \(w_{1},w_{10}\). Consistently, the GSNR of \(w_{5},w_{6}\) arise firstly (updating \(w_{5},w_{6}\) with larger LRs), then \(w_{3},w_{8}\) while the GSNR of \(w_{5},w_{6}\) decrease slowly (no need to update the converged weights using large LRs). Finally after step \(60\), the GSNR of \(w_{1},w_{10}\) begin to arise. Intuitively, GSNR helps element-wisely fine-tune the LRs for different weights.

### Hyper-parameters Sensitivity

There are two main hyper-parameters in VRGD, i.e., normalization strength factor \(\gamma\) and the equivalent GPU device number \(k\). We take use of linear regression trained with VR-SGD using \(batchsize=2048\) shown above to examine the hyper-parameter sensitivity.

Figure.3 shows that the optimal \(\gamma\) is around \((0.04,0.2)\) for linear regression. Test loss would be larger if \(\gamma\to 1\), which means **VR-SGD is reduced to SGD**. It again demonstrates that GSNR is valuable to improve final precision. On the other hand, the optimal \(k\) is around \([32,256]\). This means that each gradient mean calculated using \([8,64]\) samples on each GPU/TPU device, and gradient variance calculated using \([32,256]\) values of the gradient mean will return a good evaluation of GSNR. In fact, we do not use the optimal hyper-parameters. Instead, above experiments use \(\gamma=0.1\) and set \(k\) to the minimum GPU devices that can hold the LB without out of memory (but satisfy \(k\geq 8\), refer all of the hyper-parameters in Appendix.D). Fine-tuning \(\gamma\) and \(k\) may further improve the results.

## 8 Summary

In this paper, we propose the VRGD for large batch training using GSNR. We carry out theoretical derivations of convergence rate and generalization analysis to explain why VRGD can accelerate large batch training and reduce generalization gap. Comprehensive experiments on BERT-pretraining, ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers in LB training and perform better. Codes will be released when published.

## References

* Allen-Zhu and Hazan (2016) Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In _International conference on machine learning_, pages 699-707. PMLR, 2016.
* Allen-Zhu et al. (2019) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* Dai and Zhu (2018) Xiaowu Dai and Yuhua Zhu. Towards theoretical understanding of large batch training in stochastic gradient descent. _CoRR_, abs/1812.00542, 2018.
* Devarakonda et al. (2017) Aditya Devarakonda, Maxim Naumov, and Michael Garland. Adabatch: Adaptive batch sizes for training deep neural networks. _arXiv preprint arXiv:1712.02029_, 2017.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT_, pages 4171-4186. Association for Computational Linguistics, 2019.
* Fedus et al. (2021) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _arXiv preprint arXiv:2101.03961_, 2021.
* Floridi and Chiriatti (2020) Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines_, 30(4):681-694, 2020.
* Foley and Danskin (2017) Denis Foley and John Danskin. Ultra-performance pascal gpu and nvlink interconnect. _IEEE Micro_, 37(2):7-17, 2017.
* Ghadimi and Lan (2013) Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. _SIAM J. Optim._, 23(4):2341-2368, 2013.
* Gibiansky (2017) Andrew Gibiansky. Bringing hpc techniques to deep learning. _Baidu Research, Tech. Rep._, 2017.
* Goyal et al. (2017) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* Gupta et al. (2020) Vipul Gupta, Santiago Akle Serrano, and Dennis DeCoste. Stochastic weight averaging in parallel: Large-batch training that generalizes well. In _8th International Conference on Learning Representations, ICLR_. OpenReview.net, 2020.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _European conference on computer vision_, pages 630-645. Springer, 2016.
* Hoffer et al. (2017) Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems_, pages 1731-1741, 2017.
* Hoffer et al. (2019) Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: better training with larger batches. _arXiv preprint arXiv:1901.09335_, 2019.
* Huo et al. (2021) Zhouyuan Huo, Bin Gu, and Heng Huang. Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI_, pages 7883-7890. AAAI Press, 2021.
* Huo et al. (2019)Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in neural information processing systems_, 26, 2013.
* Johnson and Zhang [2013] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Christopher J. C. Burges, Leon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, pages 315-323, 2013.
* Keskar et al. [2017] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _5th International Conference on Learning Representations, ICLR_. OpenReview.net, 2017.
* Kumar et al. [2021] Sameer Kumar, Yu Emma Wang, Cliff Young, James Bradbury, Naveen Kumar, Dehao Chen, and Andy Swing. Exploring the limits of concurrency in ML training on google TPUS. In Alex Smola, Alex Dimakis, and Ion Stoica, editors, _Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021_. mlsys.org, 2021.
* Li et al. [2014] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In _11th USENIX Symposium on Operating Systems Design and Implementation_, pages 583-598, 2014.
* Li et al. [2014] Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter server. _Advances in Neural Information Processing Systems_, 27, 2014.
* Lin et al. [2020] Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi. Extrapolation for large-batch training in deep learning. In _International Conference on Machine Learning_, pages 6094-6104. PMLR, 2020.
* Lin et al. [2021] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. M6: A chinese multimodal pretrainer. _arXiv preprint arXiv:2103.00823_, 2021.
* Liu et al. [2020] Jinlong Liu, Guoqing Jiang, Yunzhi Bai, Ting Chen, and Huayan Wang. Understanding why neural networks generalize well through GSNR of parameters. In _8th International Conference on Learning Representations, ICLR_. OpenReview.net, 2020.
* Liu et al. [2020] Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. _Advances in Neural Information Processing Systems_, 33:7624-7636, 2020.
* Liu et al. [2021] Yong Liu, Xiangning Chen, Minhao Cheng, Cho-Jui Hsieh, and Yang You. Concurrent adversarial learning for large-batch training. _arXiv preprint arXiv:2106.00221_, 2021.
* Maleki et al. [2021] Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Olli Saarikivi, Tianju Xu, Vadim Eksarevskiy, Jaliya Ekanayake, and Emad Barsoum. Scaling distributed training with adaptive summation. In Alex Smola, Alex Dimakis, and Ion Stoica, editors, _Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021_. mlsys.org, 2021.
* McCandlish et al. [2018] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. _CoRR_, abs/1812.06162, 2018.
* Miller et al. [2017] Andrew Miller, Nick Foti, Alexander D'Amour, and Ryan P Adams. Reducing reparameterization gradient variance. _Advances in Neural Information Processing Systems_, 30, 2017.
* Nado et al. [2021] Zachary Nado, Justin M Gilmer, Christopher J Shallue, Rohan Anil, and George E Dahl. A large batch optimizer reality check: Traditional, generic optimizers suffice across batch sizes. _arXiv preprint arXiv:2102.06356_, 2021.
* Naumov and Mudigere [2020] Maxim Naumov and Dheevatsa Mudigere. Dlrm: An advanced, open source deep learning recommendation model, 2020.
* Nandand et al. [2018]* Papini et al. [2018] Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. In _International conference on machine learning_, pages 4026-4035. PMLR, 2018.
* Rainforth et al. [2018] Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, and Yee Whye Teh. Tighter variational bounds are not necessarily better. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML_, volume 80 of _Proceedings of Machine Learning Research_, pages 4274-4282. PMLR, 2018.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* Sergeev and Balso [2018] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorflow. _arXiv preprint arXiv:1802.05799_, 2018.
* Shamir and Zhang [2013] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In _International conference on machine learning_, pages 71-79. PMLR, 2013.
* Smith et al. [2018] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don't decay the learning rate, increase the batch size. In _6th International Conference on Learning Representations, ICLR_. OpenReview.net, 2018.
* Wang et al. [2013] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. _Advances in Neural Information Processing Systems_, 26, 2013.
* You et al. [2017] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. _arXiv preprint arXiv:1708.03888_, 2017.
* You et al. [2017] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training. _arXiv preprint arXiv:1708.03888_, 6(12):6, 2017.
* You et al. [2020] Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training BERT in 76 minutes. In _8th International Conference on Learning Representations, ICLR_. OpenReview.net, 2020.
* Yuan et al. [2021] Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Decentralam: Decentralized momentum SGD for large-batch deep training. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pages 3009-3019. IEEE, 2021.
* Zheng et al. [2020] Shuai Zheng, Haibin Lin, Sheng Zha, and Mu Li. Accelerated large batch optimization of BERT pretraining in 54 minutes. _CoRR_, abs/2006.13484, 2020.