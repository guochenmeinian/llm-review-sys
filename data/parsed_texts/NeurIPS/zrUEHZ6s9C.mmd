# Algorithm Selection for Deep Active Learning with Imbalanced Datasets

Jifan Zhang

University of Wisconsin - Madison

Madison, WI 53715

jifan@cs.wisc.edu

&Shuai Shao

Meta Inc.

Menlo Park, CA 94025

sshao@meta.com

&Saurabh Verma

Meta Inc.

Menlo Park, CA 94025

saurabh08@meta.com

Robert Nowak

University of Wisconsin - Madison

Madison, WI 53715

rdnowak@wisc.edu

###### Abstract

Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (**T**hompson **A**ct**I**ve **L**earning **a**l******OR**ithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms. Our implementation of TAILOR is open-sourced at https://github.com/jifanz/TAILOR.

## 1 Introduction

Active learning (AL) aims to reduce data labeling cost by iteratively and adaptively finding informative unlabeled examples for annotation. Label-efficiency is increasingly crucial as deep learning models require large amount of labeled training data. In recent years, numerous new algorithms have been proposed for deep active learning (Sener and Savarese, 2017; Gal et al., 2017; Ash et al., 2019; Kothawade et al., 2021; Citovsky et al., 2021; Zhang et al., 2022). Relative label efficiencies among algorithms, however, vary significantly across datasets and applications (Beck et al., 2021; Zhan et al., 2022). When it comes to choosing the best algorithm for a novel dataset or application, practitioners have mostly been relying on educated guesses and subjective preferences. Prior work (Baram et al., 2004; Hsu and Lin, 2015; Pang et al., 2018) have studied the online choice of active learning algorithms for linear models, but these methods become ineffective in deep learning settings (see Section 2). In this paper, we present the first principled approach for automatically selecting effective _deep_ AL algorithms for novel, unlabeled datasets.

We reduce the algorithm selection task to a multi-armed bandit problem. As shown in Figure 1, the idea may be viewed as a meta algorithm adaptively choosing among a set of candidate AL algorithms (arms). The objective of the meta algorithm is to maximize the cumulative reward incurred from running the chosen candidate algorithms. In Section 4, we propose reward functions that encouragethe collection of _class-balanced_ labeled set. As mentioned above, deep AL algorithms are generally proposed to maximize different notions of informativeness. As a result, by utilizing our algorithm selection strategy TAILOR, we annotate examples that are both _informative_ and _class-diverse_.

To highlight some of our results, as shown in Figure 2 for the CelebA dataset, TAILOR outperforms all candidate deep AL algorithms and collects the least amount of labels while reaching the same accuracy level. TAILOR achieves this by running a combination of candidate algorithms (see Appendix E) to yield an informative and class-diverse set of labeled examples (see Figure 3(c)).

Our key contributions are as follows:

* To our knowledge, we propose the first adaptive algorithm selection strategy for _deep_ active learning. Our algorithm TAILOR works particularly well on the challenging and prevalent class-imbalance settings (Kothawade et al., 2021; Emam et al., 2021; Zhang et al., 2022).
* Our framework is general purpose for both multi-label and multi-class classification. Active learning is especially helpful for multi-label classification due to the high annotation cost of obtaining _multiple_ labels for each example.
* TAILOR can choose among large number (e.g. hundreds) of candidate deep AL algorithms even under limited (10 or 20) rounds of interaction. This is particularly important since limiting the number of model retraining steps and training batches is essential in large-scale deep active learning (Citovsky et al., 2021).
* In Section 5, we provide regret analysis of TAILOR. Although TAILOR can be viewed as a sort of contextual bandit problem, our regret bound is better than that obtained by a naive reduction to a linear contextual bandit reduction (Russo and Van Roy, 2014).
* We provide extensive experiments on four multi-label and six multi-class image classification datasets (Section 6). Our results show that TAILOR obtains accuracies comparable or better than the best candidate strategy for nine out of the ten datasets. On all of the ten datasets, TAILOR succeeds in collecting datasets as class-balanced as the best candidate algorithm. Moreover, with a slightly different reward function designed for active search, TAILOR performs the best in finding the highest number of positive class labels on all multi-label datasets.

## 2 Related Work

**Adaptive Algorithm Selection in Active Learning.** Several past works have studied the adaptive selection of active learning algorithms for linear models. Donmez et al. (2007) studied the limited setting of switching between two specific strategies to balance between uncertainty and diversity. To choose among off-the-shelf AL algorithms, Baram et al. (2004) first proposed a framework that reduced the AL algorithm selectino task to a multi-armed bandit problem. That approach aims to maximize the cumulative reward in a Classification Entropy Maximization score, which measures the class-balancedness of predictions on unlabeled examples, after training on each newly labeled example. However, this becomes computationally intractable for large datasets with computationally expensive models. To remedy this problem, Hsu and Lin (2015) and Pang et al. (2018) proposed the use of importance weighted training accuracy scores for each newly labeled example. The training accuracy, however, is almost always \(100\%\) for deep learning models due to their universal approximation capability, which makes the reward signals less effective. Moreover, Hsu and Lin [2015] reduced their problem to an adversarial multi-armed bandit problem while Pang et al. [2018] also studied the non-stationarity of rewards over time.

Lastly, we would like to distinguish the goal of our paper from the line of _Learning Active Learning_ literature [Konyushkova et al., 2017, Shao et al., 2019, Zhang et al., 2020, Gonsior et al., 2021, Loffler and Mutschler, 2022], where they learn a single paramtereized policy model from offline datasets. These policies can nonetheless serve as individual candidate algorithms, while TAILOR aims to select the best subsets that are adapted for novel dataset instances.

**Multi-label Deep Active Learning.** Many active learning algorithms for multi-label classification based on linear models have been proposed [Wu et al., 2020], but few for deep learning. Multi-label active learning algorithms are proposed for two types of annotation, _example-based_ where all associated labels for an example are annotated, and _example-label-based_ where annotator assigns a binary label indicating whether the example is positive for the specific label class.

While Citovsky et al. [2021], Min et al. [2022] both propose deep active learning algorithms for example-label-based labels, we focus on example-based annotation in this paper. To this end, Ranganathan et al. [2018] propose an uncertainty sampling algorithm that chooses examples with the lowest class-average cross entropy losses after trained with weak supervision. We find the EMAL algorithm by Wu et al. [2014] effective on several datasets, despite being proposed for linear models. EMAL is based on simple uncertainty metric where one averages over binary margin scores for each class. Lastly, a multi-label task can be seen as individual single-label binary classification tasks for each class [Boutell et al., 2004]. By adopting this view, one can randomly interleave the above-mentioned AL algorithms for every class. In this paper, we include baselines derived from least confidence sampling [Settles, 2009], GALAXY [Zhang et al., 2022] and most likely positive sampling [Warmuth et al., 2001, 2003, Jiang et al., 2018].

**Balanced Multi-class Deep Active Learning.** Traditional uncertainty sampling algorithms have been adopted for deep active learning. These algorithms select uncertain examples based on scores derived from likelihood softmax scores, such as margin, least confidence and entropy [Tong and Koller, 2001, Settles, 2009, Balcan et al., 2006, Kremer et al., 2014]. The latter approaches leverage properties specific to neural networks by measuring uncertainty through dropout [Gal et al., 2017], adversarial examples [Ducoffe and Precioso, 2018] and neural network ensembles [Beluch et al., 2018]. Diversity sampling algorithms label examples that are most different from each other, based on similarity metrics such as distances in penultimate layer representations [Sener and Savarese, 2017, Geifman and El-Yaniv, 2017, Citovsky et al., 2021] or discriminator networks [Gissin and Shalev-Shwartz, 2019]. Lastly, gradient embeddings, which encode both softmax likelihood and penultimate layer representation, have become widely adopted in recent approaches [Ash et al., 2019, Wang et al., 2021, Elenter et al., 2022, Mohamadi et al., 2022]. As an example, Ash et al. [2019] uses k-means++ to query a diverse set of examples in the gradient embedding space.

**Unbalanced Multi-class Deep Active Learning.** More general and prevalent scenarios, such as unbalanced deep active classification, have received increasing attention in recent years [Kothawade et al., 2021, Emam et al., 2021, Zhang et al., 2022, Coleman et al., 2022, Jin et al., 2022, Aggarwal et al., 2020, Cai, 2022]. For instance, Kothawade et al. [2021] label examples with gradient embeddings that are most similar to previously collected rare examples while most dissimilar to out-of-distribution ones. Zhang et al. [2022] create linear one-vs-rest graphs based on margin scores. To collect a more class-diverse labeled set, GALAXY discovers and labels around the optimal uncertainty thresholds through a bisection procedure on shortest paths.

## 3 Problem Statement

### Notation

In pool based active learning, one starts with a large pool of \(N\) unlabeled examples \(X=\{x_{1},x_{2},...,x_{N}\}\) with corresponding ground truth labels \(Y=\{y_{1},y_{2},...,y_{N}\}\) initially unknown to the learner. Let \(K\) denote the total number of classes. In multi-label classification, each label \(y_{i}\) is denoted as \(y_{i}\in\{0,1\}^{K}\) with each element \(y_{i,j}\) representing the binary association between class \(j\) and example \(x_{i}\). On the other hand, in a multi-class problem, each label \(y_{i}\in\{e_{j}\}_{j\in[K]}\) is denoted by a canonical one-hot vector, where \(e_{j}\) is the \(j\)-th canonical vector representing the \(j\)-th class. Furthermore, at any time, we denote labeled and unlabeled examples by \(L,U\subset X\) correspondingly,where \(L\cap U=\emptyset\). We let \(L_{0}\subset X\) denote a small seed set of labeled examples and \(U_{0}=X\backslash L_{0}\) denote the initial unlabeled set. Lastly, an active learning algorithm \(\mathcal{A}\) takes as input a pair of labeled and unlabeled sets \((L,U)\) and returns an unlabeled example \(\mathcal{A}(L,U)\in U\).

### Adaptive Algorithm Selection Framework

In this section, we describe a generic framework that encompasses the online algorithm selection settings in Baram et al. (2004); Hsu and Lin (2015) and Pang et al. (2018). As shown in Algorithm 1, the meta algorithm has access to \(M\) candidate algorithms. At the beginning of any round \(t\), a multi-set of \(B\) algorithms are chosen, where the same algorithm can be chosen multiple times. One example is selected by each algorithm in the multiset sequentially, resulting in a total of \(B\) unique examples. The batch of examples are then labeled all at once. At the end of the round, their corresponding rewards are observed based on the newly annotated examples \(\{(x^{t,j},y^{t,j})\}_{j=1}^{B}\) selected by the algorithms. The model is also retrained on labeled examples before proceeding to the next round.

Overall, the meta algorithm aims to maximize the future cumulative reward based on noisy past reward observations of each candidate algorithm. Th reward function \(r:X\times Y\rightarrow\mathbb{R}\) is measured based on an algorithm's selected examples and corresponding labels. There are two key components to this framework: the choice of reward function and a bandit strategy that optimizes future rewards. Our particular design will be presented in Section 4.

``` Define:\(M\) candidate algorithms \(A=\{\mathcal{A}_{i}\}_{i\in[M]}\), pool \(X\), total number of rounds \(T\), batch size \(B\). Initialize: Labeled seed set \(L_{0}\subset X\), unlabeled set \(U_{0}=X\backslash L_{0}\) and initial policy \(\Pi^{1}\). for\(t=1,...,T\)do  Meta algorithm \(\Pi^{t}\) chooses multiset of algorithms \(\mathcal{A}_{\alpha_{t,1}},\mathcal{A}_{\alpha_{t,2}},...,\mathcal{A}_{\alpha _{t,B}}\), where indexes \(\alpha_{t,1},...,\alpha_{t,B}\in[M]\). Initialize selection set \(S_{t}\leftarrow\emptyset\). for\(j=1,...,B\)do  Run algorithm to select unlabeled example \(x^{t,j}:=\mathcal{A}_{\alpha_{t,j}}(L_{t-1},U_{t-1}\backslash S_{t})\) that is unselected.  Insert the example \(x^{t,j}\): \(S_{t}\gets S_{t}\cup\{x^{t,j}\}\). endfor  Annotate \(\{x^{t,j}\}_{j=1}^{B}\) and observe labels \(\{y^{t,j}\}_{j=1}^{B}\). Update sets \(L_{t}\gets L_{t-1}\cup S_{t}\), \(U_{t}\gets U_{t-1}\backslash S_{t}\).  Observe reward \(r^{t,j}=r(x^{t,j},y^{t,j})\) for each algorithm \(\mathcal{A}_{\alpha_{t,j}}\), where \(j\in[B]\).  Update policy statistics based on \(x^{t,j},y^{t,j}\) and \(r^{t,j}\) to obtain \(\Pi^{t+1}\) and retrain model on \(L_{t}\). endfor Objective: Maximize cumulative reward \(\sum_{t=1}^{T}\sum_{j=1}^{B}r^{t,j}\). ```

**Algorithm 1** General Meta Active Learning Framework for Baram et al. (2004); Hsu and Lin (2015); Pang et al. (2018)

We make the following two crucial assumptions for our framework:

**Assumption 3.1**.: Any candidate batch active learning algorithm \(\bar{A}\) can be decomposed into an iterative selection procedure \(A\) that returns one unlabeled example at a time.

The assumption has been inherently made by our framework above where an deep active learning algorithm returns one unlabeled example at a time. It entails that running \(\bar{A}\) once to collect a batch of \(B\) examples is equivalent with running the iterative algorithm \(A\) for \(B\) times. As noted in Appendix A.1, most existing deep active learning algorithms can be decomposed into iterative procedures and thus can serve as candidate algorithms in our framework.

**Assumption 3.2**.: For each round \(t\in[T]\), we assume there exist ground truth reward distributions \(\mathbb{P}_{t,1},...,\mathbb{P}_{t,M}\) for each candidate algorithm. Furthermore, for each element \(j\in[B]\) in the batch, we make the iid assumption that reward \(r^{t,j}\stackrel{{ iid}}{{\sim}}\mathbb{P}_{t,\alpha_{t,j}}\) is sampled from the distribution of the corresponding selected algorithm.

The iid assumption is made for theoretical simplicity by all of Baram et al. (2004); Hsu and Lin (2015); Pang et al. (2018). We say the distributions are _non-stationary_ if for any \(i\in[M]\), \(P_{t,i}\) varies with respect to time \(t\). Both this paper and Pang et al. (2018) study _non-stationary_ scenarios, whereas Baram et al. (2004) and Hsu and Lin (2015) assume the distributions are _stationary_ across time.

Thompson Active Learning Algorithm Selection

In this section, we present the two key components of our design, reward function and bandit strategy. In Section 4.1, we first present a class of reward functions designed for deep active learning under class imbalance. In Section 4.2, by leveraging the structure of such reward functions, we reduce the adaptive algorithm selection framework from Section 3.2 into a novel bandit setting. In Section 4.3, we then propose our algorithm TAILOR which is specifically designed for this setting. When using TAILOR on top of deep AL algorithms, the annotated examples are _informative_ and _class-diverse_.

### Reward Function

We propose reward functions that encourage selecting examples so that every class is well represented in the labeled dataset, ideally equally represented or "class-balanced". Our reward function works well even under practical scenarios such as limited number of rounds and large batch sizes (Citovsky et al., 2021). The rewards we propose can be efficiently computed example-wise as opposed to Baram et al. (2004) and are more informative and generalizable than Hsu and Lin (2015) and Pang et al. (2018). Our class-balance-based rewards are especially effective for datasets with underlying class imbalance. Recall \(y\in\{0,1\}^{K}\) for multi-label classification and \(y\in\{e_{i}\}_{i=1}^{K}\) for multi-class classification. We define the following types of reward functions.

* **Class Diversity**: To encourage better class diversity, we propose a reward that inversely weights each class by the number of examples already collected. For each round \(t\in[T]\), \[r^{t}_{div}(x,y)=\frac{1}{K}\sum_{i=1}^{K}\frac{1}{\max(1,\text{COUNT}^{t}(i) )}y_{:i}=:\langle v^{t}_{div},y\rangle\] where \(\text{COUNT}^{t}(i)\) denotes the number of examples in class \(i\) after \(t-1\) rounds and \(y_{:i}\) denotes the \(i\)-th element of \(y\). We let \(v^{t}_{div}\) denote the inverse weighting vector.
* **Multi-label Search**: As shown in Table 1, multi-label classification datasets naturally tend to have sparse labels (more \(0\)'s than \(1\)'s in \(y\)). Therefore, it is often important to search for positive labels. To encourage this, we define a stationary reward function for multi-label classification: \[r_{search}(x,y)=\frac{1}{K}\sum_{i=1}^{K}y_{:i}=:\langle v_{pos},y\rangle\quad \text{ where }v_{pos}=\frac{1}{K}\vec{1}.\]
* **Domain Specific**: Lastly, we would like to note that domain experts can define specialized weighting vectors of different classes \(v^{t}_{dom}\in[-\frac{1}{K},\frac{1}{K}]^{K}\) that are adjusted over time \(t\). The reward function simply takes the form \(r^{t}_{dom}(x,y)=\langle v^{t}_{dom},y\rangle\). As an example of multi-label classification of car information, one may prioritize classes of car brands over classes of car types, thus weighting each class differently. They can also adjust the weights over time based on needs.

### Novel Bandit Setting

We now present a novel bandit reduction that mirrors the adaptive algorithm selection framework under this novel class of rewards. In this setup, \(v_{t}\in[-\frac{1}{K},\frac{1}{K}]^{K}\) is arbitrarily chosen and non-stationary. On the other hand, for each candidate algorithm \(\mathcal{A}_{i}\in A\), we assume the labels \(y\) are sampled iid from a stationary 1-sub-Gaussian distribution \(\mathbb{P}_{\theta^{i}}\) with mean \(\theta^{i}\). Both the stationary assumption in \(P_{\theta^{i}}\) and the iid assumption are made for simplicity of our theoretical analysis only. We will describe our implementation to overcome the non-stationarity in \(\mathbb{P}_{\theta^{i}}\) in Section 6.1. Although we make the iid assumption analogous to Assumption 3.2, we demonstrate the effectiveness of our algorithm in Section 6 through extensive experiments. Additionally, note that \(\theta^{i}\in[0,1]^{K}\) for multi-label classification and \(\theta^{i}\in\Delta^{(K-1)}\) takes value in the \(K\) dimensional probability simplex for multi-class classification. In our bandit reduction, at each round \(t\),

1. [leftmargin=*]
2. Nature reveals weighting vector \(v^{t}\);
3. Meta algorithm chooses algorithms \(\alpha^{t,1},...,\alpha^{t,B}\), which sequentially select unlabeled examples;
4. Observe batch of labels \(y^{t,1},...,y^{t,B}\) all at once, where \(y^{t,j}\stackrel{{ iid}}{{\sim}}\mathbb{P}_{\theta^{\alpha_{t,j}}}\);
5. Objective: maximize rewards defined as \(r^{t,j}=\langle v^{t},y^{t,j}\rangle\).

This setting bears resemblance to a linear contextual bandit problem. Indeed, one can formulate such a problem close to our setting by constructing arms \(\phi_{i}^{t}=\text{vec}(v^{t}e_{i}^{\top})\in[-\frac{1}{K},\frac{1}{K}]^{KM}\). Here, \(\text{vec}(\cdot)\) vectorizes the outer product between \(v^{t}\) and the \(i\)-th canonical vector \(e_{i}\). A contextual bandit algorithm observes reward \(r=\langle\phi_{i}^{t},\theta^{\star}\rangle+\varepsilon\) after pulling arm \(i\), where \(\theta^{\star}=\text{vec}([\theta^{1},\theta^{2},...,\theta^{M}])\in[0,1]^{KM}\) and \(\varepsilon\) is some sub-Gaussian random noise. However, this contextual bandit formulation does not take into account the observations of \(\{y^{t,j}\}_{j=1}^{B}\) at each round, which are direct realizations of \(\theta^{1},...,\theta^{M}\). In fact, standard contextual bandit algorithms usually rely on least squares estimates of \(\theta^{1},...,\theta^{M}\) based on the reward signals (Russo and Van Roy, 2014). As will be shown in Proposition 5.1, a standard Bayesian regret upper bound from Russo and Van Roy (2014) is of order \(\widetilde{O}(BM^{\frac{3}{4}}K^{\frac{3}{4}}\sqrt{T})\). Our algorithm \(\mathtt{TAILOR}\), on the other hand, leverages the observations of \(y^{t,j}\sim\mathbb{P}_{\theta^{a,t,j}}\) and has regret upper bounded by \(\widetilde{O}(B\sqrt{MT})\) (Theorem 5.2), similar to a stochastic multi-armed bandit.

``` Input:\(M\) candidate algorithms \(A=\{\mathcal{A}_{i}\}_{i\in[M]}\), pool \(X\), total number of rounds \(T\), batch size \(B\). Initialize: For each \(i\in[M]\), \(a^{i}=b^{i}=\vec{1}\in\mathbb{R}^{+^{K}}\). for\(t=1,...,T\)do  Nature reveals \(v^{t}\in[-\frac{1}{K},\frac{1}{K}]^{K}\). Choose candidate algorithms: for\(j=1,...,B\)do  For each \(i\in[M]\), sample \(\widehat{\theta^{i}}\sim\text{Beta}(a^{i},b^{i})\) for multi-label or \(\widehat{\theta^{i}}\sim\text{Dir}(a^{i})\) for multi-class.  Choose \(\alpha^{t,j}\leftarrow\arg\max_{i\in[M]}\langle v^{t},\widehat{\theta^{i}}\rangle\). endfor Run chosen algorithms to collect batch: for\(j=1,...,B\)do  Run algorithm \(\mathcal{A}_{\alpha^{t,j}}\) to select unlabeled example \(x^{t,j}\) and insert into \(S_{t}\). endfor  Annotate examples in \(S_{t}\) to observe \(y^{t,j}\) for each \(j\in[B]\). Update posterior distributions:  For each algorithm \(i\in[M]\): \(a^{i}\gets a^{i}+\sum_{j:\alpha^{t,j}=i}y^{t,j},\quad b^{i}\gets b^{i }+\sum_{j:\alpha^{t,j}=i}(1-y^{t,j})\).  Retrain neural network to inform next round. endfor ```

**Algorithm 2**TAILOR : Thompson Active Learning Algorithm Selection

### Tailor

We are now ready to present \(\mathtt{TAILOR}\), a Thompson Sampling (Thompson, 1933) style meta algorithm for adaptively selecting active learning algorithms. The key idea is to maintain posterior distributions for \(\theta^{1},...,\theta^{M}\). As shown in Algorithm 2, at the beginning we utilize uniform priors \(\text{Unif}(\Omega)\) over the support \(\Omega\), where \(\Omega=\Delta^{(t-1)}\) and \([0,1]^{K}\) respectively for multi-label and multi-class classification. We note that the choice of uniform prior is made so that it is general purpose for any dataset. In practice, one may design more task-specific priors.

Over time, we keep an posterior distribution over each ground truth mean \(\theta^{i}\) for each algorithm \(i\in[M]\). With a uniform prior, the posterior distribution is an instance of either element-wise Beta distribution1 for multi-label classification or Dirichlet distribution for multi-class classification. During each round \(t\), we draw samples from the posteriors, which are then used to choose the best action (i.e., candidate algorithm) that has the largest predicted reward. After the batch of \(B\) candidate algorithms are chosen, we then sequentially run each algorithm to collect the batch of unlabeled examples. Upon receiving the batch of annotations, we then update the posterior distribution for each algorithm. Lastly, the neural network model is retrained on all labeled examples thus far.

Footnote 1: For \(z\in[0,1]^{d}\) and \(a,b\in\mathcal{Z}^{+^{d}}\), we say \(z\sim\text{Beta}(a,b)\) if for each \(i\in[d]\), \(z_{i}\sim\text{Beta}(a_{i},b_{i})\).

## 5 Analysis

In this section, we present regret upper bound of \(\mathtt{TAILOR}\) and compare against a linear contextual bandit upper bound from Russo and Van Roy (2014). Our time complexity analysis is in Appendix 5.1.

Given an algorithm \(\pi\), the _expected regret_ measures the difference between the expected cumulative reward of the optimal action and the algorithm's action. Formally for any fixed instance with \(\Theta=\{\theta^{1},...,\theta^{M}\}\), the _expected regret_ is defined as

\[R(\pi,\Theta):=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}\max_{i\in[M]}\langle v ^{t},\theta^{i}-\theta^{\alpha^{t,j}}\rangle\right]\]

where the expectation is over the randomness of the algorithm, e.g. posterior sampling in TAILOR.

_Bayesian regret_ simply measures the average of expected regret over different instances

\[BR(\pi):=\mathbb{E}_{\theta^{i}\sim\mathbb{P}_{0}(\Omega),i\in[M]}\left[R(\pi, \{\theta^{i}\}_{i=1}^{M})\right]\]

where \(\Omega\) denotes the support of each \(\theta^{i}\) and \(\mathbb{P}_{0}(\Omega)\) denotes the prior. Recall \(\Omega=[0,1]^{K}\) for multi-label classification and \(\Omega=\Delta^{(K-1)}\) for multi-class classification. While TAILOR is proposed based on uniform priors \(\mathbb{P}_{0}(\Omega)=\operatorname{uniform}(\Omega)\), our analysis in this section holds for arbitrary \(\mathbb{P}_{0}\) as long as the prior and posterior updates are modified accordingly in TAILOR.

First, we would like to mention a Bayesian regret upper bound for the contextual bandit formulation mentioned in 4.2. This provides one upper bound for TAILOR. As mentioned, the reduction to a contextual bandit is valid, but is only based on observing rewards and ignores the fact that TAILOR observes rewards and the full realizations \(y^{t,j}\) of \(\theta^{\alpha^{t,j}}\) that generate them. So one anticipates that this bound may be loose.

**Proposition 5.1** (Russo and Van Roy (2014)).: _Let \(\pi_{context}\) be the posterior sampling algorithm for linear contextual bandit presented in Russo and Van Roy (2014), the Bayesian regret is bounded by_

\[BR(\pi_{context})\leq\widetilde{O}(BM^{\frac{3}{4}}K^{\frac{3}{4}}\log T \sqrt{T})\]

_where \(B\) is the batch size, \(M\) is the number of candidate algorithms, \(K\) is the number of classes, and \(T\) is the number of rounds._

We omit the proof in this paper and would like to point the readers to section 6.2.1 in Russo and Van Roy (2014) for the proof sketch. As mentioned in the paper, detailed confidence ellipsoid of least squares estimate and ellipsoid radius upper bound can be recovered from pages 14-15 of Abbasi-Yadkori et al. (2011).

We now present an upper bound on the Bayesian regret of TAILOR, which utilizes standard sub-Gaussian tail bounds based on observations of \(y^{t,j}\) instead of confidence ellipsoids derived from only observing reward signals of \(r^{t,j}\).

**Theorem 5.2** (Proof in Appendix B).: _The Bayesian regret of TAILOR is bounded by_

\[BR(\texttt{TAILOR})\leq O(B\sqrt{MT(\log T+\log M)})\]

_where \(B\) is the batch size, \(M\) is the number of candidate algorithms and \(T\) is total number of rounds._

We delay our complete proof to Appendix B. To highlight the key difference of our analysis from Russo and Van Roy (2014), their algorithm only rely on observations of \(r^{t,j}\) for each round \(t\in[T]\) and element \(j\in[B]\) in a batch. To estimate \(\theta^{1},...,\theta^{M}\), they use the least squares estimator to form confidence ellipsoids. In our analysis, we utilize observations of \(y\)'s up to round \(t\) and form confidence intervals directly around each of \(\langle v^{t},\theta^{1}\rangle\),..., \(\langle v^{t},\theta^{M}\rangle\) by unbiased estimates \(\{\langle v^{t},y\rangle\).

### Time Complexity

Let \(N_{train}\) denote the total neural network training. The time complexity of collecting each batch for each active learning algorithm \(\mathcal{A}_{i}\) can be separated into \(P_{i}\) and \(Q_{i}\), which are the computation complexity for preprocessing and selection of each example respectively. As examples of preprocessing, BADGE (Ash et al., 2019) computes gradient embeddings, SIMILAR (Kothawade et al., 2021) further also compute similarity kernels, GALAXY (Zhang et al., 2022) constructs linear graphs, etc. The selection complexities are the complexities of each iteration of K-means++ in BADGE, greedy submodular optimization in SIMILAR, and shortest shortest path computation in GALAXY. Therefore, for any individual algorithm \(\mathcal{A}_{i}\), the computation complexity is then \(O(N_{train}+TP_{i}+TBQ_{i})\) where \(T\) is the total number of rounds and \(B\) is the batch size. When running TAILOR, as we do not know which algorithms are selected, we provide a worst case upper bound of \(O(N_{train}+T\cdot(\sum_{i=1}^{M}P_{i})+TB\cdot\max_{i\in[M]}Q_{i})\), where the preprocessing is done for every candidate algorithm. In practice, some of the preprocessing operations such as gradient embedding computation could be shared among multiple algorithms, thus only need to be computed once. As a practical note in all of our experiments, TAILOR is more than \(20\%\) faster than the slowest candidate algorithm as it selects a diverse set of candidate algorithms instead of running a single algorithm the entire time. Also, the most significant time complexity in practice often lies in neural network retraining. The retraining time dominates the running time of all algorithms including reward and Thompson sampling complexities.

## 6 Experiments

In this section, we present results of TAILOR in terms of classification accuracy, class-balance of collected labels, and total number of positive examples for multi-label active search. Motivated by the observations, we also propose some future directions at the end.

### Setup

**Datasets.** Our experiments span ten datasets with class-imbalance as shown in Table 1. For multi-label experiments, we experiment on four datasets including CelebA, COCO, VOC and Stanford Car datasets. While the Stanford Car dataset is a multi-class classification dataset, we transform it into a multi-label dataset as detailed in Appendix A.3. For multi-class classification datasets, ImageNet, Kuzushiji-49 and Caltech256 are naturally unbalanced datasets, while CIFAR-10 with 2 classes, CIFAR-100 with 10 classes and SVHN with 2 classes are derived from the original dataset following Zhang et al. (2022). Specifically, we keep the first \(K-1\) classes from the original dataset and treat the rest of the images as a large out-of-distribution \(K\)-th class.

**Implementation Details.** We conduct experiments on varying batch sizes anywhere from \(B=500\) to \(B=10000\). To mirror a limited training budget (Citovsky et al., 2021; Emam et al., 2021), we allow \(10\) or \(20\) batches in total for each dataset, making it extra challenging for our adaptive algorithm selection due to the limited rounds of interaction.

Moreover, we assumed observations of \(y\) are sampled from stationary distributions \(\mathbb{P}_{\theta^{1}},...,\mathbb{P}_{\theta^{M}}\) in our analysis. However, these distributions could be dynamically changing. In our implementation, we use a simple trick to discount the past observations, where we change the posterior update in Algorithm 2 to \(a^{i}\leftarrow\gamma a^{i}+\sum_{j:\alpha^{i},j=i}y^{t,j}\) and \(b^{i}\leftarrow\gamma b^{i}+\sum_{j:\alpha^{i,j}=i}(1-y^{t,j})\). We set the discounting factor \(\gamma\) to be \(.9\) across all experiments. As will be discussed in Section 6.3, we find non-stationarity in

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & & & \multicolumn{2}{c}{Class Imb} & Binary Imb \\ Dataset & \(K\) & \(N\) & Ratio & Ratio \\ \hline CelebA (Liu et al., 2018) & 40 & 162770 &.0273 &.2257 \\ COCO (Lin et al., 2014) & 80 & 82081 &.0028 &.0367 \\ VOC (Everingham et al., 2010) & 20 & 10000 &.0749 &.0721 \\ CAR (Krause et al., 2013) & 10 & 12948 &.1572 &.1200 \\ ImageNet (Deng et al., 2009) & 1000 & 1281167 &.5631 & — \\ Kuzushiji-49 (Clanuuf et al., 2018) & 49 & 23236 &.0545 & — \\ Caltech256 (Griffin et al., 2007) & \(256\) & \(24486\) &.0761 & — \\ IMB CIFAR-10 (Krizhevsky et al., 2009) & \(2\) & \(50000\) &.1111 & — \\ IMB CIFAR-100 (Krizhevsky et al., 2009) & \(10\) & \(50000\) &.0110 & — \\ IMB SVHN (Netzer et al., 2011) & \(2\) & \(73257\) &.0724 & — \\ \hline \hline \end{tabular}
\end{table}
Table 1: Details for multi-label and multi-class classification datasets. \(K\) and \(N\) denote the number of classes and pool size respectively. _Class Imbalance Ratio_ represents the class imbalance ratio between the smallest and the largest class. We also report _Binary Imbalance Ratio_ for multi-label datasets, which is defined as the average positive ratio over classes, i.e., \(\frac{1}{K}\sum_{i\in[K]}(N_{i}/N)\) where \(N_{i}\) denotes the number of examples in class \(i\).

\(\{\mathbb{P}_{\theta^{k}}\}_{k=1}^{M}\) an interesting future direction to study. Lastly, we refer the readers to Appendix A for additional implementation details.

**Baseline Algorithms.** In our experiments, we choose a representative and popular subset of the deep AL algorithms and active search algorithms discussed in Section 2 as our baselines. To demonstrate the ability of TAILOR, number of candidate algorithms \(M\) ranges from tens to hundreds for different datasets. The baselines can be divided into three categories:

* We include off-the-shelf active learning and active search algorithms such as **EMAL**[Wu et al., 2014] and **Weak Supervision**[Ranganathan et al., 2018] for multi-label classification and **Confidence sampling**[Settles, 2009], **BADGE**[Ash et al., 2019], **Modified Submodular** optimization motivated by Kothawade et al. [2021] for multi-class classification. More implementation details can be found in Appendices A.1 and A.2.
* We derive individual candidate algorithms based on a per-class decomposition [Boutell et al., 2004]. For most likely positive sampling [Warmuth et al., 2001, 2003, Jiang et al., 2018], an active search strategy and abbreviated as **MLP**, we obtain \(K\) algorithms where the \(i\)-th algorithm selects examples most likely to be in the \(i\)-th class. For multi-label classification, we also include \(K\) individual **GALAXY** algorithms [Zhang et al., 2022] and \(K\)**Uncertainty sampling** algorithms. To further elaborate, the original **GALAXY** work by Zhang et al. [2022] construct \(K\) one-vs-rest linear graphs, one for each class. **GALAXY** requires finding the shortest shortest path among all \(K\) graphs, an operation whose computation scales linearly in \(K\). When \(K\) is large, this becomes computationally prohibitive to run. Therefore, we instead include \(K\) separate GALAXY algorithms, each only bisecting on one of the one-vs-rest graphs. This is equivalent with running \(K\)**GALAXY** algorithms, one for each binary classification task between class \(i\in[K]\) and the rest. For **Uncertainty sampling** in multi-label settings, we similarly have \(K\) individual uncertainty sampling algorithms, where the \(i\)-th algorithm samples the most uncertain example based only on the binary classification task of class \(i\). As baselines for each type of algorithms above, we simply interleave the set of \(K\) algorithms uniformly at random.
* We compare against other adaptive meta selection algorithms, including **Random Meta** which chooses candidate algorithms uniform at random and **ALBL Sampling**[Hsu and Lin, 2015]. The candidate algorithms include all of the active learning baselines. In Appendix C, we also provide an additional study of including active search baselines as candidate algorithms.

### Results

**Multi-class and Multi-label Classification.** For evaluation, we focus on TAILOR's comparisons against both existing meta algorithms and the best baseline respectively. In all classification experiments, TAILOR uses the class diversity reward in Section 4.1. For accuracy metrics, we utilize mean average precision for multi-label classification and balanced accuracy for multi-class classification. As a class diversity metric, we look at the size of the smallest class based on collected labels. All experiments are measured based on active annotation performance over the pool [Zhang et al., 2022].

Figure 3: Performance of TAILOR against baselines on selected settings. (a) and (b) shows accuracy metrics of the algorithms. (c) shows class-balancedness of labeled examples. All performances are averaged over four trials with standard error plotted for each algorithm. The curves are smoothed with a sliding window of size \(3\).

As shown in Figures 2, 3 and Appendix D, when comparing against existing meta algorithms, TAILOR performs better on all datasets in terms of both accuracy and class diversity metrics. **ALBL sampling** performs similar to **Random Meta** in all datasets, suggesting the ineffectiveness of training accuracy based rewards proposed in Hsu and Lin (2015) and Pang et al. (2018). When comparing against the best baseline algorithm, TAILOR performs on par with the best baseline algorithm on nine out of ten datasets in terms of accuracy and on all datasets in terms of class diversity. On the CelebA dataset, TAILOR even outperforms the best baseline by significant margin in accuracy. As discussed in Appendix E, TAILOR achieves this by selecting a _combination_ of other candidate algorithm instead of choosing only the best baseline. On four out of the ten datasets, TAILOR outperforms the best baseline in class diversity. Collectively, this shows the power of TAILOR in identifying the best candidate algorithms over different dataset scenarios. Moreover in Appendix D.4, we conduct an ablation study of the accuracy of the rarest class (determined by the ground truth class distribution). TAILOR significantly outperform baselines suggesting its advantage in improving the accuracy on _all_ classes. Lastly, shown in Appendix E, we also find TAILOR selects algorithms more aggressively than existing meta algorithms. The most frequent algorithms also align with the best baselines.

On the other hand for the Caltech256 dataset shown in Figure 16, TAILOR under-performs **confidence sampling** in terms of accuracy. We conjecture this is because the larger classes may not have sufficient examples and have much space for improvement before learning the smaller classes. Nevertheless, TAILOR was able to successfully collect a much more class-diverse dataset while staying competitive to other baseline methods.

**Multi-label Search.** We use the multi-label search reward proposed in Section 4.1. As shown in Figure 4 and Appendix D.3, on three of the four datasets, TAILOR performs better than the best baseline algorithm in terms of total collected positive labels. On the fourth dataset, TAILOR performs second to and on par with the best baseline. This shows TAILOR's ability in choosing the best candidate algorithms for active search.

### Future Work

While our experiments focus on class-imbalanced settings, TAILOR's effectiveness on balanced datasets warrants future study through further experiments and alternative reward design. We also find studying non-stationarity in label distributes \(\{\mathbb{P}_{\theta_{i}}\}_{i=1}^{M}\) an interesting next step.

## 7 Choosing Candidate Algorithms

Our paper proposes an adaptive selection procedure over candidate deep AL algorithms. When judging individual deep AL algorithms, current standards in the research community tend to focus on whether an algorithm performs well on _all_ dataset and application instances. However, we see value in AL algorithms that perform well only in certain instances. Consider, for example, an AL algorithm that performs well on 25% of previous applications, but poorly on the other 75%. One may wish to include this algorithm in TAILOR because the new application might be similar to those where it performs well. From the perspective of TAILOR, a "good" AL algorithm need not perform well on all or even most of a range of datasets, it just needs to perform well on a significant number of datasets.

On the other hand, as suggested by our regret bound that scales with \(M\), one should not include too many algorithms. In fact, there are exponential number of possible AL algorithms, which could easily surpass our labeling budget and overwhelm the meta selection algorithm. In practice, one could leverage extra information such as labeling budget, batch size and model architecture to choose proper set of candidate algorithms to target their settings.

Figure 4: Total positive labels for active search, CelebA

## Acknowledgement

We would like to thank Aude Hofleitner and Shawnndra Hill for the support of this research project, and Kevin Jamieson, Yifang Chen and Andrew Wagenmaker for insightful discussions. Robert Nowak would like to thank the support of NSF Award 2112471.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Aggarwal et al. (2020) Umang Aggarwal, Adrian Popescu, and Celine Hudelot. Active learning for imbalanced datasets. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1428-1437, 2020.
* Ash et al. (2021) Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active learning with fisher embeddings. _Advances in Neural Information Processing Systems_, 34:8927-8939, 2021.
* Ash et al. (2019) Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. _arXiv preprint arXiv:1906.03671_, 2019.
* Balcan et al. (2006) Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In _Proceedings of the 23rd international conference on Machine learning_, pages 65-72, 2006.
* Baram et al. (2004) Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. _Journal of Machine Learning Research_, 5(Mar):255-291, 2004.
* Beck et al. (2021) Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, and Rishabh Iyer. Effective evaluation of deep active learning on image classification tasks. _arXiv preprint arXiv:2106.15324_, 2021.
* Beluch et al. (2018) William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M Kohler. The power of ensembles for active learning in image classification. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9368-9377, 2018.
* Boutell et al. (2004) Matthew R Boutell, Jiebo Luo, Xipeng Shen, and Christopher M Brown. Learning multi-label scene classification. _Pattern recognition_, 37(9):1757-1771, 2004.
* Cai (2022) Xinmeng Cai. Active learning for imbalanced data: The difficulty and proportions of class matter. _Wireless Communications and Mobile Computing_, 2022, 2022.
* Citovsky et al. (2021) Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. Batch active learning at scale. _Advances in Neural Information Processing Systems_, 34:11933-11944, 2021.
* Clanuwat et al. (2018) Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. _arXiv preprint arXiv:1812.01718_, 2018.
* Coleman et al. (2022) Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I Zeki Yalniz. Similarity search for efficient active learning and search of rare concepts. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6402-6410, 2022.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Donmez et al. (2007) Pinar Donmez, Jaime G Carbonell, and Paul N Bennett. Dual strategy active learning. In _European Conference on Machine Learning_, pages 116-127. Springer, 2007.
* Ducoffe and Precioso (2018) Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based approach. _arXiv preprint arXiv:1802.09841_, 2018.
* Elenter et al. (2022) Juan Elenter, Navid NaderiAlizadeh, and Alejandro Ribeiro. A lagrangian duality approach to active learning. _arXiv preprint arXiv:2202.04108_, 2022.
* Emam et al. (2021) Zeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah Goldblum, and Tom Goldstein. Active learning at the imagenet scale. _arXiv preprint arXiv:2111.12880_, 2021.
* Elsasser et al. (2020)Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _International journal of computer vision_, 88(2):303-338, 2010.
* Gal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In _International Conference on Machine Learning_, pages 1183-1192. PMLR, 2017.
* Geifman and El-Yaniv (2017) Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. _arXiv preprint arXiv:1711.00941_, 2017.
* Gissin and Shalev-Shwartz (2019) Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. _arXiv preprint arXiv:1907.06347_, 2019.
* Gonsior et al. (2021) Julius Gonsior, Maik Thiele, and Wolfgang Lehner. Imital: Learning active learning strategies from synthetic data. _arXiv preprint arXiv:2108.07670_, 2021.
* Griffin et al. (2007) Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hsu and Lin (2015) Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In _Twenty-Ninth AAAI conference on artificial intelligence_, 2015.
* Jiang et al. (2018) Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, and Roman Garnett. Efficient nonmyopic batch active search. In _32nd Conference on Neural Information Processing Systems (NeurIPS 2018)_, 2018.
* Jin et al. (2022) Qiuye Jin, Mingzhi Yuan, Haoran Wang, Manning Wang, and Zhijian Song. Deep active learning models for imbalanced image classification. _Knowledge-Based Systems_, 257:109817, 2022.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Konyushkova et al. (2017) Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. _Advances in neural information processing systems_, 30, 2017.
* Kothawade et al. (2021) Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: Submodular information measures based active learning in realistic scenarios. _Advances in Neural Information Processing Systems_, 34:18685-18697, 2021.
* Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* Kremer et al. (2014) Jan Kremer, Kim Steenstrup Pedersen, and Christian Igel. Active learning with support vector machines. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 4(4):313-326, 2014.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.
* Liu et al. (2018) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. _Retrieved August_, 15(2018):11, 2018.
* Loffler and Mutschler (2022) Christoffer Loffler and Christopher Mutschler. Iale: Imitating active learner ensembles. _Journal of Machine Learning Research_, 23(107):1-29, 2022.
* Liu et al. (2018)Xue-Yang Min, Kun Qian, Ben-Wen Zhang, Guojie Song, and Fan Min. Multi-label active learning through serial-parallel neural networks. _Knowledge-Based Systems_, 251:109226, 2022.
* Mohamadi et al. (2022) Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. Making look-ahead active learning strategies feasible with neural tangent kernels. _arXiv preprint arXiv:2206.12569_, 2022.
* Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* Pang et al. (2018) Kunkun Pang, Mingzhi Dong, Yang Wu, and Timothy M Hospedales. Dynamic ensemble active learning: A non-stationary bandit with expert advice. In _2018 24th International Conference on Pattern Recognition (ICPR)_, pages 2269-2276. IEEE, 2018.
* Ranganathan et al. (2018) Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok Chakraborty, and Sethuraman Panchanathan. Multi-label deep active learning with label correlation. In _2018 25th IEEE International Conference on Image Processing (ICIP)_, pages 3418-3422. IEEE, 2018.
* Russo and Van Roy (2014) Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. _Mathematics of Operations Research_, 39(4):1221-1243, 2014.
* Sener and Savarese (2017) Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. _arXiv preprint arXiv:1708.00489_, 2017.
* Settles (2009) Burr Settles. Active learning literature survey. 2009.
* Shao et al. (2019) Jingyu Shao, Qing Wang, and Fangbing Liu. Learning to sample: an active learning framework. In _2019 IEEE International Conference on Data Mining (ICDM)_, pages 538-547. IEEE, 2019.
* Thompson (1933) William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* Tong and Koller (2001) Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. _Journal of machine learning research_, 2(Nov):45-66, 2001.
* Wang et al. (2021) Haonan Wang, Wei Huang, Andrew Margenot, Hanghang Tong, and Jingrui He. Deep active learning by leveraging training dynamics. _arXiv preprint arXiv:2110.08611_, 2021.
* Warmuth et al. (2001) Manfred K Warmuth, Gunnar Ratsch, Michael Mathieson, Jun Liao, and Christian Lemmen. Active learning in the drug discovery process. In _NIPS_, pages 1449-1456, 2001.
* Warmuth et al. (2003) Manfred K Warmuth, Jun Liao, Gunnar Ratsch, Michael Mathieson, Santosh Putta, and Christian Lemmen. Active learning with support vector machines in the drug discovery process. _Journal of chemical information and computer sciences_, 43(2):667-673, 2003.
* Wu et al. (2014) Jian Wu, Victor S Sheng, Jing Zhang, Pengpeng Zhao, and Zhiming Cui. Multi-label active learning for image classification. In _2014 IEEE international conference on image processing (ICIP)_, pages 5227-5231. IEEE, 2014.
* Wu et al. (2020) Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova, Christine Leon Swisher, Zhiming Cui, and Pengpeng Zhao. Multi-label active learning algorithms for image classification: Overview and future promise. _ACM Computing Surveys (CSUR)_, 53(2):1-35, 2020.
* Zhan et al. (2022) Xueying Zhan, Qingzhong Wang, Kuan-hao Huang, Haoyi Xiong, Dejing Dou, and Antoni B Chan. A comparative survey of deep active learning. _arXiv preprint arXiv:2203.13450_, 2022.
* Zhang et al. (2020) Jifan Zhang, Lalit Jain, and Kevin Jamieson. Learning to actively learn: A robust approach. _arXiv preprint arXiv:2010.15382_, 2020.
* Zhang et al. (2022) Jifan Zhang, Julian Katz-Samuels, and Robert Nowak. Galaxy: Graph-based active learning at the extreme. _arXiv preprint arXiv:2202.01402_, 2022.

Implementation Details

### Deep Active Learning Decomposition

For any uncertainty sampling algorithm, picking the top-\(B\) most uncertain examples can be easily decomposed into an iterative procedure that picks the next most uncertain example. Next, for diversity based deep active learning algorithms, one usually rely on a greedy iterative procedure to collect a batch, e.g. K-means++ for BADGE (Ash et al., 2019) and greedy K-centers for Coreset (Sener and Savarese, 2017). Lastly, deep active learning algorithms such as Cluster-Margin (Citovsky et al., 2021) and GALAXY (Zhang et al., 2022) have already proposed their algorithms as iterative procedures that select unlabeled examples sequentially.

### Implementation of Modified Submodular

Instead of requiring access to a balanced holdout set (Kothawade et al., 2021), we construct the balanced set using training examples. We use the Submodular Mutual Information function FLQMI as suggested by Table 1 of Kothawade et al. (2021). The proposed greedy submodular optimization is itself an iterative procedure that selects one example at a time. While SIMILAR usually performs well, our modification that discards the holdout set is unfortunately ineffective in our experiments. This is primarily due to the lack of the holdout examples, which may often happen in practical scenarios.

### Stanford Car Multi-label Dataset

We transform the original labels into \(10\) binary classes of

1. If the brand is "Audi".
2. If the brand is "BMW".
3. If the brand is "Chevrolet".
4. If the brand is "Dodge".
5. If the brand is "Ford".
6. If the car type is "Convertible".
7. If the car type is "Coupe".
8. If the car type is "SUV".
9. If the car type is "Van".
10. If the car is made in or before 2009.

### Negative Weighting for Common Classes

For multi-label classifications, for some classes, there could be more positive associations (label of \(1\)s) than negative associations (label of \(0\)s). Therefore, in those classes, the rarer labels are negative. In class diverse reward \(\langle v_{div}^{t},y\rangle\) in Section 4.1, we implement an additional weighting of \(\mathbbm{1}_{rare}^{t}*v_{div}\), where \(*\) denotes an elementwise multiplication. Here, each element \(\mathbbm{1}_{rare,i}^{t}\in\{1,-1\}\) takes value \(-1\) when \(\text{COUNT}^{t}(i)\) is larger than half the size of labeled set. This negative weighting can been seen as upsampling negative class associations when positive associations are the majority.

### Model Training

All of our experiments are conducted using the ResNet-18 architecture (He et al., 2016) pretrained on ImageNet. We use the Adam optimizer (Kingma and Ba, 2014) with learning rate of 1e-4 and weight decay of 5e-5.

## Appendix B Proof of Theorem 5.2

Our proof follows a similar procedure from regret analysis for Thompson Sampling of the stochastic multi-armed bandit problem (Lattimore and Szepesvari, 2020). Let \(\alpha^{t}:=\{\alpha^{t,j}\}_{j=1}^{B}\) and \(\{y^{t,j}\}_{j=1}^{B}\) denote the actions and observations from the \(i\)-th round. We define the history up to \(t\) as \(H_{t}=\{\alpha^{1},y^{1},\alpha^{2},y^{2},...,\alpha^{t-1},y^{t-1}\}\). Moreover, for each \(i\in[M]\), we define \(H_{t,i}=\{y^{t^{\prime},j}\in H_{t}:\alpha^{t^{\prime},j}=i\}\) as the history of all observations made by choosing the \(i\)-th arm (algorithm).

Now we analyze reward estimates at each round \(t\). When given history \(H_{t}\) and arm \(i\in[M]\), each observation \(y\in H_{t,i}\) is an unbiased estimate of \(\theta^{i}\) as \(y\sim\mathbb{P}_{\theta^{i}}\). Therefore, for any fixed \(v^{t}\), \(\langle v^{t},y\rangle\) is an unbiased estimate of the expected reward \(\langle v^{t},\theta^{i}\rangle\), which we denote by \(\mu^{t,i}\).

For each arm \(i\), we can then obtain empirical reward estimate \(\bar{\mu}^{t,i}\) of the true expected reward \(\mu^{t,i}\) by \(\bar{\mu}^{t,i}:=\frac{1}{1\lor|H_{t,i}|}\sum_{y\in H_{t,i}}\langle v^{t},y\rangle\) where \(\bar{\mu}^{t,i}=0\) if \(|H_{t,i}|=0\). Since expected rewards and reward estimates are bounded by \([-1,1]\), by standard sub-Gaussian tail bounds, we can then construct confidence interval,

\[\mathbb{P}\left(\forall i\in[M],t\in[T],|\bar{\mu}^{t,i}-\mu^{t,i}|\leq d^{t,i }\right)\geq 1-\frac{1}{T}\]

where \(d^{t,i}:=\sqrt{\frac{8\log(MT^{2})}{1\lor|H_{t,i}|}}\). Additionally, we define upper confidence bound as \(U^{t,i}=\text{clip}_{[-1,1]}\left(\bar{\mu}^{t,i}+d^{t,i}\right)\).

At each iteration \(t\), we have the posterior distribution \(\mathbb{P}(\Theta=\cdot|H_{t})\) of the ground truth \(\Theta=\{\theta^{i}\}_{i=1}^{M}\). \(\widehat{\Theta}=\{\widehat{\theta}^{i}\}_{i=1}^{M}\) is sampled from this posterior. Consider \(i^{t}_{\star}=\arg\max_{i\in M}\langle v^{t},\theta^{i}\rangle\) and \(\alpha^{t,j}=\arg\max_{i\in M}\langle v^{t},\widehat{\theta}^{i}\rangle\). The distribution of \(i^{t}_{\star}\) is determined by the posterior \(\mathbb{P}(\Theta=\cdot|H_{t})\). The distribution of \(\alpha^{t,j}\) is determined by the distribution of \(\widehat{\Theta}\), which is also \(\mathbb{P}(\Theta=\cdot|H_{t})\). Therefore, \(i^{t}_{\star}\) and \(\alpha^{t,j}\) are identically distributed. Furthermore, since the upper confidence bounds are deterministic functions of \(i\) when given \(H_{t}\), we then have \(\mathbb{E}[U^{t,\alpha^{t,j}}|H_{t}]=\mathbb{E}[U^{t,i^{t}_{\star}}|H_{t}]\).

As a result, we upper bound the Bayesian regret by

\[BR(\texttt{TAILOR})=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B }\mu^{t,i^{t}_{\star}}-\mu^{t,\alpha^{t,j}}\right]\] \[= \mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}(\mu^{t,i^{t}_{\star }}-U^{t,i^{t}_{\star}})+(U^{t,\alpha^{t,j}}-\mu^{t,\alpha^{t,j}})\right].\]

Now, note that since \(\bar{\mu}^{t,i}\in[-1,1]\) we have \(\text{clip}_{[-1,1]}\left(\bar{\mu}^{t,i}+d^{t,i}\right)=\text{clip}_{[-\infty,1]}\left(\bar{\mu}^{t,i}+d^{t,i}\right)\), where only the upper clip takes effect. Based on the sub-Gaussian confidence intervals \(\mathbb{P}\left(\forall i\in[M],t\in[T],|\bar{\mu}^{t,i}-\mu^{t,i}|\leq d^{t,i }\right)\geq 1-\frac{1}{T}\), we can derive the following two confidence bounds:

\[\mathbb{P}(\forall i\in[M],t\in[T],\mu^{t,i}>U^{t,i}) =\mathbb{P}(\forall i\in[M],t\in[T],\mu^{t,i}>\text{clip}_{[-1,1 ]}(\bar{\mu}^{t,i}+d^{t,i}))\] \[=\mathbb{P}(\forall i\in[M],t\in[T],\mu^{t,i}>\bar{\mu}^{t,i}+d^{ t,i})\:,\:\text{since}\:\mu^{t,i}\leq 1\] \[=\mathbb{P}(\forall i\in[M],t\in[T],\mu^{t,i}-\mu^{t,i}>d^{t,i}) \leq\frac{1}{2T}\] \[\mathbb{P}(\forall i\in[M],t\in[T],U^{t,i}-\mu^{t,i}>2d^{t,i}) =\mathbb{P}(\forall i\in[M],t\in[T],\text{clip}_{[-1,1]}(\bar{\mu} ^{t,i}+d^{t,i})-\mu^{t,i}>2d^{t,i})\] \[\leq\mathbb{P}(\forall i\in[M],t\in[T],\bar{\mu}^{t,i}+d^{t,i}-\mu ^{t,i}>2d^{t,i})\] \[=\mathbb{P}(\forall i\in[M],t\in[T],\bar{\mu}^{t,i}-\mu^{t,i}>d^{ t,i})\leq\frac{1}{2T}.\]

Now with the decomposition,

\[BR(\texttt{TAILOR})=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B} \mu^{t,i^{t}_{\star}}-\mu^{t,\alpha^{t,j}}\right]\] \[= \mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}\mu^{t,i^{t}_{\star}} -U^{t,i^{t}_{\star}}\right]+\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}U^{t, \alpha^{t,j}}-\mu^{t,\alpha^{t,j}}\right]\]we can bound the two expectations individually.

First, to bound \(\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}\mu^{t,i_{*}^{t}}-U^{t,i_{*}^{t}}\right]\), we note that \(\mu^{t,i_{*}^{t}}-U^{t,i_{*}^{t}}\) is negative with high probability. Also, the maximum value this can take is bounded by \(2\) as \(\mu^{t,i},U^{t,i}\in[-1,1]\). Therefore, we have

\[\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}\mu^{t,i_{*}^{t}}-U^{t,i_{*}^{t}} \right]\leq\left(\sum_{t=1}^{T}\sum_{j=1}^{B}0\cdot\mathbb{P}(\mu^{t,i_{*}^{t} }<=U^{t,i_{*}^{t}})+2\cdot\mathbb{P}(\mu^{t,i_{*}^{t}}>U^{t,i_{*}^{t}})\right) \leq 2TB\cdot\frac{1}{2T}=B.\]

Next, to bound \(\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}U^{t,\alpha^{t,j}}-\mu^{t,\alpha^ {t,j}}\right]\) we decompose it similar to the above:

\[\mathbb{E}\left[\sum_{t=1}^{T}\sum_{j=1}^{B}U^{t,\alpha^{t,j}}- \mu^{t,\alpha^{t,j}}\right] \leq\left(\sum_{t=1}^{T}\sum_{j=1}^{B}2\mathbb{P}(U^{t,\alpha^{t,j}}-\mu^{t,\alpha^{t,j}}>2d^{t,i})\right)+\left(\sum_{t=1}^{T}\sum_{j=1}^{B}2 d^{t,i}\right)\] \[\leq B+\left(\sum_{t=1}^{T}\sum_{j=1}^{B}\sqrt{\frac{32\log(MT^{2 })}{1\lor|H_{t,\alpha^{t,j}}|}}\right)\]

where recall that \(|H_{t,i}|\) is the number of samples collected using algorithm \(i\) in rounds \(\leq t\).

To bound the summation, we utilize the fact that \(\frac{1}{1\lor|H_{t,i}|}\leq\frac{B}{k}\) for each \(k\in[|H_{t,i}|,|H_{t+1,i}|]\), since \(|H_{t+1,i}|-|H_{t,i}|\leq B\). As a result, we get

\[\sum_{t=1}^{T}\sum_{j=1}^{B}\sqrt{\frac{32\log(MT^{2})}{1\lor|H_{ t,\alpha^{t,j}}|}}\] \[\leq\sum_{t=1}^{T}\sum_{i=1}^{M}\sum_{k=1}^{|H_{T,i}|}\sqrt{\frac {32\log(MT^{2})\cdot B}{k}}\] \[\leq O(\sqrt{B(\log T+\log M)}\sum_{i=1}^{M}\sqrt{|H_{T,i}|})\] \[\leq O(\sqrt{B(\log T+\log M)})\cdot O(\sqrt{BMT})=O(B\sqrt{MT( \log T+\log M)})\]

where last two inequalities follow from simple algebra and the fact that \(\sum_{i=1}^{M}|H_{T,i}|=TB\).

Finally, to combine all of the bounds above, we get \(BR(\texttt{TAILOR})\leq B+B+O(B\sqrt{MT(\log T+\log M)})=O(B\sqrt{MT(\log T+ \log M)})\).

[MISSING_PAGE_FAIL:18]

Full Results

All of the results below are averaged from four individual trials except for Imagenet, which is the result of a single trial.

### Multi-label Classification

Figure 7: CelebA

Figure 8: COCO

Figure 9: VOC

Figure 10: Stanford Car

[MISSING_PAGE_EMPTY:23]

Figure 12: CIFAR-100, 10 classes

Figure 13: SVHN, 2 classes

Figure 14: Kuzushiji-49

Figure 16: Caltech256

Figure 15: Caltech256

[MISSING_PAGE_EMPTY:28]

Figure 21: CelebA, Rarest Class Accuracy

Figure 20: Stanford Car, Total Number of Positive Labels

What Algorithms Does Tailor Choose?

In the following two figures, we can see TAILOR chooses a non-uniform set of algorithms to focus on for each dataset. On CelebA, TAILOR out-perform the best baseline, EMAL sampling, by a significant margin. As we can see, TAILOR rely on selecting a _combination_ of other candidate algorithms instead of only selecting EMAL.

On the other hand, for the Stanford car dataset, we see TAILOR's selection mostly align with the baselines that perform well especially in the later phase.

In the following figures, we plot the number of times the most frequent candidate algorithm is chosen. As can be shown, TAILOR chooses candidate algorithm much more aggressively than other meta algorithms in eight out of the ten settings.

Figure 23: TAILOR Top-10 Most Selected Candidate Algorithms on Stanford Car Dataset

Figure 22: TAILOR Top-10 Most Selected Candidate Algorithms on CelebA Dataset

Figure 26: SVHN, 2 Classes, Number of Pulls of The Most Frequent Selection

Figure 24: CIFAR-10, 2 Classes, Number of Pulls of The Most Frequent Selection

Figure 25: CIFAR-100, 10 Classes, Number of Pulls of The Most Frequent Selection

Figure 28: Kuzushiji-49, Number of Pulls of The Most Frequent Selection

Figure 27: ImageNet-1k, Number of Pulls of The Most Frequent Selection

Figure 29: Caltech256, Number of Pulls of The Most Frequent Selection

Figure 31: COCO, Number of Pulls of The Most Frequent Selection

Figure 32: VOC, Number of Pulls of The Most Frequent Selection

Figure 30: CelebA, Number of Pulls of The Most Frequent Selection

Figure 33: Stanford Car, Number of Pulls of The Most Frequent Selection