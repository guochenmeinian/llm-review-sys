# DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology

 Marco Aversa

University of Glasgow and Dotphoton

Glasgow, United Kingdom

marco.aversa@glasgow.ac.uk

&Gabriel Nobis

Fraunhofer HHI

Berlin, Germany

gabriel.nobis@hhi.fraunhofer.de

&Miriam Hagele

Aignostics

Berlin, Germany

miriam.haegele@aignostics.com

&Kai Standvoss

Aignostics

Berlin, Germany

kai.standvoss@aignostics.com

&Mihaela Chirica

Institute of Pathology, LMU Munich

Munich, Germany

mihaela.chirica@med.uni-muenchen.de

&Roderick Murray-Smith

University of Glasgow

Glasgow, United Kingdom

roderick.murray-smith@glasgow.ac.uk

&Ahmed Alaa

UC Berkeley

Berkeley, California

amalaa@berkeley.edu

&Lukas Ruff

Aignostics

bErlin, Germany

lukas.ruff@aignostics.com

&Daniela Ivanova

University of Glasgow

Glasgow, United Kingdom

daniela.ivanova@glasgow.ac.uk

&Wojciech Samek

Fraunhofer HHI and TU Berlin

Berlin, Germany

wojciech.samek@hhi.fraunhofer.de

&Frederick Klauschen

Institute of Pathology, LMU Munich

Munich, Germany

f.klauschen@lmu.de

&Bruno Sanguinetti

Dotphoton

Zug, Switzerland

bruno.sanguinetti@dotphoton.com

&Luis Oala

Dotphoton

Zug, Switzerland

luis.oala@dotphoton.com

###### Abstract

We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiringsmall patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffIInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data.

## 1 Introduction

Deep learning (DL) models are promising auxiliary tools for medical diagnosis [1; 2; 3]. Applications like segmentation and classification have been refined and pushed to the limit on natural images [4]. However, these models trained on rich datasets still have limited applications in medical data. While segmentation models rely on sharp object contours when applied to natural data, in medical imaging, the model struggles to detect a specific feature because it has a "limited ability to handle objects with missed boundaries" and often "miss tiny and low-contrast objects" [5; 6]. Therefore, task-specific medical applications require their own specialised and fine-grained annotation. Data labelling is arguably one of the most critical bottlenecks in healthcare machine learning (ML) applications. In histopathology, pathologists examine the histological slide at multiple levels, usually starting with a lower magnification to analyse the tissue architecture and cellular arrangement and gradually proceeding to a higher magnification to examine cell morphology and subcellular features, such as the appearance and number of nucleoli, chromatin density and cytoplasm appearance. Annotating features within gigapixel whole slide images (WSIs) with this level of detail demands effort and time, often leading to sparse, limited annotated data. In addition, due to privacy regulations and ethics [7; 8], having access to medical data can be challenging since it has been shown that it is possible to extract patients' sensitive information [9] from this data.

In histopathology, state-of-the-art ML models require the context of the entire WSIs, with features at different scales, in order to distinguish between different tumor sub-types, grades and stages [10]. Despite the demonstrated effectiveness of diffusion models (DMs) in generating natural images compared to other approaches, they still have rarely been applied in medical imaging. Existing

Figure 1: a) Examples of synthetic and real \(2048\times 2048\) images. b) Pairs of \(512\times 512\) synthetic tiles (top) with the closest real images found with Inception-v3 near-neighbour (bottom).

generative models in histopathology can generate images of relatively small resolution compared to WSIs. To give a few examples, the application of Generative Adversarial Networks (GANs) in cervical dysplasia detection [11], glioma classification [12], and generating images of breast and colorectal cancer [13], generate images with \(256\times 128\) px, \(384\times 384\) px and \(224\times 224\) px, respectively. In spite of their current limitations in generating images at scales necessary to fully address all medical concerns, the use of synthetic data in medical imaging can provide a valuable solution to the persistent issue of data scarcity [14; 15; 16; 17]. Models generally improve after data augmentation and synthetic images are equally informative as real images when added to the training set [18; 19]. Data augmentation could also help with the underrepresentation in data sets of rare cancer subtypes. By adding synthetic images to the training set, Chen et al. [20] demonstrated that their model had better accuracy in detecting chromophobe renal cell carcinoma, which is a rare subtype of renal cell carcinoma. Furthermore, Doleful et al. [21] showed how synthetic histological images could be used for educational purposes for pathology residents. Regarding the challenges highlighted before, we present a novel sampling method to generate large histological images with long-range pixel correlation (see Fig. 1), aiming to extend up to the resolution of the WSI.

Our contributions are as follows: 1) We introduce DiffInfinite, a hierarchical generative framework that generates arbitrarily large images, paired with their segmentation masks. 2) We introduce a fast outpainting method that can be efficiently parallelized. 3) The quality of DiffInfinite data is evaluated by ten experienced pathologists as well as downstream machine learnings tasks (classification and segmentation) and anti-duplication metrics to assess the leakage of patient data from the training set.

## 2 Related Work

Large-content image generation can be reduced to inpainting/outpainting tasks. Image inpainting is the problem of reconstructing unknown or unwanted areas within an image. A closely related task is image outpainting, which aims to predict visual content beyond the boundaries of an image. In both cases, the newly in- or outpainted image regions have to be visually indistinguishable with respect to the rest of the image. Such image completion approaches can help utilise models trained on smaller patches for the purpose of generating large images, by initially generating the first patch, followed by its extension outward in the desired direction.

Traditional approachesTraditional methods for image region completion rely on repurposing known image features, necessitating costly nearest neighbour searches for suitable pixels or patches [22; 23; 24; 25; 26]. Such methods often falter with complex or large regions [24]. In contrast, DL enables novel, realistic image synthesis for inpainting and outpainting. Some methods like Deep Image Prior [27] condition new image areas on the existing image, while others aim to learn natural image priors for realistic generation [28; 29].

Generative modelling for conditional image synthesisGANs have dominated image-to-image translation tasks like inpainting and outpainting for years [28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42]. Recently, DMs have surpassed GANs in various image generation tasks [43]. Palette [44] was the first to apply DMs to tasks like inpainting and outpainting. RePaint [45] and ControlNet [46] demonstrate resampling and masking techniques for conditioning using a pre-trained diffusion model. SinDiffusion [47] and DiffCollage [48] offer state-of-the-art outpainting solutions using DMs trained with overlapping patches. In parallel to our work, Bond-Taylor and Willcocks [49] developed a related approach called \(\infty\)-Diff which trains on random coordinates, allowing the generation of infinite-resolution images during sampling. However, in contrast to our approach the method does not involve image compression in a latent space.

Synthetic data assessmentThe authenticity of synthetic data produced by DMs, trained on vast paired labelled datasets [50], remains contentious. Ethical implications necessitate distinguishing if generated images are replicas of training data [51; 52]. The task is complicated due to subjective visual similarities and diverse dataset ambiguities. Various metrics have been proposed for quantifying data replication, including information theory distances from real data [53], consistency measurements using downstream models [54; 55], comparison with inpainted areas [52], and detection of "forgotten" examples [56].

## 3 Preliminaries

Diffusion ModelsDMs [57; 58; 59] represent a class of parameterized Markov chains that effectively optimize the lower variational bound associated with the likelihood function of the unknown data distribution. By iteratively adding small amounts of noise until the image signal is destroyed and then learning to reverse this process, DMs can approximate complex distributions much more faithfully than GANs [60]. The increased diversity of samples while preserving sample fidelity comes at the cost of training and sampling speed, with DMs being much slower than GANs [43]. The universally adopted solution to this problem is to encode the images from pixel space into a lower dimensional latent space via a Vector Quantised-Variational AutoEncoder (VQ-VAE), and perform the diffusion process over the latents, before decoding back to pixel space [61]. Pairing this with the Denoising Diffusion Implicit Models (DDIMs) sampling method [62] leads to faster sampling while preserving the DM objective

\[z_{t-1}=\sqrt{\alpha_{t-1}}\left(\frac{z_{t}-\sqrt{1-\alpha_{t}}\epsilon_{ \theta}(z_{t},t)}{\sqrt{\alpha_{t}}}\right)+\sqrt{1-\alpha_{t-1}-\sigma_{t}^{ 2}}\epsilon_{\theta}(z_{t},t)+\sigma_{t}\epsilon_{t},\] (1)

where \(z_{t}\) is the latent variable at time step \(t\) in the VQ-VAE latent space, \(\alpha_{t}\) is the noise scheduler, \(\epsilon_{\theta}\) is the noise learned by the model and \(\epsilon_{t}\) is random noise. Conditioning can be achieved either by specifically feeding the condition with the noised data [44; 63], by guiding an unconditional model using an external classifier [64; 65] or by classifier-free guidance [66] used in this work, where the

Figure 2: DiffImfinite generation method. a) Large-scale context mask generation. A diffusion model conditioned on a large-scale conditional prompt (e.g. Adenocarcinoma subtype) generates a low-resolution mask. The mask is upsampled via linear interpolation to the desired image size. b) Diffusion steps on large images. Given a random position, we select a sub-tile with its segmentation mask. A diffusion model generates in parallel the next step conditioned on each conditional label, or prompt, found in the mask. The outputs are masked individually with the corresponding label. The next step is the union of all the sub-patches. c) Tracking time steps pixel-wise. We keep track of the time step of each pixel in the large image. The model evolves only the pixels with the higher time step on each iteration.

convex combination

\[\tilde{\epsilon}_{\theta}(z_{t},c)=(1+\omega)\epsilon_{\theta}(z_{t},c)-\omega \epsilon_{\theta}(z_{t},\varnothing),\] (2)

of a conditional diffusion model \(\epsilon_{\theta}(z_{t},c)\) and an unconditional model \(\epsilon_{\theta}(z_{t},\varnothing)\) is used for noise estimation. The parameter \(\omega\) controls the tradeoff between conditioning and diversity, since \(\omega>0\) introduces more diversity in the generated data by considering the unconditional model while \(\omega=0\) uses only the conditional model.

## 4 Infinite Diffusion

The DiffInfinite approach we present here1, is a generative algorithm to generate arbitrarily large images without imposing conditional independence, allowing for long-range correlation structural information. The method overcomes this limitation of DMs for large-content generation by deploying multiple realizations of a DM on smaller patches. In this section, we first define a mathematical description of this hierarchical generation model and then describe the sampling method paired with a masked conditioned generation process.

Footnote 1: Code available at https://github.com/marcoversa/diffinfinite

### The Method

Let \(X\sim\mathcal{X}\) be a large-content generating random variable taking values in \(\mathbb{R}^{KD}\). Using the approach of latent diffusion models [61], the high-dimensional content is first mapped to the latent space \(\mathbb{R}^{D}\) by \(\Phi(X)=Y\sim\mathcal{Y}_{\Phi}\). For simplicity, we assume throughout this work the existence of an ideal encoder-decoder pair \((\Phi,\Psi)\) such that \(\Psi(\Phi(X))=X\) is the identity on \(\mathbb{R}^{KD}\). Assume further, to have a reverse time model \((SM_{\theta},\epsilon_{\theta})\) at hand consisting of a sampling method \(SM_{\theta}\) and a learned model \(\epsilon_{\theta}\) trained on small patches \(Z\sim\mathcal{Z}_{\Phi}\) taking values in \(\mathbb{R}^{d}\). The reverse time model transforms \(z_{T}\sim\mathcal{N}(0,I_{d})\) over the time steps \(t\in\{T,T-1,...,1\}\) recursively by

\[z_{t-1}=SM_{\theta}(z_{t})\] (3)

to an approximate instance of \(\mathcal{Z}_{\Phi}\). We aim to sample instances from \(\mathcal{Y}_{\Phi}\) by deploying multiple realizations of the reverse time model \((SM_{\theta},\epsilon_{\theta})\). Towards that goal, define the set of projections

\[\mathcal{C}:=\{proj_{I}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}\ |\ I\subset \mathbb{N}\ \textit{correspond to }d\ \textit{indices of connected pixels in }\mathbb{R}^{D}\},\] (4)

where \(proj\in\mathcal{C}\) models a crop \(proj(Y)\in\mathbb{R}^{d}\) of \(d\) connected pixels from the latent image \(Y\). Since the model \(\epsilon_{\theta}\) is trained on images taking values in \(\mathbb{R}^{d}\) the standing assumption is

**Assumption 1**: _Any projection \(proj\in\mathcal{C}\) maps \(Y\) to the same distribution \(proj(Y)\sim\mathcal{Z}_{\Phi}\) in \(\mathbb{R}^{d}\)._

Since the goal is to approximate an instance of \(\mathcal{Y}_{\Phi}\), we initialize the sampling method by \(y_{T}\sim\mathcal{N}(0,I_{D})\) and proceed in the following way: Given \(y_{t}\), randomly choose \(proj_{I_{1}},...,proj_{I_{m}}\in\mathcal{C}\) independent of the state \(y_{t}\) such that \(proj_{I_{1}},...,proj_{I_{m}}\) are non equal crops that cover all latent pixels in \(\mathbb{R}^{D}\). To be more precise, for every \(i\in\{1,...,D\}\) we find at least one \(j\in\{1,...,m\}\) with \(i\in I_{j}\). For every projection \(proj_{I_{1}},...,proj_{I_{m}}\) we calculate the crop \(z_{t}^{j}=proj_{I_{j}}(y_{t})\) of the current state \(y_{t}\) and perform one step of the reverse time model following the sampling scheme

\[z_{t-1}^{j}=SM_{\theta}(z_{t}^{j}),\quad j\in\{1,...,m\}.\] (5)

This results in overlapping estimates \(z_{t-1}^{1},...,z_{t-1}^{m}\) of the subsequent state \(t-1\) and we simply assign to every pixel in the latent space the first value computed for this pixel such that

\[[y_{t-1}]_{i}=[z_{t-1}^{j}]_{l},\quad\textit{where }j=\min\big{\{}j^{\prime}\ |\ i\in I_{j^{\prime}}\big{\}}\] (6)

and \(l\) refers to the entry in \(z_{t-1}^{j}\) corresponding to \(i\) with \([proj_{I_{j}}(y_{t-1})]_{l}=[y_{t-1}]_{i}\). Hence, starting from \(y_{T}\sim\mathcal{N}(0,I_{D})\) we sample in the first step from a distribution

\[y_{T-1}\sim p_{T-1,\theta}(y\,|\,y_{T},proj_{I_{1}},...,proj_{I_{m}}).\] (7)

Using Bayes' theorem, this distribution simplifies to

\[p_{T-1,\theta}(y\,|\,y_{T},proj_{I_{1}},...,proj_{I_{m}})=p_{T-1,\theta}(y\,| \,y_{T}),\] (8)since we sample the projections independently from \(y_{T}\). Repeating the argument, we sample in every step from a distribution \(y_{t-1}\sim p_{t-1,\theta}(y|y_{t},...,y_{T})\) over \(\mathbb{R}^{D}\) instead of sampling from \(z_{t-1}\sim q_{t-1,\theta}(z|z_{t},...,z_{T})\) over \(\mathbb{R}^{d}\). Hence, we approximate the true latent distribution \(\mathcal{Y}_{\Phi}\) by the approximate distribution with density \(p_{0,\theta}(y|y_{1},...,y_{T})\). In contrast to [48], our method does not use the assumption of conditional independence and the method can be applied to a wide range of DMs, without an adjustment of the training method. As the authors of [48] point out in their section on limitations, the assumption of conditional independence is not well-suited in cases of a data distribution with long-range dependence. For image generation in the medical context, we aim to circumvent this assumption as we do not want to claim that the density of a given region depends only on one neighboring region. The drawback of dropping the assumption is that we only approximate the reverse time model of the latent image distribution \(\mathcal{Y}_{\Phi}\) indirectly, by multiple realizations of a reverse time model that approximates \(\mathcal{Z}_{\Phi}\).

### Semi-supervised Guidance

In order to generate diverse high-fidelity data, DMs require lots of training data. Perhaps, training on a few samples still extracts significant features but it lacks variability, resulting in simple replicas. Here, we show how to enhance synthetic data diversity using classifier-free guidance as a semi-supervised learning method. In the classifier-free guidance [66], a single model is trained conditionally and unconditionally on the same dataset. We adapt the training scheme using two separate datasets. The model is guided by a small and sparsely annotated dataset \(q_{1}\), used for the conditional training step, while extracts features by the large unlabelled dataset \(q_{0}\), used on the unconditional training step (see Alg.1)

\[(z_{0},c)=\begin{cases}(z_{0},\varnothing)\sim q_{0}(z_{0})&\text{if }u\geqslant p_{ unc}\\ (z_{0},c)\sim q_{1}(z_{0},c)&\text{otherwise}\end{cases},\] (9)

where \(u\) is sampled from a uniform distribution in [0,1], \(p_{unc}\) is the probability of switching from the conditional to the unconditional setting and \(\varnothing\) is a null label. During the sampling, a tradeoff between conditioning and diversity is controlled via the parameter \(\omega\) in eq.2.

### Sampling

High-level content generationThe outputs of DMs have pixel consistency within the training image size. Outpainting an area with a generative model might lead to unrealistic and odd artifacts due to poor long-range spatial correlations. Here, we show how to predict pixels beyond the image's boundaries by generating a hierarchical mapping of the data. The starting point is the generation of the highest-level representation of the data. In our case, it is the sketch of the cellular arrangement in the WSI (see Figure 1(a)). Since higher-frequency details are unnecessary at this stage, we can downsample the masks until the clustering pattern is still recognizable. The diffusion model, conditioned on the context prompt (e.g. Adenocarcinoma subtype), learns the segmentation masks which contain the cellular macro-structures information.

Random patch diffusionGiven a segmentation mask \(M\), we can proceed with the large image sampling according to Section 4.1 in the latent space \(\mathbb{R}^{D}\) of \(Y=\Phi(X)\) (see Alg.2). Since we trained a conditional diffusion model with conditions \(c_{1},...,c_{N}\), the learned model takes the form \(\epsilon_{\theta}(z_{t},t)=(\epsilon_{\theta}(z_{t},t|c_{1}),...,\epsilon_{ \theta}(z_{t},t|c_{N}))\). Given \(y_{t}\), we first sample projections \(proj_{I_{1}},...,proj_{I_{m}}\in\mathcal{C}\), corresponding to different crops of \(d\) connected pixels up to the \(m\)-th projection with \(\cup_{j=1}^{m}I_{j}=\{1,...,D\}\) and \(\cup_{j=1}^{m}I_{j}\setminus\cup_{j=1}^{m-1}I_{j}+\bigotimes\) (see the left hand-side of Figure 1(b)). Note that \(m\) is not fixed, but varies over the sampling steps and is upper bounded by the number of possible crops of \(d\) connected pixels. The random selection of

Figure 3: Comparison of sampling speed for DiffCollage and DiffInfinite, measuring diffusion steps required for image sampling. Demonstrating increased efficiency of DiffInfinite for larger images.

the projection is implemented such that regions with latent pixels of low projection coverage are more likely. Secondly, we calculate for every projection \(j\in\{1,...,m\}\) the crop \(proj_{I_{j}}(y_{t})\) and perform one step of the DDIM sampling procedure using the classifier-free guidance model \((1+\omega)\epsilon_{\theta}(z_{i}^{j},t,c)-\omega\epsilon_{\theta}(z_{i}^{j},t,\varnothing)\), where \(\epsilon_{\theta}\) is the learned model and \(z_{i}^{j}=proj_{I_{j}}(y_{t})\). This results for every pixels \(i\in I_{j}\) in \(N\) values \(DDIM_{\theta,c_{1}}(proj_{I_{j}}(y_{t})),...,DDIM_{\theta,c_{N}}(proj_{I_{j}}(y _{t}))\), one for every condition \(c_{i}\) (see the right hand-side of Figure 2b). If \(i\notin I_{j^{\prime}}\) for all \(j^{\prime}\), the pixel \(i\) has not been considered yet and we assign \(i\) the value \([y_{t-1}]_{i}=[DDIM_{\theta,M_{i}}(proj_{I_{j}}(y_{t}))]_{l}\), where \(l\) corresponds to the pixel \(i\) under the projection \(I_{j}\) and \(M_{i}\) is the value of \(i\) in the mask \(M\). Since we are updating random projections of the overall image, in the \(t\)-th step pixels either have the time index \(t\) or \(t+1\), resulting in a reversed diffusion process of differing time states. We initialize a tensor \(L_{t}\), with the same size \(D\) as the latent variable, to keep track of the time index for each pixel. Each element is set to \(L_{T}\equiv T\). In the \(j\)-th iteration of the \(t\)-th step we only update the pixels that have not been considered in one of the previous iterations of the \(t\)-th diffusion step, hence all the pixels in \(i\in I_{j}\) with \(proj_{I_{j}}(L_{t})_{i}=t+1\), similarly to the inpainting mask in the Repaint sampling method [45].

```
1: Randomly train on labelled or unlabelled data with probability \(p_{unc}\), \(u\sim Uniform[0,1]\)
2:\((z_{0},c)=\left\{(z_{0},c)\sim q_{0}(z_{0})\right.\) if\(u\geq p_{unc}\)then
3: Sample random time step
4:\(t\sim Uniform\{1,...,T\}\)
5: Sample noise. \(\sim\mathcal{N}(0,\mathbf{I})\)
6: Compute data, \(z_{1}=\tau x_{0}+\sigma_{t}\epsilon\)
7: Take gradient descent: step
8:\(\nabla_{\theta}\|c-\epsilon_{\theta}(z_{1},t,c)\|^{2}\)
9:until converged ```

**Algorithm 1** DiffInfinite Training

```
1: High-level segmentation mask \(M\in\mathbb{R}^{D}\) and learned model \(\epsilon_{\theta}\)
2: Synthetic image \(X\) with the mask size initialization:
3:\(y_{T}\sim\mathcal{N}(0,\mathbf{I})\), index set \(I_{0}-\varnothing\) and time state tensor \(L_{T}\equiv T\)
4:Repeat
5:for\(t\in(T-1,...0)\)do
6:while\(\cup_{j=0}^{m}I_{j}+\{1,...,D\}\)do
7:\(m\gets m+1\)
8: Select randomly \(proj_{I_{m}}\in\mathcal{C}(\{proj_{I_{1}},\dots,proj_{I_{m-1}}\}\)
9: Crep \(z_{1}^{m}=proj_{I_{m}}(y_{t})\)
10:for all conditions \(n\in\{1,...,N\}\)do
11: DDM sampling with classifier-free guidance
12:\(z_{1}^{m}|_{1}\sim p_{\theta,c}(z_{1}^{m},z_{\theta})\)
13:endfor
14:for all indices \(i\in I_{m}\)do
15:if\(i\neq I_{j}\)do
16:\([y_{t-1}]_{i}\leftarrow[z_{t-1}^{m}|M_{i}]_{l}\)
17:\(proj_{I_{m}}(L_{t})_{i}\gets t\)
18: such that \([proj_{I_{m}}(y_{t-1})]_{l}=[z_{t-1}^{m}|M_{i}]_{l}\)
19: end if
20: endfor
21: endwhile
22:endfor
23:\(X\leftarrow\Psi(y_{0})\) ```

**Algorithm 2** DiffInfinite Sampling

To restore the pixels that already received an update, i.e. every pixel \(i\in I_{j}\) with \(proj_{I_{j}}(L_{t})_{i}=t\), we store a replica of the previous diffusion step for every pixel. Finally, we update all the time states in \(L_{t}\) that received an update in the \(j\)-th iteration to \(t\) resulting in \(proj_{I_{j}}(L_{t})_{i}=t\) for all \(i\in I_{j}\). See the top row of Fig.2c for an illustration of the evolution of \(L_{t}\). The random patch diffusion can also be applied to mask generation, where the only condition is the context prompt. This method can generate segmentation masks of arbitrary sizes with the correlation length bounded by two times the training mask image size.

ParallelizationThe sampling method proposed has several advantages. In Zhang et al. [48] each sequential patch is outpainted from the previous one with 50\(\%\) of the pixels shared. Here, the randomization eventually leads to every possible overlap with the neighboring patches. This introduces a longer pixel correlation across the whole generated image, avoiding artifacts due to tiling. In Figure 3, we show that the number of steps in the whole large image generation process is drastically reduced with the random patching method with respect to the sliding window one. Moreover, in the sliding sampling method, the model can be paralleled only \(2\) or \(4\) times, depending if we are outpainting the image horizontally or on both axis. In our approach, we can parallelize the sampling up to the computational resource limit.

## 5 Data Assessment

To assess synthetic images for medical image analysis, we need to take various dimensions of data assessment into account. We extend traditional metrics from the natural image community with qualitative and quantitative assessments specific to the medical context. For the qualitative analysis, a team of pathologists evaluated the images for histological plausibility. The quantitative assessment entailed a proof-of-concept that a model can learn sensible features from the synthetically generated image patches for a relevant downstream task. As data protection is highly relevant regarding patient data, we performed evaluations to rule out memorization effects of the generative model.

### Traditional Fidelity

We evaluate the fidelity of synthetic \(512\times 512\) images by calculating Improved Precision (IP) and Improved Recall (IR) metrics between 10240 real and synthetic images [67].2 The IP evaluates synthetic data quality, while the IR measures data coverage. Despite their unsuitability for histological data [68, 69], Frechet-Inception Distance (FID) and Inception Score (IS) [70, 71] are reported for comparison with [72] and Shrivastava and Fletcher [73].3 The metrics' explanations and formulas can be found in Appendix C.

Footnote 2: https://github.com/blandocs/improved-precision-and-recall-metric-pytorch

Footnote 3: https://github.com/toshas/torch-fidelity

In Table 1 (left), we report an IP of \(0.94\) and an IR of \(0.70\), indicating good quality and coverage of the generated samples. However, we note that these metrics are only somewhat comparable due to the different types of images generated by MorphDiffusion [72] and NASDM [73]. For the large images of size \(2048\times 2048\), we rely solely on the IP and IR for quantitative evaluation due to the limited number of \(200\) generated large images. As shown in Figure 3(a) of [67], FID is unsuitable for evaluating such a small sample size, while IP and IR are more reliable. In Table 1 (right), we find that generating images first results in slightly higher IR, while generating the mask first achieves an IP of 0.98. For the sake of completeness we also report the scores then combining the two datasets. To compare our method to DiffCollage we generate \(200\) images using [48]. DiffInfinite performs better than DiffCollage wrt. to IP and IR. The drop of IR to \(0.22\) might be a result of the tiling artifacts observable in the LHS of Figure 11.

### Domain Experts Assessment

To assess the histological plausibility of our generated images, we conducted a survey with a cohort of ten experienced pathologists, averaging 8.7 years of professional tenure. The pathologists were tasked with differentiating between our synthetized images and real image patches extracted from whole slide images. We included both small patches (512 \(\times\) 512 px) as commonly used for downstream tasks as well as large patches (2048 \(\times\) 2048 px). Including large patches enabled us to additionally evaluate the modelled long-range correlations in terms of transitions between tissue types as well as growth patterns which are usually not observable on the smaller patch sizes but essential in histopathology. In total the survey contained 60 images, in equal parts synthetic and real images as well as small and large patches. The overall ability of pathologists to discern between real and synthetic images was modest, with an accuracy of 63%, and an average reported confidence level of 2.22 on a 1-7 Likert scale. While we observed high inter-rater variance, there was no clear correlation between experience and accuracy (r(8) =.069, p=.850), nor between confidence level and accuracy (r(8) =.446, p=.197). Furthermore there was no significant correlation between the participants' completion time of the survey and the number of correct responses (r(8) = -.08, p=.826).

Surprisingly, we found a similar performance for both, real and synthetic images. This indicates that, while clinical practice is mostly based on visual assessment, it is not a common task for pathologists to be restricted to parts of the whole slide image only. More detailed visualizations of the individual scores can be found in Appendix B. Besides this satisfactory result, we additionally wanted to

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **IP**\(\uparrow\) & **IR**\(\uparrow\) & **IS**\(\uparrow\) & **FID**\(\downarrow\) \\ \hline Morph-Diffusion [72] & 0.26 & **0.85** & 2.1 & 20.1 \\ NASDM [73] & - & - & **2.7** & **15.7** \\ DiffInfinite (a) & **0.94** & 0.70 & **2.7** & 26.7 \\ \hline \hline \end{tabular} \begin{tabular}{l c c} \hline \hline  & **IP**\(\uparrow\) & **IR**\(\uparrow\) \\ \hline DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\ \hline \hline \end{tabular} \begin{tabular}{l c c} \hline \hline  & **IP**\(\uparrow\) & **IR**\(\uparrow\) \\ \hline DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\ \hline \hline \end{tabular} \begin{tabular}{l c c} \hline \hline  & **IP**\(\uparrow\) & **IR**\(\uparrow\) \\ \hline DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c} \hline \hline  & **IP**\(\uparrow\) & **IR**\(\uparrow\) \\ \hline DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Metrics to quantitatively evaluate the quality of the generated images. Left: scores for images of size \(512\times 512\). DiffInfinite (a) first generates a mask and secondly an image following Section 4.1. Right: scores for real and generated images of size \(2048\times 2048\) resized to \(512\times 512\). All methods use the same model trained on small patches of size \(512\times 512\). DiffCollage corresponds to the method proposed in [48]. DiffInfinite (b) uses the real masks, while DiffInfinite (c) first generates a mask and secondly the large image. DiffInfinite (b) & (c) refers to the mixture of the generated dataset from DiffInfinite (b) and DiffInfinite (c).

explore the limitations of our method by assessing the nuanced differences pathologists observed between synthetic and real images. While overall the structure and features seemed similar and hard to discern, they sometimes reported regions of inconsistent patterns, overly homogeneous chromatin in some of the synthetic nuclei, peculiarities in cellular and intercellular structures, and aesthetic elements. These seemed to be especially pronounced in tumours regions where sometimes the tissue architecture appeared exaggerated, the transition to stroma or surrounding tissue was too abrupt and some cells lacked distinguishable nucleoli or cytoplasm. We attribute the nuanced effect of larger image size on the accuracy on this observation (cf. Fig. 5C). Overall the finding of the conducted survey demonstrates how complex the task of distinguishing between real and synthetically generated data is even for experienced pathologists while still highlighting potential areas to improve the generative model.

### Synthetic Data for Downstream Tasks

A major interest in the availability of high quality labeled synthetic images is their use in downstream digital pathology applications. In this area, two primary challenges are the binary classification of images into cancerous or healthy tissues and the segmentation of distinct tissue areas in the tumor microenvironment. The unique ability of our technique to generate images of different cancer subtypes through the context prompt as well as the ability to create new segmentation masks and their corresponding H&E images specifically addresses these two challenges. Notably, expert annotations are costly and time consuming to acquire thus emphasizing the benefits of being able to train on purely synthetic datasets or augmenting annotated data in the low data regime. To showcase these two usecases we performed a series of experiments in both classification and segmentation settings. For all experiments, we trained a baseline classier on a relatively small number of expert annotations IH1 (#patches = 3726)) -- the same that were used to train DiffInfinite -- and additionally trained one model purely on synthetic data (IH1-S, #patches = 9974, \(\omega=0\)), and one model on the real data augmented with the synthetic images. To generate target labels for the classification experiments, we simplified the segmentation challenge by categorizing patches with at least 0.05% of pixels labeled as 'Carcinoma' in the segmentation masks as 'Carcinoma'. All other patches were labeled 'Non-Carcinoma'. We evaluated all three classification models on several out-of-distribution datasets. We utilized two proprietary datasets (from the same cancer type with similar attributes but from distinct patient groups: IH1 (# patients=13, # patches=704) and IH3 (# patients=2, # patches=2817). Moreover, we assessed the models using two public datasets (NCK-CRC [74] and PatchCamelyon [75]), both representing tissue from different organs with distinct morphologies. Our findings, summarized in Table 1(a), suggest that a classifier's out-of-distribution performance, trained with limited sample size and morphological diversity, can vary significantly (ranging from 0.628 to 0.857 balanced accuracy). This variability cannot be attributed solely to morphology but may also be influenced by

\begin{table}

\end{table}
Table 2: Zero-shot evaluation results of the downstream tasks, encompassing both classification and segmentation scenarios. We employed three distinct models for each scenario: The first, ”Trained Real,” was trained using real data (in-house IH1), which also served as the training set for DiffInfinite. The second, ”Trained Synthetic,” was trained using samples generated from DiffInfinite, and the third, ”Trained Augmented,” utilized a combination of real and synthetic data. Our evaluation extends across separate lung cohorts (internal datasets IH2 and IH3) and additional indications (external datasets NCT, CRC, PCam), with varying degrees of data drift introduced.

factors such as resolution and variations in scanning and staining techniques. Training exclusively with a larger set of synthetic images can enhance performance on some datasets (specifically IH2 and IH3), underscoring the advantages of leveraging the full training data in a semi-supervised manner within the generative model. Incorporating synthetic data as an augmentation to real data not only prevents the classifier's performance decline, as seen on NCT-CRC and Patchcamelyon, on similar datasets but also bolsters its efficiency on more distinct ones. For the more challenging segmentation task we again trained three segmentation models to differentiate between carcinoma, stroma, necrosis, and a miscellaneous class that included all other tissue types, such as artifacts. The baseline performance of the real data model on a distinct group of lung patients (dataset IH2) of a \(F_{1}\) score of \(0.614\pm 0.009\) (across three random seeds) highlights the difficulty of generalizing out of distribution in this tasks. While the purely synthetic model was not able to fully recover the baseline performance (\(0.471\pm 0.039\)), augmenting the small annotated dataset with synthetic data enhanced predictive performance to an \(F_{1}\) score of \(0.710\pm 0.021\). This boost of 10 percentage points in performance demonstrates that the synthetic data provide new, relevant information to the downstream task. In summary, our findings demonstrate the feasibility of meeting or surpassing baseline performance levels for both tasks using either entirely synthetic data or within an augmented context. Nevertheless, the advantages of employing synthetic data in downstream tasks continue to pose a challenge, not only within the medical image domain but also across various other domains [76, 77, 78], thus requiring more comprehensive assessment and thorough examination.

### Considerations on Memorization

In medicine the adherence to privacy regulations is a sensitive requirement. While it is generally not possible for domain experts to infer patient identities from the image content of a histological tile or slide alone [79], developers and users of generative models are well advised to understand the risk of correspondence between the training data and the synthesized data. To this end, we evaluate the training and synthesized data against two memorization measures. The authenticity score \(A\in[0,1]\) by [54] aims to measure the rate by which a model generates new samples (higher score means more innovative samples). Similarly, [80] aims to estimate the degree of data copying \(C_{T}\) from the training data by the generative model. A \(C_{T}\ll 0\) implies data copying, while a \(C_{T}\gg 0\) implies an underfitting of the model. The closer to \(0\) the better. See Appendix C for a precise closed form of the measures and Table 5 for the full quantitative results, indicating that the DiffInfinite model is not prone to data copying across all resolutions and variations considered here 4. The \(A\) range between \(0.86\) and \(0.98\), signifying a high rate of authenticity. While other papers unfortunately do not report such detailed memorization statistics for their models, the results by [54] suggest that a score \(\gg 0.8\) is not trivial to achieve. None of the models under consideration in [54] (VAE, DCGAN, WGAN-GP, ADS-GAN) achieve more than \(0.82\) in \(A\) on simpler data (MNIST). This interpretation is strengthened by the results of a \(C_{T}\gg 0\) which indicates that the model might even be underfitting and is not in a data copying regime. Qualitative results on the nearest neighbour search between training and synthetic data in Figure 1 further corroborate these quantitative results.

Footnote 4: We use https://github.com/marcojira/fls from [81] to calculate both scores.

## 6 Conclusions

DiffInfinite offers a novel sampling method to generate large images in digital pathology. Due to the high-level mask generation followed by the low-level image generation, synthetic images contain long-range correlations while maintaining high-quality details. Since the model trains and samples on small patches, it can be efficiently parallelized. We demonstrated that the classifier-free guidance can be extended to a semi-supervised learning method, expanding the labelled data feature space with unlabelled data. The biological plausibility of the synthetic images was assessed in a survey by \(10\) domain experts. Despite their training, most participants found it challenging to differentiate between real and synthetic data, reporting an average low confidence in their decisions. We found that samples from DiffInfinite can help in certain downstream machine learning tasks, on both in- as well as out-of-distribution datasets. Finally, authenticity metrics validate DiffInfinite's capacity to generate novel data points with little similarity to the training data which is beneficial for the privacy preserving use of generative models in medicine.

Acknowledgements

We would like to acknowledge our team of pathologists who provided valuable feedback in and outside of the conducted survey - special thank you to Frank Dubois, Niklas Preinssl, Cleopatra Schreiber, Vitaly Garg, Alexander Arnold, Sonia Villegas, Rosemarie Krupar and Simon Schallenberg. Furthermore, we would like to thank Marvin Sextro for his support in the analyses. This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B)]. RM-S is grateful for EPSRC support through grants EP/T00097X/1, EP/R018634/1 and EP/T021020/1, and DI for EP/R513222/1. MA is funded by Dotphoton, QuantIC and a UofG Ph.D. scholarship.

## References

* Aggarwal et al. [2021] Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King, Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. _NPJ digital medicine_, 4(1):65, 2021.
* Parziale et al. [2022] Antonio Parziale, Monica Agrawal, Shengpu Tang, Kristen Severson, Luis Oala, Adarsh Subbaswamy, Sayantan Kumar, Flora Schoerverth, Stefan Hegselmann, Helen Zhou, Ghada Zamzmi, Purity Mugambi, Elena Sizikova, Girmaw Abebe Tadesse, Yuyin Zhou, Taylor Killian, Haoran Zhang, Fahad Kamran, Andrea Hobby, Mars Huang, Ahmed Alaa, Harvineet Singh, Irene Y. Chen, and Shalmali Joshi. Machine learning for health (ml4h) 2022. In Antonio Parziale, Monica Agrawal, Shalmali Joshi, Irene Y. Chen, Shengpu Tang, Luis Oala, and Adarsh Subbaswamy, editors, _Proceedings of the 2nd Machine Learning for Health symposium_, volume 193 of _Proceedings of Machine Learning Research_, pages 1-11. PMLR, 28 Nov 2022.
* Springenberg et al. [2023] Maximilian Springenberg, Annika Frommholz, Markus Wenzel, Eva Weicken, Jackie Ma, and Nils Strodthoff. From modern cnns to vision transformers: Assessing the performance, robustness, and classification strategies of deep learning models in histopathology. _Medical Image Analysis_, 87:102809, 2023. ISSN 1361-8415.
* Wang et al. [2023] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. _arXiv preprint arXiv:2304.03284_, 2023.
* Hagele et al. [2020] Miriam Hagele, Philipp Seegerer, Sebastian Lapuschkin, Michael Bockmayr, Wojciech Samek, Frederick Klauschen, Klaus-Robert Muller, and Alexander Binder. Resolving challenges in deep learning-based analyses of histopathological images using explanation methods. _Scientific reports_, 10(1):1-12, 2020.
* Ma and Wang [2023] Jun Ma and Bo Wang. Segment anything in medical images. _arXiv preprint arXiv:2304.12306_, 2023.
* Rieke et al. [2020] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health with federated learning. _NPJ digital medicine_, 3(1):119, 2020.
* Oala et al. [2020] Luis Oala, Jana Fehr, Luca Gilli, Pradeep Balachandran, Alixandro Werneck Leite, Saul Calderon-Ramirez, Danny Xie Li, Gabriel Nobis, Erick Alejandro Munoz Alvarado, Giovanna Jaramillo-Gutierrez, Christian Matek, Arun Shroff, Ferath Kherif, Bruno Sanguinetti, and Thomas Wiegand. M14h auditing: From paper to practice. In Emily Alsentzer, Matthew B. A. McDermott, Fabian Falck, Suproteem K. Sarkar, Subhrajit Roy, and Stephanie L. Hyland, editors, _Proceedings of the Machine Learning for Health NeurIPS Workshop_, volume 136 of _Proceedings of Machine Learning Research_, pages 280-317. PMLR, 11 Dec 2020.
* Schwarz et al. [2019] Christopher G Schwarz, Walter K Kremers, Terry M Therneau, Richard R Sharp, Jeffrey L Gunter, Prashanthi Vemuri, Arvin Arani, Anthony J Spychalla, Kejal Kantarci, David S Knopman, et al. Identification of anonymous mri research participants with face-recognition software. _New England Journal of Medicine_, 381(17):1684-1686, 2019.

* Chen et al. [2022] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y Chen, Andrew D Trister, Rahul G Krishnan, and Faisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16144-16155, 2022.
* Xue et al. [2021] Yuan Xue, Jiarong Ye, Qianying Zhou, L Rodney Long, Sameer Antani, Zhiyun Xue, Carl Cornwell, Richard Zaino, Keith C Cheng, and Xiaolei Huang. Selective synthetic augmentation with histogram for improved histopathology image classification. _Medical image analysis_, 67:101816, 2021.
* Hou et al. [2017] Le Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M Kurc, Rajarsi R Gupta, and Joel H Saltz. Unsupervised histopathology image synthesis. _arXiv preprint arXiv:1712.05021_, 2017.
* Quiros et al. [2021] Adalberto Claudio Quiros, Roderick Murray-Smith, and Ke Yuan. PathologyGAN: learning deep representations of cancer tissue. _Journal of Machine Learning for Biomedical Imaging_, 4:1-48, 2021.
* Qasim et al. [2020] Ahmad B Qasim, Ivan Ezhov, Suprosanna Shit, Oliver Schoppe, Johannes C Paetzold, Anjany Sekuboyina, Florian Kofler, Jana Lipkova, Hongwei Li, and Bjoern Menze. Red-gan: Attacking class imbalance via conditioned generation. yet another medical imaging perspective. In _Medical Imaging with Deep Learning_, pages 655-668. PMLR, 2020.
* Chambon et al. [2022] Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, Malgorzata Polacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P Langlotz, and Akshay Chaudhari. Roentgen: Vision-language foundation model for chest x-ray generation. _arXiv preprint arXiv:2211.12737_, 2022.
* Chambon et al. [2022] Pierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari. Adapting pretrained vision-language foundational models to medical imaging domains. _arXiv preprint arXiv:2210.04133_, 2022.
* Oala et al. [2023] Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Michele Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, and Bruno Sanguinetti. Data models for dataset drift controls in machine learning with optical images. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Levine et al. [2020] Adrian B Levine, Jason Peng, David Farnell, Mitchell Nursey, Yiping Wang, Julia R Naso, Hezhen Ren, Hossein Farahani, Colin Chen, Derek Chiu, Aline Talhouk, Brandon Sheffield, Maziar Riazy, Philip P Ip, Carlos Parra-Herran, Anne Mills, Naveena Singh, Basile Tessier-Cloutier, Taylor Salisbury, Jonathan Lee, Tim Salcudean, Steven JM Jones, David G Huntsman, C Blake Gilks, Stephen Yip, and Ali Bashashati. Synthesis of diagnostic quality cancer pathology images by generative adversarial networks. _The Journal of Pathology_, 252(2):178-188, 2020.
* Fernandez et al. [2022] Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, and M Jorge Cardoso. Can segmentation models be trained with fully synthetically generated data? In _Simulation and Synthesis in Medical Imaging: 7th International Workshop, SASHIMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings_, pages 79-90. Springer, 2022.
* Chen et al. [2021] Richard Chen, Ming Lu, Tiffany Chen, Drew Williamson, and Faisal Mahmood. Synthetic data in machine learning for medicine and healthcare. _Nature Biomedical Engineering_, 5:1-5, 06 2021.
* Dolezal et al. [2023] James M Dolezal, Rachelle Wolk, Hanna M Hierommimon, Frederick M Howard, Andrew Srisuwananukorn, Dmitry Karpeyev, Siddhi Ramesh, Sara Kochanny, Jung Woo Kwon, Meghana Agni, et al. Deep learning generates synthetic cancer histology for explainability and education. _NPJ Precision Oncology_, 7(1):49, 2023.
* Efros and Leung [1999] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In _Proceedings of the seventh IEEE international conference on computer vision_, volume 2, pages 1033-1038. IEEE, 1999.

* [23] Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-structured vector quantization. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 479-488, 2000.
* [24] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 417-424, 2000.
* [25] Zongben Xu and Jian Sun. Image inpainting by patch propagation using patch sparsity. _IEEE transactions on image processing_, 19(5):1153-1165, 2010.
* [26] Lin Liang, Ce Liu, Ying-Qing Xu, Baining Guo, and Heung-Yeung Shum. Real-time texture synthesis by patch-based sampling. _ACM Transactions on Graphics (ToG)_, 20(3):127-150, 2001.
* [27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9446-9454, 2018.
* [28] Avisek Lahiri, Arnav Kumar Jain, Sanskar Agrawal, Pabitra Mitra, and Prabir Kumar Biswas. Prior guided gan based semantic inpainting. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 13696-13705, 2020.
* [29] Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and Shin'ichi Satoh. Guidance and evaluation: Semantic-aware image inpainting for mixed scenes. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXVII 16_, pages 683-700. Springer, 2020.
* [30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* [31] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2536-2544, 2016.
* [32] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5505-5514, 2018.
* [33] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image inpainting with gated convolution. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4471-4480, 2019.
* [34] Yi Wang, Ying-Cong Chen, Xin Tao, and Jiaya Jia. Vcnet: A robust approach to blind image inpainting. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV 16_, pages 752-768. Springer, 2020.
* [35] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. _arXiv preprint arXiv:2103.10428_, 2021.
* [36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. _arXiv preprint arXiv:2109.07161_, 2021.
* [37] Mark Sabini and Gili Rusak. Painting outside the box: Image outpainting with gans. _arXiv preprint arXiv:1808.08483_, 2018.
* [38] Basile Van Hoorick. Image outpainting and harmonization using generative adversarial networks. _arXiv preprint arXiv:1912.10960_, 2019.

* Lin et al. [2022] Chieh Hubert Lin, Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, and Ming-Hsuan Yang. InfinityGAN: Towards infinite-pixel image synthesis. In _International Conference on Learning Representations_, 2022.
* Cheng et al. [2022] Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and Ming-Hsuan Yang. Inout: diverse image outpainting via gan inversion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11431-11440, 2022.
* Wang et al. [2021] Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. Sketch-guided scenery image outpainting. _IEEE Transactions on Image Processing_, 30:2643-2655, 2021.
* Wang et al. [2021] Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. Rego: Reference-guided outpainting for scenery image. _arXiv preprint arXiv:2106.10601_, 2021.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-10, 2022.
* Lugmayr et al. [2022] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repairt: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11461-11471, 2022.
* Zhang and Agrawala [2023] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* Wang et al. [2022] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Sindiffusion: Learning a diffusion model from a single natural image. _arXiv preprint arXiv:2211.12445_, 2022.
* Zhang et al. [2023] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel generation of large content with diffusion models. _arXiv preprint arXiv:2303.17076_, 2023.
* Bond-Taylor and Willcocks [2023] Sam Bond-Taylor and Chris G. Willcocks. \(\infty\)-diff: Infinite resolution diffusion with subsampled mollified states, 2023.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* Sompealli et al. [2023] Gowthami Sompealli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6048-6058, 2023.
* Carlini et al. [2023] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. _arXiv preprint arXiv:2301.13188_, 2023.
* Vyas et al. [2023] Nikhil Vyas, Sham Kakade, and Boaz Barak. Provable copyright protection for generative models. _arXiv preprint arXiv:2302.10870_, 2023.
* Alaa et al. [2022] Ahmed Alaa, Boris Van Breugel, Evgeny S Saveliev, and Mihaela van der Schaar. How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. In _International Conference on Machine Learning_, pages 290-306. PMLR, 2022.
* Oala [2023] Luis Oala. _Metrological machine learning (2ML)_. 1 edition, 2023. URL https://metrological.ml.

* [56] Matthew Jagielski, Om Thakkar, Florian Tramer, Daph Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, et al. Measuring forgetting of memorized training examples. _arXiv preprint arXiv:2207.00099_, 2022.
* [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [58] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* [59] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6840-6851. Curran Associates, Inc., 2020.
* [60] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.
* [63] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23(47):1-33, 2022.
* [64] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [65] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2020.
* [66] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [67] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [68] Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. In _Proc. ICLR_, 2023.
* [69] Shane T. Barratt and Rishi Sharma. A note on the inception score. _ArXiv_, abs/1801.01973, 2018.
* [70] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [71] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* [72] Puria Azadi Moghadam, Sanne Van Dalen, Karina C. Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, and Ali Bashashati. A morphology focused diffusion probabilistic model for synthesis of histopathology images. _arXiv preprint arXiv:2209.13167v2_, 2022.

* [73] Aman Shrivastava and P Thomas Fletcher. Nasdm: Nuclei-aware semantic histopathology image generation using diffusion models. _arXiv preprint arXiv:2303.11477_, 2023.
* [74] Jakob Nikolas Kather, Niels Halama, and Alexander Marx. 100,000 histological images of human colorectal cancer and healthy tissue, April 2018. URL https://doi.org/10.5281/zenodo.1214456.
* [75] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11_, pages 210-218. Springer, 2018.
* [76] Shenghuan Sun, Gregory M Goldgof, Atul Butte, and Ahmed M Alaa. Aligning synthetic medical images with clinical knowledge using human feedback. _arXiv preprint arXiv:2306.12438_, 2023.
* [77] Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. _arXiv preprint arXiv:2308.06259_, 2023.
* [78] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. _arXiv preprint arXiv:2305.17493_, 2023.
* [79] Petr Holub, Heimo Muller, Tomas Bil, Luca Pireddu, Markus Plass, Fabian Prasser, Irene Schlunder, Kurt Zatloukal, Rudolf Nenutil, and Tomas Brazdil. Privacy risks of whole-slide image sharing in digital pathology. _Nature Communications_, 14(1):2577, 2023.
* [80] Casey Meehan, Kamalika Chaudhuri, and Sanjoy Dasgupta. A non-parametric test to detect data-copying in generative models. _CoRR_, abs/2004.05675, 2020.
* [81] Marco Jiralerspong, Avishek Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, and Gauthier Gidel. Feature likelihood score: Evaluating generalization of generative models using samples, 2023.
* [82] Nicolas Pielawski and Carolina Wahlby. Introducing hann windows for reducing edge-effects in patch-based image segmentation. _PloS one_, 15(3):e0229839, 2020.