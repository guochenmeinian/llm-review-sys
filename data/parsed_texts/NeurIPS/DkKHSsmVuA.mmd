# Optimal Rates for Bandit Nonstochastic Control

 Y. Jennifer Sun \({}^{*}\)

Princeton University

Google DeepMind

ys7849@princeton.edu

&Stephen Newman \({}^{*}\)

Princeton University

sn9581@princeton.edu

&Elad Hazan

Princeton University & Google DeepMind

ehazan@princeton.edu

###### Abstract

Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm of Gradu et al. (2020) has a \(T^{\frac{3}{4}}\) time horizon dependence, and the authors posed an open question about whether a tight rate of \(\sqrt{T}\) could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret (up to logarithmic factors) for both known and unknown systems. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.

## 1 Introduction

Linear-Quadratic Regulator (LQR) and the more general Linear-Gaussian (LQG) control problems have been extensively studied in the field of control theory due to their wide range of applications and admittance of analytical solutions by the seminal works of Bellman (1954) and Kalman (1960). LQR and LQG control problems study the design of a feedback control policy for a linear dynamical system with the goal of minimizing cumulative, possibly time-varying quadratic costs. The discrete version of the problem studies the control of the following linear dynamical system governed by dynamics \((A,B,C)\)1:

Footnote 1: The LQR/LQG dynamics can be generalized to time-varying linear dynamical systems. Here we restrict ourselves to linear time-invariant systems for simplicity.

\[\mathbf{x}_{t+1}=A\mathbf{x}_{t}+B\mathbf{u}_{t}+\mathbf{w}_{t}\;,\;\mathbf{y }_{t}=C\mathbf{x}_{t}+\mathbf{e}_{t}\;,\]

where at time \(t\), \(\mathbf{x}_{t}\) represents the system's state, \(\mathbf{u}_{t}\) represents the control exerted on the system, and \(\{\mathbf{w}_{t}\}_{t=1}^{T}\) represents a sequence of i.i.d. centered Gaussian perturbations injected to the system. For a simple example of the model, consider the problem of controlling an elevator to reach the desired floors while minimizing the discomfort (due to acceleration or deceleration) and the energy consumption. In this case, the state \(\mathbf{x}_{t}\) would be a vector consisting of the elevator's position and velocity at time \(t\). The control input \(\mathbf{u}_{t}\) is the force applied by the elevator motor at time \(t\). \(\mathbf{w}_{t}\) represents process noise at time \(t\), caused possibly by unexpected mechanical disturbances. In the generality of LQG, the system's states are not accessible. In the elevator example, one may not have the access to the measurement of the elevator's velocity or the precise position of the elevator. Instead, the algorithm has access to an observation \(\mathbf{y}_{t}\), which is a linear transformation of state perturbed by a sequence \(\{\mathbf{e}_{t}\}_{t=1}^{T}\) of i.i.d. centered Gaussian noises, i.e. the elevator's observed position. The cost isa quadratic function of both the observation and the control exerted. The goal in LQR/LQG problems is to find a control policy \(\pi\) in some policy class \(\Pi\) that minimizes the cumulative cost over a finite time horizon \(T\). With \(\mathbf{y}_{t}^{\pi},\mathbf{u}_{t}^{\pi}\) denoting the observation and control at time \(t\) resulted from executing policy \(\pi\), the objective is formally given by

\[\operatorname*{minimize}_{\pi\in\Pi}\;\;J_{T}(\pi)\stackrel{{ \text{def}}}{{=}}\sum_{t=1}^{T}c_{t}(\mathbf{y}_{t}^{\pi},\mathbf{u}_{t}^{ \pi})=\sum_{t=1}^{T}\mathbf{y}_{t}^{\pi\top}Q_{t}\mathbf{y}_{t}^{\pi}+\mathbf{ u}_{t}^{\pi\top}R_{t}\mathbf{u}_{t}^{\pi},\]

where \(Q_{t},R_{t}\) are some possibly time-varying state-weighting and control-weighting matrices, respectively. In the elevator example, if our goal is to control the elevator to reach the desired floors while minimizing the discomfort and the energy consumption, the cost function at every time step \(t\) may take the form \(c_{t}(\mathbf{y}_{t},\mathbf{u}_{t})=c_{1}\|\mathbf{y}_{t}-z_{\mathrm{goal}} \|_{2}^{2}+c_{2}\|\mathbf{u}_{t}\|_{2}^{2}\), where \(z_{\mathrm{goal}}\) is the target position of the elevator, \(c_{1},c_{2}>0\).

Variations of the LQR/LQG problem have garnered considerable interest in the machine learning community. In the recent literature of **online nonstochastic control**, several setting-based generalizations to the aforementioned linear-control framework have been explored, including:

* **Adversarial cost functions**(Cohen et al. (2018); Agarwal et al. (2019)): In this setting, the cost functions are adversarially chosen in an unpredictable way to the learner. This generalized formulation allows for applications such as building climate control in the presence of time-varying energy costs, as the energy costs may depend on unexpected demand fluctuation (Cohen et al. (2018)). More recently, the model of adversarially chosen cost functions, along with adversarial perturbations in the subsequent paragraph, give rise to building connection between control theory and meta optimization (Chen and Hazan (2023)), where the nonconvex problem of hyperparameter tuning (e.g. learning rate tuning) can be reduced to online nonstochastic control, which often admits a convex formulation.
* **Adversarial perturbations**(Agarwal et al. (2019), etc.): In this setting, differing from the usual Gaussian assumption on the perturbation distribution (\(\mathbf{w}_{t}\stackrel{{ i.i.d.}}{{\sim}}N(0,\Sigma)\)), nonstochastic control often considers arbitrary disturbances \(\mathbf{w}_{t}\) in the dynamics. The relaxed assumption on the perturbations allows for the control of nonlinear systems under model inaccuracies by approximating them as linear systems (Ghai et al. (2022)).
* **Bandit feedback**: Together with the aforementioned two generalizations, control under bandit feedback further generalizes the control problem and suits many real-world applications. In the bandit setting, the cost functions is unknown to the learner. The learner has access _only_ to the cost incurred by their chosen control. In addition, the learner has access to the system's state (\(\mathbf{x}_{t}\) in the fully observed LQR setting) or a noisy, possibly and usually low-dimensional observation (\(\mathbf{y}_{t}\) in the partially observed LQG setting). The bandit LQG problem can be illustrated with a motivating example. In the problem of controlling medical device such as a ventilator, the state usually represents a number of health status indicators of the patient, such as heart rate, blood pressure, and other related metrics. However, this state may not be fully observable. After the treatment is provided, the learner can observe the patient's response, e.g. changes in some vital indicators, with some observation noise and a cost function, e.g. deviation from the targeted breathing pattern. In this limited information setting, can a learner hope to learn meaning controls under minimal assumptions? Previously, the work of Gradu et al. (2020) and Cassel and Koren (2020) both studied the bandit nonstochastic control problem of fully observed systems and provided provable guarantees.

Taken together, these settings give rise to a general setting in differentiable reinforcement learning that strictly contains a variety of classical problems in optimal and robust control. Naturally, when adversarial costs and perturbations are considered, an optimal solution is _not_ defined a priori. Instead, the primary performance metric is _regret_: the difference between the total cost of a control algorithm and that of the best controller from a specific policy class in hindsight.

As mentioned above, this general setting of bandit nonstochastic control was considered in the recent work of Gradu et al. (2020), whose proposed Bandit Perturbation Controller (BPC) algorithm has a provable regret guarantee of \(\tilde{O}(T^{\frac{3}{4}})\) when compared to a rich policy class called disturbance action controllers (DAC) for fully observed systems. Similar setting has also been studied by Cassel and Koren (2020), who established an optimal regret up to logarithmic factor of \(\tilde{O}(\sqrt{T})\) for fully observable systems under stochastic perturbations and adversarially chosen cost functions. Recently Ghai et al. (2023) improves the dimension dependence of regret bounds using exploration in action-space rather than parameter-space. However, bandit control for partially observable systems (e.g. LQG) is less understood. Thus, these developments in the search for efficient, low-regret bandit online control algorithms leave a central open question (also stated by by Gradu et al. (2020)):

Can we achieve optimal regret \(O(\sqrt{T})\) with **bandit LQG** and **nonstochastic noise**?

Our work answers this question up to logarithmic factors. Our novel Ellipsoidal Bandit Perturbation Controller (EBPC) achieves a \(\tilde{O}(\sqrt{T})\) regret guarantee in the presence of semi-adversarial perturbations in bandit LQG problems with strongly convex cost functions, with the additional generality of possibly unknown system dynamics. By Shamir (2013), this is asymptotically optimal up to logarithmic factors, as bandit optimization over quadratics reduces to bandit control with quadratic losses under \(A=0\), \(B=I\). Our work therefore resolves the upper-bound/lower-bound gap for this generalization of LQR/LQG. The following table gives a comprehensive comparison between the regret guarantee of EBPC and existing results in literature.

### Related work

Online Nonstochastic Control and Online LQR.In the last decade, much research has been devoted to the intersection of learning and control. Abbasi-Yadkori and Szepesvari (2011) and Ibrahimi et al. (2012) considered the problem of learning a controller in LQR for known quadratic cost functions and stochastic/martingale difference perturbation sequence when the dynamics of the system is unknown, and achieved \(\tilde{O}(\sqrt{T})\)-regret in this case. Dean et al. (2018) provided the first provable low-regret, efficient algorithm to solve LQR problems with known cost functions and stochastic perturbations. Cohen et al. (2018) extended this result to changing quadratic costs with stochastic perturbations and provided a regret guarantee of \(O(\sqrt{T})\). Dale et al. (2021) consider the LQG problem with stochastic noise and unknown systems.

More recently, interest has turned to **nonstochastic control**, in which the cost functions and the perturbations can be adversarially chosen Agarwal et al. (2019). A broad spectrum of control problems were reconsidered from the nonstochastic perspective, and several different generalizations were derived. See Hazan and Singh (2022) for a comprehensive text detailing these results.

Online Bandit Convex Optimization with Memory.A classical approach to nonstochastic control of stable/stabilizable linear dynamical systems is to reduce control problems to online convex optimization with bounded memory. This reduction stems from the observation that the effects of past controls decay exponentially in stable/stabilizable systems. In the bandit online convex optimization with bounded memory setting, the learner iteratively plays a decision \(x_{t}\) in a convex set \(\mathcal{K}\subseteq\mathbb{R}^{d}\) and suffers an adversarially chosen loss \(F_{t}(x_{t-H+1:t})\), where \(x_{t-H+1:t}\) is the sequence of points \(x_{t-H+1},...,x_{t}\). In particular, the loss depends on the last \(H\) points played by the algorithm, and the only information revealed to the learner is the scalar loss that they incurred. The goal is to minimize

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline Algorithm & Noise & Observation & Feedback & System & Regret \\ \hline Agarwal et al. (2019) & Adversarial & full & full & known & \(\tilde{O}(\sqrt{T})\) \\ \hline Agarwal et al. (2019) & Stochastic & full & full & known & \(\tilde{O}(1)\) \\ \hline Foster and Simchowitz (2020) & Adversarial & full & full & known & \(\tilde{O}(1)\) \\ \hline Simchowitz et al. (2020) & Semi-Adv. & partial & full & known & \(\tilde{O}(1)\) \\ \hline Simchowitz et al. (2020) & Semi-Adv. & partial & full & unknown & \(\tilde{O}(\sqrt{T})\) \\ \hline Gradu et al. (2020) & Adversarial & full & bandit & unknown & \(\tilde{O}(T^{\frac{1}{4}})\) \\ \hline Cassel and Koren (2020) & Stochastic & full & bandit & known & \(\tilde{O}(\sqrt{T})\) \\ \hline Cassel and Koren (2020) & Adversarial & full & bandit & known & \(\tilde{O}(T^{\frac{1}{3}})\) \\ \hline
**Theorem 4.1** & **Semi-Adv.** & **partial** & **bandit** & **known** & \(\tilde{O}(\sqrt{T})\) \\ \hline
**Theorem 4.2** & **Semi-Adv.** & **partial** & **bandit** & **unknown** & \(\tilde{O}(\sqrt{T})\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of previous results to our contributions.

_regret_, the difference between the loss actually suffered and the loss suffered under the best single play in hindsight:

\[\text{Regret}_{T}\stackrel{{\text{def}}}{{=}}\sum_{t=H}^{T}F_{t}(x_{ t-H+1:t})-\min_{x\in\mathcal{K}}\sum_{t=H}^{T}F_{t}(x,\dots,x).\]

Since the loss function is unknown to the learner in the bandit setting, online bandit convex optimization algorithms make use of low-bias estimators of the true gradient or Hessian. Therefore, it is standard to measure the algorithm's performance by regret over the expectation of losses, where the expectation is taken over the randomness injected when creating such estimators.

Online convex optimization with memory in the full information setting, where the loss function is known to the learner, was proposed by Anava et al. (2015). The work of Agarwal et al. (2019), was the first to connect this to control, and to give a regret bound for online control with adversarial perturbations.

The \(\tilde{O}(T^{\frac{3}{4}})\) regret bound of Gradu et al. (2020) leveraged the bandit convex optimization method of Flaxman et al. (2005). We focus on the bandit LQR/LQG setting with strongly convex and smooth loss functions. It is thus natural to use the techniques of Hazan and Levy (2014), who obtained a \(\tilde{O}(\sqrt{T})\) regret guarantee for bandit convex optimization without memory.

Online Learning with Delay.One technical difficulty in extending OCO with memory to the bandit setting arises from the requirement of independence between every play and the noises injected from the recent \(H\) steps. We resolve this issue by adapting online learning with delay to the subroutine algorithm used in our BCO algorithm. Online learning with delay was introduced by Quanrud and Khashabi (2015). In particular, Flaspohler et al. (2021) relates online learning with delay to online learning with optimism and established a sublinear regret guarantee for mirror descent algorithms. A similar delay scheme was seen in Gradu et al. (2020).

### Notations and organization

Notation.For convenience and succinctness, we denote \(\bar{H}\stackrel{{\text{def}}}{{=}}H-1\). We use lowercase bold letters (e.g. \(\mathbf{x},\mathbf{y},\mathbf{u},\mathbf{w},\mathbf{e}\)) to denote the states, observations, controls, and noises of the dynamical system 2, and \(d_{\mathbf{x}},d_{\mathbf{u}},d_{\mathbf{y}}\) to denote their corresponding dimensions. We use \(S^{n-1}\) to denote the unit sphere in \(\mathbb{R}^{n}\), as \(S^{n-1}\cong\mathbb{R}^{n-1}\). \(\mathbb{R}_{+},\mathbb{R}_{++}\) denote all non-negative, positive real numbers, respectively. For a differentiable function \(F:(\mathbb{R}^{n})^{H}\to\mathbb{R}\), we denote the gradient of \(F\) with respect to its \(i\)th argument vector by \(\nabla_{i}F(\cdot)\). \(\rho(\cdot)\) acting on a square matrix measures the spectral radius of the matrix. For a sequence \(M=(M^{[i]})_{i\in I}\), we use \(\|M\|_{\ell_{1},\mathrm{op}}\) to denote the sum of the operator norm: \(\|M\|_{\ell_{1},\mathrm{op}}\stackrel{{\text{def}}}{{=}}\sum_{i \in I}\|M^{[i]}\|_{\mathrm{op}}\). \(\sigma(A)\) denotes the \(\sigma\)-algebra generated by a random variable \(A\). We use \(O(\cdot)\) to hide all universal constants, \(\tilde{O}(\cdot)\) to hide \(\mathrm{poly}(\log T)\) terms, and \(\mathcal{O}(\cdot)\) to hide all natural parameters.

Footnote 2: It shall be noted that unbolded \(x,y,u\) are used in the EBCO–M algorithm (Algorithm 1) and is unrelated to the linear dynamical system.

Organization.Our method has two main components: a novel algorithm for BCO with memory (EBCO-M), and its application to building a novel bandit perturbation controller (EBPC). Section 2 describes our problem setting. Section 3 gives EBCO-M and its near-optimal regret guarantee. Section 4 introduces EBPC and its regret guarantees to both known and unknown systems.

## 2 The Bandit LQG Problem

In this section we provide necessary background and describe and formalize the main problem of interest. We consider control of linear time-invariant dynamical systems of the form

\[\mathbf{x}_{t+1}=A\mathbf{x}_{t}+B\mathbf{u}_{t}+\mathbf{w}_{t}\,\ \mathbf{y}_{t}=C\mathbf{x}_{t}+\mathbf{e}_{t}\.\] (2.1)

with dynamics matrices \(A\in\mathbb{R}^{d_{\mathbf{x}}\times d_{\mathbf{x}}},B\in\mathbb{R}^{d_{ \mathbf{x}}\times d_{\mathbf{u}}},C\in\mathbb{R}^{d_{\mathbf{y}}\times d_{ \mathbf{x}}}\). Here, consistent with previous notations, \(\mathbf{x}_{t}\in\mathbb{R}^{d_{\mathbf{x}}}\) is the state of the system at time \(t\), \(\mathbf{u}_{t}\in\mathbb{R}^{d_{\mathbf{u}}}\) is the control applied at time \(t\), and \(\mathbf{w}_{t}\in\mathbb{R}^{d_{\mathbf{x}}},\mathbf{e}_{t}\in\mathbb{R}^{d_{ \mathbf{y}}}\) are the system and measurement perturbations. At each timestep, the learner may observe \(\mathbf{y}_{t}\in\mathbb{R}^{d_{\mathbf{y}}}\), which usually represents a possibly noisy projection of the state \(\mathbf{x}_{t}\) onto some (possibly and usually) low-dimensional space.

In the online bandit setting, the learner is asked to perform a control \(\mathbf{u}_{t}\) at time \(t\). After the control is performed, the adversary chooses a quadratic cost function \(c_{t}(\mathbf{y}_{t},\mathbf{u}_{t})\). The learner observes the scalar \(c_{t}(\mathbf{y}_{t},\mathbf{u}_{t})\in\mathbb{R}_{+}\) and the signal \(\mathbf{y}_{t}\), but no additional information about \(c_{t}(\cdot,\cdot)\), \(\mathbf{x}_{t}\), \(\mathbf{w}_{t}\), or \(\mathbf{e}_{t}\) is given. The goal of the learner is to minimize _expected regret_, where _regret_ against the controller class \(\Pi\) is defined as

\[\text{Regret}_{T}\stackrel{{\text{def}}}{{=}}\sum_{t=1}^{T}c_{t} (\mathbf{y}_{t},\mathbf{u}_{t})-\min_{\pi\in\Pi}\sum_{t=1}^{T}c_{t}(\mathbf{y} _{t}^{\pi},\mathbf{u}_{t}^{\pi})\;,\] (2.2)

where \(\mathbf{u}_{t}^{\pi}\) is the control exerted at time \(t\) by policy \(\pi\), and \(\mathbf{y}_{t}^{\pi}\) is the time-\(t\) observation that would have occurred against the same costs/noises if the control policy \(\pi\) were carried out from the beginning.

In controlling linear dynamical systems with partial observations, we often make use of the system's counterfactual signal had no controls been performed since the beginning of the instance:

**Definition 2.1** (Nature's \(\mathbf{y}\)).: _Nature's \(\mathbf{y}\) at time \(t\), denoted by \(\mathbf{y}_{t}^{\mathbf{nat}}\), is the signal that the system would have generated at time \(t\) under \(\mathbf{u}_{1:t}=0\). We may compute this as_

\[\mathbf{x}_{t+1}^{\mathbf{nat}}=A\mathbf{x}_{t}^{\mathbf{nat}}+\mathbf{w}_{t }\;,\;\;\;\mathbf{y}_{t}^{\mathbf{nat}}=C\mathbf{x}_{t}^{\mathbf{nat}},\]

_or, equivalently, \(\mathbf{y}_{t}^{\mathbf{nat}}=\mathbf{e}_{t}+\sum_{i=1}^{t-1}CA^{t-i-1} \mathbf{w}_{i}\)._

Critically, this may be calculated via the Markov operator:

**Definition 2.2** (Markov operator).: _The Markov operator \(G=[G^{[i]}]_{i\in\mathbb{N}}\) corresponding to a linear system parametrized by \((A,B,C)\) as in Eq.(2.1) is a sequence of matrices in \(\mathbb{R}^{d_{\mathbf{y}}\times d_{\mathbf{u}}}\) such that \(G^{[i]}\stackrel{{\text{def}}}{{=}}CA^{i-1}B,\;G^{[0]}\stackrel{{ \text{def}}}{{=}}\mathbf{0}_{d_{\mathbf{y}}\times d_{\mathbf{u}}}\)._

It follows immediately that \(\mathbf{y}_{t}^{\mathbf{nat}}\) may be computed from observations as \(\mathbf{y}_{t}^{\mathbf{nat}}=\mathbf{y}_{t}-\sum_{i=1}^{t}G^{[i]}\mathbf{u}_ {t-i}\).

### Assumptions

We impose four core assumptions on the problem:

**Assumption 2.3** (Stable system).: _We assume the system is stable: the spectral radius \(\rho(A)<1\)3._

Footnote 3: Although out of scope of this work, we refer the readers to Appendix G of Simchowitz et al. (2020) for a possible and expected extension of out results to the relaxed assumption of stability: the learner is given access to a stabilizing controller of the system.

This assumption has the following important consequence:

**Remark 2.4** (Decay of stable systems).: _That the system is stable implies that \(\exists P\succ\mathbf{0}_{d_{\mathbf{x}}\times d_{\mathbf{x}}}\), \(P\in\mathrm{Sym}(d_{\mathbf{x}})\) such that \(rP\succeq A^{\top}PA\) for some \(0\leq r<1\), and therefore \(\exists\kappa\) depending on \(\|B\|_{\mathrm{op}},\|C\|_{\mathrm{op}},\sigma_{\min}(P)\) such that \(\|G^{[i]}\|_{\mathrm{op}}\leq\kappa r^{i-1}\). Then with \(H=O(\log T)\), we can assume that \(\|G\|_{\ell_{1},\mathrm{op}}=\sum_{i=0}^{\infty}\|G^{[i]}\|_{\mathrm{op}}\leq R_ {G}\) and \(\psi_{G}(H)\stackrel{{\text{def}}}{{=}}\sum_{i=H}^{\infty}\|G^{[i] }\|_{\mathrm{op}}\leq\frac{R_{G}}{T}\)._

Together, Assumption 2.3 and its consequence Remark 2.4 allows modeling the nonstochastic control problem as an online convex optimization problem against adversary with bounded memory, as the effect of past controls decays exponentially. Assumption 2.5-2.7 introduce the assumptions on the adversarial cost functions and perturbations.

**Assumption 2.5** (Noise model).: _The perturbations \(\{\mathbf{w}_{t},\mathbf{e}_{t}\}_{t=1}^{T}\) are assumed to be semi-adversarial: \(\mathbf{w}_{t},\mathbf{e}_{t}\) decompose as sums of adversarial and stochastic components \(\mathbf{w}_{t}=\mathbf{w}_{t}^{\mathrm{adv}}+\mathbf{w}_{t}^{\mathrm{stoch}}\) and \(\mathbf{e}_{t}=\mathbf{e}_{t}^{\mathrm{adv}}+\mathbf{e}_{t}^{\mathrm{stoch}}\). The stochastic components of the perturbations are assumed to come from distributions satisfying \(\mathbb{E}\!\left[\mathbf{w}_{t}^{\mathrm{stoch}}\right]=\mathbb{E}\!\left[ \mathbf{e}_{t}^{\mathrm{stoch}}\right]=0\), \(\mathbb{E}\left[\mathbf{w}_{t}^{\mathrm{stoch}}\mathbf{w}_{t}^{\mathrm{stoch} \top}\right]\succeq\sigma_{\mathbf{w}}^{2}I\), \(\mathbb{E}\left[\mathbf{e}_{t}^{\mathrm{stoch}}\mathbf{e}_{t}^{\mathrm{stoch} \top}\right]\succeq\sigma_{\mathbf{e}}^{2}I\), \(\sigma_{\mathbf{e}}>0\). \(\{\mathbf{w}_{t},\mathbf{e}_{t}\}_{t=1}^{T}\) are bounded such that \(\|\mathbf{y}_{t}^{\mathbf{nat}}\|_{2}\leq R_{\mathrm{nat}}\), \(\forall t\), for some parameter \(R_{\mathrm{nat}}\)._The bound on \(\mathbf{y^{nat}}\) is implied by bounded noise, which is a standard assumption in literature, and the stability of the system. The semi-adversarial assumption is also seen in prior work (Simchowitz et al., 2020), and is a necessary condition for our analysis: we depend on the regret guarantee of a bandit online convex optimization with memory algorithm which requires the strong convexity of the expected loss functions conditioned on all but the \(\Theta(\mathrm{poly}(\log T))\) most recent steps of history. This assumption is essentially equivalent to the adversarial assumption in applications: in almost all systems, noise is either endemic or may be injected. We also emphasize that this assumption is much weaker than that of previous optimal-rate work: even in the known-state, known-dynamic case, the previous optimal guarantee in the bandit setting depended on _no_ adversarial perturbation (see Table 1).

**Assumption 2.6** (Cost model).: _The cost functions \(c_{t}(\cdot,\cdot)\) are assumed to be quadratic, \(\sigma_{c}\)-strongly convex, \(\beta_{c}\)-smooth, i.e. \(c_{t}(\mathbf{y},\mathbf{u})=\mathbf{y}^{\top}\mathcal{O}_{t}\mathbf{y}+ \mathbf{u}^{\top}R_{t}\mathbf{u}\) with \(\beta_{c}I\succeq Q_{t}\succeq\sigma_{c}I,\beta_{c}I\succeq R_{t}\succeq \sigma_{c}I\ \forall t\). They are also assumed to obey the following Lipschitz condition: \(\forall(\mathbf{y},\mathbf{u}),(\mathbf{y}^{\prime},\mathbf{u}^{\prime})\in \mathbb{R}^{d\mathbf{y}+d_{u}}\),_

\[|c_{t}(\mathbf{y},\mathbf{u})-c_{t}(\mathbf{y}^{\prime},\mathbf{u}^{\prime})| \leq L_{c}(\|(\mathbf{y},\mathbf{u})\|_{2}\vee\|(\mathbf{y}^{\prime},\mathbf{ u}^{\prime})\|_{2})\|(\mathbf{y}-\mathbf{y}^{\prime},\mathbf{u}-\mathbf{u}^{ \prime})\|_{2}.\] (2.3)

These conditions are relatively standard for bandit convex optimization algorithms, and are needed for the novel BCO-with-memory algorithm which underpins our control algorithm.

**Assumption 2.7** (Adversary).: \(\{c_{t}(\cdot,\cdot),\mathbf{w}_{t}^{\mathrm{adv}},\mathbf{e}_{t}^{\mathrm{adv }}\}_{t=1}^{T}\) _is chosen by the adversary ahead of time._

### Disturbance Response Controllers

Regret compares the excess cost from executing our proposed control algorithm with respect to the cost of the best algorithm in hindsight from a given policy class. In particular, low regret against a rich policy class is a very strong near-optimality guarantee. We take the comparator policy class \(\Pi\) to be the set of disturbance response controllers (DRC), formally given by the following definition.

**Definition 2.8** (Disturbance Response Controllers).: _The disturbance response controller (DRC) and the DRC policy class are defined as:_

* \(A\) _disturbance response controller (DRC)_ _is a policy_ \(\pi_{M}\) _parametrized by_ \(M=(M^{[j]})_{j=0}^{\tilde{H}}\)_, where_ \(H\in\mathbb{Z}_{++}\)_, a sequence of_ \(H\) _matrices in_ \(\mathbb{R}^{d_{u}\times d_{y}}\) _s.t. the control at time_ \(t\) _given by_ \(\pi_{M}\) _is_ \(\mathbf{u}_{t}^{\pi_{M}}=\sum_{j=0}^{\tilde{H}}M^{[j]}\mathbf{y}_{t-j}^{ \mathbf{nat}}\)_. We shorthand_ \(\mathbf{u}_{t}^{M\text{ def }}=\mathbf{u}_{t}^{\pi_{M}}\)_._
* _The_ _DRC policy class_ _parametrized by_ \(H\in\mathbb{Z}_{++}\)_,_ \(R\in\mathbb{R}_{+}\) _is the set of all disturbance response controller with bounded length_ \(H\) _and norm_ \(R\)_:_ \(\mathcal{M}(H,R)=\{M=(M^{[j]})_{j=0}^{\tilde{H}}\mid\|M\|_{\ell_{1},\mathrm{op }}=\sum_{j=0}^{\tilde{H}}\|M^{[j]}\|_{\mathrm{op}}\leq R\}\)_._

Previous works have demonstrated the richness of the DRC policy class. In particular, Theorem 1 from Simchowitz et al. (2020) has established that the DRC policy class generalizes the state-of-art benchmark class of stabilizing linear dynamic controllers (LDC) with error \(e^{-\Theta(H)}\).

### Approach and Technical Challenges

The classical approach in online nonstochastic control of stable/stabilizable systems is to reduce to a problem of online convex optimization with memory. This insight relies on the exponentially decaying effect of past states and controls on the present, which allows approximating the cost functions as functions of the most recent controls.

A core technical challenge lies in the bandit convex optimization problem obtained from the bandit control problem. In the bandit setting, no gradient information is given to the learner, and thus the learner needs to construct a low-bias gradient estimator. Previous work uses the classical spherical gradient estimator proposed by Flaxman et al. (2004), but the regret guarantee is suboptimal. We would like to leverage the ellipsoidal gradient estimator proposed by Hazan and Levy (2014). However, when extending to loss functions with memory, there is no clear mechanism for obtaining a low-bias bound for general convex functions. We exploit the quadratic structure of the LQR/LQG cost functions to build EBCO-M (Algorithm 1), which uses ellipsoidal gradient estimators. We note that even outside of the control applications, EBCO-M may be of independent interests in bandit online learning theory.

BCO with Memory: Quadratic and Strongly Convex Functions

As with previous works, our control algorithm will depend crucially on a generic algorithm for bandit convex optimization with memory (BCO-M). We present a new online bandit convex optimization with memory algorithm that explores the structure of quadratic costs to achieve near-optimal regret.

### Setting and working assumptions

In the BCO-M setting with memory length \(H\), we consider an algorithm playing against an adversary. At time \(t\), the algorithm is asked to play its choice of \(y_{t}\) in the convex constraint set \(\mathcal{K}\). The adversary chooses a loss function \(F_{t}:\mathcal{K}^{H}\rightarrow\mathbb{R}_{+}\) which takes as input the algorithm's current play as well as its previous \(\bar{H}\) plays. The algorithm then observes a cost \(F_{t}(y_{t-\bar{H}},\ldots,y_{t})\) (and no other information about \(F_{t}(\cdot)\)) before it chooses and plays the next action \(y_{t+1}\). The goal is to minimize regret with respect to the expected loss, which is the excessive loss incurred by the algorithm compared to the best fixed decision in \(\mathcal{K}\):

\[\text{Regret}_{T}\stackrel{{\text{def}}}{{=}}\sum_{t=H}^{T} \mathbb{E}[F_{t}(y_{t-\bar{H}},\ldots,y_{t})]-\min_{x\in\mathcal{K}}\sum_{t=H} ^{T}\mathbb{E}[F_{t}(x,\ldots,x)].\]

For notation convenience, we will at times shorthand \(y_{t-\bar{H}:t}\stackrel{{\text{def}}}{{=}}(y_{t-\bar{H}},\ldots,y_{t})\in\mathcal{K}^{H}\).

#### 3.1.1 Bco-M assumptions

We make the following assumptions on the loss functions \(\{F_{t}\}_{t=H}^{T}\) and the constraint set \(\mathcal{K}\).

**Assumption 3.1** (Constraint set).: \(\mathcal{K}\) _is convex, closed, and bounded with non-empty interior. \(\text{diam}(\mathcal{K})=\sup_{z,z^{\prime}\in\mathcal{K}}\|z-z^{\prime}\|_{2 }\leq D\)._

**Assumption 3.2** (Loss functions).: _The loss functions chosen by the adversary obeys the following regularity and curvature assumptions:_

* \(F_{t}:\mathcal{K}^{H}\to R_{+}\) _is quadratic and_ \(\beta\)_-smooth:_
* _Quadratic:_ \(\exists W_{t}\in\mathbb{R}^{nH\times nH},b_{t}\in\mathbb{R}^{nH},c_{t}\in \mathbb{R}\) _such that_ \(F_{t}(w)=w^{\top}W_{t}w+b_{t}^{\top}w+c_{t}\)_,_ \(\forall w\in\mathcal{K}^{H}\)_._
* _Smooth:_ \(W_{t}\preceq\beta I_{nH\times nH}\)_._
* \(F_{t}:\mathcal{K}^{H}\rightarrow\mathbb{R}_{+}\) _is_ \(\sigma\)_-strongly convex in its induced unary form:_ \(f_{t}:\mathcal{K}\rightarrow\mathbb{R}_{+}\) _with_ \(f_{t}(z)=F_{t}(z,\ldots,z)\) _is_ \(\sigma\)_-strongly convex, i.e._ \(f_{t}(z)\geq f_{t}(z^{\prime})+\nabla f_{t}(z^{\prime})^{\top}(z-z^{\prime})+ \frac{\sigma}{2}\|z-z^{\prime}\|_{2}^{2}\)_,_ \(\forall z,z^{\prime}\in\mathcal{K}\)_._
* \(F_{t}\) _satisfies the following diameter and gradient bound on_ \(\mathcal{K}\)_:_ \(\exists B,L>0\) _such that_ \[B=\sup_{w,w^{\prime}\in\mathcal{K}^{H}}|F_{t}(w)-F_{t}(w^{\prime})|,\ \ L=\sup_{w\in\mathcal{K}^{H}}\|\nabla F_{t}(w)\|_{2}.\]

In the online control problems, when formulating the cost function \(c_{t}\) as a function \(F_{t}\) of the most recent \(H\) controls played, the function \(F_{t}\) itself may depend on the entire history of the algorithm through step \(t-H\). Therefore, it is essential to analyze the regret guarantee of our BCO-M algorithm when playing against an adversary that can be \((t-H)\)-adaptive, giving rise to the following assumption.

**Assumption 3.3** (Adversarial adaptivity).: _The adversary chooses \(F_{t}\) independently of the noise \(u_{t-\bar{H}:t}\) which is drawn by the algorithm in the \(H\) most recent steps, but possibly not independently of earlier noises._

Note that Assumption 3.3 is minimal for BCO: if this fails, then in the subcase of a delayed loss, the adversary may fully control the agent's observations, resulting in no possibility of learning.

Self-concordant barrier.The algorithm makes use of a _self-concordant barrier_\(R(\cdot)\) of \(\mathcal{K}\) as the regularization function in the updates.

**Definition 3.4** (Self-concordant barrier).: _A three-time continuously differentiable function \(R(\cdot)\) over a closed convex set \(\mathcal{K}\subset\mathbb{R}^{n}\) with non-empty interior is a \(\nu\)-self-concordant barrier of \(\mathcal{K}\) if it satisfies the following two properties:_

1. _(Boundary property) For any sequence_ \(\{x_{n}\}_{n\in\mathbb{N}}\subset\mathrm{int}(\mathcal{K})\) _such that_ \(\lim_{n\to\infty}x_{n}=x\in\partial\mathcal{K}\)_,_ \(\lim_{n\to\infty}R(x_{n})=\infty\)_._
2. _(Self-concordant)_ \(\forall x\in\mathrm{int}(\mathcal{K})\)_,_ \(h\in\mathbb{R}^{n}\)_,_ 1. \(|\nabla^{3}R(x)[h,h,h]|\leq 2|\nabla^{2}R(x)[h,h]|^{3/2}\)_._ 2. \(|\langle\nabla R(x),h\rangle|\leq\sqrt{\nu}|\nabla^{2}R(x)[h,h]|^{1/2}\)_._

### Algorithm specification and regret guarantee

We present EBCO-M (Algorithm 1) for online bandit convex optimization with memory. The key novelty is the use of an ellipsoidal gradient estimator. It is difficult to establish a low-bias guarantee for ellipsoidal gradient estimator for general convex loss functions. However, thanks to the quadratic structure of the loss functions in LQR/LQG problems, we can show provable low bias for the ellipsoidal gradient estimator, and therefore achieve optimal regret.

```
1:Input: Convex, closed set \(\mathcal{K}\subseteq\mathbb{R}^{n}\) with non-empty interior, time horizon \(T\), memory length \(\bar{H}\), step size \(\eta\), \(\nu\)-self-concordant barrier \(R(\cdot)\) over \(\mathcal{K}\), convexity strength parameter \(\sigma\).
2:Initialize \(x_{t}=\arg\min_{x\in\mathcal{K}}R(x)\), \(\forall t=1,\ldots,H\).
3:Compute \(A_{t}=(\nabla^{2}R(x_{t})+\eta\sigma t\bar{H})^{-1/2}\), \(\forall t=1,\ldots,H\).
4:Sample \(u_{1},\ldots,u_{H}\sim S^{n-1}\) i.i.d. uniformly at random.
5:Set \(y_{t}=x_{t}+A_{t}u_{t},\forall t=1,\ldots,H\).
6:Set \(g_{t}=0,\forall t=1,\ldots,\bar{H}\).
7:Play \(y_{1},\ldots,y_{\bar{H}}\).
8:for\(t=H,\ldots,T\)do
9:Play \(y_{t}\), suffer loss \(F_{t}(y_{t-\bar{H}:t})\).
10:Store \(g_{t}=nF_{t}(y_{t-\bar{H}:t})\sum_{i=0}^{\bar{H}}A_{t-i}^{-1}u_{t-i}\).
11:Set \(x_{t+1}=\arg\min_{x\in\mathcal{K}}\sum_{s=H}^{t}\left(g_{s-\bar{H}}^{-\bar{ \sigma}}x+\frac{\sigma}{2}\|x-x_{s-\bar{H}}\|^{2}\right)+\frac{1}{\eta}R(x)\).
12:Compute \(A_{t+1}=(\nabla^{2}R(x_{t+1})+\eta\sigma(t+1)I)^{-1/2}\).
13:Sample \(u_{t+1}\sim S^{n-1}\) uniformly at random.
14:Set \(y_{t+1}=x_{t+1}+A_{t+1}u_{t+1}\).
15:endfor ```

**Algorithm 1** Ellipsoidal BCO with memory (EBCO-M)

Before analyzing the regret, we first make note of two properties of Algorithm 1.

**Remark 3.5** (Delayed dependence).: _In Algorithm 1, \(x_{t}\) is independent of \(u_{t-\bar{H}:t}\), \(\forall t\), and therefore \(A_{t}\) is independent of \(u_{t-\bar{H}:t}\) as \(A_{t}\) is determined by \(x_{t}\)._

**Remark 3.6** (Correctness).: \(y_{t}\) _played by Algorithm 1 lies in \(\mathcal{K}\): \(\|y_{t}-x_{t}\|_{\nabla^{2}R(x_{t})}^{2}=\|A_{t}u_{t}\|_{\nabla^{2}R(x_{t})}^{ 2}\leq\|u_{t}\|_{2}^{2}=1\), and by Proposition C.1, the Dikin ellipsoid centered at \(x_{t}\) is contained in \(\mathcal{K}\)._

We give the following two theorems on regret guarantees of the EBCO-M algorithm. In particular, Theorem 3.6.B is a slight generalization of Theorem 3.6.A in its relaxation on the curvature assumption on the sequence of loss functions \(F_{H:T}\).

**Theorem 3.6.A** (EbcO-M regret with strong convexity).: _For any sequence of cost functions \(\{F_{t}\}_{t=H}^{T}\) satisfying Assumption 3.2, constraint set \(\mathcal{K}\) satisfying Assumption 3.1, adversary satisfying Assumption 3.3, and \(H=\mathrm{poly}\left(\log T\right)\), Algorithm 1 satisfies the regret bound_

\[\text{Regret}_{T}(\text{EBCO-M})=\sum_{t=H}^{T}\mathbb{E}[F_{t}(y_{t-\bar{H}:t })]-\min_{x\in\mathcal{K}}\sum_{t=H}^{T}\mathbb{E}[f_{t}(x)]\leq\tilde{\mathcal{ O}}\left(\frac{\beta n}{\sigma}\sqrt{T}\right),\]

_where the expectation is taken over the randomness of the exploration noises \(u_{1:T}\), with \(\tilde{\mathcal{O}}\) hiding all natural parameters (\(B,D,L\)) and logarithmic dependence on \(T\)._

**Theorem 3.6.B** (Ebco-M regret with conditional strong convexity).: _Suppose Algorithm 1 is run on \(\mathcal{K}\) satisfying Assumption 3.1 against an adversary satisfying Assumption 3.3 with a sequence of random cost functions \(\{F_{t}\}_{t=H}^{T}\) satisfying that_

1. \(F_{t}\) _is quadratic, convex,_ \(\beta\)_-smooth, has diameter bound_ \(B\) _and gradient bound_ \(L\) _on_ \(\mathcal{K}^{H}\)_._
2. \(\exists\) _filtration_ \(\mathcal{G}_{1:T}\) _such that_ \(u_{1:T}\) _is independent of_ \(\mathcal{G}_{T}\)_,_ \(F_{t}\in\sigma(\mathbf{u}_{1:t-H})\cup\mathcal{G}_{t}\)_, and_ \(F_{t}\) _is conditionally_ \(\sigma\)_-strongly convex in its induced unary form:_ \(\bar{f}_{t}(z)\stackrel{{\text{def}}}{{=}}\mathbb{E}[f_{t}(z)\mid u _{1:t-H},\mathcal{G}_{t-H}]\) _is_ \(\sigma\)_-strongly convex._

_Then, Algorithm 1 satisfies the same regret bound attained in Theorem 3.6.A, i.e._

\[\text{Regret}_{T}(\texttt{EBCO-M})=\sum_{t=H}^{T}\mathbb{E}[F_{t}(y_{t-\bar{H }:t})]-\min_{x\in\mathcal{K}}\sum_{t=H}^{T}\mathbb{E}[f_{t}(x)]\leq\tilde{ \mathcal{O}}\left(\frac{\beta n}{\sigma}\sqrt{T}\right),\]

_where the expectation is taken over the randomness of the exploration noises \(u_{1:T}\) and the random functions \(F_{H:T}\), with \(\tilde{\mathcal{O}}\) hiding all natural parameters \((B,D,L)\) and logarithmic dependence on \(T\)._

## 4 Bandit Controller: Known and Unknown Systems

We will now use our BCO-with-memory algorithm to find an optimal controller (as in Gradu et al. (2020)), arguing that regret in choice of controller transfers into the setting discussed in the previous section. We first consider the case where the system is known, and then reduce the unknown system case to the known system case.

### Known systems

Applying Algorithm 1 to predict controllers4 with losses given by control losses, we obtain Algorithm 2.

Footnote 4: Notation: while our controller \(M\) is typically a tensor, it should be thought of as the output vector of Algorithm 1. As such, the relevant vector and matrix operations in that algorithm will correspond to tensor operations here, and the notation reflects that correspondence. In particular, the inner product on line 11 is an all-dimension tensor dot product and \(A\) is a square “matrix” which acts on tensors of shape \((H,d_{\mathbf{u}},d_{\mathbf{y}})\).

```
1:Input: Time horizon \(T\), memory length \(H\), Markov operator \(G\). BCO-M parameters \(\sigma,\eta\). Self-concordant barrier \(R(\cdot)\) over \(\mathcal{M}(H,R)\subset\mathbb{R}^{H\times d_{\mathbf{u}}\times d_{\mathbf{y }}}\).
2:Initialize \(M_{1}=\cdots=M_{H}=\underset{M\in\mathcal{M}(H,R)}{\arg\min}\ R(M)\).
3:Compute \(A_{i}=(\nabla^{\!\!\rho}R(M_{i})+\eta\sigma tI)^{-1/2}\), \(\forall i=1,\ldots,H\).
4:Sample \(\varepsilon_{1},\ldots,\varepsilon_{H}\sim S^{H\times d_{\mathbf{u}}\times d_{ \mathbf{y}}-1}\) i.i.d. uniformly at random.
5:Set \(\widetilde{M}_{i}=M_{i}+\varepsilon_{i}\), \(\forall i=1,\ldots,H\). Set \(g_{i}=0\), \(\forall i=1,\ldots,\bar{H}\).
6:Play control \(\mathbf{u}_{i}=0\), incur cost \(c_{i}(\mathbf{y}_{i},\mathbf{u}_{i})\), \(\forall i=1,\ldots,\bar{H}\).
7:for\(t=H,\ldots,T\)do
8:Play control \(\mathbf{u}_{t}=\mathbf{u}_{t}(\widetilde{M}_{t})=\sum_{i=0}^{\bar{H}} \widetilde{M}_{i}^{[i]}\mathbf{y}_{t-i}^{\text{nat}}\), incur cost \(c_{t}(\mathbf{y}_{t},\mathbf{u}_{t})\).
9:Observe \(\mathbf{y}_{t+1}\) and compute signal \(\mathbf{y}_{t+1}^{\text{nat}}=\mathbf{y}_{t+1}-\sum_{i=1}^{t}G^{[i]}\mathbf{u }_{t-i}\).
10:Store \(g_{t}=d_{\mathbf{u}}d_{\mathbf{y}}HC_{t}(\mathbf{y}_{t},\mathbf{u}_{t})\sum_{ i=0}^{\bar{H}}A_{t-i}^{-1}\varepsilon_{t-i}\).
11:Update \(M_{t+1}=\underset{M\in\mathcal{M}(H,R)}{\arg\min}\sum_{s=H}\left(\langle g_{s- \bar{H}},M\rangle+\frac{\sigma}{2}\|M-M_{s-\bar{H}}\|^{2}\right)+\frac{1}{\eta} R(M)\).
12:Compute \(A_{t+1}=(\nabla^{\!\!\rho}R(M_{t+1})+\eta\sigma(t+1)I)^{-1/2}\).
13:Sample \(\varepsilon_{t+1}\sim S^{H\times d_{\mathbf{u}}\times d_{\mathbf{y}}-1}\) uniformly at random. Set \(\widetilde{M}_{t+1}=M_{t+1}+A_{t+1}\varepsilon_{t+1}\).
14:endfor ```

**Algorithm 2** Ellipsoidal Bandit Perturbation Controller (EBPC)

**Theorem 4.1** (Known system control regret).: _Consider a linear dynamical system governed by known dynamics \((A,B,C)\) and the interaction model with adversarially chosen cost functionsand perturbations satisfying Assumption 2.3, 2.5, 2.6, 2.7. Then running Algorithm 2 with \(H=\Theta(\mathrm{poly}(\log T))\), \(\sigma=\sigma_{c}\left(\sigma_{\mathbf{e}}^{2}+\sigma_{\mathbf{w}}\frac{\sigma_ {\mathrm{min}}(C)}{1+\|A\|_{\infty}^{\sigma_{\mathrm{p}}}}\right)\), and \(\eta=\Theta\left(\frac{1}{d_{\mathbf{u}}d_{\mathbf{y}}L_{c}H^{3}\sqrt{T}}\right)\) guarantees_

\[\mathbb{E}[\text{Regret}_{T}(\text{EBPC})]\leq\tilde{\mathcal{O}}\left(\frac{ \beta_{c}d_{\mathbf{u}}d_{\mathbf{y}}}{\sigma_{c}}\sqrt{T}\right),\]

_where regret is defined as in Eq.(2.2), the expectation is taken over the exploration noises \(\varepsilon_{1:T}\) of the algorithm as well as the stochastic components \(\{\mathbf{w}_{t}^{\mathrm{stoch}},\mathbf{e}_{t}^{\mathrm{stoch}}\}_{t=1}^{T}\) of the perturbations \(\{\mathbf{w}_{t},\mathbf{e}_{t}\}_{t=1}^{T}\), and \(\tilde{\mathcal{O}}(\cdot)\) hides all universal constants, natural parameters, and logarithmic dependence on \(T\)._

### Unknown systems: control after estimation

Note that EBPC (Algorithm 2) relies on the access to the system's Markov operator \(G\), which is available if and only if the system dynamics \((A,B,C)\) are known. When the system dynamics is unknown, we can identify the system using a system estimation algorithm, obtain an estimated Markov operator \(\hat{G}\), and run EBPC with \(G\leftarrow\hat{G}\). Algorithm 3 outlines the estimation method of system dynamics via least squares.

```
1:Input: estimation sample size \(N\), system length \(H\).
2:Initialize: \(\hat{G}^{[t]}=0\), \(\forall t\geq H\).
3:Sample and play \(\mathbf{u}_{t}\sim N(0,I_{d_{\mathbf{u}}\times d_{\mathbf{u}}})\), \(\forall t=1,\ldots,N\).
4:Set \(\hat{G}^{[0:\bar{H}]}=\arg\min\sum_{t=H}^{N}\|\mathbf{y}_{t}-\sum_{i=0}^{\bar {H}}\hat{G}^{[i]}\mathbf{u}_{t-i}\|_{2}^{2}\).
5:Return \(\hat{G}\). ```

**Algorithm 3** System estimation via least squares (SysEst-LS)

**Theorem 4.2** (Unknown system control regret).: _Consider a linear dynamical system governed by unknown dynamics \((A,B,C)\) and the interaction model with adversarially chosen cost functions and perturbations satisfying Assumption 2.3, 2.5, 2.6, 2.7. Suppose we obtain an estimated Markov operator \(\hat{G}\) from Algorithm 3 with \(N=\lceil\sqrt{T}\rceil\) and \(H=\Theta(\mathrm{poly}\log T)\). Then Algorithm 2 with \(G\leftarrow\hat{G}\), \(H\gets 3H\), \(\sigma=\frac{1}{8}\sigma_{c}\sigma_{\mathbf{e}}^{2}\), and \(\eta=\Theta\left(\frac{1}{d_{\mathbf{u}}d_{\mathbf{y}}L_{c}H^{3}\sqrt{T}}\right)\) guarantees_

\[\mathbb{E}[\text{Regret}_{T}(\text{EBPC})]\leq\tilde{\mathcal{O}}\left(\frac{ \beta_{c}d_{\mathbf{u}}d_{\mathbf{y}}}{\sigma_{c}}\sqrt{T}\right),\]

_where regret is defined as in Eq.(2.2), the expectation is taken over the exploration noises \(\varepsilon_{1:T}\) in Algorithm 2, the sampled Gaussian controls \(\mathbf{u}_{1:N}\) in Algorithm 3, and the stochastic components \(\{\mathbf{w}_{t}^{\mathrm{stoch}},\mathbf{e}_{t}^{\mathrm{stoch}}\}_{t=1}^{T}\) of the perturbations \(\{\mathbf{w}_{t},\mathbf{e}_{t}\}_{t=1}^{T}\), and \(\tilde{\mathcal{O}}(\cdot)\) hides all universal constants, natural parameters, and logarithmic dependence on \(T\)._

## 5 Discussion and conclusion

We solve the open problem put forth by Gradu et al. (2020) on the optimal rate for online bandit control for the case of LQR/LQG control, improving to regret \(\tilde{O}(\sqrt{T})\) from \(\tilde{O}(T^{\frac{3}{4}})\) in the semi-adversarial noise model and for strongly convex LQR/LQG cost functions. Our method builds upon recent advancements in bandit convex optimization for quadratic functions, providing the first near-optimal regret algorithm for bandit convex optimization with memory in a nonstochastic setting.

It would be interesting to investigate (1) whether the results can be extended to fully adversarial noise, (2) whether a similar stable controller recovery as seen in (Chen and Hazan, 2021) for fully observable systems can be established for partially observable systems, and whether that can be incorporated to extend our result to stabilizable systems even without access to a stabilizing controller.

## 6 Acknowledgement

Elad Hazan acknowledges funding from the Office of Naval Research grant N000142312156, the NSF award 2134040, and Open Philanthropy.

## References

* Abbasi-Yadkori and Szepesvari (2011) Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear quadratic systems. In _Proceedings of the 24th Annual Conference on Learning Theory_, pages 1-26. JMLR Workshop and Conference Proceedings, 2011.
* Abernethy et al. (2008) Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In _21st Annual Conference on Learning Theory, COLT 2008_, 2008.
* Agarwal et al. (2019a) Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control with adversarial disturbances. In _International Conference on Machine Learning_, pages 111-119. PMLR, 2019a.
* Agarwal et al. (2019b) Naman Agarwal, Elad Hazan, and Karan Singh. Logarithmic regret for online control. _Advances in Neural Information Processing Systems_, 32, 2019b.
* Anava et al. (2015) Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: price of past mistakes. _Advances in Neural Information Processing Systems_, 28, 2015.
* Bellman (1954) Richard Bellman. The theory of dynamic programming. _Bulletin of the American Mathematical Society_, 60(6):503-515, 1954.
* Cassel and Koren (2020) Asaf Cassel and Tomer Koren. Bandit linear control. _Advances in Neural Information Processing Systems_, 33:8872-8882, 2020.
* Chen and Hazan (2021) Xinyi Chen and Elad Hazan. Black-box control for linear dynamical systems. In _Conference on Learning Theory_, pages 1114-1143. PMLR, 2021.
* Chen and Hazan (2023) Xinyi Chen and Elad Hazan. A nonstochastic control approach to optimization. _arXiv preprint arXiv:2301.07902_, 2023.
* Cohen et al. (2018) Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control. In _International Conference on Machine Learning_, pages 1029-1038. PMLR, 2018.
* Dean et al. (2018) Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. _Advances in Neural Information Processing Systems_, 31, 2018.
* Flaspohler et al. (2021) Genevieve E Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu, Paulo Orenstein, and Lester Mackey. Online learning with optimism and delay. In _International Conference on Machine Learning_, pages 3363-3373. PMLR, 2021.
* Flaxman et al. (2004) Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. _arXiv preprint cs/0408007_, 2004.
* Flaxman et al. (2005) Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In _Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms_, pages 385-394, 2005.
* Foster and Simchowitz (2020) Dylan Foster and Max Simchowitz. Logarithmic regret for adversarial online control. In _International Conference on Machine Learning_, pages 3211-3221. PMLR, 2020.
* Ghai et al. (2022) Udaya Ghai, Xinyi Chen, Elad Hazan, and Alexandre Megretski. Robust online control with model misspecification. In _Learning for Dynamics and Control Conference_, pages 1163-1175. PMLR, 2022.
* Ghai et al. (2023) Udaya Ghai, Arushi Gupta, Wenhan Xia, Karan Singh, and Elad Hazan. Online nonstochastic model-free reinforcement learning. _arXiv preprint arXiv:2305.17552_, 2023.
* Gradu et al. (2020) Paula Gradu, John Hallman, and Elad Hazan. Non-stochastic control with bandit feedback. _Advances in Neural Information Processing Systems_, 33:10764-10774, 2020.
* Gradu et al. (2021) Paula Gradu, John Hallman, Daniel Suo, Alex Yu, Naman Agarwal, Udaya Ghai, Karan Singh, Cyril Zhang, Anirudha Majumdar, and Elad Hazan. Deluca-a differentiable control library: Environments, methods, and benchmarking. _arXiv preprint arXiv:2102.09968_, 2021.
* Ghaha et al. (2020)Elad Hazan. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* Hazan and Levy (2014) Elad Hazan and Kfir Levy. Bandit convex optimization: Towards tight bounds. _Advances in Neural Information Processing Systems_, 27, 2014.
* Hazan and Singh (2022) Elad Hazan and Karan Singh. Introduction to online nonstochastic control. _arXiv preprint arXiv:2211.09619_, 2022.
* Hazan et al. (2007) Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2):169-192, 2007.
* Ibrahimi et al. (2012) Morteza Ibrahimi, Adel Javanmard, and Benjamin Roy. Efficient reinforcement learning for high dimensional linear quadratic systems. _Advances in Neural Information Processing Systems_, 25, 2012.
* Kalman (1960) Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.
* Lale et al. (2021) Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Adaptive control and regret minimization in linear quadratic gaussian (lqg) setting. In _2021 American Control Conference (ACC)_, pages 2517-2522. IEEE, 2021.
* Quanrud and Khashabi (2015) Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. _Advances in neural information processing systems_, 28, 2015.
* Shamir (2013) Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In _Conference on Learning Theory_, pages 3-24. PMLR, 2013.
* Simchowitz et al. (2020) Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In _Conference on Learning Theory_, pages 3320-3436. PMLR, 2020.
* Zinkevich (2003) Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_, pages 928-936, 2003.