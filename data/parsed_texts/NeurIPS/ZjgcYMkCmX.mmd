# How does Inverse RL Scale to Large State Spaces?

A Provably Efficient Approach

Filippo Lazzati

Politecnico di Milano

Milan, Italy

filippo.lazzati@polimi.it

&Micro Mutti

Technion

Haifa, Israel

&Alberto Maria Metelli

Politecnico di Milano

Milan, Italy

###### Abstract

In online Inverse Reinforcement Learning (IRL), the learner can collect samples about the dynamics of the environment to improve its estimate of the reward function. Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the _feasible reward set_. However, none of the algorithms available in the literature can scale to problems with large state spaces. In this paper, we focus on the online IRL problem in Linear Markov Decision Processes (MDPs). We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large. As a consequence, we introduce the novel framework of _rewards compatibility_, which generalizes the notion of feasible set, and we develop CATY-IRL, a sample efficient algorithm whose complexity is independent of the cardinality of the state space in Linear MDPs. When restricted to the tabular setting, we demonstrate that CATY-IRL is minimax optimal up to logarithmic factors. As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound. Finally, we devise a unifying framework for IRL and RFE that may be of independent interest.

## 1 Introduction

Inverse Reinforcement Learning (IRL) is the problem of inferring the reward function given demonstrations of an optimal behavior, i.e., from an _expert_ agent. [49, 42]. Since its formulation, much of the research effort has been put into the design of efficient algorithms for solving the IRL problem [6, 4]. Indeed, the solution of the IRL problem opens the door to a variety of interesting applications, including Apprenticeship Learning (AL) [2, 1], reward design [16], interpretability of the expert's behavior [17], and transferability to new environments [15].

Nowadays, the factor that most negatively impacts the adoption of IRL solutions in real-world applications is the intrinsic _ill-posedness_ of its formulation. The IRL problem has been historically defined as the problem of recovering _the_ reward function underlying the demonstrations [49, 42], even though mere demonstrations can be equivalently explained by a _variety_ of rewards. In other words, the IRL problem is underconstrained, even in the limit of infinite demonstrations [42, 39].

To overcome this weakness and to come up with a _single_ reward function, three main approaches are commonly adopted in the literature. (\(i\)) The first approach consists of the use of a _heuristic_ to select a specific reward function from the set of all the rewards that explain the demonstrations. Implicitly, these works re-define IRL as the problem of recovering _the_ reward function explaining the demonstrations _and_ complying with the heuristic. As an example, [42, 48] select the reward that maximizes some notion of margin, and [70] implicitly chooses the reward returned by the optimization algorithm among those that maximize the likelihood. However, these approaches may generate issues in applications [56, 15]. (\(ii\)) In the second approach, additional _constraints_ beyondmere demonstrations are enforced to guarantee the uniqueness of the reward function to recover. In "reward identifiability" works, the additional information commonly concerns some structure of the environment [25], or multiple demonstrations across various environments [5, 10]. In Reward Learning (ReL) works [19], demonstrations of optimal behavior are combined with other kinds of expert feedback, like comparisons [65]. (\(iii\)) As a third approach, recently, [39, 38] proposed the alternative formulation of IRL as the problem of recovering _all_ the reward functions compatible with the demonstrations, i.e., the _feasible reward set_. In this manner, we are not subject to the limitations of the first approach, and we do not depend on additional information like in the second approach.

In practical applications, the chosen IRL formulation has to be tackled by algorithms that use a _finite_ number of demonstrations and a limited knowledge of the dynamics of the environment. In the common _online_ IRL scenario, the learner explores the (unknown) environment, and exploits this additional information to improve its performance on the IRL task [e.g., 39, 33, 38, 68, 31]. On this basis, the IRL approach (\(iii\)) based on the _feasible set_[39, 38] displays desirable properties since "postpones" the choice of the heuristic and/or enforcement of additional constraints, with the advantage of analyzing the intrinsic complexity of the IRL problem only, without being obfuscated by other factors. In other words, this recent formulation of the IRL problem paves the way for the design and analysis of provably efficient IRL algorithms, endowed with solid theoretical guarantees.

However, the algorithms designed for learning the feasible set currently available in the literature [e.g., 39, 33, 38, 68, 31] struggle when attempting to scale them to IRL problems with _large state spaces_. This is apparent because their sample complexity exhibits an explicit dependence on the cardinality of the state space. This inevitably represents a major limitation since most real-world scenarios concern problems with large, or even continuous, state spaces [e.g., 15, 7, 40, 14].

In this context, function approximation represents an essential tool to tackle the curse of dimensionality and enforce generalization [54, 41]. Linear Markov Decision Processes (MDPs) [23, 67] offer a simple but powerful structure, in which we assume the reward function and the transition model can be expressed as linear combinations of known features, that permits theoretical analysis of the sample complexity. Even though many extensions have been developed [64, 22, 13], the Linear MDPs framework typically represents one of the first function approximation settings to analyze when focusing on a novel problem, before moving to more complex settings [e.g., 63, 61].

In this paper, we aim to shed light on the challenges of scaling the feasible reward set to large-scale problems. Motivated by its limitations when dealing with large state spaces, we introduce the novel _Rewards Compatibility_ framework. Being a generalization of the notion of feasible set, it allows us to define the new _IRL Classification Problem_, a fourth approach to cope with the ill-posedness of the IRL formulation. This permits the development of CATY-IRL (CompATibilitY for IRL), a provably efficient IRL algorithm for Linear MDPs characterized by large or even continuous state spaces.

**Original Contributions.** The main contributions of the current work can be summarized as follows:

* We prove that the notion of feasible set can _not_ be learned efficiently in MDPs with large/continuous state spaces, even under the structure enforced by Linear MDPs. Nevertheless, we show that this problem disappears under the _assumption_ that the expert's policy is known, by providing a sample efficient algorithm for such setting (Section 3).
* To overcome the need for knowing the expert's policy exactly, we propose _Rewards Compatibility_, a novel framework that formalizes the intuitive notion of _compatibility_ of a reward function with expert demonstrations. It generalizes the feasible set and allows us to define an original learning setting, _IRL classification_, based on a new formulation of IRL _classification_ task (Section 4).
* For the newly-devised framework, we develop CATY-IRL (CompATibilitY for IRL), a new sample and computationally efficient IRL algorithm for both tabular and Linear MDPs. Remarkably, this CATY-IRL does not require the additional assumption that the expert's policy is known (Section 5).
* In the tabular setting, we prove a tight minimax lower bound to the sample complexity of the IRL classification problem of \(\Omega\big{(}\frac{H^{3}SA}{\epsilon^{2}}(S+\log\frac{1}{\delta})\big{)}\) episodes, where \(S\) and \(A\) are the cardinalities of the state and action spaces, \(H\) is the horizon, \(\epsilon\) the accuracy and \(\delta\) the failure probability. This bound is _matched_ by CATY-IRL, up to logarithmic factors. Exploiting a similar construction, we show that a lower bound with the same rate holds also for the Reward-Free Exploration (RFE) problem, improving by an \(H\) factor over the RFE state-of-the-art lower bound [21] (Section 6.1).
* Finally, we formulate a novel _Objective-Free Exploration_ (OFE) setting that isolates the challenges of exploration beyond Reinforcement Learning (RL), by unifying RFE and IRL (Section 6.2).

Additional related works and the proofs of all the results are reported in Appendix A and B -E.

## 2 Preliminaries

Notation.Given an integer \(N\in\mathbb{N}\), we define \([\![N]\!]\coloneqq\{1,\ldots,N\}\). Given sets \(\mathcal{X}\) and \(\mathcal{Y}\), we denote \(\mathcal{H}_{d}(\mathcal{X},\mathcal{Y})\coloneqq\max\{\sup_{x\in\mathcal{X}} \inf_{y\in\mathcal{Y}}d(x,y),\sup_{y\in\mathcal{Y}}\inf_{x\in\mathcal{X}}d(y,x)\}\) their Hausdorff distance with inner distance \(d\). We denote by \(\Delta^{\mathcal{X}}\) the probability simplex over \(\mathcal{X}\), and by \(\Delta^{\mathcal{X}}_{\mathcal{Y}}\) the set of functions from \(\mathcal{Y}\) to \(\Delta^{\mathcal{X}}\). Sometimes, we denote the dot product between vectors \(x,y\) as \(\langle x,y\rangle\coloneqq x^{\intercal}y\). We employ \(\mathcal{O},\Omega,\Theta\) for the common asymptotic notation and \(\widehat{\mathcal{O}},\widehat{\Omega},\widehat{\Theta}\) to omit logarithmic terms.

Markov Decision Processes.A finite-horizon Markov Decision Process (MDP) without reward [45] is defined as a tuple \(\mathcal{M}\coloneqq(\mathcal{S},\mathcal{A},H,d_{0},p)\), where \(\mathcal{S}\) and \(\mathcal{A}\) are the measurable state and action spaces, \(H\in\mathbb{N}\) is the horizon, \(d_{0}\in\Delta^{\mathcal{S}}\) is the initial-state distribution, and \(p\in\mathcal{P}\coloneqq\Delta^{\mathcal{S}}_{\mathcal{S}\times\mathcal{A} \times[H]}\) is the transition model. Given a (deterministic) reward function \(r\in\mathfrak{R}\coloneqq[-1,1]^{\mathcal{S}\times\mathcal{A}\times[H]}\), we denote by \(\overline{\mathcal{M}}\coloneqq\mathcal{M}\cup\{r\}\) the MDP obtained by pairing \(\mathcal{M}\) and \(r\). Each policy \(\pi\in\Pi\coloneqq\Delta^{\mathcal{A}}_{\mathcal{S}\times[H]}\) induces in \(\overline{\mathcal{M}}\) a state-action probability distribution \(d^{p,\pi}\coloneqq\{d^{p,\pi}_{h}\}_{h\in[\![H]\!]}\) (we omit \(d_{0}\) for simplicity) that assigns, to each subset \(\mathcal{Z}\subseteq\mathcal{S}\times\mathcal{A}\), the probability of being in \(\mathcal{Z}\) at stage \(h\in[\![H]\!]\) when playing \(\pi\) in \(\overline{\mathcal{M}}\). We denote with \(\mathcal{S}^{p,\pi}_{h}\) the set of states supported by \(d^{p,\pi}_{h}\) for any action at stage \(h\), and with \(\mathcal{S}^{p,\pi}\) the disjoint union of sets \(\{\mathcal{S}^{p,\pi}_{h}\}_{h\in[\![H]\!]}\). The \(Q\)-function of policy \(\pi\) in MDP \(\overline{\mathcal{M}}\) is defined at every \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\) as \(Q^{\pi}_{h}(s,a;p,r)\coloneqq\mathbb{E}_{p,\pi}[\sum_{t=h}^{H}r_{t}(s_{t},a_{t })|s_{h}=s,a_{h}=a]\), and the optimal \(Q\)-function as \(Q^{\pi}_{h}(s,a;p,r)\coloneqq\sup_{\pi\in\Pi}Q^{\pi}_{h}(s,a;p,r)\), where the expectation \(\mathbb{E}_{p,\pi}\) is computed over the stochastic process generated by playing policy \(\pi\) in the MDP \(\overline{\mathcal{M}}\). Similarly, we define the \(V\)-function of policy \(\pi\) at \((s,h)\) as \(V^{\pi}_{h}(s;p,r)\coloneqq\mathbb{E}_{p,\pi}[\sum_{t=h}^{H}r_{t}(s_{t},a_{t })|s_{h}=s]\), and the optimal \(V\)-function as \(V^{\pi}_{h}(s;p,r)\coloneqq\sup_{\pi\in\Pi}V^{\pi}_{h}(s;p,r)\). We define the utility of \(\pi\) as \(J^{\pi}(r;p)\coloneqq\mathbb{E}_{s\sim d_{0}}[V^{\pi}_{1}(s;p,r)]\), and the optimal utility as \(J^{\pi}(r;p)\coloneqq\mathbb{E}_{s\sim d_{0}}[V^{\pi}_{1}(s;p,r)]\). A forward (sampling) model of the environment permits to collect samples starting from \(s\sim d_{0}\) and following some policy. A generative (sampling) model consists in an oracle that, given an arbitrary state-action-stage triple \(s,a,h\) in input, returns a sampled next state \(s^{\prime}\sim p_{h}(\cdot|s,a)\).

Linear MDPs.Based on [23], we say that an MDP \(\overline{\mathcal{M}}=(\mathcal{S},\mathcal{A},H,d_{0},p,r)\) is a _Linear MDP_ with a (known) feature map \(\phi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\), if for every \(h\in[\![H]\!]\), there exist \(d\in\mathbb{N}\) unknown (signed) measures \(\mu_{h}=[\mu^{1}_{h},\ldots,\mu^{d}_{h}]^{\intercal}\) over \(\mathcal{S}\) and an unknown vector \(\theta_{h}\in\mathbb{R}^{d}\), such that for every \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have \(p_{h}(\cdot|s,a)=\langle\phi(s,a),\mu_{h}(\cdot)\rangle\) and \(r_{h}(s,a)=\langle\phi(s,a),\theta_{h}\rangle\). Without loss of generality, we assume \(\|\phi(s,a)\|_{2}\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), and \(\max\{\|\theta_{h}\|_{2},\|\mu_{h}|(\mathcal{S})\|_{2}\}\leqslant\sqrt{d}\).1\(\mathcal{M}\) is a _Linear MDP without reward_ if its transition model satisfies the assumption described above.

Footnote 1: \(|\mu_{h}|(\mathcal{S})\) denotes the vector containing the variation of each measure \(\mu^{i}_{h}\) over the measurable set \(\mathcal{B}\).

BPI and RF.In both Best-Policy Identification (BPI) [37] and Reward-Free Exploration (RFE) [21], the learner has to explore the _unknown_ MDP to optimize a certain reward function. In BPI, the learner observes the reward function \(r\) during exploration, and its goal is to output a policy \(\widehat{\pi}\) such that, in the true MDP with transition model \(p\) we have \(\mathbb{P}\big{(}J^{\pi}(r;p)-J^{\pi}(r;p)\leqslant\epsilon\big{)}\geq 1-\delta\) for every \(\epsilon,\delta\in(0,1)\). RFE considers the setting in which the reward to optimize is revealed _a posteriori_ of the exploration phase. Thus the goal of the agent in RFE is to compute an estimate \(\widehat{p}\) of the true dynamics \(p\) so that \(\mathbb{P}\big{(}\sup_{r\in\mathfrak{R}}\{J^{\pi}(r;p)-J^{\widehat{\pi}_{r}}(r;p) \}\leqslant\epsilon\big{)}\geq 1-\delta\) for every \(\epsilon,\delta\in(0,1)\), where \(\widehat{\pi}_{r}\) is the optimal policy in the MDP with \(\widehat{p}\) as transition model and \(r\) as reward function.

Online IRL.We consider the online2 IRL setting [39, 68, 66, 53, 33] in which, similarly to the online AL setting [53, 66], we are given a dataset \(\mathcal{D}^{E}=\{(s^{i}_{1},a^{i}_{1},\ldots,s^{i}_{H-1},a^{i}_{H-1},s^{i}_{H}) \}_{i\in[\![^{\pi}\![\pi]\!]}\) of \(\tau^{E}\in\mathbb{N}\) trajectories collected by executing the expert's policy \(\pi^{E}\) in a certain (unknown) MDP \(\overline{\mathcal{M}}=\mathcal{M}\cup\{r^{E}\}\). We make the assumption that \(\pi^{E}\) is optimal under the true (unknown) reward \(r^{E}\) in \(\overline{\mathcal{M}}\). Since the dynamics of \(\overline{\mathcal{M}}\) is unknown, we are allowed to actively explore the environment through a _forward_ model to collect a new state-action dataset \(\mathcal{D}\). The goal is to use the latter and demonstrations in \(\mathcal{D}^{E}\) to estimate a reward function that makes the expert's policy \(\pi^{E}\) optimal. Sometimes, we will denote an IRL instance as \(\mathcal{M}\cup\{\pi^{E}\}\), and a Linear IRL instance with recovered reward \(r\) as an IRL instance in which \(\mathcal{M}\cup\{r\}\) is a Linear MDP.

## 3 Limitations of the Feasible Set

In this section, after having characterized the feasible set formulation in Linear MDPs, we show that it suffers from _statistical_ (and _computational_) inefficiency in problems with large state spaces, even under the Linear MDP assumption. We will provide a solution to these issues in Section 4.

**The Feasible Set.** According to the standard definition [e.g., 39; 33; 38; 68; 31], the feasible set contains the rewards that make the expert's policy \(\pi^{E}\) optimal, as defined below.

**Definition 3.1** (Feasible Set [31]).: _Let \(\mathcal{M}\) be an MDP without reward and let \(\pi^{E}\) be the expert's policy. The feasible set \(\mathcal{R}_{p,\pi^{E}}\) of rewards compatible with \(\pi^{E}\) in \(\mathcal{M}\) is defined as:_

\[\mathcal{R}_{p,\pi^{E}}:=\{r\in\mathfrak{R}\,|\,\,J^{\pi^{E}}(r;p)=J^{*}(r;p)\}.\]

Without function approximation, the feasible set contains a variety of rewards for any deterministic policy. In Linear MDPs, due to the feature map, the feasible set might exhibit some degeneracy.3 Definition 3.1 can be adapted to Linear MDPs with feature map \(\phi\) as: \(\mathcal{R}_{\phi,p,\pi^{E}}:=\{r\in\mathfrak{R}\,|\,J^{\pi^{E}}(r;p)=J^{*}(r ;p)\wedge\exists\theta:[\![H]\!]\to\mathbb{R}^{d},\forall(s,a,h)\in\mathcal{S} \times\mathcal{A}\times[\![H]\!]:\,r_{h}(s,a)=\langle\phi(s,a),\theta_{h} \rangle\}\). We omit \(\phi\) in \(\mathcal{R}_{\phi,p,\pi^{E}}\) for notational simplicity.

Footnote 3: We exemplify this proposition in Appendix B.1. In Appendix B.4 we generalize to infinite state spaces.

**Proposition 3.1**.: _Let \(\mathcal{M}\) be a Linear MDP without reward with a finite state space, and let \(\phi\) be a feature mapping. Let \(\{\Phi_{h}^{\pi^{E}}\}_{h\in[\![H]\!]}\) and \(\{\overline{\Phi}_{h}\}_{h\in[\![H]\!]}\) be the sets of expert's and non-expert's features, defined for every \(h\in[\![H]\!]\) as:_

\[\Phi_{h}^{\pi^{E}}:=\big{\{}\phi(s,a^{E})\,|\,s\in\mathcal{S}_{h}^{p,\pi^{E}}, \,a^{E}\in\mathcal{A}_{h}^{E}(s)\big{\}},\qquad\overline{\Phi}_{h}:=\big{\{} \phi(s,a)\,|\,s\in\mathcal{S}_{h}^{p,\pi^{E}},\,a\in\mathcal{A}\backslash \mathcal{A}_{h}^{E}(s)\big{\}},\]

_where \(\mathcal{A}_{h}^{E}(s):=\{a\in\mathcal{A}|\pi_{h}^{E}(\cdot|s)>0\}\) for every \(s\in\mathcal{S}\). If for none of the \(H\) pairs of sets \((\Phi_{h}^{\pi^{E}},\overline{\Phi}_{h})\) there exists a separating hyperplane, then \(\mathcal{R}_{p,\pi}=\{\overline{r}\}\), with \(\overline{r}_{h}(s,a)=0\,\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times \big{[}\![H]\!]\) i.e., the feasible set with linear rewards in \(\phi\) contains only the reward function that assigns zero reward everywhere._

Intuitively, expert's actions must have the largest optimal \(Q\)-value among all actions, and linearity imposes the "separability" requirement. The result holds also for MDPs with linear rewards only. We exemplify Proposition 3.1 in Appendix B.1.

**Learning the Feasible Set.** In order to highlight the challenges of learning the feasible set with large-scale MDPs, based on [38; 31], we devise the following PAC requirement.

**Definition 3.2** (PAC Algorithm).: _Let \(\epsilon,\delta\in(0,1)\), and let \(\mathfrak{A}\) be an algorithm that collects \(\tau^{E}\) samples about \(\pi^{E}\) using a generative model, and \(\tau\) episodes from a Linear MDP without reward \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\) using a forward model. Let \(\widehat{\mathcal{R}}\) be the estimate of the feasible set \(\mathcal{R}_{p,\pi^{E}}\) outputted by \(\mathfrak{A}\). Then, \(\mathfrak{A}\) is \((\epsilon,\delta)\)-PAC for IRL if \(\mathbb{P}_{\mathcal{M},\mathfrak{A}}\big{(}\mathcal{H}_{d}(\mathcal{R}_{p,\pi ^{E}},\widehat{\mathcal{R}})\leq\epsilon\big{)}\geq 1-\delta\), where \(\mathbb{P}_{\mathcal{M},\mathfrak{A}}\) is the probability measure induced by \(\mathfrak{A}\) in \(\mathcal{M}\), and \(d(r,\widehat{r})\sigma\varsigma\sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\mathbb{E} _{(s,a)\sim d_{h}^{p,\pi}(\cdot,\cdot)}\,|r_{h}(s,a)-\widehat{r}_{h}(s,a)|.\)4 The sample complexity is the pair \((\tau^{E},\tau)\)._

Footnote 4: For simplicity, we provide the full expression of distance \(d\) in Appendix B.4, Equation (1).

It is worth noting that in Definition 3.2, we are considering a generative model for collecting samples from the expert's policy, which represents the easiest learning scenario. The following result shows that, even in this convenient setting, estimating the feasible set is statistically inefficient.

**Theorem 3.2** (Statistical Inefficiency).: _Let \(\mathcal{M}\cup\{\pi^{E}\}\) be a Linear IRL instance with finite state space \(\mathcal{S}\) and deterministic expert's policy, and let \(\epsilon,\delta\in(0,1)\). If an algorithm \(\mathfrak{A}\) is \((\epsilon,\delta)\)-PAC, then \(\tau^{E}=\Omega(S)\), where \(S:=|\mathcal{S}|\) is the cardinality of the state space._

In other words, even under the easiest learning conditions (i.e., generative model and deterministic expert), the sample complexity scales directly with the cardinality of the state space \(S\), thus, it is infeasible when \(S\) is large or even infinite. Observe that this result extends to any class of MDPs that contains Linear MDPs. In Appendix B.2, we analyze if additional assumptions can drop the \(\Omega(S)\) dependence. Nevertheless, if \(\pi^{E}\) is _known_, it is possible to construct sample efficient algorithms. Algorithm 1 (whose pseudocode is presented in Appendix B.3), under the assumption that \(\pi^{E}\) is known, makes use of an inner RFE routine (Algorithm 1 of [62]) to recover the feasible set.

**Theorem 3.3**.: _Assume that \(\pi^{E}\) (along with its support \(\mathcal{S}^{p,\pi^{E}}\)) is known. Then, for any \(\epsilon,\delta\in(0,1)\), Algorithm 1 is \((\epsilon,\delta)\)-PAC for IRL with a number of episodes \(\tau\) upper bounded by:_

\[\tau\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{5}d}{\epsilon^{2}}\Big{(}d+ \log\frac{1}{\delta}\Big{)}\Big{)}.\]

**Limitations of the Feasible Set.** We can now conclude that the feasible set suffers from two main limitations. \((i)\)_Sample Inefficiency_: If \(\pi^{E}\) is unknown, it requires a number of samples that depends on the cardinality of the state space (Theorem 3.2). \((ii)\)_Lack of Practical Implementability_: It contains a continuum of rewards, thus, no practical algorithm can explicitly compute it. We will discuss in the next section how to overcome both these issues.

## 4 Rewards Compatibility

In this section, we present the main contribution of this work: _Rewards Compatibility_, a novel framework for IRL that allows us to conveniently rephrase the learning from demonstrations problem as a classification task. We anticipate that the presentation of the framework is completely general and independent of structural assumptions of the MDP (e.g., Linear MDP).

### Compatible Rewards

In the following, for ease of presentation, we consider the exact setting, i.e., when \(d_{0}\), \(p\), and \(\pi^{E}\) are known. In addition, we will drop the dependence on \(p\) when clear from the context.

In IRL, an expert agent demonstrates policy \(\pi^{E}\) assumed optimal under some (unknown) reward function \(r^{E}\), i.e., \(J^{\ast}(r^{E})=J^{\pi^{E}}(r^{E})\). The task is to recover a reward \(r\) such that \(J^{\ast}(r)=J^{\pi^{E}}(r)\). By definition, IRL tells us that \(r^{E}\) makes the demonstrated policy \(\pi^{E}\) optimal, but what about other policies? We _do not_ and _cannot_ know. Since there are (infinite) rewards making \(\pi^{E}\) optimal (but they differ in the performance attributed to other policies) we realize that there are many rewards equally "compatible" with \(\pi^{E}\).5 Clearly, wih no additional information, we are unable to identify \(r^{E}\).

Footnote 5: See Appendix C.1 for a visual intuition.

The feasible set considers only these rewards, i.e., \(r\in\mathfrak{R}\) for which \(J^{\ast}(r)=J^{\pi^{E}}(r)\), and it refuses all the others. This can be interpreted as the feasible set carrying out a _classification_ of rewards based on a "hard" notion of _compatibility_ with demonstrations. In other words, rewards \(r\) satisfying condition \(J^{\ast}(r)=J^{\pi^{E}}(r)\) are compatible with \(\pi^{E}\), and the others are not. Nevertheless, our insight is that some rewards are _"more" compatible_ with \(\pi^{E}\) than others.

**Example 4.1**.: _Consider an MDP with one state and \(H=1\) in which the expert has three actions: Eating a muffin (M), a cake (C), or some (bad) vegetable soup (S). The true reward \(r^{E}\) assigns \(r^{E}(M)=+1,r^{E}(C)=+0.99\) and \(r^{E}(S)=-1\), i.e., the expert has a (weak) preference for the muffin over the cake, while she hates the soup; thus, she will demonstrate \(\pi^{E}=M\). Let \(r_{g},r_{b}\) be:_

\[r_{g}(M)=+0.99,r_{g}(C)=+1,r_{g}(S)=-1,\qquad r_{b}(M)=-1,r_{b}(C)=-1,r_{b}(S )=+1.\]

_Intuitively, \(r_{g}\) is "more" compatible with \(\pi^{E}\) than \(r_{b}\), because it establishes that M and C are much better than S, while reward \(r_{b}\) reverses the preferences. Clearly, we make a small error if we model the preferences of the expert with \(r_{g}\) instead of the true reward \(r^{E}\). However, the notion of feasible set is completely blind to the difference between \(r_{g}\) and \(r_{b}\) at modeling \(r^{E}\), and it refuses both of them._

We propose the following "soft" definition of (non)compatibility to capture this intuition.6

Footnote 6: In Appendix C.2, a _multiplicative_ alternative definition is presented.

**Definition 4.1** (Rewards (non)Compatibility).: _Let \(\mathcal{M}\cup\{\pi^{E}\}\) be an IRL instance, and let \(r\in\mathfrak{R}\) be any reward. We define the (non)compatibility \(\overline{\mathcal{C}}_{p,\pi^{E}}:\mathfrak{R}\rightarrow\mathbb{R}_{\geq 0}\) of reward \(r\) w.r.t. \(\mathcal{M}\cup\{\pi^{E}\}\) as:_

\[\overline{\mathcal{C}}_{p,\pi^{E}}(r)\coloneqq J^{\ast}(r;p)-J^{\pi^{E}}(r;p).\]In words, the (non)compatibility of reward \(r\) w.r.t. policy \(\pi^{E}\) in problem \(\mathcal{M}\) quantifies the _suboptimality_ of \(\pi^{E}\) in the MDP \(\mathcal{M}\cup\{r\}\). By definition, rewards \(r\) belonging to the feasible set (i.e., \(r\in\mathcal{R}_{p,\pi^{E}}\)) satisfy \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)=0\), i.e., they have zero non-compatibility with \(\pi^{E}\) in \(\mathcal{M}\).7

Footnote 7: We use _(non)compatibility_ since a reward \(r\in\mathfrak{R}\) is maximally compatible when \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)=0\). Thus, the larger \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)\), the more \(r\) is non-compatible. In this sense, \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)\) quantifies the non-compatibility of \(r\).

**Example 4.1** (Continued).: _(Non)compatibility discriminates between \(r_{g}\) and \(r_{b}\). Indeed, we have that \(\overline{\mathcal{C}}_{p,\pi^{E}}(r^{E})=0\), \(\overline{\mathcal{C}}_{p,\pi^{E}}(r_{g})=0.01\), and \(\overline{\mathcal{C}}_{p,\pi^{E}}(r_{b})=2\). In words, reward \(r_{g}\) suffers from very small (non)compatibility, while \(r_{b}\) suffers from large (non)compatibility, thus we say that reward \(r_{g}\) is more compatible with \(\pi^{E}\) than \(r_{b}\), as expected._

By definition of IRL, the true reward \(r^{E}\) makes the observed \(\pi^{E}\) optimal, but reveals no information about the other policies. Thus, it is meaningful that \(\overline{\mathcal{C}}_{p,\pi^{E}}\) considers the suboptimality of \(\pi^{E}\) only, because demonstrations from \(\pi^{E}\) do not provide information about other policies, as illustrated below.

**Example 4.2**.: _Let \(r^{\prime}_{b}\) be such that \(r^{\prime}_{b}(M)=+0.99,r^{\prime}_{b}(C)=-1,r^{\prime}_{b}(S)=+1\). Clearly, \(r^{\prime}_{b}\) is much worse than \(r_{g}\) at modeling \(r^{E}\), because it does not capture the fact that the expert appreciates the cake but she hates the soup. However, demonstrations from \(\pi^{E}\) alone do not provide information about C or S, but only about \(\pi^{E}=M\) (i.e., the expert always eats the muffin). Thus, we have that \(\overline{\mathcal{C}}_{p,\pi^{E}}(r_{g})=\overline{\mathcal{C}}_{p,\pi^{E}}( r^{\prime}_{b})=0.01\), i.e., \(r_{g}\) and \(r^{\prime}_{b}\) are equally compatible with the given demonstrations._

For a discussion on comparing the (non)compatibility of different rewards, see Appendix C.4.

### The IRL Classification Formulation

Our goal is to overcome the limitations of the feasible set highlighted in Section 3. Drawing inspiration from the notion of "membership checker" algorithm in [31], we propose a novel formulation of IRL.

**Definition 4.2** (IRL Classification Problem and IRL Algorithm).: _An IRL Classification Problem instance is made of a tuple \((\mathcal{M},\pi^{E},\mathcal{R},\Delta)\), where \(\mathcal{M}\) is an MDP without reward, \(\pi^{E}\) is the expert's policy, \(\mathcal{R}\subseteq\mathfrak{R}\) is a set of rewards to classify, and \(\Delta\in\mathbb{R}_{\geqslant 0}\) is some threshold. The goal is to classify all and only the rewards \(r\in\mathcal{R}\) based on their (non)compatibility with \(\pi^{E}\) in \(\mathcal{M}\) w.r.t. \(\Delta\). In symbols:_

\[\forall r\in\mathcal{R}:\ \ \text{if}\ \ \overline{\mathcal{C}}_{p,\pi^{E}}(r) \leqslant\Delta\ \ \text{then}\ \ \text{return}\ \text{True},\ \ \text{else}\ \text{return}\ \text{False}.\]

_An IRL algorithm takes in input a reward \(r\in\mathcal{R}\) and outputs a boolean saying whether \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)\leqslant\Delta\)._

Given \(r\in\mathcal{R}\), we output whether it makes the expert's policy \(\pi^{E}\) at most \(\Delta\)-suboptimal or not. Intuitively, we classify rewards in \(\mathcal{R}\) based on how good \(\pi^{E}\) performs w.r.t. them. A \(\Delta\)-(non)compatible reward guarantees that, among its \(\Delta\)-optimal policies, there is \(\pi^{E}\), but the optimal policy might be different from \(\pi^{E}\) (see Appendix C.3 for how this relates to (forward) RL). Note that we allow for \(\mathcal{R}\neq\mathfrak{R}\) to manage scenarios in which we have some prior knowledge on \(r^{E}\), i.e., \(r^{E}\in\mathcal{R}\subset\mathfrak{R}\).

**Remark 4.1**.: _Permitting non-zero (non)compatibility is equivalent to enlarging the feasible set. Let \(\mathcal{R}=\mathfrak{R}\), and define the set of rewards positively classified as \(\mathcal{R}_{\Delta}\), i.e., \(\mathcal{R}_{\Delta}\coloneqq\{r\in\mathcal{R}\,|\,\overline{\mathcal{C}}_{p, \pi^{E}}(r)\leqslant\Delta\}\). For any \(\Delta,\Delta^{\prime}\) s.t. \(0\leqslant\Delta\leqslant\Delta^{\prime}\leqslant 2H\), we have: \(\mathcal{R}_{p,\pi^{E}}=\overline{\mathcal{R}}_{0}\subseteq\mathcal{R}_{ \Delta}\subseteq\mathcal{R}_{\Delta^{\prime}}\subseteq\mathcal{R}_{2H}= \mathfrak{R}\)._

**Discussion on Reward Compatibility.** It should be remarked that:

* _The limits of the rewards compatibility framework are the same as the limits of the feasible set._ We cannot identify \(r^{E}\) from the feasible set or among the rewards with small (non)compatibility. As aforementioned, this is an inherent limit of IRL and cannot be overcome with a more refined objective formulation, unless further information on \(r^{E}\) is available (e.g., preferences).
* _Rewards compatibility offers advantages over feasible set._ Differently from the feasible set, as we will see in Section 5, it is possible to _practically_ implement algorithms that solve the IRL classification problem, with guarantees of sample efficiency even when the state space is large.

### A Learning Framework for Online IRL Classification

In this section, we combine the online IRL setting presented in Section 2 with the IRL classification problem of Definition 4.2. Intuitively, the performance of an algorithm depends on its accuracy at estimating the (non)compatibility of the rewards, as formalized by the following PAC requirement.

**Definition 4.3** (PAC Framework).: _Let \(\epsilon,\delta\in(0,1)\), and let \(\mathcal{D}^{E}\) be a dataset of \(\tau^{E}\) expert's trajectories. An algorithm \(\mathfrak{A}\) exploring for \(\tau\) episodes is \((\epsilon,\delta)\)-PAC for the IRL classification problem if:_

\[\operatorname*{\mathbb{P}}_{\mathcal{M},\pi^{E},\mathfrak{A}}\Big{(}\sup_{r \in\mathcal{R}}\left|\overline{\mathcal{C}}_{p,\pi^{E}}(r)-\widehat{\mathcal{ C}}(r)\right|\leq\epsilon\Big{)}\geq 1-\delta,\]

_where \(\operatorname*{\mathbb{P}}_{\mathcal{M},\pi^{E},\mathfrak{A}}\) is the joint probability measure induced by \(\pi^{E}\) and \(\mathfrak{A}\) in \(\mathcal{M}\), and \(\widehat{\mathcal{C}}\) is the estimate of \(\overline{\mathcal{C}}_{p,\pi^{E}}\) computed by \(\mathfrak{A}\). The sample complexity is defined by the pair \((\tau^{E},\tau)\)._

Intuitively, our goal is to estimate the (non)compatibility of the rewards in \(\mathcal{R}\) with sufficient accuracy, so that, given a threshold \(\Delta\geq 0\), we are able to classify "most" of them correctly w.h.p. (with high probability). The concept is exemplified in Figure 2. Note that the estimation problem is independent of the threshold \(\Delta\), which can be appropriately selected to cope with noise in the demonstrations, (unknown) expert suboptimality, or to manage the amount of "false negatives" and "false positives".

**Remark 4.2**.: _For \(\eta\geq 0\), let \(\mathcal{R}_{\eta}\coloneqq\{r\in\mathcal{R}\,|\,\overline{\mathcal{C}}_{p, \pi^{E}}(r)\leq\eta\}\) and \(\widehat{\mathcal{R}}_{\eta}\coloneqq\{r\in\mathcal{R}\,|\,\widehat{\mathcal{ C}}(r)\leq\eta\}\) denote the sets of rewards positively classified using, respectively, the true (non)compatibility \(\overline{\mathcal{C}}_{p,\pi^{E}}\) and the estimate \(\widehat{\mathcal{C}}\) constructed by an \((\epsilon,\delta)\)-PAC algorithm. Then, with probability \(1-\delta\), it holds that: \(\widehat{\mathcal{R}}_{\Delta-\epsilon}\subseteq\mathcal{R}_{\Delta}\subseteq \widehat{\mathcal{R}}_{\Delta+\epsilon}\). Thus, we can trade-off the amount of "false negatives" (resp. "false positives") by, e.g., choosing the threshold \(\Delta\leftarrow\Delta+\epsilon\) (resp. \(\Delta\leftarrow\Delta-\epsilon\))._

## 5 Caty-Irl:

### A Provably Efficient Algorithm for IRL

In this section, we present CATY-IRL (CompAtibility for IRL), a provably efficient algorithm for solving the _online_ IRL _classification_ problem. We consider three different kinds of structure for the MDPs: tabular MDPs, tabular MDPs with linear rewards, and Linear MDPs. Similarly to RFE, our online IRL classification setting is made of two phases: (\(i\)) an _exploration_ phase, in which the algorithm explores the environment using the knowledge of \(\mathcal{R}\) and of the expert's dataset \(\mathcal{D}^{E}\) to collect samples about the dynamics of the MDP, and (\(ii\)) a _classification_ phase, in which it performs the classification of a reward \(r\in\mathcal{R}\) without interactions with the environment. A flow-chart is reported in Figure 1 (pseudocode in Appendix D).

**Exploration phase.** The _exploration_ phase collects a dataset \(\mathcal{D}\) in a way that depends on the structure of the MDP and of the set of rewards \(\mathcal{R}\) to be classified. Specifically, for Linear MDPs,

Figure 1: Flow-chart of CATY-IRL.

CATY-IRL executes RFLin [62]. Instead, for tabular MDPs (with or without linear reward), CATY-IRL instantiates either BPI-UCBVI [37] for each reward \(r\in\mathcal{R}\) (when \(|\mathcal{R}|=\Theta(1)\), i.e., a "small" constant w.r.t. to the size of the MDP, where "small" depends on the size of the state space, see Appendix D.2) or RF-Express [37]. Note that CATY-IRL in this phase does not use the expert's dataset \(\mathcal{D}^{E}\).

**Classification phase.** The _classification_ performs the estimation \(\widehat{\mathcal{C}}(r)\) of the (non)compatibility term \(\widetilde{\mathcal{C}}_{p,\pi^{E}}(r)\) for the single input reward \(r\in\mathcal{R}\) by splitting it into two independent estimates: \(\widehat{\mathcal{J}}^{E}(r)\approx J^{\pi^{E}}(r;p)\), which is computed with \(\mathcal{D}^{E}\) only, and \(\widehat{\mathcal{J}}^{*}(r)\approx J^{*}(r;p)\), which is computed with \(\mathcal{D}\) only. Concerning \(\widehat{\mathcal{J}}^{E}(r)\), when the reward is linear \(r_{h}(s,a)=\langle\phi(s,a),\theta_{h}\rangle\), CATY-IRL uses \(\mathcal{D}^{E}\) to construct an empirical estimate \(\widehat{\psi}^{E}\approx\psi^{p,\pi^{E}}\) of the expert's expected feature count [6]. Otherwise, it directly estimates \(\widehat{d}^{E}\approx d^{p,\pi^{E}}\) the expert's occupancy measure. Such estimates can be used to derive \(\widehat{\mathcal{J}}^{E}(r)\) straightforwardly. Regarding \(\widehat{J}^{*}(r)\), CATY-IRL exploits the _planning_ phase of the corresponding RFE (or BPI) algorithm adopted at exploration phase.8 Finally, CATY-IRL applies the (potentially negative) input threshold \(\Delta\) to the difference \(\widehat{J}^{*}(r)-\widehat{\mathcal{J}}^{E}(r)\) to perform the classification. See Appendix D for the full pseudo-code. Clearly, CATY-IRL can be implemented in practice, since it considers a single reward at a time instead of computing the full feasible set, and it is computationally efficient in linear MDPs, since it uses a computationally efficient algorithm as subroutine (see [62]).

Footnote 8: RFE/BPI algorithms, at planning phase, return a policy, and not its estimated performance. Since BPI-UCBVI, RF-Express, and RFLin each compute an estimate of \(J^{*}(r;p)\) as an intermediate step, with negligible abuse of notation, we assume that they output such estimate.

**Sample Efficiency.** The next result analyzes the sample complexity (Definition 4.3) of CATY-IRL.

**Theorem 5.1** (Sample Complexity of CATY-IRL).: _Let \(\epsilon,\delta\in(0,1)\). Then CATY-IRL is \((\epsilon,\delta)\)-PAC for IRL with a sample complexity upper bounded by:_

\[\text{Tabular MDPs:}\] \[\tau^{E} \leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}SA}{\epsilon^{2}} \Big{(}N+\log\frac{1}{\delta}\Big{)}\Big{)},\] \[\text{Tabular MDPs with linear rewards:}\] \[\tau^{E} \leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}d}{\epsilon^{2}} \log\frac{1}{\delta}\Big{)}, \tau \leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{5}d}{\epsilon^{2}} \Big{(}d+\log\frac{1}{\delta}\Big{)}\Big{)},\] \[\text{Linear MDPs:}\]

_where \(N=0\) if \(|\mathcal{R}|=\Theta(1)\), and \(N=S\) otherwise._

Some observations are in order. We conjecture that the \(d^{2}\) dependence when \(|\mathcal{R}|=\Theta(1)\) is unavoidable in Linear MDPs because of the lower bound for BPI in [62]. In tabular MDPs with deterministic expert, one might use the results in [66] to reduce the rate of \(\tau^{E}\) from \(\widetilde{\mathcal{O}}(SAH^{3}\log(\delta^{-1})/\epsilon^{2})\) to \(\widetilde{\mathcal{O}}(SH^{3/2}\log(\delta^{-1})/\epsilon^{2})\). Finally, note that the choice \(\Delta=\epsilon\) allows us to positively classify all the rewards in the feasible set \(\mathcal{R}_{p,\pi^{E}}\) w.h.p. and, in this case, other rewards positively classified have true (non)compatibility at most \(2e\) w.h.p. In light of this result we conclude that _rewards compatibility_ framework allows the _practical_ development of _sample efficient_ algorithms (e.g., CATY-IRL) in Linear MDPs with large/continuous state spaces.

## 6 Statistical Barriers and Objective-Free Exploration

In this section, we show that CATY-IRL is minimax optimal for the number of exploration episodes in tabular MDPs, and that RFE and IRL share the same theoretical sample complexity. This allows us to formulate _Objective-Free Exploration_, a unifying setting for exploration problems.

### The Theoretical Limits of IRL (and RFE) in the Tabular Setting

In CATY-IRL, we use a minimax optimal RFE algorithm for exploration. However, this does not entail that CATY-IRL is minimax optimal for the IRL classification problem. There might exist another PAC algorithm with a sample complexity smaller than CATY-IRL. The following result states that, in the tabular setting, the bound in Theorem 5.1 is tight for the number of episodes \(\tau\).

**Theorem 6.1** (IRL Classification - Lower Bound).: _Let \(\mathfrak{A}\) be an \((\epsilon,\delta)\)-PAC algorithm for the IRL classification in tabular MDPs. Let \(\tau\) be the number of exploration episodes. Then, there exists an IRL classification instance such that:_

\[\text{if }|\mathcal{R}|\geqslant 1:\ \tau\geqslant\Omega\bigg{(}\frac{H^{3}SA}{ \epsilon^{2}}\log\frac{1}{\delta}\bigg{)},\qquad\text{if }\mathcal{R}=\mathfrak{R}:\ \tau\geqslant\Omega \bigg{(}\frac{H^{3}SA}{\epsilon^{2}}\Big{(}S+\log\frac{1}{\delta}\Big{)} \bigg{)}.\]

In both cases, the lower bound is _matched_ by CATY-IRL, up to logarithmic factors. Note that CATY-IRL explores without using \(\mathcal{D}^{E}\), thus, minimax optimality for \(\tau\) can be achieved without the knowledge of \(\mathcal{D}^{E}\) at exploration phase. As a by-product, we observe that a similar lower bound construction can be made also for RFE, leading to the following result.

**Theorem 6.2** (RFE - Refined Lower Bound).: _Let \(\mathfrak{A}\) be an \((\epsilon,\delta)\)-PAC algorithm for RFE in tabular MDPs. Let \(\tau\) be the number of exploration episodes. Then, there exists an RFE instance such that:_

\[\tau\geqslant\Omega\bigg{(}\frac{H^{3}SA}{\epsilon^{2}}\Big{(}S+\log\frac{1} {\delta}\Big{)}\bigg{)}.\]

This bound improves the state-of-the-art RFE lower bound \(\Omega(\frac{H^{3}SA}{\epsilon^{2}}(\frac{S}{H}+\log\frac{1}{\delta}))\) (obtained combining the bounds in [21] and [12]) by one \(H\) factor, and it is matched by RF-Express [37].

### Objective-Free Exploration (OFE)

What is the most efficient exploration strategy that can be performed in an unknown environment? It _depends_ on the subsequent task that shall be solved. However, if the task is unknown at the exploration phase, we need a strategy that suffices for all the tasks that one might be interested in solving. Let us denote by \(\mathcal{F}\) the set of RL and IRL classification tasks. Since CATY-IRL is a sample efficient algorithm for the IRL classification problem, and it uses RFE as a subroutine, we conclude that the RFE exploration strategy is sufficient (and also minimax optimal in tabular MDPs) to obtain guarantees for class \(\mathcal{F}\). Are there other problems for which RFE exploration suffices when the specific problem instance is revealed _a posteriori_ of the exploration phase? We believe so, and in Appendix E, we identify two additional problems, i.e., Matching Performance (MP) and Imitation Learning from Observations alone (ILFO) [34], that represent potential candidates to belong to \(\mathcal{F}\).

More in general, we formulate the _Objective-Free Exploration (OFE)_ problem as follows:

**Definition 6.1** (Objective-Free Exploration).: _Given a tuple \((\mathcal{M},\mathcal{F},(\epsilon,\delta))\), where \(\mathcal{M}\) is an unknown environment (e.g., MDP without reward), and \(\mathcal{F}\) is a certain class of tasks (e.g., all RL and IRL problems), the Objective-Free Exploration (OFE) problem aims to find an exploration of the environment \(\mathcal{M}\) (e.g., RFE exploration) that permits to solve any task \(f\in\mathcal{F}\) in an \((\epsilon,\delta)\)-correct manner._

This problem is called "objective-free" because it does not require the knowledge of the specific "objective" \(f\in\mathcal{F}\) to be solved. In Appendix F, we describe a use case for OFE. We believe this is an interesting problem to be studied in future.

## 7 Conclusions

In this paper, we have shown that the feasible set cannot be learned efficiently in problems with large/continuous state spaces even under the strong structure provided by Linear MDPs. For this reason, we have introduced the powerful framework of _compatible rewards_, which formalizes the intuitive notion of compatibility of a reward function with expert demonstrations, and it allows us to formulate the IRL problem as a _classification_ task. In this context, we have devised CATY-IRL, a provably efficient IRL algorithm for Linear MDPs with large/continuous state spaces. Furthermore, in tabular MDPs, we have demonstrated the minimax optimality of CATY-IRL at exploration by presenting a novel lower bound to the IRL classification problem. As a by-product, our construction improves the current state-of-the-art lower bound for RFE. Finally, we have introduced OFE, a unifying problem setting for exploration problems, which generalizes both RFE and IRL.

**Limitations.** A limitation of our contributions concerns the adoption of the _Linear MDP_ model, whose assumptions are overly strong to be consistently applied to real-world applications. Nevertheless, while the rewards compatibility framework is general and not tied to Linear MDPs, we believe that Linear MDPs represent an important initial step toward the development of provably efficient IRLalgorithms with more general function approximation structures. Although a lower bound for Linear MDPs is missing, we believe that it represents an interesting direction for future works. Finally, we note that the _empirical validation_ of the proposed algorithm is out of the scope of this work.

**Future Directions.** Promising directions for future works concern the extension of the analysis of the _rewards compatibility_ framework beyond Linear MDPs to general function approximation and to the offline setting. In addition, it might be fascinating to extend the notion of reward compatibility to other kinds of expert feedback (in the context of ReL), and to other IRL settings (e.g., suboptimal experts). Finally, we believe that OFE should be analysed in-depth given its practical importance.

## Acknowledgments and Disclosure of Funding

AI4REALNET has received funding from European Union's Horizon Europe Research and Innovation programme under the Grant Agreement No 101119527. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.

Funded by the European Union - Next Generation EU within the project NRPP M4C2, Investment 1.,3 DD. 341 - 15 march 2022 - FAIR - Future Artificial Intelligence Research - Spoke 4 - PE0000013 - D53C22002380006.

## References

* [1] Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Ng. An application of reinforcement learning to aerobatic helicopter flight. In _Advances in Neural Information Processing Systems 19 (NeurIPS)_, 2006.
* [2] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In _International Conference on Machine Learning 21 (ICML)_, 2004.
* [3] Pieter Abbeel and Andrew Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In _International Conference on Machine Learning 22 (ICML)_, 2005.
* [4] Stephen Adams, Tyler Cody, and Peter A. Beling. A survey of inverse reinforcement learning. _Artificial Intelligence Review_, 55:4307-4346, 2022.
* [5] Kareem Amin and Satinder Singh. Towards resolving unidentifiability in inverse reinforcement learning, 2016.
* [6] Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and progress. _Artificial Intelligence_, 297:103500, 2018.
* [7] Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, and Shawn O'Banion. Massively scalable inverse reinforcement learning in google maps, 2024.
* [8] P. Dimitri Bertsekas. _Convex Optimization Theory_. Athena Scientific, 2009.
* [9] Michael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In _International Conference on Machine Learning 40 (ICML)_, 2023.
* [10] Haoyang Cao, Samuel Cohen, and Lukasz Szpruch. Identifiability in inverse reinforcement learning. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 12362-12373, 2021.
* [11] Gregory Dexter, Kevin Bello, and Jean Honorio. Inverse reinforcement learning in a continuous state space with formal guarantees. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 6972-6982, 2021.
* [12] Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In _International Conference on Algorithmic Learning Theory 32 (ALT)_, volume 132, pages 578-598, 2021.

* [13] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning 38 (ICML)_, volume 139, pages 2826-2836, 2021.
* [14] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In _International Conference on Machine Learning 33 (ICML)_, volume 48, pages 49-58, 2016.
* [15] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In _International Conference on Learning Representations 5 (ICLR)_, 2017.
* [16] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. In _Advances in Neural Information Processing Systems 30 (NeurIPS)_, 2017.
* [17] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In _Advances in Neural Information Processing Systems 29 (NeurIPS)_, 2016.
* [18] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In _Advances in Neural Information Processing Systems 29 (NeurIPS)_, 2016.
* [19] Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 4415-4426, 2020.
* [20] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In _International Conference on Machine Learning 34 (ICML)_, volume 70, pages 1704-1713, 2017.
* [21] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning 37 (ICML)_, volume 119, pages 4870-4879, 2020.
* [22] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 13406-13418, 2021.
* [23] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory 33 (COLT)_, volume 125, pages 2137-2143, 2020.
* [24] Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, and Michal Valko. Adaptive reward-free exploration. In _International Conference on Algorithmic Learning Theory 32 (ALT)_, volume 132, pages 865-891, 2021.
* [25] Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon. Reward identification in inverse reinforcement learning. In _International Conference on Machine Learning 38 (ICML)_, pages 5496-5505, 2021.
* [26] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive imitation learning. In _International Conference on Machine Learning 37 (ICML)_, volume 119, pages 5286-5295, 2020.
* [27] Edouard Klein, Matthieu Geist, Bilal Piot, and Olivier Pietquin. Inverse reinforcement learning through structured classification. In _Advances in Neural Information Processing Systems 25 (NeurIPS)_, 2012.
* [28] Abi Komanduru and Jean Honorio. On the correctness and sample complexity of inverse reinforcement learning. In _Advances in Neural Information Processing Systems 32 (NeurIPS)_, 2019.
* [29] Abi Komanduru and Jean Honorio. A lower bound for the sample complexity of inverse reinforcement learning. In _International Conference on Machine Learning 38 (ICML)_, volume 139, pages 5676-5685, 2021.

* [30] David M. Kreps. _Notes On The Theory Of Choice_. Westview Press, 1988.
* [31] Filippo Lazzati, Mirco Mutti, and Alberto Maria Metelli. Offline inverse rl: New solution concepts and provably efficient algorithms. In _International Conference on Machine Learning 41 (ICML)_, 2024.
* [32] Gen Li, Yuling Yan, Yuxin Chen, and Jianqing Fan. Minimax-optimal reward-agnostic exploration in reinforcement learning, 2023.
* [33] David Lindner, Andreas Krause, and Giorgia Ramponi. Active exploration for inverse reinforcement learning. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, pages 5843-5853, 2022.
* [34] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 1118-1125, 2018.
* [35] Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Learning from demonstration: Provably efficient adversarial policy imitation with linear function approximation. In _International Conference on Machine Learning 39 (ICML)_, volume 162, pages 14094-14138, 2022.
* [36] Manuel Lopes, Francisco Melo, and Luis Montesano. Active learning for reward estimation in inverse reinforcement learning. In _Machine Learning and Knowledge Discovery in Databases (ECML PKDD)_, pages 31-46, 2009.
* [37] Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure exploration in reinforcement learning. In _International Conference on Machine Learning 38 (ICML)_, volume 139, pages 7599-7608, 2021.
* [38] Alberto Maria Metelli, Filippo Lazzati, and Marcello Restelli. Towards theoretical understanding of inverse reinforcement learning. In _International Conference on Machine Learning 40 (ICML)_, volume 202, pages 24555-24591, 2023.
* [39] Alberto Maria Metelli, Giorgia Ramponi, Alessandro Concetti, and Marcello Restelli. Provably efficient learning of transferable rewards. In _International Conference on Machine Learning 38 (ICML)_, volume 139, pages 7665-7676, 2021.
* [40] Bernard Michini, Mark Cutler, and Jonathan P. How. Scalable reward learning from demonstration. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 303-308, 2013.
* [41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.
* [42] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In _International Conference on Machine Learning 17 (ICML)_, pages 663-670, 2000.
* [43] Donald Ornstein. On the existence of stationary optimal strategies. _Proceedings of the American Mathematical Society_, 20(2):563-569, 1969.
* [44] Riccardo Poiani, Gabriele Curti, Alberto Maria Metelli, and Marcello Restelli. Inverse reinforcement learning with sub-optimal experts, 2024.
* [45] Martin Lee Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, Inc., 1994.
* [46] Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, and Kannan Ramchandran. On the value of interaction and function approximation in imitation learning. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, volume 34, pages 1325-1336, 2021.
* [47] Nived Rajaraman, Lin Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits of imitation learning. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 2914-2924, 2020.

* [48] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In _International Conference on Machine Learning 23 (ICML)_, pages 729-736, 2006.
* [49] Stuart Russell. Learning agents for uncertain environments (extended abstract). In _Conference on Computational Learning Theory 11 (COLT)_, pages 101-103, 1998.
* [50] Stuart Russell and Peter Norvig. _Artificial Intelligence: A Modern Approach_. Prentice Hall, 3 edition, 2010.
* [51] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather than assuming, human biases for reward inference. In _International Conference on Machine Learning 36 (ICML)_, volume 97, pages 5670-5679, 2019.
* [52] Mehran Shakerinava and Siamak Ravanbakhsh. Utility theory for sequential decision making. In _International Conference on Machine Learning 39 (ICML)_, volume 162, pages 19616-19625, 2022.
* [53] Lior Shani, Tom Zahavy, and Shie Mannor. Online apprenticeship learning. In _AAAI Conference on Artificial Intelligence 36 (AAAI)_, pages 8240-8248, 2022.
* [54] David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. _Nature_, 529:484-503, 2016.
* [55] Joar Skalse and Alessandro Abate. Misspecification in inverse reinforcement learning. In _AAAI Conference on Artificial Intelligence 37 (AAAI)_, pages 15136-15143, 2023.
* [56] Joar Max Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave. Invariance in policy optimisation and partial identifiability in reward learning. In _International Conference on Machine Learning 40 (ICML)_, volume 202, pages 32033-32058, 2023.
* [57] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In _International Conference on Machine Learning 36 (ICML)_, volume 97, pages 6036-6045, 2019.
* [58] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. A Bradford Book, 2018.
* [59] Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J. Bagnell, Steven Z. Wu, Jiantao Jiao, and Kannan Ramchandran. Minimax optimal online imitation learning via replay estimation. In _Advances in Neural Information Processing Systems 35 (NeruIPS)_, volume 35, pages 7077-7088, 2022.
* [60] Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. In _Advances in Neural Information Processing Systems 20 (NeurIPS)_, 2007.
* [61] Luca Viano, Stratis Skoulakis, and Volkan Cevher. Imitation learning in discounted linear mdps without exploration assumptions, 2024.
* [62] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free RL is no harder than reward-aware RL in linear Markov decision processes. In _International Conference on Machine Learning 39 (ICML)_, volume 162, pages 22430-22456, 2022.
* [63] Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 17816-17826, 2020.
* [64] Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Reinforcement learning with general value function approximation: provably efficient approach via bounded eluder dimension. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 6123-6135, 2020.

* [65] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Furnkranz. A survey of preference-based reinforcement learning methods. _Journal of Machine Learning Research_, 18:1-46, 2017.
* [66] Tian Xu, Ziniu Li, Yang Yu, and Zhimin Luo. Provably efficient adversarial imitation learning with unknown transitions. In _Conference on Uncertainty in Artificial Intelligence 39 (UAI)_, volume 216, pages 2367-2378, 2023.
* [67] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning 36 (ICML)_, volume 97, pages 6995-7004, 2019.
* [68] Lei Zhao, Mengdi Wang, and Yu Bai. Is inverse reinforcement learning harder than standard reinforcement learning? In _International Conference on Machine Learning 41 (ICML)_, 2024.
* [69] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory 34 (COLT)_, pages 4532-4576, 2021.
* [70] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In _AAAI Conference on Artificial Intelligence 23 (AAAI)_, pages 1433-1438, 2008.

Related Works

In this appendix, we report and describe the literature that most relates to this paper. Theoretical works concerning the online IRL problem can be grouped in works that concern the feasible set, and works that do not.

Let us begin with works related to the feasible set. While the notion of feasible set has been introduced implicitly in [42], the first paper that analyses the sample complexity of estimating the feasible set in online IRL is [39]. Authors in [39] adopt the simple generative model in tabular MDPs, and devise two sample efficient algorithms. [33] focuses on the same problem as [39], but adopts a forward model in tabular MDPs. By adopting RFE exploration algorithms, they devise sample efficient algorithms. However, as remarked in [68], paper [33] suffers from a limitation in the definition of the dissimilarity between feasible sets. [38] builds upon [39] to construct the first minimax lower bound for the problem of estimating the feasible set using a generative model. The lower bound is in the order of \(\Omega\big{(}\frac{H^{3}\mathcal{SA}}{\epsilon^{2}}\big{(}S+\log\frac{1}{ \delta}\big{)}\big{)}\), where \(S\) and \(A\) are the cardinality of the state and action spaces, \(H\) is the horizon, \(\epsilon\) is the accuracy and \(\delta\) the failure probability. In addition, [38] develops US-IRL, an efficient algorithm whose sample complexity matches the lower bound. [44] analyze a setting analogous to that of [38], in which there is availability of a single optimal expert and multiple suboptimal experts with known suboptimality. [31] analyse the problem of estimating the feasible set when no active exploration of the environment is allowed, but the learner is given a batch dataset collected by some behavior policy \(\pi^{b}\). Interestingly, [31] focuses on two novel learning targets that are suited for the offline setting, i.e., a subset and a superset of the feasible set. Authors in [31] demonstrate that such sets are the tightest learnable subset and superset of the feasible set, and propose a pessimistic algorithm, PIRLO, to estimate them. [68] analyses the same offline setting as [31], but instead of focusing on the notion of feasible set directly, it considers the notion of reward mapping, which considers reward functions as parametrized by their value and advantage functions, and whose image coincides with the feasible set.

With regards to online IRL works that do not consider the feasible set, we mention [36], which analyses an active learning framework for IRL. However, [36] assumes that the transition model is known, and its goal is to estimate the expert policy only. Works [28] and [29] provide, respectively, an upper bound and a lower bound to the sample complexity of IRL for \(\beta\)-strict separable problems in the tabular setting. However, both the setting considered and the bound obtained are fairly different from ours. Analogously, [11] provides a sample efficient IRL algorithm for \(\beta\)-strict separable problems with continuous state space. However, their setting is different from ours since they assume that the system can be modelled using a basis of orthonormal functions.

### Additional Related Works

In this section, we collect additional related works that deserve to be mentioned.

Identifiability and Reward Learning.As aforementioned, the IRL problem is ill-posed, thus, to retrieve a single reward, additional constraints shall be imposed. [5] analyses the setting in which demonstrations of an optimal policy for the same reward function are provided across environments with different transition models. In this way, authors can reduce the experimental unidentifiability, and recover the state-only reward function. [10] and [26] concern reward identifiability but in entropy-regularized MDPs [70; 15]. Such setting is in some sense easier than the common IRL setting, because entropy-regularization permits a unique optimal policy for any reward function. [10] uses expert demonstrations from multiple transition models and multiple discount factors to retrieve the reward function, while [26] analyses properties of the dynamics of the MDP to increase the constraints. With regards to the more general field of Reward Learning (ReLU), we mention [19], which introduces a framework that formalizes the constraints imposed by various kinds of human feedback (like demonstrations or preferences [65]). Intuitively, multiple feedbacks about the same reward represent additional constraints beyond mere demonstrations. [56] characterizes the partial identifiability of the reward function based on various reward learning data sources.

Linear MDPs and Extensions.As explained for instance in [23], since lower bounds to the sample complexity of various RL tasks in tabular MDPs depend explicitly on the cardinality state space \(S\), then we need to add structure to the problem if we want to develop efficient algorithms that scale to large state spaces. For this reason, the works [67; 23] analyze the Linear MDP model, which enforcessome linearity constraints to the common MDP model. In this way, authors are able to provide efficient algorithms for RL in problems with large/continuous state spaces. However, there are other settings beyond Linear MDPs that are analysed in the RL literature. [20] introduces the notion of Bellman rank as complexity measure, and provides a sample efficient algorithm for problems with small Bellman rank. [64] analyzes general value function approximation when the function class has a low eluder dimension. [22] generalizes both the eluder dimension and Bellman rank complexity measures by defining the Bellman eluder dimension and providing a provably efficient algorithm. [13] introduces bilinear classes, a structural framework that, among the others, generalizes Linear MDPs.

Reward-Free Exploration (RFE) in Tabular and Linear MDPs.The RFE problem was introduced in [21], where authors provided a sample efficient algorithm and a lower bound for tabular MDPs. Later on, the state-of-the-art sample-efficient algorithms for RFE in tabular MDPs have been developed in [24, 37, 32]. It should be remarked that RFE requires more samples than common RL in tabular MDPs. [63] proposes a sample efficient algorithm for RFE in linear MDPs. [62] improves the algorithm of [63] and, interestingly, demonstrates that RFE is no harder than RL in Linear MDPs.

Online Apprenticeship Learning (AL).The first works that provide a theoretical analysis of the AL setting when the transition model is unknown are [3, 60]. Recently, [53] formulates the online AL problem, which closely resembles the online IRL problem. The main difference is that in online AL the ultimate goal is to imitate the expert, while in IRL is to recover a reward function. [66] improves the results in [53] by combining an RFE algorithm with an efficient algorithm for the estimation of the visitation distribution of the deterministic expert's policy in tabular MDPs, presented in [47]. We mention also [46, 59] for the sample complexity of estimating the expert's policy in problems with linear function approximation. In the context of Imitation Learning from Observation alone (ILfO) [34], the work [57] proposes a probably efficient algorithm for large-scale MDPs with unknown transition model. [35] provides an efficient AL algorithm based on GAIL [18] in Linear Kernel Episodic MDPs [69] with unknown transition model.

Others.We mention work [27], which considers a classification approach for IRL. However, this is fairly different from our IRL problem formulation in Section 4.

## Appendix B Additional Results and Proofs for Section 3

In this section, we provide additional results beyond those presented in Section 3, and then we report the missing proofs. Specifically, in Appendix B.1, we provide two numerical examples that explain Proposition 3.1, in Appendix B.2 we show that some additional regularity assumptions beyond the Linear MDP cannot remove the dependence on the cardinality of the state space in the sample complexity. In Appendix B.3, we report and describe the sample efficient algorithm mentioned in Section 3, while in Appendix B.4 we collect all the missing proofs of this section.

### Some Examples for Proposition 3.1

The following examples aim to explain Proposition 3.1 in a simple manner.

**Example B.1** (Non-degenerate feasible set).: _Let \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0})\cup\{\pi^{E}\}\) be an IRL instance such that \(\mathcal{S}=\{s_{1},s_{2}\},\mathcal{A}=\{a_{1},a_{2}\},H=1,d_{0}(s_{1})=d_{0 }(s_{2})=1/2,\pi^{E}(s_{1})=\pi^{E}(s_{2})=a_{1}\). Consider the feature mapping \(\phi_{1}\) s.t. \(\phi_{1}(s,a)=\mathds{1}\{a=a_{1}\}\) for all \(s\in\mathcal{S}\). Then, we have \(\Phi^{\pi^{E}}=\{1\}\) and \(\overline{\Phi}=\{0\}\). Clearly, these sets can be separated by any hyperplane \(w\in\mathbb{R}_{>0}\), since \(1\cdot w>0\cdot w\), and so \(\mathcal{R}_{p,\pi^{E}}\neq\{\overline{r}\}\), with \(\overline{r}_{h}(s,a)=0\)\(\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\). Actually, \(\mathcal{R}_{p,\pi^{E}}=\{r\in\mathfrak{R}\,|\,\exists\theta\in(0,1]:\,r_{1}( s,a)=\langle\phi(s,a),\theta\rangle\,\forall(s,a)\in\mathcal{S}\times\mathcal{A}\}\)._

**Example B.2** (Degenerate feasible set).: _Consider the same IRL instance as in the previous example, but this time consider the feature mapping \(\phi_{2}\) s.t. \(\phi_{2}(s_{1},a)=\mathds{1}\{a=a_{1}\}\), and \(\phi_{2}(s_{2},a)=\mathds{1}\{a=a_{2}\}\). Then, we have \(\Phi^{\pi^{E}}=\{0,1\}\) and \(\overline{\Phi}=\{0,1\}\). Clearly, the two sets coincide, thus they cannot be separated, and \(\mathcal{R}_{p,\pi^{E}}=\{\overline{r}\}\), with \(\overline{r}_{h}(s,a)=0\)\(\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\)._

### Additional Regularity Assumptions of the State Space do not Make the Problem Learnable

In tabular MDPs with small state space \(\mathcal{S}\), collecting samples from every state \(s\in\mathcal{S}\) is feasible, and it is exactly what previous works do:

* Under the assumption that \(\pi^{E}\) is deterministic, [39, 38] collect one sample from every \((s,h)\in\mathcal{S}\times\llbracket H\rrbracket\) using a generative model, obtaining \(\pi^{E}\) exactly.
* If \(\pi^{E}\) is stochastic, under the assumption that all actions in the support of the expert's policy are played with probability at least \(\pi_{\min}\) (see Assumption D.1 of [38]), both [38, 68] are able to learn the support of \(\pi^{E}\) exactly w.h.p. using \(\propto\!1/\pi_{\min}\) samples in the online setting.9 Footnote 9: Actually, [68] makes use of a concentrability assumption too.
* In the offline setting, assuming that the occupancy measure of the expert's policy is at least \(d_{\min}\) in all reachable \((s,a)\in\mathcal{S}\times\mathcal{A}\), then [31] learns the support of \(\pi^{E}\) exactly w.h.p. using \(\propto\!1/d_{\min}\) episodes.

However, when \(\mathcal{S}\) is large, even under the Linear MDP assumption, this is not possible. In Section 3, we have formalized this fact with the following proposition:

**Theorem 3.2** (Statistical Inefficiency).: _Let \(\mathcal{M}\cup\{\pi^{E}\}\) be a Linear IRL instance with finite state space \(\mathcal{S}\) and deterministic expert's policy, and let \(\epsilon,\delta\in(0,1)\). If an algorithm \(\mathfrak{A}\) is \((\epsilon,\delta)\)-PAC, then \(\tau^{E}=\Omega(S)\), where \(S:=|\mathcal{S}|\) is the cardinality of the state space._

Theorem 3.2 tells us that the Linear MDP assumption is too weak for the feasible set to be learnable using the PAC framework of Definition 3.2 with a number of samples independent of the cardinality of the state space. Therefore, we can try to introduce an additional assumption on the structure of the IRL problem \(\mathcal{M}\cup\{\pi^{E}\}\) and see whether it helps in alleviating the issue. Let us consider the following first assumption.

**Assumption B.1**.: _We assume a Lipschitz continuity property between features and states:_

\[\forall(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}:\quad \|\phi(s,a)-\phi(s^{\prime},a)\|_{2}\leq L\|s-s^{\prime}\|,\]

_for some \(L>0\) and some distance \(\|\cdot-\|\) in \(\mathcal{S}\)._

The intuition is that, based on the fact that in Linear MDPs the \(Q\)-function of any policy \(\pi\) is linear in the feature mapping \(Q_{h}^{\pi}(\cdot,\cdot)=(\phi(\cdot,\cdot),w_{h}^{\pi})\) for some parameter vector \(w_{h}^{\pi}\in\mathbb{R}^{d}\) (see [23]), then if we are able to \(\epsilon\)-cover the state space \(\mathcal{S}\), we can approximate the \(Q\)-function \(Q_{h}^{\pi}(s,\cdot)\) in any \(s\in\mathcal{S}\) with the \(Q\)-function \(Q_{h}^{\pi}(s^{\prime},\cdot)\) of the closest point \(s^{\prime}\) in the covering, so that \(|Q_{h}^{\pi}(s,a)-Q_{h}^{\pi}(s^{\prime},a)|=|(\phi(s,a)-\phi(s^{\prime},a)) ^{\intercal}w_{h}^{\pi}|\leq\|\phi(s,a)-\phi(s^{\prime},a)\|_{2}\|w_{h}^{\pi} \|_{2}\leq L\epsilon\|w_{h}^{\pi}\|_{2}\). However, this assumption is not sufficient.

**Proposition B.1**.: _Under the setting of Proposition 3.2, even under Assumption B.1, then an algorithm is \((\epsilon,\delta)\)-PAC only if \(\tau^{E}=\Omega(S)\)._

Assumption B.1 fails because it does not provide any information about how the knowledge of the expert's policy at a state can be "transferred" to other states, and thus we still need to sample almost all the states of \(\mathcal{S}^{p,\pi^{E}}\) to get an acceptable feasible set.

We devise another assumption to attempt to fix this issue.

**Assumption B.2**.: _We assume the following Lipschitz continuity property:_

\[\forall(s,s^{\prime})\in\mathcal{S}\times\mathcal{S}:\quad\|\phi(s,\pi_{h}^{E }(s))-\phi(s^{\prime},\pi_{h}^{E}(s^{\prime}))\|_{2}\leq L\|s-s^{\prime}\|,\]

_for some \(L>0\) and some distance \(\|\cdot-\|\) in \(\mathcal{S}\)._

This assumption says that states that are close to each other cannot have the features corresponding to the expert's action too far away from each other. From a high-level point of view, it says that the features are "somehow" regular with \(\pi^{E}\), so that when the expert lies in \(s^{\prime}\) which is really close to \(s\), then she plays an action which has the same "effect" (i.e., same transition model and same reward, due to the Linear MDP assumption) as the expert's action in \(s\).

Assumption B.2 is not comparable with Assumption B.1 since, on the one hand, it does not hold for all actions in \(\mathcal{A}\), but only for those corresponding to \(\pi^{E}\), but, on the other hand, provides information on how to transfer knowledge about \(\pi^{E}\) to neighbor states.

Let \(\Delta^{\prime}:=\min_{s\in\mathcal{S},a,a^{\prime}\in\mathcal{A}:\phi(s,a)\neq \phi(s,a^{\prime})}\|\phi(s,a)-\phi(s,a^{\prime})\|_{2}\), i.e., the smallest non-zero distance between the features of different actions. Clearly, when \(\mathcal{S}\) is finite, since in Linear MDPs also \(A:=|\mathcal{A}|\) is finite, then \(\Delta^{\prime}\) is finite too. So we can define a new quantity \(\Delta\) to be any number \(0<\Delta<\Delta^{\prime}\).

**Proposition B.2**.: _Under the setting of Proposition 3.2, under Assumption B.2, then a number of samples \(\tau^{E}=|\mathcal{N}(\frac{\Delta}{2L};\mathcal{S},\|\cdot\|)|\) is sufficient to recover \(\pi^{E}\) exactly in any \((s,h)\in\mathcal{S}\), where \(|\mathcal{N}(\frac{\Delta}{2L};\mathcal{S},\|\cdot\|)|\) is the \(\Delta/(2L)\)-covering number of space \(\mathcal{S}\) w.r.t. distance \(|\cdot|\)._

Intuitively, by constructing a covering with a sufficiently small radius in the state space \(\mathcal{S}\), then we are able to retrieve the exact expert's action in the neighborhood of each state of the covering. Doing so, we are able to construct \(\epsilon\)-correct estimates of the feasible set. Of course, this is possible as long as \(\Delta^{\prime}\) is not too small, and \(L\) is not too large. When \(\mathcal{S}\) is infinitely large or continuous, it might be possible to construct feature mappings in which \(\Delta^{\prime}\to 0\), and so the approach would still require too many samples.

However, even for cases with finite and not too small \(\Delta^{\prime}\), the result in Proposition B.2 is not satisfactory, because it just allows to retrieve \(\pi^{E}\) under a stronger assumption than Linear MDPs, but not to perform an interesting learning process. We observe that the feasible set is an "unstable" concept, in the sense that, based on Proposition 3.1, changing the expert action in a single state might reduce the feasible set from a continuum of rewards to a singleton, or vice versa.

**Remark B.1**.: _If we want to be able to recover the exact feasible set efficiently, we need to recover the exact expert's policy almost everywhere._

### Algorithm

By exploiting an RFE algorithm as sub-routine like that of Algorithm 1 in [63] or Algorithm 1 in [62], we are able to construct estimates of the transition model \(\widehat{p}\), that can be used to compute an "empirical" estimate of the feasible set \(\widehat{\mathcal{R}}\approx\mathcal{R}_{\widehat{p},\pi^{E}}\) (since \(\phi\) and \(\pi^{E}\) are known). The algorithm is presented in Algorithm 1.

``` Data: failure probability \(\delta>0\), error tolerance \(\epsilon>0\), expert policy \(\pi^{E}\), all sets \(\mathcal{Z}\subseteq\mathcal{S}\times[\![H]\!]\) that coincide with \(\mathcal{S}^{p,\pi^{E}}\) almost everywhere based on measure \(d^{p,\pi^{E}}\) \(\mathcal{D}\leftarrow\text{RFE\_Exploration}(\delta,\epsilon)\) /* Various choices */ for\(h\) in \(\{H,H-1,\ldots,2,1\}\)do \(\Lambda_{h}\gets I+\sum\limits_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\phi(s _{h}^{k},a_{h}^{k})^{\intercal}\) \(\widehat{\mu}_{h}(\cdot)\leftarrow\Lambda_{h}^{-1}\sum\limits_{k=1}^{\tau}\phi( s_{h}^{k},a_{h}^{k})\delta(\cdot,s_{h+1}^{k})\)
1 end for \(\widehat{p}_{h}(\cdot|s,a)\leftarrow\langle\phi(s,a),\widehat{\mu}_{h}(\cdot)\rangle\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\) \(\widehat{\mathcal{R}}\leftarrow\big{\{}\widehat{r}\in\mathfrak{R}\left|\, \exists\mathcal{Z},\forall(s,h)\in\mathcal{Z},\forall a\in\mathcal{A}:\ \underset{a^{\prime}\sim\pi^{E}_{h}(\cdot|s)}{\mathbb{E}}Q_{h}^{*}(s,a^{ \prime};\widehat{p},\widehat{r})\geq Q_{h}^{*}(s,a;\widehat{p},\widehat{r}) \big{\}}\) Return \(\widehat{\mathcal{R}}\) ```

**Algorithm 1**RL for Linear MDPs (known expert's policy)

Simply put, Algorithm 1 uses the dataset collected by an RFE algorithm to compute a least-squares estimate of the transition model \(\widehat{p}\), and then it returns the feasible set defined according to it (recall that \(\phi\) and \(\pi^{E}\) are known). Notice that this algorithm cannot be implemented in practice due to various reasons, like the presence of the Dirac delta \(\delta\) measure in the definition of some quantities (see Appendix B.4.3), and the fact that the feasible set is, potentially, a set containing infinite rewards. Nevertheless, Theorem 3.3 states that this algorithm is sample efficient. The proof of the theorem is provided in Appendix B.4.3.

It should be remarked that Algorithm 1 takes in input also the true support of the visit distribution of the expert policy \(\mathcal{S}^{p,\pi^{E}}\) in case \(\mathcal{S}\) is finite, and all the possible sets \(\mathcal{Z}\) that agree with \(\mathcal{S}^{p,\pi^{E}}\) a.e. based on the measure \(d^{p,\pi^{E}}\) in case \(\mathcal{S}\) is infinite. Intuitively, this set (\(\mathcal{S}^{p,\pi^{E}}\)) of \((s,h)\) pairs representsthe domain in which \(\pi^{E}\) is defined. Indeed, since the expert in the true problem \(p\) never visits pairs \((s^{\prime},h^{\prime})\notin\mathcal{S}^{p,\pi^{E}}\), its expert policy might reasonably be non well-defined there. When \(\mathcal{S}\) is infinite, we require all sets \(\mathcal{Z}\) because otherwise we cannot know which are the sets \(\mathcal{S}^{p,\pi^{E}}\backslash\mathcal{Z}\) with zero measure, i.e., in which the reward can induce an optimal action different from the expert's one, since the overall contribution to the expected return is zero.

The proof of Theorem 3.3 is obtained by using Algorithm 1 of [62] at Line 1 of Algorithm 1. In Appendix B.4.3, we demonstrate an upper bound also if we use Algorithm 1 in [63].

### Missing Proofs

Before diving into the proofs, we recall some important properties of the feasible set and of the Linear MDPs that will be useful in the proofs. First, we provide an explicit form for the feasible set presented at Definition 3.1.

**Lemma B.3** (Lemma E.1 in [31]).: _In the setting of Definition 3.1, if \(\mathcal{S}\) is finite, then the feasible set \(\mathcal{R}_{p,\pi^{E}}\) satisfies:_

\[\mathcal{R}_{p,\pi^{E}}=\Big{\{}r\in\mathfrak{R}\,\Big{|}\,\forall(s,h)\in \mathcal{S}^{p,\pi^{E}},\forall a\in\mathcal{A}:\,\underset{a^{\prime}\sim \pi^{E}_{h}(\cdot|s)}{\mathbb{E}}Q_{h}^{*}(s,a^{\prime};p,r)\geq Q_{h}^{*}(s,a ;p,r)\Big{\}}.\]

Notice that we have extended Lemma E.1 in [31] to consider stochastic expert policies (the extension is trivial). We can easily extend it to problems with large/continuous \(\mathcal{S}\).

**Lemma B.4** (Feasible Set Explicit).: _In the setting of Definition 3.1, then the feasible set \(\mathcal{R}_{p,\pi^{E}}\) satisfies:_

\[\mathcal{R}_{p,\pi^{E}}= \Big{\{}r\in\mathfrak{R}\,\Big{|}\,\forall h\in\llbracket H \rrbracket,\exists\overline{\mathcal{S}}\subseteq\mathcal{S}_{h}^{p,\pi^{E}}:d _{h}^{p,\pi^{E}}(\overline{\mathcal{S}})=0\,\wedge\,\forall s\notin\overline{ \mathcal{S}},\forall a\in\mathcal{A}:\] \[\underset{a^{\prime}\sim\pi^{E}_{h}(\cdot|s)}{\mathbb{E}}Q_{h}^{ *}(s,a^{\prime};p,r)\geq Q_{h}^{*}(s,a;p,r)\Big{\}}.\]

Simply, Lemma B.4 improves on Lemma B.3 by allowing the reward to enforce the "wrong" action (i.e., different from the expert's action) in a subset with zero measure based on the visitation distribution.

Proof.: The proof is completely analogous to that of Lemma E.1 in [31]. We just need to observe that if set \(\overline{\mathcal{S}}\) has zero measure (and the set of rewards \(\mathfrak{R}\) contains bounded rewards), then it does not affect the expected return. 

Another useful property that we need is that the \(Q\)-function is always linear in the feature map for any policy in Linear MDPs.

**Proposition B.5** (Proposition 2.3 in [23]).: _For a Linear MDP, for any policy \(\pi\), there exist weights \(\{w_{h}^{\pi}\}_{h\in\llbracket H\rrbracket}\) such that, for any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\), we have \(Q_{h}^{\pi}(s,a)=\langle\phi(s,a),w_{h}^{\pi}\rangle\)._

We can combine the results of Lemma B.4 and Proposition B.5 to obtain the following characterization of the feasible set in Linear MDPs.

**Lemma B.6**.: _In the setting of Definition 3.1, the feasible set \(\mathcal{R}_{p,\pi^{E}}\) satisfies:_

\[\mathcal{R}_{p,\pi^{E}}=\Big{\{}r\in \mathfrak{R}\,\Big{|}\,\exists\{w_{h}\}_{h\in\llbracket H \rrbracket},\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H \rrbracket:\,r_{h}(s,a)=\langle\phi(s,a),\theta_{h}\rangle\] \[\wedge\,\forall h\in\llbracket H\rrbracket,\exists\overline{ \mathcal{S}}\subseteq\mathcal{S}_{h}^{p,\pi^{E}}:d_{h}^{p,\pi^{E}}(\overline {\mathcal{S}})=0\,\wedge\,\forall s\notin\overline{\mathcal{S}},\forall a^{E }\in\mathcal{A}_{h}^{E}(s):\] \[\langle\phi(s,a^{E}),w_{h}\rangle=\underset{a\in\mathcal{A}}{ \max}\langle\phi(s,a),w_{h}\rangle\Big{\}},\]

_where \(\theta_{h}:=w_{h}-\int_{\mathcal{S}}\max_{a^{\prime}\in\mathcal{A}}\langle\phi (s^{\prime},a^{\prime}),w_{h+1}\rangle d\mu_{h}(s^{\prime})\) for all \(h\in\llbracket H\rrbracket\), and \(\mathcal{A}_{h}^{E}(s):=\{a\in\mathcal{A}|\pi^{E}_{h}(a|s)>0\}\)._

Proof.: From [45], we know that in any MDP there exists an optimal policy. Therefore, thanks to Proposition B.5, we know that the optimal \(Q\)-function \(Q^{*}\) is linear in the feature map too. So, there exist parameters \(\{w_{h}\}_{h}\) such that, for any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\), the optimal \(Q\)-function can be rewritten as \(Q_{h}^{\text{s}}(s,a)=\langle\phi(s,a),w_{h}\rangle\). From the Bellman equation, we know that:

\[Q_{h}^{\text{s}}(s,a;p,r) =r_{h}(s,a)+\int_{\mathcal{S}}V_{h+1}^{\text{s}}(s^{\prime};p,r) dp_{h}(s^{\prime}|s,a)\] \[=\langle\phi(s,a),\theta_{h}\rangle+\langle\phi(s,a),\int_{ \mathcal{S}}\max_{a^{\prime}\in\mathcal{A}}\langle\phi(s^{\prime},a^{\prime}), w_{h+1}\rangle d\mu_{h}(s^{\prime})\rangle.\]

By rearranging this equation, and removing the dot product with \(\phi(s,a)\), we obtain that:

\[\theta_{h}=w_{h}-\int_{\mathcal{S}}\max_{a^{\prime}\in\mathcal{A}}\langle\phi (s^{\prime},a^{\prime}),w_{h+1}\rangle d\mu_{h}(s^{\prime}).\]

Now, this holds in any Linear MDP. If we desire to enforce the constraints in Lemma B.4, we simply have to impose the constraint on the optimal \(Q\)-function using parameters \(\{w_{h}\}_{h}\) outside some \(\overline{\mathcal{S}}\). This concludes the proof. 

It is useful to introduce the following definitions. First we define the set of parameters that induce a \(Q\)-function compatible with \(\pi^{E}\):

\[\mathcal{W}_{p,\pi^{E}}\coloneqq\Big{\{}w:\llbracket H \rrbracket\to\mathbb{R}^{d}\,\Big{|}\,\forall h\in\llbracket H \rrbracket,\exists\overline{\mathcal{S}}\subseteq\mathcal{S}_{h}^{p,\pi^{E}}: d_{h}^{p,\pi^{E}}(\overline{\mathcal{S}})=0\,\wedge\,\forall s\notin\overline{ \mathcal{S}},\forall a^{E}\in\mathcal{A}_{h}^{E}(s):\] \[\langle\phi(s,a^{E}),w_{h}\rangle=\max_{a\in\mathcal{A}}\langle \phi(s,a),w_{h}\rangle\Big{\}}.\]

Next, we define the set of parameters of the reward function obtained by using \(Q\)-functions parametrized by \(w\in\mathcal{W}_{p,\pi^{E}}\):

Irrespective of the transition model \(\{\mu_{h}\}_{h}\) and the feature map \(\phi\), we see that it is always possible to construct a surjective map from \(\Theta_{p,\pi^{E}}\) to \(\mathcal{W}_{p,\pi^{E}}\) (the map in the definition of \(\Theta_{p,\pi^{E}}\)). Thanks to these definitions, the feasible set can be rewritten as:

\[\mathcal{R}_{p,\pi^{E}}=\{r\in\mathfrak{R}\,|\,\exists\{\theta_{h}\}_{h}\in \Theta_{p,\pi^{E}},\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times \llbracket H\rrbracket:\,r_{h}(s,a)=\langle\phi(s,a),\theta_{h}\rangle\}.\]

We are now ready to provide the proofs of the various results of this section.

#### b.4.1 Proof of Proposition 3.1

**Proposition 3.1**.: _Let \(\mathcal{M}\) be a Linear MDP without reward with a finite state space, and let \(\phi\) be a feature mapping. Let \(\{\Phi_{h}^{\pi^{E}}\}_{h\in\llbracket H\rrbracket}\) and \(\{\overline{\Phi}_{h}\}_{h\in\llbracket H\rrbracket}\) be the sets of expert's and non-expert's features, defined for every \(h\in\llbracket H\rrbracket\) as:_

\[\Phi_{h}^{\pi^{E}}\coloneqq\big{\{}\phi(s,a^{E})\,|\,s\in\mathcal{S}_{h}^{p, \pi^{E}},\,a^{E}\in\mathcal{A}_{h}^{E}(s)\big{\}},\qquad\overline{\Phi}_{h} \coloneqq\big{\{}\phi(s,a)\,|\,s\in\mathcal{S}_{h}^{p,\pi^{E}},\,a\in \mathcal{A}\backslash\mathcal{A}_{h}^{E}(s)\big{\}},\]

_where \(\mathcal{A}_{h}^{E}(s)\coloneqq\{a\in\mathcal{A}|\pi_{h}^{E}(\cdot|s)>0\}\) for every \(s\in\mathcal{S}\). If for none of the \(H\) pairs of sets \((\Phi_{h}^{\pi^{E}},\overline{\Phi}_{h})\) there exists a separating hyperplane, then \(\mathcal{R}_{p,\pi^{E}}=\{\overline{r}\}\), with \(\overline{r}_{h}(s,a)=0\,\,\forall(s,a,h)\in\mathcal{S}\times\mathcal{A} \times\llbracket H\rrbracket\) i.e., the feasible set with linear rewards in \(\phi\) contains only the reward function that assigns zero reward everywhere._

Proof.: From [8], we recall that two sets \(\mathcal{Y}_{1},\mathcal{Y}_{2}\) are separated by a hyperplane \(H=\{x|a^{\intercal}x=b\}\) if each lies in a different closed halfspace associated with \(H\), i.e., if either:

\[a^{\intercal}y_{1}\leqslant b\leqslant a^{\intercal}y_{2},\quad\forall y_{1} \in\mathcal{Y}_{1},\forall y_{2}\in\mathcal{Y}_{2},\]

or:

\[a^{\intercal}y_{2}\leqslant b\leqslant a^{\intercal}y_{1},\quad\forall y_{1} \in\mathcal{Y}_{1},\forall y_{2}\in\mathcal{Y}_{2}.\]

By definition of \(\mathcal{W}_{p,\pi^{E}}\), for each stage \(h\in\llbracket H\rrbracket\), we are looking for vectors \(w_{h}\in\mathbb{R}^{d}\) such that \(\forall(s,h)\in\mathcal{S}^{p,\pi^{E}}\), it holds that:

\[w_{h}^{\intercal}\phi(s,a)\leqslant w_{h}^{\intercal}\phi(s,a^{E})\quad \forall a^{E}\in\mathcal{A}_{h}^{E}(s),\forall a\in\mathcal{A}\backslash \mathcal{A}_{h}^{E}(s).\]In words, for each \((s,h)\in\mathcal{S}^{p,\pi^{E}}\), we are looking for non-affine separating hyperplanes between features of expert and non-expert actions. However, since the hyperplane parameter \(w_{h}\) is common to all states \(s\in\mathcal{S}_{h}^{p,\pi^{E}}\), then it must separate expert from non-expert actions at all states. This is equivalent to finding the separating hyperplanes to the sets \(\Phi_{h}^{\pi^{E}}\) and \(\overline{\Phi}_{h}\) which contain all the points. Clearly, when the separating hyperplanes do not exist at all \(h\in[H]\), then the condition in \(\mathcal{W}_{p,\pi^{E}}\) is satisfied by the zero vector alone. As a consequence, set \(\Theta_{p,\pi^{E}}\) contains only the zero vector, and so does \(\mathcal{R}_{p,\pi^{E}}\). 

**Remark B.2**.: _By using the result of Lemma B.4, we can easily convert Proposition 3.1 into a more general result by considering the impossibility of separating any pair of sets constructed by varying at will some subsets with zero measure. We will not provide such result explicitly._

#### b.4.2 Proofs of Proposition 3.2 and Appendix B.2

In the PAC framework of Definition 3.2, we have not specified formally the inner distance \(d\):

\[d(r,\widehat{r})\coloneqq\frac{1}{M_{r,\widehat{r}}}\sup_{\pi\in\Pi}\sum_{h \in[H]}(\mathop{\mathbb{E}}_{s,a})\sim_{d_{h}^{p,\pi}(\cdot,\cdot)}^{\pi}|r_{ h}(s,a)-\widehat{r}_{h}(s,a)|,\] (1)

where:

\[M_{r,\widehat{r}}\coloneqq\max\{\sqrt{d},\max_{h\in[H]}\|\theta_{h}\|_{2}, \max_{h\in[H]}|\widehat{\theta}_{h}\|_{2}\}/\sqrt{d},\]

where \(\{\theta_{h}\}_{h}\) and \(\{\widehat{\theta}_{h}\}_{h}\) are the (unbounded) parameters of rewards \(r\) and \(\widehat{r}\). As explained in [31], such normalization term allows us to work with unbounded reward functions. In practice, we are relaxing the Linear MDP assumption presented in Section 2 about the boundedness of the parameters \(\theta\) of the rewards to avoid the issue described in [38] and [31]. We still assume that the feature mapping is bounded. Observe that this relaxation does _not_ affect the results we present, which would hold even if we considered bounded parameters \(\theta\). Indeed, as visible in the proofs, the instances do not need to be constructed with unbounded \(\theta\).

**Theorem 3.2** (Statistical Inefficiency).: _Let \(\mathcal{M}\cup\{\pi^{E}\}\) be a Linear IRL instance with finite state space \(\mathcal{S}\) and deterministic expert's policy, and let \(\epsilon,\delta\in(0,1)\). If an algorithm \(\mathfrak{A}\) is \((\epsilon,\delta)\)-PAC, then \(\tau^{E}=\Omega(S)\), where \(S:=|\mathcal{S}|\) is the cardinality of the state space._

Proof.: We construct two problem instances that lie at a finite Hausdorff distance, and show that, with less than \(S\) calls to the sampling oracle, we are not able to discriminate between the two instances.

Let \(\mathcal{S}\) be the finite state space with cardinality \(S\), \(\mathcal{A}=\{a_{1},a_{2}\},H=1,d_{0}(s)=1/S\)\(\forall s\in\mathcal{S}\), \(\phi(s,a)=\mathds{1}\{a=a_{1}\}\), and consider two deterministic expert's policies \(\pi_{1}^{E}(s)=a_{1}\,\forall s\in\mathcal{S}\), and \(\pi_{2}^{E}(s)=a_{1}\,\forall s\in\mathcal{S}\backslash\{\overline{s}\}\), and \(\pi_{2}^{E}(\overline{s})=a_{2}\), for a certain \(\overline{s}\in\mathcal{S}\). The set of parameters compatible with \(\pi_{1}^{E}\) is:

\[\Theta_{p,\pi_{1}^{E}}=\{\theta\in\mathbb{R}\,|\,\theta\geq 0\},\]

since \(Q^{\pi_{1}^{E}}(s,a_{1})\geq Q^{\pi_{1}^{E}}(s,a_{2})\iff r(s,a_{1})\geq r(s,a _{2})\iff\phi(s,a_{1})\theta\geq\phi(s,a_{2})\theta\iff 1\cdot\theta\geq 0\cdot\theta\). Observe that, for \(\pi_{2}^{E}\), due to the presence of \(\overline{s}\), we have:

\[\Theta_{p,\pi_{2}^{E}}=\{\theta\in\mathbb{R}\,|\,\theta=0\},\]

since \(\overline{s}\) imposes \(\theta\leq 0\), and the other states impose \(\theta\geq 0\).

Therefore, the Hausdorff distance between the two problems is:

\[\mathcal{H}(\mathcal{R}_{\pi_{1}^{E}},\mathcal{R}_{\pi_{2}^{E}})=\sup_{\theta \geq 0}\frac{1}{\max\{1,\theta,0\}}\theta=\sup_{\theta\geq 0}\frac{1}{\max\{1, \theta\}}\theta=1.\]

Obviously, we need a \(\Omega(S)\) samples to spot, if it exists, state \(\overline{s}\), and thus distinguish between \(\mathcal{R}_{\pi_{2}^{E}}\) and \(\mathcal{R}_{\pi_{2}^{E}}\). 

**Proposition B.1**.: _Under the setting of Proposition 3.2, even under Assumption B.1, then an algorithm is \((\epsilon,\delta)\)-PAC only if \(\tau^{E}=\Omega(S)\)._

Proof.: The same proof of Proposition 3.2 works here.

[MISSING_PAGE_EMPTY:22]

and reachable \((s,a)\in\mathcal{S}\times\mathcal{A}\), for all the bounded linear functions \(V\) in class \(\mathcal{V}\), defined as:

\[\mathcal{V}\coloneqq\Big{\{}V:\mathcal{S}\times\llbracket H\rrbracket\to \llbracket-H,+H\rrbracket\,\Big{|}\,V(\cdot)=\max_{a\in\mathcal{A}}\phi(\cdot,a)^ {\intercal}w,\ \|w\|_{2}\leq 2H\sqrt{d}\Big{\}}.\] (2)

To achieve this goal, it will be useful to apply triangle inequality and to bound the following two terms separately:

\[\Big{|}\big{(}\mathbb{P}_{h}-\widehat{\mathbb{P}}\big{)}V_{h+1}(s,a)\Big{|} \leq\Big{|}\big{(}\mathbb{P}_{h}-\overline{\mathbb{P}}\big{)}V_{h+1}(s,a) \Big{|}+\Big{|}\big{(}\mathbb{P}_{h}-\widehat{\mathbb{P}}\big{)}V_{h+1}(s,a) \Big{|}.\]

Lemma B.7 and Lemma B.8, which we now present, serve exactly this purpose.

**Lemma B.7**.: _For any value function \(V\) in the class \(\mathcal{V}\), for any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\), it holds that:_

\[\Big{|}\Big{(}\overline{\mathbb{P}}_{h}-\mathbb{P}_{h}\Big{)}V_{h+1}(s,a) \Big{|}\leq\min\Big{\{}H\sqrt{d}\|\phi(s,a)\|_{\Lambda_{h}^{-1}},2H\Big{\}}.\]

Proof.: We have:

\[\Big{(}\overline{\mathbb{P}}_{h}-\mathbb{P}_{h}\Big{)}V_{h+1}(s,a) =\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\sum_{k=1}^{\tau}\phi(s_{h} ^{k},a_{h}^{k})\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})-\mathbb{P}_{h}V_{h+ 1}(s,a)\] \[\stackrel{{(\ref{eq:1})}}{{=}}\phi(s,a)^{\intercal} \Lambda_{h}^{-1}\sum_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\mathbb{P}_{h}V_{h+ 1}(s_{h}^{k},a_{h}^{k})-\phi(s,a)^{\intercal}\widetilde{w}_{h}\] \[=\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\sum_{k=1}^{\tau}\phi(s_{h} ^{k},a_{h}^{k})\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})-\phi(s,a)^{\intercal }\Lambda_{h}^{-1}\Lambda_{h}\widetilde{w}_{h}\] \[=\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\Big{[}\sum_{k=1}^{\tau} \phi(s_{h}^{k},a_{h}^{k})\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})-\Lambda_{ h}\widetilde{w}_{h}\Big{]}\] \[\stackrel{{(\ref{eq:2})}}{{=}}\phi(s,a)^{\intercal} \Lambda_{h}^{-1}\Big{[}\sum_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\mathbb{P}_{ h}V_{h+1}(s_{h}^{k},a_{h}^{k})\] \[\qquad-I\widetilde{w}_{h}-\sum_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^ {k})\phi(s_{h}^{k},a_{h}^{k})^{\intercal}\widetilde{w}_{h}\Big{]}\] \[\stackrel{{(\ref{eq:3})}}{{=}}\phi(s,a)^{\intercal} \Lambda_{h}^{-1}\bigg{[}\sum_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\mathbb{P}_{ h}V_{h+1}(s_{h}^{k},a_{h}^{k})\] \[\qquad-\sum_{k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\mathbb{P}_{h}V_ {h+1}(s_{h}^{k},a_{h}^{k})-\widetilde{w}_{h}\Big{]}\] \[=-\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\widetilde{w}_{h},\]

where at (1) we have defined vector \(\widetilde{w}_{h}\coloneqq\int_{\mathcal{S}}V_{h+1}(s^{\prime})d\mu_{h}(s^{ \prime})\), at (2) we have used the definition of \(\Lambda_{h}\), and at (3) we have recognized that \(\phi(s_{h}^{k},a_{h}^{k})^{\intercal}\widetilde{w}_{h}=\mathbb{P}_{h}V_{h+1}(s_ {h}^{k},a_{h}^{k})\).

By taking the absolute value, we can write:

\[\Big{|}\big{(}\overline{\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h +1}(s,a)\Big{|} =\big{|}\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\widetilde{w}_{h}\big{|}\] \[\stackrel{{(\ref{eq:4})}}{{\leq}}\|\widetilde{w}_{h} \|_{\Lambda_{h}^{-1}}\|\phi(s,a)\|_{\Lambda_{h}^{-1}}\] \[\stackrel{{(\ref{eq:5})}}{{\leq}}\|\widetilde{w}_{h} \|_{2}\|\phi(s,a)\|_{\Lambda_{h}^{-1}}\] \[\stackrel{{(\ref{eq:6})}}{{\leq}}H\sqrt{d}\|\phi(s,a) \|_{\Lambda_{h}^{-1}},\]

where at (4) we have applied Cauchy-Schwarz's inequality, at (5) we have bounded the quadratic form with the 2-norm and the largest eigenvector of the matrix, i.e., \(\|\widetilde{w}_{h}\|_{\Lambda_{h}^{-1}}=\sqrt{\widetilde{w}_{h}^{\intercal} \Lambda_{h}^{-1}\widetilde{w}_{h}}\leq\sqrt{\sigma}\|\widetilde{w}_{h}\|_{2}\), where \(\sigma\) is the largest eigenvalue of matrix \(\Lambda_{h}^{-1}\), and then we have upper bounded \(\sigma\leq 1\), since \(1\) is the smallest eigenvalue of invertible matrix \(\Lambda_{h}\) (see [23]); finally, at (6) we have used the fact that \(|V_{h+1}(\cdot)|\leq H\), and so that \(\|\widetilde{w}_{h}\|_{2}=\|\int_{\mathcal{S}}V_{h+1}(s^{\prime})d\mu_{h}(s^{ \prime})\|_{2}\leq H\|\mu_{h}(\mathcal{S})\|_{2}\leq H\sqrt{d}\).

The result follows by noticing that the quantity to bound cannot be larger than \(2H\). 

**Lemma B.8**.: _Let \(\delta\in(0,1)\). For any value function \(V\) in the class \(\mathcal{V}\), for any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\), with probability at least \(1-\delta/2\), it holds that:_

\[\Big{|}\Big{(}\widehat{\mathbb{P}}_{h}-\overline{\mathbb{P}}_{h}\Big{)}V_{h+1} (s,a)\Big{|}\leq\min\bigg{\{}cH\sqrt{d\log\big{(}1+\tau\big{)}+\log\frac{H}{ \delta}}\|\phi(s,a)\|_{\Lambda_{h}^{-1}},2H\bigg{\}},\]

_for some constant \(c\)._

Proof.: We can write:

\[\Big{|}\Big{(}\widehat{\mathbb{P}}_{h}-\overline{\mathbb{P}}_{h} \Big{)} V_{h+1}(s,a)\Big{|}=\bigg{|}\phi(s,a)^{\intercal}\Lambda_{h}^{-1}\sum_ {k=1}^{\tau}\phi(s_{h}^{k},a_{h}^{k})\Big{[}V_{h+1}(s_{h+1}^{k})-\mathbb{P}_{h }V_{h+1}(s_{h}^{k},a_{h}^{k})\Big{]}\bigg{|}\] \[\overset{\eqref{eq:cH}}{\lessgtr}\Big{\|}\sum_{k=1}^{\tau}\phi( s_{h}^{k},a_{h}^{k})\Big{[}V_{h+1}(s_{h+1}^{k})-\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h} ^{k})\Big{]}\Big{\|}_{\Lambda_{h}^{-1}}\Big{\|}\phi(s,a)\Big{\|}_{\Lambda_{h}^ {-1}}\] \[\overset{\eqref{eq:cH}}{\lessgtr}\sqrt{4H^{2}\Big{(}\frac{d}{2} \log(1+\tau)+\log\frac{2\mathcal{N}_{\epsilon}}{\delta}\Big{)}+8\tau^{2} \epsilon^{2}}\Big{|}\phi(s,a)\Big{|}_{\Lambda_{h}^{-1}}\] \[\overset{\eqref{eq:cH}}{\lessgtr}\sqrt{4H^{2}\Big{(}\frac{d}{2} \log(1+\tau)+2d\log\Big{(}1+\frac{H\sqrt{d}}{\epsilon}\Big{)}+\log\frac{1}{ \delta}\Big{)}+8\tau^{2}\epsilon^{2}}\Big{|}\phi(s,a)\Big{\|}_{\Lambda_{h}^{- 1}}\] \[\overset{\eqref{eq:cH}}{\lessgtr}cH\sqrt{d\log(1+\tau)+\log \frac{1}{\delta}}\Big{\|}\phi(s,a)\Big{\|}_{\Lambda_{h}^{-1}},\]

where at (1) we have applied Cauchy-Schwarz's inequality, at (2) we have applied Lemma B.13, at (3) we have upper bounded \(\mathcal{N}_{\epsilon}\) using Lemma B.12, at (4), similarly to [62], unlike [23], we see that no union bound is needed (because there is no dependence on \(\Lambda\)), thus by choosing \(\epsilon=H\sqrt{d}/\tau\), we get the passage. Passage (5) follows for some constant \(c\).

The result follows by a union bound over \(h\in[\![H]\!]\), and by noticing that the quantity to bound cannot be larger than \(2H\). 

We are now ready to upper bound the Hausdorff distance using the two lemmas just presented. Recall that we work with unbounded rewards (parameters \(\theta\)), and that the definition of inner distance \(d\) is provided in Equation (1).

**Lemma B.9**.: _With probability at least \(1-\delta/2\), the Hausdorff distance between the true feasible set \(\mathcal{R}_{p,\pi^{E}}\) and its estimate \(\widehat{\mathcal{R}}\) returned by Algorithm 1 can be upper bounded by:_

\[\mathcal{H}(\mathcal{R}_{p,\pi^{E}},\widehat{\mathcal{R}})\leq 4J^{*}(u;p),\]

_where \(u_{h}(s,a):=\min\{\beta\|\phi(s,a)\|_{\Lambda_{h}^{-1}},H\}\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\), and \(\beta:=cH\sqrt{d\log(1+\tau)+\log(H/\delta)}\) for some absolute constant \(c>0\)._

Proof.: Let us begin to bound the first branch of the Hausdorff distance.

\[\sup_{r\in\mathcal{R}_{p,\pi^{E}}}\inf_{\widehat{r}\in\widehat{ \mathcal{R}}}d(r,\widehat{r})=\sup_{r\in\mathcal{R}_{p,\pi^{E}}}\inf_{ \widehat{r}\in\widehat{\mathcal{R}}}\frac{1}{M_{r,\widehat{r}}}\sup_{r\in \mathbb{N}}\sum_{h\in[\![H]\!]}\operatorname*{\mathbb{E}}_{h\in[\![H]\!]}|r_{h} (s,a)-\widehat{r}_{h}(s,a)|\] \[\overset{\eqref{eq:cH}}{\lessgtr}\sup_{r\in\mathcal{R}_{p, \pi^{E}}}\inf_{\widehat{r}\in\widehat{\mathcal{R}}}\frac{1}{M_{r,\widehat{r}}} \sup_{r\in\mathbb{N}}\sum_{h\in[\![H]\!]}\operatorname*{\mathbb{E}}_{(s,a) \sim d_{h}^{p,\pi^{*}}(\cdot,\cdot)}\big{|}Q_{h}^{*}(s,a;p,r)-\mathbb{P}_{h}V _{h+1}^{*}(s,a;p,r)\] \[\qquad\qquad-Q_{h}^{*}(s,a;\widehat{p},\widehat{r})+\widehat{ \mathbb{P}}_{h}V_{h+1}^{*}(s,a;\widehat{p},\widehat{r})\big{|}\] \[\overset{\eqref{eq:cH}}{\lessgtr}\sup_{r\in\mathcal{R}_{p, \pi^{E}}}\frac{1}{M_{r,\widehat{r}}}\sup_{r\in\mathbb{N}}\sum_{h\in[\![H]\!]} \operatorname*{\mathbb{E}}_{(s,a)\sim d_{h}^{p,\pi^{*}}(\cdot,\cdot)}\big{|}Q _{h}^{*}(s,a;p,r)-\mathbb{P}_{h}V_{h+1}^{*}(s,a;p,r)\] \[\qquad\qquad-Q_{h}^{*}(s,a;\widehat{p},\widehat{r})+\widehat{ \mathbb{P}}_{h}V_{h+1}^{*}(s,a;\widehat{p},\widehat{r})\big{|},\]\[\stackrel{{(3)}}{{=}} \sup_{\mathrm{r}\in\mathcal{R}_{p,\pi^{E}}}\frac{1}{M_{r,\tilde{r}}} \sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{p,\pi}(\cdot, )}{\mathbb{E}}\big{[}Q_{h}^{\ast}(s,a;p,r)-\mathbb{P}_{h}V_{h+1}^{\ast}(s,a;p,r)\] \[\qquad-Q_{h}^{\ast}(s,a;p,r)+\widehat{\mathbb{P}}_{h}V_{h+1}^{ \ast}(s,a;p,r)\big{|}\] \[= \sup_{\mathrm{r}\in\mathcal{R}_{p,\pi^{E}}}\frac{1}{M_{r,\tilde{r} }}\sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{p,\pi}(\cdot, )}{\mathbb{E}}\big{[}\big{(}\widehat{\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h +1}^{\ast}(s,a;p,r)\big{|}\] \[\stackrel{{(4)}}{{\leqslant}} \sup_{\mathrm{r}\in\mathcal{R}_{p,\pi^{E}}}\sum_{h\in[\![H]\!]} \underset{(s,a)\sim d_{h}^{p,\pi}(\cdot,)}{\mathbb{E}}\big{[}\big{(}\widehat {\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}\frac{V_{h+1}^{\ast}(s,a;p,r)}{\max\{1, \max_{h}\|\theta_{h}\|_{2}/\sqrt{d}\}}\big{|}\] \[\stackrel{{(5)}}{{=}} \sup_{\mathrm{r}\in\mathcal{R}_{p,\pi^{E}}}\sum_{h\in[\![H]\!]} \underset{(s,a)\sim d_{h}^{p,\pi}(\cdot,\cdot)}{\mathbb{E}}\big{[}\big{(} \widehat{\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h+1}^{\ast}(s,a;p,\frac{r}{K })\big{|}\] \[\stackrel{{(6)}}{{\leqslant}} \sup_{\mathrm{V}\in\mathcal{V}}\sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]} \underset{(s,a)\sim d_{h}^{p,\pi}(\cdot,)}{\mathbb{E}}\big{[}\big{(}\widehat {\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h+1}(s,a)\big{|},\]

where at (1) we have simply applied the Bellman optimality equation twice w.r.t. the reward function, at (2) we have upper bounded the infimum over the second set of rewards \(\widehat{\mathcal{R}}\) with the specific choice of reward \(\tilde{r}\in\widehat{\mathcal{R}}\) provided by Lemma B.11, at (3) we use the property of \(\tilde{r}\) described in Lemma B.11, at (4) we bring term \(1/M_{r,\tilde{r}}\) inside, and then we upper bound it by: \(1/M_{r,\tilde{r}}\coloneqq 1/\max\{\sqrt{d},\max_{h}\|\theta_{h}\|_{2},\max_{h} \|\widetilde{\theta}_{h}\|_{2}\}/\sqrt{d}\leqslant 1/\max\{1,\max_{h}\| \theta_{h}\|_{2}/\sqrt{d}\}\), i.e., by simply removing one of the terms inside the maximum operator at denominator. At (5) we define \(K\coloneqq\max\{1,\max_{h}\|\theta_{h}\|_{2}/\sqrt{d}\}\), and, since the value function is linear in the reward, we apply \(K\) directly to the reward. At (6) we realize that the possible optimal value functions that can be constructed in \(p\) using rewards in \(\mathcal{R}_{p,\pi^{E}}\) normalized by \(K\) are a subset of the value functions in class \(\mathcal{V}\), i.e., of all the possible optimal value functions with parameters \(\|w_{h}\|_{2}\leqslant 2H\sqrt{d}\). This is not trivial since we are working with _unbounded_ rewards \(r\), and thus their parameters \(\{\theta_{h}\}_{h}\) can be any. The normalization by \(K\) permits this in the following manner. For any \(h\in[\![H]\!]\), we have \(r_{h}(\cdot,\cdot)/K=\langle\phi(\cdot,\cdot),\theta_{h}/K\rangle=\langle\phi( \cdot,\cdot),\theta_{h}/\max\{1,\max_{h^{\prime}}\|\theta_{h}\|_{2}/\sqrt{d}\}\rangle\). Therefore, if \(\max_{h^{\prime}}\|\theta_{h^{\prime}}\|_{2}>\sqrt{d}\), then the normalization makes sure that \(\max_{h^{\prime}}\|\theta_{h^{\prime}}\|_{2}=\sqrt{d}\), while if \(\max_{h^{\prime}}\|\theta_{h^{\prime}}\|_{2}<\sqrt{d}\), then the normalization is by \(1\) and it has no effect. In this way, we see that value functions \(V_{h+1}^{\ast}(s,a;p,\frac{r}{K})\) can be created by a simple \(r^{\prime}\) with parameters \(\{\theta_{h}^{\prime}\}_{h}\) with 2-norms bounded by \(\sqrt{d}\). This guarantees that, since by hypothesis of Linear MDPs \(\|\phi(\cdot,\cdot)\|_{2}\leqslant 1\), the value function never exceeds \(H\), and that the norm of the \(Q\)-function parameters \(\{w_{h}^{\pi}\}_{h}\) for any policy \(\pi\) can be bounded as: \(\|w_{h}^{\pi}\|_{2}\leqslant\|\theta_{h}/K\|_{2}+\|\int_{\mathbb{S}}V_{h+1}^{ \pi}(s^{\prime})d\mu_{h}(s^{\prime})\|_{2}\leqslant\sqrt{d}+H\|\mu_{h}( \mathcal{S})\|_{2}\leqslant\sqrt{d}+H\sqrt{d}\leqslant 2H\sqrt{d}\) (similarly to Lemma B.1 of [23]). It should be remarked that class \(\mathcal{V}\) is more general than the actual set of optimal value functions that can be obtained using \(r\in\mathcal{R}_{p,\pi^{E}}\) in \(p\), since such rewards induce optimal value functions for which the optimal action in \(\mathcal{S}^{p,\pi^{E}}\) is always the expert's action/s \(\pi^{E}(s)\).

Notice that the same derivation can be carried out also for the other branch of the Hausdorff distance, ending up with the same expression. Therefore, the last line is an upper bound to the Hausdorff distance:

\[\mathcal{H}_{d}(\mathcal{R}_{p,\pi^{E}},\widehat{\mathcal{R}}) \leqslant\sup_{V\in\mathcal{V}}\sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]} \underset{(s,a)\sim d_{h}^{p,\pi}(\cdot,\cdot)}{\mathbb{E}}\underset{(s,a)\sim d _{h}^{p,\pi}(\cdot,\cdot)}{\mathbb{E}}\big{[}\big{(}\widehat{\mathbb{P}}_{h}- \mathbb{P}_{h}\big{)}V_{h+1}(s,a)\big{|}\] \[\stackrel{{(7)}}{{=}} \sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{ p,\pi}(\cdot,\cdot)}{\mathbb{E}}\sup_{V\in\mathcal{V}}\big{|}\big{(}\widehat {\mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h+1}(s,a)\big{|}\] \[= \sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{ p,\pi}(\cdot,\cdot)}{\mathbb{E}}\sup_{V\in\mathcal{V}}\big{|}\big{(}\widehat{\mathbb{P}}_{h}- \mathbb{P}_{h}\big{)}V_{h+1}(s,a)\pm\overline{\mathbb{P}}_{h}V_{h+1}(s,a) \big{|}\] \[\stackrel{{(8)}}{{\leqslant}} \sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{ p,\pi}(\cdot,\cdot)}{\mathbb{E}}\sup_{V\in\mathcal{V}}\big{|}\big{(}\overline{ \mathbb{P}}_{h}-\mathbb{P}_{h}\big{)}V_{h+1}(s,a)\big{|}+\big{|}\big{(}\widehat {\mathbb{P}}_{h}-\overline{\mathbb{P}}_{h}\big{)}V_{h+1}(s,a)\big{|}\] \[\stackrel{{(9)}}{{\leqslant}} \sup_{\pi\in\Pi}\sum_{h\in[\![H]\!]}\underset{(s,a)\sim d_{h}^{ p,\pi}(\cdot,\cdot)}{\mathbb{E}}\min\bigg{\{}c_{1}H\sqrt{d\log(1+\tau)+\log\frac{H}{\delta}}\| \phi(s,a)\|_{\Lambda_{h}^{-1}},4H\bigg{\}}\]\[\leqslant 4\sup_{\pi\in\Pi}\sum_{h\in[H]}\operatorname*{\mathbb{E}}_{(s,a) \sim d_{h}^{\mu,\pi}(\cdot,\cdot)}\min\big{\{}\underbrace{c_{2}H\sqrt{d\log(1+ \tau)+\log\frac{H}{\delta}}}_{\widetilde{\omega}:\beta}\|\phi(s,a)\|_{\Lambda_{h }^{-1}},H\big{\}}\] \[=4\sup_{\pi\in\Pi}\sum_{h\in[H]}\operatorname*{\mathbb{E}}_{(s,a) \sim d_{h}^{\mu,\pi}(\cdot,\cdot)}\underbrace{\min\big{\{}\beta\|\phi(s,a)\|_{ \Lambda_{h}^{-1}},H\big{\}}}_{\widetilde{\omega}:u_{h}(s,a)}\] \[=4\sup_{\pi\in\Pi}\sum_{h\in[H]}\operatorname*{\mathbb{E}}_{(s,a) \sim d_{h}^{\mu,\pi}(\cdot,\cdot)}u_{h}(s,a)\] \[=4J^{\ast}(u;p),\]

where at (7) we have noticed that class \(\mathcal{V}\) contains the cartesian product of \(H\) sets, one for each stage, and therefore the supremum can be brought inside the summation, at (8) we have applied triangle inequality, at (9) we have applied Lemma B.7 and Lemma B.8 and used some absolute constants \(c_{1},c_{2}>0\), and also the fact that for any numbers \(x,y,w,z\), we have \(\min\{x,y\}+\min\{w,z\}\leqslant\min\{x+w,y+z\}\).

To conclude the proof of the main theorem, we simply have to observe that any RFE algorithm provides a bound to \(J^{\ast}(u^{\prime};p)\) for some \(u^{\prime}\) similar to \(u\). Depending on the RFE algorithm instantiated as sub-routine, the sample complexity of Algorithm 1 varies.

**Theorem 3.3**.: _Assume that \(\pi^{E}\) (along with its support \(\mathcal{S}^{p,\pi^{E}}\)) is known. Then, for any \(\epsilon,\delta\in(0,1)\), Algorithm 1 is \((\epsilon,\delta)\)-PAC for IRL with a number of episodes \(\tau\) upper bounded by:_

\[\tau\leqslant\widetilde{\mathcal{O}}\Big{(}\frac{H^{5}d}{\epsilon^{2}}\Big{(} d+\log\frac{1}{\delta}\Big{)}\Big{)}.\]

Proof.: To get the result, we instantiate Algorithm 1 of [62] as RFE sub-routine. Simply, observe that [62] sets \(\beta^{\prime}\) so that \(\beta^{\prime}\geqslant\widetilde{\beta}:=c^{\prime}H\sqrt{d\log(1+dH\tau)+ \log(H/\delta)}\geqslant\beta\). By Lemma B.9, we know that:

\[\mathcal{H}(\mathcal{R}_{p,\pi^{E}},\widehat{\mathcal{R}}) \leqslant 4\sup_{\pi\in\Pi}\sum_{h\in[H]}\operatorname*{\mathbb{E}}_ {(s,a)\sim d_{h}^{\mu,\pi}(\cdot,\cdot)}\min\big{\{}\beta\|\phi(s,a)\|_{ \Lambda_{h}^{-1}},H\big{\}}\] \[\leqslant 2c_{1}\beta^{\prime}\sum_{h\in[H]}\sup_{\pi\in\Pi} \operatorname*{\mathbb{E}}_{(s,a)\sim d_{h}^{\mu,\pi}(\cdot,\cdot)}\|\phi(s,a) \|_{\Lambda_{h}^{-1}},\]

for some absolute constant \(c_{1}>0\). It should be remarked that the quantity in the last line is, modulo \(c_{1}\), the quantity that [62] bound in the proof of their Theorem 1 using their algorithm. Specifically, by taking:

\[\tau\leqslant\widetilde{\mathcal{O}}\Big{(}\frac{H^{5}d}{\epsilon^{2}}\Big{(} d+\log\frac{1}{\delta}\Big{)}+\frac{H^{6}d^{9/2}}{\epsilon}\log^{4}\frac{1}{ \delta}\Big{)},\]

and a union bound over the two events that hold w.p. \(1-\delta/2\), and re-setting \(\epsilon\gets c_{1}\epsilon\), we get the result. 

Notice that if we run Algorithm 1 of [63] for exploration instead of Algorithm 1 of [62], we obtain:

**Theorem B.10**.: _If we use Algorithm 1 of [63] at Line 1 of Algorithm 1, then for any \(\epsilon,\delta\in(0,1)\), such algorithm is \((\epsilon,\delta)\)-PAC for IRL with a number of episodes \(\tau\) upper bounded by:_

\[\tau\leqslant\widetilde{\mathcal{O}}\bigg{(}\frac{H^{6}d^{3}}{\epsilon^{2}} \log\frac{1}{\delta}\bigg{)}.\]

Proof.: By Lemma B.9, we know that:

\[\mathcal{H}(\mathcal{R}_{p,\pi^{E}},\widehat{\mathcal{R}}) \leqslant 4J^{\ast}(u;p)\] \[=4\sup_{\pi\in\Pi}\sum_{h\in[H]}\operatorname*{\mathbb{E}}_{(s,a) \sim d_{h}^{\mu,\pi}(\cdot,\cdot)}\min\big{\{}\beta\|\phi(s,a)\|_{\Lambda_{h} ^{-1}},H\big{\}},\]for \(\beta:=cH\sqrt{d\log(1+\tau)+\log(H/\delta)}\). Now, let us define, similarly to Appendix A of [63], the quantities \(u^{\prime}_{h}(s,a):=\min\{\beta^{\prime}|\phi(s,a)|_{\Lambda_{h}^{-1}},H\}\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\), and \(\beta^{\prime}:=c^{\prime}dH\sqrt{\log(dH/\delta/\epsilon)}\) for some absolute constant \(c^{\prime}>0\). In addition, set the number of exploration episodes \(\tau\) to \(\tau=c^{\prime\prime}d^{3}H^{6}\log(dH\delta^{-1}\epsilon^{-1})/\epsilon^{2}\), and notice that, for appropriate choices of \(c^{\prime},c^{\prime\prime}\), it holds that: \(\beta^{\prime}\geqslant c^{\prime}dH\sqrt{\log(dH\tau/\delta)}\geqslant\beta :=cH\sqrt{d\log(1+\tau)+\log(1/\delta)}\). This entails that \(u^{\prime}_{h}(s,a)\geqslant u_{h}(s,a)\) at all \(s,a,h\), and so:

\[\mathcal{H}(\mathcal{R}_{p,\pi^{E}},\widehat{\mathcal{R}}) \leqslant cJ^{*}(u^{\prime};p)\] \[=cHJ^{*}(u^{\prime}/H;p)\] \[\overset{(1)}{\leqslant}c_{1}H\sqrt{\frac{d^{3}H^{4}\log\frac{d \tau H}{\delta}}{\tau}}\] \[\overset{(2)}{\leqslant}c_{2}\epsilon,\]

where at (1) we have applied Lemma 3.2 of [63] (reported in Lemma B.14 for simplicity) with some new constant \(c_{1}>0\), and at (2) we have simply replaced \(\tau\) with its value defined in Algorithm 1 of [63].

The result follows by union bound between the two events that hold w.p. \(1-\delta/2\) to get \(1-\delta\), and by noticing that \(c_{2}\) is a constant, thus setting \(\epsilon\gets c_{2}\epsilon\) provides the result. 

**Lemma B.11**.: _Let \(\mathcal{R}_{p,\pi^{E}}\) be the feasible set of policy \(\pi^{E}\) w.r.t. transition models \(p\), and let \(\widehat{\mathcal{R}}\) be its estimate constructed as in Algorithm 1 using the true \(\pi^{E},\mathcal{S}^{p,\pi^{E}}\) (or sets \(\mathcal{Z}\)) and some \(\widehat{p}\). For any reward \(r\in\mathcal{R}_{p,\pi^{E}}\), the reward \(\widehat{r}\) such that, for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\):_

\[\widehat{r}_{h}(s,a)=r_{h}(s,a)+\int_{s^{\prime}\in\mathcal{S}}p_{h}(s^{ \prime}|s,a)V_{h+1}^{*}(s^{\prime};p,r)-\int_{s^{\prime}\in\mathcal{S}} \widehat{p}_{h}(s^{\prime}|s,a)V_{h+1}^{*}(s^{\prime};\widehat{p},\widehat{r }),\]

_belongs to \(\widehat{\mathcal{R}}\). Moreover, observe that: \(Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\widehat{p},\widehat{r})\) at all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\). In addition, for any reward \(\widehat{r}\in\widehat{\mathcal{R}}\), it is possible to construct a reward \(r\) in analogous manner so that \(r\in\mathcal{R}_{p,\pi^{E}}\), and such that \(Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\widehat{p},\widehat{r})\) at all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\)._

Proof.: First, we consider the case when \(\mathcal{S}\) is finite. By rearranging the terms in the definition of \(\widehat{r}\), we see that, for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\):

\[\widehat{r}_{h}(s,a)+\sum_{s^{\prime}\in\mathcal{S}}\widehat{p}_{h}(s^{ \prime}|s,a)V_{h+1}^{*}(s^{\prime};\widehat{p},\widehat{r})=r_{h}(s,a)+\sum_{ s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a)V_{h+1}^{*}(s^{\prime};p,r),\]

which, by the Bellman optimality equation, entails that \(Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\widehat{p},\widehat{r})\).

We recall that \(\widehat{\mathcal{R}}\) is defined as:

\[\widehat{\mathcal{R}}=\big{\{}\widehat{r}\in\mathfrak{R}\,\big{|}\,\forall(s, h)\in\mathcal{S}^{p,\pi^{E}},\forall a\in\mathcal{A}:\underset{a^{\prime}\sim \pi^{E}_{h}(\cdot|s)}{\mathbb{E}}Q_{h}^{*}(s,a^{\prime};\widehat{p},\widehat {r})\geqslant Q_{h}^{*}(s,a;\widehat{p},\widehat{r})\big{\}},\]

while thanks to Lemma B.4, the feasible set \(\mathcal{R}_{p,\pi^{E}}\) can be written as:

\[\mathcal{R}_{p,\pi^{E}}=\big{\{}r\in\mathfrak{R}\,\big{|}\,\forall(s,h)\in \mathcal{S}^{p,\pi^{E}},\forall a\in\mathcal{A}:\underset{a^{\prime}\sim\pi^{E }_{h}(\cdot|s)}{\mathbb{E}}Q_{h}^{*}(s,a^{\prime};p,r)\geqslant Q_{h}^{*}(s,a ;p,r)\big{\}}.\]

It is clear that, if \(Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\widehat{p},\widehat{r})\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[\![H]\!]\), then since \(r\in\mathcal{R}_{p,\pi^{E}}\) we necessarily have \(\widehat{r}\in\widehat{\mathcal{R}}\).

The proof of the opposite case is completely analogous.

In the case with infinite \(\mathcal{S}\), notice that both the feasible set \(\mathcal{R}_{p,\pi^{E}}\) in Lemma B.4 and the definition of \(\widehat{\mathcal{R}}\) in Algorithm 1 make use of the same sets \(\mathcal{Z}\). Thus, we simply make the choice of reward with same \(\mathcal{Z}\) and proceed like in the finite case. 

**Lemma B.12** (Covering Number of Class \(\mathcal{V}\)).: _Let \(\mathcal{V}\) be defined as in Equation (2), and define distance dist in \(\mathcal{V}\) as \(\text{dist}(V,V^{\prime}):=\sup_{s\in\mathcal{S}}|V(s)-V^{\prime}(s)|\). Then, the \(\epsilon\)-covering number \(|\mathcal{N}(\epsilon;\mathcal{V},\text{dist})|\) of set \(\mathcal{V}\) with distance dist can be bounded as:_

\[\log|\mathcal{N}(\epsilon;\mathcal{V},\text{dist})|\leq d\log\Big{(}1+\frac{4H \sqrt{d}}{\epsilon}\Big{)}.\]

Proof.: The proof follows that of Lemma D.6 of [23], but is simpler because of the different form of \(\mathcal{V}\).

For any \(V_{1},V_{2}\in\mathcal{V}\) parametrized by \(w_{1},w_{2}\), we write:

\[\text{dist}(V_{1},V_{2}) =\sup_{s\in\mathcal{S}}\Big{|}\max_{a\in\mathcal{A}}\langle\phi(s,a),w_{1}\rangle-\max_{a\in\mathcal{A}}\langle\phi(s,a),w_{2}\rangle\Big{|}\] \[\stackrel{{(1)}}{{\leq}}\max_{(s,a)\in\mathcal{S} \times\mathcal{A}}\Big{|}\phi(s,a)^{\intercal}(w_{1}-w_{2})\Big{|}\] \[\stackrel{{(2)}}{{\leq}}\sup_{\phi:|\phi|_{2}\leq 1 }\Big{|}\phi^{\intercal}(w_{1}-w_{2})\Big{|}\] \[\stackrel{{(2)}}{{=}}\|w_{1}-w_{2}\|_{2},\]

where at (1) we have used the common bound that the absolute difference of maxima is upper bounded by the maximum of the absolute difference of the two functions, at (2) we have used the fact that the feature map is always bounded by 1 in 2-norm, and at (3) we have recognized the dual norm of the 2-norm, i.e., itself.

If we construct an \(\epsilon\)-cover of \(\mathcal{W}:=\{w\in\mathbb{R}^{d}\,|\,\|w\|_{2}\leq 2H\sqrt{d}\}\) w.r.t. the 2-norm, we get a covering number bounded by \(|\mathcal{N}(\epsilon;\mathcal{W},\|\cdot\|_{2})|\leq(1+4H\sqrt{d}/\epsilon)^{d}\). Clearly, this value upper bounds the covering number of class \(\mathcal{V}\) and the result follows. 

**Lemma B.13** (Lemma D.4 of [23]).: _Let \(\{s_{k}\}_{k=1}^{\infty}\) be a stochastic process on state space \(\mathcal{S}\) with corresponding filtration \(\{\mathcal{F}_{k}\}_{k=\Omega}^{\infty}\). Let \(\{\phi_{k}\}_{k=\Omega}^{\infty}\) be an \(\mathbb{R}^{d}\)-valued stochastic process where \(\phi_{k}\in\mathcal{F}_{k-1}\), and \(|\phi_{k}|_{2}\leq 1\). Let \(\Lambda_{\tau}=I+\sum_{k=1}^{\infty}\phi_{k}\phi_{k}^{\intercal}\). Then, for any \(\delta>0\), with probability at least \(1-\delta\), for all \(\tau\geq 0\), and any \(V\in\mathcal{V}\) so that \(\sup_{s\in\mathcal{S}}|V(s)|\leq H\), we have:_

\[\bigg{\|}\sum_{k=1}^{\tau}\phi_{k}\Big{(}V(s_{k})-\mathbb{E}\left[V(s_{k})| \mathcal{F}_{k-1}\right]\Big{)}\bigg{\|}_{\Lambda_{\tau}^{-1}}\leq 4H^{2} \Big{[}\frac{d}{2}\log(1+\tau)+\log\frac{\mathcal{N}_{\epsilon}}{\delta} \Big{]}+8\tau^{2}\epsilon^{2},\]

_where \(\mathcal{N}_{\epsilon}\) is the \(\epsilon\)-covering number of \(\mathcal{V}\) with respect to the distance \(\text{dist}(V,V^{\prime}):=\sup_{s\in\mathcal{S}}|V(s)-V^{\prime}(s)|\)._

**Lemma B.14** (Lemma 3.2 of [63]).: _With probability \(1-\delta/2\), for the function \(u^{\prime}\) defined as \(u^{\prime}_{h}(s,a):=\min\left\{\beta^{\prime}|\phi(s,a)|_{\Lambda_{h}^{-1}}, H\right\}\), with \(\beta^{\prime}:=c^{\prime}dH\sqrt{\log(dH\delta^{-1}\epsilon^{-1})}\), we have:_

\[J^{*}(u^{\prime}/H)\leq c\sqrt{\frac{d^{3}H^{4}\log\frac{d\tau H}{\delta}}{ \tau}},\]

_for some absolute constant \(c>0\)._

## Appendix C Additional Insights on Compatibility

In this appendix, we collect and describe additional insights to the notion of _rewards compatibility_ introduced in Section 4. The appendix is organized in the following manner: Appendix C.1 provides a visual explanation to the notion of rewards compatibility, in Appendix C.2 we analyse a multiplicative alternative to the definition of rewards compatibility, and Appendix C.3 discusses the conditions under which a learned reward can be used for "forward" RL, by comparing rewards with small (non)compatibility with rewards learned in previous works.

### A Visual Explanation for Rewards Compatibility

In this appendix, we aim to provide a visual intuition to the notion of rewards compatibility. For this reason, the reader should keep in mind Figure 3.

As explained in Section 4, even in the limit of infinite samples, i.e., even if we know \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\cup\{\pi^{E}\}\) exactly, and even if we assume that the expert is exactly optimal, i.e., \(J^{*}(r^{E};p)-J^{\pi^{E}}(r^{E};p)=0\) (where \(r^{E}\) is the true reward optimized by the expert), then we still do not have idea of how other policies perform. Expert demonstrations only provide information about the performance of a _single_ policy, \(\pi^{E}\), w.r.t. to the reference \(J^{*}(r^{E};p)\) under the unknown \(r^{E}\), i.e., demonstrations say that \(\pi^{E}\) in \(r^{E}\) performs as good as \(J^{*}(r^{E};p)\). But what about other policies? Demonstrations provide no information.

To see this, consider Figure 3, in which each line exemplifies the visitation distribution induced by some policy \(\pi\in\Pi\), and the point in the middle represents the starting state \(s_{0}=d_{0}\). Intuitively, observing \(d^{p,\pi^{E}}\) along with knowing that \(J^{\pi^{E}}(r^{E};p)\) is good (i.e., because of expert demonstrations), does not tell us anything about the distribution \(d^{p,\pi}\) induced by some other policy \(\overline{\pi}\) potentially arbitrarily different from \(d^{p,\pi^{E}}\). Indeed, it might be the case that \(J^{\overline{\pi}}(r^{E};p)\) is acceptable, or that it is as good as \(J^{\pi^{E}}(r^{E};p)\), or that it is very bad. We cannot know from demonstrations only.

For this reason, if we consider the set of rewards with 0-(non)compatibility, i.e., the feasible reward set, we notice that it contains the rewards \(r\) that make \(\overline{\pi}\) optimal \(J^{\overline{\pi}}(r;p)=J^{*}(r;p)\), but also the rewards \(r^{\prime}\) that make \(\overline{\pi}\) nearly optimal \(J^{\overline{\pi}}(r^{\prime};p)\approx J^{*}(r^{\prime};p)\), and also the rewards \(r^{\prime\prime}\) that make \(\overline{\pi}\) a very bad-performing policy \(J^{\overline{\pi}}(r^{\prime\prime};p)\ll J^{*}(r^{\prime\prime};p)\). Indeed, as long as both \(r,r^{\prime},r^{\prime\prime}\) make the direction pointed by \(d^{p,\pi^{E}}\) in Figure 3 a good direction, then they are in accordance with the constraint imposed by the demonstrations. The additional Degrees of Freedom (DoF) provided by policies beyond \(\pi^{E}\) (e.g., \(\overline{\pi},\dots\)) permit the ill-posedness of IRL.

We said that expert demonstrations provide information just about the performance of a single policy, \(\pi^{E}\). However, to be precise, in the context of IRL, this is not correct. Indeed, differently from the mere learning from demonstrations setting, in which we just assume that \(\pi^{E}\) is a very good-performing policy, in IRL we assume that the underlying problem is an MDP, i.e., that the expert agent is optimizing a reward function \(r^{E}\).10 This additional structure (i.e., that the underlying environment is indeed an MDP), makes sure that the performances of various directions \(d^{p,\pi}\) in Figure 3 are measured through a dot product with a fixed reward function \(r\), i.e.:

Footnote 10: When this assumption does not hold, we incur in model misspecification [55, 51].

\[J^{\pi}(r;p)=\sum_{h\in\llbracket H\rrbracket}\langle d^{p,\pi}_{h},r_{h}\rangle.\]

For this reason, we have the guarantee that the directions in the red area surrounding \(d^{p,\pi^{E}}\) are almost as good as \(d^{p,\pi^{E}}\). Indeed, for all policies \(\pi\) such that \(\sum_{h\in\llbracket H\rrbracket}\|d^{p,\pi}_{h}-d^{p,\pi^{E}}_{h}\|_{1}\leq\epsilon\), i.e., for all policies

Figure 3: In this figure, the point at the center represents the initial state \(s_{0}=d_{0}\) of the environment \(\mathcal{M}\), and each ray starting from it represents the occupancy measure \(d^{p,\pi}\) of some policy \(\pi\). The figure aims to provide the intuition that policies with rays close to each other induce similar visit distributions (e.g., both point towards the same direction in some grid-world), and policies with rays far away from each other point toward very different directions (i.e., they have different occupancy measures). The red area in the right denotes the set of directions (occupancy measures \(d^{p,\pi}\) for some \(\pi\)) that are close in \(\|\cdot\|_{1}\) norm to the direction of the expert \(d^{p,\pi^{E}}\).

\(\epsilon\)-close to \(\pi^{E}\) in 1-norm, we can write:

\[|J^{\pi^{E}}(r^{E};p)-J^{\pi}(r^{E};p)|=\Big{|}\sum_{h\in[H]}\langle{d_{h}^{p,\pi ^{E}}}-d_{h}^{p,\pi},r_{h}^{E}\rangle\Big{|}\leq\sum_{h\in[H]}\|d_{h}^{p,\pi}-d_{ h}^{p,\pi^{E}}\|_{1}\leq\epsilon.\]

In other words, policies \(\pi\) and \(\pi^{E}\) have similar performances.

However, it should be remarked that, since we aim to recover the rewards explaining the expert's preferences, then we are guaranteed that policies close in 1-norm perform similarly under any reward function (by definition of 1-norm), and so we do not risk to incur in the error of representing \(d^{p,\pi^{E}}\) and a direction \(d^{p,\pi}\) inside the red area of Figure 3 with very different performances.

### A Multiplicative Compatibility

In Section 4, we have defined an _additive_ notion of (non)compatibility, based on the difference of performance between \(\pi^{E}\) and \(\pi\)* (the optimal policy). Here, we analyze a _multiplicative_ notion of (non)compatibility, based on the ratio of the performances.11

Footnote 11: E.g., see Theorem 7.2.7 in [45], which is inspired by [43].

We make the following observation. Any reward \(r\in\mathfrak{R}\) induces, in the considered environment \(p\), an ordering in the space of policies \(\Pi\), based on the performance \(J^{\pi}(r;p)\) of each policy \(\pi\in\Pi\). It is easy to notice that for any scaling and translation parameters \(\alpha\in\mathbb{R}_{>0},\beta\in\mathbb{R}\), the reward constructed as \(r^{\prime}(\cdot,\cdot)=\alpha r(\cdot,\cdot)+\beta\) induces the same ordering as \(r\) in the space of policies.12

Footnote 12: Indeed, simply observe that, for any \(\pi\in\Pi\): \(J^{\pi}(r^{\prime};p)=J^{\pi}(\alpha r+\beta;p)=\alpha J^{\pi}(r;p)+\beta\).

For this reason, it seems desirable to use a notion of (non)compatibility such that rewards \(r\) and \(r^{\prime}(\cdot,\cdot)=\alpha r(\cdot,\cdot)+\beta\) for some \(\alpha,\beta\), suffer from the same (non)compatibility w.r.t. some expert policy \(\pi^{E}\). However, observe that, for the notion of compatibility \(\overline{\mathcal{C}}\) in Definition 4.1, we have that, for any \(r\in\mathfrak{R}\):

\[\overline{\mathcal{C}}_{p,\pi^{E}}(r+\beta)=\overline{\mathcal{C }}_{p,\pi^{E}}(r)\qquad\forall\beta\in\mathbb{R},\] \[\overline{\mathcal{C}}_{p,\pi^{E}}(\alpha r)=\alpha\overline{ \mathcal{C}}_{p,\pi^{E}}(r)\neq\overline{\mathcal{C}}_{p,\pi^{E}}(r)\qquad \forall\alpha\in\mathbb{R}_{>0}.\]

Simply put, for the additive notion of (non)compatibility \(\overline{\mathcal{C}}\), the scale (\(\alpha\)) of a reward matters, and rescaling the reward modifies the (non)compatibility.

To solve this issue, one might introduce a _multiplicative_ notion of compatibility \(\mathcal{F}\) (defined only for non-negative rewards and setting \(\mathcal{F}_{p,\pi^{E}}(r)=0\) when the denominator is 0):

\[\mathcal{F}_{p,\pi^{E}}(r)\coloneqq\frac{J^{\pi^{E}}(r;p)}{J^{\bullet}(r;p)}.\]

Clearly, the larger \(\mathcal{F}_{p,\pi^{E}}(r)\), the closer is the performance of \(\pi^{E}\) to the optimal performance. Observe that, for this definition,we have:

\[\mathcal{F}_{p,\pi^{E}}(\alpha r)=\mathcal{F}_{p,\pi^{E}}(r) \qquad\forall\alpha\in\mathbb{R}_{>0}\] \[\mathcal{F}_{p,\pi^{E}}(r+\beta)\neq\mathcal{F}_{p,\pi^{E}}(r) \qquad\forall\beta\in\mathbb{R},\]

i.e., this definition does not care about the scaling \(\alpha\) of the reward, but it is sensitive to the actual position \(\beta\) of that reward.

Therefore, both \(\overline{\mathcal{C}}\) and \(\mathcal{F}\) suffer from some "rescaling" issues. Is it possible to devise a notion of compatibility, i.e., a measure of suboptimality, for a policy, that is independent of both the scale \(\alpha\) and position \(\beta\)? Formally, we are looking for a function (notion of distance) \(f:\mathbb{R}\times\mathbb{R}\to\mathbb{R}_{\geqslant 0}\) such that, for any \(J_{1},J_{2}\in\mathbb{R}\):

\[f(\alpha J_{1}+\beta,\alpha J_{2}+\beta)=f(J_{1},J_{2}),\] (3)

for all \(\alpha\in\mathbb{R}_{>0},\beta\in\mathbb{R}\). Unfortunately, this is not possible, since it is easy to show that all the functions \(f\) of this kind are of the following type:

\[\forall J_{1},J_{2}\in\mathbb{R}\times\mathbb{R}:\ f(J_{1},J_{2})=\begin{cases} K_{+}&\text{ if }J_{1}>J_{2}\\ K_{0}&\text{ if }J_{1}=J_{2}\\ K_{-}&\text{ if }J_{1}<J_{2}\end{cases},\]for some reals \(K_{+},K_{0},K_{-}\). In words, any function \(f\) that satisfies Equation (3) is able to express just an ordering between inputs \(J_{1}\) and \(J_{2}\), but not an actual measure of sub-optimality/compatibility.

We conclude by stating that we prefer to use \(\overline{\mathcal{C}}\) instead of \(\mathcal{F}\) for the following reasons:

* First, most RL literature prefers the additive notion of suboptimality towards the multiplicative one.
* The additive notion of suboptimality is simpler to analyze w.r.t. the multiplicative one.

### When can a learned reward be used for "forward" RL?

In this appendix, we exploit the intuition developed in Appendix C.1 to discuss under which conditions we can exploit demonstrations alone to recover a single reward that _can be used for "forward" RL_, i.e., to recover a single reward \(r\) for which we have the guarantee that any \(\epsilon\)-optimal policy \(\pi\) to \(r\) in the true environment \(p\) has similar performance in the same environment \(p\) under the true reward \(r^{E}\), that is, policy \(\pi\) is an \(f(\epsilon)\)-optimal policy to \(r^{E}\) in \(p\), for some function \(f\).

Applications of IRL range from Apprenticeship Learning (AL), to reward design, to interpretability of expert's preferences. Concerning AL, it is common to "use" the reward \(r\) learned through IRL to optimize our learning agent. But what properties \(r\) should satisfy in order to obtain performance guarantees on our learning agent w.r.t. the true (unknown) \(r^{E}\)? We now list and analyze various plausible requirements.

* First, we might ask that, being \(\pi^{E}\) optimal w.r.t. \(r^{E}\), then \(\pi^{E}\in\operatorname*{arg\,max}_{\pi}J^{\pi}(r)\), i.e., that the expert policy \(\pi^{E}\) is optimal under the learned reward \(r\). However, this requirement is not satisfactory for the following reason. Reward \(r\) might induce more than one optimal policy (e.g., it might induce both \(\overline{\pi},\pi^{E}\) as optimal), and optimal policies other than \(\pi^{E}\) (e.g., \(\overline{\pi}\)) are not guaranteed to perform well under \(r^{E}\) (actually, \(\overline{\pi}\) can be any policy in \(\Pi\)). Clearly, this is not satisfactory. Observe that there are rewards in the feasible set \(\mathcal{R}_{p,\pi^{E}}\) for which multiple policies are optimal (thus, not all the rewards in the feasible set are satisfactory).
* We might additionally ask that \(\pi^{E}\) is the unique optimal policy of reward \(r\) (similarly to what happens in entropy-regularized MDPs [70, 15]). However, this is not satisfactory for the following reason. In practice, it is really difficult (almost impossible) to compute the optimal policy of a given reward. Thus, what is usually done in RL, is to settle for an \(\epsilon\)-optimal policy. Since any policy can be \(\epsilon\)-optimal under reward \(r\), then no guarantee we can have for such policy w.r.t. \(r^{E}\).
* What if we ask that \(\pi^{E}\) is at least \(\epsilon\)-optimal under \(r\) (i.e., the requirement provided by \(\epsilon\)-(non)compatible rewards)? Well, this is not satisfactory because optimal policies can be any, and because there might be other \(\epsilon\)-optimal policies that can perform arbitrarily bad under \(r^{E}\).

All the three requirements described above on \(r\) do not provide guarantees that optimizing the considered reward \(r\) provides a policy with satisfactory performance w.r.t. the true \(r^{E}\). However, as mentioned in Section 4 and in Appendix C.1, expert demonstrations _do not provide any information about the performance of policies other than \(\pi^{E}\) under \(r^{E}\)_.

**Remark C.1**.: _If we want to be sure that an \(\epsilon\)-optimal policy \(\pi\) for the learned reward \(r\) in \(p\) is if \(f(\epsilon)\)-optimal for \(r^{E}\) in \(p\) (for some function \(f\)), then, clearly, we need that all the (at least) \(\epsilon\)-optimal policies under the learned \(r\) have visitation distribution close to that of \(\pi^{E}\) in 1-norm (see Appendix C.1)._

We stress that many IRL algorithms for AL, like max-margin [2], learn a reward function just as a mere mathematical tool to compute a policy \(\pi\) which is close in 1-norm \(\|d^{\pi}-d^{\pi^{E}}\|_{1}\) to \(\pi^{E}\).

A remark about works on the feasible set.If we look at recent works about the feasible set [38, 31, 68], it might seem that these works are able to provide guarantees between \(r,r^{E}\) under distance \(d^{all}\) (see Section 3.1 of [68]), defined as:

\[d^{all}(r,r^{E}):=\sup_{\pi\in\Pi}|J^{\pi}(r)-J^{\pi}(r^{E})|.\]If \(d^{all}(r,r^{E})\) is small, then _the performance of any policy in \(r\), not just optimal policy or \(\epsilon\)-optimal policy, is similar also under \(r^{E}\)_. In other words, if we use/optimize reward \(r\), then we have the guarantee that the performance of the retrieved policy under \(r^{E}\) is more or less the same as its performance in \(r\). Therefore, clearly, _rewards \(r\) with small distance to \(r^{E}\) w.r.t. \(d^{all}\)_ can _be used for "forward" RL._ However, we have the following result:

**Proposition C.1**.: _Let \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\) be a known MDP without reward, and let \(\pi^{E}\) be a known expert's policy. Let \(r^{E}\) the true unknown reward optimized by the expert to construct \(\pi^{E}\). Then, there does not exist a learning algorithm that receives in input the pair \((\mathcal{M},\pi^{E})\) and outputs a single reward \(r\) such that \(d^{all}(r,r^{E})\leq\epsilon\) w.p. \(1-\delta\)._

Proof.: The proof is trivial. Indeed, since the feasible set \(\mathcal{R}_{p,\pi^{E}}\) contains an infinite amount of reward functions along with \(r^{E}\), and the learning algorithm cannot discriminate \(r^{E}\) inside \(\mathcal{R}_{p,\pi^{E}}\), then the best it can do is to output an arbitrary reward function \(r\in\mathcal{R}_{p,\pi^{E}}\). However, since \(\mathcal{R}_{p,\pi^{E}}\) contains, for any reward \(r\in\mathcal{R}_{p,\pi^{E}}\), at least another reward \(r^{\prime}\in\mathcal{R}_{p,\pi^{E}}\) such that \(d^{all}(r,r^{\prime})=c\) is finite and equal to some positive constant \(c>0\),13 then we can simply construct the problem instance with \(r^{E}:=r^{\prime}\) to make the learning algorithm not able to output rewards that can be used for forward learning. 

Footnote 13: This is immediate from the considerations in Appendix C.1.

Nevertheless, [38; 31; 68] seem to provide sample efficient algorithms w.r.t. \(d^{all}\).14 By looking at Proposition C.1, we realize that this is clearly a _contradiction_. What is the right interpretation?

Footnote 14: Actually, [38; 31] use different notions of distance, like \(d_{\infty}(r,r^{\prime}):=\left\lVert r-r^{\prime}\right\rVert_{\infty}\). However, we can write \(\left\lVert r-r^{\prime}\right\rVert_{\infty}\geq\left\lVert r-r^{\prime} \right\rVert_{1}/(SAH)\), and by dual norms we have that \(d^{all}(r,r^{\prime})=\sup_{\pi\in\Pi}|\langle d^{p,\pi},r-r^{\prime}\rangle| \leq\sup_{\overline{d}:\left\lVert\overline{d}\right\rVert_{\infty}\leq 1}| \langle\overline{d},r-r^{\prime}\rangle|=\left\lVert r-r^{\prime}\right\rVert_{1}\). Therefore, the guarantees of [38; 31] can be converted too \(d^{all}\) guarantees too.

The trick is that the algorithms proposed in works [68; 38; 31] are _not_ able to output a single reward \(r\) which is close to \(r^{E}\) w.r.t. \(d^{all}\), but, _for any possible reward \(r^{E}=r^{E}(V,A)\) parametrized15 by some value and advantage functions \(V,A\), they are able to output a reward \(r\) such that \(d^{all}(r,r^{E}(V,A))\) is small_. In other words, it is like if these works _assume to know_ the \(V,A\) parametrization of the true reward \(r^{E}\). Simply put, these works are able to output a reward \(r\) that can be used for "forward" RL just under such assumption. Otherwise those algorithms do not provide such guarantee.

Footnote 15: While [68] makes this parametrization explicit, [38; 31] keep the parametrization implicit, but everything is analogous.

Conclusions.To sum up, we conclude that, in general, an arbitrary reward function with small (non)compatibility can _not_ be used for "forward" learning (see Proposition C.1), because we cannot know given demonstrations alone whether the performances assigned by such reward to policies other than the expert policy are meaningful. In addition, for the same reason, we realize that also an arbitrary reward with zero (non)compatibility, i.e., an arbitrary reward in the feasible set, can _not_ be used for "forward" learning.

### Comparing the (non)compatibility of various rewards

In Section 4, we said that rewards \(r\) with smaller values of \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)\) are more compatible with \(\pi^{E}\) in \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\). However, one might provide the following "counter-example":

**Example C.1** (Question by Reviewer KylX).: _Let \(r^{1},r^{2}\) be two rewards such that \(r_{h}^{2}(s,a)=2r_{h}^{1}(s,a)\geq 0\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times\left[H\right]\). Then, clearly, \(\overline{\mathcal{C}}_{p,\pi^{E}}(r^{2})=2\overline{\mathcal{C}}_{p,\pi^{E}} (r^{1})\). Therefore, based on Section 4, we say that reward \(r^{1}\) is more compatible than \(r^{2}\) w.r.t. \(\pi^{E}\) in \(\mathcal{M}\). However, since \(r^{2}\) is just \(r^{1}\) re-scaled by a constant, the two MDPs \(\mathcal{M}\cup\{r^{1}\}\) and \(\mathcal{M}\cup\{r^{2}\}\) should be "equivalent", thus, \(r^{1}\) and \(r^{2}\) should be, intuitively, equally compatible with \(\pi^{E}\)._

However, Example C.1 misleads the correct _interpretation_ of the notion of reward function in MDPs, and in particular about the _scale_ of the rewards. Let us explain better our point.

The MDP is a model, i.e., a simplified representation of reality, which is commonly applied to 2 different kinds of real-world scenarios: \((i)\) problems in which the agent (learner in RL or expert in IRL) actually receives some kind of scalar feedback from the environment, which can be modelled as a reward function; \((ii)\) problems in which the agent does not receive a feedback from the environment,but its objective, i.e., its structure of preferences among state-action trajectories (which trajectories are better than others), satisfies some axioms that permit to represent it through a scalar reward [52; 9] (this is referred to as the Reward Hypothesis in literature [58]).

There is an enormous difference between scenario \((i)\) and scenario \((ii)\). In \((i)\) the notion of \(\epsilon\)-optimal policy is well-defined for any fixed \(\epsilon>0\), because the reward function is given and, thus, fixed. Instead, in \((ii)\), the notion of reward function is a mere mathematical artifact used to represent preferences among trajectories, whose existence is guaranteed by a set of assumptions/axioms [52; 9; 58]. As Example C.1 shows, positive affine transformations of the reward do not affect the structure of preferences represented (see [52] or Section 16.2 of [50] or [30]). Therefore, in \((ii)\), the notion of \(\epsilon\)-optimal policy is not well-defined, because rescaling a reward function \(r\) to \(kr\) changes the suboptimality of some policy \(\pi\) from \(\epsilon\) to \(k\epsilon\). In other words, for fixed \(\epsilon>0\), any policy can be made \(\epsilon\)-optimal by simply rescaling a reward \(r\) to \(kr\) for some small enough \(k>0\).

In IRL, this issue is even more influential because, although we are in setting \((i)\), we have no idea on the scale of the true reward function. For this reason, our solution is to attach to any reward \(r\) a notion of compatibility \(\overline{\mathcal{C}}(r)\) which implicitly contains information about the scale of the reward \(r\). Compatibilities of different rewards (e.g., \(r^{1}\) and \(r^{2}\) in Example C.1) cannot be compared unless the rewards have the same scale (e.g., \(r^{1}\) and \(r^{2}\) have different scales, thus their compatibilities shall not be compared).

It should be observed that in Appendix C.2 we discuss a notion of compatibility independent of the scale of the reward. However, we show that it suffers from major drawbacks that make the notion of compatibility introduced in the main paper (Definition 4.1) more suitable for the IRL problem.

In conclusion, to settle Example C.1, rewards \(r^{1}\) and \(r^{2}\) should not have the same compatibility, because they have different scales, and the notion of compatibility (i.e., suboptimality) is strictly connected to the scale of the reward. To carry out a fair comparison of compatibilities, one should rescale the compatibility of each reward based on the scale of the reward.

## Appendix D Missing Proofs and Additional Results for Section 5

This appendix is organized as follows. First, we report the full pseudo-code of CATY-IRL. Then, we provide the proof of Theorem 5.1 in Appendix D.2.

### Algorithm

In this section, we provide the extended version of CATY-IRL containing the explicit conditions under which we shall instantiate one BPI/RFE algorithm instead of another.

``` Data: Failure probability \(\delta>0\), target accuracy \(\epsilon>0\), expert demonstrations \(\mathcal{D}^{E}\), set of rewards to classify \(\mathcal{R}\), problem structure \(\imath\in\{\text{tabular},\,\text{linear rewards},\,\text{Linear MDP}\}\) if\(\imath\in\{\text{tabular},\,\text{linear rewards}\}\)then
1if\(|\mathcal{R}|\) is a small constantthen
2\(\mathcal{D}\leftarrow\{\}\) for\(r^{\prime}\in\mathcal{R}\)do
3\(\mathcal{D}\leftarrow\mathcal{D}\cup\text{BPI\_Exploration}(\delta,\epsilon/2,r^{ \prime})\) /* Algorithm BPI-UCBVI [37] */
4 end if
5
6else
7\(\mathcal{D}\leftarrow\text{RFE\_Exploration}(\delta,\epsilon/2)\) /* Algorithm RF-Express [37] */
8 end if
9
10else
11\(\mathcal{D}\leftarrow\text{RFE\_Exploration}(\delta,\epsilon/2)\) /* Algorithm RFLin [62] */
12
13 end if
14 Return \(\mathcal{D}\) ```

**Algorithm 2**CATY-IRL- exploration ``` Data: Failure probability \(\delta>0\), target accuracy \(\epsilon>0\), expert demonstrations \(\mathcal{D}^{E}\), classification threshold \(\Delta\in\mathbb{R}\), reward to classify \(r\in\mathcal{R}\), problem structure \(\imath\in\{\text{tabular, linear rewards,}\), Linear MDP}, dataset \(\mathcal{D}\) // Estimate the expert's performance \(\widehat{J}^{E}(r)\) :
1if\(\imath=\text{tabular}\)then
2\(\widehat{d}^{E}\leftarrow\text{empirical estimate of }d^{p,\pi^{E}}\text{ from }\mathcal{D}^{E}\)
3\(\widehat{J}^{E}(r)\leftarrow\sum_{h}\langle\widehat{d}^{E}_{h},r_{h}\rangle\)
4else
5\(\widehat{\psi}^{E}\leftarrow\text{empirical estimate of }\psi^{p,\pi^{E}}\text{ from }\mathcal{D}^{E}\)
6\(\widehat{J}^{E}(r)\leftarrow\sum_{h}\langle\widehat{\psi}^{E}_{h},r_{h}\rangle\)
7 end if
8 // Estimate the optimal performance \(\widehat{J}^{*}(r)\) :
9if\(\imath\in\{\text{tabular},\text{ linear rewards}\}\)then
10if\(|\mathcal{R}|\) is a small constantthen
11\(\widehat{J}^{*}(r)\leftarrow\text{BPI\_Planning}(\mathcal{D},r)\) /* Algorithm BPI-UCBVI [37] */
12
13else
14\(\widehat{J}^{*}(r)\leftarrow\text{RFE\_Planning}(\mathcal{D},r)\) /* Algorithm RF-Express [37] */
15
16 end if
17
18else
19\(\widehat{J}^{*}(r)\leftarrow\text{RFE\_Planning}(\mathcal{D},r)\) /* Algorithm RFLin [62] */
20
21 end if // Classify the reward:
22\(\widehat{\mathcal{C}}(r)\leftarrow\widehat{J}^{*}(r)-\widehat{J}^{E}(r)\)
23 class \(\leftarrow\) True if\(\widehat{\mathcal{C}}(r)\leq\Delta\)else
24return class ```

**Algorithm 3**CATY-IRL- classification

### Proof of Theorem 5.1

Notice that, according to Definition 4.3, an algorithm is \((\epsilon,\delta)\)-PAC for IRL if it computes an estimate \(\epsilon\)-close to the true (non)compatibility w.h.p.. Such definition does not depend on the specific strategy adopted by the algorithm to actually classify the input reward using the computed estimate of (non)compatibility.

Before diving into the proof of Theorem 5.1, we make the following considerations.

In the common tabular MDPs setting without additional structure, we know that the expected utility \(J^{\pi}(r;p)\) of policy \(\pi\) under reward \(r\) in environment with dynamics \(p\) can computed as:

\[J^{\pi}(r;p)=\sum_{h\in\llbracket H\rrbracket}\langle r_{h},d^{p,\pi}_{h}\rangle,\]

where \(d^{p,\pi}_{h}\) is the occupancy measure of policy \(\pi\) in \(p\). It should be remarked that both \(r_{h}\) and \(d^{p,\pi}_{h}\) have \(SA\) components for all \(h\in\llbracket H\rrbracket\).

In tabular MDPs with linear reward functions and in Linear MDPs, the reward function is linear in some feature map \(\phi\), i.e.:

\[r_{h}(\cdot,\cdot)=\langle\phi(\cdot,\cdot),\theta_{h}\rangle\qquad\forall h \in\llbracket H\rrbracket,\]

where \(\|\phi(s,a)\|_{2}\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(\max_{h}\|\theta_{h}\|_{2}\leq\sqrt{d}.\) Using this decomposition, we can rewrite the expected utility \(J^{\pi}(r;p)\) as:

\[J^{\pi}(r;p)=\sum_{h\in\llbracket H\rrbracket}\langle r_{h},d^{p,\pi}_{h}\rangle\]\[=\sum_{h\in[\![H]\!]}\langle\theta_{h}^{\intercal}\phi,d_{h}^{p, \pi}\rangle\] \[=\sum_{h\in[\![H]\!]}\theta_{h}^{\intercal}\mathbb{E}_{h,(s,a) \sim d_{h}^{p,\pi}}\phi(s,a)\] \[=\sum_{h\in[\![H]\!]}\theta_{h}^{\intercal}\psi_{h}^{p,\pi},\]

where we have defined the feature expectations \(\{\psi_{h}^{p,\pi}\}_{h\in[\![H]\!]}\) as \(\psi_{h}^{p,\pi}\coloneqq\mathbb{E}_{(s,a)\sim d_{h}^{p,\pi}}\,\phi(s,a)\). Observe that vector \(\psi_{h}^{p,\pi}\) has \(d\) components instead of the \(SA\) components of each \(d_{h}^{p,\pi}\) vector.

Since in our setting the IRL algorithm receives in input the reward function (or its parameter \(\theta\in\mathbb{R}^{d}\)), to estimate the expected utility \(J^{\pi}(r;p)\) we must estimate the visit distributions \(\{d_{h}^{p,\pi}\}_{h}\) or the feature expectations \(\{\psi_{h}^{p,\pi}\}_{h}\). However, because of the different dimensionalities of such quantities (\(SA\) versus \(d\)), the estimates might require different amounts of samples.

**Theorem 5.1** (Sample Complexity of Caty-Irl).: _Let \(\epsilon,\delta\in(0,1)\). Then CATY-IRL is \((\epsilon,\delta)\)-PAC for IRL with a sample complexity upper bounded by:_

\[\text{Tabular MDPs:} \tau^{E}\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}SA}{\epsilon ^{2}}\log\frac{1}{\delta}\Big{)}, \tau\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}SA}{\epsilon^{2}} \Big{(}N+\log\frac{1}{\delta}\Big{)}\Big{)},\] \[\text{Tabular MDPs with linear rewards:} \tau^{E}\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}dA}{\epsilon ^{2}}\log\frac{1}{\delta}\Big{)}, \tau\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}SA}{\epsilon^{2}} \Big{(}N+\log\frac{1}{\delta}\Big{)}\Big{)},\] \[\text{Linear MDPs:} \tau^{E}\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{3}dA}{\epsilon ^{2}}\log\frac{1}{\delta}\Big{)}, \tau\leq\widetilde{\mathcal{O}}\Big{(}\frac{H^{5}dA}{\epsilon^{2}} \Big{(}d+\log\frac{1}{\delta}\Big{)}\Big{)},\]

_where \(N=0\) if \(|\mathcal{R}|=\Theta(1)\), and \(N=S\) otherwise._

Proof.: To prove the theorem, we aim to find a bound to the number of samples \(\tau^{E}\) such that the estimate \(\widehat{J}^{E}(r)\approx J^{\pi^{E}}(r;p)\) is \(\epsilon/2\)-correct with probability at least \(1-\delta/2\). Next, similarly, we aim to bound \(\tau\) so that \(\widehat{J}^{\star}(r)\approx J^{\star}(r;p)\) is \(\epsilon/2\)-correct with probability at least \(1-\delta/2\). Then, the conclusion follows after performing a union bound and observing that, for any \(r\in\mathcal{R}\):

\[\Big{|}\overline{c}_{p,\pi^{E}}(r)-\widehat{\mathcal{C}}(r)\Big{|} =\Big{|}\Big{(}J^{\star}(r;p)-J^{\pi^{E}}(r;p)\Big{)}-\Big{(} \widehat{J}^{\star}(r)-\widehat{J}^{E}(r)\Big{)}\Big{|}\] \[\leq\Big{|}J^{\star}(r;p)-\widehat{J}^{\star}(r)\Big{|}+\Big{|}J^ {\pi^{E}}(r;p)-\widehat{J}^{E}(r)\Big{|}\] \[\leq\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon.\]

**Estimating \(\widehat{J}^{E}(r)\approx J^{\pi^{E}}(r;p)\)**

To estimate \(J^{\pi^{E}}(r;p)\), CATY-IRL simply computes the empirical estimate of \(\{d_{h}^{p,\pi^{E}}\}\) in case of tabular MDPs, and the empirical estimate of \(\{\psi_{h}^{p,\pi^{E}}\}\) in case of tabular MDPs with linear rewards and Linear MDPs. Notice that by empirical estimates we mean:

\[\widehat{d}_{h}^{E}(s,a)\coloneqq\frac{\sum\limits_{i\in[\![\tau^{E}]\!]} \mathds{1}\{s_{h}^{i}=s\wedge a_{h}^{i}=a\}}{\sum\limits_{i\in[\![\tau^{E}]\!]} \mathds{1}\{s_{h}^{i}=s\}}\qquad\forall(s,a,h)\in\mathcal{S}\times\mathcal{A }\times[\![H]\!],\]

and:

\[\widehat{\psi}_{h}^{E}\coloneqq\frac{\sum\limits_{i\in[\![\tau^{E}]\!]}\phi(s_ {h}^{i},a_{h}^{i})}{\tau^{E}}\qquad\forall h\in[\![H]\!].\]

Concerning the estimate of the visit distribution \(\widehat{d}^{E}\), we can use the result of Lemma 6 in [53] (we are working with bounded rewards), to obtain that:

\[\sum_{h\in[\![H]\!]}\|{d_{h}^{p,\pi^{E}}-\widehat{d}_{h}^{E}}\|_{1}\leq\sqrt{ \frac{SAH^{3}\log\frac{8SAH}{\delta}}{2\tau^{E}}}\leq\frac{\epsilon}{2}.\]Solving w.r.t. \(\tau^{E}\) we get the bound on \(\tau^{E}\).

In a completely analogous manner, we can bound the feature expectations as:

\[\sum_{h\in\llbracket H\rrbracket}\|\psi_{h}^{p,\pi^{E}}-\widehat{\psi}_{h}^{E} \|_{1}\leq\sqrt{\frac{dH^{3}\log\frac{8dH}{\delta}}{2\tau^{E}}}\leq\frac{ \epsilon}{2}.\]

Again, solving w.r.t. \(\tau^{E}\) we get the bound on \(\tau^{E}\).

**Estimating \(\widehat{J}^{\bullet}(r)\approx J^{\bullet}(r;p)\)**

Let us begin with the case in which \(\mathcal{R}\) is large. As explained for instance in Definition 4 of [66], both algorithms RF-Express [37] and RFLin [62] satisfy the _uniform policy evaluation property_, i.e., they guarantee that, for any \(\epsilon,\delta\in(0,1)\), after having explored for \(\tau\leq\widehat{\mathcal{O}}\Big{(}\frac{H^{3}S\Delta}{\epsilon^{2}}\big{(}S +\log\frac{1}{\delta}\big{)}\Big{)}\) in case of RF-Express [37], and \(\tau\leq\widehat{\mathcal{O}}\Big{(}\frac{H^{5}d}{\epsilon^{2}}\big{(}d+\log \frac{1}{\delta}\big{)}\Big{)}\) for the algorithm in [62] (we omit linear terms in \(1/\epsilon\)), they compute an estimate \(\widehat{p}\approx p\) of the true transition model such that:

\[\mathbb{P}\Big{(}\sup_{r\in\mathfrak{R},\pi\in\Pi}\big{|}J^{\pi}(r;p)-J^{\pi} (r;\widehat{p})\big{|}\leq\epsilon\Big{)}\geq 1-\delta.\]

Clearly, if such property holds, then by computing the performance of the policy \(\widehat{\pi}\) outputted by the RFE algorithm we are able to obtain an \(\epsilon/2\)-correct estimate of \(J^{\bullet}(r;p)\).16

Footnote 16: Actually, for Linear MDPs, instead of evaluating the policy returned by Algorithm 2 of [62], we can simply consider the optimistic estimate of the \(V\)-function computed by such algorithm, which has the property of being \(\epsilon\)-close to the true optimal \(V\)-function.

Concerning the case in which \(|\mathcal{R}|\) is a finite small constant, for tabular and tabular with linear rewards MDPs, we can simply use algorithm BPI-UCBVI of [37] as sub-routine, and run it as many times as there are rewards in \(\mathcal{R}\). When \(|\mathcal{R}|\) is a small constant, we can proceed with a union bound over \(\mathcal{R}\):

\[\mathbb{P}\Big{(}\sup_{r\in\mathfrak{R},\pi\in\Pi}\big{|}J^{\pi}(r;p)-J^{\pi} (r;\widehat{p})\big{|}\leq\epsilon\Big{)}\geq 1-\sum_{r\in\mathcal{R}} \mathbb{P}\Big{(}\sup_{\pi\in\Pi}\big{|}J^{\pi}(r;p)-J^{\pi}(r;\widehat{p}) \big{|}>\epsilon\Big{)}\geq 1-|\mathcal{R}|\delta.\]

This allows us to formally distinguish between small and large \(|\mathcal{R}|\) based on the following inequality:

\[S+\log\frac{1}{\delta}<\log\frac{|\mathcal{R}|}{\delta}\implies S<\log| \mathcal{R}|.\]

## Appendix E Missing Proofs and Additional Results for Section 6.1

This appendix is organized as follows. First, in Appendix E.1, we introduce two problems that share similarities with RFE and IRL, and we characterize the main differences among them. In addition, we enunciate a lower bound to the sample complexity that is common to some of these 4 problems. Next, in Appendix E.2, we provide the missing proofs.

### Four Problems

The 4 problems that we consider here are Reward-Free Exploration (RFE), Inverse Reinforcement Learning (IRL), Matching Performance (MP), and Imitation Learning from Demonstrations alone (ILFO). MP represents a novel generalization of RFE, while ILFO, introduced in [34], represents an exemplification of MP. Before enunciating the minimax lower bound, it is important to formally define each of these problems, as well as what we mean by learning in each problem.

#### e.1.1 Definition of the Problems

In all the 4 problems, the learner is placed into an _unknown_ MDP without reward \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\), i.e., an environment whose dynamics \((d_{0},p)\) is unknown to the learner. For simplicity, w.l.o.g., we assume that there is a single initial state \(s_{0}:=d_{0}\). In each problem, the learner can explore the environment at will to collect samples about the dynamics \(p\), whose knowledge improves the performance of the agent at solving the task. However, at exploration phase, the learner does not know which is the specific task it has to solve. It just knows that the specific task belongs to a given set of tasks \(\mathfrak{T}\) (e.g., set of reward functions). The agent can use the knowledge of \(\mathfrak{T}\) to engage in a more efficient task-driven exploration. For any \(\epsilon,\delta\in(0,1)\), the goal of the agent is to being able to ouputting, for any task in \(\mathfrak{T}\) a quantity \(\mathfrak{o}\) (e.g., a policy) that solves that specific task in an \(\epsilon\)-correct manner with probability at least \(1-\delta\). The ultimate goal of exploration is to collect the least number of samples that permits \((\epsilon,\delta)\)-correctness for all the tasks in \(\mathfrak{T}\).

Now, let us see what the quantities \(\mathfrak{T}\) and \(\mathfrak{o}\) represent in each of the 4 problems. In Table 1, we provide a sum up of the various definitions.

Reward-Free Exploration (RFE).In RFE, the learner receives a set of reward functions \(\mathfrak{T}=\mathcal{R}\subseteq\mathfrak{R}\) in input, and the goal is to exploit the information about \(p\) collected at exploration phase to output, for any reward \(r\in\mathcal{R}\), an \(\epsilon\)-optimal policy \(\mathfrak{o}=\widehat{\pi}_{r}\) w.p. \(1-\delta\). When \(\mathfrak{T}=\{r\}\) is a singleton, the RFE problem is commonly termed the BPI problem. In symbols, any RFE algorithm must guarantee that:

\[\mathbb{P}\Big{(}\sup_{r\in\mathcal{R}}J^{\mathfrak{s}}(r;p)-J^{\widehat{\pi }_{r}}(r;p)\leq\epsilon\Big{)}\geq 1-\delta,\]

where \(\widehat{\pi}_{r}\) is the estimate of the algorithm for reward \(r\).

Inverse Reinforcement Learning (IRL).In IRL, the learner receives in input an occupancy measure17\(\{d_{h}^{p,\pi^{E}}\}_{h\in[H]}\) and a set of reward functions \(\mathcal{R}\subseteq\mathfrak{R}\): \(\mathfrak{T}=(d^{p,\pi^{E}},\mathcal{R})\), but it does not know which specific reward it will have to classify. Under the assumption that the occupancy measure \(d^{p,\pi^{E}}\) is known,18 the problem reduces to exploiting the information about \(p\) collected at exploration phase to output, for any reward \(r\in\mathcal{R}\), an \(\epsilon\)-correct estimate \(\mathfrak{o}=\widehat{J}(r)\) of the optimal utility \(J^{\mathfrak{s}}(r)\) w.p. \(1-\delta\). In symbols, under these conditions, any IRL algorithm must guarantee that:

Footnote 17: Actually, as explained in Section 6.1, the knowledge of \(d^{p,\pi^{E}}\) at exploration phase is useless. The visit measure might be provided after the exploration along with the true reward to classify.

Footnote 18: The assumption that \(d^{p,\pi^{E}}\) is known is useful to reduce the estimation problem of the (non)compatibility of a reward \(\mathcal{C}_{p,\pi^{E}}(r)\coloneqq J^{\mathfrak{s}}(r;p)-J^{\pi^{E}}(r;p)\) to the problem of estimating the optimal utility \(J^{\mathfrak{s}}(r;p)\) only. Indeed, if \(d^{p,\pi^{E}}\) is known, then, for any reward \(r\), the utility \(J^{\pi^{E}}(r;p)\) is known.

\[\mathbb{P}\Big{(}\sup_{r\in\mathcal{R}}J^{\mathfrak{s}}(r;p)-J^{\widehat{\pi }_{r}}(r;p)\leq\epsilon\Big{)}\geq 1-\delta,\]

where \(\widehat{J}(r)\) is the estimate of the algorithm for reward \(r\).

Matching Performance (MP).In MP, the learner receives in input a set of reward functions \(\mathcal{R}\subseteq\mathfrak{R}\) and a measure of performance for each of them \(\overline{J}:\mathcal{R}\rightarrow\mathbb{R}\): \(\mathfrak{T}=(\overline{J},\mathcal{R})\). For any \(r\in\mathcal{R}\), the utility \(\overline{J}(r)\) represents a performance measure for which we aim to find the policy that achieves closest performance. Thus, in MP, the goal is to exploit the information about \(p\) collected at exploration phase to output, for any reward \(r\in\mathcal{R}\), a policy \(\mathfrak{o}=\widehat{\pi}_{r}\) such that, if we denote the policy with performance closest to \(\overline{J}(r)\) by \(\overline{\pi}_{r}\in\arg\min_{\pi}|J^{\pi}(r)-\overline{J}(r)|\), then the utility of policy \(\widehat{\pi}_{r}\) is \(\epsilon\)-close to the utility of policy \(\overline{\pi}_{r}\) w.p. \(1-\delta\). In symbols, any MP algorithm must guarantee that:

\[\mathbb{P}\Big{(}\sup_{r\in\mathcal{R}}|J^{\overline{\pi}_{r}}(r;p)-J^{ \widehat{\pi}_{r}}(r;p)|\leq\epsilon\Big{)}\geq 1-\delta,\]

where \(\overline{\pi}_{r}\in\arg\min_{\pi}|J^{\pi}(r)-\overline{J}(r)|\), and \(\widehat{\pi}_{r}\) is the estimate of the algorithm for reward \(r\).

Imitation Learning from Demonstrations alone (ILfO).In ILfO, the learner receives in input a set of _state-only_ reward functions \(\mathcal{R}\subset\mathfrak{R}\) and a _state-only_ occupancy measure \(\{\overline{d}_{h}\}_{h\in[H]}\): \(\mathfrak{T}=(\overline{d},\mathcal{R})\). Under the assumption that \(\overline{d}\) does not leak any information about the true transition model \(p\), the goal is to exploit the information about \(p\) collected at exploration phase to output, for any reward \(r\in\mathcal{R}\), a policy \(\mathfrak{o}=\widehat{\pi}_{r}\) such that, if we denote the policy with performance closest to \(\overline{J}(r)\coloneqq\sum_{h\in[H]}\langle r_{h},\overline{d}_{h}\rangle\) by \(\overline{\pi}_{r}\in\arg\min_{\pi}|J^{\pi}(r)-\overline{J}(r)|\), then the utility of policy \(\widehat{\pi}_{r}\) is \(\epsilon\)-close to the utility of policy \(\overline{\pi}_{r}\) w.p. \(1-\delta\). Simply_put, ILfO, as defined in this manner, exemplifies the MP setting by providing a functional form to \(\overline{J}:\mathcal{R}\rightarrow\mathbb{R}\) as an inner product between a certain state-only occupancy measure and the input reward. It should be remarked that the assumption made for ILfO is mild, because it is satisfied by the setting in which the expert and the learner have the same state space but different action spaces (or different dynamics). Indeed, in such case, the visit distribution \(\overline{d}\) of the expert would not leak any information about \(p\). In symbols, any ILfO algorithm must guarantee that:

\[\mathbb{P}\Big{(}\sup_{r\in\mathcal{R}}\left|J^{\overline{\pi}_{r}}(r;p)-J^{ \widehat{\pi}_{r}}(r;p)\right|\leq\epsilon\Big{)}\geq 1-\delta,\]

where \(\overline{\pi}_{r}\in\arg\min_{\pi}|J^{\pi}(r)-\overline{J}(r)|\) and \(\overline{J}(r):=\sum_{h\in[H]}\langle\gamma_{h},\overline{d}_{h}\rangle\), and \(\widehat{\pi}_{r}\) is the estimate of the algorithm for reward \(r\).

#### e.1.2 Lower Bound

We now present a minimax lower bound rate that is common to RFE, IRL, and MP. We report here the lower bounds presented in Section 6.1.

**Theorem 6.1** (IRL Classification - Lower Bound).: _Let \(\mathfrak{A}\) be an \((\epsilon,\delta)\)-PAC algorithm for the IRL classification in tabular MDPs. Let \(\tau\) be the number of exploration episodes. Then, there exists an IRL classification instance such that:_

\[\text{if }|\mathcal{R}|\geq 1:\ \tau\geq\Omega\bigg{(}\frac{H^{3}SA}{ \epsilon^{2}}\log\frac{1}{\delta}\bigg{)},\qquad\text{if }\mathcal{R}=\mathfrak{R}:\ \tau\geq \Omega\bigg{(}\frac{H^{3}SA}{\epsilon^{2}}\Big{(}S+\log\frac{1}{\delta} \Big{)}\bigg{)}.\]

Proof.: The proof is similar to that of [38]. We split the proof in two parts, by considering two classes of difficult problem instances in Lemma E.2 and Lemma E.3. Next, we combine the two bounds through \(\max\{a,b\}\geq(a+b)/2\) for all \(a,b\geq 0\). For the proof, we will assume that the expert visit distribution is known. The obtained bound represents a lower bound to the more general setting in which it is unknown. 

**Theorem E.1** (RFE - Refined Lower Bound).: _Let \(\mathfrak{A}\) be an \((\epsilon,\delta)\)-PAC algorithm for RFE in tabular MDPs. Let \(\tau\) be the number of exploration episodes. Then, there exists an RFE instance such that:_

\[\tau\geq\Omega\bigg{(}\frac{H^{3}SA}{\epsilon^{2}}\Big{(}S+\log\frac{1}{ \delta}\Big{)}\bigg{)}.\]

Proof.: The proof of this result is analogous to that of Theorem 6.1, and it employs Lemma E.2 and Lemma E.3. 

Some observations are in order. First, since MP is a more general setting than RFE, then this lower bound is a lower bound for MP too. However, this is not guaranteed for ILfO. We observe that, while for RFE and IRL the bound is tight, for MP we cannot say so because we do not have the upper bound. Notice that, in case the expert state-only distribution \(\overline{d}\) was unknown at exploration phase, and revealed afterwards, then the lower bound of Theorem 6.1 holds for ILfO too, because we might a posteriori reveal the state-only distribution \(\overline{d}\) of the optimal policy, and thus, in such manner, ILfO would be reduced to RFE.

### Missing proofs

**Lemma E.2**.: _Let IRL and RFE be the learning problems defined as in Appendix E.1. Then, for each problem, any \((\epsilon,\delta)\)-PAC algorithm must collect at least the following number of exploration episodes:_

\[\tau\geq\Omega\bigg{(}\frac{H^{3}SA}{\epsilon^{2}}\log\frac{1}{\delta}\bigg{)}.\]

\begin{table}
\begin{tabular}{c c c c c}  & BPI & MP & ILfO \\ \hline Set of Tasks \(\mathfrak{T}\) & \(\mathcal{R}\) & \((d^{p,\pi^{\pi}},\mathcal{R})\) & \((\overline{J},\mathcal{R})\) & \((\overline{d},\overline{R})\) \\ Assumptions & & \(d^{p,\pi^{\pi}}\) known & \(\overline{J}\) can be non-realisable & \(r\) state-only, \(\overline{d}\) no info \\ Output \(\mathfrak{o}\) & \(\widehat{\pi}\) & \(\widehat{J}\) & \(\widehat{\widehat{\pi}}\) & \(\widehat{\widehat{\pi}}\) \\ Goal & \(J^{\widehat{\pi}}(r;p)\approx J^{\pi}(r;p)\) & \(\widehat{J}\approx J^{\pi}(r;p)\) & \(J^{\widehat{\pi}}(r;p)\approx\overline{J}\) & \(J^{\widehat{\pi}}(r;p)\approx\sum_{h}\langle\overline{d}_{h},r_{h}\rangle\) \\ \end{tabular}
\end{table}
Table 1: Summary of the problems.

Proof.: Observe that the proof for RFE is present in [12]. Thus, we have to prove just the result for IRL. For doing so, we will use both the results of [12] and [38]. Notice that for the sake of this proof we consider \(\mathcal{R}=\{r\}\), that will reduce our problem to simple RL as, in order to compute the function \(\overline{\mathcal{C}}_{p,\pi^{E}}(r)\), we just need to compute \(J^{*}(r;p)\), being \({J^{*}}^{\pi^{E}}(r;p)\) known from the availability of \(d^{p,\pi^{E}}\) and \(r\).

**Instances Description** The hard instances considered are exactly the same as [12], and are reported in Figure 4 for simplicity. The only difference is the presence of state \(s_{E}\), to which the expert's policy \(\pi^{E}\) brings, which is absorbing. Such state is needed to make the knowledge of the expert's visit distribution \(d^{p,\pi^{E}}\) useless at inferring information about the transition model in other parts of the state-action space. Based on [12], we describe such hard instances. Similarly to [12], we assume that \(S\geq 7,A\geq 2\), and there exists an integer \(d\) such that \(S=4+(A^{d}-1)/(A-1)\), and we assume that \(H\geq 3d\). Note that [12] show how to relax the assumption on the existence of \(d\).

There are the initial state \(s_{\text{w}}\), from which the agent starts, and states \(s_{g},s_{b}\), respectively, the "good" and "bad" states which are absorbing. Moreover, there is state \(s_{E}\), which is reached by the expert, and is absorbing. The remaining \(S-4\) states are arranged in a full \(A\)-ary tree of depth \(d-1\) with root \(s_{\text{root}}\). We denote by \(\overline{H}\leq H-d\) a certain integer parameter, and by \(\mathcal{L}:=\{s_{1},s_{2},\ldots,s_{L}\}\) the set of leaves of the tree. We define \(\mathcal{I}:=\{1+d,\ldots,\overline{H}+d\}\times\mathcal{L}\times\mathcal{A}\). For any \(\imath\in\mathcal{I}\), we define and MDP \(\mathcal{M}_{\imath}\) as follows. In any state of the tree, i.e., in states \(\mathcal{S}\backslash\{s_{\text{w}},s_{g},s_{b},s_{E}\}\), the transitions are deterministic, and the \(a\)-th action of a state brings to the \(a\)-th child of that node.

The transitions from \(s_{\text{w}}\) are given by

\[p_{h}(s_{\text{w}}|s_{\text{w}},a)\coloneqq\mathds{1}\{a=a_{\text{w}},h \leq\overline{H}\}\quad\text{and}\quad p_{h}(s_{\text{root}}|s_{\text{w}},a) \coloneqq 1-p_{h}(s_{\text{w}}|s_{\text{w}},a).\]

Figure 4: Hard instances.

In other words, action \(a_{\mathrm{w}}\) allows the agent to remain in the initial state \(s_{\mathrm{w}}\) up to stage \(\overline{H}\). After stage \(\overline{H}\), the agent is forced to leave \(s_{\mathrm{w}}\) and to traverse the tree down to the leaves. Action \(a_{E}=\pi_{1}^{E}(s_{\mathrm{w}})\) is the only action that brings to state \(s_{E}\), which is absorbing. The transitions from any leaf \(s_{i}\in\mathcal{L}\) are given, as in [12], by:

\[p_{h}(s_{g}|s_{i},a)\coloneqq\frac{1}{2}+\Delta_{(h^{\bullet}, \ell^{\bullet},a^{\bullet})}(h,s_{i},a)\quad\text{and}\quad p_{h}(s_{b}|s_{i},a )\coloneqq\frac{1}{2}-\Delta_{(h^{\bullet},\ell^{\bullet},a^{\bullet})}(h,s_{i },a),\] (4)

where \(\Delta_{(h^{\bullet},\ell^{\bullet},a^{\bullet})}(h,s_{i},a)\coloneqq\mathds{1 }\{(h,s_{i},a)=(h^{\bullet},s_{\ell^{\bullet}},a^{\bullet})\}\cdot\epsilon^{\prime}\), for some \(\epsilon^{\prime}\in[0,1/2]\). For this reason, there exists a (single) leaf \(\ell^{\bullet}\) where the agent can choose an action \(a^{\bullet}\) at stage \(h^{\bullet}\) to increase its probability of arriving to the good state \(s_{g}\), which provides higher reward. We define states \(s_{g}\) and \(s_{b}\) to be absorbing, i.e., they satisfy \(p_{h}(s_{b}|s_{b},a)\coloneqq p_{h}(s_{g}|s_{g},a)\coloneqq 1\) for any action \(a\). The reward function is state-only and is defined as

\[\forall a\in\mathcal{A},\quad r_{h}(s,a)\coloneqq\mathds{1}\{s=s_{g},h \geq\overline{H}+d+1\},\]

so that even though the agent decides to stay at \(s_{\mathrm{w}}\) until stage \(\overline{H}\), it does not lose any reward. Observe that state \(s_{E}\) does not provide any reward, so that to estimate the (non)compatibility, any algorithm must provide a good estimate of the optimal performance.

Finally, we define a reference MDP \(\mathcal{M}_{0}\) which is an MDP of the above type but for which \(\Delta_{0}(h,s_{i},a)\coloneqq 0\) for all \((h,s_{i},a)\). For certain \(\epsilon^{\prime}\) and \(\overline{H}\) to choose, we define the class \(\mathbb{M}\) to be the set \(\mathbb{M}:=\{\mathcal{M}_{0}\}\cup\{\mathcal{M}_{\iota}\}_{i\in\mathcal{I}}\).

**Distance between problems** We will prove the lower bound for instance \(\mathcal{M}_{0}\). Observe that, in \(\mathcal{M}_{0}\), the optimal utility is:

\[J_{0}^{\bullet}=\frac{1}{2}(H-\overline{H}-d),\]

because there is no triple with additional bias towards \(s_{g}\). Instead, for any other \(\mathcal{M}_{\iota}\in\mathbb{M}\), the optimal utility is:

\[J_{\iota}^{\bullet}=(H-\overline{H}-d)\Big{(}\frac{1}{2}+\epsilon^{\prime} \Big{)}.\]

Therefore, if we choose \(\epsilon^{\prime}\coloneqq 2\epsilon/(H-\overline{H}-d)\), we have that, for any \(\iota\in\mathcal{I}\):

\[\big{|}J_{0}^{\bullet}-J_{\iota}^{\bullet}\big{|}=2\epsilon.\]

Thus, in particular, for any estimate \(\widehat{J}\in\mathbb{R}\) we necessarily have \(|J_{0}^{\bullet}-\widehat{J}|\leq\epsilon\implies|J_{\iota}^{\bullet}- \widehat{J}|>\epsilon\), and vice versa, i.e., we cannot provide an estimate \(\widehat{J}\) that is \(\epsilon\)-close to both \(J_{0}^{\bullet}\) and \(J_{\iota}^{\bullet}\).

**Identifying the underlying problem** Following [38], let us consider a generic \((\epsilon,\delta)\)-correct algorithm \(\mathfrak{A}\) that outputs the estimated optimal utility \(\widehat{J}\). Then, for all \(\iota\in\mathcal{I}\), we have:

\[\delta \geq\sup_{\text{all problem instances }\mathcal{M}}\mathbb{P}_{ \mathcal{M},\mathfrak{A}}\bigg{(}\big{|}J_{\mathcal{M}}^{\bullet}-\widehat{J} \big{|}\geq\epsilon\bigg{)}\] \[\geq\sup_{\ell\in(0,\iota)}\mathbb{P}_{\mathcal{M}_{\iota}, \mathfrak{A}}\bigg{(}\big{|}J_{\ell}^{\bullet}-\widehat{J}\big{|}\geq\epsilon \bigg{)}.\]

For every \(\iota\in\mathcal{I}\), we define the _identification function_\(\Psi_{\iota}\) as the index of the problem "recognized" by algorithm \(\mathfrak{A}\). In symbols:

\[\Psi_{\iota}:=\operatorname*{arg\,min}_{\ell\in\{0,\iota\}}\Big{|}J_{\ell}^{ \bullet}-\widehat{J}\Big{|}.\]

In words, given estimate \(\widehat{J}\) returned by algorithm \(\mathfrak{A}\), the identification function \(\Psi_{\iota}\) returns the problem between \(\mathcal{M}_{0}\) and \(\mathcal{M}_{\iota}\) whose optimal utility is closest to the estimate \(\widehat{J}\). For what we have seen in the previous paragraph, problems \(\mathcal{M}_{0}\) and \(\mathcal{M}_{\iota}\) lie at a distance of at least \(2\epsilon\) for all \(\iota\in\mathcal{I}\). Therefore, for \(\jmath\in\{0,\imath\}\), we have the following inclusion of events:

\[\{\Psi_{\iota}\neq\jmath\}\subseteq\{|J_{\jmath}^{\bullet}-\widehat{J}|> \epsilon\}.\]Thanks to this fact, we can continue lower bounding the probability as:

\[\max_{\ell\in\{0,\imath\}}\mathbb{P}_{\mathcal{M}_{\ell},\mathfrak{A }}\bigg{(}\bigg{|}J_{\ell}^{*}-\widehat{J}\bigg{|}\geq\epsilon\bigg{)} \geq\max_{\ell\in\{0,\imath\}}\mathbb{P}_{\mathcal{M}_{\ell}, \mathfrak{A}}\{\Psi_{\imath}\neq\ell\}\] \[\overset{\eqref{eq:1}}{\geq}\frac{1}{2}\bigg{[}\mathbb{P}_{ \mathcal{M}_{0},\mathfrak{A}}\big{(}\Psi_{\imath}\neq 0\big{)}+\mathbb{P}_{ \mathcal{M}_{\imath},\mathfrak{A}}\big{(}\Psi_{\imath}\neq\imath\big{)}\bigg{]}\] \[=\frac{1}{2}\bigg{[}\mathbb{P}_{\mathcal{M}_{0},\mathfrak{A}} \big{(}\Psi_{\imath}\neq 0\big{)}+\mathbb{P}_{\mathcal{M}_{\imath},\mathfrak{A}} \big{(}\Psi_{\imath}=0\big{)}\bigg{]}\] \[\overset{\eqref{eq:1}}{\geq}\frac{1}{4}\exp^{-\text{KL}( \mathbb{P}_{\mathcal{M}_{0},\mathfrak{A}},\mathfrak{P}_{\mathcal{M}_{\imath}, \mathfrak{A}})},\]

where at (1) we have lower bounded the maximum with the average, i.e., \(\max\{a,b\}\geq(a+b)/2\) for all \(a,b\geq 0\), and at (2) we have applied the Bretagnolle-Huber's inequality [38].

**KL-divergence computation** The proof can be concluded by upper bounding the KL divergence \(\text{KL}(\mathbb{P}_{\mathcal{M}_{0},\mathfrak{A}},\mathbb{P}_{\mathcal{M}_{ \imath},\mathfrak{A}})\) as in the proof of Theorem 7 in [12], and then summing over all the \(\Theta(SAH)\) instances to retrieve the result.

**Lemma E.3**.: _Let IRL and RFE be the learning problems defined as in Appendix E.1. For each problem, if the set of reward functions \(\mathcal{R}\) in input is \(\mathcal{R}=\mathfrak{R}\), then any \((\epsilon,\delta)\)-PAC algorithm must collect at least the following number of exploration episodes:_

\[\tau\geq\Omega\bigg{(}\frac{H^{3}S^{2}A}{\epsilon^{2}}\bigg{)}.\]

Proof.: **Instances description** The hard instances that we use for the proof of this lemma are obtained by combining the hard instances in Lemma E.2 (i.e., the hard instances of [12]), with those in [38]. Specifically, this construction is based on the intuition described in [21] that, if we want to increase the sample complexity, we have to learn transitions also _to_\(\Theta(S)\) states, and not just _from_\(\Theta(S)\) states. Observe the presence of state \(s_{E}\) (only for IRL), which plays the same role as in the proof of Lemma E.2. Any action in such state receives always reward \(-1\), thus it is meaningless for the estimate of the (non)compatibility, which reduces to the estimation of the optimal performance. In this manner, the expert distribution \(d^{p,\pi^{E}}\) does not provide additional information about the transition model of

Figure 5: Hard instances.

other portion of the state-action space. Therefore, in the following, we will present the lower bound construction as if such state did not exist.

The hard instances are reported in Figure 5. Notice that they are exactly the same instances as those presented in the proof of Lemma E.2, with the difference that, from the \(\overline{S}\) leaves (differently from earlier, we now denote the number of leaves through \(\overline{S}\) instead of \(L\)), we do not reach just two states \(s_{g},s_{b}\), but we reach \(\Theta(S)\) absorbing states, i.e., \(s^{\prime}_{1},s^{\prime}_{2},\ldots,s^{\prime}_{\overline{S}}\). The transitions from the leaves to such states is the same as in [38], and we report a description below.

Let us introduce the set \(\overline{\mathcal{I}}\coloneqq\{s_{1},\ldots,s_{\overline{S}}\}\times \mathcal{A}\times\{1+d,\ldots,\overline{H}+d\}\). Let \(\bar{\imath}\coloneqq(s_{1},a_{1},1+d)\in\overline{\mathcal{I}}\) be a specific triple of set \(\mathcal{I}\), and denote \(\mathcal{I}\coloneqq\overline{\mathcal{I}}\backslash\{\bar{\imath}\}\). Let us also introduce set \(\mathcal{V}\coloneqq\{v\in\{-1,1\}^{\overline{S}}:\sum_{j=1}^{\overline{S}}v_{ j}=0\}\). Thanks to Lemma E.6 of [38] (that we report in Lemma E.4 for simplicity), we know that there exists a subset \(\overline{V}\subseteq\mathcal{V}\) (of transition models) with cardinality at least \(2^{\overline{S}/5}\) such that, for every pair \(v,w\in\overline{\mathcal{V}}\) with \(v\neq w\), we have that \(\|v-w\|_{1}\geqslant\overline{S}/16\). In other words, we know that there exists a \(\overline{S}/16\)-packing of \(\mathcal{V}\) with cardinality at least \(2^{\overline{S}/5}\).

Following [38], we denote by \(\boldsymbol{v}=(v^{\imath})_{\imath\in\overline{\mathcal{I}}}\in\overline{ \mathcal{V}}^{\mathcal{I}}\) the generic vector of \(\overline{\mathcal{V}}^{\mathcal{I}}\). Now, for any \(\boldsymbol{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), for any triple \(\overline{\jmath}\in\mathcal{I}\), and for some parameter \(\epsilon^{\prime}\in[0,1/2]\) to choose, we construct problem instance \(\mathcal{M}_{\boldsymbol{v},\overline{\jmath}}\) as follows.

First of all, we define the transition model at triple \(\bar{\imath}\) as:

\[p_{h_{\bar{\imath}}}(s^{\prime}_{i}|s_{\bar{\imath}},a_{\imath})=\frac{1}{ \overline{S}}\quad\forall i\in[\overline{S}],\]

where observe that we use notation \(\imath=(s_{\imath},a_{\imath},h_{\imath})\in\overline{\mathcal{I}}\) to denote triples in \(\overline{\mathcal{I}}\). Instead, for the generic triple \(\imath\in\mathcal{I}\) (including triple \(\jmath\)), the probability distribution of the next state is given by:

\[p_{h_{\imath}}(s^{\prime}_{i}|s_{\imath},a_{\imath})=\frac{1}{\overline{S}}+ \frac{\epsilon^{\prime}}{\overline{S}}\boldsymbol{v}^{\imath}_{i}\quad\forall i \in[\overline{S}],\]

where \(\boldsymbol{v}^{\imath}_{i}\) represents the \(i\)-th component of the \(\imath\)-th vector in \(\boldsymbol{v}\). In words, the \(i\)-th component of vector \(\boldsymbol{v}^{\imath}\in\overline{\mathcal{V}}\) creates a bias of \(\epsilon^{\prime}/\overline{S}\) towards the next state \(s^{\prime}_{i}\) for all \(i\in[\overline{S}]\). Since \(\boldsymbol{v}^{\imath}\in\overline{\mathcal{V}}\), then \(p_{h_{\imath}}(\cdot|s_{\imath},a_{\imath})\in\Delta^{[\overline{S}]}\) for all \(\imath\in\mathcal{I}\).

We consider non-stationary reward functions. Specifically, all the rewards \(r\in\mathfrak{R}\) that we consider assign reward 1 to both triples \(\bar{\imath}\) and \(\overline{\jmath}\), i.e., \(r_{h_{\bar{\imath}}}(s_{\bar{\imath}},a_{\bar{\imath}})=1\) and \(r_{h_{\bar{\imath}}}(s_{\bar{\jmath}},a_{\bar{\jmath}})=1\). Next, for any other triple \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times\llbracket H\rrbracket\) with state different from \(s^{\prime}_{1},s^{\prime}_{2},\ldots,s^{\prime}_{\overline{S}}\), we assign reward 0. For states \(s^{\prime}_{1},s^{\prime}_{2},\ldots,s^{\prime}_{\overline{S}}\), we consider state-only rewards whose value is always 0 in stages \([1,\overline{H}+d]\), and whose value is stationary and arbitrary afterwards. Intuitively, as in [12], forcing the reward to be 0 up \(h=\overline{H}+d\) guarantees that we cannot obtain a higher expected return \(J\) by reaching the leaves states earlier (i.e., by exiting from \(s_{\mathsf{w}}\) before \(\overline{H}\)).

Given the definition above, we construct the class of instances \(\mathbb{M}\coloneqq\{\mathcal{M}_{\boldsymbol{v},\imath}:\imath\in\mathcal{I}, \boldsymbol{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\}\). Moreover, we will use the notation \(\mathcal{M}_{\boldsymbol{v}^{\imath}_{\imath}\cdot w,\jmath}\) to denote the instance in which we replace the \(\imath\) component of \(\boldsymbol{v}\), i.e., \(\boldsymbol{v}^{\imath}\), with \(w\in\mathcal{V}\) and \(\mathcal{M}_{\boldsymbol{v}^{\imath}_{\imath}\cdot 0,\jmath}\) the instance in which we replace the \(\imath\) component of \(\boldsymbol{v}\), i.e., \(v^{\imath}\), with the zero vector. Since we will always use this notation when substituting triple \(\jmath\), i.e., we always use this notation in situations as \(\mathcal{M}_{\boldsymbol{v}^{\imath}\cdot w,\jmath}\), then we omit the second parameter, and write just \(\mathcal{M}_{\boldsymbol{v}^{\imath}\cdot w}\coloneqq\mathcal{M}_{\boldsymbol{v }^{\imath}\cdot w,\jmath}\).

**Distance between problems** Consider an arbitrary problem instance \(\mathcal{M}_{\boldsymbol{v},\imath}\in\mathbb{M}\), for certain \(\imath\in\mathcal{I}\) and \(\boldsymbol{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\). Let \(r\in\mathfrak{R}\) be an arbitrary reward function that satisfies the constraints described earlier. Let \(\pi_{\bar{\imath}}\in\Pi\) be the deterministic policy that brings to triple \(\bar{\imath}\). Then, its expected return is:

\[J^{\pi_{\bar{\imath}}}(r;\mathcal{M}_{\boldsymbol{v},\imath})=1+\frac{H- \overline{H}-d}{\overline{S}}\sum_{i=1}^{\overline{S}}r_{i},\]where \(r_{i}:=r_{\overline{H}+d+1}(s^{\prime}_{i})\) for all \(i\in[\overline{S}]\). Let policy \(\pi_{\imath}\in\Pi\) be the deterministic policy that brings to triple \(\imath\). Then, its expected return is:

\[J^{\pi_{\imath}}(r;\mathcal{M}_{\boldsymbol{v},\imath})=1+\frac{H-\overline{H} -d}{\overline{S}}\sum_{i=1}^{\overline{S}}r_{i}+\epsilon^{\prime}\frac{(H- \overline{H}-d)}{\overline{S}}\sum_{i=1}^{\overline{S}}\boldsymbol{v}_{i}^{ \intercal}r_{i}.\]

Finally, let policy \(\pi_{\jmath}\in\Pi\) be the deterministic policy that brings to any other triple \(\jmath\in\mathcal{I}\backslash\{\imath\}\). Then, its expected return is:

\[J^{\pi_{\jmath}}(r;\mathcal{M}_{\boldsymbol{v},\imath})=0+\frac{H-\overline{H} -d}{\overline{S}}\sum_{i=1}^{\overline{S}}r_{i}+\epsilon^{\prime}\frac{(H- \overline{H}-d)}{\overline{S}}\sum_{i=1}^{\overline{S}}\boldsymbol{v}_{i}^{ \jmath}r_{i}.\]

It should be remarked that \((v,r)=\sum_{i\in[\overline{S}]}v_{i}r_{i}\in[-\overline{S},\overline{S}]\) for any \(r\in\mathfrak{R}\) and \(v\in\overline{\mathcal{V}}\), therefore, as long as:

\[\epsilon^{\prime}(H-\overline{H}-d)<1-\epsilon^{\prime}(H-\overline{H}-d)- \epsilon\iff\epsilon^{\prime}<\frac{1-\epsilon}{2(H-\overline{H}-d)},\] (5)

then any policy \(\pi_{\jmath}\) is cannot be \(\epsilon\)-optimal in problem \(\mathcal{M}_{\boldsymbol{v},\imath}\), in which, thus, the optimal policy shall be searched for between \(\pi_{\imath}\) and \(\pi_{\imath}\).

Now, consider an arbitrary pair \(v,w\in\overline{\mathcal{V}}\) such that \(v\neq w\), and an arbitrary triple \(\imath\in\mathcal{I}\) and vector \(\in\overline{\mathcal{V}}^{\mathcal{I}}\). We now compare problem instances \(\mathcal{M}_{\boldsymbol{v},\imath}\) and \(\mathcal{M}_{\boldsymbol{v},\imath}\). Among all possible reward functions that satisfy the definition provided in the construction of the hard instances, we find reward \(r^{\prime}\) such that, in every component \(i\in[\overline{S}]\), satisfies:

\[r^{\prime}_{i}=\begin{cases}+1&\text{if }v_{i}=+1\wedge w_{i}=-1\\ -1&\text{if }v_{i}=-1\wedge w_{i}=+1\\ 0&\text{if }v_{i}=w_{i}\end{cases}.\]

For what we have seen before about class \(\overline{\mathcal{V}}\), we know that \(|v-w|_{1}=\sum_{i\in[\overline{S}]}|v_{i}-w_{i}|\geq\overline{S}/16\), thus, since \(v,w\in\mathcal{V}\), i.e., their components belong to \(\{-1,+1\}\), we know that there are at least \(\overline{S}/32\) components of \(v,w\) that differ from each other. By using reward \(r^{\prime}\), we have that:

\[\sum_{i=1}^{\overline{S}}v_{i}r^{\prime}_{i}\geq\frac{\overline{S }}{32}\geq 0,\] \[\sum_{i=1}^{\overline{S}}w_{i}r^{\prime}_{i}\leq-\frac{\overline{S }}{32}\leq 0.\]

As a consequence, the expected returns of policies \(\pi_{\imath}\) and \(\pi_{\imath}\) in problems \(\mathcal{M}_{\boldsymbol{v},\imath}\) and \(\mathcal{M}_{\boldsymbol{v},\imath}\) are:

\[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})=J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath})=1 +\frac{H-\overline{H}-d}{\overline{S}}\sum_{i=1}^{\overline{S}}r^{\prime}_{i},\] \[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})\!\!\geq\!\!1+\frac{H-\overline{H}-d}{\overline{S}}\sum_{i=1}^{ \overline{S}}r^{\prime}_{i}+\epsilon^{\prime}\frac{(H-\overline{H}-d)}{32},\] \[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})\!\!\leq\!\!1+\frac{H-\overline{H}-d}{\overline{S}}\sum_{i=1}^{ \overline{S}}r^{\prime}_{i}-\epsilon^{\prime}\frac{(H-\overline{H}-d)}{32},\]

from which we infer that:

\[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})\geq J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})=J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath}^{ \perp})\geq J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath}^{ \perp}w).\]

Now, let us choose \(\epsilon^{\prime}>64\epsilon/(H-\overline{H}-d)\). To satisfy also the constraint in Equation (5), we can roughly assume \(\epsilon<1/256\) and set \(\epsilon^{\prime}=65\epsilon/(H-\overline{H}-d)\). Thanks to this choice, observe that:

\[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath} ^{\perp})>J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath}^{ \perp}v)+2\epsilon,\] \[J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath}^{ \perp})>J^{\pi_{\imath}}(r^{\prime};\mathcal{M}_{\boldsymbol{v},\imath}^{ \perp}w)+2\epsilon.\]In words, policy \(\pi_{\imath}\) is optimal in problem \(\mathcal{M}_{\mathbf{v}_{\pi^{\perp}}\!\!-\!v}\), and policy \(\pi_{\bar{\imath}}\) is worse than \(2\epsilon\)-suboptimal in such problem. In addition, observe that policy \(\pi_{\bar{\imath}}\) is optimal in problem \(\mathcal{M}_{\mathbf{v}_{\pi^{\perp}}\!\!-\!v}\), and policy \(\pi_{\imath}\) is worse than \(2\epsilon\)-suboptimal in such problem. We stress that any stochastic policy in-between \(\pi_{\imath}\) and \(\pi_{\bar{\imath}}\) cannot be \(\epsilon\)-optimal for both problems.

To sum up, for the choice of \(\epsilon^{\prime}\) made earlier, for arbitrary pairs of problems \(\mathcal{M}_{\mathbf{v}_{\pi^{\perp}}\!\!-\!v}\) and \(\mathcal{M}_{\mathbf{v}_{\pi^{\perp}}\!\!-\!w}\), we have seen that there exist rewards in \(\mathfrak{R}\) for which a policy \(\epsilon\)-optimal for problem \(\mathcal{M}_{\imath,v}\) is not \(\epsilon\)-optimal for problem \(\mathcal{M}_{\imath,w}\), and vice versa.

Identifying the underlying problem: RFE.We consider first RFE, and then IRL.

Let us consider an \((\epsilon,\delta)\)-correct algorithm \(\mathfrak{A}\) for RFE, that outputs, for any reward function \(r\in\mathfrak{R}\), a policy \(\widehat{\pi}_{r}\). For simplicity, we consider as output of Algorithm \(\mathfrak{A}\) a function \(\widehat{\pi}:\mathfrak{R}\rightarrow\Pi\), that takes in input a reward and outputs a policy.

For any \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), we can lower bound the error probability as:

\[\delta \geq\sup_{\text{all problem instances }\mathcal{M}}\mathbb{P}_{ \mathcal{M},\mathfrak{A}}\bigg{(}\sup_{r\in\mathfrak{R}}J^{\bm{*}}_{ \mathcal{M}}(r)-J^{\widehat{\pi}_{r}}_{\mathcal{M}}(r)\geq\epsilon\bigg{)}\] \[\overset{(1)}{\geq}\sup_{\mathcal{M}\in\mathbb{M}}\mathbb{P}_{ \mathcal{M},\mathfrak{A}}\bigg{(}\sup_{r\in\mathfrak{R}}J^{\bm{*}}_{ \mathcal{M}}(r)-J^{\widehat{\pi}_{r}}_{\mathcal{M}}(r)\geq\epsilon\bigg{)}\] \[\overset{(2)}{\geq}\max_{w\in\overline{\mathcal{V}}}\mathbb{P}_{ \mathcal{M}_{\eta^{\perp}w^{\perp}},\mathfrak{A}}\bigg{(}\sup_{r\in\mathfrak{ R}}J^{\bm{*}}_{\mathcal{M}_{\eta^{\perp}w}}(r)-J^{\widehat{\pi}_{r}}_{ \mathcal{M}_{\eta^{\perp}w}}(r)\geq\epsilon\bigg{)},\]

where at (1) we have lower bounded by replacing all possible RFE problem instances with problem instances in \(\mathbb{M}\), and at (2) we have lower bounded by replacing all instances in \(\mathbb{M}\) with just instances \(\{\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}:w\in\overline{\mathcal{V}}\}\) for the fixed triple \(\imath\) and vector \(\bm{v}\).

For every \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), we define the _identification function_\(\Psi_{\imath,\bm{v}}\) as the index of the problem \(w\in\overline{\mathcal{V}}\) "recognized" by algorithm \(\mathfrak{A}\). In symbols:

\[\Psi_{\imath,\bm{v}}:=\operatorname*{arg\,min}_{w\in\overline{\mathcal{V}}} \sup_{r\in\mathfrak{R}}J^{\bm{*}}_{\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}} }(r)-J^{\widehat{\pi}_{r}}_{\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}}(r).\]

In words, given estimate \(\widehat{\pi}:\mathfrak{R}\rightarrow\Pi\) returned by algorithm \(\mathfrak{A}\), the identification function \(\Psi_{\imath,\bm{v}}\) returns the problem in \(\{\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}:w\in\overline{\mathcal{V}}\}\) whose solution \(\pi:\mathfrak{R}\rightarrow\Pi\) is closest to the estimate \(\widehat{\pi}\). For what we have seen in the previous paragraph, for any \(v,w\in\overline{\mathcal{V}}\) with \(v\neq w\), for any fixed \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), there exists a reward function \(r^{\prime}\in\mathfrak{R}\) such that no policy can have expected utility \(\epsilon\)-close to the optimal expected utility of both problems \(\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}\) and \(\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}\). Therefore, for \(w\in\overline{\mathcal{V}}\), we have the following inclusion of events:

\[\{\Psi_{\imath,\bm{v}}\neq w\}\subseteq\Big{\{}\sup_{r\in\mathfrak{R}}J^{\bm{* }}_{\mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}}(r)-J^{\widehat{\pi}_{r}}_{ \mathcal{M}_{\bm{v}_{\pi^{\perp}w}^{\perp}}}(r)>\epsilon\Big{\}}.\]

We can continue to lower bound the probability as:

\[\max_{w\in\overline{\mathcal{V}}}\mathbb{P}_{\mathcal{M}_{\bm{v} _{\pi^{\perp}w}^{\perp}},\mathfrak{A}} \bigg{(}\sup_{r\in\mathfrak{R}}J^{\bm{*}}_{\mathcal{M}_{\bm{v}_{ \pi^{\perp}w}^{\perp}}}(r)-J^{\widehat{\pi}_{r}}_{\mathcal{M}_{\bm{v}_{\pi^{ \perp}w}^{\perp}}}(r)\geq\epsilon\bigg{)}\overset{(3)}{\geq}\frac{1}{| \mathcal{V}|}\sum_{w\in\mathcal{V}}\mathbb{P}_{\mathcal{M}_{\bm{v}_{\pi^{\perp}w }^{\perp}},\mathfrak{A}}\big{(}\Psi_{\imath,\bm{v}}\neq w\big{)}\] \[\overset{(4)}{\geq}1-\frac{1}{\log|\mathcal{V}|}\bigg{(}\frac{1}{| \mathcal{V}|}\sum_{w\in\mathcal{V}}\text{KL}(\mathbb{P}_{\mathcal{M}_{\bm{v}_{ \pi^{\perp}w}^{\perp}},\mathfrak{A}},\mathbb{P}_{\mathcal{M}_{\bm{v}_{\pi^{\perp }0}},\mathfrak{A}})-\log 2\bigg{)},\]

where at (3) we have lower bounded the maximum over \(\overline{\mathcal{V}}\) with the average, and at (4) we have applied, similary to [38], the Fano's inequality, reported in Theorem E.5 for simplicity.

Identifying the underlying problem: IRL.For IRL, it is possible to carry out a similar derivation. However, we remark that, now, the error is measured based on the expected utilities, and not on the policies.

Let us consider an \((\epsilon,\delta)\)-correct algorithm \(\mathfrak{A}\) for IRL, that outputs, for any reward function \(r\in\mathfrak{R}\), a utility \(\widehat{J}_{r}\). For simplicity, we consider as output of Algorithm \(\mathfrak{A}\) a function \(\widehat{J}:\mathfrak{R}\rightarrow\mathbb{R}\), that takes in input a reward and outputs a utility.

For any \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), we can lower bound the error probability as:

\[\delta \geq\sup_{\text{all problem instances }\mathcal{M}}\mathbb{P}_{ \mathcal{M},\mathfrak{R}}\bigg{(}\sup_{r\in\mathfrak{R}}\left|J^{\star}_{ \mathcal{M}}(r)-\widehat{J}_{r}\right|\geq\epsilon\bigg{)}\] \[\geq\max_{w\in\overline{\mathcal{V}}}\mathbb{P}_{\mathcal{M}_{ \mathfrak{R},\mathfrak{R}},\mathfrak{R}}\bigg{(}\sup_{r\in\mathfrak{R}}\left|J ^{\star}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}}}(r)-\widehat{J}_{r}\right| \geq\epsilon\bigg{)}.\]

For any \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), we define an identification function \(\Psi_{\imath,\bm{v}}\) as:

\[\Psi_{\imath,\bm{v}}:=\operatorname*{arg\,min}_{w\in\overline{ \mathcal{V}}}\sup_{r\in\mathfrak{R}}\left|J^{\star}_{\mathcal{M}_{\mathfrak{R}^ {\perp\infty}}}(r)-\widehat{J}_{r}\right|,\]

and by a reasoning analogous to that for RFE, we can continue to lower bounding as:

\[\max_{w\in\overline{\mathcal{V}}}\mathbb{P}_{\mathcal{M}_{\mathfrak{ R}^{\perp\infty}},\mathfrak{R}}\bigg{(}\] \[\geq 1-\frac{1}{\log\left|\overline{\mathcal{V}}\right|}\bigg{(} \frac{1}{\left|\overline{\mathcal{V}}\right|}\sum_{w\in\overline{\mathcal{V}}} \text{KL}(\mathbb{P}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}},\mathfrak{R}}, \mathbb{P}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}},\mathfrak{R}})-\log 2 \bigg{)},\] (6)

which represents the same lower bound obtained also for RFE.

**KL-divergence computation** The following derivation is analogous to that of [38]. To bound the KL-divergence term, for any \(\imath\in\mathcal{I}\), we can write:

\[\text{KL}(\mathbb{P}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}}, \mathfrak{R}},\mathbb{P}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}},\mathfrak{R }})\stackrel{{(\ref{eq:KL-divergence-form})}}{{=}}\mathbb{E}_{ \mathcal{M}_{w\in\mathcal{U}},\mathfrak{R}}\big{[}N^{\tau}_{h_{\imath}}(s_{ \imath},a_{\imath})\big{]}\text{KL}(p^{\mathcal{M}_{\mathfrak{R}^{\perp \infty}}}_{h_{\imath}}(\cdot|s_{\imath},a_{\imath}),p^{\mathcal{M}_{\mathfrak{ R}^{\perp\infty}}}_{h_{\imath}}(\cdot|s_{\imath},a_{\imath}))\] \[\stackrel{{(\ref{eq:KL-divergence-form})}}{{\leq}}2( \epsilon^{\prime})^{2}\operatorname*{\mathbb{E}}_{\mathcal{M}_{w\in\mathcal{ U}^{\perp\infty}},\mathfrak{R}}\big{[}N^{\tau}_{h_{\imath}}(s_{\imath},a_{\imath}) \big{]},\]

where at (1) we have applied Lemma E.7, and at (2) we have applied Lemma E.6 (having observed that the transition models differ in \(\imath\) and defined \(N^{\tau}_{h_{\imath}}(s_{\imath},a_{\imath})=\sum_{t=1}^{\tau}\mathds{1}\{(s _{\imath},a_{\imath},h_{\imath})=(s_{\imath},a_{\imath},h_{\imath})\}\)). Plugging into Equation (6), we get:

\[\delta\geq\frac{1}{\left|\overline{\mathcal{V}}\right|}\sum_{w\in\overline{ \mathcal{V}}}\mathbb{P}_{\mathcal{M}_{\mathfrak{R}^{\perp\infty}},\mathfrak{R }}(\Psi_{\imath,\bm{v}}\neq w)\implies\frac{1}{\left|\overline{\mathcal{V}} \right|}\sum_{w\in\overline{\mathcal{V}}}\operatorname*{\mathbb{E}}_{ \mathcal{M}_{w\in\mathcal{U}},\mathfrak{R}}\big{[}N^{\tau}_{h_{\imath}}(s_{ \imath},a_{\imath})\big{]}\geq\frac{(1-\delta)\log\left|\overline{\mathcal{V }}\right|-\log 2}{2(\epsilon^{\prime})^{2}}.\]

Notice that, since \(\left|\overline{\mathcal{V}}\right|=\Theta(e^{S})\) and \(\epsilon^{\prime}=\Theta(\epsilon/H)\), then this bound is in the order of \(\Omega(\frac{H^{2}S}{\epsilon^{2}})\). To get the additional \(\Omega(SAH)\) dependence, we can make the same observation as in [38], i.e., that ince the derivation is carried out for every \(\imath\in\mathcal{I}\) and \(\bm{v}\in\overline{\mathcal{V}}^{\mathcal{I}}\), we can perform the summation over \(\imath\) and the average over \(\bm{v}\). By noticing that we get a guarantee on a mean under the uniform distribution of the instances of the sample complexity, we realize that there must exist one \(\bm{v}^{\text{hard}}\in\overline{\mathcal{V}}\) for which it holds the desired \(\Omega\Big{(}\frac{H^{3}S^{2}A}{\epsilon^{2}}\Big{)}\) dependency.

#### e.2.1 Technical Tools

We report here some results from other works. The notation adopted is the same as the original works.

**Lemma E.4** (Lemma E.6 of [38]).: _Let \(\mathcal{V}=\{v\in\{-1,1\}^{D}:\sum_{j=1}^{D}v_{j}=0\}\). Then, the \(\frac{D}{16}\)-packing number of \(\mathcal{V}\) w.r.t. the metric \(d(v,v^{\prime})=\sum_{j=1}^{D}|v_{j}-v^{\prime}_{j}|\) is lower bounded by \(2^{\frac{D}{5}}\)._

**Theorem E.5**.: (Theorem E.2 of [38]) _Let \(\mathbb{P}_{0},\mathbb{P}_{1},\ldots,\mathbb{P}_{M}\) be probability measures on the same measurable space \((\Omega,\mathcal{F})\), and let \(\mathcal{A}_{1},\ldots,\mathcal{A}_{M}\in\mathcal{F}\) be a partition of \(\Omega\). Then,_

\[\frac{1}{M}\sum_{i=1}^{M}\mathbb{P}_{i}(\mathcal{A}_{i}^{c})\geq 1-\frac{\frac{1}{M} \sum_{i=1}^{M}D_{\text{KL}}(\mathbb{P}_{i},\mathbb{P}_{0})-\log 2}{\log M},\]_where \(\mathcal{A}^{c}=\Omega\backslash\mathcal{A}\) is the complement of \(\mathcal{A}\)._

**Lemma E.6** (Lemma E.4 of [38]).: _Let \(\epsilon\in[0,1/2]\) and \(\mathbf{v}\in\{-\epsilon,\epsilon\}^{D}\) such that \(\sum_{i=1}^{d}v_{i}=0\). Consider the two categorical distributions \(\mathbb{P}=\big{(}\frac{1}{D},\frac{1}{D},\ldots,\frac{1}{D}\big{)}\) and \(\mathbb{P}=\big{(}\frac{1+v_{1}}{D},\frac{1+v_{2}}{D},\ldots,\frac{1+v_{D}}{D} \big{)}\). Then, it holds that:_

\[D_{\text{KL}}(\mathbb{P},\mathbb{Q})\leq 2\epsilon^{2}\qquad\text{and}\qquad D _{\text{KL}}(\mathbb{Q},\mathbb{P})\leq 2\epsilon^{2}.\]

**Lemma E.7** (Lemma 5 of [12]).: _Let \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) be two MDPs that are identical except for their transition probabilities, denoted by \(p_{h}\) and \(p_{h}^{\prime}\), respectively. Assume that we have \(\forall(sa)\), \(p_{h}(\cdot|s,a)\ll p_{h}^{\prime}(\cdot|s,a)\). Then, for any stopping time \(\tau\) with respect to \((\mathcal{F}_{H}^{t})_{t\geq 1}\) that satisfies \(\mathbb{P}_{\mathcal{M}}\tau<\infty=1\),_

\[\text{KL}\Big{(}\mathcal{P}_{\mathcal{M}}^{\prime_{H}^{\prime}},\mathcal{P}_ {\mathcal{M}^{\prime}}^{\prime_{H}^{\prime}}\Big{)}=\sum_{s\in\mathcal{A}} \sum_{a\in\mathcal{A}}\sum_{h\in[H-1]}\mathbb{E}\,\big{[}N_{h,s,a}^{\tau}\big{]} \text{KL}\Big{(}p_{h}(\cdot|s,a),p_{h}^{\prime}(\cdot|s,a)\Big{)},\]

_where \(N_{h,s,a}^{\tau}:=\sum_{t=1}^{\tau}\mathds{1}\{(S_{h}^{t},A_{h}^{t})=(s,a)\}\) and \(I_{H}^{\tau}:\Omega\to\bigcup_{t\geq 1}\mathcal{I}_{H}^{t}:\omega\mapsto I _{H}^{\tau(\omega)}(\omega)\) is the random vector representing the history up to episode \(\tau\)._

## Appendix F A Use Case for Objective-Free Exploration (OFE)

Consider the following setting. You are given a certain MDP without reward \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,d_{0},p)\), in which you do not know neither \(d_{0}\) nor \(p\). Your job is to explore the environment to collect samples that allow you to construct estimates \(\widehat{d}_{0}\approx d_{0}\) and \(\widehat{p}\approx p\), that will be subsequently used to perform a task in a given class \(\mathscr{F}\) in an \((\epsilon,\delta)\)-correct manner. Of course the number of samples should be as small as possible. How do you explore? It depends on which problems are contained in class \(\mathscr{F}\).

A use case for OFE is the following.

**Example F.1**.: _Assume that we are given a single fixed environment (for instance, a warehouse), in which there are many tasks to do (e.g., labelling objects, putting stuff on the shelves, bringing products from one side to the other), and assume (it is reasonable) that it is desirable to have one robot for each task. To teach these robots how to behave, we decide to use RL. Since all the robots work in the same environment (warehouse), then the (unknown) transition model is the same. For this reason, an efficient exploration (potentially through RFE) is meaningful. However, we realize that some tasks are difficult to design (i.e., the rewards of such tasks). For these tasks, we prefer to use a human expert to exhibit demonstrations, and then use ReL (in particular, IRL), to learn the reward, that will be subsequently used for AL. To perform IRL nicely, the samples collected at the beginning shall be used. To sum up, we might be interested in performing multiple RL and IRL tasks in the same unknown MDP, and, for efficiency reasons, our exploration of the environment has to be performed only once (before) being given the tasks to solve._

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The Reviewer can find the list of contributions in Section 1, and the related content in the rest of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The Reviewer can find a "Limitations" paragraph in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The Reviewer can find the full set of complete proofs in the supplemental material. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [NA]  Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper does not harm human subjects or participants, and there are no data-related concerns. In addition, we believe that there are no noteworthy potential harmful consequences of our research. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper represents foundational research, and it is not tied to particular applications. In addition, we believe that there is no direct path to any negative applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.