# Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data

 Xin Zheng\({}^{1}\), Miao Zhang\({}^{2}\), Chunyang Chen\({}^{1}\), Quoc Viet Hung Nguyen\({}^{3}\), Xingquan Zhu\({}^{4}\), Shirui Pan\({}^{3\dagger}\)

\({}^{1}\)Monash University, Australia, \({}^{2}\)Harbin Institute of Technology (Shenzhen), China

\({}^{3}\)Griffith University, Australia, \({}^{4}\)Florida Atlantic University, USA

xin.zheng@monash.edu, zhangmiao@hit.edu.cn, chunyang.chen@monash.edu

henry.nguyen@griffith.edu.au, xzhu3@fau.edu, s.pan@griffith.edu.au

Corresponding authorCode is available at https://github.com/Amanda-Zheng/SFGC

###### Abstract

Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediate benefits for various graph learning tasks. However, existing graph condensation methods rely on the joint optimization of nodes and structures in the condensed graph, and overlook critical issues in effectiveness and generalization ability. In this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to distill a large-scale graph into a small-scale graph node set without explicit graph structures, _i.e._, graph-free data. Our idea is to implicitly encode topology structure information into the node attributes in the synthesized graph-free data, whose topology is reduced to an identity matrix. Specifically, SFGC contains two collaborative components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data; (2) a graph neural feature score metric for dynamically evaluating the quality of the condensed data. Through training trajectory meta-matching, SFGC aligns the long-term GNN learning behaviors between the large-scale graph and the condensed small-scale graph-free data, ensuring comprehensive and compact transfer of informative knowledge to the graph-free data. Afterward, the underlying condensed graph-free data would be dynamically evaluated with the graph neural feature score, which is a closed-form metric for ensuring the excellent expressiveness of the condensed graph-free data. Extensive experiments verify the superiority of SFGC across different condensation ratios.3

Footnote 3: Code is available at https://github.com/Amanda-Zheng/SFGC

## 1 Introduction

As prevalent graph data learning models, graph neural networks (GNNs) have attracted much attention and achieved great success [68, 36, 22, 82, 25, 24, 23]. Various graph data in the real world comprises millions of nodes and edges[37, 38, 43], reflecting diverse node attributes and complex structural connections [33, 34, 32, 54]. Modeling such large-scale graphs brings serious challenges in both data storage and GNN model designs, hindering the applications of GNNs in many industrial scenarios [74, 2, 64, 73, 80, 71, 72]. For instance, designing GNN models usually requires repeatedly training GNNs for adjusting proper hyper-parameters and constructing optimal model architectures. When taking large-scale graphs as training data, repeated training through message passing along complex graph structures, makes it highly computation-intensive and time-consuming through try-and-error.

To address these challenges brought by the scale of graph data, a natural data-centric solution [78] is graph size reduction, which transforms the real-world large-scale graph to a small-scalegraph, such as graph sampling [66; 6], graph coreset [47; 60], graph sparsification [1; 5], and graph coarsening [3; 28]. These conventional methods either extract representative nodes and edges or preserve specific graph properties from the large-scale graphs, resulting in severe limitations of the obtained small-scale graphs in the following two folds. First, the available information on derived small-scale graphs is significantly upper-bounded and limited within the range of large-scale graphs [66; 60]. Second, the preserved properties of small-scale graphs, _e.g._, spectrum and clustering, might not always be optimal for training GNNs for downstream tasks [1; 3; 28].

In light of these limitations of conventional methods, in this work, we mainly focus on graph condensation [27; 26], a new rising synthetic method for graph size reduction. Concretely, graph condensation aims to directly optimize and synthesize a small-scale condensed graph, so that the small-scale condensed graph could achieve comparable test performance as the large-scale graph when training the same GNN model. Therefore, the principal goal of graph condensation is to ensure consistent test results for GNNs when taking the large-scale graph and the small-scale condensed graph as training data.

However, due to the structural characteristic of graph data, nodes and edges are tightly coupled. This makes condensing graph data a complicated task since high-quality condensed graphs are required to jointly synthesize discriminative node attributes and topology structures. Some recent works have made initial explorations of graph condensation [27; 26]. For instance, GCOND [27] proposed the online gradient matching schema between the synthesized small-scale graph and the large-scale graph, followed by a condensed graph structure learning module for synthesizing both condensed nodes and structures. However, existing methods overlook two-fold critical issues regarding **effectiveness and generalization ability**. First, graph condensation requires a triple-level optimization to jointly learn three objectives: GNN parameters, distilled node attributes, and topology structures. Such complex optimization cannot guarantee optimal solutions for both nodes and edges in the condensed graph, significantly limiting its effectiveness as the representative of the large-scale graph. Furthermore, existing online GNN gradients [27; 26] are calculated with the short-range matching, leading to the short-sight issue of failing to imitate holistic GNN learning behaviors, limiting the quality of condensed graphs. Second, existing condensed graphs generally show poor generalization ability across different GNN models [27; 26], because different GNN models vary in their convolution operations along graph structures. As a result, existing methods are vulnerable to overfitting of specific GNN architectures by distilling convolutional information into condensed graph structures.

To deal with the above two-fold challenges, in this work, we propose a novel Structure-Free Graph Condensation paradigm, named SFGC, to distill large-scale real-world graphs into small-scale synthetic graph node sets without graph structures, _i.e._, condensed graph-free data. Different from conventional graph condensation that synthesizes both nodes and structures to derive a small-scale graph, as shown in Fig. 1, the proposed structure-free graph condensation only synthesizes a small-scaled node set to train a GNN/MLP, when it implicitly encodes topology structure information into the node attributes in the synthesized graph-free data, by simplifying the condensed topology to an identity matrix. Overall, the proposed SFGC contains two essential components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data; (2) a graph neural feature score metric for dynamically evaluating the quality of condensed graph-free data. To address the short-sight issue of existing online gradient matching, our training trajectory meta-matching scheme first trains a set of training trajectories of GNNs on the large-scale graph to acquire an expert parameter distribution, which serves as offline guidance for optimizing the condensed graph-free data. Then, the proposed SFGC conducts meta-matching to align the long-term GNN learning behaviors between the large-scale graph and condensed graph-free data by sampling from the training trajectory distribution, enabling the comprehensive and compact transfer of informative knowledge to the graph-free data. At each meta-matching step, we would obtain updated condensed graph-free data, which would be fed into the proposed graph neural feature score metric for dynamically evaluating its quality. This metric is derived based on the closed-form solutions of GNNs under the graph neural tangent kernel (GNTK) ridge regression, eliminating the iterative training

Figure 1: Comparisons of condensation _vs_ structure-free condensation on graphs.

of GNNs in the dynamic evaluation. Finally, the proposed SFGC selects the condensed graph-free data with the smallest score as the optimal representative of the large-scale graph. Our proposed structure-free graph condensation method could benefit many potential application scenarios, such as, _graph neural architecture search_[79; 81], _privacy protection_[69], _adversarial robustness_[67; 70], _continual learning_[78], and so on. We provide detailed demonstrations of how our method facilitates the development of these areas in Appendix B

In summary, the contributions of this work are listed as follows:

* We propose a novel Structure-Free Graph Condensation paradigm to effectively distill large-scale real-world graphs to small-scale synthetic graph-free data with superior expressiveness, to the best of our knowledge, for the first time.
* To explicitly imitate the holistic GNN training process, we propose the training trajectory meta-matching scheme, which aligns the long-term GNN learning behaviors between the large-scale graph and the condensed graph-free data, with the theoretical guarantee of eliminating graph structure constraints.
* To ensure the high quality of the condensed data, we derive a GNTK-based graph neural feature score metric, which dynamically evaluates the small-scale graph-free data at each meta-matching step and selects the optimal one. Extensive experiments verify the superiority of our method.

**Prior Works.** Our research falls into the research topic _dataset distillation (condensation)_[30; 59], which aims to synthesize a small typical dataset that distills the most important knowledge from a given large target dataset as its effective substitution. Considering most of the works condense image data [59; 39; 77; 76; 4], due to the complexity of graph structural data, only a few works [27; 26] address graph condensation, while our research designs a new structure-free graph condensation paradigm for addressing the effectiveness and generalization ability issues in existing graph condensation works. Our research also significantly differs from other general graph size reduction methods, for instance, graph coreset [47; 60], graph sparsification [1; 5] and so on. More detailed discussions about related works can be found in Appendix A.

## 2 Structure-Free Graph Condensation

### Preliminaries

**Notations.** Denote a large-scale graph dataset to be condensed by \(\mathcal{T}=(\mathbf{X},\mathbf{A},\mathbf{Y})\), where \(\mathbf{X}\in\mathbb{R}^{N\times d}\) denotes \(N\) number of nodes with \(d\)-dimensional features, \(\mathbf{A}\in\mathbb{R}^{N\times N}\) denotes the adjacency matrix indicating the edge connections, and \(\mathbf{Y}\in\mathbb{R}^{N\times C}\) denotes the \(C\)-classes of node labels. In general, graph condensation synthesizes a small-scale graph dataset denoted as \(\mathcal{T}^{\prime}=(\mathbf{X}^{\prime},\mathbf{A}^{\prime},\mathbf{Y}^{ \prime})\) with \(\mathbf{X}^{\prime}\in\mathbb{R}^{N^{\prime}\times d}\), \(\mathbf{A}^{\prime}\in\mathbb{R}^{N^{\prime}\times N^{\prime}}\), and \(\mathbf{Y}^{\prime}\in\mathbb{R}^{N^{\prime}\times C}\) when \(N^{\prime}\ll N\). In this work, we propose the structure-free graph condensation paradigm, which aims to synthesize a small-scale graph node set \(\mathcal{S}=(\widetilde{\mathbf{X}},\widetilde{\mathbf{Y}})\) without explicitly condensing graph structures, _i.e._, the condensed graph-free data, as an effective substitution of the given large-scale graph. Hence, \(\widetilde{\mathbf{X}}\) contains joint node context attributes and topology structure information, which is a more compact representative compared with \((\mathbf{X}^{\prime},\mathbf{A}^{\prime})\).

**Graph Condensation.** Given a GNN model parameterized by \(\boldsymbol{\theta}\), graph condensation [27] is defined to solve the following triple-level optimization objective by taking \(\mathcal{T}=(\mathbf{X},\mathbf{A},\mathbf{Y})\) as input:

\[\begin{split}\min_{\mathcal{T}^{\prime}}&\mathcal{L} \left[\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{T}^{\prime}}}(\mathbf{X}, \mathbf{A}),\mathbf{Y}\right]\\ s.t.&\boldsymbol{\theta}_{\mathcal{T}^{\prime}}= \operatorname*{arg\,min}_{\boldsymbol{\theta}}\mathcal{L}\left[\mathrm{GNN}_{ \boldsymbol{\theta}}\left(\mathbf{X}^{\prime},\mathbf{A}^{\prime}\right), \mathbf{Y}^{\prime}\right],\\ &\boldsymbol{\psi}_{\mathbf{A}^{\prime}}=\operatorname*{arg\,min}_ {\boldsymbol{\psi}}\mathcal{L}\left[\mathrm{GSL}_{\boldsymbol{\psi}}\left( \mathbf{X}^{\prime}\right)\right],\end{split}\] (1)

where \(\mathrm{GSL}_{\boldsymbol{\psi}}\) is a submodule parameterized by \(\boldsymbol{\psi}\) to synthesize the graph structure \(\mathbf{A}^{\prime}\). One of inner loops learns the optimal GNN parameters \(\boldsymbol{\theta}_{\mathcal{T}^{\prime}}\), while another learns the optimal \(\mathrm{GSL}\) parameters \(\boldsymbol{\psi}_{\mathbf{A}^{\prime}}\) to obtain the condensed \(\mathbf{A}^{\prime}\), and the outer loop updates the condensed nodes \(\mathbf{X}^{\prime}\). All these comprise the condensed small-scale graph \(\mathcal{T}^{\prime}=(\mathbf{X}^{\prime},\mathbf{A}^{\prime},\mathbf{Y}^{ \prime})\), where \(\mathbf{Y}^{\prime}\) is pre-defined based on the class distribution of the label space \(\mathbf{Y}\) in the large-scale graph.

Overall, the above optimization objective needs to solve the following variables iteratively: (1) condensed \(\mathbf{X}^{\prime}\); (2) condensed \(\mathbf{A}^{\prime}\) with \(\mathrm{GSL}_{\boldsymbol{\psi}_{\mathbf{A}^{\prime}}}\); and (3) \(\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{T}^{\prime}}}\). Jointly learning these interdependent oach a complex and nested optimization process, resulting in the limited expressiveness of the condensed graph. This dilemma motivates us to reconsider the optimization objective of graph condensation to synthesize the condensed graph more effectively.

**Graph Neural Tangent Kernel (GNTK).** As a new class of graph kernels, graph neural tangent kernel (GNTK) is easy to train with provable theoretical guarantees, and meanwhile, enjoys the full expressive power of GNNs [10; 19; 21; 41]. In general, GNTK can be taken as the infinitely-wide multi-layer GNNs trained by gradient descent. It learns a class of smooth functions on graphs with close-form solutions. More specifically, let \(G=(V,E)\) denote a graph with nodes \(V\) and edges \(E\), where each node \(v\in V\) within its neighbor set \(\mathcal{N}(v)\). Given two graphs \(G=(V,E)\) and \(G^{\prime}=(V^{\prime},E^{\prime})\) with \(n\) and \(n^{\prime}\) number of nodes, their covariance matrix between input features can be denoted as \(\Sigma^{(0)}\left(G,G^{\prime}\right)\in\mathbb{R}^{n\times n^{\prime}}\). Each element in \(\left[\Sigma^{(0)}\left(G,G^{\prime}\right)\right]_{uy^{\prime}}\) is the inner product \(\bm{h}_{u}^{\top}\bm{h}_{u^{\prime}}\), where \(\bm{h}_{u}\) and \(\bm{h}_{u^{\prime}}\) are of input features of two nodes \(u\in V\) and \(u^{\prime}\in V^{\prime}\). Then, for each GNN layer \(\ell\in\{0,1,\dots,L\}\) that has \(\mathcal{B}\) fully-connected layers with ReLU activation, GNTK calculates \(\bm{\mathcal{K}}_{(\beta)}^{(\ell)}\left\langle G,G^{\prime}\right\rangle\) for each \(\beta\in[\mathcal{B}]\):

\[\begin{split}\left[\bm{\mathcal{K}}_{(\beta)}^{(\ell)}\left\langle G,G^{\prime}\right\rangle\right]_{uu^{\prime}}&=\left[\bm{ \mathcal{K}}_{(\beta-1)}^{(\ell)}\left\langle G,G^{\prime}\right\rangle \right]_{uu^{\prime}}\left[\dot{\bm{\Sigma}}_{(\beta)}^{(\ell)}\left(G,G^{ \prime}\right)\right]_{uu^{\prime}}\\ &+\left[\bm{\Sigma}_{(\beta)}^{(\ell)}\left(G,G^{\prime}\right) \right]_{uu^{\prime}},\end{split}\] (2)

where \(\dot{\Sigma}^{(\ell)}\) denotes the derivative _w.r.t._ the \(\ell\)-th GNN layer of the covariance matrix, and the \((\ell+1)\)-th layer's covariance matrix aggregates neighbors along graph structures as \(\left[\bm{\Sigma}_{(0)}^{(\ell+1)}\left(G,G^{\prime}\right)\right]_{uu^{ \prime}}=\sum_{v\in\mathcal{N}(u)\cup\{u\}}\sum_{v^{\prime}\in\mathcal{N}(u^ {\prime})\cup\{u^{\prime}\}}\left[\bm{\Sigma}_{(\mathcal{B})}^{(\ell)}\left(G,G^{\prime}\right)\right]_{vv^{\prime}}\), ditto for the kernel \(\left[\bm{\mathcal{K}}_{(0)}^{(\ell+1)}\left(G,G^{\prime}\right)\right]_{uu^{ \prime}}\). With the GNTK matrix \(\bm{\mathcal{K}}_{(\mathcal{B})}^{(L)}\left\langle G,G^{\prime}\right\rangle \in\mathbb{R}^{n\times n^{\prime}}\) at the node level, we use the graph kernel method to solve the equivalent GNN model for node classification with closed-form solutions. This would significantly benefit the efficiency of condensed data evaluation by eliminating iterative GNN training.

### Overview of SFGC Framework

The crux of achieving structure-free graph condensation is in determining discriminative node attribute contexts, which implicitly integrates topology structure information. We compare the paradigms between existing graph condensation (GC) and our new structure-free condensation SFGC as follows:

\[\begin{split}\mathcal{T}&=(\mathbf{X},\mathbf{A}, \mathbf{Y})\rightarrow\mathcal{T}^{\prime}=(\mathbf{X}^{\prime},\mathbf{A}^{ \prime},\mathbf{Y}^{\prime}),\quad\mathrm{GC}.\\ \mathcal{T}&=(\mathbf{X},\mathbf{A},\mathbf{Y}) \rightarrow\mathcal{S}=(\widetilde{\mathbf{X}},\mathbf{I},\widetilde{ \mathbf{Y}})=\mathcal{S}=(\widetilde{\mathbf{X}},\widetilde{\mathbf{Y}}), \quad\texttt{SFGC}.\end{split}\] (3)

Figure 2: Overall pipeline of the proposed Structure-Free Graph Condensation (SFGC) framework.

[MISSING_PAGE_FAIL:5]

distribution \(P_{\mathbf{\Theta}_{\mathcal{T}}}\), which can be taken as a'meta' way to make the distilled dataset \(\mathcal{S}\) adapt different parameter initialization. That is why we call it'meta-matching'. In this way, when initializing \(\mathrm{GNN}_{\mathcal{T}}\) and \(\mathrm{GNN}_{\mathcal{S}}\) with the same model parameters, Eq. (5) contributes to aligning the learning behaviors of \(\mathrm{GNN}_{\mathcal{T}}\) that experiences \(p\)-steps optimization, to \(\mathrm{GNN}_{\mathcal{S}}\) that experiences \(q\)-steps optimization. In this way, the proposed training trajectory meta-matching schema could comprehensively imitate the long-term learning behavior of GNN training. As a result, the informative knowledge of the large-scale graph \(\mathcal{T}\) can be effectively transferred to the small-scale condensed graph-free data \(\mathcal{S}=(\widetilde{\mathbf{X}},\widetilde{\mathbf{Y}})\) in the above outer-loop optimization objective of Eq. (4).

For the inner loop, we train \(\mathrm{GNN}_{\mathcal{S}}\) on the synthesized small-scale condensed graph-free data for optimizing its model parameter until the optimal \(\widetilde{\boldsymbol{\theta}}_{\mathcal{S}}^{s}\). Therefore, the final optimization objective of the proposed SFGC is

\[\begin{split}&\min_{\mathcal{S}}\mathrm{E}_{\boldsymbol{\theta} _{\mathcal{T}}^{*}\cdots P_{\mathbf{\Theta}_{\mathcal{T}}}}\left[\mathcal{L}_ {\text{meta-tt}}\left(\boldsymbol{\theta}_{t}^{*}|_{t=t_{0}}^{p},\widetilde{ \boldsymbol{\theta}}_{t}|_{t=t_{0}}^{q}\right)\right],\\ & s.t.\quad\widetilde{\boldsymbol{\theta}}_{\mathcal{S}}^{*}= \operatorname*{arg\,min}_{\widetilde{\boldsymbol{\theta}}}\mathcal{L}_{\text{ cls}}\left[\mathrm{GNN}_{\widetilde{\boldsymbol{\theta}}}\left(\mathcal{S} \right)\right],\end{split}\] (6)

where \(\mathcal{L}_{\text{cls}}\) is the node classification loss calculated with the cross-entropy on graphs. Compared with the triple-level optimization in Eq. (1), the proposed SFGC directly replaces the learnable \(\mathbf{A}^{\prime}\) in Eq. (1) with a fixed identity matrix \(\mathbf{I}\), resulting in the condensed structure-free graph data \(\mathcal{S}=(\widetilde{\mathbf{X}},\mathbf{I},\widetilde{\mathbf{Y}})\). Without synthesizing condensed graph structures with \(\mathrm{GSL}_{\psi}\),the proposed SFGC refines the complex triple-level optimization to the bi-level one, ensuring effectiveness of the condensed graph-free data.

Hence, the advances of the training trajectory meta-matching schema in the proposed SFGC can be summarized as follows: (1) compared with the online gradient calculation, SFGC's offline parameter sampling avoids dynamically computing and storing gradients of both the large and condensed small graphs, reducing computation and memory costs during the condensation process; (2) compared with short-range matching, SFGC's long-term meta-matching avoids condensed data to short-sightedly fit certain optimization steps, contributing to a more holistic and comprehensive way to imitate GNN's learning behaviors.

### Graph Neural Feature Score

For each update of the outer loop in Eq. (6), we would synthesize the brand-new condensed graph-free data. However, evaluating the quality of the underlying condensed graph-free data in the dynamical meta-matching condensation process is quite challenging. That is because we cannot quantity a graph dataset's performance without blending it in a GNN model. And the condensed graph-free data itself cannot be measured by convergence or decision boundary. Generally, to evaluate the condensed graph-free data, we use it to train a GNN model. If the condensed data at a certain meta-matching step achieves better GNN test performance on node classification, it indicates the higher quality of the current condensed data. That means, evaluating condensed graph-free data needs an extra process of training a GNN model from scratch, leading to much more time and computation costs.

In light of this, we aim to derive a metric to dynamically evaluate the condensed graph-free data in the meta-matching process, and utilize it to select the optimal small-scale graph-free data. Meanwhile, the evaluation progress would not introduce extra GNN iterative training for saving computation and time. To achieve this goal, we first identify what characteristics such a metric should have: (1) closed-form solutions of GNNs to avoid iterative training in evaluation; (2) the ability to build strong connections between the large-scale graph and the small-scale synthesized graph-free data. In this case, the graph neural tangent kernel (GNTK) stands out, as a typical class of graph kernels, and has the full expressive power of GNNs with provable closed-form solutions. Moreover, as shown in Eq. (2), GNTK naturally builds connections between arbitrary two graphs even with different sizes.

Based on the graph kernel method with GNTK, we proposed a graph neural feature score metric \(\gamma_{\text{gnf}}\) to dynamically evaluate and select the optimal condensed graph-free data as follows:

\[\gamma_{\text{gnf}}(\mathcal{S})=\frac{1}{2}\left\|\mathbf{Y}_{\text{val}}- \boldsymbol{\mathcal{K}}\left\langle\mathcal{T}_{\text{val}},\mathcal{S} \right\rangle\left(\boldsymbol{\mathcal{K}}\left\langle\mathcal{S},\mathcal{ S}\right\rangle+\lambda\mathbf{I}\right)^{-1}\widetilde{\mathbf{Y}}\right\|^{2},\] (7)

where \(\boldsymbol{\mathcal{K}}\left\langle\mathcal{T}_{\text{val}},\mathcal{S} \right\rangle\in\mathbb{R}^{N_{\text{val}}\times N^{\prime}}\) and \(\boldsymbol{\mathcal{K}}\left\langle\mathcal{S},\mathcal{S}\right\rangle\in \mathbb{R}^{N^{\prime}\times N^{\prime}}\) denote the node-level GNTK matrices derived according to Eq. (2). And \(\mathcal{T}_{\text{val}}\) is the validation sub-graph of the large-scale graph with \(N_{\text{val}}\) numbers of nodes. Concretely, \(\gamma_{\text{gnf}}(\mathcal{S})\) calculates the graph neural tangent kernel based ridge regression error. It measures that, given an infinitely-wide GNN trained on the condensed graph \(\mathcal{S}\) with ridge regression, how close such GNN's prediction on \(\mathcal{T}_{\text{val}}\) to its ground truth labels \(\mathbf{Y}_{\text{val}}\). Note that Eq. (7) can be regarded as the Kernel Inducing Point (KIP) algorithm [39; 40] adapted to the GNTK kernel on GNN models.

Hence, the proposed graph neural feature score meets the above-mentioned characteristics as: (1) it calculates a closed-form GNTK-based ridge regression error for evaluation without iteratively training GNN models; (2) it strongly connects the condensed graph-free data with the large-scale validation graph; In summary, the overall algorithm of the proposed SFGC is presented in Algo. 1.

## 3 Experiments

### Experimental Settings

**Datasets.** Following [27], we evaluate the node classification performance of the proposed SFGC method on Cora, Citeseer [61], and Ogbn-arxiv [17] under the transductive setting, on Flickr [66] and Reddit [16] under the inductive setting. For all datasets under two settings, we use the public splits and setups for fair comparisons. We consider three condensation ratios (\(r\)) for each dataset. Concretely, \(r\) is the ratio of condensed node numbers \(rN(0<r<1)\) to large-scale node numbers \(N\). In the transductive setting, \(N\) represents the number of nodes in the entire large-scale graph, while in the inductive setting, \(N\) indicates the number of nodes in the training sub-graph of the whole large-scale graph. The dataset statistic details are shown in Appendix C.

**Baselines & Implementations.** We adopt the following baselines for comprehensive comparisons [27]: graph coarsening method [20], graph coreset methods, _i.e._, Random, Herding [60], and K-Center [51], the graph-based variant DC-Graph of general dataset condensation method DC [77], which is introduced by [27], graph dataset condensation method GCOND [27] and its variant GCOND-X without utilizing the graph structure. The whole pipeline of our experimental evaluation can be divided into two stages: (1) the condensation stage: synthesizing condensed graph-free data, where we use the classical and commonly-used GCN model [61]; (2) the condensed graph-free data test stage: training a certain GNN model (default with GCN) by the obtained condensed graph-free data from the first stage and testing the GNN on the test set of the large-scale graph with repeated 10 times. We report the average transductive and inductive node classification accuracy (ACC%) with standard deviation (std). Following [27], we use the two-layer GNN with 256 hidden units as the defaulted setting. Besides, we adopt the K-center [51] features to initialize our condensed node attributes for stabilizing the training process. Additional hyper-parameter setting details are listed in Appendix E.

### Experimental Results

**Performance of SFGC on Node Classification.** We compare the node classification performance between SFGC and other graph size reduction methods, especially the graph condensation methods. The overall performance comparison is listed in Table 1. Generally, SFGC achieves the best performance on the node classification task with 13 of 15 cases (five datasets and three condensation ratios for each of them), compared with all other baseline methods, illustrating the high quality and expressiveness of the condensed graph-free data synthesized by our SFGC. More specifically, the better performance of SFGC than GCOND and its structure-free variant GCOND-X experimentally verifies the superiority of the proposed method. We attribute such superiority to the following two aspects regarding the condensation stage. First, the long-term parameter distribution matching of our SFGC works better than the short-term gradient matching in GCOND and GCOND-X. That means capturing the long-range GNN learning behaviors facilitates to holistically imitate GNN's training process, leading to comprehensive knowledge transfer from the large-scale graph to the small-scale condensed graph-free data. Second, the structure-free paradigm of our SFGC enables more compact small-scale graph-free data. For one thing, it liberates the optimization process from triple-level objectives, alleviating the complexity and difficulty of condensation. For another thing, the obtained optimal condensed graph-free data compactly integrates node attribute contexts and topology structure information. Furthermore, on Cora and Citeseer, SFGC synthesizes better condensed graph-free data that even exceeds the whole large-scale graph dataset. These results confirm that SFGC is able to break the information limitation under the large-scale graph and effectively synthesize new, small-scale graph-free data as an optimal representation of the large-scale graph.

**Effectiveness of Structure-free Paradigm in SFGC.** The proposed SFGC introduces the structure-free paradigm without condensing graph structures in graph condensation. To verify the effectiveness of the structure-free paradigm, we compare the proposed SFGC with its variants, which synthesize graph structures in the condensation process. Specifically, we evaluate the following three different methods of synthesizing graph structures with five variants of SFGC: (1) discrete \(k\)-nearest neighbor (\(k\)NN) structures calculated by condensed node features under \(k=(1,2,5)\), corresponding to the variants SFGC-d1, SFGC-d2, and SFGC-d5; (2) cosine similarity based continuous graph structures

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Ratio (\(\nu\))} & \multicolumn{3}{c}{_Other Graph Size Reduction_} & \multicolumn{3}{c}{_Concatenation Methods_} & \multicolumn{3}{c}{_White_} \\  & & & & & & & & & & \\ \hline \multirow{4}{*}{Citeseer} & 0.9\% & 52.2\(\pm\)0.4 & 54.4\(\pm\)4.4 & 57.1\(\pm\)5.1 & 52.4\(\pm\)0.8 & 66.8\(\pm\)1.5 & 71.4\(\pm\)0.8 & 70.5\(\pm\)1.2 & **71.4\(\pm\)0.5** & \\  & 1.8\% & 59.9\(\pm\)0.5 & 64.2\(\pm\)2.7 & 66.7\(\pm\)1.6 & 64.3\(\pm\)0.1 & 66.9\(\pm\)0.9 & 68.1\(\pm\)1.1 & 70.6\(\pm\)0.5 & **72.4\(\pm\)0.4** & 71.7\(\pm\)0.1 \\  & 3.6\% & 65.3\(\pm\)0.3 & 69.1\(\pm\)0.1 & 69.0\(\pm\)0.1 & 69.1\(\pm\)1.1 & 66.3\(\pm\)1.5 & 69.4\(\pm\)1.4 & 68.5\(\pm\)1.4 & **70.6\(\pm\)0.7** & \\ \hline \multirow{4}{*}{Cora} & 1.3\% & 312.4\(\pm\)0.2 & 63.6\(\pm\)1.7 & 67.0\(\pm\)1.3 & 64.0\(\pm\)2.3 & 67.3\(\pm\)1.0 & 75.9\(\pm\)1.2 & 79.8\(\pm\)1.3 & **80.1\(\pm\)0.4** & \\  & 2.6\% & 65.2\(\pm\)0.0 & 72.8\(\pm\)1.3 & 75.4\(\pm\)1.0 & 75.2\(\pm\)1.2 & 67.6\(\pm\)1.5 & 75.2\(\pm\)0.0 & 80.1\(\pm\)0.4 & **81.7\(\pm\)0.5** & 81.2\(\pm\)0.2 \\  & 5.2\% & 70.0\(\pm\)0.1 & 76.8\(\pm\)0.1 & 76.0\(\pm\)0.1 & 67.2\(\pm\)0.1 & 67.7\(\pm\)0.2 & 76.0\(\pm\)0.0 & 75.3\(\pm\)0.3 & 83.6\(\pm\)0.6 & \\ \hline \multirow{4}{*}{Oglew-axis} & 0.05\% & 35.4\(\pm\)0.3 & 47.1\(\pm\)1.9 & 52.4\(\pm\)1.5 & 47.2\(\pm\)1.0 & 58.6\(\pm\)0.4 & 61.3\(\pm\)0.5 & 59.2\(\pm\)1.1 & **65.5\(\pm\)0.7** & \\  & 0.25\% & 43.5\(\pm\)0.2 & 57.3\(\pm\)1.1 & 66.2\(\pm\)1.5 & 58.6\(\pm\)1.5 & 59.5\(\pm\)0.5 & 64.2\(\pm\)0.6 & 63.2\(\pm\)0.3 & **66.3\(\pm\)0.4** & 71.4\(\pm\)0.1 \\  & 0.5\% & 50.4\(\pm\)1.1 & 60.0\(\pm\)0.0 & 60.0\(\pm\)0.0 & 60.0\(\pm\)0.0 & 39.4\(\pm\)0.3 & 59.3\(\pm\)0.3 & 63.0\(\pm\)0.3 & 64.0\(\pm\)0.4 & **66.8\(\pm\)0.4** & \\ \hline \multirow{4}{*}{Flickr} & 0.1\% & 41.9\(\pm\)0.2 & 41.8\(\pm\)0.2 & 42.5\(\pm\)1.5 & 42.0\(\pm\)1.0 & 46.3\(\pm\)0.2 & 45.9\(\pm\)1.0 & 45.0\(\pm\)0.1 & 46.5\(\pm\)0.4 & 46.0\(\pm\)0.2 & \\  & 0.5\% & 44.5\(\pm\)0.1 & 44.0\(\pm\)0.1 & 43.9\(\pm\)0.0 & 43.2\(\pm\)1.0 & 45.9\(\pm\)1.0 & 45.0\(\pm\)0.2 & **47.1\(\pm\)0.1** & 47.2\(\pm\)0.1 & 47.2\(\pm\)0.1 \\  & 1\% & 44.6\(\pm\)0.1 & 44.6\(\pm\)0.2 & 44.4\(\pm\)0.0 & 44.1\(\pm\)0.0 & 45.8\(\pm\)0.1 & 45.0\(\pm\)0.1 & **47.1\(\pm\)0.1** & **47.1\(\pm\)0.1 & \\ \hline \multirow{4}{*}{Reddit} & 0.05\% & 40.9\(\pm\)0.5 & 46.1\(\pm\)1.4 & 53.1\(\pm\)2.5 & 46.6\(\pm\)2.3 & 88.2\(\pm\)0.2 & 85.4\(\pm\)0.4 & 80.8\(\pm\)0.1 & **89.7\(\pm\)0.2** & **89.7\(\pm\)0.2 \\  & 0.1\% & 42.8\(\pm\)0.8 & 58.0\(\pm\)0.2 & 62.7\(\pm\)1.0 & 53.0\(\pm\)3.3 & 89.5\(\pm\)0.1 & 89.3\(\pm\)0.1 & 89.6\(\pm\)0.7 & **90.0\(\pm\)0.3** & 93.9\(\pm\)0.4 \\  & 0.2\% & 47.4\(\pm\)0.9 & 66.3\(\pm\)1.9 & 71.0\(\pm\)1.6 & 58.5\(\pm\)1.2 & **90.5\(\pm\)1.2** & 88.8\(\pm\)0.0 & 90.1\(\pm\)0.0 & 89.9\(\pm\)0.4 & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Node classification performance (ACC%\(\pm\)std) comparison between condensation methods and other graph size reduction methods with different condensation ratios. (Best results are in bold, and the second-sets are underlined.)

Figure 3: Comparisons between five variants of synthesizing graph structures _vs._ our structure-free SFGC (discrete \(k\)-nearest neighbor (\(k\)NN) structure variants: SFGC-d1 (\(k=1\)), SFGC-d2 (\(k=2\)), and SFGC-d5 (\(k=5\)), continuous graph structure variant: SFGC-c, parameterized graph structure variant: SFGC-p).

calculated by condensed node features, corresponding to the variant SFGC-c; (3) parameterized graph structure learning module with condensed node features adapted by [27], corresponding to the variant SFGC-p. We conduct experiments on three transductive datasets under nine condensation ratios for each graph structure synthesis variant and the proposed SFGC. The results are presented in Fig. 3. In general, the proposed SFGC achieves the best performance over various graph structure synthesis methods, and these results empirically verify the effectiveness of the proposed structure-free condensation paradigm. More specifically, for discrete \(k\)-nearest neighbor (\(k\)NN) structure variants, different datasets adapt different numbers of \(k\)-nearest neighbors under different condensation ratios, which means predefining the value of \(k\) can be very challenging. For example, Citeseer dataset has better performance with \(k=1\) in SFGC-d1 under \(r=0.9\%\) than SFGC-d2 and SFGC-d5, but under \(r=1.8\%\), \(k=2\) in SFGC-d2 performs better than others two. Besides, for continuous graph structure variant SFGC-c, it generally cannot exceed the discrete graph structure variants, except for Ogbn-arxiv dataset under \(r=0.05\%\). And the parameterized variant SFGC-p almost fails to synthesize satisfied condensed graphs under the training trajectory meta-matching scheme. The superior performance of SFGC to all structure-based methods demonstrates the effectiveness of its structure-free paradigm.

**Effectiveness of Graph Neural Feature Score in SFGC.** We compare the learning time between GNN iterative training _vs._ our proposed GNTK-based closed-form solutions of \(\gamma_{\text{grf}}\). Note that the iterative training evaluation strategy mandates the complete training of a GNN model from scratch at each meta-matching step, hence, we calculate its time that covers all training epochs under the best test performance for fair comparisons. Typically, for Flickr dataset (\(r=0.1\%\)), our proposed \(\gamma_{\text{grf}}\) based GNTK closed-form solutions takes only 0.015s for dynamic evaluation, which significantly outperforms the iterative training evaluation with 0.845s. The superior performance can also be observed in Ogbn-arxiv dataset (\(r=0.05\%\)) with 0.042s of our \(\gamma_{\text{grf}}\), compared with 4.264s of iterative training, illustrating our SFGC's high dynamic evaluation efficiency. More results and analysis of our proposed \(\gamma_{\text{grf}}\) in GNTK-based closed-form solutions can be found in Appendix.

**Generalization Ability of SFGC across Different GNNs.** We evaluate and compare the generalization ability of the proposed SFGC and other graph condensation methods. Concretely, we test the node classification performance of our synthesized graph-free data (condensed on GCN) with seven different GNN architectures: MLP, GAT [56], APPNP [14], Cheby [7], GCN [61], SAGE [16], and SGC [63]. It can be generally observed that the proposed SFGC achieves outstanding performance over all tested GNN architectures, reflecting its excellent generalization ability. This is because our method reduces the graph structure to the identity matrix, so that the condensed graph node set can no longer be influenced by different convolution operations of GNNs along graph structures, enabling it consistent and good performance with various GNNs.

More experimental analysis and discussions, including the effects of different ranges of long-term meta-matching, the performance on downstream unsupervised graph clustering task, visualization of our condensed structure-free node set, as well as time complexity analysis, are detailed in Appendix E.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline Datasets (Ratio) & Methods & MLP & GAT [56] & APPNP [14] & Cheby (7) & GCN [81] & SAGE [16] & SGC [13] & Arg. \\ \hline \multirow{3}{*}{Citeseer (\(r=1.8\%\))} & DC-Graph [7] & 66.2 & - & 66.4 & 64.9 & 66.2 & 65.9 & 69.6 & 66.6 \\  & GCND-X [21] & 69.6 & - & 69.7 & 70.6 & 69.7 & 69.2 & 71.6 & 70.2 \\  & GCND [21] & 63.9 & 55.4 & 69.6 & 68.3 & 70.5 & 66.2 & 70.3 & 69.0 \\  & SPG (**curcurxiv**) & **71.3** & **72.1** & **70.5** & **71.8** & **71.6** & **71.7** & **71.8** & **71.5** \\ \hline \multirow{3}{*}{Cora} & DC-Graph [7] & 67.2 & - & 67.1 & 67.7 & 67.9 & 66.2 & 72.8 & 68.3 \\  & GCND-X [21] & 76.0 & - & 77.0 & 74.1 & 75.3 & 76.0 & 76.1 & 75.7 \\  & GCND [21] & 73.1 & 66.2 & 78.5 & 76.0 & 80.1 & 78.2 & 79.3 & 78.4 \\  & SFGC (**ours**) & **81.1** & **80.8** & **78.8** & **79.0** & **81.1** & **81.9** & **79.1** & **80.3** \\ \hline \multirow{3}{*}{Ogbn-arxiv (\(r=0.25\%\))} & DC-Graph [7] & 59.9 & - & 60.0 & 55.7 & 59.8 & 60.0 & 60.4 & 59.2 \\  & GCND-X [21] & 64.1 & - & 61.5 & 59.5 & 64.2 & 64.4 & 64.7 & 62.9 \\  & GCND [21] & 62.2 & 60.0 & 63.4 & 54.9 & 63.2 & 62.6 & 63.7 & 61.6 \\  & SPG (**ours**) & **65.1** & **65.7** & **63.9** & **60.7** & **65.1** & **64.8** & **64.8** & **64.3** \\ \hline \multirow{3}{*}{Fibkr} & DC-Graph [7] & 43.1 & - & 45.7 & 43.8 & 45.9 & 45.8 & 45.6 & 45.4 \\  & GCND-X [21] & 42.1 & - & 44.6 & 42.3 & 45.0 & 44.7 & 44.4 & 44.2 \\  & GCND [21] & 44.8 & 40.1 & **45.9** & 42.8 & **47.1** & 46.2 & **46.1** & **45.6** \\  & SPG (**ours**) & **47.1** & **45.3** & - & **40.7** & **45.4** & **47.1** & **47.0** & 42.5 & 45.0 \\ \hline \multirow{3}{*}{Reddit (\(r=0.1\%\))} & DC-Graph [7] & 50.3 & - & 81.2 & 77.5 & 89.5 & 89.7 & 90.5 & 85.7 \\  & GCND-X [21] & 40.1 & - & 78.7 & 74.0 & 89.3 & 89.3 & **91.0** & 84.5 \\ \cline{1-1}  & GCND [21] & 42.5 & 60.2 & 87.8 & 75.5 & 89.4 & 89.1 & 89.6 & 86.3 \\ \cline{1-1}  & SPG (**ours**) & **89.5** & **87.1** & **88.3** & **82.8** & **89.7** & **90.3** & 89.5 & **88.2** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance across different GNN architectures.

Conclusion

This work proposes a novel Structure-Free Graph Condensation paradigm, named SFGC, to distill the large-scale graph into the small-scale graph-free node set without graph structures. Under the structure-free learning paradigm, the training trajectory meta-matching scheme and the graph neural feature score measured dynamic evaluation work collaboratively to synthesize small-scale graph-free data with superior effectiveness and good generalization ability. Extensive experimental results and analysis under large condensation ratios confirm the superiority of the proposed SFGC method in synthesizing excellent small-scale graph-free data. It can be anticipated that our work would bridge the gap between academic GNNs and industrial MLPs by synthesizing small-scale, graph-free data to address graph data scalability, while retaining the expressive performance of graph learning. Our method works on condensing the number of nodes in a single graph at the node level, and we will explore extending it to condense the number of graphs in a graph set at the graph level in the future. We will also explore the potential of unifying graphs and large language models [44] for the graph condensation task.

## Acknowledgment

In this work, S. Pan was supported by an Australian Research Council (ARC) Future Fellowship (FT210100097), and M. Zhang was supported by National Natural Science Foundation of China (NSFC) grant (62306084). This research is partially sponsored by the U.S. National Science Foundation through Grant No IIS-2302786.

## References

* (1)
* Batson et al. (2013) Joshua Batson, Daniel A Spielman, Nikhil Srivastava, and Shang-Hua Teng. 2013. Spectral Sparsification of Graphs: Theory and Algorithms. _Commun. ACM_ 56, 8 (2013), 87-94.
* Buffelli et al. (2022) Davide Buffelli, Pietro Lio, and Fabio Vandin. 2022. SizeShiftReg: a Regularization Method for Improving Size-Generalization in Graph Neural Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Cai et al. (2020) Chen Cai, Dingkang Wang, and Yusu Wang. 2020. Graph Coarsening with Neural Networks. In _International Conference on Learning Representations(ICLR)_.
* Cazenavette et al. (2022) George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. 2022. Dataset Distillation by Matching Training Trajectories. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 4750-4759.
* Chen et al. (2021) Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. 2021. A Unified Lottery Ticket Hypothesis For Graph Neural Networks. In _International Conference on Machine Learning (ICML)_. PMLR, 1695-1706.
* Chiang et al. (2019) Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. ClusterGCN: An Efficient Algorithm For Training Deep And Large Graph Convolutional Networks. In _Proceedings of the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD)_. 257-266.
* Defferrard et al. (2016) Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. _Advances in Neural Information Processing Systems (NeurIPS)_ 29 (2016).
* Deng and Russakovsky (2022) Zhiwei Deng and Olga Russakovsky. 2022. Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Ding et al. (2022) Mucong Ding, Xiaoyu Liu, Tahseen Rabbani, and Furong Huang. 2022. Faster Hyperparameter Search on Graphs via Calibrated Dataset Condensation. In _NeurIPS 2022 Workshop: New Frontiers in Graph Learning_.

* Du et al. [2019] Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu Xu. 2019. Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. _Advances in Neural Information Processing Systems (NeurIPS)_ 32 (2019).
* Du et al. [2021] Yichao Du, Pengfei Luo, Xudong Hong, Tong Xu, Zhe Zhang, Chao Ren, Yi Zheng, and Enhong Chen. 2021. Inheritance-guided hierarchical assignment for clinical automatic diagnosis. In _International Conference on Database Systems for Advanced Applications (DASFAA)_. Springer, 461-477.
* Febrinanto et al. [2023] Falih Gozi Febrinanto, Feng Xia, Kristen Moore, Chandra Thapa, and Charu Aggarwal. 2023. Graph lifelong learning: A survey. _IEEE Computational Intelligence Magazine_ 18, 1 (2023), 32-51.
* Gao et al. [2020] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2020. Graph Neural Architecture Search.. In _International Joint Conferences on Artificial Intelligence (IJCAI)_, Vol. 20. 1403-1409.
* Gasteiger et al. [2018] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. 2018. Predict then Propagate: Graph Neural Networks Meet Personalized PageRank. In _International Conference on Learning Representations (ICLR)_.
* Gupta and Chakraborty [2021] Viresh Gupta and Tanmoy Chakraborty. 2021. VIKING: Adversarial Attack on Network Embeddings via Supervised Network Poisoning. In _Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining (PAKDD)_. Springer, 103-115.
* Hamilton et al. [2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. _Advances in Neural Information Processing Systems (NeurIPS)_ 30 (2017).
* Hu et al. [2020] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. _Advances in Neural Information Processing Systems (NeurIPS)_ 33 (2020), 22118-22133.
* Huan et al. [2021] ZHAO Huan, YAO Quanming, and TU Weiwei. 2021. Search to aggregate neighborhood for graph neural network. In _International Conference on Data Engineering (ICDE)_. IEEE, 552-563.
* Huang et al. [2021] Wei Huang, Yayong Li, Richard Xu, Jie Yin, Ling Chen, Miao Zhang, et al. 2021. Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective. In _International Conference on Learning Representations (ICLR)_.
* Huang et al. [2021] Zengfeng Huang, Shengzhong Zhang, Chong Xi, Tang Liu, and Min Zhou. 2021. Scaling up Graph Neural Networks Via Graph Coarsening. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (SIGKDD)_. 675-684.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. 2018. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. _Advances in Neural Information Processing Systems (NeurIPS)_ 31 (2018).
* Jin et al. [2022] Ming Jin, Yuan-Fang Li, and Shirui Pan. 2022. Neural Temporal Walks: Motif-Aware Representation Learning on Continuous-Time Dynamic Graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Jin et al. [2023] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. 2023. Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. _arXiv preprint arXiv:2310.01728_ (2023).
* Jin et al. [2023] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S. Tseng, Yu Zheng, Lei Chen, and Hui Xiong. 2023. Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. _arXiv preprint arXiv:2310.10196_ (2023).

* Jin et al. [2022] Ming Jin, Yu Zheng, Yuan-Fang Li, Siheng Chen, Bin Yang, and Shirui Pan. 2022. Multivariate Time Series Forecasting with Dynamic Graph Neural ODEs. _IEEE Transactions on Knowledge and Data Engineering (TKDE)_ (2022).
* Jin et al. [2022] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang, and Bing Yin. 2022. Condensing Graphs Via One-Step Gradient Matching. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD)_. 720-730.
* Jin et al. [2021] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. 2021. Graph Condensation For Graph Neural Networks. In _International Conference on Learning Representations (ICLR)_.
* Jin et al. [2020] Yu Jin, Andreas Loukas, and Joseph JaJa. 2020. Graph Coarsening With Preserved Spectral Properties. In _International Conference on Artificial Intelligence and Statistics (ICAIS)_. PMLR, 4452-4462.
* Kipf and Welling [2016] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. In _In Neural Information Processing Systems Workshop_. 1-3.
* Lei and Tao [2023] Shiye Lei and Dacheng Tao. 2023. A Comprehensive Survey to Dataset Distillation. _arXiv preprint arXiv:2301.05603_ (2023).
* Liu et al. [2021] Huihui Liu, Yiding Yang, and Xinchao Wang. 2021. Overcoming catastrophic forgetting in graph neural networks. In _Proceedings of the Association for the Advanced of Artificial Intelligence (AAAI)_, Vol. 35. 8653-8661.
* Liu et al. [2023] Yixin Liu, Kaize Ding, Jianling Wang, Vincent Lee, Huan Liu, and Shirui Pan. 2023. Learning Strong Graph Neural Networks with Weak Information. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_.
* Liu et al. [2022] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip Yu. 2022. Graph Self-supervised Learning: A Survey. _IEEE Transactions on Knowledge and Data Engineering (TKDE)_ (2022).
* Liu et al. [2021] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. 2021. Anomaly Detection on Attributed Networks Via Contrastive Self-supervised Learning. _IEEE Transactions on Neural Networks and Learning Systems (TNNLS)_ 33, 6 (2021), 2378-2392.
* Liu et al. [2022] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. 2022. Towards unsupervised deep graph structure learning. In _Proceedings of the ACM Web Conference (WWW)_. 1392-1403.
* Liu et al. [2023] Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent Lee, and Shirui Pan. 2023. Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating. In _Proceedings of the Association for the Advanced of Artificial Intelligence (AAAI)_.
* Luo et al. [2023] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Normalizing flow-based neural process for few-shot knowledge graph completion. In _The 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_.
* Luo et al. [2023] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. _arXiv preprint arxiv:2310.01061_ (2023).
* Nguyen et al. [2020] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. 2020. Dataset Meta-Learning from Kernel Ridge-Regression. In _International Conference on Learning Representations (ICLR)_.
* Nguyen et al. [2021] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. 2021. Dataset Distillation with Infinitely Wide Convolutional Networks. _Advances in Neural Information Processing Systems (NeurIPS)_ 34 (2021), 5186-5198.
* Novak et al. [2022] Roman Novak, Jascha Sohl-Dickstein, and Samuel S Schoenholz. 2022. Fast Finite Width Neural Tangent Kernel. In _International Conference on Machine Learning (ICML)_. PMLR, 17018-17044.

* Pan et al. (2018) Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. 2018. Adversarially regularized graph autoencoder for graph embedding. In _Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)_. 2609-2615.
* Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. _arXiv preprint arxiv:2306.08302_ (2023).
* Pan et al. (2023) Shirui Pan, Yizhen Zheng, and Yixin Liu. 2023. Integrating Graphs with Large Language Models: Methods and Prospects. _arXiv preprint arXiv:2310.05499_ (2023).
* Parisi et al. (2019) German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. 2019. Continual lifelong learning with neural networks: A review. _Neural networks_ 113 (2019), 54-71.
* Qin et al. (2021) Yijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. 2021. Graph differentiable architecture search with structure learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, Vol. 34. 16860-16872.
* Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2001-2010.
* Rosasco et al. (2022) Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo Lomonaco, and Davide Bacciu. 2022. Distilled replay: Overcoming forgetting through synthetic samples. In _Continual Semi-Supervised Learning: First International Workshop (CSSL)_. Springer, 104-117.
* Sachdeva and McAuley (2023) Noveen Sachdeva and Julian McAuley. 2023. Data Distillation: A Survey. _arXiv preprint arXiv:2301.04272_ (2023).
* Sangermano et al. (2022) Mattia Sangermano, Antonio Carta, Andrea Cossu, and Davide Bacciu. 2022. Sample condensation in online continual learning. In _International Joint Conference on Neural Networks (IJCNN)_. IEEE, 01-08.
* Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. Active Learning for Convolutional Neural Networks: A Core-Set Approach. In _International Conference on Learning Representations (ICLR)_.
* Shang et al. (2019) Junyuan Shang, Tengfei Ma, Cao Xiao, and Jimeng Sun. 2019. Pre-training of graph augmented transformers for medication recommendation. In _International Joint Conference on Artificial Intelligence (IJCAI)_. 5953-5959.
* Sun et al. (2020) Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar. 2020. Adversarial attacks on graph neural networks via node injections: A hierarchical reinforcement learning approach. In _Proceedings of the Web Conference (WWW)_. 673-683.
* Tan et al. (2023) Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. 2023. Federated learning on non-iid graphs via structural knowledge sharing. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 37. 9953-9961.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing Data Using t-SNE. _Journal of Machine Learning Research_ 9, 11 (2008).
* Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph Attention Networks. In _International Conference on Learning Representations (ICLR)_.
* Wang et al. (2019) C Wang, S Pan, R Hu, G Long, J Jiang, and C Zhang. 2019. Attributed Graph Clustering: A Deep Attentional Embedding Approach. In _Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)_.
* Wang et al. (2017) Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017. Mgae: Marginalized graph autoencoder for graph clustering. In _Proceedings of ACM on Conference on Information and Knowledge Management (CIKM)_. 889-898.

* Wang et al. [2018] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. 2018. Dataset Distillation. _arXiv preprint arXiv:1811.10959_ (2018).
* Welling [2009] Max Welling. 2009. Herding Dynamical Weights to Learn. In _International Conference on Machine Learning (ICML)_. 1121-1128.
* Welling and Kipf [2017] Max Welling and Thomas N Kipf. 2017. Semi-supervised Classification with Graph Convolutional Networks. In _International Conference on Learning Representations (ICLR)_.
* Wiewel and Yang [2021] Felix Wiewel and Bin Yang. 2021. Condensed composite memory continual learning. In _International Joint Conference on Neural Networks (IJCNN)_. IEEE, 1-8.
* Wu et al. [2019] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying Graph Convolutional Networks. In _International Conference on Machine Learning (ICML)_. PMLR, 6861-6871.
* Xu et al. [2022] Zhe Xu, Boxin Du, and Hanghang Tong. 2022. Graph Sanitation with Application to Node Classification. In _Proceedings of the ACM Web Conference (WWW)_. 1136-1147.
* Yuan et al. [2023] Qiao Yuan, Sheng-Uei Guan, Pin Ni, Tianlun Luo, Ka Lok Man, Prudence Wong, and Victor Chang. 2023. Continual Graph Learning: A Survey. _arXiv preprint arXiv:2301.12230_ (2023).
* Zeng et al. [2019] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2019. GraphSAINT: Graph Sampling Based Inductive Learning Method. In _International Conference on Learning Representations (ICLR)_.
* Zhang et al. [2023] He Zhang, Bang Wu, Shuo Wang, Xiangwen Yang, Minhui Xue, Shirui Pan, and Xingliang Yuan. 2023. Demystifying uneven vulnerability of link stealing attacks against graph neural networks. In _International Conference on Machine Learning_. PMLR, 41737-41752.
* Zhang et al. [2022] He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, and Jian Pei. 2022. Trustworthy Graph Neural Networks: Aspects, Methods and Trends. _arXiv preprint arXiv:2205.07424_ (2022).
* Zhang et al. [2023] He Zhang, Xingliang Yuan, Quoc Viet Hung Nguyen, and Shirui Pan. 2023. On the interaction between node fairness and edge privacy in graph neural networks. _arXiv preprint arXiv:2301.12951_ (2023).
* Zhang et al. [2022] He Zhang, Xingliang Yuan, Chuan Zhou, and Shirui Pan. 2022. Projective ranking-based gnn evasion attacks. _IEEE Transactions on Knowledge and Data Engineering (TKDE)_ (2022).
* Zhang et al. [2022] Miao Zhang, Shirui Pan, Xiaojun Chang, Steven Su, Jilin Hu, Gholamreza Reza Haffari, and Bin Yang. 2022. BaLeNAS: Differentiable Architecture Search Via the Bayesian Learning Rule. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 11871-11880.
* Zhang et al. [2021] Miao Zhang, Steven W Su, Shirui Pan, Xiaojun Chang, Ehsan M Abbasnejad, and Reza Haffari. 2021. iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients. In _International Conference on Machine Learning (ICML)_. PMLR, 12557-12566.
* Zhang et al. [2021] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2021. Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. In _International Conference on Learning Representations (ICLR)_.
* Zhang et al. [2022] Wentao Zhang, Ziqi Yin, Zeang Sheng, Yang Li, Wen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, and Bin Cui. 2022. Graph Attention Multi-Layer Perceptron. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD)_. 4560-4570.
* Zhang et al. [2019] Xiaotong Zhang, Han Liu, Qimai Li, and Xiao Ming Wu. 2019. Attributed graph clustering via adaptive graph convolution. In _Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)_. 4327-4333.
* Zhao and Bilen [2023] Bo Zhao and Hakan Bilen. 2023. Dataset Condensation with Distribution Matching. In _IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_.

* [77] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset Condensation with Gradient Matching. In _International Conference on Learning Representations (ICLR)_.
* [78] Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew, and Shirui Pan. 2023. Towards Data-centric Graph Machine Learning: Review and Outlook. _arXiv preprint arXiv:2309.10979_ (2023).
* [79] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. 2022. Graph Neural Networks for Graphs with Heterophily: A Survey. _arXiv preprint arXiv:2202.07082_ (2022).
* [80] Xin Zheng, Miao Zhang, Chunyang Chen, Chaojie Li, Chuan Zhou, and Shirui Pan. 2022. Multi-relational Graph Neural Architecture Search with Fine-grained Message Passing. In _IEEE International Conference on Data Mining (ICDM)_.
* [81] Xin Zheng, Miao Zhang, Chunyang Chen, Qin Zhang, Chuan Zhou, and Shirui Pan. 2023. Auto-heg: Automated graph neural network on heterophilic graphs. _arXiv preprint arXiv:2302.12357_ (2023).
* [82] Yizhen Zheng, Shirui Pan, Vincent Cs Lee, Yu Zheng, and Philip S Yu. 2022. Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* [83] Fan Zhou and Chengtai Cao. 2021. Overcoming catastrophic forgetting in graph neural networks with experience replay. In _Proceedings of the Association for the Advanced of Artificial Intelligence (AAAI)_, Vol. 35. 4714-4722.
* [84] Daniel Zugner and Stephan Gunnemann. 2019. Adversarial Attacks on Graph Neural Networks via Meta Learning. In _International Conference on Learning Representations (ICLR)_.

## Appendix

This is the appendix of our work: **'Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data'**. In this appendix, we provide more details of the proposed SFGC in terms of related works, potential application scenarios, dataset statistics, method analysis, and experimental settings with some additional results.

## Appendix A Related Works

**Dataset Distillation (Condensation)** aims to synthesize a small typical dataset that distills the most important knowledge from a given large target dataset, such that the synthesized small dataset could serve as an effective substitution of the large target dataset for various scenarios [30, 49], _e.g._, model training and inference, architecture search, and continue learning. Typically, DD [59] and DC-KRR [39] adopted the meta-learning framework to solve bi-level distillation objectives through calculating meta-gradients. In contrast, DC [77], DM [76], and MTT [4] designed surrogate functions to avoid unrolled optimization through the gradient matching, feature distribution matching, and training trajectory matching, respectively, where the core idea is to effectively mimic the large target dataset in the synthesized small dataset. Except for the image data condensed by the above-mentioned works, GCOND [27] first extended the online gradient matching scheme in DC [77] to structural graph data, along with parameterized graph structure learning module to synthesize condensed edge connections. Furthermore, DosCond [26] proposed single-step gradient matching to synthesize graph nodes, with a probabilistic graph model to condense structures on the graph classification task. In this work, we eliminate the process of synthesizing graph structures and propose a novel structure-free graph condensation paradigm, to distill the large-scale graph to the small-scale graph-free data, leading to the easier optimization process of condensation. Meanwhile, the structure-free characteristic allows condensed data better generalization ability to different GNN architectures.

**Graph Size Reduction** aims to reduce the graph size to fewer nodes and edges for effective and efficient GNN training, including graph sampling [66, 6], graph coreset [47, 60], graph sparsification [1, 5], graph coarsening [3, 28], and recently rising graph condensation [27, 9, 26]. Concretely, graph sampling methods [66, 6] and graph coreset methods [47, 60] sample or select the subset of nodes and edges from the whole graph, such that the information of the derived sub-graph is constrained by the whole large-scale graph, which considerably limits the expressiveness of the size-reduced graph. Moreover, graph sparsification methods [1, 5] and graph coarsening methods [3, 28] reduce the number of edges and nodes by simplifying the edge connections and grouping node representations of the large-scale graph, respectively. The core idea of both sparsification and coarsening is to preserve specific large-scale graph properties (_e.g._, spectrum and principle eigenvalues) in the sparse and coarsen small graph. The preserved graph properties in the small-scale graph, however, might not be suitable for downstream GNN tasks. In contrast, our work focuses on graph condensation to directly optimize and synthesize the small-scale condensed data, which breaks information constraints of the large-scale graph and encourages consistent GNN test performance.

## Appendix B Potential Application Scenarios

We would like to highlight the significance of graph condensation task to various application scenarios within the research field of dataset distillation/condensation, while comprehensive overviews can be found in survey works [30, 59]. Specifically, we present several potential scenarios where our proposed structure-free graph condensation method could bring benefits:

**Graph Neural Architecture Search**. Graph neural architecture search (GraphNAS) aims to develop potential and expressive GNN architectures beyond existing human-designed GNNs. By automatically searching in a space containing various candidate GNN architecture components, GraphNAS could derive powerful and creative GNNs with superior performance on specific graph datasets for specific tasks [80, 81, 46, 13, 18]. Hence, GraphNAS needs to repeatedly train different potential GNN architectures on the specific graph dataset, and ultimately selects the optimal one. When in the large-scale graph, this would incur severe computation and memory costs. In this case, searching on our developed small-scale condensed graph-free data, a representative substitution of the large-scale graph, could significantly benefit for saving many computation costs and accelerating new GNN architecture development in GraphNAS research field.

**Privacy Protection.** Considering the outsourcing scenario of graph learning tasks, the original large-scale graph data is not allowed to release due to privacy, for example, patients expect to use GNNs for medical diagnosis without their personal medical profiles being leaked [52; 11]. In this case, as a compact and representative substitution, the synthesized small-scale condensed graph could be used to train GNN models, so that the private information of the original graph data can be protected. Besides, considering the scenario that over-parameterized GNNs might easily memorize training data, inferring the well-trained models could cause potential privacy leakage issue. In this case, we could release a GNN model trained by the synthesized small-scale condensed graph, so that the model avoids explicitly training on the original large-scale graph and consequently helps protect its data privacy.

**Adversarial Robustness.** In practical applications, GNNs might be attacked with disrupted performance, when attackers impose adversarial perturbations to the original graph data [68], for instance, poisoning attacks on graph data [53; 15; 84], where attackers attempt to alter the edges and nodes of training graphs of a target GNN. Training on poisoned graph data could significantly damage GNNs' performance. In this case, given a poisoned original training graph, graph condensation could synthesize a new condensed graph from it, which we use to train the target GNN would achieve comparable test performance with that trained by the original training graph before being poisoned. Hence, the new condensed graph could eliminate adversarial samples in the original poisoned graph data with great adversarial robustness, so that using it to train a GNN would not damage its performance for inferring test graphs.

**Continual learning.** Continual learning (CL) aims to progressively accumulates knowledge over a continuous data stream to support future learning while maintaining previously learned information [45; 12; 65]. One of key challenges of CL is catastrophic forgetting [31; 83], where knowledge extracted and learned from old data/tasks are easily forgotten when new information from new data/tasks are learned. Some works have studied that data distillation/condensation is an effective solution to alleviate catastrophic forgetting [8; 48; 50; 62], where the distilled and condensed data is taken as representative summary stored in a replay buffer that is continually updated to instruct the training of subsequent data/tasks.

To summarize, graph condensation task holds great promise and is expected to bring significant benefits to various graph learning tasks and applications. By producing compact, high-quality, small-scale condensed graph data, graph condensation has the potential to enhance the efficiency and effectiveness of future graph machine learning works.

## Appendix C Dataset Details

We provide the details of the original dataset statistics in Table A1. Moreover, we also compare the statistics of our condensed graph-free data with GCOND [27] condensed graphs in Table A2. It can be observed that both GCOND [27] and our proposed SFGC significantly reduce the numbers of nodes and edges from large-scale graphs, as well as the data storage. Importantly, our proposed SFGC directly reduces the number of edges to 0 by eliminating graphs structures in the condensation process, but with superior node attribute contexts integrating topology structure information.

## Appendix D More Analysis of Structure-free Paradigm

In this section, we theoretically analyze the rationality of the proposed structure-free paradigm from the views of statistical learning and information flow, respectively.

The View of Statistical Learning.We start from the graph condensation optimization objective of synthesizing graphs structures in Eq. (1) of the main submission. Considering its inner loops \(\bm{\theta}_{\mathcal{T}^{\prime}}=\operatorname*{arg\,min}_{\bm{\theta}} \mathcal{L}\left[\operatorname{GNN}_{\bm{\theta}}\left(\mathbf{X}^{\prime}, \mathbf{A}^{\prime}\right),\mathbf{Y}^{\prime}\right]\) with \(\mathbf{A}^{\prime}=\operatorname{GSL}_{\bm{\psi}}\left(\mathbf{X}^{\prime}\right)\), it equals to learn the conditional probability \(Q(\mathbf{Y}^{\prime}\mid\mathcal{T}^{\prime})\) given the condensed graph \(\mathcal{T}^{\prime}=(\mathbf{X}^{\prime},\mathbf{A}^{\prime},\mathbf{Y}^{ \prime})\) as

\[Q(\mathbf{Y}^{\prime}\mid\mathcal{T}^{\prime}) \approx\sum_{\mathbf{A}^{\prime}\in\bm{\psi}(\mathbf{X}^{\prime})} Q(\mathbf{Y}^{\prime}\mid\mathbf{X}^{\prime},\mathbf{A}^{\prime})Q(\mathbf{A}^{ \prime}\mid\mathbf{X}^{\prime})\] (8) \[=\sum_{\mathbf{A}^{\prime}\in\bm{\psi}(\mathbf{X}^{\prime})}Q( \mathbf{X}^{\prime},\mathbf{A}^{\prime},\mathbf{Y}^{\prime})/Q(\mathbf{X}^{ \prime},\mathbf{A}^{\prime})\cdot Q(\mathbf{X}^{\prime},\mathbf{A}^{\prime})/Q (\mathbf{X}^{\prime})\] \[=\sum_{\mathbf{A}^{\prime}\in\bm{\psi}(\mathbf{X}^{\prime})}Q( \mathbf{X}^{\prime},\mathbf{A}^{\prime},\mathbf{Y}^{\prime})/Q(\mathbf{X}^{ \prime})\] \[= Q(\mathbf{X}^{\prime},\mathbf{Y}^{\prime})/Q(\mathbf{X}^{ \prime})=Q(\mathbf{Y}^{\prime}\mid\mathbf{X}^{\prime}),\]

where we simplify the notation of graph structure learning module \(\operatorname{GSL}_{\bm{\psi}}\) as parameterized \(\bm{\psi}\left(\mathbf{X}^{\prime}\right)\). As can be observed, when the condensed graph structures are learned from the condensed nodes as \(\mathbf{A}^{\prime}\in\bm{\psi}\left(\mathbf{X}^{\prime}\right)\), the optimization objective of the conditional probability is not changed, while its goal is still to solve the posterior probability \(Q(\mathbf{Y}^{\prime}\mid\mathbf{X}^{\prime})\). In this way, eliminating graph structures to conduct structure-free condensation is rational from the view of statistical learning. By directly synthesizing the graph-free data, the proposed SFGC could ease the optimization process and directly transfer all the informative knowledge of the large-scale graph to the condensed graph node set without structures. Hence, the proposed SFGC conducts more compact condensation to derive the small-scale graph-free data via Eq. (6) of the main manuscript, whose node attributes already integrate implicit topology structure information.

The View of Information Flow.For training on large-scale graphs to obtain offline parameter trajectories, we solve the node classification task on \(\mathcal{T}=(\mathbf{X},\mathbf{A},\mathbf{Y})\) with a certain GNN model as

\[\bm{\theta}_{\mathcal{T}}^{*}=\operatorname*{arg\,min}_{\bm{\theta}} \mathcal{L}_{\mathrm{cls}}\left[\operatorname{GNN}_{\bm{\theta}}(\mathbf{X}, \mathbf{A}),\mathbf{Y}\right],\] (9)

where \(*\) denotes the optimal training parameters that build the training trajectory distribution \(P_{\bm{\Theta}_{\mathcal{T}}}\). The whole graph information, _i.e._, node attributes \(\mathbf{X}\) and topology structures \(\mathbf{A}\) are both embedded in the latent space of GNN network parameters. Hence, the large-scale graph information flows to GNN parameters as \((\mathbf{X},\mathbf{A})\Rightarrow P_{\bm{\Theta}_{\mathcal{T}}}\). In this way, by meta-sampling in the trajectory distribution, Eq. (4) and Eq. (5) in the main manuscript explicitly transfer learning behaviors of the large-scale graph to the parameter space \(\widetilde{\bm{\theta}}_{\mathcal{S}}\) of \(\operatorname{GNN}_{\mathcal{S}}\) as \(P_{\bm{\Theta}_{\mathcal{T}}}\Rightarrow\widetilde{\bm{\theta}}_{\mathcal{S}}\). As a result, the informative knowledge of the large-scale graphs, _i.e._, node attributes and topology structure information \((\mathbf{X},\mathbf{A})\), would be comprehensively transferred as \((\mathbf{X},\mathbf{A})\Rightarrow P_{\bm{\Theta}_{\mathcal{T}}}\Rightarrow \widetilde{\bm{\theta}}_{\mathcal{S}}\). In this way, we could identify the critical goal of graph condensation is to further transfer the knowledge in \(\widetilde{\bm{\theta}}_{\mathcal{S}}\) to the output condensed graph data as:

\[(\mathbf{X},\mathbf{A})\Rightarrow P_{\bm{\Theta}_{\mathcal{T}}} \Rightarrow\bm{\Theta}_{\mathcal{S}}\Rightarrow\mathcal{T}^{\prime}=( \mathbf{X}^{\prime},\mathbf{A}^{\prime}),\quad\mathrm{GC}.\] (10) \[(\mathbf{X},\mathbf{A})\Rightarrow P_{\bm{\Theta}_{\mathcal{T}}} \Rightarrow\bm{\Theta}_{\mathcal{S}}\Rightarrow\mathcal{S}=(\widetilde{ \mathbf{X}}),\quad\mathtt{SFGC}.\]

where GC and SFGC are corresponding to the existing graph condensation and the proposed structure-free graph condensation, respectively.

Hence, from the view of information flow, we could observe that condensing structures would not inherit more information from the large-scale graph. Compared with GC which formulates the condensed graph into nodes and structures, the proposed SFGC directly distills all the large-scale graph knowledge into the small-scale graph node set without structures. Consequently, the proposed SFGC conducts more compact condensation to derive the small-scale graph-free data, which implicitly encodes the topology structure information into the discriminative node attributes.

## Appendix E More Experimental Settings and Results

### Time Complexity Analysis & Dynamic Memory Cost Comparison

We first analyze the time complexity of the proposed method and compare the running time between our proposed SFGC and GCOND [27].

Let the number of GCN layers be \(L\), the large-scale graph node number be \(N\), the small-scale condensed graph node number be \(N^{\prime}\), the the feature dimension be \(d\), the time complexity of calculating training trajectory meta-matching objective function is about \(TK\mathcal{O}(LN^{\prime}d^{2}+LN^{\prime}d)\), for each process of the forward, backward, and training trajectory meta-matching loss calculation, where \(T\) denotes the number of iterations and \(K\) denotes the meta-matching steps. Note that the offline expert training stage costs an extra \(TK\mathcal{O}(LEd+LNd^{2})\) on the large-scale graph, where \(E\) is the number of edges.

In contrast, for GCOND, it has at least \(TK\mathcal{O}(LN^{\prime 2}d+LN^{\prime}d)+TK\mathcal{O}(N^{\prime 2}d^{2})\), and also additional \(TK\mathcal{O}(LEd+LNd^{2})\) on the large-scale graph, where \(K\) denotes the number of different initialization here. It can be observed that our proposed SFGC has a smaller time complexity compared to GCOND, which can be mainly attributed to our structure-free paradigm when the adjacency matrix related calculation in \(\mathcal{O}(LN^{\prime 2}d)\) can be avoided. The corresponding comparison of running time in the graph condensation process can be found in Table A3. As can be observed, both results on time complexity and running time could verify the superiority of the proposed SFGC.

Moreover, we present the comparison result of the dynamic tensor used memory cost between the online short-range gradient matching method GCOND [27] and our offline long-range meta-matching SFGC. As shown in Fig. A1, we consider three stages of optimizing the objective function, _i.e._, before outer optimization, in the outer and inner optimization, and after outer optimization. It can be observed that the proposed SFGC could significantly alleviate heavy online memory and computation costs. This can be attributed to its offline parameter matching schema.

### Effectiveness of Graph Neural Feature Score in SFGC

To verify the effectiveness of graph neural feature score \(\gamma_{\text{gnf}}\) in the proposed SFGC, we consider the following two aspects in dynamic evaluation: (1) node classification performance at different meta-matching steps in Table A4; (2) learning time comparison between iterative GNN training and our closed-form \(\gamma_{\text{gnf}}\) in Fig. A2.

As shown in Table A4, we select certain meta-matching step intervals, _i.e._, 1000, 2000, and 3000, for testing their condensed data's performance, which is a commonly-used evaluation strategy for existing methods. Here, we set long-enough meta-matching steps empirically to ensure sufficient learning to expert training trajectory-built parameter distribution. And we compare these interval-step results with the performance of our condensed graph-free data, which is selected at certain steps ofthe meta-matching process according to the metric \(\gamma_{\text{gnf}}\). Overall, \(\gamma_{\text{gnf}}\) could select optimal condensed graph-free data with superior effectiveness at best meta-matching steps.

For the learning time comparison between GNN iterative training _vs._ GNTK-based closed-form solutions of \(\gamma_{\text{gnf}}\) in Fig. A2, we consider the time of GNN iterative training that covers all training epochs under the best test performance for fair comparisons. This is due to the fact that the iterative training evaluation strategy mandates the complete training of a GNN model from scratch at each meta-matching step. For instance, in Flickr dataset (\(r=0.1\%\)), we calculate 200 epochs running time, _i.e._, 0.845s, which is the optimal parameter setting for training GNN under 0.1% condensation ratio. As can be generally observed, for all datasets, the proposed GNTK-based closed-form solutions of \(\gamma_{\text{gnf}}\) significantly save the learning time for evaluating the condensed graph-free data in meta-matching, illustrating our SFGC's high dynamic evaluation efficiency.

### Analysis of Different Meta-matching Ranges

To explore the effects of different ranges of long-term meta-matching, we present the different step combinations of \(q\) steps (student) in \(\mathrm{GNN}_{\mathcal{S}}\) and \(p\) steps (expert) of \(\mathrm{GNN}_{\mathcal{T}}\) in Eq. (5) of the main manuscript on Ogbn-arxiv dataset under \(r=0.05\%\). The results are shown in Fig. A3. As can be observed, there exists the optimal step combination of \(q\) student steps (600) and expert \(p\) steps (1800). Under such a setting, the condensed small-scale graph-free data has the best node classification performance. Moreover, the quality and expressiveness of the condensed graph-free data moderately vary with different step combinations, but the variance is not too drastic.

More detailed settings of hyper-parameters of \(q\) steps (student) in \(\mathrm{GNN}_{\mathcal{S}}\) and \(p\) steps (expert) of \(\mathrm{GNN}_{\mathcal{T}}\) in the long-term meta-matching, as well as the meta-matching learning rate (LR) in the outer-level optimization and \(\mathrm{GNN}_{\mathcal{S}}\) learning rate (step size) \(\zeta\) (Algorithm 1 of the main manuscript) in the inner-level optimization, are listed in Table A5.

[MISSING_PAGE_FAIL:21]

As can be observed, our condensed graph enables the GNN model to achieve comparable results with many graph node clustering baseline methods, even though we do not customize the optimization objective targeting node clustering task in the condensation process. These results could justify that: (1) the condensed graph-free data that is synthesized based on GNN classification experts, could also work well in other tasks, even without task-specific customization in the condensation process; (2) the condensed graph-free data contains adequate information about the original large-scale graph, which can be taken as the representative and informative substitution of the original large-scale graph, reflecting the good performance of our proposed method in graph condensation.

### Visualization of Our Condensed Graph-free Data

we present t-SNE [55] plots of the condensed graph-free data of our proposed SFGC under the minimum condensation ratios over all datasets in Fig. A4. The condensed graph-free data shows a well-clustered pattern over Cora and Citeseer. In contrast, on larger-scale datasets with larger condensation ratios, we can also observe some implicit clusters within the same class. These results show that the small-scale graph-free data synthesized by our method has discriminative and representative node attributes that capture comprehensive information from large-scale graphs.