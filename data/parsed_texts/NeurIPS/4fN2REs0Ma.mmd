# Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers

 Siyu Chen

Department of Statistics and Data Science, Yale University

siyu.chen.sc3226@yale.edu

&Heejune Sheen

Department of Statistics and Data Science,

Yale University

heejune.sheen@yale.edu

&Tianhao Wang

Toyota Technological Institute at Chicago

tianhao.wang@ttic.edu

&Zhuoran Yang

Department of Statistics and Data Science,

Yale University

zhuoran.yang@yale.edu

equal contribution

###### Abstract

In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures. In particular, most existing work only theoretically explains how the attention mechanism facilitates ICL under certain data models. It remains unclear how the other building blocks of the transformer contribute to ICL. To address this question, we study how a two-attention-layer transformer is trained to perform ICL on \(n\)-gram Markov chain data, where each token in the Markov chain statistically depends on the previous n tokens. We analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention, and a feed-forward layer with normalization. We prove that the gradient flow with respect to a cross-entropy ICL loss converges to a limiting model that performs a generalized version of the "induction head" mechanism with a learned feature, resulting from the congruous contribution of all the building blocks. In the limiting model, the first attention layer acts as a _copier_, copying past tokens within a given window to each position, and the feed-forward network with normalization acts as a _selector_ that generates a feature vector by only looking at informationally relevant parents from the window. Finally, the second attention layer is a _classifier_ that compares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output. Our theory is further validated by simulation experiments.

## 1 Introduction

In-context learning (ICL) (Brown et al., 2020) has emerged as a crucial aspect of large language model (LLM) (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Anthropic, 2023; Team et al., 2023) functionality, enabling pre-trained LLMs to solve user-specified tasks during inference without updating model parameters. In ICL, a pre-trained LLM, typically a transformer, receives prompts containing a few demonstration examples sampled from a task-specific distribution and produces the desired output for that task. This capability is noteworthy because the tasks addressed during the ICL might not be part of the original training data set. The success of ICL requires the LLM to perform certain learning processes during inference.

Although many previous works aim to demystify ICL from either empirical or theoretical perspectives, the theoretical foundations of ICL remain elusive. This is primarily due to the complexity of transformer architectures, which integrate token and position embeddings, multiple layers of multi-head softmax attention, layer normalization, and feedforward neural networks. When it comes to understanding how the ICL ability emerges in transformers after training, existing works often focus on simplified models, such as linear attention mechanisms or single-layer transformers (Von Oswald et al., 2023), and ICL tasks are typically confined to linear regression (Akyurek et al., 2023). This leaves a gap in understanding how full-fledged transformer architectures facilitate ICL of more complex tasks, especially when latent causal structures exist among the tokens in a sequence.

In this paper, our aim is to narrow this gap by studying how a two-attention-layer transformer is trained to perform ICL of an \(n\)-gram Markov chain model, where each token in the Markov chain statistically depends on the \(n\) tokens before it, known as the parent set. Specifically, we consider a transformer model with relative positional embedding (RPE) (He et al., 2020), multi-head softmax attention, and a feed-forward network (FFN) layer with normalization. We employ such a transformer model to predict the \((L+1)\)-th token of an \(n\)-gram Markov chain, with the first \(L\) tokens given as the prompt, where \(L+1\) is the sequence length. Here the \(L\)-token sequence is sampled from a random Markov chain model, where a random transition kernel obeying the \(n\)-gram Markov property is used to generate sequences. The token sequence is fed into the transformer model, which outputs a probability distribution over the vocabulary set to predict the \((L+1)\)-th token. To train the transformer model, we sample token sequences from these random Markov chain models and minimize the cross-entropy loss between the predicted token distribution and the true token distribution.

Under this setting, we aim to answer the following three questions: (i) _Does the gradient flow with respect to the cross-entropy loss converge during training?_ (ii) _If yes, how does the limiting model perform ICL?_ (iii) _How do the building blocks of the transformer model contribute to ICL?_

Main Results.We provide an affirmative answer to the Question (i) by proving that the gradient flow converges during training. In particular, we identify three phases of training dynamics: in the first stage, FFN learns the potential parent set; in the second stage, each attention head of the first multi-head softmax attention layer learns to focus on a single parent token selected by FFN; and in the final stage, the parameter of the second attention layer increases, and the transformer approaches the limiting model. Moreover, for Questions (ii) and (iii), we show that the limiting model performs a specialized form of exponential kernel regression, dubbed "**generalized induction head**", which requires the congruous contribution of all the building blocks. Specifically, the first attention layer acts as a _copier_, copying past tokens within a given window to each position. The FFN layer acts as a _selector_ that generates a feature vector by only looking at informationally relevant parents from the window according to a modified \(\chi^{2}\)-mutual information. Finally, the second attention layer is an _exponential kernel classifier_ that compares the features at each position with those created for the output position \(L+1\), and uses the resulting similarity scores to generate the desired output. When specialized to the case where \(n=1\), the limiting model selects the true parent token and implements the _induction head_ mechanism (Elhage et al., 2021). In this case, we recover the theory in Nichani et al. (2024). Our theory is complemented by numerical experiments, which validate the three-phase training dynamics and mechanism of generalized induction head.

To our best knowledge, our work is the first to provide a comprehensive understanding of how ICL is empowered by a collaboration of different building blocks in a transformer model. In particular, we identify the pivotal roles played by RPE in the copier component, the FFN layer with normalization in the selector component, and attention in the classifier component. We believe our work will shed light on the theoretical understanding of ICL for more complicated tasks.

Related Works.Our work adds to the rapidly growing literature on understanding in-context learning by transformers. We defer an in-depth discussion on related works in Appendix SSA due to the page limit.

Roadmap.The rest of the paper is organized as follows: We introduce the problem setup of ICL of Markov chains in SS2. Then in SS3, we present the main theoretical results and related discussions. A proof sketch is provided in SSD. Finally, we present corresponding experiment results in SSB, and the detailed proofs are deferred to the Appendix.

Notation.We denote by \(e_{1},\dots,e_{d}\) the standard basis vectors in \(\mathbb{R}^{d}\) and by \(\mathbf{1}\) the all-one vector in \(\mathbb{R}^{d}\). We denote by \(\sigma(\cdot)\) the softmax function such that the \(i\)-th coordinate of \(\sigma(x)\) is \(\exp(x_{i})/\sum_{l=1}^{L}\exp(x_{l})\) for \(x\in\mathbb{R}^{L}\). By default, the softmax operation will always be applied row-wise. For any integer \(n>0\), we denote \([n]:=\{1,\dots,n\}\). For a vector \(w\in\mathbb{R}^{M}\), we denote by \(w_{i}\) the \(i\)-th entry of \(w\) and \(w_{-i}\) the \((M+1-i)\)-th entry of \(w\) for positive integer \(i\in[M]\). For a matrix \(W\), we denote by \(W(i,j)\) the entry at the \(i\)-th row and \(j\)-th column of \(W\). For two vectors \(u\) and \(v\), we write \(u/v\) as the vector obtained by taking element-wise division between \(u\) and \(v\). We denote by \(a\lor b\) and \(a\wedge b\) the maximum and minimum of \(a\) and \(b\), respectively. We denote by \(x_{s:t}\) the sequence \(\{x_{s},x_{s+1},\dots,x_{t}\}\). For a class \(\mathcal{X}\), we denote by \(\Delta(\mathcal{X})\) the space of probability measures over \(\mathcal{X}\). We use the standard big O notation throughout the paper.

## 2 Problem Setup: In-Context Learning of Markov Chains

In this section, we present the details of the problem setting. In particular, we first introduce the statistical problem of ICL of \(n\)-gram Markov chains in SS2.1 and then lay out the details of the transformer model in SS2.2.

### In-Context Learning and \(n\)-Gram Markov Chains

We study how autoregressive transformers are trained to perform in-context learning (ICL). A pre-trained transformer can be viewed as a conditional distribution \(f_{\texttt{tf}}(\cdot\,|\,\texttt{prompt})\) over a finite vocabulary set \(\mathcal{X}\), where prompt is a sequence of tokens in \(\mathcal{X}\). We consider an in-context unsupervised learning problem where the pre-trained transformer \(f_{\texttt{tf}}\) is used to predict the \((L+1)\)-th token \(x_{L+1}\) with the first \(L\) tokens being the prompt. Here \(L\) is a fixed number and the joint distribution of the sequence \(x_{1:(L+1)}\) is sampled from a random \(n\)-gram Markov chain. In other words, with \(x_{1:(L+1)}\) sampled from some distribution, we evaluate how well \(f_{\texttt{tf}}(\cdot\,|\,x_{1:L})\) predicts the distribution of \(x_{L+1}\).

\(n\)-Gram Markov Chains.We assume the data comes from a mixture of \(n\)-gram Markov chain model, denoted by a tuple \((\mathcal{X},\texttt{pa},\mathcal{P},\mu_{0})\), where \(\mathcal{X}\) is the state space and \(\texttt{pa}=(-r_{1},\dots,-r_{n})\) is the parent set with positive integers \(r_{1}<r_{2}<\dots<r_{n}\). That is, for each \(l>r_{n}\), \(x_{l}\) only statistically depends on \((x_{l-r_{n}},\dots,x_{l-r_{1}})\), which is denoted by \(X_{\texttt{pa}(l)}\) and referred to as the parent tokens of \(x_{l}\). We let \(d=|\mathcal{X}|\) denote the vocabulary size. Moreover, \(\mathcal{P}\) is a probability distribution over the set of Markov transition kernels respecting the parent structure specified by \(\texttt{pa}\), and \(\mu_{0}\) is the joint distribution of the first \(r_{n}\) tokens \(x_{1:r_{n}}\). Note that the size of the parent set \(n\) can be smaller than or equal to \(r_{n}\). Thus, the sequence \(x_{1:(L+1)}\) is generated as follows: (i) sample initial \(r_{n}\) tokens \((x_{1},\dots,x_{r_{n}})\sim\mu_{0}\), (ii) sample a random transition kernel \(\pi\sim\mathcal{P}\), where \(\pi\colon\mathcal{X}^{n}\to\Delta(\mathcal{X})\), and (iii) sample token \(x_{l}\sim\pi(\cdot\,|\,X_{\texttt{pa}(l)})\) for \(l=r_{n}+1,\dots,L+1\). See Figure 1 for an illustration of the generating model of \(x_{1:(L+1)}\).

Cross-Entropy Loss.When \(x_{1:(L+1)}\) is generated, \(x_{1:L}\) is fed into the transformer \(f_{\texttt{tf}}\) to predict \(x_{L+1}\). To assess the performance of ICL, we adopt the population cross-entropy (CE) loss

\[\mathcal{L}(f_{\texttt{tf}})=-\mathbb{E}_{\pi\sim\mathcal{P},x_{1:(L+1)}}\big{[} \log\big{(}f_{\texttt{tf}}(x_{L+1}\,|\,x_{1:L})+\epsilon\big{)}\big{]},\] (2.1)

where \(\epsilon>0\) is a small constant introduced for numerical stability and in the sequel we will take \(\varepsilon=O(L^{-1/2})\). Here, the expectation is taken with respect to the joint distribution of \(x_{1:(L+1)}\) (including the randomness of \(\pi\sim\mathcal{P}\)). When setting \(\epsilon=0\), we note that minimizing this cross-entropy loss is equivalent to minimizing the KL divergence

\[\mathbb{E}_{\pi\sim\mathcal{P},x_{1:L}}\big{[}\text{KL}(\pi(\cdot\,|\,X_{ \texttt{pa}(L+1)})\,\|\,f_{\texttt{tf}}(\cdot\,|\,x_{1:L}))\big{]}.\]

As a remark, we also relax a condition in Nichani et al. (2024) where the last token \(x_{L}\) has to be resampled from a uniform distribution. In addition, our analysis can also be extended to sequential CE loss, which corresponds to predicting every token in the sequence given the past rather than just the last token \(x_{L+1}\). This is closer to the training paradigm used in practice (Brown et al., 2020). See SSC.4 for a further discussion on the sequential CE loss.

Figure 1: A two-gram Markov chain with parent set \(\texttt{pa}=\{-1,-2\}\).

### A Two-Layer Transformer Model

We consider a class of two-attention-layer transformer model, denoted by \(\mathtt{TF}(M,H,d,D)\), which incorporates Relative Positional Embedding (RPE) (He et al., 2020), Multi-Head Attention (MHA) (Vaswani et al., 2017), and a Feed-Forward network (FFN) with normalization. Here \(M\) is an integer that specifies the window size of RPE, \(H\) is the number of heads in the first attention layer, \(d\) is the vocabulary size, and \(D\) is an integer that controls the complexity of FFN. The details of \(\mathtt{TF}(M,H,d,D)\) are as follows.

Token Embedding, Input and Output.Note that each token takes values in \(\mathcal{X}\) with \(d=|\mathcal{X}|\). We embed the tokens into one-hot vectors in \(\mathbb{R}^{d}\), and thus we can identify \(\mathcal{X}\) as the canonical basis in \(\mathbb{R}^{d}\), i.e., \(\mathcal{X}=\{e_{1},\dots,e_{d}\}\). A transformer model can be viewed as a mapping from \(\mathbb{R}^{(L+1)\times d}\) to \(\Delta(\mathcal{X})\). In particular, given the input sequence \(x_{1:L}\), we denote \(X=(x_{1},\dots,x_{L})^{\top}\in\mathbb{R}^{L\times d}\), and we append a zero vector \(\mathbf{0}\in\mathbb{R}^{d}\) to the sequence, and define \(\widetilde{X}=(x_{1},\dots,x_{L},\mathbf{0})^{\top}\in\mathbb{R}^{(L+1) \times d}\). The transformer takes \(\widetilde{X}\) as input and outputs a probability distribution over \(\mathcal{X}\).

The First Attention Layer.The input sequence is processed by the first attention layer with \(H\) parallel heads. In all heads, we discard the token information and only use RPE to compute the attention score. Specifically, each attention head \(h\) maps \(\widetilde{X}\) into a sequence in \(\mathbb{R}^{d}\) with length \(L+1\), denoted by \(V^{(h)}=(v_{1}^{(h)},\dots,v_{L+1}^{(h)})^{\top}\in\mathbb{R}^{(L+1)\times d}\). For any \(l\in[L+1]\), \(v_{l}^{(h)}\) is computed via

\[v_{l}^{(h)}=\sum_{j=1}^{L}\sigma_{j}\big{(}W_{P}^{(h)}(l,\cdot)\big{)}\cdot x_ {j}=\sum_{j=1}^{L}\frac{\exp\bigl{(}W_{P}^{(h)}(l,j)\bigr{)}\cdot x_{j}}{\sum_ {k=1}^{L}\exp\bigl{(}W_{P}^{(h)}(l,k)\bigr{)}}.\] (2.2)

That is, we use the RPE parameter \(W_{P}^{(h)}\) to construct a weighted sum over the input sequence at each position \(l\in[L+1]\). Here \(W_{P}^{(h)}\) is the RPE matrix of the \(h\)-th head.

Feed-Forward Network with Normalization.Following the first attention layer, we concatenate the outputs of the \(H\) attention heads and define \(V=(V^{(1)},\dots,V^{(H)})\in\mathbb{R}^{(L+1)\times Hd}\). Here we abuse the notation and write \(V=(v_{1},\dots,v_{L+1})^{\top}\), i.e., each \(v_{l}\) is the \(l\)-th row of \(V\). For any vector \(v\in\mathbb{R}^{Hd}\), we can split it into \((v^{(1)\top},\dots,v^{(H)\top})^{\top}\) where each block \(v^{(h)}\in\mathbb{R}^{d}\). For embedding dimension \(d_{e}\), each vector of \(V\) is passed through an FFN \(\phi(\cdot):\mathbb{R}^{Hd}\to\mathbb{R}^{d_{e}}\), which specifies a polynomial kernel such that for any \(v,v^{\prime}\in\mathbb{R}^{Hd}\), we have

\[\langle\phi(v),\phi(v^{\prime})\rangle=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{ \mathcal{S}}^{2}\cdot\prod_{h\in\mathcal{S}}\langle v^{(h)},v^{\prime(h)}\rangle.\] (2.3)

Here, the low-degree parent set \([H]_{\leq D}:=\{\mathcal{S}\subseteq[H]:|\mathcal{S}|\leq D\}\) contains all subsets of \([H]\) with cardinality at most \(D\), and \(\{c_{\mathcal{S}}:\mathcal{S}\in[H]_{\leq D}\}\) are the corresponding trainable parameters of \(\phi(\cdot)\). Therefore, the FFN \(\phi(\cdot)\) specifies a kernel on the output of the multihead attention which induces a special inner product structure. While (2.3) characterizes \(\phi(\cdot)\) implicitly, we provide an explicit construction of \(\phi(\cdot)\) in Lemma C.1 as a vector-valued mapping whose entries are monomials of the

Figure 2: Illustration of the relationship between RPE vector \(w^{(h)}\) and corresponding matrix \(W_{P}^{(h)}\).

input's entries. Moreover, the complexity of \(\phi(\cdot)\) is controlled by the maximum degree \(D\), which also influences the embedding dimension \(d_{e}\) as we show in the construction.

Furthermore, to control the magnitude of the FFN outputs, we normalize \(\phi(\cdot)\) by letting \(u_{l}=\phi(v_{l})/\sqrt{C_{D}}\) for all \(l\in[L+1]\), where we define \(C_{D}=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\). Such a normalization scheme is motivated by the standard layer normalization (Ba et al., 2016) in transformer architectures. To motivate the use of \(\sqrt{C_{D}}\) as the normalization, consider a special case where the positional embeddings, after the softmax function, produce attention weights that are close to one-hot for each head. Then \(v_{l}^{(h)}\) in (2.2) is equal to some token in \(x_{1:L}\). As a result, each \(v_{l}\) consists of \(H\) tokens and

\[\|\phi(v_{l})\|_{2}=\sqrt{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2} \cdot\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{l}^{(h)}\rangle}=\sqrt{C_{D}}.\]

Thus, \(u_{l}\) is roughly equivalent to the output of the layer normalization \(\phi(v_{l})/\|\phi(v_{l})\|_{2}\) (without trainable parameters). Although our theoretical analysis and simulations focus on this simplified version of layer normalization, our additional experiments in SSB.2 demonstrate that it aligns well with the performance of the actual layer normalization.

The Second Attention Layer.The normalized vector sequence \(U=(u_{1},\ldots,u_{L+1})^{\top}\) and the original sequence \(\widetilde{X}\) are then fed into the second attention layer to generate the final output. In particular, \(u_{L+1}\) is used as the query to compare with the keys \(\{u_{M+1},\ldots,u_{L}\}\), and the resulting attention scores are used to aggregate the values \(x_{(M+1):L}\). This attention layer has a single head and a scalar trainable parameter \(a\). We let \(U_{1:L}=(u_{1},\ldots,u_{L})^{\top}\in\mathbb{R}^{L\times d_{e}}\) and denote by \(\mathtt{Mask}(\cdot)\) the mask that sets every entry of the first \(M\) rows of a matrix to be \(-\infty\). The final output is given by

\[y=\sum_{j=M+1}^{L}\sigma_{j}\big{(}a\cdot u_{L+1}^{\top}\mathtt{Mask}(U_{1:L} ^{\top})\big{)}\cdot x_{j}=\sum_{j=M+1}^{L}\frac{\exp\!\big{(}a\cdot u_{L+1}^{ \top}u_{j}\big{)}\cdot x_{j}}{\sum_{k=M+1}^{L}\exp\!\big{(}a\cdot u_{L+1}^{ \top}u_{k}\big{)}}.\] (2.4)

Note that the softmax function in (2.4) yields a probability distribution over \([L]\) and that \(x_{1:L}\) is a sequence of one-hot vectors. Thus \(y\) in (2.4) is a probability distribution over \(\mathcal{X}\). The mask operator is included here just to simplify our analysis while in the experiments we are not using the mask.

In summary, given the input \(\widetilde{X}\in\mathbb{R}^{(L+1)\times d}\), in the matrix form, our transformer model \(\mathtt{TF}(M,H,d,D)\) consecutively applies the following operations:

**First Attention:** \[V^{(h)}=\sigma(W_{P}^{(h)})\widetilde{X} \in\mathbb{R}^{(L+1)\times d},\;\forall h\in[H];\] (2.5)
**Concatenate:** \[V=[V^{(1)},\ldots,V^{(H)}] \in\mathbb{R}^{(L+1)\times Hd};\] (2.6)
**FFN & Normalize:** \[U=\phi(V)/\sqrt{C_{D}} \in\mathbb{R}^{(L+1)\times d_{e}};\] (2.7)
**Second Attention:** \[y^{\top}=\sigma\big{(}a\cdot u_{L+1}^{\top}\mathtt{Mask}(U_{1:L}^{\top}) \big{)}X \in\mathbb{R}^{1\times d}.\]

The trainable parameters of the above transformer model are denoted by

\[\Theta=\big{\{}a,\{w_{-1}^{(h)},\ldots,w_{-M}^{(h)}\}_{h\in[H]},\{c_{\mathcal{ S}}:\mathcal{S}\in[H]_{\leq D}\}\big{\}}.\]

We remark that the transformer model in (2.6) is known as a disentangled transformer (Friedman et al., 2024), which is a version of the transformer model that is more amenable for theoretical analysis. One thing to be noted is that there is a residual connection that directly copies \(\widetilde{X}\) to the output of the FFN & Normalize block, which gives us \([U,\widetilde{X}]\), and the second attention layer will treat the copied \(\widetilde{X}\) as the value in the attention mechanism. We omit the residual connection in the above paradigm for notation simplicity. As shown in Nichani et al. (2024), any standard transformer model can be expressed as a disentangled transformer by specializing the attention weights to allow feature concatenation.

Our goal is to investigate whether the transformer model \(\mathtt{TF}(M,H,d,D)\) can perform ICL over \(n\)-gram Markov chains and further, whether such capability can be learned from data with common training algorithms like gradient descent.

Theoretical Results

In this section, we present the theoretical results. We first show in SS3.1 and SSC.1 that there exists a transformer in \(\mathbbm{TF}(M,H,d,D)\) that implements a generalized "induction head" mechanism (Olsson et al., 2022) with a learned feature, which serves as a natural algorithm for learning \(n\)-gram Markov chains. Then in SS3.2 we prove that the gradient flow in (3.4) finds such a desired model asymptotically.

### Generalized Induction Head Mechanism for Learning \(n\)-Gram Markov Chains

Recall that we define the mixture of \(n\)-gram Markov chain model \((\mathcal{X},\mathsf{pa},\mathcal{P},\mu_{0})\) in SS2.1, where \(\mathcal{P}\) is a distribution over the Markov transition kernels. For regularity, we assume existence of a unique stationary distribution for any \(\pi\in\operatorname{supp}(\mathcal{P})\), where a rigorous statement is deferred to Assumption 3.5. We also assume the window size \(M>r_{n}\). For any \(n\)-gram Markov chain with transition kernel \(\pi\sim\mathcal{P}\), we let \(\mu^{\pi}\in\Delta(\mathcal{X}^{M+1})\) denote the stationary distribution of the Markov chain over a window of size \(M+1\). Here we use \(\{z_{\ell}\}_{l\geq 1}\) to denote a random sequence of tokens generated by the Markov chain. Then \(\mu^{\pi}\) denotes the joint distribution of a block of \(M+1\) tokens \((z_{l-M},\ldots,z_{l-1},z_{l})\) under the stationary distribution of \(\pi\), where \(l>M\) is an integer.

In the following, we introduce a generalized induction head (GH) estimator for the task of predicting \(x_{L+1}\) given \(x_{1:L}\), which is based on the following simple idea: \(x_{L+1}\)_should be similar to a previous token \(x_{l}\) if their parents are similar_. As the parent set pa is unknown, GIH adopts an information-theoretic criterion to select a subset of previous tokens as a proxy of the parents. Specifically, GIH uses a modified version of \(\chi^{2}\)-mutual information, which is defined as follows.

**Definition 3.1** (Modified \(\chi^{2}\)-Mutual Information).: _We take a length-\((M+1)\) windows \((z_{l-M},\ldots,z_{l-1},z_{l})\) for some \(l>M\) and suppose the sequence is sampled from stationary distribution \(\mu^{\pi}\) with \(\pi\sim\mathcal{P}\). Let \(Z=(z_{l-M},\ldots,z_{l-1})\). For any subset \(\mathcal{S}\subseteq[M]\), we use \(Z_{-\mathcal{S}}\) to denote the subvector of \(Z\) containing entries of the form \(z_{l-s}\), \(\forall s\in\mathcal{S}\). For instance, suppose \(\mathcal{S}=\{2,5\}\), then \(Z_{-\mathcal{S}}=(z_{l-5},z_{l-2})\). The modified \(\chi^{2}\)-mutual information for \(\mathcal{S}\) is defined as_

\[\widetilde{I}_{\chi^{2}}(\mathcal{S})=\mathbb{E}_{\pi\sim\mathcal{P},(z,Z) \sim\mu^{\pi}}\bigg{[}\bigg{(}\sum_{e\in\mathcal{X}}\frac{[\mu^{\pi}(z=e\,| \,Z_{-\mathcal{S}})]^{2}}{\mu^{\pi}(z=e)}-1\bigg{)}\cdot\mu^{\pi}(Z_{- \mathcal{S}})\bigg{]},\] (3.1)

_where \(\mu^{\pi}(z=\cdot\,|\,Z_{-\mathcal{S}})\) is the conditional distribution of \(z\) induced by \(\mu^{\pi}\) given the partial history \(Z_{-\mathcal{S}}\), and \(\mu^{\pi}(Z_{-\mathcal{S}}),\mu^{\pi}(z)\) are the marginal distributions of \(Z_{-\mathcal{S}}\) and \(z\) under \((z,Z)\sim\mu^{\pi}\)._

Intuitively, \(\widetilde{I}_{\chi^{2}}(\mathcal{S})\) is modified from the vanilla \(\chi^{2}\)-mutual information (\(\chi^{2}\)-MI) between two random variables (Polyanskiy and Wu, 2024) and quantifies how much information the partial history \(Z_{-\mathcal{S}}\) contains about \(z\). In particular, we incorporate an additional \(\mu^{\pi}(Z_{-\mathcal{S}})\) term that decreases with the growing size of \(\mathcal{S}\). To see the rationality, we first introduce a GIH estimator based on the modified \(\chi^{2}\)-mutual information.

**Definition 3.2** (Generalized Induction Head).: _A GIH estimator with window size \(M\in\mathbb{N}\), feature size \(D\in\mathbb{N}\) is denoted by \(\mathtt{GIH}(\cdot;M,D)\), which maps \(x_{1:L}\) to a distribution over \(\mathcal{X}\). We let \(\mathcal{S}^{\star}\) be the information-optimal subset (referred to as the "information set" in the sequel2) of \([M]\) with size no more than \(D\) that maximizes the modified \(\chi^{2}\)-mutual information \(\widetilde{I}_{\chi^{2}}(\cdot)\) defined in (3.1). That is, we define the information set \(S^{\star}\) as_

Footnote 2: With a slight abuse of notation, we also call \(X_{l-\mathcal{S}^{\star}}:=(x_{l-s}:s\in\mathcal{S}^{\star})\) the information set of the \(l\)-th token \(x_{l}\).

\[\mathcal{S}^{\star}=\operatorname{argmax}_{\mathcal{S}\in[M]_{\leq 0}} \widetilde{I}_{\chi^{2}}(\mathcal{S}).\] (3.2)

_Then \(\mathtt{GIH}(x_{1:L};M,D)\) outputs_

\[y^{\star}:=\begin{cases}N^{-1}\cdot\sum_{l=M+1}^{L}x_{l}\cdot\mathds{1}(X_{l- \mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{\star}}),\text{ if }N\geq 1,\\ (L-M)^{-1}\cdot\sum_{l=M+1}^{L}x_{l},\quad\text{otherwise}.\end{cases}\] (3.3)

_Here, we define \(X_{l-\mathcal{S}^{\star}}\) as the set \(\{x_{l-s}:s\in\mathcal{S}^{\star}\}\) and \(N=\sum_{l=M+1}^{L}\mathds{1}(X_{l-\mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{ \star}})\)._

Note that \(\mathcal{S}^{\star}\) defined in (3.2) depends on the choices of \(M\) and \(D\) and serves as a proxy of the unknown parent set pa based on \(\widetilde{I}_{\chi^{2}}(\cdot)\) defined in (3.1). In a nutshell, the GIH estimator checkswhether the partial histories of \(X_{l-\mathcal{S}^{\star}}\) and \(X_{L+1-\mathcal{S}^{\star}}\) match and aggregate all the tokens \(x_{l}\) that have a matching partial history to predict \(x_{L+1}\). As a remark, using the modified \(\chi^{2}\)-MI as the information criterion rules out redundancy in the information set \(\mathcal{S}^{\star}\) in the following sense:

\(\bullet\)\(\mathcal{S}^{\star}\) **cannot be a superset of the true parents.** Note that if \(\mathcal{S}\) is a superset of the true parent set, by the Markov property, \(z\) and \(Z_{-S}\) are conditionally independent given the true parents \(Z_{\text{pa}}\). Thus, maximizing the vanilla \(\chi^{2}\)-mutual information yields multiple maximizers, i.e., all the supersets of the true parent set. However, with the modification in (3.1), any superset yields a strictly smaller \(\widetilde{I}_{\chi^{2}}\) compared to the exact parent set, making them suboptimal.

\(\bullet\)**The modified \(\chi^{2}\)-MI selects informative partial history.** Even a true parent may bear relatively little information about the target compared to other parents sometimes. Meanwhile, exact match of a larger set of partial history becomes much harder as it tends to appear less frequently in the context sequence, leading to poor estimation accuracy for the estimator in (3.3). The modified \(\chi^{2}\)-MI reaches a balance by selecting the informative partial history while penalizing the size of the information set.

The term involving \(\mu^{\pi}(z=\cdot\,|\,Z_{-\mathcal{S}})\) can be viewed as the _signal_ part which helps us to find an informative subset \(\mathcal{S}\). The term \(\mu^{\pi}(Z_{-\mathcal{S}})\) can be viewed as _penalty on the model complexity_ which favors smaller subsets. Thus, the modified \(\chi^{2}\)-MI strikes a balance between these two objectives and enables us to find a good proxy \(\mathcal{S}^{\star}\) of pa when \(L\) is finite. Moreover, when \(L\) is sufficiently large, we identify two scenarios in which maximizing \(\widetilde{I}_{\chi^{2}}(\cdot)\) yields the true parent set (see SSC.7 for details). Moreover, the GIH estimator is a generalization of the induction head mechanism (Elhage et al., 2021) to the stochastic setting with multiple parents, where we give the model more flexibility to learn based on a partial history that does not necessarily correspond to the true parent set. As we will show in SSC.1, the GIH mechanism can be implemented by the transformer model.

### Convergence Guarantee of Gradient Flow

In the following, we present the convergence guarantee for gradient flow. To simplify the discussion, we consider the case where \(H=M\), meaning there are enough heads to implement the GIH mechanism by having each head copy a unique parent token from a window of size \(M\). Let us first introduce the paradigm of training by gradient flow.

Training Paradigm.Consider training a transformer \(\mathtt{TF}(M,H,d,D)\) in (2.5) with \(M=H\) to perform ICL on the \(n\)-gram Markov chain model introduced in SS2.1. Specifically, we define \(\mathcal{L}(\Theta)\) as the population cross-entropy loss in (2.1), where the transformer model \(f_{\mathtt{tf}}\) is given by (2.5) with a parameter \(\Theta\). Ideally, when training the parameter \(\Theta\) with gradient flow, the dynamics with respect to the loss \(\mathcal{L}(\Theta)\) is given by:

\[\partial_{t}\Theta(t)=-\nabla\mathcal{L}\big{(}\Theta(t)\big{)}.\] (3.4)

We consider a three-stage training paradigm where, in each stage, only a specific subset of the weights is trained by gradient flow. The three stages are outlined in Table 1. Specifically, in the first stage, we only train the FFN layer via gradient flow while keeping other weights fixed. We then only train the RPE weights in the first attention layer in the second stage. Finally, we only train the weight \(a\) in the second attention layer in the last stage, while fixing the rest of the parameters. This training approach is primarily used for analytical convenience; in practice, the entire model can be trained simultaneously, and similar convergence results are reported in SSB.2. From a theoretical standpoint, we will also justify the three-stage paradigm in the discussion following Theorem 3.6.

Initialization Conditions.Before presenting our main results about how training by gradient flow induces the GIH structure, let us introduce the following assumption on the initialization of the weights. We define the _information gap_ within the \(D\)-degree parent set \([H]_{\leq D}\) as

\[\Delta\widetilde{I}_{\chi^{2}}=\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})- \max_{S\in[H]_{\leq D}\setminus\{\mathcal{S}^{\star}\}}\widetilde{I}_{\chi^{2 }}(\mathcal{S}),\] (3.5)

where we recall that \(\mathcal{S}^{\star}\) defined in (3.2) maximizes the modified \(\chi^{2}\) mutual information.

**Assumption 3.3** (Initialization).: _We assume that the following holds at initialization:_1. _For the first attention layer's RPE weights,_ \(w_{-h}^{(h)}\geq w_{-j}^{(h)}+\Delta w\) _for all_ \(h,j\in[H]\) _with_ \(j\neq h\)_, where_ \(\Delta w>0\) _is a positive scalar satisfying_ \[\Delta w\geq\log(M-1)-\log\Big{[}\Big{(}1+\Delta\widetilde{I}_{\chi^{\lambda} }/(14\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star}))\Big{]}^{\frac{1}{2H}}-1 \Big{)}.\] (3.6)
2. _The scalar parameter_ \(a\) _in the second attention layer satisfies_ \(0<a\leq O(L^{-3/2})\)_._

The first assumption on the RPE is used to induce the correspondence between parents and heads during the training by slightly breaking the symmetry between different attention heads. The second assumption on the scale of \(a\) ensures that the attention probability given by the second attention layer is close to the uniform distribution over \([L]\). These initialization conditions enable us to derive clean descriptions for the dynamics of the first attention layer and the FFN, shedding light on their respective roles in executing ICL.

We now outline our assumptions on the Markov chain used in the data generation process. Recall that \(r_{n}\) is the largest absolute integer in the parent set pa. For any position \(l\), we define the history \(Z=(z_{l-r_{n}},\ldots,z_{l-1})\) as the last state and \(Z^{\prime}=(z_{l-r_{n}+1},\ldots,z_{l})\) as the current state. Since the parent of the new token \(z_{l}\) is already included in \(Z\), \(Z^{\prime}\) is independent of all prior history given \(Z\), forming a Markov chain.

We define \(P_{\pi}\) as the \(d^{r_{n}}\times d^{r_{n}}\) transition matrix for this Markov chain, where states are successive \(r_{n}\)-tokens. Each row of \(P_{\pi}\) is indexed by \(Z^{\prime}\) and each column by \(Z\). The matrix element \(P_{\pi}(Z^{\prime},Z)\) is thus given by

\[P_{\pi}(Z^{\prime},Z)=\pi(z^{\prime}_{l}\mid Z_{\mathbf{pa}(l)})\cdot\mathds{ I}(Z^{\prime}_{l-r_{n}+1:-1}=Z_{l-r_{n}+1:-1}).\]

This means that to transition from \(Z\) to \(Z^{\prime}\), all elements of \(Z^{\prime}\) except for \(z^{\prime}_{-1}\) must match the last \(r_{n}-1\) tokens of \(Z\). The token \(z^{\prime}_{l}\) is then sampled according to the transition kernel \(\pi\) and depends only on the parent \(Z_{\mathbf{pa}(l)}\). The above definition is in fact independent of the position \(l\) as the transition kernel \(\pi\) is the same across all positions. Note that \(P_{\pi}\) is also a stochastic matrix but with zero entries due to the indicator. To proceed, we need the following notion of primitive matrix to state our assumption on \(P_{\pi}\).

**Definition 3.4** (Primitive Matrix).: _A nonnegative and irreducible square matrix \(P\) is called primitive if there exists a positive integer \(k\) such that all entries of \(P^{k}\) are positive._

We defer more details about the above definition to SSC.3. By the celebrated Perron-Frobenius theorem, if a stochastic matrix \(P_{\pi}\) is also primitive, then (i) there exists a unique stationary distribution for the Markov chain; (ii) \(P_{\pi}\) has a unique leading eigenvalue equal to \(1\), and the corresponding eigenvector is the stationary distribution. Next, we state the assumptions on the mixture of Markov chains for data generation.

**Assumption 3.5** (Markov Chain).: _For any \(\pi\in\operatorname{supp}(\mathcal{P})\), we assume that:_

1. _The transition matrix_ \(P_{\pi}\) _is primitive. In particular, we assume that there exists_ \(\lambda<1\) _such that the eigenvalue of_ \(P_{\pi}\) _with the second largest magnitude satisfies_ \(|\lambda_{2}(P_{\pi})|\leq\lambda\)_. Note that_ \(\lambda_{2}(P_{\pi})\) _can be complex-valued._

\begin{table}
\begin{tabular}{c l l} \hline \hline Stage & Weights to Train & Description \\ \hline I & \(\{c_{\mathcal{S}}\}_{\mathcal{S}\in[H]_{\leq D}}\) in the FFN & Ratio \(c_{\mathcal{S}^{\star}}(t)/c_{\mathcal{S}}(t)\) grows exponentially, \\  & layer & learning the low-degree features with \(\mathcal{S}^{\star}\), \\ \hline II & \(\{w^{(h)}\}_{h\in[H]}\) in the RPE of & \(1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)})^{2}\) decays polynomially \\  & the first attention layer, & training each head in \(\mathcal{S}^{\star}\) to be a copier, \\ \hline III & \(a\) in the weight of the second & \(a(t)\) experiences a two-stage growth, \\  & attention layer & learning the softmax aggregator for GIH, \\ \hline \hline \end{tabular}
\end{table}
Table 1: Three-stage training paradigm for gradient flow. Here, the “Weights to Train” column indicates the weights updated in each stage, and the “Description” column summarizes the corresponding results from Theorem3.6.

_._
2. _There exists_ \(\gamma>0\) _such that the transition kernel satisfies_ \(\pi(x\mid X_{\mathsf{pa}})\geq\gamma\) _for any_ \((x,X_{\mathsf{pa}})\)_._

In fact, the second condition \(\pi(\cdot\mid X_{\mathsf{pa}})>\gamma\) already ensures that \(P_{\pi}\) must be primitive, as is required by the first condition. See Corollary F.14 for details. On the high level, the first assumption guarantees a unique stationary distribution as well as a fast mixing rate of the Markov chain by ensuring a spectral gap for \(P_{\pi}\). The second assumption implies a lower bound on the probability for any set \(\mathcal{S}\subseteq[M]\) under the stationary distribution, i.e., \(\mu^{\pi}(X_{l-\mathcal{S}})\geq\gamma^{|\mathcal{S}|}\) for any \(l>M\). See Corollary F.15 for details.

Now we are ready to present our main theoretical result on training transformers by gradient flow.

**Theorem 3.6** (Convergence of Gradient Flow).: _Suppose Assumption 3.3 and Assumption 3.5 hold. Consider \(H\geq M\). We set \(\varepsilon=L^{-1/2}\) for the cross-entropy loss and assume \(L\) is sufficiently large. Then the following holds for the three-stage training of gradient flow:_

**Stage I: Parent Selection by FFN.**: _Let_ \(C_{D}(t)=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}(t)^{2}\) _and_ \(p_{\mathcal{S}^{\star}}(t)=c_{\mathcal{S}^{\star}}^{2}(t)/C_{D}(t)\)_. Then in the first stage with duration_ \(t_{1}\asymp C_{D}(0)\log L/(a(0)\Delta\tilde{T}_{\chi^{2}})\)_, the ratio_ \(c_{\mathcal{S}^{\star}}/c_{\mathcal{S}}\) _grows exponentially fast for any_ \(\mathcal{S}\neq\mathcal{S}^{\star}\)_, and_ \(\mathcal{S}^{\star}\) _dominates exponentially fast in the sense that,_

\[1-p_{\mathcal{S}^{\star}}(t)\leq(1-p_{\mathcal{S}^{\star}}(0))\cdot\exp\bigl{(} -(2C_{D})^{-1}\cdot a(0)\cdot p_{\mathcal{S}^{\star}}(0)\cdot\Delta\tilde{T} _{\chi^{2}}\cdot t\bigr{)},\quad\forall t\in[0,t_{1}).\]
**Stage II: Concentration of The First Attention.**: _Define_ \(\sigma^{(h)}(t)=\sigma(w^{(h)}(t))\in\mathbb{R}^{M}\)_, and let_ \(\sigma_{\min}(t):=\min_{h\in\mathcal{S}^{\star}}\sigma^{(h)}_{-h}(t)\)_. Then in the second stage with duration_ \(t_{2}-t_{1}\asymp L/(a(0)\Delta\tilde{T}_{\chi^{2}})\)_, the first layer's attention heads have attention probabilities concentrated on the optimal information set_ \(\mathcal{S}^{\star}\) _in the sense that for any_ \(t\in[t_{1},t_{1}+t_{2})\)_,_

\[1-\prod_{h\in\mathcal{S}^{\star}}(\sigma^{(h)}_{-h}(t))^{2}\leq\frac{2| \mathcal{S}^{\star}|\cdot(M-1)}{a(0)\cdot\Delta\tilde{T}_{\chi^{2}}\cdot \sigma_{\min}(0)\cdot(t-t_{1})/2+\exp(\Delta w)+(M-1)}\wedge 1.\]
**Stage III: Growth of The Second Attention.**: _For some constants_ \(c_{1},c_{2}\) _depending on_ \((\mathcal{P},\mathcal{S}^{\star})\) _with_ \(0<c_{1}<c_{2}\)_, there exists a small constant_ \(\delta>0\) _such that the growth of_ \(a(t)\) _exhibits the following two sub-stages: (i) When_ \(a(t)\asymp\log(c_{1}/\delta)\)_, it holds that_ \(\partial a(t)\asymp e^{a(t)}\)_; (ii) After_ \(a(t)\) _has grown such that_ \(a(t)\asymp\log(c_{2}/\delta)\)_, then_ \(\partial_{t}a(t)\asymp 1/a(t)\) _until it reaches the value_ \(\log L/8\)_._

See SSD for a proof sketch and SSE for the detailed proof. We require that \(L\) is sufficiently large, and the specific conditions for \(L\) are deferred to SSE.1.

**Interpretation of Training Dynamics.** We empirically verify Theorem 3.6 by conducting a simulation experiment. In particular, we train a transformer with \(H=M=3\) and \(D=2\) based on Markov chain data with \(d=2\), \(L=100\) and \(\mathsf{pa}=\{-1,-2\}\). We sample the transition kernel from a Dirichlet prior such that \(\mathcal{S}^{\star}=\{1,2\}\) also matches the parent set. For more details on this simulation, see SSD. The results are shown in Figure 3 and align perfectly with Theorem 3.6. From Theorem 3.6, we can interpret the three stages of training dynamics as follows.

* In the first stage, the training of FFN parameters learns a _selector_ that selects an informative set \(\mathcal{S}^{\star}\) by realizing the corresponding feature embedding through the polynomial kernel. That is, when \(t\) is sufficiently large, we have \(p_{\mathcal{S}^{\star}}(t)\approx 1\) and \(p_{\mathcal{S}}(t)\approx 0\) for all \(\mathcal{S}\neq\mathcal{S}^{\star}\). In this case, for any input vectors \(v,v^{\prime}\in\mathbb{R}^{Hd}\), the inner product in (2.3) reduces to \[\langle\phi(v),\phi(v^{\prime})\rangle\approx c_{\mathcal{S}^{\star}}^{2}\cdot \prod_{h\in\mathcal{S}^{\star}}\langle v^{(h)},v^{\prime(h)}\rangle.\] That is, FFN only selects the blocks in \(\mathcal{S}^{\star}\) as the feature. We observe this phenomenon in the experiment, where we set \(\mathcal{S}^{\star}=\{1,2\}\). As shown in Figure 3-(a), it is clear that \(c_{\mathcal{S}^{\star}}\) immediately dominates the rest of \(c_{\mathcal{S}}\)'s within only a few gradient epochs.
* In the second stage, we update the parameters of the RPE. This stage turns the first attention layer into a _copier_ by establishing the correspondence between the attention heads and the parents in the selected \(\mathcal{S}^{\star}\). That is, each attention head copies a particular parent in \(\mathcal{S}^{\star}\). Specifically, when \(t\) is sufficiently large, for any \(h\in\mathcal{S}^{\star}\), \(\sigma^{(h)}(t)=\sigma(w^{(h)}(t))\approx 1\). Recalling the construction of RPE, this implies that \(v^{(h)}_{l}\) in (2.2) becomes \(x_{l-h}\) for all \(h\in\mathcal{S}^{\star}\). As shown in Figure 3-(b), in the experiment, the first two heads initialized towards the first two parents will deterministically copy parents \(-1\) and \(-2\) eventually. The third head stays close to its initial value. This head has a negligible effect on the output because \(3\notin\mathcal{S}^{\star}\) and \(p_{\mathcal{S}^{\star}}\approx 1\).
* After the first two stages are completed, we know that the features constructed approximately satisfy (C.1) up to a proportionality factor. Then, in the final training stage, the scalar weight \(a\) in the second attention layer keeps increasing. Thus, this stage learns an exponential kernel _classifier_ as specified in (C.2). When \(a(t)\) is sufficiently large, the learned transformer is close to a classifier that uses covariate-label pairs of the form \((X_{l-\mathcal{S}^{\star}},x_{l})\) to predict \(x_{L+1}\). In particular, when \(a(t)\) goes to infinity, the transformer exactly becomes the GIH mechanism given in Definition 3.2. Moreover, we theoretically prove that the increasing trajectory of \(a(t)\) has two stages, where \(\mathrm{d}a(t)/\mathrm{d}t\) is initially large and gradually decays, this is also clearly observed in the experiment. See Figure 3-s(c) for details.

In summary, we theoretically show that the limiting model obtained by three-stage training approximately implements the GIH mechanism. We will prove that the difference between these two estimators is at most \(O(L^{-1/8})\). We defer the formal statement and proof to SSE.5. Moreover, as an answer to the Question (iii) raised in SS1, the different components of the transformer architecture are all critical for achieving this: FFN with normalization realizes the _selector_, the multi-head design of attention supports the _copier_, and finally, the softmax operation facilitates the exponential kernel _classifier_. These components work organically as a whole system, yielding the trained transformer's capability of ICL of \(n\)-gram Markov chains.

Another takeaway from Theorem 3.6 is a strict separation in the growth rate of these three stages. In particular, the convergence rates of the corresponding components of the transformer model in these three stages range from exponentially fast (Stage I), polynomially fast (Stage II), to logarithmically slow (Stage III). With such two exponential separations of convergence rates, we expect that these three stages naturally arise when we simultaneously train the whole model via gradient descent/flow. We empirically verify this argument and the details are deferred to SSB.2.

In SSC.7, we provide more intuitive interpretation of the modified \(\chi^{2}\)-mutual information, which demonstrates a balance of model complexity and information richness.

## 4 Conclusion and Future Work

In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning \(n\)-gram Markov chains in an in-context way. Our work opens new directions for developing a rigorous understanding of the transformer models, which includes understanding the induction head mechanism with standard FFN layer and investigating the training dynamics beyond a single loop of this induction head mechanism. We defer readers to SSC.8 for more discussions.

Figure 3: An illustration of the transformer parameters during the three-stage training. We train a transformer in \(\mathtt{TF}(M=3,H=3,d=3,D=2)\) with \(L=100\), \(\mathtt{pa}=\{-1,-2\}\). See §B and Figure 4 for more details of the simulation.

Acknowledgement

We acknowledge Shaobo Wang for his help with the experiments. We also thank Jason D. Lee, Alex Damian, and Eshaan Nichani for their helpful discussions. Zhuoran Yang acknowledges the support of NSF under the award DMS-2413243.

## References

* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.
* Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H. and Sra, S. (2023). Transformers learn to implement preconditioned gradient descent for in-context learning. _arXiv preprint arXiv:2306.00297_.
* Ahuja et al. (2023) Ahuja, K., Panwar, M. and Goyal, N. (2023). In-context learning through the bayesian prism. _arXiv preprint arXiv:2306.04891_.
* Akyurek et al. (2023) Akyurek, E., Schuurmans, D., Andreas, J., Ma, T. and Zhou, D. (2023). What learning algorithm is in-context learning? investigations with linear models. In _The Eleventh International Conference on Learning Representations_.
* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M. et al. (2022). Flamingo: a visual language model for few-shot learning. _Advances in neural information processing systems_, **35** 23716-23736.
* Anthropic (2023) Anthropic (2023). Model card and evaluations for claude models.
* Ba et al. (2016) Ba, J. L., Kiros, J. R. and Hinton, G. E. (2016). Layer normalization. _arXiv preprint arXiv:1607.06450_.
* Bai et al. (2023) Bai, Y., Chen, F., Wang, H., Xiong, C. and Mei, S. (2023). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _arXiv preprint arXiv:2306.04637_.
* Bietti et al. (2024) Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H. and Bottou, L. (2024). Birth of a transformer: A memory viewpoint. _Advances in Neural Information Processing Systems_, **36**.
* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020). Language models are few-shot learners. _Advances in neural information processing systems_, **33** 1877-1901.
* Cabannes et al. (2024) Cabannes, V., Arnal, C., Bouaziz, W., Yang, A., Charton, F. and Kempe, J. (2024). Iteration head: A mechanistic study of chain-of-thought. _arXiv preprint arXiv:2406.02128_.
* Chen and Li (2024) Chen, S. and Li, Y. (2024). Provably learning a multi-head attention layer. _arXiv preprint arXiv:2402.04084_.
* Chen et al. (2024) Chen, S., Sheen, H., Wang, T. and Yang, Z. (2024). Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. _arXiv preprint arXiv:2402.19442_.
* Chen et al. (2022) Chen, S., Yang, D., Li, J., Wang, S., Yang, Z. and Wang, Z. (2022). Adaptive model design for markov decision process. In _International Conference on Machine Learning_. PMLR.
* Chen and Zou (2024) Chen, X. and Zou, D. (2024). What can transformer learn with varying depth? case studies on sequence learning tasks. _arXiv preprint arXiv:2404.01601_.
* Cheng et al. (2023) Cheng, X., Chen, Y. and Sra, S. (2023). Transformers implement functional gradient descent to learn non-linear functions in context. _arXiv preprint arXiv:2312.06528_.
* Collins et al. (2024) Collins, L., Parulekar, A., Mokhtari, A., Sanghavi, S. and Shakkottai, S. (2024). In-context learning with transformers: Softmax attention adapts to function lipschitzness. _arXiv preprint arXiv:2402.11639_.
* Deora et al. (2023) Deora, P., Ghaderi, R., Taheri, H. and Thrampoulidis, C. (2023). On the optimization and generalization of multi-head attention. _arXiv preprint arXiv:2310.12680_.
* Edelman et al. (2024) Edelman, B. L., Edelman, E., Goel, S., Malach, E. and Tsilivis, N. (2024). The evolution of statistical induction heads: In-context learning markov chains. _arXiv preprint arXiv:2402.11004_.
* Edelman et al. (2022) Edelman, B. L., Goel, S., Kakade, S. and Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_. PMLR.
* Edelman et al. (2023)Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T. et al. (2021). A mathematical framework for transformer circuits. _Transformer Circuits Thread_, **1** 1.
* Friedman et al. (2024) Friedman, D., Wettig, A. and Chen, D. (2024). Learning transformer programs. _Advances in Neural Information Processing Systems_, **36**.
* Fu et al. (2023) Fu, D., Chen, T.-Q., Jia, R. and Sharan, V. (2023). Transformers learn higher-order optimization methods for in-context learning: A study with linear models. _arXiv preprint arXiv:2310.17086_.
* Giannou et al. (2023) Giannou, A., Rajput, S., Sohn, J.-Y., Lee, K., Lee, J. D. and Papailiopoulos, D. (2023). Looped transformers as programmable computers. In _Proceedings of the 40th International Conference on Machine Learning_ (A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato and J. Scarlett, eds.), vol. 202 of _Proceedings of Machine Learning Research_. PMLR.
* Giannou et al. (2024) Giannou, A., Yang, L., Wang, T., Papailiopoulos, D. and Lee, J. D. (2024). How well can transformers emulate in-context newton's method? _arXiv preprint arXiv:2403.03183_.
* Guo et al. (2023) Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S. and Bai, Y. (2023). How do transformers learn in-context beyond simple functions? a case study on learning with representations. _arXiv preprint arXiv:2310.10616_.
* He et al. (2024) He, J., Chen, S., Zhang, F. and Yang, Z. (2024). From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems. _arXiv preprint arXiv:2405.19883_.
* He et al. (2020) He, P., Liu, X., Gao, J. and Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled attention. _arXiv preprint arXiv:2006.03654_.
* Honovich et al. (2022) Honovich, O., Shaham, U., Bowman, S. R. and Levy, O. (2022). Instruction induction: From few examples to natural language task descriptions. _arXiv preprint arXiv:2205.10782_.
* Huang et al. (2023) Huang, Y., Cheng, Y. and Liang, Y. (2023). In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_.
* Jelassi et al. (2022) Jelassi, S., Sander, M. and Li, Y. (2022). Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, **35** 37822-37836.
* Jeon et al. (2024) Jeon, H. J., Lee, J. D., Lei, Q. and Van Roy, B. (2024). An information-theoretic analysis of in-context learning. _arXiv preprint arXiv:2401.15530_.
* Kim and Suzuki (2024) Kim, J. and Suzuki, T. (2024). Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. _arXiv preprint arXiv:2402.01258_.
* Li et al. (2024) Li, Y., Huang, Y., Ildiz, M. E., Rawat, A. S. and Oymak, S. (2024). Mechanics of next token prediction with self-attention. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Li et al. (2023) Li, Y., Li, Y.-F. and Risteski, A. (2023). How do transformers learn topic structure: Towards a mechanistic understanding. _arXiv preprint arXiv:2303.04245_.
* Lin et al. (2023) Lin, L., Bai, Y. and Mei, S. (2023). Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. _arXiv preprint arXiv:2310.08566_.
* Liu et al. (2022) Liu, B., Ash, J., Goel, S., Krishnamurthy, A. and Zhang, C. (2022). Transformers learn shortcuts to automata. _ArXiv_, **abs/2210.10749**.
* Mahankali et al. (2023) Mahankali, A., Hashimoto, T. B. and Ma, T. (2023). One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_.
* Makkuva et al. (2024a) Makkuva, A. V., Bondaschi, M., Ekbote, C., Girish, A., Nagle, A., Kim, H. and Gastpar, M. (2024a). Local to global: Learning dynamics and effect of initialization for transformers. _arXiv preprint arXiv:2406.03072_.
* Makkuva et al. (2024b) Makkuva, A. V., Bondaschi, M., Girish, A., Nagle, A., Jaggi, M., Kim, H. and Gastpar, M. (2024b). Attention with markov: A framework for principled analysis of transformers via markov chains. _arXiv preprint arXiv:2402.04161_.
* Makkuva et al. (2024c)Meyer, C. D. (2023). _Matrix analysis and applied linear algebra_. SIAM.
* Muller et al. (2021) Muller, S., Hollmann, N., Arango, S. P., Grabocka, J. and Hutter, F. (2021). Transformers can do bayesian inference. _ArXiv_, **abs/2112.10510**.
* Nichani et al. (2024) Nichani, E., Damian, A. and Lee, J. D. (2024). How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_.
* Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A. et al. (2022). In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_.
* Polyanskiy and Wu (2024) Polyanskiy, Y. and Wu, Y. (2024). _Information Theory: From Coding to Learning_. Cambridge University Press.
* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019). Language models are unsupervised multitask learners. _OpenAI blog_, **1** 9.
* Rajaraman et al. (2024a) Rajaraman, N., Bondaschi, M., Ramchandran, K., Gastpar, M. and Makkuva, A. V. (2024a). Transformers on markov data: Constant depth suffices. _arXiv preprint arXiv:2407.17686_.
* Rajaraman et al. (2024b) Rajaraman, N., Jiao, J. and Ramchandran, K. (2024b). Toward a theory of tokenization in lms. _arXiv preprint arXiv:2404.08335_.
* Sanford et al. (2023) Sanford, C., Hsu, D. and Telgarsky, M. (2023). Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_.
* Sheen et al. (2024) Sheen, H., Chen, S., Wang, T. and Zhou, H. H. (2024). Implicit regularization of gradient flow on one-layer softmax attention. _arXiv preprint arXiv:2403.08699_.
* Sinii et al. (2023) Sinii, V., Nikulin, A., Kurenkov, V., Zisman, I. and Kolesnikov, S. (2023). In-context reinforcement learning for variable action spaces. _arXiv preprint arXiv:2312.13327_.
* Song and Zhong (2023) Song, J. and Zhong, Y. (2023). Uncovering hidden geometry in transformers via disentangling position and context. _arXiv preprint arXiv:2310.04861_.
* Tarzanagh et al. (2023a) Tarzanagh, D. A., Li, Y., Thrampoulidis, C. and Oymak, S. (2023a). Transformers as support vector machines. _ArXiv_, **abs/2308.16898**.
* Tarzanagh et al. (2023b) Tarzanagh, D. A., Li, Y., Zhang, X. and Oymak, S. (2023b). Max-margin token selection in attention mechanism. _arXiv preprint arXiv:2306.13596_.
* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A. et al. (2023). Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_.
* Thrampoulidis (2024) Thrampoulidis, C. (2024). Implicit bias of next-token prediction. _arXiv preprint arXiv:2402.18551_.
* Tian et al. (2023a) Tian, Y., Wang, Y., Chen, B. and Du, S. (2023a). Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. _arXiv preprint arXiv:2305.16380_.
* Tian et al. (2023b) Tian, Y., Wang, Y., Zhang, Z., Chen, B. and Du, S. (2023b). Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. _arXiv preprint arXiv:2310.00535_.
* Vasudeva et al. (2024) Vasudeva, B., Deora, P. and Thrampoulidis, C. (2024). Implicit bias and fast convergence rates for self-attention. _arXiv preprint arXiv:2402.05738_.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. and Polosukhin, I. (2017). Attention is all you need. _Advances in neural information processing systems_, **30**.
* Von Oswald et al. (2023) Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A. and Vladymyrov, M. (2023). Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_. PMLR.
* Wang et al. (2022) Wang, K., Variengien, A., Conmy, A., Shlegeris, B. and Steinhardt, J. (2022). Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. _arXiv preprint arXiv:2211.00593_.
* Wang et al. (2023)Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M. and Le, Q. V. (2021). Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_.
* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D. et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, **35** 24824-24837.
* Wu et al. (2023) Wu, J., Zou, D., Chen, Z., Braverman, V., Gu, Q. and Bartlett, P. L. (2023). How many pretraining tasks are needed for in-context learning of linear regression? _arXiv preprint arXiv:2310.08391_.
* Xie et al. (2021) Xie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2021). An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_.
* Zhang et al. (2023a) Zhang, R., Frei, S. and Bartlett, P. L. (2023a). Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_.
* Zhang et al. (2022) Zhang, Y., Liu, B., Cai, Q., Wang, L. and Wang, Z. (2022). An analysis of attention via the lens of exchangeability and latent variable models. _arXiv preprint arXiv:2212.14852_.
* Zhang et al. (2023b) Zhang, Y., Zhang, F., Yang, Z. and Wang, Z. (2023b). What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv:2305.19420_.
* Zhou et al. (2022) Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. et al. (2022). Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_.

###### Contents

* 1 Introduction
* 2 Problem Setup: In-Context Learning of Markov Chains
	* 2.1 In-Context Learning and \(n\)-Gram Markov Chains
	* 2.2 A Two-Layer Transformer Model
* 3 Theoretical Results
	* 3.1 Generalized Induction Head Mechanism for Learning \(n\)-Gram Markov Chains
	* 3.2 Convergence Guarantee of Gradient Flow
* 4 Conclusion and Future Work
* 5 Acknowledgement
* A Related Works
* B Experiments
* B.1 Training with Stage Splitting
* B.2 Training without Stage Splitting
* B.3 Prior and Length Generalization
* C Additional Background and Discussions
* C.1 How Does Transformer Implement the GIH Mechanism?
* C.2 Feed-Forward Network for Polynomial Kernel
* C.3 Perron-Frobenius Theorem
* C.4 Sequential CE Loss
* C.5 Standard \(\chi^{2}\)-Divergence and Mutual Information
* C.6 More Details on the Generalized Induction Head Mechanism
* C.7 Further Discussions on the GIH Mechanism
* C.8 Conclusion and Future Directions
* D Proof Sketch
* D.1 Simplification of the Transformer Model at Initialization
* D.2 Analysis for Training the FFN and the First Attention Layer
* D.2.1 Training the FFN: Identification of the Information Set \(\mathcal{S}^{*}\)
* D.2.2 Training the First Attention Layer: Convergence of \(\sigma(w^{(h)})\) to One-Hot Vector
* D.3 Analysis for the Training of the Second Attention Layer
* E Analysis of the Training Dyanamics
* E.1 Conditions on the Sequence Length
* E.2 Analysis for Stage I
* E.2.1 Additional Proofs for the Stage I

* Analysis for Stage II
* E.3.1 Additional Proofs for Stage II
* E.4 Analysis for Stage III
* E.4.1 Additional Proofs for Stage III
* E.5 Lemma on GIH Approximation Error
* Auxiliary Lemmas and Their Proofs
* F.1 Useful Inequalities
* F.2 Approximation Errors for Dynamics Analysis
* F.3 Lemmas on Concentration of Markov Chain

### Organization of The Appendix

The appendices are organized as follows:

* In SSA, we present an in-depth discussion on the related works.
* In SSB, we discuss the experimental details.
* In SSC, we discuss the implementation of GIH mechanism, provide explicit expressions for the FFN realizing a low-degree polynomial kernel, and review basics related to concepts mentioned in the main text.
* In SSD, we provide a high-level overview of the proof of our main results.
* In SSE, we present the proof for Theorem 3.6.
* In SFF, we collect auxiliary results used in the proof of Theorem 3.6.

## Appendix A Related Works

In Context Learning (ICL).Commercial Large Language Models (LLMs) such as ChatGPT (Brown et al., 2020), GPT-4 (Achiam et al., 2023), and Gemini (Team et al., 2023) typically operate in an autoregressive manner. These models exhibit remarkable ICL capabilities, without requiring further training. Previous research explores various aspects of the in-context learning (ICL) ability of these models. This includes their performance in zero-shot and few-shot learning scenarios (Honovich et al., 2022; Wei et al., 2021), the use of the chain of thought method to enhance reasoning (Wei et al., 2022; Zhou et al., 2022), and learning with multi-modalities (Alayrac et al., 2022). Moreover, recent research highlights the properties and advantages of using transformers beyond the traditional ICL setting, thereby broadening our understanding of their capabilities and applications (Edelman et al., 2022; Li et al., 2023; Jelassi et al., 2022; Sanford et al., 2023; Giannou et al., 2023; Liu et al., 2022; Tarzanagh et al., 2023; Tian et al., 2023; Song and Zhong, 2023; Deora et al., 2023; Chen and Li, 2024; Rajaraman et al., 2024).

There is a large and growing body of literature on understanding how transformer architecture enables ICL. One strand of research proposes to understand ICL by casting it as a version of Bayesian inference expressed by the transformer architecture. See, e.g., Xie et al. (2021); Muller et al. (2021); Zhang et al. (2022, 2023); Ahuja et al. (2023); Jeon et al. (2024); He et al. (2024) and the references therein. Another line of work investigates how transformers internally emulate specific algorithms to solve ICL tasks, where Akyurek et al. (2023); Von Oswald et al. (2023); Fu et al. (2023); Ahn et al. (2023); Mahankali et al. (2023); Giannou et al. (2024); Wu et al. (2023) focus on learning with linear regression tasks and Bai et al. (2023); Cheng et al. (2023); Collins et al. (2024); Guo et al. (2023) investigate transformers' capabilities in learning with nonlinear functions. However, all of these works above focus on regression tasks where token (or token pairs) in the prompt sequences are i.i.d. or uncorrelated, which may not capture the more sophisticated data structures in real-world applications.

In addition, to study ICL with correlated data, there is also substantial interest in understanding how ICL operates over data drawn from Markov chains, providing insight into how transformer architectures contribute to ICL in these settings (Edelman et al., 2024; Makkuva et al., 2024; Chen and Zou, 2024). Furthermore, Lin et al. (2023); Sinii et al. (2023) show how transformers can solve reinforcement learning problems in an in-context fashion.

While many of the aforementioned works focus on the expressivity of the transformer model on different ICL tasks and the statistical properties of the learned models, understanding training dynamics from an optimization perspective is also crucial for comprehending ICL by transformers. The training dynamics for one-layer attention models have been investigated under different data models for both regression and classification tasks (Zhang et al., 2023; Huang et al., 2023; Tarzanagh et al., 2023; Kim and Suzuki, 2024; Chen et al., 2024; Vasudeva et al., 2024; Li et al., 2024; Thrampoulidis, 2024; Shen et al., 2024). These studies offer a thorough characterization of the training process, yet they have limitations -- they are not directly applicable to data drawn from Markov processes and are confined to single-layer attention. Our work belongs to this line of research and we adopt a two-attention-layer transformer architecture, which is more complicated than the transformer studied in these works.

Induction Head.Elhage et al. (2021) introduce the concept of "induction heads" as the mechanism underlying the ICL capabilities of transformers. Since then, there has been a surge of interest in understanding the induction head mechanism and its role in ICL. At a high level, the induction head mechanism works by matching the history of the current token with those seen previously in the sequence and then predicting the next token based on the matched historical sub-sequences. Olsson et al. (2022) provide empirical evidence highlighting that induction heads are crucial in facilitating the ICL capabilities of transformers. Bietti et al. (2024); Edelman et al. (2024) conduct a further empirical investigation into the development of induction heads specifically tailored for the ICL of bi-gram data models. Rajaraman et al. (2024) provide explicit constructions of single-head transformers with constant depths that can learn \(n\)-gram data. Also, a wider range of functionalities exhibited by induction heads that interact with various other mechanisms have been observed by Wang et al. (2022).

From a theoretical perspective, Nichani et al. (2024) study the ICL of _first-order_ Markov chains using a two-layer transformer and demonstrate the formation of the induction head mechanism. Makkuva et al. (2024) also prove that training a single layer attention with a feed-forward layer on _first-order_ Markov data (with \(\{0,1\}\) vocabulary) can converge to either to global or local minima depending on the initialization. However, the _first-order_ assumption seems to be quite restrictive, especially when modeling the natural language, where the tokens can depend on multiple previous tokens. Most related to our work is Nichani et al. (2024), where they analyzed how training by gradient descent enables a two-layer transformer to learn the latent causal graph underlying the ICL data. However, the analysis in Nichani et al. (2024) applies to Markov chains where each token has at most one parent, and it remains unclear how to extend the analysis to more general \(n\)-gram Markov chains.

In this work, we show that a generalized version of the induction head mechanism can emerge when training a two-layer transformer on \(n\)-gram Markov chains. Moreover, our transformer models are more sophisticated, incorporating features like relative positional embedding, multi-head attention, an FNN layer, and normalization. Notably, we provide an in-depth dynamics analysis of the corresponding FFN layer and two-layer multi-head attention.

## Appendix B Experiments

In this section, we first detail the setup for the experiment in Figure 3, and then provide additional results for training a model that also incorporates the word embedding matrices \(W_{Q}\), \(W_{K}\), \(W_{V}\) and the output embedding matrix \(W_{O}\) in the first attention layer. Let us first detail the data setup that is used for all the experiments in this work.

Data generation.The dataset for the ICL task is generated as \(n\)-gram Markov chains as described in SS2.1. We take \(\texttt{pa}=\{-1,-2\}\) as the parent set. Thus, the number of parents is \(n=2\) and the token embedding dimension is \(d=3\). Note that for each sequence, the transition matrix \(\pi(x\,|\,x_{\texttt{pa}})\) is of shape \(d\times d^{n}\). We assign a prior distribution \(\mathcal{P}\) for the transition matrix, which is defined such that each column of the transition matrix of kernel \(\pi\) is independently drawn from a symmetric Dirichlet distribution with parameter \(\alpha=0.01\), i.e., \(\pi(\cdot|x_{\texttt{pa}})\sim\text{Dir}(\alpha\cdot\mathbf{1}_{d})\). Note that each chain has different transition kernel \(\pi\) but follows the same prior distribution \(\mathcal{P}\). We randomly sample 10,000 Markov chains with \(L=100\) from the prior distribution \(\mathcal{P}\); 9,000 are used for training and 1,000 for validation.

### Training with Stage Splitting

we present the simulation results with model \(\mathtt{TF}(M,H,d,D)\) in (2.5) and training in the three-stage manner. We configure the model with window size \(M=3\), number of heads \(H=3\), vocabulary size \(d=3\) and maximal FFN degree \(D=2\).

Model initialization.The RPE weight matrix \(W_{P}^{(h)}\) is initialized such that the \((-i)\)-th diagonal of \(W_{P}^{(h)}\) has value \(w_{-i}^{(h)}\) for \(i=1,2,\ldots,M\), while all other entries are initialized to \(-\infty\). See Figure 2 for an interpretation. We initialize \(w_{-h}^{(h)}=3\) and set the remaining entries within the size-\(M\) window to \(0.01\) to ensure symmetrization-breaking and some initial correspondence between heads and parents. For the FFN layer that learns the polynomial features, all \(c_{\mathcal{S}}\) for \(\mathcal{S}\in[H]_{\leq D}\) are initialized to \(0.01\). The initial value of \(a\) in the second attention layer is set to \(0.01\).

Training settings.The models are trained using gradient descent with respect to the cross-entropy loss and a constant learning rate that is set to one for all stages. We train the model in Stage I (update parameters \(\{c_{\mathcal{S}}\}\) only) for 2000 epochs, in Stage II (update parameters \(\{w^{(h)}\}\) only) for 50,000 epochs, and in Stage III (update parameter \(a\) only) for 5000 epochs, respectively. All experiments are conducted using a single Nvidia A100 GPU. The results are shown in Figure 4, which matches our theoretical results.

Figure 4: An illustration of the transformer parameters during the three-stage training. This is the same figure as Figure 3. We train a transformer in \(\mathtt{TF}(M=3,H=3,d=3,D=2)\) with \(L=100\), \(\mathtt{pa}=\{-1,-2\}\). In (a) we show the evolution of \(\{p_{\mathcal{S}}\}_{\mathcal{S}\in[H]_{\leq D}}\) in the first stage of training where \(p_{\mathcal{S}}=c_{\mathcal{S}}^{2}/\sum_{\mathcal{S}^{\prime}\in[H]_{\leq D} }c_{\mathcal{S}^{\prime}}^{2}\). We use a binary coding in \(\{0,1\}^{3}\) to indicate each subset \(\mathcal{S}\). Recall that “110” represents \(=\{1,2\}\), which is exactly \(\mathcal{S}^{\star}\). This figure shows that \(p_{\mathcal{S}^{\star}}\) gradually increases to one while the any other \(p_{\mathcal{S}}\) decays to zero. In (b) we plot the RPE weights of the first attention layer before and after the second stage of training. Here the \(h\)-th column corresponds to the RPE weight vector of head \(h\). This figure shows that \(w_{-1}^{(1)}\) and \(w_{-2}^{(2)}\) increase to a large number after training, while \(w_{-3}^{(3)}\) stays close to its initial value. Thus, we have \(\sigma(w^{(1)})\approx\sigma(w^{(2)})\approx 1\). That is, the first two heads are trained to attend to parents \(-1\) and \(-2\), respectively. In (c) we plot the evolution of \(a\) in the last stage of training. This figure clearly exhibits a two-step growth pattern and \(a\) keeps increasing throughout this stage. In summary, the results of the simulation experiments coincide with the theoretical results.

### Training without Stage Splitting

Previously in SSB.1, we show the simulation results on the simplified model (2.5). Now we present the results of additional experiments based on the full model defined as follows.

**First Attention:**

\[\widehat{V}^{(h)}=\sigma\big{(}\widehat{X}W_{Q}^{(h)}{W_{K}^{(h)}}^{ \top}\widehat{X}^{\top}+W_{P}^{(h)}\big{)}\widehat{X}{W_{V}^{(h)}}^{\top} \in\mathbb{R}^{(L+1)\times d};\]
**Concatenate & Normalize:**

\[V=\text{LN}\big{(}[\widehat{V}^{(1)},\dots,\widehat{V}^{(H)}, \widehat{X}]\big{)} \in\mathbb{R}^{(L+1)\times(H+1)d};\]
**FFN & Normalize:**

\[\widehat{U}=\phi(V)/\sqrt{C_{D}} \in\mathbb{R}^{(L+1)\times d_{c}};\]
**Concatenate**

\[\widehat{X}^{\prime}=[\widetilde{U},V] \in\mathbb{R}^{(L+1)\times((H+1)d+d_{c})};\]
**Second Attention:**

\[Y=\sigma\big{(}a\cdot(\widetilde{x}^{\prime}_{L+1})^{\top}( \widehat{X}^{\prime}_{1:L})^{\top}\big{)}X \in\mathbb{R}^{(L+1)\times d}.\]

In head \(h\) of the first attention layer, \(W_{P}^{(h)}\) is the relative positional embedding matrix, and we include \(W_{Q}^{(h)}\in\mathbb{R}^{d\times d}\), \(W_{K}^{(h)}\in\mathbb{R}^{d\times d}\) and \(W_{V}^{(h)}\in\mathbb{R}^{d\times d}\) as the weight matrices for the query, key, and value

Figure 5: An illustration of the evolution of gradient descent dynamics when training a transformer model specified in SSB.2 with word embedding matrices \(\{W_{Q},W_{K},W_{V}\}\). Here the dynamics are not split into three stages and each gradient descent step updates all parameters. We set \(M=H=3\), \(d=3\), and \(D=2\), the number of input token is \(L=100\), and Markov chain has parent set \(\text{pa}=\{-1,-2\}\). In (a) we show the training loss of the model, which shows that the loss decreases and converges to some value. In (b) we show the evolution of \(p_{\mathcal{S}}\) where we use binary coding \(\{0,1\}^{3}\) to indicate each subset \(\mathcal{S}\). Here, \(p_{\mathcal{S}^{*}}\) has code “\(110\)”, which corresponds to the true parent set. This figure shows that initially a wrong \(p_{\mathcal{S}}\) dominates at the early stage of training, which corresponds to \(\mathcal{S}=\{2,3\}\) (code “011”). Then eventually \(p_{\mathcal{S}^{*}}\) increases and becomes dominant. However, \(p_{\mathcal{S}^{*}}\) does not increase to one and is about \(0.6\), and there are two \(p_{\mathcal{S}}\)’s that are about \(0.2\). In (c) we show the RPE weights of the first attention layer before and after training. The entries corresponding to the true parents, \(w_{-1}^{(1)}\) and \(w_{-2}^{(2)}\), significantly increase after training, while \(w_{-3}^{(3)}\) slightly increases from initialization. This figure shows that each attention head focuses on copying a single previous token. In (d) we show the evolution of the weight \(a\) in the second attention layer. We observe a similar “elbow” curve as in Figure 3-(c).

projections, respectively. That is, in the full model, we the attention heads has more weight matrices than the simplified model. Another difference is that we also explicitly include the residual link that copies \(\widetilde{X}\) to the output of the first attention layer. For the FFN layer, \(\phi:\mathbb{R}^{(H+1)d}\to\mathbb{R}^{d_{e}}\) is the same feed-forward network specified in (2.3). Here, we use a standard \(\ell_{2}\)-layer-normalization \(\text{LN}(\cdot)\), defined as

\[\text{LN}([x,y])=\left[\frac{x}{\|x\|_{2}},\frac{y}{\|y\|_{2}} \right].\]

The second attention layer takes \(X\) as the value, which comes from the residual link (i.e., concatenation of \(\widetilde{U}\) and \(V\) while \(\widetilde{X}\) in \(V\) remains the same after \(\ell_{2}\)-normalization). In comparison to the simplified model in (2.5), here we incorporate the query, key and value projections for the first layer as in a standard transformer architecture.

Our training setup is similar to that in SSB.1. We use the same dataset and a similar training settings. All these weight matrices \(W_{Q}^{(h)}\), \(W_{K}^{(h)}\) and \(W_{V}^{(h)}\) are initialized as identity matrices scaled by 0.001. We initialized the RPE vector \(w^{(h)}\) as \(w_{-h}^{(h)}=1\) for \(h=1,2,3\), and leave the remaining entries within the length-\(M\) window to 0.01. We trained the model with all parameters together for 10,000 epochs with the same loss function and learning rate. As illustrated in Figure 5, the full model converged to a state comparable to our simplified model. We further plot the \(W_{Q}^{(1)},W_{K}^{(1)},W_{V}^{(1)}\) for the first head after training in Figure 6. The results demonstrate that the model converges to a point where the query and key projections are close to zero, which leaves the RPE weights to dominate the attention mechanism. This fact justifies our simplification in (2.5) where we remove the query and key projection weights and set \(W_{V}^{(h)}\) to be identity matrix.

### Prior and Length Generalization

We further test the model learned by the three-stage training on sequences coming from different priors and of different lengths. Note that our pre-trained transformer learns to perform GIH. As introduced in SS3.1, the GIH estimator can be applied to a sequence with an arbitrary length and does not concern the prior distribution of the underlying Markov chain. Thus, it is natural to see if the pre-trained transformer can also generalize to different lengths and prior distributions.

Recall that we train the transformer model with sequence length \(L=100\) and the concentration parameter of the Dirichlet prior is \(\alpha=0.01\). Here, we test the pre-trained transformer on new sequences of different lengths and sampled from different prior distributions. That is, with a different

Figure 6: A visualization of the word embedding matrices \(W_{Q}^{(1)}\), \(W_{K}^{(1)}\), \(W_{V}^{(1)}\) of a pre-trained transformer with \(M=H=3\), \(d=3\), and \(D=2\). These are the parameters in of the first attention head in the first attention layer. Since \(d=3\), all word embedding matrices are of shape \(3\times 3\). As shown in (a) and (b), \(W_{Q}^{(1)}\) and \(W_{K}^{(1)}\) do not change much compared to their initialization value \(0.001\). Thus, they are both close to the zero matrix and play a negligible role in the first attention layer. Besides, in (c) we plot \(W_{V}^{(1)}\), which establishes a clear diagonal structure, with the diagonal entries growing to \(0.07\) compared to the initialization value \(0.001\). Thus, \(W_{V}^{(1)}\) is proportional to the identity matrix.

concentration parameter \(\alpha\), we sample a random Markov chain, and generate a sequence of length \(L\), and evaluate of cross-entropy loss for predicting \(x_{L+1}\). Here we choose \(\alpha\in\{0.05,0.1,0.2\}\) and range \(L\) from \(10\) to \(1000\). When generating the data, the Markov chains share the same parent set \(\text{pa}=\{-1,-2\}\) with the pre-training data. The results are shown in Figure 7. The results show a decreasing trend in testing loss as the sequence length increases. For \(\alpha=0.2\), we observe first a small increase in the test loss when \(L\) just exceeds \(100\), but then the loss decreases as \(L\) increases further. This experiment shows that the pre-trained transformer indeed generalizes in length and is robust to the change of prior distribution.

## Appendix C Additional Background and Discussions

### How Does Transformer Implement the GIH Mechanism?

In the following, we briefly illustrate how a two-attention-layer transformer model as introduced in (2.5) implements the GIH mechanism. As we will show in SS3.2, gradient flow with respect to the cross-entropy loss converges to this transformer in the limit.

**Step I: The First Attention Layer Copies the Information Set \(\mathcal{S}^{\star}\) to the Current Position.** Suppose the number of heads is equal to the window size for simplicity, i.e., \(H=M\). Then, attention

Figure 8: Illustration of the GIH mechanism in a two-attention-layer transformer model. Here, \(\text{pa}=\{-1,-2\}\), \(M=3\) and \(\mathcal{S}^{\star}=\{1,2\}\). The first attention layer copies the parents (including the information set \(\mathcal{S}^{\star}\)) to the current position. Then the FFN layer together with layer normalization generates the features \(u_{l}\) using the parent tokens within the information set \(\mathcal{S}^{\star}\). The second attention layer treats each \(x_{l}\) as the value, and aggregates \(x_{l}\) as the prediction by matching the keys and query that come from the learned features using the attention mechanism. The \(L+1\)-th token is padded with zeros in the input.

Figure 7: Generalization capability of our model to different sequence lengths and prior distributions. We plot the cross-entropy loss of the pre-trained transformer model on sequences with different lengths sampled from Markov chains with different prior distributions. The prior is Dirichlet distribution with \(\alpha\in\{0.05,0.1,0.2\}\) and we vary the length \(L\) in \(\{10,20,50,100,200,400,700,1000\}\). The pre-training data contains sequences of length \(L=100\) and \(\alpha=0.01\). For different \(\alpha\), we see that the error has a decreasing trend as \(L\) increases. This shows that the pre-trained transformer can generalize in length and is robust to the distributional shift due to a change of prior.

head \(h\in\mathcal{S}^{\star}\) can attend to the \(h\)-th parent token by setting the RPE weights in the softmax function to be \(w^{(h)}=\rho\cdot e_{-h}\) for a sufficiently large \(\rho\), where \(e_{-h}\in\mathbb{R}^{M}\) is the canonical basis vector with the \((M+1-h)\)-th entry being one and all other entries being zero. As a result, each \(v_{l}^{(h)}\) for \(h\in\mathcal{S}^{\star}\) satisfies \(v_{l}^{(h)}\approx x_{l-h}\).

Step II: FFN Generates the Polynomial Features of the Information Set \(\mathcal{S}^{\star}\).As we have introduced in (2.3), each learnable \(c_{\mathcal{S}}\) in the FFN layer determines the contribution of the corresponding subset \(\mathcal{S}\) to the output feature. To let the optimal information set \(\mathcal{S}^{\star}\) dominate the output, we set \(c_{\mathcal{S}^{\star}}=1\) whereas \(c_{\mathcal{S}}=0\) for all \(\mathcal{S}\neq\mathcal{S}^{\star}\). The exact form of the output of the FFN layer, \(\phi(v_{l})\), is deferred to SSC.2. Here the only property we require is that

\[s_{l}\!:=\!\langle\phi(v_{l}),\phi(v_{L+1})\rangle=\prod_{h\in \mathcal{S}^{\star}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\approx\mathds{1 }(X_{l-\mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{\star}}),\] (C.1)

Here \(X_{l-\mathcal{S}^{\star}}:=(x_{l-s}:s\in\mathcal{S}^{\star})\) and in the last equation we use the orthogonality and normalization of the vocabulary embeddings.

Step III: The Second Attention Layer Aggregates Tokens with Matching History on \(\mathcal{S}^{\star}\).We can interpret \(s_{l}\) in (C.1) as an indicator for whether the information set of a token \(x_{l}\) matches the information set of the token \(x_{L+1}\). Then for the second attention layer, by setting \(a\) to be sufficiently large, the output will become

\[y=\sum_{l=M+1}^{L}\frac{\exp\bigl{(}a\cdot s_{l}\bigr{)}\cdot x _{l}}{\sum_{k=M+1}^{L}\exp\bigl{(}a\cdot s_{k}\bigr{)}}\approx\begin{cases}N^{- 1}\cdot\sum_{l=M+1}^{L}x_{l}\cdot\mathds{1}(X_{l-\mathcal{S}^{\star}}=X_{L+1- \mathcal{S}^{\star}}),\text{ if }N\geq 1,\\ (L-M)^{-1}\cdot\sum_{l=M+1}^{L}x_{l},\quad\text{otherwise},\end{cases}\] (C.2)

where \(N=\sum_{l=M+1}^{L}\mathds{1}(X_{l-\mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{ \star}})\). That is, if at least one token \(x_{l}\) has a matching information set as \(x_{L+1}\), i.e., their histories restricted to \(\mathcal{S}^{\star}\) are the same, the second attention layer outputs the average of such tokens. Otherwise, it outputs the average of previous tokens from \(x_{M+1}\) to \(x_{L}\). In LemmaE.6 in the appendix, we will show that the model learned by gradient flow implements the GIH mechanism up to a diminishing approximation error.

The weights of the transformer constructed above are illustrated in Figure9. We consider the transformer model with \(M=H=3\), \(d=3\), and \(D=2\). In this case, in the first attention layer, for each \(h\in[3]\), \(W_{P}^{(h)}\) has three finite parameters \(w_{-1}^{(h)},w_{-2}^{(h)}\), and \(w_{-3}^{(h)}\). By our construction, we have \(w_{-h}^{(h)}=\rho\) for all \(h\in[3]\) and the rest of the entries of \(\{w^{(h)}\}_{h\in[3]}\) are all equal to zero. In Figure9-(a) we plot the top ten by ten block of \(W_{P}^{(1)}\), where \(w_{-1}^{(1)}=\rho\) is shown in yellow and \(w_{-2}^{(1)}=w_{-3}^{(1)}\) are shown in purple. The gray color stands for \(-\infty\) entries. In Figure9-(b) we plot \(\{w^{(h)}\}_{h\in[3]}\). In Figure9-(c) we plot the parameters of the FFN. Since \(H=3\) and \(D=2\), \([H]_{\leq D}\) contains seven elements: \(\varnothing\), \(\{1\}\), \(\{2\}\), \(\{3\}\), \(\{1,2\}\), \(\{1,3\}\), and \(\{2,3\}\). We use binary strings of length \(3\) to index these seven subsets, where the \(i\)-th bit indicates whether element \(i\) is included in the subset. For instance, "110" represents \(\{1,2\}\). We set \(\mathcal{S}^{\star}=\{1,2\}\), \(c_{\mathcal{S}^{\star}}=1\), and \(c_{\mathcal{S}}=0\) for any other \(\mathcal{S}\).

### Feed-Forward Network for Polynomial Kernel

**Lemma C.1**.: _Recall the FFN satisfying (2.3), which maps a vector \(z\in\mathbb{R}^{dH}\) to a vector in \(\mathbb{R}^{d_{e}}\). We write \(z\) as \((z^{(1)},\dots,z^{(H)})\) where \(z^{(h)}\in\mathbb{R}^{d}\) for all \(h\in[H]\). Let \(z_{i}^{(h)}\) be the \(i\)-th entry of \(z^{(h)}\). Then we can explicitly construct \(\phi(\cdot)\) by letting_

\[\phi\bigl{(}(z^{(1)},\dots,z^{(H)})\bigr{)}=\biggl{(}c_{\mathcal{S}}\cdot\prod_ {h\in\mathcal{S}}z_{i_{h}}^{(h)}:\{i_{h}\}_{h\in\mathcal{S}}\subseteq[d], \mathcal{S}\in[H]_{\leq D}\Bigr{)},\] (C.3)

_which is equivalent to_

\[\phi\bigl{(}(z^{(1)},\dots,z^{(H)})\bigr{)}=\Bigl{(}c_{\mathcal{S}}\cdot\mathrm{ vec}\bigl{(}\otimes_{h\in\mathcal{S}}(z^{(h)})\bigr{)}\Bigr{)}_{\mathcal{S}\in[H]_{ \leq D}}\,,\]

_where \(\mathrm{vec}(\cdot)\) is the vectorization operator that transforms a tensor into a vector by stacking all the entries in the tensor. That is, for any \(\mathcal{S}\), we consider the \(|\mathcal{S}|\) vectors in \(\mathbb{R}^{d}\), \(\{z^{(h)}\}_{h\in\mathcal{S}}\). In (C.3) we compute all possible products of the entries of these vectors and multiply them by \(c_{\mathcal{S}}\). In particular, for each \(\mathcal{S}\in[H]_{\leq D}\), we enumerate \(i_{h}\in[d]\) for all \(h\in\mathcal{S}\). Therefore, the output dimension of \(\phi\) is given by_

\[d_{e}=\sum_{\mathcal{S}\in[H]_{\leq D}}d^{|\mathcal{S}|}.\] (C.4)

Proof.: First, we note that the indices of \(\phi(\cdot)\) have a grouped structure -- we first enumerate all subsets in \([H]_{\leq D}\) and then enumerate all monomials with superscripts in \(\mathcal{S}\). Since there are \(d^{|\mathcal{S}|}\) monomials, the output dimension is given by (C.4).

It remains to verify (2.3) with \(\phi(\cdot)\) defined in (C.3). To this end, we note that for any \(u,v\in\mathbb{R}^{dH}\) and any \(\mathcal{S}\in[H]_{\leq D}\), we have

\[\sum_{(i_{h})_{h\in\mathcal{S}}\in[d]^{|\mathcal{S}|}}\bigg{(} \prod_{h\in\mathcal{S}}u_{i_{h}}^{(h)}\cdot v_{i_{h}}^{(h)}\bigg{)}=\prod_{h \in\mathcal{S}}\biggl{(}\sum_{i_{h}\in[d]}u_{i_{h}}^{(h)}\cdot v_{i_{h}}^{(h) }\bigg{)}=\prod_{h\in\mathcal{S}}\langle u^{(h)},v^{(h)}\rangle,\]

which directly implies (2.3). Therefore, we conclude the proof of this lemma. 

### Perron-Frobenius Theorem

Next, we review the basics for the celebrated Perron-Frobenius theorem on non-negative matrices (Meyer, 2023, Chapter 7). We consider the following class of irreducible matrices.

**Definition C.2** (Irreducible Matrix).: _A non-negative square matrix \(P\in\mathbb{R}_{+}^{d\times d}\) is called irreducible if the induced directed graph \(\mathcal{G}\) is strongly connected, i.e., for any pair of nodes in the graph, there always exists a directed path that connects these two nodes. Here, the induced graph \(\mathcal{G}\) is defined based on \(d\) nodes with adjacent matrix \(A\) given by \(A_{ij}=\mathds{1}(P_{ij}\neq 0)\)._

In particular, if \(P\) is a stochastic matrix that corresponds to a \(d\)-state Markov chain, then starting from any state, we can reach any other state with positive probability in a finite number of steps. The irreducibility property also has an equivalent definition in the matrix form. That is, for any permutation matrix \(T\), \(TPT^{-1}\) cannot be written as an upper triangular block matrix with the following form

\[\begin{bmatrix}M_{1}&M_{2}\\ 0&M_{3}\end{bmatrix}.\]

In other words, an irreducible matrix does not have a nontrivial absorbing subspace that aligns with the standard basis.

Figure 9: Limiting model of \(\mathtt{TF}(M=3,H=3,d=3,D=2)\) that implements the GIH mechanism with \(L=100\), \(\mathtt{pa}=\{-1,-2\}\). (a): The top left \(10\) by \(10\) block of \(W_{P}^{(1)}\) that attends to the \(-1\) parent. (b): The RPE weight heatmap for all 3 heads, where the \(h\)-th column corresponds to the RPE weight vector of head \(h\). (c): In the GIH mechanism, only one \(c_{\mathcal{S}}^{*}\) for the optimal information set \(\mathcal{S}^{*}\) dominates. For the label of the \(x\)-axis, we use a binary coding \(\{0,1\}^{3}\) to indicate each subset \(\mathcal{S}\). Here, \(\mathcal{S}^{*}=\{1,2\}\) is the parent set, which is represented by “110”.

In this work, we require more than the irreducibility property from the transition matrix \(P_{\pi}\) defined in SS3.2. In fact, we need the existence of a unique stationary distribution (which is not guaranteed by the irreducibility) so that the chain has a sufficiently fast mixing rate. This enables us to learn with a finite sequence length \(L\). To achieve this, one typically needs the second largest magnitude of the eigenvalues of \(P_{\pi}\), denoted by \(\lambda\), to be bounded away from \(1\), which is the leading eigenvalue of the transition matrix. The difference \(1-\lambda\) is also referred to as the spectral gap. It is well-known that if all the entries of \(P_{\pi}\) are positive, then \(P_{\pi}\) is irreducible and there is only one leading eigenvalue on the spectral circle with the corresponding eigenvector given by the chain's stationary distribution \(\mu^{\pi}\), and all the other eigenvalues have magnitude strictly less than \(1\). However, for our case, the transition matrix \(P_{\pi}\) has zero entries by definition. Fortunately, the nice property on the existence of spectral gap can be generalized to a class called _primitive_ matrix.

**Definition C.3** (Primitive Matrix).: _A nonnegative and irreducible square matrix \(P\) is called primitive if there exists an integer \(k\) such that all the entries of \(P^{k}\) are positive._

By definition of the primitive matrix, one can immediately see that for any \(k^{\prime}>k\), \(P^{k^{\prime}}_{\pi}\) is a positive matrix. The following is the celebrated Perron-Frobenius theorem that characterizes the spectral structure of the primitive matrices.

**Theorem C.4** (Perron-Frobenius Theorem for Primitive Matrices).: _Let \(P\) be a primitive matrix. Then the following statements hold:_

1. _The leading eigenvalue of_ \(P\) _is real and positive, and it is the unique eigenvalue with the largest magnitude. In particular, if_ \(P\) _is a stochastic matrix, then the leading eigenvalue is_ \(1\)_._
2. _The leading eigenvector of_ \(P\) _is positive and unique up to a scaling factor. In particular, if_ \(P\) _is a stochastic matrix, then the leading eigenvector is the stationary distribution of the Markov chain with transition kernel_ \(P\)_._

The Perron-Frobenius theorem guarantees the existence of a unique stationary distribution \(\mu^{\pi}\) when the transition matrix \(P_{\pi}\) is primitive. In particular, when we further assume that the transition matrix \(P_{\pi}\) has a spectral gap, the chain is sufficiently mixed, meaning that we can thus approximate sum over the entire sequence with an average with respect to the stationary distribution. In particular, the approximation error will decays with the sequence length \(L\).

### Sequential CE Loss

In this work, we only consider the prediction error on the last token in the sequence as in (2.1):

\[\mathcal{L}(f_{\mathtt{tf}})=-\mathbb{E}_{\pi\sim\mathcal{P},x_{1:(L+1)}} \big{[}\log\bigl{(}f_{\mathtt{tf}}(x_{L+1}\,|\,x_{1:L})+\epsilon\bigr{)}\big{]}.\]

In practice however, people often train the transformer model by minimizing the cross-entropy (CE) loss over the entire sequence. We demonstrate that our analysis can be extended to training on the entire sequence. In this vein, we define the sequential CE loss as

\[\mathcal{L}_{\mathtt{seq}}(f_{\mathtt{tf}})=\sum_{l=1}^{L}-\mathbb{E}_{\pi \sim\mathcal{P},X}\big{[}\log\bigl{(}f_{\mathtt{tf}}(x_{l+1}\,|\,x_{1:l})+ \epsilon\bigr{)}\big{]}.\] (C.5)

One can equivalently view this sequential CE loss as an aggregation of the CE loss for sequence length ranging from \(1\) to \(L\). We argue from the following two perspectives that our analysis can be extended to the sequential cross-entropy (CE) loss:

1. Due to the use of relative positional embedding (RPE), the transformer's predictions are invariant to the _absolute_ positions of tokens within a sequence. Intuitively, this implies that even if we choose a different sequence length \(L^{\prime}\), the model can still handle the task in the same manner.
2. By Assumption 3.5, the chain is sufficiently mixed for large \(L\). In the analysis, we actually use \(X_{l-M:l}=(x_{l},x_{l-1},\ldots,x_{l-M})\sim\mu^{\pi}\), where \(\mu^{\pi}\) is the stationary distribution over a length-\((M+1)\) window, to approximate the aggregation over \(X_{l-M:l}\) for \(l=M+1,\ldots,L\) in the sequence. For example, this approximation is reflected in the transition from (D.3) to (D.4) in the proof sketch in SSD. Since changing the sequence length does not affect the underlying stationary distribution, the only issue is the approximation error. In particular, for sufficiently large \(L\), the CE loss at large \(l\) constitutes the majority of the sequential CE loss in (C.5), making the CE loss at small \(l\) negligible.

### Standard \(\chi^{2}\)-Divergence and Mutual Information

The \(\chi^{2}\)-divergence (or \(\chi^{2}\)-distance) between two probability distributions \(P\) and \(Q\) in the same probability space is defined as:

\[D_{\chi^{2}}(P\|Q)=\sum_{x\in\operatorname{supp}(Q)}\frac{(P(x)-Q(x))^{2}}{Q(x)},\]

where the summation is taken over all elements \(x\) in the sample space where \(Q(x)>0\). The \(\chi^{2}\)-mutual information between two random variables \(X\) and \(Y\) with joint distribution \(P_{XY}\) and marginal distributions \(P_{X}\) and \(P_{Y}\) is defined as:

\[I_{\chi^{2}}(X;Y)=D_{\chi^{2}}(P_{XY}\|P_{X}\otimes P_{Y})=\sum_{y}D_{\chi^{2}} (P_{X\,|\,Y}(\cdot\,|\,y)\|P_{X}(\cdot))P_{Y}(y).\]

where \(P_{X}\otimes P_{Y}\) is the product of the marginals, meaning \((P_{X}\otimes P_{Y})(x,y)=P_{X}(x)P_{Y}(y)\). For a Markov chain \(X\to Y\to Z\), the \(\chi^{2}\)-mutual information satisfies the data processing inequality

\[I_{\chi^{2}}(X;Z)\leq I_{\chi^{2}}(Y;Z),\]

which follows from the observation that \(\chi^{2}\)-divergence is also an \(f\)-divergence.

### More Details on the Generalized Induction Head Mechanism

Recall that we define the Generalized Induction Head (GIH) estimator in (3.3). Specifically, \(\texttt{GIH}(x_{1:L};M,D)\) is constructed in two steps. First, we find the information-optimal subset \(\mathcal{S}^{\star}\) of \([M]\) by solving (3.2). Second, we build a \(d\)-class kernel classifier to predict \(x_{L+1}\), where the "data" used by such a classifier are \(\{\psi_{\mathcal{S}^{\star}}(l),x_{l}\}_{l\in[M+1,L]}\). Here \(\{\psi_{\mathcal{S}^{\star}}(l),l\in[M+1,L+1]\}\) are features constructed at each position based on the partial history given \(\mathcal{S}^{\star}\). In particular, similar to (C.3), for any subset \(\mathcal{S}\) of \([M]\), any input token sequence \(x_{1:L}\), and any position \(l\in[M+1,L+1]\), we define \(\psi_{\mathcal{S}}(l)=\psi_{\mathcal{S}}(l;x_{1:L})\) as

\[\psi_{\mathcal{S}}(l)=\operatorname{vec}\Bigl{(}\mathop{\otimes}_{s\in \mathcal{S}}x_{l-s}\Bigr{)}=\bigg{(}\prod_{s\in\mathcal{S}}(x_{l-s})_{i_{s}}: \{i_{s}\}_{s\in\mathcal{S}}\subseteq[d]\Big{)}\in\mathbb{R}^{d^{|\mathcal{S}|}}.\]

In other word, \(\psi_{\mathcal{S}}(l)\) is given by expanding the rank-\(1\) tensor spanned by \(\{x_{l-s}\}_{s\in\mathcal{S}}\) into a vector. Here \(x_{l-s}\in\mathcal{X}\) is a vector in \(\mathbb{R}^{d}\) and we let \((x_{l-s})_{i_{s}}\) denote its \(i_{s}\)-th entry. The rationale behind \(\psi_{\mathcal{S}}(l)\) is similar to \(\phi\) introduced in (C.3). We form a long vector containing all the products of the entries of vectors \(\{x_{l-s}\}_{s\in\mathcal{S}}\). Here we omit the dependency of \(\psi_{\mathcal{S}}\) on the input sequence \(x_{1:L}\) to simplify the notation. Furthermore, \(\psi_{\mathcal{S}}\) induces a polynomial kernel such that for any \(l,m\in[M+1,L+1]\), we have

\[\langle\psi_{\mathcal{S}}(l),\psi_{\mathcal{S}}(m)\rangle=\prod_{s\in \mathcal{S}}\langle x_{l-s},x_{m-s}\rangle=\mathds{1}\{x_{l-s}=x_{m-s},\forall s \in\mathcal{S}\}.\]

That is, feature \(\psi_{\mathcal{S}}\) selects the token position pairs \((l,m)\) such that the partial histories induced by \(\mathcal{S}\) at position \(l\) and \(m\) are exactly the same.

Based on \(\{\psi_{\mathcal{S}^{\star}}(l),x_{l}\}_{l\in[M+1,L]}\), GIH forms a kernel classifier using the indicator kernel. Specifically, for any \(j\in[d]\), by (3.3), \(\texttt{GIH}(x_{1:L};M,D)\) outputs each \(e_{j}\in\mathcal{X}\) with probability

\[\mathbb{P}\bigl{(}\texttt{GIH}(x_{1:L};M,D)=e_{j}\bigr{)}=\frac{\sum_{l=M+1}^{L }\mathds{1}\{x_{l-s}=x_{L+1-s},\forall s\in\mathcal{S}^{\star}\}\cdot \mathds{1}\{x_{l}=e_{j}\}}{\sum_{m=M+1}^{L}\mathds{1}\{x_{m-s}=x_{L+1-s}, \forall s\in\mathcal{S}^{\star}\}}.\]

### Further Discussions on the GIH Mechanism

We conclude this section with further discussions on the modified \(\chi^{2}\)-mutual information and low-degree polynomial kernel for the FFN within the GIH mechanism.

On the Modified \(\chi^{2}\)-Mutual Information.Now that we have shown how gradient flow approaches the desired GIH model, it is natural to ask the following questions: What is the optimal subset \(\mathcal{S}^{\star}\) that the model selects? How well does the model perform? For the purpose of illustration, let us consider a symmetric case where the stationary distribution \(\mu^{\pi}\) over a length-\(r_{n}\) window is uniformover \(\mathcal{X}^{T_{n}}\). One can verify that in this case, the stationary distribution over a window of any other length is uniform as well, and the modified mutual information can be simplified into

\[\log\tilde{I}_{\chi^{2}}(\mathcal{S})=\log I_{\chi^{2}}(\mathcal{S})-|\mathcal{ S}|\log d,\] (C.6)

where \(I_{\chi^{2}}(\mathcal{S})\) is the standard \(\chi^{2}\) mutual information between \(\mu^{\pi}(z\,|\,Z_{-\mathcal{S}})\) and \(\mu^{\pi}(z)\), and the second term \(|\mathcal{S}|\log d\) serves as a penalty on the _model complexity_. Thus, the GIH mechanism is _reaching a balance between the model complexity and the information richness_. Below we characterize two scenarios where the model will select the exact parent set, i.e., \(\mathcal{S}^{\star}=\mathsf{pa}\).

1. If \(n=1\), i.e., each token only has one parent, then \(\mathcal{S}^{\star}=\mathsf{pa}\). This is because \(\mathcal{S}^{\star}\) simultaneously maximizes both terms in (C.6), thus reproducing the results in Nichani et al. (2024).
2. If \(n\) is known a priori and restricting the polynomial kernel to \(\mathcal{S}\in[H]_{=n}=\{\mathcal{S}\in[H]:|\mathcal{S}|=n\}\) for the FFN layer, then \(\mathcal{S}^{\star}=\mathsf{pa}\). Here, the penalty term does not influence the selection and the exact parent set maximizes the mutual information by the data-processing inequality.

In the general case, however, the model could be much more flexible, and it is possible that the model selects only a subset of the true parent set or even some non-parent tokens that are also informative. The rationale is that with a more complex model, e.g., selecting a large \(\mathcal{S}\), the model are able to make more accurate predictions for large \(L\) but may endure a large estimation error for small \(L\), as the exact matching \(X_{l-\mathcal{S}}=X_{L+1-\mathcal{S}}\) may appear rarely in the sequence.

On the Low-Degree Polynomial Kernel.The goal of using a low-degree polynomial kernel in (2.3) is to strike a balance between model complexity (which is also related to computational cost) and the model's accuracy. In this regard, we have the following corollary.

**Corollary C.5**.: _We always have \(|\mathcal{S}^{\star}|\leq n\) regardless of the choice of \(D\), where \(\mathcal{S}^{\star}=\operatorname*{argmax}_{[H]\leq D}\log\tilde{I}_{\chi^{2} }(\mathcal{S})\) for \(\tilde{I}_{\chi^{2}}(\mathcal{S})\) in (C.6)_

The reasoning behind this corollary is as follows. Consider any set \(\mathcal{S}\) with \(|\mathcal{S}|>n\), we have \(I_{\chi^{2}}(\mathcal{S})\leq I_{\chi^{2}}(\mathsf{pa})\) as the true parent set is the most informative. Moreover, since \(|\mathsf{pa}|=n<|\mathcal{S}|\), \(\mathcal{S}\) suffers from a larger penalty. As a result, we have \(\log\tilde{I}_{\chi^{2}}(\mathcal{S})<\log\tilde{I}_{\chi^{2}}(\mathsf{pa})\) when \(\mathcal{S}\) has more than \(n\) elements. In other words, it is without loss of generality to set \(D\leq n\).

### Conclusion and Future Directions

In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning \(n\)-gram Markov chains in an in-context way. Our theoretical analysis underscores a congruous interplay between the multihead attention mechanism, the feed-forward network, and layer normalization that yields a generalized version of the induction head mechanism during the training. In particular, we prove that the generalized induction head mechanism adopts a modified \(\chi^{2}\)-mutual information criterion for parent selection that strikes a balance between information richness and model complexity. To our best knowledge, our work gives the first theoretical evidence for learning an induction head mechanism with \(n\)-gram Markov data, which potentially sheds light on the inner workings of large-scale transformer models.

Our work opens new directions for developing a rigorous understanding of the transformer models. A natural direction would be that if one can find such a mechanism with standard FFN layer using multi-layer perceptron and standard layer normalization in the more practical transformer model. The intuition is that our FFN layer in (2.3), which is further instantiated in (C.3), lies in the space of low-degree polynomials and can be well represented by a MLP with sufficient dimensions and proper activation functions. Initial attempts to learn nonlinear features have also been made by Kim and Suzuki (2024). Another direction is to investigate the training dynamics beyond a single loop of this induction head mechanism, e.g., iteration head with recursively refined predictions (Cabannes et al., 2024), and how the induction head mechanism occurs in multi-layer transformer models.

## Appendix D Proof Sketch

In this section, we discuss the main ingredients of analysis of gradient flow. First, we show in SSD.1 how to simplify the model based on our choice of the initialization and the structure of the disentangled transformer. We then proceed to present the main proof ideas for the three stages of the gradient flow dynamics, where the training yields the following behaviors:* Stage I: A unique \(\mathcal{S}^{*}\in[H]_{\leq D}\) stands out such that the associated parameter \(c_{\mathcal{S}^{*}}\) dominates those of the other sets. As a result, \(p_{\mathcal{S}}^{*}(t)=c_{\mathcal{S}^{*}}^{2}(t)/C_{D}(t)\) approaches to one.
* Stage II: For each \(h\in\mathcal{S}^{*}\), \(\sigma(w^{(h)})\) approaches a one-hot vector \(e_{M+1-h}\in\mathbb{R}^{M}\), where \(w^{(h)}\) contains the parameters of RPE of the \(h\)-th head. During this stage, each head concentrates on copying a particular parent.
* Stage III: Finally, \(a\) grows and reaches \(\mathcal{O}(\log L)\). As a result, the trained model approximately implements the GIH mechanism \(\mathtt{GIH}(x_{1:L};M,D)\).

### Simplification of the Transformer Model at Initialization

We first simplify the expression of the transformer model at initialization under Assumption 3.3, by showing that the attention scores of the second attention layer admit a simpler form.

For the second attention layer, we write the output as \(y^{\top}=\sigma(as)X\) where \(s:=u_{L+1}^{\top}\mathtt{Mask}(U_{1:L}^{\top})\in\mathbb{R}^{1\times L}\) is the row vector of the similarity scores. Recall from (2.5) that the FFN layer with normalization outputs \(U=\phi(V)/\sqrt{C_{D}}\in\mathbb{R}^{(L+1)\times d_{\iota}}\), and we denote the \(l\)-th row of \(U\) by \(u_{l}=\phi(v_{l})/\sqrt{C_{D}}\). For \(l=M+1,\ldots,L\), the \(l\)-th entry of \(s\) is given by

\[s_{l}=\langle u_{l},u_{L+1}\rangle=\langle\phi(v_{l}),\phi(v_{L+1})\rangle/C_{ D},\]

and the other entries are all \(-\infty\). By the property of the FFN layer in (2.3) and the definition \(C_{D}=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\), we can rewrite the above attention score as

\[s_{l}=\frac{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\cdot\prod_{h \in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle}{\sum_{\mathcal{S}\in [H]_{\leq D}}c_{\mathcal{S}}^{2}},\quad\text{for $l=M+1,\ldots,L$.}\] (D.1)

Note that under Assumption 3.3, by the definition of \(\Delta w\) in (3.6), we have a sufficiently large gap \(w_{-h}^{(h)}-w_{-j}^{(h)}\) for all \(j\neq h\) at initialization. Thus, \(\exp(w_{-h}^{(h)})\gg\exp(w_{-j}^{(h)})\) for all \(j\neq h\), which implies the following approximation:

\[v_{l}^{(h)}=\sum_{k=1}^{M}\frac{\exp(w_{-k}^{(h)})}{\sum_{j=1}^{M}\exp(w_{-j}^ {(h)})}\cdot x_{l-k}\approx x_{l-h},\quad\text{for $l=M+1,\ldots,L$.}\]

This further implies that for \(l=M+1,\ldots,L\), we have

\[\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\approx\prod_ {h\in\mathcal{S}}\langle x_{l-h},x_{L+1-h}\rangle=\mathds{1}\{x_{l-i}=x_{L+1- i}\text{ for $i\in\mathcal{S}$}\},\] (D.2)

which is a binary value indicating whether the query and the key token's history match on the subset \(\mathcal{S}\). Combining (D.1) and (D.2), we obtain the following simplified expression for \(s_{l}\):

\[s_{l}\approx\frac{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\cdot \mathds{1}\{x_{l-i}=x_{L+1-i}\text{ for $i\in\mathcal{S}$}\}}{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{ \mathcal{S}}^{2}}=\sum_{\mathcal{S}\in[H]_{\leq D}}p_{\mathcal{S}}\cdot \mathds{1}\{x_{l-i}=x_{L+1-i}\text{ for $i\in\mathcal{S}$}\}}\]

where we denote \(p_{\mathcal{S}}=c_{\mathcal{S}}^{2}/\sum_{\mathcal{S}\in[H]_{\leq D}}c_{ \mathcal{S}}^{2}\) for \(\mathcal{S}\in[H]_{\leq D}\).

In summary, when \(\Delta w\) is sufficiently large, \(v_{l}^{(h)}\) approximately copies the token \(x_{l-h}\). As a result, the attention score \(s_{l}\) satisfies

\[s_{l}\approx\sum_{\mathcal{S}\in[H]_{\leq D}}p_{\mathcal{S}}\cdot\mathds{1}\{x _{l-i}=x_{L+1-i}\text{ for $i\in\mathcal{S}$}\}.\]

### Analysis for Training the FFN and the First Attention Layer

The first two training stages involve the dynamics of the weights of the FFN, \(\{c_{\mathcal{S}}\}_{\mathcal{S}\in[H]_{\leq D}}\), and the weights of the first attention layer, \(\{w^{(h)}\}_{h=1}^{H}\). The analyses of these two stages have similar structures and contain the following essential steps:

1. Derive the explicit expression of the dynamics of the weights, via direct calculations.

[MISSING_PAGE_FAIL:29]

Since the value of \(f(t)\) is independent of the specific choice of set \(\mathcal{S}\), it is clear that the set \(\mathcal{S}\) achieving the fastest growth rate is the information-optimal set \(\mathcal{S}^{\star}=\operatorname*{argmax}_{\mathcal{S}\in[H]_{\leq D}}\tilde{T} _{\chi^{2}}(\mathcal{S})\) that maximizes the modified \(\chi^{2}\)-MI within \([H]_{\leq D}\).

Convergence of \(p_{\mathcal{S}^{\star}}\).Note that \(p_{\mathcal{S}}=c_{\mathcal{S}}^{2}/\sum_{\mathcal{S}^{\prime}\in[H]_{\leq D}}c _{\mathcal{S}^{\prime}}^{2}\) quantifies the contribution of the set \(\mathcal{S}\) to the feature produced by the FFN layer. Thus, it is the _relative growth rate_ of \(c_{\mathcal{S}}^{2}\) that matters. Towards this end, it follows from (D.5) that, for all \(\mathcal{S}\in[H]_{\leq D}\backslash\{\mathcal{S}^{\star}\}\),

\[\partial_{t}\log\frac{c_{\mathcal{S}^{\star}}^{2}}{c_{\mathcal{S}}^{2}}\approx \frac{4a}{C_{D}}\cdot\left(\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})- \widetilde{I}_{\chi^{2}}(\mathcal{S})\right)\geq\frac{4a}{C_{D}}\cdot\Delta \widetilde{I}_{\chi^{2}}.\] (D.6)

Here we recall from (3.5) that \(\Delta\widetilde{I}_{\chi^{2}}\) quantifies the minimal gap between the modified \(\chi^{2}\)-MI of \(\mathcal{S}^{\star}\) and any other set in \([H]_{\leq D}\). The lower bound given by (D.6) ensures that for all \(\mathcal{S}\neq\mathcal{S}^{\star}\), the ratio \(c_{\mathcal{S}^{\star}}^{2}/c_{\mathcal{S}}^{2}\) grows exponentially fast, which further implies that \(p_{\mathcal{S}^{\star}}\) approaches one exponentially fast. This concludes the first stage of the training dynamics.

#### d.2.2 Training the First Attention Layer: Convergence of \(\sigma(w^{(h)})\) to One-Hot Vector

As we proceed to the second stage after \(p_{\mathcal{S}^{\star}}\approx 1\), it suffices to show how \(\sigma(w^{(h)})\) converges to a one-hot vector \(e_{M+1-h}\) for \(h\in\mathcal{S}^{\star}\) in order to show that the model converges to the GIH mechanism. Recall that we denote \(X=(x_{1},\ldots,x_{L})\in\mathbb{R}^{L\times d}\). For notational convenience, we denote \(\sigma^{(h)}:=\sigma(w^{(h)})\) and let \(X_{(l-M):(l-1)}\in\mathbb{R}^{M\times d}\) denote the submatrix of \(X\) with rows \(l-M,\ldots,l-1\) for any \(l\). Following our convention, we let \(\sigma^{(h)}_{-i}\) denote the \((M+1-i)\)-th entry of \(\sigma^{(h)}\) and similarly for \(w^{(h)}_{-i}\).

Calculation of the Dynamics of \(w^{(h)}\).The main idea for analyzing \(\{w^{(h)}\}_{h=1}^{H}\) is the same as that in the previous stage: It suffices to analyze the _difference between the growth rates_ of different coordinates of \(w^{(h)}\) for \(h\in\mathcal{S}^{\star}\). In particular, we care about how quickly \(w^{(h)}_{-h}\) grows compared to other coordinates if \(w^{(h)}_{-h}\) is initialized to be larger than the remaining coordinates:

\[\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i} =\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\frac{\partial\ell}{\partial s _{l}}\bigg{(}\frac{\partial s_{l}}{\partial w^{(h)}_{-h}}-\frac{\partial s_{l }}{\partial w^{(h)}_{-i}}\bigg{)}\bigg{]}\] (D.7) \[=a\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sigma_{l}(as)\left(\sum_{k= 1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\right)\bigg{(}\frac{ \partial s_{l}}{\partial w^{(h)}_{-h}}-\frac{\partial s_{l}}{\partial w^{(h)}_ {-i}}\bigg{)}\bigg{]}.\]

Now, we invoke the result obtained in the previous stage that \(p_{\mathcal{S}^{\star}}\approx 1\), which gives us \(s_{l}\approx\prod_{h\in\mathcal{S}^{\star}}\langle v^{(h)}_{l},v^{(h)}_{L+1}\rangle\). Consequently, for any \(h\in\mathcal{S}^{\star}\), we have

\[\frac{\partial s_{l}}{\partial w^{(h)}_{-i}}\approx\frac{\partial}{\partial w ^{(h)}_{-i}}\prod_{h^{\prime}\in\mathcal{S}^{\star}}\langle v^{(h^{\prime})}_ {l},v^{(h^{\prime})}_{L+1}\rangle=\bigg{(}\prod_{h^{\prime}\in\mathcal{S}^{ \star}\backslash\{h\}}\langle v^{(h^{\prime})}_{l},v^{(h^{\prime})}_{L+1} \rangle\bigg{)}\cdot b^{\top}_{l}(e_{M+1-i}-(\sigma^{(h)})^{\top})\sigma^{(h)}_{ -i}\] (D.8)

where the equality follows from the fact that \(w^{(h)}_{-i}\) only affects \((v^{(h)}_{l},v^{(h)}_{L+1})\) and differentiating through the softmax function. Here we define \(b_{l}:=X_{(l-M):(l-1)}v^{(h)}_{L+1}+X_{(L+1-M):L}v^{(h)}_{l}\) to simplify the notation. Combining (D.7) and (D.8), we obtain

\[\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\approx ag^{\top}_{h}\bigg{(} \sigma^{(h)}_{-i}\left(e_{M+1-h}-e_{M+1-i}\right)+(\sigma^{(h)}_{-h}-\sigma^{(h)} _{-i})\sum_{j\neq h}\sigma^{(h)}_{-j}(e_{M+1-h}-e_{M+1-j})\bigg{)},\] (D.9)

where we introduce the following notation

\[g_{h}:=\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sigma_{l}(a\cdot s)\cdot\left(\sum_{k =1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\right)\cdot\prod_{h^{ \prime}\in\mathcal{S}\backslash\{h\}}\langle v^{(h^{\prime})}_{l},v^{(h^{ \prime})}_{L+1}\rangle b_{l}\bigg{]}.\]A detailed deviation of (D.9) can be found in (E.16). Notice that \(\sigma^{(h)}_{-h}-\sigma^{(h)}_{-i}\) is positive at initialization. Now suppose \(\sigma^{(h)}_{-h}-\sigma^{(h)}_{-i}>0\) holds at current time \(t\). Then, lower bounding \(\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\) boils down to lower bounding \(g^{\top}_{h}\left(e_{M+1-h}-e_{M+1-i}\right)\) for \(i\neq h\). Furthermore, if we can show that \(\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\) is lower bounded by some positive value, the gap \(\sigma^{(h)}_{-h}-\sigma^{(h)}_{-i}\) will further increase. Since \(\sum_{i=1}^{M}\sigma^{(h)}_{-i}\equiv 1\), this will create a reinforcing loop that makes \(\sigma^{(h)}_{-h}\) monotonically increase.

Relate the Dynamics to the Modified \(\chi^{2}\)-MI by Approximations.We demonstrate next that \(g^{\top}_{h}\left(e_{M+1-h}-e_{M+1-i}\right)\) for \(i\neq h\) admits a lower bound depending on the information gap \(\Delta\widetilde{I}_{\chi^{2}}\). Specifically, using the same strategy for (D.3), we have by definition that

\[g^{\top}_{h}e_{M+1-i}\] (D.10) \[\quad\approx\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\left[\left( \sum_{k=1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\right)\cdot \mathds{1}(x_{l-j}=x_{L+1-j},j\in\mathcal{S}^{\star}\setminus\{h\})\cdot b^{ \top}_{l}e_{M+1-i}\right]\]

where for \(b_{l}\) we have by the same approximation \(v^{(h)}_{l}\approx x_{l-h}\) and \(v^{(h)}_{L+1}\approx x_{L+1-h}\) as in (D.2) that

\[b^{\top}_{l}e_{M+1-i}=v^{(h)}_{L+1}\,{}^{\top}x_{l-i}+{v^{(h)}_{l} \,{}^{\top}x_{L+1-i}}\approx\mathds{1}(x_{L+1-h}=x_{l-i})+\mathds{1}(x_{l-h}= x_{l-i}).\] (D.11)

Now we consider the case \(i=h\) and \(i\neq h\) separately:

1. (\(i=h\)) For \(g^{\top}_{h}e_{M+1-h}\), we simply set \(i=h\) in (D.11), and the indicator \(\mathds{1}(x_{L+1-h}=x_{l-h})\) will exactly compensate for the exclusion of \(h\) in the indicator function of (D.10). Drawing an analogy to how we go from (D.3) to (D.5), we obtain \[g^{\top}_{h}e_{M+1-h}\approx 2\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star}).\]
2. (\(i\neq h\)) For \(g^{\top}_{h}e_{M+1-i}\) with \(i\neq h\) in (D.11), we apply the same reasoning as in the previous case. Additionally, by using the Cauchy-Schwarz inequality, the following inequality holds up to a small error (see Lemma F.7 for a detailed derivation): \[g^{\top}_{h}e_{M+1-i}\leq\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})+ \widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star}\setminus\{h\}\cup\{i\})\leq 2 \widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})-\Delta\widetilde{I}_{\chi^{2}}.\]

Plugging this back into the dynamics in (D.9), we conclude that for all \(i\neq h\),

\[\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\geq a\cdot\sigma^{(h)}_{-i} \cdot\Delta\widetilde{I}_{\chi^{2}}.\]

Convergence of \(\sigma(w^{(h)})\).Combining the arguments in the previous two steps, we can now say that \(\sigma^{(h)}_{-h}\) will monotonically increase. It remains to show that \(\sigma^{(h)}_{-h}\) converges to one. Note that \(\log(\sigma^{(h)}_{-h}/\sigma^{(h)}_{-i})=w^{(h)}_{-h}-w^{(h)}_{-i}\) by the definition of the softmax function. Therefore,

\[\partial_{t}\log\bigl{(}\sigma^{(h)}_{-h}/\sigma^{(h)}_{-i}\bigr{)}=\partial_{ t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\geq a\cdot\sigma^{(h)}_{-i}\cdot \Delta\widetilde{I}_{\chi^{2}}=a\cdot\Delta\widetilde{I}_{\chi^{2}}\cdot\sigma^{ (h)}_{-h}(0)\cdot\bigl{(}\sigma^{(h)}_{-i}/\sigma^{(h)}_{-h}\bigr{)}\]

where \(\sigma^{(h)}_{-h}(0)\) is the initial value of \(\sigma^{(h)}_{-h}\) at time \(t=0\). One can now rearrange the term and pick the ratio \(\sigma^{(h)}_{-i}/\sigma^{(h)}_{-h}\) as the variable to track in the dynamics. A refined analysis in the convergence analysis in SSE.3 shows that \(\sigma^{(h)}\) converges to a one-hot vector with \(\sigma^{(h)}_{-h}\) going to one. In particular, the convergence rate is determined by the information gap \(\Delta\widetilde{I}_{\chi^{2}}\) according to the above formula.

### Analysis for the Training of the Second Attention Layer

In the last stage, we turn to the training of \(a\) given that all \(\sigma^{(h)}\)'s for \(h\in\mathcal{S}^{\star}\) are approximately one-hot vectors. The following approximation of the dynamics of \(a(t)\) is performed in the region \(a\leq O(\log L)\), where the signal term in the dynamics dominates the approximation error.

Calculation of the Dynamics of \(a\).After Stages I and II, the output is approximated as \(y(k)\approx y^{\star}(k)\!:=\!\sum_{l=1}^{L}\sigma_{l}^{\star}\,\mathds{1}(x_{l}= e_{k})\) for each \(k\in[d]\). Here the weighting coefficients \(\sigma_{1}^{\star},\ldots,\sigma_{L}^{\star}\) satisfy

\[\sigma_{l}^{\star}\propto\exp\left(a\cdot\mathds{1}(X_{l-\mathcal{S}^{\star}} =X_{L+1-\mathcal{S}^{\star}})\right).\]

Note that for each \(l\in[L]\), \(\sigma_{l}^{\star}\) indicates the importance assigned to the \(l\)-th token based on the corresponding history of \(x_{l}\) over the information set \(\mathcal{S}^{\star}\). In the population counterpart, when the chain has sufficiently mixed, for given \(X_{L+1-\mathcal{S}^{\star}}\), we can roughly view each \((x_{l},X_{l-\mathcal{S}^{\star}})\) as being sampled from a _reweighed version of the stationary distribution_:

\[\widehat{\mu}^{\pi}(x_{l},X_{l-\mathcal{S}^{\star}}\,|\,X_{L+1-\mathcal{S}^{ \star}})\propto\mu^{\pi}(x_{l},X_{l-\mathcal{S}^{\star}})\cdot\exp\left(a \cdot\mathds{1}(X_{l-\mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{\star}})\right).\]

Following the same argument as those in the previous stages, replacing the sum over \(l\) with the expectation over the stationary distribution, we arrive at

\[\partial_{t}a\approx\mathbb{E}_{\pi\sim\mathcal{P},(x,X_{-\mathcal{S}^{\star }},z,Z_{-\mathcal{S}^{\star}})\sim q^{\pi}}\bigg{[}\,\mathds{1}(X_{-\mathcal{ S}^{\star}}=Z_{-\mathcal{S}^{\star}})\cdot\bigg{(}\sum_{k=1}^{d}\frac{ \mathds{1}(x=z=e_{k})}{\widehat{\mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{\star} })}-1\bigg{)}\bigg{]}.\] (D.12)

See detailed derivations of the above approximation in SSE.4. Comparing the above expression with (D.4) in Stage I, one can see that here \((x,X_{-\mathcal{S}^{\star}})\) and \((z,Z_{-\mathcal{S}^{\star}})\) are no longer independent because now the model has learned to perform a _information-theoretic feature selection_, i.e., focusing on tokens sharing the same set of features based on the information set \(\mathcal{S}^{\star}\), which is defined according to the modified \(\chi^{2}\)-mutual information. In fact, the underlying joint distribution \(q^{\pi}\) is given by \(q^{\pi}=\mu^{\pi}(x,X_{-\mathcal{S}^{\star}})\cdot\widehat{\mu}^{\pi}(z,Z_{- \mathcal{S}^{\star}}\,|\,X_{-\mathcal{S}^{\star}})\).

Divergence of \(a\).As the dynamics of \(a\) has no closed-form expression due to the nonlinearity in the reweighed distribution \(\widehat{\mu}^{\pi}\), we resort to providing characterization for cases where \(a\) is either sufficiently small or large. In both cases, the lower and upper bounds of (D.12) can be derived, respectively. Using these bounds, we can argue rigorously that for small \(a\), it undergoes super-exponential growth until it reaches a critical "elbow" value. After that, when \(a\) becomes even larger, it grows logarithmically until it reaches \(\Omega(\log L)\).

## Appendix E Analysis of the Training Dyanamics

Masking the Simplified Model.Recall that we apply a mask to the first \(M\) position in the simplified model. Therefore, we only allow index \(l\) to run from \(M+1\) to \(L\) in the following analysis. In the following, we first specify the conditions on \(L\) that are required for the analysis of the training dynamics and then present the proof of Theorem3.6.

### Conditions on the Sequence Length

We first introduce the following condition on \(L\):

\[L\geq\Omega\bigg{(}\frac{1}{\Delta\widehat{\mu}_{\chi^{2}}^{2}(1-\lambda) \gamma^{r_{n}+2}}\bigg{)},\quad L\geq(1-\lambda)^{-1}\gamma^{-D},\quad\sqrt{L }\geq M\lor d,\] (E.1)

where \(\Omega\) only hides a universal constant that does not depend on the model parameters. The conditions in (E.1) will facilitate our analysis for Stage I and Stage II. For the last stage, we require

\[L\geq 2M+r_{n}\frac{\log\gamma^{-1}}{\lambda^{-1}},\quad\frac{L}{(\log L)^{4}} \geq\Omega\bigg{(}\frac{1}{\kappa^{4}\gamma^{8+2|\mathcal{S}^{\star}|}}\cdot \left(\frac{\sqrt{M}+d}{(1-\lambda)^{1/2\gamma|\mathcal{S}^{\star}|+2+r_{n}/4 }}\right)^{4}\bigg{)},\] (E.2)

where

\[\kappa\!:=\!\mathbb{E}\left[D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot \,|\,X_{-\mathcal{S}^{\star}}))\right]\wedge\mathbb{E}\left[D_{\chi^{2}}(\mu^ {\pi}(\cdot\,|\,X_{-\mathcal{S}^{\star}})\,\|\,\mu^{\pi}(\cdot))\right]\wedge 1,\]

and \(\Omega\) only hides universal constants that do not depend on the model parameters. Here, \(\mu^{\pi}(x,X_{-\mathcal{S}^{\star}})\) denotes the stationary distribution of the Markov chain over token \(x\) and its parents \(X_{-\mathcal{S}^{\star}}\), with \(\mathcal{S}^{\star}\) being the information set defined in (3.2).

### Analysis for Stage I

In this section, we analyze the dynamics of the parameters \(\{c_{\mathcal{S}}^{2}\}_{\mathcal{S}\in[H]_{\leq D}}\) in the first stage of training. We will show that there is a unique \(\mathcal{S}_{*}\in[H]_{\leq D}\) such that \(c_{\mathcal{S}}^{2}\), dominates all the other \(c_{\mathcal{S}}^{2}\)'s at the end of the first stage. In addition, we will characterize how fast this happens and provide a corresponding convergence rate.

Proof Strategy.At a high level, the strategy is to analyze \(\partial_{t}\log c_{\mathcal{S}^{*}}^{2}-\partial_{t}\log c_{\mathcal{S}}^{2}\) for all \(\mathcal{S}\neq\mathcal{S}^{*}\) via the following steps:

1. **Dynamics Calculation.** First, we calculate the dynamics of \(\log c_{\mathcal{S}}^{2}\) for each fixed \(\mathcal{S}\). By selecting sufficiently small values for \(a\) and \(\varepsilon\), and leveraging the mixing properties of the Markov chain with large \(L\), the dynamics of \(\log c_{\mathcal{S}}^{2}\) is approximately governed by the modified mutual information \(\widetilde{I}_{\chi^{2}}(\mathcal{S})\).
2. **Lower Bound for The Growth Rate.** Consequently, we are able to lower bound the difference between the growth rates, \(\partial_{t}\log c_{\mathcal{S}^{*}}^{2}-\partial_{t}\log c_{\mathcal{S}}^{2}\), in terms of \(\Delta\widetilde{I}_{\chi^{2}}\), the gap between the modified mutual information of \(\mathcal{S}^{*}\) and the second-best set.
3. **Convergence.** Finally, we derive the convergence using the above lower bound.

Before presenting the proof, we first remind the readers of a few definitions and notations. Recall that our simplified model is given by

\[y=(\sigma(as)X)^{\top}=\sum_{l=M+1}^{L}\sigma_{l}(as)\cdot x_{l},\quad\text{ where}\quad s_{l}=\frac{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\cdot\prod_{h \in\mathcal{S}}(v_{l}^{(h)},v_{L+1}^{(h)})}{\sum_{\mathcal{S}\in[H]_{\leq D}} c_{\mathcal{S}}^{2}}\]

Also recall that \(C_{D}(t)=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}(t)\) and \(p_{\mathcal{S}}(t)=c_{\mathcal{S}}^{2}(t)/C_{D}(t)\) for each \(\mathcal{S}\in[H]_{\leq D}\). The loss function can be rewritten as

\[\mathcal{L}=\mathbb{E}[\ell],\quad\text{where}\quad\ell=-\langle x_{L+1},\log (y+\varepsilon\mathbf{1})\rangle.\]

Here the expectation \(\mathbb{E}\) is taken over both the sequence \((x_{1},\ldots,x_{L+1})\) and the Markov kernel \(\pi\sim\mathcal{P}\). We abbreviate \(\sigma\equiv\sigma(as)\) for convenience and denote by \(\sigma_{l}\) the \(l\)-th element of \(\sigma\). By direct calculation, we have

\[\frac{\partial\ell}{\partial y}=-\frac{x_{L+1}}{y+\varepsilon\mathbf{1}}, \quad\frac{\partial y}{\partial\sigma}=X^{\top},\quad\frac{\partial\sigma}{ \partial s_{l}}=a\cdot\sigma_{l}(as)\cdot(e_{l}^{\top}-\sigma),\]

Then applying the chain rule, we have

\[\frac{\partial\ell}{\partial s_{l}}=\frac{\partial\ell}{\partial y}\frac{ \partial y}{\partial\sigma}\frac{\partial\sigma}{\partial s_{l}}=-a\left( \frac{x_{L+1}}{y+\varepsilon\mathbf{1}}\right)^{\top}(x_{l}-y)\cdot\sigma_{l} (as).\] (E.3)

In addition,

\[\frac{\partial s_{l}}{\partial c_{\mathcal{S}}}=\frac{2c_{\mathcal{S}}\prod_{h \in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle}{\sum_{\mathcal{S}^{ \prime}\in[H]_{\leq D}}c_{\mathcal{S}^{\prime}}^{2}}-\frac{2c_{\mathcal{S}}s _{l}}{\sum_{\mathcal{S}^{\prime}\in[H]_{\leq D}}c_{\mathcal{S}^{\prime}}^{2}} =\frac{2c_{\mathcal{S}}}{C_{D}}\bigg{(}\prod_{h\in\mathcal{S}}\langle v_{l}^{ (h)},v_{L+1}^{(h)}\rangle-s_{l}\bigg{)}.\]

Now, we are ready to present the proof of Theorem 3.6 for the first stage of training. We remind readers that here only \(\{c_{\mathcal{S}}\}_{\mathcal{S}[H]_{\leq D}}\) are trained, and we omit the dependence on \(t\) for convenience.

Proof of Theorem 3.6: Stage I.As discussed in the proof strategy above, we first derive the dynamics of \(\log c_{\mathcal{S}}^{2}\) for each fixed \(\mathcal{S}\in[H]_{\leq D}\). Then we compare the growth rate of \(c_{\mathcal{S}}^{2}\), with any other \(c_{\mathcal{S}}^{2}\).

Calculation of The Dynamics of \(\log c_{\mathcal{S}}^{2}\).We fix a \(\mathcal{S}\in[H]_{\leq D}\) and apply the chain rule \(\partial\ell/\partial c_{\mathcal{S}}=\sum_{l=M+1}^{L}\partial\ell/\partial s _{l}\cdot\partial s_{l}/\partial c_{\mathcal{S}}\) and the gradient flow formula that \(\partial_{t}c_{\mathcal{S}}^{2}=-2c_{\mathcal{S}}\cdot\partial\mathcal{L}/ \partial c_{\mathcal{S}}\). We have

\[\partial_{t}c_{\mathcal{S}}^{2}=\frac{4ac_{\mathcal{S}}^{2}}{C_{D}}\sum_{l=M+1}^ {L}\mathbb{E}\bigg{[}\sigma_{l}(as)\cdot\left(\frac{x_{L+1}}{y+\varepsilon \mathbf{1}}\right)^{\top}(x_{l}-y)\cdot\bigg{(}\prod_{h\in\mathcal{S}}\langle v _{l}^{(h)},v_{L+1}^{(h)}\rangle-s_{l}\bigg{)}\bigg{]}.\] (E.4)_In the following, we consider a fixed \(\pi\) for error analysis and take expectation over \(\pi\) again when plugging in everything back into the dynamics._ To simplify the expression of \(\partial_{t}c_{\mathcal{S}}^{2}\), we define quantities \(g_{0,\mathcal{S}}\) and \(f\) as

\[g_{0,\mathcal{S}} :=\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\sigma_{l}(as)\sum_{k= 1}^{d}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}-\frac{ y(k)\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\biggr{)}\prod_{h\in\mathcal{S}} \langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]},\] \[f :=\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\sigma_{l}(as)\sum_{k= 1}^{d}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}-\frac{ y(k)\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\biggr{)}\cdot s_{l}\bigg{]}.\]

Note that here \(f\) does not depend on \(\mathcal{S}\). Based on the above definitions, we can rewrite (E.4) as

\[\partial_{t}\log c_{\mathcal{S}}^{2}=\frac{1}{c_{\mathcal{S}}^{2}}\cdot \partial_{t}c_{\mathcal{S}}^{2}=\frac{4a}{C_{D}}\cdot\mathbb{E}_{\pi\sim \mathcal{P}}[g_{0,\mathcal{S}}-f].\] (E.5)

Using this, it can be shown that \(C_{D}(t)\) does not change during the training, as described in the following lemma.

**Lemma E.1**.: _The quantity \(C_{D}(t)=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}(t)\) is preserved along the gradient flow over \(\{c_{\mathcal{S}}\}_{\mathcal{S}\in[H]_{\leq D}}\), i.e., \(\partial_{t}C_{D}(t)\equiv 0\)._

This lemma will be useful in the following analysis, and we defer its proof to SSE.2.1. Next, we proceed to further simplify the dynamics in (E.5) by approximating \(g_{0,\mathcal{S}}\).

Simplification of \(\partial_{t}\log c_{\mathcal{S}}^{2}\).To approximiate \(g_{0,\mathcal{S}}\), we introduce the following quantities:

\[g_{1,\mathcal{S}} :=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\biggl{(} \sum_{k=1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+\varepsilon}- \frac{\bar{y}(k)\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+\varepsilon}\biggr{)} \prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]},\] \[g_{2,\mathcal{S}} :=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\biggl{(} \sum_{k=1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})}-1 \biggr{)}\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]},\] \[g_{3,\mathcal{S}} :=\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}}\bigg{[} \biggl{(}\sum_{k=1}^{d}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\biggr{)} \prod_{h\in\mathcal{S}}\langle v^{(h)}(Z),v^{(h)}(X)\rangle\bigg{]},\]

where \(Z=(z_{-M},\ldots,z_{-1})\) is independent of \(X=(x_{-M},\ldots,x_{-1})\) and we define

\[v^{(h)}(X):=\sum_{i=1}^{M}\sigma_{-i_{h}}^{(h)}x_{-i_{h}},\quad v^{(h)}(Z):= \sum_{i=1}^{M}\sigma_{-i_{h}}^{(h)}z_{-i_{h}},\quad\text{and}\ \bar{y}:=\frac{1}{L-M}\sum_{l=M+1}^{L}x_{l}.\]

Here \(\bar{y}(k)\) is the \(k\)-th entry of \(\bar{y}\). We remark that each of \(g_{1,\mathcal{S}},g_{2,\mathcal{S}},g_{3,\mathcal{S}}\) is a function of \(\pi\) and \(t\), but we omit the dependence for brevity.

From \(g_{0,\mathcal{S}}\) to \(g_{1,\mathcal{S}}\), we replace attention probability \(\sigma_{l}(as)\) by the uniform average with factor \(1/L\), which yields \(\bar{y}\). From \(g_{1,\mathcal{S}}\) to \(g_{2,\mathcal{S}}\), we replace the empirical distribution \(\bar{y}\) with the stationary distribution \(\mu^{\pi}\) and drop the small constant \(\varepsilon\). Finally, from \(g_{2,\mathcal{S}}\) to \(g_{3,\mathcal{S}}\), we replace the average over the sequence by the expectation over the stationary distribution \(\mu^{\pi}\) of the underlying Markov chain. We will show that the approximation error in each step is small, given that \(a\) and \(\varepsilon\) are sufficiently small and the Markov chain mixes well for a large \(L\).

* For the approximation of \(g_{0,\mathcal{S}}\) by \(g_{1,\mathcal{S}}\), note that when \(a\) is small, the attention probability \(\sigma_{l}(as)\approx 1/(L-M)\) for all \(l\in[L]\). More specifically, it follows from Lemma F.3 that \[|g_{0,\mathcal{S}}-g_{1,\mathcal{S}}|\leq\frac{8ad}{\varepsilon^{2}}.\]
* For the approximation of \(g_{1,\mathcal{S}}\) by \(g_{2,\mathcal{S}}\), we leverage the approximation \(\bar{y}(k)\approx\mu^{\pi}(e_{k})\) due to the mixing of the Markov chain for large \(L\). The result in Lemma F.4 implies that \[|g_{1,\mathcal{S}}-g_{2,\mathcal{S}}|\leq 4\cdot\frac{(1-\lambda)^{-1/2}(D_{ \chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1)^{1/4}+2\sqrt{M}}{L^{1/2}\gamma}+\gamma^{-1}\varepsilon\]where \(\mu_{0}(\cdot)\) is the initial distribution over the first \(r_{n}\) tokens. Here we abuse the notation of \(\mu^{\pi}\) in \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) to denote the stationary distribution over the last \(r_{n}\) tokens. Since \(\mu^{\pi}_{\min}\geq\gamma\) by Assumption 3.5, we have \[D_{\chi^{2}}(\mu_{0}\|\mu^{\pi})=\sum_{X}\left(\mu(X)-\mu^{\pi}(X)\right)^{2}/ \mu^{\pi}(X)\leq\sum_{X}1/\mu^{\pi}(X)\leq(2/\gamma)^{r_{n}}.\] (E.6) Therefore, we can further simplify the above bound as \[|g_{1,\mathcal{S}}-g_{2,\mathcal{S}}|=O\bigg{(}\frac{1}{\sqrt{L(1-\lambda) \gamma^{r_{n}+2}}}+\frac{\varepsilon}{\gamma}\bigg{)}.\]
* Finally, the approximation of \(g_{2,\mathcal{S}}\) by \(g_{3,\mathcal{S}}\) follows from the mixing property of the Markov chain. In particular, it follows from Lemma F.5 that \[|g_{2,\mathcal{S}}-g_{3,\mathcal{S}}|\leq\frac{8M}{L\gamma}+\frac{16\sqrt{D_ {\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{L(1-\lambda)\gamma^{|\mathcal{S}|/2+1} }\leq O\bigg{(}\frac{1}{L(1-\lambda)\gamma^{|\mathcal{S}|/2+r_{n}/2+1}}\bigg{)}.\]

Combining the above results, and by the assumption that \(a=a(0)=O(1/L^{3/2})\) and \(\varepsilon=1/\sqrt{L}\), we obtain the following approximation error:

\[|g_{0,\mathcal{S}}-g_{3,\mathcal{S}}| =O\left(\frac{ad}{\varepsilon^{2}}\right)+O\bigg{(}\frac{1}{\sqrt {L(1-\lambda)\gamma^{r_{n}+2}}}+\frac{\varepsilon}{\gamma}\bigg{)}+O\bigg{(} \frac{1}{L(1-\lambda)\gamma^{|\mathcal{S}|+2+r_{n}/2}}\bigg{)}\] \[\leq O\bigg{(}\frac{1}{\sqrt{L(1-\lambda)\gamma^{r_{n}+2}}}+ \frac{1}{L(1-\lambda)\gamma^{D/2+r_{n}/2+1}}\bigg{)}\leq O\bigg{(}\frac{1}{ \sqrt{L(1-\lambda)\gamma^{r_{n}+2}}}\bigg{)},\]

where we note that \(|\mathcal{S}|\leq D\) for any \(\mathcal{S}\in[H]_{\leq D}\) and the last inequality holds by also noting our condition on \(L\) in (E.1) that \(L\geq\Omega((1-\lambda)^{-1}\gamma^{-D})\). As a result, the dynamics of \(c_{\mathcal{S}}^{2}\) in (E.5) can be approximated as follows:

\[\partial_{t}\log c_{\mathcal{S}}^{2}=\frac{4a}{C_{D}}\cdot\mathbb{E}_{\pi \sim\mathcal{P}}[g_{3,\mathcal{S}}-f]+\mathcal{E},\quad\text{where }| \mathcal{E}|\leq O\bigg{(}\frac{a}{C_{D}\sqrt{L(1-\lambda)\gamma^{r_{n}+2}}} \bigg{)},\] (E.7)

where \(\mathcal{O}(\cdot)\) hides universal constants that do not depend on the model parameters. Here and in the sequel, we let \(\mathcal{E}\) denote an error term that is of the order \(O(a/\sqrt{C_{D}^{2}L(1-\lambda)\gamma^{r_{n}+2}})\) where the specific constant hidden in \(O(\cdot)\) may change from line to line, but does not depend on the model parameters. In fact, we can show \(C_{D}\) remains constant by Lemma E.1 and \(a\) is not updated during this stage. Thus, the error term \(|\mathcal{E}|\) is of scale \(O(aL^{-1/2})\).

Lower Bound for The Difference \(\partial_{t}\log c_{\mathcal{S}^{*}}^{2}-\partial_{t}\log c_{\mathcal{S}}^{2}\).The reason for approximating \(g_{0,\mathcal{S}}\) by \(g_{3,\mathcal{S}}\) in the previous step is that the latter is more interpretable, in the sense that we can relate it to the modified \(\chi^{2}\) mutual information \(\widetilde{I}_{\chi^{2}}(\mathcal{S})\). Recall that for each \(\mathcal{S}\in[H]_{\leq D}\), the modified \(\chi^{2}\)-mutual information is

\[\widetilde{I}_{\chi^{2}}(\mathcal{S})=\mathbb{E}_{\pi\sim\mathcal{P},(z,Z) \sim\mu^{\pi}}\bigg{[}\bigg{(}\sum_{e\in\mathcal{X}}\frac{\mu^{\pi}(z=e\,|\,Z _{-\mathcal{S}})^{2}}{\mu^{\pi}(z=e)}-1\bigg{)}\cdot\mu^{\pi}(Z_{-\mathcal{S} })\bigg{]}.\]

Note that \(f\) in (E.7) is independent of \(\mathcal{S}\), and will be canceled when computing \(\partial_{t}\log c_{\mathcal{S}^{*}}^{2}-\partial_{t}\log c_{\mathcal{S}}^{2}\):

\[\partial_{t}\log c_{\mathcal{S}^{*}}^{2}-\partial_{t}\log c_{\mathcal{S}}^{2}= \frac{4a}{C_{D}}\cdot\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}^{*}}-g_ {3,\mathcal{S}}]\pm 2|\mathcal{E}|.\]

Thus, it suffices to consider \(\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}^{*}}-g_{3,\mathcal{S}}]\). It follows from Lemma F.6 that for each \(\mathcal{S}\in[H]_{\leq D}\), \(\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}}]\) satisfies

\[\bigg{|}\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}}]-\prod_{h\in\mathcal{ S}}(\sigma_{-h}^{(h)})^{2}\cdot\widetilde{I}_{\chi^{2}}(\mathcal{S})\bigg{|} \leq\bigg{(}1-\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\bigg{)}\cdot \widetilde{I}_{\chi^{2}}(\mathcal{S}^{*}).\]

This yields a lower bound for \(\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}^{*}}]\) and an upper bound for \(\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}}]\) for each \(\mathcal{S}\neq\mathcal{S}^{*}\), i.e.,

\[\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}^{*}}] \geq\prod_{h\in\mathcal{S}^{*}}(\sigma_{-h}^{(h)})^{2}\cdot \widetilde{I}_{\chi^{2}}(\mathcal{S}^{*})-\bigg{(}1-\prod_{h\in\mathcal{S}^{*}}( \sigma_{-h}^{(h)})^{2}\bigg{)}\cdot\widetilde{I}_{\chi^{2}}(\mathcal{S}^{*}),\] \[\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3,\mathcal{S}}] \leq\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\cdot\widetilde{I }_{\chi^{2}}(\mathcal{S})+\bigg{(}1-\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^ {2}\bigg{)}\cdot\widetilde{I}_{\chi^{2}}(\mathcal{S}^{*}),\quad\text{for all }\mathcal{S}\neq\mathcal{S}^{*}.\]Consequently,

\[\partial_{t}\log c_{\mathcal{S}^{\star}}^{2}-\partial_{t}\log c_{ \mathcal{S}}^{2} =\frac{4a}{C_{D}}\cdot\mathbb{E}_{\pi\sim\mathcal{P}}[g_{3, \mathcal{S}^{\star}}-g_{3,\mathcal{S}}]\pm 2|\mathcal{E}|\] \[\geq\frac{4a}{C_{D}}\bigg{(}\prod_{h\in\mathcal{S}^{\star}}( \sigma_{-h}^{(h)})^{2}\cdot\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})-\prod _{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\cdot\widetilde{I}_{\chi^{2}}( \mathcal{S})\bigg{)}\] \[\qquad-\frac{4a}{C_{D}}\bigg{(}2-\prod_{h\in\mathcal{S}^{\star}}( \sigma_{-h}^{(h)})^{2}-\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\bigg{)} \widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})-2|\mathcal{E}|\] \[\geq\frac{4a}{C_{D}}\bigg{(}\bigg{(}2\prod_{h\in\mathcal{S}^{ \star}}(\sigma_{-h}^{(h)})^{2}-2\bigg{)}\widetilde{I}_{\chi^{2}}(\mathcal{S}^ {\star})+\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\cdot\Delta\widetilde{I }_{\chi^{2}}\bigg{)}-2|\mathcal{E}|,\]

where the second inequality follows from the definition \(\Delta\widetilde{I}_{\chi^{2}}=\min_{S\in[H]_{\leq D}\setminus\{\mathcal{S}^{ \star}\}}\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})-\widetilde{I}_{\chi^{ 2}}(\mathcal{S})\). Moreover, since each \((\sigma_{-h}^{(h)})^{2}\in(0,1)\), we have \(\prod_{h\in\mathcal{S}}(\sigma_{-h}^{(h)})^{2}\geq\prod_{h=1}^{H}(\sigma_{-h} ^{(h)})^{2}\) for any \(\mathcal{S}\in[H]_{\leq D}\). Applying this to the above inequality, we obtain

\[\partial_{t}\log c_{\mathcal{S}^{\star}}^{2}-\partial_{t}\log c_{ \mathcal{S}}^{2}\geq\frac{4a}{C_{D}}\bigg{(}2\prod_{h=1}^{H}(\sigma_{-h}^{(h) })^{2}\cdot\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})+\prod_{h=1}^{H}( \sigma_{-h}^{(h)})^{2}\cdot\Delta\widetilde{I}_{\chi^{2}}-2\widetilde{I}_{\chi ^{2}}(\mathcal{S}^{\star})\bigg{)}-2|\mathcal{E}|,\] (E.8)

Exponential Growth of \(c_{\mathcal{S}^{\star}}^{2}\).We proceed to show that the first term in (E.8) dominates the error term \(\mathcal{E}\) and thus leads to the exponential growth of \(c_{\mathcal{S}^{\star}}^{2}\).

Note that by Assumption 3.3, \(w_{-h}^{(h)}\geq w_{-j}^{(h)}+\Delta w\) for all \(j\neq h\) and \(h\in[H]\), where the quantity \(\Delta w\) satisfies

\[\Delta w\geq\log{(M-1)}-\log{\bigg{(}\bigg{(}1+\frac{\Delta \widetilde{I}_{\chi^{2}}}{14\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})} \bigg{)}^{\frac{1}{2H}}}-1\bigg{)}.\] (E.9)

Recall that we are not updating the RPE parameters during this stage, so \(\sigma^{(h)}\) is fixed for all \(h\in[H]\).

_So the gap condition_ (E.9) _holds throughout Stage I._ This conditions ensures that \(w_{-h}^{(h)}\gg w_{-j}^{(h)}\), so \(\prod_{h\in[H]}(\sigma_{-h}^{(h)})^{2}\) is sufficiently large. More precisely, given that head \(h\) is more focused on the \((-h)\)-th position by having a gap \(\Delta w\) in the initialization, we can further show by definition of the softmax function that

\[\sigma_{-h}^{(h)}\geq\frac{1}{1+(M-1)\exp(-\Delta w)},\forall h \in[H]\Rightarrow\prod_{h=1}^{H}(\sigma_{-h}^{(h)})^{2}\geq\frac{1}{\big{(}1+( M-1)\exp(-\Delta w)\big{)}^{2H}}.\] (E.10)

Plugging (E.9) into (E.10), we have by additionally noting that \(\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})\geq\Delta\widetilde{I}_{\chi^{2} }>0\) that

\[\prod_{h=1}^{H}(\sigma_{-h}^{(h)})^{2}\geq\bigg{(}1+\frac{\Delta \widetilde{I}_{\chi^{2}}}{14\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})} \bigg{)}^{-1}>\frac{2\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})+2/3\cdot \Delta\widetilde{I}_{\chi^{2}}}{2\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})+ \Delta\widetilde{I}_{\chi^{2}}},\]

which implies that

\[2\prod_{h=1}^{H}(\sigma_{-h}^{(h)})^{2}\cdot\widetilde{I}_{\chi^{ 2}}(\mathcal{S}^{\star})+\prod_{h=1}^{H}(\sigma_{-h}^{(h)})^{2}\cdot\Delta \widetilde{I}_{\chi^{2}}-2\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})\geq \frac{2}{3}\Delta\widetilde{I}_{\chi^{2}}.\] (E.11)

Moreover, when \(L\) is sufficiently large such that \(L\geq\Omega((\Delta\widetilde{I}_{\chi^{2}}^{2}(1-\lambda)\gamma^{r_{n}+2})^{-1})\), \(\mathcal{E}\) in (E.8) satisfy \(|\mathcal{E}|\leq 13a\Delta\widetilde{I}_{\chi^{2}}/6C_{D}\), where \(\Omega\) hides a universal constant that does not depend on the model parameters. Therefore, combining (E.8) and (E.11), we conclude that

\[\partial_{t}\log c_{\mathcal{S}^{\star}}^{2}-\partial_{t}\log c_{ \mathcal{S}}^{2}\geq\frac{8a\Delta\widetilde{I}_{\chi^{2}}}{3C_{D}}-2|\mathcal{E }|\geq\frac{a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}}.\] (E.12)

This implies that \(c_{\mathcal{S}^{\star}}^{2}\) grows exponentially fast and becomes dominant.

Convergence of \(p_{\mathcal{S}^{\star}}\).In this part, we treat all the model parameters as a function of time \(t\). For simplicity, we omit the dependence on \(t\) when it is clear from the context. It remains to derive the convergence of \(p_{\mathcal{S}^{\star}}=c_{\mathcal{S}^{\star}}^{2}/C_{D}\). Expanding \(C_{D}=\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}\), we can directly calculate the derivative of \(p_{\mathcal{S}^{\star}}\) as follows:

\[\partial_{t}\log(1-p_{\mathcal{S}^{\star}}) =\partial_{t}\log\bigg{(}1-\frac{c_{\mathcal{S}^{\star}}^{2}}{ \sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}}\bigg{)}=\frac{C_{D}}{C_{ D}-c_{\mathcal{S}^{\star}}^{2}}\cdot\partial_{t}\bigg{(}1-\frac{c_{\mathcal{S}^{ \star}}^{2}}{\sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2}}\bigg{)}\] \[=\frac{C_{D}}{C_{D}-c_{\mathcal{S}^{\star}}^{2}}\cdot\frac{-( \sum_{\mathcal{S}\in[H]_{\leq D}}c_{\mathcal{S}}^{2})\cdot\partial_{t}c_{ \mathcal{S}^{\star}}^{2}+c_{\mathcal{S}^{\star}}^{2}\cdot\sum_{\mathcal{S}\in[ H]_{\leq D}}\partial_{t}c_{\mathcal{S}}^{2}}{(\sum_{\mathcal{S}\in[H]_{\leq D}}c_{ \mathcal{S}}^{2})^{2}}\] \[=\frac{1}{C_{D}(C_{D}-c_{\mathcal{S}^{\star}}^{2})}\sum_{\mathcal{ S}\in[H]_{\leq D}}(-c_{\mathcal{S}}^{2}\cdot\partial_{t}c_{\mathcal{S}^{\star}}^{2} +c_{\mathcal{S}^{\star}}^{2}\cdot\partial_{t}c_{\mathcal{S}}^{2})\] \[=\frac{1}{C_{D}(C_{D}-c_{\mathcal{S}^{\star}}^{2})}\sum_{\mathcal{ S}\in[H]_{\leq D}\setminus\{\mathcal{S}^{\star}\}}c_{\mathcal{S}^{\star}}^{2} \cdot c_{\mathcal{S}}^{2}\cdot(-\partial_{t}\log c_{\mathcal{S}^{\star}}^{2}+ \partial_{t}\log c_{\mathcal{S}}^{2})\]

where in the last equality we use the fact that \(\partial_{t}\log c_{\mathcal{S}}^{2}=(\partial_{t}c_{\mathcal{S}}^{2})/c_{ \mathcal{S}}^{2}\). Applying (E.12) to each \(\mathcal{S}\neq\mathcal{S}^{\star}\), we further have

\[\partial_{t}\log(1-p_{\mathcal{S}^{\star}}) \leq\frac{1}{C_{D}(C_{D}-c_{\mathcal{S}^{\star}}^{2})}\sum_{ \mathcal{S}\in[H]_{\leq D}\setminus\{\mathcal{S}^{\star}\}}c_{\mathcal{S}^{ \star}}^{2}\cdot c_{\mathcal{S}}^{2}\cdot\bigg{(}-\frac{a\Delta\widetilde{I}_{ \chi^{2}}}{2C_{D}}\bigg{)}\] \[=\frac{1}{C_{D}(C_{D}-c_{\mathcal{S}^{\star}}^{2})}\cdot c_{ \mathcal{S}^{\star}}^{2}\cdot(C_{D}-c_{\mathcal{S}^{\star}}^{2})\cdot\bigg{(}- \frac{a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}}\bigg{)}=-\frac{c_{\mathcal{S}^{ \star}}^{2}\cdot a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}^{2}}<0.\]

This implies that \(p_{\mathcal{S}^{\star}}=c_{\mathcal{S}^{\star}}^{2}/C_{D}\) monotonically increases, and thus \(c_{\mathcal{S}^{\star}}^{2}(t)\geq c_{\mathcal{S}^{\star}}^{2}(0)\) for any \(t\geq 0\) because \(C_{D}\) is constant by Lemma E.1 and \(c_{\mathcal{S}^{\star}}^{2}(0)\) is the initial value for \(c_{\mathcal{S}^{\star}}^{2}\), at time \(t=0\). Therefore, we can further replace \(c_{\mathcal{S}^{\star}}^{2}\) by its initial value in the above inequality, which yields

\[\partial_{t}\log(1-p_{\mathcal{S}^{\star}})\leq-\frac{c_{\mathcal{S}^{\star}}^{ 2}(0)a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}^{2}}=-\frac{p_{\mathcal{S}^{\star} }(0)a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}}\]

We remark that the above upper bound is independent of \(t\). Finally, applying the Gronwall's inequality to \(\log(1-p_{\mathcal{S}^{\star}})\), we obtain

\[1-p_{\mathcal{S}^{\star}}(t)\leq(1-p_{\mathcal{S}^{\star}}(0))\cdot\exp\bigg{(} -\frac{p_{\mathcal{S}^{\star}}(0)a\Delta\widetilde{I}_{\chi^{2}}}{2C_{D}}\cdot t \bigg{)}.\]

With training time \(t_{1}\geq(2C_{D}(0)\log L)/(a\cdot p_{\mathcal{S}^{\star}}(0)\Delta\widetilde{ I}_{\chi^{2}})\), we can guarantee that

\[1-p_{\mathcal{S}^{\star}}(t_{1})\leq L^{-1}.\]

This concludes the proof for the first stage of the training. 

#### e.2.1 Additional Proofs for the Stage I

We conclude this subsection with the proof of Lemma E.1.

Proof of Lemma e.1.: By (E.5), we have

\[\partial_{t}c_{\mathcal{S}}^{2}=\mathbb{E}_{\pi\sim\mathcal{P}}[4a\cdot p_{ \mathcal{S}}(g_{0,\mathcal{S}}-f)].\]

Moreover, by the definition of \(g_{0,\mathcal{S}}\) and \(f\), it holds that \(\sum_{\mathcal{S}\in[H]_{\leq D}}p_{\mathcal{S}}g_{0,\mathcal{S}}=f\). Then,

\[\partial_{t}C_{D}=\sum_{\mathcal{S}\in[H]_{\leq D}}\partial_{t}c_{\mathcal{S}}^ {2}=4a\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{\mathcal{S}\in[H]_{\leq D }}p_{\mathcal{S}}g_{0,\mathcal{S}}-f\bigg{]}\equiv 0.\]

Thus, the quantity \(C_{D}\) is preserved under the dynamics.

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_FAIL:39]

where in the first identity, we add and then subtract term \(\sigma_{-i}^{(h)}e_{M+1-h}\). Combining (E.15) and (E.16) yields for each \(i\in[M]\) that

\[\partial_{t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}\] (E.17) \[\quad=a\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}g_{h,0}^{\top} \bigg{(}\sigma_{-i}^{(h)}\left(e_{M+1-h}-e_{M+1-i}\right)+\left(\sigma_{-h}^{(h )}-\sigma_{-i}^{(h)}\right)\sum_{j=1}^{M}\sigma_{-j}^{(h)}(e_{M+1-h}-e_{M+1-j} )\bigg{)}\bigg{]}.\]

Simplification of \(\partial_{t}w_{-i}^{(h)}\).We proceed by deriving approximations to the vector \(g_{h,0}\), which will help us identify the dominant term in the dynamics \(\partial_{t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}\). Specifically, we define

\[g_{h,1}:=\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\sigma_{l}(as )\sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon }-\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\bigg{)}\prod_{h^{ \prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v_{l}^{(h^{\prime})},v_{L+1}^ {(h^{\prime})}\rangle b_{l}\bigg{]},\] \[g_{h,2}:=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[} \sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+ \varepsilon}-\frac{\bar{y}(k)\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+ \varepsilon}\bigg{)}\prod_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v _{l}^{(h^{\prime})},v_{L+1}^{(h^{\prime})}\rangle b_{l}\bigg{]},\] \[g_{h,3}:=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[} \bigg{(}\sum_{k=1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})} -1\bigg{)}\prod_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v_{l}^{(h ^{\prime})},v_{L+1}^{(h^{\prime})}\rangle b_{l}\bigg{]},\] \[g_{h,4}:=\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}} \bigg{[}\bigg{(}\sum_{k=1}^{d}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1 \bigg{)}\prod_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v^{(h^{ \prime})}(Z),v^{(h^{\prime})}(X)\rangle b(X,Z)\bigg{]},\]

where \(Z=[z_{-M},\ldots,z_{-1}]^{\top}\in\mathbb{R}^{M\times d}\) is an independent copy of \(X=[x_{-M},\ldots,x_{-1}]^{\top}\in\mathbb{R}^{M\times d}\), and

\[v^{(h)}(X):=\sum_{i=1}^{M}\sigma_{-i}^{(h)}x_{-i},\quad v^{(h)}(Z ):=\sum_{i=1}^{M}\sigma_{-i}^{(h)}z_{-i},\] \[b(X,Z):=Z(v^{(h)}(X))+X(v^{(h)}(Z)),\quad\bar{y}:=\frac{1}{L-M} \sum_{l=M+1}^{L}x_{l}.\]

The strategy of gradually approximating \(g_{h,0}\) by \(g_{h,1},g_{h,2},g_{h,3}\) and \(g_{h,4}\) is similar to the analysis in Stage I. To see the intuition, from \(g_{h,0}\) to \(g_{h,1}\), we use the fact that \(p_{\mathcal{S}^{*}}\approx 1\) and \(p_{\mathcal{S}}\approx 0\) for any other \(\mathcal{S}\), which is a result of Stage 1. From \(g_{h,1}\) to \(g_{h,2}\), we replace \(y\) by the empirical mean \(\bar{y}\), thanks to the fact that \(\sigma_{l}(a)\approx 1/L\) when \(a\) is small. Then, from \(g_{h,2}\) to \(g_{h,3}\), we replace the empirical distribution \(\bar{y}\) with the stationary distribution of the Markov chain. These two steps also appear in the analysis of Stage 1. Finally, to go from \(g_{h,3}\) to \(g_{h,4}\), we leverage the rapid mixing of the Markov chain.

Note that the common structures in (E.15) are \(g_{h,0}^{\top}(e_{M+1-h}-e_{M+1-i})\) for \(i\neq h\). Hence, we only need to understand the approximation error in each step for \(g_{h,0}^{\top}(e_{M+1-h}-e_{M+1-i})\). Recall that we are focusing on \(h\in\mathcal{S}^{*}\) in this stage.

* From \(g_{h,0}\) to \(g_{h,1}\), we remove the terms in the summation that are weighted down by \(p_{\mathcal{S}}\) for any \(\mathcal{S}\neq\mathcal{S}^{*}\) due to the rapid dominance of \(p_{\mathcal{S}^{*}}\) from Stage I. Recall that \(p_{\mathcal{S}^{*}}\) converges to one at an exponential rate while all other \(p_{\mathcal{S}}\)'s converge to zero. For simplicity, let us define \[\rho(\mathcal{S}):=\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[} \sigma_{l}(as)\sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k) +\varepsilon}-\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\bigg{)} \bigg{.}\cdot\prod_{h^{\prime}\in\mathcal{S}\setminus\{h\}}\langle v_{l}^{(h^{ \prime})},v_{L+1}^{(h^{\prime})}\rangle b_{l}\bigg{]}(e_{M+1-h}-e_{M+1-i}).\]By the triangular inequality, we have \[\left|(g_{h,0}-g_{h,1})^{\top}\left(e_{M+1-h}-e_{M+1-i}\right) \right|=\bigg{|}\sum_{\begin{subarray}{c}S\in[H]_{l}\leq D\setminus\{S^{\star}\} \\ i\hbar\leq S\end{subarray}}p_{\mathcal{S}}\cdot\rho(\mathcal{S})-\rho(\mathcal{ S}^{\star})\bigg{|}\] \[\leq(1-p_{\mathcal{S}^{\star}})\cdot\big{|}\rho(\mathcal{S}^{ \star})\big{|}+\sum_{\mathcal{S}\in[H]_{l}\leq D\setminus\{\mathcal{S}^{\star}\} }p_{\mathcal{S}}\cdot|\rho(\mathcal{S})|\leq 16(1-p_{\mathcal{S}^{\star}}),\] where in the last line we use the claim that \(|\rho(\mathcal{S})|\leq 8\) for all \(\mathcal{S}\). To see this point, note that by definition of \(b_{l}\) in (E.14), we have \[\left|b_{l}^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\right|=\left| \langle v_{L+1}^{(h)},x_{l-h}-x_{l-i}\rangle-\langle v_{l}^{(h)},x_{L+1-h}-x_{ L+1-i}\rangle\right|\leq 4,\] \[\bigg{|}\prod_{h^{\prime}\in\mathcal{S}\setminus\{h\}}\langle v _{l}^{(h^{\prime})},v_{L+1}^{(h^{\prime})}\rangle\bigg{|}\leq 1,\] since \(v_{l}^{(h)}\) and \(x_{l}\) have norm at most \(1\). Then, by Lemma F.2 where we plug in the upper bound \(4\) for the function \(f(\cdot)\) in the lemma, we conclude that \(\rho(\mathcal{S})\leq 8,\quad\forall\mathcal{S}\in[H]_{\leq D}\setminus\{ \mathcal{S}^{\star}\}\). Define \(\Delta_{1}:=1-p_{\mathcal{S}^{\star}}(t_{1})\), and \(\Delta_{1}\leq 1/L\) by the results from Stage I. Thus, we obtain \[\big{|}(g_{h,0}-g_{h,1})^{\top}(e_{M+1-h}-e_{M+1-i})\big{|}\leq 16\Delta_{1} \leq 16/L.\]
* For the approximation of \(g_{h,1}\) by \(g_{h,2}\), we use the fact that \(\sigma_{l}(as)\approx 1/L\) when \(a\) is sufficiently small. Specifically, we also take the absolute bound for \(f(\cdot)\) as \(4\) in Lemma F.3 and obtain \[\big{|}(g_{h,1}-g_{h,2})^{\top}\left(e_{M+1-h}-e_{M+1-i})\big{|} \leq\frac{32ad}{\varepsilon^{2}}.\]
* For the approximation of \(g_{h,2}\) by \(g_{h,3}\), we use the fact that \(\overline{y}(k)\approx\mu^{\pi}(e_{k})\) for large \(L\). More precisely, it follows from Lemma F.4 with the upper bound \(4\) for \(f(\cdot)\) in the lemma that \[\big{|}(g_{h,2}-g_{h,3})^{\top}\left(e_{M+1-h}-e_{M+1-i})\big{|} \leq 16\cdot\frac{(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1 )^{1/4}+2\sqrt{M}}{L^{1/2}\gamma}+4\gamma^{-1}\varepsilon.\]
* Finally, to go from \(g_{h,3}\) to \(g_{h,4}\), we leverage the rapid mixing of the Markov chain. Intuitively, when \(l\) and \(L+1\) are far apart, \(x_{l}\) and its parents in \(\mathcal{S}^{\star}\) are independent of \(x_{L+1}\) and its parents in \(\mathcal{S}^{\star}\). This observation yields the approximation of \(g_{h,3}^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\) by \(g_{h,4}^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\). To simplify the notation, define two scalars \[\widetilde{g}_{h,l} :=\left(\sum_{k=1}^{d}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^ {\pi}(e_{k})}-1\right)\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}} \langle v_{l}^{(h^{\prime})},v_{L+1}^{(h^{\prime})}\rangle,\] \[\widetilde{g}_{h,4} :=\left(\sum_{k=1}^{d}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{ k})}-1\right)\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}\langle v ^{(h^{\prime})}(Z),v^{(h^{\prime})}(X)\rangle.\] Using the notation above, we have \[\big{|}(g_{h,3}-g_{h,4})^{\top}\big{(}e_{M+1-h}-e_{M+1-i})\big{|}\] \[=\bigg{|}\bigg{(}\sum_{l=M+1}^{L}\frac{\mathbb{E}_{X|\pi}\big{[} \widetilde{g}_{h,l}b_{l}^{\top}\big{]}}{L-M}-\mathbb{E}_{(x,X),(z,Z)\sim\mu^{ \pi}\otimes\mu^{\pi}}\big{[}\widetilde{g}_{h,4}b(X,Z)^{\top}\big{]}\bigg{)} \big{(}e_{M+1-h}-e_{M+1-i}\big{)}\bigg{|}.\] Recall that \[b_{l}^{\top}\left(e_{M+1-h}-e_{M+1-i}\right) =\langle v_{L+1}^{(h)},x_{l-h}-x_{l-i}\rangle-\langle v_{l}^{(h )},x_{L+1-h}-x_{L+1-i}\rangle,\] \[b(X,Z)^{\top}\left(e_{M+1-h}-e_{M+1-i}\right) =\langle v^{(h)}(X),z_{-h}-z_{-i}\rangle-\langle v^{(h)}(Z),x_{-h }-x_{-i}\rangle.\]We apply the triangular inequality to obtain that

\[\big{|}(g_{h,3}-g_{h,4})^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\big{|}\] \[\qquad\leq\Bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi} \big{[}\widetilde{g}_{h,l}\langle v^{(h)}_{L+1},x_{l-h}\rangle\big{]}-\mathbb{ E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}}\big{[}\widetilde{g}_{h,4}\langle v^{(h) }(Z),x_{-h}\rangle\big{]}\Bigg{|}\] \[\qquad\qquad+\Bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X| \pi}\big{[}\widetilde{g}_{h,l}\langle v^{(h)}_{L+1},x_{l-i}\rangle\big{]}- \mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}}\big{[}\widetilde{g}_{h,4} \langle v^{(h)}(Z),x_{-i}\rangle\big{]}\Bigg{|}\] \[\qquad\qquad+\Bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X| \pi}\big{[}\widetilde{g}_{h,l}\langle v^{(h)}_{l},x_{L+1-i}\rangle\big{]}- \mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}}\big{[}\widetilde{g}_{h,4 }\langle v^{(h)}(X),z_{-h}\rangle\big{]}\Bigg{|}\] \[\qquad\qquad+\Bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X| \pi}\big{[}\widetilde{g}_{h,l}\langle v^{(h)}_{l},x_{L+1-i}\rangle\big{]}- \mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}}\big{[}\widetilde{g}_{h,4 }\langle v^{(h)}(X),z_{-h}\rangle\big{]}\Bigg{|}.\]

Each term on the right-hand side can be bounded by Lemma F.5, where in the lemma we take \((\sigma^{(h)})_{h^{\prime}\in\mathcal{S}^{*}}\in\mathbb{R}^{M\times|\mathcal{ S}^{*}|}\) and \(((\sigma^{(h^{\prime})})_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}},e_{h}) \in\mathbb{R}^{M\times|\mathcal{S}^{*}|}\) as the two lists of vectors on the \(M\)-dimensional probability simplex for \(\widetilde{\sigma}\) and \(\sigma\) respectively. Consequently, we have

\[\big{|}(g_{h,3}-g_{h,4})^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\big{|}\leq \frac{8M}{L\gamma}+\frac{16}{L(1-\lambda)\gamma^{|\mathcal{S}|/2+r_{n}/2+1}}.\]

Combining the above results and setting \(\varepsilon=1/\sqrt{L}\), \(a=a(0)\leq O(1/L^{3/2})\) and together with the conditions in (E.1), we have

\[\big{|}(g_{h,0}-g_{h,4})^{\top}\left(e_{M+1-h}-e_{M+1-i}\right)\big{|}=| \mathcal{E}|=O\bigg{(}\frac{1}{\sqrt{L(1-\lambda)\gamma^{r_{n}+2}}}\bigg{)},\]

where \(O(\cdot)\) hides universal constants independent of the parameters of the model. We remark that while the left hand side is a function of \(t\), the upper bound is independent of \(t\). Then, we can rewrite (E.17) as

\[\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\] \[\quad=a\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sigma^{(h)}_ {-i}\cdot g^{\top}_{h,4}\left(e_{M+1-h}-e_{M+1-i}\right)+(\sigma^{(h)}_{-h}- \sigma^{(h)}_{-i})\cdot\sum_{j=1}^{M}\sigma^{(h)}_{-j}\cdot g^{\top}_{h,4}(e_{M +1-h}-e_{M+1-j})\bigg{]}\] \[\qquad\qquad\qquad\pm a\bigg{(}\sigma^{(h)}_{-i}+(\sigma^{(h)}_{-h }-\sigma^{(h)}_{-i})\sum_{j=1,j\neq h}^{M}\sigma^{(h)}_{-j}\bigg{)}\cdot| \mathcal{E}|.\] (E.18)

Lower Bound for The Difference \(\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\).To show \(\partial_{t}w^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}>0\), we first derive the lower bound of \(\mathbb{E}_{\pi\sim\mathcal{P}}[g^{\top}_{h,4}\left(e_{M+1-h}-e_{M+1-i}\right)]\) for any \(i\neq h\). Since \((x,X)\) and \((z,Z)\) are independent and identically distributed, by the definition of \(b(X,Z)\),

\[\mathbb{E}_{\pi\sim\mathcal{P}}\left[g^{\top}_{h,4}\left(e_{M+1-h }-e_{M+1-i}\right)\right]\] \[\quad=2\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{\pi}} \bigg{[}\sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1 \bigg{)}\prod_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v^{(h^{ \prime})}(Z),v^{(h^{\prime})}(X)\rangle\cdot\langle v^{(h)}(X),z_{-h}\rangle \bigg{]}\] \[\quad\quad-2\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\otimes\mu^{ \pi}}\bigg{[}\sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})} -1\bigg{)}\prod_{h^{\prime}\in\mathcal{S}^{*}\setminus\{h\}}\langle v^{(h^{ \prime})}(Z),v^{(h^{\prime})}(X)\rangle\cdot\langle v^{(h)}(X),z_{-i}\rangle \bigg{]}\] \[\quad=2\tau_{h,1}-2\tau_{h,2},\]where we introduce the following quantities for convenience:

\[\tau_{h,1}:=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\sigma}\otimes\mu^{ \pi}}\bigg{[}\sum_{k=1}^{d}\biggl{(}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k} )}-1\biggr{)}\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}\langle v^{ (h^{\prime})}(Z),v^{(h^{\prime})}(X)\rangle\cdot\langle v^{(h)}(X),z_{-h} \rangle\bigg{]},\] \[\tau_{h,2}:=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\sigma}\otimes\mu^ {\pi}}\bigg{[}\sum_{k=1}^{d}\biggl{(}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{ k})}-1\biggr{)}\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}\langle v^{ (h^{\prime})}(Z),v^{(h^{\prime})}(X)\rangle\cdot\langle v^{(h)}(X),z_{-i} \rangle\bigg{]}.\]

The quantities \(\tau_{h,1}\) and \(\tau_{h,2}\) can be further approximated. Specifically, by applying Lemma F.6 to \(\tau_{h,1}\),, where in the lemma we take \((\sigma^{(h)})_{h^{\prime}\in\mathcal{S}^{\star}}\in\mathbb{R}^{M\times| \mathcal{S}^{\star}|}\) and \(((\sigma^{(h^{\prime})})_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}},e _{h})\in\mathbb{R}^{M\times|\mathcal{S}^{\star}|}\) as the two lists of vectors on the \(M\)-dimensional probability simplex for \(\sigma\) and \(\widehat{\sigma}\) respectively, and we obtain

\[\bigg{|}\tau_{h,1}-\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}( \sigma^{(h^{\prime})}_{-h^{\prime}})^{2}\cdot\sigma^{(h)}_{-h}\cdot\widetilde{ \mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star})\bigg{|}\leq\biggl{(}1-\prod_{h^{ \prime}\in\mathcal{S}^{\star}\setminus\{h\}}(\sigma^{(h^{\prime})}_{-h^{ \prime}})^{2}\cdot\sigma^{(h)}_{-h}\biggr{)}\widetilde{\mathcal{I}}_{\chi^{2} }(\mathcal{S}^{\star}).\] (E.19)

Drawing on the analagous reasoning as in the proof of Lemma F.6, we can approximate \(\tau_{h,2}\) as follows:

\[\bigg{|}\tau_{h,2}-\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}( \sigma^{(h^{\prime})}_{-h^{\prime}})^{2}\cdot\sigma^{(h)}_{-h}\cdot\psi\bigg{|} \leq\biggl{(}1-\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}(\sigma ^{(h^{\prime})}_{-h^{\prime}})^{2}\cdot\sigma^{(h)}_{-h}\biggr{)}\widetilde{ \mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star}),\] (E.20)

where

\[\psi:=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\sigma}\otimes\mu^{\pi}}\bigg{[} \prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}\mathds{1}(x_{-h^{ \prime}}=z_{-h^{\prime}})\cdot\mathds{1}(x_{-h}=z_{-i})\cdot\biggl{(}\sum_{k= 1}^{d}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\biggr{)}\bigg{]}.\]

To establish the lower bound for \(\tau_{h,1}-\tau_{h,2}\), let us begin by establishing an upper bound for \(\psi\), which is approximately equal to \(\tau_{h,2}\). We invoke Lemma F.7 with \(\mathcal{S}=\mathcal{S}^{\star}\) and \(\mathcal{S}^{\prime}=\mathcal{S}^{\star}\setminus\{h\}\cup\{i\}\) in the lemma to obtain

\[\psi\leq\frac{1}{2}\widetilde{\mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star})+ \frac{1}{2}\widetilde{\mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star}\backslash\{h \}\cup\{i\})\leq\widetilde{\mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star})-\frac {1}{2}\cdot\Delta\widetilde{\mathcal{I}}_{\chi^{2}},\quad\forall i\neq h\]

Leveraging this for (E.19) and (E.20),

\[2\tau_{h,1}-2\tau_{h,2} \geq\prod_{h^{\prime}\in\mathcal{S}^{\star}\setminus\{h\}}( \sigma^{(h^{\prime})}_{-h^{\prime}})^{2}\cdot\sigma^{(h)}_{-h}\cdot\Delta \widetilde{\mathcal{I}}_{\chi^{2}}-4\biggl{(}1-\prod_{h^{\prime}\in\mathcal{S} ^{\star}\setminus\{h\}}(\sigma^{(h^{\prime})}_{-h^{\prime}})^{2}\cdot\sigma^{( h)}_{-h}\biggr{)}\widetilde{\mathcal{I}}_{\chi^{2}}(\mathcal{S}^{\star})\] \[\geq\prod_{h\in\mathcal{S}^{\star}}(\sigma^{(h)}_{-h})^{2}\cdot \Delta\widetilde{\mathcal{I}}_{\chi^{2}}-4\biggl{(}1-\prod_{h\in\mathcal{S}^{ \star}}(\sigma^{(h)}_{-h})^{2}\biggr{)}\widetilde{\mathcal{I}}_{\chi^{2}}( \mathcal{S}^{\star}),\] (E.21)

where in the second line we multiply an additional \(\sigma^{(h)}_{-h}\) to the product as \(\sigma^{(h)}_{-h}\in[0,1]\).

Next, we provide a lemma showing that \(\partial_{t}\sigma^{(h)}_{-h}\) is growing for all time \(t\geq t_{1}\), where \(t_{1}\) is the starting time of the second stage.

**Lemma E.2** (Reinforced Growth of \(\sigma^{(h)}_{-h}\)).: _For all \(h\in\mathcal{S}^{\star}\), we have for all \(i\neq h\) at any \(t\geq t_{1}\):_

\[\partial_{t}\sigma^{(h)}_{-h}>0,\quad\text{and}\quad\partial_{t}\log\sigma^{(h) }_{-h}-\partial_{t}\log\sigma^{(h)}_{-i}=\partial_{t}w^{(h)}_{-h}-\partial_{t}w^ {(h)}_{-i}>0.\] (E.22)

Proof.: See SSE.3.1 for the proof. 

In the proof of Lemma E.2, we will use the following useful proposition.

**Proposition E.3**.: _Suppose \(\prod_{h\in\mathcal{S}^{\star}}(\sigma^{(h)}_{-h})^{2}\geq 1/(1+(M-1)\exp(-\Delta w))^{2| \mathcal{S}^{\star}|}\) with \(\Delta w\) satisfying (3.6), and \(\sigma^{(h)}_{-h}>\sigma^{(h)}_{-i}\) for any \(i\neq h,h\in\mathcal{S}^{\star}\) at a given time \(t\). Suppose Assumption 3.5 holds and \(L\) satisfies (E.1). It holds that_

\[\partial_{t}\log\sigma^{(h)}_{-h}-\partial_{t}\log\sigma^{(h)}_{-i}=\partial_{t}w ^{(h)}_{-h}-\partial_{t}w^{(h)}_{-i}\geq\frac{a\Delta\widetilde{\mathcal{I}}_{ \chi^{2}}}{2}\biggl{(}\sigma^{(h)}_{-i}+(\sigma^{(h)}_{-h}-\sigma^{(h)}_{-i}) \sum_{j=1,j\neq h}^{M}\sigma^{(h)}_{-j}\biggr{)}>0,\]

\[\partial_{t}\sigma^{(h)}_{-h}>0,\qquad\forall i\neq h,\quad\forall h\in\mathcal{S} ^{\star}.\] (E.23)Proof.: See SSE.3.1 for the proof. 

Lemma E.2 implies that during Stage II, for all \(i\neq h\) and \(h\in\mathcal{S}^{\star}\), we have \(w_{-h}^{(h)}>w_{-i}^{(h)}\) and \(\sigma_{-h}^{(h)}>\sigma_{-i}^{(h)}\) for all \(t\geq t_{1}\). In addition, as \(\sigma_{-h}^{(h)}\) is growing, all the conditions in Proposition E.3 are satisfied for any \(t\geq t_{1}\), and hence all the conclusions in (E.23).

Convergence of \(\sigma^{(h)}\).Finally, we characterize the convergence rate of \(\sigma^{(h)}\). For the convergence analysis, we adhere to the convention used in the previous stage, treating all model parameters as functions of the training time \(t\), where \(t=t_{1}\) marks the start of the second stage. With a slight abuse of notation, we denote by \(\sigma_{-i}^{(h)}(t)\) the value of \(\sigma_{-i}(w^{(h)}(t))\) at time \(t\), where \(w^{(h)}(t)\) is the input to the softmax function, and \(\sigma_{-i}(\cdot)\) refers to the \((M+1-i)\)-th element of the softmax probability. For simplicity, we sometimes omit the time index \(t\) when the context makes it clear.

Note that \(\partial_{t}\sigma_{-h}^{(h)}>0\) for all \(h\in\mathcal{S}^{\star}\). Hence by the definition of the softmax operation, we have

\[\sigma_{-i}^{(h)} =\sigma_{-h}^{(h)}\cdot\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)}))\geq \sigma_{-h}^{(h)}(t_{1})\cdot\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})\] \[=\sigma_{-h}^{(h)}(0)\cdot\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})),\] (E.24)

where the first inequality follows from the monotone growth of \(\sigma_{h}^{(h)}\), and the second line follows from the fact that the first attention layer is untouched during the first stage. Note that here in (E.24), \(\sigma^{(h)}\) and \(w^{(h)}\) are functions of \(t\). Now, putting together (E.23) and (E.24), and also noting that \(\sigma_{-h}^{(h)}>\sigma_{-i}^{(h)}\) for all \(i\neq h\) and \(h\in\mathcal{S}^{\star}\), it follows that

\[\partial_{t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}\geq\frac{a\Delta\widetilde{ I}_{\chi^{2}}}{2}\sigma_{-i}^{(h)}\geq\frac{a\Delta\widetilde{I}_{\chi^{2}}}{2} \cdot\sigma_{-h}^{(h)}(0)\cdot\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})).\]

Rearranging the terms, and using the fact that \(w_{-h}^{(h)}(t_{1})-w_{-i}^{(h)}(t_{1})\geq\Delta w\) by Assumption 3.3, we get

\[\exp\left(w_{-h}^{(h)}(t)-w_{-i}^{(h)}(t)\right)\geq\frac{a\Delta\widetilde{ I}_{\chi^{2}}\cdot\sigma_{-h}^{(h)}(0)}{2}\cdot(t-t_{1})+\exp(\Delta w).\]

This yields a lower bound for \(\sigma_{-h}^{(h)}(t)\) as follows:

\[\sigma_{-h}^{(h)}(t)=\frac{1}{1+\sum_{i\neq h}\exp(w_{-i}^{(h)}(t)-w_{-h}^{(h) }(t))}\geq\frac{1}{1+(M-1)\cdot(a\Delta\widetilde{I}_{\chi^{2}}\cdot\sigma_{ \min}(0)\cdot(t-t_{1})/2+\exp(\Delta w))^{-1}},\]

where we define \(\sigma_{\min}(0):=\min_{h\in\mathcal{S}^{\star}}\sigma_{-h}^{(h)}(0)\). Consequently, we have

\[1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)}(t))^{2} \leq 1-\left(\frac{1}{1+(M-1)\cdot(a\Delta\widetilde{I}_{\chi^{2}} \cdot\sigma_{\min}(0)\cdot(t-t_{1})/2+\exp(\Delta w))^{-1}}\right)^{2| \mathcal{S}^{\star}|}\] \[=1-\left(1-\frac{(M-1)}{(a\Delta\widetilde{I}_{\chi^{2}}\cdot \sigma_{\min}(0)\cdot(t-t_{1})/2+\exp(\Delta w))+(M-1)}\right)^{2|\mathcal{S}^ {\star}|}.\]

Now, we consider large \(t\) such that

\[\frac{(M-1)}{(a\Delta\widetilde{I}_{\chi^{2}}\cdot\sigma_{\min}(0)\cdot(t-t_{ 1})/2+\exp(\Delta w))+(M-1)}<\frac{1}{2|\mathcal{S}^{\star}|}.\]

Then, we can apply the inequality \((1-x)^{n}\geq 1-nx\) for \(x\in[0,1/n]\) and \(n\geq 1\) to obtain

\[1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)}(t))^{2}\leq\frac{2| \mathcal{S}^{\star}|\cdot(M-1)}{a\Delta\widetilde{I}_{\chi^{2}}\cdot\sigma_{ \min}(0)\cdot(t-t_{1})/2+\exp(\Delta w)+(M-1)}.\]

Therefore, with training time \(t_{2}=4L|\mathcal{S}^{\star}|\cdot(M-1)/a\Delta\widetilde{I}_{\chi^{2}}\cdot \sigma_{\min}(0)+t_{1}\), we can ensure that

\[1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)}(t_{2}))^{2}\leq L^{-1}.\]

This completes the proof for Stage II.

#### e.3.1 Additional Proofs for Stage II

We conclude this subsection with the proof of Lemma E.2 and Proposition E.3.

Proof of Proposition E.3.: The condition \(\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)})^{2}\geq 1/(1+(M-1)\exp(-\Delta w))^{2 |\mathcal{S}^{\star}|}\) with \(\Delta w\) in (3.6) implies that

\[\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)})^{2}\geq\left(1+\frac{ \Delta\widetilde{I}_{\chi^{2}}}{14\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star })}\right)^{-1}\geq\frac{4\widetilde{I}_{\chi^{2}}(\mathcal{S}^{\star})+\frac{ 2}{3}\Delta\widetilde{I}_{\chi^{2}}}{4\widetilde{I}_{\chi^{2}}(\mathcal{S}^{ \star})+\Delta\widetilde{I}_{\chi^{2}}}.\] (E.25)

Combining (E.21) and (E.25) yields

\[\mathbb{E}_{\pi\sim\mathcal{P}}\left[g_{h,4}^{\top}\left(e_{M+1-h}-e_{M+1-i} \right)\right]=2\tau_{h,1}-2\tau_{h,2}\geq\frac{2}{3}\Delta\widetilde{I}_{ \chi^{2}}\] (E.26)

for any \(i\neq h\). Applying (E.26) to (E.18), since each \(\sigma_{-i}^{(h)}>0\) and \(\sigma_{-h}^{(h)}>\sigma_{-i}^{(h)}\) at time \(t\) for all \(i\neq h,h\in\mathcal{S}^{\star}\), it holds that

\[\partial_{t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}\geq a\bigg{(}\sigma_{-i}^{( h)}+(\sigma_{-h}^{(h)}-\sigma_{-i}^{(h)})\cdot\sum_{j=1,j\neq h}^{M}\sigma_{-j}^{(h) }\bigg{)}\cdot\bigg{(}\frac{2}{3}\Delta\widetilde{I}_{\chi^{2}}-|\mathcal{E} |\bigg{)}.\]

Then since we assume a sufficiently large \(L\geq\Omega((\Delta\widetilde{I}_{\chi^{2}}^{2}(1-\lambda)\gamma^{\tau_{n}+2} )^{-1})\), it holds that \(|\mathcal{E}|\leq\Delta\widetilde{I}_{\chi^{2}}/6\), we further have

\[\partial_{t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}\geq\frac{a\Delta\widetilde{ I}_{\chi^{2}}}{2}\bigg{(}\sigma_{-i}^{(h)}+(\sigma_{-h}^{(h)}-\sigma_{-i}^{(h)}) \sum_{j=1,j\neq h}^{M}\sigma_{-j}^{(h)}\bigg{)}>0,\quad\forall i\neq h,\quad \forall h\in\mathcal{S}^{\star}.\]

As \(\partial_{t}\log\sigma_{-h}^{(h)}-\partial_{t}\log\sigma_{-i}^{(h)}=\partial_ {t}w_{-h}^{(h)}-\partial_{t}w_{-i}^{(h)}>0\) by property of the softmax function, and \(\sum_{i=1}^{M}\partial_{t}\sigma_{-i}^{(h)}=0\), we have \(\partial_{t}\sigma_{-h}^{(h)}>0\) for all \(h\in\mathcal{S}^{\star}\). This completes the proof of Proposition E.3. 

Proof of Lemma e.2.: We give a proof to Lemma E.2 by contradiction. Note that at the beginning of the second stage \(t=t_{1}\), we have all the conditions for Proposition E.3 satisfied by the initialization conditions in Assumption 3.3. Then, by (E.23) in Proposition E.3, we have \(\partial_{t}\log\sigma_{-h}^{(h)}-\partial_{t}\log\sigma_{-i}^{(h)}>0\) and \(\partial_{t}\sigma_{-h}^{(h)}>0\) for all \(i\neq h\) and \(h\in\mathcal{S}^{\star}\) at \(t=t_{1}\).

Next, assume that \(\tau>t_{1}\) is the smallest time such that at least \(\partial_{t}\sigma_{-h}^{(h)}\leq 0\) or \(\partial_{t}\log\sigma_{-h}^{(h)}-\partial_{t}\log\sigma_{-i}^{(h)}\leq 0\) for some \(i\neq h\) and \(h\in\mathcal{S}^{\star}\). By definition of \(\tau\), we have (E.22) holds for any moment \(t\in[t_{1},\tau)\). As \(\sigma_{-h}^{(h)}\) and the gap \(\sigma_{-h}^{(h)}-\sigma_{-i}^{(h)}\) are monotonically increasing, we have by the initialization condition and the boundedness of the gradient that at time \(\tau\):

\[\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)})^{2}\geq 1/(1+(M-1)\exp(-\Delta w))^{2 |\mathcal{S}^{\star}|},\quad\text{and}\quad\sigma_{-h}^{(h)}>\sigma_{-i}^{(h)}, \quad\forall i\neq h,\quad\forall h\in\mathcal{S}^{\star}.\]

Hence, by Proposition E.3, we have \(\partial_{t}\log\sigma_{-h}^{(h)}-\partial_{t}\log\sigma_{-i}^{(h)}>0\) and \(\partial_{t}\sigma_{-h}^{(h)}>0\) for all \(i\neq h\) and \(h\in\mathcal{S}^{\star}\) at time \(\tau\), which contradicts the definition of \(\tau\). This completes the proof of Lemma E.2. 

### Analysis for Stage III

In this section, we derive the dynamics of the second attention layer's weights \(a\) in Stage III. We characterize the dynamics of \(a\) when \(a<O(\log L)\), where the signal term of the dynamics dominates the approximation error. We provide the growth rate of the weights for two regimes: when \(a\) is either sufficiently small or large.

[MISSING_PAGE_FAIL:46]

In summary, the difference between \(f_{0}\) and \(f_{1}\) is controlled by the convergence results from Stage I.
* Our second step is to characterize the approximation error incurred by the difference between the ideal attention scores and the actual attention scores in the second attention layer. Let us define \(s_{l}^{\star}=\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\) as the ideal attention score for the second attention layer. We invoke Lemma F.1 to have for all \(l\in[L]\), \[|s_{l}-s_{l}^{\star}|\leq\Delta_{1}+\Delta_{2},\quad\text{where}\quad\Delta_{ 2}\operatorname{:=}1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)}(t_{2}) )^{2}.\] Corresponding to \(\{s_{l}^{\star}\}_{l=M+1}^{L}\), we define \[\sigma_{l}^{\star}\operatorname{:=}\frac{\exp\big{(}a\prod_{h\in\mathcal{S}^{ \star}}\mathds{1}(x_{l-h}=x_{L+1-h})\big{)}}{\sum_{l^{\prime}=M+1}^{L}\exp \big{(}a\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l^{\prime}-h}=x_{L+1-h}) \big{)}},\quad y^{\star}(k)\operatorname{:=}\sum_{l=M+1}^{L}\sigma_{l}^{\star }\mathds{1}(x_{l}=e_{k}),\quad\forall k\in[d].\] In the vector form, we have \(y^{\star}=\sum_{l=M+1}^{L}\sigma_{l}^{\star}x_{l}\). Leveraging the above approximations, we define an approximation of \(f_{1}\) as \[f_{2}\operatorname{:=}\operatorname{\mathbb{E}}\bigg{[}\sum_{l=M+1}^{L}\sigma _{l}^{\star}\sum_{k=1}^{d}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{ \star}(k)+\varepsilon}-\frac{y^{\star}(k)\mathds{1}(x_{L+1}=e_{k})}{y^{\star} (k)+\varepsilon}\biggr{)}\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{ L+1-h})\bigg{]}.\] Applying Lemma F.9, it holds that \[|f_{1}-f_{2}|\leq 12\cdot(1+a(t)\cdot\varepsilon^{-1})\cdot(\Delta_{1}+ \Delta_{2})\] In summary, this error terms captures the difference between the ideal weights and the actual weights obtained by gradient flow at the end of Stage II.
* Note that \(y^{\star}(k)\) is also random due to the randomness in \(\sigma_{l}^{\star}\), and as \(L\) is sufficiently large, we want to replace \(y^{\star}(k)\) with its population counterpart. Let \(z\in\mathcal{X}\) and \(Z=(z_{-M},\ldots,z_{-1})\in\mathcal{X}^{M}\) be two random variables and we define similarly for \(x\in\mathcal{X}\) and \(X=(x_{-M},\ldots,x_{-1})\in\mathcal{X}^{M}\). To this end, we define a reweighted distribution \[\widetilde{\mu}^{\pi}(z,Z\,|\,X_{-\mathcal{S}^{\star}})=\frac{\mu^{\pi}(z,Z) \exp\big{(}a\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(z_{-h}=x_{-h})\big{)}}{ \sum_{z^{\prime},Z^{\prime}}\mu^{\pi}(z^{\prime},Z^{\prime})\exp\big{(}a\prod_ {h\in\mathcal{S}^{\star}}\mathds{1}(z^{\prime}_{-h}=x_{-h})\big{)}},\] (E.27) where \(\mu^{\pi}\) is the stationary distribution of the Markov chain over a window of size \(M+1\). This can be viewed as a reweighting of the stationary distribution over \((z,Z)\) by an exponential term that depends on the sequence \(X_{-\mathcal{S}^{\star}}\). We use \(\widetilde{\mu}^{\pi}(z=e_{k}\,|\,X_{L+1-\mathcal{S}^{\star}})\) to replace \(y^{\star}(k)\) and define \(f_{3}\) as \[f_{3}\operatorname{:=}\operatorname{\mathbb{E}}\bigg{[}\sum_{l=M+1}^{L}\sigma _{l}^{\star}\sum_{k=1}^{d}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{ \widetilde{\mu}^{\pi}(z=e_{k}\,|\,X_{L+1-\mathcal{S}^{\star}})}-\mathds{1}(x_{ L+1}=e_{k})\biggr{)}\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h}) \bigg{]}.\] One can immediately draw a connection to Lemma F.4 as both targets characterize the gap between the empirical and population distributions. The only difference is that this time we have the distribution reweighed by some exponential term. For completeness, we provide the approximation result in Lemma F.10, which bounds the difference between \(f_{2}\) and \(f_{3}\) as \[|f_{2}-f_{3}|\leq\frac{8(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+ 1)^{1/4}+8\sqrt{M}}{L^{1/2}\cdot\gamma|\mathcal{S}^{\star}|+1}+\frac{2d \varepsilon}{\gamma}\lesssim\frac{\sqrt{M}+d}{L^{1/2}(1-\lambda)^{1/2}\gamma| \mathcal{S}^{\star}|+1+r_{n}/4}.\] where \(\mu_{0}(\cdot)\) is the initial distribution for the first \(r_{n}\) tokens in the Markov chain. Here and in the sequel, we simply use \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) to denote \(D_{\chi^{2}}(\mu_{0}(X_{1:r_{n}}=\cdot)\,\|\,\mu^{\pi}(X_{1:r_{n}}=\cdot))\) when it is clear from the context. In the last inequality, we use the fact that \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\leq\gamma^{-r_{n}}\) by (E.6) and the condition \(\varepsilon=L^{-1/2}\).
* Note that in the expression of \(f_{3}\), each \(\sigma_{l}^{\star}\) still implicitly depends on the actual value of the sequence \(X\). Since \(L\) is large and the Markov chain is well-mixed, we can approximate 

[MISSING_PAGE_EMPTY:48]

where the last line holds by noting that with sufficiently large \(t_{1}\) and \(t_{2}\) we have \(\Delta_{1}+\Delta_{2}\leq L^{-1}\), and \(\varepsilon=L^{-1/2}\). Here, express the error in terms of the trainable parameter \(a\) and define

\[\xi(a)\asymp\frac{\sqrt{M}+d}{L^{1/2}(1-\lambda)^{1/2}\gamma|^{\mathcal{S}^{ \star}}|+2+r_{n}/4}+a\cdot L^{-1/2}.\]

In particular, we have for \(a=O(\log L)\) that

\[\xi(a)=O\bigg{(}\frac{\sqrt{M}+d}{L^{1/2}(1-\lambda)^{1/2}\gamma|^{\mathcal{S }^{\star}}|+2+r_{n}/4}+\frac{\log L}{L^{1/2}}\bigg{)}.\] (E.29)

In a nutshell, we conclude that when the weight \(a\) satisfies \(a<O(\log L)\), the dynamics of \(a\) can be approximated by

\[\partial_{t}a=f_{5}\pm\xi(a).\] (E.30)

The following proposition helps us reformulate \(f_{5}\) in a form that facilitates the analysis of the dynamics of \(a\).

**Proposition E.4**.: _The term \(f_{5}\) can be reformulated as_

\[f_{5}=\mathbb{E}_{\pi,X_{-\mathcal{S}^{\star}}\sim\mu^{\pi}}\left[J(X_{- \mathcal{S}^{\star}};a,\pi)\cdot e^{a}\cdot\left(r^{\pi}(X_{-\mathcal{S}^{ \star}})\right)^{3}\cdot\mu^{\pi}(X_{-\mathcal{S}^{\star}})\right],\]

_where \(r^{\pi}(X_{-\mathcal{S}^{\star}};a)=(1+\mu^{\pi}(X_{-\mathcal{S}^{\star}}) \cdot(e^{a}-1))^{-1}\) is the inverse of the normalization factor of \(\widetilde{\mu}^{\pi}\) in (E.27) and_

\[J(X_{-\mathcal{S}^{\star}};a,\pi)=\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X _{-\mathcal{S}^{\star}})-\mu^{\pi}(x=e_{k}))^{2}}{(1-r^{\pi}(X_{-\mathcal{S}^ {\star}};a))\cdot\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{\star}})+r^{\pi}(X_{- \mathcal{S}^{\star}};a)\cdot\mu^{\pi}(x=e_{k})}.\]

Proof.: See SSE.4.1 for the proof. 

Inspired by this form, we define an alternative function \(\tilde{\mathcal{J}}(\cdot;r,\pi)\) as

\[\tilde{\mathcal{J}}(X_{-\mathcal{S}^{\star}};r,\pi)\!:=\!\sum_{k\in[d]}\frac{( \mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{\star}})-\mu^{\pi}(x=e_{k}))^{2}}{(1-r )\cdot\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{\star}})+r\cdot\mu^{\pi}(x=e_{k} )},\quad r\in[0,1]\] (E.31)

where we replace \(r^{\pi}(X_{-\mathcal{S}^{\star}};a)\) by a parameter \(r\in[0,1]\). As exactly calculating the inverse normalization factor \(r^{\pi}(X_{-\mathcal{S}^{\star}};a)\) is intractable, we instead seek to find an upper and lower bound for \(r^{\pi}(X_{-\mathcal{S}^{\star}};a)\) and plug them into \(\tilde{J}(\cdot;r,\pi)\) to bound \(f_{5}\) Suppose that \(r^{\pi}(X_{-\mathcal{S}^{\star}};a)\) enjoys the following parameter-dependent upper and lower bounds:

\[r_{-}(a)\leq r^{\pi}(X_{-\mathcal{S}^{\star}};a)\leq r_{+}(a),\quad\forall X_{ -\mathcal{S}^{\star}}\in\mathcal{X}^{|\mathcal{S}^{\star}|},\quad\forall\pi \in\operatorname{supp}(\mathcal{P}).\]

Thus, an upper and lower bound to \(J(X_{-\mathcal{S}^{\star}};a,\pi)\) can be given by

\[\inf_{r\in[r_{-}(a),r_{+}(a)]}\widetilde{\mathcal{J}}(X_{-\mathcal{S}^{\star}} ;r,\pi)\leq J(X_{-\mathcal{S}^{\star}};a,\pi)\leq\sup_{r\in[r_{-}(a),r_{+}(a)] }\widetilde{J}(X_{-\mathcal{S}^{\star}};r,\pi).\]

In order to effectively tackle these bounds, we then study the properties of \(\widetilde{J}(\cdot;r,\pi)\) next.

**Proposition E.5**.: _Define_

\[D_{+}(X_{-\mathcal{S}^{\star}},\pi)=\max\big{\{}D_{\chi^{2}}(\mu^{\pi}(\cdot )\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{\star}})),D_{\chi^{2}}(\mu^{\pi}( \cdot\,|\,X_{-\mathcal{S}^{\star}})\,\|\,\mu^{\pi}(\cdot))\big{\}}\,.\]

_The function \(\widetilde{J}(X_{-\mathcal{S}^{\star}};r,\pi)\) with \(r\in[0,1]\) defined in (E.31) satisfies the following properties:_

1. \(\widetilde{J}(X_{-\mathcal{S}^{\star}};r,\pi)\) _is convex in_ \(r\)_._
2. \(\widetilde{J}(X_{-\mathcal{S}^{\star}};r,\pi)\leq D_{+}(X_{-\mathcal{S}^{\star} },\pi)\)_._
3. \(\widetilde{J}(X_{-\mathcal{S}^{\star}};r,\pi)\) _is Lipschitz continuous in_ \(r\) _with Lipschitz constant_ \(\gamma^{-1}D_{+}(X_{-\mathcal{S}^{\star}},\pi)\)_._

Proof.: See SSE.4.1 for the proof.

Upper and Lower Bounding \(J(X_{-\mathcal{S}^{*}};a,\pi)\).Previously, we show via a reformulation of \(f_{5}\) that it suffices to bound \(J(X_{-\mathcal{S}^{*}};a,\pi)\). In the sequel, we let

\[D_{+}(X_{-\mathcal{S}^{*}},\pi) =\max\left\{D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,| \,X_{-\mathcal{S}^{*}})),D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}} )\,\|\,\mu^{\pi}(\cdot))\right\},\] \[\rho =\max\left\{\max_{X_{-\mathcal{S}^{*}},\pi}\frac{D_{+}(X_{- \mathcal{S}^{*}},\pi)}{D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\, X_{-\mathcal{S}^{*}}))},\max_{X_{-\mathcal{S}^{*}},\pi}\frac{D_{+}(X_{-\mathcal{S}^{*}},\pi)}{D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}( \cdot))}\right\}.\]

It can be noticed that

\[\rho \leq\max\left\{\max_{X_{-\mathcal{S}^{*}},\pi}\frac{D_{\chi^{2}} (\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))}{\chi^{2}( \mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))},\max_{X_{- \mathcal{S}^{*}},\pi}\frac{D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{* }})\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))}{D_{\chi^{2}}(\mu^{\pi}( \cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))}\right\}\] \[\leq\max\left\{\max_{X_{-\mathcal{S}^{*}},\pi}\frac{\mu^{\pi}( \cdot)}{\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})},\max_{X_{-\mathcal{S}^{*}},\pi}\frac{\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})}{\mu^{\pi}(\cdot)} \right\}\leq\gamma^{-1},\]

where the second inequality follows from noting that the \(\chi^{2}\)-divergence defined as \(D_{\chi^{2}}(\mu\,\|\,\nu)=\sum_{x}{(\mu(x)-\nu(x))^{2}}/{\nu(x)}\), and \(D_{\chi^{2}}(\mu\,\|\,\nu)/D_{\chi^{2}}(\nu\,\|\,\mu)\leq\sup_{x}{\mu(x)}/{ \nu(x)}\).

Apparently, \(r^{\pi}(X_{-\mathcal{S}^{*}};a)\) is a function of \(a\) and enjoys the following parameter-dependent upper and lower bounds:

\[r_{+}(a) =(1+\min_{X_{-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}} )(e^{a}-1))^{-1},\] \[r_{-}(a) =(1+\max_{X_{-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*} })(e^{a}-1))^{-1}.\]

If \(a\) is small, we see that both \(r_{+}(a)\) and \(r_{-}(a)\) are close to \(1\), and we directly have

\[r_{-}(a)\leq r^{\pi}(X_{-\mathcal{S}^{*}};a)\leq 1,\quad\text{where}\quad 1- \max_{X_{-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)\leq r_ {-}(a)<1.\]

This suggests an upper bound of \(J(X_{-\mathcal{S}^{*}};a,\pi)\) as

\[J(X_{-\mathcal{S}^{*}};a,\pi) \leq\sup_{r\in[r_{-}(a),1]}\widetilde{J}(X_{-\mathcal{S}^{*}};r, \pi)\leq\widetilde{J}(X_{-\mathcal{S}^{*}};1,\pi)+\gamma^{-1}\cdot D_{+}(X_{- \mathcal{S}^{*}},\pi)\cdot(1-r_{-}(a))\] \[\leq D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\, \mu^{\pi}(\cdot))+\gamma^{-1}\cdot D_{+}(X_{-\mathcal{S}^{*}},\pi)\cdot\max_{X_ {-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\] \[\leq D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\, \mu^{\pi}(\cdot))\cdot\left(1+\gamma^{-2}\cdot\max_{X_{-\mathcal{S}^{*}},\pi} \mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\right),\]

where the second line follows from the Lipschitz continuity property, and the last line holds because the ratio \(D_{+}(X_{-\mathcal{S}^{*}},\pi)/D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S }^{*}})\,\|\,\mu^{\pi}(\cdot))\) is upper bounded by \(\rho\), and further by \(\gamma^{-1}\). A similar lower bound can be obtained by changing the sign of \(\gamma^{-2}\cdot\max_{X_{-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}}) \cdot(e^{a}-1)\). Hence, we h

\[J(X_{-\mathcal{S}^{*}};a,\pi)=D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^ {*}})\,\|\,\mu^{\pi}(\cdot))\cdot\left(1\pm\gamma^{-2}\cdot\max_{X_{-\mathcal{S}^ {*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\right).\] (E.32)

On the other hand, when \(a\) becomes large, we have both \(r_{+}(a)\) and \(r_{-}(a)\) close to \(0\), and we have

\[0\leq r^{\pi}(X_{-\mathcal{S}^{*}};a)\leq r_{+}(a),\quad\text{where}\quad 0<r_{+}(a) \leq\frac{1}{\min_{X_{-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1 )}.\]

In a similar fashion, we have the following upper bound:

\[J(X_{-\mathcal{S}^{*}};a,\pi) \leq\sup_{r\in[0,r_{+}(a)]}\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi )\leq\widetilde{J}(X_{-\mathcal{S}^{*}};0,\pi)+\gamma^{-1}\cdot D_{+}(X_{- \mathcal{S}^{*}},\pi)\cdot r_{+}(a)\] \[=D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{- \mathcal{S}^{*}}))+\gamma^{-1}\cdot\frac{D_{+}(X_{-\mathcal{S}^{*}},\pi)}{\min_{X_ {-\mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)}\] \[\leq D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{- \mathcal{S}^{*}}))\cdot\left(1+\frac{\gamma^{-2}}{\min_{X_{-\mathcal{S}^{*}},\pi} \mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)}\right).\]

We can similarly obtain a lower bound by changing the sign of the second term inside the bracket. Hence, we have

\[J(X_{-\mathcal{S}^{*}};a,\pi)=D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot \,|\,X_{-\mathcal{S}^{*}}))\cdot\left(1\pm\frac{\gamma^{-2}}{\min_{X_{- \mathcal{S}^{*}},\pi}\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)}\right).\] (E.33)Divergence of \(a\).Recall that we have shown the dynamics of \(a\) in (E.30), where \(\xi(a)\) is negligible when \(L\) goes to infinity. Thus, when \(L\) is sufficiently large, we see by the nonnegativity of \(f_{5}\) that \(a(t)\) continues to increase as \(t\) increases until it reaches a point where \(f_{5}\) no longer dominates the approximation error. To characterize the regime where \(f_{5}\geq\xi(a)\), we first note that for \(a\leq\log L\) it holds by (E.29) that

\[\xi(a)=O(L^{-1/2}\log L)\approx L^{-1/2},\]

where \(\approx\) hides logarithmic factors. For \(f_{5}\), we recall from Proposition E.4 that

\[f_{5}=\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\bigg{[} \frac{J(X_{-\mathcal{S}^{*}})\cdot e^{a}}{\big{(}1+\mu^{\pi}(X_{-\mathcal{S}^{* }})\cdot(e^{a}-1)\big{)}^{3}}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})\bigg{]},\]

where for small \(a\) we have \(f_{5}=\Omega(1)\) and for large \(a\) we have \(f_{5}=\Omega(e^{-2a})\). Thus, \(e^{-2a}\geq L^{-1/2}\) gives the condition for \(f_{5}\) to dominate the approximation error, which gives \(a=O(\log L)\). In the sequel, we consider the dynamics for \(a\leq(\log L)/8\) and give a more rigorous analysis.

We use the notation \(x=o(1)\) to denote that a term is much smaller than \(1\), for example, \((\log\log L)^{-1}=o(1)\). For any \(x_{0}\) and \(\delta\), we write \(x=x_{0}\pm\delta\) to indicate that \(x\) is bounded within \([x_{0}-\delta,x_{0}+\delta]\). In the following, we assume there exists \(\delta\) satisfying \(\delta\leq\gamma^{2}/4\wedge 1/8\) and

\[\delta\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{- \mathcal{S}^{*}}}D_{\chi^{2}}\big{(}\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\, \|\,\mu^{\pi}(\cdot)\big{)}\cdot\big{(}\mu^{\pi}(X_{-\mathcal{S}^{*}})\big{)} ^{2}\bigg{]} \geq\xi(\log L),\] \[\delta\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{- \mathcal{S}^{*}}}\frac{D_{\chi^{2}}\big{(}\mu^{\pi}(\cdot\,|\,\mu^{\pi}(\cdot \,|\,X_{-\mathcal{S}^{*}})\big{)}\cdot L^{-1/4}\big{)}}{\mu^{\pi}(X_{- \mathcal{S}^{*}})}\bigg{]} \geq\xi(\log L).\]

Note that

\[\xi(\log L)\leq O\bigg{(}\frac{\sqrt{M}+d}{L^{1/2}(1-\lambda)^{1/2}\gamma| \mathcal{S}^{*}|+2+r_{n}/4}+\frac{\log L}{L^{1/2}}\bigg{)}.\]

By additionally noting that \(\mu^{\pi}(X_{-\mathcal{S}^{*}})\geq\gamma^{|\mathcal{S}^{*}|}\) thanks to the lower bound of the transition probability, we are able to find such a \(\delta\) if we have

\[\frac{L}{(\log L)^{4}}\geq\Omega\bigg{(}\frac{1}{\kappa^{4}\gamma^{8+2| \mathcal{S}^{*}|}}\cdot\Big{(}\frac{\sqrt{M}+d}{(1-\lambda)^{1/2}\gamma| \mathcal{S}^{*}|+2+r_{n}/4}\Big{)}^{4}\bigg{)},\]

where \(\kappa\) is defined as

\[\kappa\,{:=}\,\mathbb{E}\left[D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\, \mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))\right]\wedge\mathbb{E}\left[D_{ \chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot)) \right]\wedge 1,\]

and \(\Omega(\cdot)\) only hides universal constants. Note that this is already guaranteed by the condition on \(L\) in (E.2). In particular, we can just take \(\delta=\gamma^{2}/4\wedge 1/8\) in the following analysis.

Small \(a\).Consider the case where \(a\) is small in the sense that \(\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\leq\delta\) for any \(X_{-\mathcal{S}^{*}}\) and \(\pi\in\operatorname{supp}(\mathcal{P})\). In fact, one can directly deduce from our previous results that \(1-\delta\leq r_{-}(a)\leq r^{\pi}(X_{-\mathcal{S}^{*}};a)<1\) and

\[1-3\delta\leq(r^{\pi}(X_{-\mathcal{S}^{*}};a))^{3}\leq 1.\]

For \(J(X_{-\mathcal{S}^{*}};a,\pi)\), we combine the condition that \(\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\leq\delta\) with (E.32) to obtain that

\[J(X_{-\mathcal{S}^{*}};a,\pi)=\big{(}1\pm\gamma^{-2}\delta\big{)} \cdot D_{\chi^{2}}\big{(}\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{ \pi}(\cdot)\big{)},\quad\text{where}\quad\gamma^{-2}\delta\leq 1/4.\]

Combining the above two results with Proposition E.4, we have

\[f_{5} =\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\left[J(X_{- \mathcal{S}^{*}};a,\pi)\cdot e^{a}\cdot\big{(}r^{\pi}(X_{-\mathcal{S}^{*}}) \big{)}^{3}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})\right]\] \[=\big{(}1\pm(\gamma^{-2}+3)\delta\big{)}\cdot\mathbb{E}_{\pi\sim \mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}}\big{(}\mu^{\pi}( \cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot)\big{)}\cdot\mu^{\pi}(X_{ -\mathcal{S}^{*}})^{2}\bigg{]}\cdot e^{a}.\]Also, the noise term \(\xi+\psi(a)\) is upper bounded by

\[\xi+\psi(\log L) \leq\delta\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{- \mathcal{S}^{*}}}D_{\chi^{2}}\big{(}\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\, \|\,\mu^{\pi}(\cdot)\big{)}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})^{2}\bigg{]}\] \[\leq\delta\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{- \mathcal{S}^{*}}}D_{\chi^{2}}\big{(}\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\, \|\,\mu^{\pi}(\cdot)\big{)}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})^{2}\bigg{]} \cdot e^{a}\]

by the construction of \(\delta\). Combining all the above results, we have the dynamics of \(a\) as

\[\partial_{t}a=\big{(}1\pm(\gamma^{-2}+4)\delta\big{)}\cdot\mathbb{E}_{\pi\sim \mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}}(\mu^{\pi}(\cdot\,| \,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))\cdot\mu^{\pi}(X_{-\mathcal{S}^ {*}})^{2}\bigg{]}\cdot e^{a}.\]

A simple reformulation gives

\[-\partial_{t}e^{-a}=\big{(}1\pm(\gamma^{-2}+4)\delta\big{)}\cdot\mathbb{E}_{ \pi\sim\mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}}(\mu^{\pi}( \cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))\cdot\mu^{\pi}(X_{- \mathcal{S}^{*}})^{2}\bigg{]},\]

which implies that for small \(a\), the growth follows

\[a(t)\leq-\log\bigg{(}e^{-a(0)}-(1+(\gamma^{-2}+4)\delta)\cdot \mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}} (\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))\cdot\mu^{ \pi}(X_{-\mathcal{S}^{*}})^{2}\bigg{]}\cdot t\bigg{)},\] \[a(t)\geq-\log\bigg{(}e^{-a(0)}-(1-(\gamma^{-2}+4)\delta)\cdot \mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}} (\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))\mu^{\pi}(X_{ -\mathcal{S}^{*}})^{2}\bigg{]}\cdot t\bigg{)}.\]

Therefore, in the beginning, \(a(t)\) grows super exponentially fast.

Large \(a\).As \(a\) grows large such that \(\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)\geq\delta^{-1}\) for all \(X_{-\mathcal{S}^{*}}\) and \(\pi\in\operatorname{supp}(\mathcal{P})\), we conclude that \(0<r^{\pi}(X_{-\mathcal{S}^{*}};a)\leq r_{+}(a)\leq\delta\) and

\[\frac{r^{\pi}(X_{-\mathcal{S}^{*}};a)^{3}}{(\mu^{\pi}(X_{-\mathcal{S}^{*}})e^{ a})^{-3}}=\frac{(\mu^{\pi}(X_{-\mathcal{S}^{*}})e^{a})^{3}}{(1+\mu^{\pi}(X_{- \mathcal{S}^{*}})(e^{a}-1))^{3}}=\bigg{(}1-\frac{1-\mu^{\pi}(X_{-\mathcal{S}^{ *}})}{1+\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a}-1)}\bigg{)}^{3},\]

which implies that

\[1-3\delta\leq\frac{r^{\pi}(X_{-\mathcal{S}^{*}};a)^{3}}{(\mu^{\pi}(X_{- \mathcal{S}^{*}})e^{a})^{-3}}\leq 1.\]

For \(J(X_{-\mathcal{S}^{*}};a,\pi)\), we combine the condition that \(\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)\geq\delta^{-1}\) with (E.33) to obtain that

\[J(X_{-\mathcal{S}^{*}};a,\pi)=(1\pm\gamma^{-2}\delta)\cdot D_{\chi^{2}}(\mu^{ \pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})),\quad\text{where} \quad\gamma^{-2}\delta\leq 1/4.\]

Combining the above two results with Proposition E.4, we have

\[f_{5} =\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\,\Big{[}J(X_{ -\mathcal{S}^{*}};a,\pi)\cdot e^{a}\cdot\big{(}r^{\pi}(X_{-\mathcal{S}^{*}}) \big{)}^{3}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})\Big{]}\] \[=\big{(}1\pm(\gamma^{-2}+3)\delta\big{)}\cdot\mathbb{E}_{\pi\sim \mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}}(\mu^{\pi}(\cdot)\, \|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))\cdot\frac{e^{-2a}}{\mu^{\pi}(X_ {-\mathcal{S}^{*}})}\bigg{]}.\]

For the noise term \(\xi+\psi(a)\), we have

\[\delta\cdot\mathbb{E}_{\pi\sim\mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D _{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})) \cdot\frac{e^{-2a}}{\mu^{\pi}(X_{-\mathcal{S}^{*}})}\bigg{]}\geq\xi+\psi(a),\]

which can be verified by the condition on \(\delta\) as well as the fact that we are only considering \(a\leq(\log L)/8\). We thus have for the gradient that

\[\partial_{t}a=(1\pm(\gamma^{-2}+4)\delta)\cdot\mathbb{E}_{\pi\sim\mathcal{P}} \bigg{[}\sum_{X_{-\mathcal{S}^{*}}}D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}( \cdot\,|\,X_{-\mathcal{S}^{*}}))\cdot\frac{e^{-2a}}{\mu^{\pi}(X_{-\mathcal{S}^{*} })}\bigg{]}.\]By rearranging the terms, we further have

\[\partial_{t}e^{2a}=(1\pm(\gamma^{-2}+4)\delta)\cdot\mathbb{E}_{\pi\sim\mathcal{P}} \bigg{[}\sum_{X_{-\mathcal{S}^{*}}}\frac{D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^ {\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))\cdot}{2\mu^{\pi}(X_{-\mathcal{S}^{*}})} \bigg{]}.\]

Suppose this large \(a\) regime starts at \(t_{0}\) with value \(a(t_{0})\). Thus, for large \(a\), the growth rate is characterized by

\[a(t)=\frac{1}{2}\log\bigg{(}(1\pm(\gamma^{-2}+4)\delta)\cdot\mathbb{E}_{\pi \sim\mathcal{P}}\bigg{[}\sum_{X_{-\mathcal{S}^{*}}}\frac{D_{\chi^{2}}(\mu^{ \pi}(\cdot)\,\|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}}))}{2\mu^{\pi}(X_{- \mathcal{S}^{*}})}\bigg{]}\cdot(t-t_{0})+e^{2a(t_{0})}\bigg{)},\]

which is logarithmically fast. This step ends until \(a\) reaches the value \((\log L)/8\). This concludes the proof. 

#### e.4.1 Additional Proofs for Stage III

We conclude the proof of Stage III by providing the proof of Proposition E.4 and Proposition E.5.

Proof of Proposition E.4.: In this paragraph, we aim to gain more insight in \(f_{5}\). By the definition of \(f_{5}\) in (E.28), we can rewrite \(f_{5}\) as

\[f_{5} =\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\bigg{[}\bigg{(} \sum_{k=1}^{d}\frac{\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})^{2}}{\tilde{ \mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})}-1\bigg{)}\cdot\tilde{\mu}^{\pi}( Z_{-\mathcal{S}^{*}}=X_{-\mathcal{S}^{*}}\,|\,X_{-\mathcal{S}^{*}})\bigg{]}\] \[=\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\bigg{[}\sum_{ k=1}^{d}\bigg{(}\frac{\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})}{\tilde{ \mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})}-1\bigg{)}^{2}\cdot\tilde{\mu}^{ \pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})\cdot\tilde{\mu}^{\pi}(Z_{-\mathcal{S}^{ *}}=X_{-\mathcal{S}^{*}}\,|\,X_{-\mathcal{S}^{*}})\bigg{]},\]

where in the last step, we use the simple fact

\[\sum_{x}\frac{p(X=x\,|\,Y)^{2}}{q(X=x\,|\,Y)}-1=\sum_{x}\bigg{(}\frac{p(X=x\, |\,Y)}{q(X=x\,|\,Y)}-1\bigg{)}^{2}\cdot q(X=x\,|\,Y).\]

In the definition of \(f_{5}\), the key quantity we aim to understand is the reweighted distribution \(\tilde{\mu}^{\pi}(z,Z\,|\,X_{-\mathcal{S}^{*}})\). For the readers' convenience, we copy the definition of the reweighted distribution here:

\[\tilde{\mu}^{\pi}(z,Z\,|\,X_{-\mathcal{S}^{*}})=\frac{\mu^{\pi}(z,Z)\exp \big{(}a\prod_{h\in\mathcal{S}^{*}}\mathds{1}(z_{-h}=x_{-h})\big{)}}{\sum_{z^ {\prime},Z^{\prime}}\mu^{\pi}(z^{\prime},Z^{\prime})\exp\big{(}a\prod_{h\in \mathcal{S}^{*}}\mathds{1}(z^{\prime}_{-h}=x_{-h})\big{)}},\] (E.34)

A key observation is that the reweighting only depends on the value of \(Z_{-\mathcal{S}^{*}}\). Let \(\bar{\mathcal{S}}^{*}=[M]\backslash\mathcal{S}^{*}\) and denote by \(Z_{-\bar{\mathcal{S}}^{*}}=(z_{-h})_{h\in\bar{\mathcal{S}}^{*}}\). Following the above observation, we can additionally condition on \(Z_{-\mathcal{S}^{*}}\) and conclude that

\[\tilde{\mu}^{\pi}(z,Z_{-\bar{\mathcal{S}}^{*}}\,|\,Z_{-\mathcal{S} ^{*}},X_{-\mathcal{S}^{*}}) =\frac{\tilde{\mu}^{\pi}(z,Z_{-\bar{\mathcal{S}}^{*}},Z_{-\bar{ \mathcal{S}}^{*}}\,|\,X_{-\mathcal{S}^{*}})}{\sum_{z^{\prime},Z^{\prime}_{-\bar {\mathcal{S}}^{*}}}\tilde{\mu}^{\pi}(z^{\prime},Z^{\prime}_{-\bar{\mathcal{S}}^{ *}},Z_{-\bar{\mathcal{S}}^{*}}\,|\,X_{-\mathcal{S}^{*}})}\] \[=\frac{\mu^{\pi}(z,Z_{-\bar{\mathcal{S}}^{*}},Z_{-\mathcal{S}^{*} })\exp\big{(}a\prod_{h\in\mathcal{S}^{*}}\mathds{1}(z_{-h}=x_{-h})\big{)}}{\sum_{z ^{\prime},Z^{\prime}_{-\bar{\mathcal{S}}^{*}}}\mu^{\pi}(z^{\prime},Z^{\prime}_{ -\bar{\mathcal{S}}^{*}},Z_{-\mathcal{S}^{*}})\exp\big{(}a\prod_{h\in\mathcal{ S}^{*}}\mathds{1}(z_{-h}=x_{-h})\big{)}}\] \[=\frac{\mu^{\pi}(z,Z_{-\bar{\mathcal{S}}^{*}},Z_{-\mathcal{S}^{*} })}{\mu^{\pi}(Z_{-\bar{\mathcal{S}}^{*}})}=\mu^{\pi}(z,Z_{-\bar{\mathcal{S}}^{*} }\,|\,Z_{-\mathcal{S}^{*}}),\]

as when fixing \(Z_{-\bar{\mathcal{S}}^{*}}\), the exponential reweighting terms cancel out in the numerator and denominator in the definition (E.34). Using the above identity, we are able to expand \(\tilde{\mu}^{\pi}(z\,|\,X_{-\mathcal{S}^{*}})\) as

\[\tilde{\mu}^{\pi}(z\,|\,X_{-\mathcal{S}^{*}}) =\sum_{Z_{-\mathcal{S}^{*}}}\mu^{\pi}(z\,|\,Z_{-\mathcal{S}^{*}}) \cdot\tilde{\mu}^{\pi}(Z_{-\mathcal{S}^{*}}\,|\,X_{-\mathcal{S}^{*}})\] \[=\sum_{Z_{-\mathcal{S}^{*}}}\mu^{\pi}(z\,|\,Z_{-\mathcal{S}^{*}}) \cdot\frac{\mu^{\pi}(Z_{-\mathcal{S}^{*}})+\mu^{\pi}(X_{-\mathcal{S}^{*}})(e^{a} -1)\cdot\mathds{1}(Z_{-\mathcal{S}^{*}}=X_{-\mathcal{S}^{*}})}{1+\mu^{\pi}(X_{- \mathcal{S}^{*}})(e^{a}-1)}\] \[=\frac{\mu^{\pi}(z)+\mu^{\pi}(x=z\,|\,X_{-\mathcal{S}^{*}})\cdot \mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}-1)}{1+\mu^{\pi}(X_{-\mathcal{S}^{*}}) \cdot(e^{a}-1)}.\]where the second equality follows from the fact that the reweighing term in \(\widetilde{\mu}^{\pi}\) lifts the likelihood of \(Z_{-\mathcal{S}^{*}}=X_{-\mathcal{S}^{*}}\) by a factor of \(e^{a}\) relative to the base distribution \(\mu^{\pi}(Z_{-\mathcal{S}^{*}})\), and the denominator is just the normalization constant. In the sequel, we let \(r^{\pi}(X_{-\mathcal{S}^{*}};a)=(1+\mu^{\pi}(X_{-\mathcal{S}^{*}})\cdot(e^{a}- 1))^{-1}\) be the inverse of the normalization constant. We then have

\[\widetilde{\mu}^{\pi}(z\,|\,X_{-\mathcal{S}^{*}})=r^{\pi}(X_{- \mathcal{S}^{*}};a)\cdot\mu^{\pi}(z)+(1-r^{\pi}(X_{-\mathcal{S}^{*}};a))\cdot \mu^{\pi}(x=z\,|\,X_{-\mathcal{S}^{*}}).\] (E.35)

On the other hand, by definition of \(\widetilde{\mu}^{\pi}\) in (E.34), we directly have

\[\widetilde{\mu}^{\pi}(Z_{-\mathcal{S}^{*}}=X_{-\mathcal{S}^{*}} \,|\,X_{-\mathcal{S}^{*}}) =\frac{\mu^{\pi}(X_{-\mathcal{S}^{*}})e^{a}}{\sum_{Z^{\prime}_{- \mathcal{S}^{*}}}\mu^{\pi}(Z^{\prime}_{-\mathcal{S}^{*}})\exp\big{(}a\prod_{h \in\mathcal{S}^{*}}\mathds{1}(z^{\prime}_{-h}=x_{-h})\big{)}}\] \[=e^{a}r^{\pi}(X_{-\mathcal{S}^{*}};a)\cdot\mu^{\pi}(X_{-\mathcal{ S}^{*}}).\] (E.36)

Combining both (E.35) and (E.36) we have for \(f_{5}\) that

\[f_{5} =\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\bigg{[}\sum_ {k\in[d]}\bigg{(}\frac{\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})}{r^{\pi}(X _{-\mathcal{S}^{*}};a)\cdot\mu^{\pi}(x=e_{k})+(1-r^{\pi}(X_{-\mathcal{S}^{*}};a ))\cdot\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})}-1\bigg{)}^{2}\] \[\cdot\widetilde{\mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})\cdot \widetilde{\mu}^{\pi}(Z_{-\mathcal{S}^{*}}=X_{-\mathcal{S}^{*}}\,|\,X_{- \mathcal{S}^{*}})\bigg{]}\] \[=\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\bigg{[}\sum_ {k\in[d]}\bigg{(}\frac{\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})-\mu^{\pi}( x=e_{k})}{r^{\pi}(X_{-\mathcal{S}^{*}};a)\cdot\mu^{\pi}(x=e_{k})+(1-r^{\pi}(X_{- \mathcal{S}^{*}};a))\cdot\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})}\bigg{)} ^{2}\] \[\cdot\widetilde{\mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})\cdot e ^{a}r^{\pi}(X_{-\mathcal{S}^{*}};a)^{3}\cdot\mu^{\pi}(X_{-\mathcal{S}^{*}})\bigg{]}\] \[=\mathbb{E}_{\pi,X_{-\mathcal{S}^{*}}\sim\mu^{\pi}}\underbrace{ \bigg{[}\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})-\mu^{ \pi}(x=e_{k}))^{2}}{\widetilde{\mu}^{\pi}(z=e_{k}\,|\,X_{-\mathcal{S}^{*}})} \cdot e^{a}r^{\pi}(X_{-\mathcal{S}^{*}};a)^{3}\cdot\mu^{\pi}(X_{-\mathcal{S}^{* }})\bigg{]}}_{J(X_{-\mathcal{S}^{*}};a,\pi)}.\]

Here, we note that \(J(\cdot;a,\pi)\) is a function depending on both \(a\) and \(\pi\), and can be expanded as

\[J(X_{-\mathcal{S}^{*}};a,\pi)=\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e _{k}\,|\,X_{-\mathcal{S}^{*}})-\mu^{\pi}(x=e_{k}))^{2}}{(1-r^{\pi}(X_{- \mathcal{S}^{*}};a))\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})+r^{\pi}(X_{- \mathcal{S}^{*}};a)\mu^{\pi}(x=e_{k})}.\]

Hence, we complete the proof of Proposition E.4. 

Proof of Proposition E.5.: Also, note that \(\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)\) is convex in \(r\), as by taking the derivative of \(\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)\) with respect to \(r\), we have

\[\frac{\partial\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)}{\partial r } =\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})- \mu^{\pi}(e_{k}))^{3}}{((1-r)\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})+r\mu^{ \pi}(e_{k}))^{2}},\] \[\frac{\partial^{2}\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)}{ \partial r^{2}} =2\cdot\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})- \mu^{\pi}(e_{k}))^{4}}{((1-r)\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})+r\mu^ {\pi}(e_{k}))^{3}}\geq 0.\]

Hence, a naive upper bound for \(\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)\) is

\[\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi) \leq\max\{\widetilde{J}(X_{-\mathcal{S}^{*}};0,\pi),\widetilde{J}( X_{-\mathcal{S}^{*}};1,\pi)\}\] \[\leq\max\left\{D_{\chi^{2}}(\mu^{\pi}(\cdot)\,\|\,\mu^{\pi}(\cdot \,|\,X_{-\mathcal{S}^{*}})),D_{\chi^{2}}(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{* }})\,\|\,\mu^{\pi}(\cdot))\right\},\]where we remind the readers that \(D_{\chi^{2}}(\mu\,\|\,\nu)=\sum_{x}{(\mu(x)-\nu(x))^{2}}/{\nu(x)}\). Next, we show that

\(\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)\) is Lipschitz continuous in \(r\):

\[\left|\frac{\partial\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)}{ \partial r}\right| =\bigg{|}\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S }^{*}})-\mu^{\pi}(e_{k}))^{3}}{((1-r)\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*} })+r\mu^{\pi}(e_{k}))^{2}}\bigg{|}\] \[\leq\sum_{k\in[d]}\frac{(\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{* }})-\mu^{\pi}(e_{k}))^{2}}{(1-r)\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})+r \mu^{\pi}(e_{k})}\cdot\bigg{|}\frac{\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*} })-\mu^{\pi}(e_{k})}{(1-r)\mu^{\pi}(x=e_{k}\,|\,X_{-\mathcal{S}^{*}})+r\mu^{ \pi}(e_{k})}\] \[\leq\gamma^{-1}\cdot\max\left\{D_{\chi^{2}}(\mu^{\pi}(\cdot)\, \|\,\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})),D_{\chi^{2}}(\mu^{\pi}(\cdot\, |\,X_{-\mathcal{S}^{*}})\,\|\,\mu^{\pi}(\cdot))\right\},\]

where we use both the upper bound for \(\widetilde{J}(X_{-\mathcal{S}^{*}};r,\pi)\) and the lower bound for the transition kernel that both \(\mu^{\pi}(\cdot\,|\,X_{-\mathcal{S}^{*}})\) and \(\mu^{\pi}(\cdot)\) are bounded between \(\gamma\) and \(1\). 

### Lemma on GIH Approximation Error

Now given the convergence result for the training dynamics, the natural question to ask is how well the learned model implements the GIH mechanism. In the following part of this section, we state the lemma on the approximation error and also present a formal proof of the lemma.

**Lemma E.6**.: _Suppose Assumption 3.5 holds and consider training a transformer model \(\mathtt{TF}(M,H,d,D)\) with \(H=M\). Let_

\[\Delta_{1}\!:=\!1-p_{\mathcal{S}^{*}}(t_{1}),\quad\Delta_{2}\!:=\!1-\prod_{h \in\mathcal{S}^{*}}(\sigma_{-h}^{(h)}(t_{2}))^{2},\]

_where \(t_{1}\) and \(t_{2}\) are the ending time for the first two stages of the training, respectively. Suppose the error \(\Delta_{1},\Delta_{2}=O(L^{-1})\) after the first two stages' training, and \(a=\Theta(\log L)\) after the last stage's training. Let \(y\) be the output of the model in (2.5) after the training and \(y^{*}\) be the output of the GIH mechanism \(\mathtt{GIH}(x_{1:L};M,D)\) defined in Definition 3.2. Then for any \(\pi\in\mathrm{supp}(\mathcal{P})\) and with high probability \(1-O(L^{-1})\), it holds that_

\[\left\|y^{\star}-y\right\|_{1}=O(L^{-a/\log L}).\]

Proof of Lemma E.6.: Let \(s_{l}^{\star}=\prod_{h\in\mathcal{S}^{*}}\mathds{1}(x_{l-h}=x_{L+1-h})\) and \(s_{l}=\langle u_{L+1},u_{l}\rangle\). Invoking Lemma F.1, the model misspecification error is bounded by

\[\max_{M<l\leq L}|s_{l}^{\star}-s_{l}|\leq(\Delta_{1}+\Delta_{2})\!:=\!\Delta.\] (E.37)

We note that the second layer's attention weight \(a\) can be as large as \((\log L)/8\). We are comparing the output of the model with the GIH mechanism \(\mathtt{GIH}(x_{1:L};M,D)\). Let \(N=\sum_{l>M}\prod_{h\in\mathcal{S}^{*}}\mathds{1}(x_{l-h}=x_{L+1-h})\). The output of this GIH mechanism is given by

\[y^{\star}\!:=\!\begin{cases}N^{-1}\cdot\sum_{l=M+1}^{L}x_{l}\cdot\prod_{h\in \mathcal{S}^{*}}\mathds{1}(x_{l-h}=x_{L+1-h}),\quad\text{if}\quad N\geq 1,\\ (L-M)^{-1}\cdot\sum_{l=M+1}^{L}x_{l},\quad\text{otherwise}.\end{cases}\]

We define

\[\sigma_{l}^{\star}=\begin{cases}N^{-1}\cdot\prod_{h\in\mathcal{S}^{*}}\mathds{1 }(x_{l-h}=x_{L+1-h}),\quad\text{if}\quad N\geq 1,\\ (L-M)^{-1},\quad\text{otherwise},\end{cases}\]

with \(\sigma^{\star}=(\sigma_{l}^{\star})_{l>M}\). Since \(\|x_{l}\|_{1}=1\), the \(\ell\)-1 norm of the difference between \(y^{\star}\) and the model's actual output is given by

\[\left\|y^{\star}-y\right\|_{1}\leq\left\|\sigma^{\star}-\sigma\right\|_{1}.\]

Let us define the set \(\Gamma=\{L\geq l>M:\prod_{h\in\mathcal{S}}\mathds{1}(x_{l-h}=x_{L+1-h})=1\}\) and \(\bar{\Gamma}=\{L\geq l>M:\prod_{h\in\mathcal{S}}\mathds{1}(x_{l-h}=x_{L+1-h})=0\}\). Using (E.37), for \(l\in\Gamma\), we have \(1\geq s_{l}\geq s_{l}^{\star}-\Delta=1-\Delta\) and for \(l\in\bar{\Gamma}\), we have \(0\leq s_{l}\leq s_{l}^{\star}+\Delta=\Delta\). Consider the normalization factor in the softmax function.

\[\mathcal{Z}\!:=\!\sum_{l=M+1}^{L}\exp(a\cdot s_{l}).\]By the split of the set \(\Gamma\) and \(\bar{\Gamma}\) and noting that \(|\Gamma|=N\), the normalization factor is lower and upper bounded by

\[\mathcal{Z} \geq N\exp(a\cdot(1-\Delta))+(L-M-N)\cdot=:\mathcal{Z}_{-},\] \[\mathcal{Z} \leq N\exp(a)+(L-M-N)\cdot\exp(a\cdot\Delta)=:\mathcal{Z}_{+}.\]

Let us consider the event \(N\geq 1\) in the following. We then have for \(l\in\Gamma\) that

\[|\sigma_{l}^{\star}-\sigma_{l}| =\left|\frac{\exp(a\cdot s_{l})}{\mathcal{Z}}-\frac{1}{N}\right| \leq\left|\frac{\exp(a\cdot(1-\Delta))}{\mathcal{Z}_{-}}-\frac{1}{N}\right| \bigvee\left|\frac{\exp(a\cdot(1-\Delta))}{\mathcal{Z}_{+}}-\frac{1}{N}\right|\] \[\leq\left|\frac{1}{N\exp(-a\Delta)+(L-M-N)\cdot\exp(-a)}-\frac{1 }{N}\right|\] \[\qquad\bigvee\left|\frac{\exp(-2a\Delta)}{N\exp(-a\Delta)+(L-M-N )\exp(-a)}-\frac{1}{N}\right|\] \[\leq\frac{N\cdot(1-\exp(-a\Delta))+(L-M-N)\cdot\exp(-a)}{(N\exp( -a\Delta)+(L-M-N)\cdot\exp(-a))\cdot N}\leq\frac{1-\exp(-a\Delta)}{N\exp(-a \Delta)}+\frac{L\cdot\exp(-a)}{N^{2}\exp(-a\Delta)}.\]

Note that \(a\Delta=o(1)\) due to the assumption that \(\Delta=O(L^{-1})\) and \(a=o(L)\). The right hand side is upper bounded by \(O(a\Delta/N)+O(L\exp(-a)/N^{2})\). For \(l\in\bar{\Gamma}\), we have

\[|\sigma_{l}^{\star}-\sigma_{l}|=\sigma_{l}\leq\frac{\exp(a\Delta)}{\mathcal{Z} _{-}}\leq\frac{\exp(a\cdot(2\Delta-1))}{N}=O\left(\frac{\exp(-a)}{N}\right).\]

In summary,

\[\left\|y^{\star}-y\right\|_{1} \leq\left\|\sigma^{\star}-\sigma\right\|_{1}\leq\sum_{l\in\Gamma} |\sigma_{l}^{\star}-\sigma_{l}|+\sum_{l\in\bar{\Gamma}}\sigma_{l}\] \[\leq N\cdot O\left(\frac{a\Delta N+L\exp(-a)}{N^{2}}\right)+L \cdot O\left(\frac{\exp(-a)}{N}\right)\leq O\left(a\Delta+\frac{L\exp(-a)}{N} \right).\] (E.38)

The above inequality holds whenever \(N\geq 1\). Now we aim to upper bound the probability that \(N=0\). Note that \(N=\sum_{l=M+1}^{L}\mathds{1}(X_{l-\mathcal{S}^{\star}}=X_{L+1-\mathcal{S}^{ \star}})\). We consider the following second moment:

\[\leq D_{\chi^{2}}\bigg{(}(L-M)^{-1}\sum_{l=M+1}^{L}\mathds{1}(X_{ l-\mathcal{S}^{\star}}=\cdot)\left\|\,\mu^{\pi}(\cdot)\right)\] \[\lesssim\frac{M}{L(1-\lambda)\cdot\gamma^{|\mathcal{S}^{\star}|/ 2}},\quad\forall E\in\mathcal{X}^{|\mathcal{S}^{\star}|},\]

where the first inequality holds by noting that \(D_{\chi^{2}}(\mu\,\|\,\nu)=\sum_{\pi}(\mu(x)-\nu(x))^{2}/\nu(x)\) and the last inequality holds by Lemma F.18. Therefore, by the Chebyshev's inequality, we have

\[\mathbb{P}\bigg{(}\left|L^{-1}\sum_{l=1}^{L}\mathds{1}(X_{l-\mathcal{S}^{ \star}}=E)-\mu^{\pi}(E)\right|\geq t\bigg{)}\leq\frac{1}{L(1-\lambda)\cdot \gamma^{|\mathcal{S}^{\star}|}\cdot t^{2}}.\]

We can take \(t=\min_{E\in\mathcal{X}^{|\mathcal{S}^{\star}|}}\mu^{\pi}(E)/2\) and by also taking a union bound over \(E\in\mathcal{X}^{|\mathcal{S}^{\star}|}\) (which gives a \(d^{|\mathcal{S}^{\star}|}\) factor), we conclude that with high probability \(\widetilde{O}(1-L^{-1})\) it holds that \(N\geq tL=L\cdot\min_{E\in\mathcal{X}^{|\mathcal{S}^{\star}|}}\mu^{\pi}(E)/2\). Thus, it follows from (E.38) that with high probability

\[\left\|y^{\star}-y\right\|_{1}\leq O\left(a\Delta+\frac{\exp(-a)}{\min_{E\in \mathcal{X}^{|\mathcal{S}^{\star}|}}\mu^{\pi}(E)/2}\right)=O\left(L^{-1}\log L +L^{-a/\log L}\right).\]

Hence, we complete the proof of Lemma E.6.

Auxiliary Lemmas and Their Proofs

In this appendix, we present the auxiliary lemmas used to derive the approximation of the gradient flow dynamics in the proof of Theorem 3.6, which is presented in the previous appendix. The proofs of these lemmas are presented right below their statements.

### Useful Inequalities

The following lemma provides a bound on the model misspecification error, which is the difference between the model's output and the ideal output.

**Lemma F.1** (Model Misspecification Error).: _Let \(u_{L+1}\) be the output feature after the FFN & Normalization layer. Then, the model misspecification error defined as_

\[\max_{l\in[L]}\left|\langle u_{L+1},u_{l}\rangle-\prod_{h\in\mathcal{S}^{*}} \mathds{1}(x_{l-h}=x_{L+1-h})\right|\]

_is bounded by \(\Delta_{1}+\Delta_{2}\), where \(\Delta_{1}\) and \(\Delta_{2}\) are the errors after the training of the first and second stages, respectively, and are defined respectively as_

\[\Delta_{1}\operatorname{:=}1-p_{\mathcal{S}^{*}},\qquad\Delta_{2} \operatorname{:=}1-\prod_{h\in\mathcal{S}^{*}}(\sigma_{-h}^{(h)})^{2}.\]

Proof of Lemma F.1.: By definition of the output feature \(u_{l}\) after the FFN & Normalization layer:

\[\langle u_{L+1},u_{l}\rangle=\sum_{\mathcal{S}\in[H]_{\leq D}}p_{\mathcal{S}} \cdot\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle.\]

As each \(v_{l}^{(h)}\) is a convex combination of \(X_{\mathcal{M}(l)}\) where \(\mathcal{M}(l)=\{l-M,\ldots,l-1\}\), \(\|v_{l}^{(h)}\|_{2}\leq 1\). Thus,

\[\left|\langle u_{L+1},u_{l}\rangle-\prod_{h\in\mathcal{S}^{*}} \langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\right| =\bigg{|}\sum_{\mathcal{S}\in[H]_{\leq D}}p_{\mathcal{S}}\cdot \prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle-\prod_{h\in \mathcal{S}^{*}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{|}\] \[\leq\bigg{|}-(1-p_{\mathcal{S}^{*}})\prod_{h\in\mathcal{S}^{*}} \langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle+\sum_{\mathcal{S}\in[H]_{\leq D} \setminus\{\mathcal{S}^{*}\}}p_{\mathcal{S}}\cdot\prod_{h\in\mathcal{S}} \langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{|}\] \[\leq\max\left\{1-p_{\mathcal{S}^{*}},\sum_{\mathcal{S}\in[H]_{ \leq D}\setminus\{\mathcal{S}^{*}\}}p_{\mathcal{S}}\right\}=1-p_{\mathcal{S}^ {*}}\operatorname{=:}\Delta_{1},\]

where \(\Delta_{1}\) is the error after the training of the first stage. Since \(v_{l}^{(h)}=\sum_{j\in M}\sigma_{-j}^{(h)}x_{l-j}\), we have

\[\prod_{h\in\mathcal{S}^{*}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle =\prod_{h\in\mathcal{S}^{*}}\bigg{(}\sum_{i,j\in[M]^{2}}\sigma_{-i }^{(h)}\sigma_{-j_{h}}^{(h)}\langle x_{l-i},x_{L+1-j}\rangle\bigg{)}\] \[=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\in[M]^{2|\mathcal{ S}^{*}|}}\prod_{h\in\mathcal{S}^{*}}\sigma_{-i_{h}}^{(h)}\sigma_{-j_{h}}^{(h)} \mathds{1}(x_{l-i_{h}}=x_{L+1-j_{h}}).\]

Here in the second equality, we exchange the order of summation and product. The last term of the second equality can be understood as follows. We first pick \(|\mathcal{S}^{*}|\) index pairs \(\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\) arbitrarily, with each \(i_{h},j_{h}\in[H]\). Then we evaluate the product \(\prod_{h\in\mathcal{S}^{*}}\sigma_{-i_{h}}^{(h)}\sigma_{-j_{h}}^{(h)}\mathds{1 }(x_{l-i_{h}}=x_{L+1-j_{h}})\) given these indices. Then we sum over all possible values that \(\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\) can take.

The above equation implies that

\[\left|\prod_{h\in\mathcal{S}^{*}}\langle v_{l}^{(h)},v_{L+1}^{(h)} \rangle-\prod_{h\in\mathcal{S}^{*}}(\sigma_{-h}^{(h)})^{2}\mathds{1}(x_{l-h}= x_{L+1-h})\right|\] \[\quad=\bigg{|}\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\neq \{(h,h)\}_{h\in\mathcal{S}^{*}}}\prod_{h\in\mathcal{S}^{*}}\sigma_{-i_{h}}^{(h)} \sigma_{-j_{h}}^{(h)}\mathds{1}(x_{l-i_{h}}=x_{L+1-j_{h}})\bigg{|}\] \[\quad\leq\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\neq\{(h,h) \}_{h\in\mathcal{S}^{*}}}\prod_{h\in\mathcal{S}^{*}}\sigma_{-i_{h}}^{(h)} \sigma_{-j_{h}}^{(h)}\leq 1-\prod_{h\in\mathcal{S}^{*}}(\sigma_{-h}^{(h)})^{2} \operatorname{=:}\Delta_{2},\] (F.1)where the last inequality follows from the fact that

\[\sum_{(i_{h},j_{h})_{h\in\mathcal{S}^{*}}}\prod_{h\in\mathcal{S}^{*}} \sigma^{(h)}_{-i_{h}}\sigma^{(h)}_{-j_{h}}=\prod_{h\in\mathcal{S}^{*}}\bigg{(} \sum_{i,j\in[M]^{2}}\sigma^{(h)}_{-i}\sigma^{(h)}_{-j}\bigg{)}=\prod_{h\in \mathcal{S}^{*}}\bigg{(}\sum_{i\in[M]}\sigma^{(h)}_{-i}\bigg{)}^{2}=1.\]

Here the summation sign in the right-hand side of the second equality indicates that in the last line of (F.1) we sum over all possible values that \(\{(i_{h},j_{h})\}_{h\in\mathcal{S}^{*}}\) can take, except for the only case where \((i_{h},j_{h})=(h,h)\) for all \(h\in[H]\).

In summary, by triangle inequality, we have shown that

\[\bigg{|}\langle u_{L+1},u_{l}\rangle-\prod_{h\in\mathcal{S}^{*}} \mathds{1}(x_{l-h}=x_{L+1-h})\bigg{|}\leq\Delta_{1}+\Delta_{2}.\]

The proof is completed. 

Next, in Lemma F.2, we establish a uniform bound for the quantity involved in the gradient.

**Lemma F.2**.: _Let \(y(k)=\sum_{l=M+1}^{L}\sigma_{l}\mathds{1}(x_{l}=e_{k})\) for each \(k\in[d]\) where \(\sum_{l=M+1}^{L}\sigma_{l}=1\) and \(\sigma_{l}\geq 0\) for all \(l\in[L]\). Let \(\varepsilon\) and \(C\) be two positive numbers. For any \(C\)-bounded function \(f:\mathcal{X}^{L+1}\to[-C,C]\), we have_

\[\bigg{|}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k=1}^{d}\bigg{(} \frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}-\frac{y(k)\,\mathds{ 1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\bigg{)}\cdot f(X)\bigg{|}\leq 2C.\]

Proof of Lemma f.2.: By the triangular inequality, we have

\[\bigg{|}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k=1}^{d}\bigg{(} \frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}-\frac{y(k)\,\mathds{ 1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\bigg{)}\cdot f(X)\bigg{|}\] \[\quad\leq C\cdot\left|\sum_{k=1}^{d}\sum_{l=M+1}^{L}\sigma_{l} \cdot\mathds{1}(x_{l}=e_{k})\cdot\frac{\mathds{1}(x_{L+1}=e_{k})}{y(k)+ \varepsilon}\right|+C\cdot\left|\sum_{k=1}^{d}\sum_{l=M+1}^{L}\sigma_{l}\cdot \frac{y(k)\cdot\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\right|\] \[\quad=2C\cdot\left|\sum_{k=1}^{d}\frac{y(k)\cdot\mathds{1}(x_{L+ 1}=e_{k})}{y(k)+\varepsilon}\right|\leq 2C,\]

where in the equality, we use the definition \(y(k)=\sum_{l=M+1}^{L}\sigma_{l}\mathds{1}(x_{l}=e_{k})\) and \(\sum_{l=M+1}^{L}\sigma_{l}=1\). Now we conclude the proof of this lemma. 

### Approximation Errors for Dynamics Analysis

Next, Lemma F.3 addresses the approximation error induced by \(\sigma_{l}\approx 1/L\) in the transformer model. The approximation error will be for \(g_{0,\mathcal{S}}\) to \(g_{1,\mathcal{S}}\) for Stage I and \(g_{h,1}\) to \(g_{h,2}\) for Stage II.

**Lemma F.3**.: _For the transformer model defined in (2.5) and any bounded function \(f:\mathcal{X}^{L+1}\to\mathbb{R}\) such that \(\sup_{x\in\mathcal{X}^{L}}|f(x)|\leq C\) for a constant \(C>0\), define two quantities \(A\) and \(B\) as_

\[A :=\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\sigma_{l}(as)\cdot \sum_{k\in[d]}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon }-\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}\bigg{)}\cdot f(X) \bigg{]},\] \[B :=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+\varepsilon}- \frac{\bar{y}(k)\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+\varepsilon}\bigg{)} \cdot f(X)\bigg{]},\]

_where \(s=u_{L+1}^{\top}U_{1:L}^{\top}\) and \(\bar{y}=(L-M)^{-1}\sum_{l=M+1}^{L}x_{l}\). Then, for all \(a\in[0,1]\) and \(\varepsilon\in(0,1]\), it holds that_

\[|A-B|\leq\frac{8Cad}{\varepsilon^{2}}.\]Proof of Lemma F.3.: By triangular inequality, we have

\[|A-B|\leq \sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sum_{k\in[d]}\left\{\left|\sigma_{ l}\big{(}a\cdot s\big{)}-\frac{1}{L-M}\right|\cdot\left|\frac{\mathds{1}(x_{L+1}=x_{l} =e_{k})}{y(k)+\varepsilon}\right|\right.\] \[\qquad+\left.\frac{1}{L-M}\right|\frac{\mathds{1}(x_{L+1}=x_{l} =e_{k})}{y(k)+\varepsilon}-\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+ \varepsilon}\bigg{|}\] \[\qquad+\left|\sigma_{l}\big{(}a\cdot s\big{)}-\frac{1}{L-M} \right|\cdot\left|\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon} \right|\!\!,\] \[\qquad+\frac{1}{L-M}\bigg{|}\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k} )}{y(k)+\varepsilon}-\frac{\bar{y}(k)\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+ \varepsilon}\bigg{|}\bigg{\}}\cdot f(X)\bigg{]}.\]

Note that \(0\leq s_{l}\leq 1\) for all \(l=M+1,\ldots,L\) thanks to the layer normalization. Then, for the softmax operation, we have

\[\frac{1}{1+(L-M-1)\exp(a)}\leq\sigma_{l}\big{(}a\cdot s\big{)}\leq\frac{\exp(a )}{L-M-1+\exp(a)},\]

which implies that

\[\left|\sigma_{l}\big{(}a\cdot s\big{)}-\frac{1}{L-M}\right| \leq\max\left\{\frac{1}{L-M}-\frac{1}{1+(L-M-1)\exp(a)},\;\frac{ \exp(a)}{L-M-1+\exp(a)}-\frac{1}{L-M}\right\}\] \[\leq\frac{\exp(a)-1}{L-M-1}.\] (F.2)

Since indicator functions are bounded above by \(1\), we have

\[\left|\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}\right|\leq \frac{1}{\varepsilon},\quad\left|\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+ \varepsilon}\right|\leq\frac{1}{\varepsilon},\] (F.3)

For the second term, we have

\[\left|\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}- \frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+\varepsilon}\right| \leq\frac{|\bar{y}(k)-y(k)|}{\varepsilon^{2}}\leq\frac{\sum_{l=M+ 1}^{L}|\sigma_{l}\big{(}a\cdot s^{\top}\big{)}-(L-M)^{-1}|}{\varepsilon^{2}}\] \[\leq\frac{\exp(a)-1}{\varepsilon^{2}},\] (F.4)

where the last inequality follows from (F.2). Similarly, the following bound can be derived:

\[\left|\frac{y(k)\,\mathds{1}(x_{L+1}=e_{k})}{y(k)+\varepsilon}-\frac{\bar{y}( k)\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+\varepsilon}\right|\leq\frac{\exp(a )-1}{\varepsilon}.\] (F.5)

Combining (F.2), (F.3), (F.4) and (F.5), it holds that

\[|A-B|\leq\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}4\sum_{k\in[d]}\frac{\exp(a)-1}{ \varepsilon^{2}(L-M)}\cdot f(X)\bigg{]}\leq\frac{4Cd(\exp(a)-1)}{\varepsilon^ {2}}\leq\frac{8Cd}{\varepsilon^{2}},\]

where the last inequality follows from \(\exp(x)-1\leq 2x\) for \(0\leq x\leq 1\). This concludes the proof of the lemma. 

Lemma F.4 provides the approximation error introduced by \(\mu^{\pi}(e_{k})\approx\bar{y}(k)\) in the transformer model.

**Lemma F.4**.: _For the transformer model defined in (2.5) and any bounded function \(f:\mathcal{X}^{L}\to\mathbb{R}\) such that \(\sup_{x\in\mathcal{X}^{L}}|f(x)|\leq C\) for a constant \(C>0\), define two quantities \(A\) and \(B\) as_

\[A :=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}(k)+\varepsilon}- \frac{\bar{y}(k)\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}(k)+\varepsilon}\bigg{)} \cdot f(X)\bigg{]},\] \[B :=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)} \cdot f(X)\bigg{]},\]_where \(\bar{y}=(L-M)^{-1}\sum_{l=M+1}^{L}x_{l}\). Under Assumption 3.5, it holds that_

\[|A-B|\leq 4C\cdot\frac{(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1)^{1/4}+2\sqrt{M}}{L^{1/2}\gamma}+C\gamma^{-1}\varepsilon.\]

_where \(\mu_{0}(\cdot)\) is the initial distribution over the first \(r_{n}\) tokens \(X_{1:r_{n}}\). Here we let \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) to denote \(D_{\chi^{2}}(\mu_{0}(X_{1:r_{n}}=\cdot)\,\|\,\mu^{\pi}(X_{1:r_{n}}=\cdot))\), i.e., the \(\chi^{2}\)-divergence between \(\mu_{n}\) and the distribution over the first \(r_{n}\) tokens under the stationary distribution \(\mu^{\pi}\)._

Proof of Lemma F.4.: Let us use \(\bar{y}_{X}(\cdot)\) to remind the readers that \(\bar{y}(\cdot)\) is also a function of \(X\). We simplify the expectation \(\mathbb{E}_{X|\pi}\) by \(\mathbb{E}\) in this proof. By rearranging the terms, we have

\[|A-B|=\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[} \bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\bar{y}_{X}(k)+ \varepsilon}-\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_ {k})}\] \[-\sum_{k\in[d]}\frac{\bar{y}_{X}(k)\cdot\mathds{1}(x_{L+1}=e_{k})} {\bar{y}_{X}(k)+\varepsilon}+1\bigg{)}\cdot f(X)\bigg{]}\bigg{|}\] \[=\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\bigg{(} \sum_{k\in[d]}\Big{(}\frac{\mu^{\pi}(e_{k})-\bar{y}_{X}(k)}{(\bar{y}_{X}(k)+ \varepsilon)\cdot\mu^{\pi}(e_{k})}-\frac{\varepsilon}{(\bar{y}_{X}(k)+ \varepsilon)\cdot\mu^{\pi}(e_{k})}\Big{)}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\] \[-\sum_{k\in[d]}\frac{\varepsilon\,\mathds{1}(x_{L+1}=e_{k})}{ \bar{y}_{X}(k)+\varepsilon}\bigg{)}\cdot f(X)\bigg{]}\bigg{|}.\]

Here, we have three terms to control. For the first error term, we define

\[\mathrm{err}_{1}:= \bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sum_{k\in [d]}\frac{\mu^{\pi}(e_{k})-\bar{y}_{X}(k)}{(\bar{y}_{X}(k)+\varepsilon)\cdot\mu ^{\pi}(e_{k})}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\cdot f(X)\bigg{]}\bigg{|}\] \[\leq\frac{C}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sum_{k\in[d]} \frac{|\mu^{\pi}(e_{k})-\bar{y}_{X}(k)|}{(\bar{y}_{X}(k)+\varepsilon)\cdot\mu ^{\pi}(e_{k})}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\bigg{]}\] \[\leq C\cdot\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{|\mu^{\pi}(e_{k} )-\bar{y}_{X}(k)|}{\mu^{\pi}(e_{k})}\cdot\mathds{1}(x_{L+1}=e_{k})\bigg{]}.\]

The first inequality above holds by noting that \(\sup_{X}|f(X)|\leq C\) and the last inequality holds by noting that \(\bar{y}_{X}(e_{k})=(L-M)^{-1}\sum_{l=M+1}^{L}\mathds{1}(x_{l}=e_{k})\). Using Cauchy-Schwarz inequality, we arrive at

\[\mathrm{err}_{1} \leq C\cdot\bigg{(}\mathbb{E}\bigg{[}\sum_{k\in[d]}\Big{(}\frac{ \mu^{\pi}(e_{k})-\bar{y}_{X}(k)}{\sqrt{\mu^{\pi}(e_{k})}}\Big{)}^{2}\bigg{]} \cdot\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=e_{k})}{\mu^{ \pi}(e_{k})}\bigg{]}\bigg{)}^{1/2}\] \[\leq C\gamma^{-1/2}\cdot\sqrt{\mathbb{E}\left[D_{\chi^{2}}\left( \bar{y}_{X}(\cdot)\,\|\,\mu^{\pi}(x_{L+1}=\cdot)\right)\right]}.\]

For the second term, we similarly have

\[\mathrm{err}_{2} =\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\sum_{k\in[d]}\mathbb{E} \bigg{[}\frac{\varepsilon}{(\bar{y}_{X}(k)+\varepsilon)\mu^{\pi}(e_{k})}\cdot \mathds{1}(x_{L+1}=x_{l}=e_{k})\cdot f(X)\bigg{]}\bigg{|}\] \[\leq C\bigg{|}\sum_{k\in[d]}\mathbb{E}\bigg{[}\frac{\varepsilon \cdot\mathds{1}(x_{L+1}=e_{k})}{\mu^{\pi}(e_{k})}\bigg{]}\bigg{|}\leq C\gamma^ {-1}\varepsilon.\]

Lastly, we have the error term

\[\mathrm{err}_{3}:= \frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\sum_{k\in[d]} \frac{\varepsilon\,\mathds{1}(x_{L+1}=e_{k})}{\bar{y}_{X}(k)+\varepsilon}\cdot f (X)\bigg{]}\leq C\cdot\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon\, \mathds{1}(x_{L+1}=e_{k})}{\bar{y}_{X}(k)+\varepsilon}\bigg{]}\] \[\leq C\cdot\bigg{|}\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{ \varepsilon\,\mathds{1}(x_{L+1}=e_{k})}{\mu^{\pi}(e_{k})+\varepsilon}\bigg{]} \bigg{|}+C\cdot\bigg{|}\sum_{k\in[d]}\mathbb{E}\bigg{[}\frac{\varepsilon(\bar{y}_ {X}(k)-\mu^{\pi}(e_{k}))\cdot\mathds{1}(x_{L+1}=e_{k})}{(\mu^{\pi}(e_{k})+ \varepsilon)(\bar{y}_{X}(k)+\varepsilon)}\bigg{]}\bigg{|}.\]Here, the first term is upper bounded by \(C\gamma^{-1}\varepsilon\), and for the second term we have by Cauchy-Schwartz that

\[C\cdot\left|\sum_{k\in[d]}\mathbb{E}\bigg{[}\frac{\varepsilon( \bar{y}_{X}(k)-\mu^{\pi}(e_{k}))\cdot\mathds{1}(x_{L+1}=e_{k})}{(\mu^{\pi}(e_{k })+\varepsilon)(\bar{y}_{X}(k)+\varepsilon)}\bigg{]}\right|\] \[\quad\leq C\cdot\sqrt{\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{(\bar {y}_{X}(k)-\mu^{\pi}(e_{k}))^{2}}{\mu^{\pi}(e_{k})}\bigg{]}\cdot\mathbb{E} \bigg{[}\sum_{k\in[d]}\frac{\varepsilon^{2}\,\mathds{1}(x_{L+1}=e_{k})}{(\bar{y }_{X}(k)+\varepsilon)^{2}\mu^{\pi}(e_{k})}\bigg{]}}\] \[\quad\leq C\gamma^{-1/2}\cdot\sqrt{\mathbb{E}_{X}D_{\chi^{2}}( \bar{y}_{X}(\cdot)\,\|\,\mu^{\pi}(x_{L+1}=\cdot))},\]

which shares a similar upper bound as \(\mathrm{err}_{1}\). Now we invoke Lemma F.18 to conclude that

\[|A-B| \leq\mathrm{err}_{1}+\mathrm{err}_{2}+\mathrm{err}_{3}\leq 2C \gamma^{-1/2}\cdot\sqrt{\mathbb{E}_{X}D_{\chi^{2}}(\bar{y}_{X}(\cdot)\,\|\,\mu ^{\pi}(x_{L+1}=\cdot))}+C\gamma^{-1}\varepsilon\] \[\leq 2C\gamma^{-1/2}\bigg{(}\frac{4(1-\lambda)^{-1}\sqrt{D_{ \chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}+16M}{L\cdot\min_{x_{L+1}}\mu^{\pi}(x_{L+1} )}\bigg{)}^{1/2}+C\gamma^{-1}\cdot\varepsilon\] \[\leq 2C\gamma^{-1}\cdot\frac{2(1-\lambda)^{-1/2}(D_{\chi^{2}}( \mu_{0}\,\|\,\mu^{\pi})+1)^{1/4}+4\sqrt{M}}{L^{1/2}}+C\gamma^{-1}\varepsilon.\]

Hence, we complete our proof of Lemma F.4. 

Lemma F.5 covers the approximation error due to the mixing property of the Markov chain.

**Lemma F.5**.: _Let \(\mathcal{S}\in[H]_{\leq D}\) be a fixed set. For any \(h\in\mathcal{S}\), let \(\widehat{\sigma}^{(h)}\) and \(\sigma^{(h)}\) be two fixed probability distributions over \([M]\). That is, for any \(i,j\in[M]\), we have \(\widetilde{\sigma}^{(h)}_{-i},\sigma^{(h)}_{-j}\in[0,1]\), and \(\sum_{i=1}^{M}\widetilde{\sigma}^{(h)}_{-i}=\sum_{j=1}^{M}\sigma^{(h)}_{-j}=1\). Given these distributions over \([M]\), we define_

\[\widetilde{v}^{(h)}_{L+1}:=\sum_{i\in[M]}\widetilde{\sigma}^{(h)}_{-i}\cdot x _{L+1-i},\qquad\text{and}\qquad v^{(h)}_{l}:=\sum_{j\in[M]}\sigma^{(h)}_{-j} \cdot x_{l-j},\]

_where we let \(x_{l}\in\mathcal{X}\) denote the \(l\)-th token in the Markov chain for all \(l\in[L+1]\). Moreover, with slight abuse of notation, we let \((z,Z)=(z,z_{-1},\ldots,z_{-M})\in\mathcal{X}^{M+1}\) and \((x,X)=(x,x_{-1},\ldots,x_{-M})\in\mathcal{X}^{M+1}\) be two independent random variables sampled from the stationary distribution \(\mu^{\pi}\). We define random variables \(\widehat{v}^{(h)}(Z)\) and \(v^{(h)}(X)\) as_

\[\widehat{v}^{(h)}(Z):=\sum_{i\in[M]}\widetilde{\sigma}^{(h)}_{-i}\cdot z_{-i},\qquad\text{and}\qquad v^{(h)}(X):=\sum_{j\in[M]}\sigma^{(h)}_{-j}\cdot x_{-j}.\]

_Using \(\widehat{v}^{(h)}_{L+1}\), \(v^{(h)}_{l}\), \(\widehat{v}^{(h)}(Z)\), and \(v^{(h)}(X)\), we define two quantities \(A\) and \(B\) as_

\[A:=\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)} \cdot\prod_{h\in\mathcal{S}}\langle v^{(h)}_{l},\widehat{v}^{(h)}_{L+1}\rangle \bigg{]},\] \[B:=\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)} \cdot\prod_{h\in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X)\rangle \bigg{]},\]

_where \(\mathbb{E}_{X\,|\,\pi}\) means that the expectation is taken with respect to the randomness of the Markov chain with transition \(\pi\). Then, under Assumption 3.5, we have_

\[|A-B|\leq\frac{8M}{L\gamma}+\frac{16\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1 }}{L(1-\lambda)\gamma^{|\mathcal{S}|/2+1}},\]

_where \(\mu_{0}(\cdot)\) is the initial distribution over the first \(r_{n}\) tokens \(X_{1:r_{n}}\) and \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) is a short-hand notation of \(D_{\chi^{2}}(\mu_{0}(X_{1:r_{n}}=\cdot)\,\|\,\mu^{\pi}(X_{1:r_{n}}=\cdot))\)._Proof of Lemma F.5.: By triangular inequality, we have

\[|A-B|\leq\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi} \bigg{[}\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e _{k})}\bigg{)}\cdot\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},\widehat{v}_{L+1} ^{(h)}\rangle\bigg{]}\] \[\qquad-\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}\bigg{)} \cdot\Big{(}\prod_{h\in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X) \rangle\Big{)}\bigg{]}\bigg{|}\] \[\qquad+\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}_{X|\pi} \bigg{[}\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]} -\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\bigg{(}\prod_{h \in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X)\rangle\bigg{)}\bigg{]} \bigg{|}.\]

We will establish the upper bounds for each of the absolute value terms. We first focus on the first absolute value term.

Bounding the First Absolute Value Term.Let \(p^{\pi}(X)\) denote the joint distribution of the whole sequence \(X\) under kernel \(\pi\). By the definitions of \(\widehat{v}_{L+1}^{(h)}\) and \(v_{l}^{(h)}\), we have

\[\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle =\sum_{i_{h},j_{h}\in[M]}\sigma_{-i_{h}}^{(h)}\cdot\sigma_{-j_{h} }^{(h)}\cdot\langle x_{L+1-i_{h}},x_{l-j_{h}}\rangle\] \[=\sum_{i_{h},j_{h}\in[M]}\sum_{k\in[d]}\sigma_{-i_{h}}^{(h)} \cdot\sigma_{-j_{h}}^{(h)}\cdot\mathds{1}(x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k}),\]

where we use \((i_{h},j_{h})\) as the indices to highlight that they are associated with head \(h\). And we use \(k_{h}\in[d]\) to index all the possible common values for \(x_{l-i_{h}}\) and \(x_{L+1-j_{h}}\). Then plugging this equality into \(\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\) and exchanging the order of product and summation, we have

\[\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^ {\pi}(e_{k})}\bigg{)}\cdot\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{( h)}\rangle\] (F.6) \[\qquad=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\sum_{\{k_{h}\} \leq s,k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(z=e_{k})}\cdot \bigg{(}\prod_{h\in\mathcal{S}}\sigma_{-i_{h}}^{(h)}\cdot\sigma_{-j_{h}}^{(h)} \cdot\mathds{1}(x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k_{h}})\bigg{)},\]

where the summation means that we sum over all possible values that \(\{i_{h},j_{h},k_{h}\}_{h\in\mathcal{S}}\) and \(k\) can take. Specifically, each \(i_{h}\) and \(j_{h}\) take values in \([M]\), and each \(k_{h}\) and \(k\) takes values in \([d]\). Moreover, using the property of indicator functions, we can further simplify (F.6) by gathering all indicators:

\[\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^ {\pi}(e_{k})}\bigg{)}\cdot\prod_{h\in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{( h)}\rangle\] (F.7) \[\qquad=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\bigg{(}\prod_{h \in\mathcal{S}}\sigma_{-i_{h}}^{(h)}\cdot\sigma_{-j_{h}}^{(h)}\bigg{)}\cdot \bigg{(}\sum_{\{k_{h}\}_{h\in\mathcal{S}},k\in[d]}\frac{\mathds{1}(x_{L+1}=x_ {l}=e_{k},x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S})}{\mu^ {\pi}(z=e_{k})}\bigg{)}.\]

Now we take expectations with respect to the randomness of \(X\) on both ends of (F.7) and get

\[\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\bigg{(}\sum_{k\in[ d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})}\bigg{)}\cdot\prod_{h \in\mathcal{S}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]}\] \[=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\!\!\left(\!\prod_{h\in \mathcal{S}}\widehat{\sigma}_{-i_{h}}^{(h)}\sigma_{-j_{h}}^{(h)}\right)\cdot \sum_{\{k_{h}\}_{h\in\mathcal{S}},k\in[d]}\frac{\sum_{l=M+1}^{L}p^{\pi}(x_{L+ 1}=x_{l}=e_{k},x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S})}{(L -M)\cdot\mu^{\pi}(z=e_{k})}.\]

To further simplify the above equality, we define a new probability distribution over \(X_{L+1-M:L+1}\) and another subsequence of length \(M+1\). Note that \(X_{L+1-M:L+1}\) contains is a subsequence with \(M+1\) tokens. We let \((z,Z)=(z,z_{-1},\ldots,z_{-1},z_{-M})\) denote a random token sequence of size \(M+1\) in reverse order. We define a joint distribution \(\widehat{p}^{\pi}\) over \(X_{L+1-M:L+1}\) and \((z,Z)\) as follows. Let \(E=(E_{0},E_{-1},\ldots,E_{-M})\) and \(E^{\prime}=(E^{\prime}_{0},E^{\prime}_{-1},\ldots,E^{\prime}_{-M})\) be two elements in \(\mathcal{X}^{M+1}\). That is,each component of \(E\) and \(E^{\prime}\) are in \(\mathcal{X}\). The probability mass function of \(\widehat{p}^{\pi}\) is defined as

\[\widehat{p}^{\pi}\big{(}(x_{L+1},x_{L},\ldots,x_{L+1-M})=E,(z,Z)=E^{ \prime}\big{)}\] (F.8) \[\qquad=\frac{1}{L-M}\sum_{l=M+1}^{L}p^{\pi}\big{(}(x_{L+1},x_{L}, \ldots,x_{L+1-M})=E,(x_{l},x_{l-1},\ldots,x_{l=M})=E^{\prime}\big{)}.\]

That is, \(\widehat{p}^{\pi}\) can be viewed as the joint distribution of \(X_{L+1-M:L+1}\) with an averaged distribution of the history. When \(L\) is sufficiently large, by the mixing property of the Markov chain, we expect that, under \(\widehat{p}^{\pi}\), \((z,Z)\) is approximately independent of \(X_{L+1-M:L+1}\), and the marginal distributions of \((z,Z)\) and \(X_{L+1-M:L+1}\) are both close to the stationary distribution \(\mu^{\pi}\). We will translate this intuition into a rigorous argument in Lemma F.17, which bounds the total-variation distance between \(\widehat{p}^{\pi}\) and the product distribution \(\mu^{\pi}\times\mu^{\pi}\).

With \(\widehat{p}^{\pi}\) defined in (F.8), we can rewrite the expectation above as

\[\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\bigg{(}\sum_{k \in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\mu^{\pi}(e_{k})}\bigg{)}\cdot \prod_{h\in\mathcal{S}}\langle v^{(h)}_{l},v^{(h)}_{L+1}\rangle\bigg{]}\] (F.9) \[\qquad=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\!\!\left(\prod _{h\in\mathcal{S}}\widehat{\sigma}^{(h)}_{-i_{h}}\sigma^{(h)}_{-j_{h}}\right) \cdot\sum_{\{k_{h}\}_{h\in\mathcal{S}},k\in[d]}\frac{\widehat{p}^{\pi}(x_{L+1 }=z=e_{k},x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S})}{\mu^{ \pi}(z=e_{k})}.\]

Similarly, by the definitions of \(\widetilde{v}^{(h)}(Z)\) and \(v^{(h)}(Z)\), we can write \(\langle\widetilde{v}^{(h)}(Z),v^{(h)}(X)\rangle\) as

\[\langle\widetilde{v}^{(h)}(Z),v^{(h)}(X)\rangle=\sum_{i_{h},j_{h}\in[M]}\sum_ {k_{h}\in[d]}\sigma^{(h)}_{-i_{h}}\cdot\sigma^{(h)}_{-j_{h}}\cdot\mathds{1} \big{(}z_{-i_{h}}=x_{j_{h}}=e_{k}\big{)}.\]

Then, multiplying these terms with \(h\in\mathcal{S}\), we can write

\[\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k })}\bigg{)}\cdot\prod_{h\in\mathcal{S}}\langle\widetilde{v}^{(h)}(Z),v^{(h)}( X)\rangle\] \[\qquad=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\bigg{(}\prod_{h \in\mathcal{S}}\sigma^{(h)}_{-i_{h}}\cdot\sigma^{(h)}_{-j_{h}}\bigg{)}\cdot \bigg{(}\sum_{\{k_{h}\}_{h\in\mathcal{S}},k\in[d]}\frac{\mathds{1}(z=x=e_{k},z _{-i_{h}}=x_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S})}{\mu^{\pi}(z=e_{k})} \bigg{)}.\] (F.10)

Recall that here \((z,Z)=(z,z_{-1},\ldots,z_{-M})\) and \((x,X)=(x,x_{-1},\ldots,x_{-M})\) are independently sampled from the stationary distribution \(\mu^{\pi}\). Taking the expectation under \(\mu^{\pi}\), we have

\[\mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}\bigg{)}\cdot \bigg{(}\prod_{h\in\mathcal{S}}\langle\widetilde{v}^{(h)}(Z),v^{(h)}(X) \rangle\bigg{)}\bigg{]}\] (F.11) \[\qquad=\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\!\!\left(\!\prod _{h\in\mathcal{S}}\widehat{\sigma}^{(h)}_{-i_{h}}\sigma^{(h)}_{-j_{h}}\!\right) \cdot\!\sum_{\{k_{h}\}_{h\in\mathcal{S}},k\in[d]}\!\!\frac{\mu^{\pi}(x=e_{k},x _{-i_{h}}=e_{k_{h}},\forall h\in\mathcal{S})\cdot\mu^{\pi}(z=e_{k},z_{-j_{h}}= e_{k_{h}},\forall h\in\mathcal{S})}{\mu^{\pi}(z=e_{k})}.\]

To bound the first absolute value term in the upper bound on \(|A-B|\), we aim to compare (F.9) and (F.11). To this end, let us fix collections of index pairs \((i_{h},j_{h})_{h\in\mathcal{S}}\). Let \(\mathcal{S}_{1}=\{i_{h}:h\in\mathcal{S}\}\) and \(\mathcal{S}_{2}=\{j_{h}:h\in\mathcal{S}\}\) be the unique values in \((i_{h})_{h\in\mathcal{S}}\) and \((j_{h})_{h\in\mathcal{S}}\). Since there might exists two elements \(h\) and \(h^{\prime}\) in \(\mathcal{S}\) such that \(i_{h}=i_{h^{\prime}}\) or \(j_{h}=j_{h^{\prime}},|\mathcal{S}_{1}|\) and \(|\mathcal{S}_{2}|\) might be strictly less than \(|\mathcal{S}|\). As a result, \(\widehat{p}^{\pi}(x_{L+1}=z=e_{k},x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\forall h \in\mathcal{S})\) only involves random variables \(x_{L+1}\), \(X_{L+1-S_{1}}=\{x_{L+1-i_{h}}\}_{i\in\mathcal{S}_{1}}\), \(z\), \(Z_{-\mathcal{S}_{2}}=\{z_{-j}\}_{j\in\mathcal{S}_{2}}\), which are a subset of the random variables defined in (F.8). Similarly,

\[\mu^{\pi}(x=e_{k},x_{-i_{h}}=e_{k_{h}},\forall h\in\mathcal{S})\cdot\mu^{\pi}(z=e _{k},z_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S})\]

only involves a subset of random variables \(x\), \(X_{-\mathcal{S}_{1}}=\{x_{-i}\}_{i\in\mathcal{S}_{1}}\), \(z\), and \(Z_{-\mathcal{S}_{2}}\). Let us define \(\bar{E}=(E_{0},(E_{-i})_{i\in\mathcal{S}_{1}})\in\mathcal{X}^{|\mathcal{S}_{1}|+1}\) and \(\bar{E}^{\prime}=(E^{\prime}_{0},(E^{\prime}_{-j})_{j\in\mathcal{S}_{2}})\in \mathcal{X}^{|\mathcal{S}_{2}|+1}\). By enumerating \(\bar{E}\)

[MISSING_PAGE_EMPTY:64]

Bounding the Second Absolute Value Term.For the second absolute value term, an analogous argument can be applied. In fact, the proof is simpler because we only need to handle \(\langle v_{l}^{(h)},v_{L+1}^{(h)}\) and \(\langle\widehat{v}^{(h)}(Z),v^{(h)}(Z)\rangle\) and do not have indicators \(\mathds{1}(x_{L+1}=x_{l})\) and \(\mathds{1}(x=z)\).

Similar to the derivation in (F.9) and (F.11),

\[\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\prod_{h \in\mathcal{S}}\langle v_{l}^{(h)},\widehat{v}_{L+1}^{(h)}\rangle\bigg{]}- \mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\bigg{(}\prod_{h \in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X)\rangle\bigg{)}\bigg{]} \bigg{|}\] \[=\bigg{|}\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\bigg{(}\prod_{ h\in\mathcal{S}}\widehat{\sigma}_{-i_{h}}^{(h)}\sigma_{-j_{h}}^{(h)}\bigg{)}\cdot \hskip-8.535827pt\sum_{\{k_{h}\}_{h\in\mathcal{S}}}\hskip-8.535827pt\left( \widehat{p}^{\pi}(x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S}) \right.\] (F.17) \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad-\mu^{\pi}(x_{-i_{h}}=e_{k_{h}},\forall h\in \mathcal{S})\cdot\mu^{\pi}(z_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S}) \bigg{)}\bigg{|}.\]

Similar to (F.12), for any fixed collection of index pairs \((i_{h},j_{h})_{h\in\mathcal{S}}\), we let \(\mathcal{S}_{1}=\{i_{h}:h\in\mathcal{S}\}\) and \(\mathcal{S}_{2}=\{j_{h}:h\in\mathcal{S}\}\) denote the unique values in \((i_{h})_{h\in\mathcal{S}}\) and \((j_{h})_{h\in\mathcal{S}}\). By Lemma F.17, we have

\[\bigg{|}\sum_{\{k_{h}\}_{h\in\mathcal{S}}}\hskip-8.535827pt\left( \widehat{p}^{\pi}(x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\forall h\in\mathcal{S}) -\mu^{\pi}(x_{-i_{h}}=e_{k_{h}},\forall h\in\mathcal{S})\cdot\mu^{\pi}(z_{-j_ {h}}=e_{k_{h}},\forall h\in\mathcal{S})\right)\bigg{|}\] (F.18) \[\qquad\leq 2\big{\|}\widehat{p}^{\pi}(\widetilde{Y}=\cdot, \widetilde{Y}^{\prime}=\cdot)-\mu^{\pi}(\widetilde{Y}=\cdot)\times\mu^{\pi}( \widetilde{Y}^{\prime}=\cdot)\big{\|}_{\mathrm{TV}}\leq\frac{4M}{L}+\frac{8 \sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{L(1-\lambda)\cdot\gamma^{| \mathcal{S}|/2}}.\]

Here we use \(\widetilde{Y}\) and \(\widetilde{Y}^{\prime}\) as placeholders for random variables \(X_{L+1-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\). We note that Lemma F.17 can be applied to any subsets of \(X_{L+1-M:L+1}\) and \((z,Z)\). Therefore, combining (F.16), (F.17), and (F.18), we conclude that

\[\bigg{|}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathbb{E}\bigg{[}\prod_{h \in\mathcal{S}}\langle v_{l}^{(h)},\widehat{v}_{L+1}^{(h)}\rangle\bigg{]}- \mathbb{E}_{(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\bigg{(}\prod_{h \in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X)\rangle\bigg{)}\bigg{]} \bigg{|}\] \[\qquad\leq\frac{4M}{L}+\frac{8\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^ {\pi})+1}}{L(1-\lambda)\gamma^{|\mathcal{S}|/2}}.\]

Note that the second upper bound is dominated by the previous one. This completes the proof of Lemma F.5. 

Lemma F.6 provides an approximation result using the definition of the modified \(\chi^{2}\)-mutual information.

**Lemma F.6**.: _Consider a fixed set \(\mathcal{S}\in[H]_{\leq D}\). For any \(h\in\mathcal{S}\), let \(\widetilde{\sigma}^{(h)}\) and \(\sigma^{(h)}\) be two probability distributions over \([M]\). That is, for any \(i,j\in[M]\), we have \(\widetilde{\sigma}_{-i}^{(h)},\sigma_{-j}^{(h)}\in[0,1]\), and \(\sum_{i=1}^{M}\widetilde{\sigma}_{-i}^{(h)}=\sum_{j=1}^{M}\sigma_{-j}^{(h)}=1\). Moreover, we let \((z,Z)=(z,z_{-M},\ldots,z_{-1})\in\mathcal{X}^{M+1}\) and \((x,X)=(x,x_{-M},\ldots,x_{-1})\in\mathcal{X}^{M+1}\) be two independent random variables sampled from the stationary distribution \(\mu^{\pi}\). We define random variables \(\widetilde{v}^{(h)}(Z)\) and \(v^{(h)}(X)\) as_

\[\widetilde{v}^{(h)}(Z):=\sum_{i\in[M]}\widetilde{\sigma}_{-i}^{(h)}\cdot z_{-i},\qquad\text{and}\qquad v^{(h)}(X):=\sum_{j\in[M]}\sigma_{-j}^{(h)}\cdot x_{-j}.\]

_Let \((i_{h}^{*},j_{h}^{*})_{h\in\mathcal{S}}\) be any fixed collection of index pairs, where \(i_{h}^{*}\in[M]\) and \(j_{h}^{*}\in[M]\) for all \(h\in\mathcal{S}\). We define quantities \(A\) and \(B\) as_

\[A:=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\bigg{(} \sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)}\cdot\prod_ {h\in\mathcal{S}}\langle\widehat{v}^{(h)}(Z),v^{(h)}(X)\rangle,\bigg{]},\]

\[B:=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\prod_{h\in \mathcal{S}}\mathds{1}(x_{-i_{h}^{*}}=z_{-j_{h}^{*}})\cdot\bigg{(}\sum_{k=1}^{d} \frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)}\bigg{]}.\]

_Under Assumption 3.5, it holds that_

\[\bigg{|}\mathbb{E}_{\pi}\left[A\right]-\prod_{h\in\mathcal{S}}\sigma_{-i_{h}^{*} }^{(h)}\widetilde{\sigma}_{-j_{h}^{*}}^{(h)}\cdot B\bigg{|}\leq\Big{(}1-\prod_{h \in\mathcal{S}}\sigma_{-i_{h}^{*}}^{(h)}\widetilde{\sigma}_{-j_{h}^{*}}^{(h)} \Big{)}\cdot I_{\chi^{2}}(\mathcal{S}^{*}).\]Proof of Lemma f.6.: To simplify the notation, we define a signal set \(\Gamma(\mathcal{S})\) and an error set \(\bar{\Gamma}(\mathcal{S})\) as

\[\Gamma(\mathcal{S}):=\{(i_{h}^{\star},j_{h}^{\star})_{h\in\mathcal{S}}\}\,,\qquad \bar{\Gamma}(\mathcal{S}):=\Big{\{}(i_{h},j_{h})_{h\in\mathcal{S}}\in([M]\times [M])^{|\mathcal{S}|}\Big{\}}\setminus\Gamma(\mathcal{S}).\]

Similar to (F.10), we can write \(\mathbb{E}_{\pi}[A]\) as

\[\mathbb{E}_{\pi}[A]=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}} \bigg{[}\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}}\prod_{h\in\mathcal{S}}\sigma _{-i_{h}}^{(h)}\widetilde{\sigma}_{-j_{h}}^{(h)}\cdot\mathds{1}(x_{-i_{h}}=z_ {-j_{h}})\cdot\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{ k})}-1\bigg{)}\bigg{]},\]

where we exchange the order of product and summation. Using the notation \(\bar{\Gamma}(\mathcal{S})\), we can split the summation into two parts:

\[\mathbb{E}_{\pi}[A] =\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \prod_{h\in\mathcal{S}}\sigma_{-i_{h}}^{(h)}\widetilde{\sigma}_{-j_{h}}^{(h)} \cdot\mathds{1}(x_{-i_{h}^{\star}}=z_{-j_{h}^{\star}})\cdot\bigg{(}\sum_{k\in [d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)}\bigg{]}\] \[\quad+\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}} \bigg{[}\sum_{\{(i_{h},j_{h})\}_{h\in\mathcal{S}}\in\bar{\Gamma}(\mathcal{S})} \prod_{h\in\mathcal{S}}\sigma_{-i_{h}}^{(h)}\widetilde{\sigma}_{-j_{h}}^{(h)} \cdot\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}( x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)}\bigg{]}\] \[=\prod_{h\in\mathcal{S}}\sigma_{-i_{h}^{\star}}^{(h)}\widetilde{ \sigma}_{-j_{h}^{\star}}^{(h)}\cdot B\] \[\quad+\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}} \bigg{[}\sum_{(i_{h},j_{h})_{h\in\mathcal{S}}\in\bar{\Gamma}(\mathcal{S})} \prod_{h\in\mathcal{S}}\sigma_{-i_{h}}^{(h)}\widetilde{\sigma}_{-j_{h}}^{(h)} \cdot\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\bigg{(}\sum_{k\in[d]}\frac{\mathds{1}( x=z=e_{k})}{\mu^{\pi}(e_{k})}-1\bigg{)}\bigg{]}.\]

Here last equality holds by the definition of the \(B\) and the fact that \(\widetilde{\sigma}^{(h)}\) and \(\sigma^{(h)}\) are fixed vectors.

Therefore, to prove this lemma, it suffices to upper bound the second term above. To this end, we apply Lemma F.7 stated below for any fixed set of indices \((i_{h},j_{h})_{h\in\mathcal{S}}\in\bar{\Gamma}(\mathcal{S})\). Specifically, let \(\mathcal{S}_{1}=\{i_{h}\colon h\in\mathcal{S}\}\) and \(\mathcal{S}_{2}=\{j_{h}\colon h\in\mathcal{S}\}\) denote the unique values of \((i_{h})_{h\in\mathcal{S}}\) and \((j_{h})_{h\in\mathcal{S}}\). Lemma F.7 implies that

\[\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\prod_{h\in \mathcal{S}}\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\cdot\bigg{(}\sum_{k\in[d]}\frac{ \mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\bigg{)}\bigg{]}\leq I_{\chi^{2}}( \mathcal{S}^{\star}).\] (F.19)

Combining (F.19) with the fact that

\[\sum_{(i_{h},j_{h})_{h\in\mathcal{S}}\in\bar{\Gamma}(\mathcal{S})}\prod_{h\in \mathcal{S}}\sigma_{-i_{h}}^{(h)}\widetilde{\sigma}_{-j_{h}}^{(h)}=1-\prod_{h \in\mathcal{S}}\sigma_{-i_{h}^{\star}}^{(h)}\widetilde{\sigma}_{-j_{h}^{\star} }^{(h)},\]

the desired term is bounded above by \((1-\prod_{h\in\mathcal{S}}\sigma_{-i_{h}^{\star}}^{(h)}\widetilde{\sigma}_{-j_{h }^{\star}}^{(h)})\cdot I_{\chi^{2}}(\mathcal{S}^{\star})\), which concludes the proof. 

**Lemma F.7**.: _Let \(\mathcal{S}\in[H]_{\leq D}\) be a fixed subset and let \(\{(i_{h},j_{h})\}_{h\in\mathcal{S}}\) be a fixed collection of index pairs, where \(i_{h},j_{h}\in[M]\) for all \(h\in\mathcal{S}\). Let \(\mathcal{S}_{1}=\{i_{h}\colon h\in\mathcal{S}\}\) and \(\mathcal{S}_{2}=\{j_{h}\colon h\in\mathcal{S}\}\) denote the unique values of \((i_{h})_{h\in\mathcal{S}}\) and \((j_{h})_{h\in\mathcal{S}}\). We let \((z,Z)=(z,z_{-M},\ldots,z_{-1})\in\mathcal{X}^{M+1}\) and \((x,X)=(x,x_{-M},\ldots,x_{-1})\in\mathcal{X}^{M+1}\) be two independent random variables sampled from the stationary distribution \(\mu^{\pi}\), where \(\pi\) is the transition kernel of the Markov chain and is sampled from prior \(\mathcal{P}\). If Assumption 3.5 holds, it follows that_

\[\mathbb{E}_{\pi\sim\mathcal{P},(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{ \pi}} \bigg{[}\prod_{h\in\mathcal{S}}\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\cdot \bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\bigg{)} \bigg{]}\] \[\leq\frac{1}{2}\left(\widetilde{I}_{\chi^{2}}(\mathcal{S}_{1})+ \widetilde{I}_{\chi^{2}}(\mathcal{S}_{2})\right)\leq\widetilde{I}_{\chi^{2}}( \mathcal{S}^{\star}),\]

_where \(\widetilde{I}_{\chi^{2}}(\mathcal{S})\) is the modified \(\chi^{2}\)-mutual information defined in Definition 3.1 and \(\mathcal{S}^{\star}=\operatorname{argmax}_{\mathcal{S}\in[H]_{\leq D}} \widetilde{I}_{\chi^{2}}(\mathcal{S})\)._

Proof of Lemma f.7.: We first note that it is allowed \(|\mathcal{S}_{1}|\neq|\mathcal{S}_{2}|\) as there could be duplicate values in both \((i_{h})_{h\in\mathcal{S}}\) and \((j_{h})_{h\in\mathcal{S}}\), while \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) are the unique values. In the sequel, we let \(X_{-\mathcal{S}_{1}}\) denote \(\{x_{-i_{h}}\}_{h\in\mathcal{S}}\) and let \(Z_{-\mathcal{S}_{2}}\) denote \(\{z_{-j_{h}}\}_{h\in\mathcal{S}}\), where repeated elements are removed. Moreover, we let \(\{X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}}\}\) be the event that \(x_{-i_{h}}=z_{-j_{h}}\) for all \(h\in\mathcal{S}\). Notice that \(\prod_{h\in\mathcal{S}}\mathds{1}(x_{-i_{h}}=z_{-j_{h}})=\mathds{1}(X_{- \mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}})\). Then, we have

\[\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \prod_{h\in\mathcal{S}}\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\cdot\bigg{(}\sum_{k\in[ d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\bigg{)}\bigg{]}\] (F.20) \[=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \mathds{1}(X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}})\cdot\bigg{(}\sum_{k\in[ d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\bigg{)}\bigg{]}\] \[=\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[} \bigg{(}\sum_{k\in[d]}\frac{\mu^{\pi}(x=e_{k}|X_{-\mathcal{S}_{1}})\cdot\mu^{ \pi}(z=e_{k}|Z_{-\mathcal{S}_{2}})}{\mu^{\pi}(z=e_{k})}-1\bigg{)}\cdot\mathds{ 1}(X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}})\bigg{]}\] \[=\mathbb{E}_{\pi,(X,Z)\sim\mu^{\pi}\times\mu^{\pi}}\bigg{[}\sum_{ k\in[d]}\bigg{(}\frac{\mu^{\pi}(x=e_{k}|X_{-\mathcal{S}_{1}})}{\mu^{\pi}(x=e_{k})}-1 \bigg{)}\cdot\bigg{(}\frac{\mu^{\pi}(z=e_{k}|Z_{-\mathcal{S}_{2}})}{\mu^{\pi}( z=e_{k})}-1\bigg{)}\cdot\mu^{\pi}(z=e_{k})\cdot\mathds{1}(X_{-\mathcal{S}_{1}}=Z_{- \mathcal{S}_{2}})\bigg{]}.\]

Here in the second equality, we take a conditional expectation given \(X_{-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\). The last equality can be verified by direct computation. To simplify the expectation above, we aim to transform the indicator of \(\mathds{1}(X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}})\) into probabilities involving \(X_{-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\). To this end, we need to explicitly enumerate all possible values that \(X_{-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\) can take. This is challenging, as there may be duplicated values in both \(i_{h}\) and \(j_{h}\), and thus \(X_{-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\) can have different sizes. However, since \(\mathcal{S}_{1}\) is a "reduction" of \(\{i_{h}\}_{h\in\mathcal{S}}\), we can revert to the original space and consider \(E=(E_{h})_{h\in\mathcal{S}}\in\mathcal{X}^{|\mathcal{S}|}\) that _respects_ the reduction from \(\{i_{h}\}_{h\in\mathcal{S}}\) to \(\mathcal{S}_{1}\). Here each \(E_{h}\) is the value \(x_{i_{h}}\) takes. In other words, with \((i_{h})_{h\in\mathcal{S}}\) that might have duplicated values, we consider the values taken by \((x_{i_{h}})_{h\in\mathcal{S}}\), with duplicates allowed. And \(E\) has the same duplication structure as \((x_{i_{h}})_{h\in\mathcal{S}}\). In the following, we describe these values by introducing the notion of compatibility.

**Definition F.8** (Compatible Value Set).: _We say that \(E\in\mathcal{X}^{|\mathcal{S}|}\) is compatible with \((i_{h})_{h\in\mathcal{S}}\) if, for any \(h\neq h^{\prime}\) such that \(i_{h}=i_{h^{\prime}}\), we have \(E_{h}=E_{h^{\prime}}\). In other words, the unique values in \(E\) can be indexed by \(\{i_{h}\}_{h\in\mathcal{S}}=\mathcal{S}_{1}\) if \(E\) is compatible with \((i_{h})_{h\in\mathcal{S}}\)._

By this definition, \(E\) is compatible with \((i_{h})_{h\in\mathcal{S}}\) if it respect duplication pattern of \((x_{i_{h}})_{h\in\mathcal{S}}\). If \(i_{h}=i_{h^{\prime}}\), then we know that \(x_{i_{h}}\) and \(x_{i_{h^{\prime}}}\) is the same token. Since \(x_{i_{h}}\) and \(x_{i_{h^{\prime}}}\) take values \(E_{h}\) and \(E_{h^{\prime}}\), we must have \(E_{h}=E_{h^{\prime}}\). As a concrete example, suppose \(\mathcal{S}=\{1,2,3\}\), and the values of \((i_{h})_{h\in\mathcal{S}}\) are given by \((i_{1},i_{2},i_{3})=(1,2,1)\). Therefore, we have \(\mathcal{S}_{1}=\{1,2\}\), which contains the unique values of \((i_{1},i_{2},i_{3})\). Now, let \(E=(E_{1},E_{2},E_{3})\). For \(E\) to be _compatible_ with \((i_{h})_{h\in\mathcal{S}}\), we must have \(E_{1}=E_{3}\) since \(i_{1}=i_{3}\). There is no restriction on \(E_{2}\). So, a compatible value set for this example could be \(E=(a,b,a)\), where \(a\) and \(b\) are elements of \(\mathcal{X}\).

In the sequel, we define \(\mathcal{E}\) as the set of vectors in \(\mathcal{X}^{|\mathcal{S}|}\) that are compatible with both \(\{i_{h}\}_{h\in\mathcal{S}}\) and \(\{j_{h}\}_{h\in\mathcal{S}}\), i.e.,

\[\mathcal{E}=\left\{E\in\mathcal{X}^{|\mathcal{S}|}\mid E\text{ is compatible with both }(i_{h})_{h\in\mathcal{S}}\text{ and }(j_{h})_{h\in\mathcal{S}}\right\}.\]

The compatibility condition allows us to assign \(x_{-i_{h}},z_{-j_{h}}\) the value \(E_{h}\) for all \(h\in\mathcal{S}\) when \(E\in\mathcal{E}\). Under this assignment, the constraint \(X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}}\) is automatically satisfied. We use the notation \(\{X_{-\mathcal{S}_{1}}=E\}\) to denote the event that \(x_{-i_{h}}=E_{h}\) for all \(h\in\mathcal{S}\), and similarly for \(Z_{-\mathcal{S}_{2}}=E\). In particular, we are able to rewrite the indicator \(\mathds{1}(X_{-\mathcal{S}_{1}}=Z_{-\mathcal{S}_{2}})\) as \(\sum_{E\in\mathcal{E}}\mathds{1}(X_{-\mathcal{S}_{1}}=E,Z_{-\mathcal{S}_{2}}=E _{-\mathcal{S}_{2}})\)\(E\)). Then we can rewrite (F.20) by separating \(X_{-\mathcal{S}_{1}}\) and \(Z_{-\mathcal{S}_{2}}\) as

\[\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\biggl{[} \prod_{h\in\mathcal{S}}\mathds{1}\left(x_{-i_{h}}=z_{-j_{h}}\right)\cdot\biggl{(} \sum_{k\in[d]}\frac{\mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\biggr{)} \biggr{]}\\ =\mathbb{E}_{\pi}\biggl{[}\sum_{E\in\mathcal{E}}\sum_{k\in[d]} \biggl{(}\frac{\mu^{\pi}(x=e_{k}|X_{-\mathcal{S}_{1}}=E)}{\mu^{\pi}(x=e_{k})}- 1\biggr{)}\cdot\biggl{(}\frac{\mu^{\pi}(z=e_{k}|Z_{-\mathcal{S}_{2}}=E)}{\mu^{ \pi}(z=e_{k})}-1\biggr{)}\\ \cdot\mu^{\pi}(X_{-\mathcal{S}_{1}}=E)\cdot\mu^{\pi}(Z_{- \mathcal{S}_{2}}=E)\cdot\mu^{\pi}(z=e_{k})\biggr{]}\\ \leq\frac{1}{2}\mathbb{E}_{\pi}\biggl{[}\sum_{E\in\mathcal{E}} \sum_{k\in[d]}\biggl{(}\frac{\mu^{\pi}(x=e_{k}|X_{-\mathcal{S}_{1}})}{\mu^{\pi }(x=e_{k})}-1\biggr{)}^{2}\cdot\mu^{\pi}(z=e_{k})\cdot\bigl{(}\mu^{\pi}(X_{- \mathcal{S}_{1}}=E)\bigr{)}^{2}\biggr{]}\\ +\frac{1}{2}\mathbb{E}_{\pi}\biggl{[}\sum_{E\in\mathcal{E}}\sum_{ k\in[d]}\biggl{(}\frac{\mu^{\pi}(z=e_{k}|Z_{-\mathcal{S}})}{\mu^{\pi}(z=e_{k})}-1 \biggr{)}^{2}\cdot\mu^{\pi}(z=e_{k})\cdot\bigl{(}\mu^{\pi}(Z_{-\mathcal{S}_{2} }=E)\bigr{)}^{2}\biggr{]}.\] (F.21)

where in the last inequality, we apply \(ab\leq a^{2}+b^{2}/2\).

Next, for each \(E\in\mathcal{E}\), consider \(E^{\prime}=(E^{\prime}_{i})_{i\in\mathcal{S}_{1}}\) such that

\[E^{\prime}_{i_{h}}=E_{h},\quad\forall h\in\mathcal{S}.\] (F.22)

Note that for each \(E\in\mathcal{E}\), \(E^{\prime}\) must exist and is unique. The existence follows from the compatibility definition, which allows us to index all the unique values in \(E\) by restricting the indices to the set \(\mathcal{S}_{1}\). The uniqueness is due to the fact that (F.22) completely determines all the values in \(E^{\prime}\) because enumerating over \(i_{h}\) for \(h\in\mathcal{S}\) is just the same as enumerating over \(i\) for \(i\in\mathcal{S}_{1}\). In fact, \(E^{\prime}\) contains all the unique values of \(E\). In the above example, we have \(\mathcal{S}_{1}=\{1,2\}\) and thus \(E^{\prime}=(a,b)\) when \(E=(a,b,a)\).

Since \(E^{\prime}\) is uniquely defined based on \(E\), we are able to define an operator \(\mathcal{J}_{1}\) that maps \(E\in\mathcal{E}\) to \(E^{\prime}\in\mathcal{X}^{|\mathcal{S}_{1}|}\) according to the mapping given in (F.22). Let \(\mathcal{J}_{1}(\mathcal{E})\) be the image of \(\mathcal{E}\) under \(\mathcal{J}_{1}\). It is important to note that for each \(E^{\prime}\in\mathcal{J}_{1}(\mathcal{E})\), there is also a unique pre-image \(E\in\mathcal{E}\) such that \(\mathcal{J}_{1}(E)=E^{\prime}\) according to the rule (F.22). _Therefore, \(\mathcal{J}_{1}\) is an one-to-one mapping from \(\mathcal{E}\) to \(\mathcal{J}_{1}(\mathcal{E})\)._ In the following, for any \(E^{\prime}\in\mathcal{J}_{1}(\mathcal{E})\), we denote by \(\{X_{-\mathcal{S}_{1}}=E^{\prime}\}\) the event where \(x_{-i}=E^{\prime}_{i}\) for all \(i\in\mathcal{S}_{1}\). Equivalently, we have \(x_{-i_{h}}=E^{\prime}_{i_{h}}=E_{h}\) for all \(h\in\mathcal{S}\). Thus, the event \(\{X_{-\mathcal{S}_{1}}=E^{\prime}\}\) is exactly the same as \(\{X_{-\mathcal{S}_{1}}=E\}\) introduced above. Therefore, the first term on the right hand side of (F.21) can be reformulated as

\[\frac{1}{2}\cdot\mathbb{E}_{\pi}\biggl{[}\sum_{E\in\mathcal{E}} \sum_{k\in[d]}\biggl{(}\frac{\mu^{\pi}(x=e_{k}|X_{-\mathcal{S}_{1}})}{\mu^{\pi} (x=e_{k})}-1\biggr{)}^{2}\cdot\mu^{\pi}(z=e_{k})\cdot\mu^{\pi}(X_{-\mathcal{S} _{1}}=E)^{2}\biggr{]}\\ =\frac{1}{2}\cdot\mathbb{E}_{\pi}\biggl{[}\sum_{E^{\prime}\in \mathcal{J}_{1}(\mathcal{E})}\sum_{k\in[d]}\biggl{(}\frac{\mu^{\pi}(x=e_{k}|X_{- \mathcal{S}_{1}})}{\mu^{\pi}(x=e_{k})}-1\biggr{)}^{2}\cdot\mu^{\pi}(z=e_{k}) \cdot\bigl{(}\mu^{\pi}(X_{-\mathcal{S}_{1}}=E^{\prime})\bigr{)}^{2}\biggr{]}\\ \leq\frac{1}{2}\cdot\mathbb{E}_{\pi}\biggl{[}\sum_{E^{\prime}\in \mathcal{X}^{|\mathcal{S}_{1}|}}\sum_{k\in[d]}\biggl{(}\frac{\mu^{\pi}(x=e_{k}|X_{- \mathcal{S}_{1}})}{\mu^{\pi}(x=e_{k})}-1\biggr{)}^{2}\cdot\mu^{\pi}(z=e_{k}) \cdot\bigl{(}\mu^{\pi}(X_{-\mathcal{S}_{1}}=E^{\prime})\bigr{)}^{2}\biggr{]}= \frac{1}{2}\widetilde{I}_{\chi^{2}}(\mathcal{S}_{1}),\]

where the equality follows from the bijection between \(\mathcal{E}\) and \(\mathcal{J}_{1}(\mathcal{E})\), and the last inequality holds by noting that \(\mathcal{J}_{1}(\mathcal{E})\subseteq\mathcal{X}^{|\mathcal{S}_{1}|}\). The last equality follows from the definition of the modified mutual information. The argument for the second term on the right hand side of (F.21) is similar, and we hence conclude that

\[\mathbb{E}_{\pi,(x,X),(z,Z)\sim\mu^{\pi}\times\mu^{\pi}}\biggl{[}\prod_{h\in \mathcal{S}}\mathds{1}(x_{-i_{h}}=z_{-j_{h}})\cdot\biggl{(}\sum_{k\in[d]}\frac{ \mathds{1}(x=z=e_{k})}{\mu^{\pi}(z=e_{k})}-1\biggr{)}\biggr{]}\leq\frac{1}{2} \widetilde{I}_{\chi^{2}}(\mathcal{S}_{1})+\frac{1}{2}\widetilde{I}_{\chi^{2}}( \mathcal{S}_{2}).\]

Lastly, note that \(\widetilde{I}_{\chi^{2}}(\mathcal{S})\leq\widetilde{I}_{\chi^{2}}(\mathcal{S}^{ \star})\) for any \(\mathcal{S}\in[H]_{\leq D}\) by the optimality of \(\mathcal{S}^{\star}\). Hence, we complete the proof of Lemma F.7. 

Lemma F.9 quantifies the approximation error from \(\sigma_{l}\approx\sigma_{l}^{\star}\) and \(y(k)\approx y^{\star}(k)\) for Stage III.

**Lemma F.9**.: _For the transformer model defined in (2.5), define two quantities \(f_{1}\) and \(f_{2}\) as_

\[f_{1} :=\mathbb{E}\left[\sum_{l=M+1}^{L}\sigma_{l}\sum_{k=1}^{d}\biggl{(} \frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\varepsilon}-\frac{y(k)\,\mathds{1 }(x_{L+1}=e_{k})}{y(k)+\varepsilon}\biggr{)}\cdot\prod_{h\in\mathcal{S}^{ \star}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\right],\] \[f_{2} :=\mathbb{E}\left[\sum_{l=M+1}^{L}\sigma_{l}^{\star}\sum_{k=1}^{d }\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{\star}(k)+\varepsilon}- \frac{y^{\star}(k)\,\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+\varepsilon} \biggr{)}\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h}) \right],\]

_where the expectation is taken over all the randomness in the data, and_

\[\sigma_{l}^{\star}:=\frac{\exp\left(a\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds {1}(x_{l-h}=x_{L+1-h})\right)}{\sum_{l^{\prime}=1}^{L}\exp\left(a\cdot\prod_{ h\in\mathcal{S}^{\star}}\mathds{1}(x_{l^{\prime}-h}=x_{L+1-h})\right)},\quad y^{ \star}(k)\,{:=}\,\sum_{l=M+1}^{L}\sigma_{l}^{\star}\,\mathds{1}(x_{l}=e_{k}),\]

_with \(\mathcal{S}^{\star}\) is the optimal information set. Under Assumption 3.5, it holds that_

\[|f_{1}-f_{2}|\leq 12\cdot(1+a\varepsilon^{-1})\cdot(\Delta_{1}+\Delta_{2}),\]

_where \(\Delta_{1}\,{:=}\,1-ps\)- and \(\Delta_{2}\,{:=}\,1-\prod_{h\in\mathcal{S}^{\star}}(\sigma_{-h}^{(h)})^{2}\)._

Proof.: We separate the approximation error into three parts \(|f_{1}-f_{2}|\leq\mathrm{err}_{1}+\mathrm{err}_{2}+\mathrm{err}_{3},\) which are explained in detail as follows.

The First Error Term.Here, the first error \(\mathrm{err}_{1}\) captures the error of replacing \(\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\) with \(\prod_{h\in\mathcal{S}^{\star}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\) in \(f_{2}\):

\[\mathrm{err}_{1} :=\biggl{|}\mathbb{E}\biggl{[}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_ {k\in[d]}\Bigl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{\star}(k)+ \varepsilon}-\frac{y^{\star}(k)\,\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+ \varepsilon}\Bigr{)}\cdot\] \[\cdot\Bigl{(}\prod_{h\in\mathcal{S}^{\star}}\langle v_{l}^{(h)},v _{L+1}^{(h)}\rangle-\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h })\Bigr{)}\biggr{]}\biggr{|},\]

Using Lemma F.2, we have

\[\biggl{|}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k\in[d]}\Bigl{(}\frac{\mathds{1 }(x_{L+1}=x_{l}=e_{k})}{y^{\star}(k)+\varepsilon}-\frac{y^{\star}(k)\, \mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+\varepsilon}\Bigr{)}\biggr{|}\leq 2.\]

Then using Lemma F.1, we conclude that

\[\mathrm{err}_{1}\leq 2\sup_{l\in[L]}\biggl{|}\prod_{h\in\mathcal{S}^{\star}} \langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle-\prod_{h\in\mathcal{S}^{\star}} \mathds{1}(x_{l-h}=x_{L+1-h})\biggr{|}\leq 2(\Delta_{1}+\Delta_{2}).\]

The Second Error Term.The second error term characterizes the difference in \(\sigma_{l}\) and \(\sigma_{l}^{\star}\):

\[\mathrm{err}_{2} :=\biggl{|}\mathbb{E}\biggl{[}\sum_{l=M+1}^{L}(\sigma_{l}^{\star}- \sigma_{l})\cdot\sum_{k\in[d]}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{ y^{\star}(k)+\varepsilon}-\frac{y^{\star}(k)\,\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+ \varepsilon}\biggr{)}\cdot\prod_{h\in\mathcal{S}^{\star}}\langle v_{l}^{(h)},v _{L+1}^{(h)}\rangle\biggr{|}\biggr{]}.\]

To characterize such an error, we invoke equation (53) in Lemma 5.1 of Chen et al. (2022). This lemma states that for \(\sigma\) and \(\sigma^{\star}\) being the output of the softmax function with scaling parameters \(a\) for \(s\) and \(s^{\star}\) respectively, i.e.,

\[\sigma=\frac{\exp(as)}{\sum_{l=M+1}^{L}\exp(as_{l})},\qquad\text{and}\qquad \sigma^{\star}=\frac{\exp(as^{\star})}{\sum_{l=M+1}^{L}\exp(as_{l}^{\star})},\]

it holds that \(\left\lVert\sigma-\sigma^{\star}\right\rVert_{1}\leq 4a\cdot\left\lVert s-s^{ \star}\right\rVert_{\infty}\). Consequently, we have \(\left\lVert\sigma-\sigma^{\star}\right\rVert_{1}\leq 4a\cdot(\Delta_{1}+\Delta_{2})\) by Lemma F.1. We notice that

\[\biggl{|}\sum_{k\in[d]}\biggl{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{ \star}(k)+\varepsilon}-\frac{y^{\star}(k)\,\mathds{1}(x_{L+1}=e_{k})}{y^{ \star}(k)+\varepsilon}\biggr{)}\cdot\prod_{h\in\mathcal{S}^{\star}}\langle v_{l }^{(h)},v_{L+1}^{(h)}\rangle\biggr{|}\leq\max\{\varepsilon^{-1},1\}=\varepsilon^{ -1}.\]

Thus, \(\mathrm{err}_{2}\leq 4a\varepsilon^{-1}\cdot(\Delta_{1}+\Delta_{2})\).

The Third Error Term.The last error term characterizes the difference between \(y^{\star}\) and \(y\):

\[\operatorname{err}_{3} :=\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k \in[d]}\Big{(}\frac{1}{y^{\star}(k)+\varepsilon}-\frac{1}{y(k)+\varepsilon} \Big{)}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\cdot\prod_{h\in\mathcal{S}^{\star} }\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]}\bigg{|}\] \[\qquad+\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}\cdot \sum_{k\in[d]}\Big{(}\frac{y^{\star}(k)}{y^{\star}(k)+\varepsilon}-\frac{y(k) }{y(k)+\varepsilon}\Big{)}\cdot\mathds{1}(x_{L+1}=e_{k})\cdot\prod_{h\in \mathcal{S}^{\star}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]}\bigg{|}.\]

By noting that \(y^{\star}=\sum_{l=M+1}^{L}\sigma_{l}^{\star}x_{l}\) and \(y=\sum_{l=M+1}^{L}\sigma_{l}x_{l}\), we have

\[\left\|y^{\star}-y\right\|_{1}=\bigg{\|}\sum_{l=M+1}^{L}(\sigma_{l}^{\star}- \sigma_{l})x_{l}\bigg{\|}_{1}\leq\sum_{l=M+1}^{L}\left|\sigma_{l}^{\star}- \sigma_{l}\right|_{1}\cdot\left\|x_{l}\right\|_{1}\leq\|\sigma-\sigma^{\star} \|_{1}\leq 4a\cdot(\Delta_{1}+\Delta_{2}).\]

The first term of \(\operatorname{err}_{3}\) can be bounded by

\[\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k \in[d]}\Big{(}\frac{1}{y^{\star}(k)+\varepsilon}-\frac{1}{y(k)+\varepsilon} \Big{)}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\cdot\prod_{h\in\mathcal{S}^{\star} }\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]}\bigg{|}\] \[\quad\leq\sum_{k\in[d]}\frac{|y(k)-y^{\star}(k)|}{(y^{\star}(k)+ \varepsilon)(y(k)+\varepsilon)}\cdot y(k)\,\mathds{1}(x_{L+1}=e_{k})\leq\|y-y ^{\star}\|_{1}\cdot\varepsilon^{-1}\leq 4a\varepsilon^{-1}(\Delta_{1}+\Delta_{2}).\]

Moreover, for the second term of \(\operatorname{err}_{3}\), we have

\[\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}\cdot\sum_{k \in[d]}\bigg{(}\frac{y^{\star}(k)}{y^{\star}(k)+\varepsilon}-\frac{y(k)}{y(k)+ \varepsilon}\bigg{)}\cdot\mathds{1}(x_{L+1}=e_{k})\cdot\prod_{h\in\mathcal{S}^ {\star}}\langle v_{l}^{(h)},v_{L+1}^{(h)}\rangle\bigg{]}\bigg{|}\] \[\quad\leq\sum_{k\in[d]}\frac{|y(k)-y^{\star}(k)|}{(y^{\star}(k)+ \varepsilon)(y(k)+\varepsilon)}\cdot\varepsilon\cdot\mathds{1}(x_{L+1}=e_{k}) \leq 4a\varepsilon^{-1}(\Delta_{1}+\Delta_{2}).\]

It then holds that

\[|f_{1}-f_{2}| \leq\operatorname{err}_{1}+\operatorname{err}_{2}+\operatorname{ err}_{3}\leq 2(\Delta_{1}+\Delta_{2})+4a\varepsilon^{-1}(\Delta_{1}+\Delta_{2})+8a \varepsilon^{-1}(\Delta_{1}+\Delta_{2})\] \[=12\cdot(1+a\varepsilon^{-1})\cdot(\Delta_{1}+\Delta_{2}).\]

Therefore, we complete the proof of Lemma F.9. 

**Lemma F.10**.: _Let us define for brevity,_

\[\widetilde{\mu}_{X}^{\pi}(z,Z)=\widetilde{\mu}^{\pi}(z,Z\,|\,X_{L+1-\mathcal{S }^{\star}})=\frac{\mu^{\pi}(z,Z)\exp\big{(}a\cdot\prod_{h\in\mathcal{S}^{\star }}\mathds{1}(z_{-h}=x_{L+1-h})\big{)}}{\sum_{z^{\prime},Z^{\prime}}\mu^{\pi}(z ^{\prime},Z^{\prime})\exp\big{(}a\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{ 1}(z^{\prime}_{-h}=x_{L+1-h})\big{)}},\]

_where \(Z=(z_{-M},\ldots,z_{-1})\) and \(\mu^{\pi}\) is the stationary distribution of the Markov chain over a window of size \(M+1\). We denote by \(\widetilde{\mu}_{X}^{\pi}(e_{k})=\widetilde{\mu}_{X}^{\pi}(z=e_{k})\) where \(\widetilde{\mu}_{X}^{\pi}(z)\) is the marginal distribution for \(z\) and serves as the population counterpart for \(y^{\star}=\sum_{l=M+1}^{L}\sigma_{l}^{\star}x_{l}\). We define quantity \(A\) and \(B\) as_

\[A :=\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}^{\star}\sum_{k=1}^{ d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{\star}(k)+\varepsilon}- \frac{y^{\star}(k)\,\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+\varepsilon} \bigg{)}\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\bigg{]}.\] \[B :=\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}^{\star}\sum_{k=1}^{ d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\widetilde{\mu}_{X}^{\pi}(e_{k})}- \mathds{1}(x_{L+1}=e_{k})\bigg{)}\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{ l-h}=x_{L+1-h})\bigg{]}.\]

_Under Assumption 3.5, we have_

\[|A-B|\leq\frac{8(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1)^{1/4}+8 \sqrt{M}}{L^{1/2}\cdot\gamma^{|\mathcal{S}^{\star}|+1}}+\frac{2d\varepsilon}{ \gamma}.\]Proof of Lemma F.10.: The proof follows the same arguments as Lemma F.4. We remind the readers that \(y^{\star}(k)\) is also a function of the whole chain \(X\). We note that

\[|A-B| =\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}^{\star}\cdot \bigg{(}\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{y^{\star}(k)+ \varepsilon}-\sum_{k\in[d]}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\widehat{ \mu}_{X}^{\star}(e_{k})}\] \[\qquad\qquad\qquad\qquad\qquad\qquad-\sum_{k\in[d]}\frac{y^{ \star}(k)\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+\varepsilon}+1\bigg{)}\cdot \prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\bigg{]}\bigg{|}\] \[=\bigg{|}\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}^{\star}\cdot \bigg{(}\sum_{k\in[d]}\Big{(}\frac{\widehat{\mu}_{X}^{\star}(e_{k})-y^{\star} (k)}{(y^{\star}(k)+\varepsilon)\cdot\widehat{\mu}_{X}^{\star}(e_{k})}-\frac{ \varepsilon}{(y^{\star}(k)+\varepsilon)\cdot\widehat{\mu}_{X}^{\star}(e_{k})} \Big{)}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\sum_{k \in[d]}\frac{\varepsilon\,\mathds{1}(x_{L+1}=e_{k})}{y^{\star}(k)+\varepsilon} \bigg{)}\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h}) \bigg{]}\bigg{|}.\]

To handle this error, we define three error terms as

\[\mathrm{err}_{1} :=\bigg{|}\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\widehat{\mu}_{X }^{\star}(e_{k})-y^{\star}(k)}{(y^{\star}(k)+\varepsilon)\cdot\widehat{\mu}_{X }^{\star}(e_{k})}\cdot\sum_{l=M+1}^{L}\sigma_{l}^{\star}\cdot\mathds{1}(x_{L+1 }=x_{l}=e_{k})\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h} )\bigg{]}\bigg{|},\] \[\mathrm{err}_{2} :=\bigg{|}\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon}{(y^{ \star}(k)+\varepsilon)\cdot\widehat{\mu}_{X}^{\star}(e_{k})}\cdot\sum_{l=M+1} ^{L}\sigma_{l}^{\star}\cdot\mathds{1}(x_{L+1}=x_{l}=e_{k})\cdot\prod_{h\in \mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\bigg{]}\bigg{|},\] \[\mathrm{err}_{3} :=\bigg{|}\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon}{y^{ \star}(k)+\varepsilon}\cdot\mathds{1}(x_{L+1}=e_{k})\cdot\sum_{l=M+1}^{L} \sigma_{l}^{\star}\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+ 1-h})\bigg{]}\bigg{|}.\]

For the first error term, we have that

\[\mathrm{err}_{1} \leq\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{|\widehat{\mu}_{X}^{ \star}(e_{k})-y^{\star}(k)|}{(y^{\star}(k)+\varepsilon)}\cdot\sum_{l=M+1}^{L} \frac{\sigma_{l}^{\star}\,\mathds{1}(x_{l}=e_{k})}{\widehat{\mu}_{X}^{\star}(e _{k})}\bigg{]}\] \[=\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{|\widehat{\mu}_{X}^{\star} (e_{k})-y^{\star}(k)|}{(y^{\star}(k)+\varepsilon)}\cdot\frac{y^{\star}(k)}{ \widehat{\mu}_{X}^{\star}(e_{k})}\bigg{]}\leq\gamma^{-1}\cdot\mathbb{E}\bigg{[} \sum_{k\in[d]}|\widehat{\mu}_{X}^{\star}(e_{k})-y^{\star}(k)|\bigg{]},\]

where we recall that by assumption, \(\gamma\) provides a lower bound for \(\pi(\cdot\,|\,X_{\text{pa}})\), hence also a lower bound for \(\widetilde{\mu}_{X}^{\star}(e_{k})\). Next, we invoke Proposition F.19 which provides an upper bound for the difference between the empirical and population distributions in terms of the \(\ell_{1}\)-norm:

\[\mathbb{E}\bigg{[}\bigg{\|}\widehat{\mu}_{X}^{\star}(z=\cdot)-y^ {\star}(\cdot)\bigg{\|}_{1}\bigg{]} \leq\frac{4\big{(}(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\, \mu^{\pi})+1}+4M\big{)}^{1/2}}{L^{1/2}\cdot\min_{\pi,x_{L+1},X_{L+1-\mathcal{S} ^{\star}}}\,\mu^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{\star}})}\] \[\leq\frac{4(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi })+1)^{1/4}+8\sqrt{M}}{L^{1/2}\cdot\gamma|^{\mathcal{S}^{\star}+1}}.\] (F.23)

Hence, we control the first error term.

For the second error term, we follow the same procedure and obtain an upper bound as

\[\mathrm{err}_{2}\leq\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon}{\widehat {\mu}_{X}^{\star}(e_{k})}\cdot\sum_{l=M+1}^{L}\frac{\sigma_{l}^{\star}\, \mathds{1}(x_{l}=e_{k})}{(y^{\star}(k)+\varepsilon)}\bigg{]}\leq\mathbb{E} \bigg{[}\sum_{k\in[d]}\frac{\varepsilon}{\widehat{\mu}_{X}^{\star}(e_{k})} \bigg{]}\leq\gamma^{-1}d\varepsilon.\]For the last error term, it holds that

\[\operatorname{err}_{3} \leq\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon}{y^{\star}(k) +\varepsilon}\cdot\mathds{1}(x_{L+1}=e_{k})\bigg{]}\] \[\leq\bigg{|}\mathbb{E}\bigg{[}\sum_{k\in[d]}\frac{\varepsilon\, \mathds{1}(x_{L+1}=e_{k})}{\tilde{\mu}_{X}^{\pi}(e_{k})+\varepsilon}\bigg{]} \bigg{|}+\bigg{|}\sum_{k\in[d]}\mathbb{E}\bigg{[}\frac{\varepsilon(y^{\star}(k) -\tilde{\mu}_{X}^{\pi}(e_{k}))\cdot\mathds{1}(x_{L+1}=e_{k})}{(\tilde{\mu}_{X }^{\pi}(e_{k})+\varepsilon)(y^{\star}(k)+\varepsilon)}\bigg{]}\bigg{|}\] \[\leq\frac{\varepsilon}{\gamma}+\mathbb{E}\bigg{[}\sum_{k\in[d]} \frac{|y^{\star}(k)-\tilde{\mu}_{X}^{\pi}(e_{k})|}{\gamma}\bigg{]}\leq\frac{ \varepsilon}{\gamma}+\frac{4(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^ {\pi})+1)^{1/4}+8\sqrt{M}}{L^{1/2}\cdot\gamma|^{\mathcal{S}^{\star}|+1}}.\]

where the last inequality follows directly from (F.23).

In summary, the difference between \(f_{2}\) and \(f_{3}\) is bounded by

\[|f_{2}-f_{3}|\leq\operatorname{err}_{1}+\operatorname{err}_{2}+\operatorname{ err}_{3}\leq\frac{8(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1)^{1/4} +8\sqrt{M}}{L^{1/2}\cdot\gamma|^{\mathcal{S}^{\star}|+1}}+\frac{2d\varepsilon }{\gamma},\]

which completes our proof of Lemma F.10. 

The following lemmas are for analyzing the error \(|f_{3}-f_{4}|\) for Stage III.

**Lemma F.11**.: _We define_

\[A :=\mathbb{E}\bigg{[}\sum_{l=M+1}^{L}\sigma_{l}^{\star}\cdot \sum_{k=1}^{d}\bigg{(}\frac{\mathds{1}(x_{L+1}=x_{l}=e_{k})}{\tilde{\mu}_{X}^{ \pi}(e_{k})}-\mathds{1}(x_{L+1}=e_{k})\bigg{)}\cdot\prod_{h\in\mathcal{S}^{ \star}}\mathds{1}(x_{l-h}=x_{L+1-h})\bigg{]},\] \[B :=\mathbb{E}_{X,(z,Z)\sim\tilde{\mu}_{X}^{\pi}}\bigg{[}\sum_{k=1} ^{d}\bigg{(}\frac{\mathds{1}(x_{L+1}=z=e_{k})}{\tilde{\mu}_{X}^{\pi}(e_{k})}- \mathds{1}(x_{L+1}=e_{k})\bigg{)}\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1 }(z_{l-h}=x_{L+1-h})\bigg{]},\]

_where_

\[\sigma_{l}^{\star} :=\frac{\exp\big{(}a\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1 }(x_{l-h}=x_{L+1-h})\big{)}}{\sum_{l^{\prime}=1}^{L}\exp\big{(}a\cdot\prod_{h \in\mathcal{S}^{\star}}\mathds{1}(x_{l-h}=x_{L+1-h})\big{)}},\] \[\tilde{\mu}_{X}^{\pi}(z,Z) :=\tilde{\mu}^{\pi}(z,Z\,|\,X_{L+1-\mathcal{S}^{\star}})=\frac{ \mu^{\pi}(z,Z)\exp\big{(}a\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(z_{-h }=x_{L+1-h})\big{)}}{\sum_{z^{\prime},Z^{\prime}}\mu^{\pi}(z^{\prime},Z^{ \prime})\exp\big{(}a\cdot\prod_{h\in\mathcal{S}^{\star}}\mathds{1}(z_{-h}^{ \prime}=x_{L+1-h})\big{)}}.\]

_Under Assumption 3.5, we have_

\[|A-B|\leq\frac{8\gamma^{-1}(1-\lambda)^{-1/2}(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1)^{1/4}+16\gamma^{-1}\sqrt{M}}{L^{1/2}\cdot\gamma|^{\mathcal{S}^{\star} |+1}}.\]

Proof of Lemma F.11.: For \(Z=(z_{-M},\ldots,z_{-1})\) and \(Z^{\prime}=(z_{-M}^{\prime},\ldots,z_{-1}^{\prime})\), we let \(Z_{-\mathcal{S}^{\star}}=(z_{-h})_{h\in\mathcal{S}^{\star}}\), we define

\[\widehat{\mu}_{X}^{\pi}(z,Z)=\frac{1}{L-M}\sum_{l=M+1}^{L} \mathds{1}(x_{l}=z,X_{l-M:l-1}=Z),\] \[R(Z,X_{L+1-\mathcal{S}^{\star}})=\exp\bigg{(}a\cdot\prod_{h\in \mathcal{S}^{\star}}\mathds{1}(z_{-h}=x_{L+1-h})\bigg{)}.\]

Using these notations, we can rewrite the normalizing factor in \(\tilde{\mu}_{X}^{\pi}\) and \(\sigma_{l}^{\star}\) respectively as

\[\Phi=\sum_{z,Z}\mu^{\pi}(z,Z)\cdot R(Z,X_{L+1-\mathcal{S}^{\star}}),\quad \widehat{\Phi}=\sum_{z,Z}\widehat{\mu}_{X}^{\pi}(z,Z)\cdot R(Z,X_{L+1-\mathcal{ S}^{\star}}).\]

We also define

\[\phi(z,Z_{-\mathcal{S}^{\star}})=\mu^{\pi}(z,Z_{-\mathcal{S}^{\star}})\cdot R(Z_ {-\mathcal{S}^{\star}},X_{-\mathcal{S}^{\star}}),\quad\widehat{\phi}(z,Z_{- \mathcal{S}^{\star}})=\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{\star}})\cdot R (Z_{-\mathcal{S}^{\star}},X_{L+1-\mathcal{S}^{\star}}).\]If we further define \(\widehat{\nu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})=\sum_{l=M+1}^{L}\mathds{1}(x_{l}=z,X_{l-\mathcal{S}^{*}}=Z_{-\mathcal{S}^{*}})\), then we have

\[\widehat{\nu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})=\frac{\widehat{\mu }_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{-\mathcal{S}^{*}},X_{L+1- \mathcal{S}^{*}})}{\widehat{\Phi}}=\frac{\widehat{\phi}(z,Z_{-\mathcal{S}^{*}}) }{\widehat{\Phi}},\] \[\widetilde{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})=\frac{\mu^{\pi }(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{-\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}}) }{\Phi}=\frac{\phi(z,Z_{-\mathcal{S}^{*}})}{\Phi}.\]

Using the above definitions and relationship, \(A\) and \(B\) can be rewritten as

\[A=\mathbb{E}\bigg{[}\sum_{k=1}^{d}\frac{\widehat{\phi}(e_{k},X_{L+1-\mathcal{ S}^{*}})}{\widehat{\Phi}\cdot\widetilde{\mu}_{X}^{\pi}(e_{k})}-\frac{\widehat{ \phi}(X_{L+1-\mathcal{S}^{*}})}{\widehat{\Phi}}\bigg{]},\quad B=\mathbb{E} \bigg{[}\sum_{k=1}^{d}\frac{\phi(e_{k},X_{L+1-\mathcal{S}^{*}})}{\Phi\cdot \widetilde{\mu}_{X}^{\pi}(e_{k})}-\frac{\phi(X_{L+1-\mathcal{S}^{*}})}{\Phi} \bigg{]}.\]

Therefore, the difference between \(A\) and \(B\) is given by

\[|A-B| \leq\frac{2}{\gamma}\cdot\mathbb{E}\bigg{[}\sum_{z,Z_{-\mathcal{ S}^{*}}}\bigg{|}\frac{\phi(z,Z_{-\mathcal{S}^{*}})}{\Phi}-\frac{\widehat{\phi}(z,Z_{- \mathcal{S}^{*}})}{\widehat{\Phi}}\bigg{]}\bigg{|}\leq\frac{2}{\gamma}\cdot \mathbb{E}\bigg{[}\sum_{z,Z_{-\mathcal{S}^{*}}}\bigg{|}\widetilde{\mu}_{X}^{ \pi}(z,Z_{-\mathcal{S}^{*}})-\widehat{\nu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}) \bigg{|}\bigg{]}\] \[\leq\frac{8\gamma^{-1}\cdot\big{(}(1-\lambda)^{-1}\sqrt{D_{ \chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}+4M\big{)}^{1/2}}{L^{1/2}\cdot\min_{x_{L+1 },X_{L+1-\mathcal{S}^{*}}}\,\mu^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}.\]

where the last inequality follows from the result in Proposition F.19. Invoking the lower bound \(\mu^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})\geq\gamma^{|\mathcal{S}^{*}|+1}\), we complete the proof of Lemma F.11. 

### Lemmas on Concentration of Markov Chain

Recall that we previously define \(X=(x_{1},\ldots,x_{L})\) as the observed sequence and \(x_{L+1}\) as the value at time \(L+1\) to be predicted. For generality, we will use \(X=(x_{1},\ldots,x_{L+1})\) to denote the whole sequence in the following proof. We denote by \(p^{\pi}(\cdot)\) the joint distribution for the sequence \(X\) with kernel \(\pi\). Recall that we have the parent set \(\mathsf{pa}=\{-r_{1},\ldots,-r_{n}\}\), and as the start of a chain, we sample the first \(r_{n}\) tokens by \((x_{1},\ldots,x_{r_{n}})\sim\mu_{0}\).

In the sequel, we will study concentration properties of the Markov chain \(X\) for a window of tokens with window size at most \(M\), where \(M>r_{n}\). To proceed, let us consider a fixed set \(\mathcal{S}\subseteq[M]\). For any \(l\in[M+1,L+1]\), we define \(Y_{l}=(x_{l},X_{l-\mathcal{S}})\) as a new vector containing the token at position \(l\) and also the tokens in the past \(\mathcal{S}\) positions prior to \(x_{l}\). Here, we follow the convention that \(X_{l-\mathcal{S}}=(X_{l-i})_{i\in\mathcal{S}}\). We also consider another fixed subset \(\mathcal{S}^{\prime}\subseteq[M]\) and similarly define \(Y^{\prime}_{l}=(x_{l},X_{l-\mathcal{S}^{\prime}})\).

The concentration properties of the Markov chain are rooted in the fact that when conditioning on all the parents, the current token is independent of all the past tokens. Given the parent set structure \(\mathsf{pa}=\{-r_{1},\ldots,-r_{n}\}\), we aim to make \(Y_{L+1}\) approximately independent of \(Y_{l}\) by conditioning on some intermediate parent sets. To this end, we define \(A=(x_{L+1-M},\ldots,x_{L-M+r_{n}})\in\mathcal{X}^{r_{n}}\) and \(B_{l}=(x_{l-r_{n}+1},\ldots,x_{l})\in\mathcal{X}^{r_{n}}\) as these intermediate parent sets. By the Markov property and the parent set structure, we have the following conditional independence relations:

\[Y_{L+1}\perp\!\!\!\perp(B_{l},Y_{l})\,|\,A,\quad(Y_{L+1},A)\perp\!\!\!\perp Y_{l} \,|\,B_{l},\quad\forall l=M+1,\ldots,L-M+r_{n}.\]

To illustrate, let us consider the first condition \(Y_{L+1}\perp\!\!\!\perp(B_{l},Y_{l})\,|\,A\). When \(l\leq L-M+r_{n}\), the \(B_{l}\) and \(Y_{l}\) are both contained in the history \(\{x_{k}\colon k\leq L-M+r_{n}\}=A\cup\{x_{k}\colon k\leq L-M\}\). When conditioning on \(A\), the randomness of \((B_{l},Y_{l})\) is measurable by the \(\sigma\)-algebra generated by the "past" \(\{x_{k}\colon k\leq L-M\}\). Moreover, the randomness of \(Y_{L+1}\) is measurable by the \(\sigma\)-algebra generated by the "future" \(\{x_{k}\colon k\in[L+1-M+r_{n},L+1]\}\) when conditioning on \(A\). Notice that the parent to the any element in the future \(\{x_{k}\colon k\in[L+1-M+r_{n},L+1]\}\) is either contained in \(A\), or can be generated conditioned on \(A\) without touching further history \(\{x_{k}\colon k\leq L-M\}\). Thus, by the Markov property, conditioning on \(A\), \(Y_{L+1}\) is independent of the past \(\{x_{k}\colon k\leq L-M\}\), and in particular, \((B_{l},Y_{l})\). Similarly, since \(B\) contains the parent of \(x_{l+1}\), conditioning on \(B\), \(Y_{l}\) is independent of \(x_{l+1}\) and later tokens. Moreover, given \(B\), the randomness of \(Y_{l}\) comes from the randomness of \(x_{l-M},\ldots,x_{l-r_{n}}\). Since \(l\leq L-M+r_{n}\), we have \(L+1-M\geq l+1-r_{n}\). As a result, conditioning on \(B\), the randomness of \((Y_{L+1},A)\) comes from tokens generated no earlier than \(x_{l+1}\). Therefore, \((Y_{L+1},A)\) and \(Y_{l}\) are conditionally independent given \(B_{l}\). We visualize the definition of \(Y_{L+1}\), \(A\), \(B_{l}\), and \(Y_{l}\) in Figure 10Similarly, for \(Y^{\prime}_{l}=(x_{l},X_{l-\mathcal{S}^{\prime}})\) defined using the subset \(\mathcal{S}^{\prime}\), we also parallel conditional independence relations:

\[Y^{\prime}_{L+1}\perp\!\!\!\perp(B_{l},Y^{\prime}_{l})\,|\,A,\quad(Y^{\prime}_{L +1},A)\perp\!\!\!\perp Y^{\prime}_{l}\,|\,B_{l},\quad\forall l=M+1,\dots,L-M+r_ {n}.\]

In particular, we also have

\[Y_{L+1}\perp\!\!\!\perp(B_{l},Y^{\prime}_{l})\,|\,A,\quad(Y_{L+1},A)\perp\!\! \!\perp Y^{\prime}_{l}\,|\,B_{l},\quad\forall l=M+1,\dots,L-M+r_{n}.\] (F.24)

Using \(\{Y_{l},Y^{\prime}_{l}\}\), we define a joint distribution \(\widehat{p}^{\pi}\) over \(2+|\mathcal{S}|+|\mathcal{S}^{\prime}|\) tokens as follows. For any \(E\in\mathcal{X}^{|\mathcal{S}|+1}\) and \(E^{\prime}\in\mathcal{X}^{|\mathcal{S}^{\prime}|+1}\), the probability mass function of \(\widehat{p}^{\pi}\) is defined as

\[\widehat{p}^{\pi}(Y_{L+1}=E,Y^{\prime}=E^{\prime})\] \[:= \,\frac{1}{L-M}\sum_{l=M+1}^{L}p^{\pi}(Y_{L+1}=E,Y^{\prime}_{l}=E ^{\prime})\] \[= \,\frac{1}{L-M}\sum_{l=M+1}^{L}\sum_{A,B_{l}}\mu^{\pi}(Y_{L+1}=E \,|\,A)\cdot P_{\pi}^{L-M+r_{n}-l}(A\,|\,B_{l})\cdot p^{\pi}(Y^{\prime}_{l}=E ^{\prime}\,|\,B_{l})\cdot p^{\pi}(B_{l}).\] (F.25)

Here, \(Y^{\prime}\) is just a placeholder for \(Y^{\prime}_{l}\) as \(\widehat{p}\) takes an average over \(l\) and does not depend on any specific position index. The summation \(\sum_{A,B_{l}}\) means we sum over all possible values that \(A\) and \(B_{l}\) can take. In the last line of (F.25), we decompose the joint distribution \(p^{\pi}(Y_{L+1}=E,Y^{\prime}_{l}=E^{\prime})\) into the product of the conditional distributions by the Markov property in (F.24). That is,

\[p^{\pi}(Y_{L+1}=E,Y^{\prime}_{l}=E^{\prime}) =\sum_{A,B_{l}}p^{\pi}(Y_{L+1}=E,Y^{\prime}_{l}=E^{\prime},A,B_{l})\] \[=\sum_{A,B_{l}}p^{\pi}(B_{l})\cdot p^{\pi}(Y_{L+1}=E,A\,|\,B_{l}) \cdot p^{\pi}(Y_{l}=E^{\prime}\,|\,B_{l})\] \[=\sum_{A,B_{l}}p^{\pi}(Y_{L+1}=E\,|\,A)\cdot p^{\pi}(A\,|\,B_{l}) \cdot p^{\pi}(Y_{l}=E^{\prime}\,|\,B_{l})\cdot p^{\pi}(B_{l}).\]

Here the second equality follows from the fact that \((Y_{L+1},A)\perp\!\!\!\perp Y^{\prime}_{l}\,|\,B_{l}\) and the last equality follows from the fact that \(Y_{L+1}\perp\!\!\!\perp(B_{l},Y^{\prime}_{l})\,|\,A\), which implies \(p^{\pi}(Y_{L+1}=E\,|\,A,B_{l})=p^{\pi}(Y_{L+1}=E\,|\,A)\). Moreover, we denote by \(P_{\pi}^{i}\) the \(i\)-step transition kernel of the chain, which corresponds to the \(i\)-th power of the transition matrix \(P_{\pi}\). Here, we are following the convention in the main text that

\[P_{\pi}(Z^{\prime},Z)=\pi(z^{\prime}_{l}\,|\,\,Z_{\text{pa}(l)})\cdot\mathbf{1 }(Z^{\prime}_{l-r_{n}+1:-1}=Z_{l-r_{n}+1:-1}).\] (F.26)

In the following, we always consider a fixed transition kernel \(\pi\) and omit the superscript/subscript \(\pi\) in the matrix notation. We denote the transition matrix by \(P_{\pi}\) and the stationary distribution by \(\mu^{\pi}\) for a window of length \(r_{n}\). For the transition matrix, we index each row by the next \(r_{n}\)-window \(Z^{\prime}\) and each column by the current \(r_{n}\)-window \(Z\). Under this notation, since both \(A\) and \(B_{l}\) have lengths \(r_{n}\), we have

\[p^{\pi}(A\,|\,B_{l})=P_{\pi}^{L-M+r_{n}-l}(A,B_{l}).\] (F.27)

Here \(P_{\pi}^{L-M+r_{n}-l}(A\,|\,B_{l})\) corresponds to the \((A,B_{l})\)-entry of the matrix \((P_{\pi})^{L-M+r_{n}-l}\). Combining (F.24) and (F.27), we obtain the last equality in (F.25).

Figure 10: Illustration of the definition of \(Y_{L+1}\), \(A\), \(B_{l}\), and \(Y_{l}\). When conditioned on \(A\), \(Y_{L+1}\) is independent of \((B_{l},Y_{l})\). When conditioned on \(B_{l}\), \(Y_{l}\) is independent of \((A,Y_{L+1})\).

In the sequel, to simplify the notation, we write \(P_{\pi}\) and \(\mu^{\pi}\) as \(P\) and \(\mu\) respectively. Let us consider the reweighted transition kernel

\[K:=\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}^{-1}\cdot P\cdot\operatorname{ diag}\left(\sqrt{\mu}\right),\]

where \(\sqrt{\mu}\) is the element-wise square root of \(\mu\). Since the transition matrix is _primitive_ by assumption and having only one eigenvalue with value one on its spectral circle, we also have for \(K\) that the leading eigenvalue is one with eigenvector \(\sqrt{\mu}\), i.e. \(\sqrt{\mu}=K\sqrt{\mu}\) and \(\sqrt{\mu^{\top}}=\sqrt{\mu^{\top}}K\). However, the projection in the leading eigenspace (or the Perron projection) is not of our interest. The following property of \(K\) will be useful in the subsequent proof.

**Proposition F.12**.: _For the reweighted transition matrix \(K\), we have for any integer \(i\geq 0\)_

\[P^{i}-\mu\mathbf{1}^{\top}=\operatorname{diag}\left(\sqrt{\mu}\right)\cdot \bigl{(}K-\sqrt{\mu}\sqrt{\mu}^{\top}\bigr{)}^{i}\cdot\operatorname{diag} \bigl{(}\sqrt{\mu}^{-1}\bigr{)}\]

Proof of Proposition F.12.: \[P^{i}-\mu\mathbf{1}^{\top} =\Bigl{(}\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}\cdot K \cdot\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}^{-1}\Bigr{)}^{i}-\mu \mathbf{1}^{\top}\] \[=\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}\cdot\Bigl{(}K^{i} -\sqrt{\mu}\sqrt{\mu}^{\top}\Bigr{)}\cdot\operatorname{diag}\bigl{(}\sqrt{\mu }\bigr{)}^{-1}\] \[=\operatorname{diag}\left(\sqrt{\mu}\right)\cdot\bigl{(}K-\sqrt{ \mu}\sqrt{\mu}^{\top}\bigr{)}^{i}\cdot\operatorname{diag}\bigl{(}\sqrt{\mu}^{ -1}\bigr{)},\]

where the last equality holds by noting that \(K-\sqrt{\mu}\sqrt{\mu}^{\top}\) project \(\sqrt{\mu}\) to the zero vector, and for any \(v\perp\sqrt{\mu}\), we have \((K-\sqrt{\mu}\sqrt{\mu}^{\top})v=Kv\). Thus for any test vector \(x\):

\[(K-\sqrt{\mu}\sqrt{\mu}^{\top})^{i}x =(K-\sqrt{\mu}\sqrt{\mu}^{\top})^{i}(x-\langle\sqrt{\mu},x\rangle \cdot\sqrt{\mu})\] \[=K^{i}(x-\langle\sqrt{\mu},x\rangle\cdot\sqrt{\mu})=(K^{i}-\sqrt{ \mu}\sqrt{\mu}^{\top})x.\]

This completes the proof of Proposition F.12. 

Indeed, the second largest eigenvalue of \(K\) (in magnitude) determines the mixing rate of the chain. Let \(\lambda\) denote the eigenvalue of \(K\) with the second largest magnitude.

Furthermore, if the transition kernel \(\pi\) admits a lower bound \(\gamma>0\), then we can guarantee that both \(p^{\pi}\) and \(\mu^{\pi}\) admit a uniform lower bound.

**Proposition F.13** (Uniform Lower Bound).: _Suppose \(\pi(\cdot\,|\,X_{\mathsf{pa}})\geq\gamma\) uniformly for some \(\gamma>0\) and \(\mathsf{pa}=\{-r_{1},\ldots,-r_{n}\}\). Suppose \(X_{1:r_{n}}\sim\mu_{0}(\cdot)\) where \(\mu_{0}\in\Delta(\mathcal{X}^{r_{n}})\). Then for any \(S\) tokens \(x_{l_{1}},x_{l_{2}},\ldots,x_{l_{S}}\) such that \(l_{s}\geq r_{n}\) for any \(s\in[S]\), we have_

\[p^{\pi}(x_{l_{1}},\ldots,x_{l_{S}})\geq\gamma^{S}.\]

Using Proposition F.13, we show that the transition matrix \(P_{\pi}\) is primitive.

**Corollary F.14** (Uniform Lower Bound Implies Primitive Transition).: _Under the condition of Proposition F.13, with \(\pi(\cdot\,|\,X_{\mathsf{pa}})\geq\gamma>0\), the transition matrix defined in (F.26) is primitive._

Proof of Corollary F.14.: If the initial distribution is set to be any one-hot vector in \(\Delta(\mathcal{X}^{r_{n}})\), and taking \(x_{l_{1}},\ldots,x_{l_{S}}\) in Proposition F.13 to be \(x_{r_{n}+1},\ldots,x_{2r_{n}}\), we conclude that \(p^{\pi}(X_{r_{n}+1:2r_{n}}\,|\,X_{1:r_{n}})>0\) holds for any \(X_{r_{n}+1:2r_{n}},X_{1:r_{n}}\in\mathcal{X}^{r_{n}}\). Recall from the definition that for a primitive matrix \(P\), we can find some positive integer \(k\) such that \(P^{k}\) has all positive entries. For our case, we can set \(k=r_{n}\) and everything follows by noting that \(p^{\pi}(X_{r_{n}+1:2r_{n}}\,|\,X_{1:r_{n}})=P^{r_{n}}_{\pi}(X_{r_{n}+1:2r_{n}},X _{1:r_{n}})\). 

Another corollary of Proposition F.13 is that, if we take \(\mu_{0}=\mu^{\pi}\), which is the stationary distribution, we can replace \(p^{\pi}\) in Proposition F.13 by \(\mu^{\pi}\).

**Corollary F.15**.: _Suppose \(\pi(\cdot\,|\,X_{\mathsf{pa}})\geq\gamma\) uniformly for some \(\gamma>0\) and \(\mathsf{pa}=\{-r_{1},\ldots,-r_{n}\}\). For the stationary distribution \(\mu^{\pi}\) and \(S\) tokens \(x_{l_{1}},x_{l_{2}},\ldots,x_{l_{S}}\) such that \(l_{s}\geq r_{n}\) for any \(s\in[S]\), we have \(\mu^{\pi}(x_{l_{1}},\ldots,x_{l_{S}})\geq\gamma^{S}\)._

We prove Proposition F.13 as follows.

Proof of Proposition F.13.: Without loss of generality, suppose that \(M\leq l_{1}<l_{2}<\ldots<l_{S}\). We will prove the statement by induction on the number of tokens \(S\). If \(S=1\), we can rewrite

\[p^{\pi}(x_{l_{1}})=\sum_{X_{\texttt{pa}(l_{1})}}\pi(x_{l_{1}}\,|\,X_{\texttt{pa} (l_{1})})p^{\pi}(X_{\texttt{pa}(l_{1})})\geq\sum_{X_{\texttt{pa}(l_{1})}}\gamma \cdot p^{\pi}(X_{\texttt{pa}(l_{1})})\geq\gamma.\]

Now, suppose the statement holds for \(1,2,\ldots,S-1\). Let \(Y=x_{l_{1}},\ldots,x_{l_{s-1}}\). Then, we have

\[p^{\pi}(x_{l_{1}},\ldots,x_{l_{S}}) =\sum_{X_{\texttt{pa}(l_{S})}\setminus Y}\pi(x_{l_{S}}\,|\,X_{ \texttt{pa}(l_{S})})\cdot p^{\pi}(Y)\cdot p^{\pi}(X_{\texttt{pa}(l_{S})} \setminus Y)\] \[\geq\sum_{X_{\texttt{pa}(l_{S})}\setminus Y}\gamma\cdot p^{\pi}( Y)\cdot p^{\pi}(X_{\texttt{pa}(l_{S})}\setminus Y)=\gamma\cdot p^{\pi}(Y)\geq \gamma^{S},\]

where the last inequality holds by the induction condition. Hence, we finish the proof. 

Before analyzing \(\widehat{p}^{\pi}\), we first study a simpler convergence result: quantifying the closeness between \(\sum_{l=M+1}^{L}\eta^{L-l}p^{\pi}(B_{l}=b)/\sum_{l=M+1}^{L}\eta^{L-l}\) and \(\mu^{\pi}(b)\) for certain values of \(\eta\in(0,1]\).

**Lemma F.16**.: _Following the notations introduced above, for the Markov chain with parent set \(\texttt{pa}=\{-r_{1},\ldots,-r_{n}\}\), let \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) be the \(\chi^{2}\)-divergence between the initial distribution \(\mu_{0}\) and the stationary distribution \(\mu^{\pi}\) over the first \(r_{n}\) tokens. Take any \(\mathcal{S}\subseteq[M]\) and let \(Y_{l}=(x_{l},X_{l-\mathcal{S}})\) for \(l=M+1,\ldots,L+1\). Suppose \(L/2\geq M\geq r_{n}\). We have_

\[\left\|\frac{\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=\cdot)}{L-M}-\mu^{\pi}( Y_{L+1}=\cdot)\right\|_{\mathrm{TV}} \leq\frac{2\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{L(1-\lambda)},\] (F.28) \[\|p^{\pi}(Y_{L+1}=\cdot)-\mu^{\pi}(Y_{L+1}=\cdot)\|_{\mathrm{TV}} \leq\lambda^{L-M}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}.\] (F.29)

Proof of Lemma F.16.: Let \(c_{l}=\eta^{L-l}/\sum_{l=r_{n}}^{L-M+r_{n}}\eta^{L-l}\), where \(\eta\in[0,1]\) is a constant to be determined. Denote by \(\mu_{0}\), a vector of length \(|\mathcal{X}|^{r_{n}}\), the initial distribution of the chain. We begin by quantifying the total variation (TV) distance:

\[\left\|\frac{\sum_{l=r_{n}}^{L-M+r_{n}}\lambda^{L-l}p^{\pi}(B_{l}=\cdot)}{\sum _{l=r_{n}}^{L-M+r_{n}}\lambda^{L-l}}-\mu^{\pi}(\cdot)\right\|_{\mathrm{TV}}= \left\|\,\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\cdot(p^{\pi}(B_{l}=\cdot)-\mu^{\pi}( \cdot))\,\right\|_{\mathrm{TV}}.\]

Let \(b\in\mathcal{X}^{r_{n}}\), representing the value for a length-\(r_{n}\) window. Using matrix notation, we have:

\[\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\,(p^{\pi}(B_{l}=b)-\mu^{\pi}(b)) =\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\cdot\mathbf{1}_{b}^{\top}P^{l-r_{ n}}(\mu_{0}-\mu)=\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\cdot\mathbf{1}_{b}^{\top}(P^{l-r_{ n}}-\mu\mathbf{1}^{\top})\mu_{0}\] \[=\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\cdot\mathbf{1}_{B}^{\top}\mathrm{ diag}\,(\sqrt{\mu})\left(K-\sqrt{\mu}\sqrt{\mu}^{\top}\right)^{l-r_{n}}\mathrm{ diag}\,(\sqrt{\mu})^{-1}\,\mu_{0},\]

where \(\mathbf{1}_{b}\) is the indicator vector corresponding to \(b\). The last equality follows from Proposition F.12. For any test vector \(u\in\{0,1\}^{|\mathcal{X}|^{r_{n}}}\), using the variational representation of TV distance:

\[\left\|\,\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\,(p^{\pi}(B_{l}=\cdot)- \mu^{\pi}(\cdot))\,\right\|_{\mathrm{TV}}=\max_{u\in\{0,1\}^{|\mathcal{X}|^{r_{ n}}}}u^{\top}\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\,(p^{\pi}(B_{l}=\cdot)-\mu^{\pi}( \cdot))\] \[\quad=\max_{u\in\{0,1\}^{|\mathcal{X}|^{r_{n}}}}\sum_{l=r_{n}}^{L- M+r_{n}}c_{l}\cdot u^{\top}\mathrm{diag}\,(\sqrt{\mu})\cdot\left(K-\sqrt{\mu}\sqrt{\mu} ^{\top}\right)^{l-r_{n}}\cdot\mathrm{diag}\,(\sqrt{\mu})^{-1}\cdot\mu_{0}\] \[\quad\leq\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\cdot\lambda^{l-r_{n}} \cdot\left\|\mathrm{diag}\,(\sqrt{\mu})^{-1}\cdot\mu_{0}\right\|_{2}=\sum_{l=r_{ n}}^{L-M+r_{n}}c_{l}\cdot\lambda^{l-r_{n}}\cdot\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1},\] (F.30)where the inequality holds by \(\|u^{\top}\mathrm{diag}\left(\sqrt{\mu}\right)\|_{2}\leq\|\sqrt{\mu}\|_{2}=1\) and \(K-\sqrt{\mu}\sqrt{\mu}^{\top}\) has leading eigenvalue with magnitude \(\lambda\). The last identity follows directly from the definition of the \(\chi^{2}\)-divergence that \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1=\sum_{b}\mu_{0}(b)^{2}/\mu^{\pi}(b)\).

Substituting the definition of \(c_{l}\), we have

\[\left\|\frac{\sum_{l=r_{n}}^{L-M+r_{n}}\eta^{L-l}p^{\pi}(B_{l}=b)}{\sum_{l=r_{ n}}^{L-M+r_{n}}\eta^{L-l}}-\mu^{\pi}(A=b)\right\|_{\mathrm{TV}}\leq\frac{\sum_{l=r_{ n}}^{L-M+r_{n}}\eta^{L-l}\cdot\lambda^{l-r_{n}}\cdot\sqrt{D_{\chi^{2}}(\mu_{0}\, \|\,\mu^{\pi})+1}}{\sum_{l=r_{n}}^{L-M+r_{n}}\eta^{L-l}}.\]

We consider two special cases. In the first case, we set \(\eta=\lambda\), which gives us

\[\left\|\frac{\sum_{l=r_{n}}^{L-M+r_{n}}\lambda^{L-l}p^{\pi}(B_{l} =b)}{\sum_{l=r_{n}}^{L-M+r_{n}}\lambda^{L-l}}-\mu^{\pi}(A=b)\right\|_{\mathrm{ TV}} \leq\frac{\sum_{l=r_{n}}^{L-M+r_{n}}\lambda^{L-r_{n}}\cdot\sqrt{D _{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(1-\lambda^{L-M})/(1-\lambda)}\] \[\leq\frac{L\cdot\lambda^{L-r_{n}}\cdot(1-\lambda)}{1-\lambda^{L- M}}\cdot\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}.\]

In the second case, we set \(\eta=1\), which gives us

\[\left\|\frac{\sum_{l=r_{n}}^{L-M+r_{n}}p^{\pi}(B_{l}=\cdot)}{L-M}-\mu^{\pi}(A =\cdot)\right\|_{\mathrm{TV}}\leq\frac{\sum_{l=r_{n}}^{L-M+r_{n}}\lambda^{l-r _{n}}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{L-M}\leq\frac{\sqrt{D_{ \chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M)(1-\lambda)}.\]

Note that the TV distance is an \(f\)-divergence. Thus, we can use the data processing inequality to obtain the desired result for \(Y_{l}\) from the above inequality. To do so, note that \(\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=\cdot)/(L-M)\) and \(\mu^{\pi}(Y_{L+1}=\cdot)\) can be transformed from \(\sum_{l=r_{n}}^{L-M+r_{n}-1}p^{\pi}(B_{l}=\cdot)/(L-M)\) and \(\mu^{\pi}(A=\cdot)\) by the same emission kernel

\[p^{\pi}(Y_{L+1}=\cdot\,|\,A=\cdot)=p^{\pi}(Y_{l}=\cdot\,|\,B_{l-M+r_{n}}= \cdot)=\mu^{\pi}(Y_{L+1}=\cdot\,|\,A=\cdot)=\mu^{\pi}(Y_{l}=\cdot\,|\,B_{l-M+ r_{n}}=\cdot).\]

Therefore, by the data processing inequality, it holds that

\[\left\|\frac{\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=\cdot)}{L-M}-\mu^{\pi}(Y_{L+1}= \cdot)\right\|_{\mathrm{TV}}\leq\left\|\frac{\sum_{l=r_{n}}^{L-M+r_{n}}p^{\pi }(B_{l}=\cdot)}{L-M}-\mu^{\pi}(A=\cdot)\right\|_{\mathrm{TV}}\leq\frac{\sqrt{ D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M)(1-\lambda)}.\]

Similarly for \(p^{\pi}(Y_{L+1}=\cdot)\) and \(\mu^{\pi}(\cdot)\), we have

\[\|p^{\pi}(Y_{L+1}=\cdot)-\mu^{\pi}(Y_{L+1}=\cdot)\|_{\mathrm{TV}}\leq\|p^{ \pi}(A=\cdot)-\mu^{\pi}(A=\cdot)\|_{\mathrm{TV}}\]

where the latter two inequality follows from the same arguments as in (F.30). Hence, the proof is completed. 

We have established that the average \(\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=\cdot)/(L-M)\) converges to \(\mu^{\pi}(A=\cdot)\) in total variation distance. This represents a "first-order" convergence since it involves the average of the marginal distribution of \(Y_{l}\). However, the quantity of interest in (F.25) is the average of the joint distribution of \(Y_{L+1}\) and \(Y_{l}\), which concerns "second-order" convergence. This is studied in the following lemma.

**Lemma F.17**.: _Following the notations introduced above, for the Markov chain with parent set \(\mathsf{pa}=\{-r_{1},\ldots,-r_{n}\}\), let \(D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})\) be the \(\chi^{2}\)-divergence between the initial distribution \(\mu_{0}\) and the stationary distribution \(\mu^{\pi}\) over the first \(r_{n}\) tokens. Take any \(\mathcal{S},\mathcal{S}^{\prime}\subseteq[M]\) and let \(Y_{l}=(x_{l},X_{l-\mathcal{S}})\) and \(Y^{\prime}_{l}=(x_{l},X_{l-\mathcal{S}^{\prime}})\) for \(l=M+1,\ldots,L+1\). Suppose \(L/2\geq M\geq r_{n}\). For \(\widehat{p}^{\pi}\) defined in (F.25), we have_

\[\|\widehat{p}^{\pi}(Y_{L+1}=\cdot,Y^{\prime}=\cdot)-\mu^{\pi}(Y_{L+1}=\cdot) \times\mu^{\pi}(Y^{\prime}=\cdot)\|_{\mathrm{TV}}\leq\frac{2M}{L}+\frac{4 \sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{L(1-\lambda)\cdot\sqrt{\min_{E} \mu^{\pi}(Y_{L+1}=E)}}.\]

_In particular, we have_

\[\left\|\widehat{p}^{\pi}(Y_{L+1}=\cdot,Y^{\prime}=\cdot)-\mu^{\pi}(Y _{L+1}=\cdot)\times\left(\frac{1}{L-M}\sum_{l=M+1}^{L}p^{\pi}(Y^{\prime}_{l}= \cdot)\right)\right\|_{\mathrm{TV}}\] \[\quad\leq\frac{2M}{L}+\frac{2\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1}}{L(1-\lambda)\cdot\sqrt{\min_{E}\mu^{\pi}(Y_{L+1}=E)}}.\] (F.31)Proof of Lemma F.17.: Let us take \(\mu^{\pi}(E)\cdot(L-M)^{-1}\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=E^{\prime})\) as the intermediate distribution, and we have by (F.25) that

\[\widehat{p}^{\pi}(Y_{L+1}=E,Y^{\prime}=E^{\prime})-\mu^{\pi}(Y_{L+1 }=E)\cdot\left(\frac{1}{L-M}\sum_{l=M+1}^{L}p^{\pi}(Y_{l}=E^{\prime})\right)\] \[=\underbrace{\frac{1}{L-M}\sum_{l=M+1}^{L-M+r_{n}}\sum_{A,B_{l}} \mu^{\pi}(Y_{L+1}=E\,|\,A)\cdot\left(P^{L-l-(M-r_{n})}(A\,|\,B_{l})-\mu^{\pi}( A)\right)\cdot p^{\pi}(Y_{l}^{\prime}=E^{\prime}\,|\,B_{l})\cdot p^{\pi}(B_{l})}_{ \text{(I)}}\] \[\quad+\underbrace{\frac{1}{L-M}\sum_{l=L-M+r_{n}+1}^{L}\left(p^{ \pi}(Y_{L+1}=E,Y_{l}^{\prime}=E^{\prime})-\mu^{\pi}(Y_{L+1}=E)p^{\pi}(Y_{l}^{ \prime}=E^{\prime})\right)}_{\text{(II)}}.\] (F.32)

where we use the fact that \(\sum_{A}\mu^{\pi}(Y_{L+1}=E\,|\,A)\mu^{\pi}(A)=\mu^{\pi}(Y=E)\) for the first line. The second term on the right hand side can be easily controlled as we already have an \(L^{-1}\) factor. We let \(\operatorname{TV}_{0}\) be the total variation distance of the second term. It is easy to see that

\[\operatorname{TV}_{0}:=\frac{1}{2}\sum_{E,E^{\prime}}|\text{(II)}|\leq\frac{ M-r_{n}}{L-M}\leq\frac{M}{L-M},\]

where we remark that (II) is a function of both \(E\) and \(E^{\prime}\), and the total variation distance is just taking the sum of the absolute values of the differences. Here, we also use the fact that \(L\geq 2M\). Using Proposition F.12, we can also rewrite the first term on the right hand side of (F.32) in the matrix form as

\[\text{(I)}=\frac{1}{L-M}\sum_{l=M+1}^{L-M+r_{n}}\mu^{\pi}(Y_{L+1} =\cdot\,|\,A=\cdot)\cdot\operatorname{diag}\left(\sqrt{\mu}\right)\cdot \left(K-\sqrt{\mu}\sqrt{\mu}^{\top}\right)^{L-l-(M-r_{n})}\cdot\operatorname{ diag}\left(\sqrt{\mu}\right)^{-1}\] \[\qquad\qquad\cdot\operatorname{diag}(p^{\pi}(B_{l}=\cdot))\cdot p ^{\pi}(Y_{l}^{\prime}=\cdot\,|\,B_{l}=\cdot)^{\top}.\]

When considering the \(\ell_{1}\)-norm of the above term, we introduce a test matrix \(U\) of shape \(|\mathcal{X}|^{|Y_{L+1}|}\times|\mathcal{X}|^{|Y_{L+1}|}\) with each element of \(U\) chosen from \(\{0,1\}\). Let \(\operatorname{TV}_{1}\) be the total variation distance of the first term (I). Then, we have

\[\operatorname{TV}_{1}\leq\max_{U}\operatorname{Tr} \bigg{[}\frac{1}{L-M}\sum_{l=M+1}^{L-M+r_{n}}\mu^{\pi}(Y_{L+1}= \cdot\,|\,A=\cdot)\cdot\operatorname{diag}\left(\sqrt{\mu}\right)\cdot\left(K -\sqrt{\mu}\sqrt{\mu}^{\top}\right)^{L-l-(M-r_{n})}\] \[\qquad\qquad\cdot\operatorname{diag}\left(\sqrt{\mu}\right)^{-1} \cdot\operatorname{diag}(p^{\pi}(B_{l}=\cdot))\cdot p^{\pi}(Y_{l}^{\prime}= \cdot\,|\,B_{l}=\cdot)^{\top}\cdot U(\cdot,\cdot)^{\top}\bigg{]}.\]

To upper bound this quantity, we consider each row of \(U\) as \(U(E,\cdot)=u(\cdot\,|\,E)^{\top}\). Note that \(u(\cdot\,|\,E)\) is also a \(\{0,1\}\)-valued vector. By expanding the trace, we have

\[\operatorname{TV}_{1}\leq\sum_{E}\max_{u(\cdot\,|\,E)}\frac{1}{L- M}\sum_{l=M+1}^{L-M+r_{n}}\mu^{\pi}(Y_{L+1}=E\,|\,A=\cdot)\cdot\operatorname{diag} \left(\sqrt{\mu}\right)\] \[\qquad\cdot\left(K-\sqrt{\mu}\sqrt{\mu}^{\top}\right)^{L-l-(M-r_ {n})}\cdot\operatorname{diag}\left(\sqrt{\mu}\right)^{-1}\cdot\operatorname{ diag}(p^{\pi}(B_{l}=\cdot))\cdot p^{\pi}(Y_{l}^{\prime}=\cdot\,|\,B_{l}=\cdot)^{ \top}\cdot u(\cdot\,|\,E).\]

Note that the \(\ell_{2}\)-norm of the vector in the last line can be upper bounded by

\[\Big{\|} \big{(}K-\sqrt{\mu}\sqrt{\mu}^{\top}\big{)}^{L-l-(M-r_{n})}\cdot \operatorname{diag}\left(\sqrt{\mu}\right)^{-1}\cdot\operatorname{diag}(p^{\pi} (B_{l}=\cdot))\cdot p^{\pi}(Y_{l}^{\prime}=\cdot\,|\,B_{l}=\cdot)^{\top} \cdot u(\cdot\,|\,E)\Big{\|}_{2}\] \[\leq\Big{\|}\lambda^{L-l-(M-r_{n})}\cdot\operatorname{diag}\left( \sqrt{\mu}\right)^{-1}\cdot\operatorname{diag}(p^{\pi}(B_{l}=\cdot))\cdot \mathbf{1}\Big{\|}_{2}=\lambda^{L-l-(M-r_{n})}\big{\|}\operatorname{diag} \left(\sqrt{\mu}\right)^{-1}\cdot p^{\pi}(B_{l}=\cdot)\big{\|}_{2}\] \[=\lambda^{L-l-(M-r_{n})}\sqrt{D_{\chi^{2}}(p^{\pi}(B_{l}=\cdot)\, \|\,\mu^{\pi}(B_{l}=\cdot))+1}\leq\lambda^{L-l-(M-r_{n})}\sqrt{D_{\chi^{2}}(\mu_ {0}\,\|\,\mu^{\pi})+1},\] (F.33)where the first inequality holds by noting that \(p^{\pi}(Y_{l}^{\prime}=\cdot\,|\,B_{l}=\cdot)^{\top}\cdot u(\cdot\,|\,E)\) is a vector with element within \([0,1]\), and also invoking the operator norm of the matrix \(\widehat{K}-\sqrt{\mu}\sqrt{\mu}^{\top}\). The second identity follows from the definition of the \(\chi^{2}\)-divergence that \(D_{\chi^{2}}(p^{\pi}(B_{l}=\cdot)\,\|\,\mu^{\pi}(\cdot))+1=\sum_{b}p^{\pi}(B_{l }=b)^{2}/\mu^{\pi}(b)\). The last inequality is the data processing inequality as \(p^{\pi}(B_{l}=\cdot)\) can be transformed from \(\mu_{0}(B_{r_{n}})\) and \(\mu^{\pi}(B_{l})\) can be transformed from \(\mu^{\pi}(B_{r_{n}})\) by the same emission kernel \(\mu^{\pi}(B_{l}=\cdot\,|\,B_{r_{n}}=\cdot)\). Consequently, we have for the TV distance that

\[\mathrm{TV}_{1} \leq\frac{1}{L-M}\sum_{l=M+1}^{L-M+r_{n}}\lambda^{L-l-(M-r_{n})} \cdot\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}\] \[\qquad\qquad\cdot\max_{\{v(\cdot\,|\,E)\}_{E}:\,\,\|v(\cdot\,|\,E )\|_{2}\leq 1}\sum_{E,A}\mu^{\pi}(Y_{L+1}=E\,|\,A)\cdot\sqrt{\mu^{\pi}(A)}\cdot v(A\,| \,E)\] \[\leq\frac{\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M)(1- \lambda)}\cdot\max_{\{v(\cdot\,|\,E)\}_{E}:\,\|v(\cdot\,|\,E)\|_{2}\leq 1} \sum_{A,E}\frac{\mu^{\pi}(A\,|\,Y_{L+1}=E)}{\sqrt{\mu^{\pi}(A)}}\cdot v(A\,|\,E )\cdot\mu^{\pi}(Y_{L+1}=E)\] \[\leq\max_{\{v(\cdot\,|\,E)\}_{E}:\,\|v(\cdot\,|\,E)\|_{2}\leq 1}\frac{\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1}}{(L-M)(1-\lambda)}\cdot\sqrt{I_{\chi^{2}}(A;Y_{L+1})+1}\cdot\sqrt{ \sum_{A,E}v(A\,|\,E)^{2}\cdot\mu^{\pi}(Y_{L+1}=E)}.\]

where in the first equality, we use the variational form of the \(\ell_{2}\)-norm for vector \(\mu^{\pi}(Y_{L+1}=E\,|\,A=\cdot)\cdot\mathrm{diag}(\sqrt{\mu})\). In the second inequality, we apply (F.33) and use the Bayes rule. The last inequality follows from the Cauchy-Schwarz inequality. Here, the mutual information \(I_{\chi^{2}}(A;Y_{L+1})+1\) can be upper bounded by

\[I_{\chi^{2}}(A;Y_{L+1})+1=\sum_{A,E}\frac{\mu^{\pi}(Y_{L+1}=E\,|\,A)}{\mu^{ \pi}(Y_{L+1}=E)}\cdot\mu^{\pi}(Y_{L+1}=E,A)\leq\frac{1}{\min_{E}\mu^{\pi}(Y_{ L+1}=E)},\]

and the last term involving \(v(A\,|\,E)\) can be upper bounded by \(1\) thanks to the constraint on \(v(\cdot\,|\,E)\).

In conclusion,

\[\mathrm{TV}_{1}\leq\frac{\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M) (1-\lambda)\cdot\sqrt{\min_{E}\mu^{\pi}(Y_{L+1}=E)}}.\]

Lastly, let us relate the intermediate distribution to the final distribution \(\mu^{\pi}(Y=\cdot)\times\mu^{\pi}(Y^{\prime}=\cdot)\), where we define the total variation distance \(\mathrm{TV}_{2}\) as

\[\mathrm{TV}_{2} :=\left\|\mu^{\pi}(\cdot)\cdot\left(\frac{1}{L-M}\sum_{l=M+1}^{L}p ^{\pi}(Y_{l}^{\prime}=\cdot)\right)-\mu^{\pi}(\cdot)\cdot\mu^{\pi}(\cdot) \right\|_{\mathrm{TV}}=\left\|\left(\frac{1}{L-M}\sum_{l=M+1}^{L}p^{\pi}(Y_{l} ^{\prime}=\cdot)\right)-\mu^{\pi}(\cdot)\right\|_{\mathrm{TV}}.\]

Invoking (F.28) of Lemma F.16, we have this quantity upper bounded by

\[\mathrm{TV}_{2}\leq\frac{\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M) (1-\lambda)}.\]

Using the triangular inequality for the total variation distance, we have

\[\|\widehat{p}^{\pi}(Y_{L+1}=\cdot,Y^{\prime}=\cdot)-\mu^{\pi}(Y_{L +1}=\cdot)\times\mu^{\pi}(Y^{\prime}=\cdot)\|_{\mathrm{TV}}\] \[\leq\mathrm{TV}_{0}+\mathrm{TV}_{1}+\mathrm{TV}_{2}\] \[\leq\frac{M}{L-M}+\frac{2\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1}}{(L-M)(1-\lambda)\cdot\sqrt{\min_{E}\mu^{\pi}(Y_{L+1}=E)}}\] \[\leq\frac{2M}{L}+\frac{4\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi} )+1}}{L(1-\lambda)\cdot\sqrt{\min_{E}\mu^{\pi}(Y_{L+1}=E)}},\]

and the upper bound for (F.31) follows by the same arguments. Hence, the proof is completed. 

In the following, we use a similar technique as in Lemma F.17 to derive a bound for the chi-square divergence.

**Lemma F.18**.: _For the \(\chi^{2}\)-divergence between the empirical distribution \((L-M)^{-1}\sum_{l=M+1}^{L}\mathds{1}(Y_{l}=\cdot)\) and the stationary distribution \(\mu^{\pi}(\cdot)\), we have_

\[\mathbb{E}\bigg{[}D_{\chi^{2}}\bigg{(}\frac{1}{L-M}\sum_{l=M+1}^{L}\mathds{1}(Y _{l}=\cdot)\,\Big{\|}\,\mu^{\pi}(Y_{L+1}=\cdot)\bigg{)}\bigg{]}\leq\frac{4(1- \lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}+16M}{L\cdot\min_{E} \mu^{\pi}(Y_{L+1}=E)},\]

_where the expectation is with respect to \(X\sim p^{\pi}\)._

Proof of Lemma F.18.: By definition of the \(\chi^{2}\)-divergence, what we aim to bound is just

\[\mathbb{E}\bigg{[}\sum_{E}\bigg{(}(L-M)^{-1}\sum_{l=M+1}^{L} \mathds{1}(Y_{l}=E)-\mu^{\pi}(E)\bigg{)}^{2}\,\bigg{/}\mu^{\pi}(E)\bigg{]}\] \[\quad=\mathbb{E}\bigg{[}\sum_{E}\frac{(L-M)^{-2}\sum_{l,l^{\prime }=M+1}^{L}\mathds{1}(Y_{l}=Y_{l^{\prime}}=E)-\mu^{\pi}(E)^{2}}{\mu^{\pi}(E)} \bigg{]}\] \[\quad=\mathbb{E}\bigg{[}\sum_{E}\sum_{l,l^{\prime}=M+1}^{L}\frac {\mathds{1}(Y_{l}=Y_{l^{\prime}}=E)}{(L-M)^{2}\mu^{\pi}(E)}-1\bigg{]}=\sum_{E} \sum_{l,l^{\prime}=M+1}^{L}\frac{p^{\pi}(Y_{l}=Y_{l^{\prime}}=E)}{(L-M)^{2} \mu^{\pi}(E)}-1.\]

To study the above quantity, for \(l\geq 2M-r_{n}+2\), we define

\[J_{1}(l)\,{:=}\sum_{E}\sum_{l^{\prime}=M+1}^{l-M+r_{n}-1}\frac{p^{\pi}(Y_{l}=Y _{l^{\prime}}=E)}{(L-M)^{2}\mu^{\pi}(E)}-\frac{l-2M+r_{n}}{(L-M)^{2}}.\]

Following our convention, we let \(A_{l}=X_{l-M:l-M+r_{n}-1}\) and \(B_{l^{\prime}}=X_{l^{\prime}-r_{n}+1:l^{\prime}}\) be two length-\(r_{n}\) window and by the Markov property, we have

\[Y_{l+1}\,\perp\!\!\!\perp(B_{l^{\prime}},Y_{l^{\prime}})\,|\,A_{l},\quad(Y_{l+ 1},B_{l})\,\perp\!\!\!\perp Y_{l^{\prime}}\,|\,B_{l^{\prime}}.\]

Let us fix an index \(l\geq 2M-r_{n}+2\) and take a summation over \(M+1\leq l^{\prime}\leq l-M+r_{n}-1\). Expanding the joint distribution, we have

\[J_{1}(l)\,{:=}\,\frac{1}{(L-M)^{2}}\sum_{l^{\prime}=M+1}^{l-M+r _{n}-1}\sum_{E,A_{l},B_{l^{\prime}}}\mu^{\pi}(Y_{l}=E\,|\,A_{l})\cdot\big{(}P^ {l-l^{\prime}-M+r_{n}-1}(A_{l}\,|\,B_{l^{\prime}})-\mu^{\pi}(A_{l})\big{)}\] \[\qquad\qquad\qquad\cdot p^{\pi}(Y_{l^{\prime}}=E\,|\,B_{l^{ \prime}})\cdot p^{\pi}(B_{l^{\prime}})\cdot\mu^{\pi}(Y_{l^{\prime}}=E)^{-1}\] \[\quad=\frac{1}{(L-M)^{2}}\sum_{l^{\prime}=M+1}^{l-M+r_{n}-1} \operatorname{Tr}\Bigl{[}\mu^{\pi}(Y_{l}=\cdot\,|\,A_{l}=\cdot)\cdot\operatorname {diag}\,(\sqrt{\mu})\cdot\big{(}K-\sqrt{\mu}\sqrt{\mu}^{\top}\big{)}^{l-l^{ \prime}-M+r_{n}-1}\] \[\qquad\qquad\qquad\cdot\operatorname{diag}\,(\sqrt{\mu})^{-1} \cdot\operatorname{diag}(p^{\pi}(B_{l^{\prime}}=\cdot))\cdot p^{\pi}(Y_{l^{ \prime}}=\cdot\,|\,B_{l^{\prime}}=\cdot)^{\top}\cdot\operatorname{diag}(\mu^{ \pi}(Y_{l^{\prime}}=\cdot)^{-1})\Bigr{]}\] \[\qquad=\frac{1}{(L-M)^{2}}\sum_{l^{\prime}=M+1}^{l-M+r_{n}-1} \operatorname{Tr}\Bigl{[}\operatorname{diag}(\mu^{\pi}(Y_{l^{\prime}}=\cdot) ^{-1/2})\cdot\mu^{\pi}(Y_{l}=\cdot\,|\,A_{l}=\cdot)\cdot\operatorname{diag} \,(\sqrt{\mu})\] \[\qquad\qquad\qquad\qquad\cdot\big{(}K-\sqrt{\mu}\sqrt{\mu}^{\top }\big{)}^{l-l^{\prime}-M+r_{n}-1}\cdot\operatorname{diag}\bigl{(}\sqrt{\mu} \bigr{)}^{-1}\cdot\operatorname{diag}(p^{\pi}(B_{l^{\prime}}=\cdot))\] \[\qquad\qquad\qquad\qquad\cdot p^{\pi}(Y_{l^{\prime}}=\cdot\,| \,B_{l^{\prime}}=\cdot)^{\top}\cdot\operatorname{diag}(\mu^{\pi}(Y_{l^{\prime} }=\cdot)^{-1/2})\Bigr{]},\]

where the first identity follows from the fact that

\[\sum_{E,A_{l},B_{l}^{\prime}}\mu^{\pi}(Y_{l}=E\,|\,A_{l})\cdot\mu ^{\pi}(A_{l})\cdot p^{\pi}(Y_{l^{\prime}}=E\,|\,B_{l^{\prime}})\cdot p^{\pi}(B_ {l^{\prime}})\cdot\mu^{\pi}(Y_{l^{\prime}}=E)^{-1}\] \[\quad=\sum_{E}p^{\pi}(Y_{l^{\prime}}=E)\cdot\mu^{\pi}(Y_{l^{\prime }}=E)\cdot\mu^{\pi}(Y_{l^{\prime}}=E)^{-1}=1,\]

and the second identity follows from Proposition F.12. We next invoke the Cauchy-Schwarz inequality for trace, i.e., \(\operatorname{Tr}(W^{\top}V)^{2}\leq\operatorname{Tr}(W^{\top}W)\operatorname{ Tr}(V^{\top}V)\), where we take

\[W^{\top} =\operatorname{diag}(\mu^{\pi}(Y_{l}=\cdot)^{-1/2})\cdot\mu^{\pi} (Y_{l}=\cdot\,|\,A_{l}=\cdot)\cdot\operatorname{diag}(\sqrt{\mu})\cdot\big{(}K- \sqrt{\mu}\sqrt{\mu}^{\top}\big{)}^{l-l^{\prime}-M+r_{n}-1},\] \[V =\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}^{-1}\cdot\operatorname{ diag}(p^{\pi}(B_{l^{\prime}}=\cdot))\cdot p^{\pi}(Y_{l^{\prime}}=\cdot\,|\,B_{l^{ \prime}}=\cdot)^{\top}\cdot\operatorname{diag}(\mu^{\pi}(Y_{l^{\prime}}=\cdot)^{-1/ 2})\] \[=\operatorname{diag}\bigl{(}\sqrt{\mu}\bigr{)}\cdot p^{\pi}(Y_{l^{ \prime}}=\cdot\,|\,B_{l^{\prime}}=\cdot)^{\top}\cdot\operatorname{diag}(\mu^{\pi}( Y_{l^{\prime}}=\cdot)^{-1/2})\]Note that

\[\sqrt{\operatorname{Tr}(W^{\top}W)} \leq\lambda^{l-l^{\prime}-M+r_{n}-1}\cdot\sqrt{\operatorname{Tr} \left(\operatorname{diag}(\mu^{\pi}(Y_{l}=\cdot)^{-1})\mu^{\pi}(Y_{l}=\cdot \,|\,A=\cdot)\operatorname{diag}\left(\mu\right)\mu^{\pi}(Y_{l}=\cdot\,|\,A= \cdot)^{\top}\right)}\] \[=\lambda^{l-l^{\prime}-M+r_{n}-1}\cdot\sqrt{\sum_{A_{l},Y_{l}} \frac{\mu^{\pi}(Y_{l},A_{l})^{2}}{\mu^{\pi}(Y_{l})\cdot\mu^{\pi}(A_{l})}}.\]

Following the same calculation, we have

\[\sqrt{\operatorname{Tr}(V^{\top}V)}=\sqrt{\sum_{Y_{l^{\prime}},B_{l^{\prime}} }\frac{p^{\pi}(Y_{l^{\prime}},B_{l^{\prime}})^{2}}{\mu^{\pi}(Y_{l^{\prime}}) \mu^{\pi}(B_{l^{\prime}})}}.\]

Therefore,

\[J_{1}(l) \leq\frac{1}{(L-M)^{2}}\sum_{l^{\prime}=M+1}^{l-M+r_{n}-1} \lambda^{l-l^{\prime}-M+r_{n}-1}\cdot\sqrt{\sum_{A_{l},Y_{l}}\frac{\mu^{\pi} (Y_{l},A_{l})^{2}}{\mu^{\pi}(Y_{l})\cdot\mu^{\pi}(A_{l})}\cdot\sum_{Y_{l^{ \prime}},B_{l^{\prime}}}\frac{p^{\pi}(Y_{l^{\prime}},B_{l^{\prime}})^{2}}{\mu^ {\pi}(Y_{l^{\prime}})\mu^{\pi}(B_{l^{\prime}})}}.\]

We further have

\[\sum_{Y_{l^{\prime}},B_{l^{\prime}}}\frac{p^{\pi}(Y_{l^{\prime}},B_ {l^{\prime}})^{2}}{\mu^{\pi}(Y_{l^{\prime}})\mu^{\pi}(B_{l^{\prime}})} \leq\max_{Y_{l^{\prime}},B_{l^{\prime}}}\left\{\frac{p^{\pi}(Y_{ l^{\prime}}\,|\,B_{l^{\prime}})}{\mu^{\pi}(Y_{l^{\prime}})}\right\}\cdot \sum_{B_{l^{\prime}}}\frac{p^{\pi}(B_{l^{\prime}})^{2}}{\mu^{\pi}(B_{l^{\prime }})}\] \[\leq\frac{D_{\chi^{2}}(p^{\pi}(B_{l^{\prime}}=\cdot)\,\|\,\mu^{ \pi}(B_{l^{\prime}}=\cdot))+1}{\min_{E}\mu^{\pi}(Y_{L+1}=E)}\leq\frac{D_{\chi ^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}{\min_{E}\mu^{\pi}(Y_{L+1}=E)},\]

where the last inequality holds by the data processing inequality. Similarly, we have

\[\sum_{A_{l},Y_{l}}\frac{\mu^{\pi}(Y_{l},A_{l})^{2}}{\mu^{\pi}(Y_{l})\cdot\mu^{ \pi}(A_{l})}\leq\max_{Y_{l},A_{l}}\left\{\frac{\mu^{\pi}(Y_{l}\,|\,A_{l})}{ \mu^{\pi}(Y_{l})}\right\}\leq\frac{1}{\min_{E}\mu^{\pi}(Y_{L+1}=E)}.\]

Therefore, we conclude that

\[J_{1}(l) \leq\frac{\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M)^{ 2}(1-\lambda)\cdot\min_{E}\mu^{\pi}(Y_{L+1}=E)},\]

and

\[2\sum_{l=2M-r_{n}+2}^{L}J_{1}(l) \leq\frac{2\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}}{(L-M) (1-\lambda)\cdot\min_{E}\mu^{\pi}(Y_{L+1}=E)},\]

where we double the value as \(l>l^{\prime}\) only contributes to half of the terms in the double summation. Note that in the above summation for \(l>l^{\prime}\), we only include terms satisfying \(l-l^{\prime}\geq M-r_{n}+1\) and \(l-(M+1)\geq M-r_{n}+1\). For the remaining \((l,l^{\prime})\) not included above, each term is bounded above by

\[\left|\frac{1}{(L-M)^{2}}\bigg{(}\sum_{E}\frac{p^{\pi}(Y_{l}=Y_{ l^{\prime}}=E)}{\mu^{\pi}(E)}-1\bigg{)}\right| \leq\frac{1}{(L-M)^{2}\min_{E}\mu^{\pi}(Y_{L+1}=E)},\]

and we have no more than \(4L(M-r_{n}+1)\) of these terms in total. As a result, we conclude with \(L/2\geq M\geq r_{n}\) that

\[J_{1} \leq\frac{4(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^ {\pi})+1}+16M}{L\cdot\min_{E}\mu^{\pi}(Y_{L+1}=E)}.\]

Hence, we complete the proof of Lemma F.18. 

**Proposition F.19**.: _Let us define_

\[\widetilde{\mu}^{\pi}_{X}(z,Z_{-\mathcal{S}^{*}}) =\frac{\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})\exp\left(a\cdot\prod_{h \in\mathcal{S}^{*}}\mathds{1}(z_{-h}=x_{L+1-h})\right)}{\sum_{z^{\prime},Z^{ \prime}_{-\mathcal{S}^{*}}}\mu^{\pi}(z^{\prime},Z^{\prime}_{-\mathcal{S}^{*}}) \exp\left(a\cdot\prod_{h\in\mathcal{S}^{*}}\mathds{1}(z^{\prime}_{-h}=x_{L+1-h} )\right)},\]_where \(Z_{-\mathcal{S}^{*}}=(z_{-h})_{h\in\mathcal{S}^{*}}\) and \(\mu^{\pi}\) is the stationary distribution of the Markov chain over a window of size \(M+1\). We also treat \(\widehat{\mu}_{X}^{\pi}(\cdot)\) as a length \(|\mathcal{X}|\) vector where \(\mathcal{X}\) is the state space of the Markov chain. Let \(\widehat{\nu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})=\sum_{l=M+1}^{L}\sigma_{l}^{ \pi}\,\mathds{1}(x_{l}=z,X_{l-\mathcal{S}^{*}}=Z_{-\mathcal{S}^{*}})\) where_

\[\sigma_{l}^{\pi}=\frac{\exp(a\cdot\prod_{h\in\mathcal{S}^{*}}\mathds{1}(x_{l- h}=x_{L+1-h}))}{\sum_{l^{\prime}=M+1}^{L}\exp(a\cdot\prod_{h\in\mathcal{S}^{*}} \mathds{1}(x_{l^{\prime}-h}=x_{L+1-h}))}.\]

_Then, we have_

\[\mathbb{E}_{X}\left[\|\widehat{\mu}_{X}^{\pi}(z=\cdot,Z_{-\mathcal{S}^{*}}= \cdot)-\widehat{\nu}_{X}^{\pi}(z=\cdot,Z_{-\mathcal{S}^{*}}=\cdot)\|_{1} \right]\leq\frac{4\big{(}(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{ \pi})+1}+4M\big{)}^{1/2}}{L^{1/2}\cdot\min_{x_{L+1},X_{L+1-\mathcal{S}^{*}}} \,\mu^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}.\]

Proof of Proposition F.19.: To unify the notations, we let \(Z=(z_{-M},\ldots,z_{-1})\) and define

\[\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})=\frac{1}{L-M}\sum_{l=M+1}^{L} \mathds{1}(x_{l}=z,X_{l-\mathcal{S}^{*}}=Z_{-\mathcal{S}^{*}}),\]

\[R(Z_{-\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}})=\exp\left(a\cdot\mathds{1}(Z_{ -\mathcal{S}^{*}}=x_{L+1-\mathcal{S}^{*}})\right).\]

Using these notations, we can define the normalizing factor in \(\widehat{\mu}_{X}^{\pi}\) and \(y_{X}^{*}\) respectively as

\[\Phi=\sum_{z,Z_{-\mathcal{S}^{*}}}\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{ -\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}}),\quad\widehat{\Phi}=\sum_{z,Z_{- \mathcal{S}^{*}}}\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{- \mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}}).\]

We also define

\[\phi(z,Z_{-\mathcal{S}^{*}})=\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{- \mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}}),\quad\widehat{\phi}(z,Z_{-\mathcal{ S}^{*}})=\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{-\mathcal{S}^{*}},X_{L+ 1-\mathcal{S}^{*}}).\]

We can then rewrite the objective as

\[\|\widehat{\mu}_{X}^{\pi}(z=\cdot,Z_{-\mathcal{S}^{*}}=\cdot)- \widehat{\nu}_{X}^{\pi}(z=\cdot,Z_{-\mathcal{S}^{*}}=\cdot)\|_{1}\] \[=\sum_{z,Z_{-\mathcal{S}^{*}}}\left|\frac{\phi(z,Z_{-\mathcal{S}^{ *}})}{\Phi}-\frac{\widehat{\phi}(z,Z_{-\mathcal{S}^{*}})}{\widehat{\Phi}} \right|\leq\!\!\sum_{z,Z_{-\mathcal{S}^{*}}}\!\!\frac{\widehat{\phi}(z,Z_{- \mathcal{S}^{*}})\cdot|\widehat{\Phi}-\Phi|+|\phi(z,Z_{-\mathcal{S}^{*}})- \widehat{\phi}(z,Z_{-\mathcal{S}^{*}})|\cdot\widehat{\Phi}}{\Phi\cdot\widehat{ \Phi}}\] \[=\frac{|\widehat{\Phi}-\Phi|+\sum_{z,Z_{-\mathcal{S}^{*}}}|\phi(z,Z_ {-\mathcal{S}^{*}})-\widehat{\phi}(z,Z_{-\mathcal{S}^{*}})|}{\Phi}\leq\frac{2 \sum_{z,Z_{-\mathcal{S}^{*}}}|\phi(z,Z_{-\mathcal{S}^{*}})-\widehat{\phi}(z,Z_{- \mathcal{S}^{*}})|}{\Phi}.\]

Furthermore, notice that

\[\frac{\sum_{z,Z_{-\mathcal{S}^{*}}}|\phi(z,Z_{-\mathcal{S}^{*}})- \widehat{\phi}(z,Z_{-\mathcal{S}^{*}})|}{\Phi}=\frac{\sum_{z,Z_{-\mathcal{S}^{* }}}|(\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{ S}^{*}}))\cdot R(Z_{-\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}})|}{\sum_{z,Z_{- \mathcal{S}^{*}}}\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})\cdot R(Z_{-\mathcal{S}^{*}}, X_{L+1-\mathcal{S}^{*}})}\] \[\leq\frac{\sum_{z,Z_{-\mathcal{S}^{*}}}|(\mu^{\pi}(z,Z_{- \mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}))|+(e^{a}-1) \sum_{z,Z_{-\mathcal{S}^{*}}\in\Gamma_{X}}|\mu^{\pi}(z,Z_{-\mathcal{S}^{*}}) -\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})|}{1+(e^{a}-1)\cdot\sum_{z,Z_{ -\mathcal{S}^{*}}\in\Gamma_{X}}\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})}\] \[\leq\sum_{z,Z_{-\mathcal{S}^{*}}}|\mu^{\pi}(z,Z_{-\mathcal{S}^{*} })-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})|+\frac{\sum_{z,Z_{-\mathcal{S}^{* }}\in\Gamma_{X}}|(\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_ {-\mathcal{S}^{*}}))|}{\sum_{z,Z_{-\mathcal{S}^{*}}\in\Gamma_{X}}\mu^{\pi}(z,Z_ {-\mathcal{S}^{*}})}.\] (F.34)

where we define \(\Gamma_{X}=\{Z_{-\mathcal{S}^{*}}:Z_{-\mathcal{S}^{*}}=X_{L+1-\mathcal{S}^{*}}\}\). Note that when \(Z_{-\mathcal{S}^{*}}\in\Gamma_{X}\), we have \(R(Z_{-\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}})=e^{a}\) and when \(Z_{-\mathcal{S}^{*}}\notin\Gamma_{X}\), we have \(R(Z_{-\mathcal{S}^{*}},X_{L+1-\mathcal{S}^{*}})=1\). For the first term on the right-hand side of (F.34), we have by Cauchy-Schwarz that

\[\mathbb{E}_{X}\bigg{[}\sum_{z,Z_{-\mathcal{S}^{*}}}|\mu^{\pi}(z,Z_ {-\mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}})|\,\bigg{]} \leq\bigg{(}\mathbb{E}_{X}\bigg{[}\sum_{z,Z_{-\mathcal{S}^{*}}} \frac{(\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{ S}^{*}}))^{2}}{\mu^{\pi}(z,Z_{-\mathcal{S}^{*}})}\bigg{]}\bigg{)}^{1/2}\] \[\leq\bigg{(}\frac{4(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu_{0}\,\| \,\mu^{\pi})+1}+16M}{L\cdot\min_{x_{L+1},X_{L+1-\mathcal{S}^{*}}}\mu^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}\bigg{)}^{1/2},\]where in the last inequality, we invoke Lemma F.18 where we take \(Y_{l}=x_{l}\) in the lemma. For the second term on the right hand of (F.34), we note that

\[\mathbb{E}_{X}\bigg{[} \frac{\sum_{z,Z_{-\mathcal{S}^{*}}\in\Gamma_{X}}\left[\left(\mu^{ \pi}(z,Z_{-\mathcal{S}^{*}})-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}) \right)\right]}{\sum_{z,Z_{-\mathcal{S}^{*}}\in\Gamma_{X}}\mu^{\pi}(z,Z_{- \mathcal{S}^{*}})}\bigg{]}\] \[\leq\sum_{E,z}\mathbb{E}_{X}\left[\frac{\left|\mu^{\pi}(z,Z_{- \mathcal{S}^{*}}=E)-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}=E)\right|}{ \mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}\cdot\mathds{1}(X_{L+1-\mathcal{S}^{*}}=E) \right]\] \[\leq\sum_{E,z}\left(\mathbb{E}_{X}\bigg{[}\Big{(}\frac{\mu^{\pi} (z,Z_{-\mathcal{S}^{*}}=E)-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}=E)}{ \sqrt{\mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}}\Big{)}^{2}\bigg{]}\cdot\frac{p^{\pi} (X_{L+1-\mathcal{S}^{*}}=E)}{\mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}\right)^{1/2}\] \[\leq\left(\mathbb{E}_{X}\bigg{[}\sum_{E,z}\frac{\left(\mu^{\pi}( z,Z_{-\mathcal{S}^{*}}=E)-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}=E) \right)^{2}}{\mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}\bigg{]}\cdot\sum_{E,z}\frac{p ^{\pi}(X_{L+1-\mathcal{S}^{*}}=E)}{\mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}\bigg{)}^ {1/2},\] (F.35)

where the last two inequalities follow from the Cauchy-Schwarz inequality. We have an upper bound for the second term on the right-hand side of (F.35) that

\[\bigg{(}\sum_{E,z}\frac{p^{\pi}(X_{L+1-\mathcal{S}^{*}}=E)}{\mu^{\pi}(Z_{- \mathcal{S}^{*}}=E)}\bigg{)}^{1/2}\leq\sqrt{\frac{1}{\min_{E}\mu^{\pi}(Z_{- \mathcal{S}^{*}}=E)}}.\]

We can also apply Lemma F.18 to the first term with \(Y_{L+1}=(x_{L+1},X_{L+1-\mathcal{S}^{*}})\) and conclude that

\[\bigg{(}\mathbb{E}_{X}\bigg{[}\sum_{E,z}\frac{\left(\mu^{\pi}(z,Z_ {-\mathcal{S}^{*}}=E)-\widehat{\mu}_{X}^{\pi}(z,Z_{-\mathcal{S}^{*}}=E) \right)^{2}}{\mu^{\pi}(Z_{-\mathcal{S}^{*}}=E)}\bigg{]}\bigg{)}^{1/2}\] \[\qquad\leq\bigg{(}\frac{4(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}(\mu _{0}\,\|\,\mu^{\pi})+1}+16M}{L\cdot\min_{x_{L+1},X_{L+1-\mathcal{S}^{*}}}\mu^ {\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}\bigg{)}^{1/2}.\]

In summary, we have

\[\mathbb{E}_{X}\left[\left\|\widehat{\mu}_{X}^{\pi}(e_{k})-y^{*}( k)\right\|_{1}\right]\] \[\qquad\leq\frac{2}{\min_{x_{L+1},X_{L+1-\mathcal{S}^{*}}}\mu^{ \pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}\cdot\bigg{(}\frac{(1-\lambda)^{-1}\sqrt {D_{\chi^{2}}(\mu_{0}\,\|\,\mu^{\pi})+1}+4M}{L}\bigg{)}^{1/2}\] \[\qquad\qquad+2\bigg{(}\frac{(1-\lambda)^{-1}\sqrt{D_{\chi^{2}}( \mu_{0}\,\|\,\mu^{\pi})+1}+4M}{L\cdot\min_{x_{L+1},X_{L+1-\mathcal{S}^{*}}}\mu ^{\pi}(x_{L+1},X_{L+1-\mathcal{S}^{*}})}\big{)}\bigg{)}^{1/2}.\]

Note that the second term is dominated by the first term. Thus, we conclude the proof of Proposition F.19.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The convergence results are established in Theorem 3.6 and we relate the learned transformer model to the generalized induction head in the following discussions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed in SSB.2 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the full set of assumptions in Assumption 3.3 and Assumption 3.5. We provide a complete and correct proof in SSE and SSF. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the complete details for our numerical experiment in SSB. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We do not release the data and code, but the details provided in SSB are sufficient for reproducing the synthetic data and the experiment results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the training and test details in SSB. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not report error bars because the training behavior is consistent across different runs, and the goal of the experiments is to corroborate our main theoretical results, rather than achieving better performance on benchmarks. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide information about compute resources in SSB. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and we confirm that our submission adheres to the guidelines therein. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: In the current paper, we focus on developing theoretical understanding of transformers, and the goal is to analyze existing architectures instead of proposing new models for better performance. Therefore, we do not see immediate societal impact of our paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We use common and standard Python libraries and write our own code for the experiments. Also, the data used in experiments is synthetic. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce new assets in the current paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We perform theoretical analysis and numerical simulations on synthetic data, and the process is not related to human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: As clarified in the answer above, our study is not related to human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.