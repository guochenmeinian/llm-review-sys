# Tighter Convergence Bounds for Shuffled SGD

via Primal-Dual Perspective

 Xufeng Cai

Department of Computer Sciences

University of Wisconsin-Madison

xcai74@wisc.edu

&Cheuk Yin Lin

Department of Computer Sciences

University of Wisconsin-Madison

cylin@cs.wisc.edu

Equal contribution

Jelena Diakonikolas

Department of Computer Sciences

University of Wisconsin-Madison

jelena@cs.wisc.edu

###### Abstract

Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets _without replacement_ and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of _sampling with replacement_. It is only very recently that SGD using sampling without replacement - shuffled SGD - has been analyzed with matching upper and lower bounds. However, we observe that those bounds are too pessimistic to explain often superior empirical performance of data permutations (sampling without replacement) over vanilla counterparts (sampling with replacement) on machine learning problems. Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR). Notably, our new bounds predict faster convergence than existing bounds in the literature - by up to a factor of \(O(\sqrt{n})\), mirroring benefits from tighter convergence bounds using component smoothness parameters in randomized coordinate methods. Lastly, we numerically demonstrate on common machine learning datasets that our bounds are indeed much tighter, thus offering a bridge between theory and practice.

## 1 Introduction

Originally proposed in [38], SGD has been broadly studied in the machine learning literature due to its effectiveness in large-scale settings, where full gradient computations are often computationally prohibitive. When applied to unconstrained finite-sum problems

\[\min_{\bm{x}\in\mathbb{R}^{d}}f(\bm{x}),\ \ \text{where}\ \ f(\bm{x}):=\frac{1}{n} \sum_{i=1}^{n}f_{i}(\bm{x}),\] (P)

SGD performs the update \(\bm{x}_{t}=\bm{x}_{t-1}-\eta\nabla f_{i_{t}}(\bm{x}_{t-1})\) for \(i_{t}\in[n]\) (\([n]:=\{1,\dots,n\}\)), in each iteration \(t\). Traditional theoretical analysis for SGD builds upon the assumption of sampling \(i_{t}\in[n]\) with replacement according to a fixed distribution \(\mathbf{p}=(p_{1},\dots,p_{n})^{\top}\) over \([n]\), which leads to\(\mathbb{E}_{i_{t}}[\nabla f_{i_{t}}(\bm{x}_{t-1})/(np_{i_{t}})]=\nabla f(\bm{x}_{t- 1})\), and thus much of the (deterministic) gradient descent-style analysis can be transferred to this setting. By contrast, no such connection between the component and the full gradient can be established for shuffled SGD -- which employs sampling _without replacement_ -- making its analysis much more challenging. As a result, despite its fundamental nature, there were no non-asymptotic convergence results for shuffled SGD until a very recent line of work [2; 12; 20; 21; 30; 31; 34; 35; 42]. All existing results consider general finite sum problems, with the same regularity condition constant (Lipschitz constant of \(f_{i}\) or its gradient) assumed for all the component functions. As a result, the obtained convergence bounds are typically no better than for (full) gradient descent, and are only better than the bounds for SGD with replacement sampling if the algorithm is run for many full passes over the data [30; 34].

Furthermore, there is a large gap between the empirical performance of shuffled SGD and the predicted convergence rates from prior work [20; 30]. One cause for this discrepancy are overly pessimistic bounds on the step size in prior work, which are of order \(1/(nL_{\max})\), where \(L_{\max}\) is the maximum smoothness constant over components \(f_{i}\) in (P). In practice, the step sizes are tuned to achieve better convergence bounds than predicted by the current theory. We illustrate how restrictions on the step size affect convergence of shuffled SGD (with random permutations in each epoch) in Fig. 1, where we plot the resulting optimality gap over full data passes when shuffled SGD is applied to logistic regression problems on standard datasets. To compare the effect of the step size \(\eta\) from prior work and our work, we choose take \(\eta=1/(\sqrt{2}nL_{\max})\) based on [30], and \(\eta=1/(n\sqrt{\tilde{L}\tilde{L}})\) from our work, where \(\tilde{L},\tilde{L}\) are our novel fine-grained, data-dependent smoothness parameters defined in Section 3 for smooth convex finite-sum problems with linear predictors. As can be observed from Fig. 1, larger step sizes resulting from our theory lead to faster convergence of shuffled SGD and, as a result, our convergence bounds better predict the performance of shuffled SGD.

Building on these insights, we introduce a fined-grained theoretical analysis to transparently show how the structure of the data and the possibly different Lipschitz constants of the component functions or their gradients affect the performance of shuffled SGD, thus providing a better explanation of the heuristic success of shuffled SGD in modern machine learning.

### Background and related work

SGD (with replacement) has been extensively studied in many settings (see e.g., [1; 9; 10; 38] for convex optimization). Compared to SGD, shuffled SGD usually exhibits faster convergence in practice [8; 37], and is easier and more efficient to implement [5]. For each epoch \(k\), shuffled SGD-style algorithms perform incremental gradient updates based on the sample ordering (permutation of the data points) denoted by \(\pi^{(k)}\). There are three main choices of data permutations: (i) \(\pi^{(k)}\equiv\pi\) for some fixed permutation of \([n]\) for all epochs, where shuffled SGD reduces to the incremental gradient (IG) method; (ii) \(\pi^{(k)}\equiv\tilde{\pi}\) where \(\tilde{\pi}\) is randomly chosen only once, at the beginning of the first epoch, referred to as the shuffle-once (SO) scheme; (iii) \(\pi^{(k)}\) randomly generated at the beginning of each epoch, referred to as random reshuffling (RR).

For general smooth convex settings, the convergence of shuffled SGD has been established only recently. For the number of epochs \(K\) sufficiently large, [31] proved a convergence rate

Figure 1: An illustration of the convergence behaviour of shuffled SGD for logistic regression problems on LIBSVM datasets luke, leu and a9a, where we use step sizes from existing bounds and our work. Due to randomness, we average over \(20\) runs for each plot and include a ribbon around each line to show its variance. However, as suggested by the concentration of \(\hat{L}\) (see Section 4.1 and Appendix E), the variance across multiple runs is negligible, hence the ribbons are not observable.

\(\mathcal{O}(1/\sqrt{nK})\) for RR, which leads to the complexity matching SGD. This result was later improved to \(\mathcal{O}(1/(n^{1/3}K^{2/3}))\) by [12; 30; 34] for \(K\) sufficiently large and with bounded variance assumed at the minimizer, while the same rate holds for SO [30]. These results were complemented by matching lower bounds in [12], under sufficiently small step sizes as utilized in prior work. The results in [30; 34] require restricted \(\mathcal{O}(1/(nL))\) step sizes and reduce to \(\mathcal{O}(1/K)\) for small \(K\), acquiring the same iteration complexity as full-gradient methods. Unlike in strongly convex settings, we are not aware of any follow-up work with improvements under small \(K\) for smooth convex settings.

The major difficulty in analyzing shuffled SGD comes from characterizing the difference between the intermediate iterate and the iterate after one full data pass, for which current analysis (see e.g., [30] in smooth convex settings) uses the global smoothness constant with a triangle inequality. Such a bound may be too pessimistic and fail capturing the nuances of intermediate progress of shuffled SGD, which leads to a small step size and large \(K\) restrictions. To provide a more fine-grained analysis that narrows the theory-practice gap for shuffled SGD, we notice that such a proof difficulty is reminiscent of the analysis of cyclic block coordinate methods relating the partial gradients to the full one. This natural connection was further emphasized in studies of cyclic methods with random permutations [24; 47]; however, these results were limited to convex quadratics. More generally, it is possible to interpret shuffled SGD as a primal-dual method performing cyclic updates on the dual side (see (PD) in Section 2.1 and (PL-PD) in Section 3). We note here that prior work on dual coordinate methods [41] provided theoretical guarantees only for the algorithms that choose the dual coordinate to optimize uniformly at random, while the cyclic variant (related to shuffled SGD) had only been studied numerically up until this work. Further discussion of related work appears in Appendix A.

### Contributions

In this work, we study the convergence rates of shuffled SGD in various settings through a unified primal-dual perspective, making intriguing connections to cyclic coordinate methods. This analysis framework is novel and allows us to leverage cyclic bias accumulation techniques on the dual side to obtain fine-grained convergence bounds. The obtained bounds mirror the improvements in randomized coordinate methods, which come from different coordinate smoothness parameters. While coordinate methods are no better than full-gradient methods in the worst case, on typical problem instances, they are much faster and the improvements come precisely from a more fine-grained view of smoothness. We see a similar phenomenon in our analysis, which highlights the usefulness of the fine-grained smoothness characterizations introduced in our work.

We provide improved bounds for all three popular data permutation strategies RR, SO and IG, in smooth convex settings. When the problem objective narrows to empirical risk minimization with linear predictors, we are able to exploit the data-dependent structure and uncouple the linear and nonlinear parts of the objective function, allowing us to provide tighter data-dependent bounds, up to a factor of \(O(\sqrt{n})\). Moreover, we show that our techniques extend to non-smooth convex settings, providing improved bounds over existing work.

We summarize our results and compare them to the state of the art in Table 1. As is standard, all complexity results in Table 1 are expressed in terms of individual (component) gradient evaluations. They represent the number of gradient evaluations required to construct a solution with (expected) optimality gap \(\epsilon\), given a target error \(\epsilon>0\).

Extensions to mini-batching and IG.When presenting our results for general finite-sum problems (in Section 2), we consider simple updates without mini-batching for ease of presentation and to avoid introducing excessive notation. However, we emphasize that all our results can be extended to shuffled SGD with mini-batching. Our results are also the first to provide convergence bounds that demonstrate benefits of mini-batching in shuffled SGD. For completeness and generality, the proofs in the appendix are carried out for mini-batch settings with arbitrary batch sizes \(b\in\{1,\ldots,n\}\). Thus, all the results stated in Section 2 can be recovered by setting \(b=1\). Moreover, our framework can provide similar fine-grained convergence bounds for IG. However, as IG is not as commonly used in practice compared to RR and SO and due to space constraints, we only present our results for RR and SO in the main body and include the results for IG in the appendix.

### Notation

We consider a real \(d\)-dimensional Euclidean space \((\mathbb{R}^{d},\|\cdot\|)\) where \(d\) is finite and \(\|\cdot\|\) is the \(\ell_{2}\)-norm. For a vector \(\bm{x}\), we let \(\bm{x}^{j}\) denote its \(j\)-th coordinate. For any positive integer \(m\), we use \([m]\) to denote the set \(\{1,2,\ldots,m\}\). Given a matrix \(\bm{A}\), \(\|\bm{A}\|:=\sup_{\bm{x}\in\mathbb{R}^{d},\|\bm{x}\|\leq 1}\|\bm{A}\bm{x}\|\) denotes its operator norm. For a positive definite matrix \(\bm{\Lambda}\), \(\|\cdot\|_{\bm{\Lambda}}\) denotes the Mahalanobis norm, \(\|\bm{x}\|_{\bm{\Lambda}}:=\sqrt{\langle\bm{\Lambda}\bm{x},\bm{x}\rangle}\). We use \(\bm{I}\) to denote the identity matrix, and \(\operatorname{diag}(\bm{v})\) to denote the diagonal matrix with vector \(\bm{v}\) on the main diagonal. For any \(j\in[n]\), we define \(\bm{I}_{j\uparrow}\) as the matrix obtained from the identity matrix \(\bm{I}\) by setting the first \(j\) diagonal elements to zero, and let \(\bm{I}_{j}\) be the matrix with only the \(j\)-th diagonal element nonzero and equal to \(1\). To handle the cases with random data permutations, we use the following definitions corresponding to the data permutation \(\pi=\{\pi^{1},\pi^{2},\ldots,\pi^{n}\}\) of \([n]\): \(\bm{A}_{\pi}:=\big{[}\bm{a}_{\pi_{1}},\bm{a}_{\pi_{2}},\ldots,\bm{a}_{\pi_{n} }\big{]}^{\top}\) permuting the rows based on \(\pi\) given a matrix \(\bm{A}=[\bm{a}_{1},\bm{a}_{2},\ldots,\bm{a}_{n}]^{\top}\), and \(\bm{v}_{\pi}:=\big{(}\bm{v}^{\pi_{1}},\bm{v}^{\pi_{2}},\ldots,\bm{v}^{\pi_{n} }\big{)}^{\top}\) permuting the coordinates/subvectors based on \(\pi\) given a vector \(\bm{v}=(\bm{v}^{1},\bm{v}^{2},\ldots,\bm{v}^{n})^{\top}\).

## 2 Primal-Dual Framework for Smooth Convex Finite-Sum Problems

Throughout this section, we make the following standard assumptions.

**Assumption 1**.: _Each \(f_{i}\) is convex and \(L_{i}\)-smooth, and there exists a minimizer \(\bm{x}_{*}\in\mathbb{R}^{d}\) for \(f(\bm{x})\)._

Assumption 1 implies that \(f\) and all component functions \(f_{i}\) are \(L\)-smooth, where \(L_{\max}:=\max_{i\in[n]}L_{i}\). It also implies that each convex conjugate \(f_{i}^{*}\) is \(\frac{1}{L_{i}}\)-strongly convex [3]. In this section, we define \(\bm{\Lambda}=\operatorname{diag}(\underbrace{L_{1},\ldots,L_{1}}_{d},\ldots, \underbrace{L_{n},\ldots,L_{n}}_{d})\in\mathbb{R}^{nd\times nd}\), and slightly abuse the notation to use \(\bm{\Lambda}_{\pi}=\text{diag}\big{(}\underbrace{L_{\pi^{1}},\ldots,L_{\pi^{ 1}}}_{d},\ldots,\underbrace{L_{\pi^{n}},\ldots,L_{\pi^{n}}}_{d}\big{)}\) given a permutation \(\pi\) of \([n]\). For the permutation \(\pi_{k}\) at the \(k\)-th epoch, we denote \(\bm{\Lambda}_{k}=\bm{\Lambda}_{\pi_{k}}\), for brevity.

We further assume that the variance at \(\bm{x}_{*}\) is bounded, same as prior work [30; 34].

**Assumption 2**.: _The quantity \(\sigma_{*}^{2}=\frac{1}{n}\sum_{i=1}^{n}\|\nabla f_{i}(\bm{x}_{*})\|^{2}\) is bounded._

### Primal-dual view of shuffled SGD

Problem (P) can be reformulated into a primal-dual form using the standard Fenchel conjugacy argument (see, e.g., [13; 14]),

\[\min_{\bm{x}\in\mathbb{R}^{d}}\max_{\bm{y}\in\mathbb{R}^{nd}}\Big{\{}\mathcal{L} (\bm{x},\bm{y}):=\frac{1}{n}\sum_{i=1}^{n}\Big{(}\left<\bm{y}^{i},\bm{x} \right>-f_{i}^{*}(\bm{y}^{i})\Big{)}\Big{\}},\] (PD)

where we slightly abuse the notation to denote \(\bm{y}=(\bm{y}^{1},\ldots,\bm{y}^{n})^{\top}\in\mathbb{R}^{nd}\) and \(f_{i}^{*}\) is the convex conjugate of \(f_{i}^{*}(\bm{y})=\sup_{\bm{x}\in\mathbb{R}^{d}}\big{\{}\left<\bm{y},\bm{x} \right>-f_{i}(\bm{x})\big{\}}\). We let \(\bm{y}_{\bm{x}}=(\bm{y}_{\bm{x}}^{1},\ldots,\bm{y}_{\bm{x}}^{n})^{\top}\in \mathbb{R}^{nd}\) be the conjugate pair of \(\bm{x}\in\mathbb{R}^{d}\), i.e., \(\bm{y}_{\bm{x}}^{i}=\arg\max_{\bm{y}\in\mathbb{R}^{d}}\{\left<\bm{y},\bm{x} \right>-f_{i}^{*}(\bm{y})\}\), and we denote \(\bm{y}_{\ast}=\bm{y}_{\ast}\).

Given a primal-dual pair \((\bm{x},\bm{y})\), the primal-dual gap of (PD) is defined by \(\text{Gap}(\bm{x},\bm{y})=\max_{\left<\bm{u},\bm{v}\right>}\{\mathcal{L}(\bm{ x},\bm{v})-\mathcal{L}(\bm{u},\bm{y})\}\). In particular, we consider the pair \((\bm{x},\bm{y}_{\ast})\) for \(\bm{x}\in\mathbb{R}^{d}\), and bound \(\text{Gap}^{\bm{v}}(\bm{x},\bm{y}_{\ast}):=\mathcal{L}(\bm{x},\bm{v})- \mathcal{L}(\bm{x}_{\ast},\bm{y}_{\ast})\) for an arbitrary but fixed \(\bm{v}\). To finally obtain the function value gap \(f(\bm{x})-f(\bm{x}_{\ast})\) for (P), we only need to choose \(\bm{v}=\arg\max_{\bm{v}}\mathcal{L}(\bm{x},\bm{w})=\bm{y}_{\bm{x}}\).

Using this primal-dual formulation and standard convex conjugacy arguments, we can _equivalently_ write the standard shuffled SGD algorithm in a primal-dual form as summarized in Algorithm 1.

```
1:Input: Initial point \(\bm{x}_{0}\in\mathbb{R}^{d}\), step size \(\{\eta_{k}\}>0,\) number of epochs \(K>0\)
2:for\(k=1\) to \(K\)do
3: Generate some permutation \(\pi_{k}\) of \([n]\) (either deterministic or random)
4:\(\bm{x}_{k-1,1}=\bm{x}_{k-1}\)
5:for\(i=1\) to \(n\) in the ordering of \(\pi_{k}\)do
6:\(\bm{y}_{k}^{i}=\arg\max_{\bm{y}^{i}\in\mathbb{R}^{d}}\Big{\{}\left<\bm{y}^{i},\bm{x}_{k-1,i}\right>-f_{i}^{*}(\bm{y}^{i})\Big{\}}\)
7:\(\bm{x}_{k-1,i+1}=\arg\min_{\bm{x}\in\mathbb{R}^{d}}\Big{\{}\left<\bm{y}_{k}^{i},\bm{x}\right>+\frac{1}{2\eta_{k}}\|\bm{x}-\bm{x}_{k-1,i}\|^{2}\Big{\}}=\bm{x} _{k-1,i}-\eta_{k}\nabla f(\bm{x}_{k-1,i})\)
8:endfor
9:\(\bm{x}_{k}=\bm{x}_{k-1,n+1}\), \(\bm{y}_{k}=\left(\bm{y}_{k}^{1},\bm{y}_{k}^{2},\ldots,\bm{y}_{k}^{n}\right)^{\top}\)
10:endfor
11:Return:\(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/\sum_{k=1}^{K}\eta_{k}\) ```

**Algorithm 1** Shuffled SGD (Primal-Dual View, General Convex Smooth)

Improved bounds with new smoothness constants.To prove a convergence bound for shuffled SGD in this general setting, we first construct an upper estimate of \(\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{\ast})\) for some fixed \(\bm{v}\) to be set later, as summarized in the following lemma.

**Lemma 1**.: _Under Assumption 1, for any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{i}\}_{i=1}^{n}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{n+1}\) generated by Algorithm 1 satisfy_

\[\begin{split}\mathcal{E}_{k}\leq&\ \frac{\eta_{k}}{n}\sum_{i=1}^{n}\left<\bm{y}_{k}^{i},\bm{x}_{k}-\bm{x}_{k-1,i+1 }\right>+\frac{\eta_{k}}{n}\sum_{i=1}^{n}\left<\bm{v}_{k}^{i}-\bm{y}_{k}^{i}, \bm{x}_{k}-\bm{x}_{k-1,i}\right>\\ &-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{A}_{k}^{-1}}^{2 }-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{\ast,k}\|_{\bm{A}_{k}^{-1}}^{2}- \frac{1}{2n}\sum_{i=1}^{n}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|^{2},\end{split}\] (1)

_where \(\mathcal{E}_{k}:=\eta_{k}\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{\ast})+\frac{1}{2 n}\|\bm{x}_{\ast}-\bm{x}_{k}\|_{2}^{2}-\frac{1}{2n}\|\bm{x}_{\ast}-\bm{x}_{k-1}\|_{2}^{2}\), \(\bm{v}_{k}=\bm{v}_{\pi^{(k)}}\), and \(\bm{y}_{\ast,k}=\bm{y}_{\ast,\pi^{(k)}}\) are the (block-wise) permuted vectors based on the permutation \(\pi_{k}\) at the \(k\)-th epoch._

We note that the first term \(\mathcal{T}_{1}:=\frac{\eta_{k}}{n}\sum_{i=1}^{n}\left<\bm{y}_{k}^{i},\bm{x}_{k}- \bm{x}_{k-1,i+1}\right>\) from Lemma 1 can be aggregated into the terms capturing the primal progress within one epoch and cancelled by the last term in Eq. (1). The precise bound on \(\mathcal{T}_{1}\) and its proof are provided in Lemma 10 in Appendix C.1. The second term \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{n}\left<\bm{v}_{k}^{i}-\bm{y}_{k }^{i},\bm{x}_{k}-\bm{x}_{k-1,i}\right>\) requires us to relate the intermediate iterate \(\bm{x}_{k-1,i}\) to the iterate \(\bm{x}_{k}\) after one full data pass, which corresponds to a partial sum of the component gradients, each at different iterates \(\{\bm{x}_{k-1,j}\}_{i=i}^{n}\). In contrast to prior analyses (e.g., Mishchenko et al. [30]) using the global smoothness and triangle inequality to bound this partial sum, we provide a tighter bound on \(\mathcal{T}_{2}\) that tracks the progress of the cyclic update on the dual side, in the aggregate.

To simplify the notation in the following lemmas and to clearly compare our results, we introduce the following novel definitions of smoothness constants for shuffled SGD:

\[\begin{split}\hat{L}_{\pi}^{g}&:=\frac{1}{n^{2}}\big{\|} \boldsymbol{\Lambda}_{\pi}^{1/2}\big{(}\sum_{i=1}^{n}\boldsymbol{I}_{d(i-1) \uparrow}\boldsymbol{E}\boldsymbol{E}^{\top}\boldsymbol{I}_{d(i-1)\uparrow} \big{)}\boldsymbol{\Lambda}_{\pi}^{1/2}\big{\|}_{2},&\hat{L}^{g} =\max_{\pi}\hat{L}_{\pi}^{g},\\ \tilde{L}_{\pi}^{g}&:=\big{\|}\boldsymbol{\Lambda}_{ \pi}^{1/2}\big{(}\sum_{i=1}^{n}\boldsymbol{I}_{(di)}\boldsymbol{E} \boldsymbol{E}^{\top}\boldsymbol{I}_{(di)}\big{)}\boldsymbol{\Lambda}_{\pi}^{ 1/2}\big{\|}_{2},&\tilde{L}^{g}=\max_{\pi}\tilde{L}_{\pi}^{g}, \end{split}\] (2)

where \(\boldsymbol{I}_{(di)}=\sum_{j=d(i-1)+1}^{di}\boldsymbol{I}_{j}\) and \(\boldsymbol{E}=[\underbrace{\boldsymbol{I}_{d},\ldots,\boldsymbol{I}_{g}}_{n}] ^{\top}\in\mathbb{R}^{nd\times d}\). Permutation-dependent quantities \(\hat{L}_{\pi}^{g}\) and \(\tilde{L}_{\pi}^{g}\) defined in (2) are obtained directly from our analysis. We remark that \(\hat{L}^{g}\) is bounded by the average smoothness of \(f\) and \(\tilde{L}^{g}\) is bounded by the max of individual smoothness constants of \(f_{i}\); see more details in Appendix B. However, as we argue in later sections, these upper bounds on \(\hat{L}_{\pi}^{g}\) and \(\tilde{L}_{\pi}^{g}\) are loose in general, and so the convergence bounds based on \(\hat{L}_{\pi}^{g}\) and \(\tilde{L}_{\pi}^{g}\) that we obtain align better with the empirical performance of shuffled SGD.

Assuming that a uniformly random data shuffling strategy is used (SO or RR), the resulting bound on \(\mathcal{T}_{2}\) is summarized in Lemma 2, while its proof is deferred to Appendix B.

**Lemma 2**.: _Under Assumptions 1 and 2, for any \(k\in[K]\), the iterates \(\{\boldsymbol{y}_{k}^{i}\}_{i=1}^{n}\) and \(\{\boldsymbol{x}_{k-1,i}\}_{i=1}^{n+1}\) generated by Algorithm 1 with uniformly random shuffling (RR/SO) satisfy_

\[\mathbb{E}[\mathcal{T}_{2}]\leq\mathbb{E}\Big{[}\eta_{k}^{3}n\hat{L}_{\pi^{(k )}}^{g}\tilde{L}_{\pi^{(k)}}^{g}\|\boldsymbol{y}_{k}-\boldsymbol{y}_{*,k}\|_{ \boldsymbol{\Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\boldsymbol{v}_{k}- \boldsymbol{y}_{k}\|_{\boldsymbol{\Lambda}_{k}^{-1}}^{2}\Big{]}+\frac{\eta_{ k}^{3}(n+1)\tilde{L}^{g}}{6}\sigma_{*}^{2},\]

_where \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{n}\big{\langle}\boldsymbol{v}_{ k}^{i}-\boldsymbol{y}_{k}^{i},\boldsymbol{x}_{k}-\boldsymbol{x}_{k-1,i}\big{\rangle}\), \(\boldsymbol{v}_{k}=\boldsymbol{v}_{\pi^{(k)}}\) and \(\boldsymbol{y}_{*,k}=\boldsymbol{y}_{*,\pi^{(k)}}\)._

With Lemmas 1 and 2 in tow, we are ready to present the main result of this section.

**Theorem 1**.: _Under Assumptions 1 and 2, if \(\eta_{k}\leq\frac{1}{n\sqrt{2L_{\pi}^{g}(k)}\tilde{L}_{\pi^{(k)}}^{g}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), the output \(\hat{\boldsymbol{x}}_{K}\) of Algorithm 1 with uniformly random (RR/SO) shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\boldsymbol{x}}_{K})-f(\boldsymbol{x}_{*}))]\leq \frac{1}{2n}\|\boldsymbol{x}_{0}-\boldsymbol{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K} \frac{\eta_{k}^{3}(n+1)\tilde{L}^{g}}{6}\sigma_{*}^{2}.\]

_As a consequence, for any \(\epsilon>0,\) there exists a choice of a constant step size \(\eta_{k}=\eta\) for which \(\mathbb{E}[f(\hat{\boldsymbol{x}}_{K})-f(\boldsymbol{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\sqrt{\tilde{L}\tilde{L}^{g}\|\boldsymbol{x}_{0}- \boldsymbol{x}_{*}\|_{2}^{2}}}{\epsilon}+\frac{\sqrt{\tilde{n}\tilde{L}^{g} \sigma_{*}\|\boldsymbol{x}_{0}-\boldsymbol{x}_{*}\|_{2}^{2}}}{\epsilon^{3/2}} \big{)}\) individual gradient queries._

## 3 Tighter Bounds for Convex Finite-Sum Problems with Linear Predictors

To study the effect of the structure of the data on the convergence of shuffled SGD, we sharpen the focus from general finite-sum problems to convex finite-sum with linear predictors:

\[\min_{\boldsymbol{x}\in\mathbb{R}^{d}}\Big{\{}f(\boldsymbol{x}):=\frac{1}{n} \sum_{i=1}^{n}\ell_{i}(\boldsymbol{a}_{i}^{\top}\boldsymbol{x})\Big{\}},\] (PL)

where \(\boldsymbol{a}_{i}\in\mathbb{R}^{d}\) (\(i\in[n]\)) are data vectors and \(\ell_{i}:\mathbb{R}\rightarrow\mathbb{R}\) are convex and either smooth or Lipschitz nonsmooth functions associated with the linear predictors \(\langle\boldsymbol{a}_{i},\boldsymbol{x}\rangle\) for \(i\in[n]\). In addition to their explicit dependence on the data, it is worth noting that problems of the form (PL) cover most of the standard convex ERM problems where shuffled SGD is commonly applied, such as support vector machines, least absolute deviation, least squares, and logistic regression.

Problem (PL) admits an explicit primal-dual formulation using the standard Fenchel conjugacy argument (see, e.g., [13; 14]),

\[\min_{\boldsymbol{x}\in\mathbb{R}^{d}}\max_{\boldsymbol{y}\in\mathbb{R}^{n}} \Big{\{}\mathcal{L}(\boldsymbol{x},\boldsymbol{y}):=\frac{1}{n}\left\langle \boldsymbol{A}\boldsymbol{x},\boldsymbol{y}\right\rangle-\frac{1}{n}\sum_{i=1}^ {n}\ell_{i}^{*}(\boldsymbol{y}^{i})=\frac{1}{n}\sum_{i=1}^{n}\big{(}\boldsymbol {a}_{i}^{\top}\boldsymbol{x}\boldsymbol{y}^{i}-\ell_{i}^{*}(\boldsymbol{y}^{i}) \big{)}\Big{\}},\] (PL-PD)

where \(\boldsymbol{A}=[\boldsymbol{a}_{1},\boldsymbol{a}_{2},\ldots,\boldsymbol{a}_{n}] ^{\top}\in\mathbb{R}^{n\times d}\) is the data matrix and \(\ell_{i}^{*}:\mathbb{R}\rightarrow\mathbb{R}\) is the convex conjugate of \(\ell_{i}\). This observation allows us to again interpret without-replacement SGD updates as cyclic coordinate updates on the dual side. Note that due to the objective structure in (PL), the primal-dual formulation (PL-PD) can decouple the linear (\(\bm{a}_{i}^{\top}\bm{x}\)) and the non-linear (\(\ell_{i}\)) parts within individual loss functions \(f_{i}\). We redefine the conjugate pair of \(\bm{x}\in\mathbb{R}^{d}\) to be \(\bm{y}_{\bm{x}}=(\bm{y}_{\bm{x}}^{1},\ldots,\bm{y}_{\bm{x}}^{n})^{\top}\in \mathbb{R}^{n}\), with \(\bm{y}_{\bm{x}}^{i}=\arg\max_{\bm{y}^{i}\in\mathbb{R}}\{\bm{y}^{i}\bm{a}_{i}^ {\top}\bm{x}-\ell_{i}^{s}(\bm{y}^{i})\}\).

In this section, we consider shuffled SGD with _mini-batch_ estimators of size \(b\) and assume without loss of generality that \(n=bm\) for some positive integer \(m\). The detailed primal-dual view of shuffled SGD adapted to (PL-PD) and mini-batch estimators is provided in Alg. 2 in Appendix C.

### Smooth and convex objectives

Throughout this subsection, we make the following (standard) assumptions, corresponding to Assumptions 1 and 2 from Section 2.

**Assumption 3**.: _Each \(\ell_{i}\) is convex and \(L_{i}\)-smooth \((i\in[n])\), i.e., \(|\ell_{i}^{\prime}(x)-\ell_{i}^{\prime}(y)|\leq L_{i}|x-y|\) for any \(x,y\in\mathbb{R}\). There exists a minimizer \(\bm{x}_{*}\in\arg\min_{\bm{x}\in\mathbb{R}^{d}}f(\bm{x})\)._

We remark that Assumption 3 implies that both \(f\) and each component function \(f_{i}(\bm{x})=\ell_{i}(\bm{a}_{i}^{\top}\bm{x})\) are \(L_{\max}\)-smooth, where \(L_{\max}=\max_{i\in[n]}L_{i}\|\bm{a}_{i}\|_{2}^{2}\). Assumption 3 also implies that each convex conjugate \(\ell_{i}^{*}\) is \(\frac{1}{L_{i}}\)-strongly convex [3]. In the following, we let \(\bm{\Lambda}=\text{diag}(L_{1},L_{2},\ldots,L_{n})\), and \(\bm{\Lambda}_{\pi}=\text{diag}\big{(}L_{\pi^{1}},L_{\pi^{2}},\ldots,L_{\pi^{n}} \big{)}\), given a permutation \(\pi\) of \([n]\).

We further assume bounded variance at \(\bm{x}_{*}\), same as prior work [30, 34, 45, 46].

**Assumption 4**.: \(\sigma_{*}^{2}:=\frac{1}{n}\sum_{i=1}^{n}\|\nabla f_{i}(\bm{x}_{*})\|^{2}= \frac{1}{n}\sum_{i=1}^{n}(\ell_{i}^{\prime}(\bm{a}_{i}^{\top}\bm{x}_{*}))^{2} \|\bm{a}_{i}\|_{2}^{2}\) _is bounded._

Improved bounds with new smoothness constants.Our convergence bounds depend on the smoothness parameters defined in Eq. (3) below. We provide a detailed discussion on how these parameters relate to traditional smoothness parameters both in the worst case and on typical datasets, in Section 4.1, with additional numerical results provided in Appendix E.

\[\begin{split}\hat{L}_{\pi}&:=\frac{1}{mn}\big{\|} \bm{\Lambda}_{\pi}^{1/2}\big{(}\sum_{j=1}^{m}\bm{I}_{b(j-1)\uparrow}\bm{A}_{ \pi}\bm{A}_{\pi}^{\top}\bm{I}_{b(j-1)\uparrow}\big{)}\bm{\Lambda}_{\pi}^{1/2} \big{\|}_{2},\hskip 28.452756pt\hat{L}=\max_{\pi}\hat{L}_{\pi},\\ \tilde{L}_{\pi}&:=\frac{1}{b}\big{\|}\bm{\Lambda}_{ \pi}^{1/2}\big{(}\sum_{j=1}^{m}\bm{I}_{(j)}\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\bm{ I}_{(j)}\big{)}\bm{\Lambda}_{\pi}^{1/2}\big{\|}_{2},\hskip 56.905512pt\tilde{L}=\max_{\pi} \tilde{L}_{\pi},\end{split}\] (3)

where \(\bm{I}_{(j)}:=\sum_{i=b(j-1)+1}^{bj}\bm{I}_{i}\). In comparison to the smoothness constants defined in Eq. (2) for general finite-sum problems, we note that the constants in Eq. (3) applying to generalized linear models are tighter and more informative estimates, as the data matrix \(\bm{A}\) and the smoothness constants from the nonlinear part \(\bm{\Lambda}\) are separated in Eq. (3). Thus, the constants \(\hat{L}_{\pi}\) and \(\tilde{L}_{\pi}\) directly depend on the data matrix, which explicitly demonstrates how the structure of the data affects the convergence of shuffled SGD. The following theorem states the convergence of Algorithm 2 with these new refined smoothness constants, while its proof is provided in Appendix C.

**Theorem 2**.: _Under Assumptions 3 and 4, if \(\eta_{k}\leq\frac{b}{n\sqrt{2L_{\pi^{(k)}}L_{\pi^{(k)}}}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), then the output \(\hat{\bm{x}}_{K}\) of Alg. 1 with uniformly random (RR/SO) shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{b}{2n}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{6 b^{2}(n-1)}\sigma_{*}^{2}.\]

_As a result, given \(\epsilon>0\), there exists a constant step size \(\eta_{k}=\eta\) such that \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\sqrt{\tilde{L}\tilde{L}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2 }^{2}}{\epsilon}+\sqrt{\frac{(n-b)(n+b)}{n(n-1)}\sqrt{n\tilde{L}}\sigma_{*}\| \bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}\big{)}\) individual gradient queries._

A few remarks are in order here. When \(b=n\), we recover the standard guarantee of gradient descent, which serves as a sanity check as in this case the algorithm reduces to standard gradient descent. When \(\epsilon=\Omega(\frac{(n-b)(n+b)\sigma_{*}^{2}}{n^{2}(n-1)L})\), the resulting complexity is \(\mathcal{O}\big{(}\frac{n\sqrt{\tilde{L}\tilde{L}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2 }^{2}}{\epsilon}\big{)}\). Observe that this case can happen when either \(\epsilon\) is large (compared to, say, \(1/n\)) or when \(\sigma_{*}\) is small (it is, in fact, possible for \(\sigma_{*}\) to be zero, which happens, for example, when the data rows are linearly independent). Unlikein bounds from previous work, we observe from our bounds the benefit of using shuffled SGD compared to full gradient descent, where the difference is by a factor that can be as large as \(\sqrt{n}\), as we have discussed in the introduction (see also Section 4). When \(\epsilon=\mathcal{O}(\frac{(n-b)(n+b)\sigma^{2}}{n^{2}(n-1)L}),\) the second term in our complexity bound dominates. In this case, when \(b=1\), we recover the state of the art results from [12; 30; 34], while for \(b>1\) our bound provides the \(\Omega\big{(}\sqrt{\frac{n(n-1)}{(n-b)(n+b)}\cdot\frac{L}{L}}\big{)}\)-factor improvement, providing insights into benefits from the mini-batching strategy commonly used in practice.

### Extension to non-smooth convex objectives

In non-smooth settings, we make the following standard assumption.

**Assumption 5**.: _Each \(\ell_{i}\) is convex and \(G_{i}\)-Lipschitz \((i\in[n])\), i.e., \(|\ell_{i}(x)-\ell_{i}(y)|\leq G_{i}|x-y|\) for any \(x,y\in\mathbb{R}\); thus \(|g_{i}(x)|\leq G_{i}\) where \(g_{i}(x)\in\partial\ell_{i}(x)\). There exists a minimizer \(\bm{x}_{*}\in\arg\min_{\bm{x}\in\mathbb{R}^{d}}f(\bm{x})\)._

If Assumption 5 holds, each \(\ell_{i}(\bm{a}_{i}^{\top}\bm{x})\) is also \(G_{\max}\)-Lipschitz with respect to \(\bm{x}\), where \(G_{\max}=\max_{i\in[n]}G_{i}\|\bm{a}_{i}\|_{2}\). To state our results, we define \(\bm{\Gamma}:=\text{diag}(G_{1}^{2},G_{2}^{2},\ldots,G_{n}^{2})\) and \(\bm{\Gamma}_{\pi}=\text{diag}\big{(}G_{\pi_{1}}^{2},G_{\pi_{2}}^{2},\ldots,G_{ \pi_{n}}^{2}\big{)}\), given a data permutation \(\pi\) of \([n]\).

We now extend our analysis of Algorithm 1 to convex nonsmooth Lipschitz settings, where the conjugate functions \(\ell_{i}^{*}(y^{i})\) are only convex. Proceeding as in Lemma 1, we obtain a bound on the primal-dual gap similar to (1), but lose two retraction terms induced by smoothness. Instead of cancelling the corresponding error terms like in the smooth case, we rely on the boundedness of the subgradients to bound these terms under a sufficiently small step size, which is common in nonsmooth Lipschitz settings. Similar to Section 2, we introduce the following quantities to obtain a tighter guarantee with respect to the data matrix and Lipschitz constants

\[\hat{G}_{\pi} :=\frac{1}{mn}\big{\|}\bm{\Gamma}_{\pi}^{1/2}\big{(}\sum_{j=1}^{m }\bm{I}_{b(j-1)\uparrow}\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\bm{I}_{b(j-1)\uparrow }\big{)}\bm{\Gamma}_{\pi}^{1/2}\big{\|}_{2},\] \[\tilde{G}_{\pi} :=\frac{1}{b}\big{\|}\bm{\Gamma}_{\pi}^{1/2}\big{(}\sum_{j=1}^{m }\bm{I}_{(j)}\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\bm{I}_{(j)}\big{)}\bm{\Gamma}_{ \pi}^{1/2}\big{\|}_{2}.\]

We discuss the improvements in convergence from \(\hat{G}_{\pi}\) and \(\tilde{G}_{\pi}\) in Section 4, while the convergence of Algorithm 2 is described in Theorem 3, with its proof deferred to Appendix D.

**Theorem 3**.: _Under Assumption 5, if \(H_{K}=\sum_{k=1}^{K}\eta_{k}\) and \(\bar{G}=\mathbb{E}_{\pi}[\sqrt{\hat{G}_{\pi}\tilde{G}_{\pi}}]\), the output \(\hat{\bm{x}}_{K}\) of Alg. 1 with possible uniformly random shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{1}{2n}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}2\eta_{k}^{2}n\bar{G},\]

_As a result, for any \(\epsilon>0,\) there exists a step size \(\eta_{k}=\eta\) such that \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\bar{G}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{2}} \big{)}\) individual gradient queries._

## 4 Discussion of Our New Smoothness Constants and Numerical Results

To succinctly explain where our improvements come from, we now consider (PL) where \(\ell_{i}\) is \(1\)-smooth and \(b=1\), ignoring the gains from the mini-batch estimators (for large \(K\)) and our softer guarantee that handles individual smoothness constants. For this specific case, \(\hat{L}=L_{\max}=\max_{1\leq i\leq n}\|\bm{a}_{i}\|^{2},\) and thus our results for the smooth case and the RR and SO variants match state of the art in the second term, which dominates when there are many (\(K=\Omega\big{(}\frac{L_{\max}^{2}D^{2}n}{\sigma_{*}^{2}}\big{)}\)) epochs. When there are \(K=O(\frac{L_{\max}^{2}D^{2}n}{\sigma_{*}^{2}})\) epochs in the SO and RR variants or for all regimes of \(K\) in the IG variant, the difference between our and state of the art bounds comes from the constant \(\hat{L}\) that replaces \(L_{\max}\), and our improvement is by a factor \(\sqrt{L_{\max}/\hat{L}}\). Note that \(\mathcal{O}(\frac{nL_{\max}}{\epsilon})\) from prior bounds, which is the dominating term in the small \(K\) regime, is even worse than the complexity of full gradient descent, as the full gradient Lipschitz constant of \(f\) in this case is \(\frac{1}{n}\|\bm{A}\bm{A}^{\top}\|_{2}\leq L_{\max}\).

Given a worst-case permutation \(\bar{\pi}\), and denoting by \(\bm{A}_{\bar{\pi}}\) the data matrix \(\bm{A}\) with its rows permuted according to \(\bar{\pi}\), our constant \(\hat{L}\) can be bounded above by \(L_{\max}\) using the following sequence of inequalities:

\[\hat{L}=\tfrac{1}{n^{2}}\|\sum_{j=1}^{n}\bm{I}_{(j-1)\uparrow}\bm{A }_{\bar{\pi}}\bm{A}_{\bar{\pi}}^{\top}\bm{I}_{(j-1)\uparrow}\|_{2} \overset{(i)}{\leq}\tfrac{1}{n^{2}}\sum_{j=1}^{n}\|\bm{I}_{(j-1) \uparrow}\bm{A}_{\bar{\pi}}\bm{A}_{\bar{\pi}}^{\top}\bm{I}_{(j-1)\uparrow}\|_{2}\] \[\overset{(ii)}{\leq}\tfrac{1}{n^{2}}\sum_{j=1}^{n}\|\bm{A}_{\bar{ \pi}}\bm{A}_{\bar{\pi}}^{\top}\|_{2}\] (4) \[\overset{(iii)}{\leq}\tfrac{1}{n}\sum_{i=1}^{n}\|\bm{a}_{i}\|_{2} ^{2}\leq\max_{1\leq i\leq n}\|\bm{a}_{i}\|_{2}^{2}=L_{\max},\]

where \((i)\) holds by the triangle inequality, \((ii)\) holds because the operator norm of the matrix \(\bm{I}_{(j-1)\uparrow}\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\bm{I}_{(j-1)\uparrow}\) (equal to the operator norm of the bottom right \((n-j+1)\times(n-j+1)\) submatrix of \(\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\)) is always at most \(\|\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\|=\|\bm{A}\bm{A}^{\top}\|\), for any permutation \(\pi\), and \((iii)\) holds by bounding above the operator norm of a symmetric matrix by its trace. Hence \(\hat{L}\) is never larger than \(L_{\max}\), but can generally be much smaller, due to the sequence of inequality relaxations in (4). While each of these inequalities can be loose, we emphasize that \((iii)\) is almost always loose, by a factor that can be as large as \(n\).

As a specific example where \(\hat{L}\) is smaller than \(L_{\max}\) by a factor of \(n\), consider the example of Gaussian data, where we draw \(n\) i.i.d. standard Gaussian vectors from \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\) and take \(d=n\). By standard concentration results, with high probability, all columns/rows of \(\mathbf{A}_{\bar{\pi}}\) in this case are near-orthogonal (see, e.g., [7, Chapter]) and \(\|\bm{a}_{i}\|_{2}^{2}\approx d=n\) for all \(i\). As a result, the operator norm to trace inequality (\(iii\)) is loose by a factor \(d=n\), with high probability. Note that in this example all individual smoothness parameters of components \(f_{i}\) are essentially the same (w.h.p.) and equal \(\|\bm{a}_{i}\|_{2}^{2}\), thus the improvement of our bound on the smoothness parameter does not come from averaging but from the structure of the data. This observation is important for contrasting the results from Section 2 and Section 3. In particular, focusing solely on the finite sum structure and ignoring the structure of the data matrix would provide no improvements in the resulting convergence bounds.

As further evidence, we empirically evaluate \(L_{\max}/\hat{L}\) on 15 large-scale machine learning datasets and demonstrate that on those datasets \(L_{\max}/\hat{L}\) is of the order \(n^{\alpha},\) for \(\alpha\in[0.15,0.96]\) (see Sec. 4.1 for more details), providing strong evidence of a tighter guarantee as a function of \(n\).

For the nonsmooth settings, by a similar sequence of inequalities, we can show that \(\bar{G}\leq G_{\max}^{2},\) which can be loose by a factor \(1/n\) due to the operator norm to trace inequality. Thus, our bound is never worse than what would be obtained from the full subgradient method, but can match the bound of standard SGD, or even improve1 upon it for at least some data matrices \(\bm{A}\).

Footnote 1: This is because it is possible for inequalities \((i)\) and \((ii)\) to be loose, in addition to \((iii)\).

### Numerical results and discussion

In this section, we provide empirical evidence to support our claim about usefulness of the new convergence bounds obtained in our work. In particular, we conduct numerical evaluations to compare \(\hat{L}\) to the classical smoothness constant \(L\) on synthetic datasets and on popular machine learning benchmark datasets.

For a more streamlined comparison and to focus on the dependence on the data matrix, we assume that the loss functions \(\ell_{i}\) all have the same smoothness constant, which leads to \(L_{\max}/\hat{L}=(\max_{1\leq i\leq n}\{\|\bm{a}_{i}\|^{2}\})/(\tfrac{1}{n^{2}} \|\sum_{j=1}^{n}\bm{I}_{(j-1)\uparrow}\bm{A}_{\bar{\pi}}\bm{A}_{\bar{\pi}}^{ \top}\bm{I}_{(j-1)\uparrow}\|_{2})\). Since the scale of the smoothness constant of the loss functions is irrelevant for the ratio \(L_{\max}/\hat{L}\) in this case, for simplicity, we take it to equal one. Note that assuming different smoothness constants over component loss functions would only make our bound better compared to related work (see Eq. (3) and the discussion following it).

We also compare \(\hat{L}\) and \(L_{\max}\) on a number of benchmarking datasets from LIBSVM [15], MNIST [17], CIFAR10 [22], and Broad Bioimage Benchmark Collection [28]. For each dataset, we generate a uniformly random permutation \(\pi\) for the data matrix \(\bm{A}\) and compute \(\hat{L}_{\pi}\). We repeat this procedure \(1000\) times for all datasets and display the average \(L_{\max}/\hat{L}_{\pi}\) in Table 2, except for e2006train, CIFAR10, MNIST, and BBBC005 where we do \(20\) repetitions due to limitations of computation resources required for each calculation. We observe that among the datasets that we consider, which contain all three data matrix "shapes" \(d>>n\), \(d<<n\), and \(d\approx n\), our novel bound dependent on \(\hat{L}\) is much tighter. For instance, for rcv1 and real-sim datasets, where \(d\) and \(n\) are of the same order, we observe that \(L_{\max}/\hat{L}\) are approximately \(111\) and \(194\), respectively. For news20 dataset where \(d>>n\), \(L_{\max}/\hat{L}\approx 42.1\). For MNIST, where \(d<<n\), \(L_{\max}/\hat{L}\approx 19.1\). Further results are provided in Appendix E.

Finally, as a justification for using the empirical mean of \(\hat{L}_{\pi}\) over random permutations \(\pi\) in the results displayed in Table 2, we observe in our evaluations that the values of \(L_{\max}/\hat{L}_{\pi}\) are fairly concentrated around their empirical mean values. Histogram plots showing the empirical distributions of \(L_{\max}/\hat{L}_{\pi}\) for each of the datasets are provided in Appendix E.

We conclude with a few additional remarks. Our results indicate that the structure of the data is important for predicting behavior of popular machine learning methods such as variants of shuffled SGD considered in our work, and thus should be incorporated in their study: as demonstrated in the Gaussian data example, considering simple finite sum structure and ignoring the dependence on the data can lead to overly pessimistic bounds. Thus it would be interesting to provide a further theoretical study of shuffled SGD that incorporates distributional assumptions for the data. Additionally, as mentioned in the previous paragraph, we empirically observed that permutation-dependent parameter \(\hat{L}_{\pi}\) concentrates around its mean for permutations generated uniformly at random. Thus, it would be interesting to consider whether our theoretical results can be strengthened to depend on the mean value of \(\hat{L}_{\pi}\) (as opposed to maximum). We leave such considerations for future work.

## Acknowledgements

This research was supported in part by the U.S. Office of Naval Research under contract number N00014-22-1-2348.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Dataset & \#Features (\(d\)) & \#D datapoints (\(n\)) & \(L_{\max}/\hat{L}\) & \(\log_{n}L_{\max}/\hat{L}\) & \(\log_{\min(d,n)}L_{\max}/\hat{L}\) \\ \hline a1a & 123 & 1605 & 5.50 & 0.231 & 0.354 \\ A9a & 123 & 32561 & 5.49 & 0.164 & 0.354 \\ BBBC005 & 361920 & 19201 & 18.3 & 0.295 & 0.295 \\ BBBC010 & 361920 & 201 & 7.04 & 0.368 & 0.368 \\ cifar10 & 3072 & 50000 & 10.0 & 0.213 & 0.287 \\ Duke & 7129 & 44 & 38.0 & 0.962 & 0.962 \\ E2006train & 150360 & 16087 & 5.35 & 0.173 & 0.173 \\ gistette & 5000 & 6000 & 3.52 & 0.145 & 0.148 \\ Leu & 7129 & 38 & 32.8 & 0.960 & 0.960 \\ mnist & 780 & 60000 & 19.1 & 0.268 & 0.443 \\ news20 & 1355191 & 19996 & 42.1 & 0.378 & 0.378 \\ rcv1 & 47236 & 20242 & 111 & 0.475 & 0.475 \\ real-sim & 20958 & 72309 & 194 & 0.471 & 0.529 \\ sonar & 60 & 208 & 6.26 & 0.344 & 0.448 \\ Tmc2007 & 30438 & 21519 & 10.9 & 0.239 & 0.239 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The following table shows the computed values of \(L_{\max}/\hat{L}\) where \(\hat{L}\) is the empirical mean of \(\hat{L}_{\pi}\) over random permutations. We note that the quantity \(\sqrt{L_{\max}/\hat{L}}\) represents the improvement provided by the bound via our novel primal-dual perspective, compared to previous work.

## References

* Agarwal et al. [2009] Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar. Information-theoretic lower bounds on the oracle complexity of convex optimization. In _Proc. NeurIPS'09_, 2009.
* Ahn et al. [2020] Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component convexity and large epoch requirements. In _Proc. NeurIPS'20_, 2020.
* Beck [2017] Amir Beck. _First-order methods in optimization_. SIAM, 2017.
* Beck and Tetruashvili [2013] Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type methods. _SIAM Journal on Optimization_, 23(4):2037-2060, 2013.
* Bengio [2012] Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. _Neural Networks: Tricks of the Trade: Second Edition_, 2012.
* Bertsekas and Tsitsiklis [2000] Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. _SIAM Journal on Optimization_, 10(3):627-642, 2000.
* Blum et al. [2020] Avrim Blum, John Hopcroft, and Ravi Kannan. _Foundations of Data Science_. Cambridge University Press, Cambridge, 2020. ISBN 9781108485067. doi: DOI:https://www.cambridge.org/core/books/foundations-of-data-science/6A43CE830DE83BED6CC5171E62BOAA9E.
* Bottou [2009] Leon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In _Proc. Symposium on Learning and Data Science, Paris'09_, 2009.
* Bottou et al. [2018] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM Review_, 60(2):223-311, 2018.
* Bubeck et al. [2015] Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 2015.
* Cai et al. [2022] Xufeng Cai, Chaobing Song, Stephen J Wright, and Jelena Diakonikolas. Cyclic block coordinate descent with variance reduction for composite nonconvex optimization. _arXiv preprint arXiv:2212.05088_, 2022.
* Cha et al. [2023] Jaeyoung Cha, Jaewook Lee, and Chulhee Yun. Tighter lower bounds for shuffling SGD: Random permutations and beyond. _arXiv preprint arXiv:2303.07160_, 2023.
* Chambolle and Pock [2011] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. _Journal of Mathematical Imaging and Vision_, 40(1):120-145, 2011.
* Chambolle et al. [2018] Antonin Chambolle, Matthias J Ehrhardt, Peter Richtarik, and Carola-Bibiane Schonlieb. Stochastic primal-dual hybrid gradient algorithm with arbitrary sampling and imaging applications. _SIAM Journal on Optimization_, 28(4):2783-2808, 2018.
* Chang and Lin [2011] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 2(3):1-27, 2011.
* De Sa [2020] Christopher M De Sa. Random reshuffling is not always better. In _Proc. NeurIPS'20_, 2020.
* Deng [2012] Li Deng. The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* Gurbuzbalaban et al. [2019] M Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo. Convergence rate of incremental gradient and incremental Newton methods. _SIAM Journal on Optimization_, 29(4):2542-2565, 2019.
* Gurbuzbalaban et al. [2017] Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A Parrilo, and Nuri Vanli. When cyclic coordinate descent outperforms randomized coordinate descent. In _Proc. NeurIPS'17_, 2017.
* Gurbuzbalaban et al. [2021] Mert Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic gradient descent. _Mathematical Programming_, 186:49-84, 2021.

* [21] Jeff Haochen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In _Proc. ICML'19_, 2019.
* [22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [23] Zehua Lai and Lek-Heng Lim. Recht-re noncommutative arithmetic-geometric mean conjecture is false. In _Proc. ICML'20_, 2020.
* [24] Ching-Pei Lee and Stephen J Wright. Random permutations fix a worst case for cyclic coordinate descent. _IMA Journal of Numerical Analysis_, 39(3):1246-1275, 2019.
* [25] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Jason D Lee. Incremental methods for weakly convex optimization. _arXiv preprint arXiv:1907.11687_, 2019.
* [26] Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Mingyi Hong. On faster convergence of cyclic block coordinate descent-type methods for strongly convex minimization. _The Journal of Machine Learning Research_, 18(1):6741-6764, 2017.
* [27] Cheuk Yin Lin, Chaobing Song, and Jelena Diakonikolas. Accelerated cyclic coordinate dual averaging with extrapolation for composite convex optimization. _arXiv preprint arXiv:2303.16279_, 2023.
* [28] Vebjorn Ljosa, Katherine L. Sokolnicki, and Anne E Carpenter. Broad bioimage benchmark collection. https://bbbc.broadinstitute.org/image_sets, 2012. Accessed: 2023-05-16.
* [29] Olvi L Mangasarian and MV Solodov. Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 1993.
* [30] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis with vast improvements. In _Proc. NeurIPS'20_, 2020.
* [31] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates for general smooth convex functions. In _Proc. ICML'19_, 2019.
* [32] Angelia Nedic and Dimitri P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. _SIAM Journal on Optimization_, 12(1):109-138, 2001.
* [33] Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. _SIAM Journal on Optimization_, 22(2):341-362, 2012.
* [34] Lam M Nguyen, Quoc Tran-Dinh, Dzung T Phan, Phuong Ha Nguyen, and Marten Van Dijk. A unified convergence analysis for shuffling-type gradient methods. _The Journal of Machine Learning Research_, 22(1):9397-9440, 2021.
* [35] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. In _Proc. ICML'20_, 2020.
* [36] Benjamin Recht and Christopher Re. Toward a noncommutative arithmetic-geometric mean inequality: Conjectures, case-studies, and consequences. In _Proc. COLT'12_, 2012.
* [37] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix completion. _Mathematical Programming Computation_, 2013.
* [38] Herbert Robbins and Sutton Monro. A stochastic approximation method. _The Annals of Mathematical Statistics_, pages 400-407, 1951.
* [39] Itay Safran and Ohad Shamir. How good is SGD with random shuffling? In _Proc. COLT'20_, 2020.
* [40] Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate descent methods. _SIAM Journal on Optimization_, 23(1):576-601, 2013.

* [41] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. _Journal of Machine Learning Research_, 14(1), 2013.
* [42] Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In _Proc. NeurIPS'16_, 2016.
* [43] Chaobing Song and Jelena Diakonikolas. Fast cyclic coordinate dual averaging with extrapolation for generalized variational inequalities. _arXiv preprint arXiv:2102.13244_, 2021.
* [44] Ruoyu Sun and Yinyu Ye. Worst-case complexity of cyclic coordinate descent: \(O(n^{2})\) gap with randomized version. _Mathematical Programming_, 185(1):487-520, 2021.
* [45] Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. SMG: A shuffling gradient-based method with momentum. In _Proc. ICML'21_, 2021.
* [46] Trang H Tran, Katya Scheinberg, and Lam M Nguyen. Nesterov accelerated shuffling gradient method for convex optimization. In _Proc. ICML'22_, 2022.
* [47] Stephen Wright and Ching-pei Lee. Analyzing random permutations for cyclic coordinate descent. _Mathematics of Computation_, 89(325):2217-2248, 2020.
* [48] Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex optimization. _SIAM Journal on Optimization_, 25(3):1686-1716, 2015.
* [49] Yangyang Xu and Wotao Yin. A globally convergent algorithm for nonconvex optimization based on block coordinate update. _Journal of Scientific Computing_, 72(2):700-734, 2017.
* [50] Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali H Sayed. Stochastic learning under random reshuffling with constant step-sizes. _IEEE Transactions on Signal Processing_, 67(2):474-489, 2018.
* [51] Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond. In _Proc. ICLR'22_, 2022.

**Supplementary Material**

Outline.The supplementary material of the paper is organized as follows:

* Section A provides a brief survey on shuffled SGD and its related work.
* Section B presents the proofs related to the smooth convex setting from Section 2, where we only assume each component function \(f_{i}\) to be convex and \(L_{i}\)-smooth.
* Section C presents the proofs related to the smooth convex setting with linear predictors from Section 3.
* Section D presents the proofs related to the non-smooth convex setting with linear predictors from Section 3.
* Section E presents the full details of the computational experiments performed in the paper.

## Appendix A Further Related Work

In this section, we continue with the discussion on the background of shuffled SGD from Section 1. We would like to briefly recall that shuffled SGD usually performs better in practice when compared to SGD, and is also easier and more efficient to implement. However, in terms of the theoretical analysis, sampling without replacement introduces the sampling bias at each iteration, making it difficult to approximate shuffled SGD by full gradient descent. Using empirical observations, shuffled SGD was conjectured to converge much faster than SGD with replacement, based on the _noncommutative arithmetic-geometric mean inequality_ conjecture [36], which was later proved to be false [16, 23]. As a consequence, whether or not shuffled SGD can be faster than SGD at least in some regimes remained open [8] until a breakthrough result in [20], where it was shown that for the class of smooth strongly convex optimization problems, the convergence of the RR variant of shuffled SGD is essentially of the order-\((1/K^{2})\) for \(K\) full passes of the data (also called epochs), which is faster than order-\((1/nK)\) convergence of SGD for sufficiently large \(K\). This bound for the smooth strongly convex case was later improved under various regimes and additional assumptions [2, 21, 30, 31, 34, 42], while the tightest of those bounds were matched by lower bounds in [12, 35, 39, 51].

Since our results are for the general (non-strongly) convex regimes, in this section we focus on the results that apply to those (convex, smooth or nonsmooth Lipschitz) regimes. For convex nonsmooth Lipschitz problems, we are only aware of the results in [42]. These results are only useful when the number of data passes \(K\) is small and the number of component functions \(n\) is large, as they contain an irreducible order-\(\frac{1}{\sqrt{n}}\) error, and are not directly comparable to our results.

For the IG variant of SGD without replacement (deterministic order), asymptotic convergence was established in [6, 29], with further convergence results for both smooth and nonsmooth settings provided in [18, 25, 30, 32, 34, 50]. As IG does not benefit from randomization, it is known to have a worse convergence bound than RR under the Lipschitz Hessian assumption [18, 21], which was also shown in more general settings [30].

In this paper, we viewed shuffled SGD as a primal-dual method where the updates are performed on the dual side in a cyclic manner, thus we can leverage techniques from general cyclic methods. However, in contrast to randomized methods (corresponding to standard SGD), cyclic methods are usually more challenging to analyze [33], basic variants exhibit much worse _worst-case_ complexity than even full gradient methods [4, 19, 26, 26, 40, 44, 48, 49], with more refined results being established only recently [11, 27, 43]. While the inspiration for our work came from these recent results [11, 27, 43], they are completely technically disjoint. First, all these results rely on non-standard block Lipschitz assumptions, which are not present in our work. Second, all of them leverage proximal gradient-style cyclic updates to carry out the analysis, which is inapplicable in our case for the cyclic updates on the dual side, as otherwise the method would not correspond to (shuffled) SGD. Finally, [27, 43] utilize extrapolation steps, which would break the connection to shuffled SGD in our setting, while [11] relies on a gradient descent-type descent lemma, which is impossible to establish in our setting.

Omitted Proofs From Section 2

In this section, we consider the general finite-sum setting where we assume that each component function \(f_{i}\) is convex and smooth, and derive the refined analysis under this setting. Here we focus on the smooth convex problems as prior work did [30; 34], since smoothness is essential to showing the advantage of shuffled SGD [31] over SGD, otherwise the rate of SGD is optimal. In particular, we study the general smooth convex finite-sum problem (P)

\[\min_{\bm{x}\in\mathbb{R}^{d}}\Big{\{}f(\bm{x}):=\frac{1}{n}\sum_{i=1}^{n}f_{ i}(\bm{x})\Big{\}},\] (P)

where each \(f_{i}\) is convex and smooth. (P) is equivalent to

\[\min_{\bm{x}\in\mathbb{R}^{d}}\max_{\bm{y}\in\mathbb{R}^{nd}}\Big{\{}\mathcal{ L}(\bm{x},\bm{y}):=\frac{1}{n}\sum_{i=1}^{n}\Big{(}\left\langle\bm{y}^{i},\bm{x} \right\rangle-f_{i}^{*}(\bm{y}^{i})\Big{)}=\frac{1}{n}\left\langle\bm{E}\bm{x },\bm{y}\right\rangle-\frac{1}{n}\sum_{i=1}^{n}f_{i}^{*}(\bm{y}^{i})\Big{\}},\] (PD)

where we slightly abuse the notation in this section and use \(\bm{y}^{i}\in\mathbb{R}^{d}\) to be the \(i\)-th \(d\) elements of the vector \(\bm{y}\) such that \(\bm{y}=(\bm{y}^{1},\dots,\bm{y}^{n})^{\top}\in\mathbb{R}^{nd}\), \(\bm{E}=[\underbrace{I_{d},\dots,I_{d}}_{n}]^{\top}\in\mathbb{R}^{nd\times d}\) is the vertical concatenation of \(n\) identity matrices \(\bm{I}_{d}\in\mathbb{R}^{d\times d}\) and \(f_{i}^{*}\) is the convex conjugate of \(f_{i}\) defined by \(f_{i}(\bm{x})=\sup_{\bm{y}^{i}\in\mathbb{R}^{d}}\left\langle\bm{y}^{i},\bm{x} \right\rangle-f_{i}^{*}(\bm{y}^{i})\). In the following, we consider the mini-batch estimator of batch size \(b\), and let \(\bm{y}^{(i)}\in\mathbb{R}^{bd}\) denote the vector comprised of the \(i^{\text{th}}\)\(bd\) elements of \(\bm{y}\). For simplicity and without loss of generality, we assume that \(n=bm\) for some positive integer \(m\), so that \(\bm{y}=(\bm{y}^{(1)},\dots,\bm{y}^{(m)})^{\top}\). Note that if choosing \(b=1\), our setting is the same as the ones in [30; 34]. Then we have the primal-dual view of shuffled SGD scheme for general smooth convex minimization as in Alg. 1, where \(\bm{E}_{b}^{\top}=[\underbrace{I_{d},\dots,I_{d}}_{b}]^{\top}\in\mathbb{R}^{ bd\times d}\) is the vertical concatenation of \(b\) identity matrices \(\bm{I}_{d}\in\mathbb{R}^{d\times d}\). Given the data permutation \(\pi^{(k)}=\{\pi_{1}^{(k)},\pi_{2}^{(k)},\dots,\pi_{n}^{(k)}\}\) of \([n]\) at the \(k\)-th epoch, we use the same notation of \(\bm{v}_{k}=(\bm{v}^{\pi_{1}^{(k)}},\dots,\bm{v}^{\pi_{n}^{(k)}})^{\top}\in \mathbb{R}^{nd}\), \(\bm{y}_{*,k}=(\bm{y}_{*}^{\pi_{1}^{(k)}},\dots,\bm{y}_{*}^{\pi_{n}^{(k)}})^{ \top}\in\mathbb{R}^{nd}\) as in previous sections except now each \(\bm{v}^{\pi_{i}^{(k)}},\bm{y}_{*}^{\pi_{i}^{(k)}}\) are \(d\)-dimensional subvectors. Further, we denote the permuted smoothness constant matrices by \(\bm{\Lambda}_{k}=\operatorname{diag}(\underbrace{L_{\pi_{1}^{(k)}},\dots,L_{ \pi_{1}^{(k)}}}_{d},\dots,\underbrace{L_{\pi_{n}^{(k)}},\dots,L_{\pi_{n}^{(k )}}}_{d})\in\mathbb{R}^{nd\times nd}\), and we use \(\bm{I}\) for \(\bm{I}_{nd}\in\mathbb{R}^{nd\times nd}\) throughout this section.

New smoothness constants and comparisons.We first recall the new smoothness constants for any permutation \(\pi\) of \([n]\), defined in Eq. (2):

\[\hat{L}_{\pi}^{g} :=\frac{1}{mn}\big{\|}\bm{\Lambda}_{\pi}^{1/2}\big{(}\sum_{i=1}^ {m}\bm{I}_{bd(i-1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\big{)} \bm{\Lambda}_{\pi}^{1/2}\big{\|}_{2},\qquad\hat{L}^{g}=\max_{\pi}\hat{L}_{\pi}^ {g},\] \[\tilde{L}_{\pi}^{g} :=\frac{1}{b}\big{\|}\bm{\Lambda}_{\pi}^{1/2}\big{(}\sum_{i=1}^ {m}\bm{I}_{(di)}\bm{E}\bm{E}^{\top}\bm{I}_{(di)}\big{)}\bm{\Lambda}_{\pi}^{1/2} \big{\|}_{2},\qquad\qquad\qquad\tilde{L}^{g}=\max_{\pi}\tilde{L}_{\pi}^{g},\]

where \(\bm{I}_{(di)}=\sum_{j=bd(i-1)+1}^{bdi}\bm{I}_{j}\).

To compare \(\hat{L}_{\pi}^{g}\) and \(L:=\max_{i\in[n]}L_{i}\), we make use of the Kronecker product with notation \(\otimes\) defined by

\[\bm{A}\otimes\bm{B}=\begin{bmatrix}A_{11}\bm{B}&\cdots&A_{1n}\bm{B}\\ \vdots&\ddots&\vdots\\ A_{m1}\bm{B}&\cdots&A_{nn}\bm{B}\end{bmatrix}\]

for two matrices \(\bm{A}\in\mathbb{R}^{m\times n}\) and \(\bm{B}\in\mathbb{R}^{p\times q}\). The following lemma states a useful fact for the Kronecker product.

**Lemma 3**.: _For square matrices \(\bm{A}\) and \(\bm{B}\) of sizes \(p\) and \(q\) and with eigenvalues \(\lambda_{i}\) (\(i\in[p]\)) and \(\mu_{j}\) (\(j\in[q]\)) respectively, the eigenvalues of \(\bm{A}\otimes\bm{B}\) are \(\lambda_{i}\mu_{j}\) for \(i\in[p],j\in[q]\)._We now use the following chain of inequalities to compare \(\tilde{L}_{\pi}^{g}\) and \(L\) for any permutation \(\pi\) of \([n]\):

\[\tilde{L}_{\pi}^{g} =\frac{1}{mn}\norm{\bm{\Lambda}_{\pi}^{1/2}\Big{(}\sum_{i=1}^{m} \bm{I}_{bd(i-1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\Big{)}\bm{ \Lambda}_{\pi}^{1/2}}_{2}\] \[\leq\frac{1}{n}\norm{\bm{\Lambda}^{1/2}\bm{E}\bm{E}^{\top}\bm{ \Lambda}^{1/2}}_{2}\] \[=\frac{1}{n}\norm{(\bm{I}_{\pi}\bm{I}_{\pi}^{\top})\otimes\bm{I}_ {d}}_{2}\] \[\stackrel{{(i)}}{{=}}\frac{1}{n}\sum_{i=1}^{n}L_{i} \leq L,\]

where we define \(\bm{I}_{\pi}=\big{(}\sqrt{L_{\pi_{1}}},\sqrt{L_{\pi_{2}}},\dots,\sqrt{L_{\pi_{n }}}\big{)}^{\top}\). For \((i)\), we use Lemma 3 and notice that the eigenvalues of \(\bm{I}_{d}\) all equal \(1\), while the largest eigenvalue of \(\bm{I}_{\pi}\bm{I}_{\pi}^{\top}=\norm{\bm{l}}_{2}^{2}=\sum_{i=1}^{n}L_{i}\), so the operator norm of \((\bm{l}_{k}\bm{I}_{k}^{\top})\otimes\bm{I}_{d}\) is \(\sum_{i=1}^{n}L_{i}\).

To compare \(\tilde{L}_{\pi}^{g}\) and \(L\), we notice that

\[\bm{\Lambda}_{\pi}^{1/2}\big{(}\sum_{i=1}^{m}\bm{I}_{(di)}\bm{E}\bm{E}^{\top} \bm{I}_{(di)}\big{)}\bm{\Lambda}_{\pi}^{1/2}=\sum_{i=1}^{m}\bm{I}_{(di)}\bm{ \Lambda}_{\pi}^{1/2}\bm{E}\bm{E}^{\top}\bm{\Lambda}_{\pi}^{1/2}\bm{I}_{(di)}\]

is a block diagonal matrix whose operator norm is the maximum of the operator norms over its diagonal block submatrices, so we have

\[\tilde{L}_{\pi}^{g} =\frac{1}{b}\max_{i\in[m]}\norm{\bm{I}_{(di)}\bm{\Lambda}_{\pi}^{ 1/2}\bm{E}\bm{E}^{\top}\bm{\Lambda}_{\pi}^{1/2}\bm{I}_{(di)}}\] \[=\frac{1}{b}\max_{i\in[m]}\norm{\bm{I}_{(di)}\big{(}(\bm{I}_{\pi} \bm{I}_{\pi}^{\top})\otimes\bm{I}_{d}\big{)}\bm{I}_{(di)}}\] \[\stackrel{{(i)}}{{=}}\max_{i\in[m]}\frac{1}{b}\sum_{j =1}^{b}L_{\pi_{b(i-1)+j}}\leq L,\]

where for \((i)\) we use Lemma 3 for each submatrix \((\bm{l}_{\pi}^{(i)}\bm{l}_{\pi}^{(i)\top})\otimes\bm{I}_{d}\) and \(\bm{l}_{\pi}^{(i)}=(0,\dots,0,\sqrt{L_{\pi_{b(i-1)+1}}},\dots,\sqrt{L_{\pi_{b 1}}},0,\dots,0)^{\top}\). Similar to the case of generalized linear models, the inequality is tight when \(b=1\) but can be loose for other values of \(b\).

Before proceeding to the omitted proofs, we first state the following standard definitions and first-order characterization of strong convexity, for completeness.

**Definition 1**.: _A function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is said to be \(\mu\)-strongly convex with parameter \(\mu>0\), if for any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\) and any \(\lambda\in(0,1)\):_

\[f(\lambda\bm{x}+(1-\lambda)\bm{y})\leq\lambda f(\bm{x})+(1-\lambda)f(\bm{y})- \frac{\mu}{2}\lambda(1-\lambda)\norm{\bm{x}-\bm{y}}_{2}^{2}.\]

**Lemma 4**.: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be a continuous \(\mu\)-strongly convex function with \(\mu>0\). Then, for any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\):_

\[f(\bm{y})\geq f(\bm{x})+\langle\bm{g}_{\bm{x}},\bm{y}-\bm{x}\rangle+\frac{\mu} {2}\norm{\bm{x}-\bm{y}}_{2}^{2},\]

_where \(\bm{g}_{\bm{x}}\in\partial f(\bm{x})\), and \(\partial f(\bm{x})\) is the subdifferential of \(f\) at \(\bm{x}\)._

We also include the following lemma on the variance bound under without-replacement sampling, which is useful for our proof.

**Lemma 5**.: _Let \(\mathcal{B}\) be the set of \(|\mathcal{B}|=b\) samples from \([n]\), drawn without replacement and uniformly at random. Then, \(\forall\bm{x}\in\mathbb{R}^{d}\),_

\[\mathbb{E}_{\mathcal{B}}\Big{[}\norm{\frac{1}{b}\sum_{i\in\mathcal{B}}\nabla f _{i}(\bm{x})-\nabla f(\bm{x})}_{2}^{2}\Big{]}=\frac{n-b}{b(n-1)}\mathbb{E}_{i} \big{[}\norm{\nabla f_{i}(\bm{x})-\nabla f(\bm{x})}_{2}^{2}\big{]}.\]Proof.: We first expand the square on the left-hand side, as follows

\[\mathbb{E}_{\mathcal{B}}\Big{[}\big{\|}\frac{1}{b}\sum_{i\in\mathcal{ B}}\nabla f_{i}(\bm{x})-\nabla f(\bm{x})\big{\|}_{2}^{2}\Big{]}\] \[=\frac{1}{b^{2}}\mathbb{E}_{\mathcal{B}}\Big{[}\sum_{i,i^{\prime} \in\mathcal{B}}\left\langle\nabla f_{i}(\bm{x})-\nabla f(\bm{x}),\nabla f_{i^{ \prime}}(\bm{x})-\nabla f(\bm{x})\right\rangle\Big{]}\] \[=\frac{1}{b^{2}}\mathbb{E}_{\mathcal{B}}\Big{[}\sum_{i,i^{\prime }\in\mathcal{B},i\neq i^{\prime}}\left\langle\nabla f_{i}(\bm{x})-\nabla f( \bm{x}),\nabla f_{i^{\prime}}(\bm{x})-\nabla f(\bm{x})\right\rangle\Big{]}+ \frac{1}{b}\mathbb{E}_{i}\big{[}\|\nabla f_{i}(\bm{x})-\nabla f(\bm{x})\|_{2}^ {2}\big{]}.\]

Since the batch \(\mathcal{B}\) is sampled uniformly and without replacement from \([n]\), the probability that any pair \((i,i^{\prime})\) from \([n]\) with \(i\neq i^{\prime}\) is in \(\mathcal{B}\) is \(\frac{b(b-1)}{n(n-1)}\). By the linearity of expectation, we have

\[\mathbb{E}_{\mathcal{B}}\Big{[}\sum_{i,i^{\prime}\in\mathcal{B}, i\neq i^{\prime}}\left\langle\nabla f_{i}(\bm{x})-\nabla f(\bm{x}),\nabla f _{i^{\prime}}(\bm{x})-\nabla f(\bm{x})\right\rangle\Big{]}\] \[=\mathbb{E}_{\mathcal{B}}\Big{[}\sum_{i,i^{\prime}\in[n],i\neq i^ {\prime}}\mathbb{1}_{i,i^{\prime}\in\mathcal{B}}\left\langle\nabla f_{i}(\bm{ x})-\nabla f(\bm{x}),\nabla f_{i^{\prime}}(\bm{x})-\nabla f(\bm{x})\right\rangle \Big{]}\] \[=\sum_{i,i^{\prime}\in[n],i\neq i^{\prime}}\mathbb{E}_{\mathcal{B }}\Big{[}\mathbb{1}_{i,i^{\prime}\in\mathcal{B}}\left\langle\nabla f_{i}(\bm{ x})-\nabla f(\bm{x}),\nabla f_{i^{\prime}}(\bm{x})-\nabla f(\bm{x})\right\rangle \Big{]}\] \[=\frac{b(b-1)}{n(n-1)}\sum_{i,i^{\prime}\in[n],i\neq i^{\prime}} \left\langle\nabla f_{i}(\bm{x})-\nabla f(\bm{x}),\nabla f_{i^{\prime}}(\bm{x })-\nabla f(\bm{x})\right\rangle,\]

where \(\mathbb{1}\) is the indicator function such that \(\mathbb{1}_{i,i^{\prime}\in\mathcal{B}}=1\) if both \(i,i^{\prime}\in\mathcal{B}\) and is equal to zero otherwise. Hence, we obtain

\[\mathbb{E}_{\mathcal{B}}\Big{[}\big{\|}\frac{1}{b}\sum_{i\in \mathcal{B}}\nabla f_{i}(\bm{x})-\nabla f(\bm{x})\big{\|}^{2}\Big{]}\] \[=\frac{b-1}{bn(n-1)}\sum_{i,i^{\prime}\in[n],i\neq i^{\prime}} \left\langle\nabla f_{i}(\bm{x})-\nabla f(\bm{x}),\nabla f_{i^{\prime}}(\bm{x })-\nabla f(\bm{x})\right\rangle+\frac{1}{b}\mathbb{E}_{i}\big{[}\|\nabla f_{i }(\bm{x})-\nabla f(\bm{x})\|^{2}\big{]}\] \[=\frac{b-1}{bn(n-1)}\sum_{i,i^{\prime}\in[n]}\left\langle\nabla f _{i}(\bm{x})-\nabla f(\bm{x}),\nabla f_{i^{\prime}}(\bm{x})-\nabla f(\bm{x}) \right\rangle+\frac{n-b}{b(n-1)}\mathbb{E}_{i}\big{[}\|\nabla f_{i}(\bm{x})- \nabla f(\bm{x})\|^{2}\big{]}\] \[\overset{(i)}{=}\frac{n-b}{b(n-1)}\mathbb{E}_{i}\big{[}\|\nabla^{ j}f_{i}(\bm{x})-\nabla^{j}f(\bm{x})\|^{2}\big{]},\]

where \((i)\) is due to \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i}\) having the finite sum structure. 

Now we provide the omitted proofs from Section 2.

**Lemma 6**.: _Under Assumption 1, for any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 1 satisfy_

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{ \top}\bm{y}_{k}^{(i)},\bm{x}_{k}-\bm{x}_{k-1,i+1}\Big{\rangle}+\frac{\eta_{k}}{ n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top}\big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)} \big{)},\bm{x}_{k}-\bm{x}_{k-1,i}\Big{\rangle}\] (5) \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{\Lambda}_{ k}^{-1}}^{2}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}- \frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|^{2},\]

_where \(\mathcal{E}_{k}:=\eta_{k}\mathrm{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})+\frac{b}{ 2n}\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2 }^{2}\)._

Proof.: We first note that based on Line 6 of Alg. 1, we have

\[\Big{\langle}\bm{E}_{b}^{\top}\bm{y}^{(i)},\bm{x}_{k-1,i}\Big{\rangle}-\sum_{j=1 }^{b}f_{\pi_{b(i-1)+j}^{(k)}}^{*}(\bm{y}^{j})=\sum_{j=1}^{b}\Big{(}\left\langle \bm{y}^{j},\bm{x}_{k-1,i}\right\rangle-f_{\pi_{b(i-1)+j}^{(k)}}^{*}(\bm{y}^{j })\Big{)}.\]Since the max problem defining \(\bm{y}_{k}\) is separable, we have for \(b(i-1)+1\leq j\leq bi\) and \(i\in[m]\)

\[\bm{y}_{k}^{j}=\operatorname*{arg\,max}_{\bm{y}^{j}\in\mathbb{R}^{d}}\Big{\{} \left\langle\bm{y}^{j},\bm{x}_{k-1,i}\right\rangle-f^{*}_{\pi_{j}^{(k)}}(\bm{y }^{j})\Big{\}},\]

which leads to \(\bm{x}_{k-1,i}\in\partial f^{*}_{\pi_{j}^{(k)}}(\bm{y}_{k}^{j})\). Further, since each component function \(f_{j}^{*}\) is \(\frac{1}{L_{j}}\)-strongly convex thus for \(b(i-1)+1\leq j\leq bi\), we also have

\[f^{*}_{\pi_{j}^{(k)}}(\bm{y}_{k}^{j})\geq f^{*}_{\pi_{j}^{(k)}}(\bm{y}_{k}^{j} )+\left\langle\bm{x}_{k-1,i},\bm{v}_{k}^{j}-\bm{y}_{k}^{j}\right\rangle+\frac {1}{2L_{\pi_{j}^{(k)}}}\|\bm{v}_{k}^{j}-\bm{y}_{k}^{j}\|^{2},\]

which leads to

\[\mathcal{L}(\bm{x}_{k},\bm{v})\] \[=\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{v}_{k}^{(i)},\bm{x}_{k-1,i}\right\rangle-\sum_{j=b(i-1)+1}^{bi}f^{*}_{\pi_{ j}^{(k)}}(\bm{y}_{k}^{j})\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\left\langle\bm{E}_{b}^{ \top}\bm{v}_{k}^{(i)},\bm{x}_{k}-\bm{x}_{k-1,i}\right\rangle\] \[\leq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{k-1,i}\right\rangle-\sum_{j=b(i-1)+1}^{bi}f^{*}_{\pi_{ j}^{(k)}}(\bm{y}_{k}^{j})\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\left\langle\bm{E}_{b}^{ \top}\bm{v}_{k}^{(i)},\bm{x}_{k}-\bm{x}_{k-1,i}\right\rangle\] \[\quad-\frac{1}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|^{2}_{\bm{\Lambda}_{k} ^{-1}}.\]

Using the same argument, as \(\bm{x}_{*}\in\partial f^{*}_{i}(\bm{y}_{*}^{i})\) for \(i\in[n]\), we have

\[f^{*}_{\pi_{i}^{(k)}}(\bm{y}_{k}^{i})\geq f^{*}_{\pi_{i}^{(k)}}(\bm{y}_{*,k}^{i} )+\left\langle\bm{x}_{*},\bm{y}_{k}^{i}-\bm{y}_{*,k}^{i}\right\rangle+\frac{1} {2L_{\pi_{i}^{(k)}}}\|\bm{y}_{k}^{i}-\bm{y}_{*,k}^{i}\|^{2}.\]

Thus,

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*})\] \[=\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{*,k}^{(i)},\bm{x}_{*}\right\rangle-\sum_{j=b(i-1)+1}^{bi}f^{*}_{\pi_{ j}^{(k)}}(\bm{y}_{*,k}^{j})\Big{)}\] \[\geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{*}\right\rangle-\sum_{j=b(i-1)+1}^{bi}f^{*}_{\pi_{j}^ {(k)}}(\bm{y}_{k}^{j})\Big{)}+\frac{1}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}\] \[=\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{*}\right\rangle+\frac{b}{2\eta_{k}}\|\bm{x}_{*}-\bm{x} _{k-1,i}\|^{2}-\frac{b}{2\eta_{k}}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|^{2}-\sum_{j=b(i -1)+1}^{bi}f^{*}_{\pi_{j}^{(k)}}(\bm{y}_{k}^{j})\Big{)}\] \[\quad+\frac{1}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{2}_{\bm{\Lambda}_{k }^{-1}}.\]

Using the updating scheme of \(\bm{x}_{k-1,i+1}\) and noticing that \(\phi_{k}^{i}(\bm{x})=\left\langle\bm{E}_{b}^{\top}\bm{y}_{k}^{(i)},\bm{x} \right\rangle+\frac{b}{2\eta_{k}}\|\bm{x}-\bm{x}_{k-1,i}\|^{2}\) is \(\frac{b}{\eta_{k}}\)-strongly convex and minimized at \(\bm{x}_{k-1,i+1}\), we have

\[\left\langle\bm{E}_{b}^{\top}\bm{y}_{k}^{(i)},\bm{x}_{*}\right\rangle+\frac{b} {2\eta_{k}}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|^{2}\] \[\geq\left\langle\bm{E}_{b}^{\top}\bm{y}_{k}^{(i)},\bm{x}_{k-1,i+1} \right\rangle+\frac{b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|^{2}+\frac{ b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}-\bm{x}_{*}\|^{2},\]which leads to

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) \geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{k-1,i+1}\right\rangle+\frac{b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|^{2}-\sum_{j=b(i-1)+1}^{bi}f_{\pi_{j}^{(i)}}^{*}(\bm{y}_{ k}^{i})\Big{)}\] \[\quad+\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\Big{(}\|\bm{x}_{k-1,i+1}- \bm{x}_{*}\|^{2}-\|\bm{x}_{k-1,i}-\bm{x}_{*}\|^{2}\Big{)}+\frac{1}{2n}\|\bm{y}_ {k}-\bm{y}_{*,k}\|^{2}_{\bm{\Lambda}_{k}^{-1}}\] \[=\frac{1}{n}\sum_{i=1}^{m}\Big{(}\left\langle\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{k-1,i+1}\right\rangle+\frac{b}{2\eta_{k}}\|\bm{x}_{k- 1,i+1}-\bm{x}_{k-1,i}\|^{2}-\sum_{j=b(i-1)+1}^{bi}f_{\pi_{j}^{(i)}}^{*}(\bm{y}_ {k}^{j})\Big{)}\] \[\quad+\frac{b}{2n\eta_{k}}\Big{(}\|\bm{x}_{k}-\bm{x}_{*}\|^{2}-\| \bm{x}_{k-1}-\bm{x}_{*}\|^{2}\Big{)}+\frac{1}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{ 2}_{\bm{\Lambda}_{k}^{-1}}.\]

Hence, combining the bounds on \(\mathcal{L}(\bm{x}_{k},\bm{v})\) and \(\mathcal{L}(\bm{x}_{*},\bm{y}_{*})\) and letting

\[\mathcal{E}_{k}:=\eta_{k}\big{(}\mathcal{L}(\bm{x}_{k},\bm{v})-\mathcal{L}( \bm{x}_{*},\bm{y}_{*})\big{)}+\frac{b}{2n}\|\bm{x}_{k}-\bm{x}_{*}\|^{2}-\frac{ b}{2n}\|\bm{x}_{k-1}-\bm{x}_{*}\|^{2},\]

we obtain

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{ \top}\bm{y}_{k}^{(i)},\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\Big{\rangle}+\frac{\eta _{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top}\bm{v}_{k}^{(i)},\bm{x}_{k }-\bm{x}_{k-1,i}\Big{\rangle}\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i} \|^{2}\] \[=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top} \bm{y}_{k}^{(i)},\bm{x}_{k}-\bm{x}_{k-1,i+1}\Big{\rangle}+\frac{\eta_{k}}{n} \sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top}\big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^ {(i)}),\bm{x}_{k}-\bm{x}_{k-1,i}\Big{\rangle}\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i} \|^{2},\]

thus completing the proof. 

We note that the first inner product term \(\mathcal{T}_{1}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top }\bm{y}_{k}^{(i)},\bm{x}_{k}-\bm{x}_{k-1,i+1}\Big{\rangle}\) in Eq. (5) can be cancelled by the last negative term \(-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|^{2}\) therein, as precisely proved in Lemma 10 of Appendix C. In the following subsections, we continue our analysis and handle the remaining terms in Eq. (5) according to different shuffling and derive the final complexity.

### Random reshuffling/shuffle-once schemes

We introduce the following lemma to bound the second inner product term \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{ \top}\big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\bm{x}_{k}-\bm{x}_{k-1,i} \Big{\rangle}\) in Lemma 6 when there are random permutations.

**Lemma 7**.: _Under Assumptions 1 and 2, for any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 1 with uniformly random shuffling (RR/SO) satisfy_

\[\mathbb{E}[\mathcal{T}_{2}]\leq\mathbb{E}\Big{[}\frac{\eta_{k}^{3}n\hat{L}_{ \pi^{(k)}}^{g}\tilde{L}_{\pi^{(k)}}^{g}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|^{2}_{ \bm{\Lambda}_{k}^{-1}}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|^{2}_{\bm{ \Lambda}_{k}^{-1}}\Big{]}+\frac{\eta_{k}^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)} \sigma_{*}^{2},\]

_where \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top} \big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\bm{x}_{k}-\bm{x}_{k-1,i} \Big{\rangle}\)._Proof.: First note that \(\bm{x}_{k}-\bm{x}_{k-1,i}=\sum_{j=i}^{m}(\bm{x}_{k-1,j+1}-\bm{x}_{k-1,j})=-\frac{ \eta_{k}}{b}\sum_{j=i}^{m}\bm{E}_{b}^{\top}\bm{y}_{k}^{(j)}=-\frac{\eta_{k}}{b} \bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{k}\), so we have

\[\frac{\eta_{k}}{n}\sum_{i=1}^{m}\left\langle\bm{E}_{b}^{\top} \big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\bm{x}_{k}-\bm{x}_{k-1,i}\right\rangle\] \[= \frac{\eta_{k}}{n}\sum_{i=1}^{m}\left\langle\bm{E}_{b}^{\top} \big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\sum_{j=i}^{m}(\bm{x}_{k-1,j+1} -\bm{x}_{k-1,j})\right\rangle\] \[= -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\left\langle\bm{E}^{\top} \bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow} \bm{y}_{k}\right\rangle\] \[= \underbrace{-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\left\langle \bm{E}^{\top}\bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\right\rangle}_{\mathcal{I}_{1}}\] \[\underbrace{-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\left\langle \bm{E}^{\top}\bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}\bm{y}_{*,k}\right\rangle}_{\mathcal{I}_{2}}.\]

For the term \(\mathcal{I}_{1}\), we use Young's inequality with \(\alpha>0\) to be set later and obtain

\[\mathcal{I}_{1}= -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\left\langle\bm{E}^{\top} \bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}( \bm{y}_{k}-\bm{y}_{*,k})\right\rangle\] \[\leq \frac{\eta_{k}^{2}}{2bn\alpha}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_ {(di)}(\bm{v}_{k}-\bm{y}_{k})\|^{2}+\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{ m}\|\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\|^{2}.\] (6)

Further, notice that

\[\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I} _{bd(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\|^{2}\] \[= \frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}(\bm{y}_{k}-\bm{y}_{*,k})^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow }(\bm{y}_{k}-\bm{y}_{*,k})\] \[= \frac{\eta_{k}^{2}\alpha}{2bn}(\bm{y}_{k}-\bm{y}_{*,k})^{\top} \Big{(}\sum_{i=1}^{m}\bm{I}_{bd(i-1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1 )\uparrow}\Big{)}(\bm{y}_{k}-\bm{y}_{*,k})\] \[= \frac{\eta_{k}^{2}\alpha}{2bn}(\bm{y}_{k}-\bm{y}_{*,k})^{\top} \bm{\Lambda}_{k}^{-1/2}\bm{\Lambda}_{k}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{bd(i -1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\Big{)}\bm{\Lambda}_{k} ^{1/2}\bm{\Lambda}_{k}^{-1/2}(\bm{y}_{k}-\bm{y}_{*,k})\] \[\leq \frac{\eta_{k}^{2}\alpha}{2bn}\Big{\|}\bm{\Lambda}_{k}^{1/2} \Big{(}\sum_{i=1}^{m}\bm{I}_{bd(i-1)\uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1 )\uparrow}\Big{)}\bm{\Lambda}_{k}^{1/2}\Big{\|}_{2}\|\bm{y}_{k}-\bm{y}_{*,k} \|_{\bm{\Lambda}_{k}^{-1}}^{2}\] \[= \frac{\eta_{k}^{2}m\alpha}{2b}\hat{L}_{\pi^{(k)}}^{g}\|\bm{y}_{k} -\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2},\] (7)

where for the last inequality we use Cauchy-Schwarz inequality. Using the same argument, we can bound

\[\frac{\eta_{k}^{2}}{2bn\alpha}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_ {(di)}(\bm{v}_{k}-\bm{y}_{k})\|^{2} \leq\frac{\eta_{k}^{2}}{2bn\alpha}\Big{\|}\bm{\Lambda}_{k}^{1/2} \Big{(}\sum_{i=1}^{m}\bm{I}_{(di)}\bm{E}\bm{E}^{\top}\bm{I}_{(di)}\Big{)}\bm{ \Lambda}_{k}^{1/2}\Big{\|}_{2}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}\] \[=\frac{\eta_{k}^{2}}{2n\alpha}\tilde{L}_{\pi^{(k)}}^{g}\|\bm{v}_{k}- \bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\] (8)

Thus, combining (6)-(8) and choosing \(\alpha=2\eta_{k}\tilde{L}_{\pi^{(k)}}^{g}\), we obtain

\[\mathcal{I}_{1}\leq\frac{\eta_{k}^{3}n\hat{L}_{\pi^{(k)}}^{g}\tilde{L}_{\pi^{(k)}} ^{g}}{b}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{4 n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]For the term \(\mathcal{I}_{2}\), we again apply Young's inequality with \(\beta>0\) to be set later and obtain

\[\mathcal{I}_{2} = -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{E}^{\top} \bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y }_{*,k}\big{\rangle}\] \[\leq \frac{\eta_{k}^{2}\beta}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_ {bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}+\frac{\eta_{k}^{2}}{2bn\beta}\sum_{i=1}^{m }\|\bm{E}^{\top}\bm{I}_{(di)}(\bm{v}_{k}-\bm{y}_{k})\|^{2}\] \[\leq \frac{\eta_{k}^{2}\beta}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_ {bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}+\frac{\eta_{k}^{2}\tilde{L}_{\pi^{(k)}}^{ g}}{2n\beta}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

Choosing \(\beta=2\eta_{k}\tilde{L}^{g}\) and using the fact that \(\tilde{L}_{\pi^{(k)}}^{g}\leq\tilde{L}^{g}\), we have

\[\mathcal{I}_{2}\leq\frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\sum_{i=1}^{m}\|\bm{E}^ {\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}+\frac{\eta_{k}}{4n}\|\bm{v}_{ k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

Hence, combining the above two estimates with \(m=n/b\), we have

\[\mathcal{T}_{2}\leq\frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\sum_{i=1}^{m}\|\bm{E} ^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}+\frac{\eta_{k}^{3}n\hat{L}_ {\pi^{(k)}}^{g}\tilde{L}_{\pi^{(k)}}^{g}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{ \bm{\Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{ \Lambda}_{k}^{-1}}^{2}.\]

First, consider the RR scheme. Taking conditional expectation on both sides w.r.t. the randomness up to but not including \(k\)-th epoch, we have

\[\mathbb{E}_{k}[\mathcal{T}_{2}] \leq \frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\mathbb{E}_{k}\Big{[}\sum_{i =1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}\Big{]}\] \[+\mathbb{E}_{k}\Big{[}\frac{\eta_{k}^{3}\tilde{n}\hat{L}_{\pi^{(k) }}^{g}\tilde{L}_{\pi^{(k)}}^{g}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{ \Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{ \Lambda}_{k}^{-1}}^{2}\Big{]}.\]

For the first term, since the only randomness comes from the permutation \(\pi^{(k)}\), we can proceed as in the proof of Lemma 11 and obtain

\[\frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\mathbb{E}_{k}\Big{[}\sum_{i =1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}\Big{]} \stackrel{{(i)}}{{=}}\frac{\eta_{k}^{3}\tilde{L}^{g}}{ bn}\sum_{i=1}^{m}\mathbb{E}_{\pi^{(k)}}\Big{[}\|\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}\bm{y}_{*,k}\|^{2}\Big{]}\] \[=\frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\sum_{i=1}^{m}(n-b(i-1))^{2} \mathbb{E}_{\pi^{(k)}}\Big{[}\Big{\|}\frac{\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}\bm{y}_{*,k}}{n-b(i-1)}\Big{\|}^{2}\Big{]}\] \[\stackrel{{(ii)}}{{\leq}}\frac{\eta_{k}^{3}\tilde{L}^{ g}}{bn}\sum_{i=1}^{m}(n-b(i-1))^{2}\frac{b(i-1)}{(n-b(i-1))(n-1)}\sigma_{*}^{2}\] \[=\frac{\eta_{k}^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*} ^{2},\]

where we use the linearity of expectation for \((i)\), and \((ii)\) is due to Lemma 5 and the definition \(\sigma_{*}^{2}=\frac{1}{n}\sum_{i=1}^{n}\|\nabla f_{i}(\bm{x}_{*})\|^{2}=\frac{ 1}{n}\sum_{i=1}^{n}\|\bm{y}_{*}^{i}\|^{2}\). Then taking expectation w.r.t. all randomness on both sides, we obtain

\[\mathbb{E}[\mathcal{T}_{2}]\leq\mathbb{E}\Big{[}\frac{\eta_{k}^{3}n\hat{L}_{ \pi^{(k)}}^{g}\tilde{L}_{\pi^{(k)}}^{g}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{ \Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{ \Lambda}_{k}^{-1}}^{2}\Big{]}+\frac{\eta_{k}^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n -1)}\sigma_{*}^{2}.\]

Finally, we remark that the above argument for bounding the term \(\frac{\eta_{k}^{3}\tilde{L}^{g}}{bn}\mathbb{E}_{k}\Big{[}\sum_{i=1}^{m}\|\bm{E}^ {\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*,k}\|^{2}\Big{]}\) also applies to the SO scheme, in which case there is only one random permutation at the very beginning that induces the randomness. 

We state the final convergence rate and complexity in the following theorem and provide the proof for completeness.

**Theorem 4**.: _Under Assumptions 1 and 2, if \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}^{g}(n)\tilde{L}^{g}_{*(k)}}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), the output \(\hat{\bm{x}}_{K}\) of Algorithm 1 with uniformly random (RR/SO) shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{b}{2n}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k}^{3}\tilde{L}^{g}(n-b)(n+b)} {6b^{2}(n-1)}\sigma_{*}^{2}.\]

_As a consequence, for any \(\epsilon>0\), there exists a choice of a constant step size \(\eta_{k}=\eta\) for which \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\sqrt{\tilde{L}^{g}\tilde{L}^{g}}\|\bm{x}_{0}-\bm{x}_ {*}\|_{2}^{2}}{\epsilon}+\sqrt{\frac{(n-b)(n+b)}{n(n-1)}\frac{\sqrt{\tilde{L} ^{g}\sigma_{*}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}}\big{)}\) gradient queries._

Proof.: Combining the bounds in Lemma 10 and 2 and plugging them into Eq. (5), we obtain

\[\mathbb{E}[\mathcal{E}_{k}]\leq\mathbb{E}\Big{[}\Big{(}\frac{\eta_{k}^{3} \tilde{L}^{g}_{*(k)}\tilde{L}^{g}_{*(k)}}{b^{2}}-\frac{\eta_{k}}{2n}\Big{)}\| \bm{y}_{k}-\bm{y}_{*,k}\|_{\tilde{\bm{A}}_{k}^{-1}}^{2}\Big{]}+\frac{\eta_{k} ^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

For the stepsize \(\eta_{k}\) such that \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}^{g}_{*(k)}\tilde{L}^{g}_{*(k)}}}\), we have \(\frac{\eta_{k}^{3}\tilde{n}\tilde{L}^{g}_{*(k)}\tilde{L}^{g}_{*(k)}}{b^{2}}- \frac{\eta_{k}}{2n}\leq 0\), thus

\[\mathbb{E}[\mathcal{E}_{k}]\leq\frac{\eta_{k}^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^ {2}(n-1)}\sigma_{*}^{2}.\]

Using our definition of \(\mathcal{E}_{k}\) and telescoping from \(k=1\) to \(K\), we have

\[\mathbb{E}\Big{[}\sum_{k=1}^{K}\eta_{k}\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_ {*})\Big{]}\leq\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{0}\|_{2}^{2}-\frac{b}{2n} \mathbb{E}[\|\bm{x}_{*}-\bm{x}_{K}\|_{2}^{2}]+\sum_{k=1}^{K}\frac{\eta_{k}^{3} \tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

Noticing that \(\mathcal{L}(\bm{x},\bm{v})\) is convex in \(\bm{x}\) for a fixed \(\bm{v}\), we have \(\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\leq\sum_{k=1}^{K}\eta_{k} \text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})/H_{K}\), where \(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/H_{K}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), which leads to

\[\mathbb{E}\Big{[}H_{K}\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\Big{]} \leq\frac{b}{2n}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k} ^{3}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

Further choosing \(\bm{v}=\bm{y}_{\hat{\bm{x}}_{K}}\), we obtain

\[\mathbb{E}[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}]\leq\frac{b}{2n }\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k}^{3}\tilde{L}^{ g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\] (9)

To analyze the individual gradient oracle complexity, we choose constant stepsizes \(\eta\leq\frac{b}{n\sqrt{2\tilde{L}^{g}\tilde{L}^{g}}}\), then Eq. (9) will become

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\frac{b}{2n\eta K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{\eta^{2}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)} \sigma_{*}^{2}.\]

Without loss of generality, we assume that \(b\neq n\), otherwise the method and its analysis reduce to (full) gradient descent. We consider the following two cases:

* "Small \(K\)" case: if \(\eta=\frac{b}{n\sqrt{2\tilde{L}^{g}\tilde{L}^{g}}}\leq\Big{(}\frac{3b^{3}(n-1) \|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)\tilde{L}^{g}K\sigma_{2}^{2}}\Big{)} ^{1/3}\), we have \[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\] \[\leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{ \eta^{2}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}\] \[\leq\frac{\sqrt{\tilde{L}^{g}\tilde{L}^{g}}}{\sqrt{2}K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{1}{2}\Big{(}\frac{(n-b)(n+b)}{n^{2}(n-1)}\Big{)}^{1 /3}\frac{(\tilde{L}^{g})^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3 }}{3^{1/3}K^{2/3}}.\]* "Large \(K\)" case: if \(\eta=\left(\frac{3b^{3}(n-1)\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)L^{g}K \sigma_{*}^{2}}\right)^{1/3}\leq\frac{b}{n\sqrt{2\tilde{L}^{g}\tilde{L}^{g}}}\), we have \[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})] \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{ \eta^{2}\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}\] \[\leq\Big{(}\frac{(n-b)(n+b)}{n^{2}(n-1)}\Big{)}^{1/3}\frac{( \tilde{L}^{g})^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{3^{1/ 3}K^{2/3}}.\] Combining these two cases by setting \(\eta=\min\Big{\{}\frac{b}{n\sqrt{2\tilde{L}^{g}\tilde{L}^{g}}},\,\Big{(} \frac{3b^{3}(n-1)\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)L^{g}K\sigma_{ *}^{2}}\Big{)}^{1/3}\Big{\}}\), we obtain \[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})] \leq\frac{\sqrt{\tilde{L}^{g}\tilde{L}^{g}}}{\sqrt{2}K}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}+\Big{(}\frac{(n-b)(n+b)}{n^{2}(n-1)}\Big{)}^{1/3} \frac{(\tilde{L}^{g})^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3} }{3^{1/3}K^{2/3}}.\] Hence, to guarantee \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) for \(\epsilon>0\), the total number of individual gradient evaluations will be \[nK\geq\max\Big{\{}\frac{n\sqrt{2\tilde{L}^{g}\tilde{L}^{g}}\|\bm{x}_{0}-\bm{x} _{*}\|_{2}^{2}}{\epsilon},\Big{(}\frac{(n-b)(n+b)}{n-1}\Big{)}^{1/2}\frac{2^{3 /2}(\tilde{L}^{g})^{1/2}\sigma_{*}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{3^{1/2} \epsilon^{3/2}}\Big{\}},\] as claimed. 

### Incremental gradient descent (IG)

In this subsection, we provide the convergence results for incremental gradient descent which does not involve random permutations. We first prove the technical lemma below to bound the term \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top }\big{(}\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\bm{x}_{k}-\bm{x}_{k-1,i} \Big{\rangle}\) in Eq. (5) of Lemma 6.

**Lemma 8**.: _For any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 1 with fixed data ordering satisfy_

\[\mathcal{T}_{2} \leq\frac{\eta_{k}^{3}\tilde{L}^{g}}{b^{2}}\tilde{L}_{0}^{g}\| \bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}- \bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] (10) \[\quad+\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\tilde{L}_{0}^{g} \tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b )^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\]

Proof.: Proceeding as in the proof of Lemma 7, we have

\[\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Big{\langle}\bm{E}_{b}^{\top} \big{(}\bm{v}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\bm{x}_{k}-\bm{x}_{k-1,i}\Big{\rangle}\] \[=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\Bigg{\langle}\bm{E}_{b}^{\top }\big{(}\bm{v}^{(i)}-\bm{y}_{k}^{(i)}\big{)},\sum_{j=i}^{m}(\bm{x}_{k-1,j+1}- \bm{x}_{k-1,j})\Bigg{\rangle}\] \[=\ -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\Big{\langle}\bm{E}^{\top }\bm{I}_{(di)}(\bm{v}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{ k}\Big{\rangle}\] \[=\underbrace{-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle} \bm{E}^{\top}\bm{I}_{(di)}(\bm{v}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}(\bm{y}_{k}-\bm{y}_{*})\big{\rangle}}_{\mathcal{I}_{1}}\] \[\underbrace{-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle} \bm{E}^{\top}\bm{I}_{(di)}(\bm{v}-\bm{y}_{k}),\bm{E}^{\top}\bm{I}_{bd(i-1) \uparrow}\bm{y}_{*}\big{\rangle}}_{\mathcal{I}_{2}}.\]For both terms \(\mathcal{I}_{1}\) and \(\mathcal{I}_{2}\), we apply Young's inequality with \(\alpha=2\eta_{k}\tilde{L}_{0}^{g}\) and obtain

\[\mathcal{I}_{1} \leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top} \bm{I}_{bd(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*})\|_{2}^{2}+\frac{\eta_{k}^{2}}{2 bn\alpha}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{(di)}(\bm{v}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}n\alpha}{2b^{2}}\hat{L}_{0}^{g}\|\bm{y}_{k} -\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}^{2}}{2n\alpha}\tilde{L}_{ 0}^{g}\|\bm{v}-\bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] \[=\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}^{g}\tilde{L}_{0}^{g}\| \bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{4n}\|\bm{v}-\bm {y}_{k}\|_{\bm{\Lambda}^{-1}}^{2},\] (11)

and

\[\mathcal{I}_{2} \leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top} \bm{I}_{bd(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2bn\alpha} \sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{(di)}(\bm{v}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{E}^{\top} \bm{I}_{bd(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2n\alpha} \tilde{L}_{0}^{g}\|\bm{v}-\bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] \[=\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{nb}\sum_{i=1}^{m}\|\bm{E}^{ \top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}}{4n}\|\bm{v}- \bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}.\] (12)

We now show that the term \(\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{nb}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{ bd(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}\) is no larger than either \(\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{b^{2}}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm {\Lambda}^{-1}}^{2}\) or \(\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\). This is trivial when \(b=n\) as \(\bm{E}^{\top}\bm{I}_{0\uparrow}\bm{y}_{*}=\sum_{i=1}^{n}\bm{y}_{*}^{i}=\bm{0}\). When \(b<n\), to show the former one, we have

\[\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*} \|_{2}^{2} \leq\left\|\bm{\Lambda}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{bd(i-1) \uparrow}\bm{E}\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\Big{)}\bm{\Lambda}^{1/2} \right\|_{2}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}\] \[=mn\hat{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}=\frac{n^{2 }}{b}\hat{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}.\]

To prove the latter one, we notice that

\[\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i-1)\uparrow}\bm{y}_{*} \|_{2}^{2} =\sum_{i=1}^{m}\Big{\|}\sum_{j=b(i-1)+1}^{n}\bm{y}_{*}^{j}\Big{\|} _{2}^{2}=\sum_{i=0}^{m-1}\Big{\|}\sum_{j=bi+1}^{n}\bm{y}_{*}^{j}\Big{\|}_{2}^{2 }=\sum_{i=1}^{m-1}\Big{\|}\sum_{j=bi+1}^{n}\bm{y}_{*}^{j}\Big{\|}_{2}^{2}\] \[=\sum_{i=1}^{m-1}\Big{\|}\sum_{j=1}^{bi}\bm{y}_{*}^{j}\Big{\|}_{2} ^{2},\]

using the fact that \(\sum_{i=1}^{n}\bm{y}_{*}^{i}=\bm{0}\). Then using Young's inequality we obtain

\[\sum_{i=1}^{m-1}\Big{\|}\sum_{j=1}^{bi}\bm{y}_{*}^{j}\Big{\|}_{2}^{2} \leq\sum_{i=1}^{m-1}bi\sum_{j=1}^{bi}\|\bm{y}_{*}^{j}\|_{2}^{2}\] \[\leq b(m-1)\sum_{i=1}^{m-1}\sum_{j=1}^{bi}\|\bm{y}_{*}^{j}\|_{2}^{2}\] \[=b(m-1)\sum_{i=1}^{m-1}\sum_{j=b(i-1)+1}^{bi}(m-i)\|\bm{y}_{*}^{j} \|_{2}^{2}\] \[\leq b(m-1)^{2}\sum_{i=1}^{(m-1)b}\|\bm{y}_{*}^{i}\|_{2}^{2}.\]

Further noticing that \(\sum_{i=1}^{(m-1)b}\|\bm{y}_{*}^{i}\|_{2}^{2}\leq\sum_{i=1}^{n}\|\bm{y}_{*}^{i}\| ^{2}=n\sigma_{*}^{2}\), we have

\[\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{nb}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i-1 )\uparrow}\bm{y}_{*}\|_{2}^{2}\leq\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{nb}b(m-1)^ {2}n\sigma_{*}^{2}=\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}(n-b)^{2}}{b^{2}}\sigma_{*}^{2}.\]The same bound also captures the case \(b=n\) and leads to

\[\frac{\eta_{k}^{3}\tilde{L}_{0}^{g}}{nb}\sum_{i=1}^{m}\|\bm{E}^{\top}\bm{I}_{bd(i- 1)\uparrow}\bm{y}_{*}\|_{2}^{2}\leq\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L }_{0}^{g}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k} ^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\] (13)

Hence, combining Eq. (11)-(13), we obtain

\[\mathcal{I}_{2} \leq\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}^{g}\tilde{L}_{0}^{g}\| \bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}- \bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] \[\quad+\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}^{g} \tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b )^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}},\]

which finishes the proof. 

We are now ready to state our convergence results for IGD in the following theorem, with its proof provided for completeness.

**Theorem 5**.: _Under Assumptions 1 and 2, if \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}_{0}^{g}}\tilde{L}_{0}^{g}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), the output \(\hat{\bm{x}}_{K}\) of Algorithm 1 with a fixed permutation satisfies_

\[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}\leq\frac{b}{2n}\|\bm{x}_{ 0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}} \hat{L}_{0}^{g}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{ \eta_{k}^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\]

_As a consequence, for any \(\epsilon>0\), there exists a choice of a constant step size \(\eta_{k}=\eta\) such that \(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\leq\epsilon\) after \(\mathcal{O}\Big{(}\frac{n\sqrt{L_{0}^{g}\tilde{L}_{0}^{g}}\|\bm{x}_{0}-\bm{x }_{*}\|_{2}^{2}}{\epsilon}+\frac{\min\big{\{}\sqrt{nL_{0}^{g}\tilde{L}_{0}^{g} \tilde{L}_{0}^{g}}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}},\frac{(n-b)\sqrt{L_{0}^{ g}}\sigma_{*}}\big{\}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}\Big{)}\) gradient queries._

Proof.: Combining the bounds in Lemma 10 and 8 and plugging them into Eq. (5) in Lemma 6 without random permutations, we have

\[\mathcal{E}_{k}\leq\Big{(}\frac{\eta_{k}^{3}n\hat{L}_{0}^{g}\tilde{L}_{0}^{g} }{b^{2}}-\frac{\eta_{k}}{2n}\Big{)}\|\bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1 }}^{2}+\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}^{g}\tilde{L}_{0}^{g} \|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2}} \tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\]

If \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}_{0}^{g}\tilde{L}_{0}^{g}}}\), we have \(\frac{\eta_{k}^{3}n\hat{L}_{0}^{g}\tilde{L}_{0}^{g}}{b^{2}}-\frac{\eta_{k}}{2n}\leq 0\), thus

\[\mathcal{E}_{k}\leq\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}^{g} \tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b )^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\]

Using the definition of \(\mathcal{E}_{k}\) and telescoping from \(k=1\) to \(K\), we obtain

\[\sum_{k=1}^{K}\eta_{k}\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*}) \leq\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{0}\|_{2}^{2}-\frac{b}{2n}\|\bm {x}_{*}-\bm{x}_{K}\|_{2}^{2}\] \[\quad+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L }_{0}^{g}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^ {3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\]

Noticing that \(\mathcal{L}(\bm{x},\bm{v})\) is convex w.r.t. \(\bm{x}\), we have \(\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\leq\sum_{k=1}^{K}\eta_{k} \text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})/H_{K}\), where \(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/H_{K}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), so we obtain

\[H_{K}\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\leq\frac{b}{2n}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L} _{0}^{g}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3} (n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}},\]

Further choosing \(\bm{v}=\bm{y}_{\hat{\bm{x}}_{K}}\), we obtain

\[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}\leq\frac{b}{2n}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L} _{0}^{g}\tilde{L}_{0}^{g}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^ {3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}^{g}\sigma_{*}^{2}\Big{\}}.\] (14)

[MISSING_PAGE_FAIL:26]

Combining these two cases, we obtain

\[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\leq\frac{\sqrt{\hat{L}_{0}^{g}\tilde{L}_{0}^{g}} }{\sqrt{2}K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{2^{1/3}(n-b)^{2/3}(\tilde{L} ^{g})^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.\]

To guarantee \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) for \(\epsilon>0\), the total number of required individual gradient evaluations will be

\[nK\geq\max\Big{\{}\frac{n\sqrt{2\hat{L}_{0}^{g}\tilde{L}_{0}^{g}}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}}{\epsilon},\frac{4(n-b)(\tilde{L}_{0}^{g})^{1/2}\sigma_{ *}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}\Big{\}}.\] (16)

Combining Eq. (15) and Eq. (16), we finally have

\[nK \geq\frac{n\sqrt{2\hat{L}_{0}^{g}\tilde{L}_{0}^{g}}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}}{\epsilon}\] \[\quad+\min\Big{\{}\frac{4n^{1/2}\big{(}\tilde{L}_{0}^{g}\tilde{L }_{0}^{g}\big{)}^{1/2}\|\bm{y}_{*}\|_{\Lambda^{-1}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2 }^{2}}{\epsilon^{3/2}},\frac{4(n-b)(\tilde{L}_{0}^{g})^{1/2}\sigma_{*}\|\bm{x} _{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}\Big{\}},\]

thus finishing the proof. 

## Appendix C Omitted Proofs for Smooth Convex Settting From Section 3

Before proceeding to the omitted proofs for the smooth convex settings in finite-sum with linear predictors, we first recall its primal-dual reformulation, then state the specialized version of a primal-dual shuffled SGD algorithm in Algorithm 2. Recall that (PL) admits an explicit reformulation using convex conjugates of \(\ell_{i}\):

\[\min_{\bm{x}\in\mathbb{R}^{d}}\max_{\bm{y}\in\mathbb{R}^{n}}\Big{\{}\mathcal{ L}(\bm{x},\bm{y}):=\frac{1}{n}\left\langle\bm{A}\bm{x},\bm{y}\right\rangle- \frac{1}{n}\sum_{i=1}^{n}\ell_{i}^{*}(\bm{y}^{i})=\frac{1}{n}\sum_{i=1}^{n} \big{(}\bm{a}_{i}^{\top}\bm{x}\bm{y}^{i}-\ell_{i}^{*}(\bm{y}^{i})\big{)} \Big{\}}\] (PL-PD)

where \(\bm{y}_{\bm{x}}^{i}=\operatorname*{arg\,max}_{\bm{y}^{i}\in\mathbb{R}}\{\bm{y }^{i}\bm{a}_{i}^{\top}\bm{x}-\ell_{i}^{*}(\bm{y}^{i})\}\) (different from the general smooth convex finite-sum settings in Section 2 and Appendix B). Further, for notational convenience, we assume that the partition is ordered, in the sense that for \(1\leq j<j^{\prime}\leq m\), \(\max_{i\in\mathcal{S}^{j}}i<\min_{i^{\prime}\in\mathcal{S}^{j^{\prime}}}i^{ \prime}\).2 We denote by \(\bm{y}^{(j)}\) the subvector of \(\bm{y}\in\mathbb{R}^{n}\) indexed by the elements of \(\mathcal{S}^{j}\), and by \(\bm{A}^{(j)}\) the submatrix obtained from \(\bm{A}\in\mathbb{R}^{n\times d}\) by selecting the rows indexed by \(\mathcal{S}^{j}\).

Footnote 2: This is without loss of generality, as it can be achieved by reordering the rows in the data matrix.

Based on the formulation (PL-PD), we view shuffled SGD as a primal-dual method with block coordinate updates on the dual side, as summarized in Algorithm 2, for completeness. To see the equivalence, in \(i\)-th inner iteration of \(k\)-th epoch, we first update the \(i\)-th block \(\bm{y}_{k}^{(i)}\in\mathbb{R}^{b}\) of the dual vector \(\bm{y}_{k-1}\in\mathbb{R}^{n}\) based on \(\bm{x}_{k-1,i}\) as in Line 6. Since the dual update has a decomposable structure, this maximization step corresponds to computing the (sub)grictions \(\{\ell_{\pi_{j}^{(k)}}^{\prime}(\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}) \}_{j=b(i-1)+1}^{bi}\) at \(\bm{x}_{k-1,i}\) for the batch of individual losses indexed by \(\{\pi_{j}^{(k)}\}_{j=b(i-1)+1}^{bi}\). Then in Line 7, we perform a minimization step using \(\bm{y}_{k}^{(i)}\) to compute \(\bm{x}_{k-1,i+1}\) on the primal side. Combining these two steps, we have \(\bm{x}_{k-1,i+1}=\bm{x}_{k-1,i}-\frac{\eta_{k}}{b}\sum_{j=b(i-1)+1}^{bi}\ell_ {\pi_{j}^{(k)}}^{\prime^{\top}}(\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}) \bm{a}_{\pi_{j}^{(k)}}\), which is exactly the _original primal shuffled SGD updating scheme_.

### Omitted Proofs for the Random Reshuffling/Shuffle-Once Schemes

**Lemma 9**.: _Given \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 2 for \(k\in[K]\), let \(\mathcal{E}_{k}:=\eta_{k}\mathrm{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})+\frac{b}{2 n}\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2}^{2}\). If Assumption 3 holds, then_

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k} ^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i+1})\] (17) \[\quad+\frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}_{k}^{(i)}-\bm {y}_{k}^{(i)}\big{)}^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{\Lambda}_{ k}^{-1}}^{2}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{ k}^{-1}}^{2}\] \[\quad-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1} \|_{2}^{2},\]

Proof.: By Line 6 in Alg. 2, we have \(\bm{y}_{k}^{(i)}=\arg\max_{\bm{y}\in\mathbb{R}^{b}}\Big{\{}\bm{y}^{\top}\bm{A} _{k}^{(i)}\bm{x}_{k-1,i}-\sum_{j=1}^{b}\ell_{\pi_{b(i-1)+j}}^{*}(\bm{y}^{j}) \Big{\}}\) for \(i\in[m]\). Notice that since

\[\bm{y}^{\top}\bm{A}_{k}^{(i)}\bm{x}_{k-1,i}-\sum_{j=1}^{b}\ell_{\pi_{b(i-1)+j} ^{(k)}}^{*}(\bm{y}^{j})=\sum_{j=1}^{b}\Big{(}\bm{y}^{j}\bm{a}_{\pi_{b(i-1)+j} ^{(k)}}^{\top}\bm{x}_{k-1,i}-\ell_{\pi_{b(i-1)+j}^{(k)}}^{*}(\bm{y}^{j})\Big{)}\]

is separable, we have \(\bm{y}_{k}^{j}=\arg\max_{\bm{v}\in\mathbb{R}}\{y\bm{a}_{\pi_{j}^{(k)}}^{\top} \bm{x}_{k-1,i}-\ell_{\pi_{j}^{(k)}}^{*}(y)\}\) for \(b(i-1)+1\leq j\leq bi\), thus \(\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}\in\partial\ell_{\pi_{j}^{(k)}}^{*} (\bm{y}_{k}^{j})\). Since \(\ell_{i}^{*}\) is \(\frac{1}{L_{i}}\)-strongly convex by Assumption 3, then by Lemma 4 we obtain for \(b(i-1)+1\leq j\leq bi\)

\[\ell_{\pi_{j}^{(k)}}^{*}\big{(}\bm{v}_{k}^{j}\big{)}\geq\ell_{\pi_{j}^{(k)}}^{ *}(\bm{y}_{k}^{j})+\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}\big{(}\bm{v}_{k }^{j}-\bm{y}_{k}^{j}\big{)}+\frac{1}{2L_{\pi_{j}^{(k)}}}\big{(}\bm{v}_{k}^{j}- \bm{y}_{k}^{j}\big{)}^{2},\]

which leads to

\[\mathcal{L}(\bm{x}_{k},\bm{v}) =\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^{ (i)}\bm{x}_{k-1,i}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{v}_{k}^{j })\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{x}_ {k}-\bm{x}_{k-1,i})\] \[\leq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k} ^{(i)}\bm{x}_{k-1,i}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{k}^ {j})\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{x} _{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{1}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{\Lambda}_{k}^{-1}} ^{2}.\] (18)Using the same argument for \(\mathcal{L}(\bm{x}_{*},\bm{y}_{*})\) as \(\bm{a}_{j}^{\top}\bm{x}_{*}\in\partial\ell_{j}^{*}(\bm{y}_{j}^{j})\) for \(j\in[n]\), we have

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) =\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{*,k}^{(i)\top}\bm{A}_{k}^ {(i)}\bm{x}_{*}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{*,k}^{j} )\Big{)}\] \[\geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k} ^{(i)}\bm{x}_{*}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{k}^{j} )\Big{)}+\frac{1}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\] (19)

Adding and substracting the term \(\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}\) on the R.H.S. of Eq. (19), we obtain

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) \geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k }^{(i)}\bm{x}_{*}+\frac{b}{2\eta_{k}}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}- \sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{k}^{j})\Big{)}\] \[\quad-\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\|\bm{x}_{*}-\bm{x}_{k-1, i}\|_{2}^{2}+\frac{1}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

By Line 7 of Alg. 2, we have \(\bm{x}_{k-1,i+1}=\arg\min_{\bm{x}\in\mathbb{R}^{d}}\Big{\{}\bm{y}_{k}^{(i)\top }\bm{A}_{k}^{(i)}\bm{x}+\frac{b}{2\eta_{k}}\|\bm{x}-\bm{x}_{k-1,i}\|_{2}^{2} \Big{\}}\). Further noticing that \(\phi_{k}^{(i)}(\bm{x}):=\bm{y}_{k}^{(i)\top}\bm{A}_{k}^{(i)}\bm{x}+\frac{b}{2 \eta_{k}}\|\bm{x}-\bm{x}_{k-1,i}\|_{2}^{2}\) is \(\frac{b}{\eta_{k}}\)-strongly convex w.r.t. \(\bm{x}\) and \(\nabla\phi_{k}^{(i)}(\bm{x}_{k-1,i+1})=\bm{0}\), we have

\[\phi_{k}^{(i)}(\bm{x}_{*})\geq\phi_{k}^{(i)}(\bm{x}_{k-1,i+1})+ \frac{b}{2\eta_{k}}\|\bm{x}_{*}-\bm{x}_{k-1,i+1}\|_{2}^{2},\]

which leads to

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) \geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k }^{(i)}\bm{x}_{k-1,i+1}+\frac{b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\| _{2}^{2}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{k}^{j})\Big{)}\] \[\quad+\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\big{(}\|\bm{x}_{*}-\bm{ x}_{k-1,i+1}\|_{2}^{2}-\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}\big{)}+\frac{1}{2n} \|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}\] \[\stackrel{{(i)}}{{=}}\frac{1}{n}\sum_{i=1}^{m}\Big{(} \bm{y}_{k}^{(i)\top}\bm{A}_{k}^{(i)}\bm{x}_{k-1,i+1}+\frac{b}{2\eta_{k}}\|\bm{ x}_{k-1,i+1}-\bm{x}_{k-1,i}\|_{2}^{2}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}( \bm{y}_{k}^{j})\Big{)}\] \[\quad+\frac{b}{2n\eta_{k}}\|\bm{x}_{k}-\bm{x}_{*}\|_{2}^{2}-\frac {b}{2n\eta_{k}}\|\bm{x}_{k-1}-\bm{x}_{*}\|_{2}^{2}+\frac{1}{2n}\|\bm{y}_{k}- \bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2},\] (20)

where we telescope from \(i=1\) to \(m\) for the term \(\sum_{i=1}^{m}\big{(}\|\bm{x}_{*}-\bm{x}_{k-1,i+1}\|_{2}^{2}-\|\bm{x}_{*}-\bm{x} _{k-1,i}\|_{2}^{2}\big{)}\), and use the definitions that \(\bm{x}_{k}=\bm{x}_{k-1,m+1}\) and \(\bm{x}_{k-1}=\bm{x}_{k-1,1}\) for \((i)\).

Combining the bounds from Eq. (18) and Eq. (20) and denoting

\[\mathcal{E}_{k}:=\eta_{k}\big{(}\mathcal{L}(\bm{x}_{k},\bm{v})- \mathcal{L}(\bm{x}_{*},\bm{y}_{*})\big{)}+\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k}\|_{ 2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2}^{2},\]

we obtain

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k }^{(i)}(\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1})+\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{v}_ {k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{\Lambda}_{ k}^{-1}}^{2}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}- \frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\|_{2}^{2}\] \[=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k}^{(i )}(\bm{x}_{k}-\bm{x}_{k-1,i+1})+\frac{\eta_{k}}{n}\sum_{i=1}^{m}(\bm{v}_{k}^{(i) }-\bm{y}_{k}^{(i)})^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}_{k}\|_{\bm{\Lambda}_{k}^ {-1}}^{2}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}- \frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\|_{2}^{2},\]

thus completing the proof.

**Lemma 10**.: _For any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) in Algorithm 2 satisfy_

\[\mathcal{T}_{1}=\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\|_{2 }^{2}-\frac{b}{2n}\|\bm{x}_{k-1}-\bm{x}_{k}\|_{2}^{2}.\]

Proof.: By Line 7 in Alg. 2, we have \(\bm{A}_{k}^{(i)\top}\bm{y}_{k}^{(i)}=\frac{b}{\eta_{k}}(\bm{x}_{k-1,i}-\bm{x}_ {k-1,i+1})\). Further noticing that \(\bm{x}_{k}-\bm{x}_{k-1,i+1}=-\sum_{j=i+1}^{m}(\bm{x}_{k-1,j}-\bm{x}_{k-1,j+1})\), we obtain

\[\mathcal{T}_{1}:= \ \frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k}^ {(i)}(\bm{x}_{k}-\bm{x}_{k-1,i+1})\] \[= \ -\frac{b}{n}\sum_{i=1}^{m}\sum_{j=i+1}^{m}\left\langle\bm{x}_{k -1,i}-\bm{x}_{k-1,i+1},\bm{x}_{k-1,j}-\bm{x}_{k-1,j+1}\right\rangle\] \[= \ \frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\|^ {2}-\frac{b}{2n}\Big{\|}\sum_{i=1}^{m}(\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1})\Big{\|} ^{2},\]

thus completing the proof. 

**Lemma 11**.: _Under Assumption 4, for any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 2 with uniformly random shuffling satisfy_

\[\mathbb{E}[\mathcal{T}_{2}]\leq\mathbb{E}\Big{[}\frac{\eta_{k}^{3}n\hat{L}_{ \pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{A}_{k} ^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{A}_{k}^{-1}}^{2} \Big{]}+\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

Proof.: By Line 7 in Alg. 2, we have \(\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}=\frac{\eta_{k}}{b}\bm{A}_{k}^{(i)\top}\bm{y}_{ k}^{(i)}\). Using the definition of \(\bm{I}_{j\uparrow}\) for \(0\leq j\leq n-1\) as in Section 1, we obtain

\[\bm{x}_{k}-\bm{x}_{k-1,i}=-\sum_{j=i}^{m}(\bm{x}_{k-1,j}-\bm{x}_{k-1,j+1})=- \frac{\eta_{k}}{b}\sum_{j=i}^{m}\bm{A}_{k}^{(j)\top}\bm{y}_{k}^{(j)}=-\frac{ \eta_{k}}{b}\bm{A}_{k}\bm{I}_{b(i-1)\uparrow}\bm{y}_{k}.\]

Also, we have \(\bm{A}_{k}^{(i)\top}(\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i)})=\bm{A}_{k}\bm{I}_{(i)} (\bm{v}_{k}-\bm{y}_{k})\) by the definition of \(\bm{I}_{(i)}\) in Section 3. Combining these two observations, we have

\[\mathcal{T}_{2}:= \ \frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}_{k}^{(i)}-\bm{y}_ {k}^{(i)}\big{)}^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[= \ -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^ {\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{k},\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v}_{k }-\bm{y}_{k})\big{\rangle}\] \[\stackrel{{(i)}}{{=}} \ -\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^ {\top}\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k}),\bm{A}_{k}^{\top}\bm{I}_ {(i)}(\bm{v}_{k}-\bm{y}_{k})\big{\rangle}\] (21) \[-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k},\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v}_{ k}-\bm{y}_{k})\big{\rangle}\,,\] (22)

where we make a decomposition w.r.t. \(\bm{y}_{*,k}\) in \((i)\). For the first term in Eq. (21), we use Young's inequality for \(\alpha>0\) and have

\[\begin{split}&-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm {A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k}),\bm{A}_{k}^{\top} \bm{I}_{(i)}(\bm{v}_{k}-\bm{y}_{k})\big{\rangle}\\ \leq&\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\| \bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\|_{2}^{2}+ \frac{\eta_{k}^{2}}{2bn\alpha}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v }_{k}-\bm{y}_{k})\|_{2}^{2}.\end{split}\] (23)Expanding the squares and rearranging the terms in Eq. (23), we have

\[\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top}\bm{I }_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\|_{2}^{2}\] (24) \[=\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}(\bm{y}_{k}-\bm{y}_{ *,k})^{\top}\bm{I}_{b(i-1)\uparrow}\bm{A}_{k}\bm{A}_{k}^{\top}\bm{I}_{b(i-1) \uparrow}(\bm{y}_{k}-\bm{y}_{*,k})\] \[=\frac{\eta_{k}^{2}\alpha}{2bn}(\bm{y}_{k}-\bm{y}_{*,k})^{\top} \Big{(}\sum_{i=1}^{m}\bm{I}_{b(i-1)\uparrow}\bm{A}_{k}\bm{A}_{k}^{\top}\bm{I}_{ b(i-1)\uparrow}\Big{)}(\bm{y}_{k}-\bm{y}_{*,k})\] \[=\frac{\eta_{k}^{2}\alpha}{2bn}(\bm{y}_{k}-\bm{y}_{*,k})^{\top} \bm{\Lambda}_{k}^{-1/2}\bm{\Lambda}_{k}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{b(i- 1)\uparrow}\bm{A}_{k}\bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}\Big{)}\bm{ \Lambda}_{k}^{1/2}\bm{\Lambda}_{k}^{-1/2}(\bm{y}_{k}-\bm{y}_{*,k})\] \[\stackrel{{(i)}}{{\leq}}\frac{\eta_{k}^{2}\alpha}{2 bn}\Big{\|}\bm{\Lambda}_{k}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{b(i-1)\uparrow}\bm{A}_{k} \bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}\Big{)}\bm{\Lambda}_{k}^{1/2}\Big{\|} _{2}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2},\]

where we use Cauchy-Schwarz inequality for \((i)\). Using a similar argument, we also have

\[\frac{\eta_{k}^{2}}{2bn\alpha}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top}\bm{I}_{(i)}( \bm{v}_{k}-\bm{y}_{k})\|_{2}^{2}\leq\frac{\eta_{k}^{2}}{2bn\alpha}\Big{\|}\bm{ \Lambda}_{k}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{(i)}\bm{A}_{k}\bm{A}_{k}^{\top} \bm{I}_{(i)}\Big{)}\bm{\Lambda}_{k}^{1/2}\Big{\|}_{2}\|\bm{\nu}_{k}-\bm{y}_{k} \|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

By the definitions of \(\hat{L}_{\pi^{(k)}}\) and \(\tilde{L}_{\pi^{(k)}}\), and choosing \(\alpha=2\eta_{k}\tilde{L}_{\pi^{(k)}}\) in Eq. (23), we obtain

\[-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*,k}),\bm{A}_{k}^{\top}\bm{I} _{(i)}(\bm{v}_{k}-\bm{y}_{k})\big{\rangle}\] (25) \[\leq\frac{\eta_{k}^{3}n\hat{L}_{\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{ b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}+\frac{\eta_{k}}{4n} \|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

For the second term in Eq. (22), we apply Young's inequality with \(\beta>0\) and proceed as above:

\[-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k},\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v} _{k}-\bm{y}_{k})\big{\rangle}\] \[\leq\frac{\eta_{k}^{2}\beta}{2bn}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top} \bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2bn\beta} \sum_{i=1}^{m}\|\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v}_{k}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}\beta}{2bn}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top} \bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2n\beta} \tilde{L}_{\pi^{(k)}}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

Noticing that \(\tilde{L}_{\pi^{(k)}}\leq\tilde{L}\), we choose \(\beta=2\eta_{k}\tilde{L}\) and obtain

\[-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k},\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v} _{k}-\bm{y}_{k})\big{\rangle}\] (26) \[\leq\frac{\eta_{k}^{3}\tilde{L}}{nb}\sum_{i=1}^{m}\|\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}+\frac{\eta_{k}}{4n}\|\bm{v} _{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}.\]

Combining Eq. (25) and Eq. (26), we have

\[\mathcal{T}_{2}\leq\frac{\eta_{k}^{3}\tilde{L}}{nb}\sum_{i=1}^{m}\|\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}+\frac{\eta_{k}^{3}n\hat{L}_{ \pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda} _{k}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^ {2}.\] (27)

We first assume the RR scheme. Taking conditional expectation w.r.t. the randomness up to but not including \(k\)-th epoch, we have

\[\mathbb{E}_{k}[\mathcal{T}_{2}] \leq\frac{\eta_{k}^{3}\tilde{L}}{nb}\mathbb{E}_{k}\Big{[}\sum_{i=1}^ {m}\|\bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}\Big{]}\] \[\quad+\mathbb{E}_{k}\Big{[}\frac{\eta_{k}^{3}n\hat{L}_{\pi^{(k)}} \tilde{L}_{\pi^{(k)}}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}+ \frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{\Lambda}_{k}^{-1}}^{2}\Big{]}.\]For the first term \(\frac{\eta_{k}^{3}\tilde{L}}{nb}\mathbb{E}_{k}\Big{[}\sum_{i=1}^{m}\|\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}\Big{]}\), the only randomness is from the random permutation \(\pi^{(k)}\). In this case, each term \(\bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\) can be considered as a sum of a batch sampled without replacement from \(\{\bm{y}_{*}^{j}\bm{a}_{j}\}_{j\in[n]}\), while \(\sum_{j=1}^{n}\bm{y}_{*}^{j}\bm{a}_{j}=0\) as \(\bm{x}_{*}\) is the minimizer, we then can use Lemma 5 and obtain

\[\frac{\eta_{k}^{3}\tilde{L}}{nb}\mathbb{E}_{k}\Big{[}\sum_{i=1}^{ m}\|\bm{A}_{k}^{\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}\Big{]} \stackrel{{(\text{\tiny{ij}})}}{{=}}\frac{\eta_{k}^{3} \tilde{L}}{nb}\sum_{i=1}^{m}\mathbb{E}_{\pi^{(k)}}[\|\bm{A}_{k}^{\top}\bm{I}_{ b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}]\] \[=\frac{\eta_{k}^{3}\tilde{L}}{nb}\sum_{i=1}^{m}\big{(}n-b(i-1) \big{)}^{2}\mathbb{E}_{\pi^{(k)}}\Big{[}\Big{\|}\frac{\bm{A}_{k}^{\top}\bm{I}_ {b(i-1)\uparrow}\bm{y}_{*,k}}{n-b(i-1)}\Big{\|}_{2}^{2}\Big{]}\] \[\stackrel{{(i)}}{{\leq}}\frac{\eta_{k}^{3}\tilde{L}} {nb}\sum_{i=1}^{m}\big{(}n-b(i-1)\big{)}^{2}\frac{b(i-1)}{(n-b(i-1))(n-1)} \sigma_{*}^{2}\] \[=\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2},\]

where \((i)\) is due to the linearity of expectation, and we use our definition \(\sigma_{*}^{2}=\frac{1}{n}\sum_{j=1}^{n}(\bm{y}_{*}^{j})^{2}\|\bm{a}_{j}\|_{2 }^{2}=\mathbb{E}_{j}\big{[}\|\bm{y}_{*}^{j}\bm{a}_{j}\|_{2}^{2}\big{]}\) for \((ii)\). Taking expectation w.r.t. all the randomness on both sides and using the law of total expectation, we obtain

\[\mathbb{E}[\mathcal{T}_{2}]\leq\mathbb{E}\Big{[}\frac{\eta_{k}^{3}n\tilde{L}_ {\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{b^{2}}\|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{A}_{k }^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}_{k}-\bm{y}_{k}\|_{\bm{A}_{k}^{-1}}^{2} \Big{]}+\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

For the SO scheme, since there is only one random permutation generated at the very beginning, we can take expectation w.r.t. all the randomness on both sides of (27), and the randomness for the term \(\frac{\eta_{k}^{3}\tilde{L}}{nb}\mathbb{E}\Big{[}\sum_{i=1}^{m}\|\bm{A}_{k}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*,k}\|_{2}^{2}\Big{]}\) is only from the initial random permutation. So the above argument still applies to this case, and we complete the proof. 

**Theorem 2**.: _Under Assumptions 3 and 4, if \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}_{\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), then the output \(\hat{\bm{x}}_{K}\) of Alg. 1 with uniformly random (RR/SO) shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{b}{2n}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{ 6b^{2}(n-1)}\sigma_{*}^{2}.\]

_As a result, given \(\epsilon>0,\) there exists a constant step size \(\eta_{k}=\eta\) such that \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\sqrt{\tilde{L}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}}{ \epsilon}+\sqrt{\frac{(n-b)(n+b)}{n(n-1)}}\frac{\sqrt{\tilde{n}\tilde{L}\sigma_ {*}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}}{\epsilon^{3/2}}\big{)}\) individual gradient queries._

Proof.: Combining the bounds in Lemma 10 and 11 and plugging them into Eq. (17), we obtain

\[\mathbb{E}[\mathcal{E}_{k}]\leq\mathbb{E}\Big{[}\Big{(}\frac{\eta_{k}^{3}n \tilde{L}_{\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{b^{2}}-\frac{\eta_{k}}{2n}\Big{)} \|\bm{y}_{k}-\bm{y}_{*,k}\|_{\bm{A}_{k}^{-1}}^{2}\Big{]}+\frac{\eta_{k}^{3} \tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

For the stepsize \(\eta_{k}\) such that \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}_{\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}}\), we have \(\frac{\eta_{k}^{3}n\tilde{L}_{\pi^{(k)}}\tilde{L}_{\pi^{(k)}}}{b^{2}}-\frac{ \eta_{k}}{2n}\leq 0\), thus

\[\mathbb{E}[\mathcal{E}_{k}]\leq\frac{\eta_{k}^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1 )}\sigma_{*}^{2}.\]

Noticing that \(\mathcal{E}_{k}=\eta_{k}\text{Gap}^{\text{\tiny{w}}}(\bm{x}_{k},\bm{y}_{*})+ \frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k -1}\|_{2}^{2}\) and telescoping from \(k=1\) to \(K\), we have

\[\mathbb{E}\Big{[}\sum_{k=1}^{K}\eta_{k}\text{Gap}^{\text{\tiny{w}}}(\bm{x}_{k}, \bm{y}_{*})\Big{]}\leq\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{0}\|_{2}^{2}-\frac{b}{2n} \mathbb{E}[\|\bm{x}_{*}-\bm{x}_{K}\|_{2}^{2}]+\sum_{k=1}^{K}\frac{\eta_{k}^{3} \tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]Noticing that \(\mathcal{L}(\bm{x},\bm{v})\) is convex w.r.t. \(\bm{x}\), we have \(\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\leq\sum_{k=1}^{K}\eta_{k}\text{ Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})/H_{K}\), where \(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/H_{K}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), which leads to

\[\mathbb{E}\Big{[}H_{K}\text{Gap}^{\bm{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\Big{]} \leq\frac{b}{2n}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k} ^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\]

Further choosing \(\bm{v}=\bm{y}_{\hat{\bm{x}}_{K}}\), we obtain

\[\mathbb{E}[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}] \leq\frac{b}{2n}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{\eta_{k} ^{3}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}.\] (28)

To analyze the individual gradient oracle complexity, we choose constant stepsizes \(\eta\leq\frac{b}{n\sqrt{2\tilde{L}\tilde{L}}}\), then Eq. (28) will become

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\frac{b}{2n\eta K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{\eta^{2}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_ {*}^{2}.\]

Without loss of generality, we assume that \(b\neq n\), otherwise the method and its analysis reduce to (full) gradient descent. We consider the following two cases:

\(\bullet\) "Small \(K\)" case: if \(\eta=\frac{b}{n\sqrt{2\tilde{L}\tilde{L}}}\leq\Big{(}\frac{3b^{3}(n-1)\|\bm{x} _{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)LKs\sigma_{*}^{2}}\Big{)}^{1/3}\), we have

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})] \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{ \eta^{2}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}\] \[\leq\frac{\sqrt{\tilde{L}\tilde{L}}}{\sqrt{2}K}\|\bm{x}_{0}-\bm{x }_{*}\|_{2}^{2}+\frac{1}{2}\Big{(}\frac{(n-b)(n+b)}{n^{2}(n-1)}\Big{)}^{1/3} \frac{\tilde{L}^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{3^{ 1/3}K^{2/3}}.\]

\(\bullet\) "Large \(K\)" case: if \(\eta=\Big{(}\frac{3b^{3}(n-1)\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)LKs \sigma_{*}^{2}}\Big{)}^{1/3}\leq\frac{b}{n\sqrt{2\tilde{L}\tilde{L}}}\), we have

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})] \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{ \eta^{2}\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\sigma_{*}^{2}\] \[\leq\Big{(}\frac{(n-b)(n+b)}{n^{2}(n-1)}\Big{)}^{1/3}\frac{\tilde {L}^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.\]

Combining these two cases by setting \(\eta=\min\Big{\{}\frac{b}{n\sqrt{2\tilde{L}\tilde{L}}},\,\Big{(}\frac{3b^{3}(n -1)\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{n(n-b)(n+b)LKs\sigma_{*}^{2}}\Big{)}^{1/3}\Big{\}}\), we obtain

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\frac{\sqrt{\tilde{L}\tilde{L} }}{\sqrt{2}K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\Big{(}\frac{(n-b)(n+b)}{n^{2} (n-1)}\Big{)}^{1/3}\frac{\tilde{L}^{1/3}\sigma_{*}^{2/3}\|\bm{x}_{0}-\bm{x}_{* }\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.\]

Hence, to guarantee \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) for \(\epsilon>0\), the total number of individual gradient evaluations will be

\[nK\geq\max\Big{\{}\frac{n\sqrt{2\tilde{L}\tilde{L}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2 }^{2}}{\epsilon},\Big{(}\frac{(n-b)(n+b)}{n-1}\Big{)}^{1/2}\frac{2^{3/2}\tilde{L }^{1/2}\sigma_{*}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{3^{1/2}\epsilon^{3/2}}\Big{\}},\]

as claimed. 

### Omitted Proofs for Incremental Gradient Descent

We now provide the proof for convergence of IGD in the smooth convex settings. We first prove the following technical lemma, which bounds the inner product term \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}^{(i)}-\bm{y}_{k}^ {(i)}\big{)}^{\top}\bm{A}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\) without random permutations involved.

**Lemma 12**.: _For any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 2 with fixed data ordering satisfy_

\[\mathcal{T}_{2} \leq\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{k} -\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}-\bm{y}_{k}\| _{\bm{\Lambda}^{-1}}^{2}\] (29) \[\quad+\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}\tilde{L }_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2} }\tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\]

Proof.: Proceeding as in Lemma 11, we have

\[\mathcal{T}_{2} :=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}^{(i)}-\bm{y}_{k}^ {(i)}\big{)}^{\top}\bm{A}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[=-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}^{\top }\bm{I}_{b(i-1)\uparrow}\bm{y}_{k},\bm{A}^{\top}\bm{I}_{(i)}(\bm{v}-\bm{y}_{k })\big{\rangle}\] (30) \[=-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}^{\top }\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*}),\bm{A}^{\top}\bm{I}_{(i)}(\bm {v}-\bm{y}_{k})\big{\rangle}\] \[\quad-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*},\bm{A}^{\top}\bm{I}_{(i)}(\bm{v}-\bm{y} _{k})\big{\rangle}\,,\] (31)

For both terms in Eq. (30) and Eq. (31), we use Young's inequality for \(\alpha=2\eta_{k}\tilde{L}_{0}>0\) and proceed as in Eq. (24) to obtain

\[\quad-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}^{ \top}\bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*}),\bm{A}^{\top}\bm{I}_{(i)}( \bm{v}-\bm{y}_{k})\big{\rangle}\] \[\leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{A}^{\top} \bm{I}_{b(i-1)\uparrow}(\bm{y}_{k}-\bm{y}_{*})\|_{2}^{2}+\frac{\eta_{k}^{2}}{2 bn\alpha}\sum_{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{(i)}(\bm{v}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}n\alpha}{2b^{2}}\hat{L}_{0}\|\bm{y}_{k}-\bm {y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}^{2}}{2n\alpha}\tilde{L}_{0} \|\bm{v}-\bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] \[=\frac{\eta_{k}^{3}}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{k}- \bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{4n}\|\bm{v}-\bm{y}_{k}\|_{ \bm{\Lambda}^{-1}}^{2}\] (32)

and

\[\quad-\frac{\eta_{k}^{2}}{bn}\sum_{i=1}^{m}\big{\langle}\bm{A}^{ \top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*},\bm{A}^{\top}\bm{I}_{(i)}(\bm{v}-\bm{y} _{k})\big{\rangle}\] \[\leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{A}^{\top} \bm{I}_{b(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2bn\alpha}\sum _{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{(i)}(\bm{v}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{A}^{\top} \bm{I}_{b(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2n\alpha}\tilde {L}_{0}\|\bm{v}-\bm{y}_{k}\|_{\bm{\Lambda}^{-1}}^{2}\] \[=\frac{\eta_{k}^{3}\tilde{L}_{0}}{nb}\sum_{i=1}^{m}\|\bm{A}^{\top} \bm{I}_{b(i-1)\uparrow}\bm{y}_{*}\|_{2}^{2}+\frac{\eta_{k}}{4n}\|\bm{v}-\bm{y} _{k}\|_{\bm{\Lambda}^{-1}}^{2},\] (33)

where again we used \(\alpha=2\eta_{k}\tilde{L}_{0}\). We then prove the term \(\frac{\eta_{k}^{3}\tilde{L}_{0}}{nb}\sum_{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{b(i-1 )\uparrow}\bm{y}_{*}\|_{2}^{2}\) in Eq. (33) is no larger than the minimum of \(\frac{\eta_{k}^{3}\tilde{L}_{0}}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm {\Lambda}^{-1}}^{2}\) and \(\frac{\eta_{k}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\). Note that when \(b=n\), we have \(\bm{A}^{\top}\bm{I}_{(0)\uparrow}\bm{y}_{*}=0\), so this term disappears. When \(b<n\), the former one can be derived as in Eq.(24), which gives

\[\sum_{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*}\|_ {2}^{2} \leq\Big{\|}\bm{\Lambda}^{1/2}\Big{(}\sum_{i=1}^{m}\bm{I}_{b(i-1) \uparrow}\bm{A}\bm{A}^{\top}\bm{I}_{b(i-1)\uparrow}\Big{)}\bm{\Lambda}^{1/2} \Big{\|}_{2}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}=mn\hat{L}_{0}\|\bm{y}_{*}\|_{ \bm{\Lambda}^{-1}}^{2}\] \[=\frac{n^{2}}{b}\hat{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}.\]For the latter one, we notice that

\[\sum_{i=1}^{m}\left\|\bm{A}^{\top}\bm{I}_{b(i-1)\uparrow}\bm{y}_{*} \right\|_{2}^{2} =\sum_{i=1}^{m}\Big{\|}\sum_{j=b(i-1)+1}^{n}\bm{y}_{*}^{j}\bm{a}_{j} \Big{\|}_{2}^{2}\] \[=\sum_{i=0}^{m-1}\Big{\|}\sum_{j=bi+1}^{n}\bm{y}_{*}^{j}\bm{a}_{j} \Big{\|}_{2}^{2}\] \[=\sum_{i=1}^{m-1}\Big{\|}\sum_{j=bi+1}^{n}\bm{y}_{*}^{j}\bm{a}_{j} \Big{\|}_{2}^{2}=\sum_{i=1}^{m-1}\Big{\|}\sum_{j=1}^{bi}\bm{y}_{*}^{j}\bm{a}_{j }\Big{\|}_{2}^{2},\]

by using the fact that \(\sum_{j=1}^{n}\bm{y}_{*}^{j}\bm{a}_{j}=0\). Using Young's inequality, we have

\[\sum_{i=1}^{m-1}\Big{\|}\sum_{j=1}^{bi}\bm{y}_{*}^{j}\bm{a}_{j} \Big{\|}_{2}^{2} \leq\sum_{i=1}^{m-1}bi\sum_{j=1}^{bi}\|\bm{y}_{*}^{j}\bm{a}_{j} \|_{2}^{2}\] \[\leq b(m-1)\sum_{i=1}^{m-1}\sum_{j=1}^{bi}\|\bm{y}_{*}^{j}\bm{a}_{j }\|_{2}^{2}\] \[=b(m-1)\sum_{i=1}^{m-1}\sum_{j=b(i-1)+1}^{bi}(m-i)\|\bm{y}_{*}^{j} \bm{a}_{j}\|_{2}^{2}\] \[\leq b(m-1)^{2}\sum_{i=1}^{(m-1)b}\|\bm{y}_{*}^{j}\bm{a}_{j}\|_{2 }^{2}.\]

By the definition that \(\sigma_{*}^{2}=\frac{1}{n}\sum_{j=1}^{n}\|\bm{y}_{*}^{j}\bm{a}_{j}\|_{2}^{2}\) and \(\sum_{i=i}^{(m-1)b}\|\bm{y}_{*}^{j}\bm{a}_{j}\|_{2}^{2}\leq\sum_{j=1}^{n}\|\bm {y}_{*}^{j}\bm{a}_{j}\|_{2}^{2}=n\sigma_{*}^{2}\), we obtain

\[\frac{\eta_{k}^{3}\tilde{L}_{0}}{nb}\sum_{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{b(i-1 )\uparrow}\bm{y}_{*}\|_{2}^{2}\leq\frac{\eta_{k}^{3}\tilde{L}_{0}}{b}b(m-1)^{2 }\sigma_{*}^{2}=\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}.\] (34)

Note that the bound in Eq. (34) equals to zero when \(b=n\), which recovers the case of full gradient descent, so we have

\[\frac{\eta_{k}^{3}\tilde{L}_{0}}{nb}\sum_{i=1}^{m}\|\bm{A}^{\top}\bm{I}_{b(i-1 )\uparrow}\bm{y}_{*}\|_{2}^{2}\leq\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}} \tilde{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k }^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\] (35)

Combining Eq. (32)-(35), we obtain

\[\mathcal{T}_{2}\leq\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y }_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+\frac{\eta_{k}}{2n}\|\bm{v}-\bm{y}_ {k}\|_{\bm{\Lambda}^{-1}}^{2}+\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\tilde{L} _{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b )^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}},\]

thus finishing the proof. 

**Theorem 6**.: _Under Assumptions 3 and 4, if \(\eta_{k}\leq\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), the output \(\hat{\bm{x}}_{K}\) of Alg. 2 with a fixed permutation satisfies_

\[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}\leq\frac{b}{2n}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}} \hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^ {3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\]

_As a consequence, given \(\epsilon>0,\) there exists a constant step size \(\eta_{k}=\eta\) such that \(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\leq\epsilon\) after the number of gradient queries bounded by \(\mathcal{O}\Big{(}\frac{n\sqrt{\tilde{L}_{0}\tilde{L}_{0}}\|\bm{x}_{0}-\bm{x}_{* }\|_{2}^{2}}{\epsilon}+\frac{\min\Big{\{}\sqrt{\tilde{L}_{0}\tilde{L}_{0}}\tilde{ L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}},(n-b)\sqrt{\tilde{L}_{0}\sigma_{*}}\Big{\}}\| \bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}\Big{)}.\)_Proof.: Proceeding as in Lemmas 9 and 10, but without random permutations, we have

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}^{(i) }(\bm{x}_{k}-\bm{x}_{k-1,i+1})+\frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}^{(i )}-\bm{y}_{k}^{(i)}\big{)}^{\top}\bm{A}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{v}\|_{\bm{\Lambda}^{-1}}^ {2}-\frac{\eta_{k}}{2n}\|\bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}-\frac {b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\|_{2}^{2}\] \[\leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\big{(}\bm{v}^{(i)}-\bm{y}_{k} ^{(i)}\big{)}^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})-\frac{\eta_{k}} {2n}\|\bm{y}_{k}-\bm{v}\|_{\bm{\Lambda}^{-1}}^{2}-\frac{\eta_{k}}{2n}\|\bm{y}_ {k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}.\] (36)

Using the bound in Lemma 12 and applying Eq. (29) into Eq. (36), we obtain

\[\mathcal{E}_{k}\leq\Big{(}\frac{\eta_{k}^{3}n\hat{L}_{0}\tilde{L}_{0}}{b^{2}} -\frac{\eta_{k}}{2n}\Big{)}\|\bm{y}_{k}-\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}+ \min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_ {\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_ {*}^{2}\Big{\}}.\]

If \(\eta_{k}\leq\frac{b}{n\sqrt{2}L_{0}\tilde{L}_{0}}\), we have \(\frac{\eta_{k}^{3}n\hat{L}_{0}\tilde{L}_{0}}{b^{2}}-\frac{\eta_{k}}{2n}\leq 0\), thus

\[\mathcal{E}_{k}\leq\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat{L}_{0}\tilde{L} _{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n-b)^{2}}{b^{2}} \tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\]

Noticing that \(\mathcal{E}_{k}=\eta_{k}\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})+\frac{b}{2n }\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2} ^{2}\) and telescoping from \(k=1\) to \(K\), we have

\[\sum_{k=1}^{K}\eta_{k}\text{Gap}^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})\] \[\leq\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{0}\|_{2}^{2}-\frac{b}{2n}\| \bm{x}_{*}-\bm{x}_{K}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n }{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{ \eta_{k}^{3}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\]

Noticing that \(\mathcal{L}(\bm{x},\bm{v})\) is convex w.r.t. \(\bm{x}\), we have \(\text{Gap}^{\bm{v}}(\hat{x}_{K},\bm{y}_{*})\leq\sum_{k=1}^{K}\eta_{k}\text{Gap} ^{\bm{v}}(\bm{x}_{k},\bm{y}_{*})/H_{K}\), where \(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/H_{K}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), so we obtain

\[H_{K}\text{Gap}^{\bm{v}}(\hat{x}_{K},\bm{y}_{*})\leq\frac{b}{2n}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat {L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n -b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}},\]

Further choosing \(\bm{v}=\bm{y}_{\hat{x}_{K}}\), we obtain

\[H_{K}\big{(}f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\big{)}\leq\frac{b}{2n}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\min\Big{\{}\frac{\eta_{k}^{3}n}{b^{2}}\hat {L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2},\frac{\eta_{k}^{3}(n -b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\Big{\}}.\] (37)

To analyze the individual gradient oracle complexity, we choose constant stepsizes \(\eta\leq\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}}\) and assume \(b<n\) without loss of generality, then Eq. (37) becomes

\[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})\leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_ {2}^{2}+\min\Big{\{}\frac{\eta^{2}n}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_ {\bm{\Lambda}^{-1}}^{2},\frac{\eta^{2}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2 }\Big{\}}.\]

When \(\hat{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}\leq\frac{(n-b)^{2}}{n}\sigma_{*} ^{2}\), we set \(\eta=\min\Big{\{}\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}},\,\Big{(}\frac{b^{3} \|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{2n^{2}\tilde{L}_{0}\tilde{L}_{0}\tilde{L}_{0} \tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}}\Big{)}^{1/3}\Big{\}}\) and consider the following two possible cases:

* "Small \(K\)" case: if \(\eta=\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}}\leq\Big{(}\frac{b^{3}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}}{2n^{2}\tilde{L}_{0}\tilde{L}_{0}\tilde{L}_{0}\|\bm{y}_{*} \|_{\bm{\Lambda}^{-1}}^{2}}\Big{)}^{1/3}\), we have \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{ \eta^{2}n}{b^{2}}\hat{L}_{0}\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}\] \[\leq\frac{\sqrt{\tilde{L}_{0}\tilde{L}_{0}}}{\sqrt{2}K}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}+\frac{\hat{L}_{0}^{1/3}\tilde{L}_{0}^{1/3}\|\bm{y}_{*}\|_{ \bm{\Lambda}^{-1}}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{2^{2/3}n^{1/3}K^{2/3}}.\]* "Large \(K\)" case: if \(\eta=\left(\frac{b^{3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{2n^{2}L_{0}L_{0}K\|\bm{y} _{*}\|_{\bm{\Lambda}^{-1}}^{2}}\right)^{1/3}\leq\frac{b}{\sqrt{2L_{0}L_{0}}}\), we have \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{\eta ^{2}n}{b^{2}}\hat{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}^{2}\] \[\leq\frac{2^{1/3}\hat{L}_{0}^{1/3}\tilde{L}_{0}^{1/3}\|\bm{y}_{
* }\|_{\bm{\Lambda}^{-1}}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{n^{1/3}K^{2/ 3}}.\] Combining these two cases, we have \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{\sqrt{\hat{L}_{0}\tilde{L}_{0}}}{\sqrt{2}K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{2^{1/3}\hat{L}_{0}^{1/3}\tilde{L}_{0}^{1/3}\|\bm{ y}_{*}\|_{\bm{\Lambda}^{-1}}^{2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{n^{1/3}K^{2/ 3}}.\] Hence, to guarantee \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) for \(\epsilon>0\), the total number of individual gradient evaluations will be \[nK\geq\max\Big{\{}\frac{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}\|\bm{x}_{0}-\bm{x} _{*}\|_{2}^{2}}{\epsilon},\frac{4n^{1/2}\tilde{L}_{0}^{1/2}\tilde{L}_{0}^{1/2} \|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^ {3/2}}\Big{\}}.\] (38) When \(\frac{(n-b)^{2}}{n}\sigma_{*}^{2}\leq\tilde{L}_{0}\|\bm{y}_{*}\|_{\bm{\Lambda} ^{-1}}^{2}\), we set \(\eta=\min\Big{\{}\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}},\,\Big{(}\frac{ b^{3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{2n(n-b)^{2}L_{0}K\sigma_{*}^{2}} \Big{)}^{1/3}\Big{\}}\) and consider the two cases as below:
* "Small \(K\)" case: if \(\eta=\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}}\leq\Big{(}\frac{b^{3}\|\bm{ x}_{0}-\bm{x}_{*}\|_{2}^{2}}{2n(n-b)^{2}\tilde{L}_{0}K\sigma_{*}^{2}}\Big{)}^{1/3}\), we have \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{b}{2n\eta K}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\frac{\eta ^{2}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*}^{2}\] \[\leq\frac{\sqrt{\tilde{L}_{0}\tilde{L}_{0}}}{\sqrt{2}K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{(n-b)^{2/3}\tilde{L}_{0}^{1/3}\sigma_{*}^{2/3}\| \bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{2^{2/3}n^{2/3}K^{2/3}}.\]
* "Large \(K\)" case: if \(\eta=\Big{(}\frac{b^{3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{2n(n-b)^{2}\tilde{L} _{0}K\sigma_{*}^{2}}\Big{)}^{1/3}\leq\frac{b}{n\sqrt{2\tilde{L}_{0}\tilde{L}_ {0}}}\), we have \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{\sqrt{\tilde{L}_{0}\tilde{L}_{0}}}{\sqrt{2}K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{\eta^{2}(n-b)^{2}}{b^{2}}\tilde{L}_{0}\sigma_{*} ^{2}\] \[\leq\frac{2^{1/3}(n-b)^{2/3}\tilde{L}_{0}^{1/3}\sigma_{*}^{2/3}\| \bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.\] Combining these two cases, we obtain \[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}) \leq\frac{\sqrt{\tilde{L}_{0}\tilde{L}_{0}}}{\sqrt{2}K}\|\bm{x}_{0 }-\bm{x}_{*}\|_{2}^{2}+\frac{2^{1/3}(n-b)^{2/3}\tilde{L}_{0}^{1/3}\sigma_{*}^{ 2/3}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.\] To guarantee \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) for \(\epsilon>0\), the total number of individual gradient evaluations will be \[nK\geq\max\Big{\{}\frac{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}\|\bm{x}_{0}-\bm{x} _{*}\|_{2}^{2}}{\epsilon},\frac{4(n-b)\tilde{L}_{0}^{1/2}\sigma_{*}\|\bm{x}_{0}- \bm{x}_{*}\|_{2}^{2}}{\epsilon^{3/2}}\Big{\}}.\] (39) Combining Eq. (38) and Eq. (39), we finally have \[nK \geq\frac{n\sqrt{2\tilde{L}_{0}\tilde{L}_{0}}\|\bm{x}_{0}-\bm{x}_{*} \|_{2}^{2}}{\epsilon}\] \[\quad+\min\Big{\{}\frac{4n^{1/2}\tilde{L}_{0}^{1/2}\tilde{L}_{0}^{ 1/2}\|\bm{y}_{*}\|_{\bm{\Lambda}^{-1}}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{ \epsilon^{3/2}},\frac{4(n-b)\tilde{L}_{0}^{1/2}\sigma_{*}\|\bm{x}_{0}-\bm{x}_{*} \|_{2}^{2}}{\epsilon^{3/2}}\Big{\}},\] thus finishing the proof.
Omitted Proofs for Non-Smooth Convex Setting From Section 3

Before we prove Theorem 3 in convex Lipschitz settings, for completeness, we first recall the following standard first-order characterization of convexity.

**Lemma 13**.: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be a continuous convex function. Then, for any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\):_

\[f(\bm{y})\geq f(\bm{x})+\left\langle\bm{g}_{\bm{x}},\bm{y}-\bm{x}\right\rangle,\]

_where \(\bm{g}_{\bm{x}}\in\partial f(\bm{x})\), and \(\partial f(\bm{x})\) is the subdifferential of \(f\) at \(\bm{x}\)._

The following technical lemma provides a primal-dual gap bound in convex nonsmooth settings.

**Lemma 14**.: _For any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 2 satisfy_

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\left(\bm{y}_{k}^{(i)\top} \bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i+1})+(\bm{v}_{k}^{(i)}-\bm{y}_{k}^{(i )})^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\right)\] (40) \[\quad-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1} \|_{2}^{2},\]

_where \(\mathcal{E}_{k}:=\eta_{k}\big{(}\mathcal{L}(\bm{x}_{k},\bm{v})-\mathcal{L}( \bm{x}_{*},\bm{y}_{*})\big{)}+\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}- \frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2}^{2}\)._

Proof.: By the same argument as in the proof for Lemma 9, we know that \(\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}\in\partial\ell_{\pi_{j}^{(k)}}^{*} (\bm{y}_{k}^{j})\) for \(b(i-1)+1\leq j\leq bi\), then by Lemma 13 we have

\[\ell_{\pi_{j}^{(k)}}^{*}\big{(}\bm{v}_{k}^{j}\big{)}\geq\ell_{\pi_{j}^{(k)}}^{ *}(\bm{y}_{k}^{j})+\bm{a}_{\pi_{j}^{(k)}}^{\top}\bm{x}_{k-1,i}\big{(}\bm{v}_{k} ^{j}-\bm{y}_{k}^{j}\big{)},\]

which leads to

\[\mathcal{L}(\bm{x}_{k},\bm{v}) =\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^ {(i)}\bm{x}_{k-1,i}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*(\bm{v}_{k}^{ j})}\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{x}_{k} -\bm{x}_{k-1,i})\] (41) \[\leq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k} ^{(i)}\bm{x}_{k-1,i}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*(\bm{y}_{k}^ {j})}\Big{)}+\frac{1}{n}\sum_{i=1}^{m}\bm{v}_{k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{ x}_{k}-\bm{x}_{k-1,i}).\]

Using the same argument for \(\mathcal{L}(\bm{x}_{*},\bm{y}_{*})\) as \(\bm{a}_{j}^{\top}\bm{x}_{*}\in\partial\ell_{j}^{*}(\bm{y}_{k}^{j})\) for \(j\in[n]\), we have

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) =\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{*,k}^{(i)\top}\bm{A}_{k} ^{(i)}\bm{x}_{*}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*(\bm{y}_{*,k}^{j}) }\Big{)}\] (42) \[\geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k }^{(i)}\bm{x}_{*}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*(\bm{y}_{k}^{j}) }\Big{)}.\]

Adding and substracting the term \(\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}\) on the R.H.S. of Eq. (42), we obtain

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) \geq\frac{1}{n}\sum_{i=1}^{m}\Big{(}\bm{y}_{k}^{(i)\top}\bm{A}_{k }^{(i)}\bm{x}_{*}+\frac{b}{2\eta_{k}}\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}- \sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*(\bm{y}_{k}^{j})}\Big{)}\] \[\quad-\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\|\bm{x}_{*}-\bm{x}_{k-1, i}\|_{2}^{2}.\]

Denote \(\phi_{k}^{(i)}(\bm{x}):=\bm{y}_{k}^{(i)\top}\bm{A}_{k}^{(i)}\bm{x}+\frac{b}{2\eta_{k}}\| \bm{x}-\bm{x}_{k-1,i}\|_{2}^{2}\), which is \(\frac{b}{\eta_{k}}\)-strongly convex w.r.t. \(\bm{x}\). Noticing that \(\bm{x}_{k-1,i+1}=\arg\min_{\bm{x}\in\mathbb{R}^{d}}\left\{\bm{y}_{k}^{(i)\top} \bm{A}_{k}^{(i)}\bm{x}+\frac{b}{2\eta_{k}}\|\bm{x}-\bm{x}_{k-1,i}\|^{2}\right\}\) by Line 7 of Alg. 2, we have \(\nabla\phi_{k}^{(i)}(\bm{x}_{k-1,i+1})=\bm{0}\), which leads to

\[\phi_{k}^{(i)}(\bm{x}_{*})\geq\phi_{k}^{(i)}(\bm{x}_{k-1,i+1})+\frac{b}{2\eta_{k }}\|\bm{x}_{*}-\bm{x}_{k-1,i+1}\|_{2}^{2}.\]Thus, we obtain

\[\mathcal{L}(\bm{x}_{*},\bm{y}_{*}) \geq\frac{1}{n}\sum_{i=1}^{m}\left(\bm{y}_{k}^{(i)\top}\bm{A}_{k}^{( i)}\bm{x}_{k-1,i+1}+\frac{b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}-\bm{x}_{k-1,i}\|_{2}^{2}- \sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y}_{k}^{j})\right)\] \[\quad+\frac{b}{2n\eta_{k}}\sum_{i=1}^{m}\left(\|\bm{x}_{*}-\bm{x}_ {k-1,i+1}\|_{2}^{2}-\|\bm{x}_{*}-\bm{x}_{k-1,i}\|_{2}^{2}\right)\] \[\overset{(i)}{=}\frac{1}{n}\sum_{i=1}^{m}\left(\bm{y}_{k}^{(i) \top}\bm{A}_{k}^{(i)}\bm{x}_{k-1,i+1}+\frac{b}{2\eta_{k}}\|\bm{x}_{k-1,i+1}- \bm{x}_{k-1,i}\|_{2}^{2}-\sum_{j=b(i-1)+1}^{bi}\ell_{\pi_{j}^{(k)}}^{*}(\bm{y} _{k}^{j})\right)\] \[\quad+\frac{b}{2n\eta_{k}}\|\bm{x}_{k}-\bm{x}_{*}\|_{2}^{2}- \frac{b}{2n\eta_{k}}\|\bm{x}_{k-1}-\bm{x}_{*}\|_{2}^{2},\] (43)

where \((i)\) is by telescoping \(\sum_{i=1}^{m}\left(\|\bm{x}_{*}-\bm{x}_{k-1,i+1}\|_{2}^{2}-\|\bm{x}_{*}-\bm{x }_{k-1,i}\|_{2}^{2}\right)\) and using \(\bm{x}_{k}=\bm{x}_{k-1,m+1}\) and \(\bm{x}_{k-1}=\bm{x}_{k-1,1}\), which both hold by definition.

Combining the bounds from Eq. (41) and Eq. (43), and denoting

\[\mathcal{E}_{k}:=\eta_{k}\big{(}\mathcal{L}(\bm{x}_{k},\bm{v})-\mathcal{L}(\bm {x}_{*},\bm{y}_{*})\big{)}+\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac {b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2}^{2},\]

we finally obtain

\[\mathcal{E}_{k} \leq\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k} ^{(i)}(\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1})+\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{v} _{k}^{(i)\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\| _{2}^{2}\] \[=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\bm{y}_{k}^{(i)\top}\bm{A}_{k}^{( i)}(\bm{x}_{k}-\bm{x}_{k-1,i+1})+\frac{\eta_{k}}{n}\sum_{i=1}^{m}(\bm{v}_{k}^{(i) }-\bm{y}_{k}^{(i)})^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\] \[\quad-\frac{b}{2n}\sum_{i=1}^{m}\|\bm{x}_{k-1,i}-\bm{x}_{k-1,i+1}\| _{2}^{2},\]

thus completing the proof. 

Note that we can still use Lemma 10 to bound the first inner product term in Eq. (40), as we are studying the same algorithm. The following lemma provides a bound on the second inner product term \(\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\left(\bm{v}_{k}^{(i)}-\bm{y }_{k}^{(i)}\right)^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})\) in Eq. (40).

**Lemma 15**.: _Under Assumption 5, for any \(k\in[K]\), the iterates \(\{\bm{y}_{k}^{(i)}\}_{i=1}^{m}\) and \(\{\bm{x}_{k-1,i}\}_{i=1}^{m+1}\) generated by Algorithm 2 satisfy_

\[\mathcal{T}_{2}\leq\frac{\eta_{k}^{2}\sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{ (k)}}}}{b}\|\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}+\frac{\eta_{k}^{2} \sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{(k)}}}}{4b}\|\bm{v}_{k}-\bm{y}_{k}\|_ {\mathbf{\Gamma}_{k}^{-1}}^{2}.\] (44)

Proof.: Proceeding as in Lemma 11, we have

\[\mathcal{T}_{2}:=\frac{\eta_{k}}{n}\sum_{i=1}^{m}\left(\bm{v}_{k}^{(i)}-\bm{y }_{k}^{(i)}\right)^{\top}\bm{A}_{k}^{(i)}(\bm{x}_{k}-\bm{x}_{k-1,i})=-\frac{ \eta_{k}^{2}}{bn}\sum_{i=1}^{m}\left\langle\bm{A}_{k}^{\top}\bm{I}_{b(i-1)\top} \bm{y}_{k},\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v}_{k}-\bm{y}_{k})\right\rangle.\]

Using Young's inequality for some \(\alpha>0\) and proceeding as in Eq. (24), we obtain

\[\mathcal{T}_{2} \leq\frac{\eta_{k}^{2}\alpha}{2bn}\sum_{i=1}^{m}\|\bm{A}_{k}^{\top }\bm{I}_{b(i-1)\top}\bm{y}_{k}\|_{2}^{2}+\frac{\eta_{k}^{2}}{2bn\alpha}\sum_{i= 1}^{m}\|\bm{A}_{k}^{\top}\bm{I}_{(i)}(\bm{v}_{k}-\bm{y}_{k})\|_{2}^{2}\] \[\leq\frac{\eta_{k}^{2}n\alpha}{2b^{2}}\hat{G}_{\pi^{(k)}}\|\bm{y}_ {k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}+\frac{\eta_{k}^{2}}{2n\alpha}\tilde{G}_{\pi^{ (k)}}\|\bm{v}_{k}-\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2},\]where we use our definitions that \(\hat{G}_{\pi^{(k)}}:=\frac{1}{m}\big{\|}\mathbf{\Gamma}_{k}^{1/2}\big{(}\sum_{j=1}^ {m}\bm{I}_{b(j-1)\uparrow}\bm{A}_{k}\bm{A}_{k}^{\top}\bm{I}_{b(j-1)\uparrow}\big{)} \mathbf{\Gamma}_{k}^{1/2}\big{\|}_{2}\) and \(\tilde{G}_{\pi^{(k)}}:=\frac{1}{b}\big{\|}\mathbf{\Gamma}_{k}^{1/2}\big{(} \sum_{j=1}^{m}\bm{I}_{(j)}\bm{A}_{k}\bm{A}_{k}^{\top}\bm{I}_{(j)}\big{)}\mathbf{ \Gamma}_{k}^{1/2}\big{\|}_{2}\). It remains to choose \(\alpha=\frac{2b}{n}\sqrt{\frac{\tilde{G}_{k}}{G_{k}}}\) to finish the proof. 

We are now ready to prove Theorem 3 for the convergence of shuffled SGD in the convex nonsmooth Lipschitz settings.

**Theorem 3**.: _Under Assumption 5, if \(H_{K}=\sum_{k=1}^{K}\eta_{k}\) and \(\bar{G}=\mathbb{E}_{\pi}[\sqrt{\hat{G}_{\pi}\hat{G}_{\pi}}]\), the output \(\hat{\bm{x}}_{K}\) of Alg. 1 with possible uniformly random shuffling satisfies_

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{1}{2n}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}2\eta_{k}^{2}n\bar{G},\]

_As a result, for any \(\epsilon>0,\) there exists a step size \(\eta_{k}=\eta\) such that \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\) after \(\mathcal{O}\big{(}\frac{n\bar{G}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{2 }}\big{)}\) individual gradient queries._

Proof.: To simplify the presentation of our analysis, we first assume \(\|\bm{v}\|_{\mathbf{\Gamma}^{-1}}^{2}\leq n\), which will be later verified by our choice of \(\bm{v}=\bm{y}_{\hat{\bm{x}}_{K}}\) and Assumption 5.

Combining the bounds in Lemma 10 and 15 and plugging them into Eq. (40), we have

\[\mathcal{E}_{k} \leq\frac{\eta_{k}^{2}\sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{( k)}}}}{b}\|\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}+\frac{\eta_{k}^{2}\sqrt{ \hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{(k)}}}}{4b}\|\bm{v}_{k}-\bm{y}_{k}\|_{ \mathbf{\Gamma}_{k}^{-1}}^{2}\] \[\overset{(i)}{\leq}\frac{\eta_{k}^{2}\sqrt{\hat{G}_{\pi^{(k)}} \tilde{G}_{\pi^{(k)}}}}{b}\|\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}+\frac{ \eta_{k}^{2}\sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{(k)}}}}{2b}(\|\bm{v}\|_{ \mathbf{\Gamma}_{1}^{-1}}^{2}+\|\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2})\] \[\overset{(ii)}{\leq}\frac{2\eta_{k}^{2}n\sqrt{\hat{G}_{\pi^{(k)}} \tilde{G}_{\pi^{(k)}}}}{b},\] (45)

where we use Young's inequality for \(\|\bm{v}_{k}-\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}\) and \(\|\bm{v}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}=\|\bm{v}\|_{\mathbf{\Gamma}^{-1}}^{2}\) as \(\bm{v}\) is a fixed vector for \((i)\), and \((ii)\) is due to \(\|\bm{y}_{k}\|_{\mathbf{\Gamma}_{k}^{-1}}^{2}\leq n\) by Assumption 5 and assuming that \(\|\bm{v}\|_{\mathbf{\Gamma}^{-1}}^{2}\leq n\). Proceeding as the proof for Theorem 2, we first assume the RR scheme and take conditional expectation w.r.t. the randomness up to but not including \(k\)-th epoch, then we obtain

\[\mathbb{E}_{k}[\mathcal{E}_{k}]\leq\frac{2\eta_{k}^{2}n\mathbb{E}_{k}\big{[} \sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{(k)}}}\big{]}}{b}.\]

Since the randomness only comes from the random permutation \(\pi^{(k)}\), we have

\[\mathbb{E}_{k}[\mathcal{E}_{k}]\leq\frac{2\eta_{k}^{2}n\mathbb{E}_{\pi}[\sqrt{ \hat{G}_{\pi}\tilde{G}_{\pi}}]}{b}.\]

For notational convenience, we denote \(\bar{G}=\mathbb{E}_{\pi}[\sqrt{\hat{G}_{\pi}\tilde{G}_{\pi}}]\), and further take expectation w.r.t. all the randomness on both sides and use the law of total expectation to obtain

\[\mathbb{E}[\mathcal{E}_{k}]\leq\frac{2\eta_{k}^{2}n\bar{G}}{b}.\] (46)

For the SO scheme, there is one random permutation \(\pi\) generated at the very beginning such that \(\pi^{(k)}=\pi\) for all \(k\in[K]\). So we can directly take expectation w.r.t. all the randomness on both sides of Eq. (45), with the randomness only from \(\pi\), which leads to the same bound as Eq. (46) with \(\mathbb{E}\big{[}\sqrt{\hat{G}_{\pi^{(k)}}\tilde{G}_{\pi^{(k)}}}\big{]}=\mathbb{E }_{\pi}\big{[}\sqrt{\hat{G}_{\pi}\tilde{G}_{\pi}}\big{]}\). Note that for incremental gradient (IG) descent, we can let \(\tilde{G}=\sqrt{\hat{G}_{0}\tilde{G}_{0}}\) without randomness involved, where \(\hat{G}_{0}=\hat{G}_{\pi^{(0)}}\) and \(\tilde{G}_{0}=\tilde{G}_{\pi^{(0)}}\) w.r.t. the initial, fixed permutation \(\pi^{(0)}\) of the data matrix \(\bm{A}\).

Noticing that \(\mathcal{E}_{k}=\eta_{k}\text{Gap}^{\text{v}}(\bm{x}_{k},\bm{y}_{*})+\frac{b}{2n} \|\bm{x}_{*}-\bm{x}_{k}\|_{2}^{2}-\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{k-1}\|_{2}^{2}\) and telescoping from \(k=1\) to \(K\), we have

\[\mathbb{E}\Big{[}\sum_{k=1}^{K}\eta_{k}\text{Gap}^{\text{v}}(\bm{x}_{k},\bm{y}_ {*})\Big{]}\leq\frac{b}{2n}\|\bm{x}_{*}-\bm{x}_{0}\|_{2}^{2}-\frac{b}{2n} \mathbb{E}[\|\bm{x}_{*}-\bm{x}_{K}\|_{2}^{2}]+\sum_{k=1}^{K}\frac{2\eta_{k}^{2 }n\bar{G}}{b}.\]

Noticing that \(\mathcal{L}(\bm{x},\bm{v})\) is convex wrt \(\bm{x}\), we have \(\text{Gap}^{\text{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\leq\sum_{k=1}^{K}\eta_{k} \text{Gap}^{\text{v}}(\bm{x}_{k},\bm{y}_{*})/H_{K}\), where \(\hat{\bm{x}}_{K}=\sum_{k=1}^{K}\eta_{k}\bm{x}_{k}/H_{K}\) and \(H_{K}=\sum_{k=1}^{K}\eta_{k}\), so we obtain

\[\mathbb{E}\Big{[}H_{K}\text{Gap}^{\text{v}}(\hat{\bm{x}}_{K},\bm{y}_{*})\Big{]} \leq\frac{b}{2n}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{2\eta_{ k}^{2}n\bar{G}}{b}.\]

Further choosing \(\bm{v}=\bm{y}_{\hat{\bm{x}}_{K}}\), which also verifies \(\|\bm{v}\|_{\tilde{\bm{\Gamma}}^{-1}}^{2}=\|\bm{y}_{\hat{\bm{x}}_{K}}\|_{ \tilde{\bm{\Gamma}}^{-1}}^{2}\leq n\) by Assumption 5, we obtain

\[\mathbb{E}[H_{K}(f(\hat{\bm{x}}_{K})-f(\bm{x}_{*}))]\leq\frac{b}{2n}\|\bm{x}_ {0}-\bm{x}_{*}\|_{2}^{2}+\sum_{k=1}^{K}\frac{2\eta_{k}^{2}n\bar{G}}{b}.\]

To analyze the individual gradient oracle complexity, we choose constant stepsize \(\eta\). Then, the above bound becomes

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\frac{b}{2n\eta K}\|\bm{x}_{ 0}-\bm{x}_{*}\|_{2}^{2}+\frac{2n\eta\bar{G}}{b}.\]

Choosing \(\eta=\frac{b\|\bm{x}_{0}-\bm{x}_{*}\|_{2}}{2n\sqrt{K}\bar{G}}\), we have

\[\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\frac{2\sqrt{\bar{G}}\|\bm{x} _{0}-\bm{x}_{*}\|_{2}}{\sqrt{\bar{K}}}.\]

Hence, given \(\epsilon>0\), to ensure \(\mathbb{E}[f(\hat{\bm{x}}_{K})-f(\bm{x}_{*})]\leq\epsilon\), the total number of individual gradient evaluations will be

\[nK\geq\frac{4n\bar{G}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{2}},\]

thus completing the proof. 

We now briefly discuss this result. The total number of individual gradient queries is \(\mathcal{O}\big{(}\frac{n\bar{G}\|\bm{x}_{0}-\bm{x}_{*}\|_{2}^{2}}{\epsilon^{2}} \big{)}\), which appears independent of the batch size, but this is actually not the case, as the parameter \(\bar{G}=\mathbb{E}_{\pi}[\sqrt{\bar{G}_{\pi}\bar{G}_{\pi}}]\) depends on the block partitioning, due to Eq. (4). When \(b=n\), as a sanity check, we recover the standard guarantee of (full) subgradient descent, which is expected, as in this case shuffled SGD reduces to subgradient descent. When \(b=1\), however, the bound is worse than the corresponding bound for standard SGD, by a factor \(\mathcal{O}(n\bar{G}/G^{2})\). By a similar sequence of inequalities as in Eq. (4), this factor is never worse than \(n\), but it is typically much smaller, taking values as small as 1. We note that it is not known whether a better bound is possible for shuffled SGD in this setting, as the only seemingly tighter upper bound from [42] applies only for constant \(K\), when \(n=\Omega(\frac{1}{\epsilon^{2}})\), and under an additional boundedness assumption for the algorithm iterates.

## Appendix E Experiment Details

We implement the computation of \(\hat{L}\) and \(L_{\max}\) in Julia, a high-performance scientific computation programming language, and compute matrix operator norms using the default settings in the Julia Arpack Package. However, limited by computational memory and time constraint, our selection of datasets is focused on moderately large-scale datasets of \(n\) in the order of \(O(10^{5})\). We also include comparisons of small datasets such as a1 and sonar.

### Evaluations of \(L_{\max}/\tilde{L}_{\pi}\) on Synthetic Gaussian Datasets

We first study the gap between \(\tilde{L}_{\pi}\) and \(L_{\max}\) for different batch sizes \(b\), as shown in Figure 2. As in Section 4.1, we focus on their dependence on the data matrix, and assume that the loss functions \(\ell_{i}\) all have the same smoothness constant. In this case, the ratio \(L_{\max}/\tilde{L}_{\pi}\) that characterizes the gap between \(\tilde{L}_{\pi}\) and \(L_{\max}\) will become \(L_{\max}/\tilde{L}_{\pi}=(\max_{1\leq i\leq n}\{\|\bm{a}_{2}\|_{2}^{2})/\big{(} \frac{1}{b}\|\sum_{j=1}^{m}\bm{I}_{(j)}\bm{A}_{\pi}\bm{A}_{\pi}^{\top}\bm{I}_{(j )}\|_{2}\big{)}\). In particular, we run experiments on standard Gaussian data of size \((n,d)\). We fix the dimension \(d=500\), and vary the number of samples with \(n=100,500,1000,2000\). In Figure 2, we plot the ratio \(L_{\max}/\tilde{L}_{\pi}\) versus the batch size \(b\) for \(100\) different random permutations \(\pi\), where the dotted lines represent the mean values and the filled regions indicate the standard deviation of permutations. We observe that the ratio \(L_{\max}/\tilde{L}_{\pi}\) is concentrated around its empirical mean and exhibits \(b^{\alpha}\) (\(\alpha\in[0.74,0.87]\)) growth as the batch size \(b\) increases. In particular, if we choose \(b=\sqrt{n}\), the ratio can be \(\mathcal{O}(n^{0.4})\).

### Distributions of \(L_{\max}/\hat{L}_{\pi}\)

In this subsection, we include histograms in Figure 3 to illustrate the spread of \(L_{\max}/\hat{L}_{\pi}\) with respect to random permutations, for completeness. We observe that in all the examples \(L_{\max}/\hat{L}_{\pi}\) is concentrated around its empirical mean. The following plots are normalized, with y-axis representing the empirical probability density. The x-axis represents \(L_{\max}/\hat{L}_{\pi}\).

Figure 2: Illustrations of \(L_{\max}/\tilde{L}_{\pi}\) for different batch size \(b\) on synthetic Gaussian data of size \((n,d)\).

Figure 3: Visualization of the empirical distributions of \(L/\hat{L}\) for \(15\) large-scale datasets.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction clearly state the scope of our work and contributions, see Section 1.2. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, see detailed discussion in the introdcution. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We state our assumptions in sections 2 and 3, and the proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We fully disclose the public datasets and tools we use for the numerical computations in Section 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: N/A Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We list the details of our experiments in Section 4.1 and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We use various ways, including ribbon plots and histograms, to illustrate the variance of our numerically computed values. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We list the details of all computational tools in Section 4.1 and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is a primarily theoretical work and we conform to the rules with NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use public benchmarking datasets from LIBSVM [15], MNIST [17], CIFAR10 [22], and Broad Bioimage Benchmark Collection [28], and have properly cited and credited the asset's creators. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.