# Generalizability of experimental studies

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Experimental studies are a cornerstone of machine learning (ML) research. A common, but often implicit, assumption is that the results of a study will generalize beyond the study itself, e.g. to new data. That is, there is a high probability that repeating the study under different conditions will yield similar results. Despite the importance of the concept, the problem of measuring generalizability remains open. This is probably due to the lack of a mathematical formalization of experimental studies. In this paper, we propose such a formalization and develop a quantifiable notion of generalizability. This notion allows to explore the generalizability of existing studies and to estimate the number of experiments needed to achieve the generalizability of new studies. To demonstrate its usefulness, we apply it to two recently published benchmarks to discern generalizable and non-generalizable results. We also publish a Python module that allows our analysis to be repeated for other experimental studies.

## 1 Introduction

Due to the importance of experimental studies, the machine learning (ML) community advocates for high methodological standards [20; 12; 13; 17; 8; 31; 32; 44]. Failure to meet these standards can have significant consequences, such as the ongoing reproducibility crisis [6; 47; 50; 51; 30].

Reproducibility is not the only desirable property of a study. For example, the reader expects that the best encoders of categorical features identified in [41] will not only remain the best when the study is reproduced, but will also outperform their competitors on new datasets. This property of getting the same results from different data is known as _replicability_[46; 48]. Replicability is a special case of _generalizability_, the property of obtaining the same results with any change in the inputs. The assumption of generalizability is arguably the main motivation for extensive experimental studies and benchmarks. However, existing definitions of generalizability do not _quantify_ how well the results of a study can be transferred to other contexts. This hinders the usefulness of such studies and leads to confusion. For example, articles [38; 41; 49; 42] and [19; 8; 11; 13; 29; 43] report that the results of experimental studies are often contradictory.

Quantifying generalizability can also help determine the appropriate size of experimental studies. For example, one dataset is unlikely to be sufficient to draw far-reaching conclusions, but \(10^{6}\) datasets are likely enough. Of course, such large studies are usually not practical: it is crucial to determine the minimum amount of data needed to achieve generalizability. This principle also applies to decisions other than the number of datasets, such as the choice of quality metric and the initialization seed.

A notion similar to generalizability is model replicability [1; 22; 23; 24; 33; 36; 37]. A model is \(\rho\)-replicable if, given i.i.d. samples from the same data distribution, the trained models are the same with probability \(1-\rho\)[33]. Adapting this definition to quantify generalizability is not trivial, as it requires formalizing experimental studies. The latter must take into account several aspects: the research question, the results of a study, and how to compare the results. Regarding the problem ofdefining the size of experimental studies, the current literature addresses the (crucial, but orthogonal) problem of choosing appropriate experimental factors [20; 12; 13; 17; 8; 31; 32; 44]. While these studies recommend varying the factors, they do not help decide how many of the factor levels are enough.

Our contributions are as follows:

1. we formalize experimental studies and their results;
2. we propose a quantifiable definition of the generalizability of experimental studies;
3. we develop an algorithm to estimate the size of a study to obtain generalizable results;
4. we consider two recent experimental studies on categorical encoders [41] and Large Language Models [55] and show how their results may or may not be generalizable.
5. we will publish the genexpy1 Python module to repeat our analysis in other studies.

Footnote 1: https://anonymous.4open.science/r/genexpy-B94D

Paper outline: Section 2 is related work, Section 3 formalizes experimental studies, Section 4 defines generalizability and provides the algorithm to estimate the required size of a study for generalizability, Section 5 contains the case studies, Section 6 describes limitations and concludes.

## 2 Related work

We first discuss the literature related to the motivation we are tackling, i.e., why experimental studies may not generalize. Second, we overview the existing concept of model replicability, closely related to our work. Finally, we show other meanings that these words can assume in other domains.

Non-generalizable results.It is well known that experimental results can significantly vary based on design choices [38; 41; 49; 42]. Possible reasons include an insufficient number of datasets [19; 41; 3; 12] as well as differences in hyperparameter tuning [13; 41], initialization seed [30], and hardware [56]. As a result, the statistical benchmarking literature advocates for experimenters to motivate their design choices [7; 43; 11; 13; 44] and clearly state the conclusions they are attempting to draw from their study [7; 45].

Replicability and generalizability in ML.Our work formalizes the definitions of replicability and generalizability given in [48; 46]. Intuitively, replicable work consists of repeating an experiment on different data, while generalizable work varies other factors as well -- e.g., quality metric, implementation. A recent line of work, initiated by [33], has linked replicability to model stability: a \(\rho\)-replicable model learns (with probability \(1-\rho\)) the same parameters from different i.i.d. samples. This definition has later been adapted and applied to other learning algorithms [23], clustering [24], reinforcement learning [22; 37], convex optimization [1], and learning rules [36]. Recent efforts have been bridging the gap between replicability, differential privacy, generalization error, and global stability [15; 16; 26; 45; 21]. However, these applications remain limited to model replicability.

Replicability and generalizability in Science.In other fields of Science, generalizability and replicability take different meanings. In social sciences, generalizability theory is a tool to quantify the effect of different factors on numerical responses [14]. In medicine, the replicability proposed in [34] is the probability of observing a positive treatment effect in a meta-study. Although these concepts are related to generalizability of experimental studies, they are limited to purely numerical responses or specific study designs.

## 3 Experiments and experimental studies

An _experimental study_ is a set of _experiments_ comparing the same _alternatives_ under different _experimental conditions_. An experimental condition is a tuple of _levels_ of _experimental factors_, the parameters defining the experiments. Different factors play different roles in the study: the _design_ and _held-constant_ factors are fixed by design, while the generalizability of a study is defined in terms of the _allowed-to-vary_ factors. The study aims at answering a _research question_, which defines its _scope_ and _goals_.

_Example 3.1_.: (The "checkmate-in-one" task, cf. Figure 1) An experimenter wants to compare three Large Language Models (LLMs), the _alternatives_, on the "checkmate-in-one" task [55, 2, 5, 4, 18]. The assignment is to find the unique checkmating move from a position of pieces on a chessboard: an LLM succeeds if and only if it outputs the correct move. The experimenter considers two _experimental factors_: the number of shots, \(n\), and the initial position on the chessboard, \(\text{pos}_{l}\). The number of shots is a _design factor_, while the initial position is an _allowed-to-vary_ factor. The experimenter wants to find if \(\text{LLM}_{1}\) ranks consistently against the other two LLMs when changing the initial position, for a fixed number of shots.

The rest of this section defines the terms introduced above.

### Experiments

An experiment evaluates all the considered _alternatives_ under a _valid experimental condition_.

Alternatives.An alternative \(a\in A\) is an object compared in the study, like an LLM in Example 3.1. Here, \(A\) is the set of alternatives considered in the study, with cardinality \(n_{a}\).

Experimental factors.An experimental factor is anything that could, in principle, affect the result of an experiment. \(i\) denotes a factor, \(C_{i}\) the (possibly infinite) set of _levels_\(i\) can take, \(c\in C_{i}\) a level of \(i\), and \(I\) the set of all factors. We adapt Montgomery's classification of experimental factors [44, Chapter 1] and discern between _design factors_, _held-constant factors_, and _allowed-to-vary factors_.

* _Design factors_, e.g., whether and how to tune the hyperparameters, quality metrics, number of shots, are chosen by the experimenter.
* _Held-constant factors_, e.g., implementation, initialization seed, number of cross-validated folds, may affect the outcome but are not in the scope of the experiment and are fixed by the experimenter.
* _Allowed-to-vary factors_, e.g., "dataset" or "chesboard position" in Example 3.1, may affect the outcome but cannot be held constant: the experimenter expects results to generalize w.r.t. these factors; \(\text{\sc{adv}}\) denotes them.

Experimental conditions.An _experimental condition_\(\mathbf{c}\) is a tuple of levels of experimental factors, \(\mathbf{c}=\left(c_{i}\right)_{i\in I}\in C\subseteq\prod_{i\in I}C_{i}\). We endow \(C\) with a probability \(\mu\), as we will need to sample from it to define the result of a study in Section 3. The probability space \(\left(C,\mathcal{F},\mu\right)\) is the _universe of valid experimental conditions_. \(C\) may not coincide with \(\prod_{i\in I}C_{i}\) as some experimental conditions may be _invalid_, i.e., illegal or not of interest. Validity has to be assessed on a case-by-case basis. For instance, in Example 3.1, \(C=\left\{\left(\text{pos}_{l},n\right)\right\}_{l,n}\), where \(\text{pos}_{l}\) is a legal configuration of pieces on a chessboard and \(m\) is the non-negative number of shots.

Experimental results.The _experiment function_\(E\) evaluates the alternatives \(A\) under a valid experimental condition \(\mathbf{c}\in C\). Unless necessary, we consider \(A\) fixed and omit it in our notation. We require that \(E:C\rightarrow\mathcal{R}_{n_{a}}\) is a measurable function, for some fixed \(A\). Finally, the _result_ of an experiment \(E\left(A,\mathbf{c}\right)\) is a ranking on \(A\).

Figure 1: Two empirical studies on the checkmate-in-one task, cf. Example 3.1.

**Definition 3.1** (Ranking (with ties)).: A ranking \(r\) on \(A\) is a transitive and reflexive binary endorelation on \(A\). Equivalently, \(r\) is a totally ordered partition of \(A\) into _tiers_ of equivalent alternatives. \(r(a)\) denotes the _rank_ of \(a\in A\), i.e., the position of the tier of \(a\) in the ordering. W.l.o.g. \((\mathcal{R}_{n_{a}},\mathcal{P}(\mathcal{R}_{n_{a}}))\) denotes the measure space of all rankings of \(n_{a}\) objects, where \(\mathcal{P}\) indicates the power set.

_Example 3.1_ (Continued).: The result of an experiment on \((\text{pos}_{l},n)\) is a ranking of the three LLMs, according to whether or not they output the checkmating move. Suppose that only LLM\({}_{1}\) and LLM\({}_{2}\) output the correct move. Then \(E(\text{pos}_{l},n)\) ranks LLM\({}_{1}\) and LLM\({}_{2}\) tied as best and LLM\({}_{3}\) as worst.

### Experimental studies

A study is defined by its _research question_\(\mathcal{Q}\), i.e., its _scope_ and _goals_. The _scope_ consists of the alternatives \(A\), the valid experimental conditions \(C\), and the allowed-to-vary factors \(I_{\text{av}}\). The _goal_ is the kind of conclusions one is attempting to draw from the study. For now, the goal is a statement of interests, i.e., a set of strings.

**Definition 3.2** (Research question).: The research question \(\mathcal{Q}=(A,C,I_{\text{av}},\text{goals})\) is a tuple containing the set of alternatives \(A\), the experimental conditions \(C\), the set of allowed-to-vary-factors \(I_{\text{av}}\), and the goals of the study.

_Example 3.1_ (Continued).: The research question of the "checkmate-in-one" study is as follows. The _scope_ is \(\left(A=\left\{\text{LLM}_{a}\right\}_{a=1,2,3},C=\left\{(\text{pos}_{l},n) \right\}_{l,n},I_{\text{av}}=\left\{\text{``position''}\right\})\right)\). The _goal_ is "Does LLM\({}_{1}\) rank consistently against the other LLMs?"

A crucial element of our formalization is the distinction between _ideal_ and _empirical_ studies. An ideal study exhausts its research question; however, its result is not observable. An empirical study is an observable sample of an ideal study.

#### 3.2.1 Ideal studies

The _ideal study_ on a research question \(\mathcal{Q}=(A,C,I_{\text{av}},\text{goals})\) is the experimental study consisting of an experiment for each valid experimental condition \(\mathbf{c}\in C\). We say that such a study exhausts \(\mathcal{Q}\). Hence, there exists exactly one ideal study on \(\mathcal{Q}\). The _result_ of an ideal study is the probability distribution of the results of its experiments. Recall that the experiment function \(E:\,(C,\mathcal{F},\mu)\,\rightarrow\,(\mathcal{R}_{n_{a}},\mathcal{P}\,( \mathcal{R}_{n_{a}}))\) is measurable.

**Definition 3.3** (Result of an ideal study).: The _result of an ideal study_ with research question \(\mathcal{Q}=(A,C,I_{\text{av}},\text{goals})\) is

\[S\left(\mathcal{Q}\right)=\mathbb{P}:\mathcal{R}_{n_{a}} \rightarrow[0,1]\] \[r \mapsto\mathbb{P}\left(r\right)\coloneqq\mu\left(E^{-1}(r)\right),\]

where \(E^{-1}(r)=\left\{\mathbf{c}:E(\mathbf{c})=r\right\}\subseteq C\) is the preimage of \(r\) through \(E\).

In general, multiple experiments of a study may yield identical results. Definition 3.3 supports this by assigning a higher probability mass to results that occur more often.

#### 3.2.2 Empirical studies

Consider again a research question \(\mathcal{Q}=(A,C,I_{\text{av}},\text{goals})\). In practice, as \(C\) might be infinite or too large, one can only run experiments on a sample of valid experimental conditions \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\overset{\text{iid}}{\sim}(C,\mu)\). The study performed on \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\) is _an empirical study_ on \(\mathcal{Q}\), of _size_\(N\). As for ideal studies, the result of an empirical study is the probability distribution of the results of its experiments.

**Definition 3.4** (Result of an empirical study).: The _result of an empirical study_ on \(\mathcal{Q}\) is

\[\hat{S}_{N}\left(\mathcal{Q}\right):\mathcal{R}_{n_{a}} \rightarrow[0,1]\] \[r \mapsto\#\left\{j\in\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}:E\left( A,\mathbf{c}_{j}\right)=r\right\}.\]

Where \(\mathcal{Q}\), \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\) is a research question and a set of valid experimental conditions as above.

The result of an empirical study can be thought of as the empirical distribution of a sample following the distribution of the result of the corresponding ideal study. With a slight abuse of notation, indicating both the sample and its empirical distribution as \(\hat{S}_{N}\left(\mathcal{Q}\right)\), we write

\[\hat{S}_{N}\left(\mathcal{Q}\right)\stackrel{{\mathrm{iid}}}{{ \sim}}S\left(\mathcal{Q}\right).\]

## 4 Generalizability of experimental studies

The currently accepted definition of generalizability is the property of two independent studies with the same research question to yield similar results [46, 48]. Although intuitive, this notion is not directly applicable as it does not provide a way to measure the generalizability of a study. We now introduce a quantifiable notion of generalizability of experimental studies, as the probability that any two empirical studies approximating the same ideal study yield similar results.

**Definition 4.1** (Generalizability).: Let \(\mathcal{Q}=\left(A,C,I_{\text{div}},\kappa\right)\) be the research question of an ideal study, let \(\mathbb{P}=S(\mathcal{Q})\) be the result of that study, and let \(d\) be some distance between probability distributions. The _generalizability of the ideal study on \(\mathcal{Q}\)_ is

\[\text{Gen}\left(\mathcal{Q};\varepsilon,n\right):=\mathbb{P}^{n}\otimes \mathbb{P}^{n}\left(\left(X_{j},Y_{j}\right)_{j=1}^{n}:d(X,Y)\leq\varepsilon \right),\] (1)

where \(\varepsilon\in\mathbb{R}^{+}\) is a _similarity threshold_.

As the result of an ideal study is usually unobservable (cf. Section 3.2), we do not know the true distribution \(\mathbb{P}\). However, we can observe the result of an empirical study, \(\hat{\mathbb{P}}_{N}=\hat{S}_{N}\left(\mathcal{Q}\right)\), which approximates \(\mathbb{P}\) under the assumption that the experimental conditions are i.i.d. samples from \(C\). As the sample size \(N\) increases (the empirical study becomes larger), \(\hat{\mathbb{P}}_{N}\) converges in distribution to \(\mathbb{P}\).

Definition (1) requires a distance \(d\) between probability distributions. In the next sections, we propose to use a generalizability based on kernels and Maximum Mean Discrepancy (MMD) [27], as it allows to compute generalizability w.r.t. different research questions. The underlying idea is that we can capture the goal of a study with an appropriate kernel. We conclude this section with an algorithm to estimate the number of experimental conditions required to obtain generalizable results.

### Similarity between rankings -- kernels

Whether two experimental results (i.e., rankings) are similar or not ultimately depends on the goal of the study. For instance, consider two rankings on \(A=\left\{a_{1},a_{2},a_{3}\right\}\), \(\mathbf{r}=\left(1,2,3\right)\) and \(\mathbf{r}^{\prime}=\left(1,3,2\right)\), where \(r_{i}\) is the tier of alternative \(a_{i}\). The conclusions drawn from \(r\) and \(r^{\prime}\) are identical if one's goal is to find the best alternative, but very different if one's goal is to obtain an ordering of the alternatives. One can use kernels to quantify the similarity between experimental results. Kernels are suitable to formalize the aspects of the result of a study one wants to generalize, i.e., the goals of the study. For instance, one kernel is suitable to identify the best tier while another kernel focuses on the position of a specific alternative. In the following, we describe three representative kernels that cover a wide spectrum of possible goals.

Borda kernel.The Borda kernel is suitable for goals in the form "Is the alternative \(a^{*}\) consistently ranked the same?". It uses the Borda count: the number of alternatives (weakly) dominated by a given one [9]. For a pair of rankings, we compute the Borda counts of \(a^{*}\), and then take their difference.

\[{{\kappa}_{b}^{a^{*},\nu}}\left(r_{1},r_{2}\right)=e^{-\nu|b_{1}-b_{2}|},\]

where \(b_{l}=\left\{a\in A:r_{l}(a)\geq r_{l}(a^{*})\right\}\) is the number of alternatives dominated by \(a^{*}\) in \(r_{l}\) and \(\nu\in\mathbb{R}\) is the kernel bandwidth. The Borda kernel takes values in \(\left[e^{(-\nu n_{a})},1\right]\). If \(\nu\) is too large compared to \(\nicefrac{{1}}{{\left|b_{1}-b_{2}\right|}}\), the kernel is oversensitive and will penalize every deviation too much. On the contrary, if \(\nu\) is too small, the kernel is undersensitive and will not penalize deviations unless they are very large. As \(|b_{1}-b_{2}|\in[0,n_{a}]\), we recommend \(\nu=\nicefrac{{1}}{{n_{a}}}\).

Jaccard kernel.The Jaccard kernel is suitable for goals in the form "Are the best alternatives consistently the same ones?". As it measures the similarity between sets [25, 10], we use it to compare the top-\(k\) tiers of two rankings.

\[\kappa_{j}^{k}\left(r_{1},r_{2}\right)=\frac{\left|r_{1}^{-1}([k])\cap r_{2}^{-1 }([k])\right|}{\left|r_{1}^{-1}([k])\cup r_{2}^{-1}([k])\right|},\]

where \(r^{-1}([k])=\{a\in A:r_{1}(a)\leq k\}\) is the set of alternatives whose rank is better than or equal to \(k\). The Jaccard kernel takes values in \([0,1]\).

Mallows kernel.The Mallows kernel is suitable for goals in the form "Are the alternatives ranked consistently?". It measures the overall similarity between rankings [35, 40, 39]. We adapt the original definition in [39] for ties,

\[\kappa_{m}^{\nu}(r_{1},r_{2})=e^{-\nu n_{d}},\]

where \(n_{d}=\sum_{a_{1},a_{2}\in A}|\operatorname{sign}\left(r_{1}(a_{1})-r_{1}(a_ {2})\right)-\operatorname{sign}\left(r_{2}(a_{1})-r_{2}(a_{2})\right)|\) is the number of discordant pairs and \(\nu\in\mathbb{R}\) is the kernel bandwidth. If a pair is tied in one ranking but not in the other, one counts it as half a discordant pair. The Mallows kernel takes values in \(\left[\exp\left(-2\nu\binom{n_{d}}{2}\right),1\right]\). If \(\nu\) is too large compared to \(\nicefrac{{1}}{{n_{d}}}\), the kernel is oversensitive and it will penalize every deviation too much. On the contrary, if \(\nu\) is too small, the kernel is undersensitive and will not penalize deviations unless they are very large. As \(n_{d}\in\left[0,\binom{n_{d}}{2}\right]\), we recommend \(\nu=\nicefrac{{1}}{{\binom{n_{d}}{2}}}\).

### Distance between distributions -- Maximum Mean Discrepancy

Having sorted out how to measure the similarity between the results of experiments, we now discuss how to measure the distance between the results of studies. We chose the Maximum Mean Discrepancy (MMD) [27], for the following reasons. First, MMD is compatible with the kernels described in Section 4.1, i.e., it takes into consideration the goal of the studies. Second, it handles sparse distributions well; this is needed as empirical studies are typically small compared to the number of all possible rankings, which grows exponentially in the number of alternatives. 2 Finally, it comes with bounds and theoretical guarantees, which we will use in Section 4.3.

Footnote 2: Fubini or ordered Bell numbers, OEIS sequence A000670.

**Definition 4.2** (MMD (empirical distributions)).: Let \(X\) be a set with a kernel \(\kappa\), and let \(\mathbb{Q}_{1}\) and \(\mathbb{Q}_{2}\) be two probability distributions on \(\mathcal{R}_{n_{a}}\). Let \(\mathbf{x}=\left(x_{i}\right)_{i=1}^{n},\mathbf{y}=\left(y_{i}\right)_{i=1}^{m}\) be two i.i.d. samples from \(\mathbb{Q}_{1}\) and \(\mathbb{Q}_{2}\) respectively. Then,

\[\text{MMD}\left(\mathbf{x},\mathbf{y}\right)^{2}\coloneqq\frac{1}{n^{2}}\sum _{i,j=1}^{n}\kappa(x_{i},x_{j})+\frac{1}{m^{2}}\sum_{i,j=1}^{m}\kappa(y_{i},y_ {j})-\frac{2}{mn}\sum_{\begin{subarray}{c}i=1\ldots n\\ j=1\ldots m\end{subarray}}\kappa(x_{i},y_{j}).\]

**Proposition 4.1**.: _MMD takes values in \(\left[0,\sqrt{2\cdot\left(\kappa_{\text{sup}}-\kappa_{\text{inf}}\right)}\right]\), where \(\kappa_{\text{sup}}=\sup_{x,y\in X}\kappa(x,y)\) and \(\kappa_{\text{inf}}=\inf_{x,y\in X}\kappa(x,y)\)._

### How many experiments ensure generalizability?

When designing a study, an experimenter has to decide how many experiments to run in order to obtain generalizable results. In other words, they need to choose a (minimum) sample size \(n^{*}\) that achieves the desired generalizability \(\alpha^{*}\) and the desired similarity \(\varepsilon^{*}\).

\[n^{*}=\min\left\{n\in\mathbb{N}_{0}:\text{Gen}\left(\mathbb{P};\varepsilon^{*},n\right)\geq\alpha^{*}\right\}.\] (2)

To estimate \(n^{*}\) we make use of a linear dependency between the logarithms of the sample size \(n\) and the logarithm of the \(\alpha^{*}\)-quantile of MMD \(\varepsilon_{n}^{\alpha^{*}}\) that we have observed in our experiments.

**Proposition 4.2**.: \(\forall\alpha^{*}\)_, there exist \(\beta_{0}\geq 0,\beta_{1}\leq 0\) s.t._

\[\log(n)\approx\beta_{1}\log\left(\varepsilon_{n}^{\alpha^{*}}\right)+\beta_{0}\] (3)

Appendix A.3.2 provides a proof for a simplified case. Proposition 4.2 suggests that one can use a small set of \(N\) preliminary experiments to estimate \(n^{*}\). One can then iteratively improve that estimate with the results of additional experiments.

Our algorithm, shown in detail in Appendix A.3.3, requires specifying the desired generalizability, \(\alpha^{*}\), and the similarity threshold between the studies results, \(\varepsilon^{*}\). Then, it performs the following steps:1. it estimates the \(\alpha^{*}\)-quantile of MMD for all \(n\) less than some budget \(n_{\text{max}}\). If there exists an \(n\) less than \(n_{\text{max}}\) that satisfies the condition in (2), we return it as \(n^{*}\);
2. it then fits the linear model in (3), computing the coefficients \(\beta_{0}\) and \(\beta_{1}\);
3. finally, it outputs \(n^{*}=\exp\left(\beta_{1}\log\left(\varepsilon_{n}^{\alpha^{*}}\right)+\beta_ {0}\right)\), which satisfies the condition in (2) thanks to Proposition 4.2.

In practice, choosing \(\varepsilon^{*}\) is hardly interpretable as it is a threshold on MMD. To solve this, we propose choosing \(\varepsilon^{*}\) as a function of another parameter \(\delta^{*}\), such that

\[\varepsilon^{*}(\delta^{*})=\sqrt{2(\kappa_{\text{sup}}-f_{\kappa}(\delta^{* }))}.\]

Here, \(\delta^{*}\) represents the distance between two rankings as computed by the kernel and \(f_{\kappa}\) is the function linking the distance to the kernel value. For instance, for the Jaccard kernel, \(\delta^{*}\) is simply the Jaccard coefficient between the top-\(k\) tiers of two rankings, \(f_{\kappa}(\delta^{*})=1-\delta^{*}\), and \(\varepsilon^{*}(\delta^{*})=\sqrt{2(1-(1-\delta^{*}))}\). For the Mallows kernel (with our recommendation for \(\nu\)), \(\delta^{*}\) is the fraction of discordant pairs, \(f_{\kappa}(x)=e^{-x}\), and \(\varepsilon^{*}(\delta^{*})=\sqrt{2(1-e^{-\delta^{*}})}\). As a concrete example, achieving \((\alpha^{*}=0.99,\delta^{*}=0.05)\)-generalizable results for the Jaccard kernel means that, with probability \(0.99\), the average Jaccard coefficient between two rankings drawn from the results is \(0.95\).

## 5 Case studies

### Case Study 1: A benchmark of categorical encoders

We now evaluate the generalizability of a recent study [41] that analyzes the performance of encoders for categorical data. The performance of an encoder is approximated by the quality of a model trained on the encoded data. The _design factors_ are the model, the tuning strategy for the pipeline, and the quality metric for the model, while the only _allowed-to-vary factor_ is the dataset. We impute missing values in the results of the study by assigning the worst rank. We evaluate how well the results of the study generalize w.r.t. three goals:

1. Find out if One-Hot encoder (a popular encoder) ranks consistently amongst its competitors, using the Borda kernel with \(\nu=\nicefrac{{1}}{{n_{a}}}\).
2. Investigate if some encoders outperform all the others using the Jaccard kernel with \(k=1\).
3. Evaluate whether the encoders are typically ranked in a similar order, using the Mallows kernel with \(\nu=\nicefrac{{1}}{{\left(\frac{n_{a}}{2}\right)}}\).

Figure 2 shows the predicted \(n^{*}\) for different choices of \(\alpha^{*}\) and \(\delta^{*}\), the other one fixed at \(0.95\) and \(0.05\) respectively. The variance in the boxes comes from variance in the design factors. For example, the results for the design factors "decision tree, full tuning, accuracy" have a different \((\alpha^{*},\delta^{*})\)-generalizability than the results for "SVM, no tuning, accuracy". We observe on the left that -- as expected -- obtaining generalizable results requires more experiments as the desired

Figure 2: Predicted \(n^{*}\) for categorical encoders.

generalizability \(\alpha^{*}\) increases. We can also see that the variance of the boxes increases with \(\alpha^{*}\). This means that the choice of the design factors has a larger influence on the achieved generalizability. We observe the same when decreasing \(\delta^{*}\), as it corresponds to a stricter similarity condition on the rankings. In the rather extreme cases of \(\alpha^{*}=0.7\) or \(\delta^{*}=0.3\), even less than 10 datasets are enough to achieve \((\alpha^{*},\delta^{*})\)-generalizability.

Consider now goal \(g_{2}\) for two different choices of design factors: (A); "decision tree, full tuning, accuracy" and (B): "SVM, full tuning, balanced accuracy". Furthermore, let \((\alpha^{*},\delta^{*})=(0.95,0.05)\): we estimate \(n^{*}=28\) for (A) and \(n^{*}=34\) for (B), corresponding to the bottom and top whiskers of the corresponding box in Figure 2. As both (A) and (B) were evaluated using \(n=30\) experiments, we conclude that the results of (A) are (barely) \((0.95,0.05)\)-generalizable, while those of (B) are not. Hence, one should run more experiments with fixed factors (B) to make the study generalizable.

### Case study 2: BIG-bench -- A benchmark of Large Language Models

We now evaluate the generalizability of BIG-bench [55], a collaborative benchmark of Large Language Models (LLMs). The benchmark compares LLMs on different tasks, such as the checkmate-in-one task (cf. Example 3.1), and for different numbers of shots. Task and number of shots are the _design factors_. Every task has a number of subtasks, which is the _allowed-to-vary factor_. We stick to the preferred scoring for each subtask. As the results have too many missing values to impute them, we only consider the experimental conditions where at least \(80\%\) of the LLMs had results, and to the LLMs whose results cover at least \(80\%\) of the conditions.

Similar to before, we define the three goals as follows:

* Find out if GPT3 (to date, one of the most popular LLMs) ranks consistently amongst its competitors, using the Borda kernel with \(\nu=\nicefrac{{1}}{{n_{a}}}\).
* Investigate if some encoders outperform all the others using the Jaccard kernel with \(k=1\).
* Evaluate whether the LLMs are typically ranked in a similar order, using the Mallows kernel with \(\nu=\nicefrac{{1}}{{\left(\frac{n_{a}}{2}\right)}}\).

Figure 3 shows the predicted \(n^{*}\) for different choices of \(\alpha^{*}\) and \(\delta^{*}\), the other one fixed at \(0.95\) and \(0.05\) respectively. Again, the variance in the boxes comes from variance in the design factors, i.e., the task and the number of shots. As before, increasing \(\alpha^{*}\) or decreasing \(\delta^{*}\) leads to higher \(n^{*}\). Unlike in the previous section, \(n^{*}\) for \(g_{2}\) greatly depends on the combination of fixed factors, as we now detail.

Consider now goal \(g_{2}\) for two different choices of design factors: (A): "conlang_translation, 0 shots", and (B): "arithmetic, 2 shots". Furthermore, let \((\alpha^{*},\delta^{*})=(0.95,0.05)\). For this choice of of parameters, we estimate \(n^{*}=44\) for (A), corresponding to the top whisker of the corresponding box in Figure 2. As the study evaluates (A) on 10 subtasks, it is therefore not \((0.95,0.05)\)-generalizable. In fact, we estimate that this would require 34 more subtasks. For (B), on the other hand, we estimate \(n^{*}=1\): the best 2-shot LLM for the observed subtasks is always PALM 535b. Hence, the result of a single experiment is enough to achieve \((0.95,0.05)\)-generalizability.

Figure 3: Predicted \(n^{*}\) for LLMs.

Note that, although we correctly estimated \(n^{*}=1\) for (B), this estimate relies on 10 preliminary experiments. In other words, our algorithm was able to quantify _in hindsight_ that a single experiment would have been enough to obtain generalizable results. Of course, however, one cannot trust an estimate of \(n^{*}\) based on only one experiment. The next section thus investigates how the number of preliminary experiments influences the estimate of \(n^{*}\).

### How many preliminary experiments?

This section evaluates the influence of the number of preliminary experiments \(N\) on \(n^{*}\). For each study, we consider the design factor combinations for which we have at least \(50\) experiments. This results in 23 out of 48 combinations for the categorical encoders and 9 out of 24 combinations for the LLMs. For each of those combinations, we consider the estimate \(n^{*}_{50}\) made at \(N=50\) as the ground truth and observe how the estimates of \(n^{*}\) for \(N<50\) differ. Figure 4 shows the relative error \(\nicefrac{{|n^{*}_{N}-n^{*}_{50}|}}{{n^{*}_{50}}}\), for different goals: the relative errors behave very differently. For goal \(g_{3}\) (Mallows kernel), even \(n^{*}_{10}\) is close to \(n^{*}_{50}\) for a majority of the design factor combinations. On the contrary, one needs 20 to 30 preliminary experiments for goal \(g_{1}\) (Borda kernel). This means that knowing the goals of a study when performing preliminary experiments can help understand how trustworthy the estimate of \(n^{*}\) is.

## 6 Conclusion

Limitations & future work.First, we dealt with experimental results as rankings. Other forms of results, e.g., the absolute performance of alternatives according to some quality measure, will require the development of appropriate kernels. Second, our approach uses kernels to compute the similarity of experimental results and MMD the distance between the results of studies. There are, however. other possible choices. Third, we processed missing evaluations by either dropping them or imputing them. One could analyze different solutions, for instance by adapting the kernels to missing values. Fourth, we estimate the distribution of the MMD by sampling multiple times from the results. A non-asymptotic theory of MMD, at least for some kernels, might yield more insights in improving this procedure. Fifth, we plan to investigate the possibility of actively selecting experiments to obtain good estimates of the required size \(n^{*}\) with less preliminary experiments. Sixth and related to the previous one, we intend to obtain some guarantees on the convergence of \(n^{*}\) to the true value.

Conclusions.An experimental study is generalizable if, with high probability, its findings will hold under different experimental conditions, e.g., on unseen datasets. Non-generalizable studies might be of limited use or even misleading. This study is, to our knowledge, the first to develop a quantifiable notion for the generalizability of experimental studies. To achieve this, we formalize experiments, experimental studies and their results -- rankings and distributions over rankings. Our approach allows us to estimate the number of experiments needed to achieve a desired level of generalizability in new experimental studies. We demonstrate its utility showing generalizable and non-generalizable results in two recent experimental studies.

Figure 4: Relative error in the estimate of \(n^{*}\) against \(n^{*}_{50}\).

## Acknowledgments

* [1] Kwangjun Ahn et al. "Reproducibility in Optimization: Theoretical Framework and Limits". In: _NeurIPS_. 2022.
* [2] Scott Alexander. "A very unlikely chess game, 2020". In: _URL https://slatestarcodex. com/2020/01/06/a-very-unlikely-chesgame/.(cited on pp. 29 and 30)_ ().
* [3] Maxime Alvarez et al. "A Revealing Large-Scale Evaluation of Unsupervised Anomaly Detection Algorithms". In: _CoRR_ abs/2204.09825 (2022).
* [4] Prithviraj Ammanabrolu et al. "Bringing stories alive: Generating interactive fiction worlds". In: _Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment_. Vol. 16. 1. 2020, pp. 3-9.
* [5] Prithviraj Ammanabrolu et al. "Toward automated quest generation in text-adventure games". In: _arXiv preprint arXiv:1909.06283_ (2019).
* [6] Monya Baker. "1,500 scientists lift the lid on reproducibility". In: _Nature_ 533.7604 (2016).
* [7] Thomas Bartz-Beielstein et al. "Benchmarking in Optimization: Best Practice and Open Issues". In: _CoRR_ abs/2007.03488 (2020).
* [8] Alessio Benavoli et al. "Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis". In: _J. Mach. Learn. Res._ 18 (2017), 77:1-77:36.
* [9] JC de Borda. "M'emoire sur les' elections au scrutin". In: _Histoire de l'Acad'emie Royale des Sciences_ (1781).
* [10] Mathieu Bouchard, Anne-Laure Jousselme, and Pierre-Emmanuel Dore. "A proof for the positive definiteness of the Jaccard index matrix". In: _International Journal of Approximate Reasoning_ 54.5 (2013), pp. 615-626.
* [11] Anne-Laure Boulesteix, Rory Wilson, and Alexander Hapfelmeier. "Towards evidence-based computational statistics: lessons from clinical research on the role and design of real-data benchmark studies". In: _BMC Medical Research Methodology_ 17 (2017), pp. 1-12.
* [12] Anne-Laure Boulesteix et al. "A statistical framework for hypothesis testing in real data comparison studies". In: _The American Statistician_ 69.3 (2015), pp. 201-212.
* [13] Xavier Bouthillier et al. "Accounting for Variance in Machine Learning Benchmarks". In: _MLSys_. mlsys.org, 2021.
* [14] Robert L Brennan. "Generalizability theory". In: _Educational Measurement: Issues and Practice_ 11.4 (1992), pp. 27-34.
* [15] Mark Bun et al. "Stability Is Stable: Connections between Replicability, Privacy, and Adaptive Generalization". In: _STOC_. ACM, 2023, pp. 520-527.
* [16] Zachary Chase, Shay Moran, and Amir Yehudayoff. "Stability and Replicability in Learning". In: _FOCS_. IEEE, 2023, pp. 2430-2439.
* [17] Giorgio Corani et al. "Statistical comparison of classifiers through Bayesian hierarchical modelling". In: _Mach. Learn._ 106.11 (2017), pp. 1817-1837.
* [18] Sahith Dambekodi et al. "Playing text-based games with common sense". In: _arXiv preprint arXiv:2012.02757_ (2020).
* [19] Mostafa Dehghani et al. "The Benchmark Lottery". In: _CoRR_ abs/2107.07002 (2021).
* [20] Janez Demsar. "Statistical Comparisons of Classifiers over Multiple Data Sets". In: _J. Mach. Learn. Res._ 7 (2006), pp. 1-30.
* [21] Peter Dixon et al. "List and Certificate Complexities in Replicable Learning". In: _NeurIPS_. 2023.
* [22] Eric Eaton et al. "Replicable Reinforcement Learning". In: _NeurIPS_. 2023.
* [23] Hossein Esfandiari et al. "Replicable Bandits". In: _ICLR_. OpenReview.net, 2023.
* [24] Hossein Esfandiari et al. "Replicable Clustering". In: _NeurIPS_. 2023.
* [25] Thomas Gartner, Quoc Viet Le, and Alex J Smola. "A short tour of kernel methods for graphs". In: _Under Preparation_ (2006).

* [26] Badih Ghazi et al. "On User-Level Private Convex Optimization". In: _ICML_. Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 11283-11299.
* [27] Arthur Gretton et al. "A Kernel Method for the Two-Sample-Problem". In: _NIPS_. MIT Press, 2006, pp. 513-520.
* [28] Arthur Gretton et al. "A Kernel Two-Sample Test". In: _J. Mach. Learn. Res._ 13 (2012), pp. 723-773.
* [29] Odd Erik Gundersen, Kevin L. Coakley, and Christine R. Kirkpatrick. "Sources of Irreproducibility in Machine Learning: A Review". In: _CoRR_ abs/2204.07610 (2022).
* [30] Odd Erik Gundersen et al. "On Reporting Robust and Trustworthy Conclusions from Model Comparison Studies Involving Neural Networks and Randomness". In: _ACM-REP_. ACM, 2023, pp. 37-61.
* [31] Torsten Hothorn et al. "The design and analysis of benchmark experiments". In: _Journal of Computational and Graphical Statistics_ 14.3 (2005), pp. 675-699.
* [32] Karl Huppler. "The Art of Building a Good Benchmark". In: _TPCTC_. Vol. 5895. Lecture Notes in Computer Science. Springer, 2009, pp. 18-30.
* [33] Russell Impagliazzo et al. "Reproducibility in learning". In: _STOC_. ACM, 2022, pp. 818-831.
* [34] Iman Jaljuli et al. "Quantifying replicability and consistency in systematic reviews". In: _Statistics in Biopharmaceutical Research_ 15.2 (2023), pp. 372-385.
* [35] Yunlong Jiao and Jean-Philippe Vert. "The Kendall and Mallows Kernels for Permutations". In: _IEEE Trans. Pattern Anal. Mach. Intell._ 40.7 (2018), pp. 1755-1769.
* [36] Alkis Kalavasis et al. "Statistical Indistinguishability of Learning Algorithms". In: _ICML_. Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 15586-15622.
* [37] Amin Karbasi et al. "Replicability in Reinforcement Learning". In: _NeurIPS_. 2023.
* [38] Fred Lu, Edward Raff, and James Holt. "A Coreset Learning Reality Check". In: _AAAI_. AAAI Press, 2023, pp. 8940-8948.
* [39] Colin L Mallows. "Non-null ranking models. I". In: _Biometrika_ 44.1/2 (1957), pp. 114-130.
* [40] Horia Mania et al. "On kernel methods for covariates that are rankings". In: (2018).
* [41] Federico Matteucci, Vadim Arzamasov, and Klemens Bohm. "A benchmark of categorical encoders for binary classification". In: _NeurIPS_. 2023.
* [42] Duncan C. McElfresh et al. "On the Generalizability and Predictability of Recommender Systems". In: _NeurIPS_. 2022.
* [43] Iven Van Mechelen et al. "A white paper on good research practices in benchmarking: The case of cluster analysis". In: _WIREs Data. Mining. Knowl. Discov._ 13.6 (2023).
* [44] Douglas C Montgomery. _Design and analysis of experiments_. John wiley & sons, 2017.
* [45] Shay Moran, Hilla Schefler, and Jonathan Shafer. "The Bayesian Stability Zoo". In: _NeurIPS_. 2023.
* [46] Engineering National Academies of Sciences, Medicine, et al. _Reproducibility and replicability in science_. National Academies Press, 2019.
* [47] Roger D Peng. "Reproducible research in computational science". In: _Science_ 334.6060 (2011), pp. 1226-1227.
* [48] Joelle Pineau et al. "Improving Reproducibility in Machine Learning Research(A Report from the NeurIPS 2019 Reproducibility Program)". In: _J. Mach. Learn. Res._ 22 (2021), 164:1-164:20.
* [49] Zhen Qin et al. "RD-Suite: A Benchmark for Ranking Distillation". In: _NeurIPS_. 2023.
* [50] Edward Raff. "Does the Market of Citations Reward Reproducible Work?" In: _ACM-REP_. ACM, 2023, pp. 89-96.
* [51] Edward Raff. "Research Reproducibility as a Survival Analysis". In: _AAAI_. AAAI Press, 2021, pp. 469-478.
* [52] Isaac J Schoenberg. "Metric spaces and positive definite functions". In: _Transactions of the American Mathematical Society_ 44.3 (1938), pp. 522-536.
* [53] Bernhard Scholkopf. "The kernel trick for distances". In: _Advances in neural information processing systems_ 13 (2000).
* [54] J Laurie Snell and John G Kemeny. "Mathematical models in the social sciences". In: _(No Title)_ (1962).

* [55] Aarohi Srivastava et al. "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models". In: _CoRR_ abs/2206.04615 (2022).
* [56] Donglin Zhuang et al. "Randomness in Neural Network Training: Characterizing the Impact of Tooling". In: _MLSys_. mlsys.org, 2022.

Details for Section 4

### Details for Section 4.1

This section contains the proofs to show that the similarities introduced in Section 4.1 are kernels, i.e., symmetric and positive definite functions. As symmetry is a clear property of all of them, we only discuss their positive definiteness. Our proofs for the Borda and Mallows kernels follow that in [35]: we define a distance \(d\) on the set of rankings \(\mathcal{R}_{n_{a}}\) and show that \((\mathcal{R}_{n_{a}},d)\) is isometric to an \(L_{2}\) space. This ensures that \(d\) is a conditionally positive definite (c.p.d.) function and, thus, that \(e^{-\nu d}\) is positive definite [52, 53]. Our proof for the Jaccard kernel, instead, follows without much effort from previous results. For ease of reading, we restate the definitions as well.

**Definition A.1** (Borda kernel).: \[\kappa_{b}^{a^{*},\nu}\left(r_{1},r_{2}\right)=e^{-\nu|d_{1}-d_{2}|},\] (4)

where \(d_{l}=\{a\in A:r_{l}(a)\geq r_{l}(a^{*})\}\) is the number of alternatives dominated by \(a^{*}\) in \(r_{l}\) and \(\nu\in\mathbb{R}\).

**Proposition A.1**.: _The Borda kernel as defined in (4) is a kernel._

Proof.: Define a distance

\[d:\mathcal{R}_{n_{a}}\times\mathcal{R}_{n_{a}} \rightarrow\mathbb{R}^{+}\] \[\left(r_{1},r_{2}\right) \mapsto\left|d_{1},d_{2}\right|,\]

where \(d_{l}=\{a\in A:r_{l}(a)\geq r_{l}(a^{*})\}\) is the number of alternatives dominated by \(a^{*}\) in \(r_{l}\). Now, \((\mathcal{R}_{n_{a}},d)\) is isometric to \((\mathbb{R},\left\lVert\cdot\right\rVert_{2})\) via the map \(r_{l}\mapsto d_{l}\). Hence, \(d\) is c.p.d. and \(\kappa_{b}\) is a kernel. 

**Definition A.2** (Jaccard kernel).: \[\kappa_{j}^{k}\left(r_{1},r_{2}\right)=\frac{\left|r_{1}^{-1}([k])\cap r_{2}^ {-1}([k])\right|}{\left|r_{1}^{-1}([k])\cup r_{2}^{-1}([k])\right|},\] (5)

where \(r^{-1}([k])=\{a\in A:r_{1}(a)\leq k\}\) is the set of alternatives whose rank is better than or equal to \(k\).

**Proposition A.2**.: _The Jaccard kernel as defined in (5) is a kernel._

Proof.: It is already know that the Jaccard coefficients for sets is a kernel [25, 10]. As the Jaccard kernel for rankings is equivalent to the Jaccard coefficient for the \(k\)-best tiers of said rankings, the former is also a kernel. 

**Definition A.3** (Mallows kernel).: \[\kappa_{m}^{\nu}(r_{1},r_{2})=e^{-\nu n_{d}},\] (6)

where \(n_{d}=\sum_{a_{1},a_{2}\in A}\left|\text{sign}\left(r_{1}(a_{1})-r_{1}(a_{2}) \right)-\text{sign}\left(r_{2}(a_{1})-r_{2}(a_{2})\right)\right|\) is the number of discordant pairs and \(\nu\in\mathbb{R}\) is the kernel bandwidth.

**Proposition A.3**.: _The Mallows kernel as defined in (6) is a kernel._

Proof.: The number of discordant pairs \(n_{d}\) is a distance on \(\mathcal{R}_{n_{a}}\)[54]. Consider now the mapping of a ranking into its adjacency matrix,

\[\Phi:\mathcal{R}_{n_{a}} \rightarrow\{0,1\}^{n_{a}\times n_{a}}\] \[r \mapsto\left(\text{sign}\left(r(i)-r(j)\right)\right)_{i,j=1}^{n_{ a}}.\]

Then,

\[n_{d}=\left\lVert\Phi(r_{1})-\Phi(r_{2})\right\rVert_{1}=\left\lVert\Phi(r_{1})-\Phi(r_{2})\right\rVert_{2}^{2}\]

where \(\left\lVert\cdot\right\rVert_{p}\) indicates the entry-wise matrix \(p\)-norm and the equality holds because the entries of the matrices are either 0 or 1. As a consequence, \((\mathcal{R}_{n_{a}},n_{d})\) is isometric to \((\mathbb{R}^{n_{a}\times n_{a}},\left\lVert\cdot\right\rVert_{2})\) via \(\Phi\). Hence, \(n_{d}\) is c.p.d. and \(\kappa_{m}\) is a kernel.

### Details for Section 4.2

**Proposition 4.1**.: _MMD takes values in \(\left[0,\sqrt{2\cdot(\kappa_{\text{sup}}-\kappa_{\text{inf}})}\right]\), where \(\kappa_{\text{sup}}=\sup_{x,y\in X}\kappa(x,y)\) and \(\kappa_{\text{inf}}=\inf_{x,y\in X}\kappa(x,y)\)._

Proof.: \[0 \leq\text{MMD}_{\kappa}\left(\mathbf{x},\mathbf{y}\right)^{2}= \frac{1}{n^{2}}\sum_{i,j=1}^{n}\kappa(x_{i},x_{j})+\frac{1}{m^{2}}\sum_{i,j=1} ^{m}\kappa(y_{i},y_{j})-\frac{2}{mn}\sum_{\begin{subarray}{c}i=1\dots n\\ j=1\dots m\end{subarray}}\kappa(x_{i},y_{j})\] (7) \[\leq\frac{1}{n^{2}}\sum_{i,j=1}^{n}\kappa_{\text{sup}}+\frac{1}{ m^{2}}\sum_{i,j=1}^{n}\kappa_{\text{sup}}-\frac{2}{mn}\sum_{\begin{subarray}{c}i =1\dots n\\ j=1\dots m\end{subarray}}\kappa_{\text{inf}}\] \[=2(\kappa_{\text{sup}}-\kappa_{\text{inf}})\]

### Details for Section 4.3

#### a.3.1 Choice of \(\alpha^{*}\), \(\varepsilon^{*}\), and \(\delta^{*}\)

Consider a research question \(\mathcal{Q}=(A,C,I_{\text{av}},\kappa)\) and the corresponding ideal study with result \(\mathbb{P}\). The algorithm introduced in Section 4.3 aims at finding the minimum \(n^{*}\) such that, given two independent empirical studies on \(\mathcal{Q}\), they achieve similar results. It has two hyperparameters, \(\alpha^{*}\) and \(\varepsilon^{*}\). \(\alpha^{*}\in[0,1]\) is the generalizability that one wants to achieve from the study, i.e., the probability that two independent realizations of the same ideal study will yield similar results. \(\varepsilon^{*}\in\mathbb{R}^{+}\) is a similarity threshold: the results of two empirical studies \(\mathbf{x},\mathbf{y}\overset{\text{iid}}{\sim}\mathbb{P}\) are similar if \(\text{MMD}_{\kappa}(\mathbf{x},\mathbf{y})\leq\varepsilon^{*}\). However, as it is, \(\varepsilon^{*}\) is not interpretable. Instead, adapting the proof of Proposition 4.1, we can bound MMD by imposing a condition on the kernel, as we'll now illustrate. The key remark is that we are looking for a condition in the form

\[\text{MMD}_{\kappa}\left(\mathbf{x},\mathbf{y}\right)\leq\varepsilon^{*}= \sqrt{2(\kappa_{\text{sup}}-\delta^{\prime})},\]

where \(\delta^{\prime}\in[0,\kappa_{\text{sup}}]\) replaces the third summatory in (7). In other terms, we can interpret \(\delta^{\prime}\) as the minimum acceptable value for the average of the kernel, \(\mathbb{E}_{\mathbb{P}^{2}}\left[\kappa(x,y)\right]\). We now go a step further and compute \(\delta^{\prime}\) (a condition on the kernel) from \(\delta^{*}\in[0,1]\) (a condition on the rankings). The relation between \(\delta^{\prime}\) and \(\delta^{*}\) changes with the kernel, and so does the interpretation of \(\delta^{*}\). For the three kernels we discuss in Section 4.1:

* _Mallows kernel with \(\nu=\nicefrac{{1}}{{\left(\frac{n}{2}\right)}}\)_:_\(\delta^{*}\) is the fraction of discordant pairs, \(\delta^{\prime}=e^{-\delta^{*}}\)_._
* _Jaccard kernel_:_\(\delta^{*}\) is the intersection over union of the top \(k\) tiers, \(\delta^{\prime}=1-\delta^{*}\)_._
* _Borda kernel with \(\nu=\nicefrac{{1}}{{n}}\)_:_\(\delta^{*}\) is the difference in relative position of \(a^{*}\) in the rankings, normalized to the length of the rankings, \(\delta^{\prime}=e^{-\delta^{*}}\)

#### a.3.2 Proof of proposition 4.2

**Proposition 4.2**.: \(\forall\alpha^{*}\)_, there exist \(\beta_{0}\geq 0,\beta_{1}\leq 0\) s.t._

\[\log(n)\approx\beta_{1}\log\left(\varepsilon_{n}^{\alpha^{*}}\right)+\beta_{0}\] (3)Proof.: We provide a proof replacing MMD with the distribution-free bound defined in [28].

\[\mathbb{P}^{n}\otimes\mathbb{P}^{n}\left(\left(X_{j},Y_{j}\right)_{ j=1}^{n}:\text{MMD}(X,Y)-\left(\frac{2\kappa_{\text{sup}}}{n}\right)>\varepsilon \right)<\exp\left(-\frac{n\varepsilon^{2}}{4\kappa_{\text{sup}}}\right)\] \[\overset{\eqref{eq:MMD}}{\Longrightarrow} \mathbb{P}^{n}\otimes\mathbb{P}^{n}\left(\left(X_{j},Y_{j}\right)_ {j=1}^{n}:\text{MMD}(X,Y)>\varepsilon^{\shortmid}\right)<\exp\left(-\frac{n \left(\varepsilon^{\prime}-\left(\frac{2\kappa_{\text{sup}}}{n}\right)\right)^ {2}}{4\kappa_{\text{sup}}}\right)\] \[\overset{\eqref{eq:MMD}}{\Longrightarrow} \mathbb{P}^{n}\otimes\mathbb{P}^{n}\left(\left(X_{j},Y_{j}\right)_ {j=1}^{n}:\text{MMD}(X,Y)>n^{-\frac{1}{2}}\left(\sqrt{-\log\left(1-\alpha \right)4\kappa_{\text{sup}}}\right)+\sqrt{2\kappa_{\text{sup}}}\right)<1-\alpha\] \[\overset{\eqref{eq:MMD}}{\Longrightarrow} \mathbb{P}^{n}\otimes\mathbb{P}^{n}\left(\left(X_{j},Y_{j}\right)_ {j=1}^{n}:\text{MMD}(X,Y)\leq n^{-\frac{1}{2}}\left(\sqrt{-\log\left(1-\alpha \right)4\kappa_{\text{sup}}}\right)+\sqrt{2\kappa_{\text{sup}}}\right)\geq\alpha\]

where:

1. \(\varepsilon^{\prime}=\varepsilon+\sqrt{2\kappa_{\text{sup}}/n}\).
2. \(1-\alpha=\exp\left(-\frac{n\left(\varepsilon^{\prime}-\left(\frac{2\kappa_{ \text{sup}}}{n}\right)\right)^{2}}{4\kappa_{\text{sup}}}\right)\) and \(\varepsilon^{\shortmid}=n^{-\frac{1}{2}}\left(\sqrt{-\log\left(1-\alpha \right)4\kappa_{\text{sup}}}+\sqrt{2\kappa_{\text{sup}}}\right)\).
3. Take the complementary event.

Now,

\[q_{n}^{\alpha}=n^{-\frac{1}{2}}\left(\sqrt{-\log\left(1-\alpha \right)4\kappa_{\text{sup}}}\right)+\sqrt{2\kappa_{\text{sup}}}\] \[\Rightarrow n=\left(q_{n}^{\alpha}\right)^{-2}\left(\sqrt{-4\kappa_{\text{ sup}}\log\left(1-\alpha\right)}+\sqrt{2\kappa_{\text{sup}}}\right)^{2}\] \[\Rightarrow \log(n)=-2\log(q_{n}^{\alpha})+2\log\left(\sqrt{-4\kappa_{\text {sup}}\log\left(1-\alpha\right)}+\sqrt{2\kappa_{\text{sup}}}\right).\]

concluding the proof. 

_Remark_.: Although theoretically sound, using the abovementioned bound instead of MMD leads to excessively conservative estimates for \(n^{*}\), roughly one order of magnitude greater than the empirical estimate.

#### a.3.3 Pseudocode for the algorithm

```
0:\(\alpha^{*}\)\(\triangleright\) desired generalizability
0:\(\delta^{*}\)\(\triangleright\) similarity threshold on rankings
0:\(\mathcal{Q}\)\(\triangleright\) research question, \(\mathcal{Q}=(A,C,I_{\text{atv}},\kappa)\)
0:\(N\)\(\triangleright\) size of preliminary study
0:\(n_{\text{max}}\)\(\triangleright\) maximum sample size to compute MMD
0:\(n_{\text{rep}}\)\(\triangleright\) number of repetitions to compute MMD
0:procedureEstimateNstar(\(\alpha^{*},\delta^{*},\mathcal{Q},N,n_{\text{max}},n_{\text{rep}}\)) \(\varepsilon^{*}\leftarrow\) compute \(\varepsilon^{*}\) from \(\delta^{*}\)\(\triangleright\) cf. Appendix A.3  sample \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\overset{\text{iid}}{\sim}C\)\(n_{\text{max}}\leftarrow\min\left\{n_{\text{max}},\left[N/2\right]\right\}\)\(\triangleright\) we need two disjoint samples of size \(n_{\text{max}}\) from \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\) for\(n=1\ldots n_{\text{max}}\)do \(\text{mmds}\leftarrow\text{empty list}\) for\(n=1\ldots n_{\text{rep}}\)do  sample without replacement \(\left(\mathbf{c}_{j}\right)_{j=1}^{2n_{\text{max}}}\sim\left\{\mathbf{c}_{j} \right\}_{j=1}^{N}\) \(\mathbf{x}\leftarrow\left(\mathbf{c}_{j}\right)_{j=1}^{n_{\text{max}}}\)\(\triangleright\) split the disjoint samples \(\mathbf{y}\leftarrow\left(\mathbf{c}_{j}\right)_{j=n_{\text{max}}}^{2n_{\text{ max}}}\) append \(\text{MMD}\left(\mathbf{x},\mathbf{y}\right)\) to mmds endfor \(\varepsilon_{n}^{\alpha^{*}}\leftarrow\alpha^{*}\)-quantile of mmds endfor  fit a linear regression \(\log(n)=\beta_{1}\log\left(\varepsilon_{n}^{\alpha^{*}}\right)+\beta_{0}\)\(n_{N}^{*}\leftarrow\beta_{1}\log(\varepsilon^{*})+\beta_{0}\) return \(n_{N}^{*}\) endprocedureprocedureRunExperiments(\(\alpha^{*},\delta^{*},\mathcal{Q},n_{\text{max}},n_{\text{rep}},\text{step}\)) \(N\leftarrow\text{step}\) while\(n^{*}>N\)do  sample \(\left\{\mathbf{c}_{j}\right\}_{j=1}^{N}\overset{\text{iid}}{\sim}C\)\(n^{*}\leftarrow\text{EstimateNstar(\alpha^{*},\delta^{*},\mathcal{Q},N,n_{\text{max}},n_{\text{rep}})}\)\(N\gets N+\text{step}\) endwhile endprocedure ```

**Algorithm 1** Compute \(n_{N}^{*}\) from preliminary studyNeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In order, our claims are: the formalization in Section 3; the definition generalizability in Section 4; the algorithm for study size in Section 4.3, the case studies in Section 5, and we provide a link to the anonymized GitHub repository for the module. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The proofs are in the Appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: On GitHub. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [Yes]

Justification: On GitHub.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Section 5. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The boxplots in Section 5 show the variability for the choice of fixed factors. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The analysis we showcase in Section 5 executes very fast, requiring in total less than 4 hours on a standard office laptop. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: To the best of our knowledge, we referenced all sources in the appropriate way. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our Python module is documented on GitHub. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.