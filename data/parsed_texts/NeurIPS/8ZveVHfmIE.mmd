# On the Convergence of Encoder-only Shallow Transformers

 Yongtao Wu

LIONS, EPFL

yongtao.wu@epfl.ch

&Fanghui Liu

University of Warwick

fanghui.liu@warwick.ac.uk

&Grigorios G Chrysos

LIONS, EPFL

University of Wisconsin-Madison

chrysos@wisc.edu

Work done at LIONS, EPFL.

Volkan Cevher

LIONS, EPFL

volkan.cevher@epfl.ch

###### Abstract

In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a _realistic_ setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the _separation_ on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.

## 1 Introduction

Transformers (Vaswani et al., 2017) have demonstrated unparalleled success in influential applications (Devlin et al., 2019; Brown et al., 2020; Wang et al., 2018; Dosovitskiy et al., 2021; Liu et al., 2022). A fundamental theoretical topic concerns the global convergence, i.e., the training dynamics of Transformers, which would be helpful for further analysis, e.g., in-context learning (von Oswald et al., 2022; Akyurek et al., 2023), generalization (Li et al., 2023). In fact, even within a simplified Transformer framework under certain specific regimes, the global convergence guarantees still remain an elusive challenge.

To theoretically understand this, let us first recall the exact format of the self-attention mechanism, the core ingredient of the Transformer. Given the input \(\bm{X}\in\mathbb{R}^{d_{s}\times d}\) (\(d_{s}\) is the number of tokens and \(d\) is the feature dimension of each token), a self-attention mechanism is defined as:

\[\text{Self-attention}(\bm{X})\triangleq\sigma_{s}\left(\tau_{0}(\bm{X}\bm{W} _{Q}^{\top})\left(\bm{X}\bm{W}_{K}^{\top}\right)^{\top}\right)\left(\bm{X} \bm{W}_{V}^{\top}\right)=\sigma_{s}\left(\tau_{0}\bm{X}\bm{W}_{Q}^{\top}\bm{W} _{K}\bm{X}^{\top}\right)\left(\bm{X}\bm{W}_{V}^{\top}\right)\,\]

where \(\sigma_{s}\) is the row-wise softmax function, \(\tau_{0}\in\mathbb{R}^{+}\) is the scaling factor, and the learnable weights are \(\bm{W}_{Q},\bm{W}_{K},\bm{W}_{V}\in\mathbb{R}^{d_{m}\times d}\) with the width \(d_{m}\). Given \(\bm{X}\), the input of softmax depends on \(\tau_{0}\bm{W}_{Q}^{\top}\bm{W}_{K}\), including the scaling factor \(\tau_{0}\) and initialization schemes for learnable parameters, and thus determines the output of softmax and then affects the performance of Transformers in boththeory and practice. There are several scaling schemes in previous literature. For instance, given \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) initialized by standard Gaussian, the scaling factor \(\tau_{0}\) is chosen by

* \(\tau_{0}=d_{m}^{-1/2}\) in the original Transformer (Vaswani et al., 2017): each element in \(\tau_{0}\bm{W}_{Q}^{\top}\bm{W}_{K}\) is a random variable with mean \(0\) and variance \(1\). This scaling avoids the blow-up of value inside softmax as \(d_{m}\) increases (Hron et al., 2020).
* \(\tau_{0}=d_{m}^{-1}\): This scaling stems from the neural tangent kernel (NTK) analysis (Jacot et al., 2018), a commonly used technical tool for convergence analysis of fully-connected (or convolutional) networks under an infinite-width setting \(d_{m}\to\infty\). However, for Transformer, if one uses this scaling under the infinite-width setting, then by the law of large numbers, we have \(\lim_{d_{m}\to\infty}\tau_{0}[\bm{W}_{Q}^{\top}\bm{W}_{K}]^{(ij)}=0\). As a result, the input of softmax is zero and the softmax degenerates to a pooling layer. That means, the non-linearity is missing, which motivates researchers to carefully rethink this setting.

For instance, under the \(\tau_{0}=d_{m}^{-1}\) setting, Yang (2020) use the same query and key matrices to prevent the softmax from degenerating into a pooling layer. Besides, to avoid the analytic difficulty of softmax due to the fact that each element of the output depends on all inputs, Hron et al. (2020) substitute softmax with ReLU under the \(\tau_{0}=d_{m}^{-1/2}\) and infinite width setting for simplicity.

Clearly, there exists a gap between theoretical analysis and practical architectures on the use of softmax, and accordingly, this leads to the following open question:

_How can we ensure the global convergence of Transformers under a realistic setting?_

The primary contribution of this work is to establish the convergence theory of shallow Transformer under a _realistic_ setting. Despite its shallow and encoder-only architecture, our Transformer model captures all the fundamental components found on typical Transformers, including the self-attention mechanism with the softmax activation function, one feedforward ReLU layer, one average pooling layer, and a linear output layer, _cf._ Eq. (1.2). We adopt the \(\tau_{0}=d_{m}^{-1/2}\) scaling under the finite-width setting and compare the results of LeCun/He initializations, which are commonly used in practical applications. Besides, the convergence result under the \(\tau_{0}=d_{m}^{-1}\) setting (as well as the NTK based analysis) is also studied, which facilitates a comprehensive comparison. Our theoretical results demonstrate _notable separations_ among scaling settings, initializations, and architectures as below:

* _Scaling:_ The global convergence can be achieved under both \(\tau_{0}=d_{m}^{-1/2}\) and \(\tau_{0}=d_{m}^{-1}\). Nevertheless, as suggested by our theory: for a small \(d_{m}\), there is no significant difference for these two scaling schemes on the convergence; but for a large enough \(d_{m}\), the \(\tau_{0}=d_{m}^{-1/2}\) scaling admits a faster convergence rate of Transformers than that with \(\tau_{0}=d_{m}^{-1}\). Interestingly, under this \(\tau_{0}=d_{m}^{-1}\) setting, our theory also demonstrates the _separation_ on the convergence result, depending on whether the input is formed along sequence dimension (\(d=1\)) or embedding dimension (\(d_{s}=1\)).
* _Initialization:_ Under LeCun and He initialization, our shallow Transformer admits a faster convergence rate than the NTK initialization. This could be an explanation for the seldom usage of NTK initialization for Transformer training in practice.
* _Architecture:_ Quadratic over-parameterization is enough to ensure the global convergence of our shallow Transformer. As a comparison, if the self-attention mechanism is substituted by a feed-forward ReLU layer, our shallow Transformer is close to a three-layer fully-connected ReLU neural networks to some extent, requiring cubic over-parameterization for global convergence.

We firmly believe that our theoretical analysis takes a significant step towards unraveling the mysteries behind Transformers from the perspective of global convergence. We hope that our analytical framework and insights on various initialization and scaling techniques would be helpful in training modern, large-scale Transformer-based models (Radford et al., 2018; Brown et al., 2020).

## 2 Related work

**Self-attention, Transformer:** Regarding training dynamics, Snell et al. (2021) explain why single-head attention focuses on salient words by analyzing the evolution throughout training. Hron et al. (2020) show that the output of Transformer converges to Gaussian process kernel and provide the NTK formulation of Transformer. Recently, Li et al. (2023) provide sample complexity of shallow Transformer to study its generalization property under a good initialization from pretrained model. The separation between the Transformer and CNN is recently explored. Jelassi et al. (2022) provably demonstrate that Vision Transformer (ViT) has the ability to learn spatial structure without additional inductive bias such as the spatial locality in CNN. Chen et al. (2022) study the loss landscape of ViT and find that ViT converges at sharper local minima than ResNet. Park and Kim (2022) show that ViT is a low-pass filter while CNN is a high-pass filter, thus, these two models can be complementary.

**NTK, lazy training, Hessian:** The NTK was introduced by Jacot et al. (2018) to connect the infinite-width neural network trained by gradient descent and the kernel regression. The roles of such kernel include analysis of the training dynamics of the neural network in the over-parameterization regime (Allen-Zhu et al., 2019; Chizat et al., 2019; Du et al., 2019, 2020; Zou et al., 2020). The global convergence, generalization bound, and memorization capacity largely depend on the minimum eigenvalue of the NTK (Cao and Gu, 2019; Zhu et al., 2022; Nguyen et al., 2021; Bombari et al., 2022). Even though the NTK is extended from FCNN to several typical networks including Transformer (Tirer et al., 2020; Huang et al., 2020; Arora et al., 2019, 2021; Alemohammad et al., 2021; Nguyen and Mondelli, 2020), it has not been used to analyze the global convergence of Transformer. On the other hand, the stability of the tangent kernel during training is required when connecting to kernel regression, but such stability can not be explained by the phenomenon of lazy training (Chizat et al., 2019), which indicates a small change of the parameters from initialization. The hessian spectral bound is the main reason for the stability of kernel, as mentioned in Liu et al. (2020).

**Over-parameterization for convergence analysis:** Due to over-parameterization, neural networks (NNs) can fit arbitrary labels with zero training loss when trained with (stochastic) gradient descent (SGD), both theoretically Li and Liang (2018); Du et al. (2019) and empirically (Zhang et al., 2017). This leads to an interesting question in theory: _how much overparameterization is enough to ensure global convergence of NNs?_ A common recipe for the proof of global convergence relies on the variant of Polyak-Lojasiewicz condition (Polyak, 1963; Liu et al., 2022), NTK (Du et al., 2019, 2020; Zou and Gu, 2019; Allen-Zhu et al., 2019), or the minimum eigenvalue of the gram matrix (Nguyen, 2021; Bombari et al., 2022). In Appendix B.3, we provide a comprehensive overview of a recent line of work that improves the over-parameterization condition for ensuring the convergence of NNs. However, the over-parameterization condition for Transformer to achieve global convergence remains elusive from existing literature and we make an initial step towards this question.

## 3 Problem setting

This section includes the problem setting with notations and model formulation of the shallow Transformer that is studied in this paper.

### Notation

Vectors (matrices) are symbolized by lowercase (uppercase) boldface letters, e.g., \(\bm{w}\), \(\bm{W}\). We use \(\|\cdot\|_{\mathrm{F}}\) and \(\|\cdot\|_{2}\) to represent the Frobenius norm and the spectral norm of a matrix, respectively. The Euclidean norm of a vector is symbolized by \(\|\cdot\|_{2}\). The superscript with brackets is used to represent the element of a vector/matrix, e.g., \(w^{(i)}\) is the \(i^{\text{th}}\) element of \(\bm{w}\). The superscript without brackets symbolizes the parameters at different training step, e.g., \(\bm{\theta}^{t}\). We denote by \([N]=\{1,\ldots,N\}\) for short. We use \(\sigma_{\min}(\cdot)\) and \(\lambda_{\min}(\cdot)\) to represent the minimum singular value and minimum eigenvalue of a matrix. The NTK matrix and hessian matrix of the network are denoted by \(\bm{K}\) and \(\bm{H}\), respectively. The order notation, e.g., \(\widetilde{\mathcal{O}}\), \(\widetilde{\Omega}\), omits the logarithmic factor. More detailed notation can be found in Table 2 of the appendix.

Let \(X\subseteq\mathbb{R}^{d_{s}\times d}\) be a compact metric space and \(Y\subseteq\mathbb{R}\), where \(d\) is the dimension of each token, \(d_{s}\) is the total sequence length of the input. The training set \(\{(\bm{X}_{n},y_{n})\}_{n=1}^{N}\) is assumed to be iid sampled from an unknown probability measure on \(X\times Y\). In this paper, we focus the regression task by employing the squared loss. The goal of our regression task is to find a hypothesis, i.e., a Transformer \(f:X\to Y\) in our work, such that \(f(\bm{X};\bm{\theta})\) parameterized by \(\bm{\theta}\) is a good approximation of the label \(y\in Y\) corresponding to a new sample \(\bm{X}\in X\). We use a vector \(\bm{\theta}\) to denote the collection of all learnable parameters.

### Model formulation of shallow Transformer

Throughout this work, we consider the encoder of Transformer, which can be applied to both regression and classification tasks (Yuksel et al., 2019; Dosovitskiy et al., 2021). Given an input \(\bm{X}\in\mathbb{R}^{d_{s}\times d}\), the model is defined as below:

\[\bm{A}_{1} =\text{Self-attention}(\bm{X})\triangleq\sigma_{s}\left(\tau_{0}( \bm{X}\bm{W}_{Q}^{\top})\left(\bm{X}\bm{W}_{K}^{\top}\right)^{\top}\right) \left(\bm{X}\bm{W}_{V}^{\top}\right),\] (1.1) \[\bm{A}_{2} =\tau_{1}\sigma_{r}(\bm{A}_{1}\bm{W}_{H})\,,\qquad\bm{a}_{3}= \varphi(\bm{A}_{2}),\qquad f(\bm{X};\bm{\theta})=\bm{a}_{3}^{\top}\bm{w}_{O}\,,\] (1.2)

where the output is \(f(\bm{X};\bm{\theta})\in\mathbb{R}\), \(\tau_{0}\) and \(\tau_{1}\) are two scaling factors. The ingredients of a Transformer with width \(d_{m}\) are defined as follows:

* A _self-attention_ mechanism (Eq. (1.1)): \(\sigma_{s}\) is the row-wise softmax function; the learnable parameters are \(\bm{W}_{Q},\bm{W}_{K},\bm{W}_{V}\in\mathbb{R}^{d_{m}\times d}\). We employ Gaussian initialization \(\bm{W}_{Q}^{(ij)}\sim\mathcal{N}(0,\eta_{Q})\), \(\bm{W}_{K}^{(ij)}\sim\mathcal{N}(0,\eta_{K})\), \(\bm{W}_{V}^{(ij)}\sim\mathcal{N}(0,\eta_{V})\) with \(i\in[d_{m}]\) and \(j\in[d]\). Refer to Table 1 for typical initialization examples.
* A _feed-forward ReLU_ layer (in Eq. (1.2)): \(\sigma_{r}\) is the ReLU activation function; the learnable parameter is \(\bm{W}_{H}\in\mathbb{R}^{d_{m}\times d_{m}}\). Following Yang et al. (2022), we combine \(\bm{W}_{V}\) and \(\bm{W}_{H}\) together (by setting \(\bm{W}_{H}=\bm{I}\) ) for ease of the analysis. Note that it does not mean its training dynamics are the same as the joint-training of these two adjacent matrices.
* An _average pooling_ layer (in Eq. (1.2)): \(\varphi\) indicates the column-wise average pooling. Note that the average pooling layer is applied along the sequence length dimension to ensure the final output is a scalar, which is commonly used in practical Vision Transformer or theoretical analysis (Dosovitskiy et al., 2021; Yang, 2020).
* An _output_ layer (in Eq. (1.2)) with learnable parameter \(\bm{w}_{O}\in\mathbb{R}^{d_{m}}\), initialized by \(\bm{w}_{O}^{(i)}\sim\mathcal{N}(0,\eta_{O})\).

**Remarks:** Proper initialization and scaling are required to ensure the convergence and learnability, as seen in previous work (Jacot et al., 2018; Tirer et al., 2022; Lee et al., 2019). For our convergence analysis, we consider standard Gaussian initialization with different variances and different scaling factor that includes three typical initialization schemes in practice. In Table 1, we detail the formula of LeCun initialization, He initialization, and NTK initialization.

Given \(N\) input samples \(\{\bm{X}_{n}\}_{n=1}^{N}\), the corresponding ground truth label, the final output of network, and the output of the last hidden layer, are denoted by:

\[\bm{y}\triangleq\{y_{n}\}_{n=1}^{N}\in\mathbb{R}^{N},\quad\bm{f}(\bm{\theta}) \triangleq\{f(\bm{X}_{n};\bm{\theta})\}_{n=1}^{N}\in\mathbb{R}^{N},\quad\bm{ F}_{\rm pre}(\bm{\theta})\triangleq\{\bm{a}_{3}(\bm{X}_{n};\bm{\theta})\}_{n=1}^{N} \in\mathbb{R}^{N\times d_{m}}.\]

We consider standard gradient descent (GD) training of Transformer, as illustrated in Algorithm 1. Here the squared loss is expressed as \(\ell(\bm{\theta})=\frac{1}{2}\left\|\bm{f}(\bm{\theta})-\bm{y}\right\|_{2}^{2}\).

``` Input: data \((\bm{X}_{n},y_{n})_{n=1}^{N}\), step size \(\gamma\).  Initialize weights as follows: \(\bm{\theta}^{0}:=\{\bm{W}_{Q}^{0},\bm{W}_{K}^{0},\bm{W}_{V}^{0},\bm{W}_{O}^{0}\}\). for\(t=0\) to \(t^{\prime}-1\)do \(\bm{W}_{Q}^{t+1}=\bm{W}_{Q}^{t}-\gamma\cdot\nabla_{\bm{W}_{Q}}\ell(\bm{\theta}^{t})\), \(\bm{W}_{K}^{t+1}=\bm{W}_{K}^{t}-\gamma\cdot\nabla_{\bm{W}_{K}}\ell(\bm{\theta}^{ t})\), \(\bm{W}_{K}^{t+1}=\bm{W}_{U}^{t}-\gamma\cdot\nabla_{\bm{W}_{V}}\ell(\bm{\theta}^{ t})\), \(\bm{W}_{O}^{t+1}=\bm{W}_{O}^{t}-\gamma\cdot\nabla_{\bm{W}_{O}}\ell(\bm{\theta}^{ t})\). endfor Output: the model based on \(\bm{\theta}^{t^{\prime}}\). ```

**Algorithm 1**Gradient descent training

## 4 Main results

In this section, we study the convergence guarantee of Transformer training by GD under the squared loss. Firstly, we provide a general analytical framework in Section 4.1 covering different initialization

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Init. & \(\eta_{O}\) & \(\eta_{V}\) & \(\eta_{Q}\) & \(\eta_{K}\) & \(\tau_{1}\) \\ \hline LeCun & \(d_{m}^{-1}\) & \(d^{-1}\) & \(d^{-1}\) & \(d^{-1}\) & \(1\) \\ He & \(2d_{m}^{-1}\) & \(2d^{-1}\) & \(2d^{-1}\) & \(2d^{-1}\) & \(1\) \\ NTK & 1 & 1 & 1 & 1 & \(d_{m}^{-1/2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Common initialization methods with their variances of Gaussian distribution and scaling factors. The choice of \(\tau_{1}=d_{m}^{-1/2}\) is based on standard NTK initialization on prior literature (Du et al., 2019).

schemes, where we identify the condition for achieving global convergence. Next, we validate these conditions for several practical initialization schemes under \(\tau_{0}=d_{m}^{-1/2}\) in Section 4.2 and \(\tau_{0}=d_{m}^{-1}\) in Section 4.3, respectively. We include NTK-based results for self-completeness. Discussion on the convergence results with different scalings, initializations, and architectures is present in Section 4.4.

### General framework for convergence analysis

Before presenting our result, we introduce a basic assumption.

**Assumption 1**.: _The input data is bounded \(\|\bm{X}\|_{\mathrm{F}}\leq\sqrt{d_{s}}C_{x}\) with some positive constant \(C_{x}\)._

**Remark:** This assumption is standard as we can always scale the input. The embedding of token is usually assumed to have a unit norm (Li et al., 2023), which is unrelated to \(d\).

Now we are ready to present our proposition, with the proof deferred to Appendix C.2. Notice that our proposition holds with high probability under some conditions, which depend on certain initialization schemes and scaling factors. Since our proposition is devoted to a unifying analysis framework under various initialization schemes, we do not include specific probabilities here. The validation of the required conditions and probability is deferred to Sections 4.2 and 4.3, respectively.

**Proposition 1**.: _Consider the Transformer with \(d_{m}\geq N\). Let \(C_{Q},C_{K},C_{V},C_{O}\) be some positive constants, and define the following quantities at initialization for simplification:_

\(\bullet\) _The norm of the parameters:_

\[\bar{\lambda}_{Q}\triangleq\left\|\bm{W}_{Q}^{0}\right\|_{2}+C_{Q},\ \bar{\lambda}_{K}\triangleq\left\|\bm{W}_{K}^{0}\right\|_{2}+C_{K},\ \bar{\lambda}_{V}\triangleq\left\|\bm{W}_{V}^{0}\right\|_{2}+C_{V},\ \bar{ \lambda}_{O}\triangleq\left\|\bm{w}_{O}^{0}\right\|_{2}+C_{O}.\]

\(\bullet\) _Two auxiliary terms: \(\rho\triangleq N^{1/2}d_{s}^{3/2}\tau_{1}C_{x},\quad z\triangleq\bar{\lambda} _{O}^{2}\left(1+4\tau_{0}^{2}C_{x}^{4}d_{s}^{2}\bar{\lambda}_{V}^{2}\left( \bar{\lambda}_{Q}^{2}+\bar{\lambda}_{K}^{2}\right)\right).\)_

_Under Assumption 1, we additionally assume that the minimum singular value of \(\bm{F}_{\mathrm{pre}}^{0}\), i.e., \(\alpha\triangleq\sigma_{\min}\left(\bm{F}_{\mathrm{pre}}^{0}\right)\) satisfies the following condition at initialization:_

\[\alpha^{2} \geq 8\rho\max\left(\bar{\lambda}_{V}C_{Q}^{-1},\bar{\lambda}_{O} C_{V}^{-1},2\tau_{0}C_{x}^{2}d_{s}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{ \lambda}_{O}C_{Q}^{-1},2\tau_{0}C_{x}^{2}d_{s}\bar{\lambda}_{Q}\bar{\lambda}_{V }\bar{\lambda}_{O}C_{K}^{-1}\right)\sqrt{2\ell(\bm{\theta}^{0})}\,,\] (2) \[\alpha^{3} \geq 32\rho^{2}z\sqrt{2\ell(\bm{\theta}^{0})}/\bar{\lambda}_{O}\,.\] (3)

_If the step size satisfies \(\gamma\leq 1/C\) with a constant \(C\) depending on (\(\bar{\lambda}_{Q},\bar{\lambda}_{K},\bar{\lambda}_{V},\bar{\lambda}_{O},\ell( \bm{\theta}^{0}),\rho,\tau_{0}\)), then GD converges to a global minimum as follows:_

\[\ell(\bm{\theta}^{t})\leq\left(1-\gamma\frac{\alpha^{2}}{2}\right)^{t}\ \ell(\bm{\theta}^{0})\,,\quad\forall t\geq 0\,.\] (4)

**Remark:** The parameter \(\alpha\) in Eqs. (2) and (3) controls the convergence rate of global convergence, and the condition will be verified in the next subsection. The step-size \(\gamma\) is inversely proportional to \(N^{1/2}\) due to the construction of \(C\) in Appendix C.2.

**Proof sketch:** The main idea of our convergence analysis is based on the variant of Polyak-Lojasiewicz (PL) inequality (Polyak, 1963, Nguyen, 2021), i.e., \(||\nabla\ell(\bm{\theta})||_{2}^{2}\geq 2\lambda_{\min}(\bm{K})\ell(\bm{ \theta})\geq 2\lambda_{\min}(\bm{F}_{\mathrm{pre}}\bm{F}_{\mathrm{pre}}^{\top}) \ell(\bm{\theta})\). Thus, if the minimum singular value of \(\bm{F}_{\mathrm{pre}}\) is strictly greater than 0, then minimizing the gradient on the LHS will drive the loss to zero. To this end, Proposition 1 can be split into two parts. First, by induction, at every time step, each parameter in the Transformer can be bounded w.h.p; the minimum singular value of \(\bm{F}_{\mathrm{pre}}\) is bounded away for some positive quality at the initialization point. Secondly, we prove that the Lipschitzness of the network gradient, which means the loss function is almost smooth. Combining the above two results, the global convergence can be achieved. Furthermore, based on different initialization and scaling schemes, we are able to validate Eqs. (2) and (3) via the spectral norm estimation of \(\bar{\lambda}_{Q}\), \(\bar{\lambda}_{K}\), \(\bar{\lambda}_{V}\), \(\lambda_{O}\) and a positive lower bound for \(\bm{F}_{\mathrm{pre}}\) in the following section.

### LeCun and He initialization under the \(d_{m}^{-1/2}\) setting

Here we aim to show that, under the LeCun/He initialization with the setting of \(d_{m}^{-1/2}\), the conditions in Eqs. (2) and (3) will be satisfied with high probability and scaling schemes in Table 1 and hence lead to global convergence. To derive our result, we need the following assumptions on the input data regarding the rank and dissimilarity of data.

**Assumption 2**.: _We assume that the input data \(\bm{X}\) has full row rank._

**Remark:** This assumption requires that each row \(\bm{X}^{(i,:)}\) is linearly independent for any \(i\in[d_{s}]\), which is fair and attainable in practice. For example, for language tasks, even though there might be some repeated token in \(\bm{X}\), each row in \(\bm{X}\) can be uncorrelated when added with positional embedding. Similarly, in image tasks with Visual Transformer, the raw image is grouped by patch and mapped via a linear layer to construct \(\bm{X}\), thus each row in \(\bm{X}\) can be uncorrelated.

**Assumption 3**.: _For any data pair \((\bm{X}_{n},\bm{X}_{n^{\prime}})\), with \(n\neq n^{\prime}\) and \(n,n^{\prime}\in[N]\), then we assume that \(\mathbb{P}\left(\left|\langle\bm{X}_{n}^{\top}\bm{X}_{n},\bm{X}_{n^{\prime}}^ {\top}\bm{X}_{n^{\prime}}\rangle\right|\geq t\right)\leq\exp(-t^{\hat{c}})\) with some constant \(\hat{c}>0\)._

**Remark:** We discuss the rationale behind this assumption:

The idea behind Assumption 3 is that different data points admit a small similarity. To be specific, for two data points \(\bm{X}_{n}\) and \(\bm{X}_{n^{\prime}}\) with \(n\neq n^{\prime}\), their inner product reflects the similarity of their respective (empirical) covariance matrix. We expect that this similarity is small with a high probability. The spirit of this assumption also exists in previous literature, e.g., Nguyen et al. (2021). The constant \(\hat{c}\) determines the decay of data dissimilarity. A larger \(\hat{c}\) results in less separable data. Our assumption has no requirement on \(\hat{c}\) such that \(\hat{c}\) can be small enough, which allows for a general data distribution.

**Verification of assumption.** Here we experimentally validate this assumption under a standard language IMDB dataset (Maas et al., 2011). We randomly sample 100 (normalized) sentences with embedding and plot the probability (frequency) of \(\mathbb{P}\left(\left|\langle\bm{X}_{n}^{\top}\bm{X}_{n},\bm{X}_{n^{\prime}}^ {\top}\bm{X}_{n^{\prime}}\rangle\right|\geq t\right)\) as \(t\) increases. We repeat it over 10 runs and plot the result in Figure 1. We can observe an exponential decay as \(t\) increases, which implies that our assumption is fair. Besides, the validation of Assumption 3 on image data, e.g., MNIST by ViT, is deferred to Appendix E.2.

Now we are ready to present our main theorem under the \(d_{m}^{-1/2}\) setting. The proof is deferred to Appendix C.3.

**Theorem 1**.: _Under the setting of LeCun/He initialization in Table 1 and Assumptions 1 to 3, suppose \(d_{m}\geq d\), and given \(\tau_{0}=d_{m}^{-1/2}\), \(d_{m}=\tilde{\Omega}(N^{3})\), then with probability at least \(1-8e^{-d_{m}/2}-\delta-\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\) for proper \(\delta\), the GD training of Transformer converges to a global minimum with sufficiently small step size \(\gamma\) as in Eq. (4)._

**Remark:** The probability relates to several randomness sources, e.g., data sampling, dissimilarity of data, and parameter initialization. The quantity \(\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\) can be small for a small enough \(\hat{c}\) as discussed in Assumption 3. Further discussion on our result refers to Section 4.4 for details. Besides, our proof framework is also valid for the \(\tau_{0}=d_{m}^{-1}\) setting. We demonstrate that \(d_{m}=\tilde{\Omega}(N^{2})\) suffices to achieve global convergence, see Appendix C.3 for details.

**Proof sketch:** To check whether the conditions in Eqs. (2) and (3) hold, the key idea is to provide the lower bound of \(\alpha\). Then we upper bound \(\bar{\lambda}_{Q},\bar{\lambda}_{K},\lambda_{V},\bar{\lambda}_{O}\) based on concentration inequalities to upper bound the initial loss, one key step is to utilize Gershgorin circle theorem (Gershgorin, 1931) to provide a lower bound for \(\alpha\). Lastly, we plug these bound into the condition Eqs. (2) and (3) in order to obtain the requirement for the width \(d_{m}\).

### NTK initialization under the \(d_{m}^{-1}\) setting

The NTK theory, as a representative application of the \(\tau_{0}=d_{m}^{-1}\) setting, can be also used for analysis of training dynamics. We also include the NTK results in this section for self-completeness. In this section, we first derive the limiting NTK formulation of Transformer under the \(d_{m}^{-1}\) scaling scheme, and then show the global convergence of Transformers.

Figure 1: Validation of Asm. 3.

**Lemma 1**.: _Denote \(\bm{\Phi}^{\star}:=:[\frac{1}{d_{s}}\bm{X}_{1}^{\top}\bm{1}_{d_{s}},...,\frac{1}{d _{s}}\bm{X}_{N}^{\top}\bm{1}_{d_{s}}]^{\top}\in\mathbb{R}^{N\times d}\), then the limiting NTK matrix \(\bm{K}\in\mathbb{R}^{N\times N}\) of Transformer under the NTK initialization with \(\tau_{0}=d_{m}^{-1}\) has the following form:_

\[\bm{K}=d_{s}^{2}\mathbb{E}_{\bm{w}\sim\mathcal{N}(\bm{0},\bm{I})}\left(\sigma_ {r}\left(\bm{\Phi}^{\star}\bm{w}\right)\sigma_{r}\left(\bm{\Phi}^{\star}\bm{w} \right)^{\top}\right)+d_{s}^{2}\mathbb{E}_{\bm{w}\sim\mathcal{N}(\bm{0},\bm{I })}\left(\sigma_{r}\left(\bm{\Phi}^{\star}\bm{w}\right)\sigma_{r}\left(\bm{ \Phi}^{\star}\bm{w}\right)^{\top}\right)\left(\bm{\Phi}^{\star}\bm{\Phi}^{ \star}\right).\]

**Remark:** The formulation of \(\bm{\Phi}^{\star}\) implies that the self-attention layer degenerates as \(\frac{1}{d_{s}}\bm{1}_{d_{s}\times d_{s}}\bm{X}\bm{W}_{V}^{\top}\), i.e., the _dimension missing_ effect as mentioned before.

Now we are ready to present our convergence result under the \(d_{m}^{-1}\) scaling with the proof deferred to Appendix C.4.

**Theorem 2**.: _Under the setting of NTK initialization in Table 1 and Assumptions 1 to 3, suppose \(d_{m}=\widehat{\Omega}(N)\), then with probability at least \(1-8e^{-d_{m}/2}-\delta-\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\), the GD training of Transformer converges to a global minimum with sufficiently small \(\gamma\) as in Eq. (4)._

**Proof sketch:** The overall idea is the same as the proof of the previous theorem, i.e., we need to provide the lower bound of \(\alpha\). However, in this case, the limit for the output of softmax exists so that we can apply concentration inequality to lower bound the \(\alpha\). Lastly, we plug these bound into the condition Eqs. (2) and (3) in order to obtain the requirement for the width \(d_{m}\).

Besides, the stability of NTK during training allows us to build a connection on training dynamics between the Transformer (assuming a squared loss) and the kernel regression predictor. Next, in order to show that the NTK is stable during GD training, below we prove that the spectral norm of Hessian is controlled by the width.

**Theorem 3** (Hessian norm is controlled by the width).: _Under Assumption 1 and scaling \(\tau_{0}=d_{m}^{-1}\), given any fixed \(R>0\), and any \(\bm{\theta}^{t}\in B(\bm{\theta},R):=\{\bm{\theta}:\|\bm{\theta}-\bm{\theta}^ {0}\|_{2}\leq R\}\), \(\bm{\theta}^{0}\) as the weight at initialization, then with probability at least \(1-8e^{-d_{m}/2}\), the Hessian spectral norm of Transformer satisfies: \(\|\bm{H}(\bm{\theta}^{t})\|_{2}\leq\mathcal{O}\Big{(}d_{m}^{-1/2}\Big{)}\)._

**Remark:** By (Liu et al., 2020, Proposition 2.3), the small Hessian norm is a sufficient condition for small change of NTK. Thus, Theorem 3 can be an indicator for the stability of NTK. Besides, Theorem 3 supplements the result in (Park and Kim, 2022) which exhibits empirically a relationship between the Hessian norm and the width but lacks theoretical proof.

### Discussion on convergence results

We compare the derived results under different scaling schemes, initializations, and architectures.

**Comparison of scaling schemes:** The global convergence can be achieved under both \(\tau_{0}=d_{m}^{-1/2}\) and \(\tau_{0}=d_{m}^{-1}\). Nevertheless, as suggested by our theory, for a small \(d_{m}\), there is no significant difference between these two scaling schemes on the convergence; but for a large enough \(d_{m}\), the \(\tau_{0}=d_{m}^{-1/2}\) scaling admits a faster convergence rate of Transformers than that of \(\tau_{0}=d_{m}^{-1}\) due to a more tight estimation of the lower bound of \(\alpha\), see Appendix C.6 for details. The intuition behind the lower convergence rate under the setting of large width and \(\tau_{0}=d_{m}^{-1}\) is that the input of softmax is close to zero such that softmax roughly degenerates as a pooling layer, losing the ability to fit data. This can be also explained by Lemma 1 from the perspective of _dimension missing_: self-attention (\(\bm{X}\)) degenerates as \(\frac{1}{d_{s}}\bm{1}_{d_{s}\times d_{s}}\bm{X}\bm{W}_{V}^{\top}\).

The result under the \(\tau_{0}=d_{m}^{-1}\) setting requires weaker over-parameterization than the \(\tau_{0}=d_{m}^{-1/2}\) setting. Nevertheless, we do not claim that \(\tau_{0}=d_{m}^{-1}\) is better than \(\tau_{0}=d_{m}^{-1/2}\). This is because, under the over-parameterization regime, the scaling \(\tau_{0}=d_{m}^{-1}\) makes the self-attention layer close to a pooling layer. This analysis loses the ability to capture the key characteristics of the self-attention mechanism in Transformers.

Note that the lower bound of the minimum eigenvalue is in the constant order, which is tight (since it matches the upper bound). Based on this, by studying the relationship between \(d_{m}\) and \(\lambda_{0}\), we can prove that quadratic (cubic) over-parameterization is required for \(d_{m}^{-1}\) (\(d_{m}^{-1/2}\)) scaling. This quadratic over-parameterization requirement could be relaxed if a better relationship is given while it is still unclear and beyond our proof technique.

**Comparison on initializations:** Though our results achieve the same convergence rate under these initialization schemes, we can still show the _separation_ on \(\alpha\) that affects the convergence in Eq. (4) among these initialization schemes. To be specific, we verify that under LeCun and He initialization, the lower bound of \(\alpha^{2}\) is tighter than that of NTK initialization, and hence admits faster convergence, see Appendix C.5 for further details. This can be an explanation of the seldom usage of NTK initialization in practice. Besides, the NTK initialization scheme allows for a larger step size than LeCun/He initialization for training. That means, if \(\alpha\) is the same in these three initialization schemes, we usually choose a large step size under the NTK initialization scheme, see Appendix E.1.

**Comparison on architectures:** Note that the Transformer defined in Eq. (1.2) includes a self-attention layer, a feedforward ReLU layer, and an output layer. Our result proves that a cubic (quadratic) over-parameterization condition is required for \(d_{m}^{-1/2}\) (\(d_{m}^{-1}\)) under LeCun initialization. As a comparison, a three-layer fully-connected ReLU network under LeCun initialization requires \(d_{m}=\tilde{\Omega}(N^{3})\)(Nguyen, 2021).

The aforementioned result holds for matrix input. Although not as frequent, some data inputs are naturally in vector form. Two ways to feed the input into Transformer are either formulating along sequence dimension (\(d=1\)) or along embedding dimension (\(d_{s}=1\)). The following result shows the separation under these two settings to understand when the Transformer works well or not regarding the input.

**Corollary 1** (Convergence of vector input).: _Consider LeCun initialization with \(\tau_{0}=d_{m}^{-1}\) scaling, given vector input \(\bm{x}\in\mathbb{R}^{\tilde{d}}\), if one feeds the input to Transformer by setting \(d_{s}=1,d=\tilde{d}\), then training with GD can converge to a global minimum. On the contrary, if one sets \(d_{s}=\tilde{d},d=1\), the conditions in Eqs. (2) and (3) do not hold, the convergence can not be guaranteed by our theory._

**Remark:** Such a result is motivated by considering least squares. Specifically, given input \(\bm{X}\in\mathbb{R}^{N\times 1}\), then the NTK \(\bm{X}\bm{X}^{\top}\) is a rank-one matrix. As a result, when the augmented matrix \([\bm{X},\bm{y}]\) is not rank-one, then there is no solution to the linear system so that GD training can not converge to zero loss. The empirical validation can be found in Figure 3.

**Technical difficulty.** The technical difficulty of our analysis includes handling the softmax function and scaling schemes beyond NTK initialization. Previous convergence analysis, e.g., (Du et al., 2019, Nguyen, 2021) cannot be applied to our setting because of the following issues. First, different from classical activation functions, e.g., ReLU, in softmax each element of the output depends on all input. To tackle the interplay between dimensions, we build the connection between the output of softmax and \(\bm{X}\bm{X}^{\top}\) to disentangle the nonlinear softmax function. By doing so, a lower bound on the minimum singular value of \(\bm{F}_{\text{pre}}\) in Proposition. 1 can be well controlled by \(\bm{X}\bm{X}^{\top}\) and the output of softmax.

Regarding different initializations and scaling, previous NTK-based analysis is only valid under the \(d_{m}^{-1}\) setting (the softmax degenerates to an all-one vector) but is inapplicable to the realistic \(d_{m}^{-1/2}\) setting, as discussed in the introduction. To tackle this issue, we analyze the input/output of softmax under LeCun/He initialization and identify the optimization properties of the loss function for global convergence under the finite-width setting.

## 5 Experimental validations

Our experiments are organized as follows: In Section 5.1, we conduct experiments with the model Eq. (1.2) on synthetic data and study the training dynamics. Next, we show convergence results on ViT (Dosovitskiy et al., 2021) on the standard MNIST dataset in Section 5.2. Additional results and detail on the experimental setup are deferred to Appendix E.

### Fitting synthetic data

In this section, we corroborate our theoretical findings on the synthetic data. We generate \(100\) data points where the input \(\bm{X}\in\mathbb{R}^{10\times 100}\) is sampled from standard Gaussian distribution. The corresponding label \(y\in\mathbb{R}\) is generated from standard Gaussian distribution. Squared loss is selected as the criterion. We apply gradient descent on the shallow Transformer defined in Eq. (1.2) with LeCun initialization and \(\tau_{0}=d_{m}^{-1/2}\) for \(400\) epochs with a fixed step size \(\gamma=1\). We test different widths of the network including \(d_{m}=\{10,100,1000,4000\}\) and plot the training progress inFigure 2. The result shows that the training can converge except for the case with sub-linear width, see the linear convergence rate in Figure 1(a) and the small movement of the weight in Figure 1(b). For these cases, the weights do not change much after 50 epochs while the losses are still decreasing. In Figure 1(c), we keep track of the evolution of NTK along each epoch. Specifically, the kernel distance is given by:

\[\text{Distance}\left(\bm{K},\widetilde{\bm{K}}\right)=1-\frac{ \operatorname{Tr}\left(\bm{K}\widetilde{\bm{K}}^{\top}\right)}{\sqrt{ \operatorname{Tr}\left(\bm{K}\bm{K}^{\top}\right)}\sqrt{\operatorname{Tr} \left(\widetilde{\bm{K}}\widetilde{\bm{K}}^{\top}\right)}}\,,\]

which quantitatively compares two kernels by the relative rotation, as used in Fort et al. (2020). Figure 1(c) shows that the kernel changes rapidly at the beginning of training while being approximately constant at later stages. The experiment with \(\tau_{0}=d_{m}^{-1}\), which is deferred to Appendix E.1, obtains similar results.

Additionally, the experiment with other initialization schemes is deferred to Appendix E.1, where we observe that NTK initialization indeed yields slower convergence than LeCun/He initialization, which is consistent with our theoretical finding.

Next, in order to verify corollary 1, we feed vector input \(\bm{x}_{n}\in\mathbb{R}^{100}\) into Transformer by setting either \(d_{s}=1,d=100\) or \(d_{s}=100,d=1\) under LeCun initialization with \(\tau_{0}=d_{m}^{-1}\). Figure 3 shows that the training of Transformer with \(d_{s}=1,d=100\) is similar to that of two-layer FCNN with the same width \(d_{m}=100\). However, the case of \(d_{s}=100,d=1\) fails to converge, which is consistent with our theoretical finding.

### Fitting real-world dataset

Beyond synthetic data, in this section, we examine the convergence performance of Vision Transformer (ViT) on classification task on MNIST dataset (LeCun et al., 1998), which includes 10 classes of images with size \(28\times 28\). We use a single layer and single head ViT. The dimension of \(d\) is 64. We change the dimension of the query, key, and value from 16 to 1024 and 16384. The network is optimized with SGD with step size \(0.1\), and momentum \(0.9\) for \(50\) epochs. We repeat the experiment for three runs. We present the convergence results over \(3\) runs in Figure 3(a), which shows that when the width is smaller, e.g., \(16\), both \(\tau_{0}=d_{m}^{-1}\) and \(\tau_{0}=d_{m}^{-1/2}\) scaling have similar

Figure 3: Convergence result on synthetic data with vector input.

Figure 2: Visualization of the training process of Transformers with LeCun initialization and \(\tau_{0}=d_{m}^{-1/2}\) scaling on synthetic data. (a) Linear convergence. (b) Rate of change of the weights during training. Observe that the weights change very slowly after the \(50^{\text{th}}\) epoch. (c) Evolution of the NTK during the training. The result mirrors the plot (b) and demonstrates how the kernel varies significantly at the beginning of the training and remains approximately constant later. As the width increases, the empirical NTK becomes more stable.

convergence results. However, as the width increases to \(1024\) and \(16384\), the \(\tau_{0}=d_{m}^{-1}\) setting admits a slower rate than that of \(\tau_{0}=d_{m}^{-1/2}\), especially a extremely slow rate under \(d_{m}=16384\). This is consistent with our theoretical result on the _dimension missing_ effect such that the \(\tau_{0}=d_{m}^{-1}\) setting makes Transformer difficult to fit data. Additionally, we visualize the attention map in Figure 3(b), i.e., the output of softmax in the self-attention layer before training and after training under \(\tau_{0}=d_{m}^{-1/2}\) setting. We could see that the self-attention layer changes a lot during training.

## 6 Conclusion

We present a comprehensive analysis of the global convergence properties of shallow Transformer under various scaling and initialization schemes. Regarding scaling schemes, for a large width setting, the difference on convergence between \(\tau_{0}=d_{m}^{-1/2}\) and \(\tau_{0}=d_{m}^{-1}\) can be demonstrated both theoretically and empirically. Our theory is able to explain this in a _dimension missing_ view. Regarding initialization schemes, our theory prefers to using LeCun and He initialization in Transformer training, which allows for a faster convergence rate than NTK initialization. Though our analysis is limited to shallow Transformers, we believe our framework can be extended to deep Transformers. We provide further discussion into this extension in Appendix C.7. Exploring the convergence properties of deep Transformers is indeed an intriguing avenue for future research.

## Acknowledgements

We thank the reviewers for their constructive feedback. We thank Zhenyu Zhu for the discussion and help in this work. This work has received funding from the Swiss National Science Foundation (SNSF) under grant number 200021_205011. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). Corresponding author: Fanghui Liu.

Figure 4: Convergence curve on MNIST dataset with different scaling schemes and different widths in (a). Visualization of attention map in (b).

## References

* Akyurek et al. (2023) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In _International Conference on Learning Representations (ICLR)_, 2023.
* Alemohammad et al. (2021) Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent neural tangent kernel. In _International Conference on Learning Representations (ICLR)_, 2021.
* Allen-Zhu et al. (2019a) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning (ICML)_, pages 242-252. PMLR, 2019a.
* Allen-Zhu et al. (2019b) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. In _Advances in neural information processing systems (NeurIPS)_, volume 32, 2019b.
* Arora et al. (2019a) Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning (ICML)_, 2019a.
* Arora et al. (2019b) Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in neural information processing systems (NeurIPS)_, 32, 2019b.
* Bombari et al. (2022) Simone Bombari, Mohammad Hossein Amani, and Marco Mondelli. Memorization and optimization in deep neural networks with minimum over-parameterization. In _Advances in neural information processing systems (NeurIPS)_, 2022.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems (NeurIPS)_, 2020.
* Cao & Gu (2019) Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. In _Advances in neural information processing systems (NeurIPS)_, 2019.
* Chen et al. (2022) Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In _International Conference on Learning Representations (ICLR)_, 2022.
* Chen et al. (2021) Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to learn deep re{lu} networks? In _International Conference on Learning Representations (ICLR)_, 2021.
* Chizat et al. (2019) Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems (NeurIPS)_, 2019.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.
* Du et al. (2019a) Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International Conference on Machine Learning (ICML)_, 2019a.
* Du et al. (2019b) Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations (ICLR)_, 2019b.
* Du et al. (2019c)Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning (ICML)_, pages 5793-5831, 2022.
* Fort et al. (2020) Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In _Advances in neural information processing systems (NeurIPS)_, 2020.
* Gershgorin (1931) Semyon Aranovich Gershgorin. About delimiting the intrinsic values of a matrix. _Proc. Russian Acad. Sci_, page 749-754, 1931.
* Hastie et al. (2022) Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. _Annals of Statistics_, 2022.
* Hron et al. (2020) Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In _International Conference on Machine Learning (ICML)_, 2020.
* Huang et al. (2020) Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better than deep feedforward networks?--a neural tangent kernel perspective. _Advances in neural information processing systems (NeurIPS)_, 33:2698-2709, 2020.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in neural information processing systems (NeurIPS)_, 2018.
* Jelassi et al. (2022) Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in neural information processing systems (NeurIPS)_, 2022.
* LeCun et al. (1998) Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* Lee et al. (2019) Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems (NeurIPS)_, 2019.
* Li et al. (2023) Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. In _International Conference on Learning Representations (ICLR)_, 2023.
* Li and Liang (2018) Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems (NeurIPS)_, 2018.
* Liang and Rakhlin (2020) Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel "ridgeless" regression can generalize. _The Annals of Statistics_, 2020.
* Liao and Couillet (2018) Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional data. In _International Conference on Machine Learning_, pages 3063-3071. PMLR, 2018.
* Liu et al. (2020) Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. _Advances in neural information processing systems (NeurIPS)_, 2020.
* Liu et al. (2022a) Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 2022a.
* Liu et al. (2021) Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Refined analysis beyond double descent. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2021.
* Liu et al. (2022b) Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022b.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
* Mirsky (1975) Leon Mirsky. A trace inequality of john von neumann. _Monatshefte fur mathematik_, 79:303-306, 1975.
* Montanari and Zhong (2022) Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. _The Annals of Statistics_, 50(5):2816-2847, 2022.
* Nguyen (2021) Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear widths. In _International Conference on Machine Learning (ICML)_, 2021.
* Nguyen et al. (2021) Quynh Nguyen, Marco Mondelli, and Guido F Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In _International Conference on Machine Learning (ICML)_, 2021.
* Nguyen and Mondelli (2020) Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. _Advances in neural information processing systems (NeurIPS)_, 2020.
* Oymak and Soltanolkotabi (2020) Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* Park and Kim (2022) Namuk Park and Songkuk Kim. How do vision transformers work? In _International Conference on Learning Representations (ICLR)_, 2022.
* Polyak (1963) Boris Teodorovich Polyak. Gradient methods for minimizing functionals. _Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki_, 3(4):643-653, 1963.
* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. 2018.
* Snell et al. (2021) Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.
* Song et al. (2021) Chaehwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher. Subquadratic overparameterization for shallow neural networks. In _Advances in neural information processing systems (NeurIPS)_, 2021.
* Tirer et al. (2020) Tom Tirer, Joan Bruna, and Raja Giryes. Kernel-based smoothness analysis of residual networks. _arXiv preprint arXiv:2009.10008_, 2020.
* Tirer et al. (2022) Tom Tirer, Joan Bruna, and Raja Giryes. Kernel-based smoothness analysis of residual networks. In _Mathematical and Scientific Machine Learning_, 2022.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems (NeurIPS)_, 2017.
* Vershynin (2012) Roman Vershynin. _Introduction to the non-asymptotic analysis of random matrices_, page 210-268. Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006.
* Vershynin (2018) Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge University Press, 2018.
* Oswald et al. (2022) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* Wainwright (2019) Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* Wainwright et al. (2019)Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.
* Yang [2020] Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. _arXiv preprint arXiv:2006.14548_, 2020.
* Yang et al. [2022] Yongyi Yang, Zengfeng Huang, and David Wipf. Transformers from an optimization perspective. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in neural information processing systems (NeurIPS)_, 2022.
* Yuksel et al. [2019] Atf Emre Yuksel, Yasar Alim Turkmen, Arzucan Ozgur, and Berna Altunel. Turkish tweet classification with transformer encoder. In _Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)_, pages 1380-1387, 2019.
* Zhang et al. [2017] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations (ICLR)_, 2017.
* Zhu et al. [2022] Zhenyu Zhu, Fanghui Liu, Grigorios G Chrysos, and Volkan Cevher. Generalization properties of nas under activation and skip connection search. In _Advances in neural information processing systems (NeurIPS)_, 2022.
* Zou and Gu [2019] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. In _Advances in neural information processing systems (NeurIPS)_, volume 32, 2019.
* Zou et al. [2020] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep relu networks. _Machine learning_, 2020.

### Contents of the Appendix

The Appendix is organized as follows:

* In Appendix A, we summarize the symbols and notation used in this work.
* In Appendix B.1, we provide a theoretical overview for neural tangent kernel (NTK). The background in Sub-Exponential random variables is elaborated in Appendix B.2. More detailed related work on the convergence analysis of nerual networks can be found in Appendix B.3.
* The proofs for the convergence of Transformer are included in Appendix C.
* The derivations and the proofs for the NTK are further elaborated in Appendix D, including the formulation of NTK, minimum eigenvalue of NTK, and the relationship between the Hessian and the width.
* Further details on the experiments are developed in Appendix E.
* Limitations and societal impact of this work are discussed in Appendix F and Appendix G, respectively.

Symbols and Notation

We include the core symbols and notation in Table 2 for facilitating the understanding of our work.

## Appendix B Theoretical background

### Preliminary on NTK

In this section, we summarize how training a neural network by minimizing squared loss, i.e., \(\ell(\bm{\theta}^{t})=\frac{1}{2}\sum_{n=1}^{N}(f(\bm{X}_{n};\bm{\theta}^{t})-y_{ n})^{2}\), via gradient descent can be characterized by the kernel regression predictor with NTK.

By choosing an infinitesimally small learning rate, we can obtain the following gradient flow:

\[\frac{d\bm{\theta}^{t}}{dt}=-\nabla\ell(\bm{\theta}^{t})\,.\]

\begin{table}
\begin{tabular}{c|c|c} \hline Symbol & Dimension(s) & Definition \\ \hline \(\mathcal{N}(\mu,\sigma)\) & - & Gaussian distribution of mean \(\mu\) and variance \(\sigma\) \\ \hline \(\left\|\bm{w}\right\|_{2}\) & - & Euclidean norms of vectors \(\bm{w}\) \\ \(\left\|\bm{W}\right\|_{2}\) & - & Spectral norms of matrix \(\bm{W}\) \\ \(\left\|\bm{W}\right\|_{2}\) & - & Frobenius norms of matrix \(\bm{W}\) \\ \(\left\|\bm{W}\right\|_{1}\) & - & Nuclear norms of matrix \(\bm{W}\) \\ \(\lambda_{\text{max}}(\bm{W}),\lambda_{\text{max}}(\bm{W})\) & - & Minimum and maximum eigenvalues of matrix \(\bm{W}\) \\ \(\sigma_{\text{min}}(\bm{W}),\sigma_{\text{max}}(\bm{W})\) & - & Minimum and Maximum singular values of matrix \(\bm{W}\) \\ \(\bm{w}^{(i)}\) & - & \(i\)-th element of vectors \(\bm{w}\) \\ \(\bm{W}^{(i)}\) & - & \((i,j)\)-th element of matrix \(\bm{W}\) \\ \(\bm{W}^{t}\) & - & \(\bm{W}\) at time step \(t\) \\ \(\circ\) & - & Hadamard product \\ \hline \(\sigma_{s}(\bm{W})\) & - & Row-wise Softmax activation for matrix \(\bm{W}\) \\ \(\sigma_{s}(\bm{W})\) & - & Element-wise ReLU activation for matrix \(\bm{W}\) \\ \(1\) (event) & - & Indicator function for event \\ \hline \(N\) & - & Size of the dataset \\ \(d_{m}\) & - & Width of intermediate layer \\ \(d_{s}\) & - & Sequence length of the input \\ \(d\) & - & Dimension of each token \\ \(\eta_{Q},\eta_{K},\eta_{Y},\eta_{O}\) & - & Variance of Gaussian initialization of \(\bm{W}_{Q},\bm{W}_{K},\bm{W}_{Y},\bm{w}_{O}\) \\ \(\tau_{0},\tau_{1}\) & - & Scaling factor \\ \(\gamma\) & - & Step size \\ \hline \(\bm{X}_{n}\) & \(\mathbb{R}^{d,xd}_{n}\) & The \(n\)-th data point \\ \(y_{n}\) & \(\mathbb{R}\) & The \(n\)-th target \\ \(\mathcal{D}_{\mathcal{Y}}\) & - & Input data distribution \\ \(\mathcal{D}_{\mathcal{Y}}\) & - & Target data distribution \\ \(\bm{\beta}_{n,n}:=\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(i)}\bm{W}_{Q}^{\dagger} \bm{W}_{K}\bm{X}_{n}^{\top}\right)^{\top}\) & \(\mathbb{R}^{d_{s}}\) & The \(i\)-th row of the output of softmax of the \(n\)-th data point \\ \hline \(\bm{y}:=[y_{1},...,y_{N}]^{\top}\) & \(\mathbb{R}^{N}\) & Ground truth label of the data samples \(\{\bm{X}_{n}\}_{n=1}^{N}\) \\ \(\bm{f}(\bm{\theta}):=[f(\bm{X}_{1};\bm{\theta}),...,f(\bm{X}_{N};\bm{\theta})]^{ \top}\) & \(\mathbb{R}^{N}\) & Network output given data samples \(\{\bm{X}_{n}\}_{n=1}^{N}\) \\ \(\bm{F}_{\text{rec}}(\bm{\theta}):=[a(\bm{\theta}),\bm{\Lambda}(\bm{\theta}),..., a(\bm{X}_{N};\bm{\theta})]^{\top}\) & \(\mathbb{R}^{N\times d_{n}}\) & Output of the last hidden layer given \(\{\bm{X}_{n}\}_{n=1}^{N}\) \\ \(\bm{f}_{\text{rec}}:=a_{s}(\bm{X};\bm{\theta})\) & \(\mathbb{R}^{d_{n}}\) & Output of the last hidden layer given\(\bm{X}\) \\ \hline \(\bm{W}_{Q},\bm{W}_{K}\) & \(\mathbb{R}^{d_{n},xd}_{n}\) & Learnable parameters \\ \(\bm{W}_{V},\bm{w}_{O}\) & \(\mathbb{R}^{d_{n},xd}_{n}\) & Learnable parameters \\ \hline \(\bar{\lambda}_{Q}\triangleq\left\|\bm{W}_{Q}^{0}\right\|_{2}+C_{Q},\bar{ \lambda}_{K}\triangleq\left\|\bm{W}_{K}^{0}\right\|_{2}+C_{K}\) & \(\mathbb{R}\) & Parameters norm \\ \(\bar{\lambda}_{V}\triangleq\left\|\bm{W}_{V}^{0}\right\|_{2}+C_{V},\bar{ \lambda}_{Q}\triangleq\left\|\bm{w}_{Q}^{0}\right\|_{2}+C_{O}\) & \(\mathbb{R}\) & Parameters norm \\ \(\rho\triangleq N^{1/2}d_{s}^{3/2}\tau_{1}C_{x}\), \(z\triangleq\bar{\lambda}_{Q}^{2}\left(1+4\bar{\tau}_{0}^{2}c_{s}^{2}d_{s}^{3/2} \left(\bar{\lambda}_{Q}^{2}+\bar{\lambda}_{K}^{2}\right)\right)\) & \(\mathbb{R}\) & Auxiliary terms \\ \hline \(f_{i}\) & \(\mathbb{R}\) & Output of network for input \(\bm{X}_{i}\) \\ \hline \(\mathcal{O},o,\Omega\) and \(\Theta\) & - & Standard Bachmann–Landau order notation \\ \hline \end{tabular}

* The superscript with bracket represents the element of a vector/matrix, e.g., \(\bm{w}^{(i)}\) is the \(i^{\text{th}}\) element of \(\bm{w}\).
* The superscript without bracket symbolizes the parameters at different training steps, e.g., \(\bm{\theta}^{t}\).
* The subscript without bracket symbolizes the variable associated to the \(n\)-th data sample, e.g., \(\bm{X}_{n}\).

\end{table}
Table 2: Core symbols and notations used in this paper.

By substituting the loss into the above equation and using the chain rule, we can find that the network outputs \(f(\bm{\theta}^{t})\in\mathbb{R}^{N}\) admit the following dynamics:

\[\frac{df(\bm{\theta}^{t})}{dt}=-\bm{K}^{t}(f(\bm{\theta}^{t})-\bm{y})\,,\] (5)

where \(\bm{K}^{t}=\left(\frac{\partial f(\bm{\theta}^{t})}{\partial\bm{\theta}} \right)\left(\frac{\partial f(\bm{\theta}^{t})}{\partial\bm{\theta}}\right)^{ \top}\in\mathbb{R}^{N\times N}\). Jacot et al. (2018); Arora et al. (2019b) have shown that for fully-connected neural networks, under the infinite-width setting and proper initialization, \(\bm{K}^{t}\) will be stable during training and \(\bm{K}^{0}\) will converge to a fixed matrix \(\bm{K}^{\star}\in\mathbb{R}^{N\times N}\), where \((\bm{K}^{\star})^{(ij)}=K^{\star}(\bm{X}_{i},\bm{X}_{j})\) is the NTK value for the inputs \(\bm{X}_{i}\) and \(\bm{X}_{j}\). Then, we rewrite Eq. (5) as:

\[\frac{df(\bm{\theta}^{t})}{dt}=-\bm{K}^{\star}(f(\bm{\theta}^{t})-\bm{y})\,.\]

This implies the network output for any \(\bm{X}\in\mathbb{R}^{d_{s}\times d}\) can be calculated by the kernel regression predictor with the associated NTK:

\[f(\bm{X})=(K^{\star}(\bm{X},\bm{X}_{1}),\cdots,K^{\star}(\bm{X},\bm{X}_{N})) \cdot(\bm{K}^{\star})^{-1}\bm{y}\,,\]

where \(K^{\star}(\bm{X},\bm{X}_{n})\) is the kernel value between test data \(\bm{X}\) and training data \(\bm{X}_{n}\).

### Preliminary on Sub-Exponential random variables

Below, we overview the definition of a sub-exponential random variable and few related lemma based on Wainwright (2019). A random variable \(X\) with mean \(\mu\) is called sub-exponential random variable if there exist non-negative parameters \((\nu,\alpha)\) such that

\[\mathbb{E}[e^{\lambda(X-\mu)}]\leq e^{\frac{\nu^{2}\lambda^{2}}{2}}\quad\text { for all }|\lambda|<\frac{1}{\alpha},\]

and we denote by \(X\sim SE(\nu,\alpha)\).

**Lemma 2**.: _The multiplication of a scalar \(s\in\mathbb{R}^{+}\) and a sub-exponential random variable \(X\sim SE(\nu,\alpha)\) is still a sub-exponential random variable: \(sX\sim SE(s\nu,s\alpha)\)._

**Lemma 3**.: _Given a set of independent sub-exponential random variables \(X_{i}\sim(\nu_{i},\alpha_{i})\) for \(i\in 1,...,N\), then \(\sum_{i=1}^{N}X_{i}\sim(\sqrt{\sum_{i=1}^{N}\nu_{i}^{2}},\max_{i}\alpha_{i})\)._

**Lemma 4**.: _Given a sub-exponential random variable \(X\sim SE(\nu,\alpha)\) with mean \(\mu\), the following inequality holds with probability at least \(1-\delta\):_

\[|X-\mu|<\max\left(\nu\sqrt{2\log\frac{2}{\delta}},2\alpha\log\frac{2}{\delta} \right)\,.\]

### Related work on over-parameterization for convergence analysis

Recent empirical observation shows that neural networks can fit arbitrary labels with zero training loss when applying the first-order methods, e.g., gradient descent (GD) (Zhang et al., 2017). Due to the highly non-convex and non-smooth instinct of the neural network, a large body of work have attempted to explain such a phenomenon. Early work studied the convergence of stochastic gradient descent (SGD) for training two-layer over-parameterized ReLU neural network with cross-entropy loss (Li and Liang, 2018). Du et al. (2019) show that only training the first layer of two-layer ReLU network with square loss by GD can lead to global convergence under the assumption that the Gram matrix is positive definite. Du et al. (2019) extend the result to deep neural network with smooth activation function and shows that the convergence is guaranteed when the widths of all the hidden layers scale in \(\Omega(N^{4})\), where \(N\) is the number of data points. Meanwhile, Zou and Gu (2019) prove that the condition for the convergence of GD for deep ReLU network is \(\Omega(N^{8})\), which improves upon Allen-Zhu et al. (2019) that show the result of \(\Omega(N^{24})\). Allen-Zhu et al. (2019) also provide several convergence analyses under the setting of SGD and various loss functions. The assumption regarding the positive definiteness of the Gram matrix made in Du et al. (2019) has been rigorously proved in Nguyen et al. (2021). This facilitates Nguyen (2021) to demonstrate that deep ReLU network under LeCUN initialization with width in the order \(\Omega(N^{3})\) is enough for global convergence. Recent breakthrough (Bombari et al., 2022) improves previous results by showing that sub-linear layer widths suffice for deep neural network with smooth activation function.

## Appendix C Proof for convergence analysis

This section is developed for the proof of the convergence result and we outline the flowchart below: Specifically, in Appendix C.1, we provide some auxiliary lemmas. Lemma 5 and Lemma 6 show that the norm of the parameters can be bounded with high probability at initialization. Lemmas 7, 9 and 10 present that the network output and the output of softmax between two adjacent time steps can be upper bounded. The Lipschitzness of network gradient and its norm is bounded in Lemmas 13 and 14. In Appendix C.2, we prove the convergence of the general cases, i.e., Proposition. 1. In Appendix C.3 and C.4 we present the proof for \(d_{m}^{-1/2}\) and \(d_{m}^{-1}\) scaling.

### Auxiliary lemmas

**Lemma 5** (Corollary 5.35 of Vershynin (2012)).: _For a weight matrix \(\bm{W}\in\mathbb{R}^{d_{1}\times d_{2}}\) where each element is sampled independently from \(\mathcal{N}(0,1)\), for every \(\zeta\geq 0\), with probability at least \(1-2\exp(-\zeta^{2}/2)\) one has:_

\[\sqrt{d_{1}}-\sqrt{d_{2}}-\zeta\leq\sigma_{\min}(\bm{W})\leq\sigma_{\max}(\bm{ W})\leq\sqrt{d_{1}}+\sqrt{d_{2}}+\zeta,\]

_where \(\sigma_{\max}(\bm{W})\) and \(\sigma_{\min}(\bm{W})\) represents the maximum and minimum singular value of \(\bm{W}\), respectively._

**Lemma 6** (Upper bound of spectral norms of initial weight).: _For a weight matrix \(\bm{W}\in\mathbb{R}^{d_{m}\times d}\) where \(d_{m}>d\), each element is sampled independently from \(\mathcal{N}(0,1)\), with probability at least \(1-2\exp(-d_{m}/2)\), one has:_

\[\|\bm{W}\|_{2}\leq 3\sqrt{d_{m}}.\]

Proof of Lemma 6.: Following Lemma 5, one has:

\[\|\bm{W}\|_{2}\leq\sqrt{d_{m}}+\sqrt{d}+\zeta.\]

Letting \(\zeta=\sqrt{d_{m}}\) and using the fact that \(d_{m}>d\) complete the proof. 

**Lemma 7**.: _Recall from Table 2, \(\bm{\beta}_{i}=\sigma_{s}\left(\tau_{0}\bm{X}^{(i,:)}\bm{W}_{Q}^{\top}\bm{W}_{ K}\bm{X}^{\top}\right)^{\top}\) is the \(i\)-th row of the output of Softmax, if \(\max\left(\left\|\bm{W}_{Q}^{t}\right\|_{2},\left\|\bm{W}_{Q}^{t^{\prime}} \right\|_{2}\right)\leq\bar{\lambda}_{Q},\max\left(\left\|\bm{W}_{K}^{t} \right\|_{2},\left\|\bm{W}_{K}^{t^{\prime}}\right\|_{2}\right)\leq\bar{ \lambda}_{K},\) then its difference

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & Model & Depth & Initialization & Activation & Width \\ \hline Allen-Zhu et al. (2019) & FCNN/CNN & Deep & NTK & ReLU & \(\Omega(N^{24}L^{12})\) \\ Du et al. (2019) & FCNN/CNN & Deep & NTK & Smooth & \(\Omega(N^{4}2^{\mathcal{O}(L)})\) \\ \hline Oymak and Soltanolkotabi (2020) & FCNN & Shallow & Standard Gaussian & ReLU & \(\Omega(N^{2})\) \\ Zou and Gu (2019) & FCNN & Deep & He & ReLU & \(\Omega(N^{8}L^{12})\) \\ Du et al. (2019) & FCNN & Shallow & NTK & ReLU & \(\Omega(N^{6})\) \\ Nguyen (2021) & FCNN & Deep & LeCun & ReLU & \(\Omega(N^{3})\) \\ Chen et al. (2021) & FCNN & Deep & NTK & ReLU & \(\Omega(L^{22})\) \\ Song et al. (2021) & FCNN & Shallow & He/LeCun & Smooth & \(\Omega(N^{3/2})\) \\ Bombari et al. (2022) & FCNN & Deep & He/LeCun & Smooth & \(\Omega(\sqrt{N})\) \\ \hline Allen-Zhu et al. (2019) & RNN & - & NTK & ReLU & \(\Omega(N^{e}),c>1\) \\ \hline Hron et al. (2020) & Transformer & Deep & NTK & ReLU & - \\ Yang (2020) & Transformer & Deep & NTK & Softmax+ReLU & - \\
**Our** & Transformer & Shallow & Table 1 & Softmax+ReLU & \(\Omega(N)\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Over-parameterization conditions for the convergence analysis of neural network under gradient descent training with squared loss. \(L\) is the depth of the network.

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

Then, we have

\[\frac{\partial\bm{v}^{(j)}}{\partial\bm{u}^{(k)}}=\frac{\partial \frac{\exp\left(\bm{u}^{(j)}\right)}{\sum_{n=1}^{d_{s}}\exp\left(\bm{u}^{(i)} \right)}}{\partial\bm{u}^{(k)}} =\left\{\begin{array}{ll}\frac{-\exp\left(\bm{u}^{(i)}\right)- \exp\left(\bm{u}^{(k)}\right)}{\left(\sum_{n=1}^{d_{s}}\exp\left(\bm{u}^{(i)} \right)\right)^{2}}&\text{if }j\neq k\\ \frac{\exp\left(\bm{u}^{(i)}\right)\sum_{n=1}^{d_{s}}\exp\left(\bm{u}^{(i)} \right)-\left(\exp\left(\bm{u}^{(k)}\right)\right)^{2}}{\left(\sum_{n=1}^{d_{ s}}\exp\left(\bm{u}^{(i)}\right)\right)^{2}}&\text{if }j=k\end{array}\right.\]

Thus

\[\frac{\partial\bm{v}}{\partial\bm{u}}=\text{diag}(\bm{v})-\bm{vv}^{\top}\,.\]

**Lemma 12** (Upper bound for the loss gradient norm).: _If \(\left\|\bm{W}_{Q}^{t}\right\|_{2}\leq\bar{\lambda}_{Q},\left\|\bm{W}_{K}^{t} \right\|_{2}\leq\bar{\lambda}_{K},\left\|\bm{W}_{V}^{t}\right\|_{2}\leq\bar{ \lambda}_{V},\left\|\bm{w}_{O}^{t}\right\|_{2}\leq\bar{\lambda}_{O},\) then the gradient norm with respect to \(\bm{W}_{Q},\bm{W}_{K},\bm{W}_{V},\bm{w}_{O}\) can be upper bounded by:_

\[\left\|\nabla_{\bm{W}_{Q}}\ell(\bm{\theta}^{\top})\right\|_{\text {F}} \leq 2\rho\tau_{0}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O}d_ {s}C_{x}^{2}\left\|\bm{f}^{t}-\bm{y}\right\|_{2}\,, \left\|\nabla_{\bm{W}_{K}}\ell(\bm{\theta}^{\top})\right\|_{\text {F}}\leq 2\rho\tau_{0}\bar{\lambda}_{Q}\bar{\lambda}_{V}\bar{\lambda}_{O}d_ {s}C_{x}^{2}\left\|\bm{f}^{t}-\bm{y}\right\|_{2}\,,\] \[\left\|\nabla_{\bm{W}_{V}}\ell(\bm{\theta}^{\top})\right\|_{\text {F}} \leq\rho\bar{\lambda}_{O}\left\|\bm{f}^{t}-\bm{y}\right\|_{2}\,, \left\|\nabla_{\bm{w}_{O}}\ell(\bm{\theta}^{\top})\right\|_{2}\leq\rho\bar{ \lambda}_{V}\left\|\bm{f}^{t}-\bm{y}\right\|_{2}\,.\]

Proof.: To simplify the notation, in the proof below, we hide the index \(t\). Firstly, consider the gradient w.r.t \(\bm{W}_{V}\),

\[\left\|\nabla_{\bm{W}_{V}}\ell(\bm{\theta})\right\|_{\text{F}}= \left\|-\sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\frac{\partial f(\bm{X}_{n})}{ \partial\bm{W}_{V}}\right\|_{\text{F}}\] \[=\tau_{1}\left\|\sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\sum_{i=1}^{d_ {s}}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}_{n}^{\top}\bm{ \beta}_{i,n}\right)\right)\bm{\beta}_{i,n}^{\top}\bm{X}_{n}\,\right\|_{\text{F }}\leq\tau_{1}\sum_{n=1}^{N}\left|(f(\bm{X}_{n})-y_{n})\right|d_{s}^{3/2}\bar{ \lambda}_{O}C_{x}\] \[\leq\tau_{1}\sqrt{N\sum_{n=1}^{N}\left|(f(\bm{X}_{n})-y_{n}) \right|^{2}}d_{s}^{3/2}\bar{\lambda}_{O}C_{x}=\tau_{1}d_{s}^{3/2}\bar{\lambda} _{O}C_{x}\sqrt{N}\left\|\bm{f}-\bm{y}\right\|_{2}=\rho\bar{\lambda}_{O}\left\| \bm{f}-\bm{y}\right\|_{2}\,,\] (7)

where the last equality is by the definition of \(\rho\) in Proposition. 1. Next, consider the gradient w.r.t \(\bm{w}_{O}\),

\[\left\|\nabla_{\bm{w}_{O}}\ell(\bm{\theta})\right\|_{2}=\left\|- \sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\frac{\partial f(\bm{X}_{n})}{\partial\bm {w}_{O}}\right\|_{2}=\tau_{1}\left\|\sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\sum_{i =1}^{d_{s}}\sigma_{r}\left(\bm{W}_{V}\bm{X}_{n}^{\top}\bm{\beta}_{i,n}\right) \right\|_{2}\] \[\leq\tau_{1}\sum_{n=1}^{N}\left|(f(\bm{X}_{n})-y_{n})\right|d_{s}^ {3/2}\bar{\lambda}_{V}C_{x}\leq\tau_{1}\sqrt{N\sum_{n=1}^{N}\left|(f(\bm{X}_{n} )-y_{n})\right|^{2}}d_{s}^{3/2}\bar{\lambda}_{V}C_{x}=\rho\bar{\lambda}_{V} \left\|\bm{f}-\bm{y}\right\|_{2}\,,\] (8)Next, consider the gradient w.r.t \(\bm{W}_{Q}\),

\[\left\|\nabla_{\bm{W}_{Q}}\ell(\bm{\theta})\right\|_{\mathrm{F}}= \left\|-\sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\frac{\partial f(\bm{X}_{n})}{ \partial\bm{W}_{Q}}\right\|_{\mathrm{F}}\] \[=\tau_{0}\tau_{1}\left\|\sum_{n=1}^{N}(f(\bm{X}_{n})-y_{n})\sum_{ i=1}^{d_{s}}\bm{W}_{k}\bm{X}_{n}^{\top}\left(\mathrm{diag}(\bm{\beta}_{i,n})-\bm{ \beta}_{i,n}\bm{\beta}_{i,n}^{\top}\right)\bm{X}_{n}\bm{W}_{V}^{\top}\left( \bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}_{n}^{\top}\bm{\beta}_{i,n }\right)\right)\bm{X}_{n}^{(i,:)}\right\|_{\mathrm{F}}\] \[\leq 2\tau_{0}\tau_{1}\sum_{n=1}^{N}\left|(f(\bm{X}_{n})-y_{n}) \right|d_{s}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O}(C_{x}\sqrt{d_ {s}})^{3}\] \[\leq 2\tau_{0}\tau_{1}\sqrt{N\sum_{n=1}^{N}\left|(f(\bm{X}_{n})-y_ {n})\right|^{2}}d_{s}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O}(C_{x} \sqrt{d_{s}})^{3}\] \[=2\rho\tau_{0}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O} d_{s}C_{x}^{2}\left\|\bm{f}-\bm{y}\right\|_{2}\,.\] (9)

Similarly, for the gradient w.r.t \(\bm{W}_{K}\), we have:

\[\left\|\nabla_{\bm{W}_{K}}\ell(\bm{\theta})\right\|_{\mathrm{F}}\leq 2\rho \tau_{0}\bar{\lambda}_{Q}\bar{\lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2} \left\|\bm{f}-\bm{y}\right\|_{2}\,.\] (10)

**Lemma 13** (Upper bound for the network function gradient norm).: _If \(\left\|\bm{W}_{Q}^{t}\right\|_{2}\leq\bar{\lambda}_{Q},\left\|\bm{W}_{K}^{t} \right\|_{2}\leq\bar{\lambda}_{K},\left\|\bm{W}_{V}^{t}\right\|_{2}\leq\bar{ \lambda}_{V},\left\|\bm{w}_{O}^{t}\right\|_{2}\leq\bar{\lambda}_{O},\) then one has:_

\[\left\|\nabla_{\bm{\theta}}\bm{f}^{t}\right\|_{2}\leq c_{2}\,,\]

_where_

\[c_{2}\triangleq\rho\sqrt{\bar{\lambda}_{O}^{2}+\bar{\lambda}_{V}^{2}+(2\tau_{ 0}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2})^{2}+(2 \tau_{0}\bar{\lambda}_{Q}\bar{\lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2})^{2 }}\,.\] (11)

Proof.: To simplify the notation, in the proof below, we hide the index \(t\). Firstly, note that:

\[\left\|\nabla_{\bm{\theta}}\bm{f}\right\|_{2}\leq\left\|\nabla_{ \bm{\theta}}\bm{f}\right\|_{\mathrm{F}}\] \[=\sqrt{\sum_{n=1}^{N}\left(\left\|\nabla_{\bm{w}_{O}}f(\bm{X}_{n} ;\bm{\theta})\right\|_{2}^{2}+\left\|\nabla_{\bm{w}_{Q}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}^{2}+\left\|\nabla_{\bm{w}_{K}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}^{2}+\left\|\nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}^{2}\right)}.\] (12)

Then following the step as in Eqs. (7) to (10), each term can be bounded as follows:

\[\sum_{n=1}^{N}\left(\left\|\nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}^{2}\right)\leq(\rho\bar{\lambda}_{O})^{2}, \sum_{n=1}^{N}\left(\left\|\nabla_{\bm{w}_{O}}f(\bm{X}_{n};\bm{ \theta})\right\|_{2}\right)\leq(\rho\bar{\lambda}_{V})^{2}\,,\] \[\sum_{n=1}^{N}\left(\left\|\nabla_{\bm{w}_{Q}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}\right)\leq(2\rho\tau_{0}\bar{\lambda}_{K}\bar{ \lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2})^{2}, \sum_{n=1}^{N}\left(\left\|\nabla_{\bm{w}_{K}}f(\bm{X}_{n};\bm{ \theta})\right\|_{\mathrm{F}}\right)\leq(2\rho\tau_{0}\bar{\lambda}_{Q}\bar{ \lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2})^{2}\,.\]

Plugging these bounds back Eq. (12) finishes the proof. 

**Lemma 14** (Upper bound for the Lipschitzness of the network gradient).: _Suppose \(\max\left(\left\|\bm{W}_{Q}^{t}\right\|_{2},\left\|\bm{W}_{Q}^{t^{\prime}} \right\|_{2}\right)\leq\bar{\lambda}_{Q},\max\left(\left\|\bm{W}_{K}^{t} \right\|_{2},\left\|\bm{W}_{K}^{t^{\prime}}\right\|_{2}\right)\leq\bar{ \lambda}_{K},\max\left(\left\|\bm{W}_{V}^{t}\right\|_{2},\left\|\bm{W}_{V}^{t^{ \prime}}\right\|_{2}\right)\leq\bar{\lambda}_{V},\max\left(\left\|\bm{W}_{V}^{t }\right\|_{2},\left\|\bm{W}_{V}^{t^{\prime}}\right\|_{2}\right)\leq\bar{ \lambda}_{V},\max\left(\left\|\bm{w}_{O}^{t}\right\|_{2},\left\|\bm{w}_{O}^{t }\right\|_{2}\right)\leq\bar{\lambda}_{O},\) and define \(z\triangleq 2\tau_{0}C_{x}^{2}d_{s}(\bar{\lambda}_{Q}+\bar{\lambda}_{K})\), then one has_

\[\left\|\nabla_{\bm{\theta}}\bm{f}^{t^{\prime}}-\nabla_{\bm{\theta}}\bm{f}^{t} \right\|_{2}\leq c_{3}\left\|\bm{\theta}^{t^{\prime}}-\bm{\theta}^{t}\right\|_{2}\,,\] (13)_where_

\[(c_{3})^{2}\triangleq N(\tau_{1}C_{x}d_{s}^{3/2}(1+\bar{\lambda}_{V}z))^ {2}\] (14) \[\quad+N\left\{\tau_{0}\tau_{1}C_{x}d_{s}\left\{2\bar{\lambda}_{Q}d_ {s}C_{x}^{2}\left[\bar{\lambda}_{V}\left(\bar{\lambda}_{O}C_{x}\sqrt{d_{s}}(1+ \bar{\lambda}_{V}z)+1\right)+\bar{\lambda}_{O}\right]+\bar{\lambda}_{V}\bar{ \lambda}_{O}[C_{x}^{2}d_{s}+3\bar{\lambda}_{K}z]\right\}\right\}^{2}\] \[\quad+N\left\{\tau_{0}\tau_{1}C_{x}d_{s}\left\{2\bar{\lambda}_{Q}d _{s}C_{x}^{2}\left[\bar{\lambda}_{V}\left(\bar{\lambda}_{O}C_{x}\sqrt{d_{s}}(1+ \bar{\lambda}_{V}z)+1\right)+\bar{\lambda}_{O}\right]+\bar{\lambda}_{V}\bar{ \lambda}_{O}[C_{x}^{2}d_{s}+3\bar{\lambda}_{K}z]\right\}\right\}^{2}\,.\]

Proof.: Firstly, note that:

\[\left\|\nabla_{\bm{\theta}}\bm{f}^{t^{\prime}}-\nabla_{\bm{\theta }}\bm{f}^{t}\right\|_{2}^{2}\] (15) \[\leq\sum_{n=1}^{N}\left(\left\|\nabla_{\bm{w}_{O}}f(\bm{X}_{n}; \bm{\theta}^{t^{\prime}})-\nabla_{\bm{w}_{O}}f(\bm{X}_{n};\bm{\theta}^{t}) \right\|_{2}^{2}+\left\|\nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{\theta}^{t^{\prime }})-\nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{\rm F}^{2}\] \[\quad\quad+\left\|\nabla_{\bm{W}_{Q}}f(\bm{X}_{n};\bm{\theta}^{t^{ \prime}})-\nabla_{\bm{W}_{Q}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{\rm F}^{2}+ \left\|\nabla_{\bm{W}_{K}}f(\bm{X}_{n};\bm{\theta}^{t^{\prime}})-\nabla_{\bm{ W}_{K}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{\rm F}^{2}\right)\,.\]

Then, we will prove each term separately. Regarding the first term in Eq. (15), we have

\[\left\|\nabla_{\bm{w}_{O}}f(\bm{X}_{n};\bm{\theta}^{t^{\prime}})- \nabla_{\bm{w}_{O}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{2}\] (16) \[=\left\|\bm{f}_{\text{pre}}^{\bm{t}^{\prime}}-\bm{f}_{\text{pre}} ^{t}\right\|_{2}\] \[\leq\tau_{1}C_{x}d_{s}^{3/2}\left(\left\|\bm{W}_{V}^{t^{\prime}}- \bm{W}_{V}^{t}\right\|_{2}+\bar{\lambda}_{V}2\tau_{0}C_{x}^{2}d_{s}\left(\bar{ \lambda}_{Q}\left\|\bm{W}_{K}^{t^{\prime}}-\bm{W}_{K}^{t}\right\|_{2}+\bar{ \lambda}_{K}\left\|\bm{W}_{Q}^{t^{\prime}}-\bm{W}_{Q}^{t}\right\|_{2}\right)\right)\] \[\leq\tau_{1}C_{x}d_{s}^{3/2}\left(1+\bar{\lambda}_{V}2\tau_{0}C_{ x}^{2}d_{s}\left(\bar{\lambda}_{Q}+\bar{\lambda}_{K}\right)\right)\left\|\bm{ \theta}^{t^{\prime}}-\bm{\theta}^{t}\right\|_{2}\] \[=\tau_{1}C_{x}d_{s}^{3/2}(1+\bar{\lambda}_{V}z)\left\|\bm{\theta} ^{t^{\prime}}-\bm{\theta}^{t}\right\|_{2}\,.\]

where the first inequality is by Eq. (6), and in the last equality is by the definition of \(z\) in the lemma. Regarding the second term in Eq. (15), we have:

\[\left\|\nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{\theta}^{t^{\prime}})- \nabla_{\bm{W}_{V}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{\rm F}\] (17) \[=\tau_{1}\left\|\sum_{i=1}^{d_{s}}\left(\bm{w}_{O}^{t^{\prime}} \circ\sigma_{r}\left(\bm{W}_{V}^{t^{\prime}}\bm{X}_{n}^{\top}\bm{\beta}_{i,n}^{ t^{\prime}}\right)\right)\bm{\beta}_{i,n}^{t^{\prime}}{}^{\top}\bm{X}_{n}-\sum_{i=1}^{d_{ s}}\left(\bm{w}_{O}^{t}\circ\sigma_{r}\left(\bm{W}_{V}^{t}\bm{X}_{n}^{\top}\bm{ \beta}_{i,n}^{t}\right)\right)\bm{\beta}_{i,n}^{t^{\top}}\bm{X}_{n}\right\|_{ \rm F}\] \[\leq\tau_{1}C_{x}\sqrt{d_{s}}\sum_{i=1}^{d_{s}}\left\|\left(\bm{w }_{O}^{t^{\prime}}\circ\sigma_{r}\left(\bm{W}_{V}^{t^{\prime}}\bm{X}_{n}^{\top} \bm{\beta}_{i,n}^{t^{\prime}}\right)\right)\bm{\beta}_{i,n}^{t^{\prime}}{}^{ \top}-\left(\bm{w}_{O}^{t}\circ\sigma_{r}\left(\bm{W}_{V}^{t}\bm{X}_{n}^{\top} \bm{\beta}_{i,n}^{t}\right)\right)\bm{\beta}_{i,n}^{t^{\prime}}\right\|_{\rm F}\] \[\leq\tau_{1}C_{x}\sqrt{d_{s}}\sum_{i=1}^{d_{s}}\left[\bar{\lambda}_ {O}\left\|\bm{\beta}_{i,n}^{t^{\prime}}-\bm{\beta}_{i,n}^{t}\right\|_{2}+\left\| \bm{w}_{O}^{t^{\prime}}\circ\sigma_{r}\left(\bm{W}_{V}^{t^{\prime}}\bm{X}_{n}^{ \top}\bm{\beta}_{i,n}^{t^{\prime}}\right)-\bm{w}_{O}^{t}\circ\sigma_{r}\left(\bm {W}_{V}^{t}\bm{X}_{n}^{\top}\bm{\beta}_{i,n}^{t^{\prime}}\right)\right\|_{2} \right].\]

The term \(\left\|\bm{\beta}_{i,n}^{t^{\prime}}-\bm{\beta}_{i,n}^{t}\right\|_{2}\) can be bounded by Lemma 7 as follows:

\[\left\|\bm{\beta}_{i,n}^{t^{\prime}}-\bm{\beta}_{i,n}^{t}\right\|_{2}\leq 2\tau_{0}C_{x}^{2}d_{s} \left(\bar{\lambda}_{Q}\left\|\bm{W}_{K}^{t^{\prime}}-\bm{W}_{K}^{t}\right\|_{2}+ \bar{\lambda}_{K}\left\|\bm{W}_{Q}^{t^{\prime}}-\bm{W}_{Q}^{t}\right\|_{2} \right)\leq z\left\|\bm{\theta}^{t^{\prime}}-\bm{\theta}^{t}\right\|_{2}\,.\] (18)

[MISSING_PAGE_FAIL:24]

where the second inequality uses the result in Eq. (19).

\[\begin{split}&\left\|\bm{U}_{i,n}^{t^{\prime}}-\bm{U}_{i,n}^{t} \right\|_{2}\\ &\leq\left\|\bm{W}_{K}^{t^{\prime}}\bm{X}_{n}^{\top}\left(\text{ diag}(\bm{\beta}_{i,n}^{t^{\prime}})-\bm{\beta}_{i,n}^{t^{\prime}}{\bm{\beta}_{i,n}^{t^{ \prime}}}^{\top}\right)-\bm{W}_{K}^{t}\bm{X}_{n}^{\top}\left(\text{diag}(\bm{ \beta}_{i,n}^{t})-\bm{\beta}_{i,n}^{t}{\bm{\beta}_{i,n}^{t}}^{\top}\right) \right\|_{2}\left\|\bm{X}_{n}\right\|_{2}\\ &\leq C_{x}\sqrt{d_{s}}\left[\left\|\bm{W}_{K}^{t^{\prime}}-\bm{W }_{K}^{t}\right\|_{2}\left\|\bm{X}_{n}\right\|_{2}\left\|\text{diag}(\bm{\beta} _{i,n}^{t^{\prime}})-\bm{\beta}_{i,n}^{t^{\prime}}{\bm{\beta}_{i,n}^{t^{\prime }}}^{\top}\right\|_{2}\\ &\qquad\qquad+\left\|\bm{W}_{K}^{t}\right\|_{2}\left\|\bm{X}_{n} \right\|_{2}\left\|\text{diag}(\bm{\beta}_{i,n}^{t^{\prime}})-\bm{\beta}_{i,n}^ {t^{\prime}}{\bm{\beta}_{i,n}^{t^{\prime}}}^{\top}-\text{diag}(\bm{\beta}_{i, n}^{t})-\bm{\beta}_{i,n}^{t}{\bm{\beta}_{i,n}^{t^{\prime}}}^{\top}\right\|_{2} \right]\\ &\leq C_{x}^{2}d_{s}\left[\left\|\bm{\theta}^{t^{\prime}}-\bm{ \theta}^{t}\right\|_{2}+\bar{\lambda}_{K}\left(\left\|\text{diag}(\bm{\beta}_{ i,n}^{t^{\prime}})-\text{diag}(\bm{\beta}_{i,n}^{t})\right\|_{2}+\left\|\bm{ \beta}_{i,n}^{t^{\prime}}{\bm{\beta}_{i,n}^{t^{\prime}}}^{\top}-\bm{\beta}_{i, n}^{t}{\bm{\beta}_{i,n}^{t^{\prime}}}^{\top}\right\|_{2}\right)\right] \\ &\leq C_{x}^{2}d_{s}\left[\left\|\bm{\theta}^{t^{\prime}}-\bm{ \theta}^{t}\right\|_{2}+\bar{\lambda}_{K}\left(\left\|\bm{\beta}_{i,n}^{t^{ \prime}}-\bm{\beta}_{i,n}^{t}\right\|_{\infty}+\left(\left\|\bm{\beta}_{i,n}^ {t}\right\|_{2}+\left\|\bm{\beta}_{i,n}^{t}\right\|_{2}\right)\left\|\bm{\beta }_{i,n}^{t^{\prime}}-\bm{\beta}_{i,n}^{t}\right\|_{2}\right)\right]\\ &\leq C_{x}^{2}d_{s}\left[\left\|\bm{\theta}^{t^{\prime}}-\bm{ \theta}^{t}\right\|_{2}+3\bar{\lambda}_{K}\left\|\bm{\beta}_{i,n}^{t^{\prime}}- \bm{\beta}_{i,n}^{t}\right\|_{2}\right]\\ &\leq C_{x}^{2}d_{s}\left[\left\|\bm{\theta}^{t^{\prime}}-\bm{ \theta}^{t}\right\|_{2}+3\bar{\lambda}_{K}2\tau_{0}C_{x}^{2}d_{s}\left(\bar{ \lambda}_{Q}\left\|\bm{W}_{K}^{t^{\prime}}-\bm{W}_{K}^{t}\right\|_{2}+\bar{ \lambda}_{K}\left\|\bm{W}_{Q}^{t^{\prime}}-\bm{W}_{Q}^{t}\right\|_{2}\right) \right]\\ &\leq[C_{x}^{2}d_{s}+3\bar{\lambda}_{K}z]\left\|\bm{\theta}^{t^{ \prime}}-\bm{\theta}^{t}\right\|_{2}\,,\end{split}\] (23)

where the last second inequality is by Lemma 7. Plugging back Eq. (21), we obtain:

\[\begin{split}&\left\|\nabla_{\bm{W}_{Q}}f(\bm{X}_{n};\bm{\theta }^{t^{\prime}})-\nabla_{\bm{W}_{Q}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{F}\\ &\leq\tau_{0}\tau_{1}C_{x}d_{s}\left\{2\bar{\lambda}_{K}d_{s}C_{x }^{2}\left[\bar{\lambda}_{V}\left(\bar{\lambda}_{O}C_{x}\sqrt{d_{s}}(1+\bar{ \lambda}_{V}z)+1\right)+\bar{\lambda}_{O}\right]+\bar{\lambda}_{V}\bar{ \lambda}_{O}[C_{x}^{2}d_{s}+3\bar{\lambda}_{K}z]\right\}\left\|\bm{\theta}^{t^ {\prime}}-\bm{\theta}^{t}\right\|_{2}\,.\end{split}\] (24)

Similarly, the fourth term in Eq. (15) can be bounded by:

\[\begin{split}&\left\|\nabla_{\bm{W}_{K}}f(\bm{X}_{n};\bm{\theta }^{t^{\prime}})-\nabla_{\bm{W}_{K}}f(\bm{X}_{n};\bm{\theta}^{t})\right\|_{F}\\ &\leq\tau_{0}\tau_{1}C_{x}d_{s}\left\{2\bar{\lambda}_{Q}d_{s}C_{x }^{2}\left[\bar{\lambda}_{V}\left(\bar{\lambda}_{O}C_{x}\sqrt{d_{s}}(1+\bar{ \lambda}_{V}z)+1\right)+\bar{\lambda}_{O}\right]+\bar{\lambda}_{V}\bar{\lambda}_ {O}[C_{x}^{2}d_{s}+3\bar{\lambda}_{K}z]\right\}\left\|\bm{\theta}^{t^{\prime}}- \bm{\theta}^{t}\right\|_{2}\,.\end{split}\] (25)

Plugging the upper bound for these four terms back Eq. (15) finishes the proof. 

### Proof of Proposition. 1

Proof.: We can reformulate Eq. (1.2) as:

\[f(\bm{X})=\tau_{1}\bm{w}_{O}^{\top}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\bm{W}_{V} \bm{X}^{\top}\bm{\beta}_{i}\right)=\bm{w}_{O}^{\top}\bm{f}_{\rm pre}\,,\]

with \(\bm{\beta}_{i}:=\sigma_{s}\left(\tau_{0}\bm{X}^{(i,\cdot)}\bm{W}_{Q}^{\top}\bm{W} _{K}\bm{X}^{\top}\right)^{\top}\in\mathbb{R}^{d_{s}}\). We show by induction for every \(t\geq 0\)

\[\begin{cases}\left\|\bm{W}_{Q}^{s}\right\|_{2}\leq\bar{\lambda}_{Q},\left\|\bm{ W}_{Q}^{s}\right\|_{2}\leq\bar{\lambda}_{K},\quad s\in[0,t],\\ \left\|\bm{W}_{V}^{s}\right\|_{2}\leq\bar{\lambda}_{V},\left\|\bm{w}_{0}^{s} \right\|_{2}\leq\bar{\lambda}_{Q},\quad s\in[0,t],\\ \sigma_{\rm min}\left(\bm{F}_{\rm pre}^{s}\right)\geq\frac{1}{2}\alpha,\quad s \in[0,t],\\ \ell(\bm{\theta}^{s})\leq\left(1-\gamma\frac{\alpha^{2}}{2}\right)^{s}\ell( \bm{\theta}^{0}),\quad s\in[0,t].\end{cases}\] (26)

It is clear that Eq. (26) holds for \(t=0\). Assume that Eq. (26) holds up to iteration \(t\). By the triangle inequality, we have

\[\begin{split}&\left\|\bm{W}_{Q}^{t+1}-\bm{W}_{Q}^{0}\right\|_{ \rm F}\leq\sum_{s=0}^{t}\left\|\bm{W}_{Q}^{s+1}-\bm{W}_{Q}^{s}\right\|_{\rm F }=\gamma\sum_{s=0}^{t}\left\|\nabla_{\bm{W}_{Q}}\ell(\bm{\theta}^{s})\right\|_{ \rm F}\\ &\leq 2\gamma\rho\tau_{0}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{ \lambda}_{O}d_{s}C_{x}^{2}\sum_{s=0}^{t}\sqrt{2\ell(\bm{\theta}^{s})}\leq 2 \gamma\rho\tau_{0}\bar{\lambda}_{K}\bar{\lambda}_{V}\bar{\lambda}_{O}d_{s}C_{x}^{2} \sum_{s=0}^{t}\left(1-\gamma\frac{\alpha^{2}}{2}\right)^{s/2}\sqrt{2\ell(\bm{ \theta}^{0})}\,,\end{split}\] (27)

[MISSING_PAGE_EMPTY:26]

where the last inequality is by induction rule. Then, we bound each term separately. Note that:

\[\left\|\bm{W}_{Q}^{t+\phi}-\bm{W}_{Q}^{0}\right\|_{\mathrm{F}}\leq \left\|\bm{W}_{Q}^{t+\phi}-\bm{W}_{Q}^{t}\right\|_{\mathrm{F}}+\sum_{s=0}^{t-1} \left\|\bm{W}_{Q}^{s+1}-\bm{W}_{Q}^{s}\right\|_{\mathrm{F}}\] \[=\phi\gamma\left\|\nabla_{\bm{W}_{Q}}\ell(\bm{\theta}^{\top}) \right\|_{\mathrm{F}}+\gamma\sum_{s=0}^{t-1}\left\|\nabla_{\bm{W}_{Q}}\ell( \bm{\theta}^{s})\right\|_{\mathrm{F}}\leq\gamma\sum_{s=0}^{t}\left\|\nabla_{ \bm{W}_{Q}}\ell(\bm{\theta}^{s})\right\|_{\mathrm{F}}\,.\]

Then following exact the same step as in Eqs. (26), (27) and (30), we have: \(\left\|\bm{W}_{Q}^{t+\phi}\right\|_{2}\leq\bar{\lambda}_{Q}\). By the same method, we have: \(\left\|\bm{W}_{K}^{t+\phi}\right\|_{2}\leq\bar{\lambda}_{K}\), \(\left\|\bm{W}_{V}^{t+\phi}\right\|_{2}\leq\bar{\lambda}_{V}\), \(\left\|\bm{w}_{O}^{t+\phi}\right\|_{2}\leq\bar{\lambda}_{O}\). Now we proceed to bound the first term in Eq. (35).

\[\left\|\bm{f}^{t+\phi}-\bm{f}^{t}\right\|_{2}\] (36) \[\leq\rho\left\|\bm{W}_{V}^{t+\phi}\right\|_{2}\left\|\bm{w}_{O}^ {t+\phi}-\bm{w}_{O}^{t}\right\|_{2}+\left\|\bm{w}_{O}^{t}\right\|_{2}\left\| \bm{F}_{\mathrm{pre}}^{t+\phi}-\bm{F}_{\mathrm{pre}}^{t}\right\|_{2}\] \[\leq\rho\left\|\bm{W}_{V}^{t+\phi}\right\|_{2}\left\|\bm{w}_{O}^{ t+\phi}-\bm{w}_{O}^{t}\right\|_{2}\] \[\quad\quad+\rho\left\|\bm{w}_{O}^{t}\right\|_{2}\left(\left\|\bm{ W}_{V}^{t+\phi}-\bm{W}_{V}^{t}\right\|_{2}+\left\|\bm{W}_{V}^{t}\right\|_{2}2 \tau_{0}C_{a}^{2}d_{s}\left(\left\|\bm{W}_{Q}^{t+\phi}\right\|_{2}\left\|\bm{ W}_{K}^{t+\phi}-\bm{W}_{K}^{t}\right\|_{2}+\left\|\bm{W}_{K}^{t}\right\|_{2} \left\|\bm{W}_{Q}^{t+\phi}-\bm{W}_{Q}^{t}\right\|_{2}\right)\right)\] \[\leq\rho\bar{\lambda}_{V}\left\|\bm{\theta}^{t+\phi}-\bm{\theta}^ {\top}\right\|_{2}+\rho\bar{\lambda}_{O}(1+\bar{\lambda}_{V}2\tau_{0}C_{a}^{2 }d_{s}(\bar{\lambda}_{Q}+\bar{\lambda}_{K}))\left\|\bm{\theta}^{t+\phi}-\bm{ \theta}^{\top}\right\|_{2}\] \[=\rho(\bar{\lambda}_{V}+\bar{\lambda}_{Q}+\bar{\lambda}_{V}2\tau _{0}C_{a}^{2}d_{s}(\bar{\lambda}_{Q}+\bar{\lambda}_{K}))\left\|\bm{\theta}^{t+ \phi}-\bm{\theta}^{\top}\right\|_{2}\] \[\triangleq c_{1}\left\|\bm{\theta}^{t+\phi}-\bm{\theta}^{\top} \right\|_{2}\,,\]

where the first inequality is by Lemma 10, the second inequality is by Lemma 9. Next, the second term in Eq. (35) can be bounded by Lemma 13. The third term in Eq. (35) can be bounded by Lemma 14. As a result, Eq. (35) has the following upper bound:

\[\left\|\nabla_{\bm{\theta}}\ell(\bm{\theta}^{t+\phi})-\nabla_{\bm{\theta}}\ell( \bm{\theta}^{\top})\right\|_{2}\leq c_{1}c_{2}\left\|\bm{\theta}^{t+\phi}- \bm{\theta}^{\top}\right\|_{2}+2c_{3}\ell(\bm{\theta}^{0})\left\|\bm{\theta}^{t +\phi}-\bm{\theta}^{\top}\right\|_{2}\,\triangleq C\left\|\bm{\theta}^{t+\phi }-\bm{\theta}^{\top}\right\|_{2}\,,\] (37)

where \(c_{1},c_{2},c_{3}\) are defined at Eqs. (11), (14) and (36), and we further define \(C\triangleq c_{1}c_{2}+2c_{3}\ell(\bm{\theta}^{0})\). Lastly, by applying Lemma 4.3 in Nguyen and Mondelli (2020) and Eq. (37), we have:

\[\ell(\bm{\theta}^{t+1})\leq\ell(\bm{\theta}^{\top})+\langle\nabla_ {\bm{\theta}}\ell(\bm{\theta}^{\top}),\bm{\theta}^{t+1}-\bm{\theta}^{\top} \rangle+\frac{C}{2}\left\|\bm{\theta}^{t+1}-\bm{\theta}^{\top}\right\|_{ \mathrm{F}}^{2}\] \[=\ell(\bm{\theta}^{\top})-\gamma\left\|\nabla_{\bm{\theta}}\ell( \bm{\theta}^{\top})\right\|_{\mathrm{F}}^{2}+\frac{C}{2}\gamma^{2}\left\| \nabla_{\bm{\theta}}\ell(\bm{\theta}^{\top})\right\|_{\mathrm{F}}^{2}\] \[\leq\ell(\bm{\theta}^{\top})-\frac{1}{2}\gamma\left\|\nabla_{\bm{ \theta}}\ell(\bm{\theta}^{\top})\right\|_{\mathrm{F}}^{2}\quad\text{(By the condition on $\gamma$)}\] \[\leq\ell(\bm{\theta}^{\top})-\frac{1}{2}\gamma\left\|\nabla_{\bm{ w}_{o}}\ell(\bm{\theta}^{\top})\right\|_{2}^{2}\] \[=\ell(\bm{\theta}^{\top})-\frac{1}{2}\gamma\left\|(\bm{F}_{ \mathrm{pre}}^{t})^{\top}(\bm{f}^{t}-\bm{y})\right\|_{2}^{2}\] \[\leq(1-\gamma\frac{\alpha}{2})\ell(\bm{\theta}^{\top})\quad\text{( By induction assumption)}\,,\]

which concludes the proof.

### Proof of Theorem 1

**Lemma 15**.: _Let \(\bm{\Phi}=[\bm{X}_{1}^{\top}\bm{\beta}_{1,1},...,\bm{X}_{N}^{\top}\bm{\beta}_{1,N}]^{\top}\in\mathbb{R}^{N\times d}\quad\), where \(\bm{\beta}_{1,n}=\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(1,\cdot)}\bm{W}_{Q}^{\top}\bm{W}_{ K}\bm{X}_{n}^{\top}\right)^{\top},n\in[N]\), then under Assumptions 2 and 3, with probability at least \(1-\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\), one has: \(\lambda_{0}:=\lambda_{\min}\left(\mathbb{E}_{\bm{w}\sim\mathcal{N}(0,\eta\times 1 _{d_{s}})}[\sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{T}]\right)\geq \Theta(\eta_{V}/d_{s})\)._

Proof.: Due to Assumption 2, for any data \(\bm{X}\), the matrix \(\bm{X}\bm{X}^{\top}\) is positive definite and thus has positive minimum eigenvalue. We denote it as \(\lambda_{\min}(\bm{X}\bm{X}^{\top})\geq C_{\lambda}\).

According to (Nguyen et al., 2021, Lemma 5.3), using the Hermite expansion of \(\sigma_{r}\), one has:

\[\lambda_{0}\geq\eta_{V}\mu(\sigma_{r})^{2}\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{ \top})\,,\] (38)

where \(\mu(\sigma_{r})\) is the 1-st Hermite coefficient of ReLU satisfying \(\mu(\sigma_{r})>0\).

Now we proceed to provide a lower bound for \(\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\). For notational simplicity, define:

\[\bm{B}_{ij}=\bm{\beta}_{1,i}\bm{\beta}_{1,j}^{\top}\in\mathbb{R}^{d_{s}\times d _{s}},\bm{C}_{ij}=\bm{X}_{i}\bm{X}_{j}^{\top}\in\mathbb{R}^{d_{s}\times d_{s}}.\]

Then we can rewrite \(\bm{\Phi}\bm{\Phi}^{\top}\) as follows:

\[\bm{\Phi}\bm{\Phi}^{\top}=\begin{bmatrix}\operatorname{Trace}(\bm{B}_{\bm{ \downarrow}}^{\top}\bm{C}_{11})&\operatorname{Trace}(\bm{B}_{\bm{\downarrow} }^{\top}\bm{C}_{12})&\cdots&\operatorname{Trace}(\bm{B}_{\bm{\downarrow}}^{ \top}\bm{C}_{1N})\\ \operatorname{Trace}(\bm{B}_{\bm{\downarrow}}^{\top}\bm{C}_{21})&\operatorname {Trace}(\bm{B}_{\bm{\downarrow}2}^{\top}\bm{C}_{22})&\cdots&\operatorname{ Trace}(\bm{B}_{2N}^{\top}\bm{C}_{2N})\\ \vdots&\vdots&\vdots&\vdots\\ \operatorname{Trace}(\bm{B}_{N1}^{\top}\bm{C}_{N1})&\operatorname{Trace}(\bm {B}_{N2}^{\top}\bm{C}_{N2})&\cdots&\operatorname{Trace}(\bm{B}_{NN}^{\top}\bm{ C}_{NN})\end{bmatrix}.\]

By Gershgorin circle theorem (Gershgorin, 1931), there exists \(k\in[N]\) such that:

\[\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\geq\operatorname{Trace}(\bm{B}_{kk} ^{\top}\bm{C}_{kk})-\sum_{j\neq k}\operatorname{Trace}(\bm{B}_{kj}^{\top}\bm{C }_{kj})\,.\] (39)

Using Von Neumann's trace inequality (Mirsky, 1975) and noting that \(\bm{B}_{kj}\) is a rank one matrix, one has:

\[\begin{split}&\operatorname{Trace}(\bm{B}_{kj}^{\top}\bm{C}_{ kj})\leq\sigma_{\max}(\bm{B}_{kj})\sigma_{\max}(\bm{C}_{kj})=\left\|\bm{\beta}_{1,k} \right\|_{2}\left\|\bm{\beta}_{1,j}\right\|_{2}\sqrt{\lambda_{\max}(\bm{C}_{ kj}\bm{C}_{kj}^{\top})}\\ &\leq\left\|\bm{\beta}_{1,k}\right\|_{2}\sqrt{\operatorname{ Trace}(\bm{C}_{kj}\bm{C}_{kj}^{\top})}=\left\|\bm{\beta}_{1,k} \right\|_{2}\sqrt{\left\langle\bm{X}_{k}^{\top}\bm{X}_{k},\bm{X}_{j}^{\top} \bm{X}_{j}\right\rangle}\,,\end{split}\] (40)

where we use the definition of the inner product between two matrices and \(\left\|\bm{\beta}_{1,j}\right\|_{2}\leq 1\). By Assumption 2, we have \(\lambda_{\min}(\bm{X}_{k}\bm{X}_{k}^{\top})\geq C_{\lambda}\), where \(C_{\lambda}\) is some positive constant. By setting \(t:=\left\|\bm{\beta}_{1,k}\right\|_{2}^{2}C_{\lambda}^{2}/(N-1)^{2}\) in Assumption 3, with probability at least \(1-\exp\left(-\Omega((N-1)^{-\hat{c}})\right)\), one has

\[\sqrt{\left\langle\bm{X}_{k}^{\top}\bm{X}_{k},\bm{X}_{j}^{\top}\bm{X}_{j} \right\rangle}\leq\left\|\bm{\beta}_{1,k}\right\|_{2}C_{\lambda}(N-1)^{-1}\,, \quad\forall j\neq k\,.\]

Plugging back Eq. (39) and Eq. (40), we obtain:

\[\begin{split}&\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\geq \operatorname{Trace}(\bm{B}_{kk}^{\top}\bm{C}_{kk})-C_{\lambda}\left\|\bm{\beta }_{1,k}\right\|_{2}^{2}\geq\lambda_{\min}(\bm{C}_{kk})\operatorname{Trace}( \bm{B}_{kk})-C_{\lambda}\left\|\bm{\beta}_{1,k}\right\|_{2}^{2}\\ &=\lambda_{\min}(\bm{X}_{k}\bm{X}_{k}^{\top})\left\|\bm{\beta}_{1,k }\right\|_{2}^{2}-C_{\lambda}\left\|\bm{\beta}_{1,k}\right\|_{2}^{2}\geq \Theta(\left\|\bm{\beta}_{1,k}\right\|_{2}^{2})\geq\Theta(1/d_{s})\,,\end{split}\] (41)

where the last inequality is by the lower bound of \(\bm{\beta}_{1,k}\) in Lemma 8. Lastly, plugging the lower bound of \(\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\) back Eq. (38) finishes the proof.

**Remark:** The estimation of \(\lambda_{0}\) is actually tight because its upper bound is also in a constant order. To be specific, denote \(\bm{G}:=\sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{\top}\), we have

\[\lambda_{0}:=\lambda_{\min}\left(\mathbb{E}_{\bm{w}}\bm{G}\right)\leq\frac{ \operatorname{tr}(\mathbb{E}_{\bm{w}}\bm{G})}{N}=\frac{\sum_{n=1}^{N}\mathbb{E}_ {\bm{w}}[\sigma_{r}(\bm{\Phi}^{(n,:)}\bm{w})]^{2}}{N}\,,\] (42)

where \(\bm{\Phi}^{(n,:)}=\bm{\beta}_{1,n}^{\top}\bm{X}_{n}\). Next, by Liao and Couillet (2018) (Sec.A in Supplementary Material):

\[\mathbb{E}_{\bm{w}}[\sigma_{r}(\bm{\Phi}^{(n,:)}\bm{w})]^{2}=\frac{\eta_{V}}{2 \pi}\left\|\bm{\Phi}^{(n,:)}\right\|^{2}\arccos(-1)=\frac{\left\|\bm{\Phi}^{(n,: )}\right\|^{2}}{2}\,.\] (43)Combine Eq. (42) and Eq. (43), we have

\[\lambda_{0}\leq\frac{\sum_{n=1}^{N}\eta_{V}\|\bm{\Phi}^{(n,:)}\|_{2}^{2}}{2N}\leq \eta_{V}d_{s}C_{x}^{2}\leq\mathcal{O}(1)\,.\]

That means, our estimation on \(\lambda_{0}\) is tight as its upper and lower bounds match with each other.

Now we are ready to present the proof for LeCun initialization under \(\tau_{0}=d_{m}^{-1/2}\) scaling.

Proof.: We select \(C_{Q}=C_{K}=1=C_{V}=C_{O}=1\), then by Lemma 5, with probability at least \(1-8e^{-d_{m}/2}\), we have:

\[\begin{split}\bar{\lambda}_{V}&=\mathcal{O}(\sqrt{ d_{m}/d}),\quad\bar{\lambda}_{O}=\mathcal{O}(1)\,,\\ \bar{\lambda}_{Q}&=\mathcal{O}(\sqrt{d_{m}/d}), \quad\bar{\lambda}_{K}=\mathcal{O}(\sqrt{d_{m}/d})\,.\end{split}\] (44)

Plugging Eq. (44) into Eqs. (2) and (3), it suffices to prove the following equations.

\[\alpha^{2} \geq\mathcal{O}\!\left(\sqrt{N}d_{s}^{3/2}C_{x}\sqrt{d_{m}/d} \right)\sqrt{2\ell(\bm{\theta}^{0})}\] (45) \[\alpha^{3} \geq\mathcal{O}\left(Nd_{s}^{3}C_{x}^{2}(1+4C_{x}^{4}d_{s}^{2}d _{m}d^{-2})\right)\sqrt{2\ell(\bm{\theta}^{0})}\] (46)

Next, we will provide the lower bound for \(\alpha^{2}=\lambda_{\min}((\bm{F}^{0}_{\mathrm{pre}})(\bm{F}^{0}_{\mathrm{pre }})^{\top})\). In the following context, we hide the index \(0\) for simplification. One can note that \(\bm{F}_{\mathrm{pre}}\bm{F}^{\top}_{\mathrm{pre}}\) is the summation of PSD matrices, thus, it suffices to lower bound: \(\lambda_{\min}(\hat{\bm{F}}_{\mathrm{pre}}\hat{\bm{F}}^{\top}_{\mathrm{pre}})\), where we introduce the following notations:

\[\begin{split}&\hat{\bm{F}}_{\mathrm{pre}}\hat{\bm{F}}^{\top}_{ \mathrm{pre}}=\tau_{1}^{2}\sigma_{r}(\bm{\Phi}\bm{W}_{v}^{\top})\sigma_{r}(\bm {\Phi}\bm{W}_{v}^{\top})^{\top}\\ &\bm{\Phi}=[\bm{X}_{1}^{\top}\bm{\beta}_{1,1},...,\bm{X}_{N}^{ \top}\bm{\beta}_{1,N}]^{\top}\\ &\bm{\beta}_{1,n}=\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(1,:)}\bm{ W}_{Q}^{\top}\bm{W}_{K}\bm{X}_{n}^{\top}\right)^{\top}n\in[N].\end{split}\] (47)

By Matrix-Chernoff inequality, we can obtain that (e.g. Lemma 5.2 of Nguyen et al. [2021]) w.p at least \(1-\delta_{1}\),

\[\lambda_{\min}(\hat{\bm{F}}_{\mathrm{pre}}\hat{\bm{F}}^{\top}_{\mathrm{pre}}) \geq d_{m}\lambda_{0}/4,\] (48)

as long as it holds \(d_{m}\geq\tilde{\Omega}(N/\lambda_{0})\), where \(\lambda_{0}=\lambda_{\min}\left(\mathbb{E}_{\bm{w}\sim\mathcal{N}(0,\eta_{V}l_ {d})}[\sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{\top}]\right)\), and \(\tilde{\Omega}\) hides logarithmic factors depending on \(\delta_{1}\). Lastly, w.p. at least \(1-\delta_{2}\), one has \(\sqrt{2\ell(\bm{\theta}^{0})}\leq\tilde{\mathcal{O}}(\sqrt{N})\). Plugging back Eqs. (2) and (3), it suffices to prove the following inequality.

\[d_{m}\lambda_{0}/4\geq\tilde{\mathcal{O}}(Nd_{s}^{3/2}C_{x}\sqrt {d_{m}/d})\,,\] (49) \[(d_{m}\lambda_{0}/4)^{3/2}\geq\tilde{\mathcal{O}}(N^{3/2}d_{s}^{ 3}C_{x}^{2}(1+4C_{x}^{4}d_{s}^{2}d_{m}d^{-2}))\,,\] (50)

By Lemma 15, with probability at least \(1-\exp{(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1}))}\), one has \(\lambda_{0}\geq\Theta(\eta_{V}/d_{s})=\Theta(d^{-1}d_{s}^{-1})\). Thus, when \(d_{m}\geq\tilde{\Omega}(N^{3})\), all of the above conditions hold. As a result, the conditions in Eqs. (45) and (46) are satisfied and the convergence of training Transformer is guaranteed as in Eq. (4). Note that one can achieve the same width requirement and probability for He initialization, and the proof bears resemblance to the LeCun initialization.

For the proof under LeCun initialization and \(\tau_{0}=d_{m}^{-1}\) scaling, we follow the same strategy. Specifically: As the same in the proof with \(\tau_{0}=d_{m}^{-1/2}\) scaling, we select \(C_{Q}=C_{K}=C_{V}=C_{O}=1\), then plugging Eq. (44) into Eqs. (2) and (3), it suffices to prove the following equations.

\[\alpha^{2} \geq\mathcal{O}\!\left(\sqrt{N}d_{s}^{3/2}C_{x}\sqrt{d_{m}/d} \right)\sqrt{2\ell(\bm{\theta}^{0})}\,,\] (51) \[\alpha^{3} \geq\mathcal{O}\left(Nd_{s}^{3}C_{x}^{2}(1+4C_{x}^{4}d_{s}^{2}d ^{-2})\right)\sqrt{2\ell(\bm{\theta}^{0})}\,.\] (52)

Next, we will provide the lower bound for \(\alpha^{2}=\lambda_{\min}((\bm{F}^{0}_{\mathrm{pre}})(\bm{F}^{0}_{\mathrm{pre}}) ^{\top})\). In the following context, we hide the index \(0\) for simplification. One can note that \(\bm{F}_{\mathrm{pre}}\bm{F}^{\top}_{\mathrm{pre}}\) is the summation of PSD matrices,thus, it suffices to lower bound: \(\lambda_{\min}(\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top})\), where we introduce the following notations:

\[\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top}=\tau_{1}^ {2}\sigma_{r}(\bm{\Phi}\bm{W}_{v}^{\top})\sigma_{r}(\bm{\Phi}\bm{W}_{v}^{\top})^ {\top}\,,\] (53) \[\bm{\Phi}=[\bm{X}_{1}^{\top}\bm{\beta}_{1,1},...,\bm{X}_{N}^{\top} \bm{\beta}_{1,N}]^{\top}\,,\] \[\bm{\beta}_{1,n}=\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(1,:)}\bm{W} _{Q}^{\top}\bm{W}_{K}\bm{X}_{n}^{\top}\right)^{\top}n\in[N]\,.\]

By Eq. (48) w.p at least \(1-\delta_{1}\), \(\lambda_{\min}(\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top})\geq d _{m}\lambda_{0}/4\),, as long as it holds \(d_{m}\geq\tilde{\Omega}(N/\lambda_{0})\), where \(\lambda_{0}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim\mathcal{N}(0,\eta_{V} \downarrow_{d})}[\sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{\top} ]\right)\), and \(\tilde{\Omega}\) hides logarithmic factors depending on \(\delta_{1}\). Lastly, note that the activation function in the output layer \(\sigma_{r}\) is 1-Lipschitz and is applied to \(\sigma_{r}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}\right)\), where \(\bm{X}^{\top}\bm{\beta}_{i}\) is bounded due to the softmax's property in Lemma 8, then by Lemma C.1 of Nguyen and Mondelli (2020), w.p. at least \(1-\delta_{3}\), one has \(\sqrt{2\ell(\bm{\theta}^{0})}\leq\tilde{\mathcal{O}}(\sqrt{N})\). Plugging back Eqs. (2) and (3), it suffices to prove the following inequality.

\[d_{m}\lambda_{0}/4\geq\tilde{\mathcal{O}}(Nd_{s}^{3/2}C_{x}\sqrt{d_{m}/d})\,,\] (54) \[(d_{m}\lambda_{0}/4)^{3/2}\geq\tilde{\mathcal{O}}(N^{3/2}d_{s}^{ 3}C_{x}^{2}(1+4C_{x}^{4}d_{s}^{2}d^{-2}))\,.\] (55)

By Lemma 15, with probability at least \(1-\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\), one has \(\lambda_{0}\geq\Theta(\eta_{V}/d_{s})=\Theta(d^{-1}d_{s}^{-1})\). Thus, when \(d_{m}\geq\tilde{\Omega}(N^{2})\), all of the above conditions hold. As a result, the conditions in Eqs. (45) and (46) are satisfied and the convergence of training Transformer is guaranteed as in Eq. (4). Note that one can achieve the same width requirement and probability for He initialization, and the proof bears resemblance to the LeCun initialization.

### Proof of Theorem 2 (NTK analysis)

Proof.: Below, we present the proof for NTK initialization. We select \(C_{Q}=C_{K}=C_{V}=C_{O}=1\), then by Lemma 5, with probability at least \(1-8e^{-d_{m}/2}\), we have:

\[\bar{\lambda}_{V} =\mathcal{O}(\sqrt{d_{m}}+\sqrt{d}),\quad\bar{\lambda}_{O}= \mathcal{O}(\sqrt{d_{m}})\,,\] (56) \[\bar{\lambda}_{Q} =\mathcal{O}(\sqrt{d_{m}}+\sqrt{d}),\quad\bar{\lambda}_{K}= \mathcal{O}(\sqrt{d_{m}}+\sqrt{d})\,.\]

When \(d_{m}\geq d\), plugging Eq. (56) into Eqs. (2) and (3), it suffices to prove the following equations.

\[\alpha^{2}\geq\mathcal{O}(\sqrt{N}d_{s}^{5/2}C_{x}^{3})\sqrt{2 \ell(\bm{\theta}^{0})}\,,\] (57) \[\alpha^{3}\geq\mathcal{O}(Nd_{s}^{3}C_{x}^{2}(1+4C_{x}^{4}d_{s}^ {2}))\sqrt{2\ell(\bm{\theta}^{0})}\,.\] (58)

Next, we will provide the lower bound for \(\alpha^{2}=\lambda_{\min}((\bm{F}_{\text{pre}}^{\top})(\bm{F}_{\text{pre}}^{0}) ^{\top})\). In the following context, we hide the index \(0\) for simplification. One can note that \(\bm{F}_{\text{pre}}\bm{F}_{\text{pre}}^{\top}\) is the summation of PSD matrices, thus, it suffices to lower bound: \(\lambda_{\min}(\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top})\), where we introduce the following notations:

\[\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top}=\tau_{1}^ {2}\sigma_{r}(\bm{\Phi}\bm{W}_{v}^{\top})\sigma_{r}(\bm{\Phi}\bm{W}_{v}^{\top} )^{\top}\,,\] (59) \[\bm{\Phi}=[\bm{X}_{1}^{\top}\bm{\beta}_{1,1},...,\bm{X}_{N}^{\top }\bm{\beta}_{1,N}]^{\top}\,,\] \[\bm{\beta}_{1,n}=\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(1,:)}\bm{W} _{Q}^{\top}\bm{W}_{K}\bm{X}_{n}^{\top}\right)^{\top}n\in[N].\]

By Eq. (48), w.p at least \(1-\delta\), \(\lambda_{\min}(\hat{\bm{F}}_{\text{pre}}\hat{\bm{F}}_{\text{pre}}^{\top})\geq d _{m}\lambda_{0}/4\), as long as it holds \(d_{m}\geq\tilde{\Omega}(N/\lambda_{0})\), where \(\lambda_{0}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim\mathcal{N}(0,l_{d})}[ \sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{\top}]\right)\), and \(\tilde{\Omega}\) hides logarithmic factors depending on \(\delta_{1}\). Lastly, w.p. at least \(1-\delta_{3}\), one has \(\sqrt{2\ell(\bm{\theta}^{0})}\leq\tilde{\mathcal{O}}(\sqrt{N})\). Plugging back Eqs. (2) and (3), it suffices to prove the following inequality.

\[d_{m}\lambda_{0}/4\geq\tilde{\mathcal{O}}(Nd_{s}^{5/2}C_{x}^{3})\,,\] (60) \[(d_{m}\lambda_{0}/4)^{3/2}\geq\tilde{\mathcal{O}}(N^{3/2}d_{s}^{3} C_{x}^{2}(1+4C_{x}^{4}d_{s}^{2}))\,.\] (61)

By Lemma 15, with probability at least \(1-\exp\left(-\Omega((N-1)^{-\hat{c}}d_{s}^{-1})\right)\), we have \(\lambda_{0}\geq\Omega(1)\). Thus, all of the above conditions hold when \(d_{m}=\tilde{\Omega}(N)\). As a result, the conditions in Eqs. (57) and (58) are satisfied and the convergence of training Transformer is guaranteed.

### Discussion for different initialization schemes

Recall that the convergence result in Theorem 2 shows:

\[\ell(\bm{\theta}^{\top})\leq\left(1-\gamma\frac{\alpha^{2}}{2}\right)^{t}\ \ell(\bm{ \theta}^{0}).\]

Thus, to discuss the convergence speed for different initialization, we need to check the lower bound for \(\alpha^{2}\). From the proofs for different initialization schemes above, we have the following lower bound for \(\alpha^{2}\), i.e.,

\[\alpha^{2}=\lambda_{\min}(\hat{\bm{F}}_{\mathrm{pre}}\hat{\bm{F}}_{\mathrm{ pre}}^{\top})\geq\tau_{1}^{2}d_{m}\lambda_{0}/4\geq\tau_{1}^{2}\eta_{V}d_{m} \Omega(N/d),\]

with high probability. Plugging the value of \(\tau_{1}\) and \(\eta_{V}\), we observe that for LeCun initialization and He initialization: \(\alpha^{2}\geq\Omega(d_{m}N/d)\) while for NTK initialization: \(\alpha^{2}\geq\Omega(N/d)\). Thus, the convergence speed of LeCun initialization and He initialization is faster than NTK initialization. As a result, faster step-size is required for NTK initialization.

### Discussion for \(\tau_{0}=d_{m}^{-1}\) and \(\tau_{0}=d_{m}^{-1/2}\)

Eq. (4) indicates that the convergence speed is affected by \(\alpha\), there we compare the lower bound for \(\alpha\) for these two scaling. In Appendix C.3, we have proved that under the LeCun initialization, one has \(\alpha^{2}\geq d_{m}\lambda_{0}/4\geq d_{m}\eta_{V}\mu(\sigma_{\tau})^{2}\Theta (\left\lVert\beta_{1,k}\right\rVert_{2}^{2})\). Note that this bound holds for these two different scaling, which is inside \(\bm{\beta}\). Thus in the next, we need to see the difference between the lower bound of \(\left\lVert\bm{\beta}_{1,k}\right\rVert_{2}^{2})\) in the case of these two scalings. Specifically, for \(\tau_{0}=d_{m}^{-1/2}\) scaling, we have proved that \(\left\lVert\beta_{1,k}\right\rVert_{2}^{2}\geq 1/d_{s}\) by Lemma 8. However, for the case of \(\tau_{0}=d_{m}^{-1}\) scaling, when the width is large enough, the value inside the softmax tends to zero, as a result, \(\left\lVert\bm{\beta}_{1,k}\right\rVert_{2}^{2}\approx 1/d_{s}\). Thus, we can see that as the width increases, the convergence speed of \(\tau_{0}=d_{m}^{-1/2}\) could be faster. Lastly, we remark on the difference in the step size for these two scales, which can be seen from the definition of \(C\) and its corresponding \(c_{1},c_{2},c_{3}\) in Appendix C.2.

### Discussion for extension to deep Transformer and residual Transformer

Extension from our shallow Transformer to deep Transformer is not technically difficult as they share the same analysis framework. Nevertheless, the extension requires several tedious derivations and calculations involving the query, key, and value matrices along different layers. Here we point out the proof roadmap for this extension. The first step is following Proposition. 1 to provide the sufficient condition for the convergence guarantee, e.g., Eqs. (2) and (3). The second step is similar to Theorem 2, where we need to verify the aforementioned assumptions for different initialization. In the second part, the main task is to prove the lower bound of \(\alpha:=\sigma_{\min}\left(\bm{F}_{\mathrm{pre}}^{0}\right)\), where \(\bm{F}_{\mathrm{pre}}\) is the output of the last hidden layer. One can apply concentration inequality to bound the difference between \(\sigma_{\min}\left(\bm{F}_{\mathrm{pre}}^{0}\right)\) and \(\sigma_{\min}\left(\bm{F}_{\mathrm{pre}}^{*0}\right)\), where the latter is the corresponding limit in infinite width. Lastly, one needs to plug the lower bound into the assumptions in order to obtain the width requirement.

Our proof framework is general and can handle the following residual Transformer. Here we give a proof sketch to show how to achieve this. Specifically, we consider the residual block in the self-attention layer:

\[\mathbf{A_{1}}=\text{Self-attention}(\mathbf{X})\triangleq\sigma_{\mathbf{s} }\left(\tau_{0}(\mathbf{X}\mathbf{W}_{\mathbf{Q}}^{\top})\left(\mathbf{X} \mathbf{W}_{\mathbf{K}}^{\top}\right)^{\top}\right)\left(\mathbf{X}\mathbf{W} _{\mathbf{V}}^{\top}\right)+\mathbf{X}.\]

As a result, the output becomes

\[f(\bm{X})=\tau_{1}\bm{w}_{O}^{\top}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\bm{W}_{V} \bm{X}^{\top}\bm{\beta}_{i}+\left(\bm{X}^{(i:)}\right)^{\top}\right).\]

To prove the convergence of the above residual Transformer, the first part that will be modified is the proof for Proposition 1. The formula for \(\bm{f}_{\mathrm{pre}}\) becomes as follows:

\[\bm{f}_{pre}=\tau_{1}\bm{w}_{O}^{\top}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\bm{W}_ {V}\bm{X}^{\top}\bm{\beta}_{i}+\left(\bm{X}^{(i:)}\right)^{\top}\right).\]Lemmas 7 and 8 remain unchanged. In Lemma 9, only the first step in the proof changes while the remaining part does not change because the term \(\bm{X}^{(i:)}\top\) in two adjacent time steps cancels out. In Lemma 10, we have

\[||\tau_{1}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\bm{W}_{V}^{\prime}\bm{X}_{1}^{ \top}\bm{\beta}_{i,1}^{\prime}(\bm{X}^{(i:)})\right)||_{2}\leq\tau_{1}d_{s} \left(||\bm{W}_{V}^{\prime}||_{2}d_{s}^{1/2}C_{x}+C_{x}\right).\]

Similarly, in the remaining lemmas, we need to add the additional term \(C_{x}\) for the upper bound of \(||\bm{X}^{(i:)}||_{2}\).

The second part is the proof for Theorem 1 regarding the lower bound for \(\alpha_{0}\). By Weyl's inequality:

\[\alpha_{0}=\sigma_{\min}(\bm{F}_{\mathrm{pre}})\geq\sigma_{\min}(\bm{F}_{ \mathrm{pre}}^{*})-||\bm{F}_{\mathrm{pre}}-\bm{F}_{\mathrm{pre}}^{*}||_{2},\]

where we denote by \(\bm{F}_{\mathrm{pre}}=[\bm{f}_{\mathrm{pre}}(\bm{X}_{1}),\cdots,\bm{f}_{ \mathrm{pre}}(\bm{X}_{N})]\), and \(\bm{F}_{\mathrm{pre}}^{*}\) is the corresponding one without the residual connection. Then we can upper bound the second term can be bounded as follows:

\[||\bm{F}_{\mathrm{pre}}-\bm{F}_{\mathrm{pre}}^{*}||_{2}\leq||\bm{ F}_{\mathrm{pre}}-\bm{F}_{\mathrm{pre}}^{*}||_{F}\] (62) \[\leq\sqrt{N}||\tau_{1}\bm{w}_{O}^{\top}\sum_{i=1}^{d_{s}}\sigma_{ r}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}+(\bm{X}^{(i:)})^{\top}\right)- \tau_{1}\bm{w}_{O}^{\top}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\bm{W}_{V}\bm{X}^{ \top}\bm{\beta}_{i}\right)||_{2}\] \[\leq\sqrt{N}\tau_{1}d_{s}||\bm{w}_{O}||_{2}||\bm{W}_{V}||_{2}C_{x}.\]

The remains step follows the same as previous analysis.

### Linear over-parametrization and attention module behaving as a pooling layer

In this section, we discuss the link between the linear over-parametrization and attention module behaving as a pooling layer under the \(d_{m}^{-1}\) scaling. First, due to the \(d_{m}^{-1}\) scaling, the attention module degenerates to a pooling layer according to the law of large numbers. In this case, the nonlinearity on \(\bm{X}\) disappears and thus the minimum eigenvalue of \(\bm{\Theta}\bm{\Theta}^{\top}\) can be estimated via \(\bm{X}\bm{X}^{\top}\). Accordingly, this leads to the minimum eigenvalue in the order of \(\Omega(N/d)\), and thus linear over-parameterization is enough.

## Appendix D Proof for NTK

In this section, we elaborate the proof for Lemma 1 in Appendix D.1, the proof for Theorem 3 in Appendix D.3, respectively.

### Proof of Lemma 1

Proof.: We will compute the inner product of the Jacobian of each weight separately. Firstly, we analyze \(\bm{w}_{O}\). Let us denote by \(\bm{\beta}_{i}:=\sigma_{s}\left(\tau_{0}\bm{X}^{(i,:)}\bm{W}_{Q}^{\top}\bm{W} _{K}\bm{X}^{\top}\right)^{\top}\in\mathbb{R}^{d_{s}}\). Then:

\[\frac{\partial f(\bm{X})}{\partial\bm{w}_{O}}=\tau_{1}\sum_{i=1}^{d_{s}}\sigma _{r}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}\right)\;.\]

The inner product of the gradient is:

\[\begin{split}&\lim_{d_{m}\to\infty}\left\langle\frac{\partial f (\bm{X})}{\partial\bm{w}_{O}},\frac{\partial f(\bm{X}^{\prime})}{\partial\bm {w}_{O}}\right\rangle\\ &=\tau_{1}^{2}\sum_{i=1,j=1}^{d_{s}}\lim_{d_{m}\to\infty}\left( \sigma_{r}\left(\bm{W}_{V}\bm{X}^{\prime\top}\bm{\beta}_{j}^{\prime}\right) \right)^{\top}\left(\sigma_{r}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i} \right)\right)\\ &=d_{s}^{2}\mathbb{E}_{\bm{w}\sim\mathcal{N}(\mathbf{0},\bm{I})} \left(\sigma_{r}\left(\bm{w}^{\top}\bm{X}^{\prime\top}\bm{1}_{d_{s}}\right) \right)\left(\sigma_{r}\left(\bm{w}^{\top}\bm{X}^{\top}\bm{1}_{d_{s}}\right) \right)\,,\end{split}\] (63)

where the second equality uses the law of large numbers. Secondly, we analyze \(\bm{W}_{Q}\):

\[\frac{\partial f(\bm{X})}{\partial W_{Q}^{(p,q)}}=\tau_{0}\tau_{1}\sum_{i=1}^{ d_{s}}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{ \beta}_{i}\right)\right)^{\top}\bm{W}_{V}\bm{X}^{\top}\left(\text{diag}(\bm{ \beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{X}\bm{W}_{K}^{\top} \bm{e}_{p}\bm{e}_{q}^{\top}\bm{X}^{(i:)\top}\;.\]Thus:

\[\frac{\partial f(\bm{X})}{\partial\bm{W}_{Q}}=\tau_{0}\tau_{1}\sum_{i=1}^{d_{s}} \bm{W}_{k}\bm{X}^{\top}\left(\text{diag}(\bm{\beta}_{i})-\bm{\beta}_{i}\bm{ \beta}_{i}^{\top}\right)\bm{X}\bm{W}_{V}^{\top}\left(\bm{w}_{O}\circ\dot{ \sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}\right)\right)\bm{X}^{( i,:)}\,.\]

The inner product of the gradient is:

\[\lim_{d_{m}\to\infty}\left\langle\frac{\partial f(\bm{X})}{ \partial\bm{W}_{V}},\frac{\partial f(\bm{X}^{\prime})}{\partial\bm{W}_{V}}\right\rangle\] \[=\lim_{d_{m}\to\infty}\tau_{0}^{2}\tau_{1}^{2}\sum_{i=1,j=1}^{d_{s }}\text{Trace}\left(\bm{W}_{k}\bm{X}^{\top}\left(\text{diag}(\bm{\beta}_{i})- \bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{X}\bm{W}_{V}^{\top}\left(\bm{w }_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}\right) \right)\bm{X}^{(i,:)}\] \[\bm{X}^{(j,:)\top}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm {W}_{V}\bm{X}^{\prime\top}\bm{\beta}_{j}^{\prime}\right)\right)^{\top}\bm{W}_ {V}\bm{X}^{\prime\top}\left(\text{diag}(\bm{\beta}_{j}^{\prime})-\bm{\beta}_{ j}^{\prime}\bm{\beta}_{j}^{\prime\top}\right)\bm{X}^{\prime}\bm{W}_{k}^{\top}\right)\] \[=\lim_{d_{m}\to\infty}\sum_{i=1,j=1}^{d_{s}}\bm{X}^{(i,:)}\bm{X}^ {(j,:)\top}\langle\tau_{0}\tau_{1}\left(\bm{X}^{\top}\left(\text{diag}(\bm{ \beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{X}\bm{W}_{V}^{\top} \left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{ i}\right)\right)\,,\] \[\tau_{0}\tau_{1}\bm{W}_{k}^{\top}\bm{W}_{k}\bm{X}^{\prime\top} \left(\text{diag}(\bm{\beta}_{j}^{\prime})-\bm{\beta}_{j}^{\prime}\bm{\beta}_ {j}^{\prime\top}\right)\bm{X}^{\prime}\bm{W}_{V}^{\top}\left(\bm{w}_{O}\circ \dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\prime\top}\bm{\beta}_{j}^{\prime} \right)\right)\rangle\,.\]

For the first term of the inner product, we have:

\[\lim_{d_{m}\to\infty}\tau_{0}\tau_{1}\bm{X}^{\top}\left(\text{ diag}(\bm{\beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{X}\bm{W}_{V}^{ \top}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{ \beta}_{i}\right)\right)\] \[=\lim_{d_{m}\to\infty}\bm{X}^{\top}\left(\text{diag}(\bm{\beta}_{ i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{X}\begin{bmatrix}\tau_{0}\tau_{1} \sum_{k=1}^{d_{m}}W_{V}^{(k,1)}w_{O}^{(k)}\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{ X}^{\top}\bm{\beta}_{i}\right)^{(k)}\\ \vdots\\ \tau_{0}\tau_{1}\sum_{k=1}^{d_{m}}W_{V}^{(k,d)}w_{O}^{(k)}\dot{\sigma_{r}} \left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{i}\right)^{(k)}\end{bmatrix}\] \[=\bm{X}^{\top}\left(\text{diag}(\bm{1}_{d_{s}})-\bm{1}_{d_{s}} \bm{1}_{d_{s}}^{\top}\right)\bm{X}\begin{bmatrix}0\\ \vdots\\ 0\end{bmatrix}=0\,,\]

where the second equality is by the law of large numbers and \(\mathbb{E}w=0\) for a random variable \(w\sim\mathcal{N}(0,1)\). Thus: \(\lim_{d_{m}\to\infty}\left\langle\frac{\partial f(\bm{X})}{\partial\bm{W}_{O}}, \frac{\partial f(\bm{X}^{\prime})}{\partial\bm{W}_{Q}}\right\rangle=0\). Similarly, \(\lim_{d_{m}\to\infty}\left\langle\frac{\partial f(\bm{X})}{\partial\bm{W}_{K}}, \frac{\partial f(\bm{X}^{\prime})}{\partial\bm{W}_{K}}\right\rangle=0\). Lastly, we analyze \(\bm{W}_{V}\):

\[\frac{\partial f(\bm{X})}{\partial W_{V}^{(p,q)}}=\tau_{1}\sum_{i=1}^{d_{s}} \left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{ i}\right)\right)^{\top}\bm{e}_{p}\bm{e}_{q}^{\top}\bm{X}^{\top}\bm{\beta}_{i}.\]

Thus:

\[\frac{\partial f(\bm{X})}{\partial\bm{W}_{V}}=\tau_{1}\sum_{i=1}^{d_{s}} \left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{ i}\right)\right)\bm{\beta}_{i}^{\top}\bm{X}\,.\]

The inner product of the gradient is:

\[\lim_{d_{m}\to\infty}\left\langle\frac{\partial f(\bm{X})}{\partial \bm{W}_{V}},\frac{\partial f(\bm{X}^{\prime})}{\partial\bm{W}_{V}}\right\rangle\] \[=\lim_{d_{m}\to\infty}\tau_{1}^{2}\sum_{i=1,j=1}^{d_{s}}\text{ Trace}\Big{(}\big{(}\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\top}\bm{\beta}_{ i}\right)\big{)}\,\bm{\beta}_{i}^{\top}\bm{X}\bm{X}^{\prime\top}\bm{\beta}_{j}^{ \prime}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\prime\top} \bm{\beta}_{j}^{\prime}\right)\right)^{\top}\Big{)}\] \[=\lim_{d_{m}\to\infty}\tau_{1}^{2}\sum_{i=1,j=1}^{d_{s}}\left(\bm{w }_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_{V}\bm{X}^{\prime\top}\bm{\beta}_{j}^{ \prime}\right)\right)^{\top}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{W}_ {V}\bm{X}^{\top}\bm{\beta}_{i}\right)\right)\bm{\beta}_{i}^{\top}\bm{X}\bm{X}^{ \prime\top}\bm{\beta}_{j}^{\prime}\] \[=\lim_{d_{m}\to\infty}\tau_{1}^{2}\sum_{i=1,j=1}^{d_{s}}\sum_{k=1 }^{d_{m}}(w_{O}^{(k)})^{2}\dot{\sigma_{r}}\left(\bm{W}_{V}^{(k,:)}\bm{X}^{\prime \top}\bm{\beta}_{j}^{\prime}\right)\dot{\sigma_{r}}\left(\bm{W}_{V}^{(k,:)}\bm{X} ^{\prime\top}\bm{\beta}_{i}\right)\bm{\beta}_{i}^{\top}\bm{X}\bm{X}^{\prime \top}\bm{\beta}_{j}^{\prime}\] \[=d_{s}^{2}\mathbb{E}_{\bm{w}\sim\mathcal{N}(\bm{0},\bm{I})}\left( \dot{\sigma_{r}}\left(\bm{w}^{\top}\bm{X}^{\prime\top}\bm{1}_{d_{s}}\right) \right)\left(\dot{\sigma_{r}}\left(\bm{w}^{\top}\bm{X}^{\top}\bm{1}_{d_{s}} \right)\right)\left(\bm{1}_{d_{s}}^{\top}\bm{X}\bm{X}^{\prime\top}\bm{1}_{d_{s }}\right)\,,\] (64)

where the last equality uses the law of large numbers.

### NTK minimum eigenvalue

**Lemma 16**.: _Given \(\bm{\Phi}\) defined in Eq. (53) and \(\bm{\Phi}^{\star}\) defined in Lemma 1, denote \(\lambda_{*}=\lambda_{\min}(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top})\), and suppose the width satisfies \(d_{m}=\Omega(\frac{N^{2}\sqrt{\log(2d^{2}N^{2}/\delta)}}{\lambda_{2}^{2}})\), then with probability at least \(1-\delta\), one has \(\left\|\bm{\Phi}\bm{\Phi}^{\top}-\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\right\| _{\mathrm{F}}\leq\frac{\lambda_{*}}{4}\) and \(\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\geq\frac{3\lambda_{*}}{4}\)._

Proof.: In the following content, the variable with \({}^{\star}\) indicates the corresponding variable with infinite width \(d_{m}\). According to the definition of \(\bm{\Phi}\) in Eq. (53) and \(\bm{\Phi}^{\star}\) in Lemma 1, for each entry in \(\bm{\Phi}\bm{\Phi}^{\top}\) and \(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\), we have

\[\begin{split}&|(\bm{\Phi}\bm{\Phi}^{\top}-\bm{\Phi}^{\star} \bm{\Phi}^{\star\top})^{(n,r)}|=|\bm{\beta}_{1,n}^{\top}\bm{X}_{n}\bm{X}_{r}^{ \top}\bm{\beta}_{1,r}-\bm{\beta}_{1,n}^{\star\top}\bm{X}_{n}\bm{X}_{r}^{\top} \bm{\beta}_{1,r}^{\star}|\\ &\leq\left\|\bm{X}_{n}\bm{X}_{r}^{\top}(\bm{\beta}_{1,n}-\bm{ \beta}_{1,n}^{\star})\right\|_{2}+\left\|\bm{X}_{n}\bm{X}_{r}^{\top}(\bm{\beta }_{1,r}-\bm{\beta}_{1,r}^{\star})\right\|_{2}\leq C_{x}^{2}d_{s}\big{(}\big{\|} \bm{\beta}_{1,n}-\bm{\beta}_{1,n}^{\star}\big{\|}_{2}+\big{\|}\bm{\beta}_{1,r} -\bm{\beta}_{1,r}^{\star}\big{\|}_{2}\big{)}\,.\end{split}\] (65)

Next, we will bound \(\left\|\bm{\beta}_{1,n}-\bm{\beta}_{1,n}^{\star}\right\|_{2}\),

\[\begin{split}&\left\|\bm{\beta}_{1,n}-\bm{\beta}_{1,n}^{\star} \right\|_{2}=\left\|\sigma_{s}\left(\tau_{0}\bm{X}_{n}^{(1,:)}\bm{W}_{Q}^{\top }\bm{W}_{K}\bm{X}_{n}^{\top}\right)^{\top}-\sigma_{s}\left(\tau_{0}\bm{X}_{n}^ {(1,:)}\bm{W}_{Q}^{\star\top}\bm{W}_{K}^{\star}\bm{X}_{n}^{\top}\right)^{\top }\right\|_{2}\\ &\leq 2C_{x}^{2}d_{s}\left\|\tau_{0}\bm{W}_{Q}^{\top}\bm{W}_{K}-\tau_{0 }\bm{W}_{Q}^{\star\top}\bm{W}_{K}^{\star}\right\|_{\mathrm{F}}\quad\left(\text{ By Lemma \ref{eq:NTK}}\right).\end{split}\] (66)

Let first consider the absolute value of each element of \(\tau_{0}\bm{W}_{Q}^{\top}\bm{W}_{K}-\tau_{0}\bm{W}_{Q}^{\star\top}\bm{W}_{K}^{\star}\), i.e.,

\[|\tau_{0}\bm{W}_{Q}^{\top}\bm{W}_{K}-\tau_{0}\bm{W}_{Q}^{\star\top}\bm{W}_{K}^ {\star}|^{(i,j)}=|\tau_{0}\sum_{q=1}^{d_{m}}W_{Q}^{(q,i)}W_{k}^{(q,j)}-\tau_{ 0}\sum_{q=1}^{d_{m}}W_{Q}^{\star(q,i)}W_{k}^{\star(q,j)}|.\]

Since \(W_{Q}^{(q,i)}W_{k}^{(q,j)}\sim SE(\sqrt{2}\eta_{QK},\sqrt{2}\eta_{QK})\) is sub-exponential random variable, by Lemma 2 and Lemma 3, \(\tau_{0}\sum_{q=1}^{d_{m}}W_{Q}^{(q,i)}W_{k}^{(q,j)}\sim SE(\tau_{0}\eta_{QK} \sqrt{2d_{m}},\sqrt{2}\tau_{0}\eta_{QK})\), by Bernstein's inequality, when \(d_{m}\geq 2\log(2/\delta)\), the following inequality holds with probability at least \(1-\delta\):

\[|\tau_{0}\sum_{q=1}^{d_{m}}W_{Q}^{(q,i)}W_{k}^{(q,j)}-\tau_{0}\sum_{q=1}^{d_{ m}}W_{Q}^{\star(q,i)}W_{k}^{\star(q,j)}|\leq 2\tau_{0}\eta_{QK}\sqrt{d_{m}\log \left(2/\delta\right)}\,.\] (67)

Substituting \(\delta=\frac{\delta^{\prime}}{d^{2}}\) and applying union bound, we can obtain that when \(d_{m}\geq 2\log\frac{2d^{2}}{\delta^{\prime}}\), with probability at least \(1-\delta^{\prime}\):

\[\left\|\bm{\beta}_{1,n}-\bm{\beta}_{1,n}^{\star}\right\|_{2}\leq 4\tau_{0}\eta_{ QK}C_{x}^{2}d_{s}d\sqrt{d_{m}\log(2d^{2}/\delta^{\prime})}\,.\] (68)

Substituting back to Eq. (65), the following inequality holds with the same width requirement and probability. Applying the union bound over the index \((n,r)\) for \(n\in[N]\) and \(r\in[N]\), and substituting \(\delta^{\prime}=\frac{\delta^{\prime\prime}}{N^{2}}\), we obtain that when the width \(d_{m}\geq 2\log\frac{2d^{2}N^{2}}{\delta^{\prime}}\), the following inequality holds with probability at least \(1-\delta^{\prime\prime}\):

\[\left\|\bm{\Phi}\bm{\Phi}^{\top}-\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\right\| _{\mathrm{F}}=\sqrt{\sum_{n=1}^{N}\sum_{r=1}^{N}|(\bm{\Phi}\bm{\Phi}^{\top}-\bm{ \Phi}^{\star}\bm{\Phi}^{\star\top})^{(n,r)}|^{2}}\leq 8N\tau_{0}\eta_{ QK}C_{x}^{4}d_{s}d\sqrt{d_{m}\log(2d^{2}N^{2}/\delta^{\prime\prime})}\,.\]

In the case of LeCun initialization when \(d_{m}=\Omega(\frac{N^{2}\sqrt{\log(2d^{2}N^{2}/\delta^{\prime\prime})}}{\lambda_{ 0}^{2}})\), \(\big{\|}\bm{\Phi}\bm{\Phi}^{\top}-\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\big{\|}_{ \mathrm{F}}\leq\frac{\lambda_{*}}{4}\). Lastly, one has:

\[\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\geq\lambda_{*}-\big{\|}\bm{\Phi}\bm{ \Phi}^{\top}-\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\big{\|}_{2}\geq\lambda_{*} -\big{\|}\bm{\Phi}\bm{\Phi}^{\top}-\bm{\Phi}^{\star}\bm{\Phi}^{\star\top}\big{\|}_{ \mathrm{F}}\geq\frac{3\lambda_{*}}{4}.\]

where the first inequality is by Weyl's inequality. One can easily check that the same result also holds for He initialization and NTK initialization. 

**Lemma 17**.: _Given the \(\bm{\Phi}^{\star}\) defined in Lemma 1, then when \(N\geq\Omega(d^{4})\), with probability at least \(1-e^{-d}\) one has: \(\lambda_{\min}(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top})\geq\Theta(N/d)\,.\)_Proof.: Firstly, let us define \(\bm{Z}^{\star}=\bm{\Phi}^{\star}\bm{\Sigma}^{-1/2}\), then:

\[\lambda_{\min}(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top})=\lambda_{ \min}(\bm{\Phi}^{\star\top}\bm{\Phi}^{\star})=\lambda_{\min}(\bm{\Sigma}^{1/2}{ \bm{Z}^{\star}}^{\top}\bm{Z}^{\star}\bm{\Sigma}^{1/2\top})\] \[\geq\lambda_{\min}(\bm{Z}^{\star\top}\bm{Z}^{\star})\lambda_{\min }(\bm{\Sigma})\geq\lambda_{\min}(\bm{Z}^{\star\top}\bm{Z}^{\star})C_{\Sigma}\,,\]

where the last inequality is by Assumption 4). Note that we can reformulate \(\bm{Z}^{\star}\) by plugging \(\bm{\Phi}^{\star}\) as follows:

\[\bm{Z}^{\star}=\bm{\Phi}^{\star}\bm{\Sigma}^{-1/2}=\begin{bmatrix}\frac{1}{d_ {s}}\sum_{i=1}^{d_{s}}\bm{X}_{1}^{(i,:)}\bm{\Sigma}^{-1/2}\\ \vdots\\ \frac{1}{d_{s}}\sum_{i=1}^{d_{s}}\bm{X}_{N}^{(i,:)}\bm{\Sigma}^{-1/2}\end{bmatrix}\,.\]

Recall that in Assumption 4, we define the random vector \(\bm{x}=\bm{X}_{n}^{(i,:)\top}\) and write the covariance matrix for each feature of \(\bm{x}\) as \(\bm{\Sigma}=\mathbb{E}[\bm{x}\bm{x}^{\top}]\). Here we further define the random vector \(\bm{z}=\bm{\Sigma}^{-1/2}\bm{x}\,.\) Clearly, the covariance matrix for each feature of \(\bm{z}\) is \(\bm{\Sigma}_{z}=\mathbb{E}[\bm{z}\bm{z}^{\top}]=\mathbb{I}_{d}\). Therefore, by Proposition 4 of Zhu et al. (2022), we have \(\lambda_{\min}(\bm{Z}^{\star\top}\bm{Z}^{\star})\geq\Theta(N/d)\), which finishes the proof.

**Lemma 18**.: _Suppose the number of samples \(N\geq\Omega(d^{4})\) and the width \(d_{m}=\tilde{\Omega}(N^{2}\lambda_{*}^{-2})\), where \(\lambda_{*}=\lambda_{\min}(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top})\), then under Assumption 4, with probability at least \(1-\delta-e^{-d}\), one has \(\lambda_{0}=\lambda_{\min}\left(\mathbb{E}_{\bm{w}\sim\mathcal{N}(0,\eta_{V} \mathbb{I}_{d})}[\sigma_{r}(\bm{\Phi}\bm{w})\sigma_{r}(\bm{\Phi}\bm{w})^{T}] \right)\geq\Theta(\eta_{V}N/d).\)_

Proof.: By the Hermite expansion of \(\sigma_{r}\), one has:

\[\lambda_{0}\geq\eta_{V}\mu(\sigma_{r})^{2}\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{ \top})\,,\]

where \(\mu(\sigma_{r})\) is the 1-st Hermite coefficient of ReLU satisfying \(\mu(\sigma_{r})>0\). By Lemma 16,, when the width satisfies \(d_{m}=\tilde{\Omega}(N^{2}\lambda_{*}^{-2})\), then with probability at least \(1-\delta\), one has \(\lambda_{\min}(\bm{\Phi}\bm{\Phi}^{\top})\geq\frac{3}{4}\lambda_{*}\,.\) Furthermore, by Lemma 17, when \(N\geq\Omega(d^{4})\) w.p. at least \(1-e^{-d}\) one has \(\lambda_{*}=\Theta(N/d)\,.\) Thus, with probability at least \(1-\delta-e^{-d}\), one has:

\[\lambda_{0}\geq\frac{3}{4}\eta_{V}\mu(\sigma_{r})^{2}\lambda_{*}\geq\Theta( \eta_{V}N/d)).\]

Below, we provide a lower bound for the minimum eigenvalue of NTK, which plays a key role in analyzing convergence, generalization bounds, and memorization capacity (Arora et al., 2019, Nguyen et al., 2021, Montanari and Zhong, 2022, Bombari et al., 2022).

To prove this, we need the following assumption.

**Assumption 4**.: _Let \(\bm{x}=\sum_{i=1}^{d_{s}}(\bm{X}^{(i,:)})^{\top}\), \(\bm{\Sigma}=\mathbb{E}[\bm{x}\bm{x}^{\top}]\), then we assume that \(\bm{\Sigma}\) is positive definite, i.e., \(\lambda_{\min}(\bm{\Sigma})\geq C_{\Sigma}\) for some positive constant \(C_{\Sigma}\)._

**Remark:** This assumption implies that the covariance matrix along each feature dimension is positive definite. In statistics and machine learning (Liu et al., 2021, Liang and Rakhlin, 2020, Vershynin, 2018, Hastie et al., 2022), the covariance of \(\bm{x}\) is frequently assumed to be an identity matrix or positive definite matrix.

**Lemma 19** (Minimum eigenvalue of limiting NTK).: _Under Assumptions 1 and 4 and scaling \(\tau_{0}=d_{m}^{-1}\), when \(N\geq\Omega(d^{4})\), the minimum eigenvalue of \(\bm{K}\) can be lower bounded with probability at least \(1-e^{-d}\) as: \(\lambda_{\min}(\bm{K})\geq\mu(\sigma_{r})^{2}\Omega(N/d)\,,\) where \(\mu(\sigma_{r})\) represents the first Hermite coefficient of the ReLU activation function._

Proof.: By the Hermite expansion of \(\sigma_{r}\), we have:

\[\lambda_{\min}(\bm{K})>\lambda_{\min}(\mathbb{E}_{\bm{w}\sim \mathcal{N}(\bm{0},\bm{I})}\left(\sigma_{r}\left(\bm{\Phi}^{\star}\bm{w}\right) \sigma_{r}\left(\bm{\Phi}^{\star}\bm{w}\right)^{\top}\right))\] \[=\lambda_{\min}\bigg{(}\sum_{s=0}^{\infty}\mu_{s}(\sigma_{r})^{2} \bigcirc_{i=1}^{s}(\bm{\Phi}^{\star}\bm{\Phi}^{\star\top})\bigg{)}\quad\text{[ Nguyen and Mondelli, 2020, Lemma D.3]}\] \[\geq\mu(\sigma_{r})^{2}\lambda_{\min}(\bm{\Phi}^{\star}\bm{\Phi}^{ \star\top})\,.\]Note that when \(N\geq\Omega(d^{4})\), based on Lemma 17 and the fact that \(\mathbf{\Phi}^{*\top}\mathbf{\Phi}^{*\top}\) and \(\mathbf{\Phi}^{*\top}\mathbf{\Phi}^{*}\) share the same non-zero eigenvalues, with probability at least \(1-e^{-d}\) one has:

\[\lambda_{\min}(\bm{K})\geq\mu(\sigma_{r})^{2}(\frac{N}{d}-9N^{2/3}d^{1/3})=\mu( \sigma_{r})^{2}\Theta(N/d)\,,\]

which completes the proof. 

**Remark:** The minimum eigenvalue of the NTK plays an important role in the global convergence, similar to \(\alpha\) in Proposition. 1. By defining \(\alpha\triangleq\sigma_{\min}\left(\bm{F}^{0}_{\mathrm{pre}}\right)\), under the over-parameterized regime with \(d_{m}\geq N\), we have \(\alpha^{2}=\lambda_{\min}((\bm{F}^{0}_{\mathrm{pre}})^{\top}\bm{F}^{0}_{ \mathrm{pre}})\), which is the exact minimum eigenvalue of the empirical NTK with respect to the weight in the output layer.

### Proof of Theorem 3

Proof.: Before starting the proof, we introduce some useful lemmas that are used to analyze the randomness of initialized weight.

**Lemma 20**.: _Given an initial weight vector \(\bm{w}\in\mathbb{R}^{d_{m}}\) where each element is sampled independently from \(\mathcal{N}(0,1)\), for any \(\tilde{\bm{w}}\) such that \(\|\tilde{\bm{w}}-\bm{w}\|_{2}\leq R\), with probability at least \(1-2\exp(-d_{m}/2)\), one has:_

\[\|\tilde{\bm{w}}\|_{2}\leq R+3\sqrt{d_{m}}\,.\] (69)

Proof of Lemma 20.: By triangle inequality and Lemma 6:

\[\|\tilde{\bm{w}}\|_{2}\leq\|\tilde{\bm{w}}-\bm{w}\|_{2}+\|\bm{w}\|_{2}\leq R+3 \sqrt{d_{m}}\,.\] (70)

Now we are ready to start our proof, in the following content, we avoid the tilde symbol (\(\tilde{\cdot}\)) over the weight for simplicity. Because we aim to study the effect of width (\(d_{m}\)), to simplify the proof, we set the embedding dimension of the input as one, we can write the output layer of the network into the following form:

\[f(\bm{x})=\tau_{0}\sum_{i=1}^{d_{s}}\sigma_{r}\left(\sigma_{s}\left(\tau_{1}x^ {(i)}\bm{w}_{Q}^{\top}\bm{w}_{K}\bm{x}^{\top}\right)\left(\bm{w}_{V}\bm{x}^{ \top}\right)\right)^{\top}\bm{w}_{O}\] (71)

The Hessian matrix \(\bm{H}\) of the network can be written as the following structure:

\[\bm{H}=\left(\begin{array}{cccc}\bm{H}_{Q,Q}&\bm{H}_{Q,K}&\bm{H}_{Q,V}&\bm{ H}_{Q,O}\\ \bm{H}_{K,Q}&\bm{H}_{K,K}&\bm{H}_{K,V}&\bm{H}_{K,O}\\ \bm{H}_{V,Q}&\bm{H}_{V,K}&\bm{H}_{V,V}&\bm{H}_{V,O}\\ \bm{H}_{O,Q}&\bm{H}_{O,K}&\bm{H}_{O,V}&\bm{H}_{O,O}\\ \end{array}\right)\,,\] (72)

where we partition the Hessian \(\bm{H}\) of the network to each Hessian block e.g., \(\bm{H}_{Q,Q}=\frac{\partial^{2}f}{\partial\bm{w}_{Q}\partial\bm{w}_{Q}},\bm{ H}_{Q,K}=\frac{\partial^{2}f}{\partial\bm{w}_{Q}\partial\bm{w}_{K}}\), etc. Based on the triangle inequality of the norm, the spectral norm of the Hessian \(\bm{H}\) can be upper bounded by the sum of the spectral norm of each Hessian block. We start by analyzing \(\bm{H}_{Q,Q}\). The time step \(t\) is hidden for simplification.

\[\frac{\partial f(\bm{x})}{\partial w_{Q}^{(j)}}=\tau_{0}\tau_{1}\sum_{i=1}^{d_ {s}}x^{(i)}w_{K}^{(j)}\bm{w}_{V}^{\top}\left(\bm{w}_{O}\circ\dot{\sigma_{r}} \left(\bm{w}_{V}\bm{x}^{\top}\bm{\beta}_{i}\right)\right)\bm{x}^{\top}\left( \text{diag}(\bm{\beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{x}\,,\] (73)

where the Jacobian of Softmax is given by Lemma 11. Next, we calculate the second-order derivative. Each element of the Hessian \(\bm{H}_{Q,Q}\) is

\[H_{Q,Q}^{(j,p)}=\frac{\partial^{2}f(\bm{x})}{\partial w_{Q}^{(j)}\partial w_{Q} ^{(p)}}=\tau_{0}^{2}\tau_{1}\sum_{i=1}^{d_{s}}w_{K}^{(j)}w_{K}^{(p)}(x^{(i)})^ {2}\bm{w}_{V}^{\top}\left(\bm{w}_{O}\circ\dot{\sigma_{r}}\left(\bm{w}_{V}\bm{x} ^{\top}\bm{\beta}_{i}\right)\right)\bm{x}^{\top}\bm{\Gamma}_{i}\bm{x}\,,\]where we denote by \(\boldsymbol{\Gamma}_{i}=\left\{\text{diag}\left(\boldsymbol{\beta}_{i}\circ \boldsymbol{x}-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\boldsymbol {x}\right)\ -\ \left(\text{diag}\left(\boldsymbol{\beta}_{i}\right)-\boldsymbol{\beta}_{i} \boldsymbol{\beta}_{i}{}^{\top}\right)\boldsymbol{x}\boldsymbol{\beta}_{i}{}^ {\top}\ -\boldsymbol{\beta}_{i}\boldsymbol{x}^{\top}\left(\text{diag}\left( \boldsymbol{\beta}_{i}\right)-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^ {\top}\right)\boldsymbol{x}\right\},\circ\) symbolizes Hadamard product. According to initialization of \(\boldsymbol{w}_{O}\), \(\boldsymbol{w}_{V}\), \(\boldsymbol{w}_{Q}\), \(\boldsymbol{w}_{K}\) and Lemma 20, with probability at least \(1-8e^{-d_{m}/2}\), we have

\[\left\|\boldsymbol{w}_{V}\right\|_{2} \leq 3\sqrt{\eta_{V}d_{m}}+R,\quad\left\|\boldsymbol{w}_{O} \right\|_{2}\leq 3\sqrt{\eta_{O}d_{m}}+R,\] \[\left\|\boldsymbol{w}_{Q}\right\|_{2} \leq 3\sqrt{\eta_{Q}d_{m}}+R,\quad\left\|\boldsymbol{w}_{K} \right\|_{2}\leq 3\sqrt{\eta_{K}d_{m}}+R\,.\]

The spectral norm of \(\boldsymbol{H}_{Q,Q}\) can be bounded with probability at least \(1-8e^{-d_{m}/2}\):

\[\|\boldsymbol{H}_{Q,Q}\|_{2} =\|\tau_{0}^{2}\tau_{1}\sum_{i=1}^{d_{s}}(x^{(i)})^{2}\boldsymbol {w}_{V}^{\top}\left(\boldsymbol{w}_{O}\circ\sigma_{r}\left(\boldsymbol{w}_{V} \boldsymbol{x}^{\top}\boldsymbol{\beta}_{i}\right)\right)\boldsymbol{x}^{\top }\boldsymbol{\Gamma}_{i}\boldsymbol{x}\boldsymbol{w}_{K}\boldsymbol{w}_{K}^{ \top}\|_{2}\] \[\leq|\tau_{0}^{2}\tau_{1}\sum_{i=1}^{d_{s}}(x^{(i)})^{2} \boldsymbol{w}_{V}^{\top}\left(\boldsymbol{w}_{O}\circ\sigma_{r}\left( \boldsymbol{w}_{V}\boldsymbol{x}^{\top}\boldsymbol{\beta}_{i}\right)\right) \boldsymbol{x}^{\top}\boldsymbol{\Gamma}_{i}\boldsymbol{x}\|\|\boldsymbol{w}_{ K}\boldsymbol{w}_{K}^{\top}\|_{2}\quad\text{(Homogeneity of norm)}\] \[\leq|\tau_{0}^{2}\tau_{1}\sum_{i=1}^{d_{s}}(x^{(i)})^{2} \boldsymbol{x}^{\top}\boldsymbol{\Gamma}_{i}\boldsymbol{x}\|\|\boldsymbol{w}_{ O}\|_{2}\|\boldsymbol{w}_{V}\|_{2}\|\boldsymbol{w}_{K}\|_{2}^{2}\quad\text{(Cauchy-Schwarz inequality)}\] \[\leq\tau_{0}^{2}\tau_{1}\|\sum_{i=1}^{d_{s}}(x^{(i)})^{2} \boldsymbol{x}^{\top}\boldsymbol{\Gamma}_{i}\boldsymbol{x}\left|\left(3\sqrt{ \eta_{O}d_{m}}+R\right)\left(3\sqrt{\eta_{V}d_{m}}+R\right)(3\sqrt{\eta_{Q}d_ {m}}+R)(3\sqrt{\eta_{K}d_{m}}+R)\right.\] (Lemma 20) \[=\mathcal{O}(1/\sqrt{d_{m}})\,,\]

where the last equality is by the bound of \(\left\|\boldsymbol{\Gamma}_{i}\right\|_{2}\), by triangle inequality:

\[\left\|\boldsymbol{\Gamma}_{i}\right\|_{2}\leq\left\|\text{diag} \left(\boldsymbol{\beta}_{i}\circ\boldsymbol{x}-\boldsymbol{\beta}_{i} \boldsymbol{\beta}_{i}{}^{\top}\boldsymbol{x}\right)\right\|_{2}+\left\| \left(\text{diag}\left(\boldsymbol{\beta}_{i}\right)-\boldsymbol{\beta}_{i} \boldsymbol{\beta}_{i}{}^{\top}\right)\boldsymbol{x}\boldsymbol{\beta}_{i}{}^ {\top}\right\|_{2}+\left\|\boldsymbol{\beta}_{i}\boldsymbol{x}^{\top}\left( \text{diag}\left(\boldsymbol{\beta}_{i}\right)-\boldsymbol{\beta}_{i} \boldsymbol{\beta}_{i}{}^{\top}\right)\boldsymbol{x}\right\|_{2}\,.\] (74)

For the first part of Eq. (74), we have

\[\left\|\text{diag}\left(\boldsymbol{\beta}_{i}\circ\boldsymbol{x} -\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\boldsymbol{x}\right) \right\|_{2}\leq\left\|\text{diag}\left(\boldsymbol{\beta}_{i}\circ \boldsymbol{x}-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\boldsymbol {x}\right)\right\|_{2}\leq\left\|\left(\boldsymbol{\beta}_{i}\circ\boldsymbol {x}-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\boldsymbol{x}\right) \right\|_{\infty}\] \[\leq\left\|\left(\boldsymbol{\beta}_{i}\circ\boldsymbol{x}\right) \right\|_{\infty}+\left\|\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top} \boldsymbol{x}\right\|_{\infty}\leq\left\|\boldsymbol{x}\right\|_{\infty}+| \boldsymbol{\beta}_{i}{}^{\top}\boldsymbol{x}|\leq\left\|\boldsymbol{x}\right\| _{2}+\left\|\boldsymbol{\beta}_{i}\right\|_{2}\left\|\boldsymbol{x}\right\|_{2} \leq 2C_{x}\,.\]

For the second part of Eq. (74), we have

\[\left\|\left(\text{diag}\left(\boldsymbol{\beta}_{i}\right)- \boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\right)\boldsymbol{x} \boldsymbol{\beta}_{i}{}^{\top}\right\|_{2}\leq\left(\left\|\boldsymbol{\beta} _{i}\right\|_{\infty}+\left\|\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{ \top}\right\|_{2}\right)\left\|\boldsymbol{x}\boldsymbol{\beta}_{i}{}^{\top} \right\|_{2}\] \[\leq 2\left\|\boldsymbol{x}\boldsymbol{\beta}_{i}{}^{\top} \right\|_{2}\leq 2\left\|\boldsymbol{x}\right\|_{2}\left\|\boldsymbol{\beta}_{i}{}^{ \top}\right\|_{2}\leq 2C_{x}\,.\]

For the third part of Eq. (74), we have

\[\left\|\boldsymbol{\beta}_{i}\boldsymbol{x}^{\top}\left(\text{diag} \left(\boldsymbol{\beta}_{i}\right)-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{ }^{\top}\right)\boldsymbol{x}\right\|_{2}\leq\left\|\boldsymbol{\beta}_{i} \boldsymbol{x}^{\top}\right\|_{2}\left\|\left(\text{diag}\left(\boldsymbol{ \beta}_{i}\right)-\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}{}^{\top}\right) \right\|_{2}\left\|\boldsymbol{x}\right\|_{2}\,.\]

Next, we analyze the Hessian \(\boldsymbol{H}_{Q,K}\), where each element is:

\[H_{Q,K}^{(j,p)}=\frac{\partial^{2}f}{\partial w_{Q}^{(j)}\partial w_{K}^{(p)}}= \frac{\partial^{2}f(\boldsymbol{x})}{\partial w_{Q}^{(j)}\partial w_{Q}^{(p)}}= \tau_{0}^{2}\tau_{1}\sum_{i=1}^{d_{s}}w_{K}^{(j)}w_{Q}^{(p)}(x^{(i)})^{2} \boldsymbol{w}_{V}^{\top}\left(\boldsymbol{w}_{O}\circ\sigma_{r}\left( \boldsymbol{w}_{V}\boldsymbol{x}^{\top}\boldsymbol{\beta}_{i}\right)\right) \boldsymbol{x}^{\top}\boldsymbol{\Gamma}_{i}\boldsymbol{x}\,.\]

Due to the symmetry, similar to \(\boldsymbol{H}_{Q,Q}\), the spectral norm of \(\boldsymbol{H}_{Q,K}\) can be bounded with probability at least \(1-8e^{-d_{m}/2}\): \(\|\boldsymbol{H}_{Q,K}\|_{2}=\mathcal{O}(1/\sqrt{d_{m}})\).

Next, we analyze the Hessian \(\boldsymbol{H}_{Q,V}\), where each element is:

\[H_{Q,V}^{(j,p)}=\frac{\partial^{2}f}{\partial w_{Q}^{(j)}\partial w_{V}^{(p)}}= \tau_{0}\tau_{1}\sum_{i=1}^{d_{s}}w_{K}^{(j)}x^{(i)}w_{Q}^{(p)}\dot{\sigma}_{r} \left(w_{V}^{(p)}\boldsymbol{x}^{\top}\boldsymbol{\beta}_{i}\right)\boldsymbol{x}^{ \top}\left(\text{diag}(\boldsymbol{\beta}_{i})-\boldsymbol{\beta}_{i}\boldsymbol{ \beta}_{i}^{\top}\right)\boldsymbol{x}\,.\]The spectral norm of \(\bm{H}_{Q,V}\) can be bounded with probability at least \(1-8e^{-d_{m}/2}\):

\[\|\bm{H}_{Q,V}\|_{2} =\|\tau_{0}\tau_{1}\sum_{i=1}^{d_{s}}x^{(i)}\dot{\sigma_{r}}\left(w _{V}^{(p)}\bm{x}^{\top}\bm{\beta}_{i}\right)\bm{x}^{\top}\left(\text{diag}(\bm{ \beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{x}\bm{w}_{K}\bm{w}_{ O}{}^{\top}\|_{2}\] \[\leq|\tau_{0}\tau_{1}\sum_{i=1}^{d_{s}}x^{(i)}\dot{\sigma_{r}} \left(w_{V}^{(p)}\bm{x}^{\top}\bm{\beta}_{i}\right)\bm{x}^{\top}\left(\text{ diag}(\bm{\beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\right)\bm{x}\|\bm{w}_{K} \bm{w}_{O}{}^{\top}\|_{2}\quad\text{(Homogeneity of norm)}\] \[\leq\tau_{0}\tau_{1}\sum_{i=1}^{d_{s}}|x_{i}|\|\text{diag}(\bm{ \beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\|_{2}\|\bm{x}\|_{2}^{2}\|\bm{ w}_{K}\|_{2}\|\bm{w}_{O}\|_{2}\quad\text{(Cauchy--Schwarz inequality)}\] \[\leq\tau_{0}\tau_{1}\sum_{i=1}^{d_{s}}|x_{i}|\|\text{diag}(\bm{ \beta}_{i})-\bm{\beta}_{i}\bm{\beta}_{i}^{\top}\|_{2}\|\bm{x}\|_{2}^{2}\left( 3\sqrt{\eta_{QK}d_{m}}+R\right)\left(3\sqrt{\eta_{O}d_{m}}+R\right)\quad\text {(Lemma \ref{lem:def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_defdef_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_defdef_def_def_def_def_def_def_def_def_def_def_def_def_defdef_def_def_def_def_def_def_def_def_def_def_def_defdef_def_def_def_def_def_def_defdef_def_def_def_def_def_def_def_def_def_defdef_def_def_def_defdef_def_def_defdef_def_def_defdef_def_defdef_defdef_def_def_def_def_def_def_def_def_defdef_def_defdef_def_defdef_defdef_def_def_defdef_def_defdef_def_defdef_def_def_defdef_def_defdef_def_def_def_defdef_def_defdef_def_def_defdef_def_defdef_def_def_defdef_def_def_def_defdef_def_def_defdef_def_defdef_def_def_defdef_def_defdef_defdef_def_def_def_def_defdef_def_def_def_defdef_defdef_def_defdef_def_defdef_def_defdef_defdef_def_defdef_def_defdef_def_defdef_defdef_def_defdef_def_defdef_defdef_def_defdef_defdef_def_defdef_def_defdef_defdef_defdef_defdef_def_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdef_defdefdef_defdef_defdef_defdef_defdef_defdefdef_defdefdef_defdefdef_defdef_defdefdefdef_defdefdef_defdefdef_

## Appendix E Additional details on experiments

### Additional result in Section 5.1

In this section, we present the convergence curve for different initialization schemes following the setup in Section 5.1. The results in Figures 4(a) to 4(c) show that when using the same step size \(\gamma=1\) for LeCun and He initialization exhibits similar behavior while NTK initialization leads to slower convergence. This is consistent with our theoretical finding. We also depict the result with larger step size \(\gamma=10\) for NTK initialization in Figure 4(d), following our analysis in Appendix C.5.

### Additional result and set-up in Section 5.2

We have validated Assumption 3 on language data in the main paper. Here we will validate it on image dataset. Specifically, we choose MNIST dataset and consider the embedding in the ViT as \(\bm{X}\), then we plot \(\mathbb{P}\left(\left|\langle\bm{X}_{n}^{\top}\bm{X}_{n},\bm{X}_{n^{\prime}}^ {\top}\bm{X}_{n^{\prime}}\rangle\right|\geq t\right)\) as \(t\) increases in Figure 6, where we could see exponential decay. The architecture of ViT is the same as in Section 5.2.

Additionally, in Figure 7, we can see that \(d_{m}^{-1/2}\)setting achieves faster convergence in training loss and higher test accuracy. In Figure 8, we conduct the experiment on MNIST with regression task by using the same architecture. We take two classes \(\{0,1\}\) in MNIST for binary classification, the output of our regression is taken by \(\text{sign}(f)\), and the training error is given by \(\sum_{n=1}^{N}[\text{sign}(f(\bm{X}_{n}))-y_{n}]^{2}\). We can see that the convergence speed of \(d_{m}^{-1/2}\) is faster than that of \(d_{m}^{-1}\). Overall, these separation results on both classification and regression tasks provide good justification for the \(d_{m}^{-1/2}\) scaling.

Figure 5: Comparison of convergence curve for different initialization schemes. The convergence speed under Lecun/He initialization is generally faster than that of NTK initialization with the same step size.

Figure 6: Verification of Assumption 3 on MNIST dataset.

## Appendix F Limitation

Firstly, in this work, we prove the global convergence guarantee for Transformer under different initialization schemes. Nevertheless, we only consider the gradient descent training under squared loss.

Figure 8: Regression task (with MSE loss) on MNIST dataset.

Figure 7: Test accuracy of classification task on MNIST dataset.

Figure 9: Experimental validation of the theoretical results on Transformers with \(\tau_{0}=d_{m}^{-1}\) scaling trained on synthetic data. (a) Linear convergence. (b) Rate of change of the weights during training. Observe that the weights change very slowly after the \(50^{\text{th}}\) epoch. (c) Evolution of the NTK during the training. The result mirrors the plot (b) and demonstrates how the kernel varies significantly at the beginning of the training and remains approximately constant later. As the width increases, the NTK becomes more stable.

Our result does not cover SGD training and other loss functions, e.g., hinge loss and cross-entropy loss. Secondly, the model architecture being analyzed in this work is the encoder of Transformer which is widely used for regression problems or image classification problems. We do not consider the entire Transformer including the decoder, which is designed for sequence-to-sequence modeling. Lastly, our model does not cover the deep Transformer. We believe our analytic framework paves a way for further analyzing large-scale Transformer.

## Appendix G Societal impact

This work offers a convergence analysis for the Transformer, which is a core element within contemporary large language models. Our theoretical analysis with realistic initializations and scaling schemes lays a theoretical foundation for the interested practitioner in the ML community to further study other priorities of Transformer such as generalization and in-context learning. We do not expect any negative societal bias from this work.