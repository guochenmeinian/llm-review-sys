# Generalized Linear Bandits with Limited Adaptivity

 Ayush Sawarni

Stanford University

ayushsaw@stanford.edu

&Nirjhar Das

Indian Institute of Science Bangalore

nirjhardas@iisc.ac.in

&Siddharth Barman

Indian Institute of Science Bangalore

barman@iisc.ac.in

&Gaurav Sinha

Microsoft Research India

gauravsinha@microsoft.com

Work done while author was at Microsoft Research IndiaWork done while author was at Microsoft Research India

###### Abstract

We study the generalized linear contextual bandit problem within the constraints of limited adaptivity. In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity settings. Given a budget \(M\) on the number of policy updates, in the first setting, the algorithm needs to decide upfront \(M\) rounds at which it will update its policy, while in the second setting it can adaptively perform \(M\) policy updates during its course. For the first setting, we design an algorithm B-GLinCB, that incurs \(\tilde{O}(\sqrt{T})\) regret when \(M=\Omega\left(\log\log T\right)\) and the arm feature vectors are generated stochastically. For the second setting, we design an algorithm RS-GLinCB that updates its policy \(\tilde{O}(\log^{2}T)\) times and achieves a regret of \(\tilde{O}(\sqrt{T})\) even when the arm feature vectors are adversarially generated. Notably, in these bounds, we manage to eliminate the dependence on a key instance dependent parameter \(\kappa\), that captures non-linearity of the underlying reward model. Our novel approach for removing this dependence for generalized linear contextual bandits might be of independent interest.

## 1 Introduction

Contextual Bandits (CB) is an archetypal framework that models sequential decision making in time-varying environments. In this framework, the algorithm (decision maker) is presented, in each round, with a set of arms (represented as \(d\)-dimensional feature vectors), and it needs to decide which arm to play. Once an arm is played, a reward corresponding to the played arm is accrued. The regret of the round is defined as the difference between the maximum reward possible in that round and the reward of the played arm. The goal is to design a policy for selecting arms that minimizes cumulative regret (referred to as the regret of the algorithm) over a specified number of rounds, \(T\). In the last few decades, much progress has been made in designing algorithms for special classes of reward models, e.g. linear model [3, 4, 1, 16], logistic model [6, 2, 7, 28] and generalized linear models [8, 19].

However, despite this progress, there is a key challenge that prevents deployment of CB algorithms in the real world. Practical situations often allow for very limited adaptivity, i.e., do not allow CB algorithms to update their policy at all rounds. For example, in clinical trials [10], each trial involves administering medical treatments to a cohort of patients, with medical outcomes observed and collected for the entire cohort at the conclusion of the trial. This data is then used to design the treatment for the next phase of the trial. Similarly, in online advertising [25] and recommendations [18], updating the policy after every iteration during deployment is often infeasible due to infrastructural constraints. A recent line of work [23, 22, 11, 12, 21, 9, 26] tries to address this limitationby developing algorithms that try to minimize cumulative regret while ensuring that only a limited number of policy updates occur. Across these works, two settings (called **M1** and **M2** from here onwards) of limited adaptivity have been popular. Both **M1, M2** provide a budget \(M\) to the algorithm, determining the number of times it can update its policy. In **M1**[23, 13], the algorithm is required to decide upfront a sub-sequence of \(M\) rounds where policy updates will occur. While in **M2**[1, 23]), the algorithm is allowed to adaptively decide (during its course) when to update its policy.

Limited adaptivity algorithms were recently proposed for the CB problem with linear reward models under the **M1** setting [23, 11], and optimal regret guarantees were obtained when the arm feature vectors were stochastically generated. Similarly, in their seminal work on linear bandits, [1] developed algorithms for the **M2** setting and proved optimal regret guarantees with no restrictions on the arm vectors. While these results provide tight regret guarantees for linear reward models, extending them to generalized linear models is quite a challenge. Straightforward extensions lead to sub-optimal regret with a significantly worse dependence on an instance dependent parameter \(\kappa\) (See Section 2 for definition) that captures non-linearity of the problem instance. In fact, to the best of our knowledge, developing optimal algorithms for the CB problem with generalized linear reward models under the limited adaptivity settings **M1**, **M2**, is an open research question. This is the main focus of our work. We make the following contributions.

### Our Contributions

\(\bullet\) We propose B-GLinCB, an algorithm that solves the CB problem for bounded (almost surely) generalized linear reward models (Definition 2.1) under the **M1** setting of limited adaptivity. We prove that, when the arm feature vectors are generated stochastically, the regret of B-GLinCB at the end of \(T\) rounds is \(\tilde{O}(\sqrt{T})\), when \(M=\Omega(\log\log T)\). When \(M=O(\log\log T)\), we prove an \(\tilde{O}(T^{2^{M-1}/(2^{M}-2)})\) regret guarantee. While the algorithm bears a slight resemblance to the one in [23], direct utilization of their key techniques (distributional optimal design) results in a regret guarantee that scales linearly with the instance dependent non-linearity \(\kappa\). On the other hand, the leading terms in our regret guarantee for B-GLinCB have no dependence on \(\kappa\). To achieve this, we make novel modifications to the key technique of distributional optimal design in [23]. Along with this, the rounds for policy updates are also chosen more carefully (in a \(\kappa\) dependent fashion), leading to a stronger regret guarantee.

\(\bullet\) We propose RS-GLinCB, an algorithm that solves the CB problem for bounded (almost surely) generalized linear reward models (Definition 2.1) under the **M2** setting of limited adaptivity. RS-GLinCB builds on a similar algorithm in [1] by adding a novel context-dependent criterion for determining if a policy update is needed. This new criterion allows us to prove optimal regret guarantee (\(\tilde{O}(\sqrt{T})\)) with only \(O(\log^{2}T)\) updates to the policy. It is quite crucial for the generalized linear reward settings since, without it, the resultant regret guarantees have a linear dependence on \(\kappa\).

\(\bullet\) Our work also resolves a conjecture in [17] by proving an optimal (\(\tilde{O}(\sqrt{T})\)) regret guarantee (for the CB problem with logistic reward model) that does not depend polynomially on \(S\) (the known upper bound on the size of the model parameters, i.e. \(\|\theta^{\star}\|\leq S\), See Section 2) 3. RS-GLinCB is, to our knowledge, the first CB algorithm for generalized linear reward models that is both computationally efficient (amortized \(O(\log T)\) computation per round) and incurs optimal regret. We also perform experiments in Section 5 that validate its superiority both in terms of regret and computational efficiency in comparison to other baseline algorithms proposed in [14] and [6].

Footnote 3: This requires a non-convex projection. We discuss its convex relaxation in Appendix E.

### Important Remarks on Contributions and Comparison with Prior Work

_Remark 1.1_ (\(\kappa\)**-independence**).: For both B-GLinCB and RS-GLinCB, our regret guarantees are free of \(\kappa\) (in their leading term), an instance-dependent parameter that can be exponential in the size of the unknown parameter vector, i.e., \(\|\theta^{\star}\|\) (See Section 2 for definition). Our contribution in this regard is two-fold. Not only do we prove \(\kappa\)-independent regret guarantees under the limited adaptivity constraint, we also characterize a broad class of generalized linear reward models for which a \(\kappa\)-independent regret guarantee can be achieved. Specifically, our results imply that the CB problem with generalized linear reward models originally proposed in [8] and subsequently studied in literature [19, 14, 24] admits a \(\kappa\)-independent regret.

_Remark 1.2_ (**Computational efficiency**).: Efforts to reduce the total time complexity to be linear in \(T\) have been active in the CB literature with generealized linear rewards models. For e.g., [14] recently devised computationally efficient algorithms but they suffer from regret dependence on \(\kappa\). Optimal (\(\kappa\)-independent) guarantees were recently achieved for logistic reward models [6; 2], and the algorithms were subsequently made computationally efficient in [7; 28]. However, the techniques involved rely heavily on the structure of the logistic model and do not easily extend to more general models. To the best of our knowledge, ours is the first work that achieves optimal \(\kappa\)-independent regret guarantees for bounded generalized linear reward models while remaining computationally efficient4.

Footnote 4: While RS-GLinCB and B-GLinCB have total running time of \(\widetilde{O}(T)\), their per-round complexity can reach \(O(T)\). This stands in contrast to [7], which maintains efficiency in both total and per-round time complexity.

_Remark 1.3_ (**Self Concordance of bounded GLMs**).: In order to prove \(\kappa\)-independent regret guarantees, we prove a key result about self concordance of bounded (almost surely) generalized linear models (Definition 2.1) in Lemma 2.2. This result was postulated in [8] for GLMs (with the same definition as ours), but no proof was provided. While [6; 7] partially tackled this issue for logistic reward models5, in our work, we prove self concordance for much more general generalized linear models.

Footnote 5: In [5], a claim about \(\kappa\) independent regret for all generalized linear models with bounded \(\theta^{*}\) was made; however, we can construct counterexamples to this claim (see Appendix C, Remark C.5).

## 2 Notations and Preliminaries

**Notations:** A policy \(\pi\) is a function that maps any given arm set \(\mathcal{X}\) to a probability distribution over the same set, i.e., \(\pi(\mathcal{X})\in\Delta(\mathcal{X})\), where \(\Delta(\mathcal{X})\) is the probability simplex supported on \(\mathcal{X}\). We will denote matrices in bold upper case (e.g. \(\mathbf{M}\)). \(\left\|x\right\|\) denotes the \(\ell_{2}\) norm of vector \(x\). We write \(\left\|x\right\|_{\mathbf{M}}\) to denote \(\sqrt{x^{\top}\mathbf{M}x}\) for a positive semi-definite matrix \(\mathbf{M}\) and vector \(x\). For any two real numbers \(a\) and \(b\), we denote by \(a\wedge b\) the minimum of \(a\) and \(b\). Throughout, \(\widetilde{O}(\cdot)\) denotes big-O notation but suppresses log factors in all relevant parameters. For \(m,n\in\mathbb{N}\) with \(m<n\), we denote the set \(\left\{1,\ldots,n\right\}\) by \([n]\) and \(\left\{m,\ldots,n\right\}\) by \([m,n]\).

**Definition 2.1** (Glm).: A Generalized Linear Model or GLM with parameter vector \(\theta^{*}\in\mathbb{R}^{d}\) is a real valued random variable \(r\) that belongs to the exponential family with density function

\[\mathbb{P}(r\mid x)=\exp\left(r\cdot\left\langle x,\theta^{*}\right\rangle-b \left(\left\langle x,\theta^{*}\right\rangle\right)+c\left(r\right)\right)\]

Function \(b\) (called the log-partition function) is assumed to be twice differentiable and \(\dot{b}\) is assumed to be monotone. Further, we assume that \(r\in[0,R]\) almost surely for some known \(R\in\mathbb{R}\).

Important properties of GLMs such as \(\mathbb{E}[r\mid x]=\dot{b}(\left\langle x,\theta^{*}\right\rangle)\) and variance \(\mathbb{V}[r\mid x]=\ddot{b}(\left\langle x,\theta^{*}\right\rangle)\) are detailed in Appendix C. We define the link function \(\mu\) as \(\mu\left(\left\langle x,\theta^{*}\right\rangle\right):=\mathbb{E}[r\mid x]\). Thus, \(\mu\) is also monotone. We now present a key Lemma on GLMs (see Appendix C for details) that enables us to achieve optimal regret guarantees for our algorithms designed in Sections 3 and 4.

**Lemma 2.2** (Self-Concordance of GLMs).: _For any GLM supported on \([0,R]\) almost surely, the link function \(\mu(\cdot)\) satisfies \([\ddot{\mu}(z)]\leq R\dot{\mu}(z)\), for all \(z\in\mathbb{R}\)._

Next we describe the two CB problems with GLM rewards that we address in this paper. Let \(T\in\mathbb{N}\) be the total number of rounds. At round \(t\in[T]\), we receive an arm set \(\mathcal{X}_{t}\subset\mathbb{R}^{d}\), with number of arms \(K=\left|\mathcal{X}_{t}\right|\) and must select an arm \(x_{t}\in\mathcal{X}_{t}\). Following this, we receive a reward \(r_{t}\) sampled from the GLM distribution \(\mathbb{P}(r|x_{t})\) with unknown \(\theta^{*}\).

**Problem 1:** In this problem we assume that at each round \(t\), the set of arms \(\mathcal{X}_{t}\subset\mathbb{R}^{d}\) is drawn from an unknown distribution \(\mathcal{D}\). Further, we assume the constraints of limited adaptivity setting \(\mathbf{M1}\), i.e., the algorithm is given a budget \(M\in\mathbb{N}\) and needs to decide upfront the \(M\) rounds at which it will update its policy. Let \(\mathsf{supp}\left(\mathcal{D}\right)\) denote the support of distribution \(\mathcal{D}\). We want to design an algorithm that minimizes the expected cumulative regret given as

\[\text{R}_{T}=\mathbb{E}\left[\,\sum_{t=1}^{T}\max_{x\in\mathcal{X}_{t}}\mu \left(\left\langle x,\theta^{*}\right\rangle\right)\;-\;\sum_{t=1}^{T}\mu\left( \left\langle x_{t},\theta^{*}\right\rangle\right)\,\right]\]Here, the expectation is taken over the randomness of the algorithm, the distribution of rewards \(r_{t}\), and the distribution of the arm set \(\mathcal{D}\).

**Problem 2:** In this problem we do not make any assumptions on the arm feature vectors, i.e., the arm vectors can be adversarially chosen. However, we assume the constraints of limited adaptivity setting **M2**, i.e., the algorithm is given a budget \(M\in\mathbb{N}\) and needs to adaptively decide the \(M\) rounds at which it will update its policy (during its course). We want to design an algorithm that minimizes the cumulative regret given as

\[\text{R}_{T}=\sum_{t=1}^{T}\max_{x\in\mathcal{X}_{t}}\mu\left(\left\langle x, \theta^{*}\right\rangle\right)\;-\;\sum_{t=1}^{T}\mu\left(\left\langle x_{t}, \theta^{*}\right\rangle\right)\]

Finally, for both the problems, the Maximum Likelihood Estimator (MLE) of \(\theta^{*}\) can be calculated by minimizing the sum of the log-losses. The log-loss is defined for any given arm \(x\), its (stochastic) reward \(r\) and vector \(\theta\in\mathbb{R}^{d}\) (as the estimator of the true unknown \(\theta^{*}\)) as follows: \(\ell(\theta,x,r)\coloneqq-r\cdot\langle x,\theta\rangle+\int_{0}^{\left\langle x,\theta\right\rangle}\mu(z)dz\). After \(t\) rounds, the MLE \(\widehat{\theta}\) is computed as \(\widehat{\theta}=\arg\min_{\theta}\sum_{s=1}^{t}\ell(\theta,x_{s},r_{s})\).

### Instance Dependent Non-Linearity Parameters

As in prior works [6; 7], we define instance dependent parameters that capture non-linearity of the underlying instance and critically impact our algorithm design. The performance of Algorithm 1 (B-GLinCB) that solves Problem 1, can be quantified using three such parameters that are defined using the derivative of the link function \(\dot{\mu}(\cdot)\). Specifically, for any arm set \(\mathcal{X}\), write optimal arm \(x^{*}=\arg\max_{x\in\mathcal{X}}\mu\left(\left\langle x,\theta^{*}\right\rangle\right)\) and define,

\[\kappa\coloneqq\max_{\mathcal{X}\in\texttt{supp}(\mathcal{D})}\max_{x\in \mathcal{X}}\;\frac{1}{\dot{\mu}\left(\left\langle x,\theta^{*}\right\rangle \right)},\quad\frac{1}{\kappa^{*}}\coloneqq\max_{\mathcal{X}\in\texttt{supp}( \mathcal{D})}\;\dot{\mu}\left(\left\langle x^{*},\theta^{*}\right\rangle\right),\quad\frac{1}{\widehat{\kappa}}\coloneqq\mathop{\mathbb{E}}_{\mathcal{X} \sim\mathcal{D}}\left[\dot{\mu}\left(\left\langle x^{*},\theta^{*}\right\rangle \right)\right]\] (1)

_Remark 2.3_.: These quantities feature prominently in our regret analysis of Algorithm 1. In particular, the dominant term in our regret bound scales as \(O(\sqrt{T/\kappa^{*}})\). We also note that \(\widehat{\kappa}\geq\kappa^{*}\); in fact, for specific distributions \(\mathcal{D}\), the gap between them can be significant. Hence, we also provide a regret upper bound of \(O(\sqrt{T/\widehat{\kappa}})\). In this latter case, however, we incur a worse dependence on \(d\). Section 3 provides a quantified form of this trade-off.

Algorithm 2 (RS-GLinCB) that solves Problem 2, requires another such non-linearity parameter \(\kappa\)6, defined as,

Footnote 6: We overload the notation to match that in the literature. \(\kappa\) in the context of Problem 1 is defined via (1), while \(\kappa\) in the context of Problem 2 by (2).

\[\kappa\coloneqq\max_{x\in\cup_{t=1}^{T}\mathcal{X}_{t}}\frac{1}{\dot{\mu} \left(\left\langle x,\theta^{*}\right\rangle\right)}\] (2)

We note that, here, \(\kappa\) is defined considering the parameter vector \(\theta^{*}\) in contrast to prior work on logistic bandits [7], where its definition involved a maximization over all vectors \(\theta\) with \(\|\theta\|\leq S\) (known upper bound of \(\|\theta^{*}\|\)). Hence, \(\kappa\) as defined here is potentially much smaller and can lead to lower regret, compared to prior works. Standard to the CB literature with GLM rewards, we will assume that tight upper bounds on these parameters is known to the algorithms.

**Assumption 2.4**.: We make the following additional assumptions which are standard for the CB problem with linear or GLM reward models.

\(\bullet\) For every round \(t\in[T]\), and each arm \(x\in\mathcal{X}_{t}\), \(\|x\|\leq 1\).

\(\bullet\) Let \(\theta^{*}\) be the unknown parameter of the GLM reward, then \(\|\theta^{*}\|\leq S\) for a known constant \(S\).

### Optimal Design Policies

**G-optimal Design** Given an arm set \(\mathcal{X}\), the G-optimal design policy \(\pi_{G}\) is the solution of the following optimization problem: \(\arg\min_{\lambda\in\Delta(\mathcal{X})}\max_{x\in\mathcal{X}}\|x\|_{\mathbf{ U}(\lambda)^{-1}}^{2}\), where \(\mathbf{U}(\lambda)=\mathbb{E}_{x\sim\lambda}[xx^{\mathsf{T}}]\). Now consider the following optimization problem, also known as the D-optimal design problem: \(\max_{\lambda\in\Delta(\mathcal{X})}\log\mathsf{Det}(\mathbf{U}(\lambda))\). This is a concave maximization problem as opposed to the G-optimal design which is non-convex. We have the following equivalence theorem due to Kiefer and Wolfowitz [15]:

**Lemma 2.5** (Keifer-Wolfowitz).: _Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be any set of arms and \(\mathbf{W}_{G}\) be the expected design matrix, defined as \(\mathbf{W}_{G}\coloneqq\mathbb{E}_{x\sim\pi_{G}(\mathcal{X})}\left[xx^{\mathsf{ T}}\right]\), with \(\pi_{G}(\mathcal{X})\) as the solution to the D-optimal design problem. Then, \(\pi_{G}(\mathcal{X})\) also solves the G-optimal design problem, and for all \(x\in\mathcal{X}\), \(\|x\|_{\mathbf{W}_{G}^{-1}}^{2}\leq d\)._

**Distributional optimal design** Notably, the upper bound on \(\|x\|_{\mathbf{W}_{G}^{-1}}\) specified in Lemma 2.5 holds only for the arms \(x\) in \(\mathcal{X}\). When the arm set \(\mathcal{X}_{t}\) varies from round to round, securing a guarantee analogous to Lemma 2.5 is generally challenging. Nonetheless, when the arm sets \(\mathcal{X}_{t}\) are drawn from a distribution, it is possible to extend the guarantee, albeit with a worse dependence on \(d\); see Section A.5 in Appendix A. Improving this dependence motivates the need of studying Distributional optimal design and towards this we utilize the results of [23].

The distributional optimal design policy is defined using a collection of tuples \(\mathcal{M}=\{(p_{i},\mathbf{M}_{i}):p_{1},\ldots,p_{n}\geq 0\text{ and }\sum_{i}p_{i}=1\}\), wherein each \(\mathbf{M}_{i}\) is a \(d\times d\) positive semi-definite matrix and \(n\leq 4d\log d\). The collection \(\mathcal{M}\) is detailed next. Let \(\text{softmax}_{\alpha}\left(\{s_{1},\ldots,s_{k}\}\right)\) denote the probability distribution where the \(i^{th}\) element is sampled with probability \(\frac{\sum_{j=1}^{n}s_{j}^{n}}{\sum_{j=1}^{n}s_{j}^{n}}\). For a specific \(\mathcal{M}=\{(p_{i},\mathbf{M}_{i})\}_{i=1}^{n}\), and each \(i\in[n]\) write \(\pi_{\mathbf{M}_{i}}\left(\mathcal{X}\right)=\text{softmax}_{\alpha}(\left\{ \left\|x\right\|_{\mathbf{M}_{i}}^{2}:x\in\mathcal{X}\right\})\). Finally, with \(\pi_{G}\) as the G-optimal design policy (Section 2.2), we define the Distributional optimal design policy \(\pi\) as

\[\pi\left(\mathcal{X}\right)=\begin{cases}\pi_{G}\left(\mathcal{X}\right)&\text {with probability }1/2\\ \pi_{\mathbf{M}_{i}}\left(\mathcal{X}\right)&\text{with probability }p_{i}/2 \end{cases}\]

Given a collection of arm sets \(\{\mathcal{X}_{1},\ldots,\mathcal{X}_{s}\}\) (called _core set_) sampled from the distribution \(\mathcal{D}\), we utilize Algorithm 2 of [23] to find the collection \(\mathcal{M}\); see Algorithm 4 of [23]. Overall, the computed \(\mathcal{M}\) induces a policy \(\pi\) that upholds the following guarantee.

**Lemma 2.6** (Theorem 5, [23]).: _Let \(\pi\) be the Distributional optimal design policy that has been learnt from \(s\) independent samples \(\mathcal{X}_{1},\ldots\mathcal{X}_{s}\sim\mathcal{D}\). Also, let \(\mathbf{W}\) denote the expected design matrix, \(\mathbf{W}=\mathbb{E}_{\mathcal{X}\sim\mathcal{D}}\left[\mathbb{E}_{x\sim \pi(\mathcal{X})}\left[xx^{\mathsf{T}}\mid\mathcal{X}\right]\right]\). Then,_

\[\mathbb{P}\left\{\mathbb{E}_{\mathcal{X}\sim\mathcal{D}}\left[\max_{x\in \mathcal{X}}\ \|x\|_{\mathbf{W}^{-1}}\right]\leq O\left(\sqrt{d\log d}\right)\right\}\geq 1 -\exp\left(O\left(d^{4}\log^{2}d\right)-sd^{-12}\cdot 2^{-16}\right).\]

## 3 B-GLinCB

In this section, we present B-GLinCB (Algorithm 1) that solves **Problem 1** described in Section 2, which enforces constraints of limited adaptivity setting **M1**. Given limited adaptivity budget \(M\in\mathbb{N}\), our algorithm first computes the batch length for each of the \(M\) batches (i.e., determine rounds wherethe policy remains constant). We build upon the batch length construction in [9]; however, the first batch is chosen to be \(\kappa\) dependent which crucially helps in removing \(\kappa\) from the leading term in the regret. 7

Footnote 7: We note that in case \(\kappa\) is unknown, any known upper bound on \(\kappa\) suffices for the algorithm.

**Batch Lengths**: For each batch \(k\in[M]\), let \(\mathcal{T}_{k}\) denote all the rounds within the \(k^{th}\) batch. We will refer to the first batch \(\mathcal{T}_{1}\) as the warm-up batch. The batch lengths \(\tau_{k}\coloneqq|\mathcal{T}_{k}|\), \(k\in[M]\) are calculated as follows:

\[\tau_{1}:=\left(\frac{\sqrt{\kappa}\,e^{3S}d^{2}\gamma^{2}}{S}\alpha\right)^{ 2/3},\quad\tau_{2}:=\alpha,\quad\tau_{k}:=\alpha\sqrt{\tau_{k-1}},\text{ for }k\in[3,M]\] (3)

where \(\gamma\coloneqq 30RS\sqrt{d\log T}\)8 and \(\alpha=T^{\frac{1}{2(1-2^{-M+1})}}\) if \(M\leq\log\log T\) and \(\alpha=2\sqrt{T}\) otherwise.

Footnote 8: Recall that \(R\) provides an upper bound on the stochastic rewards and \(S\) is an upper bound on the norm of \(\theta^{*}\).

During the warm-up batch (Lines 2, 3), the algorithm follows the G-optimal design policy, \(\pi_{G}\). At the end of the warm-up batch (Line 4), the algorithm computes the Maximum Likelihood Estimate (MLE), \(\widehat{\theta}_{w}\), of \(\theta^{*}\)9, and design matrix \(\mathbf{V}\coloneqq\sum_{t\in\mathcal{T}_{k}}x_{t}x_{t}^{\mathsf{T}}+\lambda \mathbf{I}\), with parameter \(\lambda=20Rd\log T\).

Footnote 9: In case the MLE lies outside the set \(\{\theta^{*}:\|\theta^{*}\|\leq S\}\), we apply the projection step detailed in Appendix E.

Now, for each batch \(k\geq 2\) and every round \(t\in\mathcal{T}_{k}\), the algorithm updates \(\mathcal{X}_{t}\) by eliminating arms from it using the confidence bounds (see Equation (7)) computed in the previous batches (Line 10). The algorithm next computes \(\widetilde{\mathcal{X}}_{t}\), a scaled version of \(\mathcal{X}_{t}\), as follows, with \(\beta(x)\) define in equation (5),

\[\widetilde{\mathcal{X}}_{t}\coloneqq\left\{\sqrt{\hat{\mu}(\langle x,\widehat {\theta}_{w}\rangle)/\beta(x)}\ \ x:\ x\in\mathcal{X}_{t}\right\}.\] (4)

Finally, we use the distributional optimal design policy \(\pi_{k}\), on the scaled arm set \(\widetilde{\mathcal{X}}_{t}\), to sample the next arm (Line 11). At the end of every batch, we equally divide the batch \(\mathcal{T}_{k}\) into two sets \(\mathcal{A}\) and \(\mathcal{B}\). We use samples from \(\mathcal{A}\) to compute the estimator \(\widehat{\theta}_{k}\) and the scaled design matrix \(\mathbf{H}_{k}\). The rounds in \(\mathcal{B}\) are used to compute \(\pi_{k+1}\), the distributional optimal design policy for the next batch. It is important to note while the policy \(\pi_{k}\) is utilized in each round (Line 11) to draw arms, it is updated (to \(\pi_{k+1}\)) only at the end of the batch. Hence, conforming to setting **M1**, the algorithm updates the selection policy at \(M\) rounds that were decided upfront.

**Confidence Bounds:** The scaled design matrix \(\mathbf{H}_{k}\), an estimator of the Hessian, is computed at the end of each batch \(k\in 2,\ldots,M\) (Line 13):

\[\mathbf{H}_{k}=\sum_{t\in\mathcal{A}}\left(\hat{\mu}(\langle x_{t},\widehat {\theta}_{w}\rangle)/\beta(x_{t})\right)x_{t}x_{t}^{\mathsf{T}}+\lambda \mathbf{I},\quad\text{where}\quad\beta(x)=\exp\left(R\min\left\{2S,\gamma \sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\right\}\right)\] (5)

where \(\mathcal{A}\) is the first half of \(\mathcal{T}_{k}\). Using this, we define the upper and lower confidence bounds (\(UCB_{k}\) and \(LCB_{k}\)) computed at the end of batch \(\mathcal{T}_{k}\):

\[UCB_{k}(x) \coloneqq\begin{cases}\langle x,\widehat{\theta}_{w}\rangle+ \gamma\sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}&k=1\\ \langle x,\widehat{\theta}_{k}\rangle+\gamma\left\|x\right\|_{\mathbf{H}_{k}^ {-1}}&k>1\end{cases},\] (6) \[LCB_{k}(x) \coloneqq\begin{cases}\langle x,\widehat{\theta}_{w}\rangle- \gamma\sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}&k=1\\ \langle x,\widehat{\theta}_{k}\rangle-\gamma\left\|x\right\|_{\mathbf{H}_{k}^ {-1}}&k>1\end{cases}\] (7)

_Remark 3.1_.: The confidence bounds employed by the algorithm exhibit a significant distinction between the first batch and subsequent batches. While the first batch's bounds are influenced by the parameter \(\kappa\), subsequent batches utilize \(\kappa\)-independent bounds. This difference arises from the use of the standard design matrix \(\mathbf{V}\) in the first batch and a scaled design matrix \(\mathbf{H}_{k}\) (equation 5) in later batches, leveraging the self-concordance property of GLM rewards to achieve \(\kappa\)-independence. Notably, the first batch's confidence bounds influence the scaling factor \(\beta(x)\) in later batches, creating a trade-off (addressed in the regret analysis in Appendix A) where an inaccurate estimate of \(\widehat{\theta}_{w}\) can exponentially increase the scaling factor and confidence bounds.

In Theorem 3.2 and Corollary 3.3, we present our regret guarantee for B-GLinCB. Detailed proofs for both are provided in Appendix A. The computational efficiency of B-GLinCB is discussed in Appendix D.

**Theorem 3.2**.: _Algorithm 1 (B-GLinCB) incurs regret \(\mathsf{R}_{T}\leq(\mathsf{R}_{1}+\mathsf{R}_{2})\log\log T\)10, where_

Footnote 10: Note that \(\mathsf{R}_{T}\) is expected regret. See the 2.

\[\mathsf{R}_{1} =O\left(RSd\ \left(\sqrt{\frac{d}{\widetilde{\kappa}}}\wedge \sqrt{\frac{1}{\kappa^{*}}}\right)T^{\frac{1}{2\left(1-2^{1-M}\right)}}\log T\right)\text { and}\] \[\mathsf{R}_{2} =O\left(\kappa^{1/3}d^{2}e^{2S}(RS\log T)^{2/3}T^{\frac{1}{3(1-2 ^{1-M})}}\right).\]

**Corollary 3.3**.: _When the number of batches \(M\geq\log\log T\), Algorithm 1 achieves a regret bound of_

\[\mathsf{R}_{T}\leq\widetilde{O}\left(\left(\sqrt{\frac{d}{\widetilde{\kappa}} }\wedge\sqrt{\frac{1}{\kappa^{*}}}\right)dRS\sqrt{T}+d^{2}e^{2S}(S^{2}R^{2} \kappa T)^{1/3}\right).\]

_Remark 3.4_.: Scaling the arm set (as in (4)) for optimal design is a crucial aspect of our algorithm, allowing us to obtain tight estimates of \(\dot{\mu}\left(\langle x,\theta^{*}\rangle\right)\) (see Lemma A.10). This result relies on multiple novel ideas and techniques, including self-concordance for GLMs, matrix concentration, Bernstein-type concentration for the canonical exponential family (Lemma A.1), and application of distributional optimal design on scaled arm set.

_Remark 3.5_.: The \(\kappa\)-dependent batch construction is a crucial feature of our algorithm, enabling effective estimation of \(\dot{\mu}(\langle x,\theta^{*}\rangle)\) at the end of the first batch. Since the first batch incurs regret linear in its length, achieving a \(\kappa\)-independent guarantee requires the first batch to be \(o(\sqrt{T})\). We demonstrate that choosing \(\tau_{1}=O(T^{\frac{1}{3}})\) is sufficient for this purpose (see Appendix A).

## 4 Rs-GLinCB

In this section we present RS-GLinCB (Algorithm 2) that solves **Problem 2** described in Section 2, which enforces constraints of limited adaptivity setting **M2**. This algorithm incorporates a novel switching criterion (Line 4), extending the determinant-doubling approach of [1]. Additionally, we introduce an arm-elimination step (Line 12) to obtain tighter regret guarantees. Throughout this section, we set \(\lambda=d\log(T/\delta)/R^{2}\) and \(\gamma=25RS\sqrt{d\log\left(T/\delta\right)}\).

At round \(t\), on receiving an arm set \(\mathcal{X}_{t}\), RS-GLinCB first checks the Switching Criterion I (Line 4). This criterion checks whether for any arm \(x\in\mathcal{X}_{t}\) the quantity \(\left\lVert x\right\rVert_{\mathsf{V}^{-1}}\) is greater than a carefully chosen \(\kappa\)-dependent threshold. Here \(\mathsf{V}\) is the design matrix corresponding to all arms that have been played in the rounds in \(\mathcal{T}_{o}\) (\(:=\) the set of rounds preceding round \(t\), where Switching Criterion I was triggered). Under this criterion the arm that maximizes \(\left\lVert x\right\rVert_{\mathsf{V}^{-1}}\) is played (call this arm \(x_{t}\)) and the corresponding reward is obtained. Subsequently in Line 6, the set \(\mathcal{T}_{o}\) is updated to include \(t\); thedesign matrix \(\mathbf{V}\) is updated as \(\mathbf{V}\leftarrow\mathbf{V}+x_{t}x_{t}^{\intercal}\); and the scaled design matrix \(\mathbf{H}_{t+1}\) is set to \(\mathbf{H}_{t}\). The MLE is computed (Line 7) based on the data in the rounds in \(\mathcal{T}_{o}\) to obtain \(\widehat{\theta}_{o}\).

When Switching Criterion I is not triggered, the algorithm first checks (Line 9) the Switching Criterion II, that is whether the determinant of the scaled design matrix \(\mathbf{H}_{t}\) has become more than double of that of \(\mathbf{H}_{\tau}\) (where \(\tau\) is the last round before \(t\) when Switching Criterion II was triggered). If Switching Criterion II is triggered at round \(t\), then in Line 10, the algorithm sets \(\tau\gets t\) and recomputes the MLE over all the past rounds except those in \(\mathcal{T}_{o}\) to obtain \(\widehat{\theta}\). Then \(\widehat{\theta}\) is projected into an ellipsoid around \(\widehat{\theta}_{o}\) to obtain the estimate \(\widehat{\theta}_{\tau}\) via the following optimization problem11,

Footnote 11: This optimization problem is non-convex. However, a convex relation of this optimization problem is detailed in Appendix E, which leads to slightly worse regret guarantees in \(\mathsf{poly}(R,S)\).

\[\min_{\theta}\left\|\sum_{s\in\mathcal{T}_{o}}\left(\mu\left(\left\langle x_{s },\theta\right\rangle\right)-\mu(\left\langle x_{s},\widetilde{\theta}\right \rangle\right)\right)x_{s}\right\|_{\mathbf{H}(\theta)}\text{ s.t. }\left\|\theta- \widehat{\theta}_{o}\right\|_{\mathbf{V}}\leq\gamma\sqrt{\kappa}.\] (8)

Here \(\mathbf{H}(\theta)\coloneqq\sum_{s\in\mathcal{T}_{o}}\dot{\mu}\left(\left\langle x _{s},\theta\right\rangle\right)x_{s}x_{s}^{\intercal}\). After checking Switching Criterion II, the algorithm performs an arm elimination step (Line 12) based on the parameter estimate \(\widehat{\theta}_{o}\) as follows: for every arm \(x\in\mathcal{X}_{t}\), we compute \(UCB_{o}(x)=\left\langle x,\widehat{\theta}_{o}\right\rangle+\gamma\sqrt{ \kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\) and \(LCB_{o}(x)=\left\langle x,\widehat{\theta}_{o}\right\rangle-\gamma\sqrt{ \kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\)12. Then, \(\mathcal{X}_{t}\) is updated by eliminating from it the arms with \(UCB_{o}(\cdot)\) less than the highest \(LCB_{o}(\cdot)\). For arms in the reduced arm set \(\mathcal{X}_{t}\), RS-GLinCB computes the index \(UCB(x,\mathbf{H}_{\tau},\widehat{\theta}_{\tau})\coloneqq\left\langle x, \widehat{\theta}_{\tau}\right\rangle+150\left\|x\right\|_{\mathbf{H}_{\tau}^{ -1}}\sqrt{d\log\left(T/\delta\right)}\), and plays the arm \(x_{t}\) with the highest index (Line 13). After observing the subsequent reward \(r_{t}\), the algorithm updates the scaled design matrix \(\mathbf{H}_{t}\) (Line 14) as follows: \(\mathbf{H}_{t+1}\leftarrow\mathbf{H}_{t}+(\dot{\mu}(\left\langle x_{t}, \widehat{\theta}_{o}\right\rangle)/e)x_{t}x_{t}^{\intercal}\). With this, the round \(t\) ends and the algorithm moves to the next round. Next, in Lemma 4.1 and Theorem 4.2 we present the guarantees on number of policy updates and regret, respectively, for RS-GLinCB. Detailed proofs for both are provided in Appendix B.

Footnote 12: We note that in case \(\kappa\) is unknown, any known upper bound on \(\kappa\) suffices for the algorithm.

**Lemma 4.1**.: _RS-GLinCB (Algorithm 2), during its entire execution, updates its policy at most \(O(R^{4}S^{2}\;\kappa d^{2}\;\log^{2}(T/\delta))\) times._

**Theorem 4.2**.: _Given \(\delta\in(0,1)\), with probability \(\geq 1-\delta\), the regret of RS-GLinCB (Algorithm 2) satisfies \(\mathsf{R}_{T}=O\big{(}d\sqrt{\sum_{t\in[T]}\dot{\mu}\left(\left\langle x_{t}^ {\star},\theta^{\star}\right\rangle\right)}\log\left(RT/\delta\right)+\;\kappa d ^{2}R^{5}S^{2}\log^{2}\left(T/\delta\right)\big{)}.\)_

_Remark 4.3_.: Switching Criterion I is essential in delivering tight regret guarantees in the non-linear setting. Unlike existing literature [7], which relies on warm-up rounds based on observed rewards (hence heavily dependent on reward models), RS-GLinCB presents a context-dependent criterion that implicitly checks whether the estimate \(\dot{\mu}(\left\langle x,\widehat{\theta}_{o}\right\rangle)\) is within a constant factor of \(\dot{\mu}\left(\left\langle x,\theta^{\star}\right\rangle\right)\) (see Lemmas B.3 and B.4). We show that the number of times Switching Criterion I is triggered is only \(O(\kappa d^{2}\log^{2}(T))\) (see Lemma B.11), hence incurring a small regret in these rounds.

_Remark 4.4_.: Unlike [1], our determinant-doubling Switching Criterion II uses the scaled design matrix \(\mathbf{H}_{t}\) instead of the unscaled version (similar to \(\mathbf{V}\)). The matrix \(\mathbf{H}_{t}\), estimating the Hessian of the log-loss, is crucial for achieving optimal regret. This modification is crucial in extending algorithms satisfying limited adaptivity setting \(\mathbf{M2}\) for the CB problem with a linear reward model to more general GLM reward models.

_Remark 4.5_.: The feasible set for the optimization stated in \(8\) is an ellipsoid around \(\widehat{\theta}_{o}\), which contains \(\theta^{\star}\) with high probability. Deviating from existing literature on GLM Bandits which projects the estimate into the ball set of radius \(S\) (\(\{\theta:\|\theta\|\leq S\}\)), our projection step leads to tighter regret guarantees; notably, the leading \(\sqrt{T}\) term is free of parameters \(S\) (and \(R\)). This resolves the conjecture made in [17] regarding the possibility of obtaining \(S\)-free regret in the \(\sqrt{T}\) term in logistic bandits.

_Remark 4.6_.: The regret guarantees of the logistic bandit algorithms in [2, 17] have a second-order term that is minimum of an arm-geometry dependent quantity (see Theorem 3 of [17]) and a \(\kappa\)-dependent term similar to our regret guarantee. Although our analysis is not able to accommodate this arm-geometry dependent quantity, we underscore that our algorithm is computationally efficient while the above works are not. In fact, to the best of our knowledge, the other known efficient algorithms for logistic bandits [7, 28] also do not achieve the arm-geometry dependent regret term. It can be interesting to design an efficient algorithm that is able to achieve the same guarantees in the second-order regret term as in [2, 17].

## 5 Experiments

We tested the practicality of our algorithm RS-GLinCB against various baselines for logistic and generalized linear bandits. For these experiments, we adjusted the Switching Criterion I threshold constant in RS-GLinCB to \(0.01\) and used data from both Switching Criteria (I and II) rounds to estimate \(\widetilde{\theta}\). These modifications do not affect the overall efficiency as \(\widetilde{\theta}\) is calculated only \(O(\log(T))\) times. The experiment code is available at https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity.

**Logistic.** We compared RS-GLinCB against ECOLog[7] and GLOC[14], the only algorithms with overall time complexity \(\tilde{O}(T)\) for this setting. The dimension was set to \(d=5\), number of arms per round to \(K=20\), and \(\theta^{*}\) was sampled from a \(d\)-dimensional sphere of radius \(S=5\). Arms were sampled uniformly from the \(d\)-dimensional unit ball. We ran simulations for \(T=20,000\) rounds, repeating them 10 times. RS-GLinCB showed the smallest regret with a flattened regret curve, as seen in Fig. 1 (top-left).

**Probit.** For the probit reward model, we compared RS-GLinCB against GLOC and GLM-UCB[8]. The dimension was set to \(d=5\) and number of arms per round to \(K=20\). \(\theta^{*}\) was sampled from a \(d\)-dimensional sphere of radius \(S=3\). Arm features were generated similarly as in the logistic bandit simulation. We ran simulations for \(T=5,000\) rounds, repeating them 10 times. RS-GLinCB outperformed both baselines, as shown in Fig. 1 (top-right).

**Comparing Execution Times.** We compared the execution times of RS-GLinCB and ECOLog. We created two logistic bandit instances with \(d=5\) and \(K=20\), and different \(\kappa\) values. We ran both algorithms for \(T=20,000\) rounds, repeating each run 20 times. For low \(\kappa\), RS-GLinCB took about one-fifth of the time of ECOLog, and for high \(\kappa\), slightly more than one-third, as seen in Fig. 1 (left-bottom). This demonstrates that RS-GLinCB has a significantly lower computational overhead compared to ECOLog. We also compared the execution times of RS-GLinCB and GLOC under the probit reward model, creating two bandit instances with \(d=5\) and \(K=20\), but with differing \(\kappa\). We ran both algorithms for \(T=20,000\) rounds, repeating each run 20 times. The result is shown in Fig. 1 (bottom-right). We observe that for low \(\kappa\), RS-GLinCB takes less than half time of GLOC while for high \(\kappa\), it takes about two-third time of GLOC. A more detailed discussion of these experiments is provided in Appendix D.

Figure 1: Top: Cumulative Regret vs. number of rounds for Logistic (left) and Probit (right) reward models. Bottom: (left) Execution times of ECOLog and RS-GLinCB for different values of \(\kappa\) (low \(\kappa=9.3\) and high \(\kappa=141.6\)) for Logistic rewards. (right) Execution times of GLOC and RS-GLinCB for different values of \(\kappa\) (low \(\kappa=17.6\) and high \(\kappa=202.3\)) for Probit rewards.

Conclusion and Future Work

The Contextual Bandit problem with GLM rewards is a ubiquitous framework for studying online decision-making with non-linear rewards. We study this problem with a focus on limited adaptivity. In particular, we design algorithms B-GLinCB and RS-GLinCB that obtain optimal regret guarantees for two prevalant limited adaptivity settings **M1** and **M2** respectively. A key feature of our guarantees are that their leading terms are independent of an instance dependent parameter \(\kappa\) that captures non-linearity. To the best of our knowledge, our paper provides the first algorithms for the CB problem with GLM rewards under limited adaptivity (and otherwise) that achieve \(\kappa\)-independent regret. The regret guarantee of RS-GLinCB, not only aligns with the best-known guarantees for Logistic Bandits but enhances them by removing the dependence on \(S\) (upper bound on \(\|\theta^{*}\|\)) in the leading term of the regret and therefore resolves a conjecture in [17]. The batch learning algorithm B-GLinCB, for \(M=\Omega(\log{(\log{T})})\), achieves a regret of \(\widetilde{O}\left(dRS\left(\sqrt{d/\widehat{\kappa}}\wedge\sqrt{1/\kappa^{* }}\right)\sqrt{T}\right)\). We believe that the dependence on \(d\) along with the \(\widehat{\kappa}\) term is not tight and improving the dependence is a relevant direction for future work.

## Acknowledgments and Disclosure of Funding

Siddharth Barman gratefully acknowledges the support of the Walmart Center for Tech Excellence (CSR WMGT-23-0001) and a SERB Core research grant (CRG/2021/006165).

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Marc Abeille, Louis Faury, and Clement Calauzenes. Instance-wise minimax-optimal algorithms for logistic bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 3691-3699. PMLR, 2021.
* [3] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* [4] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Geoffrey Gordon, David Dunson, and Miroslav Dudik, editors, _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pages 208-214, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR.
* [5] Louis Faury. _Variance-sensitive confidence intervals for parametric and offline bandits_. Theses, Institut Polytechnique de Paris, Oct 2021.
* [6] Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In _International Conference on Machine Learning_, pages 3052-3060. PMLR, 2020.
* [7] Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clement Calauzenes. Jointly efficient and optimal algorithms for logistic bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 546-580. PMLR, 2022.
* [8] Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. _Advances in neural information processing systems_, 23, 2010.
* [9] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem. _Advances in Neural Information Processing Systems_, 32, 2019.
* [10] International Stroke Trial Collaborative Group et al. The international stroke trial (ist): a randomised trial of aspirin, subcutaneous heparin, both, or neither among 19 435 patients with acute ischaemic stroke. _The Lancet_, 349(9065):1569-1581, 1997.

* [11] Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu Ye. Sequential batch learning in finite-action linear contextual bandits. _arXiv preprint arXiv:2004.06321_, 2020.
* [12] Osama Hanna, Lin Yang, and Christina Fragouli. Learning from distributed users in contextual linear bandits without sharing the context. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [13] Osama Hanna, Lin Yang, and Christina Fragouli. Efficient batched algorithm for contextual linear bandits with large action space via soft elimination. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [14] Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. _Advances in Neural Information Processing Systems_, 30, 2017.
* [15] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. _Canadian Journal of Mathematics_, 12:363-366, 1960.
* [16] Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* [17] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial) logistic bandits via regret-to-confidence-set conversion. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, pages 4474-4482. PMLR, 02-04 May 2024.
* [18] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670, 2010.
* [19] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* [20] Blake Mason, Kwang-Sung Jun, and Lalit Jain. An experimental design approach for regret minimization in logistic bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7736-7743, 2022.
* [21] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. In Peter Grunwald, Elad Hazan, and Satyen Kale, editors, _Proceedings of The 28th Conference on Learning Theory_, volume 40 of _Proceedings of Machine Learning Research_, pages 1456-1456, Paris, France, 03-06 Jul 2015. PMLR.
* [22] Zhimei Ren and Zhengyuan Zhou. Dynamic batch learning in high-dimensional sparse linear contextual bandits. _Management Science_, 70(2):1315-1342, 2024.
* [23] Yufei Ruan, Jiaqi Yang, and Yuan Zhou. Linear bandits with limited adaptivity and learning distributional optimal design. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 74-87, 2021.
* [24] Yoan Russac, Olivier Capp'e, and Aurelien Garivier. Algorithms for non-stationary generalized linear bandits. _ArXiv_, abs/2003.10113, 2020.
* [25] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using multi-armed bandit experiments. _Marketing Science_, 36(4):500-522, 2017.
* [26] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 47(3):1904-1931, 2022.
* [27] Joel A Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* [28] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret and constant computation cost. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

## Generalized Linear Bandits with Limited Adaptivity Appendix

### Table of Contents

* 1 Introduction
	* 1.1 Our Contributions
	* 1.2 Important Remarks on Contributions and Comparison with Prior Work
* 2 Notations and Preliminaries
	* 2.1 Instance Dependent Non-Linearity Parameters
	* 2.2 Optimal Design Policies
* 3 B-GLinCB
* 4 RS-GLinCB
* 5 Experiments
* 6 Conclusion and Future Work
* A Regret Analysis of B-GLinCB
* A.1 Additional Notation
* A.2 Concentration Inequalities and Confidence Intervals
* A.3 Preliminary Lemmas
* A.4 Proof of Theorem 3.2
* A.5 Optimal Design Guarantees
* B Regret Analysis of RS-GLinCB
* B.1 Confidence Sets for Switching Criterion I rounds
* B.2 Confidence Sets for non-Switching Criterion I rounds
* B.3 Bounding the instantaneous regret
* B.4 Proof of Theorem 4.2
* B.5 Bounding number of policy updates: Proof of Lemma 4.1
* B.6 Some Useful Lemmas
* C Useful Properties of GLMs
* D Computational Cost
* E Projection
* E.1 Convex Relaxation
Regret Analysis of B-GLinCB

### Additional Notation

We write \(\mathbf{c}\) to denote absolute constant(s) that appears throughout our analysis. Our analysis also utilizes the following function

\[\gamma(\lambda)=24RS\left(\sqrt{\log\left(T\right)+d}+\frac{R\left(\log\left(T \right)+d\right)}{\sqrt{\lambda}}\right)+2S\sqrt{\lambda}.\] (9)

Note that \(\gamma(\lambda)\) is a 'parameterized' version of \(\gamma\) (which was defined in section 3). In our proof, we present the arguments using this parameterized version. A direct minimization of the above expression in terms of \(\lambda\) would not suffice since we need \(\lambda\) to be sufficiently large for certain matrix concentration lemmas to hold (see Section A.5). However, later we show that setting \(\lambda\) equal to \(\mathbf{c}Rd\log T\) leads to the desired bounds.

We use \(\widetilde{x}\) to denote the scaled versions of the arms (see Line 11 of the algorithm); in particular,

\[\widetilde{x}\coloneqq\sqrt{\frac{\dot{\mu}\left(\left\langle x,\widehat{ \theta}_{w}\right\rangle\right)}{\beta(x)}}x\] (10)

Furthermore, to capture the non-linearity of the problem, we introduce the term \(\phi(\lambda)\):

\[\phi(\lambda)\coloneqq\frac{\sqrt{\kappa}\;e^{3S}\;\gamma(\lambda)^{2}}{S}.\]

Recall that the scaled data matrix \(\mathbf{H}_{k}\) (for each batch \(k\)) was computed using \(\widehat{\theta}_{w}\) as follows

\[\mathbf{H}_{k}=\sum_{t\in\mathcal{T}_{k}}\frac{\dot{\mu}\left(\left\langle x_ {j},\widehat{\theta}_{w}\right\rangle\right)}{\beta(x_{t})}x_{t}x_{t}^{\mathsf{ T}}+\lambda\mathbf{I}.\]

Following the definition of \(\mathbf{H}_{k}\) and using the true vector \(\theta^{*}\) we define

\[\mathbf{H}_{k}^{*}=\sum_{t\in\mathcal{T}_{k}}\dot{\mu}\left(\left\langle x_{t},\theta^{*}\right\rangle\right)x_{t}x_{t}^{\mathsf{T}}+\lambda\mathbf{I}.\]

We will show that \(\mathbf{H}_{k}\) dominates \(\mathbf{H}_{k}^{*}\) with high probability. Furthermore, we assume that the MLE estimator \(\theta^{*}\) obtained by minimizing the log-loss objective always satisfies \(\left\|\widehat{\theta}\right\|\leq S\). In case, that's not true, one can use the non-convex projection described in Appendix E. The projected vector satisfies the same guarantees as described in the subsequent lemmas up to a multiplicative factor of \(2\). Hence, the assumption \(\left\|\widehat{\theta}\right\|\leq S\) is non-limiting.

### Concentration Inequalities and Confidence Intervals

**Lemma A.1** (Bernstein's Inequality).: _Let \(X_{1},\ldots,X_{n}\) be a sequence of independent random variables with \(\left|X_{t}-\mathbb{E}\left[X_{t}\right]\right|\leq b\). Also, let sum \(S:=\sum_{t=1}^{n}\left(X_{t}-\mathbb{E}\left[X_{t}\right]\right)\) and \(v:=\sum_{t=1}^{m}\text{Var}[X_{t}]\). Then, for any \(\delta\in[0,1]\), we have_

\[\mathbb{P}\left\{S\geq\sqrt{2v\log\frac{1}{\delta}}+\frac{2b}{3}\log\frac{1}{ \delta}\right\}\leq\delta.\]

**Lemma A.2**.: _Let \(\mathcal{X}=\left\{x_{1},x_{2},\ldots,x_{s}\right\}\in\mathbb{R}^{d}\) be a set of vectors with \(\left\|x_{t}\right\|\leq 1\), for all \(t\in[s]\), and let scalar \(\lambda\geq 0\). Also, let \(r_{1},r_{2},\ldots,r_{s}\in[0,R]\) be independent random variables distributed by the canonical exponential family; in particular, \(\mathbb{E}\left[r_{s}\right]=\mu\left(\left\langle x_{s},\theta^{*}\right\rangle\right)\) for \(\theta^{*}\in\mathbb{R}^{d}\). Further, let \(\widehat{\theta}=\arg\min_{\theta}\sum_{s=1}^{t}\ell(\theta,x_{s},r_{s})\) be the maximum likelihood estimator of \(\theta^{*}\) and let matrix_

\[\mathbf{H}^{*}=\sum_{j=1}^{s}\dot{\mu}\left(\left\langle x_{j},\theta^{*} \right\rangle\right)x_{j}x_{j}^{\mathsf{T}}+\lambda\mathbf{I}.\]_Then, with probability at least than \(1-\frac{1}{T^{2}}\), the following inequality holds_

\[\left\|\theta^{*}-\widehat{\theta}\right\|_{\mathbf{H}^{*}}\leq 24RS\left( \sqrt{\log\left(T\right)+d}+\frac{R\left(\log\left(T\right)+d\right)}{\sqrt{ \lambda}}\right)+2S\sqrt{\lambda}\] (11)

Proof.: We first define the following quantities

\[\alpha(x,\theta^{*},\widehat{\theta}) \coloneqq\int_{v=1}^{1}\dot{\mu}\left(\left\langle x,\theta^{*} \right\rangle+v\langle x,\left(\widehat{\theta}-\theta^{*}\right)\rangle \right)dv\] \[\mathbf{G} \coloneqq\sum_{j=1}^{s}\alpha(x,\theta^{*},\widehat{\theta})x_{j }x_{j}^{\mathsf{T}}+\frac{\lambda}{1+2RS}\mathbf{I}\]

Using Lemma C.2 we have

\[\mathbf{G}\succeq\frac{1}{1+2RS}\;\mathbf{H}^{*}\] (12)

Hence we write

\[\left\|\theta^{*}-\widehat{\theta}\right\|_{\mathbf{H}^{*}} \leq\sqrt{\left(1+2RS\right)}\left\|\theta^{*}-\widehat{\theta} \right\|_{\mathbf{G}}\] \[=\sqrt{1+2RS}\left\|\mathbf{G}\left(\theta^{*}-\widehat{\theta} \right)\right\|_{\mathbf{G}^{-1}}\] \[=\sqrt{1+2RS}\left\|\sum_{j=1}^{s}\left(\left\langle\theta^{*},x _{j}\right\rangle-\left\langle\widehat{\theta},x_{j}\right\rangle\right) \alpha(x,\theta^{*},\widehat{\theta})x_{j}+\frac{\lambda}{1+2RS}\left(\theta^{ *}-\widehat{\theta}\right)\right\|_{\mathbf{G}^{-1}}\] \[\leq\sqrt{1+2RS}\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle \theta^{*},x_{j}\right\rangle\right)-\mu\left(\left\langle\widehat{\theta},x_{ j}\right\rangle\right)\right)x_{j}\right\|_{\mathbf{G}^{-1}}+\frac{\lambda}{ \sqrt{1+2RS}}\left\|\theta^{*}-\widehat{\theta}\right\|_{\mathbf{G}^{-1}}\] \[\leq\sqrt{1+2RS}\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle \theta^{*},x_{j}\right\rangle\right)-\mu\left(\left\langle\widehat{\theta},x_{ j}\right\rangle\right)\right)x_{j}\right\|_{\mathbf{G}^{-1}}+2S\sqrt{\lambda}\] (since \[\mathbf{G}\succeq\frac{\lambda}{1+2RS}\mathbf{I}\] and \[\left\|\theta^{*}-\widehat{\theta}\right\|_{2}\leq 2S\] ) \[\leq 3RS\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right)-\mu\left(\left\langle\widehat{\theta},x_{j}\right\rangle \right)\right)x_{j}\right\|_{\mathbf{H}^{*-1}}+2S\sqrt{\lambda}\] (Using (12) and assuming \[RS\geq 1\] )

Now by the optimality condition on \(\widehat{\theta}\) we have \(\sum_{j=1}^{s}\mu\left(\left\langle x_{j},\widehat{\theta}\right\rangle \right)x_{j}=\sum_{j=1}^{s}r_{j}x_{j}\) (see equation (3) [8]). Hence, we write

\[\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j} \right\rangle\right)-\mu\left(\left\langle\widehat{\theta},x_{j}\right\rangle \right)\right)x_{j}\right\|_{\mathbf{H}^{*-1}}=\left\|\sum_{j=1}^{s}\left(\mu \left(\left\langle\theta^{*},x_{j}\right\rangle\right)-r_{j}\right)x_{j} \right\|_{\mathbf{H}^{*-1}}\] (13)

Let \(\mathcal{B}\) denote the unit ball in \(\mathbb{R}^{d}\). We can write

\[\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j} \right\rangle\right)-r_{j}\right)x_{j}\right\|_{\mathbf{H}^{*-1}}=\max_{y\in \mathcal{B}}\langle y,\mathbf{H}^{*-1/2}\sum_{j=1}^{s}\left(\mu\left(\left\langle \theta^{*},x_{j}\right\rangle\right)-r_{j}\right)x_{j}\rangle\]We construct an \(\varepsilon\)-net for the unit ball, denoted as \(\mathcal{C}_{\varepsilon}\). For any \(y\in\mathcal{B}\), we define \(y_{\varepsilon}\coloneqq\arg\min_{b\in\mathcal{C}_{\varepsilon}}\left\|b-y \right\|_{2}\). We can now write

\[\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j} \right\rangle\right)-r_{j}\right)x_{j}\right\|_{\mathbf{H}^{*-1}}\] \[=\max_{y\in\mathcal{B}}\langle y-y_{\varepsilon},\mathbf{H}^{*-1 /2}\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j}\right\rangle \right)-r_{j}\right)x_{j}\rangle+\left\langle y_{\varepsilon},\mathbf{H}^{*-1/ 2}\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right) -r_{j}\right)x_{j}\rangle\] \[\leq\max_{y\in\mathcal{B}}\varepsilon\left\|\sum_{j=1}^{s}\left( \mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right)-r_{j}\right)x_{j} \right\|_{\mathbf{H}^{*-1}}+\left\langle y_{\varepsilon},\mathbf{H}^{*-1/2} \sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right) -r_{j}\right)x_{j}\rangle\]

Rearranging, we obtain

\[\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j} \right\rangle\right)-r_{j}\right)x_{j}\right\|_{\mathbf{H}^{*-1}}\leq\frac{1}{ 1-\varepsilon}\langle y_{\varepsilon},\mathbf{H}^{*-1/2}\sum_{j=1}^{s}\left( \mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right)-r_{j}\right)x_{j}\rangle\]

Next, we use Lemma A.3 (stated below) with \(\delta=T^{2}|\mathcal{C}_{\varepsilon}|\) and union bound over all vectors in \(\mathcal{C}_{\varepsilon}\). We also observe that \(|\mathcal{C}_{\varepsilon}|\leq\left(\frac{2}{\varepsilon}\right)^{d}\). Substituting \(\epsilon=1/2\) and using Lemma A.3, we obtain that the following holds with probability greater than \(1-\frac{1}{T^{2}}\),

\[\left\|\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j }\right\rangle\right)-r_{j}\right)x_{j}\right\|_{\mathbf{H}^{*-1}} \leq 3\sqrt{\log\left(T^{2}|\mathcal{C}_{\varepsilon}|\right)}+ \frac{4R}{3\sqrt{\lambda}}\log\left(T^{2}|\mathcal{C}_{\varepsilon}|\right)\] \[\leq 8\left(\sqrt{\log\left(T\right)+d}+\frac{R\left(\log\left(T \right)+d\right)}{\sqrt{\lambda}}\right)\]

Substituting in equations (13), we get the desired inequality in the lemma statement. 

**Lemma A.3**.: _Let \(y\) be a fixed vector with \(\left\|y\right\|\leq 1\). Then, with the notation stated in Lemma A.2, the following inequality holds with probability at least \(1-\delta\)_

\[\sum_{j=1}^{s}\left(\mu\left(\left\langle\theta^{*},x_{j}\right\rangle\right) -r_{j}\right)y^{\mathsf{T}}\mathbf{H}^{*-1/2}x_{j}\leq\sqrt{2\log\frac{1}{ \delta}}+\frac{2R}{3\sqrt{\lambda}}\log\frac{1}{\delta}.\]

Proof.: Let us denote the \(j^{th}\) term of the sum as \(Z_{j}\). Note that each random variable \(Z_{j}\) has variance \(\text{Var}(Z_{j})=\dot{\mu}\left(\left\langle x_{j},\theta^{*}\right\rangle \right)\left(y^{\mathsf{T}}\mathbf{H}^{*-1/2}x_{j}\right)^{2}\). Hence, we have

\[\sum_{j=1}^{s}\text{Var}(Z_{j}) =\sum_{j=1}^{s}\dot{\mu}\left(\left\langle\theta^{*},x_{j}\right \rangle\right)\left(y^{\mathsf{T}}\mathbf{H}^{*-1/2}x_{j}\right)^{2}\] \[=y^{\mathsf{T}}y\leq 1.\]

Moreover, each \(Z_{j}\) is at most \(\frac{R}{\sqrt{\lambda}}\) (since \(\left\|x_{j}\right\|\leq 1\), \(\mathbf{H}^{*}\succeq\lambda\mathbf{I}\) and \(r\in[0,R]\)). Now applying Lemma A.1, we have

\[\mathbb{P}\left\{\sum_{j=1}^{s}Z_{j}\geq\sqrt{2\log\frac{1}{\delta}}+\frac{2R} {3\sqrt{\lambda}}\log\frac{1}{\delta}\right\}\leq\delta.\]

**Corollary A.4**.: _Let \(x_{1},x_{2},\ldots,x_{\tau}\) be the sequence of arms pulled during the warm-up batch and let \(\widehat{\theta}_{w}\) be the estimator of \(\theta^{*}\) computed at the end of the batch. Then, for any vector \(x\) and \(\lambda\geq 0\) the following bound holds with probability greater than \(1-\frac{1}{T^{2}}\)_

\[|\langle x,\theta^{*}-\widehat{\theta}_{w}\rangle|\leq\sqrt{\kappa}\left\|x \right\|_{\mathbf{V}^{-1}}\gamma(\lambda).\]

Proof.: This result is derived directly from Lemma A.2 and the definition of \(\gamma(\lambda)\) (see 9). By applying the lemma, we obtain

\[|\langle x,\theta^{*}-\widehat{\theta}_{w}\rangle|\leq\left\|x\right\|_{ \mathbf{H}^{*-1}}\left\|\theta^{*}-\widehat{\theta}_{w}\right\|_{\mathbf{H}^{ *}}\leq\left\|x\right\|_{\mathbf{H}^{*-1}}\gamma(\lambda)\]

Considering the definition of \(\kappa\), we have \(\dot{\mu}\left(\left(\mathbf{x},\theta^{*}\right)\right)\geq\frac{1}{\kappa}\). This implies that \(\mathbf{H}^{*}\succeq\frac{1}{\kappa}\mathbf{V}\)13 which in turn leads to the inequality \(\left\|x\right\|_{\mathbf{H}^{*-1}}\leq\sqrt{\kappa}\left\|x\right\|_{ \mathbf{V}^{-1}}\). 

Footnote 13: For logistic rewards, we note that instead of using the worst case bound of \(\kappa\), one can make use of an upper bound on \(S\coloneqq\left\|\theta\right\|\) as done in [20] That is, we lower bound \(\dot{\mu}\left(\left\langle x,\theta^{*}\right\rangle\right)\geq\dot{\mu} \left(S\left\|x\right\|\right)\) and use \(\mathbf{H}^{*}\succeq\sum_{t}\dot{\mu}\left(S\left\|x_{t}\right\|\right)x_{t} x_{t}^{t}\).

### Preliminary Lemmas

**Lemma A.5**.: _For each batch \(k\geq 2\) and the scaled data matrix \(\mathbf{H}_{k}\) computed at the end of batch, the following bound holds with probability at least \(1-\frac{1}{T^{2}}\):_

\[\mathbf{H}_{k}\preceq\mathbf{H}_{k}^{*}.\]

Proof.: If the event stated in Lemma A.4 holds,

From Lemma C.2, we apply the multiplicative bound on \(\dot{\mu}\) to obtain

\[\dot{\mu}\left(\left\langle x,\widehat{\theta}_{w}\right\rangle\right)\leq\dot {\mu}\left(\left\langle x,\theta^{*}\right\rangle\right)\exp\left(R|\langle x,\widehat{\theta}_{w}-\theta^{*}\rangle|\right)\]

Via Corollary A.4 we have \(|\langle x,\widehat{\theta}_{w}\rangle-\langle x,\theta^{*}\rangle|\leq \sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\gamma(\lambda)\). Additionally, given that \(\left\|\widehat{\theta}\right\|,\)\(\|\theta^{*}\|\leq S\) and \(\|x\|\leq 1\) we also have \(|\langle x,\widehat{\theta}_{w}\rangle-\langle x,\theta^{*}\rangle|\leq 2S\). Hence, we write

\[\dot{\mu}\left(\left\langle x,\widehat{\theta}_{w}\right\rangle\right) \leq\dot{\mu}\left(\left\langle x,\theta^{*}\right\rangle\right) \exp\left(R\min\{\sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\gamma(\lambda ),2S\}\right)\] \[\leq\dot{\mu}\left(\left\langle x,\theta^{*}\right\rangle\right) \beta(x)\]

Substituting these results into the definitions of \(\mathbf{H}_{k}\) and \(\mathbf{H}_{k}^{*}\) proves the lemma statement. 

**Claim A.6**.: _The Algorithm 1 runs for at most \(\log\log T\) batches._

Proof.: When \(M\leq\log\log T\) then the claim trivially holds. When \(M\geq\log\log T+1\), we define the length of the second batch, \(\tau_{2}\), as \(2\sqrt{T}\). The length of the \(M^{th}\) batch is

\[\tau_{M} =(2\sqrt{T})^{\sum_{k=1}^{M-1}\frac{1}{2^{k-1}}}\] \[\geq 2T\;T^{\frac{-1}{2M-1}}\] \[\geq 2T\;T^{\frac{-1}{2\log\log T}}\] ( \[M\geq\log\log T+1\] ) \[\geq T.\]

**Corollary A.7**.: _Let \(\widehat{\theta}_{k}\) be the estimator of \(\theta^{*}\) calculated at the end of the \(k^{th}\) batch. Then for any vector \(x\) the following holds with probability greater than \(1-\frac{\log\log T}{T^{2}}\) for every batch \(k\geq 2\)._

\[|\langle x,\theta^{*}-\widehat{\theta}_{k}\rangle|\leq\|x\|_{\mathbf{H}_{k}^{-1 }}\gamma(\lambda)\]Proof.: This result is a direct consequence of Lemma A.2 and the definition of \(\gamma(\lambda)\) (see 9). According to the lemma, we have

\[|\langle x,\theta^{*}-\widehat{\theta}_{k}\rangle| \leq\left\|x\right\|_{\mathbf{H}_{k}^{-1}}\left\|\theta^{*}- \widehat{\theta}_{k}\right\|_{\mathbf{H}_{k}^{*}}\] \[\leq\left\|x\right\|_{\mathbf{H}_{k}^{-1}}\gamma(\lambda)\]

Using Lemma A.5, we can further bound \(\left\|x\right\|_{\mathbf{H}_{k}^{-1}}\leq\left\|x\right\|_{\mathbf{H}_{k}^{- 1}}\). Finally, a union bound over all batches and considering the fact that there are at most \(\log\log T\) batches (Claim A.6) we establish the corollary's claim. 

**Claim A.8**.: _For any \(x\in[0,M]\) the following holds_

\[e^{x}\leq\left(e^{M}-1\right)\frac{x}{M}+1.\]

Proof.: The claim follows from the convexity of \(e^{x}\). 

**Lemma A.9**.: _Let \(x\in\mathcal{X}\) be the selected in any round of batch \(k\geq 2\) in the algorithm, and let \(x^{*}\) be the optimal arm in the arm set \(\mathcal{X}\), i.e., \(x^{*}=\operatorname*{arg\,max}_{x\in\mathcal{X}}\mu\left(\langle x,\bar{ \theta}^{*}\rangle\right)\). With probability greater than \(1-\frac{\log\log T}{T^{2}}\), the following inequality holds-_

\[\mu\left(\langle x^{*},\theta^{*}\rangle\right)-\mu\left(\langle x,\theta^{*} \rangle\right)\leq 6\phi(\lambda)\sum_{\begin{subarray}{c}y\in\{x,x^{*}\}\\ \tilde{y}\in\{z,\tilde{x}^{*}\}\end{subarray}}\left\|y\right\|_{\mathbf{V}^{- 1}}\left\|\tilde{y}\right\|_{\mathbf{H}_{k-1}}+2\gamma(\lambda)\sqrt{\bar{\mu} \left(\langle x^{*},\theta^{*}\rangle\right)}\left(\left\|\tilde{x}^{*}\right\| _{\mathbf{H}_{k-1}^{-1}}+\left\|\tilde{x}\right\|_{\mathbf{H}_{k-1}^{-1}}\right)\]

Proof.: We begin by applying Taylor's theorem, which yields the following for some \(z\) between \(\langle x,\theta^{*}\rangle\) and \(\langle x^{*},\theta^{*}\rangle\)

\[|\mu\left(\langle x^{*},\theta^{*}\rangle\right)-\mu\left(\langle x,\theta^{*}\rangle\right)|\] (14) \[=\dot{\mu}(z)\left|\langle x,\theta^{*}\rangle-\langle x^{*}, \widehat{\theta}_{k-1}\rangle+\langle x^{*},\widehat{\theta}_{k-1}\rangle- \langle x,\widehat{\theta}_{k-1}\rangle+\langle x,\widehat{\theta}_{k-1} \rangle-\langle x,\theta^{*}\rangle\right|\] \[\leq 2\dot{\mu}(z)\left(\left\|x^{*}\right\|_{\mathbf{H}_{k-1}^{-1}} \gamma(\lambda)+\left\|x\right\|_{\mathbf{H}_{k-1}^{-1}}\gamma(\lambda)\right) \text{(via Corollary A.7)}\] \[\leq 2\dot{\mu}(z)\gamma(\lambda)\left(\sqrt{\frac{\beta(x^{*})}{ \dot{\mu}\left(\langle x^{*},\widehat{\theta}_{w}\rangle\right)}}\left\| \tilde{x}^{*}\right\|_{\mathbf{H}_{k-1}^{-1}}+\sqrt{\frac{\beta(x)}{\dot{\mu }\left(\langle x,\widehat{\theta}_{w}\rangle\right)}}\left\|\tilde{x}\right\| _{\mathbf{H}_{k-1}^{-1}}\right)\] \[\leq 2\gamma(\lambda)\sqrt{\dot{\mu}(z)}\sqrt{\frac{\dot{\mu}(z) \beta(x^{*})}{\dot{\mu}\left(\langle x^{*},\widehat{\theta}_{w}\rangle\right) }}\left\|\tilde{x}^{*}\right\|_{\mathbf{H}_{k-1}^{-1}}+2\sqrt{\dot{\mu}(z)} \gamma(\lambda)\sqrt{\frac{\dot{\mu}(z)\beta(x)}{\dot{\mu}\left(\langle x, \widehat{\theta}_{w}\rangle\right)}}\left\|\tilde{x}\right\|_{\mathbf{H}_{k- 1}^{-1}}\] (15)We now invoke Lemmas C.2 and A.4 to obtain

\[\sqrt{\frac{\hat{\mu}(z)}{\hat{\mu}\left(\left\langle x,\widehat{ \theta}_{w}\right\rangle\right)}\beta(x)} \leq\sqrt{\exp\left(\min\left\{2S,\left|z-\left\langle x,\widehat{ \theta}_{w}\right\rangle\right|\right\}\right)\beta(x)}\] (by stated assumptions and Lemma C.2) \[\leq\exp\left(\min\left\{S,\frac{\left|\left\langle x,\theta^{*} \right\rangle-\left\langle x,\widehat{\theta}_{w}\right\rangle\right|+\left| \left\langle x,\theta^{*}\right\rangle-\left\langle x^{*},\theta^{*}\right\rangle \right|}{2}\right\}\right)\sqrt{\beta(x)}\] \[\leq\exp\left(\min\left\{S,\frac{\left|\left\langle x,\theta^{*} \right\rangle-\left\langle x,\widehat{\theta}_{w}\right\rangle\right|+\left| \left\langle x,\theta^{*}\right\rangle-\left\langle x^{*},\theta^{*}\right\rangle \right|}{2}\right\}\right)\sqrt{\beta(x)}\] (since

\[z\in[\left\langle x,\theta^{*}\right\rangle,\left\langle x_{t}^{*},\theta^{* }\right\rangle]\]

) \[\leq\exp\left(\min\left\{S,\frac{3\sqrt{\kappa}\left\|x\right\|_{ \mathbf{V}^{-1}}\gamma\left(\lambda\right)+2\sqrt{\kappa}\left\|x^{*}\right\|_ {\mathbf{V}^{-1}}\gamma\left(\lambda\right)}{2}\right\}\right)\sqrt{\beta(x)}\] (using Lemma A.4 and the elimination criteria) \[\leq\exp\left(\min\left\{2S,2\sqrt{\kappa}\left\|x\right\|_{ \mathbf{V}^{-1}}\gamma\left(\lambda\right)+\sqrt{\kappa}\left\|x^{*}\right\|_ {\mathbf{V}^{-1}}\gamma\left(\lambda\right)\right\}\right)\] (substituting the definition of

Similarly, we also have

Further, we can simplify each term in equation (15) as

\[\sqrt{\hat{\mu}(z)}\left(\sqrt{\frac{\hat{\mu}(z)\beta(x^{*})}{ \hat{\mu}\left(\left\langle x^{*},\widehat{\theta}_{w}\right\rangle\right)}} \left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{k-1}^{-1}}\right)\] \[\leq 2\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*} \right\rangle\right)}\left(\left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{k-1}^ {-1}}\exp\left(\min\left\{3S,3\sqrt{\kappa}\gamma\left(\lambda\right)\left( \left\|x\right\|_{\mathbf{V}^{-1}}+\left\|x^{*}\right\|_{\mathbf{V}^{-1}} \right)\right\}\right)\right)\] \[\leq 6\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*} \right\rangle\right)\kappa\,e^{3S}\,\gamma(\lambda)}\left(\left\|x^{*}\right\| _{\mathbf{V}^{-1}}+\left\|x\right\|_{\mathbf{V}^{-1}}\right)\left\|\widehat{x }^{*}\right\|_{\mathbf{H}_{k-1}^{-1}}+2\gamma(\lambda)\sqrt{\hat{\mu}\left( \left\langle x^{*},\theta^{*}\right\rangle\right)}\left\|\widehat{x}^{*} \right\|_{\mathbf{H}_{k-1}^{-1}}\] (via Claim A.8) \[\leq 6\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*} \right\rangle\right)\kappa\,e^{3S}\,\gamma(\lambda)}^{2}\left(\left\|x^{*} \right\|_{\mathbf{V}^{-1}}+\left\|x\right\|_{\mathbf{V}^{-1}}\right)\left\| \widehat{x}^{*}\right\|_{\mathbf{H}_{k-1}^{-1}}\] \[\qquad+2\gamma(\lambda)\sqrt{\hat{\mu}\left(\left\langle x^{*}, \theta^{*}\right\rangle\right)}\left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{k-1} ^{-1}}\] \[\leq 6\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*}\right\rangle \right)}\phi(\lambda)\left(\left\|x^{*}\right\|_{\mathbf{V}^{-1}}+\left\|x \right\|_{\mathbf{V}^{-1}}\right)\left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{ k-1}^{-1}}+2\gamma(\lambda)\sqrt{\hat{\mu}\left(\left\langle x^{*}, \theta^{*}\right\rangle\right)}\left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{k-1 }^{-1}}\]

Finally, we substitute the above bound in (15) to obtain

\[\left|\mu\left(\left\langle x^{*},\theta^{*}\right\rangle\right)- \mu\left(\left\langle x,\theta^{*}\right\rangle\right)\right|\leq 6\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*} \right\rangle\right)}\phi(\lambda)\left(\left\|x^{*}\right\|_{\mathbf{V}^{-1}} +\left\|x\right\|_{\mathbf{V}^{-1}}\right)\left(\left\|\widehat{x}^{*}\right\|_ {\mathbf{H}_{k-1}^{-1}}+\left\|\widehat{x}\right\|_{\mathbf{H}_{k-1}^{-1}}\right)\] \[\qquad+2\sqrt{\hat{\mu}\left(\left\langle x^{*},\theta^{*} \right\rangle\right)}\left(\left\|\widehat{x}^{*}\right\|_{\mathbf{H}_{k-1}^{-1 }}+\left\|\widehat{x}\right\|_{\mathbf{H}_{k-1}^{-1}}\right)\]

For Phase \(k\), the distribution of the remaining arms after the elimination step (\(\mathcal{X}\) in line 10 of the Algorithm 1) is represented as \(\mathcal{D}_{k}\).

**Lemma A.10**.: _During any round in batch \(k\) of Algorithm 1, and for an absolute constant \(c\), we have_

\[\mathbb{E}\left[\left|\mu\left(\left\langle x^{*},\theta^{*}\right\rangle\right)- \mu\left(\left\langle x,\theta^{*}\right\rangle\right)\right\|\leq c\left(\frac{ \phi(\lambda)d^{2}}{\sqrt{\tau_{1}\,\,\tau_{k-1}}}+\frac{\gamma(\lambda)}{\sqrt {\tau_{k-1}}}\left(\frac{d}{\sqrt{\hat{\kappa}}}\wedge\sqrt{\frac{d\log d}{ \kappa^{*}}}\right)\right)\]Proof.: The proof here invokes Lemma A.9. We begin by noting that

\[\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\sum_{ \begin{subarray}{c}\underline{y}\in\{x,x^{*}\}\\ \overline{y}\in\{\widetilde{x},\widetilde{x}^{*}\}\end{subarray}}\|y\|_{ \mathbf{V}^{-1}}\|\widetilde{y}\|_{\mathbf{H}^{-1}_{k-1}} \leq 4\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \left[\max_{x\in\mathcal{X}}\|x\|_{\mathbf{V}^{-1}}\max_{\mathbf{x}\in \mathcal{X}}\|\widetilde{x}\|_{\mathbf{H}^{-1}_{k-1}}\right]\] \[\leq 4\sqrt{\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D} _{k}}\left[\max_{x\in\mathcal{X}}\|x\|_{\mathbf{V}^{-1}}^{2}\right]\operatorname* {\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\left[\max_{\mathbf{x}\in \mathcal{X}}\|\widetilde{x}\|_{\mathbf{H}^{-1}_{k-1}}^{2}\right]}\] (via Jensen's inequality) \[\leq 4\sqrt{\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D} }\left[\max_{x\in\mathcal{X}}\|x\|_{\mathbf{V}^{-1}}^{2}\right]\operatorname* {\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k-1}}\left[\max_{\mathbf{x}\in \mathcal{X}}\|\widetilde{x}\|_{\mathbf{H}^{-1}_{k-1}}^{2}\right]}\] (via Claim A.11) \[\leq c\left(\sqrt{\frac{d^{2}}{\tau_{1}}\cdot\frac{d^{2}}{\tau_{k -1}}}\right)\] (using Lemma A.17)

We also have

\[\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\left[ \sqrt{\mu\left(\left\langle x^{*},\theta^{*}\right\rangle\right)}\left(\| \widetilde{x}^{*}\|_{\mathbf{H}^{-1}_{k-1}}+\|\widetilde{x}\|_{\mathbf{H}^{-1} _{k-1}}\right)\right]\] \[\leq 2\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \left[\sqrt{\mu\left(\left\langle x^{*},\theta^{*}\right\rangle\right)}\max_{x \in\mathcal{X}}\|x\|_{\mathbf{H}^{-1}_{k-1}}\right]\] \[\leq 2\min\left\{\sqrt{\kappa^{*}}\operatorname*{\mathbb{E}}_{ \mathcal{X}\sim\mathcal{D}_{k}}\left[\max_{x\in\mathcal{X}}\|x\|_{\mathbf{H}^{ -1}_{k-1}}\right],\sqrt{\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D} _{k}}\left[\mu\left(\left\langle x^{*},\theta^{*}\right\rangle\right)\right] \operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\left[\|x\|_{ \mathbf{H}^{-1}_{k-1}}^{2}\right]\right\}\] (using the definition of \[\kappa^{*}\] for the first bound and Jensen for the second) \[\leq c\left(\sqrt{\frac{d\log d}{\kappa^{*}\tau_{k-1}}}\wedge\sqrt{ \frac{d^{2}}{\widehat{\kappa}\tau_{k-1}}}\right)\] (using Lemma A.17)

Substituting the above bounds in Lemma A.9 we obtained the stated inequality. This completes the proof. 

### Proof of Theorem 3.2

We trivially upper bound the regret incurred during the warm-up batch as \(\tau_{1}R\); recall that \(R\) denotes the upper bound on the rewards and \(\tau_{1}\) denotes the length of the first (warm-up) batch; see equation (5)).

For each batch \(k\) and an absolute constant \(c\), Lemma A.10 gives us

\[\operatorname*{\mathbb{E}}\left[\sum_{t\in\mathcal{T}_{k}}\mu \left(\left\langle x_{t}^{*},\theta^{*}\right\rangle\right)-\mu\left(\left \langle x_{t},\theta^{*}\right\rangle\right)\right] \leq\tau_{k}\cdot c\left(\frac{\phi(\lambda)d^{2}}{\sqrt{\tau_ {1}}\ \tau_{k-1}}+\frac{\gamma(\lambda)}{\sqrt{\tau_{k-1}}}\left(\frac{d}{ \sqrt{\widehat{\kappa}}}\wedge\sqrt{\frac{d\log d}{\kappa^{*}}}\right)\right)\] \[\leq c\left(\frac{\phi(\lambda)d^{2}}{\sqrt{\tau_{1}}}\alpha+ \left(\frac{d}{\sqrt{\widehat{\kappa}}}\wedge\sqrt{\frac{d\log d}{\kappa^{*}}} \right)\gamma(\lambda)\alpha\right)\] (via (3))

Since there are at most \(\log\log T\) batches, we can upper bound the regret as

\[R_{T}\leq c\left(\tau_{1}R+\frac{\phi(\lambda)d^{2}}{\sqrt{\tau_{1}}}\alpha+ \left(\frac{d}{\sqrt{\widehat{\kappa}}}\wedge\sqrt{\frac{d\log d}{\kappa^{*}}} \right)\gamma(\lambda)\alpha\right)\log\log(T)\]

Setting \(\tau_{1}=\left(\frac{\phi(\lambda)d^{2}\alpha}{R}\right)^{2/3}\) we get

\[R_{T}\leq O\left(\left(\frac{\phi(\lambda)d^{2}\alpha}{R}\right)^{2/3}+ \left(\frac{d}{\sqrt{\widehat{\kappa}}}\wedge\sqrt{\frac{d\log d}{\kappa^{*}}} \right)\gamma(\lambda)\alpha\right)\log\log(T)\]

Now with the choice of \(\lambda=20dR\log T\), we have

\[\gamma(\lambda)\leq\gamma\quad\text{ where }\gamma=30RS\sqrt{d\log T}.\]Substituting \(\alpha=T^{\frac{1}{2(1-2-3M)}}\) and \(\phi(\lambda)=\frac{\sqrt{\kappa}\ e^{3S}\ \gamma^{2}}{S}\) we get

\[R_{T}\leq O\left(\left(\sqrt{\kappa}d^{3}\ e^{3S}\ RST^{\frac{1}{2(1-2-3M)}} \log T\right)^{2/3}+\left(\frac{d}{\sqrt{\hat{\kappa}}}\wedge\sqrt{\frac{d\log d }{\kappa^{*}}}\right)RST^{\frac{1}{2(1-2^{-3M})}}\sqrt{d\log T}\right).\]

### Optimal Design Guarantees

In this section, we study the optimal design policies utilized in different batches of the algorithm. Specifically, \(\pi_{G}\) denotes the G-optimal design policy applied during the warm-up batch, while \(\pi_{k}\) refers to the Distributional Optimal Design policy calculated at the end of batch \(k\) ( and used in the \((k+1)^{th}\) batch). Recall that the distribution of the remaining arms after the elimination step ( \(\mathcal{X}\) in line 10 of the Algorithm) is represented as \(\mathcal{D}_{k}\). We define expected design matrices for each policy:

\[\mathbf{W}_{G} \coloneqq\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \left[\mathop{\mathbb{E}}_{x\sim\pi_{G}(\mathcal{X})}\left[xx^{\mathsf{T}}| \mathcal{X}\right]\right]\] \[\mathbf{W}_{k} \coloneqq\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \left[\mathop{\mathbb{E}}_{x\sim\pi_{k}(\mathcal{X})}\left[\widetilde{xx}^{ \mathsf{T}}|\mathcal{X}\right]\right]\]

Recall, for all batches starting from the second batch (\(k\geq 2\)), we employ the scaled arm set, denoted as \(\widetilde{\mathcal{X}}\), for learning and action selection under the Distributional optimal design policy. However, during the initial warm-up batch, we utilize the original, unscaled arm set.

**Claim A.11**.: _The following holds for any positive semidefinite matrix \(\mathbf{A}\) and any batch \(k\)-_

\[\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\max_{x\in\mathcal{X}} \left\|x\right\|_{A}\leq\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \max_{x\in\mathcal{X}}\left\|x\right\|_{A}\quad\forall j\in[k-1].\]

This is due to the fact that the set of surviving arms in batch \(k\) is always a smaller set than the previous batches.

**Lemma A.12** (Lemma 4 [23]).: _The expected data matrix \(\mathbf{W}_{G}\) satisfies. We have_

\[\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\left[\max_{x\in\mathcal{X}} \ \left\|x\right\|_{\mathbf{W}^{-1}_{G}}^{2}\right]\leq d^{2}\]

**Lemma A.13** (Theorem 5 [23]).: _Let the Distributional optimal design \(\pi\) which has been learnt from \(s\) independent samples \(\mathcal{X}_{1},\ldots\mathcal{X}_{s}\sim\mathcal{D}\) and let \(\mathbf{W}\) denote the expected data matrix, \(\mathbf{W}=\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\left[\mathop{ \mathbb{E}}_{x\sim\pi(\mathcal{X})}\left[xx^{\mathsf{T}}|\mathcal{X}\right]\right]\). We have_

\[\mathbb{P}\left\{\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\left[\max_ {x\in\mathcal{X}}\ \left\|x\right\|_{\mathbf{W}^{-1}}\right]\leq O\left(\sqrt{d\log d}\right) \right\}\geq 1-\exp\left(O\left(d^{4}\log^{2}d\right)-sd^{-12}\cdot 2^{-16} \right).\] (16)

**Lemma A.14**.: _Under the notation of Lemma A.13, we have_

\[\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\left[\max_{x\in\mathcal{X}} \ \left\|x\right\|_{\mathbf{W}^{-1}}^{2}\right]\leq 2d^{2}.\] (17)

Proof.: Recall that the Distributional optimal design policy samples according to the \(\pi_{G}\) policy with half probability. Hence, we have

\[\mathbf{W}=\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}} \left[\mathop{\mathbb{E}}_{x\sim\pi(\mathcal{X})}\left[xx^{\mathsf{T}}| \mathcal{X}\right]\right]\succeq\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{ D}_{k}}\left[\frac{1}{2}\mathop{\mathbb{E}}_{x\sim\pi_{G}(\mathcal{X})}\left[ xx^{\mathsf{T}}|\mathcal{X}\right]\right]=\frac{1}{2}\mathbf{W}_{G}\]

Therefore,

\[\mathop{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\left[\max_{x\in\mathcal{X}} \ \left\|x\right\|_{\mathbf{W}^{-1}}^{2}\right]\leq 2\mathop{\mathbb{E}}_{ \mathcal{X}\sim\mathcal{D}}\left[\max_{x\in\mathcal{X}}\ \left\|x\right\|_{\mathbf{W}^{-1}_{G}}^{2}\right]\leq 2d^ {2}.\]

**Lemma A.15** (Matrix Chernoff [27, 23]).: _Let \(x_{1},x_{3},\ldots,x_{n}\sim\mathcal{D}\) be vectors, with \(\left\|x_{t}\right\|\leq 1\), then we have_

\[\mathbb{P}\left\{3\varepsilon n\mathbf{I}+\sum_{i=1}^{n}x_{i}x_{i}^{\mathsf{T} }\succeq\frac{n}{8}\mathop{\mathbb{E}}_{x\sim\mathcal{D}}\left[xx^{\mathsf{T }}\right]\right\}\geq 1-2d\exp\left(-\frac{\varepsilon n}{8}\right)\]

**Corollary A.16**.: _In Algorithm 1 the warm-up matrix \(\mathbf{V}\), with \(\lambda\geq 16\log\left(Td\right)\), satisfies the following with probability greater than \(1-\frac{1}{T^{2}}\)._

\[\mathbf{V}\succeq\frac{\tau_{1}}{8}\operatorname*{\mathbb{E}}_{ \mathcal{X}\sim\mathcal{D}}\operatorname*{\mathbb{E}}_{x\sim\pi_{G}(\mathcal{ X})}xx^{\mathsf{T}}\]

_Similarly \(\mathbf{H}_{k}\), with \(\lambda\geq 6\log\left(Td\right)\) satisfies the following for each batch \(k\geq 2\) with probability greater than \(1-\frac{1}{T^{2}}\)_

\[\mathbf{H}_{k}\succeq\frac{\tau_{k}}{8}\operatorname*{\mathbb{E}}_{ \mathcal{X}\sim\mathcal{D}_{k}}\operatorname*{\mathbb{E}}_{\widetilde{x}\sim \pi_{k}(\mathcal{X})}\widetilde{x}\widetilde{x}^{\mathsf{T}}.\]

Proof.: The results for both \(\mathbf{V}\) and \(\mathbf{H}_{k}\) are obtained directly by applying Lemma A.15 with \(\varepsilon=\frac{\log\left(T\right)}{\tau_{1}}\) and \(\varepsilon=\frac{\log\left(T\right)}{\tau_{k}}\), respectively. 

We note that the analysis of [23] gives an optimal guarantee (in expectation) on \(\left\|x\right\|_{\mathbf{V}^{-1}}\), but not on \(\left\|x\right\|_{\mathbf{V}^{-1}}^{2}\). We obtain such a bound here and use it in the analysis.

**Lemma A.17**.: _The following holds with probability greater than \(1-\frac{1}{T^{2}}\)_

\[\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}}\max_{x \in\mathcal{X}}\left\|x\right\|_{\mathbf{V}^{-1}}^{2} \leq O\left(\frac{d^{2}}{\tau_{1}}\right)\] (18) \[\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\max_ {x\in\mathcal{X}}\left\|\widetilde{x}\right\|_{\mathbf{H}_{k}^{-1}}^{2} \leq O\left(\frac{d^{2}}{\tau_{k}}\right)\quad\forall k\in[M]\] (19)

_We also have that for sufficiently large \(T\gtrsim O\left(d^{32}(\log 2T)^{2}\right)\), the following holds with probability greater than \(1-\frac{\log\log T}{T}\)_

\[\operatorname*{\mathbb{E}}_{\mathcal{X}\sim\mathcal{D}_{k}}\max_{x\in \mathcal{X}}\left\|\widetilde{x}\right\|_{\mathbf{H}_{k}^{-1}}\leq O\left( \sqrt{\frac{d}{\tau_{k}}}\right)\quad\forall k\in[M]\] (20)

Proof.: First, we note from Corollary A.16 that the following holds with high probability

\[\left\|x\right\|_{\mathbf{V}^{-1}} \leq\frac{8}{\tau_{1}}\left\|x\right\|_{\mathbf{W}_{G}^{-1}}\] \[\left\|\widetilde{x}\right\|_{\mathbf{H}_{k}^{-1}} \geq\frac{8}{\tau_{1}}\left\|\widetilde{x}\right\|_{\mathbf{W}_{k} ^{-1}}\]

We obtain the first two inequalities, (18) and (19), by a direct use of Corollary A.16. For (20) we note that for every phase we have at least \(O(\sqrt{T})\) samples for learning the Distributional optimal design policy (for any M). Since, \(T\geq d^{32}\log 2T^{2}\) the event stated in Lemma A.13 holds with probability greater than \(1-\frac{1}{T^{2}}\). 

## Appendix B Regret Analysis of \(\mathsf{RS-GLinCB}\)

Recall \(\mathcal{T}_{o}\) denotes the set of rounds when switching criterion I is satisfied. We write \(\tau_{o}\) to denote the size of the set \(\tau_{o}=|\mathcal{T}_{o}|\). We define the following (scaled) data matrix

\[\mathbf{H}_{w}^{*}=\sum_{s\in\mathcal{T}_{o}}\dot{\mu}\left(\left\langle x_{s },\theta^{*}\right\rangle\right)x_{s}x_{s}^{\mathsf{T}}+\lambda\mathbf{I}\.\]

We will specify the regularizer \(\lambda\) later. We also define

\[\gamma\coloneqq\mathbf{c}RS\sqrt{d\log\frac{T}{\delta}}\]

Below, we state the main concentration bound used in the proof.

**Lemma B.1** (Theorem 1 of [6]).: _Let \(\{\mathcal{F}_{t}\}_{t=1}^{\infty}\) be a filtration. Let \(\{x_{t}\}_{t=1}^{\infty}\) be a stochastic process in \(B_{1}(d)\) such that \(x_{t}\) is \(\mathcal{F}_{t}\) measurable. Let \(\{\eta_{t}\}_{t=1}^{\infty}\) be a martingale difference sequence such that \(\eta_{t}\) is \(\mathcal{F}_{t}\) measurable. Furthermore, assume we have \(|\eta_{t}|\leq 1\) almost surely, and denote \(\sigma_{t}^{2}=\mathbb{V}[\eta_{t}|\mathcal{F}_{t}]\). Let \(\lambda>0\) and for any \(t\geq 1\) define:_

\[S_{t}=\sum_{s=1}^{t-1}\eta_{s}x_{s}\qquad\quad\mathbf{H}_{t}=\sum_{s=1}^{t-1} \sigma_{s}^{2}x_{s}x_{s}^{\intercal}+\lambda\mathbf{I}\]

_Then, for any \(\delta\in(0,1]\),_

\[\mathbb{P}\left[\exists t\geq 1:\|S_{t}\|_{\mathbf{H}_{t}^{-1}}\geq\frac{ \sqrt{\lambda}}{2}+\frac{2}{\sqrt{\lambda}}\log\left(\frac{\det(\mathbf{H}_{t })^{\frac{1}{2}}}{\lambda^{d/2}\delta}\right)+\frac{2}{\sqrt{\lambda}}d\log(2 )\right]\leq\delta\]

### Confidence Sets for Switching Criterion I rounds

**Lemma B.2**.: _At any round \(t\), let \(\widehat{\theta}_{o}\) be the maximum likelihood estimate calculated using set of rewards observed in the rounds \(\mathcal{T}_{o}\). With probability at least \(1-\delta\) we have_

\[\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{H}_{w}^{*}}\leq\gamma.\]

Proof.: Let us define the matrix \(\mathbf{G}_{w}=\sum_{s\in\mathcal{T}_{o}}\alpha(x,\theta^{*},\widehat{\theta} _{o})x_{s}x_{s}^{\intercal}+\lambda\mathbf{I}\). First, we note that by self-concordance property of \(\mu\) (Lemma C.2), \(\mathbf{G}_{w}\succeq\frac{1}{1+2RS}\mathbf{H}_{w}^{*}\). Hence,

\[\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{H}_{w}^{*}} \leq\sqrt{1+2RS}\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{G}_{ w}}\] \[=\sqrt{1+2RS}\left\|\mathbf{G}_{w}\left(\widehat{\theta}_{o}- \theta^{*}\right)\right\|_{\mathbf{G}_{w}^{-1}}\] \[=\sqrt{1+2RS}\left\|\sum_{s\in\mathcal{T}_{o}}\left(\left\langle \widehat{\theta}_{o},x_{s}\right\rangle-\left\langle\theta^{*},x_{s}\right\rangle \right)\alpha(x_{s},\widehat{\theta}_{o},\theta^{*})x_{s}+\lambda\widehat{ \theta}_{o}-\lambda_{t}\theta^{*}\right\|_{\mathbf{G}_{w}^{-1}}\] \[=\sqrt{1+2RS}\left\|\sum_{s\in\mathcal{T}_{o}}\left(\mu\left( \left\langle\widehat{\theta}_{o},x_{s}\right\rangle\right)-\mu\left(\left\langle \theta^{*},x_{s}\right\rangle\right)\right)x_{s}+\lambda\widehat{\theta}_{o}- \lambda_{t}\theta^{*}\right\|_{\mathbf{G}_{w}^{-1}}\] (Taylor's theorem) \[\leq(1+2RS)\left\|\sum_{s\in\mathcal{T}_{o}}\left(\mu\left( \left\langle\widehat{\theta}_{o},x_{s}\right\rangle\right)-\mu\left(\left\langle \theta^{*},x_{s}\right\rangle\right)\right)x_{s}+\lambda\widehat{\theta}_{o}- \lambda\theta^{*}\right\|_{\mathbf{H}_{w}^{*-1}}\] ( \[\mathbf{G}_{w}\succeq\frac{1}{1+2RS}\mathbf{H}_{w}^{*}\]

Since \(\widehat{\theta}_{o}\) is the maximum likelihood estimate, by optimality condition, we have the following relation: \(\sum_{s\in\mathcal{T}_{o}}\mu\left(\left\langle x_{s},\widehat{\theta}_{o} \right\rangle\right)x_{s}+\lambda\widehat{\theta}_{o}=\sum_{s\in\mathcal{T}_{o }}r_{s}x_{s}\). Substituting this above, we get

\[\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{H}_{w}^{*}} \leq(1+2RS)\left\|\sum_{s\in\mathcal{T}_{o}}\left(r_{s}-\mu\left( \left\langle\theta^{*},x_{s}\right\rangle\right)\right)x_{s}-\lambda\theta^{*} \right\|_{\mathbf{H}_{w}^{*-1}}\] \[\leq(1+2RS)\left\|\sum_{s\in\mathcal{T}_{o}}\left(r_{s}-\mu \left(\left\langle\theta^{*},x_{s}\right\rangle\right)\right)x_{s}\right\|_{ \mathbf{H}_{w}^{*-1}}+\lambda\left\|\theta^{*}\right\|_{\mathbf{H}_{w}^{*-1}}\] \[\leq(1+2RS)\left\|\sum_{s\in\mathcal{T}_{o}}\eta_{s}x_{s}\right\| _{\mathbf{H}_{w}^{*-1}}+S\sqrt{\lambda}.\qquad\qquad(\mathbf{H}_{w}^{*}\succeq \lambda_{t}\mathbf{I},\left\|\theta^{*}\right\|_{2}\leq S)\]

where \(\eta_{s}\coloneqq(r_{s}-\mu\left(\left\langle\theta^{*},x_{s}\right\rangle \right))\).

We will now apply Lemma B.1\(\eta_{s}\) scaled by \(R\). First note that \(\left\|\sum_{s\in\mathcal{T}_{o}}\eta_{s}x_{s}\right\|_{(\mathbf{H}_{w}^{*})^{ -1}}=\left\|\sum_{s\in\mathcal{T}_{o}}\frac{\eta_{o}}{R}x_{s}\right\|_{(R^{2} \mathbf{H}_{w}^{*})^{-1}}\) which, in turn ensures that the noise variable is upper bounded by \(1\)Applying B.1 we get

\[\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{H}^{*}_{w}}\leq S\sqrt{R ^{2}\lambda}\] \[\qquad\qquad+(1+2RS)\left(\frac{\sqrt{R^{2}\lambda}}{2}+\frac{2d}{ \sqrt{R^{2}\lambda}}\log\left(1+\frac{\tau_{o}}{R^{2}\lambda d}\right)+\frac{2} {\sqrt{R^{2}\lambda}}\log\left(\frac{1}{\delta}\right)+\frac{2}{\sqrt{R^{2} \lambda}}d\log(2)\right)\]

Simplifying constants and setting \(\sqrt{R^{2}\lambda}=\mathbf{c}\sqrt{d\log(T)+\log(1/\delta)}\), we have \(\left\|\widehat{\theta}_{o}-\theta^{*}\right\|_{\mathbf{H}^{*}_{w}}\leq \mathbf{c}RS\sqrt{d\log(T/\delta)+\log(1/\delta)}\leq\mathbf{c}RS\sqrt{d\log \left(T/\delta\right)}\). 

### Confidence Sets for non-Switching Criterion I rounds

We define \(\mathcal{E}_{w}\) be the event defined in Lemma B.2, that is, \(\mathcal{E}_{w}=\{\|\widehat{\theta}_{o}-\theta^{*}\|_{\mathbf{H}^{*}_{w}} \leq\gamma\}\).

**Lemma B.3**.: _If in round \(t\) the switching criteria I is not satisfied and the event \(\mathcal{E}_{w}\) holds, we have_

\[|\langle x,\widehat{\theta}_{o}-\theta^{*}\rangle|\leq\frac{1}{R}\quad\text{ for all }x\in\mathcal{X}_{t}.\]

Proof.: \[|\langle x,\widehat{\theta}_{o}-\theta^{*}\rangle| \leq\|x\|_{\mathbf{H}^{*-1}_{w}}\cdot\|\widehat{\theta}_{o}- \theta^{*}\|_{\mathbf{H}^{*}_{w}}\] (Cauchy-Schwarz) \[\leq\|x\|_{\mathbf{H}^{*-1}_{w}}\gamma\] (via Lemma B.2 ) \[\leq\gamma\sqrt{\kappa}\|x\|_{\mathcal{V}^{-1}_{w}}\] ( \[\mathbf{V}_{w}\preceq\kappa\mathbf{H}^{*}_{w}\] ) \[\leq\gamma\sqrt{\kappa}\frac{1}{\sqrt{R^{2}\kappa\gamma^{2}}}\] (warm-up criteria is not satisfied) \[\leq\frac{1}{R}\]

Recall that \(\mathbf{H}_{t}\) is defined in line 14 of Algorithm 2. Further, we define

\[\mathbf{H}^{*}_{t}=\sum_{s\in[t-1]\setminus\mathcal{T}_{o}}\dot{\mu}\left( \langle x_{s},\theta^{*}\rangle\right)x_{s}x_{s}^{\intercal}+\lambda\mathbf{I}\]

**Corollary B.4**.: _Under event \(\mathcal{E}_{w}\), \(\mathbf{H}_{t}\preceq\mathbf{H}^{*}_{t}\preceq e^{2}\mathbf{H}_{t}\)_

Proof.: For a given \(s\in[t-1]\setminus\mathcal{T}_{o}\), let \(\widehat{\theta}_{o}^{*}\) denote the value of \(\widehat{\theta}_{o}\) in that round. Then, for all \(x\in\mathcal{X}_{s}\), by Lemma C.2,

\[\dot{\mu}\left(\langle x,\widehat{\theta}_{o}^{*}\rangle\right)\exp(-R| \langle x,\widehat{\theta}_{o}^{*}-\theta^{*}\rangle|)\leq\dot{\mu}\left( \langle x,\theta^{*}\rangle\right)\leq\dot{\mu}\left(\langle x,\widehat{\theta }_{o}^{*}\rangle\right)\exp(R|\langle x,\widehat{\theta}_{o}^{*}-\theta^{*} \rangle|)\]

Applying lemma B.3, gives \(e^{-1}\dot{\mu}\left(\langle x,\widehat{\theta}_{o}^{*}\rangle\right)\leq \dot{\mu}\left(\langle x,\theta^{*}\rangle\right)\leq e^{1}\dot{\mu}\left( \langle x,\widehat{\theta}_{o}^{*}\rangle\right)\). Thus,

\[\mathbf{H}_{t}=\sum_{s\in[t-1]\setminus\mathcal{T}_{o}}e^{-1}\dot{\mu}\left( \langle x_{s},\widehat{\theta}_{o}^{*}\rangle\right)x_{s}x_{s}^{\intercal}+ \lambda\mathbf{I}\preceq\sum_{s\in[t-1]\setminus\mathcal{T}_{o}}\dot{\mu} \left(\langle x_{s},\theta^{*}\rangle\right)x_{s}x_{s}^{\intercal}+\lambda \mathbf{I}=\mathbf{H}^{*}_{t}\]

. Further, \(\mathbf{H}^{*}_{t}\preceq\sum_{s\in[t-1]\setminus\mathcal{T}_{o}}e^{2\frac{ \dot{\mu}\left(\langle x_{s},\widehat{\theta}_{o}^{*}\rangle\right)}{e}}x_{s} x_{s}^{\intercal}+e^{2}\lambda\mathbf{I}=e^{2}\mathbf{H}_{t}\). 
Recall that \(\tau\) is the round when the Switching Criterion II (Line 9) is satisfied. Now we define the following quantities:

\[g_{\tau}(\theta) =\sum_{s\in[\tau-1]\setminus\mathcal{T}_{o}}\mu\left(\left\langle x _{s},\theta\right\rangle\right)x_{s}+\lambda_{\tau}\theta\] \[\mathbf{H}_{\tau}(\theta) =\sum_{s\in[\tau-1]\setminus\mathcal{T}_{o}}\hat{\mu}\left( \left\langle x_{s},\theta\right\rangle\right)x_{s}x_{s}^{\intercal}+\lambda \mathbf{I}\] \[\Theta =\left\{\theta:\left\|\theta-\widehat{\theta}_{o}\right\|_{ \mathbf{V}}\leq\gamma\sqrt{\kappa}\right\}\] \[\tilde{\theta} =\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{d}}\sum_{s\in[t -1]\setminus\mathcal{T}_{o}}\ell(\theta,x_{s},r_{s})\] \[\beta \coloneqq\mathbf{c}\sqrt{d\log(T/\delta)}\]

Moreover, recall the following definition \(\widehat{\theta}_{\tau}\):

\[\widehat{\theta}_{\tau}=\operatorname*{arg\,min}_{\theta\in\Theta}\left\|g_{ \tau}(\theta)-g_{\tau}(\tilde{\theta})\right\|_{\mathbf{H}_{\tau}(\theta)^{- 1}}\] (21)

**Lemma B.5**.: _Under event \(\mathcal{E}_{w}\), \(\left\|\widehat{\theta}_{\tau}-\theta^{*}\right\|_{\mathbf{V}}\leq 2\gamma \sqrt{\kappa}\)_

Proof.: First, we observe from Lemma B.2 that \(\left\|\widehat{\theta}_{o}-\theta^{*}\right\|_{\mathbf{H}_{w}^{*}}\leq\gamma\). Using \(\mathbf{V}\preceq\kappa\mathbf{H}_{w}^{*}\), we can write \(\left\|\widehat{\theta}_{o}-\theta^{*}\right\|_{\mathbf{V}}\leq\gamma\sqrt{\kappa}\). This implies that \(\theta^{*}\in\Theta\). Now, \(\widehat{\theta}_{\tau}\in\Theta\) by virtue of being a feasible solution to the optimization in (21). Thus,

\[\left\|\widehat{\theta}_{\tau}-\theta^{*}\right\|_{\mathbf{V}} =\left\|\widehat{\theta}_{\tau}-\widehat{\theta}_{o}+\widehat{ \theta}_{o}-\theta^{*}\right\|_{\mathbf{V}}\] \[\leq\left\|\widehat{\theta}_{\tau}-\widehat{\theta}_{o}\right\| _{\mathbf{V}}+\left\|\widehat{\theta}_{o}-\theta^{*}\right\|_{\mathbf{V}}\] (triangle inequality) \[\leq 2\gamma\sqrt{\kappa}\]

**Lemma B.6**.: _Let \(\delta\in(0,1)\). Then, under event \(\mathcal{E}_{w}\), with probability \(1-\delta\), \(\left\|\widehat{\theta}_{\tau}-\theta^{*}\right\|_{\mathbf{H}_{\tau}^{*}}\leq\beta\)._

Proof.: We have for all rounds \(s\in[\tau-1]\setminus\mathcal{T}_{o}\),

\[|\langle x_{s},\theta^{*}-\widehat{\theta}_{\tau}\rangle| \leq\left\|x_{s}\right\|_{\mathbf{V}^{-1}}\left\|\theta^{*}- \widehat{\theta}_{\tau}\right\|_{\mathbf{V}}\] (Cauchy-Schwarz) \[\leq\left\|x_{s}\right\|_{\mathbf{V}^{-1}}2\gamma\sqrt{\kappa}\] (by Lemma B.5 ) \[\leq 2\gamma\sqrt{\kappa}\frac{1}{R\gamma\sqrt{\kappa}}\] (warm up criterion not satisfied) \[=\frac{2}{R}\]

Also note that \(\theta^{*}\in\Theta\). Hence,

\[\left\|\widehat{\theta}_{\tau}-\theta^{*}\right\|_{\mathbf{H}_{ \tau}^{*}} \leq 2(1+2)\left\|g_{\tau}(\theta^{*})-\sum_{s\in[\tau-1]}r_{s}x_{s} \right\|_{\mathbf{H}^{*-1}}\] (by Lemma E.1 ) \[\leq 6\mathbf{c}\sqrt{d\log(T/\delta)}\] (by Lemma B.1 )

Let the event in lemma B.6 be denoted by \(\mathcal{E}_{\tau}\), or in other words, \(\mathcal{E}_{\tau}=\{|\widehat{\theta}_{\tau}-\theta^{*}||_{\mathbf{H}_{\tau} ^{*}}\leq\beta\}\).

### Bounding the instantaneous regret

In this subsection, we will only consider rounds \(t\in[T]\) which does not satisfy Switching Criterion I. Let \(x_{t}\in\mathcal{X}_{t}\) be the played arm defined via line 13 Algorithm 2. Further, let \(x_{t}^{*}\in\mathcal{X}_{t}\) be the best available arm in that round.

**Corollary B.7**.: _Under the event \(\mathcal{E}_{\tau}\), for all \(x\in\mathcal{X}_{t}\), we have, \(|\langle x,\widehat{\theta}_{\tau}-\theta^{*}\rangle|\leq\beta\|x\|_{\mathbf{ H}_{\tau}^{-1}}\)._

Proof.: \[|\langle x,\widehat{\theta}_{\tau}-\theta^{*}\rangle| \leq\|x\|_{\mathbf{H}_{\tau}^{*-1}}\cdot\|\widehat{\theta}_{\tau }-\theta^{*}\|_{\mathbf{H}};\] (Cauchy-Schwartz) \[\leq\beta\|x\|_{\mathbf{H}_{\tau}^{*-1}}\] (by Lemma B.6) \[\leq\beta\|x\|_{\mathbf{H}_{\tau}^{-1}}\] ( \[\mathbf{H}_{\tau}\preceq\mathbf{H}_{\tau}^{*}\] )

**Lemma B.8**.: _Under event \(\mathcal{E}_{\tau}\), \(\langle x_{t}^{*}-x_{t},\theta^{*}\rangle\leq 2\sqrt{2}\beta\left\|x_{t}\right\|_{ \mathbf{H}_{t}^{*-1}}\)._

Proof.: \[\langle x_{t}^{*},\theta^{*}\rangle-\langle x_{t},\theta^{*}\rangle \leq\left(\langle x_{t}^{*},\widehat{\theta}_{\tau}\rangle+ \beta\left\|x_{t}^{*}\right\|_{\mathbf{H}_{t}^{*-1}}\right)-\left(\langle x_{ t},\widehat{\theta}_{\tau}\rangle-\beta\left\|x_{t}\right\|_{\mathbf{H}_{t}^{*-1}}\right)\] (by Corollary B.7) \[\leq\left(\langle x_{t},\widehat{\theta}_{\tau}\rangle+\beta \left\|x_{t}\right\|_{\mathbf{H}_{\tau}^{*-1}}\right)-\left(\langle x_{t}, \widehat{\theta}_{\tau}\rangle-\beta\left\|x_{t}\right\|_{\mathbf{H}_{\tau}^{* -1}}\right)\] (optimistic \[x_{t}\], see line 13 algo. 2) \[=2\beta\left\|x_{t}\right\|_{\mathbf{H}_{\tau}^{*-1}}\] (Lemma B.13, \[\frac{\det(\mathbf{H}_{\tau}^{-1})}{\det(\mathbf{H}_{t}^{-1})}=\frac{\det( \mathbf{H}_{t})}{\det(\mathbf{H}_{\tau}})\leq 2\] )

**Lemma B.9**.: _The arm set \(\mathcal{X}_{t}^{\prime}\) obtained after eliminating arms from \(\mathcal{X}_{t}\) (line 12 Algorithm 2), under event \(\mathcal{E}_{w}\), satisfies: (a) \(x_{t}^{*}\in\mathcal{X}_{t}\), (b) \(\langle x_{t}^{*}-x_{t},\theta^{*}\rangle\leq\frac{4}{R}\)_

Proof.: Suppose \(x^{\prime}=\arg\max_{x\in\mathcal{X}_{t}}LCB_{o}(x)\). Now, we have, for all \(x\in\mathcal{X}_{t}\),

\[|\langle x,\widehat{\theta}_{o}-\theta^{*}\rangle| \leq\left\|x\right\|_{\mathbf{H}_{\omega}^{*-1}}\left\|\widehat{ \theta}_{o}-\theta^{*}\right\|_{\mathbf{H}_{w}^{*}}\] (Cauchy-Schwarz) \[\leq\gamma\left\|x\right\|_{\mathbf{H}_{w}^{*-1}}\] (by Lemma B.2) \[\leq\gamma\sqrt{\kappa}\left\|x\right\|_{\mathbf{V}^{-1}}\] ( \[\mathbf{V}\preceq\kappa\mathbf{H}_{w}^{*}\] )

Thus, \(UCB_{o}(x_{t}^{*})=\langle x_{t}^{*},\widehat{\theta}_{o}\rangle+\gamma\sqrt{ \kappa}\left\|x_{t}^{*}\right\|_{\mathbf{V}^{-1}}\geq\langle x_{t}^{*},\theta ^{*}\rangle\geq\langle x^{\prime},\theta^{*}\rangle\geq\langle x^{\prime}, \widehat{\theta}_{o}\rangle-\gamma\sqrt{\kappa}\left\|x^{\prime}\right\|_{ \mathbf{V}^{-1}}=LCB_{o}(x^{\prime})\), where the second inequality is due to optimality of \(x_{t}^{*}\). Hence, \(x_{t}^{*}\) is not eliminated, implying \(x_{t}^{*}\in\mathcal{X}_{t}^{\prime}\). This completes the proof of (a).

Since \(x_{t}\) is also in \(\mathcal{X}_{t}^{\prime}\) (by definition),

\[UCB_{o}(x_{t})=\langle x_{t},\widehat{\theta}_{o}\rangle+\gamma \sqrt{\kappa}\left\|x_{t}\right\|_{\mathbf{V}^{-1}} \geq\langle x^{\prime},\widehat{\theta}_{o}\rangle-\gamma\sqrt{ \kappa}\left\|x^{\prime}\right\|_{\mathbf{V}^{-1}}\] \[\geq\langle x_{t}^{*},\widehat{\theta}_{o}\rangle-\gamma\sqrt{ \kappa}\left\|x_{t}^{*}\right\|_{\mathbf{V}^{-1}}\] ( \[x^{\prime}\] has max \[LCB_{o}(\cdot)\] )

Again, using the fact that \(\langle x_{t}^{*},\widehat{\theta}_{o}\rangle\geq\langle x_{t}^{*},\theta^{*} \rangle-\gamma\sqrt{\kappa}\left\|x_{t}^{*}\right\|_{\mathbf{V}^{-1}}\) and \(\langle x_{t},\widehat{\theta}_{o}\rangle\leq\langle x_{t},\theta^{*}\rangle+ \gamma\sqrt{\kappa}\left\|x_{t}\right\|_{\mathbf{V}^{-1}}\), we obtain,

\[\langle x_{t},\theta^{*}\rangle+2\gamma\sqrt{\kappa}\left\|x_{t} \right\|_{\mathbf{V}^{-1}}\geq\langle x_{t}^{*},\theta^{*}\rangle-2\gamma \sqrt{\kappa}\left\|x_{t}^{*}\right\|_{\mathbf{V}^{-1}}\] \[\text{which gives us,}\quad\langle x_{t}^{*}-x_{t},\theta^{*} \rangle\leq 2\gamma\sqrt{\kappa}\left\|x_{t}\right\|_{\mathbf{V}^{-1}}+2\gamma \sqrt{\kappa}\left\|x_{t}^{*}\right\|_{\mathbf{V}^{-1}}\]

Finally, since Switching Criterion I is not satisfied in this round, \(\left\|x\right\|_{\mathbf{V}^{-1}}<\frac{1}{R\gamma\sqrt{\kappa}}\) for all \(x\in\mathcal{X}_{t}\). Plugging this above,

\[\langle x_{t}^{*}-x_{t},\theta^{*}\rangle\leq\frac{4}{R}\]

[MISSING_PAGE_FAIL:26]

Putting things back,

\[\text{R}_{T}\leq\mathbf{c}d\log(RT/\delta)\sqrt{\sum_{t\in[T]\backslash\mathcal{T}_ {o}}\dot{\mu}\left(\left\langle x_{t}^{*},\theta^{*}\right\rangle\right)}+ \mathbf{c}R^{5}S^{2}\kappa\log(T/\delta)^{2}.\]

### Bounding number of policy updates: Proof of Lemma 4.1

We first obtain a bound on the number of rounds when Switching Criterion I is satisfied. Then we restate Lemma 4.1 and present its proof. Here, we use \(\mathcal{T}_{o}\) to denote the collection of all rounds till \(T\) for which Switching Criterion I is satisfied.

**Lemma B.11**.: _Algorithm 2, during its entire execution, satisfies the Switching Criterion I at most \(2dR^{2}\kappa\gamma^{2}\log\left(T/\delta\right)\) times._

Proof.: Recall that Switching Criterion I (Line 4) is satisfied, when \(\|x\|_{\mathbf{V}^{-1}}^{2}>1/(R^{2}\kappa\gamma^{2})\) for some \(x\in\mathcal{X}_{t}\). Let \(\mathbf{V}_{m}\) be the sequence of \(\mathbf{V}\) matrices (line 6 of Algorithm 2) for \(m\in\mathcal{T}_{o}\). That is, \(\mathbf{V}_{1}=\lambda\mathbf{I},\mathbf{V}_{m}=\sum_{s\in[m-1]\cap\mathcal{T }_{o}}x_{s}x_{s}^{\intercal}+\lambda\mathbf{I}\). In these rounds, by Line 5 of Algorithm 2, we have that the arm played \(x_{t}\) is such that \(x_{t}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{t}}\left\|x\right\|_{ \mathbf{V}_{t}^{-1}}\). Therefore,

\[\sum_{t\in\mathcal{T}_{o}}\|x_{t}\|_{\mathbf{V}_{t}^{-1}}^{2}\geq\frac{\tau_{ o}}{R^{2}\kappa\gamma^{2}}\] (22)

Furthermore, by the Elliptic Potential Lemma (Lemma B.12) we have

\[\sum_{t\in\mathcal{T}_{o}}\left\|x_{t}\right\|_{\mathbf{V}_{t}^{-1}}^{2}\leq 2 d\log\left(1+\frac{\tau_{o}}{\lambda d}\right)\] (23)

Combining (23) and (22) we have

\[\tau_{o}\leq 2dR^{2}\kappa\gamma^{2}\log\left(1+\frac{\tau_{o}}{\lambda d} \right)\leq 2dR^{2}\kappa\gamma^{2}\log\left(T\right)\leq 2dR^{2}\kappa \gamma^{2}\log\left(T/\delta\right)\]

**Lemma** (4.1).: _Algorithm 2, during its entire execution, updates its policy at most \(O(R^{4}S^{2}\ \kappa d^{2}\ \log^{2}(T/\delta))\) times._

Proof.: Note that in Algorithm 2, policy changes happen only in the rounds when either of the Switching Criteria are triggered. The number of times Switching Criterion I is triggered is bounded by Lemma B.11. On the other hand, the number of times Switching Criterion II is triggered is equal to the number of times determinant of \(\mathbf{H}_{t}\) doubles, which is bounded by Lemma B.15. Thus in total, the number of policy changes in Algorithm 2 is upper bounded by \(2dR^{2}\kappa\gamma^{2}\log(T/\delta)+\mathbf{c}d\log(T)\). 

### Some Useful Lemmas

**Lemma B.12** (Elliptic Potential Lemma (Lemma 10 [1])).: _Let \(x_{1},x_{2},\ldots x_{t}\) be a sequence of vectors in \(\mathbb{R}^{d}\) and let \(\left\|x_{s}\right\|_{2}\leq L\) for all \(s\in[t]\). Further, let \(\mathbf{V}_{s}=\sum_{m=1}^{s-1}x_{m}x_{m}^{\intercal}+\lambda\mathbf{I}\). Suppose \(\lambda\geq L^{2}\). Then,_

\[\sum_{s=1}^{t}\left\|x_{s}\right\|_{\mathbf{V}_{s}^{2}}^{2}\leq 2d\log\left(1+ \frac{L^{2}t}{\lambda d}\right)\] (24)

**Lemma B.13** (Lemma 12 of [1]).: _Let \(A\succeq B\succ 0\). Then_

\[\sup_{x\neq 0}\frac{x^{\intercal}Ax}{x^{\intercal}Bx}\leq\frac{\det(A)}{\det(B)}\]

**Lemma B.14** (Lemma 10 of [1]).: _Let \(\{x_{s}\}_{s=1}^{t}\) be a set of vectors. Define the sequence \(\{\mathbf{V}_{s}\}_{s=1}^{t}\) as \(\mathbf{V}_{1}=\lambda\mathbf{I}\), \(\mathbf{V}_{s+1}=\mathbf{V}_{s}+x_{s}x_{s}^{\intercal}\) for \(s\in[t-1]\). Further, let \(\left\|x_{s}\right\|_{2}\leq L\ \forall\ s\in[t]\). Then,_

\[\det(\mathbf{V}_{t})\leq\left(\lambda+tL^{2}/d\right)^{d}\.\]

[MISSING_PAGE_FAIL:28]

Proof.: Without loss of generality, assume that \(z_{2}\geq z_{1}\). Note that by property of integration \(\int_{a}^{b}f(x)dx=\int_{b}^{a}f(b+a-x)dx\), \(\alpha(z_{1},z_{2})=\alpha(z_{2},z_{1})\). Now, by proposition C.1, and the fact that \(\ddot{\mu}(z)=\ \ddot{b}\ (z)\), we have for any \(v\in\mathbb{R}\) and \(z\geq z_{1}\),

\[-R\dot{\mu}(v)\leq \ddot{\mu}(v)\leq R\dot{\mu}(v)\] (Lemma C.1) \[-R\leq \frac{\ddot{\mu}(v)}{\dot{\mu}(v)}\leq R\] \[-R\int_{z}^{z_{1}}dv\leq \int_{z}^{z_{1}}\frac{\ddot{\mu}(v)}{\dot{\mu}(v)}dv\leq R\int_{z }^{z_{1}}dv\] \[-R(z-z_{1})\leq \log\left(\frac{\dot{\mu}(z)}{\dot{\mu}(z_{1})}\right)\leq R(z-z _{1})\] \[\dot{\mu}(z_{1})\exp(-R(z-z_{1}))\leq \dot{\mu}(z)\leq\dot{\mu}(z_{1})\exp(R(z-z_{1}))\]

Putting \(z=z_{2}\) establishes 26. To show 25, we further set \(z=z_{1}+u(z_{2}-z_{1})\) for \(u\in[0,1]\), (note that \(z\geq z_{1}\)) and integrate on \(u\),

\[\dot{\mu}(z_{1})\int_{0}^{1}\exp(-Ru(z_{2}-z_{1}))du\leq \int_{0}^{1}\dot{\mu}(z_{1}+u(z_{2}-z_{1}))du\leq\int_{0}^{1}\exp (Ru(z_{2}-z_{1}))du\] \[\text{which gives}\quad\dot{\mu}(z_{1})\frac{1-\exp(-R(z_{2}-z_{1}))}{ R(z_{2}-z_{1})}\leq \alpha(z_{1},z_{2})\leq\dot{\mu}(z_{1})\frac{\exp(R(z_{2}-z_{1}))-1 }{R(z_{2}-z_{1})}\]

Next, we use the fact that for \(x>0\), \(e^{-x}\leq(1+x)^{-1}\) which on rearranging gives \((1-e^{-x})/x\geq 1/(1+x)\). Applying this inequality to the LHS above finishes the proof. Note that similar exercise can be repeated with \(z_{2}\leq z_{1}\) to get the same result for \(z_{2}\).

For 27, we have, by application of 26, \(\dot{\mu}(z_{1}+v(z_{2}-z_{1}))\geq\dot{\mu}(z_{1})\exp(R|v(z_{2}-z_{1})|)\). Therefore,

\[\tilde{\alpha}(z_{1},z_{2}) =\int_{v=0}^{1}(1-v)\dot{\mu}\left(z_{1}+v(z_{2}-z_{1})\right)dv\] \[\geq\int_{v=0}^{1}(1-v)\dot{\mu}(z_{1})\exp(-R|v(z_{1}-z_{2})|)dv\] \[=\dot{\mu}(z_{1})\int_{v=0}^{1}(1-v)\exp(-Rv|(z_{1}-z_{2})|)dv (v\in[0,1])\] \[=\dot{\mu}(z_{1})\left(\frac{1}{R|z_{1}-z_{2}|}+\frac{\exp(-R|z_{1 }-z_{2}|)-1}{R^{2}|z_{1}-z_{2}|^{2}}\right)\] \[\geq\dot{\mu}(z_{1})\cdot\frac{1}{2+R|z_{1}-z_{2}|} \text{(Lemma \ref{lem:27} of \@@cite[cite]{[\@@bibref{}{lem:27}{}{}]})}\]

Next we state some nice properties of the GLM family that is the key in deriving Lemma C.1.

**Lemma C.3** (Properties of GLMs).: _For any random variable \(r\) that is distributed by a canonical exponential family, we have_

1. \(\mathbb{E}\left[r\right]=\mu\left(z\right)=\dot{b}\left(z\right)\)__
2. \(\mathbb{V}[r]=\mathbb{E}\left[\left(r-\mathbb{E}\left[r\right]\right)^{2} \right]=\dot{\mu}\left(z\right)=\ddot{b}\left(z\right)\)__
3. \(\mathbb{E}\left[\left(r-\mathbb{E}[r]\right)^{3}\right]=\ \ddot{b}\left(z\right)\)__Proof.:
1. Indeed, since \(p_{z}(r)\) is a probability distribution, \(\int_{r}p_{z}(r)dr=1\) which in turn implies that \(b(z)=\log\left(\int_{r}\exp(rz+c(r))dr\right)\). Thus, taking derivative, \[\dot{b}(z) =\frac{1}{\int_{r}\exp(rz+c(r))dr}\int_{r}\frac{\partial}{ \partial z}\exp(rz+c(r))dr\] \[=\exp(-b(z))\int_{r}r\exp(rz+c(r))dr\] \[=\int_{r}r\exp(rz-b(z)+c(r))dr=\mathbb{E}[r]\]
2. Let \(f(z)\coloneqq\int_{r}r\exp(rz+c(r))dr\). Thus, \(\dot{b}(z)=\exp(-b(z))f(z)\). Taking derivative on both sides, \[\ddot{b}(z) =-\dot{b}(z)\exp(-b(z))f(z)+\exp(-b(z))\dot{f}(z)\] \[=-\,\mathbb{E}[r]^{2}+\exp(-b(z))\int_{r}r^{2}\exp(rz+c(r))dr\] \[=-\,\mathbb{E}[r]^{2}+\int_{r}r^{2}\exp(rz-b(z)+c(r))dr\] \[=-\,\mathbb{E}[r]^{2}+\mathbb{E}[r^{2}]=\mathbb{V}[r]\]
3. Again let \(f(z)\coloneqq\int_{r}r^{2}\exp(rz+c(r))dr\). Thus, \(\ddot{b}(z)=-\dot{b}(z)^{2}+\exp(-b(z))f(z)\). Taking derivative on both sides, \[\dddot{b}(z) =-2\dot{b}(z)\ddot{b}(z)-\dot{b}(z)\exp(-b(z))f(z)+\exp(-b(z)) \dot{f}(z)\] \[=-2\dot{b}(z)\ddot{b}(z)-\dot{b}(z)\,\mathbb{E}[r^{2}]+\int_{r}r^ {3}\exp(rz-b(z)+c(r))dr\] \[=-2\,\mathbb{E}[r]\mathbb{V}[r]-\mathbb{E}[r]\,\mathbb{E}[r^{2}]+ \mathbb{E}[r^{3}]\] Now, let us expand \(\mathbb{E}[(r-\mathbb{E}[r])^{3}]\). \[\mathbb{E}[(r-\mathbb{E}[r])^{3}] =\mathbb{E}[r^{3}-3r^{2}\,\mathbb{E}[r]+3r\,\mathbb{E}[r]^{2}- \mathbb{E}[r]^{3}]\] \[=\mathbb{E}[r^{3}]-3\,\mathbb{E}[r]\,\mathbb{E}[r^{2}]+3\, \mathbb{E}[r]^{3}-\mathbb{E}[r]^{3}\] \[=\mathbb{E}[r^{3}]-\mathbb{E}[r]\,\mathbb{E}[r^{2}]-2\,\mathbb{E }[r]\left(-\mathbb{E}[r^{2}]+\mathbb{E}[r]^{2}\right)\] \[=\mathbb{E}[r^{3}]-\mathbb{E}[r]\,\mathbb{E}[r^{2}]-2\,\mathbb{E }[r]\mathbb{V}[r]\]

**Corollary C.4**.: _For all exponential family, \(b(\cdot)\) is a convex function._

Proof.: Indeed, note that \(\ddot{b}(z)=\mathbb{V}[r]\) which is always non-negative. Thus, \(\ddot{b}(z)\geq 0\) implying that \(b(\cdot)\) is convex. 

_Remark C.5_.: In [5] Section 1.4.1, the author claims that if the GLM parameter \(z\) lies in a bounded set, then the GLM is self-concordant, i.e., \(|\ddot{\mu}(z)|\leq a\dot{\mu}(z)\), for some appropriate constant \(a\) over this bounded set. Thereafter the author notes that the techniques developed in [5] guarantees \(\kappa\)-free regret rates (in \(\sqrt{T}\) term) for such GLMs (i.e., all GLMs with bounded parameter). However, the claim regarding self-concordance of GLMs is not true in general. There are classes of GLMs whose parameters may be restricted in a bounded set, but for them no constant \(a\) exists. One such example is the exponential distribution. The link function \(\mu\) for exponential distribution is given as \(\mu(z)=-\frac{1}{z}\). If we allow \(z\) to lie in the set \((-c,0)\) for some positive \(c\), then we have \(\mu(z)\) strictly increasing (satisfying our assumption on monotonicity of \(\mu\), thus a valid example). However, for this GLM,

\[\dot{\mu}(z)=\frac{1}{z^{2}}\qquad\ddot{\mu}(z)=-\frac{2}{z^{3}}\]Note that \(\hat{\mu}(z)\) is positive for the assumed support of \(z\). Suppose this GLM is self-concordant, then we must have some positive constant \(a\) such that

\[|\hat{\mu}(z)|=-\frac{2}{z^{3}}\leq a\hat{\mu}(z)=a\frac{1}{z^{2}}\.\]

Simplifying, we obtain the following relation:

\[-\frac{2}{z}\leq a\.\]

However, since \(z\in(-c,0)\), we have \(\lim_{z\to 0}-\frac{2}{z}\rightarrow\infty\). Hence, no constant \(a\) is possible. By this counterexample it can be seen that bounded parameter set is not enough to guarantee self-concordance of GLMs. In this work, we give a characterization of self-concordance of GLMs with bounded support of the random variable. It will be interesting to understand a complete characterization of self-concordance of GLMs.

## Appendix D Computational Cost

Consider a log-loss minimization oracle that returns the unconstrained MLE for a given GLM class with a computational complexity of \(C_{opt}\cdot n\), when the log-loss is computed over \(n\) data points. Let the maximum number of arms available every round be \(K\). Furher, let the computational cost of an oracle that solves the non-convex optimization \(\mathcal{\mathbb{8}}\) be \(NC_{opt}\).

**Computational Cost of B-GLinCB**: In the B-GLinCB algorithm, we employ the log-loss oracle at the end of each batch. The estimator \(\widehat{\theta}\) calculated at the end of a batch of length \(\tau\) incurs a computational cost of \(C_{opt}\tau\). Furthermore, this oracle is invoked for a maximum of \(M\leq\log\log T\) batches. Additionally, the computation of the distributional optimal design at the end of each batch is efficient in \(d\) (\(\text{poly}(d)\)). Moreover, in every round, the algorithm solves the \(D/G-\) Optimal Design problem (requiring \(O(d\log d)\) computation) and runs elimination based on prior (at most \(\log\log T\)) phases. Hence, the amortized cost per round of B-GLinCB is \(O(K\log\log T+d\log d+C_{opt})\).

**Computational Cost of RS-GLinCB**: In the RS-GLinCB algorithm, the estimator \(\widehat{\theta}_{o}\) is computed each time Switching Criterion I is triggered. Additionally, during rounds when Switching Criterion I is not triggered, the estimator \(\widehat{\theta}_{\tau}\) is computed a maximum of \(O(\log(T))\) times. These computations involve utilizing both the log-loss oracle and the non-convex projection oracle. Furthermore, in each non-Switching Criterion I round, the algorithm executes an elimination step. This yields an amortized time complexity of \(O(C_{opt}\log\left(T\right)+NC_{opt}\log^{2}(T)+K)\) per round.

**Performance in Practice**: As evident from Fig. 1, RS-GLinCB has much better computational performance in practice. We ran all the experiments on an Azure Data Science VM equipped with AMD EPYC 7V13 64-Core Processor (clock speed of 2.45 GHz) and Linux Ubuntu 20.04 LTS operating system. It was ensured that no other application processes were running while we tested the performance. We implemented and tested our code in Python, and measured the execution times using time.time() command. We allowed no operations for 10 seconds after every run to let the CPU temperature come back to normal, in case the execution heats up the CPU, thereby causing subsequent runs to slow down.

Comparison with ECOLog[7] shows that execution time for RS-GLinCB is significantly smaller. We posit that this is because RS-GLinCB solves a large convex optimization problem but less frequently, resulting into smaller overhead at the implementation level, while ECOLog solves a smaller convex optimization problem, but does so every round. On an implementation level, this translates into more function calls and computation. Further, we observe that with increasing \(\kappa\), the execution time of RS-GLinCB increases, which is in accordance with Lemma 4.1 that quantifies the number of policy switches as an increasing function of \(\kappa\).

While comparing with GLOC[14], we observe that RS-GLinCB performs better than GLOC in both high and low \(\kappa\) regimes. Since GLOC runs an online convex optimization (online Newton step) algorithm to generate its confidence sets, the time taken by GLOC is nearly constant with changing \(\kappa\). On the other hand, in accordance with Lemma 4.1, the computational cost of RS-GLinCB increases with \(\kappa\). However, after a few initial rounds, when neither of the switching criteria are triggered, RS-GLinCB does not need to solve any computationally intensive optimization problem, hence these rounds execute very fast. In practice, with typical data distribution, RS-GLinCB reaches this stage much before what the worst-case guarantees show, hence we see it perform better than GLOC.

Projection

We describe the projection step used in Algorithms 1 and 2. We present arguments similar to the ones made in Appendix B.3 of [6]. We write

\[\mathbf{H}(\theta)=\sum_{s=1}^{t}\dot{\mu}\left(\left\langle\theta,x_{s}\right\rangle \right)x_{s}x_{s}^{\mathsf{T}}+\lambda\mathbf{I}\]

Recall, \(\mathbf{H}^{*}=\mathbf{H}(\theta^{*})\). Let \(\widehat{\theta}\) be the MLE estimator of \(\theta^{*}\) calculated after the sequence arm pulls \(x_{1},x_{2},\ldots,x_{t}\). Let \(r_{1},r_{2},\ldots,r_{t}\) be the corresponding observed rewards. We project \(\widehat{\theta}\) to a set \(\Theta\) by solving the following optimization problem

\[\widetilde{\theta}\coloneqq\operatorname*{arg\,min}_{\theta\in\Theta}\left\| \sum_{s=1}^{t}(\mu\left(\left\langle x_{s},\theta\right\rangle\right)-\mu \left(\left\langle x_{s},\widehat{\theta}\right\rangle\right))x_{s}\right\|_{ \mathbf{H}(\theta)^{-1}}\] (28)

**Lemma E.1**.: _Using the notations described above, if \(\theta^{*}\in\Theta\) and \(\max_{i\in[t]}|\langle x_{i},\widetilde{\theta}-\theta^{*}\rangle|\leq c/R\), then we have_

\[\left\|\widetilde{\theta}-\theta^{*}\right\|_{\mathbf{H}(\theta^{*})}\leq 2 (1+c)\left\|\sum_{s=1}^{t}(\mu\left(\left\langle x_{s},\theta^{*}\right\rangle \right)-r_{s})x_{s}\right\|_{\mathbf{H}(\theta^{*})^{-1}}\]

Proof.: First, we note that by self-concordance property of \(\mu\) (lemma C.2), for any \(s\in[t]\),

\[\alpha(x_{s},\widetilde{\theta},\theta^{*}) \geq\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{1+R|\langle x_{s},\widetilde{\theta}-\theta^{*}\rangle|}\] \[\geq\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{1+R(c/R)} (\max_{i\in[s]}|\langle x_{s},\widetilde{\theta}-\theta^{*} \rangle|\leq c/R)\] \[=\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{1+c}\]

Similarly, we have \(\alpha(x_{s},\widetilde{\theta},\theta^{*})\geq\frac{\dot{\mu}\left(\left\langle x _{s},\widetilde{\theta}\right\rangle\right)}{1+c}\).

Let us define the matrix \(\mathbf{G}=\sum_{s\in[t]}\alpha(x,\widetilde{\theta},\theta^{*})x_{s}x_{s}^{ \mathsf{T}}\). Using the above fact, we obtain the relation: \(\mathbf{G}\succeq\frac{1}{1+c}\mathbf{H}^{*}\) and \(\mathbf{G}\succeq\frac{1}{1+c}\mathbf{H}(\widetilde{\theta})\). Also define the vector \(g(\theta)=\sum_{s\in[t]}\mu\left(\left\langle\theta,x_{s}\right\rangle\right) x_{s}\). Now,

\[\left\|\widetilde{\theta}-\theta^{*}\right\|_{\mathbf{H}^{*}} \leq\sqrt{1+c}\left\|\widetilde{\theta}-\theta^{*}\right\|_{ \mathbf{G}} (\mathbf{H}^{*}\preceq(\sqrt{1+c})\mathbf{G})\] \[=\sqrt{1+c}\left\|\mathbf{G}\left(\widetilde{\theta}-\theta^{*} \right)\right\|_{\mathbf{G}^{-1}}\] \[=\sqrt{1+c}\left\|\sum_{s\in[t]}\left(\alpha(x_{s},\widetilde{ \theta},\theta^{*})\langle\widetilde{\theta}-\theta^{*},x_{s}\rangle\right)x_{ s}\right\|_{\mathbf{G}^{-1}}\] \[=\sqrt{1+c}\left\|\sum_{s\in[t]}\left(\mu\left(\left\langle x_{s },\widetilde{\theta}\right\rangle\right)-\mu\left(\left\langle x_{s},\theta^ {*}\right\rangle\right)x_{s}\right\|_{\mathbf{G}^{-1}}\] (Taylor's theorem) \[=\sqrt{1+c}\left\|\left(\sum_{s\in[t]}\mu\left(\left\langle \widetilde{\theta},x_{s}\right\rangle\right)x_{s}\right)-\left(\sum_{s\in[t] }\mu\left(\left\langle\theta^{*},x_{s}\right\rangle\right)x_{s}\right)\right\| _{\mathbf{G}^{-1}}\]Let \(g(\theta)=\sum_{s=1}^{t}\dot{\mu}\left(\left\langle x_{s},\theta\right\rangle \right)x_{s}\) for any \(\theta\). Therefore, we have,

\[\left\|\widetilde{\theta}-\theta^{*}\right\|_{\mathbf{H}^{*}} \leq\sqrt{1+c}\left\|g(\widetilde{\theta})-g(\theta^{*})\right\| _{\mathbf{G}^{-1}}\] \[=\sqrt{1+c}\left\|g(\widetilde{\theta})-g(\widehat{\theta})+g( \widehat{\theta})-g(\theta^{*})\right\|_{\mathbf{G}^{-1}}\] \[\leq\sqrt{1+c}\left(\left\|g(\widetilde{\theta})-g(\widehat{ \theta})\right\|_{\mathbf{G}^{-1}}+\left\|g(\widehat{\theta})-g(\theta^{*}) \right\|_{\mathbf{G}^{-1}}\right)\] ( \[\triangle\] inequality) \[\leq(1+c)\left(\left\|g(\widehat{\theta})-g(\widehat{\theta}) \right\|_{\mathbf{H}(\widetilde{\theta})^{-1}}+\left\|g(\widehat{\theta})-g( \theta^{*})\right\|_{\mathbf{H}^{*-1}}\right)\] ( \[\mathbf{H}^{*-1}\succeq(\sqrt{1+c})\mathbf{G}^{-1}\] ) \[\leq 2(1+c)\left\|g(\widehat{\theta})-g(\theta^{*})\right\|_{ \mathbf{H}^{*-1}}\] (by ( 28 )) \[=2(1+c)\left\|g(\theta^{*})-\sum_{s\in[t]}r_{s}x_{s}\right\|_{ \mathbf{H}^{*-1}}\] ( \[\widehat{\theta}\] is the unconstrained MLE, \[g(\widehat{\theta})=\sum_{s\in[t]}r_{s}x_{s}\].)

### Convex Relaxation

The optimization problem in (28) is a non-convex optimization problem and therefore it is not clear what is the computational complexity of the problem. However, it is possible to substitute this optimization problem with a convex one, whose computational complexity can be better tractable. The process is similar to the one detailed in [2, section 6]. Here we briefly outline the steps.

Let \(\mathcal{L}_{t}(\theta)=\sum_{s=1}^{t}\ell(\theta,x_{s},r_{s})\) and \(\breve{\theta}\) be defined as follows:

\[\breve{\theta}\coloneqq\operatorname*{arg\,min}_{\breve{\theta}\in\Theta} \mathcal{L}_{t}(\theta)\] (29)

Note that when the set \(\Theta\) is a convex set, then the above optimization problem is convex by property of the log-likelihood function of GLMs. Hence it can be solved efficiently. With this projected \(\breve{\theta}\), we have the following guarantee:

**Lemma E.2**.: _Suppose \(\left\|g(\widehat{\theta})-g(\theta^{*})\right\|_{\mathbf{H}^{*-1}}\leq\gamma\) and \(\lambda=\gamma/R\). If \(\theta^{*}\in\Theta\) and \(\max_{i\in[t]}|\left\langle x_{i},\breve{\theta}-\theta^{*}\right\rangle|\leq c /R\), then we have_

\[\left\|\breve{\theta}-\theta^{*}\right\|_{\mathbf{H}(\theta^{*})}\leq\bm{c} \sqrt{(2+c)R^{3}S}\gamma\]

Proof.: First we note that by self-concordance property of \(\mu\), for any \(s\in[t]\),

\[\tilde{\alpha}(x_{s},\theta^{*},\breve{\theta}) \geq\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{2+R|\langle x_{s},\breve{\theta}-\theta^{*}\rangle|}\] (Lemma C.2) \[\geq\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{2+R(c/R)}\] ( \[\max_{i\in[s]}|\left\langle x_{s},\widetilde{\theta}-\theta^{*} \right\rangle|\leq c/R\] ) \[=\frac{\dot{\mu}\left(\left\langle x_{s},\theta^{*}\right\rangle \right)}{2+c}\]

Let us define \(\tilde{\mathbf{G}}(\theta^{*},\theta)\coloneqq\sum_{s=1}^{t}\tilde{\alpha}(x_ {s},\theta^{*},\theta)x_{s}x_{s}^{\intercal}\). Using the above fact, we obtain \(\tilde{\mathbf{G}}(\theta^{*},\theta)\succeq\frac{1}{2+c}\mathbf{H}^{*}\).

We now follow closely the proof outlined in Appendix B.3 of [2] with minor changes. By second-order Taylor's expansion, for any \(\theta\in\mathbb{R}^{d}\), we can write

\[\mathcal{L}_{t}(\theta)-\mathcal{L}_{t}(\theta^{*})-\langle\nabla \mathcal{L}_{t}(\theta^{*}),\theta-\theta^{*}\rangle =\left\lVert\theta-\theta^{*}\right\rVert_{\mathbf{G}(\theta, \theta^{*})}^{2}\] \[\geq\frac{1}{2+c}\left\lVert\theta-\theta^{*}\right\rVert_{ \mathbf{H}^{*}}^{2}.\]

Taking absolute value on both sides, and substituting \(\theta=\breve{\theta}\),

\[\left\lVert\breve{\theta}-\theta^{*}\right\rVert_{\mathbf{H}^{*}}^ {2} \leq(2+c)\left(\left\lvert\mathcal{L}_{t}(\breve{\theta})- \mathcal{L}_{t}(\theta^{*})\right\rvert+\left\lvert\langle\nabla\mathcal{L}_{ t}(\theta^{*}),\breve{\theta}-\theta^{*}\rangle\right\rvert\right)\] ( \[\triangle\] -inequality) \[\leq(2+c)\left(\left\lvert\mathcal{L}_{t}(\breve{\theta})- \mathcal{L}_{t}(\theta^{*})\right\rvert+\left\lVert\nabla\mathcal{L}_{t}( \theta^{*})\right\rVert_{\mathbf{H}^{*-1}}\left\lVert\breve{\theta}-\theta^{*} \right\rVert_{\mathbf{H}^{*}}\right)\] (Cauchy-Schwarz) \[=(2+c)\left(\left\lvert\mathcal{L}_{t}(\breve{\theta})- \mathcal{L}_{t}(\theta^{*})\right\rvert+\left\lVert g(\theta^{*})-\sum_{s\in[ t]}r_{s}x_{s}\right\rVert_{\mathbf{H}^{*-1}}\left\lVert\breve{\theta}-\theta^{*} \right\rVert_{\mathbf{H}^{*}}\right)\]

Recall that \(\widehat{\theta}\) is the unconstrained MLE, therefore \(\nabla\mathcal{L}_{t}(\widehat{\theta})=\mathbf{0}\). By a similar Taylor expansion as above and some algebraic manipulations (see Appendix B.3 of [2]), we have, for \(\theta^{*}\).

\[\mathcal{L}_{t}(\theta^{*})-\mathcal{L}_{t}(\widehat{\theta}) \leq\left\lVert g(\theta^{*})-g(\widehat{\theta})\right\rVert_{ \mathbf{G}(\theta^{*},\widehat{\theta})^{-1}}^{2}\] \[\leq\frac{R}{\sqrt{\lambda}}\left\lVert g(\theta^{*})-g(\widehat{ \theta})\right\rVert_{\mathbf{H}^{*-1}}^{2}+\left\lVert g(\theta^{*})-g( \widehat{\theta})\right\rVert_{\mathbf{H}^{*-1}}\] \[\leq\frac{R}{\sqrt{\lambda}}\gamma^{2}+\gamma\] (Lemma B.1 ) \[\leq 2R^{3}S\gamma\] (recall \[\sqrt{R^{2}\lambda}=\gamma/RS\] )

We also have, by definition of \(\breve{\theta}\), whenever \(\theta^{*}\in\Theta\), \(\mathcal{L}_{t}(\breve{\theta})\leq\mathcal{L}_{t}(\theta^{*})\), therefore we have \(\mathcal{L}_{t}(\breve{\theta})-\mathcal{L}_{t}(\widehat{\theta})\leq\mathcal{ L}_{t}(\theta^{*})-\mathcal{L}_{t}(\widehat{\theta})\leq 2R^{3}S\gamma\) Thus, we have,

\[\left\lVert\breve{\theta}-\theta^{*}\right\rVert_{\mathbf{H}^{*}}^{2}\leq(2+c )\left(4R^{3}S\gamma+\gamma\left\lVert\breve{\theta}-\theta^{*}\right\rVert_ {\mathbf{H}^{*}}\right)\]

Using the inequality that for some \(x^{2}\leq bx+c\implies x\leq b+\sqrt{c}\), we have,

\[\left\lVert\breve{\theta}-\theta^{*}\right\rVert_{\mathbf{H}^{*}} \leq(2+c)\gamma+\sqrt{(2+c)4R^{3}S\gamma}\] \[=\mathbf{c}\sqrt{(2+c)R^{3}S}\gamma\]

### NeurIPS Paper Checklist

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers.**

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See Section 1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Conclusion (Section 6) Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All details are stated in Section 2 and the proofs are given in details in the Appendices A, B, C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Detailed discussion is done in Section 5 and Appendix D. Code can be accessed at https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Full code with documentation is provided at https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Execution Time plots have errorbars, but Regret plots do not as regret plots were observed to get cluttered although the variation was not significant. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: There are no potential harms caused by the research process as it is mainly theoretical and the experiments are all in simulation. There might be Societal Impact of the algorithms developed here when applied in practical decision-making settings. The study of this beyond the scope of current work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No]Justification: The problem studied is of a theoretical interest and we do not foresee any negative societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The data and models are only toy data/fully simulated and therefore of no real impact. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All credits regarding code are given in the README.md file at https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: All details are provided in the README.md of our codebase https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work is mainly theoretical with only simple simulated experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve human participants. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.