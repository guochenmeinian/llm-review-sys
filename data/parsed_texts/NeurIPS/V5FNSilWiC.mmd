# Strategic Behavior in Two-sided Matching Markets with Recommendation-enhanced Preference-formation

 Stefania Ionescu

Department of Informatics

University of Zurich

Andreasstr. 15, Zurich, 8050

ionescu@ifi.uzh.ch

&Yuhao Du

Department of CS and Engineering

University at Buffalo

Buffalo NY, 14260

yuhaodu@buffalo.edu

&Kenneth Joseph

Department of CS and Engineering

University at Buffalo

Buffalo NY, 14260

kjoseph@buffalo.edu

&Aniko Hannak

Department of Informatics

University of Zurich

Andreasstr. 15, Zurich, 8050

hannak@ifi.uzh.ch

###### Abstract

Two-sided matching markets have long existed to pair agents in the absence of regulated exchanges. A common example is school choice, where a matching mechanism uses student and school preferences to assign students to schools. In such settings, forming preferences is both difficult and critical. Prior work has suggested various prediction mechanisms that help agents make decisions about their preferences. Although often deployed together, these matching and prediction mechanisms are almost always analyzed separately. The present work shows that at the intersection of the two lies a previously unexplored type of strategic behavior: agents returning to the market (e.g., schools) can attack future predictions by interacting short-term non-optimally with their matches. Here, we first introduce this type of strategic behavior, which we call an adversarial interaction attack. Next, we construct a formal economic model that captures the feedback loop between prediction mechanisms designed to assist agents and the matching mechanism used to pair them. Finally, in a simplified setting, we prove that returning agents can benefit from using adversarial interaction attacks and gain progressively more as the trust in and accuracy of predictions increases. We also show that this attack increases inequality in the student population.

## 1 Introduction

In _two-sided matching markets_, agents are partitioned in two disjoint sets (e.g., students and schools) and want to get paired with agents from the other set for future bilateral exchanges (e.g., a student learns at a school and the school provides instruction to the student) [26]. In such markets, agents typically report their preferences over potential matches and a _matching mechanism_ uses those preferences to produce an _assignment_, i.e. a pairing between agents. Because forming one's preference is hard, agents benefit from external recommendations.

Since predictive models have become increasingly accessible, reliable, and trusted [17], they also became more frequently used to inform preference-formation in matching markets. One such example is school choice. After the introduction of the No Child Left Behind Act, in 2001, students from low-performing schools were allowed to transfer to better-performing schools. However, initially, only a few students took advantage of this opportunity, partly because it was hard for parents to assess whichschool would improve their child's performance. In 2008, a content-based recommender system called SmartChoice was deployed for focus group participants; its goal was to help parents with the assessment by identifying the best schools based on predictions on the student's development at that school [27]. Another example is in refugee assignment, where refugees are matched with locations. Here, the preferences of locations over refugees are given by a machine learning (ML) model that predicts the level of integration success (e.g., measured by the probability of employment within 90 days) of an individual at a given location [5]. 'The Swiss government has recently implemented a randomized test to examine the performance of data-driven algorithms for outcome-based assignment' [3]. Similar approaches have been developed for other application domains such as labor market [24] and course allocation [15].

These examples show that using predictive models to inform the preferences of agents in matching markets is not a problem of the future, but rather one of the present. However, prior work has evaluated potential vulnerabilities _separately_ in the matching mechanisms [10; 7; 2] and prediction-based recommendations [23; 21]. When considering the matching algorithm, one usually analyzes the incentives of individuals by, for example, asking whether the mechanism is _strategy-proof_ (i.e. whether agents have an incentive to misreport their preferences). Similarly, there is a broad literature on attacks on recommendations [8; 23; 21] and predictive models which might face _poisoning_ or _evasion_ attacks (i.e. attacks which either inject fake data to trigger an unfaithful model or perturb the testing input to trigger a misclassification) [11].

In this paper, we argue that, in addition to considering vulnerabilities of the matching and prediction mechanisms independently, it is critical to also look at vulnerabilities in systems that _combine them together_. Specifically, we consider systems similar to those described above, i.e, where: (a) agents of one side (the _returning side_) come back to the matching market in subsequent rounds (e.g., schools), (b) agents have post-matching objectives (e.g., schools want to be prestigious and minimize their cost), (c) agents on the returning side _have the power to shape the quality of the interaction_ with those they are assigned to via post-matching decisions (e.g, schools can increase the performance of students through extra-curricular preparations or integration programs), and (d) these interactions impact outcomes for the non-returning side, which in turn influence future predictions (e.g., predictions based on the outcomes of past students change the preferences of current students).

By making the feedback loop between the matching and prediction mechanisms explicit, we uncover a new type of strategic behavior: the returning side can attack the system by changing their interactions with their matches. Even though in the short-run it might be beneficial for both matched parties to have the best possible interaction (e.g., both locations and refugees want refugees to be employed as soon as possible), the returning side (e.g., locations) might want to sacrifice their utility in the current round for different future predictions (e.g., locations might postpone hiring difficult-to-integrate refugees so they will not be allocated similar refugees in the future). We call such post-matching strategic behavior--where agents sacrifice their short-term utility to trigger different long-term predictions--_adversarial interaction attacks_. Similarly, we call a system where agents cannot benefit from such attacks _interaction-proof_.

In this paper, we first build a formal model for repeated two-sided matching markets with prediction-enhanced preference formation. Second, we use this model to define _adversarial interaction attacks_ and the optimization problem faced by returning agents. Finally, we create a simplified setting (in the main text, and an agent-based model in the appendix) to analyze when and by how much agents can benefit from interaction-strategic behavior. We show that, for some systems, (a) the returning agents have an incentive to use adversarial interaction attacks, (b) the utility gains obtained through such attacks increase as predictions become more accurate and trusted, (c) once a returning agent attacks, others have an incentive to implement more severe attacks, and (d) the non-returning agents are unevenly negatively affected by such attacks.

## 2 Other Related Work

Adversarial Attacks in Recommendations.From past literature in recommender systems (RS), the most similar adversarial attacks to the one we consider are _shelling attacks_ (i.e., attacks where fake users and ratings are created to trigger different future recommendations). These works mainly focused on crafting adversarial examples [23; 21]. Differently, Christakopoulou and Banerjee [8] analyzed shelling attacks from a machine learning and optimization perspective by building on the literature on poisoning attacks in classification tasks [16], and formulating the problem as a two-player general-sum game between an RS and an Adversarial Attacker.

However, our setting has some key differences. First, the returning agents usually cannot inject fake users (e.g., because there are public records of which students attended which schools). Instead, agents attack by adapting their actions. This distinction has both social implications (the non-returning agents experience a different outcome while the attack takes place) and economic implications (the returning agent implementing the attack sacrifices their short-term utility). Second, the existence of capacity constraints entails a rivalry for available seats (if a student is accepted at a school then another student is not accepted, and vice versa). Consequently, when attacking for being matched to a specific (type of) agent, there are two intents to do so: (a) rank higher in the preferences of agents who are of interest, and _(b) rank lower in the preferences of agents who are not of interest_. The latter is different from classical recommender system applications and is increasingly important when the returning agents cannot fully and freely express their preferences. Third, also because of competition, agents do not decide to whom they are allocated (not all students can get a place to their most preferred school); instead, they report their preferences to a matching market, which produces an assignment. Therefore, attacks in this setting are not only efficient if they change the most preferred option of a non-returning agent, but also if they change the ordering of options lower in the preference ranking.

_Strategic Behavior in Matching Markets._ As noted above, matching markets (MM) are widely used to pair agents based on their reported preferences; in such settings agents can behave strategically by, e.g., misreporting their preferences. Prior work shows that users find and implement such profitable manipulations which leads to congestion and inefficiencies [7]. Thus, strategy-proofness is a common desiderata in real-world application domains; e.g., in the Boston school choice system, this triggered a transition from the Boston to the Deferred Acceptance mechanism [1]. In this paper we analyse strategic-interacting, which differs from strategic-reporting as it is not an attack on the mechanism alone, but rather on its combination with the prediction-based preference-formation process. When proposing predictive modelling for refugee assignment, Bansak et al. [5] distinguished between the modeling phase (when the predictive model is built) and the matching phase (when the assignment is produced). We generalise and extend this framework by also considering the phase when the paired agents interact and new data is produced. Moreover, we build on prior literature in economics when creating the model (for formalising the matching phase [25]) and when analyzing the system (for unilateral deviations and Nash equilibria [22]).

_Using Simulations to Understand Long-Term Effects._ The machine learning community has seen an increasing use of simulation to study the interaction between users and recommendations. RecSim [12] provides an environment that naturally supports sequential interaction with users. Mansoury et al. [19] proposed a method for simulating interactions between users and RSs to study the impact of the resulting feedback loop on the popularity bias amplification. Bountouridis et al. [6] built a framework called SIREN to study how RSs will impact users' news consumption preference in the long term. Similarly, scholarship within MM used simulations to understand the effect of different design choices. Erdil and Ergin [10] developed an experiment in order to compare the performance of two alternative matching algorithms in the school choice setting. In the context of online dating, Ionescu et al. [13] proposed an agent-based model (ABM) to test the effects of different platform interventions in reducing racial homogamy. At the intersection of ML and two-sided markets, Malgonde et al. [18] built an ABM to test the effects of introducing a two-sided RS within a complex adaptive business system. Similar to this previous work, we create a simplified setting to model a complex socio-technical system. We use both theoretical analysis (in the main text) and simulations (in the appendix - to confirm that the results carry to more realistic settings). We, however, have a different goal: to understand how the characteristics of the market affect the incentives to use and effects of using adversarial interaction attacks.

## 3 Problem Formulation

Our model for the system is composed of three phases: (a) modeling, (b) matching, and (c) interacting. The modeling stage builds a predictive model that forecasts the interaction outcome of two agents if matched. During the matching phase, the non-returning agents use this model to inform their preferences 1 ; next, a _matching algorithm_ pairs each agent in the non-returning side of the market (student) with an agent in the returning side of the market. Finally, in the interacting stage, the agents - already paired according to the assignment obtained during matching - interact with each other (students attend the classes at their assigned school). This interaction will produce an outcome (e.g., SAT scores of students) that is kept as a record and used to inform future predictions. The entire system then repeats in a series of rounds with new non-returning agents each time, but with the same returning agents. Figure 1 shows an overview of all the phases and the interactions between them. In the remainder of this section, we formalise each of the stages and present the decision problem faced by the returning agents.

Footnote 1: Alternatively, the predictive model could be used to inform the preferences of the returning agents. For simplicity, we chose not to extend our model to capture this alternative in the main text and only include it in the appendix. Moreover, here we use school choice as a running example; see the appendix for other examples.

### The Matching Stage

We start with the notation for the agents. Let \(X^{t}\) be the set of non-returning agents to be matched at time (round) \(t\), and \(Y\) be the set of returning agents. Moreover, we denote by \(\succ_{x}\) the preference of the non-returning agent \(x\in X^{t}\) over the returning agents, \(Y\). In other words, \(\succ_{x}\) is a ranking over \(Y\). Similarly, \(\succ_{y}\) is the preference of \(y\in Y\) over \(X\). These preferences could be either unweighted (i.e., the agent only knows the ordering of their options) or weighted (i.e., the agent also knows how much more they prefer each potential match over another). In the weighted case, the preference \(\succ_{x}\) has an associated weight function \(w_{x}:Y\rightarrow\mathbb{R}\) mapping each option \(y\) to the strength \(x\) wants to be matched to \(y\).

A matching is a pairing of non-returning agents to returning ones, i.e., \(\mu:X\to Y\cup\{\bot\}\). Here, \(\mu(x)=\bot\) signifies that \(x\) remained unassigned2. More generally, a matching procedure (or allocation rule) maps the preferences of agents to a matching. In other words, a matching procedure \(M\) is a function mapping every \((\{\succ_{x}\}_{x\in X},\{\succ_{y}\}_{y\in Y})\) to a matching \(\mu\). We denote by \(\mu^{t}\) the matching in round \(t\), i.e. \(\mu^{t}=M((\{\succ_{x}\}_{x\in X^{t}},\{\succ_{y}\}_{y\in Y}))\).

Footnote 2: In simplified scenarios where each returning agent can be matched to at most one non-returning agent (e.g., due to capacity constraints), we can express the matching function as a function from \(Y\) to \(X\cup\{\bot\}\). More precisely, \(\mu(y)=x\) when \(x\) is matched with \(y\) and \(\bot\) if \(y\) remains unmatched. Since this refers to the same pairing, we avoid introducing additional notation and will also use \(\mu\) for the mapping of returning to non-returning agents in such simplified cases.

### The Interacting Stage

After being matched, the agents interact. For every agent pair \((x,y)\), there is a set of possible outcomes, depending on their interaction. Since \(x\) is not returning to the market, we assume they will always prefer the best outcome for them. Therefore, the set of possible outcomes depends on the actions of the returning agent, \(y\). We denote the set of outcomes \(y\) could choose from when interacting with \(x\) by \(\mathcal{O}_{y}(x)\).

Depending on the resulting outcome, each agent has some value, cost, and utility. If \(y\) chooses outcome \(o\) when interacting with \(x\) we denote by \(v^{o}_{y}(x)\) their value, by \(c^{o}_{y}(x)\) their cost, and by

Figure 1: Overview of the interaction between different components of a system with a forecasting-based preference formation and a matching procedure. The interaction predictions can be used to inform the preferences of a. the non-returning side of the market (e.g. students), or b. the returning side of the market (e.g. locations).

\(u_{y}^{o}(x)=v_{y}^{o}(x)-c_{y}^{o}(x)\) their utility. We use the analogous notation for \(x\). We also make the simplifying assumptions that (a) the value is symmetric, i.e. \(v_{y}^{o}(x)=v_{x}^{o}(y)\), and (b) the cost of the non-returning side is null, i.e. that \(c_{x}^{o}(y)=0\) and \(u_{x}^{o}(y)=v_{y}^{o}(x)\).

As an example, a school \(y\) might have the choice set \(\mathcal{O}_{y}(x)=\{80,100\}\) since it could either invest in extracurricular preparation for \(x\) (case in which \(x\) scores around \(100\%\) on state-administered tests) or not to invest (and \(x\) scores around \(80\%\)). If school \(y\) incurs a fixed cost of \(5\) when preparing a student, then for the outcome \(80\) both the student and the school have a utility of \(80\) while for the outcome \(100\) the student has a utility of \(100\) while the school subtracts the cost from the value, and thus has a utility of \(100-5=95\).

### The Modeling Stage

The prediction model at time \(t\) is informed by the history of interactions until that time. We denote the history up to time \(t\) by \(H^{t}=\{(x,y,o,t^{\prime})|y=\mu^{t^{\prime}}(x),y\text{ interacted with }x\text{ at time }t^{\prime}\leq t\text{ with outcome }o\in\mathcal{O}_{y}(x)\}\). The history thus records the agents matched so far, together with the time and outcome of their interaction.

The prediction algorithm uses the history as an input. Its output is a _hypothesis_, i.e., a function which maps a pair of agents to their expected interaction outcome. We use the usual statistical framework in learning theory and denote the hypothesis by a parameterized function \(h_{\theta_{t}}:X\times Y\rightarrow\mathbb{R}\). The prediction is thus based on the parameter \(\theta_{t}\) at time \(t\), which is obtained by solving the optimization problem \(\theta_{t}=\arg\min_{B}L(\theta,H^{t-1})\). Here, \(L\) is the loss function.

The hypothesis predicts values for a potential interaction between a non-returning agent, \(x\), and all the returning agents. Therefore, it induces a preference weight function and a ranking over the returning agents. Formally, \(h_{\theta_{t}}(x,\cdot):Y\rightarrow\mathbb{R}\) gives a weighted preference over \(Y\), which we denote by \(\succ_{x}^{h}\). By exposing agent \(x\) to this ranking, their final preference might change. To capture this change, each agent \(x\) has a _prediction integration function_, \(h_{\theta_{t}}^{x}\), that transforms a (weighted) preference of \(x\) into a new preference, \(\succ_{x}^{\prime}\), using the hypothesis-based ranking, \(\succ_{x}^{h}\).

To continue the previous example, let us assume that student \(x\) initially believes schools \(A\) and \(B\) are equally good for them, i.e. \(\succ_{x}\) is \(A\sim B\) with, say, a weight function \(w_{x}(A)=w_{x}(B)=80\). Assume the hypothesis predicts \(h_{\theta_{t}}(x,A)=100\) and \(h_{\theta_{t}}(x,B)=80\), which corresponds to the ranking \(A\succ_{x}^{h}B\). The exposure of student \(x\) to this prediction-based ranking changes their preference to a new one which lies between their original opinion, \(\succ_{x}\), and the suggested one, \(\succ_{x}^{h}\). For instance, the new preference could correspond to the weight profile obtained by averaging the original and the suggested ones. Then, the new preference, \(h_{\theta_{t}}^{x}(\succ_{x})=\succ_{x}^{\prime}\), has the weight function \(w_{x}^{\prime}(A)=90\) and \(w_{x}^{\prime}(B)=80\) thus ranking \(A\succ_{x}^{\prime}B\).

### The Decision Problem of Returning Agents

The decision problem faced by the returning agents when they choose how to interact with their matches can be formalised as a dynamic programming task over the infinite horizon. They choose the action that maximizes the sum of their utility now plus the expected discounted utility in the future if they take that action. Using the notation introduced in the previous section, the maximum expected-discounted utility of a returning agent, \(y\), from time \(t\) onward is 3

Footnote 3: To achieve this succinct problem formulation, we made some simplifying assumptions and notations (details in Subsection 2.3 from the appendix).

\[U_{y}\left(\mu^{t}\right)=\max_{o\in\mathcal{O}_{y}(\mu^{t}(y))}\left(u_{y}^{ o}\left(\mu^{t}(y)\right)+\beta\mathbb{E}\left[U_{y}\left(\mu^{t+1} \right)|(\mu^{t}(y),y,o,t)\in H^{t}\right]\right),\] (1)

where we make the standard assumption in economics that agents steeply discount future utility and denote the discount factor by \(\beta\). The _optimal strategy_ of a returning agent is to take, at every round, the interaction leading to the outcome that maximizes the expected-discounted utility.

The straightforward interaction strategy is to always choose the outcome maximising the 1-step utility, i.e. choose \(\arg\max_{o\in\mathcal{O}_{y}(\mu^{t}(y))}\left(u_{y}^{o}\left(\mu^{t}(y) \right)\right)\). When a returning agent uses this strategy, we say it _interacts truthfully_. Naturally, in general, the optimal strategy needs not be the truthful one. The resulting gap leaves space for strategic interactions. We refer to strategies involving non-truthfulinteractions as _adversarial interaction attacks_ and call the systems in which agents cannot benefit from such attacks _interaction-proof_.

To give two simple examples, a system in which the preference formation is solely based on the attributes of schools (e.g., how far away the school is, what subjects are taught, what are the final examinations) and past data on interactions is not used would be interaction-proof. Similarly, a system that only uses the history of interactions before announcing the decision to introduce the ML-algorithm would also be interaction-proof. This is because, in both of these cases, the future expected utility is constant with respect to the choice of interaction. Thus, the utility from now onward is maximized by choosing the outcome giving the highest utility in the current round.

## 4 Simplified Setting

While useful for formalizing the definition of adversarial interaction attacks within a variety of contexts, the problem formulation proposed in the previous section is too general to allow for a tractable and insightful numerical analysis. Thus, we operationalize the framework by creating a simplified model. The main goal of such a model is to support both proving that returning agents can benefit from adversarial interaction attacks and understanding why and when this could be the case. The remainder of this section introduces the simplified model. To improve readability, we refer to returning agents as schools and non-returning agents as students. However, the model could be read through the lens of alternative applications (e.g., locations and refugees).

_Interaction._ We assume that there are two schools, \(Y=\{A,B\}\), and two types of students: cheap, \(C\), and expensive, \(E\). Moreover, each school has the capacity to accept only one student. Any interaction between a school and a student leads to an outcome between zero and one (i.e., \(\mathcal{O}_{y}(x)=[0,1]\ \forall y\in Y,x\in\{C,E\}\)). This outcome matches its value: \(v^{o}_{y}(x)=o\). However, schools have different costs for a given outcome depending on the type of student. More precisely, cheap students require negligible specialized assistance, thus having a zero cost for any outcome, i.e. \(c^{o}_{y}(C)=0\), while expensive students require a cost of three quarters of the value of the chosen outcome, i.e. \(c^{o}_{y}(E)=3/4\cdot o\). Thus, the resulting utility for a school \(y\in Y\) is given by \(u^{o}_{y}(C)=o\) and \(u^{o}_{y}(E)=o/4\). Note that this always leads to non-negative utilities. To account for cases when returning agents incur large integration costs from accepting any match (e.g., for refugee assignment or the allocation of under-performing students [27; 5]) we also analyze the scenario of negative utilities. We do so by offsetting the cost by a constant \(q>1\), thus giving \(u^{o}_{y}(C)=o-q\) and \(u^{o}_{y}(E)=o/4-q\). Unless otherwise specified, we consider the non-negative model for utilities (i.e., without the offsets).

_Preferences._ For simplicity, we assume students have a uniform random chance of preferring each school, and predictions are based on the outcome of the most recent interaction of the same-type student at the school. More precisely, a student \(x\) will have a \(1/2\) chance of having the preference profile \(A\succ_{x}B\) and a \(1/2\) chance of having the preference profile \(B\succ_{x}A\). For the same student \(x\), the prediction algorithm gives to each school \(y\) a weight \(w^{h}_{x}(y)\) equal to the last outcome of a same-type student at school \(y\) based on the history of interactions4. For example, a student of type \(E\) will start with a \(50\%\) chance of thinking that \(A\succ_{E}B\); if, according to the history, the most recent student of type \(E\) who attended school \(A\) had a worse outcome than the most recent student of type \(E\) who attended school \(B\) then an accurate prediction-based hypothesis will recommend \(B\succ_{E}^{h}A\).

Footnote 4: In the absence of historical interactions between a type of student and a school we assume the outcome was \(1\). We believe this is the most sensible assumption as prior to introducing a prediction mechanism to inform preferences there is a limited impact of past outcomes on future assignments. As a result, schools would likely optimize the 1-step utility and interact truthfully.

_Accuracy and trust._ The impact of predictions depends on their accuracy and trust, two metrics on which system designers usually try to improve on [20; 9]. To account for this, we model the accuracy via a parameter \(\alpha\), which captures the probability that the recommendation is the one given by the most recent outcome with a same-type student. In the example above, the hypothesis will generate \(B\succ_{E}^{h}A\) with probability \(\alpha\) and the reversed ranking (i.e., \(A\succ_{E}^{h}B\)) with probability \(1-\alpha\). Similarly, the trust of students in the recommendations is given by \(\theta\), which captures the probability that students adopt the recommended ranking as opposed to maintaining their original preference over the alternatives (i.e., \(\succ_{x}^{\prime}\) is \(\succ_{x}^{h}\) with probability \(\theta\) and \(\succ_{x}\) with probability \(1-\theta\)). The accuracy and trust are just extensions to help understand the changes in incentives to attack as the power of our prediction models improves. Since the final goal of predictions is to be accurate and trustworthy, for most of the analysis we will assume both \(\alpha\) and \(\theta\) are \(1\).

_Matching._ Matching mechanisms are complex procedures that, based on the preferences of all agents, assign students to schools. Serial dictatorship, Boston, and deferred acceptance are all examples of such mechanisms which were deployed in the school choice setting (see the appendix for the definition of each). While, in theory, schools have the chance of expressing their preferences, in practice, these preferences are often given by a lottery with some priority ordering (e.g. students with siblings at the same school have a higher priority at that school) [1]. Thus, the true preferences of schools usually have a limited impact on the final allocation. For the theoretical analysis, we consider random serial dictatorship (RSD), which (1) picks a random ordering of students and (2) assigns, in turn (according to this ordering), each student to the most preferred school that still has available places. In the appendix, we use simulations to investigate the effects of using alternative mechanisms.

_Analysis._ To see whether schools have an incentive to interact strategically, we compare the utility of one school, \(y\), under two scenarios: (a) when both schools interact truthfully (i.e., pick outcome \(1\) for their interaction with all students) and (b) when \(y\) interacts strategically at some level \(l\) (i.e., picks outcome \(1-l\) where \(l\in(0,1]\) for its interaction with expensive students) while the other school interacts truthfully. If the expected utility of \(y\) increases when it unilaterally interacts strategically, then \(y\) has an incentive to deviate from interacting truthfully and adopt such a strategy. Using a game-theoretic framework, we say that schools are agents which can choose actions corresponding to _adversarial attacks at levels \(l\in\{0,1/L,2/L,\ldots,1\}\)5_: an attack at level \(l\) means that the school always chooses the outcome \(1-l\) when interacting with expensive students and the outcome \(1\) with cheap students. Thus, \(l=0\) represents a truthful interaction. With respect to this set of actions, each school has a _best response_ (i.e., an action that gives it the highest utility) given the actions of others. If both schools play the best response to the other's actions and, thus, none of them can change their level of attack to achieve a higher expected utility, we say the action profile is a _Nash Equilibrium_.

Footnote 5: Here, \(L\) is an arbitrary large natural number. This makes the action space discrete, thus capturing the often-used assumption that agents do not have infinitesimal control over the outcomes. Instead, there is an atomic unit of change (\(1/L\) in our case) they can make.

## 5 Results

The results section starts with three subsections, each titled based on its main takeaway. The first two focus on results under the assumption that prediction models have perfect levels of trust and accuracy, while the third subsection will investigate incentives to attack as those levels change. Since proofs are mostly mathematical formalizations of intuitive behavior, we reserve them for the appendix and solely provide the intuition within the main text. To position the results within the broader context, each of these three subsections ends with a paragraph discussing the implications of its results. We also include a final subsection explaining why we opted for a simple model and analysis and how it could be extended.

### Not all systems are interaction-proof

We first consider a scenario where, at each timestep, there is one student of each type (i.e, \(X^{t}=\{C,E\}\)) and schools put sufficient weight on the future expected utility (i.e., \(\beta\) is sufficiently large). In such a case, Theorem 1 states that schools can benefit from implementing an adversarial interaction attack. The proof (see appendix) compares the expected utilities under truthful and strategic interactions and shows that an attack at any level \(l\neq 0\) produces an increase in expected discounted utility when \(\beta>2/3\).

**Theorem 1**: _Assume \(X^{t}=\{C,E\}\), schools are sufficiently forward-looking (\(\beta>2/3\)), and predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). Then, schools have an incentive to deviate from truthfully interacting and implement adversarial interaction attacks._

It is important to note that this takeaway is heavily based on the composition of the market (i.e., the number and type of students available). For example, Theorem 2 shows that when there are only expensive students, a strategy profile where all schools interact truthfully is a Nash Equilibrium. Similarly, Theorem 3 shows that, even when students induce a negative utility for the schools, as long as they are of the same type and sufficiently many to guarantee that each school will be allocated a student, then schools still do not have an incentive to implement adversarial interaction attacks. The full proofs are included in the appendix. They formalize the intuition that, in such scenarios, schools either (a) prefer being allocated a student to remaining unmatched or (b) have the same type of allocated student disregarding their interaction outcomes.

**Theorem 2**: _Assume predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). When there are only expensive students, a strategy profile where both schools interact truthfully is a Nash Equilibrium._

**Theorem 3**: _Assume predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). When all students induce the same negative utility for schools, and there are at least two students, a strategy profile where both schools interact truthfully is a Nash Equilibrium._

Yet, there are also scenarios when forward-looking schools have an incentive to interact strategically with all available students, disregarding their type. One such instance is when there is one student giving schools a negative utility for any choice of action. This example is covered by Theorem 4. The main intuition behind the proof is that the attacking school has the opportunity to avoid being allocated any student, which gives it a better utility than being allocated one. If the weight given to future gains (i.e., \(\beta\)) is sufficiently large, then schools prefer to decrease their utility in the current round in order to maximize the chance of having a null utility in subsequent rounds.

**Theorem 4**: _Assume predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). If there is only one student giving schools a negative utility for any outcome and schools are sufficiently forward-looking, then schools have an incentive to deviate from truthfully interacting and implement adversarial interaction attacks._

These toy examples prove that it is not sufficient to analyze the matching mechanism in isolation from the preference-formation process. In particular, Roth [25] proved that RSD is a strategy-proof mechanism for both sides. However, our theorems show that a system containing the same mechanism is not necessarily interaction-proof, thus challenging the desirability of implementing such a mechanism.

Once a school implements an attack, the other school has an incentive to respond with more severe attacks

The prior analysis showed that, in some setups, schools have an incentive to interact strategically. It remains, however, unclear how a school would best respond to an adversarial attack of the other school. Theorem 5 states the best response of a school is to attack at the smallest level that is still higher than the current level of attack of the other school. The proof (see appendix) formalizes the intuition that schools want an attack that sacrifices their short-term utility the least, while still triggering the prediction mechanism not to recommend them to expensive students.

**Theorem 5**: _Assume predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). When \(X^{t}=\{C,E\}\) and schools are sufficiently forward-looking (\(\beta>4/5\)), the best response of school \(A\) for a strategy \(B\) of attacking at a level \(l\) is to attack at a level \(\min\{1,l+1/L\}\)._

This has a few important implications. First, it suggests that schools would likely not attack at high levels directly, thus having a limited impact on the welfare of expensive students. However, little by little, schools would attack at increasingly higher levels until both schools choose an outcome of \(0\) (i.e., level of attack \(1\)) for expensive students. Corollary 1, which is an immediate consequence of Theorem 5, states that the resulting strategy profile when both schools attack at a maximum level is a Nash Equilibrium. This equilibrium is, however, undesirable for both schools, as it is Pareto dominated by the initial choice of actions; i.e., if both schools interact truthfully, then both of their expected utilities are higher than when the schools attack at the maximum level.

**Corollary 1**: _Assume predictions have perfect trust and accuracy (i.e., \(\alpha=\theta=1\)). When \(X^{t}=\{C,E\}\) and schools are sufficiently forward-looking, the strategy profile where both schools use adversarial interaction attacks at the highest level (i.e., \(l=1\)) is a Nash Equilibrium. However, this is Pareto dominated by the strategy profile where all schools interact truthfully._Altogether, these results show the system resulting from introducing prediction-based recommendations to inform preferences within a matching market could encourage returning agents to engage in higher and higher levels of adversarial interaction attacks. This behavior ultimately leads to an equilibrium that is detrimental to all agents. For our example, it lowered the welfare (i.e., the average value of outcomes) of expensive students and reduced the expected discounted utilities of both schools. At a societal level, such a system could thus result in increased inequalities among the student population. Therefore, there is a need for a system with prediction and matching mechanisms that together lead to different interaction incentives for returning agents. As illustrated by the next two subsections, two potential solutions are to lower the impact of outcome-based predictions or use matching mechanisms that let schools express their true preferences.

Increases in trust in and accuracy of recommendations lead to higher discounted expected utilities under adversarial interaction attacks

Finally, we acknowledge the general trend of increasing the accuracy of and trust in prediction [20; 9]. Before incorporating prediction mechanisms, prior outcomes have a limited impact on preferences (e.g., from anecdotal experiences of isolated past students, or advice from counselors [27]). When predictions are made algorithmically, there is a higher potential for personalizing them; however, due to the high level of individuality, algorithms struggle to produce accurate predictions. This affects both the trust of students in recommendations and leads to a challenge for designers to improve accuracy. Theorem 6 shows that improving these parameters affects the impact of adversarial interaction attacks. The proof is based on the following observation: the expected gain in utility from using a strategic interaction is strictly increasing with both trust and accuracy.

**Theorem 6**: _When \(X^{t}=\{C,E\}\), the expected utility of a school using an adversarial interaction attack is a continuous function that increases monotonously in both accuracy (\(\alpha\)) and trust (\(\theta\))._

Naturally, when predictions are disregarded by students (i.e., \(\theta=0\)) or are random (i.e., \(\alpha=0.5\)) it is best for schools to interact truthfully - see Theorem 7. When increasing the accuracy and trust, the utility under attacks increases (by Theorem 6) until adversarial attacks constitute a best response (as shown by Theorem 1). This means that, depending on the specifics of the system, such as the parameter \(\beta\), for any given level of trust, there exists some maximum value of \(\alpha\) where truthful interaction is still the optimal strategy. The proof of this statement can be formalized using the intermediate value theorem and the three aforementioned results.

**Theorem 7**: _When \(X^{t}=\{C,E\}\), the truthful strategy profile is a Nash Equilibrium under predictions with 50% accuracy (\(\alpha=0.5\)) or null levels of trust (\(\theta=0\))._

In short, the above analysis shows that, even if adversarial interaction attacks do not currently increase the expected utility, they might if we introduce prediction mechanisms or improve their trust and accuracy. This underlines the importance of re-evaluating incentives when considering such changes. Moreover, it shows that limiting the impact of outcome-based predictions could, in turn, limit the gains of returning agents from interacting strategically.

### Discussion

Our paper (1) created an economic model to allow for a general definition of adversarial interaction attacks and (2) simplified the setting to prove these attacks could indeed be attractive for returning agents. We underline that our goal was to illustrate the emerging feedback loop and show the importance of considering it when prediction algorithms inform preferences. Thus, we favored a simple model together with a basic theoretical analysis. As such, the current analysis can be extended in various ways, such as by using more realistic prediction and matching mechanisms (MM), accounting for more diverse agents, defining and analyzing incentives for undetectable interaction attacks, and extending the space of strategies.

Perhaps the main question for future work is whether adversarial interaction attacks remain attractive in more realistic settings. As a first step to address this question, we extended the simple setting presented above to a more realistic agent-based model. In particular, this model uses k-Nearest Neighbors [14; 4] to predict future outcomes, compares multiple MM (including Deferred Acceptance and Boston), and increases the number of agents. The simulation results both confirm the takeawaysfrom the theoretical analysis and provide some additional insight. First, they show that different MM are more susceptible to adversarial interaction attacks depending on the distribution of students with respect to their attributes. Second, they suggest that allowing schools to express their true preferences over students (instead of using lotteries) can, in some cases, eliminate the incentives of using adversarial attacks. The exception is when utilities are negative and there are fewer students than places at schools. Third, it illustrates the inequality between students: as suggested by the theoretical results, the welfare of students decreases when schools interact strategically, and this decrease is mostly borne by the expensive students. We provide details on the agent-based model and visualizations for the simulation results within the appendix; to ease reproducibility, the code is available on GitHub6

Footnote 6: GitHub link: https://github.com/Stefanial/Predictions-MM.

## 6 Conclusion

Predictive models are increasingly used to inform preference-formation in matching markets (MM). However, the robustness of the prediction mechanism under adversarial attacks and of MMs under strategic behavior are usually investigated separately. In the present work, we extend existing models by including the interacting stage in which agents, matched by the MM, interact with each other, thus generating new training data. Doing so makes the feedback loop between the prediction model and the MM explicit and uncovers a new type of strategic behavior: the agents that return to the market in subsequent rounds can deviate from the most profitable interactions in the current round in order to attack the predictions and matchings of future rounds.

In a simplified setting, we prove that returning agents could indeed have an incentive to interact strategically. Doing so is _more effective_ as predictions get _better_ in terms of accuracy (and trust). In alignment with previous work on recommendations [20], this suggests we should look beyond accuracy when designing prediction mechanisms for such systems. Moreover, we find that, when returning agents choose to adopt this attack, it has perverse consequences for the utility of all agents. In particular, it both lowers the overall utility _and_ increases inequality among non-returning agents (students). These issues reflect how strategic behaviors of social institutions (schools) cause social inequalities independent of individual potential. Altogether, this work indicates that both aspects of the matching and prediction mechanisms are key in developing robust systems and sets the framework for a dialog between the ML and MM communities.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Vlad Gavrila, Atri Rudra, Nicolo Pagan, and the anonymous reviewers for their valuable feedback. KJ and YD gratefully acknowledge the support by the NSF award IIS1939579, with partial support from Amazon.

## References

* [1]A. Abdulkadiroglu, P. A. Pathak, A. E. Roth, and T. Sonmez (2006) Changing the Boston school choice mechanism: strategy-proofness as equal access. Mimeographed. Harvard University. Cited by: SS1.
* [2]A. Abdulkadiroglu and T. Sonmez (2003) School choice: a mechanism design approach. American economic review93, pp. 729-747. Cited by: SS1.
* [3]A. Acharya, K. Bansak, and J. Hainmueller (2019) Combining Outcome-Based and Preference-Based Matching: the g-constrained priority mechanism. arXiv preprint arXiv:1902.07355. Cited by: SS1.
* [4]R. Asif, A. Merceron, and M. K. Pathan (2014) Predicting student academic performance at degree level: a case study. International Journal of Intelligent Systems and Applications7 (1), pp. 49. Cited by: SS1.
* [5]K. Bansak, J. Ferwerda, J. Hainmueller, A. Dillon, D. Hangartner, D. Lawrence, and J. Weinstein (2018) Improving refugee integration through data-driven algorithmic assignment. Science359 (6373), pp. 325-329. Cited by: SS1.

* Bountouridis et al. (2019) Dimitrios Bountouridis, Jaron Harambam, Mykola Makhortykh, Monica Marrero, Nava Tintarev, and Claudia Hauff. 2019. SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_ (Atlanta, GA, USA) _(FAT* '19)_. 150-159.
* Budish and Cantillon (2012) Eric Budish and Estelle Cantillon. 2012. The multi-unit assignment problem: Theory and evidence from course allocation at Harvard. _American Economic Review_ 102, 5 (2012), 2237-71.
* Christakopoulou and Banerjee (2019) Konstantina Christakopoulou and Arindam Banerjee. 2019. Adversarial attacks on an oblivious recommender. In _Proceedings of the 13th ACM Conference on Recommender Systems_. 322-330.
* Dietvorst et al. (2018) Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2018. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. _Management Science_ 64, 3 (2018), 1155-1170.
* Erdil and Ergin (2008) Aytek Erdil and Haluk Ergin. 2008. What's the matter with tie-breaking? Improving efficiency in school choice. _American Economic Review_ 98, 3 (2008), 669-89.
* Huang et al. (2011) Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. 2011. Adversarial machine learning. In _Proceedings of the 4th ACM workshop on Security and artificial intelligence_. 43-58.
* le et al. (2019) Eugene le, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG]
* Ionescu et al. (2021) Stefania Ionescu, Aniko Hannak, and Kenneth Joseph. 2021. An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_. 412-423.
* Kabakchieva et al. (2010) Dorina Kabakchieva, Kamelia Stefanova, and Valentin Kisimov. 2010. Analyzing university data for determining student profiles and predicting performance. In _Educational Data Mining 2011_.
* Kurniadi et al. (2019) D Kurniadi, E Abdurchman, HLHS Warnars, and W Suparta. 2019. A proposed framework in an intelligent recommender system for the college student. In _Journal of Physics: Conference Series_, Vol. 1402. IOP Publishing, 066100.
* Li et al. (2016) Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poisoning attacks on factorization-based collaborative filtering. _arXiv preprint arXiv:1608.08182_ (2016).
* Logg et al. (2019) Jennifer M Logg, Julia A Minson, and Don A Moore. 2019. Algorithm appreciation: People prefer algorithmic to human judgment. _Organizational Behavior and Human Decision Processes_ 151 (2019), 90-103.
* Malgonde et al. (2020) Onkar Malgonde, He Zhang, Balaji Padmanabhan, and Moez Limayem. 2020. TAMIING COMPLEXITY IN SEARCH MATCHING: TWO-SIDED RECOMMENDER SYSTEMS ON DIGITAL PLATFORMS. _Mis Quarterly_ 44, 1 (2020).
* Mansoury et al. (2020) Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2020. Feedback Loop and Bias Amplification in Recommender Systems. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_ (Virtual Event, Ireland) _(CIKM '20)_. Association for Computing Machinery, New York, NY, USA, 2145-2148.
* McNee et al. (2006) Sean M McNee, John Riedl, and Joseph A Konstan. 2006. Being accurate is not enough: how accuracy metrics have hurt recommender systems. In _CHI'06 extended abstracts on Human factors in computing systems_. 1097-1101.
* Mobasher et al. (2007) Bamshad Mobasher, Robin Burke, Runa Bhaumik, and Chad Williams. 2007. Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness. _ACM Transactions on Internet Technology (TOIT)_ 7, 4 (2007), 23-es.
* Myerson (2013) Roger B Myerson. 2013. _Game theory_. Harvard university press.
* O'Mahony et al. (2004) Michael O'Mahony, Neil Hurley, Nicholas Kushmerick, and Guenole Silvestre. 2004. Collaborative recommendation: A robustness analysis. _ACM Transactions on Internet Technology (TOIT)_ 4, 4 (2004), 344-377.

* [24] Ioannis Paparrizos, B Barla Cambazoglu, and Aristides Gionis. 2011. Machine learned job recommendation. In _Proceedings of the fifth ACM Conference on Recommender Systems_. 325-328.
* [25] Alvin E Roth. 1982. The economics of matching: Stability and incentives. _Mathematics of operations research_ 7, 4 (1982), 617-628.
* [26] Alvin E Roth and Marilda Sotomayor. 1992. Two-sided matching. _Handbook of game theory with economic applications_ 1 (1992), 485-541.
* [27] David C Wilson, Suzanne Leland, Kenneth Godwin, Andrew Baxter, Ashley Levy, Jamie Smart, Nadia Najjar, and Jayakrishnan Andaparambil. 2009. SmartChoice: An online recommender system to support low-income families in public school choice. _AI Magazine_ 30, 2 (2009), 46-46.