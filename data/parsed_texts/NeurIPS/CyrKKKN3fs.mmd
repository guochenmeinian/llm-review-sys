# FairMedFM: Fairness Benchmarking

for Medical Imaging Foundation Models

 Ruinan Jin\({}^{1,4}\), Zikang Xu\({}^{2}\), Yuan Zhong\({}^{3}\), Qingsong Yao\({}^{2}\), Qi Dou\({}^{3}\), S. Kevin Zhou\({}^{2}\), Xiaoxiao Li\({}^{1,4}\)

\({}^{1}\)The University of British Columbia

\({}^{2}\)University of Science and Technology of China

\({}^{3}\)The Chinese University of Hong Kong

\({}^{4}\)Vector Institute

###### Abstract

The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks - classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Checkout FairMedFM's project page and open-sourced codebase, which supports extendible functionalities and applications as well as inclusive for studies on FMs in medical imaging over the long term.

## 1 Introduction

Foundation Model (FM) facilitated medical image analysis is playing a pivotal role in healthcare [2, 3]. These models, which leverage large-scale pretraining and fine-tuning [6], have demonstrated remarkable capabilities in various medical imaging tasks, including classification [69, 41] and segmentation [55, 39]. As the use of FMs proliferates in medical imaging, addressing the challenges of evaluating and ensuring their fairness and utility becomes increasingly critical [28, 24, 77], where biases in model performance can result in significant disparities in patient care and outcomes.

Creating benchmarks for algorithm fairness in medical imaging can lead to consistent experiment settings and ensure standardization. There are efforts to benchmark fairness algorithms in non-FM-based traditional machine learning for medical imaging [77, 50, 23, 76, 13, 72]. However, the fairness

[MISSING_PAGE_FAIL:2]

2. With FairMedFM, we conducted a thorough analysis from various perspectives, where we found that (1) Bias is prevalent in using FMs for medical imaging tasks, and the fairness-utility trade-off in these tasks is influenced not only by the choice of FMs but also by how they are used; (2) There is significant dataset-aware disparities between SA groups in most FMs; (3) Consistent disparities in SA occur across various FMs on the same dataset; and (4) Existing bias mitigation strategies do not demonstrate strong effectiveness in FM parameter-efficient fine-tuning scenarios.
3. We open-source FairMedFM, an extensible implementation for launching the FMs for medical image analysis, to prompt the study of FMs for medical imaging and fairness evaluation in the community.

The scope of our work is to establish a more comprehensive benchmark for medical imaging, focusing on classification and segmentation tasks, binarized SAs and commonly used FM strategies from the literature. Our objective is to raise awareness of fairness issues within the medical imaging community and assist in developing fair algorithms in the machine learning community, by promoting more accessible and reproducible methods for fairness evaluation.

## 2 Preliminaries on Foundation Models, Medical Imaging, and Fairness

### FMs in Medical Imaging

FMs have recently garnered widespread interest due to their powerful generalization capabilities. These large models are designed to learn from large-scale unsupervised data. In medical applications, FMs are particularly valuable because massive amounts of unlabeled medical data are easier to obtain than labeled data, which requires costly expert annotations. Typically, FMs are pre-trained on broad datasets to acquire medical knowledge using two primary training objectives: recovering masked words or vision patches [66], and aligning features of paired text and images through contrastive learning [49]. Their pre-trained representations can be successfully applied to various downstream medical tasks with minimal or no reliance on expert labels. In this paper, we focus primarily on classification and segmentation tasks.

Figure 1: Overview of the FairMedFM framework, a standardized pipeline to investigate fairness on diverse datasets (2D, 2.5D, and 3D), comprehensive functionalities (various FMs, tasks, usages, and debias algorithms), thorough evaluation metrics. The details are explained in Sec. 3.

**Classification FMs in Medical Imaging.** Classificaiton FMs vary in architecture, which can be roughly categorized into two groups. The first one is _vision models (VMs)_, which takes images as input and learns a general representation across diverse datasets by different self-supervised proxy tasks. MedMAE [66] and MedLVM [43] are trained by masked imaging modeling [17] and graph matching, respectively; DINov2 [45], MoCo-CXR [59], and C2L [75] decrease the feature disparities of the paired and augmented vision patches. Another group is _vision-language models (VLMs)_ (e.g., CLIP [16], MedCLIP [65], PubMedCLIP [15], BiomedCLIP [74]), which are trained to attract the feature of paired text and images, while BLIP [33] design a \(q\)-former for vision-text alignment.

In classification, we follow the common workflow [8] to consider \(D=(\mathcal{X},\mathcal{Y},\mathcal{A})\) to be a set of distributions where we have input \(x\) in space \(\mathcal{X}\), the disease label \(y\in\{0,1\}^{C}\) in space \(\mathcal{Y}\), and sensitive attributes \(a\) in space \(\mathcal{A}\). Let \(f_{v}(\cdot):\mathcal{X}\to v\in\mathbb{R}^{k}\) denote the vision encoder of a foundation model which embeds the inputs into feature \(v\) with dimension \(k\).

**Segmentation FMs in Medical Imaging.** Following the trend of the large model, the Segment Anything Model (SAM) [30] shows great potential in segmentation task, which is trained on 1B labeled natural data and offers the zero-shot ability to generate segmentations mask only based on point or box prompts as input. In terms of both data structure and context, medical images exhibit significant differences from natural images. First, medical images vary in modalities, including 2D grayscale images (X-ray), 2D RGB images (dermatology), and 3D volumes (CT). Especially for 3D images, MedSAM [38] processes 3D volumes with slice-wise operations, while SAM-Med3D [63] is a 3D model trained on massive labeled 3D medical volumes. Furthermore, considering the domain gap between medical and natural images, fine-tuning is necessary for applying Segmentation FMs (SegFMs) (pre-trained on natural images like SAM [30]) in medical to learn medical context. Similar to classification, we consider \(D=(\mathcal{X},\mathcal{S},\mathcal{A})\), where we have the segmentation mask \(s\in\{\mathbb{R}^{w\times h\times d}\}^{C}\) in label space \(\mathcal{S}\).

### Fairness in Medical Imaging

**Defintion of Fairness.** Fairness, a critical aspect of AI ethics, urging that deep learning models should not have skewed outcomes towards personalities with diverse demographics, has been widely studied in computer vision [26] and natural language processing [35]. Among various fairness definitions, _group fairness_ is the most common one, which ensures that the model's performance is consistent across different groups. Let \(Y,\hat{Y}\) be the ground truth label and prediction of the model, respectively, and \(A\in\{0,1\}\) be the SA. One of the group fairness metrics, Accuracy Parity, is defined as \(P(\hat{Y}=Y|A=0)=P(\hat{Y}=Y|A=1)\).

**Fairness in Medical Imaging.** Fairness issues _do_ exist in deep learning-based medical image analysis, especially for the marginalized populations. For example, Seyyed-Kalantari _et al_. [52] find that their Chest X-ray classifier has a higher underdiagnosis rate for Hispanic female patients. Similar phenomenons also occur in other medical tasks, including regression [47], segmentation [48], reconstruction [11], etc.

### FMs Meet Fairness in Medical Imaging

Previous studies have shown that unfairness can be induced to FMs from the pre-training datasets, the fine-tuning process, the application of downstream tasks [35]. However, most of the current studies on fairness in medical imaging mainly focus on the traditional models, while evaluating and mitigating unfairness in FMs is still in its infancy. Khan _et al_. [28] benchmarked six classification FMs on sex and race. However, the fairness metrics and datasets involved in their study are limited. Therefore, extra efforts is required to evaluate whether these FMs in medical imaging have fair outcomes before applying them in real clinical scenarios.

## 3 FairMedFM

Fig. 1 presents the pipeline of FairMedFM framework, which offers an easy-to-use codebase for benchmarking the fairness of FMs in medical imaging. FairMedFM contains 17 datasets (9 for classification and 8 for segmentation) and 20 FMs (11 for classification and 9 for segmentation). It also integrates 9 fairness metrics (5 for classification and 4 for segmentation) and 6 unfairnessmitigation algorithms (3 for classification and 3 for segmentation), trying to provide a relatively comprehensive benchmark for fairness in medical imaging FMs.

### Datasets

The following 17 publicly avaliable datasets are included in FairMedFM to evaluate fairness in FMs in medical imaging: CheXpert [22], MIMIC-CXR [25], HAM10000 (CLS) [61], FairVLMed10k [36], GF3300 [37], PAPILA [31], BRSET [42], HAM10000 (SEG) [61], TUSC [53], FairSeg [60], Montgomery County X-ray [7], KiTS 2023 [18], CANDI [27], IRCADb [58], SPIDER [62]. These datasets vary in the following aspects: (1) _Task type:_ classification and segmentation; (2) _Dimension:_ 2D and 3D; (3) _Modality:_ OCT, X-ray, CT, MRI, Ultrasound, Fundus, OCT, and dermatology; (4) _Body part:_ brain, eyes, skin, thyroid, chest, liver, kidney, and spine; (5) _Number of classes:_ ranging from 2 to 15; (6) _Number of samples:_ ranging from 20 to more than 350k; (7) _Sensitive attribute:_ sex, age, race, preferred language, skin tone, etc. (8) _SA skewness (Male : Female):_ ranging from 0.19 to 1.67. The details of these datasets can be found in the Appendix B.

### Models

**Classification FMs.** Eleven FMs from two categories are used for evaluation: (1) _vision models (VMs)_: C2L [75], DINOV2 [45], MedLVM [43], MedMAE [66], MoCo-CXR [59]; (2) _vision-language models (VLMs)_: CLIP [16], BLIP [34], BLIP2 [33], MedCLIP [65], PubMed-CLIP [15], BiomedCLIP [74]. For all models, _LP_ is used for fine-tuning; for VLMs, we also conduct _CLIP-ZS_ and _CLIP-Adapt_. Since these FMs are 2D models, we use 2.5D slices for 3D data and report volume-wise results.

FairMedFM evaluate the fairness of FMs under three commonly used protocols for classification:

\(\bullet\)_Linear probing (LP)._ A classification head \(h(\cdot):v\rightarrow\mathcal{Y}\) is trained to map the FM's embedding \(v\) to the prediction \(\hat{y}=h(f_{v}(x))\).

\(\bullet\)_Parameter-efficient fine-tuning (PEFT)._ PEFT aims to fine-tune FMs with a classification head to new downstream tasks with minimal computational overhead. FairMedFM evaluate fairness of FMs in the fine-tuning setting with modern PEFT strategies (e.g., LoRA [20] and pruning).

\(\bullet\)_CLIP-ZS and CLIP-Adapt._ Vision-language models offer the zero-shot classification ability for FMs, which compares the similarities of the vision embedding \(f_{v}(x)\) between different class-wise prototypes text embeddings \(f_{t}(x)\) of positive and negative prompts (e.g., "There is no pneumonia.") for each class. We consider both zero-shot _(CLIP-ZS)_ inference as well as a simple adaptation strategy _(CLIP-Adapt)_[57] which fine-tunes the class prototypes initialized with CLIP zero-shot prototypes.

**Segmentation FMs.** Nine SegFMs are selected from three categories for evaluation: (1) _general-SegFMs:_ SAM [30], MobileSAM [71], TinySAM [56]; (2) _2D Med-SegFMs:_ MedSAM [38], SAM-Med2D [9], and FT-SAM [9]; (3) _3D Med-SegFMs:_ SAM-Med3D [63], FastSAM3D [54]. All the four segmentation prompts described in Sec. 2.1 are used for 2D SegFMs. we adopt _rand_ and _rands_ for SAM-Med3D and FastSAM, _rands_ and _bbox_ for SegVol following their official implementation.

For SAM-family models, extra point prompts \(p_{\text{point}}\) or bounding box prompt \(p_{\text{bbox}}\) are required in the inference stage. Therefore, FairMedFM examine the fairness of SegFMs with different types of prompts including (1) _center:_ the center point of the mask; (2) _rand:_ 1 random point inside the mask; (3) _rands:_ 5 random points inside the mask; (4) _bbox:_ the bounding box of the mask. These prompts can be generated either directly from the ground truth mask or manually annotated. To thoroughly evaluate the fairness of SegFMs, FairMedFM provides the interface for both 2D and 3D SAM, and the access to fine-tune the SAM on the specific medical dataset with full supervision.

### Unfairness Mitigation Methods

FairMedFM provides popular and generalizable bias mitigation strategies and integrates them with the FMs. Following literature specialized in bias mitigation algorithms [77], we categorize them into the following:(1) _Group rebalancing_ is a technique used to address bias in datasets by adjusting the representation of different subgroups [21; 48], ensuring that minority groups have equal representation during training. This helps to mitigate biases that can arise from imbalanced datasets. (2) _Adversarial training_ is a method for reducing bias by training models in a way that they learn to make predictions while simultaneously being penalized for recognizing SA [70; 29]. This promotes fairness by minimizing the influence of biased features. (3) _Fairness constraints_ are used to ensure that models are trained to produce fair outcomes for different subgroups. This approach involves adding the differentiable form of fairness metrics to the training objective directly [44], or adjusting weights of the loss function for different subgroups to penalize the model for making biased predictions [60]. (4) _Subgroup-tailored modeling_ is a method that allows subgroups to have different model parameters, enabling the model to learn different representations for subgroups. This specialized modification can be applied on part of the model, i.e. fairness-aware adaptors [68], or the entire model [64]. (5) _Domain generalization_ aims to improve a model's ability to perform well across various domains, including those not encountered during training [64]. This approach seeks to create models that generalize better by finding robust solutions that work well in different scenarios [51].

### Evaluation Metrics Taxonomy

**Utility** refers to the effectiveness of the model in making accurate predictions. Examples include the **Area Under the receiver operating characteristic Curve (AUC)** for classification and **Dice similarity score (DSC)** for segmentation.

**Group fairness** is evaluated from four aspects following common practice [8]: (1) _Outcome-consistency fairness_, which evaluates discrepancies in the model's performance (e.g., accuracy, components of confusion matrix, etc.) between different sensitive groups. In classification, we include **delta AUC (AUC\({}_{\Delta}\))**, which is measured as the maximum AUC gap among subgroups; and **Equalized Odds (EqOdds)**, which measures the differences in true positive and false positive rates between advantaged and disadvantaged groups. In segmentation, we include **delta DSC (DSC\({}_{\Delta}\))**, which assesses if both groups receive approximately equal predictive performance; and **DSC skewness (DSC\({}_{\text{Skew}}\))**, which measures the degree of skewness of DSC between advantaged and disadvantaged groups. (2) _Predictive alignment fairness_, which focuses on the alignment between predicted probabilities and actual outcomes. It ensures that predicted scores accurately reflect true likelihoods, providing a reliable basis for decision-making across different groups. We report the **expected calibration error gap (ECE\({}_{\Delta}\))** in classification, where a high value indicates an optimal decision threshold; (3) _Positive-parity fairness_, which ensures that the positive classification rate is equal for both unprivileged and privileged groups, preventing any group from being overlooked. We note that this is an optional evaluative aspect that may not be applicable to all scenarios. For example, positive parity is compromised in diagnosing glaucoma, where morbidity rates differ between males and females; (4) _Representation fairness_, which evaluates fairness from the aspects of feature representation learned in the latent space, by estimating either group-wise feature separability among subgroups. We report the last two metrics in the Appendix E.2.

**Utility-fairness trade-off** takes both utility and fairness into account. It can be evaluated by combining utility and fairness metrics. Besides, equity scaling measurements that involve both aspects could also be used. In classification, we measure the **equity-scaled AUC (AUC\({}_{\text{ES}}\))**, which takes both utility and fairness into account. In segmentation, we report the **equity-scaled DSC (DSC\({}_{\text{ES}}\))**, which measures the tradeoff between overall utility and utility variations [60].

The mathematical definitions of the above metrics are presented in the Appendix C.

## 4 Results

In this section, we highlight the representative observations and takeaways from benchmarking the fairness of FMs on image classification and segmentation tasks utilizing FairMedFM framework. We choose to present the fairness results concerning sex since it is the most common SA shared across datasets. However, our method supports a broader range of SAs as listed in the Tab. 1. _We direct readers to Appendix E.1 for additional results on more SAs and extensive evaluations, which corroborate the observations discussed in the main text._ We also present the evaluation for _positive-parity-fairness_, _representation-fairness_, and _statistics test_ in Appendix E.2.

**Bias widely exists in FMs for medical imaging**. Fig. 2 and Fig. 3 present the fairness metrics on various classification and segmentation tasks as stated in Sec. 1. In classification, the AUC\({}_{\Delta}\) has an average value larger than 5% over all methods but presents large variations across methods. Similarly, in segmentation, our results in both 2D and 3D datasets reveal similar disparities, with notable high DSC\({}_{\Delta}\) of SegVol and FT-SAM reaching up to 10%.

**Careful selection and use of FMs are needed for ensuring a good fairness-utility trade-off.** These pervasive biases challenge the fairness-utility tradeoff of FMs in medical applications. We observe that the fairness-utility trade-off in these tasks is influenced not only by the choice of FMs but also by how they are used as shown in Fig. 4. We report trade-off scores AUC\({}_{\text{ES}}\) and DSC\({}_{\text{ES}}\), for each datasets on the selected models for both tasks respectively. Compared with CLIP-ZS models, a simple adaptation, CLIP-Adapt, has proven effective in significantly boosting the fairness-utility trade-off in medical applications. Further, we evaluate the effects of segmentation tasks on the choice of prompts (including _center_, _rand_, _rands_, and _bbox_), and the types of SegFMs (including _2D General-SegFMs_ and _2D Med-SegFMs_). Compared to _center_ and _rand_, models using _rands_ and _bbox_ tend to be fairer across different datasets. This might be due to the tighter constraints on the segmentation provided by _rands_ and _bbox_. Regarding models, General-SegFMs achieve a better fairness-utility trade-off than Med-SegFMs.

**Consistent disparities in SA occur across various FMs on the same dataset.** The performance of FMs shows dataset-specific biases, favoring one category of the given SA over the other, depending on the dataset. We present the four datasets in classification tasks and use sex as an example, presented

Figure 3: Bias in segmentation tasks. DSC\({}_{\Delta}\) is the fairness evaluation metric. Note that SAM-Med3D, FastSAM-3D, and SegVol are only applicable for 3D datasets.

Figure 2: Bias in classification tasks. AUC\({}_{\Delta}\) is the fairness evaluation metric. “\(\downarrow\)” denotes vision-language models, and “\(*\)” denotes pure vision models, where CLIP-ZS and CLIP-Adapt are not applicable.

in Fig. 5. The BREST and MIMIC-CXR dataset shows a higher performance for females compared to males across various models. Conversely, the FairVLMed10k and GF3300 dataset indicates better performance for males. Results on other SAs and segmentation tasks can be found in Appendix E.1.

**Existing unfairness mitigation strategies are not always effective.** While various unfairness mitigation methods for traditional neural networks have been proposed, their effectiveness on FMs for medical imaging remains underexplored. This study evaluates three mitigation algorithms for both classification and segmentation tasks. For classification, we apply two PEFT strategies (LP and LoRA) on three datasets (MIMIC-CXR, HAM10000 (CLS), and FairVLMed10k), while experiments for segmentation are conducted on the HAM10000 (SEG) dataset following the SAMed pipeline [73]. As shown in Tab. 2, although some mitigation strategies show better fairness metrics compared to the baseline, their utility-fairness tradeoffs do not always exceed (for example, LoRA + GroupDRO vs. LoRA). Besides, some mitigation algorithms show both lower utility and worse fairness (SAMed + InD vs. SAMed), which means that existing unfairness mitigation strategies are not always effective for FMs. Potential reasons could come from the scaling gap in training data and model parameters between the'small models' and large-scale FMs. Although there have been studies that focus on unfairness mitigation for FMs [24; 67], extra efforts are required to guarantee the fairness of FMs.

## 5 Conclusion

In this work, we introduced FairMedFM, a pioneering benchmark aimed at comprehensively evaluating the fairness of FMs in healthcare. Our pipeline demonstrates versatility by supporting various

Figure 4: Fairness-utility tradeoff in FMs for different components on (a-b) classification and (c-d) segmentation tasks. We use AUC\({}_{\text{ES}}\) and DSC\({}_{\text{ES}}\) as evaluation metrics. (a) Fairness-utility tradeoff for different classification usages (using CLIP as an example); (b) Fairness-utility tradeoff for general-purpose and medical-specific FMs in classification; (c) Fairness-utility tradeoff for different prompts in segmentation (using SAM-Med2D as an example); (d) Degree of fairness for different SegFM categories in segmentation.

Figure 5: Consistent disparities in SA occur across various FMs on the same dataset. (a) LP; (b) CLIP-Adapt. Points on the black dashline represent equal utility.

tasks, such as classification and segmentation, and by adapting to 20 different FMs, including both general-purpose and medical-specific models. By integrating a wide range of functionalities, such as LP, CLIP-Adapt, prompt-based segmentation, PEFT, and bias mitigation strategies, our framework enables comprehensive evaluations that are essential for developing fair and effective medical imaging solutions. With FairMedFM, we conducted in-depth analysis and revealed four key findings and takeaways: (1) Bias widely exists in FMs for medical imgaing tasks; (2) Different FMs and their variant usages present different fairness-utility trade-offs, therefore careful selection and proper use of FMs are crucial for ensuring a good fairness-utility trade-off; (3) The performance of various FMs exhibits consistent dataset-specific biases, which aligns with the SA distribution in individual datasets; and (4) The existing unfairness mitigation strategies are not always effective in FM settings.

**Future development plan.** Despite our efforts to include a wide range of datasets, FM methods, and fairness algorithms in our work to greatly enhance the comprehensiveness of existing benchmarks, there remains potential for further refinement and expansion. Our future work will continue to improve the comprehensiveness of our benchmark based on the framework and codebase of FairMedFM. The current pipeline FairMedFM is sufficiently flexible to extend; therefore, we will continue to incorporate new medical imaging datasets and emerging FM architectures over time. In addition to medical image classification and segmentation, the scope of our study will encompass predictive modeling, object detection, and vision-based question answering. Moreover, we intend to incorporate a broader range of fairness definitions in our evaluations and to investigate a wider array of bias-mitigation algorithms. We will ensure that FairMedFM's open-sourced codebase remains actively maintained and updated at the forefront of promoting equitable healthcare technologies.

## Acknowledgments and Disclosure of Funding

Y. Zhong and Q. Dou are supported by the Research Grants Council of Hong Kong Special Administrative Region, China (Project No. T45-401/22-N). Z. Xu, O. Yao, and S.K. Zhouare supported in part by the Natural Science Foundation of China under Grant 62271465, SuzhouBasic Research Program under Grant SYG202338, and Open Fund Project of Guangdong Academy of Medical Sciences, China (No. YKY-KF202206). R. Jin and X. Li are supported by the UBC Advanced Research Computing and Digital Research Alliance of Canada. We thank the reviewers for their insightful feedback and comments.

## References

* [1] Parnian Afshar, Shahin Heidarian, Nastaran Enshaei, Farnoosh Naderkhani, Moezedin Javad Rafiee, Anastasia Oikonomou, Faranak Babaki Fard, Kaveh Samimi, Konstantinos N Plataniotis, and Arash Mohammadi. Covid-ct-md, covid-19 computed tomography scan dataset applicable in machine learning and deep learning. _Scientific Data_, 8(1):121, 2021.

\begin{table}
\begin{tabular}{c|l c c c c c c c} \hline \hline
**Category** & **Method** & AUC\({}_{\text{Avg}}\uparrow\) & AUC\({}_{\text{Femndc}}\uparrow\) & AUC\({}_{\text{Medc}}\uparrow\) & AUC\({}_{\text{A}}\downarrow\) & E\(\text{Odds}\uparrow\) & E\(\text{CE}_{\text{A}}\downarrow\) & AUC\({}_{\text{ES}}\uparrow\) \\ \hline \multirow{6}{*}{CLS} & LP & 76.07 & 75.51 & 76.04 & 0.53 & 96.08 & 0.75 & 75.87 \\  & LP + GroupDRO [51] & 75.70 & 75.15 & 75.68 & 0.54 & 95.93 & 0.83 & 75.50 \\  & LP + Resampling [77] & 75.66 & 76.01 & 75.45 & 0.56 & 94.65 & 1.78 & 75.45 \\  & LP + LAFTR [40] & 75.80 & 76.15 & 75.69 & 0.46 & 95.13 & 0.93 & 75.63 \\ \cline{2-10}  & LoRA & 82.52 & 83.25 & 81.30 & 1.95 & 96.18 & 0.35 & 81.72 \\  & LoRA + GroupDRO [51] & 79.54 & 79.95 & 78.73 & 1.22 & 96.74 & 0.17 & 79.06 \\  & LoRA + Resampling [77] & 83.27 & 84.32 & 82.46 & 1.86 & 95.65 & 1.01 & 83.04 \\  & LoRA + LAFTR [40] & 80.50 & 80.08 & 80.70 & 0.62 & 97.87 & 0.22 & 80.25 \\ \hline \hline \multirow{6}{*}{SEG} & **Method** & DSC\({}_{\text{Avg}}\uparrow\) & DSC\({}_{\text{Femndc}}\uparrow\) & DSC\({}_{\text{Mdn}}\uparrow\) & DSC\({}_{\text{A}}\downarrow\) & DSC\({}_{\text{STp}}\downarrow\) & DSC\({}_{\text{Nov}}\downarrow\) & DSC\({}_{\text{cls}}\uparrow\) \\ \cline{1-1} \cline{2-10}  & SAM (best) & 83.55 & 84.47 & 82.74 & 1.73 & 0.86 & 1.11 & 82.83 \\ \cline{1-1}  & SAMed [73] & 90.92 & 90.98 & 90.87 & 0.10 & 0.05 & 1.01 & 90.87 \\ \cline{1-1}  & SAMed + FEBs [60] & 91.63 & 91.64 & 91.62 & 0.02 & 0.01 & 1.00 & 91.62 \\ \cline{1-1}  & SAMed + Resampling [77] & 91.51 & 91.60 & 91.43 & 0.17 & 0.09 & 1.02 & 91.43 \\ \cline{1-1}  & SAMed + Imb [64] & 88.85 & 89.61 & 88.19 & 1.42 & 0.71 & 1.14 & 88.22 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation of bias mitigation methods. Best and second best results are highlighted. All results are in percentage. Results for classification (CLS) are averaged across MIMIC-CXR, HAM10000 (CLS), and FairVLMed10k; Results for segmentation (SEG) are computed on HAM10000 (SEG).

* [2] Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, Amirhossein Kazerouni, Islem Rekik, and Dorit Merhof. Foundational models in medical imaging: A comprehensive survey and future vision. _arXiv preprint arXiv:2310.18689_, 2023.
* [3] Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting Chen, Nenad Tomasev, Jovana Mitrovic, Patricia Strachan, et al. Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging. _Nature Biomedical Engineering_, 7(6):756-779, 2023.
* [4] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. Ai fairness 360: an extensible toolkit for detecting. _Understanding, and Mitigating Unwanted Algorithmic Bias_, 2, 2018.
* [5] Sarah Bird, Miro Dudik, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in ai. _Microsoft, Tech. Rep. MSR-TR-2020-32_, 2020.
* [6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [7] Sema Candemir, Stefan Jaeger, Kannappan Palaniappan, Jonathan P Musco, Rahul K Singh, Zhiyun Xue, Alexandros Karargyris, Sameer Antani, George Thoma, and Clement J McDonald. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. _IEEE transactions on medical imaging_, 33(2):577-590, 2013.
* [8] Simon Caton and Christian Haas. Fairness in machine learning: A survey. _ACM Computing Surveys_, 56(7):1-38, 2024.
* [9] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiangand Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, and Yu Qiao. Sam-med2d, 2023.
* [10] Andrew Chester, Yun Sing Koh, Jorg Wicker, Quan Sun, and Junjae Lee. Balancing utility and fairness against privacy in medical data. In _2020 IEEE Symposium Series on Computational Intelligence (SSCI)_, pages 1226-1233. IEEE, 2020.
* [11] Yuning Du, Yuyang Xue, Rohan Dharmakumar, and Sotirios A Tsaftaris. Unveiling fairness biases in deep learning-based brain mri reconstruction. In _Workshop on Clinical Image-Based Procedures_, pages 102-111. Springer, 2023.
* [12] Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. Segvol: Universal and interactive volumetric medical image segmentation. _arXiv preprint arXiv:2311.13385_, 2023.
* [13] Raman Dutt, Ondrej Bohdal, Sotirios A Tsaftaris, and Timothy Hospedales. Fairtune: Optimizing parameter efficient fine tuning for fairness in medical image analysis. _arXiv preprint arXiv:2310.05055_, 2023.
* [14] Garabed Eknovan. Adolphe quetelet (1796-1874)--the average man and indices of obesity, 2008.
* [15] Sedigheh Eslami, Gerard de Melo, and Christoph Meinel. Does clip benefit visual question answering in the medical domain as much as it does in the general domain? _arXiv preprint arXiv:2112.13906_, 2021.
* [16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369, 2023.
* [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.

* [18] Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmen George, Xi Yang, Jianpeng Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean McSweeney, Ranveveveer Vasdev, Chris Hornung, Rafat Solaiman, James Schoephoerster, Bailey Abernathy, David Wu, Safa Abdulkadir, Ben Byun, Justice Spriggs, Griffin Struty, Alexandra Austin, Ben Simpson, Michael Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan Moore, Anna Jacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos Papanikolopoulos, and Christopher Weight. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct, 2023.
* [19] Geoffrey E Hinton and Sam Roweis. Stochastic neighbor embedding. _Advances in neural information processing systems_, 15, 2002.
* [20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [21] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In _Conference on Causal Learning and Reasoning_, pages 336-351. PMLR, 2022.
* [22] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 590-597, 2019.
* [23] Leonardo Iurada, Silvia Bucci, Timothy M Hospedales, and Tatiana Tommasi. Fairness meets cross-domain learning: A benchmark of models and metrics. _IEEE Access_, 2024.
* [24] Ruinan Jin, Wenlong Deng, Minghui Chen, and Xiaoxiao Li. Universal debiased editing for fair medical image classification. _arXiv preprint arXiv:2403.06104_, 2024.
* [25] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_, 6(1):317, 2019.
* [26] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1548-1558, 2021.
* [27] David N Kennedy, Christian Haselgrove, Steven M Hodge, Pallavi S Rane, Nikos Makris, and Jean A Frazier. Candishare: a resource for pediatric neuroimaging data. _Neuroinformatics_, 10:319-322, 2012.
* [28] Muhammad Osama Khan, Muhammad Muneeb Afzal, Shujaat Mirza, and Yi Fang. How fair are medical imaging foundation models? In _Machine Learning for Health (ML4H)_, pages 217-231. PMLR, 2023.
* [29] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training deep neural networks with biased data. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9012-9020, 2019.
* [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [31] Oleksandr Kovalyk, Juan Morales-Sanchez, Rafael Verdu-Monedero, Inmaculada Selles-Navarro, Ana Palazon-Cabanes, and Jose-Luis Sancho-Gomez. Papila: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment. _Scientific Data_, 9(1):291, 2022.

* [32] Ira Ktena, Olivia Wiles, Isabela Albuquerque, Sylvestre-Alvise Rebuffi, Ryutaro Tanno, Abhijit Guha Roy, Shekoofeh Azizi, Danielle Belgrave, Pushmeet Kohli, Taylan Cemgil, et al. Generative models improve fairness of medical classifiers under distribution shifts. _Nature Medicine_, pages 1-8, 2024.
* [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International conference on machine learning_, pages 12888-12900. PMLR, 2022.
* [35] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language models. _arXiv preprint arXiv:2308.10149_, 2023.
* [36] Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. _arXiv preprint arXiv:2403.19949_, 2024.
* [37] Yan Luo, Yu Tian, Min Shi, Louis R Pasquale, Lucy Q Shen, Nazlee Zebardast, Tobias Elze, and Mengyu Wang. Harvard glaucoma fairness: a retinal nerve disease dataset for fairness learning and fair identity normalization. _IEEE Transactions on Medical Imaging_, 2024.
* [38] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. _Nature Communications_, 15(1):654, 2024.
* [39] Jun Ma and Bo Wang. Towards foundation models of biological image segmentation. _Nature Methods_, 20(7):953-955, 2023.
* [40] David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In _International Conference on Machine Learning_, pages 3384-3393. PMLR, 2018.
* [41] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. _Nature_, 616(7956):259-265, 2023.
* [42] Luis Filipe Nakayama, Mariana Goncalves, L Zago Ribeiro, Helen Santos, Daniel Ferraz, Fernando Malerbi, Leo Anthony Celi, and Caio Regatieri. A brazilian multilabel ophthalmological dataset (brset), 2023.
* [43] Duy MH Nguyen, Hoang Nguyen, Nghiem T Diep, Tan N Pham, Tri Cao, Binh T Nguyen, Paul Swoboda, Nhat Ho, Shadi Albarqouni, Pengtao Xie, et al. Lvm-med: Learning large-scale self-supervised vision models for medical imaging via second-order graph matching. _arXiv preprint arXiv:2306.11925_, 2023.
* [44] Tochi Oguguo, Ghada Zamzmi, Sivaramakrishnan Rajaraman, Feng Yang, Zhiyun Xue, and Sameer Antani. A comparative study of fairness in medical machine learning. In _2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)_, pages 1-5. IEEE, 2023.
* [45] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [46] Ronald Carl Petersen, Paul S Aisen, Laurel A Beckett, Michael C Donohue, Anthony Collins Gamst, Danielle J Harvey, CR Jack Jr, William J Jagust, Leslie M Shaw, Arthur W Toga, et al. Alzheimer's disease neuroimaging initiative (adni) clinical characterization. _Neurology_, 74(3):201-209, 2010.
* [47] Carolina Picarra and Ben Glocker. Analysing race and sex bias in brain age prediction. In _Workshop on Clinical Image-Based Procedures_, pages 194-204. Springer, 2023.

* [48] Esther Puyol-Anton, Bram Ruijsink, Stefan K Piechnik, Stefan Neubauer, Steffen E Petersen, Reza Razavi, and Andrew P King. Fairness in cardiac mr image analysis: an investigation of bias due to data imbalance in deep learning based segmentation. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part III 24_, pages 413-423. Springer, 2021.
* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [50] Eliane Roosli, Selen Bozkurt, and Tina Hernandez-Boussard. Peeking into a black box, the fairness and generalizability of a mimic-iii benchmarking model. _Scientific Data_, 9(1):24, 2022.
* [51] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [52] Laleh Seyyed-Kalantari, Haoran Zhang, Matthew BA McDermott, Irene Y Chen, and Marzyeh Ghassemi. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. _Nature medicine_, 27(12):2176-2182, 2021.
* [53] Stanford AIMI shared datasets. Thyroid ultrasound cine-clip. https://stanfordaimi.azurewebsites.net/datasets/a72f2b02-7b53-4c5d-963c-d7253220bfd5.
* [54] Yiqing Shen, Jingxing Li, Xinyuan Shao, Blanca Inigo Romillo, Ankush Jindal, David Dreizin, and Mathias Unberath. Fastsam3d: An efficient segment anything model for 3d volumetric medical images. _arXiv preprint arXiv:2403.09827_, 2024.
* [55] Peilun Shi, Jianing Qiu, Sai Mu Dalike Abaxi, Hao Wei, Frank P-W Lo, and Wu Yuan. Generalist vision foundation models for medical imaging: A case study of segment anything model on zero-shot medical segmentation. _Diagnostics_, 13(11):1947, 2023.
* [56] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen. Tinysam: Pushing the envelope for efficient segment anything model. _arXiv preprint arXiv:2312.13789_, 2023.
* [57] Julio Silva-Rodriguez, Sina Hajimiri, Ismail Ben Ayed, and Jose Dolz. A closer look at the few-shot adaptation of large vision-language models. _arXiv preprint arXiv:2312.12730_, 2023.
* [58] Luc Soler, Alexandre Hostettler, Vincent Agnus, Arnaud Charnoz, J Fasquel, Johan Moreau, A Osswald, Mourad Bouhadjar, and Jacques Marescaux. 3d image reconstruction for comparison of algorithm database: A patient specific anatomical and medical image database. _IRCAD, Strasbourg, France, Tech. Rep_, 1(1), 2010.
* [59] Hari Sowrirajan, Jingbo Yang, Andrew Y Ng, and Pranav Rajpurkar. Moco-cxr: Moco pretraining improves representation and transferability of chest x-ray models. _2.1_, 3.2, 3
* [60] Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, and Mengyu Wang. Fairseg: A large-scale medical image segmentation dataset for fairness learning using segment anything model with fair error-bound scaling. In _International Conference on Learning Representations (ICLR)_, 2024.
* [61] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham1000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. _Scientific data_, 5(1):1-9, 2018.
* [62] Jasper W van der Graaf, Miranda L van Hooff, Constantinus FM Buckens, Matthieu Rutten, Job LC van Susante, Robert Jan Kroeze, Marinus de Kleuver, Bram van Ginneken, and Nikolas Lessmann. Lumbar spine segmentation in mr images: a dataset and a public benchmark. _Scientific Data_, 11(1):264, 2024.

* [63] Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, et al. Sam-med3d. _arXiv preprint arXiv:2310.15161_, 2023.
* [64] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8919-8928, 2020.
* [65] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. _arXiv preprint arXiv:2210.10163_, 2022.
* [66] Junfei Xiao, Yutong Bai, Alan Yuille, and Zongwei Zhou. Delving into masked autoencoders for multi-label thorax disease classification. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3588-3600, 2023.
* [67] Zikang Xu, Fenghe Tang, Quan Quan, Qingsong Yao, and S Kevin Zhou. Apple: Adversarial privacy-aware perturbations on latent embedding for unfairness mitigation. _arXiv preprint arXiv:2403.05114_, 2024.
* MICCAI 2023_, pages 307-317, Cham, 2023. Springer Nature Switzerland.
* [69] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.
* [70] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 335-340, 2018.
* [71] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. _arXiv preprint arXiv:2306.14289_, 2023.
* [72] Haoran Zhang, Natalie Dullerud, Karsten Roth, Lauren Oakden-Rayner, Stephen Pfohl, and Marzyeh Ghassemi. Improving the fairness of chest x-ray classifiers. In _Conference on health, inference, and learning_, pages 204-233. PMLR, 2022.
* [73] Kaidong Zhang and Dong Liu. Customized segment anything model for medical image segmentation. _arXiv preprint arXiv:2304.13785_, 2023.
* [74] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. _arXiv preprint arXiv:2303.00915_, 2023.
* [75] Hong-Yu Zhou, Shuang Yu, Cheng Bian, Yifan Hu, Kai Ma, and Yefeng Zheng. Comparing to learn: Surpassing imagenet pretraining on radiographs by comparing image representations. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part I 23_, pages 398-407. Springer, 2020.
* [76] Yuyin Zhou, Shih-Cheng Huang, Jason Alan Fries, Alaa Youssef, Timothy J Amrhein, Marcello Chang, Imon Banerjee, Daniel Rubin, Lei Xing, Nigam Shah, et al. Radfusion: Benchmarking performance and fairness for multimodal pulmonary embolism detection from ct and ehr. _arXiv preprint arXiv:2111.11665_, 2021.
* [77] Yongshuo Zong, Yongxin Yang, and Timothy Hospedales. Medfair: benchmarking fairness for medical imaging. _arXiv preprint arXiv:2210.01725_, 2022.
*

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See abstract.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? We have included our main claim and summarized our contributions in both the abstract and the introduction. 2. Did you describe the limitations of your work? We have described our limitations in the appendix. 3. Did you discuss any potential negative societal impacts of your work? Our benchmark focuses on investigating fairness in medical imaging, which does not have a negative societal impact. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? We confirm that our paper conforms with the ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Our benchmark focuses on investigating fairness in medical imaging, which does not include theoretical analysis. 2. Did you include complete proofs of all theoretical results? Our benchmark focuses on investigating fairness in medical imaging, which does not include theoretical analysis.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? We provide publically available code on GitHub, which is linked through our abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We provide our training details in the Appendix. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We executed the experiments in three random seeds and reported the averaged results. The error bar is provided whenever the visualization allows. 4. Did you include the total amount of computing and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? We provide our computing resources in the Appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? For all datasets and existing baselines we used in the study, we include proper citations. 2. Did you mention the license of the assets? Our code is provided under the Creative License, which we declared in the Appendix and in the URL. 3. Did you include any new assets either in the supplemental material or as a URL? We do not include any further assets except what we talked about above.

* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] We did not use any private resources that need consent.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] All data we used in the work are secondary data coming from existing resources (publically available). They are properly handled by the corresponding party before their release.
* If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] We are not using crowdsourcing or conducting research with human subjects. We are not using crowdsourcing or conducting research with human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] We are not using crowdsourcing or conducting research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] We are not using crowdsourcing or conducting research with human subjects.

**Road Map of Appendix** Our appendix is organized into five sections. The related work is in Sec. A, where Sec. A.1 restates the need for a fairness benchmark in medical FMs based on existing literature in fairness in medical imaging. Sec. A.2 reviews existing FMs and their usages, and details the FMs used in our work. Sec. B provides detailed dataset information and SA subgroup distribution. Sec. C explains the metrics used in FairMedFM, including their mathematical formulas. Sec. D lists the detailed implementation of our experiments. Finally, Sec. E presents additional results and observations.

## Appendix A Related Work

### Fairnesss in Medical Imaging

Bias widely exists in deep learning-based medical image analysis and has been studied by several recent studies. In the FM domain, [67] proposed to add perturbations with adversarial training on the latent embedding space to mitigate bias in segmentation while [24] adds the debiased edit on the input image to mitigate the bias when FM's gradient is inaccessible. In the non-FM deep learning tasks, [10] investigates the trade-off among the fairness, privacy and utility in medical data; [32] uses diffusion model to generate the synthetic images and argument the training data for mitigating the bias; [77] benchmarks the commonly used bias mitigation strategies on medical image classifications.

However, with the quick development of FM in medical image analysis, FM-based diagnostics become more and more popular. In the fairness domain, except the very recent study tastes the bias mitigation strategies in them [67, 24], few literatures provide a comprehensive overview of FMs in medical image analysis in the perspective of fairness. This gap motivates us to create the comprehensive benchmark, FairMedFM, which offers an evaluation pipeline covering 17 diverse medical imaging datasets, 20 FMs, and their usages. This benchmark addresses the need for a consistent evaluation and standardized process to investigate FMs' fairness in medical imaging. To restate, our objective is to raise awareness of fairness issues within the medical imaging community and assist in developing fair algorithms in the machine learning community, by promoting more accessible and reproducible methods for fairness evaluation.

### Foundation Models

FairMedFM focuses on benchmarking the FMs for classification and segmentation as details in A.2.1 and A.2.2. Fig. 6 visualized the usages of FMs in this study.

Figure 6: (a) Usages for classification, where the embedding of FM’s image encoder is applied for CLIP-ZS, CLIP-Adapt, LP and PEFT; (b) Usages of SAM-family for segmentation, where the embedding of the image is passed to the mask decoder for generating the segmentation mask.

#### a.2.1 Classification

In classification, FairMedFM implements both the VM and the VLM ranging from general-purpose and medical-specific FMs as detailed in Tab. 3. The usages of FMs for classification are outlined in Fig. 6, where CLIP-ZS, CLIP-Adapt, LP, and PEFT are included in FairMedFM as shown in Fig. 6 and detailed in Sec. 3.2 in the main paper.

#### a.2.2 Segmentation

Tab. 4 details the segmentation FMs used in this paper, which can be categorized into 2D natural SegFMs, 2D medical SegFMs, and 3D medical SegFMs. These models are based on the origin SAM architecture as shown in Fig. 6 (b), which consists of an image encoder, a prompt encoder and a mask decoder.

\begin{table}
\begin{tabular}{c|c|l} \hline \hline
**Category** & **Domain** & **Model** & **Link** \\ \hline \multirow{4}{*}{2D} & \multirow{4}{*}{Natural} & SAM[30] & https://github.com/facebookresearch/segment-anything \\  & & MobileSAM[71] & https://github.com/ChanningZhang/WhielSLAM \\  & & TinySAM[56] & https://github.com/xinghachen/ TinySAM \\ \cline{2-3}  & \multirow{4}{*}{Medical} & MedSAM[38] & https://github.com/bowang-lab/MedSAM \\  & & SAM-MedGD[9] & https://github.com/OpenOlLab/SAM-Med2D \\  & & FT-SAM[9] & https://drive.google.com/file/d/1J4qq8M2Z7dv1eoxMTJ4F1AgF265iVFN8/view \\ \hline \multirow{3}{*}{3D} & \multirow{4}{*}{Medical} & SAM-MedGD[36] & https://github.com/OpenOlLab/SAM-Med2D \\  & & FastSAM3D[54] & https://github.com/cradelo/FastSAM3D \\  & & SegVol[12] & https://github.com/BAIAI-DCAI/SegVol \\ \hline \hline \end{tabular}
\end{table}
Table 4: FMs used in segmentation.

\begin{table}
\begin{tabular}{c|c|l|l} \hline \hline
**Category** & **Domain** & **Model** & **Link** \\ \hline \multirow{4}{*}{VM} & \multirow{4}{*}{Medical} & DINOv2[45] & https://github.com/facebookresearch/dimov2 \\ \cline{2-3}  & & LVM-Med[43] & https://github.com/doybonninsguyen/LVM-Med \\  & & MoMeMLE[66] & https://github.com/lambert-rofacial_mac \\  & & Co-CAK[59] & https://github.com/amaffordgraph/MooC-GR \\  & & CLIP[75] & https://github.com/fumymzhou/CL2_MICCIA2020 \\ \hline \multirow{4}{*}{VLM} & \multirow{4}{*}{General} & BLIP[31] & https://github.com/classfac/BLIP \\  & & BLIP[31] & https://github.com/allesforce/LAVIS/tree/main/projects/blip2 \\  & & CLIP[49] & https://github.com/openai/CLIP \\ \cline{2-3}  & \multirow{4}{*}{Medical} & MedCLIP[65] & https://github.com/ByamNagZf/MedCLIP \\  & & PubMedCLIP[15] & https://github.com/sarnsEL/PubMedLIP/tree/main/PubMedLIP \\  & & BiomedCLIP[74] & https://huggingface.com/microsoft/BiomedCLIP-PubMedMENT_26-vi_base_patchL6_224 \\ \hline \hline \end{tabular}
\end{table}
Table 3: FMs used in classification.

## Appendix B Datasets Details

### Classification Datasets

**CheXpert** is a large dataset of chest X-rays labeled for the presence of 14 different observations as well as uncertainty labels. It includes 221006 chest X-ray images, making it widely used for the development and evaluation of medical imaging models. Many FMs, like MedCLIP, adapt it as part of the training data.

**MIMIC-CXR** is another publicly available dataset of chest radiographs with corresponding radiology reports. This dataset, consisting of 357542 chest X-ray images, is part of the MIMIC family, providing rich clinical context and metadata alongside imaging data.

**HAM10000** dataset consists of 9948 dermatoscopic images of pigmented skin lesions. These images are categorized into seven different types of skin conditions. It is used for both classification and segmentation fairness evaluation.

**FairVLMed10k** is a medical dataset containing 10000 diverse visual-linguistic pairs designed to address fairness in medical AI, where it was first used to train the FairCLIP [36]. It includes various types of medical images of the eye paired with descriptive text annotations.

**GF3300** also a dataset designed for evaluating medical fairness, which includes 3,300 subjects with both 2D and 3D imaging data of the retinal nerve, helping to promote fairness in medical AI for glaucoma detection.

**PAPILA** is a dataset of papillary thyroid carcinoma images, accompanied by detailed clinical and pathological information. This dataset includes 420 color fundus images of the eye, aiming to aid in the development of diagnostic tools for thyroid cancer.

**BRSET** consists of 16266 breast ultrasound images, annotated with benign and malignant labels. These color fundus images of the eye are crucial for advancing breast cancer detection and classification algorithms.

**COVID-CT-MD** is a dataset of computed tomography scans for patients diagnosed with COVID-19. It includes 305 lung CT images with annotations for COVID-19 manifestations, aiding in the development of diagnostic tools for the pandemic.

**ADNI-1.5T** is part of the Alzheimer's Disease Neuroimaging Imaging and includes MRI scans acquired at 1.5 Tesla. It consists of 550 brain MRI images, used extensively in research focused on early detection and progression tracking of Alzheimer's disease.

Tab. 5 lists the references, modality, body part, size, and SAs information of these datasets.

Figure 7: SA distribution of classification datasets.

### Segmentation Datasets

**HAM10000** contains 10,000 2D RGB dermatology images, with binary segmentation masks for skin lesion. We use sex and age as the sensitive attribute. For age, we regard age larger than 60 as the _old_ group, and age smaller than 60 as the _young_ group.

**TUSC** contains 860 2D thyroid ultrasound images, with binary segmentation masks for thyroid nodule. We use sex and age as the sensitive attribute. For age, we regard age larger than 60 as the _old_ group, and age smaller than 60 as the _young_ group.

**FairSeg** contains 10,000 2D OCT images, with three-class segmentation masks for optic cup and rim. We use sex, age, race, language, and marital status as the sensitive attribute. For age, we regard age larger than 60 as the _old_ group, and age smaller than 60 as the _young_ group. We categorize race into _White_, _Black_, and _Asian_. We categorize language into _English_, _Spanish_, and _Other_. We categorize marital status into _Marriage or Partnered_, _Single_, _Divorced_, _Widowed_ and _Legally seperated_. We categorize ethnicity into _Hispanic_ and _Non-Hispanic_.

**Montgomery County X-ray (montgomery)** contains 137 2D chest X-ray images, with three-class segmentation masks for left lung and right lung. We use sex and age as the sensitive attribute. For age, we regard age larger than 60 as the _old_ group, and age smaller than 60 as the _young_ group.

**KiTS2023 (KiTS)** contains 489 3D kidney CT volumes, with four-class segmentation masks for kidney, tumor, and cyst. We use sex and bmi as the sensitive attribute. For bmi, we categorize into underweight, normal, overweight, and obese following [14].

**CANDI** contains 103 3D brain MRI volumes, with multi-class segmentation masks for many brain structures. We select six classes including left/right brain white matter, left/right cerebral cortex, and left/right ventricle for segmentation. We use sex, age, and handedness as the sensitive attribute. For

Figure 8: SA distribution of segmentation datasets.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Type** & **Dataset** & **Modality** & **Body Part** & **\# Images** & **Sensitive Attribute** \\ \hline \multirow{7}{*}{2D} & CheXpert [22] & Chest X-ray & Chest & 221,006 & Sex, Age, Race \\  & MIMIC-CXR [25] & Chest X-ray & Chest & 357,542 & Sex, Age, Race \\  & HAM10000 [61] & Dermatology & Skin & 9,948 & Sex, Age \\  & FairVLMed10k [36] & SLO Fundus & Eye & 10,000 & Sex, Age, Race, Language \\  & GF3300 [37] & OCT RNFL thickness & Eye & 3,300 & Sex, Age, Race, Language \\  & PAPILA [31] & Color Fundus & Eye & 420 & Sex, Age \\  & BRSET [42] & Color Fundus & Eye & 16,266 & Sex, Age \\ \hline \multirow{2}{*}{3D} & COVID-CT-MD [1] & Lung CT & Chest & 305 & Sex, Age \\  & ADNI-1.5T [46] & Brain MRI & Brain & 550 & Sex, Age \\ \hline \hline \end{tabular}
\end{table}
Table 5: Classification datasets details.

age, we regard age larger than 10 as the _old_ group, and age smaller than 10 as the _young_ group as the maximum age in CANDI dataset is about 16. For handedness, we categorize it into _left-handed_ and _right-handed_.

**IRCADb** contains 20 3D liver CT volumes, with multi-class segmentation masks for many abdomen organs. We select six classes including class 1, 17, 33, 65, 129, and 193 for segmentation. We use sex as the SA.

**SPIDER** contains 218 3D spine MRI volumes, with multi-class segmentation masks for vertebra and disc. We use all the fifteen classes for segmentation, and use sex as the sensitive attribute.

Tab. 6 lists reference, modality, body part, size, and SAs information of these datasets.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Type** & **Dataset** & **Modality** & **Body Part** & **\# Images** & **Sensitive Attribute** \\ \hline \multirow{4}{*}{2D} & HAM10000 [61] & Dermatology & Skin & 10,015 & Sex, Age \\  & TUSC [53] & Ultra Sound & Thyroid & 860 & Sex, Age \\  & FairSeg [60] & OCT & Eye & 10,000 & Sex, Age, Race, Marital Status, etc. \\  & Montgomery County X-ray [7] & X-ray & Chest & 137 & Sex, Age \\ \hline \multirow{4}{*}{3D} & KiTS 2023 [18] & CT & Kidney & 489 & Sex, Body Mass Index (BMI) \\  & CANDI [27] & MRI & Brain & 103 & Sex, Age, Handedness \\ \cline{1-1}  & IRCADb [58] & CT & Liver & 20 & Sex \\ \cline{1-1}  & SPIDER [63] & MRI & Spine & 218 & Sex \\ \hline \hline \end{tabular}
\end{table}
Table 6: Segmentation datasets details.

Metrics

### Utility

We use Area Under the receiver operating characteristic Curve (AUC) and Accuracy for evaluating utility in classification and Dice Similarity Coefficient (DSC) for segmentation. The formulas for these metrics are:

\[\text{AUC}=\frac{1}{n_{1}n_{0}}\sum_{i=1}^{n_{1}}\sum_{j=1}^{n_{0}}\mathbf{1}(s_{ i}>s_{j})\]

where \(n_{1}\) is the number of positive samples, \(n_{0}\) is the number of negative samples, \(s_{i}\) is the score for the \(i\)-th positive sample, and \(s_{j}\) is the score for the \(j\)-th negative sample. The indicator function \(\mathbf{1}(s_{i}>s_{j})\) is 1 if \(s_{i}\) is greater than \(s_{j}\), and 0 otherwise.

\[\text{ACC}=\frac{TP+TN}{TP+TN+FP+FN}\]

where \(TP\) is the number of true positives, \(TN\) is the number of true negatives, \(FP\) is the number of false positives, and \(FN\) is the number of false negatives.

\[\text{DSC}=\frac{2\cdot|\hat{Y}\cap Y|}{|\hat{Y}|+|Y|}\]

where \(\hat{Y}\) is the set of predicted positive samples, and \(Y\) is the set of actual positive samples. The DSC measures the overlap between the predicted and actual positive samples.

### Fairness

Fairness measurements are categorized into three criteria: Positive-Parity Fairness, Outcome-Consistency Fairness, and Predictive-Alignment Fairness [8].

_Positive-Parity Fairness_ metrics primarily consider the positive rate, ensuring equal consideration in positive classifications for both unprivileged and privileged groups. For example, in disease screening, it is crucial that both groups have an equal chance of being flagged as positive cases, ensuring no group is overlooked. We apply Demographic Parity (DP) as the criterion for this group. The formula for Disparity Impact is given by:

\[\text{DP}=\left|\Pr(\hat{Y}=1|A=0)-\Pr(\hat{Y}=1|A=1)\right|\]

where \(Y\) is the prediction conditioned by SA. This ratio measures the disparity between the group with the minimum performance and the group with the maximum performance.

_Outcome-Consistency Fairness_ measures assess the discrepancies in confusion matrix components between different sensitive groups. Common metrics include Equal Opportunity and Equal Odds. Equal Opportunity assesses whether both groups receive approximately equal scores by calculating the gaps in Accuracy (\(\text{Acc}_{\Delta}\)), AUC (\(\text{AUC}_{\Delta}\)), and DSC (\(\text{DSC}_{\Delta}\)). Equalized Odds ensures that the model performs equally well across SA groups in terms of both TPR and FPR. In our study, we measure equal odds by calculating the differences in TPR and FPR between the advantaged and disadvantaged groups. The formulas for these metrics are:

\[\text{AUC/ACC/DSC}_{\Delta}=\text{AUC/ACC/DSC}_{\max}-\text{AUC/ACC/DSC}_{ \min}\]

\[\text{AUC/ACC/DSC}_{\min}=\min\text{AUC/ACC/DSC}\]

\[\text{AUC/ACC/DSC}_{\text{Skew}}=\frac{\max_{i}\left(1-\text{AUC/ACC/DSC}_{a }\right)}{\min_{i}\left(1-\text{AUC/ACC/DSC}_{a}\right)}\]\[\text{EqOdds}= \frac{1}{2}\left|\Pr(\hat{Y}=1|Y=1,A=0)-\Pr(\hat{Y}=1|Y=1,A=1)\right|\] (1) \[+ \frac{1}{2}\left|\Pr(\hat{Y}=1|Y=0,A=0)-\Pr(\hat{Y}=1|Y=0,A=1)\right|\]

_Predictive-Alignment Fairness_ metrics focus on the predicted probabilities or scores, aiming to evaluate how well the predicted probabilities of outcomes align with the actual outcomes. The Expected Calibration Error (ECE) evaluates if the predicted scores are indicative of true likelihoods, thus providing a reliable basis for decision-making across different groups, where a high value leads to an optimal decision threshold. We measure the gap of ECE between different SA group, \(\text{ECE}_{\Delta}\), where a higher gap indicates strong bis in terms of the predictive alignment. The formulas for it is:

\[\text{ECE}_{\Delta}=\left|\frac{1}{N}\sum_{i=1}^{N}\left|p_{i}-o_{i}\right|- \frac{1}{N^{\prime}}\sum_{j=1}^{N^{\prime}}\left|p_{j}^{\prime}-o_{j}^{\prime} \right|\right|\]

where \(N\) is the total number of samples, \(p_{i}\) is the predicted probability for sample \(i\), and \(o_{i}\) is the actual outcome for sample \(i\). \(N\) and \(N^{\prime}\) belong to two different subgroups.

_Representation Fairness_ inspects the integration between the model and the dataset, trying to figure out the relationship between fairness and the feature distribution in the latent space. Generally, a feature distribution that is hard to separate will result in lower bias. In this paper, we visualize the representation fairness by t-SNE [19].

### Fairness-utility Tradeoff

The fairness-utility tradeoff pursues a balance between fairness and utility. Following [37; 60], we use Equity Scaling measurements of AUC (\(\text{AUC}_{\text{ES}}\)) and DSC (\(\text{DSC}_{\text{ES}}\)) for classification and segmentation, respectively. The equations are as follows.

\[\text{AUC}_{\text{ES}} =\frac{\overline{\text{AUC}}}{1+\text{AUC}_{\Delta}}\] (2) \[\text{DSC}_{\text{ES}} =\frac{\overline{\text{DSC}}}{1+\text{DSC}_{\Delta}}\] (3)

where \(\overline{\text{AUC}}\) and \(\overline{\text{DSC}}\) are the average AUC and DSC over all data samples. \(\text{AUC}_{\Delta}\) and \(\text{DSC}_{\Delta}\) are the standard deviation of AUC and DSC across all subgroups defined by sensitive attributes, respectively.

### Statistics Test

We perform statistical significance tests to ensure that any observed performance in benchmarking FMs' s performance in medical imaging is not due to occasion on specific datasets. It assesses their performance across various datasets to draw meaningful and robust conclusions. We adapt the statistics evaluation in [77], where the relative ranks for each FM's performance are calculated on individual datasets and then averaged. The Friedman test is executed and the Critical Difference (CD) figure is plotted.

Experimental Details

### Classification

The classification pipeline follows a straightforward approach, employing common data pre-processing strategies. During hyper-parameter selection, we first determine the optimal learning rate using LP. This learning rate is then applied to CLIP-Adapt and other PEFT methods, given the similar parameter scales of the adapters.

#### d.1.1 Data Pre-processing

For 2D datasets, we resize all images to \(256\times 256\) and then apply CenterCrop to achieve a size of \(224\times 224\). All datasets are normalized using the ImageNet mean and standard deviation, as most FMs are initialized with these parameters. For 3D datasets, we utilize a 2.5D loading approach. Initially, the volumes are resized to a longitudinal axis size of 32. Subsequently, slices are processed independently through the 2D pre-processing pipeline and input into 2D foundation models. The final volume-wise prediction is obtained by maximizing the predictions of all slices in the volume.

#### d.1.2 Subgroup Definition

We follow previous works to binarize sensitive attributes and define subgroup pairs [22, 77]. The sensitive attributes included in FairMedFM are listed in Tab. 5 and Tab. 6.

**Sex.** We follow the metadata in the original dataset to binarize the data into Male and Female subgroups.

**Age**. We use different thresholds to distinguish between Young and Old data points. By default, we use a threshold of 60 for all datasets except COVID-CT-MD and ADNI-1.5T, where the thresholds are 50 and 75, respectively. These threshold choices are primarily aimed at constructing a balanced testing set and ensuring a sufficient number of data points.

**Race**. We split data samples into White and Non-white subgroups.

**Language**. We split data samples into English and Non-English subgroups.

**BMI & Handedness & Marital Status**. The subgroup splitting of these sensitive attributes is introduced in Sec. B.2.

#### d.1.3 Hyper-parameters

In classification, we initially use LP to identify the optimal learning rate and batch size. Once the loss converges and the training and testing performance align, we apply the same set of hyper-parameters for CLIP-Adapt, LP, and full fine-tuning. For all experiments, we use the AdamW optimizer with cosine annealing schedule configured with batch size of 128 and weight decay of 0.05. All models are trained for 100 epochs, during which we observe the convergence of loss and the alignment of training and testing metrics. Our experiments are conducted on a single NVIDIA A100 GPU.

We also include multiple unfairness mitigation algorithms, of which the hyper-parameters are grid-searched and listed as follows:

**LP & CLIP-Adapt & Resampling.** Learning rate: \(2.5\times 10^{-4}\).

**Group DRO.** Learning rate: \(2.5\times 10^{-4}\); Group adjustments: 1.

**LAFTR.** Learning rate: \(3\times 10^{-4}\); Adversarial coefficients: \(0.1\).

### Segmentation

The evaluation of SegFMs consists of two step, i.e. data pre-processing, prompt generation, and network inference. Note that for multi-class tasks, we process each class seperately, and average the results across different classes.

#### d.2.1 Data Pre-processing

As there are 2D datasets and 3D datasets used in this paper, and different SegFMs can only accept either 2D or 3D input, we first pre-process the datasets as follows:

**2D models + 2D datasets.** The RGB grayscale images and grayscale images are resize to (1024, 1024, 3), and directly sent to 2D SegFMs including SAM, MobileSAM, TinySAM, MedSAM, SAM-Med2D, and FT-SAM.

**2D models + 3D datasets.** The 3D MRI/CT volumes are firstly normalized to [0, 1]. Then, the slices along the Axial plane are splited, and only slices that have ground truth segmentation masks are resized to (1024, 1024, 3) and saved for evaluation using 2D SegFMs.

**3D models + 3D datasets.** The 3D MRI/CT volumes are firstly normalized to [0, 1], and directly sent to 3D SegFMs including SAM-Med3D, FastSAM-3D, and SegVol.

#### d.2.2 Prompt Generation

In this paper, the prompts are generated from the ground truth mask to obtain better utilities. For _rand_ and _rands_, we use Random Number Generator with equal weights for each point. Following the official implementation, we use _center_, _rand_, _rands_ and _bbox_ for all the 2D SegFMs, use _rand_ and _rands_ for SAM-Med3D and FastSAM-3D, and use _rands_ and _bbox_ for SegVol.

#### d.2.3 Network Inference

The DSC scores are computed based on the origin shape of the input image. For 2D models + 2D datasets and 3D models + 3D datasets, we directly compute the sample-wise DSC, for 2D models + 3D dataset, we first aggregate slice-wise results to get sample-wise prediction, and then compute the sample-wise DSC.

#### d.2.4 t-SNE Visualization

t-SNE are presented for only 2D datasets + 2D models. We use feature map after the image encoder, which is of shape (256, 64, 64). We average the feature map across the second and the third channel to get a feature vector with the shape of 256, and t-SNE is computed using Python scikit-learn package.

#### d.2.5 Hyper-parameters

We finetune the original SAM using the implementation of SAMed on HAM10000 dataset. The HAM10000 dataset is randomly split into train and test with a ratio of 8:2. Earlystop strategy is applied in the training procedure. The hyper-parameters for unfairness mitigation are as follows:

**SAMed.** Learning rate: 0.005; Optimizer: AdamW; Max epoch: 100; Batchsize: 16.

**FEBS.** Learning rate: 0.005; Optimizer: AdamW; Max epoch: 20; Batchsize: 16; Dice loss coefficient: 0.8. FEBS loss temperature coefficient: 1.

**Resampling.** Learning rate: 0.005; Optimizer: AdamW; Max epoch: 20; Batchsize: 8.

**InD.** Learning rate: 0.005; Optimizer: AdamW; Max epoch: 20; Batchsize: 16.

[MISSING_PAGE_FAIL:26]

Figure 11: Bias in segmentation tasks with more SAs. DSC\({}_{\Delta}\) is the fairness evaluation metric. Note that SAM-Med3D, FastSAM-3D, and SegVol are only applicable for 3D datasets.

Figure 10: Bias in classification tasks with more SAs. DP is the fairness evaluation metric. “\(\downarrow\)” denotes vision-language models, and “\(*\)” denotes pure vision models, where CLIP-ZS and CLIP-Adapt are not applicable.

Figure 14: *:General purpose 2D SegFMs; *: Medical 2D SegFMs; *: Medical 3D SegFMs.

Figure 12: Fairness-utility tradeoff in FMs for different components on classification with more SAs. We use AUC\({}_{\text{ES}}\) and DSC\({}_{\text{ES}}\) as evaluation metrics. (a, c) Fairness-utility tradeoff for different classification usages (using CLIP as an example); (b, d) Fairness-utility tradeoff for general-purpose and medical-specific FMs.

Figure 13: Fairness-utility tradeoff in FMs for different components on segmentation with more SAs. We use AUC\({}_{\text{ES}}\) and DSC\({}_{\text{ES}}\) as evaluation metrics. (a) Fairness-utility tradeoff for different prompts in segmentation (using SAM-Med2D as an example); (b) Degree of fairness for different SegFM categories.

[MISSING_PAGE_EMPTY:29]

**Representation fairness**. The t-SNE of 2D SegFMs on the four 2D datasets are shown in Fig. 16. Here we use sex as an example. As shown in Fig. 16, compared to the TUSC dataset and FairSeg dataset, the latent space of the HAM10000 dataset is more separable. This is roughly aligned with the results in Fig. 3, where the DSC\({}_{\Delta}\) of the HAM10000 dataset are larger than the rest datasets. This finding provides potential for us to mitigate unfairness for SegFMs by manipulating the latent space, which has been explored in APPLE [67]. On the other hand, considering that the group-wise separability is similar acrosss different SegFMs, these four tasks might suffer more unfairness from the data than from the model.

Figure 16: Unfairess potentially exists in the latent space of SegFMs. \(\bullet\): data points of the Male; \(\bullet\): data points of the Female.

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_EMPTY:32]

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{FairSeg} & \multirow{4}{*}{SAM} & center & 45.01 & 44.83 & 45.27 & 0.45 & 0.22 & 36.82 \\  & & rand & 45.22 & 44.74 & 45.87 & 1.12 & 0.56 & 28.94 \\  & & rands & 55.02 & 54.42 & 55.85 & 1.43 & 0.71 & 32.13 \\  & & bbox & 63.35 & 63.26 & 63.42 & 0.16 & 0.08 & 58.52 \\ \cline{2-8}  & \multirow{4}{*}{MobileSAM} & center & 34.59 & 33.97 & 35.47 & 1.50 & 0.75 & 19.74 \\  & & rand & 32.41 & 31.84 & 33.22 & 1.38 & 0.69 & 19.21 \\  & & rands & 45.86 & 45.47 & 46.41 & 0.94 & 0.47 & 31.20 \\  & & bbox & 65.64 & 65.34 & 65.86 & 0.52 & 0.26 & 52.20 \\ \cline{2-8}  & \multirow{4}{*}{TinySAM} & center & 43.47 & 43.40 & 43.55 & 0.15 & 0.07 & 40.53 \\  & & rand & 48.06 & 48.00 & 48.13 & 0.13 & 0.07 & 45.02 \\  & & rands & 59.57 & 59.44 & 59.77 & 0.33 & 0.16 & 51.24 \\  & & bbox & 67.47 & 67.14 & 67.70 & 0.56 & 0.28 & 52.60 \\ \cline{2-8}  & \multirow{4}{*}{MedSAM} & center & 2.48 & 2.41 & 2.58 & 0.17 & 0.08 & 2.29 \\  & & rand & 1.72 & 1.71 & 1.74 & 0.03 & 0.02 & 1.69 \\  & & rands & 17.88 & 17.84 & 17.92 & 0.08 & 0.04 & 17.23 \\  & & bbox & 44.67 & 43.80 & 45.28 & 1.48 & 0.74 & 25.67 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 32.12 & 31.49 & 32.56 & 1.07 & 0.54 & 20.92 \\  & & rand & 34.29 & 33.65 & 34.75 & 1.10 & 0.55 & 22.12 \\  & & rands & 59.69 & 59.21 & 60.03 & 0.82 & 0.41 & 42.33 \\  & & bbox & 49.09 & 48.66 & 49.38 & 0.72 & 0.36 & 36.09 \\ \hline \multirow{8}{*}{FT-SAM} & center & 16.35 & 16.02 & 16.59 & 0.57 & 0.29 & 12.70 \\  & & rand & 13.51 & 13.16 & 13.76 & 0.60 & 0.30 & 10.39 \\ \cline{1-1}  & & rands & 31.42 & 31.01 & 31.71 & 0.70 & 0.35 & 23.27 \\ \cline{1-1}  & & bbox & 49.34 & 48.77 & 49.76 & 0.99 & 0.50 & 32.95 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Segmentation results on the FairSeg dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{MobileSAM} & center & 59.01 & 57.34 & 61.01 & 3.67 & 1.83 & 20.81 \\  & & rand & 51.31 & 50.87 & 51.85 & 0.98 & 0.49 & 34.44 \\  & & rands & 63.39 & 62.18 & 64.84 & 2.66 & 1.33 & 27.21 \\  & & bbox & 51.09 & 50.44 & 51.88 & 1.44 & 0.72 & 29.70 \\ \cline{2-8}  & \multirow{4}{*}{TinySAM} & center & 48.73 & 48.17 & 49.41 & 1.24 & 0.62 & 30.08 \\  & & rand & 45.51 & 45.34 & 45.64 & 0.30 & 0.15 & 39.57 \\  & & hands & 50.49 & 50.38 & 50.62 & 0.24 & 0.12 & 45.08 \\  & & bbox & 75.13 & 71.69 & 79.25 & 7.56 & 3.78 & 15.72 \\ \cline{2-8}  & \multirow{4}{*}{TinySAM} & center & 58.83 & 56.19 & 61.99 & 5.80 & 2.90 & 15.08 \\  & & rand & 53.38 & 51.16 & 56.04 & 4.88 & 2.44 & 15.52 \\  & & hands & 63.71 & 61.47 & 66.39 & 4.92 & 2.46 & 18.41 \\  & & bbox & 83.46 & 82.21 & 84.96 & 2.75 & 1.38 & 35.14 \\ \cline{2-8}  & \multirow{4}{*}{MedSAM} & center & 0.29 & 0.29 & 0.30 & 0.01 & 0.01 & 0.29 \\  & & rand & 0.97 & 0.95 & 0.99 & 0.04 & 0.02 & 0.95 \\  & & rands & 14.63 & 14.52 & 14.77 & 0.25 & 0.12 & 13.00 \\  & & bbox & 74.78 & 74.43 & 75.22 & 0.79 & 0.39 & 53.61 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 86.65 & 85.39 & 88.16 & 2.77 & 1.38 & 36.33 \\  & & rand & 87.12 & 85.80 & 88.69 & 2.89 & 1.45 & 35.63 \\ \cline{1-1}  & & hands & 89.48 & 88.66 & 90.46 & 1.80 & 0.90 & 47.09 \\ \cline{1-1}  & & bbox & 90.92 & 90.30 & 91.67 & 1.37 & 0.69 & 53.96 \\ \hline \multirow{8}{*}{FT-SAM} & center & 11.54 & 10.32 & 12.99 & 2.67 & 1.33 & 4.94 \\ \cline{1-1}  & & rand & 6.22 & 5.38 & 7.23 & 1.85 & 0.93 & 3.23 \\ \cline{1-1}  & & hands & 19.01 & 17.04 & 21.37 & 4.33 & 2.17 & 6.01 \\ \cline{1-1}  & & bbox & 69.95 & 67.10 & 73.38 & 6.28 & 3.14 & 16.90 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Segmentation results on the HAM10000 dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{MobileSAM} & center & 71.80 & 65.75 & 78.55 & 12.80 & 6.40 & 9.70 \\  & rand & 66.33 & 59.15 & 74.33 & 15.19 & 7.59 & 7.72 \\  & rands & 88.46 & 86.62 & 90.55 & 3.92 & 1.96 & 29.86 \\  & bbox & 92.75 & 92.43 & 93.11 & 0.68 & 0.34 & 69.22 \\ \hline \multirow{8}{*}{MobileSAM} & center & 56.87 & 55.52 & 57.98 & 2.45 & 1.23 & 25.56 \\  & rand & 51.26 & 49.48 & 52.73 & 3.24 & 1.62 & 19.54 \\  & rands & 62.46 & 61.62 & 62.95 & 1.34 & 0.67 & 37.40 \\  & bbox & 88.75 & 88.58 & 88.84 & 0.26 & 0.13 & 78.54 \\ \hline \multirow{8}{*}{TinySAM} & center & 78.62 & 77.86 & 79.33 & 1.47 & 0.73 & 45.38 \\  & rand & 69.00 & 65.67 & 71.53 & 5.86 & 2.93 & 17.56 \\  & rands & 79.82 & 78.71 & 80.60 & 1.89 & 0.95 & 41.04 \\  & bbox & 90.17 & 90.08 & 90.19 & 0.11 & 0.05 & 85.47 \\ \hline \multirow{8}{*}{MedSAM} & center & 1.18 & 1.09 & 1.26 & 0.17 & 0.09 & 1.08 \\  & rand & 2.65 & 1.96 & 3.11 & 1.15 & 0.58 & 1.68 \\ \cline{1-1}  & rands & 27.86 & 27.00 & 28.43 & 1.43 & 0.71 & 16.24 \\ \cline{1-1}  & bbox & 79.56 & 77.85 & 81.44 & 3.59 & 1.80 & 28.44 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 88.34 & 87.06 & 89.86 & 2.80 & 1.40 & 36.77 \\ \cline{1-1}  & rand & 82.50 & 78.52 & 87.44 & 8.92 & 4.46 & 15.10 \\ \cline{1-1}  & rands & 91.15 & 90.83 & 91.55 & 0.72 & 0.36 & 67.02 \\ \cline{1-1}  & bbox & 91.73 & 91.06 & 92.51 & 1.45 & 0.73 & 53.10 \\ \hline \multirow{8}{*}{FT-SAM} & center & 6.17 & 5.74 & 6.61 & 0.87 & 0.43 & 4.30 \\ \cline{1-1}  & rand & 4.66 & 4.58 & 4.79 & 0.20 & 0.10 & 4.24 \\ \cline{1-1}  & rands & 24.56 & 18.16 & 30.16 & 12.00 & 6.00 & 3.51 \\ \cline{1-1}  & bbox & 75.46 & 74.23 & 76.62 & 2.39 & 1.20 & 34.34 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Segmentation results on the Montgomery dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{MobileSAM} & center & 27.13 & 26.85 & 28.60 & 1.75 & 0.88 & 14.47 \\  & rand & 26.96 & 26.52 & 29.28 & 2.76 & 1.38 & 11.33 \\  & rands & 32.23 & 32.13 & 32.77 & 0.64 & 0.32 & 24.42 \\  & bbox & 87.06 & 86.41 & 87.19 & 0.78 & 0.39 & 62.63 \\ \cline{1-1} \cline{2-10}  & center & 24.70 & 24.00 & 28.36 & 4.36 & 2.18 & 7.77 \\  & rands & 24.81 & 24.03 & 28.93 & 4.90 & 2.45 & 7.19 \\  & hands & 27.53 & 26.81 & 31.26 & 4.45 & 2.23 & 8.54 \\  & bbox & 82.84 & 81.12 & 83.16 & 2.04 & 1.02 & 41.01 \\ \hline \multirow{8}{*}{TinySAM} & center & 27.18 & 26.93 & 28.51 & 1.58 & 0.79 & 15.18 \\  & rand & 25.50 & 24.96 & 28.30 & 3.34 & 1.67 & 9.55 \\ \cline{1-1}  & hands & 30.69 & 30.61 & 31.15 & 0.54 & 0.27 & 24.17 \\ \cline{1-1}  & bbox & 88.33 & 87.81 & 88.43 & 0.62 & 0.31 & 67.43 \\ \hline \multirow{8}{*}{TUSC} & center & 0.92 & 0.88 & 0.93 & 0.05 & 0.03 & 0.90 \\  & rand & 1.14 & 0.56 & 1.25 & 0.69 & 0.34 & 0.85 \\ \cline{1-1}  & rands & 17.75 & 16.79 & 17.93 & 1.14 & 0.57 & 11.31 \\ \cline{1-1}  & bbox & 69.07 & 68.88 & 70.07 & 1.19 & 0.59 & 43.30 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 18.35 & 15.35 & 18.92 & 3.57 & 1.79 & 6.59 \\ \cline{1-1}  & rand & 14.93 & 12.81 & 15.34 & 2.53 & 1.26 & 6.59 \\ \cline{1-1}  & hands & 54.07 & 51.05 & 54.65 & 3.60 & 1.80 & 19.31 \\ \cline{1-1}  & bbox & 57.38 & 53.20 & 58.18 & 4.98 & 2.49 & 16.44 \\ \hline \multirow{8}{*}{FT-SAM} & center & 2.26 & 2.17 & 2.75 & 0.58 & 0.29 & 1.75 \\ \cline{1-1}  & rand & 1.71 & 1.70 & 1.76 & 0.06 & 0.03 & 1.66 \\ \cline{1-1}  & hands & 9.94 & 9.74 & 10.98 & 1.24 & 0.62 & 6.14 \\ \cline{1-1}  & bbox & 44.91 & 42.00 & 45.46 & 3.46 & 1.73 & 16.45 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Segmentation results on the TUSC dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{SAM} & center & 26.43 & 23.59 & 28.13 & 4.54 & 6.14 & 24.73 \\  & rand & 37.66 & 32.56 & 41.09 & 8.52 & 8.85 & 34.44 \\  & rands & 43.10 & 39.60 & 45.27 & 5.67 & 9.51 & 39.27 \\  & bbox & 57.51 & 54.14 & 61.45 & 7.31 & 7.54 & 53.46 \\ \hline \multirow{8}{*}{MobileSAM} & center & 18.24 & 16.61 & 19.32 & 2.71 & 3.27 & 17.37 \\  & rand & 24.12 & 23.40 & 23.83 & 0.42 & 5.55 & 22.58 \\  & rands & 29.80 & 28.21 & 29.60 & 1.39 & 7.12 & 27.38 \\  & bbox & 55.14 & 51.45 & 58.45 & 7.01 & 6.54 & 51.84 \\ \hline \multirow{8}{*}{TinySAM} & center & 28.60 & 26.42 & 29.10 & 2.68 & 6.14 & 26.83 \\  & rand & 40.95 & 36.07 & 43.84 & 7.77 & 9.69 & 37.23 \\  & rands & 44.37 & 42.87 & 44.49 & 1.62 & 7.80 & 40.94 \\  & bbox & 57.26 & 53.47 & 60.99 & 7.52 & 7.52 & 53.31 \\ \hline \multirow{8}{*}{MedSAM} & center & 1.23 & 0.62 & 1.38 & 0.76 & 0.78 & 1.21 \\  & rand & 1.12 & 0.85 & 1.28 & 0.44 & 0.33 & 1.12 \\ \cline{1-1}  & rands & 12.66 & 11.03 & 12.88 & 1.86 & 3.18 & 12.11 \\ \cline{1-1}  & bbox & 43.43 & 42.33 & 44.78 & 2.45 & 5.07 & 41.16 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 27.74 & 25.24 & 28.07 & 2.83 & 4.91 & 26.20 \\ \cline{1-1}  & rand & 36.05 & 33.23 & 37.71 & 4.48 & 3.48 & 34.59 \\ \cline{1-1}  & rands & 46.37 & 45.69 & 46.94 & 1.25 & 3.48 & 44.90 \\ \cline{1-1}  & bbox & 38.39 & 37.00 & 40.44 & 3.44 & 5.31 & 36.25 \\ \hline \multirow{8}{*}{FT-SAM} & center & 13.22 & 10.46 & 14.34 & 3.87 & 4.12 & 12.49 \\ \cline{1-1}  & rand & 16.68 & 12.65 & 19.16 & 6.51 & 4.48 & 15.63 \\ \cline{1-1}  & rands & 28.76 & 26.12 & 30.32 & 4.19 & 3.28 & 27.60 \\ \cline{1-1}  & bbox & 38.33 & 36.95 & 39.08 & 2.13 & 3.20 & 37.34 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Segmentation results on the IRCADb dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{TinySAM} & center & 19.56 & 19.29 & 19.91 & 0.62 & 0.36 & 19.50 \\  & rand & 27.50 & 27.25 & 27.84 & 0.59 & 0.48 & 27.32 \\  & rands & 29.36 & 28.84 & 30.03 & 1.19 & 0.76 & 29.05 \\  & bbox & 55.35 & 54.96 & 55.85 & 0.89 & 0.45 & 55.11 \\ \hline \multirow{8}{*}{MobileSAM} & center & 13.33 & 13.14 & 13.47 & 0.33 & 0.17 & 13.30 \\  & rand & 13.87 & 13.61 & 14.07 & 0.47 & 0.23 & 13.85 \\  & rands & 18.32 & 18.29 & 18.35 & 0.06 & 0.21 & 18.29 \\  & bbox & 52.53 & 52.22 & 52.92 & 0.70 & 0.35 & 52.36 \\ \hline \multirow{8}{*}{TinySAM} & center & 22.03 & 21.55 & 22.65 & 1.10 & 0.55 & 21.93 \\  & rand & 28.97 & 28.62 & 29.42 & 0.80 & 0.40 & 28.84 \\  & rands & 24.84 & 24.46 & 25.34 & 0.87 & 0.52 & 24.72 \\  & bbox & 52.69 & 52.45 & 52.99 & 0.54 & 0.27 & 52.55 \\ \hline \multirow{8}{*}{CANDI} & center & 0.40 & 0.36 & 0.43 & 0.08 & 0.04 & 0.40 \\  & rand & 16.19 & 16.15 & 16.23 & 0.08 & 0.06 & 16.16 \\ \cline{1-1}  & rands & 14.79 & 14.54 & 15.10 & 0.56 & 0.31 & 14.72 \\ \cline{1-1}  & bbox & 40.29 & 40.01 & 40.65 & 0.64 & 0.56 & 40.09 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 9.23 & 8.82 & 9.76 & 0.93 & 0.47 & 9.18 \\ \cline{1-1}  & rand & 14.68 & 14.23 & 15.27 & 1.04 & 0.52 & 14.56 \\ \cline{1-1}  & rands & 32.59 & 32.12 & 33.21 & 1.10 & 0.55 & 32.41 \\ \cline{1-1}  & bbox & 28.23 & 27.77 & 28.84 & 1.07 & 0.54 & 28.08 \\ \hline \multirow{8}{*}{FT-SAM} & center & 3.61 & 3.51 & 3.73 & 0.22 & 0.28 & 3.60 \\ \cline{1-1}  & rand & 8.47 & 8.39 & 8.58 & 0.19 & 0.34 & 8.43 \\ \cline{1-1}  & rands & 21.89 & 21.63 & 22.21 & 0.58 & 0.56 & 21.76 \\ \cline{1-1}  & bbox & 22.10 & 21.76 & 22.54 & 0.78 & 0.44 & 21.99 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Segmentation results on the CANDI dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{SAM} & center & 22.44 & 21.91 & 22.47 & 0.56 & 1.05 & 22.24 \\  & rand & 34.02 & 31.62 & 35.06 & 3.44 & 1.72 & 33.42 \\  & rands & 41.26 & 38.86 & 42.36 & 3.51 & 1.75 & 40.47 \\  & bbox & 75.11 & 75.12 & 75.19 & 0.07 & 0.63 & 74.64 \\ \cline{2-8}  & \multirow{3}{*}{MobileSAM} & center & 8.12 & 7.92 & 8.45 & 0.52 & 0.38 & 8.09 \\  & rand & 11.22 & 11.13 & 11.22 & 0.09 & 0.36 & 11.18 \\  & rands & 12.89 & 12.55 & 13.44 & 0.89 & 0.58 & 12.80 \\  & bbox & 68.67 & 68.15 & 69.73 & 1.58 & 0.79 & 68.19 \\ \cline{2-8}  & \multirow{3}{*}{TinySAM} & center & 19.00 & 18.29 & 19.09 & 0.80 & 1.17 & 18.80 \\  & rand & 27.53 & 25.54 & 28.34 & 2.81 & 1.41 & 27.17 \\  & rands & 31.82 & 31.62 & 31.86 & 0.24 & 1.06 & 31.50 \\  & bbox & 72.19 & 71.81 & 72.99 & 1.17 & 0.59 & 71.78 \\ \cline{2-8}  & \multirow{3}{*}{MedSAM} & center & 0.57 & 0.54 & 0.59 & 0.05 & 0.10 & 0.57 \\  & rands & 1.17 & 1.13 & 1.22 & 0.09 & 0.12 & 1.16 \\  & rands & 17.80 & 17.43 & 18.49 & 1.06 & 0.71 & 17.66 \\  & bbox & 46.77 & 46.31 & 46.97 & 0.66 & 0.95 & 46.30 \\ \cline{2-8}  & \multirow{3}{*}{SAM-Med2D} & center & 31.22 & 30.57 & 31.45 & 0.88 & 0.88 & 30.93 \\  & rand & 40.83 & 38.02 & 42.09 & 4.07 & 2.04 & 40.05 \\  & rands & 49.90 & 48.48 & 50.39 & 1.91 & 1.08 & 49.52 \\  & bbox & 47.20 & 45.43 & 47.87 & 2.44 & 1.22 & 46.78 \\ \hline \multirow{8}{*}{FT-SAM} & center & 15.61 & 15.27 & 16.22 & 0.95 & 0.47 & 15.54 \\  & rand & 18.60 & 18.24 & 18.73 & 0.49 & 0.27 & 18.56 \\ \cline{1-1}  & rands & 39.80 & 39.11 & 39.96 & 0.86 & 0.92 & 39.51 \\ \cline{1-1}  & bbox & 45.59 & 44.93 & 45.70 & 0.77 & 1.25 & 45.07 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Segmentation results on the KiTS dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Model** & **Prompt** & DSC\({}_{\text{Avg}}\) & DSC\({}_{\min}\) & DSC\({}_{\max}\) & DSC\({}_{\Delta}\) & DSC\({}_{\text{STD}}\) & DSC\({}_{\text{ES}}\) \\ \hline \multirow{8}{*}{SAM} & center & 24.54 & 23.00 & 25.71 & 2.70 & 1.46 & 24.13 \\  & rand & 24.62 & 23.03 & 25.82 & 2.79 & 1.40 & 24.22 \\  & rands & 34.45 & 34.03 & 35.21 & 1.18 & 0.82 & 34.16 \\  & bbox & 68.49 & 68.80 & 69.90 & 1.10 & 0.63 & 68.06 \\ \cline{2-8}  & \multirow{3}{*}{MobileSAM} & center & 14.13 & 14.32 & 14.65 & 0.34 & 0.49 & 14.02 \\  & rand & 13.03 & 13.01 & 13.54 & 0.53 & 0.60 & 12.91 \\  & rands & 20.56 & 19.23 & 20.34 & 1.11 & 0.62 & 20.40 \\  & bbox & 65.46 & 65.07 & 65.79 & 0.72 & 0.57 & 65.08 \\ \cline{2-8}  & \multirow{3}{*}{TinySAM} & center & 16.87 & 16.40 & 16.82 & 0.42 & 0.65 & 16.75 \\  & rands & 16.80 & 16.03 & 16.66 & 0.63 & 0.69 & 16.67 \\  & rands & 27.40 & 25.82 & 26.89 & 1.07 & 0.98 & 27.16 \\  & bbox & 67.20 & 67.11 & 67.69 & 0.58 & 0.46 & 66.90 \\ \cline{2-8}  & \multirow{3}{*}{MedSAM} & center & 0.94 & 0.82 & 0.97 & 0.15 & 0.17 & 0.94 \\  & rand & 1.06 & 0.87 & 1.01 & 0.14 & 0.15 & 1.05 \\  & rands & 23.45 & 22.95 & 24.17 & 1.21 & 0.80 & 23.26 \\  & bbox & 50.09 & 50.22 & 51.48 & 1.26 & 1.03 & 49.58 \\ \hline \multirow{8}{*}{SAM-Med2D} & center & 24.84 & 21.29 & 24.02 & 2.73 & 1.39 & 24.46 \\  & rand & 24.05 & 20.73 & 23.49 & 2.76 & 1.41 & 23.70 \\ \cline{1-1}  & rands & 39.51 & 37.86 & 40.43 & 2.56 & 1.32 & 39.04 \\ \cline{1-1}  & bbox & 31.03 & 32.87 & 33.81 & 0.94 & 0.89 & 30.77 \\ \hline \multirow{8}{*}{FT-SAM} & center & 11.36 & 8.45 & 10.06 & 1.61 & 0.84 & 11.25 \\ \cline{1-1}  & rand & 10.93 & 7.98 & 9.60 & 1.62 & 0.81 & 10.82 \\ \cline{1-1}  & rands & 20.31 & 15.64 & 18.48 & 2.84 & 1.42 & 20.01 \\ \cline{1-1}  & bbox & 36.70 & 32.52 & 35.32 & 2.80 & 1.71 & 35.99 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Segmentation results on the SPIDER dataset with 2D FMs.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline

## Appendix F Codebase

As depicted in Fig. 19, the FairMedFM codebase captures comprehensive modules for benchmarking the fairness of foundation models in medical image analysis. We build the codebase using PyTorch. For more details, please refer to our open-sourced repository: https://github.com/FairMedFM/FairMedFM.

1. **Dataloader** provides a consistent interface for loading and processing imaging data across various modalities and dimensions, supporting both classification and segmentation tasks.
2. **Model** is a one-stop library that includes implementations of the most popular pre-trained foundation models for medical image analysis.
3. **Usage Wrapper** encapsulates foundation models for various use cases and tasks, including linear probe, zero-shot inference, PEFT, promptable segmentation, etc.
4. **Trainer** offers a unified workflow for fine-tuning and testing wrapped models, and includes state-of-the-art unfairness mitigation algorithms.
5. **Evaluation** includes a set of metrics and tools to visualize and analyze fairness across different tasks.

We note that all the modules are designed to be easily replicated and extended. The following example demonstrates how to implement the wrapper for CLIP-Adapt with simple modifications.

``` classCLIPWrapper(BaseWrapper): def__init__(self,model,base_text_features): super()__init__(model) #zero-shotclass prototypes self.base_text_features =base_text_features #class prototypesaretrainableinCLIP-Adapt self.prototypes=nn.Parameter(base_text_features.clone()) forparaminself.model.parameters(): param.requires_grad=False defforward(self,x): returnsself.model.forward_clip(x,self.prototypes)

Figure 19: The structure of the open-source FairMedFM codebase.