SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT +
Footnote †: Submitted to 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Do not distribute.

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption.

## 1 Introduction

Modern artificial neural networks (ANNs) have been highly successful for a wide range of natural language processing (NLP) and computer vision (CV) tasks. However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference. In recent years, spiking neural networks (SNNs), arguably known as the third generation of neural network [Maas, 1997], have attracted a lot of attention due to their high biological plausibility, event-driven property and low energy consumption [Roy et al., 2019]. Like biological neurons, SNNs use discrete spikes to process and transmit information. Nowadays, neuromorphic hardware can be used to fulfill spike-based computing, which provides a promising way to implement artificial intelligence with much lower energy consumption.

Spiking neural networks have achieved great success in image classification task [Hu et al., 2018, Yin et al., 2020, Fang et al., 2021, Ding et al., 2021, Kim et al., 2022a, Zhou et al., 2022] and there have been some works [Plank et al., 2021, Lv et al., 2023, Zhu et al., 2023] that have demonstrated the efficacy of SNNs in language tasks partially. However, the backbone networks employed in SNNs for language tasks are overly simplistic, which significantly lowers the upper bound on the performance of their SNN models. For instance, the SNN used by Lv et al. [2023], which is built upon TextCNN [Kim, 2014], demonstrates a notable performance gap compared to those built on Transformer-based[Vaswani et al., 2017] large language models like BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] on multiple classification benchmarks.

Recently, Spikformer was proposed by Zhou et al. [2022], which first introduced Transformer architecture to SNNs and significantly narrowed the gap between SNNs and ViT [Dosovitskiy et al., 2020] on ImageNet [Deng et al., 2009] and CIFAR-10. We think that Spikformer provides the possibility to construct complex language representation models. As shown in Figure 1, considering the discrete nature of textual data, we improve the architecture of Spikformer to make it suitable for language tasks. we replace certain modules that were originally designed for image processing with language-friendly modules. Please see Section 3.2 for details on the improvement in network architecture. In general, a deeper ANN model often implies better performance. Increasing the depth of a ANN allows for the extraction of more complex and abstract features from the input data. However, Fang et al. [2020a] have shown that deep SNNs directly trained with backpropagation through time (BPTT) [Werbos, 1990] using surrogate gradients (See Section2.1) could suffer from the problem of gradient vanishing or exploding due to "self-accumulating dynamics". Therefore, we proposed to use knowledge distillation [Hinton et al., 2015] to train language Spikformers so that the deviation of surrogate gradients in Spikformer would not be rapidly accumulated[Qiu et al., 2023].

Inspired by the widely-used "pre-training + fine-tuning" recipe [Sun et al., 2019, Liu, 2019, Gururangan et al., 2020], we present a two-stage knowledge distillation strategy. In stage \(1\), we choose BERT as teacher model and the improved Spikformer as student model. We utilize a large collection of unlabelled texts to align features produced by two models in the embedding layer and multiple hidden layers. In stage \(2\), we use a BERT fine-tuned on a task-specific dataset as teacher and the model that completes stage \(1\) as student. At this stage, we first do the data augmentation for task-specific dataset and then employ the logits predicted by the teacher model to further guide the student model. After two-stage knowledge distillation, a spiking language model, named SpikeBERT, can be built by distilling knowledge from BERT. The experiment results show that SpikeBERT not only can outperform the state-of-the-art SNNs-like frameworks in text classification task but also achieve competitive performance to BERTs. The experiments of the ablation study (Section 4.5) also show that "pre-training distillation" plays an important role in training SpikeBERT.

The major contribution of this study can be summarized as follows:

* We improve the architecture of Spikformer for language processing and propose a two-stage, "pre-training + task-specific" knowledge distillation training method, in which the improved Spikformers are pre-trained on a huge collection of unlabelled texts before they are further fine-tuned on task-specific datasets by distilling the knowledge of feature extractions and predictive powers from BERTs.
* We empirically show that SpikeBERT achieved significantly higher performance than existing SNNs on \(6\) different language benchmark datasets for both English and Chinese.

Figure 1: (a) The architecture of Spikformer [Zhou et al., 2022]. (b) The architecture of our SpikeBERT. We improve Spikformer in its architecture, making it possible to process languages. Firstly, Spiking Patch Splitting (SPS) module was replaced with a word embedding layer so that the network can take discrete words (or tokens) as input. Secondly, we make the shape of the attention map yielded by Spiking Self Attention (SSA) to be \(N\times N\), rather than \(D\times D\), where \(D\) and \(N\) denote dimensionality of hidden layers and the length of inputs respectively. Lastly, the convolution layers were replaced with linear layers, and the batch normalization with layer normalization. \(L\) and \(L^{{}^{\prime}}\) denote the number of encoder blocks in Spikfomer and SpikeBERT, respectively.

* This study is among the first to show the feasibility of transferring the knowledge of BERT-like large language models to spiking-based architectures that can achieve comparable results but with much less energy consumption.

## 2 Related Work

### Spiking Neural Networks

SNNs use discrete spike trains instead of continuous decimal values to compute and transmit information. Spiking neurons, such as Izhikevich neuron (Izhikevich, 2003) and Leaky Integrate-and-Fire (LIF) neuron (Wu et al., 2017), are usually applied to generate spike trains. However, due to the non-differentiability of spikes, training SNNs has been a great challenge for the past two decades. Currently, there are two mainstream approaches to address this problem.

ANN-to-SNN ConversionANN-to-SNN conversion method (Diehl et al., 2015; Cao et al., 2015; Rueckauer et al., 2017; Hu et al., 2018) aims to convert weights of a well-trained ANN to its SNN counterpart by replacing the activation function with spiking neuron layers and adding scaling rules such as weight normalization (Diehl et al., 2016) and threshold constraints (Hu et al., 2018). This approach suffers from a large number of time steps during the conversion.

Backpropagation with Surrogate GradientsAnother popular approach is to utilize surrogate gradients (Neffci et al., 2019) during error backpropagation, enabling the entire procedure to be differentiable. Multiple surrogate gradients functions have been proposed, including the Sigmoid surrogate function (Zenke and Ganguli, 2017), Fast-Sigmoid (Zheng and Mazumder, 2018), and ATan (Fang et al., 2020). Backpropagation through time (BPTT) (Werbos, 1990) is one of the most popular methods for directly training SNNs(Shrestha and Orchard, 2018; Kang et al., 2022), which applies the traditional backpropagation algorithm (LeCun et al., 1989) to the unrolled computational graph. In recent years, several BPTT-like training strategies have been proposed, including SpatioTemporal Backpropagation (STBP) (Wu et al., 2017), STBP with Temporal Dependent Batch Normalization (STBP-tdBN) (Zheng et al., 2020), and Spatio-Temporal Dropout Backpropagation (STDB) (Rathi et al., 2020). These strategies have demonstrated high performance under specific settings. For more detailed information about Backpropagation Through Time (BPTT), please refer to Appendix A.

### Knowledge Distillation

Hinton et al. (2015) proposed the concept of knowledge distillation by utilizing the "response-based" knowledge (i.e., soft labels) of the teacher model to transfer knowledge. However, when this concept was first proposed, the features captured in the hidden layers were neglected, as they only focused on the final probability distribution at that time. To better learn from teacher models, some works (Zagoruyko and Komodakis, 2016; Heo et al., 2019; Chen et al., 2021) have advocated for incorporating hidden feature alignment during the distillation process. In addition, relation-based knowledge distillation has been introduced by Park et al. (2019), demonstrating that the interrelations between training data examples were also essential.

Recently, there have been a few studies (Kushawaha et al., 2020; Takuya et al., 2021; Qiu et al., 2023) in which knowledge distillation approaches were introduced to train SNNs. However, most of them focused on image classification task only, which cannot be trivially applied to language tasks. In this study, we propose a two-stage knowledge distillation approach to train the proposed SpikeBERT for text classification tasks, which is among the first ones to show the feasibility of transferring the knowledge to SNNs from large language models.

## 3 Method

In this section, we describe how we improve the architecture of Spikformer and introduce our two-stage distillation approach for training SpikeBERT. Firstly, we will depict how spiking neurons and surrogate gradients work in spiking neural networks. Then we will show the simple but effective modification of Spikformer to enable it to represent text information. Lastly, we will illustrate "pre-training + task-specific" distillation in detail.

### Spiking Neurons and Surrogate Gradients

Leaky integrate-and-fire (LIF) neuron (Wu et al., 2017) is one of the most widely used spiking neurons. Similar to the traditional activation function such as ReLU, LIF neurons operate on a weighted sum of inputs, which contributes to the membrane potential \(U_{t}\) of the neuron at time step \(t\). If membrane potential of the neuron reaches a threshold \(U_{\mathrm{thr}}\), a spike \(S_{t}\) will be generated:

\[S_{t}=\begin{cases}1,&\text{if }U_{t}\geq U_{\mathrm{thr}};\\ 0,&\text{if }U_{t}<U_{\mathrm{thr}}.\end{cases}\] (1)

We can regard the dynamics of the neuron's membrane potential as a resistor-capacitor circuit (Maas, 1997). The approximate solution to the differential equation of this circuit can be represented as follows:

\[U_{t}=I_{t}+\beta U_{t-1}-S_{t-1}U_{\mathrm{thr}},\quad I_{t}= WX_{t}\] (2)

where \(X_{t}\) are inputs to the LIF neuron at time step \(t\), \(W\) is a set of learnable weights used to integrate different inputs, \(I_{t}\) is the weighted sum of inputs, \(\beta\) is the decay rate of membrane potential, and \(U_{t-1}\) is the membrane potential at time \(t-1\). The last term of \(S_{t-1}U_{\mathrm{thr}}\) is introduced to model the spiking and membrane potential reset mechanism.

In addition, we follow Fang et al. (2020) and use Arctangent-like surrogate gradients function, which regards the Heaviside step function (Equation 1) as:

\[S\approx\frac{1}{\pi}\arctan(\frac{\pi}{2}\alpha U)+\frac{1}{2}\] (3)

Therefore, the gradients of \(S\) in Equation 3 are:

\[\frac{\partial S}{\partial U}=\frac{\alpha}{2}\frac{1}{(1+(\frac{\pi}{2} \alpha U)^{2})}\] (4)

where \(\alpha\) defaults to \(2\).

### SpikeBERT Architecture

Spikformer (Zhou et al., 2022) is the first hardware-friendly Transformer-based spiking neural network, whose architecture is shown in Figure 1 (a). The most crucial module is the Spiking Self Attention (SSA), which utilizes discrete spikes to implement the self-attention mechanism without employing a softmax function:

\[\begin{split}\mathrm{SSA}\left(Q_{s},K_{s},V_{s}\right)& =\mathrm{S(BN(MLP(}Q_{s}K_{s}^{T}V_{s}*\tau))\right)\\ Q_{s}=\mathrm{S_{Q_{s}}}\left(\mathrm{BN}\left(X_{s}W_{Q_{s}} \right)\right),& K_{s}=\mathrm{S_{K_{s}}}\left(\mathrm{BN}\left(X _{s}W_{K_{s}}\right)\right),\quad V_{s}=\mathrm{S_{V_{s}}}\left(\mathrm{BN} \left(X_{s}W_{V_{s}}\right)\right)\end{split}\] (5)

where \(\mathrm{S}\) is Heaviside step function like Equation 1, \(X_{s}\in\mathbb{R}^{T\times L\times D}\) is the input of SSA, \(T\) is number of time steps, \(\mathrm{BN}\) is batch normalization, \(\tau\) is a scaling factor. Outputs of SSA and \(Q_{s},K_{s},V_{s}\) are all matrix containing \(0\) and \(1\). \(W_{Q_{s}},W_{K_{s}},W_{V_{s}}\) and \(\mathrm{MLP}\) are all learnable decimal parameters.

We modify Spikformer so that it can effectively process textual data. Firstly, we replace Spiking Patch Splitting (SPS) module with a word embedding layer and a spiking neuron layer so that it can process sentences. Meanwhile, we find that the shape of the attention map in vanilla Spikformer is \(D\times D\) where \(D\) is the dimensionality of the hidden layers, which is unreasonable in language tasks. For language tasks, the features shared with words in different positions by attention mechanism are more important than those in different dimensions. Therefore, we reshape the attention map in Spiking Self Attention (SSA) module to \(N*N\) where \(N\) is the length of inputs. Lastly, we use linear layers and layer normalization (LN) instead of convolution layers and batch normalization(BN). We show the architecture of SpikeBERT in Figure 1 (b).

### Two-stage Distillation

Two-stage distillation is the key to enabling the student model with language processing ability. The first stage is to align the embeddings and hidden features between BERT and the improved Spikformer using a large-scale corpus. The second stage is to distill logits and cross-entropy information on a task-specific dataset from a fine-tuned BERT to the model finishing stage 1. We show the overview of our method in Figure 2.

#### 3.3.1 Stage 1. Pre-training Distillation

Given a pre-trained BERT [Devlin et al., 2019] irrelevant to downstream tasks as teacher \(TM\) and an improved Spikformer as student \(SM\), our goal in this stage is to align the embeddings and hidden features of \(TM\) and \(SM\) with a collection of unlabelled texts. We will introduce embedding alignment loss and feature alignment loss in the following.

Feature Alignment LossThis loss \(L_{fea}\) is to measure the similarity of features between \(TM\) and \(SM\) at every hidden layer. However, the shape of the student model's feature \(F_{sm}\) at every layer is \(T\times N\times D\) but that of BERT's feature \(F_{tm}\) is \(N\times D\), where \(T\) is the number of time steps, \(D\) is the dimensionality of hidden layers and \(L\) is sample length. What's more, \(F_{sm}\) is a matrix only containing \(0\) and \(1\) but \(F_{tm}\) is a decimal matrix. To address the issue of different dimensions between \(F_{tm}\) and \(F_{sm}\), as well as the disparity between continuous features of \(TM\) and discrete features of \(SM\), a transformation strategy is necessary. We follow the feature transformation approaches of Heo et al. [2019], Chen et al. [2021], Qiu et al. [2023] to map the features of \(TM\) and \(SM\) to the same content space:

\[F^{{}^{\prime}}_{tm}=F_{tm},\quad F^{{}^{\prime}}_{sm}=\mathrm{ LayerNorm}(\mathrm{MLP}(\sum_{t}^{T}(F^{t}_{sm})))\] (6)

However, we find it hard to align the features generated by the student model with those generated by BERT for the first few layers in this stage. We think that's because the student model might require more network layers to capture the essential features via the interaction among the inputs. As shown in Figure 2, we choose to ignore some front layers when calculating feature alignment loss. Assume

Figure 2: Overview of our two-stage distillation method (pre-training + task-specific distillation) for training SpikeBERT. \(T\) is the number of time steps of features in every layer. Notice that the logits loss and cross-entropy loss are only considered in stage \(2\). The varying shades of color represent the magnitude of the floating-point values. The dotted line under \(L^{i}_{fea}\) indicates that features of some hidden layers can be ignored when calculating feature alignment loss. If the student model contains different numbers of layers from the teacher model, we will align features every few layers.

BERT contains \(B\) Transformer blocks (i.e., \(B\) layers) and assume the student model contains \(M\) Spike Transformer Block. Therefore, we will align features every \(\lceil\frac{B}{M}\rceil\) layers if \(B>M\). For layer \(i\) in student model, its feature alignment loss is \(L_{fea}^{i}=||F_{tm}^{{}^{\prime}}-F_{sm}^{{}^{\prime}}||_{2}\).

Embedding Alignment LossAs discussed in Section 3.2, the embeddings of the input sentences are not in the form of spikes until they are fed forward into the Heaviside step function. Define \(E_{tm}\) and \(E_{sm}\) as the embeddings of teacher and student, respectively so the feature alignment loss is \(L_{fea}^{i}=||E_{tm}-\mathrm{MLP}(E_{sm})||_{2}\). The MLP layer is a transformation playing a similar role as that in Equation 6.

To sum up, in stage \(1\), the total loss \(L_{1}\) is the sum of chosen layer's feature alignment loss:

\[L_{1}=\sigma_{1}\sum_{i}L_{fea}^{i}+\sigma_{2}L_{emb}\] (7)

where the hyperparameters \(\sigma_{1}\) and \(\sigma_{2}\) are used to balance the learning of embeddings and features.

#### 3.3.2 Stage 2. Task-specific Distillation

In stage \(2\), we take a BERT fine-tuned on a task-specific dataset as the teacher model, and the model completed stage \(1\) as the student. To accomplish a certain language task, there should be a task-specific head over the basic language model as shown in Figure 2. For example, it is necessary to add an MLP layer over BERT for text classification. Besides, data augmentation is a commonly used and highly effective technique in knowledge distillationJiao et al. (2019); Tang et al. (2019); Liu et al. (2022). In the following, we will discuss our approach to data augmentation, as well as the logits loss and cross-entropy loss.

Data AugmentationIn the distillation approach, a small dataset may be insufficient for the teacher model to fully express its knowledgeBa and Caruana (2013). To tackle this issue, we augment the training set in order to facilitate effective knowledge distillation. We follow Tang et al. (2019) to augment the training set:

* Firstly, we randomly replace a word with \(\left[\mathrm{MASK}\right]\) token with probability \(p_{mask}\).
* Secondly, we replace a word with another of the same POS tag with probability \(p_{pos}\).
* Thirdly, we randomly sample an \(n\)-gram from a training example with probability \(p_{ng}\), where \(n\) is randomly selected from \(\left\{1,2,...,5\right\}\).

Logits LossFollowing Hinton et al. (2015), we take logits, also known as soft labels, into consideration, which lets the student learn the prediction distribution of the teacher. To measure the distance between two distributions, we choose KL-divergence: \(L_{logits}=\sum_{i}^{c}p_{i}log\left(\frac{p_{i}}{q_{i}}\right)\), where \(c\) is the number of categories, \(p_{i}\) and \(q_{i}\) denote the prediction distribution of the teacher model and student model.

Cross-entropy LossCross-entropy loss can help the student model learn from the samples in task-specific datasets: \(L_{ce}=-\sum_{i}^{c}\hat{q}_{i}log\left(q_{i}\right)\), where \(\hat{q}_{i}\) represents the one-hot label vector.

Therefore, the total loss \(L_{2}\) of stage \(2\) contains three terms:

\[L_{2}=\lambda_{1}\sum_{i}L_{fea}^{i}+\lambda_{2}L_{emb}+\lambda_{3}L_{logits}+\lambda_{4}L_{ce}\] (8)

where \(\lambda_{1}\), \(\lambda_{2}\), \(\lambda_{3}\), and \(\lambda_{4}\) are the hype-parameters that control the weight of these loss.

For both stages, we adopt backpropagation through time (BPTT), which is suitable for training spiking neural networks. You can see the detailed derivation in Appendix A if interested.

## 4 Experiments

We conduct four sets of experiments. The first is to evaluate the accuracy of SpikeBERT trained with the proposed method on \(6\) datasets of text classification datasets. The second experiment is to compare the theoretical energy consumption of BERT and that of SpikeBERT. The third experiment is an ablation study about the training process. The last experiment is to figure out how the performance of SpikeBERT is impacted by the number of time steps and model depth.

### Datasets

As mentioned in Section 3.3.1, a large-scale parallel corpus will be used to train student models in Stage 1. For the English corpus, we choose the "20220301.en" subset of Wikipedia1 and the whole Bookcorpus[Zhu et al., 2015], which are both utilized to pre-train a BERT [Devlin et al., 2019]. For the Chinese corpus, we choose Chinese-Wikipedia dump2 (as of Jan. 4, 2023). Additionally, we follow Lv et al. (2023) to evaluate the SpikeBERT trained with the proposed distillation method on six text classification datasets: MR[Pang and Lee, 2005], SST-2[Socher et al., 2013], SST-5, Subj, ChnSenti, and Waimai. The dataset details are provided in Appendix B.

Footnote 1: https://dumps.wikimedia.org/

Footnote 2: https://dumps.wikimedia.org/zhwiki/latest/

### Implementation Details

Firstly, we set the number of encoder blocks in SpikeBERT to \(12\). Additionally, we set the threshold of common spiking neurons \(U_{thr}\) as \(1.0\) but set the threshold of neurons in the spiking self-attention block as \(0.25\) in SpikeBERT. In addition, we set decay rate \(\beta=0.9\) and scaling factor \(\tau\) as \(0.125\). We also set the time step \(T\) of spiking inputs as \(4\) and sentence length to \(256\) for all datasets.

To construct SpikeBERT, we use two Pytorch-based frameworks: SnnTorch [Eshraghian et al., 2021] and SpikingJelly [Fang et al., 2020b]. Besides, we utilize bert-base-cased 3 from Huggingface as teacher model for English datasets and Chinese-bert-wurm-base4[Cui et al., 2019] for Chinese datasets.

Footnote 3: https://huggingface.co/brf1/chinese-bert-wurm

Footnote 4: https://huggingface.co/brf1/chinese-bert-wurm

In addition, we conduct pre-training distillation on \(4\) NVIDIA A100-PCIE GPUs and task-specific distillation on \(4\) NVIDIA GeForce RTX 3090 GPUs. Since surrogate gradients are required during backpropagation, we set \(\alpha\) in Equation 3 as \(2\). In stage \(1\), we set the batch size as \(128\) and adopt AdamW [Loshchilov and Hutter, 2017] optimizer with a learning rate of \(5e^{-4}\) and a weight decay rate of \(5e^{-3}\). The hyperparameters \(\sigma_{1}\) and \(\sigma_{2}\) in Equation 7 are both set to 1.0. In stage \(2\), we set the batch size as \(32\) and the learning rate to \(5e^{-5}\). For data augmentation, we set \(p_{mask}=p_{pos}=0.1\), \(p_{ng}=0.25\). To balance the weights of the four types of loss in Equation 8, we set \(\lambda_{1}=0.1\), \(\lambda_{2}=0.1\), \(\lambda_{3}=1.0\), and \(\lambda_{4}=0.1\).

### Results

We report in Table 1 the accuracy achieved by SpikeBERT trained with "pre-training + task-specific" distillation on \(6\) datasets, compared to \(2\) baselines: 1) SNN-TextCNN proposed by Lv et al. (2023); 2) improved Spikformer directly trained with gradient descent algorithm using surrogate gradients.

Table 1 demonstrates that the SpikeBERT trained with two-stage distillation achieves state-out-of-art performance across \(6\) text classification datasets. Compared to SNN-TextCNN, SpikeBERT achieved up to \(5.42\%\) improvement in accuracy (\(3.49\%\) increase on average) for all text classification benchmarks. Furthermore, SpikeBERT outperforms TextCNN, which is considered a representative

\begin{table}
\begin{tabular}{l|c c c c|c c|c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{5}{c|}{**English Dataset**} & \multicolumn{2}{c|}{**Chinese Dataset**} & \multirow{2}{*}{**Avg.**} \\ \cline{2-2} \cline{4-7}  & **MR** & **SST-2** & **Subj** & **SST-5** & **ChnSenti** & **Waimal** \\ \hline TextCNN [Kim, 2014] & \(77.41\) & \(83.25\) & \(94.00\) & \(45.48\) & \(86.74\) & \(88.49\) & \(79.23\) \\ FT BERT [Devlin et al., 2019] & \(87.63\) & \(92.31\) & \(95.90\) & \(50.41\) & \(89.48\) & \(90.27\) & \(84.33\) \\ \hline SNN-TextCNN [Lv et al., 2023] & \(75.45\) & \(80.91\) & \(90.60\) & \(41.63\) & \(85.02\) & \(86.66\) & \(76.71\) \\ Directly-trained Spikformer & \(76.38\) & \(81.55\) & \(91.80\) & \(42.02\) & \(85.45\) & \(86.93\) & \(77.36\) \\ SpikeBERT [Ours] & \(\bm{80.69}\) & \(\bm{85.39}\) & \(\bm{93.00}\) & \(\bm{46.11}\) & \(\bm{86.36}\) & \(\bm{89.66}\) & \(\bm{80.20}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Classification accuracy achieved by different methods on \(6\) datasets. A BERT model fine-tuned on the dataset is denoted as “FT BERT”. The improved Spikformer directly trained with surrogate gradients on the dataset is denoted as “Directly-trained Spikformer”. All reported experimental results are averaged across \(10\) random seeds.

artificial neural network, and even achieves comparable results to the fine-tuned BERT by a small drop of \(4.13\%\) on average in accuracy for text classification task. What's more, Table 1 demonstrates that SpikeBERT can also be applied well in Chinese datasets (ChnSenti and Waimai).

Fang et al. (2020) propose that, in image classification task, surrogate gradients of SNNs may lead to gradient vanishing or exploding and it is even getting worse with the increase of model depth. We found this phenomenon in language tasks as well. Table 1 reveals that the accuracy of directly-trained Spikformer is noticeably lower than SpikeBERT on some benchmarks, such as MR, SST-\(5\), and ChnSenti. This is likely because the directly-trained Spikformer models have not yet fully converged due to gradient vanishing or exploding.

### Energy Consumption

An essential advantage of SNNs is the low consumption of energy during inference. We compare the theoretical energy consumption per sample of fine-tuned BERT and SpikeBERT on \(6\) test datasets and report the results in Table 2. The way to calculate floating point operations (FLOPs), synaptic operations (SOPs), and the theoretical energy consumption (Power) is shown in Appendix C.

It is worth noting that the energy consumption of SpikeBERT is significantly lower than that of fine-tuned BERT, which is an important advantage of SNNs over ANNs in terms of energy efficiency. As shown in Table 2, SpikeBERT demands only \(25.00\%\) of the energy that fine-tuned BERT needs to achieve comparable performance on average. Moreover, on the Subj dataset, SpikeBERT can reduce energy consumption by up to \(77.17\%\) compared to fine-tuned BERT for predicting each text example. This indicates that SpikeBERT is a promising candidate for energy-efficient text classification in resource-constrained scenarios.

### Ablation Study and Impact of Hyper-parameters

In this section, we conduct ablation studies to investigate the contributions of: a) different stages of the proposed knowledge distillation method, and b) different types of loss in Equation 8.

As we can see in Table 4.5, SpikeBERTs without either stage \(1\) or stage \(2\) experience about \(3.20\%\) performance drop on average. Therefore, we conclude that the two distillation stages are both essential for training SpikeBERT. Furthermore, we observed that the average performance dropped from \(76.30\) to \(73.27\) when excluding the logits loss, demonstrating that the logits loss \(L_{logits}\) has the greatest impact on task-specific distillation. Meanwhile, data augmentation (DA) plays an important role in Stage \(2\), contributing to an increase in average performance from \(75.54\) to \(76.30\).

We investigate how the performance of SpikeBERT is affected by the two important hyperparameters: time steps \(T\) and model depth. To this end, we conduct two experiments: (a) varying the number of

\begin{table}
\begin{tabular}{l|l|c|c|c} \hline \hline
**Dataset** & **Model** & **FLOPs / SOPs(G)** & **Power (mJ)** & **Energy Reduction** & **Accuracy (\(\%\))** \\ \hline \multirow{2}{*}{**ChnSenti**} & FT BERT & \(22.46\) & \(103.38\) & \(89.48\) \\  & SpikeBERT & \(28.47\) & \(27.62\) & \(\mathbf{73.28\%\downarrow}\) & \(86.36\) \\ \hline \multirow{2}{*}{**Waimai**} & FT BERT & \(22.46\) & \(103.38\) & \(\mathbf{73.91\%\downarrow}\) & \(90.27\) \\  & SpikeBERT & \(27.81\) & \(26.97\) & \(\mathbf{73.91\%\downarrow}\) & \(89.66\) \\ \hline \multirow{2}{*}{**MR**} & FT BERT & \(22.23\) & \(102.24\) & \(\mathbf{74.93\%\downarrow}\) & \(87.63\) \\  & SpikeBERT & \(26.94\) & \(25.63\) & \(\mathbf{74.93\%\downarrow}\) & \(80.69\) \\ \hline \multirow{2}{*}{**SST-2**} & FT BERT & \(22.23\) & \(102.24\) & \(\mathbf{73.78\%\downarrow}\) & \(92.31\) \\  & SpikeBERT & \(27.46\) & \(26.81\) & \(\mathbf{73.78\%\downarrow}\) & \(85.39\) \\ \hline \multirow{2}{*}{**Subj**} & FT BERT & \(22.23\) & \(102.24\) & \(\mathbf{77.17\%\downarrow}\) & \(95.90\) \\  & SpikeBERT & \(25.92\) & \(23.34\) & \(\mathbf{77.17\%\downarrow}\) & \(93.00\) \\ \hline \multirow{2}{*}{**SST-5**} & FT BERT & \(22.23\) & \(102.24\) & \(\mathbf{76.92\%\downarrow}\) & \(50.41\) \\  & SpikeBERT & \(26.01\) & \(23.60\) & \(\mathbf{76.92\%\downarrow}\) & \(46.11\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Energy consumption per sample of fine-tuned BERT and SpikeBERT during inference on \(6\) text classification benchmarks. “FLOPs” denotes the floating point operations of fine-tuned BERT. “SOPs” denotes the synaptic operations of SpikeBERT. “Power” denotes the average theoretical energy required for each test example prediction.

the time steps of spike inputs when training SpikeBERT; and (b) training a variant of SpikeBERT with different encoder block depths, specifically \(6,12,18\), using our proposed two-stage method.

Figure 3 (a) shows how the accuracy of SpikeBERT varies with the increase of time steps. We find that, with the increase of time steps, the accuracy increases first, then remains unchanged, and reaches its maximum roughly at \(T=4\). Theoretically, the performance of SpikeBERT should be higher with bigger time steps. However, the performance of models with \(8\) and \(12\) time steps is even worse than that with \(4\) time steps on ChnSenti and Waimai datasets. A plausible explanation is that using excessively large time steps may introduce too much noise in the spike trains.

In addition, as we can see from Figure 3 (b), the accuracy of SpikeBERT is generally insensitive to the model depths and even gets lower in some datasets. We think that's because more spike Transformer blocks mean more spiking neurons (See Section 2.1), introducing more surrogate gradients when error backpropagation through time. Higher model depth often brings better model performance for traditional deep neural networks. However, it seems that deeper spiking neural networks cannot make further progress in performance. Many previous SNNs works (Zheng et al., 2020; Fang et al., 2020; Kim et al., 2022b) have proved this deduction.

## 5 Conclusion

In this study, we extended and improved Spikformer to process language tasks and proposed a new promising training paradigm for training SpikeBERT inspired by the notion of knowledge distillation. We presented a two-stage, "pre-training + task-specific" knowledge distillation method by transferring the knowledge from BERTs to SpikeBERT for text classification tasks. We empirically show that our SpikeBERT outperforms the state-of-the-art SNNs and can even achieve comparable results to BERTs with much less energy consumption across multiple datasets for both English and Chinese, leading to future energy-efficient implementations of BERTs or large language models.

\begin{table}
\begin{tabular}{c|c c c c|c|c} \hline \hline \multicolumn{1}{c|}{**Models**} & **MR** & **SST-2** & **Subl** & **SST-5** & **Avg.** & **Drop** \\ \hline SpikeBERT & \(80.69\) & \(85.39\) & \(93.00\) & \(46.11\) & \(76.30\) & – \\ \hline w/o Stage 1 & \(76.04\) & \(82.26\) & \(91.80\) & \(42.16\) & \(73.07\) & \(-3.23\) \\ w/o Stage 2 & \(75.91\) & \(82.26\) & \(91.90\) & \(42.58\) & \(73.14\) & -\(3.16\) \\ \hline \multirow{4}{*}{Stage 2} & w/o DA & \(80.22\) & \(84.90\) & \(92.20\) & \(44.84\) & \(75.54\) & -\(0.76\) \\  & w/o \(L_{feat}\) & \(78.35\) & \(83.48\) & \(92.20\) & \(43.57\) & \(74.40\) & -\(1.90\) \\ \cline{1-1}  & w/o \(L_{emb}\) & \(79.67\) & \(83.10\) & \(92.00\) & \(43.48\) & \(74.56\) & -\(1.74\) \\ \cline{1-1}  & w/o \(L_{logits}\) & \(76.19\) & \(82.64\) & \(91.90\) & \(42.35\) & \(73.27\) & -\(3.03\) \\ \cline{1-1}  & w/o \(L_{ce}\) & \(80.43\) & \(85.23\) & \(93.00\) & \(45.86\) & \(76.13\) & -\(0.17\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studies of the two-stage distillation method. Row \(3\) and \(4\) show ablation experiment results on the two steps of our proposed method. Row \(5\) to \(9\) are ablation experiment results on different parts of Equation 8. “DA” stands for data augmentation.

Figure 3: (a) Accuracy versus the number of time steps. (b) Accuracy influenced by model depth.

## References

* [1] OpenAI. Introducing chatgpt. 2022. URL https://openai.com/blog/chatgpt.
* [2] OpenAI. GPT-4 technical report. 2023. URL https://arxiv.org/abs/2303.08774.
* [3] Wofgang Maas. Networks of spiking neurons: the third generation of neural network models. _Neural Networks_, 14:1659-1671, 1997.
* [4] Kaushik Roy, Akhilesh R. Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence with neuromorphic computing. _Nature_, 575:607-617, 2019.
* [5] Yangfan Hu, Huajin Tang, and Gang Pan. Spiking deep residual networks. _IEEE Transactions on Neural Networks and Learning Systems_, 2018.
* [6] Bojian Yin, Federico Corradi, and Sander M. Boht'e. Effective and efficient computation with multiple-timescale spiking recurrent neural networks. _International Conference on Neuromorphic Systems 2020_, 2020.
* [7] Wei Fang, Zhaofei Yu, Yanqing Chen, Tiejun Huang, Timothee Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks. In _Neural Information Processing Systems_, 2021.
* [8] Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang. Optimal ann-snn conversion for fast and accurate inference in deep spiking neural networks. _ArXiv_, abs/2105.11654, 2021.
* [9] Youngeun Kim, Yuhang Li, Hyoungseob Park, Yeshwanth Venkatesha, and Priyadarshini Panda. Neural architecture search for spiking neural networks. _ArXiv_, abs/2201.10355, 2022a.
* [10] Zhaokun Zhou, Yue sheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Liuliang Yuan. Spikformer: When spiking neural network meets transformer. _ArXiv_, abs/2209.15425, 2022.
* [11] Philipp Plank, A. Rao, Andreas Wild, and Wolfgang Maass. A long short-term memory for ai applications in spike-based neuromorphic hardware. _Nat. Mach. Intell._, 4:467-479, 2021.
* [12] Changze Lv, Jianhan Xu, and Xiaoqing Zheng. Spiking convolutional neural networks for text classification. In _The Eleventh International Conference on Learning Representations_, 2023.
* [13] Rui-Jie Zhu, Qihang Zhao, and Jason K Eshraghian. Spikegpt: Generative pre-trained language model with spiking neural networks. _arXiv preprint arXiv:2302.13939_, 2023.
* [14] Yoon Kim. Convolutional neural networks for sentence classification. In _EMNLP_, 2014.
* [15] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _ArXiv_, abs/1706.03762, 2017.
* [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2019.
* [17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv_, abs/1907.11692, 2019.
* [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ArXiv_, abs/2010.11929, 2020.
* [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.

Wei Fang, Zhaofei Yu, Yanqing Chen, Timothee Masquelier, Tiejun Huang, and Yonghong Tian. Incorporating learnable membrane time constant to enhance learning of spiking neural networks. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 2641-2651, 2020a.
* Werbos [1990] Paul J. Werbos. Backpropagation through time: What it does and how to do it. _Proc. IEEE_, 78:1550-1560, 1990.
* Hinton et al. [2015] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. _ArXiv_, abs/1503.02531, 2015.
* Qiu et al. [2023] Haonan Qiu, Munan Ning, Li Yuan, Wei Fang, Yanqi Chen, Changlin Li, Tao Sun, Zhengyu Ma, and Yonghong Tian. Self-architectural knowledge distillation for spiking neural networks, 2023.
* Sun et al. [2019] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to fine-tune bert for text classification? In _China National Conference on Chinese Computational Linguistics_, 2019.
* Liu [2019] Yang Liu. Fine-tune bert for extractive summarization. _ArXiv_, abs/1903.10318, 2019.
* Gururangan et al. [2020] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. _ArXiv_, abs/2004.10964, 2020.
* Izhikevich [2003] Eugene M. Izhikevich. Simple model of spiking neurons. _IEEE transactions on neural networks_, 14:6:1569-72, 2003.
* Wu et al. [2017] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. _Frontiers in Neuroscience_, 12, 2017.
* Diehl et al. [2015] Peter Udo Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer. Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. _2015 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8, 2015.
* Cao et al. [2015] Yongqiang Cao, Yang Chen, and Deepak Khosla. Spiking deep convolutional neural networks for energy-efficient object recognition. _International Journal of Computer Vision_, 113:54-66, 2015.
* Rueckauer et al. [2017] Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. _Frontiers in Neuroscience_, 11, 2017.
* Diehl et al. [2016] Peter Udo Diehl, Guido Zarrella, Andrew S. Cassidy, Bruno U. Pedroni, and Emre O. Neftci. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. _2016 IEEE International Conference on Rebooting Computing (ICRC)_, pages 1-8, 2016.
* Neftci et al. [2019] Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. _IEEE Signal Processing Magazine_, 36:51-63, 2019.
* 1541, 2017.
* Zheng and Mazumder [2018] Nan Zheng and Pinaki Mazumder. A low-power hardware architecture for on-line supervised learning in multi-layer spiking neural networks. _2018 IEEE International Symposium on Circuits and Systems (ISCAS)_, pages 1-5, 2018.
* Shrestha and Orchard [2018] S. Shrestha and G. Orchard. Slayer: Spike layer error reassignment in time. _ArXiv_, abs/1810.08646, 2018.
* Zhang et al. [2018]Taewook Kang, Kwang-Il Oh, Jaejin Lee, and Wangrok Oh. Comparison between stdp and gradient-descent training processes for spiking neural networks using mnist digits. _2022 13th International Conference on Information and Communication Technology Convergence (ICTC)_, pages 1732-1734, 2022.
* LeCun et al. [1989] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. _Neural Computation_, 1:541-551, 1989.
* Zheng et al. [2020] Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking neural networks. In _AAAI Conference on Artificial Intelligence_, 2020.
* Rathi et al. [2020] Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation. _ArXiv_, abs/2005.01807, 2020.
* Zagoruyko & Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. _ArXiv_, abs/1612.03928, 2016.
* Heo et al. [2019] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1921-1930, 2019.
* Chen et al. [2021] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5006-5015, 2021.
* Park et al. [2019] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3962-3971, 2019.
* Kushawaha et al. [2020] R. K. Kushawaha, S. Kumar, Biplab Banerjee, and Rajbabu Velmurugan. Distilling spikes: Knowledge distillation in spiking neural networks. _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 4536-4543, 2020.
* Takuya et al. [2021] Sugahara Takuya, Renyuan Zhang, and Yasuhiko Nakashima. Training low-latency spiking neural network through knowledge distillation. _2021 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)_, pages 1-3, 2021.
* Fang et al. [2020b] Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Yonghong Tian, and other contributors. Spikingjelly, 2020b. Accessed: YYYY-MM-DD.
* Jiao et al. [2019] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In _Findings_, 2019.
* Tang et al. [2019] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy J. Lin. Distilling task-specific knowledge from bert into simple neural networks. _ArXiv_, abs/1903.12136, 2019.
* Liu et al. [2022] Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao. Multi-granularity structural knowledge distillation for language model compression. In _Annual Meeting of the Association for Computational Linguistics_, 2022.
* Ba & Caruana [2013] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In _NIPS_, 2013.
* Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. _2015 IEEE International Conference on Computer Vision (ICCV)_, pages 19-27, 2015.

Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In _ACL_, 2005.
* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _EMNLP_, 2013.
* Eshraghian et al. [2021] Jason Kamran Eshraghian, Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Bennamoun, Doo Seok Jeong, and Wei D. Lu. Training spiking neural networks using lessons from deep learning. _ArXiv_, abs/2109.12894, 2021.
* Cui et al. [2019] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-training with whole word masking for chinese bert. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:3504-3514, 2019.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2017.
* Kim et al. [2022b] Youngeun Kim, Joshua Chough, and Priyadarshini Panda. Beyond classification: Directly training spiking neural networks for semantic segmentation. _Neuromorphic Computing and Engineering_, 2(4):044015, 2022b.
* Indiveri et al. [2015] G. Indiveri, Federico Corradi, and Ning Qiao. Neuromorphic architectures for spiking deep neural networks. _2015 IEEE International Electron Devices Meeting (IEDM)_, pages 4.2.1-4.2.4, 2015.

[MISSING_PAGE_EMPTY:14]

* **MR**: MR stands for Movie Review and it consists of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating [Pang and Lee, 2005].
* **SST-5**: SST-\(5\) contains \(11,855\) sentences extracted from movie reviews for sentiment classification [Socher et al., 2013]. There are \(5\) categories (very negative, negative, neutral, positive, and very positive).
* **SST-2**: The binary version of SST-\(5\). There are just \(2\) classes (positive and negative).
* **Subj**: The task of this dataset is to classify a sentence as being subjective or objective5. Footnote 5: https://www.cs.cornell.edu/people/pabo/movie-review-data/
* **ChnSenti**: ChnSenti comprises about \(7,000\) Chinese hotel reviews annotated with positive or negative labels6. Footnote 6: https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv
* **Waimai**: There are about \(12,000\) Chinese user reviews collected by a food delivery platform for binary sentiment classification (positive and negative)7 in this dataset.

Footnote 7: https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_1&k/wsimai_1&k.csv

## Appendix C Theoretical Energy Consumption Calculation

For spiking neural networks (SNNs), the theoretical energy consumption of layer \(\xi\) can be calculated as

\[\text{Power}(\xi)=0.9\text{pJ}\times\text{SOPs}(\xi)\] (16)

where \(0.9\text{pJ}\) is the energy consumption per synaptic operation (SOP) [Indiveri et al., 2015, Hu et al., 2018, Zhou et al., 2022]. The number of synaptic operations at the layer \(\xi\) of an SNN is estimated as

\[\text{SOPs}(\xi)=T\times\gamma\times\text{FLOPs}(\xi)\] (17)

where \(T\) is the number of times step required in the simulation, \(\gamma\) is the firing rate of input spike train of the layer \(\xi\), and FLOPs\((\xi)\) is the estimated floating point operations at the layer \(\xi\).

For classical artificial neural networks, the theoretical energy consumption required by the layer \(\xi\) can be estimated by

\[\text{Power}(\xi)=4.6\text{pJ}*\text{FLOPs}(\xi)\] (18)

Note that \(1\text{J}=10^{3}\) mJ \(=10^{12}\) pJ.

## Appendix D Discussion of Limitations

In the image classification task, spiking neural networks have demonstrated comparable performance to ViT on CIFAR-10-DVS and DVS-128-Gesture datasets, which are neuromorphic event-based image datasets created using dynamic vision sensors. We think that the performance gap bewteen SNNs and ANNs in language tasks is mainly due to the lack of neuromorphic language datasets. It is unfair to evaluate SNNs on the datasets that were created to train and evaluate ANNs because these datasets are mostly processed by continuous values. However, it is quite hard to convert language to neuromorphic information without information loss. We hope there will be a new technology to transfer sentences to neuromorphic spikes.

In addition, GPU memory poses a limitation in our experiments. Spiking neural networks have an additional dimension, denoted as \(T\) (time step), compared to artificial neural networks. Increasing the number of time steps allows for capturing more information but results in an increased demand for GPU memory by a factor of \(T\). During our experiments, we observe that maintaining the same number of time steps during training requires reducing the sentence length of input sentences, which significantly constrains the performance of our models. We remain optimistic that future advancements will provide GPUs with sufficient memory to support the functionality of SNNs.