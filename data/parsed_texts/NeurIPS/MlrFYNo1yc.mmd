# Minimum norm interpolation by perceptra:

Explicit regularization and implicit bias

Jiyoung Park

Department of Statistics

Texas A&M University

wldydd15510@tamu.edu

&Ian Pelakh

Department of Mathematics

Iowa State University

ispelakh@iastate.edu

&Stephan Wojtowytsch

Department of Mathematics

University of Pittsburgh

s.woj@pitt.edu

###### Abstract

We investigate how shallow ReLU networks interpolate between known regions. Our analysis shows that empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow. With and without explicit regularization, we numerically study the implicit bias of common optimization algorithms towards known minimum norm interpolants.

## 1 Introduction

Modern neural networks mostly operate in an overparametrized regime, i.e. they possess more tunable parameters than the number of data points contributing to the loss function. Safran and Shamir (2018); E et al. (2019); Du et al. (2018); Chizat and Bach (2018) associate overparametrization with better training properties, and Belkin et al. (2019, 2020) find it to enhance statistical generalization (see also (Loog et al., 2020) for historical context). For many architectures, overparametrization leads to the ability to fit any values \(y_{i}\) at a given set of data points \(\{x_{1},\ldots,x_{n}\}\), and Cooper (2018) shows that generically, the set of weights for which the neural network interpolates prescribed values is a submanifold of high dimension and co-dimension in parameter space. Which solution on the manifold is dynamically chosen by an optimization algorithm, and which solutions have favorable generalization properties, is an active area of research in theoretical machine learning.

Current practice is to estimate a model's generalization to previously unseen data by assessing its performance on a hold-out set (a posteriori error estimate) or by using uniform estimates on the generalization of all elements of a function class (a priori estimate if the function class is encoded ahead of time by explicit regularization, a posteriori if membership is determined after optimization). Neither approach yields information on what a neural network does outside the support of the data distribution, a topic of great interest for the study of distributional shift and adversarial stability.

The main contribution of this work is split between two complementary lines of investigation:

1. We prove that neural network minimizers of regularized empirical risk functionals converge to minimum norm interpolants of a given target function in an infinite parameter limit (Section 2). Our result improves previous works to more general settings (See Remark 2.2).
2. Training a neural network generally corresponds to solving a non-convex minimization problem. While we provide convergence guarantees for empirical risk minimizers, in general there is no guarantee that a training algorithm finds a global minimizer of an empirical risk functional. Even if convergence holds, it is unclear _which_ minimizer is selected (in the overparametrized regime, where the set of minimizers is a high-dimensional manifold). In settings where the minimum norm interpolant is known (Section 3), we compare numerical solutions to theoretical predictions to better understand (1) the predictivepower of theoretically studying empirical risk minimizers and (2) the implicit bias of different optimization algorithms (Sections 4 and 5). We believe this to be a useful benchmark problem for a better understanding of explicit regularization and implicit bias in optimization.

Minimum norm interpolants are the regression analogue to maximum margin classifiers. They are associated with favorable generalization properties and relative stability even against adversarial perturbations.

### Previous work

Minimum norm interpolation.In classes of linear functions, minimum norm interpolation has a long history as ridge regression (minimal \(\ell^{2}\)-norm) or as the least absolute shrinkage selection and operator (LASSO, minimal \(\ell^{1}\)-norm). To the best of our knowledge, minimum norm interpolation by neural networks has only been studied for shallow neural networks in one dimension by Hanin (2021); Debarre et al. (2022); Boursier and Flammarion (2023) and in odd dimension for certain radially symmetric data by Wojtowytsch (2022). For classification problems, minimum norm/maximum margin classifiers were considered by E and Wojtowytsch (2022). For finite datasets, the set of minimum norm interpolants was characterized by Parhi and Nowak (2021). A parametrization of the same function class by neural networks with multiple linear layers and a single ReLU layer induces different concepts of minimum norm interpolation as studied by Ongie and Willett (2022).

Implicit bias.The implicit bias of parameter optimization algorithms has been studied for gradient flows with infinitely wide, but shallow ReLU networks by Chizat et al. (2019); Jin and Montifar (2020) and for stochastic gradient descent and diagonal linear networks by Pesme et al. (2021). Chizat and Bach (2020) prove convergence to a maximum margin classifier for infinitely wide ReLU networks with one hidden layer if the parameters follow the gradient flow of an (unregularized) logistic loss risk functional. Many authors, including Damian et al. (2021); Li et al. (2021); Wu et al. (2022) and Wojtowytsch (2020), study the bias of SGD towards solutions at which the loss landscape is 'flat' in the parameter space. Hochreiter and Schmidhuber (1997) conjectured such minimizers to have favorable generalization properties. In many cases, minimizers tend to be flatter if the parameters associated to them are not excessively large. Yang et al. (2021) describe several phase-transitions in parameter space for the relation between flatness in parameter space and generalization. Zhou et al. (2020) compare the implicit bias of SGD and ADAM. Smith et al. (2021); Barrett and Dherin (2020) argue that gradient descent resembles a gradient flow in a modified (regularized) loss landscape more closely than the gradient flow of the original loss function. Hajjar and Chizat (2022) investigate the role of symmetries in optimization.

Barron space.The Barron class is adapted to ReLU networks with a single hidden layer and weights of bounded average magnitude. Slightly different versions of the same function space have been studied by Bach (2017); E et al. (2019, 2020); Ongie et al. (2019); E and Wojtowytsch (2020, 2021); Caragea et al. (2020); Parhi and Nowak (2021); Wojtowytsch (2022); Siegel and Xu (2022, 2023) under various names such as \(\mathcal{F}_{1}\), Radon-BV or the variation space of the ReLU dictionary.

### Preliminaries

Conventions.\(\mu\) always denotes a \(\sigma^{2}\)-sub-Gaussian probability measure on the data domain \(\mathbb{R}^{d}\), i.e.

\[\exists\;C,\sigma>0\qquad\text{s.t.}\quad\mathbb{E}_{X\sim\mu}\big{[}\exp \left(\lambda\{\|X\|-\mathbb{E}\|X\|\}\right)\big{]}\leq C\,\exp\left(\frac{ \lambda^{2}\sigma^{2}}{2}\right)\qquad\forall\;\lambda>0.\]

All norms are Frobenius norms (\(\ell^{2}\)-norm for vectors). For \(n\in\mathbb{N}\), \(S_{n}=\{x_{n,1},\ldots,x_{n,n}\}\) is a set of \(n\) iid samples from the distribution \(\mu\), independent of \(S_{n^{\prime}}\) for \(n^{\prime}\neq n\). When unambiguous, we denote \(x_{n,i}=x_{i}\). We take \(\ell(f,y)=|f-y|^{2}\) as the mean squared error/\(\ell^{2}\)-loss function, but we remark that the theoretical analysis remains valid if \(\ell_{MSE}\) is replaced by \(\ell^{1}\)-loss or a Huber or pseudo-Huber loss

\[\ell_{Hub}(f,y)=\begin{cases}|f-y|^{2}&\text{if }|y-f|<1\\ 2|y-f|-1&\text{if }|y-f|\geq 1\end{cases},\qquad\ell_{pH}(y,h)=\sqrt{1+|y-f|^{2}}-1.\]

For \(m\in\mathbb{N}\) and \((a,W,b)\in\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\times\mathbb{R}^{m+1}\), let

\[f_{(a,W,b)}:\mathbb{R}^{d}\to\mathbb{R},\quad f_{(a,W,b)}(x)=b_{0}+\sum_{i=1} ^{m}a_{i}\,\sigma(w_{i}\cdot x+b_{i}),\quad\sigma(z)=\operatorname{ReLU}(z)= \max\{z,0\},\]i.e. \(f_{(a,W,b)}\) is a ReLU network with a single hidden layer and weights \(a,W\) and biases \(b\). The vector \(w_{i}\in\mathbb{R}^{d}\) is the \(i\)-th row of the matrix \(W\). For \(m,n\in\mathbb{N}\) and \(\lambda\geq 0\), we denote the regularized empirical risk functional as \(\widehat{\mathcal{R}}_{n,m,\lambda}:\mathbb{R}^{m}\times\mathbb{R}^{m\times d }\times\mathbb{R}^{m+1}\rightarrow[0,\infty)\):

\[\widehat{\mathcal{R}}_{n,m,\lambda}(a,W,b)=\frac{1}{2n}\sum_{i=1}^{n}\ell \left(f_{(a,W,b)}(x_{i}),\;y_{i}\right)+\frac{\lambda}{2}\left(\|a\|_{2}^{2}+ \|W\|_{Frob}^{2}\right).\]

Concepts.We introduce what we dub 'homogeneous Barron space' \(\mathcal{B}\) heuristically here, and in greater detail in Appendix C. As a measure of magnitude of the function, we consider the weight decay (or Tikhonov) regularizer \(\frac{1}{2}\big{(}\|a\|_{2}^{2}+\|W\|_{Frob}^{2}\big{)}\) of the parametrized function \(f_{(a,W,b)}\) which does not control the magnitude of the bias vector.

Note that the function class \(f_{(a,W,b)}\) and its complexity do not change when we consider representations of the form \(f_{m}(x)=\frac{1}{m}\sum_{i=1}^{m}a_{i}\,\sigma(w_{i}\cdot x+b_{i})\) with a regularizer \(\frac{1}{2m}\big{(}\|a\|_{2}^{2}+\|W\|_{Frob}^{2}\big{)}=\frac{1}{2m}\sum_{i=1 }^{m}\left(a_{i}^{2}+\|w_{i}\|_{2}^{2}\right)\). A continuum analogue to these functions represented as an 'empirical average' over individual neurons is a general expectation representation \(f_{\pi}(x)=\mathbb{E}_{(a,w,b)\sim\pi}\big{[}a\,\sigma(w^{T}x+b)\big{]}\) for some probability distribution \(\pi\) on parameter space and a regularizer \(\frac{1}{2}\mathbb{E}_{(a,w,b)\sim\pi}[a^{2}+\|w\|_{2}^{2}]\). As the parametrization of a function by a neural network is generally non-unique, we define the Barron semi-norm \([f]_{\mathcal{B}}=\inf_{\{\pi:f_{\pi}=f\}}\mathbb{E}_{(a,w,b)\sim\pi}[a^{2}+\| w\|_{2}^{2}]\) as the lowest value attained by the regularizer over all possible parametrizations. For a more comprehensive understanding, interested readers may refer to Appendix C.

## 2 General convergence result

We first state a general convergence result to a minimum norm interpolant of given data generated by functions in the homogeneous Barron class \(\mathcal{B}\).

**Theorem 2.1**.: _Take \(\mu,S_{n},\widehat{\mathcal{R}}_{n,m,\lambda}\) as in Section 1.2, \(f^{*}\in\mathcal{B}\), and let \(y_{i}=f^{*}(x_{i})\) for \(i=1,\ldots,n\). Assume that \(m,\lambda\) are parameters which scale with \(n\) as \(m_{n},\lambda_{n}\) such that_

\[\lim_{n\rightarrow\infty}\left(\lambda_{n}+\frac{1}{m_{n}}\right)=0,\qquad \lim_{n\rightarrow\infty}\left(\frac{1}{\lambda_{n}\,m_{n}}+\frac{\log n}{ \lambda_{n}\,\sqrt{n}}\right)=0.\] (1)

_Then almost surely over the random selection of data points in \(S_{n}\), the following holds: If \((a,W,b)_{n}\in\operatorname*{argmin}\widehat{\mathcal{R}}_{n,m_{n},\lambda_{n}}\) for all \(n\in\mathbb{N}\), then every subsequence of \(f_{n}:=f_{(a,W,b)_{n}}\) has a further subsequence which converges to some limit \(\hat{f}^{*}\in\mathcal{B}\) with \(\hat{f}^{*}=f^{*}\)\(\mu\)-almost everywhere and \([\hat{f}^{*}]_{\mathcal{B}}\subseteq[f^{*}]_{\mathcal{B}}\). Convergence holds in \(L^{p}(\mu)\) for all \(p<\infty\) and uniformly on compact subsets of \(\mathbb{R}^{d}\). If \(\mathbb{E}_{\mu}\|x\|+\sigma^{2}\geq 1\), then for all \(n\geq 2\), the following explicit bound holds up to higher order terms in \(n,m=m_{n},\lambda=\lambda_{n}\) with probability at least \(1-1/n^{2}\):_

\[\|f_{(a,W,b)_{n}}-f^{*}\|_{L^{2}(\mu)}^{2}\leq C\left(\frac{[f^{*}]_{\mathcal{B }}^{2}}{m}\,\mathbb{E}_{\mu}\big{[}\|x\|^{2}\big{]}+[f^{*}]_{\mathcal{B}}^{2} \left(\mathbb{E}_{\mu}\|x\|+\sigma^{2}\right)\frac{\log n}{\sqrt{n}}\,+\lambda \,[f^{*}]_{\mathcal{B}}\right).\] (2)

**Remark 2.2**.: _Theorem 2.1 extends previous results of E et al. [2019a] in several ways:_

1. _We allow for general sub-Gaussian rather than compactly supported data distributions._
2. _We do not control the magnitude of the bias variables._
3. _Our results apply to_ \(\ell^{2}\)_-loss, which is neither globally Lipschitz-continuous nor bounded._
4. _In a limiting regime, we characterize how the empirical risk minimizers interpolate in the region where no data is given by proving uniform convergence to a minimum norm interpolant._

_In sum, the first three points necessitate a more careful technical analysis than in the settings of prior work, and the unboundedness of data and loss introduces additional logarithmic terms not present for E et al. [2019a]. To the best of our knowledge, the last point is novel and our work is the first to use the notion of \(\Gamma\)-convergence in this context._If the data-distribuion \(\mu\) is supported on the entire space \(\mathbb{R}^{d}\) (e.g. a non-degenerate Gaussian), then \(\hat{f}^{*}\equiv f^{*}\). In many cases, however, \(\mu\) is supported on a small, potentially compact and generally low-dimensional subset \(M\) of the data space. In this case, the function \(f^{*}\) is only known on the closed set \(M\subsetneq\mathbb{R}^{d}\). As a result, there are many \(f\in\mathcal{B}\) such that \(f\equiv f^{*}\) on \(M\) while \(f\neq f^{*}\) on \(\mathbb{R}^{d}\) in general. The subsequential limit is one of these functions which has a minimal semi-norm \([f]_{\mathcal{B}}\).

Thus, beyond knowing that \(f_{n}\) asymptotically fits the function \(f^{*}\) perfectly at known data, Theorem 2.1 provides information about how it may interpolate at points where \(\mu\) has no information. Such knowledge is of interest when a population may naturally evolve in time (distributional shift) of if \(f_{n}\) is applied to a new problem with similar features but distinct geometry (transfer learning).

The proof of Theorem 2.1 is given in Appendix E. In the proof, we combine a direct approximation theorem to construct risk competitors with Rademacher complexity-based generalization bounds. Concentration inequalities are used to bound tail quantities. The scaling conditions that \(\frac{\log n}{\sqrt{n}}\ll\lambda\) and \(\frac{1}{m}\ll\lambda\) are used to ensure that the risk functional has sufficiently strong regularization. They can be weakened if the data measure \(\mu\) is supported on a finite set of points or the function \(f^{*}\) can be represented by a neural network with a finite number of neurons respectively. For instance, an analogous statement holds with a simpler proof for a fixed data-set \(S\) of \(n\) data points if \(\lambda\) is coupled to \(m\) such that \(\frac{1}{m\lambda_{m}}\to 0\). In this case, we take the empirical distribution \(\mu=\frac{1}{n}\sum_{x\in S}\delta_{x}\) as the population and do not need to bound the generalization gap. This model can be considered appropriate when \(n\ll m\) (heavy overparametrization). A precise statement is given in Appendix F.

The proof remains valid if we only assume that

\[\limsup_{n\to\infty}\frac{1}{\lambda_{n}}\widehat{\mathcal{R}}_{n,m_{n}, \lambda_{n}}(a_{n},W_{n},b_{n})=\inf\left\{[f]_{\mathcal{B}}:f\equiv f^{*}, \quad\mu-\text{almost everywhere}\right\},\]

i.e. if \((a,W,b)_{n}\) parametrizes a function of low excess risk. In the proof, we obtain a more precise version of (2). Using an a priori Lipschitz-bound, it is possible to obtain a rate of convergence in \(L^{p}(\mu)\) for \(p<\infty\) by interpolation. For uniform convergence on compact sets outside the support of \(\mu\), we do not obtain a rate in this work.

The proof of uniform convergence on compact sets utilizes the notion of \(\Gamma\)-convergence from the calculus of variations, a very stable notion of convergence which implies that minimizers of approximating functionals converge to the minimizer of a limiting functional. This notion has recently made inroads into machine learning applications and was used e.g. by Neumayer et al. (2023).

All results can easily be generalized to any more general function class which admits the three key ingredients: A bound on its Rademacher complexity, a compact embedding theorem, and a direct approximation theorem.

## 3 Minimum norm interpolants

### One-dimensional example

In one dimension, (Wojtowytsch, 2022, Proposition 2.5) shows that \([f]_{\mathcal{B}}=\int_{-\infty}^{\infty}|f^{\prime\prime}(x)|\,\mathrm{d}x\) for any smooth function \(f:\mathbb{R}\to\mathbb{R}\) which satisfies \(f^{\prime}(x)=0\) at some point \(x\in\mathbb{R}\) - see also previous work by Li et al. (2020); E and Wojtowytsch (2020). This one-dimensional case is, in fact, the simplest case of a general characterization of Barron functions in any dimension by Ongie et al. (2019).

Consider the task of minimizing \([f]_{\mathcal{B}}\) under the condition that \(f(x)=|x|\) if \(|x|\geq 1\). Then the minimum is attained for any smooth convex function \(f\) which satisfies the constraint since

\[2=f^{\prime}(1)-f^{\prime}(-1)=\int_{-1}^{1}f^{\prime\prime}(x)\,\mathrm{d}x \leq\int_{-1}^{1}|f^{\prime\prime}(x)|\,\mathrm{d}x=[f]_{\mathcal{B}}\]

with equality if and only if \(f^{\prime\prime}\geq 0\). The same estimate holds for non-smooth Barron functions if the second derivative is interpreted as a Radon measure. For piecewise linear functions, this corresponds to summing \(|f^{\prime}(x_{i}^{+})-f^{\prime}(x_{i}^{-})|\) over the non-smooth points \(x_{i}\). More generally, the set of minimum norm interpolants of one-dimensional convex data was characterized by Savarese et al. (2019).

**Proposition 3.1**.: _Let \(x_{0}<\cdots<x_{n}\) and \(y_{i}=f^{*}(x_{i})\) for a convex function \(f^{*}\) and \(i=0,\ldots,n\). If \(y_{1}<y_{0}\) and \(y_{n}>y_{n-1}\), then \(f\) is a minimum Barron norm interpolant of the dataset \(\{(x_{i},y_{i})\}_{i=0}^{n}\)_if and only if \(f\) is convex, \(f(x_{i})=y_{i}\) for all \(i=0,\ldots,n\) and_

\[f^{\prime}(x)=\frac{y_{1}-y_{0}}{x_{1}-x_{0}}\quad\text{ for }x<x_{1}\qquad\text{ and }\qquad f^{\prime}(x)=\frac{y_{n}-y_{n-1}}{x_{n}-x_{n-1}}\quad\text{ for }x>x_{n-1}.\]

The two given slopes are the largest values that are required for derivatives at any point. We give a proof of Proposition 3.1 in Appendix G. In full generality, minimum norm solutions have been characterized by Hanin (2021) using matching convexities to achieve minimal total curvature.

In a recent article, Boursier and Flammarion (2023) show that if the full Barron norm is controlled, i.e. if the magnitude of biases is included in the regularizer, a specific convex function is selected. This corresponds to minimizing a functional

\[\int_{-1}^{1}\left|f^{\prime\prime}(x)\right|\sqrt{1+x^{2}}\,\mathrm{d}x \qquad\text{rather than}\quad\int_{-1}^{1}\left|f^{\prime\prime}(x)\right| \mathrm{d}x.\]

The first functional prefers \(f^{\prime\prime}\) to be large close to the origin, if it has to be large anywhere. All break points occur as close to the origin as possible, in particular: Either at the origin or at data points. Unlike the Barron semi-norm penalty studied in this work, which does not select a specific minimum norm interpolant, the Barron norm penalty thus induces a _sparse_ neural network interpolant.

### Radially symmetric bump function

Another setting where we have explicit minimum norm interpolant is when we fit a bump function for radially symmetric data. Recall a result of Wojtowytsch (2022) on minimum norm fitting of certain radially symmetric data.

**Proposition 3.2**.: _(_Wojtowytsch_,_ 2022_, Theorem 3.1)_ _Let \(d\geq 3\) be an odd integer and_

\[\mathcal{F}=\big{\{}f\in C_{c}(\mathbb{R}^{d}):f(0)=1\text{ and }f(x)=0\text{ if } \|x\|\geq 1\big{\}}.\]

_Then there exists a unique radially symmetric function \(f_{d}^{*}:\mathbb{R}^{d}\to\mathbb{R}\) such that \(f_{d}^{*}\in\operatorname*{argmin}_{f\in\mathcal{F}}[f]_{\mathcal{B}}\). The norm of minimizers grows as \(\lim_{d\to\infty}[f_{d}^{*}]_{\mathcal{B}}/d\approx 3.7\)._

We note that the existence of minimum norm interpolants which are not radially symmetric is not excluded, but if \(\hat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}[f]_{\mathcal{B}}\) is any other minimum norm interpolant, then its radial average \(\operatorname*{Av}\hat{f}\) coincides with \(f_{d}^{*}\): \(\operatorname*{Av}\hat{f}\equiv f_{d}^{*}\). Here

\[\operatorname*{Av}f(x):=\int_{SO(d)}f(Ox)\,\mathrm{d}H_{O}=\int_{S^{d-1}}f \big{(}|x|\cdot\nu\big{)}\,\mathrm{d}\pi_{\nu}^{0}\] (3)

where \(H\) is the uniform distribution (Haar measure) on the group of rotations and \(\pi^{0}\) is the uniform distribution on the \(d-1\)-dimensional sphere in \(\mathbb{R}^{d}\). In (Wojtowytsch, 2022, Section 6), an algorithm is given to find the minimum norm interpolant \(f_{d}^{*}\) by numerically solving a one-dimensional polynomial approximation problem and a linear system of moment conditions.

The uniqueness statement allows us to strengthen the result of Theorem 2.1 in this case. A natural setting is to use a sub-Gaussian data distribution \(\mu\) which gives positive mass to the origin, but has no mass elsewhere in the unit ball. It should have mass everywhere outside the unit ball. Under this natural setting, we have a stronger result of Theorem 2.1.

**Corollary 3.3**.: _Take \(\mu,S_{n},\widehat{\mathcal{R}}_{n,m,\lambda}\) as in Section 1.2, and assume in addition that_

1. \(\mu(\{0\})>0\) _(positive mass at the origin)._
2. \(\mu(B_{1}(0)\setminus\{0\})=0\) _(no mass elsewhere in the unit ball)._
3. \(\mu(U)>0\) _for any open set_ \(U\subseteq\mathbb{R}^{d}\setminus\overline{B_{1}(0)}\) _(mass everywhere outside the unit ball)._

_Assume that \(m,\lambda\) scale with \(n\) as in (1). Almost surely over the random selection of data points in \(S_{n}\), the following holds: If \((a,W,b)_{n}\in\operatorname*{argmin}\widehat{\mathcal{R}}_{n,m_{n},\lambda_{n}}\) for all \(n\in\mathbb{N}\), then sequence of radial averages \(\operatorname*{Av}f_{n}\) of \(f_{n}:=f_{(a,W,b)_{n}}\) converges to \(f_{d}^{*}\) as in Proposition 3.2. Convergence holds in \(L^{2}(\mu)\) for MSE loss (with an explicit rate) and uniformly on compact subsets of \(\mathbb{R}^{d}\) (without a rate in this work)._In Corollary 3.3, we guarantee convergence to the unique radial minimum norm interpolant \(f_{d}^{*}\) (at least for the radial average), while in Theorem 2.1 we may have different subsequences that converge to different minimum norm interpolants. The proof is given in Appendix E.

## 4 Relating Interpolation, Optimization and Generalization

For a given bounded set \(K\subseteq\mathbb{R}^{d}\), Theorem 2.1 states that for a large number of neurons \(m\in\mathbb{N}\), a large number of data points \(n\in\mathbb{N}\), and a small penalty \(\lambda>0\), minimizers of the empirical risk functional \(\widehat{\mathcal{R}}_{n,m,\lambda}\) resemble a minimum norm interpolant everywhere in \(K\). As the Barron semi-norm controls the generalization gap (see Appendix D) and a minimum norm interpolant has minimal Barron norm by definition, this suggests that minimum norm interpolants are optimal in terms of generalization, at least when arguing from this upper bound.

Many authors, including Safran and Shamir (2018); Venturi et al. (2018), demonstrate that neural network training is a non-convex optimization problem. As such, it is not guaranteed that numerical optimizers (1) converge to interpolants at all, and (2) select minimum norm interpolants out of the large set of different neural networks which interpolate given data, even when a regularizer is included in the training loss functional.

On the other hand, there are settings where an optimization algorithm selects a minimum norm solution even without explicit regularization. This is easily proved for gradient descent on the overparametrized least squares regression problem \(\mathcal{R}_{n}(a)=\frac{1}{n}\sum_{i=1}^{n}|a^{T}x_{i}-y_{i}|^{2}\) with initial condition \(a=0\) and \(n<m=d\). Using entirely different methods, Chizat and Bach (2020) prove a similar result for binary classification by shallow neural networks with logistic loss. For regression problems using neural networks, analogous results are not available to the best of our knowledge. This in part motivates the following numerical investigation. Namely, we are interested in exploring the effects of explicit regularization and the implicit bias of optimization algorithms toward minimum norm interpolants. Knowing the analytically optimal solution in between given data provides us the opportunity to compare optimizers on a deeper level than merely testing their performance on unseen data generated from the same distribution.

As seen in Figure 1, the radial profile \(r\mapsto f_{d}^{*}(re_{1})\) of Wojtowytsch (2022)'s minimum norm interpolant \(f_{d}^{*}\) is so close to \(0\) on \([r_{d},\infty)\) as to be virtually indistinguishable from zero numerically for some \(r_{d}<1\) which decreases in \(d\). Indeed, the first \((d-1)/2\) derivatives of vanish at \(r=1\) due to (Wojtowytsch, 2022, Lemma 4.1) and \(0\leq f_{d}^{*}(x)\leq Cd^{3/2}((1-\|x\|^{2})/\|x\|)^{(d-3)/2)}\) due to (Wojtowytsch, 2022, Appendix D.1) for a universal constant \(C>0\). In particular, if \(d\) is large, \(\|f_{d}^{*}\|_{L^{\infty}(\mathbb{R}^{d})B_{rd}(0))}\) is negligible compared to the approximation error \(d^{2}/m\) for any reasonable dimension \(d\) and network width \(m\). Consequently, the rescaled function \(h_{d}^{*}(x):=f_{d}^{*}(r_{d}x)\) meets the constraint \(h_{d}^{*}\equiv 0\) outside \(B_{1}(0)\) almost exactly and has the smaller Barron semi-norm \([h_{d}^{*}]_{\mathcal{B}}=r_{d}\,[f_{d}^{*}]_{\mathcal{B}}\). For this reason, we compare numerical solutions to the interpolation problem in Corollary 3.3 to rescaled versions of \(f_{d}^{*}\) rather than \(f_{d}^{*}\) itself, at least in high dimension. For \(r\) below the threshold value \(r_{d}\), there is no noticable trade-off between rescaling \(f_{d}^{*}(rx)\) and data-fitting. For larger values of \(r\), the Barron semi-norm is reduced more significantly, but the data fit becomes appreciably worse.

## 5 Numerical Experiments

Our main goal in this section is to gain a more precise understanding of different optimization algorithms by comparing numerical solutions to a known minimum norm interpolant. We consider the two settings in which minimum norm interpolation by Barron functions is best understood: One-dimensional and radially symmetric functions. As a benefit, we can easily visualize the numerical results in both settings. We focus on three questions of interest.

1. **Explicit regularization.** If \(\lambda>0\) is moderately small and \(m,n\) are large, then a global minimizer of \(\widehat{\mathcal{R}}_{n,m,\lambda}\) resembles a minimum norm interpolant between known data points due to Theorem 2.1. Is the minimizer which we find numerically close to to a minimum norm interpolant, or does it merely fit the function at known data points?
2. **Implicit bias.** If \(m,n\) are large, does a training algorithm select a minimum norm interpolant out of potentially many possible solutions without explicit regularization (i.e. for \(\lambda=0\))?3. **Learning symmetries:** The optimal minimum norm interpolant \(f_{d}^{*}\) described in Proposition 3.2 is radially symmetric and satisfies \(0\leq f_{d}^{*}\leq 1\). Proposition 3.2 does not rule out the existence of other minimum norm interpolants which are not radially symmetric. Does an optimization algorithm generally find solutions which are (approximately) radially symmetric and confined to the interval \([0,1]\)? A similar consideration applies in a one-dimensional investigation with reflection symmetry.

The third question is of particular interest for algorithms like ADAM, which operate coordinate-by-coordinate and do not respect Euclidean isometries. By comparison, we expect that SGD, initialized at a radially symmetric configuration, preserves Euclidean isometries. More experiments in similar settings can be found in Appendix A.

### One-dimensional experiments

We consider the classical interpolation problem of numerical analysis: Fit values \(f^{*}(x_{i})\in\mathbb{R}\) at points \(x_{i}\in\mathbb{R}\) for \(i\in\{1,\ldots,n\}\). In contrast to classical numerical analysis, we consider overparametrized ReLU-networks with a single hidden layer as our model class. As in Section 3, we select \(f^{*}(x)=|x|\) for simplicity.

In Figure 2, a ReLU network with a single hidden layer of width \(m=200\) was trained to fit \(f^{*}(x)=|x|\) at a symmetric set containing 15 equi-spaced points in \((1,2)\). Optimizers included SGD (with learning rate \(\eta=5\cdot 10^{-5}\) and momentum \(\mu=0.99\)), SGD (\(\eta=10^{-2}\), \(\mu=0\)), ADAM (\(\eta=5\cdot 10^{-5}\) and default parameters) and the quasi-Newton L-BFGS method. Deterministic gradients based on the \(n=30\) sample points were used. The final training loss was below \(10^{-4}\) on average. The network weights were initialized by a scaled uniform Xavier initialization, i.e. uniformly in a symmetric interval of length \(2\alpha\cdot\sqrt{6/(n_{in}+n_{out})}\) where \(n_{in}\) and \(n_{out}\) denote the number of input- and output-units to a layer respectively. The 'gain' factor was selected as \(\alpha\in\{0.5,1,5\}\). Without weight decay and for small gain, the optimizers find a solution close to the smallest possible minimum norm interpolant \(f(x)=|x|\). The larger the parameters for initialization gain and weight decay penalty, the closer numerical solutions are to the largest possible minimum norm interpolant \(f(x)=\max\{|x|,1\}\).

We observe that a higher gain factor \(\alpha\) corresponds to faster initial training, but a high gain like \(\alpha=5\) produces interpolants which are non-convex without regularization, while a lower gain factor produces convex interpolants in longer time. This observation agrees with the findings of Chizat et al. (2019), who dub the large \(\alpha\) setting the 'lazy training' regime and associate it with worse generalization performance. As Pesme et al. (2021) eloquently put it: "there is a tension between generalisation and optimisation: a longer training time might improve generalisation but comes at the cost of...a longer training time."

If \(m\) is large and \(\alpha\) is not too big, the variation of solutions produced by a training algorithm vary less over different stochastic realizations - see Appendix A for experiments for \(m=1,000\). The

Figure 1: **Left:** In Dimension \(d=31\), the minimum Barron norm solution \(f_{d}^{*}\) satisfies \(f_{d}^{*}\equiv 0\) on \(\mathbb{R}^{d}\setminus B_{r_{d}}(0)\) for \(r_{d}=0.4\) to high precision, albeit not exactly. The rescaled function \(f_{d}^{*}(r_{d}x)\) is a suitable candidate for a minimum norm almost-interpolant to high accuracy. **Right:** For later use, we consider more aggressively rescaled functions \(f_{d}^{*}(r_{d}x)\) for \(d=15\), \(d=31\) with lower semi-norm, but worse data fitting properties. We note that the rescalings of these functions which have essentially the same slope as \(f_{3}^{*}\) at \(r=0\) appear to coincide. We conjecture that this statement allows for a more rigorous formulation.

dynamics are close to those of a limiting'mean field' model studied by Chizat and Bach (2018); Rotskoff and Vanden-Eijnden (2018); Mei et al. (2018); Sirignano and Spiliopoulos (2020) and Wojtowytsch (2020). In these works, the limiting model is typically derived with a factor \(1/m\) outside the function definition, which is implicit in the initialization here since \(n_{in}+n_{out}\approx m\) for both layers and the ReLU activation is positively one-homogeneous. Global convergence to a minimizer (but not necessarily a minimum norm solution) is guaranteed (up to certain technical assumptions) by Chizat and Bach (2018) and Wojtowytsch (2020).

For comparison, we also present the natural cubic spline interpolant, i.e. the function \(f\) which minimizes the stronger curvature energy \(\int_{-2}^{2}|f^{\prime\prime}(x)|^{2}\,\mathrm{d}x\) under the condition that \(f(x_{i})=|x_{i}|\) for all \(i=1,\ldots,n\). Unlike the minimum Barron interpolants, the natural cubic spline may not be convex (and in fact, it is not if \(f^{*}\) is replaced by \(h^{*}(x)=|x-0.5|\)).

### Radially symmetric data

We explore the performance of numerical optimization algorithms in the setting of Corollary 3.3 with and without explicit regularization \(\lambda\in\{0,10^{-5}\}\) in dimensions \(d=3\), \(d=15\) and \(d=31\). The numerical solution is then compared to (a rescaled version of) the analytic minimum norm interpolant \(f_{d}^{*}\) described in Proposition 3.2, which we compute by the algorithm described in (Wojtowytsch, 2022, Section 6). The rescaling factor \(r_{d}\) is chosen heuristically for an accurate match.

Figure 2: We compare numerical approximations of a target function for Momentum-GD (red), GD (magenta), ADAM (purple) and L-BFGS (brown). The target function is drawn in blue and the natural cubic spline in green. For each algorithm, we plot one representative solution to study symmetry selection properties. Vertical grey lines indicate known training data points. The initialization has gain \(\alpha\in\{0.5,1.0,5.0\}\) in the left, middle and right column. The weight decay penalty is \(\lambda\in\{0,0.002,0.005\}\) (top, middle, bottom row). For all optimization algorithms, the final loss is approximately \(0,2\cdot 10^{-4}\) and \(10^{-3}\) respectively.

Data is generated from a distribution \(\mu=\mu_{1}+\mu_{2}+\mu_{3}\) where \(\mu_{1}\) is a point mass of magnitude \(m_{1}\) at the origin, \(\mu_{2}\) is a uniform measure on the unit sphere \(S^{d-1}\) with mass \(m_{2}\) and \(\mu_{3}\) is the radially symmetric measure of mass \(1-m_{1}-m_{2}\) such that \(\|x\|_{2}\) is distributed uniformly in \([1,7]\). We numerically explored various values for \(m_{1}\in[0.1,0.4]\) and \(m_{2}\in[0.0,0.4]\) and found simulations to be relatively stable under a number of choices.

Results are presented in Figures 3, 4 and Appendix A. We find that all algorithms find a solution with radial average similar to \(f_{d}^{*}(r_{d}x)\), albeit for rescaling factors \(r_{d}\) which depend on dimension \(d\) and (to a lesser extent) the optimizer. In high dimension, solutions are not perfectly radially symmetric, but the larger amount of variation over a sphere of fixed radius is observed in the domain where \(f_{d}^{*}\approx 0\) rather than in the transition area \((0,r_{d})\). Larger datasets improve the compliance with the optimal interface and reduce the radial standard deviation. Solutions do not remain non-negative and drop below zero before leveling off as the radius increases. The drop becomes more noticeable as the dimension increases and less pronounced for wider networks.

The results are essentially identical for normal Xavier initialization and (not radially symmetric) uniform Xavier initialization. In accordance with our expectations, the radial standard deviation is higher for Adam compared to optimizers based in Euclidean geometry. While the neural network function found by Adam resembles a minimum norm interpolant, the weight decay regularizer takes significantly higher values compared to other optimization algorithms.

## 6 Conclusion

Shallow ReLU networks converge to minimum norm interpolants of given data: Provably if explicit regularization is included and empirically if it is not. We conclude with a summary of our empirical insight into the implicit bias of neural network optimizers.

Figure 3: A neural network with a single hidden layer of width \(m=12,000\) was trained by gradient descent with learning rate \(\eta=10^{-3}\) and momentum \(\mu=0.99\) in the setting of Section 5.2. The radial average is sketched by a solid red line. One radial standard deviation around the average, computed over 500 random directions, is shaded.z **Top row:** Experiment in dimension 3 (left) and dimension 15 (right). The numerical solutions are compared to \(f_{d}^{*}(r_{d}x)\) with \(r_{3}=1/1.05\) and \(r_{15}=1/2.55\). In both cases, the ‘minimum norm interpolant’ shape is attained to high accuracy. Both solutions are approximately symmetric, more so in low dimension. **Bottom row:** Numerical approximations to \(f_{15}^{*}(r_{15}\cdot)\) for neural networks of constant width \(m=12,000\), trained on data sets of different size (but for an identical number of \(200,000\) training steps with stochastic estimates computed over a batch of \(50\) data points). The shape of the radial average is comparable across different dataset sizes, but the fit of the radial average with data is improved and the radial variance reduced for larger datasets. Note that the first two simulations are set in the overparametrized regime, whereas the last experiment on the largest dataset is underparametrized.

1. With reasonable (not too large) initialization, all algorithms studied here are biased towards minimum norm interpolant profiles.
2. At least in the case of Adam, this bias is visible on the function level, but not on the parameter level, as the weight decay regularizer increases rapidly to large magnitude. Despite this, ADAM solutions often appear 'flatter' in high dimension with a lower rescaling factor \(r_{d}\).
3. Explicit regularization stabilizes towards a minimum norm interpolant shape, but at the cost of a decreased fit with the target values. Its impact is most significant for poorly chosen initial conditions.
4. When the minimum norm interpolant is non-unique, different types of minimum norm interpolants are found depending on the choice of initialization scheme and optimization algorithm. The impact of initialization scale appears more significant.
5. Optimization algorithms which are rooted in Euclidean geometry (such as SGD and momentum-SGD) more successfully preserve Euclidean symmetries compared to the 'coordinate-wise' Adam algorithm.

The last observation is not surprising for radially symmetric initialization laws as radially symmetric parameter distributions induce radially symmetric functions. It is, however, observed also for a uniform initialization scheme which only obeys coordinate symmetries.

We believe minimum norm interpolation to be a useful testbed to study the implicit bias of optimizers and the impact of initialization and regularization. While minimum norm interpolation by deeper networks has not been characterized yet, we anticipate no obstructions to implementing a similar program there in the future.

## Acknowledgements

The research of Jiyoung Park is supported by NSF DMS-2210689.

Figure 4: **Top row:** We compare different optimizers (SGD = red, Adam = green and Momentum-SGD = blue) in the setting of Section 5.2 in dimension 31 with \(n=10^{4}\) data points. All minimizers attain the minimum norm interpolant shape (left column, rescaled optimal solution in pink) – curiously for Adam, the correct shape is attained despite the fact that at \(\approx 1,600\) the weight decay regularizer is an order of magnitude higher than for the other optimizers (right column). The high regularizer value goes hand in hand with a higher radial standard deviation (middle column). The initialization is uniform Xavier (in particular, not radially symmetric). Essentially identical results are observed for the radially symmetric normal Xavier initialization with the same degree of radial symmetry in Figure 8. **Bottom row:** If we include an explicit weight-decay regularizer with weight \(\lambda=10^{-5}\), solutions resemble an optimal interpolant for a smaller rescaling factor \(r_{31,\lambda}=1/3.8\) compared to \(r_{31}=1/3.5\) without regularization. This is expected since the norm is weighted more heavily compared to data compliance. Notably, the radial standard deviation does not decrease, even for the (heavily affected) ADAM optimizer. It remains noticeable at \(0.1\) for \(r=5\) and Adam.

## References

* Adams [2022] Stefan Adams. Lecture notes in high-dimensional probability. https://warwick.ac.uk/fac/sci/maths/people/staff/stefan_adams/high-dimensional_probability_ma3k0-notes.pdf, 2022.
* Alikakos et al. [1994] Nicholas D. Alikakos, Peter W. Bates, and Xinfu Chen. Convergence of the Cahn-Hilliard equation to the Hele-Shaw model. _Arch. Rational Mech. Anal._, 128(2):165-205, 1994. ISSN 0003-9527. doi: 10.1007/BF00375025. URL http://dx.doi.org/10.1007/BF00375025.
* Bach et al. [2021] Annika Bach, Roberta Marziani, and Caterina Ida Zeppieri. \(\gamma\)-convergence and stochastic homogenisation of singularly-perturbed elliptic functionals. _arXiv preprint arXiv:2102.09872_, 2021.
* Bach [2017] Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* Barrett and Dherin [2020] David GT Barrett and Benoit Dherin. Implicit gradient regularization. _arXiv preprint arXiv:2009.11162_, 2020.
* Bauer [1958] Heinz Bauer. Minimalstellen von funktionen und extremalpunkte. _Archiv der Mathematik_, 9(4):389-393, 1958.
* Belkin et al. [2019] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* Belkin et al. [2020] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. _SIAM Journal on Mathematics of Data Science_, 2(4):1167-1180, 2020.
* Bhattacharya et al. [2016] Kaushik Bhattacharya, Marta Lewicka, and Mathias Schaffner. Plates with incompatible prestrain. _Archive for Rational Mechanics and Analysis_, 221(1):143-181, 2016.
* Boursier and Flammarion [2023] Etienne Boursier and Nicolas Flammarion. Penalising the biases in norm regularisation enforces sparsity. _arXiv preprint arXiv:2303.01353_, 2023.
* Braides [2002] Andrea Braides. \(\Gamma\)_-convergence for beginners_, volume 22 of _Oxford Lecture Series in Mathematics and its Applications_. Oxford University Press, Oxford, 2002. ISBN 0-19-850784-4. doi: 10.1093/acprof:oso/9780198507840.001.0001. URL http://dx.doi.org/10.1093/acprof:oso/9780198507840.001.0001.
* Braides and Truskinovsky [2008] Andrea Braides and Lev Truskinovsky. Asymptotic expansions by \(\gamma\)-convergence. _Continuum Mechanics and Thermodynamics_, 20:21-62, 2008.
* Bronsard and Kohn [1990] Lia Bronsard and Robert V. Kohn. On the slowness of phase boundary motion in one space dimension. _Comm. Pure Appl. Math._, 43(8):983-997, 1990. ISSN 0010-3640. doi: 10.1002/cpa.3160430804. URL http://dx.doi.org/10.1002/cpa.3160430804.
* Caragea et al. [2020] Andrei Caragea, Philipp Petersen, and Felix Voigtlaender. Neural network approximation and estimation of classifiers with classification boundary in a barron class. _arXiv preprint arXiv:2011.09363_, 2020.
* Chizat and Bach [2018] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Chizat and Bach [2020] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_, pages 1305-1338. PMLR, 2020.
* Chizat et al. [2019] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems_, 32, 2019.
* Cooper [2018] Yaim Cooper. The loss landscape of overparameterized neural networks. _arXiv preprint arXiv:1804.10200_, 2018.
* D Maso [2012] Gianni Dal Maso. _An introduction to \(\Gamma\)-convergence_, volume 8. Springer Science & Business Media, 2012.
* D'Alessio et al. [2018]Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers flat global minimizers. _Advances in Neural Information Processing Systems_, 34:27449-27461, 2021.
* De Giorgi and Franzoni (1975) Ennio De Giorgi and Tullio Franzoni. Su un tipo di convergenza variazionale. _Atti della Accademia Nazionale dei Lincei. Classe di Scienze Fisiche, Matematiche e Naturali. Rendiconti_, 58(6):842-850, 1975.
* Debarre et al. (2022) Thomas Debarre, Quentin Denoyelle, Michael Unser, and Julien Fageot. Sparsest piecewise-linear regression of one-dimensional data. _Journal of Computational and Applied Mathematics_, 406:114044, 2022.
* Dobrowolski (2010) Manfred Dobrowolski. _Angewandte Funktionalanalysis: Funktionalanalysis, Sobolev-Raume und Elliptische Differentialgleichungen_. Springer-Verlag, 2010.
* Dondl et al. (2019) Patrick W Dondl, Matthias W Kurzke, and Stephan Wojtowytsch. The effect of forest dislocations on the evolution of a phase-field model for plastic slip. _Archive for Rational Mechanics and Analysis_, 232(1):65-119, 2019.
* Du et al. (2018) Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* E and Wojtowytsch (2020) Weinan E and Stephan Wojtowytsch. Representation formulas and pointwise properties for Barron functions. _Calc. Var. Partial Differential Equations_, 61(46), 2020.
* E and Wojtowytsch (2021) Weinan E and Stephan Wojtowytsch. Kolmogorov width decay and poor approximators in machine learning: Shallow neural networks, random feature models and neural tangent kernels. _Research in the Mathematical Sciences_, 8(1):1-28, 2021.
* E and Wojtowytsch (2022) Weinan E and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. In _Mathematical and Scientific Machine Learning_, pages 270-290. PMLR, 2022.
* E et al. (2019a) Weinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural networks. _Communications in Mathematical Sciences_, 17(5):1407-1425, 2019a. doi: 10.4310/cms.2019.v17.n5.a11. URL https://doi.org/10.4310%2Fcms.2019.v17.n5.a11.
* E et al. (2019b) Weinan E, Chao Ma, and Lei Wu. A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics. _Sci. China Math_, 2019b.
* E et al. (2019c) Weinan E, Chao Ma, and Lei Wu. The barron space and the flow-induced function spaces for neural network models. _arXiv:1906.08039 [cs.LG]_, 2019c.
* Evans and Gariepy (2015) Lawrence Craig Evans and Ronald F Gariepy. _Measure theory and fine properties of functions_. CRC press, 2015.
* Friesecke et al. (2002a) Gero Friesecke, Richard D James, and Stefan Muller. Rigorous derivation of nonlinear plate theory and geometric rigidity. _Comptes Rendus Mathematique_, 334(2):173-178, 2002a.
* Friesecke et al. (2002b) Gero Friesecke, Richard D James, and Stefan Muller. A theorem on geometric rigidity and the derivation of nonlinear plate theory from three-dimensional elasticity. _Communications on Pure and Applied Mathematics_, 55(11):1461-1506, 2002b.
* Friesecke et al. (2003) Gero Friesecke, Richard D James, Maria Giovanna Mora, and Stefan Muller. Derivation of nonlinear bending theory for shells from three-dimensional nonlinear elasticity by gamma-convergence. _Comptes Rendus Mathematique_, 336(8):697-702, 2003.
* Glorot and Bengio (2010) Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
* Hajjar and Chizat (2022) Karl Hajjar and Lenaic Chizat. Symmetries in the dynamics of wide two-layer neural networks. _arXiv preprint arXiv:2211.08771_, 2022.
* H. J.-L. Lee and W.-Y. Lee (2019)Boris Hanin. Ridgeless interpolation with shallow relu networks in \(1d\) is nearest neighbor curvature extrapolation and provably generalizes on lipschitz functions. _arXiv preprint arXiv:2109.12960_, 2021.
* Hanin and Rolnick (2018) Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. _Advances in Neural Information Processing Systems_, 31, 2018.
* He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _Proceedings of the IEEE international conference on computer vision_, pages 1026-1034, 2015.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural computation_, 9(1):1-42, 1997.
* Honorio and Jaakkola (2014) Jean Honorio and Tommi Jaakkola. Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees. In Samuel Kaski and Jukka Corander, editors, _Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics_, volume 33 of _Proceedings of Machine Learning Research_, pages 384-392, Reykjavik, Iceland, 22-25 Apr 2014. PMLR. URL https://proceedings.mlr.press/v33/honorio14.html.
* Ilmanen (1993) Tom Ilmanen. Convergence of the Allen-Cahn equation to Brakke's motion by mean curvature. _J. Differential Geom._, 38(2):417-461, 1993. ISSN 0022-040X. URL http://projecteuclid.org/euclid.jdg/1214454300.
* Jin and Montufar (2020) Hui Jin and Guido Montufar. Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks. _arXiv preprint arXiv:2006.07356_, 2020.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Klenke (2006) Achim Klenke. _Wahrscheinlichkeitstheorie_, volume 1. Springer, 2006.
* Lewicka et al. (2010) Marta Lewicka, Maria Giovanna Mora, and Mohammad Reza Pakzad. Shell theories arising as low energy \(\gamma\)-limit of 3d nonlinear elasticity. _Annali della Scuola Normale Superiore di Pisa-Classe di Scienze_, 9(2):253-295, 2010.
* Li et al. (2021) Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?-a mathematical framework. _arXiv preprint arXiv:2110.06914_, 2021.
* Li et al. (2020) Zhong Li, Chao Ma, and Lei Wu. Complexity measures for neural networks with general activation functions using path-based norms. _arXiv preprint arXiv:2009.06132_, 2020.
* Loog et al. (2020) Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory of double descent. _Proceedings of the National Academy of Sciences_, 117(20):10625-10626, 2020.
* Ma et al. (2020) Chao Ma, Lei Wu, et al. Machine learning from a continuous viewpoint, i. _Science China Mathematics_, 63(11):2233-2266, 2020.
* Mei et al. (2018) Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* Modica (1987) Luciano Modica. The gradient theory of phase transitions and the minimal interface criterion. _Arch Ration Mech Anal_, 98(2):123-142, 1987.
* Modica and Mortola (1977) Luciano Modica and Stefano Mortola. Un esempio di \(\Gamma\)-convergenza. _Boll. Un. Mat. Ital. B (5)_, 14(1):285-299, 1977.
* Mugnai and Roger (2011) Luca Mugnai and Matthias Roger. Convergence of perturbed Allen-Cahn equations to forced mean curvature flow. _Indiana Univ. Math. J._, 60(1):41-75, 2011. ISSN 0022-2518. doi: 10.1512/iumj.2011.60.3949. URL http://dx.doi.org/10.1512/iumj.2011.60.3949.
* Neumayer et al. (2023) Sebastian Neumayer, Lenaic Chizat, and Michael Unser. On the effect of initialization: The scaling path of 2-layer neural networks. _arXiv preprint arXiv:2303.17805_, 2023.
* Nemirovsky et al. (2015)Greg Ongie and Rebecca Willett. The role of linear layers in nonlinear interpolating networks. _arXiv preprint arXiv:2202.00856_, 2022.
* Ongie et al. [2019] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width relu nets: The multivariate case. _arXiv preprint arXiv:1910.01635_, 2019.
* Parhi and Nowak [2021] Rahul Parhi and Robert D Nowak. Banach space representer theorems for neural networks and ridge splines. _J. Mach. Learn. Res._, 22(43):1-40, 2021.
* Parhi and Nowak [2022] Rahul Parhi and Robert D Nowak. What kinds of functions do deep neural networks learn? insights from variational spline theory. _SIAM Journal on Mathematics of Data Science_, 4(2):464-489, 2022.
* Pesme et al. [2021] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. _Advances in Neural Information Processing Systems_, 34:29218-29230, 2021.
* Rotskoff and Vanden-Eijnden [2018] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. _stat_, 1050:22, 2018.
* Rudin [1991] W. Rudin. _Functional Analysis_. International series in pure and applied mathematics. McGraw-Hill, 1991. ISBN 9780070542365.
* Safran and Shamir [2018a] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 4430-4438. PMLR, 2018a. URL http://proceedings.mlr.press/v80/safran18a.html.
* Safran and Shamir [2018b] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _International conference on machine learning_, pages 4433-4441. PMLR, 2018b.
* Sandier and Serfaty [2004] Etienne Sandier and Sylvia Serfaty. Gamma-convergence of gradient flows with applications to Ginzburg-Landau. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 57(12):1627-1672, 2004.
* Savarese et al. [2019] Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm networks look in function space? In _Conference on Learning Theory_, pages 2667-2690. PMLR, 2019.
* Serfaty [2011] Sylvia Serfaty. Gamma-convergence of gradient flows on Hilbert and metric spaces and applications. _Discrete Contin. Dyn. Syst_, 31(4):1427-1451, 2011.
* Shalev-Shwartz and Ben-David [2014] Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* Siegel and Xu [2020] Jonathan W Siegel and Jinchao Xu. Approximation rates for neural networks with general activation functions. _Neural Networks_, 128:313-321, 2020.
* Siegel and Xu [2021] Jonathan W Siegel and Jinchao Xu. Characterization of the variation spaces corresponding to shallow neural networks. _arXiv preprint arXiv:2106.15002_, 2021.
* Siegel and Xu [2022] Jonathan W Siegel and Jinchao Xu. Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks. _Foundations of Computational Mathematics_, pages 1-57, 2022.
* Siegel and Xu [2023] Jonathan W Siegel and Jinchao Xu. Characterization of the variation spaces corresponding to shallow neural networks. _Constructive Approximation_, pages 1-24, 2023.
* Sirignano and Spiliopoulos [2019] Justin Sirignano and Konstantinos Spiliopoulos. Scaling limit of neural networks with the xavier initialization and convergence to a global minimum. _arXiv preprint arXiv:1907.04108_, 2019.
* Sirignano and Spiliopoulos [2020a] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. _SIAM Journal on Applied Mathematics_, 80(2):725-752, 2020a.
* Szegedy et al. [2015]Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. _Stochastic Processes and their Applications_, 130(3):1820-1852, 2020b.
* Smith et al. [2021] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. _arXiv preprint arXiv:2101.12176_, 2021.
* Venturi et al. [2018] Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network optimization landscapes. _arXiv preprint arXiv:1802.06384_, 2018.
* Wojtowytsch [2020] Stephan Wojtowytsch. On the convergence of gradient descent training for two-layer relu-networks in the mean field regime. _arXiv preprint arXiv:2005.13530_, 2020.
* Wojtowytsch [2022] Stephan Wojtowytsch. Optimal bump functions for shallow relu networks: Weight decay, depth separation and the curse of dimensionality. _arXiv preprint arXiv:2209.01173_, 2022.
* Wu et al. [2022] Lei Wu, Mingze Wang, and Weijie Su. The alignment property of sgd noise and how it helps select flat minima: A stability analysis. _Advances in Neural Information Processing Systems_, 35:4680-4693, 2022.
* Yang et al. [2021] Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Taxonomizing local versus global structure in neural network loss landscapes. _Advances in Neural Information Processing Systems_, 34:18722-18733, 2021.
* Zhou et al. [2020] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically understanding why sgd generalizes better than adam in deep learning. _Advances in Neural Information Processing Systems_, 33:21285-21296, 2020.

## Appendix

* A Numerical Experiments
* B \(\Gamma\)-convergence
* C Homogeneous Barron spaces
* D Rademacher complexity of homogeneous Barron space
* E Proofs of the convergence theorems
* F Theorem 2.1 for finite data sets
* G Minimum norm interpolation in one dimension
* H Sub-Gaussian random variables

## Appendix A Numerical Experiments

### Hyperparameter settings and computation effort

In all experiments in Dimensions 3 and 15, the following hyperparameter settings were used unless otherwise indicated:

1. Normal Xavier initialization with gain \(\alpha=\sqrt{2}\)
2. SGD: Learning rate = \(10^{-2}\) (Dimension 15), \(10^{-3}\) (Dimension 3).
3. Momentum-SGD: Learning rate = \(10^{-3}\) and momentum \(\mu=0.99\)
4. ADAM: Learning rate = \(10^{-3}\) and PyTorch default hyperparameters for \(\beta_{1}=0.9,\beta_{2}=0.999,\varepsilon=10^{-8}\).

For experiments in Dimension 31, we drop the learning rate for ADAM after 50 of 150 epochs by a factor of 10 and for Momentum-SGD by a factor of 10 after 100 epochs.

In Dimension 3, a learning rate of \(10^{-2}\) was found numerically unstable for SGD without momentum. To compensate for the smaller learning rate and provide a fair comparison, the number of time steps was increased.

All experiments were performed on a free version of google colab or the experimenters' personal computers. One run of the model takes below fifteen minutes on a single graphics processing unit.

### Summary and interpretation of additional simulations

In this Section, we present additional numerical experiments in various situations complementary to those presented in the main body of the text. These include: Wider neural networks (Appendix A.3), experiments with different optimizers (Appendix A.4), experiments with different initialization to explore effects of scale and symmetry and the role of explicit regularization (Appendices A.8 and A.5), experiments with \(\ell^{1}\)-loss instead of \(\ell^{2}\)-loss (Appendix A.7) and repeated experiments to visualize the stochastic variation between runs (Appendix A.6).

Additionally, we present and investigation into related settings where our theoretical understanding does not apply: In Appendix A.10, we consider linearized (random feature) dynamics to explore how close we are to a (truly non-linear) neural network model. In Appendix A.11, we consider neural networks with a single hidden layer and leaky ReLU activation instead of ReLU activation. In Appendix A.12, we consider ReLU networks with multiple hidden layers. For a detailed list, see the table of contents below.

* Wide neural networks in one dimension
	* 4.4 SGD and ADAM: Dimensions 3 and 15
	* 4.5 Radial symmetry in Dimension 31
	* 4.6 Gradient descent with Momentum
	* 4.7 \(\ell^{1}\)-loss and Huber loss
	* 4.8 Initialization scaling and explicit regularization: high-dimensional radial data
	* 4.9 He initialization
* 4.10 Linearized dynamics
* 4.11 Leaky ReLU activation
* 4.12 Deeper neural networks

Our goal is not to explore questions of loss function, initialization, optimization algorithm and the impact of hyperparameters in a systematic fashion, but rather to establish problems in which a minimum norm interpolant can be found in an explicit fashion as instructive benchmarks to numerically study such questions. As a proof of concept, we provide a partial exploration of the parameter space. For the moment, we find ourselves confined to ReLU networks with a single hidden layer, as this is the only case in which explicit minimum norm interpolants are available. Minimum norm interpolation describes the shape of functions between known data clusters and is thus more expressive than a study of generalization error which is naturally confined to data clusters.

The additional experiments corroborate our findings in the main text. Before the detailed presentation, let us briefly summarize the conclusions.

1. Across a variety of different optimizers, Xavier type (= Glorot type) initialization schemes and loss functions, a minimum Barron norm interpolant-like shape was attained, to varying degrees of accuracy and with different rescaling factors.
2. Solutions are fairly radially symmetric with standard deviation in radial direction at most \(0.1\) (SGD) and \(0.2\) (Adam).
3. A geometrically distinct shape is observed for random feature models in the same regime.
4. Explicit regularization has little effect, even for poorly chosen initializations of Xavier type with high gain. We conjecture that the uniqueness of the radially symmetric minimum norm interpolant induces a higher degree of rigidity and bias, compared to the one-dimensional case where the set of minimum norm solutions was diverse (and infinite).
5. Functions display larger variation in the radial direction for He initialization. In this regime, explicit regularization has more apparent and beneficial effects on both solution shape and radial symmetry. The solution does not reduce to the random feature model in this case either.

### Wide neural networks in one dimension

In Figure 5, we present the same experiment as in Figure 2 for wider neural networks with \(m=1,000\) neurons in the hidden layer.

### SGD and ADAM: Dimensions 3 and 15

We repeat the experiment on Figure 3 for Stochastic Gradient Descent (SGD) optimizer without momentum and for the Adam optimizer of Kingma and Ba (2014). The results are displayed in Figure 6 and 7 respectively. The results strongly resemble those obtained for the SGD optimizer with momentum in Figure 3.

### Radial symmetry in Dimension 31

As indicated in Figure 4, we present computational results with the radially symmetric normal Xavier initialization in Figure 8.

Figure 5: For wider neural networks, we observe less stochastic variation between runs, as the empirical distribution of neurons is closer to a continuum limit. Solutions are generally more convex and symmetric than their narrow counterparts. The gradient descent optimizer without momentum stands out for its tendency to select solutions with highly localized second derivatives and a preference for piecewise linear functions with few linear regions, while other training algorithms select ‘smoother’ solutions with curvatures which are dispersed more evenly throughout the domain.

Figure 6: We perform the same experiments as for Figure 3, but with the SGD without momentum optimizer. Learning rate was adjusted to \(10^{-2}\) for dimension 15, but for dimension 3 we just used \(10^{-3}\) learning rate and runned 10 times more epochs, due to stability of neural network training in dimension 3 case. The results are comparable, but the rescaling factors were chosen as \(1/1.15\) in dimension 3 and \(1/2.65\) in dimension 15.

### Gradient descent with Momentum

In Figure 9, we present additional runs in the setting of Figure 3. Despite quantitative variation, the geometric shapes of solutions are stable over multiple runs and resemble the minimum norm interpolant \(f_{15}^{*}\) in all cases.

### \(\ell^{1}\)-loss and Huber loss

We repeat the experiment of Figure 3 with the \(\ell^{1}\)-loss function in the place of \(\ell^{2}\)-loss. To compensate for the lack of smoothness in the loss function, we reduce the learning rate by a factor of \(10\) to \(10^{-4}\) and increase the number of epochs by 50% to compensate. Results are reported in Figure 10.

Figure 8: Results between normal and uniform Xavier initialization are essentially identical in this experiment – compare Figure 4. (Approximate) radial symmetry is attained even when parameters are initialized in a fashion which is not radially symmetric.

Figure 7: We repeat the experiments of Figure 3, but with Adam. The numerical solutions resemble those found by Momentum-SGD, but better rescaling factors for numerical solutions are \(1/1.2\) in dimension 3 and \(1/2.75\) rather than \(1/1.05\) and \(1/2.55\), i.e. the functions are ‘flatter’.

Figure 9: Three realizations of numerical interpolations in the setting of Section 5.2, computed by SGD with learning rate \(\eta=10^{-3}\) and momentum \(\mu=0.99\). In all cases, the minimum norm interpolant shape is attained approximately, but the radial averages briefly dip below zero and exhibit a local minimum which is not found in \(f_{d}^{*}\). The variation between runs is notable, but not large.

Similarly, we repeated experiments with the Huber loss function. Since \(|f(x_{i})-f^{*}(x_{i})|<1\) over the data set during the final stages of training, the loss function coincides with \(\ell^{2}\)-loss in the long run and all experiments are identical to \(\ell^{2}\)-loss. We therefore do not present additional plots.

In the initial stages of training, Huber loss is more stable numerically than \(\ell^{2}\)-loss, especially for large gain or He initialization (compare Section A.9).

### Initialization scaling and explicit regularization: high-dimensional radial data

As noted in Section 5.1 and previously by Chizat et al. (2019), the choice of initialization affects the optimization process of neural networks. Motivated by our observations in the one-dimensional case, we consider the effects of initialization and explicit regularization in the radially symmetric setting (Section 5.2). Our results support the earlier claim that the effects of regularization are advantageous for poorly chosen initialization with high gain.

The experiments were performed in higher dimension 31 and with _uniform_ Xavier initialization for the scenario in which it is most challenging to obtain radially symmetric solutions. As in Appendix A.7, we used \(\ell^{1}\)-loss rather than \(\ell^{2}\)-loss. To compensate for the non-smoothness of the loss function, we drop the learning rate by a factor of 10 twice during the training process. Similar results were observed for \(\ell^{2}\)-loss, but the effects of initialization and regularization were less pronounced compared to the \(\ell^{1}\)-case.

Plots for a single representative run are displayed in Figure 11. The explicit regularizer has the clearest effect in the high gain regime, where explicit regularization helps to achieve a better fit with the optimal transition curve and reduces the radial standard variation. Results were less sensitive to poor initialization than the corresponding experiments in one dimension. We conjecture that a higher degree of rigidity is introduced in this setting by the fact that there exists a _unique_ minimum norm interpolant.

### He initialization

All experiments so far were performed with the initialization scaling of Glorot and Bengio (2010). Especially for deeper neural networks, the initialization scheme of He et al. (2015) is very popular. Hanin and Rolnick (2018) proves in particular that He et al. (2015)'s normalization avoids the vanishing and exploding gradients phenomenon at initialization in expectation. While this consideration does not apply to our shallow networks, we find it informative to compare the two schemes.

Figure 10: \(\ell^{1}\)-loss leads to similar numerical results with a smaller rescaling factor of \(1/2.8\) rather than \(1/2.5\) for \(\ell^{2}\)-loss. Curiously, \(f(0)>1\) for these algorithms, while \(f(0)\leq 1\) when optimizing \(\ell^{2}\)-loss. In Dimension \(d=3\), the non-smoothness leads to a minimization problem that is not well resolved by the numerical optimization algorithm.

For shallow neural networks

\[f_{m}:\mathbb{R}^{d}\rightarrow\mathbb{R},f_{m}(x)=\sum_{i=1}^{m}a_{i}\,\sigma(w_{ i}\cdot x+b_{i})\]

the effect of initialization is as follows:

1. According to Glorot and Bengio (2010), the parameters \(a_{i}\) and \(w_{ij}\), \(1\leq i\leq m\) and \(1\leq j\leq d\) are chosen as random variables with mean \(0\) and standard deviation \(\sqrt{2/(m+1)}\) for \(a_{i}\) and \(\sqrt{2/(m+d)}\) for \(w_{i,j}\). In particular, if \(m\) is much larger than \(d\), then the \(|a_{i}|\left\|w_{i}\right\|=O(1/m)\). As we add \(m\) terms of magnitude \(\sim 1/m\), we consider this the 'law of large numbers' scaling.
2. According to He et al. (2015), the parameters \(a_{i}\) and \(w_{ij}\), \(1\leq i\leq m\) and \(1\leq j\leq d\) are chosen as random variables with mean \(0\) and standard deviation \(\sqrt{2/m}\) for \(a_{i}\) and \(\sqrt{2/d}\) for \(w_{i,j}\). In particular, if \(m\) is much larger than \(d\), then the \(|a_{i}|\left\|w_{i}\right\|=O(1/\sqrt{m})\). As we add \(m\) terms of mean zero and magnitude \(\sim 1/\sqrt{m}\), we consider this the 'central limit theorem' scaling.

Many authors, such as Sirignano and Spiliopoulos (2020a,b, 2019), present the factor \(1/m\) or \(1/\sqrt{m}\) explicitly outside the neural network. As observed above, the effect of initialization can be significant. Unsurprisingly, results in the central limit regime, where all neurons contribute similarly at initialization, are more consistent and predictable. Experimental results are presented in Figure 12 in the one-dimensional setting and in Figures 13 and 14 in the case of radial symmetry. Notably, in high dimension, explicit regularization not only reduced radial variation, but also increased data compliance by reducing the rescaling factor \(r_{d}\). In this setting, we observe the benefits of explicit regularization over relying on implicit bias only.

### Linearized dynamics

Parameter optimization in neural networks depends heavily on the choice of initialization. While the dynamics are truly non-linear in the regime studied by Chizat and Bach (2018); Rotskoff and Vanden-Eijnden (2018); Mei et al. (2018); Sirignano and Spiliopoulos (2020a) and Wojtowytsch (2020), there are scalings for which the directions \(w_{i}\) remain close to their initialization for all time - see e.g. (E et al., 2019b) for a derivation. In this case, the solution produced by a neural network is similar to that of a random feature model. In this section, we numerically find the minimum norm interpolant of a random feature model by 'freezing' the inner layer coefficients at their random initialization. We find that the random feature solution differs geometrically from the Barron space solution, e.g. in that it is smooth at the origin, where the Barron space solution has a cone-like singularity of the form \(f(x)=1-c_{d}\|x\|_{2}\) for small \(x\). In particular, we find that our experiments were set appropriately in the non-linear training regime. See Figure 15.

Figure 11: We vary initialization scale (\(\alpha\in\{0.25,\sqrt{2},10\}\) from left to right) and consider training without weight decay (top row) and with weight decay \(\lambda=10^{-4}\) (bottom row). The rescaling factors are chosen to be \(1/3.9\) for \(\alpha=0.25\), \(1/3.8\) for \(\alpha=\sqrt{2}\), and \(1/4\) for \(\alpha=10\).

Figure 12: Experiments for He initialization in one Dimension in the same setting as Figure 2 with Xavier initialization. Left: No explicit regularization, right: Weight decay regularization \(\lambda=0.002\). Even for Glorot initialization with large gain, this regularizer was sufficient to induce convexity. For He initialization, it has a notable regularizing effect, but it is insufficient to impose convexity. We observe greater deviation from a linear function in the small intervals between known data points on either side of the big ‘gap’.

Figure 13: **Top row:** Experiments for He initialization in dimension \(31\) in the same setting as Appendix A.8. **Left**: No explicit regularization with a rescaling factor \(r_{d}=1/4.9\), **Right**: Weight decay regularization with \(\lambda=10^{-4}\) and rescaling factor \(r_{d}=1/4.2\). We observe that under He initialization the effects of the explicit regularizer is even more pronounced. Unlike in other experiments, the presence of regularization _increases_ the rescaling factor and thus improves the fit to training data (for the radial profile).

**Bottom row:** We repeat the same as above with MSE loss rather than \(\ell^{1}\)-loss, using the Adam optimizer with learning rate \(10^{-5}\) instead of SGD with momentum, and using an overparametrized rather than underparametrized neural network. Without explicit regularization (left), the radial standard deviation is substantial, while explicit regularization leads to a more radially symmetric function, albeit at the price of a higher rescaling factor. For easy comparison to Figure 3, we present the optimal profile as rescaled in the main document as well. Both functions achieve training loss \(<10^{-3}\), but clearly generalization is poor without regularization: While the radial average is close to the target function \(f^{*}\equiv 0\) for inputs with \(\|x\|\geq 1\), the radial variation is high.

### Leaky ReLU activation

As noted by Wojtowytsch (2022), minimum norm interpolation is not stable when passing to an equivalent norm. A Barron space theory can be developed in perfect analogy for networks with the leaky ReLU activation function, and it is easy to see that the 'Barron' spaces for both activation functions coincide with equivalent norms, depending on the negative slope of the leaky ReLU function. However, the minimum norm interpolant \(f_{4}^{\star}\) with respect to the ReLU-based Barron-norm is not guaranteed to coincide with the minimum interpolant for the leaky-ReLU-based Barron norm. Experimentally, however, we observe strong agreement between the geometry of numerical solutions here.

### Deeper neural networks

We train neural networks of depth \(L>2\) to fit the same radially symmetric data as in Section 5.2. We see in Figure 17 that for depth \(L\geq 3\), weight decay-regularized networks strongly resemble the interpolant \(f_{Lip}(x)=\max\{1-\|x\|_{2},0\}\) with minimal (Euclidean) Lipschitz constant. This function can be written as a composition of two Barron functions \(f=f_{bump}\circ f_{norm}\)

\[f_{bump}(z)=\max\{1-z,0\}=\sigma(1-z),\qquad f_{norm}(x)=\|x\|_{2}=c_{d}\, \mathbb{E}_{\nu\sim\pi^{0}}\big{[}\sigma(\nu\cdot x)\big{]}\]

Figure 14: We examine the effects of the explicit regularizer (weight decay penalty \(\lambda=0\) for the top row and \(\lambda=10^{-4}\) for the bottom row) while varying dimensions (\(d=3,15,31\) from left to right) in the He initialization scheme. Optimizer settings were identical to the top row in Figure 13. The rescaling factors were \(r_{3}=1/1.15\), \(r_{15}=1/2.1\), and \(r_{31}=1/4.2\). As dimension grows implicit bias may be insufficient to find a minimum norm interpolant shape with reasonable scaling factor and may not enforce radial symmetry. In this case explicit regularizer may have an advantage.

Figure 15: A random feature model trained on the same dataset as the neural networks. The solutions produced this way are geometrically distinct from neural network solutions as they are ‘flat’ at the origin. The left two figures correspond to different initializations: Gain \(\alpha=\sqrt{2}\) (left) and gain \(\alpha=5\) (right). Notably, the variation is higher in radial direction in high dimension and higher than for the comparable neural network model. Perhaps surprisingly, higher gain appears to induce a better implicit bias in this case. No explicit regularization was used. Here we initialized the random feature by the same law as the neural network rather than initializing the outer layer at zero, since our goal is to study neural network dynamics, not find the optimal random feature solution. For the right plot, the initialization was random normal with gain 5 in the inner layer and zero in the outer layer with unsurprisingly better results.

where \(\pi^{0}\) denotes the uniform distribution on the unit sphere and \(c_{d}\sim\sqrt{d}\) is a dimension-dependent constant. We can thus approximate \(f_{Lip}\) efficiently by neural networks of depth \(L\geq 3\) as long as the first layer is sufficiently wide.

Unlike their shallow counterparts, neural networks with multiple hidden layers have no strong geometric prior without weight decay regularization. With weight decay, the observed behavior was relatively stable over a range of dimensions, initializations scalings and optimization algorithms. We are led to conjecture that \(f_{Lip}\) is a minimum norm interpolant in this setting. The statement remains imprecise at this point as no function space theory for deeper networks with weight decay regularizer has been developed to the same extent as Barron space theory.

## Appendix B \(\Gamma\)-convergence

In this appendix, we recall the definition and a few properties of \(\Gamma\)-convergence, a popular notion of the convergence of functionals introduced by De Giorgi and Franzoni (1975) in the calculus of variations to study the convergence of minimization problems. Braides (2002); Dal Maso (2012) provide introductions to the theory and its applications. As the notion is likely not familiar to readers from the machine learning community, we provide some full proofs as well.

**Definition B.1**.: _Let \((X,d)\) be a metric space and \(F_{n},F:X\to\mathbb{R}\cup\{-\infty,\infty\}\) be functions. We say that \(F_{n}\) converges to \(F\) in the sense of \(\Gamma\)-convergence if two conditions are met:_

1. _(_\(\liminf\)_-inquality) If_ \(x_{n}\) _is a sequence in_ \(X\) _and_ \(x_{n}\to x\)_, then_ \(\liminf_{n\to\infty}F_{n}(x_{n})\geq F(x)\)_._
2. _(_\(\limsup\)_-inequality) For every_ \(x\in X\)_, there exists a sequence_ \(x_{n}^{*}\in X\) _such that_ \(x_{n}^{*}\to x\) _and_ \(\limsup_{n\to\infty}F_{n}(x_{n}^{*})\leq F(x)\)_._

Intuitively, the first condition means that \(F(x)\) is (almost) a lower bound for \(F_{n}(x_{n})\) if \(n\) is 'large' and \(x_{n}\) is 'close' to \(x\), while the second condition means that there is no larger lower bound that we could choose. The sequence \(x_{n}^{*}\) is often referred to as a'recovery sequence'. Of course,

Figure 16: Neural networks with a single hidden layer and leaky ReLU activation \(\sigma(z)=\max\{z,0\}+0.1\,\min\{z,0\}\) trained in the setting of Section 5.2. Without theoretical foundation, we observe that the shape of \(f_{d}^{*}\) is attained to high accuracy also in this setting with the same rescaling factors as in the ReLU setting. **Left:** Momentum-SGD, **Middle:** Adam, **Right:** Momentum-SGD for a wider network with \(m=25,000\).

Figure 17: Neural networks of width 250 and varying depth were trained to fit data generated as in Section 5.2 with weight decay regularizers \(10^{-2},10^{-3},10^{-4}\) and \(0\) (left to right). The initialization gain variable was chosen as \(\alpha=\sqrt{2}\) as required to avoid exploding and vanishing gradients in deeper networks. Evidently, a small amount of weight decay regularization provides useful geometric prior without diminishing the quality of data fit.

Unlike networks with one hidden layer, deeper networks have positive (or nearly positive) outputs everywhere. While two-layer networks follow the minimum norm interpolation shape closely by the origin and have radial variances which increase outside the unit ball, deeper networks have positive radial variance inside the unit ball, but are essentially radially symmetric outside – compare e.g. Figure 9.

combining the liminf- and limsup-inequalities, we find that in fact \(F_{n}(x_{n}^{*})\to F(x)\). We employ \(\Gamma\)-convergence when dealing with minimization problems where uniform convergence fails, but we hope for convergence of minimizers to minimizers.

Often, \(\Gamma\)-convergence is considered as a continuous parameter \(\varepsilon\) approaches \(0^{+}\) rather than as the discrete parameter \(n\) approaches infinity. The definitions remain largely identical (with obvious substitutions).

To get a feeling for \(\Gamma\)-convergence, we consider a particularly simple situation by looking at two constant sequences of functions. Note that the sequence is constant, not the functions.

**Example B.2**.: _Let \(X=\mathbb{R}\) and consider the constant sequences_

\[F_{n}(x)=f(x)=\begin{cases}1&x\neq 0\\ 0&x=0\end{cases},\qquad G_{n}(x)=g(x)=\begin{cases}0&x\neq 0\\ 1&x=0\end{cases}.\]

_We claim that_

\[\big{(}\Gamma-\lim_{n\to\infty}F_{n}\big{)}(x)=f(x),\quad\big{(}\Gamma-\lim_{ n\to\infty}G_{n}\big{)}(x)=0\qquad\forall\;x\in\mathbb{R}.\]

_If \(x_{n}\to x\) and \(x\neq 0\), then \(F_{n}(x_{n})=1\) and \(G_{n}(x_{n})=0\) for all but finitely many \(n\in\mathbb{N}\), meaning that \(F_{n}(x_{n})\to 1=f(x)\) and \(G_{n}(x_{n})\to 0\). It remains to consider the case \(x_{n}\to 0\)._

_We see immediately that \(F_{n}(x_{n})\geq 0=f(0)\) for all \(n\in\mathbb{N}\). Conversely, if we take \(x_{n}^{*}=0\) for all \(n\), then \(x_{n}^{*}\to 0\) and \(F_{n}(x_{n}^{*})=f(0)\to f(0)\). In total, we conclude that \(\Gamma-\lim F_{n}=f\)._

_For \(G_{n}\), we find that \(G_{n}(x_{n})\geq 0\) for all \(n\in\mathbb{N}\). Additionally, we can choose the sequence \(x_{n}=1/n\) such that \(G_{n}(x_{n})=0\) for all \(n\in\mathbb{N}\). Altogether, we find that \(\Gamma-\lim G_{n}=0\)._

More generally, if \(F_{n}=F\) for all \(n\in\mathbb{N}\) and some \(F:X\to\mathbb{R}\), then \(\Gamma-\lim_{n\to\infty}F_{n}=\overline{F}\) is the lower semi-continuous envelope of \(F\). In particular, \(\Gamma-\lim_{n\to\infty}F=F\) if and only if \(F\) is lower semi-continuous. The main useful properties of \(\Gamma\)-convergence are summarized in the following lemma.

**Lemma B.3**.: _Assume that \(F_{n}\to F\) in the sense of \(\Gamma\)-convergence, \(\varepsilon_{n}\to 0^{+}\) and \(x_{n}\in X\) is a sequence such that_

\[F_{n}(x_{n})\leq\inf_{x\in X}F_{n}(x)+\varepsilon_{n}.\]

_Assume that \(x_{n}\to x^{*}\). Then \(F(x^{*})=\inf_{x\in X}F(x)\). In particular, if \(x_{n}\) is a minimizer of \(F_{n}\) and the sequence \(x_{n}\) converges, then the limit point is a minimizer of \(F\)._

Clearly, this is most useful if we can guarantee that the sequence \(x_{n}\) converges. For many useful sequences of functionals, the existence of a convergent subsequence can be established by compactness. This is easily sufficient, as we can also pass to a subsequence in \(F_{n}\).

Proof.: Due to the liminf-inequality, we have

\[F(x^{*})\leq\liminf_{n\to\infty}F_{n}(x_{n})=\liminf_{n\to\infty}\inf_{x\in X }F_{n}(x).\]

On the other hand, let \(x\in X\) be any point. Then, due to the limsup-inequality, there exists some sequence \(x_{n}^{\prime}\) such that

\[x_{n}^{\prime}\to x,\qquad F(x)=\lim_{n\to\infty}F_{n}(x_{n}^{\prime})\geq \liminf_{n\to\infty}\inf_{x\in X}F_{n}(x).\]

In particular \(\inf_{x\in X}F(x)\geq\liminf_{n\to\infty}\inf_{x\in X}F_{n}(x)\). Combining the two estimates, we find that \(F(x^{*})\leq\inf_{x\in X}F(x)\), which means that \(x^{*}\) is a minimizer of \(F\). 

For completeness, a few observations are in order.

1. The notion of \(\Gamma\)-convergence relies on the notion of convergence on the underlying space \(X\), and \(\Gamma\)-limits can change when keeping the set \(X\) fixed, but passing to a different topology (e.g. a weak topology in infinite-dimensional spaces).
2. \(\Gamma\)-convergence is made for minimization problems, and it does not behave well under multiplication by negative real numbers: In general \(\Gamma-\lim(-F_{n})\neq-(\Gamma-\lim F_{n})\), even if both limits exist. The reason is the asymmetry between the \(\liminf\)- and the \(\limsup\)-condition. To see this, consider for instance \(F_{n}\) and \(G_{n}-1\) in Example B.2.

3. If \(F_{n}\to F\) and \(G_{n}\to G\), it is not necessarily true that \(F_{n}+G_{n}\to F+G\). While it remains true that \(\liminf_{n\to\infty}(F_{n}+G_{n})(x_{n})\geq(F+G)(x)\) if \(x_{n}\to x\), it may no longer be possible to find a recovery sequence \(x_{n}\) for \(F_{n}+G_{n}\). For example, if \(F_{n}=1_{\mathbb{Q}}\) and \(G_{n}=1_{\mathbb{R}\setminus\mathbb{Q}}\) for all \(n\in\mathbb{N}\), then \(F_{n}\xrightarrow{\Gamma}0\) and \(G_{n}\xrightarrow{\Gamma}0\), but \(F_{n}+G_{n}=1\) for all \(n\) and \(F_{n}+G_{n}\xrightarrow{\Gamma}1\). However, if \(F_{n}\xrightarrow{\Gamma}F\) and \(G_{n}\) converges to a _continuous_ limit \(G\)_uniformly_, then \((F_{n}+G_{n})\xrightarrow{\Gamma}F+G\). In particular, uniform convergence implies \(\Gamma\)-convergence. Namely, if \(G_{n}\to G\) uniformly, \(G\) is continuous and \(x_{n}\to x\), then for given \(\varepsilon>0\), we can choose \(N\in\mathbb{N}\) so large that 1. \(|G(x_{n})-G(x)|<\varepsilon/2\) for all \(n\geq N\) since \(G\) is continuous at \(x\) and 2. \(|G_{n}(x_{n})-G(x_{n})|<\varepsilon/2\) for all \(n\geq N\) due to uniform convergence. Then \[\big{|}G_{n}(x_{n})-G(x)\big{|}\leq\big{|}G_{n}(x_{n})-G(x_{n})\big{|}+\big{|} G(x_{n})-G(x)\big{|}<\varepsilon.\] In particular \(G_{n}(x_{n})\to G(x)\). Recall that \(G\) is guaranteed to be continuous if \(G_{n}\) is continuous for all \(n\in\mathbb{N}\).
4. \(\Gamma\)-convergence is unrelated to pointwise convergence of functions: Neither does it imply pointwise convergence, nor is it implied by it. Namely, the sequence \(G_{n}\) in Example B.2 has the function \(g\) as a pointwise limit and the constant function \(0\) as a \(\Gamma\)-limit.
5. \(\Gamma\)-convergence is not a notion of convergence derived from a topology. Indeed, even if \(F_{n}\) is a constant sequence, i.e. if \(F_{n}=G\) for all \(n\), it may happen that \(\Gamma-\lim_{n\to\infty}F_{n}\neq G\) (see \(G_{n}\) in Example B.2). The \(\Gamma\)-limit is related to \(G\), though: It is the lower semi-continuous envelope of the function \(G\). In fact, every \(\Gamma\)-limit is lower semi-continuous.

Despite its somewhat counterintuitive properties, \(\Gamma\)-convergence has proved invaluable in many areas of the calculus of variations. It has been applied to homogenization by Bach et al. (2021), dimension reduction for thin sheets and shells by Friesecke et al. (2002a, 2003, 2002b), Bhattacharya et al. (2016), Lewicka et al. (2010) and the study of phase boundaries by Modica and Mortola (1977), Modica (1987). While the \(\Gamma\)-convergence of functionals does not imply the convergence of their gradient flows even in situations of practical significance (see e.g. the example of Dondl et al. (2019)), Serfaty (2011), Sandier and Serfaty (2004), Mugnai and Roger (2011), Ilmanen (1993), Alikakos et al. (1994) provide important examples of situations where this can be established in a suitable sense. Even more, Bronsard and Kohn (1990) use \(\Gamma\)-convergence to gain insight into PDE dynamics.

## Appendix C Homogeneous Barron spaces

In this section, we introduce the abstract framework which is used to prove our main theoretical result, Corollary 3.3. A neural network with \(m\) neurons in a single hidden layer can be represented as

\[f_{m}(x)=b_{0}+\sum_{i=1}^{m}a_{i}\,\sigma(w_{i}^{T}x+b_{i})\qquad\text{or} \quad f_{m}(x)=b_{0}+\frac{1}{m}\sum_{i=1}^{m}a_{i}\,\sigma(w_{i}^{T}x+b_{i}).\] (4)

The network weights and biases are \((a,W,b)\in\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\times\mathbb{R}^{m+1}\). The normalization depends on personal preference, with the former being more common in practice and the latter more common in theoretic analyses. We define the weight decay regularizer by

\[R_{WD}(a,W,b)=\frac{\|a\|_{\ell^{2}}^{2}+\|W\|_{F}^{2}}{2}=\frac{1}{2}\left( \sum_{i=1}^{m}a_{i}^{2}+\sum_{i=1}^{m}\sum_{j=1}^{d}w_{ij}^{2}\right)\]

or \(R_{WD}(a,W,b)=\frac{1}{2m}\sum_{i=1}^{m}\left(a_{i}^{2}+\|w_{i}\|_{\ell^{2}}^{ 2}\right)\) respectively. Here \(\|\cdot\|_{2}\) denotes the Euclidean \(\ell^{2}\)-norm of a vector and \(\|W\|_{F}\) denotes the Frobenius norm of the matrix \(W\) whose rows are the vectors \(w_{i}^{T}\). Note that we do not control the magnitude of the biases \(b_{i}\) in the regularizer. This is a common approach, as the bias does not influence the Lipschitz-constant of the function represented by the neural network, which is useful in studying the generalization of the neural network.

We study function classes corresponding to arbitrarily wide neural networks with a single hidden layer, where the norm corresponds to the weight decay regularizer. We dub these function spaces 'homogeneous Barron spaces' in analogy to the more classical Barron spaces studied by E et al. (2019); Ma et al. (2020); E and Wojtowych (2020), which correspond to a weight decay regularizer which also controls the bias. For homogeneous Barron spaces, coordinate transformations by Euclidean motions induce an isometry of the function class, while the origin plays a special role in classical Barron spaces. This justifies our terminology, as the data space is treated as isotropic and homogeneous by this function class. Homogeneous Barron spaces have also been studied by Ongie et al. (2019); Parhi and Nowak (2021, 2022) under the name Radon-BV spaces. A closely related class of spaces has been considered as the 'variation spaces of the ReLU dictionary' by Siegel and Xu (2020, 2022, 2023).

Heuristically, (homogeneous) Barron spaces are a function class tailored to replacing the finite superposition of ReLU ridges in (4) by an arbitrary superposition while keeping the weight decay regularizer finite. Due to the lack of control over the bias term, we will see that a slightly awkward technical definition is needed. Let \(\pi\) be a probability distribution on the parameter space \(\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}\). We would like to define

\[f_{\pi,b_{0}}:\mathbb{R}^{d}\to\mathbb{R},\quad f_{\pi,b_{0}}(x)=b_{0}+\mathbb{ E}_{(a,w,b)\sim\pi}\big{[}a\,\sigma(w^{T}x+b)\big{]}=\int_{\mathbb{R}\times \mathbb{R}^{d}\times\mathbb{R}}a\,\sigma(w^{T}x+b)\,\,\mathrm{d}\pi_{(a,w,b)}\]

and

\[R_{WD}(\pi)=\frac{1}{2}\int_{\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}} |a|^{2}+\|w\|_{2}^{2}\,\,\mathrm{d}\pi_{(a,w,b)}.\]

\(f_{\pi,b_{0}}\) is an analogue of neural networks with a single hidden layer, but of arbitrary and possibly uncountably infinite width. Every finite neural network can be expressed in this fashion for the empirical measure \(\pi_{m}=\frac{1}{m}\sum_{i=1}^{m}\delta_{(a_{i},w_{i},b_{i})}\), in which case \(R_{WD}(\pi_{m})=R_{WD}(a,W,b)\).

Unfortunately, even if \(R_{WD}(\pi)<\infty\), it is not clear that the integral defining \(f_{\pi}\) exists in a meaningful sense. We do however note that formally

\[|f_{\pi}(x)-f_{\pi}(y)| =\mathbb{E}\big{[}a\big{\{}\sigma(w^{T}x+b)-\sigma(w^{T}y+b) \big{\}}\big{]}\leq\mathbb{E}\big{[}|a|\big{|}|w^{T}x+b|-|w^{T}y+b|\big{|} \big{]}\] \[\leq\|x-y\|\,\mathbb{E}\big{[}|a|\,\|w\|\big{]}\leq\frac{\|x-y\| }{2}\mathbb{E}\big{[}|a|^{2}+\|w\|^{2}\big{]}=\|x-y\|\,R_{WD}(\pi).\]

Thus, the integral defining \(f_{\pi}(x)\) exists for all \(x\) if and only if it exists for, say, \(x=0\). We exploit this in the following modified definition. Let \(\pi\) be a probability distribution on the parameter space \(\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}\) and \(y\in\mathbb{R}\). We denote

\[f_{\pi,y}(x)=y+\mathbb{E}_{(a,w,b)\sim\pi}\big{[}a\big{(}\sigma(w^{T}x+b)- \sigma(b)\big{)}\big{]}\]

By the same argument as before, we observe that

1. \(f_{\pi,y}(0)=y\) and
2. \(|f_{\pi,y}(x)-f_{\pi,y}(x^{\prime})|\leq R_{WD}(\pi)\,\|x-x^{\prime}\|\).

The class of functions of the form \(f_{\pi,y}\) forms the homogeneous Barron space. Still, every finite neural network \(f_{m}\) can be represented in this fashion with \(b_{0}=y-\sum_{i=1}^{m}a_{i}\sigma(b_{i})\) or \(b_{0}=y-\frac{1}{m}\sum_{i=1}^{m}a_{i}\sigma(b_{i})\).

**Definition C.1** (Homogeneous Barron space).: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be a function. We define the semi-norms_

\[[f]_{\mathcal{B}}=\inf_{f\equiv f_{\pi,y}}R_{WD}(\pi),\qquad[f]_{0}=|f(0)|.\]

_The homogeneous Barron space is the function class \(\mathcal{B}(\mathbb{R}^{d})=\{f:\mathbb{R}^{d}\to\mathbb{R}:[f]_{\mathcal{B} }<\infty\}\). By the definition of a function \([f]_{0}<\infty\) is automatically true._

We note a few important properties. First, we consider two function classes:

\[\mathcal{F}_{Q} =\overline{\mathrm{conv}\big{\{}a\left(\sigma(w\cdot x+b)-\sigma( b)\right):a^{2}+\|w\|^{2}}\leq 2Q\big{\}}\] \[\mathcal{F}_{Q}(R) =\overline{\mathrm{conv}\big{\{}a\left(\sigma(w\cdot x+b)-\sigma( b)\right):a^{2}+\|w\|^{2}}\leq 2Q,|b|\leq\sqrt{Q}\,R\big{\}}.\]Note that \(\mathcal{F}_{Q}(R)\subseteq\mathcal{F}_{Q}\subseteq\{f\in\mathcal{B}:f(0)=0,\,[f]_{ \mathcal{B}}\leq Q\}\). The closure is taken with respect to locally uniform convergence, i.e. pointwise convergence which is uniform on all compact sets.1 Due to the homogeneity of ReLU activation, we may prove the following.

Footnote 1: This notion of convergence is generated by a topology, but not a metric. Other notions of convergence can be considered and induce the same function class.

**Lemma C.2**.: _The identity \(\mathcal{F}_{Q}=\{f\in\mathcal{B}:f(0)=0,\,[f]_{\mathcal{B}}\leq Q\}\) holds._

While the claim is natural, its proof is surprisingly technical and postponed until the end of the section. The class \(\mathcal{F}_{Q}(R)\) will be used below for technical purposes. Of major importance below are the compact embedding theorem and the direct approximation theorem.

**Theorem C.3** (Compact embedding).: _Let \(f_{n}\in\mathcal{B}\) be a sequence such that \(\liminf_{n\to\infty}[f_{n}]_{0}+[f_{n}]_{\mathcal{B}}<+\infty\). Then there exists \(f\) in \(\mathcal{B}\) such that_

1. \(f_{n}\to f\) _in_ \(C^{0}(K)\) _for all compact sets_ \(K\subseteq\mathbb{R}^{d}\)_._
2. \(f_{n}\to f\) _in_ \(L^{p}(\mu)\) _for all measures_ \(\mu\) _with finite_ \(p\)_-th moments,_ \(p\in[1,\infty)\)_._
3. \([f]_{\mathcal{B}}\leq\liminf_{n\to\infty}[f_{n}]_{\mathcal{B}}\)_._

Proof.: It is sufficient to show that the set \(\tilde{\mathcal{F}}_{Q}=\{a\big{(}\sigma(w\cdot x+b)-\sigma(b)\big{)}\mid a^ {2}+|w|^{2}\leq 2Q\}\) is compact in \(C^{0}(K)\) and \(L^{2}(\mu)\) for all \(K\) and \(\mu\) as above, in which case also its closed convex hull is compact [16, Theorem 3.20.]. To this end, observe that the map

\[F:\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}\to C^{0}(K),\qquad(a,w,b) \mapsto a\big{(}\sigma(w^{+}+b)-\sigma(b)\big{)}\]

is continuous for any compact set \(K\). Since \(K\) is compact, we have \(K\subseteq B_{R}(0)\) for some \(R>0\) and thus \(|w\cdot x|\leq\sqrt{2Q}R\) for all \(x\in K\). In particular,

\[\sigma(w\cdot x+b)-\sigma(b)=\begin{cases}w\cdot x&\text{if }b>\sqrt{2Q}R\\ 0&\text{if }b<\sqrt{2Q}R\end{cases}.\]

Hence \(\tilde{\mathcal{F}}_{Q}=F(\{(a,w,b):a^{2}+|w|^{2}\leq 2Q,b\leq\sqrt{2Q}R\})\) is the continuous image of a compact set, hence compact. We have thus proved a compact embedding into \(C^{0}(K)\) for any compact set \(K\).

Exhausting \(\mathbb{R}^{d}\) by the sequence of compact sets \(\overline{B_{m}(0)}\), \(m\in\mathbb{N}\) and using a diagonal sequence argument, we see that under the conditions of Theorem C.3, there exists \(f\in\mathcal{B}\) such that \(f_{n}\to f\) pointwise everywhere on \(\mathbb{R}^{d}\) and uniformly on compact subsets. Additionally, we observe that \(f(0)=0\) and there exists a uniform upper bound on the Lipschitz constants of the sequence \(f_{n}\). We conclude that \(f_{n}\to f\) in \(L^{p}(\mu)\) from the Dominated Convergence Theorem using \(Q\|x\|\) as a dominating function. 

As a consequence, we find the following.

**Corollary C.4**.:
1. \(\mathcal{B}\) _is a Banach space._
2. \(C_{c}^{\infty}(\mathbb{R}^{d})\subseteq\mathcal{B}(\mathbb{R}^{d})\)_._

Proof.: The first claim follows as in [20, Lemma 1], where it is proved in a more general context for dictionaries which are compact in a Hilbert space - in our case, the dictionary \(x\mapsto a\big{\{}\sigma(w^{T}x+b)-\sigma(b)\big{\}}\). The second claim follows from [14, Corollary 1]. 

We conclude with a theorem which establishes a rate of approximation for Barron functions in a weaker topology.

**Theorem C.5** (Direct approximation).: _Let \(f\in\mathcal{B}\) and \(\mu\) a measure on \(\mathbb{R}^{d}\) with finite second moments. Then for any \(m\in\mathbb{N}\) there exist \(c\in\mathbb{R}\) and \((a_{i},w_{i},b_{i})\in\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}\) such that_

\[\sum_{i=1}^{m}a_{i}^{2}+\|w_{i}\|^{2}\leq[f]_{\mathcal{B}},\quad\left\|f-c- \sum_{i=1}^{m}a_{i}\sigma(w_{i}^{T}x+b_{i})\right\|_{L^{2}(\mu)}\leq\frac{2\,[ f]_{\mathcal{B}}}{\sqrt{m}}\,\sup_{\|w\|=1}\sqrt{\int_{\mathbb{R}^{d}}|w^{T}x|^{2} \,\,\mathrm{d}\mu_{x}}.\]Proof.: A proof of this result can be found in [20, Appendix C] in the proof of Proposition 2.6. 

Proof of Lemma c.2.: **Step 1.** Assume that \(f\in\mathcal{B}\) such that \(f(0)=0\) and \([f]_{\mathcal{B}}\leq Q\). By the Direct Approximation Theorem (which is proved in [20, Appendix C] without using Lemma C.2), we find that for every \(m\in\mathbb{N}\) and every measure \(\mu\) on \(\mathbb{R}^{d}\) with finite second moments, there exists

\[f_{m}(x)=\frac{1}{m}\sum_{i=1}^{m}a_{i}\{\sigma(w_{i}\cdot x+b_{i})-\sigma(b_{ i})\}\quad\in\mathcal{F}_{Q}\]

such that \(\|f_{m}-f\|_{L^{2}(\mu)}\leq C_{\mu}\|f\|_{\mathcal{B}}m^{-1/2}\). In particular, \(f\) is in the closed convex hull of \(\{a\left(\sigma(w^{T}x+b)-\sigma(b)\right):a^{2}+|w|^{2}\leq 2Q\}\) if the closure is taken with respect to the \(L^{2}(\mu)\) topology. Additionally, the sequence \(f_{m}\) has a uniformly bounded Lipschitz constant and is therefore compact in \(C^{0}(K)\) for all compact \(K\) by a corollary to the Arzela-Ascoli theorem [20, 21]. In particular, \(f_{m}\to f\) uniformly and thus \(f\in\mathcal{F}_{Q}\).

**Step 2.** Denote \(f_{(a,w,b)}(x)=a\left\{\sigma(w^{T}x+b)-\sigma(b)\right\}\). Since \(f_{(a,w,b)}(0)=a\{\sigma(b)-\sigma(b)\}=0\) for all \(a,w,b\), we conclude that \(f(0)=0\) for all \(f\in\mathcal{F}_{Q}\). If \(f\in\mathcal{F}_{Q}\), then there exists a sequence

\[f_{n}(x)=\sum_{i=1}^{N_{n}}\lambda_{i,n}a_{i,n}\left\{\sigma(w_{i,n}\cdot x+b _{i,n})-\sigma(b_{i,n})\right\}\]

such that \(f_{n}\to f\) locally uniformly. If the biases remain uniformly bounded, the sequence of empirical distributions

\[\pi_{n}=\frac{1}{N_{n}}\sum_{i=1}^{N_{n}}\lambda_{i,n}\,\delta_{(a_{i,n},w_{i,n},b_{i,n})}\]

has a convergent subsequence by Prokhorov's Theorem [10, 22]. We denote the limiting distribution as \(\pi\). The convergence of Radon measures implies the convergence of \(f_{n}\) to \(f_{\pi}(x)=\mathbb{E}_{(a,w,b)\sim\pi}\big{[}a\left\{\sigma(w^{T}x+b)-\sigma( b)\right\}\big{]}\) by definition. Since \(f_{n}\) converges locally uniformly by assumption, \(f=f_{\pi}\in\mathcal{B}\) and \([f]_{\mathcal{B}}\leq Q\).

If the biases do not remain bounded, we note that for every compact set \(K\subseteq\mathbb{R}^{d}\) we can extract a convergent subsequence of the measures by the same argument used to prove Theorem C.3, effectively making the sequence of biases bounded. We can extend the argument to the entire space exploiting that

\[\lim_{b\to\infty}\big{(}\sigma(w\cdot x+b)-\sigma(b)\big{)}\to\sigma(b/|b|)\,w \cdot x\]

locally uniformly. 

## Appendix D Rademacher complexity of homogeneous Barron space

Following a classical strategy implemented e.g. by E et al. [20] in a similar context, we estimate the Rademacher complexity of homogeneous Barron space and use it to bound the generalization gap (i.e. the discrepancy between empirical risk and population risk). In our setting, we face additional technical obstacles:

1. We deal with general sub-Gaussian data distributions \(\mu\) rather than data distributions with compact support.
2. We do not control the magnitude of the bias variables.
3. We consider \(\ell^{2}\)-loss, which is neither globally Lipschitz-continuous nor bounded.

In combination, these complications require a refined technical analysis similar to Appendix C. Let us summarize several notations which will be needed below.

* the empirical Rademacher complexity of a function class over a given dataset.
* the expected Rademacher complexity of a function class over a data set composed of \(n\) iid samples from the data distribution \(\mu\).
- the population risk \(\mathcal{R}(f)=\|f-f^{*}\|_{L^{2}(\mu)}^{2}=\mathbb{E}_{x\sim\mu}[|f(x)-f^{*}(x)|^ {2}]\). We generally take this to operate on the level of functions, parametrized or not. By an abuse of notation, we identify \(\mathcal{R}(a,W,b):=\mathcal{R}(f_{(a,W,b)})\).
* the empirical risk \(\widehat{\mathcal{R}}_{n}(f)=\|f-f^{*}\|_{L^{2}(\mu_{n})}^{2}=\frac{1}{n}\sum_ {i=1}^{n}|f(x_{i})-f^{*}(x_{i})|^{2}\) over a data set \(\{x_{1},\ldots,x_{n}\}\) where \(\mu_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{x_{i}}\) is the empirical measure. Equally, \(\widehat{\mathcal{R}}_{n}\) can be considered for functions or parameters with the natural identification.
* \(\widehat{\mathcal{R}}_{n,m,\lambda}\). The regularized empirical risk \[\widehat{\mathcal{R}}_{n,m,\lambda}(a,W,b)=\widehat{\mathcal{R}}_{n}(a,W,b)+ \frac{\lambda}{2}\big{(}\|a\|^{2}+\|W\|^{2}\big{)}.\] We only consider this quantity on the parameter level, where it is computable. While the weight decay regularizer is an upper bound for \([f_{(a,W,b)}]_{\mathcal{B}}\), the two are generally not the same since the parameter-to-function map of a neural network is generally not injective.
* the weight decay regularizer.
* the set of functions for which \([f]_{\mathcal{B}}\leq Q\) and \(f(0)=0\).
* the set of functions for which \([f]_{\mathcal{B}}\leq Q\) and \(|f(0)|\leq A\).

As is common in the mathematics community, \(C\) will generally denote a constant which does not depend on quantities (unless specified otherwise) and which may change value from line to line. Some facts about sub-Gaussian distributions, which we believe to be well-known to the experts, are collected in Appendix H.

**Definition D.1** (Rademacher Complexity).: _Let \(S=\{x_{1},\ldots,x_{n}\}\) be a set of points in \(\mathbb{R}^{d}\) (a data sample) and \(\mathcal{F}\) a real-valued function class. We define the empirical Rademacher complexity of \(\mathcal{F}\) on the data sample as_

\[\widehat{\operatorname{Rad}}(\mathcal{F};S)=\mathbb{E}_{\varepsilon}\left[ \sup_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}f(x_{i})\right]\]

_where \(\varepsilon_{i}\) are iid random variables which take the values \(\pm 1\) with equal probability \(\frac{1}{2}\). The population Rademacher complexity is defined as_

\[\operatorname{Rad}_{n}(\mathcal{F})=\mathbb{E}_{S\sim\mu^{n}}\big{[}\widehat {\operatorname{Rad}}(\mathcal{F};S)\big{]},\]

_i.e. as the expected empirical Rademacher complexity over a set of \(n\) iid data points._

In this section, we will find a upper bound of Rademacher Complexity of \(\mathcal{B}\). We will denote by \(S_{n}\) the set of \(n\) samples, and \(\widehat{\operatorname{Rad}}(\mathcal{F},S_{n})\) the sample Rademacher Complexity of \(\mathcal{F}\) given the samples \(S_{n}\). We furthermore denote \(R:=\max\{\|x_{1}\|,\ldots,\|x_{n}\|\}\) and consider the function classes \(\mathcal{F}_{Q}\) and \(\mathcal{F}_{Q}(R)\) as in Appendix C:

\[\mathcal{F}_{Q}=\] \[\mathcal{F}_{Q}(R)=\]

**Lemma D.2**.: _Let \(S_{n}=\{x_{1},\ldots,x_{n}\}\) be a data set in \(\mathbb{R}^{d}\). Then_

\[\widehat{\operatorname{Rad}}(\mathcal{F}_{Q},S_{n})\leq\frac{\big{(}1+3\sqrt{2 }\big{)}Q}{\sqrt{n}}\,\max_{1\leq i\leq n}\|x_{i}\|.\]

_Assume \(\mu\) is a \(\sigma^{2}\) sub-Gaussian distribution in \(\mathbb{R}^{d}\). Then_

\[\operatorname{Rad}(\mathcal{F}_{Q})\leq\big{(}1+3\sqrt{2}\big{)}Q\left(\frac{ \mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}}{\sqrt{n}}+\sigma\sqrt{2\frac{\log n }{n}}\right)\]

_for all \(n\geq 2\)._Proof.: Initially, we fix a set \(S=\{x_{2},\ldots,x_{n}\}\) of \(n\) points. We will later take the expectation over \(S\), using the sub-Gaussian property of \(\mu\) for an explicit norm bound. Define \(R:=\max_{1\leq i\leq n}\|x_{i}\|\). To this end, we first prove the following claim, which enables us to focus on only single neuron functions instead of entire \(\mathcal{F}_{Q}\):

**Claim:** Let \(\varepsilon_{1},\ldots,\varepsilon_{n}\in\mathbb{R}\). Then

\[\sup_{\mathcal{F}_{Q}}\sum_{i}\epsilon_{i}f(x_{i})=\sup_{a^{2}+\|w\|^{2}\leq 2 Q}\sum_{i}\epsilon_{i}\,a\big{\{}\sigma(w^{T}x_{i}+b)-\sigma(b)\big{\}}\]

Proof of Claim.: Note that \(\mathcal{F}_{Q}\) is the closed convex hull of single neuron ridge functions, i.e. single neuron ridge functions are the extreme points of the closed convex set \(\mathcal{F}_{Q}\).

To verify the claim, first note that \(f\mapsto\sum_{i=1}^{n}\varepsilon_{i}f(x_{i})\) is a continuous linear functional on \(C^{0}(K)\) for any compact \(K\subseteq\mathbb{R}^{d}\) containing the finite set \(S\). It is well known that \(C^{0}(K)\) is a Banach Space. Therefore, if \(\{a\big{(}\sigma(w\cdot x+b)-\sigma(b)\big{)}\mid a^{2}+|w|^{2}\leq 2Q\}\) is compact in \(C^{0}(K)\), then [11, Theorem 3.20.] implies that \(\mathcal{F}_{Q}\), a closed convex hull of the compact set, is also compact. Then, from the compactness of \(\mathcal{F}_{Q}\), we can use Bauer [1958]'s maximum principle and see that the supremum is attained at an extreme point. Compactness follows from the compact embedding Theorem, see Theorem C.3 above. 

Over the next steps, we will bound \(\widehat{\operatorname{Rad}}(\mathcal{F}_{Q},S_{n})\).

**Step 1.** In this step, we prove that

\[\widehat{\operatorname{Rad}}(\mathcal{F}_{Q};S_{n})=\widehat{\operatorname{ Rad}}\big{(}\mathcal{F}_{Q}(R);S_{n}\big{)}.\]

To show this, we first observe that if \(|b|\geq\|w\|R\), then \(\sigma(w\cdot x+b)-\sigma(b)=\sigma\big{(}sgn(b)\big{)}w\cdot x\) since \(|w^{T}x_{i}|\leq\|w\|\,\|x_{i}\|\leq|b|R\) for all \(1\leq i\leq n\). This means for \(\forall|b|\geq\|w\|R\), the precise value of \(b\) does not change the value of \(\sigma(w\cdot x+b)-\sigma(b)\).

Now, we compute the \(\widehat{\operatorname{Rad}}(\mathcal{F}_{Q},S_{n})\):

\[n\,\widehat{\operatorname{Rad}}(\mathcal{F}_{Q},S_{n}) =\mathbb{E}_{\epsilon}\left[\sup_{\mathcal{F}_{Q}}\sum_{i}\epsilon _{i}f(x_{i})\right]\] \[=\mathbb{E}_{\epsilon}\left[\sup_{a^{2}+\|w\|^{2}\leq 2Q,|b|\leq\|w\|R }\sum_{i}\epsilon_{i}a\big{\{}\sigma(w\cdot x_{i}+b)-\sigma(b)\big{\}}\right]\] \[=n\,\widehat{\operatorname{Rad}}\big{(}\mathcal{F}_{Q}(R),S_{n} \big{)}\]

For the first line, we used the claim.

**Step 2.** Using the uniform bound on the magnitude of the bias from the previous step, in this step we bound the Rademacher complexity by

\[\widehat{\operatorname{Rad}}(\mathcal{F}_{Q};S)=\widehat{\operatorname{Rad}} \big{(}\mathcal{F}_{Q}(R);S\big{)}\leq\mathbb{E}_{\epsilon}\left[\sup_{|w| \leq Q,\;|b|\leq QR}\left|\frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}\,\sigma(w \cdot x_{i}+b)\right|\right]+\frac{QR}{\sqrt{n}}\]

We verify this from the definition of Rademacher Complexity of \(\mathcal{F}_{Q}(R)\). Note that \(a\sigma(wx+b)=(\lambda a)\,\sigma\big{(}(w/\lambda)x+b/\lambda\big{)}\). In particular, we may assume without loss of generality that \(|a|^{2}=\|w\|^{2}\leq Q\) for optimal balance which makes \(a^{2}+\|w\|^{2}\) minimal without changing the neuron output.

\[n\operatorname{Rad}\big{(}\mathcal{F}_{Q}(R),S_{n}\big{)}=\mathbb{E}_{ \epsilon}\left[\sup_{\mathcal{F}_{Q}}\epsilon_{i}f(x_{i})\right]\]\[=\mathbb{E}_{\epsilon}\left[\sup_{a^{2}+\|w\|^{2}\leq Q,\;|b|\leq\sqrt{ Q}R}\left(\left|\sum_{i}\epsilon_{i}a\sigma(w\cdot x_{i}+b)\right|+\left|\sum_{i} \epsilon_{i}a\sigma(b)\right|\right)\right].\]

In this step, we only consider the first term.:

\[\mathbb{E}_{\varepsilon}\left[\sup_{|a|=\|w\|\leq\sqrt{Q},\;|b| \leq\sqrt{Q}R}\left|\sum_{i}\epsilon_{i}a\sigma(b)\right|\right] \leq\mathbb{E}_{\epsilon}\left[\sup_{|a|\leq\sqrt{Q},\;|b|\leq \sqrt{Q}R}\left|\sum_{i}\epsilon_{i}a\sigma(b)\right|\right]\] \[\leq\sup_{|a|\leq\sqrt{Q},\;|b|\leq\sqrt{Q}R}|a|\sigma(b)\, \mathbb{E}_{\epsilon}\left|\sum_{i}\epsilon_{i}\right|\leq QR\,\sqrt{n}.\]

The first line is again by applying the claim to \(\mathcal{F}_{Q}(R)\). In the last line, we used two facts:

1. \(\sigma\) is ReLU, so \(|a|\sigma(b)\leq|ab|\leq\sqrt{Q}\cdot\sqrt{Q}R=QR\) and
2. the observation that \[\mathbb{E}_{\epsilon}\left|\sum_{i}^{n}\epsilon_{i}\right|\leq\sqrt{\mathbb{E }_{\epsilon}\left|\sum_{i}^{n}\epsilon_{i}\right|^{2}}=\sqrt{\sum_{i,j=1}^{n} \mathbb{E}_{\epsilon}[\varepsilon_{i}\varepsilon_{j}]}=\sqrt{\sum_{i=1}^{n} \mathbb{E}_{\varepsilon}[\varepsilon_{i}^{2}]}=\sqrt{n}\] since \(\mathbb{E}[\varepsilon_{i}]=0\), \(\varepsilon_{i}\) and \(\varepsilon_{j}\) are independent if \(i\neq j\) and \(\varepsilon_{i}^{2}\equiv 1\).

**Step 3.** In this step, we prove that

\[\frac{1}{n}\mathbb{E}_{\varepsilon}\left[\sup_{|a|=\|w\|\leq\sqrt{Q},\;|b|\leq \sqrt{Q}R}\left|\sum_{i=1}^{n}\varepsilon_{i}a\sigma(w\cdot x_{i}+b)\right| \right]\leq\frac{3\sqrt{2}QR}{\sqrt{n}}\]

To this end, we modify the data points as \(\tilde{x_{i}}=(x_{i},R)\) and the parameters as \(\tilde{w}=(w^{T},\frac{b}{R})\). Then, observe that \(w\cdot x_{i}+b=\tilde{w}\cdot\tilde{x_{i}}\), \(\|\tilde{x_{i}}\|=\sqrt{\|x_{i}\|^{2}+R^{2}}\leq\sqrt{2}R\), and \(a^{2}+\|\tilde{w}\|^{2}=a^{2}+\|w\|^{2}+(\frac{b}{R})^{2}\leq 3Q\). Therefore, we can write the above by the following:

\[\frac{1}{n}\,\mathbb{E}_{\varepsilon}\bigg{[} \sup_{a^{2}=\|w\|^{2}\leq Q,\;|b|\leq Q}\left|\sum_{i=1}^{n} \varepsilon_{i}a\sigma(w\cdot x_{i}+b)\right|\bigg{]}\leq\frac{1}{n}\mathbb{E}_ {\varepsilon}\left[\sup_{a^{2}+\|\tilde{w}\|^{2}\leq 3Q}\left|\sum_{i=1}^{n} \varepsilon_{i}a\sigma\left(\tilde{w}\cdot\tilde{x_{i}}\right)\right|\right]\] \[=\frac{1}{n}\mathbb{E}_{\varepsilon}\left[\sup_{a^{2}+\|\tilde{w }\|^{2}\leq 3Q,\|u\|_{2}\leq 1}\frac{a^{2}+\|\tilde{w}\|^{2}}{2}\left|\sum_{i=1}^{n} \varepsilon_{i}\sigma(u\cdot\tilde{x_{i}})\right|\right]\] \[\leq\frac{3Q}{2n}\mathbb{E}_{\varepsilon}\left[\sup_{|u|_{2}\leq 1 }\left|\sum_{i=1}^{n}\varepsilon_{i}\sigma(u\cdot\tilde{x_{i}})\right|\right]\] \[\leq\frac{3Q}{n}\mathbb{E}_{\varepsilon}\left[\sup_{|u|_{2}\leq 1 }\sum_{i=1}^{n}\varepsilon_{i}\sigma(u\cdot\tilde{x_{i}})\right]\] \[=3Q\,\widehat{\operatorname{Rad}}(\sigma\circ\mathcal{H}_{2}, \tilde{S}_{n})\leq 3Q\,\widehat{\operatorname{Rad}}(\mathcal{H}_{2},\tilde{S}_{n}) \leq\frac{3Q}{\sqrt{n}}\max_{i}\|\tilde{x}_{i}\|_{2}\leq\frac{3\sqrt{2}\,QR}{ \sqrt{n}}\]

Here, \(\mathcal{H}_{2}:=\{u\in\mathbb{R}^{d}\;|\;\|u\|_{2}\leq 1\}\) and \(\tilde{S_{n}}=\{\tilde{x_{1}},\ldots,\tilde{x_{n}}\}\). When removing the absolute value, we used that the sum is always non-negative and that it is symmetric when replacing \(\varepsilon\) by \(-\varepsilon\). When removing \(\sigma\), we make use of the Contraction Lemma for Rademacher complexity [Shalev-Shwartz and Ben-David, 2014, Lemma 26.9]. Finally, for \(\operatorname{Rad}(\mathcal{H}_{2},\tilde{S}_{n})\) we used the expression for the Rademacher complexity of the class of linear functions on Hilbert space. [Shalev-Shwartz and Ben-David, 2014, Lemma 26.10]. This concludes Step 3.

**Step 4.** In this step, we finally consider sets \(S\) which are sampled from the product measure \(\mu^{n}\), i.e. sets where \(x_{1},\dots,x_{n}\) are independent data samples with law \(\mu\). From steps 1 - 3, we know that

\[\operatorname{Rad}(\mathcal{F}_{Q})=\mathbb{E}_{S_{n}\sim\mu^{n}}\left[ \widehat{\operatorname{Rad}}(\mathcal{F},S_{n})\right]\leq\frac{\big{(}1+3 \sqrt{2}\big{)}Q}{\sqrt{n}}\mathbb{E}_{(x_{1},\dots,x_{n})\sim\mu^{n}}\big{[} \max_{1\leq i\leq n}\|x_{i}\|\big{]}.\]

We bound \(\mathbb{E}_{(x_{1},\dots,x_{n})\sim\mu^{n}}\big{[}\max_{1\leq i\leq n}\|x_{i}\| \big{]}\) by Lemma H.1 to obtain

\[\operatorname{Rad}(\mathcal{F}_{Q})\leq\big{(}1+3\sqrt{2}\big{)}Q\left(\frac{ \mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}}{\sqrt{n}}+\sigma\sqrt{2\frac{\log n }{n}}\right)\qed\]

A similar result follows immediately for the more general function class

\[\mathcal{F}_{A,Q}:=\{f\in\mathcal{B}:[f]_{\mathcal{B}}\leq Q,|f(0)|\leq A\}.\] (5)

**Corollary D.3**.: _Under the same conditions as Lemma D.2, we have_

\[\operatorname{Rad}(\mathcal{F}_{A,Q})\leq\big{(}1+3\sqrt{2}\big{)}Q\left(\frac {\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}}{\sqrt{n}}+\sigma\sqrt{2\,\frac{\log n }{n}}\right)+\frac{A}{\sqrt{n}}\]

Proof.: We note that \(f\in\mathcal{F}_{A,Q}\) if and only if \(f=\tilde{f}+\alpha\) with \(f\in\mathcal{F}_{Q}\) and \(|\alpha|\leq A\). Hence, for any fixed dataset \(S\), we have

\[n\,\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{A,Q}) =\mathbb{E}_{\varepsilon}\left[\sup_{f\in\mathcal{F}_{A,Q}}\sum _{i=1}^{n}\varepsilon_{i}f(x_{i})\right]=\mathbb{E}_{\varepsilon}\left[\sup_{ f\in\mathcal{F}_{Q},|\alpha|\leq A}\sum_{i=1}^{n}\varepsilon_{i}\big{(} \alpha+f(x_{i})\big{)}\right]\] \[\leq\mathbb{E}_{\varepsilon}\left[\sup_{f\in\mathcal{F}_{Q}}\sum _{i=1}^{n}\varepsilon_{i}f(x_{i})\right]+\mathbb{E}_{\varepsilon}\left[\sup_{ |\alpha|\leq A}\sum_{i=1}^{n}\varepsilon_{i}\alpha\right]\leq\widehat{ \operatorname{Rad}}_{n}(\mathcal{F}_{Q})+\frac{A}{\sqrt{n}}\]

by the argument of Step 2 in the proof of Lemma D.2. 

A bound on the Rademacher complexity, together with the sub-Gaussian property of the distribution \(\mu\), allows us to control the 'generalization gap' in homogeneous Barron spaces.

**Corollary D.4**.: _Assume that \(\mu\) is a \(\sigma^{2}\)-sub-Gaussian distribution on \(\mathbb{R}^{d}\). Let \((X_{1},\dots,X_{n})\) be iid random variables with law \(\mu\) and \(f^{*}\) a \(\mu\)-measurable function such that_

\[|f^{*}(x)-f^{*}(0)|\leq B_{1}+B_{2}\|x\|\]

\(\mu\)_-almost everywhere. Let_

\[\widehat{\mathcal{R}}_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\big{|}f(X_{i})-f^{*}(X_ {i})\big{|}^{2},\qquad\mathcal{R}(f)=\mathbb{E}_{x\sim\mu}\big{[}|f(x)-f^{*}( x)|^{2}\big{]}.\]

_Then with probability at least \(1-2\delta\) over the random draw of \(X_{1},\dots,X_{n}\), the bound_

\[\sup_{f-f^{*}(0)\in\mathcal{F}_{A,Q}}\bigl{(}\mathcal{R}(f)-\widehat{\mathcal{ R}}_{n}(f)\bigr{)}\leq C^{*}\left(\big{(}Q+B_{2}\big{)}\left(\mathbb{E}_{x \sim\mu}\|x\|+\sigma^{2}+1\right)+A+B_{1}\right)^{2}\frac{\log(n/\delta)}{ \sqrt{n}}\]

_holds for a constant \(C^{*}>0\) which does not depend on \(\delta,Q,d,\mu\) or \(n\)._

Proof.: **Step 1.** From Lemma H.2, with probability at least \(1-\delta\) we have

\[\max_{1\leq i\leq n}\|X_{i}\|\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+ \sigma\,\sqrt{2\,\log(n/\delta)}.\]

We denote \(R_{n}:=\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\sigma\,\sqrt{2\,\log(n/\delta)}\) for simplicity.

**Step 2.** Consider the modified loss function

\[\ell_{\xi}(f)=\min\left\{f^{2},\xi^{2}\right\},\]

[MISSING_PAGE_FAIL:34]

### Convergence in \(L^{p}(\mu)\)

We start by establishing convergence in \(L^{2}(\mu)\) at an explicit convergence rate. Then, using this \(L^{2}(\mu)\) convergence we will extend this result to general \(L^{p}(\mu)\).

We introduce one of our main theorem of this section, which gives us an explicit bound of \(L^{2}(\mu)\)-loss. For convenience, we denote \(\theta:=(a,W,b)\) for the rest of the section.

**Theorem E.1** (\(L^{2}\)-convergence).: _Let \(\hat{\theta}\in\operatorname*{argmin}_{\theta}\widehat{\mathcal{R}}_{n,m, \lambda}(\theta)\). If \(\delta\geq e^{-n}\), and \(f^{*}\in\mathcal{F}_{Q^{*}}\), then with probability at least \(1-4\delta\) over the choice of random points \(x_{1},\ldots,x_{n}\) we have_

\[\mathcal{R}(f_{\hat{\theta}})\leq C\left(\frac{(Q^{*})^{2}}{m}\left(\mathbb{E }\big{[}\|x\|^{2}\big{]}\right)+\lambda Q^{*}+Q^{*}\,\left(\mathbb{E}\|x\|+ \sigma^{2}+[f^{*}]_{\mathcal{B}}\right)\,\frac{\log(n/\delta)}{\sqrt{n}}\right)\]

_up to higher order terms in the small quantities \((\lambda m)^{-1},m^{-1},n^{-1/2}\log n\)._

Proof.: **Outline.** We use Theorem C.5 for \(L^{2}(\mu_{n})\) with \(f=f^{*}\) to obtain a function for which \(\widehat{\mathcal{R}}_{n,m,\lambda}\) is low. The empirical risk minimizer (ERM) has even lower risk. The weight decay penalty additionally provides a norm-bound in homogeneous Barron space for the ERM, and we use Corollary D.4 to control the generalization gap.

**Step 1.** Due to Theorem C.5, there exists a \(\tilde{\theta}:=(\tilde{a},\tilde{w},\tilde{b})\in\mathbb{R}^{m}\times \mathbb{R}^{m\times d}\times\mathbb{R}^{m+1}\) such that

\[R_{WD}(\tilde{\theta})\leq[f^{*}]_{\mathcal{B}}\] (6)

and

\[\widehat{\mathcal{R}}_{n}(f_{\tilde{\theta}})=[f_{\tilde{\theta}}-f^{*}]_{L^{ 2}(\mu_{n})}^{2}\leq\frac{4[f^{*}]^{2}}{m}\sup_{\|w\|=1}\int_{\mathbb{R}^{d}}|w ^{T}x|^{2}d\mu_{n}\leq\frac{4[f^{*}]^{2}}{m}\left(\frac{1}{n}\sum_{i=1}^{n}\| x_{i}\|^{2}\right).\]

We will always consider \(\delta\) such that \(\log(1/\delta)\leq n\). In this regime, plugging-in the bound on the second moments of \(\mu_{n}\) from Lemma H.4 gives the corresponding bound

\[\widehat{\mathcal{R}}_{n}(f_{\hat{\theta}})\leq\frac{4[f^{*}]^{2}}{m}\left( \mathbb{E}[\|x\|^{2}]+8\sigma^{2}\sqrt{\frac{\log(1/\delta)}{n}}\right)\] (7)

with probability \(1-\delta\). We will assume that this estimate is valid for the remainder of the proof. In particular, since \(\hat{\theta}\) minimizes \(\widehat{\mathcal{R}}_{n,m,\lambda}\), we find that

\[\widehat{\mathcal{R}}_{n,m,\lambda}(\hat{\theta})\leq\widehat{\mathcal{R}}_{n,m,\lambda}(\tilde{\theta})\leq\frac{4[f^{*}]^{2}}{m}\left(\mathbb{E}[\|x\|^{ 2}]+8\sigma^{2}\sqrt{\frac{\log(1/\delta)}{n}}\right)+\lambda\,[f^{*}]_{ \mathcal{B}}.\] (8)

**Step 2.** Next, we bound \([f_{\hat{\theta}}]_{\mathcal{B}}\) and \(|f_{\hat{\theta}}(0)-f^{*}(0)|\) (\(Q\) and \(A\) in Corollary D.4). We first bound the Barron semi-norm by

\[[f_{\hat{\theta}}]_{\mathcal{B}}\leq\frac{1}{\lambda}\,\widehat{\mathcal{R}}_{ n,m,\lambda}(\hat{\theta})\leq\frac{1}{\lambda}\widehat{\mathcal{R}}_{n,m, \lambda}(\tilde{\theta}).\]

Moving on to bounding \(A\), we find from the empirical risk bound

\[\min_{1\leq i\leq n}|f-f^{*}|(x_{i})=\sqrt{\min_{1\leq i\leq n}|f-f^{*}|^{2}(x_ {i})}\leq\sqrt{\frac{1}{n}\sum_{i=1}^{n}|f-f^{*}|(x_{i})}\leq\sqrt{\widehat{ \mathcal{R}}_{n}(f)}\]

and in particular

\[\min_{1\leq i\leq n}|f_{\hat{\theta}}-f^{*}|(x_{i})\leq\sqrt{\widehat{ \mathcal{R}}_{n,m,\lambda}(\tilde{\theta})}.\]

With probability at least \(1-\delta\), we have

\[\max_{1\leq i\leq n}\|x_{i}\|\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+ \sigma\,\sqrt{2\,\log(n/\delta)}.\]

by Lemma H.2. Again, we assume that the estimate holds in the following. Hence, the index \(i\) for which the minimum is attained in (8) satisfies the bound

\[\|x_{i}\|\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\sigma\,\sqrt{2\,\log(n /\delta)}.\]Combining the bounds on \(|f_{\hat{\theta}}(x_{i})-f^{*}(x_{i})|\) and the Lipschitz constants of \(f_{\hat{\theta}},f^{*}\), we find that

\[|f_{\hat{\theta}}-f^{*}|(0) \leq|f_{\hat{\theta}}-f^{*}|(x_{i})+\left([f_{\hat{\theta}}]_{ \mathcal{B}}+[f^{*}]_{\mathcal{B}}\right)\|x_{i}\|\] \[\leq\sqrt{\widehat{\mathcal{R}}_{n,m,\lambda}(\tilde{\theta})}+ \left([f^{*}]_{\mathcal{B}}+\frac{1}{\lambda}\,\widehat{\mathcal{R}}_{n,m, \lambda}(\tilde{\theta})\right)\left(\mathbb{E}_{x\sim\mu}\big{[}\|x\big{]}+ \sigma\,\sqrt{2\,\log(n/\delta)}\right).\]

**Step 3.** Comparing \(\hat{\theta}\) to \(\tilde{\theta}\), we observe that

\[\mathcal{R}(f_{\hat{\theta}}) =\widehat{\mathcal{R}}_{n}(f_{\hat{\theta}})+\mathcal{R}(f_{\hat {\theta}})-\widehat{\mathcal{R}}_{n}(f_{\hat{\theta}})\] \[\leq\widehat{\mathcal{R}}_{n,m,\lambda}(\hat{\theta})+\mathcal{R} (f_{\hat{\theta}})-\widehat{\mathcal{R}}(f_{\hat{\theta}})\] \[\leq\widehat{\mathcal{R}}_{n,m,\lambda}(\tilde{\theta})+\mathcal{ R}(f_{\hat{\theta}})-\widehat{\mathcal{R}}_{n}(f_{\hat{\theta}})\]

where we used the fact that \(\hat{\theta}\) is a minimizer of \(\widehat{\mathcal{R}}_{n,m,\lambda}(\theta)\) and (6). In the following, we use the bound on \([f_{\hat{\theta}}]_{\mathcal{B}}\) to control the generalization gap.

**Step 4.** Recall that \([f_{\hat{\theta}}]_{\mathcal{B}}\leq[f^{*}]_{\mathcal{B}}+\frac{1}{\lambda}\, \widehat{\mathcal{R}}_{n,m,\lambda}(f_{\hat{\theta}})=[f^{*}]_{\mathcal{B}}+O ((\lambda m)^{-1})\). Thus, with probability at least \(1-2\delta\), we obtain the bound

\[\big{(}\mathcal{R}-\widehat{\mathcal{R}}_{n}\big{)}(f_{\hat{\theta}})\leq \left([f^{*}]_{\mathcal{B}}+\frac{1}{\lambda}\,\widehat{\mathcal{R}}_{n,m, \lambda}(\hat{\theta})\right)\left(\mathbb{E}\|x\|+\sigma^{2}+[f^{*}]_{ \mathcal{B}}\right)\,\frac{\log(n/\delta)}{\sqrt{n}}\]

from Corollary D.4 for a slightly modified constant \(C>0\) (with \(B_{1}=0\)) and up to higher order terms in \((\lambda m)^{-1}\) and \(n\).

**Step 5.** By the union bound, all probabilistic bounds hold simultaneously with probability at least \(1-4\delta\). In this case

\[\mathcal{R}(f_{\hat{\theta}})\leq C\left(\frac{Q^{2}}{m}\left(\mathbb{E}\big{[} \|x\|^{2}\big{]}+\sigma^{2}\,\sqrt{\frac{\log(1/\delta)}{n}}\right)+\lambda Q +[f^{*}]_{\mathcal{B}}\left(\mathbb{E}\|x\|+\sigma^{2}+[f^{*}]_{\mathcal{B}} \right)\,\frac{\log(n/\delta)}{\sqrt{n}}\right)\]

up to higher order terms in \(m^{-1},\log n/\sqrt{n},(\lambda m)^{-1}\) etc. 

Since \(\mathcal{R}(\theta)=\|f_{\theta}-f^{*}\|_{L^{2}(\mu)}^{2}\), we can interpret Theorem E.1 as a convergence statement in \(L^{2}(\mu)\) at a suitable rate. The statement generalizes to \(L^{p}\)-convergence at a rate.

**Corollary E.2** (\(L^{p}\)-convergence).: _Let \(p\in[1,\infty]\) and \(\hat{\theta}\) as in Theorem E.1. Then there exists a constant \(\tilde{C}>0\) depending on \(\mathbb{E}\|x\|,\mathbb{E}[\|x\|^{2}],\sigma^{2}\) and \(p\) such that_

\[\|f_{\hat{\theta}}-f^{*}\|_{L^{p}(\mu)}\leq\tilde{C}\,\left(\widehat{\mathcal{ R}}_{n,m,\lambda}(\hat{\theta})^{1/2}+[f^{*}]_{\mathcal{B}}\right)^{1-1/p}\|f_{ \hat{\theta}}-f^{*}\|_{L^{2}(\mu)}^{1/p}.\]

Proof.: Since \(\mu\) is sub-Gaussian, we note that all moments of \(\mu\) are finite: \(\mathbb{E}[(1+\|x\|)^{q}]<\infty\) for all \(q\in[1,\infty)\). In particular, if \(g\) is a measurable function which satisfies \(|g(x)|\leq C_{g}(1+\|x\|)\) for some \(C)g>0\), then

\[\|g\|_{L^{p}(\mu)}^{p}=\mathbb{E}\big{[}g\cdot g^{p-1}\big{]}\leq\mathbb{E} \big{[}g^{2}\big{]}^{1/2}\,\mathbb{E}\big{[}g^{2(p-1)}\big{]}^{1/2}=\|g\|_{L^{ 2}}\|g\|_{L^{2(p-1)}}^{p-1}.\]

If \(g=f_{\hat{\theta}}-f^{*}\in\mathcal{B}\), then by the continuous embedding \(\mathcal{B}\hookrightarrow L^{q}(\mu)\) we find that

\[\|f_{\hat{\theta}}-f^{*}\|_{L^{2(p-1)}(\mu)}\leq C\,\big{(}|f_{\hat{\theta}}-f^ {*}|(0)+[f_{\hat{\theta}}-f^{*}]_{\mathcal{B}}\big{)}.\]

Recall that \(|f_{\hat{\theta}}-f^{*}|(0)\leq\widehat{\mathcal{R}}_{n,m,\lambda}(\hat{\theta })^{1/2}+C[f^{*}]_{\mathcal{B}}\). 

We note that Corollary E.2 is generally suboptimal. Indeed, for \(p\leq 2\), the stronger bound

\[\|f_{\hat{\theta}}-f^{*}\|_{L^{p}(\mu)}\leq\|f_{\hat{\theta}}-f^{*}\|_{L^{2}(\mu) }=O\left(\left(\frac{1}{m}+\lambda+\frac{\log n}{\sqrt{n}}\right)^{1/2}\right)\]

holds as \(L^{2}(\mu)\) embeds continuously into \(L^{p}(\mu)\).

### Gamma-expansion of regularized risk functionals

As before, we denote \(\theta_{n}=(a,W,b)_{n}\in\mathbb{R}^{m_{n}}\times\mathbb{R}^{m_{n}\times d}\times \mathbb{R}^{m_{n}+1}\). Since \(\mathcal{R}(f_{\theta})=\|f_{\theta}-f^{*}\|_{L^{2}(\mu)}^{2}\), Theorem E.1 can be taken as a statement that \(f_{\hat{\theta}_{n}}\to f^{*}\) as \(n\to\infty\) in \(L^{2}(\mu)\). However, this does not tell us about the behavior of \(f_{\hat{\theta}_{n}}\) in a \(\mu\)-null set, i.e. where the distribution \(\mu\) provides us no information. This interpolation between known values can be deduced from our next result. We first present a simplified version, in which we assume that we have already taken the limits \(m,n\to\infty\) before taking \(\lambda\to 0\). We couple the limits \(n,m_{n},\lambda_{n}\) below.

We use the notion of \(\Gamma\)-convergence from the calculus of variations. For a brief introduction, see Appendix B. \(\Gamma\)-convergence depends on the underlying topology of the space, and we make the following convention: We say that \(f_{\lambda}\xrightarrow[]{good}f\) if \(f_{\lambda}\to f\) locally uniformly (uniformly on compact sets) and in \(L^{2}(\mu)\). Other definitions are admissible and lead to the same general theory. Since Barron functions grow at most linearly at \(\infty\) due to Lipschitz-continuity, we note that this is notion of convergence is generated by a metric

\[d(f,g)=\max_{x\in\mathbb{R}^{d}}\frac{|f(x)-g(x)|}{1+\|x\|^{2}}\]

at least on bounded subsets of Barron space. This suffices for all applications below and spares us from considering \(\Gamma\)-convergence on more general topological spaces - which is also possible.

**Theorem E.3**.: _Let_

\[\mathcal{R}_{\lambda}:\mathcal{B}\to[0,\infty),\quad\mathcal{R}_{\lambda}(f)= \|f-f^{*}\|_{L^{2}(\mu)}^{2}+\lambda\,[f]_{\mathcal{B}}.\]

_We denote_

\[F_{\lambda}:\mathcal{B}\to[0,\infty) F_{\lambda}(f)=\frac{\mathcal{R}_{\lambda}(f)}{\lambda}\ =\frac{\|f-f_{d}^{*}\|_{L^{2}(\mu)}^{2}}{\lambda}+[f]_{\mathcal{B}}\] \[F:\mathcal{B}\to[0,\infty] F(f)=\begin{cases}[f]_{\mathcal{B}}&\text{if $f=f^{*}\mu$-a.e.}\\ +\infty&\text{else}\end{cases}.\]

_Then \(\Gamma-\lim_{\lambda\to 0}F_{\lambda}=F\) with respect to the notion of convergence \(\xrightarrow[]{good}\) defined above._

Notably, the \(\Gamma\)-limit of \(\mathcal{R}_{\lambda}\) itself would be zero at all points of interest. Rescaling to consider \(F_{\lambda}\) instead has fits into the framework of \(\Gamma\)-expansions considered by Braides and Truskinovsky [2008]. Denote

\[\mathcal{F}=\{f\in\mathcal{B}:f\equiv f^{*}\ \mu\text{-a.e.}\}\] (9)

Proof.: **Step 1.liminf-inequality.** First consider \(f\in\mathcal{F}\) and assume that \(\{f_{\lambda}\}_{\lambda>0}\) is a family of functions such that \(f_{\lambda}\xrightarrow[]{good}f\).2 Then by the compactness theorem for Barron functions in coarser topologies (Theorem C.3) we have the following:

Footnote 2: It is easy to generalize this to continuous limits, but if preferred, then \(\lambda=\lambda_{n}\) can be taken to be a discrete sequence converging to zero.

\[\liminf_{\lambda\to 0^{+}}F_{\lambda}(f_{\lambda})\geq\liminf_{\lambda\to 0^{+}}[f_{ \lambda}]_{\mathcal{B}}\geq[f]_{\mathcal{B}}=F(f).\]

Now assume that \(f\notin\mathcal{F}\) and that \(f_{\lambda}\xrightarrow[]{good}f\). We need to show that \(F_{\lambda}(f_{\lambda})\to+\infty\). Since \(f\notin\mathcal{F}\), we see that \(\mathcal{R}(f)=\|f-f_{d}^{*}\|_{L^{2}(\mu)}^{2}>0\). Denote \(\varepsilon=\sqrt{\mathcal{R}(f)}\) and observe that there exists \(\Lambda>0\) such that \(\|f_{\lambda}-f\|_{L^{2}(\mu)}<\varepsilon/2\) for all \(\lambda<\Lambda\) by the definition of the notion of convergence.

In particular, we find that

\[\|f_{\lambda}-f^{*}\|_{L^{2}(\mu)}\geq\|f-f^{*}\|_{L^{2}(\mu)}-\|f_{\lambda}-f\| _{L^{2}(\mu)}\geq\varepsilon/2\]

for all \(\lambda<\Lambda\) by the inverse triangle inequality and thus

\[\liminf_{\lambda\to 0^{+}}F_{\lambda}(f_{\lambda})\geq\liminf_{\lambda\to 0^{+}} \frac{(\varepsilon/2)^{2}}{\lambda}=+\infty.\]

**Step 2.limsup-inequality.** Again, we first consider the case \(f\in\mathcal{F}\). Set \(f_{\lambda}=f\) for all \(\lambda>0\) and observe that \(f_{\lambda}\to f\) as \(\lambda\to 0^{+}\) (trivially). By the same argument \(F_{\lambda}(f_{\lambda})=F(f)=[f]_{\mathcal{B}}\) for all \(\lambda\), i.e. the constant sequence is a recovery sequence since \(F_{\lambda}(f_{\lambda})\to F(\hat{f})\).

On the other hand, if \(f\notin\mathcal{F}\), then \(\mathcal{R}(f)>0\) and thus \(F_{\lambda}(f)\to+\infty=F(f)\). Again, we can use the constant sequence as a recovery sequence, somewhat trivially. 

**Corollary E.4**.: _Assume that \(f_{\lambda}\in\operatorname{argmin}_{f\in\mathcal{B}}\mathcal{R}_{\lambda}\), i.e. \(f_{\lambda}\) minimizes \(\mathcal{R}_{\lambda}\). Then there exists \(\hat{f}\in\mathcal{B}\) such that \(f_{\lambda}\xrightarrow{good}\hat{f}\)._

Proof.: Clearly \(f_{\lambda}\) minimizes \(\mathcal{R}_{\lambda}\) if and only if it minimizes \(F_{\lambda}=\lambda^{-1}\,\mathcal{R}_{\lambda}\). We note that

\[[f_{\lambda}]_{\mathcal{B}}\leq\lambda^{-1}\mathcal{R}_{\lambda}(f_{\lambda}) \leq\lambda^{-1}\mathcal{R}(f^{*})+[f^{*}]_{\mathcal{B}}=[f^{*}]_{\mathcal{B}}.\]

In particular, by the compact embedding of Theorem C.3, there exists \(\hat{f}\in\mathcal{B}\) such that \(f_{\lambda}\xrightarrow{good}\hat{f}\) up to subsequence. By the properties of \(\Gamma\)-convergence, we conclude that \(\hat{f}\) is a minimizer of \(F\). 

We present a special case of Corollary E.4 in the setting of Proposition 3.2 which exploits the _uniqueness_ of the minimizer. Recall the definition of the radial average in (3) and \(f_{d}^{*}\) from Proposition 3.2.

**Corollary E.5**.: _Assume that \(f^{*}(0)=1\), \(f^{*}(x)=0\) if \(\|x\|\geq 1\) and \(\mu\) satisfies the conditons of Corollary 3.3. Assume additionally that \(f_{\lambda}\in\operatorname{argmin}_{f\in\mathcal{B}}\mathcal{R}_{\lambda}\), i.e. \(f_{\lambda}\) minimizes \(\mathcal{R}_{\lambda}\). Then \(\operatorname{Av}f_{\lambda}\xrightarrow{good}f_{d}^{*}\)._

Proof.: **Step 1.** Clearly \(f_{\lambda}\) minimizes \(\mathcal{R}_{\lambda}\) if and only if it minimizes \(F_{\lambda}=\lambda^{-1}\,\mathcal{R}_{\lambda}\). \(\operatorname{Av}f_{\lambda}\) is also a minimizer of \(F_{\lambda}\) since the functional is convex and rotationally symmetric, so by averaging in radial direction, we are taking a (continuous) convex combination of minimizers, which is a minimizer again.

**Step 2.** We find that

\[[\operatorname{Av}f_{\lambda}]_{\mathcal{B}}\leq F_{\lambda}(\operatorname{ Av}f_{\lambda})\leq F_{\lambda}(f_{\lambda})\leq F_{\lambda}(f_{d}^{*})=[f_{d}^{*}]_{ \mathcal{B}}.\]

By the compactness theorem for Barron functions, Theorem C.3, there exists \(f\in\mathcal{B}\) such that \(\operatorname{Av}f_{\lambda}\xrightarrow{good}f\) (up to a subsequence). Since \(F_{\lambda}\to F\) in the sense of \(\Gamma\)-convergence, we find that \(f\) is a minimizer of \(F\). Since \(f\) is also radially symmetric, we find by Proposition 3.2 that \(f\equiv f_{d}^{*}\).

**Step 3.** By the exact same logic, we could show that every subsequence of \(\{\operatorname{Av}f_{\lambda}\}\) has a further subsequence which converges to \(f_{d}^{*}\). By a standard argument in topology, the whole sequence converges. 

A similar statement can be proved in the more complicated case where \(f_{\lambda}\) is a neural network with finitely many neurons and \(F_{\lambda}\) uses a finite data set rather than a continuous expectation. In this case, the parameter \(\lambda\) must be coupled to the number of parameters \(m\) and the number of data points \(n\), such that \(\lambda\to 0\), but not too quickly. The proof is a more technically challenging variant of those of Theorem E.3 and Corollaries E.4 and E.5, which utilizes the generalization bound of D.4. To this end, we first introduce a new notion of convergence. That is, we define a notion of convergence from the parameter to function. We define a notion of convergence by saying that \(\theta_{k}:=(a_{k},W_{k},b_{k})\xrightarrow{good}f\) iff \(f_{\theta_{k}}\xrightarrow{good}f\) as \(k\to\infty\).

**Theorem E.6**.: _Consider the parameter space \(\Theta_{m}\subseteq\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\times\mathbb{R}^ {m+1}\) of neural networks with a single hidden layer of width \(m\) and the associated functions_

\[f_{\theta}(x):=b_{0}+\sum_{i=1}^{m}a_{i}\sigma(w_{i}\cdot x+b_{i})\]

_Let \(m_{n},\lambda_{n}\) scale with \(n\) according to (1). We denote_

\[F_{n}:\Theta_{m_{n}}\to[0,\infty)\qquad F_{n}(\theta)=\frac{\widehat{\mathcal{ R}}_{n,m_{n},\lambda_{n}}(\theta)}{\lambda_{n}}=\frac{\widehat{\mathcal{R}}_{n}(f_{ \theta})}{\lambda_{n}}+R_{WD}(\theta)\]\[F:\mathcal{B}\to[0,\infty]\qquad F(f)=\begin{cases}[f]_{\mathcal{B}}&\text{if $f=f^{*} \mu$-a.e. }\\ +\infty&\text{else}\end{cases}.\]

_Then almost surely over the choice of data points, we have \(\Gamma-\lim_{n\to\infty}F_{n}=F\) almost surely with respect to the notion of convergence \(\theta_{k}\xrightarrow{\text{good}}f\) defined above._

Proof.: We use \(\mathcal{F}\) as in (9) throughout. In the proof we assume that all stochastic quantities in Corollary D.4 and Theorem E.1 are satisfied with probability at least \(1-\delta_{n}\) for \(\delta_{n}=n^{-2}\). The quantity \(\log(\delta_{n})\) therefore becomes comparable to \(\log n/n\ll\lambda_{n}\). Since \(\sum_{n=1}^{\infty}n^{-2}<\infty\), we find that all conditions are met for all but finitely many \(n\in\mathbb{N}\) by the Borel-Cantelli Lemma. For questions of asymptotic convergence, we may therefore assume that the statements of both Theorems apply without qualifying for high probability. Note that \(n^{-2}\geq e^{-n}\) for all \(n\geq 2\) as needed for Theorem E.1.

**Step 1.liminf-inequality.** Again, we consider the cases \(f\in\mathcal{F}\) and \(f\notin\mathcal{F}\) separately. First, when \(f\in\mathcal{F}\), we apply the same method we did in Theorem E.3. For any sequence of parameters \(\theta_{n}\xrightarrow{\text{good}}f\), by Theorem C.3 the following holds:

\[\liminf_{n\to\infty}F_{n}(\theta_{n})\geq\liminf_{n\to\infty}R_{WD}(\theta_{n} )\geq\liminf_{n\to\infty}[f_{\theta_{n}}]_{\mathcal{B}}\geq[f]_{\mathcal{B}}=F (f).\]

The second inequality comes from \([f_{\theta_{n}}]_{\mathcal{B}}\) being an infimum of weight decay regularizers with any arbitrary probability measure on parameter space.

Second, when \(f\notin\mathcal{F}\), we need to show \(\liminf_{n\to\infty}F_{n}(\theta_{n})=\infty\) for any \(\theta_{n}\xrightarrow{\text{good}}f\). We distinguish two prototypical cases:

1. \([f_{\theta_{n}}]_{\mathcal{B}}\to+\infty\) as \(n\to\infty\). In this case \(F(\theta_{n})\geq[f_{\theta_{n}}]_{\mathcal{B}}\to+\infty\) as well by the same logic as above.
2. \(\limsup_{n\to\infty}[f_{\theta_{n}}]_{\mathcal{B}}<+\infty\). In this case, we take \(\varepsilon:=\|f-f^{*}\|_{L^{2}(\mu)}/2>0\). Then there exists \(N\in\mathbb{N}\) such that for all \(n\geq N\) we have \[\|f_{\theta_{n}}-f^{*}\|_{L^{2}(\mu)}\geq\|f-f^{*}\|_{L^{2}(\mu)}-\|f_{\theta_ {n}}-f\|_{L^{2}(\mu)}\geq\varepsilon\] for all \(n\geq N\) by definition. Additionally \[F_{n}(\theta_{n}) \geq\frac{\widehat{\mathcal{R}}_{n}(f_{\theta_{n}})-\mathcal{R}( f_{\theta_{n}})}{\lambda_{n}}+\frac{\mathcal{R}(f_{\theta_{n}})}{\lambda_{n}}+[f_{ \theta_{n}}]_{\mathcal{B}}\] \[\geq\frac{\widehat{\mathcal{R}}_{n}(f_{\theta_{n}})-\mathcal{R}( f_{\theta_{n}})}{\lambda_{n}}+\frac{\varepsilon^{2}}{\lambda_{n}}+[f_{ \theta_{n}}]_{\mathcal{B}}.\] Due to Corollary D.4 and the arguments of Theorem E.1 to control the discrepancy at \(0\), we have \[\widehat{\mathcal{R}}_{n}(f_{\theta_{n}})-\mathcal{R}(f_{\theta_{n}})=O\left( \frac{\log n}{\sqrt{n}}\right).\] in this case. Since \(\log n/\sqrt{n}\ll\lambda_{n}\) by assumption, we note that \[\lim_{n\to\infty}\frac{\hat{\mathcal{R}}_{n}(f_{\theta_{n}})-\mathcal{R}(f_{ \theta_{n}})}{\lambda_{n}}=0\] and thus \[\lim_{n\to\infty}F_{n}(\theta_{n})\geq\liminf_{n\to\infty}\left(0+\frac{ \varepsilon^{2}}{\lambda_{n}}+0\right)=+\infty.\]

The same holds in the general case by passing to subsequences.

**Step 2.limsup-inequality.** As in Theorem C.5, the case \(f\notin\mathcal{F}\) follows from the \(\liminf\)-inequality in an essentially trivial fashion. We therefore only consider the case \(f\in\mathcal{F}\). An approximating sequence in this case is constructed from Theorem C.5 as in Theorem E.1 or Theorem E.3. Namely, we find \(\tilde{\theta}_{n}\) such that

\[F_{n}(\tilde{\theta}_{n})\leq\frac{C}{\lambda_{n}m_{n}}\left(1+\frac{\log n}{ \sqrt{n}}\right)+[f^{*}]_{\mathcal{B}}\qquad\Rightarrow\quad\limsup_{n\to \infty}F_{n}(\tilde{\theta}_{n})\leq[f^{*}]_{\mathcal{B}}.\qed\]

**Remark E.7**.: _The key ingredients for the proofs of both Theorem E.1 and Theorem E.6 are Theorem C.5 and Corollary D.4, but they are combined differently. While they are paired in Theorem E.1 to obtain a precise rate, they occur separately in Theorem E.6: Theorem C.5 is used for the \(\limsup\)-inequality while Corollary D.4 enters in the proof of the \(\liminf\)-inequality. Analogously, the condition \(\lambda_{n}\ll\log n/\sqrt{n}\) is used in the proof of the \(\liminf\)-condition while the fact that \(\frac{1}{m_{n}}\ll\lambda_{n}\) is used in the proof of the \(\limsup\)-inequality._

### Proofs of the main theorems

The statements of the Theorems in the main body of the text can easily be deduced from the statements proved in this Appendix.

Proof of Theorem 2.1.: Convergence in \(L^{p}\) holds by Theorem E.1 for \(1\leq p\leq 2\) and Corollary E.2 (general \(p\)). The proof of uniform convergence follows from Theorem E.6 in the same fashion that Corollary E.4 follows from Theorem E.3. The explicit bound is obtained from Theorem E.1 with \(\delta_{n}=\frac{1}{4n^{2}}\). 

Proof of Corollary 3.3.: This follows in the same way as the proof of Theorem 2.1 with modifications as in Corollary E.5. 

## Appendix F Theorem 2.1 for finite data sets

Finally, we note that a version of Theorem 2.1 holds if the data set \(S=\{x_{1},\ldots,x_{n}\}\) is kept fixed. The proof is a combination of those of Theorems E.3 and E.6, as we deal with a finite approximating neural network, but do not require generalization bounds. The details are left to the reader.

**Theorem F.1**.: _We make the following assumptions._

1. _Let_ \(S=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\) _be a fixed dataset of_ \(n\) _data points in_ \(x_{i}\in\mathbb{R}^{d}\) _and labels_ \(y_{i}\in\mathbb{R}\)_._
2. _Let the loss function_ \(\ell(f,y)\) _be the mean squared error_ \(\ell_{MSE}(f,y)=|f-y|^{2}\)_._
3. _Assume that_ \(\lambda_{m}\) _is a sequence of parameters such that_ \(\lambda_{m}\to 0\)_,_ \(1/m\ll\lambda_{m}\) _as_ \(m\to\infty\)_._

_Consider the regularized empirical risk functional \(\widehat{\mathcal{R}}_{m}:\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\times \mathbb{R}^{m}\to[0,\infty)\),_

\[\widehat{\mathcal{R}}_{m}(a,W,b)=\frac{1}{2n}\sum_{i=1}^{n}\ell\left(f_{(a,W,b )}(x_{i}),\,y_{i}\right)+\frac{\lambda_{m}}{2}\left(\|a\|_{2}^{2}+\|W\|_{Frob }^{2}\right).\]

_Then if \((a,W,b)_{m}\in\operatorname*{argmin}\widehat{\mathcal{R}}_{m}\) for all \(m\in\mathbb{N}\), then every subsequence of \(f_{m}:=f_{(a,W,b)_{m}}\) has a further subsequence which converges to some limit \(\hat{f}^{*}\in\mathcal{B}\) uniformly on compact subset of \(\mathbb{R}^{d}\). The limiting function satisfies_

\[\hat{f}^{*}\in\operatorname*{argmin}_{\{f\in\mathcal{B}:f(x_{i})=y_{i}\ \forall i\}}\ [f]_{ \mathcal{B}}.\]

## Appendix G Minimum norm interpolation in one dimension

Proof of Proposition 3.1.: Any Barron function \(f\) is also Lipschitz-continuous, in particular differentiable almost everywhere and \(f(b)-f(a)=\int_{a}^{b}f^{\prime}(x)\,\mathrm{d}x\) for all \(a,b\in\mathbb{R}\) by Rademacher's Theorem (see e.g. [Evans and Gariepy, 2015, Section 3.1]). In particular, there exist points \(x^{+},x^{-}\in(a,b)\) such that

\[f^{\prime}(x^{-})\leq\frac{f(b)-f(a)}{b-a}=\frac{1}{b-a}\int_{a}^{b}f^{\prime} (x)\,\mathrm{d}x\leq f^{\prime}(x^{+}).\]

If \(f^{\prime}\) is not constant, both inequalities are satisfied strictly. Under the assumptions, there exists \(a\in(x_{0},x_{1})\) and \(b\in(x_{n-1},x_{n})\) such that

\[f^{\prime}(a)\leq\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\leq 0\leq\frac{f(x_{n})-f(x _{n-1})}{x_{n}-x_{n-1}}\leq f^{\prime}(b).\] (10)Since the derivative \(f^{\prime}\) changes sign, we conclude by [202, Proposition 2.5] that \([f]_{\mathcal{B}}=\int_{-\infty}^{\infty}\,\mathrm{d}|\mu|\) where the Radon measure \(\mu\) is the distributional derivative of \(f^{\prime}\) and \(|\mu|\) denotes the total variation measure of \(\mu\). Since \(f^{\prime}\) is differentiable at \(a,b\), neither point is an atom of \(\mu\) and thus

\[\frac{f(x_{n})-f(x_{n-1})}{x_{n}-x_{n-1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0} }\leq f^{\prime}(b)-f^{\prime}(a)\leq\int_{a}^{b}\,\mathrm{d}\mu\leq\int_{a}^{ b}\,\mathrm{d}|\mu|\leq[f]_{\mathcal{B}}.\]

Equality holds if and only if \(|\mu|=\mu\), i.e. if \(\mu\geq 0\) in the sense of signed measures and if and only if the inequality in (10) can only be satisfied with equality. The first condition means that \(f\) must be convex, the second implies that the derivative of \(f\) must be constant in the intervals \((x_{0},x_{1})\) and \((x_{n-1},x_{n})\). The same can easily be seen for the larger intervals \((-\infty,x_{1})\) and \((x_{n-1},\infty)\). 

## Appendix H Sub-Gaussian random variables

In this Appendix we quickly gather some facts about sub-Gaussian random variables. For the reader's convenience, we provide full proofs. Experts are encouraged to skip ahead to Appendix D.

The first property we observe is a bound of expected maximum value of sub-Gaussian.

**Lemma H.1**.: _Let \(\mu\) be \(\sigma^{2}\)-sub-Gaussian, and for \(i=1,\ldots,n\), let \(x_{i}\) be a iid random samples from a law \(\mu\). Then, the following holds:_

\[\mathbb{E}\big{[}\max_{1\leq i\leq n}\|x_{i}\|\big{]}\leq\mathbb{E}_{x\sim\mu} \big{[}\|x\|\big{]}+\sqrt{2\log n}\,\sigma.\]

Proof.: From the sub-Gaussian assumption we have

\[\log\big{(}\mathbb{E}_{x\sim\mu}\big{[}\exp\big{(}\lambda\big{(}\|X\|-\mathbb{ E}\big{[}\|X\|\big{)}\,\big{]}\big{)}\big{)}\leq\frac{\sigma^{2}\lambda^{2}}{2}\]

for some fixed \(\sigma>0\) and all \(\lambda>0\). In particular, Jensen's inequality implies the sub-Gaussian maximal inequality

\[\mathbb{E}\big{[}\max_{1\leq i\leq n}\|x_{i}\|\big{]} =\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\mathbb{E}\big{[}\max_{ 1\leq i\leq n}\big{(}\|x_{i}\|-\mathbb{E}\|x_{i}\|\big{)}\big{]}\] \[\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\frac{1}{\lambda}\, \log\bigg{(}\mathbb{E}\left[\exp\bigg{(}\lambda\,\max_{1\leq i\leq n}\big{(} \|x_{i}\|-\mathbb{E}\|x_{i}\|\big{)}\bigg{)}\right]\] \[\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\frac{1}{\lambda}\, \log\left(\sum_{i=1}^{n}\mathbb{E}\left[\exp\big{(}\lambda\big{(}\|x_{i}\|- \mathbb{E}\|x_{i}\|\big{)}\big{)}\right]\right)\] \[=\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\frac{1}{\lambda}\, \log\bigg{(}n\,\exp\bigg{(}\frac{\lambda^{2}\sigma^{2}}{2}\bigg{)}\bigg{)}\] \[\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+\frac{\log n}{\lambda }+\frac{\sigma^{2}}{2}\,\lambda\]

for all \(\lambda>0\). We specifically select \(\lambda=\sqrt{\frac{2\log n}{\sigma^{2}}}\), making the bound

\[\mathbb{E}\big{[}\max_{1\leq i\leq n}\|x_{i}\|\big{]}\leq\mathbb{E}_{x\sim\mu} \big{[}\|x\|\big{]}+\sqrt{2\log n}\,\sigma.\qed\]

Next, we observe the concentration of maximum values among samples of sub-Gaussian distribution.

**Lemma H.2**.: _Let \(\mu\) be \(\sigma^{2}\)-sub-Gaussian, and for \(i=1,\ldots,n\), let \(x_{i}\) be a iid random samples from a law \(\mu\). Then, with probability at least \(1-\delta\), the following holds:_

\[\max_{1\leq i\leq n}\|x_{i}\|\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|\big{]}+ \sigma\,\sqrt{2\,\log(n/\delta)}.\]Proof.: For all \(t>0\), we observe that

\[\mu^{n}\left(\max_{1\leq i\leq n}\|x_{i}\|\geq\mathbb{E}_{x\sim\mu} \big{[}\|x\|\big{]}+t\right) =\mu^{n}\left(\max_{1\leq i\leq n}\exp\left(\lambda(\|x_{i}\|- \mathbb{E}\|x_{i}\|)\right)\geq\exp(\lambda t)\right)\] \[\leq e^{-\lambda t}\mathbb{E}\big{[}\max_{1\leq i\leq n}\exp\left( \left(\lambda(\|x_{i}\|-\mathbb{E}\|x_{i}\|)\right)\right]\] \[\leq n\exp\left(-\lambda t+\frac{\lambda^{2}\sigma^{2}}{2}\right).\]

For fixed \(t\), the bound becomes tightest for \(\lambda=t/\sigma^{2}\) with

\[\mu^{n}\left(\max_{1\leq i\leq n}\|x_{i}\|\geq\mathbb{E}_{x\sim\mu}\big{[}\|x \|\big{]}+t\right)\leq n\exp\left(-\frac{t^{2}}{2\sigma^{2}}\right)\leq\delta\]

if

\[-\frac{t^{2}}{2\sigma^{2}}\leq\log\left(\frac{\delta}{n}\right)\qquad\Leftrightarrow \quad t\geq\sigma\,\sqrt{2\,\log(n/\delta)}.\]

Thus with probability at least \(1-\delta\), we have

\[\max_{1\leq i\leq n}\|X_{i}\|\leq R_{n}:=\mathbb{E}_{x\sim\mu}\big{[}\|x\| \big{]}+\sigma\,\sqrt{2\,\log(n/\delta)}.\qed\]

In the following Lemma, we investigate expectation of squared norm of sub-Gaussian near the tail.

**Lemma H.3**.: _Let \(\mu\) be \(\sigma^{2}\)-sub-Gaussian distribution in \(\mathbb{R}^{d}\). For any \(R>\mathbb{E}_{x\sim\mu}\|x\|\), we have the following:_

\[\mathbb{E}_{x\sim\mu}\big{[}\|x\|^{2}\,1_{B_{R}(0)^{c}}(x)\big{]}\leq\exp\left( -\frac{\left(R-\mathbb{E}\|x\|\right)^{2}}{4\sigma^{2}}\right)\,\sqrt{2\pi} \left(\left(\mathbb{E}\|x\|\right)^{2}+2\sigma^{2}\right).\]

Proof.: Recall that

\[\mu\left(\big{\{}x:\|x\|\geq\left(\mathbb{E}_{x^{\prime}\sim\mu}\|x^{\prime}\| +t\right)\big{\}}\right)\leq\exp\left(-\frac{t^{2}}{2\sigma^{2}}\right)\]

as demonstrated in the proof of Lemma H.1 (consider \(n=1\)). Thus

\[\mathbb{E}_{x\sim\mu}\big{[}\|x\|^{2}\,1_{B_{R}(0)^{c}}(x)\big{]} \leq\int_{R}^{\infty}s^{2}\mu\big{(}\big{\{}\|x\|\geq s\big{\}} \big{)}\,\mathrm{d}s\leq\int_{R}^{\infty}s^{2}\exp\left(-\frac{\left(s- \mathbb{E}\|x\|\right)^{2}}{2\sigma^{2}}\right)\,\mathrm{d}s\] \[\leq\exp\left(-\frac{\left(R-\mathbb{E}\|x\|\right)^{2}}{4\sigma^ {2}}\right)\int_{R}^{\infty}\exp\left(-\frac{\left(s-\mathbb{E}\|x\|\right)^{2 }}{4\sigma^{2}}\right)\,\mathrm{d}s\] \[=\exp\left(-\frac{\left(R-\mathbb{E}\|x\|\right)^{2}}{4\sigma^{2} }\right)\,\sqrt{2\pi}\left(\left(\mathbb{E}\|x\|\right)^{2}+2\sigma^{2}\right).\]

In next Lemma we introduce how the mean of squared norm of sub-Gaussian is concentrated.

**Lemma H.4**.: _Assume \(\mu\) is a \(\sigma^{2}\)-sub-Gaussian distribution in \(\mathbb{R}^{d}\). Let \(x_{1},\ldots,x_{n}\) be iid random samples from law \(\mu\). Then, with probability at least \(1-\delta\),_

\[\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}\leq\mathbb{E}_{x\sim\mu}\big{[}\|x\|^{2 }\big{]}+8\sigma^{2}\max\left(\frac{\log(1/\delta)}{n},\sqrt{\frac{\log(1/ \delta)}{n}}\right).\]_In particular, if \(\delta\geq e^{-n}\) then_

\[\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}\leq\mathbb{E}\big{[}\|x\|^{2}\big{]}+8 \sigma^{2}\,\sqrt{\frac{\log(1/\delta)}{n}}\]

_with probability at least \(1-\delta\)._

Proof.: Firstly since \(\|x_{i}\|\) is \(\sigma^{2}\)-sub-Gaussian, from (Honorio and Jaakkola, 2014, Appendix B) we observe that \(\|x_{i}\|^{2}\) is \((4\sqrt{2}\sigma^{2},4\sigma^{2})\)-sub-exponential. Next, by independence, for all \(\lambda>0\) and for all \(|t|\leq\frac{1}{4\sigma^{2}}\) we have the following:

\[\mathbb{E}\left[\exp\left(\lambda\big{(}\sum_{i=1}^{n}\|x_{i}\|^ {2}-\mathbb{E}\big{[}\sum_{i=1}^{n}\|x_{i}\|^{2}\big{]}\big{)}\right)\right] =\prod_{i=1}^{n}\mathbb{E}\left[\exp\left(\lambda\big{(}\|x_{i} \|^{2}-\mathbb{E}\big{[}\|x_{i}\|^{2}\big{]}\big{)}\right)\right]\] \[\leq\prod_{i=1}^{n}\exp\left(\frac{32\sigma^{4}\lambda^{2}}{2}\right)\] \[=\exp\left(\frac{32n\sigma^{4}\lambda^{2}}{2}\right)\]

which implies that \(\sum_{i=1}^{n}\|x_{i}\|^{2}\) is \((4\sqrt{2n}\sigma^{2},4\sigma^{2})\)-sub-exponential. Thus the tail bound of sub-exponential (Adams, 2022, Proposition 2.38) applied to \(\sum_{i=1}^{n}\|x_{i}\|^{2}\) yields

\[\mu^{n}\left(\big{|}\sum_{i=1}^{n}\|x_{i}\|^{2}-\mathbb{E}\big{[}\sum_{i=1}^{ n}\|x_{i}\|^{2}\big{]}\geq s\right)\leq\exp\left(-\frac{1}{2}\min\left(\frac{s^{2 }}{32n\sigma^{4}},\frac{s}{4\sigma^{2}}\right)\right)\]

Plugging-in \(s=nt\), we have the following:

\[\mu^{n}\left(\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}-\mathbb{E}_{x\sim\mu}\|x\| ^{2}\geq t\right)\leq\exp\left(-\frac{1}{2}n\min\left(\frac{t^{2}}{32\sigma^{ 4}},\frac{t}{4\sigma^{2}}\right)\right)\]

Lastly, take \(\delta=\exp\left(-\frac{1}{2}n\min(\frac{t^{2}}{32\sigma^{4}},\frac{t}{4\sigma ^{2}})\right)\). By rearranging the \(t\) with respect to \(\delta\), we have the following:

\[\mu^{n}\left(\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}-\mathbb{E}_{x\sim\mu}\|x\| ^{2}\geq 8\sigma^{2}\max\left(\sqrt{\frac{\log(1/\delta)}{n}},\frac{\log(1/ \delta)}{n}\right)\right)\leq\delta.\]

This proves the first part of the Lemma. Specifically, we have the following:

\[\mu^{n}\left(\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}-\mathbb{E}_{x \sim\mu}\|x\|^{2}\geq 8\sigma^{2}\frac{\log(1/\delta)}{n}\right) \leq\delta\quad\text{if }\delta\leq e^{-n}\] \[\mu^{n}\left(\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}-\mathbb{E}_{x \sim\mu}\|x\|^{2}\geq 8\sigma^{2}\sqrt{\frac{\log(1/\delta)}{n}}\right) \leq\delta\quad\text{if }\delta\geq e^{-n}\]

The second inequality proves the last part of the Lemma.