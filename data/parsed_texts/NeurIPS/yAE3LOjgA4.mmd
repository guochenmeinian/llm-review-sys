# From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks

 Clementine C. J. Domine\({}^{\dagger,\ddagger,1}\)

 Clementine.domine.20@ucl.ac.uk

&Nicolas Anguita\({}^{\dagger,2}\)

nicolas.anguita20@imperial.ac.uk

Alexandra M. Proca\({}^{2}\) &Lukas Braun\({}^{3}\) &Daniel Kunin\({}^{4}\) &Pedro A.M. Mediano\({}^{\ddagger,2,5}\)

Andrew M. Saxe\({}^{\ddagger 1,6,7}\)

###### Abstract

Biological and artificial neural networks create internal representations for complex tasks. In artificial networks, the ability to form task-specific representations is shaped by datasets, architectures, initialization strategies, and optimization algorithms. Previous studies show that different initializations lead to either a lazy regime, where representations stay static, or a rich regime, where they evolve dynamically. This work examines how initialization affects learning dynamics in deep linear networks, deriving exact solutions for \(\lambda\)-balanced initializations, which reflect the weight scaling across layers. These solutions explain how representations and the Neural Tangent Kernel evolve from rich to lazy regimes, with implications for continual, reversal, and transfer learning in neuroscience and practical applications.

+
Footnote †: dagger\) First authors

\(1\). Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom

\(2\). Department of Computing, Imperial College London, London, United Kingdom

\(3\). Department of Experimental Psychology, University of Oxford, Oxford, United Kingdom

\(4\). Institute for Computational and Mathematical Engineering, Stanford University, Stanford, USA

\(5\). Division of Psychology and Language Sciences, University College London, London, United Kingdom

\(6\). Sainsbury Wellcome Centre, University College London, London, United Kingdom

\(7\). CIFAR Azrieli Global Scholar, CIFAR, Toronto, Canada

\(\ddagger\) Co-senior Author

## 1 Introduction

Biological and artificial neural networks learn internal representations that enable complex tasks such as categorization, reasoning, and decision-making. Both systems often develop similar representations from comparable stimuli, suggesting shared information processing mechanisms Yamins et al. (2014). Although not yet fully understood, this similarity has garnered significant interest from neuroscience, AI, and cognitive science Haxby et al. (2001); Laakso and Cottrell (2000); Morcos et al. (2018); Kornblith et al. (2019); Moschella et al. (2022). The success of neural models relies on their ability to form these representations and extract relevant features from data to build internal representations, a complex process that in machine learning is defined by two regimes: _lazy_ and _rich_ Saxe et al. (2014); Pennington et al. (2017); Chizat et al. (2019); Bahri et al. (2020). Despite significant advances, these learning regimes and their characterization are not yet fully understood and would benefit from clearer theoretical predictions, particularly regarding the influence of prior knowledge (initialization) on the learning regime. We discuss related works in the appendix A.

**Our contributions.** (1) We derive exact solutions for the gradient flow in unequal-input-output two-layer deep linear networks, under a broad range of lambda-balanced initialization conditions (Section 2). (2) We model the full range of learning dynamics from _lazy_ to _rich_, showing that this transition is influenced by a complex interaction of architecture, _relative scale_, and _absolute scale_, (Section 3). (3) We present applications relevant to both the neuroscience and machine learning field, providing exact solutions for continual learning dynamics, reversal learning dynamics, and transfer learning (Section 4).

## 2 Exact Learning Dynamics

**Setting** Consider a supervised learning task where input vectors \(\mathbf{x}_{n}\in\mathbb{R}^{N_{i}}\), from a set of \(P\) training pairs \(\{(\mathbf{x}_{n},\mathbf{y}_{n})\}_{n=1}^{P}\), need to be mapped to their corresponding target output vectors \(\mathbf{y}_{n}\in\mathbb{R}^{N_{o}}\). We learn this task with a two-layer linear network model that produces the output prediction\(\hat{\mathbf{y}}_{n}=\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{x}_{n}\), with weight matrices \(\mathbf{W}_{1}\in\mathbb{R}^{N_{h}\times N_{i}}\) and \(\mathbf{W}_{2}\in\mathbb{R}^{N_{o}\times N_{h}}\), where \(N_{h}\) is the number of hidden units. The network's weights are optimized using full batch gradient descent with learning rate \(\eta\) (or respectively time constant \(\tau=\frac{1}{\eta}\)) on the mean squared error loss \(\mathcal{L}(\hat{\mathbf{y}},\mathbf{y})=\frac{1}{2}\left\langle||\hat{ \mathbf{y}}-\mathbf{y}||^{2}\right\rangle\), where \(\langle\cdot\rangle\) denotes the average over the dataset. The dynamics are completely determined by the input covariance and input-output correlation matrices of the dataset, defined as \(\tilde{\mathbf{\Sigma}}^{xx}=\frac{1}{P}\sum_{n=1}^{P}\mathbf{x}_{n}\mathbf{x }_{n}^{T}\in\mathbb{R}^{N_{i}\times N_{i}}\quad\text{and}\quad\tilde{\mathbf{ \Sigma}}^{yx}=\frac{1}{P}\sum_{n=1}^{P}\mathbf{y}_{n}\mathbf{x}_{n}^{T}\in \mathbb{R}^{N_{o}\times N_{i}}\), and the initialization \(\mathbf{W}_{2}(0),\mathbf{W}_{1}(0)\). Our objective is to describe the entire dynamics of the network's output and internal representations based on this initialization and the task statistics. We consider an approach first introduced in the foundational work of Fukumizu Fukumizu (1998) and extended in recent work by Braun et al. (2022), which rather than consider the dynamics of the parameters directly, we consider the dynamics of a matrix of the important statistics. In particular, defining \(\mathbf{Q}=\begin{bmatrix}\mathbf{W}_{1}&\mathbf{W}_{2}^{T}\end{bmatrix}^{T} \in\mathbb{R}^{(N_{i}+N_{o})\times N_{h}}\), we consider the \((N_{i}+N_{o})\times(N_{i}+N_{o})\) matrix \(\mathbf{Q}\mathbf{Q}^{T}(t)=\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}(t )&\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}(t)\\ \mathbf{W}_{2}\mathbf{W}_{1}(t)&\mathbf{W}_{2}\mathbf{W}_{2}^{T}(t)\end{bmatrix}\), which is divided into four quadrants with interpretable meanings. The approach tracks several key statistics collected in the matrix. The off-diagonal blocks contain the network function \(\hat{\mathbf{Y}}(t)=\operatorname{W}_{2}\operatorname{W}_{1}(t)\mathbf{X}\), which can be used to evaluate the dynamics of the loss as shown in Fig. 1. The on-diagonal blocks capture the correlation structure of the weight matrices, allowing for the calculation of the temporal evolution of the network's internal representations. This includes the representational similarity matrices (RSM) of the neural representations within the hidden layer, as first defined by Braun et al. (2022),\(\operatorname{RSM}_{I}=\mathbf{X}^{T}\mathbf{W}_{1}^{T}\mathbf{W}_{1}(t) \mathbf{X}\), \(\operatorname{RSM}_{O}=\mathbf{Y}^{T}(\mathbf{W}_{2}\mathbf{W}_{2}^{T}(t))^{+ }\mathbf{Y}\), where \(+\) denotes the pseudoinverse; and the network's finite-width NTK Jacot et al. (2018); Lee et al. (2019); Arora et al. (2019)\(\operatorname{NTK}=\mathbf{I}_{N_{o}}\otimes\mathbf{X}^{T}\mathbf{W}_{1}^{T} \mathbf{W}_{1}(t)\mathbf{X}+\mathbf{W}_{2}\mathbf{W}_{2}^{T}(t)\otimes \mathbf{X}^{T}\mathbf{X}\), where \(\mathbf{I}\) is the identity matrix and \(\otimes\) is the Kronecker product. Hence, the dynamics of \(\mathbf{Q}\mathbf{Q}^{T}\) describes the important aspects of network behaviour.

We extend previous solutions Fukumizu (1998); Braun et al. (2022); Kunin et al. (2024) and derive exact solutions for the dynamics of \(\mathbf{Q}\mathbf{Q}^{T}\) in unequal-input-output under a broad range of lambda-balanced initialization conditions. See Appendix B.2 for a further discussion of the assumptions and their relation to previous works. The proof of the Theorem and lemma leading to the theorem is in Appendix C. With this solution we can calculate the exact temporal dynamics of the loss, network function, RSMs and NTK (Fig. 1A, C) over a range of lambda-balanced initializations. **Implementation and simulation.** Simulation details are in Appendix F.7.

## 3 Rich and Lazy Learning

In this section we aim to gain a deeper understanding of the transition between the _rich_ and _lazy_ regimes by examining the dynamics as a function of lambda - the _relative scale_ - as it varies between positive and negative infinity.

**Dynamics of the singular values.** Here we examine a _lambda-balanced_ linear network initialized with _task-aligned_ weights. Previous research Saxe et al. (2019) has demonstrated that initial weights that are aligned with the task remain aligned throughout training, restricting the learning dynamics to the singular values of the network. As shown in Fig.4 B, as \(\lambda\) approaches zero, the dynamics resemble sigmoidal learning curves that traverse between saddle points, characteristic ofthe _rich_ regime Braun et al. (2022). In this regime the network learns the most salient features first, which can be beneficial for generalization Lampinen and Ganguli (2018). Conversely, as shown in Fig.4 A and C, as the magnitude of \(\lambda\) increases, the dynamics become exponential, characteristic of the _lazy_ regime. In this regime, all features are treated equally and the network's dynamics resemble that of a shallow network. Overall, our results highlight the critical influence of both the _absolute scale_ and the _relative scale_\(\lambda\) has in shaping the learning dynamics, from sigmoidal to exponential, steering the network between the _rich_ and _lazy_ regimes. The proof can be found in Appendix D.1.

**The dynamics of the representations.** We examine how the representations of the parameters \(\bm{W}_{1}\) and \(\bm{W}_{2}\) evolve during training. With lambda-balanced initializations, the structure persists throughout training, allowing us to recover the dynamics up to a time-dependent orthogonal transformation. The singular values \(\bm{S}_{\lambda}\) of the weights are adjusted based on \(\lambda\), splitting the representation into two parts (Theorem D.2). Using \(\mathbf{Q}\mathbf{Q}^{T}(t)\), we capture the temporal dynamics of hidden layer activations and analyze whether the network adopts a _rich_ or _lazy_ representation, depending on \(\lambda\). Upon convergence, the internal representation satisfies \(\mathbf{W}_{1}^{T}\mathbf{W}_{1}=\mathbf{\tilde{V}}\mathbf{\tilde{S}}_{1}^{2 }\mathbf{\tilde{V}}^{T}\) and \(\mathbf{W}_{2}\mathbf{W}_{2}^{T}=\mathbf{\tilde{U}}\mathbf{\tilde{S}}_{2}^{2 }\mathbf{\tilde{U}}^{T}\), with detailed proof in Theorem D.3. For a hierarchical semantic task Saxe et al. (2014); Braun et al. (2022), the representational similarity of inputs (\(\mathbf{\tilde{V}}\mathbf{\tilde{S}}\mathbf{\tilde{V}}^{T}\)) and targets (\(\mathbf{\tilde{U}}\mathbf{\tilde{S}}\mathbf{\tilde{U}}^{T}\)) aligns with the task structure. When training a two-layer network with relative scale \(\lambda=0\), the representational similarity matrices match the task upon convergence (Theorem D.3). As \(\lambda\) approaches positive or negative infinity, the network transitions to the _lazy_ regime, adopting task-agnostic representations (Theorem D.4, Fig. 2). The NTK becomes static and identity-like, while downscaled representations remain structured. This property can enhance generalization during fine-tuning, as shown in Section 4. In contrast, large Gaussian initializations result in _lazy_ learning, lacking structural representation. We propose a new _semi-structured lazy_ regime, where initialization determines whether task-specific features reside in input or output layers.

Figure 1: **A** The temporal dynamics of the numerical simulation of the loss, network function, correlation of input and output weights, and the NTK (row 1-5 respectively) are exactly matched by the analytical solution for \(\lambda=-2\). **B**\(\lambda=0.001\) Large initial weight values. **C**\(\lambda=2\) initial weight values.

**Representation robustness and sensitivity to noise.** In appendix D.3 we examine the relationship between the learning regime and the robustness of the learned representations to added noise in the inputs and parameters. In practice, parameter noise could be interpreted as the noise occurring within the neurons of a biological network. We find that a rich solution may enable a more robust representation in such systems.

**The impact of the architecture.** Thus far, we have found that the magnitude of the _relative scale_ parameter \(\lambda\) determines the extent or rich and lazy learning. Here, we explore how a network's learning regime is influenced by the interaction of its architecture and the sign of the _relative scale_. We consider three types of network architectures, depicted in Fig. 3A: _funnel networks_, which narrow from input to output (\(N_{i}>N_{h}=N_{o}\)); _inverted-funnel networks_, which expand from input to output (\(N_{i}=N_{h}<N_{o}\)); and _square networks_, where input and output dimensions are equal (\(N_{i}=N_{h}=N_{o}\)). Our solution, \(\mathbf{Q}\mathbf{Q}^{T}\), captures the NTK dynamics across these different network architectures. To examine the NTK's evolution under varying \(\lambda\) initializations, we compute the kernel distance from initialization, as defined in Fort et al. (2020). As shown in Fig. 3B, we observe that funnel networks consistently enter the _lazy_ regime as \(\lambda\rightarrow\infty\), while inverted-funnel networks do so as \(\lambda\rightarrow-\infty\). The NTK remains static during the initial phase, rigorously confirming the rank argument first introduced by Kunin et al. (2024) for the multi-output setting. In the opposite limits of \(\lambda\), these networks transition from a _lazy_ regime to a _rich_ regime. During this second alignment phase, the NTK matrix undergoes changes, indicating an initial _lazy_ phase followed by a _delayed rich_ phase. We further investigate and quantify this _delayed rich_ regime, showing the NTK movement over training in Fig. 3C. This behavior is also quantified in Theorem D.6, which describes the rate of learning in this network. For square networks with equal input and output dimensions, this behavior is discussed in Section 3. Across all architectures, as \(\lambda\to 0\), the networks consistently transition into the _rich_ regime. Altogether, we further characterize the _delayed rich_ regime in wide networks.

Figure 3: **A. Schematic representations of the network architectures considered, from left to right: funnel network, square network, and inverted-funnel network. B. The plot shows the NTK kernel distance from initialization, as defined in Fort et al. (2020) across the three architecture depicted schematically. C. The NTK kernel distance away from initialization over training time.**

Figure 2: **A A semantic learning task with the SVD of the input-output correlation matrix of the task. (top) \(U\) and \(V\) represent the singular vectors, and \(S\) contains the singular values. (bottom) The respective RSMs for the input and for the output task. B Simulation results and C Theoretical input and output representation matrices after training, showing convergence when initialized with varying lambda values, according to the initialization scheme described in F.7. D Final RSMs matrices after training converged when initialised from random large weights. E After convergence, the network’s sensitivity to input noise (top panel) is invariant to \(\lambda\), but the sensitivity to parameter noise increases as \(\lambda\) becomes smaller (or larger) than zero.**Application

**Continual learning.** In line with the framework presented by Braun et al. (2022), our approach describes the exact solutions of the networks dynamics trained across a sequence of tasks. As detailed in Appendix E.1, we demonstrate that, regardless of the chosen value of lambda, training on subsequent tasks can result in the overwriting of previously acquired knowledge, leading to catastrophic forgetting McCloskey Cohen (1989); Ratcliff (1990); French (1999).

**Reversal learning.** As demonstrated in Braun et al. (2022), reversal learning theoretically does not succeed in deep linear networks as the initalization aligns with the separatrix of a saddle point. While simulations show that the learning dynamics can escape the saddle point due to numerical imprecision, the process is catastrophically slowed in its vicinity. However, when \(\lambda\) is non-zero, reversal learning dynamics consistently succeed, as they avoid passing through the saddle point due to the initialization scheme. This is both theoretically proven and numerically illustrated in Appendix E.2. We also present a spectrum of reversal learning behaviors controlled by the _relative scale_\(\lambda\), ranging from _rich_ to _lazy_ learning regimes. This spectrum has the potential to explain the diverse dynamics observed in animal behavior, offering insights into the learning regimes relevant to various neuroscience experiments.

**Transfer learning.** We consider how different \(\lambda\) initializations influence generalization to a new feature after being trained on an initial task. We observe in Appendix figure 7 that the task specific structure of the data is effectively transferred to the new feature when the representation is task-specific and \(\lambda\) is zero. Conversely, when the output feature representation is _lazy_, meaning the hidden representation lacks adaptation, no task specific generalization is observed. Strikingly, when \(\lambda\) is positive, the task specific structure in the input weights remains small but structured, while the output weights exhibit a _lazy_ representation and the network generalizes to the task specific features. This suggests that the _lazy_ regime structure can be beneficial for transfer learning.

## 5 Discussion

We derive exact solutions to the learning dynamics within a tractable model class: deep linear networks. We examine the transition between the _rich_ and _lazy_ regimes by analyzing the dynamics as a function of \(\lambda\)--the _relative scale_--across its full range from positive to negative infinity. Our analysis demonstrates that the _relative scale_, \(\lambda\), plays a crucial role in managing the transition between _rich_ and _lazy_ regimes.

#### Acknowledgments

This research was funded in whole, or in part, by the Wellcome Trust [216386/Z/19/Z]. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. C.D. and A.S. were supported by the Gatsby Charitable Foundation (GAT3755). Further, A.S. was supported by the Sainsbury Wellcome Centre Core Grant (219627/Z/19/Z) and A.S. is a CIFAR Azrieli Global Scholar in the Learning in Machines & Brains program. A.M.P. was supported by the Imperial College London President's PhD Scholarship. L.B. was supported by the Woodward Scholarship awarded by Wadham College, Oxford. and the Medical Research Council [MR/N013468/1]. D.K. thanks the Open Philanthropy AI Fellowship for support. This research was funded in whole, or in part, by the Wellcome Trust [216386/Z/19/Z].

#### Author Contributions

Clementine and Nicolas led the work on the equal input-output solution to the exact dynamics, with support from Daniel. Clementine lead the extention to this work to the unequal input-output case from priminilary work from Nicolas. Then she examining the impact of the architecture. Clementine and Nicolas also spearheaded the development of the minimal model for _rich_ and _lazy_ learning, emphasizing exact solutions and representation convergence. Daniel, Nicolas, and Clementine collaborated on analyzing the dynamics of singular values, while Lukas led the investigation into representation robustness and sensitivity to noise. Clementine and Nicolas primarily conducted the analysis of the continual learning framework, while Alexandra and Clementine focused on transfer learning in the _rich_ and _lazy_ settings. Clementine took the lead on the reversal learning analysis. She was primarily responsible for leading the manuscript writing, with assistance from Daniel. Initial work on this project was carried during Nicolas masters supervised by Clementine and Pedro. All authors contributed to writing the appendix and refining the final manuscript.

## References

* Allen-Zhu et al. (2019) Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019a.
* Allen-Zhu et al. (2019b) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pp. 242-252. PMLR, 2019b.
* Arora et al. (2018a) Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. _arXiv preprint arXiv:1810.02281_, 2018a.
* Arora et al. (2018b) Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In _International Conference on Machine Learning_, pp. 244-253. PMLR, 2018b.
* Arora et al. (2019a) Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019a.
* Arora et al. (2019b) Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019b.
* Azulay et al. (2021) Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal mirror descent. In _International Conference on Machine Learning_, pp. 468-477. PMLR, 2021.
* Ba et al. (2022) Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems_, 35:37932-37946, 2022.
* Bahri et al. (2020) Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. _Annual Review of Condensed Matter Physics_, 11:501-528, 2020.
* Baldi and Hornik (1989) Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. _Neural networks_, 2(1):53-58, 1989.
* Bernardi et al. (2020) Silvia Bernardi, Marcus K Benna, Mattia Rigotti, Jerome Munuera, Stefano Fusi, and C Daniel Salzman. The geometry of abstraction in the hippocampus and prefrontal cortex. _Cell_, 183(4):954-967, 2020.
* Braun et al. (2022) Lukas Braun, Clementine Domine, James Fitzgerald, and Andrew Saxe. Exact learning dynamics of deep linear networks with prior knowledge. _Advances in Neural Information Processing Systems_, 35:6615-6629, 12 2022. URL https://papers.nips.cc/paper_files/paper/2022/hash/2b3bb2c95195130977a51b3bb251c40a-Abstract-Conference.html.
* Braun et al. (2024) Lukas Braun, Christopher Summerfield, and Andrew Saxe. Preserving knowledge during learning [unpublished manuscript]. _Department of Experimental Psychology, University of Oxford_, 2024.
* Chaudhuri et al. (2019) Rishidev Chaudhuri, Berk Gercek, Biraj Pandey, Adrien Peyrache, and Ila Fiete. The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep. _Nature neuroscience_, 22(9):1512-1520, 2019.
* Chizat and Bach (2018) Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Chizat et al. (2019)* Chizat and Bach (2020) Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on learning theory_, pp. 1305-1338. PMLR, 2020.
* Chizat et al. (2019) Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* Cui et al. (2024) Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborova, and Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. _arXiv preprint arXiv:2402.04980_, 2024.
* Du and Hu (2019) Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In _International Conference on Machine Learning_, pp. 1655-1664. PMLR, 2019.
* Du et al. (2019) Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pp. 1675-1685. PMLR, 2019.
* Du et al. (2018) Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* Farrell et al. (2023a) Matthew Farrell, Stefano Recanatesi, and Eric Shea-Brown. From lazy to rich to exclusive task representations in neural networks and neural codes. _Current Opinion in Neurobiology_, 83:102780, 2023a. doi: 10.1016/j.conb.2023.102780.
* Farrell et al. (2023b) Matthew Farrell, Stefano Recanatesi, and Eric Shea-Brown. From lazy to rich to exclusive task representations in neural networks and neural codes. _Current Opinion in Neurobiology_, 83:102780-102780, 12 2023b. doi: 10.1016/j.conb.2023.102780.
* Flesch et al. (2022) Timo Flesch, Keno Juechems, Tsvetomira Dumbalska, Andrew Saxe, and Christopher Summerfield. Orthogonal representations for robust context-dependent task performance in brains and neural networks. _Neuron_, 110:4212-4219, 12 2022. doi: 10.1016/j.neuron.2022.12.004.
* Fort et al. (2020) Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. _Advances in Neural Information Processing Systems_, 33:5850-5861, 2020.
* French (1999) Robert M French. Catastrophic forgetting in connectionist networks. _Trends in cognitive sciences_, 3(4):128-135, 1999.
* Fukumizu (1998) Kenji Fukumizu. Effect of batch learning in multilayer neural networks. _IEEE Transactions on Neural Networks_, 11:17-26, 1998. doi: 10.1109/72.822506.
* Geiger et al. (2020) Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2020:113301, 11 2020. doi: 10.1088/1742-5468/abc4de. URL https://arxiv.org/pdf/1906.08034.
* Gidel et al. (2019) Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gissin et al. (2019) Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental learning drives generalization. _arXiv preprint arXiv:1909.12051_, 2019.
* Haxby et al. (2001) James V Haxby, M Ida Gobbini, Maura L Furey, Alumit Ishai, Jennifer L Schouten, and Pietro Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. _Science_, 293(5539):2425-2430, 2001.
* Huh (2020) Dongsung Huh. Curvature-corrected learning dynamics in deep neural networks. In _International Conference on Machine Learning_, pp. 4552-4560. PMLR, 2020.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Jader et al. (2018)Arthur Jacot, Francois Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. _arXiv preprint arXiv:2106.15933_, 2021.
* Kornblith et al. (2019) Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In _International conference on machine learning_, pp. 3519-3529. PMLR, 2019.
* Kunin et al. (2022) Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli. The asymmetric maximum margin bias of quasi-homogeneous neural networks. _arXiv preprint arXiv:2210.03820_, 2022.
* Kunin et al. (2024) Daniel Kunin, Allan Raventos, Clementine Domine, Feng Chen, David Klindt, Andrew Saxe, and Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning, 06 2024. URL https://arxiv.org/abs/2406.06158.
* Laakso and Cottrell (2000) Aarre Laakso and Garrison Cottrell. Content and cluster analysis: assessing representational similarity in neural systems. _Philosophical psychology_, 13(1):47-76, 2000.
* Lampinen and Ganguli (2018) Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. _arXiv preprint arXiv:1809.10374_, 2018.
* Lee et al. (2019) Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems_, 32, 2019.
* Lee et al. (2022) Sebastian Lee, Stefano Sarao Mannelli, Claudia Clopath, Sebastian Goldt, and Andrew Saxe. Maslow's hammer for catastrophic forgetting: Node re-use vs node activation. _arXiv preprint arXiv:2205.09029_, 2022.
* Lewkowycz et al. (2020) Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* Li et al. (2020) Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. _arXiv preprint arXiv:2012.09839_, 2020.
* Liu et al. (2023) Yuhan Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas, Eric Shea-Brown, and Guillaume Lajoie. How connectivity structure shapes rich and lazy learning in neural circuits. _ArXiv_, 2023.
* Luo et al. (2021) Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu neural networks at infinite-width limit. _Journal of Machine Learning Research_, 22(71):1-47, 2021.
* Marcotte et al. (2023) Sibylle Marcotte, Remi Gribonval, and Gabriel Peyre. Abide by the law and follow the flow: conservation laws for gradient flows, 12 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/c7bee9b76be21146fd592fc2b46614d5-Abstract-Conference.html.
* McCloskey and Cohen (1989) Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pp. 109-165. Elsevier, 1989.
* Mei et al. (2018) Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* Morcos et al. (2018) Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks with canonical correlation. _Advances in neural information processing systems_, 31, 2018.
* Moschella et al. (2022) Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. Relative representations enable zero-shot latent space communication. _arXiv preprint arXiv:2209.15430_, 2022.
* Mo et al. (2020)* Ostojic and Fusi (2024) Srdjan Ostojic and Stefano Fusi. Computational role of structure in neural activity and connectivity. _Trends in Cognitive Sciences_, 2024.
* Pennington et al. (2017) Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. _Advances in neural information processing systems_, 30, 2017.
* Poggio et al. (2018) Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier Boix, and Jack Hidary. Theory iib: Generalization in deep networks. _arXiv preprint arXiv:1806.11379_, 2018.
* Raposo et al. (2014) David Raposo, Matthew T Kaufman, and Anne K Churchland. A category-free neural population supports evolving demands during decision-making. _Nature neuroscience_, 17(12):1784-1792, 2014.
* Ratcliff (1990) Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. _Psychological review_, 97(2):285, 1990.
* Rigotti et al. (2013) Mattia Rigotti, Omri Barak, Melissa R Warden, Xiao-Jing Wang, Nathaniel D Daw, Earl K Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497(7451):585-590, 2013.
* Rotskoff and Vanden-Eijnden (2018) Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks. _Advances in neural information processing systems_, 31, 2018.
* Rotskoff and Vanden-Eijnden (2022) Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. _Communications on Pure and Applied Mathematics_, 75(9):1889-1935, 2022.
* Saxe et al. (2013) Andrew Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _openreview.net_, 12 2013. URL https://openreview.net/forum?id=_wzWkpTDF_9C.
* Saxe et al. (2014) Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* Saxe et al. (2019a) Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019a. doi: 10.1073/pnas.1820226116. URL https://www.pnas.org/content/116/23/11537.
* Saxe et al. (2019b) Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019b.
* Sirignano and Spiliopoulos (2020) Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. _SIAM Journal on Applied Mathematics_, 80(2):725-752, 2020.
* Tang et al. (2019) Evelyn Tang, Marcelo G Mattar, Chad Giusti, David M Lydon-Staley, Sharon L Thompson-Schill, and Danielle S Bassett. Effective learning is accompanied by high-dimensional and efficient representations of neural activity. _Nature neuroscience_, 22(6):1000-1009, 2019.
* Tarmoun et al. (2021) Salma Tarmoun, Guilherme Franca, Benjamin D Haeffele, and Rene Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In _International Conference on Machine Learning_, pp. 10153-10161. PMLR, 2021.
* Tu et al. (2024) Zhenfeng Tu, Santiago Aranguri, and Arthur Jacot. Mixed dynamics in linear networks: Unifying the lazy and active regimes. _arXiv preprint arXiv:2405.17580_, 2024.
* Tye et al. (2024) Kay Tye, Earl Miller, Felix Taschbach, Marcus Benna, Mattia Rigotti, and Stefano Fusi. Mixed selectivity: Cellular computations for complexity. _Neuron_, 112, 05 2024. doi: 10.1016/j.neuron.2024.04.017.
* Tye et al. (2019)Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Conference on Learning Theory_, pp. 3635-3673. PMLR, 2020.
* Xu and Zheng (2024) Xiangxiang Xu and Lizhong Zheng. Neural feature learning in function space *. _Journal of Machine Learning Research_, 25:1-76, 2024. URL https://jmlr.org/papers/volume25/23-1202/23-1202.pdf.
* Xu and Ziyin (2024) Yizhou Xu and Liu Ziyin. When does feature learning happen? perspective from an analytically solvable model. _arXiv preprint arXiv:2401.07085_, 2024.
* Yamins et al. (2014) Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. _Proceedings of the national academy of sciences_, 111(23):8619-8624, 2014.
* Yang and Hu (2020) Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. _arXiv preprint arXiv:2011.14522_, 2020.
* Yang et al. (2022) Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. _arXiv preprint arXiv:2203.03466_, 2022.
* Zhu et al. (2023) Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning. _arXiv preprint arXiv:2306.04815_, 2023.
* Ziyin et al. (2022) Liu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. _Advances in Neural Information Processing Systems_, 35:24446-24458, 2022.
* Zou et al. (2020) Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep relu networks. _Machine learning_, 109:467-492, 2020.

Related Work

**Lazy regime.** Extensive research has identified a fundamental phenomenon in overparameterized neural networks: during training, these networks frequently remain near their linearized form, undergoing minimal changes in the parameter space Chizat et al. (2019). Consequently, they adopt learning dynamics akin to kernel regression, characterized by the Neural Tangent Kernel (NTK) matrix and exhibiting exponential learning behavior Du et al. (2018); Jacot et al. (2018); Du et al. (2019); Allen-Zhu et al. (2019, 2019); Zou et al. (2020). This behavior, known as the _lazy_ or kernel regime, typically occurs in infinitely wide architectures and can be triggered by large variance initialization at the start of training Jacot et al. (2018); Chizat et al. (2019). While the _lazy_ regime offers valuable insights into how networks converge to a global minimum, it does not fully account for the generalization capabilities of neural networks trained with standard initializations. It is, therefore, widely believed that another regime, driven by small or vanishing initializations, underpins some of the successes of neural networks.

**Rich regime.** In contrast, the _rich_ feature-learning regime is characterized by a NTK that evolves throughout training, accompanied by non-convex dynamics that navigate saddle points Baldi and Hornik (1989); Saxe et al. (2014, 2019); Jacot et al. (2021). This regime features sigmoidal learning curves and simplicity biases, such as low-rankness Li et al. (2020) or sparsity Woodworth et al. (2020). Numerous studies have shown that the _absolute scale_ of initialization drives the _rich_ regime, which typically emerges at small initialization scales Chizat et al. (2019); Geiger et al. (2020). However, it's also been shown that even at small initialization scales, differences in weight magnitudes between layers can induce the _lazy_ learning regime Azulay et al. (2021); Kunin et al. (2024). This highlights the significance of both _absolute scale_ (initialization variance) and _relative scale_ (difference in weight magnitude between layers) in generating diverse learning dynamics. Beyond _absolute scale_ and _relative scale_, additional aspects of initialization can profoundly affect feature learning, including the effective rank of the weight matrices Liu et al. (2023), layer-specific initialization variances Yang and Hu (2020); Luo et al. (2021); Yang et al. (2022), and the use of large learning rates Lewkowycz et al. (2020); Ba et al. (2022); Zhu et al. (2023); Cui et al. (2024). These findings illustrate the effect of initialization on inducing complex learning behavior through the resulting dynamics. Here we develop a solvable model which captures these diverse phenomena.

**Rich and lazy regimes in the brain.** The distinction between _rich_ and _lazy_ learning may also hold implications for neuroscience, where neural representations have been argued to have task-specific or task-agnostic characteristics in different settings Farrell et al. (2023); Ostojic and Fusi (2024); Tye et al. (2024). The _lazy_ regime can be linked to the non-linear mixed selectivity of neurons, where task variables are represented in a high-dimensional space which mixes various potentially relevant variables Raposo et al. (2014); Tang et al. (2019); Rigotti et al. (2013); Bernardi et al. (2020). Conversely, the _rich_ regime aligns with linear mixed selectivity Tye et al. (2024) and the manifold learning regime, where the brain encodes tasks on a structured, low-dimensional, task-specific manifold, as observed in grid cells within the entorhinal cortex Chaudhuri et al. (2019); Bernardi et al. (2020); Flesch et al. (2022).

**Linear networks.** Our work builds upon a rich body of research on deep linear networks, which, despite their simplicity, have proven to be valuable models for understanding more complex neural networks Baldi and Hornik (1989); Fukumizu (1998); Saxe et al. (2014). Previous research has extensively analyzed convergence Arora et al. (2018); Du and Hu (2019), generalization properties Lampinen and Ganguli (2018); Poggio et al. (2018); Huh (2020), and the implicit bias of gradient descent Arora et al. (2019); Woodworth et al. (2020); Chizat and Bach (2020); Kunin et al. (2022) in linear networks. These studies have also revealed that deep linear networks have intricate fixed point structures and nonlinear learning dynamics in parameter and function space, reminiscent of phenomena observed in nonlinear networks Arora et al. (2018); Lampinen and Ganguli (2018). Seminal work by Saxe et al. (2014) laid the groundwork by providing exact solutions to gradient flow dynamics under task-aligned initializations, demonstrating that the largest singular values are learned first during training. This analysis has been extended to deep linear networks Arora et al. (2018, 2019); Ziyin et al. (2022) with more flexible initialization schemes Gidel et al. (2019); Tarmoun et al. (2021); Gissin et al. (2019). This work directly builds on the matrix Riccati formulation proposed by Fukumizu (1998); Braun et al. (2022) which extends these solutions to wide networks. We extend and refine these results to obtain the dynamics for lambda-balanced initialization dynamics of networks to more clearly demonstrate the impact of initialization on _rich_ and _lazy_ learning regimes also developed in Tu et al. (2024) for a set of orthogonal initalizations. Our work extends previous analysis Xu and Ziyin (2024); Kunin et al. (2024) of these regime to wide networks. Previous studies leveraged these solutions primarily to characterize convergence rates; however, our work goes beyond this by providing a comprehensive characterization of the complete dynamics of the system Tarmoun et al. (2021).

**Infinite-width networks.** Recent advances in understanding the _rich_ regime have largely stemmed from examining how the initialization variance and layer-wise learning rates must scale in the infinite-width limit to maintain consistent behavior in activations, gradients, and outputs. Several studies have employed statistical mechanics tools to derive analytical solutions for the _rich_ population dynamics of two-layer nonlinear neural networks initialized using the _mean field_ parameterization Mei et al. (2018); Rotskoff and Vanden-Eijnden (2018); Chizat and Bach (2018); Sirignano and Spiliopoulos (2020); Rotskoff and Vanden-Eijnden (2022); Sirignano and Spiliopoulos (2020). Other methods for analyzing deep network dynamics include the NTK limit, where the network effectively performs kernel regression without feature learning Jacot et al. (2018); Lee et al. (2019); Arora et al. (2019). Our solution allows us to the study the evolution of the NTK and the influence of _absolute scale_ and _relative scale_ on the transition between _lazy_ and _rich_ learning in finite width networks Jacot et al. (2021); Xu and Ziyin (2024); Kunin et al. (2024); Chizat et al. (2019). Furthermore, these approaches typically require numerical integration or operate within a limited learning regime, and are unable to describe the learning dynamics of hidden representations. Instead, our work focuses on the impact of initialization on representation learning dynamics and derives explicit analytical solutions within tractable models.

## Appendix B Preliminaries

### Appendix: Balanced Condition

**Definition B.1** (Definition of \(\lambda\)_-balanced_ property (Saxe et al. (2013), Marcotte et al. (2023))).: The weights \(\bm{W_{1}},\bm{W_{2}}\) are \(\lambda\)_-balanced_ if and only if there exists a **Balanced Coefficient**\(\lambda\in\mathbb{R}\) such that:

\[B(\bm{W_{1}},\bm{W_{2}})=\bm{W_{2}^{T}}\bm{W_{2}}-\bm{W_{1}}\bm{W_{1}}^{T}= \lambda\mathbf{I}\] (1)

where \(B\) is called the **Balanced Computation**.

For \(\lambda=0\) we have **Zero-Balanced** given as \(\mathbf{A}\mathbf{1}\) (\(0\)). \(\mathbf{W_{1}}(0)\mathbf{W_{1}}(0)^{T}=\mathbf{W_{2}}(0)^{T}\mathbf{W_{2}}(0)\).

**Theorem B.2**.: _Balanced Condition Persists Through Training_

_Suppose at initialization_

\[\bm{W_{2}(0)}^{T}\bm{W_{2}(0)}-\bm{W_{1}(0)}\bm{W_{1}(0)}^{T}=\lambda\mathbf{I}\] (2)

_Then for all \(t\geq 0\)_

\[\bm{W_{2}(t)}^{T}\bm{W_{2}(t)}-\bm{W_{1}(t)}\bm{W_{1}(t)}^{T}=\lambda\mathbf{I}\] (3)

Proof.: Consider:

\[\tau\frac{d}{dt}\left[\bm{W_{2}}(t)\bm{W_{2}}(t)^{T}-\bm{W_{1}}( t)\bm{W_{1}}(t)^{T}\right] =\left(\tau\frac{d}{dt}\bm{W_{2}}(t)\right)^{T}\bm{W_{2}}(t)+\bm{W _{2}}(t)^{T}\left(\tau\frac{d}{dt}\bm{W_{2}}(t)\right)\] \[\quad-\left(\tau\frac{d}{dt}\bm{W_{1}}(t)\right)\bm{W_{1}}(t)^{T} -\bm{W_{1}}(t)\left(\tau\frac{d}{dt}\bm{W_{1}}(t)\right)^{T}\] \[=\bm{W_{1}}(t)\left(\tilde{\bm{\Sigma}}^{yx}-\bm{W_{2}}(t)\bm{W_{1 }}(t)\tilde{\bm{\Sigma}}^{xx}\right)^{T}\bm{W_{2}}(t)\] \[\quad+\bm{W_{2}}(t)^{T}\left(\tilde{\bm{\Sigma}}^{yx}-\bm{W_{2}}( t)\bm{W_{1}}(t)\tilde{\bm{\Sigma}}^{xx}\right)\bm{W_{1}}(t)\] \[\quad-\bm{W_{2}}(t)^{T}\left(\tilde{\bm{\Sigma}}^{yx}-\bm{W_{2}}( t)\bm{W_{1}}(t)\tilde{\bm{\Sigma}}^{xx}\right)\bm{W_{1}}(t)\] \[\quad-\bm{W_{1}}(t)\left(\tilde{\bm{\Sigma}}^{yx}-\bm{W_{2}}(t) \bm{W_{1}}(t)\tilde{\bm{\Sigma}}^{xx}\right)\bm{W_{2}}(t)\] \[=\bm{0}\]Note that \(\bm{W_{2}(t)}^{T}\bm{W_{2}(t)}-\bm{W_{1}(t)}\bm{W_{1}(t)}^{T}\) is conserved for any initial value \(\lambda\). 

### Discussion Assumptions

* **A2** (_Whitened input_). The input data is whitened, that is \(\bm{\tilde{\Sigma}}^{xx}=\mathbf{I}\).
* **A3** (_Lambda-balanced_). The network's weight matrices are lambda-balanced at the beginning of training, that is \(\mathbf{W}_{2}(0)^{T}\mathbf{W}_{2}(0)-\mathbf{\tilde{W}}_{1}(0)\mathbf{W}_{1 }(0)^{T}=\lambda\mathbf{I}\). If this condition holds at initialization, it will persist throughout training Saxe et al. (2014); Arora et al. (2018). For completeness, we prove this in Appendix B.
* **A4** (_Dimensions_). The hidden dimension of the network is defined as \(N_{h}=\min(N_{i},N_{o})\), ensuring the network is neither bottleneck (\(N_{h}<\min(N_{i},N_{o})\)) nor overparameterized (\(N_{h}>\min(N_{i},N_{o})\)).
* **A5** (_Full-rank_). The input-output correlation of the task and the initial state of the network function have full rank, that is \(\mathrm{rank}(\bm{\tilde{\Sigma}}^{xy})=\mathrm{rank}(\mathbf{W}_{2}(0) \mathbf{W}_{1}(0))=\min(N_{i},N_{o})\).

Whitened Inputs.Although the whitened input assumption is quite strong, it is commonly used in analytical work to obtain exact solutions, and much of the existing literature relies on these solutions Fukumizu (1998); Braun et al. (2022); Kunin et al. (2024). Kunin et al. (2024) goes further by exploring the implicit bias of the trajectory without relying on exact solutions. When \(X^{\intercal}X\) is low-rank, they can only predict the trajectories in the limit as \(\lambda\rightarrow\pm\infty\). If the interpolating manifold is one-dimensional, the solution can be solved exactly in terms of \(\lambda\) (black dots).

Dimension.Fukumizu assumed equal input and output dimensions \(N_{i}=N_{o}\), but allowed for a bottleneck in the hidden dimension of the network \(N_{h}\leq N_{i}=N_{o}\). The work by Braun et al. (2022) extended Fukumizu (1998) solutions to cases with unequal input and output dimensions \(N_{i}\neq N_{o}\), but to so did not allow a bottleneck \(N_{h}=\min\{N_{i},N_{o}\}\) and added an assumption on the invertibility of a statistic of the singular vector overlap between the model and the input-output statistics. In our work we allow for unequal input and output \(N_{i}\neq N_{o}\) and do not introduce an additional invertibility assumption.

Balancedness.The main distinction between our work and prior works is that both Fukumizu (1998) and Braun et al. (2022) assumed zero-balanced \(\mathbf{W}_{1}(0)\mathbf{W}_{1}(0)^{T}=\mathbf{W}_{2}(0)^{T}\mathbf{W}_{2}(0)\), while we relax this assumption to \(\lambda\)-balanced. The zero-balanced condition restricts the networks to a _rich_ setting. We develop solutions to explore the continuum between the _rich_ and the _lazy_ regime. While some works, such as Tarmoun et al. (2021), have considered removing this constraint, their solutions remain in an unstable and mixed form. Our work, in its form enable the understanding of different learning regimes by exploring initialization properties beyond just _absolute scale_ and demonstrate that this transition can be accessed and controlled by adjusting a key parameter: the _relative scale_. Other studies, such as Kunin et al. (2024) and Xu & Zheng (2024), have similarly relaxed the balancedness assumption but were limited to single-output neuron settings.

## Appendix C Appendix: Exact learning dynamics with prior knowledge

### Appendix: Fukumizu Approach

**Lemma C.1**.: _We introduce the variables_

\[\mathbf{Q}=\begin{bmatrix}\mathbf{W}_{1}^{T}\\ \mathbf{W}_{2}\end{bmatrix}\quad\text{and}\quad\mathbf{Q}\mathbf{Q}^{T}= \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_ {2}^{T}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}.\] (4)

_Defining_

\[\mathbf{F}=\begin{bmatrix}-\frac{\lambda}{2}I&(\tilde{\Sigma}^{yx})^{T}\\ \tilde{\Sigma}^{yx}&\frac{\lambda}{2}I\end{bmatrix},\] (5)

_the gradient flow dynamics of \(\mathbf{Q}\mathbf{Q}^{T}(t)\) can be written as a differential matrix Riccati equation_

\[\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})=\mathbf{F}\mathbf{Q}\mathbf{Q}^{T} +\mathbf{Q}\mathbf{Q}^{T}\mathbf{F}-(\mathbf{Q}\mathbf{Q}^{T})^{2}.\] (6)Proof.: We introduce the variables

\[\mathbf{Q}=\begin{bmatrix}\mathbf{W}_{1}^{T}\\ \mathbf{W}_{2}\end{bmatrix}\quad\text{and}\quad\mathbf{Q}\mathbf{Q}^{T}= \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{2 }^{T}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}.\] (7)

We compute the time derivative

\[\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})=\tau\begin{bmatrix}\frac{d\mathbf{W }_{1}^{T}}{dt}\mathbf{W}_{1}+\mathbf{W}_{1}^{T}\frac{d\mathbf{W}_{1}}{dt}& \frac{d\mathbf{W}_{1}^{T}}{dt}\mathbf{W}_{2}+\mathbf{W}_{1}^{T}\frac{d\mathbf{ W}_{2}}{dt}\\ \frac{d\mathbf{W}_{2}}{dt}\mathbf{W}_{1}+\mathbf{W}_{2}\frac{d\mathbf{W}_{1}}{ dt}&\frac{d\mathbf{W}_{2}^{T}}{dt}\mathbf{W}_{2}+\mathbf{W}_{2}^{T}\frac{d \mathbf{W}_{2}}{dt}\end{bmatrix}.\] (8)

Using equations 18 and 19, we compute the four quadrants separately giving

\[\tau\left(\frac{d\mathbf{W}_{1}^{T}}{dt}\mathbf{W}_{1}+\mathbf{W}_{1}^{T}\frac {d\mathbf{W}_{1}}{dt}\right)=\] (9)

\[=(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})^{T}\mathbf{W}_{2}\mathbf{W}_{1}+ \mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})\] (10)

\[=(\Sigma^{yx})^{T}\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}^{T}\Sigma^{yx}-\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}\mathbf{W}_{2}\mathbf{W} _{1}-(\mathbf{W}_{2}\mathbf{W}_{1})^{T}\mathbf{W}_{2}\mathbf{W}_{1}\] (11)

\[=(\Sigma^{yx})^{T}\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}^{T}\Sigma^{yx}-\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}\mathbf{W}_{2}\mathbf{W }_{1}-\mathbf{W}_{1}^{T}\mathbf{W}_{1}^{T}\mathbf{W}_{1}-\lambda\mathbf{W}_{1} ^{T}\mathbf{W}_{1},\] (12)

\[\tau\left(\frac{d\mathbf{W}_{1}^{T}}{dt}\mathbf{W}_{2}^{T}+\mathbf{W}_{1}^{T} \frac{d\mathbf{W}_{2}^{T}}{dt}\right)=\] (13)

\[=(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})^{T}\mathbf{W}_{2}\mathbf{W}_{2}^ {T}+\mathbf{W}_{1}^{T}\mathbf{W}_{1}(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1} )^{T}\] (14)

\[=(\Sigma^{yx})^{T}\mathbf{W}_{2}\mathbf{W}_{2}^{T}+\mathbf{W}_{1}^{T} \mathbf{W}_{1}(\Sigma^{yx})^{T}-\mathbf{W}_{1}^{T}\mathbf{W}_{1}\mathbf{W}_{1 }^{T}\mathbf{W}_{2}^{T}-\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}\mathbf{W}_{2} \mathbf{W}_{2}^{T},\] (15)

\[\tau\left(\frac{d\mathbf{W}_{2}}{dt}\mathbf{W}_{1}+\mathbf{W}_{2}\frac{d \mathbf{W}_{1}}{dt}\right)=\] (16)

\[=(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})\mathbf{W}_{1}^{T}\mathbf{W}_{1}+ \mathbf{W}_{2}\mathbf{W}_{2}^{T}(\Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})\] (17)

\[=\Sigma^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{1}+\mathbf{W}_{2}\mathbf{W}_{2}^{T} \Sigma^{yx}-\mathbf{W}_{2}\mathbf{W}_{2}^{T}\mathbf{W}_{2}\mathbf{W}_{1}- \mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W}_{1}^{T}\mathbf{W}_{1},\] (18)

\[\tau\left(\frac{d\mathbf{W}_{2}}{dt}\mathbf{W}_{2}^{T}+\mathbf{W}_{2}\frac{d \mathbf{W}_{2}^{T}}{dt}\right)=\] (19)

\[(\tilde{\Sigma}^{yx}-\mathbf{W}_{2}\mathbf{W}_{1})\mathbf{W}_{1}^{T}\mathbf{W} _{2}^{T}+\mathbf{W}_{2}\mathbf{W}_{1}(\tilde{\Sigma}^{yx}-\mathbf{W}_{2} \mathbf{W}_{1})^{T}\] (20)

\[=\tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}+\mathbf{W}_{2} \mathbf{W}_{1}(\tilde{\Sigma}^{yx})^{T}-\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W} _{1}^{T}\mathbf{W}_{2}^{T}-\mathbf{W}_{2}\mathbf{W}_{1}(\mathbf{W}_{2}\mathbf{W }_{1})^{T}\] (21)

\[=\tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}+\mathbf{W}_{2} \mathbf{W}_{1}(\tilde{\Sigma}^{yx})^{T}-\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W} _{1}^{T}\mathbf{W}_{2}^{T}-\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W}_{1}^{T} \mathbf{W}_{2}^{T}\] (22)

\[=\tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}+\mathbf{W}_{2} \mathbf{W}_{1}(\tilde{\Sigma}^{yx})^{T}-\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W} _{1}^{T}\mathbf{W}_{2}^{T}-\mathbf{W}_{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T}\mathbf{W }_{2}^{T}+\lambda\mathbf{W}_{2}\mathbf{W}_{2}^{T}.\] (23)

Defining

\[\mathbf{F}=\begin{bmatrix}-\frac{\lambda}{2}I&(\tilde{\Sigma}^{yx})^{T}\\ \tilde{\Sigma}^{yx}&\frac{\lambda}{2}I\end{bmatrix},\] (24)

the gradient flow dynamics of \(\mathbf{Q}\mathbf{Q}\mathbf{Q}^{T}(t)\) can be written as a differential matrix Riccati equation

\[\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})=\mathbf{F}\mathbf{Q}\mathbf{Q}^{T}+ \mathbf{Q}\mathbf{Q}^{T}\mathbf{F}-(\mathbf{Q}\mathbf{Q}^{T})^{2}.\] (25)

We write \(\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})\) for completeness

\[\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})= \begin{bmatrix}-\frac{\lambda}{2}&(\tilde{\Sigma}^{yx})^{T}\\ \tilde{\Sigma}^{yx}&\frac{\lambda}{2}\end{bmatrix}\begin{bmatrix}\mathbf{W}_{1}^{T} \mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{2}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}+ \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}^{T} \begin{bmatrix}-\frac{\lambda}{2}&(\tilde{\Sigma}^{yx})^{T}\\ \tilde{\Sigma}^{yx}&\frac{\lambda}{2}\end{bmatrix}\] (26)\[= \begin{bmatrix}-\frac{\lambda}{2}&(\tilde{\Sigma}^{yx})^{T}\end{bmatrix} \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{2} \\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}+ \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{2 }\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}^{T} \begin{bmatrix}-\frac{\lambda}{2}&(\tilde{\Sigma}^{yx})^{T}\\ \tilde{\Sigma}^{yx}&\frac{\lambda}{2}\end{bmatrix}\] (27) \[-\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T }\mathbf{W}_{2}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix} \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{2 }\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}\] \[=\begin{bmatrix}-\frac{\lambda}{2}\mathbf{W}_{1}^{T}\mathbf{W}_{1 }+(\tilde{\Sigma}^{yx})^{T}\mathbf{W}_{2}\mathbf{W}_{1}&-\frac{\lambda}{2} \mathbf{W}_{1}^{T}\mathbf{W}_{2}+(\tilde{\Sigma}^{yx})^{T}\mathbf{W}_{2} \mathbf{W}_{2}^{T}\\ \tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{1}+\frac{\lambda}{2} \mathbf{W}_{2}\mathbf{W}_{1}&\tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}+\frac{\lambda}{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}\] \[+\begin{bmatrix}-\frac{\lambda}{2}\mathbf{W}_{1}^{T}\mathbf{W}_{1 }+\mathbf{W}_{1}^{T}\mathbf{W}_{1}(\tilde{\Sigma}^{yx})^{T}&\frac{\lambda}{2} \mathbf{W}_{1}^{T}\mathbf{W}_{2}+\mathbf{W}_{1}^{T}\mathbf{W}_{2}(\tilde{ \Sigma}^{yx})^{T}\\ -\frac{\lambda}{2}\mathbf{W}_{2}^{T}\mathbf{W}_{1}+\mathbf{W}_{2}\mathbf{W}_{ 1}(\tilde{\Sigma}^{yx})^{T}&\frac{\lambda}{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T} +\mathbf{W}_{2}\mathbf{W}_{2}^{T}(\tilde{\Sigma}^{yx})^{T}\end{bmatrix}\] (28) \[-\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T }\mathbf{W}_{2}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix} \begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}&\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}\\ \mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}\] \[=\begin{bmatrix}-\frac{\lambda}{2}\mathbf{W}_{1}^{T}\mathbf{W}_{ 1}+(\tilde{\Sigma}^{yx})^{T}\mathbf{W}_{2}\mathbf{W}_{1}&-\frac{\lambda}{2} \mathbf{W}_{1}^{T}\mathbf{W}_{2}+(\tilde{\Sigma}^{yx})^{T}\mathbf{W}_{2} \mathbf{W}_{2}^{T}\\ \tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{1}+\frac{\lambda}{2} \mathbf{W}_{2}\mathbf{W}_{1}&\tilde{\Sigma}^{yx}\mathbf{W}_{1}^{T}\mathbf{W}_{ 2}+\frac{\lambda}{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T}\end{bmatrix}\] \[+\begin{bmatrix}-\frac{\lambda}{2}\mathbf{W}_{1}^{T}\mathbf{W}_{ 1}+\mathbf{W}_{1}^{T}\mathbf{W}_{1}(\tilde{\Sigma}^{yx})^{T}&\frac{\lambda}{2} \mathbf{W}_{1}^{T}\mathbf{W}_{2}+\mathbf{W}_{1}^{T}\mathbf{W}_{2}(\tilde{ \Sigma}^{yx})^{T}\\ -\frac{\lambda}{2}\mathbf{W}_{2}^{T}\mathbf{W}_{1}+\mathbf{W}_{2}\mathbf{W}_{ 1}(\tilde{\Sigma}^{yx})^{T}&\frac{\lambda}{2}\mathbf{W}_{2}\mathbf{W}_{2}^{T}+ \mathbf{W}_{2}\mathbf{W}_{2}^{T}(\tilde{\Sigma}^{yx})^{T}\end{bmatrix}\] (45) \[-\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1}\mathbf{W}_{1}^{T }\mathbf{W}_{1}+\mathbf{W}_{1}^{T}\mathbf{W}_{2}\mathbf{W}_{2}^{T}\mathbf{W}_{ 1}&\mathbf{W}_{1}^{T}\mathbf{W}_{1}\mathbf{W}_{2}+\mathbf{W}_{1}^{T}\mathbf{W} _{2}\mathbf{W}_{2}^{T}\mathbf{W}_{2}\\ \mathbf{W}_{2}\mathbf{W}_{1}\mathbf{W}_{1}^{T}\mathbf{W}_{1}+\mathbf{W}_{2} \mathbf{W}_{2}^{T}\mathbf{W}_{2}\mathbf{W}_{1}&\mathbf{W}_{2}\mathbf{W}_{1} \mathbf{W}_{1}^{T}\mathbf{W}_{2}+\mathbf{W}_{2}\mathbf{W}_{2}^{T}\mathbf{W}_{ 2}\mathbf{W}_{2}^{T}\end{bmatrix}\]

The four quadrants of 8 are equivalent to equations 12, 15, 18, and 23 respectively.

### \(\mathbf{Q}\mathbf{Q}^{T}\) Diagonalisation

**Lemma C.2**.: _If \(\bm{F}=\bm{P}\bm{\Lambda}\bm{P}^{T}\) is symmetric and diagonalizable, then the matrix Riccati differential equation \(\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})=\mathbf{F}\mathbf{Q}\mathbf{Q}^{T}+ \mathbf{Q}\mathbf{Q}^{T}\mathbf{F}-(\mathbf{Q}\mathbf{Q}^{T})^{2}\) with initialization \(\mathbf{Q}\mathbf{Q}^{T}(0)=\mathbf{Q}(0)\mathbf{Q}(0)^{T}\) has a unique solution for all \(t\geq 0\), and the solution is given by_

\[\mathbf{Q}\mathbf{Q}^{T}(t)=e^{\mathbf{F}\frac{t}{\tau}}\mathbf{Q}(0)\left[ \mathbf{I}+\mathbf{Q}(0)^{T}\bm{P}\left(\frac{e^{2\mathbf{\Lambda}\frac{t}{\tau} }-\mathbf{I}}{2\bm{\Lambda}}\right)\bm{P}^{T}\mathbf{Q}(0)\right]^{-1}\mathbf{ Q}(0)^{T}e^{\mathbf{F}\frac{t}{\tau}}.\] (29)

_This is true even when there exists \(\bm{\Lambda}_{i}=0\)._

Proof.: First we show that there exists a unique solution to the initial value problem stated. This is true by Picard-Lindelof theorem. Now we show that the provided solution satisfies the ODE. Let \(\bm{L}=e^{\mathbf{F}\frac{t}{\tau}}\mathbf{Q}(0)\) and \(\bm{C}=\mathbf{I}+\mathbf{Q}(0)^{T}\bm{P}\left(\frac{e^{2\mathbf{\Lambda}\frac{t}{ \tau}}-\mathbf{I}}{2\bm{\Lambda}}\right)\bm{P}^{T}\mathbf{Q}(0)\) such that solution \(\mathbf{Q}\mathbf{Q}^{T}(t)=\bm{L}\bm{C}^{-1}\bm{L}^{T}\). The time derivative of \(\mathbf{Q}\mathbf{Q}^{T}\) is then given by

\[\tau\frac{d}{dt}(\mathbf{Q}\mathbf{Q}^{T})=\tau\left(\frac{d}{dt}(\bm{L})\bm{C}^{ -1}\bm{L}^{T}+\bm{L}\frac{d}{dt}(\bm{C}^{-1})\bm{L}^{T}+\bm{L}\bm{C}^{-1} \frac{d}{dt}(\bm{L}^{T})\right)\] (30)

Solving for these derivatives individually, we find

\[\tau\frac{d}{dt}(\bm{L}) =\tau\frac{d}{dt}e^{\mathbf{F}\frac{t}{\tau}}\mathbf{Q}(0)=\bm{F} \bm{L}\] (31) \[\tau\frac{d}{dt}(\bm{C}^{-1}) =-\tau\bm{C}^{-1}\frac{d}{dt}(\bm{C})\bm{C}^{-1}=-\tau\bm{C}^{-1} \mathbf{Q}(0)^{T}\bm{P}\frac{d}{dt}\left(\frac{e^{2\mathbf{\Lambda}\frac{t}{\tau}}- \mathbf{I}}{2\bm{\Lambda}}\right)\bm{P}^{T}\mathbf{Q}(0)\bm{C}^{-1}\] (32)

We consider the derivative of the fraction serpately,

\[\tau\frac{d}{dt}\left(\frac{e^{2\mathbf{\Lambda}\frac{t}{\tau}}-\mathbf{I}}{2\bm {\Lambda}}\right)=e^{2\mathbf{\Lambda}\frac{t}{\tau}}\] (33)

this is true even in the limit as \(\lambda_{i}\to 0\). Plugging these derivatives back in we see that the solution satisfies the ODE. LastlyIn Appendix C.2 we prove that this equation is the unique solution to the initial value problem derived in Lemma C.2 no matter the value of \(\Lambda\). However, as discussed in Braun et al. (2022), the solution in this form is not very useable or interpretable due to the matrix inverse mixing the blocks of \(\mathbf{Q}\mathbf{Q}^{T}\). Additionally, we need to diagonalize \(\bm{F}\). To do so we consider the compact singular value decomposition \(\mathrm{SVD}(\bm{\tilde{\Sigma}}^{yx})=\mathbf{\tilde{U}}\mathbf{\tilde{S}} \mathbf{\tilde{V}}^{T}\). Here, \(\mathbf{\tilde{U}}\in\mathbb{R}^{N_{o}\times N_{h}}\) denote the left singular vectors, \(\mathbf{\tilde{S}}\in\mathbb{R}^{N_{h}\times N_{h}}\) the square matrix with ordered, non-zero eigenvalues on its diagonal, and \(\mathbf{\tilde{V}}\in\mathbb{R}^{N_{i}\times N_{h}}\) the corresponding right singular vectors. For unequal input-output dimensions (\(N_{i}\neq N_{o}\)), the right and left singular vectors are not square. Accordingly, for the case \(N_{i}>N_{h}=N_{o}\), we define \(\mathbf{\tilde{U}}^{\perp}\in\mathbb{R}^{N_{o}\times|N_{o}-N_{i}|}\) as a matrix containing orthogonal column vectors that complete the basis for \(\mathbf{\tilde{U}}\), i.e., make \(\left[\mathbf{\tilde{U}}\ \mathbf{\tilde{U}}^{\perp}\right]\) orthonormal, and \(\mathbf{\tilde{V}}^{\perp}\in\mathbb{R}^{N_{i}\times|N_{o}-N_{i}|}\) as a matrix of zeros. Conversely, when \(N_{i}=N_{h}<N_{o}\), then \(\mathbf{\tilde{V}}^{\perp}\) is a matrix containing orthogonal column vectors that complete the basis for \(\tilde{\bm{V}}\) and \(\mathbf{\tilde{U}}^{\perp}\) is a matrix of zeros. Using this SVD structure we can now describe the eigendecomposition of \(\mathbf{F}\).

### \(\bm{F}\) Diagonalization

**Lemma C.3**.: _Under assumptions of full-rank 5, the eigendecomposition of \(\mathbf{F}=\mathbf{P}\bm{\Lambda}\mathbf{P}^{T}\) where_

\[\mathbf{P}=\frac{1}{\sqrt{2}}\begin{pmatrix}\mathbf{\tilde{V}}( \mathbf{\tilde{G}}-\mathbf{\tilde{H}}\mathbf{\tilde{G}})&\mathbf{\tilde{V}}( \mathbf{\tilde{G}}+\mathbf{\tilde{H}}\mathbf{\tilde{G}})&\sqrt{2}\mathbf{ \tilde{V}}_{\perp}\\ \mathbf{\tilde{U}}(\mathbf{\tilde{G}}+\mathbf{\tilde{H}}\mathbf{\tilde{G}})&- \mathbf{\tilde{U}}(\mathbf{\tilde{G}}-\mathbf{\tilde{H}}\mathbf{\tilde{G}})& \sqrt{2}\mathbf{\tilde{U}}_{\perp}\end{pmatrix},\quad\bm{\Lambda}=\begin{pmatrix} \mathbf{\tilde{S}}_{\lambda}&0&0\\ 0&-\mathbf{\tilde{S}}_{\lambda}&0\\ 0&0&\bm{\lambda}_{\perp}\end{pmatrix}\] (34)

_and the matrices \(\mathbf{\tilde{S}}_{\lambda}\), \(\bm{\lambda}_{\perp}\), \(\mathbf{\tilde{H}}\), and \(\mathbf{\tilde{G}}\) are the diagonal matrices defined as:_

\[\mathbf{\tilde{S}}_{\lambda}=\sqrt{\mathbf{\tilde{S}}^{2}+\frac{ \lambda^{2}}{4}}\mathbf{I},\ \ \bm{\lambda}_{\perp}=\mathrm{sgn}(N_{o}-N_{i})\frac{\lambda}{2}\mathbf{I},\ \ \mathbf{\tilde{H}}= \mathrm{sgn}(\lambda)\sqrt{\frac{\mathbf{\tilde{S}}_{\lambda}-\mathbf{\tilde{ S}}}{\mathbf{\tilde{S}}_{\lambda}+\mathbf{\tilde{S}}}},\ \ \mathbf{\tilde{G}}=\frac{1}{\sqrt{ \mathbf{I}+\mathbf{\tilde{H}}^{2}}}.\] (35)

Beyond the invertibility of \(F\), notice from the equation (Fukumizu solution) we need to understand the relationship between \(F\) and \(Q(0)\). To do this the following lemma relates the structure between the SVD of the model with the SVD structure of the individual parameters.

Proof.: We leave for the reader by computing

\[\bm{F}=\bm{P}\bm{\Lambda}\bm{P}^{T}\] (36)

### Solution Unequal-Input-Output

**Theorem C.4**.: _Under the assumptions of whitened inputs, 2, lambda-balanced weights 3, no bottleneck 4, and full rank 5, the temporal dynamics of \(\mathbf{Q}\mathbf{Q}^{T}\) are_

\[\mathbf{Q}\mathbf{Q}^{T}(t)=\begin{pmatrix}Z_{1}\bm{A}^{-1}Z_{1}^{T}&Z_{1}\bm{ A}^{-1}Z_{2}^{T}\\ Z_{2}\bm{A}^{-1}Z_{1}^{T}&Z_{2}\bm{A}^{-1}Z_{2}^{T}\end{pmatrix},\]

_where the variables \(\bm{Z_{1}}\in\mathbb{R}^{N_{i}\times N_{h}}\), \(\bm{Z_{2}}\in\mathbb{R}^{N_{o}\times N_{h}}\), and \(\bm{A}\in\mathbb{R}^{N_{h}\times N_{h}}\) are defined as_

\[\bm{Z_{1}}(t)= \frac{1}{2}\mathbf{\tilde{V}}(\mathbf{\tilde{G}}-\mathbf{\tilde{H }}\mathbf{\tilde{G}})e^{\mathbf{\tilde{S}}_{\lambda}\frac{t}{\tau}}\bm{B}^{T }-\frac{1}{2}\mathbf{\tilde{V}}(\mathbf{\tilde{G}}+\mathbf{\tilde{H}}\mathbf{ \tilde{G}})e^{-\mathbf{\tilde{S}}_{\lambda}\frac{t}{\tau}}\bm{C}^{T}+\mathbf{ \tilde{V}}_{\perp}e^{\mathbf{\lambda}_{\perp}\frac{t}{\tau}}\mathbf{\tilde{V}} _{\perp}^{T}\mathbf{W}_{1}(0)^{T}\] (37) \[\bm{Z_{2}}(t)= \frac{1}{2}\mathbf{\tilde{U}}(\mathbf{\tilde{G}}+\mathbf{\tilde{ H}}\mathbf{\tilde{G}})e^{\mathbf{\tilde{S}}_{\lambda}\frac{t}{\tau}}\bm{B}^{T}+\frac{1}{2} \mathbf{\tilde{U}}(\mathbf{\tilde{G}}-\mathbf{\tilde{H}}\mathbf{\tilde{G}})e^{ -\mathbf{\tilde{S}}_{\lambda}\frac{t}{\tau}}\bm{C}^{T}+\mathbf{\tilde{U}}_{ \perp}e^{\mathbf{\lambda}_{\perp}\frac{t}{\tau}}\mathbf{\tilde{U}}_{\perp}^{T} \mathbf{W}_{2}(0)\] (38) \[\bm{A}(t)= \mathbf{I}+\bm{B}\left(\frac{e^{2\mathbf{\tilde{S}}_{\lambda} \frac{t}{\tau}}-\mathbf{I}}{4\mathbf{\tilde{S}}_{\lambda}}\right)\bm{B}^{T}- \bm{C}\left(\frac{e^{-2\mathbf{\tilde{S}}_{\lambda}\frac{t}{\tau}}-\mathbf{I}}{4 \mathbf{\tilde{S}}_{\lambda}}\right)\bm{C}^{T}+\mathbf{W}_{2}(0)^{T}\mathbf{ \tilde{U}}_{\perp}\left(\frac{e^{\mathbf{\lambda}_{\perp}\frac{t}{\tau}}- \mathbf{I}}{\bm{\lambda}_{\perp}}\right)\mathbf{\tilde{U}}_{\perp}^{T}\mathbf{W }_{2}(0)\] \[+\mathbf{W}_{1}(0)\mathbf{\tilde{V}}_{\perp}\left(\frac{e^{\mathbf{ \lambda}_{\perp}\frac{t}{\tau}}-\mathbf{I}}{\bm{\lambda}_{\perp}}\right) \mathbf{\tilde{V}}_{\perp}^{T}\mathbf{W}_{1}(0)^{T}\] (39)Proof.: We start and use the diagonalization of \(\mathbf{F}\) to rewrite the matrix exponential of \(\bm{F}\) and \(\bm{F}\). Note that \(\mathbf{P}^{T}\mathbf{P}=\mathbf{P}\mathbf{P}^{T}=\mathbf{I}\) and therefore \(\mathbf{P}^{T}=\mathbf{P}^{-1}\).

\[e^{\mathbf{F}\frac{t}{\tau}} =\mathbf{P}e^{\mathbf{F}\mathbf{P}^{T}}\] (40) \[=\frac{1}{\sqrt{2}}\begin{bmatrix}\hat{\mathbf{V}}(\tilde{\bm{G}} -\tilde{\bm{H}}\tilde{\bm{G}})&\hat{\mathbf{V}}(\tilde{\bm{G}}+\tilde{\bm{H}} \tilde{\bm{G}})&\sqrt{2}\mathbf{V}_{\perp}\\ \hat{\mathbf{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\hat{\mathbf{U} }(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})&\sqrt{2}\mathbf{V}_{\perp}\\ 0&e^{\bm{\Delta}_{\perp}\frac{t}{\tau}}\end{bmatrix}\begin{bmatrix}e^{\bm{ \mathcal{S}}_{\lambda}\frac{t}{\tau}}&0&0\\ 0&e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}&0\\ 0&e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\end{bmatrix}\begin{bmatrix} \tilde{\bm{\mathcal{S}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})}&\hat{ \mathbf{V}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&\sqrt{2}\mathbf{V}_{ \perp}\\ \sqrt{2}\mathbf{U}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\hat{\mathbf{ U}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{bmatrix}^{T}\] \[=\mathbf{O}e^{\bm{\Delta}_{\perp}^{T}}\mathbf{O}+2\mathbf{M}e^{ \bm{\Delta}_{\perp}\frac{t}{\tau}}\mathbf{M}^{T}.\]

\[e^{\mathbf{F}\frac{t}{\tau}}\mathbf{F}^{-1}e^{\mathbf{F}\frac{t}{\tau}}- \mathbf{F}^{-1} =\mathbf{O}e^{\bm{\Lambda}_{\perp}^{\frac{t}{\tau}}}\mathbf{O}^{T} \mathbf{O}\bm{\Lambda}^{-1}\mathbf{O}^{T}\mathbf{O}e^{\bm{\Lambda}_{\perp}^{ \frac{t}{\tau}}}\mathbf{O}^{T}-\mathbf{O}\bm{\Lambda}^{-1}\mathbf{O}^{T}+ \mathbf{M}(e^{\bm{\lambda}_{\perp}\frac{t}{\tau}}-\mathbf{I})(\bm{\lambda}_{ \perp})^{-1}\mathbf{M}^{T}.\] (41) \[\mathbf{F} =\mathbf{O}\bm{\Lambda}\mathbf{O}^{T}+2\mathbf{M}\bm{\lambda}_{ \perp}\mathbf{M}^{T}\] (42)

Where \(\bm{M}=\frac{1}{\sqrt{2}}\begin{bmatrix}\tilde{\mathbf{V}}_{\perp}\\ \tilde{\mathbf{U}}_{\perp}\end{bmatrix}^{T}\). Placing these expressions into equation 29 gives

\[\mathbf{Q}\mathbf{Q}^{T}(t)= \left[\mathbf{O}e^{\bm{\Lambda}_{\perp}^{\frac{t}{\tau}}}\mathbf{O }^{T}+2\mathbf{M}e^{\bm{\lambda}_{\perp}\frac{t}{\tau}}\mathbf{M}^{T}\right] \mathbf{Q}(0)\] (43) \[\mathbf{Q}(0)^{T}\left[\mathbf{O}e^{\bm{\Lambda}_{\perp}^{\frac{ t}{\tau}}}\mathbf{O}^{T}+2\mathbf{M}e^{\bm{\lambda}_{\perp}\frac{t}{\tau}} \mathbf{M}^{T}\right]^{T}\]

\[\bm{O}^{T}\bm{Q}(0)=\frac{1}{\sqrt{2}}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm {G}}-\tilde{\bm{H}}\tilde{\bm{G}})&\tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H }}\tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{pmatrix}^{T}\begin{pmatrix} \bm{W}_{1}^{T}(0)\\ \bm{W}_{2}(0)\end{pmatrix}\] \[=\frac{1}{\sqrt{2}}\begin{pmatrix}(\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})\tilde{\bm{V}}^{T}\bm{W}_{1}^{T}(0)+(\tilde{\bm{G}}+\tilde{\bm{H }}\tilde{\bm{G}})\tilde{\bm{U}}^{T}\bm{W}_{2}(0)\\ (\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})\tilde{\bm{V}}^{T}\bm{W}_{1}^{T}(0 )-(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\tilde{\bm{U}}^{T}\bm{W}_{2}(0 )\end{pmatrix}\] \[=\frac{1}{\sqrt{2}}\begin{pmatrix}\bm{B}^{T}\\ -\bm{C}^{T}\end{pmatrix}\] (44)

where

\[\mathbf{B} =\mathbf{W}_{2}(0)^{T}\tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H }}\tilde{\bm{G}})+\mathbf{W}_{1}(0)\tilde{\bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H }}\tilde{\bm{G}})\in\mathbb{R}^{N_{h}\times N_{h}}\] (45) \[\mathbf{C} =\mathbf{W}_{2}(0)^{T}\tilde{\bm{U}}(\tilde{\bm{G}}-\tilde{\bm{H }}\tilde{\bm{G}})-\mathbf{W}_{1}(0)\tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H }}\tilde{\bm{G}})\in\mathbb{R}^{N_{h}\times N_{h}}\] (46)

\[\bm{O}e^{\bm{\Lambda}t/\tau}=\frac{1}{\sqrt{2}}\begin{pmatrix}\tilde{\bm{V}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})&\tilde{\bm{V}}(\tilde{\bm{G}}+ \tilde{\bm{H}}\tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{pmatrix}\begin{pmatrix}e^{ \bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}&0\\ 0&e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\end{pmatrix}\]

\[=\frac{1}{\sqrt{2}}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})e^{\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}&\tilde{\bm{V}}( \tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{-\bm{\mathcal{S}}_{\lambda}\frac{t} {\tau}}\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{\bm{\mathcal{S}}_{ \lambda}\frac{t}{\tau}}&-\tilde{\bm{U}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G} })e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\end{pmatrix}\] (47)

\[\bm{O}e^{\bm{\Lambda}t/\tau}\bm{O}^{T}\bm{Q}(0)=\frac{1}{2}\begin{pmatrix}\tilde{ \bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})e^{\bm{\mathcal{S}}_{ \lambda}\frac{t}{\tau}}&\tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G} })e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{\bm{\mathcal{S}}_{ \lambda}\frac{t}{\tau}}&-\tilde{\bm{U}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G} })e^{-\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\end{pmatrix}\begin{pmatrix} \bm{B}^{T}\\ -\bm{C}^{T}\end{pmatrix}\]

\[=\frac{1}{2}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G} })e^{\bm{\mathcal{S}}_{\lambda}\frac{t}{\tau}}\bm{B}^{T}-\tilde{\bm{V}}( \tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{-\bm{\mathcal{S}}_{\lambda} \frac{t}{\tau}}\bm{C}^{T}\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{\bm{\mathcal{S}}_{ \lambda}\frac{t}{\tau}}\bm{B}^{T}+\tilde{\bm{U}}(\tilde{\bm{G}}-\tilde{\bm{H}}

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

#### c.5.1 Proof Exact learning dynamics with prior knowledge unequal dimension

We follow a similar derivation presented in Braun et al. (2022) and start with the following equation

\[\mathbf{Q}\mathbf{Q}^{T}(t)= \underbrace{\left[\mathbf{O}e^{\mathbf{A}\frac{t}{\tau}}\mathbf{O}^ {T}+2\mathbf{M}e^{\boldsymbol{\lambda}_{\perp}\frac{t}{\tau}}\mathbf{M}^{T} \right]\mathbf{Q}(0)}_{\mathbf{L}}\] \[\underbrace{\left[\mathbf{I}+\frac{1}{2}\mathbf{Q}(0)^{T}\left( \mathbf{O}\left(e^{2\mathbf{A}\frac{t}{\tau}}-\mathbf{I}\right)\boldsymbol{ \Lambda}^{-1}\mathbf{O}^{T}+\mathbf{M}(e^{\boldsymbol{\lambda}_{\perp}\frac{t }{\tau}}-\mathbf{I})\boldsymbol{\lambda}_{\perp}^{-1}\mathbf{M}^{T}\right) \mathbf{Q}(0)\right]^{-1}}_{\mathbf{C}^{-1}}\] (64) \[= \mathbf{L}\mathbf{C}^{-1}\mathbf{R},\] (65)

Substituting our solution into the matrix Riccati equation then yields

\[\tau\frac{d}{dt}\mathbf{Q}\mathbf{Q}^{T}=\mathbf{F}\mathbf{Q} \mathbf{Q}^{T}+\mathbf{Q}\mathbf{Q}^{T}\mathbf{F}-(\mathbf{Q}\mathbf{Q}^{T})^ {2}\] (66) \[\Rightarrow\tau\frac{d}{dt}\mathbf{L}\mathbf{C}^{-1}\mathbf{R} \overset{?}{=}\mathbf{F}\mathbf{L}\mathbf{C}^{-1}\mathbf{R}+\mathbf{L} \mathbf{C}^{-1}\mathbf{R}\mathbf{F}-\mathbf{L}\mathbf{C}^{-1}\mathbf{R} \mathbf{L}\mathbf{C}^{-1}\mathbf{R}.\] (67)

Using the chain rule \(\partial(\mathbf{A}\mathbf{B})=(\partial\mathbf{A})\mathbf{B}+\mathbf{A}( \partial\mathbf{B})\) and the identities

\[\frac{d}{dt}(\mathbf{A}^{-1})=\mathbf{A}^{-1}(\frac{d}{dt}\mathbf{A})\mathbf{ A}^{-1}\qquad\text{and}\qquad\frac{d}{dt}(e^{t\mathbf{A}})=\mathbf{A}e^{t \mathbf{A}}=e^{t\mathbf{A}}\mathbf{A}\] (68)

\[\tau\frac{d}{dt}\mathbf{Q}\mathbf{Q}^{T} =\tau\frac{d}{dt}\mathbf{L}\mathbf{C}^{-1}\mathbf{R}\] (69) \[=\tau\left(\frac{d}{dt}\mathbf{L}\right)\mathbf{C}^{-1}\mathbf{R }+\tau\mathbf{L}\left(\frac{d}{dt}C^{-1}\mathbf{R}\right)\] (70) \[=\tau\left(\frac{d}{dt}\mathbf{L}\right)\mathbf{C}^{-1}\mathbf{R }+\tau\mathbf{L}\mathbf{C}^{-1}\left(\frac{d}{dt}\mathbf{R}\right)+\tau \mathbf{L}\left(\frac{d}{dt}\mathbf{C}^{-1}\right)\mathbf{R},\] (71)

Next, we note that

\[\mathbf{O}=\frac{1}{\sqrt{2}}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}- \tilde{\bm{H}}\tilde{\bm{G}})&\tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H}} \tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{pmatrix}^{T}\frac{1}{\sqrt{2 }}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})& \tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{pmatrix}^{T}\frac{1}{\sqrt{2 }}\begin{pmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})& \tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{pmatrix}\] (73) \[=\mathbf{I}\] (74)

\[\mathbf{O}^{T}\mathbf{M} =\frac{1}{\sqrt{2}}\begin{bmatrix}\tilde{\bm{V}}(\tilde{\bm{G}}- \tilde{\bm{H}}\tilde{\bm{G}})&\tilde{\bm{V}}(\tilde{\bm{G}}+\tilde{\bm{H}} \tilde{\bm{G}})\\ \tilde{\bm{U}}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})&-\tilde{\bm{U}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\end{bmatrix}\frac{1}{\sqrt{2}} \begin{bmatrix}\tilde{\bm{V}}_{\perp}\\ \tilde{\bm{U}}_{\perp}\end{bmatrix}\] (75) \[=\frac{1}{2}\begin{bmatrix}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{ \bm{G}})^{T}\tilde{\bm{V}}^{T}\tilde{\bm{V}}_{\perp}+(\tilde{\bm{G}}+\tilde{ \bm{H}}\tilde{\bm{G}})^{T}\tilde{\bm{U}}^{T}\tilde{\bm{U}}_{\perp}\\ (\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})^{T}\tilde{\bm{V}}^{T}\tilde{\bm{ V}}_{\perp}-(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})^{T}\tilde{\bm{U}}^{T} \tilde{\bm{U}}_{\perp}\end{bmatrix}\] (76) \[=\mathbf{0}\] (77)and

\[\mathbf{M}^{T}\mathbf{O} =\frac{1}{\sqrt{2}}\left[\tilde{\mathbf{V}}_{\perp}^{T}\quad\tilde{ \mathbf{U}}_{\perp}^{T}\right]\left(\frac{\tilde{\mathbf{V}}(\tilde{\mathbf{G}}- \tilde{\mathbf{H}}\tilde{\mathbf{G}})}{\tilde{\mathbf{U}}(\tilde{\mathbf{G}}+ \tilde{\mathbf{H}}\tilde{\mathbf{G}})}-\tilde{\mathbf{U}}(\tilde{\mathbf{G}}- \tilde{\mathbf{H}}\tilde{\mathbf{G}})\right)\] (78) \[=\frac{1}{2}\left[\tilde{\mathbf{V}}_{\perp}^{T}\tilde{\mathbf{ V}}(\tilde{\mathbf{G}}-\tilde{\mathbf{H}}\tilde{\mathbf{G}})+\tilde{\mathbf{U}}_{ \perp}^{T}\tilde{\mathbf{U}}(\tilde{\mathbf{G}}+\tilde{\mathbf{H}}\tilde{ \mathbf{G}})\right]\] (79) \[=\mathbf{0}.\] (80)

we get

\[\tau\frac{d}{dt}\mathbf{Q}\mathbf{Q}^{T} =\tau\frac{d}{dt}\left(\mathbf{LC}^{-1}\mathbf{R}\right)\] (81) \[=\tau\left(\frac{d}{dt}\mathbf{L}\right)\mathbf{C}^{-1}\mathbf{R }+\tau\mathbf{L}\left(\frac{d}{dt}C^{-1}\mathbf{R}\right)\] (82) \[=\tau\left(\frac{d}{dt}\mathbf{L}\right)\mathbf{C}^{-1}\mathbf{R }+\tau\mathbf{LC}^{-1}\left(\frac{d}{dt}\mathbf{R}\right)+\tau\mathbf{L} \left(\frac{d}{dt}\mathbf{C}^{-1}\right)\mathbf{R},\] (83)

with

\[\tau\left(\frac{d}{dt}\mathbf{L}\right)\mathbf{C}^{-1}\mathbf{R} =\tau\left(\mathbf{O}\frac{1}{\tau}\mathbf{\Lambda}e^{\Lambda \frac{t}{\tau}}\mathbf{O}^{T}+2\mathbf{M}\frac{\lambda_{\perp}\mathbf{I}}{2 \tau}e^{\lambda_{\perp}\frac{t}{\tau}}\mathbf{M}^{T}\right)\mathbf{Q}(0) \mathbf{C}^{-1}\mathbf{R}\] (84) \[=\left(\mathbf{O}\mathbf{\Lambda}e^{\Lambda\frac{t}{\tau}} \mathbf{O}^{T}+\mathbf{M}\lambda_{\perp}\mathbf{I}e^{\lambda_{\perp}\frac{t}{ \tau}}\mathbf{M}^{T}\right)\mathbf{Q}(0)\mathbf{C}^{-1}\mathbf{R}\] (85) \[=\left(\mathbf{O}\lambda_{\perp}\mathbf{O}^{T}+2\mathbf{M} \mathbf{\lambda}_{\perp}\mathbf{M}^{T}\right)\left(\mathbf{O}e^{\Lambda\frac {t}{\tau}}\mathbf{O}^{T}+2\mathbf{M}e^{\lambda_{\perp}\frac{t}{\tau}}\mathbf{ M}^{T}\right)\mathbf{Q}(0)\mathbf{C}^{-1}\mathbf{R}\] (86) \[=\mathbf{FLC}^{-1}\mathbf{R},\] (87)

\[\tau\mathbf{LC}^{-1}\left(\frac{d}{dt}\mathbf{R}\right) =\tau\mathbf{LC}^{-1}\mathbf{Q}(0)^{T}\left(\mathbf{O}\frac{1}{ \tau}e^{\Lambda\frac{t}{\tau}}\mathbf{\Lambda}\mathbf{O}^{T}+2\mathbf{M}e^{ \lambda_{\perp}\frac{t}{\tau}}\frac{\lambda_{\perp}\mathbf{I}}{2\tau}\mathbf{ M}^{T}\right)\] (88) \[=\mathbf{LC}^{-1}\mathbf{Q}(0)^{T}\left(\mathbf{O}\frac{1}{\tau} e^{\Lambda\frac{t}{\tau}}\mathbf{\Lambda}\mathbf{O}^{T}+2\mathbf{M}e^{\lambda_{ \perp}\frac{t}{\tau}}\frac{\lambda_{\perp}\mathbf{I}}{2\tau}\mathbf{M}^{T}\right)\] (89) \[=\mathbf{LC}^{-1}\mathbf{RF}\] (90)

and

\[\tau\mathbf{L}\left(\frac{d}{dt}\mathbf{C}^{-1}\right)\mathbf{R} =-\tau\mathbf{LC}^{-1}\left(\frac{d}{dt}\mathbf{C}\right) \mathbf{C}^{-1}\mathbf{R}\] (91) \[=-\mathbf{LC}^{-1}\bigg{[}\tau\frac{1}{2}\mathbf{Q}(0)^{T} \mathbf{O}2\frac{1}{\tau}e^{2\mathbf{\Lambda}\frac{t}{\tau}}\mathbf{\Lambda} \mathbf{\Lambda}^{-1}\mathbf{O}^{T}\mathbf{Q}(0)\] (92) \[\qquad\qquad+\tau\frac{1}{2}\mathbf{Q}(0)^{T}4\frac{1}{\tau} \mathbf{M}e^{\lambda_{\perp}\frac{t}{\tau}}\mathbf{\lambda}_{\perp}\left( \mathbf{\lambda}_{\perp}\right)^{-1}\mathbf{M}^{T}\mathbf{Q}(0)\bigg{]}\mathbf{ C}^{-1}\mathbf{R}\] \[=-\mathbf{LC}^{-1}\bigg{[}\mathbf{Q}(0)^{T}\mathbf{O}e^{\Lambda \frac{t}{\tau}}\mathbf{O}^{T}\mathbf{Q}(0)+2\mathbf{Q}(0)^{T}\mathbf{M}e^{ \lambda_{\perp}\frac{t}{\tau}}\mathbf{M}^{T}\mathbf{Q}(0)\bigg{]}\mathbf{C}^{- 1}\mathbf{R}\] (93) \[=-\mathbf{LC}^{-1}\bigg{[}\mathbf{Q}(0)^{T}\mathbf{O}e^{\Lambda \frac{t}{\tau}}\mathbf{O}^{T}\mathbf{O}e^{\Lambda\frac{t}{\tau}}\mathbf{O}^{T} \mathbf{Q}(0)\] \[\qquad\qquad\qquad+2\mathbf{Q}(0)^{T}\mathbf{O}e^{\Lambda\frac{t} {\tau}}\underbrace{\mathbf{O}^{T}\mathbf{M}}_{\mathbf{0}}e^{\lambda_{\perp} \frac{t}{\tau}}\mathbf{M}^{T}\mathbf{Q}(0)\] (94) \[\qquad\qquad\qquad+2\mathbf{Q}(0)^{T}\mathbf{M}e^{\lambda_{\perp} \frac{t}{\tau}}\mathbf{M}^{T}\mathbf{O}e^{\Lambda\frac{t}{\tau}}\mathbf{O}^{T} \mathbf{Q}(0)\] \[\qquad\qquad\qquad+4\mathbf{Q}(0)^{T}\mathbf{M}e^{\lambda_{\perp} \frac{t}{\tau}}\mathbf{M}^{T}\mathbf{M}e^{\lambda_{\perp}\frac{t}{\tau}}\mathbf{ M}^{T}\mathbf{Q}(0)\bigg{]}\mathbf{C}^{-1}\mathbf{R}\] \[=-\mathbf{LC}^{-1}\mathbf{RLC}^{-1}\mathbf{R}.\] (95)Finally, substituting equations 84, 88 and 91 into the left hand side of equation 67 proves equality. \(\square\)

## Appendix D Rich-Lazy

### Dynamics of the Singular Values

**Theorem D.1**.: _Under the assumptions of Theorem C.4 and with a task-aligned initialization given by \(\bm{W}_{1}(0)=\bm{R}\bm{S}_{1}\tilde{\bm{V}}^{T}\) and \(\bm{W}_{2}(0)=\tilde{\bm{U}}\bm{S}_{2}\bm{R}^{T}\), where \(\bm{R}\in\mathbb{R}^{N_{h}\times N_{h}}\) is an orthonormal matrix, then the network function is given by the expression \(\bm{W}_{2}\bm{W}_{1}(t)=\tilde{\bm{U}}\bm{S}(t)\tilde{\bm{V}}^{T}\) where \(\bm{S}(t)\in\mathbb{R}^{N_{h}\times N_{h}}\) is a diagonal matrix of singular values with elements \(s_{\alpha}(t)\) that evolve according to the equation,_

\[s_{\alpha}(t)=s_{\alpha}(0)+\gamma_{\alpha}(t;\lambda)\left(\tilde{s}_{\alpha} -s_{\alpha}(0)\right),\] (96)

_where \(\tilde{s}_{\alpha}\) is the \(\alpha\) singular value of \(\tilde{\bm{S}}\) and \(\gamma_{\alpha}(t;\lambda)\) is a \(\lambda\)-dependent monotonic transition function for each singular value that increases from \(\gamma_{\alpha}(0;\lambda)=0\) to \(\lim_{t\to\infty}\gamma_{\alpha}(t;\lambda)=1\) defined as_

\[\gamma_{\alpha}(t;\lambda)=\frac{\tilde{s}_{\lambda,\alpha}s_{\lambda,\alpha} \sinh\left(2\tilde{s}_{\lambda,\alpha}\frac{t}{\tau}\right)+\left(\tilde{s}_ {\alpha}s_{\alpha}+\frac{\lambda^{2}}{4}\right)\cosh\left(2\tilde{s}_{\lambda, \alpha}\frac{t}{\tau}\right)-\left(\tilde{s}_{\alpha}s_{\alpha}+\frac{\lambda ^{2}}{4}\right)}{\tilde{s}_{\lambda,\alpha}s_{\lambda,\alpha}\sinh\left(2 \tilde{s}_{\lambda,\alpha}\frac{t}{\tau}\right)+\left(\tilde{s}_{\alpha}s_{ \alpha}+\frac{\lambda^{2}}{4}\right)\cosh\left(2\tilde{s}_{\lambda,\alpha} \frac{t}{\tau}\right)+\tilde{s}_{\alpha}\left(\tilde{s}_{\alpha}-s_{\alpha} \right)},\] (97)

_where \(\tilde{s}_{\lambda,\alpha}=\sqrt{\tilde{s}_{\alpha}^{2}+\frac{\lambda^{2}}{4}}\), \(s_{\lambda,\alpha}=\sqrt{s_{\alpha}(0)^{2}+\frac{\lambda^{2}}{4}}\), and \(s_{\alpha}=s_{\alpha}(0)\). We find that under different limits of \(\lambda\), the transition function converges pointwise to the sigmoidal (\(\lambda\to 0\)) and exponential (\(\lambda\to\pm\infty\)) transition functions,_

\[\gamma_{\alpha}(t;\lambda)\to\begin{cases}\frac{e^{2\tilde{s}_{\alpha}\frac{t }{\tau}}-1}{e^{2\tilde{s}_{\alpha}\frac{t}{\tau}}-1+\frac{\tilde{s}_{\alpha}(0 )}{4}}&\text{as }\lambda\to 0,\\ 1-e^{-|\lambda|\frac{t}{\tau}}&\text{as }\lambda\to\pm\infty\end{cases}.\] (98)

Proof.: According to Theorem C.4, the network function is given by the equation

\[\bm{W}_{2}\bm{W}_{1}(t)=\bm{Z}_{2}(t)\bm{A}^{-1}(t)\bm{Z}_{1}^{T}(t),\] (99)

which depends on the variables of the initialization \(\bm{B}\) and \(\bm{C}\). Plugging the expressions for a task-aligned initialization \(\bm{W}_{1}(0)\) and \(\bm{W}_{2}(0)\) into these variables we get the following simplified expressions,

\[\bm{B} =\bm{R}\underbrace{\left(\bm{S}_{2}(\tilde{\bm{G}}+\tilde{\bm{H}} \tilde{\bm{G}})+\bm{S}_{1}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\right) }_{\bm{D}_{B}},\] (100) \[\bm{C} =\bm{R}\underbrace{\left(\bm{S}_{2}(\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})-\bm{S}_{1}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})\right) }_{\bm{D}_{C}},\] (101)

where we define the diagonal matrices \(\bm{D}_{B}\) and \(\bm{D}_{C}\) for ease of notation. Using these expressions, we now get the following time-dependent expressions for \(\bm{Z}_{2}(t)\), \(\bm{A}^{-1}(t)\), and \(\tilde{\bm{Z}}_{1}(t)\),

\[\bm{Z}_{1}(t) =\frac{1}{2}\tilde{\bm{V}}\left((\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})e^{\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{B}-(\tilde{\bm{G}}+ \tilde{\bm{H}}\tilde{\bm{G}})e^{-\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{C} \right)\bm{R}^{T}\] (102) \[\bm{Z}_{2}(t) =\frac{1}{2}\tilde{\bm{U}}\left((\tilde{\bm{G}}+\tilde{\bm{H}} \tilde{\bm{G}})e^{\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{B}+(\tilde{\bm{G}}- \tilde{\bm{H}}\tilde{\bm{G}})e^{-\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{C} \right)\bm{R}^{T}\] (103) \[\bm{A}(t) =\bm{R}\left(\bm{\mathrm{I}}+\left(\frac{e^{2\bm{S}_{\lambda}\frac {t}{\tau}}-\bm{\mathrm{I}}}{4\tilde{\bm{S}}_{\lambda}}\right)\bm{D}_{B}^{2}- \left(\frac{e^{-2\bm{S}_{\lambda}\frac{t}{\tau}}-\bm{\mathrm{I}}}{4\tilde{\bm{ S}}_{\lambda}}\right)\bm{D}_{C}^{2}\right)\bm{R}^{T}\] (104)

Plugging these expressions into the expression for the network function, notice that the \(\bm{R}\) terms cancel each other resulting in following equation

\[\bm{W}_{2}\bm{W}_{1}(t)=\tilde{\bm{U}}\underbrace{\left(\frac{\left((\tilde{ \bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})e^{\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{ B}-(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{-\bm{S}_{\lambda}\frac{t}{ \tau}}\bm{D}_{C}\right)\left((\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})e^{ \bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{B}+(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{ \bm{G}})e^{-\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{C}\right)}{4\bm{\mathrm{I}}+ \left(\frac{e^{2\bm{S}_{\lambda}\frac{t}{\tau}}-\bm{\mathrm{I}}}{\bm{S}_{ \lambda}}\right)\bm{D}_{B}^{2}-\left(\frac{e^{-2\bm{S}_{\lambda}\frac{t}{\tau}}- \bm{\mathrm{I}}}{\bm{S}_{\lambda}}\right)\bm{D}_{C}^{2}}\right)\tilde{\bm{V}}^{T},\] (105)Notice that the middle term is simply a product of diagonal matrices. We can factor the numerator of this expressions as,

\[(\tilde{\bm{G}}^{2}-\tilde{\bm{H}}^{2}\tilde{\bm{G}}^{2})e^{2\bm{S}_{\lambda}\frac {t}{\tau}}\bm{D}_{B}^{2}+\left((\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})^{2} -(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})^{2}\right)\bm{D}_{B}\bm{D}_{C}-( \tilde{\bm{G}}^{2}-\tilde{\bm{H}}^{2}\tilde{\bm{G}}^{2})e^{-2\bm{S}_{\lambda} \frac{t}{\tau}}\bm{D}_{C}^{2}\] (106)

We can further factor this expression as,

\[\tilde{\bm{G}}^{2}(\mathbf{I}-\tilde{\bm{H}}^{2})\left(e^{2\tilde{\bm{S}}_{ \lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda}\frac{t}{ \tau}}\bm{D}_{C}^{2}\right)-4\tilde{\bm{G}}^{2}\tilde{\bm{H}}\bm{D}_{B}\bm{D}_{C}.\] (107)

Putting it all together we find that \(\bm{S}(t)\) can be expressed as,

\[\bm{S}(t)=\frac{\tilde{\bm{G}}^{2}(\mathbf{I}-\tilde{\bm{H}}^{2})\left(e^{2 \bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda} \frac{t}{\tau}}\bm{D}_{C}^{2}\right)-4\tilde{\bm{G}}^{2}\tilde{\bm{H}}\bm{D}_{ B}\bm{D}_{C}}{4\mathbf{I}+\left(\frac{e^{2\tilde{\bm{S}}_{\lambda}\frac{t}{\tau}}- \mathbf{I}}{\bm{S}_{\lambda}}\right)\bm{D}_{B}^{2}-\left(\frac{e^{-2\tilde{\bm {S}}_{\lambda}\frac{t}{\tau}}-\mathbf{I}}{\bm{S}_{\lambda}}\right)\bm{D}_{C}^{ 2}}.\] (108)

Now using the relationship between \(\tilde{\bm{H}}\) and \(\tilde{\bm{G}}\) we use the following two identities:

\[\tilde{\bm{G}}^{2}(\mathbf{I}-\tilde{\bm{H}}^{2})=\frac{\tilde{\bm{S}}}{\tilde {\bm{S}}_{\lambda}},\qquad 4\tilde{\bm{G}}^{2}\tilde{\bm{H}}=\frac{\lambda}{ \tilde{\bm{S}}_{\lambda}}\] (109)

Plugging these identities into the previous expression and multiplying the numerator and denominator by \(\tilde{\bm{S}}_{\lambda}\) gives,

\[\bm{S}(t)=\frac{\tilde{\bm{S}}\left(e^{2\tilde{\bm{S}}_{\lambda}\frac{t}{ \tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda}\frac{t}{\tau}}\bm{D}_{C}^{ 2}\right)-\lambda\bm{D}_{B}\bm{D}_{C}}{4\tilde{\bm{S}}_{\lambda}+e^{2\bm{S}_{ \lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\bm{S}_{\lambda}\frac{t}{\tau}}\bm {D}_{C}^{2}+\bm{D}_{C}^{2}-\bm{D}_{B}^{2}}.\] (110)

Add and subtract \(\tilde{\bm{S}}\left(4\tilde{\bm{S}}_{\lambda}+\bm{D}_{C}^{2}-\bm{D}_{B}^{2}\right)\) from the numerator such that

\[\bm{S}(t)=\tilde{\bm{S}}-\frac{\tilde{\bm{S}}\left(4\tilde{\bm{S}}_{\lambda}+ \bm{D}_{C}^{2}-\bm{D}_{B}^{2}\right)+\lambda\bm{D}_{B}\bm{D}_{C}}{4\tilde{\bm{ S}}_{\lambda}+e^{2\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\bm{S}_{ \lambda}\frac{t}{\tau}}\bm{D}_{C}^{2}+\bm{D}_{C}^{2}-\bm{D}_{B}^{2}}.\] (111)

Using the form of \(\bm{D}_{B}\) and \(\bm{D}_{C}\) notice the following two identities:

\[\bm{D}_{B}\bm{D}_{C}=\frac{\lambda}{\tilde{\bm{S}}_{\lambda}}\left(\tilde{\bm {S}}-\bm{S}_{2}\bm{S}_{1}\right),\qquad\bm{D}_{C}^{2}-\bm{D}_{B}^{2}=-\frac{4}{ \tilde{\bm{S}}_{\lambda}}\left(\tilde{\bm{S}}\bm{S}_{2}\bm{S}_{1}+\frac{ \lambda^{2}}{4}\mathbf{I}\right)\] (112)

From the second identity we can derive a third identity,

\[4\tilde{\bm{S}}_{\lambda}+\bm{D}_{C}^{2}-\bm{D}_{B}^{2}=4\frac{\tilde{\bm{S}}} {\tilde{\bm{S}}_{\lambda}}\left(\tilde{\bm{S}}-\bm{S}_{2}\bm{S}_{1}\right)\] (113)

Plugging the first and third identities into the numerator for the previous expression gives,

\[\bm{S}(t)=\tilde{\bm{S}}-\frac{\frac{\left(4\bm{S}^{2}+\lambda^{2}\mathbf{I} \right)}{\tilde{\bm{S}}_{\lambda}}\left(\tilde{\bm{S}}-\bm{S}_{2}\bm{S}_{1} \right)}{4\tilde{\bm{S}}_{\lambda}+e^{2\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_ {B}^{2}-e^{-2\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{C}^{2}+\bm{D}_{C}^{2}-\bm {D}_{B}^{2}}.\] (114)

Multiply numerator and denominator by \(\frac{\tilde{\bm{S}}_{\lambda}}{4}\) and simplify terms gives the expression,

\[\bm{S}(t)=\tilde{\bm{S}}-\frac{\tilde{\bm{S}_{\lambda}}^{2}}{\tilde{\bm{S}_{ \lambda}}^{2}+\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(e^{2\bm{S}_{\lambda}\frac {t}{\tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda}\frac{t}{\tau}}\bm{D}_{C}^{ 2}\right)-\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(\bm{D}_{B}^{2}-\bm{D}_{C}^{2} \right)}\left(\tilde{\bm{S}}-\bm{S}_{2}\bm{S}_{1}\right).\] (115)

Thus we have found the transition function,

\[\gamma(t;\lambda)=\frac{\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(e^{2\tilde{\bm{S} }_{\lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda}\frac{t}{ \tau}}\bm{D}_{C}^{2}\right)+\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(\bm{D}_{C}^{ 2}-\bm{D}_{B}^{2}\right)}{\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(e^{2\bm{S}_{ \lambda}\frac{t}{\tau}}\bm{D}_{B}^{2}-e^{-2\tilde{\bm{S}}_{\lambda}\frac{t}{ \tau}}\bm{D}_{C}^{2}\right)+\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(4\tilde{\bm{S} }_{\lambda}+\bm{D}_{C}^{2}-\bm{D}_{B}^{2}\right)}.\] (116)We will use our previous identities and the definitions of \(\bm{D}_{B}^{2}\) and \(\bm{D}_{C}^{2}\) to simplify this expression. Notice the following identity,

\[\frac{\tilde{\bm{S}}_{\lambda}}{4}\left(e^{2\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D} _{B}^{2}-e^{-2\bm{S}_{\lambda}\frac{t}{\tau}}\bm{D}_{C}^{2}\right)=\tilde{\bm{S} }_{\lambda}\bm{S}_{\lambda}\sinh\left(2\tilde{\bm{S}}_{\lambda}\frac{t}{\tau} \right)+\left(\tilde{\bm{S}}\bm{S}(0)+\frac{\lambda^{2}}{4}\mathbf{I}\right) \cosh\left(2\tilde{\bm{S}}_{\lambda}\frac{t}{\tau}\right)\] (117)

Putting it all together we get

\[\gamma(t;\lambda)=\frac{\tilde{\bm{S}}_{\lambda}\bm{S}_{\lambda}\sinh\left(2 \tilde{\bm{S}}_{\lambda}\frac{t}{\tau}\right)+\left(\tilde{\bm{S}}\bm{S}(0)+ \frac{\lambda^{2}}{4}\mathbf{I}\right)\cosh\left(2\tilde{\bm{S}}_{\lambda} \frac{t}{\tau}\right)-\left(\tilde{\bm{S}}\bm{S}(0)+\frac{\lambda^{2}}{4} \mathbf{I}\right)}{\tilde{\bm{S}}_{\lambda}\bm{S}_{\lambda}\sinh\left(2\tilde {\bm{S}}_{\lambda}\frac{t}{\tau}\right)+\left(\tilde{\bm{S}}\bm{S}(0)+\frac{ \lambda^{2}}{4}\mathbf{I}\right)\cosh\left(2\tilde{\bm{S}}_{\lambda}\frac{t}{ \tau}\right)+\tilde{\bm{S}}\left(\tilde{\bm{S}}-\bm{S}(0)\right)}\] (118)

We will now show why under certain limits of \(\lambda\) this expression simplifies to the sigmoidal and exponential dynamics discussed in the previous section.

**Sigmoidal dynamics.** When \(\lambda=0\), then \(\tilde{\bm{S}}_{\lambda}=\tilde{\bm{S}}\) and \(\bm{S}_{\lambda}=\bm{S}(0)\). Notice, that the coefficients for the hyperbolic functions all simplify to \(\tilde{\bm{S}}\bm{S}(0)\). Using the hyperbolic identity \(\sinh(x)+\cosh(x)=e^{x}\), we can simplify the expression for the transition function to

\[\gamma(t;\lambda)=\frac{\tilde{\bm{S}}\bm{S}(0)e^{2\tilde{\bm{S}}\frac{t}{\tau }}-\tilde{\bm{S}}\bm{S}(0)}{\tilde{\bm{S}}\bm{S}(0)e^{2\tilde{\bm{S}}\frac{t}{ \tau}}-\tilde{\bm{S}}\bm{S}(0)+\tilde{\bm{S}}^{2}}.\] (119)

Dividing the numerator and denominator by \(\tilde{\bm{S}}\bm{S}(0)\) gives the final expression.

**Exponential dynamics.** In the limit as \(\lambda\to\pm\infty\) the expressions \(\tilde{\bm{S}}_{\lambda}\to\frac{|\lambda|}{2}\) and \(\bm{S}_{\lambda}\to\frac{|\lambda|}{2}\). Additionally, in these limits because \(\frac{\lambda^{2}}{4}\mathbf{I}\gg\tilde{\bm{S}}\bm{S}(0)\) then \(\left(\tilde{\bm{S}}\bm{S}(0)+\frac{\lambda^{2}}{4}\mathbf{I}\right)\to\frac{ \lambda^{2}}{4}\mathbf{I}\). As a result of these simplifications the coefficients for the hyperbolic functions all simplify to \(\frac{\lambda^{2}}{4}\mathbf{I}\). As a result we can again use the hyperbolic identity \(\sinh(x)+\cosh(x)=e^{x}\) to simplify the expression as

\[\gamma(t;\lambda)=\frac{\frac{\lambda^{2}}{4}e^{|\lambda|\frac{t}{\tau}}-\frac {\lambda^{2}}{4}\mathbf{I}}{\frac{\lambda^{2}}{4}e^{|\lambda|\frac{t}{\tau}}+ \tilde{\bm{S}}\left(\tilde{\bm{S}}-\bm{S}(0)\right)}.\] (120)

Dividing the numerator and denominator by \(\frac{\lambda^{2}}{4}\) results in all terms without a coefficient proportional to \(\lambda^{2}\) vanishing, which simplifying further gives the final expression. 

### Dynamics of the representation from the Lazy to the Rich Regime

The _lazy_ and _rich_ regimes are defined by the dynamics of the NTK of the network. _Lazy_ learning occurs when the NTK is constant, _rich_ learning occurs when it is not. (Farrell et al. (2023b))

The NTK intuitively measures the movement of the network representations through training. As shown in (Braun et al. (2022)), in specific experimental setup, we can calculate the NTK of the network in terms of the internal representations in a straightforward way:

\[\mathrm{NTK}=\mathbf{I}_{N_{o}}\otimes\mathbf{X}^{T}\mathbf{W}_{1}^{T}\mathbf{ W}_{1}(t)\mathbf{X}+\mathbf{W}_{2}\mathbf{W}_{2}^{T}(t)\otimes\mathbf{X}^{T} \mathbf{X}\] (121)

In order to better understand the effect of \(\lambda\) on NTK dynamics, we first prove some theorems involving the Singular Values of the \(\lambda\)_-balanced_ weights, and the representations of a \(\lambda\)_-balanced_ network.

Figure 4: Simulated and analytical dynamics of the singular values of the network function with _relative scale_ lambda \(\mathbf{A}\)\(\lambda=-2\)\(\mathbf{B}\)\(\lambda=0\)\(\mathbf{C}\)\(\lambda=2\) initialized as described in F.7.

#### d.2.1 Lambda-balanced singular value

**Theorem D.2**.: _Under a \(\lambda\)-Balanced initialization 3, if the network function \(\bm{W}_{2}\bm{W}_{1}(t)=\bm{U}(t)\bm{S}(t)\bm{V}^{T}(t)\) is full-rank 5 and we define \(\bm{S}_{\bm{\lambda}}(t)=\sqrt{\bm{S}^{2}(t)+\frac{\lambda^{2}}{4}\mathbf{I}}\), then we can recover the parameters \(\bm{W}_{2}(t)=\bm{U}(t)\bm{S}_{2}(t)\bm{R}^{T}(t)\), \(\bm{W}_{1}(t)=\bm{R}(t)\bm{S}_{1}(t)\bm{V}^{T}(t)\) up to time-dependent orthogonal transformation \(\bm{R}(t)\) of size \(N_{h}\times N_{h}\), where_

\[\bm{S}_{1}(t)=\left(\left(\bm{S}_{\bm{\lambda}}(t)-\frac{\lambda \mathbf{I}}{2}\right)^{\frac{1}{2}}\quad 0_{\max(0,N_{i}-N_{o})}\right)\qquad\bm{S}_{2}(t)= \ \left(\left(\bm{S}_{\bm{\lambda}}(t)+\frac{\lambda\mathbf{I}}{2}\right)^{\frac {1}{2}}0_{\max(0,N_{o}-N_{i})}\right)\] (122)

Proof.: We prove the case \(N_{i}\leq N_{o}\) and \(N_{h}=min(N_{i},N_{o})\). The proof for \(N_{o}\leq N_{i}\) follows the same structure. Let \(\bm{U}\bm{S}\bm{V}^{T}=\bm{W}_{2}(t)\bm{W}_{1}(t)\) be the Singular Value Decomposition of the product of the weights at training step \(t\). We will use \(\bm{W}_{2}=\bm{W}_{2}(t),\bm{W}_{1}=\bm{W}_{1}(t)\) as a shorthand.

By properties of Singular Value Decomposition, we can write \(\bm{W}_{2}=\bm{U}\bm{S}_{2}\bm{R}^{T},\bm{W}_{1}=\bm{R}\bm{S}_{1}\bm{V}^{T}\), where \(\bm{R}\) is an orthonormal matrix and \(\bm{S}_{2},\bm{S}_{1}\) are diagonal (possibly rectangular) matrices.

The Balanced property states that \(\bm{W}_{2}^{T}\bm{W}_{2}-\bm{W}_{1}\bm{W}_{1}^{T}=\lambda\mathbf{I}\). We know this holds for any \(t\) since this is a conserved quantity in linear networks.

Hence

\[\bm{R}\bm{S}_{2}^{T}\bm{S}_{2}\bm{R}^{T}-\bm{R}\bm{S}_{1}\bm{S}_{1}\bm{R}^{T} =\lambda\mathbf{I}\] (123)

\[\bm{S}_{2}^{T}\bm{S}_{2}-\bm{S}_{1}\bm{S}_{1}=\lambda\mathbf{I}\] (124)

The matrices \(\bm{S}_{1},\bm{S}_{2},\) have shapes \((N_{h},N_{i})\), \((N_{o},N_{h})\) respectively. We introduce the diagonal matrices \(\bm{\hat{S}}_{1}\) of shape \((N_{h},N_{i})\), \(\bm{\hat{S}}_{2}\) of shape \((N_{i},N_{h})\) such that the zero matrix has size \((N_{o}-N_{i},N_{h})\) :

\[\bm{S}_{1}=\left(\bm{\hat{S}}_{1}\right),\quad\bm{S}_{2}=\begin{pmatrix}\bm{ \hat{S}}_{2}\\ 0\end{pmatrix}\] (125)

Hence

\[\bm{S}_{2}^{T}\bm{S}_{2}-\bm{S}_{1}\bm{S}_{1}=\lambda\mathbf{I}\] (126)

From the equation above and the fact that \(\bm{\hat{S}}_{1}\bm{\hat{S}}_{2}=\bm{S}\) we derive that:

\[\bm{\hat{S}}_{2}=\left(\frac{\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{2}}+\lambda \mathbf{I}}{2}\right)^{\frac{1}{2}},\quad\bm{\hat{S}}_{1}=\left(\frac{\sqrt{ \lambda^{2}\mathbf{I}+4\bm{S}^{2}}-\lambda\mathbf{I}}{2}\right)^{\frac{1}{2}},\] (127)

Hence

\[\bm{W}_{2}=\bm{U}\left(\begin{pmatrix}\frac{\sqrt{\lambda^{2}\mathbf{I}+4 \bm{S}^{2}}+\lambda\mathbf{I}}{2}\\ 0_{\max(0,N_{e}-N_{i})}\end{pmatrix}^{\frac{1}{2}},\bm{R}^{T},\quad\bm{W}_{1}= \bm{R}\left(\begin{pmatrix}\frac{\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{2}}- \lambda\mathbf{I}}{2}\end{pmatrix}^{\frac{1}{2}}&0_{\max(0,N_{i}-N_{o})} \right)\bm{V}^{T}\] (128)

#### d.2.2 Convergence proof

With our solution, \(\mathbf{QQ}^{T}(t)\), which captures the temporal dynamics of the similarity between hidden layer activations, we can analyze the network's internal representations in relation to the task. This allows us to determine whether the network adopts a _rich_ or _lazy_ representation, depending on the value of \(\lambda\). Consider a \(\lambda\)-Balanced network training on data \(\bm{\Sigma}^{yx}=\tilde{\bm{U}}\tilde{\bm{S}}\tilde{\bm{V}}^{T}\). We assume that the convergence is toward global minima and B is invertible

**Theorem D.3**.: _Under the assumptions of Theorem C.5, the network function converges to \(\tilde{\mathbf{U}}\tilde{\mathbf{S}}\tilde{\mathbf{V}}^{T}\) and acquires the internal representation, that is \(\mathbf{W}_{1}^{T}\mathbf{W}_{1}=\tilde{\mathbf{V}}\tilde{\mathbf{S}}_{1}^{2} \tilde{\mathbf{V}}^{T}\) and \(\mathbf{W}_{2}\mathbf{W}_{2}^{T}=\tilde{\mathbf{U}}\tilde{\mathbf{S}}_{2}^{2} \tilde{\mathbf{U}}^{T}\)_

Proof.: As training time increases, all terms including a matrix exponential with negative exponent in Equation 58 vanish to zero, as \(\bm{S}_{\bm{\lambda}}=\tilde{\bm{S}_{\lambda}}\) is a diagonal matrix with entries larger zero

As training time increases, all terms in the equations vanish to zero. Terms in Equation 58 decay as

\[\lim_{t\to\infty}e^{-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}}\frac{t}{ \tilde{\tau}}}=\bm{0}.\] (129)

and

\[\lim_{t\to\infty}e^{\lambda_{\perp}\frac{t}{\tilde{\tau}}}e^{-\sqrt{\tilde{ \bm{S}}^{2}+\frac{\lambda^{2}}{4}}\frac{t}{\tilde{\tau}}}=\bm{0}.\] (130)

where \(\tilde{\bm{S}_{\bm{\lambda}}}=\tilde{\bm{S}_{\lambda}}\) is a diagonal matrix with entries larger zero

Therefore, in the temporal limit, eq. 58 reduces to

\[\lim_{t\to\infty}\mathbf{QQ}^{T}(t) =\lim_{t\to\infty}\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1} (t)&\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}(t)\\ \mathbf{W}_{2}\mathbf{W}_{1}(t)&\mathbf{W}_{2}^{T}\mathbf{W}_{2}(t)\end{bmatrix}\] (131) \[=\begin{bmatrix}\tilde{\mathbf{V}}(\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})\\ \tilde{\mathbf{U}}(\tilde{\bm{H}}\tilde{\bm{G}}+\tilde{\bm{G}})\end{bmatrix} \left[\tilde{\bm{S}_{\lambda}}^{-1}\right]^{-1}\left[(\tilde{\mathbf{V}}( \tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}}))^{T}\quad(\tilde{\mathbf{U}}( \tilde{\bm{H}}\tilde{\bm{G}}+\tilde{\bm{G}}))^{T}\right]\] (132) \[=\begin{bmatrix}\tilde{\mathbf{V}}(\tilde{\bm{G}}-\tilde{\bm{H}} \tilde{\bm{G}})\tilde{\bm{S}}_{\lambda}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{ \bm{G}})^{T}\tilde{\mathbf{V}}^{T}&\tilde{\mathbf{V}}(\tilde{\bm{G}}-\tilde{ \bm{H}}\tilde{\bm{G}})\tilde{\bm{S}}_{\lambda}(\tilde{\bm{H}}\tilde{\bm{G}}+ \tilde{\bm{G}})^{T}\tilde{\mathbf{U}}^{T}\\ \tilde{\mathbf{U}}(\tilde{\bm{H}}\tilde{\bm{G}}+\tilde{\bm{G}})\tilde{\bm{S}}_ {\lambda}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})^{T}\tilde{\mathbf{V}} ^{T}&\tilde{\mathbf{U}}(\tilde{\bm{H}}\tilde{\bm{G}}+\tilde{\bm{G}})\tilde{ \bm{S}}_{\lambda}(\tilde{\bm{H}}\tilde{\bm{G}}+\tilde{\bm{G}})^{T}\tilde{ \mathbf{U}}^{T}\end{bmatrix}.\] (133)

\[(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})\tilde{\bm{S}}_{\bm{\lambda}}( \tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})=\frac{\bm{S}_{\bm{\lambda}}(1- \tilde{\bm{H}}^{2})}{1+\tilde{\bm{H}}^{2}}=\tilde{\bm{S}}\] (134)

\[\tilde{\bm{S}}_{\lambda}(\tilde{\bm{G}}-\tilde{\bm{H}}\tilde{\bm{G}})^{2}= \frac{\tilde{\bm{S}}_{\lambda}(1+\tilde{\bm{H}}^{2})}{1+\tilde{\bm{H}}^{2}}- \frac{\tilde{\bm{S}}_{\lambda}(2\tilde{\bm{H}})}{1+\tilde{\bm{H}}^{2}}=\frac{ \sqrt{4\bm{S}^{2}+\lambda^{2}\mathbf{I}}-\lambda\mathbf{I}}{2}\] (135)

\[\tilde{\bm{S}}_{\lambda}(\tilde{\bm{G}}+\tilde{\bm{H}}\tilde{\bm{G}})^{2}= \frac{\tilde{\bm{S}}_{\lambda}(1+\tilde{\bm{H}}^{2})}{1+\tilde{\bm{H}}^{2}}+ \frac{\tilde{\bm{S}}_{\lambda}(2\tilde{\bm{H}})}{1+\tilde{\bm{H}}^{2}}=\frac{ \sqrt{4\bm{S}^{2}+\lambda^{2}\mathbf{I}}+\lambda\mathbf{I}}{2}\] (136)

\[\lim_{t\to\infty}\mathbf{QQ}^{T}(t) =\lim_{t\to\infty}\begin{bmatrix}\mathbf{W}_{1}^{T}\mathbf{W}_{1 }(t)&\mathbf{W}_{1}^{T}\mathbf{W}_{2}^{T}(t)\\ \mathbf{W}_{2}\mathbf{W}_{1}(t)&\mathbf{W}_{2}^{T}\mathbf{W}_{2}(t)\end{bmatrix}\] (137) \[=\begin{bmatrix}\tilde{\mathbf{V}}\bm{S}_{1}^{2}\tilde{\mathbf{V} }^{T}&\tilde{\mathbf{V}}\tilde{\bm{S}}\tilde{\mathbf{U}}^{T}\\ \tilde{\mathbf{U}}\tilde{\bm{S}}\tilde{\mathbf{V}}^{T}&\tilde{\mathbf{U}}\bm{S}_ {2}^{2}\tilde{\mathbf{U}}^{T}\end{bmatrix}.\] (138)

#### d.2.3 Representation in the limit

**Theorem D.4**.: _Under the assumptions of Theorem C.5, training on data \(\bm{\Sigma}^{yx}=\tilde{\bm{U}}\tilde{\bm{S}}\tilde{\bm{V}}^{T}\), as \(\lambda\to\infty\) the representation tends to_

\[\bm{W}_{2}\bm{W}_{2}^{T}=\tilde{\bm{U}}\begin{pmatrix}\lambda\bm{ \mathrm{I}}&0_{\max(0,N_{o}-N_{i})}\\ 0_{\max(0,N_{o}-N_{i})}&0\end{pmatrix}\tilde{\bm{U}}^{T}\quad\bm{W}_{1}^{T}\bm {W}_{1}=\frac{1}{\lambda}\tilde{\bm{V}}\begin{pmatrix}\tilde{\bm{S}}^{2}&0_{max (0,N_{i}-N_{o})}\\ 0_{\max(0,N_{i}-N_{o})}&0\end{pmatrix}\tilde{\bm{V}}^{T}\]

_As \(\lambda\to-\infty\)_

\[\bm{W}_{2}\bm{W}_{2}^{T}=-\frac{1}{\lambda}\tilde{\bm{U}}\begin{pmatrix}\tilde {\bm{S}}^{2}&0_{\max(0,N_{o}-N_{i})}\\ 0_{\max(0,N_{o}-N_{i})}&0\end{pmatrix}\tilde{\bm{U}}^{T},\quad\bm{W}_{1}^{T}\bm {W}_{1}=\tilde{\bm{V}}\begin{pmatrix}-\lambda\bm{\mathrm{I}}&0_{\max(0,N_{i}-N _{o})}\\ 0_{\max(0,N_{i}-N_{o})}&0\end{pmatrix}\tilde{\bm{V}}^{T}\]

_As \(\lambda\to-\infty\)_

\[\bm{W}_{2}\bm{W}_{2}^{T}=-\frac{1}{\lambda}\tilde{\bm{U}}\begin{pmatrix}\tilde {\bm{S}}^{2}&0_{\max(0,N_{o}-N_{i})}\\ 0_{\max(0,N_{o}-N_{i})}&0\end{pmatrix}\tilde{\bm{U}}^{T},\quad\bm{W}_{1}^{T}\bm {W}_{1}=\tilde{\bm{V}}\begin{pmatrix}-\lambda\bm{\mathrm{I}}&0_{\max(0,N_{i}-N _{o})}\\ 0_{\max(0,N_{i}-N_{o})}&0\end{pmatrix}\tilde{\bm{V}}^{T}\]

Proof.: We start from the representation derived in D.3 and using the Taylor expansion of \(f(x)=\sqrt{1+x^{2}}\), we compute

\[\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4\tilde{\bm{S}}^{2}}+ \lambda\bm{\mathrm{I}}}{2}=\frac{|\lambda|\sqrt{1+\left(\frac{2\tilde{\bm{S}}} {\lambda}\right)^{2}}+\lambda\bm{\mathrm{I}}}{2}\] (139) \[\frac{|\lambda|\left(1+\left(\frac{2\tilde{\bm{S}}}{\lambda} \right)^{2}+O(\lambda^{-4})\right)+\lambda\bm{\mathrm{I}}}{2}=\frac{|\lambda| +\lambda}{2}+\frac{\tilde{\bm{S}}^{2}}{|\lambda|}+O(\lambda^{-3})\] (140)

Hence

\[\lim_{\lambda\to\infty}\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4 \tilde{\bm{S}}^{2}}+\lambda\bm{\mathrm{I}}}{2}=\lambda\bm{\mathrm{I}},\quad \lim_{\lambda\to-\infty}\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4\tilde{\bm{S} }^{2}}+\lambda\bm{\mathrm{I}}}{2}=\frac{\tilde{\bm{S}}^{2}}{|\lambda|}=-\frac{ \tilde{\bm{S}}^{2}}{\lambda}\] (141)

Similarly,

\[\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4\tilde{\bm{S}}^{2}}- \lambda\bm{\mathrm{I}}}{2}=\frac{|\lambda|-\lambda}{2}+\frac{\tilde{\bm{S}}^{2 }}{|\lambda|}+O(\lambda^{-3})\] (142) \[\lim_{\lambda\to\infty}\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4 \tilde{\bm{S}}^{2}}-\lambda\bm{\mathrm{I}}}{2}=\frac{\tilde{\bm{S}}^{2}}{ \lambda},\quad\lim_{\lambda\to-\infty}\frac{\sqrt{\lambda^{2}\bm{\mathrm{I}}+4 \tilde{\bm{S}}^{2}}-\lambda\bm{\mathrm{I}}}{2}=\frac{\tilde{\bm{S}}^{2}}{| \lambda|}=-\lambda\bm{\mathrm{I}}\] (143)

Since \(\tilde{\bm{U}},\tilde{\bm{V}}\) are independent of \(\lambda\):

\[\lim_{\lambda\to\pm\infty}\bm{W}_{2}\bm{W}_{2}^{T}=\tilde{\bm{U}} \left(\lim_{\lambda\to\pm\infty}\bm{S}_{2}\right)\tilde{\bm{U}}^{T}\] (144) \[\lim_{\lambda\to\pm\infty}\bm{W}_{1}^{T}\bm{W}_{1}=\tilde{\bm{V}} \left(\lim_{\lambda\to\pm\infty}\bm{S}_{1}\right)\tilde{\bm{V}}^{T}\] (145)

As \(|\lambda|\to\infty\), one of the network representations approaches a scaled identity matrix, while the other tends toward zero. Intuitively, this suggests that the representations shift less and less as \(|\lambda|\) increases. Next, we demonstrate that the NTK becomes progressively less variable as \(|\lambda|\) grows and ultimately converges to zero.

#### b.2.4 NTK movement

Relationship between \(\lambda\) and the NTK of the network

**Theorem D.5**.: _Under the assumptions of Theorem C.5, consider a linear network training on data \(\bm{\Sigma}^{yx}=\tilde{\bm{U}}\tilde{\bm{S}}\tilde{\bm{V}}^{T}\). At any arbitrary training time \(t\geq 0\), let \(\bm{W}_{2}(t)\bm{W}_{1}(t)=\bm{U}^{*}\bm{S}^{*}\bm{V}^{*T}\). Then,_

1. _For any_ \(\lambda\in\mathbf{R}\)_:_ \[\begin{split}\text{NTK}(0)&=\mathbf{I}_{N_{o}} \otimes\bm{X}^{T}\bm{V}\begin{pmatrix}\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{* 2}-\lambda\mathbf{I}}&0\\ 0&0\end{pmatrix}\bm{V}^{T}\bm{X}\\ &\quad+\bm{U}\begin{pmatrix}\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{* 2}+\lambda\mathbf{I}}&0\\ 0&0\end{pmatrix}\bm{U}^{T}\otimes\bm{X}^{T}\bm{X}\end{split}\] (146) \[\begin{split}\text{NTK}(t)&=\mathbf{I}_{N_{o}} \otimes\bm{X}^{T}\bm{V}^{*}\begin{pmatrix}\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{ *2}-\lambda\mathbf{I}}&0\\ 0&0\end{pmatrix}\bm{V}^{*T}\\ &\quad+\bm{U}^{*}\begin{pmatrix}\sqrt{\lambda^{2}\mathbf{I}+4\bm{S}^{* 2}+\lambda\mathbf{I}}&0\\ 0&0\end{pmatrix}\bm{U}^{*T}\otimes\bm{X}^{T}\bm{X}\end{split}\] (147)
2. _As_ \(\lambda\to\infty\)_:_ \[\begin{split}\text{NTK}(t)-\text{NTK}(0)&\to \frac{1}{\lambda}\left(\mathbf{I}_{N_{o}}\otimes\bm{X}^{T}\bm{V}^{*}\tilde{\bm {S}}^{*2}\bm{V}^{*T}\bm{X}-\mathbf{I}_{N_{o}}\otimes\bm{X}^{T}\bm{V}\tilde{\bm {S}}^{2}\bm{V}^{T}\bm{X}\right)\to 0\end{split}\] (148)
3. _As_ \(\lambda\to-\infty\)_:_ \[\begin{split}\text{NTK}(t)-\text{NTK}(0)&\to \frac{1}{\lambda}\left(\bm{U}\tilde{\bm{S}}^{2}\bm{U}^{T}\otimes\bm{X}^{T}\bm{ X}-\bm{U}^{*}\tilde{\bm{S}}^{*2}\bm{U}^{*T}\otimes\bm{X}^{T}\bm{X}\right)\to 0\end{split}\] (149)

Proof.: Follows by substituting the expressions for the network representations in terms of \(\lambda\) from (Braun et al. (2022))'s expression for the NTK of a linear network. Similarly, follows from substituting the limit expressions for the network representations and the fact that the Kronecker product is linear in both arguments. 

The theorem above demonstrates that as \(|\lambda|\to\infty\), the NTK of a \(\lambda\)-Balanced network remains constant. This indicates that the network operates in the _lazy_ regime throughout all training steps. This finding is significant as it highlights the impact of weight initialization on learning regimes.

### Representation robustness and sensitivity to noise

As derived in (Braun et al., 2024), the expected mean squared error under additive, independent and identically distributed input noise with mean \(\mu=0\) and variance \(\sigma_{\mathbf{x}}^{2}\) is

\[\left\langle\frac{1}{2P}\sum_{i=1}^{P}||\mathbf{W}_{2}\mathbf{W}_{1}\left( \mathbf{x}_{\mathbf{x}}+\xi_{i}\right)-\mathbf{y}_{i}||_{2}^{2}\right\rangle_{ \xi_{\mathbf{x}}}=\sigma_{\mathbf{x}}^{2}||\mathbf{W}_{2}\mathbf{W}_{1}||_{F}^ {2}+c,\] (150)

where \(c=\frac{1}{2}\operatorname{Tr}(\tilde{\bm{\Sigma}}^{yy})-\frac{1}{2} \operatorname{Tr}(\tilde{\bm{\Sigma}}^{yx}\tilde{\bm{\Sigma}}^{yxT})\) is a noise independent constant that only depends on the statistics of the training data. In Theorem D.3 we show that the network function converges to \(\tilde{\mathbf{U}}\tilde{\mathbf{S}}\tilde{\bm{V}}^{T}\) and therefore

\[\begin{split}\sigma_{\mathbf{x}}^{2}||\mathbf{W}_{2}\mathbf{W}_{ 1}||_{F}^{2}&=\sigma_{\mathbf{x}}^{2}||\tilde{\mathbf{U}}\tilde{ \mathbf{S}}\tilde{\bm{V}}^{T}||_{F}^{2}\\ &=\sigma_{\mathbf{x}}^{2}||\tilde{\mathbf{S}}||_{F}^{2}\\ &=\sigma_{\mathbf{x}}^{2}\sum_{i=1}^{N_{h}}\tilde{\mathbf{S}}_{i}^ {2}\end{split}\] (151)As derived in (Braun et al., 2024), under the assumption of whitened inputs (Assumption 2), in the case of additive parameter noise with \(\mu=0\) and variance \(\sigma_{\mathbf{W}}^{2}\), the expected mean squared error is

\[\left\langle\frac{1}{2P}\sum_{i=1}^{P}||\left(\mathbf{W}_{2}+\xi_{ \mathbf{W}_{2}}\right)\left(\mathbf{W}_{1}+\xi_{\mathbf{W}_{1}}\right)\mathbf{ x}_{i}-\mathbf{y}_{i}||_{2}^{2}\right\rangle_{\xi_{\mathbf{W}_{1}},\xi_{ \mathbf{W}_{2}}}\] (152) \[=\frac{1}{2}N_{i}\sigma_{\mathbf{W}}^{2}||\mathbf{W}_{2}||_{F}^{2 }+\frac{1}{2}N_{o}\sigma_{\mathbf{W}}^{2}||\mathbf{W}_{1}||_{F}^{2}+\frac{1}{2 }N_{i}N_{h}N_{o}\sigma^{4}+c.\]

Using Theorem D.3, we have

\[||\mathbf{W}_{1}||_{F}^{2} =\mathrm{Tr}(\mathbf{W}_{1}^{T}\mathbf{W}_{1})\] (153) \[=\mathrm{Tr}\left(\frac{\sqrt{\lambda^{2}\mathbf{I}+4\mathbf{ \tilde{S}}^{2}}+\lambda\mathbf{I}}{2}\right)\] \[=\frac{1}{2}\left(\sum_{i=1}^{N_{h}}\sqrt{\lambda^{2}+4\mathbf{ \tilde{S}}_{i}^{2}}+\lambda\right)\]

and

\[||\mathbf{W}_{2}||_{F}^{2} =\mathrm{Tr}(\mathbf{W}_{2}\mathbf{W}_{2}^{T})\] (154) \[=\mathrm{Tr}\left(\frac{\sqrt{\lambda^{2}\mathbf{I}+4\mathbf{ \tilde{S}}^{2}}-\lambda\mathbf{I}}{2}\right)\] \[=\frac{1}{2}\left(\sum_{i=1}^{N_{h}}\sqrt{\lambda^{2}+4\mathbf{ \tilde{S}}_{i}^{2}}-\lambda\right).\]

To find the \(\lambda\) that minimises the expected loss, we substitute the equations for the norms, take the partial derivative with respect to \(\lambda\) and set it to zero

\[\frac{\partial\left\langle\mathcal{L}\right\rangle_{\xi_{\mathbf{W }_{1}},\xi_{\mathbf{W}_{2}}}}{\partial\lambda}\overset{!}{=}0\] (155) \[\Leftrightarrow \frac{1}{4}N_{i}\sigma_{\mathbf{W}}^{2}\frac{\partial}{\partial \lambda}\Big{(}\sum_{i=1}^{N_{h}}\sqrt{\lambda^{2}+4\mathbf{\tilde{S}}_{i}^{2 }}-\lambda\Big{)}+\frac{1}{4}N_{o}\sigma_{\mathbf{W}}^{2}\frac{\partial}{ \partial\lambda}\left(\sum_{i=1}^{N_{h}}\sqrt{\lambda^{2}+4\mathbf{\tilde{S}} _{i}^{2}}+\lambda\right)=0\] \[\Leftrightarrow N_{i}\sum_{i=1}^{N_{h}}\frac{\lambda}{\sqrt{\lambda^{2}+4 \mathbf{\tilde{S}}_{i}^{2}}}-N_{i}N_{h}+N_{o}\sum_{i=1}^{N_{h}}\frac{\lambda}{ \sqrt{\lambda^{2}+4\mathbf{\tilde{S}}_{i}^{2}}}+N_{o}N_{h}=0\] \[\Leftrightarrow \sum_{i=1}^{N_{h}}\frac{\lambda}{\sqrt{\lambda^{2}+4\mathbf{ \tilde{S}}_{i}^{2}}}=N_{h}\frac{N_{i}-N_{o}}{N_{i}+N_{o}}.\]

It follows, that under the assumption that \(N_{i}=N_{o}\), the equation reduces to

\[\sum_{i=1}^{N_{h}}\frac{\lambda}{\sqrt{\lambda^{2}+4\mathbf{\tilde{S}}_{i}^{2 }}}=0.\] (156)

We note, that the denominator is always positive and therefore, that the left-hand side of the equation is always larger zero for any \(\lambda>0\), and smaller than zero for any \(\lambda<0\). The euqation is therefore only solved for \(\lambda=0\).

### Effect of the architecture from the lazy to the Rich Regime

**Theorem D.6**.: _Under the conditions of Theorem C.5, when \(\lambda_{\perp}>0\), the network enters a regime referred to as the delayed-rich phase. In this phase, the learning rate is determined by two competing exponential factors:_

\[e^{\lambda_{\perp}\frac{\ell}{\tau}}e^{-\sqrt{\mathbf{S}^{2}+\frac{\lambda^{2 }}{4}}\frac{1}{\tau}}\]\[e^{-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}}\mathbf{I}\frac{t}{\tau}}.\]

_As \(\lambda\) increases, different parts of the network exhibit distinct learning behaviors: some components adapt quickly and converge exponentially with lambda, while others are constrained by the singular values of the network, resulting in slower adaptation._

Proof.: The solution to Theorem C.5 is governed by two time-dependent terms:

\[e^{-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}}\frac{t}{\tau}}\quad\text{ and}\quad e^{\lambda\perp\frac{t}{\tau}}e^{-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}} \mathbf{I}\frac{t}{\tau}}.\]

The first term exhibits exponential decay with rate \(\lambda\), approaching zero as time progresses:

\[\lim_{t\to\infty}e^{-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}}\frac{t}{ \tau}}=\bm{0}.\]

The second term also decays, but at a rate governed by the singular values \(\tilde{\bm{S}}\), as \(\lambda\) tends to infinity:

\[\lim_{t\to\infty}e^{\lambda_{\perp}\frac{t}{\tau}}e^{-\sqrt{\tilde{\bm{S}}^{2 }+\frac{\lambda^{2}}{4}}\mathbf{I}\frac{t}{\tau}}=\bm{0}.\]

Since

\[\lambda_{\perp}-\sqrt{\tilde{\bm{S}}^{2}+\frac{\lambda^{2}}{4}}\mathbf{I}>0,\]

we have

\[\lim_{\lambda\to\infty}\left(\lambda_{\perp}-\sqrt{\tilde{\bm{S}}^{2}+\frac{ \lambda^{2}}{4}}\mathbf{I}\right)=\tilde{\bm{S}}.\]

Thus, as \(\lambda\) increases, the convergence rate slows for certain parts of the network (those governed by larger singular values), while other components continue to learn more quickly. This explains the delay observed in the delayed-rich regime. 

## Appendix E Appendix: Application

### Appendix: Continual Learning

We build upon the derivation presented in Braun et al. (2022) to incorporate the dynamics of continual learning throughout the entire learning trajectory. Utilizing the assumption of whitened inputs, the entire batch loss for the \(i\)th task is

\[\mathcal{L}_{i}\left(\mathcal{T}_{j}\right) =\frac{1}{2P}\left\|\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{X}_{i}- \mathbf{Y}_{i}\right\|_{F}^{2}\] \[=\frac{1}{2P}\operatorname{Tr}\left(\left(\mathbf{W}_{2}\mathbf{ W}_{1}\mathbf{X}_{i}-\mathbf{Y}_{i}\mid\right)\left(\mathbf{W}_{2}\mathbf{W}_{1} \mathbf{X}_{i}-\mathbf{Y}_{i}\mid\right)^{T}\right)\] \[=\frac{1}{2P}\operatorname{Tr}\left(\mathbf{W}_{2}\mathbf{W}_{1} \mathbf{X}_{i}\mathbf{X}_{i}^{T}(\mathbf{W}_{2}\mathbf{W}_{1})^{T}\right)- \frac{1}{P}\operatorname{Tr}\left(\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{X}_{i} \mathbf{Y}_{i}^{T}\right)+\frac{1}{2P}\operatorname{Tr}\left(\mathbf{Y}_{i} \mathbf{Y}_{i}^{T}\right)\] \[=\frac{1}{2}\operatorname{Tr}\left(\left(\mathbf{W}_{2}\mathbf{ W}_{1}-\tilde{\bm{\Sigma}}_{i}^{yx}\right)\left(\mathbf{W}_{2}\mathbf{W}_{1}- \tilde{\bm{\Sigma}}_{i}^{yx}\right)^{T}-\tilde{\bm{\Sigma}}_{i}^{yx}\tilde{ \bm{\Sigma}}_{i}^{yx^{T}}\right)+\frac{1}{2}\left(\tilde{\bm{\Sigma}}_{i}^{yy}\right)\] \[=\frac{1}{2}\left\|\mathbf{W}_{2}\mathbf{W}_{1}-\tilde{\bm{\Sigma }}_{i}^{yx}\right\|_{F}^{2}\underbrace{-\frac{1}{2}\operatorname{Tr}\left( \tilde{\bm{\Sigma}}_{i}^{yx}\tilde{\bm{\Sigma}}_{i}^{yx^{T}}\right)+\frac{1}{2 }\left(\tilde{\bm{\Sigma}}_{i}^{yy}\right)}_{c}.\]

Hence, the extent of forgetting, denoted as \(\mathcal{F}\) for task \(\mathcal{T}_{i}\) during training on task \(\mathcal{T}_{k}\) subsequent to training the network on task \(\mathcal{T}_{j}\), specifically, the relative change in loss, is entirely dictated by the similarity structure among tasks.

Figure. 5 panel was generated by training a linear network with \(N_{i}=5\), \(N_{h}=10\), \(N_{o}=6\) subsequently on four different random regression tasks with \(N=25\). The learning rate was \(\eta=0.05\) and the initial weights were small (\(\sigma=0.0001\)).

### Appendix: Reversal Learning

As first introduced in Braun et al. (2022), in the following discussion, we assume that the input and output dimensions are equal. We denote the \(i\)-th columns of the left and right singular vectors as \(\mathbf{u}_{i}\), \(\mathbf{\tilde{u}}_{i}\), and \(\mathbf{v}_{i}\), \(\mathbf{\tilde{v}}_{i}\), respectively.

Reversal learning occurs when both the task and the initial network function share the same left and right singular vectors, i.e., \(\mathbf{U}=\mathbf{\tilde{U}}\) and \(\mathbf{V}=\mathbf{\tilde{V}}\), with the exception of one or more columns of the left singular vectors, where the direction is reversed: \(-\mathbf{u}_{i}=\mathbf{\tilde{u}}_{i}\).

It is important to note that if a reversal occurs in the right singular vectors, such that \(-\mathbf{v}_{i}=\mathbf{\tilde{v}}_{i}\), this can be equivalently represented as a reversal in the left singular vectors, as the signs of the right and left singular vectors are interchangeable.

Figure 5: Continual learning. **A** Top: Network training from small zero-balanced weights across a sequence of tasks (colored lines represent simulations, and black dotted lines represent analytical results). Bottom: Evaluation loss for the tasks in the sequence (dotted lines) while training on the current task (solid lines). As the network optimizes its function on the current task, the loss on previously learned tasks increases.

In the reversal learning setting, both \(\bm{B}=\bm{S}_{2}\bm{U}^{T}\bm{\tilde{U}}(\bm{\tilde{G}}+\bm{\tilde{H}}\bm{\tilde{G }})+\bm{S}_{1}\bm{V}^{T}\bm{\tilde{V}}(\bm{\tilde{G}}-\bm{\tilde{H}}\bm{\tilde{G }})\) and \(\bm{C}=\bm{S}_{2}\tilde{\bm{U}}^{T}\tilde{\bm{U}}(\bm{\tilde{G}}-\bm{\tilde{H}} \bm{\tilde{G}})-\bm{S}_{1}\bm{V}^{T}\tilde{\bm{V}}(\bm{\tilde{G}}+\bm{\tilde{H}} \bm{\tilde{G}})\) are diagonal matrices.

In the case where lambda is zero, the same argument given in Braun et al. (2022) follows, the diagonal entries of \(\mathbf{C}\) are zero if the singular vectors are aligned and non zero if they are reversed. Similarly, diagonal entries of \(\mathbf{B}\) are non-zero if the singular vectors are aligned and zero if they are reversed. Therefore, in the case of reversal learning, \(\mathbf{B}\) is a diagonal matrix with \(0\) values and thus is not invertible. As a consequence, the learning dynamics cannot be described by Equation 37. However, as \(\mathbf{B}\) and \(\mathbf{C}\) are diagonal matrices, the learning dynamics simplify. Let \(\mathbf{b}_{i}\), \(\mathbf{c}_{i}\), \(\mathbf{s}_{i}\) and \(\mathbf{\tilde{s}}_{i}\) denote the \(i\)-th diagonal entry of \(\mathbf{B}\), \(\mathbf{C}\), \(\mathbf{S}\) and \(\mathbf{\tilde{S}}\) respectively, then the network dynamics can be rewritten as

\[\mathbf{W}_{2}\mathbf{W}_{1}(t) =\frac{1}{2}\tilde{\mathbf{U}}\left[(\bm{\tilde{G}}+\bm{\tilde{H} }\bm{\tilde{G}})e^{\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}\mathbf{B}^{T}+(\bm {\tilde{G}}-\bm{\tilde{H}}\bm{\tilde{G}})e^{-\bm{\tilde{S}}_{\lambda}\frac{t}{ \tau}}\mathbf{C}^{T}\right)\] (157) \[\quad\quad\left[\bm{S}_{\lambda}^{-1}+\frac{1}{4}\mathbf{B}\left( e^{2\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}-\mathbf{I}\right)\bm{\tilde{S}}_{ \lambda}^{-1}\mathbf{B}^{T}-\frac{1}{4}\mathbf{C}\left(e^{-2\bm{\tilde{S}}_{ \lambda}\frac{t}{\tau}}-\mathbf{I}\right)\bm{\tilde{S}}_{\lambda}^{-1}\mathbf{ C}^{T}\right]^{-1}\] \[\quad\quad\frac{1}{2}\left((\bm{\tilde{G}}-\bm{\tilde{H}}\bm{ \tilde{G}})e^{\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}\mathbf{B}-(\bm{\tilde{G }}+\bm{\tilde{H}}\bm{\tilde{G}})e^{-\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}} \mathbf{C}\right)\bm{\tilde{V}}^{T}\] \[=\sum_{i=1}^{N_{i}}\frac{\mathbf{b}_{i}^{2}e^{2\bm{\tilde{S}}_{ \lambda}\frac{t}{\tau}}-\mathbf{c}_{i}^{2}e^{-2\bm{\tilde{S}}_{\lambda}\frac{t }{\tau}}}{4\bm{\tilde{s}}_{\lambda}^{-1}+\mathbf{b}_{i}^{2}e^{2\bm{\tilde{S}} _{\lambda}\frac{t}{\tau}}}\bm{\tilde{s}}_{\lambda\bm{i}}^{-1}-\mathbf{b}_{i}^{ 2}\bm{\tilde{s}}_{\lambda\bm{i}}^{-1}-\mathbf{c}_{i}^{2}e^{-2\bm{\tilde{S}}_{ \lambda}\frac{t}{\tau}}\bm{\tilde{s}}_{\lambda\bm{i}}^{-1}+\mathbf{c}_{i}^{2} \bm{\tilde{s}}_{\lambda\bm{i}}^{-1}\ \bm{\tilde{u}}_{i}\bm{\tilde{v}}_{i}^{T}\] (158) \[=\sum_{i=1}^{N_{i}}\frac{\mathbf{s}_{\lambda\bm{i}}b_{i}^{2} \mathbf{\tilde{s}}_{\lambda\bm{i}}-\mathbf{s}_{\lambda\bm{i}}c_{i}^{2}\mathbf{ \tilde{s}}_{i}e^{-4\mathbf{\tilde{s}}_{i}\frac{t}{\tau}}}{4\bm{\tilde{s}}_{ \lambda\bm{i}}e^{-2\bm{\tilde{s}}_{\lambda}\frac{t}{\tau}}+\mathbf{s}_{\lambda \bm{i}}\mathbf{b}_{i}^{2}\left(1-e^{-2\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}} \right)+\mathbf{s}_{\lambda\bm{i}}\mathbf{c}_{i}^{2}\left(e^{-2\bm{\tilde{S}}_{ \lambda}\frac{t}{\tau}}-e^{-4\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}\right) \bm{\tilde{u}}_{i}\mathbf{\tilde{v}}_{i}^{T}\] (159)

It follows, that in the reversal learning case, i.e. \(\mathbf{b}=0\), for each reversed singular vector, the dynamics vanish to zero

\[\lim_{t\rightarrow\infty}\frac{-\mathbf{s}_{\lambda\bm{i}}c_{i}^{2}\mathbf{ \tilde{s}}_{i}e^{-4\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}}{4\bm{\tilde{s}}_{ \lambda\bm{i}}e^{-2\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}+\mathbf{s}_{\bm{i}} c_{i}^{2}\left(e^{-2\bm{\tilde{S}}_{\lambda}\frac{t}{\tau}}-e^{-4\bm{\tilde{S}}_{ \lambda}\frac{t}{\tau}}\right)}\bm{\tilde{u}}_{i}\mathbf{\tilde{v}}_{i}^{T}=0.\] (160)

Analytically, the learning dynamics are initialized on and remain along the separatrix of a saddle point until the corresponding singular value of the network function decreases to zero and stays there, indicating convergence to the saddle point. In numerical simulations, however, the learning dynamics can escape the saddle points due to the imprecision of floating-point arithmetic. Despite this, numerical optimization still experiences significant delays, as escaping the saddle point is time-consuming Lee et al. (2022). In contrast, when the singular vectors are aligned (\(\mathbf{c}=0\)), the equation governing temporal dynamics, as described in Saxe et al. (2014), is recovered. Under these conditions, training succeeds, with the singular value of the network function converging to its target value.

\[\lim_{t\rightarrow\infty}\sum_{i=1}^{N_{i}}\frac{\mathbf{s}_{\lambda \bm{i}}\mathbf{b}_{i}^{2}\mathbf{\tilde{s}}_{\lambda\bm{i}}}{4\bm{\tilde{s}}_{ \lambda\bm{i}}e^{-2\bm{\tilde{S}}_{\lambda\bm{i}}\frac{t}{\tau}}+\mathbf{s}_{ \lambda\bm{i}}\mathbf{b}_{i}^{2}\left(1-e^{-2\bm{\tilde{s}}_{\lambda}\frac{t}{ \tau}}\right)}\bm{In summary, in the case of aligned singular vectors, the learning dynamics can be described by the convergence of singular values. However in the case of reversal learning, analytically, training does not succeed. In simulations, the learning dynamics escape the saddle point due to numerical imprecision, but the learning dynamics are catastrophically slowed in the vicinity of the saddle point as shown in figure 6.

In the case where \(\lambda\) is non-zero, the diagonal of \(\mathbf{C}\) are also non-zero; this is true regardless of whether they are reversed or aligned. Similarly, the diagonal entries of \(\mathbf{B}\) remain non-zero whether the singular vectors are aligned or reversed. Therefore, in the case of reversal learning, \(\mathbf{B}\) is a diagonal matrix with elements that are zero. In figure 6

### Appendix: Generalization and structured learning

We study how the representations learned for different \(\lambda\) initializations impact generalization of properties of the data. To do this, we consider the case where a new feature is associated to a learned item in a dataset and how this new feature may then be related to other items based on prior knowledge. In particular, we first train each network (for different values of \(-10\leq\lambda\leq 10\)) on the hierarchical semantic learning task in Section 3 and then add a new feature (e.g., 'eats worms') to a single item (e.g., the goldfish) (Fig. 7A), correspondingly increasing the output dimension to represent the novel feature. In order to learn the new feature without affecting prior knowledge, we append a randomly initialized row to \(\mathbf{W}_{2}\) and train it on the single item with the new feature, while keeping the rest of the network frozen. Thus, we only change the weights from the hidden layer to the new feature which may produce different behavior depending on how the hidden layer representations vary based on \(\lambda\). After training on the new feature-item association, we query the network with the rest of the data to observe how the new feature is associated with the other items. We find that as \(\lambda\) increases positively, the network better transfers the hierarchy such that it projects the feature onto items based on their distance to the trained item (Fig. 7B,C). For example, after learning that a goldfish eats worms, the network can extrapolate the hierarchy to infer that another fish, or birds, may also eat worms; instead, plants are not likely to eat worms. Alternatively, as \(\lambda\) becomes more negative, the network ceases to infer any hierarchical structure and only learns to map the new feature to the single item trained on. In this case, after learning that a goldfish eats worms, the network does not infer that other fish, birds, or plants may also eat worms.

Interestingly, this setting highlights how asymmetries in the representations yielded by different \(\lambda\) can actually benefit transfer and generalization. This can be shown by observing that the learning of a new feature association only depends on the first layer \(\mathbf{W}_{1}\). Let \(\hat{\bm{y}}_{f}\) denote the vector of the representation of the new feature \(f\) across items \(i\) in the dataset. Additionally, let \(\bm{w}_{2}^{(f)T}\) be the new row of weights appended to \(\mathbf{W}_{2}\) which map the hidden layer to the new feature. Following Saxe et al. (2019b), if \(\bm{w}_{2}^{(f)T}\) is initialized with small random weights and trained on item \(\hat{\bm{H}}_{i}\), it will

Figure 6: Plot showing the steps to convergence for two tasks: (1) the reversal learning task and (2) a randomly sampled continual learning task across a range of \(\lambda\) values. The reversal learning task exhibits catastrophic slowing at \(\lambda=0\).

converge to

\[\bm{w}_{2}^{(f)T} =\tilde{\bm{H}}_{i}^{T}\mathbf{W}_{1}^{T}/\|\mathbf{W}_{1}\tilde{\bm {H}}_{i}\|_{2}^{2}\] (163) \[\hat{\bm{y}}_{f} =(\tilde{\bm{H}}_{i}^{T}\mathbf{W}_{1}^{T}\mathbf{W}_{1}\tilde{\bm {H}})/\|\mathbf{W}_{1}\tilde{\bm{H}}_{i}\|_{2}^{2}\] (164)

From this we can see that differences in the representations of the new feature across items \(\hat{\bm{y}}_{f}\) across \(\lambda\) are only influenced by \(\mathbf{W}_{1}\).

In the case of the rich learning regime where \(\lambda=0\), the semantic relationship between features and items is distributed across both layers. Instead, when \(\lambda>0\), the second layer \(\mathbf{W}_{2}\) exhibits _lazy_ learning, yielding an output representation \(\mathbf{W}_{2}\mathbf{W}_{2}^{T}\) of a weighted identity matrix. However, the first layer \(\mathbf{W}_{1}\) still learns a _rich_ representation of the hierarchy, albeit at a smaller scaling. Furthermore, rather than distributing this learning across both layers, in the \(\lambda>0\) case, all learning of the hierarchy occurs in the first layer, allowing it to more readily transfer this structure to the learning of a new feature (which only depends on the first layer). Thus, in this case, the'shallowing' of the network into the first layer is actually beneficial. Finally, we can also observe the opposite case when \(\lambda<0\). Here, _rich_ learning happens in the second layer, while the first layer is _lazy_ and learns to represent a weighted identity matrix. As such, these networks do not learn to transfer the hierarchy of different items to the new feature.

## Appendix F Implementation and Simulations

The details of the simulation studies are described as follows. Specifically, \(N_{i}\), \(N_{h}\), and \(N_{o}\) represent the dimensions of the input, hidden layer, and output (target), respectively. The total number of training samples is denoted by \(N\), and the learning rate is defined as \(\eta=\frac{1}{\tau}\).

### Lambda-balanced weight initialization

In practice, to initialize the network with lambda-balanced weights, we use Algorithm F.1. In this algorithm, \(\alpha\) serves as a scaling factor that controls the variance of the weights, allowing for adjustments between smaller and larger weight initializations.

Figure 7: Transfer learning for different \(\lambda\). **A** A new feature (such as ‘eats worms’) is introduced to the dataset after training on the hierarchical semantic learning task (Section 3). A randomly initialized row is added to \(\mathbf{W}_{2}\) and trained on a single item with the new feature (for example, the goldfish), with the rest of the network frozen. The network is then tested on the transfer of the new feature to other items, such that items closer to the goldfish in the hierarchy are more likely to have the same feature. **B** The generalization loss on the untrained items with the new feature decreases as \(\lambda\) increases. **C** As \(\lambda\) increases positively, networks better transfer the hierarchical structure of the data to the representation of the new feature.

### Tasks

In the following, we describe the different tasks that are used throughout the simulation studies.

#### f.2.1 Random regression task

In the random regression task, the inputs \(\mathbf{X}\in\mathbb{R}^{N_{i}\times N}\) are generated from a standard normal distribution, \(\mathbf{X}\sim\mathcal{N}(\mu=0,\sigma=1)\). The input data \(\mathbf{X}\) is then whitened to satisfy \(\frac{1}{N}\mathbf{X}\mathbf{X}^{T}=\mathbf{I}\). The target values \(\mathbf{Y}\in\mathbb{R}^{N_{o}\times N}\) are independently sampled from a normal distribution with variance scaled according to the number of output nodes, \(\mathbf{Y}\sim\mathcal{N}(\mu=0,\alpha=\frac{1}{\sqrt{N_{o}}})\). Consequently, the network inputs and target values are uncorrelated Gaussian noise, implying that a linear solution may not always exist.

#### f.2.2 Semantic hierarchy

We use the same task as in Braun et al. (2022) and modify it to match the theoretical dynamics. The modification ensures that the inputs are whitened. In the semantic hierarchy task, input items are represented as one-hot vectors, i.e., \(\mathbf{X}=\frac{1}{8}\). The corresponding target vectors, \(\mathbf{y}_{i}\), encode the item's position within the hierarchical tree. Specifically, a value of \(1\) indicates that the item is a left child of a node, \(-1\) denotes a right child, and \(0\) indicates that the item is not a child of that node. For example, consider the blue fish: it is a blue fish, a left child of the root node, a left child of theanimal node, not part of the plant branch, a right child of the fish node, and not part of the bird, algae, or flower branches, resulting in the label \([1,1,1,0,-1,0,0,0]\). The labels for all objects in the semantic tree, as shown in Figure 2 A, are given by:

\[\mathbf{Y}=8*\begin{bmatrix}1&1&1&1&1&1&1&1\\ 1&1&1&1&-1&-1&-1&-1\\ 1&1&-1&-1&0&0&0&0\\ 0&0&0&0&1&1&-1&-1\\ 1&-1&0&0&0&0&0&0\\ 0&0&1&-1&0&0&0&0\\ 0&0&0&0&1&-1&0&0\\ 0&0&0&0&0&0&1&-1\end{bmatrix}.\] (165)

The singular value decomposition (SVD) of the corresponding correlation matrix, \(\tilde{\mathbf{\Sigma}}^{yx}\), is not unique due to identical singular values: the first two, the third and fourth, and the last four values are the same. To align the numerical and analytical solutions, this permutation invariance is addressed by adding a small perturbation to each column \(\mathbf{y}_{i}\), for \(i\in 1,...,N\), of the labels:

\[\mathbf{y}_{i}=\mathbf{y}_{i}\cdot\left(1+\frac{0.1}{i}\right),\] (166)

resulting in singular values that are nearly, but not exactly, identical.

### Figure 1

Panels B illustrates three simulations conducted on the same task with varying initial \(\lambda\)-balanced weights respectively \(\lambda=-2\), \(\lambda=0\), \(\lambda=2\). The regression task parameters were set with \((\sigma=\sqrt{10})\). The network architecture consisted of \(N_{i}=3\), \(N_{h}=2\), \(N_{o}=2\),with a learning rate of \(\eta=0.0002\). The batch size is \(N=10\). The zero-balanced weights are initialized with variance \(\sigma=0.00001\). The lambda-balanced network are initialized with \(sigma\)\(=\sqrt{1}\) of a random regression task with same architecture.

On Panel C, we plot the ballancedness \(\mathbf{W}_{2}(0)^{T}\mathbf{W}_{2}(0)-\mathbf{W}_{1}(0)\mathbf{W}_{1}(0)^{T}\) for a two layer network initialised with Lecun initialization with dimension \(N_{i}\) = 40,\(N_{h}\)= 120,\(N_{o}\)=250

### Figure 2

Panel A, B, C illustrates three simulations conducted on the same task with varying initial \(\lambda\)-balanced weights respectively \(\lambda=-2\), \(\lambda=0\), \(\lambda=2\) according to the initialization scheme described in F.7. The regression task parameters were set with \((\sigma=\sqrt{10})\). The network architecture consisted of \(N_{i}=3\), \(N_{h}=2\), \(N_{o}=2\) with a learning rate of \(\eta=0.0002\). The batch size is \(N=10\). The zero-balanced weights are initialized with variance \(\sigma=0.00001\). The lambda-balanced network are initialized with \(sigma\)\(=\sqrt{1}\) of a random regression task with same architecture.

### Figure 3

Panel A, B, C illustrates three simulations conducted on the same task with varying initial \(\lambda\)-balanced weights respectively \(\lambda=-2\), \(\lambda=0\), \(\lambda=2\) according to the initialization scheme described in F.7. The regression task parameters were set with \((\sigma=\sqrt{12})\). The network architecture consisted of \(N_{i}=3\), \(N_{h}=3\), \(N_{o}=3\) with a learning rate of \(\eta=0.0002\). The batch size is \(N=5\). The zero-balanced weights are initialized with variance \(\sigma=0.0009\). The lambda-balanced network are initialized with \(sigma\)\(=\sqrt{12}\) of a random regression task with same architecture.

### Figure 4

In Panel A presents a semantic learning task with the SVD of the input-output correlation matrix of the task. \(U\) and \(V\) represent the singular vectors, and \(S\) contains the singular values. Thisdecomposition allows us to compute the respective RSMs as \(USU^{\top}\) for the input and \(VSV^{\top}\) for the output task. The rows and columns in the SVD and RSMs are ordered identically to the items in the hierarchical tree.

The results in Panel B display simulation outcomes, while Panel C presents theoretical input and output representation matrices at convergence for a network trained on the semantic task described in Braun et al. (2022); Saxe et al. (2013),. These matrices are generated using varying initial \(\lambda\)-balanced weights set at \(\lambda=-2\), \(\lambda=0\), and \(\lambda=2\), following the initialization scheme outlined in F.7. The network architecture includes \(N_{i}=8\), \(N_{h}=8\), and \(N_{o}=8\) with a learning rate of \(\eta=0.001\) and a batch size of \(N=8\). Zero-balanced weights are initialized with a variance of \(\sigma=0.00001\), while \(\lambda\)-balanced networks are initialized with \(\sigma_{xy}=\sqrt{1}\) based on a random regression task with the same architecture.

Panel D illustrates results from running the same task and network configuration but initialized with randomly large weights having a variance of \(\sigma=1\).

In panel E, we trained a two-layer linear network with \(N_{i}=N_{h}=N_{o}=4\) on a random regression task for \(\lambda\in[-5,-4,-3,-2,-1,0,1,2,3,4,5]\) to convergence. Subsequently, we added Gaussian noise with \(\mu=0,\sigma\in[0,0.5,1]\) to the inputs (top panel) or synaptic weights (bottom panel) and calculated the expected mean squared error.

### Figure 5

Panel A illustrates schematic representations of the network architectures considered: from left to right, a funnel network (\(N_{i}=4\), \(N_{h}=2\), \(N_{o}=2\)), a square network (\(N_{i}=4\), \(N_{h}=4\), \(N_{o}=4\)), and an inverted-funnel network (\(N_{i}=2\), \(N_{h}=2\), \(N_{o}=4\)).

Panel B shows the Neural Tangent Kernel (NTK) distance from initialization, as defined in Fort et al. (2020), across the three architectures shown schematically. The kernel distance is calculated as:

\[S(t)=1-\frac{\langle K_{0},K_{t}\rangle}{\|K_{0}\|_{F}\|K_{t}\|_{F}}.\]

The simulations conducted on the same task with eleven varying initial \(\lambda\)-balanced weights in \([-9,9]\). The regression task parameters were set with \((\sigma=\sqrt{3})\). The task has batch size \(N=10\). The network has with a learning rate of \(\eta=0.01\). The lambda-balanced network are initialized with \(\sigma xy=\sqrt{1}\) of a random regression task.

Panel C shows the Neural Tangent Kernel (NTK) distance from initialization for the funnel architectures shown schematically with dimensions \(N_{i}=3\), \(N_{h}=2\), and \(N_{o}=2\). The simulations conducted on the same task with twenty one varying initial \(\lambda\)-balanced weights in \([-9,9]\). The regression task parameters were set with \((\sigma=\sqrt{3})\). The task has batch size \(N=30\). The network has with a learning rate of \(\eta=0.002\). The lambda-balanced network are initialized with \(\sigma xy=\sqrt{1}\) of a random regression task.