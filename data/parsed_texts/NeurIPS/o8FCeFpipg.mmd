# Towards the Universal Learning Principle for Graph Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Graph neural networks (GNNs) are currently highly regarded in graph representation learning tasks due to their significant performance. Although various propagation mechanisms and graph filters were proposed, few works have investigated their rationale from the perspective of learning. In this paper, we elucidate the criterion for the graph filter formed by power series, and further establish a scalable regularized learning framework that theoretically realizes very deep GNN. Following the framework, we introduce Adaptive Power GNN (APGNN), a deep GNN that employs exponentially decaying weights to aggregate graph information of varying orders, thus facilitating more effective mining of deeper neighbor information. Moreover, the multiple \(P\)-hop message passing strategy is proposed to efficiently perceive the higher-order neighborhoods. Different from other GNNs, the proposed APGNN can be seamlessly extended to an infinite-depth network. To clarify the learning guarantee, we theoretically analyze the generalization of the proposed learning framework via uniform convergence. Experimental results show that APGNN obtains superior performance compared to state-of-the-art GNNs, highlighting the effectiveness of our framework.

## 1 Introduction

Recently, Graph Neural Networks (GNNs) have shown commendable performance on numerous graph representation learning tasks. In addition, GNNs have been introduced in a variety of application tasks, such as recommendation systems [7; 11; 37], computer vision [4; 13; 23], and traffic forecasting [8; 9]. The fundamental part of GNN is the design of the propagation mechanism or the graph filter [5; 12; 27; 32; 34]. GNNs can be categorized into two groups based on the approach of formulation. Spatial-based GNN formulates propagation mechanisms through the direct aggregation of spatial features. As one of the most simple GNNs, Graph Convolutional Network (GCN) [15] designs graph convolutional layer via aggregating one-hop information on the graph. Graph Attention Network (GAT) [30] learns node relationships using an attention mechanism, enhancing the scalability of the network. For extension of inductive learning, GraphSAGE [10] employs various pooling operations as aggregation functions. Liu et. al proposed DAGNN, which integrates information from multiple receptive fields for adaptive propagation [19]. Spectral-based GNN designs graph filters by constructing filter functions in the graph Fourier domain, which aims to find a proper transformation of the graph spectrum. Chebynet constructs the localized graph filter with Chebyshev polynomial [3]. From the view of the spectrum, GCN could be seen as a Chebyshev filter with first-order truncation [15]. To construct deeper GNN, Personalize PageRank method is employed to design graph filter [16]. GNN-LF/HF [36] concludes various designs of graph filters and constructs the graph filter through a graph optimization framework.

Despite their success, few studies have explored the general rule for devising GNNs from the perspective of learning. In this paper, we start from the graph filter formed by power series anddiscuss what makes a legitimate graph filter for the construction of deep GNN. A learning principle is then proposed to summarize the rule of formulating a graph filter. Following this, we propose Adaptive Power Graph Neural Network (APGNN), which adaptively learns the task-specific graph filter for node representation learning. The main idea of APGNN is depicted in Figure 1. The parameterized graph filter is designed with regularization of the exponential decay rate. A multiple \(P\)-hop strategy is applied to enhance the capacity of perceiving the higher-order neighborhoods. Furthermore, the generalization bound of APGNN is presented with the setting of the continuous graph, which provides a learning guarantee for the proposed principle theoretically.

We conduct evaluations on five benchmark datasets on node classification tasks. The experimental results suggest the superiority of the proposed method over the existing GNNs. The theoretical analysis is also validated via the empirical study.

## 2 Preliminaries

**Notations.** Suppose we have an undirected graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{A})\) with node set \(\mathcal{V}\) and \(|\mathcal{V}|=n\). \(\mathbf{A}\in\mathbb{R}^{n\times n}\) denotes the adjacency matrix indicating the edges in \(\mathcal{E}\). Assuming that the self-loops are contained in the graph, i.e., \(a_{ii}=1\). Let \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}]^{\top}\in \mathbb{R}^{n\times d}\) be the graph signals (or features) of the nodes. We use notation \([n]\triangleq\{1,2,\cdots,n\}\) for \(n\in\mathbb{N}_{+}\). Assume that the label of \(\mathbf{x}_{i}\) is \(y_{i}\in\mathcal{Y}\) for all \(i=[n_{l}]\), where \(n_{l}\leq n\) is the number of labeled samples.

**Graph Neural Networks.** We introduce some essential concepts in GNNs. Let \(d_{i}=\sum_{j=1}^{n}A_{ij}\) be the degree of \(i\)-th node, so the degree matrix of \(\mathbf{A}\) can be defined as \(\mathbf{D}=\mathrm{diag}(d_{1},d_{2},\cdots,d_{n})\). The symmetrically normalized Laplacian is \(\mathbf{L}=\mathbf{I}-\tilde{\mathbf{A}}\), where \(\tilde{\mathbf{A}}\triangleq\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\) is normalized adjacency matrix. Consider the eigen-decomposition \(\mathbf{L}=\mathbf{U}\mathbf{A}\mathbf{U}^{\top}\), where \(\mathbf{\Lambda}=\mathrm{diag}(\lambda_{1},\cdots,\lambda_{n})\) is the diagonal matrix of eigenvalues, and \(\mathbf{U}=[\mathbf{u}_{1},\cdots,\mathbf{u}_{n}]\) represents the eigenvectors associated with the eigenvalues. Note that \(\tilde{\mathbf{A}}\) shares the same eigenvectors with \(\mathbf{L}\).

Spectral convolution on graphs is defined as the following transformation [15; 28]:

\[g*\mathbf{X}=\mathbf{U}g(\mathbf{\Lambda})\mathbf{U}^{\top}\mathbf{X},\] (1)

where \(g(\cdot):[0,2]\mapsto\mathbb{R}\) is called filter function and \(g(\mathbf{\Lambda})=\mathrm{diag}(g(\lambda_{1}),\cdots,g(\lambda_{n}))\). The common approach in GNNs is to apply polynomial functions as the filters [3; 12; 15], which leads to \(\mathbf{U}g(\mathbf{\Lambda})\mathbf{U}^{\top}=g(\mathbf{L})\). Therefore, spectral convolution is usually written as \(g*\mathbf{X}=g(\mathbf{L})\mathbf{X}\). The graph representation paradigm in GNN is generally expressed as follows:

\[\mathbf{Z}=g(\mathbf{L})f(\mathbf{X}),\quad g(\mathbf{L})=\sum_{k=0}^{K}\theta _{k}\tilde{\mathbf{A}}^{k},\] (2)

where \(\mathbf{Z}\in\mathbb{R}^{n\times c}\) denotes the node representation, and \(f(\cdot)\) represents a feature extractor such as multi-layer perceptions (MLPs).

Figure 1: An illustration of the proposed APGNN that adheres to the learning principle. The model incorporates the decay rate \(\alpha\) to suppress the information from high-order neighbors while adaptively learning bounded coefficients \(\beta\). Furthermore, it aggregates information with \(P\)-hop to enlarge the receptive field. This design enables the seamless extension of APGNN to an extremely deep network.

Learning Principle for GNNs

### The principle of devising graph filters

Current studies suggest a significant relationship between the performance of GNN and its graph filter [16, 19]. Predominantly, the general graph filters are characterized by polynomials associated with the adjacency matrix \(\tilde{\mathbf{A}}\) (or Laplacian matrix \(\mathbf{L}\)), i.e., \(g(\mathbf{L})=\sum_{k=0}^{K}\theta_{k}\tilde{\mathbf{A}}^{k}\). However, the existing methods still meet the issue that the depth of GNN is limited. The reason for this phenomenon is that these GNNs are inconsistent with their "infinite-depth" version. That is, the corresponding models lose some essential properties as the depth \(K\rightarrow\infty\). Consequently, the depth of the models is restricted. To address this issue, it is necessary to study the properties of GNNs with infinite depth. Therefore, we explore the graph filter reformulated as power series:

\[g(\tilde{\mathbf{A}})=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{k}= \sum_{k=0}^{\infty}\theta_{k}(\mathbf{I}-\mathbf{L})^{k}.\] (3)

First of all, a well-defined graph filter represented as equation 3 must be convergent. Consequently, it becomes essential to investigate the properties that the coefficients \(\theta_{k}\) should exhibit. The following lemma provides appropriate constraints for the coefficients of the graph filter.

**Lemma 1**.: _Let \(\{a_{k}\}\) and \(\{\gamma^{k}\}\) be the real number sequences, where \(\gamma\in(-1,1]\) and \(k\in\mathbb{N}\). Then \(\sum_{k}^{\infty}a_{k}\gamma^{k}\) converges uniformly and absolutely if and only if the series \(\sum_{k}^{\infty}a_{k}\) converges absolutely._

As a direct corollary, the weights of the graph filter (i.e., \(\theta_{k}\)) should satisfy the following theorem.

**Theorem 1**.: _Let \(\tilde{\mathbf{A}}=\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\) be the normalized adjacency matrix of a graph \(\mathcal{G}\) with spectral radius \(\rho(\tilde{\mathbf{A}})\leq 1\). The matrix series \(\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{k}\) converges uniformly and absolutely if and only if the series \(\sum_{k=0}^{\infty}\theta_{k}\) converges absolutely._

The proofs are shown in Appendix. Theorem 1 offers a sufficient and necessary condition for the convergence of graph filters formed by power series. Specifically, the condition requires the existence of a finite real number \(M\geq 0\),

\[\|\boldsymbol{\theta}\|_{1}\triangleq\sum_{k=0}^{\infty}|\theta_{k}|\leq M.\] (4)

Therefore, an arbitrary graph filter formed by power series should satisfy the above convergence condition, which gives the first requirement while designing GNN. Apart from convergence, we expect the graph filter to possess good analytic properties such as smoothness. To this end, Lipschitz continuity should be considered the second requirement of the graph filter. Let \(g(\cdot)\) be an \(L\)-Lipschitz continuous function, meaning that

\[|g(\lambda)-g(\lambda^{\prime})|\leq L|\lambda-\lambda^{\prime}|,\quad\forall \lambda,\lambda^{\prime}\in[0,2).\] (5)

This property indicates the stability or robustness of the model [6, 24]. If the graph is contaminated and its eigenvalues are perturbed by at most \(\epsilon\), Lipschitz continuity ensures the perturbation of the graph-filtered result is at most \(L\epsilon\). For instance, considering \(g(\lambda)=\sum_{k=0}^{\infty}(1-\lambda)^{k}/k^{2}\), which is convergent, yet the Lipschitz condition does not satisfy for \(\lambda\) closed to zero. Therefore, this graph filter might be sensitive to the input graph. Subsequently, we conclude the following criterion.

\[\mathbf{Z}=g_{\boldsymbol{\theta}}(\mathbf{L})f(\mathbf{X}),\text{ with }\| \boldsymbol{\theta}\|_{1}\leq M,\ g_{\boldsymbol{\theta}}(\cdot)\text{ is a Lipschitz function.}\] (6)

To enhance the scalability of the model, we define \(\boldsymbol{\theta}\) as a learnable parameter (though its dimension is infinite). In this way, (6) gives a regularized learning framework for GNN. Therefore, for a \(K\)-order polynomial graph filter \(g_{\boldsymbol{\theta}}^{K}(\lambda)=\sum_{k=0}^{K}\theta_{k}(1-\lambda)^{k}\), which is what we can implement in practice, the condition (6) should be satisfied to keep the consistency with its infinitely deep version \(g_{\boldsymbol{\theta}}^{\infty}(\lambda)=\sum_{k=0}^{\infty}\theta_{k}(1- \lambda)^{k}\). We will present the applications of this criterion in this section, and further analyze the learning guarantee with generalization in section 4.

### Related works

In this subsection, we investigate the relationship between our learning framework and several well-known Graph Neural Networks (GNNs), focusing on the design of graph filters. Our findings indicate that these GNNs are all special cases of our learning framework, which are summarized in Table 1.

**GCN/SGC [15; 33].** Graph convolutional network (GCN) aims to learn a node representation by stacking multiple graph convolutional layers. In each layer, GCN applies first-order Chebyshev approximation as the graph filter followed by a fully connected layer. For simplicity, we analyze one-layer GCN, which is formulated as \(\mathbf{Z}=\sigma(\tilde{\mathbf{A}}\mathbf{X}\mathbf{W})\), where \(\mathbf{W}\) is a learnable weight matrix for linear transformation. Therefore, the graph filter of one-layer GCN is \(g_{\mathrm{GCN}}(L)=\mathbf{I}-\mathbf{L}=\tilde{\mathbf{A}}\), or a trivial matrix power series:

\[g_{\mathrm{GCN}}(\mathbf{L})=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{ k},\quad\text{where }\theta_{k}=\begin{cases}1,&\text{if }k=1,\\ 0,&\text{otherwise.}\end{cases}\] (7)

It should be noted that this equation satisfies the condition described in (6).

SGC is a simplified version of GCN that eliminates the activation function and applies a single linear projection to extract features. This simplification reduces the multiple-layer GCN into a more concise model as \(\mathbf{Z}=\tilde{\mathbf{A}}^{K}\mathbf{X}\mathbf{W}\). Similarly, the graph filter of SGC can be represented as:

\[g_{\mathrm{SGC}}(\mathbf{L})=\tilde{\mathbf{A}}^{K}=\sum_{k=0}^{\infty}\theta _{k}\tilde{\mathbf{A}}^{k},\quad\text{where }\theta_{k}=\begin{cases}1,&\text{if }k=K,\\ 0,&\text{otherwise.}\end{cases}\] (8)

Both GCN and SGC use a monomial to construct the graph filter. Therefore, in the viewpoint of spectral-GNN, their graph filters are too simple to capture the spectral characteristic. Besides, the small eigenvalue vanishes when \(K\) becomes very large, leaving only the largest eigenvalue, which leads to the well-known over-smoothing problem [17].

**PPNP [16].** PPNP uses Personalized PageRank as the graph filter, which balances local information preservation and the utilization of high-order neighbor information. The model of PPNP is \(\mathbf{Z}=\alpha(\mathbf{I}-(1-\alpha)\tilde{\mathbf{A}})^{-1}\mathbf{H}=( \mathbf{I}+\beta\mathbf{L})^{-1}\mathbf{H}\), where \(\mathbf{H}=f(\mathbf{X})\) is a two-layer MLPs and \(\beta=1/\alpha-1\). Hence, the graph filter of PPNP is \(g_{\mathrm{PPNP}}(\mathbf{L})=(\mathbf{I}+\beta\mathbf{L})^{-1}\). Considering its Taylor series, we have

\[g_{\mathrm{PPNP}}(\mathbf{L})=(\mathbf{I}+\beta\mathbf{L})^{-1}=\frac{1}{1+ \beta}\sum_{k=0}^{\infty}\left(\frac{\beta}{1+\beta}\right)^{k}\tilde{ \mathbf{A}}^{k}=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}},\] (9)

where \(\theta_{k}=\beta^{k}/(1+\beta)^{k+1}\). It is straightforward to validate that \(\sum_{k=0}^{\infty}\theta_{k}=1\), and thus the convergence requirement (4) holds. Moreover, the Lipschitz condition is easily verified. Thus PPNP satisfies the criterion of (6). However, the performance of PPNP is heavily dependent on the hyperparameter \(\beta\), which must be carefully tuned to achieve optimal performance.

**DAGNN [19].** DAGNN adaptively adjusts the weight of information aggregation from different neighbors to solve the over-smoothing problem. It designs a parameterized graph filter formulated as a \(K\)-order polynomial:

\[g_{\mathrm{DAGNN}}(\mathbf{L})=\sum_{k=0}^{K}\theta_{k}\tilde{\mathbf{A}}^{k},\quad\text{s.t. }0\leq\theta_{k}\leq 1,\] (10)

where \(\theta_{k}\) is the learnable parameter with bounded constraint. Due to this adaptive learning strategy, DAGGN is able to learn a graph filter more suitable for node classification. The empirical studies suggest DAGNN works well with a proper \(K\). However, as \(K\rightarrow\infty\), the constraint \(0\leq\theta_{k}\leq 1\) cannot guarantee the convergence of the graph filter. It indicates that DAGNN is "inconsistent" with its infinitely deep version. Therefore, it can not be naturally extended to significantly deep GNN.

### Instantiation: Adaptive Power Graph Neural Network

We now introduce a novel GNN following the framework in section 3.1, called Adaptive Power GNN (APGNN). We first consider the following graph filter parameterized by \(\bm{\beta}\) with the form:

\[g_{\bm{\beta}}^{\infty}(\lambda)=\sum_{k=0}^{\infty}\beta_{k}\alpha^{k}(1- \lambda)^{k},\quad\text{where }|\beta_{k}|\leq 1,\ 0<\alpha<1,\] (11)

where the coefficient of the power series \(\theta_{k}=\beta_{k}\alpha^{k}\), with hyper-parameter \(\alpha\in(0,1)\) ensuring the convergence. Immediately, we check the condition of Lemma 1.

\[\|\bm{\theta}\|_{1}=\sum_{k=0}^{\infty}\big{|}\beta_{k}\alpha^{k}\big{|}\leq \sum_{k=0}^{\infty}\alpha^{k}\leq\frac{1}{1-\alpha}.\] (12)Hence, the power series converges on \([0,2]\) absolutely and uniformly. Similarly, the associated matrix series \(g_{\bm{\beta}}^{\infty}(\mathbf{L})=\sum_{k=0}^{\infty}\beta_{k}\alpha^{k}\tilde{ \mathbf{A}}^{k}\) also converges uniformly and absolutely by Theorem 1. Moreover, \(g_{\bm{\beta}}^{\infty}(\cdot)\) is \(\alpha(1-\alpha)^{-2}\)-Lipschitz. To see this, for any \(|\beta_{k}|\leq 1\) and \(1-\lambda\in(-1,1]\), we have

\[|\nabla g_{\bm{\beta}}^{\infty}(\lambda)|=\left|\sum_{k=1}^{\infty}(-1)^{k}k \beta_{k}\alpha^{k}(1-\lambda)^{k-1}\right|\leq\sum_{k=1}^{\infty}k\alpha^{k}= \frac{\alpha}{(1-\alpha)^{2}},\] (13)

which implies the Lipschitz continuous property. Thus, this graph filter fits the requirement of the proposed criterion. However, the model with this graph filter is unavailable in practice as the number of parameters to be learned is infinite. The \(K\)-order truncated polynomial is utilized for substitution, i.e., \(g_{\bm{\beta}}^{K}(\mathbf{L})=\sum_{k=0}^{K}\beta_{k}\alpha^{k}\tilde{\mathbf{ A}}^{k}\). We evaluate the approximation via the upper bound of \(K\)-order truncation error:

\[|g_{\bm{\beta}}^{\infty}(\lambda)-g_{\bm{\beta}}^{K}(\lambda)|\leq\sum_{k=K+1} ^{\infty}\left|\beta_{k}\alpha^{k}(1-\lambda)^{k}\right|\leq\sum_{k=K+1}^{ \infty}\alpha^{k}=\frac{\alpha^{K+1}}{1-\alpha},\] (14)

which uniformly holds for \(\forall\lambda\in[0,2]\). Likewise, the approximation error of matrix series is given by

\[\left\|g_{\bm{\beta}}^{\infty}(\mathbf{L})-g_{\bm{\beta}}^{K}(\mathbf{L}) \right\|_{2}=\left\|\mathbf{U}\left(g_{\bm{\beta}}^{\infty}(\mathbf{\Lambda} )-g_{\bm{\beta}}^{K}(\mathbf{\Lambda})\right)\mathbf{U}^{\top}\right\|_{2}= \sup_{i\in[n]}|g_{\bm{\beta}}^{\infty}(\lambda_{i})-g_{\bm{\beta}}^{K}(\lambda _{i})|\leq\frac{\alpha^{K+1}}{1-\alpha},\] (15)

where \(\lambda_{i}\) denotes the \(i\)-th eigenvalue of \(\mathbf{L}\). This upper bound is independent of the given graph, which can be controlled via tuning \(\alpha\) and \(K\). The higher \(K\) and smaller \(\alpha\) yield a better approximation to the exact graph filter \(g_{\bm{\beta}}^{\infty}(\cdot)\). Nevertheless, the small \(\alpha\) tends to limit the capability of the graph filter. Extremely, \(\alpha\to 0\) gives a trivial function \(g_{\bm{\beta}}^{K}(\lambda)=\beta_{0}\). This suggests that \(\alpha\) should be elaborately tuned to improve the performance.

Though the aforementioned graph filter is primarily motivated via spectral analysis, we can still present the spatial perspective explanation for its design. Existing GNNs aggregate the neighbor information of different hops with certain weights, which could be either manually assigned or learned adaptively. Typically, methods like GPR-GNN [2] and DAGNN [19] that learn the aggregation weight, tend to treat the neighbor's information of different hops equally. That is, the \(k\)-hop's weight are assigned with \(\theta_{k}=\mathcal{O}(1)\) for each \(k\in[K]\). However, it is shown in the previous research that the propagation with the very high-order neighbor potentially leads to the over-smoothing issue [25; 33]. The current methods magnify this flaw of the high-order graph since they cannot distinguish the significance of the information of different hops. This motivates the design of the decay rate in APGNN, i.e., we employ weights with exponential decaying rate by assigning \(\theta_{k}=\mathcal{O}(\alpha^{k})\) for some \(0<\alpha<1\). This approach emphasizes the contribution of lower-order neighbors and restricts the over-weighting of the information from high-order neighbors due to \(\theta_{k}\to 0\) with \(k\rightarrow\infty\). Therefore, it provides more effective aggregation and thus enhances the model's scalability.

To take a further step in the construction of a deep GNN, we introduce a multiple \(P\)-hop strategy for the graph filter of (11), which effectively extends the utmost neighborhood range that the graph filter can perceive by \(P\) times. Consider a different perspective regarding the construction of a filter with

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & Filter function & Setting of \(\bm{\theta}\) & Learnable \(g(\cdot)\) \\ \hline
1-layer GCN & \(g(\mathbf{L})=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{k}\) & \(\theta_{k}=\begin{cases}1,&\text{if }k=1\\ 0,&\text{otherwise}\end{cases}\) & No \\ SGC & \(g(\mathbf{L})=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{k}\) & \(\theta_{k}=\begin{cases}1,&\text{if }k=K\\ 0,&\text{otherwise}\end{cases}\) & No \\ PPNP & \(g(\mathbf{L})=\sum_{k=0}^{\infty}\theta_{k}\tilde{\mathbf{A}}^{k}\) & \(\theta_{k}=\frac{\beta^{k}}{(1+\beta)^{k+1}},\;\beta>0\) & No \\ DAGNN & \(g(\mathbf{L})=\sum_{k=0}^{K}\theta_{k}\tilde{\mathbf{A}}^{k}\) & \(0\leq\theta_{k}\leq 1\) & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 1: Graph filter for various GNNsthe utmost order \(T=KP\). The previous methods can be viewed as a one-hop graph filter by setting \(P=1\). For \(P>1\), the graph filter is able to aggregate information from a larger neighborhood in the same order. In addition, we will illustrate the advantages of this strategy from the perspective of generalization in the following section.

Summarizing the above analysis, we present the following comprehensive architecture of APGNN:

\[\mathbf{Z}=g_{\boldsymbol{\beta}}(\mathbf{L})f(\mathbf{X}),\ \ \ \ \ f(\mathbf{X})=\mathrm{MLP}(\mathbf{X}),\ \ \ \ \ g_{\boldsymbol{\beta}}(\mathbf{L})=\sum_{k=0}^{K}\beta_{k}\alpha^{k}\tilde{ \mathbf{A}}^{kP}.\] (16)

In short, APGNN incorporates the benefits from the decay rate \(\alpha\) that exponentially suppresses the information of extremely high-order neighbors and the multiple \(P\)-hop strategy to enlarge receptive fields. These approaches make it possible to realize a sufficiently deep GNN.

## 4 Generalization analysis

The theoretical analysis of GNN's generalization is widely studied. [31] provides the generalization result of the algorithmic stability of GCN in the discrete graph setting. In contrast, [14] shows the convergence and stability guarantee over the random and continuous graph. In this section, we will present the uniform generalization bound of the proposed GNN learning framework under the continuous setup.

We first introduce some notations for later discussion. Denote \(\mathbf{x}\in\mathcal{X}\) as any samples from the input space \(\mathcal{X}\) (we generally set \(\mathcal{X}\) as a subset of \(\mathbb{R}^{d}\)). Let \(\rho(\cdot)\) be a probability measure defined over \(\mathcal{X}\). Assume \(x_{j}\) is the \(j\)-th coordinate of \(\mathbf{x}\in\mathcal{X}\) and \(\mathbb{E}[x_{j}^{2}]\leq c_{\mathcal{X}}^{2}\) for any \(j\in[d]\). To describe the graph relation between each pair \((\mathbf{x},\mathbf{x}^{\prime})\) over \(\mathcal{X}\times\mathcal{X}\), we define a continuous graph function \(A(\cdot,\cdot):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}_{+}\), and its corresponding degree function is

\[d(\mathbf{x}^{\prime})=\int_{\mathcal{X}}A(\mathbf{x},\mathbf{x}^{\prime}) \mathrm{d}\rho(\mathbf{x}^{\prime}).\] (17)

Different from the setting of [18; 26], we assume \(0\leq A(\mathbf{x},\mathbf{x}^{\prime})\leq c_{U}\), and \(0<c_{L}\leq d(\mathbf{x})\) for any \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\). Therefore, we can define the symmetric normalized graph:

\[\tilde{A}(\mathbf{x},\mathbf{x}^{\prime})=\frac{A(\mathbf{x},\mathbf{x}^{ \prime})}{\sqrt{d(\mathbf{x})d(\mathbf{x}^{\prime})}}.\] (18)

Then the corresponding normalized Laplacian is \(L=I-\tilde{A}\), where \(I\) indicates the identity operator over \(\mathcal{X}\). For a graph filter function \(g_{\boldsymbol{\theta}}(\lambda)=\sum_{k=0}^{K}\theta_{k}(1-\lambda)^{k}\), graph convolution of the continuous graph is defined as the following integral operator:

\[g_{\boldsymbol{\theta}}Lf=\sum_{k=0}^{K}\theta_{k}\tilde{A}^{k}f,\ \ \ \tilde{A}f=\int_{\mathcal{X}}\tilde{A}(\cdot,\mathbf{x})f(\mathbf{x})\mathrm{d} \rho(\mathbf{x}),\] (19)

where \(\tilde{A}^{k}=\tilde{A}^{k-1}\circ\tilde{A}\) denotes \(k\)-order composition of integral operator with \(\tilde{A}^{0}=I\). Note we have \(\sum_{k=0}^{K}\theta_{k}\|\tilde{A}\|\leq\|\boldsymbol{\theta}\|_{1}\leq M\) for any \(K\in\mathbb{N}\), indicating \(\sum_{k=0}^{\infty}\theta_{k}\tilde{A}\) is absolutely summable. This guarantees the existence of graph filter on the continuous graph when \(K\rightarrow\infty\). For convenience in understanding, we provide the analysis on a simplified GNN, where we consider a semi-supervised learning task with two classes, i.e., \(y_{i}\in\mathcal{Y}\triangleq\{-1,1\}\), and utilize linear feature extractor \(f(\mathbf{X})=\mathbf{w}^{\top}\mathbf{X}\). Note that we can still extend our result for \(f(\mathbf{X})=\mathrm{MLP}(\mathbf{X})\) and multi-class cases using the techniques proposed in [1]. With the above setting, the hypothesis set over is described as

\[\mathcal{H}_{\mathcal{X}}=\{h:h(\mathbf{x})=g_{\boldsymbol{\theta}}Lf( \mathbf{x}),\ f(\mathbf{x})=\langle\mathbf{w},\mathbf{x}\rangle,\ \|\mathbf{w}\|_{2}\leq B,\ \|\boldsymbol{\theta}\|_{1}\leq M\}.\] (20)

However, the integral in each hypothesis \(h\in\mathcal{H}_{\mathcal{X}}\) is intractable since the underlying graph function and the data distribution are unknown. Therefore, we should use the "empirical version" of the hypothesis to estimate \(h\in\mathcal{H}_{\mathcal{X}}\). For this reason, we introduce the hypothesis set defined over the observed samples \(S\) and graph \(\mathcal{G}\):

\[\mathcal{H}_{S}=\left\{h:h(\mathbf{x}_{i})=\sum_{j=1}^{n}g_{\boldsymbol{ \theta}}(\mathbf{L})_{ij}\mathbf{x}_{j}^{\top}\mathbf{w},\ \ \ \|\mathbf{w}\|_{2}\leq B,\ \|\boldsymbol{\theta}\|_{1}\leq M\right\}.\] (21)Define the generalization error and the empirical error [21] as follows

\[R(h)=\mathbb{E}_{(\mathbf{x},y)}[1_{yh(\mathbf{x})\leq 0}],\quad\hat{R}(h)=\frac{1 }{n_{l}}\sum_{i=1}^{n_{l}}\min(1,\max(0,1-y_{i}h(\mathbf{x}_{i}))).\] (22)

We have the following theorem on the generalization of the proposed learning paradigm.

**Theorem 2**.: _Suppose \(g_{\bm{\theta}}(\cdot)\) is \(L_{M}\)-Lipschitz. Let \(h_{\mathbf{w},\bm{\theta}}\in\mathcal{H}_{\mathcal{X}}\) and \(h_{\mathbf{w},\bm{\theta}}\in\mathcal{H}_{S}\) share the same parameter \((\mathbf{w},\bm{\theta})\). Then there exists a constant \(C>0\) related to the graph function, with the probability at least \(1-\delta\), the following inequality holds._

\[R(h_{\mathbf{w},\bm{\theta}})\lesssim\hat{R}(\hat{h}_{\mathbf{w},\bm{\theta} })+2BMc_{\mathcal{X}}\sqrt{\frac{2d\log(2K+2)}{n_{l}}}+BCL_{M}dc_{\mathcal{X}} \sqrt{\frac{\log(2/\delta)}{n}}.\] (23)

The proof is given by excess risk decomposition, shown in Appendix. The notation "\(\lesssim\)" denotes "less than or approximately equal to the right-hand side" and guarantees an approximation error of at most \(\mathcal{O}(\sqrt{\frac{\log(1/\tau)}{n_{l}}})\) with a probability of at least \(1-\mathcal{O}(\tau)\). We remind readers the important difference between \(R(h_{\mathbf{w},\bm{\theta}})\) and \(\hat{R}(\hat{h}_{\mathbf{w},\bm{\theta}})\). The former term measures the population error over the whole input space with the **continuous** graph filter \(g_{\bm{\theta}}L\). In contrast, \(\hat{R}(\hat{h}_{\mathbf{w},\bm{\theta}})\) is the empirical risk (i.e., training risk) on the sample set \(S\) with the **discrete** graph filter \(g_{\bm{\theta}}(\mathbf{L})\). \(h_{\mathbf{w},\bm{\theta}}\) shares the same learning parameter with \(\hat{h}_{\mathbf{w},\bm{\theta}}\). Therefore, the minimization of the right-hand-side of (23) w.r.t \((\mathbf{w},\bm{\theta})\) reduces the upper bound of the population error.

We observe the first term of generalization bound is of order \(\mathcal{O}((dn_{l}^{-1}\log K)^{1/2})\), which outlines the model's complexity. Although it becomes infinity when \(K\rightarrow\infty\), the growth of this term is extremely slow as \(K\) increases. In practice, we generally set \(K<n\) since the neighbor information beyond \(n\)-hops is redundant, restricting the complexity away from infinity. Therefore, the generalization of the model is rigorously guaranteed for sufficiently large \(K\), which allows us to construct significantly deep GNN in the proposed framework. We can obtain a more precise estimation for a certain model. In the following proposition, we unveil the generalization of APGNN as a direct application of Theorem 2.

**Proposition 1**.: _Let \(\bm{\beta}\in\mathbb{R}^{K}\) and \(g_{\bm{\beta}}^{K}(\lambda)=\sum_{k=0}^{K}\beta_{k}\alpha^{k}(1-\lambda)^{k}\) where \(0<\alpha<1\) and \(\|\bm{\beta}\|_{\infty}\leq 1\). with the probability at least \(1-\delta\), the following inequality holds._

\[R(h_{\mathbf{w},\bm{\beta}})\lesssim\hat{R}(\hat{h}_{\mathbf{w},\bm{\beta}})+ \frac{2Bc_{\mathcal{X}}(1-\alpha^{K})}{1-\alpha}\sqrt{\frac{2d\log(2K+2)}{n_{ l}}}+\frac{BCdc_{\mathcal{X}}\alpha}{(1-\alpha)^{2}}\sqrt{\frac{\log(2/\delta)}{n}}.\] (24)

Proof.: This is a direct result with \(M=(1-\alpha^{K})/(1-\alpha)\) and \(L_{M}=\alpha/(1-\alpha)^{2}\) in Theorem 2. 

In (24), the complexity term becomes \(\mathcal{O}(\sqrt{\log K}(1-\alpha^{K}))\) with \(K=\lfloor T/P\rfloor\), which is relatively tighter than \(\mathcal{O}(\sqrt{\log K})\). For this term, we promote further discussion with \(P\)-hop. Since it takes \(\lfloor T/P\rfloor\) steps to reach the \(T\)-order graph, the term becomes \(\mathcal{O}(\sqrt{\log\lfloor T/P\rfloor}(1-\alpha^{\lfloor T/P\rfloor}))\). It is observed that the term decreases as \(P\) increases. Therefore, the appropriate \(P\) reduces the bound, explaining the mechanism of the \(P\)-hop method. On the other hand, larger \(\alpha\) leads to a higher bound. From the point of spatial view, the information from high-order neighbors is underused, which limits the range of the graph filter. Thus \(\alpha\) should be moderate to leverage the generalization and the capability of the model.

## 5 Experiment

In this section, we conduct node classification experiments on various benchmark datasets to evaluate the performance of APGNN. Specifically, we compare our method with state-of-the-art methods and display the corresponding learned graph filter on different data sets. Moreover, to validate the theoretical analysis, the influence of parameters \(K\), \(\alpha\), and \(P\) is also investigated in experiments.

### Experiment Setup

**Datasets.** We perform experiments on five benchmark datasets commonly used in node classification tasks. **1). Cora, Citeseer, Pubmed[29; 35]**: These are three standard citation networks where each node is a paper and each edge is a citation link. **2).Wiki-CS[20]**: This dataset defines the computer science articles as nodes, while the hyperlinks are edges. **3). MS Acadamic[16]**: The nodes represent the author and the edges represent the co-authorships. A co-authorship Microsoft Academic Graph, where the nodes are the bag-of-words representation of the papers' abstract and edges are co-authorship. The data statistics and their partitions are presented in Appendix.

**Baselines.** To evaluate the effectiveness of APGNN, we compare it with the following baseline models: 1) MLP [22], a traditional method that does not use graphs, 2) GAT [30] and GraphSAGE [10], spatial methods that aggregate neighborhoods' information, and 3) ChebNet [3], GCN [15], SGC [33], PPNP, APPNP[16], GNN-LF (iteration form), GNN-HF (iteration form) [36], and DAGNN [19], spectral methods analyzing GNNs with graph Fourier transform.

**Settings.** We conducted 10 runs for each method on each dataset, with a hidden dimension of \(64\). For all compared methods, their parameter settings follow the previous practices [19; 36]: the dropout rate is \(0.5\) except for Cora, which had a rate of \(0.8\). Furthermore, the learning rate is \(0.01\) for Cora, Citeseer, and Pubmed, but \(0.03\) for Wiki-CS and \(0.02\) for MS-Academic, while the weight decay is \(0.005\) for Cora and Pubmed, \(0.02\) for Citeseer, \(0.0005\) for Wiki-CS, and \(0.00525\) for MS-Academic. We fix the polynomial order \(K\) to \(10\) in ChebNet, APPNP, GNN-LF, GNN-HF, DAGNN, and APPNP. The best hyperparameters we choose for APGNN are presented in Appendix. To ensure a fair

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{5}{c}{**Dataset**} \\ \cline{2-6}  & **Cora** & **Citeseer** & **Pubmed** & **Wiki-CS** & **MS-Academic** \\ \hline
**MLP** & 57.79\({}_{\pm 0.11}\) & 61.20\({}_{\pm 0.08}\) & 73.23\({}_{\pm 0.05}\) & 65.66\({}_{\pm 0.20}\) & 87.79\({}_{\pm 0.42}\) \\
**ChebNet** & 79.92\({}_{\pm 0.18}\) & 70.90\({}_{\pm 0.37}\) & 76.98\({}_{\pm 0.16}\) & 63.24\({}_{\pm 1.43}\) & 90.76\({}_{\pm 0.73}\) \\
**GCN** & 82.03\({}_{\pm 0.27}\) & 71.05\({}_{\pm 0.33}\) & 79.26\({}_{\pm 0.18}\) & 72.05\({}_{\pm 0.45}\) & 92.07\({}_{\pm 0.13}\) \\
**SGC** & 81.89\({}_{\pm 0.26}\) & 72.18\({}_{\pm 0.24}\) & 78.58\({}_{\pm 0.15}\) & 72.76\({}_{\pm 0.35}\) & 89.01\({}_{\pm 0.40}\) \\
**GAT** & 82.82\({}_{\pm 0.36}\) & 71.96\({}_{\pm 0.39}\) & 79.15\({}_{\pm 0.34}\) & 74.36\({}_{\pm 0.58}\) & 91.86\({}_{\pm 0.27}\) \\
**GraphSage** & 82.14\({}_{\pm 0.25}\) & 71.80\({}_{\pm 0.36}\) & 79.20\({}_{\pm 0.27}\) & 73.17\({}_{\pm 0.41}\) & 91.53\({}_{\pm 0.15}\) \\
**PPNP** & 83.73\({}_{\pm 0.31}\) & 71.74\({}_{\pm 0.44}\) & 80.28\({}_{\pm 0.22}\) & 74.69\({}_{\pm 0.53}\) & 92.58\({}_{\pm 0.06}\) \\
**APPNP** & 83.73\({}_{\pm 0.21}\) & 71.70\({}_{\pm 0.21}\) & 80.07\({}_{\pm 0.21}\) & 74.91\({}_{\pm 0.61}\) & 92.81\({}_{\pm 0.12}\) \\
**GNN-LF(iter)** & 83.83\({}_{\pm 0.36}\) & 71.44\({}_{\pm 0.42}\) & 80.31\({}_{\pm 0.16}\) & 75.19\({}_{\pm 0.49}\) & 92.78\({}_{\pm 0.22}\) \\
**GNN-HF(iter)** & 83.68\({}_{\pm 0.31}\) & 71.58\({}_{\pm 0.36}\) & 79.99\({}_{\pm 0.22}\) & 74.71\({}_{\pm 0.55}\) & 92.72\({}_{\pm 0.31}\) \\
**DAGNN** & 82.70\({}_{\pm 0.17}\) & 71.90\({}_{\pm 0.06}\) & 80.06\({}_{\pm 0.30}\) & 75.63\({}_{\pm 0.48}\) & 92.24\({}_{\pm 0.21}\) \\ \hline
**Ours** & **84.15\({}_{\pm 0.23}\)** & **72.44\({}_{\pm 0.56}\)** & **80.74\({}_{\pm 0.24}\)** & **76.03\({}_{\pm 0.51}\)** & **93.69\({}_{\pm 0.20}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The average accuracy (\(\%\)) and standard deviation (\(\%\)) on five benchmark datasets. The highest accuracy in each column is shown in bold, while the second-best result is underlined.

Figure 2: The graph filters learned on different data sets, with the parameter \(P\) being odd in subfigure (a) and even in subfigure (b).

comparison with the compared methods, we also applied our optimal hyperparameters to them, selecting the maximum value to display.

### Analysis

**Node Classification.** As the metric for evaluation, the mean accuracy of 10 runs is used. We compare the performance of APGNN with other methods on five benchmark datasets. Experiment results are reported in Table 2. We can observe that APGNN achieves the highest accuracy across all five datasets, demonstrating its superior performance.

**Learnable Graph Filters.** Figure 2 shows the graph filters learned on various datasets via APGNN. When the parity of \(P\) varies, the graph filter has a distinctive shape. However, their shapes exhibit minimal impact on their accuracy regardless of the parity of \(P\) according to the experiment results. Moreover, the graph filters of each dataset are plotted in Appendix, more details are included in Appendix. Our results show that the graph filters learned from different datasets vary in detail, even when their parameters have similar parity, demonstrating the efficacy of APGNN in learning task-specific graph filters.

**Polynomial Order \(K\).** To gain insight into the role of polynomial order \(K\), we conduct the experiment tuning \(K\) in \(\{1,2,...,20\}\) on Cora, Citeseer, and Pubmed dataset. Our theoretical analysis supports the observation that a small \(K\) can result in a large truncation error, leading to a low accuracy rate. It can be observed that the accuracy rate has little promotion when \(K\) is larger than \(10\), although at the cost of high computational resources.

**Decay Rate \(\alpha\).** Figure 3 (b) depicts the accuracy curve corresponding to various \(\alpha\) values ranging from \(0.1\) to \(0.9\) and \(0.99\) on Cora, Citeseer and Pubmed datasets. As \(\alpha\) decreases, the classification accuracy initially increases and then declines sharply. This phenomenon verifies the theory that the truncation error decreases as \(\alpha\) decreases, but it leads to a trivial function when \(\alpha\) is extremely small.

\(P\)**-hop strategy.** We investigate the accuracy associated with varying parameters \(P\) taken from the set \(\{1,2,3,4,5,6\}\) when fixing \(T=KP=60\). As we can see in Figure 3 (c), the accuracy increase when \(P>1\). This phenomenon can be attributed to the fact that the generalization bounding decreases when \(P\) increases, which suggests that the \(P\)-hop strategy can effectively explore deeper information with the same computational complexity.

## 6 Conclusion

This paper proposes a universal learning principle for a valid construction of GNN. An instantiation named APGNN is proposed to verify the effectiveness of our framework. APGNN employs a decay rate and a multiple \(P\)-hop strategy to learn the coefficients adaptively, which can efficiently aggregate the information from high-order neighbors. We present a theoretical analysis of the generalization capabilities of both our framework and APGNN, which provides a learning guarantee. Comprehensive experiments show the superior performance of APGNN. In the future, it is worth exploring diverse graph filters based on the proposed principle. As shown in the generalization analysis, the upper bound of the model complexity relies on \(\mathcal{O}(\sqrt{\log K})\). How to devise the GNN with complexity free of the hyperparameter \(K\) is also a meaningful research direction.

Figure 3: Accuracy with different (a) \(K\). (b) \(\alpha\). (c) \(P\) (for fixing \(T\)).

## References

* [1]P. L. Bartlett, D. J. Foster, and M. J. Telgarsky (2017) Spectrally-normalized margin bounds for neural networks. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 30, pp. 6241-6250. Cited by: SS1.
* [2]E. Chien, J. Peng, P. Li, and O. Milenkovic (2021) Adaptive universal generalized pagerank graph neural network. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [3]M. Defferrard, X. Bresson, and P. Vandergheynst (2016) Convolutional neural networks on graphs with fast localized spectral filtering. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 29, pp. 2016. Cited by: SS1.
* [4]L. Deng, D. Lian, C. Wu, and E. Chen (2022) Graph convolution network based recommender systems: learning guarantee and item mixture powered strategy. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 35, pp. 3900-3912. Cited by: SS1.
* [5]J. Feng, Y. Chen, F. Li, A. Sarkar, and M. Zhang (2022) How powerful are k-hop message passing graph neural networks. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 35, pp. 4776-4790. Cited by: SS1.
* [6]F. Gama, J. Bruna, and A. Ribeiro (2020) Stability properties of graph neural networks. IEEE Transactions on Signal Processing68, pp. 5680-5695. Cited by: SS1.
* [7]F. Giuliari, G. Skenderi, M. Cristani, Y. Wang, and A. Del Bue (2022) Spatial commonsense graph for object localisation in partial scenes. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19496-19505. Cited by: SS1.
* [8]K. Guo, Y. Hu, Y. Sun, S. Qian, J. Gao, and B. Yin (2021) Hierarchical graph convolution network for traffic forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Vol. 35, pp. 151-159. Cited by: SS1.
* [9]S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan (2019) Attention based spatial-temporal graph convolutional networks for traffic flow forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Vol. 33, pp. 922-929. Cited by: SS1.
* [10]W. Hamilton, Z. Ying, and J. Leskovec (2017) Inductive representation learning on large graphs. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 30, pp.. Cited by: SS1.
* [11]K. Han, Y. Wang, J. Guo, Y. Tang, and E. Wu (2022) Vision GNN: an image is worth graph of nodes. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 35, pp. 8291-8303. Cited by: SS1.
* [12]M. He, Z. Wei, z. Huang, and H. Xu (2021) Bernnet: learning arbitrary graph spectral filters via bernstein approximation. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 14239-14251. Cited by: SS1.
* [13]X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang (2020) LightGCN: simplifying and powering graph convolution network for recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp. 639-648. Cited by: SS1.
* [14]N. Keriven, A. Bietti, and S. Vaiter (2020) Convergence and stability of graph convolutional networks on large random graphs. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Vol. 33, pp. 21512-21523. Cited by: SS1.
* [15]T. N. Kipf and M. Welling (2017) Semi-supervised classification with graph convolutional networks. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [16]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [17]Q. Li, Z. Han, and X. Wu (2018) Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Cited by: SS1.
* [18]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [19]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [20]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [21]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [22]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [23]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [24]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [25]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [26]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [27]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [28]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [29]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [30]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [31]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [32]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [33]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [34]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [35]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [36]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [37]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [38]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [39]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [40]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [41]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [42]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [43]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerankankankankank. In Proceedings of the International Conference on Learning Representations (ICLR), Cited by: SS1.
* [44]J. Klicpera, A. Bojchevski, and S. Gunnemann (2019) Predict then propagate: graph neural networks meet personalized pagerank* [18] Shaojie Li, Sheng Ouyang, and Yong Liu. Understanding the generalization performance of spectral clustering algorithms. _arXiv preprint arXiv:2205.00281_, 2022.
* [19] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)_, page 338-348, 2020.
* [20] Peter Mernyei and Catalina Cangea. Wiki-CS: A wikipedia-based benchmark for graph neural networks. _arXiv preprint arXiv:2007.02901_, 2020.
* [21] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [22] Sankar K Pal and Sushmita Mitra. Multilayer perceptron, fuzzy sets, and classification. _IEEE Transactions on neural networks_, 3(5):683-697, 1992.
* [23] Yitong Pang, Lingfei Wu, Qi Shen, Yiming Zhang, Zhihua Wei, Fangli Xu, Ethan Chang, Bo Long, and Jian Pei. Heterogeneous global graph neural networks for personalized session-based recommendation. In _Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)_, page 775-783, 2022.
* [24] Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower. Training robust neural networks using lipschitz bounds. _IEEE Control Systems Letters_, 6:121-126, 2021.
* [25] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* [26] Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. _Journal of Machine Learning Research (JMLR)_, 11(2), 2010.
* [27] Aliaksei Sandryhaila and Jose M. F. Moura. Discrete signal processing on graphs: Graph filters. In _Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 6163-6166, 2013.
* [28] Aliaksei Sandryhaila and Jose M. F. Moura. Discrete signal processing on graphs: Graph fourier transform. In _Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 6167-6170, 2013.
* [29] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI Magazine_, 29(3):93, 2008.
* [30] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2018.
* [31] Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural networks. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)_, pages 1539-1548, 2019.
* [32] Xiyuan Wang and Muhan Zhang. How powerful are spectral graph neural networks. In _Proceedings of the International Conference on International Conference on Machine Learning (ICML)_, volume 162, pages 23341-23362, 2022.
* [33] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _Proceedings of the International Conference on International Conference on Machine Learning (ICML)_, 2019.
* [34] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2019.
* [35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In _Proceedings of the International Conference on International Conference on Machine Learning (ICML)_, page 40-48, 2016.
* [36] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural networks with an optimization framework. In _Proceedings of The International World Wide Web Conference (WWW)_, page 1215-1226, 2021.

* [37] Stefano Zorzi, Shabab Bazrafkan, Stefan Habenschuss, and Friedrich Fraundorfer. PolyWorld: Polygonal building extraction with graph neural networks in satellite images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1848-1857, 2022.