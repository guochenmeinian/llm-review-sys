# Latent Neural PDE Solver for

Time-dependent Systems

Zijie Li, Saurabh Patil, Dule Shu, Amir Barati Farimani

Carnegie Mellon University

Mechanical Engineering Department

{zijieli, ssp2, dules}@andrew.cmu.edu & barati@cmu.edu

Equal contribution.

###### Abstract

Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on the high-dimensional discretized field, we propose to learn the dynamics of the system in the latent space with much coarser discretization. In our proposed framework, a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost with a fine discretization. We study the capability of the proposed framework on 2D/3D fluid flows and showcase that it has competitive performance compared to the model that operates on full-order space.

## 1 Introduction

Many intricate physical processes, from the interaction of protein dynamics to the movement of a celestial body, can be described by time-dependent partial differential equations (PDEs). The simulation of these processes is often conducted by solving these equations numerically, which requires fine discretization to resolve the necessary spatiotemporal domain to reach convergence. Deep neural network surrogates [5, 42, 48, 61, 65, 71] recently emerged as a computationally less-expensive alternative, with the potential to improve the efficiency of simulation by relaxing the requirement for fine discretization and attaining a higher accuracy on coarser grids compared to classical numerical solver [5, 42, 75].

For time-dependent systems, many neural-network-based models address the problem by approximating the solution operator \(\mathcal{G}\) that maps the state \(u_{t}\) to \(u_{t+\Delta t}\), where the input and output are sampled on discretization grid \(\{D_{i},D_{h}\}\) respectively. The input discretization grid can either remain unchanged between every layer inside the network [5, 9, 42], or fit into a hierarchical structure [19, 44, 58, 64, 79, 87] that resembles the V-Cycle in classical multi-grid method. Hierarchical network structures have been a common model architectural choice in the field of image segmentation [70] and generation [25] given their capability for utilizing multi-scale information.

In contrast to the aforementioned approaches especially those that utilize a hierarchical grid structure, our work studies the effect of decoupling dynamics prediction from upsampling/downsampling processes. Specifically, the neural network for predicting the forward dynamics (which we defined as a propagator) only operates on the coarsest resolution, while a deep autoencoder is pre-trained to compress the data from the original discretization grid \(D_{i}\) to the coarse grid \(D_{l}\) (e.g. from a \(64\times 64\) grid to an \(8\times 8\) grid). As the propagator network operates on a lower dimensional space, the trainingcost is greatly reduced and can be potentially adapted to unrolled training with a longer rollout, which is often observed to be helpful to long-term stability [14; 21]. We parameterize the model with a convolutional neural network along with several other components that are popular in neural PDE solvers, including spectral convolution and several variants of attention. We test the proposed framework on 2D and 3D time-dependent PDEs with uniform grids and showcase that the model can achieve efficient data compression and accurate prediction of forward dynamics.

## 2 Related works

Neural PDE solverNeural PDE solvers can be categorized into the following groups based on their model design. The first group employs neural networks with mesh-specific architectures, such as convolutional layers for uniform meshes or graph layers for irregular meshes. These networks learn spatiotemporal correlations within PDE data without the knowledge of the underlying equations [5; 19; 28; 36; 38; 47; 58; 62; 63; 72; 75; 79; 82; 86]. Such a data-driven approach is useful for systems with unknown or partially known physics, such as large-scale climate modeling [33; 59; 66]. The second group, known as Physics-Informed Neural Networks (PINNs) [8; 9; 20; 22; 30; 40; 41; 49; 56; 65; 76; 93], treats neural networks as a representation of the solution function. PINNs incorporate knowledge of governing equations into the loss function, including PDE residuals and consistency with boundary and initial conditions. Unlike the first group, PINNs can be trained solely on equation loss and do not necessarily require input-target data pairs. The third group, known as the neural operators[1; 3; 4; 9; 17; 22; 29; 31; 32; 42; 44; 45; 48; 50; 54], is designed to learn the mapping between function spaces. For a certain family of PDEs, neural operators can generalize and adapt to multiple discretizations without retraining. DeepONet [48] presents a pragmatic implementation of the universal operator approximation theorem[10]. Meanwhile, the concurrent research [43] in the form of the graph neural operator proposes a trainable kernel integral for approximating solution operators in parametric PDEs. Their follow-up work, Fourier Neural Operator (FNO) [42], has demonstrated high accuracy and efficiency in solving specific types of problems. Different function bases such as Fourier[15; 42; 80; 89] / wavelet bases[17], the column vectors from attention layers[9; 40], or Green's function approximation[2; 78], have been be used for operator learning. For more physically consistent predictions[46; 88], neural operator training can be combined with PINN principles.

Two-stage model for image compression and synthesisThe utilization of a two-stage model for image synthesis has gained significant attention in the field of computer vision in recent years. VQ-VAEs[67] adopts a two-stage approach for generating images within a latent space. In the initial stage, the approach compresses images into this latent space, using model components such as an encoder, a codebook, and a decoder. Subsequently, in the second stage, a latent model is introduced to predict the latent characteristics of the compressed images, and the decoder from the first stage is used to transform the predicted latent representation back into image pixels. VQ-GANs[13] is developed to scale autoregressive transformers to large image generation by employing adversarial and perceptual objectives for first-stage training. Most recently, several works have developed latent diffusion models with promising results ranging from image[68], point clouds[92] to text generation[35]. Within the domain of neural PDE solvers, the widely employed Encoder-Process-Decoder (EPD) scheme, used to map the input solution at time \(t\) to the subsequent time step, stands as a conventional and direct method [6; 27; 57; 61; 71; 74]. As an alternative, researchers have explored propagating the system dynamics in the latent space, aiming to diminish computational complexity and minimize memory usage [34; 90]. Evolving the system dynamics in latent space can involve utilization of recurrent neural networks like LSTM [90], linear propagators grounded in the assumptions of the Koopman operator [37; 51; 52; 55; 77], attention mechanism [24], recurrent MLPs [39] or state-space model [60]. In this work, we propose to use an autoencoder to embed inputs into the latent space, and a simple yet effective convolutional propagator is employed to learn the dynamics of the time-dependent system within this latent space.

## 3 Methodology

### Problem definition

We are interested in solving time-dependent PDEs of the following form:

\[\frac{\partial u(\mathbf{x},t)}{\partial t} =F(u(\mathbf{x},t),t),\quad\mathbf{x}\in\Omega,t\in[0,T] \tag{1}\] \[u(\mathbf{x},0) =u_{0}(\mathbf{x}),\quad\mathbf{x}\in\Omega, \tag{2}\]

where \(T\) denotes the time horizon and some boundary condition for \(\mathbf{x}\in\partial\Omega\) is provided _a priori_. To solve this initial value problem, a neural network is trained to approximate the following mapping:

\[u(\mathbf{x},t+\Delta t)=\mathcal{A}(u(\mathbf{x},t)), \tag{3}\]

with a fixed \(\Delta t\), and the system is assumed to be Markovian such that \(u(\mathbf{x},t+2\Delta t)=\mathcal{A}(\mathcal{A}(u(\mathbf{x},t)))\).

In practice, the function of interest at a particular time step \(u(\cdot,t)\) is sampled on a \(m\)-point discretization grid \(D\). For a hierarchical model like U-Net, the grid will be altered internally between different layers and the mapping \(\mathcal{A}\) is a composition of a sequence of mapping \(\{\mathcal{A}_{0},\ldots,\mathcal{A}_{l}\}\) which operates on grids \(\{D_{0},\ldots,D_{l}\}\) with \(D_{0}=D\) and the number of grid points \(m_{l}<m_{l-1}<\cdots<m_{0}\). In contrast to the aforementioned hierarchical model, we propose to learn \(\mathcal{A}\) on the coarsest grid \(D_{l}\).

### Autoencoder for dimension reduction

One of the most straightforward ways to project the function from the original grid to a coarser grid is interpolation (_e.g._, bicubic interpolation). However, interpolation can result in significant information loss about the function, as a coarser grid can only evaluate a limited bandwidth and cannot distinguish frequencies that are higher than the Nyquist frequency. To achieve a less lossy compression of the input, we train an encoder network \(\phi\) to project the input into latent space when coarsening its spatial grid. In the meantime, we train the decoder network \(\psi\) to recover the input from the latent embedding that are represented on the coarse grid. The goal of training these two networks is to achieve data compression without too much loss of information such that their composition approximates an identity mapping: \(I\approx\phi\circ\psi\).

In this work, we exploit the fact that the grid structure we are dealing with is uniform and that the majority part of the autoencoder is parameterized with convolutional neural networks (CNN) which are effective for compressing imagery data [13; 69; 84]. On top of the CNNs modules, we also introduced several other modules that have been shown to be effective for PDE surrogate modeling.

Spectral convolutionSpectral convolution layer is first proposed in Fourier Neural Operator [42] as a parameterization of the learnable kernel integral [32]. It applies a discrete Fourier transform to the input and then multiplies the \(k\)-lowest modes with learnable complex weights. Given input function \(u_{l}\), the spectral convolution computes the kernel integral as follows:

\[u_{l+1}(x)=\int_{\Omega}\kappa(x,y)u_{l}(y)dy=\sum_{\xi_{1}=0}^{\xi_{1}^{\max }}\ldots\sum_{\xi_{n}=0}^{\xi_{n}^{\max}}W_{\mathbf{j}}\mathbf{c}_{j}f_{j}(x),\quad j =\xi_{1}\xi_{2}\ldots\xi_{n} \tag{4}\]

Figure 1: (a) An autoencoder is trained to project the input field to latent field with much coarser discretization. (b) A neural network is trained to predict the latent field at different time steps.

where \(W\in\mathbb{C}(\xi_{n}^{\max}\times\xi_{n}^{\max}\times...\xi_{n}^{\max})\times d_{ c}\times d_{c}\) is the learnable weight, \(f_{j}\) is the \(j\)-th Fourier basis function: \(\exp\left(2i\pi\sum_{d}\frac{x_{d}\xi_{d}}{m_{d}}\right)\) with \(m_{d}\) being the resolution along the \(d\)-th dimension, \(x_{d}\) being the coordinate for \(d\)-th dimension, and \(\mathbf{c}_{j}=<u_{l},f_{j}>\) denotes the channel-wise inner product between input function and Fourier series. Unlike the CNN layer, spectral convolution is able to capture multi-scale features that correspond to different frequencies within a single layer. It is also computationally efficient on a uniform grid as the \(c_{j}\) can be efficiently computed via fast Fourier Transformation (FFT). In addition, Gupta and Brandstetter [19] hypothesized that suppressing high-frequency modes with spectral convolution before downsampling can further improve the performance of the network.

AttentionScaled-dot product attention [85] has become the state-of-the-art models for natural language processing [7, 11] and computer vision tasks [12] with its capability to capture non-local interactions and compute data-dependent weights. Attention is also closely related to the kernel integral [32] defined in the previous subsection, with its theoretical property on specific PDE problems analyzed in several prior works[9, 16, 31]. Given the \(i\)-th input feature vector \(\mathbf{u}_{i}\) with channel size \(d_{c}\), the (self-)attention can be defined as:

\[\mathbf{z}_{i}=\sum_{j=1}^{m}\alpha_{ij}\mathbf{v}_{j},\quad\alpha_{ij}=\frac {\exp\left(\mathbf{q}_{i}\cdot\mathbf{k}_{j}/\sqrt{d_{c}}\right)}{\sum_{s=1}^ {m}\exp\left(\mathbf{q}_{i}\cdot\mathbf{k}_{s}/\sqrt{d_{c}}\right)}, \tag{5}\]

where: \(\mathbf{q}_{i}=W_{q}\mathbf{u}_{i},\mathbf{k}_{i}=W_{k}\mathbf{u}_{i},\mathbf{ v}_{i}=W_{v}\mathbf{u}_{i}\) respectively, and \(\{W_{q},W_{k},W_{v}\}\in\mathbb{R}^{d_{c}\times d_{c}}\) are learnable weights. We plug the self-attention layer into the decoder and investigate its effect on learning the latent embedding.

## 4 Experiments

We test out the proposed model on two time-dependent fluid problems and compared our model to a state-of-the-art neural PDE solver Fourier Neural Operator [42]. For all the problems we sample the data on a spatial grid of resolution \(64\) along each axis.

### Datasets

2D incompressible flowThe 2D incompressible flow we considered here is the 2D flow dataset proposed in Li et al. [42], which is based on 2D Navier-Stokes equation under vorticity formulation. The vorticity form reads as:

\[\begin{split}\frac{\partial\omega(\mathbf{x},t)}{\partial t}+ \mathbf{u}(\mathbf{x},t)\cdot\nabla\omega(\mathbf{x},t)&=\nu \nabla^{2}\omega(\mathbf{x},t)+f(\mathbf{x}),\quad\mathbf{x}\in(0,1)^{2},t\in (0,T],\\ \nabla\cdot\mathbf{u}(\mathbf{x},t)&=0,\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \mathbf{x}\in(0,1)^{2},t\in[0,T],\\ \omega(\mathbf{x},0)&=\omega_{0}(\mathbf{x}),\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\mathbf{x} \in(0,1)^{2},\end{split} \tag{6}\]

where \(\omega\) denotes vorticity: \(\omega:=\nabla\times u\), the initial condition \(\omega_{0}\) is sampled from the Gaussian random field, the boundary condition is periodic, the viscosity coefficient \(\nu\) is \(1e-4\) and the forcing term is defined as: \(f(\mathbf{x})=0.1(\sin 2\pi(x_{1}+x_{2})+\cos 2\pi(x_{1}+x_{2}))\). We are interested in learning to simulate the system (by predicting vorticity) from \(t=5\) to \(t=35\) with \(30\) seconds of time duration. The reference numerical simulation data is generated via the pseudo-spectral method. The dataset contains \(1000\) trajectories where we use \(900\) for training and \(100\) for testing.

3D smoke buoyancyThe second benchmark problem is 3D Navier-Stokes equation coupled with advection equation proposed in Li et al. [41] and similar 2D cases have been studied in prior works [3, 18, 81]. The equation describes the motion of rising smoke in a closed box,

\[\begin{split}\frac{\partial\mathbf{u}(\mathbf{x},t)}{\partial t} +\mathbf{u}(\mathbf{x},t)\cdot\nabla\mathbf{u}(\mathbf{x},t)&= \nu\nabla^{2}\mathbf{u}(\mathbf{x},t)-\frac{1}{\rho}\nabla p(\mathbf{x},t)+ \mathbf{f}(\mathbf{x},t),\quad\mathbf{x}\in(0,L)^{3},t\in(0,T],\\ \frac{\partial d(\mathbf{x},t)}{\partial t}+\mathbf{u}(\mathbf{ x},t)\cdot\nabla d(\mathbf{x},t)&=0,\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\where \(d\) depicts a marker field for smoke and is subjects to the Neumann boundary condition: \(\partial d/\partial n=0\), the velocity field \(\mathbf{u}\) is under Dirichlet boundary condition: \(\mathbf{u}(\mathbf{x},t)=0,\mathbf{x}\in\partial\Omega\), the initial condition of the marker field \(d\) is sampled from a random field, the forcing term is based on the Bousinessq model \(\mathbf{f}(\mathbf{x},t)=[0,0,\eta d(\mathbf{x},t)]\) with \(\eta\) being the buoyancy factor. We study the case with viscosity coefficient \(\nu=0.003\) and buoyancy factor \(\eta=0.50\). The goal is to predict the marker field and velocity field from \(t=0\) to \(t=12\), with domain size \(L=8\). The reference simulation data is generated using _phiflow_[26] with pressure projection and Macormack advection scheme [73]. The dataset contains \(2200\) trajectories among which we use \(2000\) for training and \(200\) for testing.

### Implementation

AutoencoderThe encoder and decoder are mainly built upon convolutional layers. Internally they comprise a stack of downsampling/upsampling blocks, where each block downsamples/upsamples the spatial resolution by a factor of \(2\). Each block contains a residual convolution block and a downsampling/upsampling layer. The residual convolution block consists of group normalization [91] and two \(3\times 3\) convolution layers. the downsampling layer uses a \(3\times 3\) convolution layer with a stride of \(2\), and the upsampling layer upsamples the resolution by using nearest interpolation followed by a \(3\times 3\) convolution layer. We also investigate the influence of inserting spectral convolution layers into each downsampling block and add self-attention layers to the lowest resolution following prior works on image synthesis [13, 68]. For the \(2\)D problem, we set the latent resolution to \(8\times 8\) and the latent dimension to \(16\). For the \(3\)D problem, we set the latent resolution to \(16\times 16\times 16\) and the latent dimension to \(64\). In addition, on the \(3\)D problem, we use the multi-dimensional factorized attention [41] instead of standard attention to reduce the computational cost.

PropagatorWe use a simple residual convolution network [23] to forecast the forward dynamics in the latent space, where each residual block contains a group normalization layer and three convolution layers with \(3\times 3\) convolution kernels. We also employ dilated convolution for the middle convolution layer to capture longer-range interaction. For the \(2\)D problem, we use \(3\) residual blocks with network width \(128\). For the \(3\)D problem, we use \(4\) residual blocks.

BaselineOn the \(2\)D problem, we tested out two versions of the FNO. The first version is based on the hyperparameter provided in the original paper [42], where the model width is \(32\) and \(8\) lowest modes are used at each spectral convolution layer. We also test out a larger version with a width of \(64\) and use a mode number of \(16\). On the \(3\)D problem, we use a width of \(64\) and a mode number of \(12\) as increasing the mode number for 3D spectral convolution will drastically increase the model parameter (by cubic).

TrainingWe first train the autoencoder by minimizing the relative \(L^{2}\) reconstruction loss for around 150k iterations with constant learning rate \(3e-5\) using batch size of \(64/16\) respectively for 2D/3D. We then train the propagator by minimizing the mean squared error between predicted embeddings and embedding of reference data for another 150k iteration with a learning rate of \(5e-4\) and a cosine annealing schedule. For FNO we train it with a learning rate \(5e-4\) and a cosine annealing scheduling to minimize the relative \(L^{2}\) prediction loss. The total training iterations are also set to 150k. Different from the original FNO paper, we do not use full rollout during training as we observe reducing the rollout steps during training can significantly improve the performance on NS2D. * We rollout for 2 steps for all models unless stated otherwise.

Footnote *: On 2D Navier-Stokes, FNO (8 modes) has a prediction error of \(0.2596\) if using fully rollout training, whereas rolling out for 2 steps yields an error of \(0.1689\).

### Results

In this section, we present the comparison between the proposed framework and other models. We observe that the proposed model consistently outperforms FNO which operates on the full mesh space and for lower-dimensional problems like 2D fluid flow the performance gap is more significant. On more complex 3D flow, the model is able to compress the original data to a much coarser (4 times coarser) resolution and learns to predict with accuracy on par with full-order models. Furthermore, as the temporal model operates on a much coarser discretization, we can afford longer rollout training to allow gradient propagated from farther future which can further improve the model's performance on predicting the equilibriium state of the smoke marker field (Figure 1(c)). (Sample visualization of the best model's prediction are presented in Appendix A)

We also study how different training strategies will influence the model's performance (Figure 1(b)). We maintain consistent hyperparameters and explore three training strategies: the two-stage method discussed in the previous subsection (referred to as "two-stage"), training the autoencoder and propagator simultaneously by minimizing both reconstruction and prediction loss jointly (referred to as "combined"), and considering the autoencoder and propagator as an unified entity to predict the subsequent step (referred to as "autoregressive"). We also compare two-stage training to Dilated-ResNet [74] that employs a Encode-Process-Decode (EPD) scheme [6, 61, 71]. We find that two-stage training yields the best performance compared to other strategies, which indicates the advantage of two-stage training in obtaining high-quality coarse-graining of the system.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multirow{2}{*}{FNO2D} & \multicolumn{2}{c}{Latent-PDE 2D} & \multirow{2}{*}{FNO3D} & \multicolumn{2}{c}{Latent-PDE 3D} \\ \cline{3-3} \cline{5-6}  & & Autoencoder & & & Autoencoder & Propagator \\ \hline Fwd + Bwd time (sec) & 0.067 & 0.103 & 0.013 & 2.223 & 1.375 & 0.372 \\ Memory (GB) & 1.87 & 2.54 & 0.25 & 33.33 & 37.15 & 8.10 \\ \# of params (M) & 16.8 & 9.7 & 1.4 & 226.5 & 38.8 & 5.4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Computational cost of different models’ training. 2D benchmark is carried out on RTX 3090, using a batch size of 64. 3D benchmark is carried out on A6000, using a batch size of 16.

Figure 2: Quantitative study on model’s performance. Latent-PDE denotes our proposed latent neural PDE solver. “Base” model contains only residual convolutional blocks and fully-connected layers. “CoarseAttn” means we add self-attention to the bottleneck part of the model. “Full-Fourier” means we add spectral convolution layers at the top two downsampling blocks in the encoder and decoder of “CoarseAttn” model. “Res” means we replace spectral convolution layer with residual convolutional blocks. x-step models are rollout for x steps during the training.

Compared to FNO that has log-linear complexity with respect to the grid size, the training of the proposed model is relatively slower when combining the time cost for autoencoder training and temporal model training. However, since the temporal model training is much more efficient in latent-pde solver, its training can be less costly on system that requires rolling out for more steps during training.

## 5 Conclusion

In this work, we study a straightforward yet effective data-driven framework for predicting time-dependent PDEs. We show that training the temporal model in the mesh-reduced space improves the computation efficiency and is beneficial for problems that feature latent dynamics distributed on a low-dimensional manifold. The observation in this study is also in alignment with the recent success of a series of image synthesis models that learn the generative model in the latent space instead of pixel space [13; 68; 83]. As this work only considers uniform mesh, an interesting future direction would be the extension to arbitrary meshes and geometries.

## References

* Bhattacharya et al. [2021] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. Model reduction and neural networks for parametric pdes, 2021.
* Boulle et al. [2022] Nicolas Boulle, Seick Kim, Tianyi Shi, and Alex Townsend. Learning green's functions associated with time-dependent partial differential equations. _Journal of Machine Learning Research_, 23(218):1-34, 2022.
* Brandstetter et al. [2022] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford neural layers for pde modeling. _arXiv preprint arXiv:2209.04934_, 2022.
* Brandstetter et al. [2022] Johannes Brandstetter, Max Welling, and Daniel E Worrall. Lie point symmetry data augmentation for neural PDE solvers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 2241-2256. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/brandstetter22a.html](https://proceedings.mlr.press/v162/brandstetter22a.html).
* Brandstetter et al. [2022] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. _arXiv preprint arXiv:2202.03376_, 2022.
* Brandstetter et al. [2022] Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=vSix3HPYKSU](https://openreview.net/forum?id=vSix3HPYKSU).
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Cai et al. [2021] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (pinns) for fluid mechanics: A review. _Acta Mechanica Sinica_, 37(12):1727-1738, 2021.
* Cao [2021] Shuhao Cao. Choose a transformer: Fourier or galerkin. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 24924-24940. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf).
* Chen and Chen [1995] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. _IEEE Transactions on Neural Networks_, 6(4):911-917, 1995. doi: 10.1109/72.392253.

* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Esser et al. [2021] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2021.
* Geneva and Zabaras [2022] Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. _Neural Networks_, 146:272-289, 2022.
* Guibas et al. [2021] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. _arXiv preprint arXiv:2111.13587_, 2021.
* Guo et al. [2022] Ruchi Guo, Shuhao Cao, and Long Chen. Transformer meets boundary value inverse problems. _arXiv preprint arXiv:2209.14977_, 2022.
* Gupta et al. [2021] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equations, 2021.
* Gupta and Brandstetter [2022] Jayesh K. Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling, 2022.
* Gupta and Brandstetter [2022] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling. _arXiv preprint arXiv:2209.15616_, 2022.
* Han et al. [2018] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. _Proceedings of the National Academy of Sciences_, 115(34):8505-8510, 2018.
* Han et al. [2022] Xu Han, Han Gao, Tobias Pfaff, Jian-Xun Wang, and Li-Ping Liu. Predicting physics in mesh-reduced space with temporal attention. _arXiv preprint arXiv:2201.09113_, 2022.
* Hao et al. [2023] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. _arXiv preprint arXiv:2302.14376_, 2023.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* Hemmasian and Farimani [2023] AmirPouya Hemmasian and Amir Barati Farimani. Reduced-order modeling of fluid flows with transformers. _Physics of Fluids_, 35(5), 2023.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.
* Holl et al. [2020] Philipp Holl, Vladlen Koltun, Kiwon Um, and Nils Thuerey. Pilflow: A Differentiable PDE Solving Framework for Deep Learning via Physical Simulations. In _NeurIPS Workshop_, 2020. URL [http://montrealrobotics.ca/diffcvgp/assets/papers/3.pdf](http://montrealrobotics.ca/diffcvgp/assets/papers/3.pdf).
* Hsieh et al. [2019] Jun-Ting Hsieh, Shengjia Zhao, Stephan Eismann, Lucia Mirabella, and Stefano Ermon. Learning neural PDE solvers with convergence guarantees. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=rklawh0qK7](https://openreview.net/forum?id=rklawh0qK7).
* JANNY et al. [2023] Steven JANNY, Aurelien Beneteau, Madiha Nadri, Julie Digne, Nicolas THOME, and Christian Wolf. EAGLE: Large-scale learning of turbulent fluid dynamics with mesh transformers. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=mfIX4QpsARJ](https://openreview.net/forum?id=mfIX4QpsARJ).

* Jin et al. [2022] Pengzhan Jin, Shuai Meng, and Lu Lu. Mionet: Learning multiple-input operators via tensor product. _SIAM Journal on Scientific Computing_, 44(6):A3490-A3514, 2022.
* Karniadakis et al. [2021] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* Kissas et al. [2022] Georgios Kissas, Jacob Seidman, Leonardo Ferreira Guilhoto, Victor M. Preciado, George J. Pappas, and Paris Perdikaris. Learning operators with coupled attention, 2022.
* Kovachki et al. [2021] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021.
* Lam et al. [2022] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, and Peter Battaglia. Graphcast: Learning skillful medium-range global weather forecasting, 2022.
* Lee and Carlberg [2021] Kookjin Lee and Kevin T. Carlberg. Deep conservation: A latent-dynamics model for exact satisfaction of physical conservation laws. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(1):277-285, May 2021. URL [https://ojs.aaai.org/index.php/AAAI/article/view/16102](https://ojs.aaai.org/index.php/AAAI/article/view/16102).
* Li et al. [2022] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. _Advances in Neural Information Processing Systems_, 35:4328-4343, 2022.
* Li et al. [2019] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids, 2019.
* Li et al. [2020] Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional koopman operators for model-based control. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=H1ldzA4tPr](https://openreview.net/forum?id=H1ldzA4tPr).
* Li and Farimani [2022] Zijie Li and Amir Barati Farimani. Graph neural network-accelerated lagrangian fluid simulation. _Computers & Graphics_, 103:201-211, 2022. ISSN 0097-8493. doi: [https://doi.org/10.1016/j.cag.2022.02.004](https://doi.org/10.1016/j.cag.2022.02.004). URL [https://www.sciencedirect.com/science/article/pii/S0097849322000206](https://www.sciencedirect.com/science/article/pii/S0097849322000206).
* Li et al. [2022] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _arXiv preprint arXiv:2205.13671_, 2022.
* Li et al. [2023] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=EPPqt3uERT](https://openreview.net/forum?id=EPPqt3uERT).
* Li et al. [2023] Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for pde surrogate modeling, 2023.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020.

* Li et al. [2023] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations, 2023.
* Lotzsch et al. [2022] Winfried Lotzsch, Simon Ohler, and Johannes Otterbach. Learning the solution operator of boundary value problems using graph neural networks. In _ICML 2022 2nd AI for Science Workshop_, 2022. URL [https://openreview.net/forum?id=4vx9FQA7wiC](https://openreview.net/forum?id=4vx9FQA7wiC).
* Lu et al. [2019] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Lu et al. [2021] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. _SIAM review_, 63(1):208-228, 2021.
* Lu et al. [2022] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, apr 2022. doi: 10.1016/j.cma.2022.114778. URL [https://doi.org/10.1016%2Fj.cma.2022.114778](https://doi.org/10.1016%2Fj.cma.2022.114778).
* Lusch et al. [2018] Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. _Nature Communications_, 9(1):4950, Nov 2018. ISSN 2041-1723. doi: 10.1038/s41467-018-07210-0. URL [https://doi.org/10.1038/s41467-018-07210-0](https://doi.org/10.1038/s41467-018-07210-0).
* Morton et al. [2018] Jeremy Morton, Antony Jameson, Mykel J Kochenderfer, and Freddie Witherden. Deep dynamical modeling and control of unsteady fluid flows. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper/2018/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf).
* Nguyen et al. [2023] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, and Aditya Grover. Climax: A foundation model for weather and climate, 2023.
* Ovadia et al. [2023] Oded Ovadia, Adar Kahana, Panos Stinis, Eli Turkel, and George Em Karniadakis. Vito: Vision transformer-operator. _arXiv preprint arXiv:2303.08891_, 2023.
* Pan and Duraisamy [2020] Shaowu Pan and Karthik Duraisamy. Physics-informed probabilistic learning of linear embeddings of nonlinear dynamics with guaranteed stability. _SIAM Journal on Applied Dynamical Systems_, 19(1):480-509, 2020. doi: 10.1137/19M1267246. URL [https://doi.org/10.1137/19M1267246](https://doi.org/10.1137/19M1267246).
* Pang et al. [2019] Guofei Pang, Lu Lu, and George Em Karniadakis. fpinns: Fractional physics-informed neural networks. _SIAM Journal on Scientific Computing_, 41(4):A2603-A2626, 2019.
* Pant et al. [2021] Pranshu Pant, Ruchit Doshi, Pranav Bahl, and Amir Barati Farimani. Deep learning for reduced order modelling and efficient temporal evolution of fluid simulations. _Physics of Fluids_, 33(10):107101, 2021. doi: 10.1063/5.0062546. URL [https://doi.org/10.1063/5.0062546](https://doi.org/10.1063/5.0062546).
* Pant et al. [2021] Pranshu Pant, Ruchit Doshi, Pranav Bahl, and Amir Barati Farimani. Deep learning for reduced order modelling and efficient temporal evolution of fluid simulations. _Physics of Fluids_, 33(10):107101, oct 2021. doi: 10.1063/5.0062546. URL [https://doi.org/10.1063%2F5.0062546](https://doi.org/10.1063%2F5.0062546).
* Pathak et al. [2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators, 2022.
* Patil et al. [2023] Saurabh Patil, Zijie Li, and Amir Barati Farimani. Hno: Hyena neural operator for solving pdes. _arXiv preprint arXiv:2306.16524_, 2023.

* Pfaff et al. [2020] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based simulation with graph networks. 2020. doi: 10.48550/ARXIV.2010.03409. URL [https://arxiv.org/abs/2010.03409](https://arxiv.org/abs/2010.03409).
* Pfaff et al. [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based simulation with graph networks, 2021.
* Prantl et al. [2022] Lukas Prantl, Benjamin Ummenhofer, Vladlen Koltun, and Nils Thuerey. Guaranteed conservation of momentum for learning particle-based fluid dynamics, 2022.
* Rahman et al. [2023] Md Ashiqur Rahman, Zachary E. Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators, 2023.
* Raissi et al. [2019] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* Rasp et al. [2020] Stephan Rasp, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. WeatherBench: A benchmark data set for data-driven weather forecasting. _Journal of Advances in Modeling Earth Systems_, 12(11), nov 2020. doi: 10.1029/2020ms002203. URL [https://doi.org/10.1029%2F2020ms002203](https://doi.org/10.1029%2F2020ms002203).
* Razavi et al. [2019] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. _Advances in neural information processing systems_, 32, 2019.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.
* Sanchez-Gonzalez et al. [2020] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks, 2020. URL [https://arxiv.org/abs/2002.09405](https://arxiv.org/abs/2002.09405).
* Sanchez-Gonzalez et al. [2020] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks, 2020.
* Selle et al. [2008] Andrew Selle, Ronald Fedkiw, Byungmoon Kim, Yingjie Liu, and Jarek Rossignac. An unconditionally stable macormack method. _Journal of Scientific Computing_, 35:350-371, 2008. URL [https://api.semanticscholar.org/CorpusID:10522058](https://api.semanticscholar.org/CorpusID:10522058).
* Stachenfeld et al. [2022] Kim Stachenfeld, Drummond Buschman Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned simulators for turbulence. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=msRBOjIz-Nh](https://openreview.net/forum?id=msRBOjIz-Nh).
* Stachenfeld et al. [2022] Kimberly Stachenfeld, Drummond B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned coarse models for efficient turbulence simulation, 2022.
* Sun et al. [2020] Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data. _Computer Methods in Applied Mechanics and Engineering_, 361:112732, 2020.
* Takeishi et al. [2017] Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi. Learning koopman invariant subspaces for dynamic mode decomposition. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 1130-1140, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

* Tang et al. [2022] Jingwei Tang, Vinicius C Azevedo, Guillaume Cordonnier, and Barbara Solenthaler. Neural green's function for laplacian systems. _Computers & Graphics_, 107:186-196, 2022.
* Thuerey et al. [2020] Nils Thuerey, Konstantin Weissenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for reynolds-averaged navier-stokes simulations of airfoil flows. _AIAA Journal_, 58(1):25-36, 2020.
* Tran et al. [2023] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators, 2023.
* Um et al. [2020] Kiwon Um, Robert Brand, Yun Raymond Fei, Philipp Holl, and Nils Thuerey. Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. _Advances in Neural Information Processing Systems_, 33:6111-6122, 2020.
* Ummenhofer et al. [2020] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation with continuous convolutions. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=B11DoJSYDH](https://openreview.net/forum?id=B11DoJSYDH).
* Vahdat et al. [2021] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space, 2021.
* van den Oord et al. [2018] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
* Wang et al. [2020] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep learning for turbulent flow prediction. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, page 1457-1466, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403198. URL [https://doi.org/10.1145/3394486.3403198](https://doi.org/10.1145/3394486.3403198).
* Wang et al. [2020] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep learning for turbulent flow prediction, 2020.
* Wang et al. [2021] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. _Science Advances_, 7(40):eabi8605, 2021. doi: 10.1126/sciadv.abi8605. URL [https://www.science.org/doi/abs/10.1126/sciadv.abi8605](https://www.science.org/doi/abs/10.1126/sciadv.abi8605).
* Wen et al. [2022] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno--an enhanced fourier neural operator-based deep-learning model for multiphase flow. _Advances in Water Resources_, 163:104180, 2022.
* Wiewel et al. [2019] Steffen Wiewel, Moritz Becher, and Nils Thuerey. Latent space physics: Towards learning the temporal evolution of fluid flow. _Computer Graphics Forum_, 38, 2019.
* Wu and He [2018] Yuxin Wu and Kaiming He. Group normalization, 2018.
* Zeng et al. [2022] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. _arXiv preprint arXiv:2210.06978_, 2022.
* Zhu et al. [2023] Min Zhu, Handi Zhang, Anran Jiao, George Em Karniadakis, and Lu Lu. Reliable extrapolation of deep neural operators informed by physics or sparse observations. _Computer Methods in Applied Mechanics and Engineering_, 412:116064, 2023.

[MISSING_PAGE_EMPTY:13]