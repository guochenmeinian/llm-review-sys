# Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning

 Jiawei Yao\({}^{1}\) Qi Qian\({}^{2}\) Juhua Hu\({}^{1}\)

\({}^{1}\) School of Engineering and Technology, University of Washington, Tacoma, WA 98402, USA

\({}^{2}\) Zoom Video Communications

{jyyao, juhuah}@uw.edu, qi.qian@zoom.us

Work done while at Alibaba Group.Corresponding author

###### Abstract

Multiple clustering aims to discover various latent structures of data from different aspects. Deep multiple clustering methods have achieved remarkable performance by exploiting complex patterns and relationships in data. However, existing works struggle to flexibly adapt to diverse user-specific needs in data grouping, which may require manual understanding of each clustering. To address these limitations, we introduce Multi-Sub, a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework in this work. Utilizing the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts expressing user preferences with their corresponding visual representations. This is achieved by automatically generating proxy words from large language models that act as subspace bases, thus allowing for the customized representation of data in terms specific to the user's interests. Our method consistently outperforms existing baselines across a broad set of datasets in visual multiple clustering tasks. Our code is available at https://github.com/Alexander-Yao/Multi-Sub.

## 1 Introduction

Clustering is a fundamental technique for analyzing data based on certain similarities, attracting extensive attention due to the abundance of unlabeled data. Traditional clustering methods [14, 22, 23] rely on general-purpose handcrafted features that may not suit specific tasks well. Deep clustering algorithms have improved clustering performance by employing Deep Neural Networks (DNNs) [21, 22, 24] to learn task-specific features. However, most of these algorithms assume a single partition of the data, while real data can be clustered differently according to different aspects, e.g., fruits in Fig. 1 can be grouped differently by color or by species.

Multiple clustering algorithms [1, 23] address this challenge by producing multiple partitions of the data for various applications,

Figure 1: The workflow of Multi-Sub that obtains a desired clustering based on the subspace spanned by reference words obtained from GPT-4 using users’ high-level interest.

showing the capability of discovering multiple clusterings from a single dataset. For instance, in e-commerce, products can be clustered by category for inventory management or by customer preferences for personalized recommendations. Recently, there has been a growing interest in incorporating deep learning techniques into multiple clustering. These techniques mainly use auto-encoders and data augmentation methods to extract a wide range of feature representations, which enhance the quality of multiple clustering (Miklautz et al., 2020; Ren et al., 2022; Yao et al., 2023).

For real-world applications, a key challenge for end users is efficiently identifying the desired clustering from multiple results based on their interests or application purposes. We observe that users are willing to indicate their interest using succinct keywords (e.g., color or species for fruits in Fig. 1). However, it is difficult to use only a concise keyword to directly extract the corresponding image representations. Fortunately, the recent development of multi-modal models like CLIP (Radford et al., 2021) that align images with their text descriptions can help bridge this gap. Nevertheless, unlike methods that can use labeled data to fine-tune pre-trained models (Gao et al., 2023; Wang et al., 2023) to learn new task-specific representations, multiple clustering often faces scenarios with ambiguous or unspecified label categories and quantities. Therefore, given only a high-level concept from the user, it is intractable to fine-tune pre-trained models to capture a particular aspect of the data in an unsupervised manner. Very recently, Multi-MaP (Yao et al., 2024) leverages CLIP to learn textual and image embeddings simultaneously that follow the user's high-level textual concept. However, to achieve better performance, they require the user to provide a contrastive concept that is different from the desired concept, which may not be feasible in many real-world applications. Moreover, they obtain the new representations at first and then apply the traditional clustering method like k-means in a separate stage. This insufficient optimization lacking refinement between stages makes the clustering performance sub-optimal.

To mitigate these challenges, in this work, we first assume that the desired image and textual representations are residing in the same subspace according to the user's specific concept. Thereafter, to capture the desired subspace better, we can ask low-cost experts like Google or large language models (LLMs) (e.g., GPT-4) for common categories under the desired concept, as illustrated in Fig. 1. Although those returned common categories may not directly capture the clustering targets, they can be applied as the subspace basis to help search the appropriate representations inside. More importantly, during the learning under the desired subspace, we also incorporate the clustering loss to learn the representations and obtain the clustering simultaneously, which significantly enhances the model's clustering performance and efficiency. The main contributions of this work can be summarized as follows.

* We present a novel multiple clustering method, Multi-Sub, that can explicitly capture a user's clustering interest by aligning the textual interest with the visual features of images. Concretely, we propose to learn the desired clustering proxy in the subspace spanned by the common categories under a user's interest.
* Unlike most existing multiple clustering methods that require distinct stages for representation learning and clustering, Multi-Sub can obtain both the desired representations and clustering simultaneously, which can significantly improve the clustering performance and efficiency.
* Extensive experiments on all publicly available multiple clustering tasks empirically demonstrate the superiority of the proposed Multi-Sub, with a precise capturing of a user's interest.

## 2 Related Work

### Multiple Clustering

Multiple clustering, a methodology capable of unveiling alternative data perspectives, has garnered significant interest. Traditional approaches for multiple clustering (Hu and Pei, 2018) employ shallow models to identify diverse data groupings. Some methods, such as COALA (Bae and Bailey, 2006) and (Qi and Davidson, 2009), utilize constraints to generate alternative clusterings. Other techniques leverage distinct feature subspaces to produce multiple clusterings, as exemplified by (Hu et al., 2017) and MNMF (Yang and Zhang, 2017). Information theory has also been applied to generate multiple clusterings, as demonstrated by (Gondek and Hofmann, 2003) and (Dang and Bailey, 2010).

Recent advancements have seen the application of deep learning to discover multiple clusterings, yielding improved clustering performance. For instance, [22] proposed a deep matrix factorization method that utilizes multi-view data to identify multiple clusterings. ENRC [17] employs an auto-encoder to learn object features and optimizes a clustering objective function to find multiple clusterings. iMClusts [14] leverages auto-encoders and multi-head attention to learn features from various perspectives and discover multiple clusterings. AugDMC [23] uses data augmentation to generate diverse image aspects and learns representations to uncover multiple clusterings. DDMC [24] employs a variational Expectation-Maximization framework with disentangled representations to achieve superior clustering outcomes. However, almost all of these methods necessitate substantial user efforts to understand and select the appropriate clustering for different application purposes. Recently, Multi-MaP [24] leverages CLIP encoders to align a user's interest with visual data by learning representations close to the interested concept but far away from a contrastive concept, significantly improving the efficiency of capturing user-desired clusterings. However, Multi-MaP requires the user to input a contrastive concept for better performance, which is often not applicable. More importantly, it separates the representation learning and clustering as two distinct stages, which may result in sub-optimal performance. These issues will be mitigated in this work.

### Multi-Modal Models

Multi-modal learning involves acquiring representations from various input modalities like image, text, or speech. Here, we focus on how vision models benefit from natural language supervision. A key model in this area is CLIP [15], which aligns images with their corresponding text using contrastive learning on a dataset of 400 million text-image pairs.

Fine-tuning adapts vision-language models, such as CLIP, for specific image recognition tasks. This is seen in CoOp [18] and CLIP-Adapter [19], the latter using residual style feature blending to enhance performance. TeS [20] highlights the efficacy of fine-tuning in improving visual comprehension through natural language supervision. With limited labeled data, zero-shot learning has gained attention. Some approaches surpass CLIP by integrating other large pre-trained models. For example, VisDesc [12] uses GPT-3 to generate contextual descriptions for class names, outperforming basic CLIP prompts. UPL [13] and TPT [20] utilize unlabeled data to optimize text prompts. InMaP [19] and the online variant [19] aid class proxies in vision space with text proxies. Recent advancements have significantly improved vision-language pre-training using large-scale noisy datasets. ALIGN [16] employs over one billion image alt-text pairs without expensive filtering, showing that corpus scale can offset noise. Similarly, BLIP-2 [14] uses a novel framework to bootstrap captions from noisy web data, enhancing both vision-language understanding and generation tasks. While these methods strive to enhance the performance of vision classification tasks, clustering presents a distinct scenario where class names are not available to extract useful information from multi-modal information as in this work.

## 3 The Proposed Method

Given a dataset of images \(\{x_{i}\}_{i=1}^{n}\) and user-defined preferences for data grouping (such as color and species), our goal is to generate clustering results that are specifically tailored to each preference. Thereafter, end users can directly use them for different application purposes without additional manual selection efforts. This process poses significant challenges, as it requires accurately aligning the complex, multi-dimensional data of images with the subjective and varied textual preferences of users. Traditional clustering methods often fail to capture these nuances, leading to a generic and less informative categorization for specific user applications.

Recently, the CLIP model [15] facilitated a more natural alignment between textual interests and visual representations. Our method, Multi-Sub, extends this alignment through a novel multi-modal subspace proxy learning approach. Fig. 2 outlines the overall framework of Multi-Sub, which is tailored to capture and respond to the diverse interests of users in clustering tasks. Multi-Sub employs a two-phase iterative approach to align and cluster images based on user-defined preferences such as color and species as described below.

### Background: Multi-Modal Pre-Training in CLIP

Let \(\{x_{i},t_{i}\}_{i=1}^{n}\) be a set of image-text pairs, where \(x_{i}\) denotes an image and \(t_{i}\) denotes its corresponding text description. We can obtain the vision and text representations of each pair by applying two encoders, \(f(\cdot)\) and \(h(\cdot)\), as \(\mathbf{x}_{i}=f(x_{i})\) and \(\mathbf{t}_{i}=h(t_{i})\). Both \(f(\cdot)\) and \(h(\cdot)\) are encoders that optimize the vision and text representations, respectively, such that \(\mathbf{x}_{i}\) and \(\mathbf{t}_{i}\) are unit vectors. The primary goal during this pre-training phase is to minimize the contrastive loss, formulated as

\[\min_{f,h}\sum_{i}-\log\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau)}{ \sum_{j}\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{j}/\tau)}-\log\frac{\exp( \mathbf{t}_{i}^{\top}\mathbf{x}_{i}/\tau)}{\sum_{j}\exp(\mathbf{t}_{i}^{\top} \mathbf{x}_{j}/\tau)}\] (1)

where \(\tau\) is a temperature parameter. The contrastive loss encourages the alignment of the image and its description while penalizing the similarity of the image with irrelevant texts [Qian et al., 2019]. The efficacy of this contrastive approach is vital for the subsequent phases of proxy word learning and fine-grained clustering, as it ensures that the foundational embeddings accurately reflect the inherent content and context of each modality.

### Subspace Proxy Word Representation

We build upon the pre-trained image and text encoders from CLIP and investigate whether we can leverage the image-text alignment to extract user-specific information. Specifically, given a fruit

Figure 2: **Multi-Sub framework. In Multi-Sub framework, Phase I (Proxy Learning and Alignment) processes each image \(x_{i}\) with user-defined textual prompts through a partially learnable image encoder (with a learnable projection layer) and a frozen text encoder. The latent factor \(\mathbf{p}_{i}\) calculates weights \(\{a_{i,k}\}_{k=1}^{K}\) based on the similarity to reference word embeddings \(\{\mathbf{z}_{i}\}_{k=1}^{K}\), which are then aggregated to form the proxy word embedding \(\mathbf{w}_{i}\). This proxy word embedding, combined with the image representation \(\mathbf{x}_{i}\), establishes the Aligned Feature Subspace for better alignment between the text and image under the user’s interest. In Phase II (Clustering), given the learned proxy word embeddings \(\{\mathbf{w}_{i}\}\) from Phase I to form pseudo-labels, the projection layer of the image encoder is further refined using the clustering loss. In Phase I, both the latent factor \(\mathbf{p}\) and the projection layer learn 100 epochs, after which the projection layer further learns 10 epochs using the clustering loss in Phase II. This alternative process repeats until convergence.**image [Hu et al., 2017] as illustrated in Fig. 2, different users may have different interests of its attributes, such as color, species, etc. However, the pre-trained image encoder in CLIP can only produce a single image embedding, which may not capture a user's interest exactly, not mentioning capturing different aspects. Furthermore, unlike classification tasks, clustering tasks do not come with concrete cluster names or numbers. Therefore, we cannot directly use the pre-trained text encoder of CLIP to generate the corresponding text embedding.

To address these challenges, we propose a subspace proxy word learning method to learn new embedding under the preferred aspect provided by the user. Thereafter, the main challenge is, given only a high-level concept like 'color' as in Fig. 2, how to effectively represent its subspace. Since the high-level concept itself cannot reflect different details under this concept in different images, it is difficult to do effective alignment between the high-level concept and images to figure out the corresponding vision subspace. Therefore, we propose to figure out the text subspace at first. Concretely, given pre-trained large language models like GPT-4 as low-cost experts, we can quickly gather common categories under a high-level concept using only one query like 'what are the common fruit colors' in Fig. 2. However, we cannot directly use the returned categories to do grouping, since they may not cover all existing categories in the data. Instead, we consider that most categories in the data under this concept are residing in the same subspace as the returned ones. Therefore, we can apply suggested categories as basis or reference words in the subspace. Then, each image's category under the desired concept can be represented by a linear combination of these reference words.

Assuming GPT-4 provides \(K\) reference words as \(\{z_{k}\}_{k=1}^{K}\), the proxy word of image \(x_{i}\) can be calculated as

\[\mathbf{w}_{i}=\sum_{k=1}^{K}a_{i,k}\phi(z_{k})\] (2)

where \(\phi(z_{k})\) is the token embedding of reference word \(z_{k}\) and \(\{a_{i,k}\}_{k=1}^{K}\) are weights corresponding to each reference word as a basis. A higher weight \(a_{i,k}\) indicates that the image \(x_{i}\)'s category is closer to the reference word \(z_{k}\). Here, we introduce trainable latent factor \(\mathbf{p}_{i}\) to learn the weight \(a_{i,k}\), and it can be calculated as

\[a_{i,k}=\frac{\exp{(\mathbf{p}_{i}\mathbf{z}_{k})}}{\sum_{j}\exp{(\mathbf{p}_ {i}\mathbf{z}_{j})}}\] (3)

where \(\mathbf{z}_{k}=\phi(z_{k})\). Thereafter, \(\mathbf{w}_{i}\) is representing the token embedding of image \(x_{i}\)'s proxy word under the preferred user concept. Once \(\mathbf{p}_{i}\) is well obtained, the image's proxy word representation under the preferred user concept is also obtained. Next, we discuss how to learn \(\mathbf{p}_{i}\) using CLIP.

### Multi-Modal Subspace Proxy Learning

As mentioned above, CLIP's text and image encoders were learned by aligning the text prompt with its corresponding image. The standard text prompt of CLIP is designed as "a photo of a fruit" for an image containing "fruit". Now, given a user's preference (e.g., color), we can rewrite the prompt as "a fruit with the color of *" denoted by \(t_{i}^{*}\) for image \(x_{i}\), where "*" is the placeholder for the unknown proxy word of image \(x_{i}\) under concept 'color' and its token embedding \(\mathbf{w}_{i}\) can be formulated as the linear superposition of reference words' token embeddings as discussed above.

Thereafter, the prompt text embedding after the text encoder can be formulated as

\[\mathbf{t}_{i}^{*}=h(\phi(t_{i}^{*})\|\phi(w_{i}))\] (4)

To effectively learn \(\mathbf{p}_{i}\), the trainable latent factors, we utilize the alignment capabilities of CLIP by adjusting these factors so that the weighted sum of reference word embeddings closely aligns with the visual representation of the image. This process involves iteratively adjusting \(\mathbf{p}_{i}\) to maximize the cosine similarity between the image's representation \(\mathbf{x}_{i}\) and its corresponding proxy word embedding \(\mathbf{w}_{i}\). The optimization is conducted with the following loss function:

\[\mathcal{L}(\mathbf{w}_{i})=-\langle f(x_{i}),h(\phi(t_{i}^{*})\|\phi(w_{i}))\rangle\] (5)It should be noted that this optimization procedure can be conducted with both the text encoder and image encoder frozen, which is very efficient. However, the image embedding extracted directly from the pre-trained image encoder may not reflect its representation under the desired user interest. Therefore, during the optimization procedure, we do freeze the text encoder but open the image encoder. Nevertheless, to preserve the strong capacity of the pre-trained image encoder in CLIP, we open only the projection layer of the image encoder, while its remaining parameters are frozen as shown in the 'Phase I' of Fig. 2.

### Clustering Loss

To enhance the clustering performance of Multi-Sub, in 'Phase II', we leverage pseudo-labels assigned using the currently learned proxy word embeddings \(\{\mathbf{w}_{i}\}\) and image embeddings \(\{\mathbf{x}_{i}\}\) from 'Phase I'. Concretely, each image \(x_{i}\) can be represented by the concatenation of its currently learned proxy word embedding \(\mathbf{w}_{i}\) and image embedding \(\mathbf{x}_{i}\), denoted as \(\mathbf{v}_{i}=[\mathbf{w}_{i},\mathbf{x}_{i}]\). The pseudo-labels can be obtained by an offline k-means on \(\{\mathbf{v}_{i}\}\), which is however not efficient. Considering that proxy words for data points within the same cluster should show similar relationships to reference words, we obtain the pseudo-labels using the highest cosine similarity between the currently learned proxy word embeddings \(\{\mathbf{w}_{i}\}\) and the reference word embeddings \(\{\mathbf{z}_{k}\}\).

Given the pseudo-labels, the image embeddings can be further optimized by opening only the projection layer of the image encoder for improved compactness and separability in clusters. This loss consists of two primary components: intra-cluster loss and inter-cluster loss, aimed at refining cluster cohesion and separation, respectively. It should be noted that to better represent each image under the desired user concept, we define the clustering loss over \(\mathbf{v}_{i}\) containing both textual and visual information.

**Intra-cluster Loss:** The intra-cluster loss is designed to minimize the distances between embeddings within the same cluster, encouraging cluster compactness. It is calculated using the following formula:

\[\mathcal{L}_{\text{intra}}=\frac{1}{N_{\text{intra}}}\sum_{i,j\in\text{intra} }\|\mathbf{v}_{i}-\mathbf{v}_{j}\|^{2}\] (6)

Here, \(\|\mathbf{v}_{i}-\mathbf{v}_{j}\|^{2}\) is the squared Euclidean distance between embeddings \(\mathbf{x}_{i}\) and \(\mathbf{x}_{j}\) of data points \(i\) and \(j\) within the same cluster, and \(N_{\text{intra}}\) denotes the number of intra-cluster pairs.

**Inter-cluster Loss:** This component aims to maximize the distances between embeddings from different clusters, thus enhancing separability. The inter-cluster loss is defined by a margin-based hinge loss as follows:

\[\mathcal{L}_{\text{inter}}=\frac{1}{N_{\text{inter}}}\sum_{i,j\in\text{inter}} \max(0,m-\|\mathbf{v}_{i}-\mathbf{v}_{j}\|)\] (7)

where \(\max(0,m-\|\mathbf{v}_{i}-\mathbf{v}_{j}\|)\) computes the hinge loss for each pair of embeddings from different clusters, ensuring a minimum margin \(m\) between them. \(N_{\text{inter}}\) is the count of inter-cluster pairs.

**Total Loss:** The overall clustering loss combines the intra- and inter-cluster losses, moderated by a balancing factor \(\lambda\):

\[\mathcal{L}_{\text{total}}=\lambda\cdot\mathcal{L}_{\text{intra}}+(1-\lambda) \cdot\mathcal{L}_{\text{inter}}\] (8)

Optimizing this loss function in 'Phase II' helps regularize the embedding space where clusters are both internally dense and well-separated from each other. It should be noted that in this phase we aim to learn a better projection layer only for the image encoder, while all others are fixed as shown in 'Phase II' of Fig. 2.

Previous methods often use a two-stage strategy that separates representation learning and clustering to simplify the optimization process. This separation, however, can lead to sub-optimal clustering results, since the learned representations may not be fully aligned with the clustering objective without refinement. In this work, we obtain both the proxy word and the clustering alternatively and simultaneously. Concretely, we first learn the proxy word in a user-preferred subspace. Then, we fix the proxy word and refine the image encoder further to obtain better image representations using the clustering objective. These two phases are repeated alternatively until convergence, where 'Phase I' learns 100 epochs and 'Phase II' learns 10 epochs in each alternating according to the empirical experience as summarized in Fig. 2.

[MISSING_PAGE_EMPTY:7]

multiple salient embedding matrices and multiple clusterings therein; **AugDMC** Yao et al. (2023) leverages data augmentations to automatically extract features related to different aspects of the data using a self-supervised prototype-based representation learning method; **DDMC** Yao and Hu (2024) combines disentangled representation learning with a variational Expectation-Maximization (EM) framework; **Multi-MaP** Yao et al. (2024) relies on a contrastive user-defined concept to learn a proxy better tailored to a user's interest. It is worth noting that, in our experiments, we apply both traditional and deep learning baselines. Traditional methods rely on hand-crafted features, while deep learning methods directly utilize the original images as input.

HyperparameterFor each user's preference, we train the model for \(1000\) epochs using Adam optimizer with a momentum of \(0.9\). We tune all the hyper-parameters based on the loss score of Multi-Sub, where the learning rate is selected from {1e-1,5e-2,1e-2,5e-3,1e-3,5e-4}, weight decay is chosen from {5e-4,1e-4,5e-5,1e-5,0} for all the experiments. Most methods obtain each clustering by applying k-means Lloyd (1982) to the newly learned representations, while ours is end-to-end. The experiments are performed on four NVIDIA GeForce RTX 2080 Ti GPUs.

Evaluation metricsConsidering the randomness of k-means for those applicable baselines, we run k-means 10 times and report the average clustering performance using two metrics, namely, Normalized Mutual Information (NMI) White et al. (2004) and Rand index (RI) Rand (1971). These metrics range from \(0\) to \(1\) with higher value indicating better performance compared to the groundtruth.

### Performance Comparison

Table 2 reports the clustering results. During the clustering stage, after we obtain the proxy word embedding of each image for a desired concept, we can concatenate the image embedding and the token embedding of proxy word. The results show that Multi-Sub consistently outperforms the baselines, demonstrating the superiority of the proposed method. This also indicates a strong generalization ability of the pre-trained model by CLIP, which can capture the features of data from different perspectives.

Our methodology uses the CLIP encoder and GPT-4 to derive clustering results, prompting an evaluation of their performance in a zero-shot manner. We introduce two zero-shot variants of CLIP: CLIP\({}_{\text{GPT}}\) and CLIP\({}_{\text{label}}\). CLIP\({}_{\text{GPT}}\) uses GPT-4 to generate candidate labels and performs zero-shot classification, while CLIP\({}_{\text{label}}\) uses ground truth labels directly, providing an optimal setting. As shown in Table 3, CLIP\({}_{\text{label}}\) generally outperforms CLIP\({}_{\text{GPT}}\) due to its use of accurate labels, while CLIP\({}_{\text{GPT}}\) introduces noise. Both variants perform equally on the Card dataset as GPT-4's labels match the groundtruth. Multi-Sub surpasses CLIP\({}_{\text{GPT}}\) and even outperforms CLIP\({}_{\text{label}}\) in all cases, demonstrating its ability to capture user-interest-based data aspects and confirming its efficacy. This superiority can be attributed to Multi-Sub's proxy word learning mechanism, which automatically adjusts textual

\begin{table}
\begin{tabular}{c c|c c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Clustering**} & \multicolumn{2}{c}{**CLIP\({}_{\text{GPT}}\)**} & \multicolumn{2}{c}{**CLIP\({}_{\text{label}}\)**} & \multicolumn{2}{c}{**Multi-Sub**} \\  & & NMI\({}^{*}\) & R\(\uparrow\) & NMI\({}^{*}\) & R\(\uparrow\) & NMI\({}^{*}\) & R\(\uparrow\) \\ \hline \multirow{2}{*}{Fruit} & Color & 0.7912 & 0.9075 & 0.8629 & 0.9780 & **0.9693** & **0.9964** \\  & Species & 0.9793 & 0.9919 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\ \hline \multirow{2}{*}{Fruit360} & Color & 0.5613 & 0.7305 & 0.5746 & 0.7673 & **0.6654** & **0.8821** \\  & Species & 0.4370 & 0.7552 & 0.5364 & 0.7631 & **0.6123** & **0.8504** \\ \hline \multirow{2}{*}{Card} & Order & 0.3518 & 0.8458 & 0.3518 & 0.8458 & **0.3921** & **0.8842** \\  & Suits & 0.2711 & 0.6123 & 0.2711 & 0.6123 & **0.3104** & **0.7941** \\ \hline \multirow{2}{*}{CMUface} & Emotion & 0.1576 & 0.6532 & 0.1590 & 0.6619 & **0.2053** & **0.8527** \\  & Glass & 0.2905 & 0.6869 & 0.4686 & 0.7505 & **0.4870** & **0.3324** \\  & Identity & 0.1998 & 0.6388 & 0.2677 & 0.7545 & **0.7441** & **0.9834** \\  & Pose & 0.4088 & 0.6473 & 0.4691 & 0.6409 & **0.5923** & **0.8736** \\ \hline \multirow{2}{*}{Stanford Cars} & Color & 0.6539 & 0.8237 & 0.6830 & 0.8642 & **0.7533** & **0.9387** \\  & Type & 0.6207 & 0.7931 & 0.6429 & 0.8456 & **0.6616** & **0.8792** \\ \hline \multirow{2}{*}{Flowers} & Color & 0.5653 & 0.7629 & 0.5828 & 0.7836 & **0.6940** & **0.8843** \\  & Species & 0.5620 & 0.7553 & 0.6019 & 0.7996 & **0.6724** & **0.8719** \\ \hline \multirow{2}{*}{CIFAR-10} & Type & 0.4935 & 0.6741 & 0.5087 & 0.7102 & **0.5271** & **0.7394** \\  & Environment & 0.4302 & 0.6507 & 0.4643 & 0.6801 & **0.4828** & **0.7096** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Variants of CLIP. The significantly best results with 95% confidence are in bold.

embeddings based on user-defined interests, creating more accurate proxy word embeddings. This approach reduces noise compared to CLIP\({}_{\text{GPT}}\), which suffers from label mismatches. Additionally, Multi-Sub's iterative learning process refines these embeddings, optimizing alignment between text and image representations.

### Ablation study

Different ways of constructing subspaceThe subspace of the proposed method can be expanded by different embeddings, i.e., the token embedding of the proxy word \(\phi(w_{i})\), the text embedding of the proxy word \(h(\phi(w_{i}))\), and the text embedding of the prompt \(\mathbf{t}_{i}^{*}=h(\phi(t_{i}^{*})\|\phi(w_{i}))\). These three kinds of embeddings can also be used to evaluate the clustering results in each case. In addition, we can use different combinations of learned embeddings (e.g., different concatenations of text and image embedding) as the final embedding for clustering. The results are shown in Table 5. It can be seen that using word token embedding usually achieves better results. This is expected since the word proxy directly reflects the image's category under the desired concept. The token word embedding subspace is also aligning well with CLIP's training method. In contrast, prompt embedding performs the worst as it introduces noise from user interest, dataset, and reference words, which are unnecessary for clustering. Additionally, most methods perform better when the same approach is used for constructing subspace and evaluating clustering results. Combining text and image embeddings generally enhances performance, capturing user interests from both aspects effectively.

Effect of text encoderTable 4 compares the performance of three text encoders--CLIP, ALIGN, and BLIP--across various datasets. The results indicate that ALIGN generally outperforms CLIP and BLIP in most tasks. This suggests that ALIGN's text encoder effectively captures and aligns textual and visual representations, enhancing clustering performance. ALIGN tends to excel in tasks that require distinguishing subtle visual differences influenced by textual descriptions, such as emotions and accessories in the CMUface dataset, and colors in the Fruit360 dataset. CLIP shows a strong tendency in identity-related tasks and complex object categorization, as evidenced by its performance in the CMUface identity task and Standford Cars type clustering. BLIP, while competitive, seems to perform better in categorical distinctions rather than abstract attributes, performing relatively well in species-related tasks across various datasets. These findings underscore the importance of effective text embeddings in multi-modal clustering frameworks.

We conducted an additional analysis using the Maximum Mean Discrepancy (MMD) metric to quantify the differences in the feature spaces generated by different text encoders (i.e., CLIP, ALIGN, and BLIP) in Table 6. The MMD results indicate that although our text prompts are simple, the

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Clustering**} & \multicolumn{2}{c}{**CLIP**} & \multicolumn{2}{c}{**ALIGN**} & \multicolumn{2}{c}{**BLIP**} \\  & & NMI\(\uparrow\) & R\(\uparrow\) & NM\(\uparrow\) & R\(\uparrow\) & NMI\(\uparrow\) & R\(\uparrow\) \\ \hline \multirow{3}{*}{Fruit360} & Color & 0.6654 & 0.8821 & **0.7031** & **0.8925** & 0.6522 & 0.8814 \\  & Species & 0.6123 & 0.8504 & **0.6426** & **0.8565** & 0.6254 & 0.8536 \\ \hline \multirow{3}{*}{Card} & Order & 0.3921 & 0.8842 & **0.4316** & **0.9023** & 0.3845 & 0.8359 \\  & & Suits & 0.3104 & 0.7941 & **0.3226** & **0.8006** & 0.3151 & 0.7956 \\ \hline \multirow{3}{*}{CMUface} & Emotion & 0.2053 & 0.8527 & **0.2148** & **0.8553** & 0.2081 & 0.8535 \\  & Glass & 0.4870 & 0.8324 & **0.4951** & 0.8351 & **0.4951** & **0.8353** \\  & Identity & 0.7441 & **0.9834** & **0.7514** & 0.9828 & 0.6833 & 0.8321 \\  & Pose & 0.5923 & 0.8736 & **0.6137** & **0.8942** & 0.5732 & 0.8427 \\ \hline \multirow{3}{*}{Standford Cars} & Color & 0.7533 & **0.9387** & **0.7624** & 0.8942 & 0.5732 & 0.8427 \\  & Type & 0.6616 & 0.8792 & **0.6712** & **0.8865** & 0.6581 & 0.8731 \\ \hline \multirow{3}{*}{Flowers} & Color & **0.694** & **0.8843** & 0.6925 & 0.8812 & 0.6843 & 0.8789 \\  & Species & **0.6724** & **0.8719** & 0.6693 & 0.8691 & 0.6627 & 0.8654 \\ \hline \multirow{3}{*}{CIFAR-10} & Type & 0.5271 & 0.7394 & **0.5342** & **0.7456** & 0.5221 & 0.7381 \\  & Environment & **0.4828** & **0.7096** & 0.4793 & 0.7064 & 0.4752 & 0.7038 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of differenttext encoders. The significantly best results with 95% confidence are in bold.

\begin{table}
\begin{tabular}{c c c|c c c c|c c c c|c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Clustering**} & \multirow{2}{*}{**Solepap**} & \multicolumn{2}{c}{**clustering with \(\lambda(\phi(w_{i}))\)**} & \multicolumn{2}{c}{**clustering with \(\lambda(\phi(w_{i}))\)**} & \multicolumn{2}{c}{**clustering with \(\phi(w_{i})\)**} & \multicolumn{2}{c}{**clustering with \(\phi(w_{i})\)**} \\  & & NMI\(\uparrow\) & R\(\uparrow\) & NM\(\uparrow\) & R\(\uparrow\) & NM\(\uparrow\) & R\(\uparrow\) & NM\(\uparrow\) & R\(\uparrow\) & NM\(\uparrow\) & R\(\uparrow\) & NMI\(\uparrow\) & R\(\uparrow\) \\ \hline \multirow{3}{*}{CIFAR-10} & Type & \(h(\phi(w_{i}))\) & 0.6496 & 0.6546 & 0.4789 & 0.6607 & 0.5298 & 0.7284 & 0.8660 & 0.6331 & 0.4867 & 0.6633 & 0.6632 & 0.6639 & 0.7096 \\  & & 0.8391 & 0.6579 & 0.4634 & 0.6499 & 0.5114 & 0.7193 & 0.4704 & 0.6586 & 0.5116 & 0.7140 & 0.6723 & 0.6524 & 0.5013 & 0.7136 \\  & & 0.7375 & 0.6589 & 0.4377 & 0.6583 & 0.5358 & 0.7211 & 0.4601 & 0.6240 & 0.5039 & 0.6699 & 0.4821 & 0.6638 & **0.5271** & **0.7374** \\ \hline \multirow{3}{*}{CIFAR-10} & \multirow{3}{*}{Environment} & \(h(\phi(w_{i}))\) & 0.42717 & 0.6640 & 0.4533 & 0.6133 & 0.4737 & 0.6695 & 0.4290 & 0.6537 & 0.4140 & 0.6662 & 0.4345 & 0.6691 & 0.4560 & 0.6615 \\  & & 0.4216 & 0.6677 & 0.4220 & 0.6533 & 0.6436 & 0.6660 & 0.4316 & 0.6690 & 0.4761 & 0.4734 & 0.5969 & 0.4161 & 0.6695 \\ \cline{1-1}  & & 0.4340 & 0.6337 & 0.4570 & 0.6572 & 0.4486 & 0.6834 & 0.4218 & 0.6541 & 0.4432 & 0.6631 & 0.4586 & 0.6567 & **0.4285** & **0.7098** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation study of Multi-Sub. The results that achieve the highest and second highest performance for each clustering are indicated by boldface and underlined numerics, respectively.

feature spaces generated by different text encoders exhibit significant distributional differences. The effectiveness of a text encoder can vary depending on the specific clustering task. For example, ALIGN tends to excel in tasks with more abstract attributes, such as colors and emotions, while CLIP shows strong performance in identity-related tasks. This variability underscores the importance of selecting an appropriate text encoder based on the specific application requirements. The difference between text encoders may come from the different corresponding pre-training tasks and this will be an interesting future direction.

VisualizationTo further demonstrate the effectiveness of Multi-Sub, we visualize the representations from CLIP\({}_{\text{label}}\), CLIP\({}_{\text{GPT}}\), and Multi-Sub for color and species clustering tasks (Figure 3). In species clustering, CLIP\({}_{\text{label}}\) shows clear boundaries using ground truth labels, while CLIP\({}_{\text{GPT}}\) introduces noise from reference words. Multi-Sub outperforms both by effectively capturing image features and user interests with proxy word embeddings. In color clustering, both CLIP\({}_{\text{label}}\) and CLIP\({}_{\text{GPT}}\) focus on species features, resulting in less distinct clusters. Multi-Sub excels by clearly distinguishing colors, leveraging user-specific interests for improved alignment. Overall, Multi-Sub consistently aligns embeddings with user interests, surpassing CLIP\({}_{\text{label}}\) and CLIP\({}_{\text{GPT}}\), demonstrating its robust multi-modal subspace proxy learning.

## 5 Conclusion and Limitations

In conclusion, our study mitigates an important challenge in multiple clustering: effectively identifying desired clustering results based on user interests or application purposes. We introduce Multi-Sub, a novel approach that integrates user-defined preferences into a customized multi-modal subspace proxy learning framework. By leveraging the synergy between CLIP and GPT-4, Multi-Sub automatically aligns textual prompts expressing user interests with corresponding visual representations. First, we observe reference words for user's interests from large language models. Given the absence of concrete class names in clustering tasks, our method uses these reference words to learn both text and vision embeddings tailored to user preferences. Extensive experiments across various visual multiple clustering tasks demonstrate that Multi-Sub consistently outperforms state-of-the-art techniques.

However, our approach has certain limitations. The reliance on large language models like GPT-4 can introduce biases inherent in these models, potentially affecting the clustering outcomes. Additionally, the field of multiple clustering lacks large, diverse datasets, which limits comprehensive evaluation. Although we have annotated CIFAR-10, more extensive datasets are needed.

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline
**Dataset** & **Clustering** & **CLIP vs. ALIGN** & **CLIP vs. BLIP** & **ALIGN vs. BLIP** \\ \hline \multirow{2}{*}{Fruit360} & Color & 0.234 & 0.198 & 0.211 \\  & Species & 0.189 & 0.172 & 0.183 \\ \hline \multirow{2}{*}{Card} & Order & 0.215 & 0.202 & 0.219 \\  & Suits & 0.198 & 0.184 & 0.192 \\ \hline \multirow{3}{*}{CMUface} & Emotion & 0.276 & 0.245 & 0.263 \\  & Glass & 0.231 & 0.217 & 0.225 \\  & Identity & 0.263 & 0.249 & 0.258 \\  & Pose & 0.245 & 0.228 & 0.239 \\ \hline \multirow{2}{*}{Stanford Cars} & Color & 0.238 & 0.223 & 0.231 \\  & Type & 0.212 & 0.198 & 0.205 \\ \hline \multirow{2}{*}{Flowers} & Color & 0.257 & 0.244 & 0.252 \\  & Species & 0.248 & 0.231 & 0.242 \\ \hline \multirow{2}{*}{CIFAR-10} & Type & 0.193 & 0.178 & 0.186 \\  & Environment & 0.178 & 0.162 & 0.174 \\ \hline \hline \end{tabular}
\end{table}
Table 6: MMD between different text encoders across datasets.

Figure 3: Visualization of feature embeddings and related labels on Fruit dataset. For the visualization of color, red, green, and yellow points indicate the color of red, green, and yellow, respectively. For the visualization of species, red, yellow, and purple points indicate the species of apple, banana, and grapes, respectively.

Acknowledgment

Yao's research was funded in part by J.P. Morgan Chase & Co and Advata Gift funding. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by J.P. Morgan Chase & Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.

## References

* Bae and Bailey (2006) E. Bae and J. Bailey. Coala: A novel approach for the extraction of an alternate clustering of high quality and high dissimilarity. In _ICDM_, pages 53-62. IEEE, 2006.
* Bishop and Nasrabadi (2006) C. M. Bishop and N. M. Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
* Dang and Bailey (2010) X. H. Dang and J. Bailey. Generation of alternative clusterings using the cami approach. In _Proceedings of the 2010 SIAM International Conference on Data Mining_, pages 118-129. SIAM, 2010.
* Fellbaum (2010) C. Fellbaum. Wordnet. In _Theory and applications of ontology: computer applications_, pages 231-243. Springer, 2010.
* Gao et al. (2023) P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. _International Journal of Computer Vision_, pages 1-15, 2023.
* Gondek and Hofmann (2003) D. Gondek and T. Hofmann. Conditional information bottleneck clustering. In _3rd ieee international conference on data mining, workshop on clustering large data sets_, pages 36-42, 2003.
* Guerin and Boots (2018) J. Guerin and B. Boots. Improving image clustering with multiple pretrained cnn feature extractors. In _British Machine Vision Conference 2018, BMVC 2018_, 2018.
* Gunnemann et al. (2014) S. Gunnemann, I. Farber, M. Rudiger, and T. Seidl. Smvc: semi-supervised multi-view clustering in subspace projections. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 253-262, 2014.
* Hu and Pei (2018) J. Hu and J. Pei. Subspace multi-clustering: a review. _Knowledge and information systems_, 56(2):257-284, 2018.
* Hu et al. (2017) J. Hu, Q. Qian, J. Pei, R. Jin, and S. Zhu. Finding multiple stable clusterings. _Knowledge and Information Systems_, 51(3):991-1021, 2017.
* Huang et al. (2022) T. Huang, J. Chu, and F. Wei. Unsupervised prompt learning for vision-language models. _arXiv preprint arXiv:2204.03649_, 2022.
* Jia et al. (2021) C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR, 2021.
* Krizhevsky et al. (2009) A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Li et al. (2023) J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* Lloyd (1982) S. Lloyd. Least squares quantization in pcm. _IEEE transactions on information theory_, 28(2):129-137, 1982.
* MacQueen et al. (1967) J. MacQueen et al. Some methods for classification and analysis of multivariate observations. In _Proceedings of the fifth Berkeley symposium on mathematical statistics and probability_, volume 1, pages 281-297. Oakland, CA, USA, 1967.
* MacQueen et al. (2017)S. Menon and C. Vondrick. Visual classification via description from large language models. _arXiv preprint arXiv:2210.07183_, 2022.
* Miklautz et al. [2020] L. Miklautz, D. Mautz, M. C. Altinigneli, C. Bohm, and C. Plant. Deep embedded non-redundant clustering. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 5174-5181, 2020.
* Ng et al. [2001] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. _Advances in neural information processing systems_, 14, 2001.
* Qi and Davidson [2009] Z. Qi and I. Davidson. A principled and flexible framework for finding alternative clusterings. In _SIGKDD_, pages 717-726, 2009.
* Qian [2023] Q. Qian. Stable cluster discrimination for deep clustering. In _ICCV_, pages 16599-16608. IEEE, 2023.
* Qian and Hu [2024] Q. Qian and J. Hu. Online zero-shot classification with clip. In _ECCV_, 2024.
* Qian et al. [2019] Q. Qian, L. Shang, B. Sun, J. Hu, H. Li, and R. Jin. Softtriple loss: Deep metric learning without triplet sampling. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6450-6458, 2019.
* Qian et al. [2022] Q. Qian, Y. Xu, J. Hu, H. Li, and R. Jin. Unsupervised visual representation learning by online constrained k-means. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16640-16649, 2022.
* Qian et al. [2024] Q. Qian, Y. Xu, and J. Hu. Intra-modal proxy learning for zero-shot visual categorization with clip. _Advances in Neural Information Processing Systems_, 36, 2024.
* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Rand [1971] W. M. Rand. Objective criteria for the evaluation of clustering methods. _Journal of the American Statistical association_, 66(336):846-850, 1971.
* Ren et al. [2022] L. Ren, G. Yu, J. Wang, L. Liu, C. Domeniconi, and X. Zhang. A diversified attention model for interpretable multiple clusterings. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* Shu et al. [2022] M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. _Advances in Neural Information Processing Systems_, 35:14274-14289, 2022.
* Wang et al. [2023] J. Wang, Y. Xu, J. Hu, M. Yan, J. Sang, and Q. Qian. Improved visual fine-tuning with natural language supervision. In _ICCV_, pages 11865-11875. IEEE, 2023.
* Wei et al. [2020] S. Wei, J. Wang, G. Yu, C. Domeniconi, and X. Zhang. Multi-view multiple clusterings using deep matrix factorization. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 6348-6355, 2020.
* White et al. [2004] J. White, S. Steingold, and C. Fournelle. Performance metrics for group-detection algorithms. _Proceedings of Interface_, 2004, 2004.
* Xie et al. [2016] J. Xie, R. Girshick, and A. Farhadi. Unsupervised deep embedding for clustering analysis. In _International conference on machine learning_, pages 478-487, 2016.
* Yang and Zhang [2017] S. Yang and L. Zhang. Non-redundant multiple clustering by nonnegative matrix factorization. _Machine Learning_, 106(5):695-712, 2017.
* Yao and Hu [2024] J. Yao and J. Hu. Dual-disentangled deep multiple clustering. In _Proceedings of the 2024 SIAM International Conference on Data Mining (SDM)_, pages 679-687. SIAM, 2024.
* Yao et al. [2023] J. Yao, E. Liu, M. Rashid, and J. Hu. Augdmc: Data augmentation guided deep multiple clustering. _Procedia Computer Science_, 222:571-580, 2023.
* Yao et al. [2024]J. Yao, Q. Qian, and J. Hu. Multi-modal proxy learning towards personalized visual multiple clustering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14066-14075, 2024.
* Yu et al. [2024] G. Yu, L. Ren, J. Wang, C. Domeniconi, and X. Zhang. Multiple clusterings: Recent advances and perspectives. _Computer Science Review_, 52:100621, 2024.
* Zhou et al. [2022] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.

Appendix

### Further Analysis

Parameters Analysis.To show the sensitivity of the balancing factor \(\lambda\) that is the only hyperparameter in our proposed method, the experiments were conducted on CIFAR-10. We varied the value of \(\lambda\) from 0.1 to 1.0 in increments of 0.1 to observe its effect on the model's performance. As shown in Fig. 4, we can observe that different values of \(\lambda\) can work with our method and the optimal performance for "Type" & "Environment" clustering is achieved when \(\lambda\) is set to 0.5. When \(\lambda\) is too low, the model focuses too much on maximizing the distances between different clusters, which can lead to less cohesive clusters. Conversely, when \(\lambda\) is too high, the model emphasizes compactness within clusters at the expense of inter-cluster separation, leading to less distinct clusters. Therefore, we set \(\lambda\) to be 0.5 for all datasets, which confirms the robustness of our method.

Model Adaptability.To evaluate how Multi-Sub adapts to new user demands not originally provided in the ground-truth of the dataset, we conducted an additional experiment using the Fruit dataset. Specifically, we introduced a new demand based on the shape of the fruits, with the prompt set as "fruit with the shape of *". We categorized the fruits into two shapes, that is, round and elongated. Although this specific demand may not be common in practical applications, it serves as an exploratory experiment to test the adaptability of our method.

The results in Table 7 demonstrate that Multi-Sub successfully adapted to the new user demand of shape. The model learned to align the textual descriptions of shapes with the visual features, resulting in a clustering under the new subspace of shape.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **NMI** & **RI** \\ \hline MSC & 0.553 & 0.762 \\ MCV & 0.586 & 0.787 \\ ENRC & 0.603 & 0.825 \\ iMClusts & 0.629 & 0.821 \\ AugDMC & 0.643 & 0.844 \\ Multi-MaP & 0.717 & 0.875 \\ Multi-Sub (ours) & **0.752** & **0.891** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Clustering performance based on shape demand on the Fruit dataset.

Figure 4: Sensitivity analysis of balancing factor \(\lambda\) on CIFAR-10 dataset.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims in the abstract and introduction are supported by detailed descriptions, empirical evaluations, and theoretical analysis provided in the body of the paper (Sections 1, 3, and 4). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the limitation in Section 5, addressing potential biases from large language models. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper specifies the datasets used, the training and testing details, the hyperparameters, and the evaluation metrics in Section 4, ensuring that the experiments can be reproduced. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The data and code are public. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides comprehensive details on the training and testing setups, including data splits, hyperparameters, and optimizer settings in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Since some baselines involve randomness using k-means, the paper did 95% significance test using 10 repeated results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides details on the computational resources used, including the type of GPUs and the total amount of compute required for the experiments, as mentioned in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research adheres to the NeurIPS Code of Ethics, ensuring transparency, reproducibility, and consideration of ethical implications throughout the study. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses the potential positive impacts of improving clustering techniques for various applications and mentions possible negative impacts such as biases introduced by large language models in Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve the release of data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper properly credits the creators of the datasets and models used, and mentions the licenses and terms of use, as detailed in Section 4. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The new datasets and code introduced in the paper are well documented. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.