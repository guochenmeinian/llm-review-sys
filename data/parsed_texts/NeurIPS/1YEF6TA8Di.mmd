# Langevin Quasi-Monte Carlo

Sifan Liu

Department of Statistics

Stanford University

Stanford, CA 94305

sfliu@stanford.edu

###### Abstract

Langevin Monte Carlo (LMC) and its stochastic gradient versions are powerful algorithms for sampling from complex high-dimensional distributions. To sample from a distribution with density \(\pi(\theta)\propto\exp(-U(\theta))\), LMC iteratively generates the next sample by taking a step in the gradient direction \(\nabla U\) with added Gaussian perturbations. Expectations w.r.t. the target distribution \(\pi\) are estimated by averaging over LMC samples. In ordinary Monte Carlo, it is well known that the estimation error can be substantially reduced by replacing independent random samples by quasi-random samples like low-discrepancy sequences. In this work, we show that the estimation error of LMC can also be reduced by using quasi-random samples. Specifically, we propose to use completely uniformly distributed (CUD) sequences with certain low-discrepancy property to generate the Gaussian perturbations. Under smoothness and convexity conditions, we prove that LMC with a low-discrepancy CUD sequence achieves smaller error than standard LMC. The theoretical analysis is supported by compelling numerical experiments, which demonstrate the effectiveness of our approach.

## 1 Introduction

Sampling from probability distributions is a crucial task in both statistics and machine learning. However, when the target distribution does not permit exact sampling, researchers often rely on Markov chain Monte Carlo (MCMC) methods. These techniques simulate a Markov chain that converges to the target distribution as its stationary distribution. Recently, MCMC samplers based on discretizing the continuous-time Langevin diffusion have become popular, due to its ease of implementation and ability to handle stochastic gradients (Welling and Teh, 2011).

The primary focus of this work is on the quality of samples generated by Langevin Monte Carlo (LMC) algorithms in terms of estimating the expectation \(\mathbb{E}_{\theta\sim\pi}\left[f(\theta)\right]\) for some integrand \(f\) by sample averages. In the context of Bayesian inference, the target distribution \(\pi\) is typically the posterior distribution, and computing the posterior expectation, posterior variance, or confidence intervals are of great interest. In the context of post-selection inference, the target distribution \(\pi\) is the probability distribution conditioned on the selection event, and computing the selection-adjusted p-value is the main task. LMC has been widely used in this problem as well (Markovic and Taylor, 2016; Shi et al., 2022). In all these situations, the accuracy of the sample average estimator is critical and affects the downstream data analysis.

In traditional Monte Carlo sampling, it is well known that using quasi-Monte Carlo (QMC) samples, instead of independent and identically distributed (i.i.d.) random samples, can lead to significant error reduction. So it is natural to ask whether we can apply QMC techniques to improve Langevin Monte Carlo sampling as well. In this work, we introduce the Langevin quasi-Monte Carlo (LQMC) algorithm, which replaces the i.i.d. random inputs in the LMC algorithm with quasi-random numbers.

These quasi-random numbers are carefully designed to sample from the target distribution more evenly and more balanced, leading to improved estimation accuracy.

Not all quasi-Monte Carlo point sets are suitable for simulating Markov chains. Suppose the Markov chain is driven by a sequence of uniform random vectors in the unit cube. A sufficient condition for the sequence is known as _completely uniformly distributed_ (CUD). In our implementation of the driving sequence, we use an entire period of a pseudo-random number generator (PRNG). While modern computer simulations often use PRNGs with a large period, such as Mersenne Twister with a period of \(2^{19937}-1\), our approach runs through the entire period of a PRNG with a relatively small period in the LMC algorithm. The advantage of using an entire period of a PRNG is that the points are more evenly distributed, which is more desirable for numerical integration. We illustrate the balancing property of an entire PRNG in Figure 1.

The main contributions of this paper are threefold. First, we propose a novel technique of using quasi-random numbers in Langevin-type algorithms, which can be applied to a wide range of such algorithms by substituting i.i.d. random numbers with a sequence of quasi-random numbers. The quasi-random numbers are constructed similarly as usual PRNGs, therefore no extra computational complexity is required. Second, we evaluate the performance of the proposed LQMC algorithm in a variety of numerical experiments, demonstrating that it can significantly reduce the mean squared error (MSE) of traditional LMC by a factor ranging from 2 to 500, depending on the problem. Finally, we provide theoretical analysis showing that LQMC can reduce the Monte Carlo part of the error from \(O(n^{-1/2})\) to \(O(n^{-1+\delta})\) for any \(\delta>0\) in situations where the Markov chain is strongly contracting and the integrand function \(f\) is sufficiently regular. This error reduction is consistent with the usual improvement achieved by using quasi-Monte Carlo in place of plain Monte Carlo.

The rest of the paper is organized as follows. In Section 2, we provide some background on LMC and QMC, followed by a review of related work. Section 3 describes the LQMC algorithm and its implementation details. In Section 4, we present theoretical guarantees for the proposed method. Finally, in Section 5, we provide empirical results to evaluate the performance of LQMC and compare it with the standard LMC algorithm.

## 2 Backgrounds

This section provides some background on Langevin Monte Carlo and quasi-Monte Carlo.

### Langevin Monte Carlo

Suppose we want to sample from the target distribution \(\pi(\theta)\propto\exp(-U(\theta))\) where \(\theta\in\mathbb{R}^{d}\) and \(U\) is known as the potential function. LMC algorithms are based on Euler-Maruyama discretization of the Langevin diffusion \(\theta(t)\), which satisfies the stochastic differential equation

\[\mathrm{d}\theta(t)=-\nabla U(\theta(t))\mathrm{d}t+\sqrt{2}\mathrm{d}W_{t},\] (1)

Figure 1: Scatter plots of 251 points generated from Mersenne Twister 19937 (left) and 251 points generated from a linear congruential generator (LCG) of period 251. Points from an entire period of a pseudo-random number generator (right) fill the unit square more evenly than the same number of points from a PRNG with a larger period (left).

where \(\{W_{t}\}_{t\geq 0}\) is a \(d\)-dimensional standard Brownian motion. Under mild technical conditions, the Langevin diffusion \(\theta(t)\) has \(\pi\) as its unique invariant distribution (Roberts and Tweedie, 1996). With a discretization step size \(h\), LMC updates the sample \(\theta_{k}\) by

\[\theta_{k+1}\leftarrow\theta_{k}-h\nabla U(\theta_{k})+\sqrt{2h}\xi_{k+1}\] (2)

where \(\xi_{k}\stackrel{{ iid}}{{\sim}}\mathcal{N}(0,I_{d})\).

In many applications, we are interested in computing the expectation \(\mu:=\mathbb{E}_{\theta\sim\pi}\left[f(\theta)\right]\) over \(\pi\) for some \(\pi\)-integrable function \(f\). The LMC estimator of \(\mu\) is the sample average

\[\hat{\mu}_{n}=\frac{1}{n}\sum_{k=1}^{n}f(\theta_{k}),\]

where \(n\) is the number of iterations.

Teh et al. (2016) provide an asymptotic bias-variance decomposition of the MSE of the weighted average \(\frac{\sum_{n=1}^{n}h_{k}f(\theta_{k})}{\sum_{k=1}^{n}h_{k}}\) and show that the optimal step size scales as \(h_{k}\asymp k^{-1/3}\), leading to an MSE of order \(O(n^{-2/3})\). Here \(h_{k}\) is the step size used at the \(k\)-th iteration. Vollmer et al. (2016) generalize this result to the non-asymptotic setting with a constant step size \(h\). They show that the MSE is of order \(O(h^{2}+\frac{1}{nh})\), where \(h^{2}\) corresponds to the squared bias and \(\frac{1}{nh}\) corresponds to the variance.

### Quasi-Monte Carlo

QMC is an alternative to Monte Carlo for numerical integration and is well-known for having much higher accuracy than Monte Carlo. QMC is primarily designed to numerically evaluate the integral \(\mu=\int_{[0,1]^{d}}f(\mathbf{u})\mathrm{d}\mathbf{u}\). It estimates \(\mu\) by taking points \(\mathbf{u}_{i}\in[0,1]^{d}\) and let the estimator be

\[\hat{\mu}=\frac{1}{n}\sum_{i=1}^{n}f(\mathbf{u}_{i}).\]

Unlike Monte Carlo which takes \(\mathbf{u}_{i}\) to be identically independently distributed (i.i.d.), QMC constructs the point set \(\{\mathbf{u}_{i}\}_{i=1}^{n}\) that aims to minimize the star discrepancy

\[D_{n}^{*}=D_{n}^{*}(\mathbf{u}_{1},\ldots,\mathbf{u}_{n})=\sup_{\mathbf{a}\in [0,1]^{d}}\bigg{|}\frac{1}{n}\sum_{i=1}^{n}1\{\mathbf{u}_{i}\in[\mathbf{0}, \mathbf{a})\}-\prod_{j=1}^{d}a_{j}\bigg{|}.\] (3)

The star discrepancy measures the uniformity of the point sets by comparing the fraction of points inside \([\mathbf{0},\mathbf{a})\) and the volume \(\prod_{j=1}^{d}a_{j}\), taking supreme over all the rectangles inside \([0,1]^{d}\) anchored at \(\mathbf{0}\). QMC can generate points with \(D_{n}^{*}=O(n^{-1}(\log n)^{d-1})\), thus QMC is also known as low-discrepancy sequence. Commonly used QMC points include Sobol' sequence (Sobol', 1967), Niederreiter's sequence (Niederreiter, 1987), Halton's sequence, and lattice rules. For a comprehensive survey, we refer to the monograph Dick and Pillichshammer (2010). If the integrand \(f\) has bounded variation in the sense of Hardy and Krause \(\|f\|_{\text{HK}}\), then the Koksma-Hlawka inequality (see e.g. Dick and Pillichshammer (2010)) bounds the integration error by

\[|\hat{\mu}-\mu|\leq D_{n}^{*}\cdot\|f\|_{\text{HK}}\leq O(n^{-1}(\log n)^{d-1}).\] (4)

While the Koksma-Hlawka inequality shows that QMC is asymptotically better than usual Monte Carlo, it doesn't provide a practical way to estimate the error. Moreover, integrands might have infinite Hardy-Krause variation.

One can apply randomization techniques to QMC to address both problems. Common randomization techniques include random shifts (Cranley and Patterson, 1976) and scrambling (Owen, 1995). For RQMC samples \(\mathbf{u}_{1},\ldots,\mathbf{u}_{n}\), each \(\mathbf{u}_{i}\sim\mathrm{Unif}([0,1]^{d})\) individually but they have the low-discrepancy property collectively with probability 1. One can estimate the error by multiple independent random replicates. For sufficiently smooth \(f\), the scrambled Sobol' sequence has variance \(O(n^{-3}(\log n)^{d-1})\)(Owen, 1997a,b).

### Related work

The first attempt to apply quasi-random numbers to simulate stochastic differential equations was made by Hofmann and Mathe (1997). They showed that if a numerical scheme is weakly convergent with i.i.d. samples, then using completely uniformly distributed (CUD) sequences also leads to consistent estimation. They also demonstrated that certain low-discrepancy sequences are not suitable for simulating SDEs. There have also been some efforts to apply QMC to MCMC. Owen and Tribble (2005) proposed to apply CUD sequences to a Metropolis algorithm and showed that the method is consistent in problems with finite state spaces. Chen et al. (2011) generalized the consistency result to continuous state spaces under the assumption that the Markov chain is a contraction. More recently, Dick et al. (2016); Dick and Rudolf (2014) proved that there exists constructions of the driving sequence \(\{\mathbf{u}_{k}\}_{k\geq 1}\) such that the discrepancy between the empirical distribution of MCMC samples and the target distribution is bounded by \(O(n^{-1/2}(\log n)^{1/2})\), the same rate achieved by random inputs. Another line of applying QMC to Markov chains is known as array-RQMC proposed by L'Ecuyer et al. (2008). Array-RQMC runs in parallel multiple Markov chains, and each iteration involves a complicated reordering of the states so that the low-discrepancy among the chains is maintained. Empirically, it achieves significantly smaller estimation error than usual MCMC, but theoretical guarantees remain a challenging open problem.

There has been a growing interest in using QMC techniques in various machine learning tasks, such as variational inference (Buchholz et al., 2018; Liu and Owen, 2021), policy learning and evaluation (Arnold et al., 2022), reinforcement learning with evolution strategies (Choromanski et al., 2019; Rowland et al., 2018), compression of large datasets (Dick and Feischl, 2021), example selection in stochastic gradient descent (SGD) (Lu et al., 2021), and deep learning for solving partial differential equations (Longo et al., 2021).

Numerous efforts have been devoted to improving LMC and stochastic gradient Langevin dynamics (SGLD). To overcome the instability of Euler-Maruyama discretization, various numerical schemes have been proposed, including higher-order integrators (Chen et al., 2015), underdamped LMC (Cheng et al., 2018), and stochastic Runge-Kutta diffusion (Li et al., 2019). For SGLD, variance reduction techniques such as SAGA and SVGR (Dubey et al., 2016) and control variates (Baker et al., 2019) have been proposed. LMC also provides a useful perspective for optimization, as demonstrated by the analyses in Chen et al. (2016); Dalalyan (2017); Raginsky et al. (2017); Xu et al. (2018); Erdogdu et al. (2018). Our contribution is orthogonal to all the aforementioned work, as our algorithm only modifies the random numbers used in the algorithm. Therefore, our method can be combined with other algorithms without interference.

## 3 QMC for LMC

In the LMC algorithm, we can think of the Markov chain as being driven by a sequence of uniform variables \(\mathbf{u}_{k}\) in the unit cube \([0,1]^{d}\). For instance, the Gaussian perturbation can be represented as \(\xi_{k}=\Phi^{-1}(\mathbf{u}_{k})\), where \(\Phi^{-1}\) denotes the inverse Gaussian CDF applied element-wise to \(\mathbf{u}_{k}\). If a stochastic gradient is employed, the randomness associated with the stochastic gradient can also be expressed as uniform variables. Therefore, we can write the transition of the Markov chain as \(\theta_{k+1}=\psi(\theta_{k},\mathbf{u}_{k+1})\). In typical computer experiments, \(\mathbf{u}_{k}\) are not really i.i.d. but are deterministic pseudo-random numbers. In this section, we will describe an alternative method of generating the pseudo-random numbers \(\mathbf{u}_{k}\), which are carefully constructed and can lead to more accurate sample averages.

The idea here is to use point sets that are more evenly distributed such as QMC points, which can lead to significant improvement in the usual Monte Carlo estimation. However, caution is required when using QMC points to simulate an SDE like (1). This is because the correlation between successive QMC samples may introduce undesired behavior in the Markov chain, as demonstrated in (Tribble, 2007, Section 3.2). To avoid the dependence among successive values, we require that the blocks of points \((v_{i},v_{i+1},\ldots,v_{i+d-1})\) for any lag \(d\) are uniformly distributed. This notion of uniformity is formally known as completely uniformly distributed (CUD, Korobov (1948)), which we define next.

We say an infinite sequence \(\{\mathbf{u}_{i}\}_{i=1}^{\infty}\subseteq[0,1]^{d}\) is uniformly distributed on \([0,1]^{d}\) if the star discrepancy \(D^{*}(\{\mathbf{u}_{i}\}_{i=1}^{n})\) goes to 0 as \(n\rightarrow\infty\), where the star discrepancy is defined in Equation 3.

**Definition 3.1** (Completely uniformly distributed sequence (CUD)).: _An infinite sequence \(\{v_{i}\}_{i=0}^{\infty}\subset[0,1]\) is called completely uniformly distributed, if for all positive integer \(d\), the sequence \(\{(v_{k},\ldots,v_{k+d-1})\}_{k=0}^{\infty}\subset\mathbb{R}^{d}\) is uniformly distributed on \([0,1]^{d}\). A triangular array \(\mathbf{v}_{n}=(v_{n,1},\ldots,v_{n,N_{n}})\) is called array-CUD, if for all positive integer \(d\), \(D^{*}((v_{n,1},\ldots,v_{n,d}),(v_{n,2},\ldots,v_{n,d+1}),\ldots,(v_{n,N_{n}-d+ 1},\ldots,v_{n,N_{n}}))\to 0\) as \(n\to\infty\), \(N_{n}\to\infty\)._

In other words, the subsequent \(d\)-tuples in a CUD sequence are uniformly distributed in the \(d\)-dimensional unit cube for any positive dimension \(d\). Now we are ready to present the main algorithm.

### LQMC algorithm

Let \(\{v_{i}\}_{i=0}^{\infty}\) be a CUD sequence. Let \(\mathbf{u}_{k}=(v_{kd},\ldots,v_{(k+1)d-1})\in\mathbb{R}^{d}\) be the \(k\)-th non-overlapping \(d\)-tuple from the sequence (\(k\geq 0\)). A CUD sequence is often constructed deterministically. They can further be randomized using the Cranley-Patterson (i.e. random shift) rotation (Cranley and Patterson, 1976)

\[\mathbf{u}_{k}\leftarrow\mathbf{u}_{k}+\Delta\mod 1,\]

where \(\Delta\sim\mathrm{Unif}([0,1]^{d})\). The Cranley-Patterson rotation randomly shifts each dimension of \(\mathbf{u}_{k}\) by a uniform random number separately. Then each \(\mathbf{u}_{k}\) is uniformly distributed on \([0,1]^{d}\). If we apply the inverse Gaussian CDF to each coordinate of \(\mathbf{u}_{k}\), then \(\Phi^{-1}(\mathbf{u}_{k})\sim\mathcal{N}(0,I_{d})\). In the Langevin-type algorithms, we will let \(\xi_{k}=\Phi^{-1}(\mathbf{u}_{k})\) and use \(\xi_{k}\) as the Gaussian perturbation in the \(k\)-th iteration. Specifically, each iteration takes the form

\[\theta_{k+1}=\theta_{k}-h\nabla U(\theta_{k})+\sqrt{2h}\cdot\Phi^{-1}(\mathbf{ u}_{k+1}),\quad k\geq 0.\]

Thus the transition map is \(\psi(\theta,\mathbf{u})=\theta-h\nabla U(\theta)+\sqrt{2h}\Phi^{-1}(\mathbf{u})\). In practice, we can only run finite many iterations. In the following, we will describe how to construct a finite CUD sequence and feed it into the LMC algorithm.

### Construction of CUD sequences

A finite CUD (array-CUD) sequence is often implemented by using an entire period of a pseudo random number generator with a small period (Tribble, 2007). There exist other constructions of CUD sequences. For further details, interested readers can refer to Levin (1999). We propose to use the linear-feedback shift register (LFSR) provided in Chen (2011), because it has demonstrated good performance and the computational effort required is comparable to other commonly used PRNGs.

The binary Galois LFSR (Tausworthe generator, Tausworthe (1965)) of order \(m\) updates the states \(b_{i}\in\{0,1\}\) recursively by

\[b_{i}=\sum_{j=0}^{m-1}a_{j}b_{i-m+j}\mod 2,\quad i\geq m\]

with initial states \(b_{0},b_{1},\ldots,b_{m-1}\) pre-specified. The \(m\)-tuple \((b_{i},b_{i+1},\ldots,b_{i+m-1})\in\text{GF}(2)^{m}\) can only take \(2^{m}\) different values. If there is an \(m\)-tuple that is all zero, then all \(b_{i}\)'s in this sequence must be zero. So the period of the sequence \(\{b_{i}\}_{i\geq 0}\) is at most \(n=2^{m}-1\). Moreover, the period is exactly equal to \(2^{m}-1\) if and only if the characteristic polynomial

\[x^{m}+a_{m-1}x^{m-1}+\ldots+a_{1}x+a_{0}\]

is a primitive polynomial over \(\text{GP}(2)\)(Niederreiter, 1992, Lemma 9.1). Given the states \(\{b_{i}\}_{i\geq 0}\) and an offset \(s>0\) such that \(\gcd(s,2^{m}-1)=1\), \(v_{i}\) is computed with

\[v_{i}=\sum_{j=0}^{m-1}b_{si+j}2^{-j-1},\quad i=0,1,\ldots,2^{m}-2.\]

That is, for each \(i\), we take the \(m\)-tuple \((b_{si+j})_{0\leq j<m}\) and interpret it as the binary expansion of \(v_{i}\). For the next step, we jump \(s\) bits ahead in the sequence \(\{b_{i}\}_{i\geq 0}\) and use the \(m\)-tuple starting from \(b_{s(i+1)}\). Chen (2011) provided a table of the LFSR generators for \(10\leq m\leq 32\). They searched the offsets so that the LFSR has good equi-distributed properties. Our experiments use the LFSR generators listed there.

Given the sequence \(\{v_{i}\}_{i=0}^{n-1}\) of length \(n\), we repeat it \(d\) times and arrange \(v_{i}\)'s in the following \(n\times d\) matrix

\[\begin{pmatrix}v_{0}&v_{1}&\cdots&v_{d-1}\\ v_{d}&v_{d+1}&\cdots&v_{2d-1}\\ \vdots&\vdots&\ddots&\vdots\\ v_{(n-1)d}&v_{(n-1)d+1}&\cdots&v_{nd-1}\end{pmatrix}.\] (5)

We run the LMC algorithm \(n=2^{m}-1\) iterations. The \(k\)-th uniform vector \(\mathbf{u}_{k}\) is the \(k\)-th row of the above matrix. The procedure is summarized in Algorithm 1.

```
0: Number of iterations \(n=2^{m}-1\) such that \(\gcd(2^{m}-1,d)=1\), step size \(h\), initial value \(\theta_{0}\)  Generate an LFSR sequence \(\{v_{i}\}_{i\geq 0}\) of period \(2^{m}-1\).  Let \(\mathbf{u}_{k}=(v_{(k-1)d},\ldots,v_{kd-1})\in[0,1]^{d}\), for \(1\leq k\leq n\).  Apply Cranley-Patterson rotation (random shift) to \(\mathbf{u}_{k}\)'s. for\(k\gets 1,\ldots,n\)do \(\theta_{k}\leftarrow\theta_{k-1}-h\nabla U(\theta_{k-1})+\sqrt{2h}\Phi^{-1}( \mathbf{u}_{k})\) endfor Output:\(\theta_{1},\ldots,\theta_{n}\) ```

**Algorithm 1** Langevin quasi-Monte Carlo (LQMC)

If \(\gcd(n,d)=1\), then each column of the matrix (5) contains no repeated values. This means that among the \(n=2^{m}-1\) iterations of the LQMC algorithm, each dimension uses one value in each sub-interval \((\frac{k}{2^{m}},\frac{k+1}{2^{m}}]\) at most once (\(0\leq k\leq 2^{m}-1\)). This perfect one-dimensional stratification is one of the reasons why CUD may achieve smaller estimation error than pseudo-random numbers. If \(\gcd(n,d)>1\), then we take \(d^{\prime}\) to be the smallest integer greater than \(d\) and co-prime with \(n\). We then create the matrix in (5) similarly but with \(d^{\prime}\) columns. In the LQMC algorithm, we take \(\mathbf{u}_{k}\) to be the \(k\)-th row of the matrix but only use the first \(d\) coordinates.

Algorithm 1 may seem to be restricted by having a fixed number of iterations, \(n=2^{m}-1\). However, in practice, the LQMC algorithm can be started with an initial value of \(m\). If the chain does not converge after \(2^{m}-1\) iterations, one can continue the chain with another freshly generated LFSR, possibly with a larger period. This allows for flexibility in adjusting the number of iterations based on the convergence of the chain. Additionally, if a burn-in period is required, one can first run the algorithm with an LFSR of a small period to serve as the burn-in stage and then continue with a larger LFSR. Furthermore, running multiple chains with independent random shifts is embarrassingly parallel. We present the algorithm in the form of the basic LMC algorithm with accurate gradient and constant learning rate. However, as we noted previously, other Langevin-type algorithms can also utilize the CUD sequence directly by substituting the pseudo-random numbers with the LFSR sequence.

## 4 Theoretical guarantee

Here we study the estimation error \(|n^{-1}\sum_{k=1}^{n}f(\theta_{k})-\pi(f)|\) of LQMC for some test function \(f\) that is 1-Lipschitz and bounded. As the first attempt to prove the convergence rate of using QMC in LMC, we impose the relatively strong conditions of smoothness and convexity.

**Assumption 1**.: _The potential function \(U\) is \(L\)-smooth_

\[\|\nabla U(\theta)-\nabla U(\theta^{\prime})\|_{2}\leq L\|\theta-\theta^{ \prime}\|_{2},\quad\forall\;\theta,\theta^{\prime},\]

_and \(M\)-strongly convex_

\[U(\theta^{\prime})\geq U(\theta)+\nabla U(\theta)^{\intercal}( \theta^{\prime}-\theta)+\frac{M}{2}\|\theta^{\prime}-\theta\|_{2}^{2},\quad \forall\;\theta,\theta^{\prime}.\]

We will also assume a constant step size \(h\). While LMC with vanishing step sizes converges weakly to the target distribution, in practice a constant step size is often used (Vollmer et al., 2016; Brosse et al., 2018). With a constant step size, we can derive a non-asymptotic error bound for LQMC.

Assumption 1 implies that if the step size \(h\leq\frac{2}{L+M}\), then the transition map \(\psi\) is a strong contraction with parameter \(\rho=1-hM\), i.e.

\[\|\psi(\theta,\mathbf{u})-\psi(\theta^{\prime},\mathbf{u})\|_{2}=\|\theta- \theta^{\prime}-h(\nabla U(\theta)-\nabla U(\theta^{\prime}))\|_{2}\leq\rho\| \theta-\theta^{\prime}\|_{2}.\] (6)

See e.g. Lemma 2 of Dalalyan and Karagulyan (2019). The strong contraction implies that if we start two chains from \(\theta\) and \(\theta^{\prime}\), and use the same random numbers at every step, then the two chains will merge exponentially fast. In other words, the state \(\theta_{k}\) largely depends on the most recent iterations and quickly forgets about the past history. Formally, let \(\mathbf{w}_{k}^{(\ell)}=(\mathbf{u}_{k},\ldots,\mathbf{u}_{k-\ell+1})\) denote the random numbers used in the most recent \(\ell\) steps. Define the \(\ell\)-step transition as

\[\theta_{k}=\psi_{\ell}(\theta_{k-\ell},\mathbf{w}_{k}^{(\ell)})\]

and let \(\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\) denote the value of \(f(\theta_{k})\) marginalized over \(\theta_{k-\ell}\sim\pi\), i.e.

\[\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})=\int f\circ\psi_{\ell}(x,\mathbf{w}_{ k}^{(\ell)})\pi(\mathrm{d}x).\]

Thus \(\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\) only depends on the most recent \(\ell\) iterations. Due to the strong contraction, \(|\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})-f(\theta_{k})|\) decays exponentially fast with \(\ell\). So for large \(\ell\), the estimation error of \(n^{-1}\sum_{k=1}^{n}f(\theta_{k})\) is close to the error of \(\frac{1}{n-\ell}\sum_{k=\ell+1}^{n}\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\). The latter can be viewed as a \(d\ell\)-dimensional numerical integration scheme based on the point set \(\{\mathbf{w}_{k}^{(\ell)}\}_{k=\ell+1}^{n}\). By leveraging the discrepancy bound of the LFSR sequence and assuming that \(\bar{f}_{\ell}\) has bounded variation in the sense of Hardy and Krause, we can derive an error bound using the Koksma-Hlawka inequality (4). Now we state the main error bound and leave the detailed proof in the Appendix A.

**Theorem 4.1** (Error bound of LQMC).: _Let Assumption 1 hold. Define the step size \(h\leq\frac{2}{L+M}\), \(\rho=1-hM\), \(\ell=\lceil(1/2)\log_{\rho}h\rceil\). Let \(\theta_{1},\ldots,\theta_{n}\) be the output of Algorithm 1 which runs \(n\) iterations with step size \(h\leq\frac{2}{L+M}\). Assume the LFSR sequence \(\{v_{i}\}_{i\geq 0}\) in use has period \(n=2^{m}-1\), offset \(s\), and gcd\((m,n)=\gcd(d\ell,n)=1\). If \(\bar{f}_{\ell}\) has bounded variation in the sense of Hardy and Krause, then as \(n\to\infty\) we have_

\[\left|\frac{1}{n}\sum_{k=1}^{n}f(\theta_{k})-\pi(f)\right|\leq C_{1}n^{-1+ \delta}+C_{2}h^{1/2},\quad\forall\ \delta>0.\]

_Here \(\delta\) hides poly-logarithmic factors \((\log n)^{d}\), \(C_{1}\) depends on \(d,\ell\) and \(\|\bar{f}_{\ell}\|_{\text{HK}}\), and \(C_{2}=\frac{3\sqrt{2}}{2}\frac{L}{M}d+\max_{0\leq k\leq n}\|\theta_{k}\|+\mathbb{ E}_{\pi}\left[\|\theta\|\right]\)._

The upper bound consists of two terms. The first term represents the numerical integration error, which arises from the discrepancy of the point set used in the integration scheme. By utilizing low-discrepancy CUD sequences, we can reduce this numerical integration error (the first term) from the standard rate of \(O(n^{-1/2})\) to a faster rate of \(O(n^{-1+\delta})\) for any \(\delta>0\). However, it is important to note that when using a constant step size \(h\) in LMC, the bias term (second term) does not vanish. This bias term includes not only the discretization error of the Langevin diffusion, but also the difference between \(f(\theta_{k})\) and its truncated version \(\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\). Consequently, the bias term in our analysis is larger than the bias term in Vollmer et al. (2016), which employs different techniques and assumptions based on the Poisson equation.

The theorem's assumption of finite Hardy-Krause variation is a common requirement in error bounds for QMC methods, and it can be challenging to verify in practice. Basu and Owen (2016) provide sufficient conditions in order for \(f\circ\psi_{\ell}\) to have finite HK variation, requiring the \(\ell\)-step transition \(\psi_{\ell}\) to be sufficiently smooth. In the next section, we aim to assess the practical performance of the proposed LQMC algorithm through numerical experiments.

## 5 Numerical experiments

To comprehensively evaluate the performance of the algorithm, we will consider both convex and non-convex potentials, both low-dimensional and high-dimensional state spaces, both accurate and stochastic gradients, both smooth and discontinuous integrands, as well as different learning rate schedules. Additional numerical results can be found in the Appendix B.

### Bayesian logistic regression

We first consider the Bayesian logistic model

\[y_{i} \mid x_{i}\sim\text{Bernoulli}((1+\exp(-x_{i}^{\intercal}\beta))^{-1 }),\quad 1\leq i\leq N,\] \[\beta \sim\mathcal{N}(0,I_{d}).\]

We take \(N=20\), \(d=10\). The features \(x_{i}\) are generated from \(\mathcal{N}(0,\Sigma)\) with \(\Sigma_{ij}=2^{-|i-j|}\). The coefficients \(\beta\) and the data \(y_{i}\)'s are generated from the same model. We consider the test functions \(f(x)=x_{j},x_{1}^{2},\mathbf{1}_{\{x_{j}>0\}}\) for \(j=1,\ldots,d\). The step size \(h\) is fixed to \(0.001\).

We compute the MSE of the estimator based on usual LMC and the proposed LQMC with CUD sequences and report the MSE averaged over all coordinates and 20 random replicates. We do not have a closed form for the expectations \(\mathbb{E}\left[f\right]\), so the ground truth is estimated using a high-accuracy estimator proposed in He et al. (2023) using scrambled Sobol' sequence with a very large sample size.

In Figure 2 (top panel), we present a log-log plot of the MSE against the number of iterations. Across all three test functions, we observe that LQMC reduces the MSE by a factor ranging from 4 to 8. As the number of iterations increases, the curve corresponding to LQMC reaches a plateau. This behavior can be attributed to the discretization error inherent in the unadjusted LMC, which cannot be further reduced by increasing the number of iterations.

In the bottom panel of Figure 2, we increase the number of observations to \(N=100\) and incorporate stochastic gradient estimation in the Langevin algorithm. Specifically, at each iteration, we estimate the gradient using a random subset of 10 observations. The results demonstrate that LQMC still provides a big improvement when \(n\) is smaller than \(2^{14}\). However, as \(n\) surpasses \(2^{14}\), we observe that the LQMC curve flattens again. It is worth noting that the improvement achieved by LQMC in this scenario is less pronounced compared to the previous example, primarily due to the presence of noise in the gradient estimates.

### Bayesian linear regression

Now we try a higher-dimensional example with Bayesian linear regression. The model is defined as

\[y_{i} \sim\mathcal{N}(x_{i}^{\intercal}\beta,\sigma^{2}=4^{-1}),\quad 1 \leq i\leq N,\] \[\beta \sim\mathcal{N}(0,I).\]

We take \(d=100\) and \(N=20\). We generate \(x_{i}\in\mathbb{R}^{d}\) similarly as in the logistic regression example. The test functions and step size are also unchanged. The posterior distribution of \(\beta\) has the closed form \(\mathcal{N}\left((\frac{X^{\intercal}X}{\sigma^{2}}+I)^{-1}\frac{XY}{\sigma^ {2}},(\frac{XY^{\intercal}X}{\sigma^{2}}+I)^{-1}\right)\). The results are shown in Figure 3. We see that even at 100 dimension, LQMC still brings a substantial improvement over LMC in terms of MSE. In

Figure 2: Bayesian logistic regression with accurate gradients (top) and stochastic gradients (bottom).

particular, for the integrand \(f(x)=x_{j}\), LQMC achieves a reduction in MSE of approximately 500-fold compared to LMC.

### A hierarchical Bayesian model

We consider a hierarchical Bayesian model known as the crossed random effect model

\[Y_{ij}\sim\mathcal{N}(\mu+a_{i}+b_{j},1),\quad 1\leq i\leq I,\;1 \leq j\leq J,\] \[\mu\sim\mathcal{N}(0,1),\;a_{i}\stackrel{{ iid}}{{ \sim}}\mathcal{N}(0,\sigma_{a}^{2}),\;b_{j}\stackrel{{ iid}}{{ \sim}}\mathcal{N}(0,\sigma_{b}^{2}),\] \[\log(\sigma_{a}^{2}),\;\log(\sigma_{b}^{2})\stackrel{{ iid}}{{ \sim}}\mathcal{N}(0,1).\]

The goal is to sample from the posterior distribution of \((\mu,\mathbf{a},\mathbf{b},\log(\sigma_{a}^{2}),\log(\sigma_{b}^{2}))\), which has dimension \(d=I+J+3\). We take \(I=3\), \(J=5\). We will consider the test functions \(f(x)=x_{j}\) (\(1\leq j\leq d\)). The ground truth of \(\mathbb{E}\left[f(x)\right]\) is estimated by Langevin dynamics with Metropolis adjustments (MALA) using a large sample size.

We will compare the performance of the LQMC algorithm using three different step sizes: a constant step size of \(10^{-4}\), a constant step size of \(10^{-2}\), and decreasing step sizes with \(h_{k}=c_{0}(c_{1}+k)^{-1/3}\). The choice of \(c_{0}\) and \(c_{1}\) ensures that the step size decreases from \(10^{-2}\) to \(10^{-4}\) throughout the entire algorithm. The use of the exponent \(-1/3\) in the decreasing step sizes is recommended in Teh et al. (2016). The results of these comparisons are presented in Figure 4.

In the small step size case (left panel), we observe that the errors of LMC and LQMC are initially comparable for small values of \(n\). This is because the algorithm converges slowly, and thus the error is dominated by the bias. However, as \(n\) increases, the improvement of LQMC becomes evident. In the large step size case (middle panel), the MSE of LQMC is consistently smaller than that of LMC even for small values of \(n\). This is because the algorithm converges faster to the target distribution with a larger step size \(h\). Therefore, the improvement of LQMC is more pronounced. Interestingly, in this particular example, using decreasing step sizes yields similar accuracy to using a constant step size of \(10^{-4}\). It is worth noting that the MSE of LMC does not decrease at a rate of \(n^{-2/3}\) as in Teh et al. (2016). This is because the line in the plot does not represent the accuracy against the iteration \(k\) within a single training process. Instead, it reflects the accuracy achieved after completing all \(n\) iterations of the algorithm, considering different values of \(n\).

Figure 4: Crossed random effect.

Figure 3: Bayesian linear regression in 100 dimensions.

### Nonconvex potential

Finally we investigate a double-well potential function \(U(x)=\frac{1}{4}x^{2}-\frac{1}{2}\log(1+x^{2})\) from Pages and Panloup (2018). We know \(\mathbb{E}\left[x\right]=0\) and \(\mathbb{E}\left[\mathbf{1}_{\left\{x>0\right\}}\right]=0.5\). The second moment \(\mathbb{E}\left[x^{2}\right]\) is computed by Gaussian quadrature. See the results in Figure 5. Since the potential has two separate local minimums, it takes longer for the Langevin algorithm to explore the space sufficiently and converge to the target distribution. Once converged, the improvement of LQMC over LMC is still significant.

## Acknowledgments and Disclosure of Funding

The author thanks Prof. Art Owen for helpful conversations. This work was partially funded by the NSF grant DMS-2152780 and the Stanford Data Science Scholars program.

## References

* Arnold et al. (2022) Arnold, S. M., L'Ecuyer, P., Chen, L., Chen, Y.-F., and Sha, F. (2022). Policy learning and evaluation with randomized quasi-Monte Carlo. In _International Conference on Artificial Intelligence and Statistics_, pages 1041-1061. PMLR.
* Baker et al. (2019) Baker, J., Fearnhead, P., Fox, E. B., and Nemeth, C. (2019). Control variates for stochastic gradient MCMC. _Statistics and Computing_, 29:599-615.
* Basu and Owen (2016) Basu, K. and Owen, A. B. (2016). Transformations and Hardy-Krause variation. _SIAM Journal on Numerical Analysis_, 54(3):1946-1966.
* Brosse et al. (2018) Brosse, N., Durmus, A., and Moulines, E. (2018). The promises and pitfalls of stochastic gradient Langevin dynamics. _Advances in Neural Information Processing Systems_, 31.
* Buchholz et al. (2018) Buchholz, A., Wenzel, F., and Mandt, S. (2018). Quasi-Monte Carlo variational inference. In _International Conference on Machine Learning_, pages 668-677. PMLR.
* Chen et al. (2016) Chen, C., Carlson, D., Gan, Z., Li, C., and Carin, L. (2016). Bridging the gap between stochastic gradient MCMC and stochastic optimization. In _Artificial Intelligence and Statistics_, pages 1051-1060. PMLR.
* Chen et al. (2015) Chen, C., Ding, N., and Carin, L. (2015). On the convergence of stochastic gradient MCMC algorithms with high-order integrators. _Advances in neural information processing systems_, 28.
* Chen (2011) Chen, S. (2011). _Consistency and convergence rate of Markov chain quasi Monte Carlo with examples_. PhD thesis, Stanford University.
* Chen et al. (2011) Chen, S., Dick, J., and Owen, A. (2011). Consistency of Markov chain quasi-Monte Carlo on continuous state spaces. _The Annals of Statistics_, 39(2):673-701.
* Cheng et al. (2018) Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2018). Underdamped Langevin MCMC: A non-asymptotic analysis. In _Conference on learning theory_, pages 300-323. PMLR.

Figure 5: Double well potential.

Choromanski, K., Pacchiano, A., Parker-Holder, J., and Tang, Y. (2019). Structured Monte Carlo sampling for nonisotropic distributions via determinantal point processes. _arXiv preprint arXiv:1905.12667_.
* Cranley and Patterson (1976) Cranley, R. and Patterson, T. N. (1976). Randomization of number theoretic methods for multiple integration. _SIAM Journal on Numerical Analysis_, 13(6):904-914.
* Dalalyan (2017) Dalalyan, A. (2017). Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent. In _Conference on Learning Theory_, pages 678-689. PMLR.
* Dalalyan and Karagulyan (2019) Dalalyan, A. S. and Karagulyan, A. (2019). User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient. _Stochastic Processes and their Applications_, 129(12):5278-5311.
* Dick and Feischl (2021) Dick, J. and Feischl, M. (2021). A quasi-monte carlo data compression algorithm for machine learning. _Journal of Complexity_, 67:101587.
* Dick and Pillichshammer (2010) Dick, J. and Pillichshammer, F. (2010). _Digital Sequences, Discrepancy and Quasi-Monte Carlo Integration_. Cambridge University Press, Cambridge.
* Dick and Rudolf (2014) Dick, J. and Rudolf, D. (2014). Discrepancy estimates for variance bounding Markov chain quasi-Monte Carlo. _Electron. J. Probab_, 19(105):1-24.
* Dick et al. (2016) Dick, J., Rudolf, D., and Zhu, H. (2016). Discrepancy bounds for uniformly ergodic Markov chain quasi-Monte Carlo. _Annals of Applied Probability_, 26(5):3178-3205.
* Dubey et al. (2016) Dubey, K. A., J Reddi, S., Williamson, S. A., Poczos, B., Smola, A. J., and Xing, E. P. (2016). Variance reduction in stochastic gradient Langevin dynamics. _Advances in neural information processing systems_, 29.
* Erdogdu et al. (2018) Erdogdu, M. A., Mackey, L., and Shamir, O. (2018). Global non-convex optimization with discretized diffusions. _Advances in Neural Information Processing Systems_, 31.
* He et al. (2023) He, Z., Zheng, Z., and Wang, X. (2023). On the error rate of importance sampling with randomized quasi-Monte Carlo. _SIAM Journal on Numerical Analysis_, 61(2):515-538.
* Hofmann and Mathe (1997) Hofmann, N. and Mathe, P. (1997). On quasi-Monte Carlo simulation of stochastic differential equations. _Mathematics of computation_, 66(218):573-589.
* Korobov (1948) Korobov, N. (1948). On functions with uniformly distributed fractional parts. In _Dokl. Akad. Nauk SSSR_, volume 62, pages 21-22.
* L'Ecuyer et al. (2008) L'Ecuyer, P., Lecot, C., and Tuffin, B. (2008). A randomized quasi-Monte Carlo simulation method for Markov chains. _Operations Research_, 56(4):958-975.
* Levin (1999) Levin, M. B. (1999). Discrepancy estimates of completely uniformly distributed and pseudorandom number sequences. _International Mathematics Research Notices_, 1999(22):1231-1251.
* Li et al. (2019) Li, X., Wu, Y., Mackey, L., and Erdogdu, M. A. (2019). Stochastic Runge-Kutta accelerates Langevin Monte Carlo and beyond. _Advances in neural information processing systems_, 32.
* Liu and Owen (2021) Liu, S. and Owen, A. B. (2021). Quasi-Monte Carlo quasi-Newton in variational Bayes. _The Journal of Machine Learning Research_, 22(1):11043-11065.
* Longo et al. (2021) Longo, M., Mishra, S., Rusch, T. K., and Schwab, C. (2021). Higher-order quasi-monte carlo training of deep neural networks. _SIAM Journal on Scientific Computing_, 43(6):A3938-A3966.
* Lu et al. (2021) Lu, Y., Meng, S. Y., and De Sa, C. (2021). A general analysis of example-selection for stochastic gradient descent. In _International Conference on Learning Representations_.
* Markovic and Taylor (2016) Markovic, J. and Taylor, J. (2016). Bootstrap inference after using multiple queries for model selection. _arXiv preprint arXiv:1612.07811_.
* Niederreiter (1987) Niederreiter, H. (1987). Point sets and sequences with small discrepancy. _Monatshefte fur Mathematik_, 104:273-337.
* Niederreiter (2016)Niederreiter, H. (1992). _Random Number Generation and Quasi-Monte Carlo Methods_. SIAM.
* Owen (1995) Owen, A. B. (1995). Randomly permuted \((t,m,s)\)-nets and \((t,s)\)-sequences. In _Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing_, pages 299-317, New York. Springer-Verlag.
* Owen (1997a) Owen, A. B. (1997a). Monte Carlo variance of scrambled net quadrature. _SIAM Journal of Numerical Analysis_, 34(5):1884-1910.
* Owen (1997b) Owen, A. B. (1997b). Scrambled net variance for integrals of smooth functions. _Annals of Statistics_, 25(4):1541-1562.
* Owen and Tribble (2005) Owen, A. B. and Tribble, S. D. (2005). A quasi-Monte Carlo Metropolis algorithm. _Proceedings of the National Academy of Sciences_, 102(25):8844-8849.
* Pages and Panloup (2018) Pages, G. and Panloup, F. (2018). Weighted multilevel langevin simulation of invariant measures. _Annals of Applied Probability_, 28(6):3358-3417.
* Raginsky et al. (2017) Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017). Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis. In _Conference on Learning Theory_, pages 1674-1703. PMLR.
* Roberts and Tweedie (1996) Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of Langevin distributions and their discrete approximations. _Bernoulli_, pages 341-363.
* Rowland et al. (2018) Rowland, M., Choromanski, K. M., Chalus, F., Pacchiano, A., Sarlos, T., Turner, R. E., and Weller, A. (2018). Geometrically coupled monte carlo sampling. _Advances in Neural Information Processing Systems_, 31.
* Shi et al. (2022) Shi, J., Liu, C., and Mackey, L. (2022). Sampling with mirrored stein operators. In _International Conference on Learning Representations_.
* Sobol' (1967) Sobol', I. M. (1967). On the distribution of points in a cube and the approximate evaluation of integrals. _Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki_, 7(4):784-802.
* Tausworthe (1965) Tausworthe, R. C. (1965). Random numbers generated by linear recurrence modulo two. _Mathematics of Computation_, 19(90):201-209.
* Teh et al. (2016) Teh, Y. W., Thiery, A. H., and Vollmer, S. J. (2016). Consistency and fluctuations for stochastic gradient Langevin dynamics. _Journal of Machine Learning Research_, 17.
* Tribble (2007) Tribble, S. D. (2007). _Markov chain Monte Carlo algorithms using completely uniformly distributed driving sequences_. PhD thesis, Citeseer.
* Vollmer et al. (2016) Vollmer, S. J., Zygalakis, K. C., and Teh, Y. W. (2016). Exploration of the (non-) asymptotic bias and variance of stochastic gradient Langevin dynamics. _The Journal of Machine Learning Research_, 17(1):5504-5548.
* Welling and Teh (2011) Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688.
* Xu et al. (2018) Xu, P., Chen, J., Zou, D., and Gu, Q. (2018). Global convergence of Langevin dynamics based algorithms for nonconvex optimization. _Advances in Neural Information Processing Systems_, 31.

Proofs

### Proof of Theorem 4.1

We start by decomposing the error \(|\frac{1}{n}\sum_{k=1}^{n}f(\theta_{k})-\pi(f)|\) into three parts

\[\left|\frac{1}{n}\sum_{k=1}^{n}f(\theta_{k})-\pi(f)\right| \leq\left|\frac{1}{n}\sum_{k=\ell+1}^{n}f(\theta_{k})-\frac{1}{n} \sum_{k=\ell+1}^{n}\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\right|+\left|\frac{ 1}{n}\sum_{k=\ell+1}^{n}\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})-\pi(f)\right|+ \frac{\ell}{n}2\|f\|_{\infty}\] \[=(I)+(II)+\frac{2\ell}{n}\|f\|_{\infty}.\]

We first upper bound \((I)\).

**Lemma 1** (Upper bound of \((I)\); adapted from Lemma 6.1.4 of Chen (2011)).: _If the transition map \(\psi\) is a contraction with parameter \(\rho\) and if \(f\) is 1-Lipschitz, then_

\[|\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})-f(\theta_{k})| \leq\left(\max_{0\leq i\leq n}\|\theta_{i}\|+\mathbb{E}_{\pi} \left[\|\theta\|\right]\right)\rho^{\ell}.\]

Proof of Lemma 1.: Note that

\[|\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})-f(\theta_{k})| \leq\int|f(\psi_{\ell}(\theta,\mathbf{w}_{k}^{(\ell)}))-f(\psi_{ \ell}(\theta_{k-\ell},\mathbf{w}_{k}^{(\ell)}))|\pi(\mathrm{d}\theta)\] \[\leq\int\|(\psi_{\ell}(\theta,\mathbf{w}_{k}^{(\ell)}))-(\psi_{ \ell}(\theta_{k-\ell},\mathbf{w}_{k}^{(\ell)}))\|\pi(\mathrm{d}\theta)\] \[\leq\rho\int\|(\psi_{\ell-1}(\theta,\mathbf{w}_{k-1}^{(\ell-1)}) )-(\psi_{\ell-1}(\theta_{k-\ell},\mathbf{w}_{k-1}^{(\ell-1)}))\|\pi(\mathrm{d}\theta)\] \[\leq\rho^{\ell}\int\|\theta-\theta_{k-\ell}\|\pi(\mathrm{d}\theta)\] \[\leq\rho^{\ell}(\max_{0\leq i\leq n}\|\theta_{i}\|+\mathbb{E}_{ \pi}\left[\|\theta\|\right]).\]

To bound \((II)\), note that \(\frac{1}{n-\ell}\sum_{k=\ell+1}^{n}\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})\) is estimating

\[\mathbb{E}\left[\bar{f}_{\ell}(\mathbf{w}^{(\ell)})\right]=\int \psi_{\ell}(\theta,\mathbf{w}^{(\ell)})\pi(\mathrm{d}\theta)\mathrm{d} \mathbf{w}^{(\ell)}=:\pi P_{\ell}(f).\]

Here, \(\pi P_{\ell}\) denote the distribution of the \(\ell\)-step state \(\theta_{\ell}\) starting from \(\theta_{0}\sim\pi\). So we have the further decomposition

\[(II) \leq\left|\frac{1}{n-\ell}\sum_{k=\ell+1}^{n}\bar{f}_{\ell}( \mathbf{w}_{k}^{(\ell)})-\pi(f)\right|+\frac{\ell}{n-\ell}\|f\|_{\infty}\] \[\leq|\pi(f)-\pi P_{\ell}(f)|+\left|\frac{1}{n-\ell}\sum_{k=\ell+1 }^{n}\bar{f}_{\ell}(\mathbf{w}_{k}^{(\ell)})-\pi P_{\ell}(f)\right|+\frac{\ell }{n-\ell}\|f\|_{\infty}\] \[\leq(II)^{\prime}+(II)^{\prime\prime}+\frac{\ell}{n-\ell}\|f\|_{ \infty}.\]

The first term \((II)^{\prime}\) is due to the discretization in time. The second term \((II)^{\prime\prime}\) is the numerical integration error.

To bound \((II)^{\prime}\), we use the following result.

**Lemma 2** (Upper bound on discretization error \((II)^{\prime}\)).: _Under Assumption 1, we have for \(f\) 1-Lipschitz,_

\[|\pi(f)-\pi P_{\ell}(f)|\leq\frac{3\sqrt{2}}{2}\frac{L}{M}h^{1/2}d.\]Proof of Lemma 2.: We let \(\theta(t)\) be the continuous-time Langevin diffusion with \(\theta(0)=\theta_{0}\sim\pi\), \(W_{t_{k+1}}-W_{t_{k}}=\sqrt{h}\xi_{k+1}\), where \(\xi_{k+1}\stackrel{{ iid}}{{\sim}}\mathcal{N}(0,I_{d})\), \(t_{k}=kh\). So we have

\[\theta(t_{k+1})=\theta(t_{k})-\int_{t_{k}}^{t_{k+1}}\nabla U(\theta(s))\mathrm{d }s+\sqrt{2h}\xi_{k+1}\]

and

\[\theta_{k+1}=\theta_{k}-h\nabla U(\theta_{k})+\sqrt{2h}\xi_{k+1}.\]

Combing the previous two equations gives

\[\theta(t_{k+1})-\theta_{k+1}=\theta(t_{k})-\theta_{k}-h[\nabla U(\theta(t_{k}) )-\nabla U(\theta_{k})]-\int_{t_{k}}^{t_{k+1}}\nabla U(\theta(s))-\nabla U( \theta(t_{k}))\mathrm{d}s.\]

Let \(\Delta_{k}=\theta(t_{k})-\theta_{k}\). The last display reads

\[\Delta_{k+1}=\Delta_{k}-h[\nabla U(\theta_{k}+\Delta_{k})-\nabla U(\theta_{k} )]-\int_{t_{k}}^{t_{k+1}}\nabla U(\theta(s))-\nabla U(\theta(t_{k}))\mathrm{d }s.\]

By the contracting property (6) in the main paper,

\[\|\Delta_{k}-h[\nabla U(\theta_{k}+\Delta_{k})-\nabla U(\theta_{k})]\|\leq \rho\|\Delta_{k}\|.\]

Taking expectation and use \(L\)-smoothness of \(U\), we have

\[\mathbb{E}\left[\|\Delta_{k+1}\|\right]\leq\rho\mathbb{E}\left[\|\Delta_{k}\| \right]+L\int_{t_{k}}^{t_{k+1}}\mathbb{E}\left[\|\theta(s)-\theta(t_{k})\| \right]\mathrm{d}s.\]

By Lemma 3 of Dalalyan and Karagulyan (2019), \(\mathbb{E}\left[\|\nabla U(\theta)\|_{2}^{2}\right]\leq Ld\). So we have \(\mathbb{E}\left[\|\nabla U(\theta)\|\right]\leq\sqrt{d\mathbb{E}\left[\|\nabla U (\theta)\|_{2}^{2}\right]}\leq\sqrt{L}d\). Because \(\theta(t)\) is a stationary process,

\[\int_{t_{k}}^{t_{k+1}}\mathbb{E}\left[\|\theta(s)-\theta(t_{k}) \|\right]\mathrm{d}s =\int_{0}^{h}\mathbb{E}\left[\|\theta(t)-\theta(0)\|\right] \mathrm{d}t\] \[=\int_{0}^{h}\mathbb{E}\left[\|-\int_{0}^{t}\nabla U(\theta(s)) \mathrm{d}s+\sqrt{2}W_{t}\|\right]\mathrm{d}t\] \[\leq\int_{0}^{h}\int_{0}^{t}\mathbb{E}\left[\|\nabla U(\theta(s)) \|\right]\mathrm{d}s\mathrm{d}t+\int_{0}^{h}\sqrt{2}\mathbb{E}\left[\|W_{t}\| \right]\mathrm{d}t\] \[=\frac{h^{2}}{2}\sqrt{L}d+\int_{0}^{h}\sqrt{2t}\mathbb{E}\left[\| \xi_{1}\|\right]\mathrm{d}t.\]

Note that

\[\mathbb{E}\left[\|\xi_{1}\|\right]=\sqrt{2}\frac{\Gamma(d/2+1/2)}{\Gamma(d/2)} \leq\sqrt{2}(\frac{d+1}{2})^{1/2}=\sqrt{d+1}.\]

Thus,

\[\int_{t_{k}}^{t_{k+1}}\mathbb{E}\left[\|\theta(s)-\theta(t_{k}) \|\right]\mathrm{d}s \leq\frac{1}{2}L^{1/2}h^{2}d+\frac{3\sqrt{2}}{2}h^{3/2}d^{1/2}\] \[\leq\frac{\sqrt{2}}{2}h^{3/2}d+\frac{3\sqrt{2}}{2}h^{3/2}d^{1/2}\] \[\leq\frac{3\sqrt{2}}{2}h^{3/2}d.\]

Denote \(r=\frac{3\sqrt{2}}{2}Lh^{3/2}d\). So

\[\mathbb{E}\left[\|\Delta_{k+1}\|\right] \leq\rho\mathbb{E}\left[\|\Delta_{k}\|\right]+r\leq\rho^{k+1} \mathbb{E}\left[\|\Delta_{0}\|\right]+\sum_{i=0}^{k}\rho^{i}r\] \[\leq\frac{r}{1-\rho}=\frac{3\sqrt{2}}{2}\frac{L}{M}h^{1/2}d\]Therefore, for any \(k\geq 1\),

\[|\pi(f)-\pi P_{k}(f)| =|\mathbb{E}\left[f(\theta(t_{k}))-\mathbb{E}\left[f(\theta_{k}) \right]\right]|\leq\mathbb{E}\left[|f(\theta(t_{k}))-f(\theta_{k})|\right]\] \[\leq\mathbb{E}\left[\|\Delta_{k}\|\right]\leq\frac{3\sqrt{2}}{2} \frac{L}{M}h^{1/2}d.\]

If we use a noisy gradient \(\hat{g}(\theta_{k})=\nabla U(\theta_{k})+e_{k}\) where \(e_{k}\) is the noise with mean zero and bounded variance such that \(\mathbb{E}(||e_{k}||_{2}^{2})\leq\sigma^{2}\), then an extra term \(2h\sigma\) will appear in Lemma 2. As \(\sigma^{2}\) is usually expected to be proportional to the dimension, this additional term is of the same order as the other term.

**Theorem A.1** (Theorem 9.8 of Niederreiter (1992)).: _Let \(v_{0},v_{1},\ldots\) be an LFSR with offset \(s\) and period \(n=2^{m}-1\) which satisfy gcd\((m,n)=1\). Then the sequence \(\{\mathbf{u}_{i}\}_{i=0}^{n-1}\subset[0,1]^{s}\) with \(\mathbf{u}_{i}=(v_{i},v_{i+1},\ldots,v_{i+s-1})\) has, on average, star-discrepancy_

\[O(n^{-1}(\log n)^{d+1}\log\log n)\]

_with an implied constant depending only on \(d\) and the average is taken over all primitive polynomials over GF(2) of degree \(m\)._

Proof of Theorem 4.1.: The error on the left-hand-side is bounded by

\[(I)+(II)^{\prime}+(II)^{\prime\prime}+\frac{4\ell}{n}\|f\|_{\infty}.\]

Lemma 1 shows that \((I)\leq(\max_{0\leq i<n}\|\theta_{i}\|+\mathbb{E}_{\pi}\left[\|\theta\|\right]) \rho^{\ell}\leq(\max_{0\leq i\leq n}\|\theta_{i}\|+\mathbb{E}_{\pi}\left[\| \theta\|\right])h^{1/2}\) since \(\ell=\lceil(1/2)\log_{\rho}h\rceil\). Lemma 2 shows that \((II)^{\prime}\leq\frac{3\sqrt{2}}{2}\frac{L}{M}dh^{1/2}\). Denote \(C_{2}=\max_{0\leq i\leq n}\|\theta_{i}\|+\mathbb{E}_{\pi}\left[\|\theta\| \right]+\frac{3\sqrt{2}}{2}\frac{L}{M}d\). So \((I)+(II)^{\prime}\leq C_{2}h^{1/2}\).

By Theorem A.1 and the condition that \(\gcd(d\ell,n)=1\), the star-discrepancy \(D^{*}(\{\bar{w}_{k}^{(\ell)}\}_{k\geq 1})\) is upper bounded by \(O(n^{-1}(\log n)^{d\ell+1}\log\log n)\). Finally, by Koksma-Hlawka inequality, we have \((II)^{\prime\prime}\leq\|\bar{f}_{\ell}\|_{\text{HK}}\cdot D^{*}(\{\bar{w}_{k }^{(\ell)}\}_{k\geq 1})\). Thus, \((II)^{\prime\prime}+\frac{4\ell}{n}\|f\|_{\infty}\leq C_{1}n^{-1+\delta}\), where \(\delta\) hides the poly-logarithmic terms in \(\log n\) and \(C_{1}\) depends on \(d,\ell,\|\bar{f}_{\ell}\|_{\text{HK}}\).

Therefore, the upper bound becomes

\[(I)+(II)^{\prime}+(II)^{\prime\prime}+\frac{4\ell}{n}\|f\|_{\infty}\leq C_{1} n^{-1+\delta}+C_{2}h^{1/2}.\]

## Appendix B Additional numerical results

The primary contribution of this work is to improve LMC as a Monte Carlo sampling algorithm, not as an optimization algorithm. Therefore, our main focus is on providing a better estimation of \(\pi(f)\) for some function of interest. Downstream tasks relying on such expectations can also benefit from LQMC. For posterior prediction, it is essential to recognize that the prediction error is not solely determined by the sampling method. Even with infinite perfect samples from the posterior, the prediction error can still arise due to model misspecification, noisy data, biased sampling, etc. So the improvement achieved by LQMC might be less pronounced when assessing the prediction error.

To investigate the performance of LQMC in a posterior prediction setting, we conducted experiments similar to those presented in Dubey et al. (2016) using three UCI datasets. Each dataset was split into a training set (70%), a validation set (10%), and a test set (20%). We performed a tuning process for the constant step size on a grid using the validation set and evaluated the prediction error on the test set. Each iteration computes the stochastic gradient using 32 data points sampled at random. Details of the datasets are in Table 1.

The results are presented in Figure 6. The \(x\)-axes represent the total number of iterations of Langevin algorithm and the \(y\)-axes represent the test error. The error bars represent the variation across 10 random replicates. It is evident that LQMC reduced the test error, although the improvement is not substantial. This aligns with our initial expectation, as the proposed method primarily enhances the accuracy of estimating the posterior mean. However, the test error often consists of other sources of error, thus the improvement achieved by the proposed method in reducing the test error might be limited.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Datasets & Parkinsons & Bike & Protein \\ \hline \(N\) (number of instances) & 5875 & 17379 & 45730 \\ \(p\) (number of features) & 21 & 12 & 9 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of datasets used for Bayesian posterior prediction.

Figure 6: Test error versus number of iterations for the three UCI datasets.