# QuIP: 2-Bit Quantization of

Large Language Models With Guarantees

 Jerry Chee

Cornell University

jerrychee@cs.cornell.edu

&Yaohui Cai

Cornell University

yc2632@cornell.edu

&Volodymyr Kuleshov

Cornell University

kuleshov@cornell.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

###### Abstract

This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from _incoherent_ weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/Cornell-RelaxML/QuIP.

## 1 Introduction

Large language models (LLMs) have enabled advances in text generation, few-shot learning, reasoning, protein sequence modeling, and other tasks [2, 30, 35]. The massive size of these models--often reaching into hundreds of billions of parameters--requires sophisticated deployment methods and motivates research into efficient inference algorithms.

This work studies the post-training quantization of LLM parameters as a way to improve their runtime efficiency [4, 8, 22, 31, 33, 34]. Our key insight is that quantization can be most effective when weight and proxy Hessian matrices are _incoherent_--that the weights themselves are even in magnitude, and the directions in which it is important to have good rounding accuracy are not too large in any one coordinate. Intuitively, incoherence can be thought of as a principled form of outlier reduction, which makes it easier to adaptively round the weights to a finite set of compressed values. We use this intuition to develop theoretically sound two-bit quantization algorithms that scale to LLM-sized models.

Specifically, we introduce quantization with incoherence processing (QuIP), a new method motivated by the above insight. QuIP consists of two steps: (1) an adaptive rounding [20] procedure, which minimizes a quadratic proxy objective \(\ell(\hat{W})=\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})\) of the error between the original weights \(W\) and the quantized weights \(\hat{W}\) using an estimate of the Hessian \(H\); (2) efficient pre- and post- processing that ensures that the weight and Hessian matrices are incoherent by multiplyingthem by a Kronecker product of random orthogonal matrices. We denote "incoherence processing" as both the pre- and post- processing steps of our procedure. Incoherence processing can be viewed as a form of outlier suppression across the weights and the activation space.

We complement our method with a theoretical analysis--the first for a quantization algorithm that scales to LLM-sized models--which analyzes the role of incoherence and shows that our quantization procedure is optimal within a general class of rounding methods. Interestingly, we find that QuIP without incoherence processing yields a more efficient implementation of an earlier algorithm, OPTQ [8]; our paper thus also provides the first theoretical analysis for that method.

Empirically, we find that incoherence processing greatly improves the quantization of large models, especially at higher compression rates, and yields the first LLM quantization method that produces viable results using only two bits per weight. For large LLM sizes (>2B parameters), we observe small gaps between 2-bit and 4-bit compression that further decrease with model size, hinting at the feasibility of accurate 2-bit inference in LLMs.

Contributions.In summary, this paper makes the following contributions: (1) we propose QuIP, a quantization method based on the insight that model parameters should ideally be incoherent; (2) we provide a theoretical analysis for a broad class of adaptive rounding methods that encompass QuIP and OPTQ; (3) we demonstrate that QuIP makes two-bit LLM compression viable for the first time.

## 2 Related Work

Adaptive rounding.Nagel et al. [20] are the first to motivate the "adaptive rounding" proxy objective (Eq. (1)) in a principled way. There are many quantization methods which quantize by optimizing this proxy objective [5; 6; 9; 12; 14; 20; 32]. Many require further retraining which can be expensive, and are not evaluated on the current largest open LLMs (OPT [35], BLOOM [30]). Lybrand and Saab [15] propose a greedy per-neuron quantization procedure that is similar to ours, except they do not consider arbitrary linear functions of the error correction. Their work bounds the proxy objective, albeit on the first layer only.

Post training quantization in large models.There is a growing body of work on PTQ in LLMs such as OPT and BLOOM. The size of these models make it difficult to apply previously developed methods. The majority of these methods make quantization easier by somehow reducing the range of weights or activations, but still use nearest rounding. SmoothQuant [31] rescales between activations and weights to remove outliers from the activations and make quantization overall easier. ZeroQuant [33] proposes a per-layer knowledge distillation method. LLM.int8(4) decompose matrix multiplications into a majority of 8 bit and a minority of 16 bit operations. LUT-GEMM [22] designs kernels to accelerate quantized matrix multiplications. RPTQ [34] reorders activations and quantizes them in groups, reducing the impact of range differences between channels.

OPTQ (Formerly known as GPTQ). OPTQ [8] is based on OBQ [7], and proposes a novel rounding method that can work on the largest OPT and BLOOM models. The method works iteratively over the weight columns in a fixed order: (1) quantize with nearest rounding and compute the error, (2) update the remaining weights with a scaled error, and (3) repeat.

Other quantization methods.There are other quantization procedures which do not round based on the proxy objective of [20], or are not designed for the largest language models [10; 11; 13; 19; 28; 29].

## 3 Quantization With Incoherence Processing: Adaptive Rounding Step

This section introduces quantization with incoherence processing (QuIP), a new method consisting of: (1) an adaptive rounding step; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence. We define and analyze step (1) in this section; the next section focuses on step (2).

Following existing state-of-the-art post-training quantization methods, we round weights per-layer by minimizing the "adaptive rounding" proxy objective, as in Nagel et al. [20],

\[\ell(\hat{W})=\mathbf{E}_{x}\left[\left\|(\hat{W}-W)x\right\|^{2}\right]= \operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right).\] (1)

Here, \(W\in\mathbb{R}^{m\times n}\) is the original weight matrix for a given linear layer, \(\hat{W}\in\mathbb{R}^{m\times n}\) are the quantized weights, \(x\in\mathbb{R}^{n}\) is an input vector drawn uniformly at random from a calibration set, and \(H\) is the second moment matrix of these vectors, interpreted as a proxy Hessian. Crucially, this formulation lets the quantization be run in parallel across neurons, which is tractable for large language models [8]. For simplicity, we will focus in this section on rounding to the integers; subsequent sections will extend the analysis to finite grids.

### LDLQ: An Optimal Adaptive Rounding Method

Our strategy is to define a family of adaptive rounding methods for optimizing objective (1) and then define LDLQ, the optimal method within that class. Our defined methods iteratively perform the following update for \(k=1,2,...,n\):

\[\hat{W}_{k}=\mathcal{Q}(W_{k}+(W_{1:(k-1)}-\hat{W}_{1:(k-1)})a_{k}),\]

where \(W_{k}\) denotes the \(k\)-th column, \(W_{1:(k-1)}\) denotes the first \(k-1\) columns, the subroutine \(\mathcal{Q}\) denotes either nearest rounding or standard unbiased rounding to the integers (which rounds up or down such that \(\mathbf{E}\left[\mathcal{Q}(z)\right]=z\)), and \(a_{k}\in\mathbb{R}^{k-1}\) is some sequence of vectors. This scheme rounds columns one at a time; at each step, it adds a "correction" term that is a linear function of the residual from the rounding we have done so far. The final \(\hat{W}\) satisfies the following matrix equation:

\[\hat{W}=\mathcal{Q}(W+(W-\hat{W})U),\] (2)

where \(U\) is a strictly upper-triangular matrix whose columns are the vectors \(a_{k}\) and \(\mathcal{Q}\) acts elementwise. Because \(U\) is upper-triangular, \(\hat{W}_{k}\) only depends on \(\hat{W}_{1:(k-1)}\).

If we let \(\eta=\mathcal{Q}(W+(W-\hat{W})U)-(W+(W-\hat{W})U)\) denote the quantization error of \(\mathcal{Q}\), we find that \(\hat{W}-W=\eta(U+I)^{-1}\) and we can rewrite objective (1) as

\[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})=\operatorname{tr}(\eta(U+I)^{- 1}H(U+I)^{-T}\eta^{T}).\] (3)

The LDLQ MethodHow should we specify \(U\), the linear feedback from the quantization error of preceding columns in (2)? Equation 3 provides an answer. If we choose \(U\leftarrow\hat{U}\) such that the LDL decomposition of \(H\) is

\[H=(\hat{U}+I)D(\hat{U}+I)^{T},\] (4)

where \(D\) is a (non-negative) diagonal matrix and \(\hat{U}\) is upper unit triangular, then the terms \((U+I)\) in Eq. (3) cancel. We denote as LDLQ the rounding procedure in Eq. (2) with \(U\leftarrow\hat{U}\) as the LDL assignment from Eq. (4). We will now see that the LDL assignment of \(U\) is in fact optimal.

### Deriving the Optimality of the LDLQ Adaptive Rounding Procedure

In order to reason about optimality, we consider weights which are worst and average-case for the proxy loss. Let \(\mathcal{A}\) denote a rounding method, and let \(\mathcal{A}(W,H)\) be the resulting quantized weights. Define the _worst-case_ (\(\mathcal{L}_{\mathrm{worst}}\)) and _average_ (\(\mathcal{L}_{\mathrm{avg}}\)) proxy losses with respect to the input weights as

\[\mathcal{L}_{\mathrm{worst}}(\mathcal{A},H) =\sup_{W\in\mathbb{R}^{m\times n}}\mathbf{E}\left[\operatorname{ tr}\left((\mathcal{A}(W,H)-W)H(\mathcal{A}(W,H)-W)^{T}\right)\right]\] (5) \[\mathcal{L}_{\mathrm{avg}}(\mathcal{A},H) =\mathbf{E}_{W\sim\mathrm{Unif}[0,1]^{m\times n}}\left[ \operatorname{tr}\left((\mathcal{A}(W,H)-W)H(\mathcal{A}(W,H)-W)^{T}\right) \right].\] (6)

**Theorem 1**.: LDLQ _is worst and average-case optimal amongst rounding methods which specify the linear feedback \(U\) as a function of \(H\) (not of \(W\)), and when rounding to the integers. That is, for all rounding methods \(\mathcal{A}\) in the class described by Eq. (2), for all positive semi-definite \(H\), and for \(\mathcal{Q}\) as either nearest or stochastic rounding,_

\[\tfrac{m}{4}\operatorname{tr}(D)=\mathcal{L}_{\mathrm{worst}}(\mathsf{LDLQ},H) \leq\mathcal{L}_{\mathrm{worst}}(\mathcal{A},H)\ \text{ and }\ \tfrac{m}{c}\operatorname{tr}(D)=\mathcal{L}_{\mathrm{avg}}(\mathsf{LDLQ},H) \leq\mathcal{L}_{\mathrm{avg}}(\mathcal{A},H),\]

_where \(D\) is the matrix from the LDL decomposition of \(H\), and \(c=12\) for nearest, \(c=6\) for stochastic._

**Remarks.** The number of rows being quantized is \(m\), and each quantization method operates across the \(n\) entries of each row. For all rounding methods described by Eq. (2), and for all positive semi-definite \(H\), \(\mathcal{Q}\) as nearest rounding achieves the same worst-case proxy loss as stochastic rounding, but achieves better average proxy loss.

Moving beyond a generic algorithm \(\mathcal{A}\) within our framework, we consider the common baselines of nearest and stochastic rounding. These methods are represented within our framework by choosing the appropriate \(\mathcal{Q}\) subroutine, and setting all entries of the linear feedback to zero.

For these baseline methods, their optimality gap to LDLQ is governed by \(\operatorname{tr}\left(D\right)\) vs. \(\operatorname{tr}\left(H\right)\). For any non-diagonal \(\tilde{H}\succeq 0\), LDLQ achieves strictly lower worst and average-case proxy loss because \(\operatorname{tr}\left(D\right)<\operatorname{tr}(\tilde{H})\). Let \(\mathcal{B}=\{\mathsf{Near},\mathsf{Stoch}\}\). Then, \(\mathcal{L}_{\operatorname{worst}}(\mathsf{LDLQ},\tilde{H})<\mathcal{L}_{ \operatorname{worst}}(\mathsf{Stoch},\tilde{H})\) and \(\mathcal{L}_{\operatorname{avg}}(\mathsf{LDLQ},\tilde{H})<\mathcal{L}_{ \operatorname{avg}}(\mathcal{B},\tilde{H})\). Across OPT models 125m to 2.7b, \(\operatorname{tr}\left(D\right)/\operatorname{tr}\left(H\right)\leq 0.65\)--empirically verifying that the gap is not insignificant. See Supplement C for full details.

### Incoherence: Optimality with a Spectral Bound

Theorem 1 gives exact expressions for the proxy loss, albeit with \(\operatorname{tr}\left(D\right)\), which can be difficult to reason about. In Figure 2, we empirically observe that \(H\) is approximately low-rank: we visualize the spectrum of several randomly chosen \(H\) from OPT-2.7b, and observe that the spectrum decays rapidly. In fact, across all layers of OPT-125m to 2.7b models, a vast majority of \(H\) matrices have fewer than a quarter of eigenvalues \(>1\%\) of the max eigenvalue; see Supplement C for full details. Given this observation about the low rank of \(H\), can we bound the behavior of LDLQ, and thus \(\operatorname{tr}\left(D\right)\), using the spectrum of \(H\)?

We do this building on a variant of the incoherence assumption that is specialized to our case [3, 24].

**Definition 1**.: _We say a symmetric Hessian matrix \(H\in\mathbb{R}^{n\times n}\) is \(\mu\)-incoherent if it has an eigender-composition \(H=Q\Lambda Q^{T}\) such that for all \(i\) and \(j\), \(|Q_{ij}|=\left|e_{i}^{T}Qe_{j}\right|\leq\mu/\sqrt{n}\). By extension, we say a weight matrix \(W\in\mathbb{R}^{m\times n}\) is \(\mu\)-incoherent if all \(i\) and \(j\), \(|W_{ij}|=\left|e_{i}^{T}We_{j}\right|\leq\mu\left\|W\right\|_{F}/\sqrt{mn}\)._

Note that "most" \(n\times n\) matrices are incoherent with \(\mu=\mathcal{O}(\sqrt{\log n})=\tilde{\mathcal{O}}(1)\) because a random orthogonal matrix has entries with squared-magnitudes that concentrate around their mean of \(1/n\). Incoherence in \(W\) can be viewed as a form of outlier reduction: a small bound on the magnitude of its entries means that we do not need to scale it as much to make it fit in the finite range of representable low-precision numbers. Figures 2 and 3 plot the max absolute weight and hessian eigenvector entries before and after our incoherence processing, on all layers in OPT-2.7b. A line with slope=1 is drawn for reference. We see that \(W\) and \(H\) are more incoherent after our incoherence processing is applied. Making \(H\) incoherent is less intuitive, but its utility is motivated by the following lemma.

**Lemma 2**.: _Let \(H\in\mathbb{R}^{n\times n}\) be a \(\mu\)-incoherent positive semi-definite symmetric matrix and let \(H=(\dot{U}+I)D(\dot{U}+I)^{T}\) be its LDL Cholesky decomposition, where \(\dot{U}\) is a strictly upper triangular matrix and \(D\) is a (non-negative) diagonal matrix. Then,_

\[\operatorname{tr}\left(D\right)\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H ^{1/2}\right)^{2}.\]

To the best of our knowledge, this is a novel result using incoherence to obtain a bound on \(\operatorname{tr}\left(D\right)\) that depends only on the spectrum of \(H\). To help interpret this result, we derive explicit proxy losses for plain nearest and stochastic rounding, which we will then compare to what LDLQ gets via Lemma 2.

```
0:\(b\in\mathbb{N}\), \(H\in\mathbb{R}^{n\times n}\) SPD, original \(W\in\mathbb{R}^{m\times n}\), \(\rho\in\mathbb{R}_{+}\), \(\alpha\in[0,1]\)
1:seeded sample random two-factor orthogonal matrices \(U\in\mathbb{R}^{m\times m}\) and \(V\in\mathbb{R}^{n\times n}\)
2:\(H=H+\alpha*\mathrm{mean}(\mathrm{diag}(H))I\)\(\triangleright\) from OPTQ
3:\(\tilde{D}\leftarrow\sqrt[4]{\mathrm{diag}(H)/\mathrm{diag}(W^{T}W)}\)\(\triangleright\sqrt[4]{-}\) applies element-wise
4:\(W\gets W\tilde{D}\); \(H\leftarrow\tilde{D}^{-1}H\tilde{D}^{-1}\)\(\triangleright\) diagonal rescaling
5:\(W\gets UWV^{T}\); \(H\gets VHV^{T}\)\(\triangleright\) incoherence
6:\(s\leftarrow\rho\|W\|_{F}/\sqrt{mn};\ W\leftarrow\frac{1}{2}(\frac{1}{s}W+1)\)\(\triangleright\) reduced quantization range due to incoherency
7:\(W\leftarrow\mathrm{clamp}(W*(2^{b}-1),0,2^{b}-1)\)\(\triangleright\) rescale \(W\) to lie within \([0,2^{b}-1]\)
8:return\(W,H,s,\tilde{D}\) ```

**Algorithm 2** QuIP - Incoherence Post-Processing

**Lemma 3**.: _Let \(H\) be symmetric positive definite. In the worst case stochastic rounding achieves \(\mathcal{L}_{\mathrm{worst}}(\mathsf{Stoch},H)=(m/4)\operatorname{tr}\left(H\right)\). In the average case nearest and stochastic rounding achieve \(\mathcal{L}_{\mathrm{avg}}(\{\mathsf{Near},\mathsf{Stoch}\},H)=(m/c) \operatorname{tr}\left(H\right)\), where \(c=12\) for nearest, and \(c=6\) for stochastic._

To interpret this result, consider \(H\) rank-\(k\) with \(\mu^{2}k<n\). By Cauchy-Schwarz, \(\operatorname{tr}(H^{1/2})^{2}\leq k\operatorname{tr}\left(H\right)\). Combining Lemma 2 with the LDLQ proxy losses of Theorem 1 and comparing with Lemma 3,

\[\mathcal{L}_{\mathrm{worst}}(\mathsf{LDLQ},H) \leq\frac{m\mu^{2}}{4n}\operatorname{tr}\left(H^{1/2}\right)^{2} \leq\frac{m\mu^{2}k}{4n}\operatorname{tr}\left(H\right)\leq\frac{m}{4} \operatorname{tr}\left(H\right)=\mathcal{L}_{\mathrm{worst}}(\mathsf{Stoch},H)\] \[\mathcal{L}_{\mathrm{avg}}(\mathsf{LDLQ},H) \leq\frac{m\mu^{2}}{cn}\operatorname{tr}\left(H^{1/2}\right)^{2} \leq\frac{m\mu^{2}k}{cn}\operatorname{tr}\left(H\right)\leq\frac{m}{c} \operatorname{tr}\left(H\right)=\mathcal{L}_{\mathrm{avg}}(\mathcal{B},H),\]

where \(\mathcal{B}\in\{\mathsf{Near},\mathsf{Stoch}\}\), and \(c\) is as given in Theorem 1. This shows that for sufficiently low-rank \(H\), LDLQ is asymptotically better than plain nearest and stochastic rounding by a factor of \(\mu^{2}k/n\).

Without incoherence: no improvement with a spectral bound.By assuming incoherence, we were able to show LDLQ gets an asymptotically better bound in terms of just the spectrum of \(H\). We might ask: _was the incoherence assumption necessary to get this result?_ The following theorem answers this question in the affirmative by showing that without incoherence, the best spectral bound for LDLQ cannot differentiate it from the nearest and stochastic rounding baselines.

**Theorem 4**.: _Consider all \(\tilde{H}\) with the same spectrum as \(H\). For any positive semi-definite \(H\), the following holds. On the worst-case loss \(\mathsf{LDLQ}\) achieves the same error as stochastic rounding,_

\[\sup_{\tilde{H}\,s.t.\ \mathrm{eig}(\tilde{H})=\mathrm{eig}(H)}\mathcal{L}_{ \mathrm{worst}}(\mathsf{LDLQ},\tilde{H})=\mathcal{L}_{\mathrm{worst}}(\mathsf{ Stoch},H)=\frac{m}{4}\operatorname{tr}\left(H\right).\]

_On the average-case loss \(\mathsf{LDLQ}\) achieves the same error as the corresponding rounding routine. Let \(\mathcal{B}=\{\mathsf{Near},\mathsf{Stoch}\}\) and \(c=12\) for nearest, \(c=6\) for stochastic._

\[\sup_{\tilde{H}\,s.t.\ \mathrm{eig}(\tilde{H})=\mathrm{eig}(H)}\mathcal{L}_{ \mathrm{avg}}(\mathsf{LDLQ}^{*},\tilde{H})=\mathcal{L}_{\mathrm{avg}}( \mathcal{B},H)=\frac{m}{c}\operatorname{tr}\left(H\right).\]

Note that the worst case for comparing LDLQ against these baselines occurs when \(H\) is diagonal, see Theorem 1 and Lemma 3. Assuming incoherence as we do is a natural way to exclude such cases.

## 4 Quantization With Incoherence Processing: Incoherence Processing Step

Next, we leverage the above incoherence analysis to introduce _incoherence processing_, the second step of the QuIP algorithm. Our strategy will be to pre-process weight and Hessian matrices to ensure the favorable incoherence properties outlined above. One straightforward way to make a symmetric matrix incoherent is to conjugate it by a uniform random orthogonal matrix: this will result in each of its eigenvectors being a random unit vector, whose entries will concentrate around magnitude \(n^{-1/2}\).

Specifically, let \(U\in\mathbb{R}^{m\times m}\) and \(V\in\mathbb{R}^{n\times n}\) be two random orthogonal matrices. (Let's temporarily ignore how these matrices are generated, or how we would efficiently perform inference.) We ensure the weight and Hessian are incoherent with high probability through random orthogonal multiplications \(\tilde{H}\gets VHV^{T}\) and \(\tilde{W}\gets UWV^{T}\). Importantly, this transformation preserves the proxy quadratic form since \(\operatorname{tr}(\tilde{W}H\tilde{W}^{T})=\operatorname{tr}((UWV^{T})(VHV^{T} )(VW^{T}U^{T}))=\operatorname{tr}(WHW^{T})\).

### Incoherence via Efficient Orthogonal Multiplication

If all we wanted to do was to store or transmit the weights of the quantized neural network, the above procedure would introduce no overhead, since we can generate a random orthogonal matrix from a seed--making it essentially free to store. However, for running _inference_ on a DNN, we need to multiply by the weight matrix \(W\), and here the need to manifest and multiply by \(n\times n\) random orthogonal matrices \(U,V\) would be prohibitive.

To handle this, we propose to instead use a distribution over random orthogonal matrices for which multiplication is fast. Let \(n=pq\) be a factorization of \(n\) (where \(p\approx q\approx\sqrt{n}\)), and set \(U=U_{L}\otimes U_{R}\) where \(U_{L}\) is sampled uniformly from the \(p\times p\) orthogonal matrices and \(U_{R}\) is sampled uniformly from the \(q\times q\) orthogonal matrices. Multiplication of a vector \(x\in\mathbb{R}^{n}\) by the matrix \(U\) can be accomplished by reshaping to a \(p\times q\) matrix, multiplying on the left by \(U_{L}\) and the right by \(U_{R}^{T}\), and then reshaping back: this takes \(O(n(p+q))=o(n^{2})\) operations. Using more than two factors in this way is also possible, but using two suffices to make this preprocessing asymptotically non-dominant.

**Lemma 5**.: _Let \(H\) be a positive semi-definite matrix on \(\mathbb{R}^{n\times n}\) and \(W\) a matrix on \(\mathbb{R}^{m\times n}\), and suppose that \(m=p_{1}\cdot p_{2}\cdots p_{k}\) and \(n=q_{1}\cdot q_{2}\cdots q_{k}\). Let \(U_{1},U_{2},\ldots,U_{k},V_{1},V_{2},\ldots,V_{k}\) be independent random orthogonal matrices on \(\mathbb{R}^{p_{i}\times p_{i}}\) and \(\mathbb{R}^{q_{i}\times q_{i}}\) respectively. Set \(U\) as the Kronecker product \(U=U_{1}\otimes U_{2}\otimes\cdots\otimes U_{k}\) and \(V\) as \(V=V_{1}\otimes V_{2}\otimes\cdots\otimes V_{k}\) Then \(VHV^{T}\) is \(\mu_{H}\)-incoherent with probability at least \(1-\delta\), and \(UWV^{T}\) is \(\mu_{W}\)-incoherent with probability at least \(1-\delta\), where_

\[\mu_{H}=A^{k/2}\log\left(\frac{Ckn^{2}}{\delta}\right)^{k/2}=\tilde{\mathcal{ O}}\left(1\right)\ \ \text{and}\ \ \mu_{W}=A^{k}\log\left(\frac{2Ckmn}{\delta}\right)^{k}=\tilde{\mathcal{O}} \left(1\right)\]

_for some global constants \(A\) and \(C\) independent of \(n\) and \(k\)._

**Remarks.** This lemma means that multiplying by a random matrix in this family suffices to make a matrix incoherent with parameter \(\mu\) only poly-logarithmic in the matrix size. In our experiments we use \(k=2\) factors to construct the orthogonal matrices \(U,V\).

### Additional Heuristics

We outline QuIP pre-processing and post-processing in Algorithms 1 and 2, respectively. In line 5 of Algorithm 1, we apply the aforementioned fast orthogonal multiplication procedure to ensure \(W\) and \(H\) are incoherent. We also randomly permute entries at the fast matrix multiplication step to prevent any correlation between attention heads from worsening performance. We introduce a number of additional heuristic improvements that further improve performance.

**Incoherence-Based Heuristics.** Line 4 diagonally rescales \(W\) and \(H\) to minimize \(\ell(\hat{W})\approx\operatorname{tr}\left(H\right)\|W\|_{F}^{2}\), effectively trading off the spectrum of these matrices to find a minimum. Motivated by the incoherence of \(W\), Line 6 computes the quantization range depending on the spectrum \(\|W\|_{F}\), instead of the typical \(\max_{i,j}|W_{ij}|\). Our full QuIP procedure is described in Algorithm 3, which contains calls to the pre- and post-processing sub-steps in Algorithms 1 and 2.

**Greedy local search.** Our basic procedure yields a good initial guess with error guarantees. We can further lower the proxy loss by running coordinate descent after LDLQ (but before post-processing), updating the weights in the same order as in the initial pass. See Supplement B for full details.

```
0:\(b\in\mathbb{N}\), \(H\in\mathbb{R}^{n\times n}\) SPD, \(W\in\mathbb{R}^{m\times n}\), \(\mathcal{Q}\in\{\textsf{Near},\textsf{Stoch}\}\), \(\rho\in\mathbb{R}_{+}\), \(\alpha\in[0,1]\)
1:\(\hat{W},H,s,\tilde{D}\leftarrow\text{Alg }1(b,H,W,\rho,\alpha)\)\(\triangleright\) QuIP Incoherence Pre-Procesing
2:\(H=(\hat{U}+I)D(\hat{U}+I)^{-1}\)\(\triangleright\) LDL decomposition
3:for\(k\in\{1,\dots,n\}\)do\(\hat{W}_{k}\leftarrow\operatorname{clamp}(\mathcal{Q}(W_{k}+(W-\hat{W}) \hat{U}_{k}),0,2^{b}-1)\)\(\triangleright\) LDLQ
4:return\(\hat{W}\leftarrow\text{Alg }2(b,H,\hat{W},s,\tilde{D})\)\(\triangleright\) QuIP Incoherence Post-Processing ```

**Algorithm 3** QuIP: Quantization with Incoherence Processing

## 5 Extensions and Further Analyses

### OPTQ is a Special Case of LDLQ

We prove a novel theoretical insight: QuIP without incoherence processing (i.e., LDLQ) is equivalent to a more efficient version of the OPTQ algorithm. That is, OPTQ falls under our class of adaptive rounding procedures with linear feedback, and is within-class optimal.

**Theorem 6**.: _OTPQ [8] falls within the class of adaptive rounding procedures with linear feedback as described by Eq. (2), and is equivalent to LDLQ in Section 3._

**Remarks.** To the best of our knowledge, this equivalence yields the first theoretical analysis of OPTQ. Even though the two methods are equivalent, LDLQ is more efficient. OPTQ's implementation requires a matrix inversion of \(H\), and two Cholesky decompositions. Our implementation of LDLQ performs no matrix inversion, and only one Cholesky decomposition.

**Empirical Verification.** The quantized outputs of the OPTQ implementation [8] are shown to be exactly identical to the outputs of our LDLQ implementation. Synthetic random data was used, with \(W\sim\operatorname{Unif}[0,1]^{1000\times 1000}\). Full details can be found in Supplement C.

### A Bound for Rounding to a Finite Grid

In Section 3, we saw that LDLQ (equivalently, OPTQ) is optimal for minimizing the adaptive rounding objective. However, this analysis assumed rounding to the integers. In practice, we do not want to round \(W\) just to the integers, but instead to scale it, shift it, and round it a finite subset corresponding to a \(b\)-bit integer. To do this, the "real" LDLQ algorithm uses a clamp operation to restrict the range of quantized values. Is LDLQ still optimal when this small change is made? It turns out that the answer is _no_, as the following concrete example illustrates.

**Finite Grid Counterexample.** Figure 4 illustrates the behavior of LDLQ and other rounding methods--when restricted via clamping to a finite 4-bit grid \([0,15]\)--on a particular example where \(H\) is a (cleverly chosen) small perturbation of \((I_{n}+\mathbf{1}_{n\times n}-e_{n}e_{n}^{T})/n\), and \(W\) has \(m=16\) and is a small perturbation of \(\mathbf{1}_{m\times n}/2\). Details of the setup appear in Supplement C. The figure shows that clamped LDLQ with nearest rounding is asymptotically worse, and the clamping to the finite grid is what causes it to be worse in this case.

Note that in our experiments in practice, OPTQ has been shown to soundly beat nearest rounding. This clamping issue does not seem to arise in practice; however, since it is _possible_ we do need to take it into account to prove useful end-to-end bounds.

**A Procedure With a Bound.** In order to address the above issues in theory, here we describe a method that acts to restrict the value of \(|\hat{W}_{ij}-W_{ij}|\), so that the rounded weights will remain inside the grid if \(W\) is sufficiently far inside. We do this via the optimization problem with hyperparameter \(c\)

minimize: \[\text{tr}\left(HR^{T}R\right)\] over: \[R\] unit upper triangular (7) subject to: \[e_{i}^{T}R^{T}Re_{i}\leq 1+c,\ \forall i\in\{1,\dots,n\}.\]

Figure 4: LDLQ underperforms.

Our "fixed" algorithm solves this convex problem (e.g. with ADMM), then runs QuIP using stochastic rounding and \(U=R^{-1}-I\) in place of the LDL decomposition. Observe that for sufficiently large \(c\), this is exactly equivalent to base QuIP, since the solution of that optimization problem is given by the LDL decomposition when the constraint is dropped. Doing this (the full algorithm is given in the supplemental) yields the following theorem.

**Theorem 7**.: _Suppose that we run Algorithm 5 (Supplement) to quantize a matrix \(W\in\mathbb{R}^{m\times n}\) by solving the objective (7). Then there exists an assignment of the algorithm's hyperparameters \(c\) and \(\rho\) such that with probability at least \(1-\delta\), all the quantized weights will be in range (no overflow or need for clipping) and_

\[\operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right)=\tilde{\mathcal{O}} \left(\frac{1}{n^{2}4^{b}}\operatorname{tr}\left(H^{1/2}\right)^{2}\|W\|_{F}^{ 2}\right).\]

In practice, because clamping rarely causes issues, and because of the significant additional compute needed to solve this program, we always just use QuIP as described in the previous sections, which is equivalent to setting \(c\) large and using nearest rounding.

## 6 Experiments

**Overview.** We quantize the OPT [35] family of models (up to 66B parameters) and Llama 2 70B [27] using various quantization and processing methods. QuIP is superior to OPTQ and other baselines across all model sizes and evaluation tasks. Most interestingly, incoherence processing yields excellent performance using as little as two bits per weight when paired with any of the quantization methods we consider (including nearest rounding). Two-bit quantization with QuIP is viable at even moderate model sizes (1B parameters), a regime where other two-bit quantization methods fail. At the largest model sizes, the difference between 2-bit and 16-bit weight performance becomes small. We compare the throughput of QuIP with OPTQ's efficient implementation on language generation and show that it is not much slower. Additional results on the effectiveness of the proxy loss, unbiased rounding, and Algorithm 5 are presented in the Supplement C.

**Setup.** The experimental infrastructure is built on top of OPTQ's [8] repository which is implemented in PyTorch [23]. We quantize the HuggingFace implementations of the OPT and Llama 2 model families. All models are quantized on a single GPU, with up to 48GB of memory. Our calibration set is the same as OPTQ; 128 random 2048 token segments from the C4 dataset [25] consisting of generic text data from crawled websites. Therefore, no task-specific data is viewed when quantizing. Following OPTQ, quantization is performed one Transformer block at a time: loaded into GPU memory, the Hessian computed, and then the weights quantized. The current block's inputs are then passed through the quantized block to produce inputs for the following block. The Hessian is computed from the quantized Transformer up to that point rather than from the full precision model; like OPTQ, we find this improves quantization. Further details on the setup can be found in Supplement C, including a description of the computational resources used to perform the experiments.

**Methods.** We evaluate compositions of several quantization and pre/post processing methods. For quantization methods, we evaluate nearest rounding, LDLQ (or OPTQ), and two variations. LDLQ-RG re-orders the weights based on \(\operatorname{diag}(H)\) to modify the quantization order and adds further greedy updates to the proxy. "Greedy" performs the greedy updates only. We evaluate the baseline preprocessing from OPTQ which adds \(H\gets H+\alpha*\operatorname{mean}(\operatorname{diag}(H))I\) for numerical stability. We also evaluate our incoherence processing in Algorithms 1 and 2, denoted as "IncP". With this notation QuIP = LDLQ + IncP, and QuIP-RG = LDLQ-RG + IncP.

**Datasets.** We evaluate on the following language generation tasks: WikiText2 [17], Penn Treebank (PTB) [16], and C4. We also evaluate on zero-shot tasks, including LAMBADA (LAMB) [21], ARC Easy (ArcE) [1], PiQA [26], and StoryCloze (SC) [18]. See Supplement C for the full set of results.

**Main Results.** QuIP is the first PTQ procedure to achieve good quantization at two bits per weight, across a variety of LLM sizes and evaluation tasks. In Figure 5 we compare QuIP and OPTQ when quantizing to 2 and 3 bits per weight (4-bit quantization works equally well for both methods); we evaluate OPT models (up to 66B) on PTB, C4, ARC Easy, and LAMBADA. QuIP is superior to OPTQ across the model sizes and evaluation tasks. At three bits, QuIP matches the full precision model reasonably well. At two bits and for larger LLMs (>2B parameters), QuIP begins to approach the performance of the full precision model. As model size increases, so does the quality of QuIP's

\begin{table}
\begin{tabular}{c|c c c c c|c c c c c}  & \multicolumn{6}{c|}{OPTQ} & \multicolumn{6}{c}{QuIP (Ours)} \\ \hline WBits & Wiki\(\downarrow\) & C4\(\downarrow\) & ArcE\(\uparrow\) & PiQA\(\uparrow\) & SC\(\uparrow\) & Wiki\(\downarrow\) & C4\(\downarrow\) & ArcE\(\uparrow\) & PiQA\(\uparrow\) & SC\(\uparrow\) \\ \hline
16 & 3.319 & 5.709 & 59.72 & 80.90 & 79.95 & 3.319 & 5.709 & 59.72 & 80.90 & 79.95 \\ \hline
4 & 3.596 & 5.905 & 58.96 & 80.52 & 79.12 & 3.531 & 5.869 & 59.81 & 80.47 & 79.63 \\
3 & 4.907 & 7.099 & 54.38 & 78.56 & 77.72 & 3.853 & 6.135 & 59.81 & 80.25 & 79.31 \\
2 & 123.908 & 70.541 & 25.34 & 50.54 & 51.75 & **6.326** & **8.937** & **54.38** & **75.08** & **75.37** \\ \hline \end{tabular}
\end{table}
Table 1: Quantizing Llama 2 70B with QuIP and OPTQ, and evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits.

\begin{table}
\begin{tabular}{c|c c c c|c c c c c}  & \multicolumn{6}{c|}{Baseline Processing} & \multicolumn{6}{c}{Incoherence Processing (Ours)} \\ \hline WBits & Wiki\(\downarrow\) & PTB\(\downarrow\) & C4\(\downarrow\) & ArcE\(\uparrow\) & LAMB\(\uparrow\) & Wiki\(\downarrow\) & PTB\(\downarrow\) & C4\(\downarrow\) & ArcE\(\uparrow\) & LAMB\(\uparrow\) \\ \hline
16 & 9.56 & 14.04 & 11.45 & 65.40 & 72.40 & 9.56 & 14.04 & 11.45 & 65.40 & 72.40 \\ \hline \multicolumn{10}{c}{OPTQ} & \multicolumn{6}{c}{**QuIP**} \\
4 & 9.59 & 14.22 & 11.56 & 64.77 & 72.39 & 9.60 & 14.18 & 11.50 & 65.32 & 73.20 \\
3 & 10.32 & 15.36 & 12.23 & 60.19 & 68.89 & 9.79 & 14.37 & 11.66 & 65.28 & 72.68 \\
2 & 71.70 & 88.19 & 29.59 & 42.47 & 25.77 & **11.48** & 17.40 & 13.55 & 57.87 & **65.24** \\ \hline \multicolumn{10}{c}{LDLQ-RG} & \multicolumn{6}{c}{**QuIP-RG**} \\
4 & 9.64 & 14.20 & 11.56 & 63.76 & 71.94 & 9.66 & 14.11 & 11.51 & 64.86 & 71.86 \\
3 & 10.31 & 15.15 & 12.15 & 63.43 & 69.78 & 9.75 & 14.44 & 11.68 & 63.51 & 71.53 \\
2 & 49.40 & 73.45 & 29.12 & 41.20 & 26.35 & 11.68 & **16.94** & 13.44 & **59.51** & 62.31 \\ \hline \multicolumn{10}{c}{Greedy} & \multicolumn{6}{c}{Greedy} & \multicolumn{6}{c}{Greedy + IncP} \\
4 & 9.69 & 14.33 & 11.59 & 63.09 & 72.37 & 9.72 & 14.23 & 11.52 & 65.99 & 71.71 \\
3 & 13.63 & 23.05 & 16.30 & 50.51 & 56.76 & 9.92 & 14.45 & 11.71 & 63.80 & 71.38 \\
2 & 4816.6 & 3473.81 & 3183.2 & 26.30 & 0.00 & 11.59 & 17.39 & **13.30** & 58.80 & 64.47 \\ \hline \multicolumn{10}{c}{Near} & \multicolumn{6}{c}{Near + IncP} \\
4 & 10.77 & 15.41 & 13.52 & 61.28 & 70.42 & 9.77 & 14.16 & 11.53 & 64.06 & 71.41 \\
3 & 1564.9 & 1526.2 & 1808.2 & 34.47 & 1.73 & 9.89 & 14.49 & 11.74 & 64.06 & 71.41 \\
2 & 41547.8 & 34348.6 & 24815.7 & 25.80 & 0.00 & 12.04 & 18.12 & 14.11 & 56.36 & 60.64 \\ \hline \end{tabular}
\end{table}
Table 2: Quantizing OPT-30b with various quantization and processing methods, and evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

Figure 5: Quantizing OPT models up to 66B parameters. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.

2-bit quantization. We provide plots on the remaining datasets in Supplement C. Note that the dip in OPTQ on OPT-66B is documented in their paper.

Table 1 shows the results of quantizing Llama 2 70B using QuIP and OPTQ. Again, QuIP achieves good quantization at two bits while OPTQ does not.

**Incoherence Processing Ablation.** Table 2 shows all combinations of quantization and processing methods evaluated on OPT-30B. At lower weight bits, QuIP's incoherence processing dramatically improves the performance of all quantization methods, across all evaluation tasks. Remarkably, all quantization methods--even nearest--are viable at two bits with our incoherence processing. Our modifications in QuIP-RG sometimes give an improvement over QuIP, but further study is required to evaluate these modifications. Figures for OPT-125M to 13B are in Supplement C.

**Throughput Comparison.** We evaluate the additional overhead of our incoherence processing during model inference by modifying OPTQ's efficient forward pass. OPTQ's implementation contains a quantized-matrix full-precision-vector product kernel and was shown to offer speedups over a FP16 baseline. Our incoherence processing additions are performed in PyTorch. Table 4 shows that our QuIP implementation is about 1.5\(\times\) slower than OPTQ.

**Further Ablation.** QuIP's incoherence processing contains several sub-steps. Table 3 shows their relative contributions; all are necessary for the full improvement. Table 5 shows that the random permutation step within the fast orthogonal multiplication also significantly reduces perplexity.

## 7 Conclusion

This paper introduced quantization with incoherence processing (QuIP), an algorithm consisting of (1) an optimal adaptive rounding procedure which minimizes a quadratic proxy of the weight error, and (2) efficient pre- and post-processing to ensure the incoherence of the weight and Hessian matrices by multiplying them by a Kronecker product of random orthogonal matrices. We showed that QuIP quantization is optimal in a general class of adaptive rounding methods with linear feedback; this theoretical analysis is the first for any quantization algorithm that scales to LLM-sized models.

Empirically, QuIP achieves the first viable two-bit quantization results for LLMs, especially at large model sizes, hinting at the feasibility of accurate 2-bit inference in LLMs.

## Acknowledgements and Disclosure of Funding

This work was partially funded by the National Science Foundation under awards DGE-1922551, CAREER awards 2046760 and 2145577, by the National Institute of Health under award MIRA R35GM151243, and a gift from CISCO.

\begin{table}
\begin{tabular}{c c c} Method & Throughput \\ \hline QuIP & 81ms \\ OPTQ & 53ms \\ \end{tabular}
\end{table}
Table 4: Average per-token throughput (batch size 1) when generating sequences of length 128 with OPT-66B on an A6000 GPU.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Wbits & Rescale & Incoherence & Rescale+Incoherence & Rescale+Incoherence+Quant Range \\ \hline
4 & 24.30 & 24.32 & 24.05 & 23.89 \\
3 & 32.62 & 42.28 & 31.32 & 26.36 \\ \end{tabular}
\end{table}
Table 3: Ablating sub-steps of QuIPâ€™s incoherence processing, see Algorithm 1. Perplexities are averaged over WikiText2, PTB, and C4 for OPT-350m.

\begin{table}
\begin{tabular}{c c} \hline \hline Wbits & \(\Delta\)Perplexity from \\  & random permute\(\downarrow\) \\ \hline
4 & -0.22 \\
3 & -9.96 \\
2 & -74.2 \\ \end{tabular}
\end{table}
Table 5: Ablating random permutation within fast orthogonal multiplication. Differences in perplexity are averaged over WikiText2, PTB, and C4 for OPT-125m.

## References

* [1]M. Boratko, H. Padigela, D. Mikkilineni, P. Yuvraj, R. Das, A. McCallum, M. Chang, A. Fokoue-Nkoutche, P. Kapanipathi, N. Mattei, R. Musa, K. Talamandupula, and M. Witbrock (2018-07) A systematic classification of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on Machine Reading for Question Answering, Melbourne, Australia, July 2018, pp. 60-70. External Links: Link, Document Cited by: SS1, SS2.
* [2]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, and et. al (2020) Language models are few-shot learners. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [3]C. De Sa, K. Olukotun, and C. Re (2015) Global convergence of stochastic gradient descent for some non-convex matrix problems. In International Conference on Machine Learning, Cited by: SS1, SS2.
* [4]T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer (2022) Llm:int8(): 8-bit matrix multiplication for transformers at scale. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [5]Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer (2019) Hawq: hessian aware quantization of neural networks with mixed-precision. In International Conference on Computer Vision, Cited by: SS1, SS2.
* [6]Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and K. Keutzer (2020) Hawq-v2: hessian aware trace-weighted quantization of neural networks. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [7]E. Frantar, S. Pal Sing, and D. Alistarh (2022) Optimal brain compression: a framework for accurate post-training quantization and pruning. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [8]E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh (2023) Optq: accurate quantization for generative pre-trained transformers. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [9]I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry (2021) Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, Cited by: SS1, SS2.
* [10]Y. Jeon, C. Lee, E. Cho, and Y. Ro (2022) Mr.biq: post-training non-uniform quantization based on minimizing the reconstruction error. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [11]Y. Li, S. Xu, B. Zhang, X. Cao, P. Gao, and G. Guo (2022) Q-vit: accurate and fully quantized low-bit vision transformer. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [12]Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu (2021) Brecq: pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [13]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [14]Z. Liu, Y. Wang, K. Han, W. Zhang, S. Ma, and W. Gao (2021) Post-training quantization for vision transformer. In Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [15]E. Lybrand and R. Saab (2021) A greedy algorithm for quantizing neural networks. In Journal of Machine Learning Research, Cited by: SS1, SS2.
* [16]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [17]Y. Liu, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu (2021) Brecq: pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [18]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [19]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [20]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [21]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [22]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [23]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [24]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1, SS2.
* [25]Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang (2023) Noisyquant: noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, Cited by: SS1* Marcus et al. [1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The Penn Treebank: Annotating predicate argument structure. In _Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994_, 1994. URL https://aclanthology.org/H94-1020.
* Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Mostafazadeh et al. [2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 839-849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.
* Nagel et al. [2019] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In _International Conference on Computer Vision_, 2019.
* Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In _International Conference on Machine Learning_, pages 7197-7206. PMLR, 2020.
* Paperno et al. [2016] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.
* Park et al. [2023] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. _arXiv preprint arXiv:2206.09557_, 2023.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Conference on Neural Information Processing Systems_, 2019.
* Prateek et al. [2013] Jain Prateek, Netrapalli Praneeth, and Sanghavi Sujay. Low-rank matrix completion using alternating minimization. In _Proceedings of the Forty-fifth Annual ACM STOC_, 2013.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
* Tata and Patel [2003] Sandeep Tata and Jignesh M Patel. Piaq: An algebra for querying protein data sets. In _International Conference on Scientific and Statistical Database Management_, 2003.
* Touvron et al. [2018] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esibou, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* [28] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In _International Conference on Machine Learning_. PMLR, 2020.
* [29] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. In _Conference on Neural Information Processing Systems_, 2022.
* [30] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, and Francois Yvon et. al. Bloom: A 176b-parameter open-access multilingual language model, 2023.
* [31] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. _arXiv preprint arXiv:2211.10438_, 2023.
* [32] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, and Kurt Keutzer. Hawq-v3: Dyadic neural network quantization. In _International Conference on Machine Learning_. PMLR, 2021.
* [33] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In _Conference on Neural Information Processing Systems_, 2022.
* [34] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. _arXiv preprint arXiv:2304.01089_, 2023.
* [35] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.

Checklist

### Broader Impacts

Our work pushes the quantization of large language models into the 2 bits per weight regime. Our aim is to drive foundational research on theoretical and empirical aspects of quantization. The ultimate goal is to enable more powerful LLMs to run more efficiently. However our work is unaware to what ends those LLMs are used.

### Limitations

The adaptive rounding [3] proxy objective considers each layer in isolation; it remains to be seen what other computationally tractable proxies could improve quantization. For example quantization methods do exist which consider interactions between layers, but so far have been too computationally expensive to be applied to the largest open LLMS.

### Experiments, Reproducibility

Our code is included in the Supplement. See the included README for instructions on how to reproduce the various experiments, including random seeds. The code also downloads all datasets used to quantize or evaluate the models.

## Appendix B Additional Method Clarifications

### Subsection 4.2 (Incoherence-Based Heuristics)

Line 4 diagonally rescales \(W\) and \(H\) to minimize \(\ell(\hat{W})\approx\operatorname{tr}\left(H\right)\|W\|_{F}^{2}\), effectively trading off the spectrum of these matrices to find a minimum. Note to minimize \(\operatorname{tr}\left(D^{-1}HD^{-1}\right)\|WD\|_{F}^{2}=(\sum_{i=1}^{n}H_{ii} /D_{i}^{2})(\sum_{i=1}^{n}D_{i}^{2}\|W_{i}\|^{2})\) implies that \(D_{i}=\sqrt{H_{ii}/\|W_{i}\|}\). Motivated by the incoherence of \(W\), Line 6 computes the quantization range depending on the spectrum \(\|W\|_{F}\), instead of the typical \(\max_{i,j}|W_{ij}|\). The parameter \(\rho\) controls the quantization range; we tune it and find that a value of 2.4 works well across all our experiments. We use \(\rho=2.4\) consistently across all experiments. Our full QuIP procedure is described in Algorithm 3, which contains calls to the pre- and post-processing sub-steps in Algorithms 1 and 2.

### Subsection 4.2 (Greedy Updates)

In this subsection, we describe the "greedy local search" method mentioned in the main body of the paper in more detail. The basic idea is to iterate over coordinates of the weights in the same order as the initial quantization method, modifying each weight in turn--but still restricting it to be a representable quantized value--so as to minimize the proxy loss while keeping the other weights fixed. These greedy updates amount to coordinate descent on the proxy loss, but restricted to the quantization grid. Greedy updates can be performed after any initial quantization method, or as a standalone method. When performed after an initial quantization method, greedy local search is a _descent method_ because the individual weight updates cannot increase the loss, but when performed alone, these greedy updates are not a descent method because the initial point (\(\hat{W}=W\)) is not feasible because it contains unquantized values that are off the representable quantization grid. Concretely, a greedy update of weight \((i,j)\) to the grid \(\{0,1,\ldots,2^{b}-1\}\) does the following, where \(\ell\) is the proxy loss:

\[\hat{W}_{ij}\leftarrow\arg\min_{z\in\{0,1,\ldots,2^{b}-1\}}\ell(\hat{W}-e_{i }e_{j}^{T}\hat{W}_{ij}+e_{i}e_{j}^{T}z).\]

(Note that \(\hat{W}-e_{i}e_{j}^{T}\hat{W}_{ij}+e_{i}e_{j}^{T}z\) is the result of setting the \((i,j)\)th entry of \(\hat{W}\) to \(z\).) A full pass of greedy updates constitutes \(mn\) of these updates performed in the same order as LDLQ. This algorithm is very simple, since it is just greedy coordinate descent. In the rest of this subsection, we will give a bit more intuition about this method by showing how this greedy algorithm falls within our framework of adaptive rounding with linear feedback.

An application of greedy local search as a single-pass stand-alone method falls under our Adaptive Rounding with Linear Feedback framework, with the linear feedback set to \(U=(H\odot M)\operatorname{diag}(H)^{-1}\), where \(M\) is the strictly upper triangular mask and \(\odot\) denotes the Hadamard (entrywise) product, as we will derive below. For ease of explanation consider a single (row) weight vector \(w\in\mathbb{R}^{1\times n}\). When looking only at column \(j\), the proxy loss from setting \(\hat{w}_{j}\) to \(z\) is

\[\ell(\hat{w}-\hat{w}e_{j}e_{j}^{T}+ze_{j}^{T})=(\hat{w}-w)H(\hat{w }-w)^{T}+2(ze_{j}^{T}-\hat{w}e_{j}e_{j}^{T})H(\hat{w}-w)^{T}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+(ze_{j}^{T}-\hat{w}e_{j}e_{j }^{T})H(ze_{j}^{T}-\hat{w}e_{j}e_{j}^{T})^{T}.\]

This is just a quadratic function in \(z\), and so its minimum value on the grid \(\{0,1,\dots,2^{b}-1\}\) will just be its minimum value on \(\mathbb{R}\) rounded to that grid. To find this minimum over \(\mathbb{R}\), we differentiate to minimize, yielding

\[0=2e_{j}^{T}H(\hat{w}-w)^{T}+2e_{j}^{T}H(ze_{j}^{T}-\hat{w}e_{j}e_{j}^{T})^{T},\]

and solving for \(z\),

\[z=-\frac{(\hat{w}-\hat{w}e_{j}e_{j}^{T}-w)He_{j}}{e_{j}^{T}He_{j}}=\hat{w}e_{j }-\frac{(\hat{w}-w)He_{j}}{e_{j}^{T}He_{j}}.\] (8)

Since when we use greedy local search as a stand-alone method, we have not updated \(\hat{w}_{j}\) yet, at this point \(\hat{w}e_{j}=we_{j}\), and so this means that a single step of greedy updates looks like

\[\hat{w}e_{j}\leftarrow\mathcal{Q}\left(we_{j}-(\hat{w}-w)\frac{He_{j}}{e_{j} ^{T}He_{j}}\right)\]

for \(\mathcal{Q}\) referring to nearest rounding with the necessary clamping. Since \(\hat{w}-w\) is zero for all entries following the \(j\)th one, this is equivalent to

\[\hat{w}e_{j}\leftarrow\mathcal{Q}(we_{j}-(\hat{w}-w)Ue_{j})\]

where \(U\) is set as \(U=(H\odot M)\operatorname{diag}(H)^{-1}\) as above. This shows how this single-pass version of greedy updates fits into our adaptive rounding with linear feedback framework.

Analyzing greedy local search as a post-processing pass is a bit more difficult, but we will see that it can also be written as something like adaptive rounding with linear feedback. Suppose that we do a pass of greedy updates, but our quantized weights start at an initial value \(\hat{w}=\tilde{w}\) already quantized from some previous method (e.g. LDLQ). Returning to (8), since we haven't updated \(\hat{w}_{j}\) yet, we'll have

\[z=\tilde{w}e_{j}-\frac{(\hat{w}-w)He_{j}}{e_{j}^{T}He_{j}}.\]

Now, all the entries of \(\hat{w}\) which come _after_\(j\) are still the ones from \(\tilde{w}\). This means that we can split this up as

\[z=we_{j}-\frac{(\hat{w}-w)_{:,1:(j-1)}H_{1:(j-1),j}+(\tilde{w}-w)_{:,(j+1):n}H _{(j+1):n,j}}{e_{j}^{T}He_{j}}\]

where the first part of this sum comes from the entries which we _may_ have already updated during this pass, the second comes from the entries which are still equal to their initial values in \(\tilde{w}\), and the case of \(w_{j}\) is handled specially, cancelling it with the \(\tilde{w}e_{j}\) term. We can write this more compactly in matrix form as

\[z=we_{j}-\frac{(\hat{w}-w)(H\odot M)e_{j}+(\tilde{w}-w)(H\odot M^{T})e_{j}}{ e_{j}^{T}He_{j}},\]where \(M\) is the strictly upper triangular mask and \(\odot\) is elementwise multiplication. This yields a final quantization step of

\[\hat{w}e_{j}\leftarrow\mathcal{Q}\left(we_{j}-(\tilde{w}-w)\frac{(H\odot M^{T})e _{j}}{e_{j}^{T}He_{j}}-(\hat{w}-w)\frac{He_{j}}{e_{j}^{T}He_{j}}\right).\]

So, more generally, if we define \(U\) as above, and set

\[V=W-(\tilde{W}-W)(H\odot M^{T})\operatorname{diag}(H)^{-1},\]

we can write a single pass of greedy updates in matrix form as

\[\tilde{W}\leftarrow\mathcal{Q}(V+(W-\hat{W})U),\]

which is very close to our rounding with linear feedback form, albeit with the difference that here \(V\) is in place of \(W\). This is made explicit in the included Greedy Updates Algorithm.

We can use this algorithm both as a whole quantization method (by setting \(\tilde{W}=W\)) or as a post-processing step (by setting \(\tilde{W}\) to the output of some other initial quantization algorithm, such as LDLQ). When we do use it as a post-processing step, we typically run multiple passes of greedy updates (e.g. 10 passes): this involves passing the output of the greedy updates algorithm back in as the input guess \(\tilde{W}\) to another run of the greedy updates algorithm, and repeating this multiple times.

## Appendix C Additional Experimental Descriptions and Results

### Subsections 3.2 and 3.3 (Empirical Properties of \(H\) Across OPT-125m to 2.7b)

Interpreting the exact proxy loss of LDLQ and nearest rounding by empirically comparing \(\operatorname{tr}\left(D\right)\) vs \(\operatorname{tr}\left(H\right)\).Theorem 1 gives the average-case proxy loss for LDLQ in terms of \(\operatorname{tr}\left(D\right)\), where \(D\) is from the LDL decomposition of \(H\). Lemma 3 gives the average-case proxy loss for standard nearest rounding in terms of \(\operatorname{tr}\left(H\right)\). We know that LDLQ is better in practice, but comparing these equations is difficult because we need to reason about \(\operatorname{tr}\left(D\right)\) vs \(\operatorname{tr}\left(H\right)\). Our paper resolves this difficulty by deriving bounds on the proxy loss for LDLQ in terms of the spectrum of \(H\) (with and without incoherence). However we also perform a quick empirical check: if \(\operatorname{tr}\left(D\right)\ll\operatorname{tr}\left(H\right)\), then our theory explains the empirical superiority of LDLQ over nearest rounding (at least on these models). Table 6 gives the ratio \(\operatorname{tr}\left(D\right)/\operatorname{tr}\left(H\right)\) across all layers for OPTQ models 125m to 2.7b; the mean value is always less than \(0.55\), and it falls as the model gets larger.

\(H\) is approximately low-rank.Subsection 3.3 plotted the normalized eigenvalues of \(H\) from 3 randomly chosen layers in OPT-2.7b. Table 6 gives much more evidence that \(H\) is consistently approximately low-rank. Across each model, we calculate the absolute and approximate fractional rank

\begin{table}
\begin{tabular}{c l|c c c} \multirow{2}{*}{Model} & \multirow{2}{*}{Processing} & \multicolumn{2}{c}{Absolute} & \multicolumn{2}{c}{Approximate} & \multirow{2}{*}{\(\operatorname{tr}\left(D\right)/\operatorname{tr}\left(H\right)\)} \\  & & Fractional Rank & Fractional Rank & \\ \hline \multirow{2}{*}{OPT-125m} & Baseline & 0.926 (\(\pm\)0.172) & 0.112 (\(\pm\)0.127) & 0.540 (\(\pm\)0.093) \\  & Incoherent & 0.910 (\(\pm\)0.196) & 0.124 (\(\pm\)0.141) & 0.534 (\(\pm\)0.094) \\ \multirow{2}{*}{OPT-350m} & Baseline & 0.916 (\(\pm\)0.180) & 0.047 (\(\pm\)0.032) & 0.445 (\(\pm\)0.100) \\  & Incoherent & 0.908 (\(\pm\)0.183) & 0.059 (\(\pm\)0.062) & 0.440 (\(\pm\)0.106) \\ \multirow{2}{*}{OPT-1.3b} & Baseline & 0.541 (\(\pm\)0.404) & 0.020 (\(\pm\)0.023) & 0.399 (\(\pm\)0.187) \\  & Incoherent & 0.543 (\(\pm\)0.405) & 0.028 (\(\pm\)0.023) & 0.393 (\(\pm\)0.189) \\ \multirow{2}{*}{OPT-2.7b} & Baseline & 0.426 (\(\pm\)0.413) & 0.019 (\(\pm\)0.015) & 0.384 (\(\pm\)0.206) \\  & Incoherent & 0.427 (\(\pm\)0.415) & 0.018 (\(\pm\)0.025) & 0.375 (\(\pm\)0.205) \\ \end{tabular}
\end{table}
Table 6: We compute \(H\) in each layer of a given model, and compute the following summary statistics. \(\operatorname{tr}\left(D\right)/\operatorname{tr}\left(H\right)\) decreases as the mode size increases, though the variance also increases. We compute the fraction of nonzero eigenvalues (i.e. absolute), and the fraction of eigenvalues \(>0.01\cdot\max(\operatorname{eig}(H))\) (i.e. approximate). The fractional rank is \(k/n\) for a rank-\(k\) matrix \(H\) with dimension \(n\). Mean and standard deviations are computed across layers in a model.

of \(H\) across all layers in OPT models 125m to 2.7b (explanations in the caption). The approximate fractional rank decreases as model size increases; for OPT-2.7b the fractional rank is \(\approx 0.02(\pm 0.02)\).

### Subsection 5.1 (Empirical Verification of OPTQ Equivalence)

We share a python script in the supplementary code which empirically verifies that our implementation of LDLQ produces quantized values exactly matching OPTQ's [1] implementation. While we prove the equivalence between LDLQ and OPTQ's respective algorithm statements, empirically comparing ours and Frantar et al. [1]'s code ensures that the respective implementations are sufficiently close to their algorithmic statements. Therefore we can be sure that LDLQ and OPTQ are equivalent in their implementation.

### Subsection 5.2 (Empirical Verification of LDLQ/OPTQ Finite Grid Counterexample)

The following code constructs a weight matrix \(W\) and Hessian matrix \(H\) where OPTQ performs worse than nearest when rounding to a finite grid.

```
1importtorch
2deffmake_counterexample(n,d,c=0.01):
3H=torch.ones(n,n)+torch.eye(n)
4H[n-1,n-1]=1.0
5H[0,1:(n-1)]+=2*c
6H[1:(n-1),0]+=2*c
7H[0,n-1]+=c
8H[n-1,0]+=c
9H[0,0]+=4*c+n*(c**2)
10W=0.499*torch.ones(d,n)+0.002*(torch.arange(n)%2)
11returnW,H ```

The intuition behind this counterexample is as follows: we want to quantize many coordinates in \(W\) in such a way that OPTQ excepts there to be a very large error correction to quantize the last entry. However, the finite grid restricts this large error correction. Note that we can achieve this poor OPTQ behavior with c=0, but here nearest rounding also does poorly. We make a small perturbation (c=0.01) to make OPTQ round in the wrong direction, but not nearest.

### Additional Details on the Experimental Setup and Computational Resources

We run experiments on a university cluster managed by a Slurm workload manager which has GPUs with up to 48GB of memory, though larger GPUs are only required for some methods on larger model sizes. Note we use the LAMBADA OpenAI version. When Greedy updates are used, we perform 10 passes over the weights in the same order as LDLQ and OPTQ, except for 5 passes on OPT-30b and OPT-66b. For the incoherence-based quantization range, we tune the parameter \(\rho\) and find that a value of 2.4 works well across all model sizes and quantization methods. We use this value for all our experiments.

### Section 6 (Main Results on Additional Evaluations)

Figure 6 shows additional results for QuIP and OPTQ on WikiText2, PiQA, and StoryCloze when quantizing to 2 and 3 bits per weight. The insights about our method QuIP remain the same after viewing these additional results: QuIP is the first PTQ procedure to achieve good quantization at two bits per weight, across a variety of LLM sizes and evaluation tasks. We evaluate on OPT models (up to 30B); 4-bit quantization works equally well for both methods. QuIP is superior to OPTQ across model sizes and evaluation tasks here.

On WikiText2 2-bit quantization, note that the trend in perplexity for QuIP mirrors the trend in perplexity for OPTQ. We run OPTQ's [1] implementation, though they did not report 2-bit results at this model size. Because OPTQ is equivalent to QuIP's quantization sub-procedure, it thus makes sense that worse performance in the quantization sub-procedure could result in worse overall performance. OPTQ increases perplexity when going from OPT-1.3b to OPT-2.7b. QuIP's perplexity also increases from OPT-1.3b to OPT-2.7b, and is unusually higher than the adjacent OPT-1.3b and 

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c|c c c c|c c c}  & \multicolumn{10}{c}{**Incoherence Processing â€” OPT-13b**} \\  & Full & \multicolumn{3}{c}{QuIP} & \multicolumn{3}{c}{QuIP-RG} & \multicolumn{3}{c}{Greedy+InCp} & \multicolumn{3}{c}{Near+InCp} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wik\(\downarrow\) & 10.13 & 10.21 & 10.5 & 16.02 & 10.35 & 10.69 & 13.81 & 10.25 & 10.61 & 13.91 & 10.34 & 10.59 & 16.12 \\ PTB\(\downarrow\) & 14.52 & 14.69 & 15.05 & 21.64 & 14.73 & 15.20 & 22.23 & 14.85 & 15.11 & 20.20 & 14.93 & 15.27 & 23.18 \\ C4\(\downarrow\) & 12.06 & 12.16 & 12.39 & 16.60 & 12.18 & 12.43 & 15.62 & 12.21 & 12.42 & 15.19 & 12.26 & 12.56 & 17.37 \\ ArcE\(\uparrow\) & 61.78 & 61.41 & 59.47 & 53.91 & 60.35 & 61.78 & 52.86 & 61.00 & 59.43 & 53.79 & 60.56 & 59.30 & 50.50 \\ LAMB\(\uparrow\) & 70.25 & 72.09 & 79.10 & 56.24 & 69.47 & 69.07 & 55.50 & 70.83 & 68.43 & 56.98 & 68.37 & 67.86 & 46.48 \\ PiQA\(\uparrow\) & 76.82 & 76.61 & 76.17 & 77.25 & 76.55 & 76.22 & 72.74 & 76.33 & 76.71 & 71.87 & 57.08 & 76.66 & 70.73 \\ SC\(\uparrow\) & 76.58 & 75.62 & 74.92 & 70.21 & 75.88 & 75.75 & 70.53 & 75.43 & 75.62 & 72.50 & 74.47 & 75.43 & 68.43 \\ \hline \end{tabular} 
\begin{tabular}{l|c|c c c c|c c c|c c c|c c c}  & \multicolumn{10}{c}{**Baseline Processing â€” OPT-13b**} \\  & Full & \multicolumn{3}{c}{OPTQ} & \multicolumn{3}{c}{LDL-QG} & \multicolumn{3}{c}{Greedy} & \multicolumn{3}{c}{Near} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wik\(\downarrow\) & 10.13 & 10.31 & 11.60 & 372.68 & 10.28 & 11.54 & 213.75 & 10.73 & 13.67 & 83.70 & 11.33 & 3.333 & 186,069 \\ PTB\(\downarrow\) & 14.52 & 14.91 & 16.59 & 344.44 & 14.85 & 16.43 & 220.38 & 15.25 & 18.62 & 7.053 & 16.40 & 2.708 & 121,291 \\ C4\(\downarrow\) & 12.06 & 12.26 & 13.34 & 135.48 & 12.24 & 13.17 & 67.48 & 12.55 & 14.30 & 4.316 & 13.32 & 2.711 & 93,834 \\ ArcE\(\uparrow\) & 61.78 & 64.77 & 60.19 & 42.47 & 60.77 & 58.54 & 32.07 & 56.61 & 51.22 & 25.38 & 61.32 & 31.10 & 25.42 \\ LAMB\(\uparrow\) & 70.25 & 72.39 & 68.89 & 25.77 & 68.72 & 65.30 & 6.58 & 68.12 & 59.36 & 00.02 & 67.22 & 00.06 & 00.00 \\ PiQA\(\uparrow\) & 76.82 & 78.56 & 78.02 & 66.05 & 76.28 & 75.08 & 59.09 & 76.50 & 73.45 & 50.98 & 76.06 & 53.10 & 49.62 \\ SC\(\uparrow\) & 76.58 & 77.53 & 75.62 & 63.59 & 76.32 & 73.52 & 56.33 & 75.68 & 72.44 & 49.40 & 74.41 & 49.71 & 48.70 \\ \hline \end{tabular}
\end{table}
Table 8: Quantizing **OPT-13b** with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

\begin{table}
\begin{tabular}{l|c c c c|c c c|c c c|c c c}  & \multicolumn{10}{c}{**Incoherence Processing â€” OPT-6.7b**} \\  & Full & \multicolumn{3}{c}{QuIP} & \multicolumn{3}{c}{QuIP-RG} & \multicolumn{3}{c}{Greedy+InCp} & \multicolumn{3}{c}{Near+InCp} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wik\(\downarrow\) & 10.86 & 10.98 & 11.51 & 22.33 & 11.20 & 11.61 & 23.75 & 11.13 & 11.62 & 19.06 & 11.18 & 11.73 & 18.57 \\ PTB\(\downarrow\) & 15.77 & 15.93 & 16.52 & 31.73 & 15.99 & 16.43 & 45.53 & 15.88 & 16.50 & 35.94 & 16.06 & 16.67 & 27.04 \\ C4\(\downarrow\) & 12.71 & 12.86 & 13.30 & 21.62 & 12.88 & 13.39 & 24.98 & 12.89 & 13.27 & 19.62 & 12.96 & 13.37 \\ ArcE\(\uparrow\) & 60.06 & 59.89 & 59.60 & 52.61 & 59.30 & 58.21 & 53.32 & 59.18 & 58.25 & 51.43 & 59.85 & 57.62 & 50.59 \\ LAMB\(\uparrow\) & 68.72 & 70.00 & 68.74 & 53.97 & 67.38 & 65.77 & 49.91 & 67.65 & 67.18 & 54.80 & 67.26 & 65.86 & 49.49 \\ PiQA\(\uparrow\) & 76.55 & 76.77 & 76.33 & 72.47 & 76.71 & 76.33 & 73.291 & 76.39 & 75.46 & 72.20 & 76.55 & 76.71 & 71.22 \\ SC\(\uparrow\) & 74.47 & 75.18 & 73.65 & 68.43 & 75.05 & 73.33 & 69.51 & 74.35 & 73.77 & 68.94 & 74.22 & 74.09 & 68.75 \\ \hline \end{tabular} 
\begin{tabular}{l|c c c c|c c c|c c c|c c c}  & \multicolumn{10}{c}{**Baseline Processing â€” OPT-6.7b**} \\  & Full & \multicolumn{3}{c}{OPTQ} & \multicolumn{3}{c}{LDL-QG} & \multicolumn{3}{c}{Greedy} & \multicolumn{3}{c}{Near} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wik\(\downarrow\) & 10.86 & 11.49 & 14.87 & 2.958 & 11.23 & 12.56 & 739.9 & 11.75 & 39.09 & 16.298 & 12.15 & 6,011 & 20,780 \\ PTB\(\downarrow\) & 15.77 & 16.54 & 22.05 & 2.521 & 16.28 & 18.58 & 1,109 & 16.93 & 66.57 & 10,708 & 18.92 & 5,440 & 14,217 \\ C4\(\downarrow\) & 12.71 & 13.16 & 17.13 & 500.07 & 12.98 & 14.34 & 15.40

\begin{table}
\begin{tabular}{l|c|c c c|c c c c|c c c c|c c c}  & \multicolumn{10}{c}{**Incoherence Processing â€” OPT-2.7b**} \\  & Full & \multicolumn{3}{c}{QuIP} & \multicolumn{3}{c}{QuIP-RG} & \multicolumn{3}{c}{Greedy+InCP} & \multicolumn{3}{c}{Near+InCP} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 12.47 & 12.39 & 17.44 & 2.998 & 12.58 & 15.07 & 1.676 & 12.68 & 12.96 & 15.65 & 12.79 & 13.79 & 28.98 \\ PTB\(\downarrow\) & 17.97 & 18.42 & 20.79 & 63.59 & 18.43 & 20.49 & 42.05 & 18.34 & 20.03 & 46.28 & 18.43 & 19.51 & 39.23 \\ C4\(\downarrow\) & 14.34 & 14.55 & 15.63 & 38.07 & 14.65 & 15.97 & 27.89 & 14.64 & 15.22 & 26.84 & 14.67 & 15.52 & 27.34 \\ ArcE\(\uparrow\) & 54.34 & 53.28 & 52.99 & 46.93 & 52.02 & 52.36 & 46.93 & 52.90 & 51.73 & 43.14 & 52.61 & 50.93 & 44.11 \\ LAMB\(\uparrow\) & 64.82 & 66.04 & 64.99 & 36.06 & 64.64 & 63.46 & 43.39 & 64.68 & 62.95 & 45.53 & 65.40 & 61.05 & 35.65 \\ POA\(\uparrow\) & 74.76 & 74.54 & 73.94 & 68.06 & 73.88 & 73.45 & 68.28 & 74.54 & 73.83 & 68.28 & 73.61 & 73.56 & 67.85 \\ SC\(\uparrow\) & 71.74 & 71.78 & 70.21 & 66.14 & 71.55 & 70.15 & 64.67 & 70.85 & 71.10 & 65.82 & 71.16 & 70.02 & 63.27 \\ \hline \multicolumn{10}{c}{**Baseline Processing â€” OPT-2.7b**} \\  & Full & \multicolumn{3}{c}{OPTQ} & \multicolumn{3}{c}{LDLQ-RG} & \multicolumn{3}{c}{Greedy} & \multicolumn{3}{c}{Near} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 12.47 & 12.93 & 17.09 & 8,949 & 12.77 & 16.47 & 7,718 & 12.95 & 18.92 & 9,665 & 16.69 & 15,685 & 10,641 \\ PTB\(\downarrow\) & 17.97 & 19.10 & 25.36 & 8,281 & 19.05 & 23.94 & 7,389 & 19.06 & 28.75 & 8,254 & 32.22 & 14,532 & 10,516 \\ C4\(\downarrow\) & 14.34 & 14.99 & 18.14 & 4.388 & 14.85 & 17.37 & 2,113 & 15.01 & 20.87 & 5,139 & 18.75 & 11,257 & 9,356 \\ ArcE\(\uparrow\) & 54.34 & 52.57 & 50.04 & 26.94 & 52.02 & 48.95 & 25.76 & 52.02 & 43.39 & 25.46 & 52.74 & 26.56 & 27.19 \\ LAMB\(\uparrow\) & 64.82 & 62.00 & 51.43 & 00.00 & 64.04 & 53.25 & 00.00 & 63.50 & 40.75 & 00.00 & 59.15 & 00.00 & 00.00 \\ PiQA\(\uparrow\) & 74.76 & 73.88 & 70.73 & 48.42 & 74.54 & 69.91 & 49.95 & 73.61 & 66.05 & 50.65 & 73.83 & 51.41 & 50.22 \\ SC\(\uparrow\) & 71.74 & 70.91 & 68.56 & 48.50 & 71.42 & 67.79 & 47.17 & 70.66 & 60.53 & 48.44 & 70.59 & 47.42 & 47.55 \\ \hline \end{tabular}
\end{table}
Table 10:  Quantizing **OPT-2.7b** with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

\begin{table}
\begin{tabular}{l|c|c c c|c c c|c c c|c c c}  & \multicolumn{10}{c}{**Incoherence Processing â€” OPT-1.3b**} \\  & Full & \multicolumn{3}{c}{QuIP} & \multicolumn{3}{c}{QuIP-RG} & \multicolumn{3}{c}{Greedy+InCP} & \multicolumn{3}{c}{Near+InCP} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 14.62 & 14.88 & 16.21 & 41.64 & 16.49 & 17.76 & 42.37 & 16.75 & 17.11 & 48.69 & 16.43 & 17.83 & 56.56 \\ PTB\(\downarrow\) & 20.29 & 20.87 & 22.76 & 47.72 & 21.93 & 23.25 & 50.17 & 22.11 & 23.76 & 54.46 & 22.19 & 24.82 & 80.40 \\ C4\(\downarrow\) & 16.07 & 16.38 & 17.12 & 29.78 & 17.53 & 18.44 & 31.49 & 17.60 & 18.54 & 34.10 & 17.74 & 19.03 & 45.56 \\ ArcE\(\uparrow\) & 50.84 & 50.72 & 49.12 & 41.88 & 49.54 & 48.82 & 41.20 & 49.66 & 48.74 & 41.08 & 46.61 & 46.59 & 38.64 \\ LAMB\(\uparrow\) & 58.92 & 56.36 & 52.47 & 27.81 & 51.62 & 48.36 & 27.27 & 49.95 & 48.38 & 19.21 & 49.76 & 51.12 & 20.20 \\ PiQA\(\uparrow\) & 72.31 & 71.22 & 71.11 & 64.85 & 71.06 & 70.24 & 63.33 & 71.00 & 70.35 & 63.66 & 71.16 & 69.80 & 62.51 \\ SC\(\uparrow\) & 70.78 & 70.08 & 68.81 & 63.02 & 69.00 & 68.05 & 63.14 & 68.49 & 67.92 & 62.64 & 69.13 & 67.79 & 58.43 \\ \hline \multicolumn{10}{c}{**Baseline Processing â€” OPT-1.3b**} \\  & Full & \multicolumn{3}{c}{OPTQ} & \multicolumn{3}{c}{LDLQ-RG} & \multicolumn{3}{c}{Greedy} & \multicolumn{3}{c}{Near} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 14.62 & 15.59 & 21.35 & 7,856 & 15.36 & 20.22 & 7,739 & 15.58 & 22.68 & 9,786 & 47.62 & 12,658 & 11,690 \\ PTB\(\downarrow\) & 20.29 & 22.03 & 30.74 & 6,858 & 21.85 & 30.10 & 5,368 & 22.00 & 35.18 & 8,441 & 73.51 & 14,705 & 11,690 \\ C4\(\downarrow\) & 16.07 & 16.96 & 21.59 & 4,028 & 16.70 & 20.21 & 2,123 & 16.96 & 22.11 & 5,129 & 27.20 & 6,415 & 8,360 \\ ArcE\(\uparrow\) & 50

### Section 6 (Evaluating the Effectiveness of the Proxy Objective)

In Table 14 we show the proxy loss of the four quantization methods we evaluate, evaluated over OPT models 125m to 2.7b. The proxy is averaged over models proxy losses normalized by their model dimension; we use \(H\) matrices computed as a result of OPTQ and nearest rounding. We do not conduct any processing in the proxy evaluation; this is an evaluation of the rounding methods only. Trends in the proxy reflect end-to-end results. OPTQ/LDLQ, LDLQ-RG, and Greedy are roughly equivalent at 2 bits, and do better than Nearest.

### Section 6 (Evaluating Unbiased Rounding in LDLQ/OPTQ)

Note in our formulation for Adaptive Rounding with Linear feedback, the \(\mathcal{Q}\) subroutine could be biased, or unbiased. It is typical to perform biased rounding in practice; here we investigate if there is

\begin{table}
\begin{tabular}{l|c|c c c|c c c|c c c|c c c}  & \multicolumn{8}{c}{**Incoherence Processing â€” OPT-125m**} \\  & Full & \multicolumn{4}{c}{QuIP} & \multicolumn{4}{c}{QuIP-RG} & \multicolumn{4}{c}{Greedy+InCP} & \multicolumn{4}{c}{Near+InCP} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 27.66 & 33.35 & 34.22 & 347.4 & 31.51 & 42.94 & 361.8 & 30.65 & 55.54 & 230.8 & 31.93 & 37.57 & 397.5 \\ PTB\(\downarrow\) & 38.99 & 40.80 & 47.34 & 430.3 & 43.28 & 51.69 & 414.1 & 41.96 & 48.79 & 250.6 & 43.08 & 52.20 & 441.9 \\ C4\(\downarrow\) & 26.56 & 27.63 & 30.92 & 177.4 & 28.74 & 33.54 & 159.0 & 28.82 & 31.41 & 49.90 & 29.28 & 33.88 & 224.0 \\ ArcE\(\uparrow\) & 40.03 & 38.89 & 37.92 & 31.99 & 39.27 & 38.26 & 31.36 & 38.80 & 37.67 & 33.21 & 38.55 & 37.42 & 32.91 \\ LAMB\(\uparrow\) & 39.16 & 33.03 & 26.37 & 01.05 & 33.75 & 16.96 & 02.17 & 37.78 & 25.34 & 04.66 & 35.65 & 25.21 & 01.82 \\ PiQA\(\uparrow\) & 61.92 & 61.64 & 61.64 & 54.24 & 61.64 & 61.92 & 55.44 & 61.10 & 60.83 & 56.47 & 61.43 & 61.10 & 53.48 \\ SC\(\uparrow\) & 59.96 & 60.03 & 59.20 & 52.13 & 59.07 & 59.26 & 51.94 & 60.15 & 59.52 & 54.04 & 59.13 & 58.88 & 53.41 \\ \hline \end{tabular} 
\begin{tabular}{l|c|c c c|c c|c c|c c|c c c}  & \multicolumn{8}{c}{**Baseline Processing â€” OPT-125m**} \\  & Full & \multicolumn{4}{c}{OPTQ} & \multicolumn{4}{c}{LDLDQ-RG} & \multicolumn{4}{c}{Greedy} & \multicolumn{4}{c}{Near} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 22.00 & 24.16 & 33.51 & 18.687 & 27.37 & 31.87 & 10.446 & 27.01 & 137.3 & 23.952 & 25.94 & 64.56 & 23,668 \\ PTB\(\downarrow\) & 31.07 & 34.17 & 47.69 & 18.161 & 33.35 & 44.38 & 8.508 & 40.39 & 153.5 & 15.176 & 36.78 & 87.22 & 28,881 \\ C4\(\downarrow\) & 22.59 & 24.71 & 31.26 & 8.418 & 24.10 & 29.86 & 3.064 & 27.84 & 73.59 & 9.099 & 26.21 & 55.15 & 17,094 \\ ArcE\(\uparrow\) & 40.36 & 38.43 & 38.38 & 26.30 & 39.06 & 37.42 & 25.46 & 38.34 & 31.06 & 24.33 & 33.68 & 36.36 & 11.5 & 28.88 \\ LAMB\(\uparrow\) & 46.67 & 45.60 & 39.20 & 00.00 & 45.26 & 32.54 & 00.02 & 51.45 & 16.63 & 00.00 & 40.66 & 27.46 & 00.00 \\ PiQA\(\uparrow\) & 64.80 & 64.04 & 63.44 & 51.25 & 65.13 & 61.97 & 49.67 & 63.49 & 55.44 & 50.60 & 63.38 & 60.55 & 51.58 \\ SC\(\uparrow\) & 63.14 & 63.78 & 61.04 & 47.55 & 62.57 & 60.53 & 48.95 & 61.36 & 54.87 & 48.44 & 63.02 & 56.84 & 48.95 \\ \hline \end{tabular}
\end{table}
Table 12: Quantizing **OPT-350m** with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

\begin{table}
\begin{tabular}{l|c c c c|c c c|c c|c c c}  & \multicolumn{8}{c}{**Incoherence Processing â€” OPT-125m**} \\  & Full & \multicolumn{4}{c}{QuIP} & \multicolumn{4}{c}{QuIP-RG} & \multicolumn{4}{c}{Greedy+InCP} & \multicolumn{4}{c}{Near+InCP} \\  & W16 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\ \hline Wiki\(\downarrow\) & 27.66 & 31.44 & 53.26 & 4,563 & 32.29 & 53.25 & 37.04 & 77.80 & 1,791 & 3,707 & 37.14 & 1,293 & 5,375 \\ PTB\(\downarrow\) & 38.99 & 45.31 & 74.79 & 4,410 & 45.56 & 75.85 & 3,596 & 101.1 & 1,403 & 4,622 & 53.93 & 1,418 & 4,267 \\ C4\(\downarrow\) & 26.56 & 29.13 & 42.55 & 2,260 & 29.40 & 41.77 & 1,820 & 65.54 & 80.95 & 1,897 & 33.90 & 836.5 & 3,665 \\ ArcE\(\uparrow\) & 40.03 & 38.51 & 35.73 & 28.62 & 39.02 & 36.36 & 27.19 & 34.05 & 26.43 & 27.15 & 36.66 & 30.39 & 26.01 \\ LAMB\(\uparrow\) & 39.16 & 33.69 & 12.36 & 00.00 & 33.26 & 15.00 & 00.00 & 12.25 & 00.00 & 00.00 & 18.22 & 00.08 & 00.00 \\ PiQA\(\uparrow\) & 61.92 & 60.83 & 59.47 & 52.23 & 61.70 & 59.58 & 50.05 & 57.62 & 49.29 & 50.49 & 61.43 & 55.88 & 51.20 \\ SC\(\uparrow\) & 59.96 & 58.88 & 56.97 & 49.78 & 59.20 & 57.03 & 48.95 & 50.99 & 47.55 & 48.82 & 59.96 & 50.03 & 47.93 \\ \hline \end{tabular}
\end{table}
Table 13: Quantizing **OPT-125m** with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

any benefit to switching to unbiased rounding schemes. Table 15 computes the average perplexity difference (i.e. \(\mathrm{unbiased-biased}\)) for LDLQ/OPTQ on WikiText2, PTB, and C4. That is, we run LDLQ with the \(\mathcal{Q}\) subroutine as stochastic rounding, instead of nearest. The average difference is positive (and large for 2 and 3 bits), meaning that unbiased rounding performs worse than biased (i.e. nearest) across OPT models 125m to 2.7b. These results indicate that in practice, we want to stick with biased rounding schemes.

### Section 6 (Evaluating Algorithm 5 Which Accounts for Clamping)

Table 16 shows results from using Algorithm 5 to quantize OPT models 125m to 1.3b, with incoherence processing and baseline processing. At 2 bits and incoherence processing, we observe modest improvements over QuIP in terms of perplexity on OPT models 125m and 350m. However, at the

\begin{table}
\begin{tabular}{c c|c c c|c c c}  & & \multicolumn{2}{c}{Incoherence Processing (ours)} & \multicolumn{3}{c}{Baseline Processing} \\ \cline{3-8} Model & WBits & Wiki & PTB & C4 & Wiki & PTB & C4 \\ \hline \multirow{3}{*}{OPT-1.3b} & 4 & 16.54 & 22.12 & 17.58 & 15.43 & 21.92 & 16.80 \\  & 3 & 18.27 & 23.96 & 18.66 & 20.45 & 28.86 & 20.68 \\  & 2 & 38.13 & 51.78 & 31.09 & 6,438.75 & 6,099.27 & 2,057.71 \\ \hline \multirow{3}{*}{OPT-350m} & 4 & 23.19 & 32.55 & 23.48 & 23.71 & 33.73 & 24.29 \\  & 3 & 25.54 & 36.74 & 25.52 & 33.01 & 45.15 & 30.09 \\  & 2 & 286.71 & 367.26 & 144.08 & 8,006.22 & 7,445.70 & 2,317.18 \\ \hline \multirow{3}{*}{OPT-125m} & 4 & 32.04 & 44.56 & 29.08 & 32.59 & 41.95 & 28.67 \\  & 3 & 40.66 & 51.90 & 32.91 & 50.73 & 74.14 & 41.04 \\ \cline{1-1}  & 2 & 1,649.83 & 240.86 & 136.55 & 3,714.11 & 4,703.76 & 1,848.72 \\ \end{tabular}
\end{table}
Table 16: Quantizing OPT models using Algorithm 5 evaluated on WikiText2, PTB, and C4. At 2 bits and incoherence processing, we see improvements over LDLQ and LDLQ-RG on OPT-125m and OPT-350m, but diminishing improvements on OPT-1.3b. Due to Algorithm 5â€™s relatively equivalent performance relative to QuIP at OPT-1.3b, and due to this algorithmâ€™s increased computational cost, we decide not to user it.

\begin{table}
\begin{tabular}{c|c c c c c} WBits & LDLQ/OPTQ & LDLQ-RG & Greedy & Near \\ \hline
4 & 104.09 & 105.23 & 120.74 & 301.18 \\
3 & 529.53 & 475.25 & 537.98 & 1,308.05 \\
2 & 2,554.89 & 2,291.02 & 2,587.17 & 5,971.69 \\ \end{tabular}
\end{table}
Table 14: Weighted average of proxy Loss \(\mathrm{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right)\) over OPT models 125m to 2.7b. Proxy is averaged over models normalized by their model dimension (768, 1024, 2048, 2560) respectively, to ensure proxy loss is comparable across models of different size. We do not conduct any processing in the proxy evaluation. Trends in the proxy largely reflect end-to-end results: at 2 bits OPTQ, LDLQ-RG, and Greedy are roughly equivalent, and all do better than nearest.

\begin{table}
\begin{tabular}{c|c c c c|c c c c}  & & \multicolumn{2}{c}{AVERAGE(Perplexity Unbiased - Perplexity Biased) on Wiki, PTB, C4 (\(\downarrow\))} \\ \hline  & \multicolumn{3}{c}{Incoherence Processing} & \multicolumn{3}{c}{Baseline Processing} \\ \cline{2-9} WBits & 125m & 350m & 1.3b & 2.7b & 125m & 350m & 1.3b & 2.7b \\ \hline
4 & 1.23 & 0.73 & 0.79 & 0.19 & 27.81 & 5.58 & 1.62 & 0.87 \\
3 & 13.26 & 7.79 & 2.14 & 4.66 & 880.4 & 499.4 & 28.63 & 16.23 \\
2 & 2,501 & 18,732 & 544.8 & 2,251 & 241.3 & 17,945 & 4,831 & 3,798 \\ \end{tabular}
\end{table}
Table 15: Average perplexity difference (i.e. unbiased - biased) for LDLQ/OPTQ on WikiText2, PTB, and C4. That is, we can run LDLQ with the \(\mathcal{Q}\) subroutine as stochastic rounding, instead of nearest. The average difference is positive, meaning that unbiased rounding performs worse than biased (i.e. nearest) across OPT models 125m to 2.7b. Note the magnitude of the gap increases at lower bits.

larger OPT-1.3b QuIP beats Algorithm 5 on 2/3 language generation tasks. In addition, Algorithm 5 is computationally more work to run. Therefore we decide not to use it.

Another observation: in practice, we don't seem to encounter constructions of \(W\) and \(H\) that are bad for LDLQ/OPTQ. Therefore this "clamping" issue seems to not be an issue in practice, especially as model size increases.

## Appendix D Proofs for Section 3 (Quantization With Incoherence Processing: Adaptive Rounding Step )

**Subsection 3.2** (Deriving the Optimality of the LDLQ Adaptive Rounding Procedure):

**Theorem 1**.: LDLQ _is worst and average-case optimal amongst rounding methods which specify the linear feedback \(U\) as a function of \(H\) (not of \(W\)), and when rounding to the integers. That is, for all rounding methods \(\mathcal{A}\) in the class described by Eq. (2), for all positive semi-definite \(H\), and for \(\mathcal{Q}\) as either nearest or stochastic rounding,_

\[\tfrac{m}{4}\operatorname{tr}(D)=\mathcal{L}_{\mathrm{worst}}(\textsf{LDLQ},H) \leq\mathcal{L}_{\mathrm{worst}}(\mathcal{A},H)\ \text{ and }\ \tfrac{m}{c}\operatorname{tr}(D)=\mathcal{L}_{\mathrm{avg}}(\textsf{ LDLQ},H)\leq\mathcal{L}_{\mathrm{avg}}(\mathcal{A},H),\]

_where \(D\) is the matrix from the LDL decomposition of \(H\), and \(c=12\) for nearest, \(c=6\) for stochastic._

Proof.: Let \(X\) be the strictly upper triangular matrix associated with the rounding procedure \(\mathcal{A}\) such that \(U\gets X\) in Eq. (2). Let \(B\equiv(X+I)^{-1}(\dot{U}+I)\) where \(\dot{U}\) is from the LDL decomposition of \(H\) in Eq. (4). The proxy loss is then,

\[\operatorname{tr}\left((\mathcal{A}(W,H)-W)H(\mathcal{A}(W,H))^{T }\right) \stackrel{{\eqref{eq:LDLQ},\eqref{eq:LDLQ}}}{{=}} \operatorname{tr}\left(\eta(X+I)^{-1}(\dot{U}+I)D(\dot{U}+I)^{T}(X+I)^{-T} \eta^{T}\right)\] \[=\operatorname{tr}\left(\eta BDB^{T}\eta^{T}\right).\] (9)

With the LDL assignment of \(U\), we further have that,

\[\operatorname{tr}\left(\eta BDB^{T}\eta^{T}\right)=\operatorname{tr}\left( \eta D\eta^{T}\right).\] (10)

First, consider the worst-case loss, \(\mathcal{L}_{\mathrm{worst}}\). The goal is to construct a particularly bad case where the entries of \(\tilde{W}\) are \(1/2\pm\epsilon\), and thus when rounding to the integers we will always have error 1/2. Construct a weight matrix \(\tilde{W}\in\mathbb{R}^{m\times n}\) such that each entry satisfies,

\[\tilde{W}_{ij}=\begin{cases}0.5-\epsilon&w.p.\ 1/2\\ 0.5+\epsilon&w.p.\ 1/2\end{cases}\ \ \Rightarrow\ \ \eta_{ij}=\begin{cases}+0.5&w.p.\ 1/2\\ -0.5&w.p.\ 1/2\end{cases},\]

and the quantization errors \(\eta\in\mathbb{R}^{m\times n}\) are for each entry \(\{+1/2,-1/2\}\) with equal probability. For this particular \(\tilde{W}\), \(\mathcal{A}\) achieves proxy loss \(\mathcal{L}_{\mathrm{worst}}(\mathcal{A},H)\stackrel{{\eqref{eq:LDLQ} }}{{=}}\mathbf{E}\left[\operatorname{tr}\left(\eta BDB^{T}\eta^{T}\right) \right]=\tfrac{m}{4}\operatorname{tr}\left(BDB^{T}\right)\), with \(\mathcal{Q}\) as either nearest or stochastic rounding. It follows from the supremum in the definition of \(\mathcal{L}_{\mathrm{worst}}\) in Eq. (5) that, \(\mathcal{L}_{\mathrm{worst}}(\mathcal{A},H)\geq\tfrac{m}{4}\operatorname{tr} \left(BDB^{T}\right)\). For the LDL assignment of \(U\), the worst case expected quantization error rounding to the integers is \(1/2\). Therefore, \(\mathcal{L}_{\mathrm{worst}}(\textsf{LDLQ},H)\stackrel{{\eqref{eq:LDLQ} }}{{=}}\tfrac{m}{4}\operatorname{tr}\left(D\right)\), again for \(\mathcal{Q}\) as either nearest or stochastic rounding. \(B\) must be a unit triangular matrix since it is the product of unit triangular matrices. Therefore \(\operatorname{tr}\left(BDB^{T}\right)\) is minimized when \(B=I\), and

\[\mathcal{L}_{\mathrm{worst}}(\textsf{LDLQ},H)\leq\mathcal{L}_{\mathrm{worst} }(\mathcal{A},H).\]

Next, consider the average loss, \(\mathcal{L}_{\mathrm{avg}}\), where \(W\sim Unif[0,1]^{m\times n}\). For \(\mathcal{Q}\) as nearest rounding, the entries of the quantization error \(\eta\) are \(Unif[-\tfrac{1}{2},\tfrac{1}{2}]\), because each entry is independent and uniformly distributed. It follows that for any entry of \(\eta\), \(\mathbf{E}\left[\eta_{ij}^{2}\right]=\int_{-1/2}^{1/2}x^{2}dx=\tfrac{1}{12}\). Therefore, \(\mathcal{L}_{\mathrm{avg}}(\mathcal{A},H)\stackrel{{\eqref{eq:LDLQ} }}{{=}}\mathbf{E}_{W\sim Unif[0,1]^{m\times n}}\left[\operatorname{tr}\left( \eta BDB^{T}\eta^{T}\right)\right]=\tfrac{m}{12}\operatorname{tr}\left(BDB^{T }\right)\). For \(\mathcal{Q}\) as stochastic rounding, the entries of the quantization error \(\eta\) are \(Unif[-1,1]\). It follows that for any entry of \(\eta\), \(\mathbf{E}\left[\eta_{ij}^{2}\right]=\int_{0}^{1}x(1-x)dx=\tfrac{1}{6}\). Note that for stochastic rounding, the quantization error will be \(x\) with probability \((1-|x|)\). Therefore, \(\mathcal{L}_{\mathrm{avg}}(\mathcal{A},H)=\tfrac{m}{6}\operatorname{tr}\left(BDB ^{T}\right)\). Based on these same calculations of \(\mathbf{E}\left[\eta_{ij}^{2}\right]\)

[MISSING_PAGE_EMPTY:24]

Proof.: By continuity of \(\operatorname{tr}\left(D\right)\) and \(\operatorname{tr}\left(H^{1/2}\right)\), it suffices to prove the lemma for positive definite \(H\). First, the closure of positive definite symmetric matrices is the set of positive semi-definite symmetric matrices. Second, consider the set of \(H\) that are positive definite and satisfy \(\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right)^{2}-\operatorname{tr} \left(D\right)\geq 0\), i.e. are non-negative. The closure of this set (i.e. \(H\succeq 0\)) must also satisfy that the inequality is non-negative.

Let \(H=Q\Lambda Q^{T}\) be the eigendecomposition of \(H\). First, observe that by incoherence,

\[e_{k}^{T}H^{1/2}e_{k}=\sum_{i=1}^{n}\lambda_{i}^{1/2}(e_{i}^{T}Qe_{k})^{2}\leq \frac{\mu^{2}}{n}\sum_{i=1}^{n}\lambda_{i}^{1/2}=\frac{\mu^{2}}{n}\operatorname {tr}\left(H^{1/2}\right).\]

Set

\[\alpha=\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right),\]

and consider the recurrence from Lemma 8 with

\[a_{k}=\frac{H^{1/2}e_{k}}{\alpha}\]

Then

\[\Sigma_{k+1}=\left(I-\alpha^{-1}e_{k}e_{k}^{T}H^{1/2}\right)\Sigma_{k}\left(I -\alpha^{-1}H^{1/2}e_{k}e_{k}^{T}\right)+e_{k}e_{k}^{T}.\]

Suppose by way of induction that for some scalar the covariance \(\Sigma_{k}\preceq\alpha H^{-1/2}\). For the base case, this obviously holds since \(\Sigma_{0}=0\). At step \(k\),

\[\Sigma_{k+1} \preceq\left(I-\alpha^{-1}e_{k}e_{k}^{T}H^{1/2}\right)\alpha H^{- 1/2}\left(I-\alpha^{-1}H^{1/2}e_{k}e_{k}^{T}\right)+e_{k}e_{k}^{T}\] \[=\alpha H^{-1/2}-2e_{k}e_{k}^{T}+\alpha^{-1}e_{k}e_{k}^{T}H^{1/2} e_{k}e_{k}^{T}+e_{k}e_{k}^{T}\] \[\preceq\alpha H^{-1/2}.\]

Note that with this assignment,

\[a_{k}^{T}\Sigma_{k}a_{k}\leq(\alpha^{-1}e_{k}^{T}H^{1/2})(\alpha H^{-1/2})( \alpha^{-1}H^{1/2}e_{k})=\alpha^{-1}e_{k}^{T}H^{1/2}e_{k}\leq 1.\]

So, by induction it follows that

\[\Sigma_{n}\preceq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right)\cdot H ^{-1/2},\]

and so

\[\operatorname{tr}\left(H\Sigma_{n}\right)\leq\frac{\mu^{2}}{n}\operatorname{ tr}\left(H^{1/2}\right)\operatorname{tr}\left(H\cdot H^{-1/2}\right)=\frac{\mu^{2}}{n} \operatorname{tr}\left(H^{1/2}\right)^{2}.\]

But from Lemma 8, we know that \(\operatorname{tr}\left(D\right)\) is the global minimum of \(\operatorname{tr}\left(H\Sigma_{n}\right)\) for any assignment of \(a_{k}\). This immediately gives us the desired result. 

**Lemma 3**.: _Let \(H\) be symmetric positive definite. In the worst case stochastic rounding achieves \(\mathcal{L}_{\operatorname{worst}}(\mathsf{Stoch},H)=(m/4)\operatorname{tr} \left(H\right)\). In the average case nearest and stochastic rounding achieve \(\mathcal{L}_{\operatorname{avg}}(\{\mathsf{Near},\mathsf{Stoch}\},H)=(m/c) \operatorname{tr}\left(H\right)\), where \(c=12\) for nearest, and \(c=6\) for stochastic._

Proof.: For nearest and stochastic rounding, set the linear feedback \(U\) in Eq. (2) to be zero. Stochastic rounding achieves worst-case loss,

\[\mathcal{L}_{\operatorname{worst}}(\mathsf{Stoch},H)\overset{(3)}{=}\sup_{W \in\mathbb{R}^{m\times n}}\mathbf{E}\left[\operatorname{tr}\left(\eta H\eta^{ T}\right)\right]=\frac{m}{4}\operatorname{tr}\left(H\right).\] (11)

For the average-case proxy loss, recall the computations of \(\mathbf{E}\left[\eta_{ij}^{2}\right]\) from the proof of Theorem 1.

\[\mathcal{L}_{\operatorname{avg}}(\mathsf{Near},H)\overset{(3)}{=} \mathbf{E}_{W\sim Unif[0,1]^{m\times n}}\left[\operatorname{tr}\left(\eta H \eta^{T}\right)\right] =\frac{m}{12}\operatorname{tr}\left(H\right)\] (12) \[\mathcal{L}_{\operatorname{avg}}(\mathsf{Stoch},H)\overset{(3)}{=} \mathbf{E}_{W\sim Unif[0,1]^{m\times n}}\left[\operatorname{tr}\left( \eta H\eta^{T}\right)\right] =\frac{m}{6}\operatorname{tr}\left(H\right).\] (13)Without incoherence: no improvement with a spectral bound

**Theorem 4**.: _Consider all \(\tilde{H}\) with the same spectrum as \(H\). For any positive semi-definite \(H\), the following holds. On the worst-case loss \(\mathsf{LDLQ}\) achieves the same error as stochastic rounding,_

\[\sup_{\tilde{H}s.t.\,\operatorname{eig}(\tilde{H})=\operatorname{eig}(H)} \mathcal{L}_{\mathrm{worst}}(\mathsf{LDLQ},\tilde{H})=\mathcal{L}_{\mathrm{ worst}}(\mathsf{Stoch},H)=\frac{m}{4}\operatorname{tr}\left(H\right).\]

_On the average-case loss \(\mathsf{LDLQ}\) achieves the same error as the corresponding rounding routine. Let \(\mathcal{B}=\{\mathsf{Near},\mathsf{Stoch}\}\) and \(c=12\) for nearest, \(c=6\) for stochastic._

Proof.: See Lemma 3 for calculations on the proxy loss for nearest and stochastic rounding.

For LDLQ, we will derive lower and upper bounds on \(\sup_{\tilde{H}s.t.\,\operatorname{eig}(\tilde{H})=\operatorname{eig}(H)} \mathcal{L}_{\mathrm{worst}}(\mathsf{LDLQ},\tilde{H})\) and \(\sup_{\tilde{H}s.t.\,\operatorname{eig}(\tilde{H})=\operatorname{eig}(H)} \mathcal{L}_{\mathrm{avg}}(\mathsf{LDLQ},\tilde{H})\), and show they are equal. To construct a lower bound, consider \(\tilde{H}=I\Lambda I\) where \(\Lambda\) are the eigenvalues of \(H\). This decomposition is also the LDL decomposition of \(\tilde{H}\), rewritten as \(\tilde{H}=(U+I)D(U+I)^{-1}\). It follows that \(\operatorname{tr}\left(D\right)=\operatorname{tr}\left(\tilde{H}\right)\) for this \(\tilde{H}\). Combine this result with the worst and average-case losses calculated in the proof of Theorem 1. For the worst-case loss from the proof of Theorem 1, \(\geq\frac{m}{4}\operatorname{tr}\left(H\right)\). The lower bound for the average-case loss is \(\geq\frac{m}{12}\operatorname{tr}\left(H\right)\) for \(\mathcal{Q}\) as nearest, and \(\geq\frac{m}{6}\operatorname{tr}\left(H\right)\) for \(\mathcal{Q}\) as stochastic. Now upper bounds are derived using the preceding calculations in Eq. (11)-(13), and using the worst and average-case optimality of LDLQ proven in Theorem 1. The lower and upper bounds are tight, proving our result. 

Appendix E Proofs for Section 4 (Quantization With Incoherence Processing: Incoherence Processing Step )

**Subsection 4.1 (Incoherence via Efficient Orthogonal Multiplication)**

**Lemma 9** (Theorem 2.4 from Lalley [2] ).: _There exist constants \(C\) and \(A\) independent of \(n\) such that for any function \(F\) from the unit sphere in \(n\) dimensions to \(\mathbb{R}\) that is 1-Lipschitz relative to the Riemannian metric on the sphere,_

\[\mathbf{P}_{x\sim\mathcal{S}_{n}}\left(F(x)-\mathbf{E}_{x\sim\mathcal{S}_{n}} [F(x)]\geq t\right)\leq C\exp\left(-\frac{nt^{2}}{A}\right)\]

**Lemma 10**.: _Let \(B\in\mathbb{R}^{m\times n}\) be a matrix, and let \(x\) be a random vector uniformly distributed on the unit sphere in \(\mathbb{R}^{n}\). Then there exist global constants \(A>0\) and \(C>0\) independent of \(m\) and \(n\) such that_

\[\mathbf{P}\left(\left\|Bx\right\|^{2}\geq\frac{A\left\|B\right\|_{F}^{2}}{n} \log\left(\frac{C}{\delta}\right)\right)\leq\delta,\]

Proof.: Let

\[F(x)=\frac{\left\|Bx\right\|}{\left\|B\right\|_{F}}.\]

Observe that

\[\nabla F(x)=\frac{B^{T}Bx}{\left\|Bx\right\|\cdot\left\|B\right\|_{F}},\]

and so

\[\left\|\nabla F(x)\right\|\leq 1.\]

Also observe that for \(x\) drawn uniformly from the sphere in \(n\) dimensions,

\[\mathbf{E}\left[F(x)\right]\leq\sqrt{\mathbf{E}\left[F(x)^{2}\right]}=\frac{ 1}{\left\|B\right\|_{F}}\cdot\sqrt{\mathbf{E}\left[\left\|Bx\right\|^{2} \right]}=\frac{1}{\sqrt{n}}.\]So, applying Lemma 9,

\[\mathbf{P}\left(\frac{\left\|Bx\right\|}{\left\|B\right\|_{F}}-\frac{1}{\sqrt{n}} \geq t\right)\leq C\exp\left(-\frac{nt^{2}}{A}\right).\]

If we let \(\delta\) be

\[\delta=C\exp\left(-\frac{nt^{2}}{A}\right),\]

then

\[\frac{A}{n}\log\left(\frac{C}{\delta}\right)=t^{2}\]

Trivially, then, for some modified global constants \(A^{\prime}\) and \(C^{\prime}\),

\[\frac{A^{\prime}}{n}\log\left(\frac{C^{\prime}}{\delta}\right)=\left(t+\frac{ 1}{\sqrt{n}}\right)^{2}\]

This means that

\[\mathbf{P}\left(\frac{\left\|Bx\right\|^{2}}{\left\|B\right\|_{F}^{2}}\geq \frac{A^{\prime}}{n}\log\left(\frac{C^{\prime}}{\delta}\right)\right)\leq\delta,\]

i.e.

\[\mathbf{P}\left(\left\|Bx\right\|^{2}\geq\frac{A^{\prime}\left\|B\right\|_{F}^ {2}}{n}\log\left(\frac{C^{\prime}}{\delta}\right)\right)\leq\delta,\]

This is what we wanted to prove. 

**Lemma 5**.: _Let \(H\) be a positive semi-definite matrix on \(\mathbb{R}^{n\times n}\) and \(W\) a matrix on \(\mathbb{R}^{m\times n}\), and suppose that \(m=p_{1}\cdot p_{2}\cdots p_{k}\) and \(n=q_{1}\cdot q_{2}\cdots q_{k}\). Let \(U_{1},U_{2},\ldots,U_{k},V_{1},V_{2},\ldots,V_{k}\) be independent random orthogonal matrices on \(\mathbb{R}^{p_{i}\times p_{i}}\) and \(\mathbb{R}^{q_{i}\times q_{i}}\) respectively. Set \(U\) as the Kronecker product \(U=U_{1}\otimes U_{2}\otimes\cdots\otimes U_{k}\) and \(V\) as \(V=V_{1}\otimes V_{2}\otimes\cdots\otimes V_{k}\) Then \(VHV^{T}\) is \(\mu_{H}\)-incoherent with probability at least \(1-\delta\), and \(UWW^{T}\) is \(\mu_{W}\)-incoherent with probability at least \(1-\delta\), where_

\[\mu_{H}=A^{k/2}\log\left(\frac{Ckn^{2}}{\delta}\right)^{k/2}=\tilde{\mathcal{ O}}\left(1\right)\ \ \text{and}\ \ \mu_{W}=A^{k}\log\left(\frac{2Ckmn}{\delta}\right)^{k}=\tilde{\mathcal{O}} \left(1\right)\]

_for some global constants \(A\) and \(C\) independent of \(n\) and \(k\)._

Proof.: First we will prove what we want to prove about \(H\); then we will prove what we want to prove about \(W\). Let \(Q\) be a matrix of eigenvectors of \(H\). Observe that since \(Q\) is an orthogonal matrix (by the spectral theorem, because \(H\) is symmetric), \(Qe_{j}\) is a unit vector, i.e. \(\left\|Qe_{j}\right\|=1\). Call \(Qe_{j}=y\). Also observe that

\[e_{i}^{T}(U_{1}\otimes U_{2}\otimes\cdots\otimes U_{k})=\left((e_{i_{1}}^{T}U_ {1})\otimes(e_{i_{2}}^{T}U_{2})\otimes\cdots\otimes(e_{i_{k}}^{T}U_{k})\right)\]

for some indices \(i_{j}\). Call \(e_{i_{j}}^{T}U_{j}=x_{j}^{T}\), and observe that the \(x_{j}\) are all independent unit random vectors. So,

\[\left((U_{1}\otimes U_{2}\otimes\cdots\otimes U_{k})Q\right)_{ij}=(x_{1} \otimes x_{2}\otimes\cdots\otimes x_{k})^{T}y\]

for random unit vectors \(x_{1},\ldots,x_{k}\) and unit vector \(y\). We can easily bound this with \(k\) applications of Lemma 10 and a union bound, yielding

\[\mathbf{P}\left(\left((x_{1}\otimes x_{2}\otimes\cdots\otimes x_{k})^{T}y \right)^{2}\geq\frac{A^{k}}{n}\log\left(\frac{C}{\delta}\right)^{k}\right)\leq k\delta,\]

Setting \(\delta\mapsto\frac{\delta}{kn^{2}}\) yields

\[\mathbf{P}\left(\left((x_{1}\otimes x_{2}\otimes\cdots\otimes x_{k})^{T}y \right)^{2}\geq\frac{A^{k}}{n}\log\left(\frac{Ckn^{2}}{\delta}\right)^{k} \right)\leq\frac{\delta}{n^{2}},\]

and unioning over all the entries of the large orthogonal matrix,

\[\mathbf{P}\left(\max_{i,j}\ \left|\left((U_{1}\otimes U_{2}\otimes\cdots\otimes U_{k}) Q\right)_{ij}\right|\geq\sqrt{\frac{A^{k}}{n}\log\left(\frac{Ckn^{2}}{ \delta}\right)^{k}}\right)\leq\delta.\]Next, for \(W\), observe that if we flatten \(W\), then \(W/\left\|W\right\|_{F}\) is a unit vector. Then any entry of the resulting matrix can be written as

\[(x_{1}\otimes x_{2}\otimes\cdots\otimes x_{k})^{T}W(y_{1}\otimes y_{2}\otimes \cdots\otimes y_{k})\]

where \(x_{1},\ldots,x_{k}\) and \(y_{1},\ldots,y_{k}\) are \(k\) independent random unit vectors. We can easily bound this with \(2k\) applications of Lemma 10 and a union bound, yielding

\[\mathbf{P}\left(\left((x_{1}\otimes x_{2}\otimes\cdots\otimes x_{k})^{T}W(y_{ 1}\otimes y_{2}\otimes\cdots\otimes y_{k})\right)^{2}\geq\frac{A^{2k}}{mn} \log\left(\frac{C}{\delta}\right)^{2k}\right)\leq 2k\delta,\]

Setting \(\delta\mapsto\frac{\delta}{2kmn}\) yields

\[\mathbf{P}\left(\left((x_{1}\otimes x_{2}\otimes\cdots\otimes x_{k})^{T}W(y_ {1}\otimes x_{2}\otimes\cdots\otimes y_{k})\right)^{2}\geq\frac{A^{2k}}{mn} \log\left(\frac{2Ckmn}{\delta}\right)^{2k}\right)\leq\frac{\delta}{mn},\]

and unioning over all the \(mn\) entries of the large orthogonal matrix,

\[\mathbf{P}\left(\max_{i,j}\ \left|e_{i}^{T}(U_{1}\otimes U_{2}\otimes\ldots U_{k} )W(V_{1}\otimes V_{2}\otimes\cdots\otimes V_{k})e_{j}\right|\geq\sqrt{\frac{A ^{2k}}{mn}\log\left(\frac{2Ckmn}{\delta}\right)^{2k}}\right)\leq\delta.\]

This is what we wanted to show.

## Appendix F Proofs for Section 5 (Extensions and Further Analyses)

**Subsection 5.1** (OPTQ is a Special Case of LDLQ):

**Theorem 6**.: _OPTQ [1] falls within the class of adaptive rounding procedures with linear feedback as described by Eq. (2), and is equivalent to LDLQ in Section 3._

Proof.: OPTQ works in the following way. After OPTQ has quantized the first \(t-1\) components of the row vector \(w\), it minimizes the proxy loss over the remaining \(n-t+1\) elements, keeping the first \(t-1\) elements fixed. It then quantizes the \(t\)th element using nearest rounding to the grid and clamping. It then proceeds to the next column. If we let \(\Delta=\hat{w}-w\), this proxy loss that it minimizes can be written in block form as

\[\ell=\Delta_{1:(t-1)}H_{1:(t-1),1:(t-1)}\Delta_{1:(t-1)}^{T}+2\Delta_{1:(t-1) }H_{1:(t-1),t:n}+\Delta_{t:n}H_{t:n,t:n}\Delta_{t:n}^{T}\]

and its minimum over \(\Delta_{t:n}\) will occur when

\[0=\Delta_{1:(t-1)}H_{1:(t-1),t:n}+\Delta_{t:n}H_{t:n,t:n},\]

i.e.

\[\Delta_{t:n}=-\Delta_{1:(t-1)}H_{1:(t-1),t:n}\left(H_{t:n,t:n}\right)^{-1}.\]

Now, suppose that \(H=\tilde{U}D\tilde{U}^{T}\) is the LDL decomposition of \(H\), where \(\tilde{U}\) is unit upper triangular and \(D\) is diagonal. Since \(\tilde{U}\) is upper triangular,

\[H_{t:n,t:n}=\tilde{U}_{t:n,t:n}D_{t:n,t:n}\tilde{U}_{t:n,t:n}^{T}.\]

Similarly,

\[H_{1:(t-1),t:n}=\tilde{U}_{1:(t-1),t:n}D_{t:n,t:n}\tilde{U}_{t:n,t:n}^{T}.\]

This means that

\[\Delta_{t:n}=-\Delta_{1:(t-1)}\tilde{U}_{1:(t-1),t:n}\left(\tilde{U}_{t:n,t:n }\right)^{-1}.\]

Now, the only part of the value of \(\Delta_{t:n}\) which matters is the first entry, since this is the one that's going to be used to make the next quantization decision. But since \(\tilde{U}_{t:n,t:n}\) is unit upper triangular and so is its inverse, \(\left(\tilde{U}_{t:n,t:n}\right)^{-1}e_{t}=e_{t}\), and so

\[\Delta_{t}=\Delta_{t:n}e_{1}=-\Delta_{1:(t-1)}\tilde{U}_{1:(t-1),t:n}e_{t}=- \Delta_{1:(t-1)}\tilde{U}_{1:(t-1),t}=-\Delta(\tilde{U}-I)e_{t}.\]

Finally, we quantize the \(t\)-th weight as

\[\hat{w}_{t}=\mathcal{Q}(w_{t}-(\hat{W}-W)(\tilde{U}-I)e_{t}).\]

This update is equivalent to our adaptive rounding with linear feedback procedure in Eq. (2), with \(U\) assigned from the LDL decomposition of \(H\)

**Subsection 5.2** (A Bound for Rounding to a Finite Grid): Algorithm 5 presents a quantization procedure which theoretically address OPTQ's clamping issue, by incorporating a restriction of \(|\hat{W}_{ij}-W_{ij}|\) into objective (7). Note that for simplicity, here we present the explicit case where only two factors are used in each Kronecker product of orthogonal matrices; however, the proof should generalize to any number of factors.

```
0:\(W\in\mathbb{R}^{m\times n}\), \(H\in\mathbb{R}^{n\times n}\), \(c>0\), \(\rho>0\)
0: factorization \(m=p_{1}p_{2}\), \(n=p_{3}p_{4}\)
0: draw\(U_{1}\in\mathbb{R}^{p_{1}\times p_{1}}\) uniformly from the set of orthogonal matrices using seed \(\mathsf{seed}(U_{1})\) draw\(U_{2}\in\mathbb{R}^{p_{2}\times p_{2}}\) uniformly from the set of orthogonal matrices using seed \(\mathsf{seed}(U_{2})\) draw\(U_{3}\in\mathbb{R}^{p_{3}\times p_{3}}\) uniformly from the set of orthogonal matrices using seed \(\mathsf{seed}(U_{3})\) draw\(U_{4}\in\mathbb{R}^{p_{4}\times p_{4}}\) uniformly from the set of orthogonal matrices using seed \(\mathsf{seed}(U_{4})\) \(W\leftarrow(U_{1}\otimes U_{2})W(U_{3}\otimes U_{4})\) \(H\leftarrow(U_{3}^{T}\otimes U_{4}^{T})H(U_{3}\otimes U_{4})\) \(W\leftarrow\frac{2^{b}-1}{2}\left(\frac{W}{\rho}+1\right)\) elementwise \(W\leftarrow\mathrm{clamp}(W,\min=0,\max=2^{b}-1))\) elementwise  use ADMM or some other solver to solve  minimize: \(\mathrm{tr}\left(HL^{T}L\right)\)  over: \(L\) unit upper triangular  subject to: \(e_{i}^{T}L^{T}Le_{i}\leq 1+c,\ \forall i\in\{1,\ldots,n\}\).  note that when \(c=\infty\), \(L^{-1}\) is the factor from the LDL decomposition of \(H\) \(\hat{U}\gets L^{-1}-I\) for\(k\in\{1,\ldots,n\}\)do\(\hat{W}_{k}\leftarrow\mathrm{clamp}(\mathcal{Q}(W_{k}+(W-\hat{W})\hat{U}_{k}),0,2^{b}-1)\)\(\triangleright\) round with LF \(\hat{W}\leftarrow\rho\left(\frac{2\hat{W}}{2^{b}-1}-1\right)\) \(\hat{W}\leftarrow(U_{1}^{T}\otimes U_{2}^{T})\hat{W}(U_{3}^{T}\otimes U_{4}^{T})\) return\(\hat{W}\) encoded as a tuple of the integer rounded values, the scale factor \(\rho\), and the seeds ```

**Algorithm 5** "Fixed" Rounding via a Convex Program

**Lemma 11**.: _Suppose that for positive definite \(\mu\)-incoherent matrix \(H\in\mathbb{R}^{n\times n}\) and scalar \(c>0\), \(L\) is the solution to the optimization problem_

\[\text{minimize:}\ \ \mathrm{tr}\left(HL^{T}L\right)\] _over:_ \[L\] _unit upper triangular_ _subject to:_ \[e_{i}^{T}L^{T}Le_{i}\leq 1+c,\ \forall i\in\{1,\ldots,n\}.\]

_Then the solution satisfies_

\[\mathrm{tr}\left(HL^{T}L\right)=\frac{\mu^{2}}{n\cdot\min(1,c)}\,\mathrm{tr} \left(H^{1/2}\right)^{2}.\]

Proof.: Let \(\eta\in\mathbb{R}^{1\times n}\) be a random standard Gaussian variable as a row vector, let \(A\) be a matrix, and consider the recurrence relation over \(x_{t}\in\mathbb{R}^{1\times n}\) given by \(x_{0}=0\) and

\[x_{t}=x_{t-1}-x_{t-1}Ae_{i}e_{i}^{T}+\eta e_{i}e_{i}^{T}\]

We first note that since \(x_{t}\) is supported only on \(\{1,\ldots,t\}\), if \(M\) denotes the strictly upper triangular mask, this update step is equivalent to

\[x_{t}=x_{t-1}-x_{t-1}(A\odot M)e_{i}e_{i}^{T}+\eta e_{i}e_{i}^{T}.\]

From here, it's fairly easy to see by induction that

\[x_{n}=-x_{n}(A\odot M)+\eta,\]

and so

\[x_{n}(I+A\odot M)=\eta,\]\[x_{n}=\eta(I+A\odot M)^{-1}.\]

Now, since \(I+A\odot M\) is a unit upper triangular matrix, its inverse is also a unit upper triangular matrix. If we let \(L=(I+A\odot M)^{-1}\), then \(L\) is a unit upper triangular matrix and

\[\mathbf{E}\left[x_{n}^{T}x_{n}\right]=L^{T}L.\]

We are going to choose \(A\) such that \(L\) is a feasible solution to our optimization problem and has the desired objective. Next, let \(\Sigma_{t}=\mathbf{E}\left[x_{t}^{T}x_{t}\right]\), and observe that

\[\Sigma_{t}=\left(I-Ae_{i}e_{i}^{T}\right)^{T}\Sigma_{t-1}\left(I-Ae_{i}e_{i}^{ T}\right)+e_{i}e_{i}^{T}.\]

Let \(\alpha>0\) be some constant to be set later, and set \(A=\alpha H^{1/2}\). Suppose by way of induction that for some constant \(\beta>0\) to be set later, \(\Sigma_{t}\preceq\beta H^{-1/2}\). The base case clearly holds since \(\Sigma_{0}=0\). For the inductive step,

\[\Sigma_{t} \preceq\beta\left(I-\alpha H^{1/2}e_{i}e_{i}^{T}\right)^{T}H^{- 1/2}\left(I-\alpha H^{1/2}e_{i}e_{i}^{T}\right)+e_{i}e_{i}^{T}\] \[=\beta H^{-1/2}-2\alpha\beta e_{i}e_{i}^{T}+\alpha^{2}\beta e_{i} e_{i}^{T}H^{1/2}e_{i}e_{i}^{T}+e_{i}e_{i}^{T}.\]

This inductive step will hold if, letting \(h=\max_{i}e_{i}^{T}H^{1/2}e_{i}\),

\[2\alpha\beta\geq 1+\alpha^{2}\beta h\]

On the other hand,

\[e_{i}^{T}L^{T}Le_{i} =\mathbf{E}\left[(x_{n}e_{i})^{2}\right]\] \[=\mathbf{E}\left[\left(-x_{i-1}Ae_{i}+\eta e_{i}\right)^{2}\right]\] \[=\mathbf{E}\left[\left(-x_{i-1}Ae_{i}\right)^{2}\right]+1\] \[=e_{i}^{T}A^{T}\Sigma_{i-1}Ae_{i}+1\] \[=\alpha^{2}e_{i}^{T}H^{1/2}\Sigma_{i-1}H^{1/2}e_{i}+1\] \[\leq\alpha^{2}\beta e_{i}^{T}H^{1/2}H^{-1/2}H^{1/2}e_{i}+1\] \[\leq\alpha^{2}\beta e_{i}^{T}H^{1/2}e_{i}+1.\]

So the constraint of our optimization problem will be satisfied if

\[\alpha^{2}\beta h\leq c.\]

To satisfy these constraints, set \(\beta=\max(h,h/c)\) and \(\alpha=\beta^{-1}\). Then

\[2\max(h,h/c)^{-1}\cdot\max(h,h/c)\geq 1+\max(h,h/c)^{-2}\cdot\max(h,h/c)\cdot h,\]

and

\[\max(h,h/c)^{-2}\cdot\max(h,h/c)\cdot h\leq c.\]

Also, the objective will be bounded by

\[\operatorname{tr}\left(HL^{T}L\right)=\operatorname{tr}\left(H\Sigma_{n} \right)\leq\beta\operatorname{tr}\left(H^{1/2}\right)=\max(1,c^{-1})\cdot h \cdot\operatorname{tr}\left(H^{1/2}\right).\]

Now, applying incoherence to bound \(h\), where \(H=U\Lambda U^{T}\) is the eigendecomposition of \(H\),

\[e_{i}^{T}H^{1/2}e_{i}=\sum_{j=1}^{n}\lambda_{j}^{1/2}(e_{i}^{T}Ue_{j})^{2} \leq\sum_{j=1}^{n}\lambda_{j}^{1/2}\frac{\mu^{2}}{n}=\frac{\mu^{2}}{n} \operatorname{tr}\left(H^{1/2}\right).\]

So this yields a whole bound of

\[\operatorname{tr}\left(HL^{T}L\right)=\frac{\mu^{2}}{n\cdot\min(1,c)} \operatorname{tr}\left(H^{1/2}\right)^{2}.\]

This is what we wanted to show.

**Lemma 12**.: _Suppose that we quantize the row vector \(w\in\mathbb{R}^{1\times n}\) using \(L\) the solution to the optimization problem_

\[\text{minimize:}\ \ \operatorname{tr}\left(HL^{T}L\right)\] \[\text{over:}\ L\text{ unit upper triangular}\] \[\text{subject to:}\ e_{i}^{T}L^{T}Le_{i}\leq 1+c,\ \forall i\in\{1, \dots,n\}\]

_and_

\[\hat{w}=\mathcal{Q}_{\mathrm{stoch}}\left(w-(\hat{w}-w)(L^{-1}-I)\right),\]

_where \(\mathcal{Q}_{\mathrm{stoch}}\) denotes elementwise unbiased stochastic rounding. Then for any \(u\in\mathbb{R}^{n}\) and any \(\delta>0\)_

\[\mathbf{P}\left(\left|(\hat{w}-w)u\right|\geq\left\|Lu\right\|\sqrt{\frac{1}{2 }\log\left(\frac{2}{\delta}\right)}\right)\leq\delta.\]

_In particular,_

\[\mathbf{P}\left(\left|(\hat{w}-w)(L^{-1}-I)e_{i}\right|\geq\sqrt{\frac{c}{2} \log\left(\frac{2}{\delta}\right)}\right)\leq\delta.\]

Proof.: Let \(\eta\) be the error of stochastic rounding, and observe that each entry is, conditioned on earlier steps, zero mean and supported on two values that differ by \(1\). Also observe that

\[\hat{w}=\left(w-(\hat{w}-w)(L^{-1}-I)\right)+\eta,\]

and so

\[\hat{w}-w=\eta L\]

and

\[\mathbf{E}\left[\exp\left((\hat{w}-w)u\right)\right]=\mathbf{E}\left[\exp\left( \eta Lu\right)\right].\]

From a repeated application of Hoeffding's lemma, we get

\[\mathbf{E}\left[\exp\left((\hat{w}-w)u\right)\right]\leq\exp\left(\frac{1}{8} \left\|Lu\right\|^{2}\right).\]

Setting \(u\mapsto\gamma u\) for \(\gamma>0\),

\[\mathbf{E}\left[\exp\left(\gamma(\hat{w}-w)u\right)\right]\leq\exp\left(\frac{ \gamma^{2}}{8}\left\|Lu\right\|^{2}\right).\]

And by Markov's inequality,

\[\mathbf{P}\left(\exp\left(\gamma(\hat{w}-w)u\right)\geq\exp(\gamma R)\right) \leq\exp(-\gamma R)\exp\left(\frac{\gamma^{2}}{8}\left\|Lu\right\|^{2}\right),\]

i.e.

\[\mathbf{P}\left((\hat{w}-w)u\geq R\right)\leq\exp\left(-\gamma R+\frac{\gamma ^{2}}{8}\left\|Lu\right\|^{2}\right).\]

Minimizing the right side over \(\gamma\) yields \(\gamma=4R\left\|Lu\right\|^{-2}\) and

\[\mathbf{P}\left((\hat{w}-w)u\geq R\right)\leq\exp\left(-2R^{2}\left\|Lu \right\|^{-2}\right).\]

By a union bound,

\[\mathbf{P}\left(\left|(\hat{w}-w)u\right|\geq R\right)\leq 2\exp\left(-2R^{2} \left\|Lu\right\|^{-2}\right).\]

Now setting the right side equal to \(\delta\),

\[\mathbf{P}\left(\left|(\hat{w}-w)u\right|\geq\left\|Lu\right\|\sqrt{\frac{1}{ 2}\log\left(\frac{2}{\delta}\right)}\right)\leq\delta.\]

This is what we wanted to show. The second statement follows from the fact that

\[\left\|L(L^{-1}-I)e_{i}\right\|^{2}=\left\|e_{i}-Le_{i}\right\|^{2}=e_{i}^{T}e _{i}-e_{i}^{T}Le_{i}-e_{i}^{T}L^{T}e_{i}+e_{i}^{T}L^{T}Le_{i}\leq 1-1-1+(1+c)=c.\]

**Lemma 13**.: _Suppose that we quantize the row vector \(w\in\mathbb{R}^{1\times n}\) using \(L\) the solution to the optimization problem_

\[\text{minimize:}\ \ \operatorname{tr}\left(HL^{T}L\right)\] \[\text{over:}\ L\text{ unit upper triangular}\] \[\text{subject to:}\ e_{i}^{T}L^{T}Le_{i}\leq 1+c,\ \forall i\in\{1, \dots,n\}\]

_and_

\[\hat{w}=\mathcal{Q}_{\mathrm{stoch}}\left(w-(\hat{w}-w)(L^{-1}-I)\right),\]

_where \(\mathcal{Q}_{\mathrm{stoch}}\) denotes elementwise unbiased stochastic rounding. Suppose that for some integer \(b\), \(1\leq w_{ij}\leq 2^{b}-2\). Then if we set_

\[c=2\left(\log\left(\frac{4mn}{\delta}\right)\right)^{-1},\]

_then with probability at least \(1-\delta\), \(0\leq\hat{w}_{ij}\leq 2^{b}-1\) and_

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\leq\frac{\mu^{2}m}{ 4n}\operatorname{tr}\left(H^{1/2}\right)^{2}\left(\log\left(\frac{4mn}{\delta }\right)^{2}\right).\]

Proof.: First, from the previous lemmas, if \(Ue_{i}\) is the \(i\)th eigenvector of \(H\), with eigenvalue \(\lambda_{i}\) since

\[\mathbf{P}\left(\lambda_{i}(e_{j}^{T}(\hat{w}-w)Ue_{i})^{2}\geq\lambda_{i} \left\|LUe_{i}\right\|^{2}\cdot\frac{1}{2}\log\left(\frac{2}{\delta}\right) \right)\leq\delta.\]

By the union bound,

\[\mathbf{P}\left(\exists i,j,\ \lambda_{i}(e_{j}^{T}(\hat{w}-w)Ue_{i})^{2}\geq \lambda_{i}\left\|LUe_{i}\right\|^{2}\cdot\frac{1}{2}\log\left(\frac{2mn}{ \delta}\right)\right)\leq\delta.\]

And so

\[\mathbf{P}\left(\sum_{i,j}\lambda_{i}(e_{j}^{T}(\hat{w}-w)Ue_{i})^{2}\geq\sum_ {i,j}\lambda_{i}\left\|LUe_{i}\right\|^{2}\cdot\frac{1}{2}\log\left(\frac{2mn }{\delta}\right)\right)\leq\delta,\]

which simplifies to

\[\mathbf{P}\left(\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\geq m \operatorname{tr}\left(HL^{T}L\right)\cdot\frac{1}{2}\log\left(\frac{2mn}{ \delta}\right)\right)\leq\delta.\]

Now applying the other lemma,

\[\mathbf{P}\left(\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\geq \frac{\mu^{2}m}{2n\cdot\min(1,c)}\operatorname{tr}\left(H^{1/2}\right)^{2} \log\left(\frac{2mn}{\delta}\right)\right)\leq\delta.\]

And substituting \(\delta\mapsto\delta/2\),

\[\mathbf{P}\left(\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\geq \frac{\mu^{2}m}{2n\cdot\min(1,c)}\operatorname{tr}\left(H^{1/2}\right)^{2} \log\left(\frac{4mn}{\delta}\right)\right)\leq\frac{\delta}{2}.\]

On the other hand, again by a union bound from the previous lemma,

\[\mathbf{P}\left(\exists i,j,\ \left|e_{j}^{T}(\hat{w}-w)(L^{-1}-I)e_{i} \right|\geq\sqrt{\frac{c}{2}\log\left(\frac{4mn}{\delta}\right)}\right)\leq \frac{\delta}{2}.\]

Setting

\[c=2\left(\log\left(\frac{4mn}{\delta}\right)\right)^{-1}\]

yields

\[\mathbf{P}\left(\exists i,j,\ \left|e_{j}^{T}(\hat{w}-w)(L^{-1}-I)e_{i} \right|\geq 1\right)\leq\frac{\delta}{2}.\]And so by another union bound, the probability that

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\leq\frac{\mu^{2}m}{4n} \operatorname{tr}\left(H^{1/2}\right)^{2}\left(\log\left(\frac{4mn}{\delta} \right)\right)^{2}\]

and

\[\max_{i,j}\;\left|e_{j}^{T}(\hat{w}-w)(L^{-1}-I)e_{i}\right|\leq 1\]

is no less than \(1-\delta\). It's clear that if this second inequality holds, the value we pass in to the stochastic quantizer will be in range, and thus so will the output. This proves what we want. 

**Theorem 14**.: _Suppose that we are given an input matrix \(w\) with bounded maximum entry magnitude \(\left\|w\right\|_{\infty}\) and we want to quantize it using \(b\) bits. Suppose that we first re-scale the entries of \(w\) by mapping_

\[w_{ij}\mapsto\frac{2^{b}-3}{2}\left(\frac{w_{ij}}{\left\|w\right\|_{\infty}}+ 1\right)+1;\]

_this guarantees that \(1\leq w_{ij}\leq 2^{b}-2\). Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling. Then then with probability at least \(1-\delta\), all the quantized weights will be in range (no overflow or need for clipping) and_

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\leq\frac{\mu^{2}m}{ n(2^{b}-3)^{2}}\operatorname{tr}\left(H^{1/2}\right)^{2}\left\|w\right\|_{ \infty}^{2}\left(\log\left(\frac{4mn}{\delta}\right)^{2}\right).\]

Proof.: This is a straightforward consequence of the previous lemma. 

**Theorem 15**.: _Suppose that we are given an input matrix \(w\) with bounded \(\left\|w\right\|_{F}\) and we want to quantize it using \(b\) bits. Suppose that we first multiply by two-factor orthogonal matrices, and then we re-scale the entries of \(w\) by mapping_

\[w_{ij}\mapsto\frac{2^{b}-3}{2}\left(\frac{w_{ij}}{\left\|w\right\|_{F}\sqrt{ \frac{A^{2}}{mn}\log\left(\frac{2Cmn}{\delta}\right)^{2}}}+1\right)+1;\]

_this guarantees that \(1\leq w_{ij}\leq 2^{b}-2\). Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling and multiplication. Then then with probability at least \(1-\delta\), all the quantized weights will be in range (no overflow or need for clipping) and_

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right) \leq\frac{A^{4}}{n^{2}(2^{b}-3)^{2}}\operatorname{tr}\left(H^{1/ 2}\right)^{2}\left\|w\right\|_{F}^{2}\left(\log\left(\frac{12Cmn^{2}}{\delta }\right)\right)^{6}\] \[=\tilde{\mathcal{O}}\left(\frac{1}{n^{2}4^{b}}\operatorname{tr} \left(H^{1/2}\right)^{2}\left\|w\right\|_{F}^{2}\right).\]

Proof.: It is a straightforward consequence of Lemma 5, that unioning over the three bounds on the infinity norm of \(w\), the incoherence of \(H\), and the stochastic rounding, with probability at least \(1-3\delta\),

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right) \leq\frac{m}{n(2^{b}-3)^{2}}\operatorname{tr}\left(H^{1/2}\right) ^{2}\left\|w\right\|_{F}^{2}\left(\log\left(\frac{4mn}{\delta}\right)^{2}\right)\] \[\qquad\qquad\cdot A^{2}\log\left(\frac{2Cn^{2}}{\delta}\right)^{2 }\cdot\frac{A^{2}}{mn}\log\left(\frac{2Cn}{\delta}\right)^{2}.\]

Substituting \(\delta\mapsto\delta/3\),

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right) \leq\frac{1}{n(2^{b}-3)^{2}}\operatorname{tr}\left(H^{1/2} \right)^{2}\left\|w\right\|_{F}^{2}\left(\log\left(\frac{12mn}{\delta}\right)^ {2}\right)\] \[\qquad\qquad\qquad\cdot A^{2}\log\left(\frac{6Cn^{2}}{\delta} \right)^{2}\cdot\frac{A^{2}}{n}\log\left(\frac{6Cn}{\delta}\right)^{2}.\]And this right side is clearly less than

\[\operatorname{tr}\left((\hat{w}-w)H(\hat{w}-w)^{T}\right)\leq\frac{A^{4}}{n^{2}(2 ^{b}-3)^{2}}\operatorname{tr}\left(H^{1/2}\right)^{2}\left\|w\right\|_{F}^{2} \left(\log\left(\frac{12Cmn^{2}}{\delta}\right)\right)^{6}.\]

This is what we wanted to show. 

**Theorem 7**.: _Suppose that we run Algorithm 5 (Supplement) to quantize a matrix \(W\in\mathbb{R}^{m\times n}\) by solving the objective (7). Then there exists an assignment of the algorithm's hyperparameters \(c\) and \(\rho\) such that with probability at least \(1-\delta\), all the quantized weights will be in range (no overflow or need for clipping) and_

\[\operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right)=\tilde{\mathcal{O}} \left(\frac{1}{n^{2}4^{b}}\operatorname{tr}\left(H^{1/2}\right)^{2}\left\|W \right\|_{F}^{2}\right).\]

Proof.: This follows directly from the previous theorem, which says explicitly what the hyperparameter assignments should be. 

## References for the Appendix

* [1] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In _International Conference on Learning Representations_, 2023.
* [2] Steve Lalley. Lecture notes on measure-theoretic probability 2. http://galton.uchicago.edu/~lalley/Courses/383/Concentration.pdf, 2018.
* [3] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In _International Conference on Machine Learning_, pages 7197-7206. PMLR, 2020.