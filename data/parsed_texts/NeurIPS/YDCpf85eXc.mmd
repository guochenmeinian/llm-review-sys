# Finding Counterfactually Optimal Action Sequences

in Continuous State Spaces

 Stratis Tsirtsis

Max Planck Institute for Software Systems

Kaiserslautern, Germany

stsirtsis@mpi-sws.org

&Manuel Gomez-Rodriguez

Max Planck Institute for Software Systems

Kaiserslautern, Germany

manuelgr@mpi-sws.org

###### Abstract

Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to _retrospectively_ analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A\({}^{*}\) algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.

## 1 Introduction

Had the chess player moved the king one round later, would they have avoided losing the game? Had the physician administered antibiotics one day earlier, would the patient have recovered? The process of mentally simulating alternative worlds where events of the past play out differently than they did in reality is known as counterfactual reasoning [1]. Thoughts of this type are a common by-product of human decisions and they are tightly connected to the way we attribute causality and responsibility to events and others' actions [2]. The last decade has seen a rapid development of reinforcement learning agents, presenting (close to) human-level performance in a variety of sequential decision making tasks, such as gaming [3; 4], autonomous driving [5] and clinical decision support [6; 7]. In conjunction with the substantial progress made in the field of causal inference [8; 9], this has led to a growing interest in machine learning methods that employ elements of counterfactual reasoning to improve or to retrospectively analyze decisions in sequential settings [10; 11; 12; 13; 14; 15].

In the context of reinforcement learning, sequential decision making is typically modeled using Markov Decision Processes (MDPs) [16]. Here, we consider MDPs with a finite horizon where each episode (_i.e._, each sequence of decisions) consists of a finite number of time steps. As an example, consider a clinician treating a patient in an intensive care unit (ICU). At each time step, the clinician observes the current state of the environment (_e.g._, the patient's vital signs) and they choose among a set of potential actions (_e.g._, standardized dosages of a drug). Consequently, the chosen action causes the environment to transition (stochastically) into a new state, and the clinician earns a reward (_e.g._,satisfaction inversely proportional to the patient's severity). The process repeats until the horizon is met and the goal of the clinician is to maximize the total reward.

In this work, our goal is to aid the retrospective analysis of individual episodes as the example above. For each episode, we aim to find an action sequence that differs slightly from the one taken in reality but, under the circumstances of that particular episode, would have led to a higher counterfactual reward. In our example above, assume that the patient's condition does not improve after a certain period of time. A counterfactually optimal action sequence could highlight to the clinician a small set of time steps in the treatment process where, had they administered different drug dosages, the patient's severity would have been lower. In turn, a manual inspection of those time steps could provide insights to the clinician about potential ways to improve their treatment policy.

To infer how a particular episode would have evolved under a different action sequence than the one taken in reality, one needs to represent the stochastic state transitions of the environment using a structural causal model (SCM) [8; 17]. This has been a key aspect of a line of work at the intersection of counterfactual reasoning and reinforcement learning, which has focused on methods to either design better policies using offline data [10; 12] or to retrospectively analyze individual episodes [11; 13]. Therein, the work most closely related to ours is by Tsirtsis et al. [13], which introduces a method to compute counterfactually optimal action sequences in MDPs with discrete states and actions using a Gumbel-Max SCM to model the environment dynamics [11]. However, in many practical applications, such as in critical care, the state of the environment is inherently continuous in nature [18]. In our work, we aim to fill this gap by designing a method to compute counterfactually optimal action sequences in MDPs with continuous states and discrete actions. Refer to Appendix A for a discussion of further related work and to Pearl [8] for an overview of the broad field of causality.

**Our contributions.** We start by formally characterizing sequential decision making processes with continuous states and discrete actions using finite horizon MDPs and a general class of _bijective_ SCMs [19]. Notably, this class of SCMs includes multiple models introduced in the causal discovery literature [20; 21; 22; 23; 24; 25]. Building on this characterization, we make the following contributions:

1. We formalize the problem of finding a counterfactually optimal action sequence for a particular episode in environments with continuous states under the constraint that it differs from the observed action sequence in at most \(k\) actions.
2. We show that the above problem is NP-hard using a novel reduction from the classic partition problem [26]. This is in contrast with the computational complexity of the problem in environments with discrete states, which allows for polynomial time algorithms [13].
3. We develop a search method based on the \(A^{*}\) algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem upon termination.

Finally, we evaluate the performance and the qualitative insights of our method by performing a series of experiments using real patient data from critical care.1

Footnote 1: Our code is accessible at https://github.com/Networks-Learning/counterfactual-continuous-mdp.

## 2 A causal model of sequential decision making processes

At each time step \(t\in[T-1]:=\{0,1,\ldots,T-1\}\), where \(T\) is a time horizon, the decision making process is characterized by a \(D\)-dimensional vector state \(\bm{s}_{t}\in\mathcal{S}=\mathbb{R}^{D}\), an action \(a_{t}\in\mathcal{A}\), where \(\mathcal{A}\) is a finite set of \(N\) actions, and a reward \(R(\bm{s}_{t},a_{t})\in\mathbb{R}\) associated with each pair of states and actions. Moreover, given an episode of the decision making process, \(\tau=\{(\bm{s}_{t},a_{t})\}_{t=0}^{T-1}\), the process's outcome \(o(\tau)=\sum_{t}R(\bm{s}_{t},a_{t})\) is given by the sum of the rewards. In the remainder, we will denote the elements of a vector \(\bm{s}_{t}\) as \(s_{t,1},\ldots,s_{t,D}\).2

Footnote 2: Table 1 in Appendix B summarizes the notation used throughout the paper.

Further, we characterize the dynamics of the decision making process using the framework of structural causal models (SCMs). In general, an SCM is consisted of four parts: (i) a set of endogenous variables (ii) a set of exogenous (noise) variables (iii) a set of structural equations assigning values to the endogenous variables, and (iv) a set of prior distributions characterizing the exogenous variables [8]. In our setting, the endogenous variables of the SCM \(\mathcal{C}\) are the random variables representing the states \(\bm{S}_{0},\ldots,\bm{S}_{T-1}\) and the actions \(A_{0},\ldots,A_{T-1}\). The action \(A_{t}\) at time step \(t\) is chosen based on the observed state \(\bm{S}_{t}\) and is given by a structural (policy) equation

\[A_{t}:=g_{A}(\bm{S}_{t},\bm{Z}_{t}),\] (1)

where \(\bm{Z}_{t}\in\mathcal{Z}\) is a vector-valued noise variable, to allow some level of stochasticity in the choice of the action, and its prior distribution \(P^{\mathcal{C}}(\bm{Z}_{t})\) is characterized by a density function \(f^{\mathcal{C}}_{\bm{Z}_{t}}\). Similarly, the state \(\bm{S}_{t+1}\) in the next time step is given by a structural (transition) equation

\[\bm{S}_{t+1}:=g_{S}(\bm{S}_{t},A_{t},\bm{U}_{t}),\] (2)

where \(\bm{U}_{t}\in\mathcal{U}\) is a vector-valued noise variable with its prior distribution \(P^{\mathcal{C}}(\bm{U}_{t})\) having a density function \(f^{\mathcal{C}}_{\bm{U}_{t}}\), and we refer to the function \(g_{S}\) as the _transition mechanism_. Note that, in Eq. 2, the noise variables \(\{\bm{U}_{t}\}_{t=0}^{T-1}\) are mutually independent and, keeping the sequence of actions fixed, they are the only source of stochasticity in the dynamics of the environment. In other words, a sampled sequence of noise values \(\{\bm{u}_{t}\}_{t=0}^{T-1}\) and a fixed sequence of actions \(\{a_{t}\}_{t=0}^{T-1}\) result into a single (deterministic) sequence of states \(\{\bm{s}_{t}\}_{t=0}^{T-1}\). This implicitly assumes that the state transitions are stationary and there are no unobserved confounders. Figure 4 in Appendix B depicts the causal graph \(G\) corresponding to the SCM \(\mathcal{C}\) defined above.

The above representation of sequential decision making using an SCM \(\mathcal{C}\) is a more general reformulation of a Markov decision process, where a (stochastic) policy \(\pi(a\,|\,\bm{s})\) is entailed by Eq. 1, and the transition distribution (_i.e._, the conditional distribution of \(\bm{S}_{t+1}\,|\,\bm{S}_{t},A_{t}\)) is entailed by Eq. 2. Specifically, the conditional density function of \(\bm{S}_{t+1}\,|\,\bm{S}_{t},A_{t}\) is given by

\[p^{\mathcal{C}}(\bm{S}_{t+1}=\bm{s}\,|\,\bm{S}_{t}=\bm{s}_{t},A_ {t}=a_{t})=p^{\mathcal{C}\,;\,do(A_{t}=a_{t})}(\bm{S}_{t+1}=\bm{s}\,|\,\bm{S}_ {t}=\bm{s}_{t})\\ =\int_{\bm{u}\in\mathcal{U}}\mathds{1}[\bm{s}=g_{S}(\bm{s}_{t},a_ {t},\bm{u})]\cdot f^{\mathcal{C}}_{\bm{U}_{t}}(\bm{u})d\bm{u},\] (3)

where \(do(A_{t}=a_{t})\) denotes a (hard) intervention on the variable \(A_{t}\), whose value is set to \(a_{t}\).3 Here, the first equality holds because \(\bm{S}_{t+1}\) and \(A_{t}\) are d-separated by \(\bm{S}_{t}\) in the sub-graph obtained from \(G\) after removing all outgoing edges of \(A_{t}\)4 and the second equality follows from Eq. 2.

Footnote 3: In general, the \(do\) operator also allows for soft interventions (_i.e._, setting a probability distribution for \(A_{t}\)).

Footnote 4: This follows directly from the rules of \(do\)-calculus. For further details, refer to Chapter 3 of Pearl [8].

Moreover, as argued elsewhere [11, 13], by using an SCM to represent sequential decision making, instead of a classic MDP, we can answer counterfactual questions. More specifically, assume that, at time step \(t\), we observed the state \(\bm{S}_{t}=\bm{s}_{t}\), we took action \(A_{t}=a_{t}\) and the next state was \(\bm{S}_{t+1}=\bm{s}_{t+1}\). Retrospectively, we would like to know the probability that the state \(\bm{S}_{t+1}\) would have been \(\bm{s}^{\prime}\) if, at time step \(t\), we had been in a state \(\bm{s}\), and we had taken an action \(a\), (generally) different from \(\bm{s}_{t},a_{t}\). Using the SCM \(\mathcal{C}\), we can characterize this by a counterfactual transition density function

\[p^{\mathcal{C}\,|\,\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{s}_ {t},A_{t}=a_{t}\,;\,do(A_{t}=a)}(\bm{S}_{t+1}=\bm{s}^{\prime}\,|\,\bm{S}_{t}= s)=\\ \int_{\bm{u}\in\mathcal{U}}\mathds{1}[\bm{s}^{\prime}=g_{S}(\bm{s },a,\bm{u})]\cdot f^{\mathcal{C}\,|\,\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{ s}_{t},A_{t}=a_{t}}(\bm{u})d\bm{u},\] (4)

where \(f^{\mathcal{C}\,|\,\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{s}_{t},A_{t}=a_{t}}\) is the posterior distribution of the noise variable \(\bm{U}_{t}\) with support such that \(\bm{s}_{t+1}=g_{S}(\bm{s}_{t},a_{t},\bm{u})\).

In what follows, we will assume that the transition mechanism \(g_{S}\) is continuous with respect to its last argument and the SCM \(\mathcal{C}\) satisfies the following form of Lipschitz-continuity:

**Definition 1**.: _An SCM \(\mathcal{C}\) is Lipschitz-continuous iff the transition mechanism \(g_{S}\) and the reward \(R\) are Lipschitz-continuous with respect to their first argument, i.e., for each \(a\in\mathcal{A}\), \(\bm{u}\in\mathcal{U}\), there exists a Lipschitz constant \(K_{a,\bm{u}}\in\mathbb{R}_{+}\) such that, for any \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), \(\|g_{S}(\bm{s},a,\bm{u})-g_{S}(\bm{s}^{\prime},a,\bm{u})\|\leq K_{a,\bm{u}}\, \|\bm{s}-\bm{s}^{\prime}\|\), and, for each \(a\in\mathcal{A}\), there exists a Lipschitz constant \(C_{a}\in\mathbb{R}_{+}\) such that, for any \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), \(|R(\bm{s},a)-R(\bm{s}^{\prime},a)|\leq C_{a}\,\|\bm{s}-\bm{s}^{\prime}\|\). In both cases, \(\|\cdot\|\) denotes the Euclidean distance._

Note that, although they are not phrased in causal terms, similar Lipschitz continuity assumptions for the environment dynamics are common in prior work analyzing the theoretical guarantees of reinforcement learning algorithms [27, 28, 29, 30, 31, 32, 33, 34, 35]. Moreover, for practical applications (_e.g._, in healthcare), this is a relatively mild assumption to make. Consider two patients whose vitals \(\bm{s}\) and \(\bm{s}^{\prime}\) are similar at a certain point in time, they receive the same treatment \(a\), and every unobserved factor \(\bm{u}\) that may affect their health is also the same. Intuitively, Definition 1 implies that their vitals will also evolve similarly in the immediate future, _i.e._, the values \(g_{S}(\bm{s},a,\bm{u})\) and \(g_{S}(\bm{s}^{\prime},a,\bm{u})\) will not differ dramatically. In this context, it is worth mentioning that, when the transition mechanism \(g_{S}\) is modeled by a neural network, it is possible to control its Lipschitz constant during training, and penalizing high values can be seen as a regularization method [36, 37].

Further, we will focus on bijective SCMs [19], a fairly broad class of SCMs, which subsumes multiple models studied in the causal discovery literature, such as additive noise models [20], post-nonlinear causal models [21], location-scale noise models [22] and more complex models with neural network components [23, 24, 25].

**Definition 2**.: _An SCM \(\mathcal{C}\) is bijective iff the transition mechanism \(g_{S}\) is bijective with respect to its last argument, i.e., there is a well-defined inverse function \(g_{S}^{-1}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathcal{U}\) such that, for every combination of \(\bm{s}_{t+1},\bm{s}_{t},a_{t},\bm{u}_{t}\) with \(\bm{s}_{t+1}=g_{S}(\bm{s}_{t},a_{t},\bm{u}_{t})\), it holds that \(\bm{u}_{t}=g_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1})\)._

Importantly, bijective SCMs allow for a more concise characterization of the counterfactual transition density given in Eq. 4. More specifically, after observing an event \(\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{s}_{t},A_{t}=a_{t}\), the value \(\bm{u}_{t}\) of the noise variable \(\bm{U}_{t}\) can only be such that \(\bm{u}_{t}=g_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1})\), _i.e._, the posterior distribution of \(\bm{U}_{t}\) is a point mass and its density is given by

\[f_{\bm{U}_{t}}^{\mathcal{C}\,|\,\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{s}_{ t},A_{t}=a_{t}}(\bm{u})=\mathds{1}[\bm{u}=g_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1})],\] (5)

where \(\mathds{1}[\cdot]\) denotes the indicator function. Then, for a given episode \(\tau\) of the decision making process, we have that the (non-stationary) counterfactual transition density is given by

\[p_{\tau,t}(\bm{S}_{t+1}=\bm{s}^{\prime}\,|\,\bm{S}_{t}=\bm{s},A_ {t}=a) :=p^{\mathcal{C}\,|\,\bm{S}_{t+1}=\bm{s}_{t+1},\bm{S}_{t}=\bm{s}_{ t},A_{t}=a_{t}\,;\,do(A_{t}=a)}(\bm{S}_{t+1}=\bm{s}^{\prime}\,|\,\bm{S}_{t}= \bm{s})\] \[=\int_{\bm{u}\in\mathcal{U}}\mathds{1}[\bm{s}^{\prime}=g_{S}(\bm {s},a,\bm{u})]\cdot\mathds{1}[\bm{u}=g_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1}) ]d\bm{u}\] \[=\mathds{1}\left[\bm{s}^{\prime}=g_{S}\left(\bm{s},a,g_{S}^{-1} \left(\bm{s}_{t},a_{t},\bm{s}_{t+1}\right)\right)\right].\] (6)

Since this density is also a point mass, the resulting counterfactual dynamics are purely deterministic. That means, under a bijective SCM, the answer to the question "_What would have been the state at time \(t+1\), had we been at state \(\bm{s}\) and taken action \(a\) at time \(t\), given that, in reality, we were at \(\bm{s}_{t}\), we took \(a_{t}\) and the environment transitioned to \(\bm{s}_{t+1}\)?_" is just given by \(\bm{s}^{\prime}=g_{S}\left(\bm{s},a,g_{S}^{-1}\left(\bm{s}_{t},a_{t},\bm{s}_{ t+1}\right)\right)\).

**On the counterfactual identifiability of bijective SCMs.** Very recently, Nasr-Esfahany and Kiciman [38] have shown that bijective SCMs are in general not counterfactually identifiable when the exogenous variable \(\bm{U}_{t}\) is multi-dimensional. In other words, even with access to an infinite amount of triplets \((\bm{s}_{t},a_{t},\bm{s}_{t+1})\) sampled from the true SCM \(\mathcal{C}\), it is always possible to find an SCM \(\mathcal{M}\neq\mathcal{C}\) with transition mechanism \(h_{S}\) and distributions \(P^{\mathcal{M}}(\bm{U}_{t})\) that entails the same transition distributions as \(\mathcal{C}\) (_i.e._, it fits the observational data perfectly), but leads to different counterfactual predictions. Although our subsequent algorithmic results do not require the SCM \(\mathcal{C}\) to be counterfactually identifiable, the subclass of bijective SCMs we will use in our experiments in Section 5 is counterfactually identifiable. The defining attribute of this subclass, which we refer to as _element-wise bijective SCMs_, is that the transition mechanism \(g_{S}\) can be decoupled into \(D\) independent mechanisms \(g_{S,i}\) such that \(S_{t+1,i}=g_{S,i}(\bm{S}_{t},A_{t},U_{t,i})\) for \(i\in\{1,\dots,D\}\). This implies \(S_{t+1,i}\perp\!\!\!\perp U_{t,j}\,|\,U_{t,i},\bm{S}_{t},A_{t}\) for \(j\neq i\), however, \(U_{t,i}\), \(U_{t,j}\) do not need to be independent. Informally, we have the following identifiability result (refer to Appendix C for a formal version of the theorem along with its proof, which follows a similar reasoning to proofs found in related work [12, 19]):

**Theorem 3** (Informal).: _Let \(\mathcal{C}\) and \(\mathcal{M}\) be two element-wise bijective SCMs such that their entailed transition distributions for \(\bm{S}_{t+1}\) given any value of \(\bm{S}_{t},A_{t}\) are always identical. Then, all their counterfactual predictions based on an observed transition \((\bm{s}_{t},a_{t},\bm{s}_{t+1})\) will also be identical._

**On the assumption of no unobserved confounding.** The assumption that there are no hidden confounders is a frequent assumption made by work at the intersection of counterfactual reasoning and reinforcement learning [10, 11, 12, 13] and, more broadly, in the causal inference literature [39, 40, 41, 42, 43]. That said, there is growing interest in developing off-policy methods for partially observable MPDs (POMDPs) that are robust to certain types of confounding [44, 45, 46], and in learning dynamic treatment regimes in sequential settings with non-Markovian structure [47, 48]. Moreover, there is a line of work focusing on the identification of counterfactual quantities in non-sequential confounded environments [49, 50, 51]. In that context, we consider the computation of (approximately) optimal counterfactual action sequences under confounding as a very interesting direction for future work.

Problem statement

Let \(\tau\) be an observed episode of a decision making process whose dynamics are characterized by a Lipschitz-continuous bijective SCM. To characterize the counterfactual outcome that any alternative action sequence would have achieved under the circumstances of the particular episode, we build upon the formulation of Section 2, and we define a non-stationary counterfactual MDP \(\mathcal{M}^{+}=(\mathcal{S}^{+},\mathcal{A},F_{\tau,t}^{+},R^{+},T)\) with deterministic transitions. Here, \(\mathcal{S}^{+}=\mathcal{S}\times[T-1]\) is an enhanced state space such that each \(\bm{s}^{+}\in\mathcal{S}^{+}\) is a pair \((\bm{s},l)\) indicating that the counterfactual episode would have been at state \(\bm{s}\in\mathcal{S}\) with \(l\) action changes already performed. Accordingly, \(R^{+}\) is a reward function which takes the form \(R^{+}((\bm{s},l),a)=R(\bm{s},a)\) for all \((\bm{s},l)\in\mathcal{S}^{+}\), \(a\in\mathcal{A}\), _i.e._, it does not change depending on the number of action changes already performed. Finally, the time-dependent transition function \(F_{\tau,t}^{+}:\mathcal{S}^{+}\times\mathcal{A}\rightarrow\mathcal{S}^{+}\) is defined as

\[F_{\tau,t}^{+}\left(\left(\bm{s},l\right),a\right)=\begin{cases}\left(g_{S} \left(\bm{s},a,g_{S}^{-1}\left(\bm{s}_{t},a_{t},\bm{s}_{t+1}\right)\right),l+1 \right)&\text{if }(a\neq a_{t})\\ \left(g_{S}\left(\bm{s},a_{t},g_{S}^{-1}\left(\bm{s}_{t},a_{t},\bm{s}_{t+1} \right)\right),l\right)&\text{otherwise.}\end{cases}\] (7)

Intuitively, here we set the transition function according to the point mass of the counterfactual transition density given in Eq. 6, and we use the second coordinate to keep track of the changes that have been performed in comparison to the observed action sequence up to the time step \(t\).

Now, given the initial state \(\bm{s}_{0}\) of the episode \(\tau\) and any counterfactual action sequence \(\{a_{t}^{\prime}\}_{t=0}^{T-1}\), we can compute the corresponding counterfactual episode \(\tau^{\prime}=\{(\bm{s}_{t}^{\prime},l_{t}),a_{t}^{\prime}\}_{t=0}^{T-1}\). Its sequence of states is given recursively by

\[(\bm{s}_{1}^{\prime},l_{1})=F_{\tau,0}^{+}\left(\left(\bm{s}_{0},0\right),a_{0 }^{\prime}\right)\ \text{ and }\ \left(\bm{s}_{t+1}^{\prime},l_{t+1}\right)=F_{\tau,0}^{+}\left(\left(\bm{s}_{t }^{\prime},l_{t}\right),a_{t}^{\prime}\right)\ \text{ for }t\in\{1,\ldots,T-1\},\] (8)

and its counterfactual outcome is given by \(o^{+}(\tau^{\prime}):=\sum_{t}R^{+}\left(\left(\bm{s}_{t}^{\prime},l_{t}\right),a_{t}^{\prime}\right)=\sum_{t}R\left(\bm{s}_{t}^{\prime},a_{t}^{\prime}\right)\).

Then, similarly as in Tsirtsis et al. [13], our ultimate goal is to find the counterfactual action sequence \(\{a_{t}^{\prime}\}_{t=0}^{T-1}\) that, starting from the observed initial state \(\bm{s}_{0}\), maximizes the counterfactual outcome subject to a constraint on the number of counterfactual actions that can differ from the observed ones, _i.e._,

\[\underset{a_{0},\ldots,a_{T-1}}{\text{maximize}}\ \ \ o^{+}(\tau^{\prime})\qquad\text{ subject to }\ \ \bm{s}_{0}^{\prime}=\bm{s}_{0}\ \text{ and }\ \sum_{t=0}^{T-1}\bm{1}[a_{t}\neq a_{t}^{\prime}]\leq k,\] (9)

where \(a_{0},\ldots,a_{T-1}\) are the observed actions. Unfortunately, using a reduction from the classic partition problem [26], the following theorem shows that we cannot hope to find the optimal action sequence in polynomial time:5

Footnote 5: The supporting proofs of all Theorems, Lemmas and Propositions can be found in Appendix D.

**Theorem 4**.: _The problem defined by Eq. 9. is NP-Hard._

The proof of the theorem relies on a reduction from the partition problem [26], which is known to be NP-complete, to our problem, defined in Eq. 9. At a high-level, we map any instance of the partition problem to an instance of our problem, taking special care to construct a reward function and an observed action sequence, such that the optimal counterfactual outcome \(o^{+}(\tau^{*})\) takes a specific value if and only if there exists a valid partition for the original instance. The hardness result of Theorem 4 motivates our subsequent focus on the design of a method that _always_ finds the optimal solution to our problem at the expense of a potentially higher runtime for some problem instances.

## 4 Finding the optimal counterfactual action sequence via A* search

To deal with the increased computational complexity of the problem, we develop an optimal search method based on the classic \(A^{*}\) algorithm [52], which we have found to be very efficient in practice. Our starting point is the observation that, the problem of Eq. 9 presents an optimal substructure, _i.e._, its optimal solution can be constructed by combining optimal solutions to smaller sub-problems. For an observed episode \(\tau\), let \(V_{\tau}(\bm{s},l,t)\) be the maximum counterfactual reward that could have been achieved in a counterfactual episode where, at time \(t\), the process is at a (counterfactual) state \(\bm{s}\), and there are so far \(l\) actions that have been different in comparison with the observed action sequence. Formally,

\[V_{\tau}(\bm{s},l,t)=\max_{a_{t}^{\prime},\ldots,a_{T-1}^{\prime}}\sum_{t^{ \prime}=t}^{T-1}R(\bm{s}_{t^{\prime}}^{\prime},a_{t^{\prime}}^{\prime})\qquad \text{ subject to }\ \ \bm{s}_{t}^{\prime}=\bm{s}\ \text{ and }\ \sum_{t^{\prime}=t}^{T-1}\bm{1}[a_{t^{\prime}}\neq a_{t^{\prime}}^{\prime}] \leq k-l.\]Then, it is easy to see that the quantity \(V_{\tau}(\bm{s},l,t)\) can be given by the recursive function

\[V_{\tau}(\bm{s},l,t)=\max_{a\in\mathcal{A}}\left\{R(\bm{s},a)+V_{\tau}\left(\bm{s }_{a},l_{a},t+1\right)\right\},\quad\text{for all }\bm{s}\in\mathcal{S},\ l<k\text{ and }t<T-1,\] (10)

where \((\bm{s}_{a},l_{a})=F_{\tau,t}^{+}\left((\bm{s},l)\,,a\right)\). In the base case of \(l=k\) (_i.e._, all allowed action changes are already performed), we have \(V_{\tau}(\bm{s},k,t)=R(\bm{s},a_{t})+V_{\tau}\left(\bm{s}_{a_{t}},l_{a_{t}},t+1\right)\) for all \(\bm{s}\in\mathcal{S}\) and \(t<T-1\), and \(V_{\tau}(\bm{s},k,T-1)=R(\bm{s},a_{T-1})\) for \(t=T-1\). Lastly, when \(t=T-1\) and \(l<k\), we have \(V_{\tau}(\bm{s},l,T-1)=\max_{a\in\mathcal{A}}R(\bm{s},a)\) for all \(\bm{s}\in\mathcal{S}\).

Given the optimal substructure of the problem, one may be tempted to employ a typical dynamic programming approach to compute the values \(V_{\tau}(\bm{s},l,t)\) in a bottom-up fashion. However, the complexity of the problem lies in the fact that, the states \(\bm{s}\) are real-valued vectors whose exact values depend on the entire action sequence that led to them. Hence, to enumerate all the possible values that \(\bm{s}\) might take, one has to enumerate all possible action sequences in the search space, which is equivalent to solving our problem with a brute force search. In what follows, we present our proposed method to find optimal solutions using the \(A^{*}\) algorithm, with the caveat that its runtime varies depending on the problem instance, and it can be equal to that of a brute force search in the worst case.

**Casting the problem as graph search.** We represent the solution space of our problem as a graph, where each node \(v\) corresponds to a tuple \((\bm{s},l,t)\) with \(\bm{s}\in\mathcal{S}\), \(l\in[k]\) and \(t\in[T]\). Every node \(v=(\bm{s},l,t)\) with \(l<k\) and \(t<T-1\) has \(|\mathcal{A}|\) outgoing edges, each one associated with an action \(a\in\mathcal{A}\), carrying a reward \(R(\bm{s},a)\), and leading to a node \(v_{a}=(\bm{s}_{a},l_{a},t+1)\) such that \((\bm{s}_{a},l_{a})=F_{\tau,t}^{+}\left((\bm{s},l)\,,a\right)\). In the case of \(l=k\), the node \(v\) has exactly one edge corresponding to the observed action \(a_{t}\) at time \(t\). Lastly, when \(t=T-1\), the outgoing edge(s) lead(s) to a common node \(v_{T}=(\bm{s}_{\emptyset},k,T)\) which we call the _goal node_, and it has zero outgoing edges itself. Note that, the exact value of \(\bm{s}_{\emptyset}\) is irrelevant, and we only include it for notational completeness.

Let \(\bm{s}_{0}\) be the initial state of the observed episode. Then, it is easy to notice that, starting from the root node \(v_{0}=(\bm{s}_{0},0,0)\), the first elements of each node \(v_{i}\) on a path \(v_{0},\ldots,v_{i},\ldots,v_{T}\) form a sequence of counterfactual states, and the edges that connect those nodes are such that the corresponding counterfactual action sequence differs from the observed one in at most \(k\) actions. That said, the counterfactual outcome \(o^{+}(\tau)=\sum_{t=0}^{T-1}R(\bm{s}_{t}^{\prime},a_{t}^{\prime})\) is expressed as the sum of the rewards associated with each edge in the path, and the problem defined by Eq. 9 is equivalent to finding the path of maximum total reward that starts from \(v_{0}\) and ends in \(v_{T}\). Figure 0(a) illustrates the search graph for a simple instance of our problem. Unfortunately, since the states \(\bm{s}\) are vectors of real values, even enumerating all the graph's nodes requires time exponential in the number of actions \(|\mathcal{A}|\), which makes classic algorithms that search over the entire graph non-practical.

To address this challenge, we resort to the \(A^{*}\) algorithm, which performs a more efficient search over the graph by preferentially exploring only parts of it where we have prior information that they are

Figure 1: Main components of our search method based on the A* algorithm. Panel (a) shows the search graph for a problem instance with \(|\mathcal{A}|=2\). Here, each box represents a node \(v=(\bm{s},l,t)\) of the graph, and each edge represents a counterfactual transition. Next to each edge, we include the action \(a\in\mathcal{A}\) causing the transition and the associated reward. Panel (b) shows the heuristic function computation, where the two axes represent a (continuous) state space \(\mathcal{S}=\mathbb{R}^{2}\) and the two levels on the z-axis correspond to differences in the (integer) values \((l,t)\) and \((l_{a},t+1)\). Here, the blue squares correspond to the finite states in the anchor set \(\mathcal{S}_{\dagger}\) and \((\bm{s}_{a},l_{a})=F_{\tau,t}^{+}\left((\bm{s},l)\,,a\right)\).

more likely to lead to paths of higher total reward. Concretely, the algorithm proceeds iteratively and maintains a queue of nodes to visit, initialized to contain only the root node \(v_{0}\). Then, at each step, it selects one node from the queue, and it retrieves all its children nodes in the graph which are subsequently added to the queue. It terminates when the node being visited is the goal node \(v_{T}\). Refer to Algorithm 2 in Appendix E for a pseudocode implementation of the \(A^{*}\) algorithm.

The key element of the \(A^{*}\) algorithm is the criterion based on which it selects which node from the queue to visit next. Let \(v_{i}=(\bm{s}_{i},l_{i},t)\) be a candidate node in the queue and \(r_{v_{i}}\) be the total reward of the path that the algorithm has followed so far to reach from \(v_{0}\) to \(v_{i}\). Then, the \(A^{*}\) algorithm visits next the node \(v_{i}\) that maximizes the sum \(r_{v_{i}}+\hat{V}_{\tau}(\bm{s}_{i},l_{i},t)\), where \(\hat{V}_{\tau}\) is a _heuristic function_ that aims to estimate the maximum reward that can be achieved via any path starting from \(v_{i}=(\bm{s}_{i},l_{i},t)\) and ending in the goal node \(v_{T}\), _i.e._, it gives an estimate for the quantity \(V_{\tau}(\bm{s}_{i},l_{i},t)\). Intuitively, the heuristic function can be thought of as an "eye into the future" of the graph search, that guides the algorithm towards nodes that are more likely to lead to the optimal solution and the algorithm's performance depends on the quality of the approximation of \(V_{\tau}(\bm{s}_{i},l_{i},t)\) by \(\hat{V}_{\tau}(\bm{s}_{i},l_{i},t)\). Next, we will look for a heuristic function that satisfies _consistency_6 to guarantee that the \(A^{*}\) algorithm as described above returns the optimal solution upon termination [52].

Footnote 6: A heuristic function \(\hat{V}_{\tau}\) is consistent iff, for nodes \(v=(\bm{s},l,t)\), \(v_{a}=(\bm{s}_{a},l_{a},t+1)\) connected with an edge associated with action \(a\), it satisfies \(\hat{V}_{\tau}(\bm{s},l,t)\geq R(\bm{s},a)+\hat{V}_{\tau}(\bm{s}_{a},l_{a},t+1)\)[53].

**Computing a consistent heuristic function.** We first propose an algorithm that computes the function's values \(\hat{V}_{\tau}(\bm{s},l,t)\) for a finite set of points such that \(l\in[k]\), \(t\in[T-1]\), \(\bm{s}\in\mathcal{S}_{\dagger}\subset\mathcal{S}\), where \(\mathcal{S}_{\dagger}\) is a pre-defined _finite_ set of states--an _anchor set_--whose construction we discuss later. Then, based on the Lipschitz-continuity of the SCM \(\mathcal{C}\), we show that these computed values of \(\hat{V}_{\tau}\) are valid upper bounds of the corresponding values \(V_{\tau}(\bm{s},l,t)\) and we expand the definition of the heuristic function \(\hat{V}_{\tau}\) over all \(\bm{s}\in\mathcal{S}\) by expressing it in terms of those upper bounds. Finally, we prove that the function resulting from the aforementioned procedure is consistent.

To compute the upper bounds \(\hat{V}_{\tau}\), we exploit the observation that the values \(V_{\tau}(\bm{s},l,t)\) satisfy a form of Lipschitz-continuity, as stated in the following Lemma.

**Lemma 5**.: _Let \(\bm{u}_{t}=g_{S}^{-1}\left(\bm{s}_{t},a_{t},\bm{s}_{t+1}\right)\), \(K_{\bm{u}_{t}}=\max_{a\in\mathcal{A}}K_{a,\bm{u}_{t}}\), \(C=\max_{a\in\mathcal{A}}C_{a}\) and the sequence \(L_{0},\ldots,L_{T-1}\in\mathbb{R}_{+}\) be such that \(L_{T-1}=C\) and \(L_{t}=C+L_{t+1}K_{\bm{u}_{t}}\) for \(t\in[T-2]\). Then, it holds that \(|V_{\tau}(\bm{s},l,t)-V_{\tau}(\bm{s}^{\prime},l,t)|\leq L_{t}\left\|\bm{s}- \bm{s}^{\prime}\right\|\), for all \(t\in[T-1]\), \(l\in[k]\) and \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\)._

Based on this observation, our algorithm proceeds in a bottom-up fashion and computes valid upper bounds of the values \(V_{\tau}(\bm{s},l,t)\) for all \(l\in[k]\), \(t\in[T-1]\) and \(\bm{s}\) in the anchor set \(\mathcal{S}_{\dagger}\). To get the tinuition, assume that, for a given \(t\), the values \(\hat{V}_{\tau}(\bm{s},l,t+1)\) are already computed for all \(\bm{s}\in\mathcal{S}_{\dagger}\), \(l\in[k]\), and they are indeed valid upper bounds of the corresponding \(V_{\tau}(\bm{s},l,t+1)\). Then, let \((\bm{s}_{a},l_{a})=F^{+}_{\tau,t}(\left(\bm{s},l\right),a)\) for some \(\bm{s}\in\mathcal{S}_{\dagger}\) and \(l\in[k]\). Since \(\bm{s}_{a}\) itself may not belong to the finite anchor set \(\mathcal{S}_{\dagger}\), the algorithm uses the values \(\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a},t+1)\) of all anchors \(\bm{s}_{\dagger}\in\mathcal{S}_{\dagger}\) in combination with their distance to \(\bm{s}_{a}\), and it sets the value of \(\hat{V}_{\tau}(\bm{s},l,t)\) in way that it is also guaranteed to be a (maximally tight) upper bound of \(V_{\tau}(\bm{s},l,t)\). Figure 0(b) illustrates the above operation. Algorithm 1 summarizes the overall procedure, which is guaranteed to return upper bounds, as shown by the following proposition:

**Proposition 6**.: _For all \(\bm{s}\in\mathcal{S}_{\dagger}\), \(l\in[k],t\in[T-1]\), it holds that \(\hat{V}_{\tau}(\bm{s},l,t)\geq V_{\tau}(\bm{s},l,t)\), where \(\hat{V}_{\tau}(\bm{s},l,t)\) are the values of the heuristic function computed by Algorithm 1._

Next, we use the values \(\hat{V}_{\tau}(\bm{s},l,t)\) computed by Algorithm 1 to expand the definition of \(\hat{V}_{\tau}\) over the entire domain as follows. For some \(\bm{s}\in\mathcal{S}\), \(a\in\mathcal{A}\), let \((\bm{s}_{a},l_{a})=F^{+}_{\tau,t}\left(\left(\bm{s},l\right),a\right)\), then, we have that

\[\hat{V}_{\tau}(\bm{s},l,t)=\begin{cases}0&t=T\\ \max\limits_{a\in\mathcal{A}^{\prime}}R(\bm{s},a)&t=T-1\\ \max\limits_{a\in\mathcal{A}^{\prime}}\left\{R(\bm{s},a)+\min\limits_{\bm{s}_ {i}\in\mathcal{S}_{\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a},t+1) +L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a}\right\|\right\}\right\}&\text{ otherwise},\end{cases}\] (11)

where \(\mathcal{A}^{\prime}=\{a_{t}\}\) for \(l=k\) and \(\mathcal{A}^{\prime}=\mathcal{A}\) for \(l<k\). Finally, the following theorem shows that the resulting heuristic function \(\hat{V}_{\tau}\) is consistent:

**Theorem 7**.: _For any nodes \(v=(\bm{s},l,t),v_{a}=(\bm{s}_{a},l_{a},t+1)\) with \(t<T-1\) connected with an edge associated with action \(a\), it holds that \(\hat{V}_{\tau}(\bm{s},l,t)\geq R(\bm{s},a)+\hat{V}_{\tau}(\bm{s}_{a},l_{a},t+1)\). Moreover, for any node \(v=(\bm{s},l,T-1)\) and edge connecting it to the goal node \(v_{T}=(\bm{s}_{\emptyset},k,T)\), it holds that \(\hat{V}_{\tau}(\bm{s},l,T-1)\geq R(\bm{s},a)+\hat{V}_{\tau}(\bm{s}_{\emptyset},k,T)\)._

**Kick-starting the heuristic function computation with Monte Carlo anchor sets.** For any \(\bm{s}\not\in\mathcal{S}_{\dagger}\), whenever we compute \(\hat{V}_{\tau}(\bm{s},l,t)\) using Eq. 11, the resulting value is set based on the value \(\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a},t+1)\) of some anchor \(\bm{s}_{\dagger}\), increased by a _penalty_ term \(L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a}\right\|\). Intuitively, this allows us to think of the heuristic function \(\hat{V}_{\tau}\) as an upper bound of the function \(V_{\tau}\) whose looseness depends on the magnitude of the penalty terms encountered during the execution of Algorithm 1 and each subsequent evaluation of Eq. 11. To speed up the \(A^{*}\) algorithm, note that, ideally, one would want all penalty terms to be zero, _i.e._, an anchor set that includes all the states \(\bm{s}\) of the nodes \(v=(\bm{s},l,t)\) that are going to appear in the search graph. However, as discussed in the beginning of Sec. 4, an enumeration of those states requires a runtime exponential in the number of actions.

To address this issue, we introduce a Monte Carlo simulation technique that adds to the anchor set the observed states \(\{\bm{s}_{0},\ldots,\bm{s}_{T-1}\}\) and all unique states \(\{\bm{s}^{\prime}_{0},\ldots,\bm{s}^{\prime}_{T-1}\}\) resulting by \(M\) randomly sampled counterfactual action sequences \(a^{\prime}_{0},\ldots,a^{\prime}_{T-1}\). Specifically, for each action sequence, we first sample a number \(k^{\prime}\) of actions to be changed and what those actions are going to be, both uniformly at random from \(\{1,\ldots,k\}\) and \(\mathcal{A}^{k^{\prime}}\), respectively. Then, we sample from \(\{0,\ldots,T-1\}\) the \(k^{\prime}\) time steps where the changes take place, with each time step \(t\) having a probability \(L_{t}/\sum_{t^{\prime}}L_{t^{\prime}}\) to be selected. This biases the sampling towards earlier time steps, where the penalty terms are larger due to the higher Lipschitz constants. As we will see in the next section, this approach works well in practice, and it allows us to control the runtime of the \(A^{*}\) algorithm by appropriately adjusting the number of samples \(M\). We experiment with additional anchor set selection strategies in Appendix F.

## 5 Experiments using clinical sepsis management data

**Experimental setup.** To evaluate our method, we use real patient data from MIMIC-III [54], a freely accessible critical care dataset commonly used in reinforcement learning for healthcare [6, 55, 56, 57]. We follow the preprocessing steps of Komorowski et al. [6] to identify a cohort of \(20{,}926\) patients treated for sepsis [58]. Each patient record contains vital signs and administered treatment information in time steps of \(4\)-hour intervals. As an additional preprocessing step, we discard patient records whose associated time horizon \(T\) is shorter than \(10\), resulting in a final dataset of \(15{,}992\) patients with horizons between \(10\) and \(20\).

To form our state space \(\mathcal{S}=\mathbb{R}^{D}\), we use \(D=13\) features. Four of these features are demographic or contextual and thus we always set their counterfactual values to the observed ones. The remaining \(\tilde{D}=9\) features are time-varying and include the SOFA score [59]--a standardized score of organ failure rate--along with eight vital signs that are required for its calculation. Since SOFA scores positively correlate with patient mortality [60], we assume that each \(\bm{s}\in\mathcal{S}\) gives a reward \(R(\bm{s})\) equal to the negation of its SOFA value. Here, it is easy to see that this reward function is just a projection of \(\bm{s}\), therefore, it is Lipschitz continuous with constant \(C_{a}=1\) for all \(a\in\mathcal{A}\). Following related work [6, 55, 57], we consider an action space \(\mathcal{A}\) that consists of \(25\) actions, which correspond to \(5\times 5\) levels of administered vasopressors and intravenous fluids. Refer to Appendix G for additional details on the features and actions.

To model the transition dynamics of the time-varying features, we consider an SCM \(\mathcal{C}\) whose transition mechanism takes a location-scale form \(g_{S}(\bm{S}_{t},A_{t},\bm{U}_{t})=h(\bm{S}_{t},A_{t})+\phi(\bm{S}_{t},A_{t} )\odot\bm{U}_{t}\), where \(h,\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{\tilde{D}}\), and \(\odot\) denotes the element-wise multiplication [22, 24]. Notably, this model is element-wise bijective and hence it is counterfactually identifiable, as shown in Section 2. Moreover, we use neural networks to model the location and scale functions \(h\) and \(\phi\) and enforce their Lipschitz constants to be \(L_{h}\) and \(L_{\phi}\), respectively. This results in a Lipschitz continuous SCM \(\mathcal{C}\) with \(K_{a,\bm{u}}=L_{h}+L_{\phi}\max_{i}|u_{i}|\). Further, we assume that the noise variable \(\bm{U}_{t}\) follows a multivariate Gaussian distribution with zero mean and allow its covariance matrix to be a (trainable) parameter.

We jointly train the weights of the networks \(h\) and \(\phi\) and the covariance matrix of the noise prior on the observed patient transitions using stochastic gradient descent with the negative log-likelihood of each transition as a loss. In our experiments, if not specified otherwise, we use an SCM with Lipschitz constants \(L_{h}=1.0\), \(L_{\phi}=0.1\) that achieves a log-likelihood only \(6\%\) lower to that of the best model trained without any Lipschitz constraint. Refer to Appendix G for additional details on the network architectures, the training procedure and the way we enforce Lipschitz continuity.7

Footnote 7: All experiments were performed using an internal cluster of machines equipped with 16 Intel(R) Xeon(R) 3.20GHz CPU cores, 512GBs of memory and 2 NVIDIA A40 48GB GPUs.

**Results.** We start by evaluating the computational efficiency of our method against (i) the Lipschitz constant of the location network \(L_{h}\), (ii) the number of Monte Carlo samples \(M\) used to generate the anchor set \(\mathcal{S}_{t}\), and (iii) the number of actions \(k\) that can differ from the observed ones. We measure efficiency using running time and the effective branching factor (EBF) [52]. The EBF is defined as a real number \(b\geq 1\) such that the number of nodes expanded by \(A^{*}\) is equal to \(1+b+b^{2}+\cdots+b^{T}\), where \(T\) is the horizon, and values close to \(1\) indicate that the heuristic function is the most efficient in guiding the search. Figure 2 summarizes the results, which show that our method maintains overall a fairly low running time that decreases with the number of Monte Carlo samples \(M\) used for the generation of the anchor set and increases with the Lipschitz constant \(L_{h}\) and the number of action changes \(k\). That may not come as a surprise since, as \(L_{h}\) increases, the heuristic function becomes more loose, and as \(k\) increases, the size of the search space increases exponentially. To put things in perspective, for a problem instance with \(L_{h}=1.0\), \(k=3\) and horizon \(T=12\), the \(A^{*}\) search led by our heuristic function is effectively equivalent to an exhaustive search over a full tree with

Figure 2: Computational efficiency of our method under different configurations, as measured by the effective branching factor (pink-left axis) and the runtime of the \(A^{*}\) algorithm (green-right axis). In Panel (a), we set \(M=2000\) and \(k=3\). In Panel (b), we set \(L_{h}=1.0\) and \(k=3\). In Panel (c), we set \(L_{h}=1.0\) and \(M=2000\). In all panels, we set \(L_{\phi}=0.1\) and error bars indicate \(95\%\) confidence intervals over \(200\) executions of the \(A^{*}\) algorithm for \(200\) patients with horizon \(T=12\).

\(2.1^{12}\approx 7,355\) leaves while the corresponding search space of our problem consists of more than \(3\) million action sequences--more than \(3\) million paths to reach from the root node to the goal node.

Next, we investigate to what extent the counterfactual action sequences generated by our method would have led the patients in our dataset to better outcomes. For each patient, we measure their counterfactual improvement--the relative decrease in cumulative SOFA score between the counterfactual and the observed episode. Figures 2(a) and 2(b) summarize the results, which show that: (i) the average counterfactual improvement shows a diminishing increase as \(k\) increases; (ii) the median counterfactual improvement is only \(5\%\), indicating that, the treatment choices made by the clinicians for most of the patients were close to optimal, even with the benefit of hindsight; and (iii) there are \(176\) patients for whom our method suggests that a different sequence of actions would have led to an outcome that is at least \(15\%\) better. That said, we view patients at the tail of the distribution as "interesting cases" that should be deferred to domain experts for closer inspection, and we present one such example in Fig. 2(c). In this example, our method suggests that, had the patient received an early higher dosage of intravenous fluids while some of the later administered fluids where replaced by vasopressors, their SOFA score would have been lower across time. Although we present this case as purely anecdotal, the counterfactual episode is plausible, since there are indications of decreased mortality when intravenous fluids are administered at the early stages of a septic shock [61].

## 6 Conclusions

In this paper, we have introduced the problem of finding counterfactually optimal action sequences in sequential decision making processes with continuous state dynamics. We showed that the problem is NP-hard and, to tackle it, we introduced a search method based on the \(A^{*}\) algorithm that is guaranteed to find the optimal solution, with the caveat that its runtime can vary depending on the problem instance. Lastly, using real clinical data, we have found that our method is very efficient in practice, and it has the potential to offer interesting insights to domain experts by highlighting episodes and time-steps of interest for further inspection.

Our work opens up many interesting avenues for future work. For example, it would be interesting to develop algorithms with approximation guarantees that run in polynomial time, at the expense of not achieving strict counterfactual optimality. Moreover, since the practicality of methods like ours relies on the assumption that the SCM describing the environment is accurate, it would be interesting to develop methods to learn SCMs that align with human domain knowledge. Finally, it would be interesting to validate our method using real datasets from other applications and carry out user studies in which the counterfactual action sequences found by our method are systematically evaluated by the human experts (_e.g._, clinicians) who took the observed actions.

**Acknowledgements.** Tsirtsis and Gomez-Rodriguez acknowledge support from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 945719).

Figure 3: Retrospective analysis of patientsâ€™ episodes. Panel (a) shows the average counterfactual improvement as a function of \(k\) for a set of \(200\) patients with horizon \(T=12\), where error bars indicate \(95\%\) confidence intervals. Panel (b) shows the distribution of counterfactual improvement across all patients for \(k=3\), where the dashed vertical line indicates the median. Panel (c) shows the observed (solid) and counterfactual (dashed) SOFA score across time for a patient who presents a \(19.9\%\) counterfactual improvement when \(k=3\). Upward (downward) arrows indicate action changes that suggest a higher (lower) dosage of vasopressors (V) and fluids (F). In all panels, we set \(M=2000\).

## References

* [1] Ruth MJ Byrne. Counterfactual thought. _Annual review of psychology_, 67:135-157, 2016.
* [2] David A Lagnado, Tobias Gerstenberg, and Ro'i Zultan. Causal responsibility and counterfactuals. _Cognitive science_, 37(6):1036-1073, 2013.
* [3] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* [4] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for atari. In _International Conference on Learning Representations_, 2020.
* [5] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2021.
* [6] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. _Nature medicine_, 24(11):1716-1720, 2018.
* [7] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. _ACM Computing Surveys (CSUR)_, 55(1):1-36, 2021.
* [8] Judea Pearl. _Causality_. Cambridge university press, 2009.
* [9] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of causal inference: foundations and learning algorithms_. The MIT Press, 2017.
* [10] Lars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez, and Jean-Baptiste Lespiau. Woulda, coulda, shoulda: Counterfactually-guided policy search. In _International Conference on Learning Representations_, 2019.
* [11] Michael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural causal models. In _International Conference on Machine Learning_, pages 4881-4890. PMLR, 2019.
* [12] Chaochao Lu, Biwei Huang, Ke Wang, Jose Miguel Hernandez-Lobato, Kun Zhang, and Bernhard Scholkopf. Sample-efficient reinforcement learning via counterfactual-based data augmentation. _arXiv preprint arXiv:2012.09092_, 2020.
* [13] Stratis Tsirtsis, Abir De, and Manuel Gomez-Rodriguez. Counterfactual explanations in sequential decision making under uncertainty. _Advances in Neural Information Processing Systems_, 34:30127-30139, 2021.
* [14] Jonathan Richens, Rory Beard, and Daniel H Thompson. Counterfactual harm. In _Advances in Neural Information Processing Systems_, volume 35, pages 36350-36365, 2022.
* [15] Kimia Noorbakhsh and Manuel Gomez-Rodriguez. Counterfactual temporal point processes. _Advances in Neural Information Processing Systems_, 35:24810-24823, 2022.
* [16] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [17] Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. On pearl's hierarchy and the foundations of causal inference. In _Probabilistic and causal inference: the works of judea pearl_, pages 507-556. 2022.
* [18] Malcolm Elliott and Alysia Coventry. Critical care: the eight vital signs of patient monitoring. _British Journal of Nursing_, 21(10):621-625, 2012.

* Nasr-Esfahany et al. [2023] Arash Nasr-Esfahany, Mohammad Alizadeh, and Devavrat Shah. Counterfactual identifiability of bijective causal models. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* Hoyer et al. [2008] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. _Advances in neural information processing systems_, 21, 2008.
* Zhang and Hyvarinen [2009] Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence_, UAI '09, page 647-655, Arlington, Virginia, USA, 2009. AUAI Press.
* Immer et al. [2023] Alexander Immer, Christoph Schultheiss, Julia E Vogt, Bernhard Scholkopf, Peter Buhlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. In _International Conference on Machine Learning_, pages 14316-14332. PMLR, 2023.
* Pawlowski et al. [2020] Nick Pawlowski, Daniel Coelho de Castro, and Ben Glocker. Deep structural causal models for tractable counterfactual inference. _Advances in Neural Information Processing Systems_, 33:857-869, 2020.
* Khemakhem et al. [2021] Ilyes Khemakhem, Ricardo Monti, Robert Leech, and Aapo Hyvarinen. Causal autoregressive flows. In _International conference on artificial intelligence and statistics_, pages 3520-3528. PMLR, 2021.
* Sanchez and Tsaftaris [2022] Pedro Sanchez and Sotirios A. Tsaftaris. Diffusion causal models for counterfactual estimation. In _First Conference on Causal Learning and Reasoning_, 2022.
* Karp [1972] Richard M Karp. Reducibility among combinatorial problems. In _Complexity of computer computations_, pages 85-103. Springer, 1972.
* Bertsekas [1975] Dimitri Bertsekas. Convergence of discretization procedures in dynamic programming. _IEEE Transactions on Automatic Control_, 20(3):415-419, 1975.
* Hinderer [2005] Karl Hinderer. Lipschitz continuity of value functions in markovian decision processes. _Mathematical Methods of Operations Research_, 62:3-22, 2005.
* Rachelson and Lagoudakis [2010] Emmanuel Rachelson and Michail G Lagoudakis. On the locality of action domination in sequential decision making. 2010.
* Pazis and Parr [2013] Jason Pazis and Ronald Parr. Pac optimal exploration in continuous space markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 27, pages 774-781, 2013.
* Pirotta et al. [2015] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision processes. _Machine Learning_, 100:255-283, 2015.
* Berkenkamp et al. [2017] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. _Advances in neural information processing systems_, 30, 2017.
* Asadi et al. [2018] Kavosh Asadi, Dipendra Misra, and Michael Littman. Lipschitz continuity in model-based reinforcement learning. In _International Conference on Machine Learning_, pages 264-273. PMLR, 2018.
* Touati et al. [2020] Ahmed Touati, Adrien Ali Taiga, and Marc G Bellemare. Zooming for efficient model-free reinforcement learning in metric spaces. _arXiv preprint arXiv:2003.04069_, 2020.
* Gottesman et al. [2021] Omer Gottesman, Kavosh Asadi, Cameron Allen, Sam Lobel, George Konidaris, and Michael Littman. Coarse-grained smoothness for rl in metric spaces. _arXiv preprint arXiv:2110.12276_, 2021.
* Anil et al. [2019] Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In _International Conference on Machine Learning_, pages 291-301. PMLR, 2019.

* [37] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. _Machine Learning_, 110:393-416, 2021.
* [38] Arash Nasr-Esfahany and Emre Kiciman. Counterfactual (non-)identifiability of learned structural causal models. _arXiv preprint arXiv:2301.09031_, 2023.
* [39] Scott Sussex, Caroline Uhler, and Andreas Krause. Near-optimal multi-perturbation experimental design for causal structure learning. _Advances in Neural Information Processing Systems_, 34:777-788, 2021.
* [40] Ioana Bica, Daniel Jarrett, and Mihaela van der Schaar. Invariant causal imitation learning for generalizable policies. _Advances in Neural Information Processing Systems_, 34:3952-3964, 2021.
* [41] Chris Cundy, Aditya Grover, and Stefano Ermon. Bcd nets: Scalable variational approaches for bayesian causal discovery. _Advances in Neural Information Processing Systems_, 34:7095-7110, 2021.
* [42] Virginia Aglietti, Neil Dhir, Javier Gonzalez, and Theodoros Damoulas. Dynamic causal bayesian optimization. _Advances in Neural Information Processing Systems_, 34:10549-10560, 2021.
* [43] Xinshi Chen, Haoran Sun, Caleb Ellington, Eric Xing, and Le Song. Multi-task learning of order-consistent causal graphs. _Advances in Neural Information Processing Systems_, 34:11083-11095, 2021.
* [44] Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable environments. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 10276-10283, 2020.
* [45] Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill. Off-policy policy evaluation for sequential decisions under unobserved confounding. _Advances in Neural Information Processing Systems_, 33:18819-18831, 2020.
* [46] Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably efficient causal reinforcement learning with confounded observational data. _Advances in Neural Information Processing Systems_, 34:21164-21175, 2021.
* [47] Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes. _Advances in Neural Information Processing Systems_, 32, 2019.
* [48] Junzhe Zhang and Elias Bareinboim. Designing optimal dynamic treatment regimes: A causal reinforcement learning approach. In _International Conference on Machine Learning_, pages 11012-11022. PMLR, 2020.
* [49] Ilya Shpitser and Eli Sherman. Identification of personalized effects associated with causal pathways. In _Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence_, volume 2018. NIH Public Access, 2018.
* [50] Juan Correa, Sanghack Lee, and Elias Bareinboim. Nested counterfactual identification from arbitrary surrogate experiments. _Advances in Neural Information Processing Systems_, 34:6856-6867, 2021.
* [51] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational and experimental data. In _International Conference on Machine Learning_, pages 26548-26558. PMLR, 2022.
* [52] Stuart Russel, Peter Norvig, et al. _Artificial intelligence: a modern approach_, volume 256. Pearson Education Limited London, 2013.
* [53] Judea Pearl. _Heuristics: intelligent search strategies for computer problem solving_. Addison-Wesley Longman Publishing Co., Inc., 1984.

* [54] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* [55] Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi. Deep reinforcement learning for sepsis treatment. _arXiv preprint arXiv:1711.09602_, 2017.
* [56] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng. Reinforcement learning for clinical decision support in critical care: comprehensive review. _Journal of medical Internet research_, 22(7):e18477, 2020.
* [57] Taylor W Killian, Haoran Zhang, Jayakumar Subramanian, Mehdi Fatemi, and Marzyeh Ghassemi. An empirical study of representation learning for reinforcement learning in healthcare. _arXiv preprint arXiv:2011.11235_, 2020.
* [58] Mervyn Singer, Clifford S Deutschman, Christopher Warren Seymour, Manu Shankar-Hari, Djillali Annane, Michael Bauer, Rinaldo Bellomo, Gordon R Bernard, Jean-Daniel Chiche, Craig M Coopersmith, et al. The third international consensus definitions for sepsis and septic shock (sepsis-3). _Jama_, 315(8):801-810, 2016.
* [59] Simon Lambden, Pierre Francois Laterre, Mitchell M Levy, and Bruno Francois. The sofa score--development, utility and challenges of accurate assessment in clinical trials. _Critical Care_, 23(1):1-9, 2019.
* [60] Flavio Lopes Ferreira, Daliana Peres Bota, Annette Bross, Christian Melot, and Jean-Louis Vincent. Serial evaluation of the sofa score to predict outcome in critically ill patients. _Jama_, 286(14):1754-1758, 2001.
* [61] Jason Waechter, Anand Kumar, Stephen E Lapinsky, John Marshall, Peter Dodek, Yaseen Arabi, Joseph E Parrillo, R Phillip Dellinger, Allan Garland, Cooperative Antimicrobial Therapy of Septic Shock Database Research Group, et al. Interaction between fluids and vasoactive agents on mortality in septic shock: a multicenter, observational study. _Critical care medicine_, 42(10):2158-2168, 2014.
* [62] Lihong Li and Michael L Littman. Lazy approximation for solving continuous finite-horizon mdps. In _AAAI_, volume 5, pages 1175-1180, 2005.
* [63] Richard Dearden, Nicholas Meuleau, Richard Washington, and Zhengzhu Feng. Dynamic programming for structured continuous markov decision problems. In _Twentieth Conference on Uncertainty in Artificial Intelligence (UAI-04)_, 2004.
* [64] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning through a causal lens. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 2493-2500, 2020.
* [65] Ioana Bica, Daniel Jarrett, Alihan Huyuk, and Mihaela van der Schaar. Learning what-if" explanations for sequential decision-making. _arXiv preprint arXiv:2007.13531_, 2020.
* [66] Amir-Hossein Karimi, Julius Von Kugelgen, Bernhard Scholkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. _Advances in neural information processing systems_, 33:265-277, 2020.
* [67] Stratis Tsirtsis and Manuel Gomez-Rodriguez. Decisions, counterfactual explanations and strategic behavior. _Advances in Neural Information Processing Systems_, 33:16749-16760, 2020.

Further related work

**Counterfactual reasoning and reinforcement learning.** As mentioned in Section 1, there is a closely related line of work [10; 11; 12; 13] that focuses on the development of machine learning methods that employ elements of counterfactual reasoning to improve or to retrospectively analyze decisions in sequential settings. Buesing et al. [10] use SCMs to express the transition dynamics in Partially Observable MDPs (POMDPs), and they propose a method to efficiently compute a policy based on counterfactual realizations of logged episodes. Lu et al. [12] adopt a similar modeling framework, and they propose a counterfactual data augmentation approach to speed up standard Q-learning. Oberst and Sontag [11] introduce the Gumbel-Max SCM to express the dynamics of an arbitrary discrete POMPD, and they develop a method for counterfactual off-policy evaluation to identify episodes where a given alternative policy would have achieved a higher reward. However, none of these works aims to find an action sequence, close to the observed sequence of a particular episode, that is counterfactually optimal.

**Planning in continuous-state MDPs.** Our work has additional connections to pieces of work that aim to approximate the optimal value function in an MDP with continuous states and a finite horizon [62; 27; 63]. Therein, the work most closely related to ours is the one by Bertsekas [27]. It shows that, under a Lipschitz continuity assumption on the transition dynamics, a value function computed via value iteration in an MDP with discretized states, converges to the optimal value function of the original (continous-state) MDP as the discretization becomes finer. Although some of the proof mechanics of this work are similar to ours, the contributions are orthogonal, as we do not employ any form of discretization, and we leverage the Lipschitz continuity assumption to compute optimal action sequences in continuous states using the \(A^{*}\) algorithm.

**Counterfactual reasoning and explainability.** Our work has ties to pieces of work that use forms of counterfactual reasoning as a tool towards learning explainable machine learning models. For example, Madumal et al. [64] propose to express the action selection process of a reinforcement learning agent as a causal graph, and they use it to generate explanations for the agent's chosen actions. Bica et al. [65] introduce an inverse reinforcement learning approach to learn an interpretable reward function from expert demonstrated behavior that is expressed in terms of preferences over potential (counterfactual) outcomes. Moreover, our work is broadly related to the work on counterfactual explanations (in classification) that aims to find a minimally different set of features that would have led to a different outcome, in settings where single decisions are taken [66; 67].

[MISSING_PAGE_EMPTY:16]

Counterfactual identifiability of element-wise bijective SCMs

In this section, we show that _element-wise bijective_ SCMs, a subclass of bijective SCMs which we formally define next, are counterfactually identifiable.

**Definition 8**.: _An SCM \(\mathcal{C}\) is element-wise bijective iff it is bijective and there exist functions \(g_{S,i}:\mathbb{R}\times\mathcal{A}\times\mathbb{R}\rightarrow\mathbb{R}\) with \(i\in\{1,\dots,D\}\) such that, for every combination of \(\bm{s}_{t+1},\bm{s}_{t},a_{t},\bm{u}_{t}\) with \(\bm{s}_{t+1}=g_{S}(\bm{s}_{t},a_{t},\bm{u}_{t})\), it holds that \(s_{t+1,i}=g_{S,i}(\bm{s}_{t},a_{t},u_{t,i})\) for \(i\in\{1,\dots,D\}\)._

Under our assumption that the transition mechanism \(g_{S}\) is continuous with respect to its third argument, it is easy to see that, for any element-wise bijective SCM, the functions \(g_{S,i}\) are always strictly monotonic functions of the respective \(u_{t,i}\). Based on this observation, we have the following theorem of counterfactual identifiability:

**Theorem 9**.: _Let \(\mathcal{C}\) and \(\mathcal{M}\) be two element-wise bijective SCMs with transition mechanisms \(g_{S}\) and \(h_{S}\), respectively, and, for any observed transition \((\bm{s}_{t},a_{t},\bm{s}_{t+1})\), let \(\bm{u}_{t}=g_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1})\) and \(\tilde{\bm{u}}_{t}=h_{S}^{-1}(\bm{s}_{t},a_{t},\bm{s}_{t+1})\). Moreover, given any \(\bm{s}\in\mathcal{S},a\in\mathcal{A}\), let \(\bm{s}^{\prime}=g_{S}(\bm{s},a,\bm{u}_{t})\) and \(\bm{s}^{\prime\prime}=h_{S}(\bm{s},a,\tilde{\bm{u}}_{t})\). If \(P^{\mathcal{C}}(\bm{S}_{t+1}\,|\,\bm{S}_{t}=\bm{s},A_{t}=a)\) = \(P^{\mathcal{M}}(\bm{S}_{t+1}\,|\,\bm{S}_{t}=\bm{s},A_{t}=a)\) for all \(\bm{s}\in\mathcal{S},a\in\mathcal{A}\), it must hold that \(\bm{s}^{\prime}=\bm{s}^{\prime\prime}\)._

Proof.: We prove the theorem by induction, starting by establishing the base case \(s^{\prime}_{1}=s^{\prime\prime}_{1}\). Without loss of generality, assume that both \(g_{S,1}\) and \(h_{S,1}\) are strictly increasing with respect to their third argument. Since the two SCMs entail the same transition distributions, we have that

\[P^{\mathcal{C}}\left(S_{t+1,1}\leq s_{t+1,1}\,|\,\bm{S}_{t}=\bm{ s}_{t},A_{t}=a_{t}\right)=P^{\mathcal{M}}\left(S_{t+1,1}\leq s_{t+1,1}\,|\,\bm{S}_ {t}=\bm{s}_{t},A_{t}=a_{t}\right)\overset{(*)}{\Rightarrow}\\ P^{\mathcal{C}}\left(g_{S,1}\left(\bm{s}_{t},a_{t},U_{t,1} \right)\leq g_{S,1}\left(\bm{s}_{t},a_{t},U_{t,1}\right)\right)=P^{\mathcal{M}} \left(h_{S,1}\left(\bm{s}_{t},a_{t},U_{t,1}\right)\leq h_{S,1}\left(\bm{s}_{t},a _{t},\tilde{u}_{t,1}\right)\right)\overset{(**)}{\Rightarrow}\\ P^{\mathcal{C}}\left(U_{t,1}\leq u_{t,1}\right)=P^{\mathcal{M}} \left(U_{t,1}\leq\tilde{u}_{t,1}\right),\]

where \((*)\) holds because both SCMs are element-wise bijective, and \((**)\) holds because \(g_{S,1}\) and \(h_{S,1}\) are increasing with respect to their third argument. Similarly, we have that

\[P^{\mathcal{C}}\left(S_{t+1,1}\leq s^{\prime}_{1}\,|\,\bm{S}_{t} =\bm{s},A_{t}=a\right) =P^{\mathcal{C}}\left(g_{S,1}\left(\bm{s},a,U_{t,1}\right)\leq g_{S,1}\left(\bm{s},a,u_{t,1}\right)\right)\] \[\overset{(*)}{=}P^{\mathcal{C}}\left(U_{t,1}\leq u_{t,1}\right)\] \[\overset{(\star)}{=}P^{\mathcal{M}}\left(U_{t,1}\leq\tilde{u}_{ t,1}\right)\] \[\overset{(\dagger)}{=}P^{\mathcal{M}}\left(h_{S,1}\left(\bm{s},a,U_{t,1}\right)\leq h_{S,1}\left(\bm{s},a,\tilde{u}_{t,1}\right)\right)\] \[=P^{\mathcal{M}}\left(S_{t+1,1}\leq s^{\prime\prime}_{1}\,|\,\bm{ S}_{t}=\bm{s},A_{t}=a\right)\] \[=P^{\mathcal{C}}\left(S_{t+1,1}\leq s^{\prime\prime}_{1}\,|\,\bm{ S}_{t}=\bm{s},A_{t}=a\right),\]

where in \((\star)\), \((\dagger)\) we have used the monotonicity of \(g_{S}\) and \(h_{S}\), and \((\star\star)\) follows from the previous result. The last equality implies that \(s^{\prime}_{1}\) and \(s^{\prime\prime}_{1}\) correspond to the same quantile of the distribution for \(S_{t+1,1}\,|\,\bm{S}_{t}=\bm{s},A_{t}=a\). Therefore, it is easy to see that \(s^{\prime}_{1}=s^{\prime\prime}_{1}\) since the opposite would be in contradiction to \(g_{S,1}\) being bijective. Note that, we can reach that conclusion irrespective of the direction of monotonicity of \(g_{S,1}\) and \(h_{S,1}\), since any change in the direction of the inequalities happening at step \((**)\) is reverted at steps \((\star)\) and \((\dagger)\).

Now, starting from the inductive hypothesis that \(s^{\prime}_{i}=s^{\prime\prime}_{i}\) for all \(i\in\{1,\dots,n\}\) with \(n<D\), we show the inductive step, _i.e._, \(s^{\prime}_{n+1}=s^{\prime\prime}_{n+1}\). Again, without loss of generality, assume that both \(g_{S,n+1}\) and \(h_{S,n+1}\) are strictly increasing with respect to their last argument. Note that, the two SCMs entail the same transition distributions, _i.e._, the same joint distributions for \(\bm{S}_{t+1}\,|\,\bm{S}_{t},A_{t}\). Following from the law of total probability, they also entail the same conditional distributions for \(S_{t+1,n+1}\,|\,\bm{S}_{t+1,\leq n},\bm{S}_{t},A_{t}\), where we use the notation \(\bm{x}_{\leq n}\) to refer to a vector that contains the first \(n\) elements of a \(D\)-dimensional vector \(\bm{x}\). Therefore, we have that

\[P^{\mathcal{C}}\left(S_{t+1,n+1}\leq s_{t+1,n+1}\,|\,\bm{S}_{t+1, \leq n}=\bm{s}_{t+1,\leq n},\bm{S}_{t}=\bm{s}_{t},A_{t}=a_{t}\right)=\] \[P^{\mathcal{M}}\left(S_{t+1,n+1}\leq s_{t+1,n+1}\,|\,\bm{S}_{t+1, \leq n}=\bm{s}_{t+1,\leq n},\bm{S}_{t}=\bm{s}_{t},A_{t}=a_{t}\right)\overset{(*)}{\Rightarrow}\]\[P^{\mathcal{C}}\left(g_{S,n+1}\left(\bm{s}_{t},a_{t},U_{t,n+1} \right)\leq g_{S,n+1}\left(\bm{s}_{t},a_{t},u_{t,n+1}\right)\,|\,\bm{U}_{t,\leq n }=\bm{u}_{t,\leq n}\right)\] \[\qquad\qquad\qquad=P^{\mathcal{M}}\left(h_{S,n+1}\left(\bm{s}_{t },a_{t},U_{t,n+1}\right)\leq h_{S,n+1}\left(\bm{s}_{t},a_{t},\tilde{u}_{t,n+1} \right)\,|\,\bm{U}_{t,\leq n}=\tilde{\bm{u}}_{t,\leq n}\right)\overset{(**)}{ \Rightarrow}\] \[P^{\mathcal{C}}\left(U_{t,n+1}\leq u_{t,n+1}\,|\,\bm{U}_{t,\leq n }=\bm{u}_{t,\leq n}\right)=P^{\mathcal{M}}\left(U_{t,n+1}\leq\tilde{u}_{t,n+1 }\,|\,\bm{U}_{t,\leq n}=\tilde{\bm{u}}_{t,\leq n}\right),\]

where for the first equality we have used the inductive hypothesis, \((*)\) holds because both SCMs are element-wise bijective, and \((**)\) holds because \(g_{S,n+1}\) and \(h_{S,n+1}\) are increasing with respect to their third argument. Similarly, we get that

\[P^{\mathcal{C}}\left(S_{t+1,n+1}\leq s^{\prime}_{n+1}\,|\,\bm{S }_{t+1,\leq n}=\bm{s}_{t+1,\leq n},\bm{S}_{t}=\bm{s},A_{t}=a\right)\] \[=P^{\mathcal{C}}\left(g_{S,n+1}\left(\bm{s},a,U_{t,n+1}\right) \leq g_{S,n+1}\left(\bm{s},a,u_{t,n+1}\right)\,|\,g_{S,\leq n}\left(\bm{s},a, \bm{U}_{t,\leq n}\right)=g_{s,\leq n}\left(\bm{s},a,\bm{u}_{t,\leq n}\right)\right)\] \[\overset{(\star)}{=}P^{\mathcal{C}}\left(U_{t,n+1}\leq u_{t,n+1} \,|\,\bm{U}_{t,\leq n}=\bm{u}_{t,\leq n}\right)\] \[\overset{(\star\star)}{=}P^{\mathcal{M}}\left(U_{t,n+1}\leq \tilde{u}_{t,n+1}\,|\,\bm{U}_{t,\leq n}=\tilde{\bm{u}}_{t,\leq n}\right)\] \[\overset{(\dagger)}{=}P^{\mathcal{M}}\left(h_{S,n+1}\left(\bm{s}, a,U_{t,1}\right)\leq h_{S,n+1}\left(\bm{s},a,\tilde{u}_{t,1}\right)\,|\,h_{S,\leq n }\left(\bm{s},a,\bm{U}_{t,\leq n}\right)=h_{s,\leq n}\left(\bm{s},a,\tilde{ \bm{u}}_{t,\leq n}\right)\right)\] \[=P^{\mathcal{M}}\left(S_{t+1,n+1}\leq s^{\prime\prime}_{n+1}\,| \,\bm{S}_{t+1,\leq n}=\bm{S}_{t+1,\leq n},\bm{S}_{t}=\bm{s},A_{t}=a\right)\] \[=P^{\mathcal{C}}\left(S_{t+1,n+1}\leq s^{\prime\prime}_{n+1}\,| \,\bm{S}_{t+1,\leq n}=\bm{s}_{t+1,\leq n},\bm{S}_{t}=\bm{s},A_{t}=a\right),\]

where in \((\star),(\dagger)\) we have used the invertibility and monotonicity of \(g_{S}\) and \(h_{S}\), and \((\star\star)\) follows from the previous result. With the same argument as in the base case, the last equality implies that \(s^{\prime}_{n+1}=s^{\prime\prime}_{n+1}\). That concludes the proof.

Proofs

### Proof of Theorem 4

We prove the hardness of our problem as defined in Eq. 9 by performing a reduction from the partition problem [26], which is known to be NP-Complete. In the partition problem, we are given a multiset of \(B\) positive integers \(\mathcal{V}=\{v_{1},\ldots,v_{B}\}\) and the goal is to decide whether there is a partition of \(\mathcal{V}\) into two subsets \(\mathcal{V}_{1},\mathcal{V}_{2}\) with \(\mathcal{V}_{1}\cap\mathcal{V}_{2}=\emptyset\) and \(\mathcal{V}_{1}\cup\mathcal{V}_{2}=\mathcal{V}\), such that their sums are equal, _i.e._, \(\sum_{v_{i}\in\mathcal{V}_{1}}v_{i}=\sum_{v_{j}\in\mathcal{V}_{2}}v_{j}\).

Consider an instance of our problem where \(\mathcal{S}=\mathcal{U}=\mathbb{R}^{2}\), \(\mathcal{A}\) contains \(2\) actions \(a_{\text{diff}},a_{\text{null}}\) and the horizon is \(T=B+1\). Let \(\mathcal{C}\) be an element-wise bijective SCM with arbitrary prior distributions \(P^{\mathcal{C}}(\bm{U}_{t})\) such that their support is on \(\mathbb{R}^{2}\) and a transition mechanism \(g_{S}\) such that

\[g_{S}(\bm{S}_{t},a_{\text{diff}},\bm{U}_{t})=\begin{bmatrix}S_{t,1}-S_{t,2}\\ 0\end{bmatrix}+\bm{U}_{t}\quad\text{and}\quad g_{S}(\bm{S}_{t},a_{\text{null}},\bm{U}_{t})=\begin{bmatrix}S_{t,1}\\ 0\end{bmatrix}+\bm{U}_{t}.\] (12)

Moreover, assume that the reward function is given by

\[\begin{split} R(\bm{S}_{t},a_{\text{diff}})=R(\bm{S}_{t},a_{ \text{null}})=&-\max\left(0,S_{t,1}-\frac{sum(\mathcal{V})}{2}-S_{t,2} \frac{sum(\mathcal{V})}{2}\right)\\ &-\max\left(0,\frac{sum(\mathcal{V})}{2}-S_{t,1}-S_{t,2}\frac{ sum(\mathcal{V})}{2}\right),\end{split}\] (13)

where \(sum(\mathcal{V})\) is the sum of all elements \(\sum_{i=1}^{B}v_{i}\). Note that, the SCM \(\mathcal{C}\) defined above is Lipschitz-continuous as suggested by the following Lemma (refer to Appendix D.1.1 for a proof).

**Lemma 10**.: _The SCM \(\mathcal{C}\) defined by Equations 12, 13 is Lipschitz-continuous according to Definition 1._

Now, assume that the counterfactual action sequence can differ in an arbitrary number of actions from the action sequence in the observed episode \(\tau\), _i.e._, \(k=T\) and, let the observed action sequence be such that \(a_{t}=a_{\text{null}}\) for \(t\in\{0,\ldots,T-1\}\). Lastly, let the initial observed state be \(\bm{s}_{0}=[0,v_{1}]\), the observed states \(\{\bm{s}_{t}\}_{t=1}^{T-2}\) be such that \(\bm{s}_{t}=\left[\sum_{i=1}^{t}v_{i},v_{t+1}\right]\) for \(t\in\{1,\ldots,T-2\}\) and the last observed state be \(\bm{s}_{T-1}=[sum(\mathcal{V}),0]\). Then, it is easy to see that the noise variables \(\bm{U}_{t}\) have posterior distributions with a point mass on the respective values

\[\bm{u}_{t}=\begin{bmatrix}v_{t+1}\\ v_{t+2}\end{bmatrix}\text{ for }t\in\{0,\ldots,T-3\}\qquad\text{and}\qquad\bm{u}_{T-2}= \begin{bmatrix}v_{T-1}\\ 0\end{bmatrix}.\]

Note that, for all \(t\in\{1,\ldots,T-2\}\), we have \(0\leq s_{t,1}<sum(\mathcal{V})\) and \(s_{t,2}\geq 1\), hence the immediate reward according to Eq. 13 is equal to \(0\). Consequently, the outcome of the observed episode \(\tau\) is \(o^{+}(\tau)=R(\bm{s}_{T-1},a_{\text{null}})=-\max(0,\frac{sum(\mathcal{V})}{2 })-\max(0,-\frac{sum(\mathcal{V})}{2})=-\frac{sum(\mathcal{V})}{2}\).

Next, we will characterize the counterfactual outcome \(o(\tau^{\prime})\) of a counterfactual episode \(\tau^{\prime}\) with a sequence of states \(\{\bm{s}^{\prime}_{t}\}_{t=0}^{T-1}\) resulting from an alternative sequence of actions \(\{a^{\prime}_{t}\}_{t=0}^{T-1}\). Let \(\mathcal{D}^{\prime}_{t}\), \(\mathcal{N}^{\prime}_{t}\) denote the set of time steps until time \(t\), where the actions taken in a counterfactual episode \(\tau^{\prime}\) are \(a_{\text{diff}}\) and \(a_{\text{null}}\) respectively. Formally, \(\mathcal{D}^{\prime}_{t}=\{t^{\prime}\in\{0,\ldots,t\}:a^{\prime}_{t^{\prime}}= a_{\text{diff}}\}\), \(\mathcal{N}^{\prime}_{t}=\{t^{\prime}\in\{0,\ldots,t\}:a^{\prime}_{t^{\prime}}= a_{\text{null}}\}\). Then, as an intermediate result, we get the following Lemma (refer to Appendix D.1.2 for a proof).

**Lemma 11**.: _It holds that \(s^{\prime}_{t,1}=\sum_{t^{\prime}\in\mathcal{N}^{\prime}_{t-1}}v_{t^{\prime}+1}\) for all \(t\in\{1,\ldots T-1\}\)._

Following from that, we get that \(0\leq s^{\prime}_{t,1}\leq sum(\mathcal{V})\) for all \(t\in\{1,\ldots,T-1\}\). Moreover, we can observe that the transition mechanism given in Eq. 12 is such that \(g_{S,2}(\bm{S}_{t},A_{t},U_{t,2})=U_{t,2}\) for all \(t\in\{0,\ldots,T-2\}\), independently of \(\bm{S}_{T}\) and \(A_{t}\). Therefore, it holds that \(s^{\prime}_{t,2}=u_{t-1,2}\geq 1\) for \(t\in\{1,\ldots,T-2\}\), and \(s^{\prime}_{0,2}=s_{0,2}=v_{1}\geq 1\). As a direct consequence, it is easy to see that \(R(\bm{s}^{\prime}_{t},a^{\prime}_{t})=0\) for all \(t\in\{0,\ldots,T-2\}\), and the counterfactual outcome is given by

\[o^{+}(\tau^{\prime})=R(\bm{s}^{\prime}_{T-1},a^{\prime}_{T-1}),\] (14)

In addition to that, we have that \(u_{T-2,2}=0\), hence

\[\bm{s}^{\prime}_{T-1}=\begin{bmatrix}\sum_{t\in\mathcal{N}^{\prime}_{T-2}}v_{t+ 1}\\ 0\end{bmatrix}\] (15)Now, we will show that, if we can find the action sequence \(\{a_{t}^{*}\}_{t=0}^{T-1}\) that gives the optimal counterfactual outcome \(o^{+}(\tau^{*})\) for the aforementioned instance in polynomial time, then we can make a decision about the corresponding instance of the partition problem, also in polynomial time. To this end, let \(\{\bm{s}_{t}^{*}\}_{t=0}^{T-1}\) be the sequence of states in the optimal counterfactual realization and, let \(\mathcal{D}_{T-2}^{*}=\{t\in\{0,\dots,T-2\}:a_{t}^{*}=a_{\text{diff}}\}\), \(\mathcal{N}_{T-2}^{*}=\{t^{\prime}\in\{0,\dots,T-2\}:a_{t^{\prime}}^{*}=a_{ \text{null}}\}\).

From Eq. 14, we get that the optimal counterfactual outcome is \(o^{+}(\tau^{*})=R(\bm{s}_{T-1}^{*},a_{T-1}^{*})\), and it is easy to see that the reward function given in Eq. 13 is always less or equal than zero. If \(o(\tau^{*})=0\), it has to hold that

\[\max\left(0,s_{T-1,1}^{*}-\frac{sum(\mathcal{V})}{2}-s_{T-1,2}^{ *}\frac{sum(\mathcal{V})}{2}\right)=\\ \max\left(0,\frac{sum(\mathcal{V})}{2}-s_{T-1,1}^{*}-s_{T-1,2}^{ *}\frac{sum(\mathcal{V})}{2}\right)=0\stackrel{{(*)}}{{\Rightarrow}}\\ \left(\sum_{t\in\mathcal{N}_{T-2}^{*}}v_{t+1}\right)-\frac{sum( \mathcal{V})}{2}\leq 0\quad\text{and}\quad\frac{sum(\mathcal{V})}{2}-\left(\sum_{t \in\mathcal{N}_{T-2}^{*}}v_{t+1}\right)\leq 0\Rightarrow\\ \sum_{t\in\mathcal{N}_{T-2}^{*}}v_{t+1}=\frac{sum(\mathcal{V})}{ 2},\]

where \((*)\) follows from Eq. 15. As a consequence, the subsets \(\mathcal{V}_{1}=\{v_{i}:i-1\in\mathcal{N}_{T-2}^{*}\}\) and \(\mathcal{V}_{2}=\{v_{i}:i-1\in\mathcal{D}_{T-2}^{*}\}\) partition \(\mathcal{V}\) and their sums are equal.

On the other hand, if \(o^{+}(\tau^{*})<0\), as we will show, there is no partition of \(\mathcal{V}\) into two sets with equal sums. For the sake of contradiction, assume there are two sets \(\mathcal{V}_{1},\mathcal{V}_{2}\) that partition \(\mathcal{V}\), with \(sum(\mathcal{V}_{1})=sum(\mathcal{V}_{2})=sum(\mathcal{V})/2\), and let \(\mathcal{N}_{T-2}^{\prime}=\{t\in\{0,\dots,T-2\}:v_{t+1}\in\mathcal{V}_{1}\}\) and \(\mathcal{D}_{T-2}^{\prime}=\{t\in\{0,\dots,T-2\}:v_{t+1}\in\mathcal{V}_{2}\}\). Then, consider the counterfactual episode \(\tau^{\prime}\) with an action sequence \(\{a_{t}^{\prime}\}_{t=0}^{T-1}\) such that its elements take values \(a_{\text{null}}\) and \(a_{\text{diff}}\) based on the sets \(\mathcal{N}_{T-2}^{\prime},\mathcal{D}_{T-2}^{\prime}\) respectively, with \(a_{T-1}^{\prime}\) taking an arbitrary value. It is easy to see that

\[o^{+}(\tau^{\prime}) =R(\bm{s}_{T-1}^{\prime},a_{T-1}^{\prime})\] \[=R\left(\begin{bmatrix}\sum_{t\in\mathcal{N}_{T-2}^{\prime}}v_{t+ 1}\\ 0\end{bmatrix},a_{T-1}^{\prime}\right)\] \[=-\max\left(0,\sum_{t\in\mathcal{N}_{T-2}^{\prime}}v_{t+1}-\frac {sum(\mathcal{V})}{2}\right)-\max\left(0,\frac{sum(\mathcal{V})}{2}-\sum_{t \in\mathcal{N}_{T-2}^{\prime}}v_{t+1}\right)\] \[=-\max\left(0,\frac{sum(\mathcal{V})}{2}-\frac{sum(\mathcal{V})} {2}\right)-\max\left(0,\frac{sum(\mathcal{V})}{2}-\frac{sum(\mathcal{V})}{2}\right)\] \[=0>o^{+}(\tau^{*}),\]

which is a contradiction. This step concludes the reduction and, therefore, the problem given in Eq. 9 cannot be solved in polynomial time, unless \(P=NP\).

#### d.1.1 Proof of Lemma 10

It is easy to see that, for all \(\bm{u}\in\mathcal{U}\) and for all \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), the function \(g_{S}(\bm{S}_{t},a_{\text{null}},\bm{u})\) satisfies \(\left\|g_{S}(\bm{s},a_{\text{null}},\bm{u})-g_{S}(\bm{s}^{\prime},a_{\text{ null}},\bm{u})\right\|\leq\left\|\bm{s}-\bm{s}^{\prime}\right\|\), and therefore \(K_{a_{\text{null}},\bm{u}}=1\) satisfies Definition 1. For the case of \(A_{t}=a_{\text{diff}}\), we have that

\[\left\|g_{S}(\bm{s},a_{\text{diff}},\bm{u})-g_{S}(\bm{s}^{\prime },a_{\text{diff}},\bm{u})\right\|=\left\|\begin{bmatrix}s_{1}-s_{2}\\ 0\end{bmatrix}-\begin{bmatrix}s_{1}^{\prime}-s_{2}^{\prime}\\ 0\end{bmatrix}\right\|=\left\|\begin{bmatrix}(s_{1}-s_{1}^{\prime})+(s_{2}^{ \prime}-s_{2})\\ 0\end{bmatrix}\right\|\\ =\left|(s_{1}-s_{1}^{\prime})+(s_{2}^{\prime}-s_{2})\right|\leq\left|s_{1}-s_{ 1}^{\prime}\right|+\left|s_{2}-s_{2}^{\prime}\right|,\]

and therefore \(\left\|g_{S}(\bm{s},a_{\text{diff}},\bm{u})-g_{S}(\bm{s}^{\prime},a_{\text{ diff}},\bm{u})\right\|^{2}\leq(s_{1}-s_{1}^{\prime})^{2}+(s_{2}-s_{2}^{\prime})^{2}+2 |s_{1}-s_{1}^{\prime}||s_{2}-s_{2}^{\prime}|\). We also have that

\[\sqrt{2}\left\|\bm{s}-\bm{s}^{\prime}\right\|=\sqrt{2}\sqrt{(s_{1}-s_{1}^{ \prime})^{2}+(s_{2}-s_{2}^{\prime})^{2}}\Rightarrow 2\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}=2(s_{1}-s_{1}^{ \prime})^{2}+2(s_{2}-s_{2}^{\prime})^{2}.\]By combining these, we get

\[2\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}- \left\|g_{S}(\bm{s},a_{\text{diff}},\bm{u})-g_{S}(\bm{s}^{\prime},a_{ \text{diff}},\bm{u})\right\|^{2}\geq(s_{1}-s_{1}^{\prime})^{2}+(s_{2}-s_{2}^{ \prime})^{2}-2|s_{1}-s_{1}^{\prime}||s_{2}-s_{2}^{\prime}|\Rightarrow\] \[2\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}-\left\|g_{S}(\bm{s},a_ {\text{diff}},\bm{u})-g_{S}(\bm{s}^{\prime},a_{\text{diff}},\bm{u})\right\|^{2} \geq(|s_{1}-s_{1}^{\prime}|-|s_{2}-s_{2}^{\prime}|)^{2}\geq 0.\]

Hence, we can easily see that \(K_{a_{\text{diff}},\bm{u}}=\sqrt{2}\) satisfies Definition 1.

Next, we need to show that, for all \(a\in\mathcal{A}\) there exists a \(C_{a}\in\mathbb{R}_{+}\) such that, for all \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), it holds \(|R(\bm{s},a)-R(\bm{s}^{\prime},a)|\leq C_{a}\left\|\bm{s}-\bm{s}^{\prime}\right\|\). Note that, to show that a function of the form \(\max(0,f(\bm{s}))\) with \(f:\mathbb{R}^{2}\to\mathbb{R}\) is Lipschitz continuous, it suffices to show that \(f(\bm{s})\) is Lipschitz continuous, since the function \(\max(0,x)\) with \(x\in\mathbb{R}\) has a Lipschitz constant equal to \(1\).

We start by showing that the function \(f(\bm{s})=s_{1}-\alpha-s_{2}\cdot\alpha\) is Lipschitz continuous, where \(\alpha=sum(\mathcal{V})/2\) is a positive constant. For an arbitrary pair \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), we have that

\[|f(\bm{s})-f(\bm{s}^{\prime})|=|s_{1}-s_{1}^{\prime}-\alpha(s_{2}-s_{2}^{ \prime})|\leq|s_{1}-s_{1}^{\prime}|+\alpha|s_{2}-s_{2}^{\prime}|\Rightarrow\]

\[|f(\bm{s})-f(\bm{s}^{\prime})|^{2}\leq(s_{1}-s_{1}^{\prime})^{2}+(s_{2}-s_{2}^ {\prime})^{2}+2\alpha|s_{1}-s_{1}^{\prime}||s_{2}-s_{2}^{\prime}|.\]

We also have that

\[\sqrt{1+\alpha}\left\|\bm{s}-\bm{s}^{\prime}\right\|=\sqrt{1+ \alpha}\sqrt{(s_{1}-s_{1}^{\prime})^{2}+(s_{2}-s_{2}^{\prime})^{2}}\Rightarrow\] \[(1+\alpha)\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}=(1+\alpha)(s_ {1}-s_{1}^{\prime})^{2}+(1+\alpha)(s_{2}-s_{2}^{\prime})^{2}\]

By combining these, we get

\[(1+\alpha)\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}-|f(\bm{s})-f(\bm{s}^{ \prime})|^{2}\geq\alpha(s_{1}-s_{1}^{\prime})^{2}+\alpha(s_{2}-s_{2}^{\prime} )^{2}-2\alpha|s_{1}-s_{1}^{\prime}||s_{2}-s_{2}^{\prime}|\Rightarrow\] \[(1+\alpha)\left\|\bm{s}-\bm{s}^{\prime}\right\|^{2}-|f(\bm{s})-f( \bm{s}^{\prime})|^{2}\geq\alpha\left(|s_{1}-s_{1}^{\prime}|-|s_{2}-s_{2}^{ \prime}|\right)^{2}\geq 0.\]

Hence, we arrive to \(|f(\bm{s})-f(\bm{s}^{\prime})|\leq\sqrt{1+\alpha}\left\|\bm{s}-\bm{s}^{\prime}\right\|\), and the function \(f\) is Lipschitz continuous. It is easy to see that the function \(\phi(\bm{s})=\alpha-s_{1}-s_{2}\cdot\alpha\) is also Lipschitz continuous with the proof being almost identical. As a direct consequence, the reward function given in Equation 13 satisfies Definition 1 with \(C_{a_{\text{null}}}=C_{a_{\text{diff}}}=2\sqrt{1+\frac{sum(\mathcal{V})}{2}}\). This concludes the proof of the lemma.

#### d.1.2 Proof of Lemma 11

We will prove the lemma by induction. For the base case of \(t=1\), we distinguish between the cases \(a_{0}^{\prime}=a_{\text{diff}}\) and \(a_{0}^{\prime}=a_{\text{null}}\). In the first case, we have \(s_{1,1}^{\prime}=u_{0,1}+s_{0,1}-s_{0,2}=v_{1}+0-v_{1}=0\) and \(\mathcal{N}_{0}^{\prime}=\emptyset\) and, therefore, the statement holds. In the second case, we have \(s_{1,1}^{\prime}=u_{0,1}+s_{0,1}=v_{1}+0=v_{1},\mathcal{N}_{0}^{\prime}=\{0\}\) and \(\sum_{t^{\prime}\in\mathcal{N}_{0}^{\prime}}v_{t^{\prime}+1}=v_{1}\). Therefore, the statement also holds.

For the inductive step (\(t>1\)), we assume that \(s_{t-1,1}^{\prime}=\sum_{t^{\prime}\in\mathcal{N}_{t-2}^{\prime}}v_{t^{\prime}+1}\) and we will show that \(s_{t,1}^{\prime}=\sum_{t^{\prime}\in\mathcal{N}_{t-1}^{\prime}}v_{t^{\prime}+1}\). Again, we distinguish between the cases \(a_{t-1}^{\prime}=a_{\text{diff}}\) and \(a_{t-1}^{\prime}=a_{\text{null}}\). However, note that, in both cases, \(s_{t-1,2}^{\prime}=u_{t-2,2}+0=v_{t}\). Therefore, in the case of \(a_{t-1}^{\prime}=a_{\text{diff}}\), we get

\[s_{t,1}^{\prime}=u_{t-1,1}+s_{t-1,1}^{\prime}-s_{t-1,2}^{\prime}=v_{t}+\sum_{t^ {\prime}\in\mathcal{N}_{t-2}^{\prime}}v_{t^{\prime}+1}-v_{t}=\sum_{t^{\prime} \in\mathcal{N}_{t-2}^{\prime}}v_{t^{\prime}+1}=\sum_{t^{\prime}\in\mathcal{N} _{t-1}^{\prime}}v_{t^{\prime}+1},\]

where the last equation holds because \(a_{t-1}^{\prime}=a_{\text{diff}}\) and, therefore, \(\mathcal{N}_{t-1}^{\prime}=\mathcal{N}_{t-2}^{\prime}\). In the case of \(a_{t-1}^{\prime}=a_{\text{null}}\), we get

\[s_{t,1}^{\prime}=u_{t-1,1}+s_{t-1,1}^{\prime}=v_{t}+\sum_{t^{\prime}\in \mathcal{N}_{t-2}^{\prime}}v_{t^{\prime}+1}=\sum_{t^{\prime}\in\mathcal{N}_{t-1 }^{\prime}}v_{t^{\prime}+1},\]

where the last equation holds because \(a_{t-1}^{\prime}=a_{\text{null}}\) and, therefore, \(\mathcal{N}_{t-1}^{\prime}=\mathcal{N}_{t-2}^{\prime}\cup\{t-1\}\).

### Proof of Lemma 5

We will prove the proposition by induction, starting from the base case, where \(t=T-1\). First, Let \(t=T-1\) and \(l=k\). It is easy to see that, if the process is at a state \(\bm{s}\in\mathcal{S}\) in the last time step with no action changes left, the best reward that can be achieved is \(R(\bm{s},a_{T-1})\), as already discussed after Eq. 10. Therefore, it holds that \(|V_{\tau}(\bm{s},k,T-1)-V_{\tau}(\bm{s}^{\prime},k,T-1)|=|R(\bm{s},a_{T-1})-R(\bm {s}^{\prime},a_{T-1})|\leq C_{a_{T-1}}\left\|\bm{s}-\bm{s}^{\prime}\right\|\leq C \left\|\bm{s}-\bm{s}^{\prime}\right\|\), where the last step holds because \(C=\max_{a\in\mathcal{A}}C_{a}\). Now, consider the case of \(t=T-1\) with \(l\) taking an arbitrary value in \(\{0,\ldots,k-1\}\). Let \(\bm{s},\bm{s}^{\prime}\) be two states in \(\mathcal{S}\) and \(a^{*}\) be the action that gives the maximum immediate reward at state \(\bm{s}\), that is, \(a^{*}=\operatorname*{argmax}_{a\in\mathcal{A}}\{R(\bm{s},a)\}\). Then, we get

\[|V_{\tau}(\bm{s},l,T-1)-V_{\tau}(\bm{s}^{\prime},l,T-1)| = |\max_{a\in\mathcal{A}}\{R(\bm{s},a)\}-\max_{a\in\mathcal{A}}\{R( \bm{s}^{\prime},a)\}|\] \[\overset{(*)}{\leq} |R(\bm{s},a^{*})-R(\bm{s}^{\prime},a^{*})|\leq C_{a^{*}}\left\| \bm{s}-\bm{s}^{\prime}\right\|\leq C\left\|\bm{s}-\bm{s}^{\prime}\right\|,\]

where \((*)\) follows from the fact that \(R(\bm{s}^{\prime},a^{*})\leq\max_{a\in\mathcal{A}}\{R(\bm{s}^{\prime},a)\}\). Therefore, for any \(l\in\{0,\ldots,k\}\) and \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), it holds that \(|V_{\tau}(\bm{s},l,T-1)-V_{\tau}(\bm{s}^{\prime},l,T-1)|\leq L_{T-1}\left\| \bm{s}-\bm{s}^{\prime}\right\|\), where \(L_{T-1}=C\).

Now, we will proceed to the induction step. Let \(t<T-1\), \(l<k\) and, as an inductive hypothesis, assume that \(L_{t+1}\in\mathbb{R}_{+}\) as defined in Lemma 5 is such that, for all \(l\in\{0,\ldots,k\}\) and \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), it holds that \(|V_{\tau}(\bm{s},l,t+1)-V_{\tau}(\bm{s}^{\prime},l,t+1)|\leq L_{t+1}\left\| \bm{s}-\bm{s}^{\prime}\right\|\). Additionally, let \((\bm{s}_{a},l_{a}),(\bm{s}^{\prime}_{a},l_{a})\) denote the enhanced states that follow from \((\bm{s},l),(\bm{s}^{\prime},l)\) after taking an action \(a\), _i.e._, \((\bm{s}_{a},l_{a})=F^{+}_{\tau,t}\left(\left(\bm{s},l\right),a\right)\) and \((\bm{s}^{\prime}_{a},l_{a})=F^{+}_{\tau,t}\left(\left(\bm{s}^{\prime},l\right),a\right)\). Lastly, let \(a^{*}\) be the action that maximizes the future total reward starting from state \(\bm{s}\), _i.e._, \(a^{*}=\operatorname*{argmax}_{a\in\mathcal{A}}\{R(\bm{s},a)+V_{\tau}\left( \bm{s}_{a},l_{a},t+1\right)\}\). Then, we have that

\[|V_{\tau}(\bm{s},l,t) - V_{\tau}(\bm{s}^{\prime},l,t)|\] \[= |\max_{a\in\mathcal{A}}\{R(\bm{s},a)+V_{\tau}\left(\bm{s}_{a},l_ {a},t+1\right)\}-\max_{a\in\mathcal{A}}\{R(\bm{s}^{\prime},a)+V_{\tau}\left( \bm{s}^{\prime}_{a},l_{a},t+1\right)\}|\] \[\overset{(*)}{\leq} |R(\bm{s},a^{*})+V_{\tau}\left(\bm{s}_{a^{*}},l_{a^{*}},t+1\right)- R(\bm{s}^{\prime},a^{*})-V_{\tau}\left(\bm{s}^{\prime}_{a^{*}},l_{a^{*}},t+1 \right)|\] \[\leq |R(\bm{s},a^{*})-R(\bm{s}^{\prime},a^{*})|+|V_{\tau}\left(\bm{s}_ {a^{*}},l_{a^{*}},t+1\right)-V_{\tau}\left(\bm{s}^{\prime}_{a^{*}},l_{a^{*}},t +1\right)|\] \[\overset{(**)}{\leq} C_{a^{*}}\left\|\bm{s}-\bm{s}^{\prime}\right\|+L_{t+1}\left\| \bm{s}_{a^{*}}-\bm{s}^{\prime}_{a^{*}}\right\|\] \[\overset{(**)}{\leq} C\left\|\bm{s}-\bm{s}^{\prime}\right\|+L_{t+1}K_{a^{*}}\left\| \bm{s}-\bm{s}^{\prime}\right\|\] \[= \left(C+L_{t+1}K_{\bm{u}_{t}}\right)\left\|\bm{s}-\bm{s}^{\prime} \right\|=L_{t}\left\|\bm{s}-\bm{s}^{\prime}\right\|.\]

In the above, \((*)\) holds due to \(R(\bm{s}^{\prime},a^{*})+V_{\tau}\left(\bm{s}^{\prime}_{a^{*}},l_{a^{*}},t+1 \right)\leq\max_{a\in\mathcal{A}}\{R(\bm{s}^{\prime},a)+V_{\tau}\left(\bm{s}^{ \prime}_{a},l_{a},t+1\right)\}\), \((**)\) follows from the inductive hypothesis, and \((**)\) holds because \(C=\max_{a\in\mathcal{A}}C_{a}\) and \(K_{\bm{u}_{t}}=\max_{a\in\mathcal{A}}K_{a,u_{t}}\). It is easy to see that, similar arguments hold for the simple case of \(l=k\), therefore, we omit the details. This concludes the inductive step and the proof of Lemma 5.

### Proof of Proposition 6

We will prove the proposition by induction, starting from the base case, where \(t=T-1\). If \(t=T-1\), the algorithm initializes \(\hat{V}_{\tau}(\bm{s},l,T-1)\) to \(\max_{a\in\mathcal{A}}R(\bm{s},a)\) for all \(\bm{s}\in\mathcal{S}_{l}\), \(l\in\{0,\ldots,k-1\}\) and \(\hat{V}_{\tau}(\bm{s},k,T-1)\) to \(R(\bm{s},a_{T-1})\). It is easy to see that those values are optimal, as already discussed after Eq. 10. Therefore, the base case \(\hat{V}_{\tau}(\bm{s},l,T-1)\geq V_{\tau}(\bm{s},l,T-1)\) follows trivially.

Now, we will proceed to the induction step. Let \(t<T-1\) and, as an inductive hypothesis, assume that \(\hat{V}_{\tau}(\bm{s},l,t+1)\geq V_{\tau}(\bm{s},l,t+1)\) for all \(\bm{s}\in\mathcal{S}_{l}\), \(l\in\{0,\ldots,k\}\). Our goal is to show that \(\hat{V}_{\tau}(\bm{s},l,t)\geq V_{\tau}(\bm{s},l,t)\) for all \(\bm{s}\in\mathcal{S}_{l}\), \(l\in\{0,\ldots,k\}\). First, let \(l<k\). For a given point \(\bm{s}\in\mathcal{S}_{l}\), Algorithm 1 finds the next state \(\bm{s}_{a}\) that would have occurred by taking each action \(a\), _i.e._, \((\bm{s}_{a},l_{a})=F^{+}_{\tau,t}\left(\left(\bm{s},l\right),a\right)\), and it computes the associated value \(V_{a}=\min_{\bm{s}_{l}\in\mathcal{S}_{l}}\{\hat{V}_{\tau}(\bm{s}_{l},l_{a},t+1)+ L_{t+1}\left\|\bm{s}_{l}-\bm{s}_{a}\right\|\}\). Then, it simply sets \(\hat{V}_{\tau}(\bm{s},l,t)\) equal to \(\max_{a\in\mathcal{A}}\left\{R(\bm{s},a)+V_{a}\right\}\). We have that

\[V_{a} =\min_{\bm{s}_{\uparrow}\in\mathcal{S}_{\uparrow}}\{\hat{V}_{\tau} (\bm{s}_{\uparrow},l_{a},t+1)+L_{t+1}\left\|\bm{s}_{\uparrow}-\bm{s}_{a} \right\|\}\] \[\overset{(*)}{\geq}\min_{\bm{s}_{\uparrow}\in\mathcal{S}_{ \uparrow}}\{V_{\tau}(\bm{s}_{\uparrow},l_{a},t+1)+L_{t+1}\left\|\bm{s}_{ \uparrow}-\bm{s}_{a}\right\|\}\] \[\overset{(**)}{\geq}\min_{\bm{s}_{\uparrow}\in\mathcal{S}_{ \uparrow}}\{V_{\tau}(\bm{s}_{a},l_{a},t+1)\}\] \[=V_{\tau}(\bm{s}_{a},l_{a},t+1),\]

where \((*)\) follows from the inductive hypothesis, and \((**)\) is a consequence of Lemma 5. Then, we get

\[\hat{V}_{\tau}(\bm{s},l,t)=\max_{a\in\mathcal{A}}\left\{R(\bm{s},a)+V_{a} \right\}\geq\max_{a\in\mathcal{A}}\{R(\bm{s},a)+V_{\tau}(\bm{s}_{a},l_{a},t+1) \}=V_{\tau}(\bm{s},l,t).\]

Additionally, when \(l=k\), we have \(\hat{V}_{\tau}(\bm{s},k,t)=R(\bm{s},a_{t})+\min_{\bm{s}_{\uparrow}\in\mathcal{ S}_{\uparrow}}\{\hat{V}_{\tau}(\bm{s}_{\uparrow},k,t+1)+L_{t+1}\left\|\bm{s}_{ \uparrow}-\bm{s}_{a_{t}}\right\|\}\) and \(V_{\tau}(\bm{s},k,t)=R(\bm{s},a_{t})+V_{\tau}(\bm{s}_{a_{t}},k,t+1)\). Therefore, the proof for \(\hat{V}_{\tau}(\bm{s},k,t)\geq V_{\tau}(\bm{s},k,t)\) is almost identical.

### Proof of Theorem 7

We start from the case where \(t=T-1\). Let \(v=(\bm{s},l,T-1)\) and, consider an edge associated with action \(a^{*}\) connecting \(v\) to the goal node \(v_{T}=(\bm{s}_{\emptyset},k,T)\) that carries a reward \(R(\bm{s},a^{*})\). Then, we have

\[\hat{V}_{\tau}(\bm{s},l,T-1)=\max_{a\in\mathcal{A}^{\prime}}R(\bm{s},a)\geq R (\bm{s},a^{*})+0=R(\bm{s},a^{*})+\hat{V}_{\tau}(\bm{s}_{\emptyset},k,T),\]

and the base case holds.

For the more general case, where \(t<T-1\), we first establish the following intermediate result, whose proof is given in Appendix D.4.1.

**Lemma 12**.: _For every \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\), \(l\in\{0,\ldots,k\}\), \(t\in\{0,\ldots,T-1\}\), it holds that \(|\hat{V}_{\tau}(\bm{s},l,t)-\hat{V}_{\tau}(\bm{s}^{\prime},l,t)|\leq L_{t} \left\|\bm{s}-\bm{s}^{\prime}\right\|\), where \(L_{t}\) is as defined in Lemma 5._

That said, consider an edge associated with an action \(a^{*}\) connecting node \(v=(\bm{s},l,t)\) to node \(v_{a^{*}}=(\bm{s}_{a^{*}},l_{a^{*}},t+1)\). Then, we have

\[\hat{V}_{\tau}(\bm{s},l,t) =\max_{a\in\mathcal{A}^{\prime}}\left\{R(\bm{s},a)+\min_{\bm{s}_{ \uparrow}\in\mathcal{S}_{\uparrow}}\left\{\hat{V}_{\tau}(\bm{s}_{\uparrow},l _{a},t+1)+L_{t+1}\left\|\bm{s}_{\uparrow}-\bm{s}_{a}\right\|\right\}\right\}\] \[\geq R(\bm{s},a^{*})+\min_{\bm{s}_{\uparrow}\in\mathcal{S}_{ \uparrow}}\left\{\hat{V}_{\tau}(\bm{s}_{\uparrow},l_{a^{*}},t+1)+L_{t+1} \left\|\bm{s}_{\uparrow}-\bm{s}_{a^{*}}\right\|\right\}\] \[\geq R(\bm{s},a^{*})+\min_{\bm{s}_{\uparrow}\in\mathcal{S}_{ \uparrow}}\left\{\hat{V}_{\tau}(\bm{s}_{a^{*}},l_{a^{*}},t+1)\right\}\] \[=R(\bm{s},a^{*})+\hat{V}_{\tau}(\bm{s}_{a^{*}},l_{a^{*}},t+1).\]

That concludes the proof and, therefore, the heuristic function \(\hat{V}_{\tau}\) is consistent.

#### d.4.1 Proof of Lemma 12

Without loss of generality, we will assume that \(l<k\), since the proof for the case of \(l=k\) is similar and more straightforward. We start from the case where \(t=T-1\) and, for two states \(\bm{s},\bm{s}^{\prime}\in\mathcal{S}\) we have

\[|\hat{V}_{\tau}(\bm{s},l,T-1)-\hat{V}_{\tau}(\bm{s}^{\prime},l,T-1)|=\left| \max_{a\in\mathcal{A}}R(\bm{s},a)-\max_{a\in\mathcal{A}}R(\bm{s}^{\prime},a)\right|\] \[=|V_{\tau}(\bm{s},l,T-1)-V_{\tau}(\bm{s}^{\prime},l,T-1)|\leq C \left\|\bm{s}-\bm{s}^{\prime}\right\|=L_{T-1}\left\|\bm{s}-\bm{s}^{\prime} \right\|,\]

where the last inequality follows from Lemma 5.

Now, consider the case \(t<T-1\), and let \((\bm{s}_{a},l_{a})\) denote the enhanced state that follows from \((\bm{s},l)\) after taking an action \(a\) at time \(t\), _i.e._, \((\bm{s}_{a},l_{a})=F_{\tau,t}^{+}\left(\left(\bm{s},l\right),a\right)\). Then, we have

\[|\hat{V}_{\tau}(\bm{s},l,t)-\hat{V}_{\tau}(\bm{s}^{\prime},l,t)|\\ =\Bigg{|}\max_{a\in\mathcal{A}}\left\{R(\bm{s},a)+\min_{\bm{s}_{ \dagger}\in\mathcal{S}_{\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a}, t+1)+L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a}\right\|\right\}\right\}\\ -\max_{a\in\mathcal{A}}\left\{R(\bm{s}^{\prime},a)+\min_{\bm{s}_ {\dagger}\in\mathcal{S}_{\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a },t+1)+L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a}^{\prime}\right\|\right\} \right\}\Bigg{|}.\] (16)

Let \(a^{*}\) be the action \(a\in\mathcal{A}\) that maximizes the first part of the above subtraction, _i.e._,

\[a^{*}=\operatorname*{argmax}_{a\in\mathcal{A}}\left\{R(\bm{s},a)+\min_{\bm{s} _{\dagger}\in\mathcal{S}_{\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a },t+1)+L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a}\right\|\right\}\right\}\]

Then, Eq. 16 implies that

\[|\hat{V}_{\tau}(\bm{s},l,t)-\hat{V}_{\tau}(\bm{s}^{\prime},l,t)| \leq\Bigg{|}R(\bm{s},a^{*})+\min_{\bm{s}_{\dagger}\in\mathcal{S}_ {\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a^{*}},t+1)+L_{t+1}\left\| \bm{s}_{\dagger}-\bm{s}_{a^{*}}\right\|\right\}\] \[\qquad-R(\bm{s}^{\prime},a^{*})-\min_{\bm{s}_{\dagger}\in \mathcal{S}_{\dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a^{*}},t+1)+ L_{t+1}\left\|\bm{s}_{\dagger}-\bm{s}_{a^{*}}^{\prime}\right\|\right\}\Bigg{|}\] \[\leq|R(\bm{s},a^{*})-R(\bm{s}^{\prime},a^{*})|\] \[\qquad+\Bigg{|}\min_{\bm{s}_{\dagger}\in\mathcal{S}_{\dagger}} \left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a^{*}},t+1)+L_{t+1}\left\|\bm{s}_{ \dagger}-\bm{s}_{a^{*}}\right\|\right\}\] \[\qquad\qquad-\min_{\bm{s}_{\dagger}\in\mathcal{S}_{\dagger}} \left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a^{*}},t+1)+L_{t+1}\left\|\bm{s}_{ \dagger}-\bm{s}_{a^{*}}^{\prime}\right\|\right\}\Bigg{|}\] (17)

Now, let \(\tilde{\bm{s}}\) be the \(\bm{s}_{\dagger}\in\mathcal{S}_{\dagger}\) that minimizes the second part of the above subtraction, _i.e._,

\[\tilde{\bm{s}}=\operatorname*{argmin}_{\bm{s}_{\dagger}\in\mathcal{S}_{ \dagger}}\left\{\hat{V}_{\tau}(\bm{s}_{\dagger},l_{a^{*}},t+1)+L_{t+1}\left\| \bm{s}_{\dagger}-\bm{s}_{a^{*}}^{\prime}\right\|\right\}.\]

As a consequence and in combination with Eq. 17, we get

\[|\hat{V}_{\tau}(\bm{s},l,t)-\hat{V}_{\tau}(\bm{s}^{\prime},l,t)| \leq|R(\bm{s},a^{*})-R(\bm{s}^{\prime},a^{*})|\\ +\left|\hat{V}_{\tau}(\tilde{\bm{s}},l_{a^{*}},t+1)+L_{t+1}\left\| \tilde{\bm{s}}-\bm{s}_{a^{*}}\right\|\] \[\qquad-\hat{V}_{\tau}(\tilde{\bm{s}},l_{a^{*}},t+1)+L_{t+1} \left\|\tilde{\bm{s}}-\bm{s}_{a^{*}}^{\prime}\right\|\Bigg{|}\] \[=|R(\bm{s},a^{*})-R(\bm{s}^{\prime},a^{*})|+L_{t+1}\left\|\tilde{ \bm{s}}-\bm{s}_{a^{*}}^{\prime}\right\|-\left\|\tilde{\bm{s}}-\bm{s}_{a^{*}}^{ \prime}\right\|\] \[\overset{(*)}{\leq}C_{a^{*}}\left\|\bm{s}-\bm{s}^{\prime}\right\| +L_{t+1}\left\|\bm{s}_{a^{*}}-\bm{s}_{a^{*}}^{\prime}\right\|\] \[\overset{(**)}{\leq}C\left\|\bm{s}-\bm{s}^{\prime}\right\|+L_{t+1 }K_{\bm{u}_{t}}\left\|\bm{s}-\bm{s}^{\prime}\right\|=L_{t}\left\|\bm{s}-\bm{s} ^{\prime}\right\|,\]

where in \((*)\) we use the triangle inequality and the fact that the SCM \(\mathcal{C}\) is Lipschitz-continuous, and \((**)\) follows from Lemma 5.

``` Input: States \(\mathcal{S}\), actions \(\mathcal{A}\), observed action sequence \(\{a_{t}\}_{t=0}^{t=T-1}\), horizon \(T\), transition function \(F_{\tau,t}^{+}\), reward function \(R\), constraint \(k\), initial state \(\bm{s}_{0}\), heuristic function \(\hat{V}_{\tau}\). Initialize: node \(v_{0}\leftarrow\{\text{``tuple''}:(\bm{s}_{0},0,0)\text{,``rwd''}:0,\text{``par''}: Null,\text{``act''}:Null\}\), \(\text{stack}\ action\_sequence\leftarrow[\,]\) \(\text{queue}\ Q\leftarrow\{root\}\) \(\text{set}\ explored\leftarrow\emptyset\) whileTruedo \(v\leftarrow\operatorname*{argmax}_{v^{\prime}\in Q}\{v^{\prime}.rwd+\hat{V}_{ \tau}(v^{\prime}.tuple)\};\ Q\gets Q\setminus v\) ; /* Next node to visit */ if\(v.tuple=(*,*,T)\)then while\(v.par\neq Null\)do \(action\_sequence.push(v.act)\) ; /* Retrieve final action sequence */ \(v=v.par\)  end if return\(action\_sequence\)  end if \(explored\gets explored\cup\{v\}\) ; /* Set node \(v\) as explored */ if\(l=k\)then \(available\_actions\leftarrow\{a_{t}\}\) else \(available\_actions\leftarrow\mathcal{A}\)  end if for\(a\in available\_actions\)do \((\bm{s},l,t)\gets v.tuple\) \((\bm{s}_{a},l_{a})\gets F_{\tau,t}^{+}\left((\bm{s},l),a\right)\) ; /* Identify \(v\)'s children nodes */ \(v_{a}\leftarrow\{\text{``tuple''}:(\bm{s}_{a},l_{a},t+1)\text{,``rwd''}:v.rwd+R( \bm{s},a)\text{,``par''}:v,\text{``act''}:a\}\) if\(v_{a}\not\in Q\)and\(v_{a}\not\in explored\)then \(Q\gets Q\cup\{v_{a}\}\) ; /* Add them to the queue if unexplored */  end if  end for ```

**Algorithm 2**Graph search via \(A^{*}\)

## Appendix E A* algorithm

Algorithm 2 summarizes the step-by-step process followed by the \(A^{*}\) algorithm. Therein, we represent each node \(v\) by an object with \(4\) attributes: (i) the "tuple" \((\bm{s},l,t)\) of the node, (ii) the total reward "rwd" of the path that has led the search from the root node \(v_{0}\) to the node \(v\), (iii) the parent node "par" from which the search arrived to \(v\), and (iv) the action "act" associated with the edge connecting the node \(v\) with its parent. In addition to the queue of nodes to visit, the algorithm maintains a set of explored nodes, and it adds a new node to the queue only if it has not been previously explored. The algorithm terminates when the goal node is chosen to be visited, _i.e._, the "tuple" attribute of \(v\) has the format \((*,*,T)\), where \(*\) denotes arbitrary values. Once the goal node \(v_{T}\) has been visited, the algorithm reconstructs the action sequence that led from the root node \(v_{0}\) to the goal node and returns it as output.

## Appendix F Experimental evaluation of anchor set selection strategies

In this section, we benchmark the anchor set selection strategy presented in Section 4 against two alternative competitive strategies using the sepsis management dataset and the same experimental setup as in Section 5. More specifically, we consider the following anchor set selection strategies:

1. _MC-Lipschitz_: This is the strategy described in depth in Section 4, based on Monte Carlo simulations of counterfactual episodes under randomly sampled counterfactual action sequences. Notably, the time steps where each counterfactual action sequence differs from the observed one are sampled proportionally to the respective Lipschitz constant \(L_{t}\) of the SCM's transition mechanism. To ensure a fair comparison with other strategies, instead of controlling the number of sampled action sequences \(M\), we fix the desired size of the anchor set \(\mathcal{S}_{\dagger}\), and we repeatedly sample counterfactual action sequences until the specified size is met.
2. _MC-Uniform_: This strategy is a variant of the previous strategy where we sample the time steps where each counterfactual action sequence differs from the observed one uniformly at random, rather than biasing the sampling towards time steps with higher Lipschitz constants \(L_{t}\).
3. _Facility-Location_: Under this strategy, the anchor set is the solution to a minimax facility location problem defined using the observed available data. Let \(\mathcal{S}_{o}\) be the union of all state vectors observed in all episodes \(\tau\) in a given dataset. Then, we choose an anchor set \(\mathcal{S}_{\dagger}\subset\mathcal{S}_{o}\) of fixed size \(|\mathcal{S}_{\dagger}|=b\), such that the maximum distance of any point in \(\mathcal{S}_{o}\) to its closest point in \(\mathcal{S}_{\dagger}\) is minimized. Here, the rationale is that counterfactual states resulting from counterfactual action sequences for one observed episode are likely to be close to the observed states of some other episode in the data. Formally, \[\mathcal{S}_{\dagger}=\operatorname*{argmin}_{\mathcal{S}^{\prime}\subset \mathcal{S}_{o};|\mathcal{S}^{\prime}|=b}\left\{\max_{\bm{s}\in\mathcal{S}_{o} }\min_{\bm{s}^{\prime}\in\mathcal{S}^{\prime}}\left\{||\bm{s}-\bm{s}^{\prime }||\right\}\right\}.\] (18) Although the above problem is known to be NP-Complete, we find a solution using the farthest-point clustering algorithm, which is known to have an approximation factor equal to \(2\) and runs in polynomial time. The algorithm starts by adding one point from \(\mathcal{S}_{o}\) to \(\mathcal{S}_{\dagger}\) at random. Then, it proceeds iteratively and, at each iteration, it adds to \(\mathcal{S}_{\dagger}\) the point from \(\mathcal{S}_{o}\) that is the furthest from all points already in \(\mathcal{S}_{\dagger}\), _i.e._, \(\mathcal{S}_{\dagger}=\mathcal{S}_{\dagger}\cup\bm{s}\), where \(\bm{s}=\max_{\bm{s}^{\prime}\in\mathcal{S}_{o}}\left\{\min_{\bm{s}_{\dagger} \in\mathcal{S}_{\dagger}}||\bm{s}^{\prime}-\bm{s}_{\dagger}||\right\}\). The algorithm terminates after \(b\) iterations.

**Results.** We compare the computational efficiency of our method under each of the above anchor set selection strategies for various values of the size of the anchor set \(|\mathcal{S}_{\dagger}|\). Figure 5 summarizes the results. We observe that the Facility-Location selection strategy performs rather poorly compared to the other two strategies, achieving an effective branching factor (EBF) higher than \(3\). In contrast, the MC-Lipschitz and MC-Uniform strategies achieve an EBF close to \(2\), which decreases rapidly as the size of the anchor set increases. Among these two strategies, the MC-Lipschitz strategy, which we use in our experiments in Section 5, achieves the lowest EBF.

Figure 5: Computational efficiency of our method, measured by means of the Effective Branching Factor (EBF), under three different anchor set selection strategies against the size of the anchor set \(\mathcal{S}_{\dagger}\). In all panels, we set \(L_{h}=1.0\), \(L_{\phi}=0.1\) and \(k=3\). Error bars indicate \(95\%\) confidence intervals over \(200\) executions of the \(A^{*}\) algorithm for \(200\) patients with horizon \(T=12\).

## Appendix G Additional details on the experimental setup

### Features and actions in the sepsis management dataset

As mentioned in Section 5, our state space is \(\mathcal{S}=\mathbb{R}^{D}\), where \(D=13\) is the number of features. We distinguish between three types of features: (i) demographic features, whose values remain constant across time, (ii) contextual features, for which we maintain their observed (and potentially varying) values throughout all counterfactual episodes and, (iii) time-varying features, whose counterfactual values are given by the SCM \(\mathcal{C}\). The list of features is as follows:

* Gender (demographic)
* Re-admission (demographic)
* Age (demographic)
* Mechanical ventilation (contextual)
* \(\text{FiO}_{2}\) (time-varying)
* \(\text{PaO}_{2}\) (time-varying)
* Platelet count (time-varying)
* Bilirubin (time-varying)
* Glasgow Coma Scale (time-varying)
* Mean arterial blood pressure (time-varying)
* Creatinine (time-varying)
* Urine output (time-varying)
* SOFA score (time-varying)

To define our set of actions \(\mathcal{A}\) we follow related work [6, 55, 57], and we consider \(25\) actions corresponding to \(5\times 5\) levels of administered vasopressors and intravenous fluids. Specifically, for both vasopressors and fluids, we find all non-zero values appearing in the data, and we divide them into \(4\) intervals based on the quartiles of the observed values. Then, we set the \(5\) levels to be the median values of the \(4\) intervals and \(0\). Table G.1 shows the resulting values of vasopressors and fluids.

### Additional details on the network architecture & training

We represent the location and scale functions \(h\) and \(\phi\) of the SCM \(\mathcal{C}\) using neural networks with \(1\) hidden layer, \(200\) hidden units and \(tanh\) activation functions. The mapping from a state \(\bm{s}\) and an action \(a\) to the hidden vector \(\bm{z}\) takes the form \(\bm{z}=tanh(W_{\bm{s}}\bm{s}+W_{a}\bm{a})\), where \(\bm{a}\) is a \(2\)-D vector representation of the respective action. The mapping from the hidden vector \(\bm{z}\) to the network's output is done via a fully connected layer with weights \(W_{z}\). To enforce a network to have a Lipschitz constant \(L\) with respect to the state input, we apply spectral normalization to the weight matrices \(W_{s}\) and \(W_{z}\), so that their spectral norms are \(\left\|W_{s}\right\|_{2}=\left\|W_{z}\right\|_{2}=1\). Additionally, we add \(2\) intermediate layers between the input and the hidden layer and between the hidden layer and the output layer, each one multiplying its respective input by a constant \(\sqrt{L}\). Since it is known that the \(tanh\) activation function has a Lipschitz constant of \(1\), it is easy to see that, by function composition, the resulting network is guaranteed to be Lipschitz continuous with respect to its state input with constant \(L\). Note

\begin{table}
\begin{tabular}{|c|c|} \hline
**Vasopressors (mcg/kg/min)** & **Intravenous fluids (mL/4 hours)** \\ \hline \(0.00\) & \(0\) \\ \hline \(0.04\) & \(30\) \\ \hline \(0.113\) & \(80\) \\ \hline \(0.225\) & \(279\) \\ \hline \(0.788\) & \(850\) \\ \hline \end{tabular}
\end{table}
Table 2: Levels of vasopressors and intravenous fluids corresponding to the \(25\) actions in \(\mathcal{A}\)that, since the matrix \(W_{a}\) is not normalized, the network's Lipschitz constant with respect to the action input can be arbitrary.

To train the SCM \(\mathcal{C}\), for each sample, we compute the negative log-likelihood of the observed transition under the SCM's current parameter values (_i.e._, network weight matrices & covariance matrix of the multivariate Gaussian prior), and we use that as a loss. Subsequently, we optimize those parameters using the Adam optimizer with a learning rate of \(0.001\), a batch size of \(256\), and we train the model for \(100\) epochs.

We train the model under multiple values of the Lipschitz constants \(L_{h}\), \(L_{\phi}\) of the location and scale networks, and we evaluate the log-likelihood of the data under each model using \(5\)-fold cross-validation. Specifically, for each configuration of \(L_{h}\) and \(L_{\phi}\), we randomly split the dataset into a training and a validation set (with a size ratio \(4\)-to-\(1\)), we train the corresponding SCM using the training set, and we evaluate the log-likelihood of the validation set based on the trained SCM. This results in the log-likelihood always being measured on a different set of data points than the one used for training. For each configuration of \(L_{h}\) and \(L_{\phi}\), we repeat the aforementioned procedure \(5\) times and we report the average log-likelihood achieved on the validation set. In addition, we train an unconstrained model without spectral normalization, which can have an arbitrary Lipschitz constant.

Figure 6 shows the decrease in log-likelihood of the respective constrained model as a percentage of the log-likelihood achieved by the unconstrained model, under various values of the Lipschitz constants \(L_{h}\), \(L_{\phi}\). We observe that, the model's performance is predominantly affected by the Lipschitz constant of the location network \(L_{h}\), and its effect is more pronounced when \(L_{h}\) takes values smaller than \(1\). Additionally, we can see that the scale network's Lipschitz constant \(L_{\phi}\) has a milder effect on performance, especially when \(L_{h}\) is greater or equal than \(1\). Since we are interested in constraining the overall Lipschitz constant of the SCM \(\mathcal{C}\), in our experiments in Section 5, we set \(L_{h}=1\) and \(L_{\phi}=0.1\), which achieves a log-likelihood only \(6\%\) lower to that of the best model trained without any Lipschitz constraint.

Figure 6: Goodness of fit of the Lipschitz-continuous SCM \(\mathcal{C}\), measured by means of the percentage decrease in log likelihood of the data in comparison with an SCM trained without Lipschitz-continuity constraints. The x and y axes correspond to different enforced values for the Lipschitz constants \(L_{h}\), \(L_{\phi}\) of the location and scale networks \(h\) and \(\phi\), respectively. Darker values indicate that the learned SCM achieves a significantly lower log likelihood than the unconstrained SCM.