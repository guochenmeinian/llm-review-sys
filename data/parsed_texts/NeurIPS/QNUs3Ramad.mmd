# Adversarial Self-Training Improves Robustness and Generalization for Gradual Domain Adaptation

 Lianghe Shi

Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Correspondence to: Weiwei Liu <liuwiewei863@gmail.com>.

###### Abstract

Gradual Domain Adaptation (GDA), in which the learner is provided with additional intermediate domains, has been theoretically and empirically studied in many contexts. Despite its vital role in security-critical scenarios, the adversarial robustness of the GDA model remains unexplored. In this paper, we adopt the effective gradual self-training method and replace vanilla self-training with adversarial self-training (AST). AST first predicts labels on the unlabeled data and then adversarially trains the model on the pseudo-labeled distribution. Intriguingly, we find that gradual AST improves not only adversarial accuracy but also clean accuracy on the target domain. We reveal that this is because adversarial training (AT) performs better than standard training when the pseudo-labels contain a portion of incorrect labels. Accordingly, we first present the generalization error bounds for gradual AST in a multiclass classification setting. We then use the optimal value of the Subset Sum Problem to bridge the standard error on a real distribution and the adversarial error on a pseudo-labeled distribution. The result indicates that AT may obtain a tighter bound than standard training on data with incorrect pseudo-labels. We further present an example of a conditional Gaussian distribution to provide more insights into why gradual AST can improve the clean accuracy for GDA.

## 1 Introduction

The key assumption of classical machine learning--that training and test data come from the same distribution--may not always hold in many real-world applications [15]. A data distribution typically evolves due to changes in conditions: for example, changing weather in vehicle identification [14], sensor aging in sensor measurement [43], the evolution of road conditions in self-driving [6], etc. To address this problem, Unsupervised Domain Adaptation (UDA) has been developed to train a model that performs well on an unlabeled target domain by leveraging labeled data from a similar yet distinct source domain.

Various works in UDA [5, 53, 34] theoretically demonstrate that the generalization error can be controlled by the domain discrepancy. Hence, the domain shift between the source domain and the target domain is expected to be small [15]. However, in some applications, the domain shift is substantial, leading to a sharp drop in the performance of the UDA method [26]. Furthermore, since changes in real-world data are more often gradual than abrupt [16], there are many intermediate domains between the source domain and the target domain. To get a better solution to the gradually shifting data, some recent works have focused on the Gradual Domain Adaptation (GDA) problem, where the learner is additionally provided with unlabeled intermediate domains. The large gap between the source and target domains is then divided up into multiple small shifts between theintermediate domains. Recently, the celebrated work [26] proposes gradual self-training, which iteratively applies the self-training method to adapt the model along the intermediate domains. Empirically, the gradual self-training method greatly improves the target domain's accuracy compared to the traditional direct domain adaptation. After that, remarkable theoretical [45] and algorithmic [10; 1] advances have been achieved in GDA. However, the adversarial robustness of the GDA model remains unexplored, despite its vital role in security-critical scenarios. Adversarial robustness refers to the invariance of a model to small perturbations of its input [39], while adversarial accuracy refers to a model's prediction accuracy on adversarial examples generated by an attacker. Numerous works [33; 41; 49] have shown that very small perturbations, even those imperceptible to humans, can successfully deceive deep neural network (DNN) models, resulting in erroneous predictions.

One popular and effective method of improving robustness is Adversarial Training (AT), which adds adversarial examples to the training data. Due to the lack of labels in the target domain, it is difficult to directly generate adversarial examples. Accordingly, this paper replaces self-training with Adversarial Self-Training (AST), which first generates pseudo-labels on the unlabeled data and then adversarially trains the model based on these pseudo-labeled data. We conduct comprehensive experiments to validate the effectiveness of gradual AST in Section 3. Compared to the gradual self-training method, the proposed gradual AST method delivers a great improvement in adversarial accuracy, from \(6.00\%\) to \(90.44\%\) on the Rotating MNIST dataset. More interestingly, we find that the clean accuracy of gradual AST on the target domain also increases from \(90.06\%\) to \(97.15\%\). This is a surprising result, since many prior works [52; 38] demonstrate that AT may hurt generalization; in other words, there is a trade-off between clean accuracy and adversarial accuracy. We then empirically investigate the reason why AST improves clean accuracy in GDA and find that the adversarially trained model has better clean accuracy than the standardly trained model when the generated pseudo-labels contain a proportion of incorrect labels.

In Section 4, we present novel generalization error bounds for the proposed gradual AST. The results show that if we have a model \(f\) with low adversarial margin loss on the source domain and the distribution shifts slightly, gradual AST can generate a new model \(f^{\prime}\) that is also adversarially robust on the target domain. To explain why gradual AST achieves better clean accuracy than vanilla gradual self-training, we use the optimal value of the Subset Sum Problem [3] to bridge the standard error on a real distribution and the adversarial error on a pseudo-labeled distribution. The result shows that AT may yield a tighter generalization guarantee than standard training on pseudo-labeled training data. We further provide an example of a conditional Gaussian distribution to illustrate why AT outperforms standard training when a proportion of the labels in the training data are incorrect.

We summarize our contributions as below:

* We are the first to apply the AST method in GDA. The proposed gradual AST method can not only improve the adversarial robustness but also the clean accuracy for GDA, which is an appealing and nontrivial result, considering that many prior works demonstrate that adversarial training may hurt generalization.
* We empirically explore the reason for the improvements in clean accuracy and find that adversarial training performs better than standard training when the pseudo-labeled training set contains a proportion of incorrect labels.
* From the theoretical perspective, we provide the generalization error bounds for gradual AST, which explain why the trained model is adversarially robust on the target domain. We provide an error bound and a toy example of Gaussian distribution to provide some intuitions for the improvements in clean accuracy.

## 2 Preliminaries

### Multiclass Classification Learning Framework

Let \(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\) be a measurable instance space, where \(\mathcal{X}\) and \(\mathcal{Y}\) denote the input space and label space, respectively. The input space \(\mathcal{X}\) is a subset of a \(d\)-dimensional space, \(\mathcal{X}\subseteq\mathbb{R}^{d}\). In this work, we focus on multiclass classification, and the label space \(\mathcal{Y}\) is \(\{1,\dots,k\}\), where \(k\) is the number of classes. Following the notations used in [5], a domain is envisioned as a tuple \(\langle P,h_{P}\rangle\), consisting of a distribution \(P\) on input space \(\mathcal{X}\) and a labeling function \(h_{P}:\mathcal{X}\rightarrow\mathcal{Y}\). In practice, the true distribution \(P\) is unknown to the learner, which has access only to the training data \(S\) drawn independent and identically distributed (i.i.d.) according to the true distribution \(P\). We use \(\widehat{P}\) to denote the empirical distribution of \(P\) according to the training data \(S\). Moreover, following the notations used in [35], we consider the class \(\mathcal{F}=\{f:\mathcal{X}\rightarrow\mathbb{R}^{k}\}\) of scoring functions \(f\). We use \(f_{y}(x)\) to indicate the output of \(f\) associated with the data point \(x\) on the \(y\)-th dimension; the output indicates the confidence of the prediction. The label with the largest score is the predicted label of \(x\). We use \(\mathcal{H}_{\mathcal{F}}=\{h_{f}(\cdot)=\underset{y\in\mathcal{Y}}{\arg\max} \!f_{y}(\cdot):f\in\mathcal{F}\}\) to denote the labeling function class induced by \(\mathcal{F}\). The expected risk and the empirical risk of a classifier \(h\in\mathcal{H}\) with respect to a labeling function \(h^{\prime}\) on distribution \(P\) are defined as \(R_{P}(h,h^{\prime})\triangleq\underset{x\sim P}{\mathbb{E}}\mathds{1}[h(x) \neq h^{\prime}(x)]\) and \(R_{\bar{P}}(h,h^{\prime})\triangleq\underset{x\sim\bar{P}}{\mathbb{E}} \mathds{1}[h(x)\neq h^{\prime}(x)]\), respectively, where \(\mathds{1}\) is the indicator function. We use \(R_{P}(h)\) and \(R_{\bar{P}}(h)\) to abbreviate \(R_{P}(h,h_{P})\) and \(R_{\bar{P}}(h,h_{P})\) respectively. Furthermore, the 0-1 loss is non-differentiable and cannot be minimized directly. Thus, a margin theory for multiclass classification was developed by [25], that replaces the 0-1 loss with the margin loss. The margin of a scoring function \(f\in\mathcal{F}\) for labeled data \((x,y)\) is defined as \(\rho_{f}(x,y)\triangleq f_{y}(x)-\underset{y^{\prime}\neq y}{\max}\!f_{y^{ \prime}}(x)\). And the expected margin loss of a scoring function \(f\in\mathcal{F}\) with respect to another scoring function \(f^{\prime}\in\mathcal{F}\) on distribution \(P\) is defined as \(R_{P}^{(\rho)}(f,f^{\prime})\triangleq\underset{x\sim P}{\mathbb{E}}\Theta_{ \rho}\circ\rho_{f}(x,h_{f^{\prime}}(x))\), where \(\Theta_{\rho}(m)=\min\{1,\max\{0,1-m/\rho\}\}\) is the ramp loss. Similarly, we use \(R_{P}^{(\rho)}(f)\) and \(R_{\bar{P}}^{(\rho)}(f)\) to abbreviate \(R_{P}^{(\rho)}(f,f_{P})\) and \(R_{\bar{P}}^{(\rho)}(f,f_{P})\) respectively. Note that \(R_{P}(h_{f},h_{f^{\prime}})\leq R_{P}^{(\rho)}(f,f^{\prime})\), since the 0-1 loss is upper bounded by the margin loss.

### Gradual Domain Adaptation

Under the standard GDA settings [26; 45], the learner is sequentially trained on \(T+1\) domains with gradual shifts. The corresponding data distributions are \(P_{0},P_{1},\ldots,P_{T}\), where \(P_{0}\) is the distribution of the source domain, \(P_{T}\) is the distribution of the target domain, and \(P_{1},\ldots,P_{T-1}\) are the distributions of the intermediate domains. For simplicity, we assume that the number of data points in each domain is the same: namely, each domain \(t\) has a set of \(n\) data drawn i.i.d. from \(P_{t}\), denoted as \(S_{t}\). Recently, [26] proposes a gradual self-training method for GDA, which successively applies self-training to each of the intermediate domains. The self-training method adapts a pre-trained model trained on the previous domain to the current domain using unlabeled data drawn from the current distribution. Specifically, given a pre-trained model \(f\) and an unlabeled data set \(S\), the model first predicts the labels of the unlabeled data, then trains \(f\) with empirical risk minimization (ERM) over these pseudo-labeled data, i.e.,

\[f^{\prime}=ST(f,S)=\underset{f^{\prime}\in\mathcal{F}}{\arg\min}R_{\bar{P}}^{ (\rho)}(f^{\prime},f)=\underset{f^{\prime}\in\mathcal{F}}{\arg\min}\frac{1}{n} \sum_{x_{i}\in S}\Theta_{\rho}\circ\rho_{f^{\prime}}(x_{i},h_{f}(x_{i})).\]

In gradual self-training, the model is first pre-trained on the labeled source domain and then successively self-trained on the unlabeled dataset of each of the intermediate domains, i.e.,

\[f_{t}=ST(f_{t-1},S_{t}),t\in\{1,\ldots,T\}.\]

Since the domain shifts between the consecutive domains are assumed to be small [10; 26; 45], the accuracy of the pseudo-labels is expected to be high at each step. The gradual self-training method aims to output a final trained classifier \(h_{f_{T}}\) with low expected risk in the target domain.

### Adversarial Self-Training

Given a classifier \(h_{f}\) and a data point \((x,y)\), we can generate the corresponding adversarial example \((x^{adv},y)\) by adversarially perturbing \(x\) in a small neighborhood \(B_{\epsilon}(x)\) of \(x\), as follows: \(x^{adv}=\arg\max_{x^{\prime}\in B_{\epsilon}(x)}\Theta_{\rho}\circ\rho_{f}(x^{ \prime},y)\). In this paper, we focus on the \(\ell_{q}\) adversarial perturbation \(B_{\epsilon}(x)\triangleq\{x^{\prime}\in\mathcal{X}:\|x^{\prime}-x\|_{q}\leq \epsilon,q\geq 1\}\), which is referred to as \(\ell_{q}\)-attack in the AT context and has been widely studied in existing works. Given a vector \(x\in\mathbb{R}^{d}\), we define the \(\ell_{q}\)-norm of \(x\) as \(\|x\|_{q}\triangleq\left(\sum_{i=1}^{d}|x_{(i)}|^{q}\right)^{1/q}\), for \(q\in[1,\infty)\), where \(x_{(i)}\) is the \(i\)-th dimension of \(x\); for \(q=\infty\), we define \(\|x\|_{\infty}\triangleq\underset{1\leq i\leq d}{\max}|x_{(i)}|\). Similar to the definition of the standard risk \(R_{P}(h,h^{\prime})\), we define the adversarial risk on distribution \(P\) for any two classifiers \(h,h^{\prime}\) as follows: \(\widetilde{R}_{P}(h,h^{\prime})\triangleq\underset{x\sim P}{\mathbb{E}}\underset{ \|\delta\|_{q}\leq\epsilon}{\max}\mathds{1}[h(x+\delta)\neq h^{\prime}(x)]\). We also define the expected adversarial margin loss for any two scoring functions \(f,f^{\prime}\) on distribution \(P\) as follows:

\[\widetilde{R}_{P}^{(\rho)}(f,f^{\prime})\triangleq\underset{x\sim P}{\mathbb{E }}\underset{\|\delta\|_{q}\leq\epsilon}{\max}\Theta_{\rho}\circ\rho_{f}(x+ \delta,h_{f^{\prime}}(x)).\] (1)

Although the expected adversarial margin loss is asymmetrical, it satisfies the triangle inequality (see proof in Appendix A.1). The empirical adversarial risk and the empirical adversarial margin loss can be defined similarly. We use \(\widetilde{R}_{P}^{(\rho)}(f)\) and \(\widetilde{R}_{\tilde{P}}^{(\rho)}(f)\) to abbreviate \(\widetilde{R}_{P}^{(\rho)}(f,f_{P})\) and \(\widetilde{R}_{\tilde{P}}^{(\rho)}(f,f_{P})\) respectively. To train a robust model with resistance to the adversarial examples, AT aims to find a classifier \(h_{f}\) that yields the minimal adversarial margin loss, i.e., \(\underset{f\in\mathcal{F}}{\arg\min}\widetilde{R}_{\tilde{P}}^{(\rho)}(f)\). However, we cannot adversarially train the model directly on the unlabeled target domain because the conventional AT method requires labeled data. Instead, we adapt the self-training method to the adversarial self-training (AST) method by generating adversarial examples over the pseudo-labels, i.e.,

\[f^{\prime}=AST(f,S)=\underset{f^{\prime}\in\mathcal{F}}{\arg\min}\widetilde{ R}_{\tilde{P}}^{(\rho)}(f^{\prime},f)=\underset{f^{\prime}\in\mathcal{F}}{ \arg\min}\frac{1}{n}\sum_{x_{i}\in S}\underset{\|\delta_{i}\|_{q}\leq\epsilon }{\max}\Theta_{\rho}\circ\rho_{f^{\prime}}(x_{i}+\delta_{i},h_{f}(x_{i})).\]

## 3 Empirical Exploration

In this section, we propose a gradual AST method to improve adversarial robustness. Surprisingly, we find that the gradual AST improves not only adversarial robustness but also clean accuracy. Our code is available at https://github.com/whustone007/AST_GDA.

**Datasets.** We run experiments on two datasets that are widely used in GDA [26; 45]. **Rotating MNIST** is a semi-synthetic dataset generated by rotating each MNIST image at an angle between \(0\) and \(60\) degrees. The \(50{,}000\) training set images are divided into three parts: a source domain of \(5000\) images (\(0\)-\(5\) degrees), \(21\) intermediate domains of \(42{,}000\) images (\(5\)-\(60\) degrees), and a set of validation data. The rotating degree gradually increases along the domain sequence. The \(10{,}000\) test set images are rotated by \(55\)-\(60\) degrees, representing the target domain. **Portraits**[18] is a real dataset consisting portraits of American high school students across a century. The model aims to classify gender. The dataset is split into a source domain of the first \(2000\) images, seven intermediate domains of the next \(14{,}000\) images, and a target domain of the next \(2000\) images.

**Methods.** The standard gradual self-training method [26] successively adapts the model to the next domain via self-training. To improve the adversarial robustness, we replace the vanilla self-training with AST. We implement multiple gradual AST methods with different starting domains \(\tau\in\{0,\dots,T+1\}\) where we begin to use AST, i.e.,

\[f_{t}=\left\{\begin{array}{cc}ST(f_{t-1},S_{t}),&1\leq t\leq\tau-1\\ AST(f_{t-1},S_{t}),&\tau\leq t\leq T\end{array}\right.\]

In particular, if \(\tau=0\), then the model is adversarially trained throughout; if \(\tau=T+1\), then the model is standardly trained throughout. Note that \(T=21\) in Rotating MNIST and \(T=7\) in Portraits.

**Implementation Details.** We implement our methods using PyTorch [37] on two Nvidia GeForce RTX 3090 Ti GPUs. Following [26], we use a \(3\)-layer convolutional network with dropout (\(0.5\)) and BatchNorm on the last layer. We use mini-batch stochastic gradient descent (SGD) with momentum \(0.9\) and weight decay \(0.001\). The batch size is \(32\) and the learning rate is \(0.001\). We train the model for \(40\) epochs in each domain. We set the radius of the bounded perturbation to be \(0.1\)[51] for Rotating MNIST and \(0.031\) for Portraits. Following [33], we use PGD-\(20\) with a single step size of \(0.01\) as the adversarial attacker.

### Results

The results of the different methods on Rotating MNIST are shown in Figure 1(a). The numbers in the abscissa represent different starting domains \(\tau\) of various methods. From Figure 1(a), we can see that the vanilla gradual self-training method (\(\tau=22\)) achieves a clean accuracy of \(90.06\%\) and adversarial accuracy of \(6.00\%\) on the target domain. Notably, the gradual AST method that uses ASTthroughout (\(\tau=0\)) improves the adversarial accuracy to \(83.54\%\) (\(+77.54\%\)). More interestingly, we find that the clean accuracy is also slightly improved by \(0.53\%\). Furthermore, the results of varying the starting domains \(\tau\) indicate that the performance will be further improved if we use the vanilla self-training method for the first few domains and then use AST from an intermediate domain. We utilize the validation set of the target domain to choose the optimal starting time \(\tau\). Experimentally, the best choice of starting time for Rotating MNIST is \(\tau=9\), which achieves clean accuracy of \(96.66\%\) (\(+6.6\%\)) and adversarial accuracy of \(88.86\%\) (\(+82.86\%\)). We find that AST not only improves adversarial robustness but also clean accuracy with a large step. The results of the different methods on Portraits are shown in Figure 1(b). We find similar results to those on Rotating MNIST: the vanilla gradual self-training method (\(\tau=22\)) achieves a clean accuracy of \(82.03\%\) and adversarial accuracy of \(40.23\%\), while the gradual AST method (\(\tau=0\)) improves the clean accuracy by \(2.73\%\) and the adversarial accuracy by \(37.40\%\). We provide more experimental results of different \(\epsilon\), domain numbers, and neural networks in Appendix B.

### Why Does Gradual AST Improve Clean Accuracy in GDA?

It is widely believed that AT would hurt generalization [52; 38] and decrease clean accuracy; thus it is surprising that the proposed gradual AST method improves the clean accuracy of the trained model in the GDA setting. In this section, we speculate on the reasons for this. Recently, some works [39; 42] have revealed that an adversarially pre-trained model tends to improve the target model's accuracy in transfer learning. We accordingly investigate whether the adversarially trained model has better clean accuracy on the next domain and thus generates more accurate pseudo-labels. However, by comparing the gradual AST methods with \(\tau=0\) and \(\tau=1\) on Rotating MNIST, we find that the model standardly trained on the source domain achieves a clean accuracy of \(97.30\%\) on the next domain, while the model adversarially trained on the source domain achieves a clean accuracy of \(96.95\%\) on the next domain. On the other hand, given that the pseudo-labels contain a small portion of incorrect labels, we wonder whether the adversarially trained model has better clean performance than the standardly trained model if trained on such a noisy training set. Since we set a fixed random seed for all gradual AST methods with varying starting domains, given a \(\tau_{0}\in\{0,\dots,T\}\), the gradual AST method with \(\tau=\tau_{0}\) and that with \(\tau=\tau_{0}+1\) have the same training processes before domain \(\tau_{0}\) and the same pseudo-label set on domain \(\tau_{0}\). The difference between the gradual AST method with \(\tau=\tau_{0}\) and that with \(\tau=\tau_{0}+1\) is whether we adversarially train the model on the domain \(\tau_{0}\). We compare the clean accuracy of these two methods on domain \(\tau_{0}\) in Figure 1(c). As we can observe from the figure, the adversarially trained model (\(\tau=\tau_{0}\)) has higher clean accuracy on domain \(\tau_{0}\) than the standardly trained model (\(\tau=\tau_{0}+1\)). Note that there are two exceptions in which AT hurts clean performance when the accuracy of pseudo-labels is very high (\(\tau_{0}=0,1\)), which explains why using AST from an intermediate domain is better than using AST throughout. Besides, we find that the adversarially trained model predicts more accurate pseudo-labels in the next domain than the standardly trained model when the current training set contains a proportion of incorrect labels. We then conclude that AT benefits the clean accuracy of GDA from two perspectives:

* For the intermediate domains, AT improves the accuracy of the pseudo-labels in the next domain. So the learner can fine-tune the model on a more accurate dataset.
* For the target domain, when the pseudo-labels contain a proportion of incorrect labels, the adversarially trained model has higher clean accuracy on the target domain than the standardly trained model.

### Sensitivity of Filtration Ratio \(\zeta\)

Following the standard experimental strategy [26; 45], for each step of self-training, we filter out a portion of images for which the model's prediction is least confident. In Figure 1, the ratio \(\zeta\) is set to \(0.1\) as in the work of [26]. We further conduct more experiments with varying \(\zeta\). With smaller \(\zeta\), more data will be included for the next domain's training, but the accuracy of the pseudo-labels will decrease. Due to the page limit, we attach the detailed results of methods with different values of \(\tau\) and \(\zeta\) in Appendix B. We find that when \(\zeta\) decreases, the performance of the standard gradual self-training also decreases. However, the gradual AST method prefers smaller \(\zeta\) even though more incorrect pseudo-labels are included in the training set. For example, when we set \(\zeta=0.05\), the gradual AST with the optimal starting domain \(\tau=9\) (chosen using the validation set) achieves clean accuracy of \(97.15\%\) (\(+7.09\%\)) and adversarial accuracy of \(90.44\%\) (\(+84.44\%\)) on Rotating MNIST.

[MISSING_PAGE_FAIL:6]

show the error correction effect of AT on the incorrect labels. Our theory thus supports the excellent performance of gradual AST shown in Section 3.

### Generalization Error Bounds for Gradual AST

We introduce the Margin Disparity Discrepancy (MDD) to measure the domain shifts, as below.

**Definition 4.1** (Mdd [53]).: Let \(P\) and \(Q\) be two distributions over \(\mathcal{X}\). Given a scoring function class \(\mathcal{F}\) and a specific scoring function \(f\in\mathcal{F}\), the MDD between \(P\) and \(Q\) is defined as

\[d^{(\rho)}_{f,\mathcal{F}}(P,Q)\triangleq\sup_{f^{\prime}\in\mathcal{F}}\left( R^{(\rho)}_{Q}(f^{\prime},f)-R^{(\rho)}_{P}(f^{\prime},f)\right).\] (2)

When the domain shift is slight, the MDD between the domains is small. The MDD is well-defined and can be empirically estimated by the training data [53], where the estimation error can be bounded by Rademacher complexity. We next present the definition of Rademacher complexity, which is widely used in generalization theory to measure the complexity of a hypothesis class.

**Definition 4.2** (Rademacher Complexity [44]).: Let \(\mathcal{G}\) be a set of real-valued functions defined over \(\mathcal{X}\). For a fixed collection \(S\) of \(n\) data points, the empirical Rademacher complexity of \(\mathcal{G}\) is defined as

\[\widehat{\mathcal{R}}_{S}(\mathcal{G})\triangleq\frac{2}{n}\underset{\varsigma }{\mathbb{E}}\left[\sup_{g\in\mathcal{G}}\sum_{x_{i}\in S}\varsigma_{i}g(x_{ i})\right].\]

The expectation is taken over \(\varsigma=(\varsigma_{1},\ldots,\varsigma_{n})\), where \(\varsigma_{i},i\in\{1,\ldots,n\}\), are independent uniform random variables taking values in \(\{-1,+1\}\).

We next introduce an adversarial margin hypothesis class \(\widetilde{\rho_{\mathcal{F}}\mathcal{F}}\).

**Definition 4.3** (Adversarial Margin Hypothesis Class).: Given a scoring function class \(\mathcal{F}\), the adversarial margin hypothesis class is defined as

\[\widetilde{\rho_{\mathcal{F}}\mathcal{F}}\triangleq\{x\mapsto\max_{\|\delta\| _{q}\leq\epsilon}\rho_{f^{\prime}}(x+\delta,h_{f}(x)):f,f^{\prime}\in\mathcal{ F}\}.\]

Having defined the Rademacher complexity and the MDD, we now focus on a pair of arbitrary consecutive domains. The following result shows that AST returns a new model \(f^{\prime}\) with bounded adversarial margin loss if the domain shifts gradually. The proof can be found in Appendix A.2.

**Theorem 4.4** (Adversarial Error Bound for AST).: _Let \(\langle P,f_{P}\rangle\) and \(\langle Q,f_{Q}\rangle\) be two domains with gradual shifts. Suppose we have a scoring function \(f\) pre-trained on domain \(\langle P,f_{P}\rangle\), and a data set \(S\) of \(n\) unlabeled data points drawn \(i.i.d.\) according to distribution \(Q\). \(f^{\prime}\) is the adapted scoring function by generated by the AST algorithm over \(S\), i.e., \(f^{\prime}=AST(f,S)\). Then, for any \(\alpha\geq 0\), the following holds with probability of at least \(1-\alpha\) over data points \(S\):_

\[\widetilde{R}^{(\rho)}_{Q}(f^{\prime})\leq\widetilde{R}^{(\rho)}_{P}(f)+ \widetilde{\gamma}^{*}+\lambda^{*}+d_{f,\mathcal{F}}(P,Q)+\frac{2}{\rho} \widehat{\mathcal{R}}_{S}(\widetilde{\rho_{\mathcal{F}}\mathcal{F}})+6\sqrt{ \frac{\log\frac{4}{\alpha}}{2n}},\]

_where \(\widetilde{\gamma}^{*}=\min_{f^{\prime}\in\mathcal{F}}\widetilde{R}^{(\rho)}_ {Q}(f^{\prime},f)\) and \(\lambda^{*}=\min_{f\in\mathcal{F}}\Big{\{}R^{(\rho)}_{P}(f)+R^{(\rho)}_{Q}(f) \Big{\}}\)._

_Remark 4.5_.: Theorem 4.4 indicates that if we have a model \(f\) with low adversarial margin loss on the domain \(\langle P,f_{P}\rangle\) and the distribution shifts slightly, AST generates a new model \(f^{\prime}\) that is also adversarially robust on the domain \(\langle Q,f_{Q}\rangle\). The \(\lambda^{*}\) term is widely used in domain adaptation theory [5, 34] to implicitly characterize the conditional distribution shift. In the setting of GDA, we assume that the domain shifts only slightly, i.e., \(\lambda^{*}\) and \(d_{f,\mathcal{F}}(P,Q)\) are relatively low.

Based on the error bounds for AST, we can apply this argument inductively and sum these error differences along the domain sequence to obtain the following corollary. See proofs in Appendix A.3.

**Corollary 4.6** (Error Bounds for Gradual AST).: _Given a sequence of domains \(\langle P_{t},f_{P_{t}}\rangle,t\in\{0,\ldots,T\}\) with gradual shifts, each intermediate domain has an unlabeled data set \(S_{t}\) drawn \(i.i.d.\) from \(P_{t}\). The model is successively trained by the AST method, i.e., \(f_{t}=AST(f_{t-1},S_{t}),t\in\{0,\ldots,T\}\)._\(\{1,\ldots,T\}\). Then, for any \(\alpha\geq 0\), the following holds with probability of at least \(1-\alpha\) over data points \(\{S_{t}\}_{t=1}^{T}\):_

\[\widetilde{R}_{P_{T}}(h_{f_{T}})\leq\widetilde{R}_{P_{0}}^{(\rho)}(f_{0})+\sum _{t=1}^{T}\kappa_{t}+\frac{2T}{\rho}\widehat{\mathcal{R}}_{S}\widetilde{( \rho_{F}\mathcal{F})}+6T\sqrt{\frac{\log\frac{4T}{\alpha}}{2n}},\]

_where \(\kappa_{t}=d_{f_{t-1},\mathcal{F}}(P_{t-1},P_{t})+\min_{f\in\mathcal{F}} \widetilde{R}_{P_{t}}^{(\rho)}(f,f_{t-1})+\min_{f\in\mathcal{F}}\left\{R_{P}^{ (\rho)}(f)+R_{Q}^{(\rho)}(f)\right\}\)._

_Remark 4.7_.: Our bound indicates that the adversarial risk on target domain can be controlled by the adversarial margin loss on the source domain and the discrepancy [53] between the intermediate domains. We also consider the standard risk and find that the standard risk of AST can be bounded similarly. We provide the bounds in Appendix A.4.

**Comparison with Previous Work.** Both of the works of [26] and [45] focus on binary classification and standard self-training, while our results non-trivially extend the error bounds into the setting of adversarial GDA and multiclass classification. Furthermore, [26] sets linear hypotheses as their training models, while we consider a more general hypothesis class that can be implemented by neural networks. [45] makes an assumption that the new model can always fit the pseudo-labels without error. As we can observe from the experiments, the training error tends to be small but never equal to \(0\). Compared to [45], we don't need the assumption, so our setting is more consistent with real experiments.

### Adversarial Training Improves Clean Accuracy for Self-Training

AT is generally considered harmful to the clean accuracy of the model [52; 38]. However, the experimental results in Section 3 indicate that AT unexpectedly improves both the clean accuracy and adversarial accuracy of the model trained on the pseudo-labels. In this section, we provide some theoretical insights showing that when the training data contains a small portion of incorrect labels, the adversarially trained model may have a tighter generalization guarantee than the standardly trained model. Our theorem is based on the following optimization problem.

**Definition 4.8**.: Given two fixed vectors \(p\) and \(p^{\prime}\) with all positive coordinates and a fixed \(N\)-dimensional \(0\)-\(1\) vector \(e\), i.e., \(e\in\{0,1\}^{N}\). It is a fixed subset of \(\{1,\ldots,N\}\). The variant of the Subset Sum Problem [3] can be defined as follows:

\[\min_{\widetilde{e}\in\{0,1\}^{N}}\left|p^{T}\widetilde{e}-{p^{\prime}}^{T}e \right|,\ \ s.t.\ \ \widetilde{e}_{i}=e_{i},\forall i\in\{1,\ldots,N\}\backslash\Pi.\]

We use \(\Psi^{*}(p^{\prime},p,e,\Pi)\) to denote the optimal value of the problem. Given two fixed vectors \(p^{\prime}\) and \(e\), the value of the term \({p^{\prime}}^{T}e\) is therefore constant. The optimization problem can be viewed as one of selecting a subset of \(p\) such that the sum of its elements is as close as possible to \({p^{\prime}}^{T}e\). Intuitively, the set \(\Pi\) adjusts the difficulty of this optimization problem. For two sets \(\Pi_{1}\subseteq\Pi_{2}\), we have \(\Psi^{*}(p^{\prime},p,e,\Pi_{1})\geq\Psi^{*}(p^{\prime},p,e,\Pi_{2})\) since we can optimize \(\widetilde{e}\) on more coordinates.

With a little abuse of notations, in this subsection, we consider a discrete real distribution \(P\) and a discrete noisy pseudo-labeled distribution \(P_{\eta}\) over \(\mathcal{X}\times\mathcal{Y}\). We denote the corresponding probability mass functions as \(p\) and \(p_{\eta}\) respectively. \(p\) and \(p_{\eta}\) can be viewed as two vectors, i.e., \(p_{i}=p(x_{i},y_{i}),(x_{i},y_{i})\in\mathcal{X}\times\mathcal{Y},i\in\{1, \ldots,|\mathcal{X}\times\mathcal{Y}|\}\). The standard risk on the real data distribution is defined as \(R_{P}(h)=\sum_{(x,y)\in\mathcal{X}\times\mathcal{Y}}p(x,y)\mathds{1}[h(x)\neq y]\) and the adversarial risk on the pseudo-labeled data distribution is defined as \(\widetilde{R}_{P_{\eta}}(h)=\sum_{(x,y)\in\mathcal{X}\times\mathcal{Y}}p_{ \eta}(x,y)\max_{\|\delta\|_{q}\leq\varepsilon}\mathds{1}[h(x+\delta)\neq y]\).

The next result uses the optimal value \(\Psi^{*}(p,p_{\eta},e,\Pi)\) to bound the difference between these two risks. The proof can be found in Appendix A.5.

**Theorem 4.9**.: _Given two distributions \(P\) and \(P_{\eta}\) over \(\mathcal{X}\times\mathcal{Y}\) with the corresponding probability mass vectors \(p\) and \(p_{\eta}\), the following bound holds for any classifier \(h\in\mathcal{H}_{\mathcal{F}}\):_

\[R_{P}(h)\leq\widetilde{R}_{P_{\eta}}(h)+\Psi^{*}(p,p_{\eta},e,\Pi_{e}),\]

_where \(e\) is the risk vector \(e_{i}=\mathds{1}[h(x_{i})\neq y_{i}]\), and \(\Pi_{e}=\{i:\exists\left\|\delta_{i}\right\|_{q}\leq\epsilon,h(x_{i}+\delta_{ i})\neq h(x_{i})\}\)._By setting \(\epsilon=0\), we can derive the error bound for standard training: \(R_{P}(h)\leq R_{P_{\eta}}(h)+\Psi^{*}(p,p_{\eta},e,\Pi_{0})\). By the definition of the set \(\Pi_{\epsilon}\), the larger the perturbation radius \(\epsilon\), the larger the size of \(\Pi_{\epsilon}\), i.e., \(\Pi_{0}\subseteq\Pi_{\epsilon}\). We then have \(\Psi^{*}(p,p_{\eta},e,\Pi_{\epsilon})\leq\Psi^{*}(p,p_{\eta},e,\Pi_{0})\). Theorem 4.9 implies that if the adversarial risk \(\widetilde{R}_{P_{\eta}}(h)\) on the pseudo-labeled distribution is well controlled by AT, then AT may yield a tighter generalization guarantee than standard training.

### A Toy Example of a Conditional Gaussian Distribution

We further provide an example of a conditional Gaussian distribution to show the efficacy of AT under incorrect pseudo-labels.

**Example 4.10** (Conditional Gaussian Distribution).: We consider the input space \(\mathcal{X}=\mathbb{R}\) and the label space \(\mathcal{Y}=\{-1,+1\}\). For simplicity, we assume that the two classes have symmetrical mean parameters \(\{-\mu,+\mu\}\) and the same standard deviations \(\sigma\). The \((\mu,\sigma)\)-conditional Gaussian distribution \(P\) can then be defined as a distribution over \(\mathcal{X}\times\mathcal{Y}=\mathbb{R}\times\{-1,+1\}\), where the conditional distribution of \(X\) given \(Y\) is \(X|Y=y\sim\mathcal{N}(y\mu,\sigma^{2})\) and the marginal distribution of \(Y\) is \(P_{Y}(\{-1\})=P_{Y}(\{+1\})=1/2\). We consider a threshold hypothesis class, i.e., \(\mathcal{H}\triangleq\{x\mapsto\text{sgn}(x-b):b\in\mathbb{R}\}\), where sgn is the sign function. We denote the hypothesis with threshold \(b\) as \(h_{b}\). In the setting of self-training, we have a labeling function \(h_{w}\), which is the model trained on the previous domain. The threshold \(w\neq 0\), since there is a distribution shift. The function \(h_{w}\) assigns \(y=-1\) to the data points where \(x<w\) and assigns \(y=+1\) to the data points where \(x>w\).

We denote the pseudo-labeled data distribution over \(\mathcal{X}\times\mathcal{Y}\) as \(P_{\eta}\). We use \(R_{P_{\eta}}(h)\) and \(\widetilde{R}_{P_{\eta}}(h)\) to respectively denote the standard \(0\)-\(1\) risk and adversarial \(0\)-\(1\) risk on the pseudo-labeled data distribution \(P_{\eta}\). We assume \(|w|\ll\mu\) and \(\epsilon\ll\mu\), since the domain shift is slight and the adversarial perturbation is subtle. We also assume \(\mu>\sigma\), which means that most samples can be well separated. The learner is provided with infinite samples. Under these assumptions, we provide the following results to demonstrate the efficacy of AT. The proof can be found in Appendix A.6.

**Theorem 4.11**.: _For the real data distribution \(P\) defined in Example 4.10, the optimal classifier that minimizes the standard risk \(R_{P}(h)\) and adversarial risk \(\widetilde{R}_{P}(h)\) is \(h_{0}(\cdot)=\text{sgn}(\cdot)\). For the pseudo-labeled distribution \(P_{\eta}\) defined in Example 4.10, the standardly trained classifier \(h_{std}\) that minimizes the standard risk \(R_{P_{\eta}}(h)\) is \(h_{std}=h_{w}\); the adversarially trained classifier \(h_{adv}\) that minimizes the adversarial risk \(\widetilde{R}_{P_{\eta}}(h)\) has the following corresponding threshold \(b_{adv}\)_

\[b_{adv}=\left\{\begin{array}{cc}w+\epsilon,&\text{ if }w<-\epsilon\\ 0,&\text{ if }-\epsilon<w<\epsilon\\ w-\epsilon,&\text{ if }w>\epsilon\end{array}\right.\]

_Remark 4.12_.: Theorem 4.11 shows that the adversarially trained model \(h_{adv}\) consistently moves the threshold close to the optimal threshold \(b=0\), leading to a smaller generalization error. We visually represent the adversarially trained model in Figure 2.

## 5 Related Work

**Self-Training.** A series of works have achieved significant progress using self-training in semi-supervised learning [29; 40] and domain adaptation [56]. The recent work [9] has proposed robust

Figure 2: The visualization of the adversarially trained classifier for a conditional Gaussian distribution. AT consistently moves the threshold \(b_{adv}\) close to the optimal threshold \(b=0\), leading to better generalization on the real data distribution. Best viewed in color.

self-training (RST) to leverage additional unlabeled data for robustness [2; 36]. While RST [9] focuses on utilizing additional unlabeled data drawn from the same distribution to improve adversarial robustness, our work investigates the different effects of vanilla self-training and adversarial self-training on clean accuracy.

**Gradual Domain Adaptation.** Unlike the UDA problem, GDA has intermediate domains between the source domain and target domain of UDA. Some works [22; 17; 48] propose various algorithms to handle these evolving domains in a computer vision context. The recent work [26] adopts the self-training method and proposes gradual self-training, which iteratively adapts the model along the domain sequence. Moreover, [26] provides the first learning theory for GDA and investigates when and why the gradual structure helps. [45] further improves the generalization error bound in [26] and reveals the influence of the number of intermediate domains on the error bound. [10] investigates a more difficult setting in which the learner is provided with a set of unordered intermediate data. The authors propose TDOL, a framework that generates synthetic domains by indexing the intermediate data. However, the aforementioned works pay insufficient attention to the adversarial robustness of the model in a GDA context. In the field of UDA, the work of [11] constructs synthetic intermediate domains by creating Grassmannian manifolds between the source and the target. Another line of work instead uses a generative network, such as a cycle generative adversarial network [23], to generate the intermediate data.

**Adversarial Training.** After [41] shows that DNNs are fragile to adversarial attacks, a large amount of works have proposed various attack methods [33; 27; 8] and defense methods [47; 19; 51; 31; 20; 24; 30]. Adversarial training [19] is one of the popular and effective methods that improves adversarial robustness by adding adversarial examples to the training dataset. From the theoretical perspective, some works focus on the sample complexity [50; 54; 12] and the generalization of adversarial training. [32] investigates the trade-off between robustness and fairness. [46; 55] study adversarial robustness under self-supervised learning.

## 6 Conclusion

In this work, we empirically demonstrate that gradual AST improves both the clean accuracy and the adversarial accuracy of the GDA model. We reveal that the reason for this performance improvement is that AT outperforms standard training when the training data contains incorrect pseudo-labels. We first provide generalization error bounds for gradual AST in a multiclass setting. We then construct the Subset Sum Problem to connect the adversarial error and the standard error, providing theoretical insights into the superiority of AST in GDA. The example of conditional Gaussian distribution is further provided to give additional insights into the efficacy of AT on pseudo-labeled training data.

## Acknowledgements

This work is supported by the National Natural Science Foundation of China under Grant 61976161, the Fundamental Research Funds for the Central Universities under Grant 2042022rc0016.

## References

* Abnar et al. [2021] Samira Abnar, Rianne van den Berg, Golnaz Ghiasi, Mostafa Dehghani, Nal Kalchbrenner, and Hanie Sedghi. Gradual domain adaptation in the wild: When intermediate distributions are absent. _arXiv preprint arXiv:2106.06080_, 2021.
* Alayrac et al. [2019] Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving adversarial robustness? In _NeurIPS_, pages 12192-12202, 2019.
* Allcock et al. [2022] Jonathan Allcock, Yassine Hamoudi, Antoine Joux, Felix Klingelhofer, and Miklos Santha. Classical and quantum algorithms for variants of subset-sum via dynamic programming. In _European Symposium on Algorithms_, pages 6:1-6:18, 2022.
* Bartlett and Mendelson [2002] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3:463-482, 2002.

* Ben-David et al. [2010] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. _Machine Learning_, 79:151-175, 2010.
* Bobu et al. [2018] Andreea Bobu, Eric Tzeng, Judy Hoffman, and Trevor Darrell. Adapting to continuously shifting domains. In _ICLR_, 2018.
* Boyd et al. [2004] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Carlini and Wagner [2017] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In _SP_, pages 39-57, 2017.
* Carmon et al. [2019] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled data improves adversarial robustness. In _NeurIPS_, pages 11190-11201, 2019.
* Chen and Chao [2021] Hong-You Chen and Wei-Lun Chao. Gradual domain adaptation without indexed intermediate domains. In _NeurIPS_, pages 8201-8214, 2021.
* Cui et al. [2014] Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen, and Xuelong Li. Flowing on riemannian manifold: Domain adaptation by shifting covariance. _IEEE Transactions on Cybernetics_, 44:2264-2273, 2014.
* Cullina et al. [2018] Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of adversaries. In _NeurIPS_, pages 228-239, 2018.
* Van der Maaten and Hinton [2008] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Erkent and Laugier [2020] Ozgur Erkent and Christian Laugier. Semantic segmentation with unsupervised domain adaptation under varying weather conditions for autonomous vehicles. _IEEE Robotics and Automation Letters_, 5:3580-3587, 2020.
* Farahani et al. [2020] Abolfazal Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R. Arabnia. A brief review of domain adaptation. _arXiv preprint arXiv:2010.03978_, 2020.
* Farshchian et al. [2019] Ali Farshchian, Juan Alvaro Gallego, Joseph Paul Cohen, Yoshua Bengio, Lee E. Miller, and Sara A. Solla. Adversarial domain adaptation for stable brain-machine interfaces. In _ICLR_, 2019.
* Gadermayr et al. [2018] Michael Gadermayr, Dennis Eschweiler, Barbara Mara Klinkhammer, Peter Boor, and Dorit Merhof. Gradual domain adaptation for segmenting whole slide images showing pathological variability. In _ICISP_, pages 461-469, 2018.
* Ginosar et al. [2017] Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee, Philipp Krahenbuhl, and Alexei A. Efros. A century of portraits: A visual historical record of american high school yearbooks. _IEEE Transactions on Computational Imaging_, 3:421-431, 2017.
* Goodfellow et al. [2015] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _ICLR_, 2015.
* Gowal et al. [2021] Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A. Mann. Improving robustness using generated data. In _NeurIPS_, pages 4218-4233, 2021.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* Hoffman et al. [2014] Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolving visual domains. In _CVPR_, pages 867-874, 2014.
* Hsu et al. [2020] Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih Hung, Hung-Yu Tseng, Maneesh Singh, and Ming-Hsuan Yang. Progressive domain adaptation for object detection. In _CVPR_, pages 749-757, 2020.

* Kang et al. [2021] Qiyu Kang, Yang Song, Qinxu Ding, and Wee Peng Tay. Stable neural ODE with lyapunov-stable equilibrium points for defending against adversarial attacks. In _NeurIPS_, pages 14925-14937, 2021.
* Koltchinskii and Panchenko [2002] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. _The Annals of Statistics_, 30:1-50, 2002.
* Kumar et al. [2020] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In _ICML_, pages 5468-5479, 2020.
* Kurakin et al. [2017] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _ICLR_, 2017.
* Ledoux and Talagrand [1991] Michel Ledoux and Michel Talagrand. _Probability in Banach Spaces: isoperimetry and processes_, volume 23. 1991.
* Lee et al. [2013] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on challenges in representation learning, ICML_, page 896, 2013.
* Li and Liu [2023] Boqi Li and Weiwei Liu. WAT: improve the worst-class robustness in adversarial training. In _AAAI_, pages 14982-14990, 2023.
* Li et al. [2022] Xiyuan Li, Zou Xin, and Weiwei Liu. Defending against adversarial attacks via neural dynamic system. In _NeurIPS_, 2022.
* Ma et al. [2022] Xinsong Ma, Zekai Wang, and Weiwei Liu. On the tradeoff between robustness and fairness. In _NeurIPS_, 2022.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* Mansour et al. [2009] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In _COLT_, 2009.
* Mohri et al. [2012] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, 2012.
* Najafi et al. [2019] Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial perturbations in learning from incomplete data. In _NeurIPS_, pages 5542-5552, 2019.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, pages 8024-8035, 2019.
* Raghunathan et al. [2019] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Adversarial training can hurt generalization. _arXiv preprint arXiv:1906.06032_, 2019.
* Salman et al. [2020] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? In _NeurIPS_, 2020.
* Sohn et al. [2020] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. pages 596-608, 2020.
* Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _ICLR_, 2014.
* Utrera et al. [2021] Francisco Utrera, Evan Kravitz, N. Benjamin Erichson, Rajiv Khanna, and Michael W. Mahoney. Adversarially-trained deep nets transfer better: Illustration on image classification. In _ICLR_, 2021.

* [43] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ramon Huerta. Chemical gas sensor drift compensation using classifier ensembles. _Sensors and Actuators B: Chemical_, 166:320-329, 2012.
* [44] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. 2019.
* [45] Haoxiang Wang, Bo Li, and Han Zhao. Understanding gradual domain adaptation: Improved analysis, optimal path and beyond. In _ICML_, pages 22784-22801, 2022.
* [46] Zekai Wang and Weiwei Liu. Robustness verification for contrastive learning. In _ICML_, pages 22865-22883, 2022.
* [47] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In _ICML_, pages 36246-36263, 2023.
* [48] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incremental adversarial domain adaptation for continually changing environments. In _ICRA_, pages 4489-4495, 2018.
* [49] Jingyuan Xu and Weiwei Liu. On robust multiclass learnability. In _NeurIPS_, 2022.
* [50] Dong Yin, Kannan Ramchandran, and Peter L. Bartlett. Rademacher complexity for adversarially robust generalization. In _ICML_, volume 97, pages 7085-7094, 2019.
* [51] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In _ICML_, pages 7472-7482, 2019.
* [52] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan S. Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In _ICML_, pages 11278-11287, 2020.
* [53] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and algorithm for domain adaptation. In _ICML_, pages 7404-7413, 2019.
* [54] Zhengyu Zhou and Weiwei Liu. Sample complexity for distributionally robust learning under chi-square divergence. _Journal of Machine Learning Research_, pages 1-27, 2023.
* [55] Xin Zou and Weiwei Liu. Generalization bounds for adversarial contrastive learning. _Journal of Machine Learning Research_, pages 114:1-114:54, 2023.
* [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. Confidence regularized self-training. In _ICCV_, pages 5982-5991, 2019.