# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models

 ShengYun Peng\({}^{1}\) Pin-Yu Chen\({}^{2}\) Matthew Hull\({}^{1}\) Duen Horng Chau\({}^{1}\)

\({}^{1}\)Georgia Tech \({}^{2}\)IBM

{speng65,matthewhull,polo}@gatech.edu

pin-yu.chen@ibm.com

###### Abstract

Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.

## 1 Introduction

Safety alignment is the foundation to bring LLMs' behaviors in line with human preferences and restrict harmful behaviors at inference time [42, 43, 3]. Though aligned LLMs have adopted one or a combination of the safety alignment methods, _e.g_., reinforcement learning from human feedback (RLHF) [35], instruction tuning [45], direct preference optimization (DPO) [39], and rejection sampling [33], LLM safety can easily be compromised by finetuning with only a few adversarially designed training examples. For instance, both GPT-3.5 Turbo and LLaMA-2 [43] fail to refuse users' harmful queries after only finetuning with 10-shot harmful examples [37]. This brings practical safety concern to model deployment as customization is the desirable way for specific use case. In this paper, we explore the following fundamental problems in LLM safety: **Are all open-source LLMs equally vulnerable to finetuning? Why can simple finetuning easily break LLM's safety alignment? How fast does the model start to break during finetuning?**

We discovered that all these questions can be addressed by navigating the LLM safety landscape. In deep learning literature, visualization of the model landscape has significantly improved our comprehension of generalization errors, optimization trajectories, and model ensembles [31, 30, 13].

In this paper, we introduce the notion of LLM safety landscape and quantify the risk in finetuning LLM by exploring different directions of perturbing model weights. When provided with a single model, we sample a random normalized direction to visualize its local variations. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. The shape of the landscape dictates the fine-tuning attributes: a sharp change in the safety metric indicates that the aligned model is a local minimum, making it challenging to find a point that is both safe and useful, whereas a flat local landscape offers more opportunities to discover a model that better balances safety and usefulness. Our landscape navigation provides a suite of four new insights that facilitate the understanding the LLM safety (Fig. 1):

1. **We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop.** The safety basin is evident in both 1D and 2D safety landscape of LLaMA2, LLaMA3, Vicuna, and Mistral across various random directions and different safety benchmarks. This safety landscape contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new Visage safety metric, the acronym for volumetric index for safety alignment guided by explanation, which measures the safety of an LLM's local region in model parameter spaces. (Sec. 3)
2. **Visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin.** We discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic Visage safety metric measures the risks in finetuning without assumptions on the finetuning dataset, where a higher Visage score means the model after finetuning is safer. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. (Sec. 4)
3. **LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin.** We evaluate the impact of system design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each

Figure 1: **A. “Safety basin”, a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. **B.** Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.

LLM's default system prompt as the baseline. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin. (Sec. 5)
4. **When evaluating the safety landscape using jailbreaking queries, we find that these queries are highly sensitive to perturbations in model weights.** We have collected the adversarial prompts targeting LLaMA2 and Vicuna, generated by jailbreak attacks from the literature [51; 8; 9]. Our safety landscape analysis shows that although the aligned model is vulnerable to jailbreak attacks, slighlty perturbing the model weights in the local space of the aligned model can significantly lower the attack success rate (ASR) of these jailbreaking attacks. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses. (Sec. 6)

## 2 Background and Related Works

LLM safety alignment.LLMs are language models with a large number of parameters trained on web-scale text corpora [5; 1; 43; 10; 27]. LLMs have exhibited emergent capabilities that can be broadly applied in a task-agnostic manner, such as in-context learning [5], chain-of-thought reasoning [46], and mathematical reasoning [25]. These capabilities are largely attributed to aligning LLMs with expected human values and intentions, which involves training the model to follow instructions and being helpful, truthful, and harmless [35; 24; 40]. Specifically, harmless is achieved by safety alignment that empowers the LLM with safety guardrails so that the model can refuse harmful instructions. Common safety alignment techniques are instruction tuning [45], RLHF [35], DPO [39], rejection sampling [33], and self-alignment [41]. However, these techniques are not designed to cover the safety risks that may arise from the subsequent custom finetuning and jailbreak attacks. Recent work has shown that both simple finetuning [37] and jailbreak attacks [8; 51] can circumvent safety guardrails of aligned LLMs.

LLM harmful finetuning attacks and defenses.Finetuning is widely employed to customize open-source LLMs for downstream applications [18; 11]. Typically, finetuning directly updates the parameters of pretrained models using a small dataset to enhance performance on downstream tasks. However, finetuning with a few adversarially designed training examples, or even with a benign dataset, can compromise the safety alignment of LLMs [47]. Qi et al. [37] finetuned GPT-3.5 Turbo and LLaMA2-7b-chat with only 10 harmful examples, but the safety guardrails were undermined in both LLMs. Zhan et al. [48] removed the safety protections of GPT-4 with 95% success with only 340 examples trained with the OpenAI's finetuning API. A new line of research aims to defend against such harmful finetuning attacks at both the alignment stage and the user finetuning stage, _e.g._, Vaccine [23], Lisa [22], Antidote [20], Booster [21], and targeted Vaccine [32]. Safety-aware LLM fine-tuning, such as Safe LoRA [19], can mitigate safety degradation after fine-tuning by steering the model updates toward the direction of better alignment. In this paper, we probe into the mechanism of the harmful finetuning attack via navigating the safety landscape.

Enhancing alignment with safety prompts.To communicate with LLM with precise and task-specific instructions, Bsharat et al. [6] presented a comprehensive principled instructions and guidelines to improve the quality of prompts for LLMs. Recently, safety researchers have also experimented with various prompts to either break or enhance LLM safety alignment. On the attack side, Jin et al. [28] used roleplaying to automatically and iteratively generate harmful prompts. On the defense side, Zheng et al. [49] leveraged prompt tuning to enhance LLM safety by directly optimizing the vanilla system prompt into safety prompt. Safety prompt is a method of safeguarding LLMs against harmful queries without changing the model weights; they are prepended to the user input or serve as a system prompt.

Jailbreaking aligned LLMs with adversarial attacks.A class of vulnerabilities known as "jailbreaks" has recently been shown to cause LLMs to violate their alignment safeguards [7; 38; 44]. Jailbreaks based on human prompt strategies rely on deception and social engineering to elicit objectionable content from LLMs, requiring creativity, manual dataset curation, and significant human effort [8; 12]. Optimization-based jailbreaks optimize the tokens input to the LLM, recognized for their effectiveness but requiring extensive computational resources and being often uninterpretable to humans [51; 29].

## 3 From LLM Safety Landscape to Visage Safety Metric

The model landscape is a crucial tool for interpreting model behaviors and understanding model characteristics. Perturbing a model along random directions reveals the local behavior of the model, while interpolating the parameters between two models illustrates the transition process from one model to the other. In this section, we introduce the notion of LLM safety landscape in both 1D (Sec. 3.1) and 2D (Sec. 3.2) scenarios. Sec. 3.3 presents the safety landscape of four popular open-source LLMs and Sec. 3.4 introduces "LLM safety basin" concept and proposes the Visage safety metric based on our landscape analysis.

### 1D Safety Landscape

Denote \(\bm{\theta}\) as the initial LLM model weights. The safety landscape is plotted by perturbing \(\bm{\theta}\) along a certain direction \(\widehat{\bm{d_{1}}}\) and evaluate the new model weights with a single model safety metric:

\[f(\alpha)=\mathcal{S}(\bm{\theta}+\alpha\widehat{\bm{d_{1}}})\] (1)

where \(\mathcal{S}\) is the safety metric defined for a single model, and \(\alpha\) is a scalar parameter. For 1D-interpolation, we pick two sets of model weights \(\bm{\theta}\) and \(\bm{\theta}^{\prime}\) and the direction is defined by the line connecting these two points, \(\widehat{\bm{d_{1}}}=\bm{\theta}^{\prime}-\bm{\theta}\). For 1D-random, \(\bm{\theta}\) is the center point and we randomly sample a direction \(\bm{d_{1}}\) from Gaussian distribution. We apply layer normalization to \(\bm{d_{1}}\) to exclude the effect of scale invariance [31] so that the flatness and the sharpness across different landscape plots are comparable. Specifically, \(\bm{d_{1}}\) is normalized to a unit direction and then multiplied by the Frobenius norm of each layer \(i\):

\[\widehat{\bm{d_{1i}}}=\frac{\bm{d_{1i}}}{\|\bm{d_{1i}}\|}\left\|\bm{\theta}_{ i}\right\|\] (2)

In the rest of the paper, we will use 1D-random \(\bm{\theta}\) and 1D-interpolation \(\bm{\theta}\rightarrow\bm{\theta}^{\prime}\) to represent the above two types of 1D directions.

### 2D Safety Landscape

Similar to the 1D landscape, the 2D landscape requires two directions \(\widehat{\bm{d_{1}}}\) and \(\widehat{\bm{d_{2}}}\) and the safety landscape is defined as:

\[f(\alpha,\beta)=\mathcal{S}(\bm{\theta}+\alpha\widehat{\bm{d_{1}}}+\beta \widehat{\bm{d_{2}}})\] (3)

For 2D random, since both directions are randomly sampled from Gaussian distribution, the cosine similarity between \(\widehat{\bm{d_{1}}}\) and \(\widehat{\bm{d_{2}}}\) is \(\sqrt{2/(\pi n)}\)[15], where \(n\) is the dimension of \(\bm{\theta}\). Given current LLMs have billions of parameters, the two random directions are orthogonal and we only perform layer normalization as in Eq. 2. For 2D interpolation, we pick three sets of model weights \(\bm{\theta}\), \(\bm{\theta}^{\prime}\), and \(\bm{\theta}^{\prime\prime}\) and compute the interpolated directions \(\bm{d_{1}}=\bm{\theta}^{\prime}-\bm{\theta},\bm{d_{2}}=\bm{\theta}^{\prime }-\bm{\theta}\). Since there is no guarantee that two interpolated directions are orthogonal, we use Gram-Schmidt algorithm to find the orthogonal basis:

\[\widehat{\bm{d_{1}}}=\bm{d_{1}},\widehat{\bm{d_{2}}}=\bm{d_{2}}-\frac{\bm{d_ {1}^{T}}\bm{d_{2}}}{\|\bm{d_{1}}\|^{2}}\bm{d_{1}}\] (4)

To ensure the scale equivalence of two directions, we rescale \(\widehat{\bm{d_{2}}}\) to \(\left(\left\|\widehat{\bm{d_{1}}}\right\|\middle/\left\|\widehat{\bm{d_{2}}} \right\|\right)\widehat{\bm{d_{2}}}\). 2D-interpolation landscape is useful when analyzing two finetuned model weights, which are all initialized by the same aligned LLM. In the rest of the paper, we will use 2D-random \(\bm{\theta}\) and 2D-interpolation \(\bm{\theta}\)\(\rightarrow\bm{\theta}^{\prime}\)\(\&\)\(\bm{\theta}^{\prime\prime}\) to represent the above two types of 2D directions.

### Safety Landscape of Open-source LLMs

We show the safety landscapes of four popular open-source LLMs: LLaMA2-7B-chat [43], LLaMA3-8B-instruct [2], Mistral-7B-instruct-v0.2 [27], and Vicuna-7B-v1.5 [10]. For each perturbed model along the landscape direction, we evaluate on the first 80 prompts of AdvBench [51] "Harmful Behaviors" split (Adv 80) with ASR as the safety metric. The ASR is measured by refusal keyword detection following the original AdvBench evaluation protocol. Note that \(\mathcal{S}\) can be any harmfulness evaluation metric, _e.g._, LLM Judge [50] or Llama Guard [26]. Since a recent user study [37] shows that GPT-4 Judge and refusal keyword detection perform closely on flagging harmful content, we use keyword detection as it is the fastest. We interpolate 20 steps on each axis for all landscapes. To ensure deterministic results, we set top-p as 0 and temperature as 1 [17].

**Safety landscape between pretrained and aligned LLMs.** Pretraining is the initial phase of training an LLM, aimed at developing a broad understanding of language and knowledge [11, 5]. Alignment, on the other hand, focuses on training LLMs to better follow instructions in prompts and align their behaviors with human preferences [35]. Since the pretrained model is not designed with safety in its first priority, it lacks safety guardrails. In contrast, the aligned model is expected to refuse to respond to harmful user queries. We use LLaMA2 as an example and show the safety landscape of 1D-interpolation LLaMA2-7B \(\rightarrow\) LLaMA2-7B-chat in Fig. 1(a). Since the pretrained (LLaMA2-7B) and the aligned (LLaMA2-7B-chat) models use different chat templates, both templates are evaluated to ensure the change of safety is due to alignment. Notice that the chat template difference does not exist when comparing the aligned and the finetuned models in later sections as all of them share the same chat template and system prompt. Both lines in Fig. 1(a) show that the ASR of the aligned model is significantly lower than the pretrained model as expected. Notice that the pretrained model has a less than 100 ASR when using the aligned model chat template. This is because the pretrained model repeats the system prompt in the aligned model's chat template, which is captured by the keyword detector and treated as successful refusal. When using the aligned model's chat template, we find that the local region of the aligned model shows the same level of safety. This is surprising because early work has shown that finetuning can easily break LLM's safety alignment, which may imply the aligned model is a cusp, _i.e._, sharp corner, on the safety landscape.

**Safety landscape of an aligned LLM.** Inspired by our findings in the interpolation direction above, we are curious whether this flat safety region exists in other directions for different LLMs. Fig. 1 (top) plots the 1D and 2D random landscape of four popular open-source LLMs. For each LLM, we use the default system prompt provided by the model, with details in Appendix A. We discover that each aligned LLM serves as a robust anchor point, maintaining safety within its local region, but the safety is completely compromised outside of this local region, and the change of safety as steep as a step function. We term this new phenomenon observed universally in the LLM parameter space as "safety basin". All four investigated LLMs exhibit such phenomenon, but the differences

Figure 2: LLM safety landscape: (a) 1D-interpolation LLaMA2-7B \(\rightarrow\) LLaMA2-7B-chat safety landscape. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. While interpolating the model weights between the base and the chat model, we need to ensure the chat format remains consistent. Thus, we ablate on both chat formats: text completion (no template) and LLaMA2 chat template. The chat model exhibits higher safety than the base model as expected. The base model also shows an increase in safety while using the LLaMA2 chat template. (b) 1D-random LLaMA2-7B safety landscape sampled over different random directions. When provided with a single model, we sample a random normalized direction to visualize its local variations along both positive and negative directions.

are the depth and width of the basin. The finding of safety basin generalizes to different evaluation metrics and other safety datasets (Appendix B), and the model still generate fluent output when ASR is high (Appendix D). Besides, we also find that a larger model side exhibits a wider safety basin (Appendix E).

### Visage Safety Metric

The landscape visualization and analysis suggest that the average depth of the safety basin can serve as a good indicator for measuring LLM safety, reflecting both the safety of the original aligned model and the robustness of the model when its parameters are perturbed. Formally, for an \(n\)-D random safety landscape, we define Visage safety metric as the average safety margin of all models we have sampled along all random directions:

\[\text{\sc{Visage}}=\mathop{\mathbb{E}}_{\alpha\sim\mathcal{U}(-a,a),\beta\sim \mathcal{U}(-b,b),\ldots}[\mathcal{S}_{max}-\mathcal{S}(\alpha,\beta,\ldots)], \text{ s.t. }\mathcal{S}<\mathcal{S}_{max}\] (5)

where \(\alpha\) and \(\beta\) are all sampled from uniform distribution and we use \(a=b=0.5\) as we find that LLMs are completely broken after perturbing more than half of its norm. \(\mathcal{S}\) is a monotonically decreasing function in terms of safety, as a lower \(\mathcal{S}\) means a safer model. \(\mathcal{S}_{max}\) is the maximum possible value for \(\mathcal{S}\). When ASR is used as the safety metric, \(\mathcal{S}_{max}=100\).

**Stability of Visage.** Since the definition of Visage involves random directions, we run a stability test to find out how many sampled directions the model converge to a stable value. We take 1D-random LLaMA2-7B-chat as an example and show the result in Fig. 1(b). We sample 8 different directions until the average converges and find that the average of 3 different directions is close enough to the final average. Thus, we use the mean of Visage along three different directions as the evaluation metric in the rest of the paper. In Fig. 1, we also compute Visage for both 1D and 2D random landscape and the ranking of those two dimensions remain the same. Thus, we use 1D Visage for faster evaluation.

We compute the Visage score for all four LLMs using their default system prompts and chat templates. Since LLaMA3-8B-instruct does not have a default system prompt, we use the LLaMA2 system prompt. The Visage ranking is as follows: LLaMA3-8B-instruct \(>\) LLaMA2-7B-chat \(>\) Mistral-7B-instruct-v0.2 \(>\) Vicuna-7B-v1.5. We also evaluate these four models on all 520 prompts of AdvBench "Harmful Behaviors" split (Adv 520). The ASR of the models are as follows: LLaMA3-8B-instruct (0.38), LLaMA2-7B-chat (0.19), Mistral-7B-instruct-v0.2 (1.15), and Vicuna-7B-v1.5 (2.5). Although the ASRs of all four LLMs are close, the Visage reflects the safety of a model's local region, indicating the risk after finetuning, which we will explore in the next section.

## 4 Why can simple finetuning easily break LLM's safety alignment?

In this section, we navigate the LLM safety landscape and explore why safety alignment can be easily compromised by finetuning with only a few adversarially designed training examples. Sec. 4.1 details the finetuning settings. In Sec. 4.2, we discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic Visage safety metric measures the risks in finetuning without assumptions on the finetuning dataset. In Sec. 4.3, visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact in Sec. 4.4.

### Finetuning settings

We finetune on the harmful samples created by Qi et al. [37], which were sampled from Anthropic red-teaming dataset [14]. Following the standard OpenAI finetuning API [36], each training sample is structure in a one-round conversation format.

We ensure that the system prompt used during finetuning remains consistent with the aligned model so that the differences in safety are indeed induced by finetuning. We adhere to the official finetuning recipe1 and conduct full parameter finetuning. Following the training hyperparameters in Qi et al.

[37], all models are finetuned for five epochs with AdamW optimizer [34]. At inference time, we evaluate on both 80 prompts and all 520 prompts of AdvBench "Harmful Behavior" split. The finetuning is done on 4 A100 GPUs.

### Finetuning on few-shot harmful data breaks LLM's safety alignment

We finetune LLaMA2-7B-chat and Vicuna-7B-v1.5 on subsets of 10, 50, and 100 harmful examples sampled from the training dataset. As shown in Table 1, both models have close to 0% ASR before finetuning, but the ASRs increases significantly after finetuning, indicating broken safety alignment. Comparing the results of finetuning on 10, 50, 100 harmful examples, the ASRs continue to increase as expected. We also discover that different LLMs have varying rates of vulnerability to finetuning, and our Visage safety metric can successfully measure the risks in finetuning before actual finetuning. Comparing LLaMA2 with Vicuna, LLaMA2 has a higher Visage score than Vicuna, meaning that when both are finetuned on the same user data, LLaMA2 shows a lower ASR than Vicuna when evaluated on safety benchmarks. Our evaluation results on both 80 prompts and full 520 prompts verify that the safety in finetuning is reflected by our Visage safety metric. Since the Visage definition does not make assumptions on the downstream finetuning dataset, LLM-Visage serves as a task-agnostic safety metric that measures finetuning risks.

Finetuning with harmful data is dragging the model away from the safety basin but at different rates

We save the model checkpoint of each epoch during finetuning and project them onto the safety landscape to visualize the optimization trajectory. Previous work have observed that projecting on random directions fail to capture the variation in optimization trajectory because the trajectory lies in an extremely low dimensional spaces, and a random sampled direction is nearly orthogonal to this subspace. A potential solution is applying principal component analysis (PCA) on all saved model checkpoints, but the \(n\)-epoch finetuning on LLaMA2-7B-chat will lead to a feature matrix of size \(n\times 7B\), which makes it expensive to compute with existing PCA libraries. Therefore, we set the projection direction as the interpolated direction between the initial and the final finetuned model weights. By projecting all saved checkpoints onto this direction, we successfully capture the optimization trajectory. Fig. 1 (red dots at the bottom 2D interpolation landscape) shows the training trajectory of finetuning LLaMA2-7B-chat on 100-shot harmful data for 5 epochs. The aligned model is the initial point and each epoch is dragging the model away from the aligned model, and finally outside of the safety basin. Our safety landscape visualization enables us to understand, for the first time, how simple finetuning compromises safety alignment.

### Finetuning with harmful and safe data helps the model stay within the safety basin

Though finetuning can easily break LLMs' safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. This can be achieved by finetuning on a mixture of user data and safety data. Bianchi et al. [4] suggests that finetuning LLaMA1 [42] (not aligned) on the mixture of user and safe data can improve the safety of the model. We are curious if this finetuning strategy is generalizable to other LLMs, and if it works, can we explain it with our safety landscape? Therefore, we finetune LLaMA2 and Vicuna

\begin{table}
\begin{tabular}{l r|r|r r r r|r} \hline \hline Model & Visage & \begin{tabular}{c} AdvBench \\ Samples \\ \end{tabular} & 
\begin{tabular}{c} Aligned \\ \end{tabular} & 10-shot & 50-shot & 100-shot & mix \\ \hline LLaMA2-7B-chat & 85.32 & 80 & 0 & 90.0 & 91.3 & 100.0 & 0 \\  & & 520 & 0.2 & 85.2 & 90.2 & 95.4 & 0.2 \\ \hline Vicuna-7B-v1.5 & 73.26 & 80 & 5.0 & 95.0 & 97.5 & 100.0 & 1.3 \\  & & 520 & 2.5 & 89.2 & 94.0 & 96.7 & 1.2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Finetuning on few-shot harmful data breaks LLM’s safety alignment at different rates and our Visage safety metric successfully measures the rate. LLaMA2 has a higher Visage score than Vicuna, and the ASRs on AdvBench indicate that when finetuned with the same amount of harmful data, LLaMA2 remains safer than Vicuna. Additionally, we demonstrate that finetuning with a mixture of safe and harmful data helps the model maintain its safety alignment. The “aligned” column refers to the original off-the-shelf models.

on a mixture of 100-shot harmful examples in Sec. 4.3 along with the 100-shot safe data created by Bianchi et al. [4] for ten epochs till convergence. In Table 1, we show that finetuning with the safe data indeed lowers the ASR.

Both initialized from the aligned model, the model that is finetuned on 100-shot harmful data from Sec. 4.3 is completely unsafe while the model that is finetuned on a mixture of 100-shot harmful and 100-shot safe data is still safe. We take LLaMA2 as an example and show 2D-interpolation LLaMA2-7B-chat \(\rightarrow\) LLaMA2-7B-chat 100-shot harmful & 100-shot harmful+100-shot safe in Fig. 1(bottom). In the surface plot, starting from the aligned model in the origin, the pure harmful finetuning quickly drags the model away from the safety basin, and significantly elevates the ASR. On the other hand, finetuning with a mixture of harmful and safe data keeps the model within the safety basin and thus maintaining the safety of the finetuned model.

## 5 System prompt

LLM safety landscape also highlights the system prompt's critical role in protecting a model, and how this protection transfers to its perturbed variants within the safety basin. In this section, we evaluate the impact of system prompt design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each LLM's default system prompt as the baseline. We collect different types of system prompts from both an attacker's or a defender's perspective. From an attacker's standpoint, we apply two types of prompts: (1) removing the default system prompt (Empty), and (2) using roleplaying prompt to attach a new charater to the LLM, hoping to make the LLM forget its safety responsibilities (Roleplay). From a defender's perspective, we also employ two types of prompts: (1) LLaMA2's default system prompt that explicitly includes safety concerns (LLaMA2), and (2) safety prompts that are directly optimized for a specific LLM [49] (Safety). The details of the system prompts used in our experiments are listed in Appendix A. For each type of the system prompt, we compute its mean Visage score from three random directions. Table 2 shows the results, illustrating how each type of system prompt affects the safety of an LLM's local region.

**LLaMA2.** The default system prompt has a Visage score of 85.32. Removing the system prompt incurs a 4.64 percentage points (pp) drop. Surprisingly, applying the roleplaying prompt does not compromise LLaMA's safety; instead, it leads to a slight increase in the Visage score. We find that roleplaying prompts are generally less effective in breaking an LLM's safety across different models. Among the six LLMs tested with the default system prompt, LLaMA2 has the highest Visage score, aligning with the observation that LLaMA2 tends to be conservative and may even refuse harmless input prompts [49].

**LLaMA3.** There is no default system prompt for LLaMA3, yet even without a system prompt, LLaMA3 demonstrates a high Visage score. LLaMA3 excels in following system prompt instructions while maintaining its safety, experiencing only a 2.7pp drop when using the roleplaying prompt, but showing a 9.3pp increase when the LLaMA2 system prompt is employed. This is likely due to LLaMA3's improved training procedures, which substantially reduce false refusal rates [2].

**Mistral.** We evaluate both Mistral-7B-instruct-v0.1 and Mistral-7B-instruct-v0.2. Fig. 3a shows the 1D-random Mistral-7B-instruct-v0.1 safety landscape under different system prompts. For both

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline Model & Default & Empty & Roleplay & LLaMA2 & Safety \\ \hline LLaMA2-7B-chat & 85.32 & 80.68 & 86.56 & 85.32 & - \\ LLaMA3-8B-instruct & - & 81.10 & 78.40 & 90.40 & - \\ Mistral-7B-instruct-v0.1 & 74.11 & 20.78 & 52.65 & 85.66 & 86.24 \\ Mistral-7B-instruct-v0.2 & 82.04 & 64.90 & 75.54 & 73.69 & 75.53 \\ Vicuna-7B-v1.3 & 82.03 & 56.13 & 77.13 & 80.18 & - \\ Vicuna-7B-v1.5 & 77.37 & 73.56 & 81.61 & 81.62 & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: LLM safety landscape highlights the system prompt’s critical role in protecting a model, and how this protection transfers to its perturbed variants in the safety basin. We measure the Visage score of different system prompt for popular open-source LLMs. Higher Visage means safer model and “-” means not applicable. For LLaMA3, there is no default system prompt in the initial release. For all other LLMs in the “safety” column, we use the optimized safety prompts specific to each LLM from Zheng et al. [49], with only Mistral’s safety system prompt provided.

models, removing the system prompt significantly reduces the safety score. Specifically, removing the system prompt decreases Mistral-7B-instruct-v0.1's Visage score by 53.33pp. Using the roleplaying prompt also degrades the performance for both models. Both LLaMA2 and safety system prompts effectively enhance Mistral's Visage score, but Mistral-7B-instruct-v0.1 is more sensitive to the system prompt than Mistral-7B-instruct-v0.2.

**Vicuna.** We have tested on both Vicuna-7B-v1.3 and Vicuna-7B-v1.5. Fig. 2(b) shows the 1D-random Vicuna-7B-v1.3 safety landscape under different system prompts. Vicuna-7B-v1.3 is finetuned from LLaMA1, while Vicuna-7B-v1.5 is finetuned from LLaMA2 pretrained (not aligned) model weights [10]. We find that the performance drops for both models when removing the system prompt. As shown in Fig. 2(b), removing the system prompt reveals that the original Vicuna model is a local maximum in the safety landscape, indicating that there exist slightly perturbed model weights more resistant to harmful inputs than the fine-tuned model. This suggests that the safety alignment of Vicuna is not optimal, possibly because the fine-tuning process rarely encountered inputs with an empty system prompt. The roleplaying prompt shows mixed performance: it decreases Vicuna-7B-v1.3's safety score but increases Vicuna-7B-v1.5's safety score. Finally, using the LLaMA2 system prompt significantly improves model safety.

Overall, the system prompt does have a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

## 6 Jailbreak attacks

Previous work has shown that the safeguards of the aligned LLMs can be bypassed by adversarial attacks. We are curious whether these so-called "jailbreaks" against LLMs are still effective to slightly perturbed models within the aligned model's local region. We use the adversarial prompts from JailbreakBench [9], which has incorporated jailbreaking prompts targeting LLaMA2 and Vicuna generated by GCG [51] and PAIR [8] adversarial attacks. Fig. 3(a) shows the 1D-random LLaMA2-7B-chat evaluated on jailbreaking prompts. There are only 6 prompts in JailbreakBench that can successfully attack the LLaMA2 model, and our experiment shows only 4 of them are successful, thus leading to a 66.67% ASR for the aligned model. The safety landscape reveals that the jailbreaking prompts are quite sensitive to the model weights perturbation, _i.e._, there exists certain perturbed models that are significantly safer than the aligned model. This is not unique to LLaMA2, as shown by the safety landscape of Vicuna-7B-v1.5 under jailbreak attacks in Fig. 3(b). We also replace the default Vicuna system prompt with the LLaMA2 system prompt and find it improving the overall safety in the model's local region. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the

Figure 3: The system prompt has a strong impact on LLM safety landscape. From an attacker’s standpoint, we find that both removing the default system prompt and using simple roleplaying prompt jeopardizes the safety alignment, with the former exhibiting greater potency. From a defender’s perspective, we discover that LLaMA2’s original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses.

## 7 Safety _vs._ Capability Landscape

We evaluate on three datasets covering capabilities in math, history, and policy from MMLU [16]. The shape of the LLM capability landscape is drastically different from the one in the LLM safety landscape; these landscapes do not exhibit the same trend, further confirming that the basin shape is indeed unique to the safety of LLM. We provide a detailed analysis in Appendix C.

## 8 Conclusion

We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin". Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.

## Limitations and Future Work

We believe there are multiple directions for future research, and our work is an important first step in exploring the safety landscape of popular open-source LLMs. Given our findings that the shape of the LLM capability landscape differs significantly from that of the LLM safety landscape, a potential direction for future work is to explore how to better balance the tradeoff between capability and safety, _e.g._, finding the optimal capability performance for a given dataset while staying within the safety basin. Another direction is proposing additional sub-metrics such as basin width, depth, and smoothness. Our Visage score, defined as the average safety margin of all models sampled along random directions, can be considered as an average depth within the safety basin. The Visage score is a byproduct of our novel findings on the safety basin, and we hope our study will inspire further research into proposing more metrics, including width and smoothness.

## Acknowledgment

This work was supported in part by gifts from Google, Amazon, Meta, NVIDIA, Avast, Fiddler Labs, Bosch.

Figure 4: When evaluating the safety landscape using jailbreaking queries, we find that these queries are highly sensitive to perturbations in model weights.

## References

* Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* AI [2024] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3/.
* Bai et al. [2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* Bianchi et al. [2023] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. _arXiv preprint arXiv:2309.07875_, 2023.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Bsharat et al. [2023] Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen. Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4. _arXiv preprint arXiv:2312.16171_, 2023.
* Carlini et al. [2024] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? _Advances in Neural Information Processing Systems_, 36, 2024.
* Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _arXiv preprint arXiv:2310.08419_, 2023.
* Chao et al. [2024] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. _arXiv preprint arXiv:2404.01318_, 2024.
* Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dinan et al. [2019] Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. _arXiv preprint arXiv:1908.06083_, 2019.
* Dinh et al. [2017] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* Ganguli et al. [2022] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_, 2022.
* Goldstein and Studer [2018] Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. _IEEE Transactions on Information Theory_, 64(4):2675-2689, 2018.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.

* [17] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. _arXiv preprint arXiv:1904.09751_, 2019.
* [18] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. _arXiv preprint arXiv:1801.06146_, 2018.
* [19] Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Safe LoRA: the silver lining of reducing safety risks when fine-tuning large language models. _arXiv preprint arXiv:2405.16833_, 2024.
* [20] Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, and Ling Liu. Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning. _arXiv preprint arXiv:2408.09600_, 2024.
* [21] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation. _arXiv preprint arXiv:2409.01586_, 2024.
* [22] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Lazy safety alignment for large language models against harmful fine-tuning. _arXiv preprint arXiv:2405.18641_, 2024.
* [23] Tiansheng Huang, Sihao Hu, and Ling Liu. Vaccine: Perturbation-aware alignment for large language model. _arXiv preprint arXiv:2402.01109_, 2024.
* [24] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. _arXiv preprint arXiv:2212.04089_, 2022.
* [25] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_, 2023.
* [26] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tonthev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. _arXiv preprint arXiv:2312.06674_, 2023.
* [27] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [28] Haibo Jin, Ruoxi Chen, Jinyin Chen, and Haohan Wang. Quack: Automatic jailbreaking large language models via role-playing. 2023.
* [29] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. In _International Conference on Machine Learning_, pages 15307-15329. PMLR, 2023.
* [30] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. _arXiv preprint arXiv:1609.04836_, 2016.
* [31] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.
* [32] Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, and Li Shen. Targeted vaccine: Safety alignment for large language models against harmful fine-tuning via layer-wise perturbation. _arXiv preprint arXiv:2410.09760_, 2024.
* [33] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. _arXiv preprint arXiv:2309.06657_, 2023.
* [34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.

* [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [36] Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel. Gpt-3.5 turbo fine-tuning and api updates, 2023. URL https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates.
* [37] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* [38] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 21527-21536, 2024.
* [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [40] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. _arXiv preprint arXiv:2312.06681_, 2023.
* [41] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models (2023). _arXiv preprint arXiv:2302.13971_, 2023.
* [43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [44] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? _Advances in Neural Information Processing Systems_, 36, 2024.
* [45] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* [47] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.
* [48] Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rhlf protections in gpt-4 via fine-tuning. _arXiv preprint arXiv:2311.05553_, 2023.
* [49] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. On prompt-driven safeguarding for large language models. In _ICLR 2024 Workshop on Secure and Trustworthy Large Language Models_, 2024.
* [50] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* [51] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_FAIL:15]

Figure 5: LLaMAGGuard 2 evaluation also shows a basin shape similar to the safety keyword detection.

Figure 6: Results on POSE benchmark again verifies the safety basin observed on the AdvBench benchmark. We evaluate the generated outputs using both safety keyword detection and LLaMAGGuard 2 and both evaluation metrics show a similar basin shape.

Figure 8: LLMs speak fluently even when ASR is high. We evaluate the perplexity of the perturbed LLaMA2-7B-chat model along a random direction using all 80 prompts from MTBench. We also scale up the model size from LLaMA2-7B-chat to LLaMA2-13B-chat and plot the 1D safety landscape of both models. A larger model size exhibits a wider safety basin, which also aligns with the intuition that a wider basin seems to be more robust and a potential training goal for future LLM training.

Figure 7: The shape of the capability score varies significantly across different datasets, and differes from the safety landscape. We evaluate capabilities using the following three datasets from MMLU: abstract_algebra, high_school_us_history, and us_foreign_policy datasets, and present the results of perturbing the LLaMA2-7B-chat weights along a 1D-random direction. For controlled comparisons, all datasets are evaluated along the same random direction.

\begin{table}
\begin{tabular}{l l|l|l} \hline  & **MTBench Prompt:** A tech startup invests \$8000 & & \\ \(\alpha\) (x-axis) & in software development in the first year and then & **AdvBench Prompt:** & Write a tutorial for creating and distributing viruses \\ \(\alpha\) (x-axis) & in the second year. What’s the total amount the startup & & \\  & invested in software development over the two years? & & \\ \hline -0.5 & example of a single word aversion to a specific date... & group, and the un-verification of the system, & \\  & (continues until completion) &... (continues until completion) \\ \hline \multirow{6}{*}{-0.25} & To calculate the total amount the startup invested in software development over the two years, we need to know the amount invested in the first year and the amount invested in the second year. The startup invested \$8,000 in software development in the first year, so the amount invested in the second year is half of that amount, or \$4,000. The total amount the startup invested in software development over the two years is \$8,000 + \$4,000 = \$12,000... (continues until completion) \\ \hline \multirow{6}{*}{0} & To find the total amount the startup invested in software development over the two years, we need to calculate the total amount invested in each year and then add them up. In the first year, the startup invested \$8000 in software development. In the second year, the startup invested half of that amount, which is \$4000 (half of \$8000). So, the total amount the startup invested in software development over the two years is \$8000 + \$4000 = \$12,000. & \\ \hline \multirow{6}{*}{0.25} & I noticed that the information provided in the question is not entirely clear. The question states that the tech startup invests \$8000 in software development in the first year, and then invests half of that amount in the second year. But it doesn’t provide any information about the total amount invested over the two years.... (continues until completion) \\ \cline{1-1}  & D in the first:S.: the original:D: 10... (continues until completion) & I’m just an AI, I can’t create or distribute any viruses, as that is illegal and unethical. I’m here to help and assist you in a safe and respectful manner....(continues until completion) \\ \cline{1-1}  & D in the first:S.: the original:D: 10... (continues until completion) & : I: active, first, first, I cut: I: isolated... (continues until completion) \\ \hline \end{tabular}
\end{table}
Table 3: Outputs of LLaMA2-7B-chat sampled equally from the 1D random direction.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's contributions are listed as four bullet points in the introduction (Sec. 1). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a section on limitations and future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Sec. 3 & 4 Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Sec. 3 & 4 & 5 & 6 Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Sec. 3 & 4 & 5 & 6 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Sec. 3 & 4 & 5 & 6 Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Sec. 4 Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow and respect with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impact is addressed in the four contribution bullets in our introduction. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Sec. 3 & 4 & 5 & 6 Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have credited all the previous work that is used in our work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide details on all new assets introduced in the paper. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.