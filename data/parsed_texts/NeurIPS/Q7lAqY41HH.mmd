# CRAG - Comprehensive RAG Benchmark

Xiao Yang*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

 Kai Sun*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Hao Xin*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nikita Bhalla

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Xiangsen Chen

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Sajal Choudhary

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Rongze Daniel Gui

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Ziran Will Jiang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Ziyu Jiang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Lingkun Kong

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Brian Moran

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Jiaqi Wang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yifan Ethan Xu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

An Yan

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Chenyu Yang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Eting Yuan

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Hanwen Zha

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nan Tang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Lei Chen

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)
its potential, RAG still faces many challenges, such as selecting the most relevant information, reducing question answering latency, and synthesizing information to answer complex questions.

A comprehensive benchmark is currently missing to advance continued research efforts in this field. Our goal is to build a benchmark that can provide a holistic view of the important capabilities and fast but reliable evaluation for RAG to propel the area forward. What is a good benchmark for QA over LLMs? We consider five critical features.

1. **Realism:** First and foremost, a good benchmark shall best reflect real use cases. In other words, a solution that achieves high metrics in the benchmark shall also perform very well in real scenarios. For example, the questions in a RAG benchmark shall be similar to questions people ask in real-world QA scenarios.
2. **Richness:** The benchmark shall contain a diverse set of instance types, covering both common use cases and some complex and advanced use cases, to represent real-world challenges and reveal possible limitations of existing solutions.
3. **Insightfulness:** The benchmark shall allow for an easy understanding of performance on different slices of the data, reflecting the capability of the solution in addressing different types of challenges.
4. **Reliability:** The benchmark shall allow reliable assessment of metrics: the ground truths shall be accurate; the metrics shall well capture the performance of the model; the evaluation shall be easy and reliable, and the computed metrics shall hold statistical significance.
5. **Longevity:** Finally, to enable research and experimental comparison in a long term, the scenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed and improved over time.

We strive to create a benchmark that have all of the aforementioned features, and we call it _CRAG - Comprehensive benchmark for RAG_. Our work makes three contributions.

Our first contribution is the dataset itself (Section 3). CRAG contains a _rich_ set of 4,409 QA pairs from five domains: _Finance, Sports, Music, Movie_, and _Open domain_. In addition to simple-fact questions (asking for an attribute of an entity), CRAG contains seven types of complex questions to cover real user queries: questions with _Conditions_, _Comparison_ questions, _Aggregation_ questions, _Multi-hop_ questions, _Set queries_, _Post-processing-heavy_ questions, and _False-premise_ questions. CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from seconds to years, allowing easy deep dives for _insights_. As we generated the questions, we referred to smart assistant use cases to make sure the questions are _realistic_, paraphrased the questions to increase the _diversity_ of expressions, and manually verified ground truths to ensure _reliability_.

In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of available information. This includes up to 50 full HTML pages for each question returned from a real-world search engine--the Brave Search API [5], and mock KGs with 2.6 million entities. For the mock KGs, we deliberately make sure that the retrieval candidates reflect noises in a _realistic_ setting.

Our second contribution is the evaluation mechanism to allow for _reliable_ comparisons. We designed 3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph and web retrieval, and end-to-end retrieval-augmented generation (Section 2). Instead of computing the percentage of correctly answered questions, our score system distinguishes hallucinated answers and missing answers, and gives the former a higher penalty as it can be more harmful to ruin user

Figure 1: QA using LLMs (a) without RAG vs. (b) with RAG.

trust. We also design an effective automatic evaluation mechanism to allow for fast evaluations and iterations (Section 4).

Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry state-of-the-art solutions on RAG (Section 5). Whereas most advanced LLMs achieve \(\leqslant 34\%\) accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions answer only \(63\%\) questions without any hallucination, still having much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity. These evaluations serve two roles: first, they demonstrate that CRAG has appropriate level of difficulty and allows insights drawn from different dimensions of diversities the benchmark has incorporated; second, they highlight the gaps and research directions to a fully trustworthy QA system.

The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.

Footnote 2: https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024

**Comparison with existing benchmarks.** Table 1 compares CRAG with existing benchmarks for factual question answering. Traditional QA benchmarks such as Natural Questions (NQ) [18], TriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but consider only web retrieved _or_ KG retrieved contents, and do _not_ adequately represent the diverse and dynamic challenges that RAG is facing. New benchmarks for LLM or RAG usually target certain capabilities of the QA system. Researchers created benchmarks to evaluate how well the systems can answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. These include answering questions with changing answers [36], integrating information from multiple documents [6], addressing multi-hop questions [33], and answering questions with long texts [26]. Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21] or F1 to evaluate the quality of the responses [16, 18]. These metrics, although working well for extractive methods, are known to not perform very effectively for LLMs that generate free-form responses [11].

Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages: comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. These features make CRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing a shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information retrieval and synthesis challenges for reliable LLM-based question answering.

## 2 Problem Description

A RAG QA system takes a question \(Q\) as input and outputs an answer \(A\); the answer is generated by LLMs according to information retrieved from external sources or directly from the knowledge internalized in the model. The answer should provide useful information to answer the question without adding any hallucination.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Benchmark & Web & KG & Mock & Dynamic & Torso and & Beyond & Question \\  & retrieval & search & API & question & tail facts & Wikipedia & size \\ \hline QALD-10 [35] & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & 0.8K \\ MS MARCO [4] & ✓ & ✗ & ✗ & not explicitly & not explicitly & ✓ & 100K \\ NQ [18] & ✓ & ✗ & ✗ & not explicitly & not explicitly & ✗ & 323K \\ RGB [6] & ✓ & ✗ & ✗ & ✗ & ✓ & 1K \\ FreshLLM [36] & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & 0.6K \\ CRAG & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 4.4K \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparing CRAG to existing benchmarks for factual question answering.

We designed three tasks. They share the same set of (question, answer) pairs but differ in the external data accessible for retrieval to augment QA. Here, we provide the content that can be leveraged in QA to ensure fair comparisons. We describe how we generated the data in Section 3.

**Task 1: Retrieval Summarization.** In Task 1, we provide up to five web pages for each question. These web pages are likely, but not guaranteed, to be relevant to the question. This task aims to test the answer generation capability of a RAG system.

**Task 2: KG and Web Retrieval Augmentation.** In Task 2, we in addition provide _mock APIs_ to access information from underlying _mock KGs_. The mock KGs store structured data relevant to the questions; answers to the questions may or may not exist in the mock KGs. The mock APIs take input parameters, oftentimes parsed from the question, and provide structured data from the mocked KGs to support answer generation. This task tests how well a RAG system 1) queries structured data sources and 2) synthesizes information from different sources.

**Task 3: End-to-end RAG.** Similar to Task 2, Task 3 also provides both web search results and mock APIs as candidates for retrieval but provides \(50\) web pages, instead of \(5\), as candidates. The larger set of web pages are more likely to provide necessary information to answer the question, but meanwhile are more likely to contain noises. As such, Task 3 in addition tests how a RAG system ranks a larger number of retrieval results.

The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG systems. The only component of a RAG system not covered by these tasks is search retrieval. One may easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully end-to-end testing.

## 3 Dataset Description

CRAG contains two parts of data: the QA pairs and the contents for retrieval. We now describe each part of the data. Data generation details can be found in Appendix A.1.1-A.1.6.

\begin{table}
\begin{tabular}{p{142.3pt} p{284.5pt}} \hline \hline
**Question type** & **Definition** \\ \hline Simple & Questions asking for simple facts that are unlikely to change overtime, such as the birth date of a person and the authors of a book. \\ \hline Simple w. Condition & Questions asking for simple facts with some given conditions, such as stock prices on a certain date and a director’s recent movies in a certain genre. \\ \hline Set & Questions that expect a set of entities or objects as the answer (e.g., “_what are the continents in the southern hemisphere?_”). \\ \hline Comparison & Questions that compare two entities (e.g., “_who started performing earlier, Adele or Ed Sheeran?_”). \\ \hline Aggregation & Questions that require aggregation of retrieval results to answer (e.g., “_how many Oscar awards did Meryl Strep win?_”). \\ \hline Multi-hop & Questions that require chaining multiple pieces of information to compose the answer (e.g., “_who acted in Ang Lee’s latest movie?_”). \\ \hline Post-processing-heavy & Questions that need reasoning or processing of the retrieved information to obtain the answer (e.g., “_how many days did Murgood Marshall serve as a Supreme Court justice?_”). \\ \hline False Premise & Questions that have a false preposition or assumption (e.g., “_What’s the name of Taylor Swift’s rap album before she transitioned to pop?_” (Taylor Swift has not yet released any rap album)). \\ \hline \hline \end{tabular}
\end{table}
Table 2: Definition of CRAG question types.

[MISSING_PAGE_FAIL:5]

Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall of 85% when using all 50 pages. It reflects multiple advantages of the benchmark by design. First, the recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is about 69%. This is comparable to what we observe in practice when developing a RAG system. The non-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit "I don't know" when the retrieval results do not contain the necessary information. Second, compared to the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the importance of extracting and understanding HTML contents. Moreover, the estimated web search recall (50 web pages) is \(93\%\) for _Web Questions_ and \(74\%\) for _KG Questions_, indicating significantly lower recall for KG questions than web questions. This aligns with our observations that web search recall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in Task 2 and 3.

**Mock KGs.** We created mock KGs that contain publicly available KG data used to generate the questions, randomly selected entities of the same type, and also "hard negative" entities with similar names (e.g., _"phantom"_ for _"phantom of the opera"_).

**Mock APIs.** We created mock APIs with pre-defined parameters to support structured search in the mock KGs. For example, for queries asking for stock prices, an example mock API is in the form of get_price_history(ticker).

We collected snapshots of the KG and web search data concurrently while posing real-time and fast-changing questions. This approach ensures that we capture the "snapshot" of the information world at the time of question answering. A RAG solution that performs well on the benchmark should also be capable of reasoning over time and generalizing to evolving questions.

In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. See Table 9 in the Appendix for a complete list of the mock APIs.

## 4 Metrics and Evaluation

In this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024 Meta KDD Cup challenge in Appendix A.2.3.

### Metrics

We use a scoring method to assess the performance of RAG systems. For each question in the evaluation set, we first label the answer with **perfect, acceptable, missing,** or **incorrect**, according to the following criteria.

**Perfect.** The response correctly answers the user's question and contains no hallucinated content.

Figure 2: For \(85\%\) of CRAG questions, the web search results are estimated to contain the ground truth facts. The curve shows that the retrieval recall grows sharply at the beginning and flattens out later on.

**Acceptable.** The response provides a useful answer to the user's question but may contain minor errors that do not harm the usefulness of the answer.

**Missing.** The response is "I don't know", "I'm sorry I can't find...", a system error such as an empty response, or a request from the system to clarify the original question.

**Incorrect.** The response provides wrong or irrelevant information to answer the user's question.

We use a scoring method with score \(1\), \(0.5\), \(0\), and \(-1\) for each _perfect, acceptable, missing_, and _incorrect_ answer, respectively, where we penalize hallucinated answers and prefer _missing_ answers to _incorrect_ ones. We then define **truthfulness** as the average score from all examples in the evaluation set for a given RAG system.

### Evaluation

Similar to previous work [37], we employ both human evaluation **(human-eval)** and model-based automatic evaluation **(auto-eval)**. In the former, we use manual grading to judge _perfect, acceptable, missing_, and _incorrect_ for each answer. In the latter, we merge _perfect_ and _acceptable_, call it **accurate**, and use a three-way scoring system with \(1,-1,0\) for _accurate_, _incorrect_, and _missing_ answers.

We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly, it is considered _accurate_; otherwise, we use LLMs to determine whether the response is _accurate, incorrect_, or _missing_. To avoid the _self-preference_ problem [25], we use two LLM evaluators: ChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the average _accurate, hallucination, missing_ rates, and _truthfulness_ scores from the two models for each RAG system. Our offline experiment shows that this two-step method yields an average F1 score of \(94.7\%\) for ChatGPT and \(98.9\%\) for Llama 3 compared to human-eval. See Appendix A.2.2 for more details.

**Test data split.** We split the data randomly into _validation_ (30%), _public test_ (30%), and _private_ (40%), and released the validation and public test sets (Appendix A.2.3). Participants of the KDD Cup challenge can use the validation and public test sets to develop and test their models, and the submitted solutions were evaluated on the private test set. Future offline users of CRAG can use the validation set for development, fine-tuning, and validation, and the public test set for testing and result reporting.

## 5 Benchmarking

In this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating that CRAG has a reasonable level of difficulty and can help draw insights and show directions in developing RAG techniques.

### Straightforward RAG solutions

**Experiment setup:** We started with running LLM-only solutions on the CRAG public test set with \(1,335\) questions, using simple prompts that encourage brief answers and _"I don't know"_ answers when the confidence is low (Appendix A.3.1). We employed Llama 2 Chat (llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and llama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3], FLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. The web-only RAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon and FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated webpage snippets using the original order from the data as the reference text, until filling up the window (similar to [17, 23, 36]). Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length KG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama 3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant query entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in Appendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the extracted entities), until filling up the window. We provide an extensive comparison of all LLMs in Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4 Turbo) in this section.

Table 5 shows the average evaluation results from the two auto-evalators (ChatGPT and Llama 3) and illustrates that the CRAG benchmark is _non-trivial_. First, the best LLM-only solutions (GPT-4 Turbo) obtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information _does_ help answer more questions reliably. Interestingly, none of the RAG solutions obtain truthfulness higher than 20%; this is because all RAG solutions introduce more hallucinations generated from irrelevant retrieval results, showing a big challenge in RAG--_How to judiciously use retrieval results without being distracted by retrieval noises?_ Third, we found that Task 2 truthfulness scores are higher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even lower hallucination rate, because the KG knowledge is typically brief but precise. Unfortunately, the improvement is mediocre, showing a second challenge in RAG--_How to best leverage the power of KG data?_ Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search ranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results) and better search recall. In particular, we found that the ground truths of over 30% of questions are available in the web retrieval results but are not included in the prompt due to the context window limitation. This shows _the importance of search ranking_ in RAG.

Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type dimension. The results reveal a lot of interesting observations and show that the CRAG benchmark allows more _insightful_ conclusions. First, it shows _which slices of the benchmark are harder_. For example, we found much lower RAG truthfulness on the _Finance_ and _Sports_ domains, for _real

\begin{table}
\begin{tabular}{l l c c c c} \hline  & **Model** & **Accuracy** & **Hallucination** & **Missing** & **Truthfulness\({}_{a}\)** \\ \hline
**LLM only** & Llama 3 70B Instruct & 32.3 & 28.9 & 38.8 & 3.4 \\  & GPT-4 Turbo & **33.5** & **13.5** & 53.0 & **20.0** \\ \hline
**Task 1** & Llama 3 70B Instruct & 35.6 & 31.1 & 33.3 & 4.5 \\  & GPT-4 Turbo & **35.9** & **28.2** & 35.9 & **7.7** \\ \hline
**Task 2** & Llama 3 70B Instruct & 37.5 & 29.2 & 33.3 & 8.3 \\  & GPT-4 Turbo & **41.3** & **25.1** & 33.6 & **16.2** \\ \hline
**Task 3** & Llama 3 70B Instruct & 40.6 & 31.6 & 27.8 & 9.1 \\  & GPT-4 Turbo & **43.6** & **30.1** & 26.3 & **13.4** \\ \hline \end{tabular}
\end{table}
Table 5: Performance of straightforward RAG solutions. All numbers are in percentage. LLM-only solutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. The subscript in “truthfulness\({}_{a}\)” denotes the result is reported by auto-eval.

Figure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain, dynamism, popularity, and question type.

time_ and _fast-changing_ facts, for _tail_ entities, and for complex questions requiring _set answers, post-processing,_ and with _false premises_. Second, it shows _where it is harder to leverage retrieval results_. Take the popularity slices as an example, we observed that GPT-4 Turbo's truthfulness dropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however, the straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso (+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). Finally, although our goal is _not_ to compare different LLMs, the different dimensions allow us to understand the strengths and weaknesses of each method. For example, although the RAG system based on Llama 3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a similar or slightly higher truthfulness in answering _simple_ and _comparison_ questions, whereas much lower truthfulness in answering _set_ and _post-processing_ questions, suggesting investigations on the reasoning capabilities.

### State-of-the-art industry solutions

Next, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. We selected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG questions, collected the responses, and applied manual grading (details in Appendix A.4).

In addition, we applied traffic weights to the questions to understand the solutions in real-world use cases. The traffic weights reflect the frequency of each question type, as defined in Table 2, in real QA traffic. We gave the same weights to all domains and reported the macro average across domains. This is because we have observed quite different domain-level distributions in different use cases, but have been observing similar distributions at the query-type level.

Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance across different dimensions. The evaluation results confirm our belief that the CRAG benchmark _reveals interesting insights and shows room for improvement for existing RAG solutions._ First, the results from SOTA solutions achieve much better truthfulness (highest \(51\%\)) compared to the straightforward solutions. However, the hallucination rate ranges from 16% to 25%, so the answers are still _not_ trustworthy. Note that the truthfulness scores between the SOTA solutions and the straightforward solutions are not completely comparable, as they have different accesses to retrieval contents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval; however, the trend is valid. Second, we observed very different latency, ranging from \(3.4\)s to \(11.6\)s, reflecting the different design options in trading off latency and quality; for example, Copilot Pro has the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier truthfulness but lowest latency. (See Appendix A.4.2 for additional results and how we measured latency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for SOTA solutions: _real-time_ and _fast-changing_ queries, and questions regarding _torso_ and _tail_ entities, showing the improvement needed for handling retrieval noises when the system relies on retrieval results to answer the question; as another example, we see lower truthfulness for queries requiring _aggregation, multi-hop reasoning_ or _post-processing_, showing the improvement space for reasoning

\begin{table}
\begin{tabular}{l l r r r r r r} \hline \hline  & **System** & **Perfect** & **Acc.** & **Hall.** & **Miss.** & **Truth\({}_{h}\)** & **Latency (ms)** \\ \hline
**Equal weighted** & Copilot Pro & **62.6** & 11.7 & 17.9 & 7.8 & **50.6** & 11,596 \\  & Gemini Advanced & 60.8 & 10.1 & 16.6 & 12.5 & 49.3 & 5,246 \\  & ChatGPT Plus & 59.8 & **13.3** & 25.0 & 1.9 & 41.5 & 6,195 \\  & Meta SG & 52.5 & 9.7 & **16.0** & 21.8 & 41.4 & **3,431** \\  & Perplexity.ai & 55.8 & 8.8 & 25.3 & 10.1 & 34.9 & 4,634 \\ \hline
**Traffic weighted** & Copilot Pro & **70.0** & 9.5 & 14.3 & 6.1 & **60.5** & - \\  & Gemini Advanced & 67.1 & 10.0 & **12.7** & 10.2 & 59.3 & - \\  & ChatGPT Plus & 61.8 & **11.4** & 25.7 & 1.3 & 41.8 & - \\  & Meta SG & 61.0 & 7.1 & 14.1 & 17.8 & 50.5 & - \\  & Perplexity.ai & 63.7 & 6.3 & 20.9 & 9.1 & 45.9 & - \\ \hline \hline \end{tabular}
\end{table}
Table 6: Benchmarking CRAG questions with industry SOTA RAG systems. Perfect, acceptable (Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulness\({}_{h}\) reported by human-eval (Truth\({}_{h}\)) are in percentages. The best system achieves truthfulness of 51% and provides perfect answers for up to 63% of questions.

in question answering. Last, truthfulness on _set_ and _false premise_ questions improved significantly in the SOTA solutions compared to the straightforward solutions, showing advancement in RAG systems in providing accurate and complete set answers and detecting _false premises_.

## 6 Conclusion

This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in retrieval-augmented generation (RAG). With detailed empirical studies, CRAG reviewed gaps in existing RAG solutions and provided valuable insights for future improvement. We plan to continue improving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-turn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to emerging challenges, and evolves for new research needs.

## Acknowledgements

We would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the partner teams. We thank Alex Boesenberg for enabling us to create web search results. We thank Sam Wexler for coordinating this project with various partners. We thank Hejia Zhang, Rares Bostan, Eryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding LLM and RAG evaluation. We thank Katie Manlove for coordinating with the budget and annotation needs. We thank our annotation team for creating many great examples: David Vu, Florian Gawllitta, Rani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte, Joshua Aramzoso. We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation tasks. We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan, Ty Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu, Jen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres, and Amy Lee.

Lei Chen's work is partially supported by National Key Research and Development Program of China Grant No. 2023YFF0725100, National Science Foundation of China (NSFC) under Grant No. U22B2060, Guangdong-Hong Kong Technology Innovation Joint Funding Scheme Project No. 2024A0505040012, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project AoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong Province Science and Technology Plan Project 2023A0505030011, Guangzhou municipality big data intelligence key lab, 2023A03J0012, Hong Kong ITC ITF grants MHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research Asia Collaborative Research Grant,

Figure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions.

HKUST-Webank joint research lab and 2023 HKUST Shenzhen-Hong Kong Collaborative Innovation Institute Green Sustainability Special Fund, from Shui On Xinitiandi and the InnoSpace GBA.

## References

* [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] AI@Meta. Llama 3 model card. 2024.
* [3] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Hesslow, J. Launay, Q. Malartic, et al. The falcon series of open language models. _arXiv preprint arXiv:2311.16867_, 2023.
* [4] P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra, T. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. MS MARCO: A human generated machine reading comprehension dataset, 2018.
* [5] Brave Software. Brave Search API.
* [6] J. Chen, H. Lin, X. Han, and L. Sun. Benchmarking large language models in retrieval-augmented generation. _arXiv preprint arXiv:2309.01431_, 2023.
* [7] W. Chen, H. Hu, X. Chen, P. Verga, and W. Cohen. MuRAG: Multimodal retrieval-augmented generator for open question answering over images and text. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, Dec. 2022.
* [8] Z. Chen, Z. Gu, L. Cao, J. Fan, S. Madden, and N. Tang. Symphony: Towards natural language query answering over multi-modal data lakes. In _CIDR_, 2023.
* [9] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. _Journal of Machine Learning Research_, 25(70):1-53, 2024.
* [10] X. L. Dong. The journey to a knowledgeable assistant with retrieval-augmented generation (rag). In _Companion of the 2024 International Conference on Management of Data_, SIGMOD/PODS '24, page 3, New York, NY, USA, 2024. Association for Computing Machinery.
* [11] M. Gao, X. Hu, J. Ruan, X. Pu, and X. Wan. Llm-based nlg evaluation: Current status and challenges. _arXiv preprint arXiv:2402.01383_, 2024.
* [12] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, and H. Wang. Retrieval-augmented generation for large language models: A survey. 2024.
* [13] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. _arXiv preprint arXiv:2311.05232_, 2023.
* [14] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. _ACM Comput. Surv._, 55(12), mar 2023.
* [15] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Association for Computational Linguistics, July 2017.
* [17] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel. Large language models struggle to learn long-tail knowledge. In _International Conference on Machine Learning_, pages 15696-15707. PMLR, 2023.
* [18] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. _Transactions of the Association of Computational Linguistics_, 2019.
* [19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. tau Yih, T. Rocktaschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

* [20] V. Lievin, C. E. Hother, A. G. Motzfeldt, and O. Winther. Can large language models reason about medical questions? _Patterns_, 5(3), 2024.
* [21] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.
* [22] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* [23] A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In _ACL_, 2023.
* [24] OpenAI. ChatGPT. https://openai.com/index/chatgpt/, 2023. Accessed: 2024-06-04.
* [25] A. Panickssery, S. R. Bowman, and S. Feng. Llm evaluators recognize and favor their own generations. _arXiv preprint arXiv:2404.13076_, 2024.
* [26] R. Pradeep, N. Thakur, S. Sharifymoghaddam, E. Zhang, R. Nguyen, D. Campos, N. Craswell, and J. Lin. Ragnar\(\backslash\)" ok: A reusable rag framework and baselines for trec 2024 retrieval-augmented generation track. _arXiv preprint arXiv:2406.16828_, 2024.
* [27] V. Rawte, S. Chakraborty, A. Pathak, A. Sarkar, S. Tonmoy, A. Chadha, A. P. Sheth, and A. Das. The troubling emergence of hallucination in large language models-an extensive definition, quantification, and prescriptive remediations. _arXiv preprint arXiv:2310.04988_, 2023.
* [28] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv_, 2023.
* [29] K. Sun, Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong. Head-to-Tail: How knowledgeable are large language models (llms)? a.k.a. will llms replace knowledge graphs? _arXiv preprint arXiv:2308.10168_, 2024.
* [30] Y. Sun, H. Xin, K. Sun, Y. E. Xu, X. Yang, X. L. Dong, N. Tang, and L. Chen. Are large language models a good replacement of taxonomies? _Proc. VLDB Endow._, 17(11):2919-2932, aug 2024.
* [31] A. Talmor and J. Berant. The web as a knowledge-base for answering complex questions, 2018.
* [32] N. Tang, C. Yang, J. Fan, L. Cao, Y. Luo, and A. Y. Halevy. Verifai: Verified generative AI. In _CIDR_, 2024.
* [33] Y. Tang and Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. _arXiv preprint arXiv:2401.15391_, 2024.
* [34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* [35] R. Usbeck, X. Yan, A. Perevalov, L. Jiang, J. Schulz, A. Kraft, C. Moller, J. Huang, J. Reineke, A.-C. Ngonga Ngomo, et al. QALD-10-the 10th challenge on question answering over linked data. _Semantic Web_, (Preprint):1-15, 2023.
* [36] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. Le, and T. Luong. FreshLLMs: Refreshing large language models with search engine augmentation, 2023.
* [37] F. Xu, Y. Song, M. Iyyer, and E. Choi. A critical evaluation of evaluations for long-form question answering. In _Association of Computational Linguistics_, 2023.
* [38] M. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec. QA-GNN: Reasoning with language models and knowledge graphs for question answering. Association for Computational Linguistics, 2021.
* [39] Y. Zhu, S. Du, B. Li, Y. Luo, and N. Tang. Are large language models good statisticians? In _NeurIPS_, 2024.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Section 3. 2. Did you describe the limitations of your work? [Yes] See Appendix A.5. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The dataset, KGs, mock APIs, and starter kits for building baseline systems and performing auto-eval are available at https://github.com/facebookresearch/CRAG/. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4.2, Appendix A.2.3, and Appendix A.4.1 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] Evaluating the performance of the same LLM on different platforms is out of the scope of this work.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Section 5.1. 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We only use publicly available and shareable data.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Appendix

### Dataset

#### a.1.1 Constructing QA pairs from KGs

We first collected a set of entities based on publicly available data. Then we created question-answer pairs in three steps for _Simple static and dynamic questions_.

_Step 1._ For each domain, we first selected an entity type and a meaningful relation \((e,r)\) and created a question template. For example, for _(music artist, first album)_, we create a template "_what is the first album of [music artist]?_".

_Step 2._ We then sampled entities from the KGs to fill in the templates and generate the full question. We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. We defined popularity based on heuristics for each entity type and created an equal number of questions for each bucket.

_Step 3._ Last, we took the associated attribute values as the answer to the question to create question-answer pairs.

We created the _Comparison, Aggregation, Set, Post-processing_, and _False-premise questions_ in a similar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled the subject entities that fit the question. We used heuristics to select entity types for these question categories.

Finally, we created multi-hop questions in three steps, similar to those described in [31]. We first sampled an entity \(e_{1}\) from the KG and selected two relation triplets following a two-hop path: \((e_{1},r_{1},e_{2})\) and \((e_{1},r_{2},e_{3})\). We then created a question template describing the path. For example, for path _(company\({}_{1}\), isparent, company\({}_{2}\))_ followed by _(company\({}_{1}\), ceo, person)_, we created the template "_who is the CEO of the parent company of [company\({}_{2}\)]?_". The answer to the new question will be \(e_{3}\) in the second triplet.

#### a.1.2 Definition of dynamism categories

#### a.1.3 Constructing QA pairs from web contents

_Step 1._ Ask annotators to write down a list of questions that could possibly be answered by web search based on a general guideline (e.g., "_what is the most popular action movie in 2023?_").

_Step 2._ Generate the web search results to answer the question.

_Step 3._ Finally, annotators reviewed the web search results to determine the ground truth answers to the questions: 1) If the search results successfully provided the necessary information, annotators recorded the ground truth answer text and the URL associated with it based on the retrieved content. Note that the answer is determined by the _query_time_ at which the web search happened, especially for the _Fast-changing_ and _Real-time_ questions. 2) Otherwise, annotators conducted further web searches to document the correct answers.

\begin{table}
\begin{tabular}{p{85.4pt} p{284.5pt}} \hline \hline
**Dynamism** & **Definition** \\ \hline Real-time & The answer to the question changes over seconds (e.g., “_What’s Costco’s stock price today?_”). \\ \hline Fast-changing & The answer to the question changes no more than daily (e.g., “_When is Laker’s game tonight?_”). \\ \hline Slow-changing & The answer to the question changes no more than yearly (e.g., “_Who won the Grammy award last year?_”). \\ \hline Static & The answer to the question does not change over time, such as the birth date of a person. \\ \hline \hline \end{tabular}
\end{table}
Table 7: Definition of dynamism categories.

Besides the QA pairs, the annotators will also provide labels for the domain, dynamism, question types, and an _answer URL_ (a URL that contains the answer to the question) for _Web Questions_.

#### a.1.4 Validation for the QA pairs

We conducted two phases of dataset validation with our in-house Linguist team.

**Phase 1. Question and meta-label validation.** After the initial round of QA pair generation, an audit session was conducted, where expert annotators reviewed the question template, questions, and meta-labels (domain, question type, etc), applying edits as necessary with 2x human review (agreement rate \(90\%+\)). All problematic questions (e.g., a wrong false-premise question) were revised, and all conflicting labels were resolved by a third more experienced auditor.

**Phase 2. Answer validation.** To ensure the answers in the benchmark are correct, we further conducted an auditing process for all the answers. For the web questions, an annotation team reviewed each question and conducted an extensive search to make sure the answer is factually correct and includes comprehensive information (such as for set questions) with 2x human review (agreement rate \(90\%+\)). A third more experienced auditor then reviewed all conflicting answers and provided a resolution. For the KG questions, a team of five engineers carefully checked the questions and queried the mock APIs manually to validate the answers. This step resulted in a \(5\%\) answer correction.

In both phases, we paid special attention to examples where the straightforward solutions output different answers from the ground truth answers and asked the auditing team to double-check those examples. This step yielded an additional \(2\%\) answer updates.

#### a.1.5 An example of retrieved web search results

#### a.1.6 The mock data and mock APIs

CRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the _real_ retrieval environment, allowing _accessible_ facilitating data and fair comparison.

CRAG provides both structured (through mock KGs) and unstructured (web search results) information to test the effectiveness of RAG systems in leveraging a diverse range of available information. First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-world search engine--the Brave Search API [5]. Different from existing benchmarks that use snippets or selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and potentially more noises as in a realistic setting. Second, CRAG provides mock KG search APIs to test structured search for RAG. The mock KGs, though much smaller in size, contain both information necessary to answer a subset of questions in the benchmark and noises that have similar entity or attribute names, again simulating real settings. Our mock KGs contain about 2.6M entities and have a signal-to-noise ratio of less than 1/30.

\begin{table}
\begin{tabular}{l l} \hline \hline Key & Value \\ \hline “page name” & “A Short History Of ChatGPT: How We Got To Where We Are Today” \\ “page url” & “https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how...” \\ “page snippet” & “OpenAI released an early demo of ChatGPT on \textless{}strong\textgreater{}November 30, 2022\textless{}/strong\textgreater{}...” \\ “page last modified” & “2024-1-18 15:32:24” \\ “html page” & “\textless{}lDOCTYPE html\textgreater{}chtml lang=”en”\textgreater{}head\textgreater{}link rel=”preload” as=”font” \\  & href=”https...” \\ \hline \hline \end{tabular}
\end{table}
Table 8: An example of web search results.

### Evaluation

#### a.2.1 Human evaluation

We run human evaluation to score each answer with respect to the metrics defined in Section 4.1. We score each _perfect_, _acceptable_, _missing_, or _incorrect_ answer with a score \(s_{p}\), \(s_{a}\), \(s_{m}\), and \(s_{in}\), respectively and define **truthfulness\({}_{h}\)** as the score of the answer by setting \(s_{p}=1\), \(s_{a}=0.5\), \(s_{m}=0\), and \(s_{in}=-1\). We then compute the average truthfulness for all examples in the evaluation set as the truthfulness score for the RAG solution.

**Human-eval instructions.** The human-eval instructions are as follows.

Given a query, a query day and time at which the query was made, the chatbot's response, grade the _accuracy_ for the response according to the criteria below:

Using an external search engine, please evaluate the factual accuracy of the response based on the grading rubric below. An accurate answer should be factually correct and provide useful information to answer the user's question.

* Accuracy = 0 (Missing). It covers the following situations. There is no response. There is a failure to provide a response to the request (e.g. "I'm sorry......., I can't do...") due to inability. E.g., Query: "Latest news about the Nobel prize today." Response: "I can't find specific information regarding the Nobel prize..." The response fails to answer and asks follow-up questions.
* Accuracy = 1 (Incorrect). It covers the following situations. The answer is unintelligible. The answer is poorly formed. The answer contains a major hallucination (e.g. wrong date,

\begin{table}
\begin{tabular}{l l} \hline \hline
**APIs** & **Descriptions** \\ \hline open\_search\_entity\_by\_name(query: str) \(>\) dict & Search for entities by name in the Open domain. \\ open\_get\_entity(entity: str) \(>\) dict & Retrieve detailed information about an entity in the \\  & Open domain. \\ \hline movie\_get\_person\_info(person\_name: str) \(>\) dict & Get information about a person related to movies. \\ movie\_get\_probe\_info(move\_name: str) \(>\) dict & Get information about a movie. \\ movie\_get\_year\_info(year: str) \(>\) dict & Get information about movies released in a specific \\  & year. \\ movie\_get\_move\_info\_by\_id(move\_id: int) \(>\) dict & Get movie information by its unique ID. \\ movie\_get\_person\_info\_by\_id(person\_id: int) \(>\) dict & Get person information by their unique ID. \\ \hline finance\_get\_company\_name(query: str) \(>\) dict & Search for company names in the finance domain. \\ finance\_get\_ticker\_by\_name(query: str) \(>\) dict & Retrieve the ticker symbol for a given company name. \\ finance\_get\_price\_history(ticker\_name: str) \(>\) dict & Get the price history for a given ticker symbol. \\ finance\_get\_detailed\_price\_history(ticker\_name: str) \(>\) dict & Get detailedity the history for a ticker symbol. \\ finance\_get\_dividends\_history(ticker\_name: str) \(>\) dict & Get divrided history for a ticker symbol. \\ finance\_get\_market\_epidiation(ticker\_name: str) \(>\) dict & Retrieve market capitalization for a ticker symbol. \\ finance\_get\_est(ticker\_name: str) \(>\) dict & Get earnings parker (EPS) for a ticker symbol. \\ finance\_get\_pe\_ratio(ticker\_name: str) \(>\) dict & Get the price-to-earnings (PE) ratio for a ticker symbol. \\ finance\_get\_info(ticker\_name: str) \(>\) dict & Get financial information for a ticker symbol. \\ \hline music\_search\_artist\_entity\_by\_name(artist\_name: str) \(>\) dict & Search for music artists by name. \\ music\_search\_entity\_by\_name(cong\_name: str) \(>\) dict & Search for songs by name. \\ music\_get\_ballboard\_rank\_date(rank: int, date: str = None) \(>\) dict & Get Billboard ranking for a specific rank and date. \\ music\_get\_bellboard\_attributes(date\_str\), attribute: str, stop\_name\_star\(\rangle\) \(>\) dict & Get attributes of a song from Billboard rankings. \\ music\_get\_path\_get\_start\_by\_year\_writer\(;int\) \(>\) dict & Get the Grammy Best New artist for a specific year. \\ music\_grammy\_get\_awad\_count\_by\_arts(artist\_name: str) \(>\) dict & Get the total Grammy awards won by a artist. \\ music\_grammy\_get\_awad\_count\_by\_pass(song\_name: str) \(>\) dict & Get the total Grammy awards won by a song. \\ music\_grammy\_get\_wext\_on\_by\_year(var\_init:) \(>\) dict & Get the Grammy Song of the Year for a specific year. \\ music\_grammy\_get\_awad\_date\_by\_artist(artist\_name: str) \(>\) dict & Get the years an artist won a Grammy award. \\ music\_grammy\_get\_wext\_album\_by\_year(var\_init:) \(>\) dict & Get the Grammy Album of the Year for a specific year. \\ music\_grammy\_get\_all\_awad\_att\_words(\langle\cdot\rangle\) \(>\) dict & Get all artists awarded the Grammy Best New artist. \\ music\_get\_artist\_birth\_place(artist\_name: str) \(>\) dict & Get the birthplace of an artist. \\ music\_get\_att\_with\_place(artist\_name: str) \(>\) dict & Get the birth date of an artist. \\ music\_get\_att\_with\_place(artist\_name: str) \(>\) dict & Get the member list of a hand. \\ music\_get\_memory(band\_name: str) \(>\) dict & Get the lifespan of an artist. \\ music\_get\_lifespan\(artist\_name: str) \(>\) dict & Get the author of a song. \\ music\_get\_song\_author(long\_time: str) \(>\) dict & Get the reflexive country of a song. \\ music\_get\_song\_release\_country(song\_name: str) \(>\) dict & Get the release date of a song. \\ music\_get\_song\_release\_date(song\_name: str) \(>\) dict & Get all works by an artist. \\ music\_get\_artist\_all\_words(artist\_name: str) \(>\) dict & Get stocker games on a specific date. \\ \hline sports\_soccer\_get\_games\_on\_date(team\_name: str, date: str) \(>\) dict & Get soccer games on a specific date. \\ sports\_ab\_get\_games\_on\_date(team\_name: str, date: str) \(>\) dict & Get NBA games on a specific date. \\ sports\_ha\_get\_play\_by\_play\_data\_by\_game\_ids(game\_ids:List\(wrong numbers, or other significant factual errors). The answer is irrelevant to the user's request. Answers in a category, location, or time window that is significantly different from the user's request, if any. There is a significant structural/formatting error. The response is otherwise a total structural/functional failure and does not contain sufficient well-formed content that can be used to determine accuracy
* Accuracy = 2 (Acceptable). It covers the following situations. The answer is acceptably correct and relevant to the user's request, but may miss some information, i.e. accurate but not complete, or mostly accurate with minor issues. The answer may contain some minor hallucination that doesn't significantly alter the overall meaning. "Minor hallucination" means the answer addressed the user's question but might be off on some additional details. The rule of thumb is to see if the answer serves the purpose of the user's question, and whether the hallucination could mislead the users on what they were asking.
* Accuracy = 3 (Accurate). The answer is factually correct, contains all the relevant information requested, responds appropriately to the query, and does not contain any hallucination.

We requested two human graders to grade each question (agreement rate \(94\%\)), and when there is a conflict, a third more experienced grader will resolve it.

#### a.2.2 Automatic evaluation

In auto-eval, we merge _perfect_ and _acceptable_ as _accurate_ and consider only three scores: 1 for _accurate_, 0 for _missing_, and -1 for _incorrect_. The \(\textbf{truthfulness}_{a}\) is calculated by the average truthfulness for all the examples in the evaluation set, and is effectively

\[\text{accuracy}-\text{hallucination},\]

where **accuracy**, **hallucination**, and **missing** are the percentage of _accurate_, _incorrect_, and _missing_ answers in the test set. These score choices penalize _incorrect_ answers, award _correct_, and assign a value of 0 to _missing_ answers.

**Auto-evaluators.** We computed the accuracy and F1 for the two auto-evaluators for the _accurate_, _incorrect_, and _missing_ examples in the public test set, respectively. Here, we considered human-eval labels as the ground truth. Table 10 shows both models attain reasonable accuracy and F1 scores as an evaluator compared to human evaluation.

**Auto-eval prompt.** The prompt we used in the auto-eval is similar to the following. We did not release the exact prompt used in the challenge to avoid prompt attack.

PROMPT = ""# Task: You are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement.

1. If the model prediction matches any provided answers from the Ground Truth Answer list, "Accuracy" should be "True"; otherwise, "Accuracy" should be "False".

2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, "Accuracy" should always be "False".

3. If the Ground Truth is "invalid question", "Accuracy" is "True" only if the model prediction is exactly "invalid question".

# Output:

Respond with only a single JSON string with an "Accuracy" field which is "True" or "False".

# Examples:

Question: how many seconds is 3 minutes 15 seconds?

Ground truth: ["195 seconds"]

\begin{table}
\begin{tabular}{l r r r r r r r} \hline \hline  & \multicolumn{2}{c}{**Accuracy**} & \multicolumn{2}{c}{**Precision**} & \multicolumn{2}{c}{**Recall**} & \multicolumn{2}{c}{**F1 score**} \\ \hline  & ChaGPT & Llama 3 & ChaGPT & Llama 3 & ChaGPT & Llama 3 & ChaGPT & Llama 3 \\ \hline
**Accurate** & 94.1 & **98.6** & **98.8** & 98.5 & 92.2 & **99.3** & 92.0 & **98.9** \\
**Incorrect** & 94.1 & **98.6** & 86.8 & **98.7** & **97.8** & 97.2 & 92.0 & **97.9** \\
**Missing** & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 \\ \hline
**Average** & 96.1 & **99.1** & 95.2 & **99.1** & 96.7 & **98.8** & 94.7 & **98.9** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models.

Prediction: 3 minutes 15 seconds is 195 seconds.

Accuracy: True

Question: Who authored The Taming of the Shrew (published in 2002)?

Ground truth: ["William Shakespeare", "Roma Gill"]

Prediction: The author to The Taming of the Shrew is Roma Shakespeare.

Accuracy: False

Question: Who played Sheldon in Big Bang Theory?

Ground truth: ["Jim Parsons", "Iain Armitage"]

Prediction: I am sorry I don't know.

Accuracy: False

#### a.2.3 KDD Cup 2024 Meta CRAG challenge

The KDD Cup 2024 Meta CRAG challenge has two stages. Stage 1 is designed for the participants to develop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2 determines the final winners. We split our benchmark data into three sets with similar distributions: _validation, public test_, and _private test_ at 30%, 30%, and 40%, respectively. We shared the validation and public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the final winners in Stage 2. We used auto-eval for Stage 1, and selected top teams with auto-eval in Stage 2 to conduct manual evaluation.

### Evaluating straightforward RAG solutions

We send each CRAG question in a prompt shown below. This prompt is designed to include the original _Query Time_ for the question and ask the LLM to answer the question **based on the query time and the retrieved result**. Note that the retrieved result was also from the same _query time_ and was provided in CRAG. Moreover, this prompt encourages brief answers and _"I don't know"_ answers when confidence is low.

#### a.3.1 Prompts used in straightforward RAG solutions

**Vanilla LLM.** PROMPT = """ You are given a Question and the time when it was asked in the Pacific Time Zone (PT), referred to as "Query Time". The query time is formatted as "mm/dd/yyyy, hh:mm:ss PT". Your task is to answer the question in as few words as possible.

Please follow these guidelines when formulating your answer:

1. If the question contains a false premise or assumption, answer "invalid question".

2. If you are uncertain or don't know the answer, respond with "I don't know".

### Question

{query}

### Query Time

{query_time}

### Answer

a.3.2 RAG with web search results (Task 1).** PROMPT = """ You are given a Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as "Query Time". The query time is formatted as "mm/dd/yyyy, hh:mm:ss PT". The references may or may not help answer the question. Your task is to answer the question in as few words as possible.

Please follow these guidelines when formulating your answer:

1. If the question contains a false premise or assumption, answer "invalid question".

2. If you are uncertain or don't know the answer, respond with "I don't know".

### Question

{query}

### Query Time

{query_time}

### References

{references}

**RAG with KG and web search results (Tasks 2 and 3).** PROMPT = """ You are given a Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as "Query Time". The query time is formatted as "mm/dd/yyyy, hh:mm:ss PT". The references may or may not help answer the question. Your task is to answer the question in as few words as possible.

Please follow these guidelines when formulating your answer:

1. If the question contains a false premise or assumption, answer "invalid question".

2. If you are uncertain or don't know the answer, respond with "I don't know".

### Question

{query}

### Query Time

{query_time}

### References

# web

{web_results}

# knowledge graph

{kg_response}

### Answer

**Query entity extraction.** PROMPT = """ You are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:

1) Determine the domain the query is about. The domain should be one of the following: "finance", "sports", "music", "movie", "encyclopedia". If none of the domains apply, use "other". Use "domain" as the key in the result json.

2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:

For 'encyclopedia' and 'other' queries, these are possible keys:

-'main_entity': extract the main entity of the query.

For 'finance' queries, these are possible keys:

-'market_identifier': stock identifiers including individual company names, stock symbols.

-'metric': financial metrics that the query is asking about. This must be one of the following: 'price', 'dividend', 'P/E ratio', 'EPS','marketCap', and 'other'.

- 'datetime': time frame that the query asks about. When datetime is not explicitly mentioned, use 'Query Time' as default.

For'movie' queries, these are possible keys:

-'movie_name': name of the movie

-'movie_aspect': if the query is about a movie, which movie aspect the query asks. This must be one of the following: 'budget', 'genres', 'original_language', 'original_title','release_date','revenue', 'title', 'cast', 'crew', 'rating', 'length'.

- 'person': person name related to moves

- 'person_aspect': if the query is about a person, which person aspect the query asks. This must be one of the following: 'acted_movies', 'directed_movies', 'oscar_awards', 'birthday'.

- 'year': if the query is about movies released in a specific year, extract the year

For'music' queries, these are possible keys:

- 'artist_name': name of the artist

- 'artist_aspect': if the query is about an artist, extract the aspect of the artist. This must be one of the following:'member', 'birth place', 'birth date', 'lifespan', 'artist work', 'grammy award count', 'grammy award date'.

-'song_name': name of the song

-'song_aspect': if the query is about a song, extract the aspect of the song. This must be one of thefollowing: 'author', 'grammy award count','release country','release date'.

For'sports' queries, these are possible keys:

-'sport_type': one of 'basketball','soccer', 'other'

- 'tournament': NBA, World Cup, Olympic.

- 'team': teams that users are interested in.

- 'datetime': time frame that the user is interested in. When datetime is not explicitly mentioned, use 'Query Time' as default.

Return the results in a FLAT json.

*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!!*

#### a.3.2 Performance of straightforward RAG solutions

Table 11 summarizes the results of straightforward RAG solutions.

### Evaluating state-of-the-art industry solutions

#### a.4.1 Quality

We send the CRAG public test set question as input to each of the SOTA RAG systems and collect the responses for human grading. Note that the original _Query Time_ and the provided retrieval results in CRAG are **not** used in this setting. We simply test the questions and ask human graders to grade the responses based on when the query was made to the SOTA system. We called Copilot Pro, Gemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as Automatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled by default in other systems. To ensure a fair comparison, we excluded these on-device components,

\begin{table}
\begin{tabular}{c l r r r r} \hline \hline  & **Model** & **Accuracy (\%)** & **Hallucination (\%)** & **Missing (\%)** & **Truthfulness (\%)** \\ \hline
**LLM only** & Lilama 2 7B Chat & 14.8 & 78.4 & **6.7** & -63.6 \\  & Lilama 2 70B Chat & 22.3 & 28.7 & 49.0 & -6.4 \\  & Lilama 3 8B Instruct & 23.7 & 33.8 & 42.6 & -10.1 \\  & Lilama 3 70B Instruct & 32.3 & 28.9 & 38.8 & 3.4 \\  & Falcon 40B & 10.8 & 41.9 & 47.3 & -31.1 \\  & FLAN-T5-XXL 11B & 9.4 & 8.7 & 81.9 & 0.7 \\  & Miriat-8x7B-Instruct-v0.1 & 20.8 & 27.0 & 52.1 & -6.2 \\  & GPT-T Auto & **33.5** & **13.5** & 53.0 & **20.0** \\ \hline
**Task 1** & Lilama 2 7B Chat & 16.4 & 83.1 & **0.5** & -66.7 \\  & Lilama 2 70B Chat & 29.3 & 61.0 & 9.7 & -31.7 \\  & Lilama 3 8B Instruct & 28.5 & 45.6 & 25.9 & -17.1 \\  & Lilama 3 70B Instruct & 35.6 & 31.1 & 33.3 & 4.5 \\  & Falcon 40B & 21.9 & 55.5 & 22.5 & -33.6 \\  & FLAN-T5-XXL 11B & 27.5 & 36.5 & 36.0 & -9.0 \\  & Miriat-8x7B-Instruct-v0.1 & 33.6 & 44.4 & 22.0 & -10.8 \\  & GPT-4 Turbo & **35.9** & **28.2** & 35.9 & **7.7** \\ \hline
**Task 2** & Lilama 2 7B Chat & 16.4 & 83.1 & **0.5** & -66.7 \\  & Lilama 2 70B Chat & 29.1 & 61.1 & 9.7 & -32.0 \\  & Lilama 3 8B Instruct & 28.6 & 45.5 & 25.9 & -16.9 \\  & Lilama 3 70B Instruct & 37.5 & 29.2 & 33.3 & 8.3 \\  & Falcon 40B & 21.9 & 55.4 & 22.7 & -33.5 \\  & FLAN-T5-XXL 11B & 27.4 & 36.6 & 36.0 & -9.2 \\  & Mixral-8x7B-Instruct-v0.1 & 33.3 & 44.6 & 22.0 & -11.2 \\  & GPT-4 Turbo & **41.3** & **25.1** & 33.6 & **16.2** \\ \hline
**Task 3** & Lilama 2 7B Chat & 16.0 & 83.6 & **0.4** & -67.6 \\  & Lilama 2 70B Chat & 31.9 & 65.7 & 2.4 & -33.7 \\  & Lilama 3 8B Instruct & 32.1 & 56.3 & 11.6 & -24.1 \\  & Lilama 3 70B Instruct & 40.6 & 31.6 & 27.8 & 9.1 \\  & Falcon 40B & 22.0 & 56.6 & 21.3 & -34.6 \\  & FLAN-T5-XXL 11B & 27.8 & 37.1 & 35.1 & -9.3 \\  & Mixral-8x7B-Instruct-v0.1 & 33.5 & 44.1 & 22.4 & -10.6 \\  & GPT-4 Turbo & **43.6** & **30.1** & 26.3 & **13.4** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance of straightforward RAG solutions on CRAG.

ensuring that answer quality was not affected. We called each system on the following dates in Pacific Time: 05/12/2024\(\sim\)05/16/2024 (Copilot Pro), 05/20/2024\(\sim\)05/28/2024 (Gemini Advanced), 05/27/2024\(\sim\)06/02/2024 (ChatGPT Plus), 05/15/2024\(\sim\)05/16/2024 (Perplexity.ai), and 07/02/2024 (Meta SG). We set the conversation style to "Precise" when calling Copilot Pro and the temperature to \(0\) when calling Perplexity.ai. We select GPT-4o and llama-3-sonar-large-32k-online as the base LLM when calling ChatGPT Plus and Perplexity.ai, respectively.

#### a.4.2 Latency

We quantified the latency by calculating the time difference between the timestamp of the query submission to the system and the timestamp when the complete response was received.

The latency of Perplexity.ai measured via API call is 2,455ms. Since latency measured by API call and web interface interactions are not directly comparable, we further called Perplexity.ai through its web interface and reported the latency under this setting in Table 6. Note that this latency may not correspond to the accuracy numbers from the API calls. For Meta SG, we estimated a latency comparable to other web interface interactions by excluding on-device components such as ASR and TTS from the overall end-to-end latency measurement.

### Limitations

The three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval candidate pool, a demanding retrieval task in its own right. This design decision ensures that the competition remains both challenging and achievable within the KDD Cup's required three-month timeframe. Despite the limitation, users of our dataset have the option to use the union of all 220K web pages as a corpus to build a retriever. While this corpus does not match the entire web, it allows for fair comparisons and manageable costs.