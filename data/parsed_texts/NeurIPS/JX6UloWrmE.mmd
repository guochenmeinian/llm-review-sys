# Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition

 Suyoung Lee, Myungsik Cho, Youngchul Sung

School of Electrical Engineering

KAIST

Daejeon 34141, Republic of Korea

{suyoung.1, ms.cho, ycsung}@kaist.ac.kr

Corresponding author

###### Abstract

Meta-reinforcement learning (meta-RL) techniques have demonstrated remarkable success in generalizing deep reinforcement learning across a range of tasks. Nevertheless, these methods often struggle to generalize beyond tasks with parametric variations. To overcome this challenge, we propose Subtask Decomposition and Virtual Training (SDVT), a novel meta-RL approach that decomposes each non-parametric task into a collection of elementary subtasks and parameterizes the task based on its decomposition. We employ a Gaussian mixture VAE to meta-learn the decomposition process, enabling the agent to reuse policies acquired from common subtasks. Additionally, we propose a virtual training procedure, specifically designed for non-parametric task variability, which generates hypothetical subtask compositions, thereby enhancing generalization to previously unseen subtask compositions. Our method significantly improves performance on the Meta-World ML-10 and ML-45 benchmarks, surpassing current state-of-the-art techniques.

## 1 Introduction

Meta-reinforcement learning (meta-RL) constitutes a dynamic field within deep reinforcement learning, focusing on training agents to quickly adapt to novel tasks by learning from a variety of training tasks [2]. By interacting with these tasks, meta-RL creates an inductive bias regarding the task dynamics and subsequently develops a policy based on this knowledge. Despite its significant contribution to the generalization capability of traditional deep RL, meta-RL is susceptible to test-time distribution shifts, which restricts its applicability to familiar in-distribution test tasks [17, 36, 39, 41].

To tackle this limitation, recent out-of-distribution (OOD) meta-RL approaches have emphasized distinct training and test task distributions, thereby achieving enhanced performance on unseen OOD test tasks with interpolated or slightly extrapolated training dynamics [12, 36, 41, 35, 1]. Although the parameters of training and test tasks are drawn from disjoint distributions, these tasks remain qualitatively similar, as they can be expressed in a shared parametric form representing the task dynamics (e.g., the same "Pick-place" task with OOD goal positions in Figure 0(b)).

In this study, we explore a more general meta-RL framework that addresses non-parametric task variability [69, 71], a topic that has received limited attention in prior research. In this context as of Figure 0(c), variations among tasks cannot be expressed through simple parametric variations, such as the parameterization of a goal position. Generalization is particularly challenging in this setting, as conventional meta-RL methods often model the inductive bias as a parametric embedding applicable to various tasks [9, 46, 74]. Within a non-parametric framework, it may not be feasible to employ a unified and generalizable parameterization of training tasks using standard meta-RL techniques.

Moreover, even if an agent successfully models the inductive bias parametrically, it is improbable that the same parameterization will be reusable for qualitatively distinct test tasks.

In addressing the challenges of non-parametric task variability in meta-RL, our primary strategy involves _decomposing each non-parametric task into a set of shared elementary subtasks_. We then parameterize each task based on the types of subtasks that constitute it. Despite the non-parametric task variability, tasks may share elementary subtasks. For instance in Figure 0(c), a "Pick-place" task can be decomposed into subtasks: "grip object" and "place object," while a "Sweep-into" task can be decomposed into subtasks: "grip object" and "push object." By employing the shared subtask parameterization, the policy can capitalize on the captured commonalities between non-parametric tasks to enhance training efficiency and generalization capabilities.

However, our approach of task parameterization based on a set of subtasks faces two primary challenges: the lack of prior information about **(1)** the set of elementary subtasks and **(2)** the decomposition of each task. To address these issues, we employ meta-learning for the subtask decomposition (SD) process using a Gaussian mixture variational autoencoder (GMVAE) [29; 8; 59]. Our GMVAE encodes the trajectory up to the current timestep into latent categorical and Gaussian contexts, which are trained to reconstruct the task's reward and transition dynamics [74]. We discover that the meta-learned latent categorical context effectively represents the subtask compositions of tasks under non-parametric variations. Consequently, the policy, using the learned subtask composition, can readily generalize to new tasks comprising previously encountered subtasks. To further enhance generalization to unseen compositions of familiar subtasks, we propose a virtual training (VT) process [35; 1] specifically designed for non-parametric task variability. We train the policy on imaginary tasks generated by the learned dynamics decoder, conditioned on hypothetical subtask compositions.

We evaluate our method on the Meta-World ML-10 and ML-45 benchmarks [71], widely used meta-RL benchmarks comprising diverse non-parametric robotic manipulation tasks. We empirically demonstrate that our method successfully meta-learns the shareable subtask decomposition. With the help of the subtask decomposition and virtual training, our method, without any offline demonstration or test-time gradient updates, achieves test success rates of 33.4% on ML-10 and 31.2% on ML-45, which improves the previous state-of-the-art by approximately 1.7 times and 1.3 times, respectively.

## 2 Background

### Meta-Reinforcement Learning

A Markov decision process (MDP), \(M=(\mathcal{S},\mathcal{A},R,T,T_{0},\gamma,H)\), is defined by a tuple comprising a set of states \(\mathcal{S}\), a set of actions \(\mathcal{A}\), a reward function \(R(r_{t+1}|s_{t},a_{t},s_{t+1})\), a transition function \(T(s_{t+1}|s_{t},a_{t})\), an initial state distribution \(T_{0}(s_{0})\), a discount factor \(\gamma\), and a horizon \(H\).

The goal of meta-RL is to learn to adapt to a distribution of MDPs with varying reward and transition dynamics. At the start of each meta-training iteration, an MDP is sampled from the distribution \(p(\mathcal{M}_{\text{train}})\) over a set of MDPs \(\mathcal{M}_{\text{train}}\). Each MDP \(M_{k}=(\mathcal{S},\mathcal{A},R_{k},T_{k},T_{0,k},\gamma,H)\) is defined with a unique reward function \(R_{k}\), transition function \(T_{k}\), and initial state distribution \(T_{0,k}\). Unlike in a multi-task setup, the agent in the meta-RL setup does not have access to the task index \(k\) that determines the MDP dynamics. The training objective is to optimize the policy \(\pi_{\psi}\) with

Figure 1: **Problem Setup. Visualizing different meta-RL scenarios with Meta-World tasks [69; 71]. The circles and crosses represent the object and goal positions, respectively. Solid blue objects indicate training tasks, while empty brown objects indicate test tasks.**

parameters \(\psi\) to maximize the expected return across all MDPs: \(\max_{\psi}\mathbb{E}_{M_{k}\sim p(\mathcal{M}_{\text{train}})}\left[\mathcal{J}_ {\text{pol}}(\psi)\right]\), where \(\mathcal{J}_{\text{pol}}(\psi)=\mathbb{E}_{T_{0,k},T_{k},\pi_{\psi}}\left[\sum_{ t=0}^{H-1}\gamma^{t}R_{k}(r_{t+1}|s_{t},a_{t},s_{t+1})\right].\) During meta-testing, standard distribution meta-RL methods are evaluated on tasks sampled from the same distribution \(p(\mathcal{M}_{\text{test}})\) as the training tasks, i.e., \(\mathcal{M}_{\text{train}}=\mathcal{M}_{\text{test}}=\mathcal{M}\) (Figure 0(a)). In contrast, OOD meta-RL methods assume strictly disjoint training and test task sets, i.e., \(\mathcal{M}=\mathcal{M}_{\text{train}}\cup\mathcal{M}_{\text{test}}\) and \(\mathcal{M}_{\text{train}}\cap\mathcal{M}_{\text{test}}=\emptyset\) (Figure 0(b)).

### Bayes-adaptive Meta-Reinforcement Learning

Since the true task index \(k\) is not provided to the agent in the meta-RL problem setup, it is important to balance exploration and exploitation while learning about the initially unknown MDP. A Bayes-adaptive agent [38; 10; 16] achieves this balance by updating its belief \(b_{t}(R,T)\) about the MDP based on its experience \(\tau_{t}=\{s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},\ldots,s_{t-1},a_{t-1},r_{t},s _{t}\}\). The agent's belief over the MDP dynamics at time \(t\) can be represented as a posterior given the trajectory, i.e., \(b_{t}(R,T)=p(R,T|\tau_{t})\). By augmenting the state with the belief to form a hyper-state space \(\mathcal{S}^{+}=\mathcal{S}\times\mathcal{B}\), where \(\mathcal{B}\) is the set of belief, a Bayes-adaptive MDP (BAMDP) can be constructed. The objective of a BAMDP is to maximize the expected return within a meta-episode while learning, where a meta-episode consists of \(n_{\text{roll}}\) rollout episodes (i.e., \(H^{+}=n_{\text{roll}}\times H\) steps) of the same MDP:

\[\mathcal{J}_{\text{pol}}^{+}(\psi)=\mathbb{E}_{b_{0},T^{+},\pi_{\psi}}\left[ \sum_{t=0}^{H^{+}-1}\gamma^{t}R^{+}(r_{t+1}|s_{t}^{+},a_{t},s_{t+1}^{+}) \right],\] (1)

where \(T^{+}(s_{t+1}^{+}|s_{t}^{+},a_{t},r_{t})=\mathbb{E}_{b_{t}}\left[T(s_{t+1},s_ {t},a_{t})\right]\mathbb{I}(b_{t+1}=p(R,T|\tau_{t+1}))\) is the transition dynamics and \(R^{+}(r_{t+1}|s_{t}^{+},a_{t},s_{t+1}^{+})=\mathbb{E}_{b_{t+1}}\left[R(r_{t+1} |s_{t},a_{t},s_{t+1})\right]\) is the reward dynamics of the BAMDP. The posterior belief update in the indicator function \(\mathbb{I}(\cdot)\) is intractable for all but simple environments.

VariBAD [74] solves the inference and posterior update of the belief by combining meta-RL and approximate variational inference [28]. At each timestep \(t\), a recurrent encoder \(q_{\phi_{h}}\) encodes the experience \(\tau_{\cdot t}\) into a hidden embedding \(h_{t}=q_{\phi_{h}}(\tau_{\cdot t})\). The approximate posterior belief over the dynamics can be represented as the parameters of a multivariate Gaussian distribution: \(b_{t}=(\mu_{\phi_{\phi}}(h_{t}),\sigma_{\phi_{h}}(h_{t}))\), where \(\mu_{\phi_{\phi}}(\cdot)\) and \(\sigma_{\phi_{h}}(\cdot)\) are neural networks. The latent context \(z_{t}\sim\mathcal{N}(\mu_{\phi_{\phi}}(h_{t}),\sigma_{\phi_{h}}^{2}(h_{t}))\) is used to estimate the MDP dynamics: \(p_{\theta_{R}}(r_{j+1}|s_{j},a_{j},s_{j+1},z_{t})\) and \(p_{\theta_{T}}(s_{j+1}|s_{j},a_{j},z_{t})\) for \(j=0,\ldots,H^{+}-1\), _including the past and future_. Then the problem of computing the posterior over the dynamics \(p(R,T|\tau_{\cdot t})\) reduces to inferring the posterior \(q_{\phi}(z_{t}|\tau_{\cdot t})\), where \(\phi=\{\phi_{h},\phi_{z}\}\). A separate policy network \(\pi_{\psi}(a_{t}|s_{t},b_{t})\) is trained to optimize the BAMDP objective in Eq. (1).

### Non-parametric Task Variability

The term "non-parametric" in the context of task variability is introduced in the Meta-World paper [69]. It is used to distinguish the task variability in Meta-World manipulation tasks from more simplistic parametric variations exhibited in standard MuJoCo tasks, such as variations in target goal positions, directions, and velocities. However, the term "non-parametric" might lead to misunderstanding. Because it could suggest that there are no parameters available for task parameterization, even though we have effectively parameterized them in our approach. Our major breakthrough is in enabling this parameterization in terms of subtask compositions, which was difficult using earlier methods that relied on simple latent parametrization. Hence, while we retain the "non-parametric" terminology, we guide the readers to view the scope of our work through the lens of modularity or composition-based generalization [27].

## 3 Method

In this section, we introduce our novel meta-RL method, named Subtask Decomposition and Virtual Training (SDVT), to handle non-parametric task variability. Our approach is based on decomposing each task into a set of elementary subtasks and parameterizing each task based on the composition of these subtasks. To achieve this, we use a Gaussian mixture variational autoencoder (GMVAE) to meta-learn the subtask decomposition process. In addition, we introduce a virtual training process that improves generalization to tasks with unseen compositions of subtasks.

### Subtask Decomposition (SD) with a Gaussian Mixture Variational Autoencoder

Our goal is to meta-learn a set of elementary subtasks and to meta-learn the decomposition of each task into a composition of these subtasks. The core of our method focuses on meta-learning the approximate subtask composition \(y_{t}\in\Delta^{K}\) sampled from a \(K\)-class categorical distribution, where \(\Delta^{K}\) denotes the \(K\)-dimensional probability simplex. For example with \(K=3\), we want the subtask composition \(y_{t}\) to be learned somewhat like \((0.5,0.5,0.0)\) for "Pick-place" and \((0.0,0.5,0.5)\) for "Sweep-into," where each dimension of \(y_{t}\) represents the weight corresponding to subtasks in the order of "place object," "grip object," and "push object." To capture such subtask information, we use a Gaussian mixture variational autoencoder (GMVAE) to represent the latent space of non-parametric tasks as a Gaussian mixture distribution. Each task is represented as a \(K\)-dimensional mixture proportion \(y_{t}\), resulting in each class representing a unique subtask shared across different tasks. Thus, the distribution of \(K\) subtasks is modeled using a categorical distribution with \(K\) classes, where each class is associated with a Gaussian distribution. The learned subtask composition \(y_{t}\) represents the agent's belief at time \(t\) about the current task's decomposition into subtasks. It is crucial to note that we don't necessarily want \(y_{t}\) to be a one-hot embedding representing the subtask that the agent is solving at time \(t\). We want \(y_{t}\) to represent a mixing proportion [56] over all possible subtasks that the agent believes to be relevant to the current task, including past and future as in Figures 1(b) and 4. This distinction is vital to the model's effectiveness in solving a range of tasks, as it allows for flexibility in subtask identification and generalization across different tasks.

ArchitectureOur full model in Figure 1(a), which is based on the VAE of VariBAD [74], consists of three main components: the encoder, decoder, and policy networks parameterized by \(\phi=\{\phi_{h},\phi_{y},\phi_{z}\}\), \(\theta=\{\theta_{z},\theta_{R},\theta_{T}\}\), and \(\psi\), respectively.

**(1)** The encoder is defined as \(q_{\phi}(y_{t},z_{t}|h_{t})=q_{\phi_{y}}(y_{t}|h_{t})q_{\phi_{z}}(z_{t}|h_{t},y _{t})\). A recurrent network encodes the past trajectory \(\tau_{:t}\) into a hidden embedding \(h_{t}=q_{\phi_{h}}(\tau_{:t})\). First, the categorical encoder \(q_{\phi_{y}}(y_{t}|h_{t}):\text{Cat}(\omega_{\phi_{y}}(h_{t}))\) samples \(y_{t}\), where \(\omega_{\phi_{y}}(h_{t})\in\Delta^{K}\). We use the Gumbel-Softmax trick [23] with a high temperature (\(\tau=1\)) when sampling \(y_{t}\) to form a soft label. Then the multivariate Gaussian encoder \(q_{\phi_{z}}(z_{t}|h_{t},y_{t}):\mathcal{N}(\mu_{\phi_{z}}(h_{t},y_{t}),\sigma _{\phi_{z}}^{2}(h_{t},y_{t}))\) samples a continuous latent context \(z_{t}\), which contains the parametric information of the subtasks, in addition to the categorical subtask information of \(y_{t}\).

**(2)** The decoder is defined as \(p_{\theta}(\tau_{:H^{+}},y_{t},z_{t})=p(y_{t})p_{\theta_{z}}(z_{t}|y_{t})p_{ \theta_{R},\theta_{T}}(\tau_{:H^{+}}|z_{t})\). It reconstructs the reward and transition dynamics for all transitions in the meta-episode \(\tau_{:H^{+}}\), using the latent context \(z_{t}\) as in Eq. (4). We assume a uniform prior of subtask composition \(p(y_{t}):\text{Uniform}(1/K)\) and a Gaussian regularization \(p_{\theta_{z}}(z_{t}|y_{t}):\mathcal{N}(\mu_{\theta_{z}}(y_{t}),\sigma_{\theta _{z}}^{2}(y_{t}))\). This encoder-decoder architecture

Figure 2: **SDVT architecture.****(a)** Our proposed architecture incorporates three main components: the encoder, decoder, and policy. An online trajectory is encoded into categorical (\(y\)) and Gaussian (\(z\)) latent contexts. These contexts, which are trained to reconstruct the forward dynamics, are utilized by the policy network. This structure is also applied to the virtual training, as illustrated in Figure 2(a), with an optional dispersion layer integrated. **(b)** An example of the learned task inference process within a meta-episode (\(H^{+}=5000\) steps) on “Pick-place” is shown. We report the values for each dimension of contexts: \(\omega_{\phi_{y}}(h_{t})\), \(\mu_{\phi_{z}}(h_{t},y_{t})\), and \(\log\sigma_{\phi_{z}}(h_{t},y_{t})\).

allows both the approximate posterior \(q_{\phi}(y_{t},z_{t}|h_{t})\) and the prior \(p(z_{t})\) to follow Gaussian mixture distributions.

**(3)** The policy network, \(\pi_{\psi}(a_{t}|s_{t},b_{t})\), is trained separately conditioned on the belief \(b_{t}=(\mu_{\phi_{s}}(h_{t},y_{t}),\sigma_{\phi_{s}}(h_{t},y_{t}))\) that are parameters of the Gaussian context. In practice, we also provide the parameters of the categorical encoder \(\omega_{\phi_{y}}(h_{t})\) to the policy, which we find to improve the performance. The parameters of the distributions (\(\omega_{\phi_{y}},\mu_{\phi_{z}},\sigma_{\phi_{z}},\mu_{\theta_{z}},\) and \(\sigma_{\theta_{z}}\)) are modeled as outputs of multilayer perceptrons (MLPs) as in Appendix C.4.

ObjectiveWe optimize the GMVAE to maximize the evidence lower bound (ELBO) for all time steps \(t=0,\ldots,H^{+}\) over the trajectory distribution \(d(M_{k},\tau_{:H^{+}})\) induced by the policy in MDP \(M_{k}\):

\[\text{ELBO}_{t}(\phi,\theta) =\mathbb{E}_{d(M_{k},\tau_{:H^{+}})}\left[\mathbb{E}_{q_{\phi}(y_{ t},z_{t}|h_{t})}\mathcal{J}_{\text{GMVAE}}\right],\] (2) \[\mathcal{J}_{\text{GMVAE}} =\alpha_{\mathcal{R}}\mathcal{J}_{\text{R-rec}}+\alpha_{T} \mathcal{J}_{\text{Tree}}+\alpha_{g}\mathcal{J}_{\text{reg}}+\alpha_{c} \mathcal{J}_{\text{cat}}.\] (3)

In addition to the reconstruction objectives \(\mathcal{J}_{\text{R-rec}}\) and \(\mathcal{J}_{\text{Tree}}\), we have additional regularization \(\mathcal{J}_{\text{reg}}\) and categorical \(\mathcal{J}_{\text{cat}}\) objectives:

\[\mathcal{J}_{\text{R-rec}}=\sum_{j=0}^{H^{+}-1}\log p_{\theta_{ R}}(r_{j+1}|s_{j},a_{j},s_{j+1},z_{t}), \quad\mathcal{J}_{\text{Tree}}=\sum_{j=0}^{H^{+}-1}\log p_{\theta_{T}}(s_{j+1 }|s_{j},a_{j},z_{t}),\] (4) \[\mathcal{J}_{\text{reg}}=\log\frac{p_{\theta_{z}}(z_{t}|y_{t})}{ q_{\phi_{z}}(z_{t}|h_{t},y_{t})},\quad\mathcal{J}_{\text{cat}}=\log\frac{p(y_{t})}{ q_{\phi_{y}}(y_{t}|h_{t})}.\] (5)

The regularization objective \(\mathcal{J}_{\text{reg}}\) minimizes the KL divergence between the learned posterior Gaussian distribution \(q_{\phi_{z}}(z_{t}|h_{t},y_{t})\) and learned Gaussian priors \(p_{\theta_{z}}(z_{t}|y_{t})\). Unlike the standard VAE that assumes a standard normal prior, we learn \(K\) distinct Gaussian priors conditioned on \(y_{t}\). The categorical objective \(\mathcal{J}_{\text{cat}}\) maximizes the conditional entropy of \(y_{t}\) given \(h_{t}\) and prevents collapse. Refer to Appendix B for the derivation of the ELBO objective. The reconstruction objectives in Eq. (4) are computed for all timesteps from the first to the terminal step of the meta-episode. Therefore, the subtask composition \(y_{t}\) at time \(t\) is not necessarily a one-hot label of the belief about the current subtask at time \(t\), but a mixture label of the belief on all subtasks that compose the current task in the past and future within the meta-episode. Under the BAMDP setup, the agent learns to reduce the uncertainty of the decomposition as quickly as possible, supporting the policy with the converged parameterization. Combining the policy objective in Eq. (1) and the sum of the ELBO objectives in Eq. (2) for all timesteps in a meta-episode, the overall objective over training tasks is to maximize:

\[\mathcal{J}(\phi,\theta,\psi)=\mathbb{E}_{M_{k}\sim p(\mathcal{M}_{\text{train }})}\left[\mathcal{J}_{\text{pol}}^{+}(\psi)+\sum_{t=0}^{H^{+}}\text{ELBO}_{t}( \phi,\theta)\right].\] (6)

Occupancy regularizationThe dimension of \(y_{t}\) or the number of underlying subtasks \(K\) is a crucial hyperparameter that should be determined based on the number of training tasks \(N_{\text{train}}\) and their similarities. However, in many cases, prior information about the optimal value of \(K\), denoted as \(K^{*}\), may not be available. One way to expand the scope of our method for unknown \(K^{*}\) is to meta-learn the number of effective subtasks as well. First, we assume \(K^{*}<N_{\text{train}}\), otherwise each task will be classified into a separate subtask with one-hot label, preventing learning shareable subtasks. We start with a sufficiently large \(K=N_{\text{train}}\) and regularize the ELBO objective to progressively reduce the number of effective subtasks (non-zero components) occupied in \(y_{t}\) with the following occupancy regularization that penalizes the usage of larger indices in the subtask composition:

\[\mathcal{J}_{\text{occ}}=-\log K\left(e^{-K+1},e^{-K+2},\ldots,e^{-1},e^{0} \right)\cdot y_{t}.\] (7)

We calculate the dot product of exponential weights and the subtask composition \(y_{t}\) to penalize the occupancy of higher dimensions of \(y_{t}\). We scale the dot product by \(\log K\) to match the scale to the upper bound of \(\mathcal{J}_{\text{cat}}\). We add \(\mathcal{J}_{\text{occ}}\) multiplied by a coefficient \(\alpha_{o}\) to the GMVAE objective in Eq. (3). Consequently, the agent prioritizes using lower indices in the decomposition to represent frequent subtasks and sparingly uses higher indices for rare subtasks as in Figure 3(b) and in Appendix E.1.

Decoder dropoutAs the GMVAE is optimized using the trajectories induced by the policy, the decoder can easily overfit the frequent states and actions of training tasks [35]. This can lead to low predicted rewards for unexperienced states and actions, regardless of the latent context \(z_{t}\). Whensuch overfitting happens, the latent context loses its task-informative value, leading to the potential underperformance of policy based on this context. Following the approach of LDM [35], we address this by applying dropout (DO) to the state and action embeddings of the decoder while leaving latent context \(z_{t}\) untouched: \(p_{\theta_{R}}\left(r_{j+1}|\text{DO}\left[s_{j},a_{j},s_{j+1}\right],z_{t}\right)\) and \(p_{\theta_{T}}\left(s_{j+1}|\text{DO}\left[s_{j},a_{j}\right],z_{t}\right)\). This dropout application is crucial for the virtual training discussed in the following section.

### Virtual Training (VT) on Generated Tasks with Imaginary Subtask Compositions

The overall objective in Eq. (6) is optimized over the trajectories of training tasks. To enhance generalization to test tasks with unseen subtask compositions, we generate virtual training tasks using the imaginary dynamics produced by the GMVAE decoder. This process resembles those in [35; 1], but their generated tasks are limited to parametric variations, e.g., generating tasks with unseen goal positions. Such methods can prepare for test tasks with unseen parametric changes but struggle to prepare for qualitatively new tasks with unseen compositions of subtasks. Our GMVAE model enables us to extend the process to the non-parametric setup by conditioning the decoder on an imaginary subtask composition \(\tilde{y}\). At the beginning of each meta-episode, we randomly determine with probability \(p_{v}\) whether to convert it into a virtual meta-episode. By training the policy on imaginary tasks, it can better prepare for test tasks with unseen subtask compositions in advance. We use the tilde accent to denote imaginary components, and the hat accent to denote estimates.

Latent context dispersionInspired by the work of Ajay et al. [1], we utilize the dispersion structure for our GMVAE to support extrapolated generalization of the latent context \(z_{t}\). Instead of directly using \(z_{t}\) as the decoder input, we insert an additional MLP \(p_{\theta_{\tilde{h}}}\) before the decoder to expand the dimension of the context (the dotted box in Figure 2a). The MLP output \(\hat{h}_{t}=p_{\theta_{\tilde{h}}}(z_{t})\) is trained to reconstruct the embedding \(h_{t}\) by appending \(\alpha_{d}\mathcal{J}_{\text{dis}}=-\alpha_{d}\|h_{t}-\hat{h}_{t}\|_{2}^{2}\) to the total GMVAE objective in Eq. (3). We then use the dispersed context \(\hat{h}_{t}\) instead of \(z_{t}\) to reconstruct the reward and transition. This trick is effective in generating imaginary tasks featuring extrapolated dynamics, albeit at the cost of increased training complexity.

Imaginary reward generationFigure 3a presents the imaginary reward generation process. The goal is to create imaginary tasks based on the distribution of training subtasks but with unseen compositions of subtasks. At the beginning of a virtual meta-episode, we randomly sample an imaginary subtask composition \(\tilde{y}\sim\text{Dirichlet}(\tilde{y})\) fixed for that virtual meta-episode, where the concentration parameter \(\tilde{y}\in\Delta^{K}\) is the empirical running mean of all \(y_{t}\) over training. By replacing the real \(y_{t}\) with the imaginary subtask composition \(\tilde{y}\), we sample an imaginary context \(\tilde{z}_{t}\sim\mathcal{N}(\mu_{\phi_{z}}(h_{t},\tilde{y}),\sigma_{\phi_{z} }^{2}(h_{t},\tilde{y}))\), imaginary dispersed context \(\tilde{\hat{h}}_{t}=p_{\theta_{\tilde{h}}}(\tilde{z}_{t})\), and finally the imagi

Figure 3: **Virtual training.****(a)** Generation of imaginary rewards with the decoder conditioned on a fixed imaginary subtask composition \(\tilde{y}\). **(b)–(e)** Examples depicting the diversity of generated imaginary tasks, where \(K=5\) and all states are from the Meta-World “Reach.” Red rods and blue circles represent the trajectories of the gripper and object, respectively. These trajectories, while generated by the same policy, differ across various imaginary subtask compositions \(\tilde{y}\).

nary reward \(\tilde{r}_{t+1}\sim p_{\theta_{R}}(r_{t+1}|\text{DO}\left[s_{t},a_{t},s_{t+1} \right],\tilde{\tilde{h}}_{t})\) accordingly using our GMVAE. We replace the reward for the next timestep, \(r_{t+1}\), with \(\tilde{r}_{t+1}\), while the states remain to be from the real training task. The imaginary reward is used for the encoder input at the next time step and for the policy, where the policy is trained to maximize the sum of generated rewards, \(\sum_{t=0}^{H^{+}-1}\gamma^{t}\tilde{r}_{t+1}\). However, the GMVAE is not trained to reconstruct the imaginary dynamics.

### Summary of the Combined Methods: SDVT-LW and SDVT

By combining the subtask decomposition (SD) and virtual training (VT) processes, we propose two methods: SDVT-LW and SDVT. Foremost, our primary contribution is the proposal of SDVT-LW, which is the lightweight (-LW) version of our method that assumes the prior knowledge of the optimal number of subtasks \(K\), therefore not employing the occupancy regularization.

Furthermore, we propose SDVT with the occupancy regularization strategy. This generalizes SDVT-LW to adapt to more difficult conditions where there is a lack of prior knowledge of the optimal number of subtasks \(K^{*}\). We initialize the number of subtasks equal to the number of training tasks and employ occupancy regularization to downscale higher dimensions, navigating to discover the most efficient number of subtasks even without prior knowledge.

For the purposes of virtual training, we adopt two methodologies that have found application in previous studies: dropout [35] and dispersion via structured VAE [1]. Their use in our work remains unchanged from their original applications. The efficacy of these components, within the context of virtual training, is demonstrated via ablations in Appendix E. We summarize the entire meta-training process with a pseudocode in Appendix A.

## 4 Related Work

Classical meta-RLClassical meta-RL methods assume that both training and test tasks are sampled from the same distribution. These methods are divided into two main categories: gradient-based methods and context-based methods. Gradient-based methods [13; 54; 48; 73; 53] learn a common initialization of a lightweight model for all tasks, allowing the agent to achieve high performance on unseen target tasks with a few steps of gradient updates. However, these methods lack online adaptation capability within a meta-episode because they require many pre-update rollouts before adaptation. Context-based methods [9; 18; 46; 32; 34; 43; 74; 40] use a recurrent or memory-augmented network to encode the collected experience into a latent context. In general, the context is trained to optimize auxiliary objectives such as reward dynamics, transition dynamics, and value function. These methods can adapt to target tasks through online, in-context learning without requiring gradient updates. However, they are vulnerable to test-time distribution shifts since the encoded context and the policy given the context are hardly generalized to out-of-distribution tasks.

Out-of-distribution meta-RLA group of recent studies focuses on training a generalizable agent that is robust to test-time distribution shifts. A group of works generates imaginary observations using image augmentation techniques [18; 21; 31; 44; 33; 61; 66]. Most of these methods depend on predefined heuristic augmentations, without utilizing the training task dynamics. On the other hand, some works explicitly address varying environment dynamics. For example, MIER [7] reuses the trajectories collected during training by relabeling according to the test dynamics. Our work is related to AdMRL [36], LDM [35], and DiAMetR [1], which generate imaginary tasks with unseen dynamics using learned models. However, these works focus on generating parametric variants of training tasks, while we focus on generalizing across non-parametric task variants.

Skill-based meta-RLOur task parameterization based on the subtask decomposition is related to the recently spotlighted skill-based meta-RL methods [11; 51; 50; 55], which aim to achieve fast generalization on unseen tasks by decomposing and extracting reusable skills from training tasks' trajectories. These works often require refined offline demonstrations to learn skills using behavioral cloning objectives, where the skills distinguish a sequence of actions given a sequence of states. For example, SimPL [42] extracts skills from offline demonstrations before meta-training, and during a meta-test, it only adapts the high-level skill policy, with the low-level policy frozen. HeLMS [47] learns a 3-level skill hierarchy by decomposing offline demonstrations of a single task. Online learning of the skills is often unstable because the set of skills should develop along with the online improvement of the policy. The subtask decomposition of our method is conceptually different from the skill decomposition [68]. Even when the policy is not stationary during online updates, the underlying reward and transition dynamics, which our model has to estimate, do not change.

## 5 Experiments

### Experimental Setup

Meta-World benchmarkThe Meta-World V2 benchmark [71] stands as the most prominent, if not the only, established benchmark for assessing meta-RL algorithms featuring non-parametric task variability. This benchmark comprises 50 qualitatively distinct robotic manipulation tasks, with each task containing 50 parametric variants that incorporate randomized goals and initial object positions. Specifically, the Meta-World Meta-Learning 10 (ML-10) benchmark, consists of \(N_{\text{train}}=10\) training tasks and \(N_{\text{test}}=5\) held-out test tasks. We denote each task by an index, where the training tasks are numbered from 1 to 10 and the test tasks from 11 to 15. Likewise, the ML-45 benchmark consists of \(N_{\text{train}}=45\) training tasks and \(N_{\text{test}}=5\) test tasks. Refer to the tables in Appendix D.1 for the set and indices of tasks. The agent must maximize its return from experience while exploring to identify the initially unknown task dynamics within a meta-episode of \(H^{+}=5000\) steps that consists of \(n_{\text{roll}}=10\) rollout episodes of horizon \(H=500\) steps each.

SDVT variants and baselines setupWe evaluate our methods SDVT and SD (only subtask decomposition without virtual training and dispersion). Without the prior knowledge of \(K^{*}\), we set \(K=N_{\text{train}}\) and apply the occupancy regularization \((\alpha_{o}=1.0)\) with \(\alpha_{c}=1.0\). We also evaluate a lightweight (-LW) version of ours with smaller \(K=5\) with \(\alpha_{c}=0.5\) and without the occupancy regularization (\(\alpha_{o}=0.0\)). To ensure a fair comparison and to exclude the gains from orthogonal contributions, we compare SDVT with state-of-the-art meta-RL methods that do not require any refined offline demonstrations, ensembles, or extensive test-time training: RL2[9], MAML [13], PEARL [46], and VariBAD [74]. We also compare with a parametric OOD meta-RL method, LDM [35], to evaluate the efficacy of our virtual training over subtasks. All methods do not perform gradient updates during the test except for MAML. Appendix C presents more implementation details. Briefly, SDVT without a Gaussian mixture reduces to LDM, LDM without virtual training reduces to VariBAD, and VariBAD without a VAE decoder reduces to RL2. Our implementation is available at https://github.com/suyoung-lee/SDVT.

Footnote 2: Our aggregation method diverges from the atypical method employed in the Meta-World paper [71], which presents the main results as the average of the _maximum_ success rate of each task (details in Appendix C.5)

Evaluation metricWe follow the standard success criterion of Meta-World as follows. A timestep is considered successful if the distance between the task-relevant object and the goal is less than a predefined threshold. A rollout episode is considered successful if the agent ever succeeds at any timestep during the rollout episode. The success of a meta-episode is defined as the success of the last (10th) rollout episode. In Table 1, we report the success rates of 750 (15 tasks \(\times\) 50 parametric variants) meta-episodes at 250M steps for ML-10 and 2500 (50 tasks \(\times\) 50 parametric variants) meta-episodes at 400M steps for ML-45.3 Likewise, we report the returns of the last rollout episodes.

Footnote 3: Our aggregation method diverges from the atypical method employed in the Meta-World paper [71], which presents the main results as the average of the _maximum_ success rate of each task (details in Appendix C.5)

### Results

Table 1 illustrates that no method attains a 100% success rate even on training tasks, emphasizing the challenge posed by non-parametric task variability. The training success rates on ML-45 are consistently lower than those on ML-10, reflecting the inherent difficulty in adapting to a broader range of tasks, such as conflicting gradients [70]. Notably, SD surpasses all other baselines in training success rate. In particular, outperforming VariBAD underscores the limitations of employing a single Gaussian distribution to model the latent task space in cases of non-parametric variability.

On the test tasks, SDVT and SDVT-LW substantially outperform all baselines, even outperforming LDM, which surpasses VariBAD with parametric virtual training. Our gain is attributed to our virtual training process, which is specifically designed for test tasks involving non-parametric variability. Notably, SDVT and SD outperform their LW counterparts in training success rates, primarily due to its fine-grained subtask decomposition which provides a more precise representation of each task. For example, a "grip object" subtask may be split and represented as a combination of two distinct subtasks "move gripper" and "tighten gripper" with a larger \(K\). In contrast, SDVT-LW scores higher test success rates than SDVT, presumably due to its coarser decomposition that allows imaginary compositions to encompass a broader range of unseen test tasks.

Please refer to the tables in Appendix D.1 for individual task results. Our methods achieve the highest success rates across all test tasks on the ML-10 benchmark. However, all methods encounter challenges in solving "Shelf-place," which includes an unseen shelf not present in the observation. As such, this task cannot be decomposed into previously seen subtasks but rather into unseen ones, making it difficult to prepare through virtual training. These tasks, composed of unseen subtasks, pose a considerable challenge for zero-shot adaptation as with SDVT. Addressing these tasks may require a substantial increase in rollouts and updates during tests, an area of potential future work. Detailed ablation results are reported in Appendix E. For additional results such as rendered videos, refer to our webpage https://sites.google.com/view/sdvt-neurips.

### Analysis

In this subsection, we explore whether the performance enhancements attributed to our methods are indeed the result of effectively decomposed subtasks and a diverse range of imaginary tasks.

Learned Subtask CompositionsFrom Figure 2b, we observe a rapid context convergence within the first rollout episode for a sufficiently meta-learned task. To validate our motivation, we visualize the converged subtask compositions in Figure 4. We find that such converged subtask compositions are shared by qualitatively similar tasks. For example, in Figure 4a, tasks (3) "Pick-place," (7) "Peg-insert-side," and (10) "Basketball," which require placing an object at a goal position, share subtask indices 1 and 3. Additionally, simple tasks that require moving the gripper in a straight line: (1) "Reach," (5) "Drawer-close," and (8) "Window-open" are classified into the same subtask 2. These observations suggest that our model can effectively meta-learn the decomposition process of tasks into shareable subtasks. Furthermore, the test tasks may not necessarily have the same subtask compositions as the training tasks. For instance, (3) "Pick-place" and (14) "Sweep-into" share subtask 3, but not subtasks 1 or 5, revealing the potential for virtual training to be effective.

Occupancy RegularizationFigure 4 indicates that it is important to set the subtask class dimension \(K\) and the categorical coefficient \(\alpha_{c}\) appropriately according to the number of training tasks and the correlations among them. Otherwise, the decomposition may suffer from a collapse or degeneration. With the occupancy regularization, we can avoid the extensive search for the optimal subtask dimension \(K^{*}\) and corresponding \(\alpha_{c}\). In Figure 4b, we find that our occupancy regularization successfully limits the use of unnecessary subtasks with higher indices as intended.

Generated imaginary tasksTo demonstrate the dynamics of the imaginary tasks, we show the trajectory of the gripper and object over a meta-episode (5000 steps) on generated imaginary tasks for different imaginary subtask compositions \(\tilde{y}\) in Figure 3. When we set \(\tilde{y}\) as a one-hot vector, the policy

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{Success Rate} & \multicolumn{4}{c}{Return} \\ \cline{2-9}  & \multicolumn{2}{c}{ML-10} & \multicolumn{2}{c}{ML-45} & \multicolumn{2}{c}{ML-10} & \multicolumn{2}{c}{ML-45} \\ \cline{2-9} Methods & Train & Test & Train & Test & Train & Test & Train & Test \\ \hline SDVT & **77.2\(\pm\)3.0** & 32.8\(\pm\)3.9 & 55.6\(\pm\)4.2 & 28.1\(\pm\)3.2 & **3656\(\pm\)62** & 1225\(\pm\)160 & 2379\(\pm\)214 & 839\(\pm\)74 \\ SDVT-LW & 62.1\(\pm\)4.1 & **33.4\(\pm\)5.0** & 50.4\(\pm\)4.1 & **31.2\(\pm\)1.2** & 3454\(\pm\)137 & **1527\(\pm\)214** & 2294\(\pm\)202 & **894\(\pm\)27** \\ SD & 77.0\(\pm\)5.9 & 30.8\(\pm\)7.7 & **61.0\(\pm\)1.7** & 23.0\(\pm\)5.1 & 3630\(\pm\)241 & 1112\(\pm\)190 & **2672\(\pm\)79** & 786\(\pm\)69 \\ SD-LW & 75.5\(\pm\)5.5 & 26.2\(\pm\)8.7 & 56.7\(\pm\)1.5 & 25.4\(\pm\)2.9 & 3525\(\pm\)297 & 1043\(\pm\)234 & 2578\(\pm\)64 & 793\(\pm\)49 \\ \hline RL\({}^{2}\) & 67.4\(\pm\)4.4 & 15.1\(\pm\)2.7 & 58.0\(\pm\)0.4 & 11.8\(\pm\)3.2 & 1159\(\pm\)83 & 715\(\pm\)33 & 1411\(\pm\)22 & 663\(\pm\)100 \\ MAML & 42.2\(\pm\)4.5 & 3.9\(\pm\)3.7 & 32.0\(\pm\)1.4 & 19.8\(\pm\)6.3 & 1822\(\pm\)136 & 439\(\pm\)78 & 1388\(\pm\)104 & 658\(\pm\)96 \\ PEARL & 23.2\(\pm\)1.9 & 0.8\(\pm\)0.5 & 10.3\(\pm\)2.4 & 6.7\(\pm\)3.3 & 1081\(\pm\)77 & 340\(\pm\)54 & 597\(\pm\)121 & 506\(\pm\)122 \\ VariBAD & 58.2\(\pm\)8.9 & 14.1\(\pm\)6.1 & 57.0\(\pm\)1.2 & 22.1\(\pm\)3.5 & 3055\(\pm\)466 & 919\(\pm\)143 & 2492\(\pm\)47 & 762\(\pm\)40 \\ LDM & 56.7\(\pm\)12.3 & 19.8\(\pm\)6.0 & 54.1\(\pm\)0.9 & 24.8\(\pm\)2.9 & 2963\(\pm\)626 & 1166\(\pm\)264 & 2515\(\pm\)67 & 768\(\pm\)63 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Meta-World V2 success rates and returns. We report the final success rates (%) and returns of our methods and baselines averaged across training tasks and test tasks of the ML-10 and ML-45 benchmarks. All results are reported as the mean success rate \(\pm\) 95% confidence interval of 8 seeds. Individual scores of all tasks are reported in Appendix D.1.**returns a trajectory that solves an elementary subtask, such as placing and reaching up. We observe that the trajectories vary across different \(\tilde{y}\), and similar compositions result in similar trajectories.

## 6 Conclusion

In conclusion, our proposed method has demonstrated a considerable enhancement in meta-RL with non-parametric task variability. This improvement is achieved by meta-learning shareable subtask decompositions and executing virtual training on the imaginary subtask compositions. However, it's essential to acknowledge certain potential limitations beyond the scope of this study, particularly when addressing test tasks involving entirely novel subtasks and in broader setups where the action and state spaces may also vary. Despite these limitations, we believe that expanding the realm of meta-RL to accommodate a wider range of task variability is a critical research topic. Incorporating orthogonal approaches such as using offline demonstrations or test time training techniques into our method could lead to interesting future work addressing the limitations. We are optimistic that our study lays a robust foundation for future research in this field.

## Acknowledgements

This work was supported in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00469, Development of Core Technologies for Task-oriented Reinforcement Learning for Commercialization of Autonomous Drones, 50%) and in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities, 50%)

Figure 4: **Learned subtask compositions on ML-10.****(a)** Default SDVT-LW **(b)** Default SDVT. **(c) and (d)** SDVT-LW with varying \(\alpha_{c}\). Each column displays the terminal subtask composition (\(y_{H^{+}}\)) of each task learned after 250M training steps of training, averaged across 50 parametric variants. The decompositions at different timesteps can be found in Appendix D.3. The results shown are from the first random seed, as different seeds yield distinct decompositions. Results of other seeds are provided in Appendix D.4.

## References

* [1] A. Ajay, A. Gupta, D. Ghosh, S. Levine, and P. Agrawal. Distributionally adaptive meta reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, 2022.
* [2] J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. Zintgraf, C. Finn, and S. Whiteson. A survey of meta-reinforcement learning. _arXiv preprint arXiv: 2301.08028_, 2023.
* [3] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Acting optimally in partially observable stochastic domains. In _National Conference on Artificial Intelligence (NCAI)_, 1994.
* [4] Y. Chandak, G. Theocharous, S. Shankar, M. White, S. Mahadevan, and P. Thomas. Optimizing for the future in non-stationary mdps. In _International Conference on Machine Learning (ICML)_, pages 119:1414-1425, 2020.
* [5] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. D. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In _International Conference on Robotics and Automation, (ICRA)_, pages 8973-8979, 2019.
* [6] B. Chen, W. Deng, and H. Shen. Virtual class enhanced discriminative embedding learning. In _Neural Information Processing Systems (NeurIPS)_, pages 31:1942-1952, 2018.
* [7] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement learning. In _International Conference on Machine Learning (ICML)_, pages 97:1282-1289, 2019.
* [8] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. In _International Conference on Learning Representations (ICLR)_, 2017.
* [9] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL\({}^{2}\): Fast reinforcement learning via slow reinforcement learning. _arXiv preprint arXiv: 1611.02779_, 2016.
* [10] M. O. Duff. _Optimal Learning: Computational Procedures for Bayes-adaptive Markov Decision Processes_. University of Massachusetts at Amherst, 2002.
* [11] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. In _International Conference on Learning Representations (ICLR)_, 2019.
* [12] R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola. Meta-q-learning. In _International Conference on Learning Representations (ICLR)_, 2020.
* [13] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International Conference on Machine Learning (ICML)_, pages 70:1126-1135, 2017.
* [14] C. Florensa, D. Held, X. Geng, and P. Abbeel. Automatic goal generation for reinforcement learning agents. In _International Conference on Machine Learning (ICML)_, pages 80:1515-1528, 2018.
* [15] Garage-Contributors. Garage: A toolkit for reproducible reinforcement learning research. https://github.com/rlworkgroup/garage, 2019.
* [16] M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar. Bayesian reinforcement learning: A survey. _Found. Trends Mach. Learn._, pages 8(5-6):359-483, 2015.
* [17] A. Gupta, B. Eysenbach, C. Finn, and S. Levine. Unsupervised meta-learning for reinforcement learning. _arXiv preprint arXiv: 1806.04640_, 2018.
* [18] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine. Meta-reinforcement learning of structured exploration strategies. In _Neural Information Processing Systems (NeurIPS)_, pages 31:5302-5311, 2018.

* [19] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In _International Conference on Learning Representations, (ICLR)_, 2020.
* [20] J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess. Meta reinforcement learning as task inference. _arXiv preprint arXiv: 1905.06424_, 2019.
* [21] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann. Generalization in reinforcement learning with selective noise injection and information bottleneck. In _Neural Information Processing Systems (NeurIPS)_, pages 32:13978-13990, 2019.
* [22] A. Jabri, K. Hsu, B. Eysenbach, A. Gupta, S. Levine, and C. Finn. Unsupervised curricula for visual meta-reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [23] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In _International Conference on Learning Representations (ICLR)_, 2017.
* [24] J. Kaddour, S. Szemundsson, and M. P. Deisenroth. Probabilistic active meta-learning. In _Neural Information Processing Systems (NeurIPS)_, pages 33:20813-20822, 2020.
* [25] L. P. Kaelbling, M. L.Littman, and A. R.Cassandra. Acting optimally in partially observable stochastic domains. In _National Conference on Artificial Intelligence_, 1994.
* [26] L. P. Kaelbling, M. L.Littman, and A. R.Cassandra. Planning and acting in partially observable stochastic domains. _Artificial Intelligence_, 101:99-134, 1998.
* [27] K. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review and perspectives. In _Journal of Artificial Intelligence Research_, pages 1401-1476, 2022.
* [28] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In _International Conference on Learning Representations (ICLR)_, 2014.
* [29] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. In _Neural Information Processing Systems (NeurIPS)_, 2014.
* [30] L. Kirsch, S. van Steenkiste, and J. Schmidhuber. Improving generalization in meta reinforcement learning using learned objectives. In _International Conference on Learning Representations (ICLR)_, 2020.
* [31] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. In _Neural Information Processing Systems (NeurIPS)_, pages 33:19884-19895, 2020.
* [32] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. In _Neural Information Processing Systems (NeurIPS)_, pages 33:741-752, 2020.
* [33] K. Lee, K. Lee, J. Shin, and H. Lee. Network randomization: A simple technique for generalization in deep reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2020.
* [34] K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin. Context-aware dynamics model for generalization in model-based reinforcement learning. In _International Conference on Machine Learning (ICML)_, pages 119:5757-5766, 2020.
* [35] S. Lee and S. Chung. Improving generalization in meta-RL with imaginary tasks from latent dynamics mixture. In _Neural Information Processing Systems (NeurIPS)_, pages 34:27222-27235, 2021.
* [36] Z. Lin, G. Thomas, G. Yang, and T. Ma. Model-based adversarial meta-reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, pages 33:10161-10173, 2020.
* [37] S. Liu, A. J. Davison, and E. Johns. Self-supervised generalisation with meta auxiliary learning. In _Neural Information Processing Systems (NeurIPS)_, pages 32:1679-1689, 2019.

* [38] J. J. Martin. _Bayesian Decision Problems and Markov Chains_. Wiley, 1967.
* [39] B. Mehta, T. Deleu, S. C. Raparthy, C. J. Pal, and L. Paull. Curriculum in gradient-based meta-reinforcement learning. _arXiv preprint arXiv:2002.07956_, 2020.
* [40] L. C. Melo. Transformers are meta-reinforcement learners. In _International Conference on Machine Learning (ICML)_, pages 162:15340-15359, 2022.
* [41] R. Mendonca, X. Geng, C. Finn, and S. Levine. Meta-reinforcement learning robust to distributional shift via model identification and experience relabeling. _arXiv preprint arXiv: 2006.07178_, 2020.
* [42] T. Nam, S.-H. Sun, K. Pertsch, S. J. Hwang, and J. J. Lim. Skill-based meta-reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2022.
* [43] R. Raileanu, M. Goldstein, A. Szlam, and R. Fergus. Fast adaptation to new environments via policy-dynamics value functions. In _International Conference on Machine Learning (ICML)_, pages 119:7920-7931, 2020.
* [44] R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus. Automatic data augmentation for generalization in reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, pages 34:5402-5415, 2021.
* [45] A. Rajeswaran, K. Lowrey, E. Todorov, and S. M. Kakade. Towards generalization and simplicity in continuous control. In _Neural Information Processing Systems (NeurIPS)_, pages 30:6550-6561, 2017.
* [46] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In _International Conference on Machine Learning (ICML)_, pages 97:5331-5340, 2019.
* [47] D. Rao, F. Sadeghi, L. Hasenclever, M. Wulfmeier, M. Zambelli, G. Vezzani, D. Tirumala, Y. Aytar, J. Merel, N. Heess, and R. Hadsell. Learning transferable motor skills with hierarchical latent mixture policies. In _International Conference on Learning Representations (ICLR)_, 2022.
* [48] J. Rothfuss, D. Lee, I. Clavera, T. Asfour, and P. Abbeel. Promp: Proximal meta-policy search. In _International Conference on Learning Representations (ICLR)_, 2019.
* [49] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv: 1707.06347_, 2017.
* [50] D. Shah, P. Xu, Y. Lu, T. Xiao, A. Toshev, S. Levine, and B. Ichter. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. In _International Conference on Learning Representations (ICLR)_, 2022.
* [51] L. X. Shi, J. J. Lim, and Y. Lee. Skill-based model-based reinforcement learning. In _Conference on Robot Learning, (CoRL)_, 2022.
* [52] X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur. Observational overfitting in reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2020.
* [53] Y. Song, A. Mavalankar, W. Sun, and S. Gao. Provably efficient model-based policy adaptation. In _International Conference on Machine Learning (ICML)_, pages 119:9088-9098, 2020.
* [54] B. C. Stadie, G. Yang, R. Houthooft, X. Chen, Y. Duan, Y. Wu, P. Abbeel, and I. Sutskever. The importance of sampling in meta-reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, pages 31:9280-9290, 2018.
* [55] L. Sun, H. Zhang, W. Xu, and M. Tomizuka. Paco: Parameter-compositional multi-task reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, 2022.
* [56] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Sharing clusters among related groups: Hierarchical dirichlet processes. In _Neural Information Processing Systems (NeurIPS)_, 2004.

* [57] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _International Conference on Intelligent Robots and Systems (IROS)_, pages 23-30, 2017.
* [58] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In _International Conference on Intelligent Robots and Systems (IROS)_, pages 5026-5033, 2012.
* [59] Y. B. Varolgunes, T. Bereau, and J. F. Rudzinski. Interpretable embeddings from molecular simulations using gaussian mixture variational autoencoders. _Machine Learning Science and Technology_, 1, 2020.
* [60] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. In _Annual Meeting of the Cognitive Science Community (CogSci)_, 2016.
* [61] K. Wang, B. Kang, J. Shao, and J. Feng. Improving generalization in reinforcement learning with mixture regularization. In _Neural Information Processing Systems (NeurIPS)_, pages 33:7968-7978, 2020.
* [62] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. _arXiv preprint arXiv: 1901.01753_, 2019.
* [63] R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In _International Conference on Machine Learning (ICML)_, pages 119:9940-9951, 2020.
* [64] S. Whiteson, B. Tanner, M. E. Taylor, and P. Stone. Protecting against evaluation overfitting in empirical reinforcement learning. In _IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)_, pages 120-127, 2011.
* [65] J. Yang, B. K. Petersen, H. Zha, and D. Faissol. Single episode policy transfer in reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2020.
* [66] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International Conference on Learning Representations (ICLR)_, 2021.
* [67] M. Yin, G. Tucker, M. Zhou, S. Levine, and C. Finn. Meta-learning without memorization. In _International Conference on Learning Representations (ICLR)_, 2020.
* [68] M. Yoo, S. Cho, and H. Woo. Skills regularized task decomposition for multi-task offline reinforcement learning. In _Neural Information Processing Systems (NeurIPS)_, 2022.
* [69] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on Robot Learning, (CoRL)_, 2019.
* [70] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi-task learning. In _Neural Information Processing Systems (NeurIPS)_, pages 33:5824-5836, 2020.
* [71] T. Yu, D. Quillen, Z. He, R. Julian, A. Narayan, H. Shively, A. Bellathur, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. _arXiv preprint arXiv:1910.10897_, 2021.
* [72] C. Zhang, O. Vinyals, R. Munos, and S. Bengio. A study on overfitting in deep reinforcement learning. _arXiv preprint arXiv: 1804.06893_, 2018.
* [73] L. M. Zintgraf, K. Shiarlis, V. Kurin, K. Hofmann, and S. Whiteson. Fast context adaptation via meta-learning. In _International Conference on Machine Learning (ICML)_, pages 97:7693-7702, 2019.
* [74] L. M. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. Varibad: A very good method for bayes-adaptive deep RL via meta-learning. In _International Conference on Learning Representations (ICLR)_, 2020.

## Appendix A Pseudocode

``` Initialize Encoder \(q_{\phi}\), decoder \(p_{\theta}\), policy \(\pi_{\psi}\), set of training tasks \(\mathcal{M}_{\text{train}}\), virtual ratio \(p_{v}\), GMVAE buffer \(\mathcal{B}_{\text{VAE}}\), policy buffer \(\mathcal{B}_{\text{pol}}\), total number of meta-episodes to train on \(n_{\text{meta}}\), number of rollout episodes per meta-episode \(n_{\text{pol}}\), running mean of subtask composition \(\bar{y}\). for meta-episode \(k=0,\dots,n_{\text{meta}}-1\)do  Sample a training task \(M_{k}\sim p(\mathcal{M}_{\text{train}})\)  Sample a virtual meta-episode flag \(V\sim\text{Bernoulli}(p_{v})\) if\(V=1\)then  Sample an imaginary subtask composition \(\tilde{y}\sim\text{Dirichlet}(\bar{y})\) endif  Reset \(h_{0},y_{0},\mathcal{B}_{\text{pol}}\) for timestep \(t=0,\dots,n_{\text{cell}}\times H-1\)do if\(t\mod H=0\)then  Reset rollout episode \(s_{t}\sim T_{0,k}(\cdot)\) endif  Sample an action \(a_{t}\sim\pi_{\psi}(\cdot|s_{t},\mu_{\phi_{z}}(h_{t},y_{t}),\sigma_{\phi_{z}}(h _{t},y_{t}),\omega_{\phi_{y}}(h_{t}))\)  Take an environment step \(s_{t+1}\sim T_{k}(\cdot|s_{t},a_{t})\) \(r_{t+1}\gets R_{k}(s_{t},a_{t},s_{t+1})\) if\(V=1\)then \(\tilde{z}_{t}\sim\mathcal{N}(\mu_{\phi_{z}}(h_{t},\tilde{y}),\sigma_{\phi_{z}}^{ 2}(h_{t},\tilde{y}))\) \(\tilde{h}_{t}=p_{\theta_{\bar{z}}}(\tilde{z}_{t})\) \(\tilde{r}_{t+1}\sim p_{\theta_{R}}(\cdot|\text{DO}\left[s_{t},a_{t},s_{t+1} \right],\tilde{h}_{t})\)  Replace \(r_{t+1}\leftarrow\tilde{r}_{t+1}\) else  Add the transition \((s_{t},a_{t},s_{t+1},r_{t+1})\) to \(\mathcal{B}_{\text{VAE}}\cdots\) VAE not trained with virtual dynamics endif  Add the transition \((s_{t},a_{t},s_{t+1},r_{t+1})\) to \(\mathcal{B}_{\text{pol}}\)  Update hidden embedding \(h_{t+1}=q_{\phi_{h}}(r_{t+1})\)  Update subtask composition \(y_{t+1}\sim\text{Cat}\left(\omega_{\phi_{y}}(h_{t+1})\right)\)  Update the running mean of subtask composition \(\bar{y}\leftarrow\text{RunningMeanUpdate}(\bar{y},y_{t+1})\) endfor  Update GMVAE \(\{\phi,\theta\}\leftarrow\{\phi,\theta\}+\nabla_{\{\phi,\theta\}}\sum_{t=0}^{H ^{+}}\text{ELBO}_{t}\) with samples from \(\mathcal{B}_{\text{VAE}}\)  Update policy \(\psi\leftarrow\psi+\nabla_{\psi}\mathcal{J}_{\text{pol}}^{+}\) with samples from \(\mathcal{B}_{\text{pol}}\)  Anneal virtual ratio \(p_{v}\gets p_{v}+\Delta p_{v}\) endfor ```

**Algorithm 1** Subtask Decomposition and Virtual Training (SDVT)

## Appendix B ELBO Derivation

The GMVAE's ELBO objective is derived as follows.

\[\mathbb{E}_{d(M_{k},\tau_{H^{+}})}\left[\log p_{\theta}(\tau_{:H^ {+}})\right] =\mathbb{E}_{d(M_{k},\tau_{:H^{+}})}\left[\log\mathbb{E}_{q_{\phi}( y_{t},z_{t}|h_{t})}\left[\frac{p_{\theta}(\tau_{:H^{+}},y_{t},z_{t})}{q_{\phi}(y_{t},z_ {t}|h_{t})}\right]\right]\] \[\geq\mathbb{E}_{d(M_{k},\tau_{:H^{+}})}\left[\mathbb{E}_{q_{\phi}( y_{t},z_{t}|h_{t})}\left[\log\frac{p_{\theta}(\tau_{:H^{+}},y_{t},z_{t})}{q_{\phi}(y_{t},z_{t}|h_{t})}\right]\right]\] \[=\mathbb{E}_{d(M_{k},\tau_{:H^{+}})}\left[\mathbb{E}_{q_{\phi}(y_{ t},z_{t}|h_{t})}\left[\log\frac{p_{\theta}(\tau_{:H^{+}}|y_{t},z_{t})p_{\theta}(z_{t}|y_ {t})p(y_{t})}{q_{\phi}(y_{t}|h_{t})q_{\phi}(z_{t}|h_{t},y_{t})}\right]\right]\] \[=\mathbb{E}_{d(M_{k},\tau_{:H^{+}})}\left[\mathbb{E}_{q_{\phi}( y_{t},z_{t}|h_{t})}\left[\log p_{\theta}(\tau_{:H^{+}}|z_{t})+\log\frac{p_{\theta}(z_{t}| y_{t})}{q_{\phi}(z_{t}|h_{t},y_{t})}+\log\frac{p(y_{t})}{q_{\phi}(y_{t}|h_{t})} \right]\right].\] (8)

Eq. (8) is equivalent to the ELBO objective in Eq. (3) without weighting coefficients. We assume that the reconstruction \(\tau_{:H^{+}}\) is conditionally independent of the subtask composition \(y_{t}\) given \(z_{t}\).

Implementation Details

### Reference Implementations

####.c.1 Reference Implementations

**MAML, RL2**, **and PEARL** To replicate the results of MAML [13], RL2[9], and PEARL [46] as reported in the Meta-World paper [71], we utilize the exact version3 of the Garage repository [15] without any modifications. For their hyperparameters, please refer to Appendix D.7, D.8, and D.9 of the Meta-World paper. MAML is the only baseline that has the advantage to take gradient updates during the test.

Footnote 3: https://github.com/rlworkgroup/garage/pull/2287

**SDVT, SD, LDM, VariBAD** Task-inference-based methods (SDVT, SD, LDM [35], and VariBAD [74]) are adapted to the Meta-World benchmark based on the VariBAD's implementation.4 We begin with VariBAD's hyperparameter configuration for MuJoCo [58] Ant-goal and modify certain hyperparameters to accommodate the Meta-World benchmark (e.g., batch size, network capacity, etc.). In line with VariBAD and LDM, we train SDVT's policy using PPO [49]. The GMVAE is based on an implementation5 that employs the Gumbel-Softmax reparameterization trick [23] when sampling the categorical subtask variable. The virtual training processes of SDVT and LDM require agent-environment interaction since they use states from the real environment. Therefore, the virtual training steps are also added when counting the total number of training steps. For a comprehensive list of SDVT hyperparameters, shared among SD, LDM, and VariBAD to ensure a fair comparison, please refer to Appendix C.3 and our source code available at https://github.com/syoung-lee/SDVT.

Footnote 4: https://github.com/lmzintgraf/varibad

Footnote 5: https://github.com/jariasf/GMVAE

### Computational Complexity

Our experiments were conducted using an Nvidia TITAN Xp. Due to the considerably larger variety of tasks compared to standard meta-RL benchmarks, non-parametric benchmarks require significantly more computational resources and time. In Table 2, we detail the total wall-clock time expended by our methods and baselines during the training and evaluation of the ML-10 benchmark. This time includes the environmental interaction time for 250M training steps, roughly 100M steps dedicated to

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & SDVT & SDVT-LW & SD & SD-LW & RL2  & MAML & PEARL & VariBAD & LDM \\ \hline Wall-clock time (hours) & 142 & 140 & 138 & 135 & 192 & 17 & 258 & 126 & 131 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Computational complexity.** The total wall-clock time required to generate the results for the ML-10, averaged across all eight random seeds.

Figure 5: **Schematic overview of algorithms. SDVT without a Gaussian mixture reduces to LDM, LDM without virtual training reduces to VariBAD, and VariBAD without a VAE decoder reduces to RL2. MAML and PEARL use fully connected networks.**

evaluations, and the time taken to train the neural networks. Despite incorporating the GMVAE and virtual training, our method's computational demand does not substantially surpass that of VariBAD.

### Hyperparameters

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Category & Description & Notation & Value (ML-10, 45) & Argument Name \\ \hline \multirow{6}{*}{General} & Rollout episode horizon & \(H\) & \(500\) & max\_episode\_steps \\  & Number of rollout episodes & \(n_{\text{roll}}\) & \(10\) & max\_rollouts\_per\_task \\  & Discount factor & \(\gamma\) & \(0.99\) & policy\_gamma \\  & Number of parallel processes & & \(10\) & num\_processes \\  & Total Environment Steps & & \(2.5e8,4.0e8\) & num\_frames \\ \hline \multirow{6}{*}{GMVAE} & Optimizer & \multicolumn{1}{c}{Adam} & optimiser\_vae \\  & Learning rate & \(1e-3\) & lr\_vae \\  & Subsample ELBO & & \(100\) & vae\_subsample\_elbos \\  & Subsample decodes & & \(100\) & vae\_subsample\_decodes \\  & Buffer size (meta-episodes) & \(|\mathcal{B}_{\text{VAE}}|\) & \(1000\) & size\_vae\_buffer \\  & ELBO categorical coefficient & \(\alpha_{c}\) & \(1.0\) & cat\_loss\_coeff \\  & ELBO categorical coefficient (-LW) & \(\alpha_{c}\) & \(0.5\) & cat\_loss\_coeff \\  & ELBO Gaussian coefficient & \(\alpha_{g}\) & \(1.0\) & gauss\_loss\_coeff \\  & ELBO reward coefficient & \(\alpha_{R}\) & \(10\) & rev\_loss\_coeff \\  & ELBO transition coefficient & \(\alpha_{T}\) & \(1000\) & state\_loss\_coeff \\  & ELBO occupancy coefficient & \(\alpha_{o}\) & \(1.0\) & occ\_loss\_coeff \\  & ELBO occupancy coefficient (-LW) & \(\alpha_{o}\) & \(0\) & occ\_loss\_coeff \\  & Gumbel softmax temperature & & \(1\) & gumbel\_temperature \\  & Number of updates per epoch & & \(10,20\) & num\_vae\_updates \\  & Subtask dimension & & \(K=N_{\text{train}}\) & \(10,45\) & vae\_mixture\_num \\  & Subtask dimension (-LW) & \(K\) & \(5\) & vae\_mixture\_num \\  & Gaussian dimension & dim\((z)\) & \(5,10\) & latent\_dim \\  & Dropout rate & \(p_{\text{drop}}\) & \(0.7\) & dropout\_rate \\  & Reward reconstruction objective & & MSE & rev\_pred\_type \\  & State reconstruction objective & & MSE & state\_pred\_type \\ \hline \multirow{6}{*}{Policy} & Algorithm & \multicolumn{1}{c}{PPO} & policy \\  & Optimiser & \multicolumn{1}{c}{Adam} & policy\_optimiser \\  & Learning rate & \(7e-4\) & lr\_policy \\  & Optimizer epsilon & & \(10e-8\) & policy\_eps \\  & PPO update epochs & & \(5\) & ppo\_num\_epochs \\  & Steps per policy update & \(\frac{|\mathcal{B}_{\text{goal}}|}{\text{num\_processes}}\) & \(5000\) & policy\_num\_steps \\  & Number of minibatches & & \(10\) & ppo\_num\_minibatch \\  & PPO clipping parameter & & \(0.1\) & ppo\_clip\_param \\  & GAE \(\lambda\) & & \(0.9\) & policy\_tau \\  & Initial standard deviation & & \(1.0\) & policy\_init\_std \\  & Minimum standard deviation & & \(0.5\) & policy\_min\_std \\  & Maximum standard deviation & & \(1.5\) & policy\_max\_std \\  & Entropy coefficient & & \(1e-3\) & policy\_entropy\_coef \\ \hline \multirow{2}{*}{Virtual training} & Initial virtual ratio & \multicolumn{1}{c}{\(0.0\)} & virtual\_ratio \\  & Virtual ratio increment per step & \(\Delta p_{v}\) & \(5e-8,2.5e-8\) & virtual\_ratio\_increment \\  & ELBO dispersion coefficient & \(\alpha_{d}\) & \(10\) & ext\_loss\_coeff \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Hyperparameters of SDVT and SD.** Hyperparameters of SDVT used for Meta-World ML-10 and ML-45 along with the notations in the manuscript and the argument names in the source code. SD shares the same hyperparameters except without those of virtual training. Different hyperparameters of our lightweight variant are denoted as (-LW).

### Network Architecture

The network architectures comprising our method are described in Table 5. Prior to being input into the encoder or decoder, all state, action, and reward inputs pass through embedding networks. The values of \(K=\text{dim}(y)\) and \(\text{dim}(z)\) differ for the ML-10 and ML-45, as indicated in Table 3.

### Aggregation method for the Success Rate

The main results in the Meta-World paper (Table 1 and Figure 6 of Yu et al. [71]) present the average of the _maximum_ success rate for each task, diverging from the raw scores in their Figure 17 and 18. For instance, if an agent achieves a 90% success rate on "Door-close" in one evaluation during training and scores 10% in all other evaluations at different times, the reported success rate for "Door-close" is 90%. The mean of these maximum scores across tasks is reported as the aggregated success rate.

This unconventional aggregation method does not accurately represent the meta-RL objective, which aims to train a single agent capable of solving multiple tasks. Meta-World calculates success rates for various tasks at distinct time points during training, even though the agent might specialize in different tasks at various stages. As a result, the agent may excel at specific tasks by chance. Consequently, evaluating the agent more frequently could yield higher _maximum_ scores. We report the conventional final performance, which better reflects the meta-RL objective.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Category & Description & Notation & Value (ML-10, 45) & Argument Name \\ \hline \multirow{7}{*}{VAE} & Optimizer & Adam & optimiser\_vae \\  & Learning rate & \(1e-3\) & lr\_vae \\  & Subsample ELBO & \(100\) & vae\_subsample\_elbos \\  & Subsample decodes & \(100\) & vae\_subsample\_decodes \\  & Buffer size (meta-episodes) & \(|\mathcal{B}_{\text{VAE}}|\) & \(1000\) & size\_vae\_buffer \\  & ELBO KL coefficient & \(0.1\) & kl\_weight \\  & ELBO reward coefficient & \(\alpha_{R}\) & \(10\) & rew\_loss\_coeff \\  & ELBO transition coefficient & \(\alpha_{T}\) & \(1000\) & state\_loss\_coeff \\  & Number of updates per epoch & \(10,20\) & num\_vae\_updates \\  & Gaussian dimension & \(\text{dim}(z)\) & \(5,10\) & latent\_dim \\  & Dropout rate (VariBAD) & \(p_{\text{drop}}\) & \(0.0\) & dropout\_rate \\  & Dropout rate (LDM) & \(p_{\text{drop}}\) & \(0.7\) & dropout\_rate \\  & Reward reconstruction objective & MSE & rew\_pred\_type \\  & State reconstruction objective & MSE & state\_pred\_type \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Hyperparameters of VariBAD and LDM.** VariBAD and LDM employ identical hyper-parameters for the general and policy categories of SDVT and SD, as shown in Table 3. The sole distinction lies in the structural variation resulting from the utilization of GMVAE and VAE. We choose the Gaussian dimension that yielded the most favorable outcomes among 5, 10, 15, and 20.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Network & Notation & Architecture & Layers & Activations (last layer) \\ \hline State embedding & & MLP & \([32]\) & (Tanh) \\ Reward embedding & & MLP & \([16]\) & (Tanh) \\ Action embedding & & MLP & \([16]\) & (Tanh) \\ Recurrent encoder & \(q_{\phi_{h}}\) & GRU & \([256]\) & (None) \\ Categorical parameter & \(\omega_{\phi_{W}}\) & MLP & \([512,512,K]\) & ReLU (Softmax) \\ Gaussian parameters & \(\mu_{\phi_{z}},\sigma_{\phi_{z}}^{2}\) & MLP & \([512,512,\text{dim}(z)]\) & ReLU (None, SoftPlus) \\ Gaussian regularization parameters & \(\mu_{\theta_{z}},\sigma_{\theta_{z}}^{2}\) & MLP & \([\text{dim}(z)]\) & (None, SoftPlus) \\ Latent context dispersion & \(p_{\theta_{z}}\) & MLP & \([256,256,256]\) & ReLU (None) \\ Reward decoder & \(p_{\theta_{R}}\) & MLP & \([64,64,32,1]\) & ReLU (None) \\ Transition decoder & \(p_{\theta_{T}}\) & MLP & \([64,64,32,40]\) & ReLU (None) \\ Policy & \(\pi_{\psi}\) & MLP & \([256,256,4]\) & Tanh (Tanh) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Network architecture.** Details of the network architecture composing the GMVAE. The numbers in the Layers column represent the dimension of the hidden layers and the output.

[MISSING_PAGE_EMPTY:19]

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Index. Task & SDVT & SDVT-LW & SD & SD-LW & RL\({}^{2}\) & MAML & PEARL & VariiBAD & LDM \\ \hline

[MISSING_PAGE_POST]

ever-pull & 10.0\(\pm\)9.8 & 16.0

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Index. Task & SDVT & SDVT-LW & SD & SD-LW & RL\({}^{2}\) & MAML & PEARL & VariBAD & LDM \\ \hline

[MISSING_PAGE_POST]

dle-press & 4676\(\pm\)192 & 4722\(\pm\)14 & 4767\(\pm\)38 & **4797\(\pm\)17** & 2276\(\pm\)43 & 3696\(\pm\)522 & 652\(\pm\)524 & 4772\(\pm\)19 & 4737\(\pm\)51 \\ -side & 4601\(\pm\)74 & 4355\(\pm\)183 & 4480\(\pm\)147 & **4789\(\pm\)14** & 2239\(\pm\)86 & 2902\(\pm\)791 & 3036\(\pm\)958 & 4728\(\pm\)22 & 4747\(\pm\)55 \\
21. Handle-pull & 853\(\pm\)523 & 83\(\pm\)30 & **1507\(\pm\)832** & 907\(\pm\)731 & 565\(\pm\)43 & 199\(\pm\)300 & 12\(\pm\)5 & 991\(\pm\)576 & 917\(\pm\)826 \\ -side & & & & & & & & & \\
22. Handle-pull & 1469\(\pm\)153 & 1491\(\pm\)110 & **2260\(\pm\)666** & 1541\(\pm\)100 & 2048\(\pm\)86 & 2001\(\pm\)597 & 29\(\pm\)19 & 1549\(\pm\)57 & 1612\(\pm\)170 \\
23. Lever-pull & 446\(\pm\)84 & 496\(\pm\)74 & 483\(\pm\)39 & **500\(\pm\)106** & 305\(\pm\)21 & 264\(\pm\)60 & 119\(\pm\)38 & 457\(\pm\)32 & 345\(\pm\)27 \\
24. Peg-insert & 767\(\pm\)408 & 757\(\pm\)48 & **1106\(\pm\)186** &

### Learning Curves

In Figures 6 through 9, the mean training and test success rates are represented by solid and dashed lines respectively, with the shaded region indicating the 95% confidence interval. All results are compiled from 8 random seeds. The data corresponding to the final training steps in these plots (250M steps for ML-10 and 400M steps for ML-45) are used to present the main results in Table 1.

Figure 6: **Learning curve on ML-10 – success rate.**

Figure 7: **Learning curve on ML-10 – return.**Figure 8: **Learning curve on ML-45 – success rate.**

Figure 9: **Learning curve on ML-45 – return.**

### Subtask Compositions Learned over Training

Figure 10: **Subtask compositions learned over training.** We visualize the development of subtask compositions for **(a)** SDVT-LW and **(b)** SDVT during training on ML-10. Similar to Figure 4, each column represents the terminal subtask composition (\(y_{H^{+}}\)) averaged across 50 parametric variants. **(a)** As training progresses, subtask compositions of (4) “Door-open” and (12) “Door-close” merge due to shared transition dynamics, which may cause the “Door-close” policy to keep the door open during testing without virtual training, underscoring the crucial role that virtual training can play in these scenarios. **(b)** With occupancy regularization, as the training progresses, the decomposition process prioritizes occupying lower indices over higher ones in the subtask compositions.

### Subtask Compositions of all Seeds

Figure 11: **Subtask compositions of all seeds.** We visualize the subtask compositions of **(a)** SDVT-LW and **(b)** on ML-10 after 250M training steps for all seeds. Decomposition processes differ among random seeds due to varying initialization and sample tasks during meta-training. One subtask can be interpreted as a combination of multiple subtasks and vice versa, depending on the seed and the dimension \(K\). We rarely have a collapse to a single Gaussian as in **(a)** Seed 5, which lowers the average performance. **(b)** With occupancy regularization, we find that the 10th element of the composition is rarely occupied, whereas the first element is occupied by many tasks.

Ablation Results

In order to confirm the source of the gain obtained from the proposed method, we conduct an extensive ablation study based on SDVT and SDVT-LW on ML-10.

### Occupancy Regularization

Figures 3(b) and 12 indicate that our occupancy regularization effectively constrains the use of higher indices as intended. We fix the default choice of \(\alpha_{o}=1.0\) that we found to work well on ML-10. Our default SDVT with \(\alpha_{o}=1.0\) scores a test success rate of 32.8%, which scored the best among \(\alpha_{o}\in\{0.0,1.0,5.0,10.0\}\) that scored 22.8%, 25.5%, and 21.0%, respectively. Also, our default SD with \(\alpha_{o}=1.0\) scores a test success rate of 30.8%, which scored the best among \(\alpha_{o}\in\{0.0,1.0,5.0,10.0\}\) that scored 22.8%, 28.4%, and 11.2%, respectively.

### Dropout and Dispersion

Table 10 demonstrates the significant roles played by both dropout and dispersion in generating extrapolated dynamics during virtual training, thereby enhancing the test success rate. As outlined in Appendix E.1 of the LDM paper [35], virtual training without dropout does not exhibit a significant improvement due to decoder overfitting on the state and action inputs of the training tasks. In fact, virtual training without dropout can even lead to a decrease in test performance.

However, it is important to note that these factors are not the primary contributors to the empirical gains of our method. Dropout is specifically essential for virtual training, but not necessarily for other methods that do not employ virtual training, such as VariBAD and RL2 (as discussed in Appendix E.2 of the LDM paper). We verified this in our ML-10 experiment using VariBAD with dropout. The inclusion of dropout marginally improves the training success rate of vanilla VariBAD from 58.2% to 63.0% and the test success rate from 14.1% to 16.5%. However, the performance enhancement achieved through dropout is not as substantial for VariBAD as it is for SDVT-LW's virtual training.

Figure 12: **Occupancy ablation. We report the learned subtask compositions of SDVT (\(K=10,\alpha_{c}=1.0\)) for different occupancy coefficients analogous to Figure 3(b).**

### Decomposition Distribution

Table 10 emphasizes the significance of hyperparameters such as subtask dimension \(K\) and categorical coefficient \(\alpha_{c}\) in determining subtask compositions. It is crucial to carefully set these hyperparameters based on the number of training tasks and their correlations. In our lightweight (LW) approach, we have selected \(K=5\) and \(\alpha_{c}=0.5\), which effectively distributes subtasks across the ML-10 tasks. When \(\alpha_{c}\) is set to a small value, task classification collapses into a few subtasks, as depicted in Figure 4(c). Conversely, with a large value of \(\alpha_{c}\), the entropy of the composition is maximized, and all tasks exhibit a uniform probability distribution, as illustrated in Figure 4(d). However, it is noteworthy that SDVT-LW outperforms VariBAD and LDM in test success rate, regardless of the specific values chosen for \(K\), such as \(K=3,7,10\). To alleviate the burden of hyperparameter tuning in our LW methods, we introduce the occupancy regularization.

### Number of parameters

Table 11 indicates that SDVT-LW employs 3 and 2 times more parameters compared to VariBAD and LDM, respectively. The encoder of our method has more parameters than VariBAD due to the added categorical layer. The decoder of SDVT-LW includes the parameters of the dispersion layers. However, we find that the model's improvement is not mainly attributed to the increased capacity. Generally, a larger capacity does not necessarily guarantee better generalization.

**VariBAD (hidden \(\times\)2)** While selecting encoder (GRU) and policy hidden sizes, we experimented with 128, 256, and 512, discovering that 256 works best for all task-inference methods (LDM, SD only, and SDVT). Notably, VariBAD (hidden \(\times 2\)) with hidden dimensions of 512 possesses more parameters than SDVT but exhibits an even lower test success rate than the vanilla VariBAD.

**VariBAD (matched)** We add MLP layers with hidden sizes of [1600, 256] into the encoder and [128] into the policy. The decoder's hidden size is increased from [64, 64, 32] to [128, 256, 160] to match the capacities of VariBAD and SDVT-LW. The components of VariBAD possess slightly more parameters than SDVT-LW. LDM employs two VariBAD encoders. While the matched baselines improve, they still lag behind ours.

### Conditioning policy on belief

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Number of Parameters} & \multicolumn{2}{c}{ML-10 Success Rate (\%)} \\ \cline{2-6} Methods & Encoder & Decoder & Policy & Sum & Train & Test \\ \hline SDVT-LW & 1,047,455 & 174,821 & 235,401 & 1,457,677 & 62.1 & **33.4** \\ SD-LW & 1,047,455 & 25,637 & 235,401 & 1,308,493 & **75.5** & 26.2 \\ LDM & 502,580 & 25,577 & 202,249 & 730,406 & 56.7 & 19.8 \\
**LDM (matched)** & 2,144,692 & 175,593 & 267,401 & 2,587,686 & 64.2 & 22.0 \\ VariBAD & 251,290 & 25,577 & 202,249 & 479,116 & 58.2 & 14.1 \\
**VariBAD (hidden \(\times\)2)** & 894,362 & 25,577 & 663,305 & 1,583,244 & 62.4 & 12.0 \\
**VariBAD (matched)** & 1,072,346 & 175,593 & 267,401 & 1,515,340 & 67.6 & 17.2 \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Number of parameters and success rates. We report the number of parameters used by our methods and baselines. We demonstrate that our gain is not from the increased capacity.**

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & SDVT-LW & \begin{tabular}{c} SDVT-LW \\ masked \((\omega_{\phi_{g}})\) \\ \end{tabular} & \begin{tabular}{c} SDVT-LW \\ masked \((\mu_{\phi_{z}},\sigma_{\phi_{z}})\) \\ \end{tabular} & 
\begin{tabular}{c} SDVT-LW \\ masked \((\omega_{\phi_{g}},\mu_{\phi_{z}},\sigma_{\phi_{z}})\) \\ \end{tabular} \\ \hline ML-10 Train & 62.1 & 58.6 & 36.9 & 34.8 \\ ML-10 Test & 33.4 & 33.6 & 25.8 & 24.5 \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Masking contexts. We present the success rates (%) on ML-10, achieved by ablating the contexts fed into the policy, to illustrate the effective utilization of learned contexts by the policy.**We employ VariBAD's stop gradient architecture to avoid the instability that may arise from the concurrent training of policy and ELBO objectives. This may lead to a potential vulnerability where the policy neglects contexts. However, in Meta-World, identical observations could belong to different tasks, such as "Reach" and "Pick-place." Therefore, the agent must rely on the inferred context. To confirm this, we evaluate SDVT by masking either \((\omega_{\phi_{y}})\) or \((\mu_{\phi_{z}},\sigma_{\phi_{z}})\) in the policy input.

Our findings reveal that masking the contexts severely damages the training and test success rates. Especially, masking the Gaussian parameters of \(z\) is more critical than masking the categorical parameter of \(y\). However, this does not suggest that \(y\) is redundant, given that the continuous context \(z\) is trained to include \(y\)'s information. Interestingly, the test success rate, with all contexts masked (i.e., solely relying on the current state \(s_{t}\)), achieves a success rate of 24.5%, surpassing all baseline scores. Although the policy does not directly employ the context learned by the GMVAE, it can still be effectively trained in the imaginary task generated by the learned GMVAE to prepare for tests with unseen compositions of subtasks.

### Smaller number of rollouts

We use \(n_{\text{roll}}=10\), following the original setup of the Meta-World benchmark [69, 71]. Notably, this benchmark utilizes dense rewards, therefore the context converges rather quickly, even within the first rollout (Figure 2b). As a result, the performance across rollouts does not exhibit substantial improvement. To provide further insight, we report the success rates for smaller \(n_{\text{roll}}\) in Table 13.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Training Success Rate (\%)} & \multicolumn{3}{c}{Test Success Rate (\%)} \\ \cline{2-7} Methods & \(n_{\text{roll}}=1\) & \(n_{\text{roll}}=2\) & \(n_{\text{roll}}=10\) & \(n_{\text{roll}}=1\) & \(n_{\text{roll}}=2\) & \(n_{\text{roll}}=10\) \\ \hline SDVT-LW & 61.4 & 62.3 & 62.1 & **32.8** & **32.7** & **33.4** \\ SD-LW & **74.9** & **75.1** & **75.5** & 25.8 & 25.2 & 26.2 \\ VariBAD & 57.0 & 58.4 & 58.2 & 14.5 & 15.3 & 14.1 \\ LDM & 55.5 & 56.0 & 56.7 & 18.4 & 18.5 & 19.8 \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Success rate on ML-10 for smaller rollouts.** We report the success rates at three different rollouts, \(n_{\text{roll}}=1,2,10\). Note that the results reported in our manuscript are for \(n_{\text{roll}}=10\).