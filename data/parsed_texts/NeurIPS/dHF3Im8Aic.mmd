# LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition

 Haoxuan Qu

SUTD

Singapore

haoxuan_qu@mymail.sutd.edu.sg

&Xiaofei Hui

SUTD

Singapore

xiaofei_hui@sutd.edu.sg

&Yujun Cai

Meta

U.S.

yujuncai@meta.com

&Jun Liu

SUTD

Singapore

jun_liu@sutd.edu.sg

Both authors contributed equally to the work.Corresponding Author

###### Abstract

Open-set object recognition aims to identify if an object is from a class that has been encountered during training or not. To perform open-set object recognition accurately, a key challenge is how to reduce the reliance on _spurious-discriminative_ features. In this paper, motivated by that different large models pre-trained through different paradigms can possess very rich while distinct implicit knowledge, we propose a novel framework named **L**arge **M**odel **C**ollaboration (**LMC**) to tackle the above challenge via collaborating different off-the-shelf large models in a training-free manner. Moreover, we also incorporate the proposed framework with several novel designs to effectively extract implicit knowledge from large models. Extensive experiments demonstrate the efficacy of our proposed framework. Code is available here.

## 1 Introduction

Open-set object recognition aims to identify if an object is from a _closed-set class_ that has appeared during training, or an _open-set class_ that has not been encountered in the training set. It is a crucial task since _open-set classes_ always exist in the real world [63], and misidentifying _open-set classes_ into _closed-set ones_ can lead to severe consequences in many practical applications [2], such as autonomous driving [19], robotics system [54], malware classification [18], and medical analysis [15]. Due to its importance, open-set object recognition has received a lot of research attention recently and various methods have been proposed [14; 36; 28; 6; 66; 56; 12].

To perform open-set object recognition well, some existing works [34; 40] have noted that a key challenge is how to reduce the reliance on _spurious-discriminative_ features. Specifically, a _spurious-discriminative_ feature refers to a feature that satisfies the following two requirements: 1) it is discriminative between a closed-set class and other closed-set classes; 2) it is shared between this closed-set class and certain open-set classes. As suggested by previous works [34; 40], while such _spurious-discriminative_ features can facilitate classification within the closed set, in open-set recognition, relying on such features can make closed-set and open-set classes to be easily confusedwith each other, and thus bring negative effects in accurately identifying open-set objects. For example, as shown in Fig. 1, given a closed set of classes including bighorn, tabby cat, and hog, within these closed-set classes, "with horn" as a feature of bighorn can be used to discriminate bighorn from other classes (tabby cat and hog). However, relying on "with horn" to perform open-set object recognition, bighorn can be easily confused with potential open-set classes such as ox and gazelle that are also "with horn". Therefore, it can be difficult to correctly identify ox and gazelle as open-set objects. To avoid open-set objects from being misidentified into closed-set classes, a common type of methods is to simulate additional virtual open-set samples or classes [14, 36, 67, 12, 34]. Specifically, among this type of methods, various external knowledge bases have been used, such as pre-trained GloVe embeddings [67] and extra datasets [12]. Yet, despite the considerable efforts made and various architectures specifically designed for this task, the open-set object recognition task remains challenging. This is because, in the real world, various different sorts of open-set classes can appear [11]. Moreover, given a trained classifier, to further equip it with the ability to identify open-set objects, most existing open-set recognition methods require an extra training process. Especially when the classifier has been deployed, conducting such an extra training process can be inconvenient, as it needs both additional training time and access to training data. Note that the training data can be no longer accessible during deployment (e.g., due to privacy concerns [35, 22]).

Recently, large models containing very rich implicit knowledge [17] have served as powerful external knowledge bases in many areas [13]. For example, ChatGPT [38] possesses rich common-sense knowledge, and has been applied in various natural language processing tasks [42, 20]. DALL-E [47], which holds strong image-generative ability, has been used in different generation tasks [32, 26]. CLIP [45] has been leveraged for visual recognition tasks [72, 41] since it can align relevant image-text pairs. Besides, being capable of aligning relevant image patches, DINO [5] has been applied in areas [55, 1] such as image retrieval.

Inspired by that large models often contain rich knowledge, here we aim to investigate _leveraging such rich knowledge to handle the open-set object recognition task_. However, using large models to perform open-set object recognition is non-trivial due to the following challenges: 1) It can be difficult to directly leverage an existing large model to effectively perform open-set object recognition. Take CLIP as an example, being able to align an image with a closed set of classes and thus finding out the most relevant class in the set, CLIP has been applied in various visual recognition tasks, such as zero-shot and few-shot image classification [45]. However, CLIP cannot handle the open-set object recognition task effectively by itself. This is because, various sorts of _open-set classes_ exist in the real world, and it is difficult to know which class should the testing image be aligned with when using CLIP. 2) Meanwhile, as the knowledge in large models is often implicit [57, 3], it can be challenging to extract the desired knowledge (e.g., in our case, the knowledge that can facilitate open-set object recognition) from large models successfully [57]. To handle these challenges, in this work, with the observation that different large models pre-trained through different paradigms can contain distinct knowledge, we first propose a novel open-set object recognition framework named **L**arge **M**odel **C**ollaboration (**LMC**). As shown in Fig. 2, the proposed LMC performs open-set object recognition in a training-free manner by leveraging rich and distinct knowledge from different off-the-shelf large models complementarily. Moreover, LMC is also incorporated with several novel designs to facilitate extracting implicit knowledge in large models effectively.

Overall, to better perform open-set object recognition, LMC aims to reduce the reliance of this task on _spurious-discriminative_ features. To achieve this, LMC proposes to simulate additional virtual open-set classes that share the _spurious-discriminative_ features through the collaboration of different large models. Specifically, in LMC, motivated by that ChatGPT possesses rich common-sense knowledge, we first adopt ChatGPT to expand the list of closed-set classes with additional virtual

Figure 1: Illustration of a _spurious-discriminative_ feature example. As shown, given “with horn” a _spurious-discriminative_ feature of the closed set, DIAS [34] as a recently proposed open-set recognition method misidentifies ox and gazelle (both with horn) as closed-set classes with high probability.

open-set classes. During this process, to extract implicit knowledge from ChatGPT effectively, we propose to guide ChatGPT with intermediate reasoning, while also enabling ChatGPT to perform self-checking on whether it covers as many _spurious-discriminative_ features as possible. Moreover, inspired by that images can catch different details that are complementary to the text domain [60], in addition to representing classes in the expanded list with their names in the text domain, we propose to further use image information to better represent these classes. Specifically, to facilitate extracting image information from large models, we propose to incorporate our framework with a novel cyclic module, which collaborates ChatGPT, DALL-E, and CLIP through a cross-assessing mechanism to generate diverse images for each class. Once both the text and image information have been extracted for each class in the expanded list, during inference, we propose to use the image-text alignment ability of CLIP and the image-image alignment ability of DINO in collaboration to perform open-set object recognition on testing images.

The contributions of our work are summarized as follows. 1) We propose LMC, a novel framework that can handle the open-set object recognition task in a training-free manner, via collaborating different off-the-shelf pre-trained large models and leveraging their knowledge in a complementary manner. 2) We introduce several designs in LMC to extract both text and image information from large models effectively. 3) On the evaluated benchmarks, LMC achieves state-of-the-art performance.

## 2 Related Work

**Open-set Object Recognition.** Due to the wide range of applications, open-set object recognition has received a lot of research attention [48; 4; 50; 14; 63; 39; 68; 53; 51; 7; 6; 33; 23; 56; 61; 19; 58; 70; 31; 49; 21]. Bendale and Boult [4] made the first attempt of applying deep neural networks into the task of open-set recognition. Specifically, they first showed that simply thresholding on the softmax probability cannot yield a robust open-set recognition, and thus proposed Openmax based on the Extreme Value Theory. Ge et al. [14] further proposed G-Openmax to generate unknown samples using generative models. After that, Neal et al. [36] showed the effectiveness of generating images that do not belong to any of the closed-set classes but look similar to those images in the training dataset. Later on, OpenGAN, which uses real open-set images to perform model selection, was proposed by Kong et al. [23]. Vaze et al. [56] proposed to well-train the closed-set classifier so that it can perform both closed-set image classification and open-set recognition well. Besides, Esmaeilpour et al. [12] proposed to train an image description generator using an extra image captioning dataset and use the trained generator to improve the open-set object recognition performance.

Different from existing approaches, in this work, from a novel perspective, we investigate how to collaborate different large models to perform open-set object recognition in a training-free manner. Besides, we also introduce several designs to extract implicit knowledge from large models effectively.

**Large Models.** Recently, a variety of large models have been proposed, such as ChatGPT [38], DALL-E [47], CLIP [45], and DINO [5]. Such large models contain rich knowledge and have been studied in various tasks [20; 65; 69; 32; 64; 27; 1], such as language translation [20], video generation [32], and image retrieval [1]. In this work, we design a new framework to handle the open-set object recognition task by cross-assessing among large models and leveraging the rich and distinct knowledge from different large models in a complementary manner.

## 3 Proposed Method

Given a testing image from \(Y_{c}\cup Y_{o}\), where \(Y_{c}\) represents closed-set classes in the training set and \(Y_{o}\) represents open-set classes that are not included in the training set, the goal of open-set object recognition is to identify if the image is from a closed-set class or an open-set class. To recognize open-set objects accurately, as suggested by previous works [40; 34], a key challenge is: how to reduce the reliance on _spurious-discriminative_ features. To better tackle this challenge, in this work, inspired by that large models can hold very rich while distinct implicit knowledge due to their different training paradigms, we propose a novel open-set object recognition framework **LMC**. Specifically, as shown in Fig. 2, LMC first extracts the implicit knowledge from different off-the-shelf large models to simulate additional virtual open-set classes that share the _spurious-discriminative_ features. After that, LMC utilizes these virtual open-set classes to make the _spurious-discriminative_ features "less discriminative" during its inference process.

Below, we first describe how LMC extracts implicit knowledge from different large models to effectively simulate virtual open-set classes. After that, we introduce the inference process of LMC.

### Virtual Open-set Class Simulation

In the proposed LMC, to reduce the reliance on _spurious-discriminative_ features, we aim to utilize the rich implicit knowledge of different large models to simulate additional virtual open-set classes. Specifically, motivated by that knowledge from different modalities (e.g., text and image) can contain different details [60], we aim to leverage implicit knowledge of large models in both the text domain and the image domain to facilitate simulation. To achieve this, we first simulate the names of virtual open-set classes and expand the list of closed-set classes with these virtual open-set classes. We then further generate diverse images for each class in the list to describe each class better.

**Simulating names of virtual open-set classes.** Inspired by that ChatGPT holds rich common-sense knowledge, here we aim to simulate names of virtual open-set classes by asking ChatGPT questions. Specifically, to perform such a simulation in high quality, we find that, it is necessary to 1) ensure that ChatGPT can well understand the question, and 2) encourage ChatGPT to cover as many _spurious-discriminative_ features as possible. To achieve these, we introduce the following two designs in our framework.

Inspired by chain-of-thought [59] that large language models can better understand the given instruction when they are guided with intermediate reasoning, to ensure that ChatGPT can well understand the question, we here also guide ChatGPT with intermediate reasoning. Specifically, we find that an effective way to guide ChatGPT is to ask ChatGPT the following three questions for each closed-set class: 1) "Given a list of classes [classes], can you describe the visual features of each class in the list?", where [classes] represents the names of the closed-set classes; 2) "What are the discriminative visual features of class [class] compared with other classes in the list?", where [class] represents the closed-set class that the current set of questions is asked for; 3) "Can you list other classes that also share these discriminative visual features?". In Fig. 3, we illustrate an example when the above three questions are asked for the class ladybug. As shown, by formulating the language command in this way, besides the final answer to question 3), ChatGPT is also demanded to generate intermediate rationales that lead to the final answer through the first two questions. This can help ChatGPT to better understand our final purpose, i.e., simulating virtual open-set classes that share _spurious-discriminative_ features. In Particular, in Fig. 3, tortoise beetle and ladybird spider are the simulated virtual open-set classes that share _spurious-discriminative_ features with the closed-set class ladybug. Through the above three questions, we can simulate the names of virtual open-set classes

Figure 3: Formulation of language command for ChatGPT in a step-by-step manner with intermediate rationales involved.

Figure 2: Illustration of the proposed LMC framework.

that share _spurious-discriminative_ features with a single closed-set class. To simulate the names of virtual open-set classes for all closed-set classes, we can simply ask the above three questions for each class in the closed set either sequentially, or concurrently via different ChatGPT instances.

In addition, to simulate a comprehensive list of virtual open-set classes to better cover the _spurious-discriminative_ features, we aim to further enable ChatGPT to perform self-checking. In other words, we aim to guide ChatGPT to re-think if there are other _spurious-discriminative_ features that are missed. To achieve this, for each closed-set class, after asking the above three questions, we expand the list of classes in question 1) with the obtained virtual open-set classes and re-ask ChatGPT the three questions. Note that by expanding the list with the obtained virtual open-set classes, we exclude the _spurious-discriminative_ features that are already covered and thus force ChatGPT to look for _spurious-discriminative_ features that are not discovered yet. This iteration of self-checking terminates when ChatGPT stops proposing new virtual open-set classes or a maximum number of cycle times is reached. After performing self-checking, inspired by [34] that all sorts of classes can appear in the real world, we also follow [34] to simulate virtual open-set samples that are less similar to closed-set samples (sharing less _spurious-discriminative_ features) via asking ChatGPT: "Given a list of classes [classes], can you name classes that are not similar to them?". Note after expanding the list of closed-set classes with simulated virtual open-set classes through applying the above designs, those _spurious-discriminative_ features that are originally discriminative among the closed-set classes would be "less discriminative" in the expanded list.

**Generating diverse images and cross-assessing.** Above we simulate the names of virtual open-set classes \(Y_{v,o}\). Here, inspired by that images can catch different details that can be complementary to the text domain [60], we aim to describe both the closed-set classes and the simulated virtual open-set classes in more detail by further generating diverse images for each class \(y\in Y_{c}\cup Y_{v,o}\). These generated images will be utilized in the inference process of LMC to reduce its reliance on _spurious-discriminative_ features.

Specifically, for each class \(y\in Y_{c}\cup Y_{v,o}\), we generate diverse images for class \(y\) through the following two steps: 1) we first ask ChatGPT to generate diverse text descriptions as: "Can you write [\(K\)] diverse descriptions, each describing a different scene about the class [\(y\)]?" where \(K\) is a hyperparameter representing the number of descriptions generated per class. 2) After that, leveraging the image-generative ability of DALL-E [47], we ask DALL-E to generate images based on these descriptions for each class as:

\[i_{y}^{k}=\text{DALL-E}(d_{y}^{k}),\textbf{ where }k\in\{1,...,K\}\] (1)

where \(d_{y}^{k}\) denotes the \(k\)-th description generated by ChatGPT for class \(y\), and \(i_{y}^{k}\) denotes the image generated by DALL-E based on \(d_{y}^{k}\). While we can directly use the images generated above during inference, as shown in Fig. 5 which demonstrates a number of such images, we realize that some of such images may not hold a high enough quality to accurately represent their desired classes (e.g., \(i_{y}^{k}\) may not represent class \(y\) accurately). Thus, directly using the above images can lead to suboptimal model performance during inference.

To address this problem, referring the images that cannot accurately represent their desired classes as the _less accurate_ images, we aim to automatically detect these _less accurate_ images, refine their corresponding descriptions, and regenerate them accordingly. To achieve this, inspired by that humans can refine their understanding based on others' feedback, here we want to investigate, _can a large model also refine its output based on another large model's feedback?_ In particular, we propose a cyclic cross-assessing module, where CLIP acts as the "feedback provider", and provides ChatGPT with feedback about the _less accurate_ images to help refine their corresponding descriptions.

Specifically, denote \(i_{y}^{k}\) an image generated for class \(y\), and \(D\) the text descriptions "a photo of [class]" for all the classes (i.e., \(Y_{c}\cup Y_{v,o}\)). The cross-assessing module operates by iteratively conducting the following three steps: **1) Detection.** To detect the _less accurate_ images, we first leverage the strong image-text alignment ability of CLIP to align \(i_{y}^{k}\) with \(D\) as:

\[p_{assess}=softmax\Big{(}\text{CLIP}_{vis}(i_{y}^{k})\big{(}\text{CLIP}_{ text}(D)\big{)}^{T}\Big{)}\] (2)

where CLIP\({}_{vis}\) denotes the CLIP's visual encoder, and CLIP\({}_{text}\) denotes the CLIP's text encoder. The image \(i_{y}^{k}\) is determined as _less accurate_ if compared to class \(y\), it is more aligned towards another class \(u\) (i.e., instead of class \(y\), another class \(u\) corresponds to the highest probability value in \(p_{assess}\)). **2) Refinement.** Next, if \(i_{y}^{k}\) is determined as a _less accurate_ image, in order for its corresponding description \(d_{y}^{k}\) to describe class \(y\) more properly, we ask ChatGPT to refine \(d_{y}^{k}\) based on feedback from CLIP as: "This description [\(d_{y}^{k}\)] seems more like class [\(u\)] to me. Can you refine it to enhance the characteristics of class [\(y\)]?". **3) Regeneration.** Lastly, the image \(i_{y}^{k}\) is regenerated through DALL-E based on the refined text description.

Through conducting the above three steps iteratively, during every iteration, we can leverage CLIP to provide specific feedback for ChatGPT so that ChatGPT can refine the descriptions more toward the direction of our desire. With these refined descriptions, images that can well represent each class will be generated. The above iteration terminates when all the generated images are most aligned with their desired classes or a maximum number of cycle times is reached. Note after reaching the maximum number of cycle times, we discard those images that are still _less accurate_.

### Inference Process

Above we expand the list of closed-set classes with additional virtual open-set classes, and generate diverse images for each class in the expanded list. In this section, we discuss how our framework utilizes the expanded list and the generated images to make the _spurious-discriminative_ features "less discriminative" during its inference process. Note our framework does not require any training process before its inference, which can facilitate its convenient usage in real-world applications.

Specifically, during inference, inspired by the strong image-text alignment ability of CLIP [45] and the strong image-image alignment ability of DINO [5], we propose to adopt both CLIP and DINO to perform open-set object recognition. Note CLIP and DINO are used in collaboration since they can complement each other well. To be more concrete, by aligning the testing image with the names of all the classes in the expanded list (i.e., \(Y_{c}\cup Y_{v,o}\)), we first adopt CLIP to match the testing image with the overall concept of each class. Meanwhile, by leveraging DINO to align the testing image with the generated images of each class in image patch level, we better match the testing image with the detailed local features of each class.

Before entering the inference process, with the notice that certain CLIP and DINO features are used repeatedly across different testing images, we first pre-store these features to keep the efficiency of the inference process. Specifically, denoting \(f_{text}^{\text{CLIP}}=\text{CLIP}_{text}(D)\) the CLIP text feature of all classes, we first store \(f_{text}^{\text{CLIP}}\), which has already been computed in the above Eq. 2. Moreover, denoting DINO\({}_{vis}\) the DINO's encoder and \(I_{y}\) the set of images generated from class \(y\) through the cyclic cross-assessing module, we compute the DINO visual feature for class \(y\) as \(f_{y}^{\text{DINO}}=\text{DINO}_{vis}(I_{y})\) and store \(f_{y}^{\text{DINO}}\) for each class \(y\in Y_{c}\cup Y_{v,o}\).

After pre-storing these features, we then introduce the inference process of the proposed LMC framework. Specifically, given a testing image \(i_{test}\), utilizing pre-stored features, LMC first aligns \(i_{test}\) with the names of all the classes in \(Y_{c}\cup Y_{v,o}\) through CLIP as:

\[p_{\text{CLIP}}=softmax\big{(}\text{CLIP}_{vis}(i_{test})(f_{text}^{\text{CLIP }})^{T}\big{)}\] (3)

where \(p_{\text{CLIP}}\) denotes the softmax probability derived from CLIP. At the same time, LMC also aligns \(i_{test}\) with the generated images through DINO as:

\[p_{\text{DINO}}=softmax\big{(}\{l_{\text{DINO}}^{y}\mid y\in Y_{c}\cup Y_{v,o }\}\big{)},\ \textbf{where}\ l_{\text{DINO}}^{y}=average\big{(}\text{DINO}_{vis}(i_{test})(f_ {y}^{\text{DINO}})^{T}\big{)}\] (4)

where \(l_{\text{DINO}}^{y}\) is derived from aligning \(i_{test}\) with \(I_{y}\) through DINO, and \(p_{\text{DINO}}\) denotes the softmax probability correspondingly calculated for all the classes in \(Y_{c}\cup Y_{v,o}\). After deriving \(p_{\text{CLIP}}\) and \(p_{\text{DINO}}\)-LMC then incorporates them to determine whether \(i_{test}\) is from the closed-set classes or open-set classes as:

\[p_{inc}=\alpha p_{\text{CLIP}}+(1-\alpha)p_{\text{DINO}},\ \ \ \ S=\max_{y\in Y_{c}} \big{(}p_{inc}(y|i_{test})\big{)}\] (5)

where \(\alpha\) is a hyperparameter denoting the incorporation weight, \(p_{inc}\) denotes the softmax probability derived from incorporating \(p_{\text{CLIP}}\) and \(p_{\text{DINO}}\), \(p_{inc}(y|i_{test})\) denotes the probability value that class \(y\) corresponds to in \(p_{inc}\), and \(S\in[0,1]\) is the closed-set score measured by our framework. Following [34], we represent \(S\) as the maximum probability value among the closed-set classes. By performing open-set object recognition in the above way, open-set images would no longer be easily misidentified into the closed-set classes due to the contained _spurious-discriminative_ features. This is because, after expanding the list of closed-set classes with the virtual open-set classes, in the expanded list, besides a certain closed-set class, a _spurious-discriminative_ feature can be held by some virtual open-set classes as well. Then, when an open-set testing image containing _spurious-discriminative_ features is aligned with the expanded list, it will tend to approach both the closed-set and virtual open-set classes that contain these features at the same time, rather than approaching only certain closed-set classes. Therefore, after softmax normalization, the closed-set score \(S\) of the open-set testing image will no longer be easily overestimated due to the _spurious-discriminative_ features contained in the image.

## 4 Experiments

To evaluate the effectiveness of our proposed framework LMC, we conduct experiments on four evaluation protocols including CIFAR10, CIFAR+10, CIFAR+50, and TinyImageNet. We conduct our experiments on an RTX 3090 GPU.

### Datasets and Evaluation Metric

**CIFAR10.** CIFAR10 [24] is a ten-class object recognition dataset. On this dataset, following [6; 56], 6 closed-set classes and 4 open-set classes are randomly sampled for evaluation each time.

**CIFAR+10 & CIFAR+50.** Following [6; 56], we also evaluate our method on CIFAR+10 and CIFAR+50, which are extensions of the CIFAR10 evaluation protocol. Specifically, following [6; 56], for the CIFAR+\(N\) experiments, 4 classes are randomly sampled from CIFAR-10 as the closed-set classes and \(N\) non-overlapping classes are randomly sampled from CIFAR-100 [24] as the open-set classes for evaluation (\(N=10\) for CIFAR+10 and \(N=50\) for CIFAR+50).

**TinyImageNet.** TinyImageNet [9] is a subset of ImageNet and contains 200 classes. Following [6; 56], each time, 20 closed-set classes and 180 open-set classes are randomly sampled.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Method & Venue & CIFAR10 & CIFAR+10 & CIFAR+50 & TinyImageNet \\ \hline \multicolumn{5}{c}{Methods that involve a training process} \\ \hline OSRCI [36] & ECCV 2018 & 69.9 \(\pm\) 3.8 & 83.8 \(\pm\) - & 82.7 \(\pm\) - & 58.6 \(\pm\) - \\ C2AE [39] & CVPR 2019 & 89.5 \(\pm\) - & 95.5 \(\pm\) - & 93.7 \(\pm\) - & 74.8 \(\pm\) - \\ RPL [7] & ECCV 2020 & 90.1 \(\pm\) - & 97.6 \(\pm\) - & 96.8 \(\pm\) - & 80.9 \(\pm\) - \\ OpenHybrid [68] & ECCV 2020 & 95.0 \(\pm\) - & 96.2 \(\pm\) - & 95.5 \(\pm\) - & 79.3 \(\pm\) - \\ CVAECapOSR [16] & ICCV 2021 & 83.5 \(\pm\) 2.3 & 88.8 \(\pm\) 1.9 & 88.9 \(\pm\) 1.7 & 71.5 \(\pm\) 1.8 \\ ARPL [6] & TPAMI 2021 & 90.1 \(\pm\) 0.5 & 96.5 \(\pm\) 0.6 & 94.3 \(\pm\) 0.4 & 76.2 \(\pm\) 0.5 \\ ARPL+C5 [6] & TPAMI 2021 & 91.0 \(\pm\) 0.7 & 97.1 \(\pm\) 0.3 & 95.1 \(\pm\) 0.2 & 78.2 \(\pm\) 1.3 \\ PMAL [31] & AAAI 2022 & 95.1 \(\pm\) - & 97.8 \(\pm\) - & 96.9 \(\pm\) - & 83.1 \(\pm\) - \\ ZOC [12] & AAAI 2022 & 93.0 \(\pm\) 1.7 & 97.8 \(\pm\) 0.6 & 97.6 \(\pm\) 0.0 & 84.6 \(\pm\) 1.0 \\ MLS [56] & ICLR 2022 & 93.6 \(\pm\) - & 97.9 \(\pm\) - & 96.5 \(\pm\) - & 83.0 \(\pm\) - \\ DIAS [34] & ECCV 2022 & 85.0 \(\pm\) 2.2 & 92.0 \(\pm\) 1.1 & 91.6 \(\pm\) 0.7 & 73.1 \(\pm\) 1.5 \\ Class-inclusion [8] & ECCV 2022 & 94.8 \(\pm\) - & 96.1 \(\pm\) - & 95.7 \(\pm\) - & 78.5 \(\pm\) - \\ ODL [30] & TPAMI 2022 & 85.7 \(\pm\) 1.3 & 89.1 \(\pm\) 1.4 & 88.3 \(\pm\) 0.0 & 76.4 \(\pm\) 1.7 \\ ODL+ [30] & TPAMI 2022 & 88.5 \(\pm\) 1.3 & 91.1 \(\pm\) 0.8 & 90.6 \(\pm\) 0.0 & 74.6 \(\pm\) 0.8 \\ CSSR [19] & TPAMI 2022 & 91.3 \(\pm\) - & 96.3 \(\pm\) - & 96.2 \(\pm\) - & 82.3 \(\pm\) - \\ RecSSR [19] & TPAMI 2022 & 91.5 \(\pm\) - & 96.0 \(\pm\) - & 96.3 \(\pm\) - & 81.9 \(\pm\) - \\ \hline \multicolumn{5}{c}{Methods that involve no extra training process} \\ \hline Softmax & & 93.7 \(\pm\) 1.7 & 96.5 \(\pm\) 0.6 & 95.1 \(\pm\) 1.3 & 83.5 \(\pm\) 2.7 \\ Ours & & **96.6 \(\pm\)** 0.3 & **98.9 \(\pm\)** 0.7 & **98.5 \(\pm\)** 0.4 & **86.7 \(\pm\)** 1.4 \\ \hline \end{tabular}
\end{table}
Table 1: The AUROC results on the detection of closed-set and open-set samples. Results are averaged over five random dataset splits following [6; 56].

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Method & Venue & CIFAR10 & CIFAR+10 & CIFAR+50 & TinyImageNet \\ \hline \multicolumn{5}{c}{Methods that involve a training process} \\ \hline GCPL [62] & CVPR 2018 & 84.3 \(\pm\) 1.7 & 91.0 \(\pm\) 1.7 & 88.3 \(\pm\) 1.1 & 59.3 \(\pm\) 5.3 \\ RPL [7] & ECCV 2020 & 85.2 \(\pm\) 1.4 & 91.8 \(\pm\) 1.2 & 89.6 \(\pm\) 0.9 & 53.2 \(\pm\) 4.6 \\ ARPL [6] & TPAMI 2021 & 86.6 \(\pm\) 1.4 & 93.5 \(\pm\) 0.8 & 91.6 \(\pm\) 0.4 & 62.3 \(\pm\) 3.3 \\ ARPL+CS [6] & TPAMI 2021 & 87.9 \(\pm\) 1.5 & 94.7 \(\pm\) 0.7 & 92.9 \(\pm\) 0.3 & 65.9 \(\pm\) 3.8 \\ ODL [30] & TPAMI 2022 & 84.8 \(\pm\) 1.4 & 92.5 \(\pm\) 1.0 & 89.8 \(\pm\) 0.7 & 64.3 \(\pm\) 3.2 \\ ODL+ [30] & TPAMI 2022 & 86.9 \(\pm\) 1.5 & 93.2 \(\pm\) 0.3 & 90.3 \(\pm\) 0.2 & 59.2 \(\pm\) 2.1 \\ \hline \multicolumn{5}{c}{Methods that involve no extra training process} \\ \hline Softmax & & 90.1 \(\pm\) 1.1 & 94.0 \(\pm\) 1.4 & 92.8 \(\pm\) 1.1 & 75.1 \(\pm\) 5.0 \\ Ours & & **93.6 \(\pm\)** 1.5 & **96.8 \(\pm\)** 0.7 & **96.4 \(\pm\)** 0.4 & **80.6 \(\pm\)** 3.4 \\ \hline \end{tabular}
\end{table}
Table 2: The OSCR results for open-set object recognition. Results are averaged over five random dataset splits following [6].

**Evaluation metric.** Following [6; 56], we use the following two metrics: AUROC and OSCR [10]. Among these two metrics, AUROC focuses on evaluating model performance on identifying open-set samples, and OSCR measures the trade-off between classification accuracy and open-set recognition performance. More details especially about the OSCR metric are in the supplementary.

### Implementation Details

In LMC, we integrate rich and distinct knowledge from off-the-shelf large models including ChatGPT, DALL-E, CLIP, and DINO in a complementary manner. Specifically, for ChatGPT, we use GPT-3.5-turbo. For CLIP, we use ViT-B/32 for its visual encoder, and the transformer architecture described in [46] for its text encoder. For DINO, we use the second version of DINO [37] with ViT-B/14. We set the maximum number of cycle times for ChatGPT's self-checking to 3. To generate diverse images for each class through the proposed cyclic cross-assessing module, we set the number of diverse detailed descriptions \(K\) generated for each class to 10, and the maximum number of cycle times for cyclic cross-assessing to 3. Besides, we set \(\alpha\) in incorporating \(p_{\text{CLIP}}\) and \(p_{\text{DINO}}\) to 0.6.

### Experimental Results

We report the AUROC results and the OSCR results respectively in Tab. 1 and Tab. 2. Specifically, besides comparing with existing open-set object recognition methods that assume no access to open-set classes, in these two tables, we also compare our framework with Softmax as a baseline that collaborates large models (CLIP and DINO) and involves no extra training process. Particularly, Softmax does not simulate any virtual open-set classes. It treats the ensemble of CLIP and DINO as a closed-set classifier and computes the maximum probability value of the classifier's output as its score \(S\) in the same way as Eq. 3 to Eq. 5 (more details about this baseline are in the supplementary). As shown, compared to state-of-the-art open-set recognition methods and the baseline Softmax, our framework achieves superior performance across different evaluation protocols, demonstrating the effectiveness of our framework.

### Ablation Studies

In this section, we conduct extensive ablation experiments on the TinyImageNet evaluation protocol. In particular, in the experiments, we report the AUROC metric averaged over five dataset splits. **More ablation studies such as experiments w.r.t. hyperparameters are in the supplementary.**

**Evaluation on the two alignments performed by LMC.** In LMC, as shown in Fig. 2, during inference, we align the testing image with the images representing each class through DINO, as well as align the testing image with the name of each class through CLIP. Above, we already evaluate our framework as a whole. Here, we further assess the following two variants of our framework, i.e., the variant (**w/o aligning with names**) that only aligns the testing image with the images representing each class, and the variant (**w/o aligning with images**) that only aligns the testing image with the name of each class. As shown in Tab. 3, ignoring either alignment would lead to performance drop compared to our framework. This demonstrates the effectiveness of our framework in concurrently aligning the testing image with the names and the generated images.

**Evaluation on the two designs for class name simulation.** To simulate names of virtual open-set classes in high quality, in our framework, we introduce two designs to formulate language commands for ChatGPT properly, i.e., guiding ChatGPT with intermediate reasoning and enabling ChatGPT to perform self-checking. Here, to assess the effectiveness of the two designs introduced, we test three variants: **1) w/o intermediate reasoning & self-checking:** this variant uses neither of the two designs and simulates names of virtual open-set classes for each closed-set class via directly asking ChatGPT the following question: "Given a list of classes [classes], can you name other classes that share visual features with [class] while these shared features are discriminative in the list?", where [class] represents a closed-set class and [classes]

\begin{table}
\begin{tabular}{l|c} \hline Method & AUROC \\ \hline w/o intermediate reasoning \& self-checking & 84.1 \\ w/o intermediate reasoning & 85.6 \\ w/o self-checking & 85.9 \\ \hline LMC & 86.7 \\ \hline \end{tabular}
\end{table}
Table 4: Evaluation on the two designs for class name simulation.

\begin{table}
\begin{tabular}{l|c} \hline Method & AUROC \\ \hline w/o aligning with names & 84.4 \\ w/o aligning with images & 82.1 \\ \hline LMC & 86.7 \\ \hline \end{tabular}
\end{table}
Table 3: Evaluation on the two alignments performed by LMC.

represents the list of closed-set classes. **2) w/o intermediate reasoning:** this variant enables ChatGPT to perform self-checking but does not guide ChatGPT with intermediate reasoning. Specifically, in this variant, after asking ChatGPT the question in variant **1)**, we expand [classes] in this question with the obtained virtual open-set classes and re-ask ChatGPT this question iteratively until ChatGPT stops proposing new virtual open-set classes or a maximum number of cycle times is reached. As with our framework, we set the maximum number of cycle times to 3 here as well. **3) w/o self-checking:** this variant guides ChatGPT with intermediate reasoning but does not perform self-checking. Specifically, in this variant, we only ask the three questions proposed in Sec. 3.1 once but not in an iterative manner. As shown in Tab. 4, our proposed framework consistently outperforms all three variants. This demonstrates (1) the effectiveness of the intermediate reasoning design, which can lead ChatGPT to better understand the asked question; (2) the effectiveness of the self-checking design, which can lead ChatGPT to simulate a more comprehensive list of virtual open-set classes.

**Impact of the cyclic cross-assessing module.** To ensure that the images generated by DALL-E can well represent their desired classes, in our framework, we propose a cyclic cross-assessing module to refine the generated images iteratively. To validate this module, we consider three alternative ways to generate images: **1) w/o cross-assessing**: this variant directly passes the generated images to the inference process of our framework without checking or refining them. **2) Check and discard**: this variant checks the generated images through CLIP and passes only those images that pass the checking (i.e., those images that are most aligned towards their desired classes) to the inference process of our framework. Note in this variant, those images that do not pass the checking are simply discarded and no refinement is involved. **3) Check and naively refine**: this variant both checks and refines the generated images. However, in this variant, we directly ask ChatGPT to refine the descriptions of those incorrect images, and the feedback from CLIP (i.e., a target refinement direction) is not involved in the refinement. As shown in Tab. 5, the performance of all three variants are worse than our method. This shows the effectiveness of our proposed cyclic cross-assessing module, which can leverage CLIP to provide specific feedback for ChatGPT, so that ChatGPT can refine the descriptions more toward the direction of our desire.

**Overlap between the virtual open-set classes and the open-set classes used for evaluation.** In our method, we simulate virtual open-set classes to lead the framework to less rely on _spurious-discriminative_ features. Here, we aim to verify that the effectiveness of our framework does not come from just matching the open-set classes used for evaluation. To achieve this, we evaluate it on two subsets of the open-set classes used for evaluation. Specifically, the first subset (**subset A**) includes all those open-set classes that are contained in the list of simulated virtual open-set classes \(Y_{v,o}\), and the second subset (**subset B**) includes all the other open-set classes that are not contained in \(\overline{Y}_{v,o}\). As shown in Tab. 6, compared to Softmax as a baseline, even for subset B, there is still a significant performance improvement, demonstrating the effectiveness of our method even when \(Y_{v,o}\) and \(Y_{o}\) do not overlap at all. Moreover, we would like to point out that the virtual open-set classes and the open-set classes

\begin{table}
\begin{tabular}{l|c|c} \hline Method & AUROC \\ \hline  & Subset A & Subset B \\ \hline Baseline(Softmax) & 83.5 & 83.6 \\ \hline LMC & 93.1 & 86.5 \\ \hline \end{tabular}
\end{table}
Table 6: Analysis on the overlap between the virtual open-set classes and the open-set classes used for evaluation.

Figure 4: Qualitative results on the TinyImageNet dataset. As shown, even when the testing image holds the _spurious-discriminative_ feature (“with beak” here), our framework can accurately identify if it is from closed-set or open-set classes. The above results are based on the fourth dataset split following [6]. More qualitative results are in the supplementary.

\begin{table}
\begin{tabular}{l|c} \hline Method & AUROC \\ \hline w/o cross-assessing & 84.5 \\ Check and discard & 84.9 \\ Check and naively refine & 85.4 \\ \hline LMC & 86.7 \\ \hline \end{tabular}
\end{table}
Table 5: Evaluation on the effectiveness of the cyclic cross-assessing module.

used for evaluation barely overlap. For example, for each data split of TinyImageNet, out of its 180 open-set classes used for evaluation, only no more than 6 classes (i.e., no more than \(3.4\%\) of classes) are within the list of virtual open-set classes. Yet, our method still achieves state-of-the-art performance. This further demonstrates that the effectiveness of our method does not naively come from matching the real open-set classes.

**Qualitative results.** Some qualitative results are shown in Fig. 4. As shown, our framework can perform open-set object recognition accurately even when the testing images contain _spurious-discriminative_ features. This demonstrates that our framework can effectively reduce the reliance on _spurious-discriminative_ features when performing open-set object recognition.

## 5 Conclusion

In this paper, we have proposed a novel open-set object recognition framework LMC, which collaborates different large models to recognize open-set objects. Specifically, in LMC, we utilize rich and distinct knowledge from different off-the-shelf large models to reduce the reliance of the proposed framework on _spurious-discriminative_ features. Moreover, we also incorporate our framework with several designs to effectively extract implicit knowledge from large models. Our framework achieves state-of-the-art performance on four widely used evaluation protocols in a training-free manner. Besides, our method may potentially also (1) be applied in other related tasks such as confidence estimation [44, 43] and uncertainty estimation [25, 29]; (2) inspire new ways for follow-up research. Below, we list a few possible new ways that follow-up research work can further investigate: (1) how to collaborate large models with conventional open-set object recognition methods to further improve performance; (2) how to facilitate the usage of black-box large models in our framework through black-box optimization algorithms such as the CMA Evolution Strategy [52]. We leave these as our future works.

## Acknowledgments and Disclosure of Funding

This work was supported by the National Research Foundation Singapore under the AI Singapore Programme (Award Number: AISG-100E-2023-121).

## References

* [1] Jon Almazan, Byungsoo Ko, Geonmo Gu, Diane Larlus, and Yannis Kalantidis. Granularity-aware adaptation for image retrieval over multiple tasks. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV_, pages 389-406. Springer, 2022.
* [2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. _arXiv preprint arXiv:1606.06565_, 2016.
* [3] Trevor Ashby, Braden K Webb, Gregory Knapp, Jackson Searle, and Nancy Fulda. Personalized quest and dialogue generation in role-playing games: A knowledge graph-and language model-based approach. In _Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems_, pages 1-20, 2023.

Figure 5: Visualization of the images generated before and after passing through the cross-assessing module. More visualizations are in the supplementary.

* [4] Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1563-1572, 2016.
* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.
* [6] Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):8065-8081, 2021.
* [7] Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun Huang, Shiliang Pu, and Yonghong Tian. Learning open set network with discriminative reciprocal points. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 507-522. Springer, 2020.
* [8] Wonwoo Cho and Jaegul Choo. Towards accurate open-set recognition via background-class regularization. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXV_, pages 658-674. Springer, 2022.
* [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [10] Akshay Raj Dhamija, Manuel Gunther, and Terrance Boult. Reducing network agnostophobia. _Advances in Neural Information Processing Systems_, 31, 2018.
* [11] Xuefeng Du, Xin Wang, Gabriel Gozum, and Yixuan Li. Unknown-aware object detection: Learning what you don't know from videos in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13678-13688, 2022.
* [12] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(6):6568-6576, Jun. 2022.
* [13] Lin Geng Foo, Hossein Rahmani, and Jun Liu. Ai-generated content (aigc) for various data modalities: A survey. _arXiv preprint arXiv:2308.14177_, 2023.
* [14] Zongyuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi. Generative openmax for multi-class open set classification. In _British Machine Vision Conference_. BMVA Press, 2017.
* [15] Zongyuan Ge and Xin Wang. Evaluation of various open-set medical imaging tasks with deep neural networks. _arXiv preprint arXiv:2110.10888_, 2021.
* [16] Yunrui Guo, Guglielmo Camporese, Wenjing Yang, Alessandro Sperduti, and Lamberto Ballan. Conditional variational capsule network for open set recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 103-111, 2021.
* [17] Ziyu Guo, Yiwen Tang, Renrui Zhang, Dong Wang, Zhigang Wang, Bin Zhao, and Xuelong Li. Viewrefer: Grasp the multi-view knowledge for 3d visual grounding with gpt and prototype guidance. _arXiv preprint arXiv:2303.16894_, 2023.
* [18] Mehadi Hassen and Philip K Chan. Learning a neural-network-based representation for open set recognition. In _Proceedings of the 2020 SIAM International Conference on Data Mining_, pages 154-162. SIAM, 2020.
* [19] Hongzhi Huang, Yu Wang, Qinghua Hu, and Ming-Ming Cheng. Class-specific semantic reconstruction for open set recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [20] Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good translator? a preliminary study. _arXiv preprint arXiv:2301.08745_, 2023.
* [21] CEN Jun, Di Luan, Shiwei Zhang, Yixuan Pei, Yingya Zhang, Deli Zhao, Shaojie Shen, and Qifeng Chen. The devil is in the wrongly-classified samples: Towards unified open-set recognition. In _The Eleventh International Conference on Learning Representations_, 2022.
* [22] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard. C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation. _arXiv preprint arXiv:2303.17132_, 2023.

* [23] Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 813-822, 2021.
* [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [26] Kyle Lennon, Katharina Fransen, Alexander O'Brien, Yumeng Cao, Matthew Beveridge, Yamin Arefeen, Nikhil Singh, and Iddo Drori. Image2lego: customized lego set generation from images. _arXiv preprint arXiv:2108.08477_, 2021.
* [27] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, and Xiang Bai. Crowdclip: Unsupervised crowd counting via vision-language model. _arXiv preprint arXiv:2304.04231_, 2023.
* [28] Ting-En Lin and Hua Xu. A post-processing method for detecting unknown intent of dialogue system via pre-trained deep neural network classifier. _Knowledge-Based Systems_, 186:104979, 2019.
* [29] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. _Advances in Neural Information Processing Systems_, 33:7498-7512, 2020.
* [30] Zhun-ga Liu, Yi-min Fu, Quan Pan, and Zuo-wei Zhang. Orientational distribution learning with hierarchical spatial attention for open set recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [31] Jing Lu, Yunlu Xu, Hao Li, Zhanzhan Cheng, and Yi Niu. Pmal: Open set recognition via robust prototype mining. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(2):1872-1880, Jun. 2022.
* [32] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. _arXiv e-prints_, pages arXiv-2303, 2023.
* [33] Dimity Miller, Niko Sunderhauf, Michael Milford, and Feras Dayoub. Class anchor clustering: A loss for distance-based open set recognition. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3570-3578, 2021.
* [34] WonJun Moon, Junho Park, Hyun Seok Seong, Cheol-Ho Cho, and Jae-Pil Heo. Difficulty-aware simulator for open set recognition. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXV_, pages 365-381. Springer, 2022.
* [35] Gaurav Kumar Nayak, Ruchit Rawal, and Anirban Chakraborty. Dad: Data-free adversarial defense at test time. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3562-3571, 2022.
* [36] Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 613-628, 2018.
* [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [39] Poojan Oza and Vishal M Patel. C2ae: Class conditioned auto-encoder for open-set recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2307-2316, 2019.

* [40] Pramuditha Perera, Vlad I. Morariu, Rajiv Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez, and Vishal M. Patel. Generative-discriminative feature representations for open-set recognition. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11811-11820, 2020.
* [41] Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, and Yin Cui. Multimodal open-vocabulary video classification via pre-trained vision and language models. _arXiv preprint arXiv:2207.07646_, 2022.
* [42] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? _arXiv preprint arXiv:2302.06476_, 2023.
* [43] Haoxuan Qu, Lin Geng Foo, Yanchao Li, and Jun Liu. Towards more reliable confidence estimation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [44] Haoxuan Qu, Yanchao Li, Lin Geng Foo, Jason Kuen, Jiuxiang Gu, and Jun Liu. Improving the reliability for confidence estimation. In _European Conference on Computer Vision_, pages 391-408. Springer, 2022.
* [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [48] Walter J. Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E. Boult. Toward open set recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(7):1757-1772, 2013.
* [49] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier detection. In _International Conference on Learning Representations_, 2020.
* [50] Lei Shu, Hu Xu, and Bing Liu. Doc: Deep open classification of text documents. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 2911-2916, 2017.
* [51] Yu Shu, Yemin Shi, Yaowei Wang, Tiejun Huang, and Yonghong Tian. P-odn: Prototype-based open deep network for open set recognition. _Scientific reports_, 10(1):7146, 2020.
* [52] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a-service. In _International Conference on Machine Learning_, pages 20841-20855. PMLR, 2022.
* [53] Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, and Guohao Peng. Conditional gaussian distribution learning for open set recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13480-13489, 2020.
* [54] Niko Sunderhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, Jurgen Leitner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. The limits and potentials of deep learning for robotics. _The International journal of robotics research_, 37(4-5):405-420, 2018.
* [55] Tuan Truong, Sadegh Mohammadi, and Matthias Lenga. How transferable are self-supervised features in medical image classification tasks? In _Machine Learning for Health_, pages 54-74. PMLR, 2021.
* [56] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: a good closed-set classifier is all you need? In _International Conference on Learning Representations_, 2022.
* [57] Fei-Yue Wang, Qinghai Miao, Xuan Li, Xingxia Wang, and Yilun Lin. What does chatgpt say: The dao from algorithmic intelligence to linguistic intelligence. _IEEE/CAA Journal of Automatica Sinica_, 10(3):575-579, 2023.

* [58] Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. OpenAUC: Towards AUC-oriented open-set recognition. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [60] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9653-9663, 2022.
* [61] Albert Xu, Xiang Ren, and Robin Jia. Conal: Anticipating outliers with large language models. _arXiv preprint arXiv:2211.15718_, 2022.
* [62] Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Robust classification with convolutional prototype learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3474-3482, 2018.
* [63] Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura. Classification-reconstruction learning for open-set recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4016-4025, 2019.
* [64] Giacomo Zara, Subhankar Roy, Paolo Rota, and Elisa Ricci. Autolabel: Clip-based framework for open-set video domain adaptation. _arXiv preprint arXiv:2304.01110_, 2023.
* [65] Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In _The Eleventh International Conference on Learning Representations_, 2023.
* [66] Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu. Modeling discriminative representations for out-of-domain detection with supervised contrastive learning. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pages 870-878, 2021.
* [67] Zhiyuan Zeng, Hong Xu, Keqing He, Yuanmeng Yan, Sihong Liu, Zijun Liu, and Weiran Xu. Adversarial generative distance-based classifier for robust out-of-domain detection. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7658-7662. IEEE, 2021.
* [68] Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 102-117. Springer, 2020.
* [69] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. _arXiv preprint arXiv:2303.02151_, 2023.
* [70] Xuelian Zhang, Xuelian Cheng, Donghao Zhang, Paul Bonnington, and Zongyuan Ge. Learning network architecture for open-set recognition. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(3):3362-3370, Jun. 2022.
* [71] Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning placeholders for open-set recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2021.
* [72] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16816-16825, 2022.