# Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis

 Diwen Wan\({}^{1}\)   Yuxiang Wang\({}^{1}\)   Ruijie Lu\({}^{1}\)   Gang Zeng\({}^{1}\)

\({}^{1}\)National Key Laboratory of General Artificial Intelligence,

School of IST, Peking University, China

{wan,yuxiang123}@stu.pku.edu.cn  {jason_lu,zeng}@pku.edu.cn

###### Abstract

While novel view synthesis for dynamic scenes has made significant progress, capturing skeleton models of objects and re-posing them remains a challenging task. To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates. Our approach utilizes 3D Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model. Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints. Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects. Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images. Please visit our project page for more results: https://dnvtmf.github.io/SK_GS/.

## 1 Introduction

Novel view synthesis for 3D scenes is important for many domains including virtual/augmented/mixed reality, game or movie productions. In recent years, Neural Radiance Fields (NeRF) [1] have witnessed significant advances in both static and dynamic scenes. Among them, 3D Gaussian Splatting (3D-GS) [2] proposed a novel point-based representation, and is capable of real-time rendering while ensuring the quality of generated images, bringing new insights to more complex task scenarios.

Although visually compelling results and fast rendering speed have been achieved in reconstructing a dynamic scene, current methods mainly focus on replaying the motion in the video, which means it just renders novel view images within the given time range, making it hard to explicitly repose or control the movement of individual objects in the scene. For some specific categories such as the human body or human head, one main approach is to leverage the category-specific prior knowledge such as templates to support the manipulation of reconstructed objects. However, it is hard for these methods to generalize to large-scale in-the-wild scenes or human-made articulated objects.

Some template-free methods attempt to address these challenges by building reposable models from videos. Watch-It-Move (WIM) [3] leverages ellipsoids, an explicit representation, to coarsely model 3D objects, and then estimate the residual by a neural network. The underlying intuition is that one or more ellipsoids can represent a functional part. By observing the motion of parts from multi-view videos, WIM can learn both the appearance and structure of articulated objects. However, the reconstruction results of WIM are of low visual quality and the training and rendering speed is slow. Apart from WIM, Articulated Point NeRF (AP-NeRF) [4] samples feature point cloud from a pre-trained dynamic NeRF model (TiNeuVox [5]) and initializes the skeleton tree using the medial axis transform algorithm. By combining linear blend skinning (LBS) and point-based rendering [6],AP-NeRF jointly optimizes dynamic NeRF and skeletal model from videos. Compared to WIM, AP-NeRF achieves higher visual fidelity while significantly reducing the training time. However, AP-NeRF cannot achieve real-time rendering, which is still far from practical application.

In this paper, we target class-agnostic novel view synthesis of reposable models without the need for a template or pose annotations, while achieving real-time rendering. To enable fast rendering speed, we opt to represent the 3D object as 3D Gaussian Splatting. To be specific, we first reconstruct the 3D dynamic model using 3D Gaussians and superpoints, where each superpoint binds Gaussians with similar motions together. These superpoints will later be treated as the parts of an object. Afterward, a skeleton model is discovered leveraging some intuitive cues under the guidance of superpoint motions from the video. Finally, we jointly optimize the skeleton model and pose parameters to match the motions of the training videos. During the optimization process of object reconstruction, we will inevitably generate a lot of redundant superpoints to fit the complex motion. To simplify the skeleton model and avoid overfitting, we employ an adaptive control strategy and regularization losses to reduce the number of superpoints. Our contributions can be summarized as follows:

* We propose a novel method based on 3D Gaussians and superpoints for modeling appearance, skeleton model, and motion of articulated dynamic objects from videos. Our approach can automatically discover the skeleton model without any category-specific prior knowledge.
* We effectively learn and control superpoints by employing an adaptive control strategy and regularization losses.
* We demonstrate excellent novel view synthesis quality while achieving real-time rendering on various datasets.

## 2 Related Works

### Static and Dynamic Neural Radiance Fields

In recent years, we have witnessed significant progress in the field of novel view synthesis empowered by Neural Radiance Fields. While vanilla NeRF [1] manages to synthesize photorealistic images for any viewpoint using MLPs, subsequent works have explored various representations such as 4D tensors [7], hash encodings [8], or other well-designed data structures [9; 10] to improve rendering quality and speed. More recently, a novel framework 3D Gaussian Splatting [2] has received widespread attention for its ability to synthesize high-fidelity images for complex scenes in real-time.

Meanwhile, many research works challenge the hypothesis of a static scene in NeRFs and attempt to synthesize novel-view images of a dynamic scene at an arbitrary time from a 2D video, which is a more challenging task since the correspondence between different frames is non-trivial. One line of research works [11; 12; 13] directly represents the dynamic scene with an additional time dimension or a time-dependent interpolation in a latent space. Another line of work [14; 15; 16; 17; 18] represents the dynamic scene as a static canonical 3D scene along with its deformation fields. While one main bottleneck of synthesizing a dynamic scene is speed, some works [19; 20; 21; 22] propose to extend 3D Gaussian Splatting into 4D to mitigate the problem. Though being able to recover a high-fidelity scene, this method cannot directly support editing and reposing objects within it. In this work, we leverage 3D Gaussian Splatting as the representation for faster rendering speed.

### Object Reposing

It's impractical to directly repose the deformation fields of dynamic NeRFs due to the complexity of high-dimension. Therefore, utilizing parametric templates based on object priors to represent deformation is adopted in many research works. The classes of parametric templates range from human faces [23; 24], and bodies [25; 26] to non-human objects like animals [27]. With the help of skeleton-based LBS and 3D or 2D annotations, these parametric templates are capable of representing articulate human heads [28; 29; 30; 31; 32; 33] and bodies [34; 35; 36; 37; 38; 39; 40; 41]. Though these template-based reposing methods can synthesize high-fidelity images, they are restricted to certain object classes and mainly deal with rigid motions, not to mention the time-consuming process of annotations.

To alleviate the excessive reliance on domain-specific skeletal models, methods based on retrieval from database [42] or adaptation from a generic graph [43; 44] are adopted. However, these methods are still of relatively low flexibility and diversity. Another line of work attempts to learn a more general template-free object representation by 3D shape recovery [3; 4; 45]. WIM [3] proposes to jointly learn a surface representation and LBS model for articulation without any supervision or prior knowledge of the structure. However, the reposing images are of low visual quality and the required training time is considerably long. AP-NeRF [4] achieves a much faster training speed by leveraging a point-based NeRF representation, but cannot support real-time rendering as well.

## 3 Methods

Our goal is to reconstruct a reposable articulated object with real-time rendering speed from videos. The pipeline of proposed method is illustrated in Fig. 1. We represent the appearance of the articulated object as 3D Gaussians in the canonical space while aggregating 3D Gaussians with similar motion into superpoints, which can be treated as rigid parts. It is noteworthy that we apply a time-variant 6 DoF transformation matrix to model the motion of the object. Based on these superpoints, we leverage several intuitive observations to guide the discovery of the skeleton model, which includes both joints and skeletons. Since the observations hold for most objects, our method does not require a category-specific template or pose annotations. To reduce the redundant superpoint, we propose an adaptive control strategy to densify, prune, and merge superpoints during the training process.

### Preliminaries: 3D Gaussian Splitting

3D Gaussian Splitting (3D-GS) [2] use a set of 3D Gaussians to represent a 3D scene. Each Gaussian \(G_{i}\): \(\{\bm{\mu}_{i},\bm{q}_{i},\bm{s}_{i},\sigma_{i},\bm{h}_{i}\}\) is associated with a position \(\bm{\mu}_{i}\), a rotation matrix \(\mathbf{R}_{i}\) which is parameterized by a quaternion \(\bm{q}_{i}\), a scaling matrix \(\mathbf{S}_{i}\) which is parameterized by a 3D vector \(\bm{s}_{i}\), opacity \(\sigma_{i}\) and spherical harmonics (SH) coefficients \(\bm{h}_{i}\). Therefore, the anisotropic 3D covariance matrix of Gaussian \(G_{i}\) is defined as \(\bm{\Sigma}_{i}=\mathbf{R}_{i}\mathbf{S}_{i}\mathbf{S}_{i}^{\top}\mathbf{R}_{ i}^{\top}\), which is positive semi-definite matrix.

To render images, 3D-GS employs EWA Splatting algorithm [46] to project a 3D Gaussian with center \(\bm{\mu}_{i}\) and covariance \(\bm{\Sigma}_{i}\) to 2D image space, and the projection can be approximated as a 2D Gaussian with center \(\bm{\mu}_{i}^{\prime}\) and covariance \(\bm{\Sigma}_{i}^{\prime}\). Let \(\mathbf{Q}\), \(\mathbf{K}\) be the viewing transformation and projection matrix, \(\bm{\mu}_{i}^{\prime}\) and \(\bm{\Sigma}_{i}^{\prime}\) are computed as

\[\bm{\mu}_{i}^{\prime}=\mathbf{K}(\mathbf{Q}\bm{\mu}_{i})/(\mathbf{Q}\bm{\mu}_ {i})_{z},\quad\bm{\Sigma}_{i}^{\prime}=\mathbf{J}\mathbf{Q}\bm{\Sigma}_{i} \mathbf{Q}^{\top}\mathbf{J}^{\top},\] (1)

Figure 1: The pipeline of proposed approach. Our approach follows a two-stage training strategy. In the first stage (_i.e_. _dynamic_ stage), we learn the 3D Gaussians and superpoints to reconstruct the appearance. Each superpoint is associated with a rigid part, and the adaptive control strategy is used to control the count. After finishing the training of _dynamic_ stage, we can discover the skeleton model based on superpoints. After we finish the second stage (_i.e_., _kinematic_ stage), we can obtain an articulated model based on the kinematic model.

where \(\mathbf{J}\) is the Jacobian of the projective transformation. Therefore, the final opacity of a 3D Gaussian at pixel coordinate \(\bm{x}\) is

\[\alpha_{i}=\sigma_{i}\exp(-\frac{1}{2}(\bm{x}-\bm{\mu}_{i}^{\prime})^{\top}\bm{ \Sigma}_{i}^{{}^{\prime}-1}(\bm{x}-\bm{\mu}_{i}^{\prime}))\] (2)

After sorting Gaussians by depth, the color at \(\bm{x}\) can be computed by volume rendering:

\[I=\sum_{i=1}^{N}(\bm{c}_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\] (3)

where RGB color \(\bm{c}_{i}\) is evaluated by SH with coefficients \(\bm{h}_{i}\) and view direction.

Given multi-view images with known camera poses, 3D-GS optimizes a static 3D scene by minimizing the following loss function:

\[\mathcal{L}_{rgb}=(1-\lambda)\mathcal{L}_{1}(I,I_{gt})+\lambda\mathcal{L}_{ \mathrm{SSIM}}(I,I_{gt})\] (4)

where \(\lambda=0.2\), and \(I_{gt}\) is the ground truth. Besides, 3D-GS is initialized from from random point cloud or SfM sparse point cloud, and an adaptive density adjustment strategy is applied to control the number of Gaussians.

### Dynamic Stage

To reconstruct an articulated object, we build the deformation based on superpoints and the linear blend skinning (LBS) [47], while the canonical model is represented by 3D-GS.

The superpoints \(\mathcal{P}=\{\bm{p}_{j}\in\mathbb{R}^{3}\}_{j=1}^{M}\) are associated with a set of 3D Gaussians, and can be used to represent the object's rigid parts. For timestamp \(t\), we directly use deformable field \(\Phi\) to learn time-variant 6 DoF transformation \([\mathbf{R}_{j}^{t},\bm{o}_{j}^{t}]\in\mathbf{SE}(\mathbf{3})\) of superpoint \(\bm{p}_{j}\) as:

\[\Phi:(\bm{p}_{j},t)\rightarrow(\mathbf{R}_{j}^{t},\bm{o}_{j}^{t}),\] (5)

where \(\mathbf{R}_{j}^{t}\in\mathbf{SO}(\mathbf{3})\) is the local rotation matrix and \(\bm{o}_{j}^{t}\in\mathbb{R}^{3}\) is the translation vector. Then, LBS is employed to derive the motion of each Gaussian by interpolating the transformations for their neighboring superpoints:

\[\bm{\mu}_{i}^{t}=\sum_{j\in\mathcal{N}_{i}}w_{ij}(\mathbf{R}_{j}^{t}\bm{\mu}_{ i}+\bm{o}_{j}^{t}),\quad\bm{q}_{i}^{t}=(\sum_{j\in\mathcal{N}_{i}}w_{ij}\bm{r}_{j} ^{t})\otimes\bm{q}_{i},\] (6)

where \(\bm{r}_{j}^{t}\in\mathbb{R}^{4}\) is the quaternion representation for matrix \(\mathbf{R}_{j}^{t}\), and \(\otimes\) is the production of quaternions. \(\mathcal{N}_{i}\) denotes the \(K\)-nearest superpoints of Gaussian \(G_{i}\). \(w_{ij}\) is the LBS weights between Gaussian \(G_{i}\) and superpoint \(\bm{p}_{j}\), which can be computed as:

\[w_{ij}=\frac{\exp(\mathbf{W}_{ij})}{\sum_{k\in\mathcal{N}_{i}}\exp(\mathbf{W}_ {ik})},\] (7)

where \(\mathbf{W}\in\mathbb{R}^{N\times M}\) is a learnable parameter.

While keeping other attributes (_i.e._, \(\bm{s}_{i},\sigma_{i},\bm{h}_{i}\)) of Gaussians the same as canonical space, we can render the image at timestamp \(t\) following Eq. 3.

### Discovery of Skeleton Model

Treating each superpoint as a rigid part of the articulated object, we can discover the skeleton model (_i.e._, the 3D joints and the connection between joints) based on the motion of superpoints. Similar to WIM [3], there are some observations to help us discover the underlying skeleton. First, if there is a joint between two superpoints \(\bm{p}_{a}\) and \(\bm{p}_{b}\), the position of \(\bm{p}_{a}\) is more likely close to the position of \(\bm{p}_{b}\). Second, when the relative pose between two parts changes, the joint between the two parts is relatively unchanged. Lastly, two connected parts can be merged if they maintain the same relative pose throughout the whole sequence.

Let \(\bm{j}_{ab}\in\mathbb{R}^{3}\) be the position of underlying joint between superpoints \(\bm{p}_{a}\) and \(\bm{p}_{b}\), and \(\mathbf{R}_{ab}^{t}\in\mathbf{SO}(\mathbf{3})\) is the relative rotation matrix between two superpoints at time \(t\). The relative transform between \(\bm{p}_{a}\) and \(\bm{p}_{b}\) can be either represented by the global transform or the rotation of the joint, that is:

\[\begin{bmatrix}\mathbf{R}_{r}^{t}&\bm{t}_{r}^{t}\\ \mathbf{0}&1\end{bmatrix}=\begin{bmatrix}\mathbf{R}_{b}^{t}&\bm{o}_{b}^{t}\\ \mathbf{0}&1\end{bmatrix}^{-1}\begin{bmatrix}\mathbf{R}_{a}^{t}&\bm{o}_{a}^{t} \\ \mathbf{0}&1\end{bmatrix}=\begin{bmatrix}\mathbf{R}_{ab}^{t}&\bm{j}_{ab}-\mathbf{ R}_{ab}^{t}\bm{j}_{ab}\\ \mathbf{0}&1\end{bmatrix}\] (8)where \(\mathbf{R}_{r}^{t}=(\mathbf{R}_{b}^{t})^{-1}\mathbf{R}_{a}^{t}=\mathbf{R}_{ab}^{t} \in\mathbf{SO}(\mathbf{3})\) and \(\mathbf{t}_{r}^{t}\in\mathbb{R}^{3}\) are the relative rotation matrix and translation vector between two superpoints respectively. Considering two joints \(\bm{j}_{ab}\) and \(\bm{j}_{ba}\) between \(\bm{p}_{a}\) and \(\bm{p}_{b}\) should be the same, we compute following distance \(d_{ab}\) for every superpoint pair \((a,b)\):

\[d_{ab}=\sum_{t}\|\bm{t}_{r}-(\bm{j}_{ab}-\mathbf{R}_{ab}^{t}\bm{j}_{ab})\|_{2}^ {2}+\lambda_{d}\|\bm{j}_{ab}-\bm{j}_{ba}\|_{2}^{2}\] (9)

where \(\lambda_{d}=1\) is the hyper-parameter. To prevent the distance from changing too quickly, we smooth it across training iterations:

\[\hat{d}_{ab}(\tau+1)=(1-\epsilon)\cdot\hat{d}_{ab}(\tau)+\epsilon\cdot d_{ab} (\tau),\] (10)

where \(\epsilon=0.1\) is the momentum, and \(\tau\) is the training iteration.

Similar to WIM [3], we discover the structure \(\Gamma\) of joints based on the distance \(\hat{d}_{ab}\) by the minimum spanning tree algorithm. We first select all pairs \((a,b)\) if superpoint \(\bm{p}_{b}\) is \(K^{\prime}\)-nearest neighborhood for superpoint \(\bm{p}_{a}\) and sort the list of \(\hat{d}_{ab}\) for those pairs in ascending order. We initialize \(\Gamma\) as an empty set. We pick pair \((a,b)\) from the lowest distance to the highest distance, and add this pair to \(\Gamma\) while there is no path between \(a\) and \(b\). After finishing the procedure, we obtain the final object structure \(\Gamma\), which is an acyclic graph, _i.e._, a tree. We choose the node whose length of the longest path from itself to any other node is the shortest as the root node. If there is more than one candidate node, we randomly choose one as the root.

### Kinematic Stage

After discovering the skeleton model, we optimize the skeleton model and fine-tune 3D Gaussians by using the kinematic model. Specifically, we first predict time-variant rotations \(\hat{\mathbf{R}}_{k}\in\mathbf{SO}(\mathbf{3})\) for each joint \(\bm{j}_{k}\) by using an deformable field \(\Psi\):

\[\Psi:(\bm{j}_{k},t)\rightarrow\hat{\mathbf{R}}_{k}\] (11)

Then we forward-warp the superpoint \(\bm{p}_{j}\) from the canonical space to the observation space of timestamp \(t\) via the kinematic model. The local transformation matrix \(\hat{\mathbf{T}}_{k}^{t}\in\mathbf{SE}(\mathbf{3})\) of each joint \(k\) is defined by a rotation \(\mathbf{R}_{k}^{t}\) around its parent joint \(\mathbf{j}_{k}\). Consequently, the final transformation of each superpoint \(\bm{p}_{j}\) can be computed as a linear combination of bone transformation:

\[\mathbf{T}_{j}^{t}=\begin{bmatrix}\mathbf{R}_{j}^{t}&\bm{o}_{j}^{t}\\ \mathbf{0}&1\end{bmatrix}=\mathbf{T}_{root}^{t}\prod_{k\in\mathbb{C}_{j}} \hat{\mathbf{T}}_{k}^{t}\text{, where }\hat{\mathbf{T}}_{k}^{t}=\begin{bmatrix}\hat{ \mathbf{R}}_{k}^{t}&\bm{j}_{k}-\hat{\mathbf{R}}_{k}^{t}\bm{j}_{k}\\ \mathbf{0}&1\end{bmatrix},\] (12)

where \(\mathbb{C}_{j}\) is the list of ancestor of superpoint \(j\) in skeleton model. \(\mathbf{T}_{root}^{t}\) is global transformation of root. Same as Sec. 3.2, we use LBS to derive the motion of each Gaussian and render images.

### Adaptive Control of Superpoints

We use the farthest point sampling algorithm to sample \(M\) Gaussians to initialize the superpoints. Simply making superpoints learnable is not enough to model complex motion patterns. More importantly, we wish to simplify the skeleton after training to ease pose editing by reducing the number of superpoints. Following 3D-GS [2] and SC-GS [48], we develop an adaptive control strategy to prune, densify, and merge superpoints.

**Prune:** To determine whether a superpoint \(\bm{p}_{j}\) should be pruned, we calculate its overall impact \(W_{j}=\sum_{i\in\hat{\mathcal{N}}_{j}}w_{ij}\), where \(\hat{\mathcal{N}}_{j}=\{i\mid j\in\mathcal{N}_{i}\}\) is the set of Gaussians whose \(K\) nearest neighbors include superpoints \(\bm{p}_{j}\). When \(W_{j}<\delta_{prune}\), we prune this superpoint as it is of little contribution to the motion of 3D Gaussians.

**Densify:** Two aspects determining whether a superpoint should be split into two superpoints. On one hand, we clone a superpoint when its impact \(W_{j}\) is greater than a threshold \(\delta_{clone}\), indicating there is a great amount of Gaussians associated with this superpoint, and cloning such superpoints helps model fine motion. On the other hand, we calculate the weighted Gaussians gradient norm of superpoint \(j\) as:

\[g_{j}=\sum_{i\in\hat{\mathcal{N}}_{j}}\frac{w_{ij}}{\sum_{k\in\hat{\mathcal{N} }_{j}}w_{kj}}\|\frac{\partial\mathcal{L}}{\partial\bm{\mu}_{i}}\|_{2}^{2},\] (13)where \(\mathcal{L}\) is the loss function, which demonstrated in Appendix A. We clone the superpoint \(\bm{p}_{j}\) if \(g_{j}\) is greater than the threshold \(\delta_{grad}\).

**Merge:** We merge the superpoints that should belong to the same rigid part. To determine which superpoints should be merged, we first calculate the transformations \(\mathbf{T}_{j}^{t}\in\mathfrak{s}\mathfrak{e}3\) for all superpoints at all training timestamps. Then, for each pair \((a,b)\) of superpoints, we calculate the average relative transformations:

\[D_{a,b}=\frac{1}{N_{t}}\sum_{t}\|\log(\mathbf{T}_{b}^{t}{}^{-1}\mathbf{T}_{a}^ {t})\|,\] (14)

where \(N_{t}\) is the number of train timestamps, \(\log()\) denotes the operation of converting a rigid transformation matrix to a Lie algebra. A small \(D_{a,b}\) indicates two superpoints have similar motion patterns. Therefore, we merge two superpoints \(\bm{p}_{a}\) and \(\bm{p}_{b}\) when \(D_{a,b}<\delta_{merge}\) and \(\bm{p}_{b}\) is the \(K^{\prime}\)-nearest superpoints of \(\bm{p}_{a}\).

## 4 Experiments

In this section, we present the evaluation of our approach, which achieves excellent view-synthesis quality and real-time rendering speed. We also evaluate the contribution of each component through an ablation study. Additionally, we demonstrate the class-agnostic reposing capability. Please refer to our project https://dnvtmf.github.io/SK_GS/ for video visualization.

### Datasets and Evaluation Metrics

To ensure fair comparison with previous work, we choose the same datasets and configurations as AP-NeRF[4]. Specifically, we choose three multi-view video datasets. First, the _D-NeRF_[14] dataset is a sparse multi-view synthesis dataset, which includes 5 humanoids, 2 other articulated objects, and a multi-component scene. Each scene contains 50-200 frames. We choose 6 of 8 scenes 1 The second dataset, _Robots_[3],contains 7 topologically varied robots with multi-view synthetic video. We use 18 views for training and 2 views for evaluation. The third dataset, _ZJU-MoCap_[34], is commonly used

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & FPS\(\uparrow\) & resolution \\ \hline WIM [3] & 29.11 & 0.9664 & 0.0350 & 0.11 & \(512\times 512\) \\ AP-NeRF [4] & 32.45 & 0.9784 & 0.0202 & 0.89 & \(512\times 512\) \\ Ours & 34.34 & 0.9809 & 0.0187 & 137.76 & \(512\times 512\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative comparison of novel view synthesis on the _Robots_ dataset.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method & Skeleton & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & FPS\(\uparrow\) & resolution & Opt. Time \\ \hline D-NeRF [14] & No & 30.48 & 0.9683 & 0.0450 & \textless{}1 & \(400\times 400\) & 20.0 hours \\ TiNeuVox-B [5] & No & 32.60 & 0.9783 & 0.0383 & 0.82 & \(400\times 400\) & 28.0 mins \\ Hexplane [49] & No & 29.81 & 0.9683 & 0.0400 & 1.37 & \(400\times 400\) & 11.5 mins \\ K-Plane hybrid [50] & No & 31.02 & 0.9717 & 0.0495 & 0.52 & \(400\times 400\) & 52.0 mins \\ \hline
4D-GS [20] & No & 34.39 & 0.9830 & 0.0190 & 141.37 & \(800\times 800\) & 20.0 mins \\ SP-GS [22] & No & 37.55 & 0.9884 & 0.0137 & 234.83 & \(800\times 800\) & 52.3 mins \\ D-3D-GS [21] & No & 40.11 & 0.9918 & 0.0120 & 42.10 & \(800\times 800\) & 66.0 mins \\ SC-GS [48] & No & 42.98 & 0.9955 & 0.0028 & 123.04 & \(400\times 400\) & 53.3 mins \\ \hline WIM [3] & Yes & 25.21 & 0.9383 & 0.0700 & 0.16 & \(400\times 400\) & 11 hours \\ AP-NeRF [4] & Yes & 30.91 & 0.9700 & 0.0350 & 1.33 & \(400\times 400\) & 150. mins \\ Ours & Yes & 38.80 & 0.9870 & 0.0095 & 103.98 & \(800\times 800\) & 90.6 mins \\ Ours & Yes & 39.23 & 0.9890 & 0.0070 & 110.90 & \(400\times 400\) & 92.5 mins \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quality comparison of novel view synthesis for the _D-NeRF_ dataset.

for dynamic human reconstruction. Following WIM [3] and AP-NeRF [4], we evaluate 5 sequences with 6 training views for each sequence. We use three metrics to evaluate the image quality of the novel view, _i.e_., peak signal-to-noise ratio (PSNR), structural similarity (SSIM) [51], and learned perceptual image patch similarity (LPIPS) [52].

### Implementation Details

We implement our framework using PyTorch. The number of superpoints is initialized as 512. For both deformable field \(\Phi\) and \(\Psi\), we adopt the architecture of NeRF[1], _i.e_., 8-layers MLP where each layer employs 256-dimensional hidden fully connected layer and ReLU activation function. We also employ positional encoding for the input coordinates and time. For optimization, we employ the Adam optimizer and use the different learning rate decay schedules for each component: the learning rate about 3D Gaussians is the same as 3D-GS, while the learning rate of other components undergoes exponential decay, ranging from 1e-3 to 1e-5. We conducted all experiments on a single NVIDIA Tesla V100 (32GB). More implementation details are shown in Appendix A.

### Baselines

We mainly compare our method to state-of-the-art template-free articulated methods for view synthesis, _i.e_. WIM [3] and AP-NeRF [4]. Besides, we also compare our method with NeRF-based and 3D-GS-based non-articulated methods. D-NeRF [14] extends NeRF to dynamic scenes by warping a static NeRF. TiNeuVox [5] improves the visual quality and training speed by using voxel grids.

Figure 2: Qualitative comparison on _D-NeRF_ datasets.

HexPlane [49] and K-Planes [50] accelerate NeRF by decomposing the space-time volume into several planes. Similar to D-NeRF, Deformable-3D-GS [21] extends static 3D-GS to the temporal domain. 4D-GS [20] accelerate Deformable-3D-GS by decomposing neural voxel encoding algorithm inspired by HexPlane. Similar to ours, SP-GS [22] and SC-GS [48] employ the superpoints/control points to reconstruct dynamic scenes. However, SP-GS and SC-GS can not extract skeleton from reconstructed model.

### Comparisons on Synthetic Dataset

In our experiments, we benchmarked our method against several baselines using the _D-NeRF_ dataset and _Robots_ datasets. The quantitative comparison results, presented in Tab. 1, demonstrate the superior performance of our approach in terms of both rendering speed and visual quality. Specifically, our method significantly outperforms WIM and AP-NeRF not only in visual quality but also in rendering speed. Compared to 3D-GS based dynamic scenes reconstruction models, our method not only have similar performance, but also discover the skeleton and can repose the object to generate a novel pose. Specifically, the rendering quality of ours is higher than 4D-GS [20] and SP-GS [22], and lower than D-3D-GS [21] and SC-GS [48]. Our method also can achieve real-time rendering (>100 FPS), which is near to the rendering speed of SC-GS [48]. Fig. 2 provides the qualitative comparisons of _D-NeRF_ dataset, which demonstrates the advantages of our method over related methods. We also provide results for _Robots_ datasets, quantitatively in Tab. 2 and qualitatively in Fig. 3. It is also clear that our approach is capable of producing high-fidelity novel views with real-time rendering speed. Per-scene results are shown in Appendix B.

### Comparison on Real-world Dataset

In Tab. 3 and Fig. 4, we compare our method to WIM and AP-NeRF in the _ZJU-MoCap_ dataset. We observe that both methods can recover the 3D shape and skeleton models. However, imperfections in the camera calibrations (see Supplement F of [53]), lead to lower visual quality in our results compared to WIM and AP-NeRF. With respect to rendering speed, our approach achieves up to 198.23 FPS. In stark contrast, the rendering speed of WIM and AP-NeRF is extremely slow.

Figure 4: Qualitative comparison for the _ZJU-MoCap_[34] dataset.

Figure 3: Qualitative comparison for the _Robots[3]_ dataset.

### Reposing

In Fig. 5, we show our model allows free changes poses and generates animating video by smoothly interpolating the skeletons posing between user-defined poses. Video examples can be found on our project webpage.

### Ablation Study

We use superpoints to model the parts and motion of the object. The adaptive control of superpoints is the key to reducing the number of superpoints. Fig. 6 (a), (b) and (c) intuitively illustrate the impact of this control strategy. Without the control strategy, the distribution of superpoints becomes uneven, with sparse representation in the arm region, which negatively impacts motion modeling. The merge process in the control strategy significantly reduces the number of superpoints (97 vs 608), while the superpoints are more distributed in the motion area. Besides, as illustrated in Fig. 6 (d), \(\mathcal{L}_{{{arg}}}\) (in Appendix A) also plays an important role in controlling the density of superpoints. See Appendix C for more ablation studies.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & FPS\(\uparrow\) & resolution \\ \hline WIM[3] & 31.08 & 0.963 & 0.053 & 0.12 & \(512\times 512\) \\ AP-NeRF[4] & 29.60 & 0.958 & 0.063 & 1.31 & \(512\times 512\) \\ ours & 29.11 & 0.961 & 0.063 & 198.23 & \(512\times 512\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative comparison for the _ZIU-MoCap_[34] dataset.

Figure 5: Reposing using skeleton. Interpolation from canonical to novel pose.

Figure 6: We visualize the rendering results of (a) our full method, (b) our method without adaptive control, (c) our method without merge superpoints, (d) our method without \(\mathcal{L}_{{{arg}}}\) (see Appendix A). #sp denotes the number of superpoints. The blue points denotes superpoints.

Discuss

### Limitations

We have demonstrated that our approach can achieve real-time rendering, state-of-the-art visual quality, and straightforward repposing capability by skeleton and kinematic models. However, there are some limitations to our approach. Firstly, similar to WIM and AP-NeRF, the learned skeleton model of our approach is restricted to the kinematic motion space exhibited in the input video. Therefore, the skeleton model may have significant differences from the actual one, and extrapolation to generate arbitrary unseen poses may cause errors. Secondly, our approach has similar limitations as other 3D-GS based methods for dynamic scenes. Specifically, the datasets with inaccurate camera poses will lead to reconstruction failures, and large motion or long-term sequences can also result in failures. Lastly, the paper focuses on building the kinematic model for one articulated object. Exporting build kinematic models for multi-component objects or complex scenes that contain multiple objects remains an opportunity for future research. Additionally, extending this approach to motion capture is an interesting research direction.

### Broader Impacts

Although our approach is universal, it is also suitable for rending novel views and poses for humans. Therefore, we acknowledge that our approach can potentially be used to generate fake images or videos. We firmly oppose the use of our research for disseminating false information or damaging reputations.

### Conclusion

We have developed a new method for real-time rendering of articulated models for high-quality novel view synthesis. Without any template or annotations, our approach can reconstruct a kinematic model from multi-view videos. With state-of-the-art visual quality and real-time rendering speed, our work represents a significant step towards the development of low-cost animatable 3D objects for use in movies, games, and education.

#### Acknowledgements

This work is supported by the Sichuan Science and Technology Program (2023YFSY0008), China Tower-Peking University Joint Laboratory of Intelligent Society and Space Governance, National Natural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-30001 of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation Center for Intelligent Robots and Systems (2018IRS11), and PEKSenseTime Joint Laboratory of Machine Vision.

## References

* [1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, "Nerf: Representing scenes as neural radiance fields for view synthesis," in _ECCV_, 2020, pp. 405-421.
* [2] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, "3d gaussian splatting for real-time radiance field rendering," _ACM TOG_, vol. 42, no. 4, Jul. 2023.
* [3] A. Noguchi, U. Iqbal, J. Tremblay, T. Harada, and O. Gallo, "Watch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects," in _CVPR_, 2022, pp. 3667-3677.
* [4] L. Uzolas, E. Eisemann, and P. Kellnhofer, "Template-free articulated neural point clouds for repposable view synthesis," in _NeurIPS_, 2023.
* [5] J. Fang _et al._, "Fast dynamic radiance fields with time-aware neural voxels," in _SIGGRAPH Asia 2022 Conference Papers_, 2022.
* [6] Q. Xu _et al._, "Point-nerf: Point-based neural radiance fields," in _CVPR_, 2022, pp. 5428-5438.
* [7] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, "Tensorf: Tensorial radiance fields," in _ECCV_, 2022, pp. 333-350.
* [8] T. Muller, A. Evans, C. Schied, and A. Keller, "Instant neural graphics primitives with a multiresolution hash encoding," _ACM TOG_, vol. 41, no. 4, 102:1-102:15, Jul. 2022.

* [9] T. Hu, S. Liu, Y. Chen, T. Shen, and J. Jia, "Efficientnerf efficient neural radiance fields," in _CVPR_, 2022, pp. 12 902-12 911.
* [10] C. Sun, M. Sun, and H.-T. Chen, "Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction," in _CVPR_, 2022, pp. 5459-5469.
* [11] W. Xian, J.-B. Huang, J. Kopf, and C. Kim, "Space-time neural irradiance fields for free-viewpoint video," in _CVPR_, 2021, pp. 9421-9431.
* [12] C. Gao, A. Saraf, J. Kopf, and J.-B. Huang, "Dynamic view synthesis from dynamic monocular video," in _ICCV_, 2021, pp. 5712-5721.
* [13] Z. Li, S. Niklaus, N. Snavely, and O. Wang, "Neural scene flow fields for space-time view synthesis of dynamic scenes," in _CVPR_, 2021, pp. 6498-6508.
* [14] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, "D-nerf: Neural radiance fields for dynamic scenes," in _CVPR_, 2021, pp. 10 313-10 322.
* [15] K. Park _et al._, "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields," _ACM TOG_, vol. 40, no. 6, Dec. 2021.
* [16] Y. Du, Y. Zhang, H.-X. Yu, J. B. Tenenbaum, and J. Wu, "Neural radiance flow for 4d view synthesis and video processing," in _ICCV_, 2021, pp. 14 304-14 314.
* [17] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely, "Dynibar: Neural dynamic image-based rendering," in _CVPR_, 2023, pp. 4273-4284.
* [18] E. Tretschk, A. Tewari, V. Golyanik, M. Zollhofer, C. Lassner, and C. Theobalt, "Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video," in _ICCV_, 2021, pp. 12 959-12 970.
* [19] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis," in _3DV_, 2024.
* [20] G. Wu _et al._, "4D gaussian splatting for real-time dynamic scene rendering," in _CVPR_, 2024, pp. 20 310-20 320.
* [21] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, "Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction," in _CVPR_, 2024, pp. 20 331-20 341.
* [22] D. Wan, R. Lu, and G. Zeng, "Superpoint gaussian splatting for real-time high-fidelity dynamic scene reconstruction," in _ICML_, 2024, pp. 49 957-49 972.
* [23] V. Blanz and T. Vetter, "A morphable model for the synthesis of 3d faces," in _ACM SIGGRAPH_, 1999, pp. 187-194.
* [24] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, "Learning a model of facial shape and expression from 4d scans.," _ACM TOG_, vol. 36, no. 6, pp. 194-1, 2017.
* [25] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, "Smpl: A skinned multi-person linear model," _ACM TOG_, vol. 34, no. 6, 248:1-248:16, 2015.
* [26] G. Pavlakos _et al._, "Expressive body capture: 3d hands, face, and body from a single image," in _CVPR_, 2019, pp. 10 975-10 985.
* [27] S. Zuffi, A. Kanazawa, D. W. Jacobs, and M. J. Black, "3d menagerie: Modeling the 3d shape and pose of animals," in _CVPR_, 2017, pp. 6365-6373.
* [28] Y. Zheng, W. Yifan, G. Wetzstein, M. J. Black, and O. Hilliges, "Pointavatar: Deformable point-based head avatars from videos," in _CVPR_, 2023, pp. 21 057-21 067.
* [29] G. Gafni, J. Thies, M. Zollhofer, and M. Niessner, "Dynamic neural radiance fields for monocular 4d facial avatar reconstruction," in _CVPR_, 2021, pp. 8649-8658.
* [30] Z. Wang _et al._, "Learning compositional radiance fields of dynamic human heads," in _CVPR_, 2021, pp. 5704-5713.
* [31] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, and J. Zhang, "Ad-nerf: Audio driven neural radiance fields for talking head synthesis," in _ICCV_, 2021, pp. 5784-5794.
* [32] Y. Zheng, V. F. Abrevaya, M. C. Buhler, X. Chen, M. J. Black, and O. Hilliges, "Im avatar: Implicit morphable head avatars from videos," in _CVPR_, 2022, pp. 13 545-13 555.
* [33] Y. Zhuang, H. Zhu, X. Sun, and X. Cao, "Mofanerf: Morphable facial neural radiance field," in _ECCV_, 2022, pp. 268-285.
* [34] S. Peng _et al._, "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans," in _CVPR_, 2021, pp. 9054-9063.
* [35] S.-Y. Su, F. Yu, M. Zollhofer, and H. Rhodin, "A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose," in _NeurIPS_, 2021, pp. 12 278-12 291.

* [36] S. Peng _et al._, "Animatable neural radiance fields for modeling dynamic human bodies," in _ICCV_, 2021, pp. 14 314-14 323.
* [37] H. Xu, T. Alldieck, and C. Sminchisescu, "H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion," in _NeurIPS_, 2021, pp. 14 955-14 966.
* [38] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, "Neural human performer: Learning generalizable radiance fields for human performance rendering," in _NeurIPS_, 2021, pp. 24 741-24 752.
* [39] T. Hu, T. Yu, Z. Zheng, H. Zhang, Y. Liu, and M. Zwicker, "Hvtr: Hybrid volumetric-textural rendering for human avatars," in _3DV_, 2022, pp. 197-208.
* [40] L. Liu, M. Habermann, V. Rudnev, K. Sarkar, J. Gu, and C. Theobalt, "Neural actor: Neural free-view synthesis of human actors with pose control," _ACM TOG_, vol. 40, no. 6, pp. 1-16, 2021.
* [41] T. Xu, Y. Fujita, and E. Matsumoto, "Surface-aligned neural radiance fields for controllable 3d human synthesis," in _CVPR_, 2022, pp. 15 883-15 892.
* [42] Y. Wu, Z. Chen, S. Liu, Z. Ren, and S. Wang, "Casa: Category-agnostic skeletal animal reconstruction," in _NeurIPS_, 2022, pp. 28 559-28 574.
* [43] C.-H. Yao, W.-C. Hung, Y. Li, M. Rubinstein, M.-H. Yang, and V. Jampani, "Lassie: Learning articulated shapes from sparse image ensemble via 3d part discovery," in _NeurIPS_, 2022, pp. 15 296-15 308.
* [44] S. Wu, R. Li, T. Jakab, C. Rupprecht, and A. Vedaldi, "Magicpony: Learning articulated 3d animals in the wild," in _CVPR_, 2023, pp. 8792-8802.
* [45] C.-H. Yao, W.-C. Hung, Y. Li, M. Rubinstein, M.-H. Yang, and V. Jampani, "Hi-lassie: High-fidelity articulated shape and skeleton discovery from sparse image ensemble," in _CVPR_, 2023, pp. 4853-4862.
* [46] M. Zwicker, H. Pfister, J. van Baar, and M. H. Gross, "Ewa volume splatting," _Proceedings Visualization, 2001. VIS '01._, pp. 29-538, 2001.
* [47] R. W. Sumner, J. Schmid, and M. Pauly, "Embedded deformation for shape manipulation," _ACM TOG_, vol. 26, no. 3, 80-es, Jul. 2007.
* [48] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, "Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes," in _CVPR_, 2024, pp. 4220-4230.
* [49] A. Cao and J. Johnson, "Hexplane: A fast representation for dynamic scenes," in _CVPR_, 2023, pp. 130-141.
* [50] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, "K-planes: Explicit radiance fields in space, time, and appearance," in _CVPR_, 2023, pp. 12 479-12 488.
* [51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, "Image quality assessment: From error visibility to structural similarity," _IEEE TIP_, vol. 13, pp. 600-612, 2004.
* [52] Z. Li, S. Niklaus, N. Snavely, and O. Wang, "Neural scene flow fields for space-time view synthesis of dynamic scenes," in _CVPR_, 2021, pp. 6494-6504.
* [53] R. Li _et al._, "Tava: Template-free animatable volumetric actors," in _ECCV_, 2022, pp. 419-436.

More Implementation Details

The whole optimization progress can be divided into three stages: _dynamic_ stage, _joints discovery_ stage and _kinematic_ stage.

### Dynamic Stage

In this stage, we aim to reconstruct the appearance of the object, and find the position of underlying joints. Therefore, we employ the \(\mathcal{L}_{joint}\), whose formula is:

\[\mathcal{L}_{joint}=\frac{1}{M^{2}}\sum_{i=1}^{M}\sum_{j=1}^{M}d_{ij}+\frac{1 }{M-1}\sum_{(i,j)\in\Gamma}d_{ij}\] (15)

Besides, to obtain a good segmentation of object parts by using superpoints, we employ some regularization losses. To reduce the total number of superpoints, we encourage nearby superpoints to possess same motion patterns via as-rigid-as-possible regularization \(\mathcal{L}_{xrap}\):

\[\mathcal{L}_{xrap}=\sum_{j=1}^{M}\sum_{k\in\mathcal{N}_{j}^{sp}}\|\log({\bf R }_{j}^{t}{}^{-1}{\bf R}_{k}^{t})\|_{2}^{2}+\|\bm{o}_{j}^{t}-\bm{o}_{k}^{t}\|_ {2}^{2}.\] (16)

where \(\mathcal{N}_{j}^{sp}\) is the set of \(K^{\prime}\)-nearest neighbor superpoints of superpoint \(j\). For the blend skinning weight, we employ \(\mathcal{L}_{smooth}\) to encourage smoothness by penalizing divergence of skinning weight:

\[\mathcal{L}_{smooth}=\sum_{i=1}^{N}\sum_{j\in\mathcal{N}_{i}}|w_{i}-w_{j}|.\] (17)

Besides, we apply \(\mathcal{L}_{sparse}\) to encourage sparsity so that one Gaussian is more likely associated with only one superpoint:

\[\mathcal{L}_{sparse}=-\sum_{i}^{N}\sum_{j}^{B}w_{ij}\log(w_{ij})+(1-w_{ij}) \log(1-w_{ij})\] (18)

In total, our training loss of _dynamic_ stage is:

\[\mathcal{L}=\lambda_{0}\mathcal{L}_{rgb}+\lambda_{1}\mathcal{L}_{joint}+ \lambda_{2}\mathcal{L}_{xrap}+\lambda_{3}\mathcal{L}_{smooth}+\lambda_{4} \mathcal{L}_{sparse}\] (19)

where \(\lambda=\{1,1.,10^{-3},0.1,0.1\}\) in our experiments.

In this stage, we conducted training for a total of 40k iterations. Similar to SC-GS [48], our training scheme is as follows:

1. In 0\(\sim\)2k iterations, we fix deformable field \(\Phi\).
2. In 2k\(\sim\)10k iterations, we train deformable field \(\Phi\) which using the position of Gaussians rather than superpoints as inputs. At 7500 iteration, we sample \(M\) Gaussians by using fatherest sampling algorithm. At 10k iteration, we initialize superpoints by traind \(M\) Gaussians, and re-initialize the Gaussians in canonical space.
3. In 10 \(\sim\) 13k iterations, we fix deformable field \(\Phi\).
4. In 13k \(\sim\) 40k iterations, all parameters are optimized.

During the first 20k iterations, We do not discover joints and skeleton, _i.e._, \(\lambda_{1}=0\). During 20k \(\sim\) 40k iterations, we update the structure of skeleton every 100 iterations. Note that we stopped the gradient between joints and other parts so that the learning of joints has no effect on the learning of Gaussians, superpoints and deformation field.

During training, we adopt the same adaptive control strategy for Gaussians as 3D-GS[2] In details, in iterations 1\(\sim\)7500 and 10k\(\sim\) 35k, we densify and prune Gaussians every 100 iterations, while the densify grad threshold is set to 0.0002 and the opacity threshold for prune is set to 0.005. We reset the opacity of Gaussians every 3k steps.

We densify and prune superpoints every 1000 steps between iterations 20k and 30k, and the hyper-parameter \(\delta_{grad}=0.0002\) and \(\delta_{prune}=0.001\). We merge superpoints every 1000 steps between iterations 30k and 40k, while threshold \(\delta_{merge}=0.0005\).

[MISSING_PAGE_FAIL:14]

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline PSNR\(\uparrow\) & JumpingJacks & Mutant & Hook & T-Rex & StandUp & HellWarrior & Average \\ \hline D-NeRF [14] & 32.80 & 31.29 & 29.25 & 31.75 & 32.79 & 25.02 & 30.48 \\ TiNeuVox-B [5] & 34.23 & 33.61 & 31.45 & 32.7 & 35.43 & 28.17 & 32.60 \\ Hexplane [49] & 31.65 & 33.79 & 28.71 & 30.67 & 34.6 & 24.24 & 29.81 \\ K-Plane hybrid [50] & 32.64 & 33.79 & 28.5 & 31.79 & 33.72 & 25.7 & 31.02 \\ \hline D-3D-GS [21] & 37.59 & 42.61 & 37.09 & 37.67 & 44.30 & 41.41 & 40.11 \\
4D-GS [20] & 35.44 & 37.43 & 33.01 & 33.61 & 38.11 & 28.77 & 34.39 \\ SP-GS [22] & 35.56 & 39.43 & 35.36 & 32.69 & 42.07 & 40.19 & 37.55 \\ SC-GS [48] & 41.62 & 45.08 & 39.81 & 40.70 & 47.81 & 42.88 & 42.98 \\ \hline WIM [3] & 29.77 & 25.80 & 25.33 & 26.19 & 27.46 & 16.71 & 25.21 \\ AP-NeRF [4] & 34.50 & 28.56 & 30.24 & 32.85 & 31.93 & 27.53 & 30.94 \\ Ours (\(800\times 800\)) & 36.95 & 40.95 & 36.64 & 35.10 & 43.52 & 39.65 & 38.80 \\ Ours (\(400\times 400\)) & 36.70 & 41.96 & 36.77 & 36.14 & 43.84 & 39.95 & 39.23 \\ \hline \hline SSIM\(\uparrow\) & JumpingJacks & Mutant & Hook & T-Rex & StandUp & HellWarrior & Average \\ \hline D-NeRF [14] & 0.98 & 0.97 & 0.96 & 0.97 & 0.98 & 0.95 & 0.9683 \\ TiNeuVox-B [5] & 0.98 & 0.98 & 0.97 & 0.98 & 0.99 & 0.97 & 0.9783 \\ Hexplane [49] & 0.97 & 0.98 & 0.96 & 0.98 & 0.98 & 0.94 & 0.9683 \\ K-Plane hybrid [50] & 0.977 & 0.983 & 0.954 & 0.981 & 0.983 & 0.952 & 0.9717 \\ \hline D-3D-GS [21] & 0.9929 & 0.987 & 0.9858 & 0.995 & 0.9947 & 0.9953 & 0.9918 \\
4D-GS [20] & 0.9857 & 0.988 & 0.9760 & 0.985 & 0.9898 & 0.9733 & 0.9830 \\ SP-GS [22] & 0.9950 & 0.9868 & 0.9804 & 0.9861 & 0.9926 & 0.9894 & 0.9884 \\ SC-GS [48] & 0.9957 & 0.9977 & 0.9934 & 0.9972 & 0.9981 & 0.9908 & 0.9955 \\ \hline WIM[3] & 0.97 & 0.95 & 0.94 & 0.94 & 0.96 & 0.87 & 0.9383 \\ AP-NeRF[4] & 0.98 & 0.96 & 0.97 & 0.98 & 0.97 & 0.96 & 0.9700 \\ Ours (\(800\times 800\)) & 0.9883 & 0.9921 & 0.9847 & 0.9877 & 0.9933 & 0.9761 & 0.9870 \\ Ours (\(400\times 400\)) & 0.9889 & 0.9951 & 0.9869 & 0.9934 & 0.9950 & 0.9749 & 0.9890 \\ \hline \hline LPIPS \(\downarrow\) & JumpingJacks & Mutant & Hook & T-Rex & StandUp & HellWarrior & Average \\ \hline D-NeRF[14] & 0.03 & 0.02 & 0.11 & 0.03 & 0.02 & 0.06 & 0.0450 \\ TiNeuVox-B[5] & 0.03 & 0.03 & 0.05 & 0.03 & 0.02 & 0.07 & 0.0383 \\ Hexplane[49] & 0.04 & 0.03 & 0.05 & 0.03 & 0.02 & 0.07 & 0.0400 \\ K-Plane hybrid[50] & 0.0468 & 0.0362 & 0.0662 & 0.0343 & 0.031 & 0.0824 & 0.0495 \\ \hline D-3D-GS[21] & 0.0126 & 0.0052 & 0.0144 & 0.0098 & 0.0063 & 0.0234 & 0.0120 \\
4D-GS[20] & 0.0128 & 0.0167 & 0.0272 & 0.0131 & 0.0074 & 0.0369 & 0.0190 \\ SP-GS [22] & 0.0069 & 0.0164 & 0.0187 & 0.0243 & 0.0096 & 0.0066 & 0.0137 \\ SC-GS [48] & 0.0030 & 0.0011 & 0.0037 & 0.0014 & 0.0008 & 0.0068 & 0.0028 \\ \hline WIM[3] & 0.04 & 0.06 & 0.06 & 0.08 & 0.04 & 0.14 & 0.0700 \\ AP-NeRF[4] & 0.03 & 0.03 & 0.05 & 0.02 & 0.02 & 0.06 & 0.0350 \\ Ours (\(800\times 800\)) & 0.0086 & 0.0038 & 0.0089 & 0.0105 & 0.0045 & 0.0203 & 0.0095 \\ Ours (\(400\times 400\)) & 0.0090 & 0.0022 & 0.0073 & 0.0054 & 0.0026 & 0.0155 & 0.0070 \\ \hline \hline FPS\(\uparrow\) & JumpingJacks & Mutant & Hook & T-Rex & StandUp & Hellwarrior & Average \\ \hline D-3D-GS [21] & 16.35 & 102.51 & 32.3 & 20.59 & 49.14 & 31.68 & 42.10 \\
4D-GS [20] & 112.89 & 129.59 & 147.38 & 144.46 & 152.46 & 161.41 & 141.37 \\ SP-GS [22] & 271.27 & 210.42 & 230.35 & 186.65 & 260.34 & 249.93 & 234.83 \\ SC-GS [48] & 128.05 & 122.01 & 129.81 & 105.24 & 132.91 & 120.24 & 123.04 \\ \hline WIM [3] & 0.19 & 0.17 & 0.13 & 0.16 & 0.18 & 0.13 & 0.16 \\ AP-NeRF [4] & 1.57 & 1.42 & 1.11 & 1.34 & 1.48 & 1.06 & 1.33 \\ Ours (\(800\times 800\)) & 106.81 & 101.04 & 103.60 & 98.38 & 102.02 & 112.03 & 103.98 \\ Ours (\(400\times 400\)) & 109.61 & 104.22 & 109.11 & 109.15 & 110.03 & 123.28 & 110.90 \\ \hline \end{tabular}
\end{table}
Table 4: Quantitative Results Per-Scene in _D-NeRF_ dataset.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline scene & hellwarrior & hook & jumpingjacks & mutant & standup & trex & average \\ \hline Training Time (h) & 1.17 & 1.52 & 1.50 & 1.55 & 1.40 & 1.92 & 1.51 \\ GPU VRAM(GB) & 1.23 & 3.63 & 2.31 & 3.33 & 2.19 & 4.32 & 2.83 \\ num. of Gaussians (\(\times 10^{5}\)) & 0.60 & 1.54 & 1.11 & 1.44 & 0.97 & 1.92 & 1.28 \\ num. of superpoints & 188 & 184 & 112 & 51 & 134 & 42 & 118.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Optimization time and required resources for each scene in the _D-NeRF_ dataset.

Figure 8: Compare rendered images between canonical space and the warp space of timestamp 0. (a) canonical space in _Dynamic_ stage, (b) at time 0 in _Dynamic_ stage, (c) canonical space in _Kinematic_ stage, (d) at time 0 in _Kinematic_ stage, (e)LBS at time 0 in _Kinematic_ stage (f)ground truth at time 0.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline PSNR\(\uparrow\) & Atlas & Baxter & Cassie & Iiwa & Nao & Pandas & Spot & Average \\ \hline WIM[3] & 25.01 & 23.36 & 30.08 & 32.77 & 28.68 & 35.93 & 27.92 & 29.11 \\ AP-NeRF[4] & 32.56 & 30.74 & 31.44 & 35.78 & 30.64 & 32.44 & 33.57 & 32.45 \\ ours & 34.89 & 31.96 & 32.08 & 39.34 & 33.24 & 32.02 & 36.85 & 34.34 \\ \hline \hline SSIM\(\uparrow\) & Atlas & Baxter & Cassie & Iiwa & Nao & Pandas & Spot & Average \\ \hline WIM[3] & 0.9405 & 0.9538 & 0.9712 & 0.9866 & 0.9546 & 0.9878 & 0.9704 & 0.9664 \\ AP-NeRF[4] & 0.9833 & 0.9759 & 0.9775 & 0.9884 & 0.9615 & 0.9785 & 0.9837 & 0.9784 \\ ours & 0.9891 & 0.9561 & 0.9792 & 0.9928 & 0.9741 & 0.9896 & 0.9856 & 0.9809 \\ \hline \hline LPIPS\(\downarrow\) & Atlas & Baxter & Cassie & Iiwa & Nao & Pandas & Spot & Average \\ \hline WIM[3] & 0.0648 & 0.0455 & 0.0359 & 0.0160 & 0.0368 & 0.0177 & 0.0283 & 0.0350 \\ AP-NeRF[4] & 0.0170 & 0.0196 & 0.0281 & 0.0129 & 0.0323 & 0.0203 & 0.0110 & 0.0202 \\ ours & 0.0092 & 0.0430 & 0.0249 & 0.0062 & 0.0187 & 0.0172 & 0.0114 & 0.0187 \\ \hline \hline FPS\(\uparrow\) & Atlas & Baxter & Cassie & Iiwa & Nao & Pandas & Spot & Average \\ \hline WIM[3] & 0.08 & 0.08 & 0.10 & 0.17 & 0.07 & 0.10 & 0.08 & 0.10 \\ AP-NeRF[4] & 0.66 & 1.26 & 0.33 & 0.35 & 1.02 & 0.39 & 1.73 & 0.82 \\ ours & 123.32 & 149.43 & 139.08 & 122.59 & 141.58 & 146.73 & 141.62 & 137.76 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Quantitative Results Per-Scene in _Robots_ dataset.

Figure 7: Compare to AP-NeRF, ours method are more robust for complex motion.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \(M\) & 128 & 256 & 384 & 512 & 640 & 768 & 896 & 1024 \\ \hline PSNR\(\uparrow\) & 39.61 & 39.70 & 39.83 & 39.69 & 39.89 & 39.56 & 39.72 & 40.43 \\ SSIM\(\uparrow\) & 0.9765 & 0.9766 & 0.9779 & 0.9773 & 0.9781 & 0.9766 & 0.9767 & 0.9800 \\ LPIPS\(\downarrow\) & 0.0191 & 0.0179 & 0.0173 & 0.0183 & 0.0169 & 0.0183 & 0.0186 & 0.0155 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study for the number of initial superpoint \(M\) on the hellwarrior scene of _D-NeRF_ dataset.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly state the main contributions, which match experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of this paper in Sec. 5.1. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide disclose all information in Sec. 3 and Appendix A needed to reproduce the experimental results of paper. Besides, we will release the source code once the paper is accepted. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We released the source code on GitHub, _i.e._, https://github.com/dnvtmf/SK_GS. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all the training and test details in Sec. 4,Sec. 3 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not report error bars or other information about statistical significance as this is not a common procedure in the field and does not contribute to understanding our evaluation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources in the paper and code. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conducted this research in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts of this paper in Sec. 5.2. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the code package or dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.