# Nuclear Norm Regularization for Deep Learning

 Christopher Scarvelis

MIT CSAIL

scarv@mit.edu &Justin Solomon

MIT CSAIL

jsolomon@mit.edu

###### Abstract

Penalizing the nuclear norm of a function's Jacobian encourages it to locally behave like a low-rank linear map. Such functions vary locally along only a handful of directions, making the Jacobian nuclear norm a natural regularizer for machine learning problems. However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its SVD. We show how to efficiently penalize the Jacobian nuclear norm using techniques tailor-made for deep learning. We prove that for functions parametrized as compositions \(f=g\circ h\), one may equivalently penalize the average squared Frobenius norms of \(Jg\) and \(Jh\). We then propose a denoising-style approximation that avoids Jacobian computations altogether. Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems. We complement our theory with an empirical study of our regularizer's performance and investigate applications to denoising and representation learning.

## 1 Introduction

Building models that adapt to the structure of their data is a key challenge in machine learning. As real-world data typically concentrates on low-dimensional manifolds, a good model \(f\) should only be sensitive to changes to its inputs along the data manifold. One may encourage this behavior by regularizing \(f\) so that its _Jacobian_\(Jf[x]\) has low rank. This causes \(f\) to locally behave like a low-rank linear map and therefore be locally constant in directions that are in the kernel of \(Jf[x]\).

How should we regularize \(f\) so that its Jacobians have low rank? Directly penalizing \(\text{rank}(Jf[x])\) during training is challenging, as the rank function is not differentiable. In light of this, the _nuclear norm_\(\|Jf[x]\|_{*}\) is an appealing alternative: Being the \(L^{1}\) norm of a matrix's singular values, it is the tightest convex relaxation of the rank function, and as it is differentiable almost everywhere, it can be included in standard deep learning pipelines.

One critical flaw in this strategy is its computational cost. The nuclear norm of a matrix is the sum of its _singular values_, so to penalize \(\|Jf[x]\|_{*}\) in training, one must (1) compute the Jacobian of a typically high-dimensional map \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\), (2) take the singular value decomposition (SVD) of this \(m\times n\) matrix, (3) sum its singular values, and (4) differentiate through each of these operations. The combined cost of these operations is prohibitive for high-dimensional data. Consequently, nuclear norm regularization has yet to be widely adopted by the deep learning community.

This work shows how to efficiently penalize the Jacobian nuclear norm \(\|Jf[x]\|_{*}\) using techniques tailor-made for deep learning. We first show that parametrizing \(f\) as a composition \(f=g\circ h\) - a feature common to _all deep learning pipelines_ - allows one to replace the expensive nuclear norm \(\|Jf[x]\|_{*}\) with two squared _Frobenius_ norms \(\|Jh[x]\|_{F}^{2}\) and \(\|Jg[h(x)]\|_{F}^{2}\), which admit an elementwise computation that avoids a costly SVD. We prove that the resulting problem is _exactly_ equivalent to the original problem with the nuclear norm penalty. We in turn approximate this by a denoising-style objective that avoids the Jacobian computation altogether. Our method issimple, efficient, and accurate - both in theory and in practice - and enables Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems.

We complement our theoretical results with an empirical study of our regularizer's performance on synthetic data. As the Jacobian nuclear norm has seldom been used as a regularizer in deep learning, we propose applications of our method to unsupervised denoising, where one trains a denoiser given a dataset of noisy images without access to their clean counterparts, and to representation learning.

**Our work makes the Jacobian nuclear norm a feasible component of deep learning pipelines**, enabling users to learn locally low-rank functions unencumbered by the heavy cost of naive Jacobian nuclear norm regularization.

## 2 Related work

Nuclear norm regularization.As penalizing the nuclear norm in matrix learning problems encourages low-rank solutions, nuclear norm regularization (NNR) has been widely used throughout machine learning. Rennie and Srebro (2005) propose NNR for collaborative filtering, where one attempts to predict user interests by aggregating incomplete information from a large pool of users. Candes et al. (2011) introduce robust PCA, which decomposes a noisy matrix into low-rank and sparse parts and uses nuclear norm regularization to learn the low-rank part. Cabral et al. (2013); Dai et al. (2014) use NNR to regularize the ill-posed structure-from-motion problem, which recovers a 3D scene from a set of 2D images.

A line of work beginning with Candes and Recht (2012) takes inspiration from compressed sensing and studies the conditions under which a low-rank matrix can be perfectly recovered from a small sample of its entries via nuclear norm minimization. This work was followed by Candes and Tao (2010); Keshavan et al. (2009); Recht (2011), which progressively sharpen the bounds on the number of samples required for exact recovery. In parallel, Cai et al. (2010) propose singular value thresholding (SVT), a simple algorithm for approximate nuclear norm minimization that avoids solving a costly semidefinite program as with earlier algorithms. However, SVT still requires computing a singular value decomposition in each iteration, which is an onerous requirement for large matrices. Motivated by this challenge, Rennie and Srebro (2005) show how to convert a nuclear norm-regularized matrix learning problem into an equivalent non-convex problem involving only squared Frobenius norms. Our work generalizes their method to non-linear learning problems.

Jacobian regularization in deep learning.Sokolic et al. (2017); Varga et al. (2017); Hoffman et al. (2020) penalize the spectral and Frobenius norms of a neural net's Jacobian with respect to its inputs to improve classifier generalization, particularly in the low-data regime. Similarly, Jakubovitz and Giryes (2018) fine-tune neural classifiers with Jacobian Frobenius norm regularization to improve adversarial robustness. Unlike our work, these papers do not consider nuclear norm regularization.

Neural ODEs (Chen et al., 2018) parametrize functions as solutions to initial value problems with neural velocity fields. Ensuring that the learned dynamics are well-conditioned minimizes the number of steps required to accurately solve these ODEs. To this end, Finlay et al. (2020) penalize the squared Frobenius norm of the velocity field's Jacobian and observe a tight relationship between the value of this norm and the step size of an adaptive ODE solver. Kelly et al. (2020) extend this approach by regularizing higher-order derivatives as well.

Finally, it has been observed as early as Webb (1994); Bishop (1995) that training a neural net on noisy data approximately penalizes the squared Frobenius norm of the network Jacobian at training data. Inspired by this observation, Vincent et al. (2008, 2010) propose denoising autoencoders for representation learning, and Rifai et al. (2011) propose directly penalizing the squared Frobenius norm of the encoder Jacobian to encourage robust latent representations. Alain and Bengio (2014) show that an autoencoder trained with a penalty on the squared Frobenius norm of its Jacobian learns the score of the data distribution for small regularization values. Recently, Kadkhodaie et al. (2024) employ a similar analysis of the denoising objective to study the generalization of diffusion models.

Denoising via singular value shrinkage.While a full survey of the denoising literature is out of scope (see e.g. Elad et al. (2023)), we highlight a handful of works that employ _singular value shrinkage_ (SVS) to denoise low-rank data corrupted by isotropic noise given a noisy data matrix. SVS denoises a noisy data matrix \(Y\) by applying a shrinkage function \(\phi\) to its singular values \(\sigma_{d}\)

[MISSING_PAGE_FAIL:3]

### Our key result

Equation (2) enables the use of simple and efficient gradient-based methods for learning a low-rank linear map by parametrizing the matrix \(A\) as a _composition_\(A=UV^{\top}\) of linear maps \(U\) and \(V^{\top}\). In deep learning, one is interested in learning _non-linear_ functions that are parametrized by compositions of simpler functions. Such functions \(f\) are differentiable almost everywhere, so they are _locally_ well-approximated by linear maps specified by their _Jacobians_\(Jf[x]\).

Encouraging the learned function to have a low-rank Jacobian is a natural prior: It corresponds to learning a function that locally behaves like a low-rank linear map. Such functions vary locally along only a handful of directions and are constant in the remaining directions. When the training data is supported on a low-dimensional manifold, these directions correspond to the tangents and normals, respectively, to the data manifold. One may implement this low-rank prior on \(Jf\) by solving the following optimization problem:

\[\inf_{f:\mathbb{R}^{n}\to\mathbb{R}^{m}x\sim\mathcal{D}(\Omega)}\left[\ell(f(x ),x)+\eta\|Jf[x]\|_{*}\right],\] (3)

where \(\ell\) is a generic differentiable loss function and \(\mathcal{D}(\Omega)\) is a data distribution supported on \(\Omega\subseteq\mathbb{R}^{n}\). If \(f\) is parametrized as a neural network and \(n,m\) are large, this problem is costly to optimize via stochastic gradient descent, as \(Jf[x]\in\mathbb{R}^{m\times n}\) and computing the subgradient of \(\|Jf[x]\|_{*}\) requires a cubic-time SVD. In fact, simply _storing_\(Jf[x]\) in memory is often intractable for large \(n,m\), which is typical when \(f\) is an image-to-image map. For example, if \(f\) is a denoiser operating on \(1024\times 1024\) RGB images, its inputs are \(3\times 1024\times 1024=3,\!145,\!728\)-dimensional, and \(Jf[x]\) occupies nearly 40 TB of memory.

To address these challenges, we first prove a theorem generalizing (2) to non-linear functions. We then show how to avoid computing \(Jf[x]\) altogether using a first-order Taylor expansion and Hutchinson's estimator. **Our primary contribution is the following result:**

**Theorem 3.1**: _Let \(D(\Omega)\) be a data distribution supported on a compact set \(\Omega\subseteq\mathbb{R}^{n}\) with measure \(\mu\) that is absolutely continuous with respect to the Lebesgue measure on \(\Omega\). Let \(\ell\in C^{1}(\mathbb{R}^{m}\times\mathbb{R}^{n})\) be a continuously differentiable loss function. Then,_

\[\inf_{f\in C^{\infty}(\Omega)}\mathop{\mathbb{E}}_{x\sim\mathcal{ D}(\Omega)}\left[\ell(f(x),x)+\eta\|Jf[x]\|_{*}\right]\\ =\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}\mathop{\mathbb{E}}_{x\sim\mathcal{D}( \Omega)}\left[\ell(g(h(x)),x)+\frac{\eta}{2}\left(\|Jg[h(x)]\|_{F}^{2}+\|Jh[x ]\|_{F}^{2}\right)\right].\] (4)

On the left-hand side, we learn a function \(f:\mathbb{R}^{n}\to\mathbb{R}^{m}\) given fixed input and output dimensions \(n,m\). On the right-hand side, we learn functions \(h:\mathbb{R}^{n}\to\mathbb{R}^{d}\) and \(g:\mathbb{R}^{d}\to\mathbb{R}^{m}\) with \(n,m\) fixed but optimize over the inner dimension \(d\). We prove this theorem in Appendix A.2 and sketch the proof below. Theorem 3.1 shows that by parametrizing \(f\) as a composition of \(g\) and \(h\) - a feature common to all deep learning pipelines - one may learn a _locally_ low-rank function without computing expensive SVDs during training.

Proof sketch.We denote the left-hand side objective by \(E_{L}(f)\) and its inf by \((L)\); we denote the right-hand side objective by \(E_{R}(g,h)\) and its inf by \((R)\). We prove that \((L)\leq(R)\) and \((R)\leq(L)\). \((L)\leq(R)\) is the easy direction. The basic observation is that if \(f=g\circ h\), then \(Jf[x]=Jg[h(x)]Jh[x]\) by the chain rule. Equation (1) then implies that

\[\|Jf[x]\|_{*}=\min_{U,V:UV^{\top}=Jf[x]}\frac{1}{2}\left(\|U\|_{F}^{2}+\|V\|_{F }^{2}\right)\leq\frac{1}{2}\left(\|Jg[h(x)]\|_{F}^{2}+\|Jh[x]\|_{F}^{2}\right).\]

\((R)\leq(L)\) is the hard direction. The proof strategy is as follows:

1. We begin with a function \(f_{m}\in C^{\infty}(\Omega)\) such that \(E_{L}(f_{m})\) is arbitrarily close to its inf over \(C^{\infty}(\Omega)\). We use \(f_{m}\) to construct parametric families of affine functions \(g_{m}^{z},h_{m}^{z}\) whose composition is a good local approximation to \(f_{m}\) in a neighborhood of \(z\in\Omega\), both pointwise and in terms of the contributions to \(E_{R}(g_{m}^{z},h_{m}^{z})\) and \(E_{L}(f_{m})\), resp., due to \(x\in\Omega\) near \(z\).

2. We then stitch together these local approximations to form a sequence of global approximations \(g_{m}^{k},h_{m}^{k}\) to \(f_{m}\). These functions are piecewise affine and hence not regular enough to lie in \(C^{\infty}(\Omega)\) as required by the right-hand side of Equation (4).
3. Finally, mollifying the piecewise affine functions \(g_{m}^{k},h_{m}^{k}\) yields a minimizing sequence of \(C^{\infty}(\Omega)\) functions \(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k}\) such that \(E_{R}(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k})\) approaches the inf of \(E_{L}\).

While Theorem 3.1 shows how to regularize a learning problem with the Jacobian nuclear norm without a cubic-time SVD, the Jacobian computation incurs a quadratic time and memory cost, which remains heavy for high-dimensional learning problems. To mitigate this issue, the following section shows how to approximate the \(\|Jg[h(x)]\|_{F}^{2}\) and \(\|Jh[x]\|_{F}^{2}\) terms in (4).

### Estimating the Jacobian Frobenius norm

When \(f:\mathbb{R}^{n}\to\mathbb{R}^{m}\) is a function between high-dimensional spaces, \(Jf[x]\) is an \(m\times n\) matrix that is costly to compute and to store in memory. Previous works employing Jacobian regularization for neural networks have noted this issue and proposed stochastic approximations based on Jacobian-vector products (JVP) against random vectors (Varga et al., 2017; Hoffman et al., 2020). As JVPs may be costly to compute for large neural nets, we propose an alternative stochastic estimator that requires only evaluations of \(f\) and analyze its error:

**Theorem 3.2**: _Let \(f:\mathbb{R}^{n}\to\mathbb{R}^{m}\) be continuously differentiable. Then,_

\[\sigma^{2}\|Jf[x]\|_{F}^{2}=\operatorname*{\mathbb{E}}_{\epsilon\sim\mathcal{ N}(0,\sigma^{2}I)}\left[\|f(x+\epsilon)-f(x)\|_{2}^{2}\right]+O(\sigma^{2}).\] (5)

Similar results appear in the ML literature as early as Webb (1994). Our proof in Appendix A.3 relies on a first-order Taylor expansion and Hutchinson's trace estimator; a similar proof is given by Alain and Bengio (2014). In practice, we obtain accurate approximations to \(\|Jf[x]\|_{F}^{2}\) by using a small noise variance \(\sigma^{2}\) and rescaling the expectation in (5) by \(\frac{1}{\sigma^{2}}\) to compensate. In Section 4, we also show that a single noise sample \(\epsilon\sim\mathcal{N}(0,\sigma^{2}I)\) suffices in practice.

Using this efficient approximation, we obtain the following regularizer:

\[\mathcal{R}(x;f)=\frac{1}{2\sigma^{2}}\operatorname*{\mathbb{E}}_{\epsilon \sim\mathcal{N}(0,\sigma^{2}I)}\left[\|g(h(x)+\epsilon)-g(h(x))\|_{2}^{2}+\|h (x+\epsilon)-h(x)\|_{2}^{2}\right],\] (6)

where \(f=g\circ h\). In practice, one may use a single draw of \(\epsilon\sim\mathcal{N}(0,\sigma^{2}I)\) per training iteration while maintaining good performance on downstream tasks; see e.g. the results in Section 5. In this case, our regularizer \(\mathcal{R}(x;f)\) costs merely two additional function evaluations, enabling it to scale to large neural networks acting on high-dimensional data. In Section 4, we show that parametrizing \(f_{\theta}\) as a neural net and solving

\[\inf_{f_{\theta}:\mathbb{R}^{n}\to\mathbb{R}^{m}x\sim\mathcal{D}(\Omega)} \left[\ell(f_{\theta}(x),x)+\eta\mathcal{R}(x;f_{\theta})\right]\] (7)

yields good approximations to the solution to (3) for problems where exact solutions are known. We then propose two applications of Jacobian nuclear norm regularization in Section 5.

## 4 Validation

In this section, we empirically validate our method on a special case of (3) for which closed-form solutions are known. We consider the following problem:

\[\inf_{f:\mathbb{R}^{n}\to\mathbb{R}}\int_{\mathbb{R}^{n}}\left[\frac{1}{2}\|f (x)-\tau(x)\|_{2}^{2}+\eta\|Jf[x]\|_{*}\right]\text{d}x,\] (8)

where \(\tau:\mathbb{R}^{n}\to\mathbb{R}\) is the indicator function of the unit ball in \(\mathbb{R}^{n}\). As \(f\) is a scalar-valued function in this problem, \(Jf[x]\) is a _vector_, and \(\|Jf[x]\|_{*}=\|\nabla f(x)\|_{2}\). This is an instance of the celebrated _Rudin-Osher-Fatemi_ (ROF) model for image denoising [Rudin et al., 1992]. Meyer [2001, p. 36] shows that the exact solution to (8) given this target function \(\tau(x)\) is \(f(x):=(1-n\eta)\tau(x)\). This is a rescaled indicator function of the unit ball.

We parametrize \(f\) as a multilayer perceptron (MLP) \(f_{\theta}\) and solve (8) along with the problem using our regularizer:

\[\inf_{f_{\theta}:\mathbb{R}^{n}\rightarrow\mathbb{R}_{x}\sim\mathcal{D}( \Omega)}\left[\frac{1}{2}\|f_{\theta}(x)-\tau(x)\|_{2}^{2}+\eta\mathcal{R}(x;f _{\theta})\right],\] (9)

where \(f_{\theta}=g_{\theta}\circ h_{\theta}\). We approximate the integral over \(\mathbb{R}^{n}\) by Monte Carlo integration over a box \(\Omega\) centered at the origin. We experiment with \(n=2\) and \(n=5\) and in each case depict results for a small and a large regularization value \(\eta\). We track the objective values of problems (8) and (9) and show that they converge to the same value, as predicted by Theorem 3.1. We also track the absolute error of both problem's solutions across training iterations and plot solutions to each problem at convergence for the 2D case. We give full implementation details in Appendix B.1.

Figure 1 depicts the exact solution to (8) for \(n=2\) and two values of \(\eta\), along with neural solutions to the same problem (8) and the problem with our regularizer (9). Solving our Jacobian-free problem (9) with 10 draws of \(\epsilon\) per training iteration yields accurate solutions to the ROF problem for both values of \(\eta\), with our problem yielding slightly diffuse transitions across the boundary of the unit disc.

Figure 2 confirms the accuracy our our method's solutions, which attain absolute error comparable to the neural solutions to (8).

Figure 3 depicts the objective values for Problems (8) and (9) on log scale across training iterations. As predicted by Theorem 3.1, both converge toward nearly identical objective values; the larger gap in Figure 3(c) is an artifact of the loss magnitudes being smaller and the plot being on log scale.

## 5 Applications

Unsupervised denoising.In this section, we apply our regularizer \(\mathcal{R}(x;f_{\theta})\) to _unsupervised denoising_. We learn a denoiser \(f_{\theta}\) that maps noisy images \(x+\epsilon\) to clean images \(x\) given a training set of noisy images _without_ their corresponding clean images. We motivate the use of our regularizer via a connection with denoising by singular value shrinkage, and demonstrate that our unsupervised denoiser nearly matches the performance of a denoiser trained on clean-noisy image pairs.

Singular value shrinkage.A line of work beginning with Shabalin and Nobel (2010) studies denoising by _singular value shrinkage_ (SVS). These works seek to recover a low-rank data matrix \(X\in\mathbb{R}^{D\times N}\) of _unknown_ rank \(r\) given only a single matrix \(Y=X+\sigma_{\epsilon}Z\) of clean data \(X\) corrupted by iid white noise \(Z\). Since the clean data is low-rank, the components of \(Y\) corresponding to its small singular values contain mostly noise, so SVS denoises \(Y\) by applying a shrinkage function \(\phi\) to its singular values \(\sigma_{d}\). This function _shrinks_ small singular values of \(Y\) while leaving larger singular values untouched. For convenience, we denote the denoised matrix by \(\phi(Y)\).

Gavish and Donoho (2017) show that under certain assumptions on the noise \(Z\) and data \(X\), one can derive the optimal shrinker \(\phi\) that asymptotically minimizes \(\|X-\phi(Y)\|_{F}^{2}\). In Appendix A.4, we show that this optimal shrinker is also the solution to the following problem:

\[\min_{A\in\mathbb{R}^{D\times D}}\frac{1}{2N}\|AY-Y\|_{F}^{2}+\eta\|A\|_{*},\] (10)

where we set \(\eta=\sigma_{\epsilon}^{2}\) to be equal to the noise variance. This problem is a special case of the following instance of Problem (3)

\[\inf_{f_{\theta}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}}\underset{y\sim \mathcal{D}(\Omega)}{\mathbb{E}}\left[\frac{1}{2}\|f_{\theta}(y)-y\|_{2}^{2}+ \eta\|Jf_{\theta}[y]\|_{*}\right]\] (11)

when \(\mathcal{D}(\Omega)\) is an empirical distribution over \(N\)_noisy_ training samples and \(f_{\theta}\) is restricted to be a linear map. Just as (10) yields an optimal shrinker for denoising low-rank data which _globally_ lies in a low-dimensional subspace, we conjecture that solving (11) yields an effective denoiser for manifold-supported data such as images, which _locally_ lie near low-dimensional subspaces - even when trained on noisy images. We test this conjecture by solving (11) using a neural denoiser \(f_{\theta}\). To make this problem tractable, we replace \(\|Jf_{\theta}[y]\|_{*}\) with our regularizer \(\mathcal{R}(y;f_{\theta})\), which we compute using a single draw of \(\epsilon\) per training iteration.

Experiments.We train our denoiser by solving (11) with \(\mathcal{D}(\Omega)\) being the empirical distribution over 288k _noisy_ images from the Imagenet training set Russakovsky et al. (2015). Consequently,

\begin{table}
\begin{tabular}{c c c c c} \multicolumn{5}{c}{**PSNR (dB) \(\uparrow\)**} \\ \hline  & \multicolumn{2}{c}{Imagenet} & \multicolumn{2}{c}{CBSD68} \\ \hline
**Method** & \(\sigma=1\) & \(\sigma=2\) & \(\sigma=1\) & \(\sigma=2\) \\ \hline BM3D & \(21.26\pm 2.81\) & \(18.71\pm 2.33\) & \(19.42\pm 1.88\) & \(16.77\pm 1.30\) \\ Ours & \(23.10\pm 3.12\) & \(21.05\pm 2.85\) & \(21.08\pm 2.04\) & \(19.10\pm 1.80\) \\ N2N & \(23.12\pm 3.05\) & \(21.21\pm 3.02\) & \(20.37\pm 1.71\) & \(19.37\pm 1.89\) \\ Supervised & \(23.37\pm 3.25\) & \(21.39\pm 2.97\) & \(21.62\pm 2.28\) & \(19.54\pm 1.95\) \\ \hline \end{tabular}
\end{table}
Table 1: Denoiser performance via average PSNR on held-out images. Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data without access to clean images.

our denoiser does not see any clean images during training. The clean images' channel intensities lie in \([-1,1]\), and we corrupt them with Gaussian noise with standard deviations \(\sigma\in\{1,2\}\). We set \(\eta=\sigma^{2}\) when solving (11). We parametrize the denoiser \(f_{\theta}=g_{\theta}\circ h_{\theta}\) as a Unet [14], letting \(h_{\theta}\) and \(g_{\theta}\) be its downsampling and upsampling blocks, resp.

We benchmark our method against a supervised denoiser trained with the usual MSE loss \(\|f_{\theta}(x+\epsilon)-x\|_{2}^{2}\) on clean-noisy pairs, Lehtinen et al. [20]'s Noise2Noise (N2N) method, which requires access to independent noisy copies of each ground truth image during training, and BM3D, a classical unsupervised denoiser [1]. We implement the supervised and N2N denoisers using the same Unet architecture as our denoiser, and train them on the same dataset with the same hyperparameters. We evaluate the denoisers via their average peak signal-to-noise ratio (PSNR) across the CBSD68 dataset [13] and across 100 randomly-drawn noisy images from the Imagenet validation set, randomly cropped to \(256\times 256\). We provide full details for this experiment in Appendix B.2.

We report each denoiser's performance in Table 1 and include 1-sigma error bars computed across the test images. Despite being trained _exclusively on highly corrupted images_, our denoiser nearly matches the performance of an ordinary supervised denoiser at both noise levels and performs comparably to Noise2Noise, which requires independent noisy copies of each ground truth image during training.

We further illustrate the comparison in Figure 4. All neural methods recover substantially more fine detail than the classical BM3D denoiser, particularly at the larger noise level \(\sigma=2\). Notably, our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data.

We also demonstrate the sparsity-inducing effect of our regularizer on the singular values of \(Jf_{\theta}\) in Figure 5, where we plot the Jacobian singular values of our denoiser and a supervised denoiser at a randomly-drawn validation image corrupted with \(\sigma=2\) Gaussian noise. We normalize the singular values so that each Jacobian's largest singular value is 1 and depict the singular values on log scale. As expected, our denoiser's Jacobian singular values decay more rapidly than those of the supervised denoiser at the same point.

These results show that our regularizer (6) can be used to construct a tractable non-linear generalization of Gavish and Donoho [20]'s optimal shrinker that performs nearly as well as a supervised denoiser on image denoising tasks, despite being trained exclusively on highly corrupted images.

Figure 4: Denoiser performance comparison on held-out image corrupted by Gaussian noise with \(\sigma=1\) (first row) and \(\sigma=2\) (second row). Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data.

Figure 5: Jacobian singular values of supervised denoiser (blue) and our denoiser (orange) evaluated at a noisy held-out image with \(\sigma=2\).

Representation learning.We now apply our method to unsupervised representation learning. We train a deterministic autoencoder consisting of an encoder \(f_{\theta}\) and a decoder \(g_{\phi}\) on the CelebA dataset (Liu et al., 2015) and approximately penalize the Jacobian nuclear norm \(\|J_{f_{\theta}}[x]\|_{*}\) of the encoder at training data \(x\in\mathcal{D}(\Omega)\) using our regularizer \(\mathcal{R}(x;f_{\theta})\). This encourages the encoder to locally behave like a low-rank linear map whose _image_ is low-dimensional. One may interpret this as a deterministic autoencoder with locally low-dimensional latent spaces. We demonstrate that the left-singular vectors of the encoder Jacobian \(J_{f_{\theta}}[x]\) are semantically meaningful directions of variation about training data in latent space. We provide full experimental details in Appendix B.3.

To demonstrate our autoencoder's ability to learn meaningful representations, we select an arbitrary training point \(x\) and traverse the latent space of our regularized autoencoder and an unregularized baseline along rays of the form \(z=f_{\theta}(x)+\alpha u_{\theta}^{d}(x)\), where \(u_{\theta}^{d}(x)\) is the \(d\)-th left-singular vector of the encoder Jacobian \(Jf_{\theta}[x]\). These left-singular vectors form a basis for the image of \(Jf_{\theta}[x]\) and approximate a basis for the tangent space of the latent manifold at \(f_{\theta}(x)\). We depict decoded images along this traversal for our regularized autoencoder in Figure 7 and for the baseline in Figure 6.

The traversals in Figure 7 are semantically meaningful. For instance, a traversal along the first singular vector edits the tint of the decoded image, and a traversal along the second singular vector edits the facial expression of the image's subject. The traversals of the unregularized autoencoder's latent space in Figure 6 edit the colors of the decoded image but are unable to control other attributes.

We also follow Higgins et al. (2017) and visualize traversals along latent variables of a \(\beta\)-VAE at the same training point \(x\) in Figure 8. While the \(\beta\)-VAE is able to discover meaningful directions of variation, the decoded images are highly diffuse, as is typical of VAEs. In contrast, our autoencoder's reconstructions retain finer details. We conjecture that our model's improved capacity results from our autoencoder's ability to learn a locally low-dimensional latent space without constraining its global structure.

## 6 Conclusion

The Jacobian nuclear norm \(\|Jf[x]\|_{*}\) is a natural regularizer for learning problems, where it steers solutions towards having low-rank Jacobians. Such functions are locally sensitive to changes in their inputs in only a few directions, which is an especially desirable prior for data that is supported on a low-dimensional manifold. However, computing \(\|Jf[x]\|_{*}\) naively requires both evaluating a Jacobian and taking its SVD; the combined cost of these operations is prohibitive for the high-dimensional maps \(f\) that often arise in deep learning.

Our work resolves this computational challenge by generalizing a surprising result (2) from matrix learning to non-linear learning problems. As they rely on parametrizing the learned function \(f=g\circ h\) as a composition of functions \(g\) and \(h\), our methods are tailor-made for deep learning, where such parametrizations are ubiquitous. We anticipate that the deep learning community will discover additional applications of Jacobian nuclear norm regularization to make use of our efficient methods.

As an efficient implementation of our regularizer relies on estimating the squared Jacobian Frobenius norm using Hutchinson's trace estimator, some error is inevitable. This error manifests itself in slightly diffuse boundaries in the solutions to the Rudin-Osher-Fatemi problem in Figure 1. However, we do not find this error problematic for high-dimensional applications such as unsupervised denoising as in Section 5. Future implementations of our method may employ more accurate estimators of the squared Jacobian Frobenius norm for applications where accuracy is of paramount concern.

## Acknowledgments and Disclosure of Funding

The MIT Geometric Data Processing Group acknowledges the generous support of Army Research Office grants W911NF2010168 and W911NF2110293, of National Science Foundation grant IIS-2335492, from the CSAIL Future of Data program, from the MIT-IBM Watson AI Laboratory, from the Wistron Corporation, and from the Toyota-CSAIL Joint Research Center.

Christopher Scarvelis acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), funding reference number CGSD3-557558-2021.

## References

* Alain and Bengio (2014) G. Alain and Y. Bengio. What regularized auto-encoders learn from the data-generating distribution. _The Journal of Machine Learning Research_, 15(1):3563-3593, 2014.
* Bishop (1995) C. M. Bishop. Training with noise is equivalent to tikhonov regularization. _Neural Computation_, 7(1):108-116, 1995. doi: 10.1162/neco.1995.7.1.108.
* Cabral et al. (2013) R. Cabral, F. De la Torre, J. P. Costeira, and A. Bernardino. Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition. In _Proceedings of the IEEE international conference on computer vision_, pages 2488-2495, 2013.
* Cai et al. (2010) J.-F. Cai, E. J. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion. _SIAM Journal on Optimization_, 20(4):1956-1982, 2010. doi: 10.1137/080738970. URL https://doi.org/10.1137/080738970.
* Candes and Recht (2012) E. Candes and B. Recht. Exact matrix completion via convex optimization. _Commun. ACM_, 55(6):111-119, jun 2012. ISSN 0001-0782. doi: 10.1145/2184319.2184343. URL https://doi.org/10.1145/2184319.2184343.
* Candes and Tao (2010) E. J. Candes and T. Tao. The power of convex relaxation: near-optimal matrix completion. _IEEE Trans. Inf. Theor._, 56(5):2053-2080, may 2010. ISSN 0018-9448. doi: 10.1109/TIT.2010.2044061. URL https://doi.org/10.1109/TIT.2010.2044061.
* Candes et al. (2011) E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? _J. ACM_, 58(3), jun 2011. ISSN 0004-5411. doi: 10.1145/1970392.1970395. URL https://doi.org/10.1145/1970392.1970395.
* Chen et al. (2018) R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Dabov et al. (2007) K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. _IEEE Transactions on Image Processing_, 16(8):2080-2095, 2007. doi: 10.1109/TIP.2007.901238.
* Dai et al. (2014) Y. Dai, H. Li, and M. He. A simple prior-free method for non-rigid structure-from-motion factorization. _International Journal of Computer Vision_, 107:101-122, 2014.
* Dai et al. (2015)M. Elad, B. Kawar, and G. Vaksman. Image denoising: The deep learning revolution and beyond--a survey paper. _SIAM Journal on Imaging Sciences_, 16(3):1594-1654, 2023. doi: 10.1137/23M1545859. URL https://doi.org/10.1137/23M1545859.
* Finlay et al. (2020) C. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. M. Oberman. How to train your neural ode. _arXiv preprint arXiv:2002.02798_, 2, 2020.
* Fritz (18) T. Fritz. Nuclear norm as minimum of frobenius norm product. MathOverflow. URL https://mathoverflow.net/q/298009. URL:https://mathoverflow.net/q/298009 (version: 2018-04-18).
* Gavish and Donoho (2014) M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is \(4/\sqrt{3}\). _IEEE Transactions on Information Theory_, 60(8):5040-5053, 2014. doi: 10.1109/TIT.2014.2323359.
* Gavish and Donoho (2017) M. Gavish and D. L. Donoho. Optimal shrinkage of singular values. _IEEE Transactions on Information Theory_, 63(4):2137-2152, 2017.
* Higgins et al. (2017) I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl.
* Hoffman et al. (2020) J. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization, 2020. URL https://openreview.net/forum?id=ryl-RTEYvB.
* Hutchinson (1989) M. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. _Communication in Statistics- Simulation and Computation_, 18:1059-1076, 01 1989. doi: 10.1080/03610919008812866.
* Jakubovitz and Giryes (2018) D. Jakubovitz and R. Giryes. Improving dnn robustness to adversarial attacks using jacobian regularization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 514-529, 2018.
* Kadkhodaie et al. (2024) Z. Kadkhodaie, F. Guth, E. P. Simoncelli, and S. Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representations. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=ANvmVS2Yr0.
* Kelly et al. (2020) J. Kelly, J. Bettencourt, M. J. Johnson, and D. K. Duvenaud. Learning differential equations that are easy to solve. _Advances in Neural Information Processing Systems_, 33:4370-4380, 2020.
* Keshavan et al. (2009) R. H. Keshavan, S. Oh, and A. Montanari. Matrix completion from a few entries. In _2009 IEEE International Symposium on Information Theory_, pages 324-328, 2009. doi: 10.1109/ISIT.2009.5205567.
* Lehtinen et al. (2018) J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and T. Aila. Noise2noise: Learning image restoration without clean data. In _International Conference on Machine Learning (ICML)_, volume 80, pages 2971-2980, March 2018.
* Liu et al. (2015) Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* Loshchilov and Hutter (2019) I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* Martin et al. (2001) D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001_, volume 2, pages 416-423 vol.2, 2001. doi: 10.1109/ICCV.2001.937655.
* Mazumder et al. (2010) R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. _Journal of Machine Learning Research_, 11(80):2287-2322, 2010. URL http://jmlr.org/papers/v11/mazumder10a.html.
* Mazumder et al. (2018)Y. Meyer. _Oscillating Patterns in Image Processing and Nonlinear Evolution Equations: The Fifteenth Dean Jacqueline B. Lewis Memorial Lectures_. American Mathematical Society, USA, 2001. ISBN 0821829203.
* Nadakuditi [2014] R. Nadakuditi. Optshrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage. _Information Theory, IEEE Transactions on_, 60:3002-3018, 05 2014. doi: 10.1109/TIT.2014.2311661.
* Recht [2011] B. Recht. A simpler approach to matrix completion. _J. Mach. Learn. Res._, 12(null):3413-3430, dec 2011. ISSN 1532-4435.
* Rennie and Srebro [2005] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In _Proceedings of the 22nd International Conference on Machine Learning_, ICML '05, page 713-719, New York, NY, USA, 2005. Association for Computing Machinery. ISBN 1595931805. doi: 10.1145/1102351.1102441. URL https://doi.org/10.1145/1102351.1102441.
* Rifai et al. [2011] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In _Proceedings of the 28th international conference on international conference on machine learning_, pages 833-840, 2011.
* Rombach et al. [2021] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* Ronneberger et al. [2015] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer, 2015.
* Rudin et al. [1992] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. _Physica D: nonlinear phenomena_, 60(1-4):259-268, 1992.
* Russakovsky et al. [2015] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* Shabalin and Nobel [2010] A. Shabalin and A. Nobel. Reconstruction of a low-rank matrix in the presence of gaussian noise. _Journal of Multivariate Analysis_, 118, 07 2010. doi: 10.1016/j.jmva.2013.03.005.
* Sokolic et al. [2017] J. Sokolic, R. Giryes, G. Sapiro, and M. R. D. Rodrigues. Robust large margin deep neural networks. _Trans. Sig. Proc._, 65(16):4265-4280, aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.2708039. URL https://doi.org/10.1109/TSP.2017.2708039.
* Tancik et al. [2020] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in neural information processing systems_, 33:7537-7547, 2020.
* Varga et al. [2017] D. Varga, A. Csiszarik, and Z. Zombori. Gradient regularization improves accuracy of discriminative models. _arXiv preprint arXiv:1712.09936_, 2017.
* Vincent et al. [2008] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In _Proceedings of the 25th international conference on Machine learning_, pages 1096-1103, 2008.
* Vincent et al. [2010] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, and L. Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. _Journal of machine learning research_, 11(12), 2010.
* Webb [1994] A. Webb. Functional approximation by feed-forward networks: a least-squares approach to generalization. _IEEE Transactions on Neural Networks_, 5(3):363-371, 1994. doi: 10.1109/72.286908.
* Zhang et al. [2021] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte. Plug-and-play image restoration with deep denoiser prior. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(10):6360-6376, 2021.
* Zhang et al. [2015]Proofs

### Proof of Equation (1)

We draw heavy inspiration from a proof by Fritz that appeared on MathOverflow. We will prove that

\[\|A\|_{*}=\min_{U,V:UV^{\top}=A}\frac{1}{2}\left(\|U\|_{F}^{2}+\|V\|_{F}^{2} \right).\]

First suppose that \(U,V\) are matrices such that \(UV^{\top}=A\). By the matrix Holder inequality,

\[\|A\|_{*}=\|UV^{\top}\|_{*}\leq\|U\|_{F}\cdot\|V\|_{F}.\]

By the arithmetic mean-geometric mean (AM-GM) inequality,

\[\|U\|_{F}\cdot\|V\|_{F}=\sqrt{\|U\|_{F}^{2}\cdot\|V\|_{F}^{2}}\leq\frac{1}{2} \left(\|U\|_{F}^{2}+\|V\|_{F}^{2}\right).\]

Combining these results, we obtain

\[\|A\|_{*}=\|UV^{\top}\|_{*}\leq\inf_{U,V:UV^{\top}=A}\frac{1}{2}\left(\|U\|_{F }^{2}+\|V\|_{F}^{2}\right).\]

To see that the inf is attained at \(\|A\|_{*}\), take the compact SVD \(A=U_{A}\Sigma V_{A}^{\top}\) and set \(U:=U_{A}\sqrt{\Sigma},V:=V_{A}\sqrt{\Sigma}\). Then clearly \(UV^{\top}=A\), and

\[\frac{1}{2}\left(\|U\|_{F}^{2}+\|V\|_{F}^{2}\right)=\frac{1}{2} \left(\|U_{A}\sqrt{\Sigma}\|_{F}^{2}+\|V_{A}\sqrt{\Sigma}\|_{F}^{2}\right)= \frac{1}{2}\left(\|\sqrt{\Sigma}\|_{F}^{2}+\|\sqrt{\Sigma}\|_{F}^{2}\right)\\ =\frac{1}{2}\left(\sum_{i=1}^{r}\sigma_{i}+\sum_{i=1}^{r}\sigma_{ i}\right)=\|A\|_{*}\]

This proves Equation (1). An alternative proof using somewhat more complex methods is given in Mazumder et al. (2010). \(\blacksquare\)

### Proof of Theorem 3.1

Let \(D(\Omega)\) be a data distribution with measure \(\mu\) supported on a compact set \(\Omega\subseteq\mathbb{R}^{n}\) that is absolutely continuously with respect to the Lebesgue measure \(\lambda(\Omega)\), and let \(\ell\in C^{1}(\mathbb{R}^{m})\) be a continuously differentiable loss function. Let \(C^{\infty}(\Omega)\) denote the set of infinitely differentiable functions on \(\Omega\). We will show that:

\[\inf_{f\in C^{\infty}(\Omega)}\underset{x\sim\mathcal{D}(\Omega) }{\mathbb{E}}[\ell(f(x),x)+\eta\|Jf[x]\|_{*}]\\ =\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}\underset{x\sim\mathcal{D}(\Omega)} {\mathbb{E}}\left[\ell(g(h(x)),x)+\frac{\eta}{2}\left(\|Jg[h(x)]\|_{F}^{2}+\| Jh[x]\|_{F}^{2}\right)\right].\] (12)

We denote the left-hand side objective by \(E_{L}(f)\) and its inf by \((L)\); we denote the right-hand side objective by \(E_{R}(g,h)\) and its inf by \((R)\). We prove that \((L)\leq(R)\) and \((R)\leq(L)\).

#### a.2.1 \((L)\leq(R)\)

This is the easy direction. The basic observation is that if we parametrize \(f=g\circ h\), then \(Jf[x]=Jg[h(x)]Jh[x]\), and Srebro's identity (1) tells us that

\[\|Jf[x]\|_{*}=\min_{U,V:UV^{\top}=Jf[x]}\frac{1}{2}\left(\|U\|_{F}^{2}+\|V\|_{ F}^{2}\right)\leq\frac{1}{2}\left(\|Jg[h(x)]\|_{F}^{2}+\|Jh[x]\|_{F}^{2}\right)\]The rest of the proof is book-keeping.

We first verify that \(C^{\infty}(\Omega)\) is closed under composition by showing that:

\[C^{\infty}(\Omega)=\left\{g\circ h:h\in C^{\infty}(\Omega),g\in C^{\infty}(h( \Omega))\right\}.\] (13)

The \(\subseteq\) inclusion is straightforward: given any \(f\in C^{\infty}(\Omega)\), just choose \(h\equiv f\) and \(g\equiv\text{Id}\); these functions are clearly in the correct classes. The \(\supseteq\) inclusion follows from the fact that the composition of \(C^{\infty}\) functions is \(C^{\infty}\) by the chain rule.

This yields:

\[\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)=\inf_{\begin{subarray}{c}h\in C^{ \infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}E_{L}(g\circ h).\]

We now use Srebro's identity as follows:

\[\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}E_{L}(g\circ h) =\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}\operatorname*{\mathbb{E}}_{x\sim \mathcal{D}(\Omega)}\left[\ell(g(h(x)),x)+\eta\|J(g\circ h)[x]\|_{*}\right]\] \[=\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}\operatorname*{\mathbb{E}}_{x\sim \mathcal{D}(\Omega)}\left[\ell(g(h(x)),x)+\eta\|Jg[h(x)]Jh[x]\|_{*}\right]\] \[\leq\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}\operatorname*{\mathbb{E}}_{x\sim \mathcal{D}(\Omega)}\left[\ell(g(h(x)),x)+\frac{\eta}{2}\left(\|Jg[h(x)]\|_{F}^ {2}+\|Jh[x]\|_{F}^{2}\right)\right]\] \[=\inf_{\begin{subarray}{c}h\in C^{\infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}E_{R}(g\circ h).\]

Consequently,

\[\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)\leq\inf_{\begin{subarray}{c}h\in C^{ \infty}(\Omega)\\ g\in C^{\infty}(h(\Omega))\end{subarray}}E_{R}(g\circ h)\]

and hence \((L)\leq(R)\).

#### a.2.2 \((R)\leq(L)\)

This is the hard direction. The proof strategy is as follows:

1. We begin with a function \(f_{m}\in C^{\infty}(\Omega)\) such that \(E_{L}(f_{m})\) is arbitrarily close to its inf over \(C^{\infty}(\Omega)\). We use \(f_{m}\) to construct parametric families of functions \(g_{m}^{z},h_{m}^{z}\) whose composition is a good local approximation to \(f_{m}\) in a neighborhood of \(z\in\Omega\), both pointwise and in terms of the energy contributions due to \(x\in\Omega\) near \(z\). This step relies crucially on our ability to construct optimal solutions to the RHS problem in (1).
2. We then stitch together these local approximations to form a sequence of global approximations \(g_{m}^{k},h_{m}^{k}\) to \(f_{m}\). These functions are piecewise affine and hence not regular enough to lie in \(C^{\infty}(\Omega)\) as required by the right-hand side of Equation (12).
3. Finally, mollifying the piecewise affine functions \(g_{m}^{k},h_{m}^{k}\) yields a minimizing sequence of \(C^{\infty}(\Omega)\) functions \(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k}\) such that \(E_{R}(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k})\) approaches the inf of \(E_{L}\).

Local approximations to \(f_{m}\).To begin, let \(f_{m}\in C^{\infty}(\Omega)\) be a function that attains \(E_{L}(f_{m})\leq\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)+\frac{1}{m}\). Fix some \(z\in\Omega\), take the thin SVD of \(Jf_{m}[z]=U_{m}(z)\Sigma_{m}(z)V_{m}(z)^{\top}\), and use it to define two parametric families of affine functions:

\[h_{m}^{z}(x)=\sqrt{\Sigma_{m}(z)}V_{m}(z)^{\top}x,\]\[g_{m}^{z}(y)=U_{m}(z)\sqrt{\Sigma_{m}(z)}y+f_{m}(z)-Jf_{m}[z]z.\]

These functions satisfy two key properties:

\[g_{m}^{z}\left(h_{m}^{z}(x)\right)=f_{m}(z)+Jf_{m}[z](x-z)=f_{m}(x)+R_{m}^{z}(x),\] (14)

where \(\|R_{m}^{z}(x)\|_{2}\in O(\|x-z\|_{2}^{2})\) by Taylor's theorem, and

\[\frac{\eta}{2}\left(\|Jg_{m}^{z}[h_{m}^{z}(x)]\|_{F}^{2}+\|Jh_{m}^{z}[x]\|_{F }^{2}\right)=\frac{\eta}{2}\left(\|\sqrt{\Sigma_{m}(z)}\|_{F}^{2}+\|\sqrt{ \Sigma_{m}(z)}\|_{F}^{2}\right)=\eta\|Jf_{m}[z]\|_{*}.\] (15)

Using (14), we obtain

\[\ell(g_{m}^{z}(h_{m}^{z}(x)),x)=\ell(f_{m}(x)+R_{m}^{z}(x),x).\] (16)

The continuity of \(\ell\) ensures that we can make \(\ell(f_{m}(x)+R_{m}^{z}(x),x)\) arbitrarily close to \(\ell(f_{m}(x),x)\) by making \(\|x-z\|_{2}\) sufficiently small.

Furthermore,

\[\|Jf_{m}[z]\|_{*}=\|Jf_{m}[x]+Jf_{m}[z]-Jf_{m}[x]\|_{*}\leq\|Jf_{m}[x]\|_{*}+ \|Jf_{m}[z]-Jf_{m}[x]\|_{*},\] (17)

and as \(f_{m}\in C^{\infty}\), \(Jf_{m}\) is a continuous function, so we can make \(\|Jf_{m}[z]-Jf_{m}[x]\|_{*}\) arbitrarily small by making \(\|x-z\|\) sufficiently small.

The compositions of these functions \(g_{m}^{z},h_{m}^{z}\) are good local approximations to \(f_{m}\) in a neighborhood of \(z\), both pointwise (by (14)) and in terms of the energy contributions arising from \(x\in\Omega\) near \(z\) (by (16) and (17)).

Global piecewise affine approximations to \(f_{m}\).We now stitch together these local approximations to form a sequence of global approximations \(g_{m}^{k},h_{m}^{k}\) to \(f_{m}\).

For each \(k\), fix a set of points \(Z_{k}=\{z_{i}\}_{i=1}^{N(k)}\) and use this set to partition \(\Omega\) into Voronoi regions \(V_{i}\). Choose the centroids \(z_{i}\) such that \(\max_{x\in V_{i}}\|x-z_{i}\|_{2}\leq\epsilon_{k}\), for \(\epsilon_{k}>0\) sufficiently small to ensure that \(\|Jf_{m}[z_{i}]\|_{*}-\|Jf_{m}[x]\|_{*}|+|\ell(f_{m}(x)+R_{m}^{z_{i}}(x),x)- \ell(f_{m}(x),x)|<\frac{1}{k}\) for all \(x\in V_{i}\) and for all regions \(V_{i}\). (The compactness of \(\Omega\) and the uniform continuity of \(\|Jf_{m}\|_{*}\) on \(\Omega\) and \(\ell\) on its domain ensures that we can always find a finite set of centroids with this property.)

Then let \(g_{m}^{k},h_{m}^{k}\) be piecewise affine functions such that \(g_{m}^{k}(x):=g_{m}^{z_{i}}(x)\) and \(h_{m}^{k}(x):=h_{m}^{z_{i}}(x)\) for all \(x\in\text{int}(V_{i})\). For all points \(x\) on a Voronoi boundary, define \(g_{m}^{k},h_{m}^{k}\) by averaging over the interiors of the Voronoi cells incident on the boundary. This yields:\[\begin{split}&\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}(\Omega)}\left[ \ell(g_{m}^{k}(h_{m}^{k}(x)),x)+\frac{\eta}{2}\left(\|Jg_{m}^{k}[h_{m}^{k}(x) ]\|_{F}^{2}+\|Jh_{m}^{k}[x]\|_{F}^{2}\right)\right]\\ &=\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}(\Omega)}\left[ \sum_{i=1}^{N(k)}\left[\ell(f_{m}(x)+R_{m}^{z_{i}}(x)),x\right)+\eta\|Jf_{m}[ z_{i}]\|_{*}\right]\cdot\mathds{1}\left[x\in V_{i}\right]\\ &\leq\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}(\Omega)}\left[ \sum_{i=1}^{N(k)}\left[\ell(f_{m}(x),x)+\eta\|Jf_{m}[x]\|_{*}+\frac{1}{k} \right]\cdot\mathds{1}\left[x\in V_{i}\right]\right]\\ &=\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}(\Omega)}\left[ \ell(f_{m}(x),x)+\eta\|Jf_{m}[x]\|_{*}+\frac{1}{k}\right]\\ &=E_{L}(f_{m})+\frac{1}{k}\\ &\leq\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)+\frac{1}{m}+\frac{1} {k}\end{split}\]

and hence \(E_{R}(g_{m}^{k},h_{m}^{k})\leq\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)+\frac{1}{ m}+\frac{1}{k}\).

Mollifying the piecewise affine approximations.The functions \(g_{m}^{k},h_{m}^{k}\) constructed in the previous section are piecewise affine and hence not regular enough to lie in \(C^{\infty}(\Omega)\) as required by \((R)\). We now mollify these functions to yield a minimizing sequence of \(C^{\infty}(\Omega)\) functions \(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k}\) such that \(E_{R}(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k})\) approaches \(\inf_{f\in C^{\infty}(\Omega)}E_{L}(f)\).

We mollify \(g_{m}^{k},h_{m}^{k}\) by convolving them against the standard mollifiers (infinitely differentiable and compactly supported on \(B(0,\epsilon)\)) to yield a sequence of \(C^{\infty}(\Omega)\) functions \(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k}\). We need to show that

\[E_{R}(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k})\leq E_{R}(g_{m}^{k},h_{m}^{k})+ \psi(\epsilon;m,k)\]

for some error \(\psi(\epsilon;m,k)\) that vanishes as \(\epsilon\to 0\) for any \(m,k\). We proceed by individually controlling the terms in \(E_{R}(g_{m,\epsilon}^{k},h_{m,\epsilon}^{k})\).

Controlling the error in \(\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}(\Omega)}\left[\ell(g_{m, \epsilon}^{k}(h_{m,\epsilon}^{k}(x)),x)\right]\).We first show that it suffices to prove that \(\|g_{m,\epsilon}^{k}\circ h_{m,\epsilon}^{k}-g_{m}^{k}\circ h_{m}^{k}\|_{L^{ 1}(\Omega,\mu)}\to 0\) for any \(m,k\). (Recall that \(\mu\) is the measure associated with the data distribution \(\mathcal{D}(\Omega)\).)

Note that as \(\ell\in C^{1}(\mathbb{R}^{m})\) and the image of the compact set \(\Omega\) under the piecewise affine function \(g_{m}^{k}\circ h_{m}^{k}\) is bounded, \(\ell\) is \(L\)-Lipschitz on \(g_{m}^{k}(h_{m}^{k}(\Omega))\). For any sequence of functions \(f_{n}\) converging to \(f\) in \(L^{1}\), we then have:

\[\begin{split}\left|\int_{\Omega}\ell(f_{n}(x),x)d\mu-\int_{ \Omega}\ell(f(x),x)d\mu\right|&\leq\int_{\Omega}\left|\ell(f_{n}( x),x)-\ell(f(x),x)\right|d\mu\\ &\leq\int_{\Omega}L\|f_{n}(x)-f(x)\|_{2}d\mu\\ &=L\int_{\Omega}\|f_{n}(x)-f(x)\|_{2}d\mu\\ &=L\|f_{n}-f\|_{L^{1}(\Omega,\mu)}\\ &\underset{\epsilon\to 0}{\rightarrow}0\end{split}\]

It therefore suffices to show that \(\|g_{m,\epsilon}^{k}\circ h_{m,\epsilon}^{k}-g_{m}^{k}\circ h_{m}^{k}\|_{L^{ 1}(\Omega,\mu)}\to 0\) for any \(m,k\). To this end, we will use the bound 

[MISSING_PAGE_FAIL:17]

We begin by showing that \(\|g_{m,\epsilon}^{k}(h_{m,\epsilon}^{k}(x))-g_{m,\epsilon}^{k}(h_{m}^{k}(x))\|_{2}=0\) for all \(x\in\Omega_{0}(m,k,\epsilon)\). To this end, first recall that the standard mollifier supported on \(B(0,\epsilon)\) is defined as follows:

\[\eta_{\epsilon}(y)=\begin{cases}C(\epsilon)\exp\left(\frac{1}{\|x\|_{2}^{2}-1 }\right)&\text{for }\|y\|_{2}\leq 1\\ 0&\text{for }\|y\|_{2}>1\end{cases}\] (20)

where \(C(\epsilon)>0\) is chosen to ensure that \(\eta_{\epsilon}\) integrates to 1. Now, if \(x\in\Omega_{0}(m,k,\epsilon)\), then \(d(x,S_{m}^{k})>\epsilon\) and hence \(B(x,\epsilon)\) is entirely contained in the Voronoi cell \(V_{i}\) containing \(x\). (Note that \(x\) cannot lie on a Voronoi boundary, as we have stripped the \(\mu\)-measure zero set of Voronoi boundaries \(S_{m}^{k}\) from \(\Omega\) before constructing \(\Omega_{0}(m,k,\epsilon)\) and \(\Omega_{1}(m,k,\epsilon)\).) On this ball \(B(x,\epsilon)\), the linearity of \(h_{m}^{k}(y):=\sqrt{\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\top}y\) and the rotational symmetry of the mollifier \(\eta_{\epsilon}\) yields:

\[h_{m,\epsilon}^{k}(x) =\int_{B(0,\epsilon)}h_{m}^{k}(y)\eta_{\epsilon}(x-y)dy\] (21) \[=\int_{B(0,\epsilon)}\sqrt{\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\top}y \eta_{\epsilon}(x-y)dy\] (22) \[=\sqrt{\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\top}\underbrace{\int_{B( 0,\epsilon)}y\eta_{\epsilon}(x-y)dy}_{=x}\] (23) \[=\sqrt{\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\top}y\] (24) \[=h_{m}^{k}(x).\] (25)

Hence for all \(x\in\Omega_{0}(m,k,\epsilon)\), \(h_{m,\epsilon}^{k}(x)=h_{m}^{k}(x)\), and it follows that \(g_{m,\epsilon}^{k}(h_{m,\epsilon}^{k}(x))=g_{m,\epsilon}^{k}(h_{m}^{k}(x))\) and therefore \(\|g_{m,\epsilon}^{k}(h_{m,\epsilon}^{k}(x))-g_{m,\epsilon}^{k}(h_{m}^{k}(x)) \|_{2}=0\). We conclude that

\[\int_{\Omega_{0}(m,k,\epsilon)}\|g_{m,\epsilon}^{k}(h_{m,\epsilon}^{k}(x))-g_ {m,\epsilon}^{k}(h_{m}^{k}(x))\|_{2}d\mu=0.\]

To bound the second term, note that the following inequalities hold \(\mu\)-almost everywhere on \(\Omega\):

\[\|g_{m,\epsilon}^{k}(h_{m,\epsilon}^{k}(x))-g_{m,\epsilon}^{k}(h_{m}^{k}(x)) \|_{2}\leq 2\sup_{y\in h_{m}^{k}(\Omega(m,k))}\|g_{m,\epsilon}^{k}(y)\|_{2} \leq 2\sup_{y\in h_{m}^{k}(\Omega(m,k))}\|g_{m}^{k}(y)\|_{2}=:M(m,k).\] (26)

The second inequality can be derived using Jensen's inequality by first showing the following for all \(y\in h_{m}^{k}(\Omega(m,k))\):

\[\|g_{m,\epsilon}^{k}(y)\|_{2} =\|\int_{B(y,\epsilon)}g_{m}^{k}(z)\underbrace{\eta_{\epsilon}(z) dz}_{:=d\eta_{\epsilon}(z)}\] \[\underbrace{\leq}_{\text{\sc Jensen}}\int_{B(y,\epsilon)}\|g_{m}^ {k}(z)\|_{2}d\eta_{\epsilon}(z)\] \[\leq\sup_{y\in h_{m}^{k}(\Omega(m,k))}\|g_{m}^{k}(y)\|_{2} \underbrace{\int_{B(y,\epsilon)}d\eta_{\epsilon}(z)}_{=1}\] \[=\sup_{y\in h_{m}^{k}(\Omega(m,k))}\|g_{m}^{k}(y)\|_{2},\]

which implies that \[\sup_{y\in h^{k}_{m}(\Omega(m,k))}\|g^{k}_{m,\epsilon}(y)\|_{2}\leq\sup_{y\in h^{k} _{m}(\Omega(m,k))}\|g^{k}_{m}(y)\|_{2}.\]

Returning to (26), we can bound the integral over the bad set \(\int_{\Omega_{1}(m,k,\epsilon)}\|g^{k}_{m,\epsilon}(h^{k}_{m,\epsilon}(x))-g^{ k}_{m,\epsilon}(h^{k}_{m}(x))\|_{2}d\mu\) as follows:

\[\int_{\Omega_{1}(m,k,\epsilon)}\|g^{k}_{m,\epsilon}(h^{k}_{m,\epsilon}(x))-g^{ k}_{m,\epsilon}(h^{k}_{m}(x))\|_{2}d\mu\leq M(m,k)\cdot\mu(\Omega_{1}(m,k, \epsilon))\]

where \(\mu(\Omega_{1}(m,k,\epsilon))\) is the measure of \(\Omega_{1}(m,k,\epsilon)\). We will now show that \(\mu(\Omega_{1}(m,k,\epsilon))\to 0\), which will imply

\[\int_{\Omega_{1}(m,k,\epsilon)}\|g^{k}_{m,\epsilon}(h^{k}_{m,\epsilon}(x))-g^ {k}_{m,\epsilon}(h^{k}_{m}(x))\|_{2}d\mu\underset{\epsilon\to 0}{\rightarrow}0,\]

and therefore allow us to conclude that

\[\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m,\epsilon}\circ h^{k}_{ m}\|^{2}_{L^{1}(\Omega,\mu)}\underset{\epsilon\to 0}{\rightarrow}0.\]

Proving that \(\mu(\Omega_{1}(m,k,\epsilon))\to 0\).Recall that:

\[\Omega_{1}(m,k,\epsilon):=\left\{x\in\Omega(m,k):d(x,S^{k}_{m})\leq\epsilon \right\},\] (27)

where \(d(x,S^{k}_{m})\) denotes the distance from the point \(x\) to the union of Voronoi boundaries \(S^{k}_{m}\). Note that this set is a union of cylinders of radius \(\epsilon\) centered at the Voronoi boundaries \(S^{k}_{m}\). As \(S^{k}_{m}\) has Lebesgue measure 0, the Lebesgue measure of the cylinders \(B(m,k,\epsilon)\) also goes to 0 as the radius \(\epsilon\to 0\). The absolute continuity of \(\mu\) then implies that \(\mu(\Omega_{1}(m,k,\epsilon))\to 0\) as well.

Using these results, we obtain

\[\int_{\Omega_{1}(m,k,\epsilon)}\|g^{k}_{m,\epsilon}(h^{k}_{m,\epsilon}(x))-g^ {k}_{m,\epsilon}(h^{k}_{m}(x))\|_{2}d\mu\underset{\epsilon\to 0}{ \rightarrow}0,\]

and therefore conclude that

\[\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m,\epsilon}\circ h^{k}_{ m}\|^{2}_{L^{1}(\Omega,\mu)}\underset{\epsilon\to 0}{\rightarrow}0.\]

This completes the proof that \(\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m,\epsilon}\circ h^{k}_{ m}\|_{L^{1}(\Omega,\mu)}\to 0\) as \(\epsilon\to 0\).

Controlling the second term \(\|g^{k}_{m,\epsilon}\circ h^{k}_{m}-g^{k}_{m}\circ h^{k}_{m}\|_{L^{1}(\Omega,\mu)}\) is easier. Indeed, \(g^{k}_{m,\epsilon}\circ h^{k}_{m}\to g^{k}_{m}\circ h^{k}_{m}\) pointwise a.e. as \(\epsilon\to 0\) by standard properties of mollifiers. Furthermore, the sequence \(g^{k}_{m,\epsilon}\circ h^{k}_{m}\) is dominated almost everywhere by the function identically equal (componentwise) to \(\max_{x\in\Omega(m,k)}g^{k}_{m}(h^{k}_{m}(x))\); this function is in \(L^{1}(\Omega,\mu)\) because \(\Omega\) is a compact domain. It follows from the dominated convergence theorem that \(\|g^{k}_{m,\epsilon}\circ h^{k}_{m}-g^{k}_{m}\circ h^{k}_{m}\|_{L^{1}(\Omega, \mu)}\to 0\) as \(\epsilon\to 0\).

We have shown that each of the following RHS terms goes to 0 as \(\epsilon\to 0\):

\[\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m}\circ h^{k}_{m}\|_{L^{1} (\Omega,\mu)}\leq\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m, \epsilon}\circ h^{k}_{m}\|_{L^{1}(\Omega,\mu)}+\|g^{k}_{m,\epsilon}\circ h^{k} _{m}-g^{k}_{m}\circ h^{k}_{m}\|_{L^{1}(\Omega,\mu)},\]

which allows us to conclude that \(\|g^{k}_{m,\epsilon}\circ h^{k}_{m,\epsilon}-g^{k}_{m}\circ h^{k}_{m}\|_{L^{1} (\Omega,\mu)}\to 0\) and consequently

\[\left|\underset{x\sim\mathcal{D}(\Omega)}{\mathbb{E}}\left[\ell(g^{k}_{m, \epsilon}(h^{k}_{m,\epsilon}(x)),x)\right]-\underset{x\sim\mathcal{D}(\Omega) }{\mathbb{E}}\left[\ell(g^{k}_{m}(h^{k}_{m}(x)),x)\right]\right|\underset{ \epsilon\to 0}{\rightarrow}0\]

[MISSING_PAGE_FAIL:20]

### Proof of Theorem 3.2

We will show that

\[\sigma^{2}\|Jf[x]\|_{F}^{2}=\mathop{\mathbb{E}}_{\epsilon\sim\mathcal{N}(0,\sigma^ {2}I)}\big{[}\|f(x+\epsilon)-f(x)\|_{2}^{2}\big{]}+O(\sigma^{2}).\]

Since \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\) is continuously differentiable, Taylor's theorem states that:

\[f(x+\epsilon)=f(x)+Jf[x]\epsilon+R(x+\epsilon),\]

where \(\|R(x+\epsilon)\|_{2}\in O(\|\epsilon\|_{2}^{2})\). Rearranging, taking squared Euclidean norms, and expanding the square, we obtain:

\[\|Jf[x]\epsilon\|_{2}^{2} =\|f(x+\epsilon)-f(x)-R(x+\epsilon)\|_{2}^{2}\] \[=\|f(x+\epsilon)-f(x)\|_{2}^{2}-2\cdot\langle f(x+\epsilon)-f(x),R(x+\epsilon)\rangle+\underbrace{\|R(x+\epsilon)\|_{2}^{2}}_{\in O(\|\epsilon \|_{2}^{2})}\] \[\leq\|f(x+\epsilon)-f(x)\|_{2}^{2}+2\underbrace{\|R(x+\epsilon) \|_{2}}_{\in O(\|\epsilon\|_{2}^{2})}\cdot\underbrace{\|f(x+\epsilon)-f(x)\|_ {2}}_{\in O(\|\epsilon\|_{2})}+O(\|\epsilon\|_{2}^{4})\] \[=\|f(x+\epsilon)-f(x)\|_{2}^{2}+O(\|\epsilon\|_{2}^{3})\] \[=\|f(x+\epsilon)-f(x)\|_{2}^{2}+O(\|\epsilon\|_{2}^{2}).\]

Hutchinson's trace estimator implies that for any matrix \(A\), \(\|A\|_{F}^{2}=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}[\|A\epsilon\|_{2}^{2}]\). In particular,

\[\sigma^{2}\|Jf[x]\|_{F}^{2} =\mathop{\mathbb{E}}_{\epsilon\sim\mathcal{N}(0,\sigma^{2}I)} \big{[}\|Jf[x]\epsilon\|_{2}^{2}\big{]}\] \[=\mathop{\mathbb{E}}_{\epsilon\sim\mathcal{N}(0,\sigma^{2}I)} \big{[}\|f(x+\epsilon)-f(x)\|_{2}^{2}+O(\|\epsilon\|_{2}^{3})\big{]}\] \[=\mathop{\mathbb{E}}_{\epsilon\sim\mathcal{N}(0,\sigma^{2}I)} \big{[}\|f(x+\epsilon)-f(x)\|_{2}^{2}\big{]}+O(\underbrace{\mathbb{E}\| \epsilon\|_{2}^{2}}_{=\sigma^{2}n})\] \[=\mathop{\mathbb{E}}_{\epsilon\sim\mathcal{N}(0,\sigma^{2}I)} \big{[}\|f(x+\epsilon)-f(x)\|_{2}^{2}\big{]}+O(\sigma^{2}),\]

which completes the proof of the result.

### Optimal shrinkage via nuclear norm regularization

Let \(X\in R^{D\times N}\) be a low-rank matrix of clean data and \(Y=X+\sigma_{\epsilon}Z\) be a matrix of data corrupted by iid white noise \(Z\). In this appendix, we show that the solution to

\[\min_{A\in\mathbb{R}^{D\times D}}\frac{1}{2N}\|AY-Y\|_{F}^{2}+\eta\|A\|_{*},\] (28)

coincides with Gavish and Donoho (2017)'s optimal shrinker for the squared Frobenius norm loss when \(\eta\) is set to the noise variance \(\sigma_{\epsilon}^{2}\) and as the "aspect ratio" \(\beta:=\frac{d}{n}\to 0\).

\(A^{*}\) is optimal for problem (28) iff \(0\in\partial\phi(A^{*})\). Using well-known results from convex optimization, this condition is equivalent to:

\[\frac{1}{N\eta}(Y-A^{*}Y)Y^{\top}\in\partial\|\cdot\|_{*}(A^{*}).\] (29)

Furthermore, the subgradient of the nuclear norm is:\[\partial\|\cdot\|_{*}(A)=\left\{U_{A}V_{A}^{\top}+W:U_{A}^{\top}W=0,WV_{A}=0, \sigma_{\max}(W)\leq 1\right\},\] (30)

where \(A=U_{A}\Sigma_{A}V_{A}^{\top}\) is an SVD of \(A\). We will show that the solution to (28) is

\[A^{*}=U\Gamma U^{\top},\] (31)

where \(Y=U\Sigma V^{\top}\) is an SVD of the noisy data matrix, and

\[\Gamma_{d}=\begin{cases}1-\frac{N\eta}{\sigma_{d}^{2}},&\sigma_{d}\geq\sqrt{N \eta}\\ 0,&\sigma_{d}\leq\sqrt{N\eta}.\end{cases}\] (32)

The idea is to use the SVD \(Y=U\Sigma V^{\top}\) and the ansatz \(A^{*}=U\Gamma U^{\top}\) to rewrite the LHS of the inclusion (29) as follows:

\[\frac{1}{N\eta}(Y-A^{*}Y)Y^{\top}=U\left(\frac{1}{N\eta}(I-\Gamma)\Sigma^{2} \right)U^{\top}.\]

We then express the middle diagonal term as follows:

\[\frac{1}{N\eta}(I-\Gamma)\Sigma^{2}=I_{T}+\frac{1}{N\eta}(I-\Gamma)\Sigma^{2} -I_{T}\]

where \(I_{T}\) is the identity matrix with all columns \(\geq T\) set to zero (i.e. an orthogonal projection matrix onto the first \(T\) coordinates). This then yields

\[U\left(\frac{1}{N\eta}(I-\Gamma)\Sigma^{2}\right)U^{\top}=U_{T}U_{T}^{\top}+U \left(\frac{1}{N\eta}(I-\Gamma)\Sigma^{2}-I_{T}\right)U^{\top},\]

where \(U_{T}=UI_{T}\) is \(U\) with all columns \(\geq T\) set to 0, and \(T\) is the first index such that \(\sigma_{T}\leq\sqrt{N\eta}\). As the optimal \(\Gamma\) in (32) sets all entries corresponding to singular values \(\sigma_{d}\leq\sqrt{N\eta}\) to zero, we can in fact rewrite \(A^{*}=(UI_{T})\Gamma(UI_{T})^{\top}\). It follows that \(U_{T}U_{T}^{\top}=UI_{T}(UI_{T})^{\top}\) is also of the form \(U_{A}V_{A}^{\top}\) for a valid SVD of \(A^{*}\) (the \(UI_{T}\) can serve as both left- and right-singular vectors).

We have therefore expressed the LHS of the inclusion (29) as:

\[\frac{1}{N\eta}(Y-A^{*}Y)Y^{\top}=U_{A}V_{A}^{\top}+W\]

for \(U_{A}V_{A}^{\top}=U_{T}U_{T}^{\top}\) and \(W=U\left(\frac{1}{N\eta}(I-\Gamma)\Sigma^{2}-I_{T}\right)U^{\top}\). This \(W\) satisfies all of the conditions in (30).

Furthermore, if we apply this optimal \(A^{*}\) to the data matrix \(Y\), we obtain \(A^{*}Y=U\Gamma\Sigma V^{\top}\), where

\[(\Gamma\Sigma)_{d}=\begin{cases}\frac{\sigma_{d}^{2}-N\eta}{\sigma_{d}},& \sigma_{d}^{2}\geq N\eta\\ 0,&\sigma_{d}^{2}\leq N\eta\end{cases}\] (33)

If we set \(\eta\) to be equal to the noise variance \(\sigma_{\epsilon}^{2}\), then this agrees exactly with the optimal shrinker for the case \(\beta:=\frac{D}{N}\to 0\) from Gavish and Donoho (2017) under the same noise model \(Y=X+\sigma_{\epsilon}Z\).

## Appendix B Experimental details

### Validation experiments: Rudin-Osher-Fatemi (ROF) problem

Architecture.In all ROF experiments, we parametrize \(f_{\theta}=g_{\theta}\circ h_{\theta}\), where \(g_{\theta}\) and \(h_{\theta}\) are both two-layer MLPs with 100 hidden units. We apply a Fourier feature mapping (Tancik et al., 2020) to the input coordinates \(x\) before passing them through \(h_{\theta}\). We use ELU activations in both neural nets and find the use of differentiable non-linearities to be crucial for obtaining accurate solutions.

Training details.We train all neural models using the AdamW optimizer (Loshchilov and Hutter, 2019) at a learning rate of \(10^{-4}\) for \(100{,}000\) iterations with a batch size of \(10{,}000\). In the \(n=2\) case, we integrate over the box \([-10,10]^{2}\), and in the \(n=5\) case, we integrate over the box \([-2,2]^{5}\).

We employ a warmup strategy for solving our problem (9). We first train our neural nets at \(\eta=0.05\) in the \(n=2\) case and \(\eta=0.01\) in the \(n=5\) case for \(10{,}000\) iterations, and then increase \(\eta\) by \(0.05\) and \(0.01\), respectively, each \(10{,}000\) iterations until we reach the desired value of \(\eta\). We then continue training until we reach \(100{,}000\) total iterations.

Each training run for (8) takes approximately 2 hours, and each training run for (9) takes approximately 45 minutes on a single V100 GPU.

### Denoising

Architecture.Our architecture for all denoising models is based on the UNet implemented in the Github repository for Zhang et al. (2021). Each model is of the form \(f=g\circ h\), where \(h\) consists of the head, downsampling blocks, and body block of the Unet, and \(g\) consists of a repeated body block, the upsampling blocks, and the tail. We replace all ReLU activations with ELU but leave the remainder of the architecture unchanged.

Training details.All neural models are trained on \(288{,}049\) images from the ImageNet Large Scale Visual Recognition Challenge 2012 training set (Russakovsky et al., 2015) which we randomly crop and rescale to \(128\times 128\). The code for loading and pre-processing this training data is borrowed from Rombach et al. (2021).

We train all neural models using the AdamW optimizer (Loshchilov and Hutter, 2019) for 2 epochs at a learning rate of \(10^{-4}\) then for a final epoch with learning rate \(10^{-5}\). Each denoising model takes approximately 5 hours to train on a single V100 GPU.

The training objective for our denoiser is (11) with \(\eta=\sigma\). The training objective for the supervised denoiser is the usual MSE loss:

\[\inf_{f_{\theta}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}}\mathop{\mathbb{E}}_ \limits_{\begin{subarray}{c}x\sim\mathcal{D}(\Omega)\\ \epsilon\sim\mathcal{N}(0,I)\end{subarray}}\left[\frac{1}{2}\|f_{\theta}(x+ \sigma\epsilon)-x\|_{2}^{2}\right],\]

where \(\mathcal{D}(\Omega)\) now denotes the empirical distribution over clean training images. The training objective for the Noise2Noise denoiser is:

\[\inf_{f_{\theta}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}}\mathop{\mathbb{E}} _{\begin{subarray}{c}x\sim\mathcal{D}(\Omega)\\ \epsilon_{1},\epsilon_{2}\sim\mathcal{N}(0,I)\end{subarray}}\left[\frac{1}{2} \|f_{\theta}(x+\sigma\epsilon_{1})-(x+\sigma\epsilon_{2})\|_{2}^{2}\right].\]

Note that this requires access to independent noisy copies of the same clean image during training.

Evaluation details.We evaluate each denoiser by measuring their average peak signal-to-noise ratio (PSNR) in decibels (dB) on 100 randomly-drawn images from the ImageNet validation set, randomly cropped to \(256\times 256\). We corrupt each held-out image with Gaussian noise with the same standard deviation that the respective models were trained on (\(\sigma\in\{1,2\}\)) and denoise them using each neural model along with BM3D (Dabov et al., 2007), a popular classical baseline for unsupervised denoising.

### Representation learning

We use the \(\beta\)-VAE implementation from the AntixK PyTorch-VAE repo and use the default hyperparameters (in particular, we set \(\beta=10\)) but set the latent dimension to 32, as we find that this yields more meaningful latent traversals. Training this \(\beta\)-VAE takes approximately 30 minutes on a single V100 GPU.

We describe the architecture and training details for our regularized and unregularized autoencoder which we use to generate the latent traversals in Figures 6 and 7. Training this autoencoder with the de

Deterministic autoencoder architecture.Our autoencoder operates on \(256\times 256\) images from the CelebA dataset. To reduce the memory and compute costs of our autoencoder, we perform a discrete cosine transform (DCT) using the torch-dct package and keep only the first 80 DCT coefficients. We then pass these coefficients into our autoencoder.

Our deterministic autoencoder consists of an encoder \(f_{\theta}\) followed by a decoder \(g_{\phi}\). The encoder \(f_{\theta}\) is parametrized as a two-layer MLP with \(10,000\) hidden units; the latent space is 700-dimensional. The decoder \(g_{\phi}\) consists of a two-layer MLP with \(10,000\) hidden units and \(3*80*80=19200\) output dimensions, followed by an inverse DCT, and finally a UNet. We use the same UNet as in the denoising experiments described in Appendix B.2.

Training details.We train our autoencoders with the following objective:

\[\inf_{f_{\theta},g_{\phi}\propto\mathcal{D}(\Omega)}\left[\frac{1}{2}\|g_{ \phi}(f_{\theta}(x))-x\|_{2}^{2}+\eta\mathcal{R}x;(f_{\theta})\right],\] (34)

where \(\mathcal{D}(\Omega)\) is the CelebA training set (Liu et al., 2015).

This is a standard deterministic autoencoder objective, with our regularizer approximating the Jacobian nuclear norm \(\|Jf_{\theta}[x]\|_{*}\) of the encoder \(f_{\theta}\).

We train the unregularized autoencoder with \(\eta=0\), and the regularized autoencoder with \(\eta=0.5\). In both cases, we train on the CelebA training set for 4 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of \(10^{-4}\). Training these autoencoders takes approximately 4 hours each on a single V100 GPU.

Generating latent traversals.To generate the latent traversals for our autoencoder, we draw a point \(x\) from the training set, compute the encoder Jacobian \(Jf_{\theta}[x]\), and take its SVD to obtain \(Jf_{\theta}[x]=U\Sigma V^{\top}\). We then take the first 5 left-singular vectors (i.e. the first 5 columns of \(U\)) and compute \(z=f_{\theta}(x)+\alpha u_{\theta}^{d}(x)\), where \(u_{\theta}^{d}(x)\) denotes the \(d\)-th column of \(U\). Here \(\alpha\) denotes a scalar coefficient; it ranges over \([-20000,20000]\) for the unregularized autoencoder and \([-2000,2000]\) for the regularized autoencoder.

To generate the latent traversals for the \(\beta\)-VAE, we encode the same training point \(x\) and replace the \(d\)-th latent coordinate with an equispaced traversal of \([-3,3]\) for \(d\in\{1,2,11\}\); these are the first three meaningful latent traversals for this training point.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claim is to have proven Theorem 3.1 - which we do in Appendix A.2. The simplicity of our method is apparent - one can implement our regularizer in a few lines of code - and its efficiency follows directly from eliminating the need to compute SVDs or Jacobian matrices. We demonstrate our method's accuracy in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our method's primary limitation is that the Hutchinson-based estimator of the squared Jacobian Frobenius norm introduced in Section 3.3 introduces error that manifests itself as somewhat diffuse boundaries in the 2D experiment in Section 4. We discuss this limitation in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We include full proofs of all theoretical results in Appendix A. We also include a proof sketch for our primary result (Theorem 3.1) in the main body of the paper; see Section 3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our primary contribution is our regularizer (6), which can be straightforwardly implemented from the formula in a few lines of code. We have included full experimental details in Appendix B and also attached code for our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have uploaded a zip file with our submission that includes code for training our models and running our experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include full experimental details in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In Table 1, we report 1-sigma error bars for the PSNR attained by each denoiser across the held-out images. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report runtimes for experiments and training runs throughout Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work conforms with the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a primarily theoretical paper; we do not anticipate a direct path towards negative applications arising from this work.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our method does not have a high risk of misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the datasets, pre-existing algorithms, and code libraries used for our experiments throughout our work. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not require IRB approval. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.