# M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on rich resourced languages such as English. In this work, we propose a diverse, task taxonomy guided, fully synthetic **M**ultilingual, **M**ulti-turn evoked instruction finetuning dataset, called **M2Lingual**, to better align LLMs on a diverse set of languages and tasks. **M2Lingual** contains a total of 182K IFT pairs that are built upon diverse seeds collected from Aya collection and Aya dataset covering 70 languages, 19 NLP tasks and general instruction-response pairs. LLMs finetuned with **M2Lingual** substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with **M2Lingual** consistently achieves competitive results across wide variety of evaluation benchmarks compared to existing multilingual IFT datasets that enable LLMs performance in only one or a few subset of the benchmarks. Specifically, LLMs finetuned with **M2Lingual** achieve strong performance on multi-turn evaluation benchmarks such as MT-Bench and across wide-variety of multilingual tasks such as XQuAD, MGSM, TyDiQA, MLQA, XNLI and XLSUM. We show efficacy of **M2Lingual** across LLMs with different sizes, especially smaller LLMs with 1.8B size which benefit massively from our dataset. Lastly, we present key analyses to highlight importance of each synthesis step of **M2Lingual**.1

Footnote 1: We will release **M2Lingual** data, pipeline and the finetuned models after decision.

## 1 Introduction

Large language models (LLMs) have achieved remarkable success [1; 20; 21; 45; 53; 43], largely fueled by the availability of a wide variety of high-quality instruction fine-tuning (IFT) datasets [49; 48; 42; 31; 52; 4; 58]. However, most IFT data curation efforts focus on English or widely spoken languages, leaving low-resource languages and multilingual datasets underrepresented [52]. Prior multilingual datasets can be either categorized into machine translated, human generated and human-AI generated datasets. Datasets like MultiAlpaca [5], Bactrian-X [29] and PolyLM [50] utilize machine translations and self-instruct [48] to generate instruction-response (IR) pairs in multiple languages. However, naive machine translation of English instructions may not capture native or regional knowledge alignment from different languages [47]. Human-generated datasets like Aya [40] and Open Assistant [24] preserve regional knowledge alignment and cultural contexts, making them higher quality compared to translated datasets. However, gathering multilingual annotations in different languages from native speakers (on a large scale) is expensive, time consuming, and prone to annotator errors [40]. Finally, human-AI generated datasets like LMSYS-1M [57], ShareGPT, Vicuna [6], and WildChat [55] involve a human interacting with an AI assistant to gather data. Although such datasets are relatively cheaper to gather compared to human-generated ones, they stillcome with several challenges. These include privacy issues, moderate complexity instructions, and necessary legal regulatory constraints[17].

As summarized in Table 1, most of the above datasets like Aya, Bactrian-X, MultiAlpaca are single-turn only, which limits a model's ability to engage in long, multilingual conversations. Additionally, IFT datasets with multi-million scale like the Aya collection [40] containing \(513M\) IR pairs or XP3 [33] having \(75+M\) pairs can be expensive to finetune LLMs or analyze their different qualitative aspects.Additionally, many multilingual datasets do not include diverse NLP tasks and general instructions across low resource languages, often containing very simple instructions, thus limiting their effectiveness in training strong multilingual instruction following LLMs.

To address these shortcomings, we present **M2Lingual**, a diverse _Multilingual_, _Multi-turn_ IFT dataset which is fully synthetic, containing machine generated \(182K\) instructions, and covering \(70\) languages. The dataset is built upon seed samples from a) the human generated Aya dataset, where general IR pairs are annotated by regional, native language speakers, and b) seeds from Aya collection that contain IR pairs from \(17\) diverse NLP tasks. Unlike previous IFT datasets that use self-instruct mechanism and machine translation to generate data in more languages, **M2Lingual** is constructed with a task-specific taxonomy guided evolve (denoted as _Evol_) conditions [51] to generate new IR pairs from the seed samples in each language. The _Evol_ taxonomy covers a diverse range of NLP tasks, regional dialects and slang, resulting in instructions that are diverse, detailed, more complex, and longer in length. Furthermore, to improve LLMs in engaging multilingual conversations we define a multi-turn _Evol_ taxonomy for generating conversational IR pairs. The multi-turn (MT) taxonomy covers a wide variety of possible subsequent user instructions as discussed in Section 3, shown in Figure 2. The proposed data enrichment taxonomy for curating new complex & diverse instruction and multi-turn conversations is generic, and can be extended to any monolingual or multilingual data. We also ensure balanced _Evol_ generations across languages, creating IR pairs equally for all 70 languages.

In evaluations, we conduct experiments across several multilingual NLP benchmarks and a multi-turn benchmark called MT-Bench [51], translated in \(8\) languages. We empirically demonstrate the effectiveness of **M2Lingual** by comparing with finetuning several LLMs from different family and sizes with existing multilingual IFT datasets. Our results show that **M2Lingual** leads to best or second best performance across a) several multilingual evaluation benchmarks datasets, and b) multi-turn MT-Bench evaluations. On the other hand, existing IFT datasets show competitive results but only in a few subset of evaluations while performing poorly in other evaluations.

The key contributions from our work are as follows:

1. [leftmargin=*]
2. We present **M2Lingual**, a fully synthetic multilingual, multi-turn IFT dataset of \(182K\) IR pairs that lead to best performance in both multilingual evaluation benchmarks and complex multi-turn evaluation dataset MT-Bench. **M2Lingual** is synthesized using a data enrichment taxonomy focused on adding instruction-specific and multi-turn specific evolve [51] complexities. Our data enrichment taxonomies and steps used in **M2Lingual** synthesis can be easily extended to other languages, monolingual settings, and any instruction seeds.
3. **M2Lingual** contains (roughly) equal distribution of IFT pairs across 70 languages ensuring strong performance improvements in multiple languages but notably in low resource languages. Additionally, smaller LLMs like QWEN-1.8B show massive improvements when finetuned with **M2Lingual**, highlighting its usefulness with more accessible models.
4. We present several key ablation studies to highlight the impact of every data enrichment and synthesis steps used in **M2Lingual** generation. We show that adding instruction-task specific complexities improves average performance across several multilingual evaluation benchmarks whereas adding multi-turn specific evols leads to strong improvements on MT-Bench.

## 2 Related Work

Pretraining LLMs is computationally expensive, and due to abundance vs scarcity of corpus in different languages [27; 34; 26; 23], the majority of pretraining is done in high resource languages like English. This often leads to LLMs performing much better in high resource languages [32; 34] compared to low resource languages. Multilingual instruction finetuning has proven to be a relatively cost effective solutions for improving multilingual performance of LLMs, especially for low resource languages [37; 5; 46]. While several IFT datasets have been introduced in the recent past, less focus has been given on synthesizing multilingual IFT datasets limiting progress in various languages.

Instruction finetuning datasets are often created from pool of numerous NLP tasks (e.g., flanT5, supernatural instructions) [8; 38; 49], machine generated (e.g., self-instruct [48]), human expert annotated (e.g., LIMA [58]) and crowd-sourced or cached from real users chat (e.g., LMSYS, WildChat) [55]. A few synthetic IFT datasets [29; 42] have leveraged LLMs such as GPT-4 for generating IR pairs via self-instruct as a relatively affordable alternative. Although very effective, a few works have highlighted issues in data generation process of self-instruct [4; 16] where generations can be considered a bit uncontrolled. For example, Alpaca [41] uses self-instruct to generate \(52K\) instructions from \(175\) seeds, where overlapping and noisy IFT pairs have been reported [4; 58]. In contrast, techniques like WizardLM [51] that generate new instructions by adding complexity (or _Evol_) on input seed instructions, have generations which are controlled by the set of _Evol_ conditions, ensuring diverse generations. For example, as shown in fig. 1, the _Concterize_\(Evol\) in left block creates a new IFT pair with a more complex but concrete python question. Furthermore, generating templated datasets for specific NLP tasks (e.g., XP3 [33]) may not contain complex and diverse inputs for aligning instruction following of LLMs. Thus, inspired from WizardLM, **M2Lingual** contains _Evol_ IFT pairs generated from a set of diverse IFT seeds. Additionally, we also create a multi-turn _Evol_ taxonomy (fig. 2) which is used to generate multi-turn IR pairs resulting in a diverse, complex conversational IFT set within **M2Lingual**. Features of some of the existing IFT datasets are summarized in table 1.

## 3 Methodology

In this section, we detail the three main synthesizing steps of **M2Lingual**. Step 1 (Section 3.1) involves selection of diverse multilingual seed data. In our work, we select seed samples from two different sources of Aya -- Aya dataset and Aya Collection, both of which receive high average approval ratio by human annotators [40]. Step 2 (Section 3.2) and 3 (Section 3.3) correspond to our novel _Evol_ taxonomy based data enrichment techniques. Specifically, in Step 2 we create an NLP task specific _Evol_ taxonomy [51] and generate new IR pairs using these _Evol_ conditions. In Step 3, we first create an _Evol_ taxonomy for multi-turn or conversational IR pairs, and then use these _Evol_ for generating new conversational, multi-turn IR pairs. Figure 1 captures an overview of each of these generation steps used in synthesis of **M2Lingual**.

### Seed Selection

Our first seed source, Aya dataset, contains general IR pairs written by native speakers/annotators which enables capturing region-specific language nuances and cultural contexts. We randomly select \(100\) prompts for each of the \(70\) languages resulting in \(7000\) seed samples from the Aya dataset. Our second seed source, Aya collection, covers \(19\) different NLP tasks where each task has parallel examples in \(113\) different languages. To ensure a proper balance of the number of examples across all languages, we only focus on \(70\) languages of the Aya dataset. We exclude two NLP tasks from Aya collection - 1) text simplification as it requires rewriting a complex or a simplified version of a sentence which is already supported by our evols, and 2) multilingual event entity task as Aya collection does not have a consistent format for this task. Finally, for each task in the Aya collection, we randomly sample \(6\) examples per language, resulting in \(6*70*17=7140\) seed examples. We select \(6\) random samples per task per language to ensure balanced amount of seed samples from Aya collection when compared to the seeds from Aya dataset. Thus, our final seed contains \(7000+7140=14140\) samples.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Size} & \multirow{2}{*}{Multi} & \multirow{2}{*}{Langs} & Resource Level & Task & General & Translated & Fully \\  & & & & Low & High & specific? & instructions? & dataset? & synthetic? \\ \hline OpenAssistant & 10K convs & ✓ & 35 & 3 & 32 & ✗ & ✗ & ✗ & ✗ \\ Aya Dataset & 200K IR pairs & ✗ & 70 & 37 (1) & 32 & ✗ & ✓ & ✗ & ✗ \\ MultiAlpaca & 52K IR pairs & ✗ & 12 & 0 & 12 & ✓ & ✓ & ✓ & ✓ \\ Bactrian-X & 3.4M IR pairs & ✗ & 52 & 15(1) & 36 & ✗ & ✓ & ✓ & ✓ \\ ShareGPT & 94K convs & ✓ & 45 & 4 (2) & 39 & ✓ & ✗ & ✗ & ✗ \\ WildChat & 1.04M convs & ✓ & 74 & 21 (3) & 50 & ✗ & ✗ & ✗ & ✗ \\
**M2Lingual** & 182K convs & ✓ & 70 & 37 (1) & 32 & ✓ & ✓ & ✗ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of multilingual IFT datasets with **M2Lingual**. Resource level classification taken from NLLB [13]. Languages not found in the NLLB table are counted as low, in parentheses.

### Task Guided _Evol_

Though the IR pairs in the seed data from Aya cover a wide variety of NLP tasks, overall they are direct and less intricate. To enhance the instruction following abilities of LLMs, especially for complex tasks and prompts, we employ _Evol_-Instruct [51] on our selected seed instructions as the second data synthesis step. _Evol_-Instruct generates a more complex instructions using _Evol_ conditions over the provided seed instruction [51]. The generic _Evol_ conditions used in the original work2 are not always applicable to a wide variety of downstream NLP tasks such as multi-hop question answering, joke explanation, etc. Furthermore, generic _Evol_ conditions also provide weak or broader guidance for new IFT pair generations, especially for seed datasets like ours which cover diverse NLP tasks.

Footnote 2: https://github.com/lcv99/evolve-instruct/blob/main/evolve.py

To address this, we create a taxonomy of _Evol_ conditions covering general instructions (for seed from Aya dataset) and each of the NLP tasks (for seeds from Aya collection) as shown in Figure 2. Specifically, we create 6 _Evol_ conditions enhancing multi-lingual features focused solely on general instructions. We then leverage GPT-4 to come up with \(9\) different _Evol_ for each NLP task. These NLP task specific _Evol_ ensure that we create _Evol_ conditions separately for a particular task. Figure 2 shows all the NLP task names and the corresponding evols.

The _Evol_ prompts to GPT-4 for each task in Figure 2 are presented in detail in Appendix 9.3. We apply these task-specific _Evol_ on our selected seeds:

* The seed samples from the Aya dataset are generic instructions therefore we apply the \(6\)_generic evols_ (Figure 2) to each seed sample. The \(6\) different evols ensure that the new instruction is more complex, challenging and captures all the multilingual variations, nuances and complexities of different languages. This results in \(7K\times 6=42K\) samples from the seeds in Aya dataset.
* The seed samples from Aya collection have \(17\) NLP tasks which are shown in the top block of Figure 2 along with their corresponding \(9\) evols. For each seed from a particular task, we apply its corresponding \(9\) evols resulting in a total of \(7140\times 9=64260\) instructions from Aya collection.
* Upon manual inspection of the generated instructions across both Aya dataset & collection, we observe that some of the instructions generated using GPT-4 have repetitive long sequences and n-grams. Therefore, following [18; 14], we filter instructions with frequent n-grams. Some requests to GPT-4 also return a time out. The final datasets thus contains \(95K\) instructions, \(37K\) from the Aya dataset and \(57K\) from Aya collection as shown in Table 2.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & **Seed** & **Evoked** & **Multi-turn** \\ \hline Aya Dataset & 7000 & 37803 & 36969 \\ Aya Collection & 7140 & 57145 & 34426 \\ \hline
**Total** & **14140** & **94948** & **71395** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c} \hline \hline
**Total** & **14140** & **94948** & **71395** \\ \hline Aya Instruction & 49.60 & 107.71 & 356.81 \\ Aye Response & 56.79 & 62.16 & 87.60 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **M2Lingual** IR pairs. Aye Instruction and Response show avg no of tokens.

Figure 1: Walk-through examples for data synthesis of **M2Lingual**. In Step 1, seeds are selected from Aya dataset (left figure) and Aya collection (right figure). In Step 2, the task specific _Evol_ taxonomy is used for generating new, complex and evolved instructions. General _Evol_ are used for seeds from Aya dataset and NLP task specific (see Joke explanation task evols) _Evol_ are used for Aya collection seed. Finally, in Step 3, multi-turn instructions are generated on top of the new evolved instruction generated from Step 2.

### Generating Multiple Turns

As the final step 3 in synthesis of **M2Lingual**, we generate multiple user-assistant turns from the task-evolved instructions produced in the previous step. A conversation between a user and an AI assistant broadly can be categorized into four categories [25] -- _Follow-up_, _Refinement_, _Expansion_, and _Recollection_. However, these categories are generic and do not encompass the full complexity and fine-grained variety of conversational interactions. To address this, we introduce a multi-turn taxonomy comprising \(21\) distinct variations of dialogue that expand upon these four categories. These \(21\) detailed taxonomy with the 4 categories improve coverage of possible variations in continuing a conversation, thus ensuring an engaging interaction between a user and an assistant [25]. We also ensure that subsequent instructions are generated in the language of the initial instruction by explicitly prompting GPT-4. The last block in Figure 2 shows all the \(21\) variations. The multi-turn _Evol_ prompts to GPT-4 are shown in Appendix 9.4. We convert the instructions to multi-turn conversations with following steps:

1. We use the prompt specified in Appendix 9.4 and replace the _{instruction}_/ with the task-evoiced generated instructions from the previous step (i.e., Step 2), _{follow_up_type}_/ with one of the \(21\) dialogue variations in 9.4, and _{language}_/ with the _Evol_ instruction language. We then pass it to GPT-4 \(n\) times to generate the next user instruction.
2. For all the generated instructions, we generate subsequent response turns using GPT-4 using the entire conversation history. To mitigate the potential impact of topic drift from the prolonged conversations [54], we restrict the number of subsequent instructions or multi-turns to \(<=4\).
3. We generate conversations for all the evoked instructions from the Aya dataset, resulting in \(36\)K conversations. For Aya collection, we pick a balanced subset of size \(35\)K across all tasks and languages and generate conversations. After applying the same post-processing steps as mentioned in Section 3.2, we end up with \(70\)K conversations.

In total, **M2Lingual** contains \(182\)K IR pairs, with the exact sizes from different steps of **M2Lingual** synthesis shown in Table 2.

## 4 Experiments

We conduct experiments across _three_ model families & _five_ model sizes -- Mistral-7B [20], LLaMA-3-8B [45] and QWEN-4B [3]. Furthermore, to demonstrate the effectiveness of our dataset across different model scales, we fine-tune both a larger model, LLaMA-2-13B [44], and a smaller model, QWEN-1.8B [3]. To evaluate how well the datasets work with instruction-tuned models, we also experiment with Mistral-Instruct-7B.

### Baseline Datasets

We use _six_ different multilingual datasets as baselines for comparison: 1) the top ranked conversation trees from **Open Assistant**[24], 2) **Aya**[40], 3) self-instruct dataset **MultiAlpaca**[50], 4) machine translated **Bactrian-X**[29] derived from Alpaca-52k [41] and Dolly-15k [12], 5) the **ShareGPT**3 collection, and 6) **WildChat**[55].

Figure 2: Taxonomy of _Evol_ applied towards creating **M2Lingual**. Part 1 includes evols on the general Aya dataset as well as the task-specific Aya collection data, after which Part 2 multi-turn _Evol_are applied for creating multiple turns in the conversation.

For a fair comparison with WildChat, we use \(200\)K non-English conversations, ensuring the same language proportions, and downsampled \(60\)K English conversations, resulting in a total of \(260\)K conversations. Similarly for Bactrian-X, we sample 1M IR pairs ensuring the same language proportion as of the original dataset.

**Additional Baselines** To highlight the importance of each step in our data curation process, we consider several ablations as baselines. Specifically we conduct experiments by training models using 1) only **Seed** samples, 2) seed samples with the generated evols (**Seed + Evol**) and 3) seeds, evols and the generated multi-turn conversations (**Seed + Evol + MT**). Finally, to see whether adding parallel data (PD) helps in improving the over model's performance, we collect \(60\)K from the Aya collection and train a baseline by augmenting the PD with our full dataset (**Seed + Evol + MT + PD**).

### Training

All training is performed on \(8\) A-\(100\)\(80\)GB NViDIA GPUs [7], with the Axolott4 framework. We used Mistral tags [20] for finetuning all models. We use a batch size of \(64\), max seq length \(8192\), learning rate of \(5\times 10^{-6}\), Adam optimizer [22] with a cosine scheduler and \(10\) warmup steps. We reserve a \(5\)% validation split, and train all the models until validation loss convergence. We compute the loss only on the targets using fp\(16\) training.

Footnote 4: https://github.com/OpenAccess-AI-Collective/axolotl

### Evaluation

**Multilingual benchmarks.** We utilize the EleutherAI evaluation framework [15] for consistent comparisons. We evaluate the performance of different multilingual datasets on the following tasks:

* _Question Answering (QA)_: We focus on \(3\) multilingual QA datasets 1) XQUAD [2] with QA across \(11\) languages, 2) TyDiQA [9] which has human generated QA in \(11\) languages and 3) MLQA [28] with QA in \(7\) languages. While QA data requires short answer phrases, conversational IR pairs might lead to longer answer span generation. Hence, we use \(3\) in-context examples to get the right output format for LLMs. In the interest of time, we keep the number of examples per language to \(100\) for XQUAD and MLQA, and \(1000\) for TyDiQA. We use the validation set for XQUAD and test set for TyDiQA & MLQA, and compute the standard F1-score.
* Arabic, English, Spanish, French, Japanese and Russian. We restrict the total number of examples to \(100\) and prompt the model to generate a summary in the same language as the context. We look at the ROUGE\({}_{\text{L}}\)[30] & BLEU [35] scores for comparison.
* _Classification_: We focus on XNLI [11] and XCOPA [36] with \(15\) and \(11\) languages respectively in a zero-shot setting. We compute the accuracy (Acc) by looking at the log-likelihood assigned to the ground truth answer on the validation set.
* _Multilingual math word problems_: We use MGSM [39], a grade-school math benchmark that translates GSM8K [10] to \(10\) different languages. Similar to QA tasks, we use \(3\) in-context examples and compute the exact match (EM) with the ground truth answer.

**Translated MT-Bench.** To evaluate the conversation and instruction following ability of multilingual models across a wide array of tasks and languages, we translate MT-Bench [56]. MT-Bench comprises of \(80\) multi-turn questions across \(8\) domains. The models are required to respond to an initial and a follow-up question and GPT-4 assesses the model's responses on a scale of \(1\) to \(10\) (\(10\) being the best), with the overall score being the mean over the two turns. We translate it into \(9\) different languages with professional linguists to ensure high quality evaluation. We modify the judge prompt to include the language of the question asked at each turn, and additionally instruct GPT-4 to make sure the responses are in the same language as the question asked. We report the average scores across all \(80\) examples for each language and also report the average MT-Bench score across all languages.

**Low-resource Languages.** We evaluate models on \(6\) low-resource languages, including Hindi, Urdu, Thai, Tamil, Bengali and Gujarati using the same aforementioned procedure. Since finding native annotators for these low-resource languages might be difficult, we leverage GPT-4 for translations.

## 5 Results

**Consistent best results:** As observed in table 3 and 4, **M2Lingual** leads to best or amongst the top results in both _multi-turn evaluations_ (Table 3) and _several NLP task comprised multilingual evaluation benchmarks_ (Table 4). On the other hand, the majority of baseline datasets show competitive results only on select evaluation settings. Specifically, on _multi-turn evaluations_, conversational IFT datasets like ShareGPT and WildChat lead to competitive performances but all of the other baseline datasets have low MT-Bench scores. This suggests importance of including multi-turn IR sets within IFT datasets. **M2Lingual** achieves top MT-Bench score in 5 languages and 2nd best in remaining 3 languages, outperforming all of the baseline datasets overall. Similarly, as shown in Table 4, **M2Lingual** also leads to performance across 4 out of 7 _multilingual NLP task evaluation benchmarks_ and 2nd best results with very close performance to the best score in remaining 3 benchmarks. It is worth noting that evaluations on classification tasks such as XCOPA and XNLI show very minimal performance variations across all IFT datasets which has been shown in other works as well [29]. In generation tasks (MGSM, XLSUM, including QA tasks MLQA, XQuAD, TyDiQA), **M2Lingual** leads to better results over all the baseline datasets. For instance, with Mistral-7B base model, our proposed **M2Lingual** outperforms the second best baseline (Bactrian-X) by 2.13 and 1.98 F1 points on MLQA and XQuAD respectively.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Dataset**} & **XQUAD** & **TyDiQA** & **MLQA** & **XLSUM** & **MGSM** & **XNLI** & **XCOPA** \\  & & F1 & F1 & F1 & ROUGE\({}_{\text{L}}\) & BLEU & EM & Acc & Acc \\ \hline \multirow{8}{*}{**M2Lingual**} & Open Assistant & 67.99 & 54.22 & 53.64 & 10.86 & 0.85 & 16.05 & 42.74 & 56.73 \\  & MultiAlpaca & 67.99 & 64.44 & 55.69 & 10.9 & 1.59 & 10.41 & 42.18 & 58.91 \\  & Backin-X & 71.91 & 66.63 & 60.27 & 3.30 & 0.20 & 17.14 & 43.91 & 58.64 \\  & ShareGPT & 66.33 & 56.97 & 50.78 & 3.31 & 0.288 & 11.32 & 41.13 & 56.09 \\  & WildChat & 72.55 & 64.27 & 59.53 & 3.91 & 0.41 & **18.41** & 43.11 & 58.00 \\  & Aya & 70.46 & 66.95 & 57.47 & **12.5** & **2.01** & 13.86 & 41.78 & 59.00 \\ \cline{2-11}  & Seed & 72.52 & 65.89 & 59.33 & 11.53 & 1.72 & 16.95 & 43.28 & 57.64 \\  & Seed + Evol & 71.01 & 65.04 & 57.47 & 9.8 & 1.37 & 14.23 & 43.00 & 57.55 \\  & Seed + Evol + MT (**M2Lingual**) & **74.53** & **67.57** & **62.40** & 10.42 & 1.38 & 15.38 & 42.12 & **59.55** \\  & Seed + Evol + MT + PD & 68.79 & 62.62 & 60.00 & 9.92 & 1.37 & 16.45 & 42.36 & 59.00 \\ \hline \multirow{8}{*}{**M2Lingual**} & Open Assistant & 64.38 & 52.65 & 47.08 & 9.38 & 1.21 & 17.36 & 46.17 & **63.82** \\  & MultiAlpaca & 75.08 & 64.49 & 59.01 & **10.93** & **1.45** & 10.68 & **46.93** & 63.55 \\  & Backin-X & 69.57 & 56.45 & 58.51 & 8.39 & 1.28 & 22.86 & 46.90 & 62.18 \\  & ShareGPT & 56.98 & 58.48 & 43.43 & 3.53 & 0.40 & 25.32 & 45.93 & 63.00 \\  & WildChat & 63.15 & 59.88 & 63.16 & 5.52 & 0.76 & 26.36 & 46.88 & 62.27 \\  & Aya & 75.14 & 59.60 & 53.14 & 10.38 & 1.39 & 22.09 & 45.64 & 63.55 \\ \cline{2-11}  & Seed & 77.27 & 68.57 & 60.01 & 9.92 & 1.45 & 17.18 & 46.02 & 62.82 \\ \cline{2-11}  & Seed + Evol + MT (**M2Lingual**) & 76.17 & **69.89** & 63.09 & 8.96 & 1.23 & **28.00** & 46.38 & 61.36 \\ \cline{2-11}  & Seed + Evol + MT (**M2Lingual**) & 75.91 & 67.84 & **63.50** & 8.87 & 1.25 & 27.36 & 46.18 & 62.55 \\ \cline{2-11}  & Seed + Evol + MT + PD & 76.69 & 59.24 & 60.02 & 9.84 & 1.37 & **29.00** & 46.37 & 62.09 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluations of LLaMA-3-8B-base & & Mistral-7B-base in different tasks. Same notations as in Table 3

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline
**Model** & **Dataset** & **MT-EN** & **MT-FR** & **MT-TT** & **MT-JP** & **MT-ES** & **MT-DE** & **MT-NL** & **MT-PT** & **MT-AVG** \\ \hline \multirow{8}{*}{**M2Lingual**} & Open Assistant & 6.72 & 5.87 (5.90) & 6.04 & 4.19 & 5.87 & 5.82 & 4.97 & 6.01 & 5.66 \\  & MultiAlpaca & 5.45 & 4.90 (5.22) & 4.63 & 3.76 & 5.01 & 4.66 & 4.51 & 4.65 & 4.77 \\  & Bactrian-X & 5.60 & 5.35 (5.26) & 5.46 & 4.82 & 5.24 & 5.53 & 4.96 & 5.31 & 5.25 \\  & ShareGPT & 7.04 & 5.93 (5.70) & 5.42 & 4.75 & 5.83 & 6.00 & 5.27 & 5.92 & 5.80 \\  & WildChat & 7.02 & 5.646 (7.67) & 6.68 & 5.50 & 6.71 & **64.33** & **65.81** & **63.9** & **65.53** \\  & & Aya & 6.43 & 5.42 (5.39) & 4.97 & 3.37 & 5.45 & 5.37 & 4.94 & 5.12 & 5.18 \\ \cline{2-11}  & Seed & 6.01 & 5.15 (5.14) & 5.35 & 3.44 & 5.07 & 5.98 & 4.62 & 4.91 & 5.04 \\  & Seed + Evol & 6.33 & 5.44 (5.30) & 5.46 & 4.74 & 5.88 & 5.61 & 5.40 & 5.78 & 5.56 \\  & Seed + Evol + MT (**M2Lingual**) & **7.33** & **67.58** (**61.81**) & **6.9** & **5.70** & **68.81** & 6.39 & 6.34 & **6.46** & **65.84** \\ \cline{2-11}  & Seed + Evol + MT + PD & 5.85 & 5.75 (5.39) & 5.60 & 4.86 & 5.81 & 5.73 & 5.32 & 5.74 & 5.55 \\ \hline \multirow{8}{*}{**M2Lingual**} & Open Assistant & 6.26 & 5.15 (5.03) & 4.95 & 4.08 & 5.26 & 4.87 & 5.01 & 5.48 & 5.12 \\  & MultiAlpaca & 4.96 & 4.65 (5.09) & 4.22 & 3.30 & 4.76 & 4.18 & 4.32 & 4.27 & 4.41 \\ \cline{1-1}  & Backin-X & 6.27 & 5.73 (5.77) & 5.73 & 4.83 & 5.95 & 5.34 & 5.41 & 5.90 & 5.66 \\ \cline{1-1}  & ShareGPT & 7.07 & 6.17 (5.76) & 6.43 & 5.40 & 6.10 & 6.07 & 5.82 & 6.13 & 6.10 \\ \cline{1-1}  & WildChat & **7.20** & **6.74 (**66.67**) & 6.78 & **6.35** & 6.86 & 6.60 & 6.58 & 6.72 & **6.75** \\ \cline{1-1}  & Aya & 5.95 & 5.01 (4.50) & 5.41 & 3.86 & 5.27 & 4.93 &

**Impact on different LLMs:** We evaluate Mistral-Instruct-7B to highlight the impact of multilingual IFT datasets on pre-instruction finetuned models. **M2Lingual** leads Mistral-Instruct-7B to achieve best performance in \(5\) of \(8\) MT-Bench language evaluations and \(5\) of the \(7\) multilingual evaluation benchmarks as shown in Tables 5 and 8 respectively. Interestingly, the improvements from **M2Lingual** in Mistral-Instruct-7B over baseline datasets is consistently higher when compared to Mistral-7B-base (Table 4) in all of the multilingual QA tasks, MGSM, and XCOPA. We also evaluate QWEN-4B model to showcase results from smaller LLM from different model family. We observe similar findings as QWEN-4B finetuned with **M2Lingual** achieves competitive results in both MT-Bench and multilingual evaluation datasets. Another interesting observation is that improvements seem relatively higher for QWEN-4B model using **M2Lingual** when compared to Mistral-7B and LLaMA-3-8B models, highlighting the usefulness of our proposed data on moderate sized LLMs.

**Ablation of M2Lingual:** As observed in our empirical studies in Tables 4, 5, 6, 7, generating _Evol_ data and appending it with seeds helps improve performance consistently in all evaluation benchmarks, highlighting impact of _Evol_. For instance, on LLaMA-3-8B, our proposed _Evol_ improves performance significantly by 10.82 points on MGSM task as compared to the Seed data. Adding multi-turn IFT pairs specifically helps boost performance in MT-Bench evaluations substantially across all of the languages with the most significant gain of 1.31 points on French for Mistral-7B model. Adding multi-turn data also helps consistently in multilingual benchmark evaluations as shown in Tables 4 and 5. _Evol_ + _MT_ and _Evol_ provides 1.50 and 0.98 points of MT-AVG performance gain over _Seed-only_ IFT data on Mistral-7B model. These findings reinforce the benefits of adding multi-turn _Evol_ IFT pairs.

**Low-resource languages:** We also compare **M2Lingual** with our most competitive baseline dataset WildChat on MT-Bench in low-resource languages. As shown in Figure 3, **M2Lingual** consistently leads to much higher MT-Bench scores in majority of the low-resource languages highlighting that existing multi-turn IFT datasets created from cached user chats may have poor coverage of low-resourced languages. On the other hand, our synthetically generated **M2Lingual** has uniform coverage of all the 70 languages in terms of number of IFT sets.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Dataset**} & **XQUAD** & **TyDiQA** & **M1QA** & **XLSUM** & **MGSM** & **XNLI** & **XCOPA** & **MT-Avg** \\  & & F1 & F1 & F1 & ROUGE\({}_{\text{L}}\) & BLEU & EM & Acc & Acc & **MT-Avg** \\ \hline \multirow{8}{*}{**Mistral-7B**} & Open Assistant & 53.63 & 45.30 & 46.34 & 4.15 & 0.29 & 17.50 & 38.52 & 58.45 & 3.47 \\  & MultiAlpaca & 51.81 & 53.81 & 40.26 & 8.9 & 1.0 & 12.1 & 38.3 & 58.40 & 2.93 \\  & Backrim-X & 46.70 & 42.79 & 42.2 & 7.1 & 0.8 & 18.6 & 38.3 & 57.70 & 3.80 \\  & ShareGPT & 41.86 & 28.20 & 36.03 & 4.58 & 0.43 & 16.95 & 37.83 & **58.55** & 3.80 \\  & WildChat & 53.18 & 49.18 & 42.81 & 5.23 & 0.56 & 19.27 & **38.74** & 58.18 & **42.92** \\  & Ava & 54.00 & 52.14 & 48.28 & **10.91** & 13.1 & 16.50 & 3.59 & 57.73 & 3.43 \\ \cline{2-11}  & Seed & **Evol** & **46.55** & **58.00** & 48.25 & 10.65 & 0.65 & 15.36 & 37.59 & 58.00 & 2.47 \\  & Seed + Evol & 52.24\({}^{*}\) & 52.50 & **49.87** & 8.50 & 1.12 & 20.77 & 38.36 & 57.91 & 3.79 \\  & Seed + Evol + MT (**M2Lingual**) & 49.12 & 47.53 & **50.36** & 8.30 & 1.02 & **21.36** & 38.37 & 58.36 & 4.23 \\  & Seed + Evol + MT + PD & 57.76 & 51.97 & 43.24 & 9.64 & 1.21 & 19.36 & 37.95 & 57.91 & **42.42** \\ \hline \multirow{8}{*}{**Mistral-7B**} & Open Assistant & 61.33 & 59.28 & 53.27 & 9.62 & 1.43 & 19.00 & 45.91 & 58.09 & 5.58 \\  & MultiAlpaca & 63.76 & 63.05 & 51.09 & 11.51 & 1.80 & 13.18 & **44.70** & 58.18 & 4.74 \\  & Backrim-X & 70.5 & 64.8 & 50.60 & 9.14 & 1.35 & 17.91 & 42.23 & 57.25 & 5.98 \\  & ShareGPT & 44.53 & 49.5 & 40.45 & 3.31 & 0.38 & 17.36 & 42.13 & 56.73 & 6.11 \\  & WildChat & 61.53 & 53.1 & 52.60 & 6.31 & 0.56 & 21.00 & 41.86 & 57.75 & 6.62 \\  & Ava & 69.9 & 66.43 & 57.27 & **12.55** & **2.05** & 16.36 & 42.84 & 58.60 & 5.20 \\ \cline{2-11}  & Seed & 68.78 & 61.54 & 56.11 & 12.45 & 2.04 & 18.27 & 43.23 & 58.45 & 3.92 \\  & Seed + Evol & **72.87** & 68.43 & 55.43 & **12.51** & 1.33 & 22.00 & 42.51 & 58.09 & 6.48 \\  & Seed + Evol + MT (**M2Lingual**) & 71.41 & 69.44 & **58.33** & 9.57 & 1.51 & 19.82 & 42.37 & **59.45** & **6.64** \\  & Seed + Evol + MT + PD & 70.04 & **69.67** & **59.13** & 9.06 & 1.46 & 22.27 & 42.92 & 57.82 & 6.56 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Evaluations of QWEN-4B & _Mistral-Instruct-7B_ in different tasks and MT-Bench score averaged across languages. Please see table 8 in appendix for MT-Bench score in each language. \(\star\) in XQUAD, TyDiQA scores for QWEN-4B show exception cases where outputs had repeated noisy patterns in multiple runs resulting in low scores.

Figure 3: Results on Low Resource translated MT-Bench for Mistral-7B and LLaMA-3-8BAdditional Analysis

**Effect of IFT datasets on different sized LLMs.** In addition to 4B, 7B, and 8B sized LLMs shown in Tables 4, 5, 6, 7, we also study impact of our IFT datasets on a smaller LLM (QWEN-1.8B) and a larger LLM (LLaMA-2-13B). As shown in Table 6, on QWEN-1.8B LLM, **M2Lingual** leads to even higher performance for various tasks when compared to our strongest baseline WildChat with the most significant improvements of \(13.64\) and \(22.24\) points on MGSM and TyDiQA tasks respectively. Similar findings for the LLaMA-2-13B model highlights the effectiveness of our proposed **M2Lingual** across various sized LLMs.

**Importance of _Evol_.** We selected \(15.1K\) seeds from Aya dataset and Aya collection as discussed in section 3.1. We generated _Evol_ and multi-turn IR pairs from these seed but as an alternative, more data can also be sampled from Aya. To highlight benefits from _Evol_ and generating multi-turn IR pairs, we sample \(94.9K\) more IR pairs from Aya collection and Aya dataset, making the total seed size as **M2Lingual** of \(110.5K\). As shown in Table 7, simply sampling more seed IFT pairs from Aya achieves low performance, whereas having the same number _Evol_IR pairs from **M2Lingual** leads to much higher performance in MT-Bench and MGSM (\(2.05\) and \(6.68\) points respectively). It is worth noting that MLQA being reading comprehension QA data requires short answer phrases for exact match. IR pairs within **M2Lingual** are longer (Table 2) and conversational which often lead to longer answer span generation for reading comprehension task. Hence, we utilized 3-shot setting to get the right output format from LLMs in our QA evaluation experiments.

## 7 Conclusion

We build **M2Lingual**, a multilingual, multi-turn IFT dataset that leads to top performances across several multilingual evaluation benchmarks. Our work presents two IFT data enrichment techniques, namely 1) taxonomy based instruction-task specific _Evol_, and 2) multi-turn _Evol_ for generating a diverse, conversational multilingual IFT dataset. **M2Lingual** contains roughly same number of IR pairs for 70 languages, resulting in substantial performance improvements in low resource languages. **M2Lingual**-also strongly improves multilingual performance of different sized LLMs ranging from 4B, 7B, 8B, and 13B parameters but in particular leads to massive improvements of small LLMs with 1.8B parameter size. Thus, **M2Lingual** presents a strong societal impact specifically for underrepresented or low-resourced languages. Additionally, the massive performance gains with **M2Lingual** on smaller 1.8B parameter LLMs also contributes towards improving accessibility to the wider community.

## 8 Limitations and Ethical Considerations

As future work and limitations of our work, **M2Lingual** can be extended to more than 3 turns to generate longer conversational IFT data, although this would be computationally expensive. Similarly, **M2Lingual** can be extended to more number of NLP tasks and languages in future work. In this work, we select seeds from Aya which does not contain specific flags for toxic, harmful, or offensive speech [40], but report low risk. We conducted manual inspection of a few generated IFT pairs from _Evol_, and did not find any harmful IFT data, but future work includes filtering **M2Lingual** by automatic safety tools.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c} \hline \hline Model Name & Dataset & MT-EN & MT-FR & MT-IT & MT-JP & MT-ES & MT-DE & MT-NL & MT-PT & MT-Avg & MGSM & MLQA & TyDiQA \\ \hline \multirow{3}{*}{Qwen-1.8B} & WildChat & 4.99 & 2.75 (2.74) & 2.08 & 1.72 & 2.88 & 1.92 & 1.54 & 2.68 & 2.59 & 8.00 & 29.39 & 42.42 \\  & MultiMedia & 3.97 & 4.93 (1.99) & 1.74 & 1.44 & 1.87 & 1.86 & 1.52 & 1.91 & 2.03 & 7.45 & 19.30 & 33.38 \\  & **M2Lingual** & **6.20** & **4.55 (4.25)** & **3.85** & **3.27** & **4.00** & **4.11** & **3.32** & **4.51** & **4.27** & **21.64** & **38.24** & **64.66** \\ \hline \multirow{3}{*}{LLaMA-2-13B} & WildChat & 6.64 & 6.25 (5.89) & 5.98 & 5.10 & 6.20 & 6.10 & 5.82 & 5.99 & 6.00 & 9.95 & 53.69 & 60.14 \\  & **M2Lingual** & 5.09 & 4.35 (4.55) & 4.35 & 3.52 & 4.47 & 4.54 & 4.69 & 4.62 & 4.46 & 7.80 & 48.74 & 59.46 \\ \cline{1-1}  & **M2Lingual** & **6.47** & **6.40 (6.20)** & **6.13** & **5.35** & **6.18** & **5.94** & **5.87** & **6.17** & **6.68** & **11.95** & **54.64** & **64.66** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Evaluations of QWEN-1.8B and LLaMA-2-13B for highlighting impact on different sized LLMs.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Model & Data & MT-Avg & XQUAD & TyDiQA & MLQA & XLSUM & MGSM & XNLI & XCOPA \\ \hline \multirow{3}{*}{Mistral-Instruct-7B} & Aya-seeds(110.5K) & 4.59 & 71.40 & 68.00 & **57.69** & **14.08/2.45** & 15.32 & 40.77 & 57.55 \\  & Seed + Evol (110.5K) & **6.64** & **72.87** & **68.43** & 55.43 & 12.51/1.33 & **22.00** & **42.51** & **58.09** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance comparison of **M2Lingual** vs Aya-seeds data of same size.

## References

* [1] OpenAI Josh Achiam et al. "GPT-4 Technical Report". In: 2023. url: https://api.semanticscholar.org/CorpusID:257532815.
* [2] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. "On the cross-lingual transferability of monolingual representations". In: _arXiv preprint arXiv:1910.11856_ (2019).
* [3] Jinze Bai et al. "Qwen Technical Report". In: _ArXiv_ abs/2309.16609 (2023). url: https://api.semanticscholar.org/CorpusID:263134555.
* [4] Lichang Chen et al. "Alpagasus: Training a better alpaca with fewer data". In: _arXiv preprint arXiv:2307.08701_ (2023).
* [5] Pinzhen Chen et al. "Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca". In: _The 18th Conference of the European Chapter of the Association for Computational Linguistics_. Association for Computational Linguistics. 2024, pp. 1-10.
* [6] Wei-Lin Chiang et al. "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality". In: _See https://vicuna. lmsys. org (accessed 14 April 2023)_ (2023).
* [7] Jack Choquette et al. "NVIDIA A100 Tensor Core GPU: Performance and Innovation". In: _IEEE Micro_ 41.02 (2021), pp. 29-35.
* [8] Hyung Won Chung et al. "Scaling instruction-finetuned language models". In: _Journal of Machine Learning Research_ 25.70 (2024), pp. 1-53.
* [9] Jonathan H Clark et al. "Tydi qa: A benchmark for information-seeking question answering in tyologically di verse languages". In: _Transactions of the Association for Computational Linguistics_ 8 (2020), pp. 454-470.
* [10] Karl Cobbe et al. "Training verifiers to solve math word problems". In: _arXiv preprint arXiv:2110.14168_ (2021).
* [11] Alexis Conneau et al. "XNLI: Evaluating cross-lingual sentence representations". In: _arXiv preprint arXiv:1809.05053_ (2018).
* [12] Mike Conover et al. _Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM_. 2023. url: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm (visited on 06/30/2023).
* [13] Marta R Costa-jussa et al. "No language left behind: Scaling human-centered machine translation". In: _arXiv preprint arXiv:2207.04672_ (2022).
* [14] AbdelRahim Elmadany, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. "Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation". In: _arXiv preprint arXiv:2310.16127_ (2023).
* [15] Leo Gao et al. _A framework for few-shot language model evaluation_. Version v0.0.1. Sept. 2021. doi: 10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.
* [16] Sreyan Ghosh et al. "A Closer Look at the Limitations of Instruction Tuning". In: _arXiv preprint arXiv:2402.05119_ (2024).
* [17] Waris Gill et al. "Privacy-Aware Semantic Cache for Large Language Models". In: _arXiv preprint arXiv:2403.02694_ (2024).
* [18] Han Guo et al. "Efficient (soft) q-learning for text generation with limited good data". In: _arXiv preprint arXiv:2106.07704_ (2021).
* [19] Tahmid Hasan et al. "XL-sum: Large-scale multilingual abstractive summarization for 44 languages". In: _arXiv preprint arXiv:2106.13822_ (2021).
* [20] Albert Q Jiang et al. "Mistral 7B". In: _arXiv preprint arXiv:2310.06825_ (2023).
* [21] Albert Q Jiang et al. "Mistral of experts". In: _arXiv preprint arXiv:2401.04088_ (2024).
* [22] Diederik P Kingma and Jimmy Ba. "Adam: A method for stochastic optimization". In: _arXiv preprint arXiv:1412.6980_ (2014).
* [23] Tom Kocmi et al. "Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet". In: _Proceedings of the Eighth Conference on Machine Translation_. 2023, pp. 1-42.
* [24] Andreas Kopf et al. "Openassistant conversations-democratizing large language model alignment". In: _Advances in Neural Information Processing Systems_ 36 (2024).
* [25] Wai-Chung Kwan et al. "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models". In: _arXiv preprint arXiv:2401.16745_ (2024).

* [26] Hugo Laurencon et al. "The bigscience roots corpus: A 1.6 tb composite multilingual dataset". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 31809-31826.
* [27] Teven Le Scao et al. "Bloom: A 176b-parameter open-access multilingual language model". In: (2023).
* [28] Patrick Lewis et al. "MLQA: Evaluating cross-lingual extractive question answering". In: _arXiv preprint arXiv:1910.07475_ (2019).
* [29] Haonan Li et al. "Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation". In: _arXiv preprint arXiv:2305.15011_ (2023).
* [30] Chin-Yew Lin. "ROUGE: A Package for Automatic Evaluation of Summaries". In: _Text Summarization Branches Out_. Barcelona, Spain: Association for Computational Linguistics, July 2004, pp. 74-81. url: https://aclanthology.org/W04-1013.
* [31] S. Longpre et al. "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning". In: _Proceedings of the 40th International Conference on Machine Learning_. 2023, pp. 22631-22648.
* [32] Niklas Muennighoff et al. "Crosslingual Generalization through Multitask Finetuning". In: _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 2023, pp. 15991-16111.
* [33] Niklas Muennighoff et al. "Crosslingual generalization through multitask finetuning". In: _arXiv preprint arXiv:2211.01786_ (2022).
* [34] Thuat Nguyen et al. "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages". In: _arXiv preprint arXiv:2309.09400_ (2023).
* [35] Kishore Papineni et al. "Bleu: a Method for Automatic Evaluation of Machine Translation". In: _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_. Ed. by Pierre Isabelle, Eugene Charniak, and Dekang Lin. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, July 2002, pp. 311-318. doi: 10.3115/1073083.1073135. url: https://aclanthology.org/P02-1040.
* [36] Edoardo Maria Ponti et al. "XCOPA: A multilingual dataset for causal commonsense reasoning". In: _arXiv preprint arXiv:2005.00333_ (2020).
* [37] Leonardo Ranaldi, Giulia Pucci, and Andre Freitas. "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations". In: _arXiv preprint arXiv:2308.14186_ (2023).
* [38] Victor Sanh et al. "Multitask Prompted Training Enables Zero-Shot Task Generalization". In: _International Conference on Learning Representations_. 2021.
* [39] Freda Shi et al. "Language models are multilingual chain-of-thought reasoners". In: _arXiv preprint arXiv:2210.03057_ (2022).
* [40] Shivalika Singh et al. "Aya dataset: An open-access collection for multilingual instruction tuning". In: _arXiv preprint arXiv:2402.06619_ (2024).
* [41] Rohan Taori et al. _Stanford Alpaca: An Instruction-following LLaMA model_. https://github.com/tatsu-lab/stanford_alpaca. 2023.
* [42] Rohan Taori et al. _Stanford alpaca: An instruction-following llama model_. 2023.
* [43] Gemini Team et al. "Gemini: a family of highly capable multimodal models". In: _arXiv preprint arXiv:2312.11805_ (2023).
* [44] Hugo Touvron et al. "Llama 2: Open foundation and fine-tuned chat models". In: _arXiv preprint arXiv:2307.09288_ (2023).
* [45] Hugo Touvron et al. "Llama: Open and efficient foundation language models". In: _arXiv preprint arXiv:2302.13971_ (2023).
* [46] Ahmet Ustun et al. "Aya model: An instruction finetuned open-access multilingual language model". In: _arXiv preprint arXiv:2402.07827_ (2024).
* [47] Bin Wang et al. "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning". In: _arXiv preprint arXiv:2309.04766_ (2023).
* [48] Yizhong Wang et al. "Self-Instruct: Aligning Language Models with Self-Generated Instructions". In: _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 2023, pp. 13484-13508.
* [49] Yizhong Wang et al. "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks". In: _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_. 2022, pp. 5085-5109.

[MISSING_PAGE_FAIL:12]

### Examples of Generated Evols and conversations

### Prompts for Evol-instruct

For Dolly, HotpotQA and MLQA we use evols from generic, OpenQA and Mintaka respectively.

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Step** & **Content** & **Evol** \\ \hline Seed Sample & What is the longest-running Broadway show ever? & \\ \hline evoled (Turn 1) Instr. & Which Broadway show holds the record for the longest run? & Concise \\ \hline GPT-4 Response & The Phantom of the Opera holds the record for the longest run on & \\  & Broadway. & \\ \hline Turn 2 & Now, break down the following multi-part question and answer & Expanding Queries \\  & each part: Who wrote this show? What are some of its most & Change Context \\  & popular songs? When did it first premiere on Broadway? & \\ \hline GPT-4 Response & 1. Who wrote this show? & \\  & The Phantom of the Opera was composed by Andrew Lloyd & Webber. The lyrics were written by Charles Hart and Richard \\  & & Stilgoe. \\  & 2. What are some of its most popular songs? & \\  & Some of its most popular songs include “The Phantom of the Opera,” “Music of the Night,” “All I Ask of You,” and “Masquerade.” & \\  & 3. When did it first premiere on Broadway? & \\  & The Phantom of the Opera first premiered on Broadway on & \\  & January 26, 1988. & \\ \hline \hline \end{tabular}
\end{table}
Table 9: Conversation example from **M2Lingual**

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Step** & **Content** & **Evol** \\ \hline Seed Sample & Formulate an answer to this elaborate question: Where is the regional airline based that Jim French is the former chairman and CEO of? & \\ \hline evoled (Turn 1) Instr. & Formulate an answer to this elaborate question: Identify the base location of the regional airline for which Jim French served as the former chairman and CEO, and explain the significance of this location in the context of the airline’s regional operations. & Combine Facts \\ \hline GPT-4 Response & Jim French served as the former chairman and CEO of Flybe, a British regional airline. The base location of Flybe was Exeter International Airport in Devon, England. & \\ \hline \hline \end{tabular} The significance of Exeter International Airport as the base location for Flybe lies in its strategic positioning within the United Kingdom. Exeter is situated in the southwest region of England, which allowed Flybe to efficiently connect various regional destinations across the UK and Europe. This location provided a central hub for operations, facilitating shorter flight routes and quick turnaround times, which are essential for regional airlines aiming to offer frequent and reliable services. Additionally, having a base in Exeter helped Flybe tap into a market with fewer competing airlines, allowing it to establish a strong regional presence and foster economic growth within the southwest of England by improving connectivity and accessibility. & Adjust Output Format \\ \hline GPT-4 Response & Jim French served as the former chairman and CEO of Flybe, a British regional airline based at Exeter International Airport in Devon, England. The significance of this location lies in its strategic positioning in the southwest of England, enabling efficient regional connectivity across the UK and Europe and fostering economic growth in the region. & \\ \hline \hline Turn 3 & Now, rephrase your response as if you were an enthusiastic travel blogger sharing fascinating details with your readers. & \\ \hline GPT-4 Response & Hey travel enthusiasts! Did you know that Jim French, the former chairman and CEO of Flybe, once led this fantastic British regional airline based at Exeter International Airport in beautiful Devon, England? It’s such a cool spot! Nestled in the southwest of England, Exeter’s prime location made it perfect for Flybe to connect countless regional destinations across the UK and Europe. This strategic hub wasn’t just about convenience; it played a crucial role in boosting regional connectivity and economic growth. So, next time you’re flying through Exeter, remember the incredible role it played in the world of regional air travel! & \\ \hline \hline \end{tabular}
\end{table}
Table 10: Conversation from **M2Lingual**

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Step** & **Content** & **Evol** \\ \hline Seed Sample & Given the following statement, predict the tags for each word: & \\  & Daniel Smith daniel.smith2@durham.ac.uk Durham University, & \\  & United Kingdom & \\ \hline evolved (Turn 1) Instr. & Identify the appropriate tags for each element in this data: Daniel & Concise \\  & Smith daniel.smith2@durham.ac.uk Durham University, United & \\  & Kingdom. Please provide a brief response. & \\ \hline GPT-4 Response & Name: Daniel Smith & \\  & Email: daniel.smith2@durham.ac.uk & \\  & Institution: Durham University & \\  & Country: United Kingdom & \\ \hline Turn 2 & Now, please reformat the output to display only the tags and & Adjust Output Format associated elements in a structured JSON format. \\ \hline GPT-4 Response & \{ "Name": "Daniel Smith", "Email": "daniel.smith2@durham.ac.uk", "Institution": "Durham University", "Country": "United \\  & Kingdom” \} \\ \hline \hline \end{tabular}
\end{table}
Table 11: Conversation from **M2Lingual**

\begin{table}
\begin{tabular}{|c p{142.3pt} p{142.3pt}|} \hline
**Step** & **Content** & **Evol** \\ \hline Seed Sample & Finish the following sentence with the best choice: To clean away the excess plastic from the edge of a project, you can & \\  & Choices: - Use a belt sander to remove the spare plastic & \\  & - Use a coffee grinder to remove the spare plastic & \\  & Answer: & \\ \hline evoled (Turn 1) Instr. & Complete the sentence with the most appropriate choice: To prevent the documents from flying around in a windy environment, you can & \\  & Choices: - Use a heavy paperweight to keep them in place & \\  & - Use a feather to keep them in place & \\  & Answer: & \\ \hline GPT-4 Response & Use a heavy paperweight to keep them in place & \\ \hline Turn 2 & Now, generate a paragraph explaining the pros and cons of using a heavy paperweight to keep documents in place in windy conditions. & Challenging Follow Up \\ \hline GPT-4 Response & Using a heavy paperweight to secure documents in a windy environment has several advantages. First, the weight of the paperweight can effectively prevent papers from being blown away, ensuring they stay in the designated area. It also allows for easy accessibility and organization of the documents as they can be quickly retrieved without the hassle of chasing loose papers. In addition, paperweights are available in various designs and materials, providing aesthetic appeal to your workspace. & \\  & However, there are also some potential drawbacks. The primary disadvantage is the risk of damage to the documents. If the paperweight is too heavy or it is dropped accidentally, it could tear or crumple the papers. Furthermore, paperweights take up desk space and can be inconvenient if you’re working with a limited area. Lastly, if the wind is extremely strong, even a heavy paperweight might not be enough to keep the documents in place. & \\ \hline Turn 3 & Now, transform your previous response into a haiku format. The haiku should have a 5-7-5 syllable structure. Reconstruct your answer if necessary to fit this format. & Random Follow Up \\ \hline GPT-4 Response & Paperweight holds firm, Yet may damage precious sheets, In strong gusts, it fails. & \\ \hline \end{tabular}
\end{table}
Table 12: Conversation from **M2Lingual**

[MISSING_PAGE_FAIL:17]

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Step** & **Content** & **Evol Type** \\ \hline \multirow{6}{*}{Seed Sample} & Continuate a scrivere il seguente testo: Oliver ha colpito Bishop con & \\  & tanta forza che il vecchio si e’stravolto all’indietro e e’caduto sul lato & \\  & del trono; poi Oliver si e’spostato sul lato di Amelie. “no’: Oliver ha & \\  & fatto un salto mentre Sam si aggrappava del palo per tirarlo fuori. “lei & \\  & e’vecchia.” & \\ \hline \multirow{6}{*}{Evolved (Turn 1) Instr.} & C’era una volta un villaggio chiamato Lanciano, famoso per le sue feste. & \\  & Un giomo, il piu coraggioso tra gli abitanti, un tomo chiamato Oliver, & \\  & decise di sidare il vecchio e autoritario leader, Bishop. Cosi, durante la & \\  & festa piu importante dell’anno, Oliver & con tanta forza che il vecchio & \\  & si e’stravolto all’indietro e’caduto sul lato del palo delle bandiere. La & \\  & folla si termo, in attesi d'evecte osa sarebbe successo dopo. Poi Oliver & \\  & si e’spostato sul lato di Amelie, la figlia di Bishop, e ha esclamato, “Non & \\  & e giusto’Lei e & \_”.” I villaggi ranon di shock. Sam, il migliore amico & \\  & di Oliver, ha fatto un salto mentre si aggrappava al palo per & \_". La \\  & tensione era palpabile. & \\ \hline \multirow{6}{*}{GPT-4 Response} & colpi Bishop & \\  & bellissima e merita di essere trattata con rispetto & \\  & stabilizzarsi & \\ \hline \multirow{6}{*}{Turn 2} & Potresti riassumere la storia in punti elenco? & Format Rewriting \\ \cline{2-2}  & Ceratamente! Ecco la storia riassunta in punti elenco: & \\  & - II villaggio di Lanciano e famoso per le sue feste. & \\  & - Oliver, il piu coraggioso abitante del villaggio, decide di sfidare il & \\  & vecchio e autoritario leader Bishop & \\  & - Drunate la festa piu importante dell’anno, Oliver colpisce Bishop con & \\  & tanta forza che quest’utilmo cade all’indietro, finendo vicino al palo & delle bandiere. & \\  & - La folla si ferma, aspettando di vedere cosa accadria dopo. & \\  & - Oliver si sposta accanto ad Amelie, la figlia di Bishop, e dichiara che & \\  & non e giusto che lei venga trattata male perche e bellissima e merita & \\  & rispetto. & \\  & - I villaggi sono in shock per la dichiarazione di Oliver. & \\  & - Sam, il migliore amico di Oliver, fa un salto e si aggrappa al palo per & \\  & stabilizzarsi. & \\  & - La tensione nell’aria e palpabile. & \\ \hline \multirow{6}{*}{Turn 3} & Per favore, riscriviu la storia in forma di elenco puntato. & Format Rewriting \\ \cline{2-2}  & Ceratamente! Ecco la storia riscritta in forma di elenco puntato: & \\  & - C’era una volta un villaggio chiamato Lanciano, famoso per le & \\  & sue feste. & \\  & - Un giorno, Oliver, il piu coraggioso tra gli abitanti, decise di sidare il & \\  & vecchio e autoritario leader, Bishop. & \\  & - Durante la festa piu importante dell’anno, Oliver colpi Bishop con tanta & \\  & forza che il vecchio si estranio all’indietro e cadde vicino al palo delle & \\  & bandiere. & \\  & - La folla si fermo, in attesa di vedere cosa sarebbe successo dopo. & \\  & - Oliver si sposto accanto ad Amelie, la figlia di Bishop, e esclamo: "Non & \\  & \& giusto’Lei e bellissima e merita di essere trattata con rispetto." & \\  & - I villaggi rimasero in shock. & \\  & - Sam, il migliore amico di Oliver, fece un salto e si aggrappo al palo per & \\  & stabilizzarsi. & \\  & - La tensione era palpabile. & \\ \hline \hline \end{tabular}
\end{table}
Table 14: Conversation from **M2Lingual** in Italian.

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Step** & **Content** & **Evol Type** \\ \hline \multirow{6}{*}{Seed Sample} & Este album, el segundo de un do notable en el ambito del hip hop, incluye & \\  & colaboraciones de varios integrantes de un colectivo conocido por sus iniciales y & \\  & su estilizacion particular. Este grupo, que surgio en la mitad de la primera decada & \\  & del siglo XXI, esta asociado a una ciudad estadooundense. ¿Podrias identificar & \\  & dicha ciudad? & \\ \hline \multirow{6}{*}{Evolved (Turn 1) Instr.} & Este album, el segundo de un do notable en el ambito del hip hop, incluye & \\  & colaboraciones de varios integrantes de un colectivo conocido por sus iniciales y & \\  & sulizacion particular. Este grupo, que surgio en la mitad de la primera decada & \\  & del siglo XXI, esta asociado a una ciudad estadooundense. ¿Podrias identificar & \\ \hline \multirow{6}{*}{GPT-4 Response} & Claro, parce que estas hablando del album "Run the Jewels 2" del duo de hip & \\  & hop Run the Jewels, compuesto por El-P y Killer Mike. Este album incluye & \\  & colaboraciones de varios miembros del colectivo conocido como “Old Future” & \\  & (estilizado como “OFWGKTA”). Odd Future es un colectivo de hip hop que & \\  & surgio a mediados de la decada de 2000 y esta asociado con la ciudad de Los & \\  & Angeles, California. & \\ \hline \multirow{6}{*}{Turn 2} & “Ahora, por favor, identififica y enumera todas las canciones de ese album en & \\  & particular, junto con sus respectivas duraciones y los artistas que colaboraron en & \\ \cline{1-1}  & cada una de ellas.” & \\ \hline \multirow{6}{*}{GPT-4 Response} & El album “Run the Jewels 2" de Run the Jewels incluye las siguientes canciones: & \\  & 1. “Jeopardy” - 3:21 & \\  & 2. “Oh My Darling Donf Cry” - 3:24 & \\  & 3. “Blockbuster Night, Part 1" - 2:32 & \\  & 4. “Close Your Eyes (And Count to Fuck)” con Zack de la Rocha - 3:54 & \\  & 5. “All My Life” - 3:08 & \\  & 6. “Lie, Cheat, Steau ” - 3:29 & \\  & 7. “Early” con BOOTS - 3:44 & \\  & 8. “All Due Respect” con Travis Barker - 3:40 & \\  & 9. “Love Again (Akinyele Back)” con Gangsta Boo - 3:02 & \\  & 10. “Crown” con Diane Coffee - 3:48 & \\  & 11. “Angel Duster” - 5:12 & \\ \hline \hline \end{tabular}
\end{table}
Table 15: Conversation from **M2Lingual** in Spanish.

[MISSING_PAGE_EMPTY:20]

\begin{table}
\begin{tabular}{c l l} \hline \hline

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline \hline Adding Distractors & Given a prompt \#given\_prompt \#given\_prompt \# in the same language that adds unrelated or distracting information in the article which is not relevant to the main topic. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Technical Jargon & Given a prompt \#given\_prompt \# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding technical jargon or industry specific terms that makes it difficult to summarize. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Inconsistencies In & Given a prompt \#given\_prompt \#represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language to include contradictions or inconsistencies within the article thus forcing the summarizer to discern which piece of information is accurate and relevant. \#given\_prompt\textgreater{} \\ \hline Multiple Topics & Given a prompt \#given\_prompt \# represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that covers multiple topics or subtopics thus make summarization more complicated. \#given\_prompt\textgreater{} \\ \hline Metaphors Idiom & Given a prompt \#given\_prompt\# represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# using complex metaphors, idioms and cultural references thus making summarization more challenging. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Long Distance & Given a prompt \#given\_prompt \#represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt \# by increasing the distance between related pieces of information in the text as this requires understanding the deeper structure of the text. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Multiple Languages & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in clanguage\_2\_5 and asking to respond in clanguage\_1\_5 thus \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Unstructured Data & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# by presenting the article in a non-linear or non-chronological format thus increasing the complexity as it becomes challenging to pick out the main points and summarize them accurately. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Personal Opinion & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# by incorporating bias or personal opinion as it greatly complicates the summarization process as the summarizer needs to remain neutral and objective. \#given\_prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 19: Abstract Summarization

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline \hline Transliteration & Rewrite the \#given\_prompt\# as an \textless{}transit\_language\textgreater{} transliteration, and create \#new\_prompt\#. \\  & Additionally, conclude with a request to respond in \#transit\_language\textgreater{} transliteration. \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Diallect & You are a brilliant \textless{}prompt\_language\textgreater{} native speaker. Rewrite \#given\_prompt\# by changing the dialect to \#prompt\_language\textgreater{} and create \#new\_prompt\#. Finally, ask to respond in the same \textless{}prompt\_language\textgreater{} dialect. Write the \#new\_prompt\# prompt\# in \textless{}prompt\textgreater{} \\ \hline Concise & Re-write the \#given\_prompt\# concisely, and create \#new\_prompt\#. Additionally, conclude with a request to respond concisely. Write the \#new\_prompt\# \textless{}prompt\#: \#qprompt\textgreater{} \\ \hline Deepen & Slightly increase the depth and breadth of \#given\_prompt\#, prompt\#, and create \#new\_prompt\#. Write the \#new\_prompt\# is \textless{}prompt\#: \#qprompt\textgreater{} \\ \hline Concertize & Make \#given\_prompt\# slightly more concrete, and create \#new\_prompt\#. Additionally, conclude with a request for an AI assistant to respond with the a detailed and concrete response. Write the \#new\_prompt\# in \textless{}prompt\_language\textgreater{} \#given\_prompt\#: \#qprompt\textgreater{} \\ \hline Increase Reasoning & If \#given\_prompt\# can be solved with just a few simple thinking processes, rewrite it to explicitly request multi-step reasoning, and create \#new\_prompt\#. Write the \#new\_prompt\# \textless{}prompt\_language\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 18: Generic

\begin{table}
\begin{tabular}{l p{142.3pt}} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Jargon & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by incorporating jargon to the jokes that are specific to a certain profession, field, or hobby thus requiring deeper knowledge of the field in order to explain the joke properly. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Slang & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that use slang or colloquial language, thus making it harder to understand and explain the punchline. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Cultural References & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that increasingly use jokes which are culture-specific as will require cultural understanding tp provide explanations. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Non Explicit Punchline & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language such that jokes have a punchline isn’t explicitly stated, but rather implied. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Time Bound & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by making jokes that were relative to a certain time period or current event, thus making it harder to grasp. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Compound Jokes & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language such that it uses compound jokes which contain multiple punchlines within the same joke. This would make the explanation task difficult as one would need to explain multiple punchlines coherently. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Language & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in \textless{}language\_2\textgreater{} such that it challenge the language skills. This would make the explanation task difficult as one would need to explain multiple punchlines coherently. Finally ask to respond with explanation in \textless{}language\_1\textgreater{} \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Long & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language but make the jokes exceedingly long where the punchline isn’t delivered immediately and requires you to remember or understand preceding parts of the joke. \#given\_prompt\textgreater{} \textless{}prompt\textgreater{} \\ \hline Double Entendre & Given a prompt \#given\_prompt\# that asks an explanation of a joke, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language but utilize jokes with double entendre, where there are two possible interpretations. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 20: Joke Explain

\begin{table}
\begin{tabular}{l p{142.3pt}} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline More Choices & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing adding more choices to the question that are relevant to the topic but not correct. This will make it challenging to answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline More Complex & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by making it more complex using technical or domain specific jargon This will make it challenging to understand the question thus making it difficult to answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Negation & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking negative questions that require the recognition of the negation included in the sentences. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multiple Corrct Options & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing adding more choices to the question that are correct and relevant to the topic. This will make it challenging as one will need to choose all correct options. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Need For Context & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking questions that require additional context other than the one provided in the topic.This will make it challenging as it will evaluate the knowledge someone has on the topic. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Justification & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking to respond with the correct answer and provide a detailed justification for the answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Distracting Long & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking questions that require additional context other than the one provided in the topic. This will make it challenging as it will evaluate the knowledge someone has on the topic. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Close Choices & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by adding more choices to the question that are closely related to each other. This will make the task more challenging. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline No Correct Option & Given a prompt \#given\_prompt\# that represents a topic and multiple choice question related to that topic, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the choices such that no choice is the correct answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 21: Flan Qa

\begin{table}
\begin{tabular}{l p{284.5pt}} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Complex Jargon & Given a prompt \#given\_prompt\# their requires answering a question about a text snippet, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by either using technical jargons, \\  & domain-specific language, technical or scientific complexities thus making the task more challenging as it requires deep understanding and specialized knowledge to answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multiple Languages & Given a prompt \#given\_prompt\# that requires answering a question about a text snippet, based upon the \#given\_prompt\# create a \#new\_prompt\# in \textless{}language\_2\textgreater{} including code-switching i.e. switching between languages within a single conversation or sentence. Finally ask to respond in \#LENGUAC\_1\textgreater{} \\  & \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Long Complex Queries & Given a prompt \#given\_prompt\# that requires answering a question about a text snippet, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by providing vital pieces of information in the text snippet in a non-linear, disconnected manner thus requiring piecing them together accurately to form an explanation.. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Disconnected Clues & Given a prompt \#given\_prompt\# that requires answering a question about a text snippet, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by translating either the question or the text snippet (but not both) in any language thus making the task more challenging as it requires understanding or different languages. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Emotion Sarcasam & Given a prompt \#given\_prompt\# that requires answering a question about a text snippet, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding emotion or sarcasm in the text snippet as recognizing and responding can be a huge challenge. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Advanced Reasoning & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by writing text snippet or question that require advanced logic or reasoning, such as those found in certain categories of IQ test, thus make it more difficult to answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Ambiguity & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by including deceptive or ambiguous phrases that might lead to misinterpretation can complicate the task. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Unknown Context & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding larger context not included in the text, as it would require to infer missing details, which adds complexity to the task. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Implicit Bias & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by including subtle biases or nuances, recognizing and appropriately responding to these can be challenging. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 22: Flan Cot

\begin{table}
\begin{tabular}{l p{142.3pt}}
**Evol Type** & **GPT-4 Prompt** \\ \hline Ambiguity & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the questions only in the same language but making it much more vague and ambiguous thus making it not so straightforward to answer. \#given\_prompt#: \textless{}prompt> \\ \hline Long Form Question & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the questions only in the same language by making it longer i.e. formulating the questions in long and complex sentences thus requiring the system to decipher the main question. \#given\_prompt#: \textless{}prompt> \\ \hline Multilingual & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the questions in \textless{}language\_2> and article in \textless{}language\_1>, having different linguistic structure. Finally, ask to answer the question in the \textless{}language\_2>. \#given\_prompt#: \textless{}prompt> \\ \hline Combine Facts & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the questions only in the same language by combining multiple facts thus making the questions more complex and requiring combining multiple facts to answer correctly. \#given\_prompt#: \textless{}prompt> \\ \hline Implicit Question & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the questions only in the same language by asking implicit questions where the answer isn’t explicit and requires understanding of the underlying implication. \#given\_prompt#: \textless{}prompt> \\ \hline Negative Questions & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by asking negative questions in the same language that require the recognition of the negation included in the sentences. \#given\_prompt#: \textless{}prompt> \\ \hline Inference & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by changing the question only in the same language by asking questions that require a degree of inference or deduction not directly provided. \#given\_prompt#: \textless{}prompt> \\ \hline Multichoice Questions & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by asking multiple choice questions in the same language based upon the given article, where the choices are from the article itself. Finally ask the model to respond with the correct choice and explain the decision. \#given\_prompt#: \textless{}prompt> \\ \hline More Reasoning & Given a prompt \#given\_prompt# that represents an article and multiple questions related to that article, based upon the \#given\_prompt# create a \#new\_prompt# by asking questions in the same language that require multistep reasoning processes, where participants need to follow a sequence of logical steps to arrive at the correct answer. \#given\_prompt#: \textless{}prompt> \\ \hline \end{tabular}
\end{table}
Table 23: Flan Coqa

[MISSING_PAGE_FAIL:27]

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Ambiguity & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language but making it much more vague and ambiguous thus making it not so straightforward to answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Long Form Question & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language by making it longer i.e. formulating the questions in long and complex sentences thus requiring the system to decipher the main question. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multilingual & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question in \textless{}language\_2\textgreater{} and article in \textless{}language\_1\textgreater{}, having different linguistic structure. Finally, ask to answer the question in the \textless{}language\_2\textgreater{}, \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Combine Facts & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language by combining multiple facts thus making the question more complex and requiring combining multiple facts to answer correctly. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Implicit Question & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language by asking implicit question where the answer isn’t explicit and requires understanding of the underlying implication. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Negative Questions & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language by asking negative questions that require the recognition of the negation included in the sentences. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Inference & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by changing the question only in the same language by asking questions that require a degree of inference or deduction not directly provided. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multichoice Questions & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking multiple choice question based upon the given article, where the choices are from the article itself. Finally ask the model to respond with the correct choice and explain the decision. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline More Reasoning & Given a prompt \#given\_prompt\# that represents an article and a question related to that article, based upon the \#given\_prompt\# create a \#new\_prompt\# by asking that require multistep reasoning processes, where participants need to follow a sequence of logical steps to arrive at the correct answer. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \end{tabular}
\end{table}
Table 26: Mintaka

\begin{table}
\begin{tabular}{l l} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Adding Distractors & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that adds unrelated or distracting information in the article which is not relevant to the main topic. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Technical Jargon & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding technical jargon or industry specifc terms that makes it difficult to summarize. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Inconsistencies In & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language to include contradictions or inconsistencies within the article thus forcing the summarizer to discern which piece of information is accurate and relevant. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multiple Topics & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that covers multiple topics or subtopics thus make summarization more complicated. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Metaphors Idiom & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# using complex metaphors, idioms and cultural references thus making summarization more challenging. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Long Distance & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# by increasing the distance between related pieces of information in the text as this requires understanding the deeper structure of the text. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Cross Lingual Summary & Given a prompt \#given\_prompt\# that represents some article about a topic, rewrite the article only and create a \#new\_prompt\# in any \textless{}language\_2\textgreater{}. Finally ask to provide a summary in the \textless{}language\_1\textgreater{}. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Unstructured Data & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# by presenting the article in a non-linear or non-chronological \# format thus increasing the complexity as it becomes challenging to pick out the main points and summarize them accurately. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Personal Opinion & Given a prompt \#given\_prompt\# that represents an article to be summarized, based upon the \#given\_prompt\# create a \#new\_prompt\# by incorporating bias or personal opinion as it greatly complicates the summarization process as the summarizer needs to remain neutral and objective.. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 27: Cross Summarization

\begin{table}
\begin{tabular}{l p{142.3pt}} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Complex Question & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by formulating the question in a more complex way, requiring deeper understanding, reasoning, and inferential abilities. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Advanced Vocab & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by using more complex language and advanced vocabulary to increase increase the difficulty level, as it requires deeper understanding of language and words to compose a context. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multiple Themes & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking question on multiple themes or topics thus making it harder to generate a context around all topics. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Ambiguity & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by making question vague and ambiguous thus making it little harder to compose a context around all topics. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline More Details & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more detail, domain specific knowledge and technical jargions to the question thus making it difficult to generate a context. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Increase Reasoning & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking question that are at the intersection of multiple topic thus require understanding of all topics and how the topics are related to each other. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Time & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking question where the context or the answer changes over time, thus assessing how up to date someone is. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Abstract Topics & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking question where the context needs to be generated on some abstract topic where opinion varies from person to person. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Structured Info & Given a prompt \#given\_prompt\# that requires composing a context around some question, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking question where the context should be generated in a structured form as bulleted list with topics and sub-topics. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 28: Adversarial Qa

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline \hline \multirow{2}{*}{Oigect Interaction} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that involves object interaction reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Oigect Properties} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires object transformation reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Logical Sequencing} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires object transformation reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Oigect Transformation} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires object transformation reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline More Choices & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more options and asking to finish with all correct options. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Justification & Given a prompt \#given\_prompt\# involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more options that are incorrect this making it difficult to identify correct option. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Incorrect Choices} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more options that are incorrect this making it difficult to identify correct option. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Double Negatives & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language with double negatives thus making it hard to understand and can increase the complexity of the task. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Theoretical Scenario & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by making the base scenarios less straightforward and more abstract thus making the task more complex. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \hline \end{tabular}
\end{table}
Table 29: Soda

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline \hline \multirow{2}{*}{Oigect Properties} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that involves object interaction reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Object Properties} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires understanding and reasoning over the object properties. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Logical Sequencing & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires logical sequence reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Object Transformation} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language that requires object transformation reasoning. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline More Choices & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more options and asking to finish with all correct options. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Justification & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by asking to give a detailed step-by-step justification of the chosen option. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Incorrect Choices} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding more options that are incorrect this making it difficult to identify correct option. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline Double Negatives & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language with double negatives thus making it hard to understand and can increase the complexity of the task. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \multirow{2}{*}{Theoretical Scenario} & Given a prompt \#given\_prompt\# that involves commonsense physical reasoning, asking to finish a sentence with two possible options based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by making the base scenarios less straightforward and more abstract thus making the task more complex. \#given\_prompt\#: \text{<prompt\#}\} \\ \hline \hline \end{tabular}
\end{table}
Table 30: Commonsense

\begin{table}
\begin{tabular}{l p{142.3pt}} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Idioms Phrases & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language Idioms and phrases have meanings different from their literal meanings, using them for paraphrasing can add complexity. \#given\_prompt\# \textless{}prompt\textgreater{} \\ \hline Abbreviations & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by converting certain commonly known phrases or organizations into their abbreviated forms thus making identification more difficult. \#given\_prompt\# \textless{}prompt\textgreater{} \\ \hline Sentence Structure & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by increasing the complexity of sentences i.e. either rearranging the individual sentences, making use of passive and active voice or changing the sentence structural form. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Information & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by adding or subtracting relevant details from one sentence which do not change the main theme but add extra entities can make it challenging. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Variation & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by introducing variations in dialect, accent, slang, or colloquial language usage can make the task complex. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Negation & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by introducing negations or double negatives, the meaning of the sentence could be the same but the formation different. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Time Navigation & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by changing the time description (from past to present or future) in paraphrased sentences. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Cultural Inferences & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by using different cultural inferences in each sentence. The task gets complicated when two sentences infer same conclusion but uses culturally different examples or metaphors. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Length Variation & Given a prompt \#given\_prompt\# that represents two sentences and asks whether the two are paraphrases or not, based upon the \#given\_prompt\# create a \#new\_prompt\# in the same language by using different sentence length one can be short and another very long. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 31: Pawsx

\begin{table}
\begin{tabular}{l l} \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Ambiguity & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language but making it much more vague and ambiguous thus making it not so straightforward to answer. \\  & \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Long Form Question & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by making it longer i.e. formulating the questions in long and complex sentences thus requiring \\  & the system to decipher the main question. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multilingual & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in \textless{}language\_2\textgreater{} \\  & have different linguistic structure. Finally, ask to answer the question in the \textless{}language\_1\textgreater{}. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Combine Facts & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by combining multiple facts thus making the question more complex and requiring combining \\  & multiple facts to answer correctly. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Implicit Question & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by asking implicit question where the answer isn’t explicit and requires understanding of the \\  & underlying implication. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Negative Questions & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by asking negative questions that require the recognition of the negation included in the sentences. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Inference Deduction & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by design question that require a degree of inference or deduction that might not be directly \\  & provided anywhere. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Multiple Answers & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by design multiple-choice questions where more than one answer could be correct, making it more \\  & complex to find the right named entities. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline Comparitive Questions & Given a prompt \#given\_prompt\# based upon the \#given\_prompt\# create a \#new\_prompt\# in the same \\  & language by asking questions would require the system to understand the comparative degree being asked \\  & about, making extraction or sorting from data more complicated. \#given\_prompt\#: \textless{}prompt\textgreater{} \\ \hline \end{tabular}
\end{table}
Table 32: Openqa

### Prompt for Multiturn Evol-instruct

\begin{table}
\begin{tabular}{l l} \hline \hline
**Evol Type** & **GPT-4 Prompt** \\ \hline Challenging & - The follow-up instruction must be challenging in terms of difficulty in comparison with the initial instruction. \\ \hline Ambiquous & - The follow-up instruction must refer to the previous result obtained from the initial instruction in an ambiguous way (e.g., summarize that under 3 paragraphs...) \\ \hline Redircection & - The follow-up instruction must abruptly change the type of the request/task or the thematic/topic of the initial instruction with no transition formula (e.g. let’s shift gears) or even referring to the initial instruction. \\ \hline Generic Rewriting & - The follow-up instruction must request a change in the [ property] of the response to the INITIAL INSTRUCTION. \\ \hline Feedback Handling & - The follow-up instruction must indicate that what the AI model responded to the INITIAL INSTRUCTION was not good enough (you must specify on which random aspect). \\ \hline Random & - The follow-up instruction must request to change the response content or format in unique and unusual ways (e.g. switch to JSON or YAML or even a custom format illustrated by a template or very specific format description, keep all words starting with certain letter, remove every other word... You must specify this way in the instruction). \\ \hline Context Retention & - The follow-up instruction must present a request/task that will test the ability of the model to retain the context of the conversation established by the previous instructions. \\ \hline Format Rewriting & - The follow-up instruction must request a change in the format of the response to the previous instruction. \\ \hline Persona Rewriting & - The follow-up instruction must request a change in the persona of the response to the previous instruction. \\ \hline Detailed Constraints & - The follow-up instruction must add detailed constraints, like specifying the desired output format. Also involves providing more specific parameters or criteria to narrow down search results. Examples include specifying keywords, time ranges, locations, categories, or sources. \\ \hline Adjust Output Format & - The follow-up instruction must ask to adjust the output format as users may request specific formats for the output, such as text-only, summarized results, or structured data formats. \\ \hline Expanding Queries & - The follow-up instruction must ask to expand on a certain topic as users might want to broaden the search scope to include related topics or synonyms. \\ \hline Refocus Queries & - The follow-up instruction must be a refocus query as users may wish to refocus the query to target a specific aspect or angle of their original request. \\ \hline Change Context & - The follow-up instruction must introduce a new topic or context that is related to the current conversation, allowing the chatbot to provide a different perspective or information. \\ \hline Clarification & - The follow-up instruction must ask for clarification as the chatbot may provide a complex or unclear response, ask for clarification to encourage it to expand on its answer. \\ \hline Chatbot Opinion & - The follow-up instruction must encourage the chatbot to provide its own perspective or opinion on a topic, which can help create a more dynamic and engaging conversation. \\ \hline Open Ended Questions & - The follow-up instruction must ask open-ended questions that require more detailed and thoughtful responses, encouraging the chatbot to provide more information and keep the conversation going. \\ \hline Complex Queries & - The follow-up instruction must ask to create a multi-part question or instruction and see how the chatbot manages to break down and answer each part. \\ \hline Pronouns & - The follow-up instruction must ask a question that uses pronouns like “it,”,"he," or “she’ after some gap in the conversation. The bot should have to remember the noun the pronoun is referring to. \\ \hline Engaging Conversation & - The follow-up instruction must engage the chatbot in a conversation about a topic that requires knowledge of previous interactions. \\ \hline Recall Information & - The follow-up instruction must ask the chatbot to recall the details of the earlier turns in the conversation. \\ \hline \hline \end{tabular}
\end{table}
Table 33: Multiturn EvolsYour goal is to create a follow-up instruction to an INITIAL INSTRUCTION given to an AI model. You must design the follow-up using these specifications:

- The follow-up instruction must read like it's addressed to an AI model and not to another human. As such it should exclude requests impossible for an AI model to do (e.g. watch a movie or build a house).

- The follow-up instruction should be fully relevant and make sense regardless of the AI model's previous answer to the INITIAL INSTRUCTION. As such, it should rely on the INITIAL INSTRUCTION only and not on a hypothetical, unknown response by the AI model.

- The follow-up instruction should be in < _[language]_ > and should be a natural continuation of the INITIAL INSTRUCTION.

_[follow_up_type]_

INITIAL INSTRUCTION: "_[instruction]_"

Provide directly the follow-up instruction requested with no additional comment, text or explanation, strictly in a valid json object:

{

"follow_up_user_prompt": "..."

}

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, the claims made in the abstract and introduction are discussed throughout the paper and empirically shown in section 5 (Results).
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, we briefly describe limitations of our work in section 8.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We do not present theoretical assumptions or proofs in our paper, which is empirical in focus.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes],

Figure 4: Multiturn Prompt to GPT-4

Justification: We share the full taxonomy in appendix of this paper, describe steps in details (section 3) to generate **M2Lingual**, and provide all model hyperparameters (section 4.2).
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide all details for each aspect of the taxonomy based evols in appendix of this paper. We will share a small sample of the dataset and our code repository in supplementary material. We will share the full dataset and trained models with the camera-ready version of the paper.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe hyperparameters in section 4.2, dataset statistics in table 1, table 2, and captions or in-rows of each of the result tables.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We could not include the standard deviations in result tables in main content due to space constraints.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Mentioned in section 4 Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We adhere to the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the ethical considerations along with limitations in Section 8.

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We discuss the low risk of toxic or offensive data in our dataset in Section 8.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all relevant work that we utilize, and follow the respective licenses.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide documentation for reproducibility and our repository (provided in supplementary material) will also have documentation.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our data is fully synthetic and we did not employ human subjects to construct the datasets.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Since the dataset us fully synthetic and we do not conduct research with human subjects.