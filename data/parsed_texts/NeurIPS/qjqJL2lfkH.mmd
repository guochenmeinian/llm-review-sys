# Rank-1 Matrix Completion with

Gradient Descent and Small Random Initialization

 Daesung Kim

Samsung Electronics

dskim95phd@gmail.com

&Hye Won Chung

School of Electrical Engineering

KAIST

hwchung@kaist.ac.kr

###### Abstract

The nonconvex formulation of the matrix completion problem has received significant attention in recent years due to its affordable complexity compared to the convex formulation. Gradient Descent (GD) is a simple yet efficient baseline algorithm for solving nonconvex optimization problems. The success of GD has been witnessed in many different problems in both theory and practice when it is combined with random initialization. However, previous works on matrix completion require either careful initialization or regularizers to prove the convergence of GD. In this paper, we study the rank-1 symmetric matrix completion and prove that GD converges to the ground truth when small random initialization is used. We show that in a logarithmic number of iterations, the trajectory enters the region where local convergence occurs. We provide an upper bound on the initialization size that is sufficient to guarantee the convergence, and show that a larger initialization can be used as more samples are available. We observe that the implicit regularization effect of GD plays a critical role in the analysis, and for the entire trajectory, it prevents each entry from becoming much larger than the others.

## 1 Introduction

Recovering a low-rank matrix from a set of linear measurements is at the heart of many statistical learning problems. Depending on the structure of the matrix and the linear measurements, it reduces to various problems such as phase retrieval [1], blind deconvolution [2], and matrix sensing [3]. Matrix completion [4] is also one such type of problem where each measurement provides an entry of the matrix, and the goal is to recover the low-rank matrix from a partial, usually very sparse, observation of the entries. One of the most notable applications of matrix completion is collaborative filtering [5], which aims to predict user preferences for items based on a highly incomplete observation of user-item ratings. There are also a number of different applications, such as principal component analysis [6] and image reconstruction [7], just to name a few.

Extensive amount of work has been dedicated to provide an efficient recovery algorithm for matrix completion with theoretical guarantees [8]. The convex relaxation based nuclear norm minimization [4; 9] was the first algorithm proven to recover the matrix with near optimal sample complexity. Despite its theoretical success, the convex algorithm was found hard to be used in practical scenarios due to its unaffordable computational complexity and memory size. Therefore, the nonconvex formulation of matrix completion with quadratic loss has received significant attention in recent years. Many different algorithms have been proposed for the nonconvex problem, and their convergence toward the ground truth has been analyzed. Examples include optimization on Grassmann manifolds [10], alternating minimization [11], projected gradient descent [12], gradient descent with regularizer [13], and (vanilla) gradient descent [14; 15].

Gradient descent (GD) has served as a baseline algorithm for solving nonconvex optimization problems. However, the convergence of GD to global minimizers is not guaranteed, and it can take exponential time to escape saddle points [16]. Nevertheless, GD with random initialization has been shown to successfully recover the global minimum in many different problems such as phase retrieval [1], matrix sensing [17], matrix factorization [18], and neural network training [19]. Previous work on matrix completion [14, 15] proved the convergence of GD under the spectral initialization, which locates the initial point in the local region of the minima. However, the role of random initialization in solving matrix completion with GD is not fully understood yet, although its success is observed in practice. Therefore, we aim to answer the following question:

_Can GD with random initialization solve the nonconvex matrix completion problem?_

We answer this question affirmatively and show that GD with small random initialization successfully converges to the ground truth for rank-1 symmetric matrix completion. In the analysis, we use vanilla GD, which does not incorporate any modifications, such as regularization or truncation, into the GD algorithm. We also characterize the entire trajectory that GD follows by showing that the trajectory is well approximated by the fully observed case. The small initialization plays a critical role in analyzing the trajectory of the early stages, where the randomly initialized vector is nearly orthogonal to the first eigenvector of the ground truth matrix. We provide a bound on the required initialization size for the algorithm to converge, and our bound suggests that one can use a larger initialization to improve the convergence speed as more samples are provided. However, in any case, GD with a small random initialization takes only logarithmic amount of time (with respect to the matrix dimension) to reach the point where local convergence can begin. To the best of our knowledge, this is the first result on matrix completion that proves the convergence of vanilla GD without a carefully designed initialization.

Although our result is restricted to the rank-1 case, we believe that this work provides an important evidence for understanding the more general rank-\(r\) case. At the end of this paper, we will discuss some technical difficulties that the rank-\(r\) case naturally has, and provide some empirical results related to them. However, studying the rank-1 matrix completion problem is not only motivated by theoretical interest, but the problem itself also appears in some practical problems such as crowdsourcing [20, 21].

**Related Works** This work is motivated by the recent success of small initialization in matrix factorization and matrix sensing. It was first conjectured in [22] that sufficiently small step sizes and initialization lead GD to converge to the minimum nuclear norm solution of a full-dimensional matrix sensing problem. The conjecture was proved in [17] for the fully overparameterized matrix sensing under the standard restricted isometry property (RIP). A recent study by [23] provided more general results by showing that the early iterations of GD with small initialization have spectral bias. Many other works such as [24, 25, 26] have also studied how GD or gradient flow with small initialization implicitly forces the recovered matrix to be low-rank. However, the recovery guarantee for matrix completion has not been provided by any work.

For the matrix sensing where RIP holds, the loss function has global benign geometry in that it does not contain any spurious local minima or non-strict saddle points [27]. In the case of matrix completion, a similar result was obtained but with a regularizer that penalizes the matrices with large rows [28]. Controlling the norm of each row (absolute value of each entry in the case of rank-1) is the biggest hurdle in the analysis of matrix completion. In the local convergence analysis of [14], it was proved that GD implicitly regularizes the largest \(\ell_{2}\)-norm of the rows of error matrices, showing that explicit regularization is unnecessary. In this paper, we also prove that such an implicit regularization is induced by GD when it starts from a point of small size. We show that the trajectory is close to the fully observed case in both \(\ell_{2}\) and \(\ell_{\infty}\) norms. Thus, the trajectory is confined to the region where it has benign geometry, and GD can converge without an explicit regularizer.

**Notations** We denote vectors with lowercase bold letters and matrices with uppercase bold letters. The components or entries of them are written without bold. We use \(\left\lVert\cdot\right\rVert_{2}\) and \(\left\lVert\cdot\right\rVert_{\infty}\) to denote \(\ell_{2}\) and \(\ell_{\infty}\)-norm of vectors, respectively, and \(\left\lVert\cdot\right\rVert_{\mathrm{F}}\) is used for Frobenius norm of matrices. For any norm \(\left\lVert\cdot\right\rVert\) and two vectors \(\bm{x},\bm{y}\), we let \(\left\lVert\bm{x}\pm\bm{y}\right\rVert=\min\{\left\lVert\bm{x}+\bm{y}\right\rVert,\left\lVert\bm{x}-\bm{y}\right\rVert\}\). Asymptotic dependencies with respect to the matrix dimension are denoted with the standard big \(O\) notations, or with the symbols, \(\lesssim,\asymp\) and \(\gtrsim\).

Problem Formulation

The matrix completion problem aims to reconstruct a low-rank matrix from partially observed entries. In this paper, we focus on the case where the ground truth matrix, denoted by \(\bm{M}^{\star}\in\mathbb{R}^{n\times n}\), is a rank-1 positive semidefinite matrix. Thus, the ground truth matrix is decomposed as \(\bm{M}^{\star}=\lambda^{\star}\bm{u}^{\star}{\bm{u}^{\star}}^{\top}\) with \(\lambda^{\star}>0\) and a unit vector \(\bm{u}^{\star}\). We define \(\bm{x}^{\star}=\sqrt{\lambda^{\star}}\bm{u}^{\star}\) so that \(\bm{M}^{\star}=\bm{x}^{\star}{\bm{x}^{\star}}^{\top}\). To follow the standard incoherence assumption, we let \(\left\|\bm{u}^{\star}\right\|_{\infty}=\sqrt{\frac{\mu}{n}}\) and allow \(\mu\) to be as large as \(\mathrm{poly}(\log n)\). We consider a random sampling model that is also symmetric as \(\bm{M}^{\star}\). Each entry in the diagonal and the upper (or lower) triangular part of \(\bm{M}^{\star}\) is independently revealed with probability \(0<p\leq 1\). We consider the noisy case where Gaussian noise is added to each observation. Formally, we get as an observation the matrix \(\bm{M}^{\circ}\) whose \((i,j)\)th entry is \(\frac{1}{p}\delta_{ij}(M^{\star}_{ij}+E_{ij})\), where \(\left[\delta_{ij}\right]_{1\leq i\leq j\leq n}\) are independent Bernoulli random variables with expectation \(p\) and \(\left[E_{ij}\right]_{1\leq i\leq j\leq n}\) are independent Gaussian random variables with the distribution \(\mathcal{N}(0,\sigma^{2})\). They are both symmetric in the sense that \(\delta_{ij}=\delta_{ji}\) and \(E_{ij}=E_{ji}\) for all \(1\leq i\leq j\leq n\). We use \(\bm{E}\) to denote the symmetric matrix whose entries are \(E_{ij}\). We denote the set of observed entries as \(\Omega:=\left\{(i,j)\mid\delta_{ij}=1\right\}\), and define an operator \(\mathcal{P}_{\Omega}\) on matrices that sets the entries not contained in \(\Omega\) to zero. (e.g. \(\bm{M}^{\circ}=\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star}+\bm{E})\))

To recover the matrix \(\bm{M}^{\star}\), we find \(\bm{x}\in\mathbb{R}^{n}\) that minimizes the nonconvex loss function \(f(\bm{x})\), which is the sum of the squared differences on the observed entries. It is explicitly written as \(f(\bm{x}):=\frac{1}{4p}\sum_{(i,j)\in\Omega}(x_{i}x_{j}-x_{i}^{\star}x_{j}^{ \star}-E_{ij})^{2}\). We apply vanilla GD to solve the optimization problem starting from a small randomly initialized vector \(\bm{x}^{(0)}\). Each entry of \(\bm{x}^{(0)}\) is sampled independently from the Gaussian distribution \(\mathcal{N}\left(0,\frac{1}{n}\beta_{0}^{2}\right)\), so that the squared norm of \(\bm{x}^{(0)}\) is expected to be \(\beta_{0}^{2}\). The update rule of GD is written as

\[\bm{x}^{(t+1)}=\bm{x}^{(t)}-\eta\nabla f\left(\bm{x}^{(t)}\right)=\bm{x}^{(t)} -\frac{\eta}{p}\mathcal{P}_{\Omega}\left(\bm{x}^{(t)}\bm{x}^{(t)\top}\right) \bm{x}^{(t)}+\eta\bm{M}^{\circ}\bm{x}^{(t)},\] (1)

where \(\eta>0\) is the step size.

We define \(F\) as the loss function \(f\) when all entries of \(\bm{M}^{\star}\) are observed without noise, i.e., \(F(\bm{x}):=\frac{1}{4}\big{\|}\bm{x}\bm{x}^{\top}-\bm{M}^{\star}\big{\|}_{ \mathrm{F}}^{2}\). We also define \(\widetilde{\bm{x}}^{(t)}\) as the trajectory of GD when it is applied to \(F\) with the same initial point \(\bm{x}^{(0)}\), i.e., \(\widetilde{\bm{x}}^{(t)}\) is the trajectory of the fully observed case. Specificially, it evolves with

\[\widetilde{\bm{x}}^{(t+1)}=\widetilde{\bm{x}}^{(t)}-\eta\nabla F(\widetilde{\bm {x}}^{(t)})=\widetilde{\bm{x}}^{(t)}-\eta\Big{\|}\widetilde{\bm{x}}^{(t)} \Big{\|}_{2}^{2}\widetilde{\bm{x}}^{(t)}+\eta\bm{M}^{\star}\widetilde{\bm{x}}^ {(t)}\] (2)

from the same starting point \(\widetilde{\bm{x}}^{(0)}=\bm{x}^{(0)}\).

Lastly, we introduce the so-called _leave-one-out_ sequences. These were the main ingredient in controlling the \(\ell_{\infty}\)-norm of trajectory in [14]. We use them for a similar purpose. For each \(l\in[n]\), we define an operator \(\mathcal{P}_{\Omega}^{(l)}\) such that \(\mathcal{P}_{\Omega}^{(l)}(\bm{X})\) is equal to \(\bm{X}\) on the \(l\)th row and column, and equal to \(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{X})\) otherwise. The \(l\)th leave-one-out sequence, \(\bm{x}^{(t,l)}\), evolves with

\[\bm{x}^{(t+1,l)}=\bm{x}^{(t,l)}-\eta\mathcal{P}_{\Omega}^{(l)}\Big{(}\bm{x}^{(t, l)}\bm{x}^{(t,l)\top}\Big{)}\bm{x}^{(t,l)}+\eta\bm{M}^{(l)}\bm{x}^{(t,l)},\] (3)

for \(\bm{x}^{(0,l)}=\bm{x}^{(0)}\), where \(\bm{M}^{(l)}=\mathcal{P}_{\Omega}^{(l)}(\bm{M}^{\star})+\bm{E}^{(l)}\), and \(\bm{E}^{(l)}\) is obtained by zeroing out the \(l\)th row and column of \(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})\).

## 3 Main Results

In this section, we present our main results. The first main result concerns the global convergence of GD with small random initialization.

**Theorem 3.1**.: _Let us consider a rank-1 matrix completion problem that recovers the matrix \(\bm{M}^{\star}=\bm{x}^{\star}{\bm{x}^{\star}}^{\top}\in\mathbb{R}^{n\times n}\) such that \(\left\|\bm{x}^{\star}\right\|_{2}=\sqrt{\lambda^{\star}}\) and \(\left\|\bm{x}^{\star}\right\|_{\infty}=\sqrt{\frac{\mu}{n}}\left\|\bm{x}^{ \star}\right\|_{2}\), where \(\mu=O(\mathrm{poly}(\log n))\). Let the initial point \(\bm{x}^{(0)}\in\mathbb{R}^{n}\) be sampled from the Gaussian distribution \(\mathcal{N}(\bm{0},\frac{1}{n}\beta_{0}^{2}\bm{I})\) and \(\bm{x}^{(t)}\) be updated with (1). Suppose that a small step size with \(\eta\lambda^{\star}<0.1\) is used and the sample complexity satisfies \(n^{2}p\gtrsim\mu^{5}n\log^{22}n\). Then, there exists \(T^{\star}=(1+o(1))\frac{1}{\eta\lambda^{\star}}\log\frac{\sqrt{\lambda^{\star}n}} {\beta_{0}}\) such that_\[\left\|\bm{x}^{(t)}\pm\bm{x}^{\star}\right\|_{2} \lesssim\frac{1}{\sqrt{\log n}}\|\bm{x}^{\star}\|_{2},\] (4) \[\left\|\bm{x}^{(t)}\pm\bm{x}^{\star}\right\|_{\infty} \lesssim\frac{1}{\sqrt{\log n}}\|\bm{x}^{\star}\|_{\infty},\] (5) \[\max_{1\leq l\leq n}\left\|(\bm{x}^{(t,l)}-\bm{x}^{\star})_{l}\right\| \lesssim\frac{1}{\sqrt{\log n}}\|\bm{x}^{\star}\|_{\infty}\] (7)

_hold at \(t=T^{\star}\) with probability at least \(1-o(1/\sqrt{\log n})\), if a sufficiently small initialization with_

\[\sqrt{\lambda^{\star}}n^{-10}\lesssim\beta_{0}\lesssim\sqrt{\lambda^{\star}} \sqrt[4]{\frac{np}{\mu^{5}\log^{26}n}}\frac{1}{\sqrt[4]{n}}\] (8)

_is used and the noise satisfies \(\sigma\lesssim\frac{\lambda^{\star}\mu}{n}\sqrt{\log n}\)._

Theorem 3.1 proves that, starting from a small random initialization, the trajectory of GD eventually enters the local region of the global minimizers \(\pm\bm{x}^{\star}\) in terms of both \(\ell_{2}\) and \(\ell_{\infty}\) norms. Combined with the result of [14], GD starts to converge linearly to either \(\bm{x}^{\star}\) or \(-\bm{x}^{\star}\) after \(t=T^{\star}\), as stated in the corollary below.

**Corollary 3.2**.: _Suppose that the conditions in Theorem 3.1 are satisfied, and let \(\rho\) be a constant such that \(1-\frac{\eta}{10}\leq\rho<1\). Then, with probability at least \(1-o(1/\sqrt{\log n})\), we have_

\[\left\|\bm{x}^{(t)}\pm\bm{x}^{\star}\right\|_{2} \lesssim\left(\frac{1}{\sqrt{\log n}}\rho^{t-T^{\star}}+\frac{ \sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\right)\left\|\bm{x}^{\star}\right\| _{2},\] (9) \[\left\|\bm{x}^{(t)}\pm\bm{x}^{\star}\right\|_{\infty} \lesssim\left(\frac{1}{\sqrt{\log n}}\rho^{t-T^{\star}}+\frac{ \sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\right)\left\|\bm{x}^{\star}\right\| _{\infty},\] (10)

_for all \(T^{\star}\leq t\leq T=O(n^{5})\)._

The desired global convergence result is provided by Corollary 3.2. Several remarks about Theorem 3.1 and Corollary 3.2 are in order.

Matrix RecoverySuppose \(\bm{x}^{(t)}\) converges to a global minimum \(\bm{y}^{\star}\) of the function \(f\), which is different from \(\pm\bm{x}^{\star}\). In such a case, despite achieving global convergence, the reconstructed matrix \(\bm{y}^{\star}\bm{y}^{\star\top}\) deviates from the ground truth matrix \(\bm{M}^{\star}\). However, Theorem 3.1 establishes that \(\bm{x}^{(t)}\) converges exclusively to the correct global minima \(\pm\bm{x}^{\star}\), so that the matrix \(\bm{M}^{\star}\) is recovered with high probability.

Leave-one-out SequenceTo apply the local convergence result of [14], in addition to (4) and (5), the existence of leave-one-out sequences \(\{\bm{x}^{(t,l)}\}_{l\in[n]}\) satisfying (6) and (7) is required. Leave-one-out sequences also play a critical role and appear naturally in the proof of Theorem 3.1.

Sample ComplexityThe required sample complexity for Theorem 3.1 to hold is optimal up to a logarithmic factor compared to the statistical lower bound of \(\Omega(n\log n)\). We have not done our best to optimize the \(\log\) factors, and about half of them can be reduced with more delicate analysis. We will discuss this briefly in Section 6.

Convergence TimeConsidering that \(\beta_{0}^{-1}\) is at most polynomial in \(n\) (due to the lower bound of (8)), only \(O(\log n)\) iterations are required for GD to enter the local region. It takes \(O(\log(\frac{1}{\epsilon}))\) more iterations to achieve \(\epsilon\)-accuracy in the local region, so the total iteration complexity is given by \(O(\log n)+O(\log(\frac{1}{\epsilon}))\).

Initialization SizeAlthough small initialization provides a good geometry to GD, a larger initialization is preferred because the convergence time, \(T^{\star}\), is inversely proportional to \(\beta_{0}\). When the sample complexity is optimal, i.e., \(n^{2}p\asymp n\operatorname{poly}(\log n)\), an upper bound on the initialization size given by Theorem 3.1 is \(n^{-\frac{1}{4}}\), ignoring the log factors. However, as more samples are provided, we are allowed to use a larger initialization to reduce the convergence time. When the sample complexity satisfies \(n^{2}p\asymp n^{1+a}\), the bound is \(n^{-\frac{1}{4}(1-a)}\) ignoring the log factors. The bound becomes nearly constant as \(a\) approaches \(1\), namely the fully observed case, and this is consistent with the previous result that small initialization is unnecessary for the fully observed case [18]. We also note that the lower bound of (8) is necessary in the proof of Theorem 3.1, since we derive probabilistic bounds for all iterations, and the lower bound limits the maximum number of iterations. However, we can further reduce the lower bound \(n^{-10}\) to \(n^{-c}\) for any constant \(c>10\) by tuning some constant factors during the proof.

**Noise Size** From the incoherence assumption, the maximum absolute value of entries of \(\bm{M}^{\star}\) is bounded by \(\frac{\lambda^{\star}\mu}{n}\). The condition \(\sigma\lesssim\frac{\lambda^{\star}\mu}{n}\sqrt{\log n}\) in Theorem 3.1 allows the standard deviation of the Gaussian noise to be much larger than the maximum entry. It also implies \(\frac{\sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\lesssim\mu\sqrt{\frac{\log n}{ np}}\), so that the upper bounds in Corollary 3.2 are dominated by the first terms at \(t=T^{\star}\) and they eventually converge to the second terms as \(t\) increases.

**Estimation Error** The current estimation bounds (4) to (7) are all proportional to \(\frac{1}{\sqrt{\log n}}\) times the norms of \(\bm{x}^{\star}\). However, if we do not allow the initialization size to grow with the sample complexity, we are able to obtain tighter bounds; if we use the fixed initialization size \(n^{-\frac{1}{4}}\) regardless of the sample complexity, in Theorem 3.1, the factor \(\frac{1}{\sqrt{\log n}}\) is improved to \(\frac{1}{\sqrt{np}}+\frac{\sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\), and the upper bound on noise size is also improved to \(\sigma\lesssim\frac{\lambda^{\star}\mu}{n}\sqrt{np}\) (not being precise on the factors of \(\mu\) and \(\log n\) here). Then, the estimation error in Corollary 3.2 is improved to \(\frac{1}{\sqrt{np}}\rho^{t}+\frac{\sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\) to match the result of [14] which uses spectral initialization. Thus, we have a tradeoff between estimation error and initialization size.

The next main result concerns the trajectory of GD before it enters the local region. The theorem states that for all \(t\leq T^{\star}\), \(\bm{x}^{(t)}\) stays close to the fully observed case \(\widetilde{\bm{x}}^{(t)}\) in both \(\ell_{2}\) and \(\ell_{\infty}\)-norm.

**Theorem 3.3**.: _Suppose that the conditions of Theorem 3.1 hold, and \(T^{\star}\) is defined as in Theorem 3.1. Then, for all \(t\leq T^{\star}\), we have_

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\frac{1}{ \sqrt{\log n}}\norm{\widetilde{\bm{x}}^{(t)}}_{2},\] (11)

_with probability at least \(1-o(1/\sqrt{\log n})\)._

**Trajectory of GD** The sequence \(\widetilde{\bm{x}}^{(t)}\) is a linear combination of \(\bm{x}^{(0)}\) and \(\bm{u}^{\star}\) (see (C.1) in the appendix), and it is easy to analyze how \(\widetilde{\bm{x}}^{(t)}\) evolves. By showing that \(\bm{x}^{(t)}\) stays close to \(\widetilde{\bm{x}}^{(t)}\) for all iterations, we not only show the convergence of GD with small initialization as in Theorem 3.1, but also characterize the exact trajectory that GD follows by Theorem 3.3.

**Implicit Regularization** One can prove that \(\widetilde{\bm{x}}^{(t)}\) is incoherent up to some log factors over all iterations, and from (11) and (12), the incoherence of \(\bm{x}^{(t)}\) is bounded by that of \(\widetilde{\bm{x}}^{(t)}\). Thus, Theorem 3.3 shows that the incoherence of \(\bm{x}^{(t)}\) is _implicitly_ controlled by GD without any regularizer. This is an improvement over the previous result on the global convergence of GD for matrix completion [28], where an explicit regularizer was used to control the \(\ell_{\infty}\)-norm of \(\bm{x}^{(t)}\), although no small initialization was used in that work.

## 4 Fully Observed Case and Proof Sketch

Before we explain the proof of Theorems 3.1 and 3.3, we describe the trajectory of the fully observed case. We characterize \(\widetilde{\bm{x}}^{(t)}\) with three variables: \(\widetilde{\alpha}_{t}=\big{|}\bm{u}^{\star\top}\widetilde{\bm{x}}^{(t)} \big{|}\), \(\widetilde{\beta}_{t}=\|\widetilde{\bm{x}}^{(t)}\|_{2}\), and \(\widetilde{\gamma}_{t}=\|\widetilde{\bm{x}}^{(t)}_{\perp}\|_{2}\), where \(\widetilde{\bm{x}}^{(t)}_{\perp}=\widetilde{\bm{x}}^{(t)}-\bm{u}^{\star}\bm{u}^ {\star\top}\widetilde{\bm{x}}^{(t)}\). According to (2), the three variables are updated with

\[\widetilde{\alpha}_{t+1}=(1-\eta\widetilde{\beta}_{t}^{2}+\eta \lambda^{\star})\widetilde{\alpha}_{t};\quad\widetilde{\gamma}_{t+1}=(1-\eta \widetilde{\beta}_{t}^{2})\widetilde{\gamma}_{t};\] \[\widetilde{\beta}_{t}^{2}=\widetilde{\alpha}_{t}^{2}+\widetilde{ \gamma}_{t}^{2}.\]

At \(t=0\), due to random initialization, the initial vector is nearly orthogonal to \(\bm{u}^{\star}\), and we have \(\widetilde{\alpha}_{0}\approx\frac{1}{\sqrt{n}}\beta_{0}\) and \(\widetilde{\gamma}_{0}\approx\widetilde{\beta}_{0}=\beta_{0}\). Also, due to the small initialization, the term \(\eta\widetilde{\beta}_{t}^{2}\) is ignorable until \(\widetilde{\beta}_{t}\) becomes sufficiently large, so \(\widetilde{\alpha}_{t}\) grows exponentially at the rate of \(1+\eta\lambda^{\star}\), while \(\widetilde{\gamma}_{t}\) remains still. Thus, in the early iterations where \((1+\eta\lambda^{\star})^{t}\) is still much less than \(\sqrt{n}\), \(\widetilde{\beta}_{t}\) is kept close to its initialvalue \(\beta_{0}\) while the trajectory becomes more parallel to \(\bm{u}^{\star}\) as \(\widetilde{\alpha}_{t}\) increases. When \((1+\eta\lambda^{\star})^{t}\) becomes much larger than \(\sqrt{n}\), the trajectory becomes almost parallel to \(\bm{u}^{\star}\) in that \(\widetilde{\beta}_{t}\approx\widetilde{\alpha}_{t}\gg\widetilde{\gamma}_{t}\). Until \(\widetilde{\beta}_{t}\) (asymptotically) reaches \(\frac{\sqrt{\lambda^{\star}}}{\sqrt{\log n}}\), we can consider \(\widetilde{\alpha}_{t}\) as increasing at a rate of \((1+\eta\lambda^{\star})\), and it takes about \(\frac{1}{\log(1+\eta\lambda^{\star})}\log\frac{\sqrt{\lambda^{\star}n}}{\beta_ {0}}\) steps to reach this point. After that, we can no longer ignore the term \(\eta\widetilde{\beta}_{t}^{2}\), and \(\widetilde{\alpha}_{t}\) increases at a slower rate as \(\widetilde{\beta}_{t}\) increases. We can show that \(\widetilde{\beta}_{t}^{2}\) becomes sufficiently close to \(\lambda^{\star}\) within \(O(\log\log n)\) additional iterations, as stated in the following lemma.

**Lemma 4.1**.: _Let \(T_{2}^{\prime}\) be the largest \(t\) such that \(\widetilde{\beta}_{t}^{2}\leq\frac{\lambda^{\star}}{64\log n}\). At \(t=T_{2}^{\prime}+\frac{6\log\log n}{\log(1+\eta\lambda^{\star})}\), we have \(\widetilde{\beta}_{t}^{2}\geq\lambda^{\star}\left(1-\frac{1}{\log n}\right)\)._

Finally, local convergence to \(\bm{u}^{\star}\) occurs in that \(\widetilde{\alpha}_{t}\) approaches \(\lambda^{\star}\) and \(\widetilde{\gamma}_{t}\) decreases exponentially with the rate \((1-\eta\lambda^{\star})\). The actual behavior of quantities \(\widetilde{\alpha}_{t}\), \(\widetilde{\beta}_{t}\), \(\widetilde{\gamma}_{t}\) are plotted in Figure 1.

We define the iterates before \((1+\eta\lambda^{\star})^{t}\) reaches \(\frac{1}{\sqrt{np}}\sqrt{n}\), within some logarithmic factors, as Phase I, and the next iterates before \(\widetilde{\beta}_{t}^{2}\) reaches \(\lambda^{\star}\left(1-\frac{1}{\log n}\right)\) as Phase II. Different techniques are used for each phase to prove that \(\bm{x}^{(t)}\) stays close to \(\widetilde{\bm{x}}^{(t)}\). At the end of Phase I, \(\widetilde{\alpha}_{t}\) is increased to \(\frac{1}{\sqrt{np}}\beta_{0}\) from its initial scale \(\frac{1}{\sqrt{n}}\beta_{0}\), but it is still not dominant over \(\beta_{0}\). Therefore, the magnitudes of both \(\bm{x}^{(t)}\) and \(\widetilde{\bm{x}}^{(t)}\) are kept close to \(\beta_{0}\) throughout Phase I, and we take advantage of the small random initialization to show that the deviation of \(\bm{x}^{(t)}\) from \(\widetilde{\bm{x}}^{(t)}\) does not increase much, and is kept at \(\sqrt{\frac{1}{np}}\) times the norms of \(\bm{x}^{(t)}\). In Phase II, we show that \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) expands at a rate of at most \((1+\eta\lambda^{\star})\). Since the norms of \(\bm{x}^{(t)}\) also grows at a rate of \((1+\eta\lambda^{\star})\) during most of Phase II, the norms of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) remain negligible compared to those of \(\bm{x}^{(t)}\). The next two sections give the main lemmas of Phase I and II, respectively, which are used to prove Theorems 3.1 and 3.3. For a visual representation of the results in the following two sections, please refer to Figure 2.

## 5 Phase I: Finding Direction

We provide detailed results and proof ideas for Phase I. Our main goal is to analyze the deviation of \(\bm{x}^{(t)}\) from \(\widetilde{\bm{x}}^{(t)}\). First, if we look at the update equations (1) and (2), the second term is proportional to the third power of \(\left\|\bm{x}^{(t)}\right\|_{2}\), while the other terms depend linearly on \(\left\|\bm{x}^{(t)}\right\|_{2}\). Thus, the second term is almost negligible due to the small initialization. Without the second terms, the difference between \(\bm{x}^{(t)}\) and \(\widetilde{\bm{x}}^{(t)}\) at \(t=1\) is \(\eta(\bm{M}^{\circ}-\bm{M}^{\star})\bm{x}^{(0)}\). From concentration inequalities, one can see that the \(\ell_{2}\) and \(\ell_{\infty}\) norms of \(\eta(\bm{M}^{\circ}-\bm{M}^{\star})\bm{x}^{(0)}\) are about \(\frac{1}{\sqrt{np}}\) times smaller than those of \(\widetilde{\bm{x}}^{(1)}\).

Due to the third terms of (1) and (2), the norms of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) can grow exponentially at a rate of \((1+\eta\lambda^{\star})\) in the worst case where \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) is parallel to \(\bm{u}^{\star}\). In such a case, the norms of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) would be larger than those of \(\widetilde{\bm{x}}^{(t)}\) at the end of Phase I, since those of \(\widetilde{\bm{x}}^{(t)}\) remain still in Phase I. However, we overcome this problem by proving that the bounds grow at most _polynomially_ with respect to \(t\), and since \(t\) is at most \(O(\log n)\), the bounds remain \(\frac{1}{\sqrt{np}}\) times smaller than the norms of \(\widetilde{\bm{x}}^{(t)}\) up to logarithmic factors throughout Phase I.

**Lemma 5.1**.: _Let \(T_{1}\) be the largest \(t\) such that \((1+\eta\lambda^{\star})^{t}\leq\sqrt{\frac{\mu^{4}\log^{21}n}{np}}\sqrt{n}\). Under the conditions of Theorem 3.1, with probability at least \(1-o(1/\sqrt{\log n})\), for all \(t\leq T_{1}\), we have_

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\mu\sqrt{ \frac{\log n}{np}}\beta_{0}t,\quad\quad\eqref{eq:T1}\quad\left\|\bm{x}^{(t)}- \widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\sqrt{\frac{\mu^{3}\log^{2}n}{ np}}\frac{\beta_{0}}{\sqrt{n}}t^{2}.\] (14)

\(T_{1}\) is defined to be the end of Phase I. Lemma 5.1 proves Theorem 3.3 for Phase I.

**Proof of (13)**  We will first demonstrate how to obtain the \(\ell_{2}\)-norm bound of Lemma 5.1. Let us define a sequence \(\widehat{\bm{x}}^{(t)}\) that is updated as

\[\widehat{\bm{x}}^{(t+1)}=\widehat{\bm{x}}^{(t)}-\eta\left\|\widehat{\bm{x}}^{( t)}\right\|_{2}^{2}\widehat{\bm{x}}^{(t)}+\eta\bm{M}^{\circ}\widehat{\bm{x}}^{(t)}; \quad\widehat{\bm{x}}^{(0)}=\bm{x}^{(0)}.\] (15)Note that the norm of \(\widetilde{\bm{x}}^{(t)}\) is used in the second term of (15). The update equation of \(\widetilde{\bm{x}}^{(t)}\) differs from \(\widetilde{\bm{x}}^{(t)}\) in the third term and from \(\bm{x}^{(t)}\) in the second term. We use \(\widehat{\bm{x}}^{(t)}\) as a proxy for bounding \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\). We first show that \(\left\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) grows at most linearly with respect to \(t\).

**Lemma 5.2**.: _With probability at least \(1-o(1/\sqrt{\log n})\), for all \(t\leq T_{1}\), we have_

\[\left\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim \mu\sqrt{\frac{\log n}{np}}\beta_{0}t.\] (16)

The proof of this lemma is based on the fact that \(\widehat{\bm{x}}^{(t)}\) is a product of \(\bm{x}^{(0)}\) and a matrix polynomial of \(\bm{I}\) and \(\bm{M}^{\circ}\), while \(\widetilde{\bm{x}}^{(t)}\) is a product between \(\bm{x}^{(0)}\) and a matrix polynomial of \(\bm{I}\) and \(\bm{M}^{\star}\). We prove the lemma by comparing the two matrix polynomials. We remark that Lemma 5.2 holds regardless of the small initialization, but it relies on the randomness of \(\bm{x}^{(0)}\).

Since \(\bm{x}^{(t)}\) and \(\widehat{\bm{x}}^{(t)}\) differ only in the second term, their initial difference is proportional to \(\beta_{0}^{3}\). More precisely, it is \(\frac{1}{\sqrt{np}}\beta_{0}^{3}\). We show that the difference grows exponentially at a rate of \((1+\eta\lambda^{\star})\).

**Lemma 5.3**.: _If (14) holds for all \(t\leq T_{1}\), we have_

\[\left\|\bm{x}^{(t)}-\widehat{\bm{x}}^{(t)}\right\|_{2}\lesssim\frac{1}{ \lambda^{\star}}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{\star})^{t} \beta_{0}^{3}\] (17)

_for all \(t\leq T_{1}\) with probability at least \(1-o(1/\sqrt{\log n})\)._

The upper bound in (17) becomes smaller than that of (16) if \((1+\eta\lambda^{\star})^{t}\beta_{0}^{2}\leq\lambda^{\star}\sqrt{\frac{1}{\mu \log^{2}n}}\). One can check that this condition is satisfied from the definition of \(T_{1}\) given in Lemma 5.1 and the bound on the initialization size (8). Thus, (13) is proved by (16) and (17).

Proof of (14) We control the \(l\)th component of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) using the \(l\)th leave-one-out sequence. Leave-one-out sequences have two important properties. First, because they are defined without only one row/column, they are extremely close to \(\bm{x}^{(t)}\), and at \(t=1\), \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\) is about \(\frac{1}{\sqrt{np}}\frac{\beta_{0}}{\sqrt{n}}\). Second, the \(l\)th component of the \(l\)th leave-one-out sequence evolves similarly to that of \(\widetilde{\bm{x}}^{(t)}\) and is easy to analyze. With these two properties, we bound the \(l\)th component of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\) as

\[\left|\left(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right)_{l}\right|\leq\left\| \bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}+\left|\left(\bm{x}^{(t,l)}-\widetilde {\bm{x}}^{(t)}\right)_{l}\right|\] (18)

Figure 2: An illustrative description of trajectory of various quantities compared to the norms of \(\bm{x}^{(t)}\) on logarithmic scales. Arrows between lines represent the ratio between them. The quantities depicted are not precise, and only the key factors are shown for simplicity. (a) In Phase I, \(\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\|_{2}\) increases linearly and \(\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\|_{2}\) increases exponentially with the rate of \((1+\eta\lambda^{\star})\). They have the same scale at the end of Phase I. (b) In Phase I, even if the \(\bm{u}^{\star}\) component of \(\bm{x}^{(t)}-\bm{x}^{(t,l)}\) grows exponentially, it remains almost orthogonal to \(\bm{u}^{\star}\) throughout the phase. (c) Phase II is divided into three parts according to the growth speed of \(\bm{x}^{(t)}\) and \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\). The ratio between them at the start and the end of each part is described, and it is at most \(\frac{1}{\sqrt{\log n}}\) in Phase II.

We claim that both \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\) and \(\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l}\right|\) increase at most polynomially with respect to \(t\) from the initial scale \(\frac{\beta_{0}}{\sqrt{n}}\frac{\beta_{0}}{\sqrt{n}}\).

**Lemma 5.4**.: _With probability at least \(1-o(1/\sqrt{\log n})\), for all \(t\leq T_{1}\), we have_

\[\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\lesssim\mu\sqrt{ \frac{\log^{2}n}{np}\frac{\beta_{0}}{\sqrt{n}}}t,\] (19)

As explained for \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\), due to the third terms of (1) and (3), \(\bm{x}^{(t)}-\bm{x}^{(t,l)}\) can also grow exponentially at the rate of \((1+\eta\lambda^{\star})\) in the worst case where \(\bm{x}^{(t)}-\bm{x}^{(t,l)}\) is parallel to \(\bm{u}^{\star}\). This contradicts our result (19) that \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\) grows only linearly. We show that \(\bm{x}^{(t)}-\bm{x}^{(t,l)}\) remains nearly orthogonal to \(\bm{u}^{\star}\) in Phase I, and thus the worst case does not occur.

**Lemma 5.5**.: _For all \(l\in[n]\) and \(t\leq T_{1}\), we have_

\[\left|\bm{u}^{(t)\top}(\bm{x}^{(t)}-\bm{x}^{(t,l)})\right|\lesssim \sqrt{\frac{\mu^{3}\log^{2}n}{np}}(1+\eta\lambda^{\star})^{t}\frac{\beta_{0}} {n}\]

_with probability at least \(1-o(1/\sqrt{\log n})\), where \(\bm{u}^{(l)}\) is the first eigenvector of \(\bm{M}^{(l)}\)._

Note that \(\bm{u}^{(l)}\) is almost parallel to \(\bm{u}^{\star}\) (see Lemma A.5 in the appendix). The \(\bm{u}^{(l)}\) component of \(\bm{x}^{(t)}-\bm{x}^{(t,l)}\) is initialized to the order of \(\frac{1}{\sqrt{np}}\frac{\beta_{0}}{n}\), which is \(\frac{1}{\sqrt{n}}\) times smaller than \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\). Although it is increased exponentially, from the definition of \(T_{1}\), the \(\bm{u}^{(l)}\) component remains much smaller than \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\) in Phase I.

One can see that \(\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l}\right|\) increases by \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\left\|\bm{u}^{\star }\right\|_{\infty}\) at each step, and summing the bound (13) up to \(t\) gives (20). Finally, (14) is obtained by putting (19) and (20) into (18).

## 6 Phase II: Expansion

In the next phase, we show that the bounds obtained in Phase I are increased at a rate of \((1+\eta\lambda^{\star})\).

**Lemma 6.1**.: _Let \(T_{2}\) be the largest \(t\) such that \(\widetilde{\beta}_{t}^{2}\leq\lambda^{\star}\left(1-\frac{1}{\log n}\right)\). Then, for all \(T_{1}<t\leq T_{2}\), we have_

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \lesssim\mu\sqrt{\frac{\log^{3}n}{np}}\beta_{0}(1+\eta\lambda^{ \star})^{t-T_{1}},\] (21) \[\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2} \lesssim\mu\sqrt{\frac{\log^{5}n}{np}}\frac{\beta_{0}}{\sqrt{n}}( 1+\eta\lambda^{\star})^{t-T_{1}},\] (22) \[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty} \lesssim\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{\sqrt{n}}( 1+\eta\lambda^{\star})^{t-T_{1}},\] (23) \[\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l} \right|\lesssim\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{\sqrt{n}}( 1+\eta\lambda^{\star})^{t-T_{1}},\] (24)

_with probability at least \(1-o(1/\sqrt{\log n})\)._

\(T_{2}\) is defined as the end of Phase II. We will explain how Lemma 6.1 leads to Theorem 3.3 in Phase II. Let us first focus on (21) and (11). We can divide Phase II into three parts according to the behavior of \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\). First, \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) is kept close to \(\beta_{0}\) until \((1+\eta\lambda^{\star})^{t}\) becomes \(\sqrt{n}\), or \((1+\eta\lambda^{\star})^{t-T_{1}}\) becomes \(\sqrt{np}\). In this part, although the bounds increase exponentially with the rate of \((1+\eta\lambda^{\star})\), the factor \(\frac{1}{\sqrt{np}}\), which was already present in (13) of Phase I, compensates for this increase. At the end of the first part, \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) is smaller than \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) by some log factors. Next, \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) grows at the rate of \((1+\eta\lambda^{\star})\) until it reaches \(\frac{\sqrt{\lambda^{\star}}}{8\sqrt{\log n}}\). Since both \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) and \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) increase with \((1+\eta\lambda^{\star})\)the ratio between them is maintained in the second part. Finally, in the remaining iterations, \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) increases with \((1-\eta\widetilde{\beta}_{t}^{2}+\eta\lambda^{\star})\) at each step, and the increment becomes smaller as it converges to \(\sqrt{\lambda^{\star}}\). Thus, as in the first part, \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) increases faster than \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\). However, from Lemma 4.1, the length of this part is \(O(\log\log n)\), and the ratio between \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) and \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) increases only by \(\log^{6}n\). We prove that the log factors already present at the end of the second part compensate this, and finally (11) holds for all \(t\) in Phase II. A more delicate analysis may prove that \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) grows at the same rate as \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\) in the third part, and this will reduce the required sample complexity by at most \(\log^{12}n\). A similar argument can be used to prove that the bounds for \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\), \(\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2}\), and \(\left|(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)})_{l}\right|\) are smaller than \(\left\|\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\) by some log factors throughout Phase II.

At the end of Phase II, \(\widetilde{\bm{x}}^{(t)}\) is very close to \(\pm\bm{x}^{\star}\) in both \(\ell_{2}\) and \(\ell_{\infty}\) norms (see Corollary C.3 in the appendix), so one can replace \(\widetilde{\bm{x}}^{(t)}\) of Lemma 6.1 with \(\pm\bm{x}^{\star}\) to prove (4) to (7) of Theorem 3.1. Hence, we can let \(T^{\star}=T_{2}\), and as explained in Section 4, \(T_{2}\) is approximately given by \(\frac{1}{\log(1+\eta\lambda^{\star})}\log\frac{\sqrt{\lambda^{\star}n}}{\beta _{0}}+O(\log\log n)\).

## 7 Simulation

In this section, we present some simulation results that support our theoretical findings.

**Trajectory of GD** With the dimension \(n=5000\), we constructed the ground truth vector \(\bm{u}^{\star}\) by sampling it from the Gaussian distribution \(\mathcal{N}(\bm{0},\frac{1}{n}\bm{I})\) and normalizing it to have unit norm. We let \(\lambda^{\star}=1\) so that the matrix \(\bm{M}^{\star}\) is given by \(\bm{u}^{\star}\bm{u}^{\star\top}\), and we randomly sampled the matrix symmetrically with a sampling rate of \(p=0.1\) and Gaussian noise of \(\sigma=\frac{0.1}{n}\). The initialization size was set to \(\beta_{0}=\frac{1}{n}\) and a step size of \(0.1\) was used for GD. Figure 3 (a) and (b) represent one trial of the experiment, but similar graphs were obtained in each repetition of the experiment. The evolution of some important quantities such as \(\left\|\bm{x}^{(t)}\right\|_{2}\) and \(\left|\bm{u}^{\star\top}\bm{x}^{(t)}\right|\) is shown in Figure 3(a). As in the fully observed case, the signal component \(\left|\bm{u}^{\star\top}\bm{x}^{(t)}\right|\) increases at the the rate of \((1+\eta\lambda^{\star})\) until it approaches \(\sqrt{\lambda^{\star}}\), and a local convergence to \(\bm{x}^{\star}\) occurs, where \(\left\|\bm{x}^{(t)}-\bm{x}^{\star}\right\|_{2}\) decreases exponentially and saturates at the level determined by the noise size \(\sigma\). In Figure 3(b), we describe the deviation of \(\bm{x}^{(t)}\) from \(\widetilde{\bm{x}}^{(t)}\) in both \(\ell_{2}\) and \(\ell_{\infty}\) norms. The solid lines represent the norms of \(\bm{x}^{(t)}\) and the dotted lines represent those of \(\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\). We can see that there is a gap between the solid and the dotted lines during the whole iterations. Thus, \(\bm{x}^{(t)}\) stays close to the trajectory of the fully observed case, as we proved in Theorem 3.3.

**Small Initialization** In the next experiment, we investigated the importance of a small initialization for the convergence of GD. We used the same conditions as in the previous experiment except \(n=500\). We measured \(\left\|\bm{x}^{(t)}\pm\bm{x}^{\star}\right\|_{2}\) at \(t=\frac{1}{\log(1+\eta\lambda^{\star})}\log\frac{\sqrt{\lambda^{\star}n}}{\beta_{ 0}}+100\) and averaged it over \(1000\) trials. We repeated the experiment while changing the initialization size from \(10^{0}\) to \(10^{-9}\) and the sampling probability from \(0.01\) to \(0.04\). The result is summarized in Figure 3(c). For all sampling probabilities, the small initialization improves the convergence of GD. Also, the performance starts to saturate at much larger initialization sizes as the sampling probability increases, and this is consistent with our finding (8) that a larger initialization is possible as more samples are available.

## 8 Discussion

In this paper, we showed that for rank-1 symmetric matrix completion with \(\ell_{2}\) loss, GD can converge to the ground truth starting from a small random initialization. Ignoring log factors, the bound on the initialization size is \(n^{-\frac{1}{4}}\) when the optimal \(n\operatorname{poly}(\log n)\) samples are provided, and the bound becomes larger as more samples are provided. The result is interesting because the loss function does not have global benign geometry if no regularizer is applied. Our result does not use any explicit regularizer and relies only on the implicit regularizing effect of GD.

The most important future work is an extension to the rank-\(r\) case. Suppose that \(\bm{M}^{\star}\) is a rank-\(r\) matrix and its eigendecomposition is given by \(\bm{U}^{\star}\bm{\Sigma}^{\star}\bm{U}^{\star\top}=\bm{X}^{\star}\bm{X}^{\star}\), where \(\bm{\Sigma}^{\star}=\operatorname{diag}(\lambda_{1}^{\star},\cdots,\lambda_{ r}^{\star})\) and \(\bm{X}^{\star}=\bm{U}^{\star}\bm{\Sigma}^{\star\frac{1}{2}}\). Then, the trajectory of GD becomes an \(n\times r\) matrix \(\bm{X}^{(t)}\), which is updated as

\[\bm{X}^{(t+1)}=\bm{X}^{(t)}-\frac{\eta}{p}\mathcal{P}_{\Omega}\Big{(}\bm{X}^{( t)}\bm{X}^{(t)\top}\Big{)}\bm{X}^{(t)}+\eta\bm{M}^{\circ}\bm{X}^{(t)}.\]

Each entry of \(\bm{X}^{(0)}\) is sampled independently from the Gaussian distribution \(\mathcal{N}\left(0,\frac{1}{n}\beta_{0}^{2}\right)\) as in the rank-1 case.

An instance of \(\bm{X}^{(t)}\) is shown in Figure 4. The same conditions as in Figure 3 are used, except that the ground truth matrix is a rank-3 matrix with non-zero eigenvalues \(1,0.75,0.5\). The singular values of \(\bm{X}^{(t)}\) behave similarly to \(\left\|\bm{x}^{(t)}\right\|_{2}\) in the rank-1 case. In the early iterations, where the orthogonal components dominate, the singular values stay close to their initial scale \(\beta_{0}\). After that, each singular value \(\sigma_{i}(\bm{X}^{(t)})\) increases at a rate of \((1+\eta\lambda_{i}^{\star})\) and saturates at \(\sqrt{\lambda_{i}^{\star}}\). We use \(\left\|\bm{X}-\bm{Y}\right\|_{\mathrm{R}}\) to denote the Frobenius norm between \(\bm{X}\) and \(\bm{Y}\) under best rotational alignment. \(\left\|\bm{X}^{(t)}-\bm{X}^{\star}\right\|_{\mathrm{R}}\) decreases exponentially and saturates at the level determined by the noise size \(\sigma\), after all singular values have saturated, as local convergence begins.

To extend the results of the rank-1 case, we need to show that \(\left\|\bm{X}^{(t)}-\widetilde{\bm{X}}^{(t)}\right\|_{\mathrm{R}}\) remains much smaller than \(\left\|\bm{X}^{(t)}\right\|_{\mathrm{F}}\) throughout the iterations, where \(\widetilde{\bm{X}}^{(t)}\) is the trajectory of the fully observed case. Before \(\sigma_{1}(\bm{X}^{(t)})\) saturates around \(\sqrt{\lambda_{1}^{\star}}\), it behaves similarly to \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\) of the rank-1 case, i.e., it expands at a rate of \((1+\eta\lambda_{1}^{\star})\) along with \(\left\|\bm{X}^{(t)}\right\|_{\mathrm{F}}\) after the early iterations. However, because each singular value grows at a different rate, a different phenomenon is observed for the rank-\(r\) case. During the iterations before \(\sigma_{i+1}(\bm{X}^{(t)})\) saturates after \(\sigma_{i}(\bm{X}^{(t)})\) does, both \(\left\|\bm{X}^{(t)}-\widetilde{\bm{X}}^{(t)}\right\|_{\mathrm{R}}\) and \(\left\|\bm{X}^{(t)}\right\|_{\mathrm{F}}\) do not increase much. Our current theory can only show that \(\left\|\bm{X}^{(t)}-\widetilde{\bm{X}}^{(t)}\right\|_{\mathrm{R}}\) increases at a rate less than \((1+\eta\lambda_{i+1}^{\star})\), and in order for \(\left\|\bm{X}^{(t)}-\widetilde{\bm{X}}^{(t)}\right\|_{\mathrm{R}}\) to remain much smaller than \(\left\|\bm{X}^{(t)}\right\|_{\mathrm{F}}\), additional sample complexity is required to compensate for the exponential increases. Therefore, we expect that the convergence of GD for the case of rank-\(r\) can be proved with the techniques developed in this paper if \(n^{1+\Theta(\kappa-1)}\operatorname{poly}(\kappa,r,\log n)\) samples are provided, where \(\kappa=\frac{\lambda_{1}^{\star}}{\lambda_{r}^{\star}}\) is the condition number. Nevertheless, whether GD can converge with the optimal \(n\operatorname{poly}(\kappa,r,\log n)\) samples for the rank-\(r\) matrix completion problem remains an open problem.

Figure 4: Trajectory of GD obtained for a rank-3 matrix with non-zero eigenvalues \(1,0.75,0.5\). The same parameters were used as in Figure 3.

## Acknowledgments and Disclosure of Funding

This research was supported by the National Research Foundation of Korea under grant 2021R1C1C11008539.

## References

* [1] Y. Chen, Y. Chi, J. Fan, and C. Ma, "Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval," _Mathematical Programming_, vol. 176, no. 1-2, pp. 5-37, 2019.
* [2] A. Ahmed, B. Recht, and J. Romberg, "Blind deconvolution using convex programming," _IEEE Transactions on Information Theory_, vol. 60, no. 3, pp. 1711-1732, 2014.
* [3] S. Tu, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht, "Low-rank solutions of linear matrix equations via procrustes flow," in _International Conference on Machine Learning_, pp. 964-973, 2016.
* [4] E. J. Candes and B. Recht, "Exact matrix completion via convex optimization," _Foundations of Computational Mathematics_, vol. 9, no. 6, pp. 717-772, 2009.
* [5] D. F. Gleich and L.-h. Lim, "Rank aggregation via nuclear norm minimization," in _Proceedings of the 17th ACM international conference on Knowledge discovery and data mining_, pp. 60-68, ACM, 2011.
* [6] E. J. Candes, X. Li, Y. Ma, and J. Wright, "Robust principal component analysis?," _Journal of the ACM_, vol. 58, no. 3, pp. 11:1-11:37, 2011.
* [7] Y. Hu, X. Liu, and M. Jacob, "A generalized structured low-rank matrix completion algorithm for mr image recovery," _IEEE Transactions on Medical Imaging_, vol. 38, no. 8, pp. 1841-1851, 2019.
* [8] Y. Chi, Y. M. Lu, and Y. Chen, "Nonconvex optimization meets low-rank matrix factorization: An overview," _IEEE Transactions on Signal Processing_, vol. 67, no. 20, pp. 5239-5269, 2019.
* [9] E. J. Candes and T. Tao, "The power of convex relaxation: Near-optimal matrix completion," _IEEE Transactions on Information Theory_, vol. 56, no. 5, pp. 2053-2080, 2010.
* [10] R. H. Keshavan, A. Montanari, and S. Oh, "Matrix completion from a few entries," _IEEE Transactions on Information Theory_, vol. 56, no. 6, pp. 2980-2998, 2010.
* [11] P. Jain, P. Netrapalli, and S. Sanghavi, "Low-rank matrix completion using alternating minimization," in _Proceedings of the forty-fifth annual ACM symposium on Theory of computing_, pp. 665-674, ACM, 2013.
* [12] Y. Chen and M. J. Wainwright, "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees," _arXiv preprint arXiv:1509.03025_, 2015.
* [13] R. Sun and Z.-Q. Luo, "Guaranteed matrix completion via non-convex factorization," _IEEE Transactions on Information Theory_, vol. 62, no. 11, pp. 6535-6579, 2016.
* [14] C. Ma, K. Wang, Y. Chi, and Y. Chen, "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and blind deconvolution," _Foundations of Computational Mathematics_, vol. 20, no. 3, pp. 451-632, 2020.
* [15] J. Chen, D. Liu, and X. Li, "Nonconvex rectangular matrix completion via gradient descent without \(\ell_{2,\infty}\) regularization," _IEEE Transactions on Information Theory_, vol. 66, no. 9, pp. 5806-5841, 2020.
* [16] S. S. Du, C. Jin, J. D. Lee, M. I. Jordan, A. Singh, and B. Poczos, "Gradient descent can take exponential time to escape saddle points," in _Advances in Neural Information Processing Systems_, 2017.
* [17] Y. Li, T. Ma, and H. Zhang, "Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations," in _Conference on Learning Theory_, pp. 2-47, 2018.
* [18] T. Ye and S. S. Du, "Global convergence of gradient descent for asymmetric low-rank matrix factorization," in _Advances in Neural Information Processing Systems_, pp. 1429-1439, 2021.

* [19] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai, "Gradient descent finds global minima of deep neural networks," in _International Conference on Machine Learning_, pp. 1675-1685, 2019.
* [20] Y. Ma, A. Olshevsky, C. Szepesvari, and V. Saligrama, "Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers," in _International Conference on Machine Learning_, pp. 3335-3344, 2018.
* [21] Q. Ma and A. Olshevsky, "Adversarial crowdsourcing through robust rank-one matrix completion," in _Advances in Neural Information Processing Systems_, pp. 21841-21852, 2020.
* [22] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro, "Implicit regularization in matrix factorization," in _Advances in Neural Information Processing Systems_, 2017.
* [23] D. Stoger and M. Soltanolkotabi, "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction," in _Advances in Neural Information Processing Systems_, pp. 23831-23843, 2021.
* [24] S. Arora, N. Cohen, W. Hu, and Y. Luo, "Implicit regularization in deep matrix factorization," in _Advances in Neural Information Processing Systems_, pp. 7413-7424, 2019.
* [25] N. Razin and N. Cohen, "Implicit regularization in deep learning may not be explainable by norms," in _Advances in Neural Information Processing Systems_, pp. 21174-21187, 2020.
* [26] Z. Li, Y. Luo, and K. L. Lyu, "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning," in _In International Conference on Learning Representations_, 2021.
* [27] S. Bhojanapalli, B. Neyshabur, and N. Srebro, "Global optimality of local search for low rank matrix recovery," in _Advances in Neural Information Processing Systems_, 2016.
* [28] R. Ge, J. D. Lee, and T. Ma, "Matrix completion has no spurious local minimum," in _Advances in Neural Information Processing Systems_, 2016.

Detailed proofs for the results explained in the main text are provided in this appendix. We say that an event happens _with high probability_ if it happens with probability at least \(1-\frac{1}{n^{C}}\) for a constant \(C>0\) and \(C\) can be made arbitrary large by controlling constant factors. A union of \(\operatorname{poly}(n)\) number of events that happens with high probability still happens with high probability. For a matrix \(\bm{A}\), we denote the spectral norm by \(\left\|\bm{A}\right\|\) and the maximum absolute value of entries by \(\left\|\bm{A}\right\|_{\infty}\). Also, the largest \(\ell_{2}\)-norm of rows of \(\bm{A}\) is denoted as \(\left\|\bm{A}\right\|_{2,\infty}\).

## Appendix A Spectral Analysis

We introduce some spectral bounds related to random sampling and Gaussian noise.

**Lemma A.1**.: _If \(n^{2}p\gtrsim n\log n\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})-\bm{M}^{\star}\right\| \lesssim\lambda^{\star}\mu\sqrt{\frac{\log n}{np}}\]

_with high probability._

**Lemma A.2**.: _If \(n^{2}p\gtrsim\mu n\log n\), for all \(l\in[n]\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})-\mathcal{P}_{\Omega}^ {(l)}(\bm{M}^{\star})\right\|\lesssim\lambda^{\star}\sqrt{\frac{\mu}{np}}\]

_with high probability._

**Lemma A.3**.: _If \(n^{2}p\gtrsim n\log^{2}n\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})\right\|\lesssim\sigma\sqrt{ \frac{n}{p}}\]

_with high probability._

Note that Lemma A.3 also implies that \(\left\|\bm{E}^{(l)}\right\|\lesssim\sigma\sqrt{\frac{n}{p}}\) for all \(l\in[n]\) with high probability. Combined with the condition \(\sigma\lesssim\frac{\lambda^{\star}\mu}{n}\sqrt{\log n}\), we have

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})\right\|\lesssim\lambda^{\star} \mu\sqrt{\frac{\log n}{np}},\quad\left\|\bm{E}^{(l)}\right\|\lesssim\lambda^{ \star}\mu\sqrt{\frac{\log n}{np}}\]

for all \(l\in[n]\). Proofs for Lemmas A.1 and A.2 are provided in Appendix G. Check Lemma 11 of [12] for the proof of Lemma A.3.

Next, we state bounds on the eigenvalues of \(\bm{M}^{\circ}\) and \(\bm{M}^{(l)}\). The first eigenvalues of \(\bm{M}^{\circ}\) and \(\bm{M}^{(l)}\) are denoted as \(\lambda^{\circ}\) and \(\lambda^{(l)}\), respectively. The following lemma is derived from Lemmas A.1 and A.2 with Weyl's Theorem.

**Lemma A.4**.: _If \(n^{2}p\gtrsim\mu n\log n\), we have_

\[\left|\lambda^{\circ}-\lambda^{\star}\right|\lesssim\lambda^{\star}\mu\sqrt{ \frac{\log n}{np}},\] (A.1)

\[\left|\lambda^{(l)}-\lambda^{\star}\right|\lesssim\lambda^{\star}\mu\sqrt{ \frac{\log n}{np}}\] (A.2)

_for all \(l\in[n]\) with high probability._

Lastly, Lemmas A.1 and A.2 with Davis-Kahan Theorem give the following lemma.

**Lemma A.5**.: _If \(n^{2}p\gtrsim\mu n\log n\), we have_

\[\left\|\bm{u}^{(l)}-\bm{u}^{\star}\right\|_{2}\lesssim\mu\sqrt{\frac{\log n}{ np}}\]

_for all \(l\in[n]\) with high probability._Initialization

In this section, we introduce some properties that the initialization vector \(\bm{x}^{(0)}\) satisfies. Recall that each entry of \(\bm{x}^{(0)}\) is sampled from \(\mathcal{N}(0,\frac{1}{n}\beta_{0}^{2})\) independently. We use \(\bm{H}\) to denote the perturbation \(\bm{M}^{\circ}-\bm{M}^{\star}\).

**Lemma B.1**.: _The initialization vector \(\bm{x}^{(0)}\) satisfies_

\[\frac{1}{2}\beta_{0}\leq\left\|\bm{x}^{(0)}\right\|_{2}\leq\frac{3}{2}\beta_{0}\] (B.1)

_with probability at least \(1-e^{-n/32}\), and_

\[\left\|\bm{x}^{(0)}\right\|_{\infty}\leq 2\sqrt{\log n}\frac{\beta_{0}}{ \sqrt{n}},\] (B.2) \[\left|\bm{u}^{\star\top}\bm{H}^{s}\bm{x}^{(0)}\right|\leq 2 \sqrt{\log n}\frac{\beta_{0}}{\sqrt{n}}\|\bm{H}\|^{s},\quad\forall s\leq 30 \log n,\] (B.3)

_with probability at least \(1-\frac{1}{n}-\frac{30\log n}{n^{2}}\). It also satisfies_

\[\frac{1}{\sqrt{\log n}}\frac{\beta_{0}}{\sqrt{n}}\leq\left|\bm{u}^{\star\top} \bm{x}^{(0)}\right|\] (B.4)

_with probability at least \(1-\frac{1}{2\sqrt{\log n}}\)._

Proof.: To bound \(\left\|\bm{x}^{(0)}\right\|_{2}\), we use the following basic concentration inequality that holds for i.i.d. standard normal variables \(\{X_{i}\}_{i\in[n]}\).

\[\mathbb{P}\Bigg{[}\Bigg{|}\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-1\Bigg{|}\geq t \Bigg{]}\leq 2e^{-nt^{2}/8}\]

If we put \(t=\frac{1}{2}\), with probability at least \(1-e^{-n/32}\), we have

\[\frac{1}{2}\beta_{0}^{2}\leq\left\|\bm{x}^{(0)}\right\|_{2}^{2}\leq\frac{3}{ 2}\beta_{0}^{2},\]

and this implies (B.1).

For a centered Gaussian random variable with standard deviation \(\sigma\), we have

\[\mathbb{P}[|X|\geq t]\leq e^{-\frac{t^{2}}{2\sigma^{2}}}.\]

Hence, an entry of \(\bm{x}^{(0)}\) is less than \(2\sqrt{\log n}\frac{\beta_{0}}{\sqrt{n}}\) with probability at least \(1-\frac{1}{n^{2}}\), and all entries of \(\bm{x}^{(0)}\) are less than \(2\sqrt{\log n}\frac{\beta_{0}}{\sqrt{n}}\) with probability at least \(1-\frac{1}{n}\). \(\bm{u}^{\star\top}\bm{H}^{s}\bm{x}^{(0)}\) follows a centered Gaussian distribution with standard deviation \(\left\|\bm{H}^{s}\bm{u}^{\star}\right\|_{2}\leq\left\|\bm{H}\right\|^{s}\left\| \bm{u}^{\star}\right\|_{2}\) for all \(s\), and (B.3) holds with probability at least \(1-\frac{10\log n}{n^{2}}\).

For a random variable \(X\) that is sampled from \(\mathcal{N}(0,\sigma^{2})\), we have

\[\mathbb{P}[|X|\leq t]\leq\frac{t}{\sqrt{2\pi\sigma^{2}}}.\]

Hence, we have

\[\frac{1}{\sqrt{\log n}}\frac{\beta_{0}}{\sqrt{n}}\leq\left|\bm{u}^{\star\top} \bm{x}^{(0)}\right|\]

with probability at least \(1-\frac{1}{2\sqrt{\log n}}\). 

Lemma B.1 implies that the \(\bm{u}^{\star}\) component of \(\bm{x}^{(0)}\) is in the range

\[\frac{1}{\sqrt{\log n}}\frac{\beta_{0}}{\sqrt{n}}\leq\left|\bm{u}^{\star\top} \bm{x}^{(0)}\right|\leq 2\sqrt{\log n}\frac{\beta_{0}}{\sqrt{n}}.\] (B.5)

**Lemma B.2**.: _We have_

\[\Big{\|}a\bm{x}^{(0)}+b\bm{u}^{\star}\Big{\|}_{\infty}\geq(|a|\beta_{0}+|b|)\frac{ 1}{\sqrt{n}}\] (B.6)

_for all \(a,b\), with probability at least \(1-\exp\Bigl{(}-\frac{n}{2\mu}\Bigr{)}\)._

Proof.: The probability that an entry of \(\bm{x}^{(0)}\) is less than \(\frac{\beta_{0}}{\sqrt{n}}\) is bounded by \(\frac{1}{\sqrt{2\pi}}\). Without loss of generality, let us assume that all entries of \(\bm{u}^{\star}\) are not negative and \(a,b\geq 0\). There are at least \(\frac{n}{\mu}\) entries of \(\bm{u}^{\star}\) that are larger than \(\frac{1}{\sqrt{n}}\). For such entries, the probability that all entries of \(\bm{x}^{(0)}\) is less than \(\frac{\beta_{0}}{\sqrt{n}}\) is bounded by \(\left(\frac{1}{\sqrt{2\pi}}\right)^{\frac{n}{\mu}}\leq\exp\Bigl{(}-\frac{n}{2 \mu}\Bigr{)}\). Hence, for at least one position, both entries of \(\bm{x}^{(0)}\) and \(\bm{u}^{\star}\) are larger than \(\frac{\beta_{0}}{\sqrt{n}}\) and \(\frac{1}{\sqrt{n}}\), respectively, with probability at least \(1-\exp\Bigl{(}-\frac{n}{2\mu}\Bigr{)}\). 

In the following sections, we assume that we are given an initialization vector \(\bm{x}^{(0)}\) that satisfies (B.1) to (B.6).

## Appendix C Fully Observed Case

We provide some lemmas related to \(\widetilde{\bm{x}}^{(t)}\) in this section. We first note that \(\widetilde{\bm{x}}^{(t)}\) is explicitly written as

\[\widetilde{\bm{x}}^{(t)}=\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2})\bm{ x}^{(0)}+\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2}+\eta\lambda^{\star})( \bm{u}^{\star\top}\bm{x}^{(0)})\bm{u}^{\star}:=A^{(t)}\bm{x}^{(0)}+B^{(t)}\bm{ u}^{\star}.\] (C.1)

Let us define \(T_{2}^{\prime}\) as the last \(t\) such that \(\widetilde{\beta}_{t}^{2}\leq\frac{\lambda^{\star}}{64\log n}\). We claim that \(T_{2}^{\prime}\leq\frac{64\log n}{\eta\lambda^{\star}}\) and prove this later. Then, for all \(t\leq T_{2}^{\prime}\), we have

\[\frac{1}{4}(1+\eta\lambda^{\star})^{t} \leq\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2}+\eta\lambda^{ \star})\leq(1+\eta\lambda^{\star})^{t}\] (C.2) \[\frac{1}{4} \leq\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2})\leq 1\]

because

\[\prod_{s=1}^{T_{2}^{\prime}}\left(\frac{1+\eta\lambda^{\star}-\eta\widetilde{ \beta}_{s}^{2}}{1+\eta\lambda^{\star}}\right)\geq\prod_{s=1}^{T_{2}^{\prime}} (1-\eta\widetilde{\beta}_{s}^{2})\geq\left(1-\frac{\eta\lambda^{\star}}{64 \log n}\right)^{\frac{64\log n}{\eta\lambda^{\star}}}\geq\frac{1}{4}\]

if \(\frac{\eta\lambda^{\star}}{64\log n}\leq\frac{1}{2}\). Note that the upper bounds in (C.2) hold even if \(t>T_{2}\).

From (C.2), we have the approximation \(\widetilde{\bm{x}}^{(t)}\approx\bm{x}^{(0)}+(1+\eta\lambda^{\star})^{t}(\bm{ u}^{\star\top}\bm{x}^{(0)})\bm{u}^{\star}\) for all \(t\leq T_{2}^{\prime}\) and the \(\ell_{2}\)-norm of \(\widetilde{\bm{x}}^{(t)}\) is also approximately given by \(\left(1+\frac{(1+\eta\lambda^{\star})^{t}}{\sqrt{n}}\right)\beta_{0}\). The \(\ell_{\infty}\)-norm is about \(\frac{1}{\sqrt{n}}\) times smaller than the \(\ell_{2}\)-norm. We make this observation rigorous with the following lemma.

**Lemma C.1**.: _For all \(t\leq T_{2}^{\prime}\), we have_

\[\frac{1}{8}\frac{1}{\sqrt{\log n}}\left(1+\frac{(1+\eta\lambda^{ \star})^{t}}{\sqrt{n}}\right)\beta_{0} \leq\Big{\|}\widetilde{\bm{x}}^{(t)}\Big{\|}_{2}\leq 2\sqrt{\log n }\left(1+\frac{(1+\eta\lambda^{\star})^{t}}{\sqrt{n}}\right)\beta_{0},\] \[\frac{1}{4}\frac{1}{\sqrt{\log n}}\left(1+\frac{(1+\eta\lambda^{ \star})^{t}}{\sqrt{n}}\right)\frac{\beta_{0}}{\sqrt{n}} \leq\Big{\|}\widetilde{\bm{x}}^{(t)}\Big{\|}_{\infty}\leq 2\sqrt{\log n }\left(1+(1+\eta\lambda^{\star})^{t}\sqrt{\frac{\mu}{n}}\right)\frac{\beta_{0} }{\sqrt{n}}.\]

Proof.: For brevity, les us drop the superscript \((t)\) and write \(\widetilde{\bm{x}}=A\bm{x}^{(0)}+B\bm{u}^{\star}\). For the upper bounds, we may use the triangle inequality

\[\Big{\|}A\bm{x}^{(0)}+B\bm{u}^{\star}\Big{\|}\leq A\Big{\|}\bm{x}^{(0)}\Big{\|} +|B||\bm{u}^{\star}\|.\]If we use (B.5) and (C.2), we get the upper bounds for \(A\) and \(B\). We have \(\left\|\bm{x}^{(0)}\right\|_{2}\leq 2\beta_{0}\) by (B.1), and the \(\ell_{\infty}\)-norm of \(\bm{x}^{(0)}\) is controlled through (B.2). These finish the proof for the upper bounds.

From the definition of \(B\), we have \(B(\bm{u}^{\star\top}\bm{x}^{(0)})\geq 0\), and

\[\left\|A\bm{x}^{(0)}+B\bm{u}^{\star}\right\|_{2}^{2} =A^{2}\left\|\bm{x}^{(0)}\right\|_{2}^{2}+B^{2}+2AB(\bm{u}^{\star \top}\bm{x}^{(0)})\] \[\geq A^{2}\left\|\bm{x}^{(0)}\right\|_{2}^{2}+B^{2}\] \[\geq\frac{1}{4}\left(A\left\|\bm{x}^{(0)}\right\|_{2}+|B|\right) ^{2}.\]

(B.1) and (B.4) together with the lower bound in (C.2) give the desired lower bound for \(\left\|\widetilde{\bm{x}}\right\|_{2}\). The lower bound for \(\ell_{\infty}\)-norm is directly implied from Lemma B.2 together with (B.4) and (C.2). 

With Lemma C.1, we have

\[\frac{1}{8}\frac{1}{\sqrt{\log n}}\frac{(1+\eta\lambda^{\star})^{T_{2}^{ \prime}}}{\sqrt{n}}\beta_{0}\leq\frac{1}{8}\frac{1}{\sqrt{\log n}}\left(1+ \frac{(1+\eta\lambda^{\star})^{T_{2}^{\prime}}}{\sqrt{n}}\right)\beta_{0} \leq\left\|\widetilde{\bm{x}}^{(T_{2}^{\prime})}\right\|_{2}\leq\frac{\sqrt{ \lambda^{\star}}}{8\sqrt{\log n}},\]

and thus

\[T_{2}^{\prime}\leq\frac{1}{\log(1+\eta\lambda^{\star})}\log\frac{\sqrt{\lambda ^{\star}n}}{\beta_{0}}\leq\frac{11\log n}{\log(1+\eta\lambda^{\star})}\leq \frac{64\log n}{\eta\lambda^{\star}}.\]

For \(t\leq T_{1}\) where \((1+\eta\lambda^{\star})^{t}\) is not big, the bounds in Lemma C.1 are simplified to

\[\frac{1}{\sqrt{\log n}}\beta_{0} \lesssim\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\sqrt{ \log n}\beta_{0},\] (C.3) \[\frac{1}{\sqrt{\log n}}\frac{\beta_{0}}{\sqrt{n}} \lesssim\left\|\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim \sqrt{\log n}\frac{\beta_{0}}{\sqrt{n}}.\] (C.4)

After \(\widetilde{\bm{x}}^{(t)}\) becomes almost parallel to \(\bm{u}^{\star}\) and before \(T_{2}^{\prime}\), we could approximate \(\widetilde{\beta}_{t}\) as increasing with the rate \((1+\eta\lambda^{\star})\). However, after \(T_{2}^{\prime}\), this approximation is invalid, and \(\widetilde{\beta}_{t}\) grows at a slower rate as it increases and it eventually converges to \(\sqrt{\lambda^{\star}}\). How much iterations will be required for it to reach \(\sqrt{\lambda^{\star}}\sqrt{1-\frac{1}{\log n}}\) after \(T_{2}^{\prime}\)? With Lemma C.2, we will prove that \(O(\log\log n)\) iterations are required after \(T_{2}^{\prime}\).

**Lemma C.2**.: _At \(t=T_{2}^{\prime}+\frac{6\log\log n}{\log(1+\eta\lambda^{\star})}\), we have \(\widetilde{\beta}_{t}^{2}\geq\lambda^{\star}\left(1-\frac{1}{\log n}\right)\)._

Proof.: From the decomposition (C.1), we have

\[\left|\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}-\left|B^{(t)} \right|\right| \leq\left\|\bm{x}^{(0)}\right\|_{2}\] \[\left|\left|\bm{u}^{\star\top}\bm{x}^{(0)}\right|-\left|B^{(t)} \right|\right| \leq\left|A^{(t)}\right|\left|\bm{u}^{\star\top}\bm{x}^{(0)} \right|\leq\left\|\bm{x}^{(0)}\right\|_{2},\]

and thus

\[\left|\widetilde{\alpha}_{t}-\widetilde{\beta}_{t}\right|\leq 2\left\|\bm{x}^{(0)} \right\|_{2}\leq\frac{\sqrt{\lambda^{\star}}}{3\log^{2}n}.\] (C.5)

holds for all \(t\). Because \(\widetilde{\beta}_{t}\gtrsim\frac{1}{\log n}\) for all \(t\geq T_{2}^{\prime}\), (C.5) implies that \(\widetilde{\beta}_{t}\) is well approximated by \(\widetilde{\alpha}_{t}\). Hence, we will focus on \(\widetilde{\alpha}_{t}\), which is an increasing sequence that evolves with

\[\widetilde{\alpha}_{t+1}=(1-\eta\widetilde{\beta}_{t}^{2}+\eta\lambda^{\star })\widetilde{\alpha}_{t}.\]

For all \(i\geq 1\), let \(N_{i}\) be the last \(t\) such that \(\lambda^{\star}-\widetilde{\alpha}_{t}^{2}\geq\frac{\lambda^{\star}}{e^{t}}\). Then, we have

\[\lambda^{\star}-\widetilde{\alpha}_{N_{i}}^{2}\geq\frac{\lambda^{\star}}{e^{t} }>\lambda^{\star}-\widetilde{\alpha}_{N_{i}+1}^{2}.\] (C.6)Let \(i\geq 2\). For all \(N_{i-1}<t\leq N_{i}\),

\[\frac{\widetilde{\alpha}_{t+1}}{\widetilde{\alpha}_{t}}=1-\eta\widetilde{\beta} _{t}^{2}+\eta\lambda^{\star}=1+\eta(\lambda^{\star}-\widetilde{\alpha}_{t}^{2}) +\eta(\widetilde{\alpha}_{t}^{2}-\widetilde{\beta}_{t}^{2})\geq 1+\frac{\eta \lambda^{\star}}{e^{i}}-\frac{\eta\lambda^{\star}}{\log^{2}n}\geq 1+0.99 \frac{\eta\lambda^{\star}}{e^{i}}.\]

We used (C.5), (C.6), and the fact that \(\widetilde{\alpha}_{t},\widetilde{\beta}_{t}\leq\sqrt{\lambda^{\star}}\) for all \(t\). This implies

\[\left(1+0.99\frac{\eta\lambda^{\star}}{e^{i}}\right)^{N_{i}-N_{i-1}-1}x_{N_{i- 1}+1}\leq x_{N_{i}}.\]

From the lower and upper bounds provided by (C.6), we have

\[\sqrt{\lambda^{\star}}\sqrt{1-\frac{1}{e^{i-1}}}\left(1+0.99\frac {\eta\lambda^{\star}}{e^{i}}\right)^{N_{i}-N_{i-1}-1}\leq\sqrt{\lambda^{\star }}\sqrt{1-\frac{1}{e^{i}}},\] \[\left(1+0.99\frac{\eta\lambda^{\star}}{e^{i}}\right)^{2(N_{i}-N_ {i-1}-1)}\leq\frac{e^{i}-1}{e^{i}}\frac{e^{i-1}}{e^{i-1}-1}=\frac{e^{i-1}- \frac{1}{e}}{e^{i-1}-1}\leq 1+\frac{1}{e^{i-1}}.\]

Taking \(\log\) on both sides and using the inequality \(\frac{1}{2}x<\log(1+x)<x\) that holds for \(0<x<1\), we get

\[N_{i}-N_{i-1}\leq 1+\frac{1}{2}\frac{\log\left(1+\frac{1}{e^{i-1}}\right)}{ \log\left(1+0.99\frac{\eta\lambda^{\star}}{e^{i}}\right)}\leq 1+\frac{e}{0.99 \eta\lambda^{\star}}.\]

For \(t\leq N_{1}\), we have

\[\frac{\widetilde{\alpha}_{t+1}}{\widetilde{\alpha}_{t}}\geq 1+0.99\frac{\eta \lambda^{\star}}{e},\]

and thus

\[\sqrt{\lambda^{\star}}\sqrt{1-\frac{1}{e}}\geq\alpha_{N_{1}}\geq\left(1+0.99 \frac{\eta\lambda^{\star}}{e}\right)^{N_{1}}\widetilde{\alpha}_{T_{2}^{ \prime}}=\left(1+0.99\frac{\eta\lambda^{\star}}{e}\right)^{N_{1}}\sqrt{\frac{ \lambda^{\star}}{21\log n}}.\]

Taking \(\log\) on both sides we get

\[N_{1}\leq 3\frac{\log\log n}{\eta\lambda^{\star}}.\]

Hence, we have

\[N_{\log\log n+1}+1 \leq\left(1+\frac{e}{0.99\eta\lambda^{\star}}\right)\log\log n+N_ {1}+1\] \[\leq\left(1+\frac{e}{0.99\eta\lambda^{\star}}\right)\log\log n+3 \frac{\log\log n}{\eta\lambda^{\star}}+1\] \[\leq\frac{6\log\log n}{\log(1+\eta\lambda^{\star})},\]

but at \(t=N_{\log\log n+1}+1\), it holds that

\[\widetilde{\alpha}_{t}^{2}>\lambda^{\star}\left(1-\frac{1}{e\log n}\right),\]

and we have

\[\widetilde{\beta}_{t}^{2}>\lambda^{\star}\left(1-\frac{1}{\log n}\right)\]

as desired. Note that \(\widetilde{\beta}_{t}\) is also an increasing sequence as \(\widetilde{\alpha}_{t}\). 

It is implied from Lemma C.2 that \(T_{2}\leq\frac{1}{\log(1+\eta\lambda^{\star})}\log\frac{\sqrt{\lambda^{\star} n}}{\beta_{0}}+\frac{6\log\log n}{\log(1+\eta\lambda^{\star})}=(1+o(1))\frac{1}{ \eta\lambda^{\star}}\log\frac{\sqrt{\lambda^{\star}n}}{\beta_{0}}\). The following corollary shows that \(\widetilde{\boldsymbol{x}}^{(t)}\) is sufficiently close to \(\boldsymbol{x}^{\star}\) at \(t=T_{2}\).

**Corollary C.3**.: _At \(t=T_{2}\), we have_

\[\min\left\{\left\|\widetilde{\boldsymbol{x}}^{(t)}-\boldsymbol{x}^{\star} \right\|_{2},\left\|\widetilde{\boldsymbol{x}}^{(t)}+\boldsymbol{x}^{\star} \right\|_{2}\right\} \lesssim\frac{1}{\sqrt{\log n}}\|\boldsymbol{x}^{\star}\|_{2},\] (C.7) \[\min\left\{\left\|\widetilde{\boldsymbol{x}}^{(t)}-\boldsymbol{x}^{ \star}\right\|_{\infty},\left\|\widetilde{\boldsymbol{x}}^{(t)}+\boldsymbol{x}^{ \star}\right\|_{\infty}\right\} \lesssim\frac{1}{\sqrt{\log n}}\|\boldsymbol{x}^{\star}\|_{\infty}.\] (C.8)Proof.: When \(B^{(t)}>0\), from the decomposition

\[\bm{x}^{(t)}-\bm{x}^{\star}=A^{(t)}\bm{x}^{(0)}+(B^{(t)}-\widetilde{\beta}_{t}) \bm{u}^{\star}+(\widetilde{\beta}_{t}-\sqrt{\lambda^{\star}})\bm{u}^{\star},\]

we have

\[\left\|\bm{x}^{(t)}-\bm{x}^{\star}\right\|_{2}\leq 2\left\|\bm{x}^{(0)}\right\|_{ 2}+\frac{\sqrt{\lambda^{\star}}}{\sqrt{\log n}}\leq\frac{2\sqrt{\lambda^{ \star}}}{\sqrt{\log n}}=\frac{2}{\sqrt{\log n}}\left\|\bm{x}^{\star}\right\|_{2}.\]

For the cases \(B^{(t)}<0\) and \(\ell_{\infty}\)-norm, we may use similar technique. 

## Appendix D Phase I

### Proof of Lemma 5.2

In this subsection, we provide a proof to the following lemma, which is a formal statement of Lemma 5.2.

**Lemma D.1**.: _With high probability, there exists a universal constant \(c_{0}>0\) such that_

\[\left\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\leq c_{0} \mu\sqrt{\frac{\log n}{np}}\beta_{0}t\] (D.1)

_for all \(t\leq T_{1}\), if \(n^{2}p\gtrsim\mu^{4}n\log^{21}n\) and the initialization point \(\bm{x}^{(0)}\) satisfies (B.1) to (B.6)._

Proof.: Let us rewrite the update equations (2) and (15) as

\[\widetilde{\bm{x}}^{(t+1)} =\left(\bm{I}-\eta\widetilde{\beta}_{t}^{2}+\eta\bm{M}^{\star} \right)\widetilde{\bm{x}}^{(t)},\] \[\widehat{\bm{x}}^{(t+1)} =\left(\bm{I}-\eta\widetilde{\beta}_{t}^{2}+\eta\bm{M}^{\diamond }\right)\widehat{\bm{x}}^{(t)},\]

where \(\widetilde{\beta}_{t}=\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}^{2}\). Then, \(\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\) is a product between \(\bm{x}^{(0)}\) and \(P^{(t)}(\bm{I},\bm{M}^{\star},\bm{H})\), which is a matrix polynomial of \(\bm{I},\bm{M}^{\star},\bm{H}\), where \(\bm{H}=\bm{M}^{\diamond}-\bm{M}^{\star}\).

\[P^{(t)}(\bm{I},\bm{M}^{\star},\bm{H}):=\left(\prod_{s=1}^{t} \left((1-\eta\widetilde{\beta}_{s}^{2})\bm{I}+\eta\bm{M}^{\star}+\eta\bm{H} \right)-\prod_{s=1}^{t}\left((1-\eta\widetilde{\beta}_{s}^{2})\bm{I}+\eta\bm{ M}^{\star}\right)\right)\] (D.2) \[\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}=P^{(t)}(\bm{I}, \bm{M}^{\star},\bm{H})\bm{x}^{(0)}\] (D.3)

We classify the terms that appear after expanding the matrix polynomial \(P^{(t)}(\bm{I},\bm{M}^{\star},\bm{H})\) into two types; 1) the terms that contain \(\bm{H}\) but not \(\bm{M}^{\star}\), 2) the terms that contain both \(\bm{H}\) and \(\bm{M}^{\star}\). We define \(P_{1}^{(t)}(\bm{I},\bm{H})\) to be a matrix polynomial of \(\bm{I}\) and \(\bm{H}\), which is equal to summation of the first type, and it is explicitly written as

\[P_{1}^{(t)}(\bm{I},\bm{H})=\prod_{s=1}^{t}\left((1-\eta\widetilde{\beta}_{s}^{ 2})\bm{I}+\eta\bm{H}\right)-\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2}) \bm{I}.\]

We correspondingly define \(P_{2}^{(t)}(\bm{I},\bm{M}^{\star},\bm{H})\) to be summation of the second type, and it is equal to

\[P_{2}^{(t)}(\bm{I},\bm{M}^{\star},\bm{H})=P^{(t)}(\bm{I},\bm{M}^{\star},\bm{H} )-P_{1}^{(t)}(\bm{I},\bm{H}).\]

For \(x,y\in\mathbb{R}\), we define \(P_{1}^{(t)}(x,y)\) as the value that is obtained by substituting \(x,y\) instead of \(\bm{I},\bm{H}\), respectively. For example, \(P_{1}^{(t)}(1,2)=\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2}+2\eta)-\prod_ {s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2})\). For \(x,y,z\in\mathbb{R}\), \(P_{2}^{(t)}(x,y,z)\) is defined in a similar manner.

We bound the contribution of each type separately because the triangle inequality gives

\[\left\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\leq\left\|P_ {1}^{(t)}(\bm{I},\bm{H})\bm{x}^{(0)}\right\|_{2}+\left\|P_{2}^{(t)}(\bm{I},\bm {M}^{\star},\bm{H})\bm{x}^{(0)}\right\|_{2}.\]

Every term in \(P_{1}^{(t)}(\bm{I},\bm{H})\) is \(\bm{H}^{s}\) times a constant. We have \(\left\|\bm{H}^{s}\bm{x}^{(0)}\right\|_{2}\leq\left\|\bm{H}\right\|^{s}\left\| \bm{x}^{(0)}\right\|_{2}\), and hence with triangle inequality

\[\left\|P_{1}^{(t)}(\bm{I},\bm{H})\bm{x}^{(0)}\right\|_{2}\leq P_{1}^{(t)}(1, \|\bm{H}\|)\beta_{0}.\]If \(n^{2}p\gtrsim\mu^{2}n\log^{3}n\), we can further bound \(P_{1}^{(t)}(1,\|\bm{H}\|)\) as

\[P_{1}^{(t)}(1,\|\bm{H}\|) =\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2}+\eta\|\bm{H}\|)- \prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2})\] \[=\prod_{s=1}^{t}(1-\eta\widetilde{\beta}_{s}^{2})\left(\prod_{s=1 }^{t}\left(1+\frac{\eta\|\bm{H}\|}{1-\eta\widetilde{\beta}_{s}^{2}}\right)-1\right)\] \[\leq\left(1+\frac{\eta}{1-\eta\lambda^{\star}}\|\bm{H}\|\right)^{ t}-1\] \[\leq\left(\exp\left(\frac{\eta}{1-\eta\lambda^{\star}}\|\bm{H}\| t\right)-1\right)\] \[\leq\frac{2\eta}{1-\eta\lambda^{\star}}\|\bm{H}\|t\]

The third line uses the fact that \(\widetilde{\beta}_{t}^{2}\leq\lambda^{\star}\) for all \(t\leq T_{1}\). The fourth and fifth lines are derived from an elementary inequality \(1+x\leq e^{x}\leq 1+2x\), which holds for small \(x>0\). Note that \(\eta\|\bm{H}\|t\lesssim\eta\lambda^{\star}\mu\sqrt{\frac{\log n}{np}}\ll 1\) from Lemmas A.1 and A.3, and the fact that \(T_{1}\lesssim\log n\).

We can decompose every term of second type as a product of \(\eta\), \(\lambda^{\star}\), \(\bm{u}^{\star}\), \(\bm{H}^{s}\bm{u}^{\star}\), \(\bm{u}^{\star\top}\bm{H}^{s}\bm{x}^{(0)}\), \(\bm{u}^{\star\top}\bm{H}^{s}\bm{u}^{\star}\), and \(\bm{u}^{\star\top}\bm{x}^{(0)}\). We describe this with some examples.

\[(\eta\bm{H})^{s_{1}}(\eta\bm{M}^{\star})(\eta\bm{H})^{s_{2}}\bm{x }^{(0)} =\eta^{s_{1}+s_{2}+1}\lambda^{\star}(\bm{H}^{s_{1}}\bm{u}^{\star}) (\bm{u}^{\star\top}\bm{H}^{s_{2}}\bm{x}^{(0)})\] \[(\eta\bm{H})^{s}(\eta\bm{M}^{\star})\bm{x}^{(0)} =\eta^{s+1}(\bm{H}^{s}\bm{u}^{\star})(\bm{u}^{\star\top}\bm{x}^{(0)})\] \[(\eta\bm{M}^{\star})(\eta\bm{H})^{s}(\eta\bm{M}^{\star})\bm{x}^{(0)} =\eta^{s+2}\lambda^{\star 2}\bm{u}^{\star}(\bm{u}^{\star\top}\bm{H} \bm{u}^{\star})(\bm{u}^{\star\top}\bm{x}^{(0)})\] \[(\eta\bm{M}^{\star})(\eta\bm{H})^{s}\bm{x}^{(0)} =\eta^{s+1}\lambda^{\star}\bm{u}^{\star}(\bm{u}^{\star\top}\bm{H}^ {s}\bm{x}^{(0)})\]

The terms \(\bm{H}^{s}\bm{u}^{\star}\) and \(\bm{u}^{\star\top}\bm{H}^{s}\bm{u}^{\star}\) are bounded with

\[\|\bm{H}^{s}\bm{u}^{\star}\|_{2}\leq\|\bm{H}\|^{s},\quad\left|\bm{u}^{\star \top}\bm{H}^{s}\bm{u}^{\star}\right|\leq\|\bm{H}\|^{s},\] (D.4)

and the terms that contain \(\bm{x}^{(0)}\) are bounded with (B.3). For every term of second type that includes \(s_{1}\) times of \(\eta\bm{M}^{\star}\) and \(s_{2}\) times of \(\eta\bm{H}\), the bounds (D.4) and (B.3) imply that \(\ell_{2}\)-norm of the term multiplied by \(\bm{x}^{(0)}\) is at most

\[(\eta\lambda^{\star})^{s_{1}}(\eta\|\bm{H}\|)^{s_{2}}2\sqrt{\frac{\log n}{n}} \beta_{0}.\]

Hence, similar to the first type, we have

\[\left\|P_{2}^{(t)}(\bm{I},\bm{M}^{\star},\bm{H})\bm{x}^{(0)}\right\|_{2}\leq P _{2}^{(t)}(1,\lambda^{\star},\|\bm{H}\|)2\sqrt{\frac{\log n}{n}}\beta_{0}.\]

If \(n^{2}p\gtrsim\mu^{2}n\log^{3}n\), we can further bound \(P_{2}^{(t)}(1,\lambda^{\star},\|\bm{H}\|)\) as

\[P_{2}^{(t)}(1,\lambda^{\star},\|\bm{H}\|) =\prod_{s=1}^{t}(1-\eta\beta_{s}^{2}+\eta\lambda^{\star}+\eta\| \bm{H}\|)-\prod_{s=1}^{t}\left(1-\eta\beta_{s}^{2}+\eta\lambda^{\star}\right)- P_{1}^{(t)}(1,\|\bm{H}\|)\] \[\leq\prod_{s=1}^{t}(1-\eta\beta_{s}^{2}+\eta\lambda^{\star}+\eta \|\bm{H}\|)-\prod_{s=1}^{t}\left(1-\eta\beta_{s}^{2}+\eta\lambda^{\star}\right)\] \[\leq\left(\prod_{s=1}^{t}(1-\eta\beta_{s}^{2}+\eta\lambda^{\star} )\right)\left(\prod_{s=1}^{t}\left(1+\frac{\eta}{1-\eta\beta_{s}^{2}+\eta \lambda^{\star}}\|\bm{H}\|\right)-1\right)\] \[\leq(1+\eta\lambda^{\star})^{t}\left((1+\eta\|\bm{H}\|)^{t}-1\right)\] \[\leq 2\eta\|\bm{H}\|t(1+\eta\lambda^{\star})^{t}.\]Combining all, we have

\[\left\|\widehat{\bm{x}}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \leq 4\eta\|\bm{H}\|t\left(1+\sqrt{\frac{\log n}{n}}(1+\eta\lambda^{ \star})^{t}\right)\beta_{0}\] \[\leq 4\eta\|\bm{H}\|t\left(1+\sqrt{\frac{\log n}{n}}(1+\eta\lambda^ {\star})^{T_{1}}\right)\beta_{0}\] \[\leq 4\eta\|\bm{H}\|t\left(1+\sqrt{\frac{\mu^{4}\log^{22}n}{np}} \right)\beta_{0}\] \[\leq c_{0}\mu\sqrt{\frac{\log n}{np}}\beta_{0}t\]

for all \(t\leq T_{1}\) for some constant \(c_{0}>0\) if \(n^{2}p\gtrsim\mu^{4}n\log^{22}n\). 

### Proof of Lemmas 5.3 to 5.5

We prove Lemmas 5.3 to 5.5 all together in an inductive manner.

**Lemma D.2**.: _Suppose that the initialization point \(\bm{x}^{(0)}\) satisfies (B.1) to (B.6). If \(n^{2}p\gtrsim\mu^{5}n\log^{22}n\), for all \(t\leq T_{1}\), we have_

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \leq 2c_{0}\mu\sqrt{\frac{\log n}{np}}\beta_{0}t,\] (D.5) \[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty} \leq(3c_{0}+c_{5})\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{ 0}}{\sqrt{n}}t^{2},\] (D.6) \[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \leq c_{2}(3c_{0}+c_{5}+1)\frac{1}{\lambda^{\star}}\sqrt{\frac{ \mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{\star})^{t}\beta_{0}^{3},\] (D.7) \[\left\|\bm{x}^{(t)}-\bm{x}^{(t,)}\right\|_{2} \leq c_{5}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n }}t,\] (D.8) \[\left|\bm{u}^{(l)\top}(\bm{x}^{(t)}-\bm{x}^{(t,)})\right| \leq c_{6}\sqrt{\frac{\mu^{3}\log^{2}n}{np}}(1+\eta\lambda^{\star })^{t}\frac{\beta_{0}}{n},\] (D.9) \[\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l} \right| \leq 3c_{0}\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}t^{2},\] (D.10)

_with high probability, where \(c_{2},c_{5},c_{6}\) are positive constants._

Before we start the proof, we introduce some notations. For \(\bm{x}\in\mathbb{R}^{n}\), let us define

\[\left\|\bm{x}\right\|_{2,i}=\sqrt{\frac{1}{p}\sum_{j=1}^{n}\delta_{ij}x_{j}^{ 2}},\quad\bm{I}_{\bm{x}}=\frac{1}{\left\|\bm{x}\right\|_{2}^{2}}\operatorname {diag}(\left\|\bm{x}\right\|_{2,1}^{2},\cdots,\left\|\bm{x}\right\|_{2,n}^{2}).\]

\(\left\|\bm{x}\right\|_{2,i}\) is the \(\ell_{2}\)-norm of \(\bm{x}\) estimated with sampling of the \(i\)th row. With this notation, we can write the gradient of \(f\) as

\[\nabla f(\bm{x})=\left\|\bm{x}\right\|_{2}^{2}\bm{I}_{\bm{x}}\bm{x}-\bm{M}^{ \circ}\bm{x}.\]

The function \(g\) is defined as

\[g(\bm{x})=\frac{1}{4p}\left\|\mathcal{P}_{\Omega}\big{(}\bm{x}\bm{x}^{\top} \big{)}\right\|_{\mathrm{F}}^{2},\]

and its gradient satisfies

\[\nabla f(\bm{x})=\nabla g(\bm{x})-\bm{M}^{\circ}\bm{x}.\]The Hessian of \(g(\bm{x})\) is equal to

\[\nabla^{2}g(\bm{x})=\|\bm{x}\|_{2}^{2}\bm{I}_{\bm{x}}+\frac{2}{p}\mathcal{P}_{ \Omega}\big{(}\bm{x}\bm{x}^{\top}\big{)}.\]

The base case \(t=0\) for induction hypotheses (D.5) to (D.10) trivially hold because all three sequences \(\bm{x}^{(t)}\), \(\widehat{\bm{x}}^{(t)}\), \(\widehat{\bm{x}}^{(t)}\) start from the same point. Now, we assume that the hypotheses hold up to the \(t\)th iteration and show that they hold at the \((t+1)\)st iteration. For brevity, we drop the superscript \((t)\) from \(\bm{x}^{(t)}\), \(\bm{x}^{(t,l)}\), \(\widehat{\bm{x}}^{(t)}\), \(\widehat{\bm{x}}^{(t)}\) and denote them as \(\bm{x}\), \(\bm{x}^{(l)}\), \(\widehat{\bm{x}}\), \(\widehat{\bm{x}}\), respectively. Also, recall that \(T_{1}\) is defined to be the last \(t\) such that \((1+\eta\lambda^{\star})^{t}\leq\sqrt{\frac{\mu^{4}\log^{2}n}{np}}\sqrt{n}\), and the magnitude of initialization satisfies \(\beta_{0}^{2}\lesssim\lambda^{\star}\sqrt{\frac{np}{\mu^{3}\log^{2}n}}\frac{1 }{\sqrt{n}}\) so that there exists a constant \(c_{1}>0\) such that \((1+\eta\lambda^{\star})^{t}\beta_{0}^{2}\leq c_{1}\frac{\lambda^{\star}}{ \sqrt{\mu\log^{5}n}}\).

(D.7) at \((t+1)\)We first decompose \(\bm{x}^{(t+1)}-\widehat{\bm{x}}^{(t+1)}\) as

\[\bm{x}^{(t+1)}-\widehat{\bm{x}}^{(t+1)} =(\bm{I}+\eta\bm{M}^{\circ})(\bm{x}-\widehat{\bm{x}})-\eta\|\bm{ x}\|_{2}^{2}\bm{I}_{\bm{x}}\bm{x}+\eta\|\widehat{\bm{x}}\|_{2}^{2}\widehat{\bm{x}}\] \[=\left(\bm{I}-\eta\|\widetilde{\bm{x}}\|_{2}^{2}\bm{I}+\eta\bm{M} ^{\circ}\right)\left(\bm{x}-\widehat{\bm{x}}\right)-\eta\left(\|\bm{x}\|_{2}^ {2}\bm{I}_{\bm{x}}-\|\widetilde{\bm{x}}\|_{2}^{2}\bm{I}\right)\bm{x}.\] (D.11)

With the help of Lemma G.8, we bound the maximum entry of a diagonal matrix \(\big{\|}\bm{x}\big{\|}_{2}^{2}\bm{I}_{\bm{x}}-\big{\|}\widehat{\bm{x}}\big{\|} _{2}^{2}\bm{I}\). We have

\[\max_{i\in[n]}\Bigl{\|}\|\bm{x}\|_{2,i}^{2}-\|\widetilde{\bm{x}} \|_{2}^{2}\Bigr{|} \lesssim n\|\bm{x}-\widetilde{\bm{x}}\|_{\infty}\|\bm{x}+ \widetilde{\bm{x}}\|_{\infty}+\sqrt{\frac{\log n}{p}}\|\widetilde{\bm{x}}\|_{ 2}\|\widetilde{\bm{x}}\|_{\infty}+\frac{\log n}{p}\|\widetilde{\bm{x}}\|_{ \infty}^{2}\] \[\lesssim(3c_{0}+c_{5})\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\beta_{0} ^{2}t^{2}+\sqrt{\frac{\log^{2}n}{np}}\beta_{0}^{2}+\frac{\log^{2}n}{np}\beta_ {0}^{2}\] \[\lesssim\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\beta_{0}^{2}((3c_{0}+ c_{5})t^{2}+1)\]

if \(n^{2}p\gtrsim n\log^{2}n\). Hence, there exists a universal constant \(c_{2}>0\) that is independent of \(t\) such that

\[\left(\max_{i\in[n]}\Bigl{\|}\Big{\|}\bm{x}^{(t)}\Big{\|}_{2,i}^{2}-\Bigl{\|} \widetilde{\bm{x}}^{(t)}\Big{\|}_{2}^{2}\Bigr{|}\right)\Bigl{\|}\bm{x}^{(t)} \Bigr{\|}_{2}\leq\frac{1}{2}c_{2}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\beta_{0}^{ 3}((3c_{0}+c_{5})t^{2}+1)\]

for all \(t\leq T_{1}\). With the decomposition (D.11), we have

\[\Bigl{\|}\bm{x}^{(t+1)}-\widehat{\bm{x}}^{(t+1)}\Bigr{\|}_{2} \leq\left(1-\eta\|\widehat{\bm{x}}\|_{2}^{2}+\eta\lambda^{\circ} \right)\|\bm{x}-\widehat{\bm{x}}\|_{2}+\eta\left(\max_{i\in[n]}\Bigl{\|}\bm{x }\|_{2,i}^{2}-\|\widetilde{\bm{x}}\|_{2}^{2}\Bigr{|}\right)\|\bm{x}\|_{2}\] \[\leq(1+\eta\lambda^{\circ})\|\bm{x}-\widehat{\bm{x}}\|_{2}+\frac {1}{2}c_{2}(\eta\lambda^{\star})^{3}\frac{1}{\lambda^{\star}}\sqrt{\frac{\mu^{ 3}\log^{3}n}{np}}\beta_{0}^{3}((3c_{0}+c_{5})t^{2}+1).\]

From (A.1), there exists a universal constant \(c_{3}>0\) such that \(\eta\lambda^{\circ}\leq\eta\lambda^{\star}+\frac{c_{3}}{\log^{2}n}\) if \(n^{2}p\gtrsim\mu^{2}n\log^{5}n\). Combining all, for all \(s\leq t\), we have

\[\Bigl{\|}\bm{x}^{(s+1)}-\widehat{\bm{x}}^{(s+1)}\Bigr{\|}_{2}\leq \left(1+\eta\lambda^{\star}+\frac{c_{3}}{\log^{2}n}\right)\Bigl{\|} \bm{x}^{(s)}-\widehat{\bm{x}}^{(s)}\Bigr{\|}_{2}\] \[+\frac{1}{2}c_{2}(\eta\lambda^{\star})^{3}\frac{1}{\lambda^{ \star}}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\beta_{0}^{3}((3c_{0}+c_{5})s^{2}+1).\]

An analysis on the recursive equation

\[x_{s+1}=\left(1+\eta\lambda^{\star}+\frac{c_{3}}{\log^{2}n}\right)x_{s}+\frac {1}{2}c_{2}(\eta\lambda^{\star})^{3}\frac{1}{\lambda^{\star}}\sqrt{\frac{\mu^{3} \log^{3}n}{np}}\beta_{0}^{3}((3c_{0}+c_{5})s^{2}+1),\quad x_{0}=0,\]

proves that

\[\Bigl{\|}\bm{x}^{(t+1)}-\widehat{\bm{x}}^{(t+1)}\Bigr{\|}_{2}\leq c_{2}(3c_{0}+c_{ 5}+1)\frac{1}{\lambda^{\star}}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{ \star})^{t+1}\beta_{0}^{3}.\]

[MISSING_PAGE_FAIL:22]

Hence, we get

\[\left\|\left\langle\overline{\mathbb{S}}\right\rangle\right\|_{2} \leq\left(\left\|\boldsymbol{M}^{\circ}-\boldsymbol{M}^{(l)} \right\|+\left\|\boldsymbol{M}^{(l)}-\lambda^{(l)}\boldsymbol{u}^{(l)} \boldsymbol{u}^{(l)\top}\right\|\right)\left\|\boldsymbol{x}-\boldsymbol{x}^{ (l)}\right\|_{2}\] \[\lesssim\lambda^{\star}\mu\sqrt{\frac{\log n}{np}}\left\| \boldsymbol{x}-\boldsymbol{x}^{(l)}\right\|_{2}.\]

Lastly, we apply Lemmas G.11 and G.13 to get

\[\left\|\left\langle\overline{\mathbb{G}}\right\rangle\right\|_{2}\lesssim \lambda^{\star}\mu\sqrt{\frac{\log n}{np}}\frac{\beta_{0}}{\sqrt{n}},\quad \left\|\left\langle\overline{\mathbb{Z}}\right\rangle\right\|_{2}\lesssim \lambda^{\star}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}},\]

There exists a universal constant \(c_{4}>0\) such that

\[\eta\left(2\left\|\left\langle\overline{\mathbb{A}}\right\rangle\right\|_{2}+ \left\|\left\langle\overline{\mathbb{Z}}\right\rangle\right\|_{2}+\left\| \left\langle\overline{\mathbb{S}}\right\rangle\right\|_{2}\right)\leq\frac{c_ {4}}{\log^{2}n}\left\|\boldsymbol{x}-\boldsymbol{x}^{(l)}\right\|_{2}\]

if \(n^{2}p\gtrsim\mu^{2}n\log^{5}n\), and there exists a universal constant \(c_{5}>0\) such that

\[\eta\left(\left\|\left\langle\overline{\mathbb{A}}\right\rangle\right\|_{2}+ \left\|\left\langle\overline{\mathbb{A}}\right\rangle\right\|_{2}+\left\| \left\langle\overline{\mathbb{G}}\right\rangle\right\|_{2}\right)\leq\frac{1} {2}c_{5}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}}.\]

if \(n^{2}p\gtrsim\mu^{5}n\log^{20}n\). Combining all, we have

\[\left\|\boldsymbol{x}^{(s+1)}-\boldsymbol{x}^{(s+1,l)}\right\|_{2} \leq\left(1-\eta\left\|\widetilde{\boldsymbol{x}}^{(s)}\right\| _{2}^{2}+\frac{c_{4}}{\log^{2}n}\right)\left\|\boldsymbol{x}^{(s)}- \boldsymbol{x}^{(s,l)}\right\|_{2}+\frac{1}{2}c_{5}\mu\sqrt{\frac{\log^{2}n} {np}}\frac{\beta_{0}}{\sqrt{n}}\] \[\leq\left(1+\frac{c_{4}}{\log^{2}n}\right)\left\|\boldsymbol{x}^ {(s)}-\boldsymbol{x}^{(s,l)}\right\|_{2}+\frac{1}{2}c_{5}\mu\sqrt{\frac{\log^ {2}n}{np}}\frac{\beta_{0}}{\sqrt{n}}.\]

for all \(s\leq t\). An analysis on the recursive equation

\[x_{s+1}=\left(1+\frac{c_{4}}{\log^{2}n}\right)x_{s}+\frac{1}{2}c_{5}\mu\sqrt{ \frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}},\quad x_{0}=0\]

gives the desired bound

\[\left\|\boldsymbol{x}^{(t+1)}-\boldsymbol{x}^{(t+1,l)}\right\|_{2} \leq\frac{\log^{2}n}{c_{4}}\left(\left(1+\frac{c_{4}}{\log^{2}n} \right)^{t+1}-1\right)\frac{1}{2}c_{5}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{ \beta_{0}}{\sqrt{n}}\] \[\leq c_{5}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n} }(t+1),\]

where we used basic inequalities \((1+x)^{a}\leq e^{ax}\) and \(e^{ax}-1\leq 2ax\) which hold if \(x\) is small and \(ax\) is small, respectively.

(D.9) at \((t+1)\)We decompose \(\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\) as

\[\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\] \[=(\bm{x}-\eta\nabla f(\bm{x}))-\left(\bm{x}^{(l)}-\eta\nabla f^{(l) }(\bm{x}^{(l)})\right)\] \[=(\bm{x}-\eta\nabla g(\bm{x}))-\left(\bm{x}^{(l)}-\eta\nabla g^{(l )}(\bm{x}^{(l)})\right)+\eta\left(\bm{M}^{\circ}\bm{x}-\bm{M}^{(l)}\bm{x}^{(l)}\right)\] \[=(\bm{x}-\eta\nabla g(\bm{x}))-\left(\bm{x}^{(l)}-\eta\nabla g(\bm {x}^{(l)})\right)-\eta\left(\nabla g(\bm{x}^{(l)})-\nabla g^{(l)}(\bm{x}^{(l)})\right)\] \[\quad+\eta(\bm{M}^{\circ}-\bm{M}^{(l)})\bm{x}+\eta\bm{M}^{(l)}( \bm{x}-\bm{x}^{(l)})\] \[=\int_{0}^{1}(\bm{I}-\eta\nabla^{2}g(\bm{x}^{(l)}(\tau))(\bm{x}- \bm{x}^{(l)})\,\mathrm{d}\tau-\eta\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}} \Big{(}\bm{x}^{(l)}\bm{x}^{(l)\top}\Big{)}-\mathcal{P}_{l}\Big{(}\bm{x}^{(l)} \bm{x}^{(l)\top}\Big{)}\right)\bm{x}^{(l)}\] \[\quad+\eta\bm{M}^{(l)}(\bm{x}-\bm{x}^{(l)})+\eta\left(\bm{M}^{ \circ}-\bm{M}^{(l)}\right)(\bm{x}-\bm{x}^{(l)})+\eta\left(\bm{M}^{\circ}-\bm{M }^{(l)}\right)\bm{x}^{(l)}\] \[=(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})(\bm{x}-\bm{x}^{(l)})-2 \eta\widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}(\bm{x}-\bm{x}^{(l)})\] \[\quad-\eta\int_{0}^{1}\left(\nabla^{2}g(\bm{x}^{(l)}(\tau))-\left( \|\widetilde{\bm{x}}\|_{2}^{2}\bm{I}+2\widetilde{\bm{x}}\widetilde{\bm{x}}^{ \top}\right)\right)(\bm{x}-\bm{x}^{(l)})\mathrm{d}\tau\] \[\quad-\eta\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}}\Big{(}\bm{x}^ {(l)}\bm{x}^{(l)\top}\Big{)}-\mathcal{P}_{l}\Big{(}\bm{x}^{(l)}\bm{x}^{(l) \top}\Big{)}\right)\bm{x}^{(l)}\] \[\quad+\eta\bm{M}^{(l)}(\bm{x}-\bm{x}^{(l)})+\eta\left(\bm{M}^{ \circ}-\bm{M}^{(l)}\right)(\bm{x}-\bm{x}^{(l)})+\eta\left(\bm{M}^{\circ}-\bm{M }^{(l)}\right)\bm{x}^{(l)},\]

where \(\bm{x}^{(l)}(\tau)=\bm{x}^{(l)}+\tau(\bm{x}-\bm{x}^{(l)})\). Then, we take inner product with \(\bm{u}^{(l)}\) on both sides.

\[\bm{u}^{(l)\top}\left(\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right) =\left(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2}\right)\bm{u}^{(l)\top }(\bm{x}-\bm{x}^{(l)})-2\eta\underbrace{\bm{u}^{(l)\top}\widetilde{\bm{x}} \widetilde{\bm{x}}^{\top}\left(\bm{x}-\bm{x}^{(l)}\right)}_{\text{\textcircled{1}}}\] \[\quad-\eta\underbrace{\int_{0}^{1}\bm{u}^{(l)\top}\left(\nabla^{ 2}g(\bm{x}(\tau))-\left(\|\widetilde{\bm{x}}\|_{2}^{2}\bm{I}+2\widetilde{\bm{ x}}\widetilde{\bm{x}}^{\top}\right)\right)(\bm{x}-\bm{x}^{(l)})\mathrm{d}\tau}_{ \text{\textcircled{2}}}\] \[\quad-\eta\underbrace{\bm{u}^{(l)\top}\left(\frac{1}{p}\mathcal{ P}_{\Omega_{l}}\Big{(}\bm{x}^{(l)}\bm{x}^{(l)\top}\Big{)}-\mathcal{P}_{l}\Big{(}\bm{x}^{(l) }\bm{x}^{(l)\top}\Big{)}\right)\bm{x}^{(l)}}_{\text{\textcircled{3}}}\] \[\quad+\eta\lambda^{(l)}\bm{u}^{(l)\top}(\bm{x}-\bm{x}^{(l)})+ \eta\underbrace{\bm{u}^{(l)\top}\left(\bm{M}^{\circ}-\bm{M}^{(l)}\right)(\bm{x }-\bm{x}^{(l)})}_{\text{\textcircled{4}}}\] \[\quad+\eta\underbrace{\bm{u}^{(l)\top}\left(\bm{M}^{\circ}-\bm{M }^{(l)}\right)\bm{x}^{(l)}}_{\text{\textcircled{5}}}\]

For the term \(\textcircled{1}\), we have

\[\left|\bm{u}^{(l)\top}\widetilde{\bm{x}}\right|\leq\left|\bm{u}^{(l)\top}\bm{x} ^{(0)}\right|+(1+\eta\lambda^{\star})^{t}\big{|}\bm{u}^{\star\top}\bm{x}^{(0)} \Big{|}\Big{|}\bm{u}^{(l)\top}\bm{u}^{\star}\Big{|}\lesssim\sqrt{\frac{\log n}{ n}}(1+\eta\lambda^{\star})^{t}\beta_{0}\]

by Lemma A.5, and thus,

\[\left|\textcircled{1}\right|\leq\left|\bm{u}^{(l)\top}\widetilde{ \bm{x}}\right||\widetilde{\bm{x}}\|_{2}\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2} \lesssim\sqrt{\frac{\log n}{n}}(1+\eta\lambda^{\star})^{t}\beta_{0}^{2} \Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2}\] \[\lesssim\mu\sqrt{\frac{\log^{3}n}{np}}\frac{\beta_{0}^{3}}{n}(1+ \eta\lambda^{\star})^{t}t\] \[\lesssim\lambda^{\star}\sqrt{\frac{\mu}{np}}\frac{\beta_{0}}{n}.\]The definition of Phase I was used to bound \((1+\eta\lambda^{\star})^{t}t\) in deriving the last line. We use (D.13) to get

\[\big{|}\widehat{\ref{eq:1}}\big{|} \lesssim\int_{0}^{1}\Big{\|}\nabla^{2}g(\bm{x}(\tau))-\Big{(}\| \widetilde{\bm{x}}\|_{2}^{2}\bm{I}+2\widetilde{\bm{x}}\widetilde{\bm{x}}^{ \top}\Big{)}\Big{\|}\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2}\Big{\|}\bm{u}^{(l )}\Big{\|}_{2}\mathrm{d}\tau\] \[\lesssim\sqrt{\frac{\mu^{3}\log^{7}n}{np}}\beta_{0}^{2}\cdot\mu \sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}}t\lesssim\lambda^{\star} \sqrt{\frac{\mu}{np\log^{15}n}}\frac{\beta_{0}}{n}.\]

We apply Lemma G.12 to \(\widehat{\ref{eq:1}}\) to yield

\[\big{|}\widehat{\ref{eq:1}}\big{|} \lesssim\Big{\|}\bm{x}^{(l)}\Big{\|}_{\infty}^{2}\sqrt{\frac{\log n }{p}}\left(\Big{\|}\bm{u}^{(l)}\Big{\|}_{2}\Big{\|}\bm{x}^{(l)}\Big{\|}_{ \infty}+\Big{\|}\bm{x}^{(l)}\Big{\|}_{2}\Big{\|}\bm{u}^{(l)}\Big{\|}_{\infty}\right)\] \[\lesssim\sqrt{\frac{\mu\log^{4}n}{np}}\frac{\beta_{0}^{3}}{n} \lesssim\lambda^{\star}\sqrt{\frac{1}{\mu^{3}n\log^{22}n}}\frac{\beta_{0}}{n}.\]

We divide \(\widehat{\ref{eq:1}}\) into two terms that are related to sampling and noise, respectively.

\[\widehat{\ref{eq:1}}=\bm{u}^{(l)\top}\left(\frac{1}{p}\mathcal{P}_{\Omega}( \bm{M}^{\star})-\mathcal{P}_{\Omega}^{(l)}(\bm{M}^{\star})\right)(\bm{x}-\bm{ x}^{(l)})+\bm{u}^{(l)\top}\left(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})-\bm{E}^{(l) }\right)(\bm{x}-\bm{x}^{(l)})\]

Then, Cauchy-Schwartz inequality is applied to yield

\[\big{|}\widehat{\ref{eq:1}}\big{|}\leq\bigg{\|}\left(\frac{1}{p}\mathcal{P}_ {\Omega}(\bm{M}^{\star})-\mathcal{P}_{\Omega}^{(l)}(\bm{M}^{\star})\right)\bm {u}^{(l)}\bigg{\|}_{2}\bigg{\|}\bm{x}-\bm{x}^{(l)}\bigg{\|}_{2}+\bigg{\|}\left( \frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})-\bm{E}^{(l)}\right)\bm{u}^{(l)}\bigg{\|} _{2}\bigg{\|}\bm{x}-\bm{x}^{(l)}\bigg{\|}_{2}.\]

Applying Lemmas G.11 and G.13 to the two terms, respectively, we get

\[\big{|}\widehat{\ref{eq:1}}\big{|}\lesssim\lambda^{\star}\sqrt{\frac{\mu^{3} \log^{2}n}{np}}\frac{1}{\sqrt{n}}\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2} \lesssim\lambda^{\star}\frac{\sqrt{\mu^{5}\log^{5}n}}{np}\frac{\beta_{0}}{n}.\]

For the term \(\widehat{\ref{eq:1}}\), we decompose it into two terms as for \(\widehat{\ref{eq:1}}\).

\[\big{|}\widehat{\ref{eq:1}}\big{|}\leq\bigg{|}\bm{u}^{(l)\top}\left(\frac{1} {p}\mathcal{P}_{\Omega}(\bm{M}^{\star})-\mathcal{P}_{\Omega}^{(l)}(\bm{M}^{ \star})\right)\bm{x}^{(l)}\bigg{|}+\bigg{|}\bm{u}^{(l)\top}\left(\frac{1}{p} \mathcal{P}_{\Omega}(\bm{E})-\bm{E}^{(l)}\right)\bm{x}^{(l)}\bigg{|}\]

Then, we apply Lemmas G.12 and G.14 to each term to obtain

\[\big{|}\widehat{\ref{eq:1}}\big{|}\lesssim\lambda^{\star}\sqrt{\frac{\mu^{3} \log^{3}n}{np}}\frac{\beta_{0}}{n}.\]

Combining all, there exists a universal constant \(c_{6}>0\) such that

\[\eta\left(2\big{|}\widehat{\ref{eq:1}}\big{|}+\big{|}\widehat{\ref{eq:1}} \big{|}+\big{|}\widehat{\ref{eq:1}}\big{|}+\big{|}\widehat{\ref{eq:1}}\big{|}+ \big{|}\widehat{\ref{eq:1}}\big{|}\right)\leq\frac{\eta\lambda^{\star}}{2}c_{6} \sqrt{\frac{\mu^{3}\log^{3}n}{np}}\frac{\beta_{0}}{n}\]

if \(n^{2}p\gtrsim\mu^{2}n\log^{3}n\), and there exists a universal constant \(c_{7}>0\) such that

\[\eta\lambda^{(l)}\leq\eta\lambda^{\star}+\frac{c_{7}}{\log^{2}n}\]

by (A.2) if \(n^{2}p\gtrsim\mu^{2}n\log^{5}n\). Finally, we have

\[\bigg{|}\bm{u}^{(l)\top}\left(\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)} \right)\bigg{|} \leq\left(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2}+\eta\lambda^{(l)} \right)\bigg{|}\bm{u}^{(l)\top}(\bm{x}-\bm{x}^{(l)})\bigg{|}+\frac{\eta\lambda^ {\star}}{2}c_{6}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\frac{\beta_{0}}{n}\] \[\leq\left(1+\eta\lambda^{\star}+\frac{c_{7}}{\log^{2}n}\right) \bigg{|}\bm{u}^{(l)\top}(\bm{x}-\bm{x}^{(l)})\bigg{|}+\frac{\eta\lambda^{\star} }{2}c_{6}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\frac{\beta_{0}}{n}.\]

An analysis on the recursive equation

\[x_{t+1}=\left(1+\eta\lambda^{\star}+\frac{c_{7}}{\log^{2}n}\right)x_{t}+\frac{ \eta\lambda^{\star}}{2}c_{6}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}\frac{\beta_{0}}{n}, \quad x_{0}=0\]

gives the bound

\[\bigg{|}\bm{u}^{(l)\top}\left(\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right)\bigg{|} \leq c_{6}\sqrt{\frac{\mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{\star})^{t+1}\frac{ \beta_{0}}{n}.\](D.10) at \((t+1)\)We decompose \((\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\) as

\[(\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}=\left(1-\eta\|\widetilde{\bm{x} }\|_{2}^{2}\right)(\bm{x}^{(l)}-\widetilde{\bm{x}})_{l}+\eta\lambda^{\star}\bm{ u}^{\star\top}(\bm{x}^{(l)}-\widetilde{\bm{x}})u_{l}^{\star}+\eta\left(\| \widetilde{\bm{x}}\|_{2}^{2}-\left\|\bm{x}^{(l)}\right\|_{2}^{2}\right)x_{l}^{ (l)},\]

and this implies

\[\left|(\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right| \leq\left(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2}\right)\left|(\bm{x }^{(l)}-\widetilde{\bm{x}})_{l}\right|\] \[+\eta\lambda^{\star}\left\|\bm{x}^{(l)}-\widetilde{\bm{x}}\right\| _{2}\|\bm{u}^{\star}\|_{\infty}+\eta\|\widetilde{\bm{x}}\|_{2}\Big{\|}\bm{x}^ {(l)}-\widetilde{\bm{x}}\Big{\|}_{2}\|\widetilde{\bm{x}}\|_{\infty}.\]

From (D.5) and (D.8), we have

\[\left\|\bm{x}^{(l)}-\widetilde{\bm{x}}\right\|_{2} \leq\left\|\bm{x}^{(l)}-\bm{x}\right\|_{2}+\left\|\bm{x}- \widetilde{\bm{x}}\right\|_{2}\leq 3c_{0}\mu\sqrt{\frac{\log n}{np}}\beta_{0}t.\]

If \(n\) is sufficiently large, we have

\[\eta\lambda^{\star}\left\|\bm{u}^{\star}\right\|_{\infty}+\eta\|\widetilde{\bm {x}}\|_{2}\|\widetilde{\bm{x}}\|_{\infty}\leq\sqrt{\frac{\mu\log n}{n}}\]

for all \(t\leq T_{1}\) because

\[\eta\lambda^{\star}\left\|\bm{u}^{\star}\right\|_{\infty}+\eta\|\widetilde{\bm {x}}\|_{2}\|\widetilde{\bm{x}}\|_{\infty}\lesssim\eta\lambda^{\star}\sqrt{ \frac{\mu}{n}}+\eta\sqrt{\frac{\log^{2}n}{n}}\beta_{0}^{2}\lesssim\sqrt{\frac{ \mu}{n}}.\]

Hence, we have

\[\left|(\bm{x}^{(s+1,l)}-\widetilde{\bm{x}}^{(s+1)})_{l}\right| \leq\left|(\bm{x}^{(s,l)}-\widetilde{\bm{x}}^{(s)})_{l}\right|+3c_{0}\sqrt{ \frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}}s\]

for all \(s\leq t\). Finally, we have

\[\left|(\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right| \leq 3c_{0}\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}\sum_{s=1}^{t}s\] \[\leq 3c_{0}\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(t+1)^{2}.\]

(D.5) at \((t+1)\)We can obtain this through the combination of (D.7) and Lemma 5.2.

\[\left\|\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\right\|_{2} \leq\left\|\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\right\|_{2}+ \left\|\widetilde{\bm{x}}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\right\|_{2}\] \[\leq c_{2}(3c_{0}+c_{5}+1)\frac{1}{\lambda^{\star}}\sqrt{\frac{ \mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{\star})^{t+1}\beta_{0}^{3}+c_{0}\mu \sqrt{\frac{\log n}{np}}\beta_{0}(t+1)\] \[\leq c_{2}(3c_{0}+c_{5}+1)\frac{1}{\lambda^{\star}}\sqrt{\frac{ \mu^{3}\log^{3}n}{np}}(1+\eta\lambda^{\star})^{T_{1}}\beta_{0}^{3}+c_{0}\mu \sqrt{\frac{\log n}{np}}\beta_{0}(t+1)\] \[\leq c_{1}c_{2}(3c_{0}+c_{5}+1)\mu\sqrt{\frac{1}{np\log^{2}n}} \beta_{0}+c_{0}\mu\sqrt{\frac{\log n}{np}}\beta_{0}(t+1)\] \[\leq 2c_{0}\mu\sqrt{\frac{\log n}{np}}\beta_{0}(t+1)\](D.6) at \((t+1)\)The \(l\)th component of \(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\) is bounded by

\[\left|(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right| \leq\left\|\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right\|_{\infty}+\left| (\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right|\] \[\leq\left\|\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right\|_{2}+\left|(\bm {x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right|\] \[\leq c_{5}\mu\sqrt{\frac{\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n} }(t+1)+3c_{0}\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{0}}{\sqrt{n}}(t+1 )^{2}\] \[\leq(3c_{0}+c_{5})\sqrt{\frac{\mu^{3}\log^{2}n}{np}}\frac{\beta_{ 0}}{\sqrt{n}}(t+1)^{2}.\]

At \(t=T_{1}\)Because \(T_{1}\lesssim\log n\), it is implied from Lemma D.2 that at \(t=T_{1}\), there exists a constant \(c_{7}>0\) such that

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \leq c_{7}\mu\sqrt{\frac{\log^{3}n}{np}}\beta_{0},\] (D.14) \[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty} \leq c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}},\] \[\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2} \leq c_{7}\mu\sqrt{\frac{\log^{4}n}{np}}\frac{\beta_{0}}{\sqrt{n}},\] \[\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l}\right| \leq c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}.\]

These bounds serve as a base case for the induction of the next part.

## Appendix E Phase II

This section is mostly devoted to the proof of Lemma E.1 which is a formal version of Lemma 6.1.

**Lemma E.1**.: _Suppose that (D.14) holds at \(t=T_{1}\) and the initialization point \(\bm{x}^{(0)}\) satisfies (B.1) to (B.6). Then, for all \(T_{1}<t\leq T_{2}\), we have_

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \leq 2c_{7}\mu\sqrt{\frac{\log^{3}n}{np}}\beta_{0}(1+\eta\lambda^{ \star})^{t-T_{1}},\] (E.1) \[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty} \leq c_{13}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(1+\eta\lambda^{\star})^{t-T_{1}},\] (E.2) \[\left\|\bm{x}^{(t)}-\bm{x}^{(t,l)}\right\|_{2} \leq 2c_{7}\mu\sqrt{\frac{\log^{5}n}{np}}\frac{\beta_{0}}{\sqrt{n}} (1+\eta\lambda^{\star})^{t-T_{1}},\] (E.3) \[\left|\left(\bm{x}^{(t,l)}-\widetilde{\bm{x}}^{(t)}\right)_{l} \right| \leq 3c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(1+\eta\lambda^{\star})^{t-T_{1}},\] (E.4)

_with high probability, where \(T_{2}\) is the largest \(t\) such that \(\widetilde{\beta}_{t}^{2}\leq\lambda^{\star}\left(1-\frac{1}{\log n}\right)\), and \(c_{13}>0\) is a constant._

Proof of Theorems 3.1 and 3.3We first explain how Theorems 3.1 and 3.3 are derived from Lemmas D.2 and E.1. We first focus on \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\). For \(t\leq T_{1}\), from Lemma D.2 and (C.3), we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\mu\sqrt{ \frac{\log^{3}n}{np}}\beta_{0}\lesssim\frac{1}{\log n}\beta_{0}\lesssim\frac{1} {\sqrt{\log n}}\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}\]provided that \(n^{2}p\gtrsim\mu^{2}n\log^{7}n\). For \(T_{1}<t\leq T_{2}\), from the definition of \(T_{1}\) and Lemma E.1, we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\sqrt{\frac{1}{ \log^{18}n}}\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{t}.\]

From the lower bound of Lemma C.1, for all \(t\leq T_{2}^{\prime}\), we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\sqrt{\frac{1} {\log^{18}n}}\left(1+\frac{(1+\eta\lambda^{\star})^{t}}{\sqrt{n}}\right)\beta_ {0}\lesssim\sqrt{\frac{1}{\log^{17}n}}\left\|\widetilde{\bm{x}}^{(t)}\right\|_ {2}\lesssim\frac{1}{\sqrt{\log n}}\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2}.\]

Now, for \(T_{2}^{\prime}<t\leq T_{2}\), we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2} \lesssim\sqrt{\frac{1}{\log^{18}n}}\left(1+\frac{(1+\eta\lambda^ {\star})^{T_{2}^{\prime}}}{\sqrt{n}}\right)\beta_{0}(1+\eta\lambda^{\star})^{ t-T_{2}^{\prime}}\] \[\lesssim\sqrt{\frac{1}{\log^{17}n}}\left\|\widetilde{\bm{x}}^{(T_ {2}^{\prime})}\right\|_{2}(1+\eta\lambda^{\star})^{t-T_{2}^{\prime}}.\]

For any \(T_{2}^{\prime}<t\leq T_{2}\), it is Lemma C.2 implied from Lemma C.2 that \((1+\eta\lambda^{\star})^{t-T_{2}^{\prime}}\leq\log^{6}n\), and we have \(\left\|\widetilde{\bm{x}}^{(T_{2}^{\prime})}\right\|_{2}\leq\left\|\widetilde{ \bm{x}}\right\|_{2}\). Hence, we get

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{2}\lesssim\sqrt{\frac{1 }{\log^{17}n}}\sqrt{\log^{12}n}\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2} \lesssim\frac{1}{\sqrt{\log n}}\left\|\widetilde{\bm{x}}^{(t)}\right\|_{2},\] (E.5)

and the proof for (11) of Theorem 3.3 is completed. If we combine this with (C.7), we are able to prove (4) of Theorem 3.1.

We move on to \(\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\). For \(t\leq T_{1}\), from Lemma D.2 and (C.4), we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\sqrt{ \frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{\sqrt{n}}\lesssim\frac{1}{\log n} \frac{\beta_{0}}{\sqrt{n}}\lesssim\frac{1}{\sqrt{\log n}}\left\|\widetilde{ \bm{x}}^{(t)}\right\|_{\infty}\]

provided that \(n^{2}p\gtrsim\mu^{3}n\log^{10}n\). For \(T_{1}<t\leq T_{2}^{\prime}\), from the definition of \(T_{1}\) and Lemma E.1, we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\sqrt{ \frac{1}{\log^{13}n}}\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{t}.\]

From the lower bound of Lemma C.1, for all \(t\leq T_{2}^{\prime}\), we have

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\sqrt{ \frac{1}{\log^{13}n}}\left(1+\frac{(1+\eta\lambda^{\star})^{t}}{\sqrt{n}} \right)\frac{\beta_{0}}{\sqrt{n}}\lesssim\sqrt{\frac{1}{\log^{12}n}}\left\| \widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\frac{1}{\sqrt{\log n}} \left\|\widetilde{\bm{x}}^{(t)}\right\|_{\infty}.\]

Now, for \(T_{2}^{\prime}<t\leq T_{2}\), if we do the same as before, we get

\[\left\|\bm{x}^{(t)}-\widetilde{\bm{x}}^{(t)}\right\|_{\infty}\lesssim\sqrt{ \frac{1}{\log^{13}n}}\sqrt{\log^{12}n}\left\|\widetilde{\bm{x}}^{(t)}\right\|_{ 2}\frac{1}{\sqrt{n}}\lesssim\frac{1}{\sqrt{\log n}}\left\|\widetilde{\bm{x}}^ {(t)}\right\|_{\infty},\]

and the proof for (12) of Theorem 3.3 is completed. If we combine this with (C.8), we are able to prove (5) of Theorem 3.1.

Going through a similar way with (22) and (24), we can complete the proof of Theorems 3.1 and 3.3.

Proof of Lemma E.1Before we start the proof, we define a function \(G\) as

\[G(\bm{x})=\frac{1}{4}\left\|\bm{x}\bm{x}^{\top}\right\|_{\mathrm{F}}^{2}.\]

The gradient of \(G\) satisfies

\[\nabla F(\bm{x})=\nabla G(\bm{x})-\bm{M}^{\star}\bm{x}.\]

Now, we assume that the hypotheses hold up to the \(t\)th iteration and show that they hold at the \((t+1)\)st iteration. For brevity, we drop the superscript \((t)\) from \(\bm{x}^{(t)}\), \(\bm{x}^{(t,l)}\), \(\widetilde{\bm{x}}^{(t)}\) and denote them as \(\bm{x}\), \(\bm{x}^{(l)}\), \(\widetilde{\bm{x}}\), respectively.

(E.1) at \((t+1)\)We decompose \(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\) as

\[\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\] \[=(\bm{x}-\eta\nabla f(\bm{x}))-(\widetilde{\bm{x}}-\eta\nabla F( \widetilde{\bm{x}}))\] \[=(\bm{x}-\eta\nabla g(\bm{x}))-(\widetilde{\bm{x}}-\eta\nabla G( \widetilde{\bm{x}}))+\eta\left(\bm{M}^{\diamond}\bm{x}-\bm{M}^{\star}\widetilde {\bm{x}}\right)\] \[=(\bm{x}-\eta\nabla g(\bm{x}))-(\widetilde{\bm{x}}-\eta\nabla g( \widetilde{\bm{x}}))-\eta\left(\nabla g(\widetilde{\bm{x}})-\nabla G( \widetilde{\bm{x}})\right)+\eta\bm{M}^{\star}(\bm{x}-\widetilde{\bm{x}})+\eta (\bm{M}^{\diamond}-\bm{M}^{\star})\bm{x}\] \[=\int_{0}^{1}(\bm{I}-\eta\nabla^{2}g(\bm{x}(\tau))(\bm{x}- \widetilde{\bm{x}})\,\mathrm{d}\tau-\eta\|\widetilde{\bm{x}}\|_{2}^{2}\left( \bm{I}_{\widetilde{\bm{x}}}-\bm{I}\right)\widetilde{\bm{x}}+\eta\bm{M}^{\star }(\bm{x}-\widetilde{\bm{x}})+\eta(\bm{M}^{\diamond}-\bm{M}^{\star})\bm{x}\] \[=\underbrace{\left((1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})\bm{I}-2 \eta\widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}+\eta\bm{M}^{\star}\right)(\bm {x}-\widetilde{\bm{x}})}_{\mbox{\textcircled{1}}}-\eta\underbrace{\int_{0}^{1} \left(\nabla^{2}g(\bm{x}(\tau))-\left(\|\widetilde{\bm{x}}\|_{2}^{2}\bm{I}+2 \widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}\right)\right)(\bm{x}-\widetilde{\bm {x}})\mathrm{d}\tau}_{\mbox{\textcircled{2}}}\] \[\quad-\eta\underbrace{\|\widetilde{\bm{x}}\|_{2}^{2}\left(\bm{I}_ {\widetilde{\bm{x}}}-\bm{I}\right)\widetilde{\bm{x}}}_{\mbox{\textcircled{3}}}+ \eta\underbrace{\left(\bm{M}^{\diamond}-\bm{M}^{\star}\right)\bm{x}}_{\mbox{ \textcircled{4}}},\]

where \(\bm{x}(\tau)=\widetilde{\bm{x}}+\tau(\bm{x}-\widetilde{\bm{x}})\). For the term \(\mbox{\textcircled{1}}\), we require a bound on

\[\left\|(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})\bm{I}-2\eta\widetilde{\bm{x}} \widetilde{\bm{x}}^{\top}+\eta\bm{M}^{\star}\right\|\!.\]

If we write \(\widetilde{\bm{x}}\) as \(\widetilde{\alpha}_{t}\bm{u}^{\star}+\widetilde{\bm{x}}_{\perp}\), we have \(\widetilde{\alpha}_{t}^{2}\leq\lambda^{\star}\) and \(\|\widetilde{\bm{x}}_{\perp}\|_{2}\lesssim\beta_{0}\). Then, we have

\[\left\|(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})\bm{I}-2\eta\widetilde {\bm{x}}\widetilde{\bm{x}}^{\top}+\eta\bm{M}^{\star}\right\|\] \[=\left\|(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})\bm{I}+\eta(\lambda ^{\star}-2\widetilde{\alpha}_{t}^{2})\bm{u}^{\star}\bm{u}^{\star}-2\eta( \widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}-\widetilde{\alpha}_{t}^{2}\bm{u}^ {\star}\bm{u}^{\star})\right\|\] \[\leq\left\|(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2})\bm{I}+\eta( \lambda^{\star}-2\widetilde{\alpha}_{t}^{2})\bm{u}^{\star}\bm{u}^{\star}\right\| +2\eta\|\widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}-\widetilde{\alpha}_{t}^{2} \bm{u}^{\star}\bm{u}^{\star}\|\] \[\leq(1+\eta\lambda^{\star})+2\eta(2\alpha_{t}\|\widetilde{\bm{x}} _{\perp}\|_{2}+\|\widetilde{\bm{x}}_{\perp}\|_{2}^{2})\] \[\leq 1+\eta\lambda^{\star}+\frac{c_{8}}{\log^{2}n}\]

for some universal constant \(c_{8}>0\). This implies the desired bound

\[\left\|(\mbox{\textcircled{1}})\right\|_{2}\leq\left(1+\eta\lambda^{\star}+ \frac{c_{8}}{\log^{2}n}\right)\left\|\bm{x}-\widetilde{\bm{x}}\right\|_{2}.\]

For all \(0\leq\tau\leq 1\), we have \(\left\|\bm{x}(\tau)-\widetilde{\bm{x}}\right\|_{\infty}\leq\left\|\bm{x}- \widetilde{\bm{x}}\right\|_{\infty}\), and the induction hypothesis (E.2) gives

\[\left\|\bm{x}-\widetilde{\bm{x}}\right\|_{\infty}\lesssim\sqrt{\frac{1}{\mu\log ^{13}n}}(1+\eta\lambda^{\star})^{T_{2}}\frac{\beta_{0}}{n}.\]

Hence, by Lemma G.10, we have

\[\left\|\nabla^{2}g(\bm{x}(\tau))-\left(\left\|\widetilde{\bm{x}} \right\|_{2}^{2}\bm{I}+2\widetilde{\bm{x}}\widetilde{\bm{x}}^{\top}\right)\right\| \lesssim n\|\bm{x}-\widetilde{\bm{x}}\|_{\infty}(\left\|\bm{x} \right\|_{\infty}+\left\|\widetilde{\bm{x}}\right\|_{\infty})+\sqrt{\frac{n \log n}{p}}\|\widetilde{\bm{x}}\|_{\infty}^{2}\] \[\lesssim\left(\sqrt{\frac{1}{\log^{12}n}}+\sqrt{\frac{\mu^{2}\log^ {3}n}{np}}\right)(1+\eta\lambda^{\star})^{2T_{2}}\frac{\beta_{0}^{2}}{n}\] \[\lesssim\lambda^{\star}\sqrt{\frac{1}{\log^{12}n}}\]

if \(n^{2}p\gtrsim\mu^{2}n\log^{15}n\) because \(\left\|\widetilde{\bm{x}}\right\|_{\infty}\lesssim\sqrt{\mu\log n}(1+\eta \lambda^{\star})^{T_{2}}\frac{\beta_{0}}{n}\) and \((1+\eta\lambda^{\star})^{T_{2}}\lesssim\sqrt{\lambda^{\star}}\frac{\sqrt{n}}{ \beta_{0}}\). This gives

\[\left\|(\mbox{\textcircled{2}})\right\|_{2}\lesssim\lambda^{\star}\sqrt{\frac{1}{ \log^{12}n}}\|\bm{x}-\widetilde{\bm{x}}\|_{2}.\] (E.6)For the term 3, we use Lemma G.8 to obtain

\[\left\|\!\left\langle\!\left\langle\!\left\langle\!\left\langle 3\right\rangle\! \right\rangle\!\right\rangle\!\right\|_{2}\lesssim\lambda^{\star}\sqrt{\frac{\mu \log n}{np}}\|\widetilde{\boldsymbol{x}}\|_{2}.\]

Lastly, the term 4 is bounded with

Combining all, there exists a universal constant \(c_{9}>0\) such that

\[\left\|\!\left\langle\!\left\langle\!\left\langle \!\left\langle 3\right\rangle\!\right\rangle\!\right\rangle\!\right\|_{2}+\left\| \!\left\langle\!\left\langle 2\right\rangle\!\right\rangle\!\right\|_{2} \leq\left(1+\eta\lambda^{\star}+\frac{c_{9}}{\log^{2}n}\right) \|\boldsymbol{x}-\widetilde{\boldsymbol{x}}\|_{2},\] \[\eta\left(\left\|\!\left\langle\!\left\langle \!\left\langle 3\right\rangle\!\right\rangle\!\right\rangle\!\right\|_{2}+\left\| \!\left\langle 4\right\rangle\!\right\|_{2} \right) \leq c_{9}\mu\sqrt{\frac{\log n}{np}}\|\widetilde{\boldsymbol{x }}\|_{2}.\]

Because \(\left\|\boldsymbol{x}^{(T_{1})}\right\|_{2}\lesssim\sqrt{\log n}\beta_{0}\) by (C.3) and \(\left\|\widetilde{\boldsymbol{x}}^{(t)}\right\|_{2}\) can grow at a rate at most \((1+\eta\lambda^{\star})\), there exists a universal constant \(c_{10}>0\) such that

\[c_{8}\mu\sqrt{\frac{\log n}{np}}\left\|\widetilde{\boldsymbol{x}}^{(t)} \right\|_{2}\leq c_{10}\mu\sqrt{\frac{\log^{2}n}{np}}(1+\eta\lambda^{\star})^ {t-T_{1}}\beta_{0}.\] (E.7)

Hence, for all \(T_{1}\leq s\leq t\), we have

\[\left\|\boldsymbol{x}^{(s+1)}-\widetilde{\boldsymbol{x}}^{(s+1)}\right\|_{2} \leq\left(1+\eta\lambda^{\star}+\frac{c_{9}}{\log^{2}n}\right)\left\| \boldsymbol{x}^{(s)}-\widetilde{\boldsymbol{x}}^{(s)}\right\|_{2}+c_{10}\mu \sqrt{\frac{\log^{2}n}{np}}(1+\eta\lambda^{\star})^{t-T_{1}}\beta_{0}.\]

An analysis on the recursive equation

\[x_{s+1}=\left(1+\eta\lambda^{\star}+\frac{c_{9}}{\log^{2}n}\right)x_{s}+c_{10 }\mu\sqrt{\frac{\log^{2}n}{np}}(1+\eta\lambda^{\star})^{t-T_{1}}\beta_{0},\quad x _{T_{1}}=c_{7}\mu\sqrt{\frac{\log^{3}n}{np}}\beta_{0}\]

proves that

\[\left\|\boldsymbol{x}^{(t+1)}-\widetilde{\boldsymbol{x}}^{(t+1)}\right\|_{2} \leq 2c_{7}\mu\sqrt{\frac{\log^{3}n}{np}}\beta_{0}(1+\eta\lambda^{\star})^{ t+1-T_{1}}.\]

**(E.3) at \((t+1)\)** Similar to the proof of (D.8), we have the decomposition

\[\boldsymbol{x}^{(t+1)}-\boldsymbol{x}^{(t+1,l)} =\underbrace{(1-\eta\|\widetilde{\boldsymbol{x}}\|_{2}^{2}-2\eta \widetilde{\boldsymbol{x}}\widetilde{\boldsymbol{x}}^{\top})(\boldsymbol{x}- \boldsymbol{x}^{(l)})}_{\mbox{\scriptsize$\left\langle\!\left\langle\! \left\langle 2\right\rangle\!\right\rangle\!\right\rangle$}}\] \[\quad-\eta\underbrace{\frac{1}{p}\mathcal{P}_{\Omega_{l}}\! \left(\boldsymbol{x}^{(l)}\boldsymbol{x}^{(l)\top}\right)-\mathcal{P}_{l} \!\left(\boldsymbol{x}^{(l)}\boldsymbol{x}^{(l)\top}\right)\right)\boldsymbol {x}^{(l)}}_{\mbox{\scriptsize$\left\langle\!\left\langle\!\left\langle \!\left\langle 2\right\rangle\!\right\rangle\!\right\rangle$}}\] \[\quad+\eta\boldsymbol{M}^{\star}(\boldsymbol{x}-\boldsymbol{x}^ {(l)})+\eta\underbrace{(\boldsymbol{M}^{\diamond}-\boldsymbol{M}^{\star})( \boldsymbol{x}-\boldsymbol{x}^{(l)})}_{\mbox{\scriptsize$\left\langle\!\!\left\langle \!\left\langle \!\left\langle 2\right\rangle\!\right\rangle\!\right\rangle$}}\] \[\quad+\eta\underbrace{\left(\frac{1}{p}\mathcal{P}_{\Omega}( \boldsymbol{M}^{\star})-\mathcal{P}_{\Omega}^{(l)}(\boldsymbol{M}^{\star}) \right)\boldsymbol{x}^{(l)}}_{\mbox{\scriptsize$\left\langle\!\left\langle \!\left\langle \!\left\langle \!\left\langle 2\right\rangle\!\right\rangle\!\right\rangle\!\right\rangle$}}+\eta \underbrace{\left(\frac{1}{p}\mathcal{P}_{\Omega}(\boldsymbol{E})-\boldsymbol{E} ^{(l)}\right)\boldsymbol{x}^{(l)}}_{\mbox{\scriptsize$\left\langle\!\left\langle \!\left\langle\!\left\langle \!\left\langle 2\right\rangle\!\right\rangle\!\right\rangle\!\right\rangle$}},\]where \(\bm{x}^{(l)}(\tau)=\bm{x}^{(l)}+\tau(\bm{x}-\bm{x}^{(l)})\). Both of the terms 1 and 2 can be bounded similar to 1 and 2 of \(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\) as

\[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\leq\left(1+\eta\lambda^{ \star}+\frac{c_{11}}{\log^{2}n}\right)\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2},\] \[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\lesssim\lambda^{\star} \sqrt{\frac{1}{\log^{12}n}}\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2}\]

for some universal constant \(c_{11}>0\). For the terms 3 and 5, we use Lemma G.11 to obtain

\[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\lesssim\sqrt{\frac{\log n }{p}}\Big{\|}\bm{x}^{(l)}\Big{\|}_{2}\Big{\|}\bm{x}^{(l)}\Big{\|}_{\infty}^{2 }\lesssim\lambda^{\star}\mu\sqrt{\frac{\log n}{np}}\frac{1}{\sqrt{n}}\| \widetilde{\bm{x}}\|_{2},\] \[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\lesssim\lambda^{\star} \mu\sqrt{\frac{\log n}{np}}\frac{1}{\sqrt{n}}\|\widetilde{\bm{x}}\|_{2},\]

and use Lemma G.13 to obtain

\[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\lesssim\lambda^{\star} \mu\sqrt{\frac{\log^{2}n}{np}}\frac{1}{\sqrt{n}}\|\widetilde{\bm{x}}\|_{2}.\]

From Lemmas A.1 and A.3, the term 4 is bounded as

\[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\leq\|\bm{M}^{\circ}-\bm{M}^{\star} \|\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2}\lesssim\lambda^{\star}\mu\sqrt{ \frac{\log n}{np}}\Big{\|}\bm{x}-\bm{x}^{(l)}\Big{\|}_{2}.\]

Combining all with (E.7), there exists a universal constant \(c_{12}>0\) such that

\[\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}+\big{\|}\widehat{\ref{eq:1}}\big{\|} _{2}+\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\leq\left(1+\eta\lambda^{\star}+ \frac{c_{12}}{\log^{2}n}\right)\|\bm{x}-\widetilde{\bm{x}}\|_{2},\]

\[\eta\left(\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}+\big{\|}\widehat{\ref{eq:1 }}\big{\|}_{2}+\big{\|}\widehat{\ref{eq:1}}\big{\|}_{2}\right)\leq c_{12}\mu \sqrt{\frac{\log^{3}n}{np}}\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{ t-T_{1}}.\]

Hence, we have

\[\Big{\|}\bm{x}^{(s+1)}-\bm{x}^{(s+1,l)}\Big{\|}_{2}\leq\left(1+ \eta\lambda^{\star}+\frac{c_{12}}{\log^{2}n}\right)\Big{\|}\bm{x}^{(s)}-\bm{x }^{(s,l)}\Big{\|}_{2}+c_{12}\mu\sqrt{\frac{\log^{3}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(1+\eta\lambda^{\star})^{t-T_{1}},\]

for all \(T_{1}\leq s\leq t\). An analysis on the recursive equation

\[x_{s+1}=\left(1+\eta\lambda^{\star}+\frac{c_{12}}{\log^{2}n} \right)x_{s}+c_{12}\mu\sqrt{\frac{\log^{3}n}{np}}\frac{\beta_{0}}{\sqrt{n}}(1+ \eta\lambda^{\star})^{t-T_{1}},\quad x_{T_{1}}=c_{7}\mu\sqrt{\frac{\log^{4}n} {np}}\frac{\beta_{0}}{\sqrt{n}}\]

proves that

\[\Big{\|}\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\Big{\|}_{2} \leq 2\left(c_{12}\mu\sqrt{\frac{\log^{3}n}{np}}(t+1-T_{1})+c_{7} \mu\sqrt{\frac{\log^{4}n}{np}}\right)\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^ {\star})^{t+1-T_{1}}\] \[\leq c_{13}\mu\sqrt{\frac{\log^{5}n}{np}}\frac{\beta_{0}}{\sqrt{n} }(1+\eta\lambda^{\star})^{t+1-T_{1}}\]

holds for some universal constant \(c_{13}>0\) because \(T_{2}-T_{1}\lesssim\log n\).

(E.4) at (\(t+1\))We use the same bound

\[\Big{|}(\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\Big{|} \leq\left(1-\eta\|\widetilde{\bm{x}}\|_{2}^{2}\right)\Big{|}(\bm{x}^{(l)}- \widetilde{\bm{x}})_{l}\Big{|}+\eta(\lambda^{\star}\|\bm{u}^{\star}\|_{\infty} +\|\widetilde{\bm{x}}\|_{2}\|\widetilde{\bm{x}}\|_{\infty})\Big{\|}\bm{x}^{(l)}- \widetilde{\bm{x}}\Big{\|}_{2}.\]that was used in the proof of (D.10). From (E.1) and (E.3), we have

\[\left\|\bm{x}^{(l)}-\widetilde{\bm{x}}\right\|_{2}\leq\left\|\bm{x}^{(l)}-\bm{x} \right\|_{2}+\left\|\bm{x}-\widetilde{\bm{x}}\right\|_{2}\leq 3c_{7}\mu\sqrt{\frac{ \log^{5}n}{np}}\beta_{0}(1+\eta\lambda^{\star})^{t-T_{1}}.\]

Combined with the fact that \(\lambda^{\star}\left\|\bm{u}^{\star}\right\|_{\infty}+\left\|\widetilde{\bm{x} }\right\|_{2}\left\|\widetilde{\bm{x}}\right\|_{\infty}\leq 3\lambda^{\star} \sqrt{\frac{n}{n}}\), there exists a universal constant \(c_{14}>0\) such that

\[\eta(\lambda^{\star}\left\|\bm{u}^{\star}\right\|_{\infty}+\left\|\widetilde{ \bm{x}}\right\|_{2}\left\|\widetilde{\bm{x}}\right\|_{\infty})\left\|\bm{x}^{( l)}-\widetilde{\bm{x}}\right\|_{2}\leq c_{14}\sqrt{\frac{\mu^{3}\log^{5}n}{np}} \frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{t-T_{1}}.\]

Hence, for all \(T_{1}\leq s\leq t\), we have

\[\left|(\bm{x}^{(s+1,l)}-\widetilde{\bm{x}}^{(s+1)})_{l}\right|\leq\left|(\bm{ x}^{(s,l)}-\widetilde{\bm{x}}^{(s)})_{l}\right|+c_{14}\sqrt{\frac{\mu^{3}\log^{5}n}{ np}}\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{t-T_{1}},\]

and this implies

\[\left|(\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right| \leq c_{14}\sqrt{\frac{\mu^{3}\log^{5}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}\sum_{s=T_{1}}^{t}(1+\eta\lambda^{\star})^{s-T_{1}}+c_{7}\sqrt{\frac {\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{\sqrt{n}}\] \[\leq 2c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(1+\eta\lambda^{\star})^{t+1-T_{1}}.\]

(E.2) at \((t+1)\)The \(l\)th component of \(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)}\) is bounded by

\[\left|(\bm{x}^{(t+1)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right| \leq\left\|\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right\|_{\infty}+\left| (\bm{x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right|\] \[\leq\left\|\bm{x}^{(t+1)}-\bm{x}^{(t+1,l)}\right\|_{2}+\left|(\bm {x}^{(t+1,l)}-\widetilde{\bm{x}}^{(t+1)})_{l}\right|\] \[\leq c_{13}\mu\sqrt{\frac{\log^{5}n}{np}}\frac{\beta_{0}}{\sqrt{n }}(1+\eta\lambda^{\star})^{t+1-T_{1}}+2c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np} }\frac{\beta_{0}}{\sqrt{n}}(1+\eta\lambda^{\star})^{t+1-T_{1}}\] \[\leq 3c_{7}\sqrt{\frac{\mu^{3}\log^{8}n}{np}}\frac{\beta_{0}}{ \sqrt{n}}(1+\eta\lambda^{\star})^{t+1-T_{1}}.\]

## Appendix F Fixed Initialization Size

In Section 3, we claimed that the estimation error is improved to \(\frac{1}{\sqrt{np}}+\frac{\sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\) if the initialization size is fixed to \(n^{-1/4}\) regardless of the sample complexity. We briefly discuss how the proofs should change in such a case. For clear presentation, \(\mu\) and \(\log n\) factors are ignored in this section.

For every bound of Phase I (Lemmas 5.1 to 5.5), \(\frac{1}{\sqrt{np}}\) is changed to \(\frac{1}{\sqrt{np}}+\frac{\sigma}{\lambda^{\star}}\sqrt{\frac{n}{p}}\), while allowing \(\sigma\) to be as large as \(\frac{\lambda^{\star}\mu}{n}\sqrt{np}\). More importantly, the definition of Phase I is changed to be the largest \(t\) such that \((1+\eta\lambda^{\star})^{t}\leq\sqrt{n}\), so it is lengthened by \(\sqrt{np}\) times than before. In the original proof, the estimation error of \(\frac{1}{\sqrt{np}}\) obtained at the end of Phase I was increased to \(\frac{1}{\mathrm{poly}(\log n)}\) during the first part of Phase II. However, if \((1+\eta\lambda^{\star})^{t}\) equals \(\sqrt{n}\) at the end of Phase I, we do not have such a part in Phase II, and the estimation error obtained at the end of Phase I is maintained through Phase II.

## Appendix G Technical Lemmas

We introduce some technical lemmas in this section. Most of them are the results of classical concentration inequalities.

**Theorem G.1** (Matrix Bernstein Inequality).: _Let \(\{\bm{X}_{i}\}\) be \(n\times n\) independent symmetric random matrices. Assume that each random matrix satisfies \(\mathbb{E}\,\bm{X}_{i}=\bm{0}\) and \(\left\|\bm{X}_{i}\right\|\leq L\) almost surely. Then, for all \(\tau\geq 0\), we have_

\[\mathbb{P}\left[\left\|\sum_{i}\bm{X}_{i}\right\|\geq\tau\right]\leq n\exp \left(\frac{-\tau^{2}/2}{V+L\tau/3}\right),\]

_where \(V=\left\|\sum_{i}\mathbb{E}(\bm{X}_{i}^{2})\right\|\)._

**Corollary G.2** (Matrix Bernstein Inequality).: _Let \(\{\bm{X}_{i}\}\) be \(n\times n\) independent symmetric random matrices. Assume that each random matrix satisfies \(\mathbb{E}\,\bm{X}_{i}=\bm{0}\) and \(\left\|\bm{X}_{i}\right\|\leq L\) almost surely. Then, with high probability, we have_

\[\left\|\sum_{i}\bm{X}_{i}\right\|\lesssim\sqrt{V\log n}+L\log n,\]

_where \(V=\left\|\sum_{i}\mathbb{E}(\bm{X}_{i}^{2})\right\|\)._

**Lemma G.3**.: _For any fixed matrix \(\bm{M}\in\mathbb{R}^{n\times n}\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M})-\bm{M}\right\|\lesssim\sqrt{ \frac{n\log n}{p}}\left\|\bm{M}\right\|_{\infty}+\frac{\log n}{p}\left\|\bm{M} \right\|_{\infty}\]

_with high probability._

Proof.: We decompose the matrix into the sum of independent symmetric matrices.

\[\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M})-\bm{M}=\sum_{i<j}\left(\frac{\delta_{ ij}}{p}-1\right)M_{ij}(\bm{e}_{i}\bm{e}_{j}^{\top}+\bm{e}_{j}\bm{e}_{i}^{\top})+ \sum_{i}\left(\frac{\delta_{ii}}{p}-1\right)M_{ii}\bm{e}_{i}\bm{e}_{i}^{\top}\]

We calculate \(L\) and \(V\) of Corollary G.2. We have \(L\leq\frac{1}{p}\left\|\bm{M}\right\|_{\infty}\) because

\[\left\|\left(\frac{\delta_{ij}}{p}-1\right)M_{ij}(\bm{e}_{i}\bm{e }_{j}^{\top}+\bm{e}_{j}\bm{e}_{i}^{\top})\right\| \leq\frac{1}{p}\left\|\bm{M}\right\|_{\infty},\] \[\left\|\left(\frac{\delta_{ii}}{p}-1\right)M_{ii}\bm{e}_{i}\bm{e} _{i}^{\top}\right\| \leq\frac{1}{p}\left\|\bm{M}\right\|_{\infty}.\]

We also have the following bound on \(V\).

\[V=\frac{1-p}{p}\left\|\sum_{i,j}{M^{*}_{ij}}^{2}\bm{e}_{i}\bm{e}_{i}^{\top} \right\|\leq\frac{n}{p}\left\|\bm{M}\right\|_{\infty}^{2}\]

Hence, Corollary G.2 implies the desired result. 

We can prove Lemma A.1 by applying Lemma G.3 to \(\bm{M}^{*}\) and using \(\left\|\bm{M}^{*}\right\|_{\infty}=\lambda^{*}\frac{\mu}{n}\).

We introduce classical Bernstein inequality and the results obtained from it.

**Theorem G.4** (Bernstein Inequality).: _Let \(\{X_{i}\}\) be independent random variables. Assume that each random variable satisfies \(\mathbb{E}\,X_{i}=0\) and \(\left|X_{i}\right|\leq L\) almost surely. Then, for all \(\tau\geq 0\), we have_

\[\mathbb{P}\left[\left|\sum_{i}X_{i}\right|\geq\tau\right]\leq 2\exp\left(\frac{- \tau^{2}/2}{V+L\tau/3}\right),\]

_where \(V=\sum_{i}\mathbb{E}\big{[}X_{i}^{2}\big{]}\)._

**Corollary G.5** (Bernstein Inequality).: _Let \(\{X_{i}\}\) be independent random variables. Assume that each random variable satisfies \(\mathbb{E}\,X_{i}=0\) and \(\left|X_{i}\right|\leq L\) almost surely. Then, with high probability, we have_

\[\left|\sum_{i}X_{i}\right|\lesssim\sqrt{V\log n}+L\log n,\]

_where \(V=\sum_{i}\mathbb{E}\big{[}X_{i}^{2}\big{]}\)._

**Lemma G.6**.: _Let and be independent Bernoulli random variables with expectation \(p\). Then, for any fixed vector \(\bm{a}\), we have_

\[\left|\sum_{i}\left(\frac{X_{i}}{p}-1\right)a_{i}\right|\lesssim\sqrt{\frac{\log n }{p}}\|\bm{a}\|_{2}+\frac{\log n}{p}\|\bm{a}\|_{\infty}\]

_with high probability._

Proof.: We can apply Corollary G.5 with and. 

**Lemma G.7**.: _If \(n^{2}p\gtrsim\mu n\log n\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})\right\|_{2,\infty} \lesssim\lambda^{\star}\sqrt{\frac{\mu}{np}}\]

_with high probability._

Proof.: Let us consider \(\ell_{2}\)-norm of the \(i\)th row of \(\bm{M}^{\circ}\).

\[\left\|\left(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star}) \right)_{i\star}\right\|_{2}^{2} =\lambda^{\star 2}u_{i}^{\star 2}\sum_{j}\frac{1}{p^{2}}\delta_{ij}u_{j }^{\star 2}\] \[\leq\frac{1}{p}{\lambda^{\star}}^{2}\|\bm{u}^{\star}\|_{\infty} ^{2}\,\left(\|\bm{u}^{\star}\|_{2}^{2}+\left(\sum_{j}\frac{1}{p}\delta_{ij}u_ {j}^{\star 2}-\|\bm{u}^{\star}\|_{2}^{2}\right)\right)\] \[\lesssim\frac{\lambda^{\star 2}\mu}{np}\left(1+\sqrt{\frac{\log n }{np}}\right)\lesssim\frac{\lambda^{\star 2}\mu}{np}\]

The third line follows from Lemma G.6. 

Proof of Lemma a.2.: The spectral norm of a symmetric matrix that has nonzero entries only on the \(l\)th row/column is bounded by twice of the norm of its \(l\)th row. Hence,

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})-\mathcal{ P}_{\Omega}^{(l)}(\bm{M}^{\star})\right\| \leq 2\left\|\left(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})- \mathcal{P}_{\Omega}^{(l)}(\bm{M}^{\star})\right)_{l\star}\right\|_{2}=2 \left\|\left(\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star})-\bm{M}^{\star} \right)_{l\star}\right\|_{2}\] \[\lesssim\left\|\frac{1}{p}\mathcal{P}_{\Omega}(\bm{M}^{\star}) \right\|_{2,\infty}+\|\bm{M}^{\star}\|_{2,\infty}\lesssim\lambda^{\star}\sqrt{ \frac{\mu}{np}},\]

where the last inequality follows from Lemma G.7. 

**Lemma G.8**.: _Let \(\bm{y}\) be a vector that is independent from the sampling. Then, if \(n^{2}p\gtrsim n\log n\), we have_

\[\max_{i\in[n]}\left\|\bm{x}\right\|_{2,i}^{2}-\left\|\bm{y}\right\|_{2}^{2} \lesssim n\|\bm{x}-\bm{y}\|_{\infty}(\left\|\bm{x}\right\|_{\infty}+\left\| \bm{y}\right\|_{\infty})+\sqrt{\frac{\log n}{p}}\|\bm{y}\|_{2}\|\bm{y}\|_{ \infty}+\frac{\log n}{p}\|\bm{y}\|_{\infty}^{2}\]

_with very high probability._

Proof.: Let us fix \(i\) and decompose the difference as

\[\left\|\bm{x}\right\|_{2,i}^{2}-\left\|\bm{y}\right\|_{2}^{2}=\frac{1}{p}\sum _{j=1}^{n}\delta_{ij}(x_{j}^{2}-y_{j}^{2})+\sum_{j=1}^{n}\left(\frac{\delta_{ ij}}{p}-1\right)y_{j}^{2}.\]

The first term is bounded as

\[\left|\frac{1}{p}\sum_{j=1}^{n}\delta_{ij}(x_{j}^{2}-y_{j}^{2})\right|\leq\left\| \bm{x}-\bm{y}\right\|_{\infty}\left\|\bm{x}+\bm{y}\right\|_{\infty}\frac{1}{p} \sum_{j=1}^{n}\delta_{ij}\lesssim n\left\|\bm{x}-\bm{y}\right\|_{\infty}\left\| \bm{x}+\bm{y}\right\|_{\infty},\]and the second term is bounded as

\[\left|\sum_{j=1}^{n}\left(\frac{\delta_{ij}}{p}-1\right)y_{j}^{2}\right|\lesssim \sqrt{\frac{\log n}{p}}\|\bm{y}\|_{2}\|\bm{y}\|_{\infty}+\frac{\log n}{p}\|\bm{y }\|_{\infty}^{2}\]

by Lemma G.6. 

**Lemma G.9**.: _Let \(\bm{y}\) be a vector that is independent from the sampling. Then, if \(n^{2}p\gtrsim n\log n\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{x}\bm{x}^{\top}\big{)}-\bm{y }\bm{y}^{\top}\right\|\lesssim n\|\bm{x}-\bm{y}\|_{\infty}(\left\|\bm{x}\right\| _{\infty}+\left\|\bm{y}\right\|_{\infty})+\sqrt{\frac{n\log n}{p}}\|\bm{y}\|_{ \infty}^{2}\]

_with very high probability._

Proof.: We have the following sequence of inequalities

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{x}\bm{x}^{\top} \big{)}-\bm{y}\bm{y}^{\top}\right\| \leq\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{x}\bm{x}^{ \top}\big{)}-\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{y}\bm{y}^{\top}\big{)} \right\|+\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{y}\bm{y}^{\top} \big{)}-\bm{y}\bm{y}^{\top}\right\|\] \[\lesssim\left\|\bm{x}\bm{x}^{\top}-\bm{y}\bm{y}^{\top}\right\|_{ \infty}\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{1}\bm{1}^{\top}\big{)} \right\|+\left\|\frac{1}{p}\mathcal{P}_{\Omega}\big{(}\bm{y}\bm{y}^{\top} \big{)}-\bm{y}\bm{y}^{\top}\right\|\] \[\lesssim n\|\bm{x}-\bm{y}\|_{\infty}(\left\|\bm{x}\right\|_{\infty }+\left\|\bm{y}\right\|_{\infty})+\sqrt{\frac{n\log n}{p}}\|\bm{y}\|_{\infty} ^{2},\]

where the second line is derived from a basic inequality \(\|\bm{A}\|\leq\left\|\left\|\bm{A}\right\|\right\|\) that holds for any matrix \(\bm{A}\), and the last line follows by applying Lemma G.3 to \(\bm{1}\bm{1}^{\top}\) and \(\bm{y}\bm{y}^{\top}\). 

**Lemma G.10**.: _Let \(\bm{y}\) be a vector that is independent from the sampling. Then, if \(n^{2}p\gtrsim n\log n\), we have_

\[\left\|\nabla^{2}g(\bm{x})-\left(\|\bm{y}\|_{2}^{2}\bm{I}+2\bm{y }\bm{y}^{\top}\right)\right\| \lesssim n\|\bm{x}-\bm{y}\|_{\infty}(\left\|\bm{x}\right\|_{ \infty}+\left\|\bm{y}\right\|_{\infty})\] \[\quad+\sqrt{\frac{\log n}{p}}\|\bm{y}\|_{2}\|\bm{y}\|_{\infty}+ \frac{\log n}{p}\|\bm{y}\|_{\infty}^{2}+\sqrt{\frac{n\log n}{p}}\|\bm{y}\|_{ \infty}^{2}\]

Proof.: This follows directly from Lemmas G.8 and G.9. 

Let us define an operator \(\mathcal{P}_{\Omega_{l}}\) such that an entry of \(\mathcal{P}_{\Omega_{l}}(\bm{X})\) is equal to that of \(\bm{X}\) if it is contained both in the \(l\)th row/column and \(\Omega\), and otherwise \(0\). We also define an operator \(\mathcal{P}_{l}\) that makes the entries outside the \(l\)th row/column zero. Then, we have

\[\frac{1}{p}\mathcal{P}_{\Omega}(\bm{X})-\mathcal{P}_{\Omega}^{(l)}(\bm{X})= \frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{X})-\mathcal{P}_{l}(\bm{X}).\]

Also, note that

\[\frac{1}{p}\mathcal{P}_{\Omega}(\bm{E})-\bm{E}^{(l)}=\frac{1}{p}\mathcal{P}_{ \Omega_{l}}(E).\]

The following lemma was also introduced in [14], but we include the proof for completeness.

**Lemma G.11**.: _Suppose that a matrix \(\bm{M}\) and a vector \(\bm{v}\) are independent from sampling of the \(l\)th row/column. If \(n^{2}p\gtrsim n\log n\), we have_

\[\left\|\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{M})-\mathcal{P}_{l}(\bm {M})\right)\bm{v}\right\|_{2}\lesssim\left\|\bm{M}\right\|_{\infty}\left(\sqrt {\frac{\log n}{p}}\|\bm{v}\|_{2}+\frac{\log n}{p}\|\bm{v}\|_{\infty}+\sqrt{ \frac{n}{p}}\|\bm{v}\|_{\infty}\right)\]

_with high probability._Proof.: If we consider the contribution of \(l\)th term and the other terms separately, we have

\[\left\|\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{M})-\mathcal{P} _{l}(\bm{M})\right)\bm{v}\right\|_{2} \leq\left|\sum_{j=1}^{n}\left(\frac{\delta_{lj}}{p}-1\right)M_{lj}v _{j}\right|+\left|v_{l}\right|\sqrt{\sum_{i=1}^{n}\left(\frac{\delta_{il}}{p}-1 \right)^{2}M_{il}^{2}}\] \[\leq\left\|\bm{M}\right\|_{\infty}\left(\left|\sum_{j=1}^{n}\left( \frac{\delta_{lj}}{p}-1\right)v_{j}\right|+\left\|\bm{v}\right\|_{\infty}\sqrt {\sum_{i=1}^{n}\left(\frac{\delta_{il}}{p}-1\right)^{2}}\right)\]

From Lemma G.6, we have

\[\left|\sum_{j=1}^{n}\left(\frac{\delta_{lj}}{p}-1\right)v_{j}\right|\lesssim \sqrt{\frac{\log n}{p}}\left\|\bm{v}\right\|_{2}+\frac{\log n}{p}\left\|\bm{v} \right\|_{\infty}\]

with high probability. Regarding the second term, notice that

\[\sum_{i=1}^{n}\left(\frac{\delta_{il}}{p}-1\right)^{2}=n+\left(\frac{1}{p}-2 \right)\sum_{i=1}^{n}\frac{\delta_{il}}{p}.\]

Lemma G.6 implies that \(\sum_{i=1}^{n}\frac{\delta_{il}}{p}\asymp n\) with high probability if \(n^{2}p\gtrsim n\log n\). Hence, we have

\[\sum_{i=1}^{n}\left(\frac{\delta_{il}}{p}-1\right)^{2}\lesssim\frac{n}{p},\]

and this finishes the proof. 

**Lemma G.12**.: _Let \(\bm{M}\) be a matrix and \(\bm{v}\), \(\bm{w}\) be vectors that are independent from sampling of the \(l\)th row/column. Then, if \(n^{2}p\gtrsim n\log n\), we have_

\[\left|\bm{w}^{\top}\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{ M})-\mathcal{P}_{l}(\bm{M})\right)\bm{v}\right|\] \[\lesssim\left\|\bm{M}\right\|_{\infty}\left(\sqrt{\frac{\log n}{p }}(\left\|\bm{v}\right\|_{2}\left\|\bm{w}\right\|_{\infty}+\left\|\bm{w} \right\|_{2}\left\|\bm{v}\right\|_{\infty})+\frac{\log n}{p}\left\|\bm{v} \right\|_{\infty}\|\bm{w}\|_{\infty}\right)\]

Proof.: We can consider the \(l\)th row and column separately by

\[\left|\bm{w}^{\top}\left(\frac{1}{p}\mathcal{P}_{\Omega_{l}}( \bm{M})-\mathcal{P}_{l}(\bm{M})\right)\bm{v}\right|\] \[\leq\left|v_{l}\sum_{i}\left(\frac{\delta_{il}}{p}-1\right)M_{il}w _{i}\right|+\left|w_{l}\sum_{j}\left(\frac{\delta_{lj}}{p}-1\right)M_{lj}v_{j} \right|+\left|\left(\frac{\delta_{ll}}{p}-1\right)M_{ll}v_{l}w_{l}\right|\] \[\leq\left\|\bm{M}\right\|_{\infty}\left(\left\|\bm{v}\right\|_{ \infty}\left|\sum_{i}\left(\frac{\delta_{il}}{p}-1\right)w_{i}\right|+\left\| \bm{w}\right\|_{\infty}\left|\sum_{j}\left(\frac{\delta_{lj}}{p}-1\right)v_{j} \right|+\frac{1}{p}\left\|\bm{v}\right\|_{\infty}\left\|\bm{w}\right\|_{\infty}\right)\]

If we apply Lemma G.6 to the summations, we get the desired result. 

**Lemma G.13**.: _Let \(\bm{E}\) be a symmetric matrix whose upper and on diagonal entries are drawn from Gaussian distribution \(\mathcal{N}(0,\sigma^{2})\) independently. Let \(\bm{v}\) be a vector that is independent from sampling of the \(l\)th row and column. Then, if \(n^{2}p\gtrsim n\log^{2}n\), we have_

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{E})\bm{v}\right\|_{2}\lesssim \sigma\left(\sqrt{\frac{\log n}{p}}\left\|\bm{v}\right\|_{2}+\frac{\sqrt{\log^ {3}n}}{p}\left\|\bm{v}\right\|_{\infty}+\sqrt{\frac{n}{p}}\left\|\bm{v}\right\| _{\infty}\right)\]

Proof.: If we consider the contribution of \(l\)th term and the other terms separately, we have

\[\left\|\frac{1}{p}\mathcal{P}_{\Omega_{l}}(\bm{E})\bm{v}\right\|_{2}\leq\frac{ 1}{p}\left|\sum_{j=1}^{n}\delta_{lj}E_{lj}v_{j}\right|+\frac{1}{p}|v_{l}|\sqrt {\sum_{i=1}^{n}\delta_{il}E_{il}^{2}}\]For the first term, we will calculate \(V\) and \(L\) of Corollary G.5. \(V\) is calculated as

\[V=\sum_{j=1}^{n}\mathbb{E}\big{[}(\delta_{lj}E_{lj}v_{j})^{2}\big{]}=p\sigma^{2} \|\boldsymbol{v}\|_{2}^{2}.\]

To find \(L\), we first note that \(\left\|\boldsymbol{E}_{l*}\right\|_{\infty}\lesssim\sigma\sqrt{\log n}\) with high probability, where \(\boldsymbol{E}_{l*}\) is the \(l\)th row of \(\boldsymbol{E}\). Thus, for all \(j\in[n]\), we have

\[|\delta_{lj}E_{lj}v_{j}|\lesssim\sigma\sqrt{\log n}\|\boldsymbol{v}\|_{\infty}.\]

Corollary G.5 implies that the first term is bounded as

\[\frac{1}{p}\bigg{|}\sum_{j=1}^{n}\delta_{lj}E_{lj}v_{j}\Bigg{|}\lesssim\sigma \left(\sqrt{\frac{\log n}{p}}\|\boldsymbol{v}\|_{2}+\frac{\sqrt{\log^{3}n}}{p} \|\boldsymbol{v}\|_{\infty}\right).\] (G.1)

For the second term, it suffices to bound

\[\bigg{|}\sum_{i=1}^{n}\delta_{il}(E_{il}^{2}-\sigma^{2})\Bigg{|}.\]

As before, we obtain \(V\) and \(L\) through

\[\sum_{i=1}^{n}\mathbb{E}\big{[}(\delta_{il}(E_{il}^{2}-\sigma^{2}) )^{2}\big{]} =p\sum_{i=1}^{n}\mathbb{E}\big{[}E_{il}^{4}-2\sigma^{2}E_{il}^{2}+ \sigma^{4}\big{]}=2\sigma^{4}np,\] \[\big{|}\delta_{il}(E_{il}^{2}-\sigma^{2})\big{|}\lesssim\sigma^{2 }\log n.\]

Corollary G.5 implies that

\[\left|\sum_{i=1}^{n}\delta_{il}(E_{il}^{2}-\sigma^{2})\right|\lesssim\sigma^{ 2}\left(\sqrt{np\log n}+\log^{2}n\right).\]

Because \(\sum_{i=1}^{n}\delta_{il}\asymp np\), we have

\[\sum_{i=1}^{n}\delta_{il}E_{il}^{2}\lesssim\sigma^{2}\left(np+\sqrt{np\log n}+ \log^{2}n\right)\lesssim\sigma^{2}np\] (G.2)

if \(n^{2}p\gtrsim n\log^{2}n\). Combining (G.1) and (G.2), we get the desired bound. 

**Lemma G.14**.: _Let \(\boldsymbol{E}\) be a symmetric matrix whose upper and on diagonal entries are drawn from Gaussian distribution \(\mathcal{N}(0,\sigma^{2})\) independently. Let \(\boldsymbol{v},\boldsymbol{w}\) be vectors that are independent from sampling of the \(l\)th row and column. Then, if \(n^{2}p\gtrsim n\log n\), we have_

Proof.: We can consider the \(l\)th row and column separately by

\[\frac{1}{p}\big{|}\boldsymbol{w}^{\top}\mathcal{P}_{\Omega_{l}}( \boldsymbol{E})\boldsymbol{v}\big{|} \leq\frac{1}{p}\bigg{|}v_{l}\sum_{i}\delta_{il}E_{il}w_{i}\bigg{|}+ \frac{1}{p}\bigg{|}w_{l}\sum_{j}\delta_{lj}E_{lj}v_{j}\bigg{|}+\frac{1}{p}| \delta_{il}E_{ll}v_{l}w_{l}|\] \[\leq\frac{\|\boldsymbol{v}\|_{\infty}}{p}\bigg{|}\sum_{i}\delta_ {il}E_{il}w_{i}\bigg{|}+\frac{\|\boldsymbol{w}\|_{\infty}}{p}\bigg{|}\sum_{j} \delta_{lj}E_{lj}v_{j}\bigg{|}+\frac{1}{p}\|\boldsymbol{v}\|_{\infty}\| \boldsymbol{w}\|_{\infty}|E_{ll}|.\]

We bound the two summations similar to (G.1) and for the last term, we note that \(|E_{ll}|\lesssim\sigma\sqrt{\log n}\) with high probability.