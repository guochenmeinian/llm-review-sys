# A Simple Framework for Generalization in Visual RL under Dynamic Scene Perturbations

 Wonil Song

Yonsei University

Seoul, South Korea

swoni192@yonsei.ac.kr

&Hyesong Choi

Ewha Womans University

Seoul, South Korea

hyesongchoi2010@gmail.com

&Kwanghoon Sohn

Yonsei University

Seoul, South Korea

khsohn@yonsei.ac.kr

&Dongbo Min

Ewha Womans University

Seoul, South Korea

dbmin@ewha.ac.kr

Corresponding Author

###### Abstract

In the rapidly evolving domain of vision-based deep reinforcement learning (RL), a pivotal challenge is to achieve generalization capability to dynamic environmental changes reflected in visual observations. Our work delves into the intricacies of this problem, identifying two key issues that appear in previous approaches for visual RL generalization: (i) imbalanced saliency and (ii) observational overfitting. Imbalanced saliency is a phenomenon where an RL agent disproportionately identifies salient features across consecutive frames in a frame stack. Observational overfitting occurs when the agent focuses on certain background regions rather than task-relevant objects. To address these challenges, we present a simple yet effective framework for generalization in visual RL (SimGRL) under dynamic scene perturbations. First, to mitigate the imbalanced saliency problem, we introduce an architectural modification to the image encoder to stack frames at the feature level rather than the image level. Simultaneously, to alleviate the observational overfitting problem, we propose a novel technique called shifted random overlay augmentation, which is specifically designed to learn robust representations capable of effectively handling dynamic visual scenes. Extensive experiments demonstrate the superior generalization capability of SimGRL, achieving state-of-the-art performance in benchmarks including the DeepMind Control Suite. 1

Footnote 1: Website and code are available at: https://w-song11.github.io/SimGRL.

## 1 Introduction

Deep reinforcement learning (RL) utilizing visual observations has achieved remarkable success across diverse domains, including robotic manipulation [21], video games [25, 1, 38], and autonomous navigation [24, 48]. However, acquiring generalizable RL policies across diverse environments remains challenging, mainly due to overfitting [44] in the high-dimensional observation space [32]. To obtain robust policies invariant to visual perturbations, a variety of approaches based on domain randomization [36, 28] and data augmentation [31, 19, 40, 29, 13, 12] have been widely proposed. These approaches operate under the assumption that exposing an agent to various augmentations during the training phase enhances its adaptability to unseen domains. Despite the encouraging results, performance still falls behind in challenging environments with dynamically changing backgrounds.

Fig. 1(a) shows the notable performance degradation of existing approaches when comparing 'Video Hard' scenes with dynamically changing backgrounds to less dynamic 'Video Easy' scenes in the DeepMind Control Suite-Generalization Benchmark (DMControl-GB) [13].

Using a gradient-based attribution mask \(M_{\rho}\)[2], we first investigate the causes of the degradation in generalization in such challenging environments by examining salient regions across consecutive stacked frames used as an RL input. Based on our analysis, we empirically identified two phenomena, highlighting them as key causes of performance degradation: (i) what we refer to as _imbalanced saliency_ and (ii) _observational overfitting_[32]. In the DMControl [35, 37], Fig. 1(b) depicts an example of the imbalanced saliency in a 'Cartpole, Swingup' task, where salient features across all stacked frames are biased to the regions corresponding to the task objects in the latest two frames. Fig. 1(c) shows an example of the observational overfitting that occurs in a 'Cheetah, Run' task. In this case, the RL agent misidentififies the ground as more salient than the 'Cheetah' object. These two problems contribute to overfitting to the training environment, making generalization of the RL agent even more challenging.

In image-based RL, the effectiveness of regularization has been demonstrated in numerous studies. DrQ [40] and RAD [19], each employing random shift and random crop augmentations as data regularization on input images, achieved remarkable performance improvements over vanilla SAC [8]. Furthermore, [32] exhibited that regularization effects from architectural modifications such as overparameterization or residual connections can help avoid observational overfitting and reduce the generalization gap.

In this context of research, we introduce a **Sim**ple yet effective framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations. Firstly, we empirically found the image-level frame stack, commonly used in traditional vision-based _model-free_ RL approaches [25, 18, 41, 40, 19], to be the main factor causing the imbalanced saliency problem. To address this issue, we propose a simple architectural modification of an image encoder, which employs a feature-level frame stack instead of the image level. This involves extracting individual feature maps for each frame from the shallow layers of the encoder, stacking them along channels, and encoding the stacked feature maps through the remaining layers. Since this approach considers solely individual frames during the initial encoding, the agent is trained to focus on spatially salient features in each consecutive frame that are essential for a given task, alleviating the imbalanced saliency problem of Fig. 1(b). Secondly, to address the observational overfitting problem by encouraging the agent to focus on the task object rather than backgrounds, we propose a new data augmentation called shifted random overlay, which is a modified version of random overlay augmentation [13]. This augmentation directly injects background dynamics irrelevant to the given task into the agent during the training phase by interpolating natural images moving in a random direction on each frame. By enabling the agent to implicitly learn to ignore task-irrelevant backgrounds and focus on task-relevant pixels, this approach alleviates the observational overfitting problem of Fig. 1(c). Furthermore, this augmentation enables the agent to adapt to test environments with dynamic perturbations in the surrounding backgrounds.

Figure 1: (a) Average performances on 6 tasks in DMControl-GB. In contrast to other methods with significant performance degradation in Video Hard, our proposed SimGRL demonstrates robust performance across all benchmarks. (b)\(-\)(c) Examples of two problematic phenomena that can cause overfitting in visual RL generalization. The background structures in the red boxes are correlated with the movement of the task object. \(s\) and \(M_{\rho}\) represent the stacked frames and attribution masks, respectively. Attribution masks in this figure were obtained using the critic trained by DrQ [40].

We verify that these strategies can remarkably improve generalization in challenging environments, achieving state-of-the-art performance without using any additional auxiliary losses or networks.

Furthermore, utilizing the attribution mask \(M_{\rho}\)[2], we introduce novel metrics, called **T**ask-**I**dentification (TID) metrics, consisting of TID score and TID variance, to quantitatively evaluate the discrimination ability on salient regions. With the proposed metrics, we quantitatively analyze the two problematic phenomena in the existing approaches, demonstrating the excellent discrimination ability of SimGRL. Moreover, we show a tendency of positive correlation between the TID score and RL generalization performance, emphasizing the importance of accurately identifying salient features in input images.

Our contributions include the following aspects:

* By utilizing gradient-based attribution masks, we highlight the two core issues of imbalanced saliency and observational overfitting, which hinder the generalization of visual RL for most model-free RL settings. Additionally, we propose TID metrics to measure the discrimination ability of an RL agent on task objects, providing insights into these issues.
* To address these problems, we propose architectural and data regularization methods through a modification to an encoder structure and an introduction of new data augmentation.
* We achieve state-of-the-art performances across video benchmarks of DMControl-GB [13], DistractingCS [34], and robotic manipulation tasks [14].

## 2 Background

Visual RL and GeneralizationWe consider a partially observable Markov decision process (POMDP) problem \(\mathcal{M}=(\mathcal{S},\mathcal{O},\mathcal{A},\mathcal{P},r,\gamma)\), where \(\mathcal{S}\) is state space, \(\mathcal{O}\) is observation space, \(\mathcal{A}\) is action space, \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\) is the transition function that defines the conditional probability distribution \(\mathcal{P}(s_{t+1}|s_{t},a_{t})\) over next states given a state \(s_{t}\in\mathcal{S}\) and an action \(a_{t}\in\mathcal{A}\) taken at time \(t\), \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is a reward function, and \(\gamma\in[0,1)\) is a discount factor. In the POMDP problem, such as the visual RL, because only the high-dimensional observations \(o_{t}\in\mathcal{O}\) can be observable [15], a state is defined as a sequence of the \(k\) consecutive image frames \(s_{t}=(o_{t-k+1},...,o_{t-1},o_{t})\)[25]. Without loss of generality, we will set \(k=3\) for the sake of notational simplicity. RL aims to learn a policy \(\pi:\mathcal{S}\rightarrow\mathcal{A}\) that maximizes the expected sum of discounted rewards \(\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}r(s_{t},r_{t})]\). In this work, we focus on the generalization problem to POMDPs \(\widehat{\mathcal{M}}=(\widehat{\mathcal{S}},\widehat{\mathcal{O}},\mathcal{ A},\mathcal{P},r,\gamma)\), where the states \(\hat{s}\in\widehat{\mathcal{S}}\) are constructed from the perturbed observations \(\hat{o}\in\widehat{\mathcal{O}}\), and a POMDP \(\widehat{M}\) is sampled from the space of POMDPs \(\mathbb{M}\), \(\widehat{M}\sim\mathbb{M}\).

Deep Q-Learning and Soft Actor-CriticDeep Q-learning [25] is a common model-free RL algorithm that aims to learn a parameterized state-action value function \(Q_{\theta}(s_{t},a_{t})\), where \(\theta\) is the deep neural network parameters of the Q-function and action is greedily selected as the one with the maximum value \(a_{t}=\operatorname*{argmax}_{a}Q_{\theta}(s_{t},a)\) at time \(t\). The training of the Q-function is achieved by minimizing a mean squared error of the Bellman residuals:

\[\mathbb{E}_{(s_{t},a_{t},s_{t+1})\sim\mathcal{B}}[(Q_{\theta}(s_{t},a_{t})-(r _{t}+\gamma\max_{a^{\prime}}Q_{\hat{\theta}}^{tgt}(s_{t+1},a^{\prime})))^{2}],\] (1)

where \(\hat{\theta}\) is the parameters of the target Q network \(Q^{tgt}\) and \(\mathcal{B}\) is a replay buffer. To increase the stability, the parameter of the target Q network is slowly updated by the exponential moving average (EMA) \(\hat{\theta}=\lambda\theta+(1-\lambda)\hat{\theta}\) with \(\lambda\ll 1\)[22]. For the continuous action space, rather than the greedy sampling, a parameterized actor \(\pi_{\phi}(s_{t})\) is employed as the policy, where \(\phi\) is the neural network parameters of the actor. Soft Actor-Critic (SAC) [8] is a common actor-critic algorithm with a state-action value \(Q_{\theta}(s_{t},a_{t})\) and a stochastic policy \(\pi_{\phi}(a_{t}|s_{t})\), and a temperature parameter \(\alpha\), which aims to optimize a \(\gamma\)-discounted maximum-entropy objective [49].

In visual RL, a parameterized encoder \(f_{\theta}:\mathbb{R}^{C\times H\times W}\rightarrow\mathbb{R}^{d}\) is employed to compress the high-dimensional image inputs and shared by both the actor and critic, where \(d\) is a dimension of the encoded feature. Consistent with the prior visual RL approaches [41; 18; 19; 40; 12], we jointly train the encoder with the critic and freeze it during actor updates, where the encoder learns representations for RL tasks by the critic loss. For notational simplicity, we denote all parameters updated by the critic loss as \(\theta\) and the actor parameters as \(\phi\).

Gradient-based Attribution MaskAn attribution map, also known as a saliency map, is designed to visualize the salient pixels in input images for given tasks. A common approach to computing the attribution map is a gradient-based method [30; 3], which indicates how sensitive the task prediction is to perturbations in the input pixels. Consistent with SGQN [2], we employ guided backpropagation [33] to compute the attribution map \(M(Q_{\theta},s_{t},a_{t})=\frac{\partial Q_{\theta}(s_{t},a_{t})}{\partial s_{t }}\), where \(s_{t}\) and \(M(Q_{\theta},s_{t},a_{t})\in\mathbb{R}^{C\times H\times W}\). Then, the binarized attribution mask \(M_{\rho}(Q_{\theta},s_{t},a_{t})\), referred to as \(\rho\)-quantile attribution mask, is computed by thresholding \(M(Q_{\theta},s_{t},a_{t})\) by the \(\rho\)-quantile, where \(M_{\rho}(Q_{\theta},s_{t},a_{t})_{(i,j,k)}=1\) if the pixel value of \(M(Q_{\theta},s_{t},a_{t})_{(i,j,k)}\) belongs to the \(\rho\)-quantile of highest values for \(M(Q_{\theta},s_{t},a_{t})\), and \(0\) otherwise. We use this attribution mask to investigate the cause of the degradation of the generalization performance of existing methods in challenging environments such as Video Hard [13]. Furthermore, to justify our analysis, we will introduce new metrics that quantitatively measure the ability to identify salient pixels in Section 4.4. Note that we leverage the attribution mask only for analysis without involving it in the training process.

## 3 Pitfalls within Conventional Practices in Visual RL Generalization

The generalization ability of RL agents is often degenerated in challenging environments characterized by dynamic perturbations and different structures compared to the training environment. In this section, we investigate the causes of the overfitting to the training environment observed in existing visual RL methods on the DMControl Generalization Benchmark (DMControl-GB) [13].

Conventional practices used to train the visual RL agent for generalization [13; 2; 42; 4] include frame stack and data augmentation. First, to reflect the temporal structure of the input state, Q-value is predicted using stacked frames [25; 18; 41; 40; 19] instead of a single image as follows:

\[q_{\theta}(s_{t},a_{t})=Q_{\theta}(f_{\theta}([o_{t-2},o_{t-1},o_{t}]),a_{t}),\quad s_{t}=(o_{t-2},o_{t-1},o_{t}),\] (2)

where \(\theta\) is a neural network parameter, \(f_{\theta}(\cdot)\) is a convolutional neural network (CNN) image encoder, \(Q_{\theta}(\cdot,\cdot)\) is a critic head, and \([\cdot]\) is a concatenation operator along a channel dimension. Subsequently, to learn the representations robust to visual perturbations, data augmentation to the visual observations is leveraged when training the encoder. Specifically, \(s_{t}\) in Eq. (2) can be substituted with \(\tau(s_{t})=(\tau(o_{t-2}),\tau(o_{t-1}),\tau(o_{t})),\ \tau\sim\mathcal{T}\), where \(\tau(\cdot)\) is a sampled transformation function from the transformation space \(\mathcal{T}\). However, we empirically found that these practices can cause the saliency imbalance between the stacked frames and easily fall into observational overfitting, resulting in performance degradation in unseen challenging environments.

Pitfall 1: Imbalanced SaliencyFig. 2(a) illustrates an example of the imbalanced saliency by SVEA [12] in the 'Cartpole' task. As described in Section 2, the attribution masks were acquired by thresholding the gradient value of the critic function with respect to input image frames [2]. In this example, an RL agent recognizes the regions of the salient objects in the two latest frames as having high saliency across all stacked frames. This makes the agent misidentify as salient pixels of \(o_{t-2}\) its background parts corresponding to the positions of the 'Cartpole' object in \(o_{t-1}\) and \(o_{t}\). As a result, \(o_{t-2}\) provides the agent with redundant information that is unnecessary for the task, leading to overfitting to training data [45; 5]. In a test environment with complicated backgrounds, the background information that is considered salient from \(o_{t-2}\) may act as noise that interferes with the decision-making process of the policy. As the latest frames are more closely related to the subsequent decision-making process, the agent tends to identify spatially consistent saliency maps based on these recent frames, leading to imbalanced saliency maps. We hypothesize that this phenomenon occurs because the encoder extracts features throughout concatenated images along the channel as in Eq. (2), forcing it to capture the same spatial saliency for all stacked images.

Pitfall 2: Observational OverfittingObservational overfitting [32] can arise when certain background elements move in synchronization with task-relevant objects. It is prone to occur particularly when the motion of the task object is not substantial, such as in the 'Cheetah, Run' task. Although data augmentation is leveraged during training, we found that observational overfitting can still occur. Furthermore, as the same augmentation is applied uniformly to all consecutive frames, it does not effectively prevent the agent from erroneously focusing on background elements. For instance, Fig. 2(b) shows an example of observational overfitting that occurs in the 'Cheetah, Run' task by SVEA, where the critic assigns more significant saliency to the ground than to the 'Cheetah' object. In a test environment, if there is a lack of correlation between objects and backgrounds present in the training environment, the agent may struggle to make accurate decisions, thereby leading to challenges in generalization.

## 4 Method

To address the problems within the conventional practices used for the generalization of visual RL, we propose a **S**imple framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations.

### Feature-Level Frame Stack

In Section 3, we identified the encoder structure that simultaneously encodes the stacked frames at the image level as the cause of the imbalanced saliency. We address this issue by introducing an architectural regularization strategy to slightly modify the encoder structure. This architectural modification involves encoding each frame individually and then encoding the stacked feature maps. To keep the computational cost almost unchanged, we partition the original encoder into two segments instead of adding new layers. Then, Eq. (2) can be modified as follows:

\[q_{\theta}(s_{t},a_{t})=Q_{\theta}(f_{\theta}^{2}([f_{\theta}^{1}(o_{t-2}),f_{ \theta}^{1}(o_{t-1}),f_{\theta}^{1}(o_{t})]),a_{t}),\quad s_{t}=(o_{t-2},o_{t-1 },o_{t}),\] (3)

where \(f_{\theta}^{1}(\cdot)\) is an image encoder to encode individual frames, and \(f_{\theta}^{2}(\cdot)\) is a feature encoder for the stacked feature maps. While \(f_{\theta}^{2}\) encodes both spatial and temporal structures from inputs, \(f_{\theta}^{1}\) conducts only spatial encoding of each frame. This simple modification allows the critic to be implicitly trained to focus on the spatially salient pixels of individual frames, enabling the agent to distinctly identify the salient pixels of each frame. In our experiments, we will verify that this simple modification of the encoder is highly beneficial for generalization, especially in challenging test environments.

### Shifted Random Overlay Augmentation

The random overlay augmentation [13] used in existing approaches augments input images through linear interpolation with a natural image \(\varepsilon\in\mathbb{R}^{C\times H\times W}\) randomly sampled from the Places [46] dataset that contains 1.8M diverse scenes:

\[\tau^{RO}(s;\varepsilon)=(\alpha\varepsilon+(1-\alpha)o_{1},...,\alpha \varepsilon+(1-\alpha)o_{n}),\quad\varepsilon\sim\mathcal{D},\] (4)

Figure 2: Examples of attribution masks and masked frames. Compared to SVEA that falls into the imbalanced saliency and observation overfitting in the ‘Cartpole, Swingup’ and ‘Cheetah, Run’ tasks, respectively, the proposed SimGRL accurately identifies the true salient pixels even in challenging ‘Video Hard’ test environments of DMControl-GB. We provide further examples of the attribution masks and masked salient regions for various environments and algorithms in Appendix F.4.

where \(s\) is a state that consists of a sequence of images \((o_{1},...,o_{n})\), \(o_{i}\in\mathbb{R}^{C\times H\times W},\)\(\mathcal{D}\) is a dataset and a common choice for the interpolation coefficient is \(\alpha=0.5\). The original random overlay augmentation uses the same natural image \(\varepsilon\) for all stacked frames. Contrarily, we introduce a shifted random overlay (SRO) augmentation to inject the task-irrelevant dynamics into the training images, which is depicted in Fig. 3. Considering a maximum shift length \(l\) for each shifting and a stacked frame number \(n\), this method first samples a natural image \(\varepsilon\in\mathbb{R}^{C\times\widehat{H}\times\widehat{W}}\) from the dataset \(\mathcal{D}\), where \((\widehat{H},\widehat{W})=(H+2(n-1)l,W+2(n-1)l)\). Then, we crop \(n\) shifted patches \(\varepsilon_{i}^{c}\in\mathbb{R}^{C\times H\times W}\) from \(\varepsilon\) to augment each frame in a shifted way. Given an upper left corner coordinate \((h_{1},w_{1})\) of \(H\times W\) sized center crop at the center of \(\varepsilon\), the coordinates \((h_{i},w_{i})\) of the upper left corner of \(\varepsilon_{i}^{c}\) are selected as follows:

\[(h_{i},w_{i})=(h_{1}+dh(i-1),w_{1}+dw(i-1)),\qquad\quad i=1,...,n,\] (5)

where \(dh,dw\sim\mathrm{unit}\{-l,l\}\). Finally, using the cropped images \(\varepsilon_{i}^{c}\) from \((h_{i},w_{i})\) in \(\varepsilon\), the shifted random overlay augmentation is defined as:

\[\tau^{SRO}(s;l,\varepsilon)=(\alpha\varepsilon_{1}^{c}+(1-\alpha)o_{1},..., \alpha\varepsilon_{n}^{c}+(1-\alpha)o_{n}),\] (6)

where we adopt \(\alpha=0.5\). This augmentation provides two implicit advantages when training RL agents. Firstly, the inclusion of background elements in motion, independent of the task, enables the RL agent to perceive that rewards are solely associated with changes in the genuine task object. Consequently, the agent is trained to concentrate on the task object, disregarding the movement of background elements, thereby alleviating the issue of observational overfitting. Secondly, this augmentation method generates training data akin to real environments with dynamic backgrounds, enabling the agent to be robust in such conditions and enhancing generalization capability.

### Simple Framework for Generalization in Visual RL (SimGRL)

Built on the SVEA [12] framework that leverages the data-mixing strategy between weak and strong data augmentations and computes the target Q-value using clean images for stability in SAC, we propose a Simple framework for Generalization in visual RL (SimGRL) under dynamic scene perturbations, which integrates the proposed two strategies. For the strong augmentation, we utilize the shifted random overlay instead of the original random overlay without shifting while the random shift [40] is employed as the weak augmentation. Considering that the clean images \(s_{t}\) and \(s_{t+1}\) already have weak augmentation, _i.e._, random shift, applied by default, the critic loss for SimGRL is defined as follows:

\[\mathcal{L}_{Q}(\theta)=\mathbb{E}_{(s_{t},a_{t},r_{t},s_{t+1})\sim\mathcal{B }}[\beta(q_{\theta}(s_{t},a_{t})-q^{tgt})^{2}+(1-\beta)(q_{\theta}(\tau^{SRO}( s_{t};l,\varepsilon),a_{t})-q^{tgt})^{2}],\] (7)

where \(q_{\theta}(\cdot,\cdot)\) is computed by Eq. (3), \(q^{tgt}=r_{t}+\gamma q_{\theta}(s_{t+1},a^{\prime})\), \(a^{\prime}\sim\pi_{\phi}(\cdot|f_{\theta}(s_{t+1}))\), and \(\hat{\theta}\) is the parameter of the target network that is updated by the exponential moving average (EMA). The strong augmentation \(\tau^{SRO}(\cdot;\cdot,\cdot)\) is the shifted random overlay in Eq. (6). Consistent with the SVEA, we adopt the data-mixing coefficient \(\beta=0.5\). When training the actor \(\pi_{\phi}\), we leverage solely clean images like the existing methods [12; 13]. The overall framework of SimGRL is illustrated in Fig. 4 and the algorithm is summarized in Appendix C.

Figure 3: Shifted random overlay (SRO) augmentation for data regularization. To inject random dynamics into the backgrounds of RL input images, we generate multiple cropped patches in a shifted manner from a sampled natural image and interpolate them to augment the input images.

### **Task-Identification (TID) Metrics**

To quantitatively evaluate the capability to identify task-relevant objects in each stacked frame as salient, we introduce novel metrics, referred to as Task-Identification (TID) score and variance, based on the \(\rho\)-quantile attribution mask \(M_{\rho}(Q_{\theta},s_{t},a_{t})\)[2] described in Section 2.

TID ScoreTID score measures how much the model identifies the task object's pixels across stacked frames, which is defined as:

\[\textit{TID}_{S}=\sqrt{\frac{N_{obj_{M}}}{N_{obj}}\times\frac{N_{obj_{M}}}{N_{ M}}}=\sqrt{\frac{(N_{obj_{M}})^{2}}{N_{obj}\times N_{M}}},\] (8)

where \(N_{obj}\) is the number of task object's pixels in input images, \(N_{M}\) is the number of pixels in attribution masks \(M_{\rho}\), \(N_{obj_{M}}\) is the number of task object's pixels included in \(M_{\rho}\), and \(\rho\) is a quantile value for thresholding the attribution map. Note that all numbers consisting of the TID score are counted across the full consecutive frames. In Eq. (8), the first term quantifies the model's ability to identify the pixels of the task object, while the second term quantifies how accurately the model identifies the task object's pixels. These two terms trade-off depending on the size of the quantile value and are upper-bounded by 1, thus leading to an upper-bounded TID score by 1. With a model that perfectly identifies all task pixels, this upper-limit value can only be achieved along with an optimal \(\rho\) value, which is the quantile of the number of the task object's pixels in frames. This \(\rho\) value can be computed by \(\rho=1-\frac{N_{obj}}{(n\times C\times H\times W)}\), where \(n\) is the number of frame stack. We explain the impact of the \(\rho\) value on this TID score in Appendix F.2.

TID VarianceTID variance measures how discriminatively the model distinguishes the task object's pixels in each frame, which is defined as:

\[\textit{TID}_{Var}=\textit{Var}[100\times(TID_{S}^{1},TID_{S}^{2},...,TID_{S}^ {n})],\] (9)

where \(TID_{S}^{i}=\sqrt{\frac{(N_{obj_{M}}^{i})^{2}}{N_{obj_{M}}^{i}\times N_{M}^{i }}}\), \(N_{obj}^{i}\), \(N_{M}^{i}\), and \(N_{obj_{M}}^{i}\) are individually counted at each frame. Since each \(TID_{S}{}^{i}\) is upper-bounded by 1, the values of \(TID_{Var}\) become too small for meaningful comparison. Therefore, we multiply \(TID_{S}{}^{i}\) by 100 to obtain a variance with a more appropriate scale for comparison.

## 5 Experiments

In this section, we present the experimental results of SimGRL on the DMControl-GB [13] video benchmarks ('Video Easy' and 'Video Hard') at \(500\mathrm{K}\) simulated training frames to demonstrate the generalization capability under dynamic scene perturbations. To tackle the vision-based control tasks with continuous action space, we employ the SAC [8] as a backbone RL algorithm, and compare SimGRL against current state-of-the-art

Figure 4: Overview of the **Simple** framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations. Differences from SVEA are marked in red.

Figure 5: Experimental setup. We evaluated the zero-shot performances for test environments with dynamic background perturbations.

methods for visual RL generalization including RAD [19], DrQ [40], SODA [13], SVEA [12], TLDA [42], SGQN [2], EAR [4], and CG2A [23]. For weak augmentation, RAD and SODA utilize random crop, while the others employ random shift, except for SAC, which does not use any data augmentations. In addition, all competitors leverage the original random overlay augmentation without shifting for strong augmentation except for RAD and DrQ, which use only weak augmentations. As default for SimGRL, we used the random shift augmentation [40] and denoted the images with only this weak augmentation as clean images with the data distribution of the training data. Additionally, we employed the proposed shifted random overlay (SRO) for strong augmentation in SimGRL and denoted images together with this augmentation as augmented ones. Implementation details are described in Appendix A, and further experimental results on other benchmarks of Distracting Control Suite [34] and robotic manipulation [14] are provided in Appendices B.7 and B.8, respectively.

### Results on DMControl-GB

We evaluated the zero-shot generalization performance on 6 tasks in 'Video Easy' and 'Video Hard' benchmarks from DMControl-GB [13]. As illustrated in Fig. 5, the easy version shares certain structures with the training environment such as the ground and shadow, while the hard version shares nothing other than the agent's object by replacing all backgrounds with distracting videos. In Table 1, SimGRL demonstrates state-of-the-art performance in 4 out of 6 tasks in the Video Easy benchmark and achieves comparable performance in the remaining two tasks, including 'Walker, Walk' and 'Cheetah, Run'. On the other hand, our approach shows outstanding performance in all 6 tasks at the Video Hard level, where SimGRL achieves performance gain by 15\(\%\) on average compared to SGQN. Specifically, SimGRL outperforms existing methods by a significant margin in 'Cartpole, Swingup' and 'Cheetah, Run', which were previously difficult to solve. We suggest that the reason for this lies in the fact that 'Cartpole, Swingup' requires detailed identification of salient objects, while 'Cheetah, Run' needs to address the observational overfitting. Both issues are mitigated in SimGRL. We provide the training curves in Appendix B.6.

Computational EfficiencyOur method has the advantage of improving generalization performance without any extra models or auxiliary losses. We compare SimGRL with SGQN [2], the existing state-of-the-art method in the Video Hard benchmark. In experiments, SimGRL can achieve a throughput of 9.54 FPS, which is 1.55\(\times\) efficient compared to SGQN's 6.16 FPS on a single NVIDIA TITAN RTX GPU. Fig. 6 shows the training curves for the zero-shot test performances over the wall-clock training time, where the averaged performances across the 6 tasks in Table 1 are presented. In this figure, SimGRL requires 44\(\%\) less wall-clock training time than SGQN to reach the same 500K frames for training, demonstrating the high computational efficiency of SimGRL. It is worth noting that SimGRL has achieved considerable enhancements in both training efficiency and test

\begin{table}
\begin{tabular}{c|c|c c c c c c c c c c} \hline \hline  & DMControl-GB & SAC & RAD & DrQ & SODA & SVEA & TLDA & SGQN & EAR & CG2A & SimGRL & \(\Delta\) \\ \hline \multirow{8}{*}{\begin{tabular}{c} \end{tabular} } & Walker, Walk & 24\(\pm\)165 & 608\(\pm\)92 & 374\(\pm\)21 & 768\(\pm\)38 & 389\(\pm\)71 & 868\(\pm\)63 & 910\(\pm\)24 & 913\(\pm\)38 & **918\(\pm\)92** & 910\(\pm\)21 & -8 (0.8\(\%\)) \\  & Walker, Sund & 389\(\pm\)11 & 879\(\pm\)64 & 926\(\pm\)30 & 955\(\pm\)13 & 961\(\pm\)8 & 973\(\pm\)65 & 955\(\pm\)9 & 970\(\pm\)23 & 958\(\pm\)66 & **973\(\pm\)4** & 0 \\  & Ball in Cup, Catch & 192\(\pm\)157 & 363\(\pm\)158 & 380\(\pm\)188 & 875\(\pm\)56 & 871\(\pm\)106 & 855\(\pm\)46 & 950\(\pm\)24 & 911\(\pm\)40 & 963\(\pm\)28 & **964\(\pm\)7** & 4 (0.1\(\%\)) \\  & Carridge, Swingup & 399\(\pm\)60 & 473\(\pm\)54 & 459\(\pm\)81 & 758\(\pm\)62 & 722\(\pm\)27 & 671\(\pm\)57 & 761\(\pm\)57 & 761\(\pm\)26 & 88\(\pm\)78 & **981\(\pm\)38** & 4 (0.6\(\%\)) \\  & Finger, Spin & 206\(\pm\)169 & 516\(\pm\)113 & 599\(\pm\)62 & 695\(\pm\)97 & 808\(\pm\)33 & 744\(\pm\)18 & 956\(\pm\)28 & 771\(\pm\)51 & 912\(\pm\)90 & **983\(\pm\)2** & 4 (27\(\%\)) \\  & Cheetah, Run & 73\(\pm\)18 & 153\(\pm\)7 & 270\(\pm\)16 & 268\(\pm\)10 & 251\(\pm\)17 & **336\(\pm\)57** & 289\(\pm\)35 & 334\(\pm\)56 & 314\(\pm\)49 & 317\(\pm\)16 & -19 (6\(\%\)) \\ \hline \multirow{8}{*}{
\begin{tabular}{c} \end{tabular} } & Walker, Walk & 212\(\pm\)247 & 80\(\pm\)10 & 211\(\pm\)52 & 312\(\pm\)32 & 33\(\pm\)65 & 292\(\pm\)13 & 739\(\pm\)21 & 833\(\pm\)69 & 67\(\pm\)18 & **73\(\pm\)31** & -43 (4\(\%\)) \\  & Walker, Sund & 221\(\pm\)57 & 229\(\pm\)45 & 252\(\pm\performance compared to the SOTA method, regardless of the typical trade-offs in terms of efficiency and performance.

Ablation StudyTo verify the effectiveness of the proposed regularizations, we compare the ablation variants with the SVEA baseline. Fig. 7 shows training curves for zero-shot test performances averaged across the 6 tasks of DMControl-GB at the Video Hard benchmark as a function of the number of stepped training frames. This result indicates that the additions of each regularization to SVEA can remarkably improve the generalization ability of the model. In particular, we emphasize that each of the two regularizations leads to sufficient improvement, where all variants of SimGRL achieve, on average, nearly 82\(\%\) better performance than SVEA. Full experimental results for each task are provided in Appendix B.1 and further in-depth discussions on the effectiveness of each regularization are described in Appendix B.2.

### Analysis with TID Metrics

To analyze the two potential overfitting problems, namely imbalanced saliency and observational overfitting, we evaluate the TID metrics, including the TID score and variance. For clarity, we compare three methods of DrQ [40], SVEA [12], and SimGRL as the former serve as baselines for the latter. We analyze two representative tasks, namely 'Cartpole, Swingup' and 'Cheetah, Run', where each problem is prominently observed. Full evaluations for all algorithms and tasks are provided in Appendix F.3. The left plot in Fig. 8 depicts that the overall distribution of training scores of the TID metrics is divided into three regions. In this plot, both DrQ and SVEA exhibit results divided into two regions, either 'low score and low variance' or'middle score and high variance', depending on the task, indicating that different phenomena appear in the two tasks. The 'low score and low variance' region can be interpreted as observational overfitting, which does not identify the correct object in all frames. On the other hand, the'middle score and high variance' area suggests that the correct object is accurately recognized only in certain frames, implying imbalanced saliency. We note that although SVEA [12] incorporates a data-mixing strategy with strong augmentation to enhance the generalization capability of DrQ [40], both problems are still observed with only minor improvements over DrQ. In contrast, SimGRL reports comparatively high scores and low variances for both tasks, indicating that it can effectively identify task-relevant objects regardless of the type of task. The middle plot shows a positive correlation between training and test TID scores at Video Hard from DMControl-GB [13]. This suggests that a high discrimination ability during training can result in a high discrimination ability during testing. Finally, the right plot shows the generalization gaps, which are computed by the performance differences between training and testing, against the TID scores. This suggests that a high discrimination ability of the task object can lead to improved generalization capability.

Figure 8: (Left to Right) Plots for the training TID variance, test TID scores, and generalization gap against training TID score.

Figure 7: Training curves for ablation variants.

Related Works

The field of domain generalization for RL has garnered considerable attention in recent years where various approaches aim to enhance the robustness of policies against visual changes. A promising approach is adapting the policy to a test domain. For example, PAD [11] suggests adopting a self-supervised task to obtain a free training signal during deployment. On the other hand, one method, proposed in [36], involves using randomly simulated RGB images. Similarly, [27] train domain-adaptive policies by randomizing dynamics during the training phase. Several works explore data augmentation techniques to improve policy generalization capacity [19; 40; 29]. For instance, RAD [19] and DrQ [40] achieve significant improvement through random crop and shift, while DrAC [29] automatically identifies the most effective augmentation with regularization terms for the policy and value function. Instead of relying solely on augmented data for policy learning, SODA [13] aims to decouple augmentation from policy learning by using soft-augmented data for policy learning and strong-augmented data for auxiliary representation learning. In a recent development, SVEA [12] designs a stabilized Q-value estimation framework to address instability issues under strong data augmentation in off-policy RL, while DBC [43] uses bisimulation metrics to learn a representation that disregards task-irrelevant information. VAI [39] extracts a universal visual foreground mask to provide an invariant observation to RL. Similarly, several methods leverage salient masks to encourage the agent to focus on import pixels. For example, TLDA [42] attempts to only augment the task-irrelevant pixels using masks obtained from the Lipschitz constant of the policy while SGQN [2] proposes saliency-guided self-supervised learning using a gradient-based saliency mask. EAR [4] attempts to learn environment-agnostic representations to enhance the robustness of policies against visual perturbations. On the other hand, several methods such as TIA [7], DRIBO [6], and RePo [47], leverage model-based RL through a recurrent state-space model (RSSM) [10; 9] to explicitly learn latent representations that focus on task-relevant features while discarding task-irrelevant ones. Our work focuses on improving the generalization ability of the RL agent based on implicit regularization approaches.

## 7 Conclusion

In this paper, we identified two key issues that degrade generalization in vision-based RL, particularly under dynamic scene perturbations, by employing a gradient-based attribution mask. To resolve these issues, we proposed a simple framework involving an architectural modification to the encoder and a new data augmentation. The proposed SimGRL algorithm has achieved state-of-the-art results on Video Hard environments in DMControl-GB, outperforming existing methods that have yet to address these challenges. Additionally, we introduced TID metrics to quantitatively evaluate the ability to discriminate task-relevant objects of RL agents. Using these metrics, we demonstrated that improving the identification of task-relevant objects can enhance the generalization capability of the vision-based RL agent.

## Acknowledgments and Disclosure of Funding

This research was supported by the National Research Foundation of Korea (NRF) grant funded by the government of Korea (MSIP) (NRF2021R1A2C2006703), by the Basic Research Lab Program through the NRF of Korea (RS-2023-00222385) and by the Yonsei Signature Research Cluster Program of 2024 (2024-22-0161).

## References

* [1] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* [2] David Bertoin, Adil Zouitine, Mehdi Zouitine, and Emmanuel Rachelson. Look where you look! saliency-guided q-networks for generalization in visual reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.

* Chattopadhyay et al. [2018] Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks. In _2018 IEEE winter conference on applications of computer vision (WACV)_, pages 839-847. IEEE, 2018.
* Choi et al. [2023] Hyesong Choi, Hunsang Lee, Seongwon Jeong, and Dongbo Min. Environment agnostic representation for visual reinforcement learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 263-273, 2023.
* Cogswell et al. [2015] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. _arXiv preprint arXiv:1511.06068_, 2015.
* Fan and Li [2022] Jiameng Fan and Wenchao Li. Dribo: Robust deep reinforcement learning via multi-view information bottleneck. In _International Conference on Machine Learning_, pages 6074-6102. PMLR, 2022.
* Fu et al. [2021] Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions. In _International Conference on Machine Learning_, pages 3480-3491. PMLR, 2021.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Hafner et al. [2020] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In _International Conference on Learning Representations_, 2020.
* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _International conference on machine learning_, pages 2555-2565. PMLR, 2019.
* Hansen et al. [2021] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In _International Conference on Learning Representations_, 2021.
* Hansen et al. [2021] Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. _Advances in neural information processing systems_, 34:3680-3693, 2021.
* Hansen and Wang [2021] Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 13611-13617. IEEE, 2021.
* Jangir et al. [2022] Rishabh Jangir, Nicklas Hansen, Sambaran Ghosal, Mohit Jain, and Xiaolong Wang. Look closer: Bridging egocentric and third-person views with transformers for robotic manipulation. _IEEE Robotics and Automation Letters_, 7(2):3046-3053, 2022.
* Kaelbling et al. [1998] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. _Artificial intelligence_, 101(1-2):99-134, 1998.
* Kay et al. [2017] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Laskin et al. [2020] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In _International Conference on Machine Learning_, pages 5639-5650. PMLR, 2020.
* Laskin et al. [2020] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. _Advances in neural information processing systems_, 33:19884-19895, 2020.

* [20] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique for generalization in deep reinforcement learning. In _International Conference on Learning Representations_, 2020.
* [21] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. _The Journal of Machine Learning Research_, 17(1):1334-1373, 2016.
* [22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [23] Siao Liu, Zhaoyu Chen, Yang Liu, Yuzheng Wang, Dingkang Yang, Zhile Zhao, Ziqing Zhou, Xie Yi, Wei Li, Wenqiang Zhang, et al. Improving generalization in visual reinforcement learning via conflict-aware gradient agreement augmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23436-23446, 2023.
* [24] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Hadsell. Learning to navigate in complex environments. In _International Conference on Learning Representations_, 2017.
* [25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* [26] Karl Pearson. Vii. mathematical contributions to the theory of evolution.--iii. regression, heredity, and panmixia. _Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character_, (187):253-318, 1896.
* [27] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 3803-3810. IEEE, 2018.
* [28] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. _arXiv preprint arXiv:1710.06542_, 2017.
* [29] Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data augmentation for generalization in reinforcement learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 5402-5415. Curran Associates, Inc., 2021.
* [30] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* [31] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. _Journal of big data_, 6(1):1-48, 2019.
* [32] Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting in reinforcement learning. In _International Conference on Learning Representations_, 2020.
* [33] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. _arXiv preprint arXiv:1412.6806_, 2014.
* [34] Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite-a challenging benchmark for reinforcement learning from pixels. _arXiv preprint arXiv:2101.02722_, 2021.
* [35] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.

* [36] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.
* [37] Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. _Software Impacts_, 6:100022, 2020.
* [38] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* [39] Xudong Wang, Long Lian, and Stella X Yu. Unsupervised visual attention and invariance for reinforcement learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6677-6687, 2021.
* [40] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International Conference on Learning Representations_, 2021.
* [41] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10674-10681, 2021.
* [42] Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, and Huazhe Xu. Don't touch what matters: Task-aware lipschitz data augmentation for visual reinforcement learning. In Lud De Raedt, editor, _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 3702-3708. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.
* [43] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In _International Conference on Learning Representations_, 2021.
* [44] Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep reinforcement learning. _arXiv preprint arXiv:1804.06893_, 2018.
* [45] Zheng Zhao, Lei Wang, and Huan Liu. Efficient spectral feature selection with minimum redundancy. In _Proceedings of the AAAI conference on artificial intelligence_, volume 24, pages 673-678, 2010.
* [46] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.
* [47] Chuning Zhu, Max Simchowitz, Siri Gadipudi, and Abhishek Gupta. Repo: Resilient model-based reinforcement learning by regularizing posterior predictability. _Advances in Neural Information Processing Systems_, 36:32445-32467, 2023.
* [48] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In _2017 IEEE international conference on robotics and automation (ICRA)_, pages 3357-3364. IEEE, 2017.
* [49] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In _Aaai_, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.

Implementation Details

In this section, we describe the implementation details used for SimGRL. For a fair comparison, we followed the same architectural designs and hyperparameters used in [13], with only a minor difference in the encoder. For the feature-regularized encoder, we used 3 layers for the image encoder \(f^{1}\) with 16 channels and 8 layers with 32 channels for the feature encoder \(f^{2}\), leading to the encoder with a total of 11 layers that are the same layer number as the encoder used in [13]. Subsequently, we projected the convolutional features of the last layer to 100-dimensional linear vectors, which are fed into actor and critic heads with 1024-dimensional hidden layers. We updated the actor parameters by freezing the encoder, and the target network parameters using an exponential moving average (EMA) with a rate of 0.01 for the critic head and 0.05 for the encoder. Both actor and target networks were updated every 2 critic updates. We used a minibatch size of 128, stacked 3 image frames each of size \(3\times 84\times 84\) as input for RL, and employed the Adam optimizer [17]. For the maximum shift length \(L\) in the shifted random overlay augmentation, we employed \(L=6\), which leads to \(3\times 108\times 108\) size of a natural image \(\varepsilon\) before shifted cropping. In Fig. 9, we present examples of augmented images by the shifted random overlay. This figure exhibits that the proposed augmentation approach injects randomly moving backgrounds into the training images. All hyperparameters are summarized in Table. 2

\begin{table}
\begin{tabular}{l l} \hline SAC hyperparameters & Value \\ \hline Replay buffer capacity & 500,000 \\ Number of training steps & 500,000 \\ Frame size & 3\(\times\)84\(\times\)84 \\ Stacked frames & 3 \\ Evaluation episodes & 30 \\ Random shift & Up to \(\pm\)4 pixels \\ Action repeat & 2 (Finger, Spin) \\  & 8 (Cartpole, Swingup) \\  & 4 (otherwise) \\ Minibatch size & 128 \\ Discount factor (\(\gamma\)) & 0.99 \\ Optimizer & Adam (\(\beta_{1}=0.9\), \(\beta_{2}=0.999\) for \(\theta\) and \(\phi\)) \\  & Adam (\(\beta_{1}=0.5\), \(\beta_{2}=0.999\) for \(\alpha\) of SAC) \\ Learning rate & \(1e-3\) (\(\theta\) and \(\phi\)) \\  & \(1e-4\) (\(\alpha\) of SAC) \\ Actor (\(\phi\)) update frequency & 2 \\ Target (\(\hat{\theta}\)) update frequency & 2 \\ Target (\(\hat{\theta}\)) EMA rate & 0.01 (critic) \\  & 0.05 (encoder) \\ Number of conv layers & 3 (\(f^{1}\), Image encoder) \\  & 8 (\(f^{2}\), Feature encoder) \\ Number of conv filters & 16 (\(f^{1}\), Image encoder) \\  & 32 (\(f^{2}\), Feature encoder) \\ Projection dim & 100 \\ Hidden dim (MLP heads) & 1024 \\ Initial temperature & 0.1 \\ SVEA coefficient (\(\beta\)) & 0.5 \\ Weak augmentation & Random shift \\ Strong augmentation & Shifted random overlay (SRO) \\ Maximum shift length (\(l\)) in SRO & Up to \(\pm\) 6 pixels \\ \hline \end{tabular}
\end{table}
Table 2: Hyperparameters used in DMControl Suite.

Figure 9: Examples of augmented images using shifted random overlay (SRO).

[MISSING_PAGE_FAIL:15]

### Discussion on Impacts of Proposed Regularizations

We discuss the impacts of the proposed regularizations based on the attribution masks and the TID metrics. First, we investigate the impacts in terms of the imbalanced saliency in the 'Cartpole, Swingup' task. In the left plot of Fig. 11, both SimGRL-F and SimGRL-S show increased TID scores compared to SVEA. However, they exhibit different effects on TID variance as SimGRL-F significantly reduces it but SimGRL-S rather increases it. This indicates that the imbalanced saliency remains in SimGRL-S while SimGRL-F alleviates this issue. Therefore, the performance improvement of SimGRL-F can be interpreted as a mitigation of the imbalanced saliency problem, while the performance improvement of SimGRL-S is a bit ambiguous. For the reason of the result with SimGRL-S, we suggest that despite the persistence of imbalanced saliency, the explicit injection of dynamic background elements might have contributed to improved generalization in such conditions. Furthermore, even though it does not distinctly discriminate task objects between the stacked frames, the increased TID score of SimGRL-S can be attributed to its improved accuracy in identifying recognized objects in certain frames, as depicted in Fig. 12. This implies that the shifted overlay augmentation implicitly encourages the model to identify task objects more accurately. As a result, SimGRL, which integrates both architectural and data regularization, demonstrates that its agent can identify task-relevant objects across stacked frames with greater accuracy and discrimination.

In the right plot of Fig. 11, all of the SimGRL variants demonstrate remarkably increased TID scores in the 'Cheetah, Run' task. Since the 'Cheetah, Run' task has less movement of task objects than the 'Cartpole, Swingup' task, differences in TID variance values for this task are not effective. Therefore, the core issue in this task is the observational overfitting [32] to ground or shadow rather than imbalanced saliency between consecutive frames. The increase in the TID score implies an enhanced ability to identify task-relevant objects, which is demonstrated in Fig. 12. As expected, this shows that the application of the shifted random overlay augmentation encourages the agent to focus on the task object while disregarding background elements. On the other hand, it is noteworthy that the frame stack at the feature level also contributes to solving this problem. We suppose this is because encoding from a single image without temporal information allows the model to concentrate on each image's most salient visual features. Consequently, by integrating those strategies, the proposed approach, SimGRL, effectively addresses the two problems highlighted in this paper that hinder generalization in visual RL.

Figure 11: Plots of ‘TID variance (Train) vs TID score (Train)’ for ablation variants.

### Impact of Number of Layers in Image Encoder

While maintaining the number of layers in the entire encoder at 11, we conducted the ablation study for the number of layers in the image encoder. To consider only the impact of the layer number, we leveraged the SimGRL-F model which employs the original random overlay without shifting for the strong augmentation rather than the shifted version. Depending on the number of layers in the image encoder, we investigated test rewards, generalization gaps, and throughputs in the 'Walker, Walk' task, where the Video Hard domain was employed for testing environments. The generalization gap was evaluated by the difference between the training and testing performances, and the throughput was computed by the FPS processed in the simulator on a single NVIDIA TITAN RTX GPU. The results presented in Fig. 13 demonstrate that the incorporation of the feature-level frame stack, facilitated by the image encoder, enhances the model's generalization capabilities in dynamic scenes. It is noteworthy that even if the number of image encoder layers is only 1, the generalization gap is remarkably reduced. Additionally, the generalization gap decreases when the number of layers in the image encoder is increased, indicating that incorporating feature maps with higher abstraction enables the improvement of generalization. However, despite the improvement in generalization performance, there does not seem to be a significant increase in practical test performance, implying a reduction in the training performance through the mitigation of overfitting. This can also be attributed to the diminished depth of the feature encoder, which is crucial for encoding temporal information. Moreover, increasing the layer number in the image encoder results in decreased throughput, primarily due to the increased computational burden associated with processing individual frames. Considering both generalization capability and computational efficiency, we selected 3 layers for the image encoder and 8 layers for the feature encoder for the experiments.

Figure 12: Examples of attribution masking for ablation variants.

Figure 13: Impacts of the number of layers in image encoders.

### Different Data Augmentations

To investigate the impact of differently augmenting the stacked frames, we provide additional experiments under two types of additional augmentations: (1) randomly cropped patches from the same image and (2) completely different images. We denote the first method as _irregularly_ shifted random overlay (I-SRO) and the second method as _arbitrarily stacked_ random overlay (A-SRO). While the SRO augmentation crops patches in a regularly shifted manner as Eq. (5), I-SRO irregularly crops patches from a sampled natural image \(\varepsilon\). In contrast to Eq. (5), the coordinates \((h_{i},w_{i})\) of the upper left corner of each cropped patch become as follows:

\[(h_{i},w_{i})=(h_{1}+dh_{i},w_{1}+dw_{i}),\hskip 28.452756pti=2,...,n,\] (10)

where \(dh_{i},dw_{i}\sim\mathrm{unif}\{-l,l\}\). On the other hand, A-SRO samples different natural images \(\varepsilon_{i}\sim\mathcal{D}\) with \(H\times W\) sizes and augments each RL frame using the different images. The processes of I-SRO and A-SRO are illustrated in Fig. 14.

For clarity of the effect for each augmentation, we do not include the feature-level frame stack and compare them in the video hard level with SRO (_i.e._, SimGRL-S in the paper) that leverages _regularly_ shifted cropped patches from the same image for data augmentation. Table 4 shows that augmentations using irregularly shifted patches (I-SRO) or completely different patches (A-SRO) across the consecutive frames can achieve comparable performance with SRO using the regularly shifted patches, implying that augmenting each frame in a different manner (_i.e._, employing SRO, I-SRO, and A-SRO) can achieve significant performance improvement over augmenting each frame uniformly (_i.e._, RO) for visual RL generalization under dynamic scenes. Among such augmentations, we have employed the (regularly) shifted random overlay (_i.e._, SRO) as our primary approach for data augmentation considering its superior averaged performance.

Figure 14: Additional data augmentation approaches for ablation study.

[MISSING_PAGE_FAIL:19]

Figure 16: Training curves on the Video Easy testing level.

Figure 17: Training curves on the Video Hard testing level.

### Results on DistractingCS

We provide the additional benchmark results on the Distracting Control Suite (DistractingCS) [34], where camera pose, background, lighting, and colors continually vary throughout an episode. In Fig. 18, we depict some examples of the DistractingCS in intensity levels = \([0.1,0.3,0.5]\). In this experiment, we utilized the same hyperparameters used in DMControl-GB. Fig. 19 demonstrates SimGRL's significantly improved generalization performances in low intensities, compared to DrQ and SVEA. In particular, SimGRL is the only method that reported practical performances in the 'Ball in Cup, Catch' and 'Finger, Spin' tasks. Furthermore, SimGRL achieves additional performance gains in the 'Walker, Walk, Catch' and 'Walker, Stand' tasks. On the other hand, despite the still low performance in 'Cartpole, Swingup', there are clear signs of improvement. These results indicate the robustness of our proposed approach not only to dynamic background changes but also to various forms of distortions, including camera pose variations at moderate intensities. Finally, we note that generalization in extremely distorted environments, such as those with high intensities in the DistractingCS, remains an open problem and will be a subject of promising future research.

Figure 19: Performance on DistractingCS. Each score reports the mean over 5 seeds.

Figure 18: Examples of DistractingCS benchmark.

[MISSING_PAGE_FAIL:22]

## Appendix C Algorithm for SimGRL

```
1:Hyperparameters: Total number of training steps \(T\), mini-batch size \(N\), learning rate \(\eta\), target network update rate \(\lambda\), actor and target network update frequency \(M\), transformation function set \(\mathcal{V}\) for random shift, natural image dataset \(\mathcal{D}\) for SRO, critic loss coefficient \(\beta\), maximum shift length \(l\) in SRO, replay buffer \(\mathcal{B}\), discount factor \(\gamma\).
2:Initialize: encoder and critic parameters \(\theta\), actor parameters \(\phi\), target network parameters \(\hat{\theta}\leftarrow\theta\).
3:for\(\mathrm{timestep}\,t=1...T\)do
4:\(a_{t}\sim\pi_{\phi}(\cdot|f_{\theta}(s_{t}))\)\(\triangleright\) Sample action from actor (= policy)
5:\(s^{\prime}_{t}\sim\mathcal{P}(\cdot|s_{t},a_{t})\)\(\triangleright\) Sample transition from environment
6:\(\mathcal{B}\leftarrow\mathcal{B}\ \cup(s_{t},a_{t},r(s_{t},a_{t}),s^{\prime}_{t})\)\(\triangleright\) Add transition to replay buffer
7:\(\{(s_{i},a_{i},r(s_{i},a_{i}),s^{\prime}_{i})\}_{i=1}^{N}\sim\mathcal{B}\)\(\triangleright\) Sample batch from replay buffer
8:for\(i=1...N\)do
9:\(s_{i}=r(s_{i};\nu_{i})\), \(s^{\prime}_{i}=\tau(s^{\prime}_{i};\nu^{\prime}_{i})\), \(\nu_{i},\nu^{\prime}_{i}\sim\mathcal{V}\)\(\triangleright\) Apply random shift augmentation
10:\(q^{tgt}_{i}=r(s_{i},a_{i})+\gamma Q_{\hat{\theta}}(f_{\hat{\theta}}(s^{\prime}_{i} ),a^{\prime}_{i})\), \(a^{\prime}_{i}\sim\pi_{\phi}(\cdot|f_{\theta}(s^{\prime}_{i}))\)\(\triangleright\) Compute target Q-value
11:\(s^{aug}_{i}=\tau^{SRO}(s_{i};l,z_{i})\), \(\varepsilon_{i}\sim\mathcal{D}\)\(\triangleright\) Apply SRO augmentation
12:endfor
13:\(\mathcal{L}_{Q}(\theta)=\frac{1}{N}\sum_{i=1}^{N}\left[\beta(Q_{\theta}(f_{ \theta}(s_{i}),a_{i})-q^{tgt}_{i})^{2}+(1-\beta)(Q_{\theta}(f_{\theta}(s^{aug }_{i}),a_{i})-q^{tgt}_{i})^{2}\right]\)
14:\(\triangleright\) Compute SimGRL loss
15:\(\theta\leftarrow\theta-\eta\nabla_{\theta}\mathcal{L}_{Q}(\theta)\)\(\triangleright\) Optimize encoder and critic for SimGRL loss
16:if every \(M\) step then
17:\(\phi\leftarrow\phi-\eta\nabla_{\phi}\mathcal{L}_{\pi}(\phi)\)\(\triangleright\) Optimize actor for actor loss
18:\(\hat{\theta}\leftarrow\lambda\theta+(1-\lambda)\hat{\theta}\)\(\triangleright\) Update target network using EMA
19:endif
20:endfor
21:Note: \(f_{\theta}(s_{t})\) represents \(f_{\theta}^{2}([f_{\theta}^{1}(o_{t-2}),f_{\theta}^{1}(o_{t-1}),f_{\theta}^{1} (o_{t})])\), where \(s_{t}=(o_{t-2},o_{t-1},o_{t})\). ```

**Algorithm 1** A **Sim**ple Framework for **G**eneralization in visual **RL** (SimGRL).

## Appendix D Analysis of Attentions

In this paper, we demonstrated that it is important for the RL model not to be disturbed by task-irrelevant backgrounds for better policy generalization. To do this, we assumed that the RL agent should at least focus more on the task object than on the background, and employed the attribution masks to analyze whether the trained RL models correctly distinguish the task objects and backgrounds. Additionally, to quantitatively analyze how well RL models can distinguish between the task object and the background, we proposed the TID metrics.

Figure 21: Attention maps were obtained from the softmax of the saliency maps before thresholding by quantile to compute the mask. The attribution masks indicate that SimGRL can effectively distinguish task-relevant objects and backgrounds, implying mitigation of the imbalanced saliency and observation overfitting. Furthermore, the attention maps indicate that SimGRL can focus on the significant parts of the task objects. In contrast, the SVEA baseline focuses on the task-irrelevant background parts.

In practice, RL policies may not make decisions based on the entire object. Instead, they could care about a few key points within such objects that are most predictive of the reward. To investigate whether the ability to distinguish between task objects and backgrounds also helps in identifying the key points that influence decisions, we provide attention maps obtained from the softmax of the attribution maps \(M(Q_{\theta},s_{t},a_{t})=\frac{\partial Q_{\theta}(s_{t},a_{t})}{\partial s_{t}}\) before thresholding for the binarized masks. Fig. 21 shows that the identification ability for the task objects of SimGRL also leads to focus on the most important key point parts such as the pole and body of the 'Cartpole' or legs of the 'Cheetah' in the attention maps. On the other hand, the SVEA baseline incorrectly considers the background features as key points.

## Appendix E Comparison to Robust RL Algorithms

In response to the recommendation of the reviewers to investigate whether the pitfalls also apply to robust RL algorithms such as DBC [43], TIA [7], DRIBO [6], and RePo [47], we provide comparative experimental results for these methods. While our work focuses on addressing the generalization issue in a purely _model-free_ vision-based RL setting, which requires the frame stack to encode temporal information, the robust RL approaches aim to explicitly learn robust representations against background distractions by utilizing _model-based_ RL. To encode temporal information, these approaches, except for DBC, train transition dynamics by leveraging complex recurrent neural network (RNN) encoders which take only a single frame as input to the encoder, thus applying a concept similar to feature-level frame stacking. On the other hand, DBC uses a CNN encoder that takes stacked frames and trains a dynamics model that predicts the latent states of the next stacked frames. Fig. 22 shows that methods like TIA, DRIBO, and RePo, which use RNN-based encoders, do not struggle with the imbalanced saliency problem, successfully identifying each salient object across consecutive frames like SimGRL. In contrast, despite employing robust representation learning, DBC still struggles with the imbalanced saliency problem, showing a bias toward identifying pixels from the object in the most recent frame as salient across consecutive frames.

Figure 23: Training curves of DBC, TIA, DRIBO, RePo, SVEA, and SimGRL. In the test stage, we used the video-hard version for the test environments. Compared to robust RL approaches that employ model-based representation learning strategies, the proposed SimGRL achieves both sample efficiency and test performance even with the simple structure.

Figure 22: The masking results indicate that DBC, which uses the image-level frame stack, struggles with the imbalanced saliency problem whereas the TIA, DRIBO, and RePo, which feed single images to a recurrent state-space model (RSSM) [10; 9] encoders sequentially, can distinctly identify salient pixels of each frame.

Fig. 23 shows the curves of zero-shot test performances for the 'Cartpole, Swingup' task in the video-hard environment according to the number of training frames. For a fair comparison, we applied the random overlay augmentation to the robust RL algorithms where following to [13], the data augmentation was applied only during the representation learning stage. As anticipated, robust RL algorithms like DRIBO and RePo, which can distinctly identify task-relevant objects, achieved better test performance (except for TIA) than methods that struggle with imbalanced saliency, such as SVEA and DBC. We suggest that the reason for the inferior performance of TIA is that the TIA agent struggles to train the Distractor Model, which explicitly captures the background, due to the more diverse background augmentations compared to the relatively limited video backgrounds from the Kinetics dataset [16] used in its original experimental setup. Notably, despite its straightforward structure and absence of additional representation learning stages, the proposed SimGRL achieved both sample efficiency and strong test performance, outperforming robust RL approaches like DRIBO and RePo that rely on model-based representation learning strategies.

## Appendix F Details on TID Metrics

### Dataset for TID Evaluation

To fairly compare TID metrics of comparison methods on the same images, we constructed a dataset for the evaluation in advance. As illustrated in Fig. 24, we excluded samples that were excessively static because the side effect of imbalanced saliency is diminished in such samples. Additionally, we excluded samples where the task object was not fully visible. Instead, we included dynamic samples to fully evaluate the impact of imbalanced saliency. To this end, as the samples obtained from random actions were less dynamic, we selected the images by running SVEA's policies, our baseline algorithm, up to 100K steps to obtain a variety of dynamic samples. Using those images, as depicted in Fig. 25, we constructed 60 pairs of images and ground truth (GT) masks of task objects for each training and testing environment in Video Hard. These GT masks are employed to count \(N_{obj}\) and \(N_{obj_{M}}\) in Eq. (8). Here, \(N_{obj}\) represents the pixel number in the GT mask while \(N_{obj_{M}}\) is determined by the pixel number in the overlapping regions between the GT and attribution masks.

Figure 24: To evaluate the TID metrics on suitable samples, we excluded overly static or partially visible images for task objects.

Figure 25: Examples in a dataset for the TID evaluation. (First and Second Row) Images from training environments. (Third and Fourth Row) Images from test environments in Video Hard.

### Impact of Quantile Value

In Fig. 26, the impact of quantile \(\rho\) values for the \(\rho\)-quantile attribution mask [2] is illustrated. Additionally, the effect of the \(\rho\) value on the TID score can be explained as follows. When the value of \(\rho\) is too small, the region of a mask is enlarged, thus making it easier for task objects to be included in the attribution mask. This corresponds to the increased \(N_{obj_{M}}\), leading to an increased \(\frac{N_{obj_{M}}}{N_{obj_{M}}}\) in the first component of the TID score in Eq. (8). Simultaneously, this also increases \(N_{M}\), balancing the TID score by decreasing \(\frac{N_{obj_{M}}}{N_{M}}\) that is the second component of the TID score. Conversely, a large value of \(\rho\) reduces the size of the mask \(N_{M}\) as in Fig. 26, consequently reducing \(N_{obj_{M}}\). On the other hand, the decreased \(N_{M}\) might increase \(\frac{N_{obj_{M}}}{N_{M}}\), balancing the TID score. Therefore, for the quantile value used to evaluate the TID score and variance, we used the optimal quantile value computed by \(\rho=1-\frac{N_{obj}}{(3\times C\times H\times W)}\) rather than a hand-designed value. However, we used \(\rho=0.95\) in our qualitative illustrations of the attribution masks, as it empirically provided the most plausible visual representation of a masked region.

### Full Results of TID Evaluation

We present comprehensive results of the TID evaluation, including a comparison of training TID variance, test TID score, and generalization performance against the training TID score. For the 'Walker' agent, we evaluated the TID scores on the 'Walk' task rather than 'Stand'. Fig. 27(a) reports higher TID scores and lower TID variances of our approach, indicating that SimGRL can distinctly identify the salient pixels across the stacked frames for all tasks. In particular, for the 'Ball In Cup' agent, measuring the TID scores accurately is difficult due to the relatively small size of the salient object. Nevertheless, SimGRL shows remarkably higher TID with lower variance scores than the competitors. These results demonstrate that SimGRL can effectively alleviate the two highlighted issues, 'imbalanced saliency' and 'observational overfitting'. In Fig. 27(b), we can observe that the high identification ability of the task object in the training environment can lead to a similar capability in test environments. Finally, Fig. 27(c) shows a positive correlation between generalization performance and the TID score, where smaller generalization gaps correspond to higher generalization performance. For the distributions in Figures 27(b) and 27(c), we present Pearson correlation coefficients [26] in Table 7. These results imply that the high identification

Figure 26: Impacts of quantile values \(\rho\) used for thresholding.

[MISSING_PAGE_EMPTY:27]

### Further Examples of Attribution Masking

For the existing state-of-the-art visual RL approaches for generalization, we provide rich examples of the attribution masks and the corresponding masked observations, where we used the images employed in the TID evaluations. Additionally, we used the quantile \(\rho=0.95\) across all examples. Figures 28 and 29 depict the examples for training and Video Hard testing environments, respectively.

Figure 28: Examples of the attribution masks and masked results for given observations in the training environments.

[MISSING_PAGE_EMPTY:30]

Figure 29: Examples of the attribution masks and masked results for given observations in the test environments of Video Hard.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction include the claims made in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] Justification: The paper has limitations, but those are not discussed in the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper includes experimental result reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The paper includes experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental setting is presented in the paper in detail. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper includes the results that are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper includes experiments compute resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research was conducted with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no safeguards risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Creators or original owners of assets used in the paper, are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.