# Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data

 Kai Helli\({}^{*,1,2}\) David Schnurr\({}^{*,1,3}\) Noah Hollmann\({}^{1,4}\) Samuel Muller\({}^{1}\) Frank Hutter\({}^{5,1}\)

\({}^{1}\) University of Freiburg, \({}^{2}\) Technical University of Munich, \({}^{3}\) ETH Zurich,

\({}^{4}\) Charite University Medicine Berlin, \({}^{5}\) ELLIS Institute Tubingen, \({}^{*}\) Equal contribution.

Correspondence to kai.helli@tum.de

###### Abstract

While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.

## 1 Introduction

In traditional machine learning the train and test data are assumed to be sampled from the same distribution [1]. However, this assumption of independent and identically distributed (i.i.d.) data is commonly violated in real-world scenarios due to distribution shifts, resulting in performance degradation of standard machine learning (ML) models over time [1, 2]. Research in the area of temporal domain generalization (Temporal DG) tries to address these shifts by developing methods that perform consistently across temporal domains and generalize beyond the training regimen, i.e. into the future. In fields such as healthcare, climate science, or finance, data is most often organized in a tabular format [3, 4]. Here shifts are driven by hidden variables such as policy or climate changes, equipment updates, seasonal changes, or activity cycles, limiting real-world model deployment [2].

Young and Steele [5] describe declining mortality quantification in a hospital system while Pasterkamp et al. [6] show deterioration of cardiovascular risk models over time, leading to increased mortality; Ganesan et al. [7] show the COVID-19 pandemic exacerbated this issue, as ICU mortality prediction models failed to adapt to the unique characteristics of COVID-19 patients; environmental models need to continually adapt to climate changes [8]; fraud detection models need to continuously adaptas the strategies of fraudsters adapt to the models themselves [9]; these feedback loops often arise in practice, where model deployment inherently causes a system change [10].

Robustness to such distribution shifts stands as a prominent challenge in current ML research [11]. So far multiple approaches have been proposed to address temporal distribution shifts using neural networks (NNs) [11, 12, 13]. However, modeling distribution shifts in tabular data presents a two-fold challenge: (i) NNs have struggled to model and extrapolate distribution shifts to date [11, 14] (ii) approaches for modeling distribution shifts have mostly employed NNs, while tree-based methods have consistently outperformed NNs in handling tabular data [3, 15, 16, 17] - leaving a wide methodological gap in addressing this common real-world scenario.

We provide a fresh perspective on predicting given distribution shifts by leveraging in-context-learning (ICL) to learn the prediction algorithm itself - bypassing many challenges encountered in this setting. Our approach builds on the foundation of Prior-Data Fitted Networks (PFNs; 18) and TabPFN (19; see Section 3.1). PFNs leverage large-scale ML and ICL techniques to approximate Bayesian inference accurately for any prior that can be sampled from. They are trained on millions of synthetic datasets sampled from this prior. For each such dataset, a supervised learning task is constructed, and the model is asked to predict on held out test samples. Then, the PFN is able to apply the principles learned on this synthetic data to real-world datasets, effectively having learned a prediction algorithm.

This paper introduces _Drift-Resilient TabPFN_, an adaptation of the TabPFN framework tailored for tabular datasets exhibiting temporal distribution shifts. Our idea is as follows: Data distribution shifts can be modeled as gradual changes to the structural causal model (SCM; 20; 21) underlying the data. By including the assumption that underlying models change over time into the approximated prior, our models learn to estimate, adapt to, and extrapolate these model changes. While likely no causal model exactly underlies real-world data, we find this approximation to be empirically and intuitively useful. Specifically, we suggest shifting edge weights between nodes that affect causal relationships while sampling instances in each dataset. To accurately model these edge shifts, we propose using a \(2^{\text{nd}}\)-order SCM, a secondary model that adjusts the primary SCM, that takes a time indicator as input and outputs the weight shifts. To represent our temporal domain indicators, we use Time2Vec [22].

Figure 1: High-level overview of our method. We train a transformer that accepts entire datasets as input to learn the learning algorithm itself by training on millions of synthetic datasets once as part of algorithm development. The trained model can be applied to arbitrary real-world datasets. In (b), X, c, and y refer to features, time domain, and label respectively. In (c), we show predictions on test domains 4 (left) and 5 (right), where we see a distribution shift. Drift-Resilient TabPFN accurately updates decision boundaries in this example.

To validate the robustness and adaptability of our approach, we conduct a comprehensive evaluation of 18 tabular datasets. Among these, 8 are synthetic toy problems, that allow us to analyze model behavior in detail. The remaining 10 are real-world datasets, each representative of different temporal distribution shifts. Our model is benchmarked against (i) state-of-the-art methods for handling temporal domain generalization (implemented in the Wild-Time benchmark [11]), as well as (ii) state-of-the-art methods for tabular data, that do not handle distribution shifts explicitly (the unmodified version of TabPFN, well-established classifiers such as XGBoost, Catboost, and LightGBM [23; 24; 25]). In addition, we qualitatively analyse the model's behavior and prediction characteristics.

Our model consistently outperforms all baselines on out-of-distribution data across both synthetic and real-world datasets while simultaneously exhibiting strong calibration. Qualitatively, we find that Drift-Resilient TabPFN dynamically adjusts its decision boundary over time, surpassing existing models in leveraging temporal domain information to extrapolate shifts far into the future and capture trends effectively.

Key contributions of this paper are:

1. We provide a novel perspective to learning under distribution shifts: Rather than specifying how exactly out-of-distribution data should be handled, we leverage in-context learning to learn an algorithm that handles such scenarios.
2. We propose a prior for temporal shifts, based on sparse, non-linear, and correlated mechanism shifts in structural causal models. This unifies shift types (covariate shift, prior probability shift, concept shift, and combinations thereof) and allows to extrapolate these changes.
3. We release Drift-Resilient TabPFN, a tabular data model for predicting under temporal distribution shifts, that automatically recognizes and adapts to shifts in the data and requires no hyperparameter tuning.
4. We extensively evaluate our models and state-of-the-art baselines on 18 synthetic and real-world datasets demonstrating improved out-of-distribution performance while requiring, on average, only 10.9s for training and prediction combined.

## 2 Background

### Distribution Shift Settings

Consider \(\mathcal{X}\), \(\mathcal{Y}\), and \(\mathcal{C}\), the sample spaces for features, labels, and domain indices, respectively. In these spaces, specific instances are represented by \(\bm{x}\), \(y\), and \(\bm{c}\). The corresponding random variables for these instances are denoted as \(X\), \(Y\), and \(C\).

A dataset is then a collection of \(n\) tuples, denoted as \(\mathcal{D}:=\{(\bm{x}_{i},y_{i},\hat{\bm{c}}_{k})\}_{i=1}^{n}\). Each tuple is thereby drawn from a conditional distribution \(\mathbb{P}(X,Y\mid C=\bm{c}_{k})\) over the spaces \(\mathcal{X}\times\mathcal{Y}\). Here, \(\bm{c}_{k}\in\mathcal{C}\) serves as a domain index that conditions the distribution from which the respective sample is drawn. Given that the true underlying temporal domain is mostly unknown in real-world data, it often is approximated and represented as \(\hat{\bm{c}}_{k}\) within the dataset. To isolate samples from a specific domain \(\hat{\bm{c}}_{k}\), we introduce the sub-dataset \(\mathcal{D}_{\hat{\bm{c}}_{k}}\), defined as \(\mathcal{D}_{\hat{\bm{c}}_{k}}:=\{(\bm{x},y,\hat{\bm{c}})\in\mathcal{D}|\hat{ \bm{c}}=\hat{\bm{c}}_{k}\}\). This adapted notation allows for a more nuanced analysis of datasets with domain shifts, building upon the frameworks presented by Wang et al. [26] and Sheth et al. [27].

Domain Generalization (DG).DG aims to train a model that generalizes across a continuum of source domains \(\mathcal{C}^{\text{train}}\) to the target domains \(\mathcal{C}^{\text{test}}\) without access to samples of the target domains. The objective is to learn a mapping function \(f:\mathcal{X}\times\mathcal{C}\rightarrow\mathcal{Y}\) that minimizes the expected loss when applied to new, previously unseen target domains.

Temporal Domain Generalization (Temporal DG).In temporal DG, a special case of DG, the domain index set \(\mathcal{C}\) is one-dimensional and follows a total ordering, \(c_{1}\leq c_{2}\leq\ldots\)[12]. In this framework, the training set is limited to source domains that precede target domains in this ordering, namely \(\mathcal{C}^{\text{train}}=\{c_{1},c_{2},\ldots,c_{t}\}\). In this setting, the objective is to learn a predictive model \(f\) that performs well on these source domains while also robustly generalizing to future, unseen domains \(\mathcal{C}^{\text{test}}=\{c_{t+1},c_{t+2},\ldots,c_{n}\}\). The indices of all training domains \(\mathcal{C}^{\text{train}}\) and the index of the current testing domain \(c_{k}\in\mathcal{C}^{\text{test}}\) are provided to the model.

### Related Work

While DG has drawn increasing attention in the research community [28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38], its temporal variant remains under-explored. We review existing DG benchmarks on tabular data, DG methods, the temporal DG benchmark Wild-Time [11], temporal DG methods, as well as relevant TaboPFN studies.

**Tabular Distribution Shift Benchmarks.** Several benchmarks have been introduced to assess methods for tabular data under distribution shifts. (1) **Shifts** and **Shifts 2.0**[39; 40] are uncertainty focused benchmarks. **Shifts 2.0** includes five tasks, two of which involve tabular data subject to temporal and spatio-temporal shifts. (2) **WhyShift**[41], focusing on spatio-temporal shifts, offers five real-world tabular datasets, including the ACS dataset [42], which we also use in a subsampled form. Evaluating 22 methods like Gradient Boosted Decision Trees (GBDTs), MLPs, and robustness techniques, WhyShift finds that robustness methods do not consistently enhance out-of-distribution (OOD) performance. They also find that while GBDTs perform better, the gap between in-distribution (ID) and OOD performance persists, likely due to GBDTs being better fitted to the ID distribution. (3) **TableShift**[14] includes 15 tabular binary classification tasks, with 10 relevant to DG. Their work evaluates 19 model types, including several DG methods, finding that methods tailored for distribution shifts do not consistently outperform GBDTs. While generalization gaps can be slightly reduced, each robustness method comes at the cost of ID performance. (4) **Wild-Tab**[43] focuses on domain generalization within tabular regression using three large datasets. Their study compares 10 generalization techniques against standard Empirical Risk Minimization (ERM) applied to MLPs. Similar to previous work [44], they find that ERM was not consistently outperformed by specialized DG methods. Notably, no advantage of GBDTs over ERM on MLPs was observed in their datasets.

Unlike these benchmarks, which focus on large-scale datasets, our work addresses the overlooked challenges of small-scale temporal distribution shift datasets. However, their insights on generalization and robustness methods align with our findings, providing valuable context for our approach.

**DG Methods.** Several techniques have been proposed to improve robustness to distribution shifts in DG [45]. Key approaches include domain-invariant learning methods, such as **Deep CORAL**[35], **IRM**[31], and **DANN**[36], which aim to learn representations that generalize across domains. Data augmentation strategies, like **Mixup**[33] and **LISA**[34], contribute to generalization by generating synthetic data variations. Additionally, robust optimization techniques, including **VRev**[37], **GroupDRO**[32], **EQRM**[38], and **SWA**[46], aim to improve performance under distributional shifts by optimizing for worst-case scenarios or incorporating model uncertainty.

In relation to our work, data augmentation strategies are conceptually similar, as we also teach the model DG rather than adding invariances to the architecture. In contrast to augmentation, though, we learn to generalize using completely artificial data on the meta level rather than through manipulation of the target dataset. The other methods focus on designing models specifically for DG, while our approach completely relies on the model to learn to handle distribution shifts. While similar to continual meta-learning (CML) approaches, our method uses domain identifiers to generalize to unseen domains, whereas CML continuously adapts to evolving contexts without identifiers [47].

**Wild-Time Benchmark.** Wild-Time [11] is a benchmark of five datasets designed to study the real-world effects of temporal distribution shifts - an area largely overlooked by previous benchmarks. While Wild-Time primarily uses non-tabular data, it evaluates a wide array of techniques on its tabular dataset, including classical supervised learning (ERM), fine-tuning, and several previously mentioned general DG methods adapted to temporal distribution shifts. Despite the diversity of methods evaluated, Wild-Time reveals a significant performance gap between ID and OOD data, with none of the 13 tested methods consistently outperforming the standard ERM approach.

In our evaluation, we employ their evaluation strategy _Eval-Fix_ and also benchmark our approach against the methods they considered. An in-depth overview of these methods is given in Appendix A.6.

**Temporal DG methods.** Recent specialized temporal DG methods include: (1) **DRAIN**[12], which employs a Bayesian framework alongside a recurrent neural network for predicting the dynamics of the model parameters across temporal domains. (2) **GI**[13], which explicitly incorporates the temporal domain as a feature, using a specialized Gradient Interpolation loss function, a time-sensitive activation, and enhanced domain reasoning via Time2Vec preprocessing [22]. However, both DRAIN and GI are limited in their ability to extrapolate beyond the near future, leading to their exclusion from our main evaluation. Notably, on the Rotated Two Moons Dataset - where DRAIN and GI have demonstrated their capabilities - our approach outperforms both. See Appendix A.4.3.4 for details.

Recent TabPFN Studies.To overcome the limitations of TabPFN in handling large samples and feature sets, typically found in DG benchmarks, several improvements have been proposed. Ma et al. [48] introduced a data distillation approach, where a distilled dataset serves as the model's context, optimized to maximize the training-data likelihood. Similarly, TuneTables [49], uses prompt-tuning to compress large datasets into a smaller context. Either of these methods could potentially be combined with Drift-Resilient TabPFN, as our modifications focus primarily on the pre-training phase, which remains unchanged in their approaches.

Another TabPFN variation, ForecastPFN [50], also introduces time dependence, but it does not consider any features. It only models simple time series data. Unlike our approach, which builds on TabPFN's SCM architecture for pre-training to handle a large set of features, ForecastPFN models synthetic time series using a single handcrafted function with sampled hyperparameters, simplifying the architecture while trading off diversity of the synthetic datasets.

## 3 Methodology

Our approach is built on PFNs, which use ICL to learn the learning algorithm itself. This approach also has a theoretical foundation as described by Muller et al. [18]: It can be viewed as approximating Bayesian prediction for a prior defined by the synthetic datasets. The trained PFN will approximate the posterior predictive distribution (PPD) and thus return a Bayesian prediction for the specified distribution over artificial datasets used during PFN training.

### Structural Causal Model Prior

Hollmann et al. [19] introduce a prior based on Structural Causal Models (SCMs; 20; 21) to model complex feature dependencies and potential causal mechanisms underlying tabular data. To sample one dataset, this prior samples an SCM which is then used to sample the examples in the dataset. In this approach, each causal representation of a sampled SCM is converted into a functional representation to enable forward computation and dataset sampling.

Consider a sampled SCM graph \(\mathcal{G}=(Z,R)\), where \(Z=\{z_{1},\ldots,z_{n}\}\) are the nodes (mechanisms) and \(R=\{r_{1},\ldots,r_{m}\}\) the edges (relationships). Each node is set using an individual assignment function \(f_{i}\), \(z_{i}=f_{i}(\{z_{j}\mid j\in\text{PA}_{i}\},\epsilon_{i})\), with PA\({}_{i}\) being the parent nodes of \(z_{i}\) and \(\epsilon_{i}\) random noise.

In the following, we will detail the underlying functional graph representation \(\tilde{\mathcal{G}}\) of an SCM \(\mathcal{G}\) sampled from the prior. We will later on apply distribution shifts on it. This representation is one level below the SCM and all nodes represent a single scalar value and all edges correspond to linear connections. The values of each node \(v\) in \(\tilde{\mathcal{G}}\) are set just like neurons in a neural network as a simple weighted sum and an activation function \(v:=h_{j}\left(\sum_{v^{\prime}\in\text{PA}_{j}}w_{i,j}v,\epsilon_{j}\right)\), where the scalar weights \(w_{i,j}\) and the activation function \(h_{j}\) are randomly chosen at graph creation and the noise term \(\epsilon_{j}\) is sampled in each forward propagation to create a variety of samples within a dataset.

The functional graph representation \(\tilde{\mathcal{G}}\) of an SCM \(\mathcal{G}\) is defined as follows:

1. **Node Expansion.** Each node \(z_{i}\) is expanded into a set of subnodes \(Z_{i}:=\{\tilde{z}_{i}^{1},\ldots,\tilde{z}_{i}^{k}\}\), each representing a scalar value.
2. **Graph Expansion.** The following steps are performed for each node \(z_{j}\) with a non-empty parent set PA\({}_{j}\): 1. A new set of subnodes \(F_{j}:=\{\tilde{f}_{j}^{1},\ldots,\tilde{f}_{j}^{l}\}\) is added. 2. We add the edges \(E_{j}:=\{Z_{i}\mid i\in PA_{j}\}\times F_{j}\cup F_{j}\times Z_{j}\).

Finally, the functional graph representation is defined as \(\tilde{\mathcal{G}}:=\left(\bigcup_{i\in\mathcal{I}}Z_{i}\cup\bigcup_{j\in \mathcal{I}}F_{j},\bigcup_{i\in\mathcal{I}}E_{i}\right)\).

The features of our dataset are defined by a random subset \(X\subseteq\bigcup_{i\in\mathcal{I}}Z_{i}\) and the target is defined by a randomly chosen \(y\in\bigcup_{i\in\mathcal{I}}Z_{i}\backslash X\). Samples in a dataset are now generated by (i) sampling all noise terms \(\epsilon_{j}\) in \(\tilde{\mathcal{G}}\), (ii) propagating the values of all nodes through the graph, and finally (iii) retrieving values at the feature and target nodes. The resulting datasets, thus abide the computational flow of both \(\mathcal{G}\) and \(\tilde{\mathcal{G}}\). For a more intuitive understanding of this construction, refer to the simplified example provided in Figure 2.

### Inducing Temporal Robustness in SCMs

We extend TabPFN's prior to model distribution shifts, allowing the model to expand its posterior predictive distribution (PPD) calculations to incorporate temporal domain information. We propose modeling the dynamic of edge shifts using a _\(2^{\text{nd}}\)-order SCM_. This \(2^{\text{nd}}\)-order SCM is itself an SCM with feature nodes specifying the magnitude of edge shifts in the base SCM's functional graph.

Specifically, to extend the scope of the functional representation of the SCM \(\tilde{\mathcal{G}}\), we introduce a version \(\tilde{\mathcal{G}}_{c_{k}}\), depending on the temporal domain \(c_{k}\in\mathcal{C}\). Our objective is to construct a time-dependent dataset \(\mathcal{D}\) which is an aggregation of individual datasets \(\mathcal{D}_{c_{1}},\mathcal{D}_{c_{2}},\ldots,\mathcal{D}_{c_{n}}\), where each dataset \(\mathcal{D}_{c_{k}}:=\{(\bm{x}_{i},y_{i},c_{k})\}_{i=1}^{n_{c_{k}}}\) contains \(n_{c_{k}}\) samples. To do this, we sample the temporal domains \(\mathcal{C}=\{c_{1},c_{2},\ldots,c_{n}\}\) and for each domain \(c_{k}\), we sample the number of samples \(n_{c_{k}}\) it contains. An illustration of exemplary domain distributions across datasets is shown in Figure 14.

We then select a sparse subset of relationships in the causal representation of the SCM \(\mathcal{G}\) to undergo temporal shifts based on the evidence that sparse shifts allow for causal reasoning [1]. For these selected edges, the \(2^{\text{nd}}\)-order SCM \(\mathcal{H}\) is used to sample shift parameters that govern the corresponding edges in the functional representation \(\tilde{\mathcal{G}}_{c_{k}}\). See Algorithm 1 for a high-level overview.

Employing a \(2^{\text{nd}}\)-order SCM for Edge Shifting.The \(2^{\text{nd}}\)-order SCM \(\mathcal{H}\) takes temporal domains \(\mathcal{C}\) as input and, through a single forward pass on the corresponding functional representation \(\tilde{\mathcal{H}}\), produces dynamic edge shifts for each edge weight \(w_{i,j}\) in \(\tilde{\mathcal{G}}\) that corresponds to an edge in \(\mathcal{G}\) that should be shifted. Note that although we perform a forward pass, there is no backward pass associated with it. Each \(2^{\text{nd}}\)-order SCM is randomly generated and used solely for sampling the weight shifts. This design allows for Bayesian reasoning over the edge shifts and enables \(\tilde{\mathcal{H}}\) to generate complex, often correlated shifts over time. While we emphasize this construction does not reflect the true underlying dynamics of edge shifts in many real-world datasets, we use this heuristic prior construction for the following reasons: (a) Features in the SCM are correlated to each other in varying degrees, mimicking real-world processes of correlated changes in the underlying generating mechanisms of real-world data (b) An SCM with NN based causal mechanisms and nonlinear activation functions can extrapolate values outside of the data distribution, generalizing the underlying function to future domains (c) As demonstrated by TabPFN, a causal model prior is a sufficiently general approximation to many real-world datasets. We have visualized this approach in Figure 3 and a selection of the functions generated by a \(2^{\text{nd}}\)-order SCM in Figure 15.

Shifting SCM edges can model various Types of Distribution Shifts.A distribution shift is characterized by changing distributions between contexts (\(\mathbb{P}(X,Y\mid C=\bm{c}_{i})\neq\mathbb{P}(X,Y\mid C=\bm{c}_{k})\)). This definition can be further broken down into the following scenarios:

_Covariate Shift_: Changes in the feature distribution (\(\mathbb{P}(X)\)) while the conditional label distribution (\(\mathbb{P}(Y|X)\)) remains constant.

_Prior Probability Shift_: Changes in the label distribution (\(\mathbb{P}(Y)\)) while the feature distribution given the labels (\(\mathbb{P}(X|Y)\)) remains constant.

_Concept Shift_: Changes in the relationship between features and labels (\(\mathbb{P}(Y|X)\) or \(\mathbb{P}(X|Y)\)) while the marginal distributions of features or labels remain constant.

Figure 2: Illustrative transformation of an SCM to one exemplary functional representation. Shaded nodes indicate that their activations cannot be sampled. Feature nodes are blue, the target node is green, input/noise nodes are purple, and all others are gray. The figure also shows the mapping of shifted edges between a causal relationship and its functional form in red, ensuring that shifts specifically target the intended causal relationships without affecting others.

These shifts can also be viewed as Bayesian networks shown in Figure 4 - all arising in our prior by sampling features and targets at varying SCM positions. For further visualizations, refer to Figure 13.

Encoding the Temporal Domain.When encoded as inputs to our model, the temporally-dependent datasets \(\mathcal{D}=\bigcup_{c_{k}\in\mathcal{C}}\mathcal{D}_{c_{k}}\) are partitioned at a particular instance into \(\mathcal{D}^{\text{train}}\) and \(\mathcal{D}^{\text{test}}\).

Temporal domains and features are normalized using only the training data, and so, to effectively encode temporal information and provide normalized inputs when projecting into the future, we use Time2Vec [22]. It converts each temporal domain index into an \(m\)-dimensional vector, using linear and sinusoidal functions characterized by learned parameters \(\omega_{i}\) and \(\varphi_{i}\). Specifically, the Time2Vec transformation for a temporal index \(c_{k}\in\mathcal{C}\) is formulated as:

\[\textbf{t2v}(c_{k})[i]=\begin{cases}\omega_{i}c_{k}+\varphi_{i},&\text{if}\; \;i=0.\\ \sin(\omega_{i}c_{k}+\varphi_{i}),&\text{if}\;\;1\leq i<m.\end{cases}\] (1)

## 4 Experiments

Evaluation Strategy.We evaluate analogous to the Eval-Fix setting outlined in Wild-Time [11], measuring both, ID and OOD performance. Here, each dataset \(\mathcal{D}\) is split into three subsets: \(\mathcal{D}^{\text{train}}\), \(\mathcal{D}^{\text{ID}}\), and \(\mathcal{D}^{\text{OOD}}\). Splits are based on a randomly sampled temporal domain \(c_{k}\) that serves as the boundary between the train and test (OOD) portion. We only use such splits, where \(\mathcal{D}^{\text{train}}\) comprises between 30% and 80% of the total domains and samples. To assess ID performance, we subsample 10% of the instances in each domain of \(\mathcal{D}^{\text{train}}\) as the ID test set and the remainder as the train set. An illustration of the Eval-Fix strategy is provided in Figure 16.

Each class in the training set is required to be represented in both \(\mathcal{D}^{\text{ID}}\) and \(\mathcal{D}^{\text{OOD}}\), and vice versa. For all datasets, we generate three random splits and average metrics across these splits. We had to limit the number of splits to three due to the constrained number of available domains and the requirement for classes to be present in both the train and test splits. Each method is trained three times, and we report the average and 95%-confidence intervals calculated across model initializations.

Figure 4: Types of distribution shifts based on the definitions by Moreno-Torres et al. [51] represented as Bayesian networks as defined by Kull and Flach [52]. Here \(X\), \(Y\), and \(C\) denote the random variables of the features, label, and context, respectively. Note that all these types of shifts naturally arise in our prior, since we sample feature and target positions, as well as the locations of shifted edges, randomly at various positions in the synthetic datasets.

Figure 3: Diagram illustrating the integration of a \(2^{\text{nd}}\)-order SCM for adaptive edge shifting across evolving temporal domains. On the right, the primary network \(\tilde{\mathcal{G}}\) generates data samples over multiple time domains, with red arrows indicating shifted edges. On the left, the \(2^{\text{nd}}\)-order SCM - an auxiliary network \(\tilde{\mathcal{H}}\) - takes an input domain \(c_{k}\in\mathcal{C}\) and outputs parameters to adaptively shift each edge weight \(w_{i}\) in the base network.

Metrics.We evaluate Accuracy, F1-Score (Harmonic mean of precision and recall, useful in imbalanced datasets), ROC AUC (Area under the receiver operating characteristic curve), and ECE (Expected Calibration Error; reflects the reliability of the model's probability outputs).

Datasets.Our benchmark comprises 18 test datasets, 8 synthetic and 10 real-world. In addition, 12 validation datasets, 4 synthetic and the remaining 8 real-world, were used to optimize the hyperparameters of our approach via random search. While some of these datasets have been analyzed in previous work, there has been no comprehensive benchmark focusing on small tabular datasets undergoing distribution shifts. To address this gap, we carefully selected and generated a diverse range of datasets that exhibit temporal distribution shifts. The selected datasets originate from open dataset platforms or previous work in DG. Ground truth domain indices \(c_{k}\in\mathcal{C}\) are known for synthetic datasets. For real-world datasets, we approximated domain indices \(\hat{c_{k}}\) based on features that encode temporal information, which we transformed into discrete intervals. Also, some real-world datasets required subsampling due to their large size, which was beyond the current architecture of TabPFN. We provide full details, including descriptions for each dataset and pre-processing steps in Section A.7 of the Appendix.

Baseline Setup.Our baselines include state-of-the-art methods for tabular prediction. These include advanced GBDTs like CatBoost [24], XGBoost [23], and LightGBM [25], which have demonstrated superior performance to standard neural network approaches in handling tabular data [17]. We also include TabPFN in its unmodified form (TabPFN\({}_{\text{base}}\); 19). Methods from the Wild-Time benchmark are examined separately and detailed in Section A.4.4.3 of the Appendix. However, we have added the two best-performing Wild-Time methods, ERM and Stochastic Weight Averaging (SWA), to the main results table. All baseline methods besides TabPFN are subject to a time budget of 1,200 seconds on 8 CPUs and 1 GPU. For each method except TabPFN, which does not require tuning, a random hyperparameter search with 3-fold time series cross-validation was used. We chose the best-performing hyperparameters based on OOD ROC AUC within the allocated time.

Among our baselines, we considered three strategies:

1. Providing the full dataset \(\mathcal{D}^{\text{train}}\) along with the corresponding domain indices \(\mathcal{C}^{\text{train}}\) as a feature, aiming to allow for better reasoning of the shifts in the dataset (all dom. w. ind.).
2. Using the dataset without domain indices \(\mathcal{D}^{\text{train}}=\{(\boldsymbol{x}_{i}^{\text{train}},y_{i}^{\text {train}})\}_{i=1}^{n}\) (all dom. w. ind.).
3. Limiting the training set to samples from the last training domain \(c_{t}\). In this setting, we also omit the corresponding domain indices, resulting in the set \(\mathcal{D}^{\text{train}}_{c_{t}}=\{(\boldsymbol{x}_{i}^{\text{train}},y_{i}^ {\text{train}})\}_{i=1}^{n_{c_{t}}}\) (last dom. w. ind.). The rationale behind the last scenario is to provide only training data closest to the subsequent test distribution. This strategy is not used for distribution shift baselines.

TabPFN Setup.For the TabPFN variants, both the original and our modified method (TabPFN\({}_{\text{dist}}\)) were pre-trained for 30 epochs across 8 GPUs. This results in a total of 30,720,000 synthetically generated datasets processed during pre-training. While this pre-training step is moderately expensive, it is done offline, in advance, and only once as part of our algorithm development. Furthermore, the preprocessing parameters of both methods were optimized once on the validation datasets by random search over 300 configurations. We chose the configurations that yielded the best OOD ROC AUC performance. The resulting model and hyperparameters are used for all datasets, resulting in, on average, 110 times faster training and prediction time on our benchmark.

Quantitative Evaluation.Our method demonstrates superior predictive performance across all metrics for OOD data in 18 test datasets, for synthetic datasets and real-world datasets, as detailed in Table 1. Compared to the strongest baseline, our method improves accuracy from 0.665 to 0.754 on synthetic datasets and from 0.712 to 0.736 on real-world datasets. It also enhances the F1 score from 0.588 to 0.697 on synthetic datasets and from 0.668 to 0.682 on real-world datasets. Additionally, it increases the ROC AUC from 0.749 to 0.844 on synthetic datasets and from 0.82 to 0.822 on real-world datasets. Furthermore, our method shows much stronger calibration on OOD samples, reducing ECE from 0.164 to 0.126 on synthetic datasets and from 0.083 to 0.062 on real-world datasets. While baselines are often overconfident on OOD data, our method is able to predict uncertainty accurately. Compared to TabPFN and GBDTs, the NN-based methods in the Wild-Time Benchmark (Section A.4.4.3) show a substantial drop in performance, likely due to the limited training data. Since our method focuses on enhancing OOD robustness rather than optimizing ID tasks, we find lower predictive performance on ID tasks. While performance gains are observed on both, real-world and synthetic data, we observe stronger improvements on synthetic datasets. This can be partly attributedto the, on average, stronger distribution shifts between ID and OOD data in our synthetic benchmark. Furthermore, real-world datasets often show multifaceted and complex shifts that are much more difficult to extrapolate into the future. Combined results across all datasets are provided in Table 5.

Qualitative Analysis.Next, we take an in-depth look at the predictions made by our method. Figure 5 illustrates the decision boundaries of our method and TabPFN\({}_{\text{base}}\) on the synthetic Intersecting Blobs dataset. In this evaluation, we restrict the training domains to \(\mathcal{C}^{\text{train}}=\{0,1,2,3\}\) and aim to predict samples in test domains \(\mathcal{C}^{\text{test}}=\{4,5,6\}\) without adding additional data to the training set. This setup requires the model to extrapolate the temporal shifts into the future based solely on existing training data. In this setting, our model accurately extrapolates decision boundaries to future domains, while TabPFN\({}_{\text{base}}\) tends to retain its initial boundary. Our analysis reveals two key attributes of our model: (i) The model decreases prediction certainty over time, improving calibration. (ii) Our model adjusts the decision boundary dynamically, boosting accuracy. Further visualizations, including decision boundaries for the Rotated Two Moons dataset, are available in Figure 7. Likewise, plots illustrating the overall shifts in these datasets are provided in Figure 6.

\begin{table}
\begin{tabular}{l l c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Variant**} & \multicolumn{3}{c|}{**Acc. \(\uparrow\)**} & \multicolumn{3}{c|}{**F1 \(\uparrow\)**} & \multicolumn{3}{c|}{**ROC \(\uparrow\)**} & \multicolumn{3}{c}{**ECE \(\downarrow\)**} \\  & & & OOD & ID & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ \hline
**TabPFN\({}_{\text{dist}}\)** & all dom. w. ind. & \(0.754_{~{}032}\) & \(0.959_{~{}011}\) & \(0.697_{~{}048}\) & \(0.935_{~{}033}\) & \(0.844_{~{}03}\) & \(0.987_{~{}002}\) & \(0.126_{~{}018}\) & \(0.038_{~{}003}\) \\ \hline \multirow{2}{*}{**TabPFN\({}_{\text{base}}\)**} & all dom. w. ind. & \(0.658_{~{}018}\) & \(0.963_{~{}006}\) & \(0.567_{~{}015}\) & \(0.935_{~{}02}\) & \(0.749_{~{}017}\) & \(0.986_{~{}007}\) & \(0.164_{~{}014}\) & \(\mathbf{0.029}_{~{}003}\) \\  & all dom. w. ind. & \(0.571_{~{}014}\) & \(0.901_{~{}006}\) & \(0.467_{~{}016}\) & \(0.848_{~{}006}\) & \(0.631_{~{}01}\) & \(0.955_{~{}03}\) & \(0.322_{~{}016}\) & \(0.053_{~{}005}\) \\  & last dom. w. ind. & \(0.651_{~{}002}\) & \(0.939_{~{}015}\) & \(0.574_{~{}002}\) & \(0.918_{~{}031}\) & \(0.727_{~{}006}\) & \(0.975_{~{}001}\) & \(0.271_{~{}011}\) & \(0.066_{~{}001}\) \\ \hline \multirow{2}{*}{**CatBoost**} & all dom. w. ind. & \(0.665_{~{}008}\) & \(0.985_{~{}003}\) & \(0.058_{~{}008}\) & \(0.089_{~{}009}\) & \(0.731_{~{}008}\) & \(0.109_{~{}002}\) & \(0.297_{~{}006}\) & \(0.037_{~{}006}\) \\  & all dom. wo. ind. & \(0.575_{~{}006}\) & \(0.885_{~{}003}\) & \(0.476_{~{}008}\) & \(0.831_{~{}002}\) & \(0.613_{~{}013}\) & \(0.942_{~{}007}\) & \(0.325_{~{}006}\) & \(0.063_{~{}014}\) \\  & last dom. wo. ind. & \(0.639_{~{}004}\) & \(0.932_{~{}017}\) & \(0.564_{~{}005}\) & \(0.916_{~{}021}\) & \(0.684_{~{}005}\) & \(0.962_{~{}012}\) & \(0.301_{~{}019}\) & \(0.065_{~{}006}\) \\ \hline \multirow{2}{*}{**XGBoost**} & all dom. w. ind. & \(0.645_{~{}018}\) & \(0.936_{~{}012}\) & \(0.57\) & \(0.109_{~{}019}\) & \(0.005_{~{}005}\) & \(0.705_{~{}011}\) & \(0.968_{~{}02}\) & \(0.253_{~{}032}\) & \(0.075_{~{}013}\) \\  & all dom. wo. ind. & \(0.582_{~{}07}\) & \(0.872_{~{}031}\) & \(0.487_{~{}047}\) & \(0.818_{~{}039}\) & \(0.621_{~{}006}\) & \(0.926_{~{}035}\) & \(0.245_{~{}008}\) & \(0.097_{~{}034}\) \\  & last dom. wo. ind. & \(0.645_{~{}008}\) & \(0.916_{~{}005}\) & \(0.565_{~{}009}\)Impact of Time2Vec Preprocessing on Model Performance.To examine Time2Vec's contributions to improved OOD performance, we conduct an ablation study in Appendix A.4.1. The ablation reveals that while Time2Vec provides at most slight improvements, the substantial performance gains are to be attributed to the prior construction used during the model's pre-training phase.

## 5 Conclusions & Limitations

In this work, we presented a Bayesian approach to address the issue of temporal domain generalization in tabular data. Specifically, we focused on enhancing TabPFN to improve its robustness to temporal distribution shifts. Within this framework, we introduced a novel approach that changes the causal relationships in the SCM prior over time, thereby enabling TabPFN to inherently adapt to these shifts. Our method outperforms all baselines on the evaluated datasets and demonstrates notable improvements both qualitatively and quantitatively, particularly on synthetic OOD datasets. Furthermore, it requires no hyperparameter tuning, is not limited to particular types of distribution shifts and takes only 10.9s for training and prediction combined.

Despite these advancements, our methodology inherits certain limitations from the underlying TabPFN model. (1) Due to the quadratic scaling of the attention mechanism with respect to the number of samples, our method does not scale to large datasets. Here, our research will benefit from the continued improvements of TabPFN, ICL, and sequence-based models in general. (2) The TabPFN, like many transformer-based models, acts as a "black box", making it challenging to interpret the model's predictions and understand the recognized distribution shifts. (3) The underlying prior for structural causal models with sparse mechanism shifts may not accurately describe all real-world datasets. While we find it to be empirically and intuitively useful, real-world shifts might have underlying complexities that our prior currently does not capture.

For future work, next to addressing the existing limitations, we have identified in initial experiments promise in extending our model to (1) transductive and (2) online continual learning settings. Also, our prior could be adapted to support (3) spatial or spatio-temporal distribution shifts. Employing a prior that (4) models shifts in the underlying causal model could improve the robustness of the baseline TabPFN in standard classification tasks where temporal shifts are often implicit.

We provide code, pre-trained models, and a Colab notebook at https://github.com/automl/Drift-Resilient_TabPFN. We further describe reproducibility, the release of code and models, the broader impact of our models, and the computational resources used for method development in Appendix A.1, A.2 and A.3. We add a discussion of baselines in A.6, an in-detail discussion of the evaluated datasets in A.7, and additional quantitative and qualitative evaluations in A.4.

Figure 5: This figure displays the predictive behavior of TabPFN\({}_{\text{dist}}\) in the top row and TabPFN\({}_{\text{base}}\) in the bottom row on the Intersecting Blobs dataset. It illustrates how each model adapts to unseen test domains when trained on domains \(\mathcal{C}^{\text{train}}=\{0,1,2,3\}\). The baseline is given the domain indices as a feature in train and test. The coloring indicates the probability of the most likely class at each point. Incorrectly classified samples are highlighted in red.

Acknowledgments

Frank Hutter acknowledges the financial support of the Hector Foundation. This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828.

We acknowledge funding by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.

## References

* Perry et al. [2022] Ronan Perry, Julius Von Kugelgen, and Bernhard Scholkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22)_, 2022. URL https://openreview.net/forum?id=kFRCypubJo.
* Vela et al. [2022] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. _Scientific Reports_, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z.
* Borisov et al. [2022] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* van Breugel and van der Schaar [2024] Boris van Breugel and Mihaela van der Schaar. Why tabular foundation models should be a research priority. _arXiv preprint arXiv:2405.01147_, 2024.
* Young and Steele [2022] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models-a case study in healthcare information systems. _International Journal of Information Management Data Insights_, 2(1):100070, 2022.
* Pasterkamp et al. [2017] Gerard Pasterkamp, Hester M Den Ruijter, and Peter Libby. Temporal shifts in clinical presentation and underlying mechanisms of atherosclerotic disease. _Nature Reviews Cardiology_, 14(1):21-29, 2017.
* Ganesan et al. [2021] Rajarajan Ganesan, Varun Mahajan, Karan Singla, Sushant Konar, Tanvir Samra, Senthil K Sundaram, Vikas Suri, Mandeep Garg, Naveen Kalra, Goverdhan D Puri, et al. Mortality prediction of covid-19 patients at intensive care unit admission. _Cureus_, 13(11), 2021.
* Beucler et al. [2024] Tom Beucler, Pierre Gentine, Janni Yuval, Ankitesh Gupta, Liran Peng, Jerry Lin, Sungduk Yu, Stephan Rasp, Fiaz Ahmed, Paul A O'Gorman, et al. Climate-invariant machine learning. _Science Advances_, 10(6):eadj7250, 2024.
* Lucas et al. [2019] Yvan Lucas, Pierre-Edouard Portier, Lea Laporte, Sylvie Calabretto, Liyun He-Guelton, Frederic Oble, and Michael Granitzer. Dataset shift quantification for credit card fraud detection. In _2019 IEEE second international conference on artificial intelligence and knowledge engineering (AIKE)_, pages 97-100. IEEE, 2019.
* Adam et al. [2020] George Alexandru Adam, Chun-Hao Kingsley Chang, Benjamin Haibe-Kains, and Anna Goldenberg. Hidden risks of machine learning applied to healthcare: Unintended feedback loops between models and future data causing model degradation. In Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, _Proceedings of the 5th Machine Learning for Healthcare Conference_, volume 126 of _Proceedings of Machine Learning Research_, pages 710-731. PMLR, 07-08 Aug 2020. URL https://proceedings.mlr.press/v126/adam20a.html.

* Yao et al. [2022] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22)_, 2022. URL https://openreview.net/forum?id=F9ENmZABB0.
* Bai et al. [2023] Guangji Bai, Chen Ling, and Liang Zhao. Temporal domain generalization with drift-aware dynamic neural networks. In _Proceedings of the International Conference on Learning Representations (ICLR'23)_, 2023. URL https://openreview.net/forum?id=sW0RsJ4nTin. Published online: iclr.cc.
* Nasery et al. [2021] Anshul Nasery, Soumyadeep Thakur, Vihari Pirtala, Abir De, and Sunita Sarawagi. Training for the future: A simple gradient interpolation loss to generalize along time. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. Liang, J. Vaughan, and Y. Dauphin, editors, _Proceedings of the 34th International Conference on Advances in Neural Information Processing Systems (NeurIPS'21)_. Curran Associates, 2021. URL https://openreview.net/forum?id=U7SBcmRf65.
* Gardner et al. [2024] Josh Gardner, Zoran Popovic, and Ludwig Schmidt. Benchmarking distribution shift in tabular data with tableshift. _Advances in Neural Information Processing Systems_, 36, 2024.
* Gorishniy et al. [2021] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. _Advances in Neural Information Processing Systems_, 34:18932-18943, 2021.
* Shwartz-Ziv and Armon [2022] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* Grinsztajn et al. [2022] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22)_, 2022. URL https://openreview.net/forum?id=F9P7_phQszn.
* Muller et al. [2022] S. Muller, N. Hollmann, S. Arango, J. Grabocka, and F. Hutter. Transformers can do bayesian inference. In _Proceedings of the International Conference on Learning Representations (ICLR'22)_, 2022. URL https://openreview.net/forum?id=KSUgKcbNf9. Published online: iclr.cc.
* Hollmann et al. [2023] N. Hollmann, S. Muller, K. Eggensperger, and F. Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In _Proceedings of the International Conference on Learning Representations (ICLR'23)_, 2023. URL https://openreview.net/forum?id=cp5PvcI6w8_. Published online: iclr.cc.
* Pearl [2009] Judea Pearl. _Causality_. Cambridge University Press, 2 edition, 2009.
* Peters et al. [2017] J. Peters, D. Janzing, and B. Scholkopf. _Elements of causal inference: foundations and learning algorithms_. The MIT Press, 2017.
* Kazemi et al. [2020] Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus Brubaker. Time2vec: Learning a vector representation of time, 2020. URL https://openreview.net/forum?id=rkklL5VVvB.
* Chen and Guestrin [2016] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In B. Krishnapuram, M. Shah, A. Smola, C. Aggarwal, D. Shen, and R. Rastogi, editors, _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'16)_, pages 785-794. ACM Press, 2016.
* Prokhorenkova et al. [2018] L. Prokhorenkova, G. Gusev, A. Vorobev, A. Dorogush, and A. Gulin. CatBoost: unbiased boosting with categorical features. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS'18)_. Curran Associates, 2018.

* Ke et al. [2017] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu. Lightgbm: A highly efficient gradient boosting decision tree. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Proceedings of the 30th International Conference on Advances in Neural Information Processing Systems (NeurIPS'17)_. Curran Associates, 2017.
* Wang et al. [2020] Hao Wang, Hao He, and Dina Katabi. Continuously indexed domain adaptation. In H. Daume III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning (ICML'20)_, volume 98. Proceedings of Machine Learning Research, 2020.
* a causal perspective, 2022.
* Muandet et al. [2013] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In S. Dasgupta and D. McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning (ICML'13)_. Omnipress, 2013. URL https://proceedings.mlr.press/v28/muandet13.html.
* Balaji et al. [2018] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS'18)_. Curran Associates, 2018.
* Motiian et al. [2017] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In _Proceedings of the IEEE international conference on computer vision_, pages 5715-5725, 2017.
* Arjovsky et al. [2020] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020.
* Sagawa et al. [2020] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In _Proceedings of the International Conference on Learning Representations (ICLR'20)_, 2020. published online: iclr.cc.
* Zhang et al. [2018] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _Proceedings of the International Conference on Learning Representations (ICLR'18)_, 2018. Published online: iclr.cc.
* Yao et al. [2022] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation, 2022.
* ECCV 2016 Workshops_, pages 443-450, Cham, 2016. Springer International Publishing. doi: 10.1007/978-3-319-49409-8_35.
* Ganin et al. [2016] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 17(1):2096-2030, jan 2016. ISSN 1532-4435.
* Krueger et al. [2021] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning (ICML'21)_, volume 139 of _Proceedings of Machine Learning Research_. PMLR, 2021. URL https://proceedings.mlr.press/v139/krueger21a.html.
* Eastwood et al. [2022] Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kugelgen, Hamed Hassani, George J. Pappas, and Bernhard Scholkopf. Probable domain generalization via quantile risk minimization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22)_, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/6f11132f6ecbbcafafdf6decfc98f7be-Paper-Conference.pdf.

* Malinin et al. [2021] Andrey Malinin, Neil Band, Alexander Ganshin, German Chesnokov, Yarin Gal, Mark J. F. Gales, Alexey Noskov, Andrey Ploskonosov, Ljudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, Vyas Raina, Denis Roginskiy, Mariya Shmatova, Panos Tigar, and Boris Yangel. Shifts: A dataset of real distributional shift across multiple large-scale tasks. _arXiv preprint arXiv:2107.07455_, 2021.
* Malinin et al. [2022] Andrey Malinin, Andreas Athanasopoulos, Muhamed Barakovic, Meritxell Bach Cuadra, Mark J. F. Gales, Cristina Granziera, Mara Graziani, Nikolay Kartashev, Konstantinos Kyriakopoulos, Po-Jui Lu, Nataliia Molchanova, Antonis Nikitakis, Vatsal Raina, Francesco La Rosa, Eli Sivena, Vasileios Tsarsitalidis, Efi Tsompopoulou, and Elena Volf. Shifts 2.0: Extending the dataset of real distributional shifts, 2022. URL https://arxiv.org/abs/2206.15407.
* Liu et al. [2023] Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language describing distribution shifts: Illustrations on tabular datasets. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* Ding et al. [2021] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. Liang, J. Vaughan, and Y. Dauphin, editors, _Proceedings of the 34th International Conference on Advances in Neural Information Processing Systems (NeurIPS'21)_. Curran Associates, 2021.
* Kolesnikov [2023] Sergey Kolesnikov. Wild-tab: A benchmark for out-of-distribution generalization in tabular regression, 2023. URL https://arxiv.org/abs/2312.01792.
* Gulrajani and Lopez-Paz [2021] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _Proceedings of the International Conference on Learning Representations (ICLR'21)_, 2021. URL https://openreview.net/forum?id=lQdXeXDoWtI. Published online: iclr.cc.
* Wang et al. [2023] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization. _IEEE Transactions on Knowledge and Data Engineering_, 35(8):8052-8072, 2023. doi: 10.1109/TKDE.2022.3178128.
* Izmailov et al. [2018] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In A. Globerson and R. Silva, editors, _Proceedings of The 34th Uncertainty in Artificial Intelligence Conference (UAI'18)_. AUAI Press, 2018.
* He et al. [2020] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei Alex Rusu, Yee Whye Teh, and Razvan Pascanu. Task agnostic continual learning via meta learning. In _4th Lifelong Machine Learning Workshop at ICML 2020_, 2020. URL https://openreview.net/forum?id=AeIzVxdJgeb.
* Ma et al. [2024] Junwei Ma, Valentin Thomas, Guangwei Yu, and Anthony Caterini. In-context data distillation with tabpfn, 2024. URL https://arxiv.org/abs/2402.06971.
* Feuer et al. [2024] Benjamin Feuer, Robin Tibor Schirrmeister, Valeria Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, and Colin White. Tunetables: Context optimization for scalable prior-data fitted networks. _arXiv preprint arXiv:2402.11137_, 2024.
* Dooley et al. [2023] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha V Naidu, and Colin White. Forecastpfn: Synthetically-trained zero-shot forecasting. In _Advances in Neural Information Processing Systems_, 2023.
* Moreno-Torres et al. [2012] Jose G. Moreno-Torres, Troy Raeder, Rocio Alaiz-Rodriguez, Nitesh V. Chawla, and Francisco Herrera. A unifying view on dataset shift in classification. _Pattern Recognition_, 45(1):521-530, 2012. ISSN 0031-3203. doi: 10.1016/j.patcog.2011.06.019. URL https://www.sciencedirect.com/science/article/pii/S0031320311002901.
* Kull and Flach [2014] Meelis Kull and Peter Flach. Patterns of dataset shift. In _First international workshop on learning over multiple contexts (LMCE) at ECML-PKDD_, volume 5, 2014.
* Vanschoren et al. [2014] J. Vanschoren, J. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in machine learning. _SIGKDD Explorations_, 15(2):49-60, 2014.

* Wilcoxon [1945] Frank Wilcoxon. Individual comparisons by ranking methods. _Biometrics Bulletin_, 1(6):80-83, 1945. ISSN 00994987. URL http://www.jstor.org/stable/3001968.
* Holm [1979] Sture Holm. A simple sequentially rejective multiple test procedure. _Scandinavian journal of statistics_, pages 65-70, 1979.
* Garcia and Herrera [2008] Salvador Garcia and Francisco Herrera. An extension on" statistical comparisons of classifiers over multiple data sets" for all pairwise comparisons. _Journal of machine learning research_, 9(12), 2008.
* Fawaz et al. [2019] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. Deep learning for time series classification: a review. _Data Mining and Knowledge Discovery_, 33(4):917-963, 2019.
* Kirkpatrick et al. [2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* Zenke et al. [2017] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In D. Precup and Y. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning (ICML'17)_, volume 70. Proceedings of Machine Learning Research, 2017.
* Chaudhry et al. [2019] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with A-GEM. In _Proceedings of the International Conference on Learning Representations (ICLR'19)_, 2019. Published online: iclr.cc.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In H. Daume III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning (ICML'20)_, volume 98. Proceedings of Machine Learning Research, 2020.
* Caron et al. [2020] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H. Lin, editors, _Proceedings of the 33rd International Conference on Advances in Neural Information Processing Systems (NeurIPS'20)_. Curran Associates, 2020.
* Ramana and Venkateswarlu [2012] Bendi Ramana and N. Venkateswarlu. ILPD (Indian Liver Patient Dataset). UCI Machine Learning Repository, 2012.
* Akbilgic [2013] Oguz Akbilgic. Istanbul Stock Exchange. UCI Machine Learning Repository, 2013.
* Strack et al. [2014] Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore. Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records. _BioMed Research International_, 2014:781670, April 2014. ISSN 2314-6133. doi:10.1155/2014/781670. URL https://doi.org/10.1155/2014/781670. Publisher: Hindawi Publishing Corporation.
* Bifet and Ikonomovska [2009] Albert Bifet and Elena Ikonomovska. Data Expo competition. OpenML, 2009. URL https://www.openml.org/search?type=data&sort=runs&id=1169&status=active.
* Annual Symposium on Computer Applications in Medical Care_, 10, 11 1988.
* Islam et al. [2020] M. M. Faniqul Islam, Rahatara Ferdousi, Sadikur Rahman, and Humayra Yasmin Bushra. Likelihood prediction of diabetes at early stage using data mining techniques. In Mousumi Gupta, Debanjan Konar, Siddhartha Bhattacharyya, and Sambhuanth Biswas, editors, _Computer Vision and Machine Intelligence in Medical Image Analysis_, pages 113-125, Singapore, 2020. Springer Singapore. ISBN 978-981-13-8798-2. doi:10.1007/978-981-13-8798-2_12.
* Candanedo [2016] Luis Candanedo. Occupancy Detection. UCI Machine Learning Repository, 2016.

* Ferreira et al. [2018] Ricardo Ferreira, Andrea Martiniano, and Renato Sassi. Behavior of the urban traffic of the city of Sao Paulo in Brazil. UCI Machine Learning Repository, 2018.
* Montiel et al. [2018] Jacob Montiel, Jesse Read, Albert Bifet, and Talel Abdessalem. Scikit-multiflow: A multi-output streaming framework. _Journal of Machine Learning Research_, 19(72):1-5, 2018. URL http://jmlr.org/papers/v19/18-251.html.
* Dispenzieri et al. [2012] Angela Dispenzieri, Jerry Katzmann, Robert Kyle, Dirk Larson, Terry Therneau, Colin Colby, Raynell Clark, Graham Mead, Shaji Kumar, L Melton, and S Rajkumar. Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population. _Mayo Clinic proceedings. Mayo Clinic_, 87:517-23, 06 2012. doi: 10.1016/j.mayocp.2012.03.009.
* Kyle et al. [2006] Robert Kyle, Terry Therneau, S Rajkumar, Dirk Larson, Matthew Plevak, Janice Offord, Angela Dispenzieri, Jerry Katzmann, and L Melton. Prevalence of monoclonal gammopathy of undetermined significance. _The New England journal of medicine_, 354:1362-9, 04 2006. doi: 10.1056/NEJMoa054494.
* Harries [1999] Michael Harries. _Splice-2 comparative evaluation: Electricity Pricing_. University of New South Wales, School of Computer Science and Engineering [Sydney], 1999. URL http://nla.gov.au/nla.arc-32869.
* SBIA 2004_, pages 286-295, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg. ISBN 978-3-540-28645-5. doi: 10.1007/978-3-540-28645-5_29.
* Martiniano and Ferreira [2018] Andrea Martiniano and Ricardo Ferreira. Absenteeism at work. UCI Machine Learning Repository, 2018.
* Janosi et al. [1988] Andras Janosi, William Steinbrunn, Matthias Pfisterer, and Robert Detrano. Heart Disease. UCI Machine Learning Repository, 1988.
* Stolfi [2019] Daniel Stolfi. Parking Birmingham. UCI Machine Learning Repository, 2019.
* Cock [2011] Dean De Cock. Ames, iowa: Alternative to the boston housing data as an end of semester regression project. _Journal of Statistics Education_, 19, 11 2011. doi: 10.1080/10691898.2011.11889627.
* Zliobaite [2011] Indre Zliobaite. Combining similarity in time and space for training set formation under concept drift. _Intelligent Data Analysis_, 15(4):589-611, 2011.
* Ahuja and Lopez-Paz [2023] Kartik Ahuja and David Lopez-Paz. A closer look at in-context learning under distribution shifts. _arXiv preprint arXiv:2305.16704_, 2023.

## Appendix A Appendix

### Reproducibility

Code Availability.In an effort to ensure reproducibility, we release code, version specification of our baselines, our pre-trained Drift-Resilient TabPFN and an interactive Colab notebook, that lets you interact with our scikit-learn interface, at https://github.com/automl/Drift-Resilient_TabPFN.

Data Availability.All real-world datasets used in our experiments are freely available at OpenML.org[53] or via the original sources referenced in Section A.7, with downloading scripts or instructionsincluded in the submission code. Code to generate our synthetic datasets can be found in our code repository, see above.

**Details of Hyperparameters for Drift-Resilient TabPFN and Baselines.** An overview of the hyperparameters used for running Drift-Resilient TabPFN and our baselines can be found in Tables 8, 9 and 10 respectively.

**Evaluation Reproducibility.** Consistent dataset splits were pre-determined to ensure comparability across all evaluations, reinforcing the reliability of our findings.

### Broader Impacts

As a general method for handling distribution shift in tabular data, Drift-Resilient TabPFN does not have immediate direct societal implications in the same way as AI systems designed to automate specific tasks or replace human jobs. However, Drift-Resilient TabPFN could offer several potential positive societal impact:

1. **Increased Model Longevity**: Our approach extends the usable lifespan of deployed ML models by adapting to distribution shifts, reducing the need for retraining and leading to cost savings and more stable performance. In healthcare, this can ensure diagnostic and prognostic models remain reliable as data shifts over time.
2. **Improved Decision Making and Long-Term Predictions**: Drift-Resilient TabPFN enhances decision-making in critical fields like finance and climate science by enabling more robust, longer-term predictions. Our Bayesian approach for tackling distribution shift provides a new perspective that can spur further methodological innovations.

However, we also recognize potential negative impacts:

1. **Potential Misuse**: Like any ML advance, more robust models could be misused for harmful purposes if not developed and deployed responsibly.
2. **Environmental Cost**: While the trained model is now efficiently applicable to various datasets, the considerable computational resources used for initial training should be noted. However, we note that these costs are one-time, while the resulting model can be applied with minimal energy usage.

### Computational Resources

In the course of our research on Drift-Resilient TabPFN, we employed substantial computational resources across various stages of model development, training, and evaluation. The computation specifics are as follows:

1. **Infrastructure:** The experiments were conducted on an internal SLURM cluster equipped with RTX 2080 TI GPUs and CPUs of type AMD EPYC 7502, 32C/64T, @ 2.50-3.35GHz.
2. **Baseline Experiments:** Each baseline experiment utilized 8 CPUs, 1 GPU, and 62.5 GB RAM, with a hyperparameter optimization (HPO) runtime of 1200 seconds per dataset per split, repeated three times to ensure reliability.
3. **Pre-training for Drift-Resilient TabPFN and TabPFN-base:** These models were pre-trained three times each, requiring 64 CPUs, 8 GPUs, and 500 GB RAM, requiring approximately 7 and 8 days respectively.
4. **Hyperparameter Optimization for Drift-Resilient TabPFN and TabPFN-base:** We used 32 CPUs, 4 GPUs, and 250 GB RAM, running approximately 40 configurations and taking about one day per pre-training session for optimizing the hyperparameters of the novel prior-data generating mechanism of Drift-Resilient TabPFN. Both TabPFN-base and Drift-Resilient TabPFN underwent preprocessing optimization that utilized 8 CPUs, 1 GPU, and 62.5 GB RAM across 300 runs, each lasting between 0.5 to 1 hour.

Additional computational resources were allocated for method development tests and other experimental setups not detailed in the final publication. Thus the full scope of the research required more computational resources than those detailed above due to these preliminary and unreported experiments.

### Additional Experiments

#### a.4.1 Ablation Studies

_Is our model's performance mostly based on Time2Vec preprocessing?_ To address this question, we conducted an ablation study where we trained a model with temporal domain indices normalized but not subjected to Time2Vec preprocessing (No T2V).

Table 2 presents the performance metrics of Drift-Resilient TabPFN, TabPFN-base, and our ablation model. The results indicate that while Time2Vec preprocessing may slightly improve model performance, it is statistically insignificant. Rather, the substantial performance improvements are largely due to our prior construction, used during the pre-training phase of the model. The decision to keep Time2Vec in the final model was guided by our HPO, which indicated a positive impact on average performance.

#### a.4.2 Perturbation of Temporal Domain Indices within the Prior

This additional experiment analyzes the impact of perturbing temporal domain indices on the performance of our Drift-Resilient TabPFN model. In real-world datasets, ground truth temporal domain information is often unknown and must be approximated, creating a crucial difference between these datasets and those generated by the prior in our methodology. To assess this, we conducted an experiment during model development wherein we intentionally modified the ground truth domain indices \(\mathcal{C}\) during the pre-training phase prior to feeding them into the transformer.

In this context, we explored three primary techniques:

Shifting Domain Boundaries Probabilistically.We shifted the boundaries of domains and reported instances near those boundaries as belonging to adjacent domains.

Merging Domains.Multiple domains were probabilistically merged into a single domain, thereby introducing ambiguity in domain information.

Noise Injection.Random noise was added to each domain indicator, further complicating reasoning about temporal distribution shifts based on these indicators.

Our experiments on our validation datasets, listed in Table 3, show that overall these perturbations adversely affect model performance. The findings suggest that the model requires ground-truth domain indices for effective training, emphasizing the importance of accurate domain information in real-world applications.

\begin{table}
\begin{tabular}{l l c c|c c|c c c|c c c} \hline \hline
**Model** & **Variant** & \multicolumn{3}{c|}{**Acc. \(\uparrow\)**} & \multicolumn{3}{c|}{**F1 \(\uparrow\)**} & \multicolumn{3}{c|}{**ROC \(\uparrow\)**} & \multicolumn{3}{c}{**ECE \(\downarrow\)**} \\  & & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ \hline
**TabPFN\({}_{\text{shot}}\)** & \multicolumn{1}{c}{no changes} & \(0.74\) & \(\mathbf{0.867}\) & \(\mathbf{0.692}\) & \(\mathbf{0.832}\) & \(\mathbf{0.837}\) & \(\mathbf{0.908}\) & \(\mathbf{0.085}\) & \(\mathbf{0.076}\) \\ \cline{2-11}  & alt dom. bound. & \(0.734\) & \(0.86\) & \(0.682\) & \(0.822\) & \(0.829\) & \(0.905\) & \(0.092\) & \(0.078\) \\
**TabPFN\({}_{\text{part.}}\)** & \multicolumn{1}{c}{all dom. bound. / merge dom.} & \(\mathbf{0.742}\) & \(0.857\) & \(0.69\) & \(0.815\) & \(0.833\) & \(0.906\) & \(0.09\) & \(0.079\) \\  & alt dom. bound. / merge dom. / noise & \(0.704\) & \(0.835\) & \(0.64\) & \(0.78\) & \(0.792\) & \(0.888\) & \(0.101\) & \(0.108\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance evaluation of Drift-Resilient TabPFN against models with perturbed domain information. Three techniques were examined: (1) Shifting domain boundaries probabilistically, reporting instances near those boundaries as belonging to the other domain; (2) Probabilistically merging multiple domains into one; (3) Adding noise to each domain indicator. Our results indicate that perturbed domain indices overall led to a decline in model performance, emphasizing the model’s requirement for ground-truth domain indices during training.

\begin{table}
\begin{tabular}{l l c|c c|c c|c c|c c} \hline \hline
**Model** & **Variant** & \multicolumn{3}{c|}{**Acc. \(\uparrow\)**} & \multicolumn{3}{c|}{**F1 \(\uparrow\)**} & \multicolumn{3}{c|}{**ROC \(\uparrow\)**} & \multicolumn{3}{c}{**ECE \(\downarrow\)**} \\  & & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ \hline
**TabPFN\({}_{\text{shot}}\)** & \multicolumn{1}{c}{no changes} & \(0.74\) & \(\mathbf{0.867}\) & \(\mathbf{0.692}\) & \(\mathbf{0.832}\) & \(\mathbf{0.837}\) & \(\mathbf{0.908}\) & \(\mathbf{0.085}\) & \(\mathbf{0.076}\) \\ \cline{2-11}  & alt dom. bound. & \(0.734\) & \(0.86\) & \(0.682\) & \(0.822\) & \(0.829\) & \(0.905\) & \(0.092\) & \(0.078\) \\
**TabPFN\({}_{\text{part.}}\)** & \multicolumn{1}{c}{all dom. bound. / merge dom.} & \(\mathbf{0.742}\) & \(0.857\) & \(0.69\) & \(0.815\) & \(0.833\) & \(0.906\) & \(0.09\) & \(0.079\) \\  & alt dom. bound. / merge dom. / noise & \(0.704\) & \(0.835\) & \(0.64\) & \(0.78\) & \(0.792\) & \(0.888\) & \(0.101\) & \(0.108\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of Drift-Resilient TabPFN with respect to the stated ablations. Metrics include ROC AUC and accuracy for both in-distribution (ID) and out-of-distribution (OOD) data.

#### a.4.3 Qualitative Analysis

A.4.3.1 Overview of the Shifts in the Datasets AnalyzedThis section offers plots of the Intersecting Blobs and Rotated Two Moons datasets across specific temporal domains. The Intersecting Blobs dataset is visualized in Figure 5(a) for domains \(\mathcal{C}=\{0,4,8,13\}\). The Rotated Two Moons dataset is presented in Figure 5(b) for domains \(\mathcal{C}=\{0,3,6,9\}\).

A.4.3.2 Decision Boundaries on Rotated Two Moons DatasetIn addition to the qualitative analysis conducted for the Intersecting Blobs dataset in the main text of this work, this subsection provides additional illustrations of the Rotated Two Moons dataset. The corresponding visualizations for our approach as well as the TabPFN baseline are provided in Figure 7.

Figure 6: This figure shows the temporal shifts of two synthetic datasets across selected domains.

Figure 7: This figure contrasts the predictive behavior of TabPFN\({}_{\text{dist}}\) and TabPFN\({}_{\text{base}}\) on the Rotated Two Moons dataset. It illustrates how each model adapts to different testing domains when trained on domains \(\mathcal{C}^{\text{train}}=\{0,1,2,3,4,5\}\). The color shading indicates the maximum class probability at each point, with decision boundaries shown when this probability exceeds 50%. Incorrectly classified samples are highlighted in red.

#### a.4.3.3 Decision Boundaries by Type of Shift

To analyze our models behavior under distinct shift types, we separate decision boundaries by shift category to provide insights into how the model adapts to each.

A.4.3.4 Comparison Against DRAIN and GITo show our improved performance compared to the state-of-the-art methods DRAIN [12] and GI [13], we compare our method with the qualitative analysis performed by the authors of DRAIN on the Rotated Two Moons dataset. In this setting, all domains except the last are used for training, with the final domain reserved for testing. We illustrate our method's decision boundary compared to those provided by DRAIN in Figure 9. The accuracy results are listed in Table 4. As the contour levels are unknown, we display only the pure decision boundary for clearer comparison. Our analysis shows that our model forecasts the rotation of the two moons more accurately compared to the baselines and adapts its decision boundary more precisely.

Figure 8: Each figure displays the predictive behavior of TabPFN\({}_{\text{dist}}\) in the top row and TabPFN\({}_{\text{base}}\) in the bottom row. It illustrates how each model adapts to unseen test domains when trained on domains \(\mathcal{C}^{\text{train}}\). The baseline is given the domain indices as a feature in train and test. The coloring indicates the probability of the most likely class at each point. Incorrectly classified samples are highlighted in red.

Figure 9: Comparison of our method against DRAIN [12] and GI [13] on the Rotated Two Moons dataset. The models were trained on domains \(\mathcal{C}=\{0,1,\dots,8\}\) and tested on domain 9. While the authors of DRAIN present different, unknown levels of the decision boundary, we present the decision boundary with 50% probability. The plots for DRAIN and GI were taken from Bai et al. [12].

[MISSING_PAGE_EMPTY:22]

#### a.4.4.2 Critical Difference Diagrams

Figure 10: This figure presents critical difference diagrams of our evaluated metrics on OOD data, analyzed using the Wilcoxon-Holm method [54, 55, 56, 57] across the best performing OOD models evaluated in this work. The diagrams indicate significant differences of Drift-Resilient TabPFN against the tree-based methods. For the F1 metric our method shows significant differences against all top performing baselines. Arrows indicate optimization direction.

A.4.4.3 Comparison Against Wild-Time MethodsThis section offers a quantitative evaluation of our model against methods found in the Wild-Time benchmark [11]. We focus exclusively on methods applicable to tabular data, omitting SimCLR and SwAV which are specifically designed for image datasets. Detailed explanations of these methods are available in Section A.6.

As a base model, we used a Multilayer Perceptron (MLP) optimized through Hyperparameter Optimization (HPO).

The findings of this quantitative comparison are presented in Table 6. Notably, none of the evaluated Wild-Time methods demonstrated performance equal with our approach or the other baseline methods on OOD data. This discrepancy is likely due to the small number of instances in the datasets used in our evaluations, which affects the generalization of these deep learning techniques. Among the Wild-Time methods, SWA emerged as the most effective in handling OOD data.

\begin{table}
\begin{tabular}{l l c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Variant**} & \multirow{2}{*}{**Acc.**\(\uparrow\)} & \multicolumn{3}{c|}{**F1\(\uparrow\)**} & \multicolumn{3}{c|}{**ROC \(\uparrow\)**} & \multicolumn{3}{c}{**ECE \(\downarrow\)**} \\  & & & OOD & ID & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ \hline \multicolumn{1}{c}{**TabFN\({}_{\text{dat}}\)**} & all dom. w. ind. & **0.744**.018 & **0.879**.012 & **0.689**.028 & **0.837**.022 & **0.832**.018 & **0.932**.002 & **0.091**.006 & **0.074**.014 \\ \hline \hline \multirow{2}{*}{**ERM**} & all dom. w. ind. & 0.627 & 0.36 & 0.821 & 0.32 & 0.525 & 0.52 & 0.771 & 0.44 & 0.688 & 0.28 & 0.877 & 0.025 & 0.263 & 0.054 & 0.108 & 0.17 \\  & all dom. w. ind. & 0.582 & 0.288 & 0.803 & 0.519 & 0.628 & 0.746 & 0.673 & 0.673 & 0.682 & 0.082 & 0.028 & 0.255 & 0.019 & 0.104 & 0.105 \\  & last dom. w. ind. & 0.587 & 0.262 & 0.784 & 0.53 & 0.528 & 0.746 & 0.711 & 0.666 & 0.184 & 0.843 & 0.322 & 0.328 & 0.19 & 0.25 \\ \hline \hline \multirow{2}{*}{**FT**} & all dom. w. ind. & 0.513 & 0.645 & 0.144 & 0.422 & 0.446 & 0.559 & 0.533 & 0.594 & 0.270 & 0.693 & 0.337 & 0.013 & 0.256 & 0.007 \\  & all dom. w. ind. & 0.544 & 0.38 & 0.677 & 0.34 & 0.481 & 0.660 & 0.604 & 0.65 & 0.255 & 0.756 & 0.031 & 0.33 & 0.03 & 0.231 & 0.061 \\ \hline \multirow{2}{*}{**EWC**} & all dom. w. ind. & 0.498 & 0.262 & 0.621 & 0.445 & 0.405 & 0.538 & 0.522 & 0.569 & 0.270 & 0.668 & 0.066 & 0.342 & 0.208 & 0.239 \\  & all dom. w. ind. & 0.518 & 0.233 & 0.686 & 0.45 & 0.456 & 0.523 & 0.617 & 0.603 & 0.120 & 0.74 & 0.309 & 0.486 & 0.195 & 0.005 \\ \hline \multirow{2}{*}{**SI**} & all dom. w. ind. & 0.478 & 0.577 & 0.63 & 0.068 & 0.389 & 0.526 & 0.515 & 0.566 & 0.617 & 0.673 & 0.203 & 0.37 & 0.046 & 0.238 & 0.012 \\  & all dom. w. w. ind. & 0.495 & 0.197 & 0.674 & 0.345 & 0.425 & 0.536 & 0.588 & 0.559 & 0.597 & 0.224 & 0.743 & 0.336 & 0.406 & 0.214 & 0.049 \\ \hline \multirow{2}{*}{**A-GEM**} & all dom. w. ind. & 0.513 & 0.674 & 0.38 & 0.405 & 0.540 & 0.576 & 0.455 & 0.594 & 0.601 & 0.743 & 0.306 & 0.367 & 0.242 & 0.223 & 0.21 \\  & all dom. w. w. ind. & 0.486 & 0.358 & 0.684 & 0.203 & 0.403 & 0.601 & 0.583 & 0.55 & 0.583 & 0.747 & 0.190 & 0.364 & 0.073 & 0.221 & 0.54 \\ \hline \hline \multirow{2}{*}{**CORAL-T**} & all dom. w. ind. & 0.579 & 0.517 & 0.777 & 0.241 & 0.481 & 0.585 & 0.714 & 0.611 & 0.637 & 0.360 & 0.837 & 0.716 & 0.252 & 0.058 & 0.143 & 0.255 \\  & all dom. w. ind. & 0.569 & 0.923 & 0.771 & 0.345 & 0.492 & 0.702 & 0.693 & 0.643 & 0.168 & 0.837 & 0.203 & 0.232 & 0.275 & 0.103 \\ \hline \multirow{2}{*}{**GroupDOR-T**} & all dom. w. ind. & 0.585 & 0.25 & 0.779 & 0.140 & 0.503 & 0.383 & 0.723 & 0.099 & 0.642 & 0.36 & 0.847 & 0.028 & 0.190 & 0.125 & 0.1 \\  & all dom. w. ind. & 0.576 & 0.235 & 0.775 & 0.232 & 0.514 & 0.233 & 0.725 & 0.365 & 0.658 & 0.084 & 0.062 & 0.476 & 0.135 & 0.04 \\ \hline \multirow{2}{*}{**IRM-T**} & all dom. w. ind. & 0.577 & 0.740 & 0.746 & 0.449 & 0.551 & 0.689 & 0.163 & 0.633 & 0.621 & 0.819 & 0.169 & 0.259 & 0.13 & 0.13 \\  & all dom. w. ind. & 0.559 & 0.18 & 0.7442 & 0.233 & 0.498 & 0.511 & 0.678 & 0.474 & 0.644 & 0.052 & 0.8213 & 0.252 & 0.313 & 0.134 & 0.011 \\ \hline \multirow{2}{*}{**Mixup**} & all dom. w. ind. & 0.617 & 0.28 & 0.82 & 0.521 & 0.166 & 0.702 & 0.068 & 0.621 & 0.289 & 0.592 & 0.239 & 0.014 & 0.14 & 0.007 \\  & all dom. w. ind. & 0.574 & 0.34 & 0.782 & 0.227 & 0.492 & 0.256 & 0.688 & 0.13 & 0.682 & 0.25 & 0.852 & 0.212 & 0.214 & 0.124 & 0.07 \\ \hline \multirow{2}{*}{**LISA**} & all dom. w. ind. & 0.621 & 0.41 & 0.822 & 0.333 & 0.517 & 0.79 & 0.76 & 0.663 & 0.679 & 0.588 & 0.112 & 0.272 & 0.246 & 0.116 & 0.2 \\  & all dom. w. ind. & 0.583 & 0.13 & 0.804 & 0.151 & 0.512 & 0.009 & 0.739 & 0.242 & 0.672 & 0.258 & 0.859 & 0.240 & 0.258 & 0.108 & 0.223 \\ \hline \hline \multirow{2}{*}{**SWA**} & all dom. w. ind. & 0.6277 & 0.47 & 0.824 & 0.03 & 0.534 & 0.669 & 0.777 & 0.14 & 0.685 & 0.366 & 0.873 & 0.158 & 0.2

#### a.4.5 Investigating Performance Saturation of Main Baseline Methods

In this subsection, we assess the computational budget allocated for HPO on our main baseline methods. By comparing model performance at 1200 seconds versus 3600 seconds per method and dataset split, we show that the increased budget for HPO does not yield significant gains overall, indicating sufficient saturation within the initial budget.

a.4.6 Analyzing the Relationship Between Out-of-Distribution Performance and Difficulty Across Datasets

In this supplementary evaluation, we investigate the correlation between the inherent difficulty of a dataset concerning distribution shifts and the performance of our approach in comparison to the baselines.

Defining Difficulty Metrics.Our first step involves assigning a difficulty score to individual dataset splits, using a basic XGBoost model that doesn't include domain indices as a feature of the dataset. This score is determined using the formula \(ID-OOD\), where both ID and OOD performances are measured using ROC AUC. This difficulty score quantifies the decline in model performance when transitioning from ID to OOD data. After calculating the difficulty scores for each dataset split, we proceed to train each of the evaluated methods across all domains, including the domain indices. We then compute the performance gap, which is also calculated as \(ID-OOD\), for each method.

These metrics are then visualized by plotting the dataset's difficulty score against its corresponding performance gap for each dataset split. We also fit linear regression lines to the scatter plots representing each method. The final plot is illustrated in Figure 11.

Although the data points cannot be interpreted directly, the linear regression suggests that our method experiences the least decline in ROC AUC performance as dataset difficulty increases, affirming its robustness. The TabPFN baseline is the second-best performer, while the other models show more significant drops in performance.

\begin{table}
\begin{tabular}{l l r r|r r r|r r|r r} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Variant**} & \multicolumn{2}{c|}{**Acc. \(\uparrow\)**} & \multicolumn{3}{c|}{**FI \(\uparrow\)**} & \multicolumn{3}{c|}{**ROC \(\uparrow\)**} & \multicolumn{3}{c}{**ECE \(\downarrow\)**} \\  & & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ \hline \multirow{3}{*}{**CatBoost11**} & all dom. w. ind. & 0.678 \({}_{007}\) & 0.872 \({}_{006}\) & 0.621 \({}_{01}\) & 0.833 \({}_{004}\) & 0.764 \({}_{007}\) & 0.914 \({}_{011}\) & 0.205 \({}_{017}\) & 0.084 \({}_{025}\) \\  & all dom. w. ind. & 0.632 \({}_{005}\) & 0.838 \({}_{008}\) & 0.068 \({}_{007}\) & 0.782 \({}_{01}\) & 0.713 \({}_{004}\) & 0.893 \({}_{026}\) & 0.226 \({}_{002}\) & 0.099 \({}_{018}\) \\  & last dom. w. ind. & 0.656 \({}_{007}\) & 0.85 \({}_{024}\) & 0.598 \({}_{01}\) & 0.813 \({}_{022}\) & 0.712 \({}_{005}\) & 0.903 \({}_{018}\) & 0.264 \({}_{019}\) & 0.136 \({}_{028}\) \\ \hline \multirow{3}{*}{**CatBoost2001**} & all dom. w. ind. & 0.677 \({}_{006}\) & 0.874 \({}_{007}\) & 0.622 \({}_{005}\) & 0.836 \({}_{01}\) & 0.766 \({}_{003}\) & 0.919 \({}_{01}\) & 0.222 \({}_{007}\) & 0.084 \({}_{009}\) \\  & all dom. w. ind. & 0.632 \({}_{003}\) & 0.836 \({}_{013}\) & 0.568 \({}_{005}\) & 0.781 \({}_{009}\) & 0.714 \({}_{012}\) & 0.894 \({}_{014}\) & 0.24 \({}_{02}\) & 0.097 \({}_{015}\) \\  & last dom. w. ind. & 0.657 \({}_{002}\) & 0.852 \({}_{014}\) & 0.599 \({}_{004}\) & 0.811 \({}_{024}\) & 0.722 \({}_{005}\) & 0.907 \({}_{01}\) & 0.256 \({}_{006}\) & 0.133 \({}_{012}\) \\ \hline \hline \multirow{3}{*}{**XGBoost11**} & all dom. w. ind. & 0.662 \({}_{013}\) & 0.864 \({}_{011}\) & 0.611 \({}_{01}\) & 0.834 \({}_{017}\) & 0.754 \({}_{009}\) & 0.911 \({}_{012}\) & 1.877 \({}_{010}\) & 0.101 \({}_{017}\) \\  & all dom. w. ind. & 0.639 \({}_{010}\) & 0.833 \({}_{020}\) & 0.578 \({}_{01}\) & 0.781 \({}_{01}\) & 0.707 \({}_{024}\) & 0.0886 \({}_{014}\) & 0.181 \({}_{019}\) & 0.124 \({}_{022}\) \\  & last dom. w. no. ind. & 0.664 \({}_{011}\) & 0.832 \({}_{01}\) & 0.598 \({}_{022}\) & 0.765 \({}_{01}\) & 0.736 \({}_{012}\) & 0.895 \({}_{014}\) & 0.195 \({}_{016}\) & 0.164 \({}_{005}\) \\ \hline \multirow{3}{*}{**XGBoost2001**} & all dom. w. ind. & 0.664 \({}_{005}\) & 0.859 \({}_{01}\) & 0.611 \({}_{013}\) & 0.828 \({}_{003}\) & 0.754 \({}_{006}\) & 0.91 \({}_{019}\) & 0.194 \({}_{018}\) & 0.111 \({}_{02}\) \\  & all dom. w. ind. & 0.633 \({}_{033}\) & 0.831 \({}_{024}\) & 0.568 \({}_{03}\) & 0.778 \({}_{03}\) & 0.718 \({}_{03}\) & 0.881 \({}_{03}\) & 0.194 \({}_{015}\) & 0.124 \({}_{02}\) \\  & last dom. w. ind. & 0.664 \({}_{01}\) & 0.824 \({}_{023}\) & 0.599 \({}_{016}\) & 0.758 \({}_{045}\) & 0.733 \({}_{009}\) & 0.087 \({}_{016}\) & 0.199 \({}_{024}\) & 0.167 \({}_{011}\) \\ \hline \hline \multirow{3}{*}{**LightGBM11**} & all dom. w. ind. & 0.647 \({}_{012}\) & 0.846 \({}_{005}\) & 0.593 \({}_{013}\) & 0.805 \({}_{003}\) & 0.736 \({}_{006}\) & 0.909 \({}_{005}\) & 0.211 \({}_{004}\) & 0.096 \({}_{007}\) \\  & all dom. w. ind. & 0.618 \({}_{018}\) & 0.852 \({}_{020}\) & 0.055 \({}_{007}\) & 0.767 \({}_{007}\) & 0.708 \({}_{007}\) & 0.888 \({}_{018}\) & 0.12 \({}_{014}\) & 0.102 \({}_{008}\) \\  & last dom. w. ind. & 0.624 \({}_{015}\) & 0.795 \({}_{021}\) & 0.546 \({}_{029}\) & 0.724 \({}_{032}\) & 0.687 \({}_{005}\) & 0.845 \({}_{01}\) & 0.231 \({}_{014}\) & 0.148 \({}_{017}\) \\ \hline \multirow{3}{*}{**LightGBM2001**} & all dom. w. ind. & 0.655 \({}_{009}\) & 0.842 \({}_{024}\) & 0.594 \({}_{008}\) & 0.8 \({}_{024}\) & 0.738 \({}_{008}\) & 0.908 \({}_{008}\) & 0.198 \({}_{009}\) & 0.093 \({}_Figure 11: This figure illustrates a comparative analysis of the resilience of the listed methods to out-of-distribution (OOD) difficulty across multiple datasets and splits. The \(x\)-axis captures the difficulty of each dataset split, while the \(y\)-axis measures the performance drop of a method compared to in-distribution (ID) performance. Individual methods are represented by scatter points and their corresponding linear regression lines, with shaded regions indicating the 95% confidence intervals for TabPFN methods. Directional arrows signify increasing or decreasing dataset difficulty. Flatter regression slopes indicate models that are more resilient to increases in dataset difficulty due to distribution shifts.

#### a.4.7 Number of Shift Observations Required for Effective Extrapolation

For this analysis, we evaluated the performance of both our method and TabPFN-base by fixing the prediction to the last domain of the Intersecting Blobs dataset and gradually increasing the number of domains available during training. Our results confirm observations made during method development: Our method requires significantly fewer training domains to accurately extrapolate shifts into the distant future, while the TabPFN-base only has acceptable decision boundaries when there is data whose distribution is close to the test domain. Below in Figure 12 are the results for this experiment with the Intersecting Blobs dataset discussed in Section A.4.3.1. The figure clearly shows that our model greatly improves its predictions with the first domains. When provided with the domains \(\mathcal{C}^{\text{train}}=\{0,\dots,3\}\), the shift is largely understood, with newer domains having little impact on performance. On the other hand, TabPFN-base requires much more training domains to achieve similar performance to our method.

Figure 12: This figure shows the performance of Drift-Resilient TabPFN and the baseline TabPFN on the Intersecting Blobs dataset. Thereby, we always test on domain \(\mathcal{C}^{\text{test}}=\{9\}\) and gradually increase on the \(x\)-axis the number of training domains starting with \(\mathcal{C}^{\text{train}}=\{0,1\}\) up to \(\mathcal{C}^{\text{train}}=\{0,1,\dots,8\}\). The results show that Drift-Resilient TabPFN achieves effective extrapolation with as few as four training domains, while TabPFN-base needs significantly more to reach similar performance.

[MISSING_PAGE_EMPTY:28]

#### a.5.3 Algorithmic Overview of Our Approach

```
1:procedureSampleDataset
2:\(\mathcal{G}\leftarrow\textsc{SampleSCM}()\)\(\triangleright\) Sample data-generating SCM
3:\(\tilde{\mathcal{G}}\leftarrow\mathcal{G}.\textsc{Expand}()\)\(\triangleright\) Expand to functional representation
4:\(\mathcal{H}\leftarrow\textsc{SampleSCM}()\)\(\triangleright\) Sample \(2^{\text{nd}}\)-order SCM
5:\(\tilde{\mathcal{H}}\leftarrow\mathcal{H}.\textsc{Expand}()\)\(\triangleright\) Expand to functional representation
6:\(\mathcal{C}\leftarrow\{c_{1},c_{2},\dots,c_{t}\}\)\(\triangleright\) Sample temporal domains
7:\(\mathcal{D}\leftarrow\emptyset\)\(\triangleright\) Initialize dataset
8:for all\(c_{k}\in\mathcal{C}\)do
9:\(\boldsymbol{\omega_{c_{k}}}\leftarrow\mathcal{H}.\textsc{Forward}(c_{k})\)\(\triangleright\) Sample edge shifts
10:\(\tilde{\mathcal{G}}_{c_{k}}\leftarrow\tilde{\mathcal{G}}.\textsc{Update}( \boldsymbol{\omega}_{c_{k}})\)\(\triangleright\) Update edge weights
11:\(\mathcal{D}_{c_{k}}\leftarrow\emptyset\)\(\triangleright\) Initialize sub-dataset
12:for all\(i\in\{1,...,n_{c_{k}}\}\)do\(\triangleright\) Sample sub-dataset
13:\((\boldsymbol{x}_{i},y_{i},c_{k})\leftarrow\tilde{\mathcal{G}}_{c_{k}}.\textsc{Forward}( \epsilon_{i})\)
14:\(\mathcal{D}_{c_{k}}\leftarrow\mathcal{D}_{c_{k}}\cup\{(\boldsymbol{x}_{i},y_ {i},c_{k})\}\)
15:endfor
16:\(\mathcal{D}\leftarrow\mathcal{D}\cup\mathcal{D}_{c_{k}}\)\(\triangleright\) Extend dataset
17:endfor
18:return\(\mathcal{D}\)\(\triangleright\) Return dataset
19:endprocedure ```

**Algorithm 1** This algorithm provides a high-level overview for generating a synthetic dataset in our prior. Although steps are depicted sequentially for clarity, many can be parallelized in actual implementation.

#### a.5.4 Illustration of Adopted Evaluation Strategy

In Figure 16, we provide an illustration of the Eval-Fix evaluation strategy, originally proposed by Yao et al. [11] in the context of the Wild-Time benchmark. It should be noted that the formalism has been adapted to align with the notation used in this paper.

Figure 16: Adapted from the Wild-Time benchmark [11], this illustration portrays the Eval-Fix evaluation strategy employed in our study. The domain boundary is indicated by \(c_{t}\), beyond which datasets are considered part of the out-of-distribution (OOD) test set \(\mathcal{D}^{\text{OOD}}\). To evaluate in-distribution (ID) performance, we subsample 10% of the samples from each dataset prior to this boundary, forming the datasets \(\mathcal{D}^{\text{train}}\) and \(\mathcal{D}^{\text{ID}}\).

### Detailed Overview of Wild-Time Methods

#### a.6.1 Classical Supervised Learning

Empirical Risk Minimization (ERM).ERM - a fundamental approach in supervised learning - focuses on minimizing the average loss over the training dataset. In Wild-Time ERM is defined as typical supervised learning without making use of any temporal information.

#### a.6.2 Continual Learning

Fine-Tuning (FT).FT extends the ERM approach by training on the data of each successive temporal domain separately, allowing the model to adapt to new distributions but risking catastrophic forgetting of past tasks.

Elastic Weight Consolidation (EWC).EWC [58] counters catastrophic forgetting by adding a regularization term that constrains the changes to important model parameters, thus preserving knowledge from previous tasks.

Synaptic Intelligence (SI).SI [59] captures a synaptic strength metric over time for each model parameter. This metric is used as a regularizer to limit changes to important parameters during learning.

Averaged Gradient Episodic Memory (A-GEM).A-GEM [60] maintains a small episodic memory and computes gradients not just for the current task but also the average of the gradients over several past tasks stored in the episodic memory.

#### a.6.3 Temporally Invariant Learning

Deep Correlation Alignment (Deep CORAL).Initially developed for domain adaptation, Deep CORAL [35] aims to align the second-order statistics of features between the source and target domains to minimize distribution shift. In the Wild-Time benchmark, this original purpose is modified to align features across different temporal domains within the training set, thus converting it into a DG method. For handling temporal shifts, it is extended into CORAL-T, which employs sliding windows to segment the data stream into temporal substreams, treating each as a separate domain for alignment. [11]

Group Distributionally Robust Optimization (GroupDRO).Originally designed at optimizing on the worst-performing group within the training data, GroupDRO [32] aims to learn a model that is robust across varying group distributions. In the Wild-Time benchmark, this method is adapted to the temporal context as GroupDRO-T. It utilizes sliding window-based segmentation to create temporal substreams, treating each as a separate group for distributionally robust optimization. [11]

Invariant Risk Minimization (IRM).IRM [31] aims to identify a data representation that is consistently predictive across different domains. In the context of Wild-Time, the method is adapted to temporal shifts and named IRM-T. It employs sliding window-based segmentation to create temporal substreams, which are then treated as individual domains for invariant risk minimization. [11]

Mixup.Mixup [33] is an interpolation-based data augmentation technique that creates new training examples by blending the features and labels of existing samples. This technique aims to enhance the model's ability to generalize across domains by diversifying the training data. It replaces the original training samples with these newly generated interpolations for more robust training.

Learning with Selective Augmentation (LISA).LISA [34], motivated by Mixup, employs selective interpolation to neutralize domain-specific information in the training data. It comes in two variants: intra-label LISA, which interpolates examples from different domains but having the same label, and intra-domain LISA, which interpolates examples within the same domain but having different labels. In Wild-Time, only intra-label LISA is used [11].

#### a.6.4 Self-Supervised Learning

Simple Framework for Contrastive Learning of Visual Representations (SimCLR).SimCLR [61] employs contrastive learning to maximize the agreement between different augmentations of the same image, thereby enhancing the quality of learned visual representations. The approach benefits from learnable nonlinear transformations and optimized contrastive loss parameters.

Swapping Assignments between multiple Views of the same image (SwAV).SwAV [62] employs a clustering approach within the contrastive learning framework. It enforces consistency between cluster assignments across different augmentations of the same image. This obviates the need for pairwise feature comparisons, offering computational efficiency.

#### a.6.5 Bayesian Learning

Stochastic Weight Averaging (SWA).SWA [46] averages multiple parameter values along the stochastic gradient descent (SGD) trajectory to improve in-distribution generalization. It operates with minimal computational overhead and aims to approximate the posterior distribution over model parameters, reflected in the flatness of the learned optima. [11]

### Datasets

#### a.7.1 Validation Datasets

##### a.7.1.1 Synthetic Datasets

Dataset 1 (Shifting Sin Classification).The Shifting Sin Classification dataset is a synthetic, binary classification dataset of 1,500 instances evenly distributed across 10 domains. Each domain is differentiated by a unique shift in the offset of the sinusoid wave function, creating distinct decision boundaries for classification. The dataset contains two features corresponding to the \(x\) and \(y\) coordinates of each instance. Instances are labeled as 1 if they lie above the sine curve and 0 otherwise in their respective domains.

Dataset 2 (Rotated Five Blobs).The Rotated Five Blobs dataset is a synthetically generated dataset consisting of five blobs rotated sequentially counterclockwise \(-20^{\circ}\) around a central point in each domain. It comprises two numerical features representing the \(x\) and \(y\) coordinates of each data point. Each blob consists of 40 samples, resulting in 200 samples per domain, for a total of 2,000 samples across the 10 domains represented.

Dataset 3 (Moving Square).The Moving Square is a synthetically generated dataset designed for a multi-class classification task. It encompasses two features and is divided into six domains, each containing 200 instances, thereby leading to a total of 1,200 samples. In the construction of this dataset, each of the four clusters--representing distinct classes--is initially located on one corner of a square. As we transition through the six domains, each cluster progressively moves along the edge of the square to the next corner.

Dataset 4 (Moving Diagonal Line).The Moving Diagonal Line dataset is a synthetic dataset, generated using the sklearn blobs function. It comprises 1,200 instances, divided across 6 domains, with each domain holding 200 instances. There are two features, corresponding to the \(x\) and \(y\) coordinate of each instance. In this dataset, there are two clusters, each representing a class, following a diagonal line that moves with each domain. Thereby, both clusters move in opposite directions along parallel diagonal next to each other. Each domain in this context represents different stages of the diagonal movement.

#### a.7.1.2 Real World Datasets

Dataset 5 (Indian Liver Patient Dataset).The Indian Liver Patient Dataset (ILPD), referenced from Ramana and Venkateswarlu [63], is tailored for the binary classification task of identifying liver disease. It contains 583 records featuring 10 attributes, including age, gender, and diverse biochemical measurements. Originating from Andhra Pradesh, India, it comprises 416 liver patient records and 167 non-liver patient records. In our settings, every 5-year age interval is considered as an individual domain. The dataset, sourced from the UCI Machine Learning Repository, is geared towards supporting the diagnosis of liver disease.

Dataset 6 (Istanbul Stock Exchange Returns).The Istanbul Stock Exchange Returns dataset sourced from the UCI Machine Learning Repository provided by Akbilgic [64] includes 536 instances of returns from the Istanbul Stock Exchange and seven international indices from June 2009 to February 2011. The eight attributes represent various market return indices. The dataset is thereby used to predict the changes in the Istanbul stock exchange given all the other indices. The target was thereby discretized into 9 categories. The data was processed by dropping the USD column of the ISE and converting dates into a monthly domain feature, introducing a time-based distribution shift.

**Dataset 7** (**Diabetes 130-US Hospitals).** The Diabetes 130-US Hospitals dataset provided by Strack et al. [65] encapsulates a decade (1999-2008) of diabetes care across 130 US hospitals, detailing 50 features related to patient demographics and hospitalization details. Criteria for inclusion are inpatient and diabetic encounters, with stays ranging from 1 to 14 days, where both lab tests and medications were administered. Features include patient identifiers, race, gender, age, admission type, duration of stay, attending physician's specialty, lab test counts, HbA1c results, diagnoses, medication details, and counts of healthcare visits prior to admission. The target of the prediction is to determine whether and in what time frame a patient will be readmitted. It is categorized into <30 days, >30 days, or No for no readmission. The original dataset, with 101,766 instances and 50 features, has been subsampled to 964 instances.

**Dataset 8** (**Airlines Delay**).: The Airlines Delay dataset, sourced from OpenML and provided by Bifet and Ikonomovska [66], contains 539,383 instances, each with 7 features. The task is to predict flight delays based on the scheduled departure information. Features include Airline, Flight, AirportFrom, AirportTo, DayOfWeek, and Time of departure. Notably, departure time, discretized to an interval of full hours, will be our distribution shift domain. The dataset was subsampled, thereby we sampled at most 60 samples per discrete time step. This resulted in 1,380 instances.

**Dataset 9** (**Pima Indians Diabetes**).: The Pima Indians Diabetes dataset from the National Institute of Diabetes and Digestive and Kidney Diseases, referenced by Smith et al. [67], contains 768 instances and 8 medical diagnostic features. These data represent female Pima Indian patients aged 21 or older. The task involves binary classification for predicting diabetes onset. We categorize each successive 2-year age interval as a separate domain, highlighting shifts in the dataset across age groups.

**Dataset 10** (**Diabetes Prediction through Questionaire).: This dataset, collected from Sylhet Diabetes Hospital in Bangladesh and provided by Islam et al. [68], aims to predict early-stage diabetes. It comprises 520 instances and 16 features, representing symptoms and demographic information of patients. The task is binary classification, predicting whether a patient has diabetes or not. Age groups of every successive 5-year interval are considered as different domains, providing 14 age-based domains.

**Dataset 11** (**Room Occupancy Detection**).: The dataset is sourced from the UCI Machine Learning Repository and provided by Candanedo [69]. The preprocessed dataset, reduced to 1800 instances from the original 20,560, is used for binary classification of room occupancy based on Temperature, Humidity, Light, and CO2 levels. After removing 'date' and 'Id' features, 'day' and 'hour' were added. Thereby the 'day' was used as the temporal domain.

**Dataset 12** (**Sao Paulo Urban Traffic Behavior**).: The Sao Paulo Urban Traffic Behavior dataset, sourced from the UCI Machine Learning Repository provided by Ferreira et al. [70], captures records of urban traffic behavior in Sao Paulo, Brazil, from December 14 to 18, 2009, and tries to predict the slowness in traffic. The dataset contains 135 instances each with 18 attributes.

Attributes are various traffic indicators such as Hour, Immobilized bus, Broken Truck, Vehicle excess, Accident victim, and more. We have discretized the target "Slowness in traffic (%)" into intervals of 7.5 percent. Each day represents a different domain, thus introducing a time-based shift in the dataset.

#### a.7.2 Test Datasets

#### a.7.2.1 Synthetic Datasets

**Dataset 13** (**Rotated Two Moons**).: The Rotated Two Moons dataset as stated by Nasery et al. [13] and Bai et al. [12], is a derivative of the 2-entangled moons dataset and includes 220 instances in each of the 10 domains. Each domain is differentiated by counter-clockwise rotations of \(18^{\circ}\), resulting in a rotation of \(18\cdot i^{\circ}\) in domain \(i\). Also, the distribution of a subset of the instances varies across domains.

**Dataset 14** (**Intersecting Blobs**).: The Intersecting Blobs Dataset is a synthetically created set tailored for complex, binary classification tasks. The dataset contains two features per sample and 120 samples per domain in a total of 14 domains. Each domain comprises three classes, each represented by 40 samples appearing as blobs. These blobs move and vary, getting quite close to one another, almost intersecting. This dynamic creates sudden shifts in decision boundaries, increasing the difficulty and complexity of the classification tasks. The continuous shifts in blobs' positions across domains are implemented by adjusting their centers and standard deviations.

**Dataset 15** (**Binary Label Shift**).: The Binary Label Shift Dataset is a synthetic dataset tailored for binary classification in an environment with prior probability shifts. The dataset consists of10 unique domains, with each containing 200 samples and each sample consisting of two features. The key feature of this dataset is the systematic manipulation of class probabilities across domains. This is realized by starting with a high probability of 0.95 for one class in the first domain, which progressively diminishes to 0.05 in the final domain. Simultaneously, the probability for the other class increases from 0.05 to 0.95, following the opposite direction. This dynamic essentially portrays a "fade out and fade in" pattern of the classes across the domains, representing the prior probability shift.

**Dataset 16** (**Rotating Hyperplane**).: The Rotating Hyperplane binary classification dataset is artificially generated based on the package scikit-multiflow provided by Montiel et al. [71]. It consists of five features, of which three shift over time. The dataset is divided into 15 domains of 100 instances each, providing a total of 1500 samples. As the name implies, the key aspect of this dataset is about a rotating hyperplane. In other words, the decision boundary - or hyperplane - shifts as we navigate from one domain to the next, making the classification task increasingly difficult.

**Dataset 17** (**RandomRBF Drift**).: The RandomRBF Drift binary classification dataset is synthetically generated based on the package scikit-multiflow provided by Montiel et al. [71]. This dataset has been constructed by introducing drifts in the data using Radial Basis Functions. It is characterized by the motion of cluster centroids, which are responsible for creating data drift. This movement can be visualized as clusters that change their positions, altering the distribution of data over time. The dataset consists of 8 features, 15 domains with 100 samples in each domain, for a total of 1,500 samples.

**Dataset 18** (**Rotating Segments**).: The Rotating Segments dataset is a synthetically generated dataset tailored for a binary classification task. The dataset visualizes a circle partitioned into four segments, similar to the slices of a cake. The data points are thereby labeled alternately. As we traverse through the ten domains, these segments undergo a rotation. Each domain contains 150 samples, accumulating to 1500 samples in total.

**Dataset 19** (**Sliding Circle**).: The Sliding Circle dataset is a synthetically generated dataset and represents a binary classification task. It comprises two features, the dataset is partitioned into ten domains, each possessing 200 samples, summing up to a total of 2,000 samples. The unique aspect of this dataset is its visual representation: a smaller circle slides around the inner perimeter of a larger circle. Within the larger circle, points are classified based on whether they lie inside the smaller sliding circle or outside of it. As we traverse through the ten domains, the position of the smaller circle changes, causing a shift in the classification of the points.

**Dataset 20** (**Shifting Two Spirals**).: The Shifting Two Spirals dataset is designed for binary classification tasks. The dataset visually represents two intertwined spirals. As we move from one domain to the next, the classification boundary in the spirals evolves. Specifically, one spiral gradually transitions its labels from the center towards the outer end, while the other spiral does the exact opposite, transitioning its labels from the outer end towards the center. This dynamic showcases a fascinating interplay of domain adaptation across the ten domains. Each domain has 200 samples, with 100 samples from each spiral, summing up to a total of 2,000 samples.

#### a.7.2.2 Real World Datasets

**Dataset 21** (**Free Light Chain Mortality**).: The free light chain dataset comprises data from a study investigating the link between serum free light chain (FLC) and mortality. It includes 1125 stratified samples per domain and target from an original pool of 7,874, featuring residents of Olmsted County, Minnesota aged 50 or more.

The task involves predicting mortality based on 9 features, which include age, sex, year of blood sample, FLC portions (kappa and lambda), the FLC group, serum creatinine, MGUS diagnosis, and days from enrollment to death or last follow-up. The feature 'chapter' was omitted because it is direct information on whether someone died or not. We treat "sample.yr", the year in which the sample was taken, as a domain, resulting in a total number of 9 domains. We suspect that both the measurements themselves and the selection of participants have changed over time. The dataset is sourced from studies by Dispenzieri et al. [72] and Kyle et al. [73].

**Dataset 22** (**Electricity**).: This dataset as used by Harries [74] and Gama et al. [75] contains electricity demand in the Australian New South Wales Electricity Market. Since the prices are not fixed, they fluctuate depending on supply and demand. It has 45,312 instances and 5 features. For reproducibility, the dataset includes additional features such as the New South Wales electricity price, which was used to form the target class according to the original paper, and the Victoria electricity price, which was not used in the original paper. The dataset features the demand of electricity in to provinces as well as the transfer between those for periods of 30 minutes. The task is a binary classification, which requires predicting whether the price in the current period is higher or lower than the average of the last 24 hours. The dataset contains seasonal data due to varying demand for electricity. The effects of long-term price trends on the class label are removed by the 24-hour moving average. We consider one-week periods as a single domain. To comply with TabPFN sequence length limits, we keep only two hourly intervals for each day and subsample 15 weeks of the whole time period.

**Dataset 23 (Absenteeism at Work).** The Absenteeism at Work dataset, sourced from the UCI Machine Learning Repository provided by Martiniano and Ferreira [76], comprises 740 instances across 21 features. It captures various attributes of employees and their working conditions, such as the reason for absence, day of the week, seasons, distance to work, and more, with the target feature being the absenteeism time in hours which was discredited into 4-quantiles.

The primary shift in this dataset is supposed to be seasonal. Thereby each consecutive season is treated as a different domain. Furthermore, no significant preprocessing or subsampling was required due to the manageable size of the dataset.

**Dataset 24 (Heart Disease Dataset).** The Heart Disease dataset, sourced from the UCI Machine Learning Repository and provided by Janosi et al. [77], targets the classification task of identifying the presence (values 1,2,3,4) or absence (value 0) of heart disease in patients. We treat the task as a binary classification, predicting only whether a patient has heart disease or not. It includes 303 instances, each with 13 health-related features such as age, sex, resting blood pressure, cholesterol levels, etc. In this context, each consecutive 4-year age interval is viewed as a single domain. Rows with missing values have been omitted.

**Dataset 25 (Parking Birmingham).** The Parking Birmingham dataset, provided by Stolfi [78] via the UCI Machine Learning Repository, initially comprises the capacity and occupancy rates(target) of multiple car parks. We have processed it to only include the car park labeled 'Others-CCCPS133', thereby reducing the number of instances to 1,294 from 35,717. The original 'LastUpdated' attribute has been transformed into 'day', 'week', and'month' features, with the 'week' serving as the temporal domain. The 'Occupancy' target, originally an absolute figure, is now presented as a percentage of the parking space utilization, discretized into 25% intervals.

**Dataset 26 (Ames Housing Prices).** The Ames Housing Dataset, curated by De Cock [79], consists of 1460 instances each with 79 features detailing various aspects of residential homes in Ames, Iowa. We use only the training portion of this dataset due to the absence of ground truth targets in the test data. We have discretized the house price, which was originally a continuous variable, into a categorical variable. This transformation was achieved by partitioning the price data into intervals. Specifically, the intervals are defined as: \([0,125k]\), \((125k,300k]\), \((300k,\infty)\). The task is to predict the price range of a home based on its features. We treat the 'YearBuilt' attribute, divided into 15-year periods, as our domain to capture changes in housing trends over time. It is to be expected that the data set shows temporal shifts, as the price distribution between older and more modern houses differs.

**Dataset 27 (Folktables US Census).** The folktables datasets, derived from the US Census Public Use Microdata Sample (PUMS) data and published by Ding et al. [42] consists of demographic and socioeconomic data between 2015 and 2021. Each year within this timeframe represents a distinct domain for a series of tasks: ACSIncome, ACSPublicCoverage, and ACSEmployment. We purposefully limited our focus to the state Maryland, to limit the shifts to the temporal domain. The size of the datasets necessitated a stratified subsample for the target per year to reach a total of approximately 1300 instances per dataset and meet the TabPFN model requirements. This subsampling ensured a representative yet computationally manageable sample.

The tasks are as follows:

* **ACSIncome:** The task predicts whether an individual's income surpasses $50,000, narrowing the ACS PUMS data to individuals over 16 who reported at least one working hour per week in the past year and a minimum income of $100. The task consists of 10 features accross the 7 domains.
* **ACSPublicCoverage:** The objective is to predict if an individual is covered by public health insurance. The dataset is filtered to include only individuals under 65 with an income below S30,000, focusing the prediction on low-income individuals ineligible for Medicare. The task consists of 19 features accross the 7 domains.
* **ACSEmployment:** The task is to predict whether an individual is employed. The dataset is filtered to include only individuals between 16 and 90 years of age. The task consists of 16 features accross the 7 domains.

**Dataset 28 (Chess).** The Chess dataset published by Zliobaite [80] is derived from recorded chess games, aiming to predict game outcomes (draw, lost, won) through a multi-class classification task. It consists of nine features which provide insights into the game details and player attributes, including move sequences, player side (white or black), current rating, opponent's rating, type of game, speed, and the date of the game (broken down into year, month, and day). The dataset is segmented into 27 domains, where each domain represents 20 consecutive games. This segmentation helps capture the evolution of a player's progress over time. The dataset, in its entirety, holds 533 instances. It has been constructed based on games played between 7 December 2007 and 26 March 2010.

### Hyperparameter Search Spaces

\begin{table}
\begin{tabular}{l l} \hline \hline
**XGBoost** & \\ \hline Parameter & Values \\ \hline learning\_rate & \(e^{L(-7,0)}\) \\ max\_depth & \(\mathcal{U}\{1,2,\ldots,10\}\) \\ subsample & \(\mathcal{U}\{0,2,1\}\) \\ colsample\_bytee & \(\mathcal{U}\{0.2,1\}\) \\ colsample\_bylevel & \(\mathcal{U}\{0.2,1\}\) \\ min\_child\_weight & \(e^{L(-16,5)}\) \\ alpha & \(e^{L(-16,2)}\) \\ lambda & \(e^{L(-16,2)}\) \\ gamma & \(e^{L(-16,2)}\) \\ n\_estimators & \(\mathcal{U}\{100,101,\ldots,4000\}\) \\ \hline
**LightGBM** & \\ \hline Parameter & Values \\ \hline num\_leaves & \(\mathcal{U}\{5,6,\ldots,50\}\) \\ max\_depth & \(\mathcal{U}\{3,4,\ldots,20\}\) \\ learning\_rate & \(e^{L(-3,0)}\) \\ n\_estimators & \(\mathcal{U}\{50,51,\ldots,2000\}\) \\ min\_child\_weight & 1e-5, 1e-3, 1e-2, 1e-1, 1, 1e-1, 1e-2, 1e-3, 1e-4 \\ subsample & \(\mathcal{U}\{0.2,0.8\}\) \\ colsample\_bytee & \(\mathcal{U}\{0.2,0.8\}\) \\ reg\_alpha & 0, 1e-1, 1, 2, 5, 7, 10, 50, 100 \\ reg\_lambda & 0, 1e-1, 1, 5, 10, 20, 50, 100 \\ \hline
**CatBoost** & \\ \hline Parameter & Values \\ \hline learning\_rate & \(e^{L(-5,0)}\) \\ random\_strength & \(\mathcal{U}\{1,2,\ldots,20\}\) \\ I2\_leaf\_reg & \(e^{L(0,log(10))}\) \\ bagging\_temperature & \(\mathcal{U}\{0.0,1.0\}\) \\ leaf\_estimation\_iterations & \(\mathcal{U}\{1,2,\ldots,20\}\) \\ iterations & \(\mathcal{U}\{100,101,\ldots,4000\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Hyperparameter search spaces we used for our baselines.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Underlying MLP (+ ERM, A-GEM, FT)** \\ \hline Parameter & Values \\ \hline train\_update\_iter & \(500,1000,2000,3000,4000,5000,6000,7000\) \\ lr & \(e^{t\mathcal{U}(-14,-4)}\) \\ use\_scheduler & True, False \\ ft\_scheduler\_gamma & \(\mathcal{U}(0,9.1,0)\) \\ weight\_decay & \(0,0,1e-5,1e-2\) \\ early\_stopping & True, False \\ early\_stop\_holdout & \(0.1,0.15,0.2\) \\ early\_stop\_patience & \(\mathcal{U}(10,11,\ldots,30\}\) \\ \hline
**LISA** \\ \hline Parameter & Values \\ \hline mix\_alpha & \(e^{t\mathcal{U}(0.5.4.0)}\) \\ cut\_mix & True, False \\ \hline
**Mixup** \\ \hline Parameter & Values \\ \hline mix\_alpha & \(e^{t\mathcal{U}(-5,0)}\) \\ \hline
**EWC** \\ \hline Parameter & Values \\ \hline gamma & \(e^{t\mathcal{U}(1.0,2.0)}\) \\ eve\_lambda & \(e^{t\mathcal{U}(0.5,2.0)}\) \\ \hline
**GroupDRO-T** \\ \hline Parameter & Values \\ \hline group\_size & \(\mathcal{U}\{1,2,\ldots,6\}\) \\ non\_overlapping & True, False \\ group\_loss\_adjustments & None, \(0.1,0.5,1.0\) \\ group\_loss\_bl & True, False \\ \hline
**SWA** \\ \hline Parameter & Values \\ \hline swa\_portion & \(\mathcal{U}(0.5,0.9)\) \\ swa\_lr\_factor & \(\mathcal{U}\{1,2,\ldots,6\}\) \\ \hline
**IRM-T** \\ \hline Parameter & Values \\ \hline group\_size & \(\mathcal{U}\{1,2,\ldots,6\}\) \\ non\_overlapping & True, False \\ imr\_lambda & \(\mathcal{U}\{1,2,\ldots,100\}\) \\ imr\_penalty\_anneal\_iters & \(0,250,500,750,100\) \\ \hline
**SI** \\ \hline Parameter & Values \\ \hline si\_c & \(\mathcal{U}(0.05,0.2)\) \\ epsilon & \(\mathcal{U}(0.0005,0.002)\) \\ \hline
**CORAL-T** \\ \hline Parameter & Values \\ \hline group\_size & \(\mathcal{U}\{1,2,\ldots,6\}\) \\ non\_overlapping & True, False \\ coral\_lambda & \(\mathcal{U}(0.1,1.0)\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Hyperparameter search spaces we used for Wild-time baselines.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Search Space** \\ \hline model\_type & single \\ N\_ensemble\_configurations & 16, None \\ preprocess\_transforms & See Table 11 \\ softmax\_temperature & log(0.75), log(0.8), log(0.9), log(0.95) \\ use\_poly\_features & True, False \\ max\_poly\_features & 50 \\ remove\_outliers & -1.7, 0.9, 0.12,0 \\ add\_fingerprint\_features & True, False \\ subsample\_samples & 0.9, 0.99, -1 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Preprocessing search spaces for Drift-Resilient TabPFN and TabPFN-base.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Values** \\ \hline names & [”safepower"], ["quantile\_uni,coarse"], ["quantile\_norm\_coarse"], ["adaptive"], ["norm\_and\_kdi"], ["quantile\_uni"], ["none"], ["robust"], ["Xdi\_uni"], ["kd\_alpha\_0.3"], ["kdi\_alpha\_3.0"], ["xfepower", "quantile\_uni"], ["kdi", "quantile\_uni"], ["none", "power"] \\ \hline categorical\_name & ["numeric", "ordinal\_very\_common\_categories\_shuffled", "onehor", "none"] \\ \hline append\_original & [True, False] \\ \hline subsample\_features & [-1, 0.99, 0.95, 0.9] \\ \hline global\_transformer & [None, “svd”] \\ \hline \hline \end{tabular}
\end{table}
Table 11: Parameters and values for enumerate_preprocess_transforms function.

## Appendix B NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We make the following main claims and give justification alongside: * _We claim "Comprehensive evaluations across 18 synthetic and real-world datasets"_ - We describe evaluation strategies in detail in Section 4 and datasets in detail in Appendix Section A.7. * _We claim ".. demonstrate large performance improvements over a wide range of baselines, such as XGB, Catboost and TabPFN. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration."_ - We justify these specific number in evaluations in Section 4. * _We claim qualitatively improved adaption to distribution shifts_ - We show this in Figure 5 and Appendix Sections A.4.3.1, A.4.3.2 and A.4.4.1. * _We claim a novel technique for combining In-Context-Learning and Distribution Shifts_ - On May, 22nd 2024 a Google Scholar Search yields one paper combining these concepts: "In-Context-Learning and Distribution Shifts" by Ahuja and Lopez-Paz [81]. This paper, however, studies the effects of distribution shifts on in-context-learning capabilities, while not exploring strategies to mitigate issues in ICL.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper acknowledges several limitations, including challenges with computational demands when scaling to larger datasets, interpretability, and model assumptions. We discuss those limitations in detail in our "Conclusions and Limitations" Section 5.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not present new theoretical proofs but focuses on algorithmic development and empirical validation.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides detailed descriptions of the experimental settings, datasets, and methodologies used, including links to the code and datasets. See our "Reproducibility" Section A.1.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the full evaluation code, datasets, and pre-trained models at https://github.com/automl/Drift-Resilient_TabPFN. See our "Reproducibility" Section A.1.
6. **Experimental Setting/Details**Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Comprehensive details regarding the experimental setups, including data splits and hyperparameter selection, are thoroughly described, allowing for a clear understanding of how experiments were conducted and how results were derived. See our "Reproducibility" Section A.1.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars and statistical significance for the experiments. We report 95% Confidence Intervals in all our quantitative results and mark this appropriately in the paper.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details about the computational resources used for the experiments, including the types of processors and the computational cost, are explicitly stated, enabling an accurate estimation of the resources required for replication. See Section A.3 for details.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper thoroughly explores the broader impacts of Drift-Resilient TabPFN. Please see Section A.2 for details.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper focuses on a methodological innovation for handling distribution shifts in tabular data, which does not inherently include high-risk data or models such as pretrained language models or image generators. The research does not involve scraped datasets or applications that directly impact individual privacy or security, thus specific high-risk safeguards are not applicable.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper credits all external resources and datasets appropriately and adheres to the licensing agreements of each. It explicitly mentions the use of datasets, with all necessary citations in Section A.7.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release documentation of the released models and code alongside the paper. We also provide experimental notebooks for easy usage.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The research did not involve crowdsourcing or direct research with human subjects. Thus, this section is not applicable to the current study.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: As the study did not involve human subjects or sensitive personal data that would require IRB approval or equivalent ethical review, this section is not applicable.