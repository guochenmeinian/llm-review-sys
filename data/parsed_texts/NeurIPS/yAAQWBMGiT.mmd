# Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning

 Yijun Dong

Courant Institute

New York University

yd1319@nyu.edu

&Hoang Phan

Center of Data Science

New York University

hvp2011@nyu.edu

&Xiang Pan

Center of Data Science

New York University

xiangpan@nyu.edu

Equal contribution.

###### Abstract

We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce **S**ketchy **M**oment **M**atching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace \(\mathcal{S}\); (ii) then the variance is reduced over \(\mathcal{S}\) via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting \(n\) samples by reducing variance over \(\mathcal{S}\) preserves the fast-rate generalization \(O(\dim(\mathcal{S})/n)\), independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.

## 1 Introduction

As the data volume and training cost explode with the unprecedented model performance, the long-standing problem of data selection [1, 2] is getting increasing attention in the modern context of deep learning from various perspectives, including data pruning [3, 4], coreset selection [1, 5, 6, 7, 8, 9], and data filtering [2, 10, 11, 12, 13]. A common goal shared by these perspectives is to train a model from scratch on less data to learn high-quality representations and achieve competitive generalization. However, empirical observations also suggest the limitation of data removal during pre-training: a seemingly inevitable tradeoff between less computation and higher-quality representations [14, 15]. While existing works on data selection have a dominating focus on the training-from-scratch setting, the sensitivity of representation learning to data and the growing availability of powerful pre-trained models calls for attention to a less studied [16] but equally important problem: data selection for finetuning.

In the simplest finetuning setting--linear probing on low-dimensional representations2, data selection falls in the classical frames of coreset selection for linear regression [17, 18, 19, 20, 21, 22, 23, 24] and optimal experimental design [25, 26, 27, 28, 29] where the generalization gap can bereduced by selecting data that minimize the associated variance. However, for high-dimensional finetuning, variance minimization alone is insufficient to characterize the generalization due to the overparametrized nature of modern architectures. Even for linear probing, when the parameter dimension \(r\) is higher than the sample size \(n\), the selected data necessarily fails to capture a subspace of the parameter space with dimension at least \(r-n\), leading to errors in addition to variance. Nevertheless, the prevailing empirical and theoretical evidence [14, 30] on the ubiquitous intrinsic low-dimensional structures of high-dimensional data/model motivates a natural question:

_Can the low intrinsic dimension be leveraged in data selection for high-dimensional finetuning?_

Low intrinsic dimension leads to variance-bias tradeoff in data selection.We provide a positive answer to this question through a variance-bias tradeoff perspective. Intuitively, we consider a low-dimensional subspace \(\mathcal{S}\) in the finetuning parameter space where the model learns the necessary knowledge for the downstream task. The generalization gap can be controlled by simultaneously reducing _the bias (redundant information) by "exploring" the finetuning parameter space to find a suitable \(\mathcal{S}\)_ and _the variance by "exploiting" the useful knowledge in \(\mathcal{S}\)_.

Given the high-dimensional nature of the finetuning parameter space, direct search for such suitable subspace \(\mathcal{S}\) is computationally infeasible in general. This leads to a follow-up question:

_How to explore the intrinsic low-dimensional structure efficiently for data selection?_

We propose a two-stage solution--_Sketchy Moment Matching (SkMM)_: (i) dimensionality reduction via gradient sketching to efficiently explore the finetuning parameter space, and (ii) variance control via moment matching to exploit useful knowledge in the low-dimensional subspace.

Gradient sketching finds a good low-dimensional subspace fast and provably.First, we construct a low-dimensional parameter subspace \(\mathcal{S}\) by sketching the model gradients. Sketching [17, 31] is a well-established dimensionality reduction tool known for affordable and accurate low-rank approximations [32, 33]. In deep learning, sketching recently extends its empirical applications to scalable estimations of influence functions for data selection [16, 34]. We make a first step toward the theoretical guarantee of gradient sketching for data selection: _gradient sketching efficiently finds a low-dimensional subspace \(\mathcal{S}\) with small bias such that selecting \(n\) samples by reducing variance over \(\mathcal{S}\) is sufficient to preserve the fast-rate generalization \(O(\dim(\mathcal{S})/n)\), linear in the low intrinsic dimension \(\dim(\mathcal{S})\) while independent of the high parameter dimension \(\tau\)_.

Moment matching in low dimension selects data that control the variance.Second, we select data that reduce variance in the low-dimensional subspace \(\mathcal{S}\) via moment matching. The variance of data selection is characterized by matching between the sketched gradient moments of the original

Figure 1: Controlling variance-bias tradeoff in data selection for high-dimensional finetuning via gradient sketching \(+\) moment matching (SkMM). Consider a toy dataset with \(N\) samples (in blue) whose finetuning gradients lie in a high-dimensional parameter space \(\mathbb{R}^{r}\) (visualized in 3D) with a low intrinsic dimension (_e.g._, three clusters). The goal is to select \(n=n_{1}+n_{2}+n_{3}<r\) samples for finetuning. (a) **Bias reduction** focuses on minimizing the low-rank approximation error, resulting in uniform selection across clusters regardless of their variance. (b) **Variance reduction3** places more emphasis on high-variance clusters and could lead to large bias by missing low-variance ones. (c) **Gradient sketching** efficiently finds a low-dimensional subspace \(\mathcal{S}\) (where \(\dim(\mathcal{S})<n\)) with small bias. (d) **Moment matching** in \(\mathcal{S}\) controls the variance within the low-bias subspace, leading to a variance-bias balance with fast-rate generalization \(O(\dim(\mathcal{S})/n)\).

and selected datasets, \(\mathbf{\tilde{\Sigma}},\mathbf{\tilde{\Sigma}}_{S}\), formally \(\mathrm{tr}(\mathbf{\tilde{\Sigma}}\mathbf{\tilde{\Sigma}}_{S}^{\intercal})\). This objective involves optimizing over the inversions of (potentially) ill-conditioned matrices, leading to a challenging discrete optimization problem [28; 35]. Under a common heuristic assumption that \(\mathbf{\tilde{\Sigma}}\), \(\mathbf{\tilde{\Sigma}}_{S}\) commute [36; 37], we introduce a continuous relaxation with a quadratic objective and linear constraints that is numerically stable (free of pseudoinverse) and can be efficiently optimized via projected gradient descent.

The contributions of this work are summarized as follows:

* We provide a rigorous generalization analysis on data selection for finetuning, illustrating the critical role of dimensionality by unveiling the variance-bias tradeoff in high dimensions.
* We show that gradient sketching provably finds a low-dimensional parameter subspace \(\mathcal{S}\) with small bias, reducing variance over which preserves the fast-rate generalization \(O(\dim(\mathcal{S})/n)\). Techniques used in analyzing gradient sketching for data selection are agnostic to the selection method or the finetuning setting and could be of independent interest.
* We introduce SkMM, a scalable two-stage data selection method for finetuning that simultaneously "explores" the high-dimensional parameter space via gradient sketching and "exploits" the information in the low-dimensional subspace via moment matching.

### Related Works

Coreset selection and low-rank approximations.From the variance-bias tradeoff perspective, data selection for high-dimensional finetuning can be viewed as a combination of (i) variance reduction in coreset selection for linear regression [17; 18; 19; 21; 23; 24] with low-dimensional features, and (ii) bias reduction via sample-wise low-rank approximation for high-dimensional matrices [38; 39; 40; 41; 42; 43; 44; 45; 46].

Gradient sketching[17; 32] based on Johnson-Lindenstrauss transforms (JLTs) [31; 47] has achieved impressive recent successes in efficient data selection [16] and attribution [34]. Despite the empirical success, theoretical understanding of the effect of gradient sketching on generalization remains limited. We make a first step toward this in the context of data selection leveraging existing theories on sketching (vide Remark 3.1 and Appendix C).

Moment matching and optimal experimental design.Moment matching is an intuitive idea for selecting low-dimensional data (_i.e._, overdetermined with coreset size \(n\) larger than data/representation dimension \(r\)), bearing various objectives like the A/V-optimality [28; 29] from optimal experimental design (OED) [25; 26; 27]. While classical OED studies the overdetermined scenario with \(n\geq r\), efforts have been made to extend the notion of V-optimality beyond the overdetermined setting [48; 49]. Nevertheless, these works focus on the general overparametrized setting without considering potential special structures in data. In the context of data selection, this can lead to pessimistic sample complexity, especially for learning problems with low-intrinsic dimensions.

For multimodal contrastive learning, recent works [12; 13] illustrated the effectiveness of moment matching via tailored data selection criteria for CLIP [50]. Distinct from our setting of general finetuning in both low and high dimensions, these works focus on data filtering (with \(n>r\)) for pretraining from scratch.

(Unsupervised) data selection.In this work, we focus on unsupervised data selection that instead of relying on labels4, leverages the geometry of the feature space and aims to select samples that are spread out, with a broad spectrum of concretizations including herding [51; 52], k-center greedy [53], leverage score sampling [24; 54; 55], adaptive sampling [44; 56], and volume sampling [39; 41].

Footnote 4: From the theory perspective, data selection for finetuning is less sensitive to labels compared to training from scratch, especially given suitable pre-trained models with reasonable zero-shot accuracy (_e.g._, Assumption 2.2).

An inspiring recent work [57] investigates the generalization of weakly supervised data selection via independent sampling in the low- (\(n\to\infty\) with fixed \(r\)) and high-dimensional (\(n,r\to\infty\) with \(n/r\to\text{constant}\)) asymptotics. Instead of the asymptotic regime, we consider a realistic setting with finite \(n\) and \(r\), without specific assumptions on the data/feature distribution other than the low intrinsic dimension. Along this line, (weakly) supervised data selection commonly make choices based on the uncertainty [58; 59; 60] or sensitivity of the loss to samples (_e.g._, influence function [4; 61; 62], sensitivity scores [5; 63; 64; 65], and heuristics based on losses and their gradients [66; 67; 68; 69]).

### Notations

Given any \(n\in\mathbb{Z}_{+}\), we denote \([n]=\{1,\cdots,n\}\). Let \(\mathbf{e}_{n}\) be the \(n\)-th canonical basis of the conformable dimension; \(\mathbf{I}_{n}\) be the \(n\times n\) identity matrix; and \(\boldsymbol{0}_{n},\mathbf{I}_{n}\in\mathbb{R}^{n}\) being vectors with all entries equal to zero and one, respectively. Let \(\mathbb{S}^{n-1}:=\{\mathbf{x}\in\mathbb{R}^{n}|\|\mathbf{x}\|_{2}=1\}\) be the unit sphere in \(\mathbb{R}^{n}\), and \(\Delta_{n}:=\left\{\mathbf{p}\in\left[0,1\right]^{b}\,\left\|\,\mathbf{p} \right\|_{1}=1\right\}\) be the dimension-\(n\) probability simplex. We adapt the standard asymptotic notations: for any functions \(f,g:\mathbb{R}_{+}\rightarrow\mathbb{R}_{+}\), we write \(f=O\left(g\right)\) or \(f\lesssim g\) if there exists some constant \(C>0\) such that \(f(x)\leq Cg(x)\) for all \(x\in\mathbb{R}_{+}\); \(f=\Omega\left(g\right)\) or \(f\gtrsim g\) if \(g=O\left(f\right)\); \(f\asymp g\) if \(f=O\left(g\right)\) and \(f=\Omega\left(g\right)\). For any matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), let \(s_{1}(\mathbf{A})\geq\cdots\geq s_{\mathrm{rank}(\mathbf{A})}(\mathbf{A})\geq 0\) be the singular values; and \(\mathbf{A}^{\dagger}\) be the Moore-Penrose pseudoinverse. Additionally for any \(k\leq\mathrm{rank}\left(\mathbf{A}\right)\), let \(\left\langle\mathbf{A}\right\rangle_{k}=\mathrm{argmin}_{\mathbf{B}:\, \mathrm{rank}(\mathbf{B})\leq k}\left\|\mathbf{A}-\mathbf{B}\right\|_{F}\) be the optimal rank-\(k\) approximation of \(\mathbf{A}\) (characterized by the rank-\(k\) truncated SVD). For any symmetric matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{d\times d}\), we write \(\mathbf{A}\succcurlyeq\mathbf{B}\) or \(\mathbf{A}-\mathbf{B}\succcurlyeq 0\) if \(\mathbf{A}-\mathbf{B}\) is positive semidefinite.

## 2 Data Selection for Finetuning

Given a data space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and a label space \(\mathcal{Y}\subseteq\mathbb{R}\), let \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\in\mathcal{X}\times\mathcal{Y}\,|\,i\in[N]\}\) be a large dataset, with matrix form \((\mathbf{X},\mathbf{y})\in\mathbb{R}^{N\times d}\times\mathbb{R}^{N}\), for some downstream task where the performance is measured by a loss function \(\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}_{\geq 0}\).

Finetuning.Let \(\mathcal{F}\) be a class of prediction functions where each \(f=h\circ\phi\in\mathcal{F}\) can be expressed as the composition of an expressive representation function \(\phi\) and a prediction head \(h\). We consider a pre-trained model \(\phi\) that yields high-quality representations for some downstream tasks on \(\mathcal{D}\) and denote \(\mathcal{F}_{|\phi}\subseteq\mathcal{F}\) as the class of finetuned models based on \(\phi\). Assume that for every \((\mathbf{x}_{i},y_{i})\in\mathcal{D}\), \(y_{i}\sim P\left(y\mid\mathbf{x}_{i}\right)\)_i.i.d._ such that there exists \(f_{*}^{\phi}\in\mathcal{F}_{|\phi}\) with respect to \(\phi\) satisfying (i) \(\mathbb{E}\left[y_{i}\mid\phi\left(\mathbf{x}_{i}\right)\right]=f_{*}^{\phi} \left(\mathbf{x}_{i}\right)\), and (ii) \(\mathbb{V}\left[y_{i}\mid\phi\left(\mathbf{x}_{i}\right)\right]\leq\sigma^{2}\) for some \(\sigma>0\) (which will be formalized later in respective settings).

Data selection.Instead of finetuning on the entire dataset \(\mathcal{D}\), we aim to select a small coreset \(\mathcal{D}_{S}\subseteq\mathcal{D}\) of size \(n\ll N\) where the generalization is close. Precisely, let \(\mathcal{D}_{S}\) be indexed by \(S\subset[N]\) and denoted as \((\mathbf{X}_{S},\mathbf{y}_{S})\in\mathbb{R}^{n\times d}\times\mathbb{R}^{n}\). With \(\mathcal{L}_{\mathcal{D}_{S}}\left(f\right)=\frac{1}{n}\sum_{(\mathbf{x},y)\in \mathcal{D}_{S}}\ell\left(f(\mathbf{x}),y\right)\) and a regularization \(\mathcal{R}:\mathcal{F}_{|\phi}\rightarrow\mathbb{R}_{\geq 0}\) associated with a hyperparameter \(\alpha\geq 0\), we want \(f_{S}=\mathrm{argmin}_{f\in\mathcal{F}_{|\phi}}\mathcal{L}_{\mathcal{D}_{S}} \left(f\right)+\alpha\cdot\mathcal{R}\left(f\right)\) to provide a low excess risk over \(\mathcal{D}\): \(\mathrm{ER}\left(f_{S}\right):=\frac{1}{N}\sum_{i=1}^{N}\ell(f_{S}(\mathbf{x}_{ i}),f_{*}^{\phi}(\mathbf{x}_{i}))\).

### Low-dimensional Linear Probing: Variance Minimization

Warming up with linear probing, we concretize the general assumption on the ground truth (_i.e._, \(\mathbb{E}\left[y_{i}\mid\phi\left(\mathbf{x}_{i}\right)\right]=f_{*}^{\phi} \left(\mathbf{x}_{i}\right)=\phi\left(\mathbf{x}_{i}\right)^{\top}\boldsymbol{ \theta}_{*}\) and \(\mathbb{V}\left[y_{i}\mid\phi\left(\mathbf{x}_{i}\right)\right]\leq\sigma^{2}\)) as follows:

**Assumption 2.1** (linear probing ground truth).: _Assume \(\mathbf{y}=\phi\left(\mathbf{X}\right)\boldsymbol{\theta}_{*}+\mathbf{z}\) for some \(\boldsymbol{\theta}_{*}\in\mathbb{R}^{r}\) where \(\mathbf{z}=\left[z_{1},\cdots,z_{N}\right]^{\top}\in\mathbb{R}^{N}\) consists of i.i.d. entries with \(\mathbb{E}\left[\mathbf{z}\right]=\boldsymbol{\theta}_{N}\) and \(\mathbb{E}\left[\mathbf{z}\mathbf{z}^{\top}\right]\preccurlyeq\sigma^{2} \mathbf{I}_{N}\)._

Consider the pre-trained representations \(\phi(\mathbf{X})\in\mathbb{R}^{N\times r}\) and \(\phi(\mathbf{X}_{S})\in\mathbb{R}^{n\times r}\) with respective moments \(\boldsymbol{\Sigma}^{\phi}:=\frac{1}{N}\phi\left(\mathbf{X}\right)^{\top}\phi \left(\mathbf{X}\right)\) and \(\boldsymbol{\Sigma}^{\phi}_{S}:=\frac{1}{n}\phi\left(\mathbf{X}_{S}\right)^{\top} \phi\left(\mathbf{X}_{S}\right)\). For low-dimensional linear probing with \(r\leq n\) (_s.t._\(\mathrm{rank}(\boldsymbol{\Sigma}^{\phi}_{S})=r\)), the linear regression \(\boldsymbol{\theta}_{S}=\mathrm{argmin}_{\boldsymbol{\theta}}\frac{1}{n}\left\| \phi\left(\mathbf{X}_{S}\right)\boldsymbol{\theta}-\mathbf{y}_{S}\right\|_{2}^{2}\) has a unique solution with excess risk \(\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)=\left\|\boldsymbol{\theta}_{S}- \boldsymbol{\theta}_{*}\right\|_{\boldsymbol{\Sigma}^{\phi}}^{2}\)5 controlled by \(\boldsymbol{\Sigma}^{\phi}\) and \(\boldsymbol{\Sigma}^{\phi}_{S}\), analogous to the V-optimality criterion [28, 29] in optimal experimental design:

Footnote 5: For any \(\mathbf{u}\in\mathbb{R}^{r}\), \(\left\|\mathbf{u}\right\|_{\boldsymbol{\Sigma}^{\phi}}:=\sqrt{\mathbf{u}^{\top} \boldsymbol{\Sigma}^{\phi}\mathbf{u}}\) is the seminorm associated with \(\boldsymbol{\Sigma}^{\phi}\succcurlyeq 0\).

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\leq \frac{\sigma^{2}}{n}\operatorname{tr}(\boldsymbol{\Sigma}^{\phi}( \boldsymbol{\Sigma}^{\phi}_{S})^{-1}),\] (1)

If \(\mathcal{D}_{S}\) satisfies \(\boldsymbol{\Sigma}^{\phi}\asymp c_{S}\boldsymbol{\Sigma}^{\phi}_{S}\) for some \(c_{S}\geq\frac{n}{N}\), then \(\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\leq c_{S} \frac{\sigma^{2}r}{n}\) (proof in Appendix B.1), where \(c_{S}\) characterizes the variance controlled by \(\mathcal{D}_{S}\), _i.e._, smaller \(c_{S}\) implies lower variance.

Despite its simplicity, uniform sampling is often observed in practice to serve as a strong baseline for data selection [1], especially when \(n\) is large. In the low-dimensional linear probing scenario, (1) provides a theoretical justification for such effectiveness of uniform sampling:

**Proposition 2.1** (Uniform sampling for low-dimensional linear probing (Appendix B.2)).: _Assume there exists (i) \(B_{\phi}>0\) such that \(\left\|\phi(\mathbf{x})\right\|_{2}\leq B_{\phi}\;\forall\;\mathbf{x}\in \mathcal{D}\); and (ii) \(\gamma>0\) with \(\mathbf{\Sigma}^{\phi}\succcurlyeq\gamma\mathbf{I}_{r}\). For \(S\) sampled uniformly (with replacement) over \(\mathcal{D}\), with probability at least \(1-\delta\) over \(S\), \(\mathbf{\Sigma}^{\phi}\Leftarrow c_{S}\mathbf{\Sigma}_{S}^{\phi}\) for any \(c_{S}>1\) if \(n\gtrsim\frac{B_{\phi}^{4}}{\gamma^{2}}\cdot\frac{r+\log(1/\delta)}{(1-1/c_{S}) ^{2}}\)._

That is, for linear probing with sufficiently low dimension \(r\ll n\), under mild regularity assumptions on data, uniform sampling enjoys a near-optimal generalization \(O(r/n)\).

### High-dimension Finetuning with Low Intrinsic Dimension: Variance-Bias Tradeoff

Extending the analysis to general finetuning, we consider a set of \(r\) finetuning parameters \(\boldsymbol{\theta}\in\mathbb{R}^{r\,6}\) (potentially with \(r\gg n\)) over a pre-trained model \(\phi\) (_e.g._, \(\boldsymbol{\theta}\) can be the parameters of the last layer (_i.e._, linear probing), last few layers, the entire network, or the LoRA [70] matrices).

Let \(\mathcal{F}_{\left|\phi\right\rangle}=\left\{f^{\phi}\left(\cdot;\boldsymbol{ \theta}\right):\mathcal{X}\rightarrow\mathbb{R}\;\middle|\;\boldsymbol{\theta }\in\mathbb{R}^{r}\right\}\) be the finetuning function class. Without loss of generality, we assume zero initialization of \(\boldsymbol{\theta}\) such that \(f^{\phi}\left(\cdot;\boldsymbol{0}_{r}\right)\) corresponds to the pre-trained model. Analogous to the assumption in [16], under locality constraint on \(\boldsymbol{\theta}\) (_e.g._, \(\left\|\boldsymbol{\theta}\right\|_{2}<1\)), the dynamics of finetuning falls in the kernel regime [71] where \(f^{\phi}\) can be approximated by its first-order Taylor expansion: \(f^{\phi}\left(\mathbf{x};\boldsymbol{\theta}\right)\approx f^{\phi}\left( \mathbf{x};\boldsymbol{0}_{r}\right)+\nabla_{\boldsymbol{\theta}}f^{\phi} \left(\mathbf{x};\boldsymbol{0}_{r}\right)^{\top}\boldsymbol{\theta}\). Then, we formalize the ground truth as follows:

**Assumption 2.2** (Finetuning ground truth).: _Given the pre-trained \(\phi\), there exists a bounded ground truth \(\boldsymbol{\theta}_{*}\in\mathbb{R}^{r}\) with \(\left\|\boldsymbol{\theta}_{*}\right\|_{2}<1\) such that for all \(\left(\mathbf{x},y\right)\in\mathcal{D}\), (i) \(\mathbb{E}\left[y\;\middle|\;\phi\left(\mathbf{x}\right)\right]=f^{\phi}_{*} \left(\mathbf{x}\right)=f^{\phi}\left(\mathbf{x};\boldsymbol{\theta}_{*}\right)\), and (ii) \(\mathbb{V}\left[y\;\middle|\;\phi\left(\mathbf{x}\right)\right]\leq\sigma^{2}\) for some \(\sigma>0\)._

Intuitively, Assumption 2.2 implies that the pre-trained model \(f^{\phi}\left(\cdot;\boldsymbol{0}_{r}\right)\) has a reasonable zero-shot performance. Given any \(S\subset\left[N\right]\) with \(\left|S\right|=n\), let \(f^{\phi}\left(\mathbf{X}_{S};\boldsymbol{\theta}\right)\in\mathbb{R}^{n}\) and \(\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{X}_{S};\boldsymbol{\theta} \right)\in\mathbb{R}^{n\times r}\) be the evaluation of \(f^{\phi}\left(\mathbf{x};\boldsymbol{\theta}\right)\) and its Jacobian over \(\mathbf{X}_{S}\) at \(\boldsymbol{\theta}\). We observe that with \(\mathbf{z}:=\mathbf{y}-f^{\phi}\left(\mathbf{X};\boldsymbol{\theta}_{*}\right)\), Assumption 2.2 implies \(\mathbf{y}-f^{\phi}\left(\mathbf{X};\boldsymbol{0}_{r}\right)\approx\mathbf{G} \boldsymbol{\theta}_{*}+\mathbf{z}\) where \(\mathbb{E}\left[\mathbf{z}\right]=\boldsymbol{\theta}_{N}\) and \(\mathbb{E}\left[\mathbf{z}\mathbf{z}^{\top}\right]\preccurlyeq\sigma^{2} \mathbf{I}_{N}\); while \(\mathbf{G}:=\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{X};\boldsymbol{ 0}_{r}\right)\in\mathbb{R}^{N\times r}\) is the Jacobian over \(\mathcal{D}\) at initialization.

Then in the kernel regime [71], the finetuning objective \(\min_{\boldsymbol{\theta}\in\mathbb{R}^{r}}\frac{1}{n}\left\|f^{\phi}\left( \mathbf{X}_{S};\boldsymbol{\theta}\right)-\mathbf{y}_{S}\right\|_{2}^{2}+ \alpha\left\|\boldsymbol{\theta}\right\|_{2}^{2}\) can be well approximated by a ridge regression problem:

\[\boldsymbol{\theta}_{S}=\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathbb{ R}^{r}}\frac{1}{n}\left\|\nabla_{\boldsymbol{\theta}}f^{\phi}\left( \mathbf{X}_{S};\boldsymbol{0}_{r}\right)\boldsymbol{\theta}-\left(\mathbf{y}_{ S}-f^{\phi}\left(\mathbf{X}_{S};\boldsymbol{0}_{r}\right)\right)\right\|_{2}^{2}+ \alpha\left\|\boldsymbol{\theta}\right\|_{2}^{2}.\] (2)

Recall \(\mathbf{G}:=\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{X};\boldsymbol{0} _{r}\right)\in\mathbb{R}^{N\times r}\) and \(\mathbf{G}_{S}:=\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{X}_{S}; \boldsymbol{0}_{r}\right)\in\mathbb{R}^{n\times r}\). With the moments \(\mathbf{\Sigma}^{\phi}=\frac{1}{N}\mathbf{G}^{\top}\mathbf{G}\) and \(\mathbf{\Sigma}_{S}^{\phi}=\frac{1}{n}\mathbf{G}_{S}^{\top}\mathbf{G}_{S}\), the excess risk \(\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right)=\left\|\boldsymbol{ \theta}_{S}-\boldsymbol{\theta}_{*}\right\|_{\mathbf{\Sigma}^{\phi}}^{2}\) satisfies7:

Footnote 7: Notice that for linear probing, \(f^{\phi}\left(\mathbf{x};\boldsymbol{\theta}\right)=f^{\phi}\left(\mathbf{x}; \boldsymbol{0}_{r}\right)+\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{x}; \boldsymbol{0}_{r}\right)^{\top}\boldsymbol{\theta}\) with \(\nabla_{\boldsymbol{\theta}}f^{\phi}\left(\mathbf{x};\boldsymbol{0}_{r}\right)= \phi\left(\mathbf{x}\right)\). Therefore, the finetuning objective can be exactly formulated as (2), and the excess risk of high-dimensional linear probing satisfies Theorem 2.2 with \(\mathbf{\Sigma}^{\phi}:=\frac{1}{N}\psi\left(\mathbf{X}\right)^{\top}\phi\left( \mathbf{X}\right)\) and \(\mathbf{\Sigma}_{S}^{\phi}:=\frac{1}{n}\phi\left(\mathbf{X}_{S}\right)^{\top} \phi\left(\mathbf{X}_{S}\right)\).

**Theorem 2.2** (Main result I: variance-bias tradeoff (Appendix B.3)).: _Given \(S\), let \(\mathbf{P}_{\mathcal{S}}\in\mathbb{R}^{r\times r}\) be an orthogonal projector onto some subspace \(\mathcal{S}\subseteq\operatorname{Range}(\mathbf{\Sigma}_{S}^{\phi})\), and \(\mathbf{P}_{\mathcal{S}}^{\perp}=\mathbf{I}_{r}-\mathbf{P}_{\mathcal{S}}\) be its orthogonal complement. Under Assumption 2.1, there exists an \(\alpha>0\) such that (2) satisfies \(\mathbb{E}\left[\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\leq \text{variance}+\textbf{bias}\) with (i) \(\textbf{variance}=\frac{2\sigma^{2}}{n}\operatorname{tr}(\mathbf{\Sigma}^{\phi}( \mathbf{P}_{\mathcal{S}}\mathbf{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}})^{ \dagger})\) and (ii) \(\textbf{bias}=2\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{ \mathcal{S}}^{\perp}\right)\left\|\boldsymbol{\theta}_{*}\right\|_{2}^{2}\)._

Specifically, the variance-bias tradeoff is controlled by the unknown \(\mathcal{S}\): expanding \(\mathcal{S}\) leads to higher variance but lower bias. Reducing the generalization gap involves finding a suitable \(\mathcal{S}\) in the high-dimensional parameter space, a computationally challenging problem addressed in Section 3.1.

It is worth highlighting that Theorem 2.2 encapsulates both the low- and high-dimensional finetuning. For low-dimensional linear probing, (1) is a special case of Theorem 2.2 (up to constants) with \(\mathbf{P}_{\mathcal{S}}=\mathbf{I}_{r}\). While in high dimension, an intrinsic low-dimensional structure (_e.g._, Assumption 2.3) is necessary for the effectiveness of data selection8.

Footnote 8: Otherwise, if all directions of \(\operatorname{Range}(\mathbf{\Sigma}^{\phi})\) are equally important, with \(n\ll r\), \(\boldsymbol{\theta}_{S}\) learned from \(\mathcal{D}_{S}\) necessarily fails to capture the orthogonal complement of \(\operatorname{Range}(\mathbf{\Sigma}_{S}^{\phi})\) and therefore \(\mathbb{E}\left[\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right)\right] \gtrsim r-n\).

**Assumption 2.3** (Low intrinsic dimension).: _Consider the second moment \(\bm{\Sigma}^{\phi}\succcurlyeq 0\) over \(\mathcal{D}\) with \(N\) samples. Let \(\overline{r}:=\min\{t\in[r]\mid\operatorname{tr}\left(\bm{\Sigma}^{\phi}-\langle \bm{\Sigma}^{\phi}\rangle_{t}\right)\leq\operatorname{tr}\left(\bm{\Sigma}^{ \phi}\right)/N\}\) be the intrinsic dimension. Assume that \(\bm{\Sigma}^{\phi}\) has a low intrinsic dimension: \(\overline{r}\ll\min\left\{N,r\right\}\)._

When the high-dimensional finetuning parameter space has a low intrinsic dimension \(\overline{r}\ll\min\left\{N,r\right\}\), Theorem 2.2 can be further concretized with suitable \(\mathcal{D}_{S}\) and associated \(\mathcal{S}\):

**Corollary 2.3** (Exploitation + exploration (Appendix B.3)).: _Under the same setting as Theorem 2.2 and Assumption 2.3, if \(S\) satisfies for some subspace \(\mathcal{S}\subseteq\operatorname{Range}(\bm{\Sigma}^{\phi}_{S})\) with \(\operatorname{rank}\left(\mathbf{P}_{\mathcal{S}}\right)\asymp\overline{r}\) and \(c_{S}\geq\frac{n}{N}\) that (i) \(\mathbf{P}_{\mathcal{S}}(c_{S}\bm{\Sigma}^{\phi}_{\mathcal{S}}-\bm{\Sigma}^{ \phi})\mathbf{P}_{\mathcal{S}}\succcurlyeq 0\) and (ii) \(\operatorname{tr}(\bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\frac{1}{S}}) \leq\frac{N}{n}\operatorname{tr}(\bm{\Sigma}^{\phi}-\langle\bm{\Sigma}^{\phi }\rangle_{\overline{r}})\), then9_

Footnote 9: We note that in contrast to the classical slow rate \(O(1/\sqrt{n})\) in low dimension (when \(n>r\)), ridge regression on \(\mathcal{D}_{S}\) in the high-dimensional finetuning (with \(n<r\)) achieves a fast rate \(O(1/n)\). This is granted by the low-rankness of \(\bm{\Sigma}^{\phi}_{S}\), which enables a more fine-grained analysis of the regularization (vide Appendix B.3).

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\leq\textbf{ variance}+\textbf{bias}\lesssim\frac{1}{n}\left(c_{S}\sigma^{2}\overline{r}+ \operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)\left\|\bm{\theta}_{*}\right\| _{2}^{2}\right).\] (3)

In particular, with \(c_{S},\sigma\lesssim 1\), \(\left\|\bm{\theta}_{*}\right\|_{2}^{2}<1\), and \(\operatorname{tr}(\bm{\Sigma}^{\phi})\asymp\overline{r}\) (depending only on the low intrinsic dimension), the generalization achieves a fast rate \(O(\overline{r}/n)\), independent of \(r\gg\overline{r}\).

In (3), (i) **bias** is reduced by exploring the parameter space for an \(\mathcal{S}\) with small low-rank approximation error \(\operatorname{tr}(\bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\frac{1}{S}}) \leq\frac{1}{n}\operatorname{tr}(\bm{\Sigma}^{\phi})\); while (ii) **variance** is reduced by exploiting information in \(\mathcal{S}\) through moment matching, \(\mathbf{P}_{\mathcal{S}}(c_{S}\bm{\Sigma}^{\phi}_{\mathcal{S}}-\bm{\Sigma}^{ \phi})\mathbf{P}_{\mathcal{S}}\succcurlyeq 0\), where smaller \(c_{S}\) means better exploitation.

## 3 Sketchy Moment Matching

A gap between Corollary 2.3 and practice is _how to find a suitable \(\mathcal{S}\) efficiently in the high-dimensional parameter space_. In this section, we introduce a simple scalable algorithm for constructing \(\mathcal{S}\) and \(\mathcal{D}_{S}\) that satisfies the exploration and exploitation conditions in Corollary 2.3.

### Find Low Intrinsic Dimension via Gradient Sketching

For high-dimensional finetuning with \(r\gg n\), a critical limit of Theorem 2.2 and Corollary 2.3 is that the large moment matrices \(\bm{\Sigma}^{\phi},\bm{\Sigma}^{\phi}_{\mathcal{S}}\) are not invertible, storable, or even directly computable, due to the prohibitive cost. As a remedy, sketching [17; 32] via Johnson-Lindenstrauss transforms [31] is a classical dimensionality reduction strategy that gets increasing recent attention for gradient approximation in large-scale machine learning problems [16; 34]10.

Footnote 10: We highlight a key nuance here: for fast influence function approximation in [16; 34], the gradient of the _loss function_ is sketched, whereas in our setting, we sketch the gradient of the _pre-trained model_\(f^{\phi}(\mathbf{x};\mathbf{0}_{r})\).

**Remark 3.1** (Gradient sketching).: _In the high-dimensional setting with \(r\gg n\), to reduce the dimensionality of the gradients \(\mathbf{G}=\nabla_{\bm{\theta}}f^{\phi}\left(\mathbf{X};\bm{\theta}_{r}\right) \in\mathbb{R}^{N\times r}\) with a low intrinsic dimension \(\overline{r}\ll\min\left\{N,r\right\}\) (Assumption 2.3), we draw a Johnson-Lindenstrauss transform [31] (JLT, formally in Definition C.1) \(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) that projects the dimension-\(r\) gradients to a lower dimension \(m\asymp\overline{r}\ll r\): \(\widetilde{\mathbf{G}}=\mathbf{G}\bm{\Gamma}\in\mathbb{R}^{N\times m}\). One of the most common constructions of JLT is the Gaussian embedding (i.e., a Gaussian random matrix with i.i.d. entries \(\bm{\Gamma}_{ij}\sim\mathcal{N}(0,1/m)\) discussed in Lemma C.3, vide Remark C.1 for a brief overview of various (fast) JLTs and their efficiency)._

While sketching is known for preserving Euclidean distances [31] and providing accurate low-rank approximations [17; 32; 33], _whether gradient sketching can convert Theorem 2.2 to an efficiently computable form without compromising the generalization guarantee?_ We answer this question affirmatively with the following theorem.

**Theorem 3.1** (Main result II: gradient sketching (formally in Theorem C.1)).: _Under Assumption 2.2 and 2.3 with a low intrinsic dimension \(\overline{r}\ll\min\left\{N,r\right\}\), draw a Gaussian embedding \(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) (Lemma C.3) with \(m\geq 11\overline{r}\). Let \(\tilde{\bm{\Sigma}}^{\phi}:=\bm{\Gamma}^{\top}\bm{\Sigma}^{\phi}\bm{\Gamma}\) and \(\tilde{\bm{\Sigma}}^{\phi}_{\mathcal{S}}:=\bm{\Gamma}^{\top}\bm{\Sigma}^{\phi}_ {\mathcal{S}}\bm{\Gamma}\) be the sketched gradient moments. For any \(\mathcal{D}_{S}\) with \(n>m\) samples such that \(\operatorname{rank}(\bm{\Sigma}^{\phi}_{\mathcal{S}})=n\), and the \(\lceil 1.1\overline{r}\rceil\)-th largest eigenvalue \(s_{\lceil 1.1\overline{r}\rceil}(\widetilde{\bm{\Sigma}}^{\phi}_{\mathcal{S}}) \geq\gamma_{S}\) for some \(\gamma_{S}>0\), with probability at least \(0.9\) over \(\bm{\Gamma}\), there exists \(\alpha>0\) where (2) satisfies \(\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\lesssim\textbf{ variance}+\textbf{sketching error}+\textbf{bias}\) with (i) **variance**\(=\frac{\sigma^{2}}{n}\operatorname{tr}(\widetilde{\bm{\Sigma}}^{\phi}_{ \mathcal{S}})^{\dagger})\), (ii) **sketching error**\(=\frac{\sigma^{2}}{n}\frac{1}{m\gamma_{S}}\|\widetilde{\bm{\Sigma}}^{\phi}( \widetilde{\bm{\Sigma}}^{\phi}_{\mathcal{S}})^{\dagger}\|_{2}\operatorname{tr}( \bm{\Sigma}^{\phi})\), and (iii) \(\textbf{bias}=\frac{1}{n}\|\tilde{\bm{\Sigma}}^{\phi}(\widetilde{\bm{\Sigma}}^{ \phi}_{\mathcal{S}})^{\dagger}\|_{2}\operatorname{tr}(\bm{\Sigma}^{\phi})\|\bm{ \theta}_{*}\|_{2}^{2}\).__If \(S\) further satisfies \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_{S}\) for some \(c_{S}\geq\frac{n}{N}\), with \(m=\max\{\sqrt{\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)/\gamma_{S}},11\bar {\tau}\}\),_

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\lesssim \textbf{variance}+\textbf{sketching error}+\textbf{bias}\lesssim\frac{c_{S}}{n} \left(\sigma^{2}m+\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)\|\bm{ \theta}_{*}\|_{2}^{2}\right).\] (4)

Comparing (4) with (3), we observe that by controlling the variance with \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) in low dimension \(m\asymp\bar{\tau}\ll r\), gradient sketching preserves the fast-rate generalization \(O(m/n)=O(\bar{r}/n)\) up to constants. That is, gradient sketching implicitly finds a random subspace \(\bm{S}\subseteq\operatorname{Range}(\bm{\Sigma}^{\phi}_{S})\) (vide (9)) that satisfies the exploration assumption in Corollary 2.3. Meanwhile, the choice of sketching size \(m\) balances the tradeoff between **variance** and **sketching error**: a larger \(m\) reduces the sketching error at the cost of higher variance. Such tradeoff is optimized at \(m=\sqrt{\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)/\gamma_{S}}\).

### Control Variance via Moment Matching

Given the intrinsic low-dimensional structure with small bias in Section 3.1, Theorem 3.1 connects generalization to the variance controlled by the matching between \(\widetilde{\bm{\Sigma}}^{\phi}\) and \(\widetilde{\bm{\Sigma}}^{\phi}_{S}\). Specifically, when the selected data \(\mathcal{D}_{S}\) satisfies \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) for some \(c_{S}\geq\frac{n}{N}\), we have \(\operatorname{tr}(\widetilde{\bm{\Sigma}}^{\phi}(\widetilde{\bm{\Sigma}}^{ \phi}_{S})^{\dagger})\leq c_{S}m\) and \(\|\widetilde{\bm{\Sigma}}^{\phi}(\widetilde{\bm{\Sigma}}^{\phi}_{S})^{\dagger} \|_{2}\leq c_{S}\) upper bounded, leading to the fast-rate generalization in (4).

```
1:Input:\(f^{\phi}\left(\cdot;\bm{0}_{r}\right)\), \(n\ll N\), \(m<n\), \(c_{S}\in[\frac{n}{N},1]\).
2:Draw a (fast) Johnson-Lindenstrauss transform \(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) (Remark 3.1).
3:Compute gradient sketching \(\widetilde{\bm{\Omega}}=\nabla_{\bm{\theta}}f^{\phi}\left(\mathbf{X};\bm{0}_{ r}\right)\bm{\Gamma}\in\mathbb{R}^{N\times m}\). (Remark 3.4)
4:Compute the spectral decomposition of \(\widetilde{\bm{\Sigma}}^{\phi}=\frac{1}{N}\widetilde{\mathbf{G}}^{\top} \widetilde{\mathbf{G}}\succcurlyeq 0\): \(\widetilde{\bm{\Sigma}}^{\phi}=\mathbf{V}\bm{\Lambda}\mathbf{V}^{\top}\) where 1. \(\mathbf{V}=[\mathbf{v}_{1},\cdots,\mathbf{v}_{m}]\in\mathbb{R}^{m\times m}\) consists of the orthonormal eigenvectors, and 2. \(\bm{\Lambda}=\operatorname{diag}\left(\lambda_{1},\cdots,\lambda_{m}\right)\) contains descending eigenvalues \(\lambda_{1}\geq\cdots\geq\lambda_{m}\geq 0\).
5:Initialize \(\mathbf{s}=[s_{1},\cdots,s_{N}]\) with \(s_{i}=\frac{1}{n}\) on \(n\) uniformly sampled \(i\)'s and \(s_{i}=0\) elsewhere.
6:Let \(\operatorname{diag}(\mathbf{s})\in\mathbb{R}^{N\times N}\) be a diagonal matrix with \(\mathbf{s}\) on diagonal. Optimizing: \[\begin{split}&\min_{\mathbf{s}\in\Delta_{N}}\;\min_{\bm{ \gamma}=[\gamma_{1},\cdots,\gamma_{m}]\in\mathbb{R}^{m}}\;\sum_{j=1}^{m} \left(\mathbf{v}_{j}^{\top}\widetilde{\mathbf{G}}^{\top}\operatorname{diag} \left(\mathbf{s}\right)\widetilde{\mathbf{G}}\mathbf{v}_{j}-\gamma_{j}\cdot \lambda_{j}\right)^{2}\\ &\text{s.t.}\quad 0\leq s_{i}\leq 1/n\;\forall\;i\in[N], \quad\gamma_{j}\geq 1/c_{S}\;\forall\;j\in[m].\end{split}\] (5)
7:Output:\(S\subset[N]\) by sampling \(n\) data from \(\mathbf{s}\in\Delta_{N}\) without replacement. ```

**Algorithm 3.1** Sketchy Moment Matching (SkMM)

While directly minimizing \(\operatorname{tr}(\widetilde{\bm{\Sigma}}^{\phi}(\widetilde{\bm{\Sigma}}^{ \phi}_{S})^{\dagger})\) involves integer programming and pseudoinverse, causing hard and numerically unstable optimization, \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) has a straightforward relaxation (vide Remark 3.2), leading to the simple and stable moment matching objective (5) in Algorithm 3.1.

**Remark 3.2** (Relaxing \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) to (5)).: _Given the spectral decomposition \(\widetilde{\bm{\Sigma}}^{\phi}=\mathbf{V}\bm{\Lambda}\mathbf{V}^{\top}\), \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) can be rewritten as \(\mathbf{V}^{\top}(\frac{1}{n}\widetilde{\mathbf{G}}^{\top}_{S}\widetilde{\mathbf{ G}}_{S})\mathbf{V}\succcurlyeq\frac{1}{c_{S}}\bm{\Lambda}\), and (5) is a relaxation: (i) instead of enforcing \(\widetilde{\bm{\Sigma}}^{\phi}\preccurlyeq c_{S}\widetilde{\bm{\Sigma}}^{\phi}_ {S}\) strictly, constraints are only imposed on the diagonal11: \(\mathbf{v}_{j}^{\top}(\frac{1}{n}\widetilde{\mathbf{G}}^{\top}_{S}\widetilde{ \mathbf{G}}_{S})\mathbf{v}_{j}\geq\lambda_{j}/c_{S}\), \(j\in[m]\); and (ii) the selection of \(S\) is relaxed to a weight vector \(\mathbf{s}\in\Delta_{N}\) with linear constraints \(0\leq s_{i}\leq 1/n\). Free of integer constraints and pseudoinverse, the quadratic data selection objective with linear constraints in (5) can be solved efficiently and stably via projected gradient descent._

Footnote 11: This is equivalent to assuming that \(\widetilde{\bm{\Sigma}}^{\phi}\), \(\widetilde{\bm{\Sigma}}^{\phi}_{S}\) commute. While such assumption does not hold in general, it is a valuable heuristic whose effectiveness has been demonstrated in various domains [36, 37].

Alternative to the moment matching heuristic in Remark 3.2, variance reduction by controlling \(\widetilde{\bm{\Sigma}}^{\phi}(\widetilde{\bm{\Sigma}}^{\phi}_{S})^{\dagger}\) in the low-dimensional subspace can be realized via various methods, including leverage score sampling [18, 19, 72, 73, 74] and V-optimal experimental design [28, 29]. We provide brief discussions on these alternatives in Appendix A.2.

**Remark 3.3** (\(c_{S}\) controls strength of moment matching).: _In Algorithm 3.1, smaller \(c_{S}\) enforces \(\widetilde{\bm{\Sigma}}^{\phi}_{S}\) to exploit more information in \(\widetilde{\bm{\Sigma}}^{\phi}\), bringing lower variance and better generalization. While the lower bound \(c_{S}\geq\frac{n}{N}\) could be tight (vide Remark B.1), in practice, the smallest feasible \(c_{S}\) depends on the data distribution and tends to be larger (e.g., \(c_{S}\approx 1\) in the experiments)._

**Remark 3.4** (Computational efficiency of SkMM).: _SkMM is efficient in both memory and computation. Consider the two stages in Algorithm 3.1: (i) Gradient sketching can be computed in parallel with input-sparsity time and on the fly without storing the (potentially) high-dimensional gradients (vide Remark C.1). (ii) After gradient sketching, variance reduction via moment matching happens in the low dimension \(m\), with a low memory footprint \(O(Nm)\), taking \(O(m^{3})\) for the spectral decomposition and \(O(Nm)\) per iteration for optimizing the moment matching objective (5)._

## 4 Experiments

### Synthetic High-dimensional Linear Probing

To ground the theoretical insight on variance-bias tradeoff in high-dimensional finetuning, we simulate linear probing with a synthetic underdetermined ridge regression problem12.

Footnote 12: Our experiment code is available at https://github.com/Xiang-Pan/sketchy_moment_matching

Setup.We consider a set of \(N=2000\) samples with high-dimensional pre-trained representations \(\phi(\mathbf{X})\in\mathbb{R}^{N\times r}\), \(r=2400\), modeled by a Gaussian mixture model (GMM) consisting of \(\overline{r}=8\) well-separated clusters, each with random sizes and variances (vide Figure 2). Samples within each cluster share the same randomly generated label. We solve the ridge regression problem (2) over the selected coreset of \(n\) samples with hyperparameter \(\alpha\) tuning. The empirical risk is evaluated over the full dataset \(\mathcal{L}_{\mathcal{D}}(\bm{\theta}_{S})=\frac{1}{N}\left\|\phi(\mathbf{X}) \bm{\theta}_{S}-\mathbf{y}\right\|_{2}^{2}\) (vide Appendix D.1 for implementation details).

Data selection.For SkMM (Algorithm 3.1), we use a sketching dimension \(m=4\overline{r}=32\) and set \(c_{S}=0.999\). We optimize (5) via Adam [75] with constraint projection under learning rate \(10^{-7}\) for \(10^{4}\) iterations and sample \(S\) from \(\mathbf{s}\in\Delta_{N}\) with the lowest objective value.

We compare SkMM to representative unsupervised data selection methods for regression, including uniform, leverage score [18; 19; 72; 73; 74], adaptive sampling [44; 56], herding [51; 52], and k-center greedy [53]. Specifically, (i) SkMM, truncated leverage score (T-leverage), and ridge leverage score sampling (R-leverage) can be viewed as different ways of variance-bias balancing; (ii) adaptive sampling (Adaptive) and k-center greedy (K-Center) focus on bias reduction (_i.e._, providing good low-rank approximation/clustering for \(\phi(\mathbf{X})\)); while (iii) Herding and uniform sampling (Uniform) reduce variance (vide Appendix D.2 for baseline details).

We observe from Figure 2 and Table 1 that balancing the variance-bias tradeoff is crucial for the generalization of data selection in high dimensions. In particular, SkMM achieves the best empirical

Figure 2: Selecting \(n=80\) data (colored in red) from the GMM dataset. Intuitively, a coreset \(\mathcal{D}_{S}\) with low bias contains at least one sample per cluster; whereas a low-variance \(\mathcal{D}_{S}\) selects more data from clusters with larger variance. We recall from Theorem 2.2 that the variance-bias balance is essential for good generalization.

risk across different coreset sizes \(n\), especially when \(n\) is small. While as \(n/N\to 1\), uniform sampling provides a strong baseline, coinciding with common empirical observations [1].

### Experiments on Regression Tasks

We further validate the effectiveness of SkMM on UTKFace [76], a real-world regression dataset for age estimation. We finetune a randomly initialized classification head on top of the feature representation of CLIP [50] with Adam [75] and learning rate \(10^{-1}\). We also retain those baselines from the above synthetic setup in this experiment.

The results for linear probing are provided in Table 2, where our method remarkably outperforms comparative baselines on UTKFace. For every coreset size, SkMM improves the performance of CLIP compared to uniform sampling. Especially for small coreset size \(n=100,200\), it achieves a Mean Absolute Error reduction of approximately \(50\%\).

### Experiments on Image Classification Tasks

While our analysis focuses on data selection for finetuning regression models, a natural question is whether the idea of SkMM applies to broader scopes. To answer this, we extend our empirical investigation to classification. In particular, we consider an imbalanced classification task: Stanford-Cars [77] with 196 classes, 8144 training samples, and 8041 testing samples where the classes are highly imbalanced with training sample sizes ranging from 24 to 68.

Finetuning.We consider two common ways of finetuning: (i) linear probing (LP) over the last layer and (ii) finetuning (FT) over the last few layers, covering both the low- (_i.e._, \(n\geq r\) for LP) and high-dimensional (_i.e._, \(r>n\) for FT) settings. For LP, we learn the last layer over the embeddings from a CLIP-pretrained ViT-B/32 [50] with a learning rate of \(10^{-1}\). For FT13, we finetuning the last two layers of an ImageNet-pretrained ResNet18 [84] with a learning rate of \(10^{-2}\). In both settings, we optimize via Adam for 50 epochs. Due to space limit constraints, detailed results for fine-tuning are deferred to the appendix.

Footnote 13: We notice that finetuning the last few layers of strong pretrained models like CLIP can distort the features and hurt the performance, as studied in [83]. Therefore, we stay with a weaker pretrained model for finetuning.

Data selection.For SkMM-LP, the gradients (of the last layer) are given by the pretrained features from CLIP. For SkMM-FT, the gradients (of the last two layers) are calculated based on a random classification head. We tune the sketching dimension \(m\in\{32,64,128,256,512\}\) and the lower bound for slackness variables \(c_{S}\in\{0.6,0.7,0.8,0.9\}\). Within suitable ranges, smaller \(m\) and larger \(c_{S}\) lead to better performance in the low data regime. Intuitively, smaller \(m\) encourages variance reduction in a more compressed subspace, and larger \(c_{s}\) leads to easier optimization.

We compare SkMM to various unsupervised and (weakly) supervised data selection methods for classification, including uniform sampling, herding [51], Contextual Diversity [78], Glister [79], GraNd [66], Forgetting [80], DeepFool [81], as well as three uncertainty-based methods, Entropy, Margin, and Least Confidence [82].

Observations.We first observe that for both LP (Table 3) and FT (Table 4), SkMM achieves competitive finetuning accuracy on StanfordCars. Since SkMM is an unsupervised process agnostic of true class sizes, the appealing performance of SkMM on the imbalanced StanfordCars dataset echoes the ability of SkMM to handle data selection among clusters of various sizes through variance-bias balance (_cf._ synthetic experiments in Figure 2). Meanwhile, for LP in the low-dimensional setting (Table 3), uniform sampling provides a surprisingly strong baseline. This coincides with the theoretical insight from Proposition 2.1 and the empirical observations in [1].

## 5 Discussion, Limitations, and Future Directions

We investigated data selection for finetuning in both low and high dimensions from a theoretical perspective. Beyond variance reduction in low dimension, our analysis revealed the _variance-bias tradeoff in data selection for high-dimensional finetuning with low intrinsic dimension_\(\overline{r}\), balancing which led to a _fast-rate generalization_\(O(\overline{r}/n)\). For efficient control of such variance-bias tradeoff in practice, we introduced SkMM that first explores the high-dimensional parameter space via _gradient sketching_ and then exploits the resulting low-dimensional subspace via _moment matching_. Theoretically, we showed that _the low-dimensional subspace from gradient sketching preserves the fast-rate generalization_. Moreover, we ground the theoretical insight on balancing the variance-bias tradeoff via synthetic experiments, while demonstrating the effectiveness of SkMM for finetuning real vision tasks.

In this work, we focus only on moment matching via optimization inspired by the analysis for variance reduction after gradient sketching. Nevertheless, there is a remarkable variety of existing low-dimensional data selection strategies (_e.g._, via greedy selection or sampling) that could potentially be extended to high dimensions leveraging sketching as an efficient pre-processing step. In linear algebra, sketching has been widely studied for accelerating, as well as stabilizing, large-scale low-rank approximations and linear solvers. However, the intuitions and theories there may or may not be directly applicable to the statistical learning regime. In light of the high-dimensional nature of deep learning where sketching brings an effective remedy, we hope that providing a rigorous generalization analysis for sketching in data selection would make a step toward bridging the classical wisdom of sketching and the analogous challenges in modern learning problems.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \(n\) & 2000 & 2500 & 3000 & 3500 & 4000 \\ \hline \multirow{2}{*}{Uniform Sampling} & Acc & 67.63 \(\pm\) 0.17 & 70.59 \(\pm\) 0.19 & 72.49 \(\pm\) 0.19 & 74.16 \(\pm\) 0.22 & 75.40 \(\pm\) 0.16 \\  & F1 & 64.54 \(\pm\) 0.18 & 67.79 \(\pm\) 0.23 & 70.00 \(\pm\) 0.20 & 71.77 \(\pm\) 0.23 & 73.14 \(\pm\) 0.12 \\ \hline \multirow{2}{*}{Herding [51]} & Acc & 67.22 \(\pm\) 0.16 & 71.02 \(\pm\) 0.13 & 73.17 \(\pm\) 0.22 & 74.64 \(\pm\) 0.18 & 75.71 \(\pm\) 0.29 \\  & F1 & 64.07 \(\pm\) 0.23 & 68.28 \(\pm\) 0.15 & 70.64 \(\pm\) 0.28 & 72.22 \(\pm\) 0.26 & 73.26 \(\pm\) 0.39 \\ \hline \multirow{2}{*}{Contextual Diversity [78]} & Acc & 67.64 \(\pm\) 0.13 & 70.82 \(\pm\) 0.23 & 72.66 \(\pm\) 0.12 & 74.46 \(\pm\) 0.17 & 75.77 \(\pm\) 0.12 \\  & F1 & 64.51 \(\pm\) 0.17 & 68.18 \(\pm\) 0.25 & 70.05 \(\pm\) 0.11 & 72.13 \(\pm\) 0.15 & 73.35 \(\pm\) 0.07 \\ \hline \multirow{2}{*}{Glister [79]} & Acc & 67.60 \(\pm\) 0.24 & 70.85 \(\pm\) 0.27 & 73.07 \(\pm\) 0.26 & 74.63 \(\pm\) 0.21 & 76.00 \(\pm\) 0.20 \\  & F1 & 64.50 \(\pm\) 0.34 & 68.07 \(\pm\) 0.38 & 70.47 \(\pm\) 0.35 & 72.18 \(\pm\) 0.25 & 73.69 \(\pm\) 0.24 \\ \hline \multirow{2}{*}{GraNd [66]} & Acc & 67.27 \(\pm\) 0.07 & 70.38 \(\pm\) 0.07 & 72.56 \(\pm\) 0.05 & 74.67 \(\pm\) 0.06 & 75.77 \(\pm\) 0.12 \\  & F1 & 64.04 \(\pm\) 0.09 & 67.48 \(\pm\) 0.09 & 69.81 \(\pm\) 0.08 & 73.44 \(\pm\) 0.13 \\ \hline \multirow{2}{*}{Forgetting [80]} & Acc & 67.59 \(\pm\) 0.10 & 70.99 \(\pm\) 0.05 & 72.54 \(\pm\) 0.07 & 74.81 \(\pm\) 0.05 & 75.74 \(\pm\) 0.01 \\  & F1 & 64.85 \(\pm\) 0.13 & 68.53 \(\pm\) 0.07 & 70.30 \(\pm\) 0.05 & 72.59 \(\pm\) 0.04 & 73.74 \(\pm\) 0.02 \\ \hline \multirow{2}{*}{DeepFool [81]} & Acc & 67.77 \(\pm\) 0.29 & 70.73 \(\pm\) 0.22 & 73.24 \(\pm\) 0.22 & 74.57 \(\pm\) 0.23 & 75.71 \(\pm\) 0.15 \\  & F1 & 64.16 \(\pm\) 0.68 & 68.49 \(\pm\) 0.53 & 70.93 \(\pm\) 0.32 & 72.44 \(\pm\) 0.27 & 73.79 \(\pm\) 0.15 \\ \hline \multirow{2}{*}{Entropy [82]} & Acc & 67.95 \(\pm\) 0.11 & 71.00 \(\pm\) 0.10 & 73.28 \(\pm\) 0.10 & 75.02 \(\pm\) 0.08 & 75.82 \(\pm\) 0.06 \\  & F1 & 64.55 \(\pm\) 0.10 & 67.95 \(\pm\) 0.12 & 70.68 \(\pm\) 0.12 & 72.46 \(\pm\) 0.12 & 73.29 \(\pm\) 0.04 \\ \hline \multirow{2}{*}{Margin [82]} & Acc & 67.53 \(\pm\) 0.14 & 71.19 \(\pm\) 0.09 & 73.09 \(\pm\) 0.14 & 74.66 \(\pm\) 0.11 & 75.57 \(\pm\) 0.13 \\  & F1 & 64.16 \(\pm\) 0.15 & 68.33 \(\pm\) 0.14 & 70.37 \(\pm\) 0.17 & 72.03 \(\pm\) 0.11 & 73.14 \(\pm\) 0.20 \\ \hline \multirow{2}{*}{Least Confidence [82]} & Acc & 67.68 \(\pm\) 0.11 & 70.99 \(\pm\) 0.14 & 73.04 \(\pm\) 0.05 & 74.65 \(\pm\) 0.09 & 75.58 \(\pm\) 0.08 \\  & F1 & 64.09 \(\pm\) 0.20 & 68.03 \(\pm\) 0.20 & 70.30 \(\pm\) 0.07 & 72.02 \(\pm\) 0.10 & 73.15 \(\pm\) 0.12 \\ \hline \multirow{2}{*}{SkMM-LP} & Acc & **68.27 \(\pm\) 0.03** & **71.53 \(\pm\) 0.05** & **73.61 \(\pm\) 0.02** & **75.12 \(\pm\) 0.01** & **76.34 \(\pm\) 0.02** \\  & F1 & **65.29 \(\pm\) 0.03** & **68.75 \(\pm\) 0.06** & **71.14 \(\pm\) 0.03** & **72.64 \(\pm\) 0.02** & **74.02 \(\pm\) 0.10** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy and F1 score (%) of LP over CLIP on StanfordCars

## Acknowledgments

The authors wish to thank Yunzhen Feng, Julia Kempe, and Christopher Musco for insightful discussions. QL was partially supported by the NYU Research Catalyst Prize and the Department of Energy under ASCR Award DE-SC0024721. YD was supported by the NYU Courant Instructorship.

## References

* [1] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning. In _International Conference on Database and Expert Systems Applications_, pages 181-195. Springer, 2022.
* [2] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. _arXiv preprint arXiv:2402.16827_, 2024.
* [3] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. _Advances in Neural Information Processing Systems_, 35:19523-19536, 2022.
* [4] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. Dataset pruning: Reducing training data by examining generalization influence. _arXiv preprint arXiv:2205.09329_, 2022.
* [5] Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. Coverage-centric coreset selection for high pruning rates. _arXiv preprint arXiv:2210.15809_, 2022.
* [6] Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. Moderate coreset: A universal method of data selection for real-world data-efficient deep learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [7] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In _International Conference on Machine Learning_, pages 6950-6960. PMLR, 2020.
* [8] Zalan Boros, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. _Advances in neural information processing systems_, 33:14879-14890, 2020.
* [9] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient and robust semi-supervised learning. _Advances in neural information processing systems_, 34:14488-14501, 2021.
* [10] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [11] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* [12] Yiping Wang, Yifang Chen, Wendan Yan, Kevin Jamieson, and Simon Shaolei Du. Variance alignment score: A simple but tough-to-beat data selection method for multimodal contrastive learning. _arXiv preprint arXiv:2402.02055_, 2024.
* [13] Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive language-image pretraining: Prioritizing data quality over quantity. _arXiv preprint arXiv:2403.12267_, 2024.
* [14] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.

* [15] Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering-data curation cannot be compute agnostic. _arXiv preprint arXiv:2404.07177_, 2024.
* [16] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. _arXiv preprint arXiv:2402.04333_, 2024.
* [17] David P Woodruff et al. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* [18] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In _2006 47th annual IEEE symposium on foundations of computer science (FOCS'06)_, pages 143-152. IEEE, 2006.
* [19] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees. _Advances in neural information processing systems_, 28, 2015.
* [20] Garvesh Raskutti and Michael W Mahoney. A statistical perspective on randomized sketching for ordinary least-squares. _Journal of Machine Learning Research_, 17(213):1-31, 2016.
* [21] Xue Chen and Eric Price. Active regression via linear-sample sparsification. In _Conference on Learning Theory_, pages 663-695. PMLR, 2019.
* [22] Brett W Larsen and Tamara G Kolda. Sketching matrix least squares via leverage scores estimates. _arXiv preprint arXiv:2201.10638_, 2022.
* [23] Michal Derezinski, Manfred K Warmuth, and Daniel Hsu. Unbiased estimators for random design regression. _Journal of Machine Learning Research_, 23(167):1-46, 2022.
* [24] Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, and Jonathan Weare. Improved active learning via dependent leverage score sampling. _arXiv preprint arXiv:2310.04966_, 2023.
* [25] Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. _Statistical science_, pages 273-304, 1995.
* [26] Friedrich Pukelsheim. _Optimal design of experiments_. SIAM, 2006.
* [27] Valerii Vadimovich Fedorov. _Theory of optimal experiments_. Elsevier, 2013.
* [28] Yining Wang, Adams Wei Yu, and Aarti Singh. On computationally tractable selection of experiments in measurement-constrained regression models. _Journal of Machine Learning Research_, 18(143):1-41, 2017.
* [29] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. _arXiv preprint arXiv:1711.05174_, 2017.
* [30] Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? _SIAM Journal on Mathematics of Data Science_, 1(1):144-160, 2019.
* [31] William B Johnson. Extensions of lipshitz mapping into hilbert space. In _Conference modern analysis and probability, 1984_, pages 189-206, 1984.
* [32] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. _SIAM review_, 53(2):217-288, 2011.
* [33] Per-Gunnar Martinsson and Joel A Tropp. Randomized numerical linear algebra: Foundations and algorithms. _Acta Numerica_, 29:403-572, 2020.
* [34] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. _arXiv preprint arXiv:2303.14186_, 2023.

* [35] Michal Cerny and Milan Hladik. Two complexity results on c-optimality in experimental design. _Computational Optimization and Applications_, 51(3):1397-1408, 2012.
* [36] Helge Blaker. Minimax estimation in linear regression under restrictions. _Journal of statistical planning and inference_, 90(1):35-55, 2000.
* [37] Qi Lei, Wei Hu, and Jason Lee. Near-optimal linear regression under distribution shift. In _International Conference on Machine Learning_, pages 6164-6174. PMLR, 2021.
* [38] Michael W Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis. _Proceedings of the National Academy of Sciences_, 106(3):697-702, 2009.
* [39] Amit Deshpande, Luis Rademacher, Santosh S Vempala, and Grant Wang. Matrix approximation and projective clustering via volume sampling. _Theory of Computing_, 2(1):225-247, 2006.
* [40] Sergey Voronin and Per-Gunnar Martinsson. Efficient algorithms for cur and interpolative matrix decompositions. _Advances in Computational Mathematics_, 43:495-516, 2017.
* [41] Michal Derezinski and Michael W Mahoney. Determinantal point processes in randomized numerical linear algebra. _Notices of the American Mathematical Society_, 68(1):34-45, 2021.
* [42] Michal Derezinski, Rajiv Khanna, and Michael W Mahoney. Improved guarantees and a multiple-descent curve for column subset selection and the nystrom method. _Advances in Neural Information Processing Systems_, 33:4953-4964, 2020.
* [43] Yijun Dong and Per-Gunnar Martinsson. Simpler is better: a comparative study of randomized pivoting algorithms for cur and interpolative decompositions. _Advances in Computational Mathematics_, 49(4):66, 2023.
* [44] Yifan Chen, Ethan N Epperly, Joel A Tropp, and Robert J Webber. Randomly pivoted cholesky: Practical approximation of a kernel matrix with few entry evaluations. _arXiv preprint arXiv:2207.06503_, 2022.
* [45] Yijun Dong, Chao Chen, Per-Gunnar Martinsson, and Katherine Pearce. Robust blockwise random pivoting: Fast and accurate adaptive interpolative decomposition. _arXiv preprint arXiv:2309.16002_, 2023.
* [46] Katherine J Pearce, Chao Chen, Yijun Dong, and Per-Gunnar Martinsson. Adaptive parallelizable algorithms for interpolative decompositions via partially pivoted lu. _arXiv preprint arXiv:2310.09417_, 2023.
* [47] Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. In _Proceedings of the fortieth annual ACM symposium on Theory of computing_, pages 563-568, 2008.
* [48] Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In _Proceedings of the 23rd international conference on Machine learning_, pages 1081-1088, 2006.
* [49] Neta Shoham and Haim Avron. Experimental design for overparameterized learning with application to single shot deep active learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(10):11766-11777, 2023.
* [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [51] Max Welling. Herding dynamical weights to learn. In _Proceedings of the 26th annual international conference on machine learning_, pages 1121-1128, 2009.
* [52] Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. _arXiv preprint arXiv:1203.3472_, 2012.
* [53] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. _arXiv preprint arXiv:1708.00489_, 2017.

* Chatterjee and Hadi [1986] Samprit Chatterjee and Ali S Hadi. Influential observations, high leverage points, and outliers in linear regression. _Statistical science_, pages 379-393, 1986.
* Drineas et al. [2011] Petros Drineas, Michael W Mahoney, Shan Muthukrishnan, and Tamas Sarlos. Faster least squares approximation. _Numerische mathematik_, 117(2):219-249, 2011.
* Deshpande and Vempala [2006] Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix approximation. In _International Workshop on Approximation Algorithms for Combinatorial Optimization_, pages 292-303. Springer, 2006.
* Kolossov et al. [2023] Germain Kolossov, Andrea Montanari, and Pulkit Tandon. Towards a statistical theory of data selection under weak supervision. _arXiv preprint arXiv:2309.14563_, 2023.
* Lindley [1956] Dennis V Lindley. On a measure of the information provided by an experiment. _The Annals of Mathematical Statistics_, 27(4):986-1005, 1956.
* Seung et al. [1992] H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In _Proceedings of the fifth annual workshop on Computational learning theory_, pages 287-294, 1992.
* Lewis [1995] David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In _Acm Sigir Forum_, volume 29, pages 13-19. ACM New York, NY, USA, 1995.
* Ting and Brochu [2018] Daniel Ting and Eric Brochu. Optimal subsampling with influence functions. _Advances in neural information processing systems_, 31, 2018.
* Wang et al. [2018] HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. _Journal of the American Statistical Association_, 113(522):829-844, 2018.
* Munteanu et al. [2018] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for logistic regression. _Advances in Neural Information Processing Systems_, 31, 2018.
* Mai et al. [2021] Tung Mai, Cameron Musco, and Anup Rao. Coresets for classification-simplified and strengthened. _Advances in Neural Information Processing Systems_, 34:11643-11654, 2021.
* Axiotis et al. [2024] Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, and Michael Wunder. Data-efficient learning via clustering-based sensitivity sampling: Foundation models and beyond. _arXiv preprint arXiv:2402.17327_, 2024.
* Paul et al. [2021] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. _Advances in Neural Information Processing Systems_, 34:20596-20607, 2021.
* Jiang et al. [2019] Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating deep learning by focusing on the biggest losers. _arXiv preprint arXiv:1910.00762_, 2019.
* Vodrahalli et al. [2018] Kailas Vodrahalli, Ke Li, and Jitendra Malik. Are all training examples created equal? an empirical study. _arXiv preprint arXiv:1811.12569_, 2018.
* Ash et al. [2021] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In _International Conference on Learning Representations_.
* Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Drineas et al. [2008] Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Relative-error cur matrix decompositions. _SIAM Journal on Matrix Analysis and Applications_, 30(2):844-881, 2008.

* [73] Mu Li, Gary L Miller, and Richard Peng. Iterative row sampling. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 127-136. IEEE, 2013.
* [74] Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1758-1777. SIAM, 2017.
* [75] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [76] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5810-5818, 2017.
* [77] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained cars. 2013.
* [78] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16_, pages 137-153. Springer, 2020.
* [79] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8110-8118, 2021.
* [80] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. _arXiv preprint arXiv:1812.05159_, 2018.
* [81] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2574-2582, 2016.
* [82] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. _arXiv preprint arXiv:1906.11829_, 2019.
* [83] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. _arXiv preprint arXiv:2202.10054_, 2022.
* [84] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [85] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [86] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [87] Daniel M Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. _Journal of the ACM (JACM)_, 61(1):1-23, 2014.
* [88] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In _Proceedings of the thirtieth annual ACM symposium on Theory of computing_, pages 604-613, 1998.
* [89] Jelani Nelson and Huy L Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In _2013 ieee 54th annual symposium on foundations of computer science_, pages 117-126. IEEE, 2013.

* [90] Franco Woolfe, Edo Liberty, Vladimir Rokhlin, and Mark Tygert. A fast randomized algorithm for the approximation of matrices. _Applied and Computational Harmonic Analysis_, 25(3):335-366, 2008.
* [91] Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. _Advances in Adaptive Data Analysis_, 3(01n02):115-126, 2011.
* [92] Kazushige Goto and Robert Van De Geijn. High-performance implementation of the level-3 blas. _ACM Transactions on Mathematical Software (TOMS)_, 35(1):1-14, 2008.
* [93] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In _Proceedings of the forty-fifth annual ACM symposium on Theory of computing_, pages 91-100, 2013.
* [94] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In _International Colloquium on Automata, Languages, and Programming_, pages 693-703. Springer, 2002.
* [95] Michael B Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In _Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms_, pages 278-287. SIAM, 2016.
* [96] Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Fixed-rank approximation of a positive-semidefinite matrix from streaming data. _Advances in Neural Information Processing Systems_, 30, 2017.
* [97] Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 62(12):1707-1739, 2009.
* [98] Daniel B Szyld. The many proofs of an identity on the norm of oblique projections. _Numerical Algorithms_, 42:309-323, 2006.

Additional Discussions

### Additional Notations

Given any matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), along with indices \(i\in[n]\), \(j\in[d]\), \(I\subseteq[n]\), and \(J\subseteq[d]\), let \(\left[\mathbf{A}\right]_{i,j}\) be the \((i,j)\)-th entry of \(\mathbf{A}\), \(\left[\mathbf{A}\right]_{i}\) be the \(i\)-th row (or the \(i\)-th entry if \(\mathbf{A}\in\mathbb{R}^{n}\) is a vector), and \(\left[\mathbf{A}\right]_{:,j}\) be the \(j\)-th column; \(\mathbf{A}_{I}=\left[\mathbf{A}\right]_{I,:}\) consists of rows in \(\mathbf{A}\) indexed by \(I\); and let \(\mathbf{A}_{I,J}=\left[\mathbf{A}\right]_{I,J}\) be the submatrix of \(\mathbf{A}\) with rows indexed by \(I\) and columns indexed by \(J\).

### Alternatives to Moment Matching Heuristic in Remark 3.2

In addition to the moment matching heuristic in Remark 3.2, variance in the resulting low-dimensional subspace from gradient sketching can be controlled by \(\widetilde{\mathbf{\Sigma}}^{\phi}(\widetilde{\mathbf{\Sigma}}_{S}^{\phi})^{\dagger}\) via alternative methods like leverage score sampling and V-optimal experimental design.

**Remark A.1** (Leverage score sampling).: _Leverage score sampling [18, 19, 72, 73, 74] provides arguably one of the most intuitive ways for selecting data based on \(\widetilde{\mathbf{G}}\in\mathbb{R}^{N\times m}\). In particular, [17, Theorem 17] implies that for a coreset of size at least \(n=\Omega(m\log(m/\delta)\epsilon^{-2})\) drawn i.i.d. with replacement via leverage score sampling over \(\widetilde{\mathbf{G}}\), \(c_{S}\leq(1+\epsilon)\frac{m}{\pi_{S}N}\) with probability at least \(1-\delta\), where \(\tau_{S}\in[0,1]\) is the minimum leverage score of \(\widetilde{\mathbf{G}}\) over the coreset \(S\).14 Such dependence on \(\tau_{S}\) can render the upper bound of \(c_{S}\) vacuous when \(\tau_{S}\to 0\)._

Footnote 14: Notice that \(\tau_{S}\) appears because samples in \(S\) are equally weighted in the data selection setting, in contrast to the standard leverage score sampling where samples are weighted by the respective sampling probabilities.

_Nevertheless, when \(\tau_{S}\) is reasonably large, leverage score sampling based on \(\widetilde{\mathbf{G}}\) can be computed more efficiently than SkMM in \(O(Nm^{2})\) time and can provide good control over \(c_{S}\). While both SkMM and leverage score sampling can facilitate variance reduction in the low-dimensional subspace, SkMM provides better empirical performance (cf. Section 4.1) at a slightly higher cost in the low intrinsic dimension \(m\) (vide Remark 3.4) as it is tailored for optimizing moment matching._

**Remark A.2** (V-optimal experimental design).: _Variance in the low-dimensional subspace can also be controlled by applying the V-optimal experimental design methods [28, 29] on \(\widetilde{\mathbf{G}}\in\mathbb{R}^{N\times m}\). For example, [29] provides a polynomial-time algorithm to find a \((1+\epsilon)\)-estimation of the V-optimal design for \(\widetilde{\mathbf{G}}\) with a coreset of size at least \(n=\Omega(m\epsilon^{-2})\); and \(c_{S}\) is effectively controlled by the V-optimality criterion \(\operatorname{tr}(\widetilde{\mathbf{\Sigma}}^{\phi}(\widetilde{\mathbf{ \Sigma}}_{S}^{\phi})^{\dagger})\)._

_While such V-optimal design methods can provide good control over \(c_{S}\) with nearly optimal sample complexity, they are computationally more expensive than SkMM (or leverage score sampling) and tend to suffer from numerical instability issues in practice. For example, the algorithm in [29] consists of two stages: (i) solving a continuous relaxation of the original discrete optimization problem posed by V-optimality, and (ii) rounding the continuous solution via regret minimization. While the cost of rounding is negligible, solving the continuous relaxation of V-optimality (in contrast to leveraging fast and stable heuristics like the one in SkMM, cf. Remark 3.2) is challenging, both in terms of computational complexity and numerical stability._

## Appendix B Proofs for Section 2.1

### Proofs of (1)

Proof of (1) and beyond.: Under the assumption \(\operatorname{rank}\left(\phi\left(\mathbf{X}_{S}\right)\right)=r\), both \(\phi\left(\mathbf{X}_{S}\right),\phi\left(\mathbf{X}\right)\) have full column rank. Therefore \(\phi\left(\mathbf{X}_{S}\right)^{\dagger}\phi\left(\mathbf{X}_{S}\right)= \phi\left(\mathbf{X}\right)^{\dagger}\phi\left(\mathbf{X}\right)=\mathbf{I}_ {r}\), and \(\boldsymbol{\theta}_{S}=\phi\left(\mathbf{X}_{S}\right)^{\dagger}\mathbf{y}_{S}\). Then, since \(\mathbf{y}=\phi\left(\mathbf{X}\right)\boldsymbol{\theta}_{*}+\mathbf{z}\) and \(\mathbf{y}_{S}=\phi\left(\mathbf{X}_{S}\right)\boldsymbol{\theta}_{*}+ \mathbf{z}_{S}\), we have

\[\boldsymbol{\theta}_{S}-\boldsymbol{\theta}_{*}=\phi\left(\mathbf{X}_{S} \right)^{\dagger}\mathbf{y}_{S}-\boldsymbol{\theta}_{*}=\left(\phi\left( \mathbf{X}_{S}\right)^{\dagger}\phi\left(\mathbf{X}_{S}\right)\boldsymbol{ \theta}_{*}-\boldsymbol{\theta}_{*}\right)+\phi\left(\mathbf{X}_{S}\right)^{ \dagger}\mathbf{z}_{S}=\phi\left(\mathbf{X}_{S}\right)^{\dagger}\mathbf{z}_{S},\]which leads to

\[\mathbb{E}\left[\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right) \right]=\mathbb{E}\left[\frac{1}{N}\left\|\phi\left(\mathbf{X}\right)\left( \boldsymbol{\theta}_{S}-\boldsymbol{\theta}_{*}\right)\right\|_{2}^{2}\right]\] \[= \operatorname{tr}\left(\left(\frac{1}{N}\phi\left(\mathbf{X} \right)^{\top}\phi\left(\mathbf{X}\right)\right)\phi\left(\mathbf{X}_{S} \right)^{\dagger}\mathbb{E}\left[\mathbf{z}_{S}\mathbf{z}_{S}^{\top}\right] \left(\phi\left(\mathbf{X}_{S}\right)^{\dagger}\right)^{\top}\right)\] \[= \sigma^{2}\operatorname{tr}\left(\left(\frac{1}{N}\phi\left( \mathbf{X}\right)^{\top}\phi\left(\mathbf{X}\right)\right)\left(\phi\left( \mathbf{X}_{S}\right)^{\top}\phi\left(\mathbf{X}_{S}\right)\right)^{-1}\right)\] \[= \frac{\sigma^{2}}{n}\operatorname{tr}\left(\boldsymbol{\Sigma}^{ \phi}\left(\boldsymbol{\Sigma}_{S}^{\phi}\right)^{-1}\right).\]

Now we explain the necessity of assuming \(c_{S}\geq n/N\) for \(\boldsymbol{\Sigma}^{\phi}\preccurlyeq c_{S}\boldsymbol{\Sigma}_{S}^{\phi}\):

Remark B.1 (Lower bound of \(c_{S}\)).: _Since \(\phi\left(\mathbf{X}\right)^{\top}\phi\left(\mathbf{X}\right)\succcurlyeq\phi \left(\mathbf{X}_{S}\right)^{\top}\phi\left(\mathbf{X}_{S}\right)\), we observe that \(N\boldsymbol{\Sigma}^{\phi}\succcurlyeq n\boldsymbol{\Sigma}_{S}^{\phi}\), which implies \(\boldsymbol{\Sigma}^{\phi}\succcurlyeq\frac{n}{N}\boldsymbol{\Sigma}_{S}^{\phi}\). Therefore, \(\boldsymbol{\Sigma}^{\phi}\preccurlyeq c_{S}\boldsymbol{\Sigma}_{S}^{\phi}\) is only possible when \(c_{S}\geq n/N\). Notice that this lower bound of \(c_{S}\) is tight, _when \(\widetilde{\mathbf{G}}\) consists of \(N-n\) rows of zeros._

Low-dimensional linear probing with moment matching.Recall from (1) that \(\mathbb{E}\left[\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right)\right]= \frac{\sigma^{2}}{n}\operatorname{tr}\left(\boldsymbol{\Sigma}^{\phi}\left( \boldsymbol{\Sigma}_{S}^{\phi}\right)^{-1}\right)\). Further assuming a suitable selection of \(\mathcal{D}_{S}\) with \(\boldsymbol{\Sigma}^{\phi}\preccurlyeq c_{S}\boldsymbol{\Sigma}_{S}^{\phi}\), we have

\[\operatorname{tr}\left(\boldsymbol{\Sigma}^{\phi}\left(\boldsymbol{\Sigma}_{ S}^{\phi}\right)^{-1}\right)\leq c_{S}\operatorname{tr}\left(\mathbf{I}_{r} \right)=c_{S}r\]

and therefore, \(\mathbb{E}\left[\operatorname{ER}\left(\boldsymbol{\theta}_{S}\right)\right] \leq c_{S}\frac{\sigma^{2}r}{n}\). 

### Proof of Proposition 2.1

Proof of Proposition 2.1.: Let \(\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}:=\left(\boldsymbol{\Sigma}^{\phi} \right)^{-1/2}\boldsymbol{\Sigma}_{S}^{\phi}\left(\boldsymbol{\Sigma}^{\phi} \right)^{-1/2}\). The goal of \(\boldsymbol{\Sigma}^{\phi}\preccurlyeq c_{S}\boldsymbol{\Sigma}_{S}^{\phi}\) can be re-expressed as \(c_{S}\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}\succcurlyeq\mathbf{I}_{r}\), or equivalently when \(c_{S}>1\), \(\left\|\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right\|_{2}\leq 1- \frac{1}{c_{S}}\). With uniform sampling, since

\[\mathbb{E}_{S}\left[\boldsymbol{\Sigma}_{S}^{\phi}\right]=\mathbb{E}_{S} \left[\frac{1}{n}\sum_{\mathbf{x}\in S}\phi\left(\mathbf{X}\right)\phi\left( \mathbf{X}\right)^{\top}\right]=\mathbb{E}_{\mathbf{x}}\left[\phi\left( \mathbf{X}\right)\phi\left(\mathbf{X}\right)^{\top}\right]=\boldsymbol{ \Sigma}^{\phi},\]

we have \(\mathbb{E}_{S}\left[\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}\right]=\mathbf{I} _{r}\). For any fixed unit vector \(\mathbf{z}\in\mathbb{S}^{r-1}\), let \(Z_{i}:=\mathbf{z}^{\top}\left(\boldsymbol{\Sigma}^{\phi}\right)^{-1/2}\phi \left(\mathbf{x}_{i}\right)\) be random variables with randomness on \(i\in[N]\). Since \(\left\|\phi(\mathbf{x})\right\|_{2}\leq B_{\phi}\ \forall\ \mathbf{x}\in\mathcal{D}\) and \(\boldsymbol{\Sigma}^{\phi}\succcurlyeq\gamma\mathbf{I}_{r}\), we observe that

\[\left|Z_{i}\right|\leq\left\|\left(\boldsymbol{\Sigma}^{\phi}\right)^{-1/2} \phi\left(\mathbf{x}_{i}\right)\right\|_{2}\leq\frac{B_{\phi}}{\sqrt{\gamma}} \quad\forall\ i\in[N]\]

is bounded. Therefore, \(Z_{i}\) is \(\left(\frac{B_{\phi}^{2}}{\gamma}\right)\)-subGaussian, and \(\left(Z_{i}^{2}-\mathbb{E}\left[Z_{i}^{2}\right]\right)=\mathbf{z}^{\top} \left(\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right)\mathbf{z}\) is \(\left(16\frac{B_{\phi}^{2}}{\gamma}\right)\)-subexponential. Then, by Bernstein's inequality [85, Theorem 2.8.2][86, Section 2.1.3], for any \(0<\epsilon_{1}\leq 16B_{\phi}^{2}/\gamma\),

\[\mathbf{P}\left[\mathbf{z}^{\top}\left(\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}- \mathbf{I}_{r}\right)\mathbf{z}\geq\epsilon_{1}\right]\leq\exp\left(-\frac{n} {2}\cdot\frac{\epsilon_{1}^{2}\gamma^{2}}{16^{2}B_{\phi}^{4}}\right).\] (6)

By recalling that \(\left\|\widehat{\boldsymbol{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right\|_{2}= \max_{\mathbf{u}\in\mathbb{S}^{r-1}}\mathbf{u}^{\top}\left(\widehat{ \boldsymbol{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right)\mathbf{u}\), Equation (6) for a fixed \(\mathbf{z}\in\mathbb{S}^{r-1}\) can be extended to the entire unit sphere \(\mathbb{S}^{r-1}\) through an \(\epsilon\)-net argument as follows. Recall that for any \(\epsilon_{2}>0\), there exists an \(\epsilon_{2}\)-net \(\mathcal{U}\subset\mathbb{S}^{r-1}\) such that \(\left|\mathcal{U}\right|\leq\left(1+\frac{2}{\epsilon_{2}}\right)^{r}\). Then, by the union bound,

\[\mathbb{P}\left[\max_{\mathbf{u}\in\mathcal{U}}\mathbf{u}^{\top} \left(\widehat{\mathbf{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right)\mathbf{u}> \epsilon_{1}\right]\leq \left(1+\frac{2}{\epsilon_{2}}\right)^{r}\exp\left(-\frac{n}{2} \cdot\frac{\epsilon_{1}^{2}\gamma^{2}}{16^{2}B_{\phi}^{4}}\right)\] \[= \exp\left(r\log\left(1+\frac{2}{\epsilon_{2}}\right)-\frac{n}{2} \cdot\frac{\epsilon_{1}^{2}\gamma^{2}}{16^{2}B_{\phi}^{4}}\right).\]

That is, with probability at least \(1-\delta\), \(\max_{\mathbf{u}\in\mathcal{U}}\mathbf{u}^{\top}\left(\widehat{\mathbf{\Sigma }}_{S}^{\phi}-\mathbf{I}_{r}\right)\mathbf{u}\leq\epsilon_{1}\) when

\[n\geq\frac{512B_{\phi}^{4}}{\gamma^{2}\epsilon_{1}^{2}}\left(r\log\left(1+ \frac{2}{\epsilon_{2}}\right)+\log\left(\frac{1}{\delta}\right)\right).\]

By the construction of the \(\epsilon_{2}\)-net \(\mathcal{U}\), for all \(\mathbf{v}\in\mathbb{S}^{r-1}\), there exists \(\mathbf{u}\in\mathcal{U}\) such that \(\left\|\mathbf{u}-\mathbf{v}\right\|_{2}\leq\epsilon_{2}\). Therefore, for any \(\mathbf{v}\in\mathbb{S}^{r-1}\), we have

\[\mathbf{v}^{\top}\left(\widehat{\mathbf{\Sigma}}_{S}^{\phi}- \mathbf{I}_{r}\right)\mathbf{v}\] \[= \mathbf{u}^{\top}\left(\widehat{\mathbf{\Sigma}}_{S}^{\phi}- \mathbf{I}_{r}\right)\mathbf{u}+\left(\mathbf{v}-\mathbf{u}\right)^{\top} \left(\widehat{\mathbf{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right)\left(\mathbf{ v}-\mathbf{u}\right)+2\left(\mathbf{v}-\mathbf{u}\right)^{\top}\left(\widehat{ \mathbf{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right)\mathbf{u}\] \[\leq \epsilon_{1}+\left\|\widehat{\mathbf{\Sigma}}_{S}^{\phi}-\mathbf{ I}_{r}\right\|_{2}\left(\epsilon_{2}^{2}+2\epsilon_{2}\right),\]

which implies \(\left\|\widehat{\mathbf{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right\|_{2}\leq \frac{\epsilon_{1}}{2-(1+\epsilon_{2})^{2}}\). By taking \(\epsilon_{2}\) as a small constant (_e.g._, \(\epsilon_{2}=\sqrt{3/2}-1\)), we have \(\left\|\widehat{\mathbf{\Sigma}}_{S}^{\phi}-\mathbf{I}_{r}\right\|_{2}\leq 1- \frac{1}{c_{S}}\) when

\[n\gtrsim\frac{B_{\phi}^{4}}{\gamma^{2}}\cdot\frac{r+\log\left(1/\delta\right) }{\left(1-1/c_{S}\right)^{2}}.\]

### Proof of Theorem 2.2

Proof of Theorem 2.2.: With \(\mathbf{\Sigma}^{\phi}=\frac{1}{N}\mathbf{G}^{\top}\mathbf{G}\), we have

\[\mathbb{E}\left[\mathrm{ER}\left(\bm{\theta}_{S}\right)\right]=\mathbb{E} \left[\frac{1}{N}\left\|\mathbf{G}\left(\bm{\theta}_{S}-\bm{\theta}_{*}\right) \right\|_{2}^{2}\right]=\mathbb{E}\left[\left\|\bm{\theta}_{S}-\bm{\theta}_{* }\right\|_{\mathbf{\Sigma}^{\phi}}^{2}\right],\]

Observing that by the optimality of \(\bm{\theta}_{S}\), we have

\[\frac{2}{n}\mathbf{G}_{S}^{\top}\left(\mathbf{G}_{S}\bm{\theta}_{S}-\mathbf{ y}_{S}\right)+2\alpha\bm{\theta}_{S}=\bm{0}_{r}.\]

Recalling that \(\mathbf{\Sigma}_{S}^{\phi}:=\frac{1}{n}\mathbf{G}_{S}^{\top}\mathbf{G}_{S}\), this implies

\[\bm{\theta}_{S}\] \[= \frac{1}{n}\left(\mathbf{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r} \right)^{-1}\mathbf{G}_{S}^{\top}\left(\mathbf{G}_{S}\bm{\theta}_{*}+\mathbf{ z}_{S}\right)\] \[= \left(\mathbf{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1} \mathbf{\Sigma}_{S}^{\phi}\bm{\theta}_{*}+\frac{1}{n}\left(\mathbf{\Sigma}_{S }^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\mathbf{G}_{S}^{\top}\mathbf{z}_{S}.\]

Therefore, with \(\mathbb{E}_{\mathbf{z}}\left[\mathbf{z}\right]=\bm{0}_{N}\), \(\mathbb{E}\left[\mathrm{ER}\left(\bm{\theta}_{S}\right)\right]\) can be decomposed the bias term and variance terms as follows:

\[\mathbb{E}\left[\mathrm{ER}\left(\bm{\theta}_{S}\right)\right]= \mathbb{E}\left[\left\|\bm{\theta}_{S}-\bm{\theta}_{*}\right\|_{\mathbf{ \Sigma}^{\phi}}^{2}\right]\] \[= \mathbb{E}_{\mathbf{z}}\left[\left\|\left(\left(\mathbf{\Sigma}_{ S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\mathbf{\Sigma}_{S}^{\phi}-\mathbf{I}_{r} \right)\bm{\theta}_{*}+\frac{1}{n}\left(\mathbf{\Sigma}_{S}^{\phi}+\alpha \mathbf{I}_{r}\right)^{-1}\mathbf{G}_{S}^{\top}\mathbf{z}_{S}\right\|_{\mathbf{ \Sigma}^{\phi}}^{2}\right]\] \[= \underbrace{\left\|\left(\left(\mathbf{\Sigma}_{S}^{\phi}+\alpha \mathbf{I}_{r}\right)^{-1}\mathbf{\Sigma}_{S}^{\phi}-\mathbf{I}_{r}\right)\bm{ \theta}_{*}\right\|_{\mathbf{\Sigma}^{\phi}}^{2}}_{\text{Bias}}+\underbrace{ \mathbb{E}_{\mathbf{z}}\left[\left\|\frac{1}{n}\left(\mathbf{\Sigma}_{S}^{\phi}+ \alpha\mathbf{I}_{r}\right)^{-1}\mathbf{G}_{S}^{\top}\mathbf{z}_{S}\right\|_{ \mathbf{\Sigma}^{\phi}}^{2}\right]}_{\text{Variance}}.\]Since \(\mathbb{E}_{\bm{z}}\left[\bm{z}_{S}\bm{\Sigma}_{S}^{\top}\right]\preccurlyeq\sigma^{2} \mathbf{I}_{n}\), the variance term can be bounded as

\[\text{Variance}\leq \frac{\sigma^{2}}{n}\operatorname{tr}\left(\left(\bm{\Sigma}_{S}^ {\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\bm{\Sigma}^{\phi}\left(\bm{\Sigma}_{S}^ {\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\bm{\Sigma}_{S}^{\phi}\right)\] \[\leq \frac{\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma}^{\phi} \left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\right),\]

where the second inequality follows from the fact that \(\left\|\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\bm{\Sigma }_{S}^{\phi}\right\|_{2}\leq 1\).

Recall that \(\mathbf{P}_{\mathcal{S}}\in\mathbb{R}^{r\times r}\) is an orthogonal projector onto any subspace \(\mathcal{S}\) of \(\operatorname{Range}\left(\bm{\Sigma}_{S}^{\phi}\right)\), and \(\mathbf{P}_{\mathcal{S}}^{\perp}=\mathbf{I}_{r}-\mathbf{P}_{\mathcal{S}}\) is the orthogonal projector onto its orthogonal complement. By observing that \(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\succcurlyeq\mathbf{P}_{\mathcal{ S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}}+\alpha\mathbf{P}_{\mathcal{S}}^{\perp}\), since \(\operatorname{Range}\left(\mathbf{P}_{\mathcal{S}}\right)\perp\operatorname{ Range}\left(\mathbf{P}_{\mathcal{S}}\right)\), we have

\[\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1}\preccurlyeq \left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} \right)^{\dagger}+\frac{1}{\alpha}\mathbf{P}_{\mathcal{S}}^{\perp}.\]

Therefore,

\[\text{Variance}\leq\frac{\sigma^{2}}{n}\left(\operatorname{tr}\left(\bm{ \Sigma}^{\phi}\left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{ \mathcal{S}}\right)^{\dagger}\right)+\frac{1}{\alpha}\operatorname{tr}\left( \bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\perp}\right)\right).\]

For the bias part, we first observe that

\[\mathbf{I}_{r}-\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-1} \bm{\Sigma}_{S}^{\phi}=\alpha\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r }\right)^{-1}.\]

Therefore,

\[\text{Bias}= \left\|\left(\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r} \right)^{-1}\bm{\Sigma}_{S}^{\phi}-\mathbf{I}_{r}\right)\bm{\theta}_{*}\right\| _{\bm{\Sigma}^{\phi}}^{2}=\left\|\alpha\left(\bm{\Sigma}_{S}^{\phi}+\alpha \mathbf{I}_{r}\right)^{-1}\bm{\theta}_{*}\right\|_{\bm{\Sigma}^{\phi}}^{2}\] \[= \alpha^{2}\operatorname{tr}\left(\left(\bm{\Sigma}_{S}^{\phi}+ \alpha\mathbf{I}_{r}\right)^{-1}\bm{\Sigma}^{\phi}\left(\bm{\Sigma}_{S}^{\phi }+\alpha\mathbf{I}_{r}\right)^{-1}\bm{\theta}_{*}\bm{\theta}_{*}^{\top}\right)\] \[\leq \alpha^{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\left(\bm{ \Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-2}\right)\left\|\bm{\theta}_{ *}\right\|_{2}^{2}.\]

Since \(\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{2}\succcurlyeq \left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} +\alpha\mathbf{I}_{r}\right)^{2}\succcurlyeq 2\alpha\cdot\mathbf{P}_{\mathcal{S}} \bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}}+\alpha^{2}\mathbf{P}_{\mathcal{ S}}^{\perp}\), we have

\[\left(\bm{\Sigma}_{S}^{\phi}+\alpha\mathbf{I}_{r}\right)^{-2}\preccurlyeq \frac{1}{2\alpha}\left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{ \mathcal{S}}\right)^{\dagger}+\frac{1}{\alpha^{2}}\mathbf{P}_{\mathcal{S}}^{ \perp},\]

and thus

\[\text{Bias}\leq\left(\frac{\alpha}{2}\operatorname{tr}\left(\bm{\Sigma}^{ \phi}\left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S} }\right)^{\dagger}\right)+\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_ {\mathcal{S}}^{\perp}\right)\right)\left\|\bm{\theta}_{*}\right\|_{2}^{2}.\]

Combining the bias and variance terms, we have

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\leq \frac{\sigma^{2}}{n}\left(\operatorname{tr}\left(\bm{\Sigma}^{ \phi}\left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S} }\right)^{\dagger}\right)+\frac{1}{\alpha}\operatorname{tr}\left(\bm{\Sigma}^ {\phi}\mathbf{P}_{\mathcal{S}}^{\perp}\right)\right)\] \[+ \left(\frac{\alpha}{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi} \left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} \right)^{\dagger}\right)+\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_ {\mathcal{S}}^{\perp}\right)\right)\left\|\bm{\theta}_{*}\right\|_{2}^{2}\] \[\leq \frac{\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma}^{\phi} \left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} \right)^{\dagger}\right)+\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_ {\mathcal{S}}^{\perp}\right)\left\|\bm{\theta}_{*}\right\|_{2}^{2}\] \[+\frac{1}{\alpha}\cdot\frac{\sigma^{2}}{n}\operatorname{tr}\left( \bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\perp}\right)+\alpha\cdot\frac{\left\| \bm{\theta}_{*}\right\|_{2}^{2}}{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi} \left(\mathbf{P}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} \right)^{\dagger}\right).\]By taking \(\alpha_{*}=\sqrt{\frac{\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\bm{ \mathrm{P}}_{\bar{S}}^{\perp}\right)\Big{/}\left(\frac{\left\|\bm{\theta}_{*} \right\|_{2}^{2}}{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\left(\bm{\mathrm{ P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\bm{\mathrm{P}}_{\mathcal{S}}\right)^{ \dagger}\right)\right)}\), we have

\[\frac{1}{\alpha_{*}}\cdot\frac{\sigma^{2}}{n}\operatorname{tr} \left(\bm{\Sigma}^{\phi}\bm{\mathrm{P}}_{\bar{S}}^{\perp}\right)+\alpha_{*} \cdot\frac{\left\|\bm{\theta}_{*}\right\|_{2}^{2}}{2}\operatorname{tr}\left( \bm{\Sigma}^{\phi}\left(\bm{\mathrm{P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi} \bm{\mathrm{P}}_{\mathcal{S}}\right)^{\dagger}\right)\] \[\leq 2\sqrt{\frac{\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma}^{ \phi}\bm{\mathrm{P}}_{\bar{S}}^{\perp}\right)\cdot\frac{\left\|\bm{\theta}_{* }\right\|_{2}^{2}}{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\left(\bm{ \mathrm{P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\bm{\mathrm{P}}_{\mathcal{S}} \right)^{\dagger}\right)}\] \[\leq \frac{1}{\sqrt{2}}\left(\frac{\sigma^{2}}{n}\operatorname{tr} \left(\bm{\Sigma}^{\phi}\left(\bm{\mathrm{P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{ \phi}\bm{\mathrm{P}}_{\mathcal{S}}\right)^{\dagger}\right)+\operatorname{tr} \left(\bm{\Sigma}^{\phi}\bm{\mathrm{P}}_{\bar{S}}^{\perp}\right)\left\|\bm{ \theta}_{*}\right\|_{2}^{2}\right).\]

Therefore overall, we have

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right) \right]\leq \frac{2\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma}^{\phi} \left(\bm{\mathrm{P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\bm{\mathrm{P}}_{ \mathcal{S}}\right)^{\dagger}\right)+2\operatorname{tr}\left(\bm{\Sigma}^{ \phi}\bm{\mathrm{P}}_{\bar{S}}^{\perp}\right)\left\|\bm{\theta}_{*}\right\|_{2 }^{2}.\]

Proof of Corollary 2.3.: Given \(\bm{\mathrm{P}}_{\mathcal{S}}(c_{S}\bm{\Sigma}_{S}^{\phi}-\bm{\Sigma}^{\phi}) \bm{\mathrm{P}}_{\mathcal{S}}\succcurlyeq 0\) and \(\operatorname{rank}\left(\bm{\mathrm{P}}_{\mathcal{S}}\right)\asymp\overline{r}\), the variance term is asymptotically upper bounded by

\[\operatorname{variance}=\frac{2\sigma^{2}}{n}\operatorname{tr}\left(\bm{\Sigma} ^{\phi}\left(\bm{\mathrm{P}}_{\mathcal{S}}\bm{\Sigma}_{S}^{\phi}\bm{\mathrm{P} }_{\mathcal{S}}\right)^{\dagger}\right)\lesssim\frac{\sigma^{2}}{n}\cdot c_{S} \overline{r}.\]

Meanwhile, given \(\operatorname{tr}(\bm{\Sigma}^{\phi}\bm{\mathrm{P}}_{\bar{S}}^{\perp})\leq \frac{N}{n}\operatorname{tr}(\bm{\Sigma}^{\phi}-\langle\bm{\Sigma}^{\phi} \rangle_{\overline{r}})\) and \(\operatorname{tr}(\bm{\Sigma}^{\phi}-\langle\bm{\Sigma}^{\phi}\rangle_{ \overline{r}})\leq\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)/N\), the bias term can be asymptotically upper bounded by

\[\operatorname{bias}=2\operatorname{tr}\left(\bm{\Sigma}^{\phi}\bm{\mathrm{P }}_{\mathcal{S}}^{\perp}\right)\left\|\bm{\theta}_{*}\right\|_{2}^{2}\leq \frac{2}{n}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)\left\|\bm{\theta}_{ *}\right\|_{2}^{2}.\]

The result follows from Theorem 2.2 by combining the variance and bias terms. 

## Appendix C Proofs for Section 3.1

### Formal Statement and Proof of Theorem 3.1

**Theorem C.1** (Formal version of Theorem 3.1).: _Under Assumption 2.2 and 2.3 with a small intrinsic dimension \(\overline{r}\ll\min\left\{N,r\right\}\), for any \(\delta\in(0,1)\), draw a Gaussian random matrix \(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) with i.i.d. entries from \(\mathcal{N}\left(0,1/m\right)\) where \(m\asymp k/\delta\) for some \(k\geq 1.1\overline{r}\). Let \(\widetilde{\bm{\Sigma}}^{\phi}:=\bm{\Gamma}^{\top}\bm{\Sigma}^{\phi}\bm{\Gamma}\) and \(\widetilde{\bm{\Sigma}}_{S}^{\phi}:=\bm{\Gamma}^{\top}\bm{\Sigma}_{S}^{\phi}\bm{\Gamma}\) be the sketched gradient moments. For any \(S\subseteq[N]\) with \(n>m\) samples such that (i) \(\operatorname{rank}(\bm{\Sigma}_{S}^{\phi})=n\), and (ii) the \(k\)-th largest eigenvalue \(s_{k}(\widetilde{\bm{\Sigma}}_{S}^{\phi})\geq\gamma_{S}\) for some \(\gamma_{S}>0\), with probability at least \(1-\delta\) over \(\bm{\Gamma}\), there exists \(\alpha>0\) where (2) satisfies_

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\lesssim \frac{\sigma^{2}}{n}\operatorname{tr}\left(\widetilde{\bm{\Sigma}} ^{\phi}\langle\widetilde{\bm{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right)\] ( _variance_ ) \[+\frac{\sigma^{2}}{n}\frac{1}{m\gamma_{S}}\left\|\widetilde{\bm{ \Sigma}}^{\phi}\langle\widetilde{\bm{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger} \right\|_{2}\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)\] \[+\frac{1}{n}\left\|\widetilde{\bm{\Sigma}}^{\phi}\langle\widetilde{ \bm{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2}\operatorname{tr}\left( \bm{\Sigma}^{\phi}\right)\left\|\bm{\theta}_{*}\right\|_{2}^{2}\quad\left(\textbf{bias }\right).\]

_If \(S\) further satisfies \(\widetilde{\bm{\Sigma}}^{\phi}\prec c_{S}\widetilde{\bm{\Sigma}}_{S}^{\phi}\) for some \(c_{S}\geq\frac{n}{N}\), taking \(m=\max\{\sqrt{\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)/\gamma_{S}},1.1 \overline{r}/\delta\}\) leads to_

\[\mathbb{E}\left[\operatorname{ER}\left(\bm{\theta}_{S}\right)\right]\lesssim \textbf{variance}+\textbf{sketching error}+\textbf{bias}\lesssim\frac{c_{S}}{n} \left(\sigma^{2}m+\operatorname{tr}\left(\bm{\Sigma}^{\phi}\right)\left\|\bm{ \theta}_{*}\right\|_{2}^{2}\right).\] (8)

We start by introducing some helpful notations for the proofs. Let \(\bm{\mathrm{G}}:=\nabla_{\bm{\theta}}f^{\phi}\left(\bm{\mathrm{X}};\bm{\mathrm{ \mathbf{0}}}_{r}\right)\in\mathbb{R}^{N\times r}\) and \(\bm{\mathrm{G}}_{S}=\left[\bm{\mathrm{G}}\right]_{S}\in\mathbb{R}^{n\times r}\) be the original gradients of \(\mathcal{D}\) and \(\mathcal{D}_{S}\), respectively. Recall that \(\bm{\Sigma}^{\phi}=\bm{\mathrm{G}}^{\top}\bm{\mathrm{G}}/N\) and \(\bm{\Sigma}_{S}^{\phi}=\bm{\mathrm{G}}_{S}^{\top}\bm{\mathrm{G}}_{S}/n\) are the corresponding second moments.

We consider a Johnson-Lindenstrauss transform (JLT) [31]\(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) as follows:

**Definition C.1** (JLT [18] (adapting [17, Definition 3])).: _For any \(\epsilon>0\), \(\delta\in(0,1)\), and \(n\in\mathbb{N}\), a random matrix \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) is a \((\epsilon,\delta,k)\)-Johnson-Lindenstrauss transform (\((\epsilon,\delta,k)\)-JLT) if for any \(\mathbf{U}\in\mathbb{R}^{r\times k}\) consisting of \(k\) orthonormal columns in \(\mathbb{R}^{r}\), with probability at least \(1-\delta\),_

\[\left\|\mathbf{I}_{k}-\mathbf{U}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{U}\right\|_{2}\leq\epsilon.\]

**Definition C.2** (JL second moment property [87] (adapting [17, Definition 12])).: _For any \(\epsilon>0\), \(\delta\in(0,1)\), a random matrix \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) satisfies the \((\epsilon,\delta)\)-JL second moment property if_

\[\mathbb{E}\left[\left(\left\|\mathbf{\Gamma}^{\top}\mathbf{u}\right\|_{2}^{2} -1\right)^{2}\right]\leq\epsilon^{2}\delta\quad\forall\;\mathbf{u}\in\mathbb{ S}^{r-1}.\]

**Lemma C.2** (Approximated matrix-matrix multplication [87] (adapting [17, Theorem 13])).: _Given \(\epsilon>0\), \(\delta\in(0,1/2)\), and a random matrix \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) satisfying the \((\epsilon,\delta)\)-JL second moment property (Definition C.2), for any matrices \(\mathbf{A},\mathbf{B}\) each with \(r\) rows,_

\[\Pr\left[\left\|\mathbf{A}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{ B}-\mathbf{A}^{\top}\mathbf{B}\right\|_{F}>3\epsilon\left\|\mathbf{A}\right\|_{F} \left\|\mathbf{B}\right\|_{F}\right]\leq\delta.\]

One of the most classical constructions of a JLT with JL second moment property is the Gaussian embedding:

**Lemma C.3** (Gaussian embedding [17, Theorem 6]).: _For any \(\epsilon>0\), \(\delta\in(0,1)\), a Gaussian random matrix \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) with i.i.d. entries \(\mathbf{\Gamma}_{ij}\sim\mathcal{N}(0,1/m)\) (i) is a \((\epsilon,\delta,k)\)-JLT if \(m\gtrsim(k+\log(1/\delta))\,\epsilon^{-2}\); and (ii) satisfies the \((\epsilon,\delta)\)-JL second moment property if \(m\gtrsim\epsilon^{-2}\delta^{-1}\)._

Proof of Lemma C.3.: The \((\epsilon,\delta,k)\)-JLT condition follows directly from [17, Theorem 6].

To show the \((\epsilon,\delta)\)-JL second moment property, we observe that for any \(\mathbf{u}\in\mathbb{S}^{r-1}\), \(\left\|\mathbf{\Gamma}^{\top}\mathbf{u}\right\|_{2}^{2}=\mathbf{u}^{\top} \mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{u}\) is an average of \(m\) independent \(\chi^{2}\) random variables with mean \(1\) and variance \(2\), we have \(\mathbb{E}\left[\left\|\mathbf{\Gamma}^{\top}\mathbf{u}\right\|_{2}^{2} \right]=1\) and its variance is \(\mathbb{E}\left[\left(\left\|\mathbf{\Gamma}^{\top}\mathbf{u}\right\|_{2}^{2} -1\right)^{2}\right]=2/m\). Therefore, \(m\gtrsim\epsilon^{-2}\delta^{-1}\) leads to the \((\epsilon,\delta)\)-JL second moment property. 

**Remark C.1** ((Fast) Johnson-Lindenstrauss transforms).: _While we mainly focus on the Gaussian embedding in the analysis for simplicity, there is a rich spectrum of JLTs with the JL second moment property [88, 89, 90, 91], some of which enjoy remarkably better efficiency than the Gaussian embedding without compromising accuracy empirically. We refer interested readers to [17, 32, 33] for in-depth reviews on different JLTs and their applications, while briefly synopisizing two common choices and their efficiency as follows._

1. _[label=()]_
2. _Subgaussian embedding_ _[_88_]_ _is a random matrix_ \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) _with i.i.d. entries from a zero-mean subgaussian distribution with variance_ \(1/m\)_. Common choices include the Rademacher distribution and Gaussian distribution (i.e., Gaussian embedding)._ _Applying subgaussian embeddings to an_ \(N\times r\) _matrix_ \(\mathbf{A}\) _with_ \(\mathrm{nnz}(\mathbf{A})\leq Nr\) _nonzero entries takes_ \(O(\mathrm{nnz}(\mathbf{A})m)\leq O(Nrm)\) _time, while the involved matrix-matrix multiplication can be computed distributedly in parallel leveraging the efficiency of Level 3 BLAS_ _[_92_]__. In practice, generating and applying Rademacher random matrices tend to be slightly faster than Gaussian embeddings due to the simple discrete support._
3. _Sparse sign matrix_ _[_89, 93_]_ _is a sparse random matrix_ \(\mathbf{\Gamma}=\sqrt{\frac{r}{\xi}}\left[\boldsymbol{\gamma}_{1},\cdots, \boldsymbol{\gamma}_{r}\right]^{\top}\in\mathbb{R}^{r\times m}\) _(_\(\xi\in\mathbb{N}\)_) with i.i.d. rows_ \(\boldsymbol{\gamma}_{j}\in\mathbb{R}^{m}\) _each consisting of_ \(\xi\) _non-zero entries at uniformly random coordinates filled with Rademacher random variables. When_ \(\xi=1\)_,_ \(\mathbf{\Gamma}\) _is known as CountSketch_ _[_94_]_ _and requires as many as_ \(m=O(k^{2})\) _columns to satisfy the JLT property with constant distortion. Increasing the sparsity slightly,_ _[_95_]_ _showed that_ \(m=O(k\log k)\) _is sufficient for constant-distortion JLT when_ \(\xi=O(\log k)\)_. In practice,_ _[_96_]_ _suggested that a small constant sparsity_ \(\xi\geq 8\) _is usually enough for many applications like low-rank approximations._ _The sparse sign matrix can be applied to an_ \(N\times r\) _matrix_ \(\mathbf{A}\) _with_ \(\mathrm{nnz}(\mathbf{A})\) _nonzero entries in_ \(O(\mathrm{nnz}(\mathbf{A})\xi)\leq O(Nr\xi)\) _time, independent of the sketching size_ \(m\)_. With careful implementation, sketching via sparse sign matrices can be significantly faster than the subgaussian embeddings in practice_ _[_33, 43_]__._Let \(\widetilde{\mathbf{G}}:=\mathbf{G}\mathbf{\Gamma}\in\mathbb{R}^{N\times m}\) and \(\widetilde{\mathbf{G}}_{S}=\mathbf{G}_{S}\mathbf{\Gamma}\in\mathbb{R}^{n\times m}\) be the sketched gradients such that

\[\widetilde{\mathbf{\Sigma}}^{\phi}:=\mathbf{\Gamma}^{\top}\mathbf{\Sigma}^{\phi }\mathbf{\Gamma}=\widetilde{\mathbf{G}}^{\top}\widetilde{\mathbf{G}}/N\in \mathbb{R}^{m\times m},\quad\widetilde{\mathbf{\Sigma}}_{S}^{\phi}:=\mathbf{ \Gamma}^{\top}\mathbf{\Sigma}_{S}^{\phi}\mathbf{\Gamma}=\widetilde{\mathbf{G}} _{S}^{\top}\widetilde{\mathbf{G}}_{S}/n\in\mathbb{R}^{m\times m}.\]

In particular, for a Gaussian embedding \(\mathbf{\Gamma}\), when \(\mathrm{rank}(\mathbf{\Sigma}_{S}^{\phi})=\mathrm{rank}(\mathbf{G}_{S})=n\), \(\mathrm{rank}(\widetilde{\mathbf{\Sigma}}_{S}^{\phi})=\mathrm{rank}( \widetilde{\mathbf{G}}_{S})=m\) almost surely.

Recall the low intrinsic dimension \(\overline{r}\) from Assumption 2.3. For any \(k\in\mathbb{N}\) with \(1.1\overline{r}\leq k<m\), let \(\mathbf{P}_{\mathcal{S}}\in\mathbb{R}^{r\times r}\) be an orthogonal projector onto a dimension-\(k\) subspace \(\mathcal{S}\subseteq\mathrm{Range}(\mathbf{\Sigma}_{S}^{\phi})\):

\[\mathbf{P}_{\mathcal{S}}:=(\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{ \dagger}\mathbf{G}_{S})^{\dagger}(\langle\widetilde{\mathbf{G}}_{S}\rangle_{k }^{\dagger}\mathbf{G}_{S})=\mathbf{G}_{S}^{\dagger}\langle\widetilde{\mathbf{G }}_{S}\rangle_{k}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger} \mathbf{G}_{S},\] (9)

and \(\mathbf{P}_{S}^{\perp}=\mathbf{I}_{r}-\mathbf{P}_{\mathcal{S}}\) be its orthogonal complement. Throughout the proof of Theorem C.1, we assume the following:

**Assumption C.1**.: _Let \(\min\left\{N,r\right\}\gg n>m>k\geq 1.1\overline{r}\) such that \(\mathrm{rank}(\mathbf{\Sigma}_{S}^{\phi})=n\). We consider a Gaussian embedding (Lemma C.3) \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) with \(m\asymp k\) such that \(s_{k}(\widetilde{\mathbf{\Sigma}}_{S}^{\phi})\geq\gamma_{S}\) for some \(\gamma_{S}>0\)._

Proof of Theorems 3.1 and C.1.: We first recall from Theorem 2.2 that

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\leq \frac{2\sigma^{2}}{n}\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\left( \mathbf{P}_{\mathcal{S}}\mathbf{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}} \right)^{\dagger}\right)+2\operatorname{tr}\left(\mathbf{\Sigma}^{\phi} \mathbf{P}_{\mathcal{S}}^{\perp}\right)\left\|\boldsymbol{\theta}_{*}\right\|_ {2}^{2}.\]

Lemma C.4 suggests that for \(m\asymp k/\delta\), with probability at least \(1-\delta/2\),

\[\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\left(\mathbf{P}_{\mathcal{S}} \mathbf{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}}\right)^{\dagger}\right) \lesssim\operatorname{tr}\left(\widetilde{\mathbf{\Sigma}}^{\phi}\langle \widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right)+\frac{n}{m \gamma_{S}}\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\mathcal{ S}}^{\perp}\right).\]

Therefore,

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right] \lesssim\frac{\sigma^{2}}{n}\operatorname{tr}\left(\widetilde{\mathbf{\Sigma}}^{ \phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right)+ \left(\frac{\sigma^{2}}{m\gamma_{S}}+\left\|\boldsymbol{\theta}_{*}\right\|_ {2}^{2}\right)\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{ \mathcal{S}}^{\perp}\right).\]

Then, applying Lemma C.7 with the union bound, we have

\[\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\perp} \right)\lesssim\frac{1}{n}\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle \widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2} \operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)\]

with probability at least \(1-\delta\). This implies

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\lesssim \frac{\sigma^{2}}{n}\left(\operatorname{tr}\left(\widetilde{ \mathbf{\Sigma}}^{\phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k }^{\dagger}\right)+\frac{1}{m\gamma_{S}}\left\|\widetilde{\mathbf{\Sigma}}^{ \phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{ 2}\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)\right)\] \[\quad+\frac{1}{n}\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle \widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2} \operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)\left\|\boldsymbol{\theta}_{*} \right\|_{2}^{2}.\]

If \(S\) further satisfies \(\widetilde{\mathbf{\Sigma}}^{\phi}\prec c_{S}\widetilde{\mathbf{\Sigma}}_{S}^{\phi}\) for some \(c_{S}\geq\frac{n}{N}\), then we have

\[\operatorname{tr}\left(\widetilde{\mathbf{\Sigma}}^{\phi}\langle\widetilde{ \mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right)\leq\operatorname{tr} \left(\widetilde{\mathbf{\Sigma}}^{\phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{ \phi}\rangle_{k}^{\dagger}\right)\leq c_{S}m,\quad\left\|\widetilde{\mathbf{ \Sigma}}^{\phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger} \right\|_{2}\leq\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle\widetilde{\mathbf{ \Sigma}}_{S}^{\phi}\rangle^{\dagger}\right\|_{2}\leq c_{S}.\]

Therefore, (7) can be further simplified as

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\lesssim \frac{c_{S}\sigma^{2}}{n}\left(m+\frac{\operatorname{tr}\left(\mathbf{\Sigma}^{ \phi}\right)}{m\gamma_{S}}\right)+\frac{c_{S}}{n}\operatorname{tr}\left( \mathbf{\Sigma}^{\phi}\right)\left\|\boldsymbol{\theta}_{*}\right\|_{2}^{2}.\]

On the right-hand-side, the first (variance) term is minimized at \(m=\sqrt{\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)/\gamma_{S}}\) where \(m+\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)/(m\gamma_{S})\leq 2\sqrt{ \operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)/\gamma_{S}}=2m\). In addition, incorporating the assumption that \(m\asymp k/\delta\) for some \(k\geq 1.1\overline{r}\), we take \(m=\max\{\sqrt{\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right)/\gamma_{S}},1.1 \overline{r}/\delta\}\) and get

\[\mathbb{E}\left[\mathrm{ER}\left(\boldsymbol{\theta}_{S}\right)\right]\lesssim \frac{c_{S}\sigma^{2}}{n}m+\frac{c_{S}}{n}\operatorname{tr}\left(\mathbf{ \Sigma}^{\phi}\right)\left\|\boldsymbol{\theta}_{*}\right\|_{2}^{2}=\frac{c_{S} }{n}\left(\sigma^{2}m+\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\right) \left\|\boldsymbol{\theta}_{*}\right\|_{2}^{2}\right).\]

Theorem 3.1 is simplified from Theorem C.1 by taking \(k=\left\lceil 1.1\overline{r}\right\rceil\) and \(\delta=0.1\)

### Upper Bounding Variance

**Lemma C.4**.: _For any \(\delta\in(0,1)\), let \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) be a Gaussian embedding (Lemma C.3) with \(m\asymp k/\delta\) columns. Then, with probability at least \(1-\delta\) over \(\mathbf{\Gamma}\),_

\[\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\left(\mathbf{P}_{S}\mathbf{ \Sigma}_{S}^{\phi}\mathbf{P}_{S}\right)^{\dagger}\right)\lesssim\operatorname {tr}\left(\mathbf{\widetilde{\Sigma}}^{\phi}\mathbf{\widetilde{\Sigma}}_{S}^{ \phi}\right)_{k}^{\dagger}\right)+\frac{n}{m\gamma_{S}}\operatorname{tr} \left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\perp}\right).\]

Proof of Lemma C.4.: We first observe that since \(\operatorname{rank}(\mathbf{\Sigma}_{S}^{\phi})=n\) implies \(\mathbf{G}_{S}\mathbf{G}_{S}^{\dagger}=\mathbf{I}_{n}\),

\[\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\mathbf{\Gamma}=\mathbf{G}_{S}\mathbf{G }_{S}^{\dagger}(\widetilde{\mathbf{G}}_{S})_{k}\mathbf{\langle}\widetilde{ \mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\dagger}\mathbf{G}_{S}\mathbf{\Gamma}= \mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\mathbf{\langle }\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\dagger}\widetilde{\mathbf{G }}_{S}=\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k},\]

and therefore, \(\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}=\operatorname{ argmin}_{\mathbf{Z}\in\mathbb{R}^{N\times n}}\left\|\mathbf{Z}\right\|_{F}^{2}\)_s.t._\(\mathbf{Z}\in\operatorname{argmin}_{\mathbf{Z}}\left\|\mathbf{G}- \mathbf{Z}\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\right\|_{F}^{2}\) and

\[\mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}=\operatorname*{argmin}_{\mathbf{Z}\in\mathbb{R}^{N \times n}}\left\|\mathbf{Z}\right\|_{F}^{2}\text{ s.t. }\mathbf{Z}\in\operatorname*{ argmin}_{\mathbf{Z}}\left\|\mathbf{(G}-\mathbf{Z}\mathbf{G}_{S}\mathbf{P}_{ \mathcal{S}}\mathbf{)\Gamma}\right\|_{F}^{2}\]

is an approximated solution from a sketched least square problem.

Accuracy of sketched least square residual.For \(m\asymp k/(\epsilon^{2}\delta)\), Lemma C.3 implies that a Gaussian embedding \(\mathbf{\Gamma}\) is a \((1/2,\delta/2,k)\)-JLT (Definition C.1) with \((\Theta(\epsilon/\sqrt{k}),\delta/2)\)-JL second moment property (Definition C.2). Then, since \(\operatorname{rank}\left(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\right)=k\), by Lemma C.5, with probability at least \(1-\delta\) over \(\mathbf{\Gamma}\),

\[\left\|\left(\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}- \mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}\right)\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\right\|_{F }^{2}\leq\epsilon^{2}\left\|\mathbf{G}-\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{ \mathcal{S}})^{\dagger}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})\right\|_{2}^{2}.\]

Since \(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}=\mathbf{G}_{S}\mathbf{G}_{S}^{\dagger }\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\mathbf{ \langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\dagger}\mathbf{G}_{S }=\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\mathbf{ \langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\dagger}\mathbf{G}_{S }\),

\[(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}(\mathbf{G}_{S}\mathbf{P}_{ \mathcal{S}})=\mathbf{G}_{S}^{\dagger}\mathbf{\langle}\widetilde{\mathbf{G}}_{ S}\mathbf{\rangle}_{k}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{ \dagger}\mathbf{G}_{S}=\mathbf{P}_{\mathcal{S}}.\]

Therefore,

\[\left\|\left(\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}- \mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}\right)\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\right\|_{F }^{2}\leq\epsilon^{2}\left\|\mathbf{G}\mathbf{P}_{\mathcal{S}}^{\perp}\right\| _{F}^{2}.\] (10)

Accuracy of sketched least square solution.To upper bound \(\left\|\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}- \mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}\right\|_{F}\), we first observe from (10) that

\[\left\|\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}- \mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}\right\|_{F}^{2}\leq\epsilon^{2}\left\|\mathbf{(G}_{S} \mathbf{P}_{\mathcal{S}})^{\dagger}\right\|_{2}^{2}\left\|\mathbf{G}\mathbf{P}_{ \mathcal{S}}^{\perp}\right\|_{F}^{2}.\]

\(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\mathbf{G}_{S}^{\top}=\mathbf{\langle} \widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\mathbf{\langle}\widetilde{\mathbf{G }}_{S}\mathbf{\rangle}_{k}^{\dagger}\mathbf{G}_{S}\mathbf{\rangle}_{S}^{\top} \mathbf{G}_{S}\mathbf{\rangle}_{S}^{\top}\). Since \(\operatorname{rank}\left(\mathbf{G}_{S}\right)=n\), Lemma C.6 implies that for a Gaussian embedding \(\mathbf{\Gamma}\), with high probability. Therefore,

\[\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}}\mathbf{G}_{S}^{\top}\succcurlyeq O \left(\frac{m}{n}\right)\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{ \dagger}\widetilde{\mathbf{G}}_{S}\mathbf{\widetilde{G}}_{S}^{\top}=O\left(\frac {m}{n}\right)\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k} \mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\top}.\]

Recall that \(\mathbf{\langle}\widetilde{\mathbf{G}}_{S}^{\phi}\mathbf{\rangle}_{k}=\frac{1}{n} \mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\top}\mathbf{ \langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\) and \(s_{k}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}^{\phi}\mathbf{\rangle}\geq\gamma_{S}\), we have

\[\left\|\mathbf{(G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}\right\|_{2}^{2}= \left\|\mathbf{(G}_{S}\mathbf{P}_{\mathcal{S}}\mathbf{G}_{S}^{\top})^{ \dagger}\right\|_{2}\leq O\left(\frac{n}{m}\right)\left\|\left(\mathbf{ \langle}\widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}^{\top}\mathbf{\langle} \widetilde{\mathbf{G}}_{S}\mathbf{\rangle}_{k}\right)^{\dagger}\right\|_{2}\] \[\leq O\left(\frac{n}{m}\right)\frac{1}{n\gamma_{S}}=O\left(\frac{1}{m \gamma_{S}}\right).\]

Therefore, applying a union bound gives that with probability at least \(1-\delta\) over \(\mathbf{\Gamma}\),

\[\left\|\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}- \mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S}\mathbf{ \rangle}_{k}^{\dagger}\right\|_{F}^{2}\leq O\left(\frac{\epsilon^{2}}{m\gamma_{S} }\right)\left\|\mathbf{G}\mathbf{P}_{\mathcal{S}}^{\perp}\right\|_{F}^{2}.\] (11)

To upper bound \(\left\|\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger}\right\|_{F}^{2}\), we observe that by (11),

\[\left\|\mathbf{G}(\mathbf{G}_{S}\mathbf{P}_{\mathcal{S}})^{\dagger} \right\|_{F}^{2}\leq 2\left\|\mathbf{\widetilde{G}}\mathbf{\langle}\widetilde{\mathbf{G}}_{S} \mathbf{\rangle}_Finally, normalizing by multiplying \(n/N\) on both sides gives

\[\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\left(\mathbf{P}_{\mathcal{S}} \mathbf{\Sigma}_{S}^{\phi}\mathbf{P}_{\mathcal{S}}\right)^{\dagger}\right) \lesssim\operatorname{tr}\left(\widetilde{\mathbf{\Sigma}}^{\phi}(\widetilde{ \mathbf{\Sigma}}_{S}^{\phi})_{k}^{\dagger}\right)+\epsilon^{2}\frac{n}{m\gamma_ {S}}\operatorname{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{ \perp}\right).\]

Taking any small constant \(\epsilon>0\) completes the proof. 

**Lemma C.5** (Adapting [17, Theorem 23]).: _For any \(\epsilon>0\) and \(\delta\in(0,1)\), let \(\mathbf{\Gamma}\in\mathbb{R}^{r\times m}\) be a \((1/2,\delta/2,k)\)-JLT (Definition C.1) with \((\Theta(\epsilon/\sqrt{k}),\delta/2)\)-JL second moment property (Definition C.2). Given \(\mathbf{A}\in\mathbb{R}^{r\times n}\) with \(\operatorname{rank}(\mathbf{A})=k\) and \(\mathbf{B}\in\mathbb{R}^{r\times N}\), let_

\[\widehat{\mathbf{W}}=\operatorname*{argmin}_{\mathbf{W}\in\mathbb{ R}^{n\times N}}\left\|\mathbf{W}\right\|_{F}^{2}\text{ s.t. }\mathbf{W}\in\operatorname*{argmin}_{\mathbf{W}}\left\|\mathbf{\Gamma}^{ \top}(\mathbf{A}\mathbf{W}-\mathbf{B})\right\|_{F}^{2},\] \[\mathbf{W}_{*}=\operatorname*{argmin}_{\mathbf{W}\in\mathbb{R}^{ n\times N}}\left\|\mathbf{W}\right\|_{F}^{2}\text{ s.t. }\mathbf{W}\in\operatorname*{argmin}_{\mathbf{W}}\left\|\mathbf{A}\mathbf{W}- \mathbf{B}\right\|_{F}^{2}.\]

_Then, with probability at least \(1-\delta\) over \(\mathbf{\Gamma}\), \(\left\|\mathbf{A}(\widehat{\mathbf{W}}-\mathbf{W}_{*})\right\|_{F}\leq \epsilon\left\|\mathbf{A}\mathbf{W}_{*}-\mathbf{B}\right\|_{F}\)._

Proof of Lemma c.5.: Analogous to the proof of [17, Theorem 23], let \(\mathbf{A}=\mathbf{Q}\mathbf{R}\) be a reduced QR decomposition of \(\mathbf{A}\) such that \(\mathbf{Q}\in\mathbb{R}^{r\times k}\) is an orthonormal basis for \(\operatorname{Range}(\mathbf{A})\), and \(\mathbf{R}\in\mathbb{R}^{k\times n}\). Reparametrizing \(\widehat{\mathbf{Z}}=\mathbf{R}\widehat{\mathbf{W}}\) and \(\mathbf{Z}_{*}=\mathbf{R}\mathbf{W}_{*}\), up to constant scaling of \(\epsilon\), it is sufficient to show

\[\left\|\mathbf{Q}(\widehat{\mathbf{Z}}-\mathbf{Z}_{*})\right\|_{F}=\left\| \widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F}\leq O(\epsilon)\left\|\mathbf{ Q}\mathbf{Z}_{*}-\mathbf{B}\right\|_{F}.\]

Since \(\mathbf{\Gamma}\in\mathbb{R}^{r\times k}\) is an \((1/2,\delta/2,k)\)-JLT, we have \(\left\|\mathbf{I}_{k}-\mathbf{Q}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{Q}\right\|_{2}\leq 1/2\) with probability at least \(1-\delta/2\) and

\[\left\|\widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F} \leq\] \[\leq \left\|\mathbf{Q}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{Q}(\widehat{\mathbf{Z}}-\mathbf{Z}_{*})\right\|_{F}+1/2\left\| \widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F}\]

which implies \(\left\|\widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F}\leq 2\left\|\mathbf{Q}^{ \top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{Q}(\widehat{\mathbf{Z}}- \mathbf{Z}_{*})\right\|_{F}\) with probability at least \(1-\delta/2\).

By the normal equation of the sketched least square problem, \(\mathbf{Q}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{Q}\widehat{ \mathbf{Z}}=\mathbf{Q}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{B}\). Thus,

\[\left\|\widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F}\leq 2\left\|\mathbf{Q}^{ \top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}(\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B })\right\|_{F}.\]

Since \(\mathbf{Q}^{\top}(\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B})=-\mathbf{Q}^{\top} \left(\mathbf{I}_{r}-\mathbf{Q}\mathbf{Q}^{\top}\right)\mathbf{B}=\mathbf{0}_{k \times N}\) and \(\mathbf{\Gamma}\) has \((\epsilon/\sqrt{k},\delta/2)\)-JL second moment property, Lemma C.2 implies that with probability at least \(1-\delta/2\),

\[\left\|\mathbf{Q}^{\top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}(\mathbf{Q} \mathbf{Z}_{*}-\mathbf{B})\right\|_{F}\leq\Theta\left(\epsilon/\sqrt{k} \right)\left\|\mathbf{Q}\mathbf{I}_{F}\left\|\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B }\right\|_{F}=\Theta(\epsilon)\left\|\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B} \right\|_{F}.\]

Then, by the union bound, with probability at least \(1-\delta\) over \(\mathbf{\Gamma}\), we have

\[\left\|\widehat{\mathbf{Z}}-\mathbf{Z}_{*}\right\|_{F}\leq 2\left\|\mathbf{Q}^{ \top}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}(\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B })\right\|_{F}\leq\epsilon\left\|\mathbf{Q}\mathbf{Z}_{*}-\mathbf{B}\right\|_{F}\]

with a suitable choice of the constant for \(\Theta(\epsilon)\). 

**Lemma C.6** ([97]).: _For a random matrix \(\mathbf{\Omega}\in\mathbb{R}^{n\times m}\) (\(n>m\)) consisting of i.i.d. subgaussian entries with mean zero and variance one, with high probability,_

\[O\left(\left(\sqrt{n}-\sqrt{m}\right)^{2}\right)\mathbf{I}_{n}\preccurlyeq \mathbf{\Omega}\mathbf{\Omega}^{\top}\preccurlyeq O\left(\left(\sqrt{n}+\sqrt{m} \right)^{2}\right)\mathbf{I}_{n}.\]

### Upper Bounding Low-rank Approximation Error

**Lemma C.7**.: _Under Assumption 2.3, let \(\bm{\Gamma}\in\mathbb{R}^{r\times m}\) be a Gaussian embedding (Lemma C.3) such that there exists \(m>k\geq 1.1\bar{r}\) satisfying \(s_{k}(\widetilde{\bm{\Sigma}}_{S}^{\phi})\geq\gamma_{S}\) for some \(\gamma_{S}>0\). Then, with probability at least \(1-\exp\left(-\Omega(\bar{r})\right)\),_

\[\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_{S}^{\perp}\right)\lesssim \frac{1}{n}\left\|\widetilde{\bm{\Sigma}}^{\phi}\langle\widetilde{\bm{\Sigma}} _{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2}\operatorname{tr}\left(\bm{\Sigma} ^{\phi}\right).\]

Proof of Lemma c.7.: Here we follow a similar proof strategy as [43, Theorem 1]. Let \(\bm{\Pi}_{S}:=\left[\mathbf{I}_{N}\right]_{S}\in\mathbb{R}^{n\times N}\) be the selection matrix associated with \(S\subseteq[N]\). We introduce the following \(N\times N\) oblique projectors:

\[\mathbf{M}_{S}:=\mathbf{G}\mathbf{G}_{S}^{\dagger}\langle\widetilde{\mathbf{G} }_{S}\rangle_{k}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{ \Pi}_{S},\quad\widetilde{\mathbf{M}}_{S}:=\widetilde{\mathbf{G}}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_{S}.\]

In particular, \(\mathbf{M}_{S}\) and \(\widetilde{\mathbf{M}}_{S}\) are the oblique projectors since with \(\bm{\Pi}_{S}\mathbf{G}=\mathbf{G}_{S}\) and \(\mathbf{G}_{S}\mathbf{G}_{S}^{\dagger}=\mathbf{I}_{n}\),

\[\mathbf{M}_{S}^{2}= \mathbf{G}\mathbf{G}_{S}^{\dagger}\langle\widetilde{\mathbf{G}} _{S}\rangle_{k}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi }_{S}\mathbf{G}_{S}\mathbf{G}_{S}^{\dagger}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}^{\dagger}\bm{\Pi}_{S}\] \[= \mathbf{G}\mathbf{G}_{S}^{\dagger}\langle\widetilde{\mathbf{G}}_ {S}\rangle_{k}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_ {S}=\mathbf{M}_{S},\]

and with \(\bm{\Pi}_{S}\widetilde{\mathbf{G}}=\widetilde{\mathbf{G}}_{S}\),

\[\widetilde{\mathbf{M}}_{S}^{2}= \widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_ {k}^{\dagger}\widetilde{\mathbf{G}}_{S}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}^{\dagger}\bm{\Pi}_{S}=\widetilde{\mathbf{G}}\langle\widetilde{ \mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_{S}=\widetilde{\mathbf{M}}_{S}.\]

Recalling \(\mathbf{P}_{\mathcal{S}}\) from (9), we observe the following identities:

\[\mathbf{M}_{S}\mathbf{G}=\mathbf{G}\mathbf{G}_{S}^{\dagger}\langle\widetilde{ \mathbf{G}}_{S}\rangle_{k}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{ \dagger}\mathbf{G}_{S}=\mathbf{G}\mathbf{P}_{\mathcal{S}};\] (12)

since \(\mathbf{G}_{S}\mathbf{G}_{S}^{\dagger}=\mathbf{I}_{n}\),

\[\widetilde{\mathbf{M}}_{S}\mathbf{M}_{S}=\widetilde{\mathbf{G}}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\mathbf{G}_{S}\mathbf{G}_{S}^{ \dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}\langle\widetilde{ \mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_{S}=\widetilde{\mathbf{G}}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_{S}=\widetilde{ \mathbf{M}}_{S};\] (13)

and

\[\widetilde{\mathbf{M}}_{S}\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G} }_{S}\rangle_{k}^{\dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}= \widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger} \widetilde{\mathbf{G}}_{S}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{ \dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}=\widetilde{\mathbf{G}} \langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\langle\widetilde{ \mathbf{G}}_{S}\rangle_{k}.\] (14)

Combining (12), (13), and (14), we have

\[\mathbf{G}\mathbf{P}_{\mathcal{S}}= \mathbf{G}-\mathbf{G}\mathbf{P}_{\mathcal{S}}=\left(\mathbf{I}_{N }-\mathbf{M}_{S}\right)\mathbf{G}\quad\text{(by (\ref{eq:12}))}\] \[= \left(\mathbf{I}_{N}-\widetilde{\mathbf{M}}_{S}\right)\mathbf{G} \mathbf{P}_{\mathcal{S}}^{\perp}\quad\text{(by (\ref{eq:12}))}\] \[= \left(\mathbf{I}_{N}-\widetilde{\mathbf{M}}_{S}\right)\left( \mathbf{I}_{N}-\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle _{k}^{\dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}\widetilde{\mathbf{G} }^{\dagger}\right)\mathbf{G}\mathbf{P}_{\mathcal{S}}^{\perp}\quad\text{(by (\ref{eq:14}))}\,.\]

Since \(\left\|\mathbf{P}_{\mathcal{S}}^{\perp}\right\|_{2}^{2}=1\), this implies

\[\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S} }^{\perp}\right)= \frac{1}{N}\left\|\mathbf{G}\mathbf{P}_{\mathcal{S}}\right\|_{F}^{2}= \frac{1}{N}\left\|\left(\mathbf{I}_{N}-\widetilde{\mathbf{M}}_{S}\right) \left(\mathbf{I}_{N}-\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}^{\dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}\widetilde{ \mathbf{G}}^{\dagger}\right)\mathbf{G}\mathbf{P}_{\mathcal{S}}^{\dagger}\right\|_ {F}^{2}\] \[\leq \frac{1}{N}\left\|\mathbf{I}_{N}-\widetilde{\mathbf{M}}_{S} \right\|_{2}^{2}\left\|\left(\mathbf{I}_{N}-\widetilde{\mathbf{G}}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}\widetilde{\mathbf{G}}^{\dagger}\right)\mathbf{G}\right\|_{F}^{2}.\]

By the operator norm identity for projectors [98], we have

\[\left\|\mathbf{I}_{N}-\widetilde{\mathbf{M}}_{S}\right\|_{2}^{2}=\left\| \widetilde{\mathbf{M}}_{S}\right\|_{2}^{2}=\left\|\widetilde{\mathbf{G}}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\bm{\Pi}_{S}\right\|_{2}^{2}= \left\|\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{ \dagger}\right\|_{2}^{2}=\frac{N}{n}\left\|\widetilde{\bm{\Sigma}}^{\phi}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\right\|_{2},\]

and therefore,

\[\operatorname{tr}\left(\bm{\Sigma}^{\phi}\mathbf{P}_{\mathcal{S}}^{\perp}\right) \leq\frac{1}{n}\left\|\widetilde{\bm{\Sigma}}^{\phi}\langle\widetilde{\bm{ \Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2}\left\|\left(\mathbf{I}_{N}- \widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}\widetilde{\mathbf{G}}^{\dagger}\right) \mathbf{G}\right\|_{F}^{2}.\]

Since \(\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\langle \widetilde{\mathbf{G}}_{S}\rangle_{k}\widetilde{\mathbf{G}}^{\dagger}\) is a rank-\(k\) orthogonal projector onto

\[\operatorname{Range}\left(\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}^{\dagger}\right)=\operatorname{Range}\left(\mathbf{G}\left(\mathbf{ \Gamma}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\right)\right)\]and Gaussian embeddings are rotationally invariant, \(\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger} \langle\widetilde{\mathbf{G}}_{S}\rangle_{k}^{\dagger}\widetilde{\mathbf{G}}_{S }\rangle_{k}\widetilde{\mathbf{G}}^{\dagger}\) shares the same distribution as \((\mathbf{G}\mathbf{\Omega})(\mathbf{G}\mathbf{\Omega})^{\dagger}\) for a \(r\times k\) Gaussian embedding \(\mathbf{\Omega}\) with \([\mathbf{\Omega}]_{i,j}\sim\mathcal{N}(0,1/k)\)_i.i.d.._ Then, we observe that \(\|(\mathbf{I}_{N}-\widetilde{\mathbf{G}}\langle\widetilde{\mathbf{G}}_{S} \rangle_{k}^{\dagger}\langle\widetilde{\mathbf{G}}_{S}\rangle_{k}\widetilde{ \mathbf{G}}^{\dagger})\mathbf{G}\|_{F}^{2}\) is the rank-\(k\) randomized range-finder error of \(\mathbf{G}\), which can be controlled according to Lemma C.8: with probability at least \(1-\exp\left(-\Omega(\overline{r})\right)\),

\[\mathrm{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\widetilde{S}}^{\perp} \right)\lesssim\frac{1}{n}\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle \widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2}\left\| \mathbf{G}-\langle\mathbf{G}\rangle_{\overline{r}}\right\|_{F}^{2}=\frac{N}{ n}\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle\widetilde{\mathbf{\Sigma}}_{S}^{ \phi}\rangle_{k}^{\dagger}\right\|_{2}\mathrm{tr}\left(\mathbf{\Sigma}^{\phi} -\langle\mathbf{\Sigma}^{\phi}\rangle_{\overline{r}}\right).\]

By the definition of \(\overline{r}\) in Assumption 2.3, \(\mathrm{tr}\left(\mathbf{\Sigma}^{\phi}-\langle\mathbf{\Sigma}^{\phi}\rangle_{ \overline{r}}\right)\leq\mathrm{tr}\left(\mathbf{\Sigma}^{\phi}\right)/N\) and thus,

\[\mathrm{tr}\left(\mathbf{\Sigma}^{\phi}\mathbf{P}_{\widetilde{S}}^{\perp} \right)\lesssim\frac{1}{n}\left\|\widetilde{\mathbf{\Sigma}}^{\phi}\langle \widetilde{\mathbf{\Sigma}}_{S}^{\phi}\rangle_{k}^{\dagger}\right\|_{2} \mathrm{tr}\left(\mathbf{\Sigma}^{\phi}\right).\]

**Lemma C.8** (Randomized range-finder error (simplifying [32, Theorem 10.7])).: _Let \(\mathbf{\Omega}\in\mathbb{R}^{r\times k}\) be a Gaussian embedding with \(\left|\mathbf{\Omega}\right|_{i,j}\sim\mathcal{N}(0,1/k)\) i.i.d.. For any \(\mathbf{G}\in\mathbb{R}^{N\times r}\) and \(\overline{r}\in\mathbb{N}\) such that \(1.1\overline{r}\leq k\ll\min\left\{N,r\right\}\), with probability at least \(1-\exp\left(-\Omega(\overline{r})\right)\),_

\[\left\|\left(\mathbf{I}_{N}-(\mathbf{G}\mathbf{\Omega})(\mathbf{G}\mathbf{ \Omega})^{\dagger}\right)\mathbf{G}\right\|_{F}\lesssim\left\|\mathbf{G}- \langle\mathbf{G}\rangle_{\overline{r}}\right\|_{F}.\]

## Appendix D Experiment Details for Section 4.1

### Implementation Details

Synthetic data generation.We consider a set of \(N=2000\) samples with high-dimensional pre-trained representations \(\phi(\mathbf{X})\in\mathbb{R}^{N\times r}\) where \(r=2400\), modeled by a Gaussian mixture model (GMM) consisting of \(\overline{r}=8\) well-separated clusters, each with random sizes and variances. Specifically, we generate the GMM dataset as follows:

* Randomly partition the \(N\) samples into \(\overline{r}=8\) clusters with sizes \(\{N_{j}\mid j\in[\overline{r}]\}\).
* For each \(j\in[\overline{r}]\), generate the cluster mean \(\boldsymbol{\mu}_{j}\in\mathbb{R}^{r}\) with \(\boldsymbol{\mu}_{j}=(Z_{j}\overline{r})\cdot\mathbf{e}_{j}\) where \(Z_{j}\sim\mathrm{Unif}([\overline{r}])\) and variance \(\sigma_{j}=Z_{j}^{\prime}\cdot\sigma_{\max}\) where \(Z_{j}^{\prime}\sim\mathrm{Unif}([0,1])\) and \(\sigma_{\max}=0.04\).
* Generate representations \(\left\{\phi(\mathbf{x}_{i})\sim\mathcal{N}(\boldsymbol{\mu}_{j},\sigma_{j}^{2} \mathbf{I}_{r})\;\middle|\;i\in[N_{j}]\right\}\)_i.i.d._ for each cluster \(j\in[\overline{r}]\).
* Draw a latent label generator \(\boldsymbol{\theta}_{g}\sim\mathcal{N}(\mathbf{0}_{r},\mathbf{I}_{r})\). For each cluster \(j\in[\overline{r}]\), assign the same label \(y_{i}=\boldsymbol{\mu}_{j}^{\top}\boldsymbol{\theta}_{g}\) for all samples \(i\in[N_{j}]\) within the cluster.

Ridge regression.We solve the ridge regression problem over the selected coreset \(\mathcal{D}_{S}\) of \(n\) samples and tune the regularization hyperparameter \(\alpha\) via grid search over \(100\) linearly spaced values in \([10^{-2},10^{2}]\) with 2-fold cross-validation.

### Baselines

We compare SkMM to the following unsupervised data selection methods for regression:

1. **Uniform sampling** (Uniform) selects \(n\) samples uniformly at random from the full dataset \(\mathcal{D}\).
2. **Herding**[51, 52] (Herding) selects data greedily to minimize the distance between the centers of the coreset \(\mathcal{D}_{S}\) and the original dataset \(\mathcal{D}\). Notice that although herding aims to reduce the "bias" of the coreset center, it fails to control our notion of bias in the low-rank approximation sense. Given the construction of the GMM dataset, herding has more emphasis on variance reduction, as illustrated in Figure 2.
3. **K-center greedy**[53](K-center) provides a greedy heuristic for the minimax facility location problem that aims to minimize the maximum distance between any non-coreset sample and the nearest coreset sample.
4. **Adaptive sampling**[44, 56] (Adaptive) iteratively samples data based on their squared norms and adaptively updates the distribution by eliminating the spanning subspace of the selected samples from the dataset. It is proved in the recent work [44] that adaptive sampling achieves nearly optimal sample complexity for low-rank approximations, matching that of volume sampling [39, 41] (with the best know theoretical guarantee) up to a logarithmic factor.

In practice, adaptive sampling generally achieves comparable accuracy to volume sampling for low-rank approximations, with considerably better efficiency [44, 45]. Due to the prohibitive cost of volume sampling in high dimensions, we choose adaptive sampling in the comparison.
5. **Truncated**[18, 72] and **ridge leverage score sampling**[19, 73, 74] (T/R-leverage) are the extensions of classical leverage score sampling [54] to high dimensions. In particular, leverage score sampling is originally designed for low-dimensional linear regression, while degenerating to uniform sampling in high dimensions. Consider the high-dimensional representations \(\phi(\mathbf{X})\in\mathbb{R}^{N\times r}\) (\(r>N\)) in our setting, for each \(i\in[N]\), * leverage score: \(l_{i}:=\phi(\mathbf{x}_{i})^{\top}(\phi(\mathbf{X})^{\top}\phi(\mathbf{X}))^{ \dagger}\phi(\mathbf{x}_{i})\), * truncated leverage score: \(l_{i}^{(m)}:=\phi(\mathbf{x}_{i})^{\top}(\langle\phi(\mathbf{X})\rangle_{m}^{ \top}\langle\phi(\mathbf{X})\rangle_{m})^{\dagger}\phi(\mathbf{x}_{i})\) for a given truncation rank \(m\), and * ridge leverage score: \(l_{i}^{(\rho)}:=\phi(\mathbf{x}_{i})^{\top}(\phi(\mathbf{X})^{\top}\phi( \mathbf{X})+\rho\mathbf{I}_{r})^{\dagger}\phi(\mathbf{x}_{i})\) for a given regularization parameter \(\rho>0\). Larger \(\rho\) brings ridge leverage score sampling closer to uniform sampling. Therefore, both truncated and ridge leverage score sampling balance the variance-bias tradeoff by adjusting the truncation rank \(m\) and regularization parameter \(\rho\), respectively.

Baseline details.For both Herding and K-center, we adopt the DeepCore implementation [1]. Notice that Herding is a deterministic algorithm. For Adaptive, we use the implementation from [45]. For T-leverage, we use a rank-\(m\) truncated SVD to compute the leverage scores, with \(m=4\overline{r}=32\) as in SkkMM (_i.e._, providing both methods approximately the same amount of information and compute). For R-leverage, we choose \(\rho=10^{3}\).

## Appendix E Additional Experiments and Details for Section 4.3

Implementation details.For CLIP standard transform, we transform the image size to 224, with normalization mean (0.48145466, 0.4578275, 0.40821073) and std (0.26862954, 0.26130258, 0.27577711).

Parameter Count.We show the parameter sizes for the two-layer fine-tuning experiments in [7]. The representation dimension \(d\) is \(512\) for ResNet18, the number of classes \(K\) is \(10\) for CIFAR-10 and \(196\) for Stanford Cars. The last layer parameter size is \(5130\) for CIFAR-10 and \(100548\) for Stanford Cars. The second but last layer parameter size is \(2364426\) for CIFAR-10. When we do

\begin{table}
\begin{tabular}{c c c c c c c} \hline  & \(n\) & 2000 & 2500 & 3000 & 3500 & 4000 \\ \hline \multirow{2}{*}{Uniform Sampling} & Acc & 29.19 \(\pm\) 0.37 & 32.83 \(\pm\) 0.19 & 35.69 \(\pm\) 0.35 & 38.31 \(\pm\) 0.16 & 40.35 \(\pm\) 0.26 \\  & F1 & 26.14 \(\pm\) 0.39 & 29.91 \(\pm\) 0.16 & 32.80 \(\pm\) 0.37 & 35.38 \(\pm\) 0.19 & 37.51 \(\pm\) 0.23 \\ \hline \multirow{2}{*}{Herding [51]} & Acc & 29.19 \(\pm\) 0.21 & 32.42 \(\pm\) 0.16 & 35.83 \(\pm\) 0.24 & 38.30 \(\pm\) 0.19 & 40.51 \(\pm\) 0.19 \\  & F1 & 25.90 \(\pm\) 0.24 & 29.48 \(\pm\) 0.23 & 23.89 \(\pm\) 0.27 & 35.50 \(\pm\) 0.22 & 37.56 \(\pm\) 0.21 \\ \hline \multirow{2}{*}{Contextual Diversity [78]} & Acc & 28.50 \(\pm\) 0.34 & 32.66 \(\pm\) 0.27 & 35.67 \(\pm\) 0.32 & 38.31 \(\pm\) 0.15 & 40.53 \(\pm\) 0.18 \\  & F1 & 25.65 \(\pm\) 0.40 & 29.79 \(\pm\) 0.29 & 32.86 \(\pm\) 0.31 & 35.55 \(\pm\) 0.14 & 37.81 \(\pm\) 0.23 \\ \hline \multirow{2}{*}{Glister [79]} & Acc & 29.16 \(\pm\) 0.26 & 32.91 \(\pm\) 0.19 & 36.03 \(\pm\) 0.20 & 38.16 \(\pm\) 0.12 & 40.47 \(\pm\) 0.16 \\  & F1 & 26.33 \(\pm\) 0.19 & 30.05 \(\pm\) 0.28 & **33.26 \(\pm\) 0.18** & 35.41 \(\pm\) 0.14 & 37.63 \(\pm\) 0.17 \\ \hline \multirow{2}{*}{GraNd [66]} & Acc & 28.59 \(\pm\) 0.17 & 32.67 \(\pm\) 0.20 & 35.83 \(\pm\) 0.16 & 38.58 \(\pm\) 0.15 & 40.70 \(\pm\) 0.11 \\  & F1 & 25.66 \(\pm\) 0.15 & 29.70 \(\pm\) 0.22 & 32.76 \(\pm\) 0.16 & 35.72 \(\pm\) 0.15 & 37.83 \(\pm\) 0.11 \\ \hline \multirow{2}{*}{Forgetting [80]} & Acc & 28.61 \(\pm\) 0.31 & 32.48 \(\pm\) 0.28 & 35.18 \(\pm\) 0.24 & 37.78 \(\pm\) 0.22 & 40.24 \(\pm\) 0.13 \\  & F1 & 25.64 \(\pm\) 0.25 & 29.58 \(\pm\) 0.30 & 32.38 \(\pm\) 0.20 & 35.16 \(\pm\) 0.18 & 37.41 \(\pm\) 0.14 \\ \hline \multirow{2}{*}{DeepFool [81]} & Acc & 24.97 \(\pm\) 0.20 & 29.02 \(\pm\) 0.17 & 32.60 \(\pm\) 0.18 & 35.59 \(\pm\) 0.24 & 38.20 \(\pm\) 0.22 \\  & F1 & 22.11 \(\pm\) 0.11 & 26.08 \(\pm\) 0.29 & 29.83 \(\pm\) 0.27 & 32.92 \(\pm\) 0.33 & 35.47 \(\pm\) 0.22 \\ \hline \multirow{2}{*}{Entropy [82]} & Acc & 28.87 \(\pm\) 0.13 & 32.84 \(\pm\) 0.20 & 35.64 \(\pm\) 0.20 & 37.96 \(\pm\) 0.11 & 40.29 \(\pm\) 0.27 \\  & F1 & 25.95 \(\pm\) 0.17 & 30.03 \(\pm\) 0.17 & 32.85 \(\pm\) 0.23 & 35.19 \(\pm\) 0.12 & 37.33 \(\pm\) 0.34 \\ \hline \multirow{2}{*}{Margin [82]} & Acc & 29.18 \(\pm\) 0.12 & 32.73 \(\pm\) 0.15 & 35.67 \(\pm\) 0.30 & 38.27 \(\pm\) 0.20 & 40.58 \(\pm\) 0.06 \\  & F1 & 26.15 \(\pm\) 0.12 & 29.66 \(\pm\) 0.05 & 32.86 \(\pm\) 0.30 & 35.61 \(\pm\) 0.17 & 37.77 \(\pm\) 0.

the last two layers fine-tuning, the total parameter size is \(8398858\) for CIFAR-10 and \(2459844\) for Stanford Cars.

StanfordCars Baselines.For DeepCore baselines, there are some methods that require warmup training before the finetuning (e.g. Uncertainty-Entropy), we use one-layer training and two-layer training (freezing other layers) in the warmup training for linear probing selection and two-layer finetuning selection. The warmup training is done with Adam optimizer with learning rate 0.01 for 10 epochs.

Finetuning DetailsWe finetune the model for 50 epochs for both linear probing and finetuning using Adam optimizer with a learning rate 0.01.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \(n\) & 2500 & 3000 & 3500 & 4000 \\ \hline Uniform Sampling & 77.55 \(\pm\) 0.16 & 78.04 \(\pm\) 0.18 & 78.46 \(\pm\) 0.09 & 78.83 \(\pm\) 0.15 \\ Herding [51] & 77.58 \(\pm\) 0.17 & 77.74 \(\pm\) 0.19 & 78.37 \(\pm\) 0.14 & 78.39 \(\pm\) 0.11 \\ Contextual Diversity [78] & 77.24 \(\pm\) 0.08 & 77.65 \(\pm\) 0.10 & 78.17 \(\pm\) 0.07 & 78.22 \(\pm\) 0.11 \\ Glister [79] & 77.46 \(\pm\) 0.13 & 77.95 \(\pm\) 0.15 & 78.19 \(\pm\) 0.10 & 78.54 \(\pm\) 0.08 \\ GraNd [66] & 77.22 \(\pm\) 0.10 & 77.74 \(\pm\) 0.07 & 78.31 \(\pm\) 0.11 & 78.49 \(\pm\) 0.10 \\ Forgetting [80] & 77.32 \(\pm\) 0.20 & 77.87 \(\pm\) 0.21 & 78.05 \(\pm\) 0.04 & 78.53 \(\pm\) 0.09 \\ DeepFool [81] & 77.25 \(\pm\) 0.09 & 77.70 \(\pm\) 0.21 & 78.04 \(\pm\) 0.12 & 78.46 \(\pm\) 0.13 \\ Entropy [82] & 77.55 \(\pm\) 0.21 & 77.74 \(\pm\) 0.12 & 78.23 \(\pm\) 0.20 & 78.41 \(\pm\) 0.08 \\ Margin [82] & 77.24 \(\pm\) 0.15 & 77.81 \(\pm\) 0.23 & 78.32 \(\pm\) 0.17 & 78.52 \(\pm\) 0.15 \\ LeastConfidence [82] & 77.46 \(\pm\) 0.23 & 77.93 \(\pm\) 0.14 & 78.21 \(\pm\) 0.17 & 78.60 \(\pm\) 0.10 \\ \hline SkMM-FT & **77.75 \(\pm\) 0.08** & **78.12 \(\pm\) 0.04** & **78.66 \(\pm\) 0.06** & **79.11 \(\pm\) 0.02** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Accuracy of FT over ResNet18 on CIFAR-10.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \(n\) & 1000 & 2000 & 3000 & 4000 \\ \hline Uniform Sampling & \(91.68\pm 0.45\) & \(92.22\pm 0.25\) & \(92.60\pm 0.14\) & \(92.79\pm 0.12\) \\ Herding [51] & \(91.68\pm 0.27\) & \(92.20\pm 0.17\) & \(92.72\pm 0.51\) & \(93.00\pm 0.45\) \\ Contextual Diversity [78] & \(91.98\pm 0.11\) & \(92.53\pm 0.05\) & \(92.81\pm 0.25\) & \(92.99\pm 0.24\) \\ Glister [79] & \(91.09\pm 0.08\) & \(91.60\pm 0.23\) & \(91.83\pm 0.08\) & \(91.87\pm 0.09\) \\ GraNd [66] & \(88.48\pm 0.90\) & \(88.89\pm 0.53\) & \(89.04\pm 0.24\) & \(89.54\pm 0.46\) \\ Forgetting [80] & \(91.51\pm 0.01\) & \(92.15\pm 0.02\) & \(92.61\pm 0.01\) & \(92.81\pm 0.01\) \\ DeepFool [81] & \(91.68\pm 0.32\) & \(91.78\pm 0.79\) & \(91.93\pm 0.89\) & \(92.18\pm 0.70\) \\ Entropy [82] & \(88.66\pm 1.01\) & \(89.96\pm 0.42\) & \(90.27\pm 1.11\) & \(91.06\pm 0.75\) \\ Margin [82] & \(91.22\pm 0.64\) & \(91.60\pm 0.92\) & \(91.94\pm 0.83\) & \(92.09\pm 0.81\) \\ Least Confidence [82] & \(89.38\pm 1.73\) & \(90.49\pm 1.47\) & \(90.83\pm 1.43\) & \(91.26\pm 1.30\) \\ \hline SkMM-LP & **92.96 \(\pm\) 0.07** & **93.38 \(\pm\) 0.01** & **93.67 \(\pm\) 0.01** & **93.78 \(\pm\) 0.04** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Classification accuracy (%) of LP over CLIP on CIFAR-10.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We summarized our contributions at the end of the introduction (Section 1). Guidelines:
2. The answer NA means that the abstract and introduction do not include the claims made in the paper.
3. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
4. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
5. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
6. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed limitations and future directions in Section 5. Guidelines:
7. The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
8. The authors are encouraged to create a separate "Limitations" section in their paper.
9. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
10. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
11. The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
12. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
13. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
14. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
15. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions are clearly stated in place with the theoretical results, while the proofs are deferred to the appendices, with clear hyperlink references. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide clear pseudocode and experiment setups in the main text, as well as implementation details in the appendices. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our experiment code for both the synthetic and real data is available at https://anonymous.4open.science/r/data_pruning. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify the experiments setting in the main context (subsection 4.1,subsection 4.3) and appendix (subsection D.1, subsection D.2) opensourced our code and scripts at https://anonymous.4open.science/r/data_pruning. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: We reported the stand error with 5 seeds. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All the experiments could be done with A40 or even smaller GPUs. We use 4 workers and 32 GB Memory. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, we follow the NeurIPS Code of Ethics. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a learning theory paper with no societal impact. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This is a learning theory paper with no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: We use the open-sourced dataset (CIFAR10) and models (CLIP), they are open for research usage. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: https://anonymous.4open.science/r/data_pruning. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.