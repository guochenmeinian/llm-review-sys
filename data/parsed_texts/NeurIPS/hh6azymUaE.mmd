# Privacy-Preserving Logistic Regression Training with

A Faster Gradient Variant

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called quadratic gradient for privacy-preserving logistic regression training. The core of quadratic gradient can be seen as an extension of the simplified fixed Hessian [4].

We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with quadratic gradient and evaluate the enhanced algorithms on several datasets. Experiments show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the raw first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training, obtaining a comparable result by only \(3\) iterations.

There is a promising chance that quadratic gradient could be used to enhance other first-order gradient methods for general numerical optimization problems.

## 1 Introduction

Given a person's healthcare data related to a certain disease, we can train a logistic regression (LR) model capable of telling whether or not this person is likely to develop this disease. However, such personal health information is highly private to individuals. The privacy concern, therefore, becomes a major obstacle for individuals to share their biomedical data. The most secure solution is to encrypt the data into ciphertexts first by Homomorphic Encryption (HE) and then securely outsource the ciphertexts to the cloud, without allowing the cloud to access the data directly. iDASH is an annual competition that aims to call for implementing interesting cryptographic schemes in a biological context. Since 2014, iDASH has included the theme of genomics and biomedical privacy. The third track of the 2017 iDASH competition and the second track of the 2018 iDASH competition were both to develop homomorphic-encryption-based solutions for building an LR model over encrypted data.

Several studies on logistic regression models are based on homomorphic encryption. Kim et al. [14] discussed the problem of performing LR training in an encrypted environment. They used the full batch gradient descent in the training process and the least-squares method to get the approximation of the sigmoid function. In the iDASH 2017 competition, Bonte and Vercauteren [4], Kim et al. [12], Chen et al. [5], and Crawford et al. [8] all investigated the same problem that Kim et al. [14] studied. In the iDASH competition of 2018, Kim et al. [13] and Blatt et al. [2] further worked on it for an efficient packing and semi-parallel algorithm. The papers most relevant to this work are [4] and [12]. Bonte and Vercauteren [4] developed a practical algorithm called the simplified fixed Hessian (SFH) method. Our study complements their work and adopts the ciphertext packing technique proposed by Kim et al. [12] for efficient homomorphic computation.

Our specific contributions in this paper are as follows:

1. We propose a new gradient variant, quadratic gradient, which can combine the first-order gradient methods and the second-order Newton-Raphson method as one.
2. We develop two enhanced gradient methods by equipping the original methods with quadratic gradient. The resulting methods show a state-of-the-art performance in the convergence speed.
3. We adopt the enhanced NAG method to implement privacy-preserving logistical regression training, to our best knowledge, which seems to be the best candidate without compromising much on computation and storage.

## 2 Preliminaries

We adopt the square brackets "\([\ ]\)" to denote the index of a vector or matrix element in what follows. For example, for a vector \(\bm{v}\in\mathbb{R}^{(n)}\) and a matrix \(M\in\mathbb{R}^{m\times n}\), \(\bm{v}[i]\) or \(\bm{v}_{[i]}\) means the \(i\)-th element of vector \(\bm{v}\) and \(M[i][j]\) or \(M_{[i][j]}\) the \(j\)-th element in the \(i\)-th row of \(M\).

### Fully Homomorphic Encryption

Fully Homomorphic Encryption (FHE) is a type of cryptographic scheme that can be used to compute an arbitrary number of additions and multiplications directly on the encrypted data. It was not until 2009 that Gentry constructed the first FHE scheme via a bootstrapping operation [9]. FHE schemes themselves are computationally time-consuming; the choice of dataset encoding matters likewise to the efficiency. In addition to these two limits, how to manage the magnitude of plaintext [11] also contributes to the slowdown. Cheon et al. [6] proposed a method to construct an HE scheme with a rescaling procedure which could eliminate this technical bottleneck effectively. We adopt their open-source implementation HEAAN while implementing our homomorphic LR algorithms. It is inevitable to pack a vector of multiple plaintexts into a single ciphertext for yielding a better amortized time of homomorphic computation. HEAAN supports a parallel technique (aka SIMD) to pack multiple numbers in a single polynomial by virtue of the Chinese Remainder Theorem and provides rotation operation on plaintext slots. The underlying HE scheme in HEAAN is well described in [12; 14; 10].

### Database Encoding Method

Kim et al. [12] proposed an efficient and promising database-encoding method by using SIMD technique, which could make full use of the computation and storage resources. Suppose that a database has a training dataset consisting of \(n\) samples with \((1+d)\) covariates, they packed the training dataset \(Z\) into a single ciphertext in a row-by-row manner.

Using this encoding scheme, we can manipulate the data matrix \(Z\) by performing HE operations on the ciphertext \(Enc[Z]\), with the help of only three HE operations - rotation, addition and multiplication.

Han et al. [10] introduced several operations to manipulate the ciphertexts, such as a procedure named "SumColVec" to compute the summation of the columns of a matrix. By dint of these basic operations, more complex calculations such as computing the gradients in logistic regression models are achievable.

### Logistic Regression

Logistic regression is widely used in binary classification tasks to infer whether a binary-valued variable belongs to a certain class or not. LR can be generalized from linear regression [15] by mapping the whole real line \((\bm{\beta}^{T}\mathbf{x})\) to \((0,1)\) via the sigmoid function \(\sigma(z)=1/(1+\exp(-z))\), where the vector \(\bm{\beta}\in\mathbb{R}^{(1+d)}\) is the main parameter of LR and the vector \(\mathbf{x}=(1,x_{1},\ldots,x_{d})\in\mathbb{R}^{(1+d)}\) the input covariate. Thus logistic regression can be formulated with the class label \(y\in\{\pm 1\}\) as follows:\[\Pr(y=+1|\mathbf{x},\bm{\beta}) =\sigma(\bm{\beta}^{T}\mathbf{x}) =\frac{1}{1+e^{-\bm{\beta}^{T}\mathbf{x}}},\] \[\Pr(y=-1|\mathbf{x},\bm{\beta}) =1-\sigma(\bm{\beta}^{T}\mathbf{x}) =\frac{1}{1+e^{+\bm{\beta}^{T}\mathbf{x}}}.\]

LR sets a threshold (usually \(0.5\)) and compares its output with it to decide the resulting class label.

The logistic regression problem can be transformed into an optimization problem that seeks a parameter \(\bm{\beta}\) to maximize \(L(\bm{\beta})=\prod_{i=1}^{n}\Pr(y_{i}|\mathbf{x}_{i},\bm{\beta})\) or its log-likelihood function \(l(\bm{\beta})\) for convenience in the calculation:

\[l(\bm{\beta})=\ln L(\bm{\beta})=-\sum_{i=1}^{n}\ln(1+e^{-y_{i}\bm{\beta}^{T} \mathbf{x}_{i}}),\]

where \(n\) is the number of examples in the training dataset. LR does not have a closed form of maximizing \(l(\bm{\beta})\) and two main methods are adopted to estimate the parameters of an LR model: (a) gradient descent method via the gradient; and (b) Newton's method by the Hessian matrix. The gradient and Hessian of the log-likelihood function \(l(\bm{\beta})\) are given by, respectively:

\[\nabla_{\bm{\beta}}l(\bm{\beta}) =\sum_{i}(1-\sigma(y_{i}\bm{\beta}^{T}\mathbf{x}_{i}))y_{i} \mathbf{x}_{i},\] \[\nabla_{\bm{\beta}}^{2}l(\bm{\beta}) =\sum_{i}(y_{i}\mathbf{x}_{i})(\sigma(y_{i}\bm{\beta}^{T}\mathbf{ x}_{i})-1)\sigma(y_{i}\bm{\beta}^{T}\mathbf{x}_{i})(y_{i}\mathbf{x}_{i})\] \[=X^{T}SX,\]

where \(S\) is a diagonal matrix with entries \(S_{ii}=(\sigma(y_{i}\bm{\beta}^{T}\mathbf{x}_{i})-1)\sigma(y_{i}\bm{\beta}^{T} \mathbf{x}_{i})\) and \(X\) the dataset.

The log-likelihood function \(l(\bm{\beta})\) of LR has at most a unique global maximum [1], where its gradient is zero. Newton's method is a second-order technique to numerically find the roots of a real-valued differentiable function, and thus can be used to solve the \(\bm{\beta}\) in \(\nabla_{\bm{\beta}}l(\bm{\beta})=0\) for LR.

## 3 Technical Details

It is quite time-consuming to compute the Hessian matrix and its inverse in Newton's method for each iteration. One way to limit this downside is to replace the varying Hessian with a fixed matrix \(\bar{H}\). This novel technique is called the fixed Hessian Newton's method. Bohning and Lindsay [3] have shown that the convergence of Newton's method is guaranteed as long as \(\bar{H}\leq\nabla_{\bm{\beta}}^{2}l(\bm{\beta})\), where \(\bar{H}\) is a symmetric negative-definite matrix independent of \(\bm{\beta}\) and "\(\leq\)" denotes the Loewner ordering in the sense that the difference \(\nabla_{\bm{\beta}}^{2}l(\bm{\beta})-\bar{H}\) is non-negative definite. With such a fixed Hessian matrix \(\bar{H}\), the iteration for Newton's method can be simplified to:

\[\bm{\beta}_{t+1}=\bm{\beta}_{t}-\bar{H}^{-1}\nabla_{\bm{\beta}}l(\bm{\beta}).\]

Bohning and Lindsay also suggest the fixed matrix \(\bar{H}=-\frac{1}{4}X^{T}X\) is a good lower bound for the Hessian of the log-likelihood function \(l(\bm{\beta})\) in LR.

### the Simplified Fixed Hessian method

Bonte and Vercauteren [4] simplify this lower bound \(\bar{H}\) further due to the need for inverting the fixed Hessian in the encrypted domain. They replace the matrix \(\bar{H}\) with a diagonal matrix \(B\) whose diagonal elements are simply the sums of each row in \(\bar{H}\). They also suggest a specific order of calculation to get \(B\) more efficiently. Their new approximation \(B\) of the fixed Hessian is:

\[B=\left[\begin{array}{cccc}\sum_{i=0}^{d}\bar{h}_{0i}&0&\ldots&0\\ 0&\sum_{i=0}^{d}\bar{h}_{1i}&\ldots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\ldots&\sum_{i=0}^{d}\bar{h}_{di}\end{array}\right],\]

where \(\bar{h}_{ki}\) is the element of \(\bar{H}\). This diagonal matrix \(B\) is in a very simple form and can be obtained from \(\bar{H}\) without much difficulty. The inverse of \(B\) can be approximated in the encrypted form by means of computing the inverse of every diagonal element of \(B\) via the iterative of Newton's method with an appropriate start value. Their simplified fixed Hessian method can be formulated as follows:

\[\bm{\beta}_{t+1} =\bm{\beta}_{t}-B^{-1}\cdot\nabla_{\bm{\beta}}l(\bm{\beta}),\] \[=\bm{\beta}_{t}-\left[\begin{array}{cccc}b_{00}&0&\dots&0\\ 0&b_{11}&\dots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\dots&b_{dd}\end{array}\right]\cdot\left[\begin{array}{c}\nabla_{0}\\ \nabla_{1}\\ \vdots\\ \nabla_{d}\end{array}\right]=\bm{\beta}_{t}-\left[\begin{array}{c}b_{00} \cdot\nabla_{0}\\ b_{11}\cdot\nabla_{1}\\ \vdots\\ b_{dd}\cdot\nabla_{d}\end{array}\right],\]

where \(b_{ii}\) is the reciprocal of \(\sum_{i=0}^{d}\bar{h}_{0i}\) and \(\nabla_{i}\) is the element of \(\nabla_{\bm{\beta}}l(\bm{\beta})\).

Consider a special situation: if \(b_{00},\dots,b_{dd}\) are all the same value \(-\eta\) with \(\eta\) > 0, the iterative formula of the SFH method can be given as:

\[\bm{\beta}_{t+1}=\bm{\beta}_{t}-(-\eta)\cdot\left[\begin{array}{c}\nabla_{0 }\\ \nabla_{1}\\ \vdots\\ \nabla_{d}\end{array}\right]=\bm{\beta}_{t}+\eta\cdot\nabla_{\bm{\beta}}l( \bm{\beta}),\]

which is the same as the formula of the naive gradient \(ascent\) method. Such coincident is just what the idea behind this work comes from: there is some relation between the Hessian matrix and the learning rate of the gradient (descent) method. We consider \(b_{ii}\cdot\nabla_{i}\) as a new enhanced gradient variant's element and assign a new learning rate to it. As long as we ensure that this new learning rate decreases from a positive floating-point number greater than 1 (such as 2) to 1 in a bounded number of iteration steps, the fixed Hessian Newton's method guarantees the algorithm will converge eventually.

The SFH method proposed by Bonte and Vercauteren [4] has two limitations: (a) in the construction of the simplified fixed Hessian matrix, all entries in the symmetric matrix \(\bar{H}\) need to be non-positive. For machine learning applications the datasets will be in advance normalized into the range [0,1], meeting the convergence condition of the SFH method. However, for other cases such as numerical optimization, it doesn't always hold; and (b) the simplified fixed Hessian matrix \(B\) that Bonte and Vercauteren [4] constructed, as well as the fixed Hessian matrix \(\bar{H}=-\frac{1}{4}X^{T}X\), can still be singular, especially when the dataset is a high-dimensional sparse matrix, such as the MNIST datasets. We extend their work by removing these limitations so as to generalize this simplified fixed Hessian to be invertible in any case and propose a faster gradient variant, which we term quadratic gradient.

### Quadratic Gradient

Suppose that a differentiable scalar-valued function \(F(\mathbf{x})\) has its gradient \(\bm{g}\) and Hessian matrix \(H\), with any matrix \(\bar{H}\leq H\) in the Loewner ordering for a maximization problem as follows:

\[\bm{g}=\left[\begin{array}{c}g_{0}\\ g_{1}\\ \vdots\\ g_{d}\end{array}\right],\quad H=\left[\begin{array}{cccc}\nabla_{00}^{2}& \nabla_{01}^{2}&\dots&\nabla_{0d}^{2}\\ \nabla_{10}^{2}&\nabla_{11}^{2}&\dots&\nabla_{1d}^{2}\\ \vdots&\vdots&\ddots&\vdots\\ \bar{\lambda}_{d0}&\bar{\lambda}_{d1}&\dots&\bar{\lambda}_{dd}\end{array} \right],\quad\bar{H}\quad=\left[\begin{array}{cccc}\bar{h}_{00}&\bar{h}_{01} &\dots&\bar{h}_{0d}\\ \bar{h}_{10}&\bar{h}_{11}&\dots&\bar{h}_{1d}\\ \vdots&\vdots&\ddots&\vdots\\ \bar{h}_{d0}&\bar{h}_{d1}&\dots&\bar{h}_{dd}\end{array}\right],\]

where \(\nabla_{ij}^{2}=\nabla_{ji}^{2}=\frac{\partial^{2}F}{\partial x_{i}\partial x _{j}}\). We construct a new Hessian matrix \(\tilde{B}\) as follows:

\[\tilde{B}=\left[\begin{array}{cccc}-\varepsilon-\sum_{i=0}^{d}|\bar{h}_{0i} |&0&\dots&0\\ 0&-\varepsilon-\sum_{i=0}^{d}|\bar{h}_{1i}|&\dots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\dots&-\varepsilon-\sum_{i=0}^{d}|\bar{h}_{di}|\end{array}\right],\]

where \(\varepsilon\) is a small positive constant to avoid division by zero (usually set to \(1e-8\)).

As long as \(\tilde{B}\) satisfies the convergence condition of the above fixed Hessian method, \(\tilde{B}\leq H\), we can use this approximation \(\tilde{B}\) of the Hessian matrix as a lower bound. Since we already assume that \(\bar{H}\leq H\), it will suffice to show that \(\tilde{B}\leq\bar{H}\). We prove \(\tilde{B}\leq\bar{H}\) in a similar way that [4] did.

**Lemma 1**.: _Let \(A\in\mathbb{R}^{n\times n}\) be a symmetric matrix, and let \(B\) be the diagonal matrix whose diagonal entries \(B_{kk}=-\varepsilon-\sum_{i}|A_{ki}|\) for \(k=1,\ldots,n\), then \(B\leq A\)._

Proof.: By definition of the Loewner ordering, we have to prove the difference matrix \(C=A-B\) is non-negative definite, which means that all the eigenvalues of \(C\) need to be non-negative. By construction of \(C\) we have that \(C_{ij}=A_{ij}+\varepsilon+\sum_{k=1}^{n}|A_{ik}|\) for \(i=j\) and \(C_{ij}=A_{ij}\) for \(i\neq j\). By means of Gerschgorin's circle theorem, we can bound every eigenvalue \(\lambda\) of \(C\) in the sense that \(|\lambda-C_{ii}|\leq\sum_{i\neq j}|C_{ij}|\) for some index \(i\in\{1,2,\ldots,n\}\). We conclude that \(\lambda\geq A_{ii}+\varepsilon+|A_{ii}|\geq\varepsilon>0\) for all eigenvalues \(\lambda\) and thus that \(B\leq A\). 

**Definition 3.1** (Quadratic Gradient).: _Given such a \(\tilde{B}\) above, we define the quadratic gradient as \(G=\bar{B}\cdot\bm{g}\) with a new learning rate \(\eta\), where \(\tilde{B}\) is a diagonal matrix with diagonal entries \(\bar{B}_{kk}=1/|\tilde{B}_{kk}|\), and \(\eta\) should be always no less than 1 and decrease to 1 in a limited number of iteration steps. Note that \(G\) is still a column vector of the same size as the gradient \(\bm{g}\). To maximize the function \(F(\mathbf{x})\), we can use the iterative formulas: \(\mathbf{x}_{k+1}=\mathbf{x}_{k}+\eta\cdot G\), just like the naive gradient. To minimize the function \(F(x)\) is the same as to just maximize the function \(-F(x)\), in which case we need to construct the \(\tilde{B}\) by any good lower bound \(\bar{H}\) of the Hessian \(-H\) of \(-F(x)\) or any good upper bound \(\bar{H}\) of the Hessian \(H\) of \(F(x)\). We point out here that \(\bar{H}\) could be the Hessian matrix \(H\) itself._

In our experiments, we use \(\bar{H}\) = \(-\frac{1}{4}X^{T}X\) to construct our \(\tilde{B}\).

### Two Enhanced Methods

Quadratic Gradient can be used to enhance NAG and Adagrad.

NAG is a different variant of the momentum method to give the momentum term much more prescience. The iterative formulas of the gradient \(ascent\) method for NAG are as follows:

\[V_{t+1} =\bm{\beta}_{t}+\alpha_{t}\cdot\nabla J(\bm{\beta}_{t}),\] (3) \[\bm{\beta}_{t+1} =(1-\gamma_{t})\cdot V_{t+1}+\gamma_{t}\cdot V_{t},\] (4)

where \(V_{t+1}\) is the intermediate variable used for updating the final weight \(\bm{\beta}_{t+1}\) and \(\gamma_{t}\in(0,1)\) is a smoothing parameter of moving average to evaluate the gradient at an approximate future position [12]. The enhanced NAG is to replace (3) with \(V_{t+1}=\bm{\beta}_{t}+(1+\alpha_{t})\cdot G\). Our enhanced NAG method is described in Algorithm 1.

Adagrad is a gradient-based algorithm suitable for dealing with sparse data. The updated operations of Adagrad and its quadratic-gradient version, for every parameter \(\bm{\beta}_{[i]}\) at each iteration step \(t\), are as follows, respectively:

\[\bm{\beta}_{[i]}^{(t+1)} =\bm{\beta}_{[i]}^{(t)}-\frac{\eta}{\varepsilon+\sqrt{\sum_{k=1} ^{t}\bm{g}_{[i]}^{(t)}\cdot\bm{g}_{[i]}^{(t)}}}\cdot\bm{g}_{[i]}^{(t)},\] \[\bm{\beta}_{[i]}^{(t+1)} =\bm{\beta}_{[i]}^{(t)}-\frac{1+\eta}{\varepsilon+\sqrt{\sum_{k=1 }^{t}G_{[i]}^{(t)}\cdot G_{[i]}^{(t)}}}\cdot G_{[i]}^{(t)}.\]

**Performance Evaluation** We evaluate the performance of various algorithms in the clear using the Python programming language on the same desktop computer with an Intel Core CPU G640 at 1.60 GHz and 7.3 GB RAM. Since our focus is on how fast the algorithms converge in the training phase, the loss function, maximum likelihood estimation (MLE), is selected as the only indicator. We evaluate four algorithms, NAG, Adagrad, and their quadratic-gradient versions (denoted as Enhanced NAG and Enhanced Adagrad, respectively) on the datasets that Kim et al. [12] adopted: the iDASH genomic dataset (iDASH), the Myocardial Infarction dataset from Edinburgh (Edinburgh), Low Birth weight Study (Ibw), Ninanes III (nhanes3), Prostate Cancer study (pcs), and Umaru Impact Study datasets (uis). The genomic dataset is provided by the third task in the iDASH competition of 2017, which consists of 1579 records. Each record has 103 binary genotypes and a binary phenotype indicating if the patient has cancer. The other five datasets all have a single binary dependent variable. Figures 1 and 2 show that except for the enhanced Adagrad method on the iDASH genomic dataset our enhanced methods all converge faster than their original ones in other cases. In all the Python```
0: training dataset \(X\in\mathbb{R}^{n\times(1+d)}\); training label \(Y\in\mathbb{R}^{n\times 1}\); learning rate \(lr\in\mathbb{R}(\texttt{set to 10.0 in this work in order to align with the baseline work})\); and the number \(\kappa\) of iterations;
0: the parameter vector \(V\in\mathbb{R}^{(1+d)}\)
1: Set \(\bar{H}\leftarrow-\frac{1}{4}X^{T}X\)\(\triangleright\)\(\bar{H}\in\mathbb{R}^{(1+d)\times(1+d)}\)
2: Set \(V\leftarrow\mathbf{0}\), \(W\leftarrow\mathbf{0}\), \(\bar{B}\leftarrow\mathbf{0}\)\(\triangleright\)\(V\in\mathbb{R}^{(1+d)}\), \(W\in\mathbb{R}^{(1+d)}\), \(\bar{B}\in\mathbb{R}^{(1+d)\times(1+d)}\)
3:for\(i:=0\) to \(d\)do
4:\(\bar{B}[i][i]\leftarrow\varepsilon\)\(\triangleright\)\(\varepsilon\) is a small positive constant such as \(1e-8\)
5:for\(j:=0\) to \(d\)do
6:\(\bar{B}[i][i]\leftarrow\bar{B}[i][i]+|\bar{H}[i][j]|\)
7:endfor
8:endfor
9: Set \(\alpha_{0}\gets 0.01\), \(\alpha_{1}\gets 0.5\times(1+\sqrt{1+4\times\alpha_{0}^{2}})\)
10:for\(count:=1\) to \(\kappa\)do
11: Set \(Z\leftarrow\mathbf{0}\)\(\triangleright\)\(Z\in\mathbb{R}^{n}\) is the inputs for sigmoid function
12:for\(i:=1\) to \(n\)do
13:for\(j:=0\) to \(d\)do
14:\(Z[i]\gets Z[i]+Y[i]\times V[j]\times X[i][j]\)
15:endfor
16:endfor
17: Set \(\boldsymbol{\sigma}\leftarrow\mathbf{0}\)\(\triangleright\)\(\boldsymbol{\sigma}\in\mathbb{R}^{n}\) is to store the outputs of the sigmoid function
18:for\(i:=1\) to \(n\)do
19:\(\boldsymbol{\sigma}[i]\gets 1/(1+\exp(-Z[i]))\)
20:endfor
21: Set \(\boldsymbol{g}\leftarrow\mathbf{0}\)
22:for\(j:=0\) to \(d\)do
23:for\(i:=1\) to \(n\)do
24:\(\boldsymbol{g}[j]\leftarrow\boldsymbol{g}[j]+(1-\boldsymbol{\sigma}[i])\times Y [i]\times X[i][j]\)
25:endfor
26:endfor
27: Set \(G\leftarrow\mathbf{0}\)
28:for\(j:=0\) to \(d\)do
29:\(G[j]\leftarrow\bar{B}[j][j]\times\boldsymbol{g}[j]\)
30:endfor
31: Set \(\eta\leftarrow(1-\alpha_{0})/\alpha_{1}\), \(\gamma\gets lr/(n\times count)\)\(\triangleright\)\(n\) is the size of training data; \(lr\) is set to 10.0 in this work
32:for\(j:=0\) to \(d\)do
33:\(w_{temp}\gets V[j]+(1+\gamma)\times G[j]\)
34:\(V[j]\leftarrow(1-\eta)\times w_{temp}+\eta\times W[j]\)
35:\(W[j]\gets w_{temp}\)
36:endfor
37:\(\alpha_{0}\leftarrow\alpha_{1}\), \(\alpha_{1}\gets 0.5\times(1+\sqrt{1+4\times\alpha_{0}^{2}})\)
38:endfor
39:return\(V\) ```

**Algorithm 1** The Enhanced Nesterov's Accelerated Gradient method experiments, the time to calculate the \(\bar{B}\) in quadratic gradient \(G\) before running the iterations and the time to run each iteration for various algorithms are negligible (few seconds).

**Results Analysis In Figure 0(a), the enhanced Adagrad algorithm failed to outperform the original Adagrad algorithm. The possible reason for that might be related to the limitations of the raw Adagrad method. Without a doubt, Adagrad is a novel algorithm initiated to accelerate each element of the gradient with different learning rates. However, Adagrad tends to converge to a suboptimal solution due to its aggressive, monotonically decreasing learning rates. This would lead to its main limitation that in the later training phase every learning rate for different components of the gradient is too close to zero due to keeping adding positive additional terms to the denominator, stopping the algorithm from learning anything.**

**On the other hand, the original Adagrad method has another little-noticed limitation: the learning rate in the first few iterations tends to be large. While this limitation does not affect the performance of the original Adagrad method to some extent, the enhanced Adagrad method exacerbates this phenomenon by a factor of about \(10^{2}\cdot\frac{\varepsilon+\sqrt{\sum_{t=1}^{t}g_{(t)}^{(t)}g_{(t)}^{(t)}}}{ \varepsilon+\sqrt{\sum_{t=1}^{t}g_{(t)}^{(t)}g_{(t)}^{(t)}}}\), leading to the Learning-Rate Explosion. Therefore, the enhanced Adagrad [7] cannot be applied to general optimization problems such as Rosenbrock's function. The exploding learning rate would be too large for the algorithm to survive the first several iterations, finally leading the optimization function to some point where its output cannot be represented by the computer system. This might explain why the performance of this algorithm in all cases, not just on the iDASH genome dataset, seems to be meaningless, numerically unstable, and fluctuates in the first few iterations.**

**Several improved algorithms upon the Adagrad method, such as RMSProp, have been proposed in order to address these issues existed, via using an exponential moving average of historical gradients rather than just the sum of all squared gradients from the beginning of training. We might be able to overcome the problems existing in the enhanced Adagrad method by adopting the enhanced Adagrad-like variants, like the enhanced Adadelta method and the enhanced RMSProp method. One research work that could confirm this hypothesis is the enhanced Adam method [7].**

Figure 1: Training results in the clear for Adagrad and Enhanced Adagrad

## 4 Privacy-preserving LR Training

Adagrad method is not a practical solution for homomorphic LR due to its frequent inversion operations. It seems plausible that the enhanced NAG is probably the best choice for privacy-preserving LR training. We adopt the enhanced NAG method to implement privacy-preserving logistic regression training. The difficulty in applying the quadratic gradient is to invert the diagonal matrix \(\bar{B}\) in order to obtain \(\bar{B}\). We leave the computation of matrix \(\bar{B}\) to data owner and let the data owner upload the ciphertext encrypting the \(\bar{B}\) to the cloud. Since data owner has to prepare the dataset and normalize it, it would also be practicable for the data owner to calculate the \(\bar{B}\) owing to no leaking of sensitive data information.

Privacy-preserving logistic regression training based on HE techniques faces a difficult dilemma that no homomorphic schemes are capable of directly calculating the sigmoid function in the LR model. A common solution is to replace the sigmoid function with a polynomial approximation by using the widely adopted least-squares method. We can call a function named " polyfit(\(\cdot\)) " in the Python package Numpy to fit the polynomial in a least-square sense. We adopt the degree 5 polynomial approximation \(g(x)\) by which Kim et al. [12] used the least square approach to approximate the sigmoid function over the domain \([-8,8]\): \(g(x)=0.5+0.19131\cdot x-0.0045963\cdot x^{3}+0.0000412332\cdot x^{5}\).

Given the training dataset \(\mathbf{X}\in\mathbb{R}^{n\times(1+d)}\) and training label \(\mathbf{Y}\in\mathbb{R}^{n\times 1}\), we adopt the same method that Kim et al. [12] used to encrypt the data matrix consisting of the training data combined with training-label information into a single ciphertext \(\text{ct}_{Z}\). The weight vector \(\beta^{(0)}\) consisting of zeros and the diagnoal elements of \(\bar{B}\) are copied \(n\) times to form two matrices. The data owner then encrypt the two matrices into two ciphertexts \(\text{ct}^{(0)}_{\beta}\) and \(\text{ct}_{\bar{B}}\), respectively.

The public cloud takes the three ciphertexts \(\text{ct}_{Z}\), \(\text{ct}^{(0)}_{\beta}\) and \(\text{ct}_{\bar{B}}\) and evaluates the enhanced NAG algorithm to find a decent weight vector by updating the vector \(\text{ct}^{(0)}_{\beta}\). Refer to [12] for a detailed description about how to calculate the gradient by HE programming.

Figure 2: Training results in the clear for NAG and Enhanced NAGExperiments

ImplementationWe implement the enhanced NAG based on HE with the library HEAN. The C++ source code is publicly available at https://anonymous.4open.science/r/IDASH2017-245B. All the experiments on the ciphertexts were conducted on a public cloud with 32 vCPUs and 64 GB RAM.

For a fair comparison with [12], we utilized the same 10-fold cross-validation (CV) technique on the same iDASH dataset consisting of 1579 samples with 18 features and the same 5-fold CV technique on the other five datasets. Like [12], We consider the average accuracy and the Area Under the Curve (AUC) as the main indicators. Tables 1 and 2 show the two experiment results, respectively. The two tables also provide the average evaluation running time for each iteration and the storage (encrypted dataset for the baseline work and encrypted dataset and \(\bar{B}\) for our method). We adopt the same packing method that Kim et al. [12] proposed and hence our solution has similar storage of ciphertexts to [12] with some extra ciphertexts to encrypt the \(\bar{B}\).

The parameters of HEAN we set are same to [12]: \(logN=16\), \(logQ=1200\), \(logp=30\), \(slots=32768\), which ensure the security level \(\lambda=80\). Refer [12] for the details of these parameters. Since our enhanced NAG method need to consume more modulus to preserve the precision of \(\bar{B}\), we use \(logp=60\) to encrypt the matrix \(\bar{B}\) and thus only can perform \(3\) iterations of the enhanced NAG method. Yet despite only \(3\) iterations, our enhanced NAG method still produces a comparable result.

## 6 Conclusion

In this paper, we proposed a faster gradient variant called quadratic gradient, and implemented the quadratic-gradient version of NAG in the encrypted domain to train the logistic regression model.

The quadratic gradient presented in this work can be constructed from the Hessian matrix directly, and thus somehow combines the second-order Newton's method and the first-order gradient (descent) method together. There is a good chance that quadratic gradient could accelerate other gradient methods such as Adagrad, Adadelta, RMSprop, Adam [8], AdaMax and Nadam, which is an open future work.

Also, quadratic gradient might substitute and supersede the line-search method, for example when using enhanced Adagrad-like methods, and could use gradient descent methods to accelerate Newton's method, resulting in super-quadratic algorithms.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline Dataset & Sample Num & Feature Num & Method & deg \(g\) & Iter Num & Storage (GB) & Learn Time (min) & Accuracy (\%) & AUC \\ \hline \multirow{2}{*}{iDASH} & \multirow{2}{*}{1579} & \multirow{2}{*}{18} & Ours & 5 & 3 & 0.08 & 3.61 & 53.38 & 0.681 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.04 & 6.07 & 62.87 & 0.689 \\ \hline \end{tabular}
\end{table}
Table 1: Implementation Results for iDASH datasets with 10-fold CV

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline Dataset & Sample Num & Feature Num & Method & deg \(g\) & Iter Num & Storage (GB) & Learn Time (min) & Accuracy (\%) & AUC \\ \hline \multirow{2}{*}{Edinburgh} & \multirow{2}{*}{1253} & \multirow{2}{*}{9} & Ours & 5 & 3 & 0.04 & 0.5 & 84.40 & 0.847 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.02 & 3.6 & 91.04 & 0.958 \\ \hline \multirow{2}{*}{lbw} & \multirow{2}{*}{189} & \multirow{2}{*}{9} & Ours & 5 & 3 & 0.04 & 0.4 & 68.65 & 0.635 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.02 & 3.3 & 69.19 & 0.689 \\ \hline \multirow{2}{*}{nhanes3} & \multirow{2}{*}{15649} & \multirow{2}{*}{15} & Ours & 5 & 3 & 0.31 & 3.7 & 79.22 & 0.490 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.16 & 7.3 & 79.22 & 0.717 \\ \hline \multirow{2}{*}{pcs} & \multirow{2}{*}{379} & \multirow{2}{*}{9} & Ours & 5 & 3 & 0.04 & 0.6 & 64.00 & 0.720 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.02 & 3.5 & 68.27 & 0.740 \\ \hline \multirow{2}{*}{uis} & \multirow{2}{*}{575} & \multirow{2}{*}{8} & Ours & 5 & 3 & 0.04 & 0.5 & 74.43 & 0.585 \\ \cline{3-10}  & & & [12] & 5 & 7 & 0.02 & 3.5 & 74.44 & 0.603 \\ \hline \end{tabular}
\end{table}
Table 2: Implementation Results for other datasets with 5-fold CV

## References

* Allison (2008) Allison, P. D. (2008). Convergence failures in logistic regression.
* Blatt et al. (2019) Blatt, M., Gusev, A., Polyakov, Y., Rohloff, K., and Vaikuntanathan, V. (2019). Optimized homomorphic encryption solution for secure genome-wide association studies. _IACR Cryptology ePrint Archive_, 2019:223.
* Bohning and Lindsay (1988) Bohning, D. and Lindsay, B. G. (1988). Monotonicity of quadratic-approximation algorithms. _Annals of the Institute of Statistical Mathematics_, 40(4):641-663.
* Bonte and Vercauteren (2018) Bonte, C. and Vercauteren, F. (2018). Privacy-preserving logistic regression training. _BMC medical genomics_, 11(4):86.
* Chen et al. (2018) Chen, H., Gilad-Bachrach, R., Han, K., Huang, Z., Jalali, A., Laine, K., and Lauter, K. (2018). Logistic regression over encrypted data from fully homomorphic encryption. _BMC medical genomics_, 11(4):3-12.
* Cheon et al. (2017) Cheon, J. H., Kim, A., Kim, M., and Song, Y. (2017). Homomorphic encryption for arithmetic of approximate numbers. In _International Conference on the Theory and Application of Cryptology and Information Security_, pages 409-437. Springer.
* Chiang (2022) Chiang, J. (2022). Quadratic gradient: Uniting gradient algorithm and newton method as one. _arXiv preprint arXiv:2209.03282_.
* Crawford et al. (2018) Crawford, J. L., Gentry, C., Halevi, S., Platt, D., and Shoup, V. (2018). Doing real work with fhe: the case of logistic regression. In _Proceedings of the 6th Workshop on Encrypted Computing & Applied Homomorphic Cryptography_, pages 1-12.
* Gentry (2009) Gentry, C. (2009). Fully homomorphic encryption using ideal lattices. In _Proceedings of the forty-first annual ACM symposium on Theory of computing_, pages 169-178.
* Han et al. (2019) Han, K., Hong, S., Cheon, J. H., and Park, D. (2019). Logistic regression on homomorphic encrypted data at scale. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 9466-9471.
* Jaschke and Armknecht (2016) Jaschke, A. and Armknecht, F. (2016). Accelerating homomorphic computations on rational numbers. In _International Conference on Applied Cryptography and Network Security_, pages 405-423. Springer.
* Kim et al. (2018a) Kim, A., Song, Y., Kim, M., Lee, K., and Cheon, J. H. (2018a). Logistic regression model training based on the approximate homomorphic encryption. _BMC medical genomics_, 11(4):83.
* Kim et al. (2019) Kim, M., Song, Y., Li, B., and Micciancio, D. (2019). Semi-parallel logistic regression for gwas on encrypted data. _IACR Cryptology ePrint Archive_, 2019:294.
* Kim et al. (2018b) Kim, M., Song, Y., Wang, S., Xia, Y., and Jiang, X. (2018b). Secure logistic regression based on homomorphic encryption: Design and evaluation. _JMIR medical informatics_, 6(2):e19.
* Murphy (2012) Murphy, K. P. (2012). _Machine learning: a probabilistic perspective_. The MIT Press, Cambridge, MA.