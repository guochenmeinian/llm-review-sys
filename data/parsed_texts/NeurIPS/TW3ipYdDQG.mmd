# Fair Streaming Principal Component Analysis:

Statistical and Algorithmic Viewpoint

 Junghyun Lee1  Hanseul Cho1  Se-Young Yun  Chulhee Yun

Kim Jachul Graduate School of AI, KAIST

{jh_lee00, jhs4015, yunseyoung, chulhee.yun}@kaist.ac.kr

Equal contributions

Footnote 1: footnotemark:

###### Abstract

Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, conditional on the sensitive attributes, match one another. However, existing approaches to fair PCA have two main problems: theoretically, there has been no statistical foundation of fair PCA in terms of learnability; practically, limited memory prevents us from using existing approaches, as they explicitly rely on full access to the entire data. On the theoretical side, we rigorously formulate fair PCA using a new notion called _probably approximately fair and optimal_ (PAFO) learnability. On the practical side, motivated by recent advances in streaming algorithms for addressing memory limitation, we propose a new setting called _fair streaming PCA_ along with a memory-efficient algorithm, fair noisy power method (FNPM). We then provide its _statistical_ guarantee in terms of PAFO-learnability, which is the first of its kind in fair PCA literature. Lastly, we verify the efficacy and memory efficiency of our algorithm on real-world datasets.

## 1 Introduction

Algorithmic fairness ensures that machine learning algorithms do not propagate nor exacerbate bias, which may lead to discriminatory decision-making (Barocas and Selbst, 2016) and thus has been a very active area of research. This has direct implications in our everyday life, including but not limited to criminal justice (Kirchner et al., 2016), education (Kizilec and Lee, 2021), and more. See Mehrabi et al. (2021) for a comprehensive survey of bias and fairness in machine learning.

Often, one needs to consider fairness for a large number of high-dimensional data points. One of the standard tools for dealing with such high-dimensional data is PCA (Hotelling, 1933; Pearson, 1901), a classical yet still one of the most popular algorithms for performing interpretable dimensionality reduction. It has been adapted as a baseline and/or standard tool in exploratory data analysis, whose application ranges from natural sciences, engineering (Abdi and Williams, 2010; Jolliffe and Cadima, 2016), and even explainable AI (Li et al., 2023; Tjoa and Guan, 2021). Due to its ubiquity and wide applicability, several works study defining fairness in PCA and developing a fair variant of it. A recent line of research (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019) defines PCA fairness in the context of fair representation (Zemel et al., 2013) in that the projected group conditional distributions should match.

However, existing fair PCA approaches suffer from two problems. Theoretically, they provide no statistical foundation of fair PCA or guarantees for their algorithms. By statistical foundation, we mean the usual PAC-learnability (Shalev-Schwartz and Ben-David, 2014) guarantees, e.g., sample complexity for ensuring optimality in explained variance and fairness constraint with high probability.

On top of that, the second problem arises from a practical viewpoint: memory limitation. All the aforementioned fair PCA algorithms assume that the learner can store the entire data points and incurs memory complexity of order at least \(\mathcal{O}(d\max(N,d))\), where \(d\) is the dimensionality of the data and \(N\) is the number of data points. As memory limitation is often a critical bottleneck in deploying machine learning algorithms (Mitliagkas et al., 2013), as much as fairness is important, it is also paramount that imposing fairness to PCA does not incur too much memory overhead. A popular approach to mitigate such memory limitation for PCA is to consider the one-pass, streaming setting. In this setting, each data point is revealed to the learner sequentially, each point is irretrievably gone unless she explicitly stores it, and she can use only \(\mathcal{O}(dk)\) memory, with \(k\) being the target dimension of projection. Indeed, without the fairness constraint, streaming PCA has been studied extensively; see Balzano et al. (2018) and references therein.

In this work, we address both problems in a principled manner. Our contributions are as follows:

* We provide an alternative formulation of fair PCA based on the "Null It Out" approach (Section 3). Based on the new formulation, we introduce the concept of _probably approximately fair and optimal_ (PAFO)-learnability to formalize the problem of fair PCA (Section 4).
* To address the memory limitation, we propose a new problem setting called _fair streaming PCA_, as well as _fair noisy power method_ (FNPM), a simple yet memory-efficient algorithm based on the noisy power method. (Section 5). We note that our algorithm incurs a much lower memory complexity even compared to the most efficient variant of fair PCA proposed by Kleindessner et al. (2023).
* We then prove that our algorithm achieves the PAFO-learnability for fair streaming PCA (Section 6). Such statistical guarantee is the first of its kind in fair PCA literature.
* Lastly, we empirically validate our algorithm on CelebA and UCI datasets. Notably, we run FNPM on the original _full-resolution_ CelebA dataset on which existing fair PCA algorithms fail due to high memory requirements. It shows turning such a non-streaming setting into a streaming setting and applying our algorithm allows one to bypass the memory limitation (Section 7).

## 2 Preliminaries

Notation.For \(\ell\geq 1\), let \(\bm{I}_{\ell}\) be the identity matrix of size \(\ell\times\ell\). For \(k<d\), we bring the _Stiefel manifold_\(\mathrm{St}(d,k)=\{\bm{A}\in\mathbb{R}^{d\times k}:\bm{A}^{\intercal}\bm{A}= \bm{I}_{k}\}\), which is the collection of all rank-\(k\) semi-orthogonal matrices. We denote an orthonormal column basis of a (full column rank) matrix \(\bm{M}\in\mathbb{R}^{d\times k}\) obtained by QR decomposition as \(\texttt{QR}(\bm{M})\in\mathrm{St}(d,k)\) and denote its column space by \(\mathrm{col}(\bm{M})\). Also, for \(\bm{A}\in\mathrm{St}(d,k)\), we denote the orthogonal projection matrix to \(\mathrm{col}(\bm{A})^{\perp}=\mathrm{null}(\bm{A}^{\intercal})\) as \(\bm{\Pi}_{\bm{A}}^{\perp}=\bm{I}_{d}-\bm{A}\bm{A}^{\intercal}=\bm{I}_{d}-\bm{ \Pi}_{\bm{A}}\). Moreover, we denote the collection of all possible \(d\)-dimensional probability distributions as \(\mathcal{P}_{d}\). For a zero-mean random matrix \(\bm{Z}\), its (scalar-valued) variance is defined as \(\mathrm{Var}(\bm{Z})=\max\left(\left\|\mathbb{E}\left[\bm{Z}\bm{Z}^{\intercal }\right]\right\|_{2},\left\|\mathbb{E}\left[\bm{Z}^{\intercal}\bm{Z}\right] \right\|_{2}\right).\) In general, \(\mathrm{Var}(\bm{Z})=\mathrm{Var}(\bm{Z}-\mathbb{E}[\bm{Z}])\). Lastly, we use the usual \(\mathcal{O}\), \(\Omega\), and \(\Theta\) notations for asymptotic analyses, where tildes (\(\tilde{\mathcal{O}}\), \(\tilde{\Omega}\), and \(\tilde{\Theta}\), resp.) are used for hiding logarithmic factors.

Setup.Assume that the sensitive attribute variable, which we will be imposing fairness, is binary, denoted by \(a\in\{0,1\}\). For each group \(a\), let \(\mathcal{D}_{a}\) be a \(d\)-dimensional distribution of mean \(\bm{\mu}_{a}\) and covariance \(\bm{\Sigma}_{a}\), both of which are assumed to be well-defined. We often call them group-conditional mean and covariance, respectively. With a fixed, _unknown_ mixture parameter \(p\in(0,1)\), let us denote the total data distribution as \(\mathcal{D}:=p\mathcal{D}_{0}+(1-p)\mathcal{D}_{1}\). Equivalently, the sensitive attribute follows \(a\sim\mathrm{Bernoulli}(p)\), and the conditional random variable \(\bm{x}|a\) is sampled from \(\mathcal{D}_{a}\). In that case, \(\bm{\mu}_{a}=\mathbb{E}[\bm{x}|a]\) and \(\bm{\Sigma}_{a}=\mathbb{E}[\bm{x}\bm{x}^{\intercal}|a]-\bm{\mu}_{a}\bm{\mu}_{a }^{\intercal}\). We often write \(p_{0}=1-p\) and \(p_{1}=p\) for brevity. We also define the mean difference \(\bm{f}:=\bm{\mu}_{1}-\bm{\mu}_{0}\) and the second moment difference \(\bm{S}:=\mathbb{E}[\bm{x}\bm{x}^{\intercal}|a]=1]-\mathbb{E}[\bm{x}\bm{x}^{ \intercal}|a=0]=\bm{\Sigma}_{1}-\bm{\Sigma}_{0}+\bm{\mu}_{1}\bm{\mu}_{1}^{ \intercal}-\bm{\mu}_{0}\bm{\mu}_{0}^{\intercal}\). Accordingly, denote the true mean and covariance of \(\mathcal{D}\) as \(\bm{\mu}\) and \(\bm{\Sigma}\), respectively. For simplicity, let us assume that \(\mathcal{D}\) is centered, i.e., \(\bm{\mu}=\bm{0}\); note that this does _not_ mean that the group conditional distributions \(\mathcal{D}_{a}\)'s are centered.

Pca.In the _offline_ setting, the full covariance matrix \(\bm{\Sigma}\) is given which is often a sample covariance matrix \(\frac{1}{n}\sum_{i=1}^{n}\bm{x}_{i}\bm{x}_{i}^{\top}\) for \(n\) data points \(\bm{x}_{1},\dots,\bm{x}_{n}\). The goal of vanilla (offline) PCA (Hotelling, 1933; Pearson, 1901) is to compute the loading matrix \(\bm{V}\in\mathbb{R}^{d\times k}\) that preserves as much variance as possible after projecting \(\bm{\Sigma}\) via \(\bm{V}\), _i.e._, maximize \(\mathrm{tr}(\bm{\Pi}_{\bm{V}}\bm{\Sigma})=\mathrm{tr}(\bm{V}^{\intercal}\bm{ \Sigma}\bm{V})\). Here, \(k<d\) is the target dimension to which the data's dimensionality \(d\) is to be reduced and is chosen by the learner. We additionally consider the constraint \(\bm{V}\in\mathrm{St}(d,k)\) to ensure that the resulting coordinate after the transformation is orthogonal and thus amenable to various statistical interpretations (Johnson and Wichern, 2008). Without any fairness constraint, Eckart-Young theorem (Eckart and Young, 1936) implies that the solution is characterized as a matrix whose columns are the top-\(k\) eigenvectors of \(\bm{\Sigma}\).

Fair PCA.Recently, it has been suggested that performing vanilla PCA on real-world datasets may exhibit bias, making the final outputted projection "unfair". As is often the case, there can be multiple definitions of fairness in PCA, but the following two are the most popular: equalizing reconstruction losses (Kamani et al., 2022; Samadi et al., 2018; Tantipongpipat et al., 2019; Vu et al., 2022), or equalizing the projected distributions (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019) from the perspective of fair representation (Zemel et al., 2013); we focus on the latter one.

## 3 An Alternative Approach to Fair PCA

### "Null It Out" Formulation of Fair PCA

In this work, we consider fair PCA as learning fair representation (Zemel et al., 2013). The goal is to preserve as much variance as possible while obfuscating any information regarding the sensitive attribute. To this end, we take the "Null It Out" approach as proposed in Ravfogel et al. (2020). Intuitively, we want to nullify the directions in which the sensitive attribute \(a\) can be inferred, and in this work, we consider two such **unfair directions**: mean difference \(\bm{f}\) and _eigenvectors_ of second moment difference \(\bm{S}\). To give the learner flexibility in choosing the trade-off between fairness and performance (measured in explained variance), let \(m\geq 1\) be the number of top eigenvectors of \(\bm{S}\) to nullify. Thus, the learner is nullifying at most \((m+1)\)-dimensional subspace that is unfair with respect to \(a\), which we refer to as the **unfair subspace**. Precisely, we formulate our fair PCA as follows:

\[\max_{\bm{V}\in\mathrm{St}(d,k)}\mathrm{tr}(\bm{V}^{\intercal}\bm{\Sigma}\bm{V }),\quad\text{subject to }\mathrm{col}(\bm{V})\subset\mathrm{col}([\bm{P}_{m}|\bm{f}])^{\perp},\] (1)

where \(d\) is the data dimensionality, \(k\) is the target dimension, and the columns of \(\bm{P}_{m}\in\mathrm{St}(d,m)\) is top-\(m\) orthonormal eigenvectors of \(\bm{S}\).

### An Explicit Characterization for Solution of Fair PCA

To first construct the unfair subspace that is spanned by \(\bm{f}\) as well as \(\bm{P}_{m}\), let us define \(\bm{U}\in\mathrm{St}(d,m^{\prime})\) to be the orthogonal matrix whose columns form a basis of \(\mathrm{col}([\bm{P}_{m}|\bm{f}])\). Then, \(\bm{U}\) has a closed form as follows: \(m^{\prime}=m\) if \(\bm{f}\in\mathrm{col}(\bm{P}_{m})\) and \(m^{\prime}=m+1\) otherwise, and

\[\bm{U}=\begin{cases}\bm{P}_{m},&\text{if }\bm{f}\in\mathrm{col}(\bm{P}_{m}) \text{,}\\ \mathtt{QR}([\bm{P}_{m}|\bm{f}])=\left[\bm{P}_{m}\left|\begin{array}{c}\bm{ g}\\ \left\|\bm{g}\right\|_{2}\end{array}\right.\right],&\text{otherwise,}\end{cases}\] (2)

where \(\bm{g}=\bm{\Pi}_{\bm{P}_{m}}^{\perp}\bm{f}\in\mathrm{col}(\bm{P}_{m})^{\perp}\). Note that \(\bm{g}\) is a vector in a direction that \(\bm{f}\) is projected onto \(\mathrm{col}(\bm{P}_{m})^{\perp}=\mathrm{null}(\bm{P}_{m}^{\intercal})\). For this \(\bm{U}\), our constraint in (1) can be interpreted as an equivalent nullity constraint \(\bm{\Pi}_{\bm{U}}\bm{V}=\bm{0}\):

\[\max_{\bm{V}\in\mathrm{St}(d,k)}\mathrm{tr}(\bm{V}^{\intercal}\bm{\Sigma}\bm{ V}),\quad\text{subject to }\bm{\Pi}_{\bm{U}}\bm{V}=\bm{0}.\] (3)

The above is equivalent to the following problem without any constraint other than semi-orthgonality:

\[\max_{\bm{V}\in\mathrm{St}(d,k)}\mathrm{tr}\left(\bm{V}^{\intercal}\bm{\Pi}_ {\bm{U}}^{\perp}\bm{\Sigma}\bm{\Pi}_{\bm{U}}^{\perp}\bm{V}\right),\] (4)

which is basically the vanilla \(k\)-PCA problem of a matrix \(\bm{\Pi}_{\bm{U}}^{\perp}\bm{\Sigma}\bm{\Pi}_{\bm{U}}^{\perp}\). Therefore, a top-\(k\) orthonormal column basis of this matrix is indeed a solution of our problem (4).

### Comparison to the Existing Covariance Matching Constraint

Previous works on fair PCA (Kleindessner et al., 2023; Olfat and Aswani, 2019) consider an exact covariance-matching constraint (namely, \(\bm{V}^{\intercal}(\bm{\Sigma}_{1}-\bm{\Sigma}_{0})\bm{V}=\bm{0}\)). In fact, this is equivalent to the condition \(\bm{V}^{\intercal}\bm{S}\bm{V}=\bm{0}\) under the mean-matching constraint \(\bm{f}^{\intercal}\bm{V}=\bm{0}\), which can be derived as \[\bm{V}^{\intercal}(\bm{\Sigma}_{1}-\bm{\Sigma}_{0})\bm{V} =\bm{V}^{\intercal}\left(\mathbb{E}[\bm{x}\bm{x}^{\intercal}|a=1]- \mathbb{E}[\bm{x}\bm{x}^{\intercal}|a=0]\right)\bm{V}-\bm{V}^{\intercal}\left( \bm{\mu}_{1}\bm{\mu}_{1}^{\intercal}-\bm{\mu}_{0}\bm{\mu}_{0}^{\intercal} \right)\bm{V}\] \[=\bm{V}^{\intercal}\bm{S}\bm{V}-\bm{V}^{\intercal}\left(\bm{f} \bm{\mu}_{1}^{\intercal}+\bm{\mu}_{0}\bm{f}^{\intercal}\right)\bm{V}=\bm{V}^ {\intercal}\bm{S}\bm{V}=\bm{0}.\]

One immediate problem with this is that the constraint may be infeasible depending on the choice of \(\bm{\Sigma}_{0}\), \(\bm{\Sigma}_{1}\), or \(\bm{S}\); e.g., when \(\bm{\Sigma}_{1}-\bm{\Sigma}_{0}\) is positive definite. For this reason, Kleindessner et al. (2023); Olfat and Aswani (2019) propose relaxations of the fairness constraints but provide no further discussions on its impact on statistical guarantees. On the contrary, our formulation is always feasible without any need for relaxation, allowing us to consider a rigorous definition of fair PCA (Definition 2) for the first time in fair PCA literature.

## 4 Statistical Viewpoint: PAFO-Learnability of PCA

As all the distribution statistics (\(\bm{\Sigma},p,\cdots\)) are unknown, the learner, given some finite number of samples, must learn all of them _and_ solve fair PCA. In supervised learning, such a problem is often formalized in a PAC-learnability framework (Shalev-Schwartz and Ben-David, 2014). In the context of PAC-learnability for unsupervised learning settings, TV-learning, which is the task of learning distribution, has been mainly considered so far (Ananthakrishnan et al., 2021; Hopkins et al., 2023). However, unlike TV-learning, it is unnecessary to learn the whole distribution in fair PCA; moreover, fair PCA has the fairness constraint \(\bm{\Pi}_{\bm{U}}\bm{V}=\bm{0}\) to be satisfied. Inspired by the unsupervised PAC-learnability as well as constrained PAC-learnability (Chamon and Ribeiro, 2020), we propose a new notion of learnability for fair PCA, called _PAFO (Probably Approximately Fair and Optimal) learnability_, as follows:

**Definition 1** (Projection Learner).: \(A\) **projection learner** _is a function that takes \(k\geq 1\) and \(d\)-dimensional samples as input and outputs a loading matrix \(\bm{V}\in\operatorname{St}(d,k)\)._

**Definition 2** (PAFO-Learnability of PCA).: _Let \(d,k,m\) be integers such that \(1\leq k<d\) and \(m<d\). We say that \(\mathcal{F}_{d}\subset\mathcal{P}_{d}\times\mathcal{P}_{d}\times(0,1)\) is_ **PAFO-learnable for PCA** _if there exists a function \(N_{\mathcal{F}_{d}}:(0,1)^{3}\to\mathbb{N}\) and a projection learner \(\mathcal{A}\) satisfying the following:_

_For every \(\varepsilon_{\mathrm{o}},\varepsilon_{\mathrm{f}}\), \(\delta\in(0,1)\) and \((\mathcal{D}_{0},\mathcal{D}_{1},p)\in\mathcal{F}_{d}\), when running \(\mathcal{A}\) on \(N\geq N_{\mathcal{F}_{d}}(\varepsilon_{\mathrm{o}},\varepsilon_{\mathrm{f}},\delta)\) i.i.d. samples from \(\mathcal{D}:=p\mathcal{D}_{1}+(1-p)\mathcal{D}_{0}\) of the form \((a,\bm{x})\), \(\mathcal{A}\) returns \(\bm{V}\) s.t., with probability at least \(1-\delta\) (over the draws of the \(N\) samples),_

\[\operatorname{tr}\left(\bm{V}^{\intercal}\bm{\Sigma}\bm{V}\right)\geq \operatorname{tr}\left(\bm{V}_{\star}^{\intercal}\bm{\Sigma}\bm{V}_{\star} \right)-\varepsilon_{\mathrm{o}},\quad\|\bm{\Pi}_{\bm{U}}\bm{V}\|_{2}\leq \varepsilon_{\mathrm{f}},\] (5)

_where \(\bm{U}\) is as defined in Eqn. (2) and \(\bm{V}_{\star}\) is any solution to Eqn. (4) (with prescribed \(k\) and \(m\))._

Like in the usual PAC-learnability, \(N_{\mathcal{F}_{d}}\) is referred to as the _sample complexity_ of fair PCA. Observe how the optimality is measured w.r.t. the optimal solution of _fair_ PCA, not the vanilla PCA. Also, the two conditions are not overlapping: vanilla PCA (overly) satisfies \(\varepsilon_{\mathrm{o}}\)-optimality in explained variance but does not satisfy \(\varepsilon_{\mathrm{f}}\)-optimality in fairness, and vice versa for a poorly chosen \(\bm{V}\in St(d,k)\) with \(\operatorname{col}(\bm{V})\subseteq\operatorname{col}([\bm{P}_{m}|\bm{f}])^{\perp}\).

## 5 Algorithmic Viewpoint: Fair Streaming PCA

We now introduce a new problem setting, _fair streaming PCA_. In this setting, the learner receives a stream of pairs \((a_{t},\bm{x}_{t})\in\{0,1\}\times\mathbb{R}^{d}\) sequentially. Note that the sensitive attribute information \(a_{t}\) is also available at each time-step; this is commonly assumed when considering fairness in streaming setting (Bera et al., 2022; El Halabi et al., 2020). Precisely, we assume the following model of the data generation process: at each time-step \(t\), a sensitive attribute is chosen as \(a_{t}\sim\operatorname{Bernoulli}(p)\), then the data is sampled from the corresponding sensitive group's conditional distribution \(\bm{x}_{t}\mid a_{t}\sim\mathcal{D}_{s_{t}}\). Importantly, as done in previous streaming PCA literature (Mitliagkas et al., 2013), we assume that the learner has only \(\mathcal{O}(dk)\) memory, where \(d\) is the data dimension and \(k\) is the target dimension. We can formally define the PAFO-learnability in this streaming setting:

**Definition 3**.: _We say that \(\mathcal{F}_{d}\subseteq\mathcal{P}_{d}\times\mathcal{P}_{d}\times(0,1)\) is_ **PAFO-learnable for streaming PCA** _if the projection learner \(\mathcal{A}\) for which Definition 2 holds uses only \(\mathcal{O}(dk)\) memory for streaming data._

### Our Algorithm: Fair Noisy Power Method (FNPM)

One only needs to estimate \(\bm{U}\) to use the off-the-shelf streaming PCA algorithm. As \(\bm{U}\) is of size \(d\times m\), storing its estimate is no problem for the memory constraint as long as \(m=\mathcal{O}(k)\). Naturally, we proceed via a two-stage approach; first, estimate \(\bm{U}\) sufficiently well, then with the fixed estimate of \(\bm{U}\), apply the noisy power method (Hardt and Price, 2014; Mitliagkas et al., 2013) for \(\bm{V}\).

For estimating \(\bm{U}\), one needs to estimate \(\bm{f}\) and \(\bm{P}_{m}\). Estimating \(\bm{f}\) can be done using the usual cumulative averaging. As for \(\bm{P}_{m}\), we can consider the two main approaches for streaming PCA: Oja's method (Huang et al., 2021; Oja, 1982; Oja and Karhunen, 1985) and noisy power method (NPM) (Hardt and Price, 2014; Mitliagkas et al., 2013). We first show that Oja's method is _inapplicable_ for our purpose, as it may ignore some eigenvectors corresponding to negative (but large in magnitude) eigenvalues of \(\bm{S}\). For instance, if \(\bm{S}=-2\bm{e}_{1}\bm{e}_{1}^{\intercal}+\bm{e}_{2}\bm{e}_{2}^{\intercal}+4 \bm{e}_{3}\bm{e}_{3}^{\intercal}\) with \(\bm{e}_{i}\) being the standard basis vectors, then Oja's method with \(m=2\) would yield \(|\bm{e}_{2}|\bm{e}_{3}|\) when we actually want \([\bm{e}_{1}|\bm{e}_{3}]\). For the same reason, simply shifting the eigenvalue spectrum by considering \(\bm{S}+\|\bm{S}\|_{2}\bm{I}\) does not work. Thus we apply NPM for estimating \(\bm{P}_{m}\) in our case, which is known to converge as long as the singular value gap of \(\bm{P}_{m}\) is large enough and norms of the noise matrices at each iterate are properly bounded (Balcan et al., 2016; Hardt and Price, 2014).

```
1Input:\(m\), Block size \(b\), Number of iterations \(T\);
2Output: A matrix \(\bm{\hat{U}}\) with orthonormal columns;
3\(\bm{W}_{0}=\texttt{QR}(\mathcal{N}(0,1)^{d\times m})\);
4\((\overline{\bm{m}}^{(0)},\overline{\bm{m}}^{(1)},B^{(0)},B^{(1)})=(\bm{0}_{d}, \bm{0}_{d},0,0)\);
5for\(t\in[T]\)do
6 Receive \(\{(a_{i},\bm{x}_{i})\}_{i=(t-1)b+1}^{tb}\);
7foreach\(a\in\{0,1\}\)do
8 Compute \(b_{t}^{(a)},\bm{m}_{t}^{(a)},\bm{C}_{t}^{(a)}\) as Eqn. (6);
9\(\overline{\bm{m}}^{(a)}\leftarrow\frac{B^{(a)}}{B^{(a)}+b_{t}^{(a)}}\overline{ \bm{m}}^{(a)}+\frac{b_{t}^{(a)}}{B^{(a)}+b_{t}^{(a)}}\bm{m}_{t}^{(a)}\);
10\(B^{(a)}\gets B^{(a)}+b_{t}^{(a)}\);
11\(\bm{W}_{t}=\texttt{QR}\left(\bm{C}_{t}^{(1)}-\bm{C}_{t}^{(0)}\right)\);
12\(\widehat{\bm{f}}\leftarrow\overline{\bm{m}}^{(1)}-\overline{\bm{m}}^{(0)}\);
13\(\widehat{\bm{g}}\leftarrow\widehat{\bm{f}}-\bm{W}_{T}\bm{W}_{T}^{\intercal} \widehat{\bm{f}}\);
14if\(\|\widehat{\bm{g}}\|_{2}=0\)then
15\(\widehat{\bm{U}}=\bm{W}_{T}\)
16else
17\(\widehat{\bm{U}}=\left[\bm{W}_{T}\mid\frac{\bar{\bm{g}}}{\|\bm{g}\|_{2}}\right]\) return\(\widehat{\bm{U}}\) ```

**Algorithm 1**UnfairSubspace

Description of the algorithms.The pseudocode of our algorithm is shown in Algorithms 1 and 2. The goal of Algorithm 1 is to estimate \(\bm{U}=\left[\bm{P}_{m}\mid\frac{\bm{g}}{\|\bm{g}\|_{2}}\right]\) in Eqn. (2) as accurately as possible. Lines 5-13 do the estimation of \(\bm{P}_{m}\) and \(\bm{g}=\bm{\Pi}_{\bm{P}_{m}}^{\perp}\bm{f}\); line 11 is the NPM to find \(\bm{P}_{m}\), lines 12 and 13 are the estimation of \(\bm{f}\) and \(\bm{g}\) respectively, and line 17 is the concatenation of the estimates of \(\bm{P}_{m}\) and \(\bm{g}/\|\bm{g}\|_{2}\). Especially at line 17, the algorithm determines whether to incorporate mean difference by checking \(\hat{\bm{g}}\), which can be proved to be correct, _i.e._, \(\bm{g}=\bm{0}\) if and only if \(\hat{\bm{g}}=\bm{0}\) with high probability. With the estimated \(\bm{U}\) from Algorithm 1, Algorithm 2 performs the usual NPM on \(\bm{\Pi}_{\bm{U}}^{\perp}\bm{\Sigma}\bm{\Pi}_{\bm{U}}^{\perp}\), as in Eqn. (4). The memory complexity of Algorithm 2 is \(\mathcal{O}(d\max(m,k))\), since we do not have to store all \(b\) or \(\mathcal{B}\) data points at each time, and all the operations used can be implemented in a manner that conforms to the memory limitation; the full pseudocodes are provided in Appendix B.

At time step \(t\) of Algorithm 1, for each \(a\in\{0,1\}\), \(b_{t}^{(a)}\) is the number of data points \(\bm{x}_{i}\)'s such that \(a_{i}=a\); \(\bm{m}_{t}^{(a)}\) is the term used for estimation of the group-wise sample mean of \(\bm{x}_{i}\)'s; \(\bm{C}_{t}^{(a)}\) is used for the group-wise sample second moment. Their forms are as follows and can be computed

[MISSING_PAGE_EMPTY:6]

**Assumption 1**.: _Consider our data generation process \(a\sim\mathrm{Bernoulli}(p)\) and \(\bm{x}\mid a\sim\mathcal{D}_{a}\). There exists \(\sigma,V,\mathcal{M},\mathcal{V},\mathcal{y}_{\mathrm{min}}>0\), \(f_{\mathrm{max}}\in(g_{\mathrm{min}},\infty)\), and \(p_{\mathrm{min}}\in(0,0.5]\) such that the followings hold for any \((\mathcal{D}_{0},\mathcal{D}_{1},p)\in\mathcal{F}_{d}\): for \(a\in\{0,1\}\), \(\mathcal{D}_{a}\in\mathrm{nSG}(\sigma)\),2_

Footnote 2: \(\bm{y}\) is norm-sub-Gaussian, denoted as \(\mathrm{nSG}(\sigma)\), if \(\mathbb{P}[\|\bm{y}-\mathbb{E}\bm{y}\|\geq t]\leq 2e^{-\frac{t^{2}}{2\sigma^{2}}}\). We refer interested readers to Jin et al. (2019, Section 2) for more discussions on norm-sub-Gaussianity.

\[\mathbb{P}\left[\left\|\bm{x}\bm{x}^{\intercal}-(\bm{\Sigma}_{a}+\bm{\mu}_{a} \bm{\mu}_{a}^{\intercal})\right\|_{2}\leq\mathcal{M}\mid a]=1,\ \ \left\|\bm{\Sigma}_{a}+\bm{\mu}_{a}\bm{\mu}_{a}^{ \intercal}\right\|_{2}\leq V,\ \ \mathrm{Var}\left(\bm{x}\bm{x}^{\intercal}\mid a \right)\leq\mathcal{V},\]

_and_

\[\left\|\bm{g}\right\|_{2}=\left\|\bm{\Pi}_{\bm{P}_{m}}^{\perp}\bm{f}\right\|_{ 2}\in\{0\}\cup[g_{\mathrm{min}},f_{\mathrm{max}}],\ \ \left\|\bm{f}\right\|_{2}\leq f_{\mathrm{max}},\ \ \text{and}\ \ p\in[p_{\mathrm{min}},1-p_{\mathrm{min}}].\]

**Assumption 2**.: _Fix \(m,k\in\mathbb{N}\). There exist \(\Delta_{m,\nu},\Delta_{k,\kappa},K_{m,\nu},K_{k,\kappa}\in(0,\infty)\) such that for any \((\mathcal{D}_{0},\mathcal{D}_{1},\cdot)\in\mathcal{F}_{d}\), the followings hold:_

\[\nu_{m}-\nu_{m+1}\geq\Delta_{m,\nu},\ \kappa_{k}-\kappa_{k+1}\geq\Delta_{k, \kappa},\ \nu_{m}\leq K_{m,\nu},\ \text{and}\ \kappa_{k}\leq K_{k,\kappa},\]

_where \(\nu_{1}\geq\cdots\geq\nu_{d}\geq 0\) and \(\kappa_{1}\geq\cdots\geq\kappa_{d}\geq 0\) are the singular values of \(\bm{S}\) and \(\bm{\Pi}_{\bm{U}}^{\perp}\bm{\Sigma}\bm{\Pi}_{\bm{U}}^{\perp}\), respectively._

We start by establishing the sample complexity bounds of Algorithms 1 and 2 based on the convergence bound for NPM by Hardt and Price (2014). Recall that NPM is an algorithm for finding top-\(r\) eigenspace (in magnitude) of a symmetric (but _not necessarily PSD_) matrix \(\bm{A}\) under a random noise \(\bm{Z}\in\mathbb{R}^{d\times k}\), by update \(\bm{V}_{t}\leftarrow\texttt{QR}(\bm{AV}_{t-1}+\bm{Z}_{t})\). We start by recalling their meta-sample complexity result for NPM, which we have slightly reformulated for our convenience:

**Lemma 1** (Corollary 1.1 of Hardt and Price (2014)).: _Let \(1\leq r<d\), \(\epsilon\in(0,1/2)\) and \(\delta\in(0,2e^{-cd})\), where \(c\) is an absolute constant.3 Let \(\bm{L}_{r}\) be the \(d\times r\) matrix, whose columns correspond to the top-\(r\) eigenvectors (in magnitude) of the symmetric (not necessarily PSD) matrix \(\bm{A}\in\mathbb{R}^{d\times d}\), and let \(\xi_{1}\geq\cdots\geq\xi_{d}\geq 0\) be the singular values of \(\bm{A}\). Assume that the noise matrices \(\bm{Z}_{t}\in\mathbb{R}^{d\times r}\) satisfy_

Footnote 3: It depends polynomially only in the sub-Gaussian moment of the data distribution \(\mathcal{D}\); see Theorem 1.1 of Rudelson and Vershynin (2009).

\[5\left\|\bm{Z}_{t}\right\|_{2}\leq\epsilon(\xi_{r}-\xi_{r+1})\quad\text{and} \quad 5\left\|\bm{L}_{r}^{\intercal}\bm{Z}_{t}\right\|_{2}\leq\frac{\delta(\xi_{r}- \xi_{r+1})}{2\sqrt{dr}},\quad\forall t\geq 1.\] (8)

_Then, after \(T=\Theta\left(\frac{\xi_{r}}{\xi_{r}-\xi_{r+1}}\log\left(\frac{d}{\varepsilon \delta}\right)\right)\) steps of NPM, \(\left\|\bm{\Pi}_{\bm{V}_{T}}^{\perp}\bm{L}_{r}\right\|_{2}\leq\epsilon\) with probability at least \(1-\delta\)._

First, we prove that the \(\bm{W}_{T}\) resulting from Algorithm 1 (NPM for the second moment gap) converges to the true value. We can view Algorithm 1's updates as \(\bm{W}_{t}\leftarrow\texttt{QR}(\bm{SW}_{t-1}+\bm{Z}_{t,1})\), where the noise matrix in this case is \(\bm{Z}_{t,1}:=(\bm{C}_{t}^{(1)}-\bm{C}_{t}^{(0)})-\bm{SW}_{t-1}\) and \(\bm{C}_{t}^{(a)}\) is as defined in Eqn. (6). The following lemma asserts that with large enough block size \(b\), the error matrices are sufficiently bounded such that the NPM iterates converge:

**Lemma 2**.: _Let \(\epsilon,\delta\in(0,1)\). It is sufficient to choose the block size \(b\) in Algorithm 1 as_

\[b=\Omega\left(\frac{\mathcal{V}}{\Delta_{m,\nu}^{2}p_{\mathrm{min}}}\left( \frac{dm}{\delta^{2}}\log\frac{m}{\delta}+\frac{1}{\epsilon^{2}}\log\frac{d}{ \delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}p_{\mathrm{min}}}\log\frac{d}{ \delta}\right)\] (9)

_to make the following hold with probability at least \(1-\delta\):_

\[5\left\|\bm{Z}_{t,1}\right\|_{2}\leq\epsilon\Delta_{m,\nu}\quad\text{and} \quad 5\left\|\bm{P}_{m}^{\intercal}\bm{Z}_{t,1}\right\|_{2}\leq\frac{\delta\Delta_{m, \nu}}{2\sqrt{dm}},\quad\forall t\geq 1,\] (10)

_where we recall that the columns of \(\bm{P}_{m}\) are the top-\(m\) (in magnitude) orthonormal eigenvectors of \(\bm{S}\)._

Let \(\widehat{\bm{U}}\) be the final estimate of the true \(\bm{U}\) outputted by Algorithm 1. For Algorithm 2, the noise matrix is \(\bm{Z}_{\tau,2}:=\left(\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{\tilde{\Sigma}}_{ \tau}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}-\bm{\Pi}_{\bm{U}}^{\perp}\bm{\Sigma} \bm{\Pi}_{\bm{U}}^{\perp}\right)\bm{V}_{\tau-1}\), where \(\bm{\tilde{\Sigma}}_{\tau}:=\frac{1}{B}\sum_{j}\bm{x}_{j}\bm{x}_{j}^{\intercal}\) is the sample covariance at time step \(\tau\) of Algorithm 2. Similarly, with large enough block size \(\mathcal{B}\), we have the following lemma:

**Lemma 3**.: _Let \(\epsilon,\delta\in(0,1)\). Suppose that \(\left\|\mathbf{\Pi}_{\widehat{U}}-\mathbf{\Pi}_{\boldsymbol{U}}\right\|_{2}\leq \frac{\Delta_{k,\kappa}}{20V}\min\left(\epsilon,\frac{\delta}{2\sqrt{dk}}\right)\). Then, it is sufficient to choose the block size \(\mathcal{B}\) in Algorithm 2 as_

\[\mathcal{B}=\Omega\left(\frac{\mathcal{V}+V^{2}}{\Delta_{k,\kappa}^{2}}\left( \frac{dk}{\delta^{2}}\log\frac{k}{\delta}+\frac{1}{\epsilon^{2}}\log\frac{d}{ \delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}+V^{2}}\log\frac{d}{\delta} \right),\] (11)

_to make the following hold with probability at least \(1-\delta\):_

\[5\left\|\boldsymbol{Z}_{\tau,2}\right\|_{2}\leq\epsilon\Delta_{k,\kappa}\quad \text{and}\quad 5\left\|\boldsymbol{Q}_{k}^{\intercal}\boldsymbol{Z}_{\tau,2} \right\|_{2}\leq\frac{\delta\Delta_{k,\kappa}}{2\sqrt{dk}},\quad\forall\tau \geq 1,\] (12)

_where the columns of \(\boldsymbol{Q}_{k}\) is the top-\(k\) (in magnitude) orthonormal eigenvectors of \(\mathbf{\Pi}_{\widehat{U}}^{\frac{1}{2}}\mathbf{\Sigma}\mathbf{\Pi}_{ \widehat{U}}^{\frac{1}{2}}\). (We note that \(\boldsymbol{Q}_{k}\) is a solution for Eqn. (4).)_

**Remark 1**.: _We emphasize that, regardless of the block size requirement in the above lemmas, all the operations can be implemented within \(o(d^{2})\) space requirement._

Combining the convergence results, we can prove the following PAFO-learnability guarantee in the memory-limited, streaming setting:

**Theorem 1**.: _Let \(d,m,k\in\mathbb{N}\) be fixed. Consider a collection \(\mathcal{F}_{d}\subset\mathcal{P}_{d}\times\mathcal{P}_{d}\times(0,1)\) satisfying Assumptions 1 and 2. Then, \(\mathcal{F}_{d}\) is PAFO-learnable for streaming PCA with FNPM, where the sufficient number of samples is given as \(N_{\mathcal{F}_{d}}(\varepsilon_{\mathrm{o}},\varepsilon_{\mathrm{f}},\delta)= N_{1}+N_{2}\), with_

\[N_{1} =\tilde{\Omega}\left(\frac{K_{m,\nu}}{p_{\min}}\left\{\frac{ \mathcal{V}}{\Delta_{m,\nu}^{3}}\left(\frac{dm}{\delta^{2}}+\frac{\alpha}{ \eta_{k}^{2}}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}\Delta_{m,\nu}} \right\}+\frac{\mathbb{1}[\boldsymbol{g}\neq\boldsymbol{0}]\sigma^{2}}{p_{ \min}g_{\min}^{2}\eta_{k}^{2}}\right),\] (Algorithm 1) \[N_{2} =\tilde{\Omega}\left(\frac{K_{k,\kappa}(\mathcal{V}+V^{2})}{ \Delta_{k,\kappa}^{3}}\left(\frac{dk}{\delta^{2}}+\frac{k^{2}V^{2}}{\varepsilon _{\mathrm{o}}^{2}}\right)+\frac{K_{k,\kappa}\mathcal{M}^{2}}{\Delta_{k,\kappa }(\mathcal{V}+V^{2})}\right),\] (Algorithm 2)

_where \(\eta_{k}=\Theta\left(\min\left\{\varepsilon_{\mathrm{f}},\frac{\Delta_{k,\kappa }}{kV^{2}}\varepsilon_{\mathrm{o}},\frac{\Delta_{k,\kappa}}{\sqrt{dk}}\delta\right\}\right)\) and \(\alpha=1+\frac{1}{\left[\boldsymbol{g}\neq\boldsymbol{0}\right]f_{\max}^{2}}\)._

Let us take a moment to digest the sample complexity in Theorem 1. The second term \(N_{2}\) is determined by the \(\varepsilon_{\mathrm{o}}\)-optimality requirement in our PAFO-learnability. Note that \(N_{2}\) has no dependencies on fairness-related quantities such as \(\varepsilon_{\mathrm{f}}\), \(p_{\min}\), and \(\Delta_{m,\nu}\). On the other hand, the first term \(N_{1}\) is not only determined by the \(\varepsilon_{\mathrm{f}}\)-fairness requirement, but also the \(\varepsilon_{\mathrm{o}}\)-optimality as well. This is the "price" of pursuing fairness; \(\boldsymbol{U}\), which encodes the unfair subspace needed to be nullified, is required to be accurately estimated as it impacts not only the level of fairness but also the resulting solution's optimality. This is clear in our formulation of fair PCA; in Eqn. (4), note how the optimal solution depends heavily on \(\mathbf{\Pi}_{\boldsymbol{U}}\).

We further elaborate on the dependencies of \(N_{1}\) on fairness-related quantities, namely \(p_{\min}\) and \(\boldsymbol{g}\). First, if \(p_{\min}\to 0\), i.e., if one of the two groups is never sampled, then the sample complexity tends to infinity, and the learnability does not hold; this aligns with our intuition, as we need samples from _both_ of the sensitive groups. Its dependency is also quite natural, as the minimum expected number of samples from either group depends linearly on \(\frac{1}{p_{\min}}\). Also, when \(\boldsymbol{g}\neq\boldsymbol{0}\), \(N_{1}\) has an additional additive term scaling linearly with \(\frac{1}{g_{\min}^{2}}\). This is because when \(\boldsymbol{g}\neq\boldsymbol{0}\), we must explicitly account for the approximation error of \(\frac{\boldsymbol{g}}{\left\|\boldsymbol{g}\right\|_{2}}\) due to the QR decomposition at the end of Algorithm 1.

## 7 Experiments on Real-World Datasets

The code for all experiments is available at github.com/HaneulJo/fair-streaming-pca.

### CelebA Dataset

We evaluate the efficacy of our proposed FNPM on the CelebA dataset (Liu et al., 2015). It has been considered by Kleindessner et al. (2023) to show the superior efficiency of their fair PCA algorithm compared to previous approaches (Lee et al., 2022; Olfat and Aswani, 2019; Ravfogel et al., 2022). However, even in Kleindessner et al. (2023), the images were resized and grey-scaled,reducing the dimension from the original 218\(\times\)178\(=\)38,804 to 80\(\times\)80\(=\)6,400. Indeed, for a modest-sized computer, it is impossible to load all 162,770 original images in training set to the memory at once, while they require a full dataset to run each step of their algorithm. Here, we use the _original_ resolution, full-color CelebA dataset. We implement our FNPM using Python JAX NumPy Module (Bradbury et al., 2023; Harris et al., 2020) and Pytorch (Paszke et al., 2017). All experiments were performed on Apple 2020 Mac mini M1 with 16GB RAM.

Although CelebA is not streaming in nature, we intend to show that transforming it to one and using our memory-efficient approach allow us to _scale up_ fair PCA. Since there are three channels of color, we run FNPM channel-wise but in _parallel_ as usual in vision tasks (Priorov et al., 2013). For each channel of colors, we project the data onto a \(k=1000\)-dimensional subspace while nullifying \(m=2\) leading eigenvectors of covariance difference.

The resulting images are displayed in Figure 1. Here, we consider 'Eyeglasses' as a sensitive attribute to divide groups. We adopt the predefined train-validation split and run our algorithm only on the training set for 5 iterations with block sizes of \(b=\mathcal{B}=32,000\). Then, using the output \(\bm{V}\) of FNPM, we project images selected from the validation set. We observe that we have images of faces wearing colorful glasses by nullifying some of the leading eigenvectors of covariance difference. For the images originally with sunglasses, their glasses get blurred, and "virtual" eyes are added to them. We provide more results on the other attributes and ablation studies on varying \(m\) and \(b\) in Appendix F.2.

### UCI Datasets

For the sake of completeness, we conduct a quantitative evaluation of our algorithm on UCI datasets (Adult Income, COMPAS, German Credit) and compare it with six previous works (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019; Ravfogel et al., 2020, 2022a; Samadi et al., 2018). The results for the Adult Income dataset are shown in Table 1. We assess several variants of our methods: in Table 1, "mean" is when we match the means and not second moments, "FNPM" is when we run our Algorithm 2 with a block size of full-batch, and "offline" is when we directly solve our "Null It Out" formulation of fair PCA (Eqn. (4)) via offline eigen-decomposition. In the table, we report the explained variance (%Var), representation fairness measured by maximum mean discrepancy (MMD\({}^{2}\)), downstream task accuracy (%Acc), and downstream task fairness in demographic parity (%\(\Delta_{\text{DP}}\)). The result showcases that our method yields competitive quantitative performance even for the common tabular datasets while being much more memory efficient. We defer the full results for other UCI datasets to Appendix F.3.

## 8 Other Related Works

Fairness in ML.There are largely two directions in the literature of algorithmic fairness. One direction is to propose a suitable and meaningful fairness definition (Dwork et al., 2012; Feldman et al., 2015; Hardt et al., 2016). The other direction is to develop _efficient_ fair algorithms, although often the fairness constraint forces the algorithm to be much more inefficient than its unfair counterpart, or it calls for a need for a completely new algorithmic approach. There are also different ways of

Figure 1: Experimental results on full-resolution **CelebA** dataset. Original image v.s. FNPM output. (Sensitive attribute: “Eyeglasses”)

imposing fairness in an ML pipeline, such as learning fair pre-processing (Biswas and Rajan, 2021), fair in-processing (Roh et al., 2021; Wan et al., 2023; Zafar et al., 2019), and more. The reader is encouraged to check Barocas et al. (2019) for a more comprehensive treatment of this subject.

Fair online/streaming Learning.Bechavod et al. (2020); Gillen et al. (2018) study individual fairness in online learning in a learning theoretic framework, even when the underlying metric is unavailable. Stemming from the concept of fair clustering as proposed in Chierichetti et al. (2017), Bera et al. (2022); Schmidt et al. (2020) study imposing demographic parity on clustering in the streaming setting. Such fairness has been considered in various other streaming problems such as online selection (Correa et al., 2021), streaming submodular optimization (El Halabi et al., 2020), and diversity maximization (Wang et al., 2022). Quite surprisingly, demographic parity (or any other concept of fairness) has never been considered in the setting of streaming PCA.

Streaming PCA.Without the fairness constraint, streaming PCA has been studied much from statistics and the machine learning community. Two prominent algorithms have been studied; the noisy power method (Mitiiagkas et al., 2013) and Oja's method (Oja, 1982). Much work has been done in improving the theoretical guarantees of streaming PCA (Balcan et al., 2016; Hardt and Price, 2014; Jain et al., 2016; Liang, 2023), improving the algorithm itself (Xu, 2023; Yun, 2018), or extending the guarantees to various different settings (Balzano et al., 2018; Bienstock et al., 2022; Kumar and Sarkar, 2023). Memory-limited, streaming versions of somewhat related problems, such as community detection (Yun et al., 2014) and low-rank matrix completion (Yun et al., 2015), have been tackled as well using similar spectral techniques as PCA (e.g., power method). However, to the best of our knowledge, fairness (regardless of the definition) has never been considered in this context of streaming PCA, which we tackle in this work and which we believe is of great importance.

## 9 Conclusion

In this work, we tackled the two outstanding problems of the existing fair PCA literature. From the theoretical side, we illustrated a new formulation of fair PCA based on the "Null It Out" approach and then provided a novel statistical framework called PAFO-learnability of fair PCA. From the practical side, we addressed the memory-limited scenarios by proposing a new problem setting called fair streaming PCA and a memory-efficient two-stage algorithm called FNPM. Based on these, we established a statistical guarantee that our algorithm achieves the PAFO-learnability for fair streaming PCA. Lastly, we ran experiments on the CelebA and UCI datasets to certify the scalability of our method.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their helpful comments and suggestions. We also thank Gwangsu Kim (Jeonbuk National University) for the helpful discussions in the initial phase of the research. This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korean government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST); No. 2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics).

## References

* Abdi and Williams (2010) Herve Abdi and Lynne J. Williams. Principal component analysis. _WIREs Computational Statistics_, 2(4):433-459, 2010.
* Ananthakrishnan et al. (2021) Nivasini Ananthakrishnan, Shai Ben-David, Tosca Lechner, and Ruth Urner. Identifying regions of trusted predictions. In _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence_, volume 161 of _Proceedings of Machine Learning Research_, pages 2125-2134. PMLR, 27-30 Jul 2021.
* Balcan et al. (2016) Maria-Florina Balcan, Simon Shaolei Du, Yining Wang, and Adams Wei Yu. An Improved Gap-Dependency Analysis of the Noisy Power Method. In _29th Annual Conference on Learning Theory_, volume 49 of _Proceedings of Machine Learning Research_, pages 284-309, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR.
* Balzano et al. (2018) Laura Balzano, Yuejie Chi, and Yue M. Lu. Streaming PCA and Subspace Tracking: The Missing Data Case. _Proceedings of the IEEE_, 106(8):1293-1310, 2018.
* Barocas and Selbst (2016) Solon Barocas and Andrew D. Selbst. Big Data's Disparate Impact. _104 California Law Review 671_, 2016.
* Barocas et al. (2019) Solon Barocas, Moritz Hardt, and Arvind Narayanan. _Fairness and Machine Learning: Limitations and Opportunities_. fairmlbook.org, 2019.
* Bechavod et al. (2020) Yahav Bechavod, Christopher Jung, and Steven Z. Wu. Metric-Free Individual Fairness in Online Learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 11214-11225. Curran Associates, Inc., 2020.
* Bera et al. (2022) Suman K. Bera, Syamantak Das, Sainyam Galhotra, and Sagar Sudhir Kale. Fair K-Center Clustering in MapReduce and Streaming Settings. In _Proceedings of the ACM Web Conference 2022_, WWW '22, page 1414-1422, New York, NY, USA, 2022. Association for Computing Machinery.
* Bienstock et al. (2022) Daniel Bienstock, Minchan Jeong, Apurv Shukla, and Se-Young Yun. Robust Streaming PCA. In _Advances in Neural Information Processing Systems_, volume 35, pages 4231-4243. Curran Associates, Inc., 2022.
* Biswas and Rajan (2021) Sumon Biswas and Hridesh Rajan. Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. In _Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, ESEC/FSE 2021, page 981-993, New York, NY, USA, 2021. Association for Computing Machinery.
* Bradbury et al. (2023) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2023. URL http://github.com/google/jax.
* Chamon and Ribeiro (2020) Luiz Chamon and Alejandro Ribeiro. Probably Approximately Correct Constrained Learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 16722-16735. Curran Associates, Inc., 2020.
* Chierichetti et al. (2017) Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair Clustering Through Fairlets. In _Advances in Neural Information Processing Systems_, volume 30, pages 5036-5044. Curran Associates, Inc., 2017.
* Chierichetti et al. (2018)Jose Correa, Andres Cristi, Paul Duetting, and Ashkan Norouzi-Fard. Fairness and Bias in Online Selection. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2112-2121. PMLR, 18-24 Jul 2021.
* 868, 1969.
* Dwork et al. (2012) Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through Awareness. In _Proceedings of the 3rd Innovations in Theoretical Computer Science Conference_, ITCS '12, page 214-226, New York, NY, USA, 2012. Association for Computing Machinery.
* Eckart and Young (1936) Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1:211--218, 1936.
* Halabi et al. (2020) Marwa El Halabi, Slobodan Mitrovic, Ashkan Norouzi-Fard, Jakab Tardos, and Jakub M Tarnawski. Fairness in Streaming Submodular Maximization: Algorithms and Hardness. In _Advances in Neural Information Processing Systems_, volume 33, pages 13609-13622. Curran Associates, Inc., 2020.
* Feldman et al. (2015) Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and Removing Disparate Impact. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '15, page 259-268, New York, NY, USA, 2015. Association for Computing Machinery.
* Ghashami et al. (2016) Mina Ghashami, Daniel J. Perry, and Jeff Phillips. Streaming Kernel Principal Component Analysis. In _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_, volume 51 of _Proceedings of Machine Learning Research_, pages 1365-1374, Cadiz, Spain, 09-11 May 2016. PMLR.
* Gillen et al. (2018) Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online Learning with an Unknown Fairness Metric. In _Advances in Neural Information Processing Systems_, volume 31, pages 2605-2614. Curran Associates, Inc., 2018.
* Golub and Van Loan (2013) Gene H. Golub and Charles F. Van Loan. _Matrix Computations_. Johns Hopkins Studies in the Mathematical Sciences. The Johns Hopkins University Press, 4 edition, 2013.
* Hardt and Price (2014) Moritz Hardt and Eric Price. The Noisy Power Method: A Meta Algorithm with Applications. In _Advances in Neural Information Processing Systems_, volume 27, pages 2861-2869. Curran Associates, Inc., 2014.
* Hardt et al. (2016) Moritz Hardt, Eric Price, and Nati Srebro. Equality of Opportunity in Supervised Learning. In _Advances in Neural Information Processing Systems_, volume 29, pages 3323-3331. Curran Associates, Inc., 2016.
* Harris et al. (2020) Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020.
* Hopkins et al. (2023) Max Hopkins, Daniel M. Kane, Shachar Lovett, and Gaurav Mahajan. Do PAC-Learners Learn the Marginal Distribution? _arXiv preprint arXiv:2302.06285_, 2023.
* Hotelling (1933) Harold Hotelling. Analysis of a complex of statistical variables into principal components. _Journal of Educational Psychology_, 24(6):417-441, 1933.
* Huang et al. (2021) De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-PCA: Efficient guarantees for Oja's algorithm, beyond rank-one updates. In _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 2463-2498. PMLR, 15-19 Aug 2021.
* Huang et al. (2018)Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm. In _29th Annual Conference on Learning Theory_, volume 49 of _Proceedings of Machine Learning Research_, pages 1147-1164, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR.
* Jin et al. (2019) Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. A Short Note on Concentration Inequalities for Random Vectors with SubGaussian Norm. _arXiv preprint arXiv:1902.03736_, 2019.
* Johnson and Wichern (2008) Richard A. Johnson and Dean W. Wichern. _Applied Multivariate Statistical Analysis_. Pearson, 6 edition, 2008.
* Jolliffe and Cadima (2016) Ian T. Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 374(2065), 2016.
* Kamani et al. (2022) Mohammad Mahdi Kamani, Farzin Haddadpour, Rana Forsati, and Mehrdad Mahdavi. Efficient fair principal component analysis. _Machine Learning_, 111(10):3671-3702, 2022.
* Kirchner et al. (2016) L Kirchner, J. Larson, S. Mattu, and J. Angwin. Machine bias. ProPublica, 2016.
* Kizilcec and Lee (2021) Rene F. Kizilcec and Hansol Lee. Algorithmic Fairness in Education. _arXiv preprint arXiv:2007.05443_, 2021.
* Kleindessner et al. (2019) Matthaus Kleindessner, Samira Samadi, Pranjal Awasthi, and Jamie Morgenstern. Guarantees for Spectral Clustering with Fairness Constraints. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 3458-3467. PMLR, 09-15 Jun 2019.
* Kleindessner et al. (2023) Matthaus Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar. Efficient fair PCA for fair representation learning. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 5250-5270. PMLR, 25-27 Apr 2023.
* Kohler and Lucchi (2017) Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled Cubic Regularization for Non-convex Optimization. In _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1895-1904. PMLR, 06-11 Aug 2017.
* Kumar and Sarkar (2023) Syamantak Kumar and Purnamrita Sarkar. Streaming PCA for Markovian Data. _arXiv preprint arXiv:2305.02456_, 2023.
* Lahoti et al. (2020) Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without Demographics through Adversarially Reweighted Learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 728-740. Curran Associates, Inc., 2020.
* Lee et al. (2022) Junghyun Lee, Gwangsu Kim, Mahbod Olfat, Mark Hasegawa-Johnson, and Chang D. Yoo. Fast and Efficient MMD-Based Fair PCA via Optimization over Stiefel Manifold. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7363-7371, Jun. 2022.
* Li et al. (2023) Xun Li, Dongsheng Chen, Weipan Xu, Haohui Chen, Junjun Li, and Fan Mo. Explainable dimensionality reduction (XDR) to unbox AI 'black box' models: A study of AI perspectives on the ethnic styles of village dwellings. _Humanities and Social Sciences Communications_, 10(1):35, Jan 2023.
* Liang (2023) Xin Liang. On the optimality of the Oja's algorithm for online PCA. _Statistics and Computing_, 33(3):62, Mar 2023.
* Liberty (2013) Edo Liberty. Simple and Deterministic Matrix Sketching. In _Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '13, page 581-588, New York, NY, USA, 2013. Association for Computing Machinery.
* Liu et al. (2015) Ziqi Liu, Yu-Xiang Wang, and Alexander Smola. Fast Differentially Private Matrix Factorization. In _Proceedings of the 9th ACM Conference on Recommender Systems_, RecSys '15, page 171-178, New York, NY, USA, 2015a. Association for Computing Machinery.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pages 3730-3738, 2015b.
* Marshall et al. (2011) Albert W. Marshall, Ingram Olkin, and Barry C. Arnold. _Inequalities: Theory of Majorization and Its Applications_. Springer Series in Statistics. Springer New York, 2 edition, 2011.
* Mehrabi et al. (2021) Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey on Bias and Fairness in Machine Learning. _ACM Computing Surveys_, 54(6), jul 2021.
* Mirsky (1975) L. Mirsky. A trace inequality of John von Neumann. _Monatshefte fur Mathematik_, 79(4):303-306, Dec 1975.
* Mitliagkas et al. (2013) Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory Limited, Streaming PCA. In _Advances in Neural Information Processing Systems_, volume 26, pages 2886-2894. Curran Associates, Inc., 2013.
* Oja (1982) Erkki Oja. Simplified neuron model as a principal component analyzer. _Journal of Mathematical Biology_, 15(3):267-273, Nov 1982.
* Oja and Karhunen (1985) Erkki Oja and Juha Karhunen. On Stochastic Approximation of the Eigenvectors and Eigenvalues of the Expectation of a Random Matrix. _Journal of Mathematical Analysis and Applications_, 106:69-84, 1985.
* Olfat and Aswani (2019) Matt Olfat and Anil Aswani. Convex Formulations for Fair Principal Component Analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 663-670, Jul. 2019.
* Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In _NIPS 2017 Workshop Autodiff_, 2017.
* Pearson (1901) Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science_, 2(11):559-572, 1901.
* Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global Vectors for Word Representation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1532-1543, Doha, Qatar, October 2014. Association for Computational Linguistics.
* Priorov et al. (2013) Andrey Priorov, Kirill Tumanov, Vladimir Volokhov, Evgeny Sergeev, and Ivan Mochalov. Applications of image filtration based on principal component analysis and nonlocal image processing. _IAENG International Journal of Computer Science_, 40(2):62-80, 2013.
* Ravfogel et al. (2020) Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7237-7256, Online, July 2020. Association for Computational Linguistics.
* Ravfogel et al. (2022a) Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D Cotterell. Linear Adversarial Concept Erasure. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 18400-18421. PMLR, 17-23 Jul 2022a.
* Ravfogel et al. (2022b) Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial Concept Erasure in Kernel Space. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6034-6055, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics.
* Roh et al. (2021) Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. FairBatch: Batch Selection for Model Fairness. In _International Conference on Learning Representations_, 2021.
* Rudelson and Vershynin (2009) Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. _Communications on Pure and Applied Mathematics_, 62(12):1707-1739, 2009.
* Rudelson et al. (2015)Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, and Santosh Vempala. The Price of Fair PCA: One Extra dimension. In _Advances in Neural Information Processing Systems_, volume 31, pages 10999-11010. Curran Associates, Inc., 2018.
* Schmidt et al. (2020) Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In _Approximation and Online Algorithms_, pages 232-251, Cham, 2020. Springer International Publishing.
* Scholkopf et al. (1998) Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. _Neural Computation_, 10(5):1299-1319, 07 1998.
* Shalev-Schwartz and Ben-David (2014) S. Shalev-Schwartz and S. Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014.
* Tantipongpipat et al. (2019) Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H Morgenstern, and Santosh Vempala. Multi-Criteria Dimensionality Reduction with Applications to Fairness. In _Advances in Neural Information Processing Systems_, volume 32, pages 15161-15171. Curran Associates, Inc., 2019.
* Tjoa and Guan (2021) Erico Tjoa and Cuntai Guan. A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI. _IEEE Transactions on Neural Networks and Learning Systems_, 32(11):4793-4813, 2021.
* Tropp (2015) Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* Ullah et al. (2018) Enayat Ullah, Poorya Mianjy, Teodor Vanislavov Marinov, and Raman Arora. Streaming Kernel PCA with \(\hat{\mathcal{O}}(\sqrt{n})\) Random Features. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* von Neumann (1937) John von Neumann. Some Matrix-Inequalities and Metrization of Matrix-Space. _Tomsk University Review_, 1:286-300, 1937.
* Vu et al. (2022) Hieu Vu, Toan Tran, Man-Chung Yue, and Viet Anh Nguyen. Distributionally Robust Fair Principal Components via Geodesic Descents. In _International Conference on Learning Representations_, 2022.
* Wan et al. (2023) Mingyang Wan, Daochen Zha, Ninghao Liu, and Na Zou. In-Processing Modeling Techniques for Machine Learning Fairness: A Survey. _ACM Transactions on Knowledge Discovery from Data_, 17 (3), mar 2023.
* Wang and Lu (2016) Chuang Wang and Yue M. Lu. Online learning for sparse PCA in high dimensions: Exact dynamics and phase transitions. In _2016 IEEE Information Theory Workshop (ITW)_, pages 186-190, 2016.
* Wang et al. (2023) Ji Wang, Ding Lu, Ian Davidson, and Zhaojun Bai. Scalable Spectral Clustering with Group Fairness Constraints. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 6613-6629. PMLR, 25-27 Apr 2023.
* Wang et al. (2022) Yanhao Wang, Francesco Fabbri, and Michael Mathioudakis. Streaming Algorithms for Diversity Maximization with Fairness Constraints. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pages 41-53, Los Alamitos, CA, USA, 2022. IEEE Computer Society.
* Xu (2023) Zhiqiang Xu. On the Accelerated Noise-Tolerant Power Method. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 7147-7175. PMLR, 25-27 Apr 2023.
* Xu and Li (2022) Zhiqiang Xu and Ping Li. Faster Noisy Power Method. In _Proceedings of The 33rd International Conference on Algorithmic Learning Theory_, volume 167 of _Proceedings of Machine Learning Research_, pages 1138-1164. PMLR, 29 Mar-01 Apr 2022.
* Yang et al. (2018) Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History PCA: A new algorithm for streaming PCA. _arXiv preprint arXiv:1802.05447_, 2018.
* Yang and Xu (2015) Wenzhuo Yang and Huan Xu. Streaming Sparse Principal Component Analysis. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 494-503, Lille, France, 07-09 Jul 2015. PMLR.
* Yang et al. (2018)* Yun (2018) Se-Young Yun. Noisy Power Method with Grassmann Average. In _2018 IEEE International Conference on Big Data and Smart Computing (BigComp)_, pages 709-712, 2018.
* Yun et al. (2014) Se-Young Yun, Marc Lelarge, and Alexandre Proutiere. Streaming, Memory Limited Algorithms for Community Detection. In _Advances in Neural Information Processing Systems_, volume 27, pages 3167-3175. Curran Associates, Inc., 2014.
* Yun et al. (2015) Se-Young Yun, Marc Lelarge, and Alexandre Proutiere. Fast and Memory Optimal Low-Rank Matrix Approximation. In _Advances in Neural Information Processing Systems_, volume 28, pages 3166-3185. Curran Associates, Inc., 2015.
* Zafar et al. (2019) Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness Constraints: A Flexible Approach for Fair Classification. _Journal of Machine Learning Research_, 20(75):1-42, 2019.
* Zemel et al. (2013) Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning Fair Representations. In _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 325-333, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
* Zhao et al. (2022) Tianxiang Zhao, Enyan Dai, Kai Shu, and Suhang Wang. Towards Fair Classifiers Without Sensitive Attributes: Exploring Biases in Related Features. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, WSDM '22, page 1433-1442, New York, NY, USA, 2022. Association for Computing Machinery.

###### Contents

* 1 Introduction
* 2 Preliminaries
* 3 An Alternative Approach to Fair PCA
	* 3.1 "Null It Out" Formulation of Fair PCA
	* 3.2 An Explicit Characterization for Solution of Fair PCA
	* 3.3 Comparison to the Existing Covariance Matching Constraint
* 4 Statistical Viewpoint: PAFO-Learnability of PCA
* 5 Algorithmic Viewpoint: Fair Streaming PCA
	* 5.1 Our Algorithm: Fair Noisy Power Method (FNPM)
	* 5.2 Previous Approaches are not Suitable for Streaming Setup
	* 5.3 Extension to Multiple Sensitive Groups/Attributes
* 6 FNPM is a PAFO-Learning Algorithm
* 7 Experiments on Real-World Datasets
	* 7.1 CelebA Dataset
	* 7.2 UCI Datasets
* 8 Other Related Works
* 9 Conclusion
* A Broader Impacts, Limitations, and Future Directions
* A.1 Broader Impacts
* A.2 Limitations and Future Directions
* B Full Pseudocodes of Algorithms 1 and 2 for Fair "Streaming" PCA
* C Extension of Algorithms 3 to Multiple and Non-binary Attributes
* D More Detailed Comparison to Kleindessner et al. (2023)
* D.1 Their Approach
* D.2 Unsuitability for the Streaming Setting
* E Proofs of Lemma 2, 3, and Theorem 1
* E.1 Notations and Assumptions
* E.2 Matrix/Vector Concentration Inequalities
* E.3 Proof of Lemma 2 - Bounding Error in Second Moment Gap
* E.4 Proof of Lemma 3 - Bounding the Final Error

* 5 Bounding the Estimation Error of Mean Difference
	* 5.6 Proof of Theorem 1 - Sample Complexity for PAFO-learnability
* F More Experiments
	* 6.1 Synthetic Example
	* 6.2 Additional Results on CelebA Dataset
	* 6.3 Full Results on UCI Dataset

## Appendix A Broader Impacts, Limitations, and Future Directions

### Broader Impacts

This work proposes a dimensionality reduction method that addresses memory efficiency _and_ fairness while providing a statistical guarantee. By identifying and nullifying the "unfair direction" inherent in the data distribution, we offer an alternative approach to fair PCA. We anticipate that our approach will motivate researchers to explore other dimensionality reduction techniques (_e.g.,_ auto-encoder) with fairness constraints. Significantly, our contribution includes a rigorous theoretical framework, PAFO learnability, which enables sample complexity analysis of fair PCA. We envision the potential for our theory to be further generalized to broader contexts, including alternative definitions of fairness and optimality of different algorithms for fair machine learning.

On the application side, the memory efficiency of our method can facilitate the scalability of fair PCA, making it viable for processing high-dimensional datasets, even in scenarios where data points arrive in a streaming fashion. One possible application of our approach is data pre-/post-processing to alleviate unfairness, such as generating fair word embeddings by eliminating sensitive attribute information through orthogonal projection. For more detailed discussions on potential future directions, please refer to below.

### Limitations and Future Directions

Here, we list some of our work's limitations and possible extensions/future directions.

Individual Fairness for PCA.Our definition of fairness in PCA only covers group fairness via fair presentation learning, which was also the case for the prior works (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019). None of the works, including ours, have yet to consider the notion of individual fairness (Dwork et al., 2012) in the context of PCA, for which we do not have a definitive answer.

Knowledge of sensitive attribute.Our framework requires the _full_ knowledge of the sensitive attribute \(s\) for all data points \(\bm{x}\)'s, which was also the case for all the previous works (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019). But, the assumption of such knowledge may not be feasible in the real world due to privacy or legal reasons (Lahoti et al., 2020; Zhao et al., 2022), or even just due to some extrinsic noises. Considering the case where the dataset may lack sensitive attributes for some or all of the data points is an important future direction.

Making the algorithm anytime.In our formulation of fair PCA, our algorithm is two-phase, with the first phase as a "burn-in" period for estimating the unfair subspace. Thus, it is not an anytime algorithm in that if the algorithm stops whilst in the first phase, then the resulting \(\bm{V}\) is random and completely uninformative. Designing an anytime variant of our algorithm is an important future direction. One possible way to achieve that is to consider other streaming PCA algorithms such as Oja's method (Oja and Karhunen, 1985) or accelerated NPM (Xu, 2023; Xu and Li, 2022).

Improving gap dependence.Our algorithm's current sample complexity analysis relies on the analysis of NPM by Hardt and Price (2014), which relies on the immediate singular value gap, \(\sigma_{k}-\sigma_{k+1}\). Balcan et al. (2016) showed that considering greater iteration rank \(q\geq k\), _i.e.,_ by considering optimization variable of greater size, leads to a better gap dependency: from \(\sigma_{k}-\sigma_{k+1}\) to \(\sigma_{k}-\sigma_{q+1}\). However, this is incompatible with our current definition of PAFO-learnability (Definition 2) because with greater iteration rank, the solution \(\bm{V}^{\star}\) to which the explained variance should be compared against is not clear. The problem is that the sample complexity guarantee of Balcan et al. (2016); Hardt and Price (2014) is derived in terms of the _sine_ angle between the top \(k\)-eigenspace of the true covariance and the estimated \(q\)-dimensional subspace. Clearing this up would allow for a better theoretical guarantee in sample complexity and thus an important future direction.

Kernelizing our frameworkKernel PCA (Scholkopf et al., 1998) is a cornerstone in modern machine learning that has lent itself to be an inspiration to many applications, both theory and practice-wise. Unlike previous fair PCA works (Kleindessner et al., 2023; Olfat and Aswani, 2019), in which the authors provided a kernelized version of their fair PCA algorithm, both our statistical framework and memory-efficient algorithm for streaming setting do not readily extend to the kernelized version. Algorithmically, to tackle the streaming setting, one may take inspiration from streaming kernel PCA (Ghashami et al., 2016; Ullah et al., 2018), which was in turn inspired by matrix sketching (Liberty, 2013).

Extending to other settings.In a similar spirit, extending our formulation of fair PCA to other streaming settings such as sparse (Wang and Lu, 2016; Yang and Xu, 2015), nonstationary (Bienstock et al., 2022), or even distributionally robust settings (Vu et al., 2022) would also be interesting.

More experimentsLast but not least, we've only performed experiments on synthetic, CelebA, and UCI datasets. It would be interesting to try our framework (either the statistical formulation, the streaming setting, or both) on datasets from other domains, such as NLP and graphs. GloVe vectors (Pennington et al., 2014) has been used as a benchmark in a similar task called "concept erasure" (Ravfogel et al., 2020, 2022b); group fairness in spectral clustering of graphs has been studied as well (Kleindessner et al., 2019; Wang et al., 2023).

Full Pseudocodes of Algorithms 1 and 2 for Fair "Streaming" PCA

Throughout the appendices, from hereon, we will refer to our algorithms by Algorithms 3 and 4 instead of Algorithms 1 and 2.

```
1Input:\(d\), \(m\), Block size \(b\), Number of iterations \(T\);
2Output: A matrix \(\widehat{\bm{U}}\) with orthonormal columns;
3\(\bm{W}_{0}\leftarrow\text{\rm QR}(\mathcal{N}(0,1)^{d\times m})\);
4\((\overline{\bm{m}}^{(0)},\overline{\bm{m}}^{(1)},B^{(0)},B^{(1)})\leftarrow( \bm{0}_{d},\bm{0}_{d},0,0)\);
5for\(t\in[T]\)do
6\((b_{t}^{(0)},b_{t}^{(1)})\leftarrow(0,0)\);
7\((\bm{m}_{t}^{(0)},\bm{m}_{t}^{(1)},\bm{C}_{t}^{(0)},\bm{C}_{t}^{(1)})\leftarrow( \bm{0}_{d},\bm{0}_{d},\bm{0}_{d\times m},\bm{0}_{d\times m})\);
8for\(i\in[b]\)do
9 Receive \((a,\bm{x})=(a_{(t-1)b+i},\bm{x}_{(t-1)b+i})\);
10\(b_{t}^{(a)}\gets b_{t}^{(a)}+1\);
11\(\bm{m}_{t}^{(a)}\leftarrow\bm{m}_{t}^{(a)}+\bm{x}\);
12\(\bm{C}_{t}^{(a)}\leftarrow\bm{C}_{t}^{(a)}+\bm{x}\bm{x}^{\intercal}\bm{W}_{t-1}\);
13foreach\(a\in\{0,1\}\)do
14if\(b_{t}^{(a)}>0\)then
15\(\bm{m}_{t}^{(a)}\leftarrow\frac{1}{b_{t}^{(a)}}\bm{m}_{t}^{(a)}\);
16\(\bm{C}_{t}^{(a)}\leftarrow\frac{1}{b_{t}^{(a)}}\bm{C}_{t}^{(a)}\);
17\(\overline{\bm{m}}^{(a)}\!\leftarrow\!\frac{B^{(a)}}{B^{(a)}+b_{t}^{(a)}}\overline {\bm{m}}^{(a)}+\frac{b_{t}^{(a)}}{B^{(a)}+b_{t}^{(a)}}\bm{m}_{t}^{(a)}\);
18\(B^{(a)}\!\gets B^{(a)}+b_{t}^{(a)}\);
19\(\bm{W}_{t}=\text{\rm QR}\left(\bm{C}^{(1)}-\bm{C}^{(0)}\right)\);
20\(\widehat{\bm{f}}\leftarrow\overline{\bm{m}}^{(1)}-\overline{\bm{m}}^{(0)}\);
21\(\widetilde{\bm{g}}\leftarrow\widehat{\bm{f}}-\bm{W}_{T}\bm{W}_{T}^{\intercal} \widehat{\bm{f}}\);
22if\(\|\widehat{\bm{g}}\|_{2}=0\)then
23\(\widehat{\bm{U}}=\bm{W}_{T}\)
24else
25\(\widehat{\bm{U}}=\left[\bm{W}_{T}\left|\frac{\widehat{\bm{g}}}{\|\bm{g}\|_{2}}\right.\right]\)
26return\(\widehat{\bm{U}}\) ```

**Algorithm 4**Fair NPM (= Algorithm 2)

```
1Input:\(d\), \(k\), Block sizes \(\mathcal{B}\), \(b\), Numbers of iterations \(\mathcal{T}\), \(T\);
2Output:\(\bm{V}_{\mathcal{T}}\in\text{\rm St}(d,k)\);
3\(\widehat{\bm{U}}\leftarrow\text{\rmPairSubspace}(b,T)\);
4\(\bm{V}_{0}\leftarrow\text{\rm QR}(\mathcal{N}(0,1)^{d\times k})\);
5for\(\tau\in[\mathcal{T}]\)do
6\(\bm{V}_{\tau}\leftarrow\bm{V}_{\tau-1}-\widehat{\bm{U}}\widehat{\bm{U}}^{ \intercal}\bm{V}_{\tau-1}\);
7\(\bm{C}\leftarrow\bm{0}_{d\times k}\);
8for\(j\in[\mathcal{B}]\)do
9 Receive \((*,\tilde{\bm{x}})=(*,\tilde{\bm{x}}_{(\tau-1)\mathcal{B}+j})\);
10\(\bm{C}\leftarrow\bm{C}+\frac{1}{b}\tilde{\bm{x}}\tilde{\bm{x}}^{\intercal}\bm{ V}_{\tau}\);
11\(\bm{V}_{\tau}\leftarrow\text{\rm QR}\left(\bm{C}-\widehat{\bm{U}}\widehat{\bm{U}}^{ \intercal}\bm{C}\right)\);
12return\(\bm{V}_{\mathcal{T}}\) ```

**Algorithm 5**UnfairSubspace (= Algorithm 2)
Extension of Algorithms 3 to Multiple and Non-binary Attributes

We remark that Algorithm 3 is a special case of the pseudocode below. By applying Algorithm 4 after applying this algorithm, we can handle the multiple non-binary attribute case and approximately and memory-efficiently solve the extended notion of fair PCA elaborated in Section 5.3.

```
1Input:\(d\), \(m_{1},\cdots,m_{\ell}\), Block size \(b\), Number of iterations \(T\);
2Output: A matrix \(\widehat{\bm{U}}\) with orthonormal columns;
3for\(r\in[\ell]\)do
4for\(a\in[g_{r}]\)do
5\(\bm{W}_{0}^{(r,a)}=\texttt{QR}(\mathcal{N}(0,1)^{d\times m_{r}})\);
6\((\overline{\bm{m}}^{(r,a,\mathrm{in})},\overline{\bm{m}}^{(r,a,\mathrm{out})}, B^{(r,a)})\leftarrow(\bm{0}_{d},\bm{0}_{d},0)\)
7for\(t\in[T]\)do
8 Receive \(\{((a_{1,i},\cdots,a_{\ell,i}),\bm{x}_{i})\}_{i=b(t-1)+1}^{bt}\);
9for\(r\in[\ell]\)do
10for\(a\in[g_{r}]\)do
11\(b_{t}^{(r,a)}=\sum_{i=b(t-1)+1}^{bt}\mathbbm{1}_{[a_{r,i}=a]}\);
12\(B^{(r,a)}\gets B^{(r,a)}+b_{t}^{(r,a)}\);
13if\(b_{t}^{(a)}>0\)then
14\(\bm{m}_{t}^{(r,\mathrm{a,in})}=\frac{1}{b_{t}^{(a)}}\sum_{i=b(t-1)+1}^{bt} \mathbbm{1}_{[a_{r,i}=a]}\bm{x}_{i}\);
15\(\bm{C}_{t}^{(r,a,\mathrm{in})}=\frac{1}{b_{t}^{(a)}}\sum_{i=b(t-1)+1}^{bt} \mathbbm{1}_{[a_{r,i}=a]}\bm{x}_{i}\bm{x}_{i}^{\intercal}\bm{W}_{t-1}^{(r,a)}\);
16
17else
18\((\bm{m}_{t}^{(r,a,\mathrm{in})},\bm{C}_{t}^{(r,a,\mathrm{in})})\leftarrow(\bm{0 }_{d},\bm{0}_{d\times m_{r}})\)
19if\(b_{t}^{(a)}<b\)then
20\(\bm{m}_{t}^{(r,a,\mathrm{out})}=\frac{1}{b-b_{t}^{(a)}}\sum_{i=b(t-1)+1}^{bt} \mathbbm{1}_{[a_{r,i}\neq a]}\bm{x}_{i}\);
21\(\bm{C}_{t}^{(r,a,\mathrm{out})}=\frac{1}{b-b_{t}^{(a)}}\sum_{i=b(t-1)+1}^{bt} \mathbbm{1}_{[a_{r,i}\neq a]}\bm{x}_{i}\bm{x}_{i}^{\intercal}\bm{W}_{t-1}^{(r,a)}\);
22
23else
24\((\bm{m}_{t}^{(r,a,\mathrm{out})},\bm{C}_{t}^{(r,a,\mathrm{out})})\leftarrow(\bm{0 }_{d},\bm{0}_{d\times m_{r}})\)
25\(\overline{\bm{m}}^{(r,a,\mathrm{in})}\leftarrow\frac{B^{(r,a)}-b_{t}^{(r,a)}} {B^{(a)}}\overline{\bm{m}}^{(r,a,\mathrm{in})}+\frac{b_{t}^{(a)}}{B^{(a)}}\bm{ m}_{t}^{(r,a,\mathrm{in})}\);
26\(\overline{\bm{m}}^{(r,a,\mathrm{out})}\leftarrow\frac{(t-1)-bB^{(r,a)}+b_{t}^{( r,a)}}{tb-B^{(a)}}\overline{\bm{m}}^{(r,a,\mathrm{out})}+\frac{b-b_{t}^{(a)}}{tb-B^{(a)}} \bm{m}_{t}^{(r,a,\mathrm{out})}\);
27\(\bm{W}_{t}^{(r,a)}=\texttt{QR}\left(\bm{C}_{t}^{(r,a,\mathrm{in})}-\bm{C}_{t}^ {(r,a,\mathrm{out})}\right)\);
28
29for\(r\in[\ell]\)do
30for\(a\in[g_{r}]\)do
31\(\widehat{\bm{f}}^{(r,a)}=\overline{\bm{m}}_{t}^{(r,a,\mathrm{in})}-\overline{ \bm{m}}_{t}^{(r,a,\mathrm{out})}\);
32
33return\(\widehat{\bm{U}}\) ```

**Algorithm 5**UnfairSubspace_Extended
More Detailed Comparison to Kleindessner et al. (2023)

### Their Approach

Kleindessner et al. (2023) considered the following formulation of fair PCA:

\[\max_{\bm{V}\in\operatorname{St}(d,k)}\operatorname{tr}(\bm{V}^{\intercal}\bm{X} ^{\intercal}\bm{X}\bm{V}),\quad\text{subject to }\bm{f}^{\intercal}\bm{V}=\bm{0}\ \wedge\ \bm{V}^{\intercal}(\bm{\Sigma}_{1}-\bm{\Sigma}_{0})\bm{V}=\bm{0},\] (13)

and proposed a reasonable approximation of the covariance constraint, which we briefly describe here. Let us write \(\bm{S}^{\prime}=\bm{\Sigma}_{1}-\bm{\Sigma}_{0}\) for brevity. As implicitly assumed in Kleindessner et al. (2023), let us assume that \(\bm{f}\neq\bm{0}\). The mean constraint is dealt with first. Denoting \(\bm{U}_{\bm{f}}\in\operatorname{St}(d,d-1)\) to be the matrix whose columns form a basis \((d-1)\)-dimensional nullspace of \(\bm{f}\), the mean constraint is satisfied if and only if \(\bm{V}\) is of the form \(\bm{U}_{\bm{f}}\bm{U}\) with \(\bm{U}\in\operatorname{St}(d-1,k)\) as the intermediate optimization variable. With this first reparametrization, the optimization now becomes

\[\max_{\bm{U}\in\operatorname{St}(d-1,k)}\operatorname{tr}(\bm{U}^{\intercal} \bm{U}_{\bm{f}}^{\intercal}\bm{X}^{\intercal}\bm{X}\bm{U}_{\bm{f}}\bm{U}), \quad\text{subject to }\bm{U}^{\intercal}\bm{U}_{\bm{f}}^{\intercal}\bm{S}^{ \prime}\bm{U}_{\bm{f}}\bm{U}=\bm{0},\] (14)

which now has only the covariance constraint.

To deal with the possible infeasibility of the covariance constraint, Kleindessner et al. (2023) proposed the following approach: letting \(\bm{M}_{\bm{f},\bm{S}^{\prime}}\in\operatorname{St}(d-1,l)\) be the matrix whose columns are the \(l\) smallest eigenvectors of \(\bm{U}_{\bm{f}}^{\intercal}\bm{S}^{\prime}\bm{U}_{\bm{f}}\) (in magnitude), \(\bm{U}\) only needs to nullify the eigenspace spanned by the remaining \(d-1-l\) eigenvectors. To see which terms are ignored with the relaxation, let \(\sum_{i=1}^{d}q_{i}^{\prime}\bm{v}^{\prime}\bm{v}_{i}^{\prime\intercal}\) be the eigenvalue decomposition of \(\bm{U}_{\bm{f}}^{\intercal}\bm{S}^{\prime}\bm{U}_{\bm{f}}\) with \(|q_{1}^{\prime}|\geq|q_{2}^{\prime}|\geq\cdots|q_{d}^{\prime}|\geq 0\). This approach essentially ignores the constraint \(\bm{U}^{\intercal}\bm{E}_{\bm{i}}\bm{U}=\bm{0}\), where

\[\bm{E}_{l}=\bm{M}_{\bm{f},\bm{S}^{\prime}}^{\intercal}\bm{U}_{\bm{f}}^{\intercal }\bm{S}^{\prime}\bm{U}_{\bm{f}}\bm{M}_{\bm{f},\bm{S}^{\prime}}=\sum_{i=d-l}^{ d-1}q_{i}^{\prime}\bm{v}^{\prime}\bm{v}_{i}^{\prime\intercal}.\] (15)

The number \(l\in\{k,\cdots,d-1\}\) controls how much the group-conditional covariances will be equalized; a smaller \(l\) means that the covariance constraint is enforced more stringently and vice versa. Ultimately, the learner can control \(l\) as a hyperparameter to create a trade-off between fairness and the explained variance. Moreover, _if_\(|q_{i}^{\prime}|\)'s are negligible for all \(i\geq d-1-l\), then \(\bm{M}_{\bm{f},\bm{S}^{\prime}}^{\intercal}\bm{U}_{\bm{f}}^{\intercal}\bm{S} ^{\prime}\bm{U}_{\bm{f}}\bm{M}_{\bm{f},\bm{S}^{\prime}}\approx\bm{0}\), and the relaxation becomes tighter.

As any \(\bm{U}\) of the form \(\bm{M}_{\bm{f},\bm{S}^{\prime}}\bm{\Lambda}\) satisfies the relaxed covariance constraint, a second reparameterization in terms of the new optimization variable \(\bm{\Lambda}\in\operatorname{St}(l,k)\) gives us

\[\max_{\bm{\Lambda}\in\operatorname{St}(l,k)}\operatorname{tr}(\bm{\Lambda}^{ \intercal}\bm{M}_{\bm{f},\bm{S}^{\prime}}^{\intercal}\bm{U}_{\bm{f}}^{ \intercal}\bm{X}^{\intercal}\bm{X}\bm{U}_{\bm{f}}\bm{M}_{\bm{f},\bm{S}^{\prime }}\bm{\Lambda}),\] (16)

which can be solved via the standard SVD-based approach for vanilla PCA. Then, the final solution is obtained as \(\bm{V}^{*}=\bm{M}_{\bm{f},\bm{S}^{\prime}}\bm{U}_{\bm{f}}\bm{\Lambda}^{*}\).

### Unsuitability for the Streaming Setting

In Section 5.2, we provided a rough overview of why existing approaches to fair PCA (Kleindessner et al., 2023; Lee et al., 2022; Olfat and Aswani, 2019) are not amenable to our streaming setting. Especially as the approach of Kleindessner et al. (2023) (described above) is almost like a PCA, one may wonder if standard techniques used in streaming PCA (Mitliagkas et al., 2013) can be used. Here, we argue in detail why that is _not_ the case, which is in sharp contrast to our approach and our reformulation of fair PCA that led to a memory-efficient fair streaming PCA algorithm.

Memory constraint.For streaming PCA without fairness constraints, the main objective could be written as \(\operatorname{tr}(\bm{V}^{\intercal}\bm{X}^{\intercal}\bm{X}\bm{V})=\sum_{i=1 }^{N}\operatorname{tr}\left(\bm{V}^{\intercal}\bm{x}_{i}\bm{x}_{i}^{\intercal} \bm{V}\right)\), which is easily amenable to _memory-limited_ algorithm such as noisy power method (Mitliagkas et al., 2013) or stochastic optimization (Oja, 1982). Both approaches utilize the fact that instead of storing \(d\times d\) matrices, one only needs to store matrix-vector product of size \(d\times 1\), e.g., \(\bm{V}^{\intercal}\bm{x}_{i}\). This is not the case for the approach of Kleindessner et al. (2023). To see this, consider Eqn. (14) without the covariance constraint, i.e., fair PCA with only the mean constraint. Even here, although the objective can be written as \(\sum_{i=1}^{N}\operatorname{tr}\left(\bm{U}^{\intercal}\bm{U}_{\bm{f}}^{ \intercal}\bm{x}_{i}\bm{x}_{i}^{\intercal}\bm{U}_{\bm{f}}\bm{U}\right)\)we must know \(\bm{U_{f}}\) in order to proceed further. Moreover, as \(\bm{U_{f}}\) is of size \(\mathcal{O}(d^{2})\), it cannot be stored nor estimated explicitly. When the covariance constraint is also taken into account, although the matrix in question \(\bm{M_{f,S^{\prime}}}\) is of dimension \((d-1)\times l\), which is within the memory constraint if \(l=\mathcal{O}(1)\), the computation of \(\bm{M_{f,S^{\prime}}}\)_requires_ the knowledge of \(\bm{U_{f}}\). This is because Kleindessner et al. (2023) dealt with the two constraints sequentially (mean first, then covariance), which forced the reparametrization to be done twice and, more importantly, coupling the memory requirement for the computation of \(\bm{U_{f}}\) and \(\bm{M_{f,S^{\prime}}}\).

Statistical consideration.The statistical guarantee (global convergence, sample complexity) of streaming PCA algorithms is often obtained by appropriately bounding the error term and using proper matrix concentration inequalities (Tropp, 2015). For example, for the sample complexity guarantee of noisy power method (Balcan et al., 2016; Hardt and Price, 2014), as the learner performs power method on the empirical covariance \(\sum_{i}\bm{x}_{i}\bm{x}_{i}^{\intercal}\) instead of the true covariance \(\bm{\Sigma}\), the proof proceeds by first showing that the empirical covariance is close enough to the true covariance (i.e., the norm of their error term is sufficiently bounded), which then implies that the variance of the estimation isn't too high. Thus to analyze the error bound of the final iterates \(\bm{V}\) when the approach of Kleindessner et al. (2023) is extended to a streaming setting, one would have to bound the estimation error of \(\bm{M_{f,S^{\prime}}}\bm{U_{f}^{\intercal}}\bm{\Sigma}\bm{U_{f}}\bm{M_{f,S^{ \prime}}}\). There are three sources of estimation error: \(\bm{M_{f,S^{\prime}}},\bm{U_{f}^{\intercal}},\bm{\Sigma}\). Recalling that \(\bm{M_{f,S^{\prime}}}\) consists of the _eigenvectors_ of \(\bm{U_{f}^{\intercal}}\bm{S^{\prime}}\bm{U_{f}}\), one can see that the estimation error of \(\bm{M_{f,S^{\prime}}}\) is actually nontrivially dependent on the estimation quality of _both_\(\bm{U}\) and \(\bm{S^{\prime}}\). Here, we say nontrivially because the error isn't simply bounded in a linear sense via the usual triangle inequality; it requires rather intricate techniques involving eigenvector perturbation theory (Davis and Kahan, 1969; Golub and Loan, 2013), which may require additional assumptions on eigenvalue gaps. As one can see later in the proof, our approach considers both constraints simultaneously and thus allows for a quite simple theoretical analysis.

[MISSING_PAGE_FAIL:25]

We provide some intuition on the assumptions that we impose here. Assumption 1 consists of three parts. The first part, which involves \(\sigma,V,\mathcal{M}\) and \(\mathcal{V}\), ensures that the maximum deviation in mean and covariance of each \(\mathcal{D}_{a}\) are well bounded; this is critical in allowing for us to use proper matrix concentration inequalities (to be described in the next subsection) and has been used in various streaming PCA literature (Bienstock et al., 2022; Huang et al., 2021; Jain et al., 2016). The second part, which involves \(g_{\min}\) and \(f_{\max}\), imposes a bound on the maximum mean separation, \(\bm{f}=\bm{\mu}_{1}-\bm{\mu}_{0}\), \(\ell_{2}\)-wise and angle-wise, respectively. If the mean difference can be arbitrarily large, then the \(\ell_{2}\)-estimation error that one has to achieve becomes arbitrarily small; precisely speaking, \(\|\bm{f}\|_{2}\) acts as a Lipschitz constant. On the other hand, if the mean difference can be arbitrarily small, then the angle-wise estimation error becomes arbitrarily large. The last part, which involves \(p_{\min}\), ensures that both groups are selected with some positive, nonvanishing probability. Assumption 2 is standard in streaming PCA literature to ensure convergence; indeed, if the singular value gap is zero, a definitive convergence result can never be obtained, as the ground-truth solution becomes vague.

### Matrix/Vector Concentration Inequalities

Before moving on to our proof, we review some useful concentration inequalities for our theoretical analysis.

**Definition 4** (Variance of random matrix).: _For a zero-mean random matrix \(\bm{Z}\), its variance is defined as_

\[\operatorname{Var}(\bm{Z})=\max\left(\left\|\mathbb{E}\left[\bm{Z}\bm{Z}^{ \intercal}\right]\right\|_{2},\left\|\mathbb{E}\left[\bm{Z}^{\intercal}\bm{Z} \right]\right\|_{2}\right).\]

_In general, \(\operatorname{Var}(\bm{Z})=\operatorname{Var}(\bm{Z}-\mathbb{E}[\bm{Z}])\)._

**Proposition 1** (Matrix Bernstein inequality (Theorem 6.6.1 of Tropp (2015))).: _Consider a finite collection \(\{\bm{Y}_{j}\}_{j=1}^{b}\) of independent matrices with the same size (\(d_{1}\times d_{2}\)). Suppose they are zero mean and they have uniformly bounded singular values, i.e.,_

\[\mathbb{E}[\bm{Y}_{j}]=\bm{0}\quad\text{ and }\quad\mathbb{P}\left(\|\bm{Y}_{j} \|\leq\mathcal{M}\right)=1\quad\text{for each }j\in[b].\]

_Let \(\mathcal{V}\) be an upper bound of matrix variance, \(\mathcal{V}\geq\operatorname{Var}\left(\bm{Y}_{j}\right)\) for all \(j\in[b]\). Then, for all \(x\geq 0\),_

\[\mathbb{P}\left(\left\|\frac{1}{b}\sum_{j=1}^{b}\bm{Y}_{j}\right\|\geq x \right)\leq(d_{1}+d_{2})\exp\left(\frac{-bx^{2}}{2(\mathcal{V}+\mathcal{M}x/ 3)}\right).\]

_In particular, if \(0\leq x\leq\frac{3\mathcal{V}}{\mathcal{M}}\)._

\[\mathbb{P}\left(\left\|\frac{1}{b}\sum_{j=1}^{b}\bm{Y}_{j}\right\|\geq x \right)\leq(d_{1}+d_{2})\exp\left(\frac{-bx^{2}}{4\mathcal{V}}\right).\]

**Proposition 2** (Vector Hoeffding inequality (Corollary 7 of Jin et al. (2019))).: _There exists an absolute constant \(\mathfrak{c}\) such that if \(\bm{y}_{1},\cdots,\bm{y}_{b}\) be independent random vectors with common dimension \(d\), and assume the following:_

\[\mathbb{E}[\bm{y}_{i}]=0,\quad\bm{y}_{i}\in\operatorname{nSG}(\sigma).\]

_Then, for any \(x\geq 0\),_

\[\mathbb{P}\left(\left\|\frac{1}{b}\sum_{j=1}^{b}\bm{y}_{j}\right\|_{2}\geq x \right)\leq 2d\exp\left(-\frac{bx^{2}}{\mathfrak{c}^{2}\sigma^{2}}\right).\]

**Remark 2**.: _With an additional assumption that the random variables are bounded, we can also consider using vector Bernstein inequality (Lemma 18 of Kohler and Lucchi (2017)), which removes the factor of dimension \(d\) from the RHS of the concentration inequality._

### Proof of Lemma 2 - Bounding Error in Second Moment Gap

Recall that Algorithm 3 performs NPM on the second moment difference matrix \(\bm{S}\) so that an iteration can be written as

\[\bm{W}_{t}=\texttt{QR}(\widehat{\bm{S}}_{t}\bm{W}_{t-1})=\texttt{QR}(\bm{S} \bm{W}_{t-1}+\bm{Z}_{t,1}),\]

where \(\bm{Z}_{t,1}=(\widehat{\bm{S}}_{t}-\bm{S})\bm{W}_{t-1}\) is the noise matrix.

The lemma below provides a general statement on sufficient size \(b\) of blocks to bound the second moment difference error \(\widehat{\bm{S}}_{t}-\bm{S}\) multiplied by some fixed matrices.

**Lemma 4**.: _Let any \(\delta>0\) and matrices \(\bm{M}\in\mathbb{R}^{m\times d}\) and \(\bm{N}\in\mathbb{R}^{d\times n}\) be given. If we choose \(b\geq\frac{4}{p_{\min}}\max\left(1,\frac{8\mathcal{M}^{2}}{9\mathcal{V}}\right) \log\frac{2(m+n)}{\delta}\), then with probability at least \(1-\delta\),_

\[\left\|\bm{M}(\widehat{\bm{S}}_{t}-\bm{S})\bm{N}\right\|_{2}\leq\mathcal{E}_{m +n}^{(\mathrm{S})}\triangleq\sqrt{\frac{32\mathcal{V}}{bp_{\min}}\log\frac{2(m +n)}{\delta}}.\]

We now proceed the proof of Lemma 2. To this end, recall that \(\bm{Z}_{t,1}=(\widehat{\bm{S}}_{t}-\bm{S})\bm{W}_{t-1}\). Firstly, applying Lemma 4 with \(\bm{M}=\bm{I}_{d}\) and \(\bm{N}=\bm{W}_{t-1}\in\mathbb{R}^{d\times m}\), a sufficient condition for having \(5\left\|\bm{Z}_{t,1}\right\|_{2}\leq\epsilon\Delta_{m,\nu}\) with probability at least \(1-\delta\) is that

\[b\geq\frac{400\mathcal{V}}{\epsilon^{2}\Delta_{m,\nu}^{2}p_{\min}}\log\frac{2 (d+m)}{\delta}+\frac{4}{p_{\min}}\max\left(1,\frac{8\mathcal{M}^{2}}{9\mathcal{ V}}\right)\log\frac{2(d+m)}{\delta}.\]

Secondly, applying Lemma 4 with \(\bm{M}=\bm{P}_{m}^{\intercal}\in\mathbb{R}^{m\times d}\) and \(\bm{N}=\bm{W}_{t-1}\in\mathbb{R}^{d\times m}\), a sufficient condition for having \(5\left\|\bm{P}_{m}^{\intercal}\bm{Z}_{t,1}\right\|_{2}\leq\frac{\delta\Delta _{m,\nu}}{2\sqrt{dm}}\) with probability at least \(1-\delta\) is that

\[b\geq\frac{3200dm\mathcal{V}}{\delta^{2}\Delta_{m,\nu}^{2}p_{\min}}\log\frac{4 m}{\delta}+\frac{4}{p_{\min}}\max\left(1,\frac{8\mathcal{M}^{2}}{9\mathcal{ V}}\right)\log\frac{4m}{\delta}.\]

Combining all bounds for \(b\), we conclude that a sufficient block size \(b\) is

\[b=\Omega\left(\frac{\mathcal{V}}{\Delta_{m,\nu}^{2}p_{\min}}\left(\frac{dm}{ \delta^{2}}\log\frac{m}{\delta}+\frac{1}{\epsilon^{2}}\log\frac{d}{\delta} \right)+\frac{\mathcal{M}^{2}}{\mathcal{V}p_{\min}}\log\frac{d}{\delta}\right).\]

Proof of Lemma 4.: Note that \((a_{i},\bm{x}_{i})\)'s are i.i.d. samples (\(i\in\{(t-1)b+1,\ldots,tb\}\)). Consider independent random matrices

\[\bm{Y}_{i}^{(a)}=\bm{M}\left(\bm{x}_{i}\bm{x}_{i}^{\intercal}-(\bm{\Sigma}_{a} +\bm{\mu}_{a}\bm{\mu}_{a}^{\intercal})\right)\bm{N}.\]

For each set \(A\subseteq[b]\), define an event \(E_{A}:=\left\{a_{i}=1_{[i\in A]}\;\forall i\in[b]\right\}\). Note that \(\mathbb{P}[E_{A}]=p_{0}^{b-|A|}p_{1}^{|A|}\). To exploit this, we first apply a peeling argument as follows: for any \(c_{0},c_{1}\in(0,1)\) with \(c_{0}+c_{1}=1\),

\[\mathbb{P}\left(\left\|\bm{M}(\widehat{\bm{S}}_{t}-\bm{S})\bm{N} \right\|_{2}\geq x\right)\] \[=\sum_{r=0}^{b}\sum_{A\in\binom{[b]}{r}}\mathbb{P}\left(\left\| \bm{M}(\widehat{\bm{S}}_{t}-\bm{S})\bm{N}\right\|_{2}\geq x\right|E_{A} \right)\mathbb{P}\left(E_{A}\right)\] \[=\sum_{r=1}^{b-1}\sum_{A\in\binom{[b]}{r}}\mathbb{P}\left(\left\| \frac{1}{r}\sum_{i\in A}\bm{Y}_{i}^{(1)}-\frac{1}{b-r}\sum_{i\in A^{c}}\bm{Y }_{i}^{(0)}\right\|_{2}\geq x\right|E_{A}\right)\mathbb{P}\left(E_{A}\right)\] \[\quad+\mathbb{P}\left(\left\|\frac{1}{b}\sum_{i=1}^{b}\bm{Y}_{i}^ {(0)}\right\|_{2}\geq c_{0}x\right|E_{\emptyset}\right)p_{0}^{b}+\mathbb{P} \left(\left\|\frac{1}{b}\sum_{i=1}^{b}\bm{Y}_{i}^{(1)}\right\|_{2}\geq c_{1}x \right|E_{[b]}\right)p_{1}^{b},\]

where the last inequality holds for the following fact: if \(\|\bm{a}-\bm{b}\|\geq x\) then \(\|\bm{a}\|\geq\lambda x\) or \(\|\bm{b}\|\geq(1-\lambda)x\) for any \(\lambda\in(0,1)\).

Now, we make the crucial observation that _conditioned_ on the event \(E_{A}\), both \(\left\{\bm{Y}_{i}^{(1)}\right\}_{i\in A}\) and \(\left\{\bm{Y}_{i}^{(0)}\right\}_{i\in A^{c}}\) are sets of i.i.d. random zero-mean \(m\times n\) matrices. In addition, with Assumption 1, we have that for each \(i\in A\),

\[\mathbb{P}\left[\left\|\bm{Y}_{i}^{(1)}\right\|_{2}\leq\mathcal{M}\Big{|}\,i \in A\right]=1,\quad\mathrm{Var}\left(\left.\bm{Y}_{i}^{(1)}\right|i\in A \right)\leq\mathcal{V}.\](and analogously for each \(i\in A^{\mathsf{c}}\)). Thus, applying the matrix Bernstein inequality5 (Proposition 1), we obtain the following: when \(0\leq x\leq\frac{3\mathcal{V}}{\mathcal{M}}\),

Footnote 5: Precisely, we use its conditional version where the means and the variances are replaced with their conditional counterparts.

\[\mathbb{P}\left(\left\|(\widehat{\bm{S}}_{t}-\bm{S})\bm{W}_{t-1} \right\|_{2}\geq x\right)\] \[\leq(m+n)\sum_{r=1}^{b-1}p_{0}^{b-r}p_{1}^{r}\sum_{A\in\binom{[b] }{r}}\left\{\exp\left(-\frac{(b-r)c_{0}^{2}x^{2}}{4\mathcal{V}}\right)+\exp \left(-\frac{rc_{1}^{2}x^{2}}{4\mathcal{V}}\right)\right\}\] \[\quad+(m+n)p_{0}^{b}\exp\left(-\frac{bc_{0}^{2}x^{2}}{4\mathcal{V }}\right)+(m+n)p_{1}^{b}\exp\left(-\frac{bc_{1}^{2}x^{2}}{4\mathcal{V}}\right)\] \[\leq(m+n)\sum_{r=0}^{b}\binom{b}{r}p_{0}^{b-r}p_{1}^{r}\left\{ \exp\left(-\frac{(b-r)c_{0}^{2}x^{2}}{4\mathcal{V}}\right)+\exp\left(-\frac{ rc_{1}^{2}x^{2}}{4\mathcal{V}}\right)\right\}\] \[=(m+n)\left\{\left(p_{0}+p_{1}\exp\left(-\frac{c_{1}^{2}x^{2}}{4 \mathcal{V}}\right)\right)^{b}+\left(p_{1}+p_{0}\exp\left(-\frac{c_{0}^{2}x^{ 2}}{4\mathcal{V}}\right)\right)^{b}\right\}<\delta.\]

For this to occur, it suffices for both terms to be bounded by \(\frac{\delta}{2}\). Let us consider only the first term, as the second term follows from symmetry. Taking the log, we have

\[\log\left((1-p_{1})+p_{1}\exp\left(-\frac{c_{1}^{2}x^{2}}{4\mathcal{V}}\right) \right)\leq\frac{1}{b}\log\frac{\delta}{2(m+n)}.\]

Using \(\log(1+y)\leq y\), it suffices to have

\[\frac{1}{b}\log\frac{2(m+n)}{\delta}\leq p_{1}\left(1-\exp\left(-\frac{c_{1}^ {2}x^{2}}{4\mathcal{V}}\right)\right).\]

Using \(y\leq 1-e^{-2y}\) for \(y\in[0,1/2]\), it now suffices to have

\[\frac{8\mathcal{V}}{bp_{1}c_{1}^{2}}\log\frac{2(m+n)}{\delta}\leq x^{2}\leq \min\left(\frac{2\mathcal{V}}{c_{1}^{2}},\frac{9\mathcal{V}^{2}}{\mathcal{M}^{ 2}}\right).\]

Combining this with the other term and, for simplicity6 choosing \(c_{0}=c_{1}=1/2\), we have our desired statement. 

Footnote 6: One could try to optimize for \(c_{0},c_{1}\) to obtain an “optimal” sample complexities. However, from some preliminary computations, this seems to be not worth pursuing, and we conjecture that it would yield the same asymptotic dependency as when \(c_{0}=c_{1}=1/2\).

### Proof of Lemma 3 - Bounding the Final Error

Recall that the iterates are \(\bm{V}_{\tau+1}=\texttt{QR}(\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\widehat{\bm{ \Sigma}}_{\tau}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}_{\tau})\), where \(\widehat{\bm{U}}\) is the output of Algorithm 3, \(\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}:=\bm{I}_{d}-\widehat{\bm{U}}\widehat{\bm{U} }^{\top}\), and \(\widehat{\bm{\Sigma}}_{\tau}:=\frac{1}{B}\sum_{j=(\tau-1)\texttt{B}+1}^{\tau \mathcal{B}}\tilde{\bm{x}}_{j}\tilde{\bm{x}}_{j}^{\top}\) is the sample covariance at time step \(\tau\) of Algorithm 4. This can be rewritten as

\[\bm{V}_{\tau+1}=\texttt{QR}(\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{\Sigma}\bm{ \Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}_{\tau}+\bm{Z}_{\tau,2}),\]

where \(\bm{Z}_{\tau,2}=(\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\widehat{\bm{\Sigma}}_{ \tau}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}-\bm{\Pi}_{\widehat{\bm{U}}}^{\perp} \bm{\Sigma}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp})\bm{V}_{\tau}\) is the noise matrix and \(\bm{\Sigma}=\sum_{a\in\{0,1\}}p_{a}(\bm{\Sigma}_{a}+\bm{\mu}_{a}\bm{\mu}_{a}^{ \intercal})\). By Assumption 1, we have that \(\|\bm{\Sigma}\|_{2}\leq V\).

The lemma below provides a general statement on a sufficient block size \(\mathcal{B}\) to bound the covariance error \(\widehat{\bm{\Sigma}}-\bm{\Sigma}\) multiplied by some fixed matrices.

**Lemma 5**.: _Let any \(\delta>0\) and matrices \(\bm{M}\in\mathbb{R}^{m\times d}\) and \(\bm{N}\in\mathbb{R}^{d\times n}\) be given. If we choose \(\mathcal{B}\geq\frac{4}{9}\frac{\mathcal{M}^{2}}{\mathcal{V}+\mathcal{V}^{2}} \log\frac{4(m+n)}{\delta}\), then the following holds with probability at least \(1-\delta\):_

\[\left\|\bm{M}(\widehat{\bm{\Sigma}}-\bm{\Sigma})\bm{N}\right\|_{2}\leq \mathcal{E}_{m+n}^{(\Sigma)}\triangleq\sqrt{\frac{4(\mathcal{V}+V^{2})}{ \mathcal{B}}\log\frac{4(m+n)}{\delta}}.\]We now proceed the proof of Lemma 3. Fix some \(\bm{V}\in\mathrm{St}(d,k)\), and from the design of our Algorithm 4, \(\widehat{\bm{U}}\) is also fixed. By our given assumption on upper bound of \(\left\|\bm{\Pi}_{\widehat{\bm{U}}}-\bm{\Pi}_{\bm{U}}\right\|_{2}\), and the fact that \(\left\|\bm{V}\right\|_{2}=\left\|\bm{Q}_{k}\right\|_{2}=1\), we have

\[\left\|\bm{Z}_{\tau,2}\right\|_{2} \leq\left\|\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}(\widehat{\bm{ \Sigma}}-\bm{\Sigma})\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\right\|_{2}+2 \|\bm{\Sigma}\|_{2}\|\bm{\Pi}_{\widehat{\bm{U}}}-\bm{\Pi}_{\bm{U}}\|_{2}\] \[\leq\left\|\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}(\widehat{\bm{ \Sigma}}-\bm{\Sigma})\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\right\|_{2}+ \frac{\epsilon\Delta_{k,\kappa}}{10}\]

and, with a similar logic,

\[\left\|\bm{Q}_{k}^{\intercal}\bm{Z}_{\tau,2}\right\|_{2}\leq\left\|\bm{Q}_{k} ^{\intercal}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}(\widehat{\bm{\Sigma}}-\bm{ \Sigma})\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\right\|_{2}+\frac{\delta \Delta_{k,\kappa}}{20\sqrt{dk}}.\]

Applying Lemma 5 with \(\bm{M}=\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\in\mathbb{R}^{d\times d}\) and \(\bm{N}=\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\in\mathbb{R}^{d\times k}\), a sufficient condition for having \(\left\|\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}(\widehat{\bm{\Sigma}}-\bm{\Sigma}) \bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\right\|_{2}\leq\frac{\epsilon\Delta _{k,\kappa}}{10}\) with probability at least \(1-\delta\) is that

\[\mathcal{B}\geq\frac{400(\mathcal{V}+V^{2})}{\epsilon^{2}\Delta_{k,\kappa}^{2 }}\log\frac{4(d+k)}{\delta}+\frac{4}{9}\frac{\mathcal{M}^{2}}{\mathcal{V}+V^{ 2}}\log\frac{4(d+k)}{\delta}.\]

Furthermore, applying Lemma 5 with \(\bm{M}=\bm{Q}_{k}^{\intercal}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\in\mathbb{R} ^{k\times d}\) and \(\bm{N}=\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\in\mathbb{R}^{d\times k}\), a sufficient condition for having \(\left\|\bm{Q}_{k}^{\intercal}\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}(\widehat{\bm{ \Sigma}}-\bm{\Sigma})\bm{\Pi}_{\widehat{\bm{U}}}^{\perp}\bm{V}\right\|_{2} \leq\frac{\delta\Delta_{k,\kappa}}{20\sqrt{dk}}\) with probability at least \(1-\delta\) is that

\[\mathcal{B}\geq\frac{1600dk(\mathcal{V}+V^{2})}{\delta^{2}\Delta_{k,\kappa}^{ 2}}\log\frac{8k}{\delta}+\frac{4}{9}\frac{\mathcal{M}^{2}}{\mathcal{V}+V^{2} }\log\frac{8k}{\delta}.\]

Combining all bounds for \(\mathcal{B}\), we conclude that a sufficient block size \(\mathcal{B}\) to have our desired statement is

\[\mathcal{B}=\Omega\left(\frac{\mathcal{V}+V^{2}}{\Delta_{k,\kappa}^{2}}\left( \frac{dk}{\delta^{2}}\log\frac{k}{\delta}+\frac{1}{\epsilon^{2}}\log\frac{d}{ \delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}+V^{2}}\log\frac{d}{\delta} \right).\]

Proof of Lemma 5.: Note that \(\tilde{\bm{x}}_{j}\)'s are i.i.d. samples from \(\mathcal{D}\) (\(j\in\{(\tau-1)\mathcal{B}+1,\ldots,\tau\mathcal{B}\}\)). Let

\[\bm{Y}_{j}=\bm{M}\left(\tilde{\bm{x}}_{j}\tilde{\bm{x}}_{j}^{\intercal}-\bm{ \Sigma}\right)\bm{N}.\]

Then, \(\bm{Y}_{i}\) are i.i.d. random zero-mean \(d\times k\) matrices across \(j\). In addition, with Assumption 1,

\[\mathbb{P}\left[\left\|\tilde{\bm{x}}\tilde{\bm{x}}^{\intercal}-\bm{\Sigma} \right\|_{2}\leq\mathcal{M}\right]=\sum_{a^{\prime}\in\{0,1\}}\mathbb{P}\left[ \left\|\tilde{\bm{x}}\tilde{\bm{x}}^{\intercal}-\bm{\Sigma}\right\|_{2}\leq \mathcal{M}|\,a=a^{\prime}\right]\mathbb{P}[a=a^{\prime}]=1,\]

and

\[\mathrm{Var}(\tilde{\bm{x}}\tilde{\bm{x}}^{\intercal}) =\left\|\mathbb{E}\left[\mathbb{E}[(\tilde{\bm{x}}\tilde{\bm{x}}^ {\intercal})^{2}|a]\right]-\mathbb{E}\left[\mathbb{E}[\tilde{\bm{x}}\tilde{ \bm{x}}^{\intercal}|a]^{2}\right\|_{2}\] \[\leq\left\|\mathbb{E}\left[\mathbb{E}\left[(\tilde{\bm{x}}\tilde{ \bm{x}}^{\intercal})^{2}|a]-\mathbb{E}[\tilde{\bm{x}}\tilde{\bm{x}}^{\intercal }|a]^{2}\right]\right\|_{2}+\left\|\mathbb{E}\left[\mathbb{E}[\tilde{\bm{x}} \tilde{\bm{x}}^{\intercal}|a]^{2}\right]-\mathbb{E}[\mathbb{E}[\tilde{\bm{x}} \tilde{\bm{x}}^{\intercal}|a]]^{2}\right\|_{2}\] \[\leq\sum_{a\in[0,1]}p_{a}\left\|\underbrace{\left[\mathbb{E}[( \tilde{\bm{x}}\tilde{\bm{x}}^{\intercal})^{2}|a]-\mathbb{E}[\tilde{\bm{x}} \tilde{\bm{x}}^{\intercal}|a]^{2}\right]^{2}}_{\mathrm{Var}(\tilde{\bm{x}} \tilde{\bm{x}}^{\intercal}|a)}+\left\|\mathbb{E}\left[(\bm{\Sigma}_{a}+\bm{\mu} _{a}\bm{\mu}_{a}^{\intercal})^{2}\right]-\mathbb{E}\left[\bm{\Sigma}_{a}+\bm{\mu} _{a}\bm{\mu}_{a}^{\intercal}\right]^{2}\right\|_{2}\] \[\leq\mathcal{V}+p_{0}p_{1}\left\|\bm{\Sigma}_{0}+\bm{\mu}_{0}\bm{ \mu}_{0}^{\intercal}-\bm{\Sigma}_{1}+\bm{\mu}_{1}\bm{\mu}_{1}^{\intercal}\right\| _{2}^{2}\] \[\leq\mathcal{V}+4p_{0}(1-p_{0})V^{2}\] \[\leq\mathcal{V}+V^{2}.\]

Applying the matrix Bernstein inequality (Proposition 1), we obtain the following: when \(0\leq x\leq\frac{3(\mathcal{V}+V^{2})}{\mathcal{M}}\),

\[\mathbb{P}\left(\left\|\frac{1}{\mathcal{B}}\sum_{i=1}^{\mathcal{B}}\bm{Y}_{i} \right\|_{2}\geq x\right)\leq(m+n)\exp\left(-\frac{\mathcal{B}x^{2}}{4( \mathcal{V}+V^{2})}\right)\leq\frac{\delta}{4}.\]

Solving for \(x\), we have

\[\frac{4(\mathcal{V}+V^{2})}{\mathcal{B}}\log\frac{4(m+n)}{\delta}\leq x^{2}\leq 9 \left(\frac{(\mathcal{V}+V^{2})}{\mathcal{M}}\right)^{2},\]

from which the statement naturally follows.

### Bounding the Estimation Error of Mean Difference

**Lemma 6**.: _For any \(\delta>0\), choose \(b\geq\frac{2}{Tp_{\min}}\log\frac{4d}{\delta}\). Then, the following holds with probability at least \(1-\delta\):_

\[\left\|\widehat{\bm{f}}-\bm{f}\right\|_{2}\leq\mathcal{E}^{\rm(f) }\triangleq\sqrt{\frac{8\mathsf{c}^{2}\sigma^{2}}{Tbp_{\min}}\log\frac{4d}{ \delta}},\]

_where \(\mathsf{c}\)_

Proof.: Again, note that \((a_{i},\bm{x}_{i})\)'s are i.i.d. samples (\(i\in[Tb]\)). Consider independent random vectors

\[\bm{y}_{i}^{(a)}=\bm{x}_{i}-\bm{\mu}_{a}.\]

For each \(A\subseteq[Tb]\), \(E_{A}=\{a_{i}=1[i\in A]\;\forall i\in[b]\}\) is an event satisfying \(\mathbb{P}(E_{A})=p_{0}^{Tb-|A|}p_{1}^{|A|}\). To exploit this, we now apply the peeling argument as follows: for any \(c_{0},c_{1}\in(0,1)\) with \(c_{0}+c_{1}=1\),

\[\mathbb{P}\left(\left\|\widehat{\bm{f}}-\bm{f}\right\|_{2} \geq x\right)\] \[=\sum_{r=0}^{Tb}\sum_{A\in\binom{[Tb]}{r}}\mathbb{P}\left( \left\|\widehat{\bm{f}}-\bm{f}\right\|_{2}\geq x\right|E_{A}\right)\mathbb{P} \left(E_{A}\right)\] \[=\sum_{r=1}^{Tb-1}\sum_{A\in\binom{[Tb]}{r}}\mathbb{P}\left( \left\|\frac{1}{r}\sum_{i\in A}\bm{y}_{i}^{(1)}-\frac{1}{Tb-r}\sum_{i\in A^{ \mathsf{c}}}\bm{y}_{i}^{(0)}\right\|_{2}\geq x\right|E_{A}\right)\mathbb{P} \left(E_{A}\right)\] \[\quad+\mathbb{P}\left(\left\|\frac{1}{Tb}\sum_{i=1}^{Tb}\bm{y}_{ i}^{(0)}\right\|_{2}\geq x\right|E_{\emptyset}\right)\mathbb{P}\left(E_{ \emptyset}\right)+\mathbb{P}\left(\left\|\frac{1}{Tb}\sum_{i=1}^{Tb}\bm{y}_{i} ^{(1)}\right\|_{2}\geq x\right|E_{[Tb]}\right)\mathbb{P}\left(E_{[Tb]}\right)\] \[\leq\sum_{r=1}^{Tb-1}\sum_{A\in\binom{[Tb]}{r}}\left\{\mathbb{P} \left(\left\|\frac{1}{Tb-r}\sum_{i\in A^{\mathsf{c}}}\bm{y}_{i}^{(0)}\right\|_ {2}\geq c_{0}x\right|E_{A}\right)+\mathbb{P}\left(\left\|\frac{1}{r}\sum_{i\in A }\bm{y}_{i}^{(1)}\right\|_{2}\geq c_{1}x\right|E_{A}\right)\right\}p_{0}^{Tb- r}p_{1}^{r}\] \[\quad+\mathbb{P}\left(\left\|\frac{1}{Tb}\sum_{i=1}^{Tb}\bm{y}_{ i}^{(0)}\right\|_{2}\geq x\right|E_{\emptyset}\right)p_{0}^{Tb}+\mathbb{P}\left( \left\|\frac{1}{Tb}\sum_{i=1}^{Tb}\bm{y}_{i}^{(1)}\right\|_{2}\geq x\right|E_{ [Tb]}\right)p_{1}^{Tb}\]

Observe that _conditioned_ on the event \(E_{A}\), both \(\left\{\bm{y}_{i}^{(1)}\right\}_{i\in A}\) and \(\left\{\bm{y}_{i}^{(0)}\right\}_{i\in A^{\mathsf{c}}}\) are sets of i.i.d. random zero-mean \(d\)-dimensional vectors. In addition, with Assumption 1, we have that for each \(i\in A\), \(\bm{y}_{i}^{(1)}\in\mathrm{nSG}(\sigma_{1})\) (and analogously for each \(i\in A^{\mathsf{c}}\)). Thus, applying the vector Bernstein inequality (Proposition 2), we obtain the following: for \(x>0\),

\[\mathbb{P}\left(\left\|\widehat{\bm{f}}-\bm{f}\right\|_{2}\geq x\right) \leq 2d\sum_{r=1}^{Tb-1}p_{0}^{Tb-r}p_{1}^{r}\sum_{A\in\binom{[Tb] }{r}}\left\{\exp\left(-\frac{(Tb-r)c_{0}^{2}x^{2}}{\mathsf{c}^{2}\sigma_{0}^{2} }\right)+\exp\left(-\frac{rc_{1}^{2}x^{2}}{\mathsf{c}^{2}\sigma_{1}^{2}} \right)\right\}\] \[\quad+2dp_{0}^{Tb}\exp\left(-\frac{Tbx^{2}}{\mathsf{c}^{2}\sigma_ {0}^{2}}\right)+2dp_{1}^{Tb}\exp\left(-\frac{Tbx^{2}}{\mathsf{c}^{2}\sigma_{1}^ {2}}\right)\] \[\leq 2d\sum_{r=0}^{Tb}\binom{Tb}{r}p_{0}^{Tb-r}p_{1}^{r}\left\{ \exp\left(-\frac{(Tb-r)c_{0}^{2}x^{2}}{\mathsf{c}^{2}\sigma_{0}^{2}}\right)+ \exp\left(-\frac{rc_{1}^{2}x^{2}}{\mathsf{c}^{2}\sigma_{1}^{2}}\right)\right\}\] \[=2d\left(p_{0}+p_{1}\exp\left(-\frac{c_{1}^{2}x^{2}}{\mathsf{c}^{2 }\sigma_{1}^{2}}\right)\right)^{Tb}+2d\left(p_{1}+p_{0}\exp\left(-\frac{c_{0}^ {2}x^{2}}{\mathsf{c}^{2}\sigma_{1}^{2}}\right)\right)^{Tb}<\delta.\]

It suffices for each of both terms to be bounded by \(\frac{\delta}{2}\). Let us consider only the first term, as the second term follows from symmetry. Taking the log, we have

\[\log\left((1-p_{1})+p_{1}\exp\left(-\frac{c_{1}^{2}x^{2}}{\mathsf{c}^{2}\sigma_ {1}^{2}}\right)\right)\leq\frac{1}{Tb}\log\frac{\delta}{4d}.\]Using \(\log(1+y)\leq y\), it suffices to have

\[\frac{1}{Tb}\log\frac{4d}{\delta}\leq p_{1}\left(1-\exp\left(-\frac{c_{1}^{2}x^{2} }{\mathfrak{c}^{2}\sigma_{1}^{2}}\right)\right).\]

Using \(y\leq 1-e^{-2y}\) for \(y\leq 1/2\), it now suffices to have

\[\frac{2\mathfrak{c}^{2}\sigma_{1}^{2}}{Tbp_{1}c_{1}^{2}}\log\frac{4d}{\delta} \leq x^{2}\leq\frac{\mathfrak{c}^{2}\sigma_{1}^{2}}{c_{1}^{2}}.\]

Combining this with the other term and, for simplicity (see the footnote on pg. 25), choosing \(c_{0}=c_{1}=1/2\), we have our desired statement. 

### Proof of Theorem 1 - Sample Complexity for PAFO-learnability

Let us fix \(\varepsilon_{\mathrm{f}},\varepsilon_{\mathrm{o}},\delta\in(0,1)\). From the definition of PAFO-learnability (Definition 2), with a large enough sample size, we must guarantee the following with probability at least \(1-\delta\):

\[\mathrm{tr}\left(\bm{V}^{\intercal}\bm{\Sigma}\bm{V}\right)\geq\mathrm{tr} \left(\bm{V}^{\star\intercal}\bm{\Sigma}\bm{V}^{\star}\right)-\varepsilon_{ \mathrm{f}},\quad\|\bm{\Pi}_{\bm{U}}\bm{V}\|_{2}\leq\varepsilon_{\mathrm{o}}.\]

Let \(\widehat{\bm{U}}\) be the final estimate of the unfair subspace \(\bm{U}\) from Algorithm 1, and let \(\bm{V}\) be the final estimate of \(\bm{Q}_{k}=\bm{V}^{\star}\), whose columns are top \(k\) eigenvectors of \(\bm{\Pi}_{\bm{U}}^{\perp}\bm{\Sigma}\bm{\Pi}_{\bm{U}}^{\perp}\).

We first recall the von Neumann trace inequality (Mirsky, 1975; von Neumann, 1937):

**Proposition 3** (Theorem H.1.g of Marshall et al. (2011)).: _If \(\bm{A},\bm{B}\) are two \(n\times n\) Hermitian matrices, then_

\[\mathrm{tr}(\bm{A}\bm{B})\leq\sum_{i=1}^{n}\lambda_{i}(\bm{A})\lambda_{i}(\bm {B}),\]

_where \(\lambda_{i}(\cdot)\) is the \(i\)-th smallest eigenvalue._

Also, the following lemma implies that the sine angle between subspaces (with the same dimension) and the corresponding Grassmannian (projection) distance are the same.

**Lemma 7**.: _Let \(\bm{A}_{1}\in\mathbb{R}^{d\times k}\) and \(\bm{B}_{1}\in\mathbb{R}^{d\times\ell}\) be semi-orthogonal matrices: \(\bm{A}_{1}^{\intercal}\bm{A}_{1}=\bm{I}_{k}\) and \(\bm{B}_{1}^{\intercal}\bm{B}_{1}=\bm{I}_{\ell}\). Define_

\[\mathrm{dist}(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})) \triangleq\left\|\bm{\Pi}_{\bm{A}_{1}}-\bm{\Pi}_{\bm{B}_{1}} \right\|_{2},\] \[\sin\left(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})\right) \triangleq\left\|\bm{\Pi}_{\bm{A}_{1}}^{\perp}\bm{B}_{1}\right\|_{2}.\]

_Then, (i) if \(k=\ell\),_

\[\mathrm{dist}(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})) =\sin\left(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})\right)\] \[=\sin\left(\mathrm{col}(\bm{B}_{1}),\mathrm{col}(\bm{A}_{1}) \right)=\sqrt{1-\sigma_{\min}(\bm{A}_{1}^{\intercal}\bm{B}_{1})^{2}},\]

_where \(\sigma_{\min}(\bm{M})\) is the minimum singular value of \(\bm{M}\). On the other hand, (ii) if \(k<\ell\),_

\[\mathrm{dist}(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})) =\sin\left(\mathrm{col}(\bm{A}_{1}),\mathrm{col}(\bm{B}_{1})\right)=1\] \[\geq\sin\left(\mathrm{col}(\bm{B}_{1}),\mathrm{col}(\bm{A}_{1}) \right)=\sqrt{1-\sigma_{\min}(\bm{A}_{1}^{\intercal}\bm{B}_{1})^{2}}.\]

Now we can write the optimality (the first inequality) as follows:

\[\mathrm{tr}\left(\bm{Q}_{k}^{\intercal}\bm{\Sigma}\bm{Q}_{k} \right)-\mathrm{tr}\left(\bm{V}^{\intercal}\bm{\Sigma}\bm{V}\right) =\mathrm{tr}\left(\bm{\Sigma}(\bm{Q}_{k}\bm{Q}_{k}^{\intercal}- \bm{V}\bm{V}^{\intercal})\right)\] \[\stackrel{{(a)}}{{\leq}}\sum_{i=1}^{d}\lambda_{i}( \bm{\Sigma})\lambda_{i}(\bm{Q}_{k}\bm{Q}_{k}^{\intercal}-\bm{V}\bm{V}^{ \intercal})\] \[\stackrel{{(b)}}{{\leq}}2k\|\bm{\Sigma}\|_{2}\|\bm{Q}_ {k}\bm{Q}_{k}^{\intercal}-\bm{V}\bm{V}^{\intercal}\|_{2}\] \[\stackrel{{(c)}}{{\leq}}2kV\|\bm{\Pi}_{\bm{V}}^{\perp} \bm{Q}_{k}\|_{2}\leq\varepsilon_{\mathrm{f}}.\]Here, \((a)\) follows from the von Neumann trace inequality, \((b)\) follows from the fact that \(\bm{Q}_{k}\bm{Q}_{k}^{\perp}-\bm{V}\bm{V}^{\intercal}\) is a symmetric matrix of rank at most \(2k\), and \((c)\) follows from Lemma 7. Thus, for the optimality, it suffices to ensure that \(\|\bm{\Pi}_{\tilde{\bm{U}}}^{\perp}\bm{Q}_{k}\|_{2}\leq\frac{\varepsilon_{ \mathrm{o}}}{2kV}\).

Similarly, for the fairness (the second inequality), it suffices to ensure that \(\|\bm{\Pi}_{\bm{U}}\bm{\Pi}_{\tilde{\bm{U}}}^{\perp}\|_{2}\leq\varepsilon_{ \mathrm{o}}\) or \(\|\bm{\Pi}_{\tilde{\bm{U}}}-\bm{\Pi}_{\bm{U}}\|_{2}\leq\varepsilon_{\mathrm{o}}\), as

\[\|\bm{\Pi}_{\bm{U}}\bm{V}\|_{2}\overset{(*)}{=}\left\|\bm{\Pi}_{\bm{U}}\bm{\Pi }_{\tilde{\bm{U}}}^{\perp}\bm{V}\right\|_{2}\leq\left\|\bm{\Pi}_{\bm{U}}\bm{ \Pi}_{\tilde{\bm{U}}}^{\perp}\right\|_{2}\overset{(*)}{\leq}\left\|\bm{\Pi}_{ \tilde{\bm{U}}}-\bm{\Pi}_{\bm{U}}\right\|_{2}\leq\varepsilon_{\mathrm{o}},\]

where \((*)\) follows from \(\bm{V}=\bm{\Pi}_{\tilde{\bm{U}}}^{\perp}\bm{V}\) (due to the design of Algorithm 4) and \((\star)\) follows from Lemma 7.

From Lemma 1 (letting \(\mathcal{T}=\Theta\left(\frac{K_{k,\kappa}}{\Delta_{k,\kappa}}\log\frac{d}{ \varepsilon\delta}\right)\)) and 3 (substituting \(\epsilon\mapsto\frac{\varepsilon_{\mathrm{f}}}{2kV}\)), _given_ that

\[\left\|\bm{\Pi}_{\tilde{\bm{U}}}-\bm{\Pi}_{\bm{U}}\right\|_{2}\leq\eta_{k}= \eta_{k}(\varepsilon_{\mathrm{f}},\varepsilon_{\mathrm{o}},\delta)\triangleq \min\left(\varepsilon_{\mathrm{o}},\frac{\Delta_{k,\kappa}\varepsilon_{\mathrm{ f}}}{40V^{2}k},\frac{\Delta_{k,\kappa}\delta}{160V\sqrt{dk}}\right),\] (17)

a total of \(N_{2}=\mathcal{T}\mathcal{B}\) samples are sufficient in Algorithm 4 for ensuring \(\varepsilon_{\mathrm{f}}\)-optimality with probability at least \(1-\frac{\delta}{2}\), where

\[N_{2}\gtrsim\left(\frac{K_{k,\kappa}(\mathcal{V}+V^{2})}{\Delta_{k,\kappa}^{ 3}}\left(\frac{dk}{\delta^{2}}\log\frac{k}{\delta}+\frac{k^{2}V^{2}}{ \varepsilon_{\mathrm{f}}^{2}}\log\frac{d}{\delta}\right)+\frac{K_{k,\kappa} \mathcal{M}^{2}}{\Delta_{k,\kappa}(\mathcal{V}+V^{2})}\log\frac{d}{\delta} \right)\log\frac{dkV}{\varepsilon_{\mathrm{f}}\delta}.\] (18)

We now focus on obtaining the sample complexity for Algorithm 3 to satisfy Eqn. (17) with probability at least \(1-\frac{\delta}{2}\). Then combining those with a union bound gives us the desired statement.

From hereon and forth, we write

\[\bm{g}\triangleq\bm{\Pi}_{\bm{P}_{m}}^{\perp}\bm{f},\quad\bm{h} \triangleq\frac{1}{\left\|\bm{g}\right\|_{2}}\bm{g},\] \[\widehat{\bm{g}}\triangleq\bm{\Pi}_{\bm{W}_{\mathcal{T}}}^{\perp} \widehat{\bm{f}},\quad\widehat{\bm{h}}\triangleq\frac{1}{\left\|\widehat{\bm {g}}\right\|_{2}}\widehat{\bm{g}}.\]

Also, we introduce the following lemma that provides a general bound of sine angle between a pair of vectors with a small \(\ell_{2}\)-distance.

**Lemma 8**.: _Consider two vectors \(\bm{a},\bm{b}\in\mathbb{R}^{d}\). If we denote the (acute) angle between \(\bm{a}\) and \(\bm{b}\) as \(\theta(\bm{a},\bm{b})\),_

\[\sin\theta(\bm{a},\bm{b})=\sin(\mathrm{span}(\bm{a}),\mathrm{span}(\bm{b}))= \left\|\frac{1}{\left\|\bm{a}\right\|_{2}^{2}}\bm{a}\bm{a}^{\intercal}-\frac{1} {\left\|\bm{b}\right\|_{2}^{2}}\bm{b}\bm{b}^{\intercal}\right\|_{2}.\]

_Consider any \(\epsilon\in\left(0,\frac{\left\|\bm{a}\right\|_{2}}{2}\right]\) and suppose \(\left\|\bm{a}-\bm{b}\right\|_{2}\leq\epsilon\). Then,_

\[\sin\theta(\bm{a},\bm{b})\leq\frac{\sqrt{2}\epsilon}{\left\|\bm{a}\right\|_{2}}.\]

Case 1.Assume \(\bm{g}\neq\bm{0}\). In this case, \(\bm{U}=[\bm{P}_{m}|\bm{h}]\), \(\left\|\bm{g}\right\|_{2}\geq g_{\min}\), and \(\left\|\bm{f}\right\|_{2}\leq f_{\max}\) by Assumption 1. We want to upper-bound the following probability to be \(<\delta/2\):

\[\mathbb{P}\left[\left\|\bm{\Pi}_{\bm{U}}\bm{\Pi}_{\tilde{\bm{U}}}^ {\perp}\right\|_{2}>\eta_{k}\right]\] \[=\mathbb{P}\left[\left(\widehat{\bm{g}}\neq\bm{0}\text{\ \bf and }\left\|\bm{\Pi}_{\bm{U}}-\bm{\Pi}_{\tilde{\bm{U}}}\right\|_{2}>\eta_{k}\right) \text{\ \bf or }\left(\widehat{\bm{g}}=\bm{0}\text{\ \bf and }\left\|\bm{\Pi}_{\bm{U}}\bm{\Pi}_{\tilde{\bm{U}}}^{\perp}\right\|_{2}>\eta_{k} \right)\right]\] \[\leq\mathbb{P}\left[\left\|\bm{\Pi}_{[\bm{P}_{m}|\bm{h}]}-\bm{ \Pi}_{[\bm{W}_{T}|\widehat{\bm{h}}]}\right\|_{2}>\eta_{k}\left|\widehat{\bm{g}} \neq\bm{0}\right]+\mathbb{P}\left[\widehat{\bm{g}}=\bm{0}\right].\]

Let us obtain a sample complexity to bound the first probability term to be \(<\frac{\delta}{4}\). Note that

\[\left\|\bm{\Pi}_{[\bm{P}_{m}|\bm{h}]}-\bm{\Pi}_{[\bm{W}_{T}|\widehat{\bm{h}}]} \right\|_{2}\leq\left\|\bm{\Pi}_{\bm{P}_{m}}-\bm{\Pi}_{\bm{W}_{T}}\right\|_{2}+ \left\|\bm{h}\bm{h}^{\intercal}-\widehat{\bm{h}}\widehat{\bm{h}}^{\intercal} \right\|_{2}.\]Firstly, to yield

\[\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}}\right\|_{2}=\left\| \mathbf{\Pi}_{\bm{W}_{T}}^{\perp}\bm{P}_{m}\right\|_{2}\leq\frac{\eta_{k}}{2}\]

with probability at least \(1-\frac{\delta}{12}\), it is sufficient that

\[T =\Theta\left(\frac{K_{m,\nu}}{\Delta_{m,\nu}}\log\left(\frac{d}{ \eta_{k}\delta}\right)\right),\] (19) \[b =\Omega\left(\frac{\mathcal{V}}{\Delta_{m,\nu}^{2}p_{\min}}\left( \frac{dm}{\delta^{2}}\log\frac{m}{\delta}+\frac{1}{\eta_{k}^{2}}\log\frac{d}{ \delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}p_{\min}}\log\frac{d}{\delta} \right),\] (20)

due to application of Lemma 1 and 2.

Secondly, we aim to make the second term \(\left\|\bm{h}\bm{h}^{\intercal}-\widehat{\bm{h}}\widehat{\bm{h}}^{\intercal} \right\|_{2}\) small with high probability. We first claim that, if \(\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2}\leq\frac{\eta_{\min}}{2}\min\left\{ \frac{\eta_{k}}{\sqrt{2}},1\right\}\), then \(\left\|\bm{h}\bm{h}^{\intercal}-\widehat{\bm{h}}\widehat{\bm{h}}^{\intercal} \right\|_{2}\leq\frac{\eta_{k}}{2}\). Here is the proof of the claim: Since \(\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2}\leq\frac{\left\|\bm{g}\right\|_{2}} {2}\), we have

\[\left\|\bm{h}\bm{h}^{\intercal}-\widehat{\bm{h}}\widehat{\bm{h}}^{\intercal} \right\|_{2}\overset{\text{Lem.\ref{lem:2011}}}{\leq}\frac{\sqrt{2}}{\left\| \bm{g}\right\|_{2}}\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2}\leq\frac{\sqrt{ 2}}{g_{\min}}\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2}\leq\frac{\eta_{k}}{2}.\]

Thus, since

\[\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2} =\left\|\left(\mathbf{\Pi}_{\bm{P}_{m}}^{\perp}-\mathbf{\Pi}_{\bm {W}_{T}}^{\perp}\right)\bm{f}+\mathbf{\Pi}_{\bm{W}_{T}}^{\perp}(\bm{f}- \widehat{\bm{f}})\right\|_{2}\] \[\leq\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}} \right\|_{2}f_{\max}+\left\|\bm{f}-\widehat{\bm{f}}\right\|_{2},\] (21)

we have

\[\mathbb{P}\left[\left\|\bm{h}\bm{h}^{\intercal}-\widehat{\bm{h}} \widehat{\bm{h}}^{\intercal}\right\|_{2}>\frac{\eta_{k}}{2}\Big{|}\widehat{ \bm{g}}\neq\bm{0}\right]\] \[\leq\mathbb{P}\left[\left\|\bm{g}-\widehat{\bm{g}}\right\|_{2}> \frac{g_{\min}}{2}\min\left\{\frac{\eta_{k}}{\sqrt{2}},1\right\}\right|\widehat {\bm{g}}\neq\bm{0}\right]\] \[\leq\mathbb{P}\left[\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{ \Pi}_{\bm{W}_{T}}\right\|_{2}>\frac{g_{\min}}{4f_{\max}}\min\left\{\frac{\eta _{k}}{\sqrt{2}},1\right\}\right|\widehat{\bm{g}}\neq\bm{0}\right]+\mathbb{P} \left[\left\|\bm{f}-\widehat{\bm{f}}\right\|_{2}>\frac{g_{\min}}{4}\min\left\{ \frac{\eta_{k}}{\sqrt{2}},1\right\}\right|\widehat{\bm{g}}\neq\bm{0}\right].\]

To have

\[\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}}\right\|_{2}\leq\frac {g_{\min}}{4f_{\max}}\min\left\{\frac{\eta_{k}}{\sqrt{2}},1\right\}\]

with probability at least \(1-\frac{\delta}{12}\), it is sufficient that

\[T =\Theta\left(\frac{K_{m,\nu}}{\Delta_{m,\nu}}\log\left(\frac{d}{ \eta_{k}\delta}\frac{f_{\max}}{g_{\min}}\right)\right),\] (22) \[b =\Omega\left(\frac{\mathcal{V}}{\Delta_{m,\nu}^{2}p_{\min}}\left( \frac{dm}{\delta^{2}}\log\frac{m}{\delta}+\frac{f_{\max}^{2}}{\eta_{k}^{2}g_{ \min}^{2}}\log\frac{d}{\delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}p_{\min }}\log\frac{d}{\delta}\right).\] (23)

Moreover, to have

\[\left\|\bm{f}-\widehat{\bm{f}}\right\|_{2}\leq\frac{g_{\min}}{4}\min\left\{ \frac{\eta_{k}}{\sqrt{2}},1\right\}\]

with probability at least \(1-\frac{\delta}{12}\), it is sufficient that

\[Tb\geq\frac{128e^{2}\sigma^{2}}{p_{\min}g_{\min}^{2}}\max\left\{\frac{2}{\eta_{ k}^{2}},1\right\}\log\frac{48d}{\delta}.\] (24)

Next, we obtain a sample complexity to bound the second probability term \(\mathbb{P}\left[\widehat{\bm{g}}=\bm{0}\right]<\frac{\delta}{4}\). Because of Eqn. (21),

\[\mathbb{P}\left[\widehat{\bm{g}}=\bm{0}\right]\leq\mathbb{P}\left[\left\|\bm{g}- \widehat{\bm{g}}\right\|_{2}\geq\left\|\bm{g}\right\|_{2}\right]\]\[\leq\mathbb{P}\left[\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W }_{T}}\right\|_{2}\geq\frac{g_{\min}}{2f_{\max}}\right]+\mathbb{P}\left[ \left\|\bm{f}-\bm{\hat{f}}\right\|_{2}\geq\frac{g_{\min}}{2}\right].\]

To have \(\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}}\right\|_{2}<\frac{g _{\min}}{2f_{\max}}\) with probability at least \(1-\frac{\delta}{8}\), it suffices to have a sample complexity in Eqn. (22) and (23). Also, to have \(\left\|\bm{f}-\bm{\hat{f}}\right\|_{2}<\frac{g_{\min}}{2}\) with probability at least \(1-\frac{\delta}{8}\), it suffices to have a sample complexity in Eqn. (24).

Combining the bounds in the **Case 1** with a union bound, we obtain a full sample complexity for Algorithm 3 as follows: a total of \(N_{1}^{(1)}=Tb\) samples are sufficient for ensuring \((\varepsilon_{t},\varepsilon_{o})\)-PAFO-learnability with probability at least \(1-\delta\), where

\[N_{1}^{(1)} =\Omega\left(\frac{K_{m,\nu}}{p_{\min}}\left\{\frac{\mathcal{V}} {\Delta_{m,\nu}^{3}}\left(\frac{dm}{\delta^{2}}\log\frac{m}{\delta}+\frac{f_{ \max}^{2}}{\eta_{k}^{2}g_{\min}^{2}}\log\frac{d}{\delta}\right)+\frac{\mathcal{ M}^{2}}{\mathcal{V}\Delta_{m,\nu}}\log\frac{d}{\delta}\right\}\log\left(\frac{d}{ \eta_{k}\delta}\frac{f_{\max}}{g_{\min}}\right)\right.\] \[\qquad\left.+\frac{\sigma^{2}}{p_{\min}g_{\min}^{2}\eta_{k}^{2}} \log\frac{d}{\delta}\right)\] \[=\tilde{\Omega}\left(\frac{K_{m,\nu}}{p_{\min}}\left\{\frac{ \mathcal{V}}{\Delta_{m,\nu}^{3}}\left(\frac{dm}{\delta^{2}}+\frac{f_{\max}^{2 }}{\eta_{k}^{2}g_{\min}^{2}}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}\Delta_{ m,\nu}}\right\}+\frac{\sigma^{2}}{p_{\min}g_{\min}^{2}\eta_{k}^{2}} \right).\] (25)

Case 2.Assume \(\bm{g}=\bm{0}\). In this case, \(\bm{U}=\bm{P}_{m}\). We want to upper-bound the follow probability to be \(<\delta/2\):

\[\mathbb{P}\left[\left\|\mathbf{\Pi}_{\bm{U}}\mathbf{\Pi}_{\bm{ \tilde{O}}}^{\perp}\right\|_{2}>\eta_{k}\right]\] \[=\mathbb{P}\left[\left(\bm{\widehat{g}}=\bm{0}\text{ and }\left\|\mathbf{\Pi}_{\bm{U}}-\mathbf{\Pi}_{\bm{\tilde{O}}}\right\|_{2}> \eta_{k}\right)\text{ or }\left(\bm{\widehat{g}}\neq\bm{0}\text{ and }\left\|\mathbf{\Pi}_{\bm{U}}\mathbf{\Pi}_{\bm{ \tilde{O}}}^{\perp}\right\|_{2}>\eta_{k}\right)\right]\] \[\leq\mathbb{P}\left[\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi }_{\bm{W}_{T}}\right\|_{2}>\eta_{k}|\,\bm{\widehat{g}}=\bm{0}\right]+\mathbb{ P}\left[\left\|\mathbf{\Pi}_{\bm{P}_{m}}\mathbf{\Pi}_{\bm{[W}_{T}]\bm{\widehat{h}}}^{ \perp}\right\|_{2}>\eta_{k}\left|\bm{\widehat{g}}\neq\bm{0}\right].\]

The first probability term is bounded by \(\frac{\delta}{4}\) given that a sufficient number of samples just like Eqn. (19) and (20). To bound the second one, note that \(\bm{\widehat{h}}\) is orthogonal to every column of \(\bm{W}_{T}\) (_i.e.,_\(\mathbf{\Pi}_{\bm{W}_{T}}\bm{\widehat{h}}=\bm{0}\)). Thus,

\[\left\|\mathbf{\Pi}_{\bm{P}_{m}}\mathbf{\Pi}_{\bm{[W}_{T}]\bm{ \widehat{h}}}^{\perp}\right\|_{2} =\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{P}_{m}}\left( \mathbf{\Pi}_{\bm{W}_{T}}+\bm{\widehat{h}}\bm{\widehat{h}}^{\intercal}\right) \right\|_{2}\] \[=\left\|\mathbf{\Pi}_{\bm{P}_{m}}\mathbf{\Pi}_{\bm{W}_{T}}^{\perp} -\left(\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}}\right)\bm{\widehat{ h}}\bm{\widehat{h}}^{\intercal}\right\|_{2}\] \[\leq 2\left\|\mathbf{\Pi}_{\bm{P}_{m}}-\mathbf{\Pi}_{\bm{W}_{T}} \right\|_{2},\]

which can be bounded by \(<\eta_{k}\) with probability at least \(1-\frac{\delta}{4}\) given that a sufficient number of samples just like Eqn. (19) and (20) again. Therefore, a total of \(N_{1}^{(2)}=Tb\) samples are sufficient for ensuring \((\varepsilon_{t},\varepsilon_{o})\)-PAFO-learnability with probability at least \(1-\delta\), where

\[N_{1}^{(1)} =\Omega\left(\frac{K_{m,\nu}}{p_{\min}}\left\{\frac{\mathcal{V}}{ \Delta_{m,\nu}^{3}}\left(\frac{dm}{\delta^{2}}\log\frac{m}{\delta}+\frac{1}{ \eta_{k}^{2}}\log\frac{d}{\delta}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V} \Delta_{m,\nu}}\log\frac{d}{\delta}\right\}\log\left(\frac{d}{\eta_{k}\delta} \right)\right)\] \[=\tilde{\Omega}\left(\frac{K_{m,\nu}}{p_{\min}}\left\{\frac{ \mathcal{V}}{\Delta_{m,\nu}^{3}}\left(\frac{dm}{\delta^{2}}+\frac{1}{\eta_{k}^{2 }}\right)+\frac{\mathcal{M}^{2}}{\mathcal{V}\Delta_{m,\nu}}\right\}\right).\] (26)

Combining **Case 1** (Eqn. (25)) and **Case 2** (Eqn. (26)) as \(N_{1}=\mathbbm{1}[\bm{g}\neq\bm{0}]N_{1}^{(1)}+\mathbbm{1}[\bm{g}=\bm{0}]N_{1}^{(2)}\), we have our desired statement.

Lastly, we provide the missing proof for our lemmas:

Proof of Lemma 7.: Let

\[\bm{A}=\left\lfloor\underbrace{\bm{A}_{1}}_{k}\right\rfloor\underbrace{\bm{A}_ {2}}_{d-k}\quad\text{and}\quad\bm{B}=\left\lfloor\underbrace{\bm{B}_{1}}_{ \ell}\right\rfloor\underbrace{\bm{B}_{2}}_{d-\ell}\]be \(d\times d\) orthogonal matrices. We first show that \(\sin\left(\operatorname{col}(\bm{A}_{1}),\operatorname{col}(\bm{B}_{1})\right)= \left\lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}\). It can be proven as

\[\left\lVert\bm{\Pi}_{\bm{A}_{1}}^{\perp}\bm{B}_{1}\right\rVert_{2}=\left\lVert \bm{A}_{2}\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}\leq\left\lVert\bm {A}_{2}^{\intercal}\bm{A}_{2}\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2} \leq\left\lVert\bm{A}_{2}\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}.\]

Thus, from now on, we will work with \(\left\lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}\) instead of \(\sin\left(\operatorname{col}(\bm{A}_{1}),\operatorname{col}(\bm{B}_{1})\right)\).

The rest of the proof borrows the idea from the proof of Golub and Loan (2013, Theorem 2.5.1). Observe that

\[\operatorname{dist}(\operatorname{col}(\bm{A}_{1}),\operatorname {col}(\bm{B}_{1})) =\left\lVert\bm{A}^{\intercal}(\bm{A}_{1}\bm{A}_{1}^{\intercal}- \bm{B}_{1}\bm{B}_{1}^{\intercal})\bm{B}\right\rVert_{2}\] \[=\left\lVert\begin{bmatrix}\bm{0}&\bm{A}_{1}^{\intercal}\bm{B}_{2 }\\ -\bm{A}_{2}^{\intercal}\bm{B}_{1}&\bm{0}\end{bmatrix}\right\rVert_{2}\] \[=\max\left\{\left\lVert\bm{B}_{2}^{\intercal}\bm{A}_{1}\right\rVert _{2},\left\lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}\right\}.\]

Note that \(\bm{A}_{2}^{\intercal}\bm{B}_{1}\) and \(\bm{A}_{2}^{\intercal}\bm{B}_{1}\) are submatrices of a \(d\times d\) orthogonal matrix \(\bm{Q}\) defined as

\[\bm{Q}\triangleq\bm{A}^{\intercal}\bm{B}=\begin{bmatrix}\bm{A}_{1}^{\intercal }\bm{B}_{1}&\bm{A}_{1}^{\intercal}\bm{B}_{2}\\ \bm{A}_{2}^{\intercal}\bm{B}_{1}&\bm{A}_{2}^{\intercal}\bm{B}_{2}\end{bmatrix}.\]

Readers might notice that \(\bm{A}_{1}^{\intercal}\bm{B}_{1}\in\mathbb{R}^{k\times\ell}\). For any unit vector (in 2-norm) \(\bm{x}\in\mathbb{R}^{k}\),

\[1=\left\lVert\bm{Q}\begin{bmatrix}\bm{x}\\ \bm{0}\end{bmatrix}\right\rVert_{2}^{2}=\left\lVert\begin{bmatrix}\bm{A}_{1}^ {\intercal}\bm{B}_{1}\bm{x}\\ \bm{A}_{2}^{\intercal}\bm{B}_{1}\bm{x}\end{bmatrix}\right\rVert_{2}^{2}= \left\lVert\bm{A}_{1}^{\intercal}\bm{B}_{1}\bm{x}\right\rVert_{2}^{2}+\left \lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\bm{x}\right\rVert_{2}^{2}.\]

Thus,

\[\left\lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\right\rVert_{2}^{2} =\max_{\bm{x}\in\mathbb{R}^{k},\left\lVert\bm{x}\right\rVert_{2 }=1}\left\lVert\bm{A}_{2}^{\intercal}\bm{B}_{1}\bm{x}\right\rVert_{2}^{2}\] \[=1-\min_{\bm{x}\in\mathbb{R}^{k},\left\lVert\bm{x}\right\rVert_{2 }=1}\left\lVert\bm{A}_{1}^{\intercal}\bm{B}_{1}\bm{x}\right\rVert_{2}^{2}\] \[=\begin{cases}1-\sigma_{\min}(\bm{A}_{1}^{\intercal}\bm{B}_{1})^{2 },&\text{if }k\geq\ell,\\ 1,&\text{if }k<\ell.\end{cases}\]

Likewise, working with \(\bm{Q}^{\intercal}\), one can deduce that

\[\left\lVert\bm{B}_{2}^{\intercal}\bm{A}_{1}\right\rVert_{2}^{2} =\max_{\bm{y}\in\mathbb{R}^{\ell},\left\lVert\bm{y}\right\rVert_ {2}=1}\left\lVert\bm{B}_{2}^{\intercal}\bm{A}_{1}\bm{y}\right\rVert_{2}^{2}\] \[=1-\min_{\bm{y}\in\mathbb{R}^{\ell},\left\lVert\bm{y}\right\rVert _{2}=1}\left\lVert\bm{B}_{1}^{\intercal}\bm{A}_{1}\bm{y}\right\rVert_{2}^{2}\] \[=\begin{cases}1-\sigma_{\min}(\bm{B}_{1}^{\intercal}\bm{A}_{1})^{2 },&\text{if }k\leq\ell,\\ 1,&\text{if }k>\ell.\end{cases}\]

Therefore, our desired statement naturally follows. 

Proof of Lemma 8.: To prove the first claim, let \(\bm{u}\) and \(\bm{v}\) be unit (\(\ell_{2}\) norm) vectors in direction of \(\bm{a}\) and \(\bm{b}\), respectively. Then, by Lemma 7,

\[\left\lVert\bm{u}\bm{u}^{\intercal}-\bm{v}\bm{v}^{\intercal}\right\rVert_{2}^{ 2}=1-\sigma_{\min}(\bm{u}^{\intercal}\bm{v})^{2}=1-\left\lvert\bm{u}^{ \intercal}\bm{v}\right\rvert^{2}=1-\cos^{2}\theta(\bm{u},\bm{v})=\sin^{2} \theta(\bm{a},\bm{b}).\]

Now, observe that \(\left\lVert\bm{b}\right\rVert_{2}\geq\left\lVert\bm{a}\right\rVert_{2}-\epsilon \geq\left\lVert\bm{a}\right\rVert_{2}/2\) and

\[2(\left\lVert\bm{a}\right\rVert_{2}\left\lVert\bm{b}\right\rVert_{2}- \left\langle\bm{a},\bm{b}\right\rangle)\leq\left\lVert\bm{a}\right\rVert_{2}^{2} +\left\lVert\bm{b}\right\rVert_{2}^{2}-2\left\langle\bm{a},\bm{b}\right\rangle= \left\lVert\bm{a}-\bm{b}\right\rVert_{2}^{2}\leq\epsilon^{2}.\]

Thus,

\[\sin^{2}\theta(\bm{a},\bm{b}) =(1+\cos\theta(\bm{a},\bm{b}))(1-\cos\theta(\bm{a},\bm{b}))\leq 2(1-\cos \theta(\bm{a},\bm{b}))\] \[=2\frac{\left\lVert\bm{a}\right\rVert_{2}\left\lVert\bm{b} \right\rVert_{2}-\left\langle\bm{a},\bm{b}\right\rangle}{\left\lVert\bm{a} \right\rVert_{2}\left\lVert\bm{b}\right\rVert_{2}}\leq\frac{2\epsilon^{2}}{ \left\lVert\bm{a}\right\rVert_{2}^{2}}.\]More Experiments

### Synthetic Example

We randomly generated two different group-conditional distributions as 10-dimensional multivariate Gaussians with different mean vectors \(\bm{\mu}_{0}\) and \(\bm{\mu}_{1}\), satisfying \(\bm{\mu}=(1-p)\bm{\mu}_{0}+p\bm{\mu}_{1}=\bm{0}\), and different covariance matrices \(\bm{\Sigma}_{0}\) and \(\bm{\Sigma}_{1}\). We choose the sampling probability parameter \(p\) as 0.2, which induces asymmetric sampling between two sensitive attributes. The covariance matrices are designed so that both of their eigenvalue spectra, \(\{\sigma_{1},\ldots,\sigma_{d}\}\), have _power-law_ decay as many practical datasets do (Liu et al., 2015), _i.e.,_\(\sigma_{j}=\Theta(j^{-\alpha})\) for some decay parameter \(\alpha\geq 1\).

We first run and compare three different algorithms: vanilla NPM (without any fairness constraint), mean-matched NPM (with only constraint \(\bm{V}^{\intercal}\bm{f}=\bm{0}\)), and FNPM with \(m=3\). To ease the visualization, we project the sampled distributions onto a 2-dimensional subspace (_i.e.,_ running 2-PCA). After running three algorithms for ten iterations and with a block size of \(b=B=1000\), we randomly sample 1000 data points and visualize the results of projecting the data points in Figure 2. In particular, for FNPM, we run 50 iterations for unfair subspace estimation (Algorithm 3) and run the other 50 iterations for PCA (Algorithm 4). We observe that FNPM does indeed enforce both mean-matching and (partial) covariance-matching, despite the setting being streaming and having asymmetric sampling probability.

### Additional Results on CelebA Dataset

In Figures 3-5, we provide additional experimental results on the CelebA dataset (Liu et al., 2015). We consider three sensitive attributes in the CelebA dataset: "Eyeglasses", "Mouth Slightly Open", and "Goattee". In all the figures, False is when the sensitive attribute is absent; True is otherwise.

Figure 3 presents the main results for the new attributes. The first row shows the original images, the second row shows the images after projecting them to \(\operatorname{col}(\bm{V}^{\star})\), and the third row shows the images after projecting them to the estimated unfair subspace, \(\operatorname{col}(\bm{\hat{U}})\). Here, both \(\bm{\hat{U}}\) and \(\bm{V}^{\star}\) are obtained from our FNPM; specifically speaking, they are obtained from Algorithm 3 and Algorithm 4, respectively. While we've used \(m=2\) for Figure 1 in the main text, we use \(m=5\) here.

We then perform the two ablation studies as described in the main text, one on the dimension \(m\) of unfair subspace and another on the block size \(b\) of Algorithm 3, for the additional sensitive attributes considered. In Figure 4, we vary \(m\in\{1,2,5,10\}\): as \(m\) increases, more features of images are "erased", making the images from the two sensitive groups less distinguishable. At the same time, more semantically meaningful features are erased as well, resulting in rather "alien-like" images. In Figure 5, we vary \(b\in\{32000,8000,3200,1600\}\): as \(b\) increases, we have a more accurate estimation of unfair subspace \(\operatorname{col}(\bm{U})\), resulting in more indistinguishable images (in sensitive attributes). As soon as the batch size \(b\) exceeds a certain threshold (e.g., \(8000\) for "Goattee"), \(\operatorname{col}(\bm{U})\) is estimated very well, and the unfair features are cleanly recovered, as it can be seen in the bottom rows.

### Full Results on UCI Dataset

The full experiment results of fair PCA and downstream tasks on UCI datasets are provided. Again, our method is competitive to the existing fair PCA algorithms across the considered UCI datasets.

Figure 2: **Synthetic Example**: Vanilla NPM v.s. Mean-matched NPM v.s. FNPM (ours).

Figure 3: CelebA, Additional results (\(m=5\)).

Figure 4: CelebA, Additional ablation on \(m\in\{1,2,5,10\}\)

Figure 5: CelebA, Additional ablation on \(b\in\{32000,8000,3200,1600\}\)

[MISSING_PAGE_EMPTY:40]

[MISSING_PAGE_EMPTY:41]

[MISSING_PAGE_EMPTY:42]