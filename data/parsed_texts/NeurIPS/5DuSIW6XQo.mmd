# Benchmarking Self-Supervised Video Representation Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Self-supervised learning is an effective way for label-free model pre-training, especially in the video domain where labeling is expensive. Existing self-supervised works in the video domain use varying experimental setups to demonstrate their effectiveness and comparison across approaches becomes challenging with no standard benchmark. In this work, we first provide a benchmark that enables a comparison of existing approaches on the same ground. Next, we study five different aspects of self-supervised learning important for videos; 1) dataset size, 2) complexity, 3) data distribution, 4) data noise, and, 5) feature analysis. To facilitate this study, we focus on six different methods along with six different network architectures and perform an extensive set of experiments on five different datasets with an evaluation of two different downstream tasks. We present several interesting insights from this study which span across different properties of pretraining and target datasets, pretext-tasks, and model architectures among others. Furthermore, we extend these findings to Video Foundation models (ViFMs). Finally, we put some of these insights to the real test and propose an approach that requires a limited amount of training data and outperforms existing state-of-the-art approaches which use 10x pretraining data. We believe this work will pave the way for researchers to a better understanding of self-supervised representation learning in videos.

## 1 Introduction

Deep learning models require a large amount of labeled data for their training. Obtaining annotations at large-scale needs a lot of effort and it becomes even more challenging as we shift from image to video domain. There are several interesting directions focusing on this issue such as domain adaptation [74], knowledge distillation [20], semi-supervised learning [77], self-supervision [31] and weakly-supervised learning [56], which attempts to rely on the knowledge learned from existing source datasets and transfer to new target datasets with minimal labels. Among these approaches, self-supervised learning use pretext task as supervisory signal and does not require any labels on source datasets which makes it more favorable.

In recent years, we have seen great progress in self-supervised learning (SSL) in video domain [75; 32; 78; 69; 49; 10]. More recently, the focus is more towards context-based learning which involves modifying input data such that to derive a classification [73; 13; 75; 32], reconstruction [78; 10] or generative [67; 58; 24; 63; 46] signal which can be used as a learning objective. The main focus of these works is designing a pretext task that is computationally inexpensive and which provides a strong supervisory signal such that the model learns meaningful _spatio-temporal_ features.

Despite this great progress, it is non-trivial to compare these approaches against each other due to a lack of standard protocols. These methods are evaluated under different conditions and there is no standard benchmark to evaluate the fair effectiveness of these methods. A recent study [(62)] attempts to take a step towards this direction, but it is mainly focused on downstream learning, without exploring the self-supervision aspect which is one of the main goals in our study. In this work, we present a benchmark where important self-supervised pre-training parameters are kept consistent across methods for a fair comparison. With the help of this benchmark, we study several critical aspects which are important for self-supervised learning; _1) effect of pretraining dataset size, 2) task complexity, 3) generalization under distribution shift, 4) robustness against data noise, 5) properties of learned features._ Fig. 1 provides an overview.

The proposed benchmark includes a large-scale assessment of context-based representative self-supervised methods for video representation learning. We analyze two different factors: 1) _learning objective_ which includes _contrastive_ vs _non-contrastive_, and 2) _data transformation_ that comprises three categories namely, _spatial_, _temporal_, and _spatio-temporal_. We study six different pretext tasks with six different models and perform our experiments on five different action recognition datasets and evaluate these approaches on two different downstream tasks, action recognition, and video retrieval. Furthermore, we extend the study to recently developed video foundation models.

We observe some interesting insights in this benchmark; 1) Contrastive tasks are fast learners but are less robust against data noise, 2) there is no direct relation that increase in pretext task complexity leads to better understanding of spatio-temporal representation learning, 3) _temporal_ based pretext tasks are more difficult to solve than _spatial_ and _spatio-temporal_, 4) spatio-temporal task can solve the pretext task independent of data distribution shifts, and finally, 5) we empirically show that these pretext tasks learn complementary features across factors such as model architecture, dataset distributions, dataset size, and pretext task. Our contributions are threefold:

* We present a benchmark for self-supervised video representation learning to compare different pretext tasks under a similar experimental setup.
* We perform extensive analysis on 5 important factors for self-supervised learning in videos; 1) dataset size, 2) task complexity, 3) distribution shift, 4) data noise, and, 5) feature analysis.
* Finally, we put some of our insights from this study to test and propose a simple approach that outperforms existing state-of-the-art methods on video action recognition with a limited

Figure 1: **Overview of proposed benchmark.** We study five different aspects in this benchmark study. Starting from left, 1) we show the analysis of _effect of dataset size vs training time_. As the dataset size increases, variation in performance decreases even with longer training time, 2) We show the effect of _task complexity_ (C1, C2, C3 - Different complexities). Bottom figure shows use case of how complexity increases for the RotNet task, and, top figure shows how the performance varies for the R21D network, 3) With different _data distribution shifts_, the third sub-figure shows the impact of _target_ data distribution on the _source_ data, 4) We look into another data distribution shift due to introduction of noise. We see how _non-contrastive_ tasks are more robust than _contrastive_ ones even with increasing levels of severity of noise. The bottom part shows an example for each type of noise. Clips are provided in supplementary, and, 5) Finally, we further analyze whether the features learn _orthogonal_ information. In this sub-figure, we show that using different architectures as teachers can substantially improve performance even in a low-data regime.

amount of pretraining data. Additionally, based on our findings, we put down a set-up recipe for future self-supervised learning algorithms to build upon.

## 2 Related Work

Self-supervised learningThere are several works in the domain of self-supervised learning for video representation learning [(31; 55)]. These approaches can be grouped into two main categories on the basis of pretext task: 1) context-based [(34; 71; 3; 19; 73; 61; 76; 13; 30; 69; 49; 10; 16; 23; 50)], and 2) cross-modal [(48; 53; 1)]. Cross-modal approaches use multiple modalities such as audio, video, optical flow, and camera positions, and rely on consistencies across these modalities. Context-based learning exploits data transformations to derive supervisory signals for training the model. Context-based pretraining tasks have evolved a lot in the past few years. Our work explores the domain of how much variation in learned representations under different transformations. In contrast to other approaches, context-based approaches exploit the spatial and temporal information independently by several transformations [(43; 19; 75; 7; 49; 69)]. Recent works have started to transform the spatial and temporal domain together [(34; 42; 61; 78; 10)]. Incorporating multiple modalities improves performance, but, it's not available for all datasets, especially large-scale datasets. In this work, we restrict our focus to single-modality (RGB) approaches.

Self-supervised benchmarkingThere are some prior efforts focusing on benchmarking self-supervised learning in the image domain. In [(21)], the authors provide a detailed analysis of image-based self-supervised learning approaches and study how dataset size scaling affects the learned representations. Similarly in [(35)], the authors analyze how different model architectures play a role in visual self-supervised learning. In both these works, the authors did not focus on the importance of various pretext tasks themselves but only showed how certain pretext tasks can be improved. Therefore, their main focus was on downstream tasks rather than pretext learning. We, on the other hand, study different pretext tasks and analyze how various aspects affect feature learning. Moreover, these works are focused on the image domain, whereas we focus on the video domain. In recent work, [(18)], a study was performed to better understand unsupervised learning in the video domain. It explored the use of several pre-text tasks from the image domain and applied them to videos. We are not merely focusing on down-stream tasks and our attention is on the self-supervised aspect which includes factors such as data subset size, task complexity, dataset distribution, and noise robustness.

## 3 Self-Supervised Configurations

We first describe the pretext tasks used in our study along with their categorization followed by details of this benchmark including network architectures, datasets, downstream tasks and evaluations.

### Tasks categorization

We analyze two different aspects of video pretext tasks: 1) transformations applied to data, and 2) learning objectives. Data transformations include, _spatial-based (S)_, _temporal-based (T)_ and _spatiotemporal (ST)_. _Spatial_ transformations include reshuffling of spatial patches, temporal consistent data augmentation, or rotation of images/patches. _Temporal_ tasks involve permutation classification of frames/clip, order verification, clips sampling at different paces, or, contrastive learning from temporal triplets. _Spatio-temporal_ tasks include those in which we modify both of these parameters simultaneously. This includes dilated sampling and simultaneous frame reconstruction, shuffling spatial and temporal domains, or, speed prediction, and contrastive visual features. Learning objectives can be either _contrastive_ [(11)] or _non-contrastive_ such as [(78)].

Following this categorization, we select at least two representative pretext tasks from each _transformation_ category, one _contrastive_ and one _non-contrastive_. We study the following pretext tasks: RotNet (Rot) [(32)], Video Clip Order Prediction (VCOP) [(75)], Playback Rate Prediction (PRP) [(78)], Spatiotemporal Contrastive Video Representation Learning (CVRL) [(49)], Temporal DiscriminativeLearning (TDL) (69) and Relative Speed Perception network (RSPNet) (10). The description of tasks are provided in the supplementary (Section C).

### Benchmark details

This section standardizes the conditions used by our benchmark to compare different pretext tasks. Further explanation for using these conditions are outlined in the supplementary.

_Datasets:_ We experiment with two different dataset types, 1) where appearance is more important, and 2) where time is more important. For appearance based, we use Kinetics-400 (33), UCF101 (57), and HMDB51 (38), where appearance is more important (recognize activity with a single frame) than temporal aspect, and for temporal aspect, we use Something Something-V2 (22) and Diving48 (39), where temporal information plays a significant role (require few frames to recognize activity). More details are in the supplementary.

_Spatio-temporal architectures:_ We consider three different network capacities, 1) small-capacity, 2) medium-capacity, and large-capacity. For small capacity networks, we use ShuffleNet V1 2.0X (79), whereas for medium capacity we focus on R(2+1)D (65) (R21D). We do not include large capacity networks in our main benchmark in the interest of computational efficiency; additional results for such a model, VideoSwin (41) is shown in the supplementary.

_Downstream tasks:_ We show results and analysis on two different downstream tasks - _action recognition_ and _clip retrieval_. These two tasks are the most prominent in the field of self-supervised learning in videos. Full finetuning is performed as opposed to linear probing to adapt models.

_Evaluation and Analysis:_ We use top-1 accuracy for action recognition and top-K for Clip retrieval. For robustness performance, we calculate the relative robustness score \((R_{s})\) using original accuracy on clean test set \((A_{c})\) and perturbed accuracy on noisy test set\((A_{p})\) as \(R_{s}=\frac{A_{c}-A_{p}}{A_{s}}\). Centered Kernel alignment (CKA) (44) maps illustrates model behaviours. More details in supplementary.

## 4 Benchmark Analysis

In this section, we perform analysis across the following five aspects:

_Effect of pretraining dataset size:_ In self-supervised learning, a natural question to ask is whether dataset size plays any role in the performance of downstream tasks. It is important to study if the increase in the size of the pretraining dataset will proportionally reciprocate in performance improvement. Also, a general trend is to train models for a very long duration at the pre-training stage. We investigate if the longer duration actually impacts the gain in performance. We look across different stages of training for multiple architectures and across different pretext tasks.

_Impact of task complexity:_ Some of the existing works show that increasing complexity leads to better representation learning, and if the complexity is decreased, the network will optimize to suboptimal solutions. We analyze this aspect in more detail with several tasks and different models.

_Effect of data distribution:_ Existing self-supervised methods perform evaluations on K400 and UCF101 datasets. Both these datasets fall into the same visual category with heavy appearance bias. However, we divert our attention towards datasets where the temporal dimension plays an important role such as SSv2 and Diving48.

_Robustness of SSL tasks:_ We study the robustness qualities of SSL methods against data noise (26). We analyze which factors play a key role in robustness of these methods against such domain shifts.

_Feature analysis:_ Finally, we look into feature space and analyze whether the learned representations are complementary in nature when models are trained under different protocols.

### Effect of dataset-size

We first analyze the effects of pre-training data size variation. The network trains on four subsets of the K400: 10k, 30k, 50k, and 100k. The number of videos per class is the same. The smaller pre-training dataset is a subset of the bigger pre-training dataset size (i.e. \(10k\subset 30k\) and so on). We look into three aspects regarding _dependence on pre-train subset size:_ a) behavior of different pretext tasks with the increase in pre-train dataset subset, b) performance across the different capacity of backbones, and, c) the effect of training time across different pretext tasks.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

from both source datasets and outperforms the teacher. Student network outperforms standalone spatio-temporal network performance in both contrastive and non-contrastive domains.

**Inference:** (i) _Knowledge can be distilled from different architectures for a given subset size (Fig. 3 (a))_, (ii) _Knowledge from different source datasets brings in complementary information (Fig. 3 (c))_, and (iii) _Orthogonal features are learned across different categories of pretext tasks (Fig. 3 (d))_.

## 5 Lessons Learned

With all the analysis along studied axes, we learned a few lessons in-between these axes such as: (i) Contrastive tasks are fast learners but are also most susceptible to noise. (ii) An increase in dataset size or complexity does not help smaller models in learning better spatio-temporal features but these features are more robust to noise. (iii) Temporal tasks are relatively more difficult to learn since looking at the correlation between time of training, increase in dataset size, and complexity, the performance gain is minimal in each of this axis. It means this category of tasks is actually difficult to solve. (iv) Spatio-temporal pretext tasks improve with the increase in complexity and dataset size (if the model permits), and their behavior to learn better spatio-temporal features is independent of data distribution. Using these lessons, we further do more analysis in feature space. From there, we observe within an axis of comparison how models learn orthogonal information. Based on those observations, we analyze if we can push the performance for downstream tasks. We look into two downstream tasks: action classification and clip retrieval.

Figure 4: **Knowledge distillation using teachers trained on multiple subset sizes on RSPNet. Student: ShuffleNet a) UCF101 and b) HMDB51. Here T1 is Teacher-1 (shufflenet) and T2 is teacher-2 (R21D). Top@$_5 Clip Retrieval - R21D on c) UCF101 and d) HMDB51, pre-trained on K400 and SSV2 - 30k subset.**

Figure 3: **Feature analysis overview. This figure shows how KD as a tool is beneficial across multiple scenarios. Brief details for each setup (Left to right): (A) _Effect of dataset size:_ Teachers (T1 and T2) are different architectures for a single subset. Student model (ST-Shuffle) CKA maps shows it learns complementary information especially for 30k. (B) _Task Complexity:_ Teachers are multiple complexities across the same task. (C1, C2, C3 - different complexities as teachers.) We observe in most of the scenarios, Student (ST) networks outperforms all teacher models which proves learning of orthogonal information from multiple teachers. (C) _Out-of-Distribution:_ Models from different _source_ datasets are teachers. Student model (ST) outperforms both teachers trained on two different datasets. (D) _Pretext Tasks:_ Spatial and temporal task networks are teachers, and, student model (ST) learnt from two different categories of pretext tasks - spatial and temporal incorporate knowledge from both and outperforms both of the teachers for both contrastive and non-contrastive.**

Clip retrievalFor this downstream task, we generate feature vectors using pretrained weights. The nearest neighbor is found by measuring the cosine distance between test and train feature vectors. We show analysis on UCF101 and HMDB51, with different source data distributions, K400 and SSv2.

_Observations:_ Spatio-temporal task still outperform other categories independent of _source_ data distribution similar to what we observe earlier. Contrastive learns better _appearance_ features during the pre-training stage given both downstream datasets are _appearance_ based. Temporal tasks have almost similar performance pre-trained on either of the _source_ datasets, which shows even with an appearance-based dataset as a pre-train dataset, the task is not focusing much on spatial features.

Action ClassificationFor this task, the model is finetuned end-to-end on downstream datasets, on UCF101 and HMDB51. In Table 5, we obtain our best performing model via knowledge distillation discussed in previous section and we show our model outperforms previous state-of-the-art approaches.

_Observations:_ With only 30k videos compared to 200k+ videos used by other pretext tasks, we show that our model outperforms by a good margin on UCF101 against single and multi-modal approaches. We got competitive results on HMDB51 with a score of 51.5%.

### Surprising Findings

We have multiple inferences from different axes of analysis. However, to club a few which are new and helpful for video self-supervised community, we list down those here:

_Dataset size and Training time Dependency:_ Against the conventional belief that a lot of training data is a _must_ to achieve the best performance, we demonstrate that beyond a certain amount of training data, additional data provides diminishing returns for SSL in terms of performance improvement. This finding has significant implications, as it allows for a substantial reduction in the training data and there is almost a 10x reduction in training time which is particularly advantageous in computationally demanding video processing tasks. Furthermore, we show how KD as a tool, outperforms the original approach (100% data) using almost 90% less data further optimizing resource utilization by 80%.

_Robustness to real-world noise:_ To our surprise, contrastive tasks are more susceptible to noise than non-contrastive. A smaller network tends to be more robust in some scenarios than a bigger network. We believe these findings are _novel and not known_ to the community as there is no existing study exploring these aspects and are helpful where robustness is necessary for real-world deployment.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline Approach & Venue & NxW/H & Backbone & Pre-training & UCF101 & HMDB51 \\ \hline
**Generative** & \multicolumn{3}{c}{-} & \multicolumn{3}{c}{-} & \multicolumn{3}{c}{-} & \multicolumn{3}{c}{-} \\ \hline VIMPACE (60) & \multicolumn{3}{c|}{-} & \multicolumn{3}{c|}{10x256} & ViT-L & HTM & 92.7 & 65.9 \\ VideoMLE (63) & NeurIPS’22 & 16x224 & ViT-B & K400 & 91.3 & 62.6 \\ MME (59) & CVPR’23 & 16x224 & ViT-B & K400 & 96.5 & 78.0 \\ DVD (70) & CVPR’23 & 16x224 & ViT-B & INIK’4K00 & 97.0 & 76.4 \\ EVEREST (28) & \multicolumn{3}{c}{-} & \multicolumn{3}{c}{16x224} & ViT-B & - & 93.4 & 68.1 \\ SCE (14) & WACV’23 & 16x224 & ResNet3D-50 & K400 & 95.3 & 74.7 \\ \hline
**Context** & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\ \hline PacePred (73) & ECCV’20 & 16x112 & R21D-18 & K400 & 77.1 & 36.6 \\ TempTrans (30) & ECCV’20 & 16x112 & R3D-18 & K400 & 79.3 & 49.8 \\ STS (68) & TPAMI-21 & 16x112 & R21D-18 & K400 & 77.8 & 40.5 \\ VideoMoCo (46) & CVPR’21 & 16x112 & R21D-18 & K400 & 78.7 & 49.2 \\ RSPNet (10) & AAAI’21 & 16x112 & R21D-18 & K400 & 81.1 & 44.6 \\ TaCo (6) & \multicolumn{3}{c}{16x224} & R21D-18 & K400 & 81.8 & 46.0 \\ TCRR(13) & CVI’U2’22 & 16x112 & R21D-18 & K400 & 88.2 & 60.0 \\ CVRL (49) & CVPR’21 & 32x224 & R21D-18 & K400 & 92.9 & 67.9 \\ TransRank (16) & CVPR’22 & 16x112 & R21D-18 & K200 & 87.8 & 60.1 \\ \hline
**Multi-Modal** & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\ \hline AVTS (37) & NeurIPS’18 & 25x224 & I3D & K400 & 83.7 & 53.0 \\ GDT (47) \(\dagger\) & - & 32x112 & R21D & IG65M & 95.2 & 72.8 \\ XDC (4) & NeurIPS’20 & 32x224 & R21D & K400 & 84.2 & 47.1 \\ \hline Ours \({}^{*}\) & - & 16x112 & R21D-18 & K400-30k & 97.3 & 51.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Comparison with previous approaches** pre-trained on K400. Ours ( \({}^{*}\) best performing) is RSPNet pretrained on 30k subset of K400. \({}^{\dagger}\) - Different pre-training data. (%)

#### Complementary knowledge:

Improvement in performance with KD from different data distributions and categories of tasks brings out a recipe for a new SSL task. This involves utilizing a multi-teacher multi-student setup, where each teacher specializes in spatial and temporal tasks and is trained on a mixture of data sources. Our analysis indicates this would provide a strong learning scenario.

#### 5.2 Recommendations

Looking into several factors, here we provide a few recommendations to set up the recipe for SSL: 1) _Training speed:_ If training time is a concern, contrastive tasks can help in reducing the pretraining time, but they could be less robust against data noise. 2) _Data distribution:_ It is always better to use a spatio-temporal pretext task irrespective of the data distribution. However, if that is not an option, the pretext task should always be aligned with the nature of the pretraining dataset. 3) _Model capacity:_ If model capacity is limited, there is no benefit of increasing pretraining dataset size and using complex pretext tasks. 4) _Robustness:_ If best performance is the goal, we should use a non-contrastive as opposed to a contrastive pretext task. 5) _Performance:_ Pretext tasks learn complementary features across model architectures, pretraining datasets, pretext tasks, and tasks complexity, therefore, this complementary knowledge can be distilled to obtain strong spatio-temporal features.

#### Extension of findings to Video Foundation Models (ViFMs)

In this section, we extend the study to ViFMs (Tables 6 and 7). We select both image-based [2; 45; 51] which are image foundation models extended to videos and video-based [80; 72] which are trained from scratch on videos. ViFMs are all trained with contrastive pretraining objective. More details about architectures are in supplementary.

#### Dataset size:

An increase in dataset size or complexity does not help smaller models in learning better spatio-temporal features (Table 6). ViCLIP and LanguageBind, despite using a significantly larger pretraining dataset, performs worse than models pretrained on the smaller Kinetics-400 dataset; A simple increase in the number of frames is outperforms models trained on larger datasets.

#### Complementary knowledge:

Improvement in performance in the case of KD from different ViFMs brings out a recipe for training a new foundational model. This involves utilizing a multi-teacher multi-student setup, where each teacher is a ViFM pretrained differently in terms of data sources, multi-stage pretraining, and pretraining objective. Our analysis (Table 7) indicates this would provide a powerful learning scenario.

## 6 Conclusion

In this study, we explore different parameters for self-supervised learning in the video domain. We set a benchmark which provides an intuitive task categorization and enables a better comparison of different pretext tasks. Such an analysis has never been explored for video understanding to the best of our knowledge. We presented several interesting insights which will open up new directions for the research community. We also demonstrate the usefulness of some of these insights where we obtain state-of-the-art performance on video action recognition using merely a 10% pretraining dataset when compared with existing methods. We believe this benchmark study will help the research community better understand self-supervised learning in the video domain.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline ViFM & Type. Pretraining Data Frames \(\times\) Rate & Accuracy \\ \hline ViF-CLIP [51] & 1 & K-400 & 32 x 2 \\ X-CLIP [45] & 1 & K-400 & 8 x 8 71.6 \\ EZ-CLIP [2] & 1 & K-400 & 8 x 8 70.5 \\ ViCILP [72] & V & InterVid-10M & 8 x 8 8 75.5 \\ LanguageBind [80] & V & VIDAL-10M & 8 x 8 8 69.9 \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c c c} \hline \hline ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\ \hline X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Analysis on ViFMs. Zero-shot classification accuracy on UCF-101. I:Image, V: Video.**

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\ \hline X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c c c c} \hline \hline ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\ \hline X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Knowledge Distillation between different ViFM pairs as teachers, and R21D as the student.**

## References

* [1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. _ArXiv_, abs/2008.04237, 2020.
* [2] Shahzad Ahmad, Sukalpa Chanda, and Yogesh S Rawat. Ez-clip: Efficient zeroshot video action recognition. _arXiv preprint arXiv:2312.08010_, 2023.
* [3] Unaiza Ahsan, Rishi Madhok, and Irfan A. Essa. Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition. _CoRR_, abs/1808.07507, 2018.
* [4] Human Alwassel, Dhruv Kumar Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. _ArXiv_, abs/1911.12667, 2020.
* [5] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. _ArXiv_, abs/2103.15691, 2021.
* [6] Yutong Bai, Haooi Fan, Ishan Misra, Ganesh Venkatesh, Yongyi Lu, Yuyin Zhou, Qihang Yu, Vikas Chandra, and Alan Loddon Yuille. Can temporal information help with contrastive self-supervised learning? _ArXiv_, abs/2011.13046, 2020.
* [7] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Michal Irani, and Tali Dekel. Speednet: Learning the speediness in videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.
* [8] Gedas Bertaisus, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? _ArXiv_, abs/2102.05095, 2021.
* [9] J. Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. _ArXiv_, abs/1907.06987, 2019.
* [10] Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, and Chuang Gan. Rspnet: Relative speed perception for unsupervised video representation learning. In _AAAI_, 2021.
* [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. _ArXiv_, abs/2002.05709, 2020.
* [12] Jinwoo Choi, Chen Gao, Joseph C.E. Messou, and Jia-Bin Huang. Why can't i dance in the mall? learning to mitigate scene bias in action recognition. In _NeurIPS_, 2019.
* [13] I. Dave, Rohit Gupta, M. N. Rizze, and M. Shah. Tclr: Temporal contrastive learning for video representation. _ArXiv_, abs/2101.07974, 2021.
* [14] Julien Denize, Jaonary Rabarisoa, Astrid Oresi, Romain Herault, and Stephane Canu. Similarity contrastive estimation for self-supervised soft contrastive learning. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 2706-2716, January 2023.
* [15] Shangchen Du, Shan You, Xiaojie Li, Jianlong Wu, Fei Wang, Chen Qian, and Changshui Zhang. Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 12345-12355. Curran Associates, Inc., 2020.
* [16] Haodong Duan, Nanxuan Zhao, Kai Chen, and Dahua Lin. Transrank: Self-supervised video representation learning via ranking-based transformation recognition. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2990-3000, 2022.
* [17] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. _ArXiv_, abs/2104.11227, 2021.
* [18] Christoph Feichtenhofer, Haooi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3298-3308, 2021.
* [19] Basura Fernando, Hakan Bilen, E. Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5729-5738, 2017.
* [20] Jianping Gou, B. Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A survey. _ArXiv_, abs/2006.05525, 2021.
* [21] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. _arXiv preprint arXiv:1905.01235_, 2019.
* [22] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruind, Peter N. Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The "something something" video database for learning and evaluating visual common sense. _2017 IEEE International Conference on Computer Vision (ICCV)_, pages 5843-5851, 2017.
* [23] Sheng Guo, Zihua Xiong, Yujie Zhong, Limin Wang, Xiaobo Guo, Bing Han, and Weilin Huang. Cross-architecture self-supervised video representation learning. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19248-19257, 2022.
* [24] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-augmented dense predictive coding for video representation learning. In _European Conference on Computer Vision_, 2020.
* [25] K. Hara, H. Kataoka, and Y. Satoh. Learning spatio-temporal features with 3d residual networks for action recognition. _2017 IEEE International Conference on Computer Vision Workshops (ICCVW)_, pages3154-3160, 2017.
* [26] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _ArXiv_, abs/1903.12261, 2019.
* [27] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan, Lorenzo Torresani, Manohar Paluri, Li Fei-Fei, and Juan Carlos Niebles. What makes a video a video: Analyzing temporal information in video understanding models and datasets. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7366-7375, 2018.
* [28] Sunil Hwang, Jaehong Yoon, Youngwan Lee, and Sung Ju Hwang. Efficient video representation learning via motion-aware token selection. _arXiv preprint arXiv:2211.10636_, 2022.
* [29] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size, 2016. cite arxiv:1602.07360Comment: In ICLR Format.
* [30] S. Jenni, Givi Meishvili, and P. Favaro. Video representation learning by recognizing temporal transformations. _ArXiv_, abs/2007.10730, 2020.
* [31] L. Jing and Y. Tian. Self-supervised visual feature learning with deep neural networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-1, 2020.
* [32] Longlong Jing, Xiaodong Yang, Jingen Liu, and Y. Tian. Self-supervised spatiotemporal feature learning via video rotation prediction. _arXiv: Computer Vision and Pattern Recognition_, 2018.
* [33] W. Kay, J. Carreira, K. Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, F. Viola, T. Green, T. Back, A. Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. _ArXiv_, abs/1705.06950, 2017.
* [34] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(01):8545-8552, Jul. 2019.
* [35] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1920-1929, 2019.
* [36] Okan Kopikliu, Neslian Kose, Ahmet Gunduz, and Gerhard Rigoll. Resource efficient 3d convolutional neural networks. In _2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)_, pages 1910-1919. IEEE, 2019.
* [37] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. In _NeurIPS_, 2018.
* [38] H. Kuehne, H. Huang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recognition. In _2011 International Conference on Computer Vision_, pages 2556-2563, 2011.
* [39] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In _ECCV_, 2018.
* [40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9992-10002, 2021.
* [41] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3192-3201, 2022.
* [42] Derhao Luo, Chang Liu, Y. Zhou, Dongbao Yang, Can Ma, Qixiang Ye, and Weiping Wang. Video cloze procedure for self-supervised spatio-temporal learning. _ArXiv_, abs/2001.00294, 2020.
* [43] I. Misra, C. L. Zitnick, and M. Hebert. Unsupervised learning using sequential verification for action recognition. _ArXiv_, abs/1603.08561, 2016.
* [44] Thao Nguyen, Maithra Raghu, and Simon Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. _ArXiv_, abs/2010.15327, 2021.
* [45] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In _European Conference on Computer Vision_, pages 1-18. Springer, 2022.
* [46] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videoomoco: Contrastive video representation learning with temporally adversarial examples. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11200-11209, 2021.
* [47] Mandela Patrick, Yuki M. Asano, Ruth Fong, Joao F. Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transformations. _ArXiv_, abs/2003.04298, 2020.
* [48] Senthil Purushwalkam and Abhinav Gupta. Pose from action: Unsupervised learning of pose features based on motion. _arXiv preprint arXiv:1609.05420_, 2016.
* [49] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, H. Wang, Serge J. Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6960-6970, 2021.
* [50] Kanchana Ranasinghe, Muzzammal Naseer, Salman Hamed Khan, Fahad Shahbaz Khan, and Michael S. Ryoo. Self-supervised video transformer. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2864-2874, 2021.
* [51] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned clip models are efficient video learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6545-6554, 2023.

* [52] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.
* [53] N. Sayed, Biagio Brattoli, and Bjorn Ommer. Cross and learn: Cross-modal self-supervision. In _German Conference on Pattern Recognition (GCPR) (Oral)_, Stuttgart, Germany, 2018.
* [54] Madeline Chantry Schiappa, Naman Biyani, Shruti Vyas, Hamid Palangi, Vibhav Vineet, and Yogesh Singh Rawat. Large-scale robustness analysis of video action recognition models. _ArXiv_, abs/2207.01398, 2022.
* [55] Madeline C Schiappa, Yogesh S Rawat, and Mubarak Shah. Self-supervised learning for videos: A survey. _ACM Computing Surveys_.
* [56] Feifei Shao, Long Chen, Jian Shao, Wei Ji, Shaoning Xiao, Lu Ye, Yueting Zhuang, and Jun Xiao. Deep learning for weakly-supervised object detection and object localization: A survey. _ArXiv_, abs/2105.12694, 2021.
* [57] K. Soomro, A. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _ArXiv_, abs/1212.0402, 2012.
* [58] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 843-852, Lille, France, 07-09 Jul 2015. PMLR.
* [59] Xinyu Sun, Peihao Chen, Liangwei Chen, Changhao Li, Thomas H Li, Mingkui Tan, and Chuang Gan. Masked motion encoding for self-supervised video representation learning. In _The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [60] Hao Tan, Jie Lei, Thomas Wolf, and Mohit Bansal. Vimpa: Video pre-training via masked token prediction and contrastive learning. _ArXiv_, abs/2106.11250, 2021.
* [61] Li Tao, Xueting Wang, and Toshihiko Yamasaki. Self-supervised video representation learning using inter-intra contrastive framework. _arXiv preprint arXiv:2008.02531_, 2020.
* [62] Fida Mohammad Thoker, Hazel Doughty, Piyush Bagad, and Cees G. M. Snoek. How severe is benchmark-sensitivity in video self-supervised learning? In _ECCV_, 2022.
* [63] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _ArXiv_, abs/2203.12602, 2022.
* [64] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In _Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)_, ICCV '15, page 4489-4497, USA, 2015. IEEE Computer Society.
* [65] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri. A closer look at spatiotemporal convolutions for action recognition. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6450-6459, 2018.
* [66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _ArXiv_, abs/1807.03748, 2018.
* [67] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* [68] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Wei Liu, and Yunhui Liu. Self-supervised video representation learning by uncovering spatio-temporal statistics. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44:3791-3806, 2022.
* [69] Jinpeng Wang, Yiqi Lin, Andy Jinhua Ma, and Pong Chi Yuen. Self-supervised temporal discriminative learning for video representation learning. _ArXiv_, abs/2008.02129, 2020.
* [70] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In _CVPR_, 2023.
* [71] X. Wang, K. He, and A. Gupta. Transitive invariance for self-supervised visual representation learning. In _2017 IEEE International Conference on Computer Vision (ICCV)_, pages 1338-1347, 2017.
* [72] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Intertwit: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.
* [73] Jiangliu Watng, Jianbo Jiao, and Yunhui Liu. Self-supervised video representation learning by pace prediction. In _European Conference on Computer Vision_, 2020.
* 46, 2020.
* [75] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [76] Ceyuan Yang, Yinghao Xu, Bo Dai, and Bolei Zhou. Video representation learning with visual tempo consistency. In _arXiv preprint arXiv:2006.15489_, 2020.
* [77] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. _ArXiv_, abs/2103.00550, 2021.
* [78] Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, and Qixiang Ye. Video playback rate perception for self-supervised spatio-temporal representation learning. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6547-6556, 2020.

* [79] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.
* [80] Bin Zhu, Bin Lin, Muan Ning, Yang Yan, Jixai Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]. Section 4 for full analysis. 2. Did you describe the limitations of your work? [Yes]. It is mentioned in supplementary. 3. Did you discuss any potential negative societal impacts of your work? [Yes]. It is discussed in supplementary. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]. Codes is attached in supplementary. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]. Section 3.2 mentions all details. Further descriptions are provided in supplementary. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]. Not applicable for our settings. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]. It is mentioned in the supplementary.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]