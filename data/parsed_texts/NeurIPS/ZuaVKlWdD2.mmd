# Injecting Multimodal Information into

Rigid Protein Docking via Bi-level Optimization

 Ruijia Wang121 Yiwu Sun11 Yujie Luo11 Shaochuan Li11 Cheng Yang2 Xingyi Cheng1 Hui Li11 Chuan Shi21 Le Song11

BioMap Research

Footnote 1: Work done during an internship at BioMap

Footnote 2: Contributed equally to this research. Each author’s contribution is provided in Section 5.

Footnote 3: Corresponding authors

###### Abstract

The structure of protein-protein complexes is critical for understanding binding dynamics, biological mechanisms, and intervention strategies. Rigid protein docking, a fundamental problem in this field, aims to predict the 3D structure of complexes from their unbound states without conformational changes. In this scenario, we have access to two types of valuable information: sequence-modal information, such as coevolutionary data obtained from multiple sequence alignments, and structure-modal information, including the 3D conformations of rigid structures. However, existing docking methods typically utilize single-modal information, resulting in suboptimal predictions. In this paper, we propose xTrimoBiDock\({}^{\alpha}\) (or BiDock for short)4, a novel rigid docking model that effectively integrates sequence-and structure-modal information through bi-level optimization. Specifically, a cross-modal transformer combines multimodal information to predict an inter-protein distance map. To achieve rigid docking, the roto-translation transformation is optimized to align the docked pose with the predicted distance map. In order to tackle this bi-level optimization problem, we unroll the gradient descent of the inner loop and further derive a better initialization for roto-translation transformation based on spectral estimation. Compared to baselines, BiDock achieves a promising result of a maximum 234% relative improvement in challenging antibody-antigen docking problem.

Footnote 4: XTrimoBiDock is a member of BioMap’s large-scale AI engine “xTrimo” series. “\(\alpha\)” denotes the academic version, to distinguish it from the commercial product xTrimoBiDock.

## 1 Introduction

Protein-protein interactions (PPIs) are essential to the basic functioning of cells and larger biological systems. Due to their importance, elucidating such interactions up to atomic detail is necessary for understanding multicomponent complexes like ribosomes and discovering protein-based drugs, e.g., antibodies and peptides. However, the experimental golden standard for determining the structure of protein complexes, such as X-ray crystallography and cryo-EM, is extremely time-consuming.

Computational protein docking [49, 7, 57, 45] provides an alternative route to predict the 3D structures of complexes from unbound states. Here, we focus on the fundamental problem of rigid protein docking [25] where no deformations occur within any protein during the docking process.

This assumption is reasonable in many biological environments and stabilizes the prediction of natural structures. Therefore, all we need is an appropriate SE(3) transformation shown in Figure 1, i.e., roto-translation transformation, that places the ligand protein at the correct orientation and location concerning the receptor protein. In this context, two types of information are available. The first is sequence-modal information, such as the coevolutionary signals captured in multiple sequence alignments (MSAs). The second is structure-modal information, like 3D coordinates of rigid bodies and bond angles. Both types of information are indispensable for performing rigid protein docking.

Classical docking software [13; 19; 46; 29; 58] generally follow a three-step framework for predicting complex structures. Firstly, a large number of candidate structures are randomly sampled to explore the conformational space of the complex. Secondly, a scoring function is employed to evaluate and rank the sampled structures based on their compatibility with the binding interface. Finally, the top-ranked structures are refined using an energy model to improve their accuracy and eliminate steric clashes. Due to the evaluation of lots of candidates, these methods tend to be computationally expensive, particularly for high-throughput workflows.

Recently, deep learning has shown significant computational speed-up in this field. EquiDock [25] has emerged as a pioneering method for applying deep learning to rigid protein docking. However, it solely relies on structure-modal information, neglecting the valuable sequence-modal information in databases. This drawback hampers its ability to capture evolutionary constraints and exploit the intricate sequence-structure relationships. Building upon the success of AlphaFold2 [28], AlphaFold-Multimer [23] has been hailed as a breakthrough in directly folding complex structures from amino acid sequences, which considers the sequence-modal information but fails to effectively utilize the given rigid structures, leading to unnatural complexes in some cases.

To overcome the aforementioned limitations, we propose xTrimoBiDock (or BiDock for short), a novel rigid docking model that seamlessly integrates sequence- and structure-modal information through bi-level optimization. (i) To effectively utilize multimodal information, we introduce a cross-modal transformer that injects multimodal information into an inter-protein distance map. (ii) To satisfy rigid docking, we optimize a roto-translation transformation to minimize the difference between the docked pose of unbound proteins and the predicted inter-protein distance map. This framework naturally lends itself to a nested bi-level optimization paradigm, where the outer loop is the learning of the cross-modal transformer and the inner loop is dedicated to solving the roto-translation transformation. Inspired by gradient-based bi-level optimization [24; 32; 11], we unroll the gradient descent of the inner loop for approximation. Due to the ruggedness of the optimization landscape, a substantial number of iterations are required for convergence. Thus, we further derive a better initialization for the roto-translation transformation using spectral estimation. Extensive experiments conducted on diverse datasets and evaluation protocols validate the effectiveness of BiDock.

In summary, our contributions are three-fold:

* We effectively leverage sequence- and structure-modal information for rigid protein docking. By naturally integrating the fusion of multimodalities and the docking of rigid bodies through bi-level optimization, we set up a new avenue for solving rigid protein docking.
* We solve the above bi-level optimization with unrolled gradient and spectral initialization. By unrolling the gradient descent of the inner loop and deriving a spectral estimation for initialization, BiDock enhances the convergence and controls the computational cost.
* Comprehensive experiments on three representative datasets demonstrate the effectiveness of the proposed model. Compared to state-of-the-art baselines, BiDock achieves the maximum 234% relative improvement in challenging but practical antibody-antigen docking.

Figure 1: Surface views of rigid protein docking. Keep receptor protein at a fixed location, and a roto-translation transformation \(T=(R,\mathbf{t})\) is predicted to place ligand protein at the correct docked pose.

Related Work

Molecular DockingMolecular docking aims to characterize the binding poses between small molecule compounds and protein targets [42; 43; 63], playing an essential role in the discovery of effective and safe treatments for various diseases [48]. Traditional computational methods have been greatly enhanced by deep learning techniques [53], which offer increased expressive power in identifying, processing, and extrapolating complex patterns in molecular data [39; 31; 21; 35; 4; 38; 16]. It is important to note that these methods have primarily focused on molecular ligands and often assume the availability of known binding pockets, limiting their direct applicability to protein-protein docking. Protein-protein docking is more challenging due to larger, flexible proteins and the need to predict unknown binding interfaces.

Protein DockingComputational docking software [46; 47; 44; 57; 58; 45; 15] predict complex structures based on a framework of candidate sampling, ranking [37; 6; 22], and refinement [50]. These methods can be financially restrictive and time-consuming, as they often involve scoring and ranking thousands of candidate structures. Recently, deep learning has made significant contributions to structural biology [30; 17; 18; 36]. Notably, AlphaFold2 [28] and RoseTTAFold [2] have been employed to improve protein structure prediction from various angles [26; 40], such as integrating physics-based docking methods [29] or extending multiple sequence alignments [10]. Additionally, methods like AlphaFold-Multimer [23] and HSRN [27] have been developed to simultaneously fold and dock two proteins. Despite their remarkable achievements, these methods violate the rigidity of rigid docking and do not consider unbounded structures. EquiDock [25] is tailored for effective rigid docking but does not fully leverage the evolutionary information encoded in protein sequences, resulting in limited performance improvement compared to traditional docking software.

Bi-level OptimizationBi-level optimization has gained attention in the deep learning community for its ability to handle nested problem structures. It finds applications in various domains, including hyperparameter optimization [14] and metakenwedge extraction [24]. Traditional bi-level optimization methods rely on game theory [51] or mathematical programming [9], which may not scale well to large datasets or have strict mathematical requirements. Alternatively, gradient descent methods offer a promising solution, and they can be divided into explicit gradient update [60], explicit proxy update [1; 8], implicit function [41], and closed-form methods [59]. The first three are approximation methods suitable for general functions, while the last one is an accurate method specifically designed for certain functions. Recent surveys [33; 11] provide a more comprehensive review of these methods. In addition, the merging AI4Science directions, such as topology design [61] and protein representation learning [12], also present nested problem structures that can be compatible with bi-level optimization. By incorporating bi-level optimization into rigid protein docking, we anticipate that our work will have a significant impact at the intersection of these research areas.

## 3 BiDock Methodology

Multimodal InputThe available information on the sequence modality mainly includes information inside the primary sequence itself and co-evolutionary information from MSAs. Following the existing work [28; 23], we extract type features \(F^{typ}\in\mathbb{R}^{N_{res}\times 21}\) and primary pair features \(F^{pp}\in\mathbb{R}^{N_{res}\times N_{res}\times 73}\) from the primary sequence, where \(N_{res}\) is the number of residues. In terms of MSAs, we leverage cluster MSA features \(F^{msa}\in\mathbb{R}^{N_{cls}\times N_{res}\times 49}\), where \(N_{cls}\) is the number of cluster centers. For the structure modality, we extract angle features \(F^{ang}\in\mathbb{R}^{N_{res}\times 57}\) and pair features \(F^{p}\in\mathbb{R}^{N_{res}\times N_{res}\times 88}\) from the rigid protein structures. The angle features provide the orientation and position of each amino acid, while the pair features contain the distance information between amino acids. We present a brief introduction for features and give further details in Appendix B.

ArchitectureThe illustration of the framework is presented in Figure 2. Given sequence-modal features \(\{F^{typ},F^{msa},F^{pp}\}\) and structure-modal features \(\{F^{ang},F^{p}\}\), the cross-modal transformer with parameters \(\phi\) transforms available features and integrates them to predict an inter-protein distance map \(\hat{D}\). To maintain the rigid-body assumption, we define the objective function of learning rotation-translation transformation \((R,\mathbf{t})\) based on the distance map \(\hat{D}\) and coordinates of rigid proteins\(\{X\in\mathbb{R}^{3\times m},Y\in\mathbb{R}^{3\times n}\}\) as follows

\[\mathcal{L}(R,\mathbf{t},\phi)=\min_{R,\mathbf{t}}\frac{1}{mn}\sum_{i=1}^{m}\sum _{j=1}^{n}\left(\|X_{i}-RY_{j}-\mathbf{t}\|-\hat{D}_{ij}(\phi)\right)^{2},\] (1)

where \(X_{i}\) is the \(i\)-th column of \(X\) and \(Y_{j}\) is the \(j\)-th column of \(Y\). Considering that optimizing roto-translation transformation \((R,\mathbf{t})\) and parameters of cross-modal transformer \(\phi\) constitutes a nested structure, we reformulate the rigid protein docking as a bi-level optimization problem

\[\phi^{*}=\operatorname*{argmin}_{\phi}\mathcal{L}^{out}\left(R^{*}(\phi), \mathbf{t}^{*}(\phi),\phi\right)\quad\text{s.t. }R^{*}(\phi),\mathbf{t}^{*}(\phi)= \operatorname*{argmin}_{R,\mathbf{t}}\mathcal{L}(R,\mathbf{t},\phi),\] (2)

where \(\mathcal{L}^{out}\) refers to the outer loss for the cross-modal transformer and its specific details will be introduced in the following subsections. To solve this bi-level optimization, we unroll the gradient descent of the inner loop to approximate \((R^{*}(\phi),\mathbf{t}^{*}(\phi))\) by

\[\begin{split} R_{t+1}&=\operatorname{OPT}(R_{t}, \nabla_{R}\mathcal{L}_{t}\left(R_{t},\mathbf{t}_{t},\phi\right)),\\ \mathbf{t}_{t+1}&=\operatorname{OPT}(\mathbf{t}_{t}, \nabla_{\mathbf{t}}\mathcal{L}_{t}\left(R_{t},\mathbf{t}_{t},\phi\right)), \end{split}\] (3)

where \(\operatorname{OPT}\) represents the optimization algorithm, such as the stochastic gradient descent (SGD). However, due to the complexity of the problem and the rugged optimization landscape, the gradient descent in the inner loop often requires a large number of iterations to converge and faces challenges in finding the global minima. Therefore, we further derive spectral initialization to provide a more favorable starting point for better convergence.

### Cross-modal Transformer

Based on the multimodal features, the cross-modal transformer predicts an inter-protein distance map, as depicted in Figure 3. We first use multilayer perceptrons (MLPs) to capture the nonlinear relationship inside features and project them into the same space

\[\begin{split} P^{pp}=\operatorname{MLP}(F^{pp})\quad P^{p}= \operatorname{MLP}(F^{p})\\ M^{typ}=\operatorname{MLP}(F^{typ})\quad M^{msa}=\operatorname{ MLP}(F^{msa})\quad M^{ang}=\operatorname{MLP}(F^{ang}),\end{split}\] (4)

where \(\{P^{pp},P^{p}\in\mathbb{R}^{N_{res}\times N_{res}\times c_{z}}\}\) are transformed pair features, \(\{M^{typ},M^{ang}\in\mathbb{R}^{N_{res}\times c_{m}}\}\) and \(M^{msa}\in\mathbb{R}^{N_{cls}\times N_{res}\times c_{m}}\) are transformed intra-sequence features. Then we add pair features as the input pair feature \(P\) of Evoformer [28]

\[P=P^{pp}+P^{p}.\] (5)

Similarly, we integrate intra-sequence features as another input

\[M=\left[(M^{typ}+M^{msa})\|M^{ang}\right],\] (6)

Figure 2: The overall framework of BiDock. Taking the information from sequence and structure modality as input, the cross-modal transformer fuses them and predicts an inter-protein distance map. To achieve rigid docking, the roto-translation transformation is learned by minimizing the difference between the docked pose of rigid structures and the predicted distance map. We unroll the gradient descent and further derive a spectral initialization to address the formed bi-level optimization.

where \(\|\) is the concatenation. Please note that the broadcast operation is used to make the dimensions consistent. Specifically, \(M^{typ}\) is broadcasted along the newly added first dimension during addition. Similarly, broadcasting is applied to the newly added first dimension of \(M^{ang}\) for concatenation. Through Evoformer, pair representation \(\hat{P}\) and evolution representation \(\hat{M}\) are obtained

\[\hat{P},\hat{M}=\textsc{EVOFOMER}(P,M).\] (7)

Finally, we utilize pair representation \(\hat{P}\) to predict the inter-protein distance map. Here we define two loss functions to supervise the learning of distance map and evolution representation. Concerning the distance map, we can directly calculate the ground truth from rigid proteins. However, this ground truth is naturally noisy because of the experimental resolution. In light of successful practice in protein structure prediction [28, 23], we use a discretized distance map to replace the exact one. Specifically, distances are discretized into 64 bins ranging from 2 to 22 A. The prediction of discretized distances is converted into a classification problem by

\[\bar{D}_{ij}=\sigma(W(\hat{P}_{ij}+\hat{P}_{ji})),\] (8)

where \(W\) is the learnable parameter and \(\sigma\) is the activation function. Then the cross-entropy loss averaged over all residue pairs is

\[\mathcal{L}_{dist}=-\frac{1}{mn}\sum_{i,j}\sum_{k=1}^{64}\bar{G}_{ij}^{k}\log \bar{D}_{ij}^{k},\] (9)

where \(\bar{G}_{ij}^{k}\) represents the \(k\)th-element of one-hot encoding of discretized actual distance. It is worth noting that the predicted distance map \(\hat{D}\) can be obtained by using the mean of each bin.

Inspired by the masked language model [20], we leverage the evolution representation to reconstruct masked MSA values. We consider 23 classes, including 20 common amino acid types, an unknown type, a gap token, and a mask token, and introduce the mask policy in Appendix B. Thus, a masked MSA loss can be defined as follows

\[\bar{M}=\mathrm{Softmax}(W\hat{M}),\] (10) \[\mathcal{L}_{msa}=-\frac{1}{N_{mask}}\sum_{i=1}^{N_{cls}}\sum_{j \in N_{mask}}\sum_{k=1}^{23}A_{ij}^{k}\log(\bar{M}_{ij}^{k}),\] (11)

where \(N_{mask}\) denotes the number of masked tokens and \(A\) is the ground truth.

Similar to Equation (1), the SE(3) transformation optimized in the inner loop will exhibit differences from the ground truth complex, generating hypergradients through the cross-modal transformer

\[\mathcal{L}^{in}(R^{*}(\phi),\mathbf{t}^{*}(\phi))=\frac{1}{mn}\sum_{i=1}^{m} \sum_{j=1}^{n}\left(\|X_{i}-R^{*}(\phi)Y_{j}-\mathbf{t}^{*}(\phi)\|-D_{ij} \right)^{2},\] (12)

Figure 3: Details on the architecture of cross-modal transformer that enables the interaction of different modalities and outputs updated representations. In particular, the pair representation is used to predict the inter-protein distance map.

where \(D\) is the ground truth distance between amino acids. Overall, the outer loss for the cross-model transformer is

\[\mathcal{L}^{out}=\lambda_{1}\mathcal{L}_{dist}+\lambda_{2}\mathcal{L}_{msa}+ \lambda_{3}\mathcal{L}^{in},\] (13)

where hyperparameters \(\lambda_{1}\), \(\lambda_{2}\) and \(\lambda_{3}\) balance the importance of different loss terms.

### Unrolled Algorithm for Hypergradient

To solve the bi-level optimization formulated as Equation (2), hyper gradient \(\bm{d}_{\phi}\mathcal{L}^{out}\) is required in the outer level and can be unrolled via the chain rule

\[\bm{d}_{\phi}\mathcal{L}^{out}=\frac{\partial\mathcal{L}^{out}}{\partial(R^{ *},\mathbf{t}^{*})}\frac{\partial(R^{*}(\phi),\mathbf{t}^{*}(\phi))}{\partial \phi}+\frac{\partial\mathcal{L}^{out}}{\partial\phi}.\] (14)

One method to calculate it is approximating \((R^{*}(\phi),\mathbf{t}^{*}(\phi))\) via optimizer

\[(R_{t},\mathbf{t}_{t})=\operatorname{OPT}\left(R_{t-1},\mathbf{t}_{t-1},\phi \right),\quad t=1,\cdots,T,\] (15)

where \(T\) denotes the number of iterations. We explicitly calculate the gradients

\[\tilde{D}_{ij} =X_{i}-R_{t}Y_{j}-\mathbf{t}_{t},\] (16) \[\nabla_{\mathbf{q}}\mathcal{L}_{t}\left(R_{t},\mathbf{t}_{t}, \phi\right) =\sum_{i,j}\frac{2}{mn}\frac{\hat{D}_{ij}-\|\tilde{D}_{ij}\|}{\| \tilde{D}_{ij}\|}\tilde{D}_{ij}\odot Y_{j}\cdot\frac{\partial R_{t}}{\partial \mathbf{q}_{t}},\] \[\nabla_{\mathbf{t}}\mathcal{L}_{t}\left(R_{t},\mathbf{t}_{t}, \phi\right) =\sum_{i,j}\frac{2}{mn}\frac{\hat{D}_{ij}-\|\tilde{D}_{ij}\|}{\| \tilde{D}_{ij}\|}\tilde{D}_{ij},\]

where rotation matrix \(R\) and quaternion \(\mathbf{q}\) are converted to each other through the Bar-Itzhack algorithm [5], and \(\odot\) means Hadamard product. After \(T\) rounds of iterations, \((R^{*}(\phi),\mathbf{t}^{*}(\phi))\) can be approximated as \((R_{T},t_{T})\). We can compute the hypergradient by substituting

\[\frac{\partial(R^{*}(\phi),\mathbf{t}^{*}(\phi))}{\partial\phi}\approx-\gamma \frac{\partial^{2}\mathcal{L}(R_{T},\mathbf{t}_{T},\phi)}{\partial(R_{T}, \mathbf{t}_{T})^{\top}\partial\phi}.\] (17)

Due to the vast search space and the rugged landscape of the loss, the gradient descent algorithm with random initialization often requires a large number of iterations to converge and struggles to find the global minima. In the next subsection, we will derive a spectral initialization to enhance convergence.

### Spectral Initialization

Recall that we intend to derive a good initialization \((R_{0},\mathbf{t}_{0})\) for the gradient descent of Equation (1). To simplify it, we denote \(\hat{Y}=RY-\mathbf{t}\mathbf{1}_{n}^{\top}\), where \(\mathbf{1}_{n}\) is the all-ones vector. Then substitute \(\hat{Y}\) into

\[\left(\|X_{i}-RY_{j}-\mathbf{t}\|-\hat{D}_{ij}\right)^{2}=X_{i}^{\top}X_{i}-2 X_{i}^{\top}\hat{Y}_{j}+\hat{Y}_{j}^{\top}\hat{Y}_{j}+\hat{D}_{ij}^{2}-2\hat{D}_{ ij}\|X_{i}-\hat{Y}_{j}\|.\] (18)

According to these variable combinations, we define four variables and two centering matrices

\[B=\begin{bmatrix}X_{1}^{\top}X_{1}&...&X_{1}^{\top}X_{1}\\ X_{2}^{\top}X_{2}&...&X_{2}^{\top}X_{2}\\...&...&...\\ X_{m}^{\top}X_{m}&...&X_{m}^{\top}X_{m}\end{bmatrix}_{m\times n}C=\begin{bmatrix} \hat{Y}_{1}^{\top}\hat{Y}_{1}&...&\hat{Y}_{m}^{\top}\hat{Y}_{n}\\ \hat{Y}_{1}^{\top}\hat{Y}_{1}&...&\hat{Y}_{n}^{\top}\hat{Y}_{n}\\...&...&...\\ \hat{Y}_{1}^{\top}\hat{Y}_{1}&...&\hat{Y}_{n}^{\top}\hat{Y}_{n}\end{bmatrix}_{m \times n}\] (19) \[E=X^{\top}\hat{Y}=\begin{bmatrix}X_{1}^{\top}\hat{Y}_{1}&...&X_{1 }^{\top}\hat{Y}_{n}\\ X_{2}^{\top}\hat{Y}_{1}&...&X_{2}^{\top}\hat{Y}_{n}\\...&...&...\\ X_{m}^{\top}\hat{Y}_{1}&...&X_{m}^{\top}\hat{Y}_{n}\end{bmatrix}_{m\times n}F= \begin{bmatrix}\hat{D}_{11}^{\top}&...&\hat{D}_{1n}^{\top}\\ \hat{D}_{21}^{\top}&...&\hat{D}_{2n}^{\top}\\...&...&...\\ \hat{D}_{m1}^{\top}&...&\hat{D}_{mn}^{\top}\end{bmatrix}_{m\times n}\]

\[H_{m}=I_{m}-\frac{1}{m}J_{m}\quad H_{n}=I_{n}-\frac{1}{n}J_{n},\]

where \(I\) represents the identity matrix and \(J\) refers to the all-ones matrix. If the condition \(\forall i,j,\|X_{i}-\hat{Y}_{j}\|=\hat{D}_{ij}\) holds, the following equation exists

\[H_{m}FH_{n}=H_{m}(B-2E+C)H_{n}=-2H_{m}EH_{n}=-2H_{m}X^{\top}RYH_{n}.\] (20)By performing singular value decomposition (SVD) for \(H_{m}X^{\top}\) and \(YH_{n}\) respectively

\[H_{m}X^{\top}=U_{X}\Sigma_{X}V_{X}^{\top},\quad YH_{n}=U_{Y}\Sigma_{Y}V_{Y}^{ \top},\] (21)

the rotation matrix can be solved as

\[R=-\frac{1}{2}V_{X}\Sigma_{X}^{-1}U_{X}^{\top}H_{m}FH_{n}V_{Y}\Sigma_{Y}^{-1}U_{ Y}^{\top}.\] (22)

Given the rotation matrix \(R\), the translation vector \(\mathbf{t}\) can be gotten using

\[H_{m}F =H_{m}(B-2E+C)=H_{m}B-2H_{m}X^{\top}RY-2H_{m}X^{\top}\mathbf{t} \mathbf{1}_{n}^{\top},\] (23) \[\mathbf{t} =-\frac{1}{2}V_{X}\Sigma_{X}^{-1}U_{X}^{\top}H_{m}(F-B+2X^{\top} \hat{R}Y)\mathbf{1}_{n}.\] (24)

By replacing random initialization with the above spectral initialization, the gradient descent will approach the global minima faster and better. We will empirically verify this conclusion through subsequent experiments.

## 4 Experiments

DatasetsWe leverage Docking Benchmark 5.5 (DB5.5) [52], a gold standard dataset tailored for rigid docking. Following the experimental setting of EquiDock [25], the dataset is randomly partitioned into a test split of size 24. For a comprehensive comparison, we curate two datasets of antibodies (VH-VL) and antibody-antigen complexes (AB-AG) from Protein Data Bank (PDB) [3] and expect them to become new benchmarks. Specifically, the training set consists of 4,890 complexes containing at least one antibody chain, while the test set comprises 68 antibody-antigen complexes released after October 2022. The docking results of variable heavy-light chains and antigen-antibody can be separately evaluated. We direct the readers of interest to Appendix A for extraction criteria and detailed identifier lists. The statistics of the dataset are summarized in Table 1.

Evaluation ProtocolWe compare BiDock with two categories of representative methods, including three docking software ZDOCK [13], ClusPro [29], and HDOCK [58], and two deep learning models EquiDock [25], and AlphaFold-Multimer (Multimer for short [23]). To measure the quality of

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline
**Datasets** & **Metrics** & **ZDOCK** & **ClusPro** & **HDOCK** & **EquiDock** & **Multimer** & **BiDock** \\ \hline \multirow{3}{*}{**DB5.5**} & _RMSD_\(\downarrow\) & 12.491\(\pm\)6.294 & 14.135\(\pm\)8.153 & 11.328\(\pm\)8.073 & 14.982\(\pm\)3.304 & 7.797\(\pm\)7.428 & **7.280\(\pm\)**8.117 \\  & _TM-score_\(\uparrow\) & 0.689\(\pm\)0.114 & 0.702\(\pm\)0.118 & 0.742\(\pm\)0.167 & 0.714\(\pm\)0.114 & 0.821\(\pm\)0.162 & **0.847\(\pm\)**0.158 \\  & _DockQ_\(\uparrow\) & 0.084\(\pm\)0.113 & 0.118\(\pm\)0.192 & 0.314\(\pm\)0.390 & 0.030\(\pm\)0.029 & 0.469\(\pm\)0.396 & **0.564\(\pm\)**0.369 \\ \hline \multirow{3}{*}{**VH-VL**} & _RMSD_\(\downarrow\) & 10.982\(\pm\)3.864 & 5.899\(\pm\)5.688 & 2.032\(\pm\)2.388 & 18.293\(\pm\)2.871 & 1.325\(\pm\)0.530 & **1.242\(\pm\)**0.602 \\  & _TM-score_\(\uparrow\) & 0.596\(\pm\)0.025 & 0.792\(\pm\)0.156 & 0.926\(\pm\)0.100 & 0.559\(\pm\)0.017 & 0.962\(\pm\)0.020 & **0.966\(\pm\)**0.021 \\  & _DockQ_\(\uparrow\) & 0.108\(\pm\)0.134 & 0.404\(\pm\)0.277 & 0.705\(\pm\)0.201 & 0.032\(\pm\)0.016 & 0.765\(\pm\)0.094 & **0.773\(\pm\)**0.187 \\ \hline \multirow{3}{*}{**AB-AG**} & _RMSD_\(\downarrow\) & 18.892\(\pm\)3.757 & 15.670\(\pm\)6.996 & 15.779\(\pm\)6.364 & 18.468\(\pm\)2.706 & 13.650\(\pm\)5.586 & **9.707\(\pm\)**8.759 \\  & _TM-score_\(\uparrow\) & 0.504\(\pm\)0.059 & 0.596\(\pm\)0.143 & 0.612\(\pm\)0.137 & 0.502\(\pm\)0.096 & 0.640\(\pm\)0.124 & **0.773\(\pm\)**0.187 \\ \cline{1-1}  & _DockQ_\(\uparrow\) & 0.035\(\pm\)0.031 & 0.108\(\pm\)0.181 & 0.090\(\pm\)0.187 & 0.043\(\pm\)0.017 & 0.108\(\pm\)0.172 & **0.342\(\pm\)**0.351 \\ \cline{1-1}  & _maxDockQ_\(\uparrow\) & 0.042\(\pm\)0.043 & 0.136\(\pm\)0.220 & 0.111\(\pm\)0.237 & 0.043\(\pm\)0.018 & 0.125\(\pm\)0.214 & **0.414\(\pm\)**0.386 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative results on protein docking. (bold: best; underline: runner-up)

\begin{table}
\begin{tabular}{l|l|l l l l} \hline \hline
**Datasets** & **\# Pairs of Proteins** & **\# Residues per Protein** & **\# Atoms per Protein** \\ \hline
**Training Set** & 4890 & 565.9 (\(\pm\)264.9) & 4334.4 (\(\pm\)2028.3) \\
**DB5.5 (Test)** & 24 & 428.4 (\(\pm\)132.0) & 3308.0 (\(\pm\)1000.5) \\
**VH-VL (Test)** & 68 & 230.1 (\(\pm\)5.4) & 1749.9 (\(\pm\)53.7) \\
**AB-AG (Test)** & 68 & 433.0 (\(\pm\)72.0) & 3346.7 (\(\pm\)568.8) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of datasets.

predictions, we report universally accepted metrics Root Mean Square Deviation (RMSD), TM-score (Template Modeling score), and DockQ [6]. Please note that in the context of antibody-antigen docking, the original DockQ metric evaluates the docking performance by treating the entire antibody as a single entity to the antigen. Furthermore, we can assess the docking results separately for variable heavy/light chain (VH/VL) to the antigen and denote the maximum value as maxDockQ. Refer to Appendix B for details of experiments, including implementation and hyperparameters.

### Main Results and Analysis

Docking ResultsTable 2 demonstrates that BiDock generally produces acceptable predictions on all datasets. Notably, BiDock achieves a significant improvement in performance on the more challenging antibody-antigen docking, with a 234% relative gain over the runner-up on the DockQ metric. These results highlight the effectiveness of our bi-level optimization, which effectively leverages multimodal information. We also observe that some established docking software, such as HDOCK, still provides reliable predictions. In contrast, deep learning methods may have some leeway in performance. For instance, the mean and deviation of RMSD evaluated from EquiDock are relatively large, indicating that some inappropriate SE(3) transformations are learned.

To be more intuitive, we select representative baselines, HDOCK and Multimer, along with BiDock to analyze their performance differences on antibody-antigen docking. The distribution of DockQ for each method is displayed using a violin plot, and a direct comparison between Multimer and BiDock is also drawn in Figure 4. It reveals that the distribution of DockQ for BiDock is more concentrated around high values, indicating a higher percentage of successful docking predictions. Overall, these results suggest that our proposed BiDock is a promising approach for rigid protein docking.

VisualizationTo visually demonstrate the superiority of our proposed BiDock, we chose the spike glycoprotein \(7F6Z\) as an example antigen. The spike glycoprotein is crucial for the entry of coronaviruses into host cells, making it a target of great interest for therapeutic intervention and vaccine development. In Figure 5, we align the ground truth structure of this protein with predictions from competitive baselines. Upon inspection, it is evident that predictions from baselines exhibit noticeable deviations from the ground truth structures. Such inaccuracies can significantly impact our understanding of the binding mechanisms and hinder the design of effective interventions. In contrast,

Figure 4: An intuitive comparison of DockQ metric on antibody-antigen docking. The box inside (a) violin plot represents 25-75 percentiles, and the median is shown by a white dot. Scatters in (b) scatter plot appear under the dashed diagonal line, indicating that BiDock outperforms Multimer on these complexes.

Figure 5: Structure comparison between predictions and the ground truth of protein complex \(7F6Z\). The ground truth structures are represented in light gray, while predictions are colored cyan.

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Methods** & **Inference time** \\ \hline
**ZDOCK** & 72.61 \\
**ClusPro** & 87.27 \\
**HDOCK** & 20.40 \\
**EquiDock** & 0.60 \\
**Multimer** & 1.07 \\
**BiDock** & 1.47 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Total inference time of different methods on antibody-antigen docking. (unit: hour)BiDock successfully captures the correct docking interface and accurately predicts the binding pose, highlighting its potential for providing biological insights and aiding drug design.

Computational EfficiencyTable 3 shows the total inference time for antibody-antigen complexes. Traditional docking software, involving candidate sampling, ranking, and refinement steps, incurs substantial computational costs. Fortunately, deep learning methods provide a significant speed-up, which is particularly important in efficient screening. Although EquiDock is the fastest, its performance falls short of traditional software due to limitations in leveraging coevolution information and simple networks. On the other hand, Multimer and BiDock exhibit comparable inference times. Considering the performance improvement of our model, this trade-off is acceptable.

### Ablation Studies

Effects of Spectral InitializationRecalling our utilization of spectral estimation to derive a numerical solution for initializing the gradient descent in the inner loop, we conduct ablation experiments to demonstrate the effectiveness of this spectral initialization. Table 4 presents the results on antibody-antigen docking, where the variant without spectral initialization is denoted as "w/o SI" and the number of gradient descent steps is listed in parentheses. It is seen that the "w/o SI" variants exhibit slower convergence and lower accuracy, even when we further increase the number of gradient descent steps. These findings further support the benefits of our proposed spectral initialization in accelerating the optimization process and achieving superior results.

Effects of Bi-level OptimizationTo justify the effectiveness of bi-level optimization, we use a two-stage strategy: training outer and inner loops separately. Specifically, we use cross-entropy and masked MSA losses to train cross-modal transformer. Based on the resulting distance map, we compute roto-translation transformation with spectral initialization. The results on antibody-antigen docking are presented in Table 4, where "w/o Bi" denotes the variant without bi-level optimization (results reported from the original paper [34]). Results support our contributions of employing bi-level optimization for end-to-end optimization, which customizes the parameter learning of the cross-modal transformer for rigid docking.

Effects of Masked MSA LossIn addition to the essential loss for the distance map, we introduce a masked MSA loss in the outer loop to supervise the learning of evolution representations. We also conduct an ablation study to investigate its significance, as shown in Table 4. The "w/o MM" refers to models without masked MSA loss. According to the results, it can be concluded that the masked MSA loss enables the cross-modal transformer to more effectively leverage the rich evolutionary information and seamlessly integrate it with the structure-modal information.

Effects of OptimizerAs shown in Equation (15), we have the flexibility to choose different optimizers for the inner loop. To explore the impact of different optimizers on convergence speed and performance, we compare the effects of SGD and Adam optimizers. We vary the number of

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **RMSD** & **DockQ** \\ \hline
**w/o SI(1)** & 14.140\(\pm\)8.032 & 0.187\(\pm\)0.256 \\
**w/o SI(2)** & 11.191\(\pm\)8.754 & 0.290\(\pm\)0.328 \\
**w/o SI(4)** & 9.806\(\pm\)8.646 & 0.335\(\pm\)0.347 \\
**w/o Bi(2)** & 10.090\(\pm\)7.817 & 0.220\(\pm\)0.232 \\
**w/o MM(2)** & 9.821\(\pm\)8.688 & 0.330\(\pm\)0.350 \\ \hline
**BiDock(2)** & **9.707\(\pm\)**8.759 & **0.342\(\pm\)**0.351 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation studies on spectral initialization, bi-level optimization, and masked MSA loss. (number in parentheses: gradient descent steps; unit: thousand; bold: best)

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **RMSD** & **DockQ** \\ \hline
**SGD(1)** & 9.939\(\pm\)8.525 & 0.333\(\pm\)0.341 \\
**SGD(2)** & 9.940\(\pm\)8.557 & 0.335\(\pm\)0.344 \\
**SGD(5)** & 9.914\(\pm\)8.566 & 0.337\(\pm\)0.345 \\
**SGD(10)** & 9.861\(\pm\)8.548 & 0.337\(\pm\)0.344 \\
**Adam(1)** & 9.780\(\pm\)8.773 & 0.341\(\pm\)0.350 \\ \hline
**Adam(2)** & **9.707\(\pm\)**8.759 & **0.342\(\pm\)**0.351 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Impacts of different optimizers on antibody-antigen docking. (number in parentheses: gradient descent steps; unit: thousand; bold: best)gradient descent steps and evaluate the final performance, as displayed in Table 5. With Adam, BiDock achieves faster convergence and attains higher accuracy, indicating that a better optimizer can navigate the landscape and find better minima during the optimization of the inner loop.

## 5 Conclusion

In this study, we introduce BiDock, a novel model for rigid protein docking that tackles the challenge of accurately predicting the 3D structure of protein complexes from unbound states. By formulating this problem as a bi-level optimization, BiDock combines the advantages of integrating multimodal information by a cross-modal transformer in the outer loop and maintaining the rigidity of learning roto-translation transformation in the inner loop. Additionally, we derive a spectral initialization to expedite convergence. The maximum 234% relative performance improvement validates the effectiveness of BiDock in rigid protein docking.

Limitations and Broader ImpactDespite the encouraging results, BiDock does not explicitly incorporate geometric constraints between residues when learning the distance map and does not account for potential atom clashes. Our future work will address these limitations and extend our framework to general proteins. Protein docking deepens our understanding of biological mechanisms and aids in the design of targeted interventions. This research may inspire the AI4Science community to pay more attention to the practical challenges in docking and promote further advancements in this field with significant real-world implications.

## Acknowledgments and Disclosure of Funding

AcknowledgmentsWe would like to extend our heartfelt appreciation to the anonymous reviewers who dedicated their valuable time and expertise to meticulously review this paper. Their constructive feedback significantly contributed to the refinement and enhancement of the quality of our work. Our gratitude also extends to our team members, whose insightful discussions and collaborative brainstorming sessions were instrumental in shaping this research.

We wish to express our sincere thanks to the Data Science team at BioMap, especially to Tingting Tang, Nachuan Shan, Shiro Hong, Di Wang, Dacheng Li, Chenrui Xu, Ke Wang, and Cheng Chen, for their exceptional support in data collection, processing, and analysis. Their efforts with complex biological datasets were indispensable to the success of our research. Furthermore, we also express our sincere thanks to the AI Infracture team at BioMap, especially to Ming Yang, Chuang Yu, Zedong Zheng, Zezhi Wang, Qian Wang, and Xiaoming Zhang, for their outstanding technical support and seamless collaboration. Their proactive approach and effective troubleshooting played a vital role in ensuring the smooth progress of our research.

Finally, thanks Aaron for proofreading the manuscript.

Author ContributionsThe author's contributions to this paper encompass a range of vital aspects. Ruijia Wang made substantial contributions to content, writing, and experiment design. Yiwu Sun took on a central role in implementing the bi-level optimization algorithm and conducting experiments. Yujie Luo and Shaochuan Li were crucial in developing the precursor algorithm, xTrimoDock [34], and also aiding in data preparation.

Le Song and Hui Li lead the antibody-antigen structure prediction project in BioMap. They made invaluable contributions by providing algorithmic insights and guidance to the project, influencing the overall conceptualization and direction of the research. Xingyi Cheng, Chuan Shi and Cheng Yang provided valuable guidance in organizing the writing of the paper.

This work was funded by BioMap. The intellectual property of BiDock is 100% owned by BioMap.

## References

* Bae and Grosse [2020] Juhan Bae and Roger B Grosse. Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. In _NeurIPS_, pages 21725-21737, 2020.
* Baek et al. [2021] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of protein structures and interactions using a three-track neural network. _Science_, 373(6557):871-876, 2021.
* Bank [1971] Protein Data Bank. Protein data bank. _Nature New Biol_, 233:223, 1971.
* Bao et al. [2021] Jingxiao Bao, Xiao He, and John ZH Zhang. Deepbsp--a machine learning method for accurate prediction of protein-ligand docking structures. _Journal of Chemical Information and Modeling_, 61(5):2231-2240, 2021.
* Bar-Itzhack [2000] Itzhack Y Bar-Itzhack. New method for extracting the quaternion from a rotation matrix. _Journal of guidance, control, and dynamics_, 23(6):1085-1087, 2000.
* Basu and Wallner [2016] Sankar Basu and Bjorn Wallner. Dockq: a quality measure for protein-protein docking models. _PloS one_, 11(8):e0161879, 2016.
* Biesiada et al. [2011] Jacek Biesiada, Aleksey Porollo, Prakash Velayutham, Michal Kouril, and Jaroslaw Meller. Survey of public domain software for docking simulations and virtual screening. _Human genomics_, 5(5):1-9, 2011.
* Bohdal et al. [2021] Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Evograd: Efficient gradient-based meta-learning and hyperparameter optimization. In _NeurIPS_, pages 22234-22246, 2021.
* Bracken and McGill [1973] Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the constraints. _Operations research_, 21(1):37-44, 1973.

* Bryant et al. [2022] Patrick Bryant, Gabriele Pozzati, and Arne Elofsson. Improved prediction of protein-protein interactions using alphafold2. _Nature communications_, 13(1):1-11, 2022.
* Chen et al. [2022] Can Chen, Xi Chen, Chen Ma, Zixuan Liu, and Xue Liu. Gradient-based bi-level optimization for deep learning: A survey. _arXiv preprint arXiv:2207.11719_, 2022.
* Chen et al. [2023] Can Chen, Jingbo Zhou, Fan Wang, Xue Liu, and Dejing Dou. Structure-aware protein self-supervised learning. _Bioinformatics_, 39(4):btd189, 2023.
* Chen et al. [2003] Rong Chen, Li Li, and Zhiping Weng. Zdock: an initial-stage protein-docking algorithm. _Proteins: Structure, Function, and Bioinformatics_, 52(1):80-87, 2003.
* Chen et al. [2019] Yihong Chen, Bei Chen, Xiangnan He, Chen Gao, Yong Li, Jian-Guang Lou, and Yue Wang. \(\lambda\)opt: Learn to regularize recommender models in finer levels. In _KDD_, pages 978-986, 2019.
* Christoffer et al. [2021] Charles Christoffer, Siyang Chen, Vijay Bharadwaj, Tunde Aderinwale, Vidhur Kumar, Matin Hormati, and Daisuke Kihara. Lzerd webserver for pairwise and multiple protein-protein docking. _Nucleic Acids Research_, 49(W1):W359-W365, 2021.
* Corso et al. [2022] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi S Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. In _NeurIPS_, 2022.
* Dai and Bailey-Kellogg [2021] Bowen Dai and Chris Bailey-Kellogg. Protein interaction interface region prediction by geometric deep learning. _Bioinformatics_, 37(17):2580-2588, 2021.
* Davila et al. [2022] Ana Davila, Zichang Xu, Songling Li, John Rozewicki, Jan Wilamowski, Sergei Kotelnikov, Dima Kozakov, Shunsuke Teraguchi, and Daron M Standley. Abadapt: an adaptive approach to predicting antibody-antigen complex structures from sequence. _Bioinformatics Advances_, 2(1):vba015, 2022.
* De Vries et al. [2010] Sjoerd J De Vries, Marc Van Dijk, and Alexandre MJJ Bonvin. The haddock web server for data-driven biomolecular docking. _Nature protocols_, 5(5):883-897, 2010.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Eberhardt et al. [2021] Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. _Journal of Chemical Information and Modeling_, 61(8):3891-3898, 2021.
* Eismann et al. [2021] Stephan Eismann, Raphael JL Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, and Ron O Dror. Hierarchical, rotation-equivariant neural networks to select structural models of protein complexes. _Proteins: Structure, Function, and Bioinformatics_, 89(5):493-501, 2021.
* Evans et al. [2021] Richard Evans, Michael O'Neill, Alexander Pritzel, Natasha Antropova, Andrew W Senior, Timothy Green, Augustin Zidek, Russell Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with alphafold-multimer. _BioRxiv_, 2021.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _ICML_, pages 1126-1135. PMLR, 2017.
* Ganea et al. [2022] Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S. Jaakkola, and Andreas Krause. Independent se(3)-equivariant models for end-to-end rigid protein docking. In _ICLR_, 2022.
* Humphreys et al. [2021] Ian R Humphreys, Jimin Pei, Minkyung Baek, Aditya Krishnakumar, Ivan Anishchenko, Sergey Ovchinnikov, Jing Zhang, Travis J Ness, Sudeep Banjade, Saket R Bagde, et al. Computed structures of core eukaryotic protein complexes. _Science_, 374(6573):eabm4805, 2021.
* Jin et al. [2022] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Antibody-antigen docking and design via hierarchical structure refinement. In _ICML_, pages 10217-10227. PMLR, 2022.

* [28] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [29] Dima Kozakov, David R Hall, Bing Xia, Kathryn A Porter, Dzmitry Padhorny, Christine Yueh, Dmitri Beglov, and Sandor Vajda. The cluspro web server for protein-protein docking. _Nature protocols_, 12(2):255-278, 2017.
* [30] Elodie Laine, Stephan Eismann, Arne Elofsson, and Sergei Grudinin. Protein sequence-to-structure learning: Is this the end (-to-end revolution)? _Proteins: Structure, Function, and Bioinformatics_, 89(12):1770-1786, 2021.
* [31] Ingoo Lee, Jongsoo Keum, and Hojung Nam. Deepconv-dti: Prediction of drug-target interactions via deep learning with convolution on protein sequences. _PLoS computational biology_, 15(6):e1007129, 2019.
* [32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In _ICLR_, 2018.
* [33] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10045-10067, 2021.
* [34] Yujie Luo, Shaochuan Li, Yiwu Sun, Ruijia Wang, Tingting Tang, Beiqi Hongdu, Xingyi Cheng, Chuan Shi, Hui Li, and Le Song. xtrimodock: Rigid protein docking via cross-modal representation learning and spectral algorithm. _bioRxiv_, pages 2023-02, 2023.
* [35] Andrew T McNutt, Paul Francoeur, Rishal Aggarwal, Tomohide Masuda, Rocco Meli, Matthew Ragoza, Jocelyn Sunseri, and David Ryan Koes. Gnina 1.0: molecular docking with deep learning. _Journal of cheminformatics_, 13(1):1-20, 2021.
* [36] Matthew McPartlon and Jinbo Xu. Deep learning for flexible and site-specific protein docking and design. _bioRxiv_, pages 2023-04, 2023.
* [37] Iain H Moal, Mieczyslaw Torchala, Paul A Bates, and Juan Fernandez-Recio. The scoring of poses in protein-protein docking: current capabilities and future directions. _BMC bioinformatics_, 14(1):1-15, 2013.
* [38] Thin Nguyen, Hang Le, Thomas P Quinn, Tri Nguyen, Thuc Duy Le, and Svetha Venkatesh. Graphdta: Predicting drug-target binding affinity with graph neural networks. _Bioinformatics_, 37(8):1140-1147, 2021.
* [39] Hakime Ozturk, Arzucan Ozgur, and Elif Ozkirimli. Deepdta: deep drug-target binding affinity prediction. _Bioinformatics_, 34(17):i821-i829, 2018.
* [40] Jimin Pei, Jing Zhang, and Qian Cong. Human mitochondrial protein complexes revealed by large-scale coevolution analysis and deep learning-based structure modeling. _Bioinformatics_, 38(18):4301-4311, 2022.
* [41] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. In _NeurIPS_, 2019.
* [42] Anna Rutkowska, Douglas W Thomson, Johanna Vappiani, Thilo Werner, Katrin M Mueller, Lars Dittus, Jana Krause, Marcel Muelbaier, Giovanna Bergamini, and Marcus Bantscheff. A modular probe strategy for drug localization, target identification and target occupancy measurement on single cell level. _ACS chemical biology_, 11(9):2541-2550, 2016.
* [43] Rita Santos, Oleg Ursu, Anna Gaulton, A Patricia Bento, Ramesh S Donadi, Cristian G Bologa, Anneli Karlsson, Bissan Al-Lazikani, Anne Hersey, Tudor I Oprea, et al. A comprehensive map of molecular drug targets. _Nature reviews Drug discovery_, 16(1):19-34, 2017.
* [44] Christina EM Schindler, Isaure Chauvot de Beauchene, Sjoerd J de Vries, and Martin Zacharias. Protein-protein and peptide-protein docking and refinement using attract in capri. _Proteins: Structure, Function, and Bioinformatics_, 85(3):391-398, 2017.

* [45] Sharon Sunny and PB Jayaraj. Fpdcock: Protein-protein docking using flower pollination algorithm. _Computational Biology and Chemistry_, 93:107518, 2021.
* [46] Mieczyslaw Torchala, Iain H Moal, Raphael AG Chaleil, Juan Fernandez-Recio, and Paul A Bates. Swarmdock: a server for flexible protein-protein docking. _Bioinformatics_, 29(6):807-809, 2013.
* [47] Ilya A Vakser. Protein-protein docking: From interaction to interactome. _Biophysical journal_, 107(8):1785-1793, 2014.
* [48] Thirumalaisamy P Velavan and Christian G Meyer. The covid-19 epidemic. _Tropical medicine & international health_, 25(3):278, 2020.
* [49] Vishwesh Venkatraman, Yifeng D Yang, Lee Sael, and Daisuke Kihara. Protein-protein docking using region-based 3d zernike descriptors. _BMC bioinformatics_, 10(1):1-21, 2009.
* [50] Jacob Verburgt and Daisuke Kihara. Benchmarking of structure refinement methods for protein complex models. _Proteins: Structure, Function, and Bioinformatics_, 90(1):83-95, 2022.
* [51] Heinrich Von Stackelberg. _Market structure and equilibrium_. Springer Science & Business Media, 2010.
* [52] Thom Vreven, Iain H Moal, Anna Vangone, Brian G Pierce, Panagiotis L Kastritis, Mieczyslaw Torchala, Raphael Chaleil, Brian Jimenez-Garcia, Paul A Bates, Juan Fernandez-Recio, et al. Updates to the integrated protein-protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2. _Journal of molecular biology_, 427(19):3031-3041, 2015.
* [53] Izhar Wallach, Michael Dzamba, and Abraham Heifets. Atomnet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery. _arXiv preprint arXiv:1510.02855_, 2015.
* [54] Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, Yiwu Sun, Yujie Luo, Hui Li, and Le Song. Fast de novo antibody structure prediction with atomic accuracy. _Cancer Research_, 83(7_Supplement):4296-4296, 2023.
* [55] Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, YiWu Sun, Chuan Shi, Hui Li, Yangang Wang, Cheng Yang, and Le Song. xtrimoabfold: Improving antibody structure prediction without multiple sequence alignments. _arXiv preprint arXiv:2212.00735_, 2022.
* [56] Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, Yiwu Sun, Chuan Shi, Yangang Wang, Cheng Yang, Hui Li, and Le Song. xtrimoabfold: De novo antibody structure prediction without msa. _ArXiv, abs/2212.00735_, 2022.
* [57] Gaoqi Weng, Ercheng Wang, Zhe Wang, Hui Liu, Feng Zhu, Dan Li, and Tingjun Hou. Hawk-dock: a web server to predict and analyze the protein-protein complex based on computational docking and mm/gbsa. _Nucleic acids research_, 47(W1):W322-W330, 2019.
* [58] Yumeng Yan, Huanyu Tao, Jiahua He, and Sheng-You Huang. The hdock server for integrated protein-protein docking. _Nature protocols_, 15(5):1829-1852, 2020.
* [59] Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. In _NeurIPS_, pages 13670-13682, 2021.
* [60] Chia-Hung Yuan and Shan-Hung Wu. Neural tangent generalization attacks. In _ICML_, pages 12230-12240. PMLR, 2021.
* [61] Jonas Zehnder, Yue Li, Stelian Coros, and Bernhard Thomaszewski. Ntopo: Mesh-free topology optimization using implicit neural representations. In _NeurIPS_, pages 10368-10381, 2021.
* [62] Tian-ming Zhou, Sheng Wang, and Jinbo Xu. Deep learning reveals many more inter-protein residue-residue contacts than direct coupling analysis. _BioRxiv_, page 240754, 2018.
* [63] Marinka Zitnik, Francis Nguyen, Bo Wang, Jure Leskovec, Anna Goldenberg, and Michael M Hoffman. Machine learning for integrating data in biology and medicine: Principles, practice, and opportunities. _Information Fusion_, 50:71-91, 2019.

Details of Dataset

Background of AntibodiesAntibodies are vital components of the immune system and are classified into various classes, including IgG, IgM, IgA, IgG, and IgE. Among them, IgG antibodies are the most abundant in the bloodstream and play a primary role in immune responses against pathogens.

As depicted in Figure 6, IgG antibodies exhibit a Y-shaped structure composed of two identical light chains and two identical heavy chains, where heavy chains provide structural stability. Each antibody chain is further divided into distinct regions. (1) The variable regions, referred to as the variable heavy (VH) and variable light (VL) regions, are located at the tips of the Y arms. These regions contribute to the specificity of antibodies in recognizing and binding to antigens. The VH and VL regions collaborate to form the fragment antigen-binding (Fab) region. (2) At the base of the Y structure, the constant regions, also known as the fragment crystallizable (Fc) region, are important in the effector functions of antibodies. The Fc region interacts with immune cells and triggers immune responses, such as the activation of complement proteins for pathogen destruction and the promotion of phagocytosis.

Given this background knowledge of antibodies, it becomes clear that antibody-antigen docking is fundamental in immune responses, therapeutic applications, vaccine development, and drug discovery. Therefore, our study places a particular emphasis on antibody-antigen docking, contributing to this field by curating a high-quality benchmark. This dataset will serve as a valuable resource for evaluating computational models in predicting antibody-antigen interactions, ultimately facilitating the development of novel therapeutics and immunological interventions.

Antibody-antigen BenchmarkThe training set comprises 4,890 complexes of antibody-antigen pairs, each consisting of proteins with a minimum of 30 residues. These complexes encompass three chains, including the light and heavy chains of the antibody, along with one antigen chain. All complexes were released before January 2022. Similarly, the test set consists of 68 antibody-antigen complexes with three chains, released after October 2022. Thus, we ensure that neither baselines nor our proposed model was trained using the test set and avoid data leakage.

In practical applications, obtaining the ground truth structures of antibody-antigen complexes poses significant challenges. Researchers often turn to existing folding models to predict them. To simulate real-world scenarios, we employ a specialized antibody model called xTrimoABFold [55; 56; 54] to predict the conformations of antibodies and AlphaFold2 [28] for antigens. Given these predicted structures as rigid structures, we construct training and test datasets essential for further analysis and investigation. Here are the PDB identifiers and their corresponding chain identifiers for the test set.

8dls:A,H,L; 8dlr:A,H,L; 8dfi:A,H,L; 8dfh:A,H,L; 8dcc:E,L,H; 8dad:H,L,B; 7zr8:A,H,L; 7zf8:H,L,E;

7xxl:C,A,B; 7xb:A,B,C; 7x26:H,I,K; 7wsl:L,D,H; 7wsi:A,H,L; 7ws6:C,H; 7ws2:D,A,E;

7wrz:H,L,R; 7wrv:C,U,V; 7wro:H,L,R; 7wrl:A,B,R; 7wrj:R,A,B; 7wog:C,A,B; 7wlc:E,H,L;

7wef:C,E,L; 7wee:E,H,L; 7wef:E,H,L; 7wcr:A,a,b; 7wbz:A,H,L; 7urq:A,H,L; 7uaq:A,H,L;

7ty:A,H,L; 7ttx:A,H,L; 7ttm:A,H,L; 7tpj:B,H,L; 7tq:Z,H,L; 7tp3:Z,H,L; 7tlz:A,B,J;

7the:A,B,C; 7tc9:B,H,L; 7t8w:L,H,D; 7t7b:A,H,L; 7t01:A,H,L; 7swp:A,H,L; 7su1:H,L,C;

7str:L,H,C; 7sem:B,F,C; 7sd5:A,H,L; 7sbu:A,H,L; 7sbg:H,L,C; 7sbd:H,L,C; 7sa6:A,H,L;

7s5p:A,H,L; 7rxp:L,H;A; 7rxi:L,H; 7rb:B,H,L; 7qtk:D,B,C; 7n0a:C,A,B; 7l08:Z,H,L;

7lo7:Z,H,L; 7kql:H,L,T; 7fjc:H,L,E; 7f7e:C,L,E; 7f6z:R,H,L; 7f6y:R,H,L; 7eng:L,H,B;

7ek0:R,H,L; 7ejz:R,H,L; 7ejy:R,H,L; 7e9p:B,L,H

Figure 6: Structure of an IgG antibody. The heavy chain is colored orange, while the light chain is blue.

Details of Implementation

BaselinesZDOCK5, ClusPro6, and HDOCK7 are user-friendly local packages suitable for automated experiments or web servers for manual submissions. We select the top-1 predicted structure from each of these methods for subsequent evaluation. For EquiDock8 and Multimer9, we utilize their pretrained models available on GitHub for the inference. It is worth emphasizing that all methods except Multimer are designed for docking two chains. Therefore, during the evaluation, we employ a sequential docking strategy. This entails initially docking the light chain and heavy chain together, followed by treating them as a unified entity for docking with the antigen. And we calculate evaluation metrics using the tools USalign10 and DockQ11.

Footnote 5: https://zdock.umassmed.edu

Footnote 6: https://cluspro.org

Footnote 7: http://hdock.phys.hust.edu.cn

Footnote 8: (MIT license) https://github.com/octavian-ganea/equidock_public

Footnote 9: (Apache-2.0 license) https://github.com/aqlaboratory/openfold

Footnote 10: (MIT license) https://github.com/pylelab/USalign

Footnote 11: (GPL-3.0 license) https://github.com/bjornwallner/DockQ

#### MSA Extraction

We utilize the heuristic approach described in [23] to pair sequences from per-chain multiple sequence alignments (MSAs). Initially, the per-chain MSA sequences are grouped based on species, with the species labels obtained from UniProt's idmapping 12. Within each specific species group, the sequences are paired together. We match the chain MSAs by minimizing the base-pair distance between the chains for prokaryotic species. While in terms of eukaryotic species, we order them based on sequence identity to the target sequence [62]. To reduce computational and memory costs, we employ the MSA clustering approach from AlphaFold2 [28]. We randomly select \(N_{cls}=252\) sequences as the MSA cluster centers, with the primary protein sequence always set as the first cluster center. The remaining sequences are assigned to their closest cluster based on the Hamming distance.

Footnote 12: https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping

#### Sequence-modal Input

The sequence modality incorporates information derived from the primary sequence itself and co-evolutionary information obtained from MSAs. Following prior research [28; 23], we extract two types of features: type features \(F^{typ}\in\mathbb{R}^{N_{res}\times 21}\) and primary pair features \(F^{pp}\in\mathbb{R}^{N_{res}\times N_{res}\times 73}\) from the primary sequence, where \(N_{res}\) represents the number of residues. Regarding MSAs, we utilize cluster MSA features \(F^{msa}\in\mathbb{R}^{N_{cls}\times N_{res}\times 49}\), where \(N_{cls}\) denotes the number of cluster centers. Specifically,

* The _type feature_\(F^{typ}\in\mathbb{R}^{N_{res}\times 21}\) comprises one-hot representations of the amino acid types, encompassing the 20 known amino acids and one additional category for unknown types.
* The _primary pair feature_\(F^{pp}\in\mathbb{R}^{N_{res}\times N_{res}\times 73}\) contains positional information within or across chains, including three components. (1) The _relative positional feature_ of size \([N_{res},N_{res},66]\) represents the relative residue indices, which are clipped between \([-32,32]\). The \(66\)-th index is used to indicate cross-chain pairs. (2) The _entity indicator_ of size \([N_{res},N_{res},1]\) identifies whether residues \(i\) and \(j\) originate from the same chain. (3) The _relative index feature_ of size \([N_{res},N_{res},6]\) introduces the relative _sym_id13_ indices clipped between \([-2,2]\). The \(6\)-th index is assigned to pairs where the two residues have different _sym_ids_. Footnote 13: The _sym_id_ is used to distinguish chains with the same sequence. For example, we consider a complex comprising five chains \(\{A,B,B,C,C\}\), where \(A\), \(B\), and \(C\) represent three unique chains. The corresponding _sym_ids_ for each chain would be \(\{1,1,2,1,2\}\), respectively.
* The _cluster MSA feature_\(F^{msa}\in\mathbb{R}^{N_{cls}\times N_{res}\times 49}\) consists of five components. (1) The _one-hot representation of the amino acid types_ with size [\(N_{cls}\), \(N_{res}\), 23], including 20 amino acids, one unknown type, one gap or missing residue, and one mask token as introduced in Section 3.1. (2) The _amino acid distribution_ of size \([N_{cls},N_{res},23]\) represents the distribution of amino acid types within each MSA cluster. (3) The _deletion indicator_ of size \([N_{cls},N_{res},1]\) indicates whether there is a deletion to the left of each residue. (4) The _deletion value_ of size \([N_{cls},N_{res},1]\) is calculated using the formula \(\frac{2}{\pi}\arctan\frac{c}{3}\), where \(c\) refers to the number of deletions to the left of each position.

(5) The _mean deletion value_ of size \([N_{cls},N_{res},1]\) is computed as \(\frac{2}{n}\arctan\frac{\bar{c}}{3}\), where \(\bar{c}\) represents the average number of deletions to all residues on the left of each position.

Structure-modal InputFor the structure modality, we extract angle features \(F^{ang}\in\mathbb{R}^{N_{res}\times 57}\) and pair features \(F^{p}\in\mathbb{R}^{N_{res}\times N_{res}\times 88}\) from the rigid protein structures. These features capture important structure-modal information and are used as input for our docking model. Specifically,

* The _angle feature_\(F^{ang}\in\mathbb{R}^{N_{res}\times 57}\) comprises three components. (1) The _one-hot representation of the amino acid types_ with a size of \([N_{res},22]\), including 20 amino acids, one unknown type, and one gap or missing residue. (2) The _angle representations_ of size \([N_{res},28]\) use sine and cosine to encode three backbone torsion angles, four side-chain torsion angles, and alternative torsion angles with \(180^{\circ}\) rotation symmetry for each local frame of residue. (3) The _angle indicator_ with size \([N_{res},7]\) indicates the presence or absence of torsion angles.
* The _pair feature_\(F^{p}\in\mathbb{R}^{N_{res}\times N_{res}\times 88}\) comprises five components. (1) The _distogram feature_ of size \([N_{res},N_{res},39]\) represents the discretized distances between C\(\beta\) atoms. In the case of glycine, which lacks C\(\beta\) atoms, C\(\alpha\) is used instead. The distances are discretized into 38 bins of equal width ranging from 3.25 to 50.75A, with an additional bin accounting for larger distances. (2) The _residue type feature_ of size \([N_{res},N_{res},44]\) is derived from expanding one-hot representations of residue types with dimensions of \([N_{res},1,22]\) and \([N_{res},22,1]\). (3) The _backbone feature_ of size \([N_{res},N_{res},3]\) is obtained by constructing the unit vector of the local frame through the Gram-Schmidt process based on the original N-C\(\alpha\)-C coordinates. (4) The _residue indicator_ with size \([N_{res},N_{res},1]\) is expanded from the indicator of residue existence. (5) The _pair indicator_ of size \([N_{res},N_{res},1]\) indicates whether the pair is masked.

MSA Mask PolicyReflecting on Section 3.1, we design a masked MSA loss to supervise the learning of evolution representations and the integration of cross-modal information. Specifically, we randomly mask each position in an MSA cluster center with a 15% probability. Each masked token is replaced according to the following policies:

* 70% probability of substitution with a special token \(\star\)
* 10% probability of substitution with a randomly selected amino acid from a uniform distribution
* 10% probability of substitution with an amino acid sampled from the MSA profile that corresponds to the position
* 10% probability of no substitution

Hyperparameter SettingsWe initialize specific parameters of the cross-modal transformer with the checkpoint of Multimer and implement bi-level optimization using TorchOpt14 library. The crop size is set to 412, and the batch size is set to 1. The coefficients in Equation (13) are \(\lambda_{1}=0.2\), \(\lambda_{2}=2.0\), and \(\lambda_{3}=10.0\). For optimization, we employ the Adam optimizer with a learning rate of \(10^{-4}\) and integrate learning rate warmup, gradually increasing the learning rate from 0 to \(10^{-4}\) within the first 100 steps. The exponential moving average (EMA) strategy applies a decay rate of \(\beta=0.999\) and undergoes updates every 200 steps. The environment where we run all experiments is:

Footnote 14: (Apache-2.0 license) https://github.com/metaopt/torchopt

* Operating system: Linux version 5.13.0-30-generic
* CPU information: AMD EPYC 7742 64-Core Processor
* GPU information: NVIDIA A100-SXM4-80GB

## Appendix C Additional Results

Effects of Noisy StructuresClassical software rely on score functions derived from statistics in the protein data bank. This dependency renders them susceptible to noise. When using folding algorithms to predict unbounded proteins, the performance of these software can degrade significantly. To validate this intuition, we conduct a docking performance analysis on the DB5.5 dataset using ground truth and predicted structures from folding models as unbounded structures, respectively. As shown in Table 6, these results illustrate that although HDOCK performs exceptionally well with ground truth, minor noise in predicted structures leads to a substantial decline in its performance. On the contrary, BiDock consistently generates acceptable predictions regardless of the input type, showcasing its robustness to noise. In real-world applications, reliance on the availability of ground truth structures is impractical. The ability of BiDock to maintain high prediction quality when confronted with noisy structures makes it an invaluable tool.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Ground Truth**} & \multicolumn{3}{c}{**Predicted Structure**} \\ \cline{2-7}  & _RMSD \(\downarrow\)_ & _TM-score \(\uparrow\)_ & _DockQ \(\uparrow\)_ & _RMSD \(\downarrow\)_ & _TM-score \(\uparrow\)_ & _DockQ \(\uparrow\)_ \\ \hline
**ZDOCK** & 11.830\(\pm\)5.227 & 0.738\(\pm\)0.120 & 0.095\(\pm\)0.130 & 12.491\(\pm\)6.294 & 0.689\(\pm\)0.114 & 0.084\(\pm\)0.113 \\
**ClusPro** & 11.486\(\pm\)7.993 & 0.780\(\pm\)0.133 & 0.204\(\pm\)0.256 & 14.135\(\pm\)8.153 & 0.702\(\pm\)0.118 & 0.118\(\pm\)0.192 \\
**HDOCK** & **3.464\(\pm\)**7.394 & **0.935\(\pm\)**0.144 & **0.815\(\pm\)**0.364 & 11.328\(\pm\)8.975 & 0.742\(\pm\)0.167 & 0.314\(\pm\)0.390 \\
**BiDock** & 6.173\(\pm\)8.825 & 0.892\(\pm\)0.156 & 0.648\(\pm\)0.432 & **7.280\(\pm\)**8.117 & **0.847\(\pm\)**0.158 & **0.564\(\pm\)**0.369 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Impacts of noisy structures on the docking performance of classical software and BiDock. (bold: best; underline: runner-up)