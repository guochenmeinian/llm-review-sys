# Spectral Evolution and Invariance in Linear-width Neural Networks

 Zhichao Wang

University of California San Diego

zhw036@ucsd.edu &Andrew Engel

Pacific Northwest National Laboratory

andrew.engel@pnnl.gov &Anand Sarwate

Rutgers, The State University of New Jersey

ads221@soe.rutgers.edu &Ioana Dumitriu

University of California San Diego

idumitriu@ucsd.edu &Tony Chiang

Pacific Northwest National Laboratory

University of Washington

tony.chiang@pnnl.gov

###### Abstract

We investigate the spectral properties of linear-width feed-forward neural networks, where the sample size is asymptotically proportional to network width. Empirically, we show that the spectra of weight in this high dimensional regime are invariant when trained by gradient descent for small constant learning rates; we provide a theoretical justification for this observation and prove the invariance of the bulk spectra for both conjugate and neural tangent kernels. We demonstrate similar characteristics when training with stochastic gradient descent with small learning rates. When the learning rate is large, we exhibit the emergence of an outlier whose corresponding eigenvector is aligned with the training data structure. We also show that after adaptive gradient training, where a lower test error and feature learning emerge, both weight and kernel matrices exhibit heavy tail behavior. Simple examples are provided to explain when heavy tails can have better generalizations. We exhibit different spectral properties such as invariant bulk, spike, and heavy-tailed distribution from a two-layer neural network using different training strategies, and then correlate them to the feature learning. Analogous phenomena also appear when we train conventional neural networks with real-world data. We conclude that monitoring the evolution of the spectra during training is an essential step toward understanding the training dynamics and feature learning.

## 1 Introduction

Deep learning theory has made insightful connections between the behavior of neural networks (NNs) and kernel machines through asymptotic analyses of the so-called kernel regime [65; 83; 48; 40; 12; 4; 84]. When the neural network (NN) is _infinitely wide_, the behavior of NN coincides with a kernel machine, and the training process, as well as the generalization performance of this ultra-wide NN, can be fully described. The performance of _finite-width_ NNs, however, does not correspond to this theory, as NNs optimized with gradient-based methods perform better than infinitely wide networks in many circumstances [47; 38; 34; 51; 25; 32; 76; 42]. This gap heavily relies on the task complexity, data distribution, architecture of the NN and the training strategy [31]. We consider a more realisticsetting, a _linear-width regime_ (LWR), when the sample size \(n\), the input feature dimension \(d\), and the width \(h\) of the hidden layer approach infinity at comparable rates. Under the LWR, we aim to empirically study this theoretical gap in generalization and spectral properties by training various NNs with different optimization tools.

The ultra-wide NN (\(h\gg n\), fixed \(d\)) stays close to the kernel machine induced by initial NN, throughout the gradient-based training processes [88; 28; 27; 11]. There are two kernels commonly studied in theory: the conjugate kernel (CK) and the neural tangent kernel (NTK). CK (or the equivalent Gaussian process kernel) is the Gram matrix of the last hidden layer, which represents training only the last layer of the network [48; 57; 61]; by contrast, the NTK is the Gram matrix of the Jacobian of the NN for all trainable parameters, which governs the gradient flow of NN [40; 28; 4]. In most theoretical results, these kernels remain fixed throughout training, which leads to a kernel gradient descent with the initial kernel [40; 16], whereas in practice the spectra of the weight matrix, CK, and NTK of the NN change while learning the features from the training data [58; 59; 30; 19; 68]. In this paper, under the LWR, we experimentally and theoretically explore the following question:

_How do the spectra of weight and kernel matrices of the NN evolve during the training process?_

This question is crucial to extend our understanding beyond the kernel regime and will help us analyze the generalization of the NN in instances when it performs better than the kernel machine. For this case, the spectral properties of the trained NN could be entirely different from the initial kernel [55; 10; 78]. Also, various spectral properties of weight and kernel matrices can reveal different features learned by different training procedures [82]. Understanding the dynamics of the spectral properties may aid in finding better approaches to training and tuning hyper-parameters for NNs. From a theoretical perspective, random matrix theory (RMT) can be further exploited to study and elucidate the NN training under the proportional limit in high dimensions [45; 72; 57; 74; 37; 61].

Our main findings/contributions are as follows.

* We find a simple scenario that exhibits different spectral properties for both weight and kernel matrices through different training procedures. With the kernel regime as a benchmark, we compare how NN generalizes with different spectral evolutions of weight and kernel matrices in NN.
* The spectra of NNs trained with full batch gradient descent (GD) are globally _invariant_, indicating that the NN is still close to a kernel machine; we prove the global convergence of GD and the invariance of the limiting spectra for both weight and kernel matrices in this scenario.
* We observe a _phase transition_ of the alignment and the emergence of a spike outside the bulk of the spectrum when the learning rate _exceeds_ some threshold. The strong alignment of the spike with the teacher model when step sizes are large confirms that the NN is indeed learning germane features from data. This observation justifies the theoretical result of [6] in an ideal two-stage training process.
* The evolution towards heavy-tailed spectra is also discovered by using adaptive methods. Our experiments rule out a _causal_ relationship between the occurrence of a heavy-tailed spectrum for the weight matrices and a good generalization. This complements the work of [60; 63; 86] where the authors had observed a strong _correlation_ between the two; while at the same time, we provide simple examples of when heavy-tailed spectra exhibit feature learning and better generalizations.

For more details on how our results fit into existing literature, please see Appendix A.

## 2 Notation and Preliminaries

Throughout this paper, \(\|\cdot\|\) denotes the \(\ell_{2}\) norm for vectors, \(\ell_{2}\rightarrow\ell_{2}\) is the operator norm for matrices, while \(\|\cdot\|_{F}\) is the Frobenius norm, and \(\odot\) represents the Hadamard product between matrices. \(o_{d,\mathbb{P}}(\cdot)\) represents little-o in probability as \(d\rightarrow\infty\).

Neural Tangent Kernel Parameterization.Consider a \(L\)-layer fully connected feedforward NN at initialization without bias term: for \(1\leq\ell\leq L-1\),

\[\mathbf{h}_{0}=\frac{\mathbf{x}}{\sqrt{d}},\ \mathbf{h}^{(\ell)}=\frac{1}{\sqrt{n_{\ell}}} \sigma(\mathbf{W}_{\ell}\mathbf{h}^{(\ell-1)}), \tag{1}\]

and \(f_{\mathbf{\theta}}(\mathbf{x})=\mathbf{v}^{\top}\mathbf{h}^{(L-1)},\) where the input vector is \(\mathbf{x}\in\mathbb{R}^{d}\), \(\mathbf{W}_{\ell}\in\mathbb{R}^{n_{\ell}\times n_{\ell-1}}\) is the weight matrix for the \(\ell\)-th layer, and \(\mathbf{v}:=[v_{1},\ldots,v_{h}]^{\top}\in\mathbb{R}^{n_{L-1}}\) is the last-layer weight. Let \(n_{0}=d\). Denote all trainable parameters by \(\mathbf{\theta}:=[\text{vec}(\mathbf{W}_{1}),\ldots,\text{vec}(\mathbf{W}_{L-1}),\mathbf{v}]^ {\top}\in\mathbb{R}^{p}\) where each parameter's initial value is independently sampled from some distribution and \(p\) is the total number of parameters. Let the training dataset be \((\mathbf{X},\mathbf{y}):=([\mathbf{x}_{1},\ldots,\mathbf{x}_{n}],\mathbf{y})\in\mathbb{R}^{d\times n }\times R^{1\times n}\); the output of this NN with respect to this dataset is \(f_{\mathbf{\theta}}(\mathbf{X})=[f_{\mathbf{\theta}}(\mathbf{x}_{1}),\ldots,f_{\mathbf{\theta}}( \mathbf{x}_{n})]\). We call the above parameterization the _NTK parameterization_. The loss function for training is a mean squared error (MSE)

\[\mathcal{L}(\mathbf{\theta}):=\frac{1}{2n}\left\lVert\mathbf{y}-f_{\mathbf{\theta}}(\mathbf{X} )\right\rVert^{2}. \tag{2}\]

We focus on the NTK parameterization and consider the kernel machine (6) induced by the initial NTK of the NN. We aim to seek the cases when the NN outperforms this kernel during the training process. For this purpose, we adopt different optimizers of training this NN to obtain different testing performances and spectral properties of trained weights and empirical kernels.

Training Processes of NNs.NNs are usually trained by gradient-based methods such as full-batch gradient descent (GD), mini-batch stochastic gradient descent (SGD), Adaptive Gradients (AdaGrad), and Adam [43]. We can represent GD by

\[\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}-\eta\nabla_{\mathbf{\theta}}\mathcal{L}(\mathbf{ \theta}_{t}), \tag{3}\]

where \(\eta\) is the learning rate and \(\nabla_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta}_{t})\) is the gradient of the training loss w.r.t. trainable parameters \(\mathbf{\theta}\) at step \(t\geq 0\). We will prove the global convergence of GD in some special (overparameterized) cases ensuring the convergence to a NN that interpolates the data. We will also show the hyper-parameters (e.g. learning rate \(\eta\)) affect the spectral properties of NNs during training.

Conjugate Kernel and Neural Tangent Kernel.1When \(L=2,\) let \(n_{1}=h\) and \(n_{0}=d\) be the widths of the output and input layer. The CK is defined as

Footnote 1: In this work, we only consider _empirical_ conjugate and neural tangent kernels of finite-width NNs.

\[\mathbf{K}^{\text{CK}}:=\mathbf{X}_{1}^{T}\mathbf{X}_{1}\in\mathbb{R}^{n\times n}, \tag{4}\]

where \(\mathbf{X}_{1}:=\frac{1}{\sqrt{h}}\sigma\Big{(}\mathbf{W}\mathbf{X}/\sqrt{d}\Big{)}\). We can view the NN as a function of all training parameters \(\mathbf{\theta}\) and input data \(\mathbf{X}\). The neural tangent kernel (NTK) is related to the gradient of this neural network function with respect to \(\mathbf{\theta}\), which is the Gram matrix of the Jacobian of the neural network function with respect to \(\mathbf{\theta}\), \(\mathbf{K}^{\text{NTK}}:=(\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}}(\mathbf{X}))^{\top}( \nabla_{\mathbf{\theta}}f_{\mathbf{\theta}}(\mathbf{X}))\). Specifically, the empirical NTK of two-layer NN can be explicitly written2 as

Footnote 2: Here we train both layers, so we have two parts in the NTK expression. If we only train the first-hidden layer, we can simply remove the second CK part. In the following, we further introduce more empirical results for practical NNs in Section 5 and two-layer NNs with Gaussian dataset in Section 3. For general formula of the empirical NTK, see [39, 30].

\[\mathbf{K}^{\text{NTK}}=\frac{1}{d}\mathbf{X}^{\top}\mathbf{X}\odot\frac{1}{h}\sigma^{ \prime}\left(\frac{1}{\sqrt{d}}\mathbf{W}\mathbf{X}\right)^{\top}\text{diag}(\mathbf{v})^ {2}\sigma^{\prime}\left(\frac{1}{\sqrt{d}}\mathbf{W}\mathbf{X}\right)+\mathbf{K}^{\text{ CK}}. \tag{5}\]

In this paper, we are interested in comparing the spectral distributions for these three matrices (weight, CK, and NTK) at initialization and the end of training.

Lazy Training.Lazy training [21] can be viewed as a linear approximation of the NN, i.e. \(f_{\mathbf{\theta}}(\mathbf{x})\approx f_{\mathbf{\theta}_{0}}(\mathbf{x})+(\mathbf{\theta}-\mathbf{ \theta}_{0})^{\top}\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}_{0}}(\mathbf{x})\), defined by minimum-norm interpolation \(\hat{\mathbf{\theta}}:=\operatorname*{arg\,min}\left\{\left\lVert\mathbf{\theta}-\mathbf{ \theta}_{0}\right\rVert:(\mathbf{\theta}-\mathbf{\theta}_{0})^{\top}\nabla_{\mathbf{ \theta}}f_{\mathbf{\theta}_{0}}(\mathbf{X})=\mathbf{y}-f_{\mathbf{\theta}_{0}}(\mathbf{X})\right\}.\) Then, lazy training also represents a kernel machine

\[\hat{f}(\mathbf{x})=f_{\mathbf{\theta}_{0}}(\mathbf{x})+(\mathbf{y}-f_{\mathbf{\theta}_{0}}(\mathbf{X }))\,\mathbf{K}(\mathbf{X},\mathbf{X})^{-1}\mathbf{K}(\mathbf{X},\mathbf{x}) \tag{6}\]where \(f(\mathbf{x})\) is the unregularized regression prediction on test data \(\mathbf{x}\in\mathbb{R}^{d}\), the kernel \(\mathbf{K}(\mathbf{X},\mathbf{X})\) is the initial \(\mathbf{K}^{\text{NTK}}\) on training data, and \(\mathbf{K}(\mathbf{X},\mathbf{x})=(\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}_{0}}(\mathbf{X}))^{\top}( \nabla_{\mathbf{\theta}}f_{\mathbf{\theta}_{0}}(\mathbf{x}))\). The asymptotic performance of \(\hat{f}(\mathbf{x})\) has been analyzed by [1] under the LWR. We view this regime as a _benchmark_: [21; 11] prove that NN through gradient flow is close to lazy training if \(h\gg n\); [38] shows NN can go beyond lazy training under a non-proportional regime.

## 3 Case Study for Linear-width NNs

In this section, we investigate a two-layer NN with synthetic data. This setting is promising for future theoretical studies by virtue of RMT. We will showcase the evolution of its spectral properties over training. A two-layer NN in (1) is defined by

\[f_{\mathbf{\theta}}(\mathbf{x}):=\frac{1}{\sqrt{h}}\sum_{i=1}^{h}v_{i}\sigma(\mathbf{w}_{i }^{\top}\mathbf{x}/\sqrt{d}). \tag{7}\]

At initialization, we assume that the first hidden-layer \(\mathbf{W}=[\mathbf{w}_{1},\ldots,\mathbf{w}_{h}]^{\top}\in\mathbb{R}^{h\times d}\) is composed of independent standard normal random vectors.

**Assumption 3.1** (Linear-width regime (LWR)).: Assume that \(\frac{n}{d}\rightarrow\gamma_{1}\) and \(\frac{h}{d}\rightarrow\gamma_{2}\) as \(n\rightarrow\infty\) where the aspect ratios \(\gamma_{1},\gamma_{2}\in(0,\infty)\) are two fixed constants.

LWR stands as a pivotal setting grounded in high-dimensional statistics [1; 61]. It offers valuable insights especially when addressing real-world datasets. This is in contrast to the infinite-width regime, in which we are already in the asymptotic limit for width at first. Hence, LWR is a better approximation of real-world datasets and practical neural networks compared with the infinite-width regime.

**Assumption 3.2** (Activation function).: Suppose that the activation function \(\sigma(x)\) is nonlinear and \(\lambda_{\sigma}\)-Lipschitz with \(|\sigma^{\prime}(x)|,|\sigma^{\prime\prime}(x)|\leq\lambda_{\sigma}\) for all \(x\in\mathbb{R}\). Moreover, \(\mathbb{E}[\sigma(z)]=0\) for \(z\sim\mathcal{N}(0,1)\).

Though the LWR is somewhat impractical, it is still more aligned with deployed models than the infinite-width regime (\(h\gg n\), fixed \(d\)). As a kernel machine, the infinite-width NN has been studied extensively [40; 28; 27; 84]. This infinite-width limit is special, however, as NNs may generally evolve beyond the kernel regime and achieve superior performance [31; 55; 10; 78].

**Assumption 3.3** (Synthetic dataset and teacher model).: Training data is \(\mathbf{X}:=[\mathbf{x}_{1},\ldots,\mathbf{x}_{n}]\in\mathbb{R}^{d\times n}\), where \(\mathbf{x}_{i}\overset{i.i.d.}{\sim}\mathcal{N}(\mathbf{0},\mathbf{\mathrm{I}}_{d})\). The training labels \(\mathbf{y}=[y_{1},\ldots,y_{n}]\) are defined by \(y_{i}=f^{*}(\mathbf{x}_{i})+\varepsilon_{i},\quad\text{for }i\in[n]\), where \(f^{*}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is the teacher model, and \(\varepsilon_{i}\) is centered sub-Gaussian noise with variance \(\sigma_{\varepsilon}^{2}\).

One of the simplest nonlinear teacher models we can generate is the single-index model, namely \(f^{*}(\mathbf{x})=\sigma^{*}(\mathbf{x}^{\top}\mathbf{\beta})\) for a fixed vector \(\mathbf{\beta}\) with \(\|\mathbf{\beta}\|=1\) and nonlinear function \(\sigma^{*}\); the hidden feature is simply \(\mathbf{\beta}\in\mathbb{R}^{d}\). In general, we can consider a multiple-index model

\[f^{*}(\mathbf{x})=\frac{1}{k}\sum_{i=1}^{k}\sigma^{*}(\mathbf{x}^{\top}\mathbf{\beta}_{i}) \tag{8}\]

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & Optimization & Learning rate \(\eta\) & \(R^{2}\) score & Test error & Spectra \\ \hline Case 1 & GD & 5.0 & 0.63582 & 0.36381 & Invariant Bulk \\ Case 2 & SGD & 0.1 & 0.60605 & 0.36879 & Invariant Bulk \\ Case 3 & SGD & 22.0 & 0.76081 & 0.23791 & Bulk+spike \\ Case 4 & Adam & 0.092 & **0.78829** & **0.21071** & Heavy tail \\ \hline  & Lazy regime & & 0.68092 & 0.3185 & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Four models with the same architecture (\(n=2000\), \(h=1500\), \(d=1000\), and \(\sigma\) is normalized \(\tanh\)), but different choices of initial learning rates and optimizers listed in Table 1. The training label noise \(\sigma_{\varepsilon}=0.3\) and the teacher model is defined by (9) with \(\sigma^{*}\) a normalized _softplus_ and \(\tau=0.2\). We observe that simply choosing an optimizer and learning rate can affect the shapes of the final spectra and the performance of the NN, as measured by \(R^{2}\) scores and test errors.

where \(\mathbf{\beta}_{i}\) are some orthogonal unit vectors. We will specifically consider a mixture of single-index and quadratic models as our teacher model in this section:

\[f^{*}(\mathbf{x})=\sigma^{*}(\mathbf{x}^{\top}\mathbf{\beta})+\frac{\tau}{d}\|\mathbf{x}\|^{2}, \tag{9}\]

for some nonlinear target \(\sigma^{*}\), signal \(\mathbf{\beta}\) and constant \(\tau\)3. Following the above assumptions and constructions, we show different spectral properties (Figure 1) for this two-layer NN using different training procedures (Table 1). Figure 1 exhibits three types of spectra after training: unchanged bulk distribution, bulk with one spike, and heavy tail in spectra. Putting things together, we can see close relationships between the spectra and the generalization of the NN. These different spectral properties actually reveal disparate features learned via different training strategies.

Footnote 3: Here, the norm term of \(\mathbf{x}\) in (9) is designed to make the teacher model more complicated to be learned. All our empirical results still hold when \(\tau=0\).

The advantage of this toy model is that we can easily extract the spectral behaviors over training and then compare them with the kernel machine. We use lazy training defined in (6) as our _benchmark_ to assist us in determining whether a NN outperforms the associated kernel machine. Table 1 compares the test errors and \(R^{2}\) scores for different optimization cases and the lazy training. By tuning the hyper-parameters, we can find specific situations where NN outperforms the lazy training (see also Figure 10(c) in Appendix B.2).

From Figures 1(a), 12 and 13 in Appendix B.2, one can observe the spectral distributions of the weight, CK and NTK matrices remain invariant and static during training in Cases \(1\&2\), which indicates both cases still belong to the lazy regime. This spectral invariance impedes further feature learning during the training process. The emergence of the outlier in Figures 1(b) and 14 of Appendix B.2, however, shows the improvement over lazy training and potential feature learning via the training process, where the spectra possibly inherit the structures in teacher models (see Section 4.2). Comparing with Case 2, Case 3 of Table 1 suggests the importance of the large learning rate regime for training NNs [52, 64, 55, 15, 3]. As a remark, our spectral results of Case 3 are consistent with the observations in [79] through RMT hypothesis testing, where the majority of trained weight matrices remain random, and the learned feature may be contained in the largest singular value (outlier) and associated vector only. From Figures 1(c) and 16 in Appendix B.2, Case 4 further exhibits more spikes and heavy tails in the trained spectra, which thoroughly goes beyond the realm of initial kernel machine. Notably, this phenomenon is not unique to Adam since heavy tails also occur with AdaGrad in Figure 22 in Appendix B.6. Although all of these cases have the same identical initialization, different methods of optimization eventually lead to various training trajectories and evolutions of the spectra of the weight and kernel matrices. To acquire feature learning, Cases \(3\&4\) cause weights to deviate far from initialization. In the following Section 4, we prove the invariance of the bulk distributions and provide more refined analyses of spikes and heavy tails in terms of feature learning.

Figure 1: Different spectral behaviors in Table 1: (a) The initial and trained spectra of \(\mathbf{W}\) in Case 1. The spectrum is invariant based on the Q-Q subplot. (b) The initial and trained spectra of \(\mathbf{K}^{\text{CK}}\) in Case 3. There is an outlier (orange arrow) in the spectrum after training. (c) The initial and trained spectra of \(\mathbf{K}^{\text{CK}}\) in Case 4. We refer to Appendix B.2 for other spectra of weight, CK, and NTK matrices in Case 1-4, where analogous phenomena hold for other matrices.

Different Spectral Behaviors in NNs

We now further explore the spectral behaviors in different cases of Table 1 by clarifying how the spectra evolve through different training processes and how this evolution may affect the NN. Following Figure 1, we study the training processes case-by-case: invariant bulk, spikes outside the bulk, and heavy-tailed distribution. Additional experiments are exhibited in Appendix B.

### Invariant Bulk Distributions

In Figure 1(a) (also Figures 12 and 13 in Appendix B.2), we observe the bulk distributions of weight and kernel matrices in Cases \(1\&2\) remain globally unchanged (invariant) over the training process. under the LWR, this is also empirically verified by Figures 10(a)\(\&\)(b) in Appendix B.2. In this section, by investigating the global convergence of GD, we prove this invariant-bulk phenomenon under certain assumptions.

For simplicity, we focus on analyzing the training process of the first-hidden layer with the second layer \(\mathbf{v}\) fixed. Denote \(f_{\mathbf{\theta}}(\mathbf{X})\) by \(f_{\mathbf{W}}(\mathbf{X})\) in this case. At any time \(t\in\mathbb{N}\), consider the gradient steps:

\[\mathbf{W}_{t+1}=\mathbf{W}_{t}-\eta\nabla_{\mathbf{W}}\mathcal{L}(\mathbf{W}_{t}). \tag{10}\]

Denote the CK and NTK at gradient step \(t\in\mathbb{N}\) by \(\mathbf{K}_{t}^{\text{CK}}:=\frac{1}{h}\sigma(\mathbf{W}_{t}\mathbf{X})^{\top}\sigma(\bm {W}_{t}\mathbf{X}),\) and \(\mathbf{K}_{t}^{\text{NTK}}:=\frac{1}{d}\mathbf{X}^{\top}\mathbf{X}\odot\frac{1}{h}\sigma ^{\prime}\left(\frac{1}{\sqrt{d}}\mathbf{W}_{t}\mathbf{X}\right)^{\top}\text{diag}( \mathbf{v}_{t})^{2}\sigma^{\prime}\left(\frac{1}{\sqrt{d}}\mathbf{W}_{t}\mathbf{X}\right)\) respectively. First, we present an elaborate description of the changes in the weight, CK, and NTK at the _early phase_ of the training (after any finite \(t\) steps) as follows.

**Lemma 4.1** (Early phase).: _Under Assumptions 3.1, 3.2 and 3.3, we further assume that \(\left\lVert\mathbf{v}\right\rVert_{\infty}\leq 1\) and \(f^{*}\) is a \(\lambda_{\sigma}\)-Lipschitz function. Given any fixed \(t\in\mathbb{N}\) and learning rate \(\eta=\Theta(1)\), after \(t\) gradient steps, the changes \(\frac{1}{\sqrt{d}}\left\lVert\mathbf{W}_{t}-\mathbf{W}_{0}\right\rVert_{F}\), \(\left\lVert\mathbf{K}_{t}^{\text{CK}}-\mathbf{K}_{0}^{\text{CK}}\right\rVert_{F}\), and \(\left\lVert\mathbf{K}_{t}^{\text{NTK}}-\mathbf{K}_{0}^{\text{NTK}}\right\rVert\) are all less than \(\frac{C}{n}\), with probability at least \(1-4n\exp\left(-cn\right)\), for some positive constants \(c,C>0\) which only depend on step \(t\) and parameters \(\eta,\gamma_{1},\gamma_{2},\lambda_{\sigma},\sigma_{\varepsilon}\)._

Lemma 4.1 shows \(\frac{1}{\sqrt{d}}\left\lVert\mathbf{W}_{t}-\mathbf{W}_{0}\right\rVert\), \(\left\lVert\mathbf{K}_{t}^{\text{CK}}-\mathbf{K}_{0}^{\text{CK}}\right\rVert\), and \(\left\lVert\mathbf{K}_{t}^{\text{NTK}}-\mathbf{K}_{0}^{\text{NTK}}\right\rVert\) are asymptotically vanishing for any fixed time \(t\). Therefore, all the eigenvalues/eigenvectors are asymptotically unchanged at the early phase of the training (see Corollary C.3 in Appendix C). Now we aim to analyze the spectra at the end of the training process (10). In this case, although we are unable to show the invariance for each eigenvalue, we can verify the invariance of the limiting bulk distributions for \(\mathbf{K}_{t}^{\text{CK}}\) and \(\mathbf{K}_{t}^{\text{CK}}\) for all \(t\).

By [81, Theorem 2.9], the smallest eigenvalue of \(\mathbf{K}_{0}^{\text{NTK}}\) has an asymptotic lower bound:

\[\lambda_{\min}(\mathbf{K}_{0}^{\text{NTK}})\geq\left(a_{\sigma}-\sum_{k=0}^{2} \eta_{k}^{2}\right)(1-o_{d,\mathbb{P}}(1)), \tag{11}\]

where \(a_{\sigma}:=\mathbb{E}[\sigma^{\prime}(\xi)^{2}]\) and \(\eta_{k}\) is the \(k\)-th Hermite coefficient of \(\sigma^{\prime}\). Hence, we can claim there exists some constant \(\alpha>0\) only dependent on \(\sigma\) such that \(\lambda_{\min}(\mathbf{K}_{0}^{\text{NTK}})\geq 4\alpha^{2}\) with high probability. Note that \(\alpha\) is not vanishing since \(\sigma\) is nonlinear. With this lower bound, we obtain the following global convergence for (10) and norm control of \(\mathbf{W}_{t}\) as \(n/d\rightarrow\gamma_{1}\) and \(h/d\rightarrow\gamma_{2}\).

**Theorem 4.2** (Global convergence).: _Under the same assumptions of Lemma 4.1, we further assume \(v_{i}\)'s are independent and centered random variables in the second layer. For any \(\eta<\min\{\frac{\alpha^{2}n}{2},\frac{n}{4\lambda_{2}^{2}(1+\sqrt{\gamma_{1} })^{2}}\}\) and all \(t\in\mathbb{N}\), there exists some \(\gamma^{*}>0\) such that, when \(\gamma_{2}\geq\gamma^{*}\), the gradient steps (10) will satisfy_

\[\ell(\mathbf{W}_{t})\leq\left(1-\frac{\eta\alpha^{2}}{2n}\right)^{t} \ell(\mathbf{W}_{0}), \tag{12}\] \[\frac{1}{4}\alpha\left\lVert\mathbf{W}_{0}-\mathbf{W}_{t}\right\rVert_{F} +\ell(\mathbf{W}_{t})\leq\ell(\mathbf{W}_{0}),\] (13) \[\sum_{t=0}^{\infty}\left\lVert\mathbf{W}_{t+1}-\mathbf{W}_{t}\right\rVert _{F}\leq\frac{4\ell(\mathbf{W}_{0})}{\alpha}, \tag{14}\]

[MISSING_PAGE_FAIL:7]

Transitions of the Spike as a Function of Learning Rate.From Case 2 to Case 3, we observe the emergence of outliers in the trained spectra when increasing the learning rate \(\eta\). This indicates a transition of the emergence of the spike outside the bulk distribution. Figure 2, analogously to the well-known BBP transition by Baik, Ben Arous, and Peche in [9] from the RMT community, shows there is a threshold (yellow region) for learning rate: the outliers only appear when \(\eta\) exceeds this threshold. We fix the same NN and dataset for all trials of training. The flat black lines in Figures 2(b) and (c) are the right edges of the limiting spectra at initialization. Figure 2(d) records the angles between \(\mathbf{\beta}\) and the leading eigenvector of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}/d\), and \(\mathbf{y}\) and the leading eigenvectors of \(\mathbf{K}_{t}^{\text{CK}}\) and \(\mathbf{K}_{t}^{\text{NTK}}\) after training for different \(\eta\). Similarly with [10], when \(\eta\) is sufficiently large (orange region), we obtain significant alignments which suggest potential feature learning. These transitions of leading eigenvalue and eigenvector alignment have been proved for \(\mathbf{W}_{t}\) by [6] for a different scenario4.

Footnote 4: We apply NTK parameterization for our neural networks and train both layers until convergence, while [6] considers the mean-field initialization and early stage of training dynamics of GD for the first layer.

Spikes of Kernel Matrices.The alignment of the kernel matrix with the training labels \(\mathbf{y}\) is defined by [22] by Kernel Target Alignment (KTA) as follows: when kernel \(\mathbf{K}\) is either CK or NTK,

\[\mathrm{KTA}=\frac{\langle\mathbf{K},\mathbf{y}^{\top}\mathbf{y}\rangle}{\|\mathbf{K}\|_{F}\| \mathbf{y}\|^{2}}. \tag{15}\]

Analogously to [10, 5, 78], Figure 3(a) depicts the evolution of KTA of CK in several cases. Based on Figure 2(d), when the spike appears outside the bulk (Case 3), its corresponding (leading) eigenvector \(\mathbf{v}_{1}\) of kernel matrix naturally dominates the alignment with \(\mathbf{y}\) (Figure 15 in Appendix B.2), which is regarded as a kernel rotation during training in [68]. Notice that this is not the common situation in Cases 1\(\&\)2 of Table 1 (and cf. Figure 11 in Appendix B.2). On the other hand, KTA measures the alignment between \(\mathbf{y}\) and the full eigenbasis of the kernel. These kernel alignments improve the speed of the convergence of training dynamics but may hurt or boost the generalization of the NNs [68, 78, 10]. Figure 3(a) indicates that Case 4 with heavy-tailed spectra after training has a larger KTA than the other cases. In this case, the emergence of a heavy tail in the spectrum is closely related to a better generalization of the NN and more significant feature learning.

### Phenomenon of Heavy-tailed Spectra

Next, we analyze the heavy-tailed spectra of weight and kernel matrices in Figure 1(c). [58, 59] found a strong correlation between the heavy-tailed spectra of trained state-of-the-art models with better generalization (Figure 7 in Appendix B.1). Heavy-tailed spectra can be viewed as an extreme of "bulk+spikes", where a fraction of the eigenvalues move out of the initial bulk. In RMT, heavy-tailed spectra generally appear when the entries of the matrix are highly correlated [59]. This could heuristically explain heavy-tailed phenomena in the spectra since the entries of well-trained \(\mathbf{W}_{t}\) should be strongly correlated. Unlike [58, 59], we focus on the heavy-tailed phenomena for both weight and kernel matrices in a simpler model (7) and provide a connection between feature learning and heavy-tailed spectra, which opens an important avenue for further theoretical analysis.

Figure 3: (a) Evolution of KTA of CK defined by (15) with respect to training labels for Cases 1, 3\(\&\)4 in Table 1. We normalize the epoch scales (\(x\)-axis) for better observations. Heavy-tailed phenomena: (b) Evolutions of PC angles \(\theta_{i}\) between feature subspace \(U\) of (8) and top 100 eigenspace of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\) during training with Adam (solid line), SGD (dashed line) and GD (dash-dot). For the first PC \(\theta_{1}\), see Figure 17 in Appendix B.4. (c) The CK spectra at two initializations for \(\mathbf{W}\): standard Gaussian and Cauchy distributions. (d) Weight spectra at initial and after SGD training. After training the weight reveals a heavy tail, but generalizes not as well as former examples (test loss \(1.47504\); \(R^{2}\) score \(-0.48\)).

Heavy Tails and Generalization.We emphasize that heavy tails are not sufficient for good generalization, in general, [60; 63]. Figures 3(c)&(d) exhibit NNs with heavy-tailed weights but in the absence of good performance at initialization. In fact, it is the alignments between the features learned from the heavy-tailed part and the features in the teacher model that finally determine the generalization error of NNs.

More precisely, we provide an example of when heavy tails indicate better generalizations. Consider the multiple-index teacher model (8) with \(k=5\) feature directions \(\mathbf{\beta}_{i}\), and train NNs (7) with GD, SGD, and Adam to get invariant bulk, bulk with one spike and heavy tails, respectively, after training. In Figure 3(b), we present the evolutions of the _principle angles_\(\theta_{i}\) between feature subspace \(U=\text{span}\{\mathbf{\beta}_{i}\}_{i=1}^{k}\) and top 100 eigenspace of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\) during different training processes. This eigenspace with respect to the top 100 eigenvalues of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\) corresponds to the heavy-tail part of the spectrum in \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\) when training NNs with Adam (solid lines in Figure 3(b)). Comparing with GD and SGD training processes, we observe strong alignments between feature space \(U\) and eigenspace w.r.t heavy tails in Adam case in Figure 3(b), which explains why Adam case (NNs with heavy-tailed spectra) generalizes better than the other two cases. For more examples, see Figures 17 and 20 in Appendix B.4. This concludes that NNs with heavy-tailed spectra can generalize better only when the teacher features from data are aligned with the heavy-tailed part of spectra. If the feature dimension in the teacher model is high (i.e. the teacher model is more complicated and intrinsically high-dimensional), then we expect to get a heavy-tailed weight spectrum of well-trained NN where the heavy-tailed part learns all the features in the teacher modes. This example explains why we can use the heavy tails to discriminate well-trained and poorly-trained large models [60; 63; 86].

## 5 Discussions and Future Directions

We empirically investigated how the spectra of \(\mathbf{W}\), \(\mathbf{K}^{\text{CK}}\), and \(\mathbf{K}^{\text{NTK}}\) evolve under the LWR for an idealized student-teacher setting. Our work implies that understanding the relationship between feature learning and training processes requires understanding the evolution of the spectra of both weight and kernel matrices. In particular, we show that different training processes affect the eigenstructure of weight and kernel matrices. Since evolution is sensitive to feature learning, we can link feature learning and different training dynamics by studying the spectra of these matrices.

While synthetic data is easier to analyze theoretically, we also investigate these spectral properties on real-world data and more complicated tasks in the following. In practice, people mainly focus on analyzing spectra of the weight matrices in fully connected layers; we choose to also focus on the spectral properties of general kernel matrices induced by the NNs, which contain abundant information [19; 55; 5; 78].

First, we show the spectra of \(\mathbf{K}^{\text{NTK}}\) before and after training for binary classification on CIFAR-2 through small CNNs in Figure 4. Similarly with Case 1, Figure 4(a) (especially in the Q-Q subplot) manifests the invariant spectral distribution of NTK through GD training while SGD exhibits a heavier tail in NTK spectrum in Figure 4(b). This phenomenon is more evident when trained by Adam in Figure 4(c) with improved accuracy. Figure 4 suggests that our observations on synthetic data in Section 3 can be extended to real-world data and on more practical architectures. We note that there is a lack of the emergence of spikes after training because spikes already exist in the initial NTK spectrum for this complicated neural architecture on real-world datasets. Figure 4(a) also indicates

Figure 4: Different NTK spectra for a small-CNN model on CIFAR-2. The subplots are Q-Q plots for the comparison between initial and trained spectra. Test accuracies: (a) 79%, (b) 84%, (c) 86.4%.

that the spectral invariance of NTK through training will impede the feature learning and the NN does not generalize well in this training process.

We also investigate the spectral properties on the pre-trained model, BERT from [26], with fine-tuning on Sentiment140 dataset of tweets5 from [36]. We fine-tune the BERT model for a binary classifier on Sentiment140 and capture the evolution of CK spectra, rather than the NTK due to the size of BERT, in Figure 5 (see also Figure 7 in Appendix B.1).

Footnote 5: [https://www.kaggle.com/datasets/kazanova/sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)

A heavy-tailed CK spectrum with several spikes already exists in this pre-trained model. Unlike Figure 4 (and cases in Table 1) where the first spike of NTK becomes larger than at random initialization after training, in Figure 5(a), the leading eigenvalue first decreases and then increases. Moreover, similarly to Figure 2(d), our Figure 5(b) shows that the alignment of the first eigenvector of the CK and training labels becomes more apparent through fine-tuning with the leading eigenvalue decrease. Heuristically, this process seems to unlearn the features in the pre-trained model and, remarkably, learn new features on the new dataset in only a few epochs of fine-tuning (see Figure 7in Appendix B.1). We believe that the evolutions of the kernel matrices and some spectral metrics are crucial for understanding feature learning through fine-tuning [82]. A more comprehensive exploration of the evolutionary spectral properties of "foundation models" may help shed further light on these phenomena.

Limitations.Although LWR has garnered significant attention in recent years, e.g., [77, 50, 17, 87, 23], we recognize the limitations inherent in LWR. Our LWR is more realistic compared with infinite-width neural networks and is one of the ways to approximate finite but very large neural networks with very large datasets, but there are more sophisticated regimes for NNs. We leave this for future theoretical work. The NTK parameterization is another limitation of this work. We expect to apply our spectral analysis for other parameterizations of NNs with more real-world datasets. See the discussion at the beginning of Appendix B.

## Acknowledgement

Z.W., A.E., I.D., and T.C. were partially supported by the Mathematics for Artificial Reasoning in Science (MARS) initiative via the Laboratory Directed Research and Development (LDRD) Program at Pacific Northwest National Laboratory (PNNL). A.S. and T.C. were also partially supported by the Statistical Inference Generates kNowledge for Artificial Learners (SIGNAL) program at PNNL. PNNL is a multi-program national laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05-76RL0-1830. Z.W. would like to thank Denny Wu and Libin Zhu for their valuable suggestions and comments.

Figure 5: We use SGD for fine-tuning the BERT model. The training accuracy is 95.90\(\%\) and the test accuracy is 84\(\%\). (a) The evolution of first and second eigenvalues of empirical CK during fine-tuning. (b) The alignments of training labels with first and second eigenvectors of CK during fine-tuning. See Figure 7 for the spectra of CK at different epochs.

## References

* [1] Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In _International Conference on Machine Learning_, pages 74-84. PMLR, 2020.
* [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.
* [3] Maksym Andriushchenko, Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step sizes learns sparse features. In _International Conference on Machine Learning_, pages 903-925. PMLR, 2023.
* [4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019.
* [5] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. In _International Conference on Learning Representations_, 2022.
* [6] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems_, 35:37932-37946, 2022.
* [7] Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In _International Conference on Learning Representations_, 2020.
* [8] Zhidong Bai and Jack W Silverstein. _Spectral analysis of large dimensional random matrices_, volume 20. Springer, 2010.
* [9] Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. _The Annals of Probability_, 33(5):1643-1697, 2005.
* [10] Aristide Baratin, Thomas George, Cesar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In _International Conference on Artificial Intelligence and Statistics_, pages 2269-2277. PMLR, 2021.
* [11] Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. _Acta numerica_, 30:87-201, 2021.
* [12] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In _International Conference on Machine Learning_, pages 541-549. PMLR, 2018.
* [13] Lucas Benigni and Sandrine Peche. Eigenvalue distribution of some nonlinear models of random matrices. _Electronic Journal of Probability_, 26:1-37, 2021.
* [14] Lucas Benigni and Sandrine Peche. Largest eigenvalues of the conjugate kernel of single-layered neural networks. _arXiv preprint arXiv:2201.04753_, 2022.
* [15] Gaspard Beugnot, Julien Mairal, and Alessandro Rudi. On the benefits of large learning rates for kernel methods. In _Conference on Learning Theory_, pages 254-282. PMLR, 2022.
* [16] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In _Advances in Neural Information Processing Systems_, pages 12873-12884, 2019.
* [17] David Bosch, Ashkan Panahi, and Babak Hassibi. Precise asymptotic analysis of deep random feature models. _arXiv preprint arXiv:2302.06210_, 2023.
* [18] Sourav Chatterjee. Convergence of gradient descent for deep neural networks. _arXiv preprint arXiv:2203.16462_, 2022.

* [19] Shuxiao Chen, Hangfeng He, and Weijie Su. Label-aware neural tangent kernel: Toward better generalization and local elasticity. _Advances in Neural Information Processing Systems_, 33, 2020.
* [20] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In _Advances in neural information processing systems_, pages 3036-3046, 2018.
* [21] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems_, 32, 2019.
* [22] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola. On kernel-target alignment. _Advances in neural information processing systems_, 14, 2001.
* [23] Hugo Cui, Florent Krzakala, and Lenka Zdeborova. Bayes-optimal learning of deep random networks of extensive-width. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 6468-6521. PMLR, 23-29 Jul 2023.
* [24] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [25] Amit Daniely and Eran Malach. Learning parities with neural networks. _Advances in Neural Information Processing Systems_, 33:20356-20365, 2020.
* [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [27] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [28] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2018.
* [29] Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In _International Conference on Learning Representations_, 2020.
* [30] Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. _Advances in neural information processing systems_, 33, 2020.
* [31] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. _Advances in Neural Information Processing Systems_, 33:5850-5861, 2020.
* [32] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(11):113301, 2020.
* [33] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. In _International Conference on Machine Learning_, pages 3452-3462. PMLR, 2020.
* [34] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? _Advances in Neural Information Processing Systems_, 33:14820-14830, 2020.
* [35] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 49(2):1029-1054, 2021.

* [36] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. _CS224N project report, Stanford_, 1(12):2009, 2009.
* [37] Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep neural networks. _Communications in Mathematical Physics_, 376(1):287-322, 2020.
* [38] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. _Advances in Neural Information Processing Systems_, 33:17116-17128, 2020.
* [39] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. In _International Conference on Machine Learning_, pages 4542-4551. PMLR, 2020.
* [40] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [41] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural networks. In _International Conference on Learning Representations_, 2020.
* [42] Stefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. _Advances in Neural Information Processing Systems_, 34, 2021.
* [43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [44] Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study. In _International Conference on Artificial Neural Networks_, pages 168-179. Springer, 2020.
* [45] Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to neural-network learning. _Physical Review Letters_, 66(18):2396, 1991.
* [46] Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training. _arXiv preprint arXiv:2002.10376_, 2020.
* [47] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. _Advances in Neural Information Processing Systems_, 33:15156-15172, 2020.
* [48] Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In _International Conference on Learning Representations_, 2018.
* [49] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* [50] Qianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization. _Physical Review X_, 11(3):031059, 2021.
* [51] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In _Conference on learning theory_, pages 2613-2682. PMLR, 2020.
* [52] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. In _Advances in Neural Information Processing Systems_, pages 11674-11685, 2019.
* [53] Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent. _Advances in Neural Information Processing Systems_, 33:13939-13950, 2020.

* [54] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 2022.
* [55] Philip M Long. Properties of the after kernel. _arXiv preprint arXiv:2105.10585_, 2021.
* [56] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. Evolution of neural tangent kernels under benign and adversarial training. _Advances in Neural Information Processing Systems_, 35:11642-11657, 2022.
* [57] Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks. _The Annals of Applied Probability_, 28(2):1190-1248, 2018.
* [58] Michael Mahoney and Charles Martin. Traditional and heavy tailed self regularization in neural network models. In _International Conference on Machine Learning_, pages 4284-4293. PMLR, 2019.
* [59] Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. _Journal of Machine Learning Research_, 22(165):1-73, 2021.
* [60] Charles H Martin, Tongsu Serena Peng, and Michael W Mahoney. Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data. _Nature Communications_, 12(1):1-13, 2021.
* [61] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766, 2022.
* [62] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* [63] Xuran Meng and Jianfeng Yao. Impact of classification difficulty on the weight matrices spectra in deep learning and application to early-stopping. _Journal of Machine Learning Research_, 24(28):1-40, 2023.
* [64] Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex problems. _OPT2020 Workshop_, 2020.
* [65] Radford M Neal. _Bayesian learning for neural networks_, volume 118. Springer Science & Business Media, 1995.
* [66] Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear widths. In _International Conference on Machine Learning_, pages 8056-8062. PMLR, 2021.
* [67] Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. Neural anisotropy directions. _Advances in Neural Information Processing Systems_, 33:17896-17906, 2020.
* [68] Guillermo Ortiz-Jimenez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neural networks actually say about generalization? _Advances in Neural Information Processing Systems_, 34, 2021.
* [69] Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian. _arXiv preprint arXiv:1906.05392_, 2019.
* [70] Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the shortest path? In _International Conference on Machine Learning_, pages 4951-4960. PMLR, 2019.

* Oymak and Soltanolkotabi [2020] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* Pennington and Bahri [2017] Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix theory. In _International Conference on Machine Learning_, pages 2798-2806. PMLR, 2017.
* Pennington and Worah [2017] Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. _Advances in neural information processing systems_, 30, 2017.
* Pennington and Worah [2018] Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-hidden-layer neural network. _Advances in neural information processing systems_, 31, 2018.
* Polaczyk and Cyranka [2022] Bartlomiej Polaczyk and Jacek Cyranka. Improved overparametrization bounds for global convergence of sgd for shallow neural networks. _Transactions on Machine Learning Research_, 2022.
* Refinetti et al. [2021] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova. Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In _International Conference on Machine Learning_, pages 8936-8947. PMLR, 2021.
* Schroder et al. [2023] Dominik Schroder, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent and error universality of deep random features learning. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 30285-30320. PMLR, 23-29 Jul 2023.
* Shan and Bordelon [2021] Haozhe Shan and Blake Bordelon. A theory of neural tangent kernel alignment and its influence on training. _arXiv preprint arXiv:2105.14301_, 2021.
* Thamm et al. [2022] Matthias Thamm, Max Staats, and Bernd Rosenow. Random matrix analysis of deep neural network weight matrices. _Physical Review E_, 106(5):054124, 2022.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wang and Zhu [2021] Zhichao Wang and Yizhe Zhu. Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks. _arXiv preprint arXiv:2109.09304_, 2021.
* Wei et al. [2022] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 23549-23588. PMLR, 17-23 Jul 2022.
* Williams [1996] Christopher Williams. Computing with infinite networks. _Advances in neural information processing systems_, 9, 1996.
* Yang [2019] Greg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. _Advances in Neural Information Processing Systems_, 32, 2019.
* Yang and Hu [2021] Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _International Conference on Machine Learning_, pages 11727-11737. PMLR, 2021.
* Yang et al. [2022] Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E Gonzalez, Kannan Ramchandran, Charles H Martin, and Michael W Mahoney. Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. _arXiv preprint arXiv:2202.02842_, 2022.
* Zavatone-Veth et al. [2022] Jacob A Zavatone-Veth, William L Tong, and Cengiz Pehlevan. Contrasting random and learned features in deep bayesian linear regression. _Physical Review E_, 105(6):064118, 2022.
* Zou et al. [2020] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes overparameterized deep relu networks. _Machine learning_, 109:467-492, 2020.

Additional Related Work

Nonlinear Random Matrix Theory and Random Feature Regression.The limiting spectrum of CK with random input dataset has been investigated by [73, 13]; whereas [57, 30] studied the spectrum of CK with more general input data. This spectrum is actually a deformed Marcenko-Pastur distribution [30], which becomes a deformed semicircular distribution [81] when \(h\gg n\). The largest eigenvalue of the CK matrix has been studied in [14], and the spectrum of the NTK was analyzed in [1, 30]. As an application, random feature ridge regression was fully determined by the limiting spectra of CK or NTK: [57, 33, 53, 61, 1]. All of these results belong to LWR.

Global Convergence of GD for Ultra Wide NNs.A recent line of work has shown the global convergences of the learning dynamics of gradient-based methods in a certain overparameterized regime, e.g. [28, 27, 69, 70, 66, 54, 75, 18]. We refer to Table 1 of [75] as a summary of these recent results. Most of the theorems in the literature require \(h\gg n\), which implies that the NTK is almost static during training, while [70, 66] can consider LWR under some specific assumptions. Recently, [18] established a new criterion for the convergence of GD which results in the global convergence of general NNs with finite width \(h\) and \(d\geq n\).

Beyond NTK Regime.Under the proportional limit, the initial kernel regression can only learn a linear component of the target [35]. Thus, it is reasonable to consider the cases beyond the NTK regime. To this end, [29, 39] considered the dynamics of NTK throughout training while [2, 7] have shown a second-order approximation of NTK, outperforming the initial kernel. In addition, there are many theoretical works analyzing when a NN outperforms the initial kernels in some specific settings: [51] proved a two-layer ReLU NN that is shown to beat any kernel method; [42] verified a two-layer CNN with some simple dataset can outperform the initial NTK for image classifications; [6] showed a NN can escape the kernel regime by only taking one specific large gradient step; [24] showed a specific gradient-based training can even learn polynomials with low-dimensional latent representation.

Evolution of NTK and Alignment in NNs.The feature learning can be characterized by the evolution of the kernel during training [31, 68, 55, 5, 56]. Specifically, [55] studied the hard-margin SVM for "after kernels" which are the CK and NTK matrices of trained NNs. One of the effective ways of depicting how the kernels evolve during training is to capture the evolution of kernel alignment [10, 78, 5, 56]. Kernel alignments between kernels and training labels essentially reveal how the NN accelerates training [78]. Also, several papers showed that the top eigenfunctions of the kernel align with the target function learned by the NN [44, 67, 68]. This becomes an efficient way of analyzing how NNs learn features through a particular gradient-based optimization.

Large Learning Rate Regime.As mentioned earlier, the large learning rate may contribute to feature learning. The benefits of large-learning-rate training have been studied from different aspects [52, 64, 15, 3]. Specifically, [46] observed that training dynamics with large learning rates differ from the small learning rate regime, where the latter regime exhibits monotone and fast convergence of training loss but may not generalize well on test data. At the early phase of training, [41] showed using lower learning rates may result in finding a region of the loss surface with worse conditioning of kernel and Hessian matrices. In [55], the after kernels of NNs trained with larger learning rates generalize better and stay more stable. [49] raised a "catapult mechanism", where gradient descent dynamics converge to flatter minima for extremely large learning rates. There is a transition as a function of the learning rate, from lazy training to the catapult regime. Section 4.2 illustrates a similar transition in our situations.

Heavy-tailed Phenomenon.The heavy-tailed phenomenon has appeared in many places in deep learning theory; [58, 59] observed that many state-of-the-art pre-trained models obtain heavy-tailed weight spectra. More precisely, these spectra have a "5+1" phase transition which relates to different degrees of regularization of the NN. With this heavy-tailed self-regularization theory, [60] further showed how to distinguish well-trained and poorly trained models by a power-law-based approximation. [63] classified trained weight spectra into three types: Marcenko-Pastur law, bulk with (few) outliers, and heavy-tailed spectra. We extend this classification to both weight and kernel matrices in Figure 1. Additionally, similarly to the discussion in 4.3, [63] showed that the difficulty of the classification problem is related to the emergence of heavy-tailed spectra in weight matrices. Thisheavy-tailed phenomenon can be used to construct metrics for evaluating the generalization of NNs [60, 86], and early stopping of NNs to avoid over-fitting [63].

## Appendix B Additional Empirical Results

There are different parameterizations for NNs at initialization. The orders of the output of NN are distinct in different cases [21, 25, 85]. This affects the size of stable and non-trivial gradient steps. The distance of trainable parameters from initialization determines whether the NN learns any features from the training data [6, Figure 2]. The performance of networks with different initializations indicates whether the NN belongs to the kernel regime or not [85]. Unlike the NTK parameterization, the _mean-field_ parameterization [62, 20] and _maximal update_ parameterization [85] tend to be feature learning.

For all NNs in our experiments, we apply a normalized and centered nonlinear activation function such that Assumption 3.2 holds (\(\mathbb{E}[\sigma(z)]=0\) for \(z\sim\mathcal{N}(0,1)\)) because we can exclude a large but trivial spike in the initial spectra of kernel matrices. In all architectures of NNs we considered, we remove the bias term of each layer and apply the NTK parameterization (1) with standard Gaussian initialization. Specifically, all entries of \(\mathbf{W}\) and \(\mathbf{v}\) in (7) at initialization are i.i.d. standard Gaussian random variables. For all experiments on synthetic datasets, we use standard Gaussian random matrices to generate the training data \(\mathbf{X}\). In addition, we consider the training label noise defined in Assumption 3.3 as \(\mathbf{\varepsilon}\sim\mathcal{N}(0,\sigma_{\varepsilon}^{2}\mathbf{I}_{n})\).

### Further Discussions on Real-world Data Experiments

For the first experiment in Section 5, we fix the small-CNN architecture, which is similar to the VGG model, and CIFAR-2 dataset, and vary the methods of optimization of training. Corresponding to Figure 4, the training and test accuracy histories for three cases are shown in Figure 6. Here, in Figure 6(a), we used GD with learning rate \(8\times 10^{-3}\); we used SGD with learning rate \(10^{-3}\), batch size 32 and momentum \(0.2\) in Figure 6(b); Figure 6(c) employs the same learning rate and batch size as 6(b) but employs Adam optimization.

In the experiment of the transformer language model in Section 5, we fine-tuned the BERT model with SGD for a binary classification on the Sentiment140 dataset. We apply this transformer and fine-tuning to sentiment analysis for social media data. For fine-tuning, the learning rate is 0.003, the batch size is 64 and the momentum is 0.8. The purpose of this experiment is to extract the spectral properties of pre-trained models and the evolution of the CK spectra over fine-tuning. Combining Figure 5, the following Figure 7 exhibits the evolution of the CK spectrum during fine-tuning. Similarly with Case 4 in Table 1, the CK spectrum of this pre-trained model (red histogram in Figure 7) possesses a heavy-tailed distribution, which suggests this transformer has received adequate training. From Figure 7(a) to 7(c), we observe the bulk distribution first shrinks then extends during fine-tuning. This is similar to the evolution of the first eigenvalue of CK in Figure 5(a). Accompanied by this spectra evolution, there is a rapid transformation of the features through fine-tuning, linking the features in the pre-trained model with features in the new dataset. We expect that further spectral analysis will elucidate the feature learning in this kind of transformer [82].

Figure 6: Training/test accuracy v.s. epochs for small-CNN model on CIFAR-2 with different optimizers.

### Additional Results for Cases in Table 1

To complement the findings in Figure 1 and Section 3, we now present additional results on synthetic data and two-layer NNs. In this section, we will always use the same architecture and dataset as the typical examples in Table 1.

Norms of the Change.Based on Figure 1, the trajectories of the weight and kernel matrices are quite different among all cases in Table 1. Hence, for all cases in Table 1, we record the changes in the weight and NTK matrices in both Frobenius norm and operator norm in Figure 8:

\[\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{0}-\mathbf{W}_{t}\right\|_{F},\;\frac{1}{\sqrt{d} }\left\|\mathbf{W}_{0}-\mathbf{W}_{t}\right\|,\;\left\|\mathbf{K}_{0}^{\text{NTK}}-\mathbf{K} _{t}^{\text{NTK}}\right\|_{F},\;\text{and}\;\left\|\mathbf{K}_{0}^{\text{NTK}}-\bm {K}_{t}^{\text{NTK}}\right\|,\]

at every epoch \(t\) through training. The changes in Figures 8(a) and (b) are much smaller than in the last case. Figure 8(c) has significant changes in both norms after training, which is consistent with the heavy-tailed phenomenon in Figure 1(c). The global optima of the last case is far from the initialization.

Following the settings of Theorem 4.2, in Figure 9, we compute the differences between initial \(\mathbf{W}_{0}\) and final \(\mathbf{W}_{s}\) in Frobenius norm, operator norm and \(2,\infty\)-norm. Empirically, Figure 9 shows

\[\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{0}-\mathbf{W}_{s}\right\|_{F},\;\frac{1}{\sqrt{d} }\left\|\mathbf{W}_{0}-\mathbf{W}_{s}\right\|,\;\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{0}- \mathbf{W}_{s}\right\|_{2,\infty}=\Theta(1) \tag{16}\]

as \(n\rightarrow\infty\) with \(n/d\rightarrow\gamma_{1}\) and \(N/d\rightarrow\gamma_{2}\), where \(s\) is the final time for GD. Here, the entry-wise \(2\)-\(\infty\) matrix norm is defined as

\[\|\mathbf{M}\|_{2,\infty}:=\max_{1\leq i\leq N}\|\mathbf{m}_{i}\|,\]

for any matrix \(\mathbf{M}\in\mathbb{R}^{N\times d}\) with the \(i\)-th row \(\mathbf{m}_{i}\in\mathbb{R}^{d}\) and \(1\leq i\leq N\). Notice that

\[\|\mathbf{M}\|_{2,\infty}\leq\|\mathbf{M}\|\leq\|\mathbf{M}\|_{F}. \tag{17}\]

Similar observations for CK and NTK in both Frobenius norm and operator norm are also apparent in Figure 9, which empirically verifies the invariance of the spectra after training. Here, we fix the aspect ratios and let \(n\) grow to keep the NNs residing in LWR. For different \(n\)'s, we repeat the experiments 10 times for average. In each experiment, we train the NN until it converges. As shown in Figure 9(a), the test losses are almost the same for different \(n\)'s. Figures 9(b)-(d) empirically validate Corollary 4.3. Moreover, the observation that \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{0}-\mathbf{W}_{s}\right\|_{F}\) and \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{0}-\mathbf{W}_{s}\right\|\) are \(\Theta(1)\) may suggest that \(\frac{1}{\sqrt{d}}\big{(}\mathbf{W}_{0}-\mathbf{W}_{s}\big{)}\) is a _low-rank_ perturbation. That is, training in LWR may be transferring some low-rank structures to the weight spectrum. This low-rank perturbation can help us better understand the spectral evolution during training. Notice that these norms of the change are different from ultra-wide NN [27; 28]. Similar phenomena can be also observed in Figures 10(a) and (b). Analogous result with different \(\sigma\) and \(\sigma^{*}\) is exhibited in Figure 30 in Appendix C. In addition, Figure 10(c) further investigates the cases when NNs can outperform lazy training as defined by (6). In these experiments, we compare the performances of GD, and SGD with small or large learning rates, and lazy training as \(n\rightarrow\infty\). Each time, we take 10 trials to average. We observe that SGD with a large learning rate (green line) can asymptotically outperform lazy training.

Figure 7: The spectra of CK of the BERT model on Sentiment140 dataset at epoch 1, 2 and 12.

Case 1.Comparing with Figure 15, Figure 11 shows no alignments with training data in GD training. This corresponds to the performances in Table 1. The performance of Case 1 is not as good as the prediction risks in Figure 15, since Figure 11 suggests that no feature learning appears after GD training. Gradient descent requires the weights to converge to some global minima close to initialization, thereby offering no guarantees for lower generalization errors. Next, Figure 12 further presents more results on GD training and indicates more evidence of kernel regime in Case 1. This shows that, from a spectral point of view, the NTK is invariant/static through training. Based on Figures 1(a) and 12, we can empirically verify Corollary C.3 stated in Appendix C. Globally, the spectra of \(\mathbf{W}\), \(\mathbf{K}^{\text{CK}}\) and \(\mathbf{K}^{\text{NTK}}\) are not changing over training as \(n/d\rightarrow\gamma_{1}\) and \(N/d\rightarrow\gamma_{2}\). The initial spectrum of weight \(\mathbf{W}_{\,0}\) converges to Marcenko-Pastur law; the initial spectrum of NTK under proportional limit has been studied by [1, 30]. Figure 12(c) demonstrates the global convergence for GD under the proportional regime, as proved in Theorem 4.2. We can observe this global convergence even for SGD, Case 2 in Table 1, although we do not have proof for it.

Case 2.As a complement, Figure 13 exhibits the spectra of \(\mathbf{W}\), \(\mathbf{K}^{\text{CK}}\) and \(\mathbf{K}^{\text{NTK}}\) for Case 2 in Table 1. The phenomena are similar to Case 1. This observation provides evidence that all results and conjectures in Section 4.1 can be extended to SGD training with sufficiently small learning rates, which is subject to future work. Analogously to Theorem 4.2, we conjecture that the global convergence when training both layers of NN with SGD still holds in this proportional limit. The proof strategy for global convergence, in this case, can again follow [69, 70]. Once we have the invariant global spectra in Corollary 4.3, we can apply the nonlinear RMT [73, 57, 13, 30] to characterize the limiting spectra under LWR.

Case 3.Next, in Figures 14 and 15, we present spectral properties for Case 3 in Table 1, where a spike detaches from the bulk after large-step-size training. Notice that Figures 1(b) and 14(a) imply that the bulk spectra for weight and CK remain unchanged over training despite the emergence of spikes. This is not true for NTK by observing Figures 14(b) and (c). The Frobenius norm of NTK changes significantly during training and is not \(O(1)\) anymore; the spectra of the first component

Figure 8: The evolution of the changes in operator/Frobenius norms of the weight/CK/NTK matrices through different training processes. Each case corresponds to the case in Table 1. Case 3 is exhibited in Figure 14(c) below.

Figure 9: Performances of NNs and changes in different norms for weight and kernels, when \(d/n=0.6\) and \(N/n=1.2\) are fixed as \(n\) is growing. The activation is normalized \(\tanh\) and the teacher model is \(f^{*}(\mathbf{x})=\sigma^{*}(\mathbf{\beta}^{\top}\mathbf{x})\) where \(\sigma^{*}\) is a normalized _softplus_. We average over 10 trials in each case. All these curves are almost flat, which indicates these values are not growing with \(\gamma_{1}\) and \(\gamma_{2}\). Here, in the second figure from the left, we normalized all weights \(\mathbf{W}\) with \(\frac{1}{\sqrt{d}}\) to observe (16).

[MISSING_PAGE_FAIL:20]

NTK both increase and the losses first increase and then drop. In Figure 19, we empirically justify that the phase transitions we presented in section 4.2 for SGD can be also extended to full-batch GD cases. We can also observe phase transitions for test losses and \(R^{2}\) scores when we are gradually increasing the learning rates. Parallel to these, a spike also appears outside the bulk distribution, which corresponds to feature alignments in Figure 19(c)&(f).

### Multiple-index Examples for Heavy-Tailed Spectra in Section 4.3

Figures 17(b) and (c) are additional results for Figure 3(b) in Section 4.3. In this experiment, we consider \(\sigma=\text{ReLU}\), \(n=5000\), \(h=2500\) and \(d=1000\) for NN (1). Comparing with the teacher model (9) used in Table 1, we employ the multiple-index teacher model (8) with \(k=5\) and \(\sigma^{*}=\sigma\). We trained this student-teacher model using GD (\(\eta=15\)), SGD (\(\eta=7.25\) and batch size 8), and Adam (\(\eta=0.007\) and batch size 16) for training this NN, respectively. Similarly with Figure 1, correspondingly, we observe invariant spectrum, bulk with one spike, and heavy tails after training respectively. Heuristically, to learn this \(f^{*}\), the weight \(\mathbf{W}\) of NN should gradually align with the feature space \(U\) spanned by \(\mathbf{\beta}_{i}\)'s. Hence, to study feature learning, we can apply principle angles to measure the alignment between \(\mathbf{W}\) and \(U\). Consider the eigen-decomposition of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}=\sum_{i=1}^{d}\lambda_{i}\mathbf{v}_{i}\mathbf{v}_{i}^{\top}\) with \(\lambda_{1}\geq\lambda_{2}\geq\ldots\geq\lambda_{d}\). Figure 3(b) shows the heavy-tailed part (the eigenspace \(E:=\text{span}\{\mathbf{v}_{i}\}_{i=1}^{100}\) is aligned with \(U\) after training, which shows how features are learned in the heavy-tailed spectra. Remarkably, the test errors for training processes with SGD and Adam are even smaller than \(\|\mathbb{P}_{>1}f^{*}\|^{2}\) and \(\|\mathbb{P}_{>2}f^{*}\|^{2}\), where \(\mathbb{P}_{>1}\) denotes the orthogonal projection onto the nonlinear part of the function w.r.t. Gaussian measure. Thus, we experimentally showed that NNs with heavy-tailed spectra can obtain feature learning and generalize better than the other two cases. Another example is exhibited in Figure 20. In this case, \(k=5\) and there are five leading outlier eigenvalues in the spectrum of the trained weight matrix, along with a heavy-tailed bulk. Interestingly, Figure 20 justifies that the eigenspace of these five leading outliers is strongly aligned with features \(\mathbf{\beta}_{i}\) for \(1\leq i\leq 5\). This indicates that heavy-tailed spectra with large spikes may have a correlation with feature learning and good generalizations.

Figure 12: Performances of Case 1 in Table 1: (a) The initial and trained spectra of the first-hidden layer \(\mathbf{W}\). (c) The initial and trained spectra of empirical NTK matrix defined by (5). Q-Q subplot shows these two spectra are almost the same. Training and test losses vs. epochs for GD (right).

Figure 13: Spectral properties for Case 2 in Table 1: (a) The initial and trained spectra of the first-hidden layer \(\mathbf{W}\). (b) The initial and trained spectra of empirical NTK are defined by (5). (c) The initial and trained spectra of empirical CK defined by (4).

[MISSING_PAGE_FAIL:22]

can get heavy-tailed distributions after training, and the phenomena are essentially the same as Case 4 of Table 1. There is no strong alignment for the first leading eigenvector, while useful features may be learned by a few top eigenvectors in the heavy-tailed spectra after training. In the second experiment, we directly apply Adam with a small initial learning rate for training. In contrast, the results of this case, presented in Figures 26 and 27, are similar to Case 3 in Table 1. The test loss and \(R^{2}\) score are close to previous examples, though slightly worse. As explained in the previous section, because there is only one spike appearing outside the bulk after training in the second case, the leading PC is highly aligned with the training dataset structure after training and the feature learning mainly stems from the outliers in this situation. This interprets the strongly anisotropic structures in trained spectra of NNs in the second case [67, 68, 78]. These two different spectral properties reveal significant differences between the global minima of these two training processes and different evolutions of the

Figure 16: Additional performance for Case 4 in Table 1: (a) The initial and trained weight spectra. Notice that there are several outliers after training, while the bulk has a heavier tail. (b) The spectra of the NTK (5) at initialization and after training. (c) The test/training losses and \(R^{2}\) score (green line) at each epoch \(t\) throughout training process. (d) Alignment between the leading PC of the weight matrix and the signal \(\beta\) in the teacher model before (red) and after (blue) training. (e) Alignment between the leading PC of the CK matrix and the training labels \(\mathbf{y}\) before/after training. (f) Alignment between the leading PC of the NTK matrix and \(\mathbf{y}\) before/after training.

Figure 17: (a) The evolution of power \(\alpha\), weighted Alpha and Log \(\alpha\)-norm (several metrics of power law tails; see [60]) during the training process in Case 4 of Table 1. (b) Evolutions of the first PC angle \(\theta_{1}\) between _feature subspace_\(U=\text{span}\{\mathbf{\beta}_{1}\}_{i=1}^{N}\) of the multiple-index model (8) and the _eigenspace_ spanned by top 100 of eigenvectors of \(\mathbf{W}_{i}^{*}\,\mathbf{W}_{t}\) during training with Adam (blue solid line), SGD ( red dashed line) and GD (green dash-dot). The final test error is 0.33865 and the \(R^{2}\) score is -0.71065 for GD. The test error is 0.10814 and the \(R^{2}\) score is 0.45373 for SGD, where one spike emerges in the weight spectrum after training. The test error is 0.08672 and the \(R^{2}\) score is 0.56195 for Adam. (c) Initial and trained spectra for weight matrices when training with Adam (blue solid line in (b)). Heavy tail emerges in this case.

spectra in NNs. Remarkably, based on the different spectral behaviors in trained weight and kernel matrices, these two experiments exhibit disparate features learned by distinct training procedures. Hence, analyzing the spectral properties in trained kernel matrices is beneficial for clarifying what features our NNs have learned during the training processes.

## Appendix C Proofs of Results in Section 4.1

### GD Analysis at Early Phase

From (10), the GD process with learning rate \(\eta>0\) can be written by

\[\boldsymbol{W}_{t+1} =\ \boldsymbol{W}_{t}+\eta\cdot\boldsymbol{G}_{t},\text{ where} \tag{18}\] \[\boldsymbol{G}_{t} =\ \frac{1}{n\sqrt{dh}}\left[\left(\boldsymbol{v}\left(\boldsymbol {y}-\frac{1}{\sqrt{h}}\boldsymbol{v}^{\top}\sigma(\boldsymbol{W}_{t} \boldsymbol{X}/\sqrt{d})\right)\right)\odot\sigma^{\prime}(\boldsymbol{W}_{t} \boldsymbol{X}/\sqrt{d})\right]\boldsymbol{X}^{\top}, \tag{19}\]

for \(t\in\mathbb{N}\), where \(\boldsymbol{y}\in\mathbb{R}^{1\times n}\). Following [6, Appendix B], in this section we prove the control for gradient step \(\boldsymbol{G}_{t}\). For simplicity, denote \(f_{t}(\boldsymbol{X}):=f_{\boldsymbol{\theta}_{t}}(\boldsymbol{X})=\frac{1}{ \sqrt{h}}\boldsymbol{v}^{\top}\sigma(\boldsymbol{W}_{t}\boldsymbol{X}/\sqrt{d })\) for \(t\in\mathbb{N}\).

Figure 19: Grid search for different learning rates when training NNs with full-batch GD in the same setting as Case 1 in Table 1. (a) Final test losses when varying learning rates. (d) \(R^{2}\) scores when varying learning rates. For all these learning rates, we did not observe heavy-tailed spectra. (b-c) present the spectral behaviors for the smallest learning rate we used in (a)&(d). (e-f) present the spectral behaviors for the largest learning rate we can use which still ensures the convergence of GD. In this case, analogously to the SGD case in Section 4.2, we observe an outlier in the trained weight matrix and strong alignment with the spike.

Figure 18: The training dynamic when training neural networks with SGD and learning rate 24 in the example of Figure 2(b-c). The learning rate we chose here is above the threshold we showed in Figure 2(b-c). We use the same architecture, dataset, and teacher model as in Section 3 of our paper. The batch size is 32. (a) The evolution of the training and test errors during training. (b) The evolution of the largest eigenvalue of the CK matrix. (c) The evolution of the largest eigenvalue of the NTK matrix. This regime corresponds to the catapult phenomenon [49].

**Lemma C.1**.: _Under the same assumptions as in Lemma 4.1, we have_

\[\mathbb{P}\left(\left\|\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\| \geq C\sqrt{n}\right) \leq\ 2e^{-cn},\] \[\mathbb{P}\left(\left\|\mathbf{y}\right\|\geq C\sqrt{n}\right) \leq\ 2e^{-cn},\]

_for some constants \(C,c>0\) only depending on \(\sigma_{\varepsilon}\), \(\lambda_{\sigma}\), \(\gamma_{1}\), and \(\gamma_{2}\)._

Proof.: Due to [30, Lemma D.4.], we can directly obtain that

\[\mathbb{P}\left(\left\|\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\| \geq C^{\prime}(\sqrt{n}+\sqrt{h})\sqrt{\frac{h}{d}}\right)\leq 2e^{-cn}.\]

Here we use the fact that both \(\mathbf{W}_{0}\) and \(\mathbf{X}\) are i.i.d. Gaussian random matrices. Then by Assumption 3.1, we conclude that we control \(\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\). Recall that Assumption 3.3 implies that \(\mathbf{y}=f^{*}(\mathbf{X})+\varepsilon\). Hence, by Lipschitz Gaussian concentration inequality [80, Theorem 5.2.2], each entry of \(f^{*}(\mathbf{X})\) has independent sub-Gaussian coordinates, whence we can get \(\|f^{*}(\mathbf{X})\|\leq C\sqrt{n}\) with probability at least \(1-2ne^{-cn}\) for some constants \(c,C>0\). On the other hand, \([\varepsilon]_{i}=\varepsilon_{i}\) are i.i.d. centered sub-Gaussian noises with variance \(\sigma_{\varepsilon}^{2}\). By [80, Theorem 3.1.1], we have

\[\mathbb{P}\left(\|\varepsilon\|\leq 2\sigma_{\varepsilon}\sqrt{n}\right) \geq 1-2\exp\left(-\frac{cn}{K^{4}}\right),\]

where the constant \(K\) is the sub-Gaussian norm defined by \(K=\max_{i}\|\varepsilon_{i}\|_{\psi_{2}}\). Hence, combining all things together, we obtain the second inequality of this lemma.

Figure 21: Additional spectral performance when training the NN by Adam with small learning rate \(\eta=0.001\), where \(n=2000,h=3000,d=1000\), \(\sigma_{\varepsilon}=0.3\) and batch size is 100. In this simulation, we only train the first hidden layer \(\mathbf{W}_{t}\). The activation function \(\sigma\) is a normalized softplus and the target function is a normalized \(\tanh\). The final test loss is 0.36219 and \(R^{2}\) score is around \(63.70\%\).

Figure 20: (Left) Initial and trained spectra for weight matrices when training with Adam. Five leading spikes emerge in this case. (Right) Evolutions of the angles between the first four PCs of \(\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\) and _feature subspace_\(U=\text{span}\{\beta_{i}\}_{i=1}^{k}\) of the multiple-index model (8) during training with Adam. Here \(k=5\). The final test error is 0.33865 and \(R^{2}\) score is -0.71065 for GD. The test error is 0.01681 and \(R^{2}\) score is 0.9154 for Adam.

**Lemma C.2**.: _Under the assumptions of Lemma 4.1, given any fixed \(t\in\mathbb{N}\) and learning rate \(\eta=\Theta(1)\), the weight matrix after \(t\) gradient steps \(\boldsymbol{W}_{t}\) defined in (18) satisfies_

\[\mathbb{P}\left(\left\|\boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{F}\geq \frac{C}{\sqrt{n}}\right)\leq\exp\left(-cn\right), \tag{20}\]

_for some positive constants \(c,C>0\) only depending on \(t,\eta,\sigma_{e}\), \(\lambda_{\sigma}\), \(\gamma_{1}\) and \(\gamma_{2}\)._

Proof.: Denote \(\sigma_{\perp}(x)=\sigma(x)-\mu_{1}x\) which is the nonlinear part of \(\sigma\) and \(\mu_{1}=\mathbb{E}[z\sigma(z)]\). Thus, \(\mathbb{E}[\sigma_{\perp}(z)z]=0\) for \(z\sim\mathcal{N}(0,1)\). Based on this, we can further decompose the gradient \(\boldsymbol{G}_{t}\) into

\[\boldsymbol{G}_{t}=\underbrace{\frac{\mu_{1}}{n\sqrt{dh}} \boldsymbol{v}\left(\boldsymbol{y}-f_{t}(\boldsymbol{X})\right)\boldsymbol{X }^{\top}}_{\boldsymbol{A}^{t}}+\underbrace{\frac{1}{n\sqrt{dh}}\left( \boldsymbol{v}\left(\boldsymbol{y}-f_{t}(\boldsymbol{X})\right)\odot\sigma^{ \prime}_{\perp}\left(\boldsymbol{W}_{t}\boldsymbol{X}/\sqrt{d}\right)\right) \boldsymbol{X}^{\top}}_{\boldsymbol{B}^{t}}. \tag{21}\]

Figure 23: Alignment between the leading PC of the weight/kernel matrices and the signal \(\boldsymbol{\beta}\) or the training labels \(\boldsymbol{y}\) before/after training for experiment in Figure 22. Analogously to Case 4 in Appendix B.2, there is no strong alignment in the leading component of weight/kernel matrices.

Figure 24: Additional performance for Adam with learning rate \(\eta=0.09\) and \(4\) epochs, then SGD with learning rate \(\eta=5\times 10^{-4}\) and \(100\) epochs, where \(n=2000,h=1500,d=1000\), \(\sigma_{e}=0.3\) and the batch size is 32. We train the NN until the training loss is less than \(10^{-10}\). The activation \(\sigma\) is normalized softplus and the target is normalized \(\tanh\). The final test loss is 0.22511 and \(R^{2}\) score is around \(0.77462\).

At first, consider \(t=0\) and bound the spectral norm of \(\mathbf{W}_{1}\). By assumption, we know \(\left\lVert\mathbf{v}\right\rVert\leq\sqrt{h}\). Due to Corollary 7.3.3 in [80], we have

\[\mathbb{P}\left(\frac{1}{\sqrt{d}}\lVert\mathbf{X}\rVert\geq 2\left(1+\sqrt{\frac{n}{ d}}\right)\right)\leq 2\exp\left(-cn\right). \tag{22}\]

Therefore, by (21), we can control \(\mathbf{A}^{0}\) and \(\mathbf{B}^{0}\) separately. Notice that, as a rank-one matrix,

\[\left\lVert\mathbf{A}^{0}\right\rVert=\left\lVert\mathbf{A}^{0}\right\rVert _{F} \leq\frac{\mu_{1}}{\sqrt{n}}\frac{\lVert\mathbf{X}\rVert}{\sqrt{d}} \frac{1}{\sqrt{n}}(\left\lVert\mathbf{y}\right\rVert+\left\lVert f_{0}(\mathbf{X}) \right\rVert)\frac{\left\lVert\mathbf{v}\right\rVert}{\sqrt{h}}\] \[\leq\frac{\mu_{1}}{\sqrt{n}}\frac{\lVert\mathbf{X}\rVert}{\sqrt{d}} \frac{\lVert\mathbf{v}\rVert}{\sqrt{h}}\frac{1}{\sqrt{n}}\left(\left\lVert\mathbf{y} \right\rVert+\frac{\lVert\mathbf{v}\rVert}{\sqrt{h}}\left\lVert\sigma(\mathbf{W}_{0} \mathbf{X}/\sqrt{d})\right\rVert\right).\]

Hence, by Lemma C.1 and (22), one can easily claim that \(\left\lVert\mathbf{A}^{0}\right\rVert\leq C/\sqrt{n}\) with probability at least \(1-e^{-cn}\) for some constants \(c,C>0\). On the other hand, since \(\mathbf{v}\left(\mathbf{y}-f_{t}(\mathbf{X})\right)\) is rank-one and \(\sigma_{\perp}^{\prime}=\sigma^{\prime}-\mu_{1}\) with \(\left\lvert\sigma^{\prime}(x)\right\rvert\leq\lambda_{\sigma}\), we can similarly obtain

\[\left\lVert\mathbf{B}^{0}\right\rVert_{F} \leq\frac{1}{n\sqrt{dh}}\left\lVert\mathbf{v}\left(\mathbf{y}-f_{t}(\mathbf{X })\right)\odot\sigma_{\perp}^{\prime}(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})\right\rVert_{ F}\left\lVert\mathbf{X}\right\rVert\] \[\leq\frac{\mu_{1}+\lambda_{\sigma}}{\sqrt{h}}\frac{\lVert\mathbf{X} \rVert}{\sqrt{d}}\frac{\lVert\mathbf{v}\rVert}{\sqrt{h}}\frac{1}{\sqrt{n}}\left( \left\lVert\mathbf{y}\right\rVert+\frac{\lVert\mathbf{v}\rVert}{\sqrt{h}}\left\lVert \sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\rVert\right).\]

As \(\mathbf{A}^{0}\), we can apply Lemma C.1 and (22) again to conclude (20) for \(t=1\).

For general \(t\), we apply induction. We assume that after the \(t\)-th gradient step with \(\eta=\Theta(1)\), Eq. (20) holds for some constants \(C,c>0\). Following [6, Lemma 16], we now show that the similar high-probability statement also holds for \(\mathbf{W}_{t+1}\) (for some different constants \(c^{\prime},C^{\prime}\)). Firstly,

Figure 26: Additional performance for Adam with learning rate \(\eta=0.002\) and \(700\) epochs, where \(n=2000,h=1500,d=1000\), \(\sigma_{\varepsilon}=0.3\) and batch size is 64. The activation \(\sigma\) is normalized softplus and the target is normalized \(\tanh\). The final test loss is 0.23954 and \(R^{2}\) score is around \(0.76027\). The orange arrows show the positions of the outliers. Spectra behaviors in this case differ from Figure 24.

Figure 25: Alignment between the leading PC of the weight/kernel matrices and the signal \(\mathbf{\beta}\) or the training labels \(\mathbf{y}\) before/after training for the experiment in Figure 24. This is analogous to Case 4 in Appendix B.2.

following the same argument as [71, Setion 6.6.1], we know that

\[\left\|f_{t}(\mathbf{X})\right\|\leq \left\|f_{0}(\mathbf{X})\right\|+\left\|f_{t}(\mathbf{X})-f_{0}(\mathbf{X})\right\|\] \[\leq \left\|f_{0}(\mathbf{X})\right\|+\frac{\lambda_{\sigma}}{\sqrt{h}} \left\|\mathbf{v}\right\|\frac{\left\|\mathbf{X}\right\|}{\sqrt{d}}\left\|\mathbf{W}_{t}- \mathbf{W}_{0}\right\|_{F}. \tag{23}\]

Note that \(\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{F}=O(1/\sqrt{n})\) with high probability by the induction hypothesis. Hence, by Lemma C.1 and (22), we have \(\left\|f_{t}(\mathbf{X})\right\|\leq C\sqrt{n}\) with high probability. Indeed, the difference between \(f_{t}(\mathbf{X})\) and \(f_{0}(\mathbf{X})\) is significantly negligible comparing with the initial value \(f_{0}(\mathbf{X})\). Similarly with \(\mathbf{A}_{0}\), \(\mathbf{A}^{t}\) satisfies

\[\left\|\mathbf{A}^{t}\right\|=\left\|\mathbf{A}^{t}\right\|_{F}\leq\frac{\mu_{1}}{ \sqrt{n}}\frac{\left\|\mathbf{X}\right\|}{\sqrt{d}}\frac{1}{\sqrt{n}}(\left\|\mathbf{y }\right\|+\left\|f_{t}(\mathbf{X})\right\|)\frac{\left\|\mathbf{v}\right\|}{\sqrt{h}}.\]

Analogously for \(\mathbf{B}^{t}\), we have

\[\left\|\mathbf{B}^{t}\right\|_{F}\leq\frac{\mu_{1}+\lambda_{\sigma}}{\sqrt{n}} \frac{\left\|\mathbf{X}\right\|}{\sqrt{d}}\frac{\left\|\mathbf{v}\right\|}{\sqrt{h}} \frac{1}{\sqrt{n}}(\left\|\mathbf{y}\right\|+\left\|f_{t}(\mathbf{X})\right\|).\]

Thus, Lemma C.1, (22), and (23) ensure that

\[\mathbb{P}\left(\left\|\mathbf{A}^{t}\right\|_{F}\geq\frac{C^{\prime}}{\sqrt{n}} \right)\leq\exp\left(-c^{\prime}n\right),\ \mathbb{P}\left(\left\|\mathbf{B}^{t} \right\|_{F}\geq\frac{C^{\prime}}{\sqrt{n}}\right)\leq\exp\left(-c^{\prime}n \right),\]

for constants \(c^{\prime},C^{\prime}>0\). Since \(\left\|\mathbf{W}_{t+1}-\mathbf{W}_{0}\right\|_{F}\leq\left\|\mathbf{W}_{t}-\mathbf{W}_{0} \right\|_{F}+\eta\left\|\mathbf{A}^{t}\right\|_{F}+\eta\left\|\mathbf{B}^{t}\right\|_ {F},\) by induction hypothesis, we can conclude that (20) holds for the \((t+1)\)-th step with some constants \(C,c>0\), which are different from the constants at the \(t\)-th step. 

As a corollary, by (17), we can also deduce the following norm bounds:

Lemma C.2 and the above bounds are empirically verified by Figure 28(a) for \(t=3\). Not only upper bounds, this simulation also shows that at early phase \(\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|\), \(\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{F}\), and \(\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{2,\infty}\) are all of the same \(\Theta(1/\sqrt{n})\) order.

As a remark, from the bound of the second term of (23), we can deduce that the change of the output of the NN satisfies

\[\left|f_{t}(\mathbf{x})-f_{0}(\mathbf{x})\right|\leq\frac{C}{\sqrt{n}},\]

for some \(t\)-dependent constant \(C>0\), any \(\mathbf{x}\sim\mathcal{N}(0,\mathbf{I})\) and any finite time \(t\). In other words, when \(\eta=\Theta(1)\), the change of the output of the NN at the early phase (i.e. \(t=\Theta(1)\)) is negligible and its order is \(O(\frac{1}{\sqrt{n}})\).

Figure 27: Alignment between the leading PC of the weight/kernel matrices and the signal \(\mathbf{\beta}\) or the training labels \(\mathbf{y}\) before/after training for the experiment in Figure 26. We can observe strong alignments, in this case, comparing with Figure 25 because of outliers in above Figure 26. These kernel alignments induce anisotropic structures in the kernel matrices during training [78].

### Proof of Lemma 4.1

In this section, we complete the proof of Lemma 4.1. We first mention the empirical validation of Lemma 4.1 in Figure 28. Notice that the changes in Frobenius norm for \(\mathbf{W}\) and \(\mathbf{K}^{\text{CK}}\) are exactly \(\Theta(1/\sqrt{n})\) and \(\Theta(1/n)\), respectively. The operator norm of \(\mathbf{K}^{\text{NTK}}\) matches with Lemma 4.1, while the Frobenius norm of the change decays slower than the rate \(\Theta(1/n)\). Additionally, in the simulation, we use \(\mathbf{v}\sim\mathcal{N}(0,\mathbf{I})\), which indicates that our assumption for \(\mathbf{v}\) in Lemma 4.1 can be weakened.

Proof of Lemma 4.1.: Lemma C.2 directly validates the control of \(\frac{1}{\sqrt{d}}\left\lVert\mathbf{W}_{t}-\mathbf{W}_{0}\right\rVert_{F}\). By virtue of this result, we now present estimates for CK and NTK. Based on [71, Section 6.6.1], we have

\[\left\lVert\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d}) \right\rVert\leq\frac{\lambda_{\sigma}}{\sqrt{d}}\|\mathbf{X}\|\|\mathbf{W}_{0}-\mathbf{W }_{t}\|_{F}. \tag{24}\]

We apply the mean value theorem to obtain this inequality. Recall the operator norm bound for Gaussian random matrix \(\mathbf{X}\) in (22). We know \(\left\lVert\mathbf{X}/\sqrt{d}\right\rVert\lesssim 1+\sqrt{\gamma_{1}}\) with high probability as \(n/d\to\gamma_{1}\). Hence, with the help of Lemma C.2, we can claim

\[\left\lVert\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d}) \right\rVert\leq C\lambda_{\sigma}(1+\sqrt{\gamma_{1}})/\sqrt{n},\]

with probability at least \(1-\exp\left(-cn\right)\), for any fixed finite \(t\in[n]\). Similarly, we can control the change in the Frobenius norm as follows:

\[\left\lVert\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d}) \right\rVert_{F}^{2}\leq\ \frac{\lambda_{\sigma}^{2}}{d}\left\lVert\mathbf{X} \right\rVert^{2}\left\lVert\mathbf{W}_{0}-\mathbf{W}_{t}\right\rVert_{F}^{2}\leq C \lambda_{\sigma}^{2}(1+\sqrt{\gamma_{1}})^{2}/n, \tag{25}\]

with probability at least \(1-\exp\left(-cn\right)\). Therefore, we can control the change in the CK matrix in the Frobenius norm by the following inequalities:

\[\left\lVert\mathbf{K}^{\text{CK}}_{t}-\mathbf{K}^{\text{CK}}_{0}\right\rVert_{F}\] \[\leq\ \frac{1}{h}\left\lVert\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})^{ \top}\left(\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d}) \right)\right\rVert_{F}+\frac{1}{h}\left\lVert\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d })^{\top}\left(\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt {d})\right)\right\rVert_{F}\] \[\leq\ \frac{1}{h}\left(\left\lVert\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})- \sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\rVert+\left\lVert\sigma(\mathbf{W}_{0}\mathbf{ X}/\sqrt{d})\right\rVert\right)\cdot\left\lVert\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})- \sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\rVert_{F}\] \[\quad+\ \frac{1}{h}\left\lVert\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d}) \right\rVert\cdot\left\lVert\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{ 0}\mathbf{X}/\sqrt{d})\right\rVert_{F}.\]

Therefore, by (24), (25) and Lemma C.1, we can claim that there exist constants \(c,C>0\) such that with probability at least \(1-\exp\left(-cn\right)\), \(\left\lVert\mathbf{K}^{\text{CK}}_{t}-\mathbf{K}^{\text{CK}}_{0}\right\rVert_{F}\) is upper bounded by \(C/n\) in the LWR.

Now we consider the change in the NTK matrix during training. Since the empirical NTK can be decomposed into two parts, one of which is exactly the CK, it suffices to consider the change of the first part of the empirical NTK. Recall that

\[\mathbf{K}_{t}:=\frac{1}{d}\mathbf{X}^{\top}\mathbf{X}\odot\frac{1}{h}\sigma^{\prime} \left(\frac{1}{\sqrt{d}}\mathbf{W}_{t}\mathbf{X}\right)^{\top}\text{diag}(\mathbf{v}_{t}) ^{2}\sigma^{\prime}\left(\frac{1}{\sqrt{d}}\mathbf{W}_{t}\mathbf{X}\right).\]

Figure 28: Empirical validations for Lemma 4.1 and Lemma C.2 at \(t=3\). Here \(\sigma_{\varepsilon}=0.2\), activation \(\sigma\) is a normalized ReLU and the target function \(\sigma^{*}\) is normalized \(\tanh\). Fix \(d/n=0.6\) and \(N/n=1.2\) as \(n\) is increasing. At each dimension, we take 25 trials to average. (a) Norms of the changes for \(\mathbf{W}_{3}-\mathbf{W}_{0}\). (b) Norms of the changes for \(\mathbf{K}^{\text{CK}}_{3}-\mathbf{K}^{\text{CK}}_{0}\). (c) Norms of the changes for \(\mathbf{K}^{\text{NTK}}_{3}-\mathbf{K}^{\text{NTK}}_{0}\).

Following the notation in [71], we denote \(\mathcal{J}(\mathbf{W}_{t}):=[\mathcal{J}(\mathbf{w}_{t}^{\top}),\ldots,\mathcal{J}(\mathbf{w}_ {N}^{t})]\in\mathbb{R}^{n\times hd}\) with \(\mathcal{J}(\mathbf{w}_{i}):=\frac{v_{i}}{\sqrt{h}}\text{diag}(\sigma^{\prime}(\mathbf{ X}^{\top}\mathbf{w}_{i}/\sqrt{d}))\mathbf{X}^{\top}/\sqrt{d}\in\mathbb{R}^{n\times d}\). Hence, \(\mathbf{K}_{t}=\mathcal{J}(\mathbf{W}_{t})\mathcal{J}(\mathbf{W}_{t})^{\top}\) and

\[\|\mathbf{K}_{t}-\mathbf{K}_{0}\| =\ \left\|\mathcal{J}(\mathbf{W}_{t})\mathcal{J}(\mathbf{W}_{t})^{\top}- \mathcal{J}(\mathbf{W}_{0})\mathcal{J}(\mathbf{W}_{0})^{\top}\right\|\] \[\leq\ 2\left\|\mathcal{J}(\mathbf{W}_{0})\right\|\left\|\mathcal{J}( \mathbf{W}_{t})-\mathcal{J}(\mathbf{W}_{0})\right\|+\left\|\mathcal{J}(\mathbf{W}_{t})- \mathcal{J}(\mathbf{W}_{0})\right\|^{2}. \tag{26}\]

By [71, Lemma 6.6], we know \(\left\|\mathcal{J}(\mathbf{W}_{0})\right\|^{2}=\left\|\mathbf{K}_{0}^{\text{NTK}}\right\|\) is upper bounded by some constant \(C>0\) with high probability. Then, we apply the inequalities from Lemma 6.5 of [71] to obtain

\[\|\mathcal{J}(\mathbf{W}_{t})-\mathcal{J}(\mathbf{W}_{0})\|^{2} \tag{27}\] \[= \left\|\left(\left(\sigma^{\prime}(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})- \sigma^{\prime}(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right)^{\top}\frac{\text{diag}(\mathbf{ v})^{2}}{h}\left(\sigma^{\prime}(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma^{\prime}(\mathbf{W}_ {0}\mathbf{X}/\sqrt{d})\right)\right)\odot\left(\frac{\mathbf{X}^{\top}\mathbf{X}}{d} \right)\right\|\] \[\leq \left\|\left(\sigma^{\prime}(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma^{ \prime}(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right)^{\top}\frac{\text{diag}(\mathbf{v})}{ \sqrt{h}}\right\|^{2}\left(\max_{i\in[n]}\left\|\mathbf{x}_{i}/\sqrt{d}\right\|^{2 }\right)\] \[\leq \frac{1}{h}\|\mathbf{v}\|_{\infty}^{2}\left\|\sigma^{\prime}(\mathbf{W}_ {t}\mathbf{X}/\sqrt{d})-\sigma^{\prime}(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\|^{2} \left(\max_{i\in[n]}\left\|\mathbf{x}_{i}/\sqrt{d}\right\|^{2}\right)\] \[\leq \frac{\lambda_{\sigma}^{2}}{h}\frac{\left\|\mathbf{X}\right\|^{2}}{d} \left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{F}^{2}\left(\max_{i\in[n]}\left\|\mathbf{x} _{i}/\sqrt{d}\right\|^{2}\right),\]

where the last inequality is due to the mean value theorem, the uniform bound on \(\sigma^{\prime\prime}\), and the assumption on the second layer \(\mathbf{v}\). Notice that Gaussian random vectors satisfy

\[\mathbb{P}\left(\max_{i\in[n]}\frac{1}{d}\|\mathbf{x}_{i}\|^{2}\geq 2\right)\leq 2ne^{-cn}, \tag{28}\]

as \(n/d\rightarrow\gamma_{1}\) and \(h/d\rightarrow\gamma_{2}\). Thus, with (22) and Lemma C.2, we obtain

\[\mathbb{P}\left(\|\mathcal{J}(\mathbf{W}_{t})-\mathcal{J}(\mathbf{W}_{0})\|\geq\frac{C \lambda_{\sigma}(1+\gamma_{1})}{n}\right)\leq 4ne^{-cn},\]

where constant \(C\) relies on the number of steps \(t\). Hence, by (26), we can finally bound in norm the difference between the initial and the trained NTK matrices at the early phase (\(t\) is finite).

**Corollary C.3**.: _For any fixed \(t\in\mathbb{N}\), \(i\in[d]\) and \(k\in[n]\), denote \(\lambda_{i}^{t}\), \(\nu_{k}^{t}\) and \(\mu_{k}^{t}\) the \(i\)-th, and \(k\)-th eigenvalues of \(\frac{1}{h}\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\), \(\mathbf{K}_{t}^{\text{CK}}\) and \(\mathbf{K}_{t}^{\text{NTK}}\), respectively. Then, under the assumptions of Lemma 4.1, we have_

\[|\lambda_{i}^{t}-\lambda_{i}^{0}|,\ |\nu_{k}^{t}-\nu_{k}^{0}|,\ |\mu_{k}^{t}-\mu_{k }^{0}|\to 0,\]

_almost surely in LWR. Consequently, the eigenvalues of \(\frac{1}{h}\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\), \(\mathbf{K}_{t}^{\text{CK}}\) and \(\mathbf{K}_{t}^{\text{NTK}}\) are the same as corresponding the eigenvalues of initial \(\frac{1}{h}\mathbf{W}_{0}^{\top}\mathbf{W}_{0}\), \(\mathbf{K}_{0}^{\text{CK}}\) and \(\mathbf{K}_{0}^{\text{NTK}}\), respectively._

This corollary is a direct outcome of Weyl's inequality from Theorem A.46 in [8]. Consequently, this corollary concludes that for any fixed \(t\geq 0\), almost surely, the limiting spectra of \(\frac{1}{h}\mathbf{W}_{t}^{\top}\mathbf{W}_{t}\), \(\mathbf{K}_{t}^{\text{CK}}\) and \(\mathbf{K}_{t}^{\text{NTK}}\) are the same as those of \(\frac{1}{h}\mathbf{W}_{0}^{\top}\mathbf{W}_{0}\), \(\mathbf{K}_{0}^{\text{CK}}\) and \(\mathbf{K}_{0}^{\text{NTK}}\) in LWR. This corollary claims that not only does the bulk of distributions stay identical to the initialization, but also that any eigenvalues stay the same as at the initialization. This shows that the smallest eigenvalue of \(\mathbf{K}_{t}^{\text{NTK}}\) has the same lower bound as \(\mathbf{K}_{0}^{\text{NTK}}\) in the early phase of training.

### Global Convergence for GD Under LWR

In this section, we study the final stage of (10) as training loss is approaching zero and prove Theorem 4.2. Figure 29 shows that the spectra are unchanged globally, even after training in this case. In Corollary 4.3, we confirm this observation for the weight, CK, and NTK matrices via Frobenius norm control. In the simulation, the second layer is initialized as \(\mathbf{v}\sim\mathcal{N}(0,\mathbf{I})\), which is more general than our assumption on \(\mathbf{v}\) in Theorem 4.2.

Proof of Theorem 4.2.: Recall the Jacobian matrix \(\mathcal{J}(\mathbf{W})\) defined in the proof of Lemma 4.1, and the definition of \(\alpha\) based on (11) in Section 3. Denote the event

\[\mathcal{A}:=\left\{\|\mathbf{X}\|\leq 2\left(1+\sqrt{\gamma_{1}}\right)\sqrt{d}, \ \max_{i\in[n]}\|\mathbf{x}_{i}\|^{2}\leq 2d,\sigma_{\min}(\mathcal{J}(\mathbf{W}_{0})) \geq 2\alpha\right\}.\]

By (22), (28) and Theorem 2.9 of [81], we have \(\mathbb{P}\left(\mathcal{A}\right)\geq 1-2e^{-cn}-2ne^{-cn}-n^{-7/3}\) for some constant \(c>0\) and all large \(n\) in LWR. In the following, conditionally on event \(\mathcal{A}\), we will apply Theorem 6.10 of [71] to obtain the global convergence. Conditionally on \(\mathcal{A}\), Lemma 6.6 of [71] implies

\[\left\|\mathcal{J}(\mathbf{W})\right\|\leq\lambda_{\sigma}\left\|\mathbf{v}\right\|_{ \infty}\left\|\mathbf{X}/\sqrt{d}\right\|\leq 2\lambda_{\sigma}(1+\sqrt{\gamma_{1}}), \tag{29}\]

for any \(\mathbf{W}\). Define \(\beta=2\lambda_{\sigma}(1+\sqrt{\gamma_{1}})\). Moreover, in terms of (27), we can verify the Lipschitz property for the Jacobian matrix as follows: conditionally on \(\mathcal{A}\),

\[\left\|\mathcal{J}(\tilde{\mathbf{W}})-\mathcal{J}(\mathbf{W})\right\|\leq\frac{2\beta }{\sqrt{h}}\left\|\tilde{\mathbf{W}}-\mathbf{W}\right\|_{F}, \tag{30}\]

for any \(\tilde{\mathbf{W}},\mathbf{W}\in\mathbb{R}^{h\times d}\). Therefore, conditionally on \(\mathcal{A}\), \(\mathcal{J}(\mathbf{W})\) is a \(L\)-Lipschitz function with respect to \(\mathbf{W}\) where \(L:=\frac{2\beta}{\sqrt{h}}\). To complete the proof, it suffices to investigate the smallest singular value of \(\mathcal{J}(\mathbf{W})\) when \(\mathbf{W}\) is in the vicinity of \(\mathbf{W}_{0}\). Recall \(\ell(\mathbf{W})=\|\mathbf{y}-f_{\mathbf{W}}(\mathbf{X})\|\). Notice that for any unit vector \(\mathbf{u}\in\mathbb{R}^{n}\), we have \(\mathbf{u}^{\top}f_{\mathbf{W}_{0}}(\mathbf{X})=\frac{1}{\sqrt{h}}\sum_{i=1}^{h}v_{i} \sigma(\mathbf{w}_{i}^{\top}\mathbf{X}/\sqrt{d})\mathbf{u}\), where \(\mathbf{w}_{i}^{\top}\) is the \(i\)-th row of \(\mathbf{W}_{0}\) for \(i\in[N]\). Consider event \(\mathcal{B}:=\left\{\left\|\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\|\leq C \sqrt{n}\right\}\) for some universal constant \(C>0\). Lemma C.1 proves \(\mathbb{P}\left(\mathcal{B}\right)\geq 1-2e^{-cn}\). By the assumption of \(\mathbf{v}\), we know each entry \(v_{i}\) is a sub-Gaussian random variable with a sub-Gaussian norm at most 1. Then, according to Hoeffding's inequality, conditionally on the event \(\mathcal{B}\), we have

\[\mathbb{P}\left(\left|\frac{1}{\sqrt{h}}\sum_{i=1}^{h}v_{i}\sigma(\mathbf{w}_{i}^{ \top}\mathbf{X}/\sqrt{d})\mathbf{u}\right|\geq t\right)\leq 2\exp\left(-ct^{2} \right),\]

for every \(t\geq 0\) and some constant \(c>0\). Let \(t=2\sqrt{n}\). Considering an \(\frac{1}{4}\)-net \(\mathcal{N}\) of the unit sphere \(\mathbb{S}^{n-1}\), we can get

\[\mathbb{P}\left(\left\|f_{\mathbf{W}_{0}}(\mathbf{X})\right\|\geq\sqrt{n}\right)\leq \mathbb{P}\left(2\max_{\mathbf{u}\in\mathcal{N}}\left|\mathbf{u}^{\top}f_{\mathbf{W}_{0}}( \mathbf{X})\right|\geq\sqrt{n}\right)\leq 9^{n}2\exp\left(-cn\right)\leq 2e^{-c^{ \prime}n}, \tag{31}\]

for some constant \(c^{\prime}>0\). Hence, based on Lemma C.1 and (31), we can obtain \(\ell(\mathbf{W}_{0})\leq C_{0}\sqrt{n}\) with high probability for some universal constant \(C_{0}>0\). Let us denote this event as \(\mathcal{C}:\left\{\ell(\mathbf{W}_{0})\leq C_{0}\sqrt{n}\right\}\). Define \(R:=4\ell(\mathbf{W}_{0})/\alpha\). For any \(\mathbf{W}\) in a ball of radius \(R\) centered at \(\mathbf{W}_{0}\), we have \(\left\|\mathbf{W}_{0}-\mathbf{W}\right\|_{F}\leq R\) and \(\left\|\mathcal{J}(\mathbf{W})-\mathcal{J}(\mathbf{W}_{0})\right\|\leq LR\), conditionally on event \(\mathcal{A}\). Thus, by (30), on event \(\mathcal{A}\cap\mathcal{C}\), the smallest singular value \(\sigma_{\min}(\mathcal{J}(\mathbf{W}))\) of the Jacobian matrix \(\mathcal{J}(\mathbf{W})\) can be bounded by

\[\sigma_{\min}(\mathcal{J}(\mathbf{W})) \geq\ \sigma_{\min}(\mathcal{J}(\mathbf{W}_{0}))-\left\|\mathcal{J}(\mathbf{W })-\mathcal{J}(\mathbf{W}_{0})\right\|\] \[\geq\ 2\alpha-LR\geq 2\alpha-\frac{8\beta}{\alpha}\frac{\ell(\mathbf{W}_{ 0})}{\sqrt{h}}\geq 2\alpha-\frac{8C\beta}{\alpha}\sqrt{\frac{\gamma_{1}}{\gamma_{2}}},\]

Figure 29: The initial and trained spectra (until training loss is less than \(10^{-5}\)) when using GD only for the first layer (\(h=3000,n=2000,d=1000\)): (a) Weight spectra. (b) \(\mathbf{K}^{\text{CK}}\) spectra. (c) \(\mathbf{K}^{\text{NTK}}\) spectra. The final \(R^{2}\) score is 0.55964 and the test loss is 0.44724. The activation is a normalized ReLU, and the target is Sigmoid.

for some universal constant \(C>0\) and sufficiently large \(n,d,h\). Notice that here constants \(C,\beta\), and \(\alpha\) do not rely on \(\gamma_{2}\). Therefore, there exists a sufficiently large \(\gamma^{*}>0\) such that for all \(\gamma_{2}\geq\gamma^{*}\), we have \(2\alpha-\frac{8C\beta}{\alpha}\sqrt{\frac{\gamma_{1}}{\gamma_{2}}}\geq\alpha\). In other words, when \(h\) is sufficiently large but still in the same order as \(n\) and \(d\), for all \(\left\|\mathbf{W}-\mathbf{W}_{0}\right\|_{F}\leq R\), we have \(\sigma_{\min}(\mathcal{J}(\mathbf{W}))\geq\alpha\) conditionally on \(\mathcal{C}\cap\mathcal{A}\). Combining with (29) and (30), conditionally on \(\mathcal{C}\cap\mathcal{A}\), all the assumptions of Theorem 6.10 by [71] are satisfied when \(\left\|\mathbf{W}-\mathbf{W}_{0}\right\|_{F}\leq R\). Therefore, when the learning rate \(\frac{n}{n}\leq\frac{1}{\beta^{2}}\min\left\{1,\frac{4\alpha}{LR}\right\}\), we can get (12)-(14) for all \(t\in\mathbb{N}\), conditionally on \(\mathcal{C}\cap\mathcal{A}\). Both events \(\mathcal{A}\) and \(\mathcal{C}\) occur with high probability and only depend on initialization \(\mathbf{W}_{0}\), \(\mathbf{X}\) and \(\mathbf{y}\). Hence we complete the proof of this theorem. Notice that since \(\gamma_{2}\geq\gamma^{*}\) is sufficiently large, \(\frac{4\alpha}{LR}\geq\frac{\alpha^{2}}{2C\beta}\sqrt{\frac{\gamma_{2}}{\gamma _{1}}}>1\). Therefore, it suffices to require \(\eta\leq n/\beta^{2}\) to conclude that (12), (13) and (14) hold with high probability. This completes the proof. Moreover, (14) further shows that for all \(t\in\mathbb{N}\),

\[\left\|\mathbf{W}_{0}-\mathbf{W}_{t}\right\|_{F}\leq R\leq C\sqrt{n}+o_{d,\mathbb{P}}( 1), \tag{32}\]

where we again apply Lemma C.1 in the following way:

\[\ell(\mathbf{W}_{0})\leq C\sqrt{n}+o_{d,\mathbb{P}}(1),\]

for some constant \(C>0\) only depending on \(\gamma_{1},\gamma_{2},\sigma_{e},\sigma\) and \(\sigma^{*}\). 

As a corollary, (14) controls the deviation of the final step weight from the initial weight. This is empirically shown in Figure 30(a), which shows that \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|\),\(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{F}\), and \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{2,\infty}\) are \(\Theta(1)\) when trainable parameters are convergent. This implies that the final \(\mathbf{W}_{t}\) is still close to the initial weight \(\mathbf{W}_{0}\), even after training. Consequently, with this observation, we can prove Corollary 4.3 in the following.

Proof of Corollary 4.3.: Based on (32), we know \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{o}-\mathbf{W}_{t}\right\|_{F}\leq C_{0}\) holds with high probability for some universal constant \(C_{0}>0\). Conditionally on this event, we can then estimate changes in CK and NTK after training. The method is analogous to Lemma 4.1. For CK, we employ Lemma C.1 and (25) to get

\[\left\|\mathbf{K}_{t}^{\text{CK}}-\mathbf{K}_{0}^{CK}\right\|_{F}\] \[\leq \frac{2}{h}\left(\left\|\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma (\mathbf{W}_{0}\mathbf{X}/\sqrt{d})\right\|+\left\|\sigma(\mathbf{W}_{0}\mathbf{X}/\sqrt{d} \right\|\right)\cdot\left\|\sigma(\mathbf{W}_{t}\mathbf{X}/\sqrt{d})-\sigma(\mathbf{W}_{0 }\mathbf{X}/\sqrt{d})\right\|_{F}\] \[\lesssim \frac{2\lambda_{\sigma}^{2}(1+\sqrt{\gamma_{1}})^{2}}{h}\left\| \mathbf{W}_{0}-\mathbf{W}_{t}\right\|_{F}^{2}+\frac{2C\sqrt{n}\lambda_{\sigma}(1+ \sqrt{\gamma_{1}})}{h}\left\|\mathbf{W}_{0}-\mathbf{W}_{t}\right\|_{F}\] \[\lesssim \frac{2\lambda_{\sigma}(1+\sqrt{\gamma_{1}})C_{0}}{\gamma_{2}^{2} }\left(\lambda_{\sigma}(1+\sqrt{\gamma_{1}})C_{0}+C\sqrt{\gamma_{1}}\right)=O _{d,\mathbb{P}}(1).\]

Figure 30: Measuring the change for the weight, CK, and NTK matrices when training NN with (10). We fix \(d/n=1.2\) and \(h/n=0.6\) when \(n\) is increasing. Here, \(\sigma\) is normalized ReLU and the target is normalized \(\tanh\). The largest \(n=6400\) and the learning rate \(\eta=5.0\) for all training processes. We train each neural network until the training losses approach zero. Each experiment repeats 4 times. In (a), we consider the changes \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|\),\(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{F}\), and \(\frac{1}{\sqrt{d}}\left\|\mathbf{W}_{t}-\mathbf{W}_{0}\right\|_{2,\infty}\).

Hence, this shows control of the change for the CK matrix after training, compared with the initial CK.

Let us denote \(\boldsymbol{w}_{i}^{t}\in\mathbb{R}^{1\times d}\) as the \(i\)-th row of \(\boldsymbol{W}_{t}\), and \(\boldsymbol{x}_{j}\) as the \(j\)-th column of \(\boldsymbol{X}\). Additionally, by Assumption 3.2, we know that

\[\left|\sigma^{\prime}(x)-\sigma^{\prime}(y)\right|\leq\lambda_{\sigma}|x-y|, \tag{33}\]

for any \(x,y\in\mathbb{R}\). For NTK, by modifying (27), one can deduce that

\[\left\|\mathcal{J}(\boldsymbol{W}_{t})-\mathcal{J}(\boldsymbol{W} _{0})\right\|_{F}^{2}=\sum_{i=1}^{h}\left\|\mathcal{J}(\boldsymbol{w}_{i}^{t}) -\mathcal{J}(\boldsymbol{w}_{i}^{0})\right\|_{F}^{2}\] \[\overset{(i)}{\leq} \frac{1}{h}\sum_{i=1}^{h}\left\|\text{diag}\left(\sigma^{\prime}( \boldsymbol{w}_{i}^{t}\boldsymbol{X}/\sqrt{d}-\sigma^{\prime}(\boldsymbol{w}_{ i}^{0}\boldsymbol{X}/\sqrt{d})\right)\right\|_{F}^{2}\left\|\frac{\boldsymbol{X}}{ \sqrt{d}}\right\|^{2}\] \[\overset{(ii)}{\leq} \frac{(1+\sqrt{\gamma_{1}})^{2}}{h}\sum_{i=1}^{h}\left\|\text{ diag}\left(\sigma^{\prime}(\boldsymbol{w}_{i}^{t}\boldsymbol{X}/\sqrt{d}-\sigma^{ \prime}(\boldsymbol{w}_{i}^{0}\boldsymbol{X}/\sqrt{d})\right)\right\|_{F}^{2} +o_{d,\mathbb{P}}(1)\] \[\overset{(iii)}{\leq} \frac{\lambda_{\sigma}^{2}(1+\sqrt{\gamma_{1}})^{2}}{h}\sum_{i=1} ^{h}\sum_{j=1}^{n}\left(\frac{1}{\sqrt{d}}(\boldsymbol{w}_{i}^{t}-\boldsymbol{ w}_{i}^{0})\boldsymbol{x}_{j}\right)+o_{d,\mathbb{P}}(1)\] \[\overset{(iv)}{\leq} \frac{\lambda_{\sigma}^{2}(1+\sqrt{\gamma_{1}})^{4}}{h}\left\| \boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{F}^{2}+o_{d,\mathbb{P}}(1)\leq \frac{\lambda_{\sigma}^{2}(1+\sqrt{\gamma_{1}})^{4}C_{0}^{2}}{\gamma_{2}}+o_{d,\mathbb{P}}(1),\]

where \((i)\) is because of [80, Exercise 6.3.3] and the assumption on \(\boldsymbol{v}\), \((ii)\) is due to (22), \((iii)\) is due to the definition of Frobenius norm and (33), and \((iv)\) is due to [80, Exercise 6.3.3] and (22). As a result, from (26), we can finally conclude that \(\left\|\boldsymbol{K}_{t}^{\text{CK}}-\boldsymbol{K}_{0}^{CK}\right\|_{F}=O_{d,\mathbb{P}}(1)\) as \(n/d\to\gamma_{1}\) and \(h/d\to\gamma_{2}\).

As for the limiting spectra of weight and kernel matrices, since we know that

\[\frac{1}{\sqrt{d}}\left\|\boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{F}, \ \left\|\boldsymbol{K}_{t}^{\text{CK}}-\boldsymbol{K}_{0}^{\text{CK}}\right\|_{ F},\ \left\|\boldsymbol{K}_{t}^{\text{NTK}}-\boldsymbol{K}_{0}^{\text{ NTK}}\right\|_{F}=O_{d,\mathbb{P}}(1),\]

we can automatically apply Corollary A.41 of [8]. This directly implies that the limiting empirical spectra of \(\frac{1}{h}\boldsymbol{W}_{t}^{\top}\boldsymbol{W}_{t}\), \(\boldsymbol{K}_{t}^{\text{CK}}\) and \(\boldsymbol{K}_{t}^{\text{NTK}}\) are the same as the limiting spectra of \(\frac{1}{h}\boldsymbol{W}_{0}^{\top}\boldsymbol{W}_{0}\), \(\boldsymbol{K}_{0}^{\text{CK}}\) and \(\boldsymbol{K}_{0}^{\text{NTK}}\), respectively, as \(n/d\to\gamma_{1}\) and \(h/d\to\gamma_{2}\) (see Figure 29). 

Further Simulations for Changes in Norms.The simulation of Figure 30 empirically coincides with the norm bounds in Theorem 4.2 for different norms. Because of (17), it suffices to only consider the Frobenius norm of the change for each matrix. As a remark, Theorem 4.2 requires \(\gamma_{2}\) to be larger than some threshold \(\gamma^{*}\) to ensure norms of the change throughout training. However, Figure 30 indicates Theorem 4.2 still holds even when \(\gamma^{*}\) is small i.e. the width \(h\) is not very large. Here in Figure 30, \(\gamma_{2}=\frac{1}{2}\gamma_{1}=0.6\). Figure 31 suggests that similar results to Theorem 4.2 and Corollary 4.3 hold for SGD when training the first layer. This is also akin to Figure 21. Moreover, we further conjecture that similar results to Theorem 4.2 and Corollary 4.3 will hold even when training both layers (3). Denote the second layer at step \(t\) by \(\boldsymbol{v}_{t}\). Then, indicated by Figure 32, \(\frac{1}{\sqrt{d}}\left\|\boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{1}\),\(\frac{1}{\sqrt{d}}\left\|\boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{F}\), \(\frac{1}{\sqrt{d}}\left\|\boldsymbol{W}_{t}-\boldsymbol{W}_{0}\right\|_{2, \infty}\), \(\left\|\boldsymbol{K}_{t}^{\text{CK}}-\boldsymbol{K}_{0}^{\text{CK}}\right\| \), \(\left\|\boldsymbol{K}_{t}^{\text{CK}}-\boldsymbol{K}_{0}^{\text{CK}}\right\|_{ F}\), \(\left\|\boldsymbol{K}_{t}^{\text{NTK}}-\boldsymbol{K}_{0}^{\text{NK}}\right\|\), and \(\left\|\boldsymbol{v}_{t}-\boldsymbol{v}_{0}\right\|/\left\|\boldsymbol{v}_{0}\right\|\) are all \(\Theta(1)\) in LWR. Since \(\left\|\boldsymbol{v}_{0}\right\|=O(\sqrt{h})\), we observe that \(\left\|\boldsymbol{v}_{t}-\boldsymbol{v}_{0}\right\|=\Theta(\sqrt{h})\) in this case. Meanwhile, unlike Corollary 4.3, Figure 32(c) cannot verify that \(\left\|\boldsymbol{K}_{t}^{\text{NTK}}-\boldsymbol{K}_{0}^{\text{NTK}}\right\| _{F}\) is still upper bounded by a constant. One possible explanation is that in this case the change in the second layer \(\boldsymbol{v}_{t}\) is much more significant than in the first-layer weight \(\boldsymbol{W}_{t}\), hence the NTK matrix may change a lot in this general case.

Similarly, Figure 33 shows norms of the change when training the NN with SGD for both layers. Here we use the same batch size 128 and learning rate \(\eta=1.0\) for all experiments when varying the dimension \(n\) but fixing the aspect ratios \(\gamma_{1}\) and \(\gamma_{2}\). All the observations are similar to the results of Corollary 4.3 except the Frobenius norm of the change for NTK. Figure 33(c) indicates that \(\left\|\boldsymbol{K}_{t}^{\text{NTK}}-\boldsymbol{K}_{0}^{\text{NTK}}\right\| _{F}\) will not be \(\Theta(1)\) anymore, which fluctuates more in this case.

Figure 31: Measuring the change for weight \(\mathbf{W}_{t}\), \(\mathbf{K}^{\text{CK}}\), and \(\mathbf{K}^{\text{NTK}}\) matrices when training NN with (10) for the first layer \(\mathbf{W}_{t}\) using SGD with batch size 200. We fix \(d/n=0.6\) and \(h/n=1.2\) when \(n\) is increasing. Here \(\sigma\) is normalized softplus and the target is normalized \(\tanh\). The largest \(n\) is \(11000\), \(\sigma_{e}=0.3\) and the learning rate is \(\eta=3.6\) for all training processes. We train each neural network until the training losses approach zero. Each experiment repeats 15 times.

Figure 32: Measuring the change for \(\mathbf{W}_{t}\), \(\mathbf{K}^{\text{CK}}\), the second layer \(\mathbf{v}_{t}\) and \(\mathbf{K}^{\text{NTK}}\) when training NN with (3) for both layer \(\mathbf{W}_{t}\) and \(\mathbf{v}_{t}\) using GD. We fix \(d/n=0.5\) and \(h/n=0.8\) when \(n\) is increasing. Here, \(\sigma\) is normalized softplus and the target is normalized \(\tanh\). The largest \(n\) is \(11000\), \(\sigma_{e}=0.3\) and the learning rate is \(\eta=3.6\) for all training processes. We train each neural network until the training losses approach zero. Each experiment repeats 15 times.

Figure 33: Measuring the change for \(\mathbf{W}_{t}\), \(\mathbf{K}^{\text{CK}}\), the second layer \(\mathbf{v}_{t}\) and \(\mathbf{K}^{\text{NTK}}\) when training NN with SGD for both layers \(\mathbf{W}_{t}\) and \(\mathbf{v}_{t}\). We fix \(d/n=0.6\) and \(h/n=1.2\) when \(n\) is increasing. Here, \(\sigma\) is normalized ReLU and the target is normalized \(\tanh\). \(\sigma_{e}=0.1\) and the learning rate is \(\eta=1.0\) for all training processes. We train each neural network until the training losses approach zero. Each experiment repeats 12 times.