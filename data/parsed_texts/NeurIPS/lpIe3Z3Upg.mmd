# Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs

 Xuandong Zhao

UC Berkeley

xuandongzhao@berkeley.edu

&Lei Li

Carnegie Mellon University

leili@cs.cmu.edu

&Yu-Xiang Wang

UC San Diego

yuxiangw@ucsd.edu

###### Abstract

In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys stability properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-stability tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson (2023)'s Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and its Gumbel watermarked counterpart) in terms of perplexity, while retaining the same stability (and detectability), hence making it a promising new approach for LLM decoding.

## 1 Introduction

Large language models (LLMs) (OpenAI, 2022, 2023, Bai et al., 2022, Touvron et al., 2023) have become increasingly popular in recent years due to their ability to generate human-like text and solve many tasks through a natural chatbot interface.

A language model predicts the next word in a sentence using a real-value function \(u(\cdot;\text{prompt},\text{prefix}):\mathcal{V}\rightarrow\mathbb{R}\), known as _logits_, which encodes the model's preferences on which word to choose. Here \(\mathcal{V}\) is the vocabulary space (typically a large discrete set of words); the "prompt" describes the task of interest; and "prefix" includes all preceding words that have been generated so far. A language model _decoder_ refers to a possibly randomized function that takes a prompt text \(x\), API access to the _logits_ function as input, and outputs a sentence \(y_{1:n}\).

**The main thrust of this paper** is to introduce a new decoder, termed _Permute-and-Flip decoding_, work out some of its intriguing properties with an application to watermarking LLM text, and hopefully convince readers that it deserves a shot at your next LLM application.

## 2 Problem Setup and Summary of Results

Before diving in, let's set up the stage with a quick tour of existing decoding methods and have a brief look into how a language model decoder can be evaluated.

**Popular existing decoding methods**: fall into three categories: (1) Planning-based methods such as beam search that aims at maximizing the sequence likelihood; (2) sampling-based methods that recursively sample from the next-word distribution, e.g., the soft(arg)max transform of the logits

\[\text{\bf Softmax sampling:}\,y_{t}\sim p(y)=\frac{e^{u(y|x,y_{1:t-1})/T}}{ \sum_{\vec{y}}e^{u(\vec{y}|x,y_{1:t-1})/T}}\] (1)where \(T\) is the _temperature_ parameter; and (3) greedy methods such as greedy decoding that simply outputs \(y_{t}=\arg\max_{y\in\mathcal{V}}u(y|x,y_{1:t-1})\) as well as its Top \(p\)[Holtzman et al., 2019] and Top \(k\) sampling variants that interpolate greedy and sampling methods.

Performance metrics.How do we compare different decoding methods? More generally, how do we evaluate LLM-generated text? These are questions far from being settled. Naturally, if there is a (possibly task-dependent) performance metric \(U_{x}:\mathcal{V}^{n}\rightarrow\mathbb{R}\) one can define, then the optimal decoder would be the one that outputs \(y_{1:n}^{*}=\arg\max_{y_{1:n}\in\mathcal{V}^{n}}U_{x}(y_{1:n})\). Often \(U_{x}\) is instantiated to be the sequence likelihood \(\sum_{t=1}^{n}\log p(y_{t}|x,y_{1:t-1})\) which is equal to \(\sum_{t=1}^{n}u_{t}(y_{t})\).

Recent works [Ippolito et al., 2019, Winer et al., 2022], however, report that strategies that aim at maximizing sequence likelihood often result in texts that are more repetitive and less effective in some downstream tasks than those from the sampling-based methods [Holtzman et al., 2019]. Depending on what the task is, there is not a one-size-fits-all performance metric, therefore is no single decoding method that works well for all tasks.

For the moment, let us stash the disputes on how to best evaluate an LLM-generated text and focus on designing methods that maximize any user-specified utility function. In fact, we will also give up on solving the sequence-level utility maximization problem1 and simply maximize a _per-step_ utility function \(u_{t}:\mathcal{V}\rightarrow\mathbb{R}\).

Footnote 1: It is known to be NP-Complete [Chen et al., 2017].

\(u_{t}\) can simply be the logits function that LLMs output, which may have already accounted for potential future utility (like the \(Q\) function in reinforcement learning) since the transformer-based language model had access to future texts during pre-training. Or \(u_{t}\) can be explicitly augmented with structure-inducing regularizers such as a lookahead heuristic as in A* decoding [Lu et al., 2021], a retrieval-based term for fact-checking [Lewis et al., 2020], or an entropy bonus for promoting diversity [Meister et al., 2020].

Our goal is thus to construct a possibly randomized algorithm \(\mathcal{A}\) that takes \(u_{t}\) as an input and outputs \(y_{t}\in\mathcal{V}\) that aims at maximizing \(\mathbb{E}_{y_{t}\sim\mathcal{A}_{u_{t}}}[u_{t}(y_{t})]\) as much as possible. In the remainder of the paper, we will simply take \(u_{t}\) as "logits" for a concrete exposition -- all results are valid when \(u_{t}\) is instantiated otherwise.

Other constraints / consideration.Why doesn't the trivial greedy decoder work? That's because there are other considerations besides text quality when selecting LLM decoders. For example, **computational efficiency and latency** are hugely important, since each API call to the _logits_ function is costly. The **diversity** of the generated text is also important, especially for creative tasks.

Moreover, the decoding procedure should be **watermarkable**[Aaronson, 2023, Kirchenbauer et al., 2023, Zhao et al., 2023, Kuditipudi et al., 2023] in the sense that one should be able to inject subtle statistical signals that can be retrieved when given a secret key, to _prove_ that the text is generated by this particular language model. Being watermarkable prevents the LLM from being used for malicious purposes such as scams [Weidinger et al., 2021], fake news [Zellers et al., 2019], and plagiarism [Stokel-Walker, 2022].

In addition to the above, one may also hope the decoding algorithm to be **stable against small perturbations** to the _logits_. Specifically,

**Definition 2.1** (Stability).: We say a decoding algorithm \(\mathcal{A}\) is \(L\)-stable if for any prompt \(x\), prefix \(y_{\leq t}\), and for any perturbed \(\tilde{u}\) such that \(\|\tilde{u}-u\|_{\infty}\leq\delta\), the log-probability ratio satisfies

\[\left|\log\left\{\frac{p_{\mathcal{A}(\tilde{u}(\cdot|x,y_{\leq t}))}(y)}{p_{ \mathcal{A}(u(\cdot|x,y_{\leq t}))}(y)}\right\}\right|\leq L\delta\ \ \forall y\in \mathcal{V}.\]

The stability helps to avoid catastrophic failure in the scenarios where the logits may be subject to data poisoning [Zhang et al., 2021, Lin et al., 2021] or jailbreaking attacks [Zhang et al., 2023, Zhao et al., 2024b]. Furthermore, _stability_ implies an intuitive notion of _diversity_, suggesting that tokens with similar logits should have comparable probabilities of being selected. For further discussion and examples, please refer to Appendix C.

Inspecting the decoding methods along the aforementioned dimensions, we notice that planning-based methods fail to be computationally efficient. While greedy decoding is efficient and has relatively low perplexity, its generated texts are neither diverse nor watermarkable (at least not using existing techniques). The sampling-based methods, however, are both watermarkable and diverse. In addition, softmax sampling is known to be \(2\)-stable, while the others that we have discussed are not stable.

**Fact 2.2**.: _Softmax sampling decoding using (1) with temperature \(T\) satisfies \((2/T)\)-stability._

Proof.: The result is implied by the differential privacy guarantee of exponential mechanism (McSherry and Talwar, 2007, Theorem 6). 

The pros and cons of different decoding methods are summarized in Table 1. From the table, we can see that there is a clear tradeoff between minimizing perplexity and preserving other properties. In particular, softmax sampling is the only method that checks all boxes, and the only one that is stable among existing decoders. This observation begs the following research question: _Is there a decoding method that is as stable as softmax sampling, but has lower perplexity?_

In this paper, we answer this question affirmatively by bringing in a technique called Permute-and-Flip sampling. Our contributions are fourfold.

1. We introduce Permute-and-Flip decoding -- a new decoding algorithm for language models based on recent development in a very different context (McKenna and Sheldon, 2020).
2. We demonstrate that existing results from McKenna and Sheldon (2020) already imply that:

* Permute-and-Flip decoding is provably stable.
* The stability-perplexity tradeoff of the PF decoding is Pareto-optimal. In particular, when compared to softmax sampling, PF decoding has up to 2x smaller expected suboptimality while having the same stability parameter \(L\).
3. We designed an analog of Aaronson (2023)'s Gumbel-Watermark for PF decoder, called the PF watermark. We show that the watermarked PF decoder samples from a distribution that is computationally indistinguishable from the non-watermarked PF decoder, and the detection procedure has precisely controlled false positive rate (FPR) and high power in identifying watermarked texts.
4. We empirically demonstrate that on open-generation tasks, PF watermark achieves the best balance of the highest detection accuracy and lowest perplexity compared to the baselines.

Overall, our proposed permute-and-flip decoding method provides a promising approach to balancing the tradeoff between perplexity and stability in LLM decoding while also admitting watermarking capabilities.

Related work and novelty.PF sampling was invented in the differential privacy (DP) literature (McKenna and Sheldon, 2020). Its stability properties are well-understood. The stability of Softmax sampling is also well-known (McSherry and Talwar, 2007). Our contribution is in applying this method to LLM decoding and connecting these known theoretical results to the broader ML audience. To our knowledge, the PF watermark is new to this paper. The design of the watermark leverages the Report-Noisy-Max interpretation of the PF sampling (Ding et al., 2021) which allows a similar pseudo-random function like the work of Aaronson (2023) to be applied. A more thorough discussion of the related work is given in Appendix A.

## 3 Permute-and-Flip Decoding its Properties

The Permute-and-Flip decoding iteratively generates the next token by a simple procedure that uses only the logits. It involves first randomly permuting the vocabulary, then flipping a sequence of biased coins according to the permuted sequence until the first "head" is seen (see Algorithm 1).

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Methods & Perplexity & Computational Efficiency & Diversity & Watermark & Stability \\ \hline Search (e.g., Beam) & Lowest & ✗ & ✗ & ✗ \\ Greedy & Low & ✓ & ✗ & ✗ & ✗ \\ Softmax Sampling & Moderate & ✓ & ✓ & ✓ & ✓ \\ Top-\(p\) Sampling & Low (for small \(p\)) & ✓ & Depends on \(p\) & ✓ & ✗ \\ Top-\(k\) Sampling & Low (for small \(k\)) & ✓ & Depends on \(k\) & ✓ & ✗ \\ \hline PF Sampling (ours) & Lower than Softmax & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different decoding methods against five desiderata.

ducing \(y^{*}\).

PF sampling was initially proposed in McKenna and Sheldon (2020) as a differentially private selection mechanism that has better utility than the more well-known exponential mechanism (McSherry and Talwar, 2007). McKenna and Sheldon (2020) also derived a plethora of theoretical properties of the PF sampling. The following theorem summarizes these results in the language of LLM decoding.

**Theorem 3.1**.: _Let the logits function be \(u\) and \(u^{*}=\max_{y\in\mathcal{V}}u(y)\). Let \(\mathrm{PF}(u)\) be the distribution of PF-sampling, and \(\mathrm{Softmax}(u)\) be the distribution in (1), both with temperature parameter \(T\). The following statements are true._

1. _(_**Same stability**_) PF-Sampling is_ \((2/T)\)_-stable._
2. _(_**Nearly greedy**_) PF-sampling obeys,_ \(\mathbb{E}_{y\sim\mathrm{PF}(u)}[u(y)]\geq u^{*}-T\log|\mathcal{V}|\)_._
3. _("_**Never worse"**_) For the same_ \(T\)_, PF-sampling is never worse than Softmax-sampling._ \[\operatorname*{\mathbb{E}}_{y\sim\mathrm{PF}(u)}[u(y)]\geq\operatorname*{ \mathbb{E}}_{y\sim\mathrm{Softmax}(u)}[u(y)]\]
4. _("_**Up to 2x better"**_) There exists logits_ \(u\) _such that PF-sampling is 2x smaller in terms of suboptimality,_ \(\mathbb{E}_{y\sim\mathrm{PF}(u)}[u^{*}-u(y)]\leq\frac{1}{2}\,\mathbb{E}_{y\sim \mathrm{Softmax}(u)}[u^{*}-u(y)]\)_._
5. _(_**Optimal stability-perplexity tradeoff**_) For any decoder_ \(P\) _that is_ \(2/T\)_-stable, if there exists_ \(u\) _such that_ \(\mathbb{E}_{y\sim P(u)}[u(y)]>\mathbb{E}_{y\sim\mathrm{PF}(u)}[u(y)]\) _then there must be another_ \(\bar{u}\) _such that_ \(\mathbb{E}_{y\sim P(\bar{u})}[\bar{u}(y)]<\mathbb{E}_{y\sim\mathrm{PF}(\bar{u })}[\bar{u}(y)]\)_._

Proof.: The theorem follows directly from McKenna and Sheldon (2020), specifically Theorem 1, Corollary 1, Theorem 2, Proposition 4, and Proposition 6. 

The first statement shows that the PF decoder enjoys exactly the same stability parameter as in Fact 2.2. The second statement provides a worst-case bound on how far PF-sampling is away from greedy-decoding as a function of the temperature \(T\) in terms of the likelihood achieved. The third and fourth statements show that PF-sampling is always "more greedy" than softmax-sampling. The last statement shows that PF-sampling is not dominated by any other decoder that is equally stable (as in Definition 2.1), thus _Pareto optimal_. These results provide strong justification on the superiority of the permute-and-flip decoder over the standard softmax sampling in minimizing perplexity.

Let's consider a simple example to compare PF decoder and Softmax decoder.

**Example 3.2**.: Let the \(|\mathcal{V}|=2\) and the corresponding logits be \([\Delta,0]\) for gap \(\Delta>0\). Softmax decoder chooses the suboptimal token with probability \(1/(1+e^{\Delta/T})\), while PF decoder chooses it w.p. \(1/(2e^{\Delta/T})\).

Since \(1/(1+x)>1/(2x)\) for all \(x>1\), the probability that the suboptimal token is chosen in PF sampling is strictly smaller than that of Softmax sampling. As shown in Figure 1, the left, we fix the Gap

Figure 1: Comparing PF decoder vs Softmax decoder using Example 3.2 shown in Figure 1, the left, we fix the Gap

\(\Delta=3.0\) and vary the temperature \(T\). On the right, we fix \(T=1.0\) and vary \(\Delta\). PF beats Softmax in all cases.

## 4 Report-Noisy-Max and Watermarking

Next, we turn to the well-motivated problem of watermarking LLM generated text. The watermarking problem aims at embedding a secret message in the generated text that (essentially) reads "Beware! I am written by an AI!". The hope is that this message can be seen by anyone who has access to a _secret key_, while ensuring that the watermarked version of the LLM generates text that has _almost the same_ distribution as (or at least very similar) to the original LLM.

More formally, a watermarking scheme includes a "Watermark" function that injects the watermark and a "Detect" function that takes a suspect text sequence \(y_{1:n}\) as input and outputs a prediction of \(1\) ("It is watermarked!") or \(0\) ("It is not!"). A wrong accusation of non-watermarked text as watermarked is called a _false positive_. A failure to detect a watermarked text is called a _false negative_. The performance of a watermark is measured by its detection _power_ (i.e., \(1\)_-false negative rate_) at a given _false positive rate_.

There are many other necessary properties for a watermarking scheme to be useful, such as _low-overhead_, _model-agnostic_ detection, and _resilience to edits_ and other evasion attacks. We refer readers to the slide deck of Aaronson (2023) and the related work section of (Zhao et al., 2024) for a review of these desiderata and known results. Among the recent attempts, two popular watermarking schemes perform satisfactorily on all the above criteria.

**Gumbel Watermark Aaronson, 2023**: that uses a "traceable" pseudo-random softmax sampling when generating the next word.
**Green-Red Watermark Kirchenbauer et al., 2023**: that randomly splits the vocabulary into Green and Red then slightly increases the logits for green tokens.

Both of them determine their pseudo-random seeds chosen according to the \(m\) preceding tokens of the current token being generated. We will focus on explaining the Gumbel watermark as it is more closely related to our approach.

**Aaronson (2023)'s Gumbel watermark.** The key idea of the Gumbel watermark leverages the "Gumbel-Max Trick", which states that:

**Fact 4.1**(Gumbel, 1948).: _The softmax sampling in (1) is equivalent to the following procedure_

\[y_{t}=\operatorname*{arg\,max}_{y\in\mathcal{V}}\frac{u_{t}(y)}{T}+G_{t}(y)\] (2)

_where \(G_{t}(y)\sim\text{Gumbel}(0,1)\) i.i.d for each \(t,y\)._

Gumbel noise can be generated using a uniform r.v.

\[\text{Gumbel}(0,1)\sim-\log\left(\log(1/\mathrm{Uniform}([0,1]))\right).\]

So given a random vector \(r_{t}\sim(\mathrm{Uniform}([0,1]))^{|\mathcal{V}|}\), we can write \(G_{t}(y)=-\log(-\log(r_{t}(y)))\).

The Watermark stage for the Gumbel-watermark essentially replaces \(\mathrm{Uniform}([0,1])\) with a _pseudo-random function_\(r_{t}(y)=F_{y_{t-m:t-1},\mathsf{k}}(y)\). Given the secret key \(\mathsf{k}\), the _pseudo-random function_ is a deterministic function with range \([0,1]^{\mathcal{V}}\), but over the distribution of the secret key \(\mathsf{k}\), \(r_{t}\) is computationally indistinguishable from sampled from truly i.i.d. uniform distribution, which ensures that the distribution of \(y_{t}\) in the watermarked model is computationally indistinguishable to the unwatermarked distribution (1).

At Detect phase of the the Gumbel watermark, the auditor who has access to the key \(\mathsf{k}\) may compute

\[\mathrm{TestScore}_{\mathrm{Gumbel}}(y_{1:n})=\sum_{t=m+1}^{n}-\log(1-r_{t}(y_{ t})).\]

If \(y_{1:n}\) is _not_ generated from the watermarked model, then the test statistic is a sum of exponential random variable thus \(\mathbb{E}[\mathrm{TestScore}(y_{1:n})]=n-m\). Meanwhile, if \(y_{1:n}\) is generated by the Gumbel watermarked model, \(\mathbb{E}[\mathrm{TestScore}(y_{1:n})]=\sum_{t=m+1}^{n}\mathbb{E}\left[\sum_ {y\in\mathcal{V}}p_{t}(y)H_{\frac{1}{p_{t}(y)}}\right]\) (3)\((n-m)+\left(\frac{\pi^{2}}{6}-1\right)\sum_{t=m+1}^{n}\mathbb{E}\left[\text{ Entropy}[p_{t}(\cdot)]\right],\) where \(p_{t}:=\mathrm{Softmax}(u_{t})\), \(H_{\alpha}:=\int_{0}^{\alpha}\frac{1-x^{n}}{1-x}dx\) is Euler's Harmonic number and Entropy denotes the standard Shannon entropy (in nats) for a discrete distribution, i.e., \(\text{Entropy}[p]=-\sum_{y\in\mathcal{V}}p(y)\log p(y)\).

Permute-and-Flip as ReportNoisyMax.It turns out that the Permute-and-Flip sampling has a similar equivalent Report-Noisy-Max form. Instead of Gumbel noise, it is the exponential noise that are added to the logits. This less-known fact is due to Ding et al. (2021)

**Fact 4.2** (Ding et al., 2021, Theorem 5).: _Permute-and-Flip Sampling in Algorithm 1 with parameter \(T\) is equivalent to_

\[y_{t}=\operatorname*{arg\,max}_{y\in\mathcal{V}}\frac{u_{t}(y)}{T}+E_{t}(y).\] (4)

_where \(E_{t}(y)\sim\text{Exponential}(1)\) i.i.d. for each \(t,y\)._

Leveraging this fact, in the remainder of the section, we develop a watermarking scheme for Report-NoisyMax that is analogous to the Gumbel-watermark.

Permute-and-Flip watermark.The natural idea is to replace the exponential noise \(E_{t}(y)\) with a pseudo-random version that depends on a secret key and a prefix with length \(m\). Observe that \(\mathrm{Exponential}(1)\sim-\log(\mathrm{Uniform}([0,1]))\), thus the standard pseudo-random function that generates uniform random variables can be used. In the detection phase, we compute:

\[\mathrm{TestScore}_{\mathrm{PF}}(y_{1:n})=\sum_{t=m+1}^{n}-\log(r_{t}(y_{t})).\]

Note that this is a simple change of sign of \(r_{t}(y_{t})\) comparing to the test score of the Gumbel watermark. Detailed pseudo-code for how the watermark works are given in Algorithm 2 and Algorithm 3.

```
1:Preparation: Randomly sample a watermark key k
2:Input: Prompt \(x\), language model \(\mathcal{M}\), pseudo-random function \(F\), watermark key k, temperature \(T\)
3:for\(t=1,2,\cdots\)do
4: Compute logits: \(u_{t}\leftarrow\mathcal{M}([x,y_{1:t-1}])\)
5: Generate a pseudo-random vector \(r_{t}(\cdot)\) using \(r_{t}(y):=F_{y_{t},m,t-1}\lambda(y)\) for \(y\in\mathcal{V}\)
6: Select the next token \(y_{t}\) using \[y_{t}=\operatorname*{arg\,max}_{y\in\mathcal{V}}\left(\frac{u_{t}(y)}{T}-\log r _{t}(y)\right)\] (5)
7:endfor
8:Output: Watermarked sequence \(y=[y_{1},...,y_{n}]\) ```

**Algorithm 2** PF watermarking: Watermark

**Theorem 4.3**.: _Assume the pseudo-randomness is perfect2, i.e., \(F_{w_{1:m},\mathsf{k}}(y)\sim\mathrm{Uniform}([0,1])\) i.i.d. \(\forall[w_{1:m},y]\in\mathcal{V}^{m+1}\)._

Footnote 2: This is a simplifying assumption. We only need \((n-m)|\mathcal{V}|\)-way independence.

_The following are true about PF watermark scheme._

1. _If_ \(y_{1:n}\) _is_ statistically independent _to the secret key_ \(\mathsf{k}\)_,_ \(\mathbb{E}\left[\mathrm{TestScore}_{\mathrm{PF}}(y_{1:n})|y_{1:n}\right]=n-m\)_._
2. _If in addition, all_ \(m\)_-grams in_ \(y_{1:n}\) _are unique, then conditioning on_ \(y_{1:n}\)_,_ \(\mathrm{TestScore}_{\mathrm{PF}}(y_{1:n})\sim\mathrm{Gamma}(n-m,1)\)_. The choice_ \(\tau=\mathrm{CDF}_{\mathrm{Gamma}(n-m,1)}^{-1}(1-\alpha)\) _ensures the false positive rate in Algorithm_ 3 _is equal to_ \(\alpha\)_._
3. _Assume_ \(y_{1:n}\) _is drawn from Algorithm_ 2_, then_The above expression in (7) may appear messy, but it is the exact calculation and captures the entropy of the distribution PF-induces for a given \(u_{t}\). To see this, let us instantiate the bound for two special cases that admit more explicit forms. We show that in Appendix B.

In conclusion, we showed that the watermarked version of PF-decoder is computationally indistinguishable from the original version of PF-decoder. Meanwhile, the test score of the PF watermark is qualitatively similar to that of the Gumbel-watermark (and identical in some cases). It is likely to produce similar detectability to the Gumbel watermark, while enjoying the performance boost that comes from replacing softmax sampling with PF.

## 5 Experiments

**Datasets and models.** We utilize two long-form text datasets in our experiments: the Colossal Clean Crawled Corpus (C4) dataset (Raffel et al., 2020) for open-ended text completion generation, and the Alpaca dataset (Taori et al., 2023) for question-answering tasks. Our primary language model is the state-of-the-art open-source model Llama-2 with 7 billion parameters. Specifically, we use the Llama-2-7B-chat model for question-answering tasks on the Alpaca dataset. For text completion tasks on the C4 dataset, we employ the base model Llama-2-7B. Furthermore, to evaluate the universal applicability of smaller models, we also assess the performance of the TinyLlama-1.1B model3(Zhang et al., 2024).

Footnote 3: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0

**Evaluation metrics.** We calculate perplexity scores from different models, using Llama2-7B to compute PPL1 and Llama2-13B to compute PPL2. We also compute MAUVE scores to measure the distributional similarity between model generations and human text as another metric for text quality (Pillutla et al., 2021). To evaluate repeitiveness, we compute seq-rep-5 across generations, which is the average repetition rate of duplicate 5-grams in a sequence (Welleck et al., 2019). For the watermark evaluation, maintaining a low false positive rate is crucial to avoid misclassifying unwatermarked text as watermarked. Therefore, we set the false positive rates at 1% and 10% for all watermark detection algorithms, adjusting the detection threshold accordingly. We report true positive rate (TPR) and F1 scores to measure the watermark detectability. We compared the well-known Gumbel Watermark (Gumbel WM) and Green-Red Watermark (KGW WM) as our main baselines. Experiments were conducted using Nvidia A600 GPUs. For the details of the experiment setting, please refer to the Appendix D.

**Text generation performance.** Table 2 shows the text perplexity of generated samples from different LLMs evaluated on two datasets. Using the same temperature, we find that PF decoding produces significantly lower perplexity compared to sampling. Although greedy decoding has the lowest perplexity, it suffers from heavy repetition, as indicated by its high seq-rep-5 score and low MAUVE score. We observe that for question-answering tasks, the perplexity is lower, likely due to the fixed form of answers and lower entropy of the text generation. Table 8 shows an example prompt and responses generated by different decoding methods.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Method** & **AUC1** & **TPR1** & **PPL2** & **Seq-rep-5** & **MAUVE** & **Method** & **AUC1** & **TPR1** & **PPL1** & **Seq-rep-5** & **MAUVE** \\ \hline
**C4, T-10, Llama2-7B** & & & & & **C4, T-0, Llama2-7B** & & & & & & \\ Greedy & - & - & 1.14\({}_{0.1}\) & 1.24\({}_{0.01}\) & 0.56 & 0.05 & Greedy & - & - & 1.28\({}_{0.16}\) & 1.75\({}_{0.03}\) & 0.12 & 0.93 \\ Sampling & - & 1.24\({}_{0.21}\) & 1.5\({}_{0.31}\) & 0.02 & 0.98 & Sampling & - & - & 2.43\({}_{0.48}\) & 4.91\({}_{0.03}\) & 0.06 & 1.00 \\ PF & - & - & 8.9\({}_{0.42}\) & 1.75\({}_{0.23}\) & 0.03 & 0.90 & PF & - & 3.54\({}_{0.46}\) & 4.11\({}_{0.08}\) & 0.10 & 0.92 \\ KGW WM & 0.989 & 0.991 & 1.66\({}_{0.38}\) & 2.06\({}_{0.20}\) & 0.61 & 1.00 & KGW WM & 0.995 & 0.991 & 5.73\({}_{0.68}\) & 6.71\({}_{0.11}\) & 0.03 & 0.99 \\ Gumbel WM & 0.997 & 0.988 & 11.4\({}_{12.36}\) & 1.24\({}_{12.36}\) & 0.04 & 0.93 & Gumbel WM & 0.993 & 0.982 & 4.03\({}_{0.47}\) & 4.17\({}_{0.10}\) & 1.00 & 1.00 \\ PF WM & 0.995 & 0.984 & 8.3\({}_{3.20}\) & 10.28\({}_{0.29}\) & 0.05 & 0.99 & PF WM & 0.993 & 0.980 & 3.38\({}_{0.37}\) & 3.99\({}_{0.10}\) & 0.13 & 1.00 \\ \hline
**Alpaca, T-10, Llama2-7B** & & & & & **Alpaca, T-10, TinyLlama-1.1B-Chat** & & & & & & & \\ Greedy & - & 1.28\({}_{0.02}\) & 1.75\({}_{0.03}\) & 0.12 & 0.93 & Greedy & - & - & 1.41\({}_{0.14}\) & 1.66\({}_{0.02}\) & 0.30 & 0.99 \\ Sampling & - & - & 1.74\({}_{0.02}\) & 2.41\({}_{0.04}\) & 0

**Watermarking results.** We compare the results of our proposed PF watermarking method with those of the Gumbel Watermark (Gumbel WM) and the Green-Red watermark (KGW WM). In Figure 1(a), we present the distribution of detection scores for the PF watermark. The PF watermark demonstrates clear detectability between positive and negative samples. The results of the watermark generation are shown in Table 2 and Figure 1(b). The PF watermark achieves the best balance of the highest detection accuracy and lowest perplexity, compared to the KGW WM and the Gumbel WM. Notably, the perplexity of the PF watermark is close to that of the PF sampling, indicating that the watermarking process does not significantly impact the quality of the generated text. All watermarking methods achieved near-perfect detection accuracy on the C4 dataset. Besides, the detection results for the small TinyLlama model are also good, demonstrating the universal applicability of the PF watermark.

**Controlling the false positive rate.** The key strength of PF watermark is its ability to precisely control the false positive rate (FPR) during detection. We validate this by conducting experiments using negative examples from diverse datasets (C4, Alpaca, unwarmarked) and different random keys. As Figure 3 shows, the empirical false positive rates align tightly with the theoretical \(\alpha\) values across different settings. This demonstrates PF watermark's effectiveness in controlling the FPR as intended.

**Additional watermarking results.** For a text watermarking design to be effective, it should be able to withstand paraphrasing attacks that an adversary may attempt to modify the watermarked text. Furthermore, the watermark should be detectable even with shorter text lengths. In Appendices D.1.1 and D.2.2, we present additional empirical results for the PF watermark, demonstrating its robustness to paraphrasing and editing attacks. The results also show that the PF watermark can still be detected even when the length of the text is reduced to only 30 tokens.

## 6 Conclusion

We introduce Permute-and-Flip (PF) decoding, a new decoding method for large language models that enjoys the same - perturbation-stability guarantees as softmax sampling while achieving substantially lower perplexity. We design a tailored watermarking scheme (PF watermark) for PF decoding that enables precise control over false positive rates while retaining high true positive rates. Our experiments demonstrate that the PF watermark achieves the best balance of the highest detection accuracy and lowest perplexity. All these intriguing properties make PF decoding a promising new approach for practical applications of large language models.

Figure 3: Comparison of empirical and theoretical false positive rates with different watermark keys. We can see that the second statement of Theorem 4.3 correctly controls the Type I error in practice.

Figure 2: Comparison of PF and Gumbel watermarks on real data.

## References

* Aaronson (2023) Scott Aaronson. Simons institute talk on watermarking of large language models, 2023. URL https://simons.berkeley.edu/talks/scott-aaronson-ut-austin-openai-2023-08-17.
* Anderson et al. (2016) Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary image captioning with constrained beam search. _arXiv preprint arXiv:1612.00576_, 2016.
* Atallah et al. (2001) Mikhail J. Atallah, Victor Raskin, Michael Crogan, Christian F. Hempelmann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. Natural language watermarking: Design, analysis, and a proof-of-concept implementation. In _Information Hiding_, 2001.
* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* Bao et al. (2023) Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. _ArXiv_, abs/2310.05130, 2023.
* Chen et al. (2023a) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. _arXiv preprint arXiv:2302.01318_, 2023a.
* Chen et al. (2017) Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. _ArXiv_, abs/1711.05408, 2017. URL https://api.semanticscholar.org/CorpusID:3666178.
* Chen et al. (2023b) Yutian Chen, Hao Kang, Vivian Zhai, Liang Li, Rita Singh, and Bhiksha Ramakrishnan. Gpt-sentinel: Distinguishing human and chatgpt generated content. _ArXiv_, abs/2305.07969, 2023b.
* Christ et al. (2023) Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. _arXiv preprint arXiv:2306.09194_, 2023.
* Ding et al. (2021) Zeyu Ding, Daniel Kifer, Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang, et al. The permute-and-flip mechanism is identical to report-noisy-max with exponential noise. _arXiv preprint arXiv:2105.07260_, 2021.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography_, pages 265-284. Springer, 2006.
* Fernandez et al. (2023) Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. _2023 IEEE International Workshop on Information Forensics and Security (WIFS)_, pages 1-6, 2023.
* GPTZero (2023) GPTZero. Gptzero: More than an ai detector preserve what's human. _GPTZero website_, 2023. URL https://gptzero.me/.
* Gumbel (1948) Emil Julius Gumbel. _Statistical theory of extreme values and some practical applications: a series of lectures_, volume 33. US Government Printing Office, 1948.
* Hokamp and Liu (2017) Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid beam search. _arXiv preprint arXiv:1704.07138_, 2017.
* Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In _International Conference on Learning Representations_, 2019.
* Hu et al. (2023) Xiaobing Hu, Pin-Yu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. _ArXiv_, abs/2307.03838, 2023a.
* Hu et al. (2023) Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. _ArXiv_, abs/2310.10669, 2023b.
* Ippolito et al. (2019) Daphne Ippolito, Reno Kriz, Joao Sedoc, Maria Kustikova, and Chris Callison-Burch. Comparison of diverse decoding methods from conditional language models. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3752-3762, 2019.
* Ippolito et al. (2019)John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _International Conference on Machine Learning_, 2023.
* Krishna et al. (2023) Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _ArXiv_, abs/2303.13408, 2023.
* Kuditipudi et al. (2023) Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. _ArXiv_, abs/2307.15593, 2023.
* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* Li et al. (2022) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. _arXiv preprint arXiv:2210.15097_, 2022.
* Liang et al. (2023) Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Y. Zou. Gpt detectors are biased against non-native english writers. _ArXiv_, abs/2304.02819, 2023.
* Lin et al. (2021) Jing Lin, Long Dang, Mohamed Rahouti, and Kaiqi Xiong. Ml attack models: adversarial attacks and data poisoning attacks. _arXiv preprint arXiv:2112.02797_, 2021.
* Liu et al. (2022) Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Yu Lan, and Chao Shen. Coco: Coherence-enhanced machine-generated text detection under data limitation with contrastive learning. _ArXiv_, abs/2212.10341, 2022.
* Lu et al. (2021) Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic a* escape decoding: Constrained text generation with lookahead heuristics. _arXiv preprint arXiv:2112.08726_, 2021.
* McKenna and Sheldon (2020) Ryan McKenna and Daniel R Sheldon. Permute-and-flip: A new mechanism for differentially private selection. _Advances in Neural Information Processing Systems_, 33:193-203, 2020.
* McSherry and Talwar (2007) Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In _Symposium on Foundations of Computer Science (FOCS'07)_, pages 94-103. IEEE, 2007.
* Meister et al. (2020) Clara Isabel Meister, Elizabeth Salesky, and Ryan Cotterell. Generalized entropy regularization or: There's nothing special about label smoothing. In _Annual Meeting of the Association for Computational Linguistics (ACL-2020)_, pages 6870-6886. Association for Computational Linguistics (ACL), 2020.
* Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _ArXiv_, abs/2301.11305, 2023.
* OpenAI (2022) OpenAI. Chatgpt: Optimizing language models for dialogue. _OpenAI blog_, 2022. URL https://openai.com/blog/chatgpt/.
* OpenAI (2023a) OpenAI. New ai classifier for indicating ai-written text. _OpenAI blog_, 2023a. URL https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text.
* OpenAI (2023b) OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023b.
* Pillutla et al. (2021) Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. _Advances in Neural Information Processing Systems_, 34:4816-4828, 2021.
* Qin et al. (2020) Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Ronan Le Bras, Antoine Bosselut, and Yejin Choi. Backpropagation-based decoding for unsupervised counterfactual and abductive reasoning. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 794-805, 2020.
* Qin et al. (2020)Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* Sadasivan et al. (2023) Vinu Sankar Sadasivan, Aounon Kumar, S. Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _ArXiv_, abs/2303.11156, 2023.
* Shi et al. (2023) Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, and Cho-Jui Hsieh. Red teaming language model detectors with language models. _ArXiv_, abs/2305.19713, 2023.
* should professors worry? _Nature_, 2022.
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* Topkara et al. (2006) Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In _Workshop on Multimedia & Security_, 2006.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Tulchinskii et al. (2023) Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, S. Barannikov, Irina Piontkovskaya, Sergey I. Nikolenko, and Evgeny Burnaev. Intrinsic dimension estimation for robust detection of ai-generated texts. _ArXiv_, abs/2306.04723, 2023.
* Vijayakumar et al. (2016) Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. _arXiv preprint arXiv:1610.02424_, 2016.
* Weidinger et al. (2021) Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models. _ArXiv_, abs/2112.04359, 2021.
* Welleck et al. (2019) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. _arXiv preprint arXiv:1908.04319_, 2019.
* Wier et al. (2022) Gian Wiher, Clara Meister, and Ryan Cotterell. On decoding strategies for neural text generators. _Transactions of the Association for Computational Linguistics_, 10:997-1012, 2022.
* Wolff (2020) Max Wolff. Attacking neural text detectors. _ArXiv_, abs/2002.11768, 2020.
* Wu et al. (2023) Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: A stealthy, efficient and resilient watermark for large language models. _ArXiv_, abs/2310.07710, 2023.
* Yang et al. (2023) Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. _ArXiv_, abs/2305.17359, 2023.
* Yoo et al. (2023) Kiyoon Yoo, Wonhyuk Ahn, Jiho Jang, and No Jun Kwak. Robust multi-bit natural language watermarking through invariant features. In _Annual Meeting of the Association for Computational Linguistics_, 2023.
* Yu et al. (2023) Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and Neng H. Yu. Gpt paternity test: Gpt generated text detection with gpt genetic inheritance. _ArXiv_, abs/2305.12519, 2023.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. _Advances in neural information processing systems_, 32, 2019.

ZeroGPT. Zerogpt: Trusted gpt-4, chatgpt and ai detector tool by zerogpt. _ZeroGPT website_, 2023. URL https://www.zerogpt.com/.
* Zhang et al. (2023) Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, and Di Wu. On the safety of open-sourced large language models: Does alignment really prevent them from being misused? _ArXiv_, abs/2310.01581, 2023.
* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024.
* Zhang et al. (2021) Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors with better consistency: Logit anchoring on clean data. _arXiv preprint arXiv:2109.01300_, 2021.
* Zhao et al. (2023) Xuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. _arXiv preprint arXiv:2302.03162_, 2023.
* Zhao et al. (2024) Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text. _ICLR 2024_, 2024a.
* Zhao et al. (2024b) Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. _arXiv preprint arXiv:2401.17256_, 2024b.
* Zhu and Wang (2022) Yuqing Zhu and Yu-Xiang Wang. Adaptive private-k-selection with adaptive k and application to multi-label pate. In _International Conference on Artificial Intelligence and Statistics_, pages 5622-5635. PMLR, 2022.

More on Related Work

### Language Model Decoding.

The decoding strategy used in text generation greatly impacts the resulting text's quality and diversity. Traditional deterministic algorithms, like greedy decoding and beam search, often lead to repetitive text (Wiher et al., 2022). To address this, diverse beam search (DBS) (Vijayakumar et al., 2016) has been developed to promote diversity in text generation. Stochastic decoding strategies, such as Top-\(k\) and Top-\(p\)(Nucleus) (Holtzman et al., 2019) sampling, balance randomness and determinism, selecting from the most likely tokens to enhance variety while maintaining coherence. The Bayes Minimum Risk (MBR) method minimizes expected risk and incorporates a utility function to navigate trade-offs between text attributes. Advanced techniques have been developed to improve decoding for large language models, including the imposition of constraints (Anderson et al., 2016; Qin et al., 2020; Hokamp and Liu, 2017; Lu et al., 2021), enhancing text quality (Li et al., 2022), and speeding up the decoding process (Chen et al., 2023a).

Our contributions are complementary to these existing methods in that we are the first to introduce a rigorous stability definition and study the tradeoff between utility (e.g. perplexity) and stability. Permute-and-flip sampling can be used as a drop-in replacement for softmax sampling whenever it is used, e.g., in standard full sampling or nucleus (Top-p) sampling. We also provide watermarking capabilities for PF-decoder. We believe that the PF decoder has the potential to become a promising new approach for language model decoding.

### Detect AI-generated Text

Another major motivation of the work is to come up with a reliable method for detecting AI-generated text, so as to prevent LLM misuse. We briefly review two categories of existing work on this problem.

Post-hoc detection.Post-hoc detection of LLM-generated text encompasses two main approaches: zero-shot detection and training-based detection. Zero-shot detection is characterized by its capacity to identify AI-generated text without needing specific training data, leveraging the inherent stylistic differences between human and machine writing. Techniques within this category, such as DetectGPT (Mitchell et al., 2023), PHD (Tulchinskii et al., 2023), DNA-GPT (Yang et al., 2023), and FastDetectGPT (Bao et al., 2023), utilize metrics like log-probability scores, n-gram frequencies, lower intrinsic dimensionality, and conditional probability to differentiate AI-generated content. In contrast, training-based detection involves fine-tuning pre-trained language models on datasets that consist of both human and LLM-generated texts to build a classifier. This method is exemplified by various systems, including commercial detection platforms (OpenAI, 2023a; GPTZero, 2023; ZeroGPT, 2023), and research projects (Chen et al., 2023b; Yu et al., 2023; Liu et al., 2022; Hu et al., 2023a), which leverage the capabilities of large language models to effectively classify text origins. However, despite post-hoc detection's effectiveness in many cases, recent studies show detection methods' stability is limited across different scenarios. They have proven fragile to adversarial attacks and biased against non-native English writers (Wolff, 2020; Sadasivan et al., 2023; Liang et al., 2023; Shi et al., 2023). Limitations in accuracy even led OpenAI to close their detector in July 2023 (OpenAI, 2023a).

LLM watermarking.The watermarking approach provides a direct solution for AI text detection by intentionally embedding detectable signals or "watermarks" within the text. Unlike post-hoc detection, watermarking aims to determine if the text originates from a specific language model and it is robust to distribution shifts. Evolving from earlier techniques such as synonym substitution (Topkara et al., 2006) and syntactic restructuring (Atallah et al., 2001), modern watermarking strategies involve integrating watermarks into the decoding process of language models (Zhao et al., 2023; Kirchenbauer et al., 2023). Aaronson (2023) works with OpenAI to first develop a Gumbel watermark that uses a "traceable" pseudo-random softmax sampling when generating the next word. Kirchenbauer et al. (2023) split the vocabulary into red-green lists based on hash values of previous n-grams and then increase the logits of green tokens to embed the watermark. Zhao et al. (2024a) provides strong theoretical guarantees for the green-red watermarks and advocates the use of a consistent red-green list to enhance stability to evasion attacks. Christ et al. (2023); Hu et al. (2023b); Kuditipudi et al. (2023); Wu et al. (2023) study watermarks that preserve the original token probability distributions.

Meanwhile, multi-bit watermarks (Yoo et al., 2023; Fernandez et al., 2023) have been proposed to embed more complex information in the generation tasks.

PF-watermark is a newcomer to the family of LLM watermarks. It is closest to the Gumbel watermark (Aaronson, 2023) and enjoys all desirable properties of the Gumbel watermark. In Section 4 we have thoroughly compared the two watermarks with theory and numerical simulation, demonstrating that PF-watermarks offer a slightly improved detectability-greedness tradeoff. Comparisons under real-data experiments were also presented in Section 5.

Our results also have interesting implications for the green-red watermark (Kirchenbauer et al., 2023). For example, we can consider a PF-sampling version of green-red watermark which may perform better than the current green-red watermark in terms of its detectability-distortion tradeoff. Specifically, the stability guarantee of the PF-decoder (Theorem 3.1 Statement 1) implies that PF Green-Red Watermark enjoys the same quality guarantee as is analyzed in (Zhao et al., 2024; Theorem 3.1) for \(\alpha=\infty\), but the more concentrated distribution might make the watermark more prominent / and more detectable for PF-sampling based Green-Red watermark. A full exploration of this idea will be presented in a future work or a longer version of the current paper.

### Differential Privacy

While the current work is not about privacy, our technical approach heavily relies on existing methods developed in the differential privacy literature. Specifically, the permute-and-flip sampling was developed by McKenna and Sheldon (2020) as an alternative to the classical exponential mechanism (McSherry and Talwar, 2007) for the problem of differentially private selection. Ding et al. (2021) shows that the PF sampling is equivalent to returning the argmax of a noisy version of the utility function perturbed by independent exponential noise. Moreover, stability can be viewed as an algorithm-centric, input-specific version of pure-differential privacy (Dwork et al., 2006).

While some of the results we present are directly implied by existing work (e.g., Theorem 3.1), we believe it is a worthwhile (and hopefully beneficial) effort to introduce these results and their implications to the broader machine learning community.

To our knowledge, we are the first to draw the connection between various versions of Report-Noisy-Max (RNM) mechanisms in differential privacy to the LLM watermarking problem. Besides Gumbel noise (Gumbel-watermark) and exponential noise (PF-watermark), there are other versions of RNM that add, e.g., Gaussian noise (Zhu and Wang, 2022; Corollary 9). We hope our work could inspire more interplay between these two research areas.

## Appendix B Special Cases of PF Watermark

**Example B.1**.: When \(\mathrm{Softmax}(u_{t})\) is \(1/k\) for an arbitrary subset of \(k\) tokens and \(0\) for others,

\[\mathbb{E}[-\log(r_{t}(y_{t}))]:=H_{k}=1+1/2+...+1/k\approx\log k.\]

Specifically, when \(k=|\mathcal{V}|\) this is the uniform distribution, (7) \(\asymp n\log|\mathcal{V}|\) while when \(k=1\), the sequence is completely deterministic (e.g., when the LLM is asked to recite the "Declaration of Independence"), then we get (7) \(=n-m\) as expected.

In the above example, (7) is identical to the expected TestScore of the Gumbel watermark in (3). This is because the distributions they sample from are also the same. To illustrate their difference, let us revisit the simple two-token case from Example 3.2 again for which we can work out the expectation of the test score explicitly.

**Example B.2**.: Let the \(|\mathcal{V}|=2\) and the corresponding logits be \([\Delta,0]\). The expected TestScore of the Gumbel and the PF watermark (for each watermarked token) are: \(\frac{H_{1+e^{-\Delta/T}}}{1+e^{-\Delta/T}}+\frac{H_{1+e^{\Delta/T}}}{1+e^{ \Delta/T}}\) and \(1+\frac{1}{2}e^{-\Delta/T}(1+\Delta/T)\) respectively, where \(H_{x}\) is the \(x^{th}\) Harmonic number.

It is a bit hard to compare them by reading the mathematical expressions, so let us compare them numerically (see Figure 3(a)). The vertical axis in the figures measures _Detectability_, which we define to be the expected difference between the TestScore of a watermarked and unwatermarked token. Since under the null the \(\mathbb{E}[-\log(r_{t}(y_{t}))]=E[-\log(1-r_{t}(y_{t}))]=1\), we can simply subtract \(1\) from the expressions in Example B.2.

Figure 3(a) indicates the PF watermark does not beat the Gumbel watermark in terms of _detectability_ when \(T\) is fixed. This should not be surprising since for the same temperature, PF watermark is better at optimizing (recall Example 3.2 and Figure 1), thus naturally the resulting distribution has less entropy to be exploited by the watermarking scheme.

A more fair comparison, would be to increase the temperature for PF watermark appropriately so we compare _detectability_ when the _suboptimality_ is aligned. This is shown in Figure 3(b). In fact we have added a second baseline that apply Gumbel watermark to the _induced sampling distribution from PF-decoding_ (shown as the dotted line). The distribution induced by PF does not have a simple form, but in our special case, it was worked out in Example 3.2.

As we can see in Figure 3(b), the PF watermark is never worse and even has a slight advantage in the middle. To say it differently, to achieve the same suboptimality, the PF watermark can afford to use a larger temperature, which not only improves the stability parameter but also compensates it sufficiently on the detectability front to outperform the Gumbel watermark. In practice, we expect PF watermark to be as effective as the Gumbel watermark, and could even be a bit better (if the temperature parameter is chosen appropriately).

## Appendix C More Discussion of Stability

In this section, we delve deeper into the concept of stability as defined in Definition 2.1.

### Stability and Its Implication for Diversity

The definition of stability implies that the LLM decoder is resistant to small perturbations in the logits. Furthermore, _stability_ implies an intuitive notion of _diversity_, which says that for tokens with similar logits, then their chances of getting chosen should be similar. More rigorously:

_Remark C.1_ (Stability implies diversity).: If \(|u_{t}(y)-u_{t}(y^{\prime})|\leq\delta\), then we can construct a \(\tilde{u}_{t}\) such that \(\tilde{u}_{t}(y)=\tilde{u}_{t}(y^{\prime})\) while satisfying \(\|u_{t}-\tilde{u}_{t}\|_{\infty}\leq\frac{\delta}{2}\). Apply triangle inequality and Definition 2.1, we get

\[\left|\log\frac{p_{\mathcal{A}_{u_{t}}(y)}}{p_{\mathcal{A}_{u_{t}}(y^{\prime} )}}\right|=\left|\log\frac{p_{\mathcal{A}_{u_{t}}(y)}}{p_{\mathcal{A}_{\tilde{ u}_{t}}(y)}}+\log\frac{p_{\mathcal{A}_{\tilde{u}_{t}}(y^{\prime})}}{p_{ \mathcal{A}_{u_{t}}(y^{\prime})}}\right|\leq L\delta.\]

### Comparison of Decoding Methods in Terms of Stability

Theorem 3.1 establishes the Pareto-optimality of PF sampling with respect to the stability-perplexity tradeoff. This implies that no other decoding algorithm can surpass PF sampling in both stability and perplexity simultaneously.

Table 3 summarizes the stability guarantees of various decoding methods. Notably, only softmax sampling and PF sampling exhibit provable stability according to Definition 2.1. This is in contrast to

Figure 4: Comparing the _detectability_ of PF watermark vs Gumbel watermark using Example B.2.

other popular methods like beam search, greedy search, and Top-\(k\)/Top-\(p\) sampling, which lack such guarantees.

In our definition, stability implies that small perturbations to logits should not significantly alter the output text distribution. This definition also constrains the degree of greediness that a decoder can exhibit, as small changes to logits may change the sorted order of the next token. Thus, it is clear why greedy decoding and Top-\(k\)/\(p\) sampling cannot be stable.

When we say an algorithm \(\mathcal{A}\) cannot be stable, we mean that for any \(L<+\infty\), there exist two decoding problems \(u_{t}\) and \(\tilde{u}_{t}\) such that \(|u_{t}-\tilde{u}_{t}|_{\infty}\leq\delta\) for some \(\delta>0\), but the log-probability ratio between \(A(u_{t})\) and \(A(\tilde{u}_{t})\) exceeds \(\delta L\).

To establish non-stability, we only need a counterexample. Below, we construct explicit counterexamples for each decoding method:

* Greedy: We have a vocabulary of two tokens. Let \(u_{t}=[0,-1]\) and \(\tilde{u}_{t}=[1,0]\). This satisfies that \(|u_{t}-\tilde{u}_{t}|_{\infty}\leq\delta\) with \(\delta=1\). But Greedy(\(u_{t}\)) outputs the second token with probability 1, Greedy (\(\tilde{u}_{t}\)) outputs the second token with probability 0. Therefore, the importance ratio is unbounded, i.e., there isn't any finite \(L\) that bounds the probability ratio.
* Top-\(k\): Let's consider \(k=2\) as an example, and let vocabulary size be \(3\). Let \(u_{t}=[0,-1,-2]\) and \(\tilde{u}_{t}=[0,-2,-1]\). Again, \(|u_{t}-\tilde{u}_{t}|_{\infty}\leq\delta\) with \(\delta=1\). Now, Top-2 sampling will never output the third token on \(u_{t}\), while it will output the third token with probability \(>0\) on \(\tilde{u}_{t}\). The importance ratio is unbounded.
* Top-\(p\): Same example as in Top-\(k\) above. The softmax of \(u_{t}\) and \(\tilde{u}_{t}\) are approximately \([0.844,0.114,0.042]\) and \([0.844,0.042,0.114]\) respectively. If we set \(p=0.95\), then Top-\(p\) with \(u_{t}\) will exclude the third token, while that with \(\tilde{u}_{t}\) will retain it -- leading to an importance ratio of \(\infty\).
* Beam Search: Since beam search with a constant score function reduces to greedy decoding, the same counter-example for greedy decoding demonstrates its non-stability.

Therefore, these counter-examples highlight that greedy, Top-\(k\), Top-\(p\), and beam search decoding are not stable according to our definition.

### The Connection Between Stability and Safety

Stability can provide robustness against certain types of adversarial attacks, such as jailbreaking, within a gray-box threat model.

Consider a scenario where an attacker can alter the logits and receive responses from an API, as is common with OpenAI's logit bias feature4. Recent research has shown that it is possible to jailbreak an LLM to encourage specific outputs like "Sure" or "Definitely" or other tokens [Zhang et al., 2023, Zhao et al., 2024b]. By employing the PF decoding method, the API provider can make jailbreaking significantly harder while also making the output watermarkble, thereby enhancing the model's safety and security.

Footnote 4: https://help.openai.com/en/articles/5247780-using-logit-bias-to-alter-token-probability-with-the-openai-api

Stability serves as a fundamental guarantee against these perturbations. That said, all attacks will eventually manifest as perturbations to logits, and having a "Lipschitz" transformation from logits to token probabilities is a nice guarantee. Without this, even tiny changes to the logits can have a big influence on the final token probability distribution.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Property** & **Softmax** & **PF** & **Beam/Greedy/Top-\(k\)/Top-\(p\)** \\ \hline Stability (Def 2.1) & \(L=2/T\) & \(L=2/T\) & \(+\infty\) (Not stable for any parameter \(L\)) \\ Expected Suboptimality (Worst-case bound) & \(T\log|V|\) & \(T\log|V|\) & 0 for “greedy”; \(T\log k\) for “Top-\(k\)” \\ Expected Suboptimality (Per instance) & Exponentially higher log-likelihood & Between 1 - 2 smaller than Softmax & 0 for “greedy”; Same as Softmax on “Top-\(k\)” \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of stability and suboptimality across different sampling methods.

For instance, consider the difference between a provably stable decoder (such as softmax) and a non-stable one in a gray-box jailbreaking scenario. Suppose the language model has a vocabulary of just three words: "Sure", "OK", and "Sorry". Initially, the logits \(u_{t}=[-2,-1,0]\) indicate that the LLM is inclined to output "Sorry". If the adversary aims to increase the likelihood of outputting "Sure", they could perturb the logits to \(\tilde{u}_{t}=[0,-1,0]\), which satisfies \(|u_{t}-\tilde{u}_{t}|_{\infty}\leq\delta\) with \(\delta=2\).

When applying Top-\(k\) sampling with \(k=2\) (as an example of several sampling methods that lack stability, such as Greedy, Top-\(k\), Top-\(p\), and Beam Search), the model would never select "Sorry", the third token, whereas with the original logits, "Sorry", would still have a non-zero probability of being selected. This demonstrates that Top-\(k\) sampling lacks _stability_. In contrast, PF sampling and softmax sampling maintain _stability_ under such perturbations.

## Appendix D Additional Experiment Details

We provide additional details on the experiments here. We use the C4 [Raffel et al., 2020] and Alpaca [Taori et al., 2023] datasets. Specifically, we select samples from C4 with text longer than 500 tokens, using the first 200 tokens as the prompt input to the language model and the next 300 tokens as the human-generated reference. This gives us 600 examples. For Alpaca, we select samples with prompts/instructions longer than 25 tokens and answers also longer than 25 tokens, giving 550 examples. Since Llama2-Chat is a fine-tuned version of Llama-2 optimized for dialogue, we use the Chat version (Llama-2-7B-Chat5) for the question-answering task and the base model (Llama-2-7B6) for the text completion task.

Footnote 5: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf

Footnote 6: https://huggingface.co/meta-llama/Llama-2-7b-hf

Given that PF decoding can integrate with Top-\(p\) sampling, which initially selects the top \(p\) tokens before normalization, we conduct the performance tests using a \(p=1.0\) for full sampling. The max generation length is set to 256 tokens for all experiments.

For perplexity calculation, we observe high variance with different methods, often influenced by outliers. To address this, we remove the top and bottom 3% of perplexity scores as outliers and then calculate the average perplexity and standard error. For MAUVE scores, we use the human-written references from C4 and Alpaca as the human distribution.

For watermarking experiments, we generate 500 watermarked and 500 unwatermarked sentences per method. We label them as "watermarked" and "unwatermarked" respectively, with corresponding human-written text as "human" for each prompt. Following Kirchenbauer et al. [2023], we use a watermark strength of \(\delta=2.0\) and green list ratio of \(\gamma=0.5\) for the KGW watermark. For fair comparison, we use the same long prefix as the pseudo-random function, hashing the previous m tokens to get the random vector for Gumbel/PF watermarks, or to split the green/red token lists. For the watermark robustness test (Table 4) we use a 4-token prefix, and an 8-token prefix elsewhere. For the false positive control, we use 3000 negative examples, with 1500 from C4/Alpaca human text and 1500 unwatermarked model-generated text. In our robustness testing, we evaluate two configurations of the DIPPER [Krishna et al., 2023] model: DIPPER-1 with lexical diversity L=40, order diversity O=40, and DIPPER-2 with L=40, O=100.

### PF Watermark Robustness Results.

To evaluate the robustness of the watermark detection, we test the PF watermark under paraphrasing and text editing attacks. Note that robustness here refers to the watermark's resilience against removal attacks, which is different from the logit stability in Definition 2.1.

We employ various paraphrase attack techniques intended to remove the watermark text. The experiments are conducted with a 4-token prefix for the pseudorandom function. We also added the original soft watermarking method with a 1-token prefix for comparison (\(m=1\) for KGW watermark [Kirchenbauer et al., 2023]).

In our experiments, we utilize two versions of the DIPPER paraphrasing model [Krishna et al., 2023], denoted as DIPPER-1 and DIPPER-2. DIPPER-2 generates more diverse paraphrases than DIPPER-1. Moreover, we test a random word deletion attack, which is a common technique used to manipulate text. These experiments represent realistic scenarios where an adversary may attempt to remove watermarks through paraphrasing or editing. The results, shown in Table 4, illustrate the robustness of the PF watermark to these paraphrasing and editing attacks. The PF watermark achieves comparable detection performance to the Gumbel watermark and KGW watermark methods when using the same long prefix as the pseudorandom function.

### Impact of Text Length on Watermark Detection.

Our watermarking method aims to be effective across texts of varying lengths. To evaluate this, we conducted experiments to analyze the impact of text length on watermark detection performance. Texts are truncated to 30, 50, 100, 150, and 200 tokens. The results, shown in Table 5, validate the robustness of our approach to different text lengths. Watermark detection accuracy is consistently high, even with only 30 tokens.

### Experiments on Variants of KGW Watermark

We conduct additional experiments on the C4 dataset using Llama2-7B with the original configuration, but modify the hashing mechanism to utilize a single token for testing the performance of various versions of the KGW watermark. We select \(\gamma\) values of 0.25 and 0.1. The results can be found in Tables 6 and 7. Upon comparing these results with the data from Figure 1(b), the PF watermark remains at the Pareto front, providing the optimal trade-off between the highest detection accuracy and the lowest perplexity.

\begin{table}
\begin{tabular}{c l c c c c c} \hline \hline \multirow{2}{*}{**Setting**} & \multirow{2}{*}{**Method**} & \multirow{2}{*}{**AUC**} & \multicolumn{2}{c}{**1\% FPR**} & \multicolumn{2}{c}{**10\% FPR**} \\  & & & TPR & F1 & TPR & F1 \\ \hline \multirow{4}{*}{No attack} & KGW (m=4) & 0.998 & 0.996 & 0.989 & 1.000 & 0.906 \\  & KGW (m=1) & 0.999 & 1.000 & 0.995 & 1.000 & 0.906 \\  & Gumbel & 0.992 & 0.979 & 0.979 & 0.986 & 0.913 \\  & PF & 0.996 & 0.977 & 0.980 & 0.993 & 0.898 \\ \hline \multirow{4}{*}{DIPPER-1} & KGW (m=4) & 0.661 & 0.057 & 0.105 & 0.317 & 0.416 \\  & KGW (m=1) & 0.876 & 0.389 & 0.554 & 0.717 & 0.720 \\  & Gumbel & 0.838 & 0.367 & 0.529 & 0.642 & 0.697 \\  & PF & 0.824 & 0.374 & 0.537 & 0.622 & 0.684 \\ \hline \multirow{4}{*}{DIPPER-2} & KGW (m=4) & 0.638 & 0.051 & 0.096 & 0.278 & 0.375 \\  & KGW (m=1) & 0.885 & 0.342 & 0.501 & 0.662 & 0.714 \\  & Gumbel & 0.764 & 0.239 & 0.380 & 0.523 & 0.608 \\  & PF & 0.795 & 0.250 & 0.394 & 0.544 & 0.625 \\ \hline \multirow{4}{*}{Random} & KGW (m=4) & 0.936 & 0.484 & 0.644 & 0.881 & 0.844 \\  & KGW (m=1) & 0.956 & 0.752 & 0.836 & 0.923 & 0.839 \\ \cline{1-1}  & Gumbel & 0.981 & 0.941 & 0.960 & 0.959 & 0.898 \\ \cline{1-1}  & PF & 0.985 & 0.936 & 0.956 & 0.966 & 0.888 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Detection results for three watermarking methods using Llama2-7B on the C4 dataset under different attacks.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{**Length**} & \multirow{2}{*}{**AUC**} & \multicolumn{2}{c}{**1\% FPR**} & \multicolumn{2}{c}{**10\% FPR**} \\  & & TPR & F1 & TPR & F1 \\ \hline
200 & 0.994 & 0.977 & 0.978 & 0.985 & 0.915 \\
150 & 0.993 & 0.975 & 0.980 & 0.985 & 0.913 \\
100 & 0.992 & 0.970 & 0.972 & 0.983 & 0.911 \\
50 & 0.987 & 0.950 & 0.966 & 0.970 & 0.902 \\
30 & 0.980 & 0.923 & 0.950 & 0.953 & 0.888 \\ \hline \hline \end{tab

## Appendix E Proofs of Technical Results

### Permute and Flip Sampling

First, let us calculate the probability of Permute-and-Flip sampling from Line 3-9 of Algorithm 1. We will use the equivalent ReportNoisy(Arg)Max form from Fact 4.2.

\[w_{t}=\operatorname*{arg\,max}_{w\in\mathcal{V}}\left(u_{w,t}-\log r_{w,t}\right)\]

First, observe that the event that "\(w\) is selected" is the same as the event that for \(u_{w}-\log r_{w}>u_{w^{\prime}}-\log r_{w^{\prime}}\) for all \(w^{\prime}\neq w\).

Observe that for each \(w^{\prime}\), this event is equivalent to a range of integral for \(w^{\prime}\)

\[u_{w}-\log r_{w}>u_{w^{\prime}}-\log r_{w^{\prime}}\Leftrightarrow\log r_{w^ {\prime}}>-u_{w}+u_{w^{\prime}}+\log r_{w}\Leftrightarrow r_{w^{\prime}}>r_{w }e^{u_{w^{\prime}}-u_{w}}\] (8)

We have

\[\Pr[w\text{ is selected}]=\mathbb{E}\left[\mathbf{1}\left(w\text{ is selected}\right)\right]\] \[= \int_{0}^{1}\prod_{w^{\prime}\neq w}\left(\int_{0}^{1}\mathbf{1 }\left(u_{w}-\log r_{w}>u_{w^{\prime}}-\log r_{w^{\prime}}\right)\mathrm{d}w^ {\prime}\right)\mathrm{d}r_{w}\] \[= \int_{0}^{1}\prod_{w^{\prime}\neq w}\left(\int_{r_{w}\exp\left(u_ {w^{\prime}}-u_{w}\right)}^{1}\mathrm{d}r_{w^{\prime}}\right)\mathrm{d}r_{w}\] \[= \int_{0}^{1}\prod_{w^{\prime}\neq w}\left(1-r_{w}\cdot e^{u_{w^{ \prime}}-u_{w}}\right)_{+}\mathrm{d}r_{w}\] \[= \int_{0}^{e^{u_{w}-u_{w^{\prime}}}}\prod_{w^{\prime}\neq w}\left( 1-r_{w}\cdot e^{u_{w^{\prime}}-u_{w}}\right)\mathrm{d}r_{w}\] (9) \[= \int_{0}^{\frac{p(w)}{p(w^{\prime})}}\prod_{w^{\prime}\neq w} \left(1-r_{w}\cdot\frac{p(w^{\prime})}{p(w)}\right)\mathrm{d}r_{w}\]

where \((x)_{+}:=\max(0,x)\), and \(p(\cdot):=\text{Softmax}(u)\). In the above, \(w^{*}=\operatorname*{arg\,max}_{w}u_{w}\), and observe that

* If \(w=w^{*},(1-r_{w}\cdot e^{u_{w^{\prime}}-u_{w}})\) cannot be negative, and \(e^{u_{w}-u_{w^{*}}}=1\).

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Temperature** & **TPR@0.01FPR** & **PPL1** \\ \hline
1.0 & 0.690 & 15.07 \\
0.8 & 0.686 & 4.434 \\
0.6 & 0.681 & 2.047 \\
0.4 & 0.643 & 1.768 \\
0.2 & 0.628 & 1.559 \\ \hline \hline \end{tabular}
\end{table}
Table 6: KGW Watermark with \(\gamma=0.1\), \(\delta=2.0\) and single token for hash

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Temperature** & **TPR@0.01FPR** & **PPL1** \\ \hline
1.0 & 0.973 & 16.31 \\
0.8 & 0.972 & 4.887 \\
0.6 & 0.934 & 2.328 \\
0.4 & 0.893 & 1.940 \\
0.2 & 0.859 & 1.791 \\ \hline \hline \end{tabular}
\end{table}
Table 7: KGW Watermark with \(\gamma=0.25\), \(\delta=2.0\) and single token for hash.

* If \(w\neq w^{*}\), then for \(r_{w}\leq e^{u_{w}-u_{w^{*}}}\), we can drop the clipping.

In both cases, we can integrate to \(e^{u_{w}-u_{w^{*}}}\), and drop the clipping in \(\left(\cdot\right)_{+}\).

Proof of Example 3.2.: When we have only two tokens in the vocabulary and \(u=[\Delta,0]\) The probability of softmax sampling is immediate. As for PF sampling, the results are obtained by instantiating (9) and solving the integrals for \(w=a\) and \(w=b\) where \(\mathcal{V}=\{a,b\}\). \(a\) is \(w^{*}\), so the integral becomes \(\Pr[a\text{ is selected}]=\int_{0}^{1}(1-re^{-\Delta})dr=1-0.5e^{-\Delta}\). The \(\Pr[b\text{ is selected}]=0.5e^{-\Delta}\). 

### Permute and Flip Watermarking

Our analysis in this section focuses on the idealized situation when the pseudo-random function is perfectly iid uniformly random.

Recall that the Permute and Flip watermark works as follows.

1. Sample the random number \(r_{y}\) from uniform distribution \(r_{y}\sim\text{Unif}(0,1)\) for all \(y\in\mathcal{V}\).
2. Output \(y_{t}=\arg\max_{w\in\mathcal{V}}\left(u_{y,t}-\log r_{y,t}\right)\)
3. Detection statistic \(\sum_{t=n-m+1}^{n}-\log r_{t,y_{t}}\)

Proof of Theorem 4.3.: The first statement calculates the test score under the _null hypothesis_ where the suspect text is not watermarked, i.e., it is statistically independent to the secret key k thus independent to \(F\) and by extension to \(r_{t,..}\) Thus in this case, when conditioning on \(y_{1:n}\), \(r_{t,y}\) remains uniformly distributed for every \(y\in\mathcal{V}\) including the \(y_{t}\) we conditioned on. \(-\log(r_{t,y_{t}})\sim\text{Exponential}(1)\) for each \(t\), thus the expected value is is \(1\) for each token. The total is \(n-m\).

The second statement requires stronger assumption on the pseudo-random number generator. The generated random vectors in each step needs to be mutually independent for all subset of of length \(n-m\) among the set of all \(m\)-grams, which is implied by the even stronger condition of perfect independent randomness assumed in this theorem, and the fact that there are no duplicate m-grams prefixes among the \(n-m\) of them. Clearly, sum of \(n-m\) independent exponential R.V.s satisfies an Erlang distribution with shape parameter \(n-m\). The inverse CDF claim follows directly.

Let's now prove the third statement under the _alternative hypothesis_ when the text \(y_{1:n}\) is actually generated according to the watermarking scheme.

We will focus on \(-\log r_{w,t}\) for \(t=m-1,2,\ldots,n\). Drop subscript \(t\) for now. Let \(\hat{w}\) be the selected token.

\[\mathbb{E}[-\log r_{\hat{w}}] =\sum_{w\in\mathcal{V}}\mathbb{P}(w\text{ is selected})\mathbb{E}[- \log r_{w}|w\text{ is selected}]\] \[=\sum_{w\in\mathcal{V}}\mathbb{E}[-\log r_{w}\cdot\mathbf{1} \left(w\text{ is selected}\right)]\]

Fix \(w\), let us calculate \(\mathbb{E}[-\log r_{\hat{w}}\cdot\mathbf{1}\left(w\text{ is selected}\right)]\).

Again, use (8) and follow the same lines of arguments as we calculate the probabilities, we get:

\[\mathbb{E}\left[-\log r_{\hat{w}}\cdot\mathbf{1}\left(w\text{ is selected} \right)\right]\] \[= \int_{0}^{1}-\log r_{w}\prod_{w^{\prime}\neq w}\left(\int_{0}^{1 }\mathbf{1}\left(u_{w}-\log r_{w}>u_{w^{\prime}}-\log r_{w^{\prime}}\right) \mathrm{d}w^{\prime}\right)\mathrm{d}r_{w}\] \[= \int_{0}^{1}-\log r_{w}\prod_{w^{\prime}\neq w}\left(\int_{r_{w} \exp\left(u_{w^{\prime}}-u_{w}\right)}^{1}\mathrm{d}r_{w^{\prime}}\right) \mathrm{d}r_{w}\] \[= \int_{0}^{1}-\log r_{w}\prod_{w^{\prime}\neq w}\left(1-r_{w} \cdot e^{u_{w^{\prime}}-u_{w}}\right)_{+}\mathrm{d}r_{w}\] \[= \int_{0}^{e^{u_{w}-u_{w}*}}-\log r_{w}\prod_{w^{\prime}\neq w} \left(1-r_{w}\cdot e^{u_{w^{\prime}}-u_{w}}\right)\mathrm{d}r_{w}\] (10) \[= \int_{0}^{\frac{p(w)}{p(w^{\prime})}}-\log r_{w}\prod_{w^{\prime }\neq w}\left(1-r_{w}\cdot\frac{p(w^{\prime})}{p(w)}\right)\mathrm{d}r_{w}.\]

Finally, observe that the proof is complete because (10) is what Statement 3 states. 

The examples we gave essentially just instantiate (10) to cases where the integral can be solved by simple integration by parts.

Proof of Example b.1.: Deterministic \(\Rightarrow\mathbb{P}(w^{*})=1\)

\[\mathbb{E}\left[-\log r_{w}\cdot\mathbf{1}\left(w\text{ is selected} \right)\right] =\int_{0}^{\frac{p(w)}{p(w^{*})}}-\log r_{w}\prod_{w^{\prime}\neq w} \left(1-r_{w}\cdot\frac{\mathbb{P}(w^{\prime})}{\mathbb{P}(w)}\right)\mathrm{ d}r_{w}\] \[=\int_{0}^{1}-\log r_{w}\,\mathrm{d}r_{w}=\begin{cases}1\text{ for }w=w^{*}\\ 0\text{ otherwise}\end{cases}\]

Fully random \(\Rightarrow u_{w}=u_{w}^{\prime}=\frac{1}{N}\) for all \(w,w^{\prime}\).

\[\mathbb{E}\left[-\log r_{w}\cdot\mathbf{1}\left(w\text{ is selected} \right)\right] =\int_{0}^{\frac{p(w)}{p(w^{*})}}-\log r_{w}\prod_{w^{\prime}\neq w} \left(1-r_{w}\cdot\frac{\mathbb{P}(w^{\prime})}{\mathbb{P}(w)}\right)\mathrm{ d}r_{w}\] \[=\int_{0}^{1}-\log r_{w}(1-r_{w})^{N-1}\,\mathrm{d}r_{w}\] \[=\int_{0}^{1}\log r_{w}\cdot\frac{1}{N}\,\mathrm{d}\left[\left(1 -r_{w}\right)^{N}-1\right]\] \[=-\int_{0}^{1}\frac{1}{N}\left[\left(1-r_{w}\right)^{N}-1\right] \mathrm{d}\log r_{w}\] \[=\int_{0}^{1}\frac{1}{N}\frac{1-(1-r_{w})^{N}}{r_{w}}\,\mathrm{d} r_{w}\] \[=\int_{0}^{1}\frac{1}{N}\frac{1-u^{N}}{1-u}\,\mathrm{d}u\] \[=\frac{1}{N}H_{N}\]

\(H_{\alpha}\) is the \(\alpha\)-th Harmonic number \(H_{\alpha}:=\int_{0}^{\alpha}\frac{1-x^{\alpha}}{1-x}dx\). The stated \(k\)-subset example is implied by the fully random case. 

Proof of Example b.2.: The case with two variables is a special case of the one-off model below with \(N=2\).

**Example E.1** (One-Off model).: Let the logits be \([-\Delta,0,...,0]\) with a total length of \(N\).

The probability distribution \([p_{1},...,p_{N}]\) from Permute-and-Flip satisfies

\[p_{1}=\frac{1}{e^{\Delta N}},\quad p_{2}=p_{3}=...=p_{N}=\frac{1}{N-1}-\frac{e^ {-\Delta}}{N(N-1)}.\]

\[\mathbb{E}[-\log(r_{\hat{w}})]=H_{N-1}+\frac{(1+\Delta)e^{-\Delta}}{N}.\]

Proof.: By (9), for the first token (with logits \(-\Delta\)) we get its probability is equal to

\[\int_{0}^{e^{-\Delta}}(1-e^{\Delta}r)^{N-1}dr=\frac{e^{-\Delta}}{N}.\]

the remaining tokens has probability equal to \(1/(N-1)\) of \(1-\frac{e^{-\Delta}}{N}\).

By (10) we have that for \(w=1\) (the suboptimal token with logits = \(-\Delta\).

\[\mathbb{E}[-\log r_{t,w}\mathbf{1}(\text{$w$ is selected})]=\int_{0}^{e^{- \Delta}}(1-e^{\Delta}r)^{N-1}dr=\frac{\Delta+H_{N}}{e^{\Delta}N}\]

For other (optimal) tokens, we get that

\[\mathbb{E}[-\log r_{t,w}\mathbf{1}(\text{$w$ is selected})] =\int_{0}^{1}-\log r(1-r)^{N-2}(1-e^{-\Delta}r)dr=\frac{H_{N-1}}{ N-1}-e^{-\Delta}\frac{H^{N}-1}{N(N-1)}\]

All integrals follows from Lemma E.2.

\[\mathbb{E}[-\log(r_{w})] =(N-1)\left(\frac{H_{N-1}}{N-1}-e^{-\Delta}\frac{H^{N}-1}{N(N-1) }\right)+\frac{\Delta+H_{N}}{e^{\Delta}N}\] \[=H_{N-1}+\frac{(1+\Delta)e^{-\Delta}}{N}.\]

**Lemma E.2**.: _for any \(a>0\) and \(N>1\)._

\[\int_{0}^{1/a}-\log x(1-ax)^{N-1}dx =\frac{\log a+H_{N}}{a+N}\] \[\int_{0}^{1}-\log x(1-x)^{n-2}dx =\frac{H_{N-1}}{N(N-1)}\]

Proof.: The proofs of both integrals follow from integration by parts. These were checked formally using WolframAlpha. The details are omitted.

[MISSING_PAGE_FAIL:23]