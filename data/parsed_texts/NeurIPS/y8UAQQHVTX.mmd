# Private Everlasting Prediction

 Moni Naor

Department of Computer Science and Applied Math, Weizmann Institute of Science. moni.naor@weizmann.ac.il. Inclument of the Judith Kleeman Professorial Chair. Research supported in part by grants from the Israel Science Foundation (no.2686/20), by the Simons Foundation Collaboration on the Theory of Algorithmic Fairness and by the Israeli Council for Higher Education (CHE) via the Weizmann Data Science Research Center.

Kobbi Nissim

Department of Computer Science, Georgetown University. kobbi.nissim@georgetown.edu. Research supported in part by by NSF grant No. CNS-2001041 and a gift to Georgetown University.

Uri Stemmer

Blavatnik School of Computer Science, Tel Aviv University, and Google Research. u@uri.co.il. Partially supported by the Israel Science Foundation (grant 1871/19) and by Len Blavatnik and the Blavatnik Family foundation.

Chao Yan

Department of Computer Science, Georgetown University. cy399@georgetown.edu. Research supported in part by by a gift to Georgetown University.

Learning often happens in settings where the underlying training data is related to individuals and privacy-sensitive. For legal, ethical, or other reasons, the learner is then required, to protect personal information from being leaked in the learned hypothesis \(h\). Private learning was introduced by Kasiviswanathan et al. (2011) as a theoretical model for studying such tasks. A _private learner_ is a PAC learner that preserves differential privacy with respect to its training set \(S\). That is, the learner's distribution on outcome hypotheses must not depend too strongly on any single example in \(S\). Kasiviswanathan et al. showed that any finite concept class can be learned privately and with sample complexity \(n=O(\log|C|)\), a value that is sometimes significantly higher than the VC dimension of the concept class \(C\).

It is now understood that the gap between the sample complexity of private and non-private learners is essential - an important example is private learning of threshold functions (defined over an ordered domain \(X\) as \(C_{thresh}=\{c_{t}\}_{t\in X}\) where \(c_{t}(x)=1_{x\in I}\)), which requires sample complexity that is asymptotically higher than the (constant) VC dimension of \(C_{thresh}\). In more detail, with _pure_ differential privacy, the sample complexity of private learning is characterized by the representation dimension of the concept class (Beimel et al., 2013). The representation dimension of \(C_{thresh}\) (hence, the sample complexity of private learning thresholds) is \(\Theta(\log|X|)\)(Feldman and Xiao, 2015). With _approximate_ differential privacy, the sample complexity of learning threshold functions is \(\Theta(\log^{*}|X|)\)(Beimel et al., 2013, Bun et al., 2015, Alon et al., 2019, Kaplan et al., 2020, Cohen et al., 2022). Hence, whether with pure or with approximate differential privacy, the sample complexity of privately learning thresholds grows with the cardinality of the domain \(|X|\) and no private learner exists for this task over infinite domains. In contrast, non-private learning is possible with constant sample complexity (independent of \(|X|\)).

Privacy preserving (black-box) prediction.Dwork and Feldman (2018) proposed privacy-preserving prediction as an alternative for private learning. Noting that "[i]t is now known that for some basic learning problems [...] producing an accurate private model requires much more data than learning without privacy," they considered a setting where "users may be allowed to query the prediction model on their inputs only through an appropriate interface". That is, a setting where the learned hypothesis is not made public and may be accessed only in a _black-box_ manner via a privacy-preserving query-answering prediction interface. The prediction interface is required to preserve the privacy of its training set \(S\):

**Definition 1.1** (private prediction interface (Dwork and Feldman, 2018) (rephrased)).: A prediction interface \(\mathcal{A}\) is \((\epsilon,\delta)\)-differentially private if for every interactive query generating algorithm \(Q\), the output of the interaction between \(Q\) and \(\mathcal{A}(S)\) is \((\epsilon,\delta)\)-differentially private with respect to \(S\).

Dwork and Feldman focused on the setting where the entire interaction between \(Q\) and \(\mathcal{A}(S)\) consists of issuing a single prediction query and answering it:

**Definition 1.2** (Single query prediction (Dwork and Feldman, 2018)).: Let \(\mathcal{A}\) be an algorithm that given a set of labeled examples \(S\) and an unlabeled point \(x\) produces a label \(v\). \(\mathcal{A}\) is an \((\epsilon,\delta)\)-differentially private prediction algorithm if for every \(x\), the output \(\mathcal{A}(S,x)\) is \((\epsilon,\delta)\)-differentially private with respect to \(S\).

W.r.t. answering a single prediction query, Dwork and Feldman showed that the sample complexity of such predictors is proportional to the VC dimension of the concept class.

### Our contributions

We extend private prediction to answering a sequence - _unlimited in length_ - of prediction queries. We refer to this as _private everlasting prediction_ (PEP). Our goal is to present a generic private everlasting predictor with low training sample complexity \(|S|\).

#### 1.1.1 Private prediction interfaces when applied to a large number of queries

We begin by examining private everlasting prediction under the framework of Definition 1.1. We prove:

**Theorem 1.3** (informal version of Theorem 3.3).: _Let \(\mathcal{A}\) be a private everlasting prediction interface for concept class \(C\) and assume \(\mathcal{A}\) bases its predictions solely on the initial training set \(S\), then there exists a private learner for concept class \(C\) with sample complexity \(|S|\)._

This means that everlasting predictors that base their prediction solely on the initial training set \(S\) are subject to the same complexity lowerbounds as private learners. Hence, to avoid private learning lowerbounds, private everlasting predictors need to rely on more than the initial training sample \(S\) as a source of information about the underlying probability distribution and the labeling concept.

In this work, we choose to allow the everlasting predictor to rely on the queries made - which are unlabeled points from the domain \(X\) - assuming the queries are drawn from the same distribution the initial training \(S\) is sampled from. This requires modifying the privacy definition, as Definition 1.1 does not protect the queries issued to the predictor.

#### 1.1.2 A definition of private everlasting predictors

Our definition of private everlasting predictors is motivated by the observations above. Consider an algorithm \(\mathcal{A}\) that is first fed with a training set \(S\) of labeled points and then executes for an unlimited number of rounds, where in round \(i\) algorithm \(\mathcal{A}\) receives as input a query point \(x_{i}\) and produces a label \(\hat{y}_{i}\). We say that \(\mathcal{A}\) is an everlasting predictor if, when the (labeled) training set \(S\) and the (unlabeled) query points are coming from the same underlying distribution, \(\mathcal{A}\) answers each query points \(x_{i}\) by invoking a good hypothesis \(h_{i}\) (i.e., \(h_{i}\) has low generalization error), and hence the label \(\hat{y}_{i}\) produced by \(\mathcal{A}\) is correct with high probability. We say that \(\mathcal{A}\) is a _private_ everlasting predictor if its sequence of predictions \(\hat{y}_{1},\hat{y}_{2},\hat{y}_{3},\ldots\) protects both the privacy of the training set \(S\)_and_ the query points \(x_{1},x_{2},x_{3},\ldots\) in face of any adversary that chooses the query points adaptively.

We emphasize that while private everlasting predictors need to exhibit average-case utility - as good prediction is required only for the case where the initial training set \(S\) and the queries \(x_{1},x_{2},x_{3},\ldots\) are selected i.i.d. from the same underlying distribution - our privacy requirement is worst-case, and holds in face of an adaptive adversary that chooses each query point \(x_{i}\) after receiving the prediction provided for \((x_{1},\ldots,x_{i-1})\), and not necessarily in accordance with any probability distribution.

#### 1.1.3 A generic construction of private everlasting predictors

We show a reduction from private everlasting prediction of a concept class \(C\) to non-private PAC learning of \(C\). Our Algorithm GenericBBL, presented in Section 6, executes in rounds.

Initialization.The input to the first round is a labeled training set \(S\) where \(|S|=O\left((\operatorname{VC}(C))^{2}\right)\), assumed to be labeled consistently with some unknown target concept \(c\in C\). Denote \(S_{1}=S\) and \(c_{1}=c\).

Rounds.Each round begins with a collection \(S_{i}\) of labeled examples and ends with a newly generated collection of labeled examples \(S_{i+1}\) that feeds as input for the next round. The size of these collections grows by a constant factor at each round, to allow for the accumulated error of the predictor to converge. The construction ensures that each labeled set \(S_{i}\) is consistent with some concept in \(c_{i}\in C\), which is not necessarily the original target concept \(c\), but has a bounded generalization error with respect to \(c\). In more detail, the construction ensures that \(\Pr_{x\sim D}[c_{i+1}(x)\neq c_{i}(x)]\) decreases by a factor of two in every round, and hence, by the triangle inequality, \(\Pr_{x\sim D}[c_{i}(x)\neq c(x)]\) is bounded.

We briefly describe the main computations performed in each round of GenericBBL.5

Footnote 5: Important details, such as privacy amplification via sampling and management of the learning accuracy and error parameters are omitted from the description provided in this section.

* **Round initialization:** At the outset of a round, the labeled set \(S_{i}\) is partitioned into sub-sets, each with number of samples which is proportional to the VC dimension (so we have \(\approx\frac{|S_{i}|}{\operatorname{VC}(C)}\) sub-sets). Each of the sub-sets is used for training a classifiernon-privately, hence creating a collection of classifiers \(F_{i}=\{f:X\rightarrow\{0,1\}\}\) that are used throughout the round.
* **Query answering:** Queries are issued to the predictor in an online manner. A query is first labeled by each of the classifiers in \(F_{i}\). Then the predicted label is computed by applying a privacy-preserving majority vote on these intermediate labels. By standard composition theorems for differential privacy, we could answer roughly \(|F_{i}|^{2}\approx\left(\frac{|S_{i}|}{\text{VC}(C)}\right)^{2}\) queries without exhausting the privacy budget.
* **Generating a labeled set for the following round:*
* A round ends with the preparation of a collection \(S_{i+1}\) of labeled samples that is to be used in the initialization of next round. We explore alternatives for creating \(S_{i+1}\):
* As at the end of a round the predictor has already labeled all the queries presented during the round's execution, one alternative is to let \(S_{i+1}\) consist of these queries and the labels provided for them. Note, however, that it is not guaranteed that the majority vote would result in a set of labels that are consistent with some concept in \(C\), even if \(S_{i}\) is consistent with some concept in \(C\). Hence, following this alternative would require the use of non-private _agnostic_ learners instead. Furthermore, the error introduced by the majority vote can be larger than the generalization error of the classifiers in \(F_{i}\) by a constant factor greater than one. This may prevent the generalization error of the classifiers used in future rounds from converging.6 Footnote 6: to see that the majority vote can increase the generalization error by a constant factor consider three classifiers \(f_{1},f_{2},f_{3}\), each with generalization error \(\alpha\), and let \(A,B,C\) be three disjoint subsets of the support of the underlying distribution, each of probability weight \(\alpha/2\). If the classifier \(f_{1}\) errs on inputs in \(A\cup B\), the classifier \(f_{2}\) errs on \(A\cup C\), and \(f_{3}\) errs on \(B\cup C\) then the majority of \(f_{1},f_{2},f_{3}\) errs on inputs in \(A\cup B\cup C\) and hence the generalization error grows from \(\alpha\) to \(1.5\alpha\).
* To overcome this problem, we use Algorithm LabelBoost - a tool developed by Beimel et al. (2021) in the context of private semi-supervised learning. LabelBoost takes as input the sample \(S_{i}\) (labeled by a concept \(c_{i}\in C\)) and the (unlabeled) queries made during the round. It labels them with a concept \(c_{i+1}\in C\) where the error of \(c_{i+1}\) with respect to \(c_{i}\), i.e., \(\text{Pr}_{x\sim D}[c_{i+1}(x)\neq c_{i}(x)]\) is bounded.
* **Controlling the generalization error:** Let \(S_{i+1}\) denote the (re)labeled query points obtained in the \(i\)th round. This is a collection of size \(|S_{i+1}|\approx\left(\frac{|S_{i}|}{\text{VC}(C)}\right)^{2}\). Hence, provided that \(|S_{i}|\gtrsim\left(\text{VC}(C)\right)^{2}\) we get that \(|S_{i+1}|>|S_{i}|\). This allows to lower the accuracy parameters of the non-private learners in each round, and hence ensure that the total error converges.

**Theorem 1.4** (informal version of Theorem 6.1).: _For every concept class \(C\), Algorithm GenericBBL is a private everlasting predictor requiring an initial set of labeled examples which is (upto polylogarithmic factors) quadratic in the VC dimension of \(C\)._

### Related work

Beyond the work of Dwork and Feldman (2018) on private prediction mentioned above, our work is related to private semi-supervised learning and joint differential privacy.

Semi-supervised private learning.As in the model of private semi-supervised learning of Beimel et al. (2021), our predictors depend on both labeled and unlabeled samples. Beyond the difference between outputting a hypothesis and providing black-box prediction, a major difference between the settings is that in the work of Beimel et al. (2021) all samples - labeled and unlabeled - are given at once at the outset of the learning process whereas in the setting of everlasting predictors the unlabeled samples are supplied in an online manner. Our construction of private everlasting predictors uses tools developed for the semi-supervised setting, and in particular Algorithm LabelBoost of of Beimel et al.

Joint differential privacy.Kearns et al. (2015) introduced joint differential privacy (JDP) as a relaxation of differential privacy applicable for mechanism design and games. For every user \(u\), JDP requires that the outputs jointly seen by all _other_ users would preserve differential privacy w.r.t. the input of \(u\). Crucially, in JDP users select their inputs ahead of the computation. In our settings, the inputs to a private everlasting predictor are prediction queries which are chosen in an online manner, and hence a query can depend on previous queries and their answers. Yet, similarly to JDP, the outputs provided to queries not performed by a user \(u\) should jointly preserve differential privacy w.r.t. the query made by \(u\). Our privacy requirement hence extends JDP to an adaptive online setting.

Additional works on private prediction.Bassily et al. (2018) studied a variant of the private prediction problem where the algorithm takes a labeled sample \(S\) and is then required to answer \(m\) prediction queries (i.e., label a sequence of \(m\) unlabeled points sampled from the same underlying distribution). They presented algorithms for this task with sample complexity \(|S|\gtrsim\sqrt{m}\). This should be contrasted with our model and results, where the sample complexity is independent of \(m\). The bounds presented by Dwork and Feldman (2018) and Bassily et al. (2018) were improved by Dagan and Feldman (2020) and by Nandi and Bassily (2020) who presented algorithms with improved dependency on the accuracy parameter in the agnostic setting.

### Discussion and open problems

We show how to transform any (non-private) learner for the class \(C\) (with sample complexity proportional to the VC dimension of \(C\)) to a private everlasting predictor for \(C\). Our construction is not polynomial time due to the use of Algorithm LabelBoost, and requires an initial set \(S\) of labeled examples which is quadratic in the VC dimension. We leave open the question whether \(|S|\) can be reduced to be linear in the VC dimension and whether the construction can be made polynomial time. A few remarks are in order:

1. While our generic construction is not computationally efficient, it does result in efficient learners for several interesting special cases. Specifically, algorithm LabelBoost can be implemented efficiently whenever given an input sample \(S\) it is possible to efficiently enumerate all possible dichotomies from the target class \(C\) over the points in \(S\). In particular, this is the case for the class of 1-dim threshold functions \(C_{thresh}\), as well as additional classes with constant VC dimension. Another notable example is the class \(C_{thresh}^{enc}\) which intuitively is an "encrypted" version of \(C_{thresh}\). Bun and Zhandry (2016) showed that (under plausible cryptographic assumptions) the class \(C_{thresh}^{enc}\) cannot be learned privately and efficiently, while it can be learned efficiently non-privately. Our construction can be implemented efficiently for this class. This provides an example where private everlasting prediction can be done efficiently, while (standard) private learning is possible but necessarily inefficient.
2. It is now known that some learning tasks require the produced model to memorize parts of the training set in order to achieve good learning rates, which in particular disallows the learning algorithm from satisfying (standard) differential privacy (Brown et al., 2021). Our notion of private everlasting prediction circumvents this issue, since the model is never publicly released and hence the fact that it must memorize parts of the sample is not of a direct privacy threat. In other words, our work puts forward a private learning model which, in principle, allows memorization. This could have additional applications in broader settings.
3. As mentioned above, in general, private everlasting predictors cannot base their predictions solely on the initial training set, and in this work we choose to rely on the _queries_ presented to the algorithm (in addition to the training set). Our construction can be easily adapted to a setting where the content of the blackbox is updated based on _fresh unlabeled samples_ (whose privacy would be preserved), instead of relying on the query points themselves. This might be beneficial to avoid poisoning attacks via the queries.

## 2 Preliminaries

### Preliminaries from differential privacy

**Definition 2.1** (\((\epsilon,\delta)\)-indistinguishability).: Let \(R_{0},R_{1}\) be two random variables over the same support. We say that \(R_{0},R_{1}\) are \((\epsilon,\delta)\)-indistinguishable if for every event \(E\) defined over the support of \(R_{0},R_{1}\),

\[\Pr[R_{0}\in E]\leq e^{\epsilon}\cdot\Pr[R_{1}\in E]+\delta\ \ \text{and}\ \ \Pr[R_{1}\in E]\leq e^{\epsilon}\cdot\Pr[R_{0}\in E]+\delta.\]

**Definition 2.2**.: Let \(X\) be a data domain. Two datasets \(x,x^{\prime}\in X^{n}\) are called _neighboring_ if \(|\{i:x_{i}\neq x_{i}^{\prime}\}|=1\).

**Definition 2.3** (differential privacy (Dwork et al., 2006)).: A mechanism \(M:X^{n}\to Y\) is \((\epsilon,\delta)\)-differentially private if \(M(x)\) and \(M(x^{\prime})\) are \((\epsilon,\delta)\)-indistinguishable for all neighboring \(x,x^{\prime}\in X^{n}\).

In our analysis, we use the post-processing and composition properties of differential privacy, that we cite in their simplest forms.

**Proposition 2.4** (post-processing).: _Let \(M_{1}:X^{n}\to Y\) be an \((\epsilon,\delta)\)-differentially private algorithm and \(M_{2}:Y\to Z\) be any algorithm. Then the algorithm that on input \(x\in X^{n}\) outputs \(M_{2}(M_{1}(x))\) is \((\epsilon,\delta)\)-differentially private._

**Proposition 2.5** (composition).: _Let \(M_{1}\) be a \((\epsilon_{1},\delta_{1})\)-differentially private algorithm and let \(M_{2}\) be \((\epsilon_{2},\delta_{2})\)-differentially private algorithm. Then the algorithm that on input \(x\in X^{n}\) outputs \((M_{1}(x),M_{2}(x)\) is \((\epsilon_{1}+\epsilon_{2},\delta_{1}+\delta_{2})\)-differentially private._

**Theorem 2.6** (Advanced composition Dwork et al. (2010)).: _Let \(M_{1},\ldots,M_{k}:X\to Y\) be \((\epsilon,\delta)\)-differentially private algorithms. Then the algorithm that on input \(x\in X\) outputs \((M_{1}(x),\ldots,M_{k}(x))\) is \((\epsilon^{\prime},k\delta+\delta^{\prime})\)-differentially private, where \(\epsilon^{\prime}=\sqrt{2k\ln(1/\delta^{\prime})}\cdot\epsilon\) for every \(\delta^{\prime}>0\)._

**Definition 2.7** (Laplace mechanism (Dwork et al., 2006)).: For \(f:X^{n}\to\mathbb{R}\) Let \(\Delta_{f}=\max(f(x)-f(x^{\prime}))\), where the maximum is taken over all neighboring \(x,x^{\prime}\in X^{n}\). The Laplace mechanism is \(M(x)=f(x)+Y\), where \(Y\) is sampled from the laplace distribution \(\text{Lap}(\Delta_{f}/\epsilon)\). The Laplace mechanism is \((\varepsilon,0)\)-differentially private.

**Definition 2.8** (Exponential mechanism (McSherry and Talwar, 2007)).: Let \(q:X^{n}\times Y\to\mathbb{R}\) be a score function defined over data domain \(X\) and output domain \(Y\). Define \(\Delta=\max(|q(x,y)-q(x^{\prime},y)|)\) where the maximum is taken over all \(y\in Y\) and all neighbouring databases \(x,x^{\prime}\in X^{n}\). The exponential mechanism is the \((\epsilon,0)\)-differentially private mechanism which selects an output \(y\in Y\) with probability proportional to \(e^{\frac{\epsilon\epsilon(x,y)}{2\Delta}}\).

**Claim 2.9** (Privacy amplification by sub-sampling (Kasiviswanathan et al., 2011)).: _Let \(\mathcal{A}\) be an \((\epsilon^{\prime},\delta^{\prime})\)-differentially private algorithm operating on a database of size \(n\). Let \(\epsilon\leq 1\) and let \(t=\frac{n}{\delta}(3+\text{exp}(\epsilon^{\prime}))\). Construct an algorithm \(\mathcal{B}\) operating the database \(D=(z_{i})_{i=1}^{t}\). Algorithm \(\mathcal{B}\) randomly selects a subset \(J\subseteq\{1,2,\ldots,t\}\) of size \(n\), and executes \(\mathcal{A}\) on \(D_{J}=(z_{i})_{i\in J}\). Then \(\mathcal{B}\) is \(\left(\epsilon,\frac{4\epsilon}{3+\text{exp}(\epsilon^{\prime})}\delta^{ \prime}\right)\)-differentially private._

### Preliminaries from PAC learning

A concept class \(C\) over data domain \(X\) is a set of predicates \(c:X\to\{0,1\}\) (called concepts) which label points of the domain \(X\) by either \(0\) or \(1\). A learner \(\mathcal{A}\) for concept class \(C\) is given \(n\) examples sampled i.i.d. from an unknown probability distribution \(\mathcal{D}\) over the data domain \(X\) and labeled according to an unknown target concept \(c\in C\). The learner should output a hypothesis \(h:X\to[0,1]\) that approximates \(c\) for the distribution \(\mathcal{D}\). More formally,

**Definition 2.10** (generalization error).: The _generalization error_ of a hypothesis \(h:X\to[0,1]\) with respect to concept \(c\) and distribution \(\mathcal{D}\) is defined as \(\text{error}_{\mathcal{D}}(c,h)=\text{Exp}_{x\sim\mathcal{D}}[|h(x)-c(x)|]\).

**Definition 2.11** (PAC learning (Valiant, 1984)).: Let \(C\) be a concept class over a domain \(X\). Algorithm \(\mathcal{A}\) is an \((\alpha,\beta,n)\)-_PAC learner_ for \(C\) if for all \(c\in C\) and all distributions \(\mathcal{D}\) on \(X\),

\[\Pr[(x_{1},\ldots,x_{n})\sim\mathcal{D}^{n}\ ;\ h\sim\mathcal{A}((x_{1},c(x_{1})),\ldots,(x_{n},c(x_{n}))\ ;\ \text{error}_{\mathcal{D}}(c,h)\leq\alpha]\geq 1-\beta,\]

where the probability is over the sampling of \((x_{1},\ldots,x_{n})\) from \(\mathcal{D}\) and the coin tosses of \(\mathcal{A}\). The parameter \(n\) is the _sample complexity_ of \(\mathcal{A}\).

See Appendix A for additional preliminaries on PAC learning.

### Preliminaries from private learning

**Definition 2.12** (private PAC learning (Kasiviswanathan et al., 2011)).: Algorithm \(\mathcal{A}\) is a \((\alpha,\beta,\epsilon,\delta,n)\)-private PAC learner if (i) \(\mathcal{A}\) is an \((\alpha,\beta,n)\)-PAC learner and (ii) \(\mathcal{A}\) is \((\epsilon,\delta)\) differentially private.

Kasiviswanathan et al. (2011) provided a generic private learner with \(O(\log(|X|))\) labeled samples.7Beimel et al. (2013) introduced the representation dimension and showed that any concept class \(C\) can be privately learned with \(\Theta(\operatorname{RepDim}(C))\) samples. For the sample complexity of \((\epsilon,\delta)\)-differentially private learning of threshold functions over domain \(X\), Bun et al. (2015) gave a lower bound of \(\Omega(\log^{*}|X|)\). Recently, Cohen et al. (2022) gave a (nearly) matching upper bound of \(\tilde{O}(\log^{*}|X|)\).

Footnote 7: We omit the dependency on \(\epsilon,\delta,\alpha,\beta\) in this brief review.

## 3 Towards private everlasting prediction

In this work, we extend private prediction to answering any sequence of prediction queries - unlimited in length. Our main goal is to present a generic private everlasting predictor with low training sample complexity \(|S|\).

**Definition 3.1** (everlasting prediction).: Let \(\mathcal{A}\) be an algorithm with the following properties:

1. Algorithm \(\mathcal{A}\) receives as input \(n\) labeled examples \(S=\{(x_{i},y_{i})\}_{i=1}^{n}\in(X\times\{0,1\})^{n}\) and selects a hypothesis \(h_{0}:X\to\{0,1\}\).
2. For round \(r\in\mathbb{N}\), algorithm \(\mathcal{A}\) gets a query, which is an unlabeled element \(x_{n+r}\in X\), outputs \(h_{r-1}(x_{n+r})\) and selects a hypothesis \(h_{r}:X\to\{0,1\}\).

We say that \(\mathcal{A}\) is an \((\alpha,\beta,n)\)-_everlasting predictor_ for a concept class \(C\) over a domain \(X\) if the following holds for every concept \(c\in C\) and for every distribution \(\mathcal{D}\) over \(X\). If \(x_{1},x_{2},\ldots\) are sampled i.i.d. from \(\mathcal{D}\), and the labels of the \(n\) initial samples \(S\) are correct, i.e., \(y_{i}=c(x_{i})\) for \(i\in[n]\), then \(\Pr\left[\exists r\geq 0\text{ s.t. error}_{\mathcal{D}}(c,h_{r})>\alpha\right]\leq\beta\), where the probability is over the sampling of \(x_{1},x_{2},\ldots\) from \(\mathcal{D}\) and the randomness of \(\mathcal{A}\).

Applying the Dwork-Feldman notion of a private prediction interface to everlasting predictors we get:

**Definition 3.2**.: An algorithm \(\mathcal{A}\) is an \((\alpha,\beta,\epsilon,\delta,n)\)-everlasting differentially private prediction interface if (i) \(\mathcal{A}\) is a \((\epsilon,\delta)\)-differentially private prediction interface \(M\) (as in Definition 1.1), and (ii) \(\mathcal{A}\) is an \((\alpha,\beta,n)\)-everlasting predictor.

As a warmup, consider an \((\alpha,\beta,\epsilon,\delta,n)\)-everlasting differentially private prediction interface \(\mathcal{A}\) for concept class \(C\) over (finite) domain \(X\) (as in Definition 3.2 above). Assume that \(\mathcal{A}\) does not vary its hypotheses, i.e. (in the language of Definition 3.1) \(h_{r}=h_{0}\) for all \(r>0\).8 Note that a computationally unlimited adversarial querying algorithm can recover the hypothesis \(h_{0}\) by issuing all queries \(x\in X\). Hence, in using \(\mathcal{A}\) indefinitely we lose any potential benefits to sample complexity of restricting access to \(h_{0}\) to being black-box and getting to the point where the lower-bounds on \(n\) from private learning apply. A consequence of this simple observation is that a generic private everlasting predictor should answer all prediction queries with a single hypothesis - it should modify its hypothesis over time as it processes new queries.

Footnote 8: Formally, \(\mathcal{A}\) can be thought of as two mechanisms \(\mathcal{A}=(M_{0},M_{1})\) where \(M_{0}\) is \((\epsilon,\delta)\)-differentially private. (i) On input a labeled training sample \(S\) mechanism \(M_{0}\) computes a hypothesis \(h_{0}\). (ii) On a query \(x\in X\) mechanism \(M_{1}\) replies \(h_{0}(x)\).

We now take this observation a step further, showing that a private everlasting predictor that answers prediction queries solely based on its training sample \(S\) (and randomness, but not on the queries) is subject to the same sample complexity lowerbounds as private learners.

Consider an \((\alpha,\beta<1/8,\epsilon,\delta,n)\)-everlasting differentially private prediction interface \(\mathcal{A}\) for concept class \(C\) over (finite) domain \(X\) that upon receiving the training set \(S\in(X\times\{0,1\})^{n}\) selects an infinite sequence of hypotheses \(\{h_{r}\}_{r\geq 0}\) where \(h_{r}:X\rightarrow\{0,1\}\). Formally, we can think of \(\mathcal{A}\) as composed of three mechanisms \(\mathcal{A}=(M_{0},M_{1},M_{2})\) where \(M_{0}\) is \((\epsilon,\delta)\)-differentially private:

* On input a labeled training sample \(S\in(X\times\{0,1\})^{n}\) mechanism \(M_{0}\) computes an initial state and an initial hypothesis \((\sigma_{0},h_{0})=M_{0}(S)\).
* On a query \(x_{n+r}\) mechanism \(M_{1}\) produces an answer \(M_{1}(x_{n+r})=h_{i}(x_{n+r})\) and mechanism \(M_{2}\) updates the hypothesis-state pair \((h_{r+1},\sigma_{r+1})=M_{2}(\sigma_{r})\).

Note that as \(M_{0}\) and \(M_{2}\) do not receive the sequence \(\{x_{n+r}\}_{r\geq 0}\) as input, the sequence \(\{h_{r}\}_{r\geq 0}\) depends solely on \(S\). Furthermore as \(M_{1}\) and \(M_{2}\) post-process the outcome of \(M_{0}\), i.e., the sequence of queries and predictions \(\{(x_{r},h_{r}(x_{r}))\}_{r\geq 0}\) preserves \((\epsilon,\delta)\)-differential privacy with respect to the training set \(S\). In Appendix B we prove:

**Theorem 3.3**.: \(\mathcal{A}\) _can be transformed into a \((O(\alpha),O(\beta),\epsilon,\delta,O(n\log(1/\beta))\)-private PAC learner for \(C\)._

## 4 Private everlasting prediction - definition

Theorem 3.3 requires us to seek private predictors whose prediction relies on more information than what is provided by the initial labeled sample. Possibilities include requiring the input of additional labeled or unlabeled examples during the lifetime of the predictor, while protecting the privacy of these examples.

We choose to rely on the queries for updating the predictor's internal state. This introduces a potential privacy risk for these queries as sensitive information about a query may be leaked in the predictions following it. Furthermore, we need take into account that a privacy attacker may choose their queries adversarially and adaptively.

**Definition 4.1** (private everlasting black-box prediction).: An algorithm \(\mathcal{A}\) is an \((\alpha,\beta,\varepsilon,\delta,n)\)-private everlasting black-box predictor for a concept class \(C\) if

1. **Prediction:**\(\mathcal{A}\) is an \((\alpha,\beta,n)\)-everlasting predictor for \(C\) (as in Definition 3.1).
2. **Privacy:** For every adversary \(\mathcal{B}\) and every \(t\geq 1\), the random variables \(\mathrm{View}^{0}_{\mathcal{B},t}\) and \(\mathrm{View}^{1}_{\mathcal{B},t}\) (defined in Figure 1) are \((\varepsilon,\delta)\)-indistinguishable.

## 5 Tools from prior works

We briefly describe tools from prior works that we use in our construction. See Appendix C for a more detailed account.

Algorithm LabelBoost[Beimel et al., 2021]:Algorithm LabelBoost takes as input a partially labeled database \(S\circ T\in(X\times\{0,1,\bot\})^{*}\) (where the first portion of the database, \(S\), contains examples by some concept \(\epsilon\in C\)) and outputs a similar database where both \(S\) and \(T\) are (re)labeled by a concept \(h\in C\) such that \(\mathrm{error}_{D}(c,h)\) is bounded. We use the following lemmata from Beimel et al. (2021):

**Lemma 5.1** (privacy of Algorithm LabelBoost).: _Let \(\mathcal{A}\) be an \((\epsilon,\delta)\)-differentially private algorithm operating on labeled databases. Construct an algorithm \(\mathcal{B}\) that on input a partially labeled database \(S\circ T\in(X\times\{0,1,\bot\})^{*}\) applies \(\mathcal{A}\) on the outcome of LabelBoost\((S\circ T)\). Then, \(\mathcal{B}\) is \((\epsilon+3,4\epsilon\delta)\)-differentially private._

**Lemma 5.2** (Utility of Algorithm LabelBoost).: _Fix \(\alpha\) and \(\beta\), and let \(S\circ T\) be s.t. \(S\) is labeled by some target concept \(c\in C\), and \(s.t.\)\(|T|\leq\frac{\beta}{\epsilon}\mathrm{VC}(C)\exp(\frac{a|S|}{2\mathrm{VC}(C)} )-|S|\). Consider the execution of LabelBoost on S\(\circ T\), and let \(h\) denote the hypothesis chosen by LabelBoost to relabel \(S\circ T\). With probability at least \((1-\beta)\) we have that \(\mathrm{error}_{S}(h)\leq\alpha\)._

## 6 A Generic Construction

Our generic construction Algorithm GenericBBL transforms a (non-private) learner for a concept class \(C\) into a private everlasting predictor for \(C\). The theorem below follows from Theorem 6.2 and Claim 6.3 which are proved in Appendix E.

**Theorem 6.1**.: _Given \(\alpha,\beta,\delta<1/16,\epsilon<1\), Algorithm GenericBBL is a \((4\alpha,4\beta,\epsilon,\delta,n)\)-private everlasting predictor, where \(n\) is set as in Algorithm GenericBBL._

**Theorem 6.2** (accuracy of algorithm GenericBBL).: _Given \(\alpha,\beta,\delta<1/16,\ \epsilon<1\), for any concept \(c\) and any round \(r\), algorithm GenericBBL can predict the label of \(x_{r}\) as \(h_{r}(x_{r})\), such that \(\Pr[error_{\mathcal{D}}(c(x_{r})\neq h_{r}(x_{r}))\leq 4\alpha]\geq 1-4\beta\)._

**Claim 6.3** (privacy of algorithm GenericBBL).: _GenericBBL is \((\epsilon,\delta)\)-differentially private._

**Remark 6.4**.: _For simplicity, we analyzed GenericBBL in the realizable setting, i.e., under the assumption that the training set \(S\) is consistent with the target class \(C\). Our construction carries over to the agnostic setting via standard arguments (ignoring computational efficiency). We refer the reader to [Beimel et al., 2021] and [Alon et al., 2020] for generic agnostic-to-realizable reductions in the context of private learning._

### Improving the sample complexity dependency on accuracy

We briefly sketch how to improve the sample complexity of Algorithm GenericBBL from \(n=\tilde{\Theta}\left(\frac{\operatorname{VC}^{2}(C)}{\alpha^{2}\epsilon^{2 }}\right)\) to \(n=\tilde{\Theta}\left(\frac{\operatorname{VC}^{2}(C)}{\alpha\epsilon^{2}}\right)\) by modifying steps 3(d)ii and 3(d)iii of Algorithm GenericBBL. To simplify the description, we consider a constant \(\epsilon\), we ignore the privacy amplification by subsampling occurring in steps 2 and 3f and illustrate how the modified algorithm would execute by considering round \(i=1\) of Algorithm GenericBBL.

Note that \(S_{1}=S\) where \(n=|S_{1}|=T_{1}\cdot\lambda_{1}\) and \(\lambda_{1}=\tilde{\Theta}(\operatorname{VC}(C)/\alpha_{1})\) is the sample complexity needed such that each of the hypotheses computed in Step 2(b) has error \(\alpha_{1}\) except for

Figure 1: Definition of \(\operatorname{View}^{0}_{\mathcal{B},t}\) and \(\operatorname{View}^{1}_{\mathcal{B},t}\).

probability \(\beta_{i}\). Applying advanced composition, we get that \(\tilde{\Theta}(T_{1}^{2})\) noisy majority queries implemented in steps steps 3(d)ii and 3(d)iii can be performed, i.e., \(R_{1}=\tilde{\Theta}(T_{1}^{2})\). Finally, to feed the next phase with \(|S_{2}|=\Theta(|S_{1}|)\) labeled samples, we need that \(R_{1}=\tilde{\Theta}(T_{1}\cdot\lambda_{1})=\tilde{\Theta}(T_{1}\cdot\text{ VC}(C)/\alpha)\) and hence \(T_{1}=\tilde{\Theta}(VC(C)/\alpha)\) resulting in \(n=\tilde{\Theta}(VC^{2}(C)/\alpha^{2})\).

However, when queries are made from the same underlying distribution \(S\) was selected, we expect that most of them would exhibit a clear majority in steps 3(d)ii and 3(d)iii, except for a fraction of \(O(\alpha)\). Hence a natural way to improve on the number of majority queries that can be performed is to replace steps 3(d)ii and 3(d)iii with a decision based on the sparse vector technique of Dwork et al. (2009). In particular, we can use the BetweenThresholds mechanism of Bun et al. (2016) to get an improved \(R_{1}=\tilde{\Theta}(T_{1}^{2}/\alpha)\) and hence get \(T_{1}=\tilde{\Theta}(VC(C))\) and \(n=\tilde{\Theta}(VC^{2}/\alpha)\).

A final important wrinkle is that the above calculation is based on the queries coming from the same underlying distribution \(S\) was selected, while our worst-case privacy requirement allows for an adversarial choice of queries that may in turn cause the execution of BetweenThresholds to halt too often and hence exhaust the privacy budget for the phase. It is hence important to estimate the number of times BetweenThresholds halts within a phase and to stop the execution of Algorithm GenericBBL when the estimate crosses a threshold. This estimate needs to be done in an online manner and should preserve differential privacy with respect to the queries.9 This can be done, e.g., using the _private counter_ algorithm of Dwork et al. (2010), which preserves differential privacy under continual observation.

Footnote 9: For example, stopping GenericBBL after the count of halts crosses a precise threshold would reveal that the last query caused BetweenThresholds to halt, and hence breach differential privacy.

## References

* Alon et al. (2019) Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite littestone dimension. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 852-860. ACM, 2019. doi: 10.1145/3313276.3316312. URL https://doi.org/10.1145/3313276.3316312.
* Alon et al. (2020) Noga Alon, Amos Beimel, Shay Moran, and Uri Stemmer. Closure properties for private classification and online prediction. In _COLT_, volume 125 of _Proceedings of Machine Learning Research_, pages 119-152. PMLR, 2020.
* Bassily et al. (2018) Raef Bassily, Abhradeep Guha Thakurta, and Om Dipakbhai Thakkar. Model-agnostic private learning. In _NeurIPS_, pages 7102-7112, 2018.
* Beimel et al. (2013a) Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. In _ITCS_, pages 97-110. ACM, 2013a.
* Beimel et al. (2013b) Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. In _APPROX-RANDOM_, pages 363-378, 2013b.
* Beimel et al. (2021) Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled examples. _Algorithmica_, 83(1):177-215, 2021.
* Brown et al. (2021) Gavin Brown, Mark Bun, Vitaly Feldman, Adam D. Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In _STOC_, pages 123-132. ACM, 2021.
* Bun and Zhandry (2016) Mark Bun and Mark Zhandry. Order-revealing encryption and the hardness of private learning. In _TCC (A1)_, volume 9562 of _Lecture Notes in Computer Science_, pages 176-206. Springer, 2016.
* Bun et al. (2015) Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and learning of threshold functions. In _FOCS_, pages 634-649, 2015.
* Bun et al. (2016) Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in differential privacy. _CoRR_, abs/1604.04618, 2016. URL http://arxiv.org/abs/1604.04618.
* Cohen et al. (2022) Edith Cohen, Xin Lyu, Jelani Nelson, Tamas Sarlos, and Uri Stemmer. Optimal differentially private learning of thresholds and quasi-concave optimization. _CoRR_, abs/2211.06387, 2022. doi: 10.48550/arXiv.2211.06387. URL https://doi.org/10.48550/arXiv.2211.06387.
* Dagan and Feldman (2020) Yuval Dagan and Vitaly Feldman. PAC learning with stable and private predictions. In _COLT_, volume 125 of _Proceedings of Machine Learning Research_, pages 1389-1410. PMLR, 2020.
* Dwork and Feldman (2018) Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, volume 75 of _Proceedings of Machine Learning Research_, pages 1693-1702. PMLR, 2018. URL http://proceedings.mlr.press/v75/dwork18a.html.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _TCC_, pages 265-284, 2006.
* Dwork et al. (2009) Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In _STOC_, pages 381-390, 2009.
* Dwork et al. (2010a) Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual observation. In _Symposium on Theory of Computing (STOC)_, pages 715-724. ACM, 2010a.
* Dwork et al. (2010b)Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In _FOCS_, pages 51-60, 2010b.
* Feldman and Xiao (2015) Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. _SIAM J. Comput._, 44(6):1740-1764, 2015. doi: 10.1137/140991844. URL http://dx.doi.org/10.1137/140991844.
* Kaplan et al. (2020) Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. In _COLT_, volume 125 of _Proceedings of Machine Learning Research_, pages 2263-2285. PMLR, 2020.
* Kasiviswanathan et al. (2011) Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? _SIAM J. Comput._, 40(3):793-826, 2011.
* Kearns et al. (2015) Michael J. Kearns, Mallesh M. Pai, Ryan M. Rogers, Aaron Roth, and Jonathan R. Ullman. Robust mediators in large games. _CoRR_, abs/1512.02698, 2015.
* McSherry and Talwar (2007) Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In _FOCS_, pages 94-103. IEEE, Oct 20-23 2007.
* Nandi and Bassily (2020) Anupama Nandi and Raef Bassily. Privately answering classification queries in the agnostic PAC model. In _ALT_, volume 117 of _Proceedings of Machine Learning Research_, pages 687-703. PMLR, 2020.
* Valiant (1984) L. G. Valiant. A theory of the learnable. _Commun. ACM_, 27(11):1134-1142, November 1984. ISSN 0001-0782. doi: 10.1145/1968.1972. URL http://doi.acm.org/10.1145/1968.1972.
* Vapnik and Chervonenkis (1971) Vladimir N. Vapnik and Alexey Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and its Applications_, 16(2):264-280, 1971.

Additional Preliminaries from PAC Learning

It is well know that that a sample of size \(\Theta(\operatorname{VC}(C))\) is necessary and sufficient for the PAC learning of a concept class \(C\), where the Vapnik-Chervonenkis (VC) dimension of a class \(C\) is defined as follows:

**Definition A.1** (VC-Dimension (Vapnik and Chervonenkis, 1971)).: Let \(C\) be a concept class over a domain \(X\). For a set \(B=\{b_{1},\ldots,b_{\ell}\}\subseteq X\) of \(\ell\) points, let \(\Pi_{C}(B)=\{(c(b_{1}),\ldots,c(b_{\ell})):c\in C\}\) be the set of all dichotomies that are realized by \(C\) on \(B\). We say that the set \(B\subseteq X\) is _shattered_ by \(C\) if \(C\) realizes all possible dichotomies over \(B\), in which case we have \(|\Pi_{C}(B)|=2^{|B|}\).

The VC dimension of the class \(C\), denoted \(\operatorname{VC}(C)\), is the cardinality of the largest set \(B\subseteq X\) shattered by \(C\).

**Theorem A.2** (VC bound).: _Let \(C\) be a concept class over a domain \(X\). For \(\alpha,\beta<1/2\), there exists an \((\alpha,\beta,n)\)-PAC learner for \(C\), where \(n=\frac{8\operatorname{VC}(C)\log(\frac{13}{\alpha})+4\log(\frac{2}{\beta})}{\alpha}\)._

## Appendix B Proof of Theorem 3.3

The proof of Theorem 3.3 follows from algorithms HypothesisLearner, AccuracyBoost and claims B.1, B.2, all described below.

In Algorithm HypothesisLearner we assume that the everlasting differentially private prediction interface \(\mathcal{A}\) was fed with \(n\) i.i.d. samples taken from some (unknown) distribution \(\mathcal{D}\) and labeled by an unknown concept \(c\in C\). Assuming the sequence of hypotheses \(\{h_{r}\}_{r\geq 0}\) produced by \(\mathcal{A}\) satisfies

\[\forall r\ \ \operatorname{error}_{\mathcal{D}}(c,h_{r})\leq\alpha\] (1)

we use it to construct - with constant probability - a hypothesis \(h\) with error bounded by \(O(\alpha)\).

``` Parameters:\(0<\beta\leq 1/8\), \(R=|X|\cdot\log(|X|)\cdot\log(1/\beta)\) Input: hypothesis sequence \(\{h_{r}\}_{r\geq 0}\)  1. for all \(x\in X\) let \(L_{x}=\emptyset\)  2. for \(r=0,1,2,\ldots,R\)  1. select \(x\) uniformly at random from \(X\) and let \(L_{x}=L_{x}\cup\{h_{r}(x)\}\)  3. if \(L_{x}=\emptyset\) for some \(x\in X\) then fail, output an arbitrary hypothesis, and halt  /* \(\Pr[\exists x\text{ such that }L_{x}=\emptyset]\leq|X|(1-\frac{1}{|X|})^{R} \approx|X|c^{-R/|X|}=\beta\ ^{*}/\)  4. for all \(x\in X\) let \(r_{x}\) be sampled uniformly at random from \(L_{x}\)  5. construct the hypothesis \(h\), where \(h(x)=r_{x}\) ```

**Algorithm**HypothesisLearner

**Claim B.1**.: _If executed on a hypothesis sequence satisfying Equation 1 then with probability at least \(3/4\) Algorithm HypothesisLearner outputs a hypothesis \(h\) satisfying \(\operatorname{error}_{\mathcal{D}}(c,h)\leq 8\alpha\)._

Proof.: Having \(\mathcal{D},c\in C\) fixed, and given a hypothesis \(h\), we define \(e_{h}(x)\) to be \(1\) if \(h(x)\neq c(x)\) and \(0\) otherwise. Thus, we can write \(\operatorname{error}_{\mathcal{D}}(c,h)=\mathbb{E}_{x\sim\mathcal{D}}[e_{h}(x)]\).

Observe that when Algorithm HypothesisLearner does not fail, \(r_{x}\) (and hence \(h(x)\)) is chosen with equal probability among \((h_{1}(x),h_{2}(x),\ldots,h_{R}(x))\) and hence \(\mathbb{E}_{\theta}[e_{h}(x)]=\mathbb{E}_{i\in_{R}[R]}[e_{h_{i}}(x)]\) where \(\theta\) denotes the randomness of HypothesisLearner. We get:

\[\mathbb{E}_{\theta}[\operatorname{error}_{\mathcal{D}}(c,h)] = \mathbb{E}_{c\sim\mathcal{D}}[e_{h}(x)]=\mathbb{E}_{x\sim\mathcal{ D}}\mathbb{E}_{\theta}[e_{h}(x)]\] \[= \mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{i\in_{R}[R]}[e_{h_{i}}(x)]= \mathbb{E}_{i\in_{R}[R]}\mathbb{E}_{x\sim\mathcal{D}}[e_{h_{i}}(x)]\] \[\leq \mathbb{E}_{i\sim\mathcal{R}}[\alpha]=\alpha.\]

By Markov inequality, we have \(\Pr_{\theta}[\operatorname{error}_{\mathcal{D}}(c,h)\geq 8\alpha]\leq 1/8\). The claim follows noting that Algorithm HypothesisLearner fails with probability at most \(\beta\leq 1/8\).

The second part of the transformation is Algorithm AccuracyBoost that applies Algorithm HypothesisLearner \(O(\log(1/\beta))\) times to obtain with high probability a hypothesis with \(O(\alpha)\) error.

``` Parameters:\(\beta\), \(R=104\ln\frac{1}{\beta}\) Input:\(R\) labeled samples with \(n\) examples each \((S_{1},\dots,S_{R})\) where \(S_{i}\in(X\times\{0,1\})^{n}\)
1. for\(i=1,2\dots R\) 1. execute \(\mathcal{A}(S_{i})\) to obtain a hypothesis sequence \(\{h^{i}_{r}\}_{r\geq 0}\) 2. execute Algorithm WeakHypothesisLearner on \(\{h^{i}_{r}\}_{r\geq 0}\) to obtain hypothesis \(h^{i}\)
2. construct the hypothesis \(\hat{h}\), where \(\hat{h}(x)=\operatorname{maj}(h^{1}(x),\dots,h^{R}(x))\). ```

**Algorithm**AccuracyBoost

**Claim B.2**.: _With probability \(1-\beta\), Algorithm AccuracyBoost output a \(24\alpha\)-good hypothesis over distribution \(\mathcal{D}\)._

Proof.: Define \(B_{i}\) to be the event where the sequence of hypotheses \(\{h^{i}_{r}\}_{r\geq 0}\) produced in Step 1a of AccuracyBoost does not satisfy Equation 1. We have,

\[\Pr[\operatorname{error}_{\mathcal{D}}(c,h_{i})>8\alpha]\leq\Pr[B]+(1-\Pr[B] )\cdot\Pr[\operatorname{error}_{\mathcal{D}}(c,h)>8\alpha]\leq\beta+1/4<3/8.\]

Hence, by the Chernoff bound, when \(R\geq 104\ln\frac{1}{\beta}\), we have at least \(7R/8\) hypotheses are \(8\alpha\)-good over distribution \(\mathcal{D}\). Consider the worst case, in which \(R/8\) hypotheses always output wrong labels. To output a wrong label of \(x\), we require at least \(3R/8\) hypotheses to output wrong labels. Thus \(h\) is \(24\alpha\)-good over distribution \(\mathcal{D}\). 

## Appendix C Tools from Prior Works

### Algorithm LabelBoost [Beimel et al., 2021]

``` Parameters: A concept class \(C\). Input: A partially labeled database \(SoT\in(X\times\{0,1,\bot\})^{*}\). % We assume that the first portion of the database (denoted \(S\)) contains labeled examples. The algorithm outputs a similar database where both \(S\) and \(T\) are (re)labeled. 1. Initialize \(H=\emptyset\). 2. Let \(P=\{p_{1},\dots,p_{\ell}\}\) be the set of all points \(p\in X\) appearing at least once in \(SoT\). Let \(\Pi_{C}(P)=\{(c(p_{1}),\dots,c(p_{\ell})):c\in C\}\) be the set of all dichotomies generated by \(C\) on \(P\). 3. For every \((z_{1},\dots,z_{\ell})\in\Pi_{C}(P)\), add to \(H\) an arbitrary concept \(c\in C\) s.t. \(c(p_{i})=z_{i}\) for every \(1\leq i\leq\ell\). 4. Choose \(h\in H\) using the exponential mechanism with privacy parameter \(\epsilon\)=1, solution set \(H\), and the database \(S\). 5. (Re)label \(SoT\) using \(h\), and denote the resulting database \((SoT)^{h}\), that is, if \(SoT=(x_{i},y_{i})_{i=1}^{t}\) then \((SoT)^{h}=(x_{i},y_{i})_{i=1}^{t}\) where \(y_{i}^{\prime}=h(x_{i})\). 6. Output \((SoT)^{h}\). ```

**Algorithm**AccuracyBoost

**Lemma C.1** (privacy of Algorithm LabelBoost [Beimel et al., 2021]).: _Let \(\mathcal{A}\) be an \((\epsilon,\delta)\)-differentially private algorithm operating on partially labeled databases. Construct an algorithm \(\mathcal{B}\) that on input a partially labeled database \(SoT\in(X\times\{0,1,\bot\})^{*}\) applies \(\mathcal{A}\) on the outcome of LabelBoos\((SoT)\). Then, \(\mathcal{B}\) is \((\epsilon+3,4e\delta)\)-differentially private._Consider an execution of LabelBoost on a database \(S\circ T\), and assume that the examples in \(S\) are labeled by some target concept \(c\in C\). Recall that for every possible labeling \(\vec{z}\) of the elements in \(S\) and in \(T\), algorithm LabelBoost adds to \(H\) a hypothesis from \(C\) that agrees with \(\vec{z}\). In particular, \(H\) contains a hypothesis that agrees with the target concept \(c\) on \(S\) (and on \(T\)). That is, \(\exists f\in H\) s.t. errors\((f)=0\). Hence, the exponential mechanism (on Step 4) chooses (w.h.p.) a hypothesis \(h\in H\) s.t. errors\((h)\) is small, provided that \(|S|\) is roughly \(\log|H|\), which is roughly \(\mathrm{VC}(C)\cdot\log(|S|+|T|)\) by Sauer's lemma. So, algorithm LabelBoost takes an input database where only a small portion of it is labeled, and returns a similar database in which the labeled portion grows exponentially.

**Lemma C.2** (utility of Algorithm LabelBoost[Beimel et al., 2021]).: _Fix \(\alpha\) and \(\beta\), and let \(S\circ T\) be s.t. \(S\) is labeled by some target concept \(c\in C\), and s.t._

\[|T|\leq\frac{\beta}{e}\mathrm{VC}(C)\exp(\frac{\alpha|S|}{2\mathrm{VC}(C)})-|S|.\]

_Consider the execution of LabelBoost on \(S\circ T\), and let \(h\) denote the hypothesis chosen on Step 4. With probability at least \((1-\beta)\) we have that \(\mathrm{error}_{S}(h)\leq\alpha\)._

## Appendix D Some Technical Facts

We refer to the execution of steps 3a-3g of algorithm GenericBBL as a _phase_ of the algorithm, indexed by \(i=1,2,3,\dots\).

In GenericBBL, we require that the set of all predictions in the phase \(i\) is \((1,\delta)\)-differentially private.

**Claim D.1**.: _For \(\delta<1\), the composited Laplace mechanism used in step 3c in the \(i\)-th iteration, is \((1,\delta)\)-differentially private._

Proof.: Let \(\epsilon^{\prime}_{i},\delta^{\prime}_{i}\) be as in Step 3c. Since \(\epsilon^{\prime}_{i}-1<2\epsilon^{\prime}_{i}\) for \(0<\epsilon^{\prime}_{i}<1\), we have

\[\sqrt{2R_{i}\ln(\frac{1}{R_{i}\delta^{\prime}_{i}})}\cdot\epsilon^{\prime}_{i} +R_{i}\epsilon^{\prime}_{i}(\epsilon^{\prime}_{i}-1)\leq\sqrt{2R_{i}\ln(\frac{ 2}{\delta})}\cdot\epsilon^{\prime}_{i}+2R_{i}{\epsilon^{\prime}_{i}}^{2}= \frac{\sqrt{2}}{3}+\frac{2}{9\ln(\frac{2}{\delta})}\leq 1.\]

The proof is concluded by using advanced composition (Theorem 2.6). 

In step 3f, GenericBBL takes a random subset of size \(\lambda_{i+1}T_{t+1}\) from \(\hat{D^{\prime}_{i}}\). We show that the size of \(\hat{D^{\prime}_{i}}\) is at least \(\lambda_{i+1}T_{t+1}\).

**Claim D.2**.: _When \(\varepsilon\leq 1\), for any \(i\geq 1\), we always have \(|\hat{D^{\prime}_{i}}|\geq\lambda_{i+1}T_{i+1}\)._

Proof.: Let \(m=3+\exp(\varepsilon+4)<200\). By the step 3c, step 3e and step 3f, \(|\hat{D}_{j}|=\frac{\varepsilon|D_{j}|}{m}=\frac{12800|S_{j}|}{m}\geq 64|S_{j}|=64 \lambda_{j}T_{j}\). Then it is sufficient to verify \(128\lambda_{j}T_{j}\geq\lambda_{j+1}T_{j+1}\)

We can verify that

\[4\lambda_{j}=4\cdot\frac{8\mathrm{VC}(C)\log(\frac{13}{\alpha_{i}})+4\log( \frac{2}{\beta_{i}})}{\alpha_{i}}=4\cdot\frac{8\mathrm{VC}(C)(\log(\frac{13} {\alpha_{j+1}})-1)+4(\log(\frac{2}{\beta_{j+1}})-1)}{2\alpha_{j+1}}\geq\lambda _{j+1}\]

and

\[16T_{j}=\frac{16\tau\cdot\lambda_{i}\cdot\log(\frac{1}{\delta})\cdot\log^{2}( \frac{\lambda_{i}}{\varepsilon\alpha_{i}\beta_{i}})}{\varepsilon}\geq\frac{4 \tau\cdot\lambda_{i+1}\cdot\log(\frac{1}{\delta})\cdot\log^{2}(\frac{\lambda_{ i+1}}{16\varepsilon\alpha_{i+1}\beta_{i+1}\beta})}{\varepsilon}\geq T_{j+1}.\]

The last inequalitu holds because \(\lambda_{j}\geq 4\) and \(\alpha_{j},\beta_{j}\leq 1/2\). 

To apply the privacy and accuracy of \(LabelBoost\), the sizes of the databases need to satisfy the inequalities in lemma C.2. We verify that in each phase, the sizes of the databases always satisfy the requirement.

**Claim D.3**.: _When \(\varepsilon\leq 1\), for any \(i\geq 1\), we have \(|\hat{D}_{i}|\leq\frac{\beta_{i}}{\varepsilon}\mathrm{VC}(C)\text{exp}\left( \frac{\alpha_{i}|\hat{S}_{i}|}{2\mathrm{VC}(C)}\right)-|\hat{S}_{i}|\)._

Proof.: By claim D.2, step 3c and step 3f,

\[|\hat{D}_{i}|=\frac{\varepsilon|D_{i}|}{m}=O\left(\lambda_{i}T_{i}\right)=O \left(\mathrm{VC}(C)\log^{2}(\mathrm{VC}(C))\cdot\text{poly}\left(\frac{1}{ \alpha_{i}},\log(\frac{1}{\beta_{i}}),\frac{1}{\varepsilon},\log(\frac{1}{ \delta})\right)\right)\]

and

\[|\hat{S}_{i}| =\frac{\varepsilon|S_{i}|}{m}\] (2) \[=O\left(\varepsilon\lambda_{i}T_{i}\right)=O\left(\lambda_{i}T_{i}\right)\] \[=O\left(\mathrm{VC}(C)\log^{2}(\mathrm{VC}(C))\cdot\text{poly} \left(\frac{1}{\alpha_{i}},\log(\frac{1}{\beta_{i}}),\frac{1}{\varepsilon}, \log(\frac{1}{\delta})\right)\right).\]

Note that

\[\frac{\beta_{i}}{\varepsilon}\mathrm{VC}(C)\text{exp}\left(\frac{\alpha_{i}| \hat{S}_{i}|}{2\mathrm{VC}(C)}\right)=\Omega\left(\mathrm{VC}^{2}(C)\cdot \text{exp}\left(\text{poly}\left(\frac{1}{\alpha_{i}},\log(\frac{1}{\beta_{i}} ),\frac{1}{\varepsilon},\log(\frac{1}{\delta})\right)\right)\right),\]

for \(T_{i}=\frac{\tau\cdot\lambda_{i}\cdot\log(\frac{1}{\delta})\log^{2}(\frac{ \lambda_{i}}{\varepsilon\alpha_{i}|\hat{S}_{i}|})}{\varepsilon}\), the inequality holds when \(\tau\geq 50\). 

## Appendix E Accuracy of Algorithm GenericBBL - proof of Theorem 6.2

We refer to the execution of steps 3a-3g of algorithm GenericBBL as a _phase_ of the algorithm, indexed by \(i=1,2,3,\ldots\).

We give some technical facts in Appendix D. In Claim E.1, we show that in each phase, samples are labeled with high accuracy. In Claim E.3, we prove that algorithm GenericBBL fails with low probability. In Claim E.4, we prove that algorithm GenericBBL predict the labels with high accuracy.

**Claim E.1**.: _For each phase \(i\) we have_

\[\Pr\left[\exists g_{i}\in C\text{ s.t. }\mathrm{error}_{S_{i}}(g)=0\text{ and }\mathrm{error}_{D}(g_{i},c)\leq\sum_{j=1}^{i}\alpha_{j}\right]\geq 1-2 \sum_{j=0}^{i}\beta_{j}.\]

Proof.: The proof is by induction on \(i\). The base case for \(i=1\) is trivial, with \(g_{1}=c\). Assume the claim holds for all \(j\leq i\). By the properties of LabelBoost (Lemma C.2) and Claim D.3, with probability at least \(1-\beta_{i}\) we have that \(S_{i}\) is labeled by a hypothesis \(g_{i}\in C\) s.t. \(\mathrm{errors}_{S_{i-1}}(g_{i-1},g_{i})\leq\alpha_{i}\). Observe that the points in \(S_{i}\) (without their labels) are chosen i.i.d. from \(\mathcal{D}\), and hence, By Theorem A.2 (VC bounds) and \(|S_{i-1}|\geq 128\lambda_{i-1}\geq\lambda_{i}\), with probability at least \(1-\beta_{i}\) we have that \(\mathrm{error}_{D}(g_{i},g_{i-1})\leq\alpha_{i}\). Hence, with probability \(1-2\hat{\beta}_{i}\), we have \(\mathrm{error}_{\mathcal{D}}(g_{i},g_{i-1})\leq\alpha_{i}\). Finally, by the triangle inequality, \(\mathrm{error}_{\mathcal{D}}(g_{i},c)\leq\sum_{j=1}^{i}\alpha_{j}\), except with probability \(2\sum_{j=1}^{i}\beta_{j}\) 

Combining claims E.1 union bound, we get:

**Claim E.2**.: _Let \(\mathcal{D}\) be an underlying distribution and let \(c\in C\) be a target concept. Then_

\[\Pr[\forall i\ \exists g_{i}\in C\text{ s.t. }\mathrm{error}_{S_{i}}(g_{i})=0 \text{ and }\mathrm{error}_{\mathcal{D}}(g_{i},c)\leq\alpha]\geq 1-2\beta.\]

For each phase \(i\), if there exists a noise \(\eta_{j}\geq 1/16\) in step 3c, we say the phase \(i\) fails. Define the following good event.

**Event \(E_{1}\):** Phase \(i\) doesn't fail for all \(i\geq 1\).

**Claim E.3**.: _When \(\lambda\geq 1,\varepsilon\leq 1,\alpha\leq 1/2,\beta\leq 1/2,\delta\leq 1/2\), event \(E_{1}\) occurs with probability at least \(1-\beta\)._

Proof.: In each phase \(i\), For each \(\eta_{j}\), by the property of Laplace distribution, we have \(\Pr[|\eta|\geq 1/16]=e^{-T_{i}\epsilon_{i}^{\prime}/16}=e^{-\frac{T_{i}}{48 \sqrt{|T_{i}|}\ln(2/\delta)}}\leq\beta_{i}/R_{i}\). This inequality holds when \(\tau\geq 1\). By union bound, we have \(\Pr[\text{phase $i$ fails}]\leq\beta_{i}\).

Using to union bound,

\[\Pr[\text{Event $E_{1}$ occurs}]\geq 1-\beta.\]

Notations.Consider the \(i\)th phase of Algorithm GenericBEL, and focus on the \(j\)-th iteration of Step 3. Fix all of the randomness of noises. Now observe that the output on step 3(d)iii is a deterministic function of the input \(x_{i,j}\). This defines a hypothesis which we denote as \(h_{i,j}\).

**Claim E.4**.: _For \(\beta<1/16\), with probability at least \(1-4\beta\), all of the hypotheses defined above are \(4\alpha\)-good w.r.t. \(\mathcal{D}\) and \(c\)._

Proof.: In the phase \(i\), by Claim E.2, with probability at least \(1-2\beta\) we have that \(S_{i}\) is labeled by a hypothesis \(g_{i}\in C\) satisfying \(\operatorname{error}_{\mathcal{D}}(g_{i},c)\leq\alpha\). We continue with the analysis assuming that this is the case.

On step 3a of the \(i\)th phase we divide \(S_{i}\) into \(T_{i}\) subsamples of size \(\lambda_{i}\) each, identify a consistent hypothesis \(f_{i}\in C\) for every subsample \(S_{i,t}\), and denote \(F_{i}=(f_{1},\ldots,f_{T})\). By Theorem A.2 (VC bounds), every hypothesis in \(F_{i}\) satisfies \(\operatorname{error}_{\mathcal{D}}(f_{i},g_{i})\leq\alpha\) with probability \(1-\beta_{i}\), in which case, by the triangle inequality we have that \(\operatorname{error}_{\mathcal{D}}(f_{i},c)\leq 2\alpha\).

Set \(T_{i}\geq\frac{512(1-4\beta_{i})\ln(\frac{1}{\beta_{i}})}{(1-64\beta_{i})^{2}}\), using Chernoff bound, it holds that for at least \(15T_{i}/16\) of the hypotheses in \(F_{i}\) have error\({}_{\mathcal{D}}(f_{i},c)\leq 2\alpha\) with probability at least \(1-\beta_{i}\).

By Claim E.3, with probability \(1-\beta\), all Laplace noises added in step 3d is less than \(\frac{T_{i}}{16}\). Let \(m:X\to\{0,1\}\) defined as \(m(x)=\operatorname{maj}_{f_{0}F_{i}}(f_{i}(x))\). For \(m\) to err on a point \(x\) (w.r.t. the target concept \(c\)), it must be that at least \(3/8\)-fraction of the \(2\alpha\)-good hypotheses in \(\hat{F}_{i}\) err on \(x\). Consider the worst case in Figure 3, based on Claim E.3, we have \(\operatorname{error}_{\mathcal{D}}(m,c)\leq 4\alpha\) with probability \(1-\beta\).

Figure 2: Hypothesis \(h_{i,j}\)Conclude all above, with probability \(1-4\beta\), all the hypotheses are \(4\alpha\)-good. 

### Privacy analysis - proof of Claim 6.3

Fix \(t\in\mathbb{N}\) and the adversary \(\mathcal{B}\). We need to show that \(\mathrm{View}^{0}_{\mathcal{B},t}\) and \(\mathrm{View}^{1}_{\mathcal{B},t}\) (defined in Figure 1) are \((\varepsilon,\delta)-indistinguishable\). We will consider separately the case where the executions differ in the training phase (Claim E.5) and the case where the difference occurs during the prediction phase (Claim E.6).

Privacy of the initial training set \(S\).Let \(S^{0},S^{1}\in(X\times\{0,1\})^{n}\) be neighboring datasets of labeled examples and let \(\mathrm{View}^{0}_{\mathcal{B},t}\) and \(\mathrm{View}^{1}_{\mathcal{B},t}\) be as in Figure 1 where \(\left((x^{0}_{1},y^{0}_{1}),\ldots,(x^{0}_{n},y^{0}_{n})\right)=S^{0}\) and \(\left((x^{1}_{1},y^{1}_{1}),\ldots,(x^{1}_{n},y^{1}_{n})\right)=S^{1}\).

**Claim E.5**.: _For all adversaries \(\mathcal{B}\), for all \(t>0\), and for any two neighbouring database \(S^{0}\) and \(S^{1}\) selected by \(\mathcal{B}\), \(\mathrm{View}^{0}_{\mathcal{B},t}\) and \(\mathrm{View}^{1}_{\mathcal{B},t}\) are \((\varepsilon,\delta)\)-indistinguishable._

Proof.: Let \(R^{\prime}_{1}=\min(t,R_{1})\). Note that \(\mathrm{View}^{b}_{\mathcal{B},R^{\prime}_{1}}\) is a prefix of \(\mathrm{View}^{b}_{\mathcal{B},t}\) which includes the labels Algorithm GenericBBL produces in Step 3(d)iii for the \(R^{\prime}_{1}\) first unlabeled points selected by \(\mathcal{B}\). Let \(S^{b}_{2}\) be the result of the first application of algorithm LabelBoost in Step 3f of GenericBBL (if \(t<R_{1}\) we set \(S^{b}_{2}\) as \(\bot\)). The creation of these random variables is depicted in Figure 4, where \(D^{L}_{1}\) denotes the labels Algorithm GenericBBL produces for the unlabeled points \(D_{1}\).

Observe that \(\mathrm{View}^{b}_{\mathcal{B},t}\) results from a post-processing (jointly by the adversary \(\mathcal{B}\) and Algorithm GenericBBL) of the random variable \(\left(\mathrm{View}^{b}_{\mathcal{B},R^{\prime}_{1}},S^{b}_{2}\right)\), and hence it suffices to show that \(\left(\mathrm{View}^{0}_{\mathcal{B},R^{\prime}_{1}},S^{0}_{2}\right)\) and \(\left(\mathrm{View}^{1}_{\mathcal{B},R^{\prime}_{1}},S^{1}_{2}\right)\) are \((\varepsilon,\delta)\)-indistinguishable.

We follow the processes creating \(\mathrm{View}^{b}_{\mathcal{B},t}\) and \(S^{b}_{2}\) in Figure 4: (i) The mechanism \(M_{1}\) corresponds to the loop in Step 3d of GenericBBL where labels are produced for the adver

Figure 3: The horizontal represents the input point. The vertical represents the hypothesis. The red parts represent the incorrect prediction. We let \(\frac{T_{i}}{16}\) hypothesis predict all labels incorrectly and let Laplace noises to be as large as \(\frac{T_{i}}{16}\). To output an incorrect label, there must exist \(\frac{3T_{i}}{8}\) hypothesis output the incorrect label. In the worst case, at most \(4\alpha\) of points are incorrectly classified.

sarily chosen points \(D_{1}^{b}\). By application of claim D.1, \(M_{1}\) is \((1,\delta)\)-differentially private. (ii) The mechanism \(M_{2}\), corresponds to the subsampling of \(\delta_{1}^{b}\) from \(S_{1}^{b}\) and the application of procedure LabelBoost on the subsample in Step 3f of GenericBBL resulting in \(S_{2}^{b}\). By application of Claim 2.9 and Lemma C.1, \(M_{2}\) is \((\varepsilon,0)\)-differentially private. Thus \((M_{1},M_{2})\) is \((\varepsilon+1,\delta)\)-differentially private. (iii) The mechanism \(M_{3}\) with input of \(S^{b}\) and output \(\left(D_{1}^{b,L},S_{2}^{b}\right)=\left(\mathrm{View}_{\mathcal{B},R_{1}^{b} }^{b},S_{2}^{b}\right)\) applies \((M_{1},M_{2})\) on the sub-sample \(S_{1}^{b}\) obtained from \(S^{b}\) in Step 2 of GenericBBL. By application of Claim 2.9\(M_{3}\) is \((\varepsilon,\frac{4\varepsilon\delta}{3+\exp(\varepsilon+1)})\)-differentially private. Since \(\frac{4\varepsilon\delta}{3+\exp(\varepsilon+1)}\leq\delta\) for any \(\varepsilon\), hence \(\left(\mathrm{View}_{\mathcal{B},R_{1}^{b}}^{0},S_{2}^{0}\right)\) and \(\left(\mathrm{View}_{\mathcal{B},R_{1}^{b}}^{1},S_{2}^{1}\right)\) are \((\varepsilon,\delta)\)-indistinguishable 

Privacy of the unlabeled points \(D\).Let \(D^{0},D^{1}\in X^{t}\) be neighboring datasets of unlabeled examples and let \(\mathrm{View}_{\mathcal{B},t}^{0}\) and \(\mathrm{View}_{\mathcal{B},t}^{1}\) be as in Figure 1 where \(\left(x_{1}^{0},\ldots,x_{t}^{0}\right)=D^{0}\) and \(\left(x_{1}^{1},\ldots,x_{t}^{1}\right)=D^{1}\).

**Claim E.6**.: _For all adversaries \(\mathcal{B}\), for all \(t>0\), and for any two neighbouring databases \(D^{0}\) and \(D^{1}\) selected by \(\mathcal{B}\), \(\mathrm{View}_{\mathcal{B},t}^{0}\) and \(\mathrm{View}_{\mathcal{B},t}^{1}\) are \((\varepsilon,\delta)\)-indistinguishable._

Proof.: Let \(D_{1}^{0},D_{2}^{0},\ldots,D_{k}^{0}\) and \(D_{1}^{1},D_{2}^{1},\ldots,D_{k}^{1}\) be the set of unlabeled databases in step 3e of GenericBBL. Without loss of generality, we assume \(D_{i}^{0}\) and \(D_{i}^{1}\) differ on one entry. When

Figure 4: Privacy of the labeled sample \(S\)

Figure 5: Privacy leakage of \(D_{i}\)

\(i=k\), \(\mathrm{View}^{0}_{\mathcal{B},I}=\mathrm{View}^{1}_{\mathcal{B},I}\) because all selected hypothesis are the same. When \(i<k\), let \(R^{\prime}=\min\left(\sum_{j=1}^{i+1}R_{j},t\right)\).

Similar to the analysis if Claim E.5, \(\mathrm{View}^{b}_{\mathcal{B},t}\) results from a post-processing of the random variable (\(\mathrm{View}^{b}_{\mathcal{B},R^{\prime}},S^{b}_{i+2}\)) (if \(t<\sum_{j=1}^{i+1}R_{j}\) we set \(S^{b}_{i+2}\) as \(\bot\)). Note that \(\mathrm{View}^{b}_{\mathcal{B},R^{\prime}_{1}}=(D^{b,L}_{1},\dots,D^{b,L}_{i}, D^{b,L}_{i+1})\), and \((D^{b,L}_{1},\dots,D^{b,L}_{i-1},D^{b,L}_{i})\) follow the same distribution for \(b\in\{0,1\}\), where \(D^{b,L}_{i}\) is the labels of points in \(D^{b}_{i}\) expect the different point. So that it suffices to show that \(\left(D^{0,L}_{i+1},S^{0}_{2}\right)\) and \(\left(D^{1,L}_{i+1},S^{1}_{2}\right)\) are \((\varepsilon,\delta)\)-indistinguishable.

We follow the processes creating \(D^{b,L}_{i+1}\) and \(S^{b}_{i+2}\) in Figure 5: (i) The mechanism \(M_{1}\) corresponds to the loop in Step 3d of GenericBBL where labels are produced for the adversarially chosen points \(D^{b}_{i+1}\). By application of claim D.1, \(M_{1}\) is \((1,\delta)\)-differentially private. (ii) The mechanism \(M_{2}\), corresponds to the subsampling of \(\hat{S}^{b}_{i+1}\) from \(S^{b}_{i+1}\) and the application of procedure LabelBoost on the subsample in Step 3f of GenericBBL resulting in \(S^{b}_{i+2}\). By application of Claim 2.9 and Lemma C.1, \(M_{2}\) is \((\varepsilon,0)\)-differentially private. Thus \((M_{1},M_{2})\) is \((\varepsilon+1,\delta)\)-differentially private. (iii) The mechanism \(M_{3}\) with input of \(\hat{D}^{b}_{i}\) and output \(\left(D^{b,L}_{i+1},S^{b}_{i+2}\right)\) applies \((M_{2},M_{3})\) on \(S_{i+1}\), which is generated from \(\hat{D}^{b}_{i}\) and in Step 3f of GenericBBL. By application of Claim C.1, \(M_{3}\) is \((\varepsilon+4,4\varepsilon\delta)\)-differentially private. (iv) The mechanism \(M_{4}\), corresponds to the subsampling \(\hat{D}^{b}_{i}\) from \(D^{b}_{i}\) and the application of \(M_{4}\) on \(\hat{D}^{b}_{i}\). By application of Claim 2.9, \(M_{4}\) is \((\varepsilon,\frac{16\varepsilon\varepsilon^{b}}{3+\exp(\varepsilon+4)})\)-differentially private. Since \(\frac{16\varepsilon\varepsilon}{3+\exp(\varepsilon+4)}\leq 1\) for any \(\varepsilon\), \(\left(D^{0,L}_{i+1},S^{0}_{2}\right)\) and \(\left(D^{1,L}_{i+1},S^{1}_{2}\right)\) are \((\varepsilon,\delta)\)-indistinguishable. 

**Remark E.7**.: _The above proofs work on the adversarially selected \(D\) because: (i) Lemma E.7 works on the adaptively selected queries. (We treat the hypothesis class \(F_{i}\) as the database, the unlabelled points \(x_{i,\ell}\) as the query parameters.) (ii) LabelBoost generates labels by applying one private hypothesis on points. The labels are differentially private by post-processing._