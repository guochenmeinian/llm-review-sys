# LRVS-Fashion: Extending Visual Search with Referring Instructions

 Simon Lepage\({}^{1,2}\) Jeremie Mary\({}^{1}\) David Picard\({}^{2}\)

\({}^{1}\) CRITEO AI Lab, Paris, France

\({}^{2}\) LIGM, Ecole des Ponts, Marne-la-Vallee, France

{s.lepage, j.mary}@criteo.com  david.picard@enpc.fr

###### Abstract

This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors.1

Footnote 1: The dataset is available at https://huggingface.co/datasets/Slep/LATION-RVS-Fashion

## 1 Introduction

Image embeddings generated by deep neural networks play a crucial role in a wide range of computer vision tasks. Image retrieval has gained substantial prominence, leading to the development of dedicated vector database systems [22]. These systems facilitate efficient retrieval by comparing embedding values and identifying the most similar images within the database.

Image similarity search in the context of fashion presents a unique challenge due to the inherently ill-founded nature of the problem. The primary issue arises from the fact that two images can be considered similar in various ways, leading to ambiguity in defining a single similarity metric. For

Figure 1: Overview of the Referred Visual Search task. Given a query image and conditioning information, the goal is to retrieve a target instance from a large gallery. _Note that a query is made of an image and an additional text or category, precising what aspect of the image is relevant._instance, two images of clothing items may be deemed similar based on their color, pattern, style, or even the model pictured. This multifaceted nature of similarity in fashion images complicates the task of developing a universally applicable similarity search algorithm, as it must account for the various ways in which images can be related.

An intuitive approach is to request users furnish supplementary information delineating their interests, such as providing an image of an individual and denoting interest in the hat (see Fig. 1). Numerous industry leaders including Google, Amazon, and Pinterest have adopted this tactic, however academic discourse on potential alternative methodologies for this task remains scarce as the domain lacks dedicated datasets. For convenience, we propose terming this task Referred Visual Search (RVS), as it is likely to garner attention from the computer vision community due to the utility for product search in extensive catalogs.

In practice, object selection in complex scenes is classically tackled using object detection and crops [21; 17; 12; 42]. Some recent approaches use categorical attributes [8] or text instead [6], and automatically crop the image based on learned attention to input attributes. It is also possible to ask the user to perform the crop himself, yet in all the situations the performance of the retrieval will be sensitive to this extraction step making it costly to build a generic retrieval tool. Recently, Jiao et al. [20] went a step further, incorporating prior knowledge about the taxonomy of fashion attributes and classes without using crops. They use a multi-granularity loss and two sub-networks to learn attribute and class-specific representations, resulting in improved robustness for fashion retrieval, yet without providing any code.

In this work, we seek to support these efforts by providing a dataset dedicated to RVS. We extracted a subset of LAION 5B [41] focused on pairs of images sharing a labeled similarity in the domain of fashion, and propose a method to eliminate the need for explicit detection or segmentation, while still producing similarities in the embedding space specific to the conditioning. We think that such end-to-end approach has the potential to be more generalizable and robust, whereas localization-dependent approaches hinge on multi-stage processing heuristics specific to the dataset.

This paper presents two contributions to the emerging field of Referred Visual Search, aiming at defining image similarity based on conditioning information.

* The introduction of a new dataset, referred to as LRVS-Fashion, which is derived from the LAION-5B dataset and comprises 272k fashion products with nearly 842k images. This dataset features a test set with an addition of more than 2M distractors, enabling the evaluation of method robustness in relation to gallery size. The dataset's pairs and additional metadata are designed to necessitate the extraction of particular features from complex images.
* An innovative method for learning to extract referred embeddings using weakly-supervised training. Our approach demonstrates superior accuracy against a strong detection-based baseline and existing published work. Furthermore, our method exhibits robustness against a large number of distractors, maintaining high R@1 even when increasing the number of distractors to 2M.

## 2 Related Work

Retrieval Datasets.Standard datasets in metric learning literature consider that the images are object-centric, and focus on single salient objects [49; 25; 45]. In the fashion domain there exist multiple datasets dedicated to product retrieval, with paired images depicting the same product and additional labeled attributes. A recurrent focus of such datasets is cross-domain retrieval, where the goal is to retrieve images of a given product taken in different situations, for exemple consumer-to-shop [31; 50; 32; 12], or studio-to-shop [32; 27]. The domain gap is in itself a challenge, with issues stemming from irregular lighting, occlusions, viewpoints, or distracting backgrounds. However, the query domain (consumer images for exemple) often contains scenes with multiple objects, making queries ambiguous. This issue has been circumvented with the use of object detectors and landmarks detectors [23; 18; 32; 12]. Some are not accessible anymore [23; 32; 50].

With more than 272k distinct training product identities captured in multi-instance scenes, our new dataset proposes an exact matching task similar to the private Zalando dataset [27], while being larger than existing fashion retrieval datasets and publicly available. We also create an opportunity for new multi-modal approaches, with captions referring to the product of interest in each complex image, and for robustness to gallery size with 2M added distractors at test time.

Instance Retrieval.In the last decade, content-based image retrieval has changed because of the arrival of deep learning, which replaced many handcrafted heuristics (keypoint extraction, descriptors, geometric matching, re-ranking...) [11]. In the industry this technology has been of interest to retail companies and search engines to develop visual search solutions, with new challenges stemming from the large scale of such databases. Initially using generic pretrained backbones to extract embeddings with minimal retraining [53], methods have evolved toward domain-specific embeddings supervised by semantic labels, and then multi-task domain-specific embeddings, leveraging additional product informations [58; 3; 46]. The latest developments in the field incorporate multi-modal features for text-image matching [59; 54; 62], with specific vision-language pretext tasks.

However, these methods often consider that the query image is unambiguous, and often rely on a region proposal system to crop the initial image [21; 60; 17; 42; 3; 10]. In our work, we bypass this step and propose an end-to-end framework, leveraging the Transformer architecture to implicitly perform this detection step conditionally to the referring information.

Referring Tasks.Referring tasks are popular in vision-language processing, in particular Referring Expression Comprehension and Segmentation where a sentence designates an object in a scene, that the network has to localize. For the comprehension task (similar to open-vocabulary object detection) the goal is to output a bounding box [34; 56; 57; 30]. The segmentation task aims at producing an instance mask for images [61; 34; 19; 7; 24] and recently videos [52; 4]. In this paper, we propose a referring expression task, where the goal is to embed the designated object of interest into a representation that can be used for retrieval. We explore the use of Grounding DINO [30] and Segment Anything [24] to create a strong baseline on our task.

Conditional Embeddings.Conditional similarity search has been studied through the retrieval process and the embedding process. On one hand, for the retrieval process, Hamilton et al. [15] propose to use a dynamically pruned random projection tree. On the other hand, previous work in conditional visual similarity learning focused on attribute-specific retrieval, defining different similarity spaces depending on chosen discriminative attributes [47; 36]. They use either a mask applied on the features [47], or different projection heads [36], and require extensive data labeling.

In Fashion, ASEN [35] uses spatial and channel attention to an attribute embedding to extract specific features in a global branch. Dong et al. [8] and Das et al. [6] build upon this model and add a local branch working on an attention-based crop. Recently, Jiao et al. [20] incorporated prior knowledge about fashion taxonomy in this process to create class-conditional embeddings based on known fine-grained attributes, using multiple attribute-conditional attention modules. In a different domain, Asai et al. [1] tackle a conditional document retrieval task, where the user intent is made explicit by concatenating instructions to the query documents. In our work, we use Vision Transformers [9] to implicitly pool features depending on the conditioning information, without relying on explicit ROI cropping or labeled fine-grained attributes.

Composed Image Retrieval (CIR) [48] is another retrieval task where the embedding of an image must be modified following a given instruction. Recent methods use a composer network after embedding the image and the modifying text [28; 5; 2]. While CIR shares similarities with RVS in terms of inputs and outputs, it differs conceptually. Our task focuses on retrieving items based on depicted attributes and specifying a similarity computation method, rather than modifying the image. In Fashion, CIR has been extended to dialog-based interactive retrieval, where an image query is iteratively refined following user instructions [14; 51; 55; 16].

## 3 Dataset

Metric learning methods work by extracting features that pull together images labeled as similar [11]. In our case, we wanted to create a dataset where this embedding has to focus on a specific object in a scene to succeed. We found such images in fashion, thanks to a standard practice in this field consisting in taking pictures of the products alone on neutral backgrounds, and worn by models in scenes involving other clothing items (see Fig. 3).

We created LAION-RVS-Fashion (abbreviated LRVS-F) from LAION-5B by collecting images of products isolated and in context, which we respectively call _simple_ and _complex_. We grouped them using extracted product identifiers. We also gathered and created a set of metadata to be used as referring information, namely LAION captions, generated captions, and generated item categories. The process is depicted Fig. 2, presented in Section 3.1 with additional details in Appendix A.3.

### Construction

Image Collection.The URLs in LRVS-F are a subset of LAION-5B, curated from content delivery networks of fashion brands and retailers. By analyzing the URL structures we identified product identifiers, which we extracted with regular expressions to recreate groups of images depicting the same product. URLs without distinct identifiers or group membership were retained as distractors.

Annotations.We generated synthetic labels for the image complexity, the category of the product, and added new captions to replace the noisy LAION alt-texts. For the complexity labels, we employed active learning to incrementally train a classifier to discern between isolated objects on neutral backdrops and photoshoot scenes. The product categories were formed by aggregating various fine-grained apparel items into 10 coarse groupings. This categorization followed the same active learning protocol. Furthermore, the original LAION captions exhibited excessive noise, including partial translations or raw product identifiers. Therefore, we utilized BLIP-2 [29] to generate new, more descriptive captions.

Dataset Split.We grouped together images associated to the same product identifier and dropped the groups that did not have at least a simple and a complex image. We manually selected 400 of them for the validation set, and 2,000 for the test set. The distractors are all the images downloaded previously that were labeled as "simple" but not used in product groups. This mostly includes images for which it was impossible to extract any product identifier.

Dataset Cleaning.In order to mitigate false negatives in our results, we utilized Locality Sensitive Hashing and OpenCLIP ViT-B/16 embeddings to eliminate duplicates. Specifically, we removed duplicates between the test targets and test distractors, as well as between the validation targets and validation distractors. Throughout our experiments, we did not observe any false negatives in the results. However, there remains a small quantity of near-duplicates among the distractor images.

Figure 3: Samples from LRVS-F. Each product is represented on at least a simple and a complex image, and is associated with a category. The simple images are also described by captions from LAION and BLIP2. Please refer to Appendix A.1 for more samples.

Figure 2: Overview of the data collection. _a)_ Selection of a subset of domains belonging to known fashion retailers. _b)_ Extraction of product identifiers in the URLs using domain-specific regular expressions. _c)_ Generation of synthetic metadata for the products (categories, captions,...) using both pretrained and finetuned models. _d)_ Deduplication of the images, and assignment to subsets.

### Composition

In total, we extracted 272,451 products for training, represented in 841,718 images. This represents 581,526 potential simple/complex positive pairs. We additionally extracted 400 products (800 images) to create a validation set, and 2,000 products (4,000 images) for a test set. We added 99,541 simple images in the validation gallery as distractors, and 2,000,014 in the test gallery.

We randomly sampled images and manually verified the quality of the labels. For the complexity labels, we measured an empirical error rate of \(1/1000\) on the training set and \(3/1000\) for the distractors. For the product categories, we measured a global empirical error rate of \(1\%\), with confusions mostly arising from semantically similar categories and images where object scale was ambiguous in isolated settings (e.g. long shirt vs. short dress, wristband vs. hairband). The BLIP2 captions we provided exhibit good quality, increasing the mean CLIP similarity with the image by \(+7.4\%\). However, as synthetic captions, they are not perfect and may contain occasional hallucinations.

Please refer to Appendix A.4 for metadata details, A.5 for considerations regarding privacy and biases and C for metadata details and a datasheet [13].

### Benchmark

We define a benchmark on LRVS-F to evaluate different methods on a held-out test set with a large number of distractors. The test set contains 2,000 unseen products, and up to 2M distractors. Each product in the set is represented by a pair of images - a simple one and a complex one. The objective of the retrieval task is to retrieve the simple image of each product from among a vast number of distractors and other simple test images, given the complex image and conditioning information.

For this dataset, we propose to frame the benchmark as an asymmetric task : the representation of simple images (the gallery) should not be computed conditionally. This choice is motivated by three reasons. First, when using precise free-form conditioning (such as LAION texts, which contain hashed product identifiers and product names) a symmetric encoding would enable a retrieval based solely on this information, completely disregarding the image query. Second, for discrete (categorical) conditioning it allows the presence of items of unknown category in the gallery, which is a situation that may occur in distractors. Third, these images only depict a single object, thus making referring information unnecessary. A similar setting is used by Asai et al. [1].

Additionally, we provide a list of subsets sampled with replacement to be used for boostrapped estimation of confidence intervals on the metrics. We created 10 subsets of 1000 test products, and 10 subsets of 10K, 100K and 1M distractors. We also propose a validation set of 400 products with nearly 100K other distractors to monitor the training and for hyperparameter search.

## 4 Conditional Embedding

Task Formulation.Let \(x_{q}\) be a query image containing several objects of interest (_e.g._, a person wearing many different clothes and items), and \(c_{q}\) the associated referring information that provides cues about what aspect of \(x_{q}\) is relevant for the query (_e.g._, a text describing which garment is of interest, or directly the class of the garment of interest). Similarly, let \(x_{t}\) be a target image, described by the latent information \(c_{t}\). The probability of \(x_{t}\) to be relevant for the query \(x_{q}\) is given by the conditional probability \(P(x_{t},c_{t}|x_{q},c_{q})\). When working with categories for \(c_{q}\) and \(c_{t}\), a filtering strategy consists in assuming independence between the images and their category,

\[P(x_{t},c_{t}|x_{q},c_{q})=P(x_{t}|x_{q})P(c_{t}|c_{q})\,\] (1)

and further assuming that categories are uncorrelated (_i.e._, \(P(c_{t}|c_{q})=\delta_{c_{q}=c_{t}}\) with \(\delta\) the Dirac distribution). In this work, we remove those assumptions and instead assume that \(P(x_{t},c_{t}|x_{q},c_{q})\) can be directly inferred by a deep neural network model. More specifically, we propose to learn a flexible embedding function \(\phi\) such that

\[\langle\phi(x_{q},c_{q}),\phi(x_{t},c_{t})\rangle\propto P(x_{t},c_{t}|x_{q},c _{q})\.\] (2)

Our approach offers a significant advantage by allowing the flexibility to change the conditioning information (\(c_{q}\)) at query time, resulting in a different representation that focuses on different aspects of the image. It is also _weakly supervised_ in the sense that the referring information \(c_{q}\) is not requiredto provide localized information about the content of interest (like a bounding box) and can be as imprecise as a free-form text, as shown in Fig. 1.

Method:We implement \(\phi\) by modifying the Vision Transformer (ViT) architecture [9]. The conditioning is an additional input token with an associated learnable positional encoding, concatenated to the sequence of image patches. The content of this token can either be learned directly (_e.g._ for discrete categorical conditioning), or be generated by another network (_e.g._ for textual conditioning). At the end of the network, we linearly project the [CLS] token to map the features to a metric space. We experimented with concatenating at different layers in the transformer, and found that concatenating before the first layer is the most sensible choice (see Appendix B.1).

We train the network with the InfoNCE loss [44, 38], following CLIP [40], which is detailed in the next paragraph. However, we hypothesize that even though our method relies on a contrastive loss, it does not explicitly require a specific formulation of it. We choose the InfoNCE loss because of its popularity and scalability. During training, given a batch of \(N\) pairs of images and conditioning \(((x_{i}^{A},c_{i}^{A});(x_{i}^{B},c_{i}^{B}))_{i=1..N}\), we compute their conditional embeddings \((z_{i}^{A},z_{i}^{B})_{i=1..N}\) with \(z=\phi(x,c)\in\mathbb{R}^{d}\). We compute a similarity matrix \(S\) where \(S_{ij}=s(z_{i}^{A},z_{j}^{B})\), with \(s\) the cosine similarity. We then optimize the similarity of the correct pair with a cross-entropy loss, effectively considering the \(N-1\) other products in the batch as negatives:

\[l(S)=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(S_{ii}\tau)}{\sum_{j=1}^{N}\exp( S_{ij}\tau)}\;,\] (3)

with \(\tau\) a learned temperature parameter, and the final loss is \(\mathcal{L}=l(S)/2+l(S^{\top})/2\). Please refer to Fig. 4 for an overview of the method. The \(\tau\) parameter is used to follow the initial formulation of CLIP [40] and is optimized by gradient during the training. At test time, we use FAISS [22] to create a unique index for the entire gallery and perform fast similarity search on GPUs.

## 5 Experiments

We compare our method to various baselines on LRVS-F, using both category- and caption-based settings. We report implementation details before analyzing the results.

### Implementation details

All our models take as input images of size \(224\times 224\), and output an embedding vector of 512 dimensions. We use CLIP weights as initialization, and then train our models for 30 epochs with AdamW [33] and a maximum learning rate of \(10^{-5}\) determined by a learning rate range test [43]. To avoid distorting pretrained features [26], we start by only training the final projection and new input

Figure 4: Overview of our method on LRVS-F. For each element in a batch, we embed the scene conditionally and the isolated item unconditionally. We optimize an InfoNCE loss over the cosine similarity matrix. \(\oplus\) denotes concatenation to the patch sequence.

embeddings (conditioning and positional) for a single epoch, with a linear warm-up schedule. We then train all parameters for the rest of the epochs with a cosine schedule.

We pad the images to a square with white pixels, before resizing the largest side to 224 pixels. During training, we apply random horizontal flip, and random resized crops covering at least 80% of the image area. We evaluate the Recall at 1 (R@1) of the model on the validation set at each epoch, and report test metrics (recall and categorical accuracy) for the best performing validation checkpoint.

We used mixed precision and sharded loss to run our experiments on multiple GPUs. B/32 models were trained for 6 hours on 2 V100 GPUs, with a total batch size of 360. B/16 were trained for 9 hours on 12 V100, with a batch size of 420. Batch sizes were chosen to maximize GPU memory use.

### Results

Detection-based BaselineWe leveraged the recent Grounding DINO [30] and Segment Anything [24] to create a baseline approach based on object detection and segmentation. In this setting, we feed the model the query image and conditioning information, which can be either the name of the category or a caption. Subsequently, we use the output crops or masks to train a ViT following the aforementioned procedure. Please refer to Tab. 1 for the results.

Initial experiments conducted with pretrained CLIP features showed a slight preference toward segmenting the object. However, training the image encoder revealed that superior performances can be attained by training the network on crops. Our supposition is that segmentation errors lead to definitive loss of information, whereas the network's capacity is sufficient for it to learn to disregard irrelevant information and recover from a badly cropped image.

Overall, using Grounding DINO makes for a strong baseline. However, it is worth highlighting that the inherent imprecision of category names frequently results in overly large bounding boxes, which in turn limits the performances of the models. Indeed, adding more information into the dataset such as bounding boxes with precise categories would help, yet this would compromise the scalability of the model as such data is costly to obtain. Conversely, the more precise boxes produced by the caption-based model reach \(67.8\%\)R@1 against 2M distractors.

Categorical ConditioningWe compare our method with categorical detection-based approaches, and unconditional ViTs finetuned on our dataset. To account for the extra conditioning information used in our method, we evaluated the latter on filtered indexes, with only products belonging to the correct category. We did not try to predict the item of interest from the input picture, and instead consider it as a part of the query. We also report unfiltered metrics for reference. Results are in Tab. 2.

Training the ViTs on our dataset greatly improves their performances, both in terms of R@1 and categorical accuracy. Filtering the gallery brings a modest mean gain of \(2-4\%\)R@1 across all quantities of distractors (Fig. 3(b)), reaching \(62.4\%\)R@1 for 2M distractors with a ViT-B/16 architecture. In practice, this approach is impractical as it necessitates computing and storing an index for each category to guarantee a consistent quantity of retrieved items. Moreover, a qualitative evaluation of the filtered results reveals undesirable behaviors. When filtering on a category divergent from the network's intrinsic focus, we observe the results displaying colors and textures associated with the automatically focused object rather than the requested one.

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c c} \multicolumn{1}{c}{} & \multicolumn{2}{c}{Distractors \(\rightarrow\)} & \multicolumn{2}{c}{**+10K**} & \multicolumn{2}{c}{**+100K**} & \multicolumn{2}{c}{**+1M**} & \multicolumn{2}{c}{**+2M**} \\ \hline \multirow{3}{*}{**Category**} & \multicolumn{2}{c}{Preprocessing} & Embedding & \%R@1 & \%C@1 & \%R@1 & \%C@1 & \%R@1 & \%C@1 & \%R@1 & \%C@1 \\ \hline \multirow{3}{*}{**Category**} & G\_DINO-T + SAM-B & CLIP V1-R/32 & 16.9 \(\downarrow\)/1.65 & 67.4 \(\downarrow\)/1.70 & 89.1 \(\downarrow\)/0.79 & 65.6 \(\downarrow\)/1.4 & 44.0 \(\downarrow\)/4.64 & 64.5 \(\downarrow\)/1.45 & 2.9 & 64.0 \\  & Gr\_DINO-T + SAM-B & MF/32 & MF/32 & 16.9 \(\downarrow\)/1.65 & 67.4 \(\downarrow\)/1.79 & 59.2 \(\downarrow\)/0.67 & 53.1 \(\downarrow\)/1.60 & 90.0 \(\downarrow\)/0.67 & 46.4 & 89.2 \\  & Gr\_DINO-T & MF/32 & 88.7 \(\downarrow\)/0.76 & 96.4 \(\downarrow\)/0.75 & 77.0 \(\downarrow\)/1.79 & 94.3 \(\downarrow\)/0.72 & 62.8 \(\downarrow\)/1.92 & 92.2 \(\downarrow\)/1.25 & 56.0 & 91.8 \\  & Gr\_DINO-B & VR/16 & VR/16 & 89.9 \(\downarrow\)/0.77 & 96.2 \(\downarrow\)/0.77 & 80.8 \(\downarrow\)/1.75 & 94.5 \(\downarrow\)/0.77 & 68.8 \(\downarrow\)/1.77 & 93.2 \(\downarrow\)/0.60 & 62.9 & 92.5 \\ \hline \multirow{3}{*}{**Ception**} & Gr\_DINO-T + SAM-B & CLIP V1-R/32 & 7.3 \(\downarrow\)/1.72 & 72.9 \(\downarrow\)/1.68 & 16.33 \(\uparrow\)/1.71 & 91.9 \(\downarrow\)/0.77 & 10.1 \(\downarrow\)/1.56 & 6.2 & 69.8 \\  & Gr\_DINO-T + SAM-B & VR/32 & 83.5 \(\downarrow\)/1.56 & 94.6 \(\downarrow\)/0.79 & 72.2 \(\downarrow\)/1.59 & 93.0 \(\downarrow\)/0.42 & 56.5 \(\downarrow\)/1.60 & 90.9 \(\downarrow\)/0.47 & 50.8 & 90.2 \\ \cline{1-1}  & Gr\_DINO-T & VR/32 & 89.7 \(\downarrow\)/0.76 & 96.7 \(\downarrow\)/0.77 & 79.0 \(\downarrow\)/0.62 & 95.1 \(\downarrow\)/0.74 & 65.4 \(\downarrow\)/1.07 & 93.1 \(\downarrow\)/1.74 & 59.0 & 92.0 \\ \cline{1-1}  & Gr\_DINO-B & VR/16 & 91.6 \(\downarrow\)/0.77 & 97.6 \(\downarrow\)/0.71 & 83.6 \(\downarrow\)/0.60 & 73.6 \(\downarrow\)/1.49 & 94.7 \(\downarrow\)/0.64 & 67.8 & 94.3 \\ \hline \end{tabular}
\end{table}
Table 1: Comparisons of results on LRVS-F for localization-based models. For 0, 10K, 100K and 1M distractors, we report bootstrapped means and standards deviations estimated from 10 randomly sampled sets. We observe superior performances from the caption-based models, due to the precision of the caption which leads to better detections.

We also compare with ASEN [8] trained on our dataset using the authors' released code. This conditional architecture uses a global and a local branch with conditional spatial attention modules, respectively based on ResNet50 and ResNet34 backbones, with explicit ROI cropping. However in our experiments the performances decrease with the addition of the local branch in the second training stage, even after tuning the hyperparameters. We report results for the global branch.

We train our CondViT using the categories provided in our dataset, learning an embedding vector for each of the 10 clothing categories. For the \(i\)-th product in the batch, we randomly select in the associated data a simple image \(x_{s}\) and its category \(c_{s}\), and a complex image \(x_{c}\). We then compute their embeddings \(z_{i}^{A}=\phi(x_{c},c_{s}),z_{i}^{B}=\phi(x_{s})\). We also experimented with symmetric conditioning, using a learned token for the gallery side (see Appendix B.1).

Our categorical CondViT-B/16, with \(68.4\%\)R@1 against 2M distractors significantly outperforms all other category-based approaches (see Fig. 5, left) and maintains a higher categorical accuracy. Furthermore, it performs similarly to the detection-based method conditioned on richer captions, while requiring easy-to-aquire coarse categories. It does so without making any assumption on the semantic nature of these categories, and adding only a few embedding weights (7.7K parameters) to the network, against 233M parameters for Grounding DINO-B. We confirm in Appendix B.2 that its attention is localized on different objects depending on the conditioning.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \multicolumn{3}{c}{Distractors \(\rightarrow\)} & \multicolumn{2}{c}{**+10K**} & \multicolumn{2}{c}{**+100K**} & \multicolumn{2}{c}{**+1M**} & \multicolumn{2}{c}{**+2M**} \\ \hline Model & \%R@1 & \%Cat@1 & \%R@1 & \%Cat@1 & \%R@1 & \%Cat@1 & \%R@1 & \%Cat@1 \\ \hline ViT-B/32 & 85.6 \(\pm\)1.08 & 93.7 \(\pm\)0.31 & 73.4 \(\pm\)1.35 & 90.9 \(\pm\)0.78 & 58.5 \(\pm\)1.37 & 87.8 \(\pm\)0.86 & 51.7 & 86.9 \\ ViT-B/16 & 88.4 \(\pm\)0.88 & 94.8 \(\pm\)0.52 & 79.0 \(\pm\)0.12 & 92.3 \(\pm\)0.73 & 66.1 \(\pm\)1.21 & 90.2 \(\pm\)0.92 & 59.4 & 88.8 \\ \hline ASEN\({}_{g}\)[8] & 63.1 \(\pm\)1.50 & 76.3 \(\pm\)1.26 & 46.1 \(\pm\)1.21 & 68.5 \(\pm\)0.84 & 29.8 \(\pm\)1.86 & 62.9 \(\pm\)1.27 & 24.1 & 62.0 \\ ViT-B/32 + Filt. & **88.9 \(\pm\)1.07** & — & 76.8 \(\pm\)1.24 & — & 62.0 \(\pm\)1.31 & — & 55.1 & — \\ CondViT-B/32 - Category & 90.9 \(\pm\)0.98 & 99.2 \(\pm\)0.31 & 80.2 \(\pm\)0.59 & 98.8 \(\pm\)0.39 & 65.8 \(\pm\)1.24 & 98.4 \(\pm\)0.65 & 59.0 & 98.0 \\ ViT-B/16 + Filt. & 90.9 \(\pm\)0.85 & — & 81.9 \(\pm\)0.87 & — & 68.9 \(\pm\)1.11 & — & 62.4 & — \\ CondViT-B/16 - Category & 93.3 \(\pm\)1.04 & 99.5 \(\pm\)0.25 & 85.6 \(\pm\)0.06 & 99.2 \(\pm\)0.35 & 74.2 \(\pm\)1.82 & 99.0 \(\pm\)0.42 & 68.4 & 98.8 \\ \hline CosMo [28] & 88.3 \(\pm\)1.30 & 97.6 \(\pm\)0.43 & 76.1 \(\pm\)0.85 & 96.0 \(\pm\)0.32 & 59.1 \(\pm\)1.42 & 94.7 \(\pm\)0.40 & 52.1 & 94.8 \\ CLIPMatch [2] & 92.9 \(\pm\)0.64 & 99.0 \(\pm\)0.33 & 81.9 \(\pm\)0.63 & 98.1 \(\pm\)0.68 & 66.9 \(\pm\)0.25 & 96.5 \(\pm\)0.67 & 59.1 & 95.5 \\ CondViT-B/32 - Caption & 92.7 \(\pm\)0.77 & 99.1 \(\pm\)0.30 & 82.8 \(\pm\)1.22 & 98.7 \(\pm\)0.40 & **68.4 \(\pm\)1.50** & **98.1 \(\pm\)0.43** & 62.1 & 98.0 \\ CondViT-B/16 - Caption & 94.2 \(\pm\)0.90 & 99.4 \(\pm\)0.37 & 86.4 \(\pm\)1.13 & 98.9 \(\pm\)0.49 & 74.6 \(\pm\)1.65 & 98.4 \(\pm\)0.58 & 69.3 & 98.2 \\ \hline \end{tabular}
\end{table}
Table 2: Comparisons of results on LRVS-F for unconditional, category-based and caption-based models. For 0, 10K, 100K and 1M distractors, we report bootstrapped means and standards deviations from 10 randomly sampled sets. Our CondViT-B/16 outperforms other methods for both groups.

Figure 5: R@1 with repects to number of added distractors, evaluated on the entire test set. Please refer to Tab. 1 and 2 for bootstrapped metrics and confidence intervals. Our categorical CondViT-B/16 reaches the performances of the best caption-based models, while using a sparser conditioning.

Textual ConditioningTo further validate our approach, we replaced the categorical conditioning with referring expressions, using our generated BLIP2 captions embedded by a Sentence T5-XL model [37]. We chose this model because it embeds the sentences in a 768-dimensional vector, allowing us to simply replace the categorical token. We pre-computed the caption embeddings, and randomly used one of them instead of the product category at training time. At test time, we used the first caption.

In Tab. 2, we observe a gain of \(3.1\%\)R@1 for the CondViT-B/32 architecture, and \(0.9\%\)R@1 for CondViT-B/16, compared to categorical conditioning against 2M distractors, most likely due to the additional details in the conditioning sentences. When faced with users, this method allows for more natural querying, with free-form referring expressions. See Figure 6 for qualitative results.

We compare these models with CIR methods: CoSMo [28] and CLIP4CIR [2]. Both use a compositor network to fuse features extracted from the image and accompanying text. CoSMo reaches performance similar to an unconditional ViT-B/32, while CLIP4CIR performs similarly to our textual CondViT-B/32. We hypothesize that for our conditional feature extraction task, early conditioning is more effective than modifying embeddings through a compositor at the network's end. Our CondViT-B/16 model significantly outperforms all other models and achieves results comparable to our caption-based approach using Grounding DINO-B (see Fig. 5, right). As the RVS task differs from CIR, despite both utilizing identical inputs, this was anticipated. Importantly, CondViT-B/16 accomplishes this without the need for explicit detection steps or dataset-specific preprocessing. Notably, we observe that our models achieve a categorical accuracy of \(98\%\) against 2M distractors, surpassing the accuracy of the best corresponding detection-based model, which stands at \(94.3\%\).

## 6 Conclusion & Limitations

We studied an approach to image similarity in fashion called Referred Visual Search (RVS), which introduces two significant contributions. Firstly, we introduced the LAION-RVS-Fashion dataset, comprising 272K fashion products and 842K images. Secondly, we proposed a simple weakly-supervised learning method for extracting referred embeddings. Our approach outperforms strong detection-based baselines. These contributions offer valuable resources and techniques for advancing image retrieval systems in the fashion industry and beyond.

However, one limitation of our approach is that modifying the text description to refer to something not present or not easily identifiable in the image does not work effectively. For instance, if the image shows a person carrying a green handbag, a refined search with "red handbag" as a condition would only retrieve a green handbag. The system may also ignore the conditioning if the desired item is small or absent in the database. Examples of such failures are illustrated in Appendix B.3. Additionally, extending the approach to more verticals would be relevant.

Figure 6: Qualitative results for our categorical (first 2 rows) and textual (last 2 rows) CondViT-B/16. We use free-form textual queries instead of BLIP2 captions to illustrate realistic user behavior, and retrieve from the whole test gallery. See Fig. 13 and 14 in the Appendix for more qualitative results.

## References

* Asai et al. [2022] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. _arXiv preprint arXiv:2211.09260_, 2022.
* Baldrati et al. [2023] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Composed image retrieval using contrastive learning and task-oriented clip-based features. _ACM Transactions on Multimedia Computing, Communications and Applications_, 2023.
* Bell et al. [2020] Sean Bell, Yiqun Liu, Sami Alsheikh, Yina Tang, Edward Pizzi, M. Henning, Karun Singh, Omkar Parkhi, and Fedor Borisyuk. GroKNet: Unified Computer Vision Model Trunk and Embeddings For Commerce. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. ACM, 2020. ISBN 978-1-4503-7998-4. doi: 10.1145/3394486.3403311.
* Botach et al. [2022] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation with multimodal transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* Chen et al. [2022] Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, and Tat-Seng Chua. Composed image retrieval with text feedback via multi-grained uncertainty regularization, 2022.
* Das et al. [2022] Nilotpal Das, Aniket Joshi, Promod Yenigalla, and Gourav Agrawal. MAPS: Multimodal Attention for Product Similarity. _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, 2022.
* Ding et al. [2021] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-Language Transformer and Query Generation for Referring Segmentation. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2021. ISBN 978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01601.
* Dong et al. [2021] Jianfeng Dong, Zhe Ma, Xiaofeng Mao, Xun Yang, Yuan He, Richang Hong, and Shouling Ji. Fine-Grained Fashion Similarity Prediction by Attribute-Specific Embedding Learning. _IEEE Transactions on Image Processing_, 2021. ISSN 1057-7149, 1941-0042. doi: 10.1109/TIP.2021.3115658.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021.
* Du et al. [2022] Ming Du, Arnau Ramisa, Amit Kumar K C, Sampath Chanda, Mengjiao Wang, Neelakandan Rajesh, Shasha Li, Yingchuan Hu, Tao Zhou, Nagashri Lakshminarayana, Son Tran, and Doug Gray. Amazon Shop the Look: A Visual Search System for Fashion and Home. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. ACM, 2022. ISBN 978-1-4503-9385-0. doi: 10.1145/3534678.3539071.
* Dubey [2022] Shiv Ram Dubey. A Decade Survey of Content Based Image Retrieval using Deep Learning. _IEEE Transactions on Circuits and Systems for Video Technology_, 2022. ISSN 1051-8215, 1558-2205. doi: 10.1109/TCSVT.2021.3080920.
* Ge et al. [2019] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2019. ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.00548.
* Gebru et al. [2021] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets, 2021.
* Guo et al. [2018] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. Dialog-based Interactive Image Retrieval. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2018.
* Hamilton et al. [2021] Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp, Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris Hoder, and William T. Freeman. MosAIC: Finding Artistic Connections across Culture with Conditional Image Retrieval. In _Proceedings of the NeurIPS 2020 Competition and Demonstration Track_. PMLR, 2021.
* Han et al. [2022] Xiao Han, Sen He, Li Zhang, Yi-Zhe Song, and Tao Xiang. UIGR: Unified Interactive Garment Retrieval. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* Han et al. [2021]* Hu et al. [2018] Huodong Hu, Yan Wang, Linjun Yang, Pavel Komlev, Li Huang, Xi Chen, Jiapei Huang, Ye Wu, Meenaz Merchant, and Arun Sacheti. Web-scale responsive visual search at bing. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, 2018.
* Huang et al. [2015] Junshi Huang, Rogerio Feris, Qiang Chen, and Shuicheng Yan. Cross-Domain Image Retrieval with a Dual Attribute-Aware Ranking Network. In _2015 IEEE International Conference on Computer Vision (ICCV)_. IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.127.
* Huang et al. [2020] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring Image Segmentation via Cross-Modal Progressive Comprehension. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2020. ISBN 978-1-72817-168-5. doi: 10.1109/CVPR42600.2020.01050.
* Jiao et al. [2023] Yang (Andrew) Jiao, Yan Gao, Jingjing Meng, Jin Shang, and Yi Sun. Learning attribute and class-specific representation duet for fine-grained fashion analysis. In _CVPR 2023_, 2023.
* Jing et al. [2015] Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, and Sarah Tavel. Visual search at pinterest. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2015.
* Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with gpus. _IEEE Transactions on Big Data_, 2019.
* Kiapour et al. [2015] M. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C. Berg, and Tamara L. Berg. Where to Buy It: Matching Street Clothing Photos in Online Shops. In _2015 IEEE International Conference on Computer Vision (ICCV)_. IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.382.
* Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.
* Krause et al. [2013] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D Object Representations for Fine-Grained Categorization. In _2013 IEEE International Conference on Computer Vision Workshops_. IEEE, 2013. ISBN 978-1-4799-3022-7. doi: 10.1109/ICCVW.2013.77.
* Kumar et al. [2022] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In _International Conference on Learning Representations_, 2022.
* Lasserre et al. [2018] Julia Lasserre, Katharina Rasch, and Roland Vollgraf. Studio2Shop: from studio photo shoots to fashion articles. In _Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods_, 2018. doi: 10.5220/0006544500370048.
* Lee et al. [2021] Seungmin Lee, Dongwan Kim, and Bohyung Han. Cosmo: Content-style modulation for image retrieval with text feedback. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* Liu et al. [2023] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.
* Liu et al. [2012] Si Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan. Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, 2012.
* Liu et al. [2016] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.124.
* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019.
* Luo et al. [2020] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2020. ISBN 978-1-72817-168-5. doi: 10.1109/CVPR42600.2020.01005.

* [35] Zhe Ma, Jianfeng Dong, Zhongzi Long, Yao Zhang, Yuan He, Hui Xue, and Shouling Ji. Fine-grained fashion similarity learning by attribute-specific embedding network. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020.
* [36] Emily Mu and John Guttag. Conditional Contrastive Networks. In _NeurIPS 2022 First Table Representation Workshop_, 2022.
* [37] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In _Findings of the Association for Computational Linguistics: ACL 2022_. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146.
* [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [39] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.
* [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 2021.
* [41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* [42] Raymond Shiau, Hao-Yu Wu, Eric Kim, Yue Li Du, Anqi Guo, Zhiyuan Zhang, Eileen Li, Kunlong Gu, Charles Rosenberg, and Andrew Zhai. Shop The Look: Building a Large Scale Visual Shopping System at Pinterest. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2020. doi: 10.1145/3394486.3403372.
* [43] Leslie N Smith. Cyclical learning rates for training neural networks. In _2017 IEEE winter conference on applications of computer vision (WACV)_. IEEE, 2017.
* [44] Kihyuk Sohn. Improved Deep Metric Learning with Multi-class N-pair Loss Objective. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2016.
* [45] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep Metric Learning via Lifted Structured Feature Embedding. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.434.
* [46] Son Tran, R. Manmatha, and C. J. Taylor. Searching for fashion products from images in the wild. In _KDD 2019 Workshop on AI for Fashion_, 2019.
* [47] Andreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017.
* [48] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In _CVPR_, 2019.
* [49] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [50] Xi Wang, Zhenfeng Sun, Wenqiang Zhang, Yu Zhou, and Yu-Gang Jiang. Matching User Photos to Online Products with Robust Deep Features. In _Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval_. ACM, 2016. ISBN 978-1-4503-4359-6. doi: 10.1145/2911996.2912002.
* [51] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. In _CVPR_, 2019.
* [52] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as Queries for Referring Video Object Segmentation. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2022. ISBN 978-1-66546-946-3. doi: 10.1109/CVPR52688.2022.00492.
* [53] Fan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi Kiapour, and Robinson Piramuthu. Visual Search at eBay. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2017. doi: 10.1145/3097983.3098162.
** [54] Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L Berg, and Ning Zhang. Commerecmm: Large-scale commerce multimodal representation learning with omni retrieval. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 2022.
* [55] Yifei Yuan and Wai Lam. Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)_, 2021.
* [56] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. In _Proceedings of the 39th International Conference on Machine Learning_. PMLR, 2022.
* [57] Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, and Wangchunshu Zhou. X\({}^{2}\)-VLM: All-in-one pre-trained model for vision-language tasks. _arXiv preprint arXiv:2211.12402_, 2022.
* [58] Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning a unified embedding for visual search at pinterest. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2019.
* [59] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2021. ISBN 978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01157.
* [60] Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya Zhang, Xiaofeng Ren, and Rong Jin. Visual Search at Alibaba. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2018. doi: 10.1145/3219819.3219820.
* [61] Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, and Honglak Lee. Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2017. ISBN 978-1-5386-0457-1. doi: 10.1109/CVPR.2017.122.
* [62] Xiaoyang Zheng, Zilong Wang, Ke Xu, Sen Li, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. MAKE: Vision-Language Pre-training based Product Retrieval in Taobao Search. In _Companion Proceedings of the ACM Web Conference 2023_, 2023. doi: 10.1145/3543873.3584627.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix A.5.
2. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] See Appendix A.5.
3. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
4. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix A.2. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Table 1 and 2. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide the URLs to the assets in Appendix A.2. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]