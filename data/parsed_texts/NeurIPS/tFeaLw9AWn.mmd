# Single-Call Stochastic Extragradient Methods

for Structured Non-monotone Variational Inequalities:

Improved Analysis under Weaker Conditions

Sayantan Choudhury

AMS & MINDS

Johns Hopkins University

&Eduard Gorbunov

MBZUAI

&Nicolas Loizou

AMS & MINDS

Johns Hopkins University

###### Abstract

Single-call stochastic extragradient methods, like stochastic past extragradient (\(\mathsf{SPEG}\)) and stochastic optimistic gradient (\(\mathsf{SOG}\)), have gained a lot of interest in recent years and are one of the most efficient algorithms for solving large-scale min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. However, despite their undoubted popularity, current convergence analyses of \(\mathsf{SPEG}\) and \(\mathsf{SOG}\) require strong assumptions like bounded variance or growth conditions. In addition, several important questions regarding the convergence properties of these methods are still open, including mini-batching, efficient step-size selection, and convergence guarantees under different sampling strategies. In this work, we address these questions and provide convergence guarantees for two large classes of structured non-monotone VIPs: (i) quasi-strongly monotone problems (a generalization of strongly monotone problems) and (ii) weak Minty variational inequalities (a generalization of monotone and Minty VIPs). We introduce the expected residual condition, explain its benefits, and show how it allows us to obtain a strictly weaker bound than previously used growth conditions, expected co-coercivity, or bounded variance assumptions. Finally, our convergence analysis holds under the arbitrary sampling paradigm, which includes importance sampling and various mini-batching strategies as special cases.

## 1 Introduction

Differentiable game formulations where several parameterized models/players compete to minimize their respective objective functions have recently gained much attention from the machine learning community. Some landmark advances in machine learning that are framed as games (or in their simplified form as min-max optimization problems) are Generative Adversarial Networks (GANs) [19; 2], adversarial training of neural networks [46; 72], reinforcement learning [9; 64], and distributionally robust learning [51; 73].

In this work, we consider a more abstract formulation of the problem and focus on solving the following unconstrained stochastic variational inequality problem (VIP):

\[\text{Find }x^{*}\in\mathbb{R}^{d}:\text{such that }F(x^{*})=\frac{1}{n}\sum_{i=1}^ {n}F_{i}(x^{*})=0\] (1)

where each \(F_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is a Lipschitz continuous operator. Problem (1) generalizes the solution of several types of _stochastic smooth games_[16; 44; 20; 7]. The simplest example is the unconstrained min-max optimization problem (also called a _zero-sum_ game):

\[\min_{x_{1}\in\mathbb{R}^{d_{1}}}\max_{x_{2}\in\mathbb{R}^{d_{2}}}\frac{1}{n} \sum_{i=1}^{n}g_{i}(x_{1},x_{2})\,,\] (2)where each component function \(g_{i}:\mathbb{R}^{d_{1}}\times\mathbb{R}^{d_{2}}\to\mathbb{R}\) is assumed to be smooth. In this scenario, operator \(F_{i}\) of (1) represents the appropriate concatenation of the block-gradients of \(g_{i}\): \(F_{i}(x):=(\nabla_{x_{1}}g_{i}(x_{1},x_{2});-\nabla_{x_{2}}g_{i}(x_{1},x_{2}))\), where \(x:=(x_{1};x_{2})\). Solving (1) then amounts to finding a stationary point \(x^{*}=(x_{1}^{*};x_{2}^{*})\) for (2), which under a convex-concavity assumption for \(g_{i}\), implies that it is a global solution for the min-max problem.

However, in modern machine learning applications, game-theoretical formulations that are special cases of problem (1) are rarely monotone. That is, the min-max optimization problem (2) does not satisfy the popular and well-studied convex-concave setting. For this reason, the ML community started focusing on non-monotone problems with extra structural properties.1 In this work, we focus on such settings (structured non-monotone operators) for which we are able to provide tight convergence guarantees and avoid the standard issues (like cycling and divergence of the methods) appearing in the more general non-monotone regime. In particular, we focus on understanding and efficiently analyze the performance of single-call extragradient methods for solving (i) \(\mu\)-quasi-strongly monotone VIPs [44, 6] and (ii) weak Minty variational inequalities [14, 33].

Footnote 1: The computation of approximate first-order locally optimal solutions for general non-monotone problems (without extra structure) is intractable. See [13] and [14] for more details.

Classes of structured non-monotone VIPs.Throughout this work we assume that operator \(F\) in (1) is \(L\)- Lipschitz i.e. \(\forall x,y\in\mathbb{R}^{d}\) operator \(F\) satisfy \(\|F(x)-F(y)\|\leq L\|x-y\|\).

As we have already mentioned, in this work, we deal with two classes of structured non-monotone problems: the \(\mu\)-quasi strongly monotone VIPs and the weak Minty variational inequalities.

**Definition 1.1**.: \(F\) is said to be \(\mu\)-quasi strongly monotone if there is \(\mu>0\) such that:

\[\forall x\in\mathbb{R}^{d}\qquad\langle F(x),x-x^{*}\rangle\geq\mu\|x-x^{*}\| ^{2}.\] (3)

Condition (3) is a relaxation of \(\mu\)-strong monotonicity, and it includes several non-monotone games as special cases [44]. Inequality (3) can be seen as an extension of the popular quasi-strong convexity assumption from optimization literature [53, 25] to the VIPs [44]. In the literature of variational inequality problems, quasi strongly monotone problems are also known as strong coherent VIPs [66] or VIPs satisfying the strong stability condition [47], or strong Minty variational inequality [14].

One of the weakest possible assumptions on the structure of non-monotone VIPs is the weak Minty variational inequality [14].

**Definition 1.2**.: We say weak Minty Variational Inequality (MVI) holds for \(F\) if for some \(\rho>0:\)

\[\forall x\in\mathbb{R}^{d}\qquad\langle F(x),x-x^{*}\rangle\geq-\rho\|F(x)\|^ {2}.\] (4)

To the best of our knowledge, the weak Minty variational inequality (4) as an assumption was first introduced in [14]. The more popular and extensively studied Minty variational inequality [12, 37, 38, 48] is a particular case of (4) with \(\rho=0\). In addition, the weak MVI condition is implied by the negative comonotonicity [4] or, equivalently, the positive cohypomonotonicity [11]. Finally, when we focus on min-max optimization problems (2), weak MVI condition (with \(\rho=0\)) is satisfied for several non-convex non-concave families of min-max objectives, including quasi-convex quasi-concave or star convex- star concave [20]. Extragradient-type methods for solving VIPs satisfying the weak MVI have been proposed in [14, 54] and [8].

### Main Contributions

Our main contributions are summarized below.

* **Expected Residual.** We propose the expected residual (ER) condition for stochastic variational inequality problems (1). We explain the benefits of ER and show how it can be used to derive an upper bound on \(\mathbb{E}\|g(x)\|^{2}\) (see Lemma 3.2) that it is strictly weaker than the bounded variance assumption and "growth conditions" previously used for the analysis of stochastic algorithms for solving (1). We prove that ER holds for a large class of operators, i.e., whenever \(F_{i}\) of (1) are Lipschitz continuous.
* **Novel Convergence Guarantees.** We prove the first convergence guarantees for SPEG (7) in the quasi-strongly monotone (3) and weak MVI (4) cases _without using the bounded variance _assumption_. We achieve that by using the proposed (ER) condition. In particular, for the class of quasi-strongly monotone VIPs, we show a linear convergence rate to a neighborhood of \(x^{*}\) when constant step-sizes are used. We also provide theoretically motivated step-size switching rules that guarantee exact convergence of \(\mathsf{SPEG}\) to \(x^{*}\). In the weak MVI case, we prove the convergence of \(\mathsf{SPEG}\) for \(\rho<\nicefrac{{1}}{{2L}}\), improving the existing restrictions on \(\rho\). We compare our results with the existing literature in Table 1.
* **Arbitrary Sampling.** Via a stochastic reformulation of the variational inequality problem (1) we explain how our convergence guarantees of \(\mathsf{SPEG}\) hold under the arbitrary sampling paradigm. This allows us to cover a wide range of samplings for \(\mathsf{SPEG}\) that were never considered in the literature before, including mini-batching, uniform sampling, and importance sampling as special cases. In this sense, our analysis of \(\mathsf{SPEG}\) is unified for different sampling strategies. Finally, to highlight the tightness of our analysis, we show that the best-known convergence guarantees of deterministic \(\mathsf{PEG}\) for strongly monotone and weak MVI can be obtained as special cases of our main theorems.

## 2 Stochastic Reformulation of VIPs & Single-Call Extragradient Methods

In this work, we provide a theoretical analysis of single-call stochastic extragradient methods that allows us to obtain convergence guarantees of any minibatch and reasonable sampling selection. We achieve that by using the recently proposed "stochastic reformulation" of the variational inequality problem (1) from [44]. That is, to allow for any form of minibatching, we use the _arbitrary sampling_ notation

\[g(x)=F_{v}(x):=\frac{1}{n}\sum_{i=1}^{n}v_{i}F_{i}(x),\] (5)

where \(v\in\mathbb{R}^{n}_{+}\) is a random _sampling vector_ drawn from a user-defined distribution \(\mathcal{D}\) such that \(\mathbb{E}_{\mathcal{D}}[v_{i}]=1,\) for \(i=1,\ldots,n\). In this setting, the original problem (1) can be equivalently written as,

\[\text{Find}\;x^{*}\in\mathbb{R}^{d}:\mathbb{E}_{\mathcal{D}}\left[F_{v}(x^{*} ):=\frac{1}{n}\sum_{i=1}^{n}v_{i}F_{i}(x^{*})\right]=0,\] (6)

where the equivalence trivially holds since \(\mathbb{E}_{\mathcal{D}}[F_{v}(x)]=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{ \mathcal{D}}[v_{i}]F_{i}(x)=F(x)\).

In this work, we consider _Stochastic Past Extragradient Method_ (SPEG) applied to (6):

\[\begin{split}\hat{x}_{k}&=x_{k}-\gamma_{k}F_{v_{k-1}}( \hat{x}_{k-1})\\ x_{k+1}&=x_{k}-\omega_{k}F_{v_{k}}(\hat{x}_{k})\end{split}\] (7)

where \(\hat{x}_{-1}=x_{0}\) and \(v^{k}\sim\mathcal{D}\) is sampled i.i.d at each iteration and \(\gamma_{k}>0\) and \(\omega_{k}>0\) are the extrapolation step-size and update step-size respectively. We note that in our convergence analysis, we allow selecting _any_ distribution \(\mathcal{D}\) that satisfies \(\mathbb{E}_{\mathcal{D}}[v_{i}]=1\ \forall i\). This means that for a different selection of \(\mathcal{D}\), (7) yields different interpretations of SPEG for solving the original problem (1).

One example of distribution \(\mathcal{D}\) is \(\tau\)-minibatch sampling, which is defined as follows.

**Definition 2.1** (\(\tau\)-Minibatch sampling).: Let \(\tau\in[n]\). We say that \(v\in\mathbb{R}^{n}\)is a \(\tau\)-minibatch sampling if for every subset \(S\in[n]\) with \(|S|=\tau\), we have that \(\mathbb{P}\left[v=\frac{n}{\tau}\sum_{i\in S}e_{i}\right]:=\frac{1}{\binom{n}{ \tau}}=\frac{\tau!(n-\tau)!}{n!}\).

By using a double counting argument, one can show that if \(v\) is a \(\tau\)-minibatch sampling, it is also a valid sampling vector (\(\mathbb{E}_{\mathcal{D}}[v_{i}]=1\)) [25]. We highlight that our analysis holds for every form of minibatching and for several choices of sampling vectors \(v\). Later in Section 5, we provide more details related to non-uniform sampling. In addition, by Definition 2.1, it is clear that if \(\tau=n\), then \(v_{i}=1\) for all \(i\in[n]\). Later in Section 4, we prove how our analysis captures the deterministic Past Extragradient Method as a special case.

In [44], an analysis of stochastic gradient descent-ascent (\(x_{k+1}=x_{k}-\omega_{k}F_{v_{k}}(x_{k})\)) under the arbitrary sampling paradigm was proposed for solving star-co-coercive VIPs. Later [20], extended this approach and provided general convergence guarantees for stochastic extragradient method (SEG) (a stochastic variant of the popular extragradient method [32; 30]) for solving quasi-strongly monotone and monotone VIPs. Despite its popularity, SEG requires two oracle calls per iteration which makes it prohibitively expensive in many large-scale applications and not easily applicable to the online learning problems [18]. This motivates us to explore in detail the convergence guarantees of single-call variants of extragradient methods (extragradient methods that require only a single oracle call per iteration).

On Single-Call Extragradient Methods.The seminal work of [56] is the first paper that proposes the deterministic Past Extragradient method. In the stochastic setting, [28] provides an analysis of several stochastic single-call extragradient methods for solving strongly monotone VIPs. In [28], it was also shown that in the unconstrained setting, the update rules of Past Extragradient and Optimistic Gradient are exactly equivalent (see also Proposition B.6 in appendix). Through this connection, and via our stochastic reformulation (6) our theoretical results hold also for the _Stochastic Optimistic Gradient Method_ (SOG): \(x_{k+1}=x_{k}-\omega_{k}F_{v_{k}}(x_{k})-\gamma_{k}(F_{v_{k}}(x_{k})-F_{v_{k-1 }}(x_{k-1}))\). [8] provides the convergence guarantees of SOG for weak MVI. To the best of our knowledge, our work is the first that provides convergence guarantees for SOG under the arbitrary sampling paradigm (captures sampling beyond uniform sampling) and also without using the bounded variance assumption.

## 3 Expected Residual

In our theoretical results, we rely on Expected Residual (ER) condition. In this section, we define ER and explain how it is connected with similar conditions used in optimization literature. We further provide sufficient conditions for ER to hold and prove how it can be used to obtain a strictly weaker upper bound of \(\mathbb{E}\|g(x)\|^{2}\) than previously used growth conditions, expected co-coercivity, or bounded variance assumptions.

**Assumption 3.1**.: We say the Expected Residual (ER) condition holds if there is a parameter \(\delta\gg 0\) such that for an unbiased estimator \(g(x)\) of the operator \(F\), we have

\[\mathbb{E}\left[\|(g(x)-g(x^{*}))-(F(x)-F(x^{*}))\|^{2}\right]\leq\frac{\delta }{2}\|x-x^{*}\|^{2}.\] (ER)

The ER condition bounds how far the stochastic estimator \(g(x)=F_{v}(x)\) (5) used in SPEG is from the true operator \(F(x)\). ER depends on both the properties of the operator \(F(x)\) and of the selection of sampling (via \(g(x)\)). Conditions similar to ER appeared before in optimization literature but they have never been used in operator theory and the analysis of \(\mathsf{SPEG}\). In particular, [24] used a similar condition for analyzing \(\mathsf{SGD}\) in stochastic optimization problems but with the right-hand side of \(\mathsf{ER}\) to be the function suboptimality \(f(x)-f(x^{*})\) (such concept is not available in VIPs). In [68] and [22], similar conditions appear under the name "Hessian variance" assumption for distributed minimization problems. In the context of distributed VIPs, a similar but stronger condition to \(\mathsf{ER}\) is used by [5].

**Bound on Operator Noise.** A common approach for proving the convergence of stochastic algorithms for solving the VIPs is assuming uniform boundedness of the stochastic operator or uniform boundedness of the variance. However, as we explain below, these assumptions either do not hold or are true only for a restrictive set of problems. In our work, we do not assume such bounds. Instead, we use the following direct consequence of \(\mathsf{ER}\).

**Lemma 3.2**.: Let \(\sigma_{*}^{2}:=\mathbb{E}\|g(\bar{x}^{*})\|^{2}<\infty\) (operator noise at the optimum is finite). If \(\mathsf{ER}\) holds, then

\[\mathbb{E}\|g(x)\|^{2}\leq\delta\|x-x^{*}\|^{2}+\|F(x)\|^{2}+2\sigma_{*}^{2}.\] (8)

Sufficient Conditions for \(\mathsf{ER}\).Let us now provide sufficient conditions which guarantee that the \(\mathsf{ER}\) condition holds and give a closed-form expression for the expected residual parameter \(\delta\) and \(\sigma_{*}^{2}=\mathbb{E}\|g(x^{*})\|^{2}\) for the case of \(\tau\)-minibatch sampling (Def. 2.1).

**Proposition 3.3**.: Let \(F_{i}\) of problem (1) be \(L_{i}\)-Lipschitz operators, then \(\mathsf{ER}\) holds. If, in addition, vector \(v\in\mathbb{R}^{n}\) is a \(\tau\)-minibatch sampling (Def. 2.1) then: \(\delta=\frac{2}{n\tau}\frac{n-\tau}{n-1}\sum_{i=1}^{n}L_{i}^{2},\) and \(\sigma_{*}^{2}=\frac{1}{n\tau}\frac{n-\tau}{n-1}\sum_{i=1}^{n}\|F_{i}(x^{*}) \|^{2}\).

Similar results to Prop. 3.3 but under different sufficient conditions have been obtained for \(\tau\)-minibatch sampling under expected smoothness and a variant of expected residual for solving minimization problems in [25] and [24] respectively. In [44], a similar proposition was derived but for the much more restrictive class of co-coercive operators.

Connection to Other Assumptions.In the proofs of our convergence results, we use the bound (8), which, as we explained above, is a direct consequence of \(\mathsf{ER}\). In this paragraph, we place this bound in a hierarchy of common assumptions used for the analysis of stochastic algorithms for solving VIPs. In the literature on stochastic algorithms for solving the VIPs and min-max optimization problems, previous works assume either bounded operator (\(\mathbb{E}\|g(x)\|^{2}\leq c\)) [1, 52], bounded variance (\(\mathbb{E}\|g(x)-F(x)\|^{2}\leq c\)) [35, 69, 30] (in Appendix C we provide a simple example where bounded variance assumption does not hold) or growth condition (\(\mathbb{E}\|g(x)\|^{2}\leq c_{1}\|F(x)\|^{2}+c_{2}\)) [36]. In all of these conditions, the parameters \(c\), \(c_{1}\), and \(c_{2}\) are usually constants that do not have a closed-form expression. The closer works to our results are [44, 6] which assumes existence of \(l_{F}>0\) such that the expected co-coercivity condition (\(\mathbb{E}\|g(x)-g(x^{*})\|^{2}\leq l_{F}\left\langle F(x),x-x^{*}\right\rangle\)) holds. Their convergence guarantees provide an efficient analysis for several variants of SGDA for solving co-coercive VIPs. In the proposition below, we prove how these conditions are related to the bound (8) obtained using \(\mathsf{ER}\).

**Proposition 3.4**.: Suppose \(F\) is a \(L\)-Lipschitz operator. Then we have the following hierarchy of assumptions:

\begin{tabular}{|c|} \hline Bounded Operator & Bounded Variance & Growth Condition \\ \hline \(F_{i}\) are \(L_{i}\)-Lipschitz & \(\xrightarrow{}\) \\ \hline Expected Cocoercivity & \\ \hline \end{tabular}

Let us also mention that [29] provided convergence guarantee of double-oracle stochastic extragradient (\(\mathsf{SEG}\)) method under the variance control condition \(\mathbb{E}\|g(x)-F(x)\|^{2}\leq(a\|x-x^{*}\|+b)^{2}\) where \(a,b\geq 0\). In their work, they focus on solving VIPs satisfying the error-bound condition, and they did not provide closed-form expressions of parameters \(a\) and \(b\). Although the analysis of [29]can be conducted with \(a>0\), the authors only provide rates for the case \(a=0\). The main difference between their results (for SEG) and our results (for SPEG) is that our bound (8) is not really an assumption, but it holds for free when \(F_{i}\) are \(L_{i}\)-Lipschitz. In addition, the values of parameters \(\delta\) and \(\sigma_{*}^{2}\) in (8) could have different values based on the sampling used in the update rule of SPEG.

## 4 Convergence Analysis

In this section, we present and discuss the main convergence results of this work. In the first part, we focus on the ones derived for \(\mu\)-quasi strongly monotone problems (3) (both for constant and decreasing step-sizes), and in the second part on the Weak Minty VIP (4).

### Quasi-Strongly Monotone Problems

Constant Step-size:We start with the case of \(\mu\)-quasi strongly monotone problems and consider the convergence of SPEG with constant step-size.

**Theorem 4.1**.: Let \(F\) be \(L\)-Lipschitz, \(\mu\)-quasi strongly monotone, and let ER hold. Choose step-sizes \(\gamma_{k}=\omega_{k}=\omega\) such that

\[0<\omega\leq\min\left\{\frac{\mu}{18\delta},\frac{1}{4L}\right\}\] (9)

for all \(k\). Then the iterates produced by SPEG, given by (7) satisfy

\[R_{k}^{2}\leq\left(1-\frac{\omega\mu}{2}\right)^{k}R_{0}^{2}+\frac{24\omega \sigma_{*}^{2}}{\mu},\] (10)

where \(R_{k}^{2}\coloneqq\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}+\|x_{k}-\hat{x}_{k-1}\| ^{2}\right]\). Hence, given any \(\varepsilon>0\), and choosing \(\omega=\min\left\{\frac{\mu}{18\delta},\frac{1}{4L},\frac{\varepsilon\mu}{48 \sigma_{*}^{2}}\right\}\), SPEG achieves \(\mathbb{E}\|x_{K}-x^{*}\|^{2}\leq\varepsilon\) after \(K\geq\max\left\{\frac{8L}{\mu},\frac{36\delta}{\mu^{2}},\frac{96\sigma_{*}^{2 }}{\varepsilon\mu^{2}}\right\}\log\left(\frac{2R_{0}^{2}}{\varepsilon}\right)\) iterations.

To the best of our knowledge, the above theorem is the first result on the convergence of SPEG that does not rely on the bounded variance assumption. Theorem 4.1 recovers the same rate of convergence with the Independent-Samples SEG (I-SEG) under assumption (8) [20], although [20] simply assume (8), while we show that it follows from Assumption 3.1 holding whenever all summands \(F_{i}\) are Lipschitz. However, in the case when all \(F_{i}\) are \(\mu\)-quasi strongly monotone and \(L_{i}\)-Lipschitz (on average), one can use Same-Sample SEG (S-SEG). The existing results for S-SEG have better exponentially decaying term [49; 20] then Theorem 4.1, e.g., in the case when \(L_{i}=L\) for all \(i\in[n]\), we have \(\delta=\mathcal{O}(L^{2})\) meaning that the exponentially decaying term in (10) is \(\mathcal{O}(R_{0}^{2}\exp(-{\mu^{2}k/L^{2}}))\), while S-SEG has much better exponentially decaying term \(\mathcal{O}(R_{0}^{2}\exp(-{\mu k/L}))\).

Such a discrepancy can be partially explained by the following fact: S-SEG can be seen as one step of deterministic Extragradient for stochastic operator \(F_{v_{k}}\) allowing to use one-iteration analysis of Extragradient without controlling the variance. In contrast, there is no version of SPEG that uses the same sample for extrapolation and update steps. This forces to use different samples for these steps and this is a key reason why SPEG cannot be seen as one iteration of deterministic Past-Extragradient for some operator. Due to this, we need to rely on some bound on the variance to handle the stochasticity in the updates; see also [20, Appendix F.1]. Therefore, in our analysis, we use Assumption 3.1, implying (8). Nevertheless, it is still an open question whether it is possible to improve the rate of SPEG in the case of \(\mu\)-quasi strongly monotone and Lipschitz operators \(F_{i}\).

To highlight the generality of Theorem 4.1, we note that for the deterministic PEG, \(\delta=0\) and \(\sigma_{*}^{2}=0\) (by selecting \(\tau=n\) in the definition 2.1 of minibatch sampling). In this case, Theorem 4.1 recovers the well-known result (up to \(\nicefrac{{1}}{{2}}\) factor in the rate) for deterministic PEG proposed in [17] as shown in the following corollary.

**Corollary 4.2**.: Let the assumptions of Theorem 4.1 hold and a deterministic version of SPEG is considered, i.e., \(\delta=0\), \(\sigma_{*}^{2}=0\). Then, Theorem 4.1 implies that for all \(k\geq 0\) the iterates produced by \(\mathsf{SPEG}\) with step-sizes \(\gamma_{k}=\omega_{k}=\omega\) such that \(0<\omega\leq\frac{1}{4L}\) satisfy \(R_{k}^{2}\leq\left(1-\frac{\omega\mu}{2}\right)^{k}R_{0}^{2}\), where \(R_{k}^{2}\coloneqq\|x_{k}-x^{*}\|^{2}+\|x_{k}-\hat{x}_{k-1}\|^{2}\).

Decreasing Step-size:In this section, we consider two different decreasing step-sizes policies for \(\mathsf{SPEG}\) applied to solve quasi-strongly monotone problems.

**Theorem 4.3**.: Let \(F\) be \(L\)-Lipschitz, \(\mu\)-quasi strongly monotone, and Assumption 3.1 hold. Let

\[\gamma_{k}=\omega_{k}\coloneqq\begin{cases}\bar{\omega},&\text{if }k\leq k^{*},\\ \frac{2k+1}{(k+1)^{2}}\frac{2}{\mu},&\text{if }k>k^{*},\end{cases}\] (11)

where \(\bar{\omega}\coloneqq\min\left\{\nicefrac{{1}}{{\left(4L\right)}},\nicefrac{{ \mu}}{{\left(18\delta\right)}}\right\}\) and \(k^{*}=\lceil\nicefrac{{4}}{{\left(\mu\bar{\omega}\right)}}\rceil\). Then for all \(K\geq k^{*}\) the iterates produced by \(\mathsf{SPEG}\) with step-sizes (11) satisfy

\[R_{K}^{2}\leq\left(\frac{k^{*}}{K}\right)^{2}\frac{R_{0}^{2}}{\exp(2)}+\frac{19 2\sigma_{*}^{2}}{\mu^{2}K},\] (12)

where \(R_{K}^{2}\coloneqq\mathbb{E}\left[\|x_{K}-x^{*}\|^{2}+\|x_{K}-\hat{x}_{K-1}\| ^{2}\right]\).

\(\mathsf{SPEG}\) with step-size policy2 (11) has two stages of convergence: during first \(k^{*}\) iterations it uses constant step-size to reach some neighborhood of the solution and then the method switches to the decreasing \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{k}})\) step-size allowing to reduce the size of the neighborhood.

Footnote 2: Similar step-size policy is used for SGD[25] and SGDA[44].

For the case of strongly monotone problems (a special case of our quasi-strongly monotone setting) [28] also analyze \(\mathsf{SPEG}\) with decreasing \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{k}})\) step-size3 under bounded variance assumption, i.e., when (8) holds with \(\delta=0\) and some \(\sigma_{*}^{2}\geq 0\), which is equivalent to the uniformly bounded variance assumption. In particular, Theorem 5[28] states \(\mathbb{E}\left[\|x_{K}-x^{*}\|^{2}\right]\leq\frac{C\sigma_{*}^{2}}{\mu^{2}K} +o\left(\frac{1}{K}\right)\) where \(C\) is some numerical constant. If the problem is strongly monotone, the result of [28] is closely related to what is obtained in Theorem 4.3: the main difference in the upper-bound is that we provide an explicit form of \(o\left(\nicefrac{{1}}{{K}}\right)\) term. Moreover, in contrast to the result from [28], Theorem 4.3 holds even when \(\delta>0\) in (8), which covers a larger class of problems.

Footnote 3: We point out the proof by [28] can be generalized to the case of constant step-size, though the authors do not consider this step-size schedule explicitly.

Following [67, 20, 6], we also consider another decreasing step-size policy.

**Theorem 4.4**.: Let \(F\) be \(L\)-Lipschitz, \(\mu\)-quasi strongly monotone, and Assumption 3.1 hold. Let \(\bar{\omega}\coloneqq\min\left\{\nicefrac{{1}}{{\left(4L\right)}},\nicefrac{{ \mu}}{{\left(18\delta\right)}}\right\}\). If for \(K\geq 0\) step-sizes \(\{\gamma_{k}\}_{k\geq 0}\), \(\{\omega_{k}\}_{k\geq 0}\) satisfy \(\gamma_{k}=\omega_{k}\) and

\[\omega_{k}\coloneqq\begin{cases}\bar{\omega},&\text{if }K\leq\frac{2}{\mu\bar{ \omega}},\\ \bar{\omega},&\text{if }K>\frac{2}{\mu\bar{\omega}}\text{ and }k\leq k_{0},\\ \frac{2}{\frac{1}{K}+\frac{2}{K}(k-k_{0})},&\text{if }K>\frac{2}{\mu\bar{ \omega}}\text{ and }k>k_{0}\end{cases},\] (13)

where \(k_{0}=\lceil\nicefrac{{K}}{{2}}\rceil\), then the iterates produced by \(\mathsf{SPEG}\) with the step-sizes defined above satisfy

\[R_{K}^{2}\leq\frac{64R_{0}^{2}}{\bar{\omega}\mu}\exp\left\{-\min\left\{\frac{ \mu}{16L},\frac{\mu^{2}}{72\delta}\right\}K\right\}+\frac{1728\sigma_{*}^{2}}{ \mu^{2}K},\] (14)

where \(R_{K}^{2}\coloneqq\mathbb{E}\left[\|x_{K}-x^{*}\|^{2}+\|x_{K}-\hat{x}_{K-1}\| ^{2}\right]\).

In contrast to (12), the rate from (14) has much better (exponentially decaying) \(o\left(\nicefrac{{1}}{{K}}\right)\) term. When \(\sigma_{*}^{2}\) is large and one needs to achieve very good accuracy of the solution, this difference is negligible, since the dominating \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{K}})\) term is the same for both bounds (up to numerical factors). However, when \(\sigma_{*}^{2}\) is small enough, e.g., the model is close to over-parameterized, or it is sufficient to achieve low accuracy of the solution, the dominating term in (14) is typically much smaller than the one from (12). Finally, it is worth mentioning, that the improvement of \(o\left(\nicefrac{{1}}{{K}}\right)\) is not achieved for free: unlike the policy from (11), step-size rule (13) relies on the knowledge of the total number of steps \(K\), which can be inconvenient for the practical use in some cases.

[MISSING_PAGE_EMPTY:8]

zero) and one \(L_{i}\) is large, \(\delta_{\text{IS}}\) is almost \(n\) times smaller than \(\delta_{\text{US}}\). In this latter scenario (when \(\delta_{\text{IS}}\) is much smaller than \(\delta_{\text{US}}\)), importance sampling could be useful and can significantly improve the performance of \(\mathsf{SPEG}\). For example, note that the exponentially decaying term in (14) decreases with \(\delta\). Hence, this term will decrease much faster with importance sampling than with uniform sampling.

## 6 Numerical Experiments

To verify our theoretical results, we run several experiments on two classes of problems, i.e., strongly monotone problems (a special case of the quasi-strongly monotone VIPs) and weak MVI problems. The code to reproduce our results can be found at https://github.com/isayantan/Single-Call-Stochastic-Extragradient-Methods.

### Strongly Monotone Problems

Our experiments consider the quadratic strongly-convex strongly-concave min-max problem from [20]. That is, we implement \(\mathsf{SPEG}\) on quadratic games of the form \(\min_{x\in\mathbb{R}^{d}}\max_{y\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_ {i}(x,y)\) where

\[f_{i}(x,y)\coloneqq\frac{1}{2}x^{\intercal}A_{i}x+x^{\intercal}B_{i}y-\frac {1}{2}y^{\intercal}C_{i}y+a_{i}^{\intercal}x-c_{i}^{\intercal}y.\] (16)

Here \(A_{i},B_{i},C_{i}\) are generated such that the quadratic game is strongly monotone and smooth. In all our experiments, we take \(n=100\) and \(d=30\). We generate positive semi-definite matrices \(A_{i},B_{i},C_{i}\) such that their eigenvalues lie in the interval \([\mu_{A},L_{A}],[\mu_{B},L_{B}]\) and \([\mu_{C},L_{C}]\) respectively. In all our experiments, we consider \(L_{A}=L_{B}=L_{C}=1\) and \(\mu_{A}=\mu_{C}=0.1,\mu_{B}=0\) unless otherwise mentioned. The vectors \(a_{i}\) and \(c_{i}\) are generated from \(\mathcal{N}_{d}(0,I_{d})\). Here, the \(i\)th operator is given by

\[F_{i}\begin{pmatrix}x\\ y\end{pmatrix}=\begin{pmatrix}\nabla_{x}f_{i}(x,y)\\ -\nabla_{y}f_{i}(x,y)\end{pmatrix}=\begin{pmatrix}A_{i}x+B_{i}y+a_{i}\\ C_{i}y-B_{i}^{\intercal}x+c_{i}\end{pmatrix}\]

In Figures 1, 2, and 3, we plot the relative error on the \(y\)-axis i.e. \(\frac{\|x_{k}-x^{*}\|^{2}}{\|x_{0}-x^{*}\|^{2}}\).

Constant vs Switching Step-size Rule.In Fig. 1, we illustrate the step-size switching rule of Theorem 4.3. We place the dotted line to mark when we switch from constant step-size to decreasing step-size. In Fig. 1, the trajectory of switching step-size rule (11) matches that of constant step-size (9) in the first phase (where \(\mathsf{SPEG}\) runs with constant step-size following (11)). However, it becomes stagnant when the constant step-size \(\mathsf{SPEG}\) reaches a neighbourhood of optimality. In contrast, the step-size of Theorem 4.3 helps the method to converge to better accuracy.

Comparison to Hsieh et al. [28].In this experiment, we compare \(\mathsf{SPEG}\) step-sizes proposed in Theorems 4.1 and 4.3 with step-sizes from [28]. To implement \(\mathsf{SPEG}\) with the step-sizes from [28], we choose \(\gamma\) and \(b\) such that \(\frac{1}{\mu}<\gamma\leq\frac{b}{4L}\) and set \(\omega_{k}=\gamma_{k}=\frac{\gamma}{k+b}\). For Fig. 1(a), we generate \(A_{i},B_{i},C_{i}\) as before. First, we sample optimal points \(x^{*},y^{*}\) from \(\mathcal{N}_{d}(0,I_{d})\) and then generate \(a_{i},c_{i}\) such that \(F(x^{*},y^{*})=0\).

\[\begin{pmatrix}a_{i}\\ c_{i}\end{pmatrix}=\begin{pmatrix}A_{i}&B_{i}\\ -B_{i}^{\intercal}&C_{i}\end{pmatrix}^{-1}\begin{pmatrix}x^{*}\\ y^{*}\end{pmatrix}.\]

In Fig. 1(a), we run the algorithms on interpolated model \(\left(F_{i}(x^{*})=0\text{ for all }i\in[n]\right)\). Since the model is interpolated, we have \(\sigma_{*}^{2}=0\) in Theorem 4.1 and linear convergence to the exact optimum asymptotically. In this setting, as shown in Fig. 1(a), our proposed step-size results in major improvement compared to the decreasing step-size selection analyzed in [28]. In Fig. 1(b), we compare the switching step-size rule with step-size from [28]. In Fig. 1(b), we generate \(a_{i},c_{i}\) from the normal distribution. In this plot, we manually switch the step-size from constant to decreasing after \(305\) steps. We observe that such a semi-empirical rule has comparable performance to the step-size selection of Hsieh et al. [28].

Figure 1: Constant vs Switching

Uniform vs. Importance Sampling.In this experiment, we highlight the advantage of using importance sampling over uniform sampling. The eigenvalues of \(A_{1},C_{1}\) are uniformly generated from the interval \([0.1,\Lambda]\) while the rest of the matrices are generated as mentioned before. We vary the value of \(\Lambda\in\{2,5,10,20\}\) and run and compare \(\mathsf{SPEG}\) with both uniform and importance sampling (see Fig. 3). For importance sampling, we use the probabilities \(p_{i}=\nicefrac{{L_{i}}}{{\sum_{j=1}^{n}L_{j}}}\). In Fig. 3, it is clear that as the value of \(\Lambda\) increases, the trajectories under uniform sampling get worse, while the trajectory under importance sampling remains almost identical. This behavior aligns well with our discussion in Section 5.

### Weak Minty Variational Inequality Problems

This experiment verifies the convergence guarantees of \(\mathsf{SPEG}\) in Theorem 4.5. Following the min-max problem mentioned in [8], we consider the objective function

\[\min_{x\in\mathbb{R}}\max_{y\in\mathbb{R}}\frac{1}{n}\sum_{i=1}^{n}\xi_{i}xy+ \frac{\zeta_{i}}{2}(x^{2}-y^{2}).\] (17)

In this experiment, we generate \(\xi_{i},\zeta_{i}\) such that \(L=8\) and \(\rho=\nicefrac{{1}}{{32}}\) for the above min-max problem [8]. We implement \(\mathsf{SPEG}\) with extrapolation step \(\gamma_{k}=0.08\) and update step \(\omega_{k}=0.01\) which satisfies the conditions on step-size in Theorem 4.5. In Fig. 4, we use a batchsize of \(6\). This plot illustrates that for some weak MVI problems the requirement on the stepsize from Theorem 4.5 can be too pessimistic and \(\mathsf{SPEG}\) with relatively small batchsize achieves reasonable accuracy of the solution. The choice of batchsize ensures that bound (15) holds and \(\delta\) is small enough to guarantee convergence of \(\mathsf{SPEG}\). We also tried to compare \(\mathsf{SPEG}\) with \(\mathsf{SEG}\)+ from [54], however, the authors do not mention their choice of update step-size. We examined several decreasing update step-size for which \(\mathsf{SEG}\)+ failed to converge. Further details on experiments can be found in Appendix G.1.

Figure 4: Trajectory of \(\mathsf{SPEG}\) for solving weak MVI. "Squared Operator Norm Error" in vertical axis denotes the \(\min_{0\leq k\leq N-1}\mathbb{E}\left[\left\|F(\hat{x}_{k})\right\|^{2}\right]\) of Theorem 4.5.

Figure 3: Comparison of \(\mathsf{SPEG}\) with Uniform and Importance Sampling for different \(\Lambda\in\{2,5,10,20\}\), where the eigenvalues of matrices \(A_{1},C_{1}\) are uniformly generated from the interval \([0.1,\Lambda]\).

Figure 2: Comparison of our \(\mathsf{SPEG}\) using our step-size against decreasing step-size of Hsieh et al. [28]. In plot (a), for constant step-size of \(\mathsf{SPEG}\) we use the upper bound of (9). In plot (b), we run our switching step-size \(\mathsf{SPEG}\) (11).

## Acknowledgement

Sayantan Choudhury acknowledges support from the Acheson J. Duncan Fund for the Advancement of Research in Statistics. Nicolas Loizou acknowledges support from CISCO Research.

## References

* [1] J. Abernethy, K. A. Lai, and A. Wibisono. Last-iterate convergence rates for min-max optimization: Convergence of hamiltonian gradient descent and consensus optimization. In _Algorithmic Learning Theory_, pages 3-47. PMLR, 2021.
* [2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _ICML_, 2017.
* [3] S. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. _NeurIPS_, 2019.
* [4] H. H. Bauschke, W. M. Moursi, and X. Wang. Generalized monotone operators and their averaged resolvents. _Mathematical Programming_, 189(1):55-74, 2021.
* [5] A. Beznosikov and A. Gasnikov. Compression and data similarity: Combination of two techniques for communication-efficient solving of distributed variational inequalities. In _Optimization and Applications: 13th International Conference, OPTIMA 2022, Petrovac, Montenegro, September 26-30, 2022, Revised Selected Papers_, pages 151-162. Springer, 2023.
* [6] A. Beznosikov, E. Gorbunov, H. Berard, and N. Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. _AISTATS_, 2023.
* [7] A. Beznosikov, B. Polyak, E. Gorbunov, D. Kovalev, and A. Gasnikov. Smooth monotone stochastic variational inequalities and saddle point problems-survey. _European Mathematical Society Magazine_, 2023.
* [8] A. Bohm. Solving nonconvex-nonconcave min-max problems exhibiting weak minty solutions. _arXiv preprint arXiv:2201.12247_, 2022.
* [9] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong. Combining deep reinforcement learning and search for imperfect-information games. _NeurIPS_, 2020.
* [10] Y. Cai, A. Oikonomou, and W. Zheng. Tight last-iterate convergence of the extragradient method for constrained monotone variational inequalities. _arXiv preprint arXiv:2204.09228_, 2022.
* [11] P. L. Combettes and T. Pennanen. Proximal methods for cohypomonotone operators. _SIAM journal on control and optimization_, 43(2):731-742, 2004.
* [12] C. D. Dang and G. Lan. On the convergence properties of non-euclidean extragradient methods for variational inequalities with generalized monotone operators. _Computational Optimization and applications_, 60(2):277-310, 2015.
* [13] C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 1466-1478, 2021.
* [14] J. Diakonikolas, C. Daskalakis, and M. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In _AISTATS_, 2021.
* [15] R. D'Orazio, N. Loizou, I. Laradji, and I. Mitliagkas. On stochastic mirror descent: Convergence analysis and adaptive variants. _ICML_, 2021.
* [16] F. Facchinei and C. Kanzow. Generalized Nash equilibrium problems. _4OR_, 5(3):173-210, 2007.
* [17] G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective on generative adversarial networks. _ICLR_, 2019.

* [18] N. Golowich, S. Pattathil, and C. Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. _NeurIPS_, 2020.
* [19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In _NeurIPS_, 2014.
* [20] E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysis and improved rates. In _AISTATS_, 2022.
* [21] E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: O (1/k) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In _AISTATS_, 2022.
* [22] E. Gorbunov, S. Horvath, P. Richtarik, and G. Gidel. Variance reduction is an antidote to byzantines: Better rates, weaker assumptions and communication compression as a cherry on the top. _ICLR_, 2023.
* [23] E. Gorbunov, A. Taylor, S. Horvath, and G. Gidel. Convergence of proximal point and extragradient-based methods beyond monotonicity: the case of negative comonotonicity. _ICML_, 2023.
* [24] R. Gower, O. Sebbouh, and N. Loizou. Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. In _AISTATS_, 2021.
* [25] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richtarik. Sgd: General analysis and improved rates. In _ICML_, 2019.
* [26] F. Hanzely and P. Richtarik. Accelerated coordinate descent with arbitrary sampling and best rates for minibatches. In _AISTATS_, 2019.
* [27] S. Horvath and P. Richtarik. Nonconvex variance reduced optimization with arbitrary sampling. In _ICML_, 2019.
* [28] Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. _NeurIPS_, 2019.
* [29] Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. _NeurIPS_, 2020.
* [30] A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* [31] A. Khaled, O. Sebbouh, N. Loizou, R. M. Gower, and P. Richtarik. Unified analysis of stochastic gradient methods for composite convex and smooth optimization. _Journal of Optimization Theory and Applications_, 2020.
* [32] G. M. Korpelevich. The extragradient method for finding saddle points and other problems. _Matecon_, 12:747-756, 1976.
* [33] S. Lee and D. Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems. _NeurIPS_, 2021.
* [34] C. J. Li, Y. Yu, N. Loizou, G. Gidel, Y. Ma, N. Le Roux, and M. Jordan. On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging. In _AISTATS_, 2022.
* [35] T. Lin, C. Jin, and M. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In _ICML_, 2020.
* [36] T. Lin, Z. Zhou, P. Mertikopoulos, and M. Jordan. Finite-time last-iterate convergence for multi-agent learning in games. In _ICML_, 2020.
* [37] M. Liu, Y. Mroueh, J. Ross, W. Zhang, X. Cui, P. Das, and T. Yang. Towards better understanding of adaptive gradient algorithms in generative adversarial nets. _ICLR_, 2020.

* [38] M. Liu, H. Rafique, Q. Lin, and T. Yang. First-order convergence theory for weakly-convex-weakly-concave min-max problems. _J. Mach. Learn. Res._, 22:169-1, 2021.
* [39] N. Loizou and P. Richtarik. A new perspective on randomized gossip algorithms. In _2016 IEEE global conference on signal and information processing (GlobalSIP)_, pages 440-444. IEEE, 2016.
* [40] N. Loizou and P. Richtarik. Convergence analysis of inexact randomized iterative methods. _SIAM Journal on Scientific Computing_, 42(6):A3979-A4016, 2020.
* [41] N. Loizou and P. Richtarik. Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. _Computational Optimization and Applications_, 77(3):653-710, 2020.
* [42] N. Loizou and P. Richtarik. Revisiting randomized gossip algorithms: General framework, convergence rates and novel block and accelerated protocols. _IEEE Transactions on Information Theory_, 67(12):8300-8324, 2021.
* [43] N. Loizou, H. Berard, A. Jolicoeur-Martineau, P. Vincent, S. Lacoste-Julien, and I. Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In _ICML_, 2020.
* [44] N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. _NeurIPS_, 2021.
* [45] N. Loizou, S. Vaswani, I. Hadj Laradji, and S. Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In _AISTATS_, 2021.
* [46] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* [47] P. Mertikopoulos and Z. Zhou. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1):465-507, 2019.
* [48] P. Mertikopoulos, B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. _ICLR_, 2019.
* [49] K. Mishchenko, D. Kovalev, E. Shulgin, P. Richtarik, and Y. Malitsky. Revisiting stochastic extragradient. In _AISTATS_, 2020.
* [50] A. Mokhtari, A. Ozdaglar, and S. Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In _AISTATS_, 2020.
* [51] H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. _NeurIPS_, 2016.
* [52] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* [53] Y. Nesterov. Linear convergence of first order methods for non-strongly convex optimization. _Mathematical Programming_, 175:69-107, 2019.
* [54] T. Pethick, P. Patrinos, O. Fercoq, V. Cevhera, et al. Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems. In _ICLR_, 2022.
* [55] T. Pethick, O. Fercoq, P. Latafat, P. Patrinos, and V. Cevher. Solving stochastic weak minty variational inequalities without increasing batch size. _ICLR_, 2023.
* [56] L. D. Popov. A modification of the arrow-hurwicz method for search of saddle points. _Mathematical notes of the Academy of Sciences of the USSR_, 28(5):845-848, 1980.
* [57] X. Qian, Z. Qu, and P. Richtarik. Saga with arbitrary sampling. In _ICML_, 2019.

* [58] Z. Qu and P. Richtarik. Coordinate descent with arbitrary sampling i: Algorithms and complexity. _Optimization Methods and Software_, 31(5):829-857, 2016.
* [59] P. Richtarik and M. Takac. On optimal probabilities in stochastic coordinate descent methods. _arXiv preprint arXiv:1310.3438_, 2013.
* [60] P. Richtarik and M. Takac. Stochastic reformulations of linear systems: algorithms and convergence theory. _SIAM Journal on Matrix Analysis and Applications_, 41(2):487-524, 2020.
* [61] M. Rolinek and G. Martius. L4: Practical loss-based stepsize adaptation for deep learning. _NeurIPS_, 2018.
* [62] E. K. Ryu, K. Yuan, and W. Yin. Ode analysis of stochastic gradient methods with optimism and anchoring for minimax problems. _arXiv preprint arXiv:1905.10899_, 2019.
* [63] O. Sebbouh, N. Gazagnadou, S. Jelassi, F. Bach, and R. Gower. Towards closing the gap between the theory and practice of svrg. _NeurIPS_, 2019.
* [64] S. Sokota, R. D'Orazio, J. Z. Kolter, N. Loizou, M. Lanctot, I. Mitliagkas, N. Brown, and C. Kroer. A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. _ICLR_, 2023.
* [65] M. V. Solodov and B. F. Svaiter. A hybrid approximate extragradient-proximal point algorithm using the enlargement of a maximal monotone operator. _Set-Valued Analysis_, 7(4):323-345, 1999.
* [66] C. Song, Z. Zhou, Y. Zhou, Y. Jiang, and Y. Ma. Optimistic dual extrapolation for coherent non-monotone variational inequalities. In _NeurIPS_, 2020.
* [67] S. U. Stich. Unified optimal analysis of the (stochastic) gradient method. _arXiv preprint arXiv:1907.04232_, 2019.
* [68] R. Szlendak, A. Tyurin, and P. Richtarik. Permutation compressors for provably faster distributed nonconvex optimization. _ICLR_, 2022.
* [69] Q. Tran Dinh, D. Liu, and L. Nguyen. Hybrid variance-reduced sgd algorithms for minimax problems with nonconvex-linear function. _NeurIPS_, 2020.
* [70] S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron. In _AISTATS_, 2019.
* [71] S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. _NeurIPS_, 2019.
* [72] J. Wang, T. Zhang, S. Liu, P.-Y. Chen, J. Xu, M. Fardad, and B. Li. Adversarial attack generation empowered by min-max optimization. _NeurIPS_, 2021.
* [73] Y. Yu, T. Lin, E. V. Mazumdar, and M. Jordan. Fast distributionally robust learning with variance-reduced min-max optimization. In _AISTATS_, 2022.

## Supplementary Material

We organize the Supplementary Material as follows: Section A discusses the existing literature related to our work. In Section B, we present some technical lemmas required for our analysis, while in Section C, we provide a simple problem where the bounded variance assumption does not hold. Then, in Section D, we provide the proofs of propositions related to Expected Residual. Next, Section E presents the proofs of the main theorems, while a proposition related to arbitrary sampling is proved in Section F. Finally, additional numerical experiments are presented in Section G.

###### Contents

* 1 Introduction
	* 1.1 Main Contributions
* 2 Stochastic Reformulation of VIPs & Single-Call Extragradient Methods
* 3 Expected Residual
* 4 Convergence Analysis
	* 4.1 Quasi-Strongly Monotone Problems
	* 4.2 Weak Minty Variational Inequality Problems
* 5 Beyond Uniform Sampling
* 6 Numerical Experiments
	* 6.1 Strongly Monotone Problems
	* 6.2 Weak Minty Variational Inequality Problems
* A Further Related Work
* B Technical Preliminaries
* C Example: A Problem where the Bounded Variance Condition not Hold
* D Proofs of Results on Expected Residual
* D.1 Proof of Lemma 3.2
* D.2 Proof of Proposition 3.3
* D.3 Proof of Proposition 3.4
* E Main Convergence Analysis Results
* E.1 Proof of Theorem 4.1
* E.2 Proof of Theorem 4.3
* E.3 Proof of Theorem 4.4
* E.4 Proof of Theorem E.4
* E.5 Proof of Theorem 4.5
* F Further Results on Arbitrary Sampling
* F.1 Proof of Proposition 5.1
* G Numerical Experiments
* G.1 More Details on the Numerical Experiments of Section 6
* G.2 Additional Experiments
* G.2.1 Strongly Monotone Quadratic Game:
* G.2.2 Weak Minty VIPs Continued
Further Related Work

The references necessary to motivate our work and connect it to the most relevant literature are included in the appropriate sections of the main body of the paper. In this section, we present a broader view of the literature, including more details on closely related work and more references to papers that are not directly related to our main results.

* **Classes of Structured Non-monotone Operators.** With an increasing interest in improved computational speed, first-order methods are the primary choice for solving VIPs. However, computation of an approximate first-order locally optimal solution of a general non-monotone VIP is intractable [13, 33]. It motivates us to exploit the additional structures prevalent in large classes of non-monotone VIPs. Recently [20, 28] provide convergence guarantees of stochastic methods for solving quasi-strongly monotone VIPs, while [29] for problems satisfying error-bound conditions. [14] defined the notion of a weak MVI (4) covering classes of non-monotone VIPs.
* **Assumptions on Operator Noise.** The standard analysis of stochastic methods for solving VIPs relies on bounded variance assumption. [8, 14, 28, 17] use bounded variance assumption (i.e. \(\mathbb{E}\|F_{i}(x)-F(x)\|^{2}\leq\sigma^{2}\) for all \(x\)) while [52, 1] assume bounded operators for their analysis. However, there are examples of simple quadratic games that do not satisfy these conditions. It has motivated researchers to look for alternative/relaxed assumptions on distributions. [44] provides convergence of Stochastic Gradient Descent Ascent Method under Expected Cocoercivity. [29, 49] considered alternative assumptions for analyzing Stochastic Extragradient Methods that do not imply boundedness of the variance. However, there is no analysis of single-call extragradient methods without bounded variance assumption.
* **Weak Minty Variational Inequalities.** Numerous contemporary studies look to identify first-order methods for efficiently solving min-max optimization problems. It varies from simple convex-concave to nontrivial nonconvex nonconcave objectives. Though there has been a significant development in the convex-concave setting, [13] demonstrates that even finding local solutions are intractable for general nonconvex nonconcave objectives. Therefore, researchers seek to identify the structure of objective functions for which it is possible to resolve the intractability issues. [14] proposes the notion of non-monotonicity, which generalizes the existence of a Minty solution (i.e., \(\rho=0\) in (4)). This problem is known as weak Minty variational inequality in the literature. [14, 54] provides convergence guarantees of the Extragradient Method for weak Minty variational inequality. They establish a convergence rate of \(\mathcal{O}(1/k)\) for the squared operator norm. [33] shows that it is possible to have an accelerated extragradient method even for non-monotone problems. Furthermore, [8] provides a convergence guarantee for the SOG with a complexity bound of \(\mathcal{O}(\varepsilon^{-2})\). However, all papers exploring stochastic extragradient methods for solving weak Minty variational inequality consider bounded variance assumption [8, 14]. Moreover, all algorithms solving Weak Minty variational inequality require increasing batchsize. Recently, [55] introduced BCSEG+ which can solve weak minty variational inequality without increasing batchsize. BCSEG+ involves three oracle calls per iteration and addition of a bias-corrected term in the extrapolation step.
* **Arbitrary Sampling Paradigm.** As we mentioned in the main paper, the stochastic reformulation (6) of the original problem (1) allows us to analyze single-call extragradient methods under the arbitrary sampling paradigm. That is, provide a unified analysis for SPEG that captures multiple sampling strategies, including \(\tau\)-minibatch and importance samplings. An arbitrary sampling analysis of a stochastic optimnization method was first proposed in the context of the randomized coordinate descent method for solving strongly convex functions in [59]. Since then, several other stochastic methods were studied in this regime, including accelerated coordinate descent algorithms [58, 26], randomized iterative methods for solving consistent linear systems [60, 41, 40], randomized gossip algorithms [39, 42], stochastic gradient descent (SGD) [25, 24], and variance reduced methods [57, 27, 31]. The first analysis of stochastic algorithms under the arbitrary sampling paradigm for solving variational inequality problems was proposed in [43, 44]. In [43, 44], the authors focus on algorithms like the stochastic Hamiltonian method, the stochastic gradient descent ascent, and the stochastic consensus optimization. These ideas were later extended to the case of Stochastic Extragradient by [20]. To the best of our knowledge, our work is the first that provides an analysis of single-call extragradient methods under the arbitrary sampling paradigm.

* **Overparameterized Models and Interpolation.** For a function \(f(x):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\) we say that interpolation condition holds if there exists \(x^{*}\) such that \(\min_{x}f_{i}(x)=f_{i}(x^{*})\) for all \(i\in[n]\) (or equivalently \(\nabla f_{i}(x^{*})=0\) for smooth convex functions) [24]. The interpolation condition is satisfied when the underlying models are sufficiently overparameterized [70]. Some known examples include deep matrix factorization and classification using neural networks [3, 61, 70]. The interpolated model structure enables SGD and other optimization algorithms to have faster convergence [24, 45, 15]. Inspired by this, one can extend the notion of the interpolation condition to operators. In this scenario, we say that the VIP (1) is interpolated if there exists solution \(x^{*}\) of (1) such that \(F_{i}(x^{*})=0\) for all \(i\in[n]\). This concept has been explored for analyzing the stochastic extragradient method in [71, 34]. We highlight that our proposed theorems show fast convergence of SPEG in this interpolated regime (when \(\sigma_{*}^{2}=0\)). To the best of our knowledge, our work is the first that proves such convergence for SPEG. In Fig. 1(a), we experimentally verify the fast convergence for solving a strongly monotone interpolated problem.
* **Deterministic Extragradient Methods.** The Extragradient method (EG) [32] and its single-call variant, Optimistic Gradient (OG) [56], were proposed to overcome the convergence issues of gradient descent-ascent method for solving monotone problems. Since their introduction, these methods have been revisited and explored in various ways. [50] analyzed EG and OG as an approximation of the Proximal Point method to solve bilinear and strongly convex-strongly concave min-max problems. [65] and [62] provide the best-iterate convergence guarantees of EG and OG with a rate of \(\mathcal{O}(\nicefrac{{1}}{{K}})\) for solving monotone problems. However, providing a last-iterate convergence rate of EG and OG for monotone VIPs has been a long-lasting open problem that was only recently resolved. The works of [18, 21, 10] prove a last-iterate \(\mathcal{O}(\nicefrac{{1}}{{K}})\) convergence rate for these methods. Finally, in the deterministic setting, some recent works provide convergence analysis of EG and OG for solving weak MVI (4) [14, 54, 8, 23].

Technical Preliminaries

Throughout our work, we assume

**Assumption B.1**.: Operator \(F\) in (1) is L Lipschitz, i.e., \(\forall x,y\in\mathbb{R}^{d}\) operator \(F\) satisfies

\[\|F(x)-F(y)\|\leq L\|x-y\|.\] (18)

Operators \(F_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) of problem (1) are \(L_{i}\)- Lipschitz, i.e., \(\forall x,y\in\mathbb{R}^{d}\) operator \(F_{i}\) satisfies

\[\|F_{i}(x)-F_{i}(y)\|\leq L_{i}\|x-y\|.\] (19)

In our proofs, we often use the following simple inequalities.

**Lemma B.2**.: For all \(a,b,a_{1},a_{2},\cdots a_{n}\in\mathbb{R}^{d},n\geq 1,\alpha>0\), we have the following inequalities:

\[\langle a,b\rangle \leq \|a\|\|b\|,\] (20) \[\langle a,b\rangle \leq \frac{1}{2\alpha}\|a\|^{2}+\frac{\alpha}{2}\|b\|^{2},\] (21) \[\|a+b\|^{2} \leq 2\|a\|^{2}+2\|b\|^{2},\] (22) \[\|a\|^{2} \geq \frac{1}{2}\|a+b\|^{2}-\|b\|^{2},\] (23) \[\left\|\sum_{i=1}^{n}a_{i}\right\|^{2} \leq n\sum_{i=1}^{n}\|a_{i}\|^{2}.\] (24)

Inequality (22) is well known as Young's Inequality. Now, we present a simple property of unbiased estimators.

**Lemma B.3**.: For an unbiased estimator \(g\) of operator \(F\) i.e. \(\mathbb{E}[g(x)]=F(x)\) we have

\[\mathbb{E}\|g(x)-F(x)\|^{2}=\mathbb{E}\|g(x)\|^{2}-\|F(x)\|^{2}.\] (25)

Next, we present the following lemma from [67], which plays a vital role in proving the convergence guarantee of Theorem 4.4.

**Lemma B.4**.: (Simplified Version of Lemma 3 from [67]) Let the non-negative sequence \(\{r_{k}\}_{k\geq 0}\) satisfy the relation \(r_{k+1}\leq(1-a\gamma_{k})r_{k}+c\gamma_{k}^{2}\) for all \(k\geq 0\), parameters \(a,c\geq 0\) and any non-negative sequence \(\{\gamma_{k}\}_{k\geq 0}\) such that \(\gamma_{k}\leq\frac{1}{h}\) for some \(h\geq a,h>0\). Then for any \(K\geq 0\) one can choose \(\{\gamma_{k}\}_{k\geq 0}\) as follows:

\[\text{if }K\leq\frac{h}{a},\qquad\gamma_{k}=\frac{1}{h},\] \[\text{if }K>\frac{h}{a}\text{ and }k<k_{0},\qquad\gamma_{k}=\frac{1}{h},\] \[\text{if }K>\frac{h}{a}\text{ and }k\geq k_{0},\qquad\gamma_{k}= \frac{2}{a(\kappa+k-k_{0})},\]

where \(\kappa=\frac{2h}{a}\) and \(k_{0}=\left\lceil\frac{K}{2}\right\rceil\). For this choice of \(\gamma_{k}\) the following inequality holds:

\[r_{K}\leq\frac{32hr_{0}}{a}\exp\left(-\frac{aK}{2h}\right)+\frac{36c}{a^{2}K}.\]

We use the next lemma to bound the trace of matrix products.

**Lemma B.5**.: For positive semidefinite matrices \(A,B\in\mathbb{R}^{d\times d}\) we have

\[\text{tr}(AB)\leq\lambda_{\max}(B)\text{tr}(A),\] (26)

where \(\lambda_{\max}(B)\) denotes the maximum eigenvalue of \(B\).

Next lemma proves equivalence of \(\mathsf{SPEG}\) and \(\mathsf{SOG}\):

**Proposition B.6** (**Equivalence of \(\mathsf{SPEG}\) and \(\mathsf{SOG}\)**).: Consider the iterates of \(\mathsf{SPEG}\left\{\bm{x}_{k},\bm{\hat{x}}_{k}\right\}_{k=1}^{\infty}\) with constant step-sizes \(\omega_{k}=\omega,\gamma_{k}=\gamma\) in (7). Then \(\hat{x}_{k}\) follows the iteration rule of \(\mathsf{SOG}\) i.e.

\[\hat{x}_{k+1}=\hat{x}_{k}-\omega_{k}F_{v_{k}}(\hat{x}_{k})-\gamma_{k}[F_{v_{k }}(\hat{x}_{k})-F_{v_{k-1}}(x_{k-1})]\] (27)

Proof.: From the update rule of \(\mathsf{SPEG}\) (7) we get

\[\hat{x}_{k+1}= x_{k+1}-\gamma F_{v_{k}}(\hat{x}_{k})\] \[= x_{k}-\omega F_{v_{k}}(\hat{x}_{k})-\gamma F_{v_{k}}(\hat{x}_{k})\] \[= x_{k}-(\omega+\gamma)F_{v_{k}}(\hat{x}_{k})\] \[= \hat{x}_{k}+\gamma F_{v_{k-1}}(\hat{x}_{k-1})-(\omega+\gamma)F_{v _{k}}(\hat{x}_{k})\] \[= \hat{x}_{k}-\omega F_{v_{k}}(\hat{x}_{k})-\gamma\Big{(}F_{v_{k}}( \hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\Big{)}.\]

This shows that \(\mathsf{SPEG}\) iterations are equivalent to \(\mathsf{SOG}\), with \(\hat{x}_{k}\) being the \(k\)-th iterate of \(\mathsf{SOG}\). 

## Appendix C Example: A Problem where the Bounded Variance Condition not Hold

Here, we provide a simple problem that does not satisfy the bounded variance assumption. Consider the linear regression problem

\[\min_{x\in\mathbb{R}}f(x):=\frac{1}{2}(a_{1}x-b_{1})^{2}+\frac{1}{2}(a_{2}x-b _{2})^{2}\]

where \(x\in\mathbb{R}\). Here \(f_{1}(x)=(a_{1}x-b_{1})^{2}\) and \(f_{2}(x)=(a_{2}x-b_{2})^{2}\). Now consider the estimator \(g(x)\) of \(\nabla f(x)\) under uniform sampling i.e. \(g(x)\) takes the value \(\nabla f_{1}(x)\) with probability \(\frac{1}{2}\) and \(\nabla f_{2}(x)\) with probability \(\frac{1}{2}\). Then we have

\[\mathbb{E}\|g(x)-\nabla f(x)\|^{2} = \frac{1}{2}\|\nabla f_{1}(x)-\nabla f(x)\|^{2}+\frac{1}{2}\| \nabla f_{2}(x)-\nabla f(x)\|^{2}\] \[= \frac{1}{2}\cdot\frac{1}{4}\|\nabla f_{1}(x)-\nabla f_{2}(x)\|^ {2}+\frac{1}{2}\cdot\frac{1}{4}\|\nabla f_{2}(x)-\nabla f_{1}(x)\|^{2}\] \[= \frac{1}{4}\|\nabla f_{1}(x)-\nabla f_{2}(x)\|^{2}\] \[= \frac{1}{4}\left(2(a_{1}x-b_{1})a_{1}-2(a_{2}x-b_{2})a_{2}\right) ^{2}\] \[= \big{(}(a_{1}^{2}-a_{2}^{2})x-(a_{1}b_{1}-a_{2}b_{2})\big{)}^{2}\]

Therefore, \(\mathbb{E}\|g(x)-\nabla f(x)\|^{2}\) is a quadratic function of \(x\) with the coefficient of \(x\) being positive. Hence, as \(x\to\infty\), we have \(\mathbb{E}\|g(x)-\nabla f(x)\|^{2}\to\infty\), which means that a constant can not bound the variance.

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_FAIL:22]

### Proof of Proposition 3.4

Here we enlist the assumptions made on operators. Suppose \(g\) is an estimator of operator \(F\).

1. **Bounded Operator:** \(\mathbb{E}\|g(x)\|^{2}\leq\sigma^{2}\)
2. **Bounded Variance:** \(\mathbb{E}\|g(x)-F(x)\|^{2}\leq\sigma^{2}\)
3. **Growth Condition:** \(\mathbb{E}\|g(x)\|^{2}\leq\alpha\|F(x)\|^{2}+\beta\)
4. **Expected Co-coercivity:** \(\mathbb{E}\|g(x)-g(x^{*})\|^{2}\leq l_{F}\left\langle F(x),x-x^{*}\right\rangle\)
5. **Expected Residual:** \(\mathbb{E}\|(g(x)-g(x^{*}))-(F(x)-F(x^{*}))\|^{2}\leq\dfrac{\delta}{2}\|x-x^{ *}\|^{2}\)
6. **Bound from Lemma 3.2:** \(\mathbb{E}\|g(x)\|^{2}\leq\delta\|x-x^{*}\|^{2}+\|F(x)\|^{2}+2\sigma_{*}^{2}\)
7. \(F_{i}\) are Lipschitz: \(\|F_{i}(x)-F_{i}(y)\|\leq L_{i}\|x-y\|\quad\forall\;i=1,\ldots,n\)

Proof.: Here we will prove Proposition 3.4

* \(1\implies 2\). Note that \(\mathbb{E}\|g(x)\|^{2}\leq\sigma^{2}\leq\|F(x)\|^{2}+\sigma^{2}\implies\mathbb{ E}\|g(x)-F(x)\|\leq\sigma^{2}\).
* \(2\implies 3\). Here \(\mathbb{E}\|g(x)-F(x)\|^{2}\leq\sigma^{2}\implies\mathbb{E}\|g(x)\|^{2}\leq \|F(x)\|^{2}+\sigma^{2}\) as \(g\) is an unbiased for estimator of \(F\). Then take \(\alpha=1\) and \(\beta=\sigma^{2}\).
* \(3\implies 6\). Note that \(\mathbb{E}\|g(x)\|^{2}\leq\alpha\|F(x)\|^{2}+\beta\leq\alpha L^{2}\|x-x^{*}\|^ {2}+\beta\). The last inequality follows from lipschitz property of \(F\) and \(F(x^{*})=0\). Then choose \(\delta=\alpha L^{2}\) and \(\sigma_{*}^{2}=\nicefrac{{\beta}}{{2}}\) to get the result.
* \(4\implies 5\). Note that expected cocoercivity and \(L\)-Lipschitzness of \(F\) imply \(\mathbb{E}\|(g(x)-g(x^{*}))-(F(x)-F(x^{*}))\|^{2}=\mathbb{E}\|g(x)-g(x^{*})\|^ {2}-\|F(x)-F(x^{*})\|^{2}\leq\mathbb{E}\|g(x)-g(x^{*})\|^{2}\leq l_{F}\left\langle F (x),x-x^{*}\right\rangle\stackrel{{\eqref{eq:2}}}{{\leq}}\frac{l _{F}}{2L}\|F(x)\|^{2}+\frac{l_{F}L}{2}\|x-x^{*}\|^{2}\leq l_{F}L\|x-x^{*}\|^{2}\).
* \(7\implies 5\). This follows from Proposition D.1.
* \(5\implies 6\). This follows from Lemma 3.2Main Convergence Analysis Results

First, we present some results followed by iterates of \(\mathsf{SPEG}\). These will play a key role in proving the Theorems later in this section. Recall that iterates of \(\mathsf{SPEG}\) are

\[\hat{x}_{k} =x_{k}-\gamma_{k}F_{v_{k-1}}(\hat{x}_{k-1}),\] \[x_{k+1} =x_{k}-\omega_{k}F_{v_{k}}(\hat{x}_{k}).\]

**Lemma E.1**.: For \(\mathsf{SPEG}\) iterates with step-size \(\omega_{k}=\gamma_{k}=\omega\), we have

\[\|x_{k+1}-x^{*}\|^{2}=\|x_{k+1}-\hat{x}_{k}\|^{2}+\|x_{k}-x^{*}\|^{2}-\|\hat{x }_{k}-x_{k}\|^{2}-2\omega\left\langle F_{v_{k}}(\hat{x}_{k}),\hat{x}_{k}-x^{* }\right\rangle.\] (31)

Proof.: We have

\[\|x_{k+1}-x^{*}\|^{2} = \|x_{k+1}-\hat{x}_{k}+\hat{x}_{k}-x_{k}+x_{k}-x^{*}\|^{2}\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}+\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}+2\left\langle\hat{x}_{k}-x_{k},x_{k}-x^{*}\right\rangle\] \[\quad+2\left\langle x_{k+1}-\hat{x}_{k},\hat{x}_{k}-x_{k}\right\rangle +2\left\langle x_{k+1}-\hat{x}_{k},x_{k}-x^{*}\right\rangle\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}+\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}+2\left\langle x_{k+1}-\hat{x}_{k},\hat{x}_{k}-x^{*}\right\rangle\] \[\quad+2\left\langle\hat{x}_{k}-x_{k},x_{k}-x^{*}\right\rangle\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}+\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}+2\left\langle x_{k+1}-\hat{x}_{k},\hat{x}_{k}-x^{*}\right\rangle\] \[\quad+2\left\langle\hat{x}_{k}-x_{k},x_{k}-x^{*}\right\rangle-2 \|\hat{x}_{k}-x_{k}\|^{2}\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}+2\left\langle x_{k+1}-\hat{x}_{k},\hat{x}_{k}-x^{*}\right\rangle\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}+2\left\langle x_{k+1}-x_{k},\hat{x}_{k}-x^{*}\right\rangle\] \[= \|x_{k+1}-\hat{x}_{k}\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}+\|x_{k}-x^{ *}\|^{2}-2\omega\left\langle F_{v_{k}}(\hat{x}_{k}),\hat{x}_{k}-x^{*}\right\rangle.\]

**Lemma E.2**.: Let \(F\) be \(L\)-Lipschitz, and let \(\mathsf{ER}\) hold. Then \(\mathsf{SPEG}\) iterates satisfy

\[\mathbb{E}_{\mathcal{D}}\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\| ^{2}\leq \delta\|\hat{x}_{k}-x^{*}\|^{2}+2\delta\|\hat{x}_{k-1}-x^{*}\|^{2}+2L^{2} \|\hat{x}_{k}-\hat{x}_{k-1}\|^{2}+6\sigma_{*}^{2}.\] (32)

Proof.: \[\mathbb{E}_{\mathcal{D}}\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1 })\|^{2} = \mathbb{E}_{\mathcal{D}}\|F_{v_{k}}(\hat{x}_{k})-F(\hat{x}_{k}) \|^{2}+\mathbb{E}_{\mathcal{D}}\|F(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\] \[\quad+2\mathbb{E}_{\mathcal{D}}\left\langle F_{v_{k}}(\hat{x}_{k} )-F(\hat{x}_{k}),F(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\right\rangle\] \[= \mathbb{E}_{v_{k}}\|F_{v_{k}}(\hat{x}_{k})-F(\hat{x}_{k})\|^{2}+ \mathbb{E}_{\mathcal{D}}\|F(\hat{x}_{k})-F(\hat{x}_{k-1})\|^{2}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \mathbb{E}_{\mathcal{D}}\|F_{v_{k}}(\hat{x}_{k})-F(\hat{x}_{k})\|^{2}+2 \mathbb{E}_{\mathcal{D}}\|F(\hat{x}_{k})-F(\hat{x}_{k-1})\|^{2}\] \[\quad+2\mathbb{E}_{\mathcal{D}}\|F(\hat{x}_{k-1})-F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}\] \[= \mathbb{E}_{\mathcal{D}}\|F_{v_{k}}(\hat{x}_{k})\|^{2}-\|F(\hat{x} _{k})\|^{2}+2\|F(\hat{x}_{k})-F(\hat{x}_{k-1})\|^{2}\] \[\quad+2\mathbb{E}_{\mathcal{D}}\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2} -2\|F(\hat{x}_{k-1})\|^{2}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \delta\|\hat{x}_{k}-x^{*}\|^{2}+2\delta\|\hat{x}_{k-1}-x^{*}\|^{2}+6 \sigma_{*}^{2}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \delta\|\hat{x}_{k}-x^{*}\|^{2}+2\delta\|\hat{x}_{k-1}-x^{*}\|^{2}+6 \sigma_{*}^{2}\] \[\quad+2L^{2}\|\hat{x}_{k}-\hat{x}_{k-1}\|^{2}.\]

**Lemma E.3**.: For \(\omega\in\left[0,\frac{1}{4L}\right]\) the following two conditions hold:

\[2\omega(\mu-\omega\delta)+8\omega^{2}L^{2}-1\leq 0,\] (33) \[\text{and}\quad 8\omega^{2}(\delta+L^{2})\leq 1-\omega\mu+9 \omega^{2}\delta.\] (34)

Proof.: Note that for \(\omega\in\left[0,\frac{1}{4L}\right]\), we have

\[2\omega(\mu-\omega\delta)+8\omega^{2}L^{2}-1\overset{\omega^{2}\delta\geq 0}{ \leq}2\omega\mu+8\omega^{2}L^{2}-1\overset{\omega\leq\frac{1}{2L}}{\leq}\frac {\mu}{2L}+\frac{1}{2}-1\overset{\mu\leq L}{\leq}0.\]

This proves the first condition. The second condition is equivalent to \(\omega(\mu-\omega\delta)+8\omega^{2}L^{2}-1\leq 0\), which is again true using similar arguments. 

### Proof of Theorem 4.1

Proof.: For \(\omega\in\left[0,\frac{\mu}{18\delta}\right]\) we have \(\omega(\mu-9\omega\delta)\geq 0\) and \(1-\omega(\mu-9\omega\delta)\leq 1-\frac{\omega\mu}{2}\). Then we derive

\[\mathbb{E}_{\mathcal{D}}[\|x_{k+1}-x^{*}\|^{2}+\|x_{k+1}-\hat{x} _{k}\|^{2}] \overset{\eqref{eq:20}}{=} \|x_{k}-x^{*}\|^{2}+2\mathbb{E}_{\mathcal{D}}\|x_{k+1}-\hat{x}_{k }\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}\] \[-2\omega\mathbb{E}_{\mathcal{D}}\left\langle F_{v_{k}}(\hat{x}_ {k}),\hat{x}_{k}-x^{*}\right\rangle\] \[= \|x_{k}-x^{*}\|^{2}+2\mathbb{E}_{\mathcal{D}}\|x_{k+1}-\hat{x}_{k }\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}\] \[-2\omega\left\langle F(\hat{x}_{k}),\hat{x}_{k}-x^{*}\right\rangle\] \[\overset{\eqref{eq:20}}{\leq} \|x_{k}-x^{*}\|^{2}+2\mathbb{E}_{\mathcal{D}}\|x_{k+1}-\hat{x}_{k }\|^{2}-\|\hat{x}_{k}-x_{k}\|^{2}\] \[-2\omega\mu\|\hat{x}_{k}-x^{*}\|^{2}\] \[= \|x_{k}-x^{*}\|^{2}+2\omega^{2}\mathbb{E}_{\mathcal{D}}\|F_{v_{k }}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\] \[-\|\hat{x}_{k}-x_{k}\|^{2}-2\omega\mu\|\hat{x}_{k}-x^{*}\|^{2}\] \[\overset{\eqref{eq:20}}{\leq} \|x_{k}-x^{*}\|^{2}+2\omega^{2}\bigg{(}\delta\|\hat{x}_{k}-x^{*}\| ^{2}+2\delta\|\hat{x}_{k-1}-x^{*}\|^{2}\] \[+2L^{2}\|\hat{x}_{k}-\hat{x}_{k-1}\|^{2}+6\sigma_{*}^{2}\bigg{)}- \|\hat{x}_{k}-x_{k}\|^{2}\] \[-2\omega\mu\|\hat{x}_{k}-x^{*}\|^{2}\] \[= \|x_{k}-x^{*}\|^{2}-2\omega(\mu-\omega\delta)\|\hat{x}_{k}-x^{*} \|^{2}\] \[+4\omega^{2}\delta\|\hat{x}_{k-1}-x^{*}\|^{2}+4\omega^{2}L^{2}\| \hat{x}_{k}-\hat{x}_{k-1}\|^{2}\] \[-\|\hat{x}_{k}-x_{k}\|^{2}+12\omega^{2}\sigma_{*}^{2}\] \[\overset{\eqref{eq:20}}{\leq} \|x_{k}-x^{*}\|^{2}-\omega(\mu-\omega\delta)\|x_{k}-x^{*}\|^{2}\] \[+2\omega(\mu-\omega\delta)\|x_{k}-\hat{x}_{k}\|^{2}+4\omega^{2} \delta\|\hat{x}_{k-1}-x^{*}\|^{2}\] \[+4\omega^{2}L^{2}\|\hat{x}_{k}-\hat{x}_{k-1}\|^{2}-\|\hat{x}_{k}- x_{k}\|^{2}\] \[+12\omega^{2}\sigma_{*}^{2}\] \[\overset{\eqref{eq:20}}{\leq} \|x_{k}-x^{*}\|^{2}-\omega(\mu-\omega\delta)\|x_{k}-x^{*}\|^{2}\] \[+2\omega(\mu-\omega\delta)\|x_{k}-\hat{x}_{k}\|^{2}+8\omega^{2} \delta\|\hat{x}_{k-1}-x_{k}\|^{2}\] \[+8\omega^{2}\delta\|x_{k}-x^{*}\|^{2}+8\omega^{2}L^{2}\|\hat{x}_{k }-x_{k}\|^{2}\] \[+8\omega^{2}L^{2}\|x_{k}-\hat{x}_{k-1}\|^{2}-\|\hat{x}_{k}-x_{k}\| ^{2}+12\omega^{2}\sigma_{*}^{2}\] \[= (1-\omega\mu+9\omega^{2}\delta)\|x_{k}-x^{*}\|^{2}\] \[+(8\omega^{2}\delta+8\omega^{2}L^{2})\|x_{k}-\hat{x}_{k-1}\|^{2}\] \[+(2\omega(\mu-\omega\delta)+8\omega^{2}L^{2}-1)\|x_{k}-\hat{x}_{k }\|^{2}+12\omega^{2}\sigma_{*}^{2}.\]Then using (33), (34) we have

\[\mathbb{E}_{\mathcal{D}}[\|x_{k+1}-x^{*}\|^{2}+\|x_{k+1}-\hat{x}_{k} \|^{2}] \leq (1-\omega\mu+9\omega^{2}\delta)\bigg{(}\|x_{k}-x^{*}\|^{2}+\|x_{k}- \hat{x}_{k-1}\|^{2}\bigg{)}\] \[+12\omega^{2}\sigma_{*}^{2}.\]

Then we take total expectation with respect to the algorithm to obtain the following recurrence:

\[R_{k+1}^{2}\leq(1-\omega\mu+9\omega^{2}\delta)R_{k}^{2}+12\omega^{2}\sigma_{*} ^{2}.\] (35)

Using the inequality \(1-\omega(\mu-9\omega\delta)\leq 1-\frac{\omega\mu}{2}\), we have

\[\mathbb{E}\bigg{[}\|x_{k+1}-x^{*}\|^{2}+\|x_{k+1}-\hat{x}_{k}\|^{2}\bigg{]} \leq\bigg{(}1-\frac{\omega\mu}{2}\bigg{)}\mathbb{E}\bigg{[}\|x_{k}-x^{*}\|^{2 }+\|x_{k}-\hat{x}_{k-1}\|^{2}\bigg{]}+12\omega^{2}\sigma_{*}^{2}.\] (36)

The theorem follows by unrolling the above recurrence. In order to compute the iteration complexity of SPEG, we consider any arbitrary \(\varepsilon>0\). Then we choose the step-size \(\omega\) such that \(\frac{24\omega\sigma_{*}^{2}}{\mu}\leq\frac{\varepsilon}{2}\) i.e. \(\omega\leq\frac{\varepsilon\mu}{48\sigma_{*}^{2}}\). Next we will choose the number of iterations \(k\) such that \((1-\frac{\omega\mu}{2})^{k}R_{0}^{2}\leq\frac{\varepsilon}{2}\). It is equivalent to choosing \(k\) such that

\[\log\bigg{(}\frac{2R_{0}^{2}}{\varepsilon}\bigg{)}\leq k\log\bigg{(}\frac{1}{1 -\frac{\omega\mu}{2}}\bigg{)}.\]

Now using the fact \(\log\big{(}\frac{1}{\rho}\big{)}\geq 1-\rho\) for \(0<\rho\leq 1\), we get \(\log\Big{(}\frac{2R_{0}^{2}}{\varepsilon}\Big{)}\leq\frac{k\omega\mu}{2}\), or equivalently \(k\geq\frac{2}{\omega\mu}\log\Big{(}\frac{2R_{0}^{2}}{\varepsilon}\Big{)}\). Therefore, with step-size \(\omega=\min\Big{\{}\frac{\mu}{18\delta},\frac{1}{4L},\frac{\varepsilon\mu}{48 \sigma_{*}^{2}}\Big{\}}\) we get the following lower bound on the number of iterations

\[k\geq\max\bigg{\{}\frac{8L}{\mu},\frac{36\delta}{\mu^{2}},\frac{96\sigma_{*}^ {2}}{\varepsilon\mu^{2}}\bigg{\}}\log\bigg{(}\frac{2R_{0}^{2}}{\varepsilon} \bigg{)}.\]

### Proof of Theorem 4.3

Proof.: For \(\omega\leq\min\big{\{}\frac{1}{4L},\frac{\mu}{18\delta}\big{\}}\), from Theorem 4.1 we obtain

\[R_{k+1}^{2}\leq\bigg{(}1-\frac{\omega\mu}{2}\bigg{)}^{k+1}R_{0}^{2}+\frac{24 \omega\sigma_{*}^{2}}{\mu}.\]

Let the step-size \(\omega_{k}=\frac{2k+1}{(k+1)^{2}}\frac{2}{\mu}\) and \(k^{*}\) be an integer that satisfies \(\omega_{k^{*}}\leq\bar{\omega}\). In particular this holds when \(k^{*}\geq\Big{\lceil}\frac{4}{\mu\bar{\omega}}-1\Big{\rceil}\). Note that \(\omega_{k}\) is decreasing in \(k\) and consequently \(\omega_{k}\leq\bar{\omega}\) for all \(k\geq k^{*}\). Therefore, from (36) we derive

\[R_{k+1}^{2}\leq\bigg{(}1-\omega_{k}\frac{\mu}{2}\bigg{)}R_{k}^{2}+12\omega_{k }^{2}\sigma_{*}^{2}\]

for all \(k\geq k^{*}\). Then we replace \(\omega_{k}\) with \(\frac{2k+1}{(k+1)^{2}}\frac{2}{\mu}\) to obtain

\[R_{k+1}^{2} \leq \bigg{(}1-\frac{2k+1}{(k+1)^{2}}\bigg{)}R_{k}^{2}+48\sigma_{*}^{2 }\frac{(2k+1)^{2}}{\mu^{2}(k+1)^{4}}\] \[= \frac{k^{2}}{(k+1)^{2}}R_{k}^{2}+48\sigma_{*}^{2}\frac{(2k+1)^{2} }{\mu^{2}(k+1)^{4}}.\]

Multiplying both sides by \((k+1)^{2}\) we get

\[(k+1)^{2}R_{k+1}^{2} \leq k^{2}R_{k}^{2}+\frac{48\sigma_{*}^{2}}{\mu^{2}}\bigg{(}\frac{2k+1 }{k+1}\bigg{)}^{2}\] \[\leq k^{2}R_{k}^{2}+\frac{192\sigma_{*}^{2}}{\mu^{2}},\]where in the last line follows from \(\frac{2k+1}{k+1}<2\). Rearranging and summing the last expression for \(t=k^{*},\cdots,k\) we obtain

\[\sum_{t=k^{*}}^{k}(t+1)^{2}R_{t+1}^{2}-t^{2}R_{t}^{2}\leq\frac{192\sigma_{*}^{2}} {\mu^{2}}(k-k^{*}).\]

Using telescopic sum and dividing both sides by \((k+1)^{2}\) we obtain

\[R_{k+1}^{2}\leq\left(\frac{k^{*}}{k+1}\right)^{2}\!R_{k^{*}}^{2}+\frac{192 \sigma_{*}^{2}(k-k^{*})}{\mu^{2}(k+1)^{2}}.\] (37)

Suppose for \(k\leq k^{*}\), we have \(\omega_{k}=\bar{\omega}=\min\left\{\frac{1}{4L},\frac{\mu}{18\delta}\right\}\) i.e. constant step-size. Then from (10), we obtain \(R_{k^{*}}^{2}\leq\left(1-\frac{\mu\bar{\omega}}{2}\right)^{k^{*}}\!R_{0}^{2}+ \frac{24\bar{\omega}\sigma_{*}^{2}}{\mu}\). This bound on \(R_{k}^{2}\), combined with (37) yields

\[R_{k+1}^{2}\leq\left(\frac{k^{*}}{k+1}\right)^{2}\!\left(1-\frac{\mu\bar{ \omega}}{2}\right)^{k^{*}}\!R_{0}^{2}+\left(\frac{k^{*}}{k+1}\right)^{2}\! \frac{24\bar{\omega}\sigma_{*}^{2}}{\mu}+\frac{192\sigma_{*}^{2}(k-k^{*})}{ \mu^{2}(k+1)^{2}}.\]

Now we want to choose \(k^{*}\) which minimizes the expression \(\left(\frac{k^{*}}{k+1}\right)^{2}\!\frac{24\bar{\omega}\sigma_{*}^{2}}{\mu}+ \frac{192\sigma_{*}^{2}(k-k^{*})}{\mu^{2}(k+1)^{2}}\). Note that, it is minimized at \(\frac{4}{\mu\bar{\omega}}\), hence we choose \(k^{*}=\left\lceil\frac{4}{\mu\bar{\omega}}\right\rceil\). Therefore, using this value of \(k^{*}\), we obtain

\[R_{k+1}^{2} \leq \left(\frac{k^{*}}{k+1}\right)^{2}\!\left(1-\frac{2}{k^{*}} \right)^{k^{*}}\!R_{0}^{2}+\frac{24\sigma_{*}^{2}}{\mu^{2}(k+1)^{2}}(8k-4k^{*})\] \[\leq \left(\frac{k^{*}}{k+1}\right)^{2}\!\left(1-\frac{2}{k^{*}} \right)^{k^{*}}\!R_{0}^{2}+\frac{192k\sigma_{*}^{2}}{\mu^{2}(k+1)^{2}}\] \[\leq \left(\frac{k^{*}}{k+1}\right)^{2}\!\frac{R_{0}^{2}}{e^{2}}+\frac {192\sigma_{*}^{2}}{\mu^{2}(k+1)}.\]

The last line follows from \(\left(1-\frac{1}{x}\right)^{x}\leq e^{-1}\) for all \(x\geq 1\). This completes the proof. 

### Proof of Theorem a.4

Proof.: For \(0<\omega_{k}\leq\left\{\frac{1}{4L},\frac{\mu}{18\delta}\right\}\) we obtain the following bound from Theorem a.1:

\[R_{k}^{2}\leq\left(1-\frac{\mu\omega_{k}}{2}\right)\!R_{k-1}^{2}+12\omega_{k} ^{2}\sigma_{*}^{2}.\]

Then using Lemma B.4 with \(a=\frac{\mu}{2},h=\frac{1}{\bar{\omega}}\) and \(c=12\sigma_{*}^{2}\) we complete the proof of this Theorem. 

### Proof of Theorem e.4

**Theorem E.4**.: Let \(F\) be \(L\)-Lipschitz and satisfy Weak Minty condition with parameter \(\rho\leqslant\nicefrac{{1}}{{(2L)}}\). Assume that inequality (8) holds (e.g., it holds whenever Assumption 3.1 holds, see Lemma 3.2). Assume that \(\gamma_{k}=\gamma\), \(\omega_{k}=\omega\) and

\[\max\left\{2\rho,\frac{1}{2L}\right\}<\gamma<\frac{1}{L},\quad 0<\omega<\min \left\{\gamma-2\rho,\frac{1}{4L}-\frac{\gamma}{4}\right\},\quad\delta\leq \frac{(1-L\gamma)L^{3}\omega}{32}.\]Then, for all \(K\geq 2\) the iterates produced by SPEG satisfy

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right] \leq \frac{(1+8\omega\gamma(\delta+L^{2})-L\gamma)\left(1+\frac{48\omega \gamma\delta}{(1-L\gamma)^{2}}\right)^{K-1}\|x_{0}-x^{*}\|^{2}}{\omega\gamma(1 -L(\gamma+4\omega))(K-1)}\] (38) \[+\frac{8\left(8+\frac{(1-L\gamma)^{2}}{K-1}\left(1+\frac{48\omega \gamma\delta}{(1-L\gamma)^{2}}\right)^{K-1}\right)\sigma_{*}^{2}}{(1-L\gamma) ^{2}(1-L(\gamma+4\omega))}.\]

Proof.: The proof closely follows the proof of Lemma C.3 and Theorem C.4 from [23]. The update rule of SPEG implies for \(k\geq 1\)

\[\|x_{k+1}-x^{*}\|^{2} = \|x_{k}-x^{*}\|^{2}-2\omega\langle x_{k}-x^{*},F_{v_{k}}(\hat{x} _{k})\rangle+\omega^{2}\|F_{v_{k}}(\hat{x}_{k})\|^{2}\] \[= \|x_{k}-x^{*}\|^{2}-2\omega\langle\hat{x}_{k}-x^{*},F_{v_{k}}( \hat{x}_{k})\rangle-2\omega\gamma\langle F_{v_{k-1}}(\hat{x}_{k-1}),F_{v_{k}} (\hat{x}_{k})\rangle\] \[\quad+\omega^{2}\|F_{v_{k}}(\hat{x}_{k})\|^{2}\] \[= \|x_{k}-x^{*}\|^{2}-2\omega\langle\hat{x}_{k}-x^{*},F_{v_{k}}( \hat{x}_{k})\rangle-\omega\gamma\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\] \[\quad-\omega(\gamma-\omega)\|F_{v_{k}}(\hat{x}_{k})\|^{2}+\omega \gamma\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\|^{2},\]

where in the last step we apply \(2\langle a,b\rangle=\|a\|^{2}+\|b\|^{2}-\|a-b\|^{2}\), which holds for all \(a,b\in\mathbb{R}^{d}\). Taking the full expectation and using \(\mathbb{E}[\mathbb{E}_{v_{k}}[\cdot]]=\mathbb{E}[\cdot]\) and Weak Minty condition, we derive

\[\mathbb{E}\left[\|x_{k+1}-x^{*}\|^{2}\right] \leq \mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]-2\omega\mathbb{E} \left[\langle\hat{x}_{k}-x^{*},F(\hat{x}_{k})\rangle\right]-\omega\gamma \mathbb{E}\left[\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]\] (39) \[\quad-\omega(\gamma-\omega)\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{ k})\|^{2}\right]+\omega\gamma\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}\right]\] \[\stackrel{{(\ref{eq:def})}}{{\leq}} \mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]-\omega\gamma\mathbb{E} \left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]\] \[\leq \mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]-\omega\gamma\mathbb{ E}\left[\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]\] \[\quad-\omega(\gamma-2\rho-\omega)\mathbb{E}\left[\|F_{v_{k}}( \hat{x}_{k})\|^{2}\right]\] \[\quad+\omega\gamma\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_ {k-1}}(\hat{x}_{k-1})\|^{2}\right]\] \[\leq \mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]-\omega\gamma\mathbb{ E}\left[\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]\] \[\quad+\omega\gamma\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_ {k-1}}(\hat{x}_{k-1})\|^{2}\right]\] \[\quad+\omega\gamma\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_ {k-1}}(\hat{x}_{k-1})\|^{2}\right],\]

where we apply Jensen's inequality \(\|F(\hat{x}_{k})\|^{2}=\|\mathbb{E}_{v_{k}}[F_{v_{k}}(\hat{x}_{k})\|^{2}\leq \mathbb{E}_{v_{k}}[\|F_{v_{k}}(\hat{x}_{k})\|^{2}]\) and \(\gamma>2\rho+\omega\). For \(k=0\) we have \(x_{1}=x_{0}-\omega F_{v_{0}}(\hat{x}_{0})=x_{0}-\omega F_{v_{0}}(x_{0})\) and

\[\mathbb{E}\left[\|x_{1}-x^{*}\|^{2}\right] = \|x_{0}-x^{*}\|^{2}-2\omega\mathbb{E}\left[\langle x_{0}-x^{*},F _{v_{0}}(x_{0})\rangle\right]+\omega^{2}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2 }\right]\] \[= \|x_{0}-x^{*}\|^{2}-2\omega\langle x_{0}-x^{*},F(x_{0})\rangle+ \omega^{2}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right].\]

Applying Weak Minty condition, we get

\[\mathbb{E}\left[\|x_{1}-x^{*}\|^{2}\right] = \|x_{0}-x^{*}\|^{2}+2\omega\rho\|F(x_{0})\|^{2}+\omega^{2} \mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] (40) \[\leq \|x_{0}-x^{*}\|^{2}+\omega(\omega+2\rho)\mathbb{E}\left[\|F_{v_{0 }}(x_{0})\|^{2}\right].\]The next step of our proof is in estimating the last term from (39). Using Young's inequality \(\|a+b\|^{2}\leq(1+\alpha)\|a\|^{2}+(1+\alpha^{-1})\|b\|^{2}\), which holds for any \(a,b\in\mathbb{R}^{d}\), \(\alpha>0\), we get for all \(k\geq 2\)

\[\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1} )\|^{2}\right]\leq \,(1+\alpha)\mathbb{E}\left[\|F(\hat{x}_{k})-F(\hat{x}_{k-1})\|^{2}\right]\] \[+(1+\alpha^{-1})\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F(\hat{x }_{k})\right.\] \[\left.-(F_{v_{k-1}}(\hat{x}_{k-1})-F(\hat{x}_{k-1}))\|^{2}\right]\] \[\leq \,(1+\alpha)L^{2}\mathbb{E}\left[\|\hat{x}_{k}-\hat{x}_{k-1}\|^{ 2}\right]\] \[+2(1+\alpha^{-1})\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F(\hat{ x}_{k})\|^{2}\right.\] \[\left.+\|F_{v_{k-1}}(\hat{x}_{k-1})-F(\hat{x}_{k-1})\|^{2}\right]\] \[\overset{\eqref{eq:1}}{\leq} \,(1+\alpha)L^{2}\mathbb{E}\left[\|\hat{x}_{k}-x_{k}+x_{k}-x_{k-1}+ x_{k-1}-\hat{x}_{k-1}\|^{2}\right]\] \[+2(1+\alpha^{-1})\delta\mathbb{E}\left[\|\hat{x}_{k}-x^{*}\|^{2}+ \|\hat{x}_{k-1}-x^{*}\|^{2}\right]\] \[+8(1+\alpha^{-1})\sigma_{*}^{2}\] \[\leq \,(1+\alpha)L^{2}\mathbb{E}\left[\|(\gamma+\omega)F_{v_{k-1}}( \hat{x}_{k-1})-\gamma F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}+\|x_{ k-1}-x^{*}\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\gamma^{2}\mathbb{E}\left[\|F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}+\|F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+8(1+\alpha^{-1})\sigma_{*}^{2}\] \[= \,(1+\alpha)L^{2}(\gamma+\omega)^{2}\mathbb{E}\left[\|F_{v_{k-1}} (\hat{x}_{k-1})\|^{2}\right]\] \[+(1+\alpha)L^{2}\gamma^{2}\mathbb{E}\left[\|F_{v_{k-2}}(\hat{x}_ {k-2})\|^{2}\right]\] \[-2(1+\alpha)L^{2}\gamma(\gamma+\omega)\mathbb{E}\left[(F_{v_{k-1} }(\hat{x}_{k-1}),F_{v_{k-2}}(\hat{x}_{k-2}))\right]\] \[+4(1+\alpha^{-1})\delta\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}+\|x_{ k-1}-x^{*}\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\gamma^{2}\mathbb{E}\left[\|F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}+\|F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+8(1+\alpha^{-1})\sigma_{*}^{2}\] \[= \,(1+\alpha)L^{2}(\gamma+\omega)^{2}\mathbb{E}\left[\|F_{v_{k-1}} (\hat{x}_{k-1})\|^{2}\right]\] \[+(1+\alpha)L^{2}\gamma^{2}\mathbb{E}\left[\|F_{v_{k-2}}(\hat{x}_ {k-2})\|^{2}\right]\] \[-(1+\alpha)L^{2}\gamma(\gamma+\omega)\mathbb{E}\left[\|F_{v_{k-1} }(\hat{x}_{k-1})\|^{2}+\|F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+(1+\alpha)L^{2}\gamma(\gamma+\omega)\mathbb{E}\left[\|F_{v_{k-1} }(\hat{x}_{k-1})-F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}+\|x_{ k-1}-x^{*}\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\gamma^{2}\mathbb{E}\left[\|F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}+\|F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+8(1+\alpha^{-1})\sigma_{*}^{2}\] \[= \,(1+\alpha)L^{2}\omega(\gamma+\omega)\mathbb{E}\left[\|F_{v_{k-1 }}(\hat{x}_{k-1})\|^{2}\right]\] \[-(1+\alpha)L^{2}\gamma\omega\mathbb{E}\left[\|F_{v_{k-2}}(\hat{x }_{k-2})\|^{2}\right]\] \[+(1+\alpha)L^{2}\gamma(\gamma+\omega)\mathbb{E}\left[\|F_{v_{k-1 }}(\hat{x}_{k-1})-F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}+\|x_{ k-1}-x^{*}\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\gamma^{2}\mathbb{E}\left[\|F_{v_{k-1}}( \hat{x}_{k-1})\|^{2}+\|F_{v_{k-2}}(\hat{x}_{k-2})\|^{2}\right]\] \[+8(1+\alpha^{-1})\sigma_{*}^{2}.\]

Since \(\hat{x}_{0}=x_{0}\) and \(\hat{x}_{1}=x_{1}-\gamma F_{v_{0}}(x_{0})=x_{0}-(\gamma+\omega)F_{v_{0}}(x_{0})\), for \(k=1\) we have

\[\mathbb{E}\left[\|F_{v_{1}}(\hat{x}_{1})-F_{v_{0}}(\hat{x}_{0})\|^{2}\right] = \mathbb{E}\left[\|F_{v_{1}}(\hat{x}_{1})-F_{v_{0}}(x_{0})\|^{2}\right]\] \[\leq \,(1+\alpha)\mathbb{E}\left[\|F(\hat{x}_{1})-F(x_{0})\|^{2}\right]\] \[+(1+\alpha^{-1})\mathbb{E}\left[\|F_{v_{1}}(\hat{x}_{1})-F(\hat{x }_{1})-(F_{v_{0}}(x_{0})-F(x_{0}))\|^{2}\right]\] \[\leq \,(1+\alpha)L^{2}\mathbb{E}\left[\|\hat{x}_{1}-x_{0}\|^{2}\right]\] \[+2(1+\alpha^{-1})\mathbb{E}\left[\Then using (8) we get,

\[\mathbb{E}\left[\|F_{v_{1}}(\hat{x}_{1})-F_{v_{0}}(\hat{x}_{0})\|^{2}\right] \stackrel{{\eqref{eq:1}}}{{\leq}} (1+\alpha)L^{2}(\gamma+\omega)^{2}\mathbb{E}\left[\|F_{v_{0}}(x_{0} )\|^{2}\right]\] \[+2(1+\alpha^{-1})\delta\mathbb{E}\left[\|\hat{x}_{1}-x^{*}\|^{2}+ \|x_{0}-x^{*}\|^{2}\right]+8(1+\alpha)\sigma_{*}^{2}\] \[\leq \left((1+\alpha)L^{2}+4(1+\alpha^{-1})\delta\right)(\gamma+ \omega)^{2}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[+6(1+\alpha^{-1})\delta\|x_{0}-x^{*}\|^{2}+8(1+\alpha)\sigma_{*}^ {2}.\]

Let \(\{w_{k}\}_{k=0}^{K-1}\) be a non-increasing sequence of positive numbers that will be specified later and \(W_{K}=\sum_{k=0}^{K-1}w_{k}\). Summing up the above two inequalities with weights \(\{w_{k}\}_{k=1}^{K-1}\), we derive

\[\sum_{k=1}^{K-1}w_{k}\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v _{k-1}}(\hat{x}_{k-1})\|^{2}\right]\leq (1+\alpha)L^{2}\sum_{k=1}^{K-3}\left(\omega(\gamma+\omega)w_{k+1} \mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right]\right.\] \[\left.-\gamma\omega w_{k+2}\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k })\|^{2}\right]\right)\] \[+(1+\alpha)L^{2}\omega(\gamma+\omega)w_{K-1}\mathbb{E}\left[\|F_{ v_{k-2}}(\hat{x}_{K-2})\|^{2}\right]\] \[-(1+\alpha)L^{2}\gamma\omega w_{2}\mathbb{E}\left[\|F_{v_{0}}(x_ {0})\|^{2}\right]\] \[+(1+\alpha)L^{2}\gamma(\gamma+\omega)\sum_{k=1}^{K-2}w_{k+1} \mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})\right.\] \[\left.-F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]+4(1+\alpha^{-1}) \delta\sum_{k=2}^{K-1}w_{k}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]\] \[+w_{k}\mathbb{E}\left[\|x_{k-1}-x^{*}\|^{2}\right]\] \[+4(1+\alpha^{-1})\delta\gamma^{2}\sum_{k=1}^{K-2}w_{k+1}\mathbb{ E}\left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right.\] \[\left.+\|F_{v_{k-1}}(\hat{x}_{k-1})\|^{2}\right]+8(1+\alpha^{-1})( W_{K}-w_{0}-w_{1})\sigma_{*}^{2}\] \[+\left((1+\alpha)L^{2}+4(1+\alpha^{-1})\delta\right)(\gamma+ \omega)^{2}w_{1}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[+6(1+\alpha^{-1})\delta w_{1}\|x_{0}-x^{*}\|^{2}+8(1+\alpha)w_{1} \sigma_{*}^{2}.\]

Next, we rearrange the terms using \(w_{k}\geq w_{k+1}\) and new notation \(\Delta_{k}=\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})-F_{v_{k-1}}(\hat{x}_{k-1})\| ^{2}\right]\):

\[\left(1-(1+\alpha)L^{2}\gamma(\gamma+\omega)\right)\sum_{k=1}^{K -1}w_{k}\Delta_{k}\leq \sum_{k=1}^{K-2}(1+\alpha)L^{2}\omega(\gamma+\omega)w_{k}\mathbb{E} \left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right]\] \[+8(1+\alpha^{-1})\delta\gamma^{2}w_{k}\mathbb{E}\left[\|F_{v_{k}} (\hat{x}_{k})\|^{2}\right]\] \[+\left((1+\alpha)L^{2}+8(1+\alpha^{-1})\delta\right)(\gamma+ \omega)^{2}w_{0}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[+12(1+\alpha^{-1})\delta\sum_{k=1}^{K-1}w_{k}\mathbb{E}\left[\|x _{k}-x^{*}\|^{2}\right]\] \[+8(1+\alpha^{-1})(W_{K}-w_{0})\sigma_{*}^{2}.\]

To simplify the above inequality we choose \(\alpha=\frac{1}{2L^{2}\gamma(\gamma+\omega)}-\frac{1}{2}\), which is positive due to \(\gamma<\nicefrac{{1}}{{L}}\) and \(\gamma+\omega<\nicefrac{{1}}{{L}}\). In this case, we have

\[(1+\alpha)L^{2}\gamma(\gamma+\omega) = \frac{1}{2}L^{2}\gamma(\gamma+\omega)+\frac{1}{2},\] \[(1+\alpha)L^{2}(\gamma+\omega)^{2} = \frac{1}{2}L^{2}(\gamma+\omega)^{2}+\frac{\gamma+\omega}{2\gamma} \leq\frac{3}{2},\] \[(1+\alpha)L^{2}\omega(\gamma+\omega) = \frac{1}{2}L^{2}\omega(\gamma+\omega)+\frac{\omega}{2\gamma}=\frac{ L\omega}{2}\left(L(\gamma+\omega)+\frac{1}{\gamma L}\right)\leq\frac{3L\omega}{2},\] \[1+\alpha^{-1} = 1+\frac{2L^{2}\gamma(\gamma+\omega)}{1-L^{2}\gamma(\gamma+\omega) }=\frac{1+L^{2}\gamma(\gamma+\omega)}{1-L^{2}\gamma(\gamma+\omega)}\leq\frac{ 2}{1-L^{2}\gamma(\gamma+\omega)},\]where we also use \(\nicefrac{{1}}{{2L}}<\gamma<\nicefrac{{1}}{{L}}\) and \(\gamma+\omega<\nicefrac{{1}}{{L}}\). Using these relations, we can continue our derivation as follows:

\[\frac{1}{2}\left(1-L^{2}\gamma(\gamma+\omega)\right)\sum_{k=1}^{K-1} w_{k}\Delta_{k} \leq \sum_{k=1}^{K-2}\left(\frac{3L\omega}{2}+\frac{16}{1-L^{2}\gamma( \gamma+\omega)}\delta\gamma^{2}\right)w_{k}\mathbb{E}\left[\|F_{v_{k}}(\hat{x} _{k})\|^{2}\right]\] \[\quad+\left(\frac{3}{2}+\frac{16}{1-L^{2}\gamma(\gamma+\omega)} \delta(\gamma+\omega)^{2}\right)w_{0}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[\quad+\frac{24}{1-L^{2}\gamma(\gamma+\omega)}\delta\sum_{k=1}^{K- 1}w_{k}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]\] \[\quad+\frac{16}{1-L^{2}\gamma(\gamma+\omega)}(W_{K}-w_{0})\sigma _{*}^{2}.\]

Dividing both sides by \(\frac{1}{2}\left(1-L^{2}\gamma(\gamma+\omega)\right)\), we derive

\[\sum_{k=1}^{K-1}w_{k}\Delta_{k} \leq \sum_{k=1}^{K-2}\left(\frac{3L\omega}{1-L^{2}\gamma(\gamma+\omega) }+\frac{32}{(1-L^{2}\gamma(\gamma+\omega))^{2}}\delta\gamma^{2}\right)w_{k} \mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right]\] \[\quad+\left(\frac{3}{1-L^{2}\gamma(\gamma+\omega)}+\frac{32}{(1-L^ {2}\gamma(\gamma+\omega))^{2}}\delta(\gamma+\omega)^{2}\right)w_{0}\mathbb{E} \left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[\quad+\frac{48}{(1-L^{2}\gamma(\gamma+\omega))^{2}}\delta\sum_{k=1 }^{K-1}w_{k}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]\] \[\quad+\frac{32}{(1-L^{2}\gamma(\gamma+\omega))^{2}}(W_{K}-w_{0}) \sigma_{*}^{2}\] \[= \sum_{k=1}^{K-2}C_{1}w_{k}\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k}) \|^{2}\right]+C_{2}w_{0}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[\quad+3C_{3}\delta\sum_{k=1}^{K-1}w_{k}\mathbb{E}\left[\|x_{k}-x^ {*}\|^{2}\right]+2C_{3}W_{K}\sigma_{*}^{2},\] (41)

where \(C_{1}=\frac{3L\omega}{1-L^{2}\gamma(\gamma+\omega)}+\frac{32}{(1-L^{2}\gamma( \gamma+\omega))^{2}}\delta\gamma^{2}\), \(C_{2}=\frac{3}{1-L^{2}\gamma(\gamma+\omega)}+\frac{32}{(1-L^{2}\gamma(\gamma+ \omega))^{2}}\delta(\gamma+\omega)^{2}\), and \(C_{3}=\frac{16}{(1-L^{2}\gamma(\gamma+\omega))^{2}}\). Summing up inequalities (39) for \(k=1,\ldots,K-1\) with weights \(w_{1},\ldots,w_{K-1}\) and (40) with weight \(w_{0}\), we get

\[\sum_{k=0}^{K-1}w_{k}\mathbb{E}\left[\|x_{k+1}-x^{*}\|^{2}\right] \leq \sum_{k=0}^{K-1}w_{k}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]- \omega\gamma\sum_{k=1}^{K-1}w_{k}\mathbb{E}\left[\|F_{v_{k-1}}(\hat{x}_{k-1}) \|^{2}\right]\] \[\quad+\omega\gamma\sum_{k=1}^{K-1}w_{k}\Delta_{k}+\omega(\omega+2 \rho)w_{0}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right].\]

Since \(w_{k}\geq w_{k+1}\), we can continue the derivation as follows:

\[\sum_{k=0}^{K-1}w_{k}\mathbb{E}\left[\|x_{k+1}-x^{*}\|^{2}\right] \leq \sum_{k=0}^{K-1}w_{k}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]- \omega\gamma\sum_{k=0}^{K-2}w_{k}\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right]\] \[\quad+\omega\gamma\sum_{k=1}^{K-1}w_{k}\Delta_{k}+\omega(\omega+2 \rho)w_{0}\mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\] \[\stackrel{{\eqref{eq:200}}}{{\leq}} \sum_{k=0}^{K-1}(1+3C_{3}\omega\gamma\delta)w_{k}\mathbb{E}\left[\|x _{k}-x^{*}\|^{2}\right]\] \[\quad-\omega\gamma(1-C_{1})\sum_{k=0}^{K-2}w_{k}\mathbb{E}\left[\| F_{v_{k}}(\hat{x}_{k})\|^{2}\right]\] \[\quad+2\omega\gamma C_{2}w_{0}\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_ {0})\|^{2}\right]+2\omega\gamma C_{3}W_{K}\sigma_{*}^{2}.\]Now we need to specify the weights \(w_{-1},w_{0},w_{1},\ldots,w_{K-1}\). Let \(w_{K-2}=1\) and \(w_{k-1}=(1+3C_{3}\omega\gamma\delta)w_{k}\). Then, rearranging the terms, dividing both sides by \(\omega\gamma(1-C_{1})W_{K-1}\), we get

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right] \leq \min_{0\leq k\leq K-1}\mathbb{E}\left[\|F_{v_{k}}(\hat{x}_{k})\|^ {2}\right]\] \[\leq \sum_{k=0}^{K-2}\frac{w_{k}}{W_{K-1}}\mathbb{E}\left[\|F_{v_{k}}( \hat{x}_{k})\|^{2}\right]\] \[\leq \frac{1}{\omega\gamma(1-C_{1})W_{K-1}}\sum_{k=0}^{K-1}\left(w_{k -1}\mathbb{E}\left[\|x_{k}-x^{*}\|^{2}\right]\right.\] \[\left.-w_{k}\mathbb{E}\left[\|x_{k+1}-x^{*}\|^{2}\right]\right)+ \frac{2C_{2}w_{0}\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_{0})\|^{2}\right]}{(1-C _{1})W_{K-1}}\] \[+\frac{2C_{3}W_{K}\sigma_{*}^{2}}{(1-C_{1})W_{K-1}}\] \[\leq \frac{w_{-1}\|x_{0}-x^{*}\|^{2}}{\omega\gamma(1-C_{1})W_{K-1}}+ \frac{2C_{2}w_{0}\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_{0})\|^{2}\right]}{(1-C_ {1})W_{K-1}}+\frac{2C_{3}W_{K}\sigma_{*}^{2}}{(1-C_{1})W_{K-1}}.\]

It remains to simplify the right-hand side of the above inequality. First, we notice that \(W_{K-1}=\sum_{k=0}^{K-2}w_{k}\geq(K-1)w_{K-2}=K-1\) since \(w_{k}\geq w_{k+1}\). Moreover, \(w_{-1}=(1+3C_{3}\omega\gamma\delta)^{K-1}\). Next,

\[C_{1} = \frac{3L\omega}{1-L^{2}\gamma(\gamma+\omega)}+\frac{32}{(1-L^{2 }\gamma(\gamma+\omega))^{2}}\delta\gamma^{2}\] \[\leq \frac{3L\omega}{1-L\gamma}+\frac{32}{(1-L\gamma)^{2}}\cdot\frac{( 1-L\gamma)L^{3}\omega}{32}\cdot\gamma^{2}\leq\frac{4L\omega}{1-L\gamma},\] \[C_{2} = \frac{3}{1-L^{2}\gamma(\gamma+\omega)}+\frac{32}{(1-L^{2}\gamma( \gamma+\omega))^{2}}\delta(\gamma+\omega)^{2}\] \[\leq \frac{3}{1-L\gamma}+\frac{32}{(1-L\gamma)^{2}}\cdot\frac{(1-L \gamma)L^{3}\omega}{32}\cdot(\gamma+\omega)^{2}\leq\frac{4}{1-L\gamma},\] \[C_{3} = \frac{16}{(1-L^{2}\gamma(\gamma+\omega))^{2}}\leq\frac{16}{(1-L \gamma)^{2}},\]

where we use \(\delta\leq\nicefrac{{(1-L\gamma)L^{3}\omega/16}}{{16}}\) and \(\gamma+\omega<\nicefrac{{1}}{{L}}\). Using these inequalities, we simplify the bound as follows:

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right] \leq \frac{(1-L\gamma)(1+3C_{3}\omega\gamma\delta)^{K-1}\|x_{0}-x^{*} \|^{2}}{\omega\gamma(1-L(\gamma+4\omega))(K-1)}\] (42) \[+\frac{8(1+3C_{3}\omega\gamma\delta)^{K-2}\mathbb{E}\left[\|F_{v_ {0}}(\hat{x}_{0})\|^{2}\right]}{(1-L(\gamma+4\omega))(K-1)}\] \[+\frac{32\sigma_{*}^{2}}{(1-L\gamma)(1-L(\gamma+4\omega))}\] \[\leq \frac{(1-L\gamma)\left(1+\frac{48\omega\gamma\delta}{(1-L\gamma) ^{2}}\right)^{K-1}\|x_{0}-x^{*}\|^{2}}{\omega\gamma(1-L(\gamma+4\omega))(K-1)}\] \[+\frac{8\left(1+\frac{48\omega\gamma\delta}{(1-L\gamma)^{2}} \right)^{K-2}\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_{0})\|^{2}\right]}{(1-L( \gamma+4\omega))(K-1)}\] \[+\frac{32\sigma_{*}^{2}}{(1-L\gamma)(1-L(\gamma+4\omega))}\]

where we use \(W_{K}=W_{K-1}+w_{K-1}\leq W_{K-1}+w_{K-2}\leq 2W_{K-1}\). Finally, we use (8) to upper-bound \(\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_{0})\|^{2}\right]\):

\[\mathbb{E}\left[\|F_{v_{0}}(\hat{x}_{0})\|^{2}\right] = \mathbb{E}\left[\|F_{v_{0}}(x_{0})\|^{2}\right]\overset{(8)}{ \leq}\delta\|x_{0}-x^{*}\|^{2}+\|F(x_{0})\|^{2}+2\sigma_{*}^{2}\] \[\leq (\delta+L^{2})\|x_{0}-x^{*}\|^{2}+2\sigma_{*}^{2}.\]Plugging this inequality in (42), we derive

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right] \leq \frac{(1+8\omega\gamma(\delta+L^{2})-L\gamma)\left(1+\frac{48\omega \gamma\delta}{(1-L\gamma)^{2}}\right)^{K-1}\|x_{0}-x^{*}\|^{2}}{\omega\gamma(1 -L(\gamma+4\omega))(K-1)}\] \[\quad+\frac{4\left(8+\frac{1-L\gamma}{K-1}\left(1+\frac{48\omega \gamma\delta}{(1-L\gamma)^{2}}\right)^{K-1}\right)\sigma_{*}^{2}}{(1-L\gamma)( 1-L(\gamma+4\omega))},\]

which concludes the proof. 

### Proof of Theorem 4.5

**Theorem E.5**.: Let \(F\) be \(L\)-Lipschitz and satisfy Weak Minty condition with parameter \(\rho<\nicefrac{{1}}{{(2L)}}\). Assume that inequality (8) holds (e.g., it holds whenever Assumption 3.1 holds, see Lemma 3.2). Assume that \(\gamma_{k}=\gamma\), \(\omega_{k}=\omega\) and

\[\max\left\{2\rho,\frac{1}{2L}\right\}<\gamma<\frac{1}{L},\quad 0<\omega<\min \left\{\gamma-2\rho,\frac{1}{4L}-\frac{\gamma}{4}\right\}.\]

Then, for all \(K\geq 2\) the iterates produced by mini-batched \(\mathsf{SPEG}\) with batch-size

\[\tau\geq\max\left\{1,\frac{32\delta}{(1-L\gamma)L^{3}\omega},\frac{48\omega \gamma\delta(K-1)}{(1-L\gamma)^{2}},\frac{2\omega\gamma\sigma_{*}^{2}(K-1)}{( 1-L\gamma)\|x_{0}-x^{*}\|^{2}}\right\}\] (43)

satisfy

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right]\leq\frac {48\|x_{0}-x^{*}\|^{2}}{\omega\gamma(1-L(\gamma+4\omega))(K-1)}.\] (44)

Proof.: Mini-batched \(\mathsf{SPEG}\) uses estimator

\[F_{v_{k}}(\hat{x}_{k})=\frac{1}{\tau}\sum_{i=1}^{\tau}F_{v_{k,i}}(\hat{x}_{k}),\]

where \(F_{v_{k,1}}(\hat{x}_{k}),\ldots,F_{v_{k,\tau}}(\hat{x}_{k})\) are independent samples satisfying (8) with parameters \(\delta\) and \(\sigma_{*}^{2}\). Using variance decomposition and independence of \(F_{v_{k,1}}(\hat{x}_{k}),\ldots,F_{v_{k,\tau}}(\hat{x}_{k})\), we get

\[\mathbb{E}_{v_{k}}\left[\|F_{v_{k}}(\hat{x}_{k})\|^{2}\right] = \mathbb{E}_{v_{k}}\left[\|F_{v_{k}}(\hat{x}_{k})-F(\hat{x}_{k}) \|^{2}\right]+\|F(\hat{x}_{k})\|^{2}\] \[= \mathbb{E}_{v_{k}}\left[\left\|\frac{1}{\tau}\sum_{i=1}^{b}(F_{v _{k,i}}(\hat{x}_{k})-F(\hat{x}_{k}))\right\|^{2}\right]+\|F(\hat{x}_{k})\|^{2}\] \[= \frac{1}{\tau^{2}}\sum_{i=1}^{\tau}\mathbb{E}_{v_{k}}\left[\|F_{ v_{k,i}}(\hat{x}_{k})-F(\hat{x}_{k})\|^{2}\right]+\|F(\hat{x}_{k})\|^{2}\] \[\stackrel{{(\ref{eq:1})}}{{\leq}} \frac{\delta}{\tau}\|\hat{x}_{k}-x^{*}\|^{2}+\|F(\hat{x}_{k})\|^{2}+ \frac{2\sigma_{*}^{2}}{\tau}.\]

That is, mini-batched estimator \(F_{v_{k}}(\hat{x}_{k})\) satisfies (8) with parameters \(\nicefrac{{\delta}}{{\tau}}\) and \(\nicefrac{{\sigma_{*}^{2}}}{{\tau}}\). Therefore, Theorem E.4 implies

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\|F(\hat{x}_{k})\|^{2}\right] \leq \frac{(1+4\omega\gamma\left(\frac{\delta}{\tau}+L^{2}\right)-L \gamma)\left(1+\frac{48\omega\gamma\delta}{(1-L\gamma)^{2}\tau}\right)^{K-1} \|x_{0}-x^{*}\|^{2}}{\omega\gamma(1-L(\gamma+4\omega))(K-1)}\] (45) \[\quad+\frac{8\left(8+\frac{1-L\gamma}{K-1}\left(1+\frac{48\omega \gamma\delta}{(1-L\gamma)^{2}\tau}\right)^{K-1}\right)\sigma_{*}^{2}}{(1-L \gamma)(1-L(\gamma+4\omega))\tau}.\]

[MISSING_PAGE_EMPTY:34]

Further Results on Arbitrary Sampling

### Proof of Proposition 5.1

Expanding the left hand side of Expected Residual (ER) condition we have

\[\mathbb{E}\|(F_{v}(x)-F_{v}(x^{*}))-(F(x)-F(x^{*}))\|^{2} \stackrel{{(\ref{eq:E})}}{{=}} \mathbb{E}\|(F_{v}(x)-F_{v}(x^{*}))\|^{2}-\|F(x)-F(x^{*})\|^{2}\] (47) \[\leq \mathbb{E}\|F_{v}(x)-F_{v}(x^{*})\|^{2}.\]

For any \(x\) and \(y\) with \(v_{i}=\frac{1}{p_{i}}\) we obtain

\[\|F_{v}(x)-F_{v}(y)\|^{2} = \frac{1}{n^{2}}\bigg{\|}\sum_{i\in S}\frac{1}{p_{i}}(F_{i}(x)-F_{ i}(y))\bigg{\|}^{2}\] \[= \sum_{i,j\in S}\bigg{\langle}\frac{1}{np_{i}}(F_{i}(x)-F_{i}(y)),\frac{1}{np_{j}}(F_{j}(x)-F_{j}(y))\bigg{\rangle}.\]

Then taking expectation on both sides we get

\[\mathbb{E}\|F_{v}(x)-F_{v}(y)\|^{2} = \sum_{C}p_{C}\sum_{i,j\in C}\bigg{\langle}\frac{1}{np_{i}}(F_{i}( x)-F_{i}(y)),\frac{1}{np_{j}}(F_{j}(x)-F_{j}(y))\bigg{\rangle}\] \[= \sum_{i,j=1}^{n}\sum_{Ci,j\in C}p_{C}\bigg{\langle}\frac{1}{np_{i }}(F_{i}(x)-F_{i}(y)),\frac{1}{np_{j}}(F_{j}(x)-F_{j}(y))\bigg{\rangle}\] \[= \sum_{i,j=1}^{n}\frac{P_{ij}}{p_{i}p_{j}}\bigg{\langle}\frac{1}{ n}(F_{i}(x)-F_{i}(y)),\frac{1}{n}(F_{j}(x)-F_{j}(y))\bigg{\rangle}.\]

Now we consider the case, where the ratio \(\frac{P_{ij}}{p_{i}p_{j}}=c_{2}\) i.e. constant for \(i\neq j\) and \(P_{ii}=p_{i}\). Then from the above computations we derive

\[\mathbb{E}\|F_{v}(x)-F_{v}(y)\|^{2} = \sum_{i\neq j}^{n}c_{2}\bigg{\langle}\frac{1}{n}(F_{i}(x)-F_{i}(y )),\frac{1}{n}(F_{i}(x)-F_{i}(y))\bigg{\rangle}\] \[\quad+\sum_{i=1}^{n}\frac{1}{n^{2}p_{i}}\|F_{i}(x)-F_{i}(y)\|^{2}\] \[= \sum_{i,j=1}^{n}c_{2}\bigg{\langle}\frac{1}{n}(F_{i}(x)-F_{i}(y) ),\frac{1}{n}(F_{i}(x)-F_{i}(y))\bigg{\rangle}\] \[\quad+\sum_{i=1}^{n}\frac{1-p_{i}c_{2}}{n^{2}p_{i}}\|F_{i}(x)-F_{ i}(y)\|^{2}\] \[\stackrel{{(\ref{eq:E})}}{{\leq}} c_{2}\|F(x)-F(y)\|^{2}+\sum_{i=1}^{n}\frac{1-p_{i}c_{2}}{n^{2}p_{i}}L_{i}^{2} \|x-y\|^{2}\] \[\stackrel{{(\ref{eq:E})}}{{\leq}} \bigg{(}c_{2}L^{2}+\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{1-p_{i}c_{2 }}{p_{i}}L_{i}^{2}\bigg{)}\|x-y\|^{2}.\]

Thus replacing \(y=x^{*}\) and combining with (47) we get the following bound on the Expected Residual:

\[\mathbb{E}\|(F_{v}(x)-F_{v}(x^{*}))-(F(x)-F(x^{*}))\|^{2}\leq\bigg{(}c_{2}L^{2 }+\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{1-p_{i}c_{2}}{p_{i}}L_{i}^{2}\bigg{)}\|x- x^{*}\|^{2}.\] (48)

For single-element sampling \(c_{2}=0\) (as probability of two points appearing in same sample is zero for single element sampling i.e. \(P_{ij}=0\)). Then we obtain

\[\delta\leq\frac{2}{n^{2}}\sum_{i=1}^{n}\frac{L_{i}^{2}}{p_{i}}\]from (48). This completes the derivation of \(\delta\) for single element sampling. To compute \(\sigma_{*}^{2}\) for single element sampling, we replace

\[P_{ij}=\begin{cases}p_{i}&\text{if }i=j\\ 0&\text{otherwise}\end{cases}\]

in (30) to get

\[\sigma_{*}^{2}=\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{1}{p_{i}}\|F_{i}(x^{*})\|^{2}.\]Numerical Experiments

In Appendix G.1, we add more details on the experiments discussed in the main paper. Furthermore, in Appendix G.2, we run more experiments to evaluate the performance of \(\mathsf{SPEG}\) on quasi-strongly monotone and weak MVI problems.

### More Details on the Numerical Experiments of Section 6

On Constant vs Switching Stepsize Rule.We run the experiments on two synthetic datasets. In Fig. 1 of the main paper, we take \(\mu_{A}=\mu_{C}=0.6\). Here we include one more plot with a similar flavor but in a different setting. For Fig. 5, we generate the data such that eigenvalues of \(A_{1},B_{1},C_{1}\) are generated uniformly from the interval \([0.1,10]\). In the new plot, similar to the main paper, we can see the benefit of switching the step-size rule of Theorem 4.3.

On Weak Minty VIPs.In this experiment, we generate \(\xi_{i},\zeta_{i}\) such that \(\frac{1}{n}\sum_{i=1}^{n}\xi_{i}=\sqrt{63}\) and \(\frac{1}{n}\sum_{i=1}^{n}\zeta_{i}=-1\). This choice of \(\xi_{i},\zeta_{i}\) ensures that \(L=8\) and \(\rho=\nicefrac{{1}}{{32}}\) for the min-max problem we considered in Section 6.2. In Fig. 6, we again implement the \(\mathsf{SPEG}\) on (17) with batchsize = \(0.15\times n\) (different batchsize compare to the plot of the main paper).

### Additional Experiments

In this subsection, we include more experiments to evaluate the performance of \(\mathsf{SPEG}\) on quasi-strongly monotone and weak MVI problems. First, we run the experiment comparing constant and switching step-size rules on a different setup than the one we included in the main paper to analyze the performance of \(\mathsf{SPEG}\) under different condition numbers. Then, we implement \(\mathsf{SPEG}\) on the weak MVI of (17). To evaluate the performance in this experiment, we plot \(\|F(\hat{x}_{k})\|^{2}/\|F(x_{0})\|^{2}\) on the \(y\)-axis.

Figure 5: Comparison of the constant step-size rule (9) with the switching step-sizes (11) on the strongly monotone quadratic game.

Figure 6: Trajectory of \(\mathsf{SPEG}\) for solving weak MVI using a batchsize = \(0.15\times n\).

#### g.2.1 Strongly Monotone Quadratic Game:

In this experiment, we compare the proposed constant step-size (9) and the switching step-size rule (11). We implement our algorithm on operator \(F:\mathbb{R}^{4}\rightarrow\mathbb{R}^{4}\) given by

\[F(x):=\frac{1}{3}\left(M_{1}(x-x_{1}^{*})+M_{2}(x-x_{2}^{*})+M_{3}(x-x_{3}^{*}) \right),\]

where \(M_{1}\), \(M_{2}\) and \(M_{3}\) are the diagonal matrices,

\[M_{1}=\begin{pmatrix}\Delta&&\\ &1&&\\ &&1&\\ &&&1\end{pmatrix},\quad M_{2}=\begin{pmatrix}1&&&\\ &\Delta&&\\ &&1&\\ &&&1\end{pmatrix},\quad M_{3}=\begin{pmatrix}1&&&\\ &1&&\\ &&\Delta&\\ &&&1\end{pmatrix}\]

and

\[x_{1}^{*}=\begin{pmatrix}\Delta\\ 0\\ 0\\ \Delta\end{pmatrix},\quad x_{2}^{*}=\begin{pmatrix}0\\ \Delta\\ 0\\ 0\end{pmatrix},\quad x_{3}^{*}=\begin{pmatrix}0\\ 0\\ \Delta\\ 0\end{pmatrix}.\]

This choice of \(M_{i}\) and \(x_{i}^{*}\) ensures that the Lipschitz constant of operator \(F\) is \(\frac{\Delta+2}{3}\) while quasi-strong monotonicity parameter (3) is \(\mu=1\). Hence the condition number of \(F\) is given by \(\frac{\Delta+2}{3}\). This allows us to vary the condition number of operator \(F\) by changing the value of \(\Delta\). For Fig. 6(a) we take \(\Delta=3\) (condition number = \(1.67\)) while for Fig. 6(b) we choose \(\Delta=10\) (condition number = \(10.67\)). The vertical dotted line in plots of Fig. 7 marks the transition point from constant to switching step-size rule as predicted by our theoretical result in Theorem 4.3.

#### g.2.2 Weak Minty VIPs Continued

In this experiment, we reevaluate the performance of SPEG on weak MVI example of (17). That is, we generate the data in exactly the same way as the ones in section 6.2 with \(n=100\). In Fig. 7(a) and 7(b), we implement SPEG with batchsize \(10\) and \(15\), respectively (we note that in this setting the full-gradient evaluation requires a batchsize of \(100\)). For these plots, we use the relative operator norm on the \(y\)-axis, i.e. \(\|F(x_{i})\|^{2}/\|F(x_{0})\|^{2}\), where \(x_{0}\) denotes the starting point of SPEG. As expected, the plots illustrate that SPEG performs better as we increase the batchsize. From Fig. 8 it is clear that with batchsize 15 SPEG reaches an accuracy close to \(10^{-10}\) while when we use a batchsize of \(10\) for the same number of iterations we are only able to converge to an accuracy of \(10^{-4}\).

Figure 7: Illustration of switching rule (11) in Theorem 4.3. The dotted line marks the transition from phase 1 (where we use constant step-size) to phase 2 (where we use decreasing step-size).

Figure 8: Performance of SPEG for solving weak MVI with different batchsizes. In plot (a) we use a batchsize of \(10\) while in plot (b) we use \(15\).