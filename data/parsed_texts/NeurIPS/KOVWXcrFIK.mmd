# Fast Attention Requires Bounded Entries

Josh Alman

josh@cs.columbia.edu. Columbia University.

Zhao Song

zsong@adobe.com. Adobe Research.

###### Abstract

In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices \(Q,K,V\in[-B,B]^{n\times d}\), and the goal is to construct the matrix \(\mathrm{Att}(Q,K,V):=\mathrm{diag}(A\mathbf{1}_{n})^{-1}AV\in\mathbb{R}^{n \times d}\), where \(A=\exp(QK^{\top}/d)\) is the 'attention matrix', and \(\exp\) is applied entry-wise. Straightforward methods for this problem explicitly compute the \(n\times n\) attention matrix \(A\), and hence require time \(\Omega(n^{2})\) even when \(d=n^{o(1)}\) is small.

In this paper, we investigate whether faster algorithms are possible by _implicitly_ making use of the matrix \(A\). We present two results, showing that there is a sharp transition at \(B=\Theta(\sqrt{\log n})\).

* If \(d=O(\log n)\) and \(B=o(\sqrt{\log n})\), there is an \(n^{1+o(1)}\) time algorithm to approximate \(\mathrm{Att}(Q,K,V)\) up to \(1/\mathrm{poly}(n)\) additive error.
* If \(d=O(\log n)\) and \(B=\Theta(\sqrt{\log n})\), assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory, it is impossible to approximate \(\mathrm{Att}(Q,K,V)\) up to \(1/\mathrm{poly}(n)\) additive error in truly subquadratic time \(n^{2-\Omega(1)}\).

This gives a theoretical explanation for the phenomenon observed in practice that attention computation is much more efficient when the input matrices have smaller entries.

## 1 Introduction

Large language models (LLMs) such as Transformer [25], BERT [1], GPT-3 [3], PaLM [1], and OPT [23] can process natural language more effectively than smaller models or traditional algorithms. This means that they can understand and generate more complex and nuanced language, which can be useful for a variety of tasks such as language translation, question answering, and sentiment analysis. LLMs can also be adapted to multiple purposes without needing to be retained from scratch. Their power is particularly exemplified by the recent success of ChatGPT, a chat software by OpenAI built on top of GPT-3 [14].

The key technical backbone of LLMs is the _attention matrix_[25, 26, 27, 28, 29, 30]. An attention matrix is a square matrix whose rows and columns correspond to words or "tokens", and whose entries correspond to the correlations between these tokens in natural text. The attention matrix is then used to calculate the importance of each input token in a sequence when producing an output. In an attention mechanism, each input token is given a weight or score, which reflects its importance or relevance to the current output being generated. These scores are calculated based on a comparison between the current output state and the input states, using a similarity function.

More formally, the attention matrix is defined as follows. Let \(Q\in\mathbb{R}^{n\times d}\) be the matrix of query tokens, and \(K\in\mathbb{R}^{n\times d}\) be the matrix of key tokens. (We focus here on the case when \(d=n^{o(1)}\), so \(d\ll n\).) The attention matrix is an \(n\times n\) matrix \(A\) where the rows and columns correspond to the input tokens in the sequence. Each entry in the matrix represents the attention weight or score between a particular input token (query token \(Q\)) and a particular output token (key token \(K\)). The diagonal entries of the matrix represent self-attention scores, which measure the importance of each token with respect to itself.

The major bottleneck to speeding up LLM operations (in the case of modeling long sequences with large \(n\)) is the time to perform attention matrix computations [17, 18, 19, 20, 21]. These computations ask us to multiply the attention matrix \(A\) with another value token matrix \(V\in\mathbb{R}^{n\times d}\).

We formally define Attention computation as follows. Throughout this paper, we write \(\exp\) to denote the _entry-wise_ exponential for matrices.

**Definition 1.1** (Exact Attention Computation \(\mathsf{EAttC}(n,d)\)).: _Given three matrices \(Q,K,V\in\mathbb{R}^{n\times d}\), output the \(n\times d\) matrix \(\mathrm{Att}(Q,K,V)\) defined by_

\[\mathrm{Att}(Q,K,V):=D^{-1}AV\]

_where \(A\in\mathbb{R}^{n\times n}\) and diagonal matrix \(D\in\mathbb{R}^{n\times n}\) are defined as_

\[A:=\exp(QK^{\top}/d),\ \ \text{and}\ \ \ D:=\mathrm{diag}(A\mathbf{1}_{n}).\]

The straightforward algorithm for this problem computes the matrix \(A\) and then performs the multiplications \(D^{-1}AV\), in time \(n^{2+o(1)}\). Since \(A\) is an \(n\times n\) matrix with \(n^{2}\) entries, it is impossible to improve on this much while explicitly computing the matrix \(A\). However, the input to the problem is not \(A\), but rather the three matrices \(Q,K,V\) which each have only \(n^{1+o(1)}\) entries. An algorithm which only _implicitly_ makes use of \(A\), without explicitly computing all its entries, could hope to run in almost linear time!

In this paper, we investigate the possibility of accelerating attention computations in this way. The two main questions we address are:

* **Q1.** When can we perform attention computations in almost linear time \(n^{1+o(1)}\)?
* **Q2.** When can we prove that subquadratic-time algorithms for attention computations are _impossible_?

In most LLMs, it suffices to _approximately_ perform attention computations throughout the inference process as long as there are reasonable precision guarantees [1, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. We therefore focus here on approximate attention computation, which can potentially be performed even faster than exact computation. Mathematically, we define the _approximate_ version of \(\mathsf{EAttC}\) as follows.

**Definition 1.2** (Approximate Attention Computation \(\mathsf{AAttC}(n,d,B,\epsilon_{a})\)).: _Let \(\epsilon_{a}>0\) and \(B>0\) be parameters. Given three matrices \(Q,K,V\in\mathbb{R}^{n\times d}\), with the guarantees that \(\|Q\|_{\infty}\leq B\), \(\|K\|_{\infty}\leq B\), and \(\|V\|_{\infty}\leq B\), output a matrix \(T\in\mathbb{R}^{n\times d}\) which is approximately equal to \(D^{-1}AV\), meaning,_

\[\|T-D^{-1}AV\|_{\infty}\leq\epsilon_{a}.\]

_Here, for a matrix \(M\in\mathbb{R}^{n\times n}\), we write \(\|M\|_{\infty}:=\max_{i,j}|M_{i,j}|\)._

Again, the straightforward algorithm for this problem runs in time \(O(n^{2}d)\leq n^{2+o(1)}\), but the input size is only \(O(nd)\leq n^{1+o(1)}\). Our goal is to investigate when faster algorithms are possible in terms of the parameters \(d,B\), and \(\epsilon_{a}\).

### Our Results

We focus on the natural setting where \(d=O(\log n)\) (the setting where we model long sequences) and \(\epsilon_{a}=1/\operatorname{poly}(n)\) (low enough error so that attention computations over an entire network can be combined). Our main results show that whether or not there is a fast algorithm for \(\mathsf{AAttC}\) critically depends on \(B\), the magnitudes of the entries in the input matrices.

We first show a lower bound, that when \(B\geq\Omega(\sqrt{\log n})\), it is impossible to design a truly subquadratic-time algorithm. Our lower bound makes use of the Strong Exponential Time Hypothesis (SETH) [11], a popular conjecture [12] from the area of fine-grained complexity regarding the time required to solve \(k\)-SAT. (See Section 4 below where we discuss SETH in more detail.)

**Theorem 1.3** (Lower bound, informal version of Theorem 4.6).: _Assuming_ SETH_, for every \(q>0\), there are constants \(C,C_{a},C_{b}>0\) such that: there is no \(O(n^{2-q})\) time algorithm for the problem \(\mathsf{AAttC}(n,d=C\log n,B=C_{b}\sqrt{\log n},\epsilon_{a}=n^{-C_{a}})\)._

Our second complementary result is a new algorithm, showing that when \(B<o(\sqrt{\log n})\), the problem can be solved very efficiently, in almost linear time.

**Theorem 1.4** (Upper bound, informal version of Theorem 3.8).: _There is an algorithm (Algorithm 1) that solves \(\mathsf{AAttC}(n,d=O(\log n),B=o(\sqrt{\log n}),\epsilon_{a}=1/\operatorname{ poly}(n))\) in time \(n^{1+o(1)}\)._

Our Theorems 1.3 and 1.4 show that the attention computation problem \(\mathsf{AAttC}\) exhibits a very tight transition at \(B=\Theta(\sqrt{\log n})\) from almost linear time to trivial quadratic time. When \(B<o(\sqrt{\log n})\) is smaller, the problem can be solved in almost linear time \(n^{1+o(1)}\) in the input size, using our algorithm for Theorem 1.4. When \(B\geq\Omega(\sqrt{\log n})\) is greater, our algorithm from Theorem 1.4 no longer applies, and furthermore our lower bound from Theorem 1.3 shows that it is _impossible_ to solve the problem in truly subquadratic time, no matter what algorithmic techniques one uses (assuming SETH).

It has been observed in LLM implementations in practice that computations are much faster when one assumes that the matrix entries are bounded or can be well-approximated using a small number of bits (see, e.g., [14, Section 2] and [17, Section 3.2.1]). Our work can be viewed as giving a theoretical explanation for this phenomenon, and helping to explain why techniques like quantization [14] and low-degree polynomial approximation [17] have been so effective in practice.

**Related Work.**

A recent work by Zandieh, Han, Daliri, and Karbasi [10] was the first to give an algorithm with provable guarantees for attention approximation. Their algorithm makes use of locality sensitive hashing (LSH) techniques [18] which, as we will discuss next, is quite different from our algorithm for Theorem 1.4 which uses the polynomial method [18, 1].

In the case when \(d=o(\log^{2}n)\), they achieve a running time of roughly \(O(n^{1.17}\cdot d/\epsilon_{r}^{2})\), where \(\epsilon_{r}\) is a _relative_ error parameter (which is similar, though not exactly the same, as our \(\epsilon_{a}\) from Definition 1.2). In particular, their algorithm applies for larger \(d\) than ours (we require \(d=O(\log n)\)), but we achieve almost linear time \(n^{1+o(1)}\) (whereas their running time is bounded below by \(\Omega(n^{1.17})\)), and our algorithm can handle any polynomial error \(\epsilon_{a}=1/\operatorname{poly}(n)\) (whereas they require \(\epsilon_{r}\geq 1/n^{o(1)}\) to not increase the running time by a polynomial factor).

It is natural to wonder whether further improvements are possible by combining our techniques with those of [10]. However, our lower bound of Theorem 1.3 shows that our algorithm of Theorem 1.4 is already essentially tight and cannot be substantially improved.

Another recent work by Keles, Wijewardena, and Hedge [19] was the first to prove a lower bound for attention computation assuming SETH. They prove, among other results, that \(\mathsf{AAttC}\) cannot be solved in truly subquadratic time in the case when \(d=\omega(\log n)\). Our Theorem 1.3 improves their result to also hold for \(d=\Theta(\log n)\), and to show how the complexity changes with the magnitude of entries \(B\) (which is not studied by [19]). As we discuss more shortly, both our lower bound proof and [19] use the high-level technique of [1], although our more fine-grained analysis of the parameters \(d,B\) requires a more intricate analysis and the use of other techniques from fine-grained complexity related to approximate nearest neighbor search [13] and the polynomial method [1].

### Technique Overview

Our high-level approach is to make use of similarities between attention computation and other computational problems related to Kernel Density Estimation (KDE). Such a relationship was investigated by recent work [19, 23]. In particular, [23] was inspired to apply LSH techniques to attention computation because of the prevalence of LSH in KDE algorithms [17, 18, 19, 30]. The main conceptual idea behind our results is that different techniques from the KDE literature, other than LSH, can be modified to apply in this setting and yield tight algoriths and lower bounds.

To design our algorithm for Theorem 1.3, we instead build off of a different line of work on KDE which makes use of the 'polynomial method in algorithm design'. Suppose \(M\in\mathbb{R}^{n\times n}\) is a matrix, \(f:\mathbb{R}\to\mathbb{R}\) is a function, and let \(f(M)\) denote the matrix one gets by applying \(f\) entry-wise to \(M\). The polynomial method is a technique for finding low-rank approximations of \(f(M)\). It shows that if \(M\) has low rank, and if \(f\) can be approximated by a low-degree polynomial, then the matrix \(f(M)\) is very close to a low-rank matrix whose low-rank decomposition can be computed efficiently.

To use this to solve \(\mathsf{AAtt}\), we make use of a recent result which bounds the degree required to approximate the exponential function by a polynomial [1] in order to find a low-rank approximation of the attention matrix \(A\). Prior work [1, 2] applied these polynomials in a similar way to solve the Gaussian KDE problem; our main observation is that by an appropriate rescaling, this approach can be modified to apply to \(\mathsf{AAtt}\) as well.

The proof of our lower bound Theorem 1.3 builds off of another line of work on the fine-grained complexity of KDE problems [1, 22, 1]. The main idea is to give a fine-grained reduction from the well-studied problem of Approximate Nearest Neighbor search \(\mathsf{ANN}\). In \(\mathsf{ANN}\), one is given as input \(n\) vectors of dimension \(d\), and an error parameter \(\epsilon>0\), and the goal is to find a pair of vectors whose distance is at most \((1+\epsilon)\) times the _minimum_ distance between any pair of the vectors. The straightforward algorithm for \(\mathsf{ANN}\) runs in quadratic time, and it is known that it is impossible to solve \(\mathsf{ANN}\) in truly subquadratic time assuming \(\mathsf{SETH}\)[20].

In order to prove our lower bound, we show that \(\mathsf{AAtt}\) can be used to solve \(\mathsf{ANN}\). The key idea is that, if the matrices \(Q\) and \(K\) from \(\mathsf{AAtt}\) are formed by concatenating the input vectors to the \(\mathsf{ANN}\) problem, then the nearest neighbor vectors correspond to the largest entries of the attention matrix \(A\). It is not immediately clear that \(\mathsf{AAtt}\) can be used to detect large entries of \(A\), since the output is rescaled by the matrix \(D^{-1}\), but we show that this can be overcome with some modifications to the input vectors which approximately balance the rows of \(A\). Prior work [1, 22] used a very similar approach to give lower bounds for KDE problems, although KDE doesn't involve any rescaling factors.

#### Roadmap.

In Section 2, we introduce relevant notation and tools from prior work. In Section 3, we present and analyze our attention algorithm. In Section 4, we prove our fine-grained attention lower bound. In Section 5, we provide a conclusion for this paper.

## 2 Preliminaries

We work in the standard real-RAM model and assume arithmetic operations on real numbers can be performed in constant time in our algorithms.

We use \(\mathcal{T}_{\mathrm{mat}}(a,b,c)\) to denote the time to multiply an \(a\times b\) matrix with another \(b\times c\) matrix. In fact, we will only make use of the straightforward, practical bound \(\mathcal{T}_{\mathrm{mat}}(a,b,c)\leq O(abc)\). In principle, fast theoretical matrix multiplication algorithms could be used instead to improve this bound and speed up our algorithms here (in exchange for making them less practical). That said, because of our parameter settings3, we will see that faster matrix multiplication could only improve low-order terms in our running times.

Footnote 3: We will make use of \(\mathcal{T}_{\mathrm{mat}}(n,n^{o(1)},n^{o(1)})\), which can be solved straightforwardly in time \(n^{1+o(1)}\), and which cannot be solved much faster since it has input size \(n^{1+o(1)}\).

For any positive integer, we use \([n]\) to denote set \(\{1,2,\cdots,n\}\).

For a matrix \(M\), we write \(\|M\|_{\infty}\) to denote its \(\ell_{\infty}\) norm, i.e., \(\|M\|_{\infty}:=\max_{i,j}|M_{i,j}|\). For a matrix \(M\), we use \(M^{\top}\) to denote its transpose.

We use \(\mathbf{1}_{n}\) to denote a length-\(n\) vector whose entries are all \(1\)s. We use \(\mathbf{0}_{n}\) to denote a length-\(n\) vector whose entries are all \(0\)s.

For any matrix \(A\in\mathbb{R}^{n\times n}\), we use \(\exp(A)\in\mathbb{R}^{n\times n}\) to denote the matrix where \(\exp(A)_{i,j}=\exp(A_{i,j})\). In other words, all the \(\exp()\) operators in this paper are applied entry-wise to matrices. In particular, we will not use matrix exponentials in this paper.

For a vector \(x\in\mathbb{R}^{n}\), we use \(\|x\|_{0}\) to denote its number of non-zero entries, we use \(\|x\|_{1}\) to denote its \(\ell_{1}\) norm, i.e., \(\|x\|_{1}:=\sum_{i=1}^{n}|x_{i}|\), and we use \(\|x\|_{2}\) to denote its \(\ell_{2}\) norm, i.e., \(\|x\|_{2}:=(\sum_{i=1}^{n}|x_{i}|^{2})^{1/2}\). For a vector \(x\), we use \(x^{\top}\) to denote its transpose.

### Additive Error for Polynomial Approximation

Our algorithm for attention computation will critically make use of a polynomial approximation for the exponential function. In particular, we use the following tight construction from previous work [1].

**Lemma 2.1** ([1]).: _Let \(B>1\) and let \(\epsilon\in(0,0.1)\). There is a polynomial \(P:\mathbb{R}\to\mathbb{R}\) of degree \(g:=\Theta\left(\max\left\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B)},B \right\}\right)\) such that for all \(x\in[0,B]\), we have_

\[|P(x)-\exp(x)|<\epsilon.\]

_Moreover, \(P\) can be computed efficiently: its coefficients are rational numbers with \(\operatorname{poly}(g)\)-bit integer numerators and denominators which can be computed in \(\operatorname{poly}(g)\) time._

### From Additive Error to Relative Error

We note that in our setting, Lemma 2.1 can be used to give a relative error approximation as well:

**Corollary 2.2**.: _Let \(B>1\) and let \(\epsilon\in(0,0.1)\). There is a polynomial \(P:\mathbb{R}\to\mathbb{R}\) of degree \(g:=\Theta(\max\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B)},B\})\) such that for all \(x\in[-B,B]\), we have_

\[|P(x)-\exp(x)|<\epsilon\cdot\exp(x).\]

Proof.: By Lemma 2.1, there is a polynomial \(Q:\mathbb{R}\to\mathbb{R}\) of degree \(g=\Theta(\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B)},B\})\) such that, for all \(y\in[0,2B]\) we have \(|Q(y)-\exp(y)|\leq\epsilon\). Our desired polynomial is the rescaled \(P(x):=Q(x+B)/\exp(B)\). Indeed, for any \(x\in[-B,B]\), we have \(\exp(x)\geq\exp(-B)\), and so

\[|P(x)-\exp(x)| =|Q(x+B)/\exp(B)-\exp(x)|\] \[=|Q(x+B)-\exp(x+B)|/\exp(B)\] \[\leq\epsilon/\exp(B)\] \[\leq\epsilon\cdot\exp(x),\]

as desired. 

## 3 Attention Algorithm

In this section, we show how to use polynomial approximations for the exponential function in order to approximately perform attention computations. In Section 3.1, we define the type of low-rank matrix approximation which we will use. In Section 3.2, we show how polynomial approximations can give rise to such low-rank matrix approximations. In Section 3.3, we bound the entries of the matrix \(QK^{\top}\in\mathbb{R}^{n\times n}\) (before converting it to the attention matrix) to confirm that our polynomial approximation applies. In Section 3.4, we state our main technique for approximating the attention matrix. In Section 3.5, we show how to control the error propagation from \(A\) to the rescaling matrix \(D\). In Section 3.6, we further explain how to control the error propagation from \(D\) and \(A\) to the resulting attention matrix. Finally, in Section 3.7, we conclude our general algorithm, and in Section 3.8, we appropriately select the parameters to achieve almost linear time.

### Matrix Low-Rank Approximation

**Definition 3.1**.: _Let \(r\geq 1\) denote a positive integer. Let \(\epsilon\in(0,0.1)\) denote an accuracy parameter. Given a matrix \(A\in\mathbb{R}_{\geq 0}^{n\times n}\), we say \(\tilde{A}\in\mathbb{R}_{\geq 0}^{n\times n}\) is an \((\epsilon,r)\)-approximation of \(A\) if_* \(\widetilde{A}=U_{1}\cdot U_{2}^{\top}\) _for some matrices_ \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) _(i.e.,_ \(\widetilde{A}\) _has rank at most_ \(r\)_), and_
* \(|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon\cdot A_{i,j}\) _for all_ \((i,j)\in[n]^{2}\)_._

### From Low Degree Polynomials to Low Rank Matrices

**Lemma 3.2**.: _Let \(M=XY^{\top}\in\mathbb{R}^{n\times n}\) denote a matrix with \(X,Y\in\mathbb{R}^{n\times d}\). Let \(P(x)\) denote a degree-\(g\) polynomial, and define \(r=\binom{2(g+d)}{2g}\)._

_There is an algorithm that runs in \(O(nrg)\) time and, given as input the matrix \(X,Y\), constructs matrices \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) such that \(P(M)=U_{1}U_{2}^{\top}\). (Here, \(P(M)\) denotes the entry-wise application of \(P\) to \(M\).)_

Due to space limitation, we defer the proof of Lemma 3.2 to Appendix A.

### Matrix \(Qk^{\top}\) Has Bounded Entries

**Lemma 3.3** (Bounded entry).: _Suppose \(B\geq 1\) and matrices \(Q,K\in\mathbb{R}^{n\times d}\) have \(\|Q\|_{\infty}\leq B\) and \(\|K\|_{\infty}\leq B\). Then, we have_

\[\|QK^{\top}/d\|_{\infty}\leq B^{2}.\]

Proof.: For each \((i,j)\in[n]\times[n]\), we have

\[|(QK^{\top})_{i,j}| =|\sum_{l=1}^{d}Q_{i,l}K_{j,l}|\] \[\leq d\cdot\|Q\|_{\infty}\cdot\|K\|_{\infty}\] \[\leq d\cdot B^{2},\]

as desired. 

### Key Lemma

Our key lemma shows that, even though the attention matrix \(A\) may have full rank, it has a low-rank approximation that is easy to compute:

**Lemma 3.4**.: _Suppose \(Q,K\in\mathbb{R}^{n\times d}\), with \(\|Q\|_{\infty}\leq B\), and \(\|K\|_{\infty}\leq B\). Let \(A:=\exp(QK^{\top}/d)\in\mathbb{R}^{n\times n}\). For accuracy parameter \(\epsilon\in(0,1)\), there is a positive integer \(g\) bounded above by_

\[g=O\Big{(}\max\Big{\{}\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B^{2})}, B^{2}\Big{\}}\Big{)},\]

_and a positive integer \(r\) bounded above by_

\[r\leq\binom{2(g+d)}{2g}\]

_such that: There is a matrix \(\widetilde{A}\in\mathbb{R}^{n\times n}\) that is an \((\epsilon,r)\)-approximation (Definition 3.1) of \(A\in\mathbb{R}^{n\times n}\). Furthermore, the matrices \(U_{1}\) and \(U_{2}\) defining \(\widetilde{A}\) can be computed in \(O(n\cdot r)\) time._

Proof.: Let \(M:=QK^{\top}/d\). From Lemma 3.3, we know that \(\|M\|_{\infty}\leq B^{2}\). Thus, applying Corollary 2.2 (with bound \(B^{2}\) on its entries), there is a degree-\(g\) polynomial \(P\) such that the matrix \(\widetilde{A}=P(M)\) is an \((\epsilon,r)\)-approximation to \(A\) (See the definition of \((\epsilon,r)\)-approximation in Definition 3.1.) We can then compute \(U_{1},U_{2}\) using Lemma 3.2, which gives the bound

\[r\leq\binom{2(g+d)}{2g}.\]

This completes the proof.

### From \(A\) to \(D\)

**Lemma 3.5**.: _Let \(A\in\mathbb{R}^{n\times n}\) be any matrix whose entires are all positive and \(\epsilon_{A}\in(0,0.1)\) be any parameter. Let \(\widetilde{A}\in\mathbb{R}^{n\times n}\) be any matrix such that, for all \((i,j)\in[n]\times[n]\), we have_

\[|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon_{A}\cdot A_{i,j}.\]

_Define the matrices \(D,\widetilde{D}\in\mathbb{R}^{n\times n}\) by \(D=\operatorname{diag}(A\mathbf{1}_{n})\) and \(\widetilde{D}=\operatorname{diag}(\widetilde{A}\mathbf{1}_{n})\). Then, for all \(i\in[n]\) we have_

\[|\widetilde{D}_{i,i}-D_{i,i}|\leq\epsilon_{A}\cdot D_{i,i}.\]

Due to space limitation, we defer the proof of Lemma 3.5 into Appendix A.

### From \(A\) and \(D\) to Attention Matrix

**Lemma 3.6**.: _Let \(\epsilon_{A},\epsilon_{D}\in(0,0.1)\) and \(B>1\) be parameters, and let \(V\in\mathbb{R}^{n\times d}\) denote a matrix with \(\|V\|_{\infty}\leq B\). Let \(A\in\mathbb{R}^{n\times n}\) be any matrix whose entires are all positive, and let \(\widetilde{A}\in\mathbb{R}^{n\times n}\) be a matrix such that, for all \((i,j)\in[n]\times[n]\) we have_

\[|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon_{A}\cdot A_{i,j}.\]

_Let \(D,\widetilde{D}\in\mathbb{R}^{n\times n}\) be any diagonal matrices with positive entries on their diagonals, with the property that, for all \(i\in[n]\), we have_

\[|\widetilde{D}_{i,i}-D_{i,i}|\leq\epsilon_{D}\cdot D_{i,i}.\]

_Then, we have_

\[\|\widetilde{D}^{-1}\widetilde{A}V-D^{-1}AV\|_{\infty}\leq(\epsilon_{A}+ \epsilon_{D})\cdot B.\]

Due to space limitation, we delay the proof of Lemma 3.6 to Appendix A.

### Main Upper Bound

**Theorem 3.7**.: _For positive integers \(n,d\) and real parameters \(\epsilon>0\) and \(B>1\), there are positive integers \(g=\Theta(\max\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B^{2})},B^{2}\})\) and \(r=\binom{2(g+d)}{2d}\) such that: There is an algorithm (Algorithm 1) that runs in \(O(\mathcal{T}_{\mathrm{mat}}(n,r,d)+nrg)\) time to solve \(\mathsf{AAttC}(n,d,B,\epsilon)\) (Definition 1.2)._

Proof.: The running time of each step is shown in Algorithm 1; its running time follows from Lemma 3.4. Its correctness follows from Lemma 3.5 and Lemma 3.6. 

```
1:procedurePolyAttention(\(Q\in\mathbb{R}^{n\times d},K\in\mathbb{R}^{n\times d},V\in\mathbb{R}^{n\times d},n,d,B,\epsilon\)) \(\triangleright\) Theorem 1.4
2:\(\triangleright\)\(\epsilon\) is the accuracy output
3:\(\triangleright\)\(\|Q\|_{\infty},\|K\|_{\infty},\|V\|_{\infty}\leq B\)
4:\(g\gets O(\max\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B^{2})},B^{2}\})\)
5:\(r\leftarrow\binom{2(g+d)}{2d}\)
6: Construct \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) via Lemma 3.4 \(\triangleright\)\(O(nrg)\) time
7:\(\widetilde{w}\gets U_{1}\cdot(U_{2}^{\top}\mathbf{1}_{n})\)\(\triangleright\)\(O(nr)\) time
8:\(\widetilde{D}^{-1}=\operatorname{diag}(\widetilde{w}^{-1})\)\(\triangleright\)\(O(n)\) time
9: Compute \(U_{2}^{\top}V\in\mathbb{R}^{r\times d}\)\(\triangleright\) Takes \(\mathcal{T}_{\mathrm{mat}}(r,n,d)\) time
10: Compute \(U_{1}\cdot(U_{2}^{\top}V)\)\(\triangleright\)\(\mathcal{T}_{\mathrm{mat}}(n,r,d)\) time
11:\(T\leftarrow\widetilde{D}^{-1}\cdot(U_{1}\cdot(U_{2}^{\top}V))\)\(\triangleright\)\(O(nd)\) time
12:return\(T\)\(\triangleright\)\(T\in\mathbb{R}^{n\times d}\)
13:endprocedure ```

**Algorithm 1** Our Algorithm

```
1:procedurePolyAttention(\(Q\in\mathbb{R}^{n\times d},K\in\mathbb{R}^{n\times d},V\in\mathbb{R}^{n\times d},n,d,B,\epsilon\)) \(\triangleright\) Theorem 1.4
2:\(\triangleright\)\(\epsilon\) is the accuracy output
3:\(\triangleright\)\(\|Q\|_{\infty},\|K\|_{\infty},\|V\|_{\infty}\leq B\)
4:\(g\gets O(\max\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B^{2})},B^{2}\})\)
5:\(r\leftarrow\binom{2(g+d)}{2d}\)
6: Construct \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) via Lemma 3.4 \(\triangleright\)\(O(nrg)\) time
7:\(\widetilde{w}\gets U_{1}\cdot(U_{2}^{\top}\mathbf{1}_{n})\)\(\triangleright\)\(O(nr)\) time
8:\(\widetilde{D}^{-1}=\operatorname{diag}(\widetilde{w}^{-1})\)\(\triangleright\)\(O(n)\) time
9: Compute \(U_{2}^{\top}V\in\mathbb{R}^{r\times d}\)\(\triangleright\) Takes \(\mathcal{T}_{\mathrm{mat}}(r,n,d)\) time
10: Compute \(U_{1}\cdot(U_{2}^{\top}V)\)\(\triangleright\)\(\mathcal{T}_{\mathrm{mat}}(n,r,d)\) time
11:\(T\leftarrow\widetilde{D}^{-1}\cdot(U_{1}\cdot(U_{2}^{\top}V))\)\(\triangleright\)\(O(nd)\) time
12:return\(T\)\(\triangleright\)\(T\in\mathbb{R}^{n\times d}\)
13:endprocedure ```

**Algorithm 2** Our Algorithm

### Proof of Theorem 1.4

**Theorem 3.8** (Upper bound, formal statement of Theorem 1.4).: \(\mathsf{AAttC}(n,d=O(\log n),B=o(\sqrt{\log n}),\epsilon_{a}=1/\operatorname{poly }(n))\) _can be solved in time \(\mathcal{T}_{\mathrm{mat}}(n,n^{o(1)},d)=n^{1+o(1)}\)._

Proof.: If we select the parameters

\[B=o(\sqrt{\log n}),\ \ \epsilon=1/\operatorname{poly}(n),\ \ \ d=O(\log n)\]

in Theorem 3.7, then we see that

\[g =O(\max\{\frac{\log(1/\epsilon)}{\log(\log(1/\epsilon)/B^{2})},B^ {2}\})\] \[=O(\max\{\frac{\log(n)}{\log(\log(n)/B^{2})},B^{2}\})\] \[=O(\max\{\frac{\log n}{\log\log n},o(\log n)\})\] \[=o(\log n),\]

where the second step follows from \(\epsilon=1/\operatorname{poly}(n)\) and the third step follows from \(B=o(\sqrt{\log n})\). Since \(g=o(\log n)\), let us write \(g=(\log n)/f\) for some \(f=\omega(1)\). We thus have that

\[r=\binom{2(d+g)}{2g}\leq\left(\frac{e(d+g)}{g}\right)^{2g}\leq 2^{O(g\log(( \log n)/g))}\leq 2^{O(\log n\log(f)/f)}<2^{o(\log n)}<n^{o(1)}.\]

The second step follows from the generic bound \(\binom{a}{b}\leq(ea/b)^{b}\) for \(1\leq b\leq a\), and the third step uses that \(d=O(\log n)\).

Since \(d,r,g\) are all bounded by \(n^{o(1)}\), our final running time is \(n^{1+o(1)}\) as desired. 

## 4 Hardness

In this section, we prove our fine-grained lower bound for attention computation. In Section 4.1, we state the Strong Exponential Time Hypothesis (\(\mathsf{SETH}\)), the main hardness assumption we will use. In Section 4.2, we define the approximate nearest neighbor search problem, and its known hardness assuming \(\mathsf{SETH}\). Finally, in Section 4.3, we give a reduction from approximate nearest neighbor search to attention computation, which implies our hardness result.

### Fine-Grained Hypotheses

The Strong Exponential Time Hypothesis (\(\mathsf{SETH}\)) was introduced by Impagliazzo and Paturi [11] over 20 years ago. It is a strengthening of the \(\mathsf{P}\neq\mathsf{NP}\) conjecture, which asserts that our current best \(\mathsf{SAT}\) algorithms are roughly optimal:

**Hypothesis 4.1** (Strong Exponential Time Hypothesis (\(\mathsf{SETH}\))).: _For every \(\epsilon>0\) there is a positive integer \(k\geq 3\) such that \(k\text{-}\mathsf{SAT}\) on formulas with \(n\) variables cannot be solved in \(O(2^{(1-\epsilon)n})\) time, even by a randomized algorithm._

\(\mathsf{SETH}\) is a popular conjecture which has been used to prove fine-grained lower bounds for a wide variety algorithmic problems. See, for instance, the survey [10].

### Nearest Neighbor Search

We will make use of a known relationship between \(\mathsf{SETH}\) and approximate nearest neighbor search.

**Definition 4.2** (Approximate Hamming Nearest Neighbor Search \((\mathsf{ANN})\)).: _For a parameter \(\epsilon>0\), in the \((1+\epsilon)\)-Approximate Hamming Nearest Neighbor Search problem for \(n\) vectors of dimension \(d\), we are given as input two sets \(A,B\subset\{0,1\}^{d}\) with \(|A|=|B|=n\), and our goal is to find an \(a^{*}\in A\) and \(b^{*}\in B\) satisfying \(\|a^{*}-b^{*}\|_{0}\leq(1+\epsilon)\cdot\min_{a\in A,b\in B}\|a-b\|_{0}\)._(This is sometimes called the 'bichromatic' \(\mathsf{ANN}\) problem, and a monochromatic version has also been studied; see, for instance, [10].) Rubinstein [14] showed that for certain parameters, it is impossible to substantially improve on the straightforward quadratic-time algorithm for \(\mathsf{ANN}\) assuming \(\mathsf{SETH}\):

**Lemma 4.3** ([14]).: _Assuming \(\mathsf{SETH}\), for every \(q>0\), there are \(\epsilon\in(0,1)\) and \(C>0\) such that \((1+\epsilon)\)-Approximate Hamming Nearest Neighbor Search in dimension \(d=C\log n\) requires \(\Omega(n^{2-q})\) time._

**Remark 4.4**.: _We may assume that 4.3 holds even in the special case where each input vector from \(A\) and \(B\) has half its entries equal to \(0\) and half equal to \(1\). Indeed, for any vector \(a\in\{0,1\}^{d}\), we can construct a new vector \(\widetilde{a}\in\{0,1\}^{2d}\) given by \(\widetilde{a}=\begin{bmatrix}a^{\top}&\overline{a}^{\top}\end{bmatrix}^{\top}\). Here \(\overline{a}\in\{0,1\}^{d}\) is the binary complement of vector \(a\), i.e., \(\overline{a}_{i}=1-a_{i}\) for all \(i\in[d]\). Thus, \(\|\widetilde{a}\|_{0}=d\). We can similarly construct a new vector \(\widetilde{b}\in\{0,1\}^{2d}\) for each \(b\in B\). After this transformation, for any \(a\in A\) and \(b\in B\), we have \(\|\widetilde{a}-\widetilde{b}\|_{0}=2\cdot\|a-b\|_{0}\), so it suffices to find an approximate nearest neighbor among these transformed vectors._

For convenience in our the analysis, we define a gap version of approximate nearest neighbor search problem \(\mathsf{Gap}-\mathsf{ANN}(n,d,t,\epsilon)\).

**Definition 4.5** (Gap approximate nearest neighbor search (\(\mathsf{Gap}-\mathsf{ANN}(n,d,t,\epsilon)\))).: _Let \(n,d\) denote two positive integers. Let \(t>0\) denote a threshold parameter. Let \(\epsilon\) denote a accuracy parameter. Given two sets of points \(A=\{a_{1},\cdots,a_{n}\}\subset\{0,1\}^{d}\) and \(B=\{b_{1},\cdots,a_{n}\}\subset\{0,1\}^{d}\): For each \(i\in[n]\), we need to distinguish the following two cases_

* _Case 1. There exists a_ \(j\in[n]\) _such that_ \(\|a_{i}-b_{j}\|_{0}<t\)_._
* _Case 2. For all_ \(j\in[n]\) _we have_ \(\|a_{i}-b_{j}\|_{2}^{2}\geq(1+\epsilon)\cdot t\)_._

An algorithm for \(\mathsf{Gap}-\mathsf{ANN}(n,d,t,\epsilon)\) can be called \(\log(nd)\) times to binary search for the answer to \(\mathsf{ANN}\), so Lemma 4.3 holds as well for \(\mathsf{Gap}-\mathsf{ANN}(n,d,t,\epsilon)\).

### Hardness Result

In the remainder of this section, we prove our lower bound for attention computation:

**Theorem 4.6** (Main Result, formal version of Theorem 1.3).: _Assuming \(\mathsf{SETH}\), for every sufficiently small \(q>0\), there are constants \(C>0\) and \(C_{\alpha}>0\) and \(C_{\beta}>1\) such that Approximate Attention Computation \(\mathsf{AAttC}\) (Definition 1.2) for parameters \((n,d=C\log n,B=C_{\beta}\sqrt{\log n},\epsilon_{a}=n^{-C_{\alpha}})\) requires \(\Omega(n^{2-q})\) time._

Proof.: This follows from combining Lemma 4.3 (hardness for approximation nearest neighbor search) and Lemma 4.7 (a reduction from approximate nearest neighbor search to approximate attention computation) which we prove below. 

**Lemma 4.7**.: _For any constant \(C_{\gamma}\in(0,0.1)\): For every \(\epsilon>0\) and \(C>0\), there exist constants \(C_{a}>0\) and \(C_{b}>0\) and such that, if \(\mathsf{AAttC}\) (Definition 1.2) for parameters \((2n,d=2C\log n,B=C_{b}\sqrt{\log n},\epsilon_{a}=n^{-C_{a}})\) can be solved in time \(T\), then \(\mathsf{Gap}-\mathsf{ANN}(n,d=C\log n,t,\epsilon)\) (Definition 4.5) can be solved in time \(O(T+n^{2-C_{\gamma}})\)._

Due to space limitation, we defer the proof of Lemma 4.7 to Appendix B.

## 5 Conclusion

In this work, we showed that how quickly one can perform attention computation depends critically on the magnitude, \(B\), of the entries of the input matrices. Our main idea was to make use of similarities between attention computation and KDE, and to show how many known techniques for KDE can also be used in this setting. Since KDE is a very well-studied problem, it would be exciting to see what other techniques can be applied to attention computation as well. One limitation of our lower bound result is, it is a conditional lower bound which is based on a well-known conjecture SETH in the area of complexity. It would be interesting to show unconditional lower bound for future work. As far as we are aware, our work does not have negative societal impacts.

AcknowledgementsJosh Alman was partly supported by a grant from the Simons Foundation (Grant Number 825870 JA). The authors would like to thank Beidi Chen for helpful discussions related to LLMs, and Feyza Duman, Chinmay Hegde, and Piotr Indyk for helpful comments on an earlier draft. The authors would like to appreciate very constructable feedbacks for NeurIPS 2023 Reviewers. The authors would like to thanks Lichen Zhang and Ruizhe Zhang for useful and helpful suggestions about proof-reading the paper.

## References

* [AA22] Amol Aggarwal and Josh Alman. Optimal-degree polynomial approximations for exponentials and gaussian kernel density estimation. In _37th Computational Complexity Conference (CCC 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* [ACM\({}^{+}\)20] Josh Alman, Timothy Chu, Gary Miller, Shyam Narayanan, Mark Sellke, and Zhao Song. Metric transforms and low rank matrices via representation theory of the real hyperrectangle. _arXiv preprint arXiv:2011.11503_, 2020.
* [ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear algebra on geometric graphs. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 541-552. IEEE, 2020.
* [AS23] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. _arXiv preprint arXiv:2310.04064_, 2023.
* [BCIS18] Arturs Backurs, Moses Charikar, Piotr Indyk, and Paris Siminelakis. Efficient density evaluation for smooth kernels. In _2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 615-626. IEEE, 2018.
* [BIS17] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the fine-grained complexity of empirical risk minimization: Kernel methods and neural networks. _Advances in Neural Information Processing Systems_, 30, 2017.
* [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems (NeurIPS)_, 33:1877-1901, 2020.
* [CDL\({}^{+}\)22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In _International Conference on Learning Representations (ICLR)_, 2022.
* [CDW\({}^{+}\)21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:17413-17426, 2021.
* [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.
* [CKNS20] Moses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis. Kernel density estimation through density constrained near neighbor search. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 172-183. IEEE, 2020.
* [CND\({}^{+}\)22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [CS17] Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 1032-1043. IEEE, 2017.

* [CS19] Moses Charikar and Paris Siminelakis. Multi-resolution hashing for fast pairwise summations. In _2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 769-792. IEEE, 2019.
* [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:6476-6489, 2020.
* [DLS23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression. _arXiv preprint arXiv:2304.10411_, 2023.
* [DSZ23] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. _arXiv preprint arXiv:2310.11685_, 2023.
* [GSWY23] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time. _arXiv preprint arXiv:2309.07418_, 2023.
* [GSY23] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. _arXiv preprint arXiv:2305.04701_, 2023.
* [GSYZ23] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention computation. _arXiv preprint arXiv:2307.08045_, 2023.
* [HJK\({}^{+}\)23] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. _arXiv preprint arXiv:2310.05869_, 2023.
* [IP01] Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. _Journal of Computer and System Sciences_, 62(2):367-375, 2001.
* [KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [KMZ23] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. _arXiv preprint arXiv:2310.01655_, 2023.
* [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International Conference on Machine Learning (ICLR)_, pages 5156-5165. PMLR, 2020.
* [KWH23] Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational complexity of self-attention. In _International Conference on Algorithmic Learning Theory_, pages 597-619. PMLR, 2023.
* [LWD\({}^{+}\)23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In _International Conference on Machine Learning_, pages 22137-22176. PMLR, 2023.
* [Ope22] OpenAI. Chatgpt: Optimizing language models for dialogue. In _OpenAI Blog_. https://openai.com/blog/chatgpt/, Nov 2022.
* [RNS\({}^{+}\)18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [Rub18] Aviad Rubinstein. Hardness of approximate nearest neighbor search. In _Proceedings of the 50th annual ACM SIGACT symposium on theory of computing (STOC)_, pages 1260-1268, 2018.

* [RWC\({}^{+}\)19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [SM19] Karthik C. S. and Pasin Manurangsi. On closest pair in euclidean metric: Monochromatic is as hard as bichromatic. _10th Innovations in Theoretical Computer Science (ITCS)_, 2019.
* [TBY\({}^{+}\)19] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4344-4353, 2019.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems (NeurIPS)_, 30, 2017.
* [Wil18] Virginia Vassilevska Williams. On some fine-grained questions in algorithms and complexity. In _Proceedings of the international congress of mathematicians: Rio de Janeiro 2018_, pages 3447-3487. World Scientific, 2018.
* [WLK\({}^{+}\)20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [ZBIW19] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In _2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)_, pages 36-39. IEEE, 2019.
* [ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In _ICML_. arXiv preprint arXiv:2302.02451, 2023.
* [ZRG\({}^{+}\)22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* [ZSZ\({}^{+}\)23] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H _2. o: Heavy-hitter oracle for efficient generative inference of large language models. In _NeurIPS_. arXiv preprint arXiv:2306.14048, 2023.

## Appendix

### Roamdap.

We provide the missing proofs of our upper bound (algorithm) result in Section A. We provide the missing proofs of our lower bound (hardness) result in Section B.

## Appendix A Missing Proofs for Upper Bound

### Proof of Lemma 3.2

**Lemma A.1** (Restatement of Lemma 3.2).: _Let \(M=XY^{\top}\in\mathbb{R}^{n\times n}\) denote a matrix with \(X,Y\in\mathbb{R}^{n\times d}\). Let \(P(x)\) denote a degree-\(g\) polynomial, and define \(r=\binom{2(g+d)}{2g}\)._

_There is an algorithm that runs in \(O(nrg)\) time and, given as input the matrix \(X,Y\), constructs matrices \(U_{1},U_{2}\in\mathbb{R}^{n\times r}\) such that \(P(M)=U_{1}U_{2}^{\top}\). (Here, \(P(M)\) denotes the entry-wise application of \(P\) to \(M\).)_

Proof.: Let \(P(x)\) denote the degree-\(g\) polynomial. Expand it in terms of its coefficients as

\[P(x)=\sum_{i=0}^{d}c_{i}\cdot x^{i}.\]

Consider the function \(\mathsf{K}:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) defined by, for \(u,v\in\mathbb{R}^{d}\),

\[\mathsf{K}(u,v):=P(\langle u,v\rangle).\]

\(\mathsf{K}\) is a degree-\(2g\) polynomial in the \(2d\) entries \(u_{1},\cdots u_{d},v_{1},\cdots,v_{d}\) of the vectors \(u,v\). Define the set \(V\) of its variables,

\[V:=\{u_{1},\cdots,u_{d},v_{1},\cdots,v_{d}\}.\]

Let \(\mathcal{F}\) denote the set of functions

\[\mathcal{F}:=\left\{f:V\to\{0,1,2,\cdots,2g\}\ |\ \sum_{v\in V}f(v)\leq 2g \right\}.\]

We can count that

\[|\mathcal{F}|=\binom{2d+2g}{2g}.\]

Hence, there are coefficients \(c_{t}\in\mathbb{R}\) for each \(t\in\mathcal{F}\) such that

\[\mathsf{K}(u,v)=\sum_{t\in\mathcal{F}}c_{t}\cdot\prod_{v\in V}v^{t(v)}.\]

Define

\[V_{u}:=\{u_{1},\cdots,u_{d}\}\]

and

\[V_{v}=V\backslash V_{u}.\]

We define \(\phi_{u}:\mathbb{R}^{d}\to\mathbb{R}^{|\mathcal{F}|}\) by, for any \(t\in\mathcal{F}\),

\[\phi_{u}(u_{1},\cdots,u_{d})_{t}=c_{t}\cdot\prod_{v_{i}\in V_{u}}u_{i}^{t(u_{ i})}.\]

Similarly, we define \(\phi_{v}:\mathbb{R}^{d}\to\mathbb{R}^{|\mathcal{F}|}\) by, for any \(t\in\mathcal{F}\),

\[\phi_{v}(v_{1},\cdots,v_{d})_{t}=\prod_{v_{i}\in V_{v}}v_{i}^{t(u_{i})}.\]Thus, we have

\[\mathsf{K}(u,v)=\langle\phi_{u}(u),\phi_{v}(v)\rangle.\]

For \(i\in[n]\), let \(X_{i}\in\mathbb{R}^{d}\) denote the \(i\)-th row of \(X\), and let \(Y_{i}\in\mathbb{R}^{d}\) denote the \(i\)-th row of \(Y\). Our algorithm can thus construct

* the matrix \(U_{1}\in\mathbb{R}^{n\times|\mathcal{F}|}\) whose \(i\)-th row is the vector \(\phi_{u}(x_{i})\) for \(i\in[n]\), and
* the matrix \(U_{2}\in\mathbb{R}^{n\times|\mathcal{F}|}\) whose \(i\)-th row is the vectors \(\phi_{v}(y_{i})\) for \(i\in[n]\).

Each entry of these matrices can be constructed by multiplying together at most \(g\) variables, so these \(n\times r\) matrices can be constructed in time \(O(nrg)\) as desired. 

### Proof of Lemma 3.5

**Lemma A.2** (Restatement of Lemma 3.5).: _Let \(A\in\mathbb{R}^{n\times n}\) be any matrix whose entires are all positive and \(\epsilon_{A}\in(0,0.1)\) be any parameter. Let \(\widetilde{A}\in\mathbb{R}^{n\times n}\) be any matrix such that, for all \((i,j)\in[n]\times[n]\), we have_

\[|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon_{A}\cdot A_{i,j}.\]

_Define the matrices \(D,\widetilde{D}\in\mathbb{R}^{n\times n}\) by \(D=\operatorname{diag}(A\mathbf{1}_{n})\) and \(\widetilde{D}=\operatorname{diag}(\widetilde{A}\mathbf{1}_{n})\). Then, for all \(i\in[n]\) we have_

\[|\widetilde{D}_{i,i}-D_{i,i}|\leq\epsilon_{A}\cdot D_{i,i}.\]

Proof.: We calculate that

\[|\widetilde{D}_{i,i}-D_{i,i}| =|\sum_{j=1}^{n}\widetilde{A}_{i,j}-\sum_{j=1}A_{i,j}|\] \[\leq\,\sum_{j=1}^{n}|\widetilde{A}_{i,j}-A_{i,j}|\] \[\leq\,\sum_{j=1}^{n}\epsilon_{A}A_{i,j}\] \[=\epsilon_{A}\cdot D_{i}.\]

where the second step follows from triangle inequality.

This completes the proof. 

### Proof of Lemma 3.6

**Lemma A.3** (Restatement of Lemma 3.6).: _Let \(\epsilon_{A},\epsilon_{D}\in(0,0.1)\) and \(B>1\) be parameters, and let \(V\in\mathbb{R}^{n\times d}\) denote a matrix with \(\|V\|_{\infty}\leq B\). Let \(A\in\mathbb{R}^{n\times n}\) be any matrix whose entires are all positive, and let \(\widetilde{A}\in\mathbb{R}^{n\times n}\) be a matrix such that, for all \((i,j)\in[n]\times[n]\) we have_

\[|\widetilde{A}_{i,j}-A_{i,j}|\leq\epsilon_{A}\cdot A_{i,j}.\]

_Let \(D,\widetilde{D}\in\mathbb{R}^{n\times n}\) be any diagonal matrices with positive entries on their diagonals, with the property that, for all \(i\in[n]\), we have_

\[|\widetilde{D}_{i,i}-D_{i,i}|\leq\epsilon_{D}\cdot D_{i,i}.\]

_Then, we have_

\[\|\widetilde{D}^{-1}\widetilde{A}V-D^{-1}AV\|_{\infty}\leq(\epsilon_{A}+ \epsilon_{D})\cdot B.\]Proof.: We have

\[\|\widetilde{D}^{-1}\widetilde{A}V-D^{-1}AV\|_{\infty}\leq\|\widetilde{D}^{-1} \widetilde{A}V-D^{-1}\widetilde{A}V\|_{\infty}+\|D^{-1}\widetilde{A}V-D^{-1}AV\| _{\infty}.\] (1)

We now bound each of these two terms separately.

First, for each \((i,j)\in[n]\times[d]\),

\[|(\widetilde{D}^{-1}\widetilde{A}V-D^{-1}\widetilde{A}V)_{i,j}| =|\sum_{l=1}^{n}(\widetilde{D}_{i,i}^{-1}-D_{i,i}^{-1})\cdot \widetilde{A}_{i,l}\cdot V_{l,j}|\] \[\leq\,\sum_{l=1}^{n}|(\widetilde{D}_{i,i}^{-1}-D_{i,i}^{-1})\cdot \widetilde{A}_{i,l}|\cdot\|V\|_{\infty}\] \[=\,\sum_{l=1}^{n}|\frac{D_{i,i}-\widetilde{D}_{i,i}}{D_{i,i} \widetilde{D}_{i,i}}\widetilde{A}_{i,l}|\cdot\|V\|_{\infty}\] \[\leq\epsilon_{D}\cdot\sum_{l=1}^{n}|\widetilde{D}_{i}^{-1} \widetilde{A}_{i,l}|\cdot\|V\|_{\infty}\] \[=\epsilon_{D}\cdot|\sum_{l=1}^{n}\widetilde{D}_{i}^{-1} \widetilde{A}_{i,l}|\cdot\|V\|_{\infty}\] \[=\epsilon_{D}\cdot\|V\|_{\infty}\] \[\leq\epsilon_{D}\cdot B\] (2)

where the second step follows from the triangle inequality, the forth step follows from \(|(D_{i,i}-\widetilde{D}_{i,i})/D_{i,i}|\leq\epsilon_{D}\), the fifth step follows from \(\widetilde{D}_{i}^{-1}>0\) and \(\widetilde{A}_{i,l}>0\), and the last step follows from our assumption on \(V\).

Second, for each \((i,j)\in[n]\times[d]\),

\[|(D^{-1}\widetilde{A}V-D^{-1}AV)_{i,j}| =|\sum_{l=1}^{n}D_{i,i}^{-1}(\widetilde{A}_{i,l}-A_{i,l})\cdot V_{ l,j}|\] \[\leq\,\sum_{l=1}^{n}|D_{i,i}^{-1}|\cdot|(\widetilde{A}_{i,l}-A_{ i,l})|\cdot\|V\|_{\infty}\] \[=\,\sum_{l=1}^{n}D_{i,i}^{-1}\cdot|(\widetilde{A}_{i,l}-A_{i,l})| \cdot\|V\|_{\infty}\] \[\leq\,\sum_{l=1}^{n}D_{i,i}^{-1}\cdot\epsilon_{A}A_{i,l}\cdot B\] \[=\epsilon_{A}\cdot B,\] (3)

where the second step follows from triangle inequality, the third step follows from \(D_{i,i}^{-1}>0\), the forth step follows from \(|\widetilde{A}_{i,l}-A_{i,l}|\leq\epsilon_{A}\cdot A_{i,l}\) and the last step follows from definition of \(D_{i,i}\).

The result follows by combining Eq. (1), and two inequalities (Eq. (2) and Eq. (3)). 

## Appendix B Missing Proofs for Lower Bound

**Lemma B.1** (Restatement of Lemma 4.7).: _For any constant \(C_{\gamma}\in(0,0.1)\): For every \(\epsilon>0\) and \(C>0\), there exist constants \(C_{a}>0\) and \(C_{b}>0\) and such that, if \(\mathsf{AAttC}\) (Definition 1.2) for parameters \((2n,d=2C\log n,B=C_{b}\sqrt{\log n},\epsilon_{a}=n^{-C_{a}})\) can be solved in time \(T\), then \(\mathsf{Gap}-\mathsf{ANN}(n,d=C\log n,t,\epsilon)\) (Definition 4.5) can be solved in time \(O(T+n^{2-C_{\gamma}})\)._

Proof.: We give an algorithm with the stated running time for \(\mathsf{Gap}-\mathsf{ANN}(n,d=C\log n,t,\epsilon)\). Let \(c>0\) be a parameter we will choose later (it will be a function of \(C\) and \(C_{\gamma}\)). Our algorithm will proceed to one of two cases depending on the value of \(t\). If \(t<c\log n\), then we will use one algorithm which runs in time \(O(n^{2-C_{\gamma}})\). Otherwise, if \(t\geq c\log n\), we will use another algorithm which runs in time \(O(T)\).

**Case 1**: \(t<c\log n\).

Let \(a_{1},\cdots,a_{n},b_{1},\cdots,b_{n}\in\{0,1\}^{d}\) be the input vectors to \(\mathsf{Gap-ANN}\), and let \(t\in[0,d]\) denote the target distance. Recall that \(d=C\log n\).

In this \(t<c\log n\) case, we will simply brute-force for the answer in the following way: We first store the vectors \(b_{1},\cdots,b_{n}\) in a lookup table, then for each \(i\in[n]\), we iterate over all vectors \(b^{\prime}\in\{0,1\}^{d}\) which have Hamming distance at most \(t\) from \(a_{i}\) and check whether \(b^{\prime}\) is in the lookup table. This determines whether there is a \(b\in B\) at distance at most \(t\) from \(a_{i}\), as desired.

For each \(i\in[n]\), we need to iterate over \(\binom{d}{t}\) choices for the vector \(b^{\prime}\), so the total running time will be \(O(n\cdot\binom{d}{t})\). By standard bounds on binomial coefficients, we know that

\[n\cdot\binom{d}{t} \leq n\cdot\binom{C\log n}{c\log n}\] \[\leq n^{1+f(C,c)}\]

for some function \(f:\mathbb{R}_{>0}\times\mathbb{R}_{>0}\rightarrow\mathbb{R}_{>0}\) with the property that, for any fixed \(C>0\), we have

\[\lim_{c\to 0}f(C,c)=0.\]

We can thus pick a sufficiently small constant \(c>0\), depending only on \(C_{\gamma}\) and \(C\) such that \(f(C,c)<1-C_{\gamma}\) and this entire brute-force takes \(O(n^{2-C_{\gamma}})\) time.

**Case 2:**\(t\geq c\log n\).

Let \(a_{1},\cdots,a_{n},b_{1},\cdots,b_{n}\in\{0,1\}^{d}\) denote the input of \(\mathsf{Gap-ANN}(n,d,t,\epsilon)\) (Definition 4.5), and recall from Remark 4.4 that we may assume each has half its entries \(0\) and half its entries \(1\). We will explain how to construct an Attention matrix using this instance.

Let \(C_{0}\geq c\) be such that

\[t:=C_{0}\log n.\] (4)

Let \(\beta>0\) and \(\widetilde{d}\geq d\) denote parameters we will choose later (see Eq. (9) and Eq. (6), respectively). Define \(\tau>0\) by

\[\tau:=\exp(\beta/2).\] (5)

Intuitively, our goal in picking these parameters is that \(\tau\) will be an upper bound on entries of the attention matrix, i.e., we will have:

\[\tau\geq\max_{i\in[n],j\in[n]}\exp(\beta\langle a_{i},b_{j}\rangle/\widetilde{ d}).\]

We will make use of an algorithm for the \(\mathsf{AAttC}(\widetilde{n},\widetilde{d},B,\epsilon_{a})\) problem, for the following parameters:

\[\widetilde{n}:=2n,\quad\widetilde{d}:=2d,\] (6)

\[B:=C_{b}\sqrt{\log n},\ \ \text{where}\ \ C_{b}:=\sqrt{40C/(C_{0}\epsilon)},\] (7)

\[\epsilon_{a}:=n^{-C_{a}},\ \ \text{where}\ \ C_{a}:=2+C_{b}^{2}(1+C_{0}/C).\] (8)

Furthermore, set

\[\beta:=B^{2}.\] (9)We define \(Q\in\mathbb{R}^{\widetilde{n}\times\widetilde{d}}\) and \(K\in\mathbb{R}^{\widetilde{n}\times\widetilde{d}}\) as

\[Q:=\sqrt{\beta}\cdot\begin{bmatrix}a_{1}^{\top}&\mathbf{1}_{d}^{\top}\\ a_{2}^{\top}&\mathbf{1}_{d}^{\top}\\ \vdots&\vdots\\ a_{n}^{\top}&\mathbf{1}_{d}^{\top}\\ \mathbf{0}_{d}^{\top}&\mathbf{1}_{d}^{\top}\\ \mathbf{0}_{d}^{\top}&\mathbf{1}_{d}^{\top}\end{bmatrix}\text{ and }K:=\sqrt{\beta}\cdot\begin{bmatrix}b_{1}^{\top}&\mathbf{0}_{d}^{\top}\\ b_{2}^{\top}&\mathbf{0}_{d}^{\top}\\ \vdots&\vdots\\ b_{n}^{\top}&\mathbf{0}_{d}^{\top}\\ \mathbf{0}_{d}^{\top}&\mathbf{1}_{d}^{\top}\\ \vdots&\vdots\\ \mathbf{0}_{d}^{\top}&\mathbf{1}_{d}^{\top}\end{bmatrix}.\]

Since each entry of \(Q\) and \(K\) is either \(\sqrt{\beta}\) or \(0\), it follows that

\[\|Q\|_{\infty} \leq\sqrt{\beta}=B\] \[\|K\|_{\infty} \leq\sqrt{\beta}=B\] \[\|QK^{\top}/\widetilde{d}\|_{\infty} \leq\frac{\beta\cdot\widetilde{d}}{\widetilde{d}}=\beta=B^{2}.\]

Using the above construction of \(Q\in\mathbb{R}^{\widetilde{n}\times\widetilde{d}}\) and \(K\in\mathbb{R}^{\widetilde{n}\times\widetilde{d}}\), we note that

\[A:=\exp(QK^{\top}/\widetilde{d})\in\mathbb{R}^{\widetilde{n}\times\widetilde {n}}\]

is given by

\[A=\begin{bmatrix}\exp(\beta\langle a_{1},b_{1}\rangle/\widetilde{d})&\exp( \beta\langle a_{1},b_{2}\rangle/\widetilde{d})&\cdots&\exp(\beta\langle a_{1},b_{n}\rangle/\widetilde{d})&\tau&\tau&\cdots&\tau\\ \exp(\beta\langle a_{2},b_{1}\rangle/\widetilde{d})&\exp(\beta\langle a_{2},b _{2}\rangle/\widetilde{d})&\cdots&\exp(\beta\langle a_{2},b_{n}\rangle/ \widetilde{d})&\tau&\tau&\cdots&\tau\\ \vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\ \exp(\beta\langle a_{n},b_{1}\rangle/\widetilde{d})&\exp(\beta\langle a_{n},b _{2}\rangle/\widetilde{d})&\cdots&\exp(\beta\langle a_{n},b_{n}\rangle/ \widetilde{d})&\tau&\tau&\cdots&\tau\\ \exp(0)&\exp(0)&\cdots&\exp(0)&\tau&\tau&\cdots&\tau\\ \exp(0)&\exp(0)&\cdots&\exp(0)&\tau&\tau&\cdots&\tau\\ \vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\ \exp(0)&\exp(0)&\cdots&\exp(0)&\tau&\tau&\cdots&\tau\end{bmatrix}.\]

(Note that we do not explicitly compute all the entries of \(A\) in our algorithm; we will make use of it only through calling our algorithm for the Attention problem.)

For each \((i,j)\in[n]\times[n]\), we know that

\[A_{i,j} =\exp(\beta\langle a_{i},b_{j}\rangle/\widetilde{d})\] \[\leq\exp(\beta\|a_{i}\|_{\infty}\cdot\|b_{j}\|_{\infty}\cdot d/ \widetilde{d})\] \[\leq\exp(\beta)\] \[=\tau\] (10)

where the first step follows from definition of \(A\in\mathbb{R}^{\widetilde{n}\times\widetilde{n}}\), the second step follows from \(\langle a_{i},b_{j}\rangle\leq\|a_{i}\|_{\infty}\cdot\|b_{j}\|_{\infty}d\), the third step follows from \(d<\widetilde{d}\) (see Eq. (6)), and the last step follows from definition of \(\tau\) (see Eq. (5)).

On the other hand, we know that that for each \((i,j)\in[n]\times[n]\),

\[A_{i,j}\geq 0\] (11)

since it is an exponential of an entry of \(QK^{\top}/\widetilde{d}\).

Using Eq. (10) and Eq. (11), combined with our expression for \(A\), it thus follows that

\[n\tau\leq(A\mathbf{1}_{\widetilde{n}})_{i}\leq 2n\tau,\ \ \forall i\in[ \widetilde{n}].\]Since \(D_{i,i}=(A\mathbf{1}_{\widetilde{n}})_{i}\), thus we know that

\[n\tau\leq D_{i,i}\leq 2n\tau,\ \ \forall i\in[\widetilde{n}].\]

Choose the vector \(v\in\mathbb{R}^{\widetilde{n}}\) defined as

\[v=\begin{bmatrix}\mathbf{1}_{n}\\ \mathbf{0}_{n}\end{bmatrix}.\]

We define \(\widetilde{t}\) as

\[\widetilde{t}:=\frac{1}{3}\exp(0.25\beta(1-t/d))/(2n\tau).\] (12)

We can show that \(\widetilde{t}\geq\epsilon_{a}\) as follows:

\[\widetilde{t} =\frac{1}{6n}\exp(0.25\beta(1-t/d)-\beta)\] \[=\frac{1}{6n}\exp(-0.75\beta-0.25\beta t/d)\] \[=\frac{1}{6n}\exp(-0.75\beta-0.25\beta C_{0}/C)\] \[=\frac{1}{6}\exp(-0.75\beta-0.25\beta C_{0}/C-\log n)\] \[=\frac{1}{6}\exp(-0.75C_{b}^{2}\log n-0.25C_{b}^{2}(C_{0}/C)\log n -\log n)\] \[\geq n^{-C_{a}}\] \[=\epsilon_{a},\]

where the second step follows from simple algebra, the third step follows from \(t=C_{0}\log n\) (Eq. (4)) and \(d=C\log n\) (assumption in Lemma statement), the second step follows from choice of \(\beta\) (Eq. (7)), and the sixth step follows from choice of \(C_{a}\) (Eq. (8)), and the last step follows from Eq. (8).

Since \(\widetilde{t}\geq\epsilon_{a}\), if we run an algorithm for Approximation Attention Computation (Definition 1.2) \(\mathsf{AAttC}(\widetilde{n},\widetilde{d},B,\epsilon_{a})\), where we pick \(V\) to be a matrix with one row \(v\) and the rest \(0\), we can output a vector \(u\in\mathbb{R}^{\widetilde{n}}\) such that, for all \(i\in[\widetilde{n}]\),

\[|u_{i}-(D^{-1}Av)_{i}|<\widetilde{t}.\]

Note that using Remark 4.4, we have

\[\|a_{i}\|_{2}^{2}/d =0.5,\ \ \forall i\in[n],\] \[\|b_{j}\|_{2}^{2}/d =0.5,\ \ \forall j\in[n].\]

Therefore, for any \((i,j)\in[n]\times[n]\),

\[\frac{1}{d}\langle a_{i},b_{j}\rangle =\frac{1}{2d}(\|a_{i}\|_{2}^{2}+\|b_{j}\|_{2}^{2}-\|a_{i}-b_{j}\|_ {2}^{2})\] \[=\frac{1}{2d}(0.5d+0.5d-\|a_{i}-b_{j}\|_{2}^{2})\] \[=0.5-0.5\|a_{i}-b_{j}\|_{2}^{2}/d,\]

where the second step follows from \(\|a_{i}\|_{2}^{2}=\|b_{j}\|_{2}^{2}=d/2\), and the last step follows from simple algebra.

Recall that our goal is to determine, for each \(i\in[n]\), whether there is a \(j\in[n]\) such that \(\|a_{i}-b_{j}\|_{2}^{2}\leq t\), or whether \(\|a_{i}-b_{j}\|_{2}^{2}\geq(1+\epsilon_{a})t\) for all \(j\in[n]\). We will show next that we can distinguish these two cases by seeing whether \(u_{i}\) is greater than or less than the value \(\widetilde{t}_{0}:=2\widetilde{t}\).

**Case 2a.**

[MISSING_PAGE_EMPTY:19]