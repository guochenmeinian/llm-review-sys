# BertaQA: How Much Do Language Models Know About Local Culture?

Julen Etxaniz  Gorka Zakkune  Aitor Soroa  Oier Lopez de Lacalle  Mikel Artetxe

HiTZ Center - Ixa, University of the Basque Country (UPV/EHU)

julen.etxaniz@ehu.eus

###### Abstract

Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or inglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.

## 1 Introduction

Large Language Models (LLMs) have obtained impressive results on a wide range of tasks, with many benchmarks being solved soon after being released [Team et al., 2023, OpenAI et al., 2024]. Nevertheless, the majority of language model research is conducted in English, and the evaluation of these models has predominantly focused on inglocentric or global subjects. For instance, GPT-4 was reported to obtain human-level performance on a wide range of professional and academic exams [OpenAI et al., 2024], but the majority of these exams belong to US programs.1 Furthermore, multilingual benchmarks tend to suffer from the same issue, as most of them are created by translating English datasets into other languages [Conneau et al., 2018, Artetxe et al., 2019, Bandarkar et al., 2023]. As such, the current evaluation of LLMs barely covers topics that are idiosyncratic to other cultures, falling short at measuring the true usefulness of LLMs for users from these communities.

Footnote 1: In particular, 33 out of 34 exams correspond to programs or organizations from the US or Canada, such as UBE, GRE or AP, and the remaining one corresponds to coding exercises.

To better assess how LLMs perform on local topics from a minority culture in comparison with global topics, we introduce BertaQA.2 BertaQA is a multiple-choice trivia dataset with 4,756 questions divided into two subsets: local questions about the Basque Country and its culture,3 and global questions about subjects of broader interest. These questions were originally authored in Basque and professionally translated into English, making the dataset fully parallel in these two languages. The questions cover 8 diverse categories, and are labeled as easy, medium or hard. As shown in Table1, the local subset includes questions like the birthplace of Julian Retegi (a renowned champion of _Basque pelota_, a local sport), while the global subset covers topics like the soundtrack of James Bond.

Our experiments show that existing LLMs perform much better on global topics than on local topics. For instance, GPT-4 Turbo obtains 91.7% accuracy on the global subset and 72.2% on the local subset. In addition, we find that continued pretraining in Basque can substantially improve the performance on the local subset at the cost of some degradation on the global subset. For example, we outperform Llama 2 70B by 13.5 points on the local subset by continuing training it on Basque data, while losing 4.1 points on the global subset. This shows that evaluating on global questions alone, as it is commonly done, can show a distorted picture, as the trends can be radically different on local questions. Similarly, we find that translation-based approaches like _translate-test_(Conneau et al., 2018) and _self-translate_(Etxaniz et al., 2023) are much more effective on global questions. All in all, our results prompt to reconsider some prior findings when reevaluated on local subjects, and demonstrate the complex interplay between language, knowledge and culture.

In summary, our paper makes the following contributions:

* We release BertaQA, a multiple-choice trivia dataset with 4,756 questions divided into two subsets: a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest.
* We evaluate a wide range of open and commercial models and show their limitations on local questions, where they obtain significantly worse results.
* We show that continued pretraining in Basque substantially improves the models' knowledge of the Basque culture, even if queried in English. This proves that it is possible to transfer knowledge from a low-resource to a high-resource language.
* We show that LLMs fail to encode knowledge in a fully language-agnostic manner, and perform better when queried in the language they acquired the relevant knowledge in--favoring Basque for local questions and English for global questions.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \multicolumn{2}{c}{**Local Questions**} & \multicolumn{1}{c}{**Global Questions**} \\ \hline
**Basque** & What does the “Karmel” magazine specialize in? & In which of these novels does the sea not appear? \\
**and** & a) Bertsolarism & **a) “The Adventures of Tom Sawyer”** \\
**Literature** & **b) Basque culture in the past and the present** & b) “Moby Dick” \\  & c) The life of the Carmelties & c) “Treasure Island” \\ \hline
**Geography** & Where’s Atxondo? & Who was imprisoned in 1964? \\
**and** & **a) In Biscay** & **a) Nelson Mandela** \\
**History** & b) In Gipuzkoa & b) Muma Abu Jamal \\  & c) In Navare & c) Charles Ghanaky \\ \hline
**Society** & Which of the following is a Basque Government institution? & What kind of energy do we use most? \\
**and** & a) IKA & **a) Oil** \\
**Tradition** & b) AEK & b) Hydroelectric power \\  & **c) HABE** & c) Nuclear power \\ \hline
**Sports** & Where was Julian Retegi born? & Which country has won the most FIFA World Cup titles? \\
**and** & a) Areso & a) Argentina \\
**Leisure** & **b) Ertasun** & b) Germany \\  & c) Exaso & c) Brazil \\ \hline
**Culture** & Who built the Gaztela Berria or Château-Neuf in Bayome? & When did the Titanie Belfast Museum open? \\
**and** & **a) The English** & **a) In 2012** \\
**Art** & b) The French & b) In 2005 \\ \hline
**Music** & Where did the dance called “Dantzari” originate? & Who wrote the soundtrack for the James Bond series? \\
**and** & a) In the Bustura area & **a) John Barry** \\
**Dance** & b) In the Erakterri area & b) Henry Marcini \\  & **c) In the Durango area** & c) John Williams \\ \hline
**Science** & Which town in Biscay is associated with dynamiaré? & What is the scientific name for daltonism? \\
**and** & a) Leioa & a) Chondostoma \\
**Technology** & **b) Gaidaka** & b) Chromatosisosis \\  & c) Ezandio & **c) Dyschromatosis** \\ \hline  & What’s the name of the film based on Bernardo Atxaga’s \\
**Cinema** & novel “Obabakokokok”? & “Some like it hot”? \\
**and** & a)”Obabakokokok” & a) The Harp \\
**Shows** & **b) “Obabako”** & b) The Didgeridoo \\  & c) “Obabako istoriakö” & **c) The Ukulele** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Examples from the English version of BertaQA for each subset and category.

* We show that translate-test and self-translate work better for global questions than local questions, demonstrating that these approaches are not always as effective as reported in prior work.

## 2 BertaQA

BertaQA is a trivia dataset comprising 4,756 multiple-choice questions, with a single correct answer and 2 additional distractors. Crucially, questions are distributed between _local_ and _global_ topics. Local questions require specific knowledge about the Basque Country and its culture, while global questions require more general world knowledge. Additionally, questions are classified into eight categories: Basque and Literature, Geography and History, Society and Traditions, Sports and Leisure, Culture and Art, Music and Dance, Science and Technology, and Cinema and Shows. Questions are also labeled according to their difficulty as easy, medium or hard. Table 1 shows examples of BertaQA.

The dataset was originally compiled in Basque by crawling public sources that are no longer available. The questions were already classified into local and global topics, and labeled according to their difficulty level and knowledge category. We inspected the dataset and confirmed that the division was well-founded. The motivation for using this global subset, as opposed to existing QA datasets, is that the questions come from the same source, so the results should be more comparable. As such, no human annotator was involved in the creation of the Basque portion of the dataset. To check whether the content is present in other websites, we wrote some of the questions verbatim in Google search using quotation marks, but received no results. Finding the answer to some of these questions on the web is still possible, but at least the same questions are not present on the web. We do this experiment in Section 4.5, where we try to find the correct answer of some questions by searching the web. While this cannot categorically discard contamination, we believe that this, along with the nature of the raw data we crawled and the results from our experiments, makes it very unlikely that existing models were exposed to the same data during training.

Starting from the original version in Basque, we also created an English version of BertaQA using a professional translation service (Elhuyar itzulpenak4). We first wrote some translation guidelines, covering things like formatting or using Wikipedia as a reference when available to translate Basque named entities. In addition, our guidelines asked translators to discard questions whose answers require knowing the Basque language (such as onomatepieas). We initially sent 100 question/answers for translation. We reviewed these translations carefully, and worked closely with the professional translators to clarify and extend the guidelines accordingly. The remaining dataset was translated in batches of 1000 samples. The translators tagged problematic samples (difficult translation, outdated information, more than one correct answer...), which we manually reviewed. During the translation process, a few of the original questions in Basque were corrected, either because the original answer was incorrect or it became outdated. In addition, we discarded a few questions that required knowledge of Basque or English, and would lose their essence if translated.

Footnote 4: https://itzulpenak.elhuyar.eus/en

The resulting dataset is balanced regarding the number of questions per category and subset, with around 300 questions in each. The number of questions per difficulty is also balanced: most categories have around 110 easy and medium questions and 80 difficult questions in each subset. The average length of the questions and the candidates is around 50 and 13 characters, respectively. The detailed statistics of the dataset are reported in Appendix A.

## 3 Experimental Settings

We evaluate a wide range of open and commercial models on the BertaQA dataset, and measure the behavior of those models when answering local and global questions. We start by describing the tested models (Section 3.1), followed by the methods used in our experiments for each model type (Section 3.2).

### Models

We experiment with a wide range of recent models, including open base models and commercial chat models. The models include:

Open Models.We tested the recent strongest base models that are publicly available, which are primarily trained in English, but show some multilingual capabilities too: Llama 2 (Touvron et al., 2023), Llama 3 (Meta, 2024), Mistral-7B (Jiang et al., 2023), Mistral-8x7B (Jiang et al., 2024), Yi (01.AI et al., 2024), Owen 1.5 (Bai et al., 2023) and Gemma (Team and Deepmind, 2024). We decided to leave multilingual models like XGLM (Lin et al., 2022) and BLOOM (Scao et al., 2023) out, as they performed close to random performance in our preliminary experiments.

Commercial Models.We focus on the leading models from OpenAI and Anthropic. Unlike open models, these models are chat models and include more languages. For OpenAI models, we tested the latest GPT3.5 Turbo (gpt-3.5-turbo-0125), GPT4 Turbo (gpt-4-0125-preview) and GPT4 (gpt-4-0614) (OpenAI et al., 2024). For Anthropic models, we also tested the most recent models: Claude 3 Opus (claude-3-opus-20240229), Claude 3 Sonnet (claude-3-sonnet-20240229), Claude 3 Haiku (claude-3-haiku-20240307).

### Methods

Open Models.We evaluated open models using the LM Evaluation Harness library (Gao et al., 2023). We used the same multiple-choice prompt as Etxaniz et al. (2024) for Basque, and a translated version of it for English (see Appendix C). Following common practice, evaluation was done in a 5-shot fashion with random examples. In all cases, we computed the log probabilities of all candidates, and picked the one with the highest score.

Commercial Models.We kept the evaluation as similar as possible to allow a fair comparison with open models. We used the same prompts and provided few-shot examples as user and assistant messages. In addition, we used the following system prompt in English to specify the expected answer format: Respond always with a single letter: A, B or C. All experiments with closed models were performed using the official APIs from OpenAI and Anthropic.

## 4 Results

In this section, we present the results and findings from our experiments. We first report the main results on the English version of BertaQA, revealing that existing models struggle with local knowledge (Section 4.1). Section 4.2 shows that continued pretraining in Basque can improve performance on local questions, proving that knowledge can be transferred from a low-resource to a high-resource language. Section 4.3 reports results on the Basque version of the benchmark, revealing that existing models fail to encode knowledge in a fully language-agnostic manner. Finally, Section 4.4 covers translate-test and self-translate, showing that they are less effective for local questions. Appendices E and F report additional results by category and difficulty.

### Main results

We first evaluate existing models in English, focusing on the difference in performance between local and global topics. As shown in Table 2, most models obtain good results on global questions. However, the performance consistently drops when evaluated on local topics, with a gap of 26.66 points on average. More concretely, state-of-the-art models like GPT-4 Turbo and Claude 3 Opus shine on the global subset, scoring above 90% accuracy. Nevertheless, these same models obtain about 72% accuracy on the local subset. The difference is even larger for open-weight models, with the best one (Llama 3 70B) obtaining less than 60% accuracy on local questions. This confirms that, despite the impressive performance of LLMs in knowledge-intensive tasks, subjects pertinent to minority cultures remain challenging. As reported in Appendices E and F, results further drop to around 53% for the worst category and 66% for the hardest difficulty, leaving ample room for improvement for future work. We manually inspected a subset of 50 local questions that the best models (GPT 4 Turbo and Claude 3 Opus) answered incorrectly in Section 4.5.

Interestingly, we find that the performance on local and global questions is strongly correlated for the models we tested (the Pearson correlation between the two scores is 0.844). Models obtaining a similar score on the local subset also obtain a similar score on the global subset. We presume that, if the training corpus of a given model was significantly more skewed towards local topics than that of another model, the former would tend to perform better in local topics, at least in relative terms. Given that we do not observe this, we hypothesize that the training recipes of existing models are roughly equivalent in how they balance global and local knowledge. However, we do find notable differences on how scaling impacts local vs. global questions for different models. For model families with the lowest scores in BertaQA, like Llama 2, scaling yields bigger gains on the global subset, as the delta between local and global questions increases from 22.80 for the smallest variant to 28.53 for the largest variant. The opposite is true for more performant model families like GPT, with the delta between local and global questions going from 27.32 for GPT-3.5 Turbo to 19.51 for GPT-4 Turbo. This suggests that it is generally easier to improve on global questions, but this subset starts saturating for the strongest models, resulting in bigger improvements on the local subset.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Variant** & **Local** & **Global** & \(\Delta\) \\ \hline Random & N/A & 33.33 & 33.33 & 0.00 \\ \hline \multirow{3}{*}{GPT} & 3.5 Turbo & 55.08 & 82.40 & 27.32 \\  & 4 & 69.88 & 91.43 & 21.55 \\  & 4 Turbo & **72.17** & **91.68** & **19.51** \\ \hline \multirow{3}{*}{Claude 3} & Haiku & 58.71 & 84.16 & 25.45 \\  & Sonnet & 58.33 & 86.41 & 28.08 \\  & Opus & **71.91** & **91.85** & **19.94** \\ \hline \multirow{3}{*}{Llama 2} & 7B & 41.54 & 64.34 & **22.80** \\  & 13B & 43.61 & 70.36 & 26.75 \\  & 70B & **49.15** & **77.68** & 28.53 \\ \hline \multirow{3}{*}{Llama 3} & 8B & 50.38 & 76.63 & 26.25 \\  & 70B & **59.56** & **84.74** & **25.18** \\ \hline \multirow{3}{*}{Qwen 1.5} & 7B & 42.51 & 71.45 & **28.94** \\  & 14B & 44.67 & 75.92 & 31.25 \\  & 72B & **54.70** & **83.99** & 29.29 \\ \hline \multirow{3}{*}{Yi} & 6B & 44.25 & 73.20 & **28.95** \\  & 9B & 43.87 & 75.00 & 31.13 \\  & 34B & **54.06** & **83.61** & 29.55 \\ \hline \multirow{3}{*}{Mistral} & 7B & 47.50 & 74.16 & 26.66 \\  & 47B & **57.40** & **82.78** & **25.38** \\ \hline \multirow{2}{*}{Gamma} & 7B & **45.69** & **76.42** & **30.73** \\ \hline
**Average** & N/A & 53.25 & 79.91 & 26.66 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results for the English version of BertaQA. The \(\Delta\) column shows the difference between local and global results. Best results and smallest \(\Delta\) differences are in bold.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Model** & **Local** & **Global** & \(\Delta\) \\ \hline Llama 2 7B & 41.54 & **64.34** & 22.80 \\ + _eu train_ & **47.72** & 53.26 & **5.54** \\ \hline Llama 2 13B & 43.61 & **70.36** & 26.75 \\ + _eu train_ & **56.60** & 67.47 & **10.87** \\ \hline Llama 2 70B & 49.15 & **77.68** & 28.53 \\ + _eu train_ & **62.61** & 73.62 & **11.01** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Effect of continually pretraining Llama 2 in Basque on the English version of BertaQA. The best results for each size and group are in bold.

### Local knowledge transfer from Basque to English

As we have just seen, existing LLMs perform much better on global questions compared to local questions. One possible explanation is that their training data is dominated by English, which has become the de-facto world language, capturing extensive knowledge about global subjects. However, knowledge about other cultures can be scarce in English when the corresponding community speaks a different language. For instance, English Wikipedia is considerably bigger than Basque Wikipedia, but articles about Basque traditions, literature or music tend to be more extensive in the Basque language. For that reason, we hypothesize that effectively leveraging training corpora in these other languages can help bridge the gap between local and global knowledge.

To test this hypothesis, we experiment with Latxa [Etxaniz et al., 2024], a family of Basque base language models that were built by continuing training Llama 2 on Basque corpora. Latxa was trained on all publicly available corpora in Basque meeting some minimum quality standards. This mostly corresponds to crawling corpora, including both processed versions of CommonCrawl as well as ad-hoc crawling of websites with high-quality content. The largest portion of it consists of news, which is the most common use of Basque on the web. While the resulting pretraining corpus is diverse in nature, we would not say that the domains and topics in BertaQA are particularly prominent on it, although it is obviously more likely that topics related to the Basque culture are discussed in the Basque language compared to English. As shown in Table 3, this continued training in Basque brings large improvements on the local subset, outperforming the original Llama 2 model by 13.46 points in the case of the 70B variant. It is remarkable that we observe these gains when performing evaluation in English, even if the continued training is done in Basque, which implies that there is knowledge transfer from Basque into English. This challenges the conventional wisdom that adding more languages hurts English performance, a phenomenon known as the _curse of multilinguality_[Conneau et al., 2020, Pfeiffer et al., 2022, Chang et al., 2024]. To the best of our knowledge, this is the first solid evidence of knowledge being transferred from a low-resource to a high-resource language.

Nevertheless, we observe the opposite effect on the global subset, where the continued training in Basque hurts performance. The degradation is relatively small for the 13B and 70B models, but more notable for the 7B model. This suggests that training on Basque data improves English performance on subjects related to Basque culture, while hurting performance on more general topics. Given that prior work mostly evaluated on global subjects, this led to the generally accepted conclusion that training on other languages harms English. We show that this conclusion does not necessarily show the full picture, since models have barely been evaluated on local topics, and their behavior there can be fundamentally different.

### Comparison of English and Basque results

All of our results so far correspond to the English version of BertaQA. In this section, we focus on the Basque version instead. Recall that the English and Basque versions are parallel (i.e. they consist of the exact same questions in different languages), so the gap in performance between the two variants reflects how effective LLMs are at leveraging the same knowledge in each language.

As shown in Table 4, the vast majority of models obtain worse results in Basque, both on local and global topics. This is expected, as these models were primarily trained in English. Despite this, many models remain competitive on the global subset, demonstrating that many questions can be answered even with limited knowledge of Basque.

The only exception is the extension of Llama 2 with continued training in Basque (_Llama 2 + eu train_). For this model, the best local results are obtained in Basque, whereas the best global results are obtained in English. This implies that the previously observed knowledge transfer between Basque and English (Section 4.2) is not perfect: while the continued training in Basque did improve local performance in English, the model performs even better in Basque. Similarly, the global knowledge coming from Llama 2 does not transfer completely to Basque. This suggests that LLMs fail to encode knowledge in a completely language-agnostic manner, and tend to perform better when queried in the original language that they acquired the relevant knowledge in.

### Translate-test and self-translate

Translating the test data into English is a popular approach to performing cross-lingual learning in low-resource languages [1]. In this section, we compare how existing translation-based approaches behave on local and global topics. Specifically, we experiment with two methods: _translate-test_, where the Basque input is translated into English using an external machine translation system (NLLB-200; [10]), and _self-translate_, where the LLM itself produces the English translation in a few-shot fashion [1].

As shown in Table 5, translation-based approaches tend to be more effective for global questions. The best example is Gemma, for which both translate-test and self-translate improve performance on the global subset, while harming performance on the local subset. For Llama 2, translating into English is almost always helpful, which is not surprising as this model is not versed in Basque. However, the improvements are substantially larger for global questions. In contrast, translation approaches are generally harmful for the extension of Llama 2 with continued training in Basque although, once again, the degradation is smaller for global questions. Together, this suggests that the positive results for translate-test and self-translate in prior work might be inflated by the fact that their evaluation was limited to global topics.

### Error Analysis

As we stated previously, some local questions are still challenging even for the best models. To get more insights of the nature of these questions, we do a qualitative analysis of of 50 local questions in English that the best models (GPT 4 Turbo and Claude 3 Opus) answered incorrectly. As expected, most questions in this sample are of medium or hard difficulty, and fall in the most challenging

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Variant** & **Local** & **Global** & \(\Delta\) \\ \hline Random & N/A & 33.33 & 33.33 & 0.00 \\ \hline \multirow{4}{*}{GPT} & 3.5 Turbo & 47.25 (-7.83) & 66.22 (-16.18) & **18.97** \\  & 4 Turbo & 62.94 (-6.94) & 85.91 (-5.52) & 22.97 \\  & 4 Turbo & **69.46** (-2.71) & **89.21** (-2.47) & 19.75 \\ \hline \multirow{4}{*}{Claude 3} & Haiku & 58.21 (-0.50) & 79.85 (-4.31) & 21.64 \\  & Sonnet & 56.13 (-2.20) & 83.24 (-3.17) & 27.11 \\  & Opus & **71.32** (-0.59) & **90.89** (-0.96) & **19.57** \\ \hline \multirow{4}{*}{Llama 2} & 7B & 34.90 (-6.64) & 37.08 (-27.26) & **2.18** \\  & 13B & 34.09 (-9.52) & 43.77 (-26.59) & 9.68 \\  & 70B & **37.39** (-11.76) & **54.22** (-23.46) & 16.83 \\ \hline \multirow{4}{*}{Llama 2} & 7B & 49.45 (+1.73) & 50.79 (-2.47) & **1.34** \\  & 13B & 60.24 (+3.64) & 65.47 (-2.00) & 5.23 \\  & 70B & **64.85** (+2.24) & **72.24** (-1.38) & 7.39 \\ \hline \multirow{4}{*}{Llama 3} & 8B & 42.60 (-7.78) & 63.09 (-13.54) & **20.49** \\  & 70B & **57.40** (-2.16) & **82.15** (-2.59) & 24.75 \\ \hline \multirow{4}{*}{Qwen 1.5} & 7B & 35.96 (-6.55) & 46.15 (-25.30) & **10.19** \\  & 14B & 37.31 (-7.36) & 53.39 (-22.53) & 16.08 \\  & 72B & **42.77** (-11.93) & **63.25** (-20.74) & 20.48 \\ \hline \multirow{4}{*}{Yi} & 6B & 37.94 (-10.32) & 46.45 (-22.99) & **8.51** \\  & 9B & 38.20 (-13.79) & 49.21 (-21.70) & 11.01 \\  & 34B & **41.03** (-6.31) & **60.41** (-26.75) & 19.38 \\ \hline \multirow{4}{*}{Mistral} & 7B & 37.18 (-5.67) & 51.17 (-25.79) & **13.99** \\  & 47B & **43.61** (-13.03) & **61.08** (-23.20) & 17.47 \\ \hline \multirow{2}{*}{Gemma} & 7B & **41.84** (-3.85) & **65.89** (-10.53) & **24.05** \\ \hline
**Average** & N/A & 47.92 (-5.64) & 63.53 (-14.41) & 15.61 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for the Basque version of BertaQA. The \(\Delta\) column shows the difference between local and global results. Numbers in parentheses show the differences with the English results. Best results and smallest \(\Delta\) differences are in bold.

categories. None of the questions can be answered by common sense, and distractors are generally challenging.

Next, we try to find the correct answer of these questions by searching the web, to measure how difficult they can be. We include half of the examples in Table 6, the rest can be found in Appendix G. First, we found the correct answer relatively easily in 24 of the questions. Next, in 20 questions, finding the correct answer was more challenging, requiring multiple web searches, or searching the web in Basque. Six of these answers where trickier to find in English, often leading to no answer or even incorrect answers. Finally, in 16 of the questions we were unable to find the correct answer. In half of the questions, searching the web led to the wrong answer. Some of these questions involve temporal variations, the correct answer has changed in time. There were also some questions where we found no answer, such as the ones including negation.

## 5 Related Work

Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI (Conneau et al., 2018), XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2019) and Belebele (Bandarkar et al., 2023). This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the culture of the original dataset.

Recently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU (Hendrycks et al., 2020) datasets have been created for Chinese (Li et al., 2023), Korean (Son et al., 2024), Indonesian (Koto et al., 2023) and Arabic (Koto et al., 2024). Other examples of language-specific evaluation benchmarks include C-Eval for Chinese (Huang et al., 2024), HAE-RAE Bench for Korean (Son et al., 2023), COPAL-ID for Indonesian (Wibowo et al., 2023) and RoCulturaBench for Romanian (Masala et al., 2024). Finally, Etxaniz et al. (2024) introduces 4 native Basque multiple-choice evaluation datasets that include local questions.

Another relevant benchmark is SeaEval (Wang et al., 2023), which introduces 4 datasets for multi-cultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Size** & **Method** & **Local** & **Global** \\ \hline \multirow{6}{*}{Llama 2} & \multirow{2}{*}{7B} & Translate-test & **37.44** (+2.54) & **55.35** (+18.27) \\  & & Self-translate & 33.80 (-1.10) & 38.71 (+1.63) \\ \cline{2-5}  & \multirow{2}{*}{13B} & Translate-test & **37.69** (+3.60) & **62.50** (+18.73) \\  & & Self-translate & 34.81 (+0.72) & 46.11 (+2.34) \\ \cline{2-5}  & \multirow{2}{*}{70B} & Translate-test & **42.68** (+5.29) & **71.03** (+16.81) \\  & & Self-translate & 39.85 (+2.46) & 55.23 (+1.01) \\ \hline \multirow{6}{*}{Llama 2 + _eu train_} & \multirow{2}{*}{7B} & Translate-test & 35.79 (-13.66) & 44.27 (-6.52) \\  & & Self-translate & **44.37** (-5.08) & **50.04** (-0.75) \\ \cline{1-1} \cline{2-5}  & \multirow{2}{*}{13B} & Translate-test & 41.79 (-18.45) & 59.36 (-6.11) \\ \cline{1-1}  & & Self-translate & **56.13** (-4.11) & **65.55** (+0.08) \\ \cline{1-1} \cline{2-5}  & \multirow{2}{*}{70B} & Translate-test & 46.28 (-18.57) & 65.47 (-6.77) \\ \cline{1-1}  & & Self-translate & **60.15** (-4.70) & **70.48** (-1.76) \\ \hline \multirow{2}{*}{Gamma} & \multirow{2}{*}{7B} & Translate-test & **41.67** (-0.17) & **69.19** (+3.30) \\ \cline{1-1}  & & Self-translate & **41.67** (-0.17) & 67.68 (+1.79) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results for translate-test and self-translate settings. Numbers in parentheses show the difference with direct inference in Basque. Best results are in bold.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Category & Diff. & Question & Candidate 0 & Candidate 1 & Candidate 2 & Ans. & Web \\ \hline Basque and & 1 & What does the “Kamel” magazine specialize in? & Bertsolarism & Basque culture & The life of the & 1 & diff. \\  & & & & in the past and & Carmeliés & & \\ Geography & 3 & Which of these towns does not have a common boundary with Elcorrio? & Zaldibar & Bergara & Matiena & 2 & easy \\ Geography & 3 & Which of these districts is not in the Pettura area of Souke? & Etxarri & Garruze & Sarricotapea & 1 & no \\ Sports and & 2 & What’s Aritz Aranburu’s birthplace? & Zarantuz & Orio & Getaria & 2 & easy \\ Basque and & 2 & Which Basque dialect is spoken in Eibar? & The Lower Deba dialect & The Central dialect & The Western dialect & 2 & easy \\ Basque and & 2 & Which of these three was the Head of a Literature & Department in the Chartered Provincial Council of Gipuzkoa? & Terea & Joan Mari & Xabier Late & 2 & diff. \\ Cinema and & 2 & The stories of how many young people is the “Hasberrika” TV show about? & Ten & Seven & Five & 0 & diff. \\ Geography & 1 & Which is the longest river that flows into the sea in the Basque Country? & The Nervion & The Adour & The Ebro & 1 & diff. \\ Geography & 2 & Which of these is the least populated and History & The canton of & The canton of & The canton of & 1 & diff. \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 6: First 25 error analysis examples annotated by difficulty of web search.

Analysis of the cultural bias of LLMs has attracted some interest in recent years. Havaldar et al. (2023) concluded that multilingual models are not multicultural, whereas Tao et al. (2023) found that GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and Protestant European countries. In a similar vein, Naous et al. (2023) also found that multilingual and Arabic monolingual LMs exhibit bias towards Western culture.

According to Liu et al. (2024) translating into English can improve the performance of English-centric LLMs on most multilingual tasks. However, for culturally related tasks requiring deeper language understanding, prompting in the native language proves to be more effective since it can capture the nuances related to culture and language. This aligns with our findings: Latxa (dubbed "+ eu train" in the experimental sections) performs better in Basque for local topics, and better in English for global topics. On the other hand, AlKhamissi et al. (2024) found that these models exhibit a higher degree of cultural alignment when they are prompted with the predominant language of the culture, and have been pre-trained on the main language of the culture. In our case, empirical results also show that pretraining in Basque improves Basque culture knowledge, and prompting in Basque leads to better results than English.

The study of LLMs from a cultural perspective is challenging. Adilazaudra et al. (2024) observed, after a survey of 39 recent papers, that none of the studies define "culture", which is a complex, multifaceted concept. Instead, they probe models on some datasets which represent certain aspects of "culture", leaving other aspects untested. Ramesh et al. (2023) argue that the vast array of cultures and languages worldwide makes it impractical to create datasets that cover all. As a result, they believe that identifying and addressing biases should move away from relying only on datasets that have limited reach and are not adaptable to every language and culture.

Despite the extensive related work on the topic, our new benchmark is unique because it is natively created, provides a professionally translated English version, and distinguishes between local and global questions. Other datasets may include local questions, but they lack specific annotations to separate them from global ones. This distinction in our dataset enables more precise experiments to analyze the limitations of models.

## 6 Conclusion and Future Work

Most existing NLP benchmarks are limited to anglocentric or global subjects. So as to understand how LLMs perform on subjects that are idiosyncratic to other cultures, we introduce BertaQA, a trivia dataset comprising a local subset with questions about the Basque culture, and a global subset with questions of broader interest. Our results show that state-of-the-art models struggle with local knowledge, despite excelling on global subjects. In addition, we find that some prior findings need to be reconsidered when reassessed on local topics. In particular, we find that continued pretraining can transfer local knowledge from Basque into English, challenging the conventional wisdom that training on low-resource languages harms high-resource languages. In addition, we show that translation-based techniques like translate-test and self-translate are more effective on global questions, suggesting that results in prior work were inflated. Given that we often observe diverging trends in local and global questions, we believe that it is critical to cover them both when evaluating LLMs in the future.

While our work is a first step in this direction, it also comes with some limitations, which we would like to address in future work. More concretely, the local subset of our benchmark is limited to questions about the Basque culture. While we expect that the general trends we observe would also apply to other minority cultures, we believe that it would be valuable to build similar datasets covering other cultures. This is generally more challenging than developing benchmarks about global subjects, as it usually requires being part of or engaging with the relevant communities. We hope that our work prompts to reconsider how LLMs are evaluated more broadly, and motivates the creation of similar datasets for other minority cultures. Besides, the English version of our dataset was created through professional translation, which could lead to _translationese_ and other translation artifacts (Artetxe et al., 2020). This typically occurs in the opposite direction, as most datasets are translated from English into other languages. In the future, we would like to analyze if this has any impact in our results, and explore authoring questions about minority cultures in English rather than translating from their respective languages.

## Acknowledgements

Julen is funded by a PhD grant from the Basque Government (PRE_2023\(2\)0060). This work is partially supported by projects DeepR3 TED2021-130295B-C31, and DeepKnowledge PID2021-12777OB-C21 funded by MCIN/AEI/10.13039/501100011033 and European Union NextGeneration EU/PRTR, as well as and the Basque Government (IXA excellence research group IT1570-22, IKERGAITU 11:4711:23:410:23/0808) and the Spanish Ministry for Digital Transformation and of Civil Service, and the EU-funded NextGenerationEU Recovery, Transformation and Resilience Plan (ILENIA project, 2022/TL22/00215335).

## References

* A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov (2020)Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 8440-8451. External Links: Document, Link Cited by: SS1.
* M. F. Adilazuarda, S. Mukherjee, P. Lavania, S. Singh, A. Dwivedi, A. F. Aji, J. O'Neill, A. Modi, and M. Choudhury (2024)Towards measuring and modeling "culture" in llms: a survey. External Links: 2004.00490 Cited by: SS1.
* K. Ahuja, R. Hada, M. Ochieng, P. Jain, H. Diddee, S. Maina, T. Ganu, S. Segal, M. Axmed, K. Bali, et al. (2023)Mega: multilingual evaluation of generative ai. arXiv. External Links: 2003.04477 Cited by: SS1.
* B. AlKhamissi, M. ElNokrashy, M. AlKhamissi, and M. Diab (2024)Investigating cultural alignment of large language models. arXiv preprint arXiv:2402.13231. Cited by: SS1.
* M. Artetxe, S. Ruder, and D. Yogatama (2019)On the cross-lingual transferability of monolingual representations. arXiv preprint arXiv:1910.11856. Cited by: SS1.
* M. Artetxe, G. Labaka, and E. Agirre (2020)Translation artifacts in cross-lingual transfer learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, pp. 7674-7684. External Links: Document, Link Cited by: SS1.
* J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, Y. Fan, W. Ge, Y. Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprint arXiv:2309.16609. Cited by: SS1.
* L. Bandarkar, D. Liang, B. Muller, M. Artetxe, S. N. Shukla, D. Husa, N. Goyal, A. Krishnan, L. Zettlemoyer, and M. Khabsa (2023)The belebele benchmark: a parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv:2308.16884. Cited by: SS1.
* T. A. Chang, C. Arnett, Z. Tu, and B. Bergen (2024)When is multilinguality a curse? language modeling for 252 high- and low-resource languages. External Links: 2004.0000 Cited by: SS1.
* A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman, H. Schwenk, and V. Stoyanov (2018)Xnli: evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053. Cited by: SS1.
* A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov (2020)Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 8440-8451. External Links: Document, Link Cited by: SS1.
* M. R. Costa-jussa, J. Cross, O. Celebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al. (2022)No language left behind: scaling human-centered machine translation. arXiv. External Links: 2002.00002 Cited by: SS1.
* J. Etxaniz, G. Azzkune, A. Soroa, O. L. de Lacalle, and M. Artetxe (2023)Do multilingual language models think better in english?. arXiv preprint arXiv:2308.01223. Cited by: SS1.
* J. Etxaniz, O. Sainz, N. Perez, I. Aldabe, G. Rigau, E. Agirre, A. Ormazabal, M. Artetxe, and A. Soroa (2024)Latxa: an open language model and evaluation suite for basque. External Links: 2402.13231 Cited by: SS1.
* L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac'h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou (2023)A framework for few-shot language model evaluation. External Links: 2308.01223 Cited by: SS1.
* S. Havaldar, S. Rai, B. Singhal, L. L. S. C. Guntuku, and L. Ungar (2023)Multilingual language models are not multicultural: a case study in emotion. arXiv preprint arXiv:2307.01370. Cited by: SS1.

D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Huang et al. [2024] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, Y. Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Jiang et al. [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* Koto et al. [2023] F. Koto, N. Aisyah, H. Li, and T. Baldwin. Large language models only pass primary school exams in indonesia: A comprehensive test on indommlu. _arXiv preprint arXiv:2310.04928_, 2023.
* Koto et al. [2024] F. Koto, H. Li, S. Shatnawi, J. Doughman, A. B. Sadallah, A. Alraeesi, K. Almubarak, Z. Alyafeai, N. Sengupta, S. Shehata, et al. Arabicmmlu: Assessing massive multitask language understanding in arabic. _arXiv preprint arXiv:2402.12840_, 2024.
* Lewis et al. [2019] P. Lewis, B. Oguz, R. Rinott, S. Riedel, and H. Schwenk. Mlqa: Evaluating cross-lingual extractive question answering. _arXiv preprint arXiv:1910.07475_, 2019.
* Li et al. [2023] H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. _arXiv preprint arXiv:2306.09212_, 2023.
* Lin et al. [2022] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhosale, J. Du, R. Pasunuru, S. Shleifer, P. S. Koura, V. Chaudhary, B. O'Horo, J. Wang, L. Zettlemoyer, Z. Kozareva, M. Diab, V. Stoyanov, and X. Li. Few-shot learning with multilingual generative language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9019-9052, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.616.
* Liu et al. [2024] C. Liu, W. Zhang, Y. Zhao, A. T. Luu, and L. Bing. Is translation all you need? a study on solving multilingual tasks with large language models. _arXiv preprint arXiv:2403.10258_, 2024.
* Masala et al. [2024] M. Masala, D. C. Ilie-Ablachim, A. Dima, D. Corlatescu, M. Zavelca, O. Olaru, S. Terian-Dan, A. Terian-Dan, M. Leordeanu, H. Velici, et al. " vorbesti romaneste?" a recipe to train powerful romanian llms with english instructions. _arXiv e-prints_, pages arXiv-2406, 2024.
* Meta [2024] Meta, 2024. URL https://llama.meta.com/llama3/.
* Naous et al. [2023] T. Naous, M. J. Ryan, A. Ritter, and W. Xu. Having beer after prayer? measuring cultural bias in large language models. _arXiv preprint arXiv:2305.14456_, 2023.
* OpenAI et al. [2020] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Gerstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschelle, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, Lukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Lukasz Kondraciuk, A. Kondrichi, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan,J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Melly, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O'Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayveriya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024.
* Pfeiffer et al. (2022) J. Pfeiffer, N. Goyal, X. Lin, X. Li, J. Cross, S. Riedel, and M. Artetxe. Lifting the curse of multilinguality by pre-training modular transformers. In M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3479-3495, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.255. URL https://aclanthology.org/2022.naacl-main.255.
* Ramesh et al. (2023) K. Ramesh, S. Sitaram, and M. Choudhury. Fairness in language models beyond english: Gaps and challenges. _arXiv preprint arXiv:2302.12578_, 2023.
* Scao et al. (2023) T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv_, 2023.
* Son et al. (2023) G. Son, H. Lee, S. Kim, J. Lee, J. W. Yeom, J. Jung, J. W. Kim, and S. Kim. Hae-rae bench: Evaluation of korean knowledge in language models. _arXiv preprint arXiv:2309.02706_, 2023.
* Son et al. (2024) G. Son, H. Lee, S. Kim, S. Kim, N. Muennighoff, T. Choi, C. Park, K. M. Yoo, and S. Biderman. Kmmlu: Measuring massive multitask language understanding in korean. _arXiv preprint arXiv:2402.11548_, 2024.
* Tao et al. (2023) Y. Tao, O. Viberg, R. S. Baker, and R. F. Kizilccc. Auditing and mitigating cultural bias in llms. _arXiv preprint arXiv:2311.14096_, 2023.
* Team and Deepmind (2024) G. Team and G. Deepmind. Gemma: Open models based on gemini research and technology, 2024. URL https://deepmind.com/research/publications/gemma-open-models-based-gemini-research-and-technology.
* Team et al. (2023) G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Touvron et al. (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv_, 2023.
* Wang et al. (2023) B. Wang, Z. Liu, X. Huang, F. Jiao, Y. Ding, A. T. Aw, and N. F. Chen. Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. _arXiv preprint arXiv:2309.04766_, 2023.
* Wibowo et al. (2023) H. A. Wibowo, E. H. Fuadi, M. N. Nityasya, R. E. Prasojo, and A. F. Aji. Copal-id: Indonesian language reasoning with local culture and nuances. _arXiv preprint arXiv:2311.01012_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] Included in Section 6 3. Did you discuss any potential negative societal impacts of your work? [No] We see no negative societal impacts 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? No training is done, only evaluations 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] The evaluation is deterministic 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Included in Appendix D
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [No] We include references 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] There is no personally identifiable information or offensive content
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Statistics

We show detailed statistics of each category, group and difficulty in Table 7.

## Appendix B Basque Examples

The same examples that were included in English in Table 1 are included in Basque in Table 8.

## Appendix C Prompts

Regarding the prompts, we used the Basque prompts described in Etxaniz et al. (2024) for multiple choice questions, which were manually translated for the English experiments (see Table 9. The answer choices were single letters (A, B, C) and the answer index was used as the index of the correct answer.

## Appendix D Compute

The experiments we performed are not very compute-intensive. We performed all the experiments using A100 80GB GPUs in our internal cluster. The largest models require using at least 3 GPUs. Evaluating each model took a few minutes, so the total compute is of a few GPU hours.

## Appendix E Results by Category

We show that previous local and global results are consistent across categories in Tables 10 and 11. The large difference between local and global is maintained across all models and categories. When comparing Llama 2 and Latxa, we see that previous results are consistent across all categories. That is, Latxa is better at local questions and Llama 2 is better at global questions. However, we see that the differences vary significantly depending on the models and categories.

For example, Latxa obtains very good results in the Basque and Literature category, both in local and global questions. In local questions, it is on par with the best commercial models. These results

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline  & & & \multicolumn{3}{c}{**Difflulty**} & \multicolumn{3}{c}{**English chars**} & \multicolumn{3}{c}{**Basque chars**} \\ \cline{2-10}
**Category** & **Group** & **Items** & **Easy** & **Medium** & **Hard** & **Question** & **Candidates** & **Question** & **Candidates** \\ \hline Basque and Literature & Local & 305 & 90 & 103 & 112 & 55.9 & 16.9 & 51.0 & 16.7 \\ Basque and Literature & Global & 310 & 91 & 108 & 111 & 53.3 & 15.7 & 51.1 & 16.0 \\ \hline Geography and History & Local & 300 & 110 & 110 & 80 & 48.9 & 12.8 & 44.4 & 12.1 \\ Geography and History & Global & 300 & 110 & 110 & 80 & 43.0 & 10.3 & 43.7 & 11.0 \\ \hline Society and Traditions & Local & 289 & 103 & 108 & 78 & 60.2 & 16.1 & 53.7 & 14.9 \\ Society and Traditions & Global & 298 & 110 & 109 & 79 & 51.1 & 18.0 & 50.4 & 18.6 \\ \hline Sports and Leisure & Local & 296 & 107 & 109 & 80 & 47.5 & 11.7 & 42.6 & 10.7 \\ Sports and Leisure & Global & 303 & 113 & 110 & 80 & 43.3 & 10.3 & 43.0 & 10.5 \\ \hline Culture and Art & Local & 295 & 105 & 110 & 80 & 43.5 & 11.5 & 39.5 & 10.2 \\ Culture and Art & Global & 286 & 98 & 108 & 80 & 40.7 & 9.1 & 38.1 & 9.6 \\ \hline Music and Dance & Local & 289 & 107 & 102 & 80 & 49.0 & 12.5 & 45.8 & 13.0 \\ Music and Dance & Global & 300 & 110 & 110 & 80 & 43.4 & 10.8 & 41.6 & 11.6 \\ \hline Science and Technology & Local & 292 & 105 & 108 & 79 & 63.3 & 12.5 & 60.0 & 12.7 \\ Science and Technology & Global & 296 & 108 & 109 & 79 & 53.0 & 11.0 & 54.0 & 11.3 \\ \hline Cinema and Shows & Local & 298 & 110 & 109 & 79 & 67.1 & 16.3 & 65.2 & 16.7 \\ Cinema and Shows & Global & 299 & 109 & 110 & 80 & 55.8 & 12.4 & 59.3 & 13.5 \\ \hline All & Local & 2364 & 837 & 859 & 668 & 54.4 & 13.8 & 49.0 & 13.4 \\ All & Global & 2392 & 849 & 874 & 669 & 48.0 & 12.2 & 47.7 & 12.8 \\ \hline All & All & 4756 & 1686 & 1733 & 1337 & 51.2 & 13.0 & 49.0 & 13.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Statistics by Category and Group. We show the number of total items and items per difficulty. We also report the average number of characters of questions and candidate answers in English and Basque.

match previous results in Latxa [Etcaniz et al., 2024], where Latxa surpasses the best commercial models on language proficiency and trivia questions in the Language and Literature categories.

There are more categories where Latxa is on par with the best models on local topics, such as Music and Dance and Cinema and Shows. These categories are also the ones where the best models struggle the most on local questions, and the difference with global topics is the largest.

## Appendix F Results by Difficulty

Results are also consistent across difficulty levels, as shown in Tables 10 and 13. All models obtain worse results in local questions at all difficulty levels. There is a clear difference between difficulties, with the biggest drop in performance happening from the easy to medium difficulty. Commercial models have a bigger drop in scores on local questions than on global questions for medium and hard

\begin{table}
\begin{tabular}{c c c} \hline \hline  & **Local Questions** & **Global Questions** \\ \hline \multirow{3}{*}{**Basque and Literature**} & Zertaz aí da “Karmel” aldizkaria? & Elebeerri hauetako zeiten ez da agertzen tissao? \\  & a) Bertsolaritzaz & **a) “Tom Sawyer-ren abeurtrak”** \\  & **b) Lehengo eta gauarko euskal kulturaz & b) “Moby Dick” \\  & c) Karmeldarren bizimouduz & c) “Alkcorraren uhartea” \\ \hline \multirow{3}{*}{**Geography and History**} & Non dago Atxondo? & Nor kartzelaratu zuten 1964an? \\  & **a) Bikzian** & **a) Nelson Mandela** \\  & b) Gipuzkoan & b) Muma Abu Jamal \\  & c) Nafarrona & c) Charles Ghankay \\ \hline \multirow{3}{*}{**Society**} & Hauteako zein dago Eusko Jaurlaritzaren menep? & Zein da gchien enabiltzen dugun energia mota? \\  & a) IKA & **a) Petrolioa** \\  & b) AEK & b) Hidroektrikoa \\  & **c) HABE** & c) Energia nuklearra \\ \hline \multirow{3}{*}{**Sports and**} & Non jaio zen Julian Retegi? & Nork irabazi ditu munduko futbol-txapelketa \\  & **and** & gehien? \\  & a) Areson & a) Argentinak \\  & **b) Erastunen** & b) Alemaniak \\  & c) Erasston & **c) Brasile** \\ \hline \multirow{3}{*}{**Culture and**} & Nortraek cariak znten Baionako Gaztelu Berria? & Noiz ircki zuten Titanic Belfast Museoa? \\  & **a) Ingelesek** & **a) 2012an** \\  & b) Frantseek & b) 2005ean \\  & c) Espainolek & c) 2002an \\ \hline \multirow{3}{*}{**Music and Dance**} & Nong dantza a jatorriz “dantzari” izeneko dantz? & Nork iidari zeu James Bond serieko soinu-banda? \\  & a) Busturialideakoa & a) John Barry-k \\  & a) BENTartieriackoa & b) Henry Mancini-k \\  & **c) Durangaldekoa** & c) John Williams-ek \\ \hline \multirow{3}{*}{**Science and Technology**} & Bizkaio zer udalerri dago lottuta dinamitarkin? & Zein da daltonisomaeri zen zientifikao? \\  & a) Leioa & a) Chondrostoma \\  & **b) Galdakao** & b) Kromoosia \\  & c) Erandio & **c) Diskromatopsia** \\ \hline \multirow{3}{*}{**Cinema and Shows**} & Nola du izena Bernardo Atxagaren “Obabakoak” eleb serrian oinarritutako filmak? & Some like it hot filmean (gaztelaniza, “Con faldas y a lo loco”), zer instrumentu jotzen zuen Marilyn Monroek? \\  & a) “Obabakoak” & a) APA \\  & **b) “Obabao”** & b) Didegrido \\ \hline \hline \end{tabular}
\end{table}
Table 8: Basque examples of local and global questions in each category.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Basque** & **English** \\ \hline Galdera: {question} & Question: {question} \\ A. {candidates[0]} & A. {candidates[0]} \\ B. {candidates[1]} & B. {candidates[1]} \\ C. {candidates[2]} & C. {candidates[2]} \\ Erantzuna: {answer} & Answer: {answer} \\ \hline \hline \end{tabular}
\end{table}
Table 9: Prompts

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l l l l l l} \hline \hline  & & \multicolumn{3}{c}{**Basque and Liter-**} & \multicolumn{3}{c}{**Geography and**} & \multicolumn{3}{c}{**Society**} & \multicolumn{3}{c}{**Sports and**} & \multicolumn{3}{c}{**Culture and Art**} & \multicolumn{3}{c}{**Music and Science**} & \multicolumn{3}{c}{**Cineine and**}{**} & \multicolumn{3}{c}{**Cine and**} \\  & & \multicolumn{3}{c}{**and Liter-**} & \multicolumn{3}{c}{**and**} & \multicolumn{3}{c}{**and**} & \multicolumn{3}{c}{**and**} & \multicolumn{3}{c}{**and**} & \multicolumn{3}{c}{**and Art**} & \multicolumn{3}{c}{**Dance**} & \multicolumn{3}{c}{**and Tech-**} & \multicolumn{3}{c}{**and**} \\  & & \multicolumn{3}{c}{**and**} & \multicolumn{3}{c}{**History**} & \multicolumn{3}{c}{**Traditions**} & \multicolumn{3}{c}{**Leisure**} & \multicolumn{3}{c}{**indology**} & \multicolumn{3}{c}{**Shows**} \\ \cline{3-19}
questions. For open models, results are more varied, but the relative drop is also generally bigger on local questions. On the most difficult local questions, some models get close to random chance.

## Appendix G Extended Error Analysis

We include the remaining 25 examples of error analysis in Table 14.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & & \multicolumn{2}{c}{**Easy**} & \multicolumn{2}{c}{**Medium**} & \multicolumn{2}{c}{**Hard**} \\ \cline{3-8}
**Model** & **Variant** & **Loc** & **Glo** & **Loc** & **Glo** & **Loc** & **Glo** \\ \hline \multirow{3}{*}{GPT} & 3.5 Turbo & 67.5 & 89.4 & 51.2 & 82.5 & 44.5 & 73.4 \\  & 4 & 78.3 & 94.4 & 67.8 & 91.7 & 62.1 & 87.4 \\  & 4 Turbo & 80.1 & 94.7 & 70.2 & 92.0 & 64.8 & 87.4 \\ \hline \multirow{3}{*}{Claude 3} & Haiku & 66.3 & 91.8 & 57.3 & 82.6 & 51.1 & 76.5 \\  & Sonnet & 66.3 & 91.8 & 54.4 & 85.7 & 53.4 & 80.6 \\  & Opus & 79.3 & 95.1 & 69.0 & 91.3 & 66.3 & 88.5 \\ \hline \multirow{3}{*}{Llama 2} & 7B & 44.9 & 72.4 & 41.0 & 63.5 & 38.0 & 55.2 \\  & 13B & 48.2 & 79.7 & 44.4 & 69.5 & 37.0 & 59.6 \\  & 70B & 57.5 & 86.2 & 47.6 & 77.7 & 40.7 & 66.8 \\ \hline \multirow{3}{*}{Latxa} & 7B & 52.7 & 60.3 & 46.0 & 52.6 & 43.7 & 45.1 \\  & 13B & 63.8 & 75.5 & 55.8 & 65.9 & 48.7 & 59.3 \\  & 70B & 70.7 & 84.2 & 62.1 & 72.0 & 53.1 & 62.3 \\ \hline \multirow{3}{*}{Llama 3} & 7B & 46.8 & 67.8 & 43.4 & 64.1 & 36.2 & 55.8 \\  & 70B & 63.6 & 88.8 & 55.8 & 83.2 & 51.8 & 72.4 \\ \hline \multirow{3}{*}{Qwen 1.5} & 7B & 46.7 & 80.6 & 40.9 & 71.1 & 39.4 & 60.4 \\  & 14B & 51.6 & 85.0 & 43.1 & 76.8 & 38.0 & 63.2 \\  & 72B & 63.8 & 90.8 & 51.0 & 83.4 & 48.1 & 76.1 \\ \hline \multirow{3}{*}{Yi} & 6B & 50.2 & 81.9 & 43.0 & 73.2 & 38.5 & 62.2 \\  & 9B & 51.3 & 83.9 & 42.1 & 74.8 & 36.8 & 64.0 \\  & 34B & 62.5 & 89.6 & 51.3 & 82.8 & 47.0 & 77.0 \\ \hline \multirow{3}{*}{Mistral} & 7B & 55.3 & 82.3 & 46.1 & 74.4 & 39.5 & 63.5 \\  & 47B & 66.7 & 90.5 & 55.2 & 82.2 & 48.7 & 73.8 \\ \hline \multirow{2}{*}{Gamma} & 7B & 53.2 & 84.7 & 43.0 & 76.9 & 39.8 & 65.3 \\ \hline
**Average** & N/A & 60.3 & 84.4 & 51.4 & 77.0 & 46.4 & 68.5 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of models in English by difficulty and group.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & & \multicolumn{2}{c}{**Easy**} & \multicolumn{2}{c}{**Medium**} & \multicolumn{2}{c}{**Hard**} \\ \cline{3-8}
**Model** & **Variant** & **Loc** & **Glo** & **Loc** & **Glo** & **Loc** & **Glo** \\ \hline \multirow{3}{*}{GPT} & 3.5 Turbo & 54.7 & 69.9 & 45.6 & 66.5 & 40.0 & 61.3 \\  & 4 Turbo & 68.0 & 89.5 & 62.1 & 85.6 & 57.8 & 81.8 \\  & 4 Turbo & 78.0 & 91.9 & 66.8 & 89.1 & 62.1 & 86.0 \\ \hline \multirow{3}{*}{Claude 3} & Haiku & 67.0 & 87.4 & 56.0 & 80.7 & 50.0 & 69.2 \\  & Sonnet & 61.8 & 88.3 & 53.2 & 82.5 & 52.8 & 77.7 \\  & Opus & 79.2 & 93.5 & 68.8 & 91.2 & 64.7 & 87.1 \\ \hline \multirow{3}{*}{Llama 2} & 7B & 37.0 & 36.8 & 33.4 & 37.1 & 34.1 & 37.5 \\  & 13B & 32.4 & 46.8 & 36.1 & 42.9 & 33.7 & 41.1 \\  & 70B & 39.2 & 55.6 & 38.5 & 55.0 & 33.7 & 51.4 \\ \hline \multirow{3}{*}{Latxa} & 7B & 56.5 & 55.0 & 48.3 & 51.6 & 42.1 & 44.4 \\  & 13B & 67.5 & 72.9 & 59.4 & 64.4 & 52.3 & 57.4 \\  & 70B & 75.8 & 82.3 & 64.6 & 70.8 & 51.5 & 61.3 \\ \hline \multirow{3}{*}{Llama 3} & 8B & 46.8 & 67.8 & 43.4 & 64.1 & 36.2 & 55.8 \\  & 70B & 63.6 & 88.8 & 55.8 & 83.2 & 51.8 & 72.4 \\ \hline \multirow{3}{*}{Qwen 1.5} & 7B & 37.3 & 47.4 & 35.4 & 46.3 & 35.0 & 44.4 \\  & 14B & 37.9 & 57.8 & 37.0 & 52.9 & 37.0 & 48.4 \\  & 72B & 46.5 & 64.0 & 42.7 & 65.5 & 38.2 & 59.5 \\ \hline \multirow{3}{*}{Yi} & 6B & 38.5 & 49.0 & 39.1 & 46.8 & 35.8 & 42.8 \\  & 9B & 41.6 & 52.4 & 37.4 & 47.9 & 35.0 & 46.8 \\  & 34B & 46.4 & 59.4 & 40.6 & 60.8 & 34.9 & 61.3 \\ \hline \multirow{3}{*}{Mistral} & 7B & 39.1 & 55.5 & 37.7 & 49.8 & 34.1 & 47.5 \\  & 47B & 48.0 & 64.4 & 40.6 & 62.0 & 41.9 & 55.6 \\ \hline \multirow{3}{*}{Gamma} & 7B & 45.3 & 72.4 & 40.3 & 66.4 & 39.5 & 57.0 \\ \hline \multirow{3}{*}{**Average**} & N/A & 52.5 & 67.3 & 47.1 & 63.6 & 43.2 & 58.6 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Results of models in English by difficulty and group.

[MISSING_PAGE_FAIL:21]