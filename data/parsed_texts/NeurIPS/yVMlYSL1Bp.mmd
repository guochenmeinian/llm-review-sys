# Diverse Shape Completion via Style Modulated Generative Adversarial Networks

 Wesley Khademi

Oregon State University

khademiw@oregonstate.edu

&Li Fuxin

Oregon State University

lif@oregonstate.edu

###### Abstract

Shape completion aims to recover the full 3D geometry of an object from a partial observation. This problem is inherently multi-modal since there can be many ways to plausibly complete the missing regions of a shape. Such diversity would be indicative of the underlying uncertainty of the shape and could be preferable for downstream tasks such as planning. In this paper, we propose a novel conditional generative adversarial network that can produce many diverse plausible completions of a partially observed point cloud. To enable our network to produce multiple completions for the same partial input, we introduce stochasticity into our network via style modulation. By extracting style codes from complete shapes during training, and learning a distribution over them, our style codes can explicitly carry shape category information leading to better completions. We further introduce diversity penalties and discriminators at multiple scales to prevent conditional mode collapse and to train without the need for multiple ground truth completions for each partial input. Evaluations across several synthetic and real datasets demonstrate that our method achieves significant improvements in respecting the partial observations while obtaining greater diversity in completions.

## 1 Introduction

With the rapid advancements in 3D sensing technologies, point clouds have emerged as a popular representation for capturing the geometry of real-world objects and scenes. Point clouds come from sensors such as LiDAR and depth cameras, and can find applications in various domains such as robotics, computer-aided design, augmented reality, and autonomous driving. However, the 3D geometry produced by such sensors is typically sparse, noisy, and incomplete, which hinders their effective utilization in many downstream tasks.

This motivates the task of 3D shape completion from a partially observed point cloud, which has seen significant research in the past few years [1, 2, 3, 4, 5, 6, 7]. Many early point cloud completion works mostly focus on generating a single completion that matches the ground truth in the training set, which does not take into account the potential uncertainty underlying the complete point cloud given the partial view. Ideally, an approach should correctly characterize such uncertainty - generating mostly similar completions when most of the object has been observed, and less similar completions when less of the object has been observed. A good characterization of uncertainty would be informative for downstream tasks such as planning or active perception to aim to reduce such uncertainty.

Figure 1: Given a partially observed point cloud (gray), our method is capable of producing many plausible completions (blue) of the missing regions.

The task of completing a 3D point cloud with the shape uncertainty in mind is called _multi-modal shape completion_[8; 9], which aims to generate diverse point cloud completions (not to be confused with multi-modality of the input, e.g. text+image). A basic idea is to utilize the diversity coming from generative models such as generative adversarial networks (GAN), where diversity, or the avoidance of _mode collapse_ to always generate the same output, has been studied extensively. However, early works [8; 9] often obtain diversity at a cost of poor fidelity to the partial observations due to their simplistic completion formulation that decodes from a single global latent vector. Alternatively, recent diffusion-based methods [10; 11; 12] and auto-regressive methods [13; 14] have shown greater generation capability, but suffer from slow inference time.

In this paper, we propose an approach to balance the diversity of the generated completions and fidelity to the input partial points. Our first novelty comes from the introduction of a _style encoder_ to encode the global shape information of complete objects. During training, the ground truth shapes are encoded with this style encoder so that the completions match the ground truth. However, multiple ground truth completions are not available for each partial input; therefore, only using the ground truth style is not enough to obtain diversity in completions. To overcome this, we randomly sample style codes to provide diversity in the other generated completions. Importantly, we discovered that **reducing** the capacity of the style encoder and adding noise to the encoded ground truth shape leads to improved diversity of the generated shapes. We believe this avoids the ground truth from encoding too much content, which may lead the model to overfit to only reconstructing ground truth shapes.

Besides the style encoder, we also take inspiration from recent work SeedFormer [7] to adopt a coarse-to-fine completion architecture. SeedFormer has shown high-quality shape completion capabilities with fast inference time, but only provides deterministic completions. In our work, we make changes to the layers of SeedFormer, making it more suitable for the multi-modal completion task. Additionally, we utilize discriminators at **multiple scales**, which enable training without multiple ground truth completions and significantly improves completion quality. We further introduce a multi-scale diversity penalty that operates in the feature space of our discriminators. This added regularization helps ensure different sampled style codes produce diverse completions.

With these improvements, we build a multi-modal point cloud completion algorithm that outperforms state-of-the-art in both the fidelity to the input partial point clouds as well as the diversity in the generated shapes. Our method is capable of fast inference speeds since it does not rely on any iterative procedure, making it suitable for real-time applications such as in robotics.

Our main contributions can be summarized as follows:

* We design a novel conditional GAN for the task of diverse shape completion that achieves greater diversity along with higher fidelity partial reconstruction and completion quality.
* We introduce a style-based seed generator that produces diverse coarse shape completions via style modulation, where style codes are learned from a distribution of complete shapes.
* We propose a multi-scale discriminator and diversity penalty for training our diverse shape completion framework without access to multiple ground truth completions per partial input.

## 2 Related work

### 3D shape generation

The goal of 3D shape generation is to learn a generative model that can capture the distribution of 3D shapes. In recent years, generative modeling frameworks have been investigated for shape generation using different 3D representations, including voxels, point clouds, meshes, and neural fields.

One of the most popular frameworks for 3D shape generation has been generative adversarial networks (GANs). Most works have explored point cloud-based GANs [15; 16; 17; 18; 19], while recent works have shown that GANs can be trained on neural representations [20; 21; 22; 23]. To avoid the training instability and mode collapse in GANs, variational autoencoders have been used for learning 3D shapes [24; 25; 26], while other works have made use of Normalizing Flows [27; 28; 29]. Recently, diverse shape generation has been demonstrated by learning denoising diffusion models on point clouds [30], latent representations [31; 32], or neural fields [33].

### Point cloud completion

Point cloud completion aims to recover the missing geometry of a shape while preserving the partially observed point cloud. PCN [1] was among the earliest deep learning-based methods that worked directly on point clouds using PointNet [34] for point cloud processing. Since then, other direct point cloud completion methods [35; 36; 4; 37; 6; 38; 7] have improved completion results by using local-to-global feature extractors and by producing completions through hierarchical decoding.

Recently, SeedFormer [7] has achieved state-of-the-art performance in point cloud completion. Their Patch Seeds representation has shown to be more effective than global shape representations due to carrying learned geometric features and explicit shape structure. Furthermore, their transformer-based upsampling layers enable reasoning about spatial relationships and aggregating local information in a coarse-to-fine manner, leading to improved recovery of fine geometric structures.

GAN-based completion networks have also been studied to enable point cloud completion learning in unpaired [39; 40; 41] or unsupervised [42] settings. To enhance completion quality, some works have leveraged adversarial training alongside explicit reconstruction losses [4; 43].

### Multimodal shape completion

Most point cloud completion models are deterministic despite the ill-posed nature of shape completion. To address this, Wu et al. [8] proposed a GAN framework that learns a stochastic generator, conditioned on a partial shape code and noise sample, to generate complete shape codes in the latent space of a pre-trained autoencoder. Arora et al. [9] attempt to mitigate the mode collapse present in [8] by using implicit maximum likelihood estimation. These methods can only represent coarse geometry and struggle to respect the partial input due to decoding from a global shape latent vector.

ShapeFormer [13] and AutoSDF [14] explore auto-regressive approaches for probabilistic shape completion. Both methods propose compact discrete 3D representations for shapes and learn an auto-regressive transformer to model the distribution of object completions on such representation. However, these methods have a costly sequential inference process and rely on voxelization and quantization steps, potentially resulting in a loss of geometric detail.

Zhou et al. [10] propose a conditional denoising diffusion model that directly operates on point clouds to produce diverse shape completions. Alternatively, DiffusionSDF [11] and SDFusion [12] first learn a compact latent representation of neural SDFs and then learn a diffusion model over this latent space. These methods suffer from slow inference times due to the iterative denoising procedure, while [11; 12] have an additional costly dense querying of the neural SDF for extracting a mesh.

### Diversity in GANs

Addressing diversity in GANs has also been extensively studied in the image domain. In particular, style-based generators have shown impressive capability in high-quality diverse image generation where several methods have been proposed for injecting style into generated images via adaptive instance normalization [44; 45; 46; 47] or weight modulation [46; 48]. In the conditional setting, diversity has been achieved by enforcing invertibility between output and latent codes [49] or by regularizing the generator to prevent mode collapse [50; 51; 52].

## 3 Method

In this section, we present our conditional GAN framework for diverse point cloud completion. The overall architecture of our method is shown in Figure 2.

Our generator is tasked with producing high-quality shape completions when conditioned on a partial point cloud. To accomplish this, we first introduce a new partial shape encoder which extracts features from the partial input. We then follow SeedFormer [7] and utilize a seed generator to first propose a sparse set of points that represent a coarse completion of a shape given the extracted partial features. The coarse completion is then passed through a series of upsampling layers that utilize transformers with local attention to further refine and upsample the coarse completion into a dense completion.

To obtain diversity in our completions, we propose a style-based seed generator that introduces stochasticity into our completion network at the coarsest level. Our style-based seed generatormodulates the partial shape information with style codes before producing a sparse set of candidate points, enabling diverse coarse shape completions that propagate to dense completions through upsampling layers. The style codes used in modulating partial shape information are learned from an object category's complete shapes via a style encoder. Finally, we introduce discriminators and diversity penalties at multiple scales to train our model to produce diverse high-quality completions without having access to multiple ground truth completions that correspond to a partial observation.

### Partial shape encoder

The goal of our partial encoder is to extract shape information in a local-to-global fashion, extracting local information that will be needed in the decoding stage to reconstruct fine geometric structures, while capturing global information needed to make sure a globally coherent shape is generated. An overview of the architecture for our proposed partial encoder is shown in Figure 2(a).

Our partial encoder takes in a partially observed point cloud \(X_{P}\) and first applies a MLP to obtain a set of point-wise features \(F_{0}\). To extract shape information in a local-to-global fashion, \(L\) consecutive downsampling blocks are applied to obtain a set of downsampled points \(X_{L}\) with local features \(F_{L}\). In each downsampling block, a grid downsampling operation is performed followed by a series of PointConv [53] layers for feature interpolation and aggregation. Following the downsampling blocks, a global representation of the partial shape is additionally extracted by an MLP followed by max pooling, producing partial latent vector \(f_{P}\).

### Style encoder

To produce multi-modal completions, we need to introduce randomness into our completion model. One approach is to draw noise from a Gaussian distribution and combine it with partial features during the decoding phase. Another option is to follow StyleGAN [46; 54] and transform noise samples through a non-linear mapping to a latent space \(\mathcal{W}\) before injecting them into the partial features. However, these methods rely on implicitly learning a connection between latent samples and shape information. Instead, we propose to learn style codes from an object category's distribution of complete shapes and sample these codes to introduce stochasticity in our completion model. Our style codes explicitly carry information about ground truth shapes, leading to higher quality and more diverse completions.

To do so, we leverage the set of complete shapes we have access to during training. We introduce an encoder \(E\) that maps a complete shape \(X\in\mathbb{R}^{N\times 3}\) to a global latent vector via a 4-layer MLP followed by max pooling. We opt for a simple architecture as we would like the encoder to capture high level information about the distribution of shapes rather than fine-grained geometric structure.

Instead of assuming we have complete shapes to extract style codes from at inference time, we learn a distribution over style codes that we can sample from. Specifically, we define our style encoder as a learned Gaussian distribution \(E_{S}(z|X)=\mathcal{N}(z|\mu(E(X)),\sigma(E(X)))\) by adding two fully connected layers, \(\mu\) and \(\sigma\), to the encoder \(E\) to predict the mean and standard deviation of a Gaussian to sample a style code from. Since our aim is to learn style codes that convey information about complete shapes that is useful for generating diverse completions, we train our style encoder with guidance from our

Figure 2: Overview of our diverse shape completion framework. A partial encoder is used to extract information from a partial point cloud. During training, a style encoder extracts style codes from complete point clouds, and at inference time style codes are randomly sampled from a normal distribution. Sampled style codes are injected into the partial information to produce diverse Patch Seeds in our style-based seed generator. The generated Patch Seeds are then upsampled into a dense completion through upsampling layers. Furthermore, discriminators and diversity penalties are used at every upsampling layer to train our model.

completion network's losses. To enable sampling during inference, we minimize the KL-divergence between \(E_{S}(z|X)\) and a normal distribution during training. We additionally find that adding noise to our sampled style codes during training leads to higher fidelity and more diverse completions.

### Style-based seed generator

We make use of the Patch Seeds representation proposed in SeedFormer [7], which enables faithfully completing unobserved regions while preserving partially observed structures. Patch Seeds are defined as a set of seed coordinates \(\mathcal{S}\in\mathbb{R}^{N_{S}\times 3}\) and seed features \(\mathcal{F}\in\mathbb{R}^{N_{S}\times C_{S}}\) produced by a seed generator. In particular, a set of upsampled features \(F_{up}\in\mathbb{R}^{N_{S}\times C_{S}}\) are generated from the partial local features \((X_{L},F_{L})\) via an Upsample Transformer [7]. Seed coordinates \(\mathcal{S}\) and features \(\mathcal{F}\) are then produced from upsampled features \(F_{up}\) concatenated with partial latent code \(f_{P}\) via an MLP.

However, the described seed generator is deterministic, prohibiting the ability to generate diverse Patch Seeds that can then produce diverse completions through upsampling layers. We propose to incorporate stochasticity into the seed generator by injecting stochasticity into the partial latent vector \(f_{P}\). We introduce a style modulator network \(M(f_{P},z)\), shown in Figure 2(b), that injects a style code \(z\) into a partial latent vector \(f_{P}\) to produce a styled partial shape latent vector \(f_{C}\). Following [54], we use weight modulation to inject style into the activation outputs of a network layer, where the demodulated weights \(w^{\prime\prime}\) used in each convolution layer are computed as:

\[s=A(z),\text{mod: }w^{\prime}_{ijk}=s_{i}\cdot w_{ijk},\text{demod: }w^{\prime\prime}_{ijk}=w^{\prime}_{ijk}\Bigg{/}\sqrt{\sum_{i,k}w^{{}^{\prime 2 }_{ijk}}+\epsilon}\] (1)

where \(A\) is an Affine transformation, and \(w\) is the original convolution weights with \(i,j,k\) corresponding to the input channel, output channel, and spatial footprint of the convolution, respectively.

### Coarse-to-fine decoder

Our decoder operates in a coarse-to-fine fashion, which has been shown to be effective in producing shapes with fine geometric structure for the task of point cloud completion [36; 38; 7; 4]. We treat our Patch Seed coordinates \(\mathcal{S}\) as our coarsest completion \(\mathcal{G}_{0}\) and progressively upsample by a factor \(r\) to produce denser completions \(\mathcal{G}_{i}\,(i=1,...,3)\). At each upsampling stage \(i\), a set of seed features are first interpolated from the Patch Seeds. Seed features along with the previous layer's points and features are then used by an Upsample Transformer [7] where local self-attention is performed to produce a new set of points and features upsampled by a factor \(r\). We replace the inverse distance weighted averaging used to interpolate seed features from Patch Seeds in SeedFormer with a PointConv interpolation. Since the importance of Patch Seed information may vary across the different layers, we believe a PointConv interpolation is more appropriate than a fixed weighted interpolation as it can learn the appropriate weighting of Patch Seed neighborhoods for each upsampling layer.

Unlike the fully-connected decoders in [8; 9], the coarse-to-fine decoder used in our method can reason about the structures of local regions, allowing us to generate cleaner shape surfaces. A coarse-to-fine design provides us with the additional benefit of having discriminators and diversity penalties at multiple resolutions, which we have found to lead to better completion fidelity and diversity.

Figure 3: (a) Architecture of our partial shape encoder. (b) Overview of our style modulator network. For each style-modulated convolution (gray box), \(w_{i}\) and \(b_{i}\) are learned weights and biases of a convolution, \(w^{\prime\prime}_{i}\) are the weights after the modulation and demodulation process, and \(A\) is a learned Affine transformation.

### Multi-scale discriminator

During training, we employ adversarial training to assist in learning realistic completions for any partial input and style code combination. We introduce a set of discriminators \(D_{i}\) for \(i=\{0,...,3\}\), to discriminate against real and fake point clouds at each output level of our generator. Each discriminator follows a PointNet-Mix architecture proposed by Wang et al. [55]. In particular, an MLP first extracts a set of point features from a shape, which are max-pooled and average-pooled to produce \(f_{max}\) and \(f_{avg}\), respectively. The features are then concatenated to produce _mix-pooled feature_\(f_{mix}=[f_{max},f_{avg}]\) before being passed through a fully-connected network to produce a final score about whether the point cloud is real or fake.

We also explored more complex discriminators that made use of PointConv or attention mechanisms, but we were unable to successfully train with any such discriminator. This is in line with the findings in [55], suggesting that more powerful discriminators may not guide the learning of point cloud shape generation properly. Thus, we instead use a weaker discriminator architecture but have multiple of them that operate at different scales to discriminate shape information at various feature levels.

For training, we use the WGAN loss [56] with R1 gradient penalty [57] and average over the losses at each output level \(i\). We let \(\hat{X}_{i}=\mathcal{G}_{i}(X_{P},z)\) be the completion output at level \(i\) for partial input \(X_{P}\) and sampled style code \(z\sim E_{S}(z|X)\), and let \(X_{i}\) be a real point cloud of same resolution. Then our discriminator loss \(\mathcal{L}_{D}\) and generator loss \(\mathcal{L}_{G}\) are defined as:

\[\mathcal{L}_{D} =\frac{1}{4}\sum_{i=0}^{3}\Big{(}\mathbb{E}_{\hat{X}\sim P(\hat{ X})}\Big{[}D_{i}(\hat{X}_{i})\Big{]}-\mathbb{E}_{X\sim P(X)}[D_{i}(X_{i})]+ \frac{\gamma}{2}\mathbb{E}_{X\sim P(X)}\big{[}\|\nabla D_{i}(X_{i})\|^{2}\big{]} \Big{)}\] (2) \[\mathcal{L}_{G} =-\frac{1}{4}\sum_{i=0}^{3}\Big{(}\mathbb{E}_{\hat{X}\sim P(\hat{ X})}[D_{i}(\hat{X}_{i})]\Big{)}\] (3)

where \(\gamma\) is a hyperparameter (\(\gamma=1\) in our experiments), \(P(\hat{X})\) is the distribution of generated shapes, and \(P(X)\) is the distribution of real shapes.

### Diversity regularization

Despite introducing stochasticity into the partial latent vector, it is still possible for the network to learn to ignore the style code \(z\), leading to mode collapse to a single completion. To address this, we propose a diversity penalty that operates in the feature space of our discriminator. Our key insight is that for a discriminator to be able to properly discriminate between real and fake point clouds, its extracted features should have learned relevant structural information. Then our assumption is that if two completions are structurally different, the discriminator's global mix-pooled features should be dissimilar as well, which we try to enforce through our diversity penalty.

Specifically, at every training iteration we sample two style codes \(z_{1}\sim E_{S}(z|X_{1})\) and \(z_{2}\sim E_{S}(z|X_{2})\) from random complete shapes \(X_{1}\) and \(X_{2}\). For a single partial input \(X_{P}\), we produce two different completions \(\mathcal{G}_{i}(X_{P},z_{1})\) and \(\mathcal{G}_{i}(X_{P},z_{2})\). We treat our discriminator \(D_{i}\) as a feature extractor and extract the mixed-pooled feature for both completions at every output level \(i\). We denote the mixed-pooled feature corresponding to a completion conditioned on style code \(z\) at output level \(i\) by \(f^{z}_{mix_{i}}\), then minimize:

\[\mathcal{L}_{div}=\sum_{i=0}^{3}\frac{1}{\big{\|}f^{z_{1}}_{mix_{i}}-f^{z_{2} }_{mix_{i}}\big{\|}_{1}}\] (4)

which encourages the generator to produce completions with dissimilar mix-pooled features for different style codes. Rather than directly using the discriminator's mix-pooled feature, we perform pooling only over the set of point features that are not in partially observed regions. This helps avoid penalizing a lack of diversity in the partially observed regions of our completions.

We additionally make use of a partial reconstruction loss at each output level on both completions:

\[\mathcal{L}_{part}=\sum_{z\in\{z_{1},z_{2}\}}\sum_{i=0}^{3}\ d^{UHD}(X_{P},\ \mathcal{G}_{i}(X_{P},z))\] (5)

where \(d^{UHD}\) stands for the unidirectional Hausdorff distance from partial point cloud to completion. Such a loss helps ensure that our completions respect the partial input for any style code \(z\).

To ensure that the completion set covers the ground truth completions in the training set, we choose to always set random complete shape \(X_{1}=X_{GT}\) and sample \(z_{1}\sim E_{S}(z|X_{GT})\), where \(X_{GT}\) is the corresponding ground truth completion to the partial input \(X_{P}\). This allows us to provide supervision at the output of each upsampling layer via Chamfer Distance (CD) for one of our style codes:

\[\mathcal{L}_{comp}=\sum_{i=0}^{3}\ d^{CD}(X_{GT},\ \mathcal{G}_{i}(X_{P},z_{1}))\] (6)

Our full loss that we use in training our generator is then:

\[\mathcal{L}=\lambda_{G}\mathcal{L}_{G}+\lambda_{comp}\mathcal{L}_{comp}+\lambda _{part}\mathcal{L}_{part}+\lambda_{div}\mathcal{L}_{div}\] (7)

We set \(\lambda_{G}=1,\lambda_{comp}=0.5,\lambda_{part}=1,\lambda_{div}=5\), which we found to be good default settings across the datasets used in our experiments.

## 4 Experiments

In this section, we evaluate our method against a variety of baselines on the task of multimodal shape completion and show superior quantitative and qualitative results across several synthetic and real datasets. We further conduct a series of ablations to justify the design choices of our method.

**Implementation Details** Our model takes in \(N_{P}=1024\) points as partial input and produces \(N=2048\) points as a completion. For training the generator, the Adam optimizer is used with an initial learning rate of \(1\times 10^{-4}\) and the learning rate is linearly decayed every \(2\) epochs with a decay rate of \(0.98\). For the discriminator, the Adam optimizer is used with a learning rate of \(1\times 10^{-4}\). We train a separate model for each shape category and train each model for 300 epochs with a batch size of 56. All models are trained on two NVIDIA Tesla V100 GPUs and take about 30 hours to train.

**Datasets** We conduct experiments on several synthetic and real datasets. Following the setup of [8], we evaluate our approach on the Chair, Table, and Airplane categories of the 3D-EPN dataset [58]. Similarly, we also perform experiments on the Chair, Table, and Lamp categories from the PartNet dataset [59]. To evaluate our method on real scanned data, we conduct experiments on the Google Scanned Objects (GSO) dataset [60]. For GSO, we share quantitative and qualitative results on the Shoe, Toys, and Consumer Goods categories. A full description is presented in the supplementary.

**Metrics** We follow [8] and evaluate with the Minimal Matching Distance (MMD), Total Mutual Difference (TMD), and Unidirectional Hausdorff Distance (UHD) metrics. MMD measures the fidelity of the completion set with respect to the ground truth completions. TMD measures the completion diversity for a partial input shape. UHD measures the completion fidelity with respect to the partial input. We evaluate metrics on \(K=10\) generated completions per partial input. Reported MMD, TMD, and UHD values in our results are multiplied by \(10^{3}\), \(10^{2}\), and \(10^{2}\), respectively.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{MMD \(\downarrow\)} & \multicolumn{4}{c}{TMD \(\uparrow\)} & \multicolumn{4}{c}{UHD \(\downarrow\)} \\ \cline{2-13} Method & Chair & Plane & Table & Avg. & Chair & Plane & Table & Avg. & Chair & Plane & Table & Avg. \\ \hline SeedFormer [7] & 0.45 & 0.17 & 0.65 & 0.42 & 0.00 & 0.00 & 0.00 & 0.00 & 1.69 & 1.27 & 1.69 & 1.55 \\ \hline KNN-latent [8] & 1.45 & 0.93 & 2.25 & 1.54 & 2.24 & 1.13 & 3.25 & 2.21 & 8.94 & 9.54 & 12.70 & 10.39 \\ cGAN [8] & 1.61 & 0.82 & 2.57 & 1.67 & 2.56 & 2.03 & 4.49 & 3.03 & 8.33 & 9.59 & 9.03 & 8.98 \\ IMLE [9] & * & * & * & * & 2.93 & **2.31** & 4.92 & **3.39** & 8.51 & 9.55 & 8.52 & 8.86 \\ Ours & **1.16** & **0.59** & **1.45** & **1.07** & **3.26** & 1.53 & **5.14** & 3.31 & **4.02** & **3.40** & **4.00** & **3.81** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on the 3D-EPN dataset. * indicates metric is not reported.

Figure 4: Qualitative comparison of multi-modal completions on the 3D-EPN dataset.

**Baselines** We compare our model against three direct multi-modal shape completion methods: cGAN [8], IMLE [9], and KNN-latent which is a baseline proposed in [8]. We further compare with the diffusion-based method PVD [10] and the auto-regressive method ShapeFormer [13]. We also share quantitative results against the deterministic point cloud completion method SeedFormer [7].

### Results

Results on the 3D-EPN dataset are shown in Table 1. SeedFormer obtains a low UHD implying that their completions respect the partial input well; however, their method produces no diversity as it is deterministic. Our UHD is significantly better than all multi-modal completion baselines, suggesting that we more faithfully respect the partial input. Additionally, our method outperforms others in terms of TMD and MMD, indicating better diversity and completion quality. This is also reflected in the qualitative results shown in Figure 4, where KNN-latent fails to produce plausible completions, while completions from cGAN contain high levels of noise.

In Table 2, we compare against other methods on the PartNet dataset. For a fair comparison with PVD, we also report metrics following their protocol (denoted by \(\dagger\))[10]. In particular, under their protocol TMD is computed on a subsampled set of 1024 points and MMD is computed on the subsampled set concatenated with the original partial input. Once again our method obtains the best diversity (TMD) across all categories, beating out the diffusion-based method PVD and the auto-regressive method ShapeFormer. Our method also achieves significantly lower UHD and shows competitive performance in terms of MMD. Some qualitative results are shown in Figure 5. We find that our method produces cleaner surface geometry and obtains nice diversity in comparison to other methods.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{MMD \(\downarrow\)} & \multicolumn{4}{c}{TMD \(\uparrow\)} & \multicolumn{4}{c}{UHD \(\downarrow\)} \\ \cline{2-13} Method & Shoe & Toys & Goods & Avg. & Shoe & Toys & Goods & Avg. & Shoe & Toys & Goods & Avg. \\ \hline SeedFormer [7] & 0.42 & 0.67 & 0.47 & 0.52 & 0.00 & 0.00 & 0.00 & 0.00 & 1.49 & 1.69 & 1.55 & 1.58 \\ \hline cGAN [8] & 1.00 & 2.75 & 1.79 & 1.85 & 1.10 & 1.87 & **1.95** & 1.64 & 5.05 & 6.61 & 6.35 & 6.00 \\ Ours & **0.85** & **1.90** & **0.99** & **1.25** & **1.71** & **2.27** & 1.89 & **1.96** & **2.88** & **3.84** & **4.68** & **3.80** \\ \hline PVD [10]\(\dagger\) & **0.66** & 2.04 & 1.11 & 1.27 & 1.15 & 2.05 & 1.44 & 1.55 & * & * & * & * \\ Ours \(\dagger\) & 0.90 & **1.72** & **1.04** & **1.22** & **2.42** & **2.88** & **2.57** & **2.62** & * & * & * & * \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on Google Scanned Objects dataset. * indicates metric is not reported. \(\dagger\) indicates methods that use an alternative computation for MMD and TMD.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{MMD \(\downarrow\)} & \multicolumn{4}{c}{TMD \(\uparrow\)} & \multicolumn{4}{c}{UHD \(\downarrow\)} \\ \cline{2-13} Method & Shoe & Toys & Goods & Avg. & Shoe & Toys & Goods & Avg. & Shoe & Toys & Goods & Avg. \\ \hline SeedFormer [7] & 0.42 & 0.67 & 0.47 & 0.52 & 0.00 & 0.00 & 0.00 & 0.00 & 1.49 & 1.69 & 1.55 & 1.58 \\ \hline cGAN [8] & 1.00 & 2.75 & 1.79 & 1.85 & 1.10 & 1.87 & **1.95** & 1.64 & 5.05 & 6.61 & 6.35 & 6.00 \\ Ours & **0.85** & **1.90** & **0.99** & **1.25** & **1.71** & **2.27** & 1.89 & **1.96** & **2.88** & **3.84** & **4.68** & **3.80** \\ \hline PVD [10]\(\dagger\) & **0.66** & 2.04 & 1.11 & 1.27 & 1.15 & 2.05 & 1.44 & 1.55 & * & * & * & * \\ Ours \(\dagger\) & 0.90 & **1.72** & **1.04** & **1.22** & **2.42** & **2.88** & **2.57** & **2.62** & * & * & * & * \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on the PartNet dataset. * indicates that metric is not reported. \(\dagger\) indicates methods that use an alternative computation for MMD and TMD.

Figure 5: Qualitative results on the PartNet dataset.

[MISSING_PAGE_FAIL:9]

### Ablation studies

In Table 4 we examine the dimensionality of our style codes. Our method obtains higher TMD when using smaller style code dimension size. We additionally find that adding a small amount of noise to sampled style codes during training further helps boost TMD while improving UHD. We believe that reducing the style code dimension and adding a small amount of noise helps prevent our style codes from encoding too much information about the ground truth shape, which could lead to overfitting to the ground truth completion. Furthermore, in Table 5, we present results with different choices for style code generation. Our proposed style encoder improves diversity over sampling style codes from a normal distribution or by using the mapping network from StyleGAN [46]. Despite having slightly worse MMD and UHD than StyleGAN's mapping network, we find the quality of completions at test time to be better when training with style codes sampled from our style encoder (see supplementary).

In our method, we made several changes to the completion network in SeedFormer [7]. We replaced the encoder from SeedFormer, which consisted of point transformer [61] and PointNet++ [62] set abstraction layers, with our proposed partial encoder as well as replaced the inverse distance weighted interpolation with PointConv interpolation in the SeedFormer decoder. To justify these changes, we compare the different architectures in our GAN framework in Table 6. We compare the performance of the original SeedFormer encoder and decoder (SF), our proposed partial encoder and SeedFormer decoder (PE + SF), and our full architecture where we replace inverse distance weighted interpolation with PointConv interpolation in the decoder (PE + SF + PCI). Our proposed partial encoder produces an improvement in TMD for slightly worse completion fidelity. Further, we find using PointConv interpolation provides an additional boost in diversity while improving completion fidelity.

The importance of our multi-scale discriminator is shown in Table 7. Using a single discriminator/diversity penalty only at the final output resolution results in a drop in completion quality and diversity when compared with our multi-scale design.

Finally, we demonstrate the necessity of our loss functions in Table 8. Without \(\mathcal{L}_{comp}\), our method has to rely on the discriminator alone for encouraging sharp completions in the missing regions. This leads to a drop in completion quality (MMD). Without \(\mathcal{L}_{part}\), completions fail to respect the partial input, leading to poor UHD. With the removal of either of these losses, we do observe an increase in TMD; however, this is most likely due to the noise introduced by the worse completion quality. Without \(\mathcal{L}_{div}\), we observe TMD drastically decreases towards zero, suggesting no diversity in the completions. This difference suggests how crucial our diversity penalty is for preventing conditional mode collapse. Moreover, we observe that when using all three losses, our method is able to obtain good completion quality, faithfully reconstruct the partial input, and produce diverse completions.

## 5 Conclusion

In this paper, we present a novel conditional GAN framework that learns a one-to-many mapping between partial point clouds and complete point clouds. To account for the inherent uncertainty present in the task of shape completion, our proposed style encoder and style-based seed generator enable diverse shape completion, with our multi-scale discriminator and diversity regularization preventing mode collapse in the completions. Through extensive experiments on both synthetic and real datasets, we demonstrate that our multi-modal completion algorithm obtains superior performance over current state-of-the-art approaches in both fidelity to the input partial point clouds and completion diversity. Additionally, our method runs in near real-time speed, making it suitable for applications in robotics such as planning or active perception.

While our method is capable of producing diverse completions, it considers only a segmented point cloud on the object. A promising future research direction is to explore how to incorporate additional scene constraints (e.g. ground plane and other obstacles) into our multi-modal completion framework.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & \multicolumn{2}{c}{MMD \(\downarrow\)} & TMD \(\uparrow\) & UHD \(\downarrow\) \\ \hline Single-scale & 1.58 & 2.05 & 4.27 \\ Multi-scale (Ours) & 1.50 & 4.36 & 3.79 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation on discriminator architecture.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{2}{c}{MMD \(\downarrow\)} & TMD \(\uparrow\) & UHD \(\downarrow\) \\ \hline w/o \(\mathcal{L}_{comp}\) & 1.62 & 4.61 & 4.58 \\ w/o \(\mathcal{L}_{part}\) & 1.81 & 5.97 & 13.03 \\ w/o \(\mathcal{L}_{div}\) & 1.70 & 0.41 & 3.57 \\ Ours & 1.50 & 4.36 & 3.79 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation on loss functions.

## Acknowledgments and Disclosure of Funding

This work was supported in part by ONR award N0014-21-1-2052, ONR/NAVSEA contract N00024-10-D-6318/DO#N0002420F8705 (Task 2: Fundamental Research in Autonomous Subsea Robotic Manipulation), NSF grant #1751412, and DARPA contract N66001-19-2-4035.

## References

* [1]W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert (2018) Pcn: point completion network. In 2018 international conference on 3D vision (3DV), pp. 728-737. Cited by: SS1.
* [2]L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. Reid, and S. Savarese (2019) Topnet: structural point cloud decoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 383-392. Cited by: SS1.
* [3]M. Liu, L. Sheng, S. Yang, J. Shao, and S. Hu (2020) Morphing and sampling network for dense point cloud completion. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34, pp. 11596-11603. Cited by: SS1.
* [4]X. Wang, M. H. Ang Jr, and G. H. Lee (2020) Cascaded refinement network for point cloud completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 790-799. Cited by: SS1.
* [5]H. Xie, H. Yao, S. Zhou, J. Mao, S. Zhang, and W. Sun (2020) Grnet: gridding residual network for dense point cloud completion. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX, pp. 365-381. Cited by: SS1.
* [6]X. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou (2021) Pointr: diverse point cloud completion with geometry-aware transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12498-12507. Cited by: SS1.
* [7]H. Zhou, Y. Cao, W. Chu, J. Zhu, T. Lu, Y. Tai, and C. Wang (2022) Feedformer: patch seeds based point cloud completion with upsample transformer. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part III, pp. 416-432. Cited by: SS1.
* [8]R. Wu, X. Chen, Y. Zhuang, and B. Chen (2020-05) Multimodal shape completion via conditional generative adversarial networks. In The European Conference on Computer Vision (ECCV), Cited by: SS1.
* [9]H. Arora, S. Mishra, S. Peng, K. Li, and A. Mahdavi-Amiri (2022-06) Multimodal shape completion via implicit maximum likelihood estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 2958-2967. Cited by: SS1.
* [10]L. Zhou, Y. Du, and J. Wu (2021-10) 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5826-5835. Cited by: SS1.
* [11]G. Chou, Y. Bahat, and F. Heide (2023) Diffusion-sdf: conditional generative modeling of signed distance functions. In The IEEE International Conference on Computer Vision (ICCV), Cited by: SS1.
* [12]Y. Cheng, H. Lee, S. Tuyakov, A. Schwing, and L. Gui (2022) SDFusion: multimodal 3d shape completion, reconstruction, and generation. arXiv. Cited by: SS1.
* [13]X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang (2022) Shapeformer: transformer-based shape completion via sparse representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.
* [14]P. Mittal, Y. Cheng, M. Singh, and S. Tulsiani (2022) AutoSDF: shape priors for 3d completion, reconstruction and generation. In CVPR, Cited by: SS1.

[MISSING_PAGE_POST]

* [17] Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 3859-3868, 2019.
* [18] Ruihui Li, Xianzhi Li, Ka-Hei Hui, and Chi-Wing Fu. Sp-gan: Sphere-guided 3d shape generation and manipulation. _ACM Transactions on Graphics (TOG)_, 40(4):1-12, 2021.
* [19] Yingzhi Tang, Yue Qian, Qijian Zhang, Yiming Zeng, Junhui Hou, and Xuefei Zhe. Warpinggan: Warping multiple uniform priors for adversarial 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6397-6405, 2022.
* [20] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5939-5948, 2019.
* [21] Marian Kleineberg, Matthias Fey, and Frank Weichert. Adversarial generation of continuous implicit shape representations. _arXiv preprint arXiv:2002.00349_, 2020.
* [22] Moritz Ibing, Isaak Lim, and Leif Kobbelt. 3d shape generation with grid-based implicit functions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13559-13568, 2021.
* [23] X Zheng, Yang Liu, P Wang, and Xin Tong. Sdf-stylegan: Implicit sdf-based stylegan for 3d shape generation. In _Computer Graphics Forum_, volume 41, pages 52-63. Wiley Online Library, 2022.
* [24] Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong. Setvae: Learning hierarchical composition for generative modeling of set-structured data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15059-15068, 2021.
* [25] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. _arXiv preprint arXiv:1511.05644_, 2015.
* [26] Matheus Gadelha, Rui Wang, and Subhransu Maji. Multiresolution tree networks for 3d point cloud processing. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 103-118, 2018.
* [27] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4541-4550, 2019.
* [28] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point flow networks for efficient point cloud generation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIII 16_, pages 694-710. Springer, 2020.
* [29] Janis Postels, Mengya Liu, Riccardo Spezialetti, Luc Van Gool, and Federico Tombari. Go with the flows: Mixtures of normalizing flows for point cloud generation and reconstruction. In _2021 International Conference on 3D Vision (3DV)_, pages 1249-1258. IEEE, 2021.
* [30] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.
* [31] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. _arXiv preprint arXiv:2210.06978_, 2022.
* [32] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural implicit 3d shape generation with latent diffusion models. _arXiv preprint arXiv:2212.00842_, 2022.
* [33] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. _arXiv preprint arXiv:2211.16677_, 2022.
* [34] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.
* [35] Xin Wen, Tianyang Li, Zhizhong Han, and Yu-Shen Liu. Point cloud completion by skip-attention network with hierarchical folding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1939-1948, 2020.
* [36] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. Pf-net: Point fractal network for 3d point cloud completion. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7662-7670, 2020.

* [37] Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Yu-Shen Liu. Pmp-net: Point cloud completion by learning multi-step point moving paths. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7443-7452, 2021.
* [38] Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Zhizhong Han. Snowflak-enet: Point cloud completion by snowflake point deconvolution with skip-transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 5499-5509, 2021.
* [39] Xin Wen, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Yu-Shen Liu. Cycle4completion: Unpaired point cloud completion using cycle transformation with missing region coding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 13080-13089, 2021.
* [40] Zhen Cao, Wenxiao Zhang, Xin Wen, Zhen Dong, Yu-shen Liu, Xiongwu Xiao, and Bisheng Yang. Ktnet: Knowledge transfer for unpaired 3d shape completion. _arXiv e-prints_, pages arXiv-2111, 2021.
* [41] Xuelin Chen, Baoquan Chen, and Niloy J Mitra. Unpaired point cloud completion on real scans using adversarial training. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* [42] Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, and Chen Change Loy. Unsupervised 3d shape completion through gan inversion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1768-1777, 2021.
* [43] Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong Chen, and Fang Wen. Style-based point generator with adversarial rendering for point cloud completion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4619-4628, 2021.
* [44] Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adversarial networks. _arXiv preprint arXiv:1810.01365_, 2018.
* [45] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2337-2346, 2019.
* [46] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* [47] Ari Heljakka, Yuxin Hou, Juho Kannala, and Arno Solin. Deep automodulators. _Advances in Neural Information Processing Systems_, 33:13702-13713, 2020.
* [48] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. _arXiv preprint arXiv:2103.10428_, 2021.
* [49] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. _Advances in neural information processing systems_, 30, 2017.
* [50] Augustus Odena, Jacob Buckman, Catherine Olsson, Tom Brown, Christopher Olah, Colin Raffel, and Ian Goodfellow. Is generator conditioning causally related to gan performance? In _International conference on machine learning_, pages 3849-3858. PMLR, 2018.
* [51] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. _arXiv preprint arXiv:1901.09024_, 2019.
* [52] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative adversarial networks for diverse image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1429-1437, 2019.
* [53] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, pages 9621-9630, 2019.
* [54] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8110-8119, 2020.

* [55] He Wang, Zetian Jiang, Li Yi, Kaichun Mo, Hao Su, and Leonidas Guibas. Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop Report_, 2021.
* [56] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* [57] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In _International conference on machine learning_, pages 3481-3490. PMLR, 2018.
* [58] Angela Dai, Charles Ruizhongtai Qi, and Matthias Niessner. Shape completion using 3d-encoder-predictor cnns and shape synthesis. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5868-5877, 2017.
* [59] Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [60] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 2553-2560. IEEE, 2022.
* [61] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 16259-16268, 2021.
* [62] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in neural information processing systems_, 30, 2017.