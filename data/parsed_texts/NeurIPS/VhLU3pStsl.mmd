# Estimating Riemannian Metric with

Noise-Contaminated Intrinsic Distance

 Jiaming Qiu

Fred Hutchinson Cancer Center

jqiu3@fredhutch.org

&Xiongtao Dai

Division of Biostatistics, School of Public Health

University of California, Berkeley

xdai@berkeley.edu

###### Abstract

We extend metric learning by studying the Riemannian manifold structure of the underlying data space induced by similarity measures between data points. The key quantity of interest here is the Riemannian metric, which characterizes the Riemannian geometry and defines straight lines and derivatives on the manifold. Being able to estimate the Riemannian metric allows us to gain insights into the underlying manifold and compute geometric features such as the geodesic curves. We model the observed similarity measures as noisy responses generated from a function of the intrinsic geodesic distance between data points. A new local regression approach is proposed to learn the Riemannian metric tensor and its derivatives based on a Taylor expansion for the squared geodesic distances, accommodating different types of data such as continuous, binary, or comparative responses. We develop theoretical foundation for our method by deriving the rates of convergence for the asymptotic bias and variance of the estimated metric tensor. The proposed method is shown to be versatile in simulation studies and real data applications involving taxi trip time in New York City and MNIST digits.

## 1 Introduction

With increasing data complexity, it has received increasing attention for capturing the geometric structure of the data space characterized by a Riemannian manifold, which consists of two vital parts: the coordinate system and the Riemannian metric. Manifold learning attempts to find low-dimensional representations as coordinates for the high-dimensional data [23, 27, 7]. Yet existing methods generally assume the local geometry is given, e.g., by the Euclidean distance between ambient data points, leading to non-isometric methods that distort the data geometry in the embedded space. On the other hand, many methods have been proposed to consider the Riemannian metric, including parametric estimation [17, 16, 15]; multi-metric learning [12]; pushforward via the Laplacian operator [21]; pull-back via Jacobian of generative models [1, 2]; Bayesian approach based on arc length of Gaussian process [14]; and dynamics of evolving probability when time-indexed samples are available [24]. It is also possible to recover the manifold in an abstract setting with only similarity measures among data points [9].

We propose to estimate the Riemannian metric utilizing similarity measures between data points when data coordinates are available. Suppose that data are generated from an unknown Riemannian manifold, while the Euclidean distance between the coordinates may not reflect the underlying geometry. We model the observed similarity measures between data points as noise-contaminated intrinsic distances, which are then used to characterize the intrinsic geometry on the Riemannian manifold. The targeted Riemannian metric is estimated in a data-driven fashion, which enables path finding via geodesics and calculus on the manifold in which the data reside.

This problem is closely related to metric learning [3; 26] where a Mahalanobis distance is commonly used to obtain the best distance for classification [11] and clustering [29] tasks. While a global metric is often the focus of earlier works, multiple local metrics [10; 28; 22; 5] are found to be useful because they better capture the data space geometry. Despite the resemblance, our target is to learn the _Riemannian metric_ instead of the _distance metric_, which fundamentally differentiates our proposal from metric learning. We emphasize the data space geometry rather than obtaining a metric optimal for subsequent supervised learning tasks. The additional smoothness of the Riemannian metric tensor field allows analysis of finer structures, while the coefficient matrix for the Mahalanobis distance is locally constant for distance metric learning.

To formulate the problem, let \((\mathcal{M},G)\) be a Riemannian manifold with Riemannian metric \(G\) and induced geodesic distance \(dist\left(\cdot,\cdot\right)\) which measures the true intrinsic difference between points. Knowing the coordinates of data points \(x_{0},x_{1}\in\mathcal{M}\), we identify each point via its coordinates (a tuple of real numbers). The noisy measures \(y\) of the intrinsic distance between data points are referred to as _similarity measures_ (equivalently dissimilarity). The response is modeled flexibly, and we consider the following common scenarios: (i) noisy distance, where \(y=dist\left(x_{0},x_{1}\right)^{2}+\epsilon\) for error \(\epsilon\), (ii) similarity/dissimilarity, where \(y=0\) if the two points \(x_{0},x_{1}\) are considered similar and \(y=1\) otherwise, and (iii) relative comparison, where a triplet of points \((x_{0},x_{1},x_{2})\) are given and \(y=1\) if \(x_{0}\) is more similar to \(x_{1}\) than to \(x_{2}\) and \(y=0\) otherwise. The binary similarity measurement is common in computer vision [e.g. 6], while the relative comparison could be useful for perceptional tasks and recommendation system [e.g. 25; 4]. We aim to estimate the Riemannian metric \(G\) and its derivatives using the coordinates and similarity measures among the data points.

The major contribution of this paper is threefold. First, we formulate a framework for probabilistic modeling of similarity measures among data on manifold via intrinsic distances. Such a framework also justifies the distance metric learning from a geometrical perspective and unveils its approximation error for the first time as we know. Second, a theoretical foundation is developed for the proposed method including asymptotic consistency. Last and most importantly, the proposed method provides a geometric interpretation for the structure of the data space induced by the similarity measures, as demonstrated in the numerical examples that include a taxi travel and an MNIST digit application.

## 2 Background in Riemannian Geometry

For brevity, _metric_ now refers to _Riemannian metric_ while _distance metric_ is always spelled out. Throughout the paper, \(\mathcal{M}\) denotes a \(d\)-dimensional manifold endowed with a coordinate chart \((U,\varphi)\), where \(\varphi:U\to\mathds{R}^{d}\) maps a point \(p\in U\subset\mathcal{M}\) on the manifold to its coordinate \(\varphi(p)=\left(\varphi^{1}(p),\ldots,\varphi^{d}(p)\right)\in\mathds{R}^{d}\). Without loss of generality, we identify a point by its coordinate as \(\left(p^{1},\ldots,p^{d}\right)\), suppressing \(\varphi\) for the coordinate chart. Upper-script Roman letters denote the components of a coordinate, e.g., \(p^{i}\) is the \(i\)-th entry in the coordinate of the point \(p\), and \(\gamma^{i}\) is the \(i\)-th component function of a curve \(\gamma:\mathds{R}\supset[a,b]\to\mathcal{M}\) when expressed on chart \(U\). The tangent space \(T_{p}\mathcal{M}\) is a vector space consisting of velocities of the form \(v=\gamma^{\prime}(0)\) where \(\gamma\) is any curve satisfying \(\gamma(0)=p\). The coordinate chart induces a basis on the tangent space \(T_{p}\mathcal{M}\), as \(\partial_{i}|_{p}=\partial/\partial x^{i}|_{p}\) for \(i=1,\ldots,d\), so that a tangent vector \(v\in T_{p}\mathcal{M}\) is represented as \(v=\sum_{i=1}^{d}v^{i}\partial_{i}\) for some \(v^{i}\in\mathds{R}\), suppressing the subscript \(p\) in the basis. We adopt the Einstein summation convention unless otherwise specified, namely \(v^{i}\partial_{i}\) denotes \(\sum_{i=1}^{d}v^{i}\partial_{i}\), where common pairs of upper- and lower-indices denotes a summation from 1 to \(d\) [see e.g., 18, pp.18-19].

The Riemannian metric \(G\) on a \(d\)-dimensional manifold \(\mathcal{M}\) is a smooth tensor field acting on the tangent vectors. At any \(p\in\mathcal{M}\), \(G(p):T_{p}\mathcal{M}\times T_{p}\mathcal{M}\to\mathds{R}\) is a symmetric bi-linear tensor/function satisfying \(G(p)(v,v)\geq 0\) for any \(v\in T_{p}\mathcal{M}\) and \(G(p)(v,v)=0\) if and only if \(v=0\). On a chart \(\varphi\), the metric is represented as a \(d\)-by-\(d\) positive definite matrix that quantifies the distance traveled along infinitesimal changes in the coordinates. With an abuse of notation, the chart representation of \(G\) is given by the matrix-valued function \(p\mapsto G(p)=[G_{ij}(p)]_{i,j=1}^{d}\in\mathds{R}^{d\times d}\) for \(p\in\mathcal{M}\), so the distance traveled by \(\gamma\) at time \(t\) for a duration of \(dt\) is \([G_{ij}(\gamma(t))\dot{\gamma}^{i}(t)\dot{\gamma}^{j}(t)]^{1/2}\). The intrinsic distance induced by \(G\), or the _geodesic distance_, is computed as \(dist\left(p,q\right)=\inf_{\alpha}\int_{0}^{1}\sqrt{\sum_{1\leq i,j\leq d}G_{ ij}(\alpha(t))\dot{\alpha}^{i}(t)\dot{\alpha}^{j}(t)dt}\) for two points \(p,q\) on the manifold \(\mathcal{M}\), where infimum is taken over any curve \(\alpha:[0,1]\to\mathcal{M}\) connecting \(p\) to \(q\).

A _geodesic curve_ (or simply _geodesic_) is a smooth curve \(\gamma:\mathds{R}\supset[a,b]\rightarrow\mathcal{M}\) satisfying the _geodesic equations_, represented on a coordinate chart as

\[\tilde{\gamma^{k}}(t)+\dot{\gamma^{i}}(t)\dot{\gamma^{j}}(t)\Gamma_{ij}^{k} \circ\gamma(t)=0,\text{ for }i,j,k=1,\ldots,d,\] (2.1)

where over-dots represent derivative w.r.t. \(t\); \(\Gamma_{ij}^{k}=\frac{1}{2}G^{kl}\left(\partial_{i}G_{jl}+\partial_{j}G_{il}- \partial_{l}G_{ij}\right)\) are the _Christoffel symbols_ at \(p\); and \(G^{kl}\) is the \((k,l)\)-element of \(G^{-1}\). Solving (2.1) with initial conditions1 produces geodesic that traces out the generalization of a straight line on the manifold, preserving travel direction with no acceleration, and is also locally the shortest path.

Footnote 1: See Section S3 of the Supplement for details about solving it in practice.

Considering the shortest path \(\gamma\) connecting \(p\) to \(q\) and applying Taylor's expansion at \(t=0\), we obtain \(dist\left(p,q\right)^{2}\approx\sum_{1\leq i,j\leq d}G_{ij}(p)(q^{i}-p^{i})(q^ {j}-p^{j})\), showing the connection between the geodesic distance the Mahalanobis distance. Our estimation method is inspired by this approximation, and we will discuss the higher-order terms shortly which unveil finer structure of the manifold.

## 3 Local Regression for Similarity Measurements

### Probabilistic Modeling for Similarity Measurements

Suppose that we observe \(N\) independent triplets \(\left(Y_{u},X_{u0},X_{u1}\right)\), \(u=1,\ldots,N\). Here, the \(X_{uj}\) are locations on the manifold identified with their coordinates \(\left(X_{uj}^{1},\ldots,X_{uj}^{d}\right)\in\mathds{R}^{d}\), \(j=1,2\), and \(Y_{u}\) are noisy similarity measures of the proximity of \(\left(X_{u0},X_{u1}\right)\) in terms of the intrinsic geodesic distance \(dist\left(\cdot,\cdot\right)\) on \(\mathcal{M}\). To account for different structures of the similarity measures, we model the response in a fashion analogous to generalized linear models. For \(X_{u0},X_{u1}\) lying in a small neighborhood \(\mathcal{U}_{p}\subset\mathcal{M}\) of a target location \(p\in\mathcal{M}\), the similarity measure \(Y_{u}\) is modeled as

\[\mathds{E}\left(Y_{u}|X_{u0},X_{u1}\right)=g^{-1}\left(dist\left(X_{u0},X_{u1} \right)^{2}\right),\] (3.1)

where \(g\) is a given link function that relates the conditional expectation to the squared distance.

_Example 3.1_.: We describe below three common scenarios modeled by the general framework (3.1).

1. Continuous response being the squared geodesic distance contaminated with noise: \[Y_{u}=dist\left(X_{u0},X_{u1}\right)^{2}+\sigma(p)\varepsilon_{u},\] (3.2) where \(\varepsilon_{1},\ldots,\varepsilon_{u}\) are i.i.d. mean zero random variables, and \(\sigma:\mathcal{M}\rightarrow\mathds{R}^{+}\) is a smooth positive function determining the magnitude of noise near the target point \(p\). This model will be applied to model trip time as noisy measure of cost to travel between locations.
2. Binary (dis)similarity response: \[\mathds{P}\left(Y_{u}=1|X_{u0},X_{u1}\right)=\mathrm{logit}^{-1}\left(dist \left(X_{u0},X_{u1}\right)^{2}-\hbar\left(p\right)\right)\] (3.3) for some smooth function \(\hbar:\mathcal{M}\rightarrow\mathds{R}\), where \(\mathrm{logit}(\mu)=\log\left(\mu/\left(1-\mu\right)\right)\), \(\mu\in(0,1)\) is the logit function. This models the case when there are latent labels for \(X_{uj}\) and \(Y_{u}\) is a measure of whether their labels are in common or not. The function \(\hbar\left(p\right)\) in (3.3) describes the homogeneity of the latent labels for points in a small neighborhood of \(p\). The latent labels could have intrinsic variation even if measures are made for the same data points \(x=X_{u0}=X_{u1}\), and the strength of which is captured by \(\hbar\left(p\right)\).
3. Binary relative comparison response, where we extend our model for triplets of points \(\left(X_{u0},X_{u1},X_{u2}\right)\), where \(Y_{u}\) stands for whether \(X_{u0}\) is more similar to \(X_{u1}\) than to \(X_{u2}\): \[\mathds{P}\left(Y_{u}=1|X_{u0},X_{u1},X_{u2}\right)=\mathrm{logit}^{-1}\left( dist\left(X_{u0},X_{u2}\right)^{2}-dist\left(X_{u0},X_{u1}\right)^{2} \right),\] (3.4) so that the relative comparison \(Y_{u}\) reflects the comparison of squared distances.

### Local Approximation of Squared Distances

It is the squared distance that determines the responses in our model (3.1), which is justified by the following local approximation. Proposition 3.1 provides a Taylor expansion for the squared geodesic distance between two geodesics with same starting point but different initial velocities (see Figure S5.1 for visualization). It is the key tool to estimate model (3.1) through local regression. Furthermore, (3.5) characterize the error of approximating geodesic distance with Mahalanobis distance. For a point \(p\) on the Riemannian manifold \(\mathcal{M}\), let \(\exp_{p}:T_{p}\mathcal{M}\to\mathcal{M}\) denote the exponential map defined by \(\exp_{p}(tv)=\gamma(t)\) where \(\gamma\) is a geodesic starting from \(p\) at time \(0\) with initial velocity \(\gamma^{\prime}(0)=v\in T_{p}\mathcal{M}\). For notational simplicity, we suppress the dependency on \(p\) in geometric quantities (e.g., the metric \(G\) is understood to be evaluated at \(p\)). For \(i=1,\dots,d\), denote \(\delta^{i}=\delta^{i}(t)=\gamma^{i}(t)-\gamma^{i}(0)\) as the difference in coordinate after a travel of time \(t\) along \(\gamma\).

**Proposition 3.1** (spread of geodesics, coordinated).: Let \(p\in\mathcal{M}\) and \(v,w\in T_{p}\mathcal{M}\) be two tangent vectors at \(p\). On a local coordinate chart, the squared geodesic distance between two geodesics \(\gamma_{0}(t)=\exp_{p}(tv)\) and \(\gamma_{1}(t)=\exp_{p}(tw)\) satisfies, as \(t\to 0\),

\[dist\left(\gamma_{0}(t),\gamma_{1}(t)\right)^{2}=\delta^{i}_{0-1}\delta^{j}_{0 -1}G_{ij}+\delta^{i}_{0-1}\left(\delta^{k}_{0}\delta^{l}_{0}-\delta^{k}_{1} \delta^{l}_{1}\right)\Gamma^{j}_{kl}G_{ij}+O(t^{4}),\] (3.5)

where for \(i,j,k,l,m=1,\dots,d\),

* \(\delta^{i}_{0}=\gamma^{i}_{0}(t)-p^{i}\), \(\delta^{i}_{1}=\gamma^{i}_{1}(t)-p^{i}\), and \(\delta^{i}_{0-1}=\delta^{i}_{0}-\delta^{i}_{1}\), i.e., \(\delta^{i}_{0}\), \(\delta^{i}_{1}\) are differences in \(i\)-th coordinates of \(\gamma_{0}(t)\) and \(\gamma_{1}(t)\) to the origin \(p\), respectively, and \(\delta^{i}_{0-1}=\delta^{i}_{0}-\delta^{i}_{1}\) is the coordinate difference between \(\gamma_{0}(t)\) and \(\gamma_{1}(t)\);
* \(G_{ij}\) and \(\Gamma^{j}_{kl}\) are the elements of the metric and Christoffel symbols at \(p\), respectively.

Proof.: See Section S5 in the Supplement. 

To the RHS of (3.5), the first term is the quadratic term in distance metric learning. The second term is the result of coordinate representation of geodesics. It vanishes under the _normal coordinate_ where the Christoffel symbols are zero.2 It inspires the use of local regression to estimate the metric tensor and the Christoffel symbols. For \(X_{u0},X_{u1}\) in a small neighborhood of \(p\), write the linear predictor as

Footnote 2: See e.g., [19] pp. 131â€“133 for normal coordinate. See [20] and Proposition S5.1 of the Supplement for coordinate-invariant version of Proposition 3.1.

\[\eta_{u}:=\beta^{(0)}+\delta^{i}_{u,0-1}\delta^{j}_{u,0-1}\beta^{(1)}_{ij}+ \delta^{k}_{u,0-1}\left(\delta^{i}_{u0}\delta^{j}_{u0}-\delta^{i}_{u1}\delta^{ j}_{u1}\right)\beta^{(2)}_{ijk},\] (3.6)

a function of the intercept \(\beta^{(0)}\) and coefficients \(\beta^{(1)}_{ij},\beta^{(2)}_{ijk}\), where \(\delta^{i}_{u0}=X^{i}_{u0}-p^{i}\), \(\delta^{i}_{u1}=X^{i}_{u1}-p^{i}\), and \(\delta^{i}_{u,0-1}=\delta^{i}_{u0}-\delta^{i}_{u1}\), for \(i,j,k,l=1,\dots,d\), and \(u=1,\dots,N\). The intercept term \(\beta^{(0)}\) is included for capturing intrinsic variation (e.g., \(\hbar(p)\) in (3.3)) and can otherwise be dropped. The link function connects the linear predictor to the conditional mean via \(\mu_{u}:=g^{-1}\left(\eta_{u}\right)\approx\mathds{E}\left(Y_{u}|X_{u0},X_{u1}\right)\) as indicated by (3.1) and (3.5), where \(\mu_{u}\) is seen as a function of the coefficients \(\beta^{(0)}\), \(\beta^{(1)}_{ij}\), and \(\beta^{(2)}_{ijk}\). Therefore, upon the specification of a loss function \(Q:\mathds{R}\times\mathds{R}\to\{0\}\cup\mathds{R}^{+}\) and non-negative weights \(w_{1},\dots,w_{N}\), the minimizers

\[(\hat{\beta}^{(0)},\hat{\beta}^{(1)}_{ij},\hat{\beta}^{(2)}_{ijk})=\underset{ \beta^{(0)},\beta^{(1)}_{ij},\beta^{(2)}_{ijk},i,j,k}{\arg\min}\quad\sum_{u=1 }^{N}Q\left(Y_{u},\,\mu_{u}\right)w_{u},\] (3.7)

subject to

\[\beta^{(1)}_{ij}=\beta^{(1)}_{ji},\;\beta^{(2)}_{ijk}=\beta^{(2)}_{jik},\; \text{for}\;i,j,k,l=1,\dots,d,\] (3.8)

are used to estimate the metric tensor and Christoffel symbols, obtaining

\[\hat{G}_{ij}=\hat{\beta}^{(1)}_{ij},\quad\hat{\Gamma}^{l}_{ij}=\hat{\beta}^{(2 )}_{ijk}\hat{G}^{kl},\] (3.9)

where \(\hat{G}^{kl}\) is the matrix inverse of \(\hat{G}\) satisfying \(\hat{G}^{kl}\hat{G}_{kj}=1_{\{j=l\}}\). The symmetry constraints (3.8) are the result of the symmetries in the metric tensor and Christoffel symbols, and are enforced by optimizing over only the lower triangular indices \(1\leq i<j\leq d\) without constraints. Asymptoticresults provide the positive-definiteness of the metric estimate, as will be shown in Proposition 4.1. To weigh the pairs of endpoints according to their proximity to the target location \(p\), we apply kernel weights specified by

\[w_{u}=h^{-2d}\prod_{i=1}^{d}K\left(\frac{X_{u0}^{i}-p^{i}}{h}\right)K\left(\frac{ X_{u1}^{i}-p^{i}}{h}\right)\] (3.10)

for some \(h>0\) and non-negative kernel function \(K\). The bandwidth \(h\) controls the bias-variance trade-off of the estimated Riemannian metric tensor and its derivatives.

Altering the link function \(g\) and the loss function \(Q\) in (3.7) enables flexible local regression estimation for models in Example 3.1.

_Example 3.2_.: Consider the following loss functions for estimating the metric tensors and the Christoffel symbols when data are drawn from model (3.2)-(3.4), respectively.

1. Continuous noisy response: use squared loss \(Q\left(y,\mu\right)=\left(y-\mu\right)^{2}\) with \(g\) being the identity link function so \(\mu_{u}=\eta_{u}\).
2. Binary (dis)similarity response: use log-likelihood of Bernoulli random variable \[Q\left(y,\mu\right)=y\log\mu+(1-y)\log\left(1-\mu\right),\] (3.11) and \(g\) the logit link, so \(\mu_{u}=\log\!\mathrm{i}^{-1}\left(\eta_{u}\right)\). The model becomes a local logistic regression.
3. Binary relative comparison response: apply the same loss function (3.11) and logit link as in the previous scenario, but here we formulate the linear predictor based on \(dist\left(X_{u0},X_{u2}\right)^{2}-dist\left(X_{u0},X_{u1}\right)^{2}\approx \eta_{u1}-\eta_{u2}\) and \[\mu_{u}=g^{-1}\left(\eta_{u1}-\eta_{u2}\right).\] (3.12) Locally, the difference in squared distances is approximated by \[\eta_{u1}-\eta_{u2} =\left(\delta_{u,0-1}^{i}\delta_{u,0-1}^{j}-\delta_{u,0-2}^{i} \delta_{u,0-2}^{j}\right)\beta_{ij}^{(1)}+\left(\delta_{u,0-1}^{k}\left(\delta_ {u0}^{i}\delta_{u0}^{j}-\delta_{u1}^{i}\delta_{u1}^{j}\right)\right)\beta_{ijk}^ {(2)}\] \[\quad-\left(\delta_{u0,-2}^{k}\left(\delta_{u0}^{i}\delta_{u0}^{j }-\delta_{u2}^{i}\delta_{u2}^{j}\right)\right)\beta_{ijk}^{(2)},\] (3.13) for \(\delta_{u2}^{i}=X_{u2}^{i}-p^{i}\) and \(\delta_{u,0-2}^{i}=\delta_{u2}^{i}-\delta_{u0}^{i}\), \(i=1,\ldots,d\). Here \(\eta_{u1}\) and \(\eta_{u2}\) are constructed in analogy to (3.6) using \(\left(X_{u0},X_{u1}\right)\) and \(\left(X_{u0},X_{u2}\right)\) pair respectively.

Examples in Section 5 will further illustrate the proposed method in those scenarios. Besides the models listed, other choices for the link \(g\) and loss function \(Q\) can also be considered under this local regression framework [8], accommodating a wide variety of data. To efficiently estimate the metric on the entire manifold \(\mathcal{M}\), we apply a procedure based on discretization and post-smoothing, as detailed in Section S3 of the Supplement. In short, kernel smoothing (weighted average) of estimated component functions \(\hat{G}_{ij}\) over some grid points provides a smooth tensor field, which ease the burden of re-estimation for every points on the manifold.

## 4 Bias and Variance of the Estimated Metric Tensor

This subsection provides asymptotic justification for model (3.2) with \(\mathbb{E}\left(Y_{u}|X_{u0},X_{u1}\right)=dist\left(X_{u0},X_{u1}\right)^{2}\) under the squared loss \(Q(\mu,y)=(\mu-y)^{2}\) and the identity link \(g(\mu)=\mu\). The estimator we analyzed here fits a local quadratic regression without intercept and approximates the squared distance by a simplified form of (3.6):

\[dist\left(X_{u0},X_{u1}\right)^{2}\approx\eta_{u}:=\delta_{u,0-1}^{i}\delta_{ u,0-1}^{j}\beta_{ij}^{(1)},\] (4.1)

for \(u=1,\ldots,N\). Given a suitable order of the indices \(i,j\) for vectorization, we rewrite the formulation into a matrix form. Denote the local design matrix and regression coefficients as

\[\mathbf{D}_{u} =\left(\delta_{u,0-1}^{1}\delta_{u,0-1}^{1},\;\ldots,\;\delta_{u, 0-1}^{i}\delta_{u,0-1}^{j},\;\ldots,\;\delta_{u,0-1}^{d}\delta_{u,0-1}^{d} \right)^{T},\] \[\boldsymbol{\beta} =\left(\beta_{11}^{(1)},\ldots,\beta_{ij}^{(1)},\ldots,\beta_{dd} ^{(1)}\right)^{T},\]so that the linear predictor \(\eta_{u}=\mathbf{D}_{u}^{T}\bm{\beta}\). Further, write \(\mathbf{D}=\left(\mathbf{D}_{1},\ldots,\mathbf{D}_{N}\right)^{T}\), \(\bm{Y}=\left(Y_{1},\ldots,\bm{Y}_{N}\right)^{T}\), and \(\mathbf{W}=\operatorname{diag}\left(w_{1},\ldots,w_{N}\right)\), with weights \(w_{u}\) specified in (3.10). The objective function in (3.7) becomes \(\left(\bm{Y}-\mathbf{D}\bm{\beta}\right)^{T}\mathbf{W}\left(\bm{Y}-\mathbf{D} \bm{\beta}\right)\), and the minimizer is \(\bm{\hat{\beta}}=\left(\mathbf{D}^{T}\mathbf{W}\mathbf{D}\right)^{-1}\mathbf{ D}^{T}\mathbf{W}\bm{Y}\), for which we will analyze the bias and variance.

To characterize the asymptotic bias and variance of the estimator, we assume the following conditions are satisfied in a neighborhood of the target \(p\). These conditions are standard and analogous to those assumed in a local regression setting [8].

* The joint density of endpoints \(\left(X_{u0},X_{u1}\right)\) is positive and continuously differentiable.
* The functions \(G_{ij},\Gamma_{ij}^{k}\) are \(C^{2}\)-smooth for \(i,j,k=1,\ldots,d\).
* The kernel \(K\) in weights (3.10) is symmetric, continuous, and has a bounded support.
* \(\sup_{u}\operatorname{var}\left(Y_{u}|X_{u0},X_{u1}\right)<\infty\).

**Proposition 4.1**.: Under (A1)-(A4), \(\operatorname{bias}\left(\hat{\bm{\beta}}|\mathbf{X}\right)=O_{p}\left(h^{2} \right),\operatorname{var}\left(\hat{\bm{\beta}}|\mathbf{X}\right)=O_{p} \left(N^{-1}h^{-4-2d}\right)\), as \(h\to 0\) and \(Nh^{2+2d}\to\infty\), where \(\mathbf{X}=\{(X_{u0},X_{u1})\}_{u=1}^{N}\) is the collection of observed endpoints.

Proof.: See Section S6 in the Supplement. 

The local approximation (4.1) is similar to a local polynomial estimation of the second derivative of a \(2d\)-variate squared geodesic distance function, explaining the order of \(h\) in the rates for bias and variance.

The positive definite constraints (as in distance metric learning) are dropped based on the asymptotic properties, thus the estimated metric could fail to be positive definite at some locations. This is alleviated by sufficient data and post-smoothing that averaging neighboring estimation.

## 5 Simulation

We illustrate the proposed method using simulated data with different types of responses as described in Example 3.1. We study whether the proposed method well estimates Riemannian geometric quantities, including the metric tensor, geodesics, and Christoffel symbols. Additional details including assessment on stability and positive-definiteness are included in Section S4 of the Supplement. Subsection S4.2 also provides an ellipsoid example for manifold of non-constant curvature.

### Unit Sphere

The usual arc-length/great circle distance on the \(d\)-dimensional unit sphere is induced by the round metric, which is expressed under the stereographic projection coordinate \(\left(x^{1},\ldots,x^{d}\right)\) as \(\hat{G}_{ij}=4\left(1+\sum_{k=1}^{d}x^{k}x^{k}\right)^{-2}1_{\{i=j\}},\) for \(i,j=1,\ldots,d\). Under the additive model (3.2) in Example 3.1, we considered either noiseless or noisy responses by setting \(\sigma(p)=0\) or \(\sigma(p)>0\) respectively.

Experiments were preformed with \(d=2\) and the finding are summarized in Figure 5.1. For continuous responses, the left panel of Figure 5.1a visualizes the true and estimated metric tensors via cost ellipses (S2.1) and the right panel shows the corresponding geodesics by solving the geodesic equations (2.1) with true and estimated Christoffel symbols. The metrics and the geodesics were well estimated under the continuous response model with or without additive errors, where the estimates overlap with the truth. Figure 5.1c evaluates the relative estimation errors \(\left\|\hat{G}-G\right\|_{F}/\left\|G\right\|_{F}\) and \(\left\|\hat{\Gamma}-\Gamma\right\|_{F}/\left\|\Gamma\right\|_{F}\) w.r.t. the Frobenius norm (S2.2) for data from the continuous model (3.2).

For binary responses under model (3.3), Figure 5.1b visualizes the data where the background color illustrates \(\hbar\). Figure 5.1d and left panel of Figure 5.1a suggest that the intercept and the metric were reasonably estimated, while the geodesics are slightly away from the truth (Figure 5.1a, right). This indicates that the binary model has higher complexity and less information is provided by the binary response (see also Figure S4.1b in the Supplement).

### Relative Comparison on the Double Spirals

A set of \(7\times 10^{4}\) points on \(\mathds{R}^{2}\) were generated around two spirals, corresponding to two latent classes \(\mathcal{A}\) and \(\mathcal{B}\) (e.g., green points in Figure (a)a are from latent class \(\mathcal{A}\)). We compare neighboring points \((X_{u0},X_{u1},X_{u2})\) to generate relative comparison response \(Y_{u}\) as follows. For \(u=1,\ldots,N\), \(Y_{u}=1\) if \(X_{u0},X_{u1}\) belong to the same latent class and \(X_{u0},X_{u2}\) belong to different classes; otherwise \(Y=0\). Figure (b)b shows a portion of the \(N=6,965,312\) comparison generated, where the hollow circles in the middle of each wedge correspond to \(X_{u0}\).

Here, contrast of the two latent classes induces the intrinsic distance, so the distance is larger across the supports of the two classes and smaller within a single support. Therefore, the resulting metric tensor should reflect less cost while moving along the tangential direction of the spirals compared to perpendicular directions. Estimates were drawn under model (3.4) by minimizing the objective (3.7) with the link function (3.12) and the local approximation (3.13).

The estimated metric shown in Figure (c)c is consistent with the interpretation of the intrinsic distance and metric induced by the class membership discussed above. Meanwhile, the estimated geodesic curve unveils the hidden circular structure of the data support as shown in Figure (d)d.

Figure 1: Simulation results for 2-dimensional sphere under stereographic projection coordinate.

## 6 New York City Taxi Trip Duration

We study the geometry induced by taxi travel time in New York City (NYC) during weekday morning rush hours. New York City Taxi and Limousine Commission (TLC) Trip Record Data was accessed on May 1st, 20223 to obtained business day morning taxi trip records including GPS coordinates for pickup/dropoff locations as \((X_{u0},X_{u1})\) and trip duration as \(Y_{u}\). Estimation to the travel time metric was drawn under model (3.2) with \(Q(y,\mu)=(y-\mu)^{2}\) and \(g(\mu)=\mu\).

Footnote 3: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page. Data format changed after our download.

Figure 6.1a shows the estimated metric for taxi travel time. The background color shows the Frobenius norm of the metric tensor, where larger values mean that longer travel time is required to pass through that location. Trips through midtown Manhattan and the financial district were estimated to be the most costly during rush hours, which is coherent to the fact that these are the city's primary business districts. Moreover, the cost ellipses represent the cost in time to travel a unit distance along different directions. This suggests that in Manhattan, it takes longer to drive along the east-west direction (narrower streets) compared to the north-south direction (wider avenues).

Geodesic curves in Figure 6.1b show where a 15-minutes taxi ride leads to starting from the Empire State Building. Each geodesic curve corresponds to one of 12 starting directions (1-12 o'clock). Note that we apply a continuous Riemannian manifold approximation to the city, so the geodesic curves provide approximations to the shortest paths between locations and need not conform to the road network. Travel appears to be faster in lower Manhattan than in midtown Manhattan. The spread of the geodesics differs along different directions, indicating the existence of non-constant curvature on the manifold and advocating for estimating the Riemannian metric tensor field instead of applying a single global distance metric.

## 7 High-dimensional Data: An Example with MNIST

The curse of dimensionality is challenging for nonparametric methods applied on data sources like images and audios. However, it is often found that apparent high-dimensional data actually lie close to some low-dimensional manifold, which is utilized by manifold learning literature to produce reasonable low-dimensional coordinate representations. The proposed method can be applied to the resulting low-dimensional coordinates to recover the high-dimensional geometry even implicitly defined, demonstrated in the following MNIST example.

We embed images in MNIST to a 2-dimensional space via tSNE [13]. Similarity between the objects was computed by the sum of the Wasserstein distance between images4 and the indicator of whether the underlying digits are different (1) or not (0). The goal is to infer the geometry of the embedded

Figure 5.2: Relative comparison on double spirals. Gray curves (solid and dashed) in the background represent the approximate support of the two latent classes.

data induced by this similarity measures. Estimation was drawn under model (3.2) with squared loss \(Q(y,\mu)=(y-\mu)^{2}\) and the identity link. An intercept term (\(\beta^{(0)}\) in (3.6)) is used to capture intrinsic variation partially due to the non-injective dimensional reduction.

The geodesics estimated from our method tend to minimize the number of switches between labels. For example, the geodesic A in Figure 7.1 remains "4" (1st row of panel (b)) throughout, while the straight line on the tSNE chart translates to a path of images switching between "4" and "9" (2nd row of panel (b)); similar phenomenon occurs for geodesics B and C (corresponding to 7th and 12th rows in (b)). Also, our estimated geodesics produce reasonable transition and reside in the space of digits, while unrestricted optimal transport (3rd, 8th, and 13th rows of panel (b)) could produce unrecognizable intermediate images. Our estimated geodesic is faithful to the geometric interpretation that a geodesic is locally the shortest path. Moreover, our proposal provide sensible image transition, especially compared to the shortest paths along neighborhood graph in the embedded space (dotted blue lines in (a) and 4, 9, 14 rows in (b)) or kNN graphs in the original Euclidean space \(\mathds{R}^{28\times 28}\) (5, 10, 15 rows in (b)). This could be useful for complex data interpolation that preserves geometry.

## 8 Discussion

We present a novel framework for inferring the data geometry based on pairwise similarity measures. Our framework targets data lying on a low-dimensional manifold where observations are expected to be dense near the locations where we wish to estimate the metric. However, these assumptions are general enough for our method to be applied to manifold data with high ambient dimension in combination with manifold embedding tools. Context-specific interpretation of the geometrical notions, e.g., for Riemannian metric and geodesics, has been demonstrated in the taxi travel and MNIST digit examples. Our method could inspire additional compelling applications to other topics such as cognition and perception research, where psychometric similarity measures are commonly made. Moreover, further development including (but not limited to) handling sparse data and ensuring positivity in the Riemannian metric estimation is needed, as discussed in Section S1 of the Supplement.

Figure 6.1: New York taxi travel time during morning rush hours.

Figure 7.1: Geometry induced by a sum of Wasserstein distance and same-digit-or-not indicator.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Dr. Zhengyuan Zhu and the reviewers for the helpful feedback and discussion. The research was partially supported by NSF grant DMS-2329879.

## References

* [1] Georgios Arvanitidis, Lars Kai Hansen, and Soren Hauberg. Latent space oddity: On the curvature of deep generative models. In _International Conference on Learning Representations_, February 2018.
* [2] Georgios Arvanitidis, Soren Hauberg, and Bernhard Scholkopf. Geometrically enriched latent spaces, August 2020.
* [3] Aurelien Bellet, Amaury Habrard, and Marc Sebban. Metric learning. _Synthesis Lectures on Artificial Intelligence and Machine Learning_, 9(1):1-151, January 2015.
* [4] Adam Berenzweig, Beth Logan, Daniel P.W. Ellis, and Brian Whitman. A large-scale evaluation of acoustic and subjective music-similarity measures. _Computer Music Journal_, 28(2):63-76, June 2004.
* [5] Shuo Chen, Lei Luo, Jian Yang, Chen Gong, Jun Li, and Heng Huang. Curvilinear distance metric learning. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)_, volume 1, pages 539-546 vol. 1, June 2005.
* [7] Ronald R. Coifman and Stephane Lafon. Diffusion maps. _Applied and Computational Harmonic Analysis_, 21(1):5-30, July 2006.
* [8] Jianqing Fan and I. Gijbels. _Local Polynomial Modelling and Its Applications_. Number 66 in Monographs on Statistics and Applied Probability. Chapman & Hall, London ; New York, 1st ed edition, 1996.
* [9] Charles Fefferman, Sergei Ivanov, Matti Lassas, and Hariharan Narayanan. Reconstruction of a Riemannian manifold from noisy intrinsic distances. _SIAM Journal on Mathematics of Data Science_, 2(3):770-808, September 2020.
* [10] Andrea Frome, Yoram Singer, Fei Sha, and Jitendra Malik. Learning globally-consistent local distance functions for shape-based image retrieval and classification. In _2007 IEEE 11th International Conference on Computer Vision_, pages 1-8, October 2007.
* [11] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classification. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 18(6):607-616, June 1996.
* [12] Soren Hauberg, Oren Freifeld, and Michael J. Black. A geometric take on metric learning. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems 25_, pages 2024-2032. Curran Associates, Inc., 2012.
* [13] Geoffrey E Hinton and Sam Roweis. Stochastic Neighbor Embedding. In _Advances in Neural Information Processing Systems_, volume 15. MIT Press, 2002.
* [14] Martin Jorgensen and Soren Hauberg. Isometric Gaussian process latent variable model for dissimilarity data. In _Proceedings of the 38th International Conference on Machine Learning_, pages 5127-5136. PMLR, July 2021.
* [15] Tam Le and Marco Cuturi. Unsupervised Riemannian metric learning for histograms using Aitchison transformations. In _Proceedings of the 32nd International Conference on Machine Learning_, pages 2002-2011. PMLR, June 2015.
* [16] G. Lebanon. Metric learning for text documents. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 28(4):497-508, April 2006.

* [17] Guy Lebanon. Learning Riemannian metrics. In _Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence_, UAI'03, pages 362-369, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc.
* [18] John M. Lee. _Introduction to Smooth Manifolds_. Number 218 in Graduate Texts in Mathematics. Springer, New York ; London, 2nd ed edition, 2013.
* [19] John M. Lee. _Introduction to Riemannian Manifolds_. Springer Berlin Heidelberg, New York, NY, 2018.
* [20] Wolfgang Meyer. Toponogov's theorem and applications. _Lecture Notes, Trieste_, 1989.
* [21] Dominique Perraul-Joncas and Marina Meila. Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery, May 2013.
* [22] Deva Ramanan and Simon Baker. Local distance functions: A taxonomy, new algorithms, and an evaluation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 33(4):794-806, April 2011.
* [23] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. _Science_, 290(5500):2323-2326, December 2000.
* [24] Christopher Scarvelis and Justin Solomon. Riemannian Metric Learning via Optimal Transport. In _The Eleventh International Conference on Learning Representations_, February 2023.
* [25] Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative comparisons. In _Advances in Neural Information Processing Systems_, volume 16. MIT Press, 2003.
* [26] Juan Luis Suarez, Salvador Garcia, and Francisco Herrera. A tutorial on distance metric learning: Mathematical foundations, algorithms, experimental analysis, prospects and challenges. _Neurocomputing_, 425:300-322, February 2021.
* [27] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. _Science_, 290(5500):2319-2323, December 2000.
* [28] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor classification. _Journal of machine learning research_, 10(2), 2009.
* [29] Eric Xing, Michael Jordan, Stuart J Russell, and Andrew Ng. Distance metric learning with application to clustering with side-information. In S. Becker, S. Thrun, and K. Obermayer, editors, _Advances in Neural Information Processing Systems_, volume 15. MIT Press, 2002.