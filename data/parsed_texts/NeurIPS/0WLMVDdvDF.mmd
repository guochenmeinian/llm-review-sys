# No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions

 Tiancheng Jin

University of Southern California

tiancheng.jin@usc.edu

Equal contribution.

Junyan Liu

University of California, San Diego

jul037@ucsd.edu

Chloe Rouyer

University of Copenhagen

chloe@di.ku.dk

&Chloe Rouyer

University of Copenhagen

chloe@di.ku.dk

&William Chang

University of California, Los Angeles

chang314@g.ucla.edu

Chen-Yu Wei

MIT Institute for Data, Systems, and Society

chenyuw@mit.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

###### Abstract

Existing online learning algorithms for adversarial Markov Decision Processes achieve \(\mathcal{O}(\sqrt{T})\) regret after \(T\) rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) regret where \(C^{\mathsf{P}}\) measures how adversarial the transition functions are and can be at most \(\mathcal{O}(T)\). While this algorithm itself requires knowledge of \(C^{\mathsf{P}}\), we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that further refinements of the algorithm not only maintains the same regret bound, but also simultaneously adapts to easier environments (where losses are generated in a certain stochastically constrained manner as in Jin et al. (2021)) and achieves \(\widetilde{\mathcal{O}}(U+\sqrt{UC^{\mathsf{L}}}+C^{\mathsf{P}})\) regret, where \(U\) is some standard gap-dependent coefficient and \(C^{\mathsf{L}}\) is the amount of corruption on losses.

## 1 Introduction

Markov Decision Processes (MDPs) are widely-used models for reinforcement learning. They are typically studied under a fixed transition function and a fixed loss function, which fails to capture scenarios where the environment is time-evolving or susceptible to adversarial corruption. This motivates many recent studies to overcome these challenges. In particular, one line of research, originated from Even-Dar et al. (2009) and later improved or generalized by e.g. Neu et al. (2010, 2010); Zimin and Neu (2013); Rosenberg and Mansour (2019); Jin et al. (2020), takes inspiration from the online learning literature and considers interacting with a sequence of \(T\) MDPs, each with an adversarially chosen loss function. Despite facing such a challenging environment, the learner can still ensure \(\mathcal{O}(\sqrt{T})\) regret (ignoring other dependence; same below) as shown by these works, that is, the learner's average performance is close to that of the best fixed policy up to \(\mathcal{O}(1/\sqrt{T})\).

However, one caveat of these studies is that they all still require the MDPs to have to the same transition function. This is not for no reason -- Abbasi Yadkori et al. (2013) shows that even with full information feedback, achieving sub-linear regret with adversarial transition functions is computationally hard, and Tian et al. (2021) complements this result by showing that under the more challenging bandit feedback, this goal becomes even information-theoretically impossible without paying exponential dependence on the episode length.

To get around such impossibility results, one natural idea is to allow the regret to depend on some measure of maliciousness \(C^{\mathsf{P}}\) of the transition functions, which is \(0\) when the transitions remain the same over time and \(\mathcal{O}(T)\) in the worst case when they are completely arbitrary. We review several such attempts at the end of this section, and point out here that they all suffer one issue: even when \(C^{\mathsf{P}}=0\), the algorithms developed in these works all suffer _linear regret_ when the loss functions are completely arbitrary, while, as mentioned above, \(\mathcal{O}(\sqrt{T})\) regret is achievable in this case. This begs the question: _when learning with completely adversarial MDPs, is \(\mathcal{O}(\sqrt{T}+C^{\mathsf{P}})\) regret achievable?_

In this work, we not only answer this question affirmatively, but also show that one can perform even better sometimes. More concretely, our results are as follows.

1. In Section 3, we develop a variant of the UOB-REPS algorithm (Jin et al., 2020), achieving \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) regret in completely adversarial environments when \(C^{\mathsf{P}}\) is known. The algorithmic modifications we propose include an enlarged confidence set, using the log-barrier regularizer, and a novel _amortized_ bonus term that leads to a critical "change of measure" effect in the analysis.
2. We then remove the requirement on the knowledge of \(C^{\mathsf{P}}\) in Section 4 by proposing a _black-box reduction_ that turns any algorithm with \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) regret under known \(C^{\mathsf{P}}\) into another algorithm with the same guarantee (up to logarithmic factors) even if \(C^{\mathsf{P}}\) is unknown. Our reduction improves that of Wei et al. (2022) by allowing adversarial losses, which presents extra challenges as discussed in Pacchiano et al. (2022). The idea of our reduction builds on top of previous adversarial model selection framework (a.k.a. Corral (Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022)), but is even more general and is of independent interest: it shows that the requirement from previous work on having a _stable_ input algorithm is actually redundant, since our method can turn _any_ algorithm into a stable one.
3. Finally, in Section 5 we also further refine our algorithm so that it simultaneously adapts to the maliciousness of the loss functions and achieves \(\widetilde{\mathcal{O}}(\min\{\sqrt{T},U+\sqrt{UC^{\mathsf{L}}}\}+C^{\mathsf{P}})\) regret, where \(U\) is some standard gap-dependent coefficient and \(C^{\mathsf{L}}\leq T\) is the amount of corruption on losses (this result unfortunately requires the knowledge of \(\widetilde{C}^{\mathsf{P}}\), but not \(U\) or \(C^{\mathsf{L}}\)). This generalizes the so-called _best-of-both-worlds_ guarantee of Jin and Luo (2020); Jin et al. (2021); Dann et al. (2023) from \(C^{\mathsf{P}}=0\) to any \(C^{\mathsf{P}}\), and is achieved by combining the ideas from Jin et al. (2021) and Ito (2021) with a novel _optimistic transition_ technique. In fact, this technique also leads to improvement on the dependence of episode length even when \(C^{\mathsf{P}}=0\).

Related WorkHere, we review how existing studies deal with adversarially chosen transition functions and how our results compare to theirs. The closest line of research is usually known as _corruption robust_ reinforcement learning (Lykouris et al., 2019; Chen et al., 2021; Zhang et al., 2021; Wei et al., 2022), which assumes a ground truth MDP and measures the maliciousness of the adversary via the amount of corruption to the ground truth -- the amount of corruption is essentially our \(C^{\mathsf{L}}\), while the amount of corruption is essentially our \(C^{\mathsf{P}}\) (these will become clear after we provide their formal definitions later). Naturally, the regret in these works is defined as the difference between the learner's total loss and that of the best policy _with respect to the ground truth MDP_, in which case \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}}+C^{\mathsf{L}})\) regret is unavoidable and is achieved by the state-of-the-art (Wei et al., 2022).

On the other hand, following the canonical definition in online learning, we define regret _with respect to the corrupted MDPs_, in which case \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) is achievable as we show (regardless how large \(C^{\mathsf{L}}\) is). To compare these results, note that the two regret definitions differ from each other by an amount of at most \(\mathcal{O}(C^{\mathsf{P}}+C^{\mathsf{L}})\). Therefore, _our result implies that of Wei et al. (2022), but not vice versa_ -- what Wei et al. (2022) achieves in our definition of regret is again \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}}+C^{\mathsf{L}})\), which is never better than ours and could be \(\Omega(T)\) even when \(C^{\mathsf{P}}=0\).

In fact, our result also improves upon that of Wei et al. (2022) in terms of the gap-dependent refinement -- their refined bound is \(\widetilde{\mathcal{O}}(\min\{\sqrt{T},G\}+C^{\mathsf{P}}+C^{\mathsf{L}})\) for some gap-dependent measure \(G\) that is known to be no less than our gap-dependent measure \(U\); on the other hand, based on earlier discussion, our refined bound _in their regret definition_ is \(\widetilde{\mathcal{O}}(\min\{\sqrt{T},U+\sqrt{UC^{\mathsf{L}}}\}+C^{\mathsf{P}}+C ^{\mathsf{L}})=\widetilde{\mathcal{O}}(\min\{\sqrt{T},U\}+C^{\mathsf{P}}+C^{ \mathsf{L}})\) and thus better. The caveat is that, as mentioned, for this refinement our result requires the knowledge of \(C^{\mathsf{P}}\), but Wei et al. (2022) does not.2 However, we emphasize again that for the gap-independent bound, our result does not require knowledge of \(C^{\mathsf{P}}\) and is achieved via an even more general black-box reduction compared to the reduction of Wei et al. (2022).

Footnote 2: When \(C^{\mathsf{P}}+C^{\mathsf{L}}\) is known, Lykouris et al. (2019) also achieves \(\widetilde{\mathcal{O}}(\min\{\sqrt{T},U\}+C^{\mathsf{P}}+C^{\mathsf{L}})\) regret, but similar to earlier discussions on gap-independent bounds, their regret definition is weaker than ours.

Finally, we mention that another line of research, usually known as _non-stationary reinforcement learning_, also allows arbitrary transition/loss functions and measures the difficulty by either the number of changes in the environment (Auer et al., 2008; Gajane et al., 2018) or some smoother measure such as the total variation across time (Wei and Luo, 2021; Cheung et al., 2023). These results are less comparable to ours since their regret (known as dynamic regret) measures the performance of the learner against the best _sequence_ of policies, while ours (known as static regret) measures the performance against the best _fixed_ policy.

## 2 Preliminaries

We consider the problem of sequentially learning \(T\) episodic MDPs, all with the same state space \(S\) and action space \(A\). We assume without loss of generality (similarly to Jin et al. (2021)) that the state space \(S\) has a layered structure and is partitioned into \(L+1\) subsets \(S_{0},\ldots,S_{L}\) such that \(S_{0}\) and \(S_{L}\) only contain the initial state \(s_{0}\) and the terminal state \(s_{L}\) respectively, and transitions are only possible between consecutive layers. For notational convenience, for any \(k<L\), we denote the set of tuples \(S_{k}\times A\times S_{k+1}\) by \(W_{k}\). We also denote by \(k(s)\) the layer to which state \(s\in S\) belongs.

Ahead of time, knowing the learner's algorithm, the environment decides the transition functions \(\{P_{t}\}_{t=1}^{T}\) and the loss functions \(\{\ell_{t}\}_{t=1}^{T}\) for these \(T\) MDPs in an arbitrary manner (unknown to the learner). Then the learner sequentially interacts with these \(T\) MDPs: for each episode \(t=1,\ldots,T\), the learner first decides a stochastic policy \(\pi_{t}:S\times A\rightarrow[0,1]\) where \(\pi_{t}(a|s)\) is the probability of taking action \(a\) when visiting state \(s\); then, starting from the initial state \(s_{t,0}=s_{0}\), for each \(k=0,\ldots,L-1\), the learner repeatedly selects an action \(a_{t,k}\) sampled from \(\pi_{t}(\cdot|s_{t,k})\), suffers loss \(\ell_{t}(s_{t,k},a_{t,k})\in[0,1]\), and transits to the next state \(s_{t,k+1}\) sampled from \(P_{t}(\cdot|s_{t,k},a_{t,k})\) (until reaching the terminal state); finally, the learner observes the losses of those visited state-action pairs (a.k.a. bandit feedback).

Let \(\ell_{t}(\pi)=\mathbb{E}\big{[}\sum_{h=0}^{L-1}\ell_{t}(s_{h},a_{h})|P_{t},\pi \big{]}\) be the expected loss of executing policy \(\pi\) in the \(t\)-th MDP (that is, \(\{(s_{h},a_{h})\}_{h=0}^{L-1}\) is a stochastic trajectory generated according to transition \(P_{t}\) and policy \(\pi\)). Then, the regret of the learner against any policy \(\pi\) is defined as \(\text{Reg}_{T}(\pi)=\mathbb{E}\big{[}\sum_{t=1}^{T}\ell_{t}(\pi_{t})-\ell_{t}( \pi)\big{]}\). We denote by \(\dot{\pi}\) one of the optimal policies in hindsight such that \(\text{Reg}_{T}(\dot{\pi})=\max_{\pi}\text{Reg}_{T}(\pi)\) and use \(\text{Reg}_{T}\triangleq\text{Reg}_{T}(\dot{\pi})\) as a shorthand.

Maliciousness Measure of the TransitionsIf the transition functions are all the same, Jin et al. (2020) shows that \(\text{Reg}_{T}=\widetilde{\mathcal{O}}(L|S|\sqrt{|A|T})\) is achievable, no matter how the loss functions are decided. However, when the transition functions are also arbitrary, Tian et al. (2021) shows that \(\text{Reg}_{T}=\Omega(\min\{T,\sqrt{2^{L}T}\})\) is unavoidable. Therefore, a natural goal is to allow the regret to smoothly increase from order \(\sqrt{T}\) to \(T\) when some maliciousness measure of the transitions increases. Specifically, the measure we use is

\[C^{\mathsf{P}}\triangleq\min_{P^{\prime}\in\mathcal{P}}\sum_{t=1}^{T}\sum_{k= 0}^{L-1}\max_{(s,a)\in S_{k}\times A}\left\lVert P_{t}(\cdot|s,a)-P^{\prime}( \cdot|s,a)\right\rVert_{1}, \tag{1}\]

where \(\mathcal{P}\) denotes the set of all valid transition functions. Let \(P\) be the transition that realizes the minimum in this definition. Then \(C^{\mathsf{P}}\) can be regarded as the same _corruption_ measure used in Chen et al. (2021); there, it is assumed that a ground truth MDP with transition \(P\) exists, and the adversary corrupts it arbitrarily in each episode to obtain \(P_{t}\), making \(C^{\mathsf{P}}\) the total amount of corruption measured in a certain norm. For simplicity, in the rest of this paper, we will also take this perspective and call \(C^{\mathsf{P}}\) the transition corruption. We also use \(C^{\mathsf{P}}_{\iota}=\sum_{k=0}^{L-1}\max_{(s,a)\in S_{k}\times A}\left\lVert P_{ \iota}(\cdot|s,a)-P(\cdot|s,a)\right\rVert_{1}\) to denote the per-round corruption (so \(C^{\mathsf{P}}=\sum_{t=1}^{T}C^{\mathsf{P}}_{t}\)). It is clear that \(C^{\mathsf{P}}=0\) when the transition stays the same for all MDPs, while in the worst case it is at most \(2TL\). Our goal is to achieve \(\text{Reg}_{T}=\mathcal{O}(\sqrt{T}+C^{\mathsf{P}})\) (ignoring other dependence), which smoothly interpolates between the result of Jin et al. (2020) for \(C^{\mathsf{P}}=0\) and that of Tian et al. (2021) for \(C^{\mathsf{P}}=\mathcal{O}(T)\).

Enlarged Confidence SetA central technique to deal with unknown transitions is to maintain a shrinking confidence set that contains the ground truth with high probability (Rosenberg and Mansour, 2019, 2019, Jin et al., 2020). With a properly enlarged confidence set, the same idea extends to the case with adversarial transitions (Lykouris et al., 2019). Specifically, all our algorithms deploy the following transition estimation procedure. It proceeds in epochs, indexed by \(i=1,2,\cdots\), and each epoch \(i\) includes some consecutive episodes. An epoch ends whenever we encounter a state-action pair whose total number of visits doubles itself when compared to the beginning of that epoch. At the beginning of each epoch \(i\), we calculate an empirical transition \(\bar{P}_{i}\) as:

\[\bar{P}_{i}(s^{\prime}|s,a)=m_{i}(s,a,s^{\prime})/m_{i}(s,a),\quad\forall(s,a, s^{\prime})\in W_{k},\;k=0,\ldots L-1, \tag{2}\]

where \(m_{i}(s,a)\) and \(m_{i}(s,a,s^{\prime})\) are the total number of visits to \((s,a)\) and \((s,a,s^{\prime})\) prior to epoch \(i\).3 In addition, we calculate the following transition confidence set.

Footnote 3: When \(m_{i}(s,a)=0\), we simply let the transition function to be uniform, that is, \(\bar{P}_{i}(s^{\prime}|s,a)=1/\left|S_{k(s^{\prime})}\right|\).

**Definition 2.1**.: _(Confidence Set of Transition Functions) Let \(\delta\in(0,1)\) be a confidence parameter. With known corruption \(C^{\mathsf{P}}\), we define the confidence set of transition functions for epoch \(i\) as_

\[\mathcal{P}_{i}=\Big{\{}P\in\mathcal{P}:\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^ {\prime}|s,a)\right|\leq B_{i}(s,a,s^{\prime}),\;\forall(s,a,s^{\prime})\in W_ {k},k=0,\ldots,L-1\Big{\}}, \tag{3}\]

_where the confidence interval \(B_{i}(s,a,s^{\prime})\) is defined, with \(\iota=\nicefrac{{|S||A|T}}{{\delta}}\) as_

\[B_{i}(s,a,s^{\prime})=\min\left\{1,16\sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a) \log{(\iota)}}{m_{i}(s,a)}}+64\cdot\frac{C^{\mathsf{P}}+\log{(\iota)}}{m_{i}(s,a)}\right\}. \tag{4}\]

Note that the confidence interval is enlarged according to how large the corruption \(C^{\mathsf{P}}\) is. We denote by \(\mathcal{E}_{\textsc{con}}\) the event that \(P\in\mathcal{P}_{i}\) for all epoch \(i\), which is guaranteed to happen with high-probability.

**Lemma 2.2**.: _With probability at least \(1-2\delta\), the event \(\mathcal{E}_{\textsc{con}}\) holds._

Occupancy Measure and Upper Occupancy BoundSimilar to previous work (Rosenberg and Mansour, 2019, Jin et al., 2020), given a stochastic policy \(\pi\) and a transition function \(\bar{P}\), we define the occupancy measure \(q^{P,\pi}\) as the mapping from \(S\times A\times S\) to \([0,1]\) such that \(q^{\bar{P},\pi}(s,a,s^{\prime})\) is the probability of visiting \((s,a,s^{\prime})\) when executing policy \(\pi\) in an MDP with transition \(\bar{P}\). Further define \(q^{\bar{P},\pi}(s,a)=\sum_{s^{\prime}\in S}q^{\bar{P},\pi}(s,a,s^{\prime})\) (the probability of visiting \((s,a)\)) and \(q^{\bar{P},\pi}(s)=\sum_{a\in A}q^{\bar{P},\pi}(s,a)\) (the probability of visiting \(s\)). Given an occupancy measure \(q\), the corresponding policy that defines it, denoted by \(\pi^{q}\), can be extracted via \(\pi^{q}(a|s)\propto q(s,a)\).

Importantly, the expected loss \(\ell_{t}(\pi)\) defined earlier equals \(\left\langle q^{P_{t},\pi},\ell_{t}\right\rangle\), making our problem a variant of online linear optimization and enabling the usage of standard algorithmic frameworks such as Online Mirror Descent (OMD) or Follow-the-Regularized-Leader (FTRL). These frameworks operate over a set of occupancy measures in the form of either \(\Omega(\bar{P})=\{q^{\bar{P},\pi}:\,\pi\text{ is a stochastic policy}\}\) for some transition function \(\bar{P}\), or \(\Omega(\bar{\mathcal{P}})=\{q^{\bar{P},\pi}:\bar{P}\in\bar{\mathcal{P}},\pi\text { is a stochastic policy}\}\) for some set of transition functions \(\bar{\mathcal{P}}\).

Following Jin et al. (2020), to handle partial feedback on the loss function \(\ell_{t}\), we need to construct loss estimators \(\widehat{\ell}_{t}\) using the (efficiently computable) _upper occupancy bound_\(u_{t}\):

\[\widehat{\ell}_{t}(s,a)=\frac{\mathbb{I}_{t}(s,a)\ell_{t}(s,a)}{u_{t}(s,a)}, \text{ where }u_{t}(s,a)=\max_{\bar{P}\in\mathcal{P}_{i(t)}}q^{\bar{P},\pi_{t}}(s,a), \tag{5}\]

\(\mathbb{I}_{t}(s,a)\) is \(1\) if \((s,a)\) is visited during episode \(t\) (so that \(\ell_{t}(s,a)\) is revealed), and \(0\) otherwise, and \(i(t)\) denotes the epoch index to which episode \(t\) belongs. We also define \(u_{t}(s)=\sum_{a\in A}u_{t}(s,a)\).

## 3 Achieving \(\mathcal{O}(\sqrt{T}+C^{\mathsf{P}})\) with Known \(C^{\mathsf{P}}\)

As the first step, we develop an algorithm that achieves our goal when \(C^{\mathsf{P}}\) is known. To introduce our solution, we first briefly review the UOB-REPS algorithm of Jin et al. (2020) (designed for \(C^{\mathsf{P}}=0\)) and point out why simply using the enlarged confidence set Eq.3 when \(C^{\mathsf{P}}\neq 0\) is far away from solving the problem. Specifically, UOB-REPS maintains a sequence of occupancy measures \(\{\widehat{q}_{t}\}_{t=1}^{T}\) via OMD: \(\widehat{q}_{t+1}=\operatorname*{argmin}_{q\in\Omega(\mathcal{P}_{(t+1)})} \eta\langle q,\widehat{\ell}_{t}\rangle+D_{\phi}(q,\widehat{q}_{t})\). Here, \(\eta>0\) is a learning rate, \(\widehat{\ell}_{t}\) is the loss estimator defined in Eq.5, \(\phi\) is the negative entropy regularizer, and \(D_{\phi}\) is the corresponding Bregman divergence.4 With \(\widehat{q}_{t}\) at hand, in episode \(t\), the learner simply executes \(\pi_{t}=\pi^{\widehat{q}_{t}}\). Standard analysis of OMD ensures a bound on the estimated regret \(\textsc{Reg}=\mathbb{E}[\sum_{t}\langle\widehat{q}_{t}-q^{P,\tilde{\pi}}, \widehat{\ell}_{t}\rangle]\), and the rest of the analysis of Jin et al. (2020) boils down to bounding the difference between Reg and Reg\({}_{T}\).

Footnote 4: The original loss estimator of Jin et al. (2020) is slightly different, but that difference is only for the purpose of obtaining a high probability regret guarantee, which we do not consider in this work for simplicity.

First IssueThis difference between Reg and Reg\({}_{T}\) leads to the first issue when one tries to analyze UOB-REPS against adversarial transitions -- it contains the following bias term that measures the difference between the optimal policy's estimated loss and its true loss:

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\tilde{\pi}}, \widehat{\ell}_{t}-\ell_{t}\right\rangle\right]=\mathbb{E}\left[\sum_{t=1}^{T} \sum_{s,a}q^{P,\tilde{\pi}}(s,a)\ell_{t}(s,a)\left(\frac{q^{P_{t},\pi_{t}}(s,a )-u_{t}(s,a)}{u_{t}(s,a)}\right)\right]. \tag{6}\]

When \(C^{\mathsf{P}}=0\), we have \(P=P_{t}\), and thus under the high probability event \(\mathcal{E}_{\textsc{conv}}\) and by the definition of upper occupancy bound, we know \(q^{P_{t},\pi_{t}}(s,a)\leq u_{t}(s,a)\), making Eq.6 negligible. However, this argument breaks when \(C^{\mathsf{P}}\neq 0\) and \(P\neq P_{t}\). In fact, \(P_{t}\) can be highly different from any transitions in \(\mathcal{P}_{i(t)}\) with respect to which \(u_{t}\) is defined, making Eq.6 potentially huge.

Solution: Change of Measure via Amortized BonusesGiven that \(q^{P,\pi_{t}}(s,a)\leq u_{t}(s,a)\) does still hold with high probability, Eq.6 is (approximately) bounded by

\[\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s,a}q^{P,\tilde{\pi}}(s,a) \frac{|q^{P_{t},\pi_{t}}(s,a)-q^{P,\pi_{t}}(s,a)|}{u_{t}(s,a)}\right]=\mathbb{E }\left[\sum_{t=1}^{T}\sum_{s}q^{P,\tilde{\pi}}(s)\frac{|q^{P_{t},\pi_{t}}(s)-q ^{P,\pi_{t}}(s)|}{u_{t}(s)}\right]\]

which is at most \(\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s}q^{P,\tilde{\pi}}(s)\frac{C^{\mathsf{P}} _{t}}{u_{t}(s)}\right]\) since \(\left|q^{P_{t},\pi_{t}}(s)-q^{P,\pi_{t}}(s)\right|\) is bounded by the per-round corruption \(C^{\mathsf{P}}_{t}\) (see CorollaryD.3.6). While this quantity is potentially huge, if we could "change the measure" from \(q^{P,\tilde{\pi}}\) to \(\widehat{q}_{t}\), then the resulting quantity \(\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s}\widehat{q}_{t}(s)\frac{C^{\mathsf{P}} _{t}}{u_{t}(s)}\right]\) is at most \(|S|C^{\mathsf{P}}\) since \(\widehat{q}_{t}(s)\leq u_{t}(s)\) by definition. The general idea of such a change of measure has been extensively used in the online learning literature (see Luo et al. (2021) in a most related context) and can be realized by changing the loss fed to OMD from \(\widehat{\ell}_{t}\) to \(\widehat{\ell}_{t}-b_{t}\) for some bonus term \(b_{t}\), which, in our case, should satisfy \(b_{t}(s,a)\approx\frac{C^{\mathsf{P}}_{t}}{u_{t}(s)}\). However, the challenge here is that \(C^{\mathsf{P}}_{t}\) is _unknown_!

Our solution is to introduce a type of efficiently computable _amortized bonuses_ that do not change the measure per round, but do so overall. Specifically, our amortized bonus \(b_{t}\) is defined as

\[b_{t}(s,a)=\begin{cases}\frac{aL}{u_{t}(s)}&\text{if }\sum_{\tau=1}^{t} \mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u_{t}(s)\rceil\} \leq\frac{C^{\mathsf{P}}}{2L},\\ 0&\text{else},\end{cases} \tag{7}\]

which we also write as \(b_{t}(s)\) since it is independent of \(a\). To understand this definition, note that \(-\lceil\log_{2}u_{t}(s)\rceil\) is exactly the unique integer \(j\) such that \(u_{t}(s)\) falls into the bin \((2^{-j-1},2^{-j}]\). Therefore, the expression \(\sum_{\tau=1}^{t}\mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u_{t }(s)\rceil\}\) counts, among all previous rounds \(\tau=1,\dots,t\), how many times we have encountered a \(u_{\tau}(s)\) value that falls into the same bin as \(u_{t}(s)\). If this number does not exceed \(\frac{C^{\mathsf{P}}}{2L}\), we apply a bonus of \(\frac{4L}{u_{t}(s)}\), which is (two times of) the maximum possible value of the unknown quantity \(\frac{C^{\mathsf{P}}_{t}}{u_{t}(s)}\); otherwise, we do not apply any bonus. The idea is that by enlarging the bonus to it maximum value and stopping it after enough times, even though each \(b_{t}(s)\) might be quite different from \(\frac{C^{\mathsf{P}}_{t}}{u_{t}(s)}\), overall they behave similarly after \(T\) episodes:

**Lemma 3.1**.: _The amortized bonus defined in Eq. (7) satisfies \(\sum_{t=1}^{T}\frac{C^{\mathsf{P}}_{t}}{u_{t}(s)}\leq\sum_{t=1}^{T}b_{t}(s)\) and \(\sum_{t=1}^{T}\widehat{q}_{t}(s)b_{t}(s)=\mathcal{O}(C^{P}\log T)\) for any \(s\)._

Therefore, the problematic term Eq. (6) is at most \(\mathbb{E}[\sum_{t}\langle q^{P,\hat{\pi}},b_{t}\rangle]\), which, if "converted" to \(\mathbb{E}[\sum_{t}\langle\widehat{q}_{t},b_{t}\rangle]\) (change of measure), is nicely bounded by \(\mathcal{O}(|S|C^{P}\log T)\). As mentioned, such a change of measure can be realized by feeding \(\widehat{\ell}_{t}-b_{t}\) instead of \(\widehat{\ell}_{t}\) to OMD, because now standard analysis of OMD ensures a bound on \(\textsc{Reg}=\mathbb{E}[\sum_{t}\langle\widehat{q}_{t}-q^{P,\hat{\pi}}, \widehat{\ell}_{t}-b_{t}\rangle]\), which, compared to the earlier definition of Reg, leads to a difference of \(\mathbb{E}[\sum_{t}\langle\widehat{q}_{t}-q^{P,\hat{\pi}},b_{t}\rangle]\) (see Appendix A.5 for details).

Second IssueThe second issue comes from analyzing Reg (which exists even if no bonuses are used). Specifically, standard analysis of OMD requires bounding a "stability" term, which, for the negative entropy regularizer, is in the form of \(\mathbb{E}[\sum_{t}\sum_{s,a}\widehat{q}_{t}(s,a)\widehat{\ell}_{t}(s,a)^{2}] =\mathbb{E}[\sum_{t}\sum_{s,a}\widehat{q}_{t}(s,a)\frac{q^{P_{t},\pi_{t}}(s,a )\ell_{t}(s,a)^{2}}{u_{t}(s,a)^{2}}]\leq\mathbb{E}[\sum_{t}\sum_{s,a}\frac{q^{P _{t},\pi_{t}}(s,a)}{u_{t}(s,a)}]\). Once again, when \(C^{\mathsf{P}}=0\) and \(P_{t}=P\), we have \(q^{P_{t},\pi_{t}}\) bounded by \(u_{t}(s,a)\) with high probability, and thus the stability term is \(\mathcal{O}(T|S||A|)\); but this breaks if \(C^{\mathsf{P}}\neq 0\) and \(P_{t}\) can be arbitrarily different from transitions in \(\mathcal{P}_{i(t)}\).

Solution: Log-Barrier RegularizerResolving this second issue, however, is relatively straightforward -- it suffices to switch the regularizer from negative entropy to log-barrier: \(\phi(q)=-\sum_{k=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{k}}\log q(s,a,s^{\prime})\), which is first used by Lee et al. (2020) in the context of learning adversarial MDPs but dates back to earlier work such as Foster et al. (2016) for multi-armed bandits. An important property of log-barrier is that it leads to a smaller stability term in the form of \(\mathbb{E}[\sum_{s,a}\widehat{q}_{t}(s,a)^{2}\widehat{\ell}_{t}(s,a)^{2}]\) (with an extra \(\widehat{q}_{t}(s,a)\)), which is at most \(\mathbb{E}[\sum_{t}\sum_{s,a}q^{P_{t},\pi_{t}}(s,a)\ell_{t}(s,a)^{2}]=\mathcal{ O}(TL)\) since \(\widehat{q}_{t}(s,a)\leq u_{t}(s,a)\). In fact, this also helps control the extra stability term when bonuses are used, which is in the form of \(\mathbb{E}[\sum_{t}\sum_{s,a}\widehat{q}_{t}(s,a)^{2}b_{t}(s,a)^{2}]\) and is at most \(4LE[\sum_{t}\langle\widehat{q}_{t},b_{t}\rangle]=\mathcal{O}(L|S|C^{\mathsf{P }}\log T)\) according to Lemma 3.1.

Putting these two ideas together leads to our final algorithm (see Algorithm 1). We prove the following regret bound in Appendix A, which recovers that of Jin et al. (2020) when \(C^{\mathsf{P}}=0\) and increases linearly in \(C^{\mathsf{P}}\) as desired.

**Theorem 3.2**.: _With \(\delta=\nicefrac{{1}}{{T}}\) and \(\eta=\min\left\{\sqrt{\frac{|S|^{2}|A|\log(\iota)}{LT}},\frac{1}{8L}\right\}\), Algorithm 1 ensures_

\[\operatorname{Reg}_{T}=\mathcal{O}\left(L|S|\sqrt{|A|T\log(\iota)}+L|S|^{4}|A| \log^{2}(\iota)+C^{\mathcal{P}}\!L|S|^{4}|A|\log(\iota)\right).\]

## 4 Achieving \(\mathcal{O}(\sqrt{T}+C^{\mathbf{P}})\) with Unknown \(C^{\mathbf{P}}\)

In this section, we address the case when the amount of corruption is unknown. We develop a black-box reduction which turns an algorithm that only deals with known \(C^{\mathbf{P}}\) to one that handles unknown \(C^{\mathbf{P}}\). This is similar to Wei et al. (2022) but additionally handles adversarial losses using a different approach. A byproduct of our reduction is that we develop an entirely _black-box_ model selection approach for adversarial online learning problems, as opposed to the _gray-box_ approach developed by the "Corral" literature (Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022) which requires checking if the base algorithm is _stable_. To achieve this, we essentially develop another layer of reduction that turns any standard algorithm with sublinear regret into a stable algorithm. This result itself might be of independent interest and useful for solving other model selection problems.

More specifically, our reduction has two layers. The bottom layer is where our novelty lies: it takes as input an arbitrary corruption-robust algorithm that operates under known \(C^{\mathbf{P}}\) (e.g., the one we developed in Section 3), and outputs a _stable_ corruption-robust algorithm (formally defined later) that still operates under known \(C^{\mathbf{P}}\). The top layer, on the other hand, follows the standard Corral idea and takes as input a stable algorithm that operates under known \(C^{\mathbf{P}}\), and outputs an algorithm that operates under unknown \(C^{\mathbf{P}}\). Below, we explain these two layers of reduction in details.

Bottom Layer (from an Arbitrary Algorithm to a Stable Algorithm)The input of the bottom layer is an arbitrary corruption-robust algorithm, formally defined as:

**Definition 4.1**.: _An adversarial MDP algorithm is corruption-robust if it takes \(\theta\) (a guess on the corruption amount) as input, and achieves the following regret for any random stopping time \(t^{\prime}\leq T\):_

\[\max_{\pi}\mathbb{E}\left[\sum_{t=1}^{t^{\prime}}\left(\ell_{t}(\pi_{t})-\ell_ {t}(\pi)\right)\right]\leq\mathbb{E}\left[\sqrt{\beta_{1}t^{\prime}}+(\beta_{2 }+\beta_{3}\theta)\mathbb{I}\{t^{\prime}\geq 1\}\right]+\Pr[C^{\mathbf{P}}_{1:t^{ \prime}}>\theta]LT\]

_for problem-dependent constants and \(\log(T)\) factors \(\beta_{1}\geq L^{2},\beta_{2}\geq L,\beta_{3}\geq 1\), where \(C^{\mathbf{P}}_{1:t^{\prime}}=\sum_{\tau=1}^{t^{\prime}}C^{\mathbf{P}}_{\tau}\) is the total corruption up to time \(t^{\prime}\)._

While the regret bound in Definition 4.1 might look cumbersome, it is in fact fairly reasonable: if the guess \(\theta\) is not smaller than the true corruption amount, the regret should be of order \(\sqrt{t^{\prime}}+\theta\); otherwise, the regret bound is vacuous since \(LT\) is its largest possible value. The only extra requirement is that the algorithm needs to be _anytime_ (i.e., the regret bound holds for any stopping time \(t^{\prime}\)), but even this is known to be easily achievable by using a doubling trick over a fixed-time algorithm. It is then clear that our algorithm in Section 3 (together with a doubling trick) indeed satisfies Definition 4.1.

As mentioned, the output of the bottom layer is a stable robust algorithm. To characterize stability, we follow Agarwal et al. (2017) and define a new learning protocol that abstracts the interaction between the output algorithm of the bottom layer and the master algorithm from the top layer:

**Protocol 1**.: In every round \(t\), before the learner makes a decision, a probability \(w_{t}\in[0,1]\) is revealed to the learner. After making a decision, the learner sees the desired feedback from the environment with probability \(w_{t}\), and sees nothing with probability \(1-w_{t}\).

In such a learning protocol, Agarwal et al. (2017) defines a stable algorithm as one whose regret smoothly degrades with \(\rho_{T}=\frac{1}{\min_{t\in[T]}w_{t}}\). For our purpose here, we additionally require that the dependence on \(C^{\mathbf{P}}\) in the regret bound is linear, which results in the following definition:

**Definition 4.2** (\(\frac{1}{2}\)-stable corruption-robust algorithm).: _A \(\frac{1}{2}\)-stable corruption-robust algorithm is one that, with prior knowledge on \(C^{\mathbf{P}}\), achieves \(\operatorname{Reg}_{T}\leq\mathbb{E}\left[\sqrt{\beta_{1}\rho_{T}T}+\beta_{2} \rho_{T}\right]+\beta_{3}C^{\mathbf{P}}\) under Protocol 1 for problem-dependent constants and \(\log(T)\) factors \(\beta_{1}\geq L^{2},\beta_{2}\geq L\), and \(\beta_{3}\geq 1\)._

For simplicity, we only define and discuss the \(\frac{1}{2}\)-stability notion here (the parameter \(\frac{1}{2}\) refers to the exponent of \(T\)), but our result can be straightforwardly extended to the general \(\alpha\)-stability notion for\(\alpha\in[\frac{1}{2},1)\) as in Agarwal et al. (2017). Our main result in this section is then that one can convert any corruption-robust algorithm into a \(\frac{1}{2}\)-stable corruption-robust algorithm:

**Theorem 4.3**.: _If an algorithm is corruption robust according to Definition 4.1 for some constants \((\beta_{1},\beta_{2},\beta_{3})\), then one can convert it to a \(\frac{1}{2}\)-stable corruption-robust algorithm (Definition 4.2) with constants \((\beta^{\prime}_{1},\beta^{\prime}_{2},\beta^{\prime}_{3})\) where \(\beta^{\prime}_{1}=\mathcal{O}(\beta_{1}\log T),\;\beta^{\prime}_{2}=\mathcal{ O}(\beta_{2}+\beta_{3}L\log T)\), and \(\beta^{\prime}_{3}=\mathcal{O}(\beta_{3}\log T)\)._

This conversion is achieved by a procedure that we call STABILISE (see Algorithm 2 for details). The high-level idea of STABILISE is as follows. Noticing that the challenge when learning in Protocol 1 is that \(w_{t}\) varies over time, we discretize the value of \(w_{t}\) and instantiate one instance of the input algorithm to deal with one possible discretized value, so that it is learning in Protocol 1 but with a _fixed_\(w_{t}\), making it straightforward to bound its regret based on what it promises in Definition 4.1.

More concretely, STABILISE instantiates \(\mathcal{O}(\log_{2}T)\) instances \(\{\mathsf{ALG}_{j}\}_{j=0}^{\lceil\log_{2}T\rceil}\) of the input algorithm that satisfies Definition 4.1, each with a different parameter \(\theta_{j}\). Upon receiving \(w_{t}\) from the environment, it dispatches round \(t\) to the \(j\)-th instance where \(j\) is such that \(w_{t}\in(2^{-j-1},2^{-j}]\), and uses the policy generated by \(\mathsf{ALG}_{j}\) to interact with the environment (if \(w_{t}\leq\frac{1}{T}\), simply ignore this round). Based on Protocol 1, the feedback for this round is received with probability \(w_{t}\). To _equalize_ the probability of \(\mathsf{ALG}_{j}\) receiving feedback as mentioned in the high-level idea, when the feedback is actually obtained, STABILISE sends it to \(\mathsf{ALG}_{j}\) only with probability \(\frac{2^{-j-1}}{w_{t}}\) (and discards it otherwise). This way, every time \(\mathsf{ALG}_{j}\) is assigned to a round, it always receives the desired feedback with probability \(w_{t}\cdot\frac{2^{-j-1}}{w_{t}}=2^{-j-1}\). This equalization step is the key that allows us to use the original guarantee of the base algorithm (Definition 4.1) and run it as it is, without requiring it to perform extra importance weighting steps as in Agarwal et al. (2017).

The choice of \(\theta_{j}\) is crucial in making sure that STABILISE only has \(C^{\mathsf{P}}\) regret overhead instead of \(\rho_{T}C^{\mathsf{P}}\). Since \(\mathsf{ALG}_{j}\) only receives feedback with probability \(2^{-j-1}\), the expected total corruption it experiences is on the order of \(2^{-j-1}C^{\mathsf{P}}\). Therefore, its input parameter \(\theta_{j}\) only needs to be of this order instead of the total corruption \(C^{\mathsf{P}}\). This is similar to the key idea of Wei et al. (2022) and Lykouris et al. (2018). See Appendix B.1 for more details and the full proof of Theorem 4.3.

Top Layer (from Known \(C^{\mathsf{P}}\) to Unknown \(C^{\mathsf{P}}\))With a stable algorithm and a regret guarantee in Definition 4.2, it is relatively standard to convert it to an algorithm with \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) regret without knowing \(C^{\mathsf{P}}\). Similar arguments have been made in Foster et al. (2020), and the idea is to have another specially designed OMD/FTRL-based master algorithm to choose on the fly among a set of instances of this stable base algorithm, each with a different guess on \(C^{\mathsf{P}}\) (the probability \(w_{t}\) in Protocol 1 is then decided by this master algorithm). We defer all details to Appendix B. The final regret guarantee is the following (\(\widetilde{\mathcal{O}}(\cdot)\) hides \(\log(T)\) factors).

**Theorem 4.4**.: _Using an algorithm satisfying Definition 4.2 as a base algorithm, Algorithm 3 (in the appendix) ensures \(\operatorname{Reg}_{T}=\tilde{\mathcal{O}}\left(\sqrt{\beta_{1}T}+\beta_{2}+\beta _{3}C^{\mathsf{P}}\right)\) without knowing \(C^{\mathsf{P}}\)._

## 5 Gap-Dependent Refinements with Known \(C^{\mathsf{P}}\)

Finally, we discuss how to further improve our algorithm so that it adapts to easier environments and enjoys a better bound when the loss functions satisfy a certain gap condition, while still maintaining the \(\mathcal{O}(\sqrt{T}+C^{\mathsf{P}})\) robustness guarantee. This result unfortunately requires the knowledge of \(C^{\mathsf{P}}\) because the black-box approach introduced in the last section leads to \(\sqrt{T}\) regret overhead already. We leave the possibility of removing this limitation for future work.

More concretely, following prior work such as Jin and Luo (2020), we consider the following general condition: there exists a mapping \(\pi^{\star}:S\to A\), a gap function \(\Delta:S\times A\rightarrow(0,L]\), and a constant \(C^{\mathsf{L}}\geq 0\), such that for any policies \(\pi_{1},\ldots,\pi_{T}\) generated by the learner, we have

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\pi_{t}}-q^{P,\pi^{\star}}, \ell_{t}\right\rangle\right]\geq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq\kappa _{L}}\sum_{a\neq\pi^{\star}(s)}q^{P,\pi_{t}}(s,a)\Delta(s,a)\right]-C^{ \mathsf{L}}. \tag{9}\]

It has been shown that this condition subsumes the case when the loss functions are drawn from a fixed distribution (in which case \(\pi^{\star}\) is simply the optimal policy with respect to the loss mean and \(P\), \(\Delta\) is the gap function with respect to the optimal \(Q\)-function, and \(C^{\mathsf{L}}=0\)), or further corrupted by an adversary in an arbitrary manner subject to a budget of \(C^{\mathsf{L}}\); we refer the readers to Jin and Luo (2020) for detailed explanation. Our main result for this section is a novel algorithm (whose pseudocode is deferred to Appendix C due to space limit) that achieves the following best-of-both-world guarantee.

**Theorem 5.1**.: _Algorithm 4 (with \(\delta=\nicefrac{{1}}{{T^{2}}}\) and \(\gamma_{t}\) defined as in Definition 5.2) ensures_

\[\operatorname{Reg}_{T}(\mathring{\pi})=\mathcal{O}\left(L^{2}|S||A|\log\left( \iota\right)\sqrt{T}+\left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|^{2}\log^{2} \left(\iota\right)\right)\]

_always, and simultaneously the following gap-dependent bound under Condition (9):_

\[\operatorname{Reg}_{T}(\pi^{\star})=\mathcal{O}\left(U+\sqrt{UC^{\mathsf{L}}}+ \left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|^{2}\log^{2}(\iota)\right),\]

_where \(U=\frac{L^{3}|S|^{2}|A|\log^{2}(\iota)}{\Delta_{\text{\tiny{min}}}}+\sum_{s \neq s_{L}}\sum_{a\neq\pi^{\star}(s)}\frac{L^{2}|S||A|\log^{2}(\iota)}{\Delta (s,a)}\) and \(\Delta_{\text{\tiny{min}}}=\min_{s\neq s_{L},a\neq\pi^{\star}(s)}\Delta(s,a)\)._

Aside from having larger dependence on parameters \(L\), \(S\), and \(A\), Algorithm 4 maintains the same \(\mathcal{O}(\sqrt{T}+C^{\mathsf{P}})\) regret as before, no matter how losses/transitions are generated; additionally, the \(\sqrt{T}\) part can be significantly improved to \(\mathcal{O}(U+\sqrt{UC^{\mathsf{L}}})\) (which can be of order only \(\log^{2}T\) when \(C^{\mathsf{L}}\) is small) under Condition (9). This result not only generalizes that of Jin et al. (2021); Dann et al. (2023) from \(C^{\mathsf{P}}=0\) to any \(C^{\mathsf{P}}\), but in fact also improves their results by having smaller dependence on \(L\) in the definition of \(U\). In the rest of this section, we describe the main ideas of our algorithm.

FTRL with Epoch ScheduleOur algorithm follows a line of research originated from Wei and Luo (2018); Zimmert and Seldin (2019) for multi-armed bandits and uses FTRL (instead of OMD) together with a certain self-bounding analysis technique. Since FTRL does not deal with varying decision sets easily, similar to Jin et al. (2021), we restart FTRL from scratch at the beginning of each epoch \(i\) (recall the epoch schedule described in Section 2). More specifically, in an episode \(t\) that belongs to epoch \(i\), we now compute \(\widehat{q}_{t}\) as \(\operatorname*{argmin}_{q}\left\langle q,\sum_{\tau=t_{i}}^{t-1}(\widehat{ \ell}_{\tau}-b_{\tau})\right\rangle+\phi_{t}(q)\), where \(t_{i}\) is the first episode of epoch \(i\), \(\widehat{\ell}_{t}\) is the same loss estimator defined in Eq. (5), \(b_{t}\) is the amortized bonus defined in Eq. (7) (except that \(\tau=1\) there is also changed to \(\tau=t_{i}\) due to restarting), \(\phi_{t}\) is a time-varying regularizer to be specified later, and the set that \(q\) is optimized over is also a key element to be discussed next. As before, the learner then simply executes \(\pi_{t}=\pi^{\widehat{q}_{t}}\) for this episode.

Optimistic TransitionAn important idea from Jin et al. (2021) is that if FTRL optimizes \(q\) over \(\Omega(\overline{P}_{i})\) (occupancy measures with respect to a fixed transition \(\widehat{P}_{i}\)) instead of \(\Omega(\mathcal{P}_{i})\) (occupancy measures with respect to a set of plausible transitions) as in UOB-REPS, then a critical _loss-shifting_ technique can be applied in the analysis. However, the algorithm lacks "optimism" when not using a confidence set, which motivates Jin et al. (2021) to instead incorporate optimism by subtractinga bonus term Bonus from the loss estimator (not to be confused with the amortized bonus \(b_{t}\) we propose in this work). Indeed, if we define the value function \(V^{\tilde{P},\pi}(s;\ell)\) as the expected loss one suffers when starting from \(s\) and following \(\pi\) in an MDP with transition \(\tilde{P}\) and loss \(\ell\), then they show that the Bonus term is such that \(V^{\tilde{P}_{i},\pi}(s;\ell-\textsc{Bonus})\leq V^{P,\pi}(s;\ell)\) for any state \(s\) and any loss function \(\ell\), that is, the performance of any policy is never underestimated.

Instead of following the same idea, here, we propose a simpler and better way to incorporate optimism via what we call _optimistic transitions_. Specifically, for each epoch \(i\), we simply define an optimistic transition function \(\tilde{P}_{i}\) such that \(\tilde{P}_{i}(s^{\prime}|s,a)=\max\left\{0,\tilde{P}_{i}(s^{\prime}|s,a)-B_{i }(s,a,s^{\prime})\right\}\) (recall the confidence interval \(B_{i}\) defined in Eq.4). Since this makes \(\sum_{s^{\prime}}\tilde{P}_{i}(s^{\prime}|s,a)\) less than \(1\), we allocate all the remaining probability to the terminal state \(s_{L}\) (which breaks the layer structure but does not really affect anything). This is a form of optimism because reaching the terminate state earlier can only lead to smaller loss. More formally, under the high probability event \(\mathcal{E}_{\textsc{con}}\), we prove \(V^{\tilde{P}_{i},\pi}(s;\ell)\leq V^{P,\pi}(s;\ell)\) for any policy \(\pi\), any state \(s\), and any loss function \(\ell\) (see LemmaC.8.3).

With such an optimistic transition, we simply perform FTRL over \(\Omega(\tilde{P}_{i})\) without adding any additional bonus term (other than \(b_{t}\)), making both the algorithm and the analysis much simpler than Jin et al. (2021). Moreover, it can also be shown that \(V^{\tilde{P}_{i},\pi}(s;\ell-\textsc{Bonus})\leq V^{\tilde{P}_{i},\pi}(s;\ell)\) (see LemmaC.8.4), meaning that while both loss estimation schemes are optimistic, ours is tighter than that of Jin et al. (2021). This eventually leads to the aforementioned improvement in the \(U\) definition.

Time-Varying Log-Barrier RegularizersThe final element to be specified in our algorithm is the time-varying regularizer \(\phi_{t}\). Recall from discussions in Section3 that using log-barrier as the regularizer is critical for bounding some stability terms in the presence of adversarial transitions. We thus consider the following log-barrier regularizer with an adaptive learning rate \(\gamma_{t}:S\times A\to\mathbb{R}_{+}\): \(\phi_{t}(q)=-\sum_{s\neq s_{L}}\sum_{a\in A}\gamma_{t}(s,a)\cdot\log q(s,a)\). The learning rate design requires combining the loss-shifting idea of Jin et al. (2021) and the idea from Ito (2021), the latter of which is the first work to show that with adaptive learning rate tuning, the log-barrier regularizer leads to near-optimal best-of-both-world guarantee for multi-armed bandits.

More specifically, following the same loss-shifting argument of Jin et al. (2021), we first observe that our FTRL update can be equivalently written as

\[\widehat{q}_{t}= \operatorname*{argmin}_{q\in\Omega(\tilde{P}_{i})}\left\langle q,\sum_{\tau=t_{i}}^{t-1}(\widehat{\ell}_{\tau}-b_{\tau})\right\rangle+\phi_{t }(q)=\operatorname*{argmin}_{x\in\Omega(\tilde{P}_{i})}\left\langle q,\sum_{ \tau=t_{i}}^{t-1}(g_{\tau}-b_{\tau})\right\rangle+\phi_{t}(q),\]

where \(g_{\tau}(s,a)=Q^{\tilde{P}_{i},\pi_{\tau}}(s,a;\widehat{\ell}_{\tau})-V^{ \tilde{P}_{i},\pi_{\tau}}(s;\widehat{\ell}_{\tau})\) for any state-action pair \((s,a)\) (\(Q\) is the standard \(Q\)-function; see AppendixC for formal definition). With this perspective, we follow the idea of Ito (2021) and propose the following learning rate schedule:

**Definition 5.2**.: _(Adaptive learning rate for log-barrier) For any \(t\), if it is the starting episode of an epoch, we set \(\gamma_{t}(s,a)=256L^{2}|S|\); otherwise, we set \(\gamma_{t+1}(s,a)=\gamma_{t}(s,a)+\frac{D\nu_{t}(s,a)}{2\gamma_{t}(s,a)}\) where \(D=\nicefrac{{1}}{{\log(t)}}\), \(\nu_{t}(s,a)=q^{\tilde{P}_{t(t)},\pi_{t}}(s,a)^{2}\left(Q^{\tilde{P}_{t(t)}, \pi_{t}}(s,a;\widehat{\ell}_{t})-V^{\tilde{P}_{t(t)},\pi_{t}}(s;\widehat{\ell }_{t})\right)^{2}\), and \(i(t)\) is the epoch index to which episode \(t\) belongs._

Such a learning rate schedule is critical for the analysis in obtaining a certain self-bounding quantity and eventually deriving the gap-dependent bound. This concludes the design of our algorithm; see AppendixC for more details.

## 6 Conclusions

In this work, we propose online RL algorithms that can handle both adversarial losses and adversarial transitions, with regret gracefully degrading in the degree of maliciousness of the adversary. Specifically, we achieve \(\widetilde{\mathcal{O}}(\sqrt{T}+C^{\mathsf{P}})\) regret where \(C^{\mathsf{P}}\) measures how adversarial the transition functions are, even when \(C^{\mathsf{P}}\) is unknown. Moreover, we show that further refinements of the algorithm not only maintain the same regret bound, but also simultaneously adapt to easier environments, with the caveat that \(C^{\mathsf{P}}\) must be known ahead of time. We leave how to further remove this restriction as a key future direction.

## Acknowledgments and Disclosure of Funding

TJ and HL are supported by NSF Award IIS-1943607 and a Google Research Scholar award. CR acknowledges partial support by the Independent Research Fund Denmark, grant number 9040-00361B.

## References

* Y. Abbasi Y. Y. Bartlett, P. L. Bartlett, V. Kanade, Y. Seldin, and C. Szepesvari (2013)Online learning in markov decision processes with adversarially chosen transition probability distributions. In Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1.
* A. Agarwal, H. Luo, B. Neyshabur, and R. E. Schapire (2017)Corralling a band of bandit algorithms. In Proceedings of the International Conference on Computational Learning Theory (COLT), Cited by: SS1.
* P. Auer, T. Jaksch, and R. Ortner (2008)Near-optimal regret bounds for reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* Y. Chen, S. Du, and K. Jamieson (2021)Improved corruption robust algorithms for episodic reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), Cited by: SS1.
* W. C. Cheung, D. Simchi-Levi, and R. Zhu (2023)Nonstationary reinforcement learning: the blessing of (more) optimism. Management Science. Cited by: SS1.
* C. Dann, C. Wei, and J. Zimmert (2023)Best of both worlds policy optimization. arXiv preprint arXiv:2302.09408. Cited by: SS1.
* E. Even-Dar, S. M. Kakade, and Y. Mansour (2009)Online markov decision processes. Mathematics of Operations Research. Cited by: SS1.
* D. J. Foster, Z. Li, T. Lykouris, K. Sridharan, and E. Tardos (2016)Learning in games: robustness of fast convergence. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* D. J. Foster, C. Gentile, M. Mohri, and J. Zimmert (2020)Adapting to misspecification in contextual bandits. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* P. Gajane, R. Ortner, and P. Auer (2018)A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions. arXiv preprint arXiv:1805.10066. Cited by: SS1.
* S. Ito (2021)Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Proceedings of the International Conference on Computational Learning Theory (COLT), Cited by: SS1.
* C. Jin, T. Jin, H. Luo, S. Sra, and T. Yu (2020)Learning adversarial markov decision processes with bandit feedback and unknown transition. In Proceedings of the International Conference on Machine Learning (ICML), Cited by: SS1.
* T. Jin and H. Luo (2020)Simultaneously learning stochastic and adversarial episodic mdps with known transition. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* T. Jin, L. Huang, and H. Luo (2021)The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* S. M. Kakade (2003)On the sample complexity of reinforcement learning. Ph.D. Thesis, University College London. Cited by: SS1.
* C. Lee, H. Luo, C. Wei, and M. Zhang (2020)Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.

* Luo et al. (2021) Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* Luo et al. (2022) Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In _Proceedings of the International Conference on Computational Learning Theory (COLT)_, 2022.
* Lykouris et al. (2018) Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the Annual ACM SIGACT Symposium on Theory of Computing_, 2018.
* Lykouris et al. (2019) Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust exploration in episodic reinforcement learning. _arXiv preprint arXiv:1911.08689_, 2019.
* Neu et al. (2010a) Gergely Neu, Andras Antos, Andras Gyorgy, and Csaba Szepesvari. Online markov decision processes under bandit feedback. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2010a.
* Neu et al. (2010b) Gergely Neu, Andras Gyorgy, and Csaba Szepesvari. The online loop-free stochastic shortest-path problem. In _Proceedings of the International Conference on Computational Learning Theory (COLT)_, 2010b.
* Pacchiano et al. (2022) Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Best of both worlds model selection. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Rosenberg and Mansour (2019a) Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2019a.
* Rosenberg and Mansour (2019b) Aviv Rosenberg and Yishay Mansour. Online stochastic shortest path with bandit feedback and unknown transition function. _Advances in Neural Information Processing Systems (NeurIPS)_, 2019b.
* Tian et al. (2021) Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.
* Wei and Luo (2018) Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In _Proceedings of the International Conference on Computational Learning Theory (COLT)_, 2018.
* Wei and Luo (2021) Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. In _Proceedings of the International Conference on Computational Learning Theory (COLT)_, 2021.
* Wei et al. (2022) Chen-Yu Wei, Christoph Dann, and Julian Zimmert. A model selection approach for corruption robust reinforcement learning. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2022.
* Zhang et al. (2021) Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Robust policy gradient against strong data corruption. In _International Conference on Machine Learning_, pages 12391-12401. PMLR, 2021.
* Zimin and Neu (2013) Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by relative entropy policy search. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2013.
* Zimmert and Seldin (2019) Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2019.

[MISSING_PAGE_EMPTY:13]

Important NotationsThroughout the appendix, we denote the transition associated with the occupancy measure \(\widehat{q}_{t}\) by \(\widehat{P}_{t}\) so that \(\widehat{q}_{t}=q^{\widehat{P}_{t},\pi_{t}}\). Recall \(i(t)\) is the epoch index to which episode \(t\) belongs. Importantly, the notation \(m_{i}(s,a)\) (and similarly \(m_{i}(s,a,a^{\prime})\)), which is a changing variable in the algorithms, denotes the initial value of this counter in all analysis, that is, the total number of visits to \((s,a)\)_prior_ to epoch \(i\). Finally, for convenience, we define \(\widehat{m}_{i}(s,a)=\max\left\{m_{i}(s,a),C^{\mathsf{P}}+\log\left(\iota \right)\right\}\), and it can be verified that the confidence interval, defined in Eq.4 as

\[B_{i}(s,a,s^{\prime})=\min\left\{1,16\sqrt{\frac{\widehat{P}_{i}(s^{\prime}|s, a)\log\left(\iota\right)}{m_{i}(s,a)}}+64\cdot\frac{C^{\mathsf{P}}+\log \left(\iota\right)}{m_{i}(s,a)}\right\},\]

can be equivalently written as

\[B_{i}(s,a,s^{\prime})=\min\left\{1,16\sqrt{\frac{\widehat{P}_{i}(s^{\prime}|s,a)\log\left(\iota\right)}{\widehat{m}_{i}(s,a)}}+64\cdot\frac{C^{\mathsf{P}}+ \log\left(\iota\right)}{\widehat{m}_{i}(s,a)}\right\}\]

since whenever \(\widehat{m}_{i}(s,a)\neq m_{i}(s,a)\), the two definitions both lead to a value of \(1\).

## Appendix A Omitted Details for Section3

In this section, we provide more details of the modified UOB-REPS algorithm as shown in Algorithm1. In the algorithm, the occupancy measure for each episode \(t\) is computed as

\[\widehat{q}_{t+1}=\operatorname*{argmin}_{q\in\Omega\left(\mathcal{P}_{i(t+1) }\right)}\eta\left\langle q,\widehat{\ell}_{t}-b_{t}\right\rangle+D_{\phi}(q, \widehat{q}_{t}), \tag{10}\]

where \(\eta>0\) is the learning rate and \(D_{\phi}(q,q^{\prime})\) is the Bregman divergence defined as

\[D_{\phi}(q,q^{\prime})=\phi(q)-\phi(q^{\prime})-\left\langle\nabla\phi(q^{ \prime}),q-q^{\prime}\right\rangle. \tag{11}\]

The Bregman divergence is induced by the log-barrier regularizer \(\phi\) given as:

\[\phi(q)=\sum_{k=0}^{L-1}\sum_{s\in S_{k}}\sum_{a\in A}\sum_{s^{\prime}\in S_{ k+1}}\log\left(\frac{1}{q(s,a,s^{\prime})}\right). \tag{12}\]

We note that the loss estimator is constructed based upon upper occupancy bound \(u_{t}\), which can be efficiently computed by Comp-UOB (Jin et al., 2020).

In the rest of this section, we prove Theorem3.2 which shows that the expected regret is bounded by

\[\mathcal{O}\left(L|S|\sqrt{|A|T\log(\iota)}+L|S|^{4}|A|\log^{2}(\iota)+C^{ \mathsf{P}}L|S|^{4}|A|\log(\iota)\right).\]

Our analysis starts from a regret decomposition similar to Jin et al. (2020), but with the amortized bonuses taken into account:

\[\text{Reg}_{T} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}}-q^ {P_{t},\hat{\pi}},\ell_{t}\right\rangle\right]\] \[=\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t },\pi_{t}}-q^{\widehat{P}_{t},\pi_{t}},\ell_{t}\right\rangle\right]}_{\text{ Error}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{ \widehat{P}_{t},\pi_{t}},\ell_{t}-\widehat{\ell}_{t}\right\rangle\right]}_{ \text{Bias}_{1}}\] \[\quad+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q ^{\widehat{P}_{t},\pi_{t}}-q^{P,\hat{\pi}},\widehat{\ell}_{t}-b_{t}\right\rangle \right]}_{\text{Reg}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q ^{P,\hat{\pi}},\widehat{\ell}_{t}-\ell_{t}\right\rangle+\sum_{t=1}^{T}\left \langle q^{\widehat{P}_{t},\pi_{t}}-q^{P,\hat{\pi}},b_{t}\right\rangle\right]}_ {\text{Bias}_{2}}\] \[\quad+\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\hat{\pi} }-q^{P_{t},\hat{\pi}},\ell_{t}\right\rangle\right],\]

where the last term can be directly bounded by \(\mathcal{O}\left(LC^{\mathsf{P}}\right)\) using CorollaryD.3.7, and Error, Bias\({}_{1}\), Reg, and Bias\({}_{2}\) are analyzed in AppendixA.2, AppendixA.3, AppendixA.4 and AppendixA.5 respectively.

### Equivalent Definition of Amortized Bonuses

For ease of exposition, we show an alternative definition of the amortized bonus \(b_{t}(s)\), which is equivalent to Eq.7 but is useful for our following analysis. Recall from Eq.7 that

\[b_{t}(s)=b_{t}(s,a)=\begin{cases}\frac{4L}{u_{t}(s)}&\text{if }\sum_{\tau=1}^{t} \mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u_{t}(s)\rceil\} \leq\frac{C^{\mathsf{P}}}{2L},\\ 0&\text{else},\end{cases} \tag{13}\]

In Eq.13, the value of \(b_{t}(s)\) relies on the cumulative sum of \(\mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u_{t}(s)\rceil\}\) and whether the cumulative sum exceeds the threshold \(\frac{C^{\mathsf{P}}}{2L}\). To reconstruct this, we define \(y_{t}(s)\) as the unique integer value \(j=-\lceil\log_{2}u_{t}(s)\rceil\) such that \(u_{t}(s)\in(2^{-j-1},2^{-j}]\) and define \(z_{t}^{j}(s)\) as the total number of times from episode \(\tau=1\) to \(\tau=t\) where \(y_{\tau}(s)=j\) holds. With these definitions, the condition under which \(b_{t}(s)=\frac{4L}{u_{t}(s)}\) can be represented by \(\sum_{j}\mathbb{I}\{y_{t}(s)=j\}\mathbb{I}\left\{z_{t}^{j}(s)\leq\nicefrac{{C ^{\mathsf{P}}}}{{2L}}\right\}\) where the summation is over all integers that \(-\lceil\log_{2}u_{t}(s)\rceil\) can take. Notice that since \(u_{t}(s)\geq\frac{1}{|S|T}\) holds for all \(s\) and \(t\) (see LemmaD.2.8), the largest possible integer value we need to consider is \(\lceil\log_{2}\left(|S|T\right)\rceil\). Therefore, \(b_{t}(s)\) can be equivalently defined as:

\[b_{t}(s)=b_{t}(s,a)=\frac{4L}{u_{t}(s)}\sum_{j=0}^{\lceil\log_{2 }(|S|T)\rceil}\mathbb{I}\{y_{t}(s)=j\}\mathbb{I}\left\{z_{t}^{j}(s)\leq\frac{C ^{\mathsf{P}}}{2L}\right\},\forall s\in S,\forall t\in\{1,\ldots,T\}. \tag{14}\]

### Bounding Error

**Lemma A.2.1** (Bound of Error).: _Algorithm1 ensures_

\[\textsc{Error}=\mathcal{O}\left(L|S|\sqrt{|A|T\log(\iota)}+L|S|^{ 2}|A|\log(\iota)\log(T)+C^{\mathsf{P}}L|S||A|\log(T)+\delta LT\right).\]

Proof.: For this proof, we consider two cases, whether the event \(\mathcal{E}_{\textsc{con}}\wedge\mathcal{E}_{\textsc{est}}\) holds or not, where \(\mathcal{E}_{\textsc{con}}\), defined above Eq.4, and \(\mathcal{E}_{\textsc{est}}\), defined in PropositionD.5.2, are both high probability events.

Suppose that \(\mathcal{E}_{\textsc{con}}\wedge\mathcal{E}_{\textsc{est}}\) holds. We have

\[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}}-q^{\widehat{P}_{t}, \pi_{t}},\ell_{t}\right\rangle\] \[\leq\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sum_{s\in S_{k}}\sum_{a\in A} \left|q^{P_{t},\pi_{t}}(s,a)-q^{\widehat{P}_{t},\pi_{t}}(s,a)\right|\] \[=\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sum_{s\in S_{k}}\left|q^{\widehat {P}_{t},\pi_{t}}(s)-q^{P,\pi_{t}}(s)\right|\] \[\leq\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sum_{s\in S_{k}}\sum_{h=0}^{L- 1}\sum_{u\in S_{h}}\sum_{v\in A}\sum_{w\in S_{h+1}}q^{P_{t},\pi_{t}}(u,v) \left|\widehat{P}_{t}(w|u,v)-P_{t}(w|u,v)\right|q^{\widehat{P}_{t},\pi_{t}}( s|w)\] \[\leq L\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A }q^{P_{t},\pi_{t}}(u,v)\left\|\widehat{P}_{t}(\cdot|u,v)-P_{t}(\cdot|u,v) \right\|_{1}\] \[\leq L\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A }q^{P_{t},\pi_{t}}(u,v)\left(\left\|P_{t}(\cdot|u,v)-P(\cdot|u,v)\right\|_{1} +\left\|\widehat{P}_{t}(\cdot|u,v)-P(\cdot|u,v)\right\|_{1}\right)\] \[\leq L\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A }q^{P_{t},\pi_{t}}(u,v)\left\|\widehat{P}_{t}(\cdot|u,v)-P(\cdot|u,v)\right\|_{ 1}+L\sum_{t=1}^{T}\sum_{h=0}^{L-1}C^{\mathsf{P}}_{t,h}\] \[=L\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A}q^{P _{t},\pi_{t}}(u,v)\left\|\widehat{P}_{t}(\cdot|u,v)-P(\cdot|u,v)\right\|_{1}+LC ^{\mathsf{P}}, \tag{15}\]where the first step follows from the fact that \(\ell_{t}(s,a)\in[0,1]\) for any \((s,a)\); the third step applies Lemma D.3.3; the fourth step rearranges the summation and uses the fact that \(\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a|w)\leq L\); and in the sixth step we define \(C^{\mathsf{P}}_{t,h}=\max_{s\in S_{h},a\in A}\left\lVert P_{t}(\cdot|s,a)-P( \cdot|s,a)\right\rVert_{1}\), so that \(C^{\mathsf{P}}=\sum_{t=1}^{T}\sum_{h=0}^{L-1}C^{\mathsf{P}}_{t,h}\). We continue to bound the first term above as:

\[\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A}q^{P_{t },\pi_{t}}(u,v)\left\lVert\widehat{P}_{t}(\cdot|u,v)-P(\cdot|u,v)\right\rVert_{1}\] \[\leq\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{u\in S_{h}}\sum_{v\in A}q ^{P_{t},\pi_{t}}(u,v)\left(\left\lVert\widehat{P}_{t}(\cdot|u,v)-\bar{P}_{i(t) }(\cdot|u,v)\right\rVert_{1}+\left\lVert\bar{P}_{i(t)}(\cdot|u,v)-P(\cdot|u,v) \right\rVert_{1}\right)\] \[\leq\sum_{h=0}^{L-1}\sum_{t=1}^{T}\sum_{u\in S_{h}}\sum_{v\in A}q ^{P_{t},\pi_{t}}(u,v)\;\mathcal{O}\left(\sqrt{\frac{|S_{h+1}|\log(\iota)}{ \widehat{m}_{i(t)}(u,v)}}+\frac{C^{\mathsf{P}}+|S_{h+1}|\log(\iota)}{\widehat{ m}_{i(t)}(u,v)}\right)\] \[\leq\mathcal{O}\left(\sum_{h=0}^{L-1}\left(\sqrt{|S_{h}||S_{h+1}| |A|T\log(\iota)}+|S_{h}||A|\log(T)\left(|S_{h+1}|\log(\iota)+C^{\mathsf{P}} \right)+\log(\iota)\right)\right)\] \[\leq\mathcal{O}\left(\sum_{h=0}^{L-1}\left(\left(|S_{h}|+|S_{h+1} |\right)\sqrt{|A|T\log(\iota)}+|S_{h}||A|\log(T)\left(|S_{h+1}|\log(\iota)+C^{ \mathsf{P}}\right)+\log(\iota)\right)\right)\] \[\leq\mathcal{O}\left(|S|\sqrt{|A|T\log(\iota)}+|S|^{2}|A|\log( \iota)\log(T)+C^{\mathsf{P}}|S||A|\log(T)\right), \tag{16}\]

where the first step uses the triangle inequality; the second step applies Corollary D.2.6 to bound two norm terms based on the fact that \(\widehat{P}_{t},\bar{P}_{i(t)}\in\mathcal{P}_{i(t)}\); the third step follows the definition of \(\mathcal{E}_{\textsc{Est}}\) from Proposition D.5.2; the fourth step uses the AM-GM inequality. Putting the result of Eq. (16) into Eq. (15) yields the first three terms of the claimed bound.

Now suppose that \(\mathcal{E}_{\textsc{con}}\wedge\mathcal{E}_{\textsc{Est}}\) does not hold. We trivially bound \(\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}}-q^{\widehat{P}_{t},\pi_{t}},\ell_ {t}\right\rangle\) by \(LT\). As the probability that this case occurs is at most \(\mathcal{O}\left(\delta\right)\), this case contributes to at most \(\mathcal{O}\left(\delta LT\right)\) regret. Finally, combining these two cases and applying Lemma D.1.1 finishes the proof. 

### Bounding \(\textsc{Bias}_{1}\)

**Lemma A.3.1** (Bound of \(\textsc{Bias}_{1}\)).: _Algorithm 1 ensures_

\[\textsc{Bias}_{1}=\mathcal{O}\left(L|S|\sqrt{|A|T\log(\iota)}+|S|^{4}|A|\log^{ 2}(\iota)+C^{\mathsf{P}}L|S|^{4}|A|\log(\iota)+\delta LT\right).\]

Proof.: We can rewrite \(\textsc{Bias}_{1}\) as

\[\textsc{Bias}_{1}=\sum_{t=1}^{T}\mathbb{E}\left[\left\langle q^{\widehat{P}_{ t},\pi_{t}},\ell_{t}-\widehat{\ell}_{t}\right\rangle\right]=\sum_{t=1}^{T} \mathbb{E}\left[\left\langle q^{\widehat{P}_{t},\pi_{t}},\mathbb{E}_{t}\left[ \ell_{t}-\widehat{\ell}_{t}\right]\right\rangle\right],\]

where the second step uses the law of total expectation and the fact that \(q^{\widehat{P}_{t},\pi_{t}}\) is deterministic given all the history up to \(t\). For any state-action pair \((s,a)\), we have

\[\mathbb{E}_{t}\left[\ell_{t}(s,a)-\widehat{\ell}_{t}(s,a)\right]=\ell_{t}(s,a) \left(1-\frac{q^{P_{t},\pi_{t}}(s,a)}{u_{t}(s,a)}\right)=\ell_{t}(s,a)\left( \frac{u_{t}(s,a)-q^{P_{t},\pi_{t}}(s,a)}{u_{t}(s,a)}\right).\]

Then, we can further rewrite and bound \(\textsc{Bias}_{1}\) as:

\[\textsc{Bias}_{1} =\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t},\pi_ {t}}(s,a)\ell_{t}(s,a)\left(\frac{u_{t}(s,a)-q^{P_{t},\pi_{t}}(s,a)}{u_{t}(s,a )}\right)\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t}, \pi_{t}}(s,a)\left(\frac{\left|u_{t}(s,a)-q^{P_{t},\pi_{t}}(s,a)\right|}{u_{t}(s,a)}\right)\right]\]\[\leq\mathcal{O}\left(LC^{\mathsf{P}}_{t}+|S|^{3}\sum_{k=0}^{L-1}\big{(} C^{\mathsf{P}}+\log\left(\iota\right)\right)|S_{k}||A|\log\left(\iota\right)\right)\] \[\quad+\mathcal{O}\left(L\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sum_{(u,v) \in S_{k}\times A}q^{P,\pi_{t}}(u,v)\sqrt{\frac{|S_{k+1}|\log\left(\iota\right) }{\widehat{m}_{i(t)}(u,v)}}\right)\]\[\leq\mathcal{O}\left(|S|^{4}|A|\left(C^{\mathsf{P}}+\log\left(\iota \right)\right)\log\left(\iota\right)+L\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sqrt{|S_{k +1}|\log\left(\iota\right)}\left(\sqrt{|S_{k}||A|T}+|S_{k}||A|\log\left(\iota \right)\right)\right)\] \[=\mathcal{O}\left(|S|^{4}|A|\left(C^{\mathsf{P}}+\log\left(\iota \right)\right)\log\left(\iota\right)+L|S|\sqrt{|A|T\log\left(\iota\right)} \right),\]

where the second step uses the Cauchy-Schwartz inequality: \(\sum_{w\in S_{k+1}}\sqrt{P(w|u,v)}\leq\sqrt{|S_{k+1}|}\) and also applies Lemma D.5.3; the fourth step bounds \(\widehat{m}_{i(t)}(u,v)\geq\log(\iota)\) for any \((u,v)\) and \(|S_{k+1}|\leq|S|\); the fifth applies Proposition D.5.2, and Corollary D.3.7 to bound \(\sum_{t=1}^{T}\sum_{k=0}^{L-1}\sum_{(u,v)\in S_{k}\times A}\left|q^{P,\pi_{t} }(u,v)-q^{P_{t},\pi_{t}}(u,v)\right|\) by \(\mathcal{O}\left(LC^{\mathsf{P}}\right)\); the last step uses the fact that \(\sqrt{xy}\leq x+y\) for any \(x,y\geq 0\), and thus we have \(\sum_{k=0}^{L-1}\sqrt{|S_{k}||S_{k+1}|}\leq 2\sum_{k=0}^{L-1}|S_{k}|=2\left|S \right|\).

Finally, using a similar argument in the proof of Lemma A.2.1 to bound the case that \(\mathcal{E}_{\mathrm{con}}\wedge\mathcal{E}_{\mathrm{EST}}\) does not hold, we complete the proof. 

### Bounding Reg

**Lemma A.4.1** (Bound of Reg).: _If learning rate \(\eta\) satisfies \(\eta\in(0,\frac{1}{8L}]\), then, Algorithm 1 ensures_

\[\textsc{Reg}=\mathcal{O}\left(\frac{|S|^{2}|A|\log\left(\iota \right)}{\eta}+\eta\left(L|S|C^{\mathsf{P}}\log T+LT\right)+\delta LT\right).\]

Proof.: We consider a specific transition \(P_{0}\) defined in Lee et al. (2020, Lemma C.4) and occupancy measure \(u\) such that

\[u=\left(1-\frac{1}{T}\right)q^{P,\tilde{\pi}}+\frac{1}{T|A|}\sum_{a\in A}q^{P_ {0},\pi_{a}},\]

where \(\pi_{a}\) is a policy such that action \(a\) is selected at every state.

By direct calculation, we have

\[D_{\phi}(u,\widehat{q_{1}}) =\sum_{k=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{k}}\left(\log\left( \frac{\widehat{q}_{1}(s,a,s^{\prime})}{u(s,a,s^{\prime})}\right)+\frac{u(s,a,s ^{\prime})}{\widehat{q}_{1}(s,a,s^{\prime})}-1\right)\] \[=\sum_{k=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{k}}\log\left(\frac{ \widehat{q}_{1}(s,a,s^{\prime})}{u(s,a,s^{\prime})}\right)+\sum_{k=0}^{L-1} \sum_{(s,a,s^{\prime})\in W_{k}}\left(|S_{k}||A||S_{k+1}|u(s,a,s^{\prime})-1\right)\] \[=\sum_{k=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{k}}\log\left(\frac{ \widehat{q}_{1}(s,a,s^{\prime})}{u(s,a,s^{\prime})}\right)\] \[\leq 3|S|^{2}|A|\log\left(\iota\right),\]

where the second step uses the definition \(\widehat{q}_{1}(s,a,s^{\prime})=\frac{1}{|S_{k}||A||S_{k+1}|}\) for \(k=0,\cdots,L-1\) and the fourth step lower-bounds \(u(s,a,s^{\prime})\geq\frac{1}{T^{3}|S|^{2}|A|}\) from (Lee et al., 2020, Lemma C.10), thereby \(u(s,a,s^{\prime})\geq\frac{1}{\iota^{3}}\), and upper-bounds \(\widehat{q}_{1}(s,a,s^{\prime})\leq 1\).

According to (Lee et al., 2020, Lemma C.4), we have \(q^{P_{0},\pi_{a}}\in\cap_{i}\)\(\Omega\left(\mathcal{P}_{i}\right)\). Therefore, \(u\) is a convex combination of points in that convex set, and we can use Lemma A.4.2 (included after this proof) to show

\[\left\langle q^{\widehat{P}_{t},\pi_{t}}-u,\widehat{\ell}_{t}-b_{t}\right\rangle\]\[\leq\frac{3|S|^{2}|A|\log\left(\iota\right)}{\eta}+2\eta\left(\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}\widehat{\ell}_{t}(s,a)^{2}+\sum_{t=1}^{ T}\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}b_{t}(s)^{2}\right). \tag{17}\]

On the one hand, we bound the first summation in Eq. (17) as

\[\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}\widehat{\ell}_{t}( s,a)^{2}=\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}\cdot\frac{ \ell_{t}(s,a)^{2}\mathbb{I}_{t}\{s,a\}}{u_{t}(s,a)^{2}}\leq LT,\]

since \(q^{\widehat{P}_{t},\pi_{t}}(s,a)\leq u_{t}(s,a)\) by definition of the upper occupancy bound.

On the other hand, we bound the second summation in Eq. (17) as

\[\sum_{t=1}^{T}\sum_{s,a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}b_{t}(s)^{2}\leq 4 L\sum_{t=1}^{T}\sum_{s}q^{\widehat{P}_{t},\pi_{t}}(s)b_{t}(s)=\mathcal{O}\left( L|S|C^{\mathsf{P}}\log T\right), \tag{18}\]

where the first step uses the facts that \(b_{t}(s)^{2}\leq 4Lb_{t}(s)\) and \(\sum_{a}q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}\leq q^{\widehat{P}_{t},\pi_{t}}(s)\), and the last step applies Lemma 3.1.

Putting these inequalities together concludes the proof. 

**Lemma A.4.2**.: _With \(\eta\in(0,\frac{1}{8L}]\), Algorithm 1 ensures_

\[\sum_{t=1}^{T}\Big{\langle}\widehat{q}_{t}-u,\widehat{\ell}_{t}-b_{t}\Big{\rangle} \leq\frac{D_{\phi}(u,\widehat{q}_{1})}{\eta}+2\eta\sum_{t=1}^{T}\sum_{s,a} \widehat{q}_{t}(s,a)^{2}\left(\widehat{\ell}_{t}(s,a)^{2}+b_{t}(s)^{2}\right), \tag{19}\]

_for any \(u\in\cap_{i}\Omega\left(\mathcal{P}_{i}\right)\)._

Proof.: To use the standard analysis of OMD with log-barrier (e.g.,see (Agarwal et al., 2017, Lemma 12)), we need to ensure that \(\eta\widehat{q}_{t}(s,a,s^{\prime})\left(\widehat{\ell}_{t}(s,a)-b_{t}(s) \right)\geq-\frac{1}{2}\), since \(\log\left(1+x\right)\geq x-x^{2}\) holds for any \(x\geq-\frac{1}{2}\). Clearly, by choosing \(\eta\in(0,\frac{1}{8L}]\), we have

\[\eta\widehat{q}_{t}(s,a,s^{\prime})\left(\widehat{\ell}_{t}(s,a)-b _{t}(s)\right)\] \[\geq-\eta\widehat{q}_{t}(s,a,s^{\prime})b_{t}(s)\] \[\geq-4L\eta\frac{\widehat{q}_{t}(s,a,s^{\prime})}{u_{t}(s)}\] \[\geq-4L\eta\] \[\geq-\frac{1}{2},\]

where the first step follows from the fact that \(\widehat{\ell}_{t}(s,a)\geq 0\) for all \(t,s,a\); the second step follows from the definition of amortized bonus in Eq. (7); the third step bounds \(\widehat{q}_{t}(s,a,s^{\prime})\leq u_{t}(s)\).

Now, we are ready to apply the standard analysis to show that

\[\sum_{t=1}^{T}\Big{\langle}\widehat{q}_{t}-u,\widehat{\ell}_{t}- b_{t}\Big{\rangle}\] \[\leq\frac{\sum_{t=1}^{T}\left(D_{\phi}(u,\widehat{q}_{t})-D_{\phi }(u,\widehat{q}_{t+1})\right)}{\eta}+\eta\sum_{t=1}^{T}\sum_{s,a,s^{\prime}} \widehat{q}_{t}(s,a,s^{\prime})^{2}\left(\widehat{\ell}_{t}(s,a)-b_{t}(s) \right)^{2}\] \[=\frac{D_{\phi}(u,\widehat{q}_{1})-D_{\phi}(u,\widehat{q}_{T+1}) }{\eta}+\eta\sum_{t=1}^{T}\sum_{s,a,s^{\prime}}\left(\widehat{q}_{t}(s,a) \widehat{P}_{t}(s^{\prime}|s,a)\right)^{2}\left(\widehat{\ell}_{t}(s,a)-b_{t}( s)\right)^{2}\] \[\leq\frac{D_{\phi}(u,\widehat{q}_{1})}{\eta}+\eta\sum_{t=1}^{T} \sum_{s,a}\widehat{q}_{t}(s,a)^{2}\left(\widehat{\ell}_{t}(s,a)-b_{t}(s) \right)^{2}\cdot\left(\sum_{s^{\prime}}\widehat{P}_{t}(s^{\prime}|s,a)^{2}\right)\]\[\leq\frac{D_{\phi}(u,\widehat{q}_{1})}{\eta}+\eta\sum_{t=1}^{T}\sum_{s,a}\widehat{q}_{t}(s,a)^{2}\left(\widehat{\ell}_{t}(s,a)-b_{t}(s)\right)^{2}\] \[\leq\frac{D_{\phi}(u,\widehat{q}_{1})}{\eta}+2\eta\sum_{t=1}^{T} \sum_{s,a}\widehat{q}_{t}(s,a)^{2}\left(\widehat{\ell}_{t}(s,a)^{2}+b_{t}(s)^{ 2}\right),\]

where the second step uses the fact that \(\widehat{q}_{t}(s,a,s^{\prime})=\widehat{q}_{t}(s,a)\cdot\widehat{P}_{t}(s^{ \prime}|s,a)\) (see (Jin et al., 2020, Lemma 1) for more details); the third step follows from the fact that the Bregman divergence is non-negative; the fourth step follows from the fact \(\sum_{s^{\prime}}\widehat{P}_{t}(s^{\prime}|s,a)=1\); the last step uses the fact that \(\left(x-y\right)^{2}\leq 2\left(x^{2}+y^{2}\right)\) for any \(x,y\in\mathbb{R}\). 

### Bounding Bias\({}_{2}\)

**Lemma A.5.1** (Bound of Bias\({}_{2}\)).: Algorithm1 _ensures_

\[\textsc{Bias}_{2}=\mathcal{O}\left(|S|C^{\mathsf{P}}\log T+\delta LT\right).\]

Proof.: We first rewrite Bias\({}_{2}\) as

\[\textsc{Bias}_{2}=\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t} \left[\left\langle q^{P,\hat{\pi}},\widehat{\ell}_{t}-\ell_{t}\right\rangle \right]-\sum_{t=1}^{T}\left\langle q^{P,\hat{\pi}},b_{t}\right\rangle\right]}_ {=:(\Omega)}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\widehat {P}_{t},\pi_{t}},b_{t}\right\rangle\right]}_{=:(\Omega)},\]

where (II) is bounded by \(\mathcal{O}\left(|S|C^{\mathsf{P}}\log T\right)\) by Lemma3.1 (whose proof is included after this proof). Now, we show that (I) is bounded by \(\mathcal{O}\left(\delta LT\right)\). Suppose that \(\mathcal{E}_{\text{\tiny{CON}}}\) holds. We then have

\[\mathbb{E}_{t}\left[\left\langle q^{P,\hat{\pi}},\widehat{\ell}_ {t}-\ell_{t}\right\rangle\right]\] \[=\sum_{s,a}q^{P,\hat{\pi}}(s,a)\left(\frac{q^{P_{t},\pi_{t}}(s,a )-u_{t}(s,a)}{u_{t}(s,a)}\right)\] \[\leq\sum_{s,a}q^{P,\hat{\pi}}(s,a)\left(\frac{\left|q^{P_{t},\pi_ {t}}(s,a)-q^{P,\pi_{t}}(s,a)\right|+q^{P,\pi_{t}}(s,a)-u_{t}(s,a)}{u_{t}(s,a)}\right)\] \[=\sum_{s,a}q^{P,\hat{\pi}}(s,a)\cdot\frac{\left|q^{P_{t},\pi_{t}}( s)-q^{P,\pi_{t}}(s)\right|}{u_{t}(s)}+\sum_{s,a}q^{P,\hat{\pi}}(s,a)\left( \frac{q^{P,\pi_{t}}(s,a)-u_{t}(s,a)}{u_{t}(s,a)}\right)\] \[\leq\sum_{s,a}q^{P,\hat{\pi}}(s,a)\cdot\frac{C_{t}^{\mathsf{P}}} {u_{t}(s)}+\sum_{s,a}q^{P,\hat{\pi}}(s,a)\left(\frac{q^{P,\pi_{t}}(s,a)-u_{t} (s,a)}{u_{t}(s,a)}\right)\] \[\leq\sum_{s,a}q^{P,\hat{\pi}}(s,a)\cdot\frac{C_{t}^{\mathsf{P}}} {u_{t}(s)},\]

where the fourth step applies CorollaryD.3.6 to bound \(\left|q^{P_{t},\pi_{t}}(s)-q^{P,\pi_{t}}(s)\right|\leq C_{t}^{\mathsf{P}}\), and in the last step, the second term (in the fifth step) is bounded by \(0\) under \(\mathcal{E}_{\text{\tiny{CON}}}\).

Since \(q^{P,\hat{\pi}}(s,a)\) is fixed over all episodes, we apply Lemma3.1 to show

\[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle q^{P,\hat{\pi}},\widehat{\ell}_ {t}-\ell_{t}\right\rangle\right]-\sum_{t=1}^{T}\left\langle q^{P,\hat{\pi}},b _{t}\right\rangle\leq\sum_{s,a}q^{P,\hat{\pi}}(s,a)\sum_{t=1}^{T}\left(\frac{ C_{t}^{\mathsf{P}}}{u_{t}(s)}-b_{t}(s)\right)\leq 0,\]

For the case that \(\mathcal{E}_{\text{\tiny{CON}}}\) does not occur, we bound the expected regret by \(\mathcal{O}\left(\delta LT\right)\). Combining two cases via LemmaD.1.1, we conclude the proof. 

**Lemma A.5.2** (Restatement of Lemma3.1).: _The amortized bonus defined in Eq.7 satisfies \(\sum_{t=1}^{T}\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\leq\sum_{t=1}^{T}b_{t}(s)\) and \(\sum_{t=1}^{T}\widehat{q}_{t}(s)b_{t}(s)=\mathcal{O}(C^{P}\log T)\) for any \(s\)._Proof.: In the following proof, we use the equivalent definition of \(b_{t}(s)\) given in Eq. (14). We first show \(\sum_{t=1}^{T}\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\leq\sum_{t=1}^{T}b_{t}(s)\). On the one hand, we have

\[\sum_{t=1}^{T}\left(\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\right) =\sum_{t=1}^{T}\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\mathbb{I}\{y_ {t}(s)=j\}\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\] \[\leq\sum_{t=1}^{T}\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\mathbb{I} \{y_{t}(s)=j\}\frac{C_{t}^{\mathsf{P}}}{2^{-j-1}}\] \[=\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{\sum_{t=1}^{T}\mathbb{ I}\{y_{t}(s)=j\}C_{t}^{\mathsf{P}}}{2^{-j-1}}\] \[\leq\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{\min\{2L\sum_{t=1} ^{T}\mathbb{I}\{y_{t}(s)=j\},C^{\mathsf{P}}\}}{2^{-j-1}}, \tag{20}\]

where the second step uses the construction of the bin, i.e., if \(u_{t}(s)\) falls into a bin \((2^{-j-1},2^{-j}]\), then, it is lower-bounded by \(2^{-j-1}\); the fourth step follows the facts that \(C_{t}^{\mathsf{P}}\leq 2L\) and \(\sum_{t=1}^{T}C_{t}^{\mathsf{P}}\leq C^{\mathsf{P}}\).

On the other hand, one can show

\[\sum_{t=1}^{T}b_{t}(s) =4L\sum_{t=1}^{T}\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{ \mathbb{I}\{y_{t}(s)=j\}\mathbb{I}\left\{z_{t}^{j}(s)\leq\frac{C^{\mathsf{P}}} {2L}\right\}}{u_{t}(s)}\] \[\geq 4L\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{\sum_{t=1}^{T} \mathbb{I}\{y_{t}(s)=j\}\mathbb{I}\left\{z_{t}^{j}(s)\leq\frac{C^{\mathsf{P}}} {2L}\right\}}{2^{-j}}\] \[\geq 2L\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{\min\{\sum_{t=1} ^{T}\mathbb{I}\{y_{t}(s)=j\},\frac{C^{\mathsf{P}}}{2L}\}}{2^{-j-1}}\] \[=\sum_{j=0}^{\lceil\log(|S|T|)\rceil}\frac{\min\{2L\sum_{t=1}^{T }\mathbb{I}\{y_{t}(s)=j\},C^{\mathsf{P}}\}}{2^{-j-1}}, \tag{21}\]

where the second step uses the construction of the bin, i.e., if \(u_{t}(s)\) falls into a bin \((2^{-j-1},2^{-j}]\), then, it is upper-bounded by \(2^{-j}\), and the third step follows the definitions of \(y_{t}(s)\) and \(z_{t}^{j}(s)\).

Combining Eq. (20) and Eq. (21), we complete the proof of \(\sum_{t=1}^{T}\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\leq\sum_{t=1}^{T}b_{t}(s)\).

For the proof of \(\sum_{t=1}^{T}\widehat{q}_{t}(s)b_{t}(s)=\mathcal{O}(|S|C^{P}\log T)\), one can show

\[\sum_{t=1}^{T}\widehat{q}_{t}(s)b_{t}(s) =4L\sum_{t=1}^{T}\sum_{i=0}^{\lceil\log(|S|T|)\rceil}\frac{q^{ \widehat{P}_{t},\pi_{t}}(s)\mathbb{I}\{y_{t}(s)=i\}\mathbb{I}\left\{z_{t}^{i} (s)\leq\frac{C^{\mathsf{P}}}{2L}\right\}}{u_{t}(s)}\] \[\leq 4L\sum_{t=1}^{T}\sum_{i=0}^{\lceil\log(|S|T|)\rceil}\mathbb{ I}\{y_{t}(s)=i\}\mathbb{I}\left\{z_{t}^{i}(s)\leq\frac{C^{\mathsf{P}}}{2L}\right\}\] \[=4L\sum_{i=0}^{\lceil\log(|S|T|)\rceil}\sum_{t=1}^{T}\mathbb{I}\{ y_{t}(s)=i\}\mathbb{I}\left\{z_{t}^{i}(s)\leq\frac{C^{\mathsf{P}}}{2L}\right\}\] \[\leq 4L\sum_{i=0}^{\lceil\log(|S|T|)\rceil}\frac{C^{\mathsf{P}}}{ 2L}\] \[=\mathcal{O}\left(C^{\mathsf{P}}\log\left(|S|T\right)\right)= \mathcal{O}\left(C^{\mathsf{P}}\log\left(T\right)\right),\]

where the first inequality uses the fact \(q^{\widehat{P}_{t},\pi_{t}}(s)\leq u_{t}(s)\), and the last equality uses \(|S|\leq T\)

### Proof of Theorem 3.2

For Reg, we choose \(\eta=\min\left\{\sqrt{\frac{[S]^{2}|A|\log(\iota)}{LT}},\frac{1}{8L}\right\}\). First consider the case \(\eta\neq\frac{1}{8L}\):

Reg \[=\mathcal{O}\left(\frac{|S|^{2}|A|\log(\iota)}{\eta}+\eta\left(L|S |C^{\mathsf{P}}\log T+LT\right)+LT\delta\right)\] \[\leq\mathcal{O}\left(\frac{|S|^{2}|A|\log(\iota)}{\eta}+\eta LT+|S |C^{\mathsf{P}}\log T+LT\delta\right)\] \[=\mathcal{O}\left(|S|\sqrt{|A|\log(\iota)\,LT}+|S|C^{\mathsf{P}} \log T+LT\delta\right),\]

where the second step uses \(\eta\leq\frac{1}{8L}\), and the third step applies choice of \(\eta\) in the case of \(\eta\neq\frac{1}{8L}\). For the case of \(\eta=\frac{1}{8L}\), we have \(T\leq 64L|S|^{2}|A|\ln(\iota)\) and show

\[\textsc{Reg}=\mathcal{O}\left(L|S|^{2}|A|\log(\iota)+|S|C^{\mathsf{P}}\log T+ LT\delta\right).\]

Finally, choosing \(\delta=\frac{1}{T}\) and putting the bound above together with Error, Bias\({}_{1}\), and Bias\({}_{2}\), we complete the proof of Theorem 3.2.

Omitted Details for Section 4

### Bottom Layer Reduction: STABILISE

Proof of Theorem 4.3.: Define indicators

\[g_{t,j} =\mathbb{I}\{w_{t}\in(2^{-j-1},2^{-j}]\}\] \[h_{t,j} =\mathbb{I}\{\mathsf{ALG}_{j}\text{ receives the feedback for episode }t\}.\]

Now we consider the regret of \(\mathsf{ALG}_{j}\). Notice that \(\mathsf{ALG}_{j}\) makes an update only when \(g_{t,j}h_{t,j}=1\). By the guarantee of the base algorithm (Definition 4.1), we have

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\pi))g_ {t,j}h_{t,j}\right]\] \[\leq\mathbb{E}\left[\sqrt{\beta_{1}\sum_{t=1}^{T}g_{t,j}h_{t,j}}+ (\beta_{2}+\beta_{3}\theta_{j})\max_{t\leq T}g_{t,j}\right]+\Pr\left[\sum_{t=1} ^{T}C_{t}^{\mathsf{P}}g_{t,j}h_{t,j}>\theta_{j}\right]LT. \tag{22}\]

We first bound the last term: Notice that \(\mathbb{E}[h_{t,j}|g_{t,j}]=2^{-j-1}g_{t,j}\) by Algorithm 2. Therefore,

\[\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}]=2^{-j-1} \sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}\leq 2^{-j-1}C^{\mathsf{P}} \tag{23}\]

By Freedman's inequality, with probability at least \(1-\frac{1}{T^{2}}\),

\[\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}h_{t,j}-\sum_{t=1}^{T}C_{t} ^{\mathsf{P}}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}]\] \[\leq 2\sqrt{2\sum_{t=1}^{T}(C_{t}^{\mathsf{P}})^{2}g_{t,j}\mathbb{ E}[h_{t,j}|g_{t,j}]\log(T)}+4L\log(T)\] \[\leq 4\sqrt{L\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}]\log(T)}+4L\log(T)\] ( \[C_{t}^{\mathsf{P}}\leq 2L\] ) \[\leq\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}\mathbb{E}[h_{t,j}|g_{ t,j}]+8L\log(T)\] (AM-GM inequality)

which gives

\[\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}h_{t,j} \leq 2\sum_{t=1}^{T}C_{t}^{\mathsf{P}}g_{t,j}\mathbb{E}[h_{t,j}|g_ {t,j}]+8L\log(T)\] \[\leq 2^{-j}C^{\mathsf{P}}+8L\log(T)\leq\theta_{j}\]

with probability at least \(1-\frac{1}{T^{2}}\) using Eq. (23). Therefore, the last term in Eq. (22) is bounded by \(\frac{1}{T^{2}}LT\leq\frac{L}{T}\).

Next, we deal with other terms in Eq. (22). Again, by \(\mathbb{E}[h_{t,j}|g_{t,j}]=2^{-j-1}g_{t,j}\), Eq. (22) implies

\[2^{-j-1}\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\pi))g_{t,j }\right]\leq\mathbb{E}\left[\sqrt{2^{-j-1}\beta_{1}\sum_{t=1}^{T}g_{t,j}}+( \beta_{2}+\beta_{3}\theta_{j})\max_{t\leq T}g_{t,j}\right]+\frac{L}{T}.\]

which implies after rearranging:

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\pi))g_{t,j}\right]\]\[\leq\mathbb{E}\left[\sqrt{\frac{1}{2^{-j-1}}\beta_{1}\sum_{t=1}^{T}g _{t,j}+\left(\frac{\beta_{2}}{2^{-j-1}}+\frac{\beta_{3}\theta_{j}}{2^{-j-1}} \right)}\max_{t\leq T}g_{t,j}\right]+\frac{L}{T2^{-j-1}}\] \[\leq\mathbb{E}\left[\sqrt{\beta_{1}\sum_{t=1}^{T}\frac{2g_{t,j}}{ w_{t}}}+\left(\frac{2\beta_{2}+16\beta_{3}L\log(T)}{2^{-j}}+4\beta_{3}C^{\mathsf{ P}}\right)\max_{t\leq T}g_{t,j}\right]+\frac{L}{T2^{-j-1}}.\] (using that when \[g_{t,j}=1\], \[\frac{1}{2^{-j-1}}\leq\frac{2}{w_{t}}\], and the definition of \[\theta_{j}\] )

Now, summing this inequality over all \(j\in\{0,1,\ldots,\lceil\log_{2}T\rceil\}\), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\pi)) \mathbb{I}\left\{w_{t}\geq\frac{1}{T}\right\}\right]\] \[\leq\mathcal{O}\left(\mathbb{E}\left[\sqrt{N\beta_{1}\sum_{t=1}^{ T}\frac{1}{w_{t}}}+(\beta_{2}+\beta_{3}L\log(T))\frac{1}{\min_{t\leq T}w_{t}}+N \beta_{3}C^{\mathsf{P}}\right]+L\right)\] \[\leq\mathcal{O}\left(\mathbb{E}\left[\sqrt{\beta_{1}T\log(T)\rho_ {T}}+(\beta_{2}+\beta_{3}L\log(T))\rho_{T}\right]+\beta_{3}C^{\mathsf{P}}\log T +L\right)\]

where \(N\leq\mathcal{O}(\log T)\) is the number of \(\mathsf{ALG}_{j}\)'s that has been executed at least once.

On the other hand,

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\pi)) \mathbb{I}\left\{w_{t}\leq\frac{1}{T}\right\}\right]\leq LT\mathbb{E}\left[ \mathbb{I}\left\{\rho_{T}\geq T\right\}\right]\leq L\mathbb{E}\left[\rho_{T} \right].\]

Combining the two parts and using the assumption \(\beta_{2}\geq L\) finishes the proof. 

### Top Layer Reduction: Corral

In this subsection, we use a base algorithm that satisfies Definition 4.2 to construct an algorithm with \(\sqrt{T}+C^{\mathsf{P}}\) regret under unknown \(C^{\mathsf{P}}\). The idea is to run multiple base algorithms, each with a different hypothesis on \(C^{\mathsf{P}}\); on top of them, run another multi-armed bandit algorithm to adaptively choose among them. The goal is to let the top-level bandit algorithm perform almost as well as the best base algorithm. This is the Corral idea outlined in Agarwal et al. (2017); Foster et al. (2020); Luo et al. (2022), and the algorithm is presented in Algorithm 3.

The top-level bandit algorithm is an FTRL with log-barrier regularizer. We first state the standard regret bound of FTRL under log-barrier regularizer, whose proof can be found in, e.g., Theorem 7 of Wei and Luo (2018).

**Lemma B.2.1**.: _The FTRL algorithm over a convex subset \(\Omega\) of the \((M-1)\)-dimensional simplex \(\Delta(M)\):_

\[w_{t}=\operatorname*{argmin}_{w\in\Omega}\left\{\left\langle w, \sum_{\tau=1}^{t-1}\ell_{\tau}\right\rangle+\frac{1}{\eta}\sum_{i=1}^{M}\log \frac{1}{w_{i}}\right\}\]

_ensures for all \(u\in\Omega\),_

\[\sum_{t=1}^{T}\langle w-u,\ell_{t}\rangle\leq\frac{M\log T}{\eta}+\eta\sum_{t =1}^{T}\sum_{i=1}^{M}w_{t,i}^{2}\ell_{t,i}^{2}\]

_as long as \(\eta w_{t,i}|\ell_{t,i}|\leq\frac{1}{2}\) for all \(t,i\)._

Proof of Theorem 4.4.: The Corral algorithm is essential an FTRL with log-barrier regularizer. To apply Lemma B.2.1, we first verify the condition \(\eta w_{t,i}|\ell_{t,i}|\leq\frac{1}{2}\) where \(\ell_{t,i}=\hat{c}_{t,i}-r_{t,i}\). By our choice of \(\eta\),

\[\eta w_{t,i}|\hat{c}_{t,i}|\leq\eta c_{t,i}\leq\frac{1}{4}, \text{(because $\beta_{2}\geq L$ by Definition \ref{def:c_t_t_i})}\]

[MISSING_PAGE_EMPTY:25]

**Bounding stability-term**:

\[\textbf{stability term}\leq 2\eta\sum_{t=1}^{T}\sum_{i=1}^{M}w_{t,i}^{2}( \hat{c}_{t,i}^{2}+r_{t,i}^{2})\]

where

\[2\eta\sum_{t=1}^{T}\sum_{i=1}^{M}w_{t,i}^{2}\hat{c}_{t,i}^{2}=2 \eta\sum_{t=1}^{T}\sum_{i=1}^{M}c_{t,i}^{2}\mathbb{I}\{i_{t}=i\}\leq\mathcal{O} (\eta L^{2}T)\]

and

\[2\eta\sum_{t=1}^{T}\sum_{i=1}^{M}w_{t,i}^{2}r_{t,i}^{2} \leq 4\eta\sum_{t=1}^{T}\sum_{i=1}^{M}(\sqrt{\beta_{1}T})^{2} \left(\frac{1}{\sqrt{\rho_{t-1,i}}}-\frac{1}{\sqrt{\rho_{t,i}}}\right)^{2}+4 \eta\beta_{2}\sum_{t=1}^{T}\sum_{i=1}^{M}\left(1-\frac{\rho_{t-1,i}}{\rho_{t,i }}\right)^{2}\] (continue from Eq. (24)) \[\leq 4\eta\beta_{1}T\times\sum_{t=1}^{T}\sum_{i=1}^{M}\left(\frac{ 1}{\sqrt{\rho_{t-1,i}}}-\frac{1}{\sqrt{\rho_{t,i}}}\right)+4\eta\beta_{2}\sum_ {t=1}^{T}\sum_{i=1}^{M}\ln\frac{\rho_{t,i}}{\rho_{t-1,i}}\] \[(\frac{1}{\sqrt{\rho_{t-1,i}}}-\frac{1}{\sqrt{\rho_{t,i}}}\leq 1 \text{ and }1-a\leq-\ln a)\] \[\leq 4\eta\beta_{1}TM^{\frac{3}{2}}+4\eta\beta_{2}M\ln T.\] (telescoping and using \[\rho_{0,i}=M\] and \[\rho_{T,i}\leq T\])

**Bounding bonus-term**:

\[\textbf{bonus-term} =\sum_{t=1}^{T}\sum_{i=1}^{M}w_{t,i}r_{t,i}-\sum_{t=1}^{T}r_{t,i}\] \[\leq\sqrt{\beta_{1}T}\sum_{t=1}^{T}\sum_{i=1}^{M}\left(\frac{1}{ \sqrt{\rho_{t-1,i}}}-\frac{1}{\sqrt{\rho_{t,i}}}\right)+\beta_{2}\sum_{t=1}^{T }\sum_{i=1}^{M}\log\frac{\rho_{t,i}}{\rho_{t-1,i}}\] \[-\left(\sqrt{\rho_{T,i}\lx@overaccentset{{\star}}{ \otimes}_{1}T}+\rho_{T,i}\lx@overaccentset{{\star}}{\otimes}_{2}- \sqrt{\rho_{0,i}\lx@overaccentset{{\star}}{\otimes}_{1}T}-\rho_{0,i }\lx@overaccentset{{\star}}{\otimes}_{2}\right)\] (continue from Eq. (24) and using \[1-a\leq-\ln a\]) \[\leq\mathcal{O}\left(\sqrt{\beta_{1}TM^{\frac{3}{2}}}+\beta_{2}M \log T\right)-\left(\sqrt{\rho_{T,i}\lx@overaccentset{{\star}}{ \otimes}_{1}T}+\rho_{T,i}\lx@overaccentset{{\star}}{\otimes}_{2} \right).\]

Combining the two terms and using \(\eta=\Theta\left(\frac{1}{\sqrt{\beta_{1}T}+\beta_{2}}\right)\), \(M=\Theta(\log T)\), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-c_{t,i^{\star} })\right] =\mathbb{E}\left[\sum_{t=1}^{T}(c_{t,i_{t}}-c_{t,i^{\star}})\right]\] \[=\mathcal{O}\left(\sqrt{\beta_{1}T\log^{3}T}+\beta_{2}\log^{2}T \right)-\mathbb{E}\left[\sqrt{\rho_{T,i^{\star}}\beta_{1}T}+\rho_{T,i^{\star} }\beta_{2}\right] \tag{25}\]

On the other hand, by Definition 4.2 and that \(C^{\mathsf{P}}\in[2^{i^{\star}-1},2^{i^{\star}}]\), we have

\[\mathbb{E}\left[\sum_{t=1}^{T}(c_{t,i^{\star}}-\ell_{t}(\dot{ \pi}))\right]\leq\mathbb{E}\left[\sqrt{\rho_{T,i^{\star}}\beta_{1}T}+\rho_{T, i^{\star}}\beta_{2}\right]+2\beta_{3}C^{\mathsf{P}}. \tag{26}\]

Combining Eq. (25) and Eq. (26), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(\ell_{t}(\pi_{t})-\ell_{t}(\dot{ \pi}))\right]\leq\mathcal{O}\left(\sqrt{\beta_{1}T\log^{3}T}+\beta_{2}\log^{2 }T\right)+2\beta_{3}C^{\mathsf{P}},\]

which finishes the proof.

## Appendix C Omitted Details for Section 5

In this section, we consider a variant of the algorithm proposed in Jin et al. (2021), which instead ensures exploration via _optimistic transitions_ and also switch the regularizer from Tsallis entropy to log-barrier. We present the pseudocode in Algorithm 4, and show that it automatically adapts to easier environments with improved gap-dependent regret bounds.

**Remark C.1**.: _Throughout the analysis, we denote by \(Q^{P,\pi}(s,a;r)\) the state-action value of state-action pair \((s,a)\) with respect to transition function \(P\) (\(P\) could be an optimistic transition function), policy \(\pi\) and loss function \(r\); similarly, we denote by \(V^{P,\pi}(s;r)\) the corresponding state-value function._

_Specifically, the state value function \(V^{P,\pi}(s;r)\) is computed in a backward manner from layer \(L\) to layer \(0\) as following:_

\[V^{P,\pi}(s;r)=\begin{cases}0,&s=s_{L},\\ \sum_{a\in A}\pi(a|s)\cdot\left(r(s,a)+\sum_{s^{\prime}\in S_{k(s)+1}}P(s^{ \prime}|s,a)V^{P,\pi}(s^{\prime};r)\right),&s\neq s_{L}.\end{cases}\]

_Similarly, the state-action value function \(Q^{P,\pi}(s,a;r)\) is calculated in the same manner:_

\[Q^{P,\pi}(s,a;r)=\begin{cases}0,&s=s_{L},\\ r(s,a)+\sum_{s^{\prime}\in S_{k(s)+1}}P(s^{\prime}|s,a)\sum_{a^{\prime}\in A} \pi(a^{\prime}|s^{\prime})Q^{P,\pi}(s^{\prime},a^{\prime};r),&s\neq s_{L}.\end{cases}\]

_Clearly, \(V^{P,\pi}(s_{0};r)\), the state value of \(s_{0}\), is equal to_

\[V^{P,\pi}(s_{0};r)=\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi}(s,a)r(s,a)\triangleq \left\langle q^{P,\pi},r\right\rangle.\]

_This equality can be further extended to the other state \(u\) as:_

\[V^{P,\pi}(u;r)=\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi}(s,a|u)r(s,a),\]

_where \(q^{P,\pi}(s,a|u)\) denotes the probability of arriving at \((s,a)\) starting at state \(u\) under policy \(\pi\) and transition function \(P\)._

### Description of the Algorithm

The construction of this algorithm follows similar ideas to the work of Jin et al. (2021), while also including several key differences. One of them is that we rely on the log-barrier regularizer, defined with a positive learning rate \(\gamma_{t}(s,a)\) as

\[\phi_{t}(q)=-\sum_{s,a}\gamma_{t}(s,a)\log\left(q(s,a)\right). \tag{28}\]

The choice of log-barrier is important for adversarial transitions as discussed in Section 3, while the adaptive choice of learning rate is important for adapting to easy environments. The formal definition of the learning rate is given in Definition 5.2, and further details and properties of the learning rate are provided in Section C.7.

As the algorithm runs a new instance of FTRL for each epoch \(i\), we modify the definition of the amortized bonus \(b_{t}\) accordingly,

\[b_{t}(s)=b_{t}(s,a)=\begin{cases}\frac{4L}{u_{t}(s)}&\text{if }\sum_{\tau=t_{i( t)}}^{t}\mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u_{t}(s) \rceil\}\leq\frac{c^{p}}{2L},\\ 0&\text{else}.\end{cases} \tag{29}\]

The bonus \(b_{t}(s)\) in Eq. (29) is defined based on each epoch \(i\), in the sense that \(\sum_{\tau=1}^{t}\mathbb{I}\{\lceil\log_{2}u_{\tau}(s)\rceil=\lceil\log_{2}u _{t}(s)\rceil\}\) counts, among all previous rounds \(\tau=t_{i},\ldots,t\) in epoch \(i\), the number of times that the value of \(u_{\tau}(s)\) falls into the same bin as \(u_{t}(s)\).

Again, for the ease of analysis, in the analysis we use the equivalent definition of the bonus \(b_{t}(s)\) defined in Eq. (14), except that the counter \(z_{t}^{j}(s)\) now will be reset to zero at the beginning of each epoch \(i\).

Next, we formally define the optimistic transition as follows.

**Definition C.1.1** (Optimistic Transition).: _For epoch \(i\), the optimistic transition \(\widetilde{P}_{i}:S\times A\times S\to[0,1]\) is defined as:_

\[\widetilde{P}_{i}(s^{\prime}|s,a)=\begin{cases}\max\left\{0,\bar{P}_{i}(s^{ \prime}|s,a)-B_{i}(s,a,s^{\prime})\right\},&(s,a,s^{\prime})\in W_{k},\\ \sum_{s^{\prime}\in S_{k+1}}\min\left\{\bar{P}_{i}(s^{\prime}|s,a),B_{i}(s,a,s^ {\prime})\right\},&(s,a)\in S_{k}\times A\text{ and }s^{\prime}=s_{L},\end{cases}\]

_where \(\bar{P}_{i}\) is the empirical transition defined in Eq. (2)._

Note that the optimistic transition \(\widetilde{P}_{i}(\cdot|s,a)\) is a valid distribution as we have

\[\sum_{s^{\prime}\in S_{k+1}}\min\left\{\bar{P}_{i}(s^{\prime}|s,a),B_{i}(s,a, s^{\prime})\right\}=1-\sum_{s^{\prime}\in S_{k+1}}\max\left\{0,\bar{P}_{i}(s^{ \prime}|s,a)-B_{i}(s,a,s^{\prime})\right\}.\]

We summarize the properties of the optimistic transition functions in Appendix C.8.

### Self-Bounding Properties of the Regret

In order to achieve best-of-both-worlds guarantees, our goal is to bound \(\text{Reg}_{T}(\pi^{\star})\) in terms of two self-bounding quantities for some \(x>0\) (plus other minor terms):

\[\begin{split}\mathbb{S}_{1}(x)&=\sqrt{x\cdot\mathbb{ E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}(s)}q^{P_{t},\pi_{ t}}(s,a)\right]},\\ \mathbb{S}_{2}(x)&=\sum_{s\neq s_{L}}\sum_{a\neq\pi^ {\star}(s)}\sqrt{x\cdot\mathbb{E}\left[\sum_{t=1}^{T}q^{P_{t},\pi_{t}}(s,a) \right]}.\end{split} \tag{30}\]

These two quantities enjoy a certain self-bounding property which is critical to achieve the gap-dependent bound under Condition (9), as they can be related back to the regret against policy \(\pi^{\star}\) itself. To see this, we first show the following implication of Condition (9).

**Lemma C.2.1**.: _Under Condition (9), the following holds._

\[\operatorname{Reg}_{T}(\pi^{\star})\geq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s _{L}}\sum_{a\neq\pi^{\star}(s)}q^{P_{t},\pi_{t}}(s,a)\Delta(s,a)\right]-C^{ \mathsf{L}}-2LC^{\mathsf{P}}-L^{2}C^{\mathsf{P}}. \tag{31}\]

Proof.: From the definition of \(\operatorname{Reg}_{T}(\pi^{\star})\), we first note that:

\[\operatorname{Reg}_{T}(\pi^{\star})\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}}-q^ {P,\pi^{\star}},\ell_{t}\right\rangle\right]\] \[\geq\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\pi_{t}}-q^{P,\pi^{\star}},\ell_{t}\right\rangle\right]-2LC^{\mathsf{P}}\] \[\geq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq \pi^{\star}(s)}q^{P,\pi_{t}}(s,a)\Delta(s,a)\right]-C^{\mathsf{L}}-2LC^{ \mathsf{P}}\]

where the third step applies Corollary D.3.7 and the last step uses the Condition (9). We continue to bound the first term above as:

\[\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{ \star}(s)}q^{P,\pi_{t}}(s,a)\Delta(s,a)\right]\] \[\geq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq \pi^{\star}(s)}q^{P_{t},\pi_{t}}(s,a)\Delta(s,a)\right]\] \[\quad-\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq \pi^{\star}(s)}\left|q^{P_{t},\pi_{t}}(s,a)-q^{P,\pi_{t}}(s,a)\right|\Delta(s,a )\right]\] \[\geq\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq \pi^{\star}(s)}q^{P_{t},\pi_{t}}(s,a)\Delta(s,a)\right]-L^{2}C^{\mathsf{P}},\]

where the last step uses \(\Delta(s,a)\in(0,L]\) and Corollary D.3.6. 

We are now ready to show the following important self-bounding properties (recall \(\Delta_{\textsc{MIN}}=\min_{s\neq s_{L},a\neq\pi^{\star}(s)}\Delta(s,a)\)).

**Lemma C.2.2** (Self-Bounding Quantities).: _Under Condition (31), we have for any \(z>0\):_

\[\mathbb{S}_{1}(x)\leq z\left(\operatorname{Reg}_{T}(\pi^{\star}) +C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{\mathsf{P}}\right)+\frac{1}{z}\left( \frac{x}{4\Delta_{\textsc{MIN}}}\right),\] \[\mathbb{S}_{2}(x)\leq z\left(\operatorname{Reg}_{T}(\pi^{\star}) +C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{\mathsf{P}}\right)+\frac{1}{z}\left( \sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}(s)}\frac{x}{4\Delta(s,a)}\right).\]

_Besides, it always holds that \(\mathbb{S}_{1}(x)\leq\sqrt{x\cdot LT}\) and \(\mathbb{S}_{2}(x)\leq\sqrt{x\cdot L|S||A|T}\)._

Proof.: For any \(z>0\), we have

\[\mathbb{S}_{1}\left(x\right)=\sqrt{\frac{x}{2z\Delta_{\textsc{MIN}}}\cdot \mathbb{E}\left[\left(2z\Delta_{\textsc{MIN}}\sum_{t=1}^{T}\sum_{s\neq s_{L}} \sum_{a\neq\pi^{\star}(s)}q^{P_{t},\pi_{t}}(s,a)\right)\right]}\]\[\leq\mathbb{E}\left[z\Delta_{\text{MIN}}\sum_{t=1}^{T}\sum_{s\neq s_{L} }\sum_{a\neq\pi^{*}(s)}q^{P_{t},\pi_{t}}(s,a)\right]+\frac{x}{4z\Delta_{\text{ MIN}}}\] \[\leq z\left(\text{Reg}_{T}(\pi^{*})+C^{\mathsf{L}}+2LC^{\mathsf{P} }+L^{2}C^{\mathsf{P}}\right)+\frac{x}{4z\Delta_{\text{MIN}}},\]

where the second step follows from the AM-GM inequality: \(2\sqrt{xy}\leq x+y\) for any \(x,y\geq 0\), and the last step follows from Condition (31).

By similar arguments, we have \(\mathbb{S}_{2}(x)\) bounded for any \(z>0\) as:

\[\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}(s)}\sqrt{\frac{x}{2z\Delta(s, a)}\cdot\mathbb{E}\left[2z\Delta(s,a)\sum_{t=1}^{T}q^{P_{t},\pi_{t}}(s,a)\right]}\] \[\leq\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}(s)}\frac{x}{4z\Delta(s, a)}+z\mathbb{E}\left[\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}(s)}\sum_{t=1}^{T}q^{P_{ t},\pi_{t}}(s,a)\Delta(s,a)\right]\] \[=z\left(\text{Reg}_{T}(\pi^{*})+C^{\mathsf{L}}+2LC^{\mathsf{P}}+L ^{2}C^{\mathsf{P}}\right)+\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}(s)}\frac{x}{4z \Delta(s,a)}.\]

Finally, by direct calculation, we can show that

\[\mathbb{S}_{1}(x)=\sqrt{x\cdot\mathbb{E}\left[\sum_{t=1}^{T}\sum_{s\neq s_{L}} \sum_{a\neq\pi^{*}(s)}q^{P_{t},\pi_{t}}(s,a)\right]}\leq\sqrt{x\cdot LT},\]

according to the fact that \(\sum_{s\neq s_{L}}\sum_{a\in A}q^{P_{t},\pi_{t}}(s,a)\leq L\). On the other hand, \(\mathbb{S}_{2}(x)\) is bounded as

\[\mathbb{S}_{2}(x)\leq\sqrt{x\cdot|S||A|\mathbb{E}\left[\sum_{s\neq s_{L}}\sum_ {a\neq\pi^{*}(s)}\sum_{t=1}^{T}q^{P_{t},\pi_{t}}(s,a)\right]}\leq\sqrt{x\cdot L |S||A|T},\]

with the help of the Cauchy-Schwarz inequality in the first step. 

Finally, we show that Algorithm 4 achieves the following adaptive regret bound, which directly leads to the best-of-both-worlds guarantee in Theorem 5.1.

**Lemma C.2.3**.: _Algorithm 4 with \(\delta=\nicefrac{{1}}{{T^{2}}}\) and learning rate defined in Definition 5.2 ensures that, for any mapping \(\pi^{*}:S\to A\), the regret \(\text{\rm Reg}_{T}(\pi^{*})\) is bounded by \(\mathcal{O}\left(\left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|^{2}\log^{2}{( \iota)}\right)\) plus_

\[\mathcal{O}\left(\mathbb{S}_{1}\left(L^{3}|S|^{2}|A|\log^{2}{( \iota)}\right)+\mathbb{S}_{2}\left(L^{2}|S||A|\log^{2}{(\iota)}\right)\right).\]

The proof of this result is detailed in Section C.3. We emphasize that this bound holds for any mapping \(\pi^{*}\), and is not limited to that policy in Eq. (9). This is important for proving the robustness result when losses are arbitrary, as shown in the following proof of Theorem 5.1.

Proof of Theorem 5.1.: When losses are arbitrary, we simply select \(\pi^{*}=\mathring{\pi}\) where \(\mathring{\pi}\) is one of the optimal deterministic policies in hindsight and obtain the following bound of \(\text{\rm Reg}_{T}(\mathring{\pi})\):

\[\mathcal{O}\left(\left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|^{2} \log^{2}{(\iota)}+\mathbb{S}_{1}\left(L^{3}|S|^{2}|A|\log^{2}{(\iota)}\right) +\mathbb{S}_{2}\left(L^{2}|S||A|\log^{2}{(\iota)}\right)\right)\] \[=\mathcal{O}\left(\left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|^{2 }\log^{2}{(\iota)}+\sqrt{L^{4}|S|^{2}|A|T\log^{2}{(\iota)}}+\sqrt{L^{3}|S|^{2} |A|^{2}T\log^{2}{(\iota)}}\right),\]

where the first step follows from Lemma C.2.3 and the second step follows from Lemma C.2.2.

Next, suppose that Condition (9) holds. We set \(\pi^{*}\) as defined in the condition and use Lemma C.2.3 to write the regret against \(\pi^{*}\) as:

\[\text{Reg}_{T}(\pi^{*})\leq\mu\left(\mathbb{S}_{1}\left(L^{3}|S|^{2}|A|\log^{2 }{(\iota)}\right)+\mathbb{S}_{2}\left(L^{2}|S||A|\log^{2}{(\iota)}\right) \right)+\xi\left(C^{\mathsf{P}}+1\right),\]where \(\mu>0\) is an absolute constant and \(\xi=\mathcal{O}\left(L^{2}|S|^{4}|A|^{2}\log^{2}(\iota)\right)\).

For any \(z>0\), according to Lemma C.2.2 (where we set the \(z\) there as \(\nicefrac{{z}}{{2\mu}}\)), we have:

\[\text{Reg}_{T}(\pi^{\star}) \leq z\left(\text{Reg}_{T}(\pi^{\star})+C^{\mathsf{L}}+2LC^{ \mathsf{P}}+L^{2}C^{\mathsf{P}}\right)\] \[\quad+\frac{\mu^{2}}{z}\left(\frac{L^{3}|S|^{2}|A|\log^{2}\left( \iota\right)}{\Delta_{\text{MIN}}}+\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}(s)} \frac{L^{2}|S||A|\log^{2}\left(\iota\right)}{\Delta(s,a)}\right)+\xi\left(C^{ \mathsf{P}}+1\right).\]

Let \(x=\frac{1-z}{z}\) and \(U=\frac{L^{3}|S|^{2}|A|\log^{2}(\iota)}{\Delta_{\text{MIN}}}+\sum_{s\neq s_{L }}\sum_{a\neq\pi^{\star}(s)}\frac{L^{2}|S||A|\log^{2}(\iota)}{\Delta(s,a)}\). Rearranging the above inequality leads to

\[\text{Reg}_{T}(\pi^{\star}) \leq\frac{z\left(C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{\mathsf{ P}}\right)}{1-z}+\frac{\mu^{2}U}{z\left(1-z\right)}+\frac{\xi\left(C^{ \mathsf{P}}+1\right)}{1-z}\] \[=\frac{\left(C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{\mathsf{P}} \right)}{x}+\left(x+2+\frac{1}{x}\right)\mu^{2}U+\left(1+\frac{1}{x}\right)\xi \left(C^{\mathsf{P}}+1\right)\] \[=\frac{1}{x}\left(C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{ \mathsf{P}}+\mu^{2}U+\xi\left(C^{\mathsf{P}}+1\right)\right)+x\cdot\mu^{2}U+2 \mu^{2}U+\xi\left(C^{\mathsf{P}}+1\right).\]

Picking the optimal \(x\) to minimize the upper bound of \(\text{Reg}_{T}(\pi^{\star})\) yields

\[\text{Reg}_{T}(\pi^{\star}) =2\sqrt{\left(C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{\mathsf{P}} +\mu^{2}U+\xi\left(C^{\mathsf{P}}+1\right)\right)\mu^{2}U}+2\mu^{2}U+\xi\left( C^{\mathsf{P}}+1\right)\] \[\leq 2\sqrt{\left(C^{\mathsf{L}}+2LC^{\mathsf{P}}+L^{2}C^{ \mathsf{P}}+\xi\left(C^{\mathsf{P}}+1\right)\right)\mu^{2}U}+4\mu^{2}U+\xi \left(C^{\mathsf{P}}+1\right)\] \[=\mathcal{O}\left(\sqrt{\left(C^{\mathsf{L}}+L^{2}|S|^{4}|A|^{2} \log^{2}\left(\iota\right)C^{\mathsf{P}}\right)}\,U+U+L^{2}|S|^{4}|A|\log^{2} \left(\iota\right)\left(C^{\mathsf{P}}+1\right)\right)\] \[\leq\mathcal{O}\left(\sqrt{UC^{\mathsf{L}}}+U+L^{2}|S|^{4}|A|^{2} \log^{2}\left(\iota\right)\left(C^{\mathsf{P}}+1\right)\right),\]

where the second step uses \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) for any \(x,y\in\mathbb{R}_{\geq 0}\) and the last step uses \(2\sqrt{xy}\leq x+y\) for any \(x,y\geq 0\). 

### Regret Decomposition of \(\text{Reg}_{T}(\pi^{\star})\) and Proof of Lemma c.2.3

In the following sections, we will first decompose \(\text{Reg}_{T}(\pi^{\star})\) for any mapping \(\pi^{\star}:S\to A\) into several parts, and then bound each part separately from Section C.4 to Section C.7, in order to prove Lemma C.2.3.

For any mapping \(\pi^{\star}:S\to A\), we start from the following decomposition of \(\text{Reg}_{T}(\pi^{\star})\) as

\[\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}},\ell_{t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi_{t}}, \widehat{\ell}_{t}-b_{t}\right\rangle\right]}_{\text{Error}_{1}}+\underbrace{ \mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\widehat{P}_{t},\pi^{\star}}, \widehat{\ell}_{t}-b_{t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi^{ \star}},\widehat{\ell}_{t}-b_{t}\right\rangle\right]}_{\text{EstReg}} \tag{32}\]

where \(\widehat{P}_{t}=\widetilde{P}_{i(t)}\) denotes the optimistic transition for episode \(t\) for simplicity (which is consistent with earlier definition such that \(\widehat{q}_{t}=q^{\widehat{P}_{t},\pi_{t}}\)). Here, EstReg is the estimated regret controlled by FTRL, while Error\({}_{1}\) and Error\({}_{2}\) are estimation errors incurred on the selected policies \(\{\pi_{t}\}_{t=1}^{T}\), and that on the comparator policy \(\pi^{\star}\) respectively.

In order to achieve the gap-dependent bound under Condition (9), we consider these two estimation error terms \(\textsc{Error}_{1}\) and \(\textsc{Error}_{2}\) together as:

\[\begin{split}&\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t}, \pi_{t}},\ell_{t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi_{t}}, \widehat{\ell}_{t}-b_{t}\right\rangle+\left\langle q^{\widehat{P}_{t},\pi^{*}}, \widehat{\ell}_{t}-b_{t}\right\rangle-\left\langle q^{P_{t},\pi^{*}},\ell_{t} \right\rangle\right]\\ &=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}},\ell_ {t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi_{t}},\mathbb{E}_{t}\left[ \widehat{\ell}_{t}\right]-b_{t}\right\rangle+\left\langle q^{\widehat{P}_{t}, \pi^{*}},\mathbb{E}_{t}\left[\widehat{\ell}_{t}\right]-b_{t}\right\rangle- \left\langle q^{P_{t},\pi^{*}},\ell_{t}\right\rangle\right],\end{split} \tag{33}\]

where \(\mathbb{E}_{t}\left[\cdot\right]\) denotes the conditional expectation given the history prior to episode \(t\).

To better analyze the conditional expectation of the loss estimators, we define \(\alpha_{t},\beta_{t}:S\times A\rightarrow\mathbb{R}\) as:

\[\alpha_{t}(s,a)\triangleq\frac{q^{P,\pi_{t}}(s,a)\ell_{t}(s,a)}{u_{t}(s,a)}, \quad\beta_{t}(s,a)\triangleq\quad\frac{\left(q^{P_{t},\pi_{t}}(s,a)-q^{P,\pi_ {t}}(s,a)\right)\ell_{t}(s,a)}{u_{t}(s,a)},\]

which ensures that \(\mathbb{E}_{t}\left[\widehat{\ell}_{t}(s,a)\right]=\alpha_{t}(s,a)+\beta_{t}(s,a)\) for any state-action pair \((s,a)\).

With the help of \(\alpha_{t},\beta_{t}\), we have

\[\begin{split}&\left\langle q^{\widehat{P}_{t},\pi_{t}},\mathbb{E}_{t} \left[\widehat{\ell}_{t}\right]-b_{t}\right\rangle=\left\langle q^{\widehat{P} _{t},\pi_{t}},\alpha_{t}\right\rangle+\left\langle q^{\widehat{P}_{t},\pi_{t}},\beta_{t}-b_{t}\right\rangle,\\ &\left\langle q^{\widehat{P}_{t},\pi^{*}},\mathbb{E}_{t}\left[ \widehat{\ell}_{t}\right]-b_{t}\right\rangle=\left\langle q^{\widehat{P}_{t}, \pi^{*}},\alpha_{t}\right\rangle+\left\langle q^{\widehat{P}_{t},\pi^{*}}, \beta_{t}-b_{t}\right\rangle,\end{split}\]

which helps us further rewrite Eq. (33) as

\[\begin{split}&\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t}, \pi_{t}}-q^{P,\pi_{t}},\ell_{t}\right\rangle+\sum_{t=1}^{T}\left\langle q^{P, \pi^{*}}-q^{P_{t},\pi^{*}},\ell_{t}\right\rangle\right]\\ &+\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\pi_{t}},\ell_ {t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi_{t}},\alpha_{t}\right \rangle+\left\langle q^{\widehat{P}_{t},\pi^{*}},\alpha_{t}\right\rangle- \left\langle q^{P,\pi^{*}},\ell_{t}\right\rangle\right]\\ &+\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\widehat{P}_{t}, \pi_{t}},b_{t}-\beta_{t}\right\rangle+\sum_{t=1}^{T}\left\langle q^{\widehat{ P}_{t},\pi^{*}},\beta_{t}-b_{t}\right\rangle\right].\end{split} \tag{34}\]

Based on this decomposition of \(\textsc{Error}_{1}+\textsc{Error}_{2}\), we then bound each parts respectively in the following lemmas.

**Lemma C.3.1**.: _For any \(\delta\in(0,1)\) and any policy sequence \(\{\pi_{t}\}_{t=1}^{T}\), Algorithm 4 ensures that_

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}}-q^{P,\pi_{t}}, \ell_{t}\right\rangle+\sum_{t=1}^{T}\left\langle q^{P,\pi^{*}}-q^{P_{t},\pi^{* }},\ell_{t}\right\rangle\right]=\mathcal{O}\left(C^{\textsf{P}}L\right).\]

**Lemma C.3.2**.: _For any \(\delta\in(0,1)\) and any mapping \(\pi^{*}:S\to A\), Algorithm 4 ensures that_

\[\begin{split}&\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P_{t},\pi_{t}},b_{t}-\beta_{t}\right\rangle+\left\langle q^{\widehat{P}_{t},\pi^{* }},\beta_{t}-b_{t}\right\rangle\right]=\mathcal{O}\left(C^{\textsf{P}}L|S|^{2 }|A|\log^{2}\left(T\right)\right).\end{split}\]

**Lemma C.3.3**.: _For any \(\delta\in(0,1)\) and any mapping \(\pi^{*}:S\to A\), Algorithm 4 ensures that_

\[\begin{split}&\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P, \pi_{t}},\ell_{t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi_{t}}, \alpha_{t}\right\rangle+\left\langle q^{\widehat{P}_{t},\pi^{*}},\alpha_{t} \right\rangle-\left\langle q^{P,\pi^{*}},\ell_{t}\right\rangle\right]\\ &=\mathcal{O}\left(\mathbb{S}_{1}\left(L^{3}|S|^{2}|A|\log^{2} \left(\iota\right)\right)+\left(C^{\textsf{P}}+1\right)L^{2}|S|^{4}|A|\log^{2 }\left(\iota\right)+\delta L|S|^{2}|A|T^{2}\right).\end{split}\]

**Lemma C.3.4**.: _With the learning rates \(\{\gamma_{t}\}_{t=1}^{T}\) defined in Definition 5.2, Algorithm 4 ensures that for any \(\delta\in(0,1)\) and any mapping \(\pi^{*}\) that,_

\[\begin{split}&\textsc{EstReg}(\pi^{*})\\ &=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\widehat{P}_{t}, \pi_{t}},\widehat{\ell}_{t}-b_{t}\right\rangle-\left\langle q^{\widehat{P}_{t},\pi^{*}},\widehat{\ell}_{t}-b_{t}\right\rangle\right]\\ &=\mathcal{O}\left(\mathbb{S}_{2}\left(L^{2}|S||A|\log^{2} \left(\iota\right)\right)+\left(C^{\textsf{P}}+1\right)L|S|^{2}|A|^{2}\log^{2} \left(\iota\right)+\delta TL^{2}|S|^{2}|A|\log(\iota)\right).\end{split}\]

[MISSING_PAGE_FAIL:33]

where the last step repeats the same argument of Lemma 3.1 for every epoch and the number of epochs is at most \(O(|S||A|\log T)\) according to (Jin et al., 2021, Lemma D.3.12).

Bounding \(\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\bar{P}_{t},\pi^{*}},\beta_{t}-b_{t }\right\rangle\right]\).For this term, we show that for any given epoch \(i\), \(\sum_{t\in E_{i}}\left\langle q^{\bar{P}_{t},\pi^{*}},\beta_{t}-b_{t}\right\rangle\leq 0\) which yields \(\sum_{t=1}^{T}\left\langle q^{\bar{P}_{t},\pi^{*}},\beta_{t}-b_{t}\right\rangle\leq 0\). To this end, we first consider a fixed epoch \(i\) and upper-bound

\[\left\langle q^{\bar{P}_{t},\pi^{*}},\beta_{t}\right\rangle=\sum_{s,a}q^{\bar{ P}_{t},\pi^{*}}(s,a)\left(\frac{\left(q^{P_{t},\pi_{t}}(s)-q^{P,\pi_{t}}(s) \right)\ell_{t}(s,a)}{u_{t}(s)}\right)\leq\sum_{s,a}q^{\bar{P}_{t},\pi^{*}}(s, a)\left(\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}\right),\]

where the second step uses Corollary D.3.6 to bound \(\left(q^{P_{t},\pi_{t}}(s)-q^{P,\pi_{t}}(s)\right)\leq C_{t}^{\mathsf{P}}\).

For any epoch \(i\) and episode \(t\in E_{i}\), we have \(\widehat{P}_{t}=\widehat{P}_{i(t)}\), which gives that

\[\sum_{t=1}^{T}\left\langle q^{\widehat{P}_{t},\pi^{*}},\beta_{t}- b_{t}\right\rangle =\sum_{i=1}^{N}\sum_{t\in E_{i}}\left\langle q^{\widehat{P}_{t}, \pi^{*}},\beta_{t}-b_{t}\right\rangle\] \[\leq\sum_{i=1}^{N}\sum_{s,a}q^{\widehat{P}_{i},\pi^{*}}(s,a)\sum_ {t\in E_{i}}\left(\frac{C_{t}^{\mathsf{P}}}{u_{t}(s)}-b_{t}(s)\right)\] \[\leq 0,\]

where the last step applies Lemma 3.1 for every epoch \(i\). Thus, \(\mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{\bar{P}_{t},\pi^{*}},\beta_{t}- b_{t}\right\rangle\right]\leq 0\) holds.

### Proof of Lemma c.3.3

We introduce the following lemma to evaluate the estimated performance via the true occupancy measure, which helps us analyze the the estimation error.

**Lemma C.6.1**.: _For any transition function pair \((P,\widehat{P})\) (\(P\) and \(\widehat{P}\) can be optimistic transition), policy \(\pi\), and loss function \(\ell\), it holds that \(\left\langle q^{P,\pi},\ell\right\rangle=\left\langle q^{\widehat{P},\pi}, \mathcal{Z}_{\ell}^{P,\widehat{P},\pi}\right\rangle\), where \(\mathcal{Z}_{\ell}^{P,\widehat{P},\pi}\) is defined as_

\[\mathcal{Z}_{\ell}^{P,\widehat{P},\pi}(s,a) =Q^{P,\pi}(s,a;\ell)-\sum_{s^{\prime}\in S_{k(s)+1}}\widehat{P}( s^{\prime}|s,a)V^{P,\pi}(s^{\prime};\ell)\] \[=\ell(s,a)+\sum_{s^{\prime}\in S_{k(s)+1}}\left(P(s^{\prime}|s,a) -\widehat{P}(s^{\prime}|s,a)\right)V^{P,\pi}(s^{\prime};\ell)\]

_for all state-action pairs \((s,a)\)._

Proof.: By direct calculation, we have:

\[V^{P,\pi}(s;\ell) =\sum_{a}\pi(a|s)Q^{P,\pi}(s,a;\ell)\] \[=\sum_{a}\pi(a|s)\left(Q^{P,\pi}(s,a;\ell)-\sum_{s^{\prime}\in S _{k(s)+1}}\widehat{P}(s^{\prime}|s,a)V^{P,\pi}(s;\ell)\right)\] \[\quad+\sum_{a}\pi(a|s)\sum_{s^{\prime}\in S_{k(s)+1}}\widehat{P} (s^{\prime}|s,a)V^{P,\pi}(s^{\prime};\ell)\] \[=\sum_{s^{\prime}\in S_{k(s)+1}}q^{\widehat{P},\pi}(s^{\prime}|s) V^{P,\pi}(s^{\prime};\ell)\] \[\quad+\sum_{a}\pi(a|s)\left(Q^{P,\pi}(s,a;\ell)-\sum_{s^{\prime} \in S_{k(s)+1}}\widehat{P}(s^{\prime}|s,a)V^{P,\pi}(s;\ell)\right)\]\[=\sum_{k=k(s)}^{L-1}\sum_{s^{\prime}\in S_{k}}\sum_{a\in A}q^{\widehat{P}, \pi}(s^{\prime},a|s)\left(Q^{P,\pi}(s^{\prime},a;\ell)-\sum_{s^{\prime\prime}\in S _{k(s^{\prime})+1}}\widehat{P}(s^{\prime\prime}|s^{\prime},a)V^{P,\pi}(s^{ \prime\prime};\ell)\right)\] \[=\sum_{k=k(s)}^{L-1}\sum_{s^{\prime}\in S_{k}}\sum_{a\in A}q^{ \widehat{P},\pi}(s^{\prime},a|s)\mathcal{Z}_{\ell}^{P,\widehat{P},\pi}(s^{ \prime},a)\]

where the second to last step follows from recursively repeating the first three steps. The proof is completed by noticing \(\left\langle q^{P,\pi},\ell\right\rangle=V^{P,\pi}(s_{0},\ell)\). 

According to Lemma C.6.1, we rewrite \(\left\langle q^{\widehat{P}_{i},\pi_{t}},\alpha_{t}\right\rangle\) and \(\left\langle q^{\widehat{P}_{i},\pi^{*}},\alpha_{t}\right\rangle\) with \(q^{P,\pi_{t}}\) and \(q^{P,\pi^{*}}\) for any policy \(\pi^{*}\) as

Therefore, we can further decompose as

\[= \mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\pi_{t}},\ell_{t}- \mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi_{t}}\right\rangle-\left\langle q ^{P,\pi^{*}},\ell_{t}-\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi^{*}} \right\rangle\right]\] \[= \mathbb{E}\left[\sum_{t=1}^{T}\left\langle q^{P,\pi_{t}}-q^{P,\pi^ {*}},\ell_{t}-\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi^{*}}\right\rangle +\left\langle q^{P,\pi_{t}},\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi^{*} }-\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi_{t}}\right\rangle\right]\] \[= \mathbb{E}\left[\underbrace{\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{ a\in A}\left(q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right)\left(\ell_{t}(s,a)- \alpha_{t}(s,a)\right)}_{\text{Term\,I}(\pi^{*})}\right]\] \[+ \mathbb{E}\left[\underbrace{\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{ a\in A}\left(q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right)\hskip-11.381102pt\sum_{s^{ \prime}\in S_{k(s)+1}}\hskip-11.381102pt\left(P(s^{\prime}|s,a)-\widehat{P}_{ t}(s^{\prime}|s,a)\right)V^{\widehat{P}_{i},\pi^{*}}(s^{\prime};\alpha_{t})}_{ \text{Term\,2}(\pi^{*})}\right]\] \[+ \mathbb{E}\left[\underbrace{\sum_{t=1}^{T}\left\langle q^{P,\pi_ {t}},\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{i},P,\pi^{*}}-\mathcal{Z}_{\alpha _{t}}^{\widehat{P}_{i},P,\pi_{t}}\right\rangle}_{\text{Term\,3}(\pi^{*})} \right]\!.\]

We will bound these terms with some self-bounding quantities in Lemma C.6.2, Lemma C.6.3 and Lemma C.6.4 respectively. In these proofs, we will follow the idea of Lemma D.1.1 to first bound these terms conditioning on the events \(\mathcal{E}_{\textsc{rst}}\) and \(\mathcal{E}_{\textsc{cos}}\) defined in Proposition D.5.2 and Eq. (4), while ensuring that these terms are always bounded by \(\mathcal{O}\left(|S|^{2}|A|T^{2}\right)\) in the worst case.

#### c.6.1 Bounding Term 1

**Lemma C.6.2**.: _For any \(\delta\in(0,1)\) and any mapping \(\pi^{\star}:S\to A\), Algorithm 4 ensures that \(\mathbb{E}\left[\text{Term\,1}(\pi^{\star})\right]\) is bounded by_

\[\mathcal{O}\left(\mathbb{S}_{1}\left(L^{2}|S|^{2}|A|\log^{2}\left(\iota\right) \right)+\delta|S|^{2}|A|T^{2}+\left(C^{\mathcal{P}}+1\right)L^{2}|S|^{4}|A| \log^{2}\left(\iota\right)\right).\]

Proof.: Clearly, we have \(\alpha_{t}(s,a)\leq|S|T\) for every state-action pair \((s,a)\), due to the fact that \(u_{t}(s)\geq\nicefrac{{1}}{{|S|T}}\) (Lemma D.2.8). Therefore, we have Term\(1(\pi^{\star})\leq|S|^{2}|A|T^{2}\) holds always. In the remaining, our main goal is to bound Term\(1(\pi^{\star})\) conditioning on \(\mathcal{E}_{\textsc{rst}}\).

For any state-action pair \((s,a)\in S\times A\) and episode \(t\), we have

\[\ell_{t}(s,a)-\alpha_{t}(s,a)=\frac{\left(u_{t}(s,a)-q^{P,\pi_{t}}(s,a)\right) \ell_{t}(s,a)}{u_{t}(s,a)}=\frac{\left(u_{t}(s)-q^{P,\pi_{t}}(s)\right)\ell_{t} (s,a)}{u_{t}(s)}\geq 0,\]conditioning on the event \(\mathcal{E}_{\text{\tiny CON}}\).

Therefore, under event \(\mathcal{E}_{\text{\tiny CON}}\wedge\mathcal{E}_{\text{\tiny EST}}\), Term 1\((\pi^{\star})\) is bounded by

\[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\in A}\left(q^{P,\pi_{t}}(s,a)-q^{P,\pi^{\star}}(s,a)\right)\left(\ell_{t}(s,a)-\alpha_{t}(s,a)\right)\] \[\leq\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\in A}\left[q^{P,\pi_{t }}(s,a)-q^{P,\pi^{\star}}(s,a)\right]_{+}\frac{\left(u_{t}(s)-q^{P,\pi_{t}}(s) \right)\ell_{t}(s,a)}{u_{t}(s)}\] \[\leq\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\in A}\frac{\left[q^{P, \pi_{t}}(s,a)-q^{P,\pi^{\star}}(s,a)\right]_{+}}{u_{t}(s)}\cdot\left|u_{t}(s)-q ^{P,\pi_{t}}(s)\right|\] \[=\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}\left(\iota\right)\sum _{t=1}^{T}\sum_{s\neq s_{L}}q^{P,\pi_{t}}(s)\cdot\left(\sum_{a}\frac{\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{\star}}(s,a)\right]_{+}}{u_{t}(s)}\right)^{2}}\right)\] \[\quad+\mathcal{O}\left(\left(C^{\mathsf{P}}+\log\left(\iota\right) \right)L^{2}|S|^{4}|A|\log\left(\iota\right)\right)\] \[\leq\mathcal{O}\left(\sqrt{L^{2}|S|^{2}|A|\log^{2}\left(\iota \right)\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}}(s)q^{P,\pi_{t}} (s,a)}+\left(C^{\mathsf{P}}+\log\left(\iota\right)\right)L^{2}|S|^{4}|A|\log \left(\iota\right)\right)\] \[\leq\mathcal{O}\left(\sqrt{L^{2}|S|^{2}|A|\log^{2}\left(\iota \right)\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}}(s)q^{P,\pi_{t}} (s,a)}+\left(C^{\mathsf{P}}+1\right)L^{2}|S|^{4}|A|\log^{2}\left(\iota\right) \right),\]

where the first step follows from the non-negativity of \(\ell_{t}(s,a)-\alpha_{t}(s,a)\); the third step applies Lemma D.5.4 with \(G=1\) as \(\sum_{a}\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{\star}}(s,a)\right]_{+}\leq q^{P,\pi _{t}}(s)\leq u_{t}(s)\); the fifth step follows from the fact that \(\sum_{s\neq s_{L}}\sum_{a\in A}\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{\star}}(s,a) \right]_{+}\leq 2L\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}}(s)q^{P,\pi_{t}} (s,a)\) according to Corollary D.3.5; the last step applies Corollary D.3.6.

Applying Lemma D.1.1 with event \(\mathcal{E}_{\text{\tiny CON}}\wedge\mathcal{E}_{\text{\tiny EST}}\) yields that

\[\mathbb{E}\left[\textsc{Term 1}(\pi^{\star})\right] =\mathcal{O}\left(\mathbb{E}\left[\sqrt{L^{2}|S|^{2}|A|\log^{2} \left(\iota\right)\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}}(s)}q ^{P_{t},\pi_{t}}(s,a)\right]\right)\] \[\quad+\mathcal{O}\left(\left(C^{\mathsf{P}}+\log\left(\iota\right) \right)L^{2}|S|^{4}|A|\log\left(\iota\right)+\delta|S|^{2}|A|T^{2}\right)\] \[=\mathcal{O}\left(\mathbb{S}_{1}\left(L^{2}|S|^{2}|A|\log^{2} \left(\iota\right)\right)+\delta|S|^{2}|A|T^{2}+\left(C^{\mathsf{P}}+\log\left( \iota\right)\right)L^{2}|S|^{4}|A|\log\left(\iota\right)\right).\]

#### c.6.2 Bounding Term 2

**Lemma C.6.3**.: _For any \(\delta\in(0,1)\) and any mapping \(\pi^{\star}:S\to A\), Algorithm 4 ensures that_

Proof.: Suppose that \(\mathcal{E}_{\text{\tiny CON}}\wedge\mathcal{E}_{\text{\tiny EST}}\) occurs. We have

\[P(s^{\prime}|s,a)\geq\widehat{P}_{t}(s^{\prime}|s,a)=\widetilde{P}_{i}(t)(s^{ \prime}|s,a),\forall(s,a,s^{\prime})\in W_{k},k=0,\ldots,L-1.\]

Therefore, we can show that

\[0\leq\sum_{s^{\prime}\in S_{k(s)+1}}\hskip-14.226378pt\left(P(s^{\prime}|s,a)- \widehat{P}_{t}(s^{\prime}|s,a)\right)V^{\widehat{P}_{t},\pi^{\star}}(s^{ \prime};\alpha_{t})\]\[=\mathcal{O}\left(L\left(\sqrt{\frac{|S|\log\left(\iota\right)}{ \widehat{m}_{i(t)}(s,a)}+\frac{|S|\left(C^{\mathsf{P}}+\log\left(\iota\right) \right)}{\widehat{m}_{i(t)}(s,a)}}\right)\right),\]

where the last step follows from the definition of optimistic transition and the fact that \(\alpha_{t}(s,a)\leq 1\).

By direct calculation, we have Term 2(\(\pi^{*}\)) bounded by

\[\mathcal{O}\left(L\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T} \left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}\left(\sqrt{\frac{|S| \log\left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}}+\frac{|S|\left(C^{\mathsf{P} }+\log\left(\iota\right)\right)}{\widehat{m}_{i(t)}(s,a)}\right)\right)\] \[\leq\mathcal{O}\left(L\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T }\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}\sqrt{\frac{|S|\log \left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}}\right)\] \[\quad+\mathcal{O}\left(L|S|\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t =1}^{T}q^{P,\pi_{t}}(s,a)\left(\frac{\left(C^{\mathsf{P}}+\log\left(\iota \right)\right)}{\widehat{m}_{i(t)}(s,a)}\right)\right)\] \[=\mathcal{O}\left(L\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T} \left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}\sqrt{\frac{|S|\log \left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}}+L|S|^{2}|A|\left(C^{\mathsf{P}}+ \log\left(\iota\right)\right)\log\left(T\right)\right),\]

where the last step follows from Lemma D.5.3.

Moreover, we have

\[\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T}\left[q^{P,\pi_{t}} (s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}\sqrt{\frac{|S|\log\left(\iota\right)}{ \widehat{m}_{i(t)}(s,a)}}\] \[\leq\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T}\sqrt{\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}}\cdot\sqrt{\frac{q^{P,\pi_{t}}(s,a)|S|\log\left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}}\] \[\leq\sqrt{\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T}\left[q^{ P,\pi_{t}}(s,a)-q^{P,\pi^{*}}(s,a)\right]_{+}}\cdot\sqrt{\sum_{s\neq s_{L}} \sum_{a\in A}\sum_{t=1}^{T}\frac{q^{P,\pi_{t}}(s,a)|S|\log\left(\iota\right)} {\widehat{m}_{i(t)}(s,a)}}\] \[=\mathcal{O}\left(\sqrt{|S|^{2}|A|\log^{2}\left(\iota\right)\sum _{s\neq s_{L}}\sum_{a\in A}\sum_{t=1}^{T}\left[q^{P,\pi_{t}}(s,a)-q^{P,\pi^{*} }(s,a)\right]_{+}}\right)\] \[\leq\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}\left(\iota\right) \sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}}(s)q^{P,\pi_{t}}(s,a)}\right)\] \[\leq\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}\left(\iota\right) \sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{*}}(s)q^{P_{t},\pi_{t}}(s,a)+ \sqrt{L^{2}|S|^{2}|A|\log^{2}C^{\mathsf{P}}}}\right),\]

where the second step uses the Cauchy-Schwarz inequality; the third step applies Lemma D.5.3; the fifth step follows from Corollary D.3.5; the last step uses Corollary D.3.6 and the fact that \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) for any \(x,y\geq 0\).

Finally, applying Lemma D.1.1 finishes the proof. 

#### c.6.3 Bounding Term 3

**Lemma C.6.4**.: _For any \(\delta\in(0,1)\) and the policy \(\pi^{\star}\), Algorithm 4 ensures that_

Proof.: Suppose that \(\mathcal{E}_{\text{\tiny{CON}}}\wedge\mathcal{E}_{\text{\tiny{ST}}}\) occurs. We first have:

\[\left\langle q^{P,\pi_{t}},\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{t},P,\pi^{ *}}-\mathcal{Z}_{\alpha_{t}}^{\widehat{P}_{t},P,\pi_{t}}\right\rangle\]\[=\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{t}}(s,a)\sum_{s^{\prime}\in S _{k(s)+1}}\left(\widehat{P}_{t}(s^{\prime}|s,a)-P(s^{\prime}|s,a)\right)\left(V^ {\widehat{P}_{t},\pi^{*}}(s^{\prime};\alpha_{t})-V^{\widehat{P}_{t},\pi_{t}}(s^ {\prime};\alpha_{t})\right)\] \[\leq 2\sum_{s\neq s_{L}}\sum_{a\in A}\sum_{s^{\prime}\in S_{k(s)+1} }q^{P,\pi_{t}}(s,a)B_{i(t)}(s,a,s^{\prime})\left|V^{\widehat{P}_{t},\pi^{*}}(s^ {\prime};\alpha_{t})-V^{\widehat{P}_{t},\pi_{t}}(s^{\prime};\alpha_{t})\right|\] \[\leq\mathcal{O}\left(\sum_{k=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{ k}}q^{P,\pi_{t}}(s,a)\sqrt{\frac{P(s^{\prime}|s,a)\log{(\iota)}}{\widehat{m}_{ i(t)}(s,a)}}\cdot\left|V^{\widehat{P}_{t},\pi^{*}}(s^{\prime};\alpha_{t})-V^{ \widehat{P}_{t},\pi_{t}}(s^{\prime};\alpha_{t})\right|\right)\] \[\quad+\mathcal{O}\left(L|S|\sum_{s\neq s_{L}}\sum_{a\in A}q^{P, \pi_{t}}(s,a)\left(\frac{C^{\mathsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(s,a)} \right)\right),\]

where the last step follows from Lemma D.2.7 and the fact that \(V^{\widehat{P}_{t},\pi}(s^{\prime};\alpha_{t})\leq L\) for any \(\pi\).

By applying Lemma D.5.3, we have the second term bounded by \(\mathcal{O}\left(\left(C^{\mathsf{P}}+\log{(\iota)}\right)L^{2}|S|^{4}|A|\log {(\iota)}\right)\). On the other hand, for the first term, we can bound \(\left|V^{\widehat{P}_{t},\pi^{*}}(s^{\prime};\alpha_{t})-V^{\widehat{P}_{t}, \pi_{t}}(s^{\prime};\alpha_{t})\right|\) as

\[\left|V^{\widehat{P}_{t},\pi^{*}}(s^{\prime};\alpha_{t})-V^{ \widehat{P}_{t},\pi_{t}}(s^{\prime};\alpha_{t})\right|\] \[\leq\sum_{u\neq s_{L}}\sum_{v\in A}q^{\widehat{P}_{t},\pi_{t}}(u| s^{\prime})\left|\pi_{t}(v|u)-\pi^{*}(v|u)\right|Q^{\widehat{P}_{t},\pi^{*}}(u,v; \alpha_{t})\] \[\leq L\sum_{u\in S}\sum_{v\in A}q^{\widehat{P}_{t},\pi_{t}}(u|s^{ \prime})\left|\pi_{t}(v|u)-\pi^{*}(v|u)\right|\] \[\leq\mathcal{O}\left(L\sum_{k=k(s^{\prime})}^{L-1}\sum_{u\in S_{k }}\sum_{v\neq\pi^{*}(u)}q^{\widehat{P}_{t},\pi_{t}}(u,v|s^{\prime})\right)\] \[\leq\mathcal{O}\left(L\sum_{k=k(s^{\prime})}^{L-1}\sum_{u\in S_{k }}\sum_{v\neq\pi^{*}(u)}q^{P,\pi_{t}}(u,v|s^{\prime})\right)\]

where the first step follows from Lemma D.3.2; the second step follows from the fact that \(Q^{\widehat{P}_{t},\pi^{*}}(u,v;\alpha_{t})\in[0,L]\); the third step uses the same reasoning as the proof of Corollary D.3.5; and the last step uses Corollary C.8.2.

Finally, we consider the following term

\[\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{h}}q^{ P,\pi_{t}}(s,a)\sqrt{\frac{P(s^{\prime}|s,a)\log{(\iota)}}{\widehat{m}_{i(t)}(s,a )}}\sum_{k=h+1}^{L-1}\sum_{u\in S_{k}}\sum_{v\neq\pi^{*}(u)}q^{P,\pi_{t}}(u,v|s ^{\prime})\] \[=\sum_{t=1}^{T}\sum_{h=0}^{L-1}\sum_{(s,a,s^{\prime})\in W_{h}} \sum_{k=h+1}^{L-1}\sum_{u\in S_{k}}\sum_{v\neq\pi^{*}(u)}\sqrt{\frac{q^{P,\pi_{ t}}(s,a)q^{P,\pi_{t}}(u,v|s^{\prime})\log{(\iota)}}{\widehat{m}_{i(t)}(s,a)}}\] \[\quad\cdot\sqrt{q^{P,\pi_{t}}(s,a)P(s^{\prime}|s,a)q^{P,\pi_{t}}( u,v|s^{\prime})}\] \[\leq\sum_{h=0}^{L-1}\sqrt{\sum_{t=1}^{T}\sum_{(s,a,s^{\prime}) \in W_{h}}\sum_{k=h+1}^{L-1}\sum_{u\in S_{k}}\sum_{v\neq\pi^{*}(u)}\frac{q^{P, \pi_{t}}(s,a)q^{P,\pi_{t}}(u,v|s^{\prime})\log{(\iota)}}{\widehat{m}_{i(t)}(s,a )}}\] \[\quad\cdot\sqrt{\sum_{t=1}^{T}\sum_{(s,a,s^{\prime})\in W_{h}}\sum _{k=h+1}^{L-1}\sum_{u\in S_{k}}\sum_{v\neq\pi^{*}(u)}q^{P,\pi_{t}}(s,a)P(s^{ \prime}|s,a)q^{P,\pi_{t}}(u,v|s^{\prime})}\] \[\leq\sum_{h=0}^{L-1}\sqrt{\left|S_{h+1}\right|}L\sum_{t=1}^{T} \sum_{s\in S_{h}}\sum_{a\in A}\frac{q^{P,\pi_{t}}(s,a)\log{(\iota)}}{\widehat{m} _{i(t)}(s,a)}\cdot\sqrt{\sum_{t=1}^{T}\sum_{u\in S}\sum_{v\neq\pi^{*}(u)}}q^{P,\pi_{t}}(u,v)\]\[\leq\left(\sum_{h=0}^{L-1}\sqrt{L|S_{h+1}||S_{h}||A|\log^{2}{(\iota)}} \right)\cdot\sqrt{\sum_{t=1}^{T}\sum_{u\in S}\sum_{v\neq\pi^{*}(u)}q^{P,\pi_{t}}( u,v)}\] \[=\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}{(\iota)}\sum_{t=1}^{T} \sum_{s\neq\pi_{s}}\sum_{a\neq\pi^{*}(a)}q^{P,\pi_{t}}(s,a)}\right)\] \[=\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}{(\iota)}\sum_{t=1}^{T} \sum_{s\neq\pi_{s}}\sum_{a\neq\pi^{*}(a)}q^{P_{t},\pi_{t}}(s,a)}+\sqrt{C^{ \mathcal{P}}\cdot L^{2}|S|^{2}|A|\log^{2}{(\iota)}}\right),\]

where the second step applies Cauchy-Schwarz inequality; the third step follows from the fact that \(\sum_{s\in S_{k}}\sum_{a\in A}\sum_{s^{\prime}\in S_{k(s)+1}}q^{P,\pi_{t}}(s, a)P(s^{\prime}|s,a)q^{P,\pi_{t}}(u,v|s^{\prime})=q^{P,\pi_{t}}(u,v)\); the fourth step follows from Lemma D.5.3 conditioning on the event \(\mathcal{E}_{\text{EST}}\); the fifth step uses the fact that \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) for \(x,y\geq 0\); the last step follows from Corollary D.3.6. 

### Proof of Lemma c.3.4

In this section we bound EstReg using a learning rate that depends on \(t\) and \((s,a)\), which is crucial to obtain a self-bounding quantity. Another key observation is that the estimated transition function is constant within each epoch, so we first bound EstReg within one epoch before summing them.

Recall that \(E_{i}\) is a set of episodes that belong to epoch \(i\) and \(N\) is the total number of epochs through \(T\) episodes. By using the fact that \(\widehat{P}_{t}=\widetilde{P}_{i}\) for episode \(t\) belonging to epoch \(i\), we make the following decomposition EstReg\((\pi^{\star})\)

\[\textsc{EstReg}(\pi^{\star}) =\mathbb{E}\left[\sum_{i=1}^{N}\sum_{t\in E_{i}}\left\langle q^{ \widetilde{P}_{i},\pi_{t}}-q^{\widetilde{P}_{i},\pi^{\star}},\widehat{\ell}_{ t}-b_{t}\right\rangle\right]\] \[\leq\mathbb{E}\left[\sum_{i=1}^{N}\mathbb{E}_{t_{i}}\left[\sum_{t \in E_{i}}\left\langle q^{\widetilde{P}_{i},\pi_{t}}-q^{\widehat{P}_{i},\pi^{ \star}},\widehat{\ell}_{t}-b_{t}\right\rangle\right]\right]=\mathbb{E}\left[ \sum_{i=1}^{N}\textsc{EstReg}_{i}(\pi^{\star})\right].\]

This learning rate is defined in Definition 5.2 and restated below.

**Definition C.7.1**.: _For any \(t\), if it is the starting episode of an epoch, we set \(\gamma_{t}(s,a)=256L^{2}|S|\); otherwise, we set_

\[\gamma_{t+1}(s,a)=\gamma_{t}(s,a)+\frac{D\nu_{t}(s,a)}{2\gamma_{t}(s,a)},\]

_where \(D=\frac{1}{\log(\iota)}\) and_

\[\nu_{t}(s,a)=q^{\widetilde{P}_{i(\iota)},\pi_{t}}(s,a)^{2}\left(Q^{\widetilde {P}_{i(\iota)},\pi_{t}}(s,a;\widehat{\ell}_{t})-V^{\widetilde{P}_{i(\iota)}, \pi_{t}}(s;\widehat{\ell}_{t})\right)^{2}. \tag{35}\]

Importantly, both \(Q^{\widetilde{P}_{i(\iota)},\pi_{t}}(s,a;\widehat{\ell}_{t})\) and \(V^{\widetilde{P}_{i(\iota)},\pi_{t}}(s;\widehat{\ell}_{t})\) can be computed, which ensures that the learning rate is properly defined.

#### c.7.1 Properties of the Learning Rate

In this section, we prove key properties of \(\nu_{t}(s,a)\) and of \(\gamma_{t}(s,a)\). We first present some results in Lemma C.7.2 that are useful to bound \(\nu_{t}(s,a)\), and then use these results to bound \(\gamma_{t}(s,a)\).

**Lemma C.7.2**.: _For any state-action pair \((s,a)\) and any episode \(t\), it holds that_

\[q^{\widehat{P}_{t},\pi_{t}}(s,a)Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{\ell }_{t})\leq L,\text{ and }q^{\widehat{P}_{t},\pi_{t}}(s)V^{\widehat{P}_{t},\pi_{t}} (s;\widehat{\ell}_{t})\leq L,\ \forall(s,a)\in S\times A,\]

_which ensures \(q^{\widehat{P}_{t},\pi_{t}}(s,a)\left(Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{ \ell}_{t})-V^{\widehat{P}_{t},\pi_{t}}(s;\widehat{\ell}_{t})\right)\in[-L,L]\). Suppose that the high-probability event \(\mathcal{E}_{\text{\rm{con}}}\) holds. Then, it further holds for all state-action pair \((s,a)\) that_

\[q^{\widehat{P}_{t},\pi_{t}}(s,a)^{2}\cdot\mathbb{E}_{t}\left[\left(Q^{\widehat {P}_{t},\pi_{t}}(s,a;\widehat{\ell}_{t})-V^{\widehat{P}_{t},\pi_{t}}(s;\widehat {\ell}_{t})\right)^{2}\right]\leq\mathcal{O}\left(L^{2}q^{\widehat{P}_{t},\pi_{ t}}(s,a)\left(1-\pi_{t}(a|s)\right)+L|S|C^{\mathcal{P}}_{t}\right),\]

_where \(\widehat{P}_{t}\) here is the optimistic transition \(\widetilde{P}_{i(t)}\) defined in Definition C.1.1._Proof.: We first verify \(q^{\widehat{P}_{t},\pi_{t}}(s,a)Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{\ell}_{t})\leq L\):

\[q^{\widehat{P}_{t},\pi_{t}}(s,a)Q^{\widehat{P}_{t},\pi_{t}}(s,a; \widehat{\ell}_{t})\] \[=q^{\widehat{P}_{t},\pi_{t}}(s,a)\sum_{h=k(s)}\sum_{u\in S}\sum_{ v\in A}q^{\widehat{P}_{t},\pi_{t}}(u,v|s,a)\widehat{\ell}_{t}(u,v)\] \[=q^{\widehat{P}_{t},\pi_{t}}(s,a)\sum_{h=k(s)}\sum_{u\in S}\sum_{ v\in A}q^{\widehat{P}_{t},\pi_{t}}(u,v|s,a)\cdot\frac{\mathbb{I}_{t}(u,v)\ell_{t} (u,v)}{u_{t}(u,v)}\] \[\leq\sum_{h=k(s)}\sum_{u\in S_{h}}\sum_{v\in A}\mathbb{I}_{t}(u, v)\cdot\frac{q^{\widehat{P}_{t},\pi_{t}}(s,a)q^{\widehat{P}_{t},\pi_{t}}(u,v|s,a)}{u _{t}(u,v)}\] \[\leq\sum_{h=k(s)}\sum_{u\in S_{h}}\sum_{v\in A}\mathbb{I}_{t}(u, v)\leq L,\]

where the second step follows from the definition of loss estimator, the fourth step follows from \(q^{\widehat{P}_{t},\pi_{t}}(s,a)q^{\widehat{P}_{t},\pi_{t}}(u,v|s,a)\leq q^{ \widehat{P}_{t},\pi_{t}}(u,v)\leq u_{t}(u,v)\), and the last step uses the fact that \(\sum_{u\in S_{h}}\sum_{v\in A}\mathbb{I}_{t}(u,v)=1\). Following the same idea, we can show that \(q^{\widehat{P}_{t},\pi_{t}}(s)V^{\widehat{P}_{t},\pi_{t}}(s;\widehat{\ell}_{t})\leq L\) as well.

Next, we have \(\mathbb{E}_{t}\left[\left(Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{\ell}_{t})-V ^{\widehat{P}_{t},\pi_{t}}(s;\widehat{\ell}_{t})\right)^{2}\right]\) bounded as

\[\mathbb{E}_{t}\left[\left(Q^{\widehat{P}_{t},\pi_{t}}(s,a; \widehat{\ell}_{t})-V^{\widehat{P}_{t},\pi_{t}}(s;\widehat{\ell}_{t})\right)^ {2}\right]\] \[=\mathbb{E}_{t}\left[\left(Q^{\widehat{P}_{t},\pi_{t}}(s,a; \widehat{\ell}_{t})-\pi_{t}(a|s)Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{\ell }_{t})-\sum_{a^{\prime}\neq a}\pi_{t}(a^{\prime}|s)Q^{\widehat{P}_{t},\pi_{t}} (s,a^{\prime};\widehat{\ell}_{t})\right)^{2}\right]\] \[\leq 2\cdot\mathbb{E}_{t}\left[\left(Q^{\widehat{P}_{t},\pi_{t}}( s,a;\widehat{\ell}_{t})-\pi_{t}(a|s)Q^{\widehat{P}_{t},\pi_{t}}(s,a;\widehat{ \ell}_{t})\right)^{2}\right]\] \[\quad+2\cdot\mathbb{E}_{t}\left[\left(\sum_{a^{\prime}\neq a}\pi _{t}(a^{\prime}|s)Q^{\widehat{P}_{t},\pi_{t}}(s,a^{\prime};\widehat{\ell}_{t}) \right)^{2}\right]\] \[=2\left(1-\pi_{t}(a|s)\right)^{2}\mathbb{E}_{t}\left[\left(Q^{ \widehat{P}_{t},\pi_{t}}(s,a;\widehat{\ell}_{t})\right)^{2}\right]+2\mathbb{E} _{t}\left[\left(\sum_{a^{\prime}\neq a}\pi_{t}(a^{\prime}|s)Q^{\widehat{P}_{t},\pi_{t}}(s,a^{\prime};\widehat{\ell}_{t})\right)^{2}\right], \tag{36}\]

where the second step follows from the fact that \(\left(x+y\right)^{2}\leq 2\left(x^{2}+y^{2}\right)\) for any \(x,y\in\mathbb{R}\).

By direct calculation, we have

\[\mathbb{E}_{t}\left[\left(Q^{\widehat{P}_{t},\pi_{t}}(s,a; \widehat{\ell}_{t})\right)^{2}\right]\] \[=\mathbb{E}_{t}\left[\left(\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}} \sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\widehat{\ell}_{t}(x,y) \right)^{2}\right]\] \[\leq L\cdot\mathbb{E}_{t}\left[\sum_{k=k(s)}^{L-1}\left(\sum_{x \in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\widehat{\ell}_{t}( x,y)\right)^{2}\right]\] \[=L\cdot\mathbb{E}_{t}\left[\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}} \sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)^{2}\widehat{\ell}_{t}(x,y)^ {2}\right]\]\[\leq L\cdot\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}}\sum_{y\in A}q^{ \widehat{P}_{t},\pi_{t}}(x,y|s,a)^{2}\cdot\left(\frac{q^{P_{t},\pi_{t}}(x,y)}{u_ {t}(x,y)^{2}}\right)\] \[=\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^{L-1 }\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\left(\frac{ q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)q^{\widehat{P}_{t},\pi_{t}}(s,a)}{u_{t}(s,a)} \right)\cdot\left(\frac{q^{P_{t},\pi_{t}}(x,y)}{u_{t}(s,a)}\right)\] \[\leq\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^{ L-1}\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\left(\frac{ q^{P_{t},\pi_{t}}(x,y)}{u_{t}(s,a)}\right)\] \[\leq\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^{ L-1}\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\left(\frac{ q^{P,\pi_{t}}(x,y)}{u_{t}(x,y)}+\frac{C_{t}^{\mathsf{P}}}{u_{t}(x,y)}\right), \tag{37}\]

where the second step uses Cauchy-Schwarz inequality; the third step uses the fact that \(\widehat{\ell}_{t}(s,a)\cdot\widehat{\ell}_{t}(s^{\prime},a^{\prime})=0\) for any \((s,a)\neq(s^{\prime},a^{\prime})\); the fourth step takes the conditional expectation of \(\widehat{\ell}_{t}(x,y)^{2}\); the sixth step follows from the fact that \(q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)q^{\widehat{P}_{t},\pi_{t}}(s,a)\leq q^{ \widehat{P}_{t},\pi_{t}}(x,y)\leq u_{t}(x,y)\) according to the definition of upper occupancy bound; the last step follows from Corollary D.3.6.

Similarly, for the second term in Eq. (36), we have

\[\mathbb{E}_{t}\left[\left(\sum_{b\neq a}\pi_{t}(b|s)Q^{\widehat{P }_{t},\pi_{t}}(s,b,\widehat{\ell}_{t})\right)^{2}\right]\] \[\leq L\cdot\mathbb{E}_{t}\left[\sum_{k=k(s)}^{L-1}\left(\sum_{x \in S_{k}}\sum_{y\in A}\left(\sum_{b\neq a}\pi_{t}(b|s)q^{\widehat{P}_{t},\pi_ {t}}(x,y|s,b)\right)\widehat{\ell}_{t}(x,y)\right)^{2}\right]\] \[=L\cdot\mathbb{E}_{t}\left[\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}} \sum_{y\in A}\left(\sum_{b\neq a}\pi_{t}(b|s)q^{\widehat{P}_{t},\pi_{t}}(x,y|s,b)\right)^{2}\widehat{\ell}_{t}(x,y)^{2}\right]\] \[\leq L\cdot\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}}\sum_{y\in A} \left(\sum_{b\neq a}\pi_{t}(b|s)q^{\widehat{P}_{t},\pi_{t}}(x,y|s,b)\right)^{2 }\left(\frac{q^{P_{t},\pi_{t}}(x,y)}{u_{t}(x,y)^{2}}\right)\] \[\leq\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s)}\sum_{k=k(s)}^{L-1} \sum_{x\in S_{k}}\sum_{y\in A}\left(\sum_{b\neq a}\pi_{t}(b|s)q^{\widehat{P}_{ t},\pi_{t}}(x,y|s,b)\right)\left(\frac{q^{P_{t},\pi_{t}}(x,y)}{u_{t}(x,y)}\right)\] \[\leq\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s)}\sum_{b\neq a}\pi_{t} (b|s)\left(\sum_{k=k(s)}^{L-1}\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t}, \pi_{t}}(x,y|s,b)\left(\frac{q^{P,\pi_{t}}(x,y)}{u_{t}(x,y)}+\frac{C_{t}^{ \mathsf{P}}}{u_{t}(x,y)}\right)\right), \tag{38}\]

where the first three steps are following the same idea of previous analysis; the fourth step uses the fact that \(\sum_{b\neq a}q^{\widehat{P}_{t},\pi_{t}}(s)\pi_{t}(b|s)q^{\widehat{P}_{t},\pi_ {t}}(x,y|s,b)\leq q^{\widehat{P}_{t},\pi_{t}}(x,y)\); the last step follows from Corollary D.3.6 as well.

Conditioning on the event \(\mathcal{E}_{\text{CON}}\), we have the term in Eq. (37) further bounded as

\[\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^{L-1} \sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\left(\frac{ q^{P,\pi_{t}}(x,y)}{u_{t}(x,y)}+\frac{C_{t}^{\mathsf{P}}}{u_{t}(x,y)}\right)\] \[\leq\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^ {L-1}\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\left(1 +\frac{C_{t}^{\mathsf{P}}}{u_{t}(x,y)}\right)\] \[=\frac{L}{q^{\widehat{P}_{t},\pi_{t}}(s,a)}\cdot\sum_{k=k(s)}^{L-1 }\sum_{x\in S_{k}}\sum_{y\in A}q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)\]\[\leq\mathcal{O}\left(L^{2}q^{\widehat{P}_{t},\pi_{t}}(s,a)\left(1- \pi_{t}(a|s)\right)\right)\] \[\leq\mathcal{O}\left(L^{2}q^{\widehat{P}_{t},\pi_{t}}(s,a)\left(1- \pi_{t}(a|s)\right)\right)\]

where the third step follows from the facts that \(q^{\widehat{P}_{t},\pi_{t}}(x,y|s,a)q^{\widehat{P}_{t},\pi_{t}}(s,a)\leq q^{ \widehat{P}_{t},\pi_{t}}(x,y)\leq u_{t}(x,y)\) for any \((s,a),(x,y)\in S\times A\), and \(\sum_{b\neq a}q^{\widehat{P}_{t},\pi_{t}}(s)\pi_{t}(b|s)q^{\widehat{P}_{t},\pi_ {t}}(x,y|s,b)\leq q^{\widehat{P}_{t},\pi_{t}}(x,y)\) similarly.

The first part of Lemma C.7.2 ensures that \(\nu_{t}(s,a)\in[0,L^{2}]\), which can be used to bound the growth of the learning rate.

**Proposition C.7.3**.: _The learning rate \(\gamma_{t}\) defined in Definition 5.2 satisfies for any state-action pair (s, a):_

\[\gamma_{t}(s,a)\leq\sqrt{D\sum_{j=t_{i(t)}}^{t-1}\nu_{j}(s,a)}+\gamma_{t_{i(t)} }(s,a)\]

_where \(i(t)\) is the epoch to which episode \(t\) belongs and \(t_{i}\) is the first episode of epoch \(i\)._

Proof.: We prove this statement by induction on \(t\). The equation trivially holds for \(t=t_{i(t)}\) which is the first round of epoch \(i(t)\). For the induction step, we first note that \(D\nu_{t}(s,a)\in[0,L^{2}]\). We introduce some notations to simplify the proof: we use \(c\) to denote \(D\nu_{t}(s,a)\), \(x\) to denote \(\gamma_{t}(s,a)\), and the induction hypothesis is \(x\leq\sqrt{S}+\gamma\) where \(S=D\sum_{j=t_{i(t)}}^{t-1}\nu_{j}(s,a)\) and \(\gamma=\gamma_{t_{i(t)}}(s,a)\). Proving the induction step is the same as proving:

\[x+\frac{c}{2x}\leq\sqrt{S+c}+\gamma. \tag{41}\]

First, we can verify that \(f(x)=x+\frac{c}{2x}\) is an increasing function of \(x\) for \(x\geq\sqrt{c/2}\). As \(c\in[0,L^{2}]\) and \(x\geq\gamma\geq L\), we can use the induction hypothesis \(x\leq\sqrt{S}+\gamma\) to upper bound \(x\) in the left-hand side of Eq. (41), and we get:

\[x+\frac{c}{2x} =\frac{x^{2}+c/2}{x}\] \[\leq\frac{S+2\gamma\sqrt{S}+\gamma^{2}+c/2}{\sqrt{S}+\gamma}\] \[=\frac{S+\gamma\sqrt{S}+c/2}{\sqrt{S}+\gamma}+\frac{\gamma\sqrt{S }+\gamma^{2}}{\sqrt{S}+\gamma}\] \[=\frac{\sqrt{S}+\gamma}{\sqrt{S}+\gamma}\left(\sqrt{S}+\frac{c}{2 \sqrt{S}+2\gamma}\right)+\frac{\gamma\sqrt{S}+\gamma^{2}}{\sqrt{S}+\gamma}\] \[=\left(\sqrt{S}+\frac{c}{2\sqrt{S}+2\gamma}\right)+\gamma\] \[\leq\left(\sqrt{S}+\frac{c}{2\sqrt{S}+c}\right)+\gamma\] \[\leq\sqrt{S+c}+\gamma,\]

where the second to last step follows from \(c\leq L^{2}\) and \(\gamma\geq L\), which ensures that \(2\sqrt{S}+2\sqrt{\gamma}\geq 2\sqrt{S+\gamma^{2}}\geq 2\sqrt{S+c}\). The last step follows from the concavity of the square-root function which ensures that \(\sqrt{S}\leq\sqrt{S+c}-\frac{c}{2\sqrt{S+c}}\). 

Then, we can use the second part of Lemma C.7.2 to continue the bound of Proposition C.7.3 and derive a bound that only depends on the suboptimal actions.

**Proposition C.7.4**.: _If \(\nu_{t}\) is defined as in Definition 5.2, we have for any deterministic policy \(\pi:S\to A\) and any state \(s\neq s_{L}\):_

\[\mathbb{E}_{t_{i}}\left[\sum_{a\in A}\sqrt{\sum_{t=t_{i}}^{t_{i+1 }-1}\nu_{t}(s,a)}\right] \leq\mathcal{O}\left(\sum_{a\neq\pi(s)}\sqrt{\mathbb{E}_{t_{i}} \left[L^{2}\sum_{t=t_{i}}^{t_{i+1}-1}q^{P_{t},\pi_{t}}(s,a)\right]}\right)\] \[\quad+\mathcal{O}\left(\delta L^{2}|S||A|\left(t_{i+1}-t_{i} \right)+\sqrt{L|S||A|^{2}\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{\rho}}\right).\]Proof.: For each epoch \(i\) and state-action pair \((s,a)\), we have:

\[\begin{split}\mathbb{E}_{t_{i}}\left[\sqrt{\sum_{t=t_{i}}^{t_{i+1}-1 }\nu_{t}(s,a)}\right]&\leq\sqrt{\mathbb{E}_{t_{i}}\left[\sum_{t=t _{i}}^{t_{i+1}-1}\nu_{t}(s,a)\right]}\\ &=\mathcal{O}\left(\sqrt{\mathbb{E}_{t_{i}}\left[\sum_{t=t_{i}}^{t _{i+1}-1}L^{2}\,q^{\widehat{P}_{t},\pi_{t}}(s,a)(1-\pi_{t}(a|s))+L|S|C_{t}^{ \mathsf{P}}\right]}\right),\end{split} \tag{42}\]

where the first step follows from Jensen's inequality, and the second step uses Lemma C.7.2. Note that, for \(a=\pi(s)\), we have:

\[q^{\widehat{P}_{t},\pi_{t}}(s,\pi(s))(1-\pi_{t}(\pi(s)|s))\leq\ q^{\widehat{P} _{t},\pi_{t}}(s)\sum_{b\neq\pi(s)}\pi_{t}(b|s)\leq\sum_{b\neq\pi(s)}q^{\widehat {P}_{t},\pi_{t}}(s,b).\]

Therefore, we have

\[\begin{split}&\mathbb{E}_{t_{i}}\left[\sum_{a\in A}\sqrt{\sum_{t=t _{i}}^{t_{i+1}-1}\nu_{t}(s,a)}\right]\\ &=\mathcal{O}\left(\sum_{a\in A}\sqrt{\mathbb{E}_{t_{i}}\left[L^{2} \sum_{t=t_{i}}^{t_{i+1}-1}q^{\widehat{P}_{t},\pi_{t}}(s,a)(1-\pi_{t}(a|s))+L|S| C_{t}^{\mathsf{P}}\right]}\right)\\ &=\mathcal{O}\left(\sum_{a\in A}\sqrt{\mathbb{E}_{t_{i}}\left[L^{2} \sum_{t=t_{i}}^{t_{i+1}-1}q^{\widehat{P}_{t},\pi_{t}}(s,a)(1-\pi_{t}(a|s)) \right]}+\sqrt{L|S||A|^{2}\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{\mathsf{P}}}\right) \\ &=\mathcal{O}\left(\sum_{a\neq\pi(s)}\sqrt{\mathbb{E}_{t_{i}}\left[ L^{2}\sum_{t=t_{i}}^{t_{i+1}-1}q^{\widehat{P}_{t},\pi_{t}}(s,a)\right]}+\sqrt{L|S||A|^{ 2}\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{\mathsf{P}}}\right)\\ &\leq\mathcal{O}\left(\delta L^{2}|S||A|\,(t_{i+1}-1-t_{i})+\sum_ {a\neq\pi(s)}\sqrt{\mathbb{E}_{t_{i}}\left[L^{2}\sum_{t=t_{i}}^{t_{i+1}-1}q^{ P,\pi_{t}}(s,a)\right]}+\sqrt{L|S||A|^{2}\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{ \mathsf{P}}}\right)\\ &\leq\mathcal{O}\left(\delta L^{2}|S||A|\,(t_{i+1}-1-t_{i})+\sum_ {a\neq\pi(s)}\sqrt{\mathbb{E}_{t_{i}}\left[L^{2}\sum_{t=t_{i}}^{t_{i+1}-1}q^{ P,\pi_{t}}(s,a)\right]}+\sqrt{L|S||A|^{2}\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{ \mathsf{P}}}\right),\end{split}\]

where the first step follows from Eq. (42); the second step uses the fact that \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) for \(x,y\geq 0\); the fourth step follows from Lemma D.1.1 with the event \(\mathcal{E}_{\text{\rm CON}}\) and Corollary C.8.2; the last step applies Corollary D.3.6. 

#### c.7.2 Bounding EstReg\({}_{i}(\pi^{*})\) for Varying Learning Rate

We now focus on bounding EstReg\({}_{i}(\pi^{*})\) for an individual epoch \(i\).

**Notations for FTRL Analysis.** For any fixed epoch \(i\) and any integer \(t\in[t_{i},t_{i+1}-1]\), we introduce the following notations.

\[\begin{split} F_{t}(q)=\left\langle q,\sum_{\tau=t_{i}}^{t-1} \left(\widehat{\ell}_{\tau}-b_{\tau}\right)\right\rangle+\phi_{t}(q)\,\quad G_{t}(q)=\left\langle q,\sum_{\tau=t_{i}}^{t}\left(\widehat{\ell}_{ \tau}-b_{\tau}\right)\right\rangle+\phi_{t}(q),\\ q_{t}=\operatorname*{argmin}_{q\in\Omega(\widetilde{P}_{t})}F_{t }(q)\,\quad\widetilde{q}_{t}=\operatorname*{argmin}_{q\in\Omega(\widetilde{P}_{t}) }G_{t}(q).\end{split} \tag{43}\]

With these notations, we have \(q_{t}=\widehat{q}_{t}=q^{\widehat{P}_{t},\pi_{t}}=q^{\widehat{P}_{t},\pi_{t}}\). Also, according to the loss shifting technique (specifically Corollary D.4.2), \(q_{t}\) and \(\widetilde{q}_{t}\) can be equivalently written as

\[q_{t}=\operatorname*{argmin}_{q\in\Omega(\widetilde{P}_{t})}\left\langle q, \sum_{\tau=t_{i}}^{t-1}\left(g_{\tau}-b_{\tau}\right)\right\rangle+\phi_{t}(q),\quad\widetilde{q}_{t}=\operatorname*{argmin}_{q\in\Omega(\widetilde{P}_{t}) }\left\langle q,\sum_{\tau=t_{i}}^{t}\left(g_{\tau}-b_{\tau}\right)\right\rangle +\phi_{t}(q), \tag{44}\]

[MISSING_PAGE_FAIL:45]

The penalty term without expectation is bounded as:

\[\sum_{t=t_{i}}^{t_{i+1}-1}\left(G_{t}(\tilde{q}_{t})-F_{t}(q_{t})- \left\langle v,\widehat{\ell}_{t}-b_{t}\right\rangle\right)\] \[=-F_{t_{i}}(q_{t_{i}})+\sum_{t=t_{i}+1}^{t_{i+1}-1}(G_{t-1}(\tilde {q}_{t-1})-F_{t}(q_{t}))+G_{t_{i+1}-1}(\tilde{q}_{t_{i+1}-1})-\left\langle v, \sum_{t=t_{i}}^{t_{i+1}-1}\left(\widehat{\ell}_{t}-b_{t}\right)\right\rangle\] \[\leq-F_{t_{i}}(q_{t_{i}})+\sum_{t=t_{i}+1}^{t_{i+1}-1}(G_{t-1}(q_ {t})-F_{t}(q_{t}))+G_{t_{i+1}-1}(v)-\left\langle v,\sum_{t=t_{i}}^{t_{i+1}-1} \left(\widehat{\ell}_{t}-b_{t}\right)\right\rangle\] \[=-\phi_{t_{i}}(q_{t_{i}})+\sum_{t=t_{i}+1}^{t_{i+1}-1}(\phi_{t-1} (q_{t})-\phi_{t}(q_{t}))+\phi_{t_{i+1}-1}(v)\] \[=-\phi_{t_{i}}(q_{t_{i}})+\sum_{t=t_{i}+1}^{t_{i+1}-1}\sum_{s \neq s_{L}}\sum_{a\in A}\left(\gamma_{t-1}(s,a)-\gamma_{t}(s,a)\right)\log\left( \frac{1}{q_{t}(s,a)}\right)+\phi_{t_{i+1}-1}(v)\] \[=-\phi_{t_{i}}(q_{t

[MISSING_PAGE_FAIL:47]

[MISSING_PAGE_EMPTY:48]

\[\leq\mathcal{O}\left(\mathbb{E}_{t_{i}}\left[\sum_{s\neq s_{L}}\sum_{a \neq\pi(s)}\sqrt{L^{2}\log\left(\iota\right)\sum_{t=t_{i}}^{t_{i+1}-1}q^{P_{i}, \pi_{t}}(s,a)}\right]\right)\] \[\quad+\mathcal{O}\left(\mathbb{E}_{t_{i}}\left[\delta L^{2}|S|^{2 }|A|\log\left(\iota\right)(t_{i+1}-t_{i})+\sqrt{L|S|^{3}|A|^{2}\log\left(\iota \right)\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{\mathsf{P}}}\right]\right), \tag{57}\]

where the first step applies the definition of \(\gamma_{t}(s,a)\) which gives: \(\frac{\nu_{t}(s,a)}{\gamma_{t}(s,a)}=\frac{2}{D}\left(\gamma_{t+1}(s,a)-\gamma _{t}(s,a)\right)\); the second step simplifies the telescopic sum and uses \(\gamma_{t_{i}}(s,a)\geq 0\); the third step uses \(D=\frac{1}{\log(\iota)}\); the fourth step uses Proposition C.7.3; the sixth step applies Proposition C.7.4. 

#### c.7.3 Bounding EstReg

Using the equality \(\textsc{EstReg}(\pi^{\star})=\mathbb{E}\big{[}\sum_{i=1}^{N}\textsc{EstReg}_{i }(\pi^{\star})\big{]}\) where \(N\) is the number of epochs, we are now ready to complete the proof of Lemma C.3.4.

**Lemma C.7.8**.: _Over the course of \(T\) episodes, Algorithm 4 runs at most \(\mathcal{O}\left(|S||A|\log(T)\right)\) epochs._

Proof.: By definition, the algorithm resets each time a counter of the number of visits to a specific state-action pair doubles. Each state-action pair is visited at most once for each of the \(T\) rounds, so it can trigger a new epoch at most \(\log T\) times. Summing on all state-action pairs finishes the proof. 

Proof of Lemma c.3.4.: Using Lemma C.7.5 and Lemma C.7.7, we have

\[\textsc{EstReg}(\pi^{\star})\] \[=\mathbb{E}\left[\sum_{i=1}^{N}\textsc{EstReg}_{i}(\pi^{\star})\right]\] \[=\mathcal{O}\left(\mathbb{E}\left[\sum_{i=1}^{N}\left(\frac{L+LC^ {\mathsf{P}}|S|}{T}+\delta L^{2}|S|^{2}|A|\log\left(\iota\right)(t_{i+1}-t_{i}) +\sqrt{L|S|^{3}|A|^{2}\log\left(\iota\right)\sum_{t=t_{i}}^{t_{i+1}-1}C_{t}^{ \mathsf{P}}}\right)\right]\right)\] \[\quad+\mathcal{O}\left(\mathbb{E}\left[\sum_{i=1}^{N}\left(C^{ \mathsf{P}}\log(|S|T)+L\sqrt{\log\left(\iota\right)}\sum_{s\neq s_{L}}\sum_{a \neq\pi^{\star}(s)}\mathbb{E}_{t_{i}}\left[\sqrt{\sum_{t=t_{i}}^{t_{i+1}-1}q^{P _{t},\pi_{t}}(s,a)}\right]\right)\right]\right)\] \[=\mathcal{O}\left(\left(\frac{L+LC^{\mathsf{P}}|S|}{T}\right)|S ||A|\log(T)+\delta L^{2}|S|^{2}|A|\log\left(\iota\right)T+|A||S|^{2}\log(\iota )\sqrt{L|A|C^{\mathsf{P}}}\right)\] \[\quad+\mathcal{O}\left(C^{\mathsf{P}}|S||A|\log(\iota)^{2}+\sqrt{ L^{2}\log\left(\iota\right)}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}(s)}\mathbb{E} \left[\sum_{i=1}^{N}\sqrt{\sum_{t=t_{i}}^{t_{i+1}-1}q^{P_{t},\pi_{t}}(s,a)} \right]\right)\] \[=\mathcal{O}\left(\left(\frac{L|S||A|\log(T)}{T}\right)+\delta L ^{2}|S|^{2}|A|\log(\iota)T+C^{\mathsf{P}}L|S|^{2}|A|\log(\iota)+|A|^{2}|S|^{2} \log(\iota)\right)\] \[\quad+\mathcal{O}\left(C^{\mathsf{P}}|S||A|\log(\iota)^{2}+\sqrt{ L^{2}|S||A|\log^{2}\left(\iota\right)}\sum_{s\neq s_{L}}\sum_{a\neq\pi^{\star}(s)} \mathbb{E}\left[\sqrt{\sum_{t=1}^{T}q^{P_{t},\pi_{t}}(s,a)}\right]\right),\]

where the first step follows from the definition of \(\textsc{EstReg}\left(\pi^{\star}\right)\); the third step uses the fact that \(N=\mathcal{O}\left(|S||A|\log\left(T\right)\right)\) and the Cauchy-Schwarz inequality; the fourth step uses \(\sqrt{xy}\leq x+y\) for any \(x,y\geq 0\) with \(x=LC^{\mathsf{P}}\) and \(y=|A|\); the last step follows from Cauchy-Schwarz inequality again.

### Properties of Optimistic Transition

We summarize the properties guaranteed by the optimistic transition defined in Definition C.1.1.

**Lemma C.8.1**.: _For any epoch \(i\), any transition \(P^{\prime}\in\mathcal{P}_{i}\), any policy \(\pi\), and any initial state \(u\in S\), it holds that_

\[q^{\widetilde{P}_{i},\pi}(s,a|u)\leq q^{P^{\prime},\pi}(s,a|u),\quad\forall(s,a )\in S\times A.\]

Proof.: We prove this result via a forward induction from layer \(k(u)\) to layer \(L-1\).

**Base Case:** for the initial state \(u\), \(q^{\widetilde{P}_{i},\pi}(u,a|u)=q^{P^{\prime},\pi}(u,a|u)=\pi(a|u)\) for any action \(a\in A\). For the other state \(s\in S_{k(u)}\), we have \(q^{\widetilde{P}_{i},\pi}(s,a|u)=q^{P^{\prime},\pi}(s,a|u)=0\).

**Induction step:** Suppose \(q^{\widetilde{P}_{i},\pi}(s,a|u)\leq q^{P^{\prime},\pi}(s,a|u)\) holds for all the state-action pair \((s,a)\) with \(k(s)<h\). Then, for any \((s,a)\in S_{h}\times A\), we have

\[q^{\widetilde{P}_{i},\pi}(s,a|u) =\pi(a|s)\cdot\sum_{s^{\prime}\in S_{h-1}}\sum_{a^{\prime}\in A}q^ {\widetilde{P}_{i},\pi}(s^{\prime},a^{\prime}|u)\widetilde{P}_{i}(s|s^{\prime },a^{\prime})\] \[\leq\pi(a|s)\cdot\sum_{s^{\prime}\in S_{h-1}}\sum_{a^{\prime}\in A }q^{P^{\prime},\pi}(s^{\prime},a^{\prime}|u)P^{\prime}(s|s^{\prime},a^{\prime})\] \[=q^{P^{\prime},\pi}(s,a|u),\]

where the second step follows from the induction hypothesis and the definition of optimistic transition in Definition C.1.1. 

**Corollary C.8.2**.: _Conditioning on the event \(\mathcal{E}_{\mathrm{CON}}\), it holds for any epoch \(i\) and any policy \(\pi\) that_

\[q^{\widetilde{P}_{i},\pi}(s,a)\leq q^{P,\pi}(s,a),\quad\forall(s,a)\in S \times A.\]

**Lemma C.8.3**.: _(Optimism of Optimistic Transition) Suppose the high-probability event \(\mathcal{E}_{\mathrm{CON}}\) holds. Then for any policy \(\pi\), any \((s,a)\in S\times A\), and any valid loss function \(\ell:S\times A\rightarrow\mathbb{R}_{\geq 0}\), it holds that_

\[Q^{\widetilde{P}_{i},\pi}\left(s,a;\ell\right)\leq Q^{P,\pi}\left(s,a;\ell \right),\text{ and }V^{\widetilde{P}_{i},\pi}\left(s;\ell\right)\leq V^{P, \pi}\left(s;\ell\right),\forall(s,a)\in S\times A.\]

Proof.: According to Corollary C.8.2, we have for all epoch \(i\) that

\[q^{\widetilde{P}_{i},\pi}(s,a|u)\leq q^{P,\pi}(s,a|u),\quad\forall u\in S \quad\forall(s,a)\in S\times A. \tag{58}\]

Therefore, we have

\[V^{\widetilde{P}_{i},\pi}\left(s;\ell\right) =\sum_{u\in S}\sum_{v\in A}q^{\widetilde{P}_{i},\pi}(u,v|s)\ell( u,v)\] \[\leq\sum_{u\in S}\sum_{v\in A}q^{P,\pi}(u,v|s)\ell(u,v)\] \[=V^{P,\pi}\left(s;\ell\right),\]

where the second step follows from Eq. (58). The statement for the \(Q\)-function can be proven in the same way. 

Next, we argue that our optimistic transition provides a tighter performance estimation compared to the approach of Jin et al. (2021). Specifically, Jin et al. (2021) proposes to subtract the following exploration bonuses \(\textsc{Bonus}_{i}:S\times A\rightarrow\mathbb{R}\) from the loss functions

\[\textsc{Bonus}_{i}(s,a)=L\cdot\min\left\{1,\sum_{s^{\prime}\in S_{k(s)+1}}B_{i }(s,a,s^{\prime})\right\},\]

where \(B_{i}(s,a,s^{\prime})\) is the confidence bound defined in Eq. (4). This makes sure \(Q^{\widetilde{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)\) is no larger than the true \(Q\)-function \(Q^{P,\pi}(s,a;\ell)\) as well, but is a looser lower bound as shown below.

**Lemma C.8.4**.: _(Tighter Performance Estimation) For any policy \(\pi\), any \((s,a)\in S\times A\), and any bounded loss function \(\ell:S\times A\rightarrow[0,1]\), it holds that_

\[Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)\leq Q^{\bar{P}_{i},\pi}\left(s,a;\ell\right),\forall(s,a)\in S\times A.\]

Proof.: We prove this result via a backward induction from layer \(L\) to layer \(0\).

**Base Case:** for the terminal state \(s_{L}\), we have \(Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)=Q^{\bar{P}_{i}, \pi}\left(s,a;\ell\right)=0\).

**Induction step:** suppose the induction hypothesis holds for all the state-action pairs \((s,a)\in S\times A\) with \(k(s)>h\). For any state-action pair \((s,a)\in S_{h}\times A\), we first have

\[Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right) =\ell(s,a)-\textsc{Bonus}_{i}(s,a)+\sum_{u\in S_{h+1}}\bar{P}_{i} (u|s,a)V^{\bar{P}_{i},\pi}\left(u;\ell-\textsc{Bonus}_{i}\right)\] \[\leq\ell(s,a)-\textsc{Bonus}_{i}(s,a)+\sum_{u\in S_{h+1}}\bar{P} _{i}(u|s,a)V^{\bar{P}_{i},\pi}\left(u;\ell\right).\]

Clearly, when \(\sum_{u\in S_{k(s)+1}}B_{i}(s,a,u)\geq 1\), we have \(Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)\leq 0\) by the definition of \(\textsc{Bonus}_{i}\), which directly implies that \(Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)\leq Q^{\bar{P}_{i },\pi}\left(s,a;\ell\right)\). So we continue the bound under the condition \(\sum_{u\in S_{k(s)+1}}B_{i}(s,a,u)<1\):

\[Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right) \leq\ell(s,a)-\textsc{Bonus}_{i}(s,a)+\sum_{u\in S_{h+1}}\bar{P} _{i}(u|s,a)V^{\bar{P}_{i},\pi}\left(u;\ell\right)\] \[\leq\ell(s,a)+\sum_{u\in S_{h+1}}\left(\bar{P}_{i}(u|s,a)-B_{i}( s,a,u)\right)V^{\bar{P}_{i},\pi}\left(u;\ell\right)\] \[\leq\ell(s,a)+\sum_{u\in S_{h+1}}\widetilde{P}_{i}(u|s,a)V^{\bar {P}_{i},\pi}\left(u;\ell\right)\] \[=Q^{\bar{P}_{i},\pi}\left(s,a;\ell\right),\]

where the second step follows from the fact that \(V^{\bar{P}_{i},\pi}\left(u;\ell\right)\leq L\); the third step follows from the definition of optimistic transition \(\widetilde{P}_{i}\).

Combining these two cases proves that \(Q^{\bar{P}_{i},\pi}\left(s,a;\ell-\textsc{Bonus}_{i}\right)\leq Q^{\bar{P}_{i },\pi}\left(s,a;\ell\right)\) for any \((s,a)\in S_{h}\times A\), finishing the induction.

Supplementary Lemmas

### Expectation

**Lemma D.1.1**.: _([Jin et al., 2021, Lemma D.3.6]) Suppose that a random variable \(X\) satisfies the following conditions:_

* \(X<R\) _where_ \(R\) _is a constant._
* \(X<Y\) _conditioning on event_ \(A\)_, where_ \(Y\geq 0\) _is a random variable._

_Then, it holds that \(\mathbb{E}\left[X\right]\leq\mathbb{E}\left[Y\right]+\Pr\left[A^{c}\right]\cdot R\) where \(A^{c}\) is the complementary event of \(A\)._

### Confidence Bound with Known Corruption

In this subsection, we show that the empirical transition is centered around the transition \(P\). Let \([T]:=\{1,\cdots,T\}\). Recall that \(E_{i}:=\{t\in[T]:\text{ episode }t\text{ belongs to epoch }i\}\), \(\iota=\frac{|S||A|T}{\delta}\), and we define the following quantities:

\[T_{i}(s,a)=\left\{t\in\cup_{j=1}^{i-1}E_{j}:\exists k\text{ such that }(s_{t,k},a_{t,k})=(s,a)\right\} \forall(s,a)\in S_{k}\times A,\forall i\geq 2,\forall k<L;\] \[C_{t}^{\mathsf{P}}(s,a,s^{\prime})=|P(s^{\prime}|s,a)-P_{t}(s^{ \prime}|s,a)|, \forall(s,a,s^{\prime})\in W_{k},\forall i\in[T],\forall k<L;\] \[C_{i}^{\mathsf{P}}(s,a,s^{\prime})=\sum_{t\in T_{i}(s,a)}C_{t}^{ \mathsf{P}}(s,a,s^{\prime}), \forall(s,a,s^{\prime})\in W_{k},\forall i\in[T],\forall k<L.\]

Note that based on definition of \(T_{i}(s,a)\), we have \(m_{i}(s,a)=|T_{i}(s,a)|\). Then, we present the following lemma which shows the concentration bound between \(P(s^{\prime}|s,a)\) and \(\bar{P}_{i}(s^{\prime}|s,a)\).

**Lemma D.2.1**.: _(Detailed restatement of Lemma 2.2) Event \(\mathcal{E}_{\text{\rm CON}}\) occurs with probability at least \(1-\delta\) where,_

\[\mathcal{E}_{\text{\rm CON}}:=\left\{\forall(s,a,s^{\prime})\in W_{k},\forall i \in[T],\forall k<L:\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right| \leq B_{i}(s,a,s^{\prime})\right\}, \tag{59}\]

_and \(B_{i}(s,a,s^{\prime})\) is defined in Eq. (4) as:_

\[B_{i}(s,a,s^{\prime})=\min\left\{1,16\sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a) \log\left(\iota\right)}{m_{i}(s,a)}}+64\frac{\left(C^{\mathsf{P}}+\log\left( \iota\right)\right)}{m_{i}(s,a)}\right\}. \tag{60}\]

Proving this lemma requires several auxiliary results stated below. For any episode \(t\in[T]\) and any layer \(k<L\), we use \(s_{t,k}^{\text{IMG}}\) to denote an imaginary random state sampled from \(P(\cdot|s_{t,k},a_{t,k})\). For any \((s,a,s^{\prime})\in W_{k}\), let

\[\bar{P}_{i}^{\text{IMG}}(s^{\prime}|s,a)=\frac{1}{m_{i}(s,a)}\sum_{t\in T_{i} (s,a)}\mathbb{I}\{s_{t,k(s)+1}^{\text{IMG}}=s^{\prime}\}.\]

We now proceed with a couple lemmas.

**Lemma D.2.2** (Lemma 2, [Jin et al., 2020]).: _Event \(\mathcal{E}^{1}\) occurs with probability at least \(1-3\delta/4\) where_

\[\mathcal{E}^{1}:=\left\{\forall(s,a,s^{\prime})\in W_{k},\forall i,k:\left|P( s^{\prime}|s,a)-\bar{P}_{i}^{\text{IMG}}(s^{\prime}|s,a)\right|\leq\bar{\omega}_{i} (s,a,s^{\prime})\right\},\]

_and \(\bar{\omega}_{i}(s,a,s^{\prime})\) for any \((s,a,s^{\prime})\in W_{k}\) and \(0\leq k\leq L-1\) is defined as_

\[\bar{\omega}_{i}(s,a,s^{\prime})=\min\left\{1,2\sqrt{\frac{\bar{P}_{i}(s^{ \prime}|s,a)\log\iota}{m_{i}(s,a)}}+\frac{14\log\iota}{3m_{i}(s,a)}\right\}. \tag{61}\]

We note that as long as \(|A|T\geq\nicefrac{{1

**Lemma D.2.3**.: _Event \(\mathcal{E}^{2}\) occurs with probability at least \(1-\delta/4\) where_

\[\mathcal{E}^{2}:=\left\{\forall(s,a,s^{\prime})\in W_{k},\forall i,k:\left|\hat{P }^{\text{\rm IMG}}_{i}(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right|\leq \omega_{i}(s,a,s^{\prime})\right\},\]

_and \(\omega_{i}(s,a,s^{\prime})\) for any \((s,a,s^{\prime})\in W_{k}\) and \(0\leq k\leq L-1\) is defined as_

\[\omega_{i}(s,a,s^{\prime})=\min\left\{1,\frac{4C_{i}^{\mathsf{P}}(s,a,s^{ \prime})}{m_{i}(s,a)}+\sqrt{\frac{24P(s^{\prime}|s,a)\log\iota}{m_{i}(s,a)}}+ \frac{6\log\iota}{m_{i}(s,a)}\right\}. \tag{62}\]

Proof.: For any fixed \((s,a,s^{\prime})\), if \(m_{i}(s,a)=0\), the claimed bound in \(\mathcal{E}^{2}\) holds trivially, so we consider the case \(m_{i}(s,a)\neq 0\) below. By definition we have:

\[\bar{P}^{\text{\rm IMG}}_{i}(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)=\frac {1}{m_{i}(s,a)}\sum_{t\in T_{i}(s,a)}\left(\mathbb{I}\{s^{\text{\rm IMG}}_{t,k (s)+1}=s^{\prime}\}-\mathbb{I}\{s_{t,k(s)+1}=s^{\prime}\}\right).\]

Then, we construct the martingale difference sequence \(\{X_{t}(s,a,s^{\prime})\}_{t=1}^{\infty}\) w.r.t. filtration \(\{\mathcal{F}_{t,k(s)}\}_{t=1}^{\infty}\) (see (Lykouris et al., 2019, Definition 4.9) for the formal definition of these filtrations) where

\[X_{t}(s,a,s^{\prime})=\mathbb{I}\{s^{\text{\rm IMG}}_{t,k(s)+1}=s^{\prime}\}- \mathbb{I}\{s_{t,k(s)+1}=s^{\prime}\}-\left(P\left(s^{\prime}|s,a\right)-P_{t} \left(s^{\prime}|s,a\right)\right).\]

With the definition of \(X_{t}(s,a,s^{\prime})\), one can show

\[\sum_{t\in T_{i}(s,a)}\mathbb{E}\left[X_{t}(s,a,s^{\prime})^{2}| \mathcal{F}_{t,k(s)}\right]\] \[\leq\sum_{t\in T_{i}(s,a)}\mathbb{E}\left[\left(\mathbb{I}\{s^{ \text{\rm IMG}}_{t,k(s)+1}=s^{\prime}\}-\mathbb{I}\{s_{t,k(s)+1}=s^{\prime}\} -\left(P\left(s^{\prime}|s,a\right)-P_{t}\left(s^{\prime}|s,a\right)\right) \right)^{2}\mid\mathcal{F}_{t,k(s)}\right]\] \[\leq\sum_{t\in T_{i}(s,a)}\mathbb{E}\left[2\left(\mathbb{I}\{s^{ \text{\rm IMG}}_{t,k(s)+1}=s^{\prime}\}-\mathbb{I}\{s_{t,k(s)+1}=s^{\prime}\} \right)^{2}\!\!+2\left(P\left(s^{\prime}|s,a\right)-P_{t}\left(s^{\prime}|s,a \right)\right)^{2}\mid\mathcal{F}_{t,k(s)}\right]\] \[\leq\sum_{t\in T_{i}(s,a)}\mathbb{E}\left[2\mathbb{I}\{s^{\text{ \rm IMG}}_{t,k(s)+1}=s^{\prime}\}+2\mathbb{I}\{s_{t,k(s)+1}=s^{\prime}\}+2C_{t }^{\mathsf{P}}(s,a,s^{\prime})\mid\mathcal{F}_{t,k(s)}\right]\] \[=2\sum_{t\in T_{i}(s,a)}\left(P(s^{\prime}|s,a)+P_{t}(s^{\prime}| s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})\right), \tag{63}\]

where the second step uses \((x-y)^{2}\leq 2(x^{2}+y^{2})\) for any \(x,y\in\mathbb{R}\); the third step uses \((x-y)^{2}\leq x^{2}+y^{2}\) for \(x,y\in\mathbb{R}_{\geq 0}\) and the fact that \(C_{t}^{\mathsf{P}}(s,a,s^{\prime})\in[0,1]\), thereby \(C_{t}^{\mathsf{P}}(s,a,s^{\prime})^{2}\leq C_{t}^{\mathsf{P}}(s,a,s^{\prime})\); the last step holds based on the definitions of \(T_{i}(s,a)\), \(s^{\text{\rm IMG}}_{t,k(s)+1}\), and \(s_{t,k(s)+1}\) as well as the fact that \(C_{t}^{\mathsf{P}}(s,a,s^{\prime})\) is \(\mathcal{F}_{t,k(s)}\)-measurable.

By using the result in Eq. (63), we bound the average second moment \(\sigma^{2}\) as

\[\sigma^{2}=\frac{\sum_{t\in T_{i}(s,a)}\mathbb{E}\left[X_{t}^{2}|\mathcal{F}_{t,k(s)}\right]}{m_{i}(s,a)}\leq\frac{2\sum_{t\in T_{i}(s,a)}\left(P(s^{\prime }|s,a)+P_{t}(s^{\prime}|s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})\right)}{m_{i} (s,a)}. \tag{64}\]

By applying Lemma D.2.4 with \(b=2\) and the upper bound of \(\sigma^{2}\) shown in Eq. (64), as well as using the fact that \(m_{i}(s,a)=|T_{i}(s,a)|\), for any \((s,a,s^{\prime})\), we have the following with probability at least \(1-\delta/(4T|S|^{2}|A|)\),

\[|\bar{P}^{\text{\rm IMG}}_{i}(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime }|s,a)|\] \[\leq\left|P(s^{\prime}|s,a)-\frac{\sum_{t\in T_{i}(s,a)}P_{t}(s^ {\prime}|s,a)}{m_{i}(s,a)}\right|+\frac{4\log\left(\frac{8m_{i}(s,a)T|S|^{2}| A|}{\delta}\right)}{3m_{i}(s,a)}\right.\] \[\quad+\sqrt{\frac{4\log\left(\frac{16m_{i}(s,a)^{2}T|S|^{2}|A|}{ \delta}\right)\sum_{t\in T_{i}(s,a)}\left(P(s^{\prime}|s,a)+P_{t}(s^{\prime}| s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})\right)}{m_{i}^{2}(s,a)}}\]\[\leq\left|P(s^{\prime}|s,a)-\frac{\sum_{t\in T_{i}(s,a)}P_{t}(s^{ \prime}|s,a)}{m_{i}(s,a)}\right|+\frac{8\log\iota}{3m_{i}(s,a)}\] \[\quad+\sqrt{\frac{12\log\iota\sum_{t\in T_{i}(s,a)}\left(P(s^{ \prime}|s,a)+P_{t}(s^{\prime}|s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})\right)}{m _{i}^{2}(s,a)}}\] \[\leq\left|\frac{\sum_{t\in T_{i}(s,a)}P(s^{\prime}|s,a)}{m_{i}(s, a)}-\frac{\sum_{t\in T_{i}(s,a)}P_{t}(s^{\prime}|s,a)}{m_{i}(s,a)}\right|+\frac{8 \log\iota}{3m_{i}(s,a)}\] \[\quad+\sqrt{\frac{12\log\iota\sum_{t\in T_{i}(s,a)}\left(P(s^{ \prime}|s,a)+P(s^{\prime}|s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})+C_{t}^{ \mathsf{P}}(s,a,s^{\prime})\right)}{m_{i}^{2}(s,a)}}\] \[\leq\frac{C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}+\sqrt{ \frac{24P(s^{\prime}|s,a)\log\iota}{m_{i}(s,a)}}+\frac{8\log\iota}{3m_{i}(s,a) }+\frac{\sqrt{24C_{i}^{\mathsf{P}}(s,a,s^{\prime})\log\iota}}{m_{i}(s,a)}\] \[\leq\frac{C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}+\sqrt{ \frac{24P(s^{\prime}|s,a)\log\iota}{m_{i}(s,a)}}+\frac{8\log\iota}{3m_{i}(s,a)} +\frac{\sqrt{6}\left(C_{i}^{\mathsf{P}}(s,a,s^{\prime})+\log\iota\right)}{m_{i }(s,a)}\] \[\leq\omega_{i}(s,a,s^{\prime}),\]

where the first inequality applies Lemma D.2.4; the second inequality bounds all logarithmic terms by \(\log\iota\) with an appropriate constant factor (using \(m_{i}(s,a)\leq T\) and \(|A|\geq 2\)); the third step follows the fact that \(P_{t}(s^{\prime}|s,a)\leq P(s^{\prime}|s,a)+C_{t}^{\mathsf{P}}(s,a,s^{\prime})\); the fourth step uses the definition \(C_{i}^{\mathsf{P}}(s,a,s^{\prime})=\sum_{t\in T_{i}(s,a)}C_{t}^{\mathsf{P}}(s,a,s^{\prime})\); the fifth step applies the inequality \(\sqrt{4xy}\leq x+y\), \(\forall x,y\geq 0\) for \(x=C_{i}^{\mathsf{P}}(s,a,s^{\prime})\) and \(y=\log\iota\). Applying a union bound over all \((s,a,s^{\prime})\) and epochs \(i\leq T\), we complete the proof. 

**Lemma D.2.4** (Anytime Version of Azuma-Bernstein).: _Let \(\{X_{i}\}_{i=1}^{\infty}\) be \(b\)-bounded martingale difference sequence with respect to \(\mathcal{F}_{i}\). Let \(\sigma^{2}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}[X_{i}^{2}|F_{i-1}]\). Then, with probability at least \(1-\delta\), for any \(N\in\mathbb{N}^{+}\), it holds that:_

\[\left|\frac{1}{N}\sum_{i=1}^{N}X_{i}\right|\leq\sqrt{\frac{2\sigma^{2}\log \left(4N^{2}/\delta\right)}{N}}+\frac{2b\log(2N/\delta)}{3N}.\]

Proof.: This follows the same argument as Lemma G.2 in [10]. 

**Lemma D.2.5**.: _Event \(\mathcal{E}\) occurs with probability at least \(1-\delta\), where_

\[\mathcal{E}:=\left\{\forall(s,a,s^{\prime})\in W_{k},\forall i,k:\left|P(s^{ \prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right|\leq\omega_{i}(s,a,s^{\prime}) +\bar{\omega}_{i}(s,a,s^{\prime})\right\}.\]

Proof.: Conditioning on events \(\mathcal{E}^{1}\) and \(\mathcal{E}^{2}\), we have

\[\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right| \leq\left|P(s^{\prime}|s,a)-\bar{P}_{i}^{\text{IMG}}(s^{\prime}| s,a)\right|+\left|\bar{P}_{i}^{\text{IMG}}(s^{\prime}|s,a)-\bar{P}_{i}(s^{ \prime}|s,a)\right|\] \[\leq\bar{\omega}_{i}(s,a,s^{\prime})+\omega_{i}(s,a,s^{\prime}). \tag{65}\]

Using a union bound for \(\mathcal{E}^{1}\) and \(\mathcal{E}^{2}\), we complete the proof. 

Armed with above results, we are now ready to prove Lemma D.2.1.

Proof of Lemma d.2.1.: Conditioning on event \(\mathcal{E}\), for any fixed \((s,a,s^{\prime})\) with \(m_{i}(s,a)\neq 0\) (otherwise the desired bound holds trivially), we have

\[\omega_{i}(s,a,s^{\prime})\] \[\leq\sqrt{\frac{24P(s^{\prime}|s,a)\log\iota}{m_{i}(s,a)}}+\frac{ 6\log\iota}{m_{i}(s,a)}+\frac{4C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}\] \[\leq\sqrt{\frac{24\left(\bar{P}_{i}(s^{\prime}|s,a)+\bar{\omega} _{i}(s,a,s^{\prime})+\frac{4C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}+ \omega_{i}(s,a,s^{\prime})\right)\log\iota}{m_{i}(s,a)}}+\frac{6\log\iota}{m_{i }(s,a)}+\frac{4C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}\]\[\leq\sqrt{\frac{24\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s,a)}}+ \sqrt{\frac{24\bar{\omega}_{i}(s,a,s^{\prime})\log\iota}{m_{i}(s,a)}}\] \[\quad+\sqrt{\frac{96\frac{C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i }(s,a)}\log\iota}{m_{i}(s,a)}}+\sqrt{\frac{24\bar{\omega}_{i}(s,a,s^{\prime}) \log\iota}{m_{i}(s,a)}}+\frac{6\log\iota}{m_{i}(s,a)}+\frac{4C_{i}^{\mathsf{P} }(s,a,s^{\prime})}{m_{i}(s,a)}\] \[\leq\sqrt{\frac{24\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s,a) }}+\bar{\omega}_{i}(s,a,s^{\prime})+\frac{\sqrt{96C_{i}^{\mathsf{P}}(s,a,s^{ \prime})\log\iota}}{m_{i}(s,a)}+\frac{\omega_{i}(s,a,s^{\prime})}{2}+\frac{24 \log\iota+4C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}\] \[\leq\sqrt{\frac{24\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s,a )}}+\bar{\omega}_{i}(s,a,s^{\prime})+\frac{28C_{i}^{\mathsf{P}}(s,a,s^{\prime} )}{m_{i}(s,a)}+\frac{\omega_{i}(s,a,s^{\prime})}{2}+\frac{25\log\iota}{m_{i}(s,a)},\]

where the second step holds under \(\mathcal{E}\); the third step uses \(\sqrt{\sum_{i=1}^{n}x_{i}}\leq\sum_{i=1}^{n}\sqrt{x_{i}}\) for all \(x_{i}\in\mathbb{R}_{\geq 0}\); the fourth step and the fifth step use \(2\sqrt{xy}\leq x+y\) for \(x,y\geq 0\). Rearranging the above, we obtain

\[\omega_{i}(s,a,s^{\prime})\leq\sqrt{\frac{96\bar{P}_{i}(s^{\prime}|s,a)\log \iota}{m_{i}(s,a)}}+2\bar{\omega}_{i}(s,a,s^{\prime})+\frac{56C_{i}^{\mathsf{ P}}(s,a,s^{\prime})}{m_{i}(s,a)}+\frac{50\log\iota}{m_{i}(s,a)} \tag{66}\]

Thus, conditioning on event \(\mathcal{E}\), one can show for all \((s,a,s^{\prime})\in W_{k}\) and \(k<L-1\)

\[\big{|}P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\big{|} \leq\omega_{i}(s,a,s^{\prime})+\bar{\omega}_{i}(s,a,s^{\prime})\] \[\leq\sqrt{\frac{96\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s,a )}}+3\bar{\omega}_{i}(s,a,s^{\prime})+\frac{56C_{i}^{\mathsf{P}}(s,a,s^{\prime })}{m_{i}(s,a)}+\frac{50\log\iota}{m_{i}(s,a)}\] \[\leq 16\sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s, a)}}+\frac{56C_{i}^{\mathsf{P}}(s,a,s^{\prime})}{m_{i}(s,a)}+\frac{64\log \iota}{m_{i}(s,a)}\] \[\leq 16\sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a)\log\iota}{m_{i}(s, a)}}+64\frac{C_{i}^{\mathsf{P}}(s,a,s^{\prime})+\log\iota}{m_{i}(s,a)}, \tag{67}\]

where the second step uses Eq. (66), the third step applies the definition of \(\bar{\omega}_{i}(s,a,s^{\prime})\). Finally, using the fact \(C_{i}^{\mathsf{P}}(s,a,s^{\prime})\leq C^{\mathsf{P}}\), we complete the proof. 

Then, we present an immediate corollary of Lemma D.2.1.

**Corollary D.2.6**.: _Consider any epoch \(i\) and any transition \(P^{\prime}\in\mathcal{P}_{i}\). The following holds (recall \(\widehat{m}_{i}\) defined at the beginning of the appendix),_

\[\|P^{\prime}(\cdot|s,a)-\bar{P}_{i}(\cdot|s,a)\|_{1}\leq 2\cdot\min\left\{1, \frac{32C^{\mathsf{P}}}{\widehat{m}_{i}(s,a)}+8\sqrt{\frac{|S_{k(s)+1}|\log \iota}{\widehat{m}_{i}(s,a)}}+\frac{32|S_{k(s)+1}|\log\iota}{\widehat{m}_{i}( s,a)}\right\}.\]

Proof.: As \(P^{\prime}\in\mathcal{P}_{i}\), we start from Eq. (67):

\[\|P^{\prime}(\cdot|s,a)-\bar{P}_{i}(\cdot|s,a)\|_{1} \leq\sum_{s^{\prime}\in k(s)+1}\left(\frac{64C_{i}^{\mathsf{P}}( s,a,s^{\prime})}{m_{i}(s,a)}+16\sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a)\log \iota}{m_{i}(s,a)}}+\frac{64\log\iota}{m_{i}(s,a)}\right)\] \[\leq\frac{64C^{\mathsf{P}}}{m_{i}(s,a)}+16\sqrt{\frac{|S_{k(s)+1 }|\log\iota}{m_{i}(s,a)}}+\frac{64|S_{k(s)+1}|\log\iota}{m_{i}(s,a)}, \tag{68}\]

where the last step uses the Cauchy-Schwarz inequality and the fact that \(\sum_{s^{\prime}\in k(s)+1}C_{i}^{\mathsf{P}}(s,a,s^{\prime})\leq C^{\mathsf{P}}\). Since \(\|P(\cdot|s,a)-\bar{P}_{i}(\cdot|s,a)\|_{1}\leq 2\), we combine this trivial bound and the bound of \(\|P(\cdot|s,a)-\bar{P}_{i}(\cdot|s,a)\|_{1}\) in Eq. (68) to arrive at

\[\|P^{\prime}(\cdot|s,a)-\bar{P}_{i}(\cdot|s,a)\|_{1}\leq 2\cdot\min\left\{1, \frac{32C^{\mathsf{P}}}{m_{i}(s,a)}+8\sqrt{\frac{|S_{k(s)+1}|\log\iota}{m_{i}( s,a)}}+\frac{32|S_{k(s)+1}|\log\iota}{m_{i}(s,a)}\right\}\]\[=2\cdot\min\left\{1,\frac{32C^{\mathsf{P}}}{\widehat{m}_{i}(s,a)}+8\sqrt{\frac{|S_{ k(s)+1}|\log\iota}{\widehat{m}_{i}(s,a)}}+\frac{32|S_{k(s)+1}|\log\iota}{ \widehat{m}_{i}(s,a)}\right\},\]

finishing the proof. 

We conclude this subsection with two other useful lemmas.

**Lemma D.2.7**.: _Conditioning on event \(\mathcal{E}_{\mathrm{con}}\), it holds for all tuple \((s,a,s^{\prime})\) and epoch \(i\) that_

\[\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right|\leq\mathcal{O} \left(\min\left\{1,\sqrt{\frac{P(s^{\prime}|s,a)\log(\iota)}{\widehat{m}_{i}(s, a)}}+\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i}(s,a)} \right\}\right).\]

Proof.: Fix the epoch \(i\) and tuple \((s,a,s^{\prime})\). According to the definitions of \(\mathcal{E}_{\mathrm{con}}\) in Eq.59 and \(\widehat{m}\), we have

\[\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right|\leq\min\left\{1,16 \sqrt{\frac{\bar{P}_{i}(s^{\prime}|s,a)\log(\iota)}{\widehat{m}_{i}(s,a)}}+64 \cdot\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i}(s,a)}\right\}.\]

Therefore, by direct calculation, we have

\[\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right|\] \[\leq 16\sqrt{\frac{\left(\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{ \prime}|s,a)\right|+P(s^{\prime}|s,a)\right)\log\left(\iota\right)}{\widehat{m} _{i}(s,a)}}+64\cdot\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i }(s,a)}\] \[\leq 8\left(\sqrt{\frac{P(s^{\prime}|s,a)\log\left(\iota\right)}{ \widehat{m}_{i}(s,a)}}+\sqrt{\frac{\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{ \prime}|s,a)\right|\log\left(\iota\right)}{\widehat{m}_{i}(s,a)}}\right)+64 \cdot\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i}(s,a)}\] \[=8\sqrt{\frac{P(s^{\prime}|s,a)\log\left(\iota\right)}{\widehat{m }_{i}(s,a)}}+64\cdot\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{ i}(s,a)}+\sqrt{\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a)\right| \cdot\frac{64\log\left(\iota\right)}{\widehat{m}_{i}(s,a)}}\] \[\leq 8\sqrt{\frac{P(s^{\prime}|s,a)\log\left(\iota\right)}{\widehat {m}_{i}(s,a)}}+96\cdot\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m} _{i}(s,a)}+\frac{1}{2}\left|P(s^{\prime}|s,a)-\bar{P}_{i}(s^{\prime}|s,a) \right|,\]

where the second step and last step follow from the fact that \(\sqrt{xy}\leq\frac{1}{2}(x+y)\) for any \(x,y\geq 0\). Finally, rearranging the above inequality finishes the proof. 

**Lemma D.2.8**.: _(Lower Bound of Upper Occupancy Measure) For any episode \(t\) and state \(s\neq s_{L}\), it always holds that \(u_{t}(s)\geq\nicefrac{{1}}{{|S|T}}\)._

Proof.: Fix the episode \(t\) and state \(s\). We prove the lemma by constructing a specific transition \(\widehat{P}\in\mathcal{P}_{i(t)}\), such that \(q^{\widehat{P},\pi}(s)\geq\nicefrac{{1}}{{|S|T}}\) for any policy \(\pi\), which suffices due to the definition of \(u_{t}(s)\). Specifically, \(\widehat{P}\) is defined as, for any tuple \((s,a,s^{\prime})\in W_{k}\) and \(k=0,\ldots,L-1\):

\[\widehat{P}(s^{\prime}|s,a)=\bar{P}_{i(t)}(s^{\prime}|s,a)\cdot\left(1-\frac{1 }{T}\right)+\frac{1}{|S_{k+1}|T}.\]

By direct calculation, one can verify that \(\widehat{P}\) is a valid transition function. Then, we show that \(\widehat{P}\in\mathcal{P}_{i(t)}\) by verifying the condition for any transition tuple \((s,a,s^{\prime})\in W_{k}\) and \(k=0,\ldots,L-1\):

\[\left|\widehat{P}(s^{\prime}|s,a)-\bar{P}_{i(t)}(s^{\prime}|s,a)\right|=\frac {1}{T}\cdot\left|\bar{P}_{i(t)}(s^{\prime}|s,a)-\frac{1}{|S_{k+1}|}\right|\leq \frac{1}{T}\leq B_{i(t)}(s,a,s^{\prime}),\]

where the last step follows from the definition of confidence intervals in Eq.4.

Finally, we show that \(q^{\widehat{P},\pi}(s)\geq\nicefrac{{1}}{{|S|T}}\) as:

\[q^{\widehat{P},\pi}(s)=\sum_{u\in S_{k(s)-1}}\sum_{v\in A}q^{\widehat{P},\pi}(u,v)\widehat{P}(s|u,v)\geq\sum_{u\in S_{k(s)-1}}\sum_{v\in A}q^{\widehat{P},\pi }(u,v)\cdot\frac{1}{T|S|}=\frac{1}{T|S|},\]

which concludes the proof.

### Difference Lemma

**Lemma D.3.1** (Theorem 5.2.1 of (Kakade, 2003)).: _(Performance Difference Lemma) For any policies \(\pi_{1},\pi_{2}\) and any loss function \(\ell:S\times A\to\mathbb{R}\),_

\[V^{P,\pi_{1}}(s_{0};\ell)-V^{P,\pi_{2}}(s_{0};\ell)\] \[=\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{2}}(s,a)\left(V^{P,\pi_ {1}}(s;\ell)-Q^{P,\pi_{1}}(s,a;\ell)\right)\] \[=\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{2}}(s)\left(\pi_{1}(a|s) -\pi_{2}(a|s)\right)Q^{P,\pi_{1}}(s,a;\ell).\]

In fact, the same also holds for our optimistic transition where the layer structure is violated. For completeness, we include a proof below.

**Lemma D.3.2**.: _For any policies \(\pi_{1},\pi_{2}\), any loss function \(\ell:S\times A\to\mathbb{R}\), and any transition \(\widetilde{P}\) where \(\sum_{s^{\prime}\in S_{k(s)}+1}\widetilde{P}(s^{\prime}|s,a)\leq 1\) for all state-action pairs \((s,a)\) (the remaining probability is assigned to \(s_{L}\)), we have_

\[V^{\widetilde{P},\pi_{1}}(s_{0};\ell)-V^{\widetilde{P},\pi_{2}}(s_{0};\ell)= \sum_{s\neq s_{L}}\sum_{a\in A}q^{\widetilde{P},\pi_{2}}(s,a)\left(V^{ \widetilde{P},\pi_{1}}(s;\ell)-Q^{\widetilde{P},\pi_{1}}(s,a;\ell)\right).\]

Proof.: By direct calculation, we have for any state \(s\neq s_{L}\):

\[V^{\widetilde{P},\pi_{1}}(s_{0};\ell)-V^{\widetilde{P},\pi_{2}}( s_{0};\ell) =\left(\sum_{a\in A}\pi_{2}(a|s_{0})\right)V^{\widetilde{P},\pi_{ 1}}(s_{0};\ell)-\sum_{a\in A}\pi_{2}(a|s_{0})Q^{\widetilde{P},\pi_{1}}(s_{0},a;\ell)\] \[\quad+\sum_{a\in A}\pi_{2}(a|s_{0})\left(Q^{\widetilde{P},\pi_{1} }(s_{0},a;\ell)-Q^{\widetilde{P},\pi_{2}}(s_{0},a;\ell)\right)\] \[=\sum_{a\in A}q^{\widetilde{P},\pi_{2}}(s_{0},a)\left(V^{ \widetilde{P},\pi_{1}}(s_{0};\ell)-Q^{\widetilde{P},\pi_{1}}(s_{0},a;\ell)\right)\] \[\quad+\sum_{a\in A}\sum_{s^{\prime}\in S_{1}}\pi_{2}(a|s_{0}) \widetilde{P}(s^{\prime}|s_{0},a)\left(V^{\widetilde{P},\pi_{1}}(s^{\prime}; \ell)-V^{\widetilde{P},\pi_{2}}(s^{\prime};\ell)\right)\] \[=\sum_{a\in A}q^{\widetilde{P},\pi_{2}}(s_{0},a)\left(V^{ \widetilde{P},\pi_{1}}(s_{0};\ell)-Q^{\widetilde{P},\pi_{1}}(s_{0},a;\ell)\right)\] \[\quad+\sum_{a\in A}\sum_{s^{\prime}\in S_{1}}q^{\widetilde{P}, \pi_{2}}(s^{\prime})\left(V^{\widetilde{P},\pi_{1}}(s^{\prime};\ell)-V^{ \widetilde{P},\pi_{2}}(s^{\prime};\ell)\right)\] \[=\sum_{s\neq s_{L}}\sum_{a\in A}q^{\widetilde{P},\pi_{2}}(s,a) \left(V^{\widetilde{P},\pi_{1}}(s;\ell)-Q^{\widetilde{P},\pi_{1}}(s,a;\ell) \right),\]

where the last step follows from recursively repeating the first three steps. 

**Lemma D.3.3**.: _(Occupancy Measure Difference, (Jin et al., 2021, Lemma D.3.11)) For any transition functions \(P_{1},P_{2}\) and any policy \(\pi\),_

\[q^{P_{1},\pi}(s)-q^{P_{2},\pi}(s) =\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P_{1},\pi}(u,v) \left(P_{1}(w|u,v)-P_{2}(w|u,v)\right)q^{P_{2},\pi}(s|w)\] \[=\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P_{2},\pi}(u,v) \left(P_{1}(w|u,v)-P_{2}(w|u,v)\right)q^{P_{1},\pi}(s|w),\]

_where \(q^{P^{\prime},\pi}(s|w)\) is the probability of visiting \(s\) starting \(w\) under policy \(\pi\) and transition \(P^{\prime}\)._

**Lemma D.3.4**.: _((Dann et al., 2023, Lemma 17)) For any policies \(\pi_{1},\pi_{2}\) and transition function \(P\),_

\[\sum_{s\neq s_{L}}\sum_{a\in A}\left|q^{P,\pi_{1}}(s,a)-q^{P,\pi_{2}}(s,a) \right|\leq L\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{1}}(s)\left|\pi_{1}(a|s )-\pi_{2}(a|s)\right|.\]

**Corollary D.3.5**.: _For any policies \(\pi_{1}\), mapping \(\pi_{2}:S\to A\) (that is, a deterministic policy), and transition function \(P\), we have_

\[\sum_{s\neq s_{L}}\sum_{a\in A}\left|q^{P,\pi_{1}}(s,a)-q^{P,\pi_{2}}(s,a)\right| \leq 2L\sum_{s\neq s_{L}}\sum_{a\neq\pi_{2}(s)}q^{P,\pi_{1}}(s,a).\]

Proof.: According to Lemma D.3.4, we have

\[\sum_{s\neq s_{L}}\sum_{a\in A}\left|q^{P,\pi_{1}}(s,a)-q^{P,\pi_{2}}(s,a) \right|\leq L\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{1}}(s)\left|\pi_{1}(a|s) -\pi_{2}(a|s)\right|.\]

Note that, for every state \(s\), it holds that

\[\sum_{a\in A}\left|\pi_{1}(a|s)-\pi_{2}(a|s)\right|=\sum_{a\neq\pi_{2}(s)}\pi_ {1}(a|s)+\left|\pi_{1}(\pi_{2}(s)|s)-1\right|=2\sum_{a\neq\pi_{2}(s)}\pi_{1}(a| s),\]

where the first step follows from the fact that \(\pi_{2}(a|s)=1\) when \(a=\pi_{2}(s)\), and \(\pi_{2}(b|s)=0\) for any other action \(b\neq\pi_{2}(s)\).

Therefore, we have

\[L\sum_{s\neq s_{L}}\sum_{a\in A}q^{P,\pi_{1}}(s)\left|\pi_{1}(a|s)-\pi_{2}(a|s )\right|\leq 2L\sum_{s\neq s_{L}}\sum_{a\neq\pi_{2}(s)}q^{P,\pi_{1}}(s,a),\]

which concludes the proof. 

According to Lemma D.3.3, we can estimate the occupancy measure difference caused by the corrupted transition function \(P_{t}\) at episode \(t\).

**Corollary D.3.6**.: _For any episode \(t\) and any policy \(\pi\), we have_

\[\left|q^{P,\pi}(s)-q^{P_{t},\pi}(s)\right|\leq C_{t}^{\mathsf{P}},\quad\forall s \neq s_{L},\quad\text{and}\quad\sum_{s\neq s_{L}}\left|q^{P,\pi}(s)-q^{P_{t}, \pi}(s)\right|\leq LC_{t}^{\mathsf{P}}.\]

Proof.: By direct calculation, we have for any episode \(t\) and any \(s\neq s_{L}\)

\[\left|q^{P,\pi}(s)-q^{P_{t},\pi}(s)\right| \leq\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi}(u,v)\left| P(w|u,v)-P_{t}(w|u,v)\right|q^{P_{t},\pi}(s|w)\] \[\leq\sum_{k=0}^{k(s)-1}\sum_{u\in S_{k}}\sum_{v\in A}q^{P,\pi}(u, v)\left\|P(\cdot|u,v)-P_{t}(\cdot|u,v)\right\|_{1}\] \[\leq C_{t}^{\mathsf{P}},\]

where the first step follows from Lemma D.3.3, the second step bounds \(q^{P_{t},\pi}(s|w)\leq 1\), and the last two steps follows from the definition of \(C_{t}^{\mathsf{P}}\).

Moreover, taking the summation over all states \(s\neq s_{L}\), we have

\[\sum_{s\neq s_{L}}\left|q^{P,\pi}(s)-q^{P_{t},\pi}(s)\right|\] \[\leq\sum_{s\neq s_{L}}\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q ^{P,\pi}(u,v)\left|P(w|u,v)-P_{t}(w|u,v)\right|q^{P_{t},\pi}(s|w)\] \[=\sum_{k=0}^{L-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi}(u,v)\left|P(w|u,v)-P_{t}(w|u,v)\right|\sum_{h=k+1}^{L-1}\sum_{s\in S_{h}}q^{P_{t},\pi}(s|w)\] \[\leq L\sum_{k=0}^{k(s)-1}\sum_{u\in S_{k}}\sum_{v\in A}q^{P,\pi}( u,v)\left\|P(\cdot|u,v)-P_{t}(\cdot|u,v)\right\|_{1}\] \[\leq LC_{t}^{\mathsf{P}},\]

where the third step follows from the fact that \(\sum_{s\in S_{h}}q^{P_{t},\pi}(s|w)=1\) for any \(h\geq k(w)\).

[MISSING_PAGE_EMPTY:59]

[MISSING_PAGE_EMPTY:60]

\[\leq L\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}\frac{q^{P,\pi_{t}}(u,v) \log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}+L\sum_{h=0}^{k(s)-1}\sum_{(x,y,z)\in W_{ h}}\frac{q^{P,\pi_{t}}(x,y)\log{(\iota)}}{\widehat{m}_{i(t)}(x,y)}\] \[\leq\mathcal{O}\left(L|S|\sum_{k=0}^{L-1}\sum_{s\in S_{k}}\sum_{a \in A}\frac{q^{P,\pi_{t}}(s,a)\log{(\iota)}}{\widehat{m}_{i(t)}(s,a)}\right),\]

where the first step follows from re-arranging the summation; the second step applies the fact that \(\sqrt{xy}\leq x+y\) for any \(x,y\geq 0\); the third step rearranges the summation order, and the fourth step follows form the facts that \(\sum_{x\in S_{k}}\sum_{y\in A}q^{P,\pi_{t}}(x,y|w)P(z|x,y)=q^{P,\pi_{t}}(z|w)\) and \(\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)P(w|u,v)q^{P,\pi_{t}}(x,y|w)=q^{P,\pi _{t}}(x,y)\) for any \(k\) and \((x,y,z)\).

Similarly, we have the term in Eq. (71) bounded as

\[\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)\sqrt {\frac{P(w|u,v)\log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}}\sum_{h=k+1}^{k(s)-1} \sum_{(x,y,z)\in W_{h}}q^{P,\pi_{t}}(x,y|w)\left(\frac{C^{\textsf{P}}+\log{( \iota)}}{\widehat{m}_{i(t)}(x,y)}\right)\] \[\leq\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}\sum_{h=k+1}^{k(s)-1 }\sum_{(x,y,z)\in W_{h}}\frac{q^{P,\pi_{t}}(u,v)q^{P,\pi_{t}}(x,y|w)\log{(\iota )}}{\widehat{m}_{i(t)}(u,v)}\] \[\quad+\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}\sum_{h=k+1}^{k(s )-1}\sum_{(x,y,z)\in W_{h}}q^{P,\pi_{t}}(u,v)P(w|u,v)q^{P,\pi_{t}}(x,y|w)\left( \frac{C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(x,y)}\right)\] \[\leq\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}\frac{q^{P,\pi_{t} }(u,v)\log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}\left(\sum_{h=k+1}^{k(s)-1}\sum_{ (x,y,z)\in W_{h}}q^{P,\pi_{t}}(x,y|w)\right)\] \[\quad+\sum_{h=0}^{L-1}\sum_{(x,y,z)\in W_{h}}\sum_{k=0}^{k(s)-1} \left(\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)P(w|u,v)q^{P,\pi_{t}}(x,y|w) \right)\frac{C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(x,y)}\] \[\leq|S|\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}\frac{q^{P,\pi_{ t}}(u,v)\log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}+\sum_{h=0}^{L-1}\sum_{(x,y,z) \in W_{h}}\sum_{k=0}^{k(s)-1}q^{P,\pi_{t}}(x,y)\left(\frac{C^{\textsf{P}}+\log {(\iota)}}{\widehat{m}_{i(t)}(x,y)}\right)\] \[\leq|S|^{2}\sum_{s\neq s_{L}}\sum_{a\in A}\frac{q^{P,\pi_{t}}(s, a)\log{(\iota)}}{\widehat{m}_{i(t)}(s,a)}+L|S|\sum_{s\neq s_{L}}\sum_{a\in A}q^{P, \pi_{t}}(s,a)\left(\frac{C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(s,a )}\right),\]

where the first step follows from the fact that \(\widehat{m}_{i(t)}(s,a)\geq C+\log{(\iota)}\) according to its definition; the third step follows from the facts that \(\sum_{x\in S_{k}}\sum_{y\in A}q^{P,\pi_{t}}(x,y|w)\leq 1\) and \(\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)P(w|u,v)q^{P,\pi_{t}}(x,y|w)=q^{P,\pi_ {t}}(x,y)\).

For the term in Eq. (72), we have

\[\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)\frac {C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}\sum_{h=k+1}^{k(s)-1} \sum_{(x,y,z)\in W_{k}}q^{P,\pi_{t}}(x,y|w)\] \[=\sum_{k=0}^{k(s)-1}\sum_{u\in S_{k}}\sum_{v\in A}\sum_{w\in S_{k+ 1}}q^{P,\pi_{t}}(u,v)\frac{C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(u, v)}\left(\sum_{h=k+1}^{(s)-1}\sum_{z\in S_{k+1}}1\right)\] \[\leq|S|^{2}\sum_{u\neq s_{L}}\sum_{v\in A}q^{P,\pi_{t}}(s,a) \left(\frac{C^{\textsf{P}}+\log{(\iota)}}{\widehat{m}_{i(t)}(u,v)}\right),\]

according to the fact that \(\sum_{x\in S_{k}}\sum_{y\in A}q^{P,\pi_{t}}(x,y|w)\leq 1\).

Putting all the bounds for the terms in Eq. (70), Eq. (71), and Eq. (72) together yields the bound of Term (b) that

\[\sum_{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)\left|P(w|u,v)-P^{ \prime}_{t}(w|u,v)\right|\sum_{h=k+1}^{k(s)-1}\sum_{(x,y,z)\in W_{h}}q^{P,\pi _{t}}(x,y|w)\left|P(z|x,y)-P^{\prime}_{t}(z|x,y)\right.\]\[=\mathcal{O}\left(|S|^{2}\sum_{u\neq s_{L}}\sum_{v\in A}q^{P,\pi_{t}}(u,v)\left( \frac{C^{\mathcal{P}}+\log\left(\iota\right)}{\widehat{m}_{i(t)}(u,v)}\right) \right).\]

Combining this bound with that of Term (a) finishes the proof. 

### Loss Shifting Technique with Optimistic Transition

**Lemma D.4.1**.: _Fix an optimistic transition function \(\widetilde{P}\) (defined in Section 5). For any policy \(\pi\) and any loss function \(\ell:S\times A\to\mathbb{R}\), we define function \(g:S\times A\to\mathbb{R}\) similar to that of Jin et al. (2021) as:_

\[g^{\widetilde{P},\pi}(s,a;\ell)\triangleq\left(Q^{\widetilde{P},\pi}(s,a;\ell) -V^{\widetilde{P},\pi}(s;\ell)-\ell(s,a)\right),\]

_where the state-action and state value function \(Q^{\widetilde{P},\pi}\) and \(V^{\widetilde{P},\pi}\) are defined with respect to the optimistic transition \(\widetilde{P}\) and \(\pi\) as following:_

\[Q^{\widetilde{P},\pi}(s,a;\ell) =\ell(s,a)+\sum_{s^{\prime}\in S_{k(s)+1}}\widetilde{P}(s^{\prime }|s,a)V^{\widetilde{P},\pi}(s^{\prime};\ell),\] \[V^{\widetilde{P},\pi}(s;\ell) =\begin{cases}0,&s=s_{L},\\ \sum_{a\in A}\pi(a|s)Q^{\widetilde{P},\pi}(s,a;\ell),&s\neq s_{L}.\end{cases}\]

_Then, it holds for any policy \(\pi^{\prime}\) that,_

\[\left\langle q^{\widetilde{P},\pi^{\prime}},q^{\widetilde{P},\pi}\right\rangle =\sum_{s\neq s_{L}}\sum_{a\in A}q^{\widetilde{P},\pi^{\prime}}(s,a)\cdot g^{ \widetilde{P},\pi}(s,a;\ell)=-V^{\widetilde{P},\pi}(s_{0};\ell),\]

_where \(V^{\widetilde{P},\pi}(s_{0};\ell)\) is only related to \(\widetilde{P}\), \(\pi\) and \(\ell\), and is independent with \(\pi^{\prime}\)._

Proof.: By the extended performance difference lemma of the optimistic transition in Lemma D.3.1, we have the following equality holds for any policy \(\pi^{\prime}\):

\[V^{\widetilde{P},\pi^{\prime}}(s_{0};\ell)-V^{\widetilde{P},\pi}(s_{0};\ell)= \sum_{s\neq s_{L}}\sum_{a\in A}q^{\widetilde{P},\pi^{\prime}}(s,a)\left(Q^{ \widetilde{P},\pi}(s,a;\ell)-V^{\widetilde{P},\pi}(s;\ell)\right).\]

On the other hand, we also have

\[V^{\widetilde{P},\pi^{\prime}}(s_{0};\ell)=\sum_{s\neq s_{L}}\sum_{a\in A}q^{ \widetilde{P},\pi^{\prime}}(s,a)\ell(s,a).\]

Therefore, subtracting \(V^{\widetilde{P},\pi^{\prime}}(s_{0};\ell)\) yields that

\[-V^{\widetilde{P},\pi}(s_{0};\ell)=\sum_{s\neq s_{L}}\sum_{a\in A}q^{ \widetilde{P},\pi^{\prime}}(s,a)\left(Q^{\widetilde{P},\pi}(s,a;\ell)-V^{ \widetilde{P},\pi}(s;\ell)-\ell(s,a)\right).\]

The proof is finished after using the definition of \(g^{\widetilde{P},\pi}\). 

Therefore, we have the following result for the FTRL framework which is similar to Jin et al. (2021); Corollary A.1.2.

**Corollary D.4.2**.: _The FTRL update in Algorithm 4 can be equivalently written as:_

\[\widehat{q}_{t}=\operatorname*{argmin}_{q\in\Omega(\widetilde{P}_{t})}\left\langle q,\sum_{\tau=t_{i}}^{t-1}(\widehat{\ell}_{\tau}-b_{\tau})\right\rangle+\phi_{t }(q)=\operatorname*{argmin}_{x\in\Omega(\widetilde{P}_{t})}\left\langle q,\sum _{\tau=t_{i}}^{t-1}(g_{\tau}-b_{\tau})\right\rangle+\phi_{t}(q),\]

_where \(g_{\tau}(s,a)=Q^{\widetilde{P}_{t},\pi_{\tau}}(s,a;\widehat{\ell}_{\tau})-V^{ \widetilde{P}_{t},\pi_{\tau}}(s;\widehat{\ell}_{\tau})\) for any state-action pair \((s,a)\)._

### Estimation Error

**Lemma D.5.1**.: _([Jin et al., 2020, Lemma 10]) With probability at least \(1-\delta\), we have for all \(k=0,\ldots,L-1\),_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi_{t}}(s,a)}{\max \big{\{}m_{i(t)}(s,a),1\big{\}}}=\mathcal{O}\left(|S_{k}||A|\log\left(T\right)+ \log\left(\frac{L}{\delta}\right)\right),\]

_and_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi_{t}}(s,a)}{\sqrt{ \max\big{\{}m_{i(t)}(s,a),1\big{\}}}}=\mathcal{O}\left(\sqrt{|S_{k}||A|T}+|S_{ k}||A|\log\left(T\right)+\log\left(\frac{L}{\delta}\right)\right).\]

Proof.: Simply replacing the stationary transition \(P\) with the sequence of transitions \(\left\{P_{t}\right\}_{t=1}^{T}\) in the proof of [Jin et al., 2020, Lemma 10] suffices. 

**Proposition D.5.2**.: _Let \(\mathcal{E}_{\textsc{est}}\) be the event such that we have for all \(k=0,\ldots,L-1\) simultaneously_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi_{t}}(s,a)}{\max \big{\{}m_{i(t)}(s,a),1\big{\}}}=\mathcal{O}\left(|S_{k}||A|\log\left(T\right) +\log\left(\iota\right)\right),\]

_and_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi_{t}}(s,a)}{\sqrt{ \max\big{\{}m_{i(t)}(s,a),1\big{\}}}}=\mathcal{O}\left(\sqrt{|S_{k}||A|T}+|S_{ k}||A|\log\left(T\right)+\log\left(\iota\right)\right).\]

_We have \(\Pr[\mathcal{E}_{\textsc{est}}]\geq 1-\delta\)._

Proof.: The proof directly follows from the definition of \(\iota\), which ensures that \(\iota\geq\nicefrac{{L}}{{\delta}}\). 

Based on the event \(\mathcal{E}_{\textsc{est}}\), we introduce the following lemma which is critical in analyzing the estimation error.

**Lemma D.5.3**.: _Suppose the event \(\mathcal{E}_{\textsc{est}}\) defined in Proposition D.5.2 holds. Then, we have for all \(k=0,\ldots,L-1\),_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P,\pi_{t}}(s,a)}{\widehat{ m}_{i(t)}(s,a)}=\mathcal{O}\left(|S_{k}||A|\log\left(T\right)+\log\left(\iota \right)\right),\]

_and,_

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}q^{P,\pi_{t}}(s,a)\left(\frac{C^{ \mathcal{P}}+\log\left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}\right)=\mathcal{ O}\left(\left(C^{\mathcal{P}}+\log\left(\iota\right)\right)|S_{k}||A|\log\left( \iota\right)\right),\]

Proof.: According to Corollary D.3.6, we have

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P,\pi_{t}}(s,a)}{\widehat{m}_{i(t)}(s,a)}\] \[\leq\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi _{t}}(s,a)}{\widehat{m}_{i(t)}(s,a)}+\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A }\frac{\big{|}q^{P,\pi_{t}}(s,a)-q^{P_{t},\pi_{t}}(s,a)\big{|}}{\widehat{m}_{i( t)}(s,a)}\] \[\leq\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\frac{q^{P_{t},\pi _{t}}(s,a)}{\max\big{\{}1,m_{i(t)}(s,a)\big{\}}}+\sum_{t=1}^{T}\sum_{(s,a)\in S _{k}\times A}\frac{C_{t}^{\mathcal{P}}}{C^{\mathcal{P}}+\log(\iota)}\] \[\leq\mathcal{O}\left(|S_{k}||A|\log\left(T\right)+\log\left(\iota \right)\right),\]where the second step follows from the definitions of \(C^{\mathsf{P}}\) and \(\widehat{m}_{i(t)}(s,a)\), and the last step applies Lemma D.5.1.

Similarly, we have

\[\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}q^{P,\pi_{t}}(s,a)\left( \frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}\right)\] \[\leq\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\left(\left|q^{P_{i},\pi_{t}}(s,a)-q^{P,\pi_{t}}(s,a)\right|+q^{P_{i},\pi_{t}}(s,a)\right)\left( \frac{C^{\mathsf{P}}+\log\left(\iota\right)}{\widehat{m}_{i(t)}(s,a)}\right)\] \[\leq\sum_{t=1}^{T}\sum_{(s,a)\in S_{k}\times A}\left(C_{t}^{ \mathsf{P}}+q^{P_{i},\pi_{t}}(s,a)\left(\frac{C^{\mathsf{P}}+\log\left(\iota \right)}{\widehat{m}_{i(t)}(s,a)}\right)\right)\] \[\leq\left|S_{k}\right|\left|A\right|\sum_{t=1}^{T}C_{t}^{\mathsf{ P}}+\left(C^{\mathsf{P}}+\log\left(\iota\right)\right)\sum_{t=1}^{T}\sum_{(s,a )\in S_{k}\times A}\frac{q^{P_{i},\pi_{t}}(s,a)}{\widehat{m}_{i(t)}(s,a)}\] \[=\mathcal{O}\left(\left|S_{k}\right|\left|A\right|C^{\mathsf{P}}+ \left(C^{\mathsf{P}}+\log\left(\iota\right)\right)\left|S_{k}\right|\left|A \right|\log\left(\iota\right)\right)\] \[=\mathcal{O}\left(\left(C^{\mathsf{P}}+\log\left(\iota\right) \right)\left|S_{k}\right||A|\log\left(\iota\right)\right),\]

where the first step adds and subtracts \(q^{P_{i},\pi_{t}}\); the second step follows from Corollary D.3.6; the fourth step uses Proposition D.5.2 due to the fact that \(\widehat{m}_{i}(s,a)\geq\max\left\{m_{i}(s,a),1\right\}\). 

**Lemma D.5.4**.: _(Extension of [4, Lemma 16] for adversarial transition) Suppose the high-probability events \(\mathcal{E}_{\textsc{test}}\) (defined in Proposition D.5.2) and \(\mathcal{E}_{\textsc{con}}\) (defined in Lemma D.2.1) hold together. Let \(P_{t}^{s}\) be a transition function in \(\mathcal{P}_{i(t)}\) which depends on \(s\), and let \(g_{t}(s)\in[0,G]\) for some \(G>0\). Then,_

\[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\left|q^{P_{t}^{s},\pi_{t}}(s)-q^ {P,\pi_{t}}(s)\right|g_{t}(s) =\mathcal{O}\left(\sqrt{\left|L|S|^{2}|A|\log^{2}\left(\iota \right)\sum_{t=1}^{T}\sum_{s\neq s_{L}}q^{P,\pi_{t}}(s)g_{t}(s)^{2}}\right)\] \[\quad+\mathcal{O}\left(G\left(C^{\mathsf{P}}+\log\left(\iota \right)\right)L^{2}|S|^{4}|A|\log\left(\iota\right)\right).\]

Proof.: By Lemma D.3.8, under the event \(\mathcal{E}_{\textsc{con}}\), we have

\[\sum_{t=1}^{T}\sum_{s\neq s_{L}}\left|q^{P_{t}^{s},\pi_{t}}(s)-q ^{P,\pi_{t}}(s)\right|g_{t}(s)\] \[\leq\mathcal{O}\left(\sum_{t=1}^{T}\sum_{s\neq s_{L}}g_{t}(s)\sum _{k=0}^{k(s)-1}\sum_{(u,v,w)\in W_{k}}q^{P,\pi_{t}}(u,v)\sqrt{\frac{P(w|u,v) \log\left(\iota\right)}{\widehat{m}_{i(t)}(u,v)}}q^{P,\pi_{t}}(s|w)\right) \tag{73}\] \[\quad+\mathcal{O}\left(G|S|^{3}\sum_{t=1}^{T}\sum_{s\neq s_{L}} \sum_{a\in A}q^{P,\pi_{t}}(s,a)\left(\frac{C^{\mathsf{P}}+\log\left(\iota \right)}{\widehat{m}_{i(t)}(s,a)}\right)\right).\]

By Lemma D.5.3, the last two terms can be bounded under the event \(\mathcal{E}_{\textsc{test}}\) as

\[\mathcal{O}\left(G|S|^{3}\sum_{t=1}^{T}\sum_{s\neq s_{L}}\sum_{ a\in A}q^{P,\pi_{t}}(s,a)\left(\frac{C^{\mathsf{P}}+\log\left(\iota\right)}{ \widehat{m}_{i(t)}(s,a)}\right)\right)\] \[\leq\mathcal{O}\left(G|S|^{3}\sum_{k=0}^{L-1}\left(C^{\mathsf{P} }+\log\left(\iota\right)\right)|S_{k}||A|\log\left(\iota\right)\right)\] \[=\mathcal{O}\left(G\left(C^{\mathsf{P}}+\log\left(\iota\right) \right)L|S|^{4}|A|\log\left(\iota\right)\right).\]

[MISSING_PAGE_EMPTY:65]

\[\leq\mathcal{O}\left(\sqrt{L|S|^{2}|A|\log^{2}{(\iota)}}\sum_{t=1}^{T}\sum_{s\neq s _{L}}q^{P,\pi_{t}}(s)g_{t}(s)^{2}\right),\]

which finishes the proof.