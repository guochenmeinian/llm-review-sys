[MISSING_PAGE_EMPTY:1]

Thus, in this paper, we propose _LRM-Zero_, trained on purely synthesized data, to explore another route which can potentially resolve the 3D data scarcity, licensing, and bias issues. The name 'Zero' highlights our synthesized and non-semantic training data, which we named as _Zeroverse_. _Zeroverse_ is a procedural, amorphic alternative to Olaverse [23] in training reconstruction models. The comparison between _Zeroverse_ and Olaverse is illustrated in Fig. 1, and more visual comparisons can be found in Appendix. The data in _Zeroverse_ is procedurally created by randomly composing primitive shapes with textures and applying shape augmentations. The process resembles the previous work Xu et al. [100]. We select five primitive shapes: cube, sphere, cylinder, cone, and torus to cover different types of surfaces and topological characteristics. The textures are randomly applied, which is realistic at low-level but do not contain high-level semantics. The three different augmentation methods, i.e., height-field, boolean difference, and, wireframes, help increase the data diversity and add more curvatures, concavity, and thin structures, respectively. The primitive shapes, textures, and an illustration of the augmentations are shown in Fig. 2. In this work, we experiment with 400K _Zeroverse_ data, which roughly matches the number of meaningful data in Ojaverse (i.e., excluding rendering failures, flatten 3D data, point clouds, unsafe data from the overall 800K data). The initial experiments indicate that further increasing the amount of data is not effective, and we refer the reader to Appendix for our early results on scaling the data size.

We validate our _Zeroverse_ by training GS-LRM [107] over it, and we denote this model as _LRM-Zero_. Surprisingly, we found that _LRM-Zero_ can achieve a reconstruction quality similar to that of GS-LRM trained on Ojaverse, seeing Fig. 1. More comparisons are provided in the Appendix. We also quantitatively evaluate the model on two standard 3D reconstruction benchmark ABO [18] and GSO [28]. For sparse-view reconstruction (i.e., 4 views and 8 views), _LRM-Zero_ reaches competitive results against GS-LRM, and the best results gap is as low as \(1.12\) PSNR, \(0.09\) SSIM, and \(0.006\) LPIPS. A plausible reason for such "zero"-shot data generalization is that 3D reconstruction (with poses) relies more on the local visual clues instead of the global semantics. This is more obvious for dense-view reconstruction (e.g. 100 input views) where single-shape optimization without any data prior can reach good results [6; 46]. For the sparse-view reconstruction that we focused, _LRM-Zero_

Figure 1: We present our _LRM-Zero_ framework trained with synthesized procedural data _Zeroverse_. _Zeroverse_ (top left) is created from random primitives with textures and augmentations, thus it does not contain semantical information as in Ojaverse (bottom left). Nevertheless, when training with the same large reconstruction model architecture [107] on both datasets, _LRM-Zero_ can match objaverse-trained LRM’s (denoted as ‘LRM’) visual quality (right part) of reconstructions. A possible explanation is that 3D reconstruction, although serves as a core task in 3D vision, rely mostly on local information instead of global semantics. Reconstruction is visualized with RGB and position-based renderings, and interactive viewers can be found on our website.

can possibly rely on the local details (such as cross-view patch correspondence) to infer the shape, where _Zeroverse_ supports _LRM-Zero_ to learn such knowledge.

We analyze the effectiveness of _Zeroverse_'s design, especially for shape augmentations. We find that each type of augmentation provides visible structural improvements for the reconstructions, and most of the improvements are reflected in the metrics of our benchmarks. We also study the impact of different dataset designs on another critical property of _LRM-Zero_: training stability. Training stability is crucial for large-scale training as large models are more prone to diverge after training for a significantly long time [72; 17; 22]. We empirically found that careless design of _Zeroverse_ can introduce significant instability during the training of _LRM-Zero_. As both data complexity and model hyperparameters can affect the training stability, a model-data co-design is helpful in our experiments, i.e., the model's hyperparameters and data properties are tuned jointly. Lastly, we show the generalizability of both _Zeroverse_ and _LRM-Zero_. For _Zeroverse_, we show that the dataset can also enable training a NeRF-based reconstruction model and reaches competitive results to Obiayverse-trained models. For _LRM-Zero_, we demonstrate that the model can generalize across different datasets including realistic 3D data, such as OmniObject3D [95] and OpenIllumination [55]. We also show that _LRM-Zero_ can be combined with off-the-shelf multi-view diffusion models to support both text-to-3D generation and image-to-3D generation.

The key contribution of this paper is to demonstrate that purely synthesized data can be utilized to learn generic 3D priors for sparse-view 3D reconstruction, a core task of 3D vision. While our work may appear straightforward, it provides a minimal, yet generalizable proof-of-concept which can inspire the community to exploit procedural 3D data for 3D tasks in the future. We also provide carefully crafted studies on the co-design of data and model, as well as their effect on training stability and generalization.

Lastly, we provide the interactive _Zeroverse_ data visualization and _LRM-Zero_ reconstruction results in our website https://desaixie.github.io/lrm-zero/. We recommend the readers to have a check. The _Zeroverse_ data synthesis script is released at https://github.com/desaixie/zeroverse, and we hope that it can facilitate future research.

## 2 Background: feed-forward reconstruction model

Feed-forward 3D reconstruction targets to learn a model that can regress the 3D shapes from multi-view images. The sparse-view version of this task is illustrated in the right part of Fig. 1, where multiple input views are presented, and the output is a 3D representation. To solve this task, LRM [41] introduces a pure-transformer based method which allows scalable training. Original LRM uses NeRF [61; 12] as 3D representation, and a bunch of later works [13; 84; 16; 99; 107] extend it to Gaussian Splatting [46], which is another 3D representation proposed recently. This paper is mostly experimented with the GS-LRM [107] architecture given its simplicity in model design (i.e., a pure-transformer architecture) and the SotA reconstruction quality.

GS-LRM predict the 3D Gaussians from the \(n\) multi-view images \(I_{1},\ldots,I_{n}\). The images are first patchified to features \(f_{1},\ldots,f_{n}\) with shared non-overlapping (i.e., stride equals to kernel size) convolutions. Then features are flattened and concatenated as the input to a self-attention transformer.

\[f_{1},\ldots,f_{n} =\mathrm{Conv}(I_{1}),\ldots,\mathrm{Conv}(I_{n})\] (1) \[x =[\mathrm{Flatten}(f_{1});\ldots;\mathrm{Flatten}(f_{n})]\] (2) \[y =\mathrm{Transformer}(x)\] (3)

The output \(y\) of the transformer will be directly interpreted as the Gaussian Splatting parameters, and serves as the representation of the output 3D object. These parameters can be rendered for training losses or viewed interactively. The GS-LRM model is purely trained with RGB rendering loss by minimizing the difference between ground truth image and the rendering images. For more details of the GS-LRM model [107] architecture and Gaussian Splatting representation [46], please refer to the original papers. After briefly introducing the backbone model architecture of _LRM-Zero_, we next introduce our procedural data _Zeroverse_ to train such a model.

## 3 The _Zeroverse_ dataset

In this section, we introduce the creation of _Zeroverse_ that supports training a sparse-view large reconstruction model (LRM). _Zeroverse_ consists of procedurally synthesized shapes with randomized parameters by revisiting the pipeline in the previous work [100], which was initially proposed for relighting and later extended for view synthesis [101] and material estimation [7, 52]. As illustrated in Fig. 2, the process first composites primitive shapes with random texturing (Sec. 3.1). Then, different augmentations are applied to enhance the diversity of the data (Sec. 3.2). As the LRM-based model only relies on multi-view rendering to train the model (i.e., do not require geometry supervision), the _Zeroverse_ objects are always saved in the compact mesh format.

### Composing primitives into textured shapes

**Primitive shapes.** Our synthetic object creation process starts with a pool of primitive shapes. The pool only consists of basic shapes for the 3D world. Specifically, in our implementation, we have 5 primitives: cube, sphere, cylinder, cone, and torus. Intuitively, cubes and spheres provides knowledge on the sharp straight lines and the purely curved shapes. Cylinders and cones contain different curved surfaces besides sphere. The specialty of torus is its hole, which is topologically different to the above shapes (i.e., a torus has genus 1). Although it is possible to create holes through combinations and augmentations (e.g., the boolean difference and wireframe in Sec. 3.2), we decide to explicitly add this capacity to our dataset. Also, the combination of multiple torus is easy to create shapes with higher genus (i.e., roughly the number of disjoint holes in a connected shape).

**Compositions.** With a reasonable pool of primitive shapes, we then compose them together to construct complex shapes, offering more diverse visual cues for the reconstruction task. We randomly sample 1 to 9 primitives (with replacement) from the primitive pool. The sampling probability of the numbers of primitives is configurable. Each sampled primitive will independently be scaled, translated, and rotated randomly. We simply combine these affine-transformed shapes together without special handling of the shape intersections or disconnections. Thus it is possible to have multiple disjoint shapes in one scene, which we will still refer to as one object. This satisfies the requirement for real-world reconstruction applications, where simple disjoint shapes would be considered as a single object.

**Texturing.** For each surface of the shape, we apply a texture randomly sampled from an internal dataset. To support the research community, in our public release version, we provide an alternative public texture dataset.

Figure 2: Illustration of the _Zeroverse_ data creation process. A random textured shape is first composited from primitive shapes and textures (Sec.3.1). Then different augmentations (i.e., height field, boolean difference, wireframes in Sec. 3.2) are applied to enhance the dataset characteristics (e.g., curved surfaces, concavity, and thin structures). More visualizations in Appendix and website.

### Shape augmentations

We apply augmentation to the textured shapes to add diversity and complexity that resembles real-world objects and is not covered by the initial shape in Sec. 3.1. We implement three augmentation operators: height field, boolean difference, and wireframe conversion for better data coverage of curved shapes, concave shapes, and thin structured respectively. These diversities of the data will be reflected by the capacity of large reconstruction models with observable structural improvements (studied in Sec. 5.1). We illustrated the process and example results in the right part of Fig. 2. We do not apply the augmentation 'boolean difference' and 'wireframe' at the same time. This is for training stability (studied in Sec. 5.2) as we empirically found that an ultra-complex shape can lead the reconstruction model training to non-convergence.

**Height fields.** Most of the surfaces (except the torus) of our primitives have constant curvatures, and we apply height fields augmentation in Xu et al. [100] to break this constraint. An illustration of the height map can be found in Fig. 2 (top right). In detail, for each face of the primitives, we apply a height field with varying heights and curvatures to displace the surface vertices, making the surface curved and bumpy. Specifically, the magnitude of height is randomly sampled at each position in the map and we use bicubic interpolation to obtain smooth surfaces.

**Boolean difference.** Concave structures are common in real-world objects, for example, bowls, hats, spoons. However, the concavity is not well captured by the previous pipeline. To resolve this, we'subtract' primitives from the shapes, which can be considering as a reversion of the 'additive' operators in the combination process. This is implemented by computing the boolean difference between the composite object in Sec. 3.1 and a basic primitive from our pool. In details, we use Blender's boolean modifier and solidify modifier to augment the initial shape. The inside faces of the resulting cut shape will have the same texture as the outside faces. Besides introducing concavity to the dataset, the boolean difference operation also expose the 'interior' of the shape (as shown in Fig. 2), which helps the reconstruction model to handle complex structures. The actual effect of the boolean operator is quite diverse, and we refer the reader to check the visualization in the Appendix.

**Wireframe.** Besides concavity, thin structures (especially the striped or repeated one) is another challenge in real-world reconstruction, for example, hairs, baskets, railings. To train a reconstruction model capable with thin structures, we want to explicitly add this characteristic to our _Zeroverse_ dataset. And for simplicity, we use the wireframe. Wireframe is a basic augmentation from the primitive shapes, which generally converts their meshes to the skeletons. It is pre-implemented in multiple libraries, and we take the shape modifier in Blender. The results are illustrated in Fig. 2 (i.e., a wireframe of torus) and more in Appendix. The texture of the wireframe is inherited from the primitive shape but usually not distinguishable due to its thin surfaces.

## 4 Experiments

### _LRM-Zero_ experiment details

For rendering the multi-view images of _Zeroverse_, we follow [41]. For each object in _Zeroverse_, we render 32 views with randomly sampled camera rotations and random distances in the range of [2.0, 3.0]. Each image is rendered at the 512 \(\times\) 512 resolution with uniform lighting. We use the same

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{GSO} & ABO \\ \cline{3-8}  & & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline \multirow{4}{*}{8 input views} & GS-LRM & **33.23** & **0.971** & **0.031** & **30.92** & **0.944** & **0.067** \\  & _LRM-Zero_ & 31.62 & 0.960 & 0.039 & 28.71 & 0.929 & 0.078 \\ \cline{2-8}  & GS-LRM & **31.90** & **0.966** & **0.030** & **30.66** & **0.949** & **0.055** \\  & _LRM-Zero_ & 30.78 & 0.957 & 0.036 & 28.82 & 0.934 & 0.065 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative results comparing _LRM-Zero_ with GS-LRM [107] (trained on Objavverse) under the 8-input-view setting. We use GSO [28] and ABO [18] evaluation datasets and PSNR, SSIM, and LPIPS [108] metrics. _LRM-Zero_ demonstrates competitive performance against GS-LRM.

network architecture and follow the hyperparameters/implementation (e.g., 80K training steps, details as GS-LRM [107]. We only decrease perceptual loss weight from 0.5 to 0.2 to improve training stability. For the results comparison, we pre-train the model with \(256\)-resolution and fine-tuned on \(512\)-resolution following GS-LRM. The overall training uses 64 A100 GPUs and takes 3 days. For analysis and ablation studies, we only run the \(256\)-resolution experiments. Please refer to the original GS-LRM paper [107] for more experimental details.

Metric evaluations for results and analysis are mostly conducted on two relatively large benchmarks: Google Scanned Objects (GSO) [28] and Amazon Berkeley Objects (ABO) [18]. In our paper, we use 8 structural input-view as the standard evaluation protocol to increase view coverage. The 4 structural input-view results are provided in Appendix. In details, for 8 structural views, we render from 0 elevation with 0, 90, 180, 270 azimuth plus 40 elevation with 45, 135, 225, and 315 azimuth, while 4 structural views render from 20 elevation with 0, 90, 180, 270 azimuth. The testing views for metric calculation are randomly sampled. The generalization experiments in Sec. 5.3 use either \(8\) random input views for generalization test, or the fixed cameras provided by the generated models. We always assume that the camera poses are provided with input views.

### Results

We evaluate _LRM-Zero_ on the benchmarks and show the results in Tab. 1. The absolute PSNR values of GSO and ABO are over 30 and 28.7 respectively, which indicates that the reconstruction has high visual quality. Compared to GS-LRM [107] trained on Objavverse, the metric still shows a gap, but within a reasonable range of \(1.1\) PSNR on GSO and \(1.9\) PSNR on ABO. The gap is larger for higher resolution (i.e., Res-512) and it is possibly due to the training configuration of our 512-res fine-tuning is sub-optimal. Qualitatively, we do not observe significant visual difference between the reconstructed 3D models from _LRM-Zero_ and GS-LRM. An example comparison is shown in Fig. 1 and some more comparisons in the Appendix. The interactive viewer of _LRM-Zero_ reconstruction results can be found in our website.

After viewing the _LRM-Zero_ visual results and the sparse-view reconstruction setup, we found that both 4-view and 8-view can not fully cover the object surfaces thus the model needs to hallucinate the invisible parts. This hallucination ability requires semantic understanding of the 3D objects while _Zeroverse_ lacks by design. It might be the major reason of result gap between _Zeroverse_-trained and Objavverse-trained models in Tab. 1. The invisible regions can be mitigated by reconstructing from more views (either capturing or generating). However, more views involves more tokens and challenges the computation cost of the current fully-connected self-attention design in GS-LRM, thus beyond the scope of current paper and we leave it as future works.

## 5 Analysis

In this section, we analyze the properties of our _LRM-Zero_ trained with the synthesized _Zeroverse_. We first conduct ablation studies in Sec. 5.1 to show the effectiveness of _Zeroverse_ augmentations. Next, Sec. 5.2 explores stabilized training of _LRM-Zero_ from both data and model perspective, as training stability is one of the key challenges in large-scale training [22; 17]. Last, we show the generalization of our methods by applying _LRM-Zero_ over diverse data, and trained different reconstruction models on _Zeroverse_.

### Ablation studies on different augmentations

We conduct ablation studies to verify the effectiveness of our height field, boolean difference, and wireframe augmentations. We show both quantative and qualitative comparisons.

**Boolean difference and wireframe augmentation.** As our sampling strategy does not apply boolean difference and wireframe augmentations jointly to avoid over-complex shapes. Therefore, we conduct the ablation study of these augmentations together. As shown in Tab. 2, we apply different sampling ratios to both augmentations (e.g., experiments id 1, 2, 3) and also exclude them in experiment 4. Boolean difference augmentation largely improves the metric (comparing experiment pair 2, 4 or 1, 3). Note that we use 60%/40% instead of 50%/50% because the later one has more instability (Sec. 5.2). The possible reason is visualized in Fig. 3: the lack of boolean augmentation in training data causes experiment 2 to show structural failure on concave shapes.

The wireframe augmentation does not show significant improvements of the metric, but it increases the visual fidelity. As shown in Fig. 4, without wireframe augmentation in its training data, _LRM-Zero_ fails to reconstruct objects with thin structures, e.g. chair and table legs, or rails.

**Height-field augmentation** Tab. 3 shows two experiments with and without height field augmentation. Both are trained on 120K objects consisting of 80K original compositional objects and 40K boolean augmentation objects. This setting is different from other ablation experiments in Tab. 2, because we had to synthesize and render objects with 0 height field probability, which do not exist in _Zeroverse_. We also uses the boolean-difference only augmentation to mitigate the effect of instability. These results reveal that height field augmentation can improve the results.

### Training stability

As discussed in Sec. 5.1, adding augmentation substantially boosts _LRM-Zero_'s performance. However, it also makes _Zeroverse_ more complex and thus introduces training instability in _LRM-Zero_. We explore various techniques to help stabilize the training from either the training side (i.e., decreasing perceptual loss weight, decreasing Guassian splitting scale clipping, decreasing view-angle threshold) or the data mixing ratio of augmentations (we found that height-filed augmentation does not introduce instability a lot thus kept it). The observations are summarized in Tab. 4. In general, we observe that shifting training hyperparameters from optimal would improve the stability. However, this would decrease the performance. Thus our final plan (as shown in experiment 6) is a more balanced augmentation mixing ratio, and only minimal change on the training side. More comprehensive experiments are in Appendix.

### Generalization

We first validate the generalization of our _Zeroverse_ by training a NeRF-based LRM model [49; 92] on it. NeRF-based model's architecture is different from GS-LRM. Also the 3D modeling is philosophically different: NeRF has a canonical space for Triplane (i.e., Eulerian representation) while Gaussian Splatting is pixel-aligned per-point prediction (i.e., Lagrangian representation).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{dataset} & \multicolumn{3}{c}{GSO} & ABO \\ \cline{2-10} id & hf-only & boolean & wireframe & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1 & default & default & default & **30.78** & **0.957** & **0.036** & **28.82** & **0.934** & **0.065** \\ \hline
2 & 60.0\% & 40.0\% & 0\% & 30.75 & 0.957 & 0.036 & 28.76 & 0.931 & 0.066 \\ \hline
3 & 66.6\% & 0\% & 33.3\% & 29.82 & 0.948 & 0.042 & 27.79 & 0.923 & 0.075 \\ \hline
4 & 100.0\% & 0\% & 0\% & 29.88 & 0.949 & 0.042 & 27.39 & 0.919 & 0.077 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation studies over boolean difference and wireframe augmentations. The height-field (hf) is applied independently to each surface with prob. 0.5.

Figure 3: Qualitative results generated by _LRM-Zero_ trained on _Zeroverse_ with (left two) and without boolean difference augmentation (right two). Right two _LRM-Zero_’s reconstruction results have structural failures on objects with concave shapes and complex structures.

Despite of these differences, the results in Tab. 5 are similar to what we observed in GS-LRM that _Zeroverse_-trained model is competitive to Objaverse-trained models.

Besides the standard benchmark GSO and ABO, we also evaluate our _LRM-Zero_ on diverse datasets to show its generalization, such as realistic 3D objects in OpenIllumination [55] and OmniObject3D [95], cross-evaluation on Zeroverse and Objaverse, and the generative outputs by Instant3D [49] and One2345++ [57]. As these experiments are for generalization test, we use 8 randomly-sampled input for OpenIllumination, OmniObject3D, Objaverse, and Zeroverse. For Instant3D and One2345++, we use the default camera setup of the generative model's outputs, where Instant3D and One2345++ have 4 and 6 structural cameras, respectively. As shown in Tab. 6, our _LRM-Zero_ is competitive. We visualize the Instant3D and One2345++ results in Fig. 5, where _LRM-Zero_ still work for these truly novel generated images, showcasing that _LRM-Zero_ can be used in the 3D generation pipeline.

## 6 Related works

3D reconstruction is an important task in 3D vision. As 3D data is usually hard to capture, 3D reconstruction gives the ability to get 3D model from other modalities (e.g., images). Traditional methods [66; 61; 56; 14] on 3D reconstruction focuses on the per-sample optimization, where the 3D

Figure 4: Qualitative results generated by _LRM-Zero_ trained on default _Zeroverse_ with (left two) and without wireframe augmentation (right two). Right two _LRM-Zero_’s reconstruction results have structural failures on objects with thin structures.

Figure 5: _LRM-Zero_’s qualitative results on Instant3D text-to-3D (left two) and One2345++ image-to-3D (right two) generated multi-view images.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{dataset} & Height Field & \multicolumn{3}{c}{GSO} & ABO \\ \hline id & \begin{tabular}{c} hf-only \\ def. 40\% \\ \end{tabular} & \begin{tabular}{c} boolean \\ def. 40\% \\ \end{tabular} & \begin{tabular}{c} wireframe \\ def. 20\% \\ \end{tabular} & \begin{tabular}{c} HF probability \\ def. 0.5 \\ \end{tabular} & \begin{tabular}{c} PSNR\(\uparrow\) \\ \end{tabular} & \begin{tabular}{c} SSIM\(\uparrow\) \\ \end{tabular} & \begin{tabular}{c} LPIPS\(\downarrow\) \\ \end{tabular} & \begin{tabular}{c} PSNR\(\uparrow\) \\ \end{tabular} & \begin{tabular}{c} SSIM\(\uparrow\) \\ \end{tabular} & 
\begin{tabular}{c} LPIPS\(\downarrow\) \\ \end{tabular} \\ \hline
1 & 60.0\% & 40.0\% & 0\% & default & **30.24** & **0.952** & **0.039** & **28.31** & **0.926** & **0.072** \\ \hline
2 & & & & 0 & 29.22 & 0.941 & 0.045 & 27.70 & 0.916 & 0.076 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablations studies on height-field augmentation.

shapes are parameterized and optimized by the rendering loss [61] or geometry loss [66, 25]. These optimization-based methods are usually slow and require adequate number of views (e.g., 100 views). Although methods are proposed [82, 30, 104, 62] to resolve these constraints for efficiency and view requirements [43, 64, 81], the speed is not largely improved.

Recent progresses advances this task with learning-based feed-forward methods [105, 94, 80, 63, 45, 50, 44]. Instead of optimization, these methods train a model from large-scale object [73, 106, 97, 23, 24] or scene [109, 71, 54] data to predict the shape directly. Besides the benefits of efficiency, these feed-forward methods can naturally support sparse-views as input (e.g., 4 to 12 input view images) because they learn data patterns from massive dataset. Some models can even go with extreme case of single-view reconstruction [105, 80, 41, 83], which needs to have data prior from realistic 3D data. Multi-view stereo methods [78, 103, 36, 87, 26, 88] are another family of feed-forward 3D reconstruction methods, but they cannot deal with sparse-view or single-view settings since they are based on local feature matching.

Synthetic data has been popular used in computer vision [38, 58, 32, 76], such as in segmentation [15, 21], object detection [42], image classification [39], deblurry [75], face analysis [93], etc. In 3D vision, synthetic data is widely used because of 3D data is harder to harvest, e.g., in depth estimation [4, 69], in optical flow [27, 60, 59], in finding multi-view correspondence [91], and for improving the 3D consistency of multi-view diffusion models [96]. Specifically to reconstruction, the

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{OpenIllumination} & \multicolumn{3}{c}{OmniObject3D} & \multicolumn{3}{c}{Objavverse-test} & \multicolumn{3}{c}{Zeroverse-test} \\ \cline{2-13}  & \multicolumn{3}{c}{PSNR\(\uparrow\)} & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline GS-LRM & 14.02 & **0.598** & 0.460 & **29.32** & **0.940** & **0.055** & **28.37** & **0.920** & **0.079** & 26.48 & 0.880 & 0.089 \\ _LRM-Zero_ & **14.44** & 0.591 & **0.455** & 25.81 & 0.909 & 0.080 & 25.88 & 0.884 & 0.112 & **28.23** & **0.912** & **0.068** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Generalization of _LRM-Zero_ to various evaluation datasets.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{dataset} & \multicolumn{3}{c}{training} & result \\ \cline{2-7} \multirow{-2}{*}{id} & \multirow{-2}{*}{hf-only} & \multirow{-2}{*}{boolean} & \multirow{-2}{*}{wireframe} & perceptual & Gaussian scale & view angle & GSO PSNR, \\  & & & loss weight & clipping & threshold & if finished \\  & & & & (default 0.5) & (default -1.2) & (default 60) & \\ \hline
1 & 100\% & 0\% & 0\% & default & default & default & 29.54 \\ \hline
2 & 20\% & 80\% & 0\% & default & default & default & failed \\ \hline
3 & & & & 0.2 & default & default & failed \\ \hline
4 & 40\% & 60\% & 0\% & 0.2 & default & default & failed \\ \hline
5 & & & & 0.2 & -1.6 & 40 & 30.32 \\ \hline
6 & 40\% & 40\% & 20\% & 0.2 & default & default & 30.78 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Illustrating the training stability issues when constructing the procedural _Zeroverse_ dataset. The instability can be resolved either with training stabilizing techniques (e.g., reducing perceptual loss weight, Gaussian scale clipping, and view angle threshold), or with reducing the complexity of _Zeroverse_. ‘failed’ experiments are usually due to model divergence.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{GSO} & \multicolumn{3}{c}{ABO} \\ \cline{2-9}  & \multicolumn{3}{c}{PSNR \(\uparrow\)} & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline NeRF-LRM-Zero & 29.33 & **0.936** & **0.065** & 28.96 & 0.921 & 0.084 \\ \hline NeRF-LRM-Objv & **29.72** & **0.936** & **0.064** & **30.79** & **0.932** & **0.071** \\ \hline \hline \end{tabular}
\end{table}
Table 5: NeRF-LRM-Zero performs competitively against NeRF-LRM-Objv.

exploration of synthetic is mainly on specific categories, for example, for face [74], for human [33], constructions [47], or for evaluation [1, 31]. Some synthetic data are template-based [9, 34, 51, 102] and injecting human's knowledge about the semantic. Xu et al. [100, 101] and their subsequent works [52, 7, 53, 77, 110] have leveraged procedurally synthesized data for relighting, view synthesis, and various appearance acquisition and rendering tasks. However, these methods are designed for captures under controlled lighting conditions or objects with specific materials; additionally, their data is created on a relatively small scale. We revisit their procedural data generation workflow, extending it with additional data augmentation techniques and scaling it up to train large reconstruction models.

## 7 Limitations

In this paper, we mainly focus on providing a proof of concept on using synthetic to tackle one of the key problems in 3D vision: 3D reconstructions, and here are part of the limitations.

**Scalability.** The scalability of such synthetic-based method is still under investigation. We have done some initial exploration and the results can be found in Appendix. From these early experiments, it seems that the convergence property and optimal training hyperparameters might be different from the standard experimental setup with real data. The scaling-up exploration would naturally involve more resources (mainly computing resources, i.e., GPU hours) which is beyond our affordability.

Also, the community also lacks a study over the scalability of reconstruction models over'real' data. Objaverse-XL [24] brings 10 more data over Objaverse but the data is much nosier, has different formats, contains a large portion without textures, and the legal concerns are not fully resolved. All these issues exposes challenges in understanding the scalability of the feed-forward reconstruction method.

**Semantics.** The synthetic data created in the way of Sec. 3 lacks of semantics (e.g., the data distribution is not supposed to match the real 3D world distribution). Thus this data might not be suitable to learn semantical-rich tasks. For the simplest example, _Zeroverse_ is hard to train single-view reconstructions as shown in MCC [94], Shap-E [45], LRM [41], etc, which learn semantic from Objaverse [23], MvImgNet [106], and Co3D [73]. At the same time, we can complete single-view reconstruction by chaining with multi-view generator [57, 89] as shown in [49], relying on the semantical understanding of multi-view generation. The exact boundary of semantic tasks and intrinsic tasks in 3D vision is still under debate.

## 8 Broader Impacts

The broader impacts of this work are overall positive. First, the proof of concept in using synthesized data would largely reduce the bias inside the real dataset. As the model has weak inductive bias (i.e., through the use of the pure-transformer architecture), the potential semantical bias is mostly from the data. Second, the 3D data are usually having license concerns, where the synthesized data can help resolve. Third, as the 3D reconstruction can be potentially learned from synthetic data without real-world semantic information, we can possibly separate the 3D generation into two problems: generation and reconstruction. The reconstruction is mostly a semantics-free task.

On the other hands, this work can potentially largely lowers the bar of 3D reconstructions, for which data is the main blocker previously. The accessible 3D generation (when chaining with generative models as shown in [49]) and 3D reconstruction ability may introduce legal concerns on 3D licensing and moral concerns on 3D identities.

## 9 Conclusion

We introduced the _LRM-Zero_ and its training data _Zeroverse_. _Zeroverse_ is constructed with procedural synthesizing, where primitive shapes are composited, textured, and then augmented. We found the LRM model trained with _Zeroverse_ can be competitive with Objaverse-trained LRMs, thus illustrating a promising direction of using synthetic data in 3D reconstruction research. We released our data creation code, and hope that it can help future research.

## Acknowledgments

We thank Kalyan Sunkavalli, Nathan Carr, Milos Hasan, Yang Zhou, and Jimei Yang for their support on this project.

## References

* [1] Pranav Acharya, Daniel Lohn, Vivian Ross, Maya Ha, Alexander Rich, Ehsan Sayyad, and Tobias Hollerer. Using synthetic data generation to probe multi-view stereo networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1583-1591, 2021.
* [2] AI@Meta. Llama 3 model card, 2024.
* [3] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. _Claude-3 Model Card_, 2024.
* [4] Amir Atapour-Abarghouei and Toby P Breckon. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2800-2810, 2018.
* [5] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: A space-time diffusion model for video generation, 2024.
* [6] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19697-19705, 2023.
* [7] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5960-5969, 2020.
* [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.
* [9] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14_, pages 561-578. Springer, 2016.
* [10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [11] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024.
* [12] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16123-16133, 2022.
* [13] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. _arXiv preprint arXiv:2312.12337_, 2023.
* [14] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorf: Tensorial radiance fields. In _European Conference on Computer Vision (ECCV)_, 2022.
* [15] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool. Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1841-1850, 2019.
* [16] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. _arXiv preprint arXiv:2403.14627_, 2024.

* [17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.
* [18] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 21126-21136, 2022.
* [19] Together Computer. Redpajama: an open dataset for training large language models, 2023.
* [20] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. _arXiv preprint arXiv:2309.15807_, 2023.
* [21] Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeffrey Mahler, and Ken Goldberg. Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 7283-7290. IEEE, 2019.
* [22] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.
* [23] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objayverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.
* [24] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objayverse-xl: A universe of 10m+ 3d objects. _Advances in Neural Information Processing Systems_, 36, 2024.
* [25] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. _arXiv preprint arXiv:2107.02791_, 2021.
* [26] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8585-8594, 2022.
* [27] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2758-2766, 2015.
* [28] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 2553-2560. IEEE, 2022.
* [29] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Enetzari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* [30] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5501-5510, 2022.
* [31] Mario Fuentes Reyes, Pablo d'Angelo, and Friedrich Fraundorfer. An evaluation of stereo and multiview algorithms for 3d reconstruction with synthetic data. _The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences_, 48:1021-1028, 2023.
* [32] Adrien Gaidon, Antonio Lopez, and Florent Perronnin. The reasonable effectiveness of synthetic visual data. _International Journal of Computer Vision_, 126(9):899-901, 2018.
* [33] Yongtao Ge, Wenjia Wang, Yongfan Chen, Yang Liu, Hao Chen, Xuan Wang, and Chunhua Shen. 3d human reconstruction in the wild with synthetic data using generative models. _ICLR Submission_, 2024.

* [34] Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger, Marcel Luthi, Sandro Schonborn, and Thomas Vetter. Morphable face models-an open framework. In _2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)_, pages 75-82. IEEE, 2018.
* [35] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. _arXiv preprint arXiv:2311.10709_, 2023.
* [36] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2495-2504, 2020.
* [37] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* [38] Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto Cipolla. Understanding real world indoor scenes with synthetic data. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4077-4085, 2016.
* [39] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is synthetic data from generative models ready for image recognition? In _The Eleventh International Conference on Learning Representations_, 2022.
* [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [41] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2024.
* [42] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexander G Schwing. Sail-vos 3d: A synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1418-1428, 2021.
* [43] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5885-5894, 2021.
* [44] Hanwen Jiang, Qixing Huang, and Georgios Pavlakos. Real3d: Scaling up large reconstruction models with real-world images. _arXiv preprint arXiv:2406.08479_, 2024.
* [45] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. _arXiv preprint arXiv:2305.02463_, 2023.
* [46] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4):1-14, 2023.
* [47] Juhyeon Kim, Jeehoon Kim, Yohan Kim, and Hyoungkwan Kim. 3d reconstruction of large-scale scaffolds with synthetic data generation and an upsampling adversarial network. _Automation in Construction_, 156:105108, 2023.
* [48] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Merea Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. VideoPoot: a large language model for zero-shot video generation. _arXiv_, 2312.14125, 2024.
* [49] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _arXiv preprint arXiv:2311.06214_, 2023.
* [50] Rui Li, Tobias Fischer, Mattia Segu, Marc Pollefeys, Luc Van Gool, and Federico Tombari. Know your neighbors: Improving single-view reconstruction via spatial vision-language reasoning. In _CVPR_, 2024.
* [51] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4d scans. _ACM Transactions on Graphics (TOG)_, 36(6):1-17, 2017.

* [52] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Learning to reconstruct shape and spatially-varying reflectance from a single image. _ACM Transactions on Graphics (TOG)_, 37(6):1-11, 2018.
* [53] Zhengqin Li, Yu-Ying Yeh, and Manmohan Chandraker. Through the looking glass: Neural 3d reconstruction of transparent shapes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1262-1271, 2020.
* [54] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. D3dvd-10k: A large-scale scene dataset for deep learning-based 3d vision. _arXiv preprint arXiv:2312.16256_, 2023.
* [55] Isabella Liu, Linghao Chen, Ziyang Fu, Liwen Wu, Haian Jin, Zhong Li, Chin Ming Ryan Wong, Yi Xu, Ravi Ramamoorthi, Zexiang Xu, et al. Openillumination: A multi-illumination dataset for inverse rendering evaluation on real objects. _Advances in Neural Information Processing Systems_, 36, 2024.
* [56] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. _Advances in Neural Information Processing Systems_, 33:15651-15663, 2020.
* [57] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. _arXiv preprint arXiv:2311.07885_, 2023.
* [58] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4040-4048, 2016.
* [59] Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. What makes good synthetic training data for learning disparity and optical flow estimation? _International Journal of Computer Vision_, 126:942-960, 2018.
* [60] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsupervised learning of optical flow with a bidirectional census loss. In _Proceedings of the AAAI conference on artificial intelligence_, 2018.
* [61] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.
* [62] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.
* [63] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.
* [64] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5480-5490, 2022.
* [65] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, and et. al. Gpt-4 technical report, 2024.
* [66] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* [67] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* [68] Guilherme Penedo, Hynek Kydlicek, Leandro von Werra, and Thomas Wolf. Fineweb, 2024.
* [69] Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner, Oliver Lehmann, Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, et al. Depthsynth: Real-time realistic synthetic data generation from cad models for 2.5 d recognition. In _2017 International conference on 3d vision (3DV)_, pages 1-10. IEEE, 2017.

* [70] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [71] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* [72] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International conference on machine learning_, pages 8821-8831. Pmlr, 2021.
* [73] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10901-10911, 2021.
* [74] Elad Richardson, Matan Sela, and Ron Kimmel. 3d face reconstruction by learning from synthetic data. In _2016 fourth international conference on 3D vision (3DV)_, pages 460-469. IEEE, 2016.
* [75] Jaesung Rim, Geonung Kim, Jungeon Kim, Junyong Lee, Seungyong Lee, and Sunghyun Cho. Realistic blur synthesis for learning image deblurring. In _European conference on computer vision_, pages 487-503. Springer, 2022.
* [76] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Aulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10912-10922, 2021.
* [77] Shen Sang and Manmohan Chandraker. Single-shot neural relighting and svbrdf estimation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIX 16_, pages 85-101. Springer, 2020.
* [78] Johannes L Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14_, pages 501-518. Springer, 2016.
* [79] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [80] Bokui Shen, Xinchen Yan, Charles R Qi, Mahyar Najibi, Boyang Deng, Leonidas Guibas, Yin Zhou, and Dragomir Anguelov. Gina-3d: Learning to generate implicit neural assets in the wild. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4913-4926, 2023.
* [81] Ruoxi Shi, Xinyue Wei, Cheng Wang, and Hao Su. Zerorf: Fast sparse view 360 {\(\backslash\)deg} reconstruction with zero pretraining. _arXiv preprint arXiv:2312.09249_, 2023.
* [82] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. _CVPR_, 2022.
* [83] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. _arXiv preprint arXiv:2312.13150_, 2023.
* [84] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. _arXiv preprint arXiv:2402.05054_, 2024.
* [85] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, and et. al. Llama 2: Open foundation and fine-tuned chat models, 2023.

* [86] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [87] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14194-14203, 2021.
* [88] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and Marc Pollefeys. Itermvs: Iterative probability estimation for efficient multi-view stereo. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8606-8615, 2022.
* [89] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. _arXiv preprint arXiv:2312.02201_, 2023.
* [90] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-Irm: Pose-free large reconstruction model for joint pose and shape prediction. _arXiv preprint arXiv:2311.12024_, 2023.
* [91] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In _CVPR_, 2024.
* [92] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh. _arXiv preprint arXiv:2404.12385_, 2024.
* [93] Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 3681-3691, 2021.
* [94] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compressive coding for 3d reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9065-9075, 2023.
* [95] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 803-814, 2023.
* [96] Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, and Arie E Kaufman. Carve3d: Improving multi-view reconstruction consistency for diffusion models with rl finetuning. _arXiv preprint arXiv:2312.13980_, 2023.
* [97] Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingtenq Qiu, Chongjie Wang, Shijie Wang, et al. Mvhumannet: A large-scale dataset of multi-view daily dressing human captures. _arXiv preprint arXiv:2312.02963_, 2023.
* [98] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. _arXiv preprint arXiv:2311.09217_, 2023.
* [99] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. _arXiv preprint arXiv:2403.14621_, 2024.
* [100] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. Deep image-based relighting from optimal sparse samples. _ACM Trans. Graph._, 37(4), 2018.
* [101] Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi. Deep view synthesis from sparse photometric images. _ACM Transactions on Graphics (ToG)_, 38(4):1-13, 2019.
* [102] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao. Facescape: a large-scale high quality 3d face dataset and detailed riggable 3d face prediction. In _Proceedings of the ieee/cvf conference on computer vision and pattern recognition_, pages 601-610, 2020.
* [103] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In _Proceedings of the European conference on computer vision (ECCV)_, pages 767-783, 2018.

* [104] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.
* [105] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4578-4587, 2021.
* [106] Xianggang Yu, Mutiona Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of multi-view images. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9150-9161, 2023.
* [107] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting, 2024.
* [108] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [109] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. _ACM Transactions on Graphics (TOG)_, 37(4):1-12, 2018.
* [110] Shilin Zhu, Zexiang Xu, Henrik Wann Jensen, Hao Su, and Ravi Ramamoorthi. Deep kernel density estimation for photon mapping. In _Computer Graphics Forum_, pages 35-45. Wiley Online Library, 2020.

## Appendix A Appendix summary

In Appendix, we provide more visualization results, more analysis, and more implementation details of our paper. Also note that both _LRM-Zero_ and GS-LRM (when involving the results) refer to the final model (i.e., with respect to their training data and training parameters) instead of just the model architecture. We also use GS-LRM to refer to the model architecture as well for simplicity.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{GSO} & \multicolumn{4}{c}{ABO} \\ \cline{3-8}  & & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline \multirow{4}{*}{4 input views} & GS-LRM & **30.52** & **0.952** & **0.050** & **29.09** & **0.925** & **0.085** \\  & _LRM-Zero_ & 28.49 & 0.937 & 0.063 & 25.40 & 0.893 & 0.115 \\ \cline{1-1} \cline{2-8}  & GS-LRM & **29.59** & **0.944** & **0.050** & **28.92** & **0.926** & **0.074** \\  & _LRM-Zero_ & 27.78 & 0.927 & 0.062 & 25.41 & 0.886 & 0.106 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Quantitative results comparing _LRM-Zero_ with GS-LRM [107] under the 4-input-view setting. We use GSO [28] and ABO [18] evaluation datasets and PSNR, SSIM, and LPIPS [108] metrics. _LRM-Zero_ demonstrates competitive performance against GS-LRM.

Figure 6: Uniformly sampled objects from _Zeroverse_ to visualize its data distribution.

[MISSING_PAGE_FAIL:19]

As shown in Tab. 1, _LRM-Zero_ performs worse than GS-LRM on average. In Fig. 8, we show some qualitative comparisons of the two model's reconstruction results. Although we provide 8 input views, for the first row in Fig. 8, there are still invisible region in the input view. This leads to _LRM-Zero_'s

Figure 8: Qualitative comparison of _LRM-Zero_ (left two columns) and GS-LRM (right two columns). When there is invisible region in the input views (first row), _LRM-Zero_ produces poor reconstruction results. When the input views have good coverage (second row to fifth row), _LRM-Zero_ performs similarly well as GS-LRM.

[MISSING_PAGE_FAIL:21]

converge well on more training data. This again aligns with our hypothesis about the importance of training convergence.

Model sizeWe experiment with 2x (700M parameters) and 3x (1B parameters) model sizes with a lowered learning rate of 3-e4 instead of the default 4e-4. At both 1x (experiment 4, 5) and 2x training steps (experiment 6, 7, 8), the larger models underperform compared their 1x model size counterparts (experiment 2, 3). We suspect that this is due to the fact that they are trained with a reduced learning rate of 3e-4 instead of the default 4e-4 for training stability purpose, which limits the models' training convergence. Also, we might need to tune more hyperparameters for the 2x and 3x model sizes to see their benefit. Due to the limited computation resources, we leave the exploration of larger model sizes as future work.

Data sizeAt 2x training steps with 1x model size, increasing data size in experiments 9 and 10 does not help model's performance compared to experiment 2. At 3x training steps with 1x model size, experiment 11 with 20x training data outperforms experiment 3, which overfits on 1x training data. By training on 8M _Zeroverse_ objects with sufficient training steps, Experiment 11 is also our overall best model. We also observe that experiment 11 performs similarly as experiment 2, which uses only 400K training data. We hypothesize this is because the benefit of 20x data is limited by the capacity of the 1x model and that training a larger model on 20x data is promising. Due to the limited computing resources, we leave this as future work.

## Appendix D Zeroverse creation details

### Sampling and Compositions of Primitives

We randomly sampled the number of the primitives from 1-to-9, with an unnormalized probability density of [5, 5, 5, 5, 4, 3, 2, 1] accordingly. This sampling strategy samples more shapes with lower number of primitives that can smooth the dataset and increase the LRM training stability. Also, the later augmentations (especially the wireframe conversion and the boolean difference) can possibly introduce more fractions and parts.

For sampled primitives, we will scales their size randomly through each axis of the shape. We first put the shape in a normalized frames (e.g., align each edge of the cube with one of the axis) and randomly sample the scaling factors of each axis independently.

For compositing the scaled primitives, we randomly sample the centers of each primitive in the bounding box \([-1,1]^{3}\), then we randomly sample a rotation of the shape (with three degrees of freedom). The composition is done by simply putting all primitives together to a single shape. We do not consider the connectivity of the synthesized objects. See Fig. 6 for an example of disjoint shape (e.g., the top right one).

### Augmentations

For the height-field augmentations, we constrain the maximal value of the height map to be proportional to the size of the face to avoid over-displacement. For more details, please refer to Xu et al. [100] and their released codebase.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{scaling} & \multicolumn{3}{c}{GSO} & ABO \\ \cline{2-10} id & Training Steps & Model Size & Data Size & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\  & def. 1x, 80K & def. 1x, 300M & def. 2x, 800K & & & & & & \\ \hline
1 & 1x & 1x & 2x & 31.90 & 0.966 & 0.030 & 30.66 & 0.949 & 0.055 \\ \hline
2 & 2x & 1x & 2x & **33.12** & **0.973** & **0.024** & **31.75** & **0.957** & **0.047** \\ \hline \hline \end{tabular}
\end{table}
Table 9: GS-LRM’s scaling experiment results.

For the boolean difference augmentation and wireframe conversion augmentation, we use the function from Blender 4. In details, we use Blender's boolean modifier and solidify modifier to augment the initial shape. We first add a random primitive shape with random size and at a random vertex on the initial shape. Then, we apply the boolean modifier, where the new primitive shape, treated as a cutter, is subtracted from the initial shape. We do not use the torus for the subtraction as it might reduce training stability. Finally, we apply the solidify modifier to the cut shape to add thickness to it. The inside faces of the resulting cut shape will have the same texture as the outside faces.

Footnote 4: https://docs.blender.org/manual/en/latest/copyright.html

We use Blender's wireframe modifier and subdivision modifier to create the wireframe of a primitive shape. We also add randomness to the thickness of the wireframe. The wireframes make up the thin structures that are missing in the initial shapes (Sec. 3.1) and add diversity to _Zeroverse_.

The augmentation is applied randomly with a given probability. We do not apply 'boolean difference' and 'wireframe' augmentations at the same time. This will breaks the independence assumption of different augmentations, but would avoid ultra-complex shapes, which improve training stability of the reconstruction model. In our implementation, we by default take \(0.4\) probability of the 'boolean difference' augmentation, \(0.2\) of the 'wireframe', and \(0.4\) of not applying both (as we want disjoint distribution for 'boolean difference' and 'wireframe' augmentations). The 'height-map' augmentation is applied independently to the above two shape augmentations, and always set at \(0.5\) independently for each surface. The results of other configuration can be found in the stability section (Sec. 5.2).

## Appendix E Data synthesis distributed implementation details

In order to synthesize the massive amount of data, we tackle the synthesis and rendering of 8M shapes in _Zeroverse_ with job parallelization. We run the independent shape synthesis and rendering jobs in parallel on 400 CPU nodes with a total of 38,400 CPU cores. The whole process takes 88 hours, where 5% of the time is spent on shape synthesis and 95% of the time is spent on rendering. Despite the relatively large number of CPU cores, the cost is negligible comparing with the training experiments cost on GPU. The training experiments in the main paper are mostly carried on a subset of 400K of the data. The early exploration of scalability uses the full dataset.

To avoid duplication in job parallelism, we first assign unique uuids and corresponding seeds for each shape to synthesize. Then, given the uuid, the seed, and the seed-induced fixed set of parameters, the synthesis and rendering jobs of each shape is independent from each other. In order to avoid exceeding local disk storage, we regularly upload the synthesized shapes and the rendered images to remote storage (e.g., AWS s3 in our experiment) and free the memory of their local copies. On each CPU node, we run multiple shape synthesis and rendering jobs in parallel to maximize the CPU utilization.

## Appendix F More training stability results

As discussed in Sec. 5.1, adding augmentation substantially boosts _LRM-Zero_'s performance. However, it also makes _Zeroverse_ much more complex and thus _LRM-Zero_'s training unstable, as shown in experiment 2 and 5 in Tab. 10. We explore various techniques to help stabilize the training. First, we adjust the perceptual loss weight. Compared to GS-LRM [107] trained on Objaverse [23] and _LRM-Zero_ trained on no-augmentation _Zeroverse_, boolean augmented _Zeroverse_ objects have high perceptual loss magnitudes that causes the excessive gradient norm. In experiment 2, we observe unusual, excessive gradient norm values in the range of 2-5. By reducing perceptual loss weight from 0.5 to 0.2 in experiment 3, the gradient norm values drop to the reasonable range of 0-1. However, experiment 3 still failed due to gradient norm explosion later.

Suspecting that boolean-augmented objects added too much dataset complexity, we reduced their ratio while increased the ratio no-augmentation objects in experiment 5 but still ended up with gradient norm explosion. Then, while keeping the same training dataset, we experimented with two techniques to further stabilize the training, i.e. reducing the Gaussian's scale clipping and the view angle threshold between the sampled views, in experiment 6, 7, and 8. Both techniques turn out to allow model to train stably for the full training steps. However, we notice that they also reduce the model's training set convergence and testing set performance.

We shift the data distribution of _Zeroverse_ to include more easy data and less complex boolean-augmented shapes. In experiment 9, 10, and 11, we find that training stabilizing techniques constraints the model's training set convergence and testing set performance. Experiment 11 further shows that by reducing the ratio of boolean-augmented complex shapes, we can stably train _LRM-Zero_. In our final version of _Zeroverse_, we aim to reduce the dataset complexity in order to achieve stable training from the data distribution side to avoid the convergence constraints that Gaussian's scale clipping or view angle threshold entails. In our empirical observation of the training data, we notice that adding boolean augmentation to objects with many basic shapes can be over complex. In experiment 12, which has the same data distribution as our final version of _Zeroverse_, we adopt a more even, smooth distribution between no-aug, boolean, and wireframe augmented objects and an easier basic-shape-number distribution to favor less basic shapes per object. It performs similarly as experiment 8 and 11. Additionally, in experiment 13, when reducing the ratio of data with boolean difference augmentation, we do not have training instability issues with the default training hyperparameters from GS-LRM, including the 0.5 perceptual loss weight.

## Appendix G Additional Experiments

We conduct additional experiments to seek more explanations on the performance gap between _LRM-Zero_ and GS-LRM. We perform an experiment on GS-LRM, training it on a randomly sampled subset of 200K Objaverse objects. As shown in Tab. 11, GS-LRM's performance only drops by 0.1 PSNR on GSO. We conduct joint training on both Objaverse and _Zeroverse_ and compare it to training only on one of the two datasets in Tab. 12. Our result shows that training on both Objaverse and Zeroverse performs better than Zeroverse only, but worse than Objaverse only. Both of these experiments likely indicate that for the single-object reconstruction task, the results start to saturate with about 200K realistic data. Since the size of Objaverse is adequate for the single object reconstruction task, and the advantages of Zeroverse are not exploited in this task. However, we believe that the advantages

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & \multicolumn{3}{c}{dataset} & \multicolumn{3}{c}{training} & \multicolumn{2}{c}{result} \\ \cline{2-7} \multirow{-2}{*}{id} & \multirow{-2}{*}{hf-only} & \multirow{-2}{*}{boolean} & \multirow{-2}{*}{wireframe} & perceptual & Gaussian scale & view angle & GSO PSNR, \\  & & & loss weight & clipping & threshold & if finished \\  & & & & (default 0.5) & (default -1.2) & (default 60) & \\ \hline
1 & 100\% & 0\% & 0\% & default & default & default & 29.54 \\ \hline
2 & & & & default & default & default & failed \\ \hline
3 & 20\% & 80\% & 0\% & 0.2 & default & default & failed \\ \hline
4 & & & & 0 & default & default & failed \\ \hline
5 & & & & 0.2 & default & default & failed \\ \hline
6 & 40\% & 60\% & 0\% & 0.2 & -1.6 & 40 & 30.32 \\ \hline
7 & & & & 0.2 & -1.6 & default & 30.42 \\ \hline
8 & & & & 0.2 & default & 40 & 30.85 \\ \hline
9 & & & & 0.2 & -1.6 & 40 & 30.14 \\ \hline
10 & 92\% & 8\% & 0\% & 0.2 & -1.6 & default & 30.46 \\ \hline
11 & & & & 0.2 & default & default & 30.86 \\ \hline
12 & 40\% & 40\% & 20\% & 0.2 & default & default & 30.78 \\ \hline
13 & 85\% & 10\% & 5\% & default & default & default & 30.62 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Illustrating the training stability issues when constructing the procedural _Zeroverse_ dataset. The instability can be resolved either with training stabilizing techniques (e.g., reducing perceptual loss weight, Gaussian scale clipping, and view angle threshold), or with reducing the complexity of _Zeroverse_. ‘failed’ experiments are usually due to model divergence.

of _Zeroverse_, i.e. the data size, texture quality, controllability are more valuable when extended to other tasks, such as scene reconstruction and relighting, where data is scarse.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{scaling} & \multicolumn{3}{c}{GSO} & ABO \\ \cline{2-10} data & Training Steps & Model Size & Data Size & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\  & def. 1x, 80K & def. 1x, 300M def. 2x, 800K & & & & & & \\ \hline
1 & 1x & 1x & 2x & **29.59** & **0.944** & **0.050** & **28.92** & **0.926** & **0.074** \\ \hline
2 & 1x & 1x & 0.5x & 29.42 & 0.942 & 0.052 & 28.75 & 0.924 & 0.075 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Scaling down GS-LRM’s training data size. When training on only 200K instead of 800K Obiayerse data, GS-LRM’s performance drops by only 0.1 PSNR on GSO.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{scaling} & \multicolumn{3}{c}{GSO} & ABO \\ \cline{2-10} id & Training Steps & Model Size & Data Size & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\  & def. 1x, 80K & def. 1x, 300M def. 2x, 800K & & & & & & & \\ \hline
1 & 1x & 1x & 2x & **29.59** & **0.944** & **0.050** & **28.92** & **0.926** & **0.074** \\ \hline
2 & 1x & 1x & 0.5x & 29.42 & 0.942 & 0.052 & 28.75 & 0.924 & 0.075 \\ \hline \hline \end{tabular}
\end{table}
Table 12: _LRM-Zero vs. LRM-Zero-Obja_ vs. GS-LRM at the 8-input-view, 256 resolution setting. Z means Zeroverse and O means Objaverse. The _LRM-Zero_ (first row) and GS-LRM (second row) results are from experiment 9 in Tab. 8 and experiment 2 in Tab. 9. The _LRM-Zero-Obja_ result (third row) is obtained by training on 800K _Zeroverse_ data and 800K Obiayerse data. While _LRM-Zero-Obja_ outperforms _LRM-Zero_, it underperforms GS-LRM.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main focus of this paper is to explore the possibility to train feed-forward reconstruction model synthetic data. This goal aligns with the title, abstract, and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see the Limitation Section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: [NA] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper proposes a feed-forward reconstruction model trained on synthetic data. For the model training, the paper follows the experimental setup (e.g., hyper-parameters) in Zhang et al. [107]. We illustrate some key implementations but would refer the readers to the original paper for clarity. For the synthetic data, the paper tries the best to cover all the implementation details. Also, to fully support reproducing, the authors will release the data generation script for a concrete specification of the method. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper will release the synthetic data generation script, which is the main blocker for reproducing. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training and test follows Zhang et al. [107]. We describe most of the details in this paper and would refer readers to Zhang et al. [107] for clarity. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: As each experiment takes significant amount of compute resource, we don't have enough computing resources to complete the significance test. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, provide both data creation and model training. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper is written on our own and follows the honest codes. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to the main paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited all previous papers that lead to our current paper's presentation. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release assets in the traditional meaning (e.g., labeled data). For the synthetic data creation, we have documented them well and will release the code to specify the process. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.