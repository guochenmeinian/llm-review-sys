# A Kernel Perspective on

Distillation-based Collaborative Learning

 Sejun Park   Kihun Hong   Ganguk Hwang

Department of Mathematical Sciences

Korea Advanced Institute of Science and Technology

{sejunpark, nuri9911, guhwang}@kaist.ac.kr

Corresponding author

###### Abstract

Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.

## 1 Introduction

Collaborative learning of AI models in decentralized settings is an important problem covered in various fields of machine learning such as distributed learning [10, 70], Federated Learning (FL) [23], peer-to-peer learning [3], and miscellaneous collaborative learning [47]. In particular, this theme has been most actively discussed in the context of FL [24, 31, 45, 61]. In this context, each local party is typically viewed as a subordinate entity within the collective learning system. For example, most FL algorithms mandate the exchange of local AI model information among participating local parties. Under this scheme, local AI models are usually subjected to restrictions in their architecture. However, from the perspective of collaboration, each local party may have to be regarded as an independent learning agent, meaning they are not obligated to fully share their model information. In short, the model (or parameter) exchange in FL algorithms can emerge as a critical issue in collaborative learning.

Fundamentally, addressing this issue necessitates an alternative medium for sharing learning information distinct from model exchange. Indeed, Distillation-based Collaborative Learning (DCL) [14, 30, 44] provides a good answer. In these algorithms, local training information is shared via the outcomes of AI models on additional unlabeled public data. The collected information is then utilized for knowledge distillation [21] to each local AI model. As mentioned in [14, 48], this procedure is agnostic to model heterogeneity and avoids the direct sharing of local AI model information. This is a key advantage that distinguishes DCL from traditional FL.

Despite its pioneering nature and potential utility, DCL has not been sufficiently explored. A significant reason for this is the lack of theoretical understanding regarding knowledge distillation and its effectiveness in massively distributed statistically heterogeneous environments. Our work stems from the fundamental question of whether DCL algorithms can be theoretically effective in these settings. Inspired by [48; 58], we analyze FedMD [30; 41], the most standard DCL algorithm from a nonparametric perspective. Specifically, we adopt an operator-theoretic approach [5; 15; 37; 53; 65] to obtain an upper rate of convergence for the nonparametric version of FedMD (named **DCL-KR**) in the expected sense. Remarkably, our analysis reveals that DCL-KR achieves a nearly minimax optimal convergence rate, where the prefactor is independent of the number of participating local parties. It is worth noting that DCL-KR is the first nearly minimax optimal collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. The novelty of our theoretical results and their comparison to prior works are provided in Section 2 and 3.

Nevertheless, our theoretical analysis does not fully demonstrate the efficacy of DCL algorithms based on neural network architectures. Instead, our theoretical results serve as inspiration for designing a novel DCL algorithm for regression that refines existing approaches. Consequently, we propose a Distillation-based Collaborative Learning algorithm over heterogeneous Neural Networks (named **DCL-NN**) for regression tasks. DCL-NN leverages kernel matching to align the feature kernels from the last hidden layer of each local AI model with an ensemble kernel. This procedure brings heterogeneous neural networks into the regime of DCL-KR.

Finally, we conduct experiments on DCL-KR and DCL-NN. To illustrate the superiority of our algorithms, we compare them with several baselines on various regression tasks. Experimental results show that DCL-KR achieves the same performance as the centralized model, even beyond the theoretical results. We also observe that DCL-NN significantly outperforms previous DCL frameworks in most settings.

In summary, our contributions are as follows:

1. In Section 3, we theoretically prove that a nonparametric version of the most standard distillation-based collaborative learning algorithm (named DCL-KR) is nearly minimax optimal in massively distributed statistically heterogeneous environments.
2. Inspired by the results provided in Section 3, we propose a distillation-based collaborative learning algorithm with heterogeneous neural networks (named DCL-NN) in Section 4.
3. In Section 5, we conduct experiments to empirically confirm our theoretical results and show the practical feasibility of our proposed algorithms.

## 2 Related Work

Federated LearningMost FL algorithms [45] communicate model parameters for collaboration. This approach has been extensively studied under various constraints, including data privacy [1], statistical heterogeneity [24; 31], communication efficiency [51], personalization [13; 59], and robustness [25]. While it has been successful both theoretically and experimentally, this type of FL is limited in terms of the privacy and flexibility of local AI models, as the algorithms directly access the structures and parameters of the local models. Our study focuses on distillation-based collaborative learning, where the privacy and flexibility of local AI models are fully guaranteed.

Distillation-based Collaborative (or Federated) LearningThe type of algorithms we investigate operates by communicating the functional information of local AI models. These algorithms typically assume the availability of additional public data points. In this case, the outcomes of local models on the public dataset are used for collaboration. For instance, Li and Wang [30], Lin et al. [41], Park et al. [48] iteratively collect predictions of local models on the public dataset and then aggregate them into a naive ensemble (with or without a fixed linear transformation) to distribute. On the other hand, Cho et al. [7], Zhang et al. [69], Fan et al. [14] apply personalized ensemble strategies by additionally learning the mutual trust between models. Makhija et al. [44] propose FedHeNN, which distills training information in the form of matching feature kernels instead of the predictions of local AI models on the public data. Both FedHeNN and DCL-NN utilize centered kernel alignment [8] to match feature kernels of local models, but DCL-NN uses the ensemble distillation for predictions as well. Thus, DCL-NN enables parties to learn from the entire input space.

Decentralized Learning with Kernel RegressionA number of studies have investigated the minimax optimal rate of regularized kernel regression algorithms such as kernel ridge regression and gradient descent-based kernel regression with early stopping [5; 15; 37; 65]. In particular, over the past decade, the growing interest in decentralized learning has led to active research in the generalization analysis of decentralized kernel regression. While divide-and-conquer algorithms [34; 38; 66; 70] play a significant role in this research flow, most of them fail to account for statistical heterogeneity and massively distributed cases, along with privacy preservation, which has received a lot of attention recently. On the other hand, decentralized kernel regression algorithms with multiple communication rounds [40; 43; 48; 58; 67] achieve superior theoretical results compared to the divide-and-conquer algorithms. However, the discussions of these algorithms primarily focus on the efficiency of resource costs [40; 43; 67], while research on relaxing environmental constraints has been scarce. For example, most of these works assume a limited number of parties to prove the optimality in a minimax sense.

To the best of our knowledge, [48; 58] stand as the only investigations that consider general decentralized environments. Similar to our work, Park et al. [48] study the convergence rate of distillation-based collaborative learning with kernel regression. However, their results demonstrate a weaker version of minimax optimality and do not cover statistically heterogeneous environments. In this regard, Su et al. [58] offer a promising methodology. They analyze nonparametric versions of FedAvg [45] and FedProx [31], representative FL algorithms involving model exchange, in general decentralized environments such as statistically heterogeneous and massively distributed scenarios. In this work, we extend their methodology to analyze FedMD [30; 41] from a nonparametric perspective in massively distributed statistically heterogeneous environments. We summarize the comparison between our work and prior studies in Table 1. Note that algorithms that do not employ Nystrom scheme (including nonparametric FedAvg [58]) fail to preserve local data privacy due to the inherent characteristics of kernel regression. On the other hand, DC-NY [66] and DKRR-NY-CM [67] can achieve the local data privacy preservation by utilizing the public data as Nystrom centers.

## 3 DCL-KR: A Nonparametric View of FedMD

In this section, we establish the theory of a nonparametric version of FedMD [30; 41], the most standard distillation-based collaborative learning algorithm.

### Preliminaries

Let \(\rho_{\mathbf{x},y}=\rho_{\mathbf{x}}\cdot\rho_{y|\mathbf{x}}\) be a Borel probability measure on \(\mathcal{X}\times\mathbb{R}\) where \(\mathcal{X}\) is a compact subset of \(\mathbb{R}^{d}\) and we assume the support of \(\rho_{\mathbf{x}}\) is \(\mathcal{X}\). The goal of the regression problem is to find a minimizer of the population risk, i.e.,

\[\min_{h:\mathcal{X}\rightarrow\mathbb{R}}\mathcal{E}(h),\qquad\mathcal{E}(h) :=\frac{1}{2}\ \mathbb{E}_{(\mathbf{x},y)\sim\rho_{\mathbf{x},y}}|y-h(\mathbf{x})|^{2}.\]

Then, the function \(f_{0}^{*}:\mathcal{X}\rightarrow\mathbb{R}\) defined by \(\mathbf{x}_{0}\mapsto\mathbb{E}_{y\sim\rho_{y|\mathbf{x}}(\cdot|\mathbf{x}_{ 0})}[y]\), \(\mathbf{x}_{0}\in\mathcal{X}\) is a target function.

\begin{table}
\begin{tabular}{l|c|c c c} \hline \hline  & interaction & local data & massively & non-i.i.d.\& \\ Methods & method & privacy & distributed & unbalanced \\ \hline DKRR [38] & divide-and-conquer & & & & \\ DC-NY [66] & divide-and-conquer & ✓ & & & \\ \hline DKRR-CM [40] & model exchange & & & & \\ DKRR-RF-CM [43] & model exchange & & & & \\ DKRR-NY-CM [67] & model exchange & ✓ & & \\ nFedAvg [58] & model exchange & & ✓ & ✓ \\ \hline IED* [48] & knowledge distillation & ✓ & ✓ & \\ DCL-KR (ours) & knowledge distillation & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparative analysis of decentralized environments for (nearly) minimax optimality of representative collaborative learning algorithms with kernel regression. nFedAvg indicates the nonparametric version of FedAvg in [58]. Note that IED [48] achieves a weaker version of minimax optimality.

Let \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be a Mercer kernel [9] where \(\kappa:=(\sup_{\mathbf{x}\in\mathcal{X}}k(\mathbf{x},\mathbf{x}))^{1/2}<\infty\) and \(\mathbb{H}_{k}\) be a reproducing kernel Hilbert space associated to \(k\). We set \(k_{\mathbf{x}}:=k(\cdot,\mathbf{x})\) and the covariance operator \(T_{k,\nu}:\mathbb{H}_{k}\to\mathbb{H}_{k}\) with respect to any Borel probability measure \(\nu\) on \(\mathcal{X}\) defined as

\[T_{k,\nu}h=\int_{\mathcal{X}}h(\mathbf{x})k_{\mathbf{x}}\ d\nu(\mathbf{x}).\]

Then we can see that \(T_{k,\nu}=\iota_{\nu}^{\top}\iota_{\nu}\) where \(\iota_{\nu}:\mathbb{H}_{k}\to L_{\nu}^{2}\) is a natural embedding, \(L_{\nu}^{2}=L^{2}(\mathcal{X},\nu)\) denotes the \(L^{2}\) space, and a superscript \({}^{\top}\) denotes the adjoint operator of a given operator. We also define the sampling operator \(S_{D}:\mathbb{H}_{k}\to\mathbb{R}^{n}\) by \(h\mapsto[h(\mathbf{x}^{1}),\cdots,h(\mathbf{x}^{n})]^{\top}\) and \(T_{k,X}:=S_{D}^{\top}S_{D}\) when \(D=\{(\mathbf{x}^{1},y^{1}),\cdots,(\mathbf{x}^{n},y^{n})\}\) with \(X=\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) is given. Since \(S_{D}\) depends only on data inputs \(X\), we can define the sampling operator for unlabeled datasets in the same way. See Appendix A.1 for further details.

#### 3.1.1 Kernel Gradient Descent with Early Stopping

Given a dataset \(D=\{(\mathbf{x}^{1},y^{1}),\cdots,(\mathbf{x}^{n},y^{n})\}\) generated from \(\rho_{\mathbf{x},y}\), consider the empirical risk \(\widetilde{\mathcal{E}}_{D}:\mathbb{H}_{k}\to\mathbb{R}\) given by

\[\widetilde{\mathcal{E}}_{D}(h)=\frac{1}{2}\|S_{D}h-\mathbf{y}\|_{2}^{2}\]

where \(\mathbf{y}=[y^{1},\cdots,y^{n}]^{\top}\). Here, \(\|\cdot\|_{2}\) denotes a scaled Euclidean norm \(\|\mathbf{v}\|_{2}=(\frac{1}{n}\sum_{i=1}^{n}\mathbf{v}_{i}^{2})^{1/2}\). From the functional derivative \(\nabla\widetilde{\mathcal{E}}_{D}(h)=S_{D}^{\top}(S_{D}h-\mathbf{y})\), the gradient descent scheme becomes

\[\nu_{1}=0,\quad\nu_{t+1}=\nu_{t}-\eta_{t}S_{D}^{\top}(S_{D}\nu_{t}-\mathbf{y} )\quad(t=1,2,\cdots)\]

where \(\{\eta_{t}\}_{t\in\mathbb{N}}\) is a set of learning rates. In this work, we set \(\eta_{t}=\eta\), \(t\in\mathbb{N}\) for a fixed \(\eta\in(0,1/\kappa^{2})\). Then, a simple calculation gives \(\nu_{t}\to S_{D}^{\top}(S_{D}S_{D}^{\top})^{-1}\mathbf{y}\) as \(t\to\infty\) provided that the operator \(S_{D}S_{D}^{\top}\) is invertible. The limit is known as the minimum norm interpolation [49] of \(D\). Since the interpolation regressor generalizes poorly unless there is no noise [32; 39], early stopping strategies are usually applied to avoid the overfitting issue. With adequate stopping rules, gradient descent-based kernel regression has an optimal rate in a minimax sense [36; 37; 65].

### Dcl-Kr Algorithm

From now on, we consider the setting that there are \(m\) parties and the \(i\)th party has a private local data \(D_{i}=\{(\mathbf{x}_{i}^{j},y_{i}^{j}):j=1,\cdots,n_{i}\}\) for \(i=1,\cdots,m\). Assume that all data \(D=\bigcup_{i=1}^{m}D_{i}\) are i.i.d. with the distribution \(\rho_{\mathbf{x},y}\) but each local dataset does not need to have the same distribution. Let \(Z=\{\mathbf{z}^{1},\cdots,\mathbf{z}^{n_{0}}\}\subset\mathcal{X}\) be the additional public inputs. The goal of all parties is to have their models that perform well on the distribution \(\rho_{\mathbf{x}}\). In other words, each party expects to be able to make good predictions not only for its local data distribution but also for unseen data distribution through collaborative learning.

Similar to [58], we construct a nonparametric version of FedMD (called **DCL-KR**), which is presented in Algorithm 1. In Algorithm 1, \(\mathcal{G}_{i}\) is a one-step local gradient descent update on \(\widetilde{\mathcal{E}}_{D_{i}}\), i.e., \(\mathcal{G}_{i}h=h-\eta S_{D_{i}}^{\top}(S_{D_{i}}h-\mathbf{y}_{i})\) where \(\mathbf{y}_{i}=[y_{i}^{1},\cdots,y_{i}^{n_{i}}]^{\top}\). Similarly, \(\tilde{\mathcal{G}}_{t}\) is a one-step gradient descent update on \(\widetilde{\mathcal{E}}_{(Z,\bm{y}_{p,t})}\), i.e., \(\tilde{\mathcal{G}}_{t}h=h-\eta S_{Z}^{\top}(S_{Z}h-\mathbf{y}_{p,t})\).

### Theoretical Results

In this subsection, we show the nearly minimax optimality of DCL-KR. To derive theoretical results, we assume the following conditions regarding regularity of noise, the kernel \(k\), and the target function \(f_{0}^{*}\) as below.

**Assumption 3.1**.: We assume \(\mathbb{E}_{y\sim\rho_{y}}y^{2}<\infty\) and

\[\int\left(\exp\left(\frac{|y-f_{0}^{*}(\mathbf{x})|}{M}\right)-\frac{|y-f_{0}^{* }(\mathbf{x})|}{M}-1\right)\ d\rho_{y|\mathbf{x}}(y|\mathbf{x})\leq\frac{ \gamma^{2}}{2M^{2}},\quad\forall\mathbf{x}\in\mathcal{X}\]

where \(M\) and \(\gamma\) are positive constants.

**Assumption 3.2**.: Let \(\lambda_{1}\geq\lambda_{2}\geq\cdots>0\) be eigenvalues of \(T_{k,\rho_{\mathbf{x}}}\). There are fixed positive constants \(C_{s}\) and \(c_{s}\) such that

\[c_{s}i^{-1/s}\leq\lambda_{i}\leq C_{s}i^{-1/s},\ \forall i\in\mathbb{N}\]

for some \(s\in(0,1)\).

**Assumption 3.3**.: The target function \(f_{0}^{*}\) satisfies

\[f_{0}^{*}\in\left\{h\in\mathbb{H}_{k}:h=T_{k,\rho_{\mathbf{x}}}^{r-1/2}g\ \text{ where }\|g\|_{\mathbb{H}_{k}}\leq R\right\}\]

for some \(r\in[\frac{1}{2},1]\) where \(T_{k,\rho_{\mathbf{x}}}^{r-1/2}\) is the \((r-1/2)\) power of operator \(T_{k,\rho_{\mathbf{x}}}\) and \(R>0\) is a fixed constant. In particular, \(f_{0}^{*}\in\mathbb{H}_{k}\).

The above assumptions determine the minimax lower rate [5] and are standard assumptions in many prior works [5, 15, 33, 35]. In detail,

* Assumption 3.1 implies that the noise is not excessively large. This assumption is a general noise condition that encompasses a wide range of cases. For instance, noise with Bernstein condition such as sub-Gaussian noise satisfies Assumption 3.1.
* Assumption 3.2 is about the eigenvalue decay of \(T_{k,\rho_{\mathbf{x}}}\). From this assumption, one can derive bounds on the effective dimension that is related to covering and entropy number conditions [15].
* Assumption 3.3 is related to the regularity of the target function, specifically how well the RKHS induced by the kernel \(k\) represents the target function.

Under these assumptions, we can theoretically show the performance guarantee of DCL-KR. The proof is provided in Appendix A.2. Note that \(\mathcal{E}(h)-\mathcal{E}(f_{0}^{*})=\frac{1}{2}\|\iota_{\rho_{\mathbf{x}}}( h-f_{0}^{*})\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}\) is the excess risk of a regressor \(h\) and so the quantity \(\|\iota_{\rho_{\mathbf{x}}}(h-f_{0}^{*})\|_{L^{2}_{\rho_{\mathbf{x}}}}\) indicates the generalization ability of \(h\).

**Theorem 3.4**.: _Under Assumption 3.1, 3.2, and 3.3, with \(n_{0}\geq n^{\frac{r}{2r+s}}(\log n)^{3}\) public inputs independently generated from \(\tilde{\rho}_{\mathbf{x}}\) such that the Radon-Nikodym derivative \(\frac{d\rho_{\mathbf{x}}}{d\tilde{\rho}_{\mathbf{x}}}\) satisfies_

\[0\leq\frac{d\rho_{\mathbf{x}}}{d\tilde{\rho}_{\mathbf{x}}}\leq B\text{ on }\mathcal{X}\text{ \ for some }B\in[1,\infty),\] (2)

_DCL-KR gives the performance guarantee_

\[\mathbb{E}\|_{\ell_{\rho_{\mathbf{x}}}}(f_{i,T}-f_{0}^{*})\|_{L_{\rho_{ \mathbf{x}}}^{2}}\leq C\cdot B^{r}n^{-\frac{r}{2r+s}}\log n\]

_for all \(i=1,\cdots,m\) where \(\eta\in(0,1/\kappa^{2})\) is a fixed learning rate, \(T\) is an adequate stopping rule, and the prefactor \(C\) does not depend on \(B\), \(m\), and \(n\)._

Since the convergence rate \(n^{-\frac{r}{2r+s}}\) is the minimax lower rate under Assumption 3.1, 3.2, and 3.3, Theorem 3.4 implies that DCL-KR has an almost same convergence rate as the minimax optimal central training when there are sufficiently many public inputs. To the best of our knowledge, this is the first work to prove the (nearly) minimax optimality of a collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. For example, divide-and-conquer algorithms work for limited \(m\). Specifically, DCL-NY [66] assumes \(m\leq O(n^{\frac{2r-1}{2r+s}})\) and DKRR-NY-CM [67] assumes \(m\leq O(n^{\frac{2r+s-1}{2r+s}})\). However, Theorem 3.4 does not require any condition on \(m\). Moreover, Theorem 3.4 deals us more general setting than the theory in [48, 58]. For example, Su et al. [58] only cover \(r=\frac{1}{2}\) of Assumption 3.3. On the other hand, Park et al. [48] do not consider Assumption 3.2 which gives a finer result. Compared with [48], we also reduce the required size of public inputs and drop the statistical homogeneity condition.

The convergence rate in Theorem 3.4 has an additional factor \(\log n\) compared with a minimax lower rate [5, 15], but this logarithm term grows slower than any polynomial. Note that an additional logarithm term commonly appears in the context of gradient descent-based kernel regression with Nystrom scheme [35, 36].

Theorem 3.4 allows that the public input distribution \(\tilde{\rho}_{\mathbf{x}}\) can be different from the local input distribution \(\rho_{\mathbf{x}}\). It is natural that the condition (2) is required since \(\tilde{\rho}_{\mathbf{x}}\) should cover \(\rho_{\mathbf{x}}\) for fully distilling training information. We can see that the discrepancy between \(\rho_{\mathbf{x}}\) and \(\tilde{\rho}_{\mathbf{x}}\) affects the upper bound in Theorem 3.4 as the multiplication of \(B^{r}\). We can remove \(B^{r}\) in the upper bound by increasing public inputs. See Appendix A.3 for details.

#### 3.3.1 Proof Sketch of Theorem 3.4 and Comments

In the proof of Theorem 3.4, we decompose the term \(\iota_{\rho_{\mathbf{x}}}(f_{i,T}-f_{0}^{*})\) into four parts, say (I), (II), (III), and (IV) (see Eq. (7)). The proof is to bound the norms of these terms. Note that DCL-KR can also be understood as a Nystrom version of nonparametric FedAvg [58] from the recurrence relation (6).

(I) and (II) appear similarly in [58], except that (I) and (II) incorporate projections. To handle these terms, we reinterpret the proof presented in [58] in operator form instead of matrix form and extend it to our setting. We obtain a norm bound of (II) containing a quantity linked to the local Rademacher complexity. (Appendix A.2.2 and A.2.3)

Comparing with [58], (III) and (IV) are additional terms induced by the procedure that distills functional information from the local regressors. We apply techniques used in [35, 48, 53] to bound (III) and (IV). (Appendix A.2.4)

Note that previous works applying local Rademacher complexity-based stopping rule [50, 58] deal with the case of \(r=\frac{1}{2}\) only. In this work, we set a new stopping rule \(T\) which is an extension of previous works [50, 58] and prove an extended version (Lemma A.6) of a well-known property [60]. As a result, our theory covers \(r\in[\frac{1}{2},1]\) which affects the minimax lower rate. (Appendix A.2.5)

## 4 DCL-NN Algorithm

In this section, we retain the problem setting from Section 3 but employ heterogeneous neural networks as the local models. Based on the theoretical results in Section 3, we propose a noveldistillation-based collaborative learning algorithm **DCL-NN** across heterogeneous neural networks in a decentralized setting.

A key factor contributing to the successful theoretical guarantee of DCL-KR lies not only in the linearity of kernel regression but also in the equality of kernels across local models. In fact, the public data predictions can vary in different directions, even if the same training data points are used when kernels differ (See Appendix B). Therefore, we match the kernels of local AI models. Specifically, we use linear feature kernels [18; 64] induced by the features from the last hidden layers of local AI models for kernel matching. For example, for a neural network \(f:\mathcal{X}\rightarrow\mathbb{R}\) where \(f(\cdot)=\mathbf{w}^{\top}g(\cdot)+b\), \(g:\mathcal{X}\rightarrow\mathbb{R}^{c}\), \(\mathbf{w}\in\mathbb{R}^{c}\), and \(b\in\mathbb{R}\) we use

\[k_{f}(\mathbf{x}^{1},\mathbf{x}^{2})=g(\mathbf{x}^{1})^{\top}g(\mathbf{x}^{2}),\quad\mathbf{x}^{1},\mathbf{x}^{2}\in\mathcal{X}\] (3)

as the feature kernel of \(f\). Through this idea, we can bring the setting closer to the regime of DCL-KR. Note that our theoretical results suggest that the target kernel should be a good kernel. Indeed, we observe that the naive ensemble

\[k=\sum_{i=1}^{m}\frac{n_{i}}{n}k_{f_{i}}.\] (4)

has a significantly better performance than individual feature kernels \(k_{f_{1}},\cdots,k_{f_{m}}\) (See Section 5 and Appendix B). Here, \(f_{i}\) is the local model of the \(i\)th party with its local feature kernel \(k_{f_{i}}\) obtained by (3) (\(i=1,\cdots,m\)). Therefore, we align local feature kernels \(k_{f_{1}},\cdots,k_{f_{m}}\) in a kernel distillation manner with the ensemble kernel \(k\) obtained by (4).

For this purpose, we introduce Centered Kernel Alignment (CKA) [8] as a kernel similarity measure. CKA is a typical measure associated with the similarity of two representations of neural networks [27] and is often used for kernel matching in neural networks [44]. To compute empirical CKA between two kernels \(k_{1}\) and \(k_{2}\) on inputs \(\{\mathbf{c}^{\dagger},\cdots,\mathbf{c}^{p}\}\), we first calculate the Gram matrices \(K_{1}=[k_{1}(\mathbf{c}^{j_{1}},\mathbf{c}^{j_{2}})]_{1\leq j_{1},j_{2}\leq p}\) and \(K_{2}=[k_{2}(\mathbf{c}^{j_{1}},\mathbf{c}^{j_{2}})]_{1\leq j_{1},j_{2}\leq p}\). We then compute the empirical CKA via

\[\widehat{\text{CKA}}(k_{1},k_{2})=\frac{\widehat{\text{HSIC}}(K_{1},K_{2})}{ \sqrt{\widehat{\text{HSIC}}(K_{1},K_{1})\widehat{\text{HSIC}}(K_{2},K_{2})}}.\]

Here, \(\widehat{\text{HSIC}}\) is an estimator of the Hilbert-Schmidt Independence Criterion (HSIC) defined as

\[\widehat{\text{HSIC}}(K_{1},K_{2})=\frac{1}{(p-1)^{2}}\text{tr}(K_{1}HK_{2}H)\]

where \(H:=I_{p}-\frac{1}{p}\mathbf{1}\mathbf{1}^{\top}\) is the centering matrix. In the kernel distillation procedure, the \(i\)th local party maximizes \(\widehat{\text{CKA}}(k_{f_{i}},k)\) on public inputs \(Z\) (\(i=1,\cdots,m\)). Notably, this procedure requires only a single communication round for exchanging pairwise feature kernel values on public inputs, ensuring that our algorithm operates exclusively within the function space.

After the kernel distillation procedure, all local AI models have similar feature kernels up to constants. So we can follow an analogous process as in DCL-KR. Note that we perform learning rate scaling described in Appendix B to compensate the kernel scale difference. It makes the impact of local iterations consistent. We also provide the complete algorithm (Algorithm 2) and further details for Section 4 in Appendix B.

## 5 Experiments

In this section, we evaluate the performance of DCL-KR and DCL-NN. We compare them with baselines on various regression tasks.

DatasetsWe use the following six regression datasets to evaluate the performance. Target variables are one-dimensional in all datasets. (1) **Toy-1D**[33] and (2) **Toy-3D**[6] are synthetic datasets with one-dimensional and three-dimensional inputs, respectively. (3) **Energy** is a tabular dataset from the UCI database [12] to predict appliances energy use with 28 features. (4) **RotatedMNIST** is an image dataset where it aims to predict the rotation angles for given rotated images of the MNIST [11] images. (5) **UTKFace**[71] and (6) **IMDB-WIKI**[42; 52] are image datasets for age estimation.

We compare kernel machine-based collaborative learning algorithms on two datasets Toy-1D and Toy-3D. On the other hand, we compare neural network-based collaborative learning algorithms on five datasets Toy-3D, Energy, RotatedMNIST, UTKFace, and IMDB-WIKI.

BaselinesWe compare DCL-KR with two central kernel regression models to verify our theoretical results. These two central models have the minimax optimal convergence rate. We also utilize existing decentralized kernel regression algorithms that does not directly share local data and models (DC-NY [66], DKRR-NY-CM [67], IED [48]) as baselines for DCL-KR. On the other hand, we adopt FedMD with unlabeled public inputs [30, 41], FedHeNN [44], and KT-pFL [69] as baselines for DCL-NN.

SetupThe number of parties ranges from 10 to 100 for kernel machine-based algorithms and is 50 for neural network-based algorithms. We construct statistically heterogeneous decentralized environments with Algorithm 3. For neural network-based algorithms, we use 4 different neural network architectures for local models in all settings. For instance, we use ResNet-18, ResNet-34, ResNet-50 [20], and MobileNetv2 [54] for large-scale image datasets. We utilize the average of Root Mean Squared Errors (RMSEs) of the local AI models on a test dataset as a performance metric. The test data points have the same distribution as the whole local data distribution. We apply FedMD with a few communication rounds for pretraining of DCL-NN. See Appendix C for detailed experimental configurations.

### Results on Kernel Machine-based Algorithms

The performance of DCL-KR and its baselines is presented in Figure 1. We set the number of parties \(m=10,20,\cdots,100\), the number of private data points \(n=50m\), and the number of public inputs \(n_{0}=n^{\frac{1}{2r+z}}(\log_{10}n)^{3}\). We first set \(\rho_{\mathbf{x}}=\bar{\rho}_{\mathbf{x}}\), i.e., the public data distribution is the same as the entire local input distribution. As shown in Figure 1, DCL-KR outperforms the baselines in all

Figure 1: Performance of central Kernel Ridge Regression (centralKRR), central Kernel Regression with Gradient Descent (centralKRGD), DC-NY, DKRR-NY-CM, IED, and DCL-KR on Toy-1D and Toy-3D

Figure 2: Performance of IED and DCL-KR with \(n_{0}\approx\alpha\cdot n^{\frac{1}{2r+z}}(\log_{10}n)^{3}\) on Toy-3D

experimental settings and achieves comparable performance to the central models. This result implies that DCL-KR has not only the nearly optimal convergence rate but also the same performance as central kernel regression models. In contrast, DC-NY and DKRR-NY-CM exhibit significantly lower performance compared with DCL-KR in massively distributed environments where their theory does not cover. IED does not show a significant performance drop in massively distributed environments even though its theory is built on the statistical homogeneity condition of local data distributions.

To further compare the performance of DCL-KR and IED, which show similar results to central models, we analyze the effect of \(n_{0}\) and \(\tilde{\rho}_{\mathbf{x}}\) on their performance (Figure 2 and 3). Figure 2 illustrates that, as expected from the theoretical results, IED requires more public inputs than DCL-KR to achieve good performance. Moreover, when there is a public distribution shift, DCL-KR maintains its convergence rate, whereas the convergence rate of IED deteriorates. (See Appendix C.3.3 for experimental details.) Overall, our experiments validate the theoretical results of DCL-KR and demonstrate its superiority over previous results. For additional experimental results and analyses, please refer to Appendix C.3.

### Results on Neural Network-based Algorithms

Table 2 shows the performance of DCL-NN and baselines on five regression tasks. We also present the performance of standalone models and centralized models to assess the performance of the collaborative algorithms. For some cases exhibiting training instability, we report the best test error (marked with asterisks) observed across all communication rounds, while relying on a fixed number of communication rounds for the other cases.

As can be seen in Table 2, DCL-NN outperforms the baselines on all regression tasks. Note that FedHeNN employs kernel matching similar to DCL-NN, but it lacks supervision of label prediction through collaboration, resulting in insufficient performance improvement compared to standalone models. Given the superior performance of DCL-NN, it is evident that incorporating supervised learning for label prediction alongside kernel matching is desirable. On the other hand, while FedMD performs significantly better than standalone models, the performance of DCL-NN is consistently better. Considering that we utilize FedMD for pretraining of DCL-NN, we can see that it performs better than FedMD-only collaborative learning by first training local models with FedMD and then using DCL-NN. In conclusion, the experimental results support the practical effectiveness and superiority of DCL-NN over baselines.

Kernel Distillation ProcedureTo verify the necessity of kernel distillation, we examine the changes in the performance of local feature kernels and the CKA between them during the kernel distillation procedure. We conduct this experiment on UTKFace. We utilize the RMSE of a kernel linear regression model trained on all local data as a kernel performance measure. The results are presented in Figure 4. As shown in Figure 4, both kernel performance and CKA undergo a temporary degradation due to the change of the objective function at the initial stages. However, as training progresses, both metrics recover and kernel performance surpasses its initial level. Since kernel distillation aims to ensure that all local feature kernels are similar with high performance, the experimental results verify the effectiveness of kernel distillation.

For additional experimental results, please refer to Appendix C.4.

## 6 Conclusions

In this work, we analyze distillation-based collaborative learning from a nonparametric perspective and propose DCL-NN, a practical algorithm as an extension. We demonstrate that DCL-KR, a nonparametric version of FedMD, has a nearly minimax optimal convergence rate in massively distributed statistically heterogeneous environments. Inspired by DCL-KR, we propose DCL-NN, a novel distillation-based collaborative learning algorithm for heterogeneous neural networks. Our experiments confirm the theoretical results of DCL-KR and demonstrate the practical effectiveness of DCL-NN. For a discussion of the limitations of our work, please refer to Appendix D.

Broader ImpactOur work explores the methodologies of collaborative learning under data and model privacy preservation. In this regard, our research holds the potential to positively impact the facilitation of collaboration among AI models without raising concerns about information disclosure. On the other hand, our work does not pose any particularly noteworthy negative consequences, given its aim to contribute to the advancement of the general field of machine learning.

## Acknowledgments and Disclosure of Funding

This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (Grant No. RS-2019-NR040050).

## References

* [1] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov. How to backdoor federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2938-2948. PMLR, 2020.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Toy-3D & Energy & RotatedMNIST & UTKFace & IMDB-WIKI \\ \hline Central & 0.041 & 0.085 & 0.139 & 0.143 & 0.095 \\ \hline Standalone & 0.288 \(\pm\) 0.008 & 0.095 \(\pm\) 0.000 & 0.680 \(\pm\) 0.003 & 0.216 \(\pm\) 0.004 & 0.137\(\pm\)0.000 \\ FedMD & 0.200 \(\pm\) 0.008 & 0.093 \(\pm\) 0.000 & 0.249 \(\pm\) 0.001 & 0.151 \(\pm\) 0.004 & 0.113\(\pm\)0.000 \\ FedHeNN & 0.264\({}^{*}\)\(\pm\) 0.009 & 0.094\({}^{*}\)\(\pm\) 0.000 & 0.405 \(\pm\) 0.016 & 0.177 \(\pm\) 0.000 & 0.140\({}^{*}\)\(\pm\)0.000 \\ KT-pFL & 0.243 \(\pm\) 0.002 & 0.093\({}^{*}\)\(\pm\) 0.000 & 0.317 \(\pm\) 0.003 & 0.167 \(\pm\) 0.001 & 0.130\({}^{*}\)\(\pm\)0.002 \\
**DCL-NN** & **0.079 \(\pm\) 0.005** & **0.087 \(\pm\) 0.001** & **0.227 \(\pm\) 0.003** & **0.148 \(\pm\) 0.001** & **0.110\(\pm\)0.000** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison of FedMD, FedHeNN, KT-pFL, and DCL-NN on five datasets. The values are presented as the average of RMSEs along with standard deviations. For calibration, the performance of standalone models and centralized models is also provided.

Figure 4: Kernel performance and CKA (with standard deviations) during the kernel distillation procedure. The performance of the target kernel obtained by (4) is also provided.

* Bartlett et al. [2005] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. _The Annals of Statistics_, 33(4):1497-1537, 2005.
* Bellet et al. [2018] A. Bellet, R. Guerraoui, M. Taziki, and M. Tommasi. Personalized and private peer-to-peer machine learning. In _International Conference on Artificial Intelligence and Statistics_, pages 473-481. PMLR, 2018.
* Bousquet [2003] O. Bousquet. Concentration inequalities for sub-additive functions using the entropy method. In _Stochastic Inequalities and Applications_, pages 213-247. Springer, 2003.
* Caponnetto and De Vito [2007] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7:331-368, 2007.
* Chang et al. [2017] X. Chang, S.-B. Lin, and D.-X. Zhou. Distributed semi-supervised learning with kernel ridge regression. _The Journal of Machine Learning Research_, 18(1):1493-1514, 2017.
* Cho et al. [2023] Y. J. Cho, J. Wang, T. Chirvolu, and G. Joshi. Communication-efficient and model-heterogeneous personalized federated learning via clustered knowledge transfer. _IEEE Journal of Selected Topics in Signal Processing_, 17(1):234-247, 2023.
* Cortes et al. [2012] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. _The Journal of Machine Learning Research_, 13(1):795-828, 2012.
* Cucker and Zhou [2007] F. Cucker and D. X. Zhou. _Learning theory: an approximation theory viewpoint_, volume 24. Cambridge University Press, 2007.
* Dean et al. [2012] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, et al. Large scale distributed deep networks. _Advances in neural information processing systems_, 25, 2012.
* Deng [2012] L. Deng. The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* Dheeru and Karra Taniskidou [2017] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
* Fallah et al. [2020] A. Fallah, A. Mokhtari, and A. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In _Advances in Neural Information Processing Systems_, volume 33, pages 3557-3568, 2020.
* Fan et al. [2023] D. Fan, C. Mendler-Dunner, and M. Jaggi. Collaborative learning via prediction consensus. In _Advances in Neural Information Processing Systems_, 2023.
* Fischer and Steinwart [2020] S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _The Journal of Machine Learning Research_, 21(1):8464-8501, 2020.
* Fujii et al. [1993] J. Fujii, M. Fujii, T. Furuta, and R. Nakamoto. Norm inequalities equivalent to heinz inequality. _Proceedings of the American Mathematical Society_, 118(3):827-830, 1993.
* Guo et al. [2017] Z.-C. Guo, S.-B. Lin, and D.-X. Zhou. Learning theory of distributed spectral algorithms. _Inverse Problems_, 33(7):074009, 2017.
* He and Ozay [2021] B. He and M. Ozay. Feature kernel distillation. In _International Conference on Learning Representations_, 2021.
* He et al. [2020] C. He, M. Annavaram, and S. Avestimehr. Group knowledge transfer: Federated learning of large cnns at the edge. _Advances in Neural Information Processing Systems_, 33:14068-14080, 2020.
* He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* Hinton et al. [2015] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.

* [22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International Conference on Machine Learning_, pages 448-456. PMLR, 2015.
* [23] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [24] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [25] S. P. Karimireddy, L. He, and M. Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In _International Conference on Learning Representations_, 2022.
* [26] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* [27] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In _International Conference on Machine Learning_, pages 3519-3529. PMLR, 2019.
* [28] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [29] M. Ledoux and M. Talagrand. _Probability in Banach Spaces: isoperimetry and processes_. Springer Science & Business Media, 2013.
* [30] D. Li and J. Wang. Fedmd: Heterogenous federated learning via model distillation. _arXiv preprint arXiv:1910.03581_, 2019.
* [31] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine Learning and Systems_, 2:429-450, 2020.
* [32] Y. Li, H. Zhang, and Q. Lin. Kernel interpolation generalizes poorly. _Biometrika_, 2023.
* [33] Y. Li, H. Zhang, and Q. Lin. On the saturation effect of kernel ridge regression. In _International Conference on Learning Representations_, 2023.
* [34] J. Lin and V. Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. _Journal of Machine Learning Research_, 21(147):1-63, 2020.
* [35] J. Lin and L. Rosasco. Optimal rates for learning with Nystrom stochastic gradient methods. _arXiv preprint arXiv:1710.07797_, 2017.
* [36] J. Lin and L. Rosasco. Optimal rates for multi-pass stochastic gradient methods. _The Journal of Machine Learning Research_, 18(1):3375-3421, 2017.
* [37] J. Lin, A. Rudi, L. Rosasco, and V. Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. _Applied and Computational Harmonic Analysis_, 48(3):868-890, 2020.
* [38] S.-B. Lin, X. Guo, and D.-X. Zhou. Distributed learning with regularized least squares. _The Journal of Machine Learning Research_, 18(1):3202-3232, 2017.
* [39] S.-B. Lin, X. Chang, and X. Sun. Kernel interpolation of high dimensional scattered data. _arXiv preprint arXiv:2009.01514_, 2020.
* [40] S.-B. Lin, D. Wang, and D.-X. Zhou. Distributed kernel ridge regression with communications. _Journal of Machine Learning Research_, 21(93):1-38, 2020.
* [41] T. Lin, L. Kong, S. U. Stich, and M. Jaggi. Ensemble distillation for robust model fusion in federated learning. _Advances in Neural Information Processing Systems_, 33:2351-2363, 2020.
* [42] Y. Lin, J. Shen, Y. Wang, and M. Pantic. Fp-age: Leveraging face parsing attention for facial age estimation in the wild. _arXiv_, 2021.

* [43] Y. Liu, J. Liu, and S. Wang. Effective distributed learning with random features: Improved bounds and algorithms. In _International Conference on Learning Representations_, 2020.
* [44] D. Makhija, X. Han, N. Ho, and J. Ghosh. Architecture agnostic federated learning for neural networks. In _International Conference on Machine Learning_, pages 14860-14870. PMLR, 2022.
* [45] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas. Communication-efficient learning of deep networks from decentralized data. In _International Conference on Artificial Intelligence and Statistics_, pages 1273-1282. PMLR, 2017.
* [46] S. Mendelson. Geometric parameters of kernel machines. In _International Conference on Computational Learning Theory_, pages 29-43. Springer, 2002.
* [47] C. Mendler-Dunner, W. Guo, S. Bates, and M. Jordan. Test-time collective prediction. _Advances in Neural Information Processing Systems_, 34:13719-13731, 2021.
* [48] S. Park, K. Hong, and G. Hwang. Towards understanding ensemble distillation in federated learning. In _International Conference on Machine Learning_, pages 27132-27187. PMLR, 2023.
* [49] V. I. Paulsen and M. Raghupathi. _An introduction to the theory of reproducing kernel Hilbert spaces_, volume 152. Cambridge University Press, 2016.
* [50] G. Raskutti, M. J. Wainwright, and B. Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. _The Journal of Machine Learning Research_, 15(1):335-366, 2014.
* [51] D. Rothchild, A. Panda, E. Ullah, N. Ivkin, I. Stoica, V. Braverman, J. Gonzalez, and R. Arora. FetchSGD: Communication-efficient federated learning with sketching. In _International Conference on Machine Learning_, pages 8253-8265. PMLR, 2020.
* [52] R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and apparent age from a single image without facial landmarks. _International Journal of Computer Vision_, 126(2-4):144-157, 2018.
* [53] A. Rudi, R. Camoriano, and L. Rosasco. Less is more: Nystrom computational regularization. _Advances in Neural Information Processing Systems_, 28, 2015.
* [54] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4510-4520, 2018.
* [55] R. Schaback and H. Wendland. Kernel techniques: from machine learning to meshless methods. _Acta Numerica_, 15:543-639, 2006.
* [56] B. Sen. A gentle introduction to empirical process theory and applications. _Lecture Notes, Columbia University_, 11:28-29, 2018.
* [57] I. Steinwart and A. Christmann. _Support vector machines_. Springer Science & Business Media, 2008.
* [58] L. Su, J. Xu, and P. Yang. A non-parametric view of fedavg and fedprox: Beyond stationary points. _The Journal of Machine Learning Research_, 24(203):1-48, 2023.
* [59] C. T Dinh, N. Tran, and J. Nguyen. Personalized federated learning with moreau envelopes. _Advances in Neural Information Processing Systems_, 33:21394-21405, 2020.
* [60] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [61] H. Wang, M. Yurochkin, Y. Sun, D. S. Papailiopoulos, and Y. Khazaeni. Federated learning with matched averaging. In _International Conference on Learning Representations_, 2020.
* [62] C. K. Williams and C. E. Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.

* [63] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Deep kernel learning. In _International Conference on Artificial Intelligence and Statistics_, pages 370-378. PMLR, 2016.
* [64] G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _International Conference on Machine Learning_, pages 11727-11737. PMLR, 2021.
* [65] Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. _Constructive Approximation_, 26:289-315, 2007.
* [66] R. Yin, Y. Liu, L. Lu, W. Wang, and D. Meng. Divide-and-conquer learning with nystrom: Optimal rate and algorithm. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 6696-6703, 2020.
* [67] R. Yin, W. Wang, and D. Meng. Distributed nystrom kernel learning with communications. In _International Conference on Machine Learning_, pages 12019-12028. PMLR, 2021.
* [68] H. Zhang, Y. Li, and Q. Lin. On the optimality of misspecified spectral algorithms. _arXiv preprint arXiv:2303.14942_, 2023.
* [69] J. Zhang, S. Guo, X. Ma, H. Wang, W. Xu, and F. Wu. Parameterized knowledge transfer for personalized federated learning. _Advances in Neural Information Processing Systems_, 34:10092-10104, 2021.
* [70] Y. Zhang, J. Duchi, and M. Wainwright. Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates. _The Journal of Machine Learning Research_, 16(1):3299-3340, 2015.
* [71] Z. Zhang, Y. Song, and H. Qi. Age progression/regression by conditional adversarial autoencoder. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5810-5818, 2017.
* [72] Z. Zhu, J. Hong, and J. Zhou. Data-free knowledge distillation for heterogeneous federated learning. In _International conference on machine learning_, pages 12878-12889. PMLR, 2021.

## Appendix A Details on Section 3

Before we start the proof of Theorem 3.4, we present basic notions.

### Basic Notions

In Subsection 3.1, the reproducing kernel Hilbert space \(\mathbb{H}_{k}\) is a subset of \(C(\mathcal{X})\), i.e., all elements in \(\mathbb{H}_{k}\) are continuous [57]. Since

\[\iota_{\rho_{\mathbf{x}}}^{\top}h(\cdot)=\langle\iota_{\rho_{\mathbf{x}}}^{ \top}h,k.\rangle_{\mathbb{H}_{k}}=\langle h,\iota_{\rho_{\mathbf{x}}}k.\rangle _{L^{2}_{\rho_{\mathbf{x}}}}=\int_{\mathcal{X}}h(\mathbf{x})k(\cdot,\mathbf{x })\ d\rho_{\mathbf{x}}(\mathbf{x}),\]

\begin{table}
\begin{tabular}{c|l} \hline \hline Notation & Meaning \\ \hline \(a\wedge b\) & minimum of \(a\) and \(b\) \\ \(a\lor b\) & maximum of \(a\) and \(b\) \\ \(\mathbb{R}^{d}\) & \(d\)-dimensional Euclidean space \\ \(\mathcal{X}\) & the input space contained in \(\mathbb{R}^{d}\) \\ \(C(\mathcal{X})\) & the collection of all continuous functions from \(\mathcal{X}\) into \(\mathbb{R}\) \\ \(\rho_{\mathbf{x},y}\) & the data generating distribution on \(\mathcal{X}\times\mathbb{R}\) \\ \(\rho_{\mathbf{x}},\rho_{y}\) & the marginal distribution of \(\rho_{\mathbf{x},y}\) on \(\mathcal{X}\) and \(\mathbb{R}\), respectively \\ \(\rho_{y|\mathbf{x}}(\cdot|\mathbf{x}_{0})\) & the conditional distribution on \(\mathbb{R}\) w.r.t. \(\mathbf{x}_{0}\in\mathcal{X}\) and \(\rho_{\mathbf{x},y}\) \\ \(\tilde{\rho}_{\mathbf{x}}\) & the public input distribution on \(\mathcal{X}\) \\ \(k\) & a given Mercer kernel \\ \(k_{\mathbf{x}}\) & \(k(\cdot,\mathbf{x})\) \\ \(\kappa\) & \((\sup_{\mathbf{x}\in\mathcal{X}}k(\mathbf{x},\mathbf{x}))^{1/2}\) \\ \(L^{2}_{\nu}\) & the \(L^{2}\) space on \(\mathcal{X}\) w.r.t. measure \(\nu\) \\ \(\mathbb{H}_{k}\) & a reproducing kernel Hilbert space associated to kernel \(k\) \\ \(T_{k,\nu}\) & the covariance operator on \(\mathbb{H}_{k}\) w.r.t. measure \(\nu\), \(T_{k,\nu}:h\mapsto\int_{\mathcal{X}}h(\mathbf{x})k_{\mathbf{x}}\ d\nu(\mathbf{x})\) \\ \(\iota_{\nu}\) & a natural embedding from \(\mathbb{H}_{k}\) into \(L^{2}_{\nu}\) \\ \(S_{D}\) & a sampling operator from \(\mathbb{H}_{k}\) into \(\mathbb{R}^{n}\), \\  & \(S_{D}:h\mapsto[h(\mathbf{x}^{1}),\cdots,h(\mathbf{x}^{n})]^{\top}\) where \(D=\{(\mathbf{x}^{i},y^{i})\}_{i=1}^{n}\) \\ \(S_{Z}\) & a sampling operator from \(\mathbb{H}_{k}\) into \(\mathbb{R}^{n}\), \\  & \(S_{Z}:h\mapsto[h(\mathbf{z}^{1}),\cdots,h(\mathbf{z}^{n})]^{\top}\) where \(Z=\{\mathbf{z}^{i}\}_{i=1}^{n}\) \\ \(T_{k,X}\) & \(S_{X}^{\top}S_{X}\) \\ \(\mathcal{E}(h)\) & the population risk of \(h\), \(\mathcal{E}(h)=\frac{1}{2}\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\rho_{ \mathbf{x},y}}|y-h(\mathbf{x})|^{2}\) \\ \(\widetilde{\mathcal{E}}_{D}(h)\) & the empirical risk of \(h\) over \(D=\{(\mathbf{x}^{i},y^{i})\}_{i=1}^{n}\), \(\widetilde{\mathcal{E}}_{D}(h)=\frac{1}{2}\|S_{D}h-\mathbf{y}\|_{2}^{2}\) \\ \(f_{0}^{*}\) & a target function from \(\mathcal{X}\) into \(\mathbb{R}\) defined as \(f_{0}^{*}(x_{0})=\mathbb{E}_{y\sim\rho_{y|\mathbf{x}}(\cdot|\mathbf{x}_{0})} [y]\), \(\mathbf{x}_{0}\in\mathcal{X}\) \\ \(\eta\) & a learning rate, \(\eta\in(0,1/\kappa^{2})\) \\ \(m\) & the number of parties \\ \(D_{i}\) & private local data of the \(i\)th party, \(\{(\mathbf{x}_{i}^{j},y_{i}^{j}):j=1,\cdots,n_{i}\}\) (\(i=1,\cdots,m\)) \\ \(X_{i}\) & inputs of \(D_{i}\), \(\{\mathbf{x}_{i}^{j}:j=1,\cdots,n_{i}\}\) (\(i=1,\cdots,m\)) \\ \(\mathbf{y}_{i}\) & labels of \(D_{i}\), \([y_{i}^{1},\cdots,y_{i}^{n_{i}}]^{\top}\) (\(i=1,\cdots,m\)) \\ \(D\) & \(D=\bigcup_{i=1}^{m}D_{i}\) \\ \(Z\) & unlabeled public data, \(\{\mathbf{z}^{1},\cdots,\mathbf{z}^{n_{0}}\}\) \\ \(M,\gamma\) & the parameters related to the regularity of noise (Assumption 3.1) \\ \(\{(\lambda_{i},\phi_{i})\}_{i=1}^{\infty}\) & eigenvalues and eigenvectors of \(T_{k,\rho_{\mathbf{x}}}\) such that \(\lambda_{1}\geq\lambda_{2}\geq\cdots>0\) from Mercer’s representation (5). \\ \(s,C_{s},c_{s}\) & the parameters related to the eigenvalue decay of \(T_{k,\rho_{\mathbf{x}}}\) (Assumption 3.2) \\ \(C_{s}^{\prime}\) & \(C_{s}^{*}/(1-s)\) \\ \(\mathcal{N}_{\nu}(\lambda)\) & \(\text{tr}(T_{k,\nu}(T_{k,\nu}+\lambda I)^{-1})\) where \(\nu\) is a probability measure \\ \(\mathcal{N}(\lambda)\) & \(\mathcal{N}_{\rho_{\mathbf{x}}}(\lambda)\) \\ \(r,R\) & the parameters related to the regularity of \(f_{0}^{*}\) (Assumption 3.3) \\ \(B\) & the uniform bound of the Radon-Nikodym derivative \(\frac{d\rho_{\mathbf{x}}}{d\rho_{\mathbf{x}}}\) in (2) \\ \(E\) & the number of local iterations at each communication round in DCL-KR (Algorithm 1) \\ \(T\) & total communication round in DCL-KR (Algorithm 1) \\ \hline \hline \end{tabular}
\end{table}
Table 3: List of some notationswe have \(T_{k,\rho_{\mathbf{x}}}=\iota_{\rho_{\mathbf{x}}}^{\top}\iota_{\rho_{\mathbf{x}}}\). The compactness of \(\iota_{\rho_{\mathbf{x}}}^{\top}\)[57] gives the fact that \(T_{k,\rho_{\mathbf{x}}}\) is compact, self-adjoint, and positive. Furthermore, Mercer's theorem [62] gives a Mercer representation

\[k(\mathbf{x}^{1},\mathbf{x}^{2})=\sum_{i=1}^{\infty}\lambda_{i}\phi_{i}( \mathbf{x}^{1})\phi_{i}(\mathbf{x}^{2}).\] (5)

The fact that \(\mathbb{H}_{k}\subset C(\mathcal{X})\) and \(T_{k,\rho_{\mathbf{x}}}=\iota_{\rho_{\mathbf{x}}}^{\top}\iota_{\rho_{\mathbf{x}}}\) implies the injectivity of \(T_{k,\rho_{\mathbf{x}}}\) and so \(\lambda_{i}\neq 0\) for all \(i\in\mathbb{N}\). We define

\[\mathcal{N}_{\nu}(\lambda):=\text{tr}(T_{k,\nu}(T_{k,\nu}+\lambda I)^{-1})\]

for any probability measure \(\nu\). For convenience, \(\mathcal{N}(\lambda):=\mathcal{N}_{\rho_{\mathbf{x}}}(\lambda)\). From [5, 15], we have \(\mathcal{N}(\lambda)\leq C_{s}^{\prime}\lambda^{-s}\) where \(C_{s}^{\prime}:=C_{s}^{s}/(1-s)\) and \(\mathcal{N}_{\nu}(\lambda)\leq\kappa^{2}\lambda^{-1}\). Given a dataset \(D=\{(\mathbf{x}^{i},y^{i})\}_{i=1}^{n}\), a similar argument as above gives

\[S_{D}^{\top}:\mathbf{c}=[\mathbf{c}_{1},\cdots,\mathbf{c}_{n}]\mapsto\frac{1} {n}\sum_{i=1}^{n}\mathbf{c}_{i}k_{\mathbf{x}^{i}}\]

and \(T_{k,X}:h\mapsto\frac{1}{n}\sum_{i=1}^{n}h(\mathbf{x}^{i})k_{\mathbf{x}^{i}}\).

Note that Assumption 3.1 implies that

\[\mathcal{E}(f_{0}^{*})=\frac{1}{2}\mathbb{E}_{(\mathbf{x},y)\sim\rho_{\mathbf{ x},y}}|y-f_{0}^{*}(\mathbf{x})|^{2}\leq\frac{\gamma^{2}}{2}<\infty.\]

We have

\[\mathbb{E}_{(\mathbf{x},y)\sim\rho_{\mathbf{x},y}}|y-h(\mathbf{x})|^{2}= \mathbb{E}_{\mathbf{x}\sim\rho_{\mathbf{x}}}|h(\mathbf{x})-f_{0}^{*}(\mathbf{ x})|^{2}+\mathbb{E}_{(\mathbf{x},y)\sim\rho_{\mathbf{x},y}}|y-f_{0}^{*}(\mathbf{x})|^{2}\]

and so the excess risk becomes

\[\mathcal{E}(h)-\mathcal{E}(f_{0}^{*})=\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim \rho_{\mathbf{x}}}|h(\mathbf{x})-f_{0}^{*}(\mathbf{x})|^{2}=\frac{1}{2}\|\iota _{\rho_{\mathbf{x}}}(h-f_{0}^{*})\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}.\]

Therefore, \(\|\iota_{\rho_{\mathbf{x}}}(h-f_{0}^{*})\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}\) indicates the generalization ability of \(h\).

Table 3 presents meaning of some notations.

### Proof of Theorem 3.4

Without loss of generality, we assume \(n\wedge n_{0}\geq\kappa^{2}e\).

#### a.2.1 Recurrence Relation of DCL-KR

Consider a subspace \(W\) of \(\mathbb{H}_{k}\) spanned by \(\{k_{\mathbf{z}},\cdots,k_{\mathbf{z}^{n_{0}}}\}\). We first show that for a fixed \(h^{*}\in\mathbb{H}_{k}\) and a gradient update \(\mathcal{G}u=u-\eta S_{Z}^{\top}(S_{Z}u-S_{Z}h^{*})\) we have \(\mathcal{G}^{t}u_{1}\to P_{Z}h^{*}\) as \(t\to\infty\) for any \(u_{1}\in W\) where \(P_{Z}\) is an orthogonal projection onto the subspace \(W\). Set \(u_{t+1}=\mathcal{G}u_{t}\) for \(t\geq 1\). Then

\[u_{t+1}=(I-\eta S_{Z}^{\top}S_{Z})u_{t}+\eta S_{Z}^{\top}S_{Z}h^{*}=(I-\eta S_ {Z}^{\top}S_{Z})^{t}u_{1}+\sum_{k=0}^{t-1}(I-\eta S_{Z}^{\top}S_{Z})^{k}\eta S_ {Z}^{\top}S_{Z}h^{*}.\]

Since \(S_{Z}h^{*}=S_{Z}P_{Z}h^{*}\), we have

\[\sum_{k=0}^{t-1}(I-\eta S_{Z}^{\top}S_{Z})^{k}\eta S_{Z}^{\top}S_{Z}h^{*}=\sum_{ k=0}^{t-1}(I-\eta S_{Z}^{\top}S_{Z})^{k}\eta S_{Z}^{\top}S_{Z}P_{Z}h^{*}=P_{Z}h^{*}-(I- \eta S_{Z}^{\top}S_{Z})^{t}P_{Z}h^{*}.\]

Note that there exists \(\{\tilde{\mathbf{z}}^{1},\cdots,\tilde{\mathbf{z}}^{n_{0}}\}\subset Z\) such that \(\{k_{\tilde{\mathbf{z}}^{1}},\cdots,k_{\tilde{\mathbf{z}}^{n_{0}}}\}\) is a basis of \(W\). Define a matrix

\[B=\begin{bmatrix}b_{11}&\cdots&b_{1\tilde{n}_{0}}\\ \vdots&\ddots&\vdots\\ b_{n_{0}1}&\cdots&b_{n_{0}\tilde{n}_{0}}\end{bmatrix}\in\mathbb{R}^{n_{0} \times\tilde{n}_{0}}\]

such that \(k_{\mathbf{z}^{i}}=\sum_{j=1}^{\tilde{n}_{0}}b_{ij}k_{\tilde{\mathbf{z}}^{j}}\). Then \(K_{Z\tilde{Z}}=BK_{\tilde{Z}\tilde{Z}}\) where

\[K_{Z\tilde{Z}}=\begin{bmatrix}k(\mathbf{z}^{1},\tilde{\mathbf{z}}^{1})&\cdots& k(\mathbf{z}^{1},\tilde{\mathbf{z}}^{\tilde{n}_{0}})\\ \vdots&\ddots&\vdots\\ k(\mathbf{z}^{n_{0}},\tilde{\mathbf{z}}^{1})&\cdots&k(\mathbf{z}^{n_{0}}, \tilde{\mathbf{z}}^{\tilde{n}_{0}})\end{bmatrix}\in\mathbb{R}^{n_{0}\times \tilde{n}_{0}}\]and

\[K_{\hat{Z}\hat{Z}}=\begin{bmatrix}k(\bar{\mathbf{z}}^{1},\bar{\mathbf{z}}^{1})& \cdots&k(\bar{\mathbf{z}}^{1},\bar{\mathbf{z}}^{\bar{n}_{0}})\\ \vdots&\ddots&\vdots\\ k(\bar{\mathbf{z}}^{\bar{n}_{0}},\bar{\mathbf{z}}^{1})&\cdots&k(\bar{\mathbf{z}}^ {\bar{n}_{0}},\bar{\mathbf{z}}^{\bar{n}_{0}})\end{bmatrix}\in\mathbb{R}^{\bar{ n}_{0}\times\bar{n}_{0}}.\]

Set \(P_{Z}h^{*}=\sum_{j=1}^{\bar{n}_{0}}a_{j}k_{\bar{\mathbf{z}}_{j}}\). Then we can see that

\[(I-\eta S_{Z}^{\top}S_{Z})\left(\sum_{j=1}^{\bar{n}_{0}}a_{j}k_{ \bar{\mathbf{z}}_{j}}\right) =\sum_{r=1}^{\bar{n}_{0}}\left(a_{r}-\frac{\eta}{n}\sum_{i=1}^{n _{0}}\sum_{j=1}^{\bar{n}_{0}}a_{j}k(\bar{\mathbf{z}}_{j},\mathbf{z}_{i})b_{ir }\right)k_{\bar{\mathbf{z}}_{r}}\] \[=\sum_{r=1}^{\bar{n}_{0}}\left(a_{r}-\frac{\eta}{n}[B^{\top}BK_{ \hat{Z}\hat{Z}}\mathbf{a}]_{r}\right)k_{\bar{\mathbf{z}}_{r}}=\sum_{r=1}^{\bar {n}_{0}}\left[\left(I-\frac{\eta}{n}B^{\top}BK_{\hat{Z}\hat{Z}}\right)\mathbf{ a}\right]_{r}k_{\bar{\mathbf{z}}_{r}}\]

where \([\cdot]_{r}\) is the \(r\)th component of the given vector and \(\mathbf{a}=[a_{1},\cdots,a_{\bar{n}_{0}}]^{\top}\). Note that \(K_{\hat{Z}\hat{Z}}\) is invertible since \(\mathbf{v}^{\top}K_{\hat{Z}\hat{Z}}\mathbf{v}=0\) implies \(\mathbf{v}=0\). We can also see that \(K_{ZZ}=BK_{\hat{Z}\hat{Z}}B^{\top}\) where

\[K_{ZZ}=\begin{bmatrix}k(\mathbf{z}^{1},\mathbf{z}^{1})&\cdots&k(\mathbf{z}^{1 },\mathbf{z}^{n_{0}})\\ \vdots&\ddots&\vdots\\ k(\mathbf{z}^{n_{0}},\mathbf{z}^{1})&\cdots&k(\mathbf{z}^{n_{0}},\mathbf{z}^{ n_{0}})\end{bmatrix}\in\mathbb{R}^{n_{0}\times n_{0}}.\]

So

\[\left\|K_{\hat{Z}\hat{Z}}^{1/2}B^{\top}BK_{\hat{Z}\hat{Z}}^{1/2}\right\|= \left\|BK_{\hat{Z}\hat{Z}}B^{\top}\right\|\leq\left\|BK_{\hat{Z}\hat{Z}}B^{ \top}\right\|_{F}\leq n\kappa^{2}.\]

Thus, \(0<\frac{\eta}{n}K_{\hat{Z}\hat{Z}}^{1/2}B^{\top}BK_{\hat{Z}\hat{Z}}^{1/2}<I\) and

\[(I-\eta S_{Z}^{\top}S_{Z})^{t}P_{Z}h^{*}=\sum_{r=1}^{\bar{n}_{0}}\left[K_{ \hat{Z}\hat{Z}}^{-1/2}\left(I-\frac{\eta}{n}K_{\hat{Z}\hat{Z}}^{1/2}B^{\top} BK_{\hat{Z}\hat{Z}}^{1/2}\right)^{t}K_{\hat{Z}\hat{Z}}^{1/2}\mathbf{a}\right]_{r}k_{ \bar{\mathbf{z}}_{r}}\to 0\]

as \(t\to\infty\). Similarly, we get \((I-\eta S_{Z}^{\top}S_{Z})^{t}u_{1}\to 0\) as \(t\to\infty\). Therefore, we attain \(\mathcal{G}^{t}u_{1}\to P_{Z}h^{*}\) as \(t\to\infty\) for any \(u_{1}\in W\).

From this fact, DCL-KR has the recurrence relation

\[f_{t}=P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\left(\overline{T}_{k,X_{i}}^{E}f_{t- 1}+\eta\sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}S_{D_{i}}^{\top}\mathbf{y}_{ i}\right)\] (6)

where \(f_{t}=f_{i,t}\) for any \(i=1,\cdots,m\) and \(\overline{T}_{k,X_{i}}:=I-\eta T_{k,X_{i}}\) for \(i=1,\cdots,m\). Then we obtain a closed form

\[f_{t}=\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E} \right)^{t}f_{0}+\sum_{j=0}^{t-1}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\eta \sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}S_{D_{i}}^{\top}\mathbf{y}_{i}.\]

We first compute

\[f_{0}^{*}-\left(\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{ E}\right)^{t}f_{0}^{*}+\sum_{j=0}^{t-1}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\eta \sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}T_{k,X_{i}}f_{0}^{*}\right).\]

From

\[\eta\sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}T_{k,X_{i}}=\eta\sum_{s=0}^{E-1}(I -\eta T_{k,X_{i}})^{s}T_{k,X_{i}}=I-(I-\eta T_{k,X_{i}})^{E},\]

we have

\[f_{0}^{*}-\left(\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{ E}\right)^{t}f_{0}^{*}+\sum_{j=0}^{t-1}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\eta \sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}T_{k,X_{i}}f_{0}^{*}\right)\] \[=f_{0}^{*}-\left(\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{ T}_{k,X_{i}}^{E}\right)^{t}f_{0}^{*}+\sum_{j=0}^{t-1}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\left(I-\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)f_{0}^{*}\right)\] \[=\left(I+\left(\sum_{i=1}^{m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_ {i}}^{E}\right)+\cdots+\left(\sum_{i=1}^{m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i }}^{E}\right)^{t-1}\right)(I-P_{Z})f_{0}^{*}\] \[=(I-P_{Z})f_{0}^{*}+\left(I+\cdots+\left(\sum_{i=1}^{m}\frac{n_{i} }{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z}\left(\sum_{i=1}^{m} \frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}-I\right)(I-P_{Z})f_{0}^{*}.\]where the last equality follows from \(P_{Z}(I-P_{Z})=0\). Thus, we obtain the equality

\[\iota_{\rho_{\mathbf{x}}}(f_{t}-f_{0}^{*})\] \[=\iota_{\rho_{\mathbf{x}}}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{t}(f_{0}-f_{0}^{*})\] \[\qquad+\iota_{\rho_{\mathbf{x}}}\sum_{j=0}^{t-1}\left(P_{Z}\sum_ {i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\sum_{i=1}^{m }\frac{n_{i}}{n}\eta\sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}S_{D_{i}}^{\top}( \mathbf{y}_{i}-S_{D_{i}}f_{0}^{*})-\iota_{\rho_{\mathbf{x}}}(I-P_{Z})f_{0}^{*}\] \[\qquad+\iota_{\rho_{\mathbf{x}}}\left(I+\cdots+\left(\sum_{i=1}^{ m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z} \left(I-\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)(I-P_{Z} )f_{0}^{*}.\] (7)

#### a.2.2 Norm Bound of First Term in (7)

We first bound the norm of the first term in (7) as

\[\left\|\iota_{\rho_{\mathbf{x}}}\left(P_{Z}\sum_{i=1}^{m}\frac{n_ {i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}(f_{0}-f_{0}^{*})\right\|_{L_{P_{ \mathbf{x}}}^{2}}\] \[\leq\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2 }\right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}} {n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}f_{0}^{*}\right\|_{\mathbb{H}_{k}}\] \[\qquad+\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1 /2}\right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{ i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}\right\|\left\|(I-P_{Z})f_{0}^{*} \right\|_{\mathbb{H}_{k}}\] (8)

where \(\lambda>0\). The first term in (8) is bounded as

\[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2} \right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}} {n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}f_{0}^{*}\right\|_{\mathbb{H}_{k}}\] \[\leq\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2 }\right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}} {n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}(T_{k,X}+\lambda I)^{r-1/2}\right\|\] \[\qquad\cdot\left\|(T_{k,X}+\lambda I)^{-(r-1/2)}T_{k,\rho_{ \mathbf{x}}}^{r-1/2}g_{0}^{*}\right\|_{\mathbb{H}_{k}}.\]

Note that

\[\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_ {i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}(T_{k,X}+\lambda I)^{r-1/2}\right\|\] \[\leq\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac {n_{i}}{n}\overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t/2r}\right\|^{2r}\]

by Lemma A.8. Set \(A_{i}=\overline{T}_{k,X_{i}}^{E}\;\Leftrightarrow\;T_{k,X_{i}}=\frac{1}{\eta }(I-A_{i}^{1/E})\). We observe that

\[\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac {n_{i}}{n}\overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t/2r}\right\|^{2r}\] \[=\left\|\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X _{i}}^{E}P_{Z}\right)^{t/2r}P_{Z}(T_{k,X}+\lambda I)P_{Z}\left(P_{Z}\sum_{i=1}^ {m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t/2r}\right\|^{r}\] \[\leq\left(\frac{1}{\eta}\left\|\left(\sum_{i=1}^{m}\frac{n_{i}} {n}P_{Z}A_{i}P_{Z}\right)^{t/2r}\left(I-\sum_{i=1}^{n}\frac{n_{i}}{n}P_{Z}A_{i} P_{Z}\right)\left(\sum_{i=1}^{m}\frac{n_{i}}{n}P_{Z}A_{i}P_{Z}\right)^{t/2r}\right\|+ \lambda\right)^{r}\]

where the equality follows from \(0\leq A_{i}\leq I\;\Rightarrow\;A_{i}^{1/E}\geq A_{i}\) and \(I\geq P_{Z}\). Since \(\sup_{x\in[0,1]}x^{t/r}(1-x)=\frac{r}{t+r}\cdot(\frac{t}{t+r})^{t/r}\), we attain the inequality

\[\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t/2r}\right\|^{2r}\leq\left(\frac{r}{t+r }\cdot\frac{1}{\eta}\left(\frac{t}{t+r}\right)^{t/r}+\lambda\right)^{r}.\] (9)Next, Lemma A.8 gives

\[\left\|(T_{k,X}+\lambda I)^{-(r-1/2)}T_{k,\rho_{\mathbf{x}}}^{r-1/2}\right\| \leq\left\|(T_{k,X}+\lambda I)^{-(r-1/2)}(T_{k,\rho_{\mathbf{x}}}+ \lambda I)^{r-1/2}\right\|\] \[\leq\left\|(T_{k,X}+\lambda I)^{-1}(T_{k,\rho_{\mathbf{x}}}+ \lambda I)\right\|^{r-1/2}\]

and

\[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\right\|\leq \left\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{1/2}(T_{k,X}+\lambda I)^{-1/2} \right\|\leq\left\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)(T_{k,X}+\lambda I)^{-1} \right\|^{1/2}.\]

By Lemma A.10,

\[\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)(T_{k,X}+\lambda I)^{-1}\|\leq 2+2 \left(\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N}( \lambda)}{n\lambda}}\right)\log(2/\delta)\right)^{2}\] (10)

holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\). Combining (9) and (10) and applying \(\frac{r}{t+r}\leq\frac{1}{t}\) and \((\frac{t}{t+r})^{t/r}\leq\frac{1}{2}\) yield

\[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2} \right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{ n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}f_{0}^{*}\right\|_{\mathbb{H}_{k}}\] \[\leq\left(\frac{r}{t+r}\cdot\frac{1}{\eta}\left(\frac{t}{t+r} \right)^{t/r}+\lambda\right)^{r}\|b_{0}^{*}\|_{\mathbb{H}_{k}}\left(2+2\left( \left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N}(\lambda) }{n\lambda}}\right)\log(2/\delta)\right)^{2}\right)^{r}\] \[\leq R\left(\frac{1}{2\eta t}+\lambda\right)^{r}\left(2+2\left( \frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N}(\lambda)}{n \lambda}}\right)^{2}\right)^{r}(\log(4/\delta))^{2r}\]

with confidence at least \(1-\delta\) where \(\delta\in(0,1)\). Therefore, putting \(\lambda=n^{-\frac{1}{2r+z}}\) yields

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1} ^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}P_{Z}f_{0}^{*}\right\| _{\mathbb{H}_{k}}\right]\] \[\leq\left(\frac{1}{t}+n^{-\frac{1}{2r+z}}\right)^{r}.\]

Here, we apply the fact that \(\mathbb{E}A=\int_{0}^{\infty}\mathbb{P}(A\geq t)\ dt\) for \(A\geq 0\).

We next turn to bound the second term in (8). Note that

\[\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{m}\frac{n _{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}\right\|\] \[=\left\|\left(\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}} ^{E}P_{Z}\right)^{t}(T_{k,X}+\lambda I)\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{ n}\overline{T}_{k,X_{i}}^{E}\right)^{t}\right\|^{1/2}\] \[\leq\left\|\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{ k,X_{i}}^{E}P_{Z}\right)^{t-1}P_{Z}(T_{k,X}+\lambda I)P_{Z}\left(P_{Z}\sum_{i=1} ^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t-1}\right\|^{1/2}.\]

Set \(A_{i}=\overline{T}_{k,X_{i}}^{E}\). Using a similar argument as before gives

\[\left\|\left(P_{Z}\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_ {i}}^{E}P_{Z}\right)^{t-1}P_{Z}(T_{k,X}+\lambda I)P_{Z}\left(P_{Z}\sum_{i=1}^{m }\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}P_{Z}\right)^{t-1}\right\|^{1/2}\] \[\leq\left(\frac{1}{\eta(2t-1)}+\lambda\right)^{1/2}\leq\left(\frac {1}{\eta t}+\lambda\right)^{1/2}.\]Since \(Z\) and \(X\) are independent, we have

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_{i=1}^{ m}\frac{n_{i}}{n}T_{k,X_{i}}^{E}\right)^{t}\right\|\left\|(I-P_{Z})f_{0}^{*} \right\|_{\mathbb{H}_{k}}\right]\] \[\leq\left(\frac{1}{\eta t}+\lambda\right)^{1/2}\mathbb{E}\left\|T _{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\right\|\cdot\mathbb{E} \left\|(I-P_{Z})f_{0}^{*}\right\|_{\mathbb{H}_{k}}.\]

We already see that

\[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\right\| \leq\left(2+2\left(\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac {4\kappa^{2}\mathcal{N}(\lambda)}{n\lambda}}\right)\log(2/\delta)\right)^{2} \right)^{1/2}\] \[\leq\left(2+2\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4 \kappa^{2}\mathcal{N}(\lambda)}{n\lambda}}\right)^{2}\right)^{1/2}\log(4/\delta)\]

holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\) and so

\[\mathbb{E}\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2} \right\|\leq 4\left(2+2(2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^{2} \right)^{1/2}\]

by putting \(\lambda=n^{-\frac{1}{2\kappa+z}}\) as before.

The remaining part is to bound \(\mathbb{E}\|(I-P_{Z})f_{0}^{*}\|_{\mathbb{H}_{k}}\). Applying Lemma A.9 yields \(\|(I-P_{Z})f_{0}^{*}\|_{\mathbb{H}_{k}}\leq\lambda_{0}^{1/2}\|(T_{k,Z}+ \lambda_{0}I)^{-1/2}T_{k,\rho_{\mathbf{x}}}^{r-1/2}\|\|g_{0}^{*}\|_{\mathbb{H} _{k}}\) where \(\lambda_{0}>0\). Then Lemma A.8 gives

\[\lambda_{0}^{1/2}\|(T_{k,Z}+\lambda_{0}I)^{-1/2}T_{k,\rho_{\mathbf{ x}}}^{r-1/2}\|\|g_{0}^{*}\|_{\mathbb{H}_{k}}\] \[\leq R\lambda_{0}^{1/2}\|(T_{k,Z}+\lambda_{0}I)^{-(1-r)}\|\|(T_{k,Z}+\lambda_{0}I)^{-(r-1/2)}T_{k,\rho_{\mathbf{x}}}^{r-1/2}\|\] \[\leq R\lambda_{0}^{r-1/2}\|(T_{k,Z}+\lambda_{0}I)^{-1/2}T_{k,\rho _{\mathbf{x}}}^{1/2}\|^{2r-1}.\]

From \(\frac{d\rho_{\mathbf{x}}}{d\rho_{\mathbf{x}}}\leq B\), we obtain

\[\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,Z}+\lambda_{0}I)^{-1/2}\| =\|\iota_{\rho_{\mathbf{x}}}(T_{k,Z}+\lambda_{0}I)^{-1/2}\|\] \[\leq B^{1/2}\|\iota_{\hat{\jmath}_{\mathbf{x}}}(T_{k,Z}+\lambda_{ 0}I)^{-1/2}\|=B^{1/2}\|T_{k,\hat{\jmath}_{\mathbf{x}}}^{1/2}(T_{k,Z}+\lambda_ {0}I)^{-1/2}\|.\]

Set \(\lambda_{0}=128(\kappa^{2}+1)^{2}(\log n_{0})^{3}/n_{0}\) where we assume \(n\) is sufficiently large such that \(\lambda_{0}\leq 1\) and \(\mathcal{N}_{\hat{\rho}_{\mathbf{x}}}(\lambda_{0})\geq 1\) for \(n_{0}\geq n^{\frac{1}{2r+z}}(\log n)^{3}\). By Lemma A.8 and Lemma A.12,

\[\|T_{k,\hat{\jmath}_{\mathbf{x}}}^{1/2}(T_{k,Z}+\lambda_{0}I)^{-1/2}\| \leq\|(T_{k,\hat{\jmath}_{\mathbf{x}}}+\lambda_{0}I)^{1/2}(T_{k,Z}+\lambda_{0 }I)^{-1/2}\|\leq\sqrt{2}\]

holds with confidence at least \(1-\delta\) where \(\delta\in[4\exp(-1/4(\kappa^{2}+1)\mathcal{B}_{0}),1)\) and

\[\mathcal{B}_{0}=\frac{1+\log\mathcal{N}_{\hat{\jmath}_{\mathbf{x}}}(\lambda_{0 })}{\lambda_{0}n_{0}}+\sqrt{\frac{1+\log\mathcal{N}_{\hat{\jmath}_{\mathbf{x}} }(\lambda_{0})}{\lambda_{0}n_{0}}}.\]

Since

\[\|(I-P_{Z})f_{0}^{*}\|_{\mathbb{H}_{k}}\leq\|f_{0}^{*}\|_{\mathbb{H}_{k}}=\|T_ {k,\rho_{\mathbf{x}}}^{r-1/2}g_{0}^{*}\|_{\mathbb{H}_{k}}\leq\|T_{k,\rho_{ \mathbf{x}}}\|^{r-1/2}\|g_{0}^{*}\|_{\mathbb{H}_{k}}\leq R\kappa^{2r-1},\]

we have

\[\mathbb{E}\|(I-P_{Z})f_{0}^{*}\|_{\mathbb{H}_{k}}\leq R\lambda_{0}^{r-1/2}B^{r -1/2}2^{r-1/2}+R\kappa^{2r-1}\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1)\mathcal{B}_{0}} \right).\]

From \(n_{0}\geq\kappa^{2}e\), we get

\[\mathcal{B}_{0} \leq\frac{\log\kappa^{2}e+\log n_{0}}{128(\kappa^{2}+1)^{2}(\log n _{0})^{3}}+\sqrt{\frac{\log\kappa^{2}e+\log n_{0}}{128(\kappa^{2}+1)^{2}(\log n _{0})^{3}}}\] \[\leq\frac{2\log n_{0}}{128(\kappa^{2}+1)^{2}(\log n_{0})^{3}}+ \sqrt{\frac{2\log n_{0}}{128(\kappa^{2}+1)^{2}(\log n_{0})^{3}}}\leq\frac{1}{4( \kappa^{2}+1)\log n_{0}}\]and so \(R\kappa^{2r-1}\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1)B_{0}}\right)\leq 4R \kappa^{2r-1}\cdot\frac{1}{n_{0}}\). Therefore,

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|\left(T_{k,X}+\lambda I)^{1/2}\left(P_{Z}\sum_ {i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{t}\right\|\left\| \left(I-P_{Z}\right)f_{0}^{*}\right\|_{\mathbb{H}_{k}}\right]\] \[\leq\left(\frac{1}{\eta t}+n^{-\frac{1}{2r+s}}\right)^{1/2}4 \left(2+2(2\kappa^{2}+2\kappa\sqrt{C_{s}})^{2}\right)^{1/2}\left(R\lambda_{0}^ {r-1/2}B^{r-1/2}2^{r-1/2}+4R\kappa^{2r-1}\cdot\frac{1}{n_{0}}\right)\] \[\lesssim B^{r-1/2}\left(\frac{1}{t}+n^{-\frac{1}{2r+s}}\right)^{ 1/2}n^{-\frac{r-1/2}{2r+s}}\]

where the last inequality comes from \(n_{0}\geq n^{\frac{1}{2r+s}}(\log n)^{3}\).

#### a.2.3 Norm Bound of Second Term in (7)

Set

\[P=\begin{bmatrix}\sum_{s=0}^{E-1}(I-\eta S_{D_{1}}S_{D_{1}}^{\top})^{s}&0& \cdots&0\\ 0&\sum_{s=0}^{E-1}(I-\eta S_{D_{2}}S_{D_{2}}^{\top})^{s}&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&\sum_{s=0}^{E-1}(I-\eta S_{D_{m}}S_{D_{m}}^{\top})^{s}\end{bmatrix}.\]

Note that

\[I-\eta S_{D}^{\top}PS_{D} =I-\sum_{i=1}^{m}\frac{n_{i}}{n}\eta S_{D_{i}}^{\top}\sum_{s=0}^{E -1}(I-\eta S_{D_{i}}S_{D_{i}}^{\top})^{s}S_{D_{i}}\] \[=I-\sum_{i=1}^{m}\frac{n_{i}}{n}(I-(I-\eta S_{D_{i}}^{\top}S_{D_{ i}})^{E})=\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}.\] (11)

Then the second term in (7) becomes

\[\iota_{\rho_{\mathbf{x}}}\sum_{j=0}^{t-1}\left(P_{Z}\sum_{i=1}^{m }\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{j}P_{Z}\sum_{i=1}^{m}\frac {n_{i}}{n}\eta\sum_{s=0}^{E-1}\overline{T}_{k,X_{i}}^{s}S_{D_{i}}^{\top}( \mathbf{y}_{i}-S_{D_{i}}f_{0}^{*})\] \[=\iota_{\rho_{\mathbf{x}}}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S _{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}).\]

We can see that

\[\left\|l_{\rho_{\mathbf{x}}}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z }S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{ *})\right\|_{L_{\rho_{\mathbf{x}}}^{2}}\] \[\leq\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2 }\right\|\left(T_{k,X}+\lambda I)^{1/2}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S _{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{* })\right\|_{\mathbb{H}_{k}}\] \[\leq\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2 }\right\|\left(\left\|T_{k,X}^{1/2}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{ D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}) \right\|_{\mathbb{H}_{k}}\right.\] \[\left.+\lambda^{1/2}\left\|\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z} S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}) \right\|_{\mathbb{H}_{k}}\right).\]

We first bound the expectation of the first term in the above. By the Cauchy-Schwartz inequality, we have

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|T_{k,X}^{1/2}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P _{Z}S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{ 0}^{*})\right\|_{\mathbb{H}_{k}}\right]\] \[\leq\left(\mathbb{E}\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\|^{2}\right)^{1/2}\left(\mathbb{E}\left\|T_{k,X}^{1/2}\sum_ {j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{ \top}P(\mathbf{y}-S_{D}f_{0})\right\|_{\mathbb{H}_{k}}^{2}\right)^{1/2}.\]Observe that

\[\left\|T_{k,X}^{1/2}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{ \top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*})\right\|_ {\mathbb{H}_{k}}\] \[=\left\|S_{D}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{\top}PS_{ D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*})\right\|_{2}\]

and

\[S_{D}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{\top}PS_{D} \right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*})\] \[=P^{-1/2}(I-(I-\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2})^{t})P^{1 /2}(\mathbf{y}-S_{D}f_{0}^{*})\] \[=(I-(I-\eta S_{D}P_{Z}S_{D}^{\top}P)^{t})(\mathbf{y}-S_{D}f_{0}^{ *}).\]

Using \(\mathbb{E}(\mathbf{y}-S_{D}f_{0}^{*})(\mathbf{y}-S_{D}f_{0}^{*})^{\top}\leq \gamma^{2}I\), we have

\[\mathbb{E}\left[\left\|(I-(I-\eta S_{D}P_{Z}S_{D}^{\top}P)^{t})( \mathbf{y}-S_{D}f_{0}^{*})\right\|_{2}^{2}|X,Z\right]\] \[=\frac{1}{n}\text{tr}\left((I-(I-\eta S_{D}P_{Z}S_{D}^{\top}P)^{ t})\mathbb{E}\left[(\mathbf{y}-S_{D}f_{0}^{*})(\mathbf{y}-S_{D}f_{0}^{*})^{ \top}\right](I-(I-\eta S_{D}P_{Z}S_{D}^{\top}P)^{t})^{\top}\right)\] \[\leq\frac{\gamma^{2}}{n}\left\|(I-(I-\eta S_{D}P_{Z}S_{D}^{\top}P )^{t})\right\|_{HS}^{2}\leq\frac{\gamma^{2}E}{n}\left\|(I-(I-\eta P^{1/2}S_{D} P_{Z}S_{D}^{\top}P^{1/2})^{t})\right\|_{HS}^{2}\]

where the last inequality follows from the fact that \(\|AB\|_{HS}\leq\|A\|\|B\|_{HS}\), \(\|AB\|_{HS}\leq\|A\|_{HS}\|B\|\), and

\[\|P^{1/2}\|^{2}\|P^{-1/2}\|^{2}\leq E\left(\sum_{s=0}^{E-1}(1-\eta\kappa^{2})^ {s}\right)^{-1}\leq E.\]

Since

\[0\leq\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2}\leq\eta P^{1/2}S_{D}S_{D}^{\top }P^{1/2}\leq I\] (12)

which follows from (11), we can see that

\[0\leq\lambda_{i}(\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2})\leq \lambda_{i}(\eta P^{1/2}S_{D}S_{D}^{\top}P^{1/2})\leq 1\] \[\Rightarrow\quad 0\leq\lambda_{i}(I-(I-\eta P^{1/2}S_{D}P_{Z}S_{D}^{ \top}P^{1/2})^{t})\leq\lambda_{i}(I-(I-\eta P^{1/2}S_{D}S_{D}^{\top}P^{1/2})^ {t})\leq 1\]

where \(\lambda_{i}(\cdot)\) is the \(i\)th largest eigenvalue of a given operator. Therefore,

\[\frac{\gamma^{2}E}{n}\left\|(I-(I-\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2})^ {t})\right\|_{HS}^{2}\leq\frac{\gamma^{2}E}{n}\left\|(I-(I-\eta P^{1/2}S_{D}S_ {D}^{\top}P^{1/2})^{t})\right\|_{HS}^{2}.\]

Using (12) and \(1\wedge u^{2}\leq 1\wedge u\) for \(u\geq 0\) lead to

\[\lambda_{i}(I-(I-\eta P^{1/2}S_{D}S_{D}^{\top}P^{1/2})^{t})^{2} =(1-(1-\eta\lambda_{i}(P^{1/2}S_{D}S_{D}^{\top}P^{1/2}))^{t})^{2}\] \[\leq 1\wedge(\eta^{2}t^{2}\lambda_{i}(P^{1/2}S_{D}S_{D}^{\top}P^{1/ 2})^{2})\] \[\leq 1\wedge(\eta t\lambda_{i}(P^{1/2}S_{D}S_{D}^{\top}P^{1/2}))\] \[\leq 1\wedge(\eta tE\hat{\lambda}_{i})\]

where \(\hat{\lambda}_{1}\geq\cdots\geq\hat{\lambda}_{n}\) are eigenvalues of \(S_{D}S_{D}^{\top}\), the first inequality comes from the Bernoulli inequality, and the last inequality follows from the fact that \(\|P\|\leq E\). We define

\[\mathcal{R}(\epsilon)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{\lambda}_{i}\wedge \epsilon^{2})}.\]

Then

\[\frac{\gamma^{2}E}{n}\left\|(I-(I-\eta P^{1/2}S_{D}S_{D}^{\top}P^{1/2})^{t}) \right\|_{HS}^{2}\leq\gamma^{2}\eta tE^{2}\cdot\mathcal{R}\left(\frac{1}{ \sqrt{\eta tE}}\right)^{2}.\]

Similarly as in Appendix A.2.2, putting \(\lambda=n^{-\frac{1}{2\epsilon+s}}\) gives

\[\mathbb{E}\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\|^{2}\leq 2 +4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}})^{2}.\]Therefore, the Cauchy-Schwartz inequality gives a bound as

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{k}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|T_{k,X}^{1/2}\sum_{j=0}^{t-1}\left(P_{Z}-\eta P _{Z}S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^ {*})\right\|_{\mathbb{H}_{k}}\right]\] \[\leq\sqrt{(2+4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^ {2})\gamma^{2}\eta tE^{2}}\cdot\left(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{ \eta tE}}\right)^{2}\right)^{1/2}.\]

We now bound the expectation of

\[\lambda^{1/2}\|T_{k,\rho_{\mathbf{k}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\|\left\| \sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z} S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*})\right\|_{\mathbb{H}_{k}}.\]

By the Cauchy-Schwartz inequality and the same argument as before, we have

\[\mathbb{E}\left[\lambda^{1/2}\|T_{k,\rho_{\mathbf{k}}}^{1/2}(T_{ k,X}+\lambda I)^{-1/2}\|\left\|\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{ \top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}) \right\|_{\mathbb{H}_{k}}\right]\] \[\leq(2+4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^{2})^ {1/2}\left(\lambda\cdot\mathbb{E}\left\|\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z} S_{D}^{\top}PS_{D}P_{Z}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}) \right\|_{\mathbb{H}_{k}}^{2}\right)^{1/2}.\]

Also, the same argument as before yields

\[\mathbb{E}\left\|\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{ \top}PS_{D}P_{Z}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*}) \right\|_{\mathbb{H}_{k}}^{2} =\frac{1}{n}\mathbb{E}\left[(\mathbf{y}-S_{D}f_{0}^{*})^{\top}A( \mathbf{y}-S_{D}f_{0}^{*})\right]\] \[\leq\frac{\gamma^{2}}{n}\mathbb{E}[\text{tr}(A)]\]

where

\[A =\eta PS_{D}P_{Z}\left(\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D} ^{\top}PS_{D}P_{Z}\right)^{j}\right)^{2}\eta P_{Z}S_{D}^{\top}P\] \[=\eta PS_{D}P_{Z}\left(\sum_{j=0}^{t-1}\left(I-\eta P_{Z}S_{D}^{ \top}PS_{D}P_{Z}\right)^{j}\right)^{2}\eta P_{Z}S_{D}^{\top}P.\]

To bound \(\mathbb{E}[\text{tr}(A)]\), note that

\[\text{tr}(A) \leq E\cdot\text{tr}\left(\eta P^{1/2}S_{D}P_{Z}\left(\sum_{j=0} ^{t-1}\left(I-\eta P_{Z}S_{D}^{\top}PS_{D}P_{Z}\right)^{j}\right)^{2}\eta P_{Z }S_{D}^{\top}P^{1/2}\right)\] \[=\eta E\cdot\text{tr}\left(\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{ 1/2}\left(\sum_{j=0}^{t-1}(I-\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2})^{j} \right)^{2}\right).\]

Let \(B=\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2}\). Then \(0\leq B\leq I\) and

\[\eta E\cdot\text{tr}\left(\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/ 2}\left(\sum_{j=0}^{t-1}(I-\eta P^{1/2}S_{D}P_{Z}S_{D}^{\top}P^{1/2})^{j} \right)^{2}\right)\] \[=\eta E\sum_{i=1}^{n}\lambda_{i}(B)\left(\sum_{j=0}^{t-1}(1- \lambda_{i}(B))^{j}\right)^{2}=\eta E\sum_{i=1}^{n}\frac{1}{\lambda_{i}(B)}(1-( 1-\lambda_{i}(B))^{t})^{2}\] \[\leq\eta E\sum_{i=1}^{n}\frac{1}{\lambda_{i}(B)}\wedge(t^{2} \lambda_{i}(B))\leq\eta E\sum_{i=1}^{n}t\wedge(t^{2}\lambda_{i}(B))\]

where the first inequality follows from \(1-x^{t}\leq 1\wedge t(1-x)\) and the second inequality follows from \(1/x\wedge t^{2}x\leq t\wedge t^{2}x\) for all \(t\geq 0\) and \(x\in[0,1]\). From the fact that

\[\lambda_{i}(B)\leq\eta\|P^{1/2}\|^{2}\lambda_{i}(S_{D}P_{Z}S_{D}^{\top})\leq\eta E \lambda_{i}(S_{D}S_{D}^{\top})=\eta E\hat{\lambda}_{i},\]we have

\[\eta E\sum_{i=1}^{n}t\wedge(t^{2}\lambda_{i}(B))\leq\eta E\sum_{i=1}^{n}t\wedge( \eta t^{2}E\hat{\lambda}_{i})=n\eta^{2}t^{2}E^{2}\cdot\mathcal{R}\left(\frac{1}{ \sqrt{\eta tE}}\right)^{2}.\]

Therefore, we obtain

\[\mathbb{E}\left[\lambda^{1/2}\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+\lambda I)^{-1/2}\|\left\|\sum_{j=0}^{t-1}\left(P_{Z}-\eta P_{Z}S_{D}^{\top }PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y}-S_{D}f_{0}^{*})\right\|_{ \mathbb{H}_{k}}\right]\] \[\leq\sqrt{(2+4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}^{3}})^{2}) \lambda\gamma^{2}\eta^{2}t^{2}E^{2})}\cdot\left(\mathbb{E}\mathcal{R}\left( \frac{1}{\sqrt{\eta tE}}\right)^{2}\right)^{1/2}.\]

In conclusion, we have an upper bound of the norm of the second term in (7) as

\[\mathbb{E}\left\|\iota_{\rho_{\mathbf{x}}}\sum_{j=0}^{t-1}\left(P _{Z}-\eta P_{Z}S_{D}^{\top}PS_{D}\right)^{j}\eta P_{Z}S_{D}^{\top}P(\mathbf{y }-S_{D}f_{0}^{*})\right\|_{L_{\rho_{\mathbf{x}}}^{2}}\] \[\leq\sqrt{(2+4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}^{3}})^{2}) \gamma^{2}\eta tE^{2}}\cdot\left(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{ \eta tE}}\right)^{2}\right)^{1/2}\] \[\qquad+\sqrt{(2+4\Gamma(3)(2\kappa^{2}+2\kappa\sqrt{C_{s}^{3}})^{ 2})(\lambda\gamma^{2}\eta^{2}t^{2}E^{2})}\cdot\left(\mathbb{E}\mathcal{R} \left(\frac{1}{\sqrt{\eta tE}}\right)^{2}\right)^{1/2}\] \[\lesssim\left(t^{1/2}+n^{-\frac{1/2}{2\tau+s}}t\right)\cdot \left(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{\eta tE}}\right)^{2}\right)^{ 1/2}\]

by taking \(\lambda=n^{-\frac{1}{2\tau+s}}\). We will bound \(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{\eta tE}}\right)^{2}\) in Appendix A.2.5.

#### a.2.4 Norm Bound of Third and Last Term in (7)

Note that

\[\left\|\iota_{\rho_{\mathbf{x}}}\left(I+\cdots+\left(\sum_{i=1}^{ m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z} \left(I-\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)(I-P_{Z} )f_{0}^{*}\right\|_{L_{\rho_{\mathbf{x}}}^{2}}\] \[\leq\|T_{k,\rho_{\mathbf{x}}}^{1/2}\left(T_{k,X}+\lambda I \right)^{-1/2}\|\] \[\qquad\cdot\left\|(T_{k,X}+\lambda I)^{1/2}\left(I+\cdots+\left( \sum_{i=1}^{m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2} \right)P_{Z}\left(I-\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E} \right)^{1/2}\right\|\] \[\qquad\cdot\left\|\left(I-\sum_{i=1}^{m}\frac{n_{i}}{n} \overline{T}_{k,X_{i}}^{E}\right)^{1/2}(I-P_{Z})f_{0}^{*}\right\|_{\mathbb{H }_{k}}\]

where \(0<\lambda\leq 1\). From (11) and \(0\leq P\leq EI\), we have \(0\leq I-\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}=\eta S_{D}^{ \top}PS_{D}\leq\eta ES_{D}^{\top}S_{D}=\eta ET_{k,X}\). Using this fact, we find that

\[\left\|(T_{k,X}+\lambda I)^{1/2}\left(I+\cdots+\left(\sum_{i=1}^{ m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z} \left(I-\sum_{i=1}^{m}\frac{n_{i}}{n}\overline{T}_{k,X_{i}}^{E}\right)^{1/2}\right\|\] \[\leq(\eta E)^{1/2}\left(\left\|T_{k,X}^{1/2}\left(I+\cdots+\left( \sum_{i=1}^{m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2} \right)P_{Z}T_{k,X}^{1/2}\right\|\right.\] \[\qquad\left.+\lambda^{1/2}\left\|\left(I+\cdots+\left(\sum_{i=1} ^{m}\frac{n_{i}}{n}P_{Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z}T_{k,X}^{1/2}\right\|\right).\]

[MISSING_PAGE_EMPTY:25]

we have \(\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{-(r-1/2)}T_{k,\rho_{\mathbf{x}}}^{r -1/2}\|\leq B^{r-1/2}\). On the other hand,

\[\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{-(r-1/2)}T_{k,\rho_{ \mathbf{x}}}\|\leq B^{r-1/2}.\]

By Lemma A.8, we have

\[\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{-(r-1/2)}\|\left\|(T_{k,X}+ \lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\right\|_{\mathbb{H}_{k}}\]

Since \(X\) and \(Z\) are independent, we have

\[\mathbb{E}\left[\left\|T_{k,\rho_{\mathbf{x}}}^{1/2}(T_{k,X}+ \lambda I)^{-1/2}\right\|\left\|(T_{k,X}+\lambda I)^{1/2}(I-P_{Z})f_{0}^{*} \right\|_{\mathbb{H}_{k}}\right]\] \[\leq RB^{r}\mathbb{E}\left[\left\|(T_{k,\rho_{\mathbf{x}}}+ \lambda I)^{1/2}(T_{k,X}+\lambda I)^{-1/2}\right\|\left\|(T_{k,X}+\lambda I)^{ 1/2}(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{-1/2}\right\|\right]\] \[\quad\quad\cdot\mathbb{E}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+ \lambda I)^{1/2}(I-P_{Z})\|^{2r}.\]

By Lemma A.8 and Lemma A.10,

\[\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{1/2}(T_{k,X}+\lambda I)^{-1/2}\|\leq \left(2+2\left(\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2} \mathcal{N}(\lambda)}{n\lambda}}\right)\log(2/\delta)\right)^{2}\right)^{1/2}\]

holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\). Also, by Lemma A.8 and Lemma A.11

\[\|(T_{k,X}+\lambda I)^{1/2}(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{-1/2}\|\leq \left(1+\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N} (\lambda)}{n\lambda}}\right)\log(2/\delta)\right)^{1/2}\]

holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\). Thus,

\[\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{1/2}(T_{k,X}+\lambda I)^{ -1/2}\|\|(T_{k,X}+\lambda I)^{1/2}(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{-1/2}\|\] \[\leq\left(2+2\left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4 \kappa^{2}\mathcal{N}(\lambda)}{n\lambda}}\right)^{2}\right)^{1/2}\left(1+ \left(\frac{2\kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N}( \lambda)}{n\lambda}}\right)\right)^{1/2}\left(\log(4/\delta)\right)^{3/2}\]

with confidence at least \(1-\delta\) where \(\delta\in(0,1)\). Set \(\lambda=128(\kappa^{2}+1)^{2}n^{-\frac{1}{2\kappa+\kappa}}\) where \(n\) is sufficiently large such that \(\lambda\leq 1\) and \(\mathcal{N}_{\tilde{\rho}_{\mathbf{x}}}(\lambda)\geq 1\). Then

\[\mathbb{E}\left[\left\|(T_{k,\rho_{\mathbf{x}}}+\lambda I)^{1/2}( T_{k,X}+\lambda I)^{-1/2}\right\|\|(T_{k,X}+\lambda I)^{1/2}(T_{k,\rho_{ \mathbf{x}}}+\lambda I)^{-1/2}\|\right]\] \[\leq 4\Gamma\left(2.5\right)(2+2(2\kappa^{2}+2\kappa\sqrt{C_{s}^{ \prime}})^{2})^{1/2}(1+2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^{1/2} \lesssim 1.\]

We now bound \(\mathbb{E}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{Z})\|^{2r}\). By Lemma A.9, we have

\[\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{Z})\|^{2r}\leq\chi^{ \prime}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(T_{k,Z}+\lambda I)^{ -1/2}\|^{2r}.\]

By Lemma A.12, \(\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(T_{k,Z}+\lambda I)^{-1/2}\|\leq\sqrt{2}\) with confidence at least \(1-4\exp(-1/4(\kappa^{2}+1)\mathcal{B}_{0})\) where

\[\mathcal{B}_{0}=\frac{1+\log\mathcal{N}_{\tilde{\rho}_{\mathbf{x}}}(\lambda)}{ \lambda n_{0}}+\sqrt{\frac{1+\log\mathcal{N}_{\tilde{\rho}_{\mathbf{x}}}(\lambda )}{\lambda n_{0}}}.\]

Also, \(\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{Z})\|\leq(\kappa^{2}+ 1)^{1/2}\) almost surely. Thus,

\[\mathbb{E}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{Z})\|^{2r} \leq 2^{r}\lambda^{r}+(\kappa^{2}+1)^{r}\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1) \mathcal{B}_{0}}\right).\]

Note that

\[\mathcal{B}_{0}\leq\frac{\log\kappa^{2}e+\log(1/\lambda)}{\lambda n_{0}}+\sqrt{ \frac{\log\kappa^{2}e+\log(1/\lambda)}{\lambda n_{0}}}\leq\frac{1}{4(\kappa^{2}+ 1)\log n}\]and so \((\kappa^{2}+1)^{r}\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1)B_{0}}\right)\leq 4( \kappa^{2}+1)^{r}\cdot\frac{1}{n}\). Therefore,

\[\mathbb{E}\|(T_{k,\hat{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{Z})\|^{2r}\leq 2 ^{r}128^{r}(\kappa^{2}+1)^{2r}\cdot n^{-\frac{r}{2r+s}}+4(\kappa^{2}+1)^{r} \cdot n^{-1}\lesssim n^{-\frac{r}{2r+s}}.\]

We can conclude that

\[\mathbb{E}\left\|-\iota_{\rho_{\mathbf{x}}}(I-P_{Z})f_{0}^{*}+ \iota_{\rho_{\mathbf{x}}}\left(I+\cdots+\left(\sum_{i=1}^{m}\frac{n_{i}}{n}P_{ Z}\overline{T}_{k,X_{i}}^{E}\right)^{t-2}\right)P_{Z}\left(I-\sum_{i=1}^{m}\frac{n_{i} }{n}\overline{T}_{k,X_{i}}^{E}\right)(I-P_{Z})f_{0}^{*}\right\|_{L^{2}_{\rho_{ \mathbf{x}}}}\] \[\leq(1+2E+E\log t+\sqrt{6\eta t\lambda}E)RB^{r}\cdot 4\Gamma\left( \ref{eq:2}\right)(2+2(2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^{2})^{1/2}(1+ 2\kappa^{2}+2\kappa\sqrt{C_{s}^{\prime}})^{1/2}\] \[\qquad\cdot\left(2^{r}128^{r}(\kappa^{2}+1)^{2r}\cdot n^{-\frac{ r}{2r+s}}+4(\kappa^{2}+1)^{r}\cdot n^{-1}\right)\] \[\lesssim B^{r}(1+\log t+t^{1/2}n^{-\frac{1/2}{2r+s}})n^{-\frac{r}{2r+s}}.\]

#### a.2.5 Stopping Rule and Rademacher Complexity Bound

For convenience, we abuse the notation \(D=\{(\mathbf{x}^{1},y^{1}),\cdots,(\mathbf{x}^{n},y^{n})\}\) and \(X=\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\). Define the local empirical Rademacher complexity

\[Q_{n}(\epsilon)=\mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1,\|g\|_{L^{2}_ {\rho_{\mathbf{x}},n}}\leq\epsilon}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}g( \mathbf{x}^{i})\right|\Bigg{|}\ X\right]\]

and the local population Rademacher complexity

\[\overline{Q}_{n}(\epsilon)=\mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1,\|g\|_{L^{2}_{\rho_{\mathbf{x}}}}\leq\epsilon}\left|\frac{1}{n}\sum_{i=1}^{n} w_{i}g(\mathbf{x}^{i})\right|\right]\]

where \(w_{1},\cdots,w_{n}\) are independent Rademacher random variables and \(\rho_{\mathbf{x},n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{\mathbf{x}^{i}}\). We also define

\[\overline{\mathcal{R}}(\epsilon)=\sqrt{\frac{1}{n}\sum_{i=1}^{\infty}\lambda_ {i}\wedge\epsilon^{2}}\]

where \(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq 0\) are eigenvalues of \(T_{k,\rho_{\mathbf{x}}}\). We recall the following well-known property.

**Lemma A.1** ([46], [60]).: _We have_

\[\overline{Q}_{n}(\epsilon)\leq\sqrt{2}\cdot\overline{\mathcal{R}}(\epsilon)\]

_for \(\epsilon>0\)._

We can prove the following lemma using a similar argument as in [46].

**Lemma A.2**.: _There is an absolute constant \(c>0\) which satisfies that for every \(\epsilon>0\),_

\[c\cdot\mathcal{R}(\epsilon)\leq Q_{n}(\epsilon).\]

Proof of Lemma a.2.: We divide the proof into three parts.

**Part 1.** Since \(T_{k,X}=S_{D}^{\top}S_{D}\), \(\hat{\lambda}_{1}\geq\hat{\lambda}_{2}\geq\cdots\geq\hat{\lambda}_{n}\geq 0\) are eigenvalues of \(T_{k,X}\). For convenience, set \(\hat{\lambda}_{i}=0\) for \(i>n\) and define \(\hat{n}\leq n\) such that \(\hat{\lambda}_{i}>0\) and \(\hat{\lambda}_{i+1}=0\). Choose an orthonormal basis \(\{\hat{\psi}_{i}\}_{i=1}^{\infty}\) of \(\mathbb{H}_{k}\) such that \(\hat{\psi}_{i}\) is an eigenvector of \(T_{k,X}\) corresponding to \(\hat{\lambda}_{i}\). Then \(\langle\hat{\psi}_{i},\hat{\psi}_{j}\rangle_{L^{2}_{\rho_{\mathbf{x}},n}}= \langle S_{D}\hat{\psi}_{i},S_{D}\hat{\psi}_{j}\rangle_{2}=\langle T_{k,X}\hat{ \psi}_{i},\hat{\psi}_{j}\rangle_{\mathbb{H}_{k}}=\delta_{\{i=j\}}\hat{\lambda}_ {i}\). We will show that

\[k_{\mathbf{x}}=\sum_{i=1}^{\hat{n}}\hat{\psi}_{i}(\mathbf{x})\hat{\psi}_{i}\]

where \(\mathbf{x}\in\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\). Let \(W_{1}\) be the subspace of \(\mathbb{H}_{k}\) spanned by \(\{\hat{\psi}_{i}:i=1,\cdots,\hat{n}\}\) and \(W_{2}\) be the subspace of \(\mathbb{H}_{k}\) spanned by \(\{k_{\mathbf{x}^{i}}:i=1,\cdots,n\}\). Observe that \(W_{1}^{\perp}=\ker T_{k,X}\) and \(W_{2}^{\perp}\subset\ker T_{k,X}\) by the reproducing property. Thus, \(W_{1}\subset W_{2}\). Conversely, choose a basis \(\{k_{\mathbf{x}^{i}}:i=1,\cdots,\hat{n}^{\prime}\}\subset\{k_{\mathbf{x}^{i}}:i= 1,\cdots,n\}\) of \(W_{2}\). Then, using a similar argument as in Appendix A.2.1 implies that there exists a matrix

\[B=\begin{bmatrix}b_{11}&\cdots&b_{1\hat{n}^{\prime}}\\ \vdots&\ddots&\vdots\\ b_{n1}&\cdots&b_{n\hat{n}^{\prime}}\end{bmatrix}\in\mathbb{R}^{n\times\hat{n}^ {\prime}}\]such that \(k_{\mathbf{x}^{i}}=\sum_{j=1}^{\hat{n}^{\prime}}b_{ij}k_{\tilde{\mathbf{x}}^{j}}\). Then \(K_{X\tilde{X}}=BK_{\tilde{X}\tilde{X}}\) where

\[K_{X\tilde{X}}=\begin{bmatrix}k(\mathbf{x}^{1},\tilde{\mathbf{x}}^{1})&\cdots&k (\mathbf{x}^{1},\tilde{\mathbf{x}}^{\hat{n}^{\prime}})\\ \vdots&\ddots&\vdots\\ k(\mathbf{x}^{n},\tilde{\mathbf{x}}^{1})&\cdots&k(\mathbf{x}^{n},\tilde{ \mathbf{x}}^{\hat{n}^{\prime}})\end{bmatrix}\in\mathbb{R}^{n\times\hat{n}^{ \prime}}\]

and

\[K_{\tilde{X}\tilde{X}}=\begin{bmatrix}k(\tilde{\mathbf{x}}^{1},\tilde{\mathbf{ x}}^{1})&\cdots&k(\tilde{\mathbf{x}}^{1},\tilde{\mathbf{x}}^{\hat{n}^{ \prime}})\\ \vdots&\ddots&\vdots\\ k(\tilde{\mathbf{x}}^{\hat{n}^{\prime}},\tilde{\mathbf{x}}^{1})&\cdots&k( \tilde{\mathbf{x}}^{\hat{n}^{\prime}},\tilde{\mathbf{x}}^{\hat{n}^{\prime}}) \end{bmatrix}\in\mathbb{R}^{\hat{n}^{\prime}\times\hat{n}^{\prime}}.\]

Since \(K_{\tilde{X},\tilde{X}}\) and \(B^{\top}B\) are invertible,

\[T_{k,X}\left(\sum_{i=1}^{\hat{n}^{\prime}}[nK_{\tilde{X},\tilde{X}}^{-1}(B^{ \top}B)^{-1}\mathbf{b}]_{i}k_{\mathbf{x}^{i}}\right)=\sum_{i=1}^{\hat{n}^{ \prime}}\mathbf{b}_{i}k_{\tilde{\mathbf{x}}},\]

for any \(\mathbf{b}=[\mathbf{b}_{1},\cdots,\mathbf{b}_{\hat{n}^{\prime}}]^{\top}\in \mathbb{R}^{\hat{n}^{\prime}}\) where \([\cdot]_{r}\) is the \(r\)th component of the given vector. Therefore, \(W_{2}\subset\text{ran }T_{k,X}=(\ker T_{k,X})^{\perp}=W_{1}\) and so \(W_{1}=W_{2}\). From this fact, we can see that \(k_{\mathbf{x}^{i}}=\sum_{r=1}^{\hat{n}}a_{r}\hat{\psi}_{r}\) for some \(a_{1},\cdots,a_{\hat{n}}\in\mathbb{R}\). Then \(a_{r}=\langle k_{\mathbf{x}^{i}},\hat{\psi}_{r}\rangle_{\mathbb{H}_{k}}=\hat{ \psi}_{r}(\mathbf{x}^{i})\) for all \(r=1,\cdots,\hat{n}\) and so we are done. Note that \(k_{\mathbf{x}}=\sum_{i=1}^{\infty}\hat{\psi}_{i}(\mathbf{x})\hat{\psi}_{i}\) where \(\mathbf{x}\in\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) since \(\hat{\psi}_{i}(\mathbf{x})=0\) for \(\mathbf{x}\in\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) and \(i>\hat{n}\).

**Part 2.** Define

\[\mathcal{F}:=\left\{h:\|h\|_{\mathbb{H}_{k}}\leq 1\text{ and }\|h\|_{L^{2}_{ \mathcal{\mathbb{R}},n}}\leq\epsilon\right\}=\left\{\sum_{i=1}^{\infty}h_{i} \hat{\psi}_{i}:\sum_{i=1}^{\infty}h_{i}^{2}\leq 1\text{ and }\sum_{i=1}^{\hat{n}}\hat{ \lambda}_{i}h_{i}^{2}\leq\epsilon^{2}\right\}\]

and

\[\mathcal{E}:=\left\{\sum_{i=1}^{\infty}h_{i}\hat{\psi}_{i}:\sum_{i=1}^{\infty }\frac{\hat{\lambda}_{i}}{\hat{\lambda}_{i}\wedge\epsilon^{2}}h_{i}^{2}\leq 1\right\}\]

where \(\frac{0}{0}=1\). Then \(\mathcal{E}\subset\mathcal{F}\) since

\[\left(\sum_{i=1}^{\infty}h_{i}^{2}\right)\vee\left(\sum_{i=1}^{\infty}\frac{ \hat{\lambda}_{i}}{\epsilon^{2}}h_{i}^{2}\right)\leq\sum_{i=1}^{\infty}\left(1 \vee\frac{\hat{\lambda}_{i}}{\epsilon^{2}}\right)h_{i}^{2}=\sum_{i=1}^{\infty} \frac{\hat{\lambda}_{i}}{\hat{\lambda}_{i}\wedge\epsilon^{2}}h_{i}^{2}\leq 1\]

for \(h=\sum_{i=1}^{\infty}h_{i}\hat{\psi}_{i}\in\mathcal{E}\). Thus,

\[\mathbb{E}\left[\sup_{h\in\mathcal{E}}\left|\sum_{i=1}^{n}\epsilon_{i}h( \mathbf{x}^{i})\right|^{2}\Biggm{|}X\right]\leq\mathbb{E}\left[\sup_{h\in \mathcal{F}}\left|\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}^{i})\right|^{2}\Biggm{|}X\right]\]

where \(\epsilon_{1},\cdots,\epsilon_{n}\) are i.i.d. Rademacher variables. By the reproducing property,

\[\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}^{i}) =\langle h,\sum_{i=1}^{n}\epsilon_{i}k_{\mathbf{x}^{i}}\rangle_{ \mathbb{H}_{k}}=\langle h,\sum_{i=1}^{n}\epsilon_{i}\sum_{j=1}^{\hat{n}}\hat{ \psi}_{j}(\mathbf{x}^{i})\hat{\psi}_{j}\rangle_{\mathbb{H}_{k}}\] \[=\sum_{j=1}^{\hat{n}}h_{j}\sum_{i=1}^{n}\epsilon_{i}\hat{\psi}_{j }(\mathbf{x}^{i})=\langle\sum_{j=1}^{\infty}\sqrt{\frac{\hat{\lambda}_{j}}{\hat{ \lambda}_{j}\wedge\epsilon^{2}}}h_{j}\hat{\psi}_{j},\sum_{j=1}^{\hat{n}}\sqrt{ \frac{\hat{\lambda}_{j}\wedge\epsilon^{2}}{\hat{\lambda}_{j}}}\sum_{i=1}^{n} \epsilon_{i}\hat{\psi}_{j}(\mathbf{x}^{i})\hat{\psi}_{j}\rangle_{\mathbb{H}_{k}}\]

where \(h=\sum_{i=1}^{\infty}h_{i}\hat{\psi}_{i}\). Thus,

\[\sup_{h\in\mathcal{E}}\left|\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}^{i})\right|^{2 }=\left\|\sum_{j=1}^{\hat{n}}\sqrt{\frac{\hat{\lambda}_{j}\wedge\epsilon^{2}}{ \hat{\lambda}_{j}}}\sum_{i=1}^{n}\epsilon_{i}\hat{\psi}_{j}(\mathbf{x}^{i})\hat{ \psi}_{j}\right\|_{\mathbb{H}_{k}}^{2}.\]

Since

\[\left\|\sum_{j=1}^{\hat{n}}\sqrt{\frac{\hat{\lambda}_{j}\wedge\epsilon^{2}}{\hat{ \lambda}_{j}}}\sum_{i=1}^{n}\epsilon_{i}\hat{\psi}_{j}(\mathbf{x}^{i})\hat{\psi}_{j }\right\|_{\mathbb{H}_{k}}^{2}=\sum_{j=1}^{\hat{n}}\frac{\hat{\lambda}_{j} \wedge\epsilon^{2}}{\hat{\lambda}_{j}}\left(\sum_{i=1}^{n}\epsilon_{i}\hat{\psi}_{j }(\mathbf{x}^{i})\right)^{2},\]we have

\[\mathbb{E}\left[\sup_{h\in\mathcal{E}}\left|\sum_{i=1}^{n}\epsilon_{i }h(\mathbf{x}^{i})\right|^{2}\Bigg{|}\ X\right] =\mathbb{E}\left[\sum_{j=1}^{\hat{n}}\frac{\hat{\lambda}_{j}\wedge \epsilon^{2}}{\hat{\lambda}_{j}}\left(\sum_{i=1}^{n}\epsilon_{i}\hat{\psi}_{j} (\mathbf{x}^{i})\right)^{2}\Bigg{|}\ X\right]\] \[=\sum_{j=1}^{\hat{n}}\frac{\hat{\lambda}_{j}\wedge\epsilon^{2}}{ \hat{\lambda}_{j}}\left(\sum_{i=1}^{n}\hat{\psi}_{j}(\mathbf{x}^{i})^{2}\right) =n\sum_{j=1}^{n}\hat{\lambda}_{j}\wedge\epsilon^{2}.\]

Therefore,

\[\sqrt{n\sum_{j=1}^{n}\hat{\lambda}_{j}\wedge\epsilon^{2}}\leq\mathbb{E}\left[ \sup_{h\in\mathcal{F}}\left|\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}^{i})\right| ^{2}\Bigg{|}\ X\right]^{1/2}.\]

**Part 3.** By Khintchine's inequality,

\[\mathbb{E}\left[\sup_{h\in\mathcal{F}}\left|\sum_{i=1}^{n}\epsilon_{i}h( \mathbf{x}^{i})\right|\Bigg{|}\ X\right]\geq\sup_{h\in\mathcal{F}}\mathbb{E} \left[\left|\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}^{i})\right|\Bigg{|}\ X \right]\geq\frac{1}{\sqrt{2}}\sup_{h\in\mathcal{F}}\left(\sum_{i=1}^{n}h( \mathbf{x}^{i})^{2}\right)^{1/2}=\sqrt{\frac{n}{2}}\epsilon.\]

Set \(Z=g(\epsilon_{1},\cdots,\epsilon_{n})\) where

\[g(t_{1},\cdots,t_{n})=\sup_{h\in\mathcal{F}}\left|\sum_{i=1}^{n}t_{i}h( \mathbf{x}^{i})\right|.\]

By Remark A.14, \(g\) is convex and \(\sup_{h\in\mathcal{F}}(\sum_{i=1}^{n}h(\mathbf{x}^{i})^{2})^{1/2}=\sqrt{n} \epsilon\)-Lipschitz on \([-1,1]^{n}\). By Lemma A.13, we have

\[\mathbb{P}\left(Z-\mathbb{E}\left[Z|X\right]\geq t\mathbb{E}\left[Z|X|\right] \ X\right)\leq\exp\left(-\frac{t^{2}}{16\epsilon^{2}n}\mathbb{E}\left[Z|X|^{2} \right)\leq\exp\left(-\frac{t^{2}}{32}\right).\]

From

\[\mathbb{E}\left[Z^{2}|X\right] =\mathbb{E}\left[Z^{2}\mathbf{1}_{\{Z<\mathbb{E}[Z|X|]\}}|X\right] +\sum_{m=0}^{\infty}\mathbb{E}\left[Z^{2}\mathbf{1}_{\{(m+1)\mathbb{E}[Z|X] \leq Z<(m+2)\mathbb{E}[Z|X]\}}|X\right]\] \[\leq\mathbb{E}[Z|X]^{2}\left(1+\sum_{m=0}^{\infty}(m+2)^{2}\exp \left(-\frac{m^{2}}{32}\right)\right),\]

we have

\[\mathbb{E}[Z|X]\geq c\cdot\mathbb{E}\left[Z^{2}|X\right]^{1/2}\geq c\cdot \sqrt{n\sum_{j=1}^{n}\hat{\lambda}_{j}\wedge\epsilon^{2}}\]

where \(c=\left(1+\sum_{m=0}^{\infty}(m+2)^{2}\exp\left(-\frac{m^{2}}{32}\right) \right)^{-1/2}\) is an absolute constant. Therefore,

\[c\cdot\sqrt{\frac{1}{n}\sum_{j=1}^{n}\hat{\lambda}_{j}\wedge\epsilon^{2}} \leq\mathbb{E}\left[\sup_{h\in\mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^{n} \epsilon_{i}h(\mathbf{x}^{i})\right|\Bigg{|}\ X\right].\]

We set the population radius as

\[\epsilon_{n}=\inf\left\{\epsilon\geq 0:\overline{Q}_{n}(\epsilon)\leq\frac{ \epsilon^{1+2r}}{16\kappa}\right\}.\]

We also define

\[\tilde{\epsilon}_{n}=\inf\left\{\epsilon\geq 0:\overline{\mathcal{R}}(\epsilon) \leq\frac{\epsilon^{1+2r}}{16\sqrt{2}\kappa}\right\}.\]

By Lemma A.1, we have \(\epsilon_{n}\leq\tilde{\epsilon}_{n}\). We can easily see that \(\mathcal{R},\overline{\mathcal{R}},Q_{n},\) and \(\overline{Q}_{n}\) are increasing functions. The following lemma can be shown by a similar argument as in [2].

**Lemma A.3**.: _If \(g:[0,\infty)\to[0,\infty)\) is a function such that \(g\) is non-decreasing and \(r\mapsto g(r)/r\) is non-increasing, then \(g\) is continuous on \((0,\infty)\)._

Since

\[\frac{\mathcal{R}(\epsilon)}{\epsilon}=\sqrt{\frac{1}{n}\sum_{i=1}^{n}1\wedge \frac{\hat{\lambda}_{i}}{\epsilon^{2}}}\quad\text{and}\quad\frac{\overline{ \mathcal{R}}(\epsilon)}{\epsilon}=\sqrt{\frac{1}{n}\sum_{i=1}^{\infty}1 \wedge\frac{\lambda_{i}}{\epsilon^{2}}},\]

\(\epsilon\mapsto\mathcal{R}(\epsilon)/\epsilon\) and \(\epsilon\mapsto\overline{\mathcal{R}}(\epsilon)/\epsilon\) are non-increasing and so \(\mathcal{R}\) and \(\overline{\mathcal{R}}\) are continuous.

**Lemma A.4**.: \(\epsilon\mapsto Q_{n}(\epsilon)/\epsilon\) _and \(\epsilon\mapsto\overline{Q}_{n}(\epsilon)/\epsilon\) are non-increasing. In particular, \(\overline{Q}_{n}\) is continuous and \(\epsilon_{n}<\infty\)._

Proof.: From the fact that

\[\frac{Q_{n}(\epsilon)}{\epsilon} =\frac{1}{\epsilon}\mathbb{E}\left[\sup_{\|g\|_{h_{k}}\leq 1, \|g\|_{L^{2}_{\mathbf{g}\mathbf{x},n}}\leq\epsilon}\left|\frac{1}{n}\sum_{i=1} ^{n}w_{i}g(\mathbf{x}_{i})\right|\Bigg{|}\ X\right]\] \[=\mathbb{E}\left[\sup_{\|g\|_{h_{k}}\leq 1/\epsilon,\|g\|_{L^{2}_{ \mathbf{g}\mathbf{x},n}}\leq 1}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}g(\mathbf{x}_{i}) \right|\Bigg{|}\ X\right],\]

we can easily see that \(\epsilon\mapsto Q_{n}(\epsilon)/\epsilon\) is non-increasing. Similarly, we can show that \(\epsilon\mapsto\overline{Q}_{n}(\epsilon)/\epsilon\) is non-increasing. Note that

\[\lim_{\epsilon\to 0^{+}}\frac{Q_{n}(\epsilon)}{\epsilon}>0\quad\text{and} \quad\lim_{\epsilon\to 0^{+}}\frac{\overline{Q}_{n}(\epsilon)}{\epsilon}>0.\]

Also, we can observe that

\[\lim_{\epsilon\to\infty}\frac{Q_{n}(\epsilon)}{\epsilon}=0\quad\text{and} \quad\lim_{\epsilon\to\infty}\frac{\overline{Q}_{n}(\epsilon)}{\epsilon}=0.\]

Since \(\epsilon\mapsto\epsilon^{2r}/16\kappa\) is increasing, goes \(0\) as \(\epsilon\to 0^{+}\), and goes \(\infty\) as \(\epsilon\to\infty\), we can conclude that \(\epsilon_{n}<\infty\). 

Similarly, we have \(\tilde{\epsilon}_{n}<\infty\). In fact, we can find the lower and the upper bound of \(\tilde{\epsilon}_{n}\) under Assumption 3.2.

**Lemma A.5**.: _We have_

\[\left(2^{9/(4r+2s)}\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}\wedge c_{s}^{1/2}\left( \frac{s}{s+2}\right)^{1/2s}\right)n^{-\frac{1}{4r+2s}}\leq\tilde{\epsilon}_{n}\]

\[\leq 2^{9/(4r+2s)}\kappa^{1/(2r+s)}\left(\frac{2-s}{1-s}\right)^{1/(4r+2s)}C_{s }^{s/(4r+2s)}n^{-\frac{1}{4r+2s}}.\]

Proof.: Since \(c_{s}i^{-1/s}\leq\lambda_{i}\leq C_{s}i^{-1/s}\), we have

\[\sqrt{\frac{1}{n}\sum_{j=1}^{\infty}(c_{s}j^{-1/s})\wedge\epsilon^{2}}\leq \overline{\mathcal{R}}(\epsilon)\leq\sqrt{\frac{1}{n}\sum_{j=1}^{\infty}(C_{s }j^{-1/s})\wedge\epsilon^{2}}.\]

We first consider the lower bound of \(\tilde{\epsilon}_{n}\). We first observe that

\[\sqrt{\frac{1}{n}\sum_{j=1}^{\infty}(c_{s}j^{-1/s})\wedge\epsilon^{2}}=\sqrt {\frac{1}{n}\left(\left|\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right| \epsilon^{2}+\sum_{j=\left|\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s} \right|+1}^{\infty}c_{s}j^{-1/s}\right)}.\]

Set

\[\epsilon=\left(2^{9/(4r+2s)}\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}\wedge c_{s}^{1/ 2}\left(\frac{s}{s+2}\right)^{1/2s}\right)n^{-\frac{1}{4r+2s}}.\]

Note that

\[c_{s}\left[\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right]^{-1/s}\geq \epsilon^{2}\quad\text{and}\quad\frac{s}{1-s}\left(\left|\left(\frac{c_{s}}{ \epsilon^{2}}\right)^{s}\right|+1\right)^{(s-1)/s}\geq\left|\left(\frac{c_{s}} {\epsilon^{2}}\right)^{s}\right|^{-1/s}\]hold. The first formula is trivial. To show the second formula, we observe that the function \(u(t)=(\frac{s}{1-s})^{s}\cdot\frac{t}{(1+t)^{1-s}}\) is an increasing function. Thus, for \(t\geq 2/s\)

\[u(t)\geq u\left(\frac{2}{s}\right)=\frac{2}{(1-s)^{s}(s+2)^{1-s}}\geq\frac{2}{2- 2s^{2}}\geq 1.\]

Here, we apply an elementary inequality: \(a^{s}b^{1-s}\leq sa+(1-s)b\quad\forall a,b>0\). Since

\[\epsilon\leq c_{s}^{1/2}\left(\frac{s}{s+2}\right)^{1/2s}\quad\Rightarrow \quad\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor\geq \left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}-1\geq\frac{2}{s},\]

putting \(t=\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor\) gives

\[\left(\frac{s}{1-s}\right)^{s}\cdot\frac{\left\lfloor\left(\frac{c_{s}}{ \epsilon^{2}}\right)^{s}\right\rfloor}{(1+\left\lfloor\left(\frac{c_{s}}{ \epsilon^{2}}\right)^{s}\right\rfloor)^{1-s}}\geq 1\]

and so the second formula holds. Therefore,

\[\sum_{j=\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+ 1}^{\infty}c_{s}j^{-1/s}\geq c_{s}\int_{\left\lfloor\left(\frac{c_{s}}{ \epsilon^{2}}\right)^{s}\right\rfloor+1}^{\infty}\frac{1}{t^{1/s}}\ dt=\frac{sc_{s}}{1-s}\left(\left\lfloor\left( \frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1\right)^{(s-1)/s}\geq c_{s }\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor^{-1/s} \geq\epsilon^{2}\]

holds and so we have

\[\sqrt{\frac{1}{n}\left(\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s} \right\rfloor\epsilon^{2}+\sum_{j=\left\lfloor\left(\frac{c_{s}}{\epsilon^{2} }\right)^{s}\right\rfloor+1}^{\infty}c_{s}j^{-1/s}\right)}\geq\sqrt{\frac{1}{ n}\left(\left\lfloor\left(\frac{c_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1 \right)\epsilon^{2}}\geq\frac{c_{s}^{s/2}}{n^{1/2}}\epsilon^{1-s}\geq\frac{ \epsilon^{1+2r}}{16\sqrt{2}\kappa}\]

where the last inequality follows from \(\epsilon\leq 2^{9/(4r+2s)}\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}n^{-\frac{1}{4r+2s}}\). We can conclude that

\[\tilde{\epsilon}_{n}\geq\left(2^{9/(4r+2s)}\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)} \wedge c_{s}^{1/2}\left(\frac{s}{s+2}\right)^{1/2s}\right)n^{-\frac{1}{4r+2s}}\]

by Lemma A.4. We now derive the upper bound of \(\tilde{\epsilon}_{n}\). Note that

\[\sqrt{\frac{1}{n}\sum_{j=1}^{\infty}(C_{s}j^{-1/s})\wedge\epsilon^{2}}=\sqrt{ \frac{1}{n}\left(\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s}\right \rfloor\epsilon^{2}+\sum_{j=\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}} \right)^{s}\right\rfloor+1}^{\infty}C_{s}j^{-1/s}\right)}.\]

Since

\[\sum_{j=\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+ 1}^{\infty}C_{s}j^{-1/s}-\int_{\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}} \right)^{s}\right\rfloor+1}^{\infty}\frac{C_{s}}{t^{1/s}}\ dt\leq C_{s}\left( \left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1\right)^ {-1/s}\]

and

\[\int_{\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1 }^{\infty}\frac{C_{s}}{t^{1/s}}\ dt=\frac{sC_{s}}{1-s}\left(\left\lfloor\left( \frac{C_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1\right)^{1-1/s}\geq\frac{sC _{s}}{1-s}\left(\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s} \right\rfloor+1\right)^{-1/s},\]

we have

\[\sum_{j=\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s}\right\rfloor+1 }^{\infty}C_{s}j^{-1/s}\leq\frac{1}{s}\int_{\left(\frac{C_{s}}{\epsilon^{2}} \right)^{s}}^{\infty}\frac{C_{s}}{t^{1/s}}\ dt=\frac{C_{s}^{s}}{1-s}\epsilon^{2 -2s}.\]

Hence, we have

\[\sqrt{\frac{1}{n}\left(\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}}\right)^{s }\right\rfloor\epsilon^{2}+\sum_{j=\left\lfloor\left(\frac{C_{s}}{\epsilon^{2}} \right)^{s}\right\rfloor+1}^{\infty}C_{s}j^{-1/s}\right)}\leq\sqrt{\frac{1}{n} \left(C_{s}^{s}+\frac{C_{s}^{s}}{1-s}\right)\epsilon^{2-2s}}.\]

Set

\[\epsilon=2^{9/(4r+2s)}\kappa^{1/(2r+s)}\left(\frac{2-s}{1-s}\right)^{1/(4r+2s)} C_{s}^{s/(4r+2s)}n^{-\frac{1}{4r+2s}}\]

which is equivalent to \(\sqrt{\frac{1}{n}\left(C_{s}^{s}+\frac{C_{s}^{s}}{1-s}\right)\epsilon^{2-2s}}= \frac{\epsilon^{1+2r}}{16\sqrt{2}\kappa}\). Therefore, we attain the upper bound of \(\tilde{\epsilon}_{n}\):

\[\tilde{\epsilon}_{n}\leq 2^{9/(4r+2s)}\kappa^{1/(2r+s)}\left(\frac{2-s}{1-s} \right)^{1/(4r+2s)}C_{s}^{s/(4r+2s)}n^{-\frac{1}{4r+2s}}.\]Without loss of generality, we assume \(n\) is sufficiently large such that

\[n\geq 2^{9}\kappa^{2}\left(\frac{2-s}{1-s}\right)C_{s}^{s}\ \Leftrightarrow\ 2^{9/(4r+2s)}\kappa^{1/(2r+s)}\left(\frac{2-s}{1-s}\right)^{1/(4r+2s)}C_{s}^{s /(4r+2s)}n^{-\frac{1}{4r+2s}}\leq 1.\]

Then \(\epsilon_{n}\leq\tilde{\epsilon}_{n}\leq 1\). We now prove the following lemma. It is an extended version of Theorem 14.1 in [60].

**Lemma A.6**.: _We have_

\[\mathbb{P}\left(\sup_{\|h\|_{\mathbb{H}_{k}\leq 1}}\frac{\left\|h\right\|_{L_{ \mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^ {2}}{\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^{2}+t^{2}}\leq\frac{1}{2}\right) \geq 1-\exp\left(-c_{1}nt^{4r}\right)\]

_for any \(t\in[\epsilon_{n},1]\) where \(c_{1}\) is a constant independent of \(t\) and \(n\)._

Proof of Lemma a.6.: We use a similar argument as in the proof of Theorem 14.1 in [60]. Define

\[Z_{n}(t):=\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{\mathcal{P}_{\mathbf{x}} }^{2}}}\leq t}\left\|\|h\|_{L_{\mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|h\|_{L_{ \mathcal{P}_{\mathbf{x}}}^{2}}^{2}\right|\]

where \(t\in(0,1]\). Let

\[\mathcal{E}:=\left\{\sup_{\|h\|_{\mathbb{H}_{k}\leq 1}}\frac{\left|\|h\|_{L_{ \mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}} ^{2}\right|}{\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^{2}+t^{2}}\leq\frac{1}{2 }\right\}^{c},\ \mathcal{A}:=\left\{Z_{n}(t)\geq\frac{t^{2}}{2}\right\},\ \tilde{\mathcal{A}}:=\left\{Z_{n}(t)\geq \frac{t^{1+2r}}{2}\right\}.\]

We first show that \(\mathcal{E}\subset\mathcal{A}\). On the event \(\mathcal{E}\), there exists \(h\in\mathbb{H}_{k}\) such that \(\|h\|_{\mathbb{H}_{k}}\leq 1\) and

\[\frac{\left|\|h\|_{L_{\mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|h\|_{L_{\mathcal{ P}_{\mathbf{x}}}^{2}}^{2}\right|}{\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^{2}+t^{2}}> \frac{1}{2}.\]

If \(\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}\leq t\), we have \(\left|\|h\|_{L_{\mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|h\|_{L_{\mathcal{P}_{ \mathbf{x}}}^{2}}^{2}\right|>\frac{1}{2}\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2 }}^{2}+\frac{1}{2}t^{2}\geq\frac{1}{2}t^{2}\). Otherwise, set \(\tilde{h}=\frac{t}{\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^{2}}h\) then \(\|\tilde{h}\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}=t\) and \(\left|\|\tilde{h}\|_{L_{\mathcal{P}_{\mathbf{x}},n}^{2}}^{2}-\|\tilde{h}\|_{L _{\mathcal{P}_{\mathbf{x}}}^{2}}^{2}\right|>\frac{t^{2}}{\|h\|_{L_{\mathcal{ P}_{\mathbf{x}}}^{2}}^{2}}\cdot\left(\frac{1}{2}\|h\|_{L_{\mathcal{P}_{\mathbf{x}}}^{2}}^ {2}+\frac{1}{2}t^{2}\right)\geq\frac{1}{2}t^{2}\). Therefore, \(\mathcal{E}\subset\mathcal{A}\). Since \(\frac{t^{2}}{2}\geq\frac{t^{1+2r}}{2}\) for \(t\in(0,1]\), we have \(\mathcal{E}\subset\mathcal{A}\subset\tilde{\mathcal{A}}\). To find an upper bound of \(\mathbb{E}Z_{n}(t)\), we use the symmetrization argument as follows:

\[\mathbb{E}Z_{n}(t) =\mathbb{E}\left[\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{ \mathcal{P}_{\mathbf{x}}}^{2}}}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}h(\mathbf{ x}^{i})^{2}-\mathbb{E}h(\mathbf{x})^{2}\right|\right]\] \[=\mathbb{E}\left[\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{ \mathcal{P}_{\mathbf{x}}}^{2}}}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}h( \mathbf{x}^{i})^{2}-\frac{1}{n}\sum_{i=1}^{n}h(\tilde{\mathbf{x}}^{i})^{2} \ \Bigg{|}\ X\right|\right]\] \[\leq\mathbb{E}\left[\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{ \mathcal{P}_{\mathbf{x}}}^{2}}}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i} \left(h(\mathbf{x}^{i})^{2}-h(\tilde{\mathbf{x}}^{i})^{2}\right)\right|\right]\] \[\leq 2\mathbb{E}\left[\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{ \mathcal{P}_{\mathbf{x}}}^{2}}}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}h( \mathbf{x}^{i})^{2}\right|\right]\]

where \(w_{1},\cdots,w_{n}\) are i.i.d. Rademacher variables. By Lemma A.15,

\[2\mathbb{E}\left[\sup_{\|h\|_{\mathbb{H}_{k}\leq 1,\|h\|_{L_{\mathcal{P}_{ \mathbf{x}}}^{2}}}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}h(\mathbf{x}^{i}) ^{2}\right|\right]\leq 4\kappa\cdot\overline{Q}_{n}(t).\]

For \(t\geq\epsilon_{n}\), we have \(4\kappa\cdot\overline{Q}_{n}(t)\leq\frac{t^{1+2r}}{4}\) since

\[\overline{Q}_{n}(\epsilon_{n})=\frac{\epsilon_{n}^{1+2r}}{16\kappa}\quad\text{ and}\quad\overline{Q}_{n}(\epsilon)\leq\frac{\epsilon^{1+2r}}{16\kappa},\ \forall\epsilon\geq\epsilon_{n}.\] (13)Since

\[\frac{1}{n}\sum_{i=1}^{n}\sup_{\|\hat{n}\|_{\mathbb{H}_{k}}\leq 1,\|\hat{n}\|_{L^{2}_{ \rho_{\mathbf{x}}}}\leq t}\mathbb{E}\left[(h(\mathbf{x}^{i})^{2}-\mathbb{E}h( \mathbf{x})^{2})^{2}\right]\leq\sup_{\|h\|_{\mathbb{H}_{k}}\leq 1,\|h\|_{L^{2}_{ \rho_{\mathbf{x}}}}\leq t}\mathbb{E}\left[h(\mathbf{x})^{4}\right]\leq\kappa^{2 }t^{2},\]

Lemma A.16 gives

\[\mathbb{P}\left(Z_{n}(t)\geq\mathbb{E}Z_{n}(t)+\frac{u^{1+2r}}{4}\right) \leq\exp\left(-\frac{\frac{1}{16}n^{2}u^{2+4r}}{4\kappa^{2}\cdot n \mathbb{E}Z_{n}(t)+2n\kappa^{2}t^{2}+\frac{2}{3}\kappa^{2}\cdot\frac{1}{4}nu^ {1+2r}}\right)\] \[\leq\exp\left(-c_{1}n\left(\frac{u^{2+4r}}{t^{1+2r}}\wedge\frac{u ^{2+4r}}{t^{2}}\wedge u^{1+2r}\right)\right)\]

where \(c_{1}=\frac{1}{96\kappa^{2}}\). Putting \(u=t\) gives

\[\mathbb{P}(\tilde{\mathcal{A}})\leq\mathbb{P}\left(Z_{n}(t)\geq\mathbb{E}Z_{n }(t)+\frac{t^{1+2r}}{4}\right)\leq\exp\left(-c_{1}n\left(t^{1+2r}\wedge t^{4r }\right)\right)\leq\exp(-c_{1}nt^{4r}),\]

i.e., \(\mathbb{P}(\mathcal{E}^{c})\geq\mathbb{P}(\tilde{\mathcal{A}}^{c})\geq 1-\exp(-c_{ 1}nt^{4r})\). 

Let us return to our problem. Consider the event

\[\mathcal{E}^{c}=\left\{\sup_{\|h\|_{\mathbb{H}_{k}}\leq 1}\frac{\left\|\|h\|_{L^ {2}_{\rho_{\mathbf{x}}}}-\|h\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}\right\|}{\|h\|_ {L^{2}_{\rho_{\mathbf{x}}}}+\tilde{c}_{n}^{2}}\leq\frac{1}{2}\right\}.\]

By Lemma A.6, we have \(\mathbb{P}(\mathcal{E}^{c})\geq 1-\exp(-c_{1}n\tilde{c}_{n}^{4r})\) where \(c_{1}\) is a constant that does not depend on \(n\). Define

\[T:=\min\left\{t\in\mathbb{N}:\frac{1}{\sqrt{\eta Et}}\leq\tilde{\epsilon}_{n} \right\}.\]

By the definition, we can easily obtain the upper bound of \(T\) as \(T<1+\frac{1}{\eta E\mathcal{E}^{c}_{n}}\). Since \(\mathcal{R}(\cdot)\) is non-decreasing,

\[\mathcal{R}\left(\frac{1}{\sqrt{\eta ET}}\right)\leq\mathcal{R}(\tilde{ \epsilon}_{n})\leq\frac{1}{c_{l}}Q_{n}(\tilde{\epsilon}_{n})\]

for some absolute constant \(c_{l}\) where the second inequality follows from Lemma A.2. Note that

\[\|h\|_{L^{2}_{\rho_{\mathbf{x}},n}}^{2}-\|h\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2} \geq-\frac{1}{2}\left(\|h\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}+\tilde{c}_{n}^{2} \right)\ \Rightarrow\ \|h\|_{L^{2}_{\rho_{\mathbf{x}}}}^{2}\geq\frac{1}{2}\|h\|_{L^{2}_{\rho_{ \mathbf{x}}}}^{2}-\frac{1}{2}\tilde{\epsilon}_{n}^{2}\]

for all \(h\) such that \(\|h\|_{\mathbb{H}_{k}}\leq 1\) on the event \(\mathcal{E}^{c}\). Thus,

\[Q_{n}(\tilde{\epsilon}_{n})=\mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1, \|g\|_{L^{2}_{\rho_{\mathbf{x}},n}}\leq\varepsilon_{n}}\left|\frac{1}{n}\sum_{ i=1}^{n}w_{i}g(\mathbf{x}^{i})\right|\left|\left|X\right.\right]\leq \mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1,\|g\|_{L^{2}_{\rho_{\mathbf{x}}}}\leq 2 \tilde{\epsilon}_{n}}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}g(\mathbf{x}^{i}) \right|\left|\left|X\right.\right]\]

on the event \(\mathcal{E}^{c}\). Set \(\mathcal{F}=\left\{g:\mathcal{X}\rightarrow\mathbb{R}:\|g\|_{\mathbb{H}_{k}} \leq 1,\|g\|_{L^{2}_{\rho_{\mathbf{x}}}}\leq 2\tilde{\epsilon}_{n}\right\}\). Then the ranges of functions in \(\mathcal{F}\) are contained in \([-\kappa,\kappa]\) and \(\mathcal{F}=-\mathcal{F}\). By Lemma A.17 we have

\[\mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1,\|g\|_{L^{2}_{\rho_{ \mathbf{x}}}}\leq 2\tilde{\epsilon}_{n}}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}g( \mathbf{x}^{i})\right|\left|\left|X\right.\right]\leq 2\mathbb{E}\left[\sup_{\|g\|_{\mathbb{H}_{k}}\leq 1,\|g\|_{L^{2}_{ \rho_{\mathbf{x}}}}\leq 2\tilde{\epsilon}_{n}}\left|\frac{1}{n}\sum_{i=1}^{n}w_{i}g( \mathbf{x}^{i})\right|\right]+c_{1}\kappa\tilde{\epsilon}_{n}^{1+2r}\]

with probability at least \(1-\exp(-c_{1}n\tilde{\epsilon}_{n}^{1+2r})\geq 1-\exp(-c_{1}n\tilde{ \epsilon}_{n}^{4r})\). Hence,

\[\mathcal{R}\left(\frac{1}{\sqrt{\eta ET}}\right)\leq\frac{2}{c_{l}}\cdot\overline {Q}_{n}(2\tilde{\epsilon}_{n})+\frac{c_{1}\kappa}{c_{l}}\tilde{\epsilon}_{n}^{1+2 r}\leq\left(\frac{1}{\kappa c_{l}}+\frac{c_{1}\kappa}{c_{l}}\right)\tilde{ \epsilon}_{n}^{1+2r}\]

holds with probability at least \(1-2\exp(-c_{1}n\tilde{\epsilon}_{n}^{4r})\). Here, the second inequality follows from (13). Therefore,

\[\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{\eta ET}}\right)^{2} \leq\left(\frac{8}{\kappa c_{l}}+\frac{\kappa c_{1}}{c_{l}}\right) ^{2}\tilde{\epsilon}_{n}^{2+4r}\cdot(0\vee(1-2\exp(-c_{1}n\tilde{\epsilon}_{n}^{4r })))+\kappa^{2}\cdot 2\exp(-c_{1}n\tilde{\epsilon}_{n}^{4r})\] \[\leq\left(\frac{8}{\kappa c_{l}}+\frac{\kappa c_{1}}{c_{l}}\right) ^{2}\tilde{\epsilon}_{n}^{2+4r}+2\kappa^{2}\exp(-c_{1}n\tilde{\epsilon}_{n}^{4r})\]

since \(\mathcal{R}\left(\frac{1}{\sqrt{\eta ET}}\right)\leq\kappa\). From the fact that \(\frac{\exp(-c_{1}n\tilde{\epsilon}_{n}^{4r})}{\tilde{\epsilon}_{n}^{2+4r}}\lesssim n ^{\frac{2r+1}{2r+i}}\exp(-c_{1}^{\prime}n^{\frac{r}{2r+i}})\lesssim 1\), we have

\[\left(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{\eta ET}}\right)^{2}\right)^{1/2} \lesssim\tilde{\epsilon}_{n}^{1+2r}\lesssim n^{-\frac{2r+1}{4r+2s}}.\]

#### a.2.6 Conclusion

Note that

\[\frac{1}{T}\lesssim\tilde{\epsilon}_{n}^{2}\lesssim n^{-\frac{1}{2r+s}}\quad\text {and}\quad T\leq 1+\frac{1}{\eta E\tilde{\epsilon}_{n}^{2}}\lesssim n^{\frac{1}{2r+s}}.\]

Therefore, we bound the expected risk as

\[\mathbb{E}\|\iota_{\rho_{\mathbf{x}}}(f_{T}-f_{0}^{*})\|_{L^{2}_{ \rho_{\mathbf{x}}}}\] \[\lesssim B^{r-1/2}\left(\frac{1}{T}+n^{-\frac{1}{2r+s}}\right)^{ 1/2}n^{-\frac{r-1/2}{2r+s}}+\left(T^{1/2}+n^{-\frac{1/2}{2r+s}}T\right)\cdot \left(\mathbb{E}\mathcal{R}\left(\frac{1}{\sqrt{\eta TE}}\right)^{2}\right)^{1/2}\] \[\quad+B^{r}(1+\log T+T^{1/2}n^{-\frac{1/2}{2r+s}})n^{-\frac{r}{2 r+s}}\] \[\lesssim B^{r}n^{-\frac{r}{2r+s}}\log n.\]

### Corollary of Theorem 3.4

As mentioned in Section 3.3, one can remove \(B^{r}\) in the upper bound in Theorem 3.4 by using more public inputs. The precise statement is as follows:

**Corollary A.7**.: _Under Assumption 3.1, 3.2, and 3.3, with \(n_{0}\geq B^{1+\epsilon}n^{\frac{1}{2r+s}}(\log(Bn))^{3}\) public inputs independently generated from \(\tilde{\rho}_{\mathbf{x}}\) satisfying (2) DCL-KR gives the performance guarantee_

\[\mathbb{E}\|\iota_{\rho_{\mathbf{x}}}(f_{j,T}-f_{0}^{*})\|_{L^{2}_{\rho_{ \mathbf{x}}}}\leq C\cdot n^{-\frac{r}{2r+s}}\log n\]

_for all \(j=1,\cdots,m\) where \(\epsilon>0\) is a fixed constant, \(\eta\in(0,1/\kappa^{2})\) is a fixed learning rate, \(T\) is an adequate stopping rule, and the prefactor \(C\) does not depend on \(B\), \(m\), and \(n\)._

Proof.: In the proof of Theorem 3.4, there are two terms in the upper bound affected by \(B\). One is

\[B^{r-1/2}\left(\frac{(\log n_{0})^{3}}{n_{0}}\right)^{r-1/2}\]

in the norm bound of the first term in (7). The other is

\[\mathbb{E}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{D_{p}})\|^ {2r}\leq 2^{r}\lambda^{r}+(\kappa^{2}+1)\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1) \mathcal{B}_{0}}\right)\]

in the norm bound of the third and fourth terms in (7). For the first part, \(n_{0}\geq B^{1+\epsilon}n^{\frac{1}{2r+s}}(\log(Bn))^{3}\) implies

\[B^{r-1/2}\left(\frac{(\log n_{0})^{3}}{n_{0}}\right)^{r-1/2}\] \[\leq B^{-\epsilon(r-1/2)}n^{-\frac{r-1/2}{2r+s}}(\log(Bn))^{-3r+ 3/2}\left((1+\epsilon)\log B+\frac{1}{2r+s}\log n+3\log\log(Bn)\right)^{3r-3/2}\] \[\lesssim n^{-\frac{r-1/2}{2r+s}}.\]

For the latter part, set \(\lambda=128(\kappa^{2}+1)^{2}n^{-\frac{1}{2r+s}}/B\). Then

\[\mathcal{B}_{0}\leq\frac{\log\kappa^{2}e+\log(1/\lambda)}{\lambda n_{0}}+ \sqrt{\frac{\log\kappa^{2}e+\log(1/\lambda)}{\lambda n_{0}}}\leq\frac{1}{4( \kappa^{2}+1)\log(Bn)}\]

and hence

\[\mathbb{E}\|(T_{k,\tilde{\rho}_{\mathbf{x}}}+\lambda I)^{1/2}(I-P_{D_{p}})\|^ {2r}\leq 2^{r}\lambda^{r}+(\kappa^{2}+1)\cdot 4\exp\left(-\frac{1}{4(\kappa^{2}+1) \mathcal{B}_{0}}\right)\lesssim B^{-r}n^{-\frac{r}{2r+s}}+\frac{1}{Bn}.\]

Since it eliminates \(B^{r}\) in the upper bound, we are done.

### Useful Lemmas

Recall Cordes' inequality [16].

**Lemma A.8** (Cordes' Inequality).: _Let \(A,B\) be two bounded positive linear operators on a seperable Hilbert space. Then for any \(s\in[0,1]\)_

\[\|A^{s}B^{s}\|\leq\|AB\|^{s}\]

_holds._

We also recall a property of projection operators.

**Lemma A.9** ([53]).: _Let \(Z\) be a bounded linear operator and \(P\) be a projection operator such that ran \(P=\text{ran }Z^{\top}\). Then for any bounded operator \(X\) and \(\lambda>0\) we have_

\[\|(I-P)X\|\leq\lambda^{1/2}\|(Z^{\top}Z+\lambda I)^{-1/2}X\|.\]

There are some useful lemmas for PAC bounds.

**Lemma A.10** ([17]).: _Let \(X=\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) be a dataset where data points are independently generated from \(\nu\). Then_

\[\|(T_{k,\nu}+\lambda I)(T_{k,X}+\lambda I)^{-1}\|\leq 2+2\left(\left(\frac{2 \kappa^{2}}{n\lambda}+\sqrt{\frac{4\kappa^{2}\mathcal{N}_{\nu}(\lambda)}{n \lambda}}\right)\log(2/\delta)\right)^{2}\]

_holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\)._

**Lemma A.11** ([38]).: _Let \(X=\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) be a dataset where data points are independently generated from \(\nu\). Then_

\[\|(T_{k,\nu}+\lambda I)(T_{k,X}+\lambda I)^{-1}\|\leq 2\]

_holds with confidence at least \(1-\delta\) where \(\delta\in(0,1)\)._

**Lemma A.12** ([48]).: _Let \(X=\{\mathbf{x}^{1},\cdots,\mathbf{x}^{n}\}\) be a dataset where data points are independently generated from \(\nu\). For \(\lambda\in(0,1]\) such that \(\mathcal{N}_{\nu}(\lambda)\geq 1\),_

\[\|(T_{k,\nu}+\lambda I)(T_{k,X}+\lambda I)^{-1}\|\leq 2\]

_holds with confidence at least \(1-\delta\) where_

\[4\exp\left(-\frac{1}{4(\kappa^{2}+1)}\cdot\left(\frac{1+\log\mathcal{N}_{\nu }(\lambda)}{\lambda n}+\sqrt{\frac{1+\log\mathcal{N}_{\nu}(\lambda)}{\lambda n }}\right)^{-1}\right)\leq\delta<1.\]

To prove Lemma A.2 in Appendix A.2.5, we introduce a concentration inequality for Lipschitz functions.

**Lemma A.13** ([60]).: _Let \(X_{1},\cdots,X_{n}\) be independent random variables whose supports are contained in \([a,b]\) and \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\) be convex and \(L\)-Lipschitz with respect to the Euclidean norm. Then we have_

\[\mathbb{P}(f(X)\geq\mathbb{E}f(X)+t)\leq\exp\left(-\frac{t^{2}}{4L^{2}(b-a)^{ 2}}\right)\]

_where \(X=[X_{1},\cdots,X_{n}]\) and \(t>0\)._

Precisely, we use the following fact in Appendix A.2.5

_Remark A.14_ ([60]).: Let \(A\subset\mathbb{R}^{n}\) be a bounded set and

\[f(\mathbf{x})=\sup_{\mathbf{a}\in A}\sum_{k=1}^{n}a_{k}\mathbf{x}_{k}\]

where \(\mathbf{x}=[\mathbf{x}_{1},\cdots,\mathbf{x}_{n}]\in[-1,1]^{n}\) and \(\mathbf{a}=[a_{1},\cdots,a_{n}]\). Since

\[f(\mathbf{x})-f(\mathbf{x}^{\prime})=\sup_{\mathbf{a}\in A}\sum_{k=1}^{n}a_{k }\mathbf{x}_{k}-\sup_{\mathbf{a}\in A}\sum_{k=1}^{n}a_{k}\mathbf{x}_{k}^{ \prime}\leq\sup_{\mathbf{a}\in A}\left\langle\mathbf{a},\mathbf{x}-\mathbf{x }^{\prime}\right\rangle_{\mathbb{R}^{n}}\leq\sup_{\mathbf{a}\in A}\|\mathbf{a} \|_{\mathbb{R}^{n}}\|\mathbf{x}-\mathbf{x}^{\prime}\|_{\mathbb{R}^{n}},\]

\(f\) is a \(\sup_{\mathbf{a}\in A}\|\mathbf{a}\|_{\mathbb{R}^{n}}\)-Lipshitz function where \(\|\cdot\|_{\mathbb{R}^{n}}\) is the Euclidean norm on \(\mathbb{R}^{n}\). We can observe that \(f\) is convex since \(f\) is a supremum of convex functions defined on a convex compact set.

To prove Lemma A.6 in Appendix A.2.5, we recall the Ledoux-Talagrand contraction inequality [29, 56] and Talagrand's inequality [4, 56].

**Lemma A.15** (Ledoux-Talagrand Contraction Inequality).: _If \(\phi:\mathbb{R}\to\mathbb{R}\) is a \(L\)-Lipshitz function, then_

\[\mathbb{E}\left[\sup_{h\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i} \phi(h(\mathbf{x}_{i}))\right]\leq L\cdot\mathbb{E}\left[\sup_{h\in\mathcal{F }}\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}h(\mathbf{x}_{i})\right].\]

**Lemma A.16** (Talagrand's Inequality).: _Let \(X_{1},\cdots,X_{n}\) be independent \(\mathcal{X}\)-valued random variables. Let \(\mathcal{F}\) be a countably family of measurable real-valued functions on \(\mathcal{X}\) such that \(\|f\|_{\infty}\leq U<\infty\) and \(\mathbb{E}f(X_{i})=0\) for all \(f\in\mathcal{F}\). Let_

\[Z:=\sup_{f\in\mathcal{F}}\sum_{i=1}^{n}f(X_{i}),\quad\sigma^{2}\geq\frac{1}{n }\sum_{i=1}^{n}\sup_{f\in\mathcal{F}}\mathbb{E}[f(X_{i})^{2}],\quad\nu_{n}:= 2U\mathbb{E}Z+n\sigma^{2}.\]

_Then_

\[\mathbb{P}(Z\geq\mathbb{E}Z+t)\leq\exp\left(-\frac{t^{2}}{2\nu_{n}+\frac{2}{ 3}Ut}\right)\]

_for all \(t\geq 0\)._

Lastly, we recall the following well-known property used in Appendix A.2.5.

**Lemma A.17** ([2]).: _Let \(\mathcal{F}\) be a class of functions with ranges in \([a,b]\) and \(w_{1},\cdots,w_{n}\) be i.i.d. Rademacher variables. Then_

\[\frac{1}{n}\mathbb{E}\left[\sup_{h\in\mathcal{F}}\sum_{i=1}^{n}w_{i}f( \mathbf{x}^{i})\right]\leq\inf_{\alpha\in(0,1)}\left(\frac{1}{1-\alpha}\frac{ 1}{n}\mathbb{E}\left[\sup_{h\in\mathcal{F}}\sum_{i=1}^{n}w_{i}f(\mathbf{x}^{i })\ \bigg{|}\ X\right]+\frac{(b-a)\log(1/\delta)}{4n\alpha(1-\alpha)}\right)\]

_holds with probability at least \(1-\delta\). Also,_

\[\frac{1}{n}\mathbb{E}\left[\sup_{h\in\mathcal{F}}\sum_{i=1}^{n}w _{i}f(\mathbf{x}^{i})\ \bigg{|}\ X\right]\] \[\leq\inf_{\alpha>0}\left((1+\alpha)\frac{1}{n}\mathbb{E}\left[ \sup_{h\in\mathcal{F}}\sum_{i=1}^{n}w_{i}f(\mathbf{x}^{i})\right]+\frac{(b-a) \log(1/\delta)}{2n}\left(\frac{1}{2\alpha}+\frac{1}{3}\right)\right)\]

_holds with probability at least \(1-\delta\)._

## Appendix B Details on DCL-NN Algorithm

As we mentioned before, DCL-NN considers the same problem as in Section 3 but local models are heterogeneous neural networks. That is, there are \(m\) parties and \(D_{i}=\{(\mathbf{x}_{i}^{j},y_{i}^{j}):j=1,\cdots,n_{i}\}\) is the private dataset of the \(i\)th party \((i=1,\cdots,m)\) where \(D=\bigcup_{i=1}^{m}D_{i}\) are i.i.d. whose distribution is \(\rho_{\mathbf{x},y}\). One remark is that the local data distributions of parties are not the same in general. To communicate training information, we introduce an unlabeled public input dataset \(Z=\{\mathbf{z}^{1},\cdots,\mathbf{z}^{n_{0}}\}\subset\mathcal{X}\). The goal of parties is to find a minimizer of the population risk \(\mathcal{E}\) defined in Section 3.

To extend DCL-KR to heterogeneous neural network settings, it is necessary to ensure that the assumptions of DCL-KR are satisfied as much as possible. Specifically, one important assumption in DCL-KR is the equality of kernels across local models. Indeed, public data predictions can vary in conflicting directions after the local training procedure, even when using the same local datasets, if the kernels differ.

For further explanation of this claim, we consider the simple case of \(E=1\) in DCL-KR where \(E\) is the number of local iterations. After the consensus prediction \(u\) is distributed to local parties, the server then receives the updated local prediction on \(Z\):

\[(I-\frac{\eta}{n_{i}}K_{ZX_{i}}K_{X_{i}\tilde{Z}}K_{\tilde{Z}\tilde{Z}}^{-1})u +\frac{\eta}{n_{i}}K_{ZX_{i}\mathbf{y}_{i}}\] (14)

from the \(i\)th local party. (The notation is consistent with Appendix A) Suppose two parties have exactly the same dataset. If the same kernel is used in these two parties, the updated local predictionswill be identical. However, if the kernels are different, this will not be the case. For kernels like the Gaussian kernel, which have high correlation between close inputs, the updated local predictions will be strongly influenced by data points close to each input. On the other hand, for kernels like the linear kernel, which have high correlation between distant inputs, the updated local prediction on \(Z\) will be influenced more by data points farther from each input. We can observe this fact from the above formula (14). This observation implies that aggregating local learning information becomes very challenging when the kernels differ. In short, using the same kernel ensures that the shift mechanisms of predictions on \(Z\) at the edges are identical, making it possible for the aggregation through simple weighted averaging to work well. This is a key element of the strong theoretical results of DCL-KR and explains why kernel matching between neural networks is necessary in DCL-NN.

Let \(f_{i}\) be a local model of the \(i\)th party such that \(f_{i}(\cdot)=\mathbf{w}_{i}^{\top}g_{i}(\cdot)+b_{i}\), \(g_{i}:\mathcal{X}\rightarrow\mathbb{R}^{c_{i}}\), \(\mathbf{w}_{i}\in\mathbb{R}^{c_{i}}\), \(c_{i}\in\mathbb{N}\), and \(b_{i}\in\mathbb{R}\) for \(i=1,\cdots,m\). Since most modern neural network architectures have a linear layer as the last layer, this setting is general enough. As (3), we set the feature kernel of \(f_{i}\) (\(i=1,\cdots,m\)) to be

\[k_{f_{i}}(\mathbf{x}^{1},\mathbf{x}^{2})=g_{i}(\mathbf{x}^{1})^{\top}g_{i}( \mathbf{x}^{2}),\quad\mathbf{x}^{1},\mathbf{x}^{2}\in\mathcal{X}.\]

To bring the setting to the DCL-KR scheme, DCL-NN matches \(k_{f_{1}},\cdots,k_{f_{m}}\) via kernel distillation procedure. Obviously, the target kernel in this procedure is a key factor in enhancing performance.

Theoretically, using the ensemble kernel

\[k=\sum_{i=1}^{m}\frac{n_{i}}{n}k_{f_{i}}.\] (15)can be a good way to construct a good kernel derived from local feature kernels. The reason is that this ensemble kernel is identical to the kernel induced by the (scaled) concatenation of the local feature maps, i.e.,

\[k(\mathbf{x}^{1},\mathbf{x}^{2}) =\sum_{i=1}^{m}\frac{n_{i}}{n}k_{f_{i}}(\mathbf{x}^{1},\mathbf{x}^ {2})=\sum_{i=1}^{m}\frac{n_{i}}{n}g_{i}(\mathbf{x}^{1})^{\top}g_{i}(\mathbf{x}^ {2})\] \[=\begin{bmatrix}\sqrt{\frac{n_{1}}{n}}g_{1}(\mathbf{x}^{1})^{\top }&\sqrt{\frac{n_{2}}{n}}g_{2}(\mathbf{x}^{1})^{\top}&\cdots&\sqrt{\frac{n_{m}} {n}}g_{m}(\mathbf{x}^{1})^{\top}\end{bmatrix}\begin{bmatrix}\sqrt{\frac{n_{1} }{n}}g_{1}(\mathbf{x}^{2})\\ \sqrt{\frac{n_{2}}{n}}g_{2}(\mathbf{x}^{2})\\ \cdots\\ \sqrt{\frac{n_{m}}{n}}g_{m}(\mathbf{x}^{2})\end{bmatrix}.\]

In other words, the ensemble kernel has greater expressive power than individual feature kernels, and with a sufficient amount of data, it leads to better performance. We empirically verify that the performance of this ensemble kernel surpasses that of individual feature kernels in Figure 4.

DCL-NN sets this ensemble kernel \(k\) as the target kernel and local parties match their local feature kernels \(k_{f_{1}},\cdots,k_{f_{m}}\) with the kernel \(k\) using the public dataset \(Z\). For this purpose, we introduce Centered Kernel Alignment (CKA) [8] as a kernel similarity measure. The CKA between two kernels \(k_{1}\) and \(k_{2}\) on the public input distribution \(\tilde{\rho}_{\mathbf{x}}\) is given by

\[\text{CKA}(k_{1},k_{2})=\frac{\text{HSIC}(k_{1},k_{2})}{\sqrt{\text{HSIC}(k_{ 1},k_{1})\text{HSIC}(k_{2},k_{2})}}\]

where \(\text{HSIC}(\cdot,\cdot)\) is a Hilbert-Schmidt Independence Criterion (HSIC) defined as

\[\text{HSIC}(k_{i},k_{j})=\mathbb{E}_{\mathbf{x}^{1},\mathbf{x}^{2}\sim\tilde{ \rho}_{\mathbf{x}}}[k_{i}^{c}(\mathbf{x}^{1},\mathbf{x}^{2})k_{j}^{c}( \mathbf{x}^{1},\mathbf{x}^{2})]\]

and the centered kernel \(k_{i}^{c}\) is given by

\[k_{i}^{c}(\mathbf{x}^{1},\mathbf{x}^{2})=k_{i}(\mathbf{x}^{1},\mathbf{x}^{2} )-\mathbb{E}_{\mathbf{x}^{2}\sim\tilde{\rho}_{\mathbf{x}}}[k_{i}(\mathbf{x}^ {1},\tilde{\mathbf{x}}^{2})]-\mathbb{E}_{\tilde{\mathbf{x}}^{1}\sim\tilde{ \rho}_{\mathbf{x}}}[k_{i}(\tilde{\mathbf{x}}^{1},\mathbf{x}^{2})]+\mathbb{E}_ {\tilde{\mathbf{x}}^{1},\tilde{\mathbf{x}}^{2}\sim\tilde{\rho}_{\mathbf{x}}}[k _{i}(\tilde{\mathbf{x}}^{1},\tilde{\mathbf{x}}^{2})],\]

\(\mathbf{x}^{1},\mathbf{x}^{2}\in\mathcal{X}\) (\(i=1,2\)). However, since we have a finite number of samples, we employ the empirical CKA. The empirical CKA between two kernels \(k_{1}\) and \(k_{2}\) on inputs \(\{\mathbf{c}^{1},\cdots,\mathbf{c}^{p}\}\) is given by

\[\widehat{\text{CKA}}(k_{1},k_{2})=\frac{\widehat{\text{HSIC}}(K_{1},K_{2})}{ \sqrt{\widehat{\text{HSIC}}(K_{1},K_{1})\widehat{\text{HSIC}}(K_{2},K_{2})}}\] (16)

where

\[K_{1}=\begin{bmatrix}k_{1}(\mathbf{c}^{1},\mathbf{c}^{1})&\cdots&k_{1}( \mathbf{c}^{1},\mathbf{c}^{p})\\ \vdots&\ddots&\vdots\\ k_{1}(\mathbf{c}^{p},\mathbf{c}^{1})&\cdots&k_{1}(\mathbf{c}^{p},\mathbf{c}^{ p})\end{bmatrix}\text{ and }K_{2}=\begin{bmatrix}k_{2}(\mathbf{c}^{1},\mathbf{c}^{1})&\cdots&k_{2}( \mathbf{c}^{1},\mathbf{c}^{p})\\ \vdots&\ddots&\vdots\\ k_{2}(\mathbf{c}^{p},\mathbf{c}^{1})&\cdots&k_{2}(\mathbf{c}^{p},\mathbf{c}^{ p})\end{bmatrix}\]

are Gram matrices and \(\widehat{\text{HSIC}}\) is an estimator of HSIC defined as

\[\widehat{\text{HSIC}}(L,M)=\frac{1}{(p-1)^{2}}\text{tr}(LHMH),\quad L,M\in \mathbb{R}^{p\times p}\]

where \(H:=I_{p}-\frac{1}{p}\mathbf{1}\mathbf{1}^{\top}\) is the centering matrix, \(I_{p}\) is a \(p\times p\) identity matrix, and \(\mathbf{1}=[1,1,\cdots,1]^{\top}\) is a \(p\)-dimensional one vector. During the kernel distillation procedure, the \(i\)th local party maximizes \(\widehat{\text{CKA}}(k_{f_{i}},k)\) on \(Z\) where \(k\) is a fixed target kernel given by (4). In practice, we use batching to perform the kernel distillation to reduce computational costs.

Due to the definition of the empirical CKA, it is necessary to calculate the Gram matrix of \(k\) over \(Z\). To this end, the \(i\)th local party calculates the Gram matrix of \(k_{f_{i}}\) over \(Z\) and uploads it to the server for \(i=1,\cdots,m\). Then the server computes the Gram matrix of \(k\) by weighted averaging the Gram matrices of local feature kernels \(k_{f_{1}},\cdots,k_{f_{m}}\). Since this process only requires communication of feature kernel values, DCL-NN still preserves the privacy of local model information.

While (empirical) CKA is a good metric for kernel matching, it is invariant to scaling, and therefore the local feature kernels resulting from the kernel distillation may have different scales. This affects the degree to which each local training influences during the DCL-KR-like follow-up procedure. Toillustrate this point, consider the following example: Let the feature kernels of two local models \(f_{1}\) and \(f_{2}\) be as follows:

\[k_{f_{1}}(\mathbf{x},\mathbf{y})=\phi(\mathbf{x})^{\top}\phi( \mathbf{y}),\qquad k_{f_{2}}(\mathbf{x},\mathbf{y})=(\alpha\phi(\mathbf{x}))^{ \top}(\alpha\phi(\mathbf{y})).\]

After distilling on the public data with consensus predictions, these two models become like

\[f_{1}(\cdot)=\mathbf{w}^{\top}\phi(\cdot)+b,\qquad f_{2}(\cdot)= \left(\frac{1}{\alpha}\mathbf{w}\right)^{\top}(\alpha\phi(\cdot))+b=\mathbf{w }^{\top}\phi(\cdot)+b,\]

i.e., two models are the same. Nevertheless, a gradient descent update on a data point \((\mathbf{x}_{0},y_{0})\) with a learning rate \(\eta\) is

\[\mathbf{w}_{1}\leftarrow\mathbf{w}-\eta(\mathbf{w}^{\top}\phi( \mathbf{x}_{0})+b-y_{0})\]

for \(f_{1}\) and

\[\mathbf{w}_{2}\leftarrow\frac{1}{\alpha}\mathbf{w}-\eta(\mathbf{ w}^{\top}\phi(\mathbf{x}_{0})+b-y_{0})\]

for \(f_{2}\). Thus, after the gradient descent step we have

\[f_{1}(\cdot)=(\mathbf{w}-\eta(\mathbf{w}^{\top}\phi(\mathbf{x}_{0})+b-y_{0})) ^{\top}\phi(\cdot)+b\]

and

\[f_{2}(\cdot)=\left(\frac{1}{\alpha}\mathbf{w}-\eta(\mathbf{w}^{ \top}\phi(\mathbf{x}_{0})+b-y_{0})\right)^{\top}(\alpha\phi(\cdot))+b=( \mathbf{w}-\alpha\eta(\mathbf{w}^{\top}\phi(\mathbf{x}_{0})+b-y_{0}))^{\top} \phi(\cdot)+b.\]

Hence the scale \(\alpha\) affects the collaborative learning procedure. To address this issue, we compute the scale \(\alpha\) using the estimator \(\widehat{\text{HSIC}}\). Specifically, at the beginning of the collaborative learning phase, we compute \(\alpha_{i}=\widehat{\text{HSIC}}(K_{i},K_{i})\) where \(K_{i}\) is the Gram matrix with respect to the feature kernel of the \(i\)th party on \(Z\) (\(i=1,\cdots,m\)). Then we set the learning rate for the \(i\)th party as

\[\eta_{0}\cdot\frac{\max_{1\leq j\leq m}\alpha_{j}^{1/2}}{\alpha_{i}^{1/2}}\]

where \(\eta_{0}\) is a base learning rate. In practice, computing HSIC over all public inputs is costly and unnecessary. Using only a small subset of public inputs is sufficient.

We present DCL-NN in Algorithm 2. Here are some remarks.

1. The feature kernel distillation procedure requires only one round of two-way communication between the server and the parties.
2. The collaborative learning procedure follows the same process as in DCL-KR with \(g_{1},\cdots,g_{m}\) fixed. Note that kernel gradient descent reduces to standard gradient descent since the kernels have finite rank. In this process, if possible, optimization on the public dataset can be performed using the closed-form solution of kernel linear regression instead of gradient descent.
3. In this work, we apply FedMD for the pretraining of DCL-NN in the experiment. However, DCL-NN is a general algorithm that can use any algorithm for pretraining to obtain good local feature kernels. For example, kernel learning techniques [63] may be applied for pretraining.
4. Our algorithm can be naturally extended to regression problems with multi-dimensional outputs.

## Appendix C Details and Further Discussion on Experiments

### Dataset Description

For all datasets, we follow Algorithm 3 to construct non-i.i.d. settings. Note that this procedure is similar to the non-i.i.d. data generation procedure in classification tasks [44, 69].

#### c.1.1 Toy-1D

Let \(\mathcal{X}=[0,1]\subset\mathbb{R}\) and \(\rho_{x}\) be the uniform distribution on \(\mathcal{X}\). The space

\[H^{1}:=\left\{f\in\text{AC}[0,1]\ \Big{|}\ f(0)=0,\int f^{\prime}(x)^{2}\ d\rho_{x }(x)<\infty\right\}\]

is the reproducing kernel Hilbert space associated to the kernel \(k(x,y)=\min(x,y)\) where \(\text{AC}[0,1]\) is the collection of all absolutely continuous functions on \([0,1]\)[33, 60]. As mentioned in [33], the covariance operator \(T_{k,\rho_{x}}\) has eigenpairs \(\{(\lambda_{i},e_{i})\}_{i\in\mathbb{N}}\) where

\[\lambda_{i}=\left(\frac{2i-1}{2}\pi\right)^{-2},\qquad e_{i}(x)=\sqrt{2}\sin \left(\frac{2i-1}{2}\pi x\right).\]

Thus, the eigenvalue decay rate \(s\) is \(\frac{1}{2}\). Set a target function

\[f_{0}^{*}(x)=\sum_{i=1}^{\infty}\frac{e_{i}(x)}{i^{3}}=\sum_{i=1}^{\infty} \frac{\sqrt{2}}{i^{3}}\sin\left(\frac{2i-1}{2}\pi x\right).\]

From the fact that

\[\left\|T_{k,\rho_{x}}^{1/2-r}\sum_{i=1}^{\infty}h_{i}e_{i}\right\|_{\mathbb{H }_{k}}\leq R\quad\Leftrightarrow\quad\sum_{i=1}^{\infty}\frac{h_{i}^{2}}{ \lambda_{i}^{2r}}\leq R^{2},\]

we have \(f_{0}^{*}=T_{k,\rho_{x}}^{1/2}g_{0}^{*}\) such that

\[\|g_{0}^{*}\|_{\mathbb{H}_{k}}=\left(\sum_{i=1}^{\infty}\frac{1}{i^{6}}\left( \frac{2i-1}{2}\pi\right)^{4}\right)^{1/2}=:R<\infty.\]

Then \(r=1\). We generate data points from \(\rho_{x}\cdot\rho_{y|x}\) such that \(\rho_{x}\) is the uniform distribution on \(\mathcal{X}\) as above and

\[\rho_{y|x}=\mathcal{N}(y|f_{0}^{*}(x),0.44^{2}).\]

We divide \(\mathcal{X}\) into a partition \(\mathcal{A}:=\left\{\left[\frac{i}{8},\frac{i+1}{8}\right]:i=0,\cdots,7\right\}\). We follow Algorithm 3 with \(m\in[10,20,\cdots,100]\), \(n=50m\), and \(n_{0}\approx n^{\frac{1}{2r+i}}(\log_{10}n)^{3}\). With \(n_{0}\approx n^{\frac{1}{2r+i}}(\log_{10}n)^{3}\), we also achieve the same bound (with different prefactors) in Theorem 3.4. In the main experiments, we set \(\rho_{\mathbf{x}}=\tilde{\rho}_{\mathbf{x}}\).

#### c.1.2 Toy-3D

Let \(\mathcal{X}=[0,1]^{3}\subset\mathbb{R}^{3}\) and \(\rho_{\mathbf{x}}\) be the uniform distribution on \(\mathcal{X}\). Define a kernel \(k(\mathbf{x},\mathbf{y})=(1-\|\mathbf{x}-\mathbf{y}\|_{\mathbb{R}^{3}})_{+}^{2}\). The reproducing kernel Hilbert space \(\mathbb{H}_{k}\) associated to the kernel \(k\) is norm-equivalent to Sobolev space \(H^{2}(\mathcal{X})\)[55] and the eigenvalue decay of \(T_{k,\rho_{\mathbf{x}}}\) is \(s=\frac{3}{4}\)[68]. Note that \(k^{\prime}(\mathbf{x},\mathbf{y})=(1-\|\mathbf{x}-\mathbf{y}\|_{\mathbb{R}^{3 }})_{+}^{6}(35\|\mathbf{x}-\mathbf{y}\|_{\mathbb{R}^{3}}^{2}+18\|\mathbf{x}- \mathbf{y}\|_{\mathbb{R}^{3}}+3)\) is a kernel and its reproducing kernel Hilbert space is norm-equivalent to Sobolev space \(H^{4}(\mathcal{X})\). Using the interpolation relation between \(H^{2}(\mathcal{X})\) and \(H^{4}(\mathcal{X})\) gives that

\[f_{0}^{*}(\mathbf{x})=(1-\|\mathbf{x}\|_{\mathbb{R}^{3}})_{+}^{6}(35\| \mathbf{x}\|_{\mathbb{R}^{3}}^{2}+18\|\mathbf{x}\|_{\mathbb{R}^{3}}+3)\]

is in \(\mathbb{H}_{k}\) and the regularity \(r\) is \(1\). Similarly as before, we generate data points from \(\rho_{\mathbf{x}}\cdot\rho_{y|\mathbf{x}}\) such that \(\rho_{\mathbf{x}}\) is the uniform distribution on \(\mathcal{X}\) and

\[\rho_{y|\mathbf{x}}=\mathcal{N}(y|f_{0}^{*}(\mathbf{x}),0.44^{2}).\]

We divide \(\mathcal{X}\) into a partition \(\mathcal{A}:=\{[\frac{k_{1}}{2},\frac{k_{1}+1}{2}]\times[\frac{k_{2}}{2}, \frac{k_{2}+1}{2}]\times[\frac{k_{3}}{2},\frac{k_{3}+1}{2}]\subset\mathbb{R}^ {3}:(k_{1},k_{2},k_{3})\in\{0,1\}^{3}\}\). Again, we follow Algorithm 3 with \(m\in[10,20,\cdots,100]\), \(n=50m\), \(n_{0}\approx n^{\frac{1}{2r+i}}(\log_{10}n)^{3}\), and \(\rho_{\mathbf{x}}=\tilde{\rho}_{\mathbf{x}}\) for kernel machine-based algorithms. For neural network-based algorithms, \(m=50\), \(n_{0}=2500\), and the other configurations are the same.

#### c.1.3 Real World Datasets

EnergyEnergy dataset is a real-world tabular dataset from the UCI database [12]. It has 28 input features including measurement time, temperature and humidity of each room, outside temperature, and wind speed. The output is the appliances energy use. We normalize all features, including the output, using MinMaxScaler. There are 12,000 training data points distributed across the parties. We use 6,000 samples as public inputs and 1,000 samples for testing. To construct a non-i.i.d. setting, we set a partition \(\mathcal{A}\) consisting of 8 subsets, each formed by splitting three normalized variables (measurement time, visibility, and dewpoint) at their midpoints. We apply Algorithm 3 with \(m=50\).

RotatedMNISTRotatedMNIST is a dataset derived from MNIST [11]. The task is to predict the rotated angle of a given rotated MNIST image. Each image is \(1\times 28\times 28\) image, and we normalize all images by their mean and variance. To generate RotatedMNIST, we rotate MNIST images by a random angle between \(-\frac{\pi}{2}\) and \(\frac{\pi}{2}\) and use the angle as the label. To construct a large-scale dataset, each image is rotated at multiple angles to generate multiple data instances. For training data, we additionally inject Gaussian noise with a standard deviation of \(0.2\) to each label. We use 200,000 images as the entire training data, 50,000 images as public inputs, and 50,000 images as test data. The whole training input distribution, public input distribution, and test input distribution are uniformly distributed across digits 0 to 9. For example, there are 20,000 rotated images of the digit '4' in the training set. For a non-i.i.d. setting, we partition the data into \(\mathcal{A}\) where \(|\mathcal{A}|=10\), based on the digit (\(0\sim 9\)) and follow Algorithm 3 with \(m=50\).

UTKFaceUTKFace dataset [71] is an image dataset used for age estimation. Since the image sizes vary, we resize all images to \(3\times 128\times 128\) and normalize them by their mean and variance for each channel. The labels are normalized to the range \([0,1]\) using MinMaxScaler. For training data, we inject Gaussian noise with a standard deviation of \(0.5\) to each label before normalization. We use 12,544 samples for training and 1,039 samples for testing. We have 6,234 public inputs. These three datasets have the same distribution for metadata (gender and race). Based on this metadata, we construct a partition \(\mathcal{A}\) with \(|\mathcal{A}|=10\) and distribute the training data among 50 parties according to Algorithm 3.

Imdb-WikiIMDB-WIKI dataset [52] is also an image dataset for age estimation. In experiments, we utilize a clean version [42]. We further resize all images to \(3\times 64\times 64\) and normalize them as UTKFace. The labels are also normalized to the range \([0,1]\) using MinMaxScaler. We use 147,107 images as the entire training data, 36,780 images as public inputs, and 56,087 images as test data. In this dataset, we utilize the triplet (head_roll, head_yaw, head_pitch) as metadata. Both training inputs and public inputs have the same distribution for metadata. We construct a decentralized setting among 50 parties using a partition \(\mathcal{A}\) based on the metadata, following Algorithm 3. The partitioning is performed by dividing the dataset into regions based on the median values of each metadata variable.

### Implementation

The experiments are implemented in PyTorch. We simulate a decentralized setting on a single deep learning workstation (Intel(R) Xeon(R) Gold 6430 with one NVIDIA GeForce RTX 4090 GPU and 189GB RAM). All DCL-KR implementations take less than 10 minutes per simulation. The non-parallel implementation of DCL-NN for large-scale datasets is completed within 48 hours. With parallel computing, the execution time for the same setup is expected to be reduced to within 2 hours.

### Experimental Setup and Results on Kernel Machine-based Algorithms

#### c.3.1 Experimental Setup Details and Main Results

In this experiment, we evaluate DCL-KR by comparing its performance against two central models and three baselines. Specifically, we employ DC-NY [66], DKRR-NY-CM [67], and IED [48] as baselines. We also compare DCL-KR with central Kernel Ridge Regression (centralKRR) and central Kernel Regression with Gradient Descent (centralKRGD). As mentioned earlier, we evaluate the performance of these algorithms on Toy-1D and Toy-3D datasets.

There are several hyperparameters for kernel machine-based algorithms: the ridge regularization hyperparameter \(\lambda\) in the ridge regressions and the number of iterations (or communication rounds) in the gradient descent-based regressions. We set \(\lambda=C\cdot n^{-\frac{1}{2r+s}}\) and \(T=\text{int}(D\cdot n^{\frac{1}{2r+s}})\) which are the optimal choices from theory. We determine the best values for \(C\) and \(D\) by grid search in our experiments (see Table 4 for the selected hyperparameter values). For centralKRGD and DCL-KR, we set the learning rate \(\eta=0.5\) which satisfies \(\eta\in(0,1/\kappa^{2})\) in Theorem 3.4. The number of local iterations \(E\) for DCL-KR is set to 5. For DKRR-NY-CM, we modify its Newton-Raphson iteration as shown below due to an instability issue, and we set the learning rate \(\eta=0.01\):

\[u\gets u-\eta\cdot\sum_{j=1}^{m}\frac{n_{j}}{n}(P_{Z}T_{k,X_{i}}P_{Z}+ \lambda I)^{-1}((P_{Z}T_{k,X}P_{Z}+\lambda I)u-P_{Z}S_{D}^{\top}\mathbf{y}).\]

The communication round \(T\) for DKRR-NY-CM is set to \(10\).

To measure performance, we sample data from the distribution presented in Appendix C.1 for each simulation and compute \(\mathbb{E}\|_{t_{\rho_{\mathbf{x}}}}(f_{i,T}-f_{0}^{*})\|_{L_{\rho_{\mathbf{x} }}^{2}}\) by averaging the Root Mean Squared Errors (RMSEs) on the test dataset. We conduct 500 simulations for each setting, and the results are summarized in Figure 1.

As shown in Table 1, DC-NY and DKRR-NY-CM have theoretical performance guarantees under the statistically homogeneous condition for a limited number of parties. However, they do not exhibit sufficiently good performance in massively distributed statistically heterogeneous settings. The performance degradation of DKRR-NY-CM appears to be linked to its second-order optimization scheme, which leads to ineffective batching in statistically heterogeneous settings. The performance degradation of DC-NY is expected, given the inherent limitations of divide-and-conquer algorithms.

In contrast, IED demonstrates relatively better performance, despite the strong assumptions underlying its theory. Nevertheless, it still exhibits performance degradation compared to centralized models. DCL-KR, on the other hand, achieves performance comparable to centralized models, validating both the theoretical results and its practical feasibility.

#### c.3.2 Effect of \(n_{0}\)

As public inputs directly affect the training information sharing, we anticipate that the performance of DCL-KR will vary depending on the number of public inputs \(n_{0}\). To examine this effect, we

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline  & centralKRR & centralKRGD & DC-NY & DKRR-NY-CM & IED & DCL-KR \\ \hline Toy-1D & \(C=0.055\) & \(D=15\) & \(C=0.006\) & \(C=0.008\) & \(C=0.025\) & \(D=2.5\) \\ Toy-3D & \(C=0.016\) & \(D=50\) & \(C=0.002\) & \(C=0.005\) & \(C=0.007\) & \(D=12.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters \(C\) and \(D\) on kernel machine-based algorithms in main resultsmeasure the performance of DCL-KR on Toy-3D for \(n_{0}\approx\alpha\cdot n^{\frac{1}{2r+s}}(\log_{10}n)^{3}\) with various \(\alpha\in\{0.1,0.3,0.5,1,2\}\). Additionally, we conduct the same experiment with IED, the most competitive baseline, for comparison.

The results are summarized in Figure 2. Consistent with theoretical results, the value of \(\alpha\) does not affect the convergence rate of IED and DCL-KR. However, IED displays significant performance variations across different \(\alpha\) values. In contrast, DCL-KR achieves its maximum performance when \(\alpha\) is not too small (i.e., \(\alpha\geq 0.3\) in Figure 2). This implies that DCL-KR requires fewer public inputs to achieve its maximal performance compared to IED, as predicted by theoretical results.

#### c.3.3 Effect of \(\tilde{\rho}_{\mathbf{x}}\)

So far, we consider the settings with \(\rho_{\mathbf{x}}=\tilde{\rho}_{\mathbf{x}}\). However, Theorem 3.4 covers the general case where \(\tilde{\rho}_{\mathbf{x}}\neq\rho_{\mathbf{x}}\). To verify this, we define the public input distribution in Toy-3D with the following density function (parametrized by \(\beta\)):

\[p(x_{1},x_{2},x_{3}|\beta)=\prod_{i=1}^{3}((2-2\beta)x_{i}+\beta),\quad(x_{1},x_{2},x_{3})\in[0,1]^{3},\quad\beta\in(0,1].\]

The Radon-Nikodym derivative \(\frac{d\rho_{\mathbf{x}}}{d\rho_{\mathbf{x}}}\) satisfies \(0\leq\frac{d\rho_{\mathbf{x}}}{d\tilde{\rho}_{\mathbf{x}}}\leq(\frac{1}{\beta })^{3}\). We conduct additional experiments to verify Theorem 3.4 and Corollary A.7, considering the case where \(\beta=0.5\) (with \(\alpha=1\)) and the case where \(\beta=0.5\) but \(\alpha=4\) to compensate. The results are provided in Figure 5. In the log-scale plot, the slope represents the convergence rate.

First, we observe that the convergence rate of DCL-KR remains unchanged when \(\beta\) is changed from 1 (i.e., \(\rho_{\mathbf{x}}=\tilde{\rho}_{\mathbf{x}}\)) to 0.5. This observation is consistent with Theorem 3.4. Additionally, regarding Corollary A.7, we confirm that DCL-KR achieves performance almost identical to the case of \(\rho_{\mathbf{x}}=\tilde{\rho}_{\mathbf{x}}\) by increasing \(n_{0}\). In contrast, the convergence rate of IED worsens when \(\tilde{\rho}_{\mathbf{x}}\) changes, even when \(n_{0}\) is increased. These experimental results highlight the advantages of DCL-KR in statistically heterogeneous environments.

### Experimental Setup and Results on Neural Network-based Algorithms

#### c.4.1 Experimental Setup Details

In the experiments on neural network-based collaborative learning algorithms, we evaluate three baselines (FedMD, FedHeNN, KT-pFL) and our algorithm DCL-NN. Note that while KT-pFL is a personalized collaborative learning algorithm, it also performs well in non-personalized settings, so we include it for comparison. Additionally, we evaluate centralized models as ideal cases and standalone models as worst cases. Centralized models are trained using all local data.

We use two tabular datasets, Toy-3D and Energy, and three image datasets, RotatedMNIST, UTKFace, and IMDB-WIKI. The number of parties is set to 50 for all settings. For the tabular datasets, we employ four different fully connected neural networks (FNNs) with a ratio of 30%, 30%, 20%, and 20%. Specifically, There are fifteen 4-layer FNNs with 32 hidden units, fifteen 4-layer FNNs with 64 hidden units, ten 5-layer FNNs with 32 hidden units, and ten 3-layer FNNs with 64 hidden units. Similarly, for the image datasets, we use four different convolutional neural networks (CNNs) with the same ratio of 30%, 30%, 20%, and 20%. For the large-scale image datasets (RotatedMNIST and IMDB-WIKI), we construct 50 local parties using fifteen ResNet-18, fifteen ResNet-34, ten ResNet-50 [20], and ten MobileNetv2 [54]. For UTKFace, which is an image dataset with limited data, we utilize four simpler CNN architectures due to the ineffectiveness of knowledge distillation with large underperforming models. The first and third CNNs share a similar architecture, featuring two convolutional layers with batch normalization [22], two max pooling layers, and two fully-connected layers at the end. They differ only in the number of channels. In contrast, the second and fourth CNNs are more complex, featuring four convolutional layers with batch normalization, two max pooling layers, and two fully-connected layers at the end. They also differ in the number of channels. We use the ReLU activation function throughout.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Toy-3D & Energy & MNIST & UTKFace & IMDB \\ \hline communication rounds & 500 & 100 & 50 & 50 & 50 \\ sample size of public data & 500 & 1000 & 5000 & 2000 & 5000 \\ learning rate & 2e-4 & 1e-4 & 1e-4 & 5e-5 & 2e-4 \\ local epochs & 10 & 10 & 5 & 5 & 5 \\ distillation epochs & 5 & 1 & 20 & 20 & 20 \\ batch size (local) & 10 & 10 & 32 & 16 & 32 \\ batch size (public) & 32 & 32 & 16 & 16 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters for FedMD

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Toy-3D & Energy & MNIST & UTKFace & IMDB \\ \hline communication rounds & \(\leq\)100 & \(\leq\)100 & 50 & 100 & \(\leq\)50 \\ sample size of public data & 500 & 500 & 5000 & 1000 & 5000 \\ learning rate & 2e-4 & 2e-4 & 1e-4 & 1e-4 & 5e-4 \\ distillation coefficient & 1 & 1 & 1 & 0.1 & 1 \\ batch size (local) & 10 & 10 & 32 & 10 & 16 \\ batch size (public) & 32 & 32 & 16 & 32 & 16 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters for FedHeNN

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Toy-3D & Energy & MNIST & UTKFace & IMDB \\ \hline batch size & 10 & 16 & 128 & 16 & 32 \\ learning rate & 1e-2 & 2e-2 & 5e-3 & 1e-4 & 5e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters for StandaloneAll optimizers used are Adam [26].2 One remark is that baseline algorithms only utilize a subset of public inputs through random sampling in each communication round, as performance tends to deteriorate due to overfitting when all public inputs are used in every round. Hyperparameters are tuned via grid search.

Footnote 2: Note that while DCL-NN should use vanilla gradient descent according to DCL-KR, Adam performs better in practice.

Standalone models are trained with cross-validation and early stopping to prevent overfitting. To evaluate centralized models, we first compute the averaged test Root Mean Squared Error (RMSE) from at least 10 simulations for each neural network architecture and then calculate the weighted average of the performances of all architectures according to their ratio. For standalone models, we use the average of the test RMSEs of local models with the hyperparameters listed in Table 5. In the table, RotatedMNIST is abbreviated as MNIST, and IMDB-WIKI is abbreviated as IMDB.

For FedHeNN, we set the number of local epochs to 30 in all experiments. For KT-pFL, we set the number of local epochs to 10, the distillation coefficient to 0.5, and the learning rate of knowledge coefficient to 1e-3. Lastly, for DCL-NN in the main experiment, we set the learning rate for kernel matching to 1e-4 and the number of communication rounds in the collaborative learning phase to 50. We use the closed-form solution to train public data and full-batch gradient descent to train local data in the collaborative learning phase of DCL-NN and utilize FedMD for pretraining in DCL-NN. In the pretraining phase, the hyperparameters are the same as in the FedMD setting, except that the number of communication rounds is 100 for tabular data and 50 for image data. The remaining hyperparameters are presented in Table 6, 7, 8, and 9. For all distillation-based collaborative learning algorithms, we simulate each setting at least 5 times with different initializations.

Communication EfficiencyCompared with FedMD and KT-pFL, DCL-NN incurs higher communication costs of \(O(n_{0}^{2})\) due to the transmission of the Gram matrix. FedHeNN also utilizes kernel matching but performs it in batches for each communication round. Thus, in scenarios requiring many communication rounds, DCL-NN is more efficient than FedHeNN. However, pretraining also demands more communication cost. We leave the study of communication-efficient methods in DCL-NN for future work.

#### c.4.2 Effect of Public Inputs

In practice, the public inputs can be sampled from a distribution whose support is disjoint from that of the whole local input distribution. In this case, the assumption of DCL-KR does not hold; however, DCL-NN can still be applicable. To evaluate the performance of DCL-NN under these conditions, we compare the performance of DCL-NN and FedMD when the distribution of public inputs differs, as in Table 10. We use CIFAR10 [28] for public inputs on UTKFace. As shown in Table 10, DCL-NN

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Toy-3D & Energy & MNIST & UTKFace & IMDB \\ \hline communication rounds & 50 & \(\leq\)50 & 100 & 50 & \(\leq\)50 \\ sample size of public data & 500 & 500 & 5000 & 2000 & 1000 \\ learning rate & 2e-4 & 1e-4 & 1e-4 & 1e-4 & 2e-4 \\ distillation epochs & 2 & 1 & 10 & 2 & 10 \\ batch size (local) & 10 & 10 & 32 & 16 & 16 \\ batch size (public) & 32 & 16 & 32 & 16 & 16 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Hyperparameters for KT-pFL

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Toy-3D & Energy & MNIST & UTKFace & IMDB \\ \hline epochs (kernel matching) & 100 & 100 & 200 & 200 & 200 \\ base learning rate (local) & 5e-2 & 1e-2 & 8e-3 & 8e-3 & 8e-3 \\ local epochs & 50 & 50 & 25 & 25 & 25 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Hyperparameters for DCL-NNdoes not yield better results in this case. Note that the kernel performance of local feature kernels is improved compared to FedMD, leading us to conclude that the performance degradation of DCL-NN with CIFAR10 is due to the violation of the DCL-KR assumption rather than the ineffectiveness of the kernel distillation procedure.

## Appendix D Limitations and Future Works

Privacy Benefits of Distillation-based Collaborative LearningDue to its black-box nature, distillation-based information interaction is expected to offer privacy preservation benefits compared to parameter exchange (mainly done in FL) as mentioned in [19]. To the best of our knowledge, there is no rigorous study that discusses the privacy preservation advantages of distillation-based collaborative learning. We hope to see further discussion on this as well.

Public Input DistributionTheorem 3.4 covers the case where the public input distribution \(\tilde{\rho}_{\mathbf{x}}\) differs from that of local data inputs \(\rho_{\mathbf{x}}\), but at least the support of \(\tilde{\rho_{\mathbf{x}}}\) must include the support of \(\rho_{\mathbf{x}}\). Therefore, we experimentally observe a performance drop in DCL-NN, a practical extension of DCL-KR, when \(\tilde{\rho}_{\mathbf{x}}\) and \(\rho_{\mathbf{x}}\) have different supports. Enhancing the robustness of DCL-NN in this scenario is considered a promising direction for future work. Our theory does not cover situations where collecting public inputs is difficult. In such cases, a seperate generative model is usually trained to generate public inputs [72]. We leave the theoretical discussion that includes these cases for future work.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & FedMD & DCL-NN \\ \hline w/ UTKFace & 0.151 \(\pm\) 0.004 (0.149) & **0.148 \(\pm\) 0.001 (0.146)** \\ w/ CIFAR10 & **0.160 \(\pm\) 0.000** (0.159) & 0.162 \(\pm\) 0.001 (0.158) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Performance comparison of FedMD and DCL-NN on UTKFace with different public datasets. In addition to performance, the kernel performance of local feature kernels, computed in the same way as before, is shown in parantheses.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately describe the motivations, theoretical/experimental contributions, and scope of our work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix D. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The theoretical results and its assumptions are clearly described in Section 3 and Appendix A. Idea of the proof is briefly described in Section 3 and the full proof is provided in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The proposed algorithms are clearly stated in Algorithm 1 and Algorithm 2. Experimental details such as experimental setting, performance measure, data preprocessing, and hyperparameter setting are also explained in Section 5 and Appendix C. The code is also provided via the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided via the supplementary material. Regarding datasets, the paper contains data source and preprocessing descriptions. The paper also provides sufficient experimental details to reproduce the experimental results. See Section 5 and Appendix C. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental details such as data split, hyperparameters, optimizers, and model architectures are provided in Section 5 and Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The experimental results contains 1-sigma error bars and the explanations about the error bars are provided. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Descriptions of computer resources are provided in Appendix C.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors verify that the research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section 6.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All data and models used in the paper are credited through citations according to the license. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is provided via the supplementary material with a well-written documentation. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.